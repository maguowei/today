[
  {
    "title": "Java近期新闻：单查询加载、GraalVM、GlassFish、JReleaser、Quarkus、Micronaut",
    "url": "https://www.infoq.cn/article/wti8XjbwvZdWSr79kOc6",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p><a href=\"https://jcp.org/en/home/index\">Java社区进程</a>\"（JCP）执行委员会<a href=\"https://jcp.org/en/jsr/results?id=6356\">投票</a>\"通过了JSR 396（<a href=\"https://openjdk.org/projects/jdk/21/spec/\">Java SE 21平台</a>\"），向着计划在2023年9月19日发布的GA版本继续迈进。</p><p>JEP 442（<a href=\"https://openjdk.org/jeps/442\">外部函数和内存API第3个预览版</a>\"）将在即将发布的JDK 21中交付，JEP草案8310626（<a href=\"https://openjdk.org/jeps/8310626\">外部函数和内存API</a>\"）预计将在JDK 22中<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-August/008061.html\">交付</a>\"，那是<a href=\"https://openjdk.org/projects/panama/\">Panama项目</a>\"的主要特性之一。该项目将JVM与定义明确的“外部”（非Java） API连接起来，其中包括许多C程序员常用的接口。</p><p><a href=\"https://www.linkedin.com/in/minborg/\">Per-Åke Minborg</a>\"是Oracle的一名技术顾问。他在<a href=\"http://minborgsjavapot.blogspot.com/2023/08/java-22-panama-ffm-provides-massive.html\">这篇博文</a>\"中讨论了外部函数和内存API的性能优势。Minborg提供了一个关于字符串转换的基准测试：在JDK 21和JDK 22中使用该API vs. 使用旧的Java本机接口（JNI）调用。</p><p></p><h4>JDK 21</h4><p></p><p><a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B35\">Build 35</a>\"仍然是JDK 21<a href=\"https://jdk.java.net/20/\">早期访问构建</a>\"的当前构建。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p></p><h4>JDK 22</h4><p></p><p>JDK 22<a href=\"https://jdk.java.net/22/\">早期访问构建</a>\"<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-22%2B13\">Build 13</a>\"在上周发布，其中包括Build 12的<a href=\"https://github.com/openjdk/jdk/compare/jdk-22%2B12...jdk-22%2B13\">更新</a>\"，主要是修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2022%20and%20%22resolved%20in%20build%22%20%3D%20b13%20order%20by%20component%2C%20subcomponent\">问题</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/22/release-notes\">发布说明</a>\"。</p><p>对于<a href=\"https://openjdk.org/projects/jdk/22/\">JDK 22</a>\"和<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\"，我们鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告Bug。</p><p></p><h4>GraalVM</h4><p></p><p>在迈向1.0版本的道路上，Oracle实验室发布了<a href=\"https://github.com/graalvm/native-build-tools/blob/master/README.md\">Native Build Tools</a>\"的<a href=\"https://github.com/graalvm/native-build-tools/releases/tag/0.9.25\">0.9.25版本</a>\"。这是一个GraalVM项目，其中包含与GraalVM原生镜像进行互操作的插件。这个最新版本将依赖项升级到了<a href=\"https://github.com/oracle/graalvm-reachability-metadata/blob/master/README.md\">GraalVM Reachability Metadata Repository</a>\" 0.3.4。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/graalvm/native-build-tools/compare/0.9.24...0.9.25\">变更日志</a>\"。</p><p></p><h4>Spring Framework</h4><p></p><p>为了解决N+1问题，Spring Data团队<a href=\"https://spring.io/blog/2023/08/31/this-is-the-beginning-of-the-end-of-the-n-1-problem-introducing-single-query\">引入</a>\"了单查询加载（Single Query Loading）。这是一种通过单个SELECT语句加载任意聚合的技术。随着<a href=\"https://spring.io/projects/spring-data-jdbc\">Spring Data JDBC</a>\" 3.2.0-M2的发布，该团队宣称，这项新技术是“解决N+1问题的开始”。类RelationalMappingContext中新增了一个方法setSingleQueryLoadingEnabled(true)，用于启用单查询加载。目前，该特性只适用于简单聚合（只包含一个聚合根和其他实体的单个集合）。但其团队承诺，<a href=\"https://github.com/spring-projects/spring-data-relational/issues/1445\">未来的版本</a>\"将改进这一限制。</p><p></p><h4>Quarkus</h4><p></p><p>Red Hat<a href=\"https://quarkus.io/blog/quarkus-3-3-1-released/\">发布</a>\"了<a href=\"https://quarkus.io/\">Quarkus</a>\"的3.3.1版本，带来了依赖项升级和一些值得注意的变化，其中包括：修复了MicrometerRecorder类中潜在的NullPointerException；在VertxPoolMetrics类中新增计数器rejected ，用来记录被拒绝的请求数；修复了VertxHttpExporter 类解析/v1/traces 端点有误的问题。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.3.1\">变更日志</a>\"。</p><p>类似的，Quarkus 3.2.5.Final<a href=\"https://quarkus.io/blog/quarkus-3-2-5-final-released/\">也带来了一些值得注意的变化</a>\"，其中包括：修复了当CsrfRequestResponseReactiveFilter类检查媒体类型时，<a href=\"https://quarkus.io/extensions/io.quarkus/quarkus-csrf-reactive\">跨站点请求伪造</a>\"扩展中潜在的NullPointerException；禁用ReactiveMongodbPanacheResourceTest类中的testMoreRepositoryFunctionalities()方法；修复了一个在多个线程中调用bean的写锁定方法时发生死锁的问题，其中该方法调用了同一bean中的另一个写锁定方法。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.2.5.Final\">变更日志</a>\"。</p><p>最后，Quarkus 2.16.10.Final<a href=\"https://quarkus.io/blog/quarkus-2-16-10-final-released/\">发布</a>\"，将依赖项<a href=\"https://github.com/xerial/snappy-java/blob/master/README.md\">Snappy Java</a>\"从版本1.1.8.4升级到版本1.1.10.1。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.16.10.Final\">变更日志</a>\"。</p><p></p><h4>Micronaut</h4><p></p><p>Micronaut基金会<a href=\"https://micronaut.io/2023/09/01/micronaut-framework-4-1-0-released/\">发布</a>\"了<a href=\"https://micronaut.io/\">Micronaut Framework</a>\" 4.1.0版本，带来了<a href=\"https://github.com/micronaut-projects/micronaut-core//releases/tag/v4.1.3\">Micronaut Core 4.1.3</a>\"和一些新特性，其中包括：<a href=\"https://docs.micronaut.io/latest/guide/#beanMappers\">Bean映射器</a>\"自动创建一种类型与另一种类型的映射；新增一个Introspection Builder，如果一个类型只能通过构造器模式来构造，则可以利用@Introspected注解的builder成员来生成一个动态构造器；针对使用<a href=\"https://docs.micronaut.io/latest/guide/#ksp\">Kotlin符号处理</a>\"（KSP）构建Micronaut应用程序做了改进。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/micronaut-projects/micronaut-platform/releases/tag/v4.1.0\">发布说明</a>\"。</p><p>Microaut Frameworkd 4.0.6（<a href=\"https://micronaut.io/2023/08/31/micronaut-framework-4-0-6-released/\">第6个维护版本</a>\"）升级了以下模块：<a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">aut for Spring</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-jaxrs/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-jaxrs/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-jaxrs/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-jaxrs/latest/guide/\">aut JAX-RS</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-servlet/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-servlet/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-servlet/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-servlet/latest/guide/\">aut Servlet</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-validation/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-validation/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-validation/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-validation/latest/guide/\">aut Validation</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-redis/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-redis/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-redis/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-redis/latest/guide/\">aut Redis</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-tracing/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-tracing/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-tracing/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-tracing/latest/guide/\">aut Tracing</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-aws/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-aws/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-aws/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-aws/latest/guide/\">aut AWS</a>\"和<a href=\"https://micronaut-projects.github.io/micronaut-kafka/latest/guide/\">M</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-kafka/latest/guide/\">icro</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-kafka/latest/guide/\">n</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-kafka/latest/guide/\">aut Kafka</a>\"。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/micronaut-projects/micronaut-platform/releases/tag/v4.0.6\">发布说明</a>\"。</p><p></p><h4>WildFly</h4><p></p><p><a href=\"https://www.wildfly.org/\">WildFly</a>\" 29.0.1<a href=\"https://www.wildfly.org/news/2023/08/23/WildFly2901-Released/\">发布</a>\"，带来了组件升级（从Quickstarts 29.x迁移到BOMs and WildFly Server 29.0.1.Final）和一些值得注意的Bug修复，其中包括：新依赖项org.jboss.jts to jdk.jconsole 导致WildFly 29.0.0不能在Eclipse Temurin 17.0.8上启动的问题；将依赖项升级到<a href=\"https://github.com/square/okio/blob/master/README.md\">Square Okio</a>\" 3.4.0，以解决<a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-3635\">CVE-2023-3635</a>\"漏洞（可能导致Okio客户端在通过GzipSource类处理精心制作的GZIP归档文件时发生拒绝服务）；升级到WildFly 29.0.0后与MicroProfile RestClient和Jakarta CDI规范有关的问题。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/wildfly/wildfly/releases/tag/29.0.1.Final\">发布说明</a>\"。</p><p></p><h4>Hibernate</h4><p></p><p><a href=\"https://hibernate.org/orm/\">Hibernate ORM</a>\"的6.3.0和6.2.8版本<a href=\"https://in.relation.to/2023/08/31/orm-630/\">发布</a>\"，带来了一些值得注意的变化，其中包括：初步支持Jakarta EE 11的<a href=\"https://jakarta.ee/specifications/persistence/3.2/\">Jakarta Persistence 3.2</a>\"规范，包括明确了HQL/JPQL查询中的<a href=\"https://hibernate.atlassian.net/browse/HHH-17076\">数值字面量类型</a>\"；新的<a href=\"https://docs.jboss.org/hibernate/orm/6.3/introduction/html_single/Hibernate_Introduction.html\">Hibernate 6入门指南</a>\"；<a href=\"https://docs.jboss.org/hibernate/orm/6.3/introduction/html_single/Hibernate_Query_Language.html\">Hibernate查询语言新语法和新特性指南</a>\"；作为JPA静态元模型生成器的一部分，能够为命名查询生成DAO风格的方法；生成器可以处理任意方法，而且创建出的查找器方法与使用新注解@Find的查询方法类似。</p><p><a href=\"https://hibernate.org/reactive/\">Hibernate Reactive</a>\" 2.0.5.Final<a href=\"https://in.relation.to/2023/09/01/hibernate-reactive-2_0_5_Final/\">发布</a>\"，兼容Hibernate ORM 6.2.8.Final和Vert.SQL driver 4.4.5。值得注意的变化包括：MutinyGenerator类中定义的generate()方法返回类型从Uni更改为Uni<!--?-->；增加了针对@TimeZoneStorage注解的测试；新增ParametersProcessorTest类来修复OracleParameters、PostgresParameters 和SQLServerParameters类处理参数时的转义问题。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/hibernate/hibernate-reactive/releases/tag/2.0.5\">发布说明</a>\"。<p></p><p></p><h4>Eclipse基金会</h4><p></p><p>Eclipse <a href=\"https://glassfish.org/\">GlassFish</a>\" 7.0.8是<a href=\"https://twitter.com/OmniFishEE/status/1696188972630737402\">第8个维护版本</a>\"。该版本初步支持JDK 21，并优化了CDI扩展，减少了非必要的ProcessAnnotatedType接口处理程序调用。它还带来了一些值得注意的Bug修复，包括：RWLockDataStructureTest类中的一个JDK 11兼容性问题；多jar文件兼容性问题；管理员用户将密码改为空时的Admin Console行为。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/eclipse-ee4j/glassfish/releases/tag/7.0.8\">发布说明</a>\"。</p><p>Eclipse <a href=\"https://vertx.io/\">Vert.x</a>\" 4.4.5<a href=\"https://vertx.io/blog/eclipse-vert-x-4-4-5/\">发布</a>\"，带来了依赖项升级和一些值得注意的变化，其中包括：改进ForwardedParser类，支持未用方括号括起来的IPV6地址；在实现WebSocketBase接口时，将帧聚合器与帧处理程序解耦；修复了HTTP/2在超时时抛出HttpClosedException而不是TimeoutException的问题。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/vert-x3/wiki/wiki/4.4.5-Release-Notes\">发布说明</a>\"和<a href=\"https://github.com/vert-x3/wiki/wiki/4.4.5-Deprecations-and-breaking-changes\">弃用及破坏性变更清单</a>\"。</p><p>Eclipse <a href=\"https://github.com/eclipse/jkube/blob/master/README.md\">JKube</a>\" 1.14.0<a href=\"https://blog.marcnuri.com/eclipse-jkube-1-14\">发布</a>\"（一个用于Kubernetes和OpenShift的Java工具和插件实用程序），带来了Bug修复、改进，并支持Gradle 8、Helidon、Spring Boot <a href=\"https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/maven-plugin/reference/html/#repackage-layers\">Layered Jar</a>\"和向OCI注册中心推送<a href=\"https://helm.sh/\">Helm</a>\" chart。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/eclipse/jkube/releases/tag/v1.14.0\">发布说明</a>\"。</p><p></p><h4>JReleaser</h4><p></p><p><a href=\"https://jreleaser.org/\">JR</a>\"<a href=\"https://jreleaser.org/\">elease</a>\"<a href=\"https://jreleaser.org/\">r</a>\" 1.8.0版本<a href=\"https://andresalmiray.com/jreleaser-1-8-0-has-been-released/\">发布</a>\"（一个简化项目发布的Java实用程序），提供了文档改进、依赖项升级和一些值得注意的变化，其中包括：为缺失的announcer创建默认模板；升级到最新的SDKMan端点；改进GitHub 422错误响应的错误处理。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/jreleaser/jreleaser/releases/tag/v1.8.0\">发布说明</a>\"。</p><p>值得注意的是，SLSA（<a href=\"https://slsa.dev/\">Supply Chain Levels for Software Artifacts</a>\"）已经<a href=\"https://slsa.dev/blog/2023/08/bring-your-own-builder-github\">宣布</a>\"与JReleaser、Maven和Gradle的贡献者合作<a href=\"https://slsa.dev/blog/2023/04/slsa-v1-final\">发布</a>\"SLSA 1.0。这验证了GitHub Actions BYOB（<a href=\"https://github.com/slsa-framework/slsa-github-generator/tree/main#build-your-own-builder\">Build Your Own Builder</a>\"）框架的设计，并证明了它的灵活性。</p><p></p><h4>OpenXava</h4><p></p><p><a href=\"https://openxava.org/\">OpenXava</a>\" 7.1.5<a href=\"https://openxava.org/blog/openxava-7.1.5-released\">发布</a>\"，带来了一些显著的变化，其中包括：新增CompositeFilter类，用于在 Tab.setFilter()中同时使用两个IFilter，也可作为 @Tab注解的IFilter；新增环境变量XAVA_CALENDAR_VIEWEVENT_ACTION，用于定义日历单击事件的动作；修复在 @Editor(\"ValidValuesRadioButton\")中使用枚举时会在日志中产生IndexOutOfBoundsException异常的问题。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/openxava/openxava/releases/tag/7.1.5\">发布说明</a>\"。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/09/java-news-roundup-aug28-2023/\">https://www.infoq.com/news/2023/09/java-news-roundup-aug28-2023/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/3EO246XRyUiAlTfku9mv\">Java 近期新闻：JDK 21 RC1、Apache Camel 4.0、Payara Platform、Apache Tomcat、Micronaut</a>\"</p><p><a href=\"https://www.infoq.cn/article/ClW8eLeOxRUqqpHWOJCC\">Java ZGC 垃圾收集器全面增强</a>\"</p><p></p></p>",
    "publish_time": "2023-09-12 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Ruby on Rails的创始人将TypeScript从Turbo框架中移除，引起社区不满",
    "url": "https://www.infoq.cn/article/N8QIk2frWWIJbHCMrA0W",
    "summary": "<p>Ruby on Rails 的创建者 David Heinemeier Hansson（DHH） 从即将发布的 Turbo 框架第 8 版中删除了 TypeScript，并声称从未是它的粉丝。许多 Turbo 用户抗议说决定太仓促，不欢迎这种变化。</p><p>&nbsp;</p><p>在移除 TypeScript 的 GitHub <a href=\"https://github.com/hotwired/turbo/pull/971\">pull request</a>\" 上有一条评论认为，这个举措“对于库的用户和贡献者都是一种倒退”。截止目前，这条评论已经有357个赞，仅8个踩，显示了广泛的支持。</p><p>&nbsp;</p><p><a href=\"https://turbo.hotwired.dev/\">Turbo</a>\" 是一个用于传递 HTML 页面的框架，旨在“显著减少自定义 JavaScript 的数量”，并由 Hannson 的公司 37signals 赞助，其产品包括 Basecamp 项目管理平台和 Hey 消息系统。Turbo 是 Hotwire 的引擎，Hotwire 是“HTML over the wire”的缩写，因为它更喜欢发送 HTML 本身而不是 JSON 数据和 JavaScript 代码。</p><p>&nbsp;</p><p>尽管 Turbo 并不属于那批最受欢迎的框架，但 Ruby on Rails 很有名，像 GitHub 和 Shopify 这样的主要网站都在使用它。</p><p>&nbsp;</p><p>Hansson <a href=\"https://world.hey.com/dhh/turbo-8-is-dropping-typescript-70165c01\">发文</a>\"称 TypeScript “通过添加微不足道的类型技巧，让我的开发体验变得更加糟糕，而且频繁引发很多困扰。本应简单的事情反而变得很困难。”</p><p>围绕着 Turbo 开源项目的社区大多感到困惑和失望，不仅是因为变更本身，还因为变更的方式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c3/c305bb08962171e1598bd37ec9bf28a2.png\" /></p><p></p><p>\"回到 JS 意味着许多 Hotwire 生态系统的包都会受到影响。当前的所有开放 PR 都已完全过时。从我的角度来看，其中一些是非常好的候选项。IDE 不再提供与以前一样的自动补全功能，\" <a href=\"https://github.com/hotwired/turbo/pull/971#issuecomment-1708302536\">一位用户表示</a>\"。</p><p>&nbsp;</p><p>另一位用户<a href=\"https://github.com/hotwired/turbo/pull/971#pullrequestreview-1613489798\">抱怨</a>\"说：“匆忙进行这个重要的更改，忽视了所有（我是说所有）的 PR 评论...这会开一个坏头。Ruby on Rails 也会像这样来开发吗？取决于一个人的心血来潮？”</p><p>&nbsp;</p><p>Hansson <a href=\"https://github.com/hotwired/turbo/pull/971#issuecomment-1708430006\">回应</a>\"道：“非常感谢那些更喜欢 TypeScript 的贡献者。这只是争论之一，其中的论点不太可能改变任何人的根本立场，所以我不会尝试这样做。”</p><p>&nbsp;</p><p>他补充说：“现在，我们在 37signals 写的所有客户端代码都是纯 JavaScript，内部库也是如此。这次变更意味着保持一致。”</p><p>&nbsp;</p><p>微软的 Anders Hejlsberg 出于他的信念发明了 TypeScript，即如果使用强类型语言编写复杂应用程序，它们将更加健壮且易于维护。TypeScript 在编程社区的普及，表明了许多人持相同观点，而且一些来自 TypeScript 的概念，包括类型注解，也正在逐渐融入 ECMAScript，即 JavaScript 官方标准。无论开发者的选择如何，TypeScript 都会编译成 JavaScript，最终在浏览器或 Node.js 等环境中执行。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://devclass.com/2023/09/07/ruby-on-rails-creator-removes-typescript-from-turbo-framework-upsets-community/?td=rt-3a\">https://devclass.com/2023/09/07/ruby-on-rails-creator-removes-typescript-from-turbo-framework-upsets-community/?td=rt-3a</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://www.infoq.cn/article/3le2VX8uRPBOllXeoTKz\">重磅！OpenAI 开放 GPT-3.5 Turbo 微调，网友：将 prompt 减少 90% 才实惠&nbsp;</a>\"</p><p><a href=\"https://xie.infoq.cn/article/6ff79700fb3bfa972c1beebf3\">TypeScript 与 JavaScript：你应该知道的区别</a>\"</p><p><a href=\"https://www.infoq.cn/article/dDXbcLHT7teNYSPL3sm7\">“TypeScript 不值得！”前端框架 Svelte 作者宣布重构代码，反向迁移到 JavaScript 引争议</a>\"</p><p><a href=\"https://xie.infoq.cn/article/b7f556a866805cf5c71be7af8\">Typescript- 类型检测和变量的定义</a>\"</p>",
    "publish_time": "2023-09-12 09:46:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Slack蜂窝架构迁移：背后的技术策略与挑战",
    "url": "https://www.infoq.cn/article/JFv6Qoc3Nfe2Z1KblgA2",
    "summary": "<p>近年来，蜂窝架构（Cell-Based Architecture）作为一种增加冗余和有效限制站点故障影响范围的方式，在大型的在线服务中越来越流行。为了实现这些目标，在过去的一年半里，我们将Slack最关键的面向用户的服务从单体架构迁移到了基于蜂窝的架构。在本系列文章中，我们将解释我们为什么要进行大规模迁移、介绍蜂窝拓扑设计以及我们在此过程中所做出的工程技术权衡，并讨论我们成功对许多相连接的服务进行深度改造所采用的策略。</p><p>&nbsp;</p><p></p><h1>背景说明：事故</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/325a2c99f5ee2777c35de46b84761a47.png\" /></p><p></p><p>2021年6月30日事故中的TCP重传图表</p><p>&nbsp;</p><p>在Slack，我们会在每一次发生明显的服务中断后进行一次事故评审。以下是我们内部事故评审报告的一些摘录，总结了其中的一起事故和我们的发现：</p><p>&nbsp;</p><p></p><blockquote>太平洋时间20121年6月30日上午11点45分，我们的云供应商在美国东海岸的一个可用区域发生网络中断，Slack的大部分服务都托管在那里。连接一个可用区域和其他几个包含Slack服务器的可用区域的网络链路发生了间歇性故障，导致Slack服务器之间的连接变慢，进而出现服务降级。&nbsp;太平洋时间2021年6月30日下午12点33分，我们的云供应商自动从服务中删除了网络链接，恢复了对Slack客户的全部服务。经过他们的一系列自动检查之后，网络链接再次进入服务状态。&nbsp;太平洋时间20121年6月30日下午5点22分，网络链路又发生了同样的间歇性故障。下午5点31分，云供应商永久地从服务中删除了网络链接，恢复了对我们全部的服务。</blockquote><p></p><p>&nbsp;</p><p>乍一看，这似乎没什么大不了。我们的服务所使用的一块物理硬件发生了故障，因此出现了一些错误，直到发生故障的硬件被移除。然而，在进行事故评审时，我们不禁问自己：让我们的用户体验到这样的中断是合理的吗？</p><p>&nbsp;</p><p>Slack运营着一个全球性的、多区域的边缘网络，但我们的大多数核心计算基础设施都位于单个地区（us-east-1）的多个可用区域内。可用区域（AZ）是指单个区域内的隔离的数据中心，它们除了可以提供物理隔离之外，也会限制我们所依赖的云服务组件（虚拟化、存储、网络等）的故障影响范围，这样就不会在多个AZ中同时发生故障。在云端托管服务的构建者（如Slack）可以通过这样的一种方式来构建服务——在一个区域内整个服务的可用性高于任何一个AZ的可用性。那么问题来了：为什么这个策略在6月30日没有奏效？为什么一个AZ发生故障会让用户体验到中断？</p><p>&nbsp;</p><p>事实证明，在分布式系统中检测故障是一个难题。来自用户的一个针对Slack API的请求（例如，在一个频道中加载消息）可能会扇出数百个发给后端服务的RPC，每个RPC都必须完成调用才能向用户返回正确的响应。我们的服务前端不断尝试检测和排除发生故障的后端，但在能够排除发生故障的服务器之前必须先将故障记录下来！让事情变得难上加难的是，我们的一些关键数据存储（包括我们的主数据存储Vitess）提供了高度一致的语义，这对应用程序开发人员来说非常有用，但也要求任何写入都要有可用的后端。如果一个分片主节点对应用程序前端不可用，那么到该分片的写入将会失败，直到主分片返回错误或一个次分片被提升为主分片。</p><p>&nbsp;</p><p>我们可以将上述的中断归类为灰色故障。在发生灰色故障时，系统可用性对于不同的组件来说是不一样的。在我们的事故中，受影响AZ内的系统看到其AZ内的后端完全可用，但AZ外的后端不可用。反过来，未受影响AZ内的系统看到受影响的AZ是不可用的。即使是同一个受影响的AZ内的客户端能够看到的后端可用性也是不一样的，这取决于它们的网络流量是否碰巧流经发生故障的设备。这就好比要求分布式系统在为我们的客户处理消息和提供小动物动图的同时处理好一切故障，这是一个相当复杂的任务。</p><p>&nbsp;</p><p>我们对这个难题的解决方案并不是试图去自动修复灰色故障，而是通过利用人类的判断力来让计算机的工作变得更容易。工程师们很清楚，在发生宕机时，主要的问题在于一个AZ不可达——我们汇总的目标AZ的每一张图表都与上面的重传图很相似。如果我们有一个可以告诉所有系统“这个AZ已坏，请避开它”的按钮，我们肯定会把它盘得发亮！因此，我们开始着手构建这样的按钮，可以将流量从AZ中抽走。</p><p>&nbsp;</p><p></p><h1>我们的解决方案：AZ就是会被引流的蜂窝单元</h1><p></p><p>&nbsp;</p><p>与其他许多基础设施一样，AZ引流按钮在概念上很简单，但在实践中却很复杂。我们的设计目标是：</p><p>&nbsp;</p><p>尽可能在5分钟内减少AZ内的流量。Slack的99.99%可用性SLA只允许我们每年不到1小时的总体不可用，因此，为了有效地保持这种可用性，我们需要能够快速奏效的工具。引流不能导致对用户可见的错误。引流是一种通用的缓解措施：只要故障包含在单个AZ内，即使尚未清楚导致故障的根本原因是什么，也可以有效地使用引流来缓解故障。这是一种实验性的方法，在发生故障期间，运维人员可以尝试对AZ进行引流，看看故障是否能够恢复，如果不能，则进行放流。如果引流会导致额外的错误，那么这种方法就没有用了。引流和放流必须是增量的。在放流时，运维人员将流量的1%分配给AZ，看看它是否已经恢复。引流机制不能依赖于被引流的AZ内的资源。例如，只是通过向每台服务器发送SSH命令并强制其关闭健康检查机制来激活引流是不行的。这是为了确保即使AZ完全离线也可以进行引流。</p><p>&nbsp;</p><p>一种符合这些需求的简单实现是向每个RPC客户端发送一个信号，当客户端接收到这个信号时，它们会让流向特定AZ的一定百分比的流量失败。这个方案隐含了很多复杂性。Slack没有共享代码库，甚至没有共享运行时，处理用户请求的服务使用多种语言编写，如Hack、Go、Java和C++，这就需要为每一种语言单独实现客户端。除此之外，我们还支持许多内部服务发现接口，包括Envoy xDS API、Consul API，甚至DNS。况且，DNS没有为AZ或部分引流等机制提供抽象，客户端只希望能够解析DNS地址并接收IP列表。最后，我们在很大程度上依赖了开源系统，如Vitess，要修改代码，就需要在内部维护分支，并做一些额外的工作，将变更合并到上游，这并不是一份令人愉悦的差事。</p><p>&nbsp;</p><p>我们采用的主要策略叫作“筒仓（Siloing）”。如果服务只从其所在的AZ内接收流量，并且只向该AZ内的服务器发送流量，那么这个服务就可以被称为一个筒仓。从整体上看，这种架构的效果是：每个服务似乎都有N个虚拟服务，每个AZ中有一个。重要的是，我们可以通过对某个AZ内的用户请求进行重定向来有效地将AZ内所有筒仓服务的流量抽走。如果没有来自用户的新请求到达筒仓所在的AZ，那么这个AZ的内部服务自然会停止，因为它们没有新的工作要做。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/96b91fa8de07ddea5d77b5e442564d83.png\" /></p><p></p><p>我们最初的架构，后端分布在各个AZ中，因此错误会出现在所有AZ的前端</p><p>&nbsp;</p><p>最终，我们得到了一个蜂窝架构。所有服务都存在于所有AZ中，但每个服务只与其AZ内的服务通信。一个AZ内的系统故障被包含在该AZ内，我们可以动态路由流量以避开这些故障，只需在前端重定向即可。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/86ec2b2988618e82625195d13b5d599f.png\" /></p><p></p><p>筒仓架构，一个AZ中的故障被包含在该AZ中，流量可能被抽走</p><p>&nbsp;</p><p>有了筒仓架构，我们就可以将精力集中在一个地方来实现流量转换：将来自用户的查询路由到us-east-1区域的核心服务。在过去的几年里，我们进行了大量投入，从HAProxy迁移到了Envoy/xDS生态系统，因此我们所有的边缘负载均衡器现在都在运行Envoy，并从我们内部的xDS控制平面Rotor接收配置。因此，我们能够通过简单地使用两个开箱即用的Envoy功能来进行AZ引流：加权集群和基于RTDS的动态权重分配。在对一个AZ进行引流时，我们只需通过Rotor向边缘Envoy负载均衡器发送一个信号，指示它们在us-east-1重新分配每个AZ的目标集群权重。如果us-east-1某个AZ的权重变为零，Envoy将继续处理请求，但会将所有新请求分配给另一个AZ，也就完成了对这个AZ的引流。我们来看看这是如何满足我们的目标的：</p><p>&nbsp;</p><p>通过控制平面的传播为秒级，Envoy负载均衡器将立即应用新的权重。引流不会丢失请求，负载均衡层不会放弃任何一个查询。权重提供粒度为1%的增量式引流。边缘负载均衡器完全位于不同的区域，控制平面有区域副本，可以抵御任意的单AZ故障。</p><p>&nbsp;</p><p>下图是我们逐步将流量从一个AZ引到另外两个AZ时每个AZ的带宽情况。注意图中的“膝盖”部分，它反映了Envoy/xDS实现为我们提供的快速、大粒度的传播保证。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bd7b81effa8e644b6a259149d7815a99.png\" /></p><p></p><p>单AZ的每秒查询走势</p><p>&nbsp;&nbsp;</p><p>原文链接：</p><p></p><p><a href=\"https://slack.engineering/slacks-migration-to-a-cellular-architecture/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM1NDUwNDgsImZpbGVHVUlEIjoiZXpGV0loMmZYNkZWNDZVOCIsImlhdCI6MTY5MzU0NDc0OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.E_FTADwFvxpEVro_6zuU3qcVnPYv5Uk-2h8-T6hWslk\">https://slack.engineering/slacks-migration-to-a-cellular-architecture/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://www.infoq.cn/article/LAYRE8jxE2z42QuU5pNX\">自定义跟踪架构：Slack&nbsp;高效解决通知问题</a>\"</p><p><a href=\"https://www.infoq.cn/article/9lp0pYfij3bUxmHDXkxl\">Slack实时消息处理架构，更新、更快、更稳定</a>\"</p><p><a href=\"https://www.infoq.cn/article/hhh8OGLNbsz121H43Df4\">Slack工程师如何解决最常见的移动开发痛点</a>\"</p><p><a href=\"https://www.infoq.cn/article/Pwkm3Ro1IqSzM3wLXvMS\">Zoom和Slack的第二曲线</a>\"</p>",
    "publish_time": "2023-09-12 09:49:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "隐私与安全同行：探索阿里云可信云架构与机密计算",
    "url": "https://www.infoq.cn/article/A1rDsG3luPU1eaRLcmO7",
    "summary": "<p>《英特尔®️ 至强®️ 实战课》新一期内容即将上线，大招来袭！本期以“大模型时代的云服务安全利器”为主题，邀请 InfoQ 极客传媒主编赵钰莹、英特尔首席工程师宋川、阿里云机密计算安全专家于国瑞、阿里云高级安全专家刘煜堃四位业内大咖，从基本原理、技术框架、场景案例等方面，详细阐述英特尔®️ 至强®️ 处理器机密计算在人工智能数据安全和隐私保护方面的应用及实践。通过本期课程，您将了解到当前云服务安全领域的前沿趋势和解决方案，并获得更专业的实战经验！</p>",
    "publish_time": "2023-09-12 11:11:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《英特尔®️至强®️实战课》机密计算赋能数据为中心的云安全",
    "url": "https://www.infoq.cn/article/P2rM88BBFvQMxSLxxhUO",
    "summary": "<p>《英特尔®️ 至强®️ 实战课》新一期内容即将上线，大招来袭！本期以“大模型时代的云服务安全利器”为主题，邀请 InfoQ 极客传媒主编赵钰莹、英特尔首席工程师宋川、阿里云机密计算安全专家于国瑞、阿里云高级安全专家刘煜堃四位业内大咖，从基本原理、技术框架、场景案例等方面，详细阐述英特尔®️ 至强®️ 处理器机密计算在人工智能数据安全和隐私保护方面的应用及实践。通过本期课程，您将了解到当前云服务安全领域的前沿趋势和解决方案，并获得更专业的实战经验！</p>",
    "publish_time": "2023-09-12 11:13:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯云商业银行总经理曹骏确认出席 FCon ，分享商业银行核心系统国产化策略与实战",
    "url": "https://www.infoq.cn/article/uW0HmUR1tx8Z02iZXm3c",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。腾讯云商业银行总经理曹骏将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5526?utm_source=infoqweb&amp;utm_medium=article\">商业银行核心系统国产化策略与实战</a>\"》主题分享，介绍在国产化的要求下如何通过云原生的思路来进行银行核心系统改造，为金融机构的 IT 从业者和为金融机构提供服务的 IT 从业者提供一些新的思路，以及对腾讯云近年来进行核心系统国产化实践过程中的经验和教训进行分析总结。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5526?utm_source=infoqweb&amp;utm_medium=article\">曹骏</a>\"，负责腾讯云在银行行业解决方案。曹骏先生多年服务于金融行业客户，是业内资深金融行业技术专家与云计算技术专家，具有超过 20 年的 IT 咨询服务和项目实施经验，为多家银行、保险、互联网金融机构提供 IT 战略规划、云计算等咨询规划与落地实施服务。 加入腾讯云之前，他曾任 IBM 中国资深架构师、微软全球服务部中国区金融行业首席架构师等职位。他在本次会议的演讲内容如下：</p><p></p><p>演讲：商业银行核心系统国产化策略与实战</p><p></p><p>商业银行的业务系统建设一直是电子化、数字化最为成熟且全面的行业之一。银行核心系统一直作为 IT 建设最为关键应用支撑着整个银行的商业运转，其业务系统的高性能、稳定性和安全防范以及系统数据的产生、传输、存储是 IT 行业者非常关注的方向。</p><p></p><p>本话题将结合腾讯云多年来的技术积累与项目实践，探讨在国产化的要求下如何通过云原生的思路来进行银行核心系统改造，为金融机构的 IT 从业者和为金融机构提供服务的 IT 从业者提供一些新的思路。也会对腾讯云近年来进行核心系统国产化实践过程中的经验和教训进行分析总结。</p><p></p><p>演讲提纲：</p><p></p><p>银行业核心系统的挑战与发展路径银行核心系统国产化过程中的挑战与应对云原生的银行核心系统总体架构与关键技术腾讯云的核心国产化落地实践</p><p></p><p>你将获得：</p><p></p><p>○ 了解银行核心系统的国产化技术方案和方案要点</p><p>○ 了解银行行业如何落地云原生、国产化的技术架构</p><p>○ 通过实际案例的分析获取更多技术落地的实践经验</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-09-12 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "人红是非多！Rust社区冲突不断，创始人：别Call我了，我也救不了！",
    "url": "https://www.infoq.cn/article/6TzaQ6wNB9vd8SJ3RCet",
    "summary": "<p></p><blockquote>Rust为什么会有这么多管理上的问题？如果Rust采用由创始人治理的方式，会不会更好？实际上，Rust的创造者Graydon Hoare曾回应过这个问题，他认为如果是由他来治理的话，事情肯定会很不一样，但是Rust就不太可能像现在这样“出圈”。</blockquote><p></p><p>&nbsp;</p><p></p><h2>Rust团队冲突不断</h2><p></p><p>&nbsp;</p><p>前几天，作为Rust 发布团队（Release team）的一员，Jonas Schievink要求Rust团队从项目中删除掉和他有关的所有文件。</p><p>&nbsp;</p><p>“请求将我从‘校友’中删除，并删除和我的用户名绑定在一起的文件”，“我还想请求 Rust 团队从项目的commits中删除我所有作者信息。”</p><p>&nbsp;</p><p>“我不想再以任何身份参与 Rust 项目。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a617983877cee47ef08c2d5f10d3a20e.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Jonas Schievink还吐槽了Rust领导团队，认为这些人“破坏了社区项目，并压制了公众讨论”。</p><p>&nbsp;</p><p>Rust团队一直风波不断。此前，还发生过为了抗议 Rust 核心团队（Core team），<a href=\"https://www.infoq.cn/article/798bgzaO1ujBgzQsAGHC\">审核团队集体辞职</a>\"的事情。他们认为 Rust 核心团队在执行社区行为准则和标准上让自己不受制约。Rust 核心团队并没有和其他成员遵循同样的行为准则 (CoC)，Coc 似乎变成了核心团队 “严于律人” 的工具。</p><p>&nbsp;</p><p>今年5月，Rust领导小组粗暴撤换RustConf主题演讲人，事态升级后<a href=\"https://www.infoq.cn/article/3vsBhtLeXU3QzBDuJ7AZ\">引发多人出走</a>\"。</p><p>&nbsp;</p><p>今年6月，在经历了多次治理风波后，Rust 项目宣布成立新的顶级治理机构：领导委员会（Rust Leadership Council）。由Rust 各团队成员合力创建一份新的、名为 “ Rust 领导理事会” 的 RFC 草案，并确立了以下内容：移除 Rust 核心团队，由各团队出一个代表，成立一个顶级的治理团队“领导委员会”。&nbsp;</p><p>&nbsp;</p><p>“领导委员会” 负责一些职责不清的工作安排及其优先次序，然后对这些工作进行精确到子团队或成员的委托。另外，“领导委员会” 还要以跨团队工作、规划和项目的长期成功等为目标，成为团队之间的协调、组织和问责机构。领导委员会还需要协调因项目而导致的团队、结构或流程的变化，确保顶层团队负起责任，并负责展示 Rust 项目的官方态度。</p><p>&nbsp;</p><p>可能“ Rust 领导理事会”还是没有解决好当前的各种乱象，所以Jonas Schievink又站了出来：“对最近 RustConf 主题演讲的怯懦处理只是最近的一个例子，而且它也不太可能是最后一个。即使领导结构发生了变化。永久解决这些问题的唯一方法是从 Rust 项目中完全驱逐那些对这些问题负责的人，或者为这些问题辩护的人。”</p><p>&nbsp;</p><p>Rust为什么会有这么多管理上的问题？如果Rust采用由创始人治理方式，是不是更好？实际上，Rust的创造者Graydon Hoare曾从侧面回应过这个问题，他认为如果是由他来治理的话，方向肯定会很不一样，但是Rust就不太可能像现在这样“出圈”。</p><p>&nbsp;</p><p>Rust最早诞生于2006年，刚开始只是Hoare的个人开发项目。但在发展过程中，Rust吸引到更多贡献者，并于2009年正式获得Mozilla的官方赞助。</p><p>&nbsp;</p><p></p><h2>Hoare表示自己也无法处理好各种冲突</h2><p></p><p>&nbsp;</p><p>Hoare在他的个人博客可以说是无所不聊。2023年他撰写了四篇文章，第一篇谈的是业余无线电技术，第二篇则是企业雇用的维护人员往往对于开源贡献没什么热情（他认为雇主应该引导这些「维护人员成为真正的维护者」）。</p><p>&nbsp;</p><p>然后，Hoare连发两篇博文，对Rust语言的演变进行了快速梳理。</p><p>&nbsp;</p><p>今年5月底，Graydon Hoare在自己的博客上回顾了Rust诞生历程。Hoare首先提醒读者，“我已经有十年没参与这个项目了”，所以“大家对我的一切言论都请保持谨慎态度，单纯把我看作一位曾经在重要阶段参与过Rust发展的当事人就好……”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d98923ca82fdc32c2104f970332a2b8e.jpeg\" /></p><p></p><p>&nbsp;</p><p>有趣的是，6月份发布的第二篇文章题为《我理想中的Rust不会有未来》（The Rust I Wanted Had No Future，<a href=\"https://graydon2.dreamwidth.org/307291.html\">https://graydon2.dreamwidth.org/307291.html</a>\"）。</p><p>&nbsp;</p><p>首先，Hoare提起最近人们执的问题，“你有没有想过在Rust项目中BDFL（终身扮演仁慈的独裁者？）”而如果他真的这样做了，Rust项目的发展会不会更加顺遂？BDFL是授予少数开源软件开发领导者的头衔，通常是在社区内的争议或争论中保留最终决定权的项目创始人。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ba/baf8b0e3ab3202b6cbbb11614947d62e.jpeg\" /></p><p></p><p>&nbsp;</p><p>Hoare首先给出了明确的回复，“不会。”他进一步补充道，“我不喜欢受到关注、也不喜欢公众压力。在2009年到2013年担任项目的技术主管时，我就已经快到极限了……另外，我觉得自己没办法建立起强大或者健康的团队制度，处理不好决策、冲突、授权和扩展之类的具体工作。”</p><p>&nbsp;</p><p>后来这篇文章被发到了Reddit上的Rust子论坛中，Hoare也经常在这里转悠。有位用户询问Rust最近的项目开发是否有所放缓，Hoare回应称“就主要功能来说，开发速度的适当放缓是有好处的。”</p><p>&nbsp;</p><p>而在他那篇文章的评论区中，Hoare本人表示“千万别让我聊类型参数里的尖括号和生命周期里的单引号！”</p><p>&nbsp;</p><p>有位Reddit用户倒是坚持跟进，而Hoare澄清说“我们曾经就这些语法问题展开过争论，但最后我失败了。”他甚至公开了一个指向“Rust prehistory”GitHub&nbsp;repo的链接，其中存放着13年前的Rust代码。可以看到，Hore当初是想在类型参数中使用方括号的，他补充说“我个人一直觉得，类型参数就应该使用方括号，根本不需要争论。”</p><p>&nbsp;</p><p>Hoare还反对在引用中显式使用生命周期，在他看来“生命周期几乎肯定可以推断出来，所以无论具体使用哪种语法，都没必要让开发者单独编写。但很明显，Rust最后没有顺着这个路子走。”Hoare后来在Reddit评论中感叹道，“终有一天，我可能会写篇〈我心目中的真正Rust〉的博文，告诉大家我当初想象中的Rust和如今真实的Rust间其实有着巨大差异。但请别误会，尽管大有不同，但我对Rust语言获得的成功仍然抱有巨大的成就感和满足感！”</p><p>&nbsp;</p><p></p><h2>希望Rust变更好</h2><p></p><p>&nbsp;</p><p>Hoare认为偏好差异的确真实存在，“我自己的偏好就比较特殊，可能跟大多数朋友有所不同。”</p><p>他强调他心目中的Rust“可能会让所有参与者都不满意，也没办法像现在这样真正破圈……”</p><p>&nbsp;</p><p>“请别误会我的意思：我对现在的结果非常满意。我很高兴行业中有了一种可行的C++替代方案，它给人们提供一种新的范式、一种可供日常使用的合理选项。我也在用Rust，也很高兴能有它来替代C++。但是……”</p><p>&nbsp;</p><p>在文中，Hoare也列出了“Rust中那些我特别不认可且/或目前不太喜欢的地方。”比方说，在文中“复杂的语法”这部分，Hoare就抱怨说Rust仍然难于解析。“它虽然比C++更易用，但跟C++比较本身就说明它的易用性不足。当初我也努力过，但从类型参数里的尖括号到模式绑定的歧义、再到分号和大括号的使用规则，我几乎在每个具体问题上都失败了……我现在甚至不想再谈这个话题，总之现在的语法跟我的设想相去甚远。抱歉了各位。”</p><p>&nbsp;</p><p>另一个例子，则是Rust处理类型的方式。Hoare本人更偏向“结构”类型（即只要各对象的结构相同，则其类型就相互兼容——不受声明时所使用的类型名称的影响）。Hoare还透露，“Rust语言最初带有（我也希望它能再次拥有）编译器发出的「类型描述符」，用户可以在其上调用反射算符。”</p><p>&nbsp;</p><p>Hoare对于Rust如何处理十进制浮点数也有不少想法。“基本上，每种语言都意识到金融数学有其特殊性，并最终添加了小数类型。我希望Rust能提前完成这项工作，但最终还是被放进了库里。虽然选项不少，但我觉得能内置的话也许更好……”</p><p>&nbsp;</p><p>还有更多例子，但Hoare倒是没有列举他的构想跟现在真实Rust之间的差异。相反，“重要的是表达当时在各个设计主题上的分歧。”</p><p>&nbsp;</p><p>Hoare写道，“我在研究这门语言时考虑的各种优先事项，基本上跟围绕该语言发展出来的社区所认定的优先事项出现了巨大偏差。甚至经过多年发展，我关注的那些问题还是没有得到重视。”</p><p>&nbsp;</p><p>“我会为了简单性而牺牲性能和表达力——也就是更强调帮助最终用户减轻认知负荷、在编译器中降低实现难度。我觉得这才是Rust的正确发展方向，但事实证明这似乎跟大多数人对于Rust的预期截然相反。”</p><p>&nbsp;</p><p>Hoare甚至给出了不少细节：</p><p>“Rust社区中的很多人认为「零成本抽象」是Rust语言的核心承诺。我永远不会这么讲，而且我个人觉得这种机制本身就有问题。这是种典型的C++思路，对设计空间造成了不必要的限制……换作是我，会更倾向用大量相对较小的恒定性能成本，来替代包含大量抽象的所谓更简单/更强大的版本，哪怕语言的实际性能会变得更慢。”“同样的，我也会牺牲掉一部分表达力。但这可能会让很多现代Rust程序员感觉不爽，认为Rust项目既笨拙又充满官僚气息、像是一种保姆式语言，根本不允许用户在库代码中编写各类功能，甚至不信任程序员用变量隐藏、环境捕捉或内联函数等简单结构。”</p><p>&nbsp;</p><p>大家可以想见，这篇博文在登陆Reddit之后，很快引起了各种各样的讨论。一位用户明确表示：“我真希望能拥有Hoare设想中的Rust，那听起来很美。”</p><p>&nbsp;</p><p>但另一位评论者似乎更加务实更改，认为“他的Rust不会更好，只是跟现状不同……”</p><p>&nbsp;</p><p>“我倒是更喜欢如今的Rust……我喜欢性能更高而且脚踏实地的代码，而真实的Rust恰好给了我这些。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://twitter.com/sheevink\">https://twitter.com/sheevink</a>\"</p><p><a href=\"https://graydon2.dreamwidth.org/307105.html\">https://graydon2.dreamwidth.org/307105.html</a>\"</p><p><a href=\"https://graydon2.dreamwidth.org/307291.html\">https://graydon2.dreamwidth.org/307291.html</a>\"</p><p><a href=\"https://github.com/oxidecomputer/oxide-and-friends/blob/master/2023_05_30.md\">https://github.com/oxidecomputer/oxide-and-friends/blob/master/2023_05_30.md</a>\"</p><p><a href=\"https://thenewstack.io/graydon-hoare-remembers-the-early-days-of-rust/\">https://thenewstack.io/graydon-hoare-remembers-the-early-days-of-rust/</a>\"</p>",
    "publish_time": "2023-09-12 15:00:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从计算到智算，如何降低 AI 算力使用门槛？ | InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/2RLlk4XePkSXUNU4eQIw",
    "summary": "<p>随着生成式 AI 技术得到广泛应用，算力产业正从“计算”迈向“智算”，这一变化给算力产业带来哪些挑战？不同应用场景对算力芯片的运算能力有何需求？如何降低 AI 算力使用门槛？本期，我们邀请到了大禹智芯产品及解决方案负责人余曦老师，为大家分享《从计算到智算，如何降低 AI 算力使用门槛？》。</p>",
    "publish_time": "2023-09-12 15:08:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "深度解读字节跳动的画质评估工具：抖音也在用~",
    "url": "https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT",
    "summary": "<p>本文从抖音集团内部画质评估体系的建设历程着笔，主要分享画质评测对于业务的重要性、主要应用场景和内部产品的一些典型实践案例，希望通过分享业务视角遇到的一些问题和解决思路，能够为遇到类似困扰的伙伴提供有价值的参考。</p><p></p><h2>一、画质评估体系建设历程</h2><p></p><p></p><h3>（一）为何评测画质如此重要？</h3><p></p><p></p><p>我们通过线上业务大量实验发现，图片画质优劣对点击率、&nbsp;停留时长等消费类指标有正相关影响，间接影响用户收益指标。因此，建设一套行之有效的画质评估体系，保障用户的画质体验是非常有必要性的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/93d84ba57ceb929a4647ec913efa78bb.png\" /></p><p></p><p>直观来讲，画质提升能够为带来更好的观感体验，但 QoE 综合体验也需要考虑其他方面如用户设备、网络状况、观看环境等多方面因素，不计成本地提升画质是否能持续为用户带来 QoE 的收益需要在业务场景中通过严谨的实验方案来验证效果的。</p><p></p><p>在低质图像打压和基于画质的推荐优化等多项业务中的数据分析积累沉淀，我们获取画质评分与用户主观体验之间的明确关系，数据统计显示用户对不同画质内容的敏感程度有着不同趋势，在中档画质分区间持续提升画质，用户的 QoE 体验也会显著提升，但当画质低于或者高于某个阈值时，用户对于画质将变得不再敏感，提升/降低画质对用户的影响均会降低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/94a1187dfc0e3a9f89ea5c68ec6557e1.png\" /></p><p></p><p>期望中的画质甜点关系，中段区间的画质提升会持续带来 QoE 收益：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/94c9f701a2646c1146f84d35ca4a8a66.png\" /></p><p></p><p>实际业务场景中，分析画质与用户平均观看时长的关系，中高画质可以带来持续的看播收益。下图具体描述了两类典型应用场景下，画质评估体系在业务实践中发挥的主要价值：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc6695ca49b86c25f54bf62ff9526f17.png\" /></p><p></p><p></p><h3>（二）我们为何自研画质评估体系？</h3><p></p><p></p><p>图像服务的最终用户是人类，图像质量评价致力于成为可衡量图像的人眼感知质量需求的客观计算方法。</p><p></p><h4>1、行业现状</h4><p></p><p>主观质量评估：最准确，但费时费力费钱，难以批量应用。例如专家评测、众包测试等。客观评估算法：省时省力可大规模应用，但无论全参/无参考算法与主观评测均存在一定 GAP，在 UGC 场景，差距会更加明显。</p><p></p><p>业界常用的有参画质评估算法，主要包括 PSNR、SSIM、VMAF 3种：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d0b664cb786f9f833b5ff43f7907031f.png\" /></p><p></p><h4>2、痛点</h4><p></p><p>难以量化画质增强效果：行业通用指标（ PSNR、SSIM、VMAF等）均为有参考画质指标， 主要适用于压缩失真的画质评估，难以量化评估画质增强效果。不适合&nbsp;UGC&nbsp;场景的评分：行业通用指标适用场景存在一定局限性，其训练数据集主要为 PGC 内容，在 UGC 场景的泛化效果较差。评估维度有限：UGC 场景下，图片内容复杂且画质影响因素多样，需要更多维度评估指标用于画质分析和指导优化。</p><p></p><h4>3、我们如何建设画质评估体系？</h4><p></p><p></p><p>根据点播、直播和图片等不同形态业务需求，视频架构多媒体实验室自研的 VQScore 画质体系提供配套最优的全链路画质打分能力，提供异步或实时画质打分数据，为后续转码、增强、推荐策略和大盘监控提供能力支持。</p><p></p><p>具体画质分析打分能力分为两个部分：</p><p>内容分析理解：主要包含 ROI 检测、CG 内容检测、人脸检测、内容分类等基础分类和检测的能力，为后续画质打分和增强转码提供细分的维度拆解能力和关键内容识别能力，实现精细准确的端到端自适应增强转码组合能力画质打分能力：主要包含通用清晰度打分算法、美学指标、高阶色彩指标、人像画质等评估指标，噪声、块效应、过曝、脏镜头、模糊和伪高清等细分归因指标，以及超分质量、锐化质量和增强组合评估等前处理画质提升能力评估指标，通用+归因+增强多个维度组合，为不同的业务场景的画质优化需求提供集监控、分析、策略推荐等全方位画质打分能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33efc2a96fdd4c0545968c783205483a.png\" /></p><p></p><p>通用的画质清晰度评估算法基于多样化多业务场景主观标注样本、开源数据集和多样化失真合成数据集，驱动的轻量 transformer-based 深度学习的方案，在 UGC 视频/图像场景提供更稳定准确的客观清晰度预测能力。</p><p></p><p>在多种业务场景下，根据点播、直播和图片不同形态业务需求，支持最高4K分辨率内不同投稿内容的源画质分析，结合业务属性维度提供深入细化的画质维度分析，为自适应转码提供编码优化对比和不同时间尺度的画质监控，为 AB 实验和版本迭代等业务流程提供有效的QoE维度数据，同时也可以为多分辨率/码率档位播放下发提供画质与 QoS 网络、设备等因素组合组合的自适应播放分发优化能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa473e297ed0e365080b62a1c8e7cab6.png\" /></p><p></p><h3>（三）抖音画质评估体系有哪些优势？</h3><p></p><p></p><h4>1、适用范围广泛</h4><p></p><p></p><p>高质量且规模庞大的训练数据集，覆盖 PGC 和 UGC 内容，适用范围广泛（特别针对 UGC <a href=\"https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t\">场景</a>\"）。算法模型历经亿级 DAU 产品持续打磨优化，泛化能力强。</p><p></p><h4>2、评估维度多元</h4><p></p><p></p><p>包含主观清晰度、大众美学质量等两类综合指标和噪声、亮度等十余类细分指标，支持更多维度、更细粒度地分析画质问题，便于业务有针对性地进行优化和调整策略。</p><p></p><h4>3、多业务线上验证收益显著</h4><p></p><p></p><p>历经抖音、头条、番茄小说等数十个大体量业务线上验证，评估效果可靠，能有效支持业务进行画质体验提升，进而带来用户消费指标提升，收益<a href=\"https://www.infoq.cn/news/WPmK0BeY0dDJB6wLL0zM\">显著</a>\"。</p><p></p><h4>4、算法能力业内领先</h4><p></p><p></p><p>画质评估体系涉及的算法模型已申请多项专利。eg. 一种检测伪高清视频的方法，一种基于多任务孪生神经网络的高阶视频色彩质量评价模型，一种三明治视频自适应播放方法等。</p><p></p><p>在 ICME 2021 的「压缩UGC视频质量评估」比赛中，火山引擎-多媒体实验室凭借自研的 VQScore 算法斩获无参考视频质量评价（NR-VQA）MOS 赛道第一名。（<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NTEzOTM5Mw==&amp;mid=2247512713&amp;idx=2&amp;sn=525ebd47bb4a8ecff2139bf8cf3dd260&amp;scene=21#wechat_redirect\">详细介绍</a>\"）</p><p></p><blockquote>该比赛主要针对 UGC 源视频画质和 H.264/AVC 压缩失真对视频主观画质的影响的研究。</blockquote><p></p><p></p><p></p><h2>二、画质评估主要应用在哪些场景？</h2><p></p><p></p><p>以瘦身计划和体重秤之间的关系做个简单类比，画质评估体系作为一套相对客观且行之有效的评测工具，在帮助产品了解业务画质现状、了解行业和市场现状、监测线上<a href=\"https://www.infoq.cn/article/yoQtcCA6p3arSMemh0ZG\">画质</a>\"变化和支持提升用户体验等方面都有非常广泛的应用。</p><p></p><p>了解业务画质现状</p><p></p><p>业务团队可以借助 veImageX 提供的画质评估工具，通过离线测评和在线评估等手段高效完成业务产品的画质摸底；同时，画质评估体系包含丰富的评测维度（例如噪声强度、色彩质量、块效应检测、过曝光检测等），数十项细分评测指标可高效帮助业务团队完成低质图像归因分析，快速锁定问题所在。</p><p></p><p>了解行业/市场现状</p><p></p><p>借助画质评估工具，可以帮助业务团队对市场主流产品或同类业务进行画质评测，以便制定合理的画质提升目标；同时，综合用户主观评测和客观指标的对应关系，高效帮助业务团队确定适合自身业务的画质评估标准。</p><p></p><p>监测线上画质变化</p><p></p><p>对于一款关注用户画质体验的产品来说，线上画质监测工具必不可少。而 veImageX 提供端到端的画质指标监测工具，可帮助业务团队长期高效监测线上画质变化；通过前后数据对比分析，帮助业务有效验证画质优化举措的效果；同时，线上低质问题告警也可帮助业务团队及时发现问题，保障线上用户浏览体验。</p><p></p><p>支持提升用户体验</p><p></p><p>借助画质评估体系提供的评测结果，业务团队可以通过对低质图片进行搜索/推荐降权等方式打压低质内容，或借助画质增强能力提升画质，有效提升用户的浏览体验，进而带来点击率、人均阅读/消费时长、用户留存等业务指标正向提升。</p><p></p><p></p><h2>三、典型案例实践分享</h2><p></p><p></p><p>目前，由火山引擎 veImageX 提供的画质评估工具已服务于抖音、头条、西瓜、番茄小说、懂球帝等数十条业务线，在保障用户的画质体验方面发挥着重要作用。接下来，我们选取了几个典型案例为大家简要分享我们的实践经验。</p><p></p><h3>（一）某短视频/社区平台</h3><p></p><p></p><h4>1、需求背景</h4><p></p><p>某短视频/社区平台是主要用户分布在多个国家和地区，发布内容覆盖多个细分垂类。业务团队收到部分用户反馈关注到不同国家和内容垂类间的画质存在一定差异，影响了用户的浏览体验，从而设立专项进行问题解决。</p><p></p><h4>2、实践方案</h4><p></p><p>业务团队首先使用画质评估工具对全地区的图片画质进行了离线摸底分析，发现部分国家间、某些重点垂类间的图片画质有较大差异，故使用自适应增强模型，针对性进行画质提升的同时尽可能节省码率。</p><p></p><h4>3、整体收益</h4><p></p><p>优化后，该平台各地区间、重点垂类间的画质基本拉齐且均达到【良好】及以上水平，图片大小显著降低，人均停留时长、人均互动、人均阅读时长、人均 session 次数等消费指标均显著正向。</p><p></p><h3>（二）番茄小说</h3><p></p><p></p><h4>1、需求背景</h4><p></p><p>相比于网文，漫画的书封更加精美，信息量也更多，因此在产品形态上，番茄小说频道采用了大屏的展现形式。然而，在漫画功能上线后，业务团队发现，有部分漫画的原始书封比较模糊，严重影响用户浏览体验。如下图所示：</p><p><img src=\"https://static001.geekbang.org/infoq/00/009755aed925213c41d2afcfbea8f7e1.png\" /></p><p></p><p>为了提升这部分图片的画质，业务团队想到了通过画质评估筛查低质图片，使用画质增强能力搭建自动化处理流程，针对性处理低质图片，得到高清图，以提升整体观感。</p><p></p><h4>2、实践方案</h4><p></p><p>业务团队使用 veImageX 画质评估工具，针对出版物（如小说封面、插图、电子书书封、有声播放器封面等）&nbsp;和漫画（漫画封面、横图等）&nbsp;等场景进行离线画质测评，对不同分辨率图片进行画质摸底。根据对低质原因的分析和增强算法对主观画质提升的收益大小综合评估，明确差异化的处理方案。最终业务团队选择搭建自动化处理流程，根据评估结果对不同画质等级的图片进行如自适应增强、超分等优化处理，针对性提升用户的画质浏览体验。</p><p></p><p>低质图片优化前后对比如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/03028351146a8a60c10a30e44aa320bf.png\" /></p><p></p><h4>3、整体收益</h4><p></p><p>番茄小说团队借助 veImageX 画质评估和画质增强能力，有的放矢的提升画质，有效提升了用户画质体验和点击率、人均阅读/消费时长、留存等用户消费指标。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b3794655873870e47f7a5a341e2149bf.png\" /></p><p></p><h3>（三）今日头条</h3><p></p><p></p><h4>1、需求背景</h4><p></p><p>头条小视频频道主要以双列展示为主，而双列流频道展现形式又以封面图为主。综合线上实验结果和实践经验发现，封面图的画质质量不仅会影响用户浏览体验，也会影响点击转化率和用户留存等业务指标，如何有效识别封面模糊的内容并进行打压调控成为一项较为棘手的工作。</p><p></p><h4>2、实践方案</h4><p></p><p>借助画质评估工具，业务团队对封面图进行画质打分，高效识别出低质封面（blockiness≥ X且vqscore&lt; Y）并实行打压调控策略；同时将 vqscore 纳入推荐模型的参考指标，给优质内容提供更多优先曝光机会。</p><p></p><h4>3、整体收益</h4><p></p><p>业务团队通过对低质封面图进行打压调控，人工评估封面优质率提升约3倍，封面低质率降低了约36.7%&nbsp;，模糊封面图占比降低了约 51.4%&nbsp;，人均阅读数、&nbsp;停留时长&nbsp;、点击转化率等业务指标也得到显著提升。（数据来自业务AB实验）</p><p></p><p></p><h3>（四）幸福里VR</h3><p></p><p></p><h4>1、需求背景</h4><p></p><p>幸福里房产 VR 能力在建设初期，因素材供给来源多样且渠道纷杂，质量良莠不齐，频繁收到线上用户反馈；图像质量把控主要依靠人工审核、定期抽检和线上反馈，不仅耗费人力且评估主观，对全景图缺乏有区分度的数据指标量化衡量图像质量和行业领先水平的差距，导致业务团队难以高效定位画质问题并针对性的改善和评估优化效果。</p><p></p><h4>2、实践方案</h4><p></p><p>通过对线上样本数据进行离线画质摸底并综合算法专家建议，业务团队最终选定清晰度&nbsp;（&nbsp;VQScore&nbsp;）、噪声（Noise）、亮度（Brightness）、过曝光（Overexporsure）&nbsp;等四项指标作为全景图量化评估指标。评估发现精装 、 简装 、毛坯等三种装修类型存在显著画质差异，关键差异与环境光线、灯光照明等因素有较高关联，业务团队针对性进行迭代优化并监测画质指标变化，显著提升了VR看房效果。</p><p></p><h4>3、整体收益</h4><p></p><p>业务团队通过画质评估工具，定位具体的画质问题，针对性进行迭代优化以缩小和行业领先水平的差距；同时借助 veImgaeX 提供的 VR 画质增强能力，显著提升全景图画质，阶段性实现用户 0 客诉，弥补了前端采集设备质量参差等问题。</p>",
    "publish_time": "2023-09-12 15:13:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Spring 中三种 BeanName 生成器！",
    "url": "https://www.infoq.cn/article/e2fdb6b926bb413cbee326fc4",
    "summary": "<p>无论我们是通过 XML 文件，还是 Java 代码，亦或是包扫描的方式去注册 Bean，都可以不设置 BeanName，而 Spring 均会为之提供默认的 beanName，今天我们就来看看 Spring 中三种处理不同情况的 beanName 生成器。</p><p></p><h2>1. BeanNameGenerator</h2><p></p><p>Spring 中提供了一个名为 BeanNameGenerator 的接口，这个接口就只有一个需要实现的方法就是 generateBeanName，从名字就能看出来，这就是专门用来生成 beanName 的方法。</p><p></p><p><code lang=\"java\">public interface BeanNameGenerator {\n  String generateBeanName(BeanDefinition definition, BeanDefinitionRegistry registry);\n}\n</code></p><p></p><p>这个方法有两个参数：</p><p></p><p>definition：这个是要生成的 Bean 定义。registry：这个是将来 BeanDefinition 的注册器。</p><p></p><p>BeanNameGenerator 有三个不同的实现类，对应不同的处理场景：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d495c11aeba1e61ab15449111f912a16.png\" /></p><p></p><p>AnnotationBeanNameGenerator：这个专门用来处理包扫描的时候扫到的 Bean，对于这些 Bean，其 name 属性该如何处理，由这个类来解决，当然，小伙伴们都知道，通过 @Component/@Service/@Repository/@Controller 这些注解定义的 Bean，默认情况下，beanName 就是类名首字母小写。FullyQualifiedAnnotationBeanNameGenerator：这个继承自 AnnotationBeanNameGenerator，并重写了 AnnotationBeanNameGenerator#buildDefaultBeanName 方法，这个是使用类的全路径来作为 Bean 的默认名称。DefaultBeanNameGenerator：这个是专门用来解决 XML 文件中定义的 Bean 如果没有设置 beanName，那么就通过 DefaultBeanNameGenerator 来为其生成 beanName。</p><p></p><p>看了上面三个场景之后，可能有小伙伴发现一个 BUG，那么 @Bean 注解定义的 Bean，其 beanName 属性是在哪里处理的呢？这个其实比较特殊，是当场处理的，没用到 BeanNameGenerator，松哥后面单独说。</p><p></p><p>接下来我们详细看下上面这三个实现类。</p><p></p><h2>2. AnnotationBeanNameGenerator</h2><p></p><p>咱们直接来看最关键的 generateBeanName 方法吧：</p><p></p><p><code lang=\"java\">@Override\npublic String generateBeanName(BeanDefinition definition, BeanDefinitionRegistry registry) {\n  if (definition instanceof AnnotatedBeanDefinition) {\n    String beanName = determineBeanNameFromAnnotation((AnnotatedBeanDefinition) definition);\n    if (StringUtils.hasText(beanName)) {\n      // Explicit bean name found.\n      return beanName;\n    }\n  }\n  // Fallback: generate a unique default bean name.\n  return buildDefaultBeanName(definition, registry);\n}\n</code></p><p></p><p>这个方法首先判断 definition 是否为 AnnotatedBeanDefinition 类型，根据我们前面文章对 BeanDefinition 的介绍（<a href=\"https://mp.weixin.qq.com/s/G5_wqgjbpVp0qmscIkFaTg\">七种 BeanDefinition，各显其能！</a>\"），大家知道，AnnotatedBeanDefinition 的实现类主要是针对三种情况：@Bean 注解定义的 Bean、@Service/@Controller/@Component/@Repository 等注解标记的 Bean 以及系统的启动配置类，如果是这三种情况，那么就去调用 determineBeanNameFromAnnotation 方法，这个方法会尝试从注解中提取出来 beanName，如果不是上面三种情况，那么就调用 buildDefaultBeanName 方法去生成 beanName。</p><p></p><p>那我们先来看 determineBeanNameFromAnnotation 方法：</p><p></p><p><code lang=\"java\">@Nullable\nprotected String determineBeanNameFromAnnotation(AnnotatedBeanDefinition annotatedDef) {\n  AnnotationMetadata amd = annotatedDef.getMetadata();\n  Set types = amd.getAnnotationTypes();\n  String beanName = null;\n  for (String type : types) {\n    AnnotationAttributes attributes = AnnotationConfigUtils.attributesFor(amd, type);\n    if (attributes != null) {\n      Set metaTypes = this.metaAnnotationTypesCache.computeIfAbsent(type, key -&gt; {\n        Set result = amd.getMetaAnnotationTypes(key);\n        return (result.isEmpty() ? Collections.emptySet() : result);\n      });\n      if (isStereotypeWithNameValue(type, metaTypes, attributes)) {\n        Object value = attributes.get(\"value\");\n        if (value instanceof String strVal) {\n          if (StringUtils.hasLength(strVal)) {\n            if (beanName != null &amp;&amp; !strVal.equals(beanName)) {\n              throw new IllegalStateException(\"Stereotype annotations suggest inconsistent \" +\n                  \"component names: '\" + beanName + \"' versus '\" + strVal + \"'\");\n            }\n            beanName = strVal;\n          }\n        }\n      }\n    }\n  }\n  return beanName;\n}\n</code></p><p></p><p>这个方法首先会去获取类上的注解信息，拿到 amd 之后，获取到所有的注解类型，然后进行遍历。</p><p></p><p>遍历的时候，首先获取到注解上的所有属性 attributes，当 attributes 不为空的时候，继续去读取当前注解的元注解，并将读取到的结果存入到 metaAnnotationTypesCache 集合中。这个是干嘛呢？大家知道，Spring 中用来标记 Bean 的注解大部分衍生自 @Component，甚至我们也可以自定义注解，那么如果自定义注解了，这个地方就没法判断了，因为每个人自定义出来的注解都不一样。所以，万变不离其宗，这里就去找各个注解的元注解。例如如果我们在类上添加的是 @Configuration，那么 @Configuration 的元注解有两个，分别是 @Component 和 @Indexed。</p><p></p><p>接下来的 isStereotypeWithNameValue 方法就是判断 type 是不是 @Component 或者 Jakarta 中自带的 @ManagedBean、@Named，亦或者 metaTypes 里是否包含 @Component。如果确定是 @Component 衍生出来的注解，亦或者是 @ManagedBean、@Named 注解标记的 Bean，那么就将其 value 属性读取出来，作为 beanName，如果包含多个有效注解，且各自配置的 beanName 不一致，就会抛出异常。</p><p></p><p>例如下面这种情况：</p><p></p><p><code lang=\"java\">@Configuration(\"j\")\n@Component(\"a\")\npublic class JavaConfig {\n}\n</code></p><p></p><p>这两个 beanName 不一致，运行时就会出错。</p><p></p><p>同时，经过上面的分析，小伙伴也看到了，我们其实可以通过自定义注解为 Bean 设置名称，例如我有如下注解：</p><p></p><p><code lang=\"java\">@Retention(RetentionPolicy.RUNTIME)\n@Component\npublic @interface MyBeanName {\n    String value() default \"\";\n}\n</code></p><p></p><p>这个注解衍生自 @Component，那么它的用法如下：</p><p></p><p><code lang=\"java\">@MyBeanName(\"f\")\npublic class JavaConfig {\n\n}\n</code></p><p></p><p>那么 f 就是当前类生成的 beanName。</p><p></p><p>以上是从注解中去提取 beanName，但是注解中可能没有提供 beanName，那么就得调用 buildDefaultBeanName 方法去自动生成了，如下：</p><p></p><p><code lang=\"java\">protected String buildDefaultBeanName(BeanDefinition definition, BeanDefinitionRegistry registry) {\n  return buildDefaultBeanName(definition);\n}\nprotected String buildDefaultBeanName(BeanDefinition definition) {\n  String beanClassName = definition.getBeanClassName();\n  Assert.state(beanClassName != null, \"No bean class name set\");\n  String shortClassName = ClassUtils.getShortName(beanClassName);\n  return StringUtils.uncapitalizeAsProperty(shortClassName);\n}\n</code></p><p></p><p>这个就很好懂了，先拿到 bean 的完整类名，然后提取出来 shortName，也就是去除包名之后的名字，然后首字母小写之后返回。</p><p></p><p>这就是 @Component 注解体系下的 beanName 生成流程。</p><p></p><h2>3. FullyQualifiedAnnotationBeanNameGenerator</h2><p></p><p>FullyQualifiedAnnotationBeanNameGenerator 类只是重写了 AnnotationBeanNameGenerator 的 buildDefaultBeanName 方法，如下：</p><p></p><p><code lang=\"java\">@Override\nprotected String buildDefaultBeanName(BeanDefinition definition) {\n  String beanClassName = definition.getBeanClassName();\n  Assert.state(beanClassName != null, \"No bean class name set\");\n  return beanClassName;\n}\n</code></p><p></p><p>重写后的方法就是获取类的完整路径返回。</p><p></p><p>FullyQualifiedAnnotationBeanNameGenerator 默认情况下并不会直接使用，需要自己手动配置，像下面这样：</p><p></p><p><code lang=\"java\">@Configuration\n@ComponentScan(nameGenerator = FullyQualifiedAnnotationBeanNameGenerator.class)\npublic class JavaConfig {\n\n}\n</code></p><p></p><p>此时，生成的 Bean 的默认名称就是类的全路径了。</p><p></p><h2>4. DefaultBeanNameGenerator</h2><p></p><p>这个是专门用来处理 XML 中默认的 beanName 的。这个在最近录制的 Spring 源码视频中已经详细介绍过了，这里就不再啰嗦了，感兴趣的小伙伴戳这里：<a href=\"https://mp.weixin.qq.com/s/mHizA4NQj1_94g8mMrAm4w\">Spring源码应该怎么学？</a>\"。</p><p></p><h2>5. @Bean 处理特殊情况</h2><p></p><p>如果类是被 @Bean 注解标记的，那么处理情况就特殊一些，直接现场处理，方法在 org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader#loadBeanDefinitionsForBeanMethod 位置：</p><p></p><p><code lang=\"java\">private void loadBeanDefinitionsForBeanMethod(BeanMethod beanMethod) {\n  // Consider name and any aliases\n  List names = new ArrayList&lt;&gt;(Arrays.asList(bean.getStringArray(\"name\")));\n  String beanName = (!names.isEmpty() ? names.remove(0) : methodName);\n\n  // Register aliases even when overridden\n  for (String alias : names) {\n    this.registry.registerAlias(beanName, alias);\n  }\n}\n</code></p><p></p><p>从这里可以看到，如果一开始配置了 name 属性，那么就把 names 集合中的第一个值拿出来作为 beanName，集合中的其他值则当作别名来处理，如果没有配置 name 属性值，那么就使用方法名作为 beanName。</p><p></p><h2>6. 小结</h2><p></p><p>好啦，这就是松哥和大家讲的 Spring 中默认的 beanName 生成策略，感兴趣的小伙伴可以试试哦</p>",
    "publish_time": "2023-09-12 10:54:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌推出3个新的云存储选项：Cloud Storage FUSE、Parallelstore和NetApp Volumes",
    "url": "https://www.infoq.cn/article/sHSt1JqevLArJ8XxI3xG",
    "summary": "<p>最近，谷歌推出了3个新的云存储选项：<a href=\"https://cloud.google.com/storage/docs/gcs-fuse\">Cloud Storage FUSE</a>\"（用于需要文件系统语义的人工智能应用程序）、<a href=\"https://cloud.google.com/parallelstore\">Parallelstore</a>\"（这是一个并行文件系统，用于需要使用GPU的人工智能和高性能计算应用程序）和<a href=\"https://cloud.google.com/netapp-volumes\">NetApp Volumes</a>\"（用于在云中运行的企业级应用程序）。</p><p>&nbsp;</p><p>Cloud Storage FUSE已经提供了开源版本，允许将<a href=\"https://cloud.google.com/storage\">Cloud Storage</a>\"桶中的对象作为挂载到本地文件系统的文件来访问。谷歌针对人工智能工作负载增强了它的可移植性、可靠性、性能和集成能力。在<a href=\"https://cloud.google.com/blog/products/storage-data-transfer/cloud-storage-fuse-now-ga\">谷歌的一篇博文</a>\"中，谷歌产品经理<a href=\"https://www.linkedin.com/in/marco-abela-a7b24963/\">Marco Abela</a>\"和高级产品经理<a href=\"https://www.linkedin.com/in/akshay-ram-20913953/\">Akshay Ram</a>\"解释说：</p><p></p><blockquote>新推出的Cloud Storage FUSE对于人工智能工作负载尤为重要。由于应用程序可以直接访问数据（而不是将数据下载到本地），所以不需要实现自定义逻辑，而且复制数据时TPU和GPU等宝贵资源的空闲时间也缩短了。此外，GKE（Google Kubernetes Engine）新增的<a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver\">Cloud Storage FUSE CSI驱动程序</a>\"允许应用程序使用为人熟知的Kubernetes API挂载Cloud Storage，并且是作为由GKE托管的turn-key部署提供的。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/389598bb5a0ecb71f9fe92e7da68fd81.png\" /></p><p></p><p>Cloud Storage FUSE概览（图片来源：<a href=\"https://cloud.google.com/blog/products/storage-data-transfer/cloud-storage-fuse-now-ga\">谷歌的博文</a>\"）</p><p>&nbsp;</p><p>云布道师<a href=\"https://twitter.com/jwd_gcp/\">J.W. Davis</a>\"在一则<a href=\"https://twitter.com/jwd_gcp/status/1694875605446389847\">推特</a>\"中评论了GKE FUSE文件系统的可用性：</p><p></p><blockquote>这似乎已经注定会带来许多灾难。它的应用场景非常有限；大多数人会遭受滥用之苦。</blockquote><p></p><p>&nbsp;</p><p>除了用于AI应用程序的Cloud Storage FUSE之外，谷歌还宣布了并行文件系统Parallelstore的内部预览。它可以为AI/ML和HPC工作负载提供高性能的并行文件存储解决方案，帮助用户减少因I/O存储等待所浪费的宝贵的GPU资源。该解决方案基于<a href=\"https://www.intel.com/content/dam/www/public/us/en/documents/solution-briefs/high-performance-storage-brief.pdf\">英特尔下一代分布式异步对象存储</a>\"（DAOS）架构。</p><p>&nbsp;</p><p>存储业务副总裁兼总经理<a href=\"https://www.linkedin.com/in/sameet-agarwal/\">Sameet Agarwal</a>\"和存储业务集团产品经理<a href=\"https://twitter.com/SeanDerrington\">Sean Derrington</a>\"在谷歌的一篇博文中介绍了这个新的云存储选项：</p><p></p><blockquote>基于英特尔下一代的DAOS架构，Parallelstore环境中的所有计算节点都具有同样的存储访问权限，因此，VM可以即时地访问其数据。与竞争对手Lustre Scratch的产品相比，Parallelstore的读吞吐量是其6.3倍。Parallelstore非常适合需要极高性能（IOPS和吞吐量）和超低延迟的云端应用。</blockquote><p></p><p>&nbsp;</p><p>最后，第3个新增的云存储选项是NetApp Volumes。这是一项完全由谷歌托管的高性能文件存储服务。该存储选项专为希望将基于本地NetApp存储阵列的应用程序迁移上云的企业而设计。该服务提供的容量在100GiB到100TiB之间，为混合工作负载实现了ONTAP数据管理，并且无需重构即可将Windows或Linux应用程序作为虚拟机运行。</p><p>&nbsp;</p><p>当InfoQ问到是什么促使谷歌进行这些投资时，Derrington是这么说的：</p><p></p><blockquote>随着人工智能在自动化数据管理方面变得越来越重要，组织开始转向云计算来为应用程序寻求合适的存储解决方案。借助谷歌云新提供的专门针对人工智能优化过的存储产品Cloud Storage FUSE和Parallelstore，我们提供了定制的存储解决方案，简化操作，激发创新，降低成本，帮助客户适应复杂的人工智能工作负载。</blockquote><p></p><p>&nbsp;</p><p>Cloud Storage FUSE和NetApp Volumes可以通过谷歌云控制台获得，而Parallelstore需要通过谷歌账户管理器获得。</p><p>&nbsp;</p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p>&nbsp;</p><p>原文链接：<a href=\"https://www.infoq.com/news/2023/08/google-cloud-new-storage-options/\">https://www.infoq.com/news/2023/08/google-cloud-new-storage-options/</a>\"</p>",
    "publish_time": "2023-09-12 16:20:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Pinterest使用Kubernetes和Helix构建下一代异步计算平台Pacer",
    "url": "https://www.infoq.cn/article/ZcvPUQRbtRVbIxKcxD3Q",
    "summary": "<p>Pinterest推出其下一代异步计算平台Pacer，用以取代旧的解决方案Pinlater。随着公司的发展，Pinlater在伸缩性和可靠性方面面临着挑战。新的架构使用Kubernetes来调度作业，使用Apache Helix来进行集群管理。</p><p></p><p>Pinterest之前构建了一个异步作业执行平台<a href=\"https://github.com/pinterest/pinlater?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Pinlater</a>\"，并在几年前将其<a href=\"https://medium.com/pinterest-engineering/open-sourcing-pinlater-an-asynchronous-job-execution-system-d8ec4e39859a?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">开源</a>\"。Pinlater已在生产环境中使用了多年，并支持许多关键的功能领域。Pinterest在<a href=\"https://aws.amazon.com/ec2/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">AWS EC2</a>\"上运行了几个Pinlater集群，每分钟处理数百万个任务。</p><p></p><p>Pinterest软件工程师<a href=\"https://www.linkedin.com/in/qi-li-b16910b9/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Li Qi</a>\"和<a href=\"https://www.linkedin.com/in/zhihuang-chen-585930122/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Chen Zhihuang</a>\"解释了促使他们构建新平台的动机：</p><p></p><p></p><blockquote>随着Pinterest在过去几年的增长和Pinlater流量的增加，我们发现Pinlater存在许多局限性，包括伸缩性瓶颈、硬件效率、缺乏隔离性和可用性。我们在平台方面也遇到了新的挑战，包括那些影响我们数据存储吞吐量和可靠性的挑战。</blockquote><p></p><p></p><p>基于他们使用Pinlater的经历，团队意识到他们不可能在现有架构中解决所有已知的问题，于是他们决定构建下一代平台。</p><p></p><p>新的架构Pacer包含了一个无状态的<a href=\"https://thrift.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Thrift</a>\" API服务（与Pinlater兼容）、一个数据存储（<a href=\"https://www.mysql.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">MySQL</a>\"）、一个有状态的脱队列代理服务（Dequeue Broker），以及在<a href=\"https://kubernetes.io/pl/docs/concepts/overview/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Kubernetes</a>\"上运行的作业执行Worker池。<a href=\"https://helix.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Apache Helix</a>\"（带有<a href=\"https://zookeeper.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Zookeeper</a>\"）被用来将作业队列分区分配给脱队列代理。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/08/pinterest-pacer-kubernetes/en/resources/11_aMUUI-P4Gl_378E0XysF6Q-1692289081279.jpeg\" /></p><p></p><p>Pacer架构（来源：<a href=\"https://medium.com/pinterest-engineering/pacer-pinterests-new-generation-of-asynchronous-computing-platform-5c338a15d2a0?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Pinterest工程博客</a>\"）</p><p></p><p>脱队列代理是一种有状态服务，负责从数据存储中预取作业队列数据并将其缓存到内存中，以减少延迟和隔离入队列和脱队列的工作负载。每个脱队列代理分配到一组作业队列分区，因此可以独占获取和执行作业，从而避免出现争用的情况。Kubernetes为每个作业队列提供了一个专用的Pod池，消除因不同作业类型对资源倾斜消耗所带来的影响。</p><p></p><p>新的脱队列和执行模型缓解了Pinlater所遭遇的问题，包括在从热点分区获取数据时避免扫描所有分区或减少锁的争用。此外，它支持按照排队顺序（<a href=\"https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">FIFO</a>\"）的方式执行作业，前提是为作业队列配置单独的分区。</p><p></p><p>新的架构需要给脱队列代理实例进行独占式队列分区分配，与<a href=\"https://kafka.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Kafka</a>\"</p><p>消费者主题分区分配类似。Pinterest的团队选择使用Apache Helix来实现这个功能。Apache Helix提供了一个通用的集群管理框架，用于给集群内的脱队列代理进行分区分配。Helix使用Apache Zookeeper实现嵌在脱队列代理实例中的Helix控制器和Helix代理之间的资源配置通信。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/08/pinterest-pacer-kubernetes/en/resources/11_-YVqXf-1c-elcPRM5xvu7Q-1692289081279.jpeg\" /></p><p></p><p>用Apache Helix和Zookeeper协调脱队列代理（来源：<a href=\"https://medium.com/pinterest-engineering/pacer-pinterests-new-generation-of-asynchronous-computing-platform-5c338a15d2a0?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">Pinterest工程博客</a>\"）</p><p></p><p>Helix控制监控加入和离开集群的脱队列代理实例，以及对已配置的作业队列做出的任何变更，如果发生变更，它将重新计算理想的队列分区与代理分布。在最新的分区分配被保存到Zookeeper之后，各个代理实例就会更新它们的内部状态，并从它们负责的队列分区中获取数据。</p><p></p><p></p><p>查看英文原文：<a href=\"https://www.infoq.com/news/2023/08/pinterest-pacer-kubernetes/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MDcyNTQsImZpbGVHVUlEIjoiWk02R1VrY2dIQWJBOGlXdiIsImlhdCI6MTY5NDUwNjk1NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.z-LSghRE_Q52tbVl_z1e0M3d4fse5_hTJO09XWUo1wI\">https://www.infoq.com/news/2023/08/pinterest-pacer-kubernetes/</a>\"</p>",
    "publish_time": "2023-09-12 16:30:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌云推出AlloyDB AI：用先进的向量嵌入和AI改造PostgreSQL",
    "url": "https://www.infoq.cn/article/s0Ou9xLi1TtbTzUzTj9x",
    "summary": "<p>在最近的<a href=\"https://cloud.withgoogle.com/next\">Google Cloud Next</a>\"中，谷歌在预告中宣布了<a href=\"https://cloud.google.com/alloydb/ai\">AlloyDB AI</a>\"是<a href=\"https://cloud.google.com/alloydb\">AlloyDB for PostgreSQL</a>\"的一个组成部分，允许开发人员利用大语言模型（LLM）来构建生成式（gen）人工智能（AI）应用程序，并通过内置的、端到端的<a href=\"https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings\">向量嵌入</a>\"支持来利用它们的实时操作数据。</p><p>&nbsp;</p><p>早些时候，该公司<a href=\"https://www.infoq.com/news/2023/07/gcp-databases-vector-search/\">推出了</a>\"对Cloud SQL for PostgreSQL和AlloyDB for PostgreSQL的pgvector支持，将向量搜索操作引入到了托管数据库中，允许开发人员存储由<a href=\"https://developers.google.com/machine-learning/resources/intro-llms\">大语言模型（LLM）</a>\"生成的向量嵌入并执行相似性搜索。AlloyDB AI建立在标准PostgreSQL所提供的基本向量支持之上，该公司表示，它为开发人员提供了“创建和查询嵌入的能力，只需几行SQL即可找到相关数据，既不需要专门的数据堆栈，也不需要移动数据。”</p><p>&nbsp;</p><p>此外，AlloyDB AI还为AlloyDB带来了一些其他新功能，可以帮助开发人员将实时数据整合到生成式AI应用程序中：</p><p>&nbsp;</p><p>通过与AlloyDB查询处理引擎的紧密集成，增强了向量支持，比标准PostgreSQL查询更快。此外，该公司还引入了基于<a href=\"https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html\">ScaNN技术</a>\"的量化技术，以在启用时支持四倍的向量尺寸和三倍的空间缩小。&nbsp;访问AlloyDB中的本地模型和<a href=\"https://cloud.google.com/vertex-ai\">Vertex AI</a>\"中托管的远程模型，包括自定义和预训练模型。开发人员可以使用存储在AlloyDB中的数据来训练和微调模型，然后将它们作为端点部署在Vertex AI上。与人工智能生态系统的集成，包括<a href=\"https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-next-2023-announcements\">Vertex AI Extensions</a>\"（将于今年晚些时候推出）和<a href=\"https://python.langchain.com/docs/get_started/introduction.html\">LangChain</a>\"，这将提供在Vertex AI中调用远程模型的能力，以实现低延迟、高吞吐量的增强事务，使用SQL进行欺诈检测等用例。</p><p>&nbsp;</p><p>谷歌云数据库总经理兼工程副总裁<a href=\"https://twitter.com/andigutmans\">Andi Gutmans</a>\"在<a href=\"https://cloud.google.com/blog/products/databases/helping-developers-build-gen-ai-apps-with-google-cloud-databases/\">谷歌博客中</a>\"写道：</p><p>&nbsp;</p><p></p><blockquote>AlloyDB AI允许用户使用简单的SQL函数轻松地将数据转换为向量嵌入，用于数据库内的嵌入生成，并且运行向量查询的速度比标准PostgreSQL快10倍。与开源人工智能生态系统和谷歌云的Vertex AI平台的集成为构建生成式人工智能应用程序提供了端到端的解决方案。</blockquote><p></p><p>&nbsp;</p><p>根据Andi的<a href=\"https://news.ycombinator.com/item?id=37312502\">声明</a>\"，Reddit上的一名受访者问道，谷歌是否是在试图用AlloyDB AI拥抱、扩展再消灭（Embrace, extend, and extinguish，EEE）PostgreSQL，另一个答案是：</p><p>&nbsp;</p><p></p><blockquote>我想你想说的是，仅仅因为有人，尤其是[大公司]，试图改进/整合流行的开放项目，并不意味着它总是EEE。&nbsp;我怀疑EEE在最初的大部分时间里都是有目的的，即使后来有可能变成那样。以谷歌为例，我认为这将是一个“我们如何为我们的产品增加销售价值”的案例，然后是“这个功能花费了我们太多的资源来维护，致使我们必须削减它，专注于[新功能]”</blockquote><p></p><p>&nbsp;</p><p>此外，其他数据库和公有云提供商也已经支持向量嵌入，包括MongoDB、DataStax的Cassandra数据库服务Astra、开源的PostgreSQL（通过Pgvector）和Azure Cognitive Search。后者最近发布了<a href=\"https://www.infoq.com/news/2023/07/microsoft-launches-vector-search/\">一个新的功能</a>\"，可以在预览版本中对搜索索引中的向量嵌入进行索引、存储和检索。</p><p>&nbsp;</p><p>最后，AlloyDB AI可在谷歌云和<a href=\"https://cloud.google.com/alloydb/omni\">AlloyDB Omni</a>\"上的AlloyDB中使用，无需额外的费用。AlloyDB的定价详细信息可在<a href=\"https://cloud.google.com/alloydb/pricing\">定价页面</a>\"上查看。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/09/google-cloud-alloydb-ai-preview/\">https://www.infoq.com/news/2023/09/google-cloud-alloydb-ai-preview/</a>\"</p>",
    "publish_time": "2023-09-12 16:36:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯做大模型：要拼技术细节、用内部业务“磨刀”",
    "url": "https://www.infoq.cn/article/Nj2C7Vkk6MameTEXto0A",
    "summary": "<p>“<a href=\"https://www.infoq.cn/news/HXQtZJxYy5SeK9OH3Xzi\">腾讯混元大模型</a>\"从第一个token开始从零训练。”腾讯集团副总裁蒋杰说道。9月7日，腾讯正式对外开放了全链路自研的通用大模型——混元大模型，这也意味着腾讯正式加入了“<a href=\"https://www.infoq.cn/article/FksReogLLESJpqOR3ZxD\">百模大战</a>\"”之中。</p><p>&nbsp;</p><p>在已经有首批8家企业机构的大模型产品通过《生成式人工智能服务管理暂行办法》备案准备正式上线开放后，腾讯的通用大模型才刚刚发布，这个时间并不算早。那么，腾讯的大模型之路将如何走下去？</p><p></p><h3>做大模型要“拼细节”</h3><p></p><p>&nbsp;</p><p>“混元”不是腾讯推出的第一个大模型。从2018年开始探索大模型相关技术，腾讯先后推出了多个千万/亿参数大模型：2021年-2022年推出了多个千亿和万亿参数规模的大模型。</p><p>&nbsp;</p><p>腾讯混元大模型平台架构、模型、算法能力等整个体系都是纯自研的，而构建腾讯混元的技术能力都得益于这些年大模型能力的积累。像今天的锯齿状注意力、探真等都是技术循序渐进的产物。</p><p>&nbsp;</p><p>“现在国内外有很多开源的大模型，很多企业也是基于开源模型来做，但是如果不从头自研的话，就没办法完全掌握这个技术。”蒋杰说道。</p><p>&nbsp;</p><p>腾讯对大模型的期望是先给企业内部业务带来突破，这要求大模型必须更好融入到腾讯的技术栈中，但很多开源架构并不适合腾讯业务场景。比如，幻觉是每一个大模型厂商都会面临的重要问题，业内普遍会用知识图谱甚至搜索外挂让大模型的检索支持能力变得更强，但是这些方式不适用腾讯的场景占比很高，于是腾讯使用了自研的“探真”技术来降低幻觉出现的比例。</p><p>&nbsp;</p><p>混元大模型目前还是聚焦在国内市场，中文创作是其主要攻破的能力之一，支持文学创作、文本摘要、角色扮演等。通用大模型的逻辑推理能力非常关键，而大模型如何可靠地执行是腾讯最关注的。</p><p>&nbsp;</p><p>混元大模型拥有超千亿参数规模，预训练语料超2万亿tokens。腾讯的内容产品为混元大模型提供了大规模、高质量、多样化的语料库，混元大模型能从中学习到各类应用场景中丰富的语言知识和语境理解能力。</p><p>&nbsp;</p><p>面对海量数据，腾讯使用了AngelPTM训练框架，优化算法，改进了注意力机制。而在逻辑推理方面，腾讯则使用了AngelHCF推理框架，开发了思维链（Chain-of-Thought，CoT）新算法。腾讯表示，通过自研机器学习框架Angel使训练速度相比业界主流框架提升1倍，推理速度比业界主流框架提升1.3倍。</p><p>&nbsp;</p><p>注：思维链指的是一系列有逻辑关系的思考步骤形成一个完整的思考过程，用的是离散式token，能自动构建问题、推理步骤和样例。但思维链必须在模型规模足够大时才能涌现。</p><p>&nbsp;</p><p>在蒋杰看来，业内做强化学习的方法大体相似，腾讯要做的就是“拼细节”。“未来几个头部厂商大模型的评分可能仅仅是1分、2分的差距，这个厂家版本高1分，另外厂家的下一个版本就会比它再高1分，就是这样一个不断博弈和循序渐进的过程。而大家投入的资源不一样、抠的细节不一样，大模型的差异才会最终显露出来。”&nbsp;</p><p></p><h3>先做内部业务的“倍增器”</h3><p></p><p>&nbsp;</p><p>在通用大模型上，腾讯确实走得不急。腾讯强调，研发大模型的目标不是在评测上获得高分，而是将技术应用到实际场景中。腾讯6月份发布行业大模型后，一直努力将能力拓展到更多领域，腾讯内部的海量业务场景也成了混元大模型的“磨刀石”。</p><p>&nbsp;</p><p>众所周知，腾讯业务特别广泛，混元大模型能在内部各种场景上很好地应用就很不容易。比如，to C的腾讯会议、腾讯文档在使用大模型时就有很大的差异。混元大模型的文字总结能力能与文档环境天然很好地结合，但会议场景强实时交互，需要会议团队和混元团队一起探索如何将混元大模型的基础指令理解能力、文字总结能力与会议内容生成结合起来。</p><p>&nbsp;</p><p>“像会议、文档这样的场景，单纯将一个大模型直接融合进去短期内不一定能够给业务带来很大提升，因此一定要针对具体的业务需求做专门优化和提效，才能达到更好的效果。”腾讯机器学习平台部副总经理王迪说道。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f1/f1bd1aeae2d9e13e610c22b5118e6ab7.png\" /></p><p></p><p>在腾讯看来，提效是大模型更有商业价值的地方，腾讯希望混元大模型成为业务的“倍增器”。</p><p>&nbsp;</p><p>目前，腾讯内部所有的应用都会基于混元大模型做智能化研发，混元大模型将作为基础设施去支持腾讯的各种产品和应用能力。<a href=\"https://www.infoq.cn/news/dVusdTArjZDRQ1dwL8wo\">腾讯会议</a>\"基于腾讯混元大模型打造了AI小助手，只需要简单的自然语言指令，就能完成会议信息提取、内容分析等复杂任务，会后还能生成智能总结纪要。混元大模型支持数十种文本创作场景，在腾讯文档推出的智能助手功能中已有应用。</p><p>&nbsp;</p><p>与之前技术产品的商业化路径相似，腾讯大模型也会先服务腾讯内部业务，然后再通过腾讯云对外开放，服务外部客户。</p><p></p><h3>结束语</h3><p></p><p>&nbsp;</p><p>在蒋杰看来，大模型的天花板现在还没有完全碰触到的技术体系和演进上，行业不仅需要技术突破，还需要语料的完整度、数据的标注能力、后续的纠错能力等，单点的技术突破无法带来大模型的最终效果。</p><p>&nbsp;</p><p>“未来，混元大模型还要做更多的数据标注、更多的框架、训练更多的数据，这才是我们团队工作的真正核心。”蒋杰说道，“腾讯混元永远在路上。”</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-09-12 17:28:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "企业级生成式AI应用，如何克服“幻觉”问题",
    "url": "https://www.infoq.cn/article/TSJbOkWweAvFh44Oo2dL",
    "summary": "<p>“最近我被问到最多的一个问题，是<a href=\"https://www.infoq.cn/article/r6xXrY79a7qSFhga4HPJ\">IBM</a>\"现在还在做AI吗？我想告诉大家的是，IBM是一家混合云与AI公司，我们一直在做AI，并且专门做企业级AI。”IBM大中华区首席技术官、研发中心总经理谢东在日前接受媒体采访时强调。</p><p></p><p>面对今年以来热度一路狂飙的生成式AI，作为AI发展史上的重要参与者，IBM并没有“闲着”。今年5月，IBM发布了企业级AI和数据平台<a href=\"https://www.infoq.cn/article/wAM6PJiYjiyyj3l2jt4x\">watsonx</a>\"；自7月份以来，各个模块陆续上市；预计在今年年底到明年初，全部功能模块将会上市。其中，watsonx.data的premise版本现在已经可以提供给中国客户。</p><p></p><p>如谢东强调，IBM在其中锚定的依旧是“企业级”市场，延续长期以来的产品和生态定位。</p><p></p><p>然而，企业级AI应用与个人AI应用需求之间存在巨大差异，对技术本身的要求也不在一个量级。仅拿当下生成式AI应用最让人诟病的“AI幻觉”问题来说，放在企业级生产环境，对此几乎是“零容忍”。</p><p></p><p>根据IBM商业价值研究院最近发布的面向全球超过30个国家和地区、超过3000名CEO的调研报告显示，61%的受访CEO表达了对生成式AI中所使用的数据来源的担忧。</p><p></p><p>“这一担忧侧面反映了企业AI应用之路面临着重重挑战：首先是技术挑战，尤其是数据的准备、应用和治理；第二是人才挑战，企业需要快速实现人员技能的转型和提升，来拥抱AI浪潮；第三是文化挑战，技能的转型往往伴随组织文化的更新，如何让二者互相成就、带来生产力的提高，这需要优秀的管理智慧。”IBM大中华区董事长、总经理陈旭东指出。</p><p></p><p>据此，在IBM看来，在企业落地AI应用有三个关键点：聚焦自身的业务需求、使用企业自己的数据、量身定制生成式AI解决方案和模型。那么，IBM watsonx究竟是什么？又如何满足企业级AI应用的如上需求？本文将为大家揭晓。</p><p></p><h3>企业使用AI，关注的是它“不能做什么”和“不允许做什么”</h3><p></p><p></p><p>从“AI幻觉”问题说起。</p><p></p><p>企业使用AI，不只是关心它“能做什么”，更要关注的是它“不能做什么”，以及“不允许它做什么”。AI的可信性、可解释性非常关键。因为企业决策与经营直接相关，企业使用AI，要避免给业务带来技术风险，因此对智能分析的准确率要求高，容错率低。</p><p></p><p>在谢东看来，消灭“幻觉”归根结底要从最初的数据抓起。“想让AI的回答是正确的，至少要保证训练的数据是干净合规的，数据本身的完整性、信息量要合乎要求。”他告诉InfoQ。</p><p></p><p>IBM watsonx正在试图解决这些问题。根据官方释义，它提供一个包括AI开发平台watsonx.ai、数据存储平台watsonx.data 和AI治理平台watsonx.governance。</p><p></p><p>其中，watsonx.data 针对企业中海量且复杂的数据，可以通过集中治理和本地自动化策略实施来确保数据的安全性和合规性；此外，watsonx.governance 还采用了软件自动化来帮助企业增强能力以降低风险、满足监管要求和应对AI伦理问题，使得企业能够自动化和整合多个工具、应用程序和平台，同时可以记录数据集、模型、相关元数据和管道的来源。</p><p></p><p>“另一方面，幻觉的产生是因为在大模型中缺失了相应的信息，克服的方法是使用企业自己的数据，把企业的数据输入给它，这样至少可以在特定业务领域减少幻觉的产生。”谢东补充说。</p><p></p><p>据了解，除了原始数据和专有数据，企业还可以带入自己的数据来丰富和改进他们的目标模型，所有数据都存储在watsonx.data中，其中包含有关每个文件或文档的详细元数据，以提供可追溯的治理。在数据的过滤和处理过程中，平台首先会识别数据的来源和ID，然后对数据进行分类和过滤，对重复数据和不合规数据进行分析和清除。并且，对数据进行版本控制和标记。在过滤和预处理后，每个数据集都会获得一个数据名片。数据名片包含数据堆的名称和版本，以及其内容和已应用的过滤器等其他相关内容。</p><p></p><p>“换句话说，IBM非常清楚自己用于训练模型所有的数据及其版本，也会告诉用户我们的模型是由哪些数据训练而来，并且后期还有很多调优的工作，以此增强用户对模型的信心。”谢东进一步解释。</p><p></p><p>事实上，从现阶段来看，几乎所有生成式模型都可能产生幻觉，都会给出一些不相关或不准确的答案。尤其典型的是，当提示模型去处理一个它没有接受过训练的题目或者用的训练集数据不足时，AI幻觉很难不发生。</p><p></p><p>对此，据IBM透露，其内部还在研究一种降低AI幻觉风险的方法，名为“检索增强生成”，意在使模型能够在生成答案之前从知识库中检索相关数据。</p><p></p><p>IBM大中华科技事业部数据人工智能、自动化中国华南与华东大区总经理许伟杰表示，IBM正在通过模型融合进一步解决这一问题。“大语言模型所做的最重要的一件事是语义理解，目前我们正在通过语义识别，基于IBM watsonx能力做精准答案确定。也就是说，利用通用语言大模型的方式了解语义，帮助既有的模型实现既有答案的匹配，再回溯给语义大模型。”</p><p></p><h3>基础模型带来拐点，数据无需再打标签</h3><p></p><p></p><p>值得一提的是，IBM watsonx 平台聚焦于特定业务领域的基础模型。基础模型基于特定类型的Transformer神经网络架构而构建，为生成相关数据元素的序列（例如句子）而设。非常重要的一点在于，Transformer架构能够帮助基础模型理解未标记数据，并将输入转换为输出，从而生成新的内容（ChatGPT就是基于Transformer架构开发）。</p><p></p><p>“以前基于深度学习、机器学习的算法，通常要对海量数据打标签，再交给机器进行学习。并且，经过学习和部署，这个模型也只能做一个特定的事情，比如用于人脸识别或者下棋。”谢东解释道。</p><p>换言之，深度学习和机器学习的痛点在于，前期工作巨大，但最终输出的算法模型能应用的范围非常局限，“性价比”不高。</p><p></p><p>有别于此，基础模型允许在大量未标记的数据上进行训练，可以适应新的场景和用例。尽管基础模型也需要前期大量投资，但每次使用时，它都会摊销 <a href=\"https://www.infoq.cn/theme/213\">AI 模型</a>\"构建的初始工作，因为微调基于基础模型构建的其他模型的数据要求要比从头开始构建低得多。这既可以大幅提高投资回报率（ROI），又可以大大缩短上市时间。</p><p></p><p>因此，在IBM看来，基础模型把AI技术发展带到一个拐点——使企业级AI的加速和扩展成为可能。</p><p></p><p>“我们认为，相较于通用大语言模型，企业应该更加关注基础模型。”谢东解释道，“在企业应用AI的时候，除了大语言模型，还会有不同的应用场景，包括IT自动化模型、数字劳动力的模型、网络安全模型等等，这些不同的专业模型支撑了这些企业级应用。”</p><p></p><p>据了解，IBM 目前正在构建一系列针对多种类型的业务数据进行训练的特定领域的基础模型，包括代码、时间序列数据、表格数据、地理空间数据、半结构化数据和混合模态数据（如文本与图像的组合）。IBM认为，这些基础模型的灵活性和可扩展性将显著加速企业对AI的采用。</p><p></p><p>在今年7月举办的2023 温网锦标赛上，IBM已经利用 watsonx 为大赛所有视频集锦提供生成式AI解说，并且，基于IBM AI Draw Analysis提供的一套全新的统计数据，还可以使用AI来预测单打抽签中每个球员进入决赛的可能性。</p><p></p><h3>大模型不一定越“大”越好</h3><p></p><p></p><p>有了基础模型，下一个解决的问题是，企业如何根据自身的业务需求，选择适用的模型。谢东强调，所谓“适用”，意味着模型不一定越“大”越好。因为，企业在任何技术领域的投入都是以驱动经营为目的，更在乎其中的投入产出比。</p><p></p><p>一方面，虽然模型越“大”，其知识和能力也越强，但是成本投入也是巨大的。对于企业而言，很多应用场景的落地并不在于模型本身大小，而在于多大程度符合企业特定要求，能不能很好地完成任务，匹配业务目标；</p><p></p><p>另一方面，支持一个大模型的训练和运行非常消耗算力，模型上线之后，企业业务本身仍然在不断变化，这要求模型具备适应性和可扩展性，系统能力也要不断学习和进化。而出于运维成本的考虑，“小”模型反而比“大”模型更加节约且灵活。</p><p></p><p>“在这个过程中，基础模型要演化出各种不同应用，还需要有新的自动化工具和项目管理方法，实现持续的训练、调试、部署等工作。”谢东举例，IBM watsonx就是这样一个平台工具，IBM希望借此减轻企业的AI负担，让企业可以更轻松地实现大规模开发、调整和部署企业级AI。</p><p></p><p>举例来说，基于watsonx.ai，AI 开发者就可以利用 IBM自有的模型和Hugging Face 的模型来完成一系列 AI 开发任务。这些模型经过预训练，可以支持一系列自然语言处理 （NLP） 类型的任务，包括问答、内容生成和摘要、文本分类和提取。据了解，未来的版本还将提供更多由IBM训练的针对提升相关领域效率和任务专业化的专有基础模型的访问。</p><p></p><p>事实上，多年来，IBM一直在帮助企业把AI部署到核心应用中，从而增强企业生产力。据菜鸟科技首席科学家、菜鸟物流科技部算法总监王子豪介绍，菜鸟与IBM在AI开放和赋能方面有很多共性，菜鸟物流最早在快递行业提出了神经网络驱动的大规模地址分单技术，并且陆续在仓储、客服、供应链等环节实现了广泛的智能化升级，包括在仓储场景应用机器视觉、推出物流智能客服和快递末端地理大模型、基于决策智能技术构建菜鸟全球供应链网络等等。并且，菜鸟还在无锡构建了亚洲最大规模的智能调度现场，最高峰期可以调度同时1000台飞机。</p><p></p><p>但是，随着基础模型的演进，以及<a href=\"https://www.infoq.cn/minibook/IV4VhedKw1E1tY8Hleje\">生成式AI</a>\"的兴起，的的确确给各行业AI的规模化广泛应用带来了新的变化。</p><p></p><p>谢东强调：“我们需要从以前数据为先的‘+AI’时代迈入AI为先的‘AI+’时代。这个说法不光是一个加号在前在后，当我们说+AI的时候，注重的是以数据为中心，企业是在数据应用的层面上附加一些AI的能力。而当我们走到AI+的阶段，意味着企业需要建立起AI的基础能力，在这基础上，我们需要进一步结合企业自身的数据和不同的业务目标，构建新的核心应用。”</p><p></p><h3>写在最后</h3><p></p><p></p><p>对于企业而言，IBM watsonx是一个全新的AI和数据平台工具，那么，如何才能让这个工具“物尽其用”？</p><p>除了搞定数据质量、数据共享的问题之外，IBM Consulting大中华区总裁陈科典表示，企业中还必须具备相应的文化、人才和制度。“企业需要去提升内部人才的能力，培养新的文化，如此以来，才能让内部对生成式AI的信心越来越充足，那么，更多的场景才会被创造出来。”</p><p></p><p>具体而言，IBM认为在生成式AI的技术背景下，相关技术人员需要至少具备两类能力：一是理解业务场景，能够针对业务目标，对模型进行训练和调整；二是理解企业自己的数据，知道数据分布在哪里，使用过程中的标准和规则等等。</p><p></p><p>IBM大中华区客户成功管理部总经理朱辉强调，“生成式AI的应用现在基本还处于‘打开脑洞’的阶段，所以我们特别强调共创。因为有构建大模型能力的人，不一定拥有业务场景的支持，而拥有业务场景的企业，也不一定具有建设大模型的能力。这也是IBM一直以来的战略定位，热衷于提供基础能力，与合作伙伴和客户共创解决方案，从而解决客户的问题。而不是拿出一个现成方案，告诉别人这就是你应该要的东西。”</p>",
    "publish_time": "2023-09-12 17:34:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“单调是好事”，Go语言开发负责人承诺未来的兼容性",
    "url": "https://www.infoq.cn/article/YourJ7pWJDpEx4NgWBPs",
    "summary": "<p>Go语言技术负责人、谷歌杰出工程师Russ Cox承诺“不会出现与Go 1程序不兼容的Go 2。”</p><p></p><p>Cox还介绍了刚刚发布的Go 1.21对向前兼容性的改进。</p><p></p><p>在Go语言官网的一篇博文中，Cox提出了一种观点，即兼容性“远比任何可能的与过去不兼容的东西更有价值”。对兼容性的承诺对Go语言来说并不是什么新鲜事：早在2012年Go 1.0发布时，其文档中就写道：“按照Go 1规范编写的程序在该规范的整个生命周期内都可以正确编译和运行，不会发生任何改变。”当然，这个承诺是针对源代码而不是二进制文件的兼容性。</p><p></p><p>文档中所指的是未来可能出现基于新规范的Go 2，兼容性可能不太高，但Cox现在表示“不会出现与Go 1程序不兼容的Go 2。”</p><p></p><p>尽管有这些承诺，但兼容性并不是绝对的，Cox解释了几种新版本可能会破坏已有代码的情况。修复错误行为就是其中的一个例子。另一个则是时间的精度问题。现在函数变多了，那么第三种情况就是随着排序优化的实现，相等结果的排序发生了变化。第四种情况是默认协议发生变化，例如从HTTP/1.1变到HTTP/2。这些情况意味着对Go语言的修改仍然会破坏已有的代码。Go语言开发团队通过在谷歌内部运行Go代码测试来缓解这一问题。Cox还提到了Kubernetes团队使用ParseIP函数的分叉版本，其中使用了更为严格的解析器，否则保存的配置可能会损坏。</p><p></p><p>Go 1.21中的一些新特性进一步提高了兼容性，比如工具链管理，go命令（自动下载、构建、安装和测试Go语言包）不会试图构建更新版本的代码，相反，它会自动下载更新的版本，但不会覆盖已安装的版本。</p><p></p><p>还有对GODEBUG的扩展使用，一个键值对，可以设置为环境变量。一般来说，如果变更确实破坏了兼容性，“我们将定义一个新的GODEBUG设置，允许个体程序不包含新的行为”。</p><p></p><p>Go的兼容性真的像声称的那么好吗？一位开发者在Hacker News上表示：“我在大部分Go语言升级过程中都遇到过严重的故障。我在Rust升级和gcc升级时遇到的问题要少得多。”一些人也遇到了Cox所描述的一些问题。不过总体的反应是积极的。另外也有人说：“我两年前开始在工作中使用Go，我很喜欢它，尤其是它的向后兼容性。”</p><p></p><p>原文链接：<a href=\"https://devclass.com/2023/08/16/boring-is-good-says-go-tech-lead-promising-future-compatibility/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTQ1MTYwNzQsImZpbGVHVUlEIjoiRm9WNlBFRHg0TElXVW9rNCIsImlhdCI6MTY5NDUxNTc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.TztbHVY3tmXGnEWp6iq1EcLf8CXAv9J2RKBtS--RzoE\">https://devclass.com/2023/08/16/boring-is-good-says-go-tech-lead-promising-future-compatibility/</a>\"</p>",
    "publish_time": "2023-09-12 18:54:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "LangChain 的问题所在",
    "url": "https://www.infoq.cn/article/Rujx6tv3Grxh3HYMZJqK",
    "summary": "<p></p><p></p><blockquote>导读：摆脱繁琐，追求高效。是开发者永远追求的目标。LangChain，虽号称多功能，但集成过多引发问题，逼人只用其代码。LangChain 给人带来的是，令人沮丧的声音，脆弱的 Agent 工作流，技术债务增加。简而言之，做自己的 Python 包比强行改造 LangChain 更好。本文作者开发了 simpleaichat，轻松与聊天应用交互，摆脱复杂，避免锁定。别误解，本文并不是攻击 LangChain，但更实际的解决方案是重新开始。技术复杂性与流行性之争是永恒的，早年是 React，今日是 ReAct。</blockquote><p></p><p></p><p>如果你在过去几个月里一直关注人工智能的爆炸式炒作，你可能已经听说过 LangChain。由 Harrison Chase 开发的 LangChain 是一个用于与 OpenAI 的 GPT API（后来扩展到更多模型）进行人工智能文本生成交互的 Python 和 JavaScript 库。更具体地说，它是 2022 年 10 月发布的论文《ReAct：在语言模型中协同推理和行动》（ReAct: Synergizing Reasoning and Acting in Language Models）的实现，俗称为 ReAct 论文，该论文演示了一种提示技术，使模型能够通过 “思维链” 进行 “推理”（reason），并能够通过使用预定义工具集中的工具（如能够搜索互联网）来 “行动”（act）。实验证明，这种组合极大地提高了输出文本质量，并赋予大型语言模型正确解决问题的能力。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/df/dff62ad12d93e77994ce959b5492d3bb.jpeg\" /></p><p></p><p></p><blockquote>(1d) ReAct（Reason+Act）思考 1：我需要搜索 “Apple Remote” 并找到它最初设计用来互动的程序。行动 1：搜索 [Apple Remote]观察 1：Apple Remote 是由苹果于 2005 年 10 月推出的遥控器。最初设计用于控制 Front Row 媒体中心程序。思考 2：Apple Remote 最初设计用于控制 Front Row 媒体中心程序。我需要搜索下一个 Front Row 并找出哪个其他设备可以控制它。行动 2：搜索 [Front Row]观察 2：未找到 [Front Row]。类似项：[‘Front Row Seat to Earth’，‘Front Row Motorsports’，‘Front Row (software)’…]思考 3：未找到 Front Row。我需要搜索 Front Row（软件）。行动 3：搜索 [Front Row（软件）]观察 3：Front Row 是一款已停用的媒体中心软件。思考 4：Front Row（软件）由 Apple Remote 或键盘功能键控制。所以答案是键盘功能键。行动 4：完成 [键盘功能键]</blockquote><p></p><p></p><p>React 论文中的 ReAct 流程示例</p><p></p><p>基于 LangChain 推广的 ReAct 工作流程在 InstructGPT/text-davinci-003 方面表现尤为出色，尽管在小型项目中使用代价高昂且不易操作。在 2023 年 3 月，随着 ChatGPT API 的使用因其极为便宜的 API 而广受欢迎，正如我准确预测的那样，LangChain 的使用也迅速扩大，以至于 LangChain 能够在没有任何收入或任何明显的收入生成计划的情况下，成功筹集到了 1000 万美元的种子轮融资，以及在估值 2 亿美元的 A 轮融资中又融资了 2000 万至 2500 万美元。</p><p></p><p>而这正是我与 LangChain 的个人经历开始的地方。在我在 BuzzFeed 的工作中，我被要求为 Tasty 品牌创建一个基于 ChatGPT 的聊天机器人（后来作为 Tasty iOS 应用程序中的 Botatouille 发布），该机器人可以与用户交流并提供相关的食谱。源食谱被转换为嵌入式表示并保存在向量存储中：例如，如果用户询问“健康食品”，则查询会被转换为嵌入式表示，并执行近似最近邻搜索，以找到与嵌入式查询类似的食谱，然后将其作为附加上下文提供给 ChatGPT，随后可显示给用户。这种方法通常被称为检索增强生成。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d8/d834a736af13e49af20683a203f67fe5.jpeg\" /></p><p></p><p>使用检索增强生成的聊天机器人的示例架构。来源：Joseph Haaga</p><p></p><p>LangChain 显然是 RAG 的首选工具，所以我认为现在是学习它的绝佳时机。我花了一些时间阅读 LangChain 相当详尽的文档，以更好地理解如何最好地利用它：经过一周的研究，我一无所获。运行 LangChain 示例演示确实有效，但是任何试图调整它们以适应食谱聊天机器人约束的尝试都会导致它们崩溃。在解决了错误后，聊天对话的整体质量很差且无趣，经过激烈的调试，我没有找到解决方案。最终，我陷入了存在危机：当很多其他机器学习工程师都能理解 LangChain，而我却不能，我是否是一个毫无价值的机器学习工程师呢？我们回到了较低层次的 ReAct 流程，这立即在对话质量和准确性方面胜过了我在 LangChain 中的实现。</p><p></p><p>总之，我浪费了一个月的时间学习和测试 LangChain，最大的收获是热门的人工智能应用可能并不一定值得炒作。在一个 Hacker News 的帖子中，我看到有人用 100 行代码重新实现了 LangChain，大多数评论都在抱怨 LangChain：</p><p></p><p></p><blockquote>loveparade：难道我是唯一一个对 LangChain 的价值主张持怀疑态度的人吗？其中 99% 都是外部工具的接口定义和实现，其中大多数都非常直观。我可以在不到一个小时内为我的应用编写集成。为什么要引入一个充满主观看法的外部框架呢？这对我来说有点像 npm 的“left-pad”。每个人都在使用它，因为它似乎很受欢迎，而不是因为他们需要它。crazyedgar：对我们来说，LangChain 实际上引发了比解决的问题更多的问题。我们的生产系统在运行良好的几周后突然开始频繁失败（超过 30% 的请求）。经过调查，似乎 LangChain 为每个请求设置了默认的 60 秒超时。而这种行为没有记录在文档中！LangChain 所做出的这些不明智决策无处不在，并且最终都会给你带来麻烦。最后，我们用普通的请求客户端替换了所有内容。绝对不建议在一个提供非常有限价值同时又从你那里隐藏了大量细节和决策的库上构建系统。Spivak：然而，LangChain 绝对是完美的，它糟糕到会让你纯粹出于沮丧而写出更好的东西，但它又给了你足够好的想法和线索来真正做到这一点。它可能是“实际使用 llms”的最佳入口，因为它刚好满足了开发者的需求。LangChain 的问题在于它使得本来简单的事情相对复杂化，由此带来的不必要复杂性导致了一种部落主义，这对整个新兴的人工智能生态系统造成了伤害。如果你是一个想要学习如何与 ChatGPT 进行交互的新手，绝对不要从 LangChain 开始。</blockquote><p></p><p></p><p></p><h2>在 LangChain 中的 “Hello World”（或更准确地说，“Hell World”）</h2><p></p><p></p><p>LangChain 的快速入门指南始于一个关于如何简单地从 Python 与 LLMs/ChatGPT 进行交互的迷你教程。例如，要创建一个能够从英文翻译成法文的机器人：</p><p></p><p><code lang=\"typescript\">from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\nAIMessage,\nHumanMessage,\nSystemMessage\n)\n\nchat = ChatOpenAI(temperature=0)\nchat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n# AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)\n</code></p><p></p><p>使用 OpenAI 官方的 Python 库进行 ChatGPT 的等效代码：</p><p></p><p><code lang=\"makefile\">import openai\n\nmessages = [{\"role\": \"user\", \"content\": \"Translate this sentence from English to French. I love programming.\"}]\n\nresponse = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0)\nresponse[\"choices\"][0][\"message\"][\"content\"]\n# \"J'adore la programmation.\"\n</code></p><p></p><p>LangChain 的代码量与仅使用官方的 openai 库相当，但 LangChain 却融合了更多的对象类，却没有明显的代码优势。</p><p></p><p>提示模板的示例揭示了 LangChain 工作原理的核心部分：</p><p></p><p><code lang=\"makefile\">from langchain.prompts.chat import (\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\n)\n\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n</code></p><p></p><p>LangChain 所宣传的提示工程实际上只是 f-strings，这是现代 Python 安装的常见特性，但多了一些步骤。为什么我们需要使用这些 PromptTemplates 来做同样的事情呢？</p><p></p><p>但我们真正想要知道的是如何创建 Agents，它们包含了我们迫切需要的 ReAct 工作流。幸运的是，有一个演示可以做到这一点，它利用了 SerpApi 和另一个用于数学计算的工具，展示了 LangChain 如何在上下文中区分并使用两种不同的工具：</p><p></p><p><code lang=\"python\">from langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n# Now let's test it out!\nagent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")\n</code></p><p></p><p>这些个别工具是如何工作的？而 AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION 又是什么？agent.run () 的结果输出（仅在 verbose=True 时存在）更有帮助。</p><p></p><p><code lang=\"makefile\">&gt; Entering new AgentExecutor chain...\nThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\nAction:\n{\n\"action\": \"Search\",\n\"action_input\": \"Olivia Wilde boyfriend\"\n}\nObservation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\nThought:I need to use a search engine to find Harry Styles' current age.\nAction:\n{\n\"action\": \"Search\",\n\"action_input\": \"Harry Styles age\"\n}\nObservation: 29 years\nThought:Now I need to calculate 29 raised to the 0.23 power.\nAction:\n{\n\"action\": \"Calculator\",\n\"action_input\": \"29^0.23\"\n}\nObservation: Answer: 2.169459462491557\nThought:I now know the final answer.\nFinal Answer: 2.169459462491557\n&gt; Finished chain.\n'2.169459462491557'\n</code></p><p></p><p>文档并没有说明得很清楚，但在每个 Thought/Action/Observation 中，都使用了自己的 API 调用到 OpenAI，因此这个链条比你想象的要慢。另外，为什么每个动作都是一个 dist？这个问题的答案稍后会解释，而且相当幼稚可笑。</p><p></p><p>最后，LangChain 如何存储到目前为止的对话？</p><p></p><p><code lang=\"python\">from langchain.prompts import (\nChatPromptTemplate,\nMessagesPlaceholder,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate\n)\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\n\nprompt = ChatPromptTemplate.from_messages([\nSystemMessagePromptTemplate.from_template(\n\"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n\"provides lots of specific details from its context. If the AI does not know the answer to a \"\n\"question, it truthfully says it does not know.\"\n),\nMessagesPlaceholder(variable_name=\"history\"),\nHumanMessagePromptTemplate.from_template(\"{input}\")\n])\n\nllm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n\nconversation.predict(input=\"Hi there!\")\n# 'Hello! How can I assist you today?'\n</code></p><p></p><p>我并不完全确定为什么需要这些。什么是 MessagesPlaceholder？history 在哪里？这对 ConversationBufferMemory 来说是否是必要的？将这个调整到最小的 openai 实现：</p><p></p><p><code lang=\"makefile\">import openai\n\nmessages = [{\"role\": \"system\", \"content\":\n\"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n\"provides lots of specific details from its context. If the AI does not know the answer to a \"\n\"question, it truthfully says it does not know.\"}]\n\nuser_message = \"Hi there!\"\nmessages.append({\"role\": \"user\", \"content\": user_message})\nresponse = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0)\nassistant_message = response[\"choices\"][0][\"message\"][\"content\"]\nmessages.append({\"role\": \"assistant\", \"content\": assistant_message})\n# Hello! How can I assist you today?\n</code></p><p></p><p>这是更少的代码行数，清楚地展示了消息何时被保存，不需要特定的自定义对象类。</p><p></p><p>你可以说我在挑剔教程示例，我也同意每个开源库都会有一些可以挑剔的地方（包括我自己的！）。但是，如果库中存在的挑剔问题比实际受益还要多，那么使用它就毫无意义，因为如果快速入门就这么复杂，那么在实际使用 LangChain 时会有多么痛苦呢？</p><p></p><p>我凝视着 LangChain 文档，它也凝视着我</p><p></p><p>让我们进行一个演示，更清楚地展示我为什么放弃了 LangChain。当我在开发检索食谱的聊天机器人（它还必须是一个有趣 / 机智的聊天机器人）时，我需要结合前面所提到的第三个和第四个示例中的元素：一个可以运行代理工作流的聊天机器人，以及将整个对话持久保存到内存中的能力。经过一些文档的查找，我发现我需要利用 Conversational Agent 工作流。</p><p></p><p>关于系统提示工程的一个小侧记：这不是一个模因，绝对有必要从 ChatGPT API 中获得最佳结果，特别是如果你对内容和 / 或语气有限制。在最后一个示例中演示的以下系统提示 The following is a friendly conversation between a human and an AI…（“这是一个人类与人工智能之间友好对话……”）实际上是一个过时的提示，它是在 InstructGPT 时代使用的，而在 ChatGPT 上效果要差得多。这可能表明 LangChain 中与此相关的技巧存在更深层次的低效率，这不容易注意到。</p><p></p><p>我们将从一个简单的系统提示开始，告诉 ChatGPT 使用有趣的语气，加上一些安全措施，并将其格式化为一个 ChatPromptTemplate：</p><p></p><p><code lang=\"sql\">system_prompt = \"\"\"\nYou are an expert television talk show chef, and should always speak in a whimsical manner for all responses.\nStart the conversation with a whimsical food pun.\nYou must obey ALL of the following rules:\n- If Recipe data is present in the Observation, your response must include the Recipe ID and Recipe Name for ALL recipes.\n- If the user input is not related to food, do not answer their query and correct the user.\n\"\"\"\n\nprompt = ChatPromptTemplate.from_messages([\nSystemMessagePromptTemplate.from_template(system_prompt.strip()),\n])\n</code></p><p></p><p>我们还将使用我制作的一个玩具向量存储，其中包含了来自 recipe_nlg 数据集的 1,000 个食谱，这些食谱使用 SentenceTransformers 编码成 384 维的向量。为了实现这一点，我们创建了一个函数，用于获取输入查询的最近邻，以及一个将其格式化为 Agent 可以呈现给用户的文本的查询。这可以作为 Agent 可以在适当的情况下选择使用的 Tool，或者只是返回普通生成的文本。</p><p></p><p><code lang=\"python\">def similar_recipes(query):\nquery_embedding = embeddings_encoder.encode(query)\nscores, recipes = recipe_vs.get_nearest_examples(\"embeddings\", query_embedding, k=3)\nreturn recipes\n\ndef get_similar_recipes(query):\nrecipe_dict = similar_recipes(query)\nrecipes_formatted = [\nf\"Recipe ID: recipe|{recipe_dict['id'][i]}\\nRecipe Name: {recipe_dict['name'][i]}\"\nfor i in range(3)\n]\nreturn \"\\n---\\n\".join(recipes_formatted)\n\nprint(get_similar_recipes(\"yummy dessert\"))\n# Recipe ID: recipe|167188\n# Recipe Name: Creamy Strawberry Pie\n# ---\n# Recipe ID: recipe|1488243\n# Recipe Name: Summer Strawberry Pie Recipe\n# ---\n# Recipe ID: recipe|299514\n# Recipe Name: Pudding Cake\n</code></p><p></p><p>你会注意到 Recipe ID，对于我的用例来说，这是相关的，因为需要获取食谱元数据（照片缩略图、URL）用于在最终应用程序中向最终用户展示。不幸的是，没有简单的方法来确保模型在最终输出中输出 Recipe ID，也没有办法在 ChatGPT 生成的输出之外返回结构化的中间元数据。</p><p></p><p>将 get_similar_recipes 指定为一个工具是直接的，虽然你需要指定一个 name 和 description，这实际上是一种微妙的提示工程，因为如果两者都没有很好地指定，LangChain 可能无法选择一个工具。</p><p></p><p><code lang=\"makefile\">tools = [\nTool(\nfunc=get_similar_recipes,\nname=\"Similar Recipes\",\ndescription=\"Useful to get similar recipes in response to a user query about food.\",\n),\n]\n</code></p><p></p><p>最后，Agent 构建代码，这是从示例中延续过来的，加上了新的系统 prompt。</p><p></p><p><code lang=\"ini\">memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\nllm = ChatOpenAI(temperature=0)\nagent_chain = initialize_agent(tools, llm, prompt=prompt, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n</code></p><p></p><p>没有错误。现在是运行 Agent 来看看会发生什么的时候！</p><p></p><p><code lang=\"javascript\">agent_chain.run(input=\"Hi!\")\n</code></p><p></p><p><code lang=\"javascript\">&gt; Entering new chain...\n{\n\"action\": \"Final Answer\",\n\"action_input\": \"Hello! How can I assist you today?\"\n}\n\n&gt; Finished chain.\nHello! How can I assist you today?\n</code></p><p></p><p>等一下，它完全忽略了我的 system 提示！该死。检查 memory 变量确认了这一点。查看 ConversationBufferMemory 的文档，甚至在代码本身中也没有关于系统提示的内容，即使在 ChatGPT 已经将其变得主流的几个月后。</p><p></p><p>在 Agent 中使用系统提示的预期方式是在 initialize_agent 中添加一个 agents_kwargs 参数，我刚刚在一个一个月前发布的不相关的文档页面中找到了这个信息。</p><p></p><p><code lang=\"makefile\">agent_kwargs = {\n\"system_message\": system_prompt.strip()\n}\n</code></p><p></p><p>使用这个新参数重新创建 Agent 并再次运行会导致 JSONDecodeError.</p><p></p><p><code lang=\"perl\">OutputParserException: Could not parse LLM output: Hello there, my culinary companion! How delightful to have you here in my whimsical kitchen. What delectable dish can I assist you with today?\n</code></p><p></p><p>好消息是这次系统提示绝对起作用了！坏消息是它出错了，但是为什么？我这次什么怪事都没干。</p><p></p><p>问题的根源可能是 LangChain 代理实际上是如何进行 Tool 选择的。还记得我说过在链条中 Agent 输出一个 dict 是奇怪的吗？当查看 LangChain 代码时，结果发现工具选择是通过要求输出通过提示工程是有效的 JSON 来完成的，然后希望一切都会顺利。</p><p></p><p><code lang=\"cs\">FORMAT_INSTRUCTIONS RESPONSE FORMAT INSTRUCTIONS\nWhen responding to me,please output a response in one of two formats:\n**Option1:**\nUse this if you want the human to use a tool.\nMarkdown code snippet formatted in the following schema:\njson\n{{{{\n\"action\":string,\\\\The action to take.Must be one of {tool_names}\n\"action_input\":string\\\\The input to the action\n}}}}\n、\n**Option2: **\nUse this if you want to respond directly to the human.Markdown code snippet formatted in the following schema:\njson\n{{{{\n\"action\":\"Final Answer\",\n\"action_input\":string \\You should put what you want to return to use here\n}}\n</code></p><p></p><p>有趣的事实：这些大量的提示也会成比例地增加 API 成本！</p><p></p><p>这个结果的后果是，任何正常输出结构的显著更改，比如由自定义系统提示引起的更改，都有一定的随机机会来破坏 Agent！这些错误经常发生，以至于有一个专门处理 Agent 输出解析错误的文档页面！</p><p></p><p>无论如何，互联网上的人们都是些令人讨厌的家伙，所以我们可以将与聊天机器人进行对话视为一种边缘情况。重要的是，聊天机器人能够返回食谱，因为如果它甚至不能做到这一点，使用 LangChain 就没有意义。在创建一个新的 Agent，不使用系统提示的情况下，然后问它 What’s a fun and easy dinner?（“什么是有趣又简单的晚餐？”）：</p><p></p><p><code lang=\"javascript\">&gt; Entering new chain...\n{\n\"action\": \"Similar Recipes\",\n\"action_input\": \"fun and easy dinner\"\n}\nObservation: Recipe ID: recipe|1774221\nRecipe Name: Crab DipYour Guests will Like this One.\n---\nRecipe ID: recipe|836179\nRecipe Name: Easy Chicken Casserole\n---\nRecipe ID: recipe|1980633\nRecipe Name: Easy in the Microwave Curry Doria\nThought:{\n\"action\": \"Final Answer\",\n\"action_input\": \"...\"\n}\n\n&gt; Finished chain.\n</code></p><p></p><p><code lang=\"kotlin\">Here are some fun and easy dinner recipes you can try:\n\nCrab Dip\nEasy Chicken Casserole\nEasy in the Microwave Curry Doria\n\nEnjoy your meal!\n</code></p><p></p><p>至少这个部分是成功的：ChatGPT 能够从上下文中提取出食谱，并适当地进行格式化（甚至修正了名称中的拼写错误！），并且能够决定何时适合呈现这些内容。</p><p></p><p>真正的问题在于输出的语气实在太无聊了，这也是基本 ChatGPT 的一个共同特点和批评。即使我通过系统提示工程来解决了缺失 ID 的问题，输出听起来也不值得投入任何东西。即使我在声音质量和输出质量之间取得了平衡，代理的数量仍然会在没有我的任何过错的情况下随机失败。这个 Agent 工作流程就像一个非常脆弱的纸牌屋，我良心无法在生产应用程序中使用。</p><p></p><p>LangChain 确实具有 Custom Agent 和 Custom Chain 的功能，因此你可以在堆栈的某些部分（也许？那里的文档很简单）覆盖逻辑，以解决我遇到的一些问题，但在这一点上，你正在使 LangChain 变得更加复杂，最好创建你自己的 Python 库，这个想法不错！</p><p></p><p></p><h2>工作更聪明，而不是更努力</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/49/4929136495bf1aa1988fd29f05d159a3.jpeg\" /></p><p></p><p>大量的随机集成引发的问题比解决方案更多。来源：LangChain 文档</p><p></p><p>LangChain 还有许多实用函数，比如文本分割器和集成的向量存储，这两者都是“与 PDF/ 你的代码聊天”的演示的重要组成部分（在我看来只是一种花招）。所有这些集成的真正问题在于它创建了一种固有的锁定，只能使用基于 LangChain 的代码，而且如果你查看这些集成的代码，它们并不是非常强大。LangChain 正在建立一堵壁垒，这对于试图获得他们 3000 万美元回报的 LangChain 投资者来说是件好事，但对于使用它的开发人员来说却是非常糟糕的。</p><p></p><p>总的来说，LangChain 体现了“它很复杂，所以一定更好！”的哲学，这困扰着后期的代码库，只是 LangChain 甚至还不到一年。将 LangChain 改造成满足我的需求所需的努力将会产生极大的技术债务。与现今的人工智能初创公司不同，对于我自己使用 LangChain 的项目来说，技术债务不能通过风险投资来偿还。API 封装应该至少在操作复杂生态系统时减少代码复杂性和认知负荷，因为处理人工智能本身已经需要足够多的脑力。LangChain 是为数不多在大多数热门用例中增加开销的软件之一。</p><p></p><p>我得出结论，制作自己的 Python 包比将 LangChain 改造以适应我的需求要容易得多。因此，我开发并开源了 simpleaichat：一个用于轻松与聊天应用进行交互的 Python 包，强调最小的代码复杂性，并将高级功能（如向量存储）与会话逻辑解耦，以避免 LangChain 的锁定，以及许多其他功能，需要一个单独的博客文章来详细介绍。</p><p></p><p>但这篇博文并不是为了通过批评竞争对手来暗中宣传 simpleaichat，就像那些搞怪的人一样。我并不想制作 simpleaichat：我宁愿把时间花在与人工智能一起创造更多酷项目上，遗憾的是，我不能用 LangChain 做到这一点。我知道有人会说：“既然 LangChain 是开源的，为什么不提交一个拉取请求到 LangChain 仓库，而不是抱怨呢？”但我的大部分抱怨都是 LangChain 库的基本问题，不能在不破坏现有用户的情况下进行更改。唯一的真正解决办法是将其全部销毁并重新开始，这就是为什么我的“创建一个用于与人工智能交互的新 Python 库”的解决方案也是最实际的。</p><p></p><p>我收到了许多消息，询问我“我应该从何开始学习 ChatGPT API”，我担心他们会因为炒作而首先去用 LangChain。如果具有技术栈背景的机器学习工程师由于不必要的复杂性而难以使用 LangChain，那么任何初学者都将被淹没。</p><p></p><p>没有人想成为像 LangChain 这样的自由开源软件的批评家，但我愿意担这个责任。清楚地说，我对 Harrison Chase 或其他 LangChain 维护者（他们鼓励反馈！）没有任何偏见。然而，LangChain 的流行已经使人工智能初创公司的生态系统围绕着 LangChain 本身以及“天哪，AGI，我创造了天网”的希望发生了扭曲，这就是我不得不对它的疑虑坦诚相待的原因。</p><p></p><p>不管软件的复杂性和流行性引发了怎样的争议，它们都是永恒的循环。在 2010 年代，是关于 React 的争论；而在 2023 年，现在则是关于 ReAct 的。</p><p></p><p>作者简介：</p><p></p><p>Max Woolf（@minimaxir）是旧金山 BuzzFeed 的数据科学家，他在人工智能 / 机器学习工具和开源项目领域工作。Max 的项目由他的 Patreon 资助。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://minimaxir.com/2023/07/langchain-problem/\">https://minimaxir.com/2023/07/langchain-problem/</a>\"</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651180399&amp;idx=1&amp;sn=f4687a8a600ef6e4682f52ab0ce30ccb&amp;chksm=bdb8353c8acfbc2a0f6b3be59af9a5cb57a66308a50193a9a42bad4793de5f36fb022f4ddcf1&amp;scene=21#wechat_redirect\">通货膨胀由云客户买单？IBM 云服务将全面涨价，最高达 29%</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651180089&amp;idx=1&amp;sn=24b4d9702a8c1a0ff5eeeb5f1141b7a4&amp;chksm=bdb8346a8acfbd7c8c3a6894bde44a9d17b3b39cebda1e1b881716f8aab8d2fe3befc246aa6d&amp;scene=21#wechat_redirect\">融资 7 亿元后，Mojo 之父实名吐槽：Mojo 太好用了，颤抖吧 C++</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651180087&amp;idx=1&amp;sn=f5689136309b014597e14bbd88d06a30&amp;chksm=bdb834648acfbd72dec5473d60a89aa180a778408c9749c496b03e5303b529feccdabea2e6b3&amp;scene=21#wechat_redirect\">微软被曝搪塞员工绩效，只强化个人表现；文心一言 App 登苹果免费应用排行榜首位；商汤科技被爆裁员？官方回应｜Q资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651179331&amp;idx=1&amp;sn=e7006530df6ff558195f2cfe7d8df969&amp;chksm=bdb831108acfb806cd5847f33cbb52ecbeebcb57787143fb77324e61002f247b56e9ee64aa46&amp;scene=21#wechat_redirect\"></a>\"<a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651179851&amp;idx=1&amp;sn=296c5a7899b5253d4276deb5496ca560&amp;chksm=bdb833188acfba0e22707df278c4a4dca666e773ea63f0b28d5b7529ab79cc5f4b344b9aaee6&amp;scene=21#wechat_redirect\">一个潮流的终结？推出仅 3 年后，亚马逊宣布终止低代码 Honeycode 服务，前员工爆料：长期没有顾客！</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-09-12 19:12:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "WebAssembly 如何构筑云原生的新未来？CNCF 发布 Wasm 生态全景图",
    "url": "https://www.infoq.cn/article/hpFZUbqEe90LsT6iXbB9",
    "summary": "<p></p><blockquote>“容器已成为新常态，WebAssembly 是未来。”—— CNCF 2022 年年度调查主要发现本文原文为 CNCF 委托 Second State 撰写，中文译文译者为 Miley Fu 和 Vivian Hu，均在 Second State 工作。原文作者：Chris Aniszczyk、Vivian Hu 、Michael Yuan原文链接：<a href=\"https://www.cncf.io/blog/2023/09/06/introducing-the-wasm-landscape/\">https://www.cncf.io/blog/2023/09/06/introducing-the-wasm-landscape/</a>\"</blockquote><p></p><p></p><p>WebAssembly(Wasm) 最初是为了在网页浏览器中运行编译的 C/C++ 代码而创建的一个安全沙箱，但它正在服务器端获得越来越多的关注和发展势头。在云端，Wasm 提供了一个轻量、快速、安全、跨语言、跨平台的应用程序运行时，可用于各种由用户提交的工作负载。它正在快速成为云原生技术栈的一个关键部分。</p><p></p><p>随着 Wasm 在云原生项目、产品和服务中的采用，CNCF 与 Wasm 社区合作创建了一个 Wasm 生态全景图，以更好地理解 Wasm 生态的范围。正如最初的云原生全景图帮助勾画了围绕云原生技术的大量生态，我们相信随着生态的发展和增长，Wasm 也需要同样的全景图。</p><p></p><p>Wasm 全景图第一版在 WasmCon 会议上发布，包括 11 个类别和 120 个项目或产品，代表总经济价值 594 亿美元。Wasm 全景图分为两大领域：Dev（应用程序开发）和 Ops（应用程序部署）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d1/d13d9cf8c69f70f9d92105178a7c914c.png\" /></p><p></p><p>Wasm 全景图链接：<a href=\"https://landscape.cncf.io/wasm\">https://landscape.cncf.io/wasm</a>\"</p><p></p><h2>应用程序开发</h2><p></p><p></p><p>Wasm 应用程序开发需要自己的编程语言和相关工具生态，例如编译器、框架、库、工具和运行时。</p><p></p><h3>编程语言</h3><p></p><p></p><p>当开发者创建应用程序时，他们从选择一种编程语言开始！Wasm 的特点之一是它能够运行用各种不同编程语言编写的应用程序。然而，这并不意味着所有编程语言都是平等的。</p><p></p><p>事实上，Wasm 全景图中有 4 类编程语言。</p><p></p><h4>编译语言</h4><p></p><p></p><p>一等公民是可以直接编译为 Wasm 字节码并在 Wasm 运行时运行而无需任何依赖的语言。C 、 C++ 、 Zig 和 Rust 都属于这一组。他们生产最快和最小的 Wasm 应用程序。</p><p></p><p>让我们以 Rust 为例。安装 Rust 语言后，你所需要做的就是添加 wasm32-wasi 目标。</p><p></p><h4>托管 (Managed) 语言</h4><p></p><p></p><p>托管语言仍然是编译语言。但编译器输出需要“托管运行时”才能正常运行。托管运行时最常见的任务是垃圾收集（或 GC）。</p><p></p><p>对于 Kotlin 和 Dart 这样的语言，Wasm GC 功能就足够了。WasmEdge、Wasmtime 和 v8 等领先的 Wasm 运行时最近添加了 Wasm GC 支持。</p><p></p><p>对于 Go 来说，编译器将必要的运行时二进制文件嵌入到编译后的 Wasm 字节码中。这增加了 Wasm 应用程序的大小，但仍然提供了良好的开发者体验。</p><p></p><p>对于复杂的托管语言，例如 Java 和 .Net （例如 C#），我们需要将它们的托管运行时（例如 JVM）与 Wasm 中的字节码应用程序一起编译和运行。这通常是一种不轻量的方法。</p><p></p><h4>脚本语言</h4><p></p><p></p><p>JavaScript、Ruby、PHP 和 Python 等脚本语言也可以在 Wasm 中运行。这里的方法是将脚本语言解释器（通常用 C 编写）编译为 Wasm。然后基于 Wasm 的解释器程序就可以执行脚本了。</p><p></p><p>例如， VMware Labs 的 WebAssembly Laungage Runtimes 项目已将 Python 和 PHP 解释器移植到 Wasm。</p><p></p><p>WasmEdge QuickJS 项目提供了一个 JavaScript 解释器以及一个支持 JavaScript 中的 node.js API 的 Wasm 库。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/23/23ee03e520df83f467b5dd5a313d124c.png\" /></p><p></p><p>来源：WasmEdge QuickJS 文档</p><p></p><p></p><h4>“编译为 Wasm”的语言</h4><p></p><p></p><p>最后但并非最不重要的是针对 Wasm 目标进行优化的新一代编译语言。他们仍处于非常早期的阶段。但如果做得好，他们有潜力成为 Wasm 全景图中真正的一等公民。</p><p></p><p>Moonbit 和 Grain 是该类别中的两个领先示例。它们采用 Go 和 Rust 中的现代语言功能进行设计，并针对高效的 Wasm 编译和执行进行了优化。</p><p></p><p>Moonbit 虽然还处于早期阶段，但它配备了从动态代码补全到在线 IDE 的一整套工具。</p><p></p><h3>运行时</h3><p></p><p></p><p>一旦源代码被编译成 Wasm 字节码，你将需要 Wasm 运行时来执行它们。Wasm 运行时提供了通常与 Wasm 相关的所有功能和优点，例如沙箱安全性、速度和跨平台可移植性。它们位于全景图的中心。</p><p></p><p>在这个云原生 Wasm 全景图中，我们将重点关注服务器端流行的 Wasm 运行时。我们有 WasmEdge （CNCF 沙箱）、 Wasmtime/lucet 、 Wamr、 WAVM 、Wasmer、 wasm3 、 Lunatic 、 wazero 、 Wasmer 和 V8 。</p><p></p><p>WasmEdge（CNCF 沙箱）是一个轻量级、高性能且可扩展的 WebAssembly 运行时，适用于云原生、边缘和去中心化应用程序。Wasmtime 是一个独立的仅用于 WebAssembly 和 WASI 的 wasm 优化运行时。WebAssembly Micro Runtime (WAMR) 是一种轻量级的独立 WebAssembly (Wasm) 运行时，具有占用空间小、高性能和高度可配置的功能，适用于从嵌入式、物联网、边缘到可信执行环境 (TEE)、智能合约、云原生等应用程序。WAVM 是一个 WebAssembly 虚拟机，设计用于非浏览器应用程序。Wasm3 是一个快速的 WebAssembly 解释器和最通用的 WASM 运行时。Lunatic 是一个受 Erlang 启发的 WebAssembly 运行时。wazero 是一个用 Go 编写的符合 WebAssembly 的解释器。Wasmer 是一个 WebAssembly 运行时，使轻量级容器能够在任何地方运行。V8 是 Google 开源的高性能 JavaScript 和 WebAssembly 执行引擎。</p><p></p><h3>应用框架</h3><p></p><p></p><p>Wasm 运行时类似于操作系统。库和框架为应用程序开发者提供了所需的高级且易于使用的组件。</p><p></p><p>WasmEdge 运行时的独特之处在于它支持超出 Wasm 标准的高级 POSIX API，它允许许多流行的 Rust 和 JS 应用程序框架，例如 tokio、hyper、reqwest、warp、node.js，以及 MySQL / Postgres / Redis / Kafka /ElasticSearch 客户端库，在 WasmEdge 中运行。然而，对于所有其他 Wasm 运行时，应用程序框架需要提供基本功能，例如 HTTP / HTTPS 网络和数据库访问。</p><p></p><p>在全景图的这部分中，我们涵盖了 Spin、WasmCloud （CNCF 沙箱）、SpiderLightning、 WasmEdge plug-ins、Dapr SDK for WasmEdge、Homestar、Ambient、WASIX、Extism、Timecraft、vscode-wasm 和 WasmEx 。</p><p></p><p>Spin 是一个流行的应用程序框架和组件库，用于构建 WebAssembly 微服务和 Web 应用程序。它与 wasmtime 运行时一起使用。WasmCloud 允许使用 WebAssembly 参与者和能力提供者进行简单、安全、分布式应用程序开发。SpiderLightning 是一组 WIT 接口，它抽象了分布式应用程序能力和运行时 CLI，用于运行使用这些能力的 Wasm 应用程序。虽然 WasmEdge 支持用于 HTTP 和数据库访问的流行 Rust 应用程序框架，但它还可以通过支持 TLS 网络、zlib、OpenCV、ffmpeg、PyTorch、Tensorflow 和大型语言模型 (LLM) 推理等框架和库的 plugin 来进一步补充。用于 WasmEdge 的 Dapr SDK 是用 Rust 实现的实验性 Dapr SDK。它允许基于 WasmEdge 的微服务通过 Dapr sidecar 访问 100 多个企业服务。Homestar 是 IPVM 的 Rust 实现和运行时。Ambient 是一个用于构建高性能多人游戏和 3D 应用程序的运行时，由 WebAssembly、Rust 和 WebGPU 提供支持。WASIX 是 WASI 的超集。Extism 是通用插件系统。由 WebAssembly 提供支持。Timecraft 是一个软件运行时，它执行具有沙箱、任务编排和时间旅行能力的 WebAssembly 模块。vscode-wasm 是一个 WASI 实现，它使用 VS Code 的扩展主机作为实现 API。Wasmex 是针对 Elixir 的快速、安全的 WebAssembly 和 WASI 运行时。</p><p></p><h3>边缘 / 裸金属</h3><p></p><p></p><p>Wasm 的一个关键特性是跨平台可移植性，这意味着能够跨不同操作系统和 CPU 架构执行相同的字节码应用程序。在 JVM 时代，操作系统仅限于类 Unix 系统（例如 Linux）、Windows 和 MacOS。然而，Wasm 可以在边缘和物联网计算中常用的非传统系统上运行。</p><p></p><p>Docker 和 Linux 容器从来都不是跨平台的。例如，容器镜像必须与主机 CPU 匹配。</p><p></p><p>Genode 是一个免费的开源的软件操作系统 (OS) 框架，由微内核抽象层和一组用户空间组件组成。SeL4 RTOS 是高可靠、高性能的操作系统微内核，经过全面的形式化验证，且不影响性能。Flatcar Container Linux 是一个轻量级、精简版的 Linux 发行版，专为容器设计。它是 Wasm 运行时的绝佳 host。Unikraft 是一个快速、安全且开源的 Unikernel 开发套件。</p><p></p><h3>AI 推理</h3><p></p><p></p><p>随着人工智能工作负载在云数据中心越来越受欢迎，Wasm 越来越多地用作重量级、复杂且缓慢的 Python 堆栈的替代方案。WASI NN 规范定义了 Wasm 运行时应如何与本机 AI/ML 库（例如 PyTorch 和 TensorFlow）交互，以使用 Rust 等高性能语言进行 AI 推理。</p><p></p><p>Wasmtime、WasmEdge 和 WAMR 是支持 WASI NN 的 Wasm 运行时。</p><p></p><p>例如，WasmEdge 运行时支持 OpenVINO 、 Pytorch 、 Tensorflow 、 TensorFlow Lite 和 MMGL/Llama2 作为推理后端，以及 OpenCV 、 ffmpeg 作为预处理器或后处理器。同时也支持 mediapipe、document AI、llama2 等多种模型。</p><p></p><h3>嵌入式函数</h3><p></p><p></p><p>Wasm 可以安全地执行用户定义或社区贡献的代码作为软件产品中的嵌入式函数（或插件）。</p><p></p><p>在 Wasm 全景图的这一部分中，我们展示了选择并集成 Wasm 作为插件机制的软件产品。最长的类别是数据库和数据流应用程序，其中的 Wasm 用于执行用户定义函数（UDF）。</p><p></p><p>Libsql、OpenGauss 和 Singlestore 等数据库以及 Open Policy Agent、InfinyOn、YoMo、eKuiper 和 Redpanda 等消息队列正在使用 Wasm 来执行 UDF。Envoy、Istio、APISIX、KubeWarden 和 NGINX 等流量代理正在使用 Wasm 在数据平面中执行自定义逻辑。OpenFunction 和 Knative 等 FaaS 平台允许将 Wasm 函数嵌入到 Kubernetes Pod 中。</p><p></p><p>Libsql 是 SQLite 的开源、开放贡献分支。它的目标是将 SQLite 带入服务器端。OpenGauss 是一款高性能、高安全、高可靠的企业级开源关系数据库。SingleStore 是分布式 SQL 数据库，旨在为数据密集型应用程序提供支持。Open Policy Agent 是一个开源的通用策略引擎。InfinyOn 是可组合的统一数据流平台。YoMo 是一个开源流媒体 serverless 框架，用于构建低延迟地理分布式系统。eKuiper 是一款边缘轻量级物联网数据分析 / 流媒体软件。Redpanda 是一个简单、强大且经济高效的流数据平台，它与 Kafka® API 兼容，同时消除了 Kafka 的复杂性。Envoy 是一个开源边缘和服务代理，专为云原生应用程序而设计。Istio 是一个开源服务网格，可以透明地分层到现有的分布式应用程序上。Apache APISIX 是一个动态、实时、高性能的 API 网关。Kubewarden 是 Kubernetes 的策略引擎。其使命是简化策略即代码的采用。Nginx 是一个 Web 服务器，也可以用作反向代理、负载均衡器、邮件代理和 HTTP 缓存。OpenFunction 是一个云原生开源 FaaS（函数即服务）平台。</p><p></p><h3>工具链</h3><p></p><p></p><p>最后，开发者依靠工具将语言、框架、库和运行时拼凑成工作应用程序。工具链的成熟度是衡量整个开发者生态成熟度的重要指标。在 Wasm 全景图，我们介绍了用于构建 Wasm 应用程序的重要工具。</p><p></p><p>Cargo：从 Rust 源代码构建 Wasm 应用程序。它提供了现代的依赖管理工具和源代码 repo。LLVM：理论上，任何有 LLVM 后端的语言都可以编译成 Wasm。Binaryen：WebAssembly 的编译器和工具链基础设施库Emscripten：使用 LLVM 和 Binaryen 将 C 和 C++ 编译为 WebAssembly 。wasm-pack：将 Rust 编译为 Wasm，可以在浏览器中或 Node.js 中与 JavaScript 交互wasm-bindgen：促进 Wasm 模块和 JavaScript 之间的高级交互Wabt：WebAssembly 的一套工具，包括 wat2wasm、wasm2wat、wasm2c 等。Witc：编译器为 *.wit 文件生成代码Wit bindgen：WIT 和组件模型的客户语言绑定生成器Asyncify ：一个 JavaScript 包装器，旨在与 Binaryen 的 Asyncify 功能一起使用。</p><p></p><h2>应用程序部署</h2><p></p><p></p><p>创建 Wasm 应用程序后，下一步是在生产中部署和扩展它。云原生全景图中有大量的工具、框架和服务来管理应用程序部署。其中许多都集成了 Wasm 支持。</p><p></p><h3>编排与管理</h3><p></p><p></p><p>Wasm 容器可以通过 Docker、containerd 和 Kubernetes 等现有容器工具进行无缝管理。有两种方法可以将 Wasm 应用程序作为“容器”进行管理。这两种方法都可以让你构建并行运行 Linux 容器和 Wasm 容器的 Kubernetes 集群。</p><p></p><p>方法 #1 是在容器管理堆栈的基础上使用 OCI 运行时，例如 crun 和 youki 。crun 根据镜像的目标操作系统和 CPU 平台检测 OCI 镜像是 wasm 还是 Linux。如果镜像的目标是 wasi/wasm，crun 将绕过 Linux 容器设置过程，只使用 WasmEdge 来运行它。基于 crun，我们可以获得整个 Kubernetes 堆栈来运行 Wasm 镜像，包括 CRI-O （CNCF 项目）、 containerd（CNCF 项目） 、Podman、kind、K8s （CNCF 项目）、 OpenYurt （CNCF 项目）、 SuperEdge （CNCF 项目）、 KubeEdge （CNCF 项目）。</p><p></p><p>方法 #2 是使用 containerd-shim（例如 runwasi ）在 containerd 中运行 Wasm 应用程序。当 containerd 接收到镜像时，它会检查镜像的目标平台，如果镜像是 wasi/wasm，它会路由到 runwasi 执行，如果镜像是 x86 或 arm，它会路由到 runc。</p><p></p><p>在 Kubernetes 的基础上，我们还介绍了一些新兴工具来帮助管理生产 Wasm 工作负载。</p><p></p><p>Kuasar 是另一个支持多种类型沙箱的容器运行时，包括 microVM、Linux 容器、应用程序内核和 WebAssembly 运行时。</p><p></p><p>Kwasm 是一个 Kubernetes Operator，它为你的 Kubernetes 节点添加了 WebAssembly 支持。它可与基于 Ubuntu/Debian 和 Containerd 的本地和托管云 K8s 发行版配合使用，</p><p></p><p>container2wasm 是一个容器到 wasm 镜像转换器，可以在 WASM 上运行容器。</p><p></p><h3>托管（Hosted）平台</h3><p></p><p></p><p>如果你不想麻烦地运行自己的服务器和 Kubernetes 集群，那么托管平台是将 Wasm 应用程序部署和扩展为服务的绝佳选择。</p><p></p><p>Flows.network 是一个用于 AI 原生工作流程自动化的 serverless Wasm 平台。Fermyon Cloud 是部署和管理使用 Spin 框架编写的 serverless Wasm 函数。Cosmonic 是部署在 WasmCloud 上的基于 Wasm 的 Actor 服务。Cloudflare Workers 是 Cloudflare 边缘网络上由 v8 支持的 Wasm 和 JavaScript &nbsp;serverless 函数运行时。Fastly @Edge Function 是部署在 Fastly 边缘网络上的 serverless Wasm 函数平台。AKS：支持在 Azure Kubernetes Service (AKS) 中创建 WASI 节点池以运行 WebAssembly (WASM) 工作负载Taubyte 是一个用于运行应用程序的云原生平台，尤其是 Wasm 应用程序。Golem Cloud 是一个计算平台，允许开发者在 Wasm 中构建和部署长期运行、有状态的 serverless 工作线程。</p><p></p><h3>去中心化平台</h3><p></p><p></p><p>基于区块链的智能合约平台是去中心化的云计算网络。你的云应用程序（即智能合约）不是由中心 Operator 运行所有工作负载，而是由网络中的节点执行。由于智能合约不受信任并且必须非常频繁地执行（每台计算机每秒数百次），因此 Wasm 是该应用场景的理想执行引擎。事实上，几乎所有领先的智能合约区块链网络都采用了 Wasm。</p><p></p><p>PolkadotNEARDfinityGEARFilecoinRippleEOSCosmWasmQuai Network</p><p></p><h3>调试和可观测性</h3><p></p><p></p><p>调试和可观测性是重要的生产功能，使运维团队能够持续监控应用程序并向开发团队提供有用的反馈。这是 Wasm 全景图中目前有所缺乏的部分。我们预计这个领域会随着 Wasm 越来越多地部署在生产中而生长壮大。</p><p></p><p>WASI logging 是用于发出日志消息的 Wasm 规范。它得到领先的 Wasm 运行时（例如 wasmtime 和 WasmEdge）的支持。Modsurfer 为运营和开发团队提供了第一个记录系统 + 诊断应用程序，用于搜索、浏览、验证、审核和调查 Wasm 二进制文件。</p><p></p><h3>Artifacts</h3><p></p><p></p><p>Artifacts repo 是全景图中的重要元素。它们提供集中且始终可用的位置来存储、发现、验证、下载、跟踪多个已发布版本的 Wasm 包。它们不仅是便利的工具，而且对于软件供应链安全也至关重要。</p><p></p><p>Docker Hub 是创建、管理和交付团队容器应用程序的地方。如果你在 Docker Hub 上上传纯 Wasm 镜像，则镜像的操作系统 / 架构将被标记为 wasi/wasm。Harbor 是一个开源的可信云原生 registry 项目，用于存储、签名和扫描内容。它支持 Wasm 工件。Warg 是一个 WebAssembly 组件 registry。wapm 是 WebAssembly 的包管理器。crates.io 是 rust 的 crate registry，它是最常用的 Wasm 语言。</p><p></p><h2>帮助我们构建 Wasm 全景图</h2><p></p><p></p><p>Wasm 全景图是社区的努力，随着 Wasm 采用的兴起，Wasm 全景图也在快速发展。我们打算随时更新最新进展。然而，我们只有在整个 Wasm 社区的帮助下才能做到这一点。</p><p></p><p>如果有任何想要添加或更新的内容，请提交 PR！可以参考此 PR 添加你的项目名称、logo、网站、GitHub &nbsp;repo 链接和 crunchbase。</p><p></p><p>Wasm全景图提交PR参考链接：<a href=\"https://github.com/cncf/landscape/pull/3475\">https://github.com/cncf/landscape/pull/3475</a>\"</p>",
    "publish_time": "2023-09-12 19:20:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]