[
  {
    "title": "OpenAI设立Superalignment团队：让 AI 对齐人类价值观，预防潜在风险",
    "url": "https://www.infoq.cn/article/J8emKvQKNjHz3hVGamV6",
    "summary": "<p><a href=\"https://openai.com/\">OpenAI</a>\"宣布成立一个专门的<a href=\"https://openai.com/blog/introducing-superalignment\">Superalignment</a>\"团队，旨在防止流氓<a href=\"https://en.wikipedia.org/wiki/Superintelligence\">Superintelligent AI</a>\"的出现。OpenAI强调了使人工智能系统与人类价值保持一致的必要性，以及主动采取措施防止潜在危害的重要性。</p><p>&nbsp;</p><p>创造符合人类理想和目标的人工智能系统的过程被称为<a href=\"https://en.wikipedia.org/wiki/AI_alignment\">人工智能校准</a>\"。这需要确保AI系统理解伦理概念、社会标准和人类目标，并据此采取行动。AI校准旨在缩小人类需求和福祉与AI系统目标之间的差距。通过将AI与人类价值相结合，减少人工智能的危害，增加其潜在的优势。</p><p>&nbsp;</p><p>OpenAI的<a href=\"https://openai.com/blog/introducing-superalignment\">Superalignment</a>\"团队将专注于促进对AI校准的理解和实现。这是一个确保AI系统按照人类价值和目标行事的过程。通过研究强大的校准方法和开发新技术，该团队旨在创建在其整个发展过程中始终以人为本的人工智能系统。</p><p></p><p></p><blockquote>OpenAI表示：“我们的目标是在四年内解决超级智能校准的核心技术挑战。”</blockquote><p></p><p>&nbsp;</p><p>OpenAI联合创始人兼首席科学家Ilya Sutsker和校准主管Jan Leike表示，像GPT-4（ChatGPT的基础）这类模型当前使用的AI校准技术，都依赖于从人类反馈中进行<a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">强化学习</a>\"。不过，这种方法依赖于人类的监督，如果AI的的智力超越了人类，变得比它的监督者更聪明，这种方法可能就行不通了。Sutsker和Leike进一步解释说，其他一些基本假设，比如在部署过程中有良好的泛化属性，或者在训练过程中无法检测和削弱监督，在未来也可能被打破。</p><p>&nbsp;</p><p><a href=\"https://en.wikipedia.org/wiki/AI_safety\">AI安全</a>\"将成为一个重要的产业。世界各国政府正在采取措施制定法规，解决人工智能各个方面的问题，包括数据隐私、算法透明度和伦理考量。欧盟正在制定全面的《<a href=\"https://artificialintelligenceact.eu/the-act/\">人工智能法案</a>\"》，美国也在采取措施制定《<a href=\"https://www.whitehouse.gov/ostp/ai-bill-of-rights/\">人工智能权利法案蓝图</a>\"》。在英国，<a href=\"https://www.gov.uk/government/news/initial-100-million-for-expert-taskforce-to-help-uk-build-and-adopt-next-generation-of-safe-ai\">基金会模型人工智能工作组</a>\"已经成立，旨在研究调查人工智能的安全问题。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/openai-superalignment-ai-safety/\">https://www.infoq.com/news/2023/07/openai-superalignment-ai-safety/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/datmEqVmS134ewFO7wel\">OpenAI或于2024年底破产？大模型太烧钱了，快把OpenAI烧没了！</a>\"</p><p><a href=\"https://www.infoq.cn/article/IzPVkcZg0jeHGcD4xP7H\">OpenAI 推出网络爬虫 GPTBot，引发网站抵御潮：信息被爬走就很可能意味着永远无法删除</a>\"</p>",
    "publish_time": "2023-08-21 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "历史库存储成本节约至少 50% ，OceanBase数据压缩核心技术解读",
    "url": "https://www.infoq.cn/article/HERjTwV7wztVIFg9Tdt0",
    "summary": "<p></p><blockquote>作者｜赵赛铜，OceanBase 高级开发工程师。目前在 OceanBase 存储-分析处理组，工作方向是存储结构和分析处理功能的维护与开发。</blockquote><p></p><p></p><p>“数据是二十一世纪的石油”，这个观点正在逐渐成为现实，现在我们有各种各样的 IT 系统不断地生产着数据，这些数据累积起来为我们的生产生活带来了很多便利。但在挖掘这些数据价值的同时，大量数据的存储与计算也带来了巨大的成本，降本增效也成为了很多 IT 系统设计的重点。</p><p></p><h2>历史库的需求与挑战——降本增效</h2><p></p><p></p><p>在订单、交易、日志等业务场景中，数据总量会不断增加。而对于这些数据的访问往往和时间有很强的相关性，通常与当前时间越接近的数据越“热”，也就是说，这些数据可能会被频繁地修改与点查。热数据的访问更多是事务型负载和实时分析负载，其数据量在整个系统中的占比相对较低。而在系统中已经存在了一段时间的数据，被称为冷数据，这些数据的被查询次数相对没有那么频繁，也很少被修改。冷数据的访问通常是少量的事务型负载和一些 ad-hoc 或报表统计等分析型负载，而且冷数据通常是稳定运行的 IT 系统中数据量的主要部分。</p><p></p><p>由于冷热数据有着明显的区别，将它们放在一套相同规格的环境中同等处理显然会浪费系统的资源，单个数据库的容量上限还可能会限制数据的存储。但是，将冷数据定期归档到更经济的存储介质中，访问数据时采用从归档数据中还原的方法，又会对历史数据的查询性能和系统的复杂度带来负面影响。因此，将数据分为线上库和历史库，将在线数据定期同步到历史库中的做法成为了越来越多系统的解决方案，通过在存储和计算成本更低的环境上部署历史库来降低成本和满足业务需求。</p><p></p><p>历史库这种模式的出现也对数据库管理系统带来了新的需求，通常一个历史库希望数据库能够：</p><p></p><p>拥有大容量的存储空间，即能够支持大量数据的存储和在线库数据高效的持续导入；提供更低的存储成本，即能够用更少的磁盘空间，更经济的存储介质存储更多的数据；提供和在线库近似的查询性能，即支持高效的少量事务型查询，同时能够支持高效的分析型查询；支持低频率的历史数据更新；对应用和在线库保持相同的访问接口，降低应用复杂度。</p><p></p><p>面对这些需求，拥有良好的单机分布式扩展能力，支持 HTAP 混合负载处理能力的 OceanBase 数据库能够同时高效地支持业务系统的在线库和历史库场景。更为关键的是， OceanBase 可以在支持业务的同时至少降低一半的存储成本，部分客户反馈，业务历史库从其他数据库迁移至 OceanBase 后，存储成本降低 80% 左右，这也是许多用户在历史库场景选择 OceanBase 的原因之一。其中的核心技术，就是 OceanBase 的 LSM-tree 存储引擎和数据库压缩功能。本文会为大家分享 OceanBase 在降本增效的重要功能-数据库压缩上的设计与思考，来探讨存储引擎怎样通过数据压缩更好地支持历史库场景。</p><p></p><h2>解决历史库需求的核心技术——数据编码与压缩</h2><p></p><p></p><p>数据压缩是数据密集型应用中常用的方法，将数据处理后变换成占用更小空间的格式，来降低存储与传输数据的成本，是一种用压缩和解压的手段，以计算时间换存储空间。</p><p></p><p>在现代计算机存储的层次架构中，与 CPU Cache 和内存相比，硬盘通常有着更低的带宽与更高的查询时延。尽管现在 SSD 、 DMA 等硬件技术对 I / O 进行了优化，硬盘的访问效率仍然和内存等设备不在同一个数量级，而通过压缩可以在相同的 I / O 带宽上支持更多数据的读写。因此，对数据进行压缩不仅可以降低存储的成本，还可以降低 I / O 的压力，压缩过的数据也使得访问数据路径上的各级 cache 都能有相对更高的命中率。</p><p></p><p>而压缩数据的过程实际上就是去除数据中的冗余，更高效地表达数据信息的过程。因此，不同场景下的数据特征与访问需求也为数据压缩提供了更多的可能。例如，音视频数据都有特定的有损压缩算法，对连续的数值数据和文本数据分别有更高效的压缩方法。而对于数据库，尤其是关系型数据库，存储的数据会受到 schema 、类型、值域等限制，相似度会比较高，这些特征使数据可以被更好地编码与压缩。</p><p></p><h2>不同数据库产品的数据压缩技术</h2><p></p><p></p><p>目前数据库产品或多或少都提供了数据压缩功能，但由于存储引擎架构和数据库应用场景的不同，产品之间的数据库压缩设计和能力也会产生差别。通常，压缩率越高的压缩算法，压缩和解压数据的 overhead 就越大，因此各种产品在设计压缩功能时都会在成本和性能之间进行权衡， OceanBase 也根据自己的架构特点设计了数据的压缩方法。</p><p></p><p>1、事务型数据库的数据压缩技术</p><p></p><p>通常针对事务型负载设计的数据库需要在平衡压缩率和性能上做出更多的努力。面向 OLTP 场景的数据库要对频繁随机写入和更新场景支持更高的 TPS ，通常会使用基于行存和 B+ 树结构的存储引擎，因此对于数据的压缩会相对保守。这种存储引擎通常会将定长的内存数据页映射到持久化数据块来管理，而且有些情况下将更新数据同步写到数据块中，会导致对页内少量行进行 DML 操作可能需要对整个数据页进行重新压缩，在读写路径上带来更多的 overhead 。而且定长数据块在进行压缩前难以确定压缩后的数据块大小，也会带来一些空间浪费等问题。</p><p></p><p>典型的例子如 MySQL ，数据写入需要更新 redo log 和内存中的数据页，然后在触发页分裂或交换时把内存中的脏数据页刷到持久化存储中。在 MySQL 的早期版本中，一个数据页在 buffer pool 中可能同时存在压缩和未压缩两份数据，分别用于读数据和更新数据后重新压缩刷脏页，开启压缩功能的表会出现明显的性能下降。在 MySQL 5.7 版本后，通过文件系统的空洞能力实现了透明压缩（ TPC ），将内存页面压缩后按照原大小刷到文件系统中，以文件系统的块大小为粒度发现并复用数据块中的空洞。这样的方法相对旧版本更灵活，压缩和解压性能相对更好，但没有解决存储空间的浪费和文件系统碎片等问题。由于存储引擎中开启压缩带来的不可避免的性能损失， MySQL 和 Oracle 都提醒开发者谨慎使用透明压缩、 OLTP 压缩等特性。</p><p></p><p>2、分析型数据库的数据压缩技术</p><p></p><p>分析型负载的数据库需要批量处理大量数据，通常对数据压缩会有更高的要求。因为面向 OLAP 场景的数据仓库等系统的数据通常是批量导入的，增量数据相对较少，所以分析型数据库通常使用列存、增量数据写日志，定期重整基线数据的存储引擎。这种存储引擎中数据压缩发生在批量导入和后台数据重整时，可以采用相对激进的压缩策略。例如，以更大的数据块为单位进行压缩，将更多数据压缩到同一个数据块中，将同一列的数据存储在相邻的数据中，并针对这一列数据的特征对数据进行压缩率更高的编码。</p><p></p><p>处理各种特定负载的数据库也会根据自己的数据结构对数据进行更高效的编码。如 ClickHouse ， Redshift 等数据仓库中都对整形数据设计了特殊的编码方式；搜索引擎也对单调递增的整形序列的编码与扫描进行了深入的优化；时序数据库 Gorilla 中提出的 Gorilla 编码知名度甚至超过了这个数据库本身。很多分析型数据库还会根据自己的查询负载对数据编码进行设计，来优化自己的查询性能。传统的数据仓库能够通过列存和编码提供较好的数据压缩能力，对特定的分析型查询负载也有更好的表现，但往往难以支持高效的数据更新。</p><p></p><p>3、 OceanBase 的数据压缩技术</p><p></p><p>作为一款 HTAP 数据库产品， OceanBase 使用基于 LSM-Tree 架构的存储引擎，同时支持 OLTP 与 OLAP 负载，这种存储架构提供了优秀的数据压缩能力。在 OceanBase 中，增量数据会写入 clog 和 memtable 中，&nbsp;OceanBase 的 memtable 是内存中的 B+ 树索引，提供高效的事务处理能力。&nbsp;memtable 会定期通过 compaction 生成硬盘持久化数据 sstable&nbsp;，多层 sstable会采用 leveled compaction 策略进行增量数据重整。sstable 中数据块的存储分为两层，其中 2M 定长的数据块（宏块）作为 sstable 写入 I / O 的最小单元，存储在宏块中的变长数据块（微块）作为数据块压缩和读 I / O 的最小单元。</p><p></p><p>在这样的存储架构下， OceanBase 的数据压缩集中发生在 compaction 过程中 sstable 的写入时，数据的在线更新与压缩得到了解耦。批量落盘的特性使其采用更激进的压缩策略。OceanBase 从 2.0 版本开始引入了行列混存的微块存储格式（ PAX ），充分利用了同一列数据的局部性和类型特征，在微块内部对一组行以列存的方式存储，并针对数据特征按列进行编码。变长的数据块和连续批量压缩的数据也可以让 OceanBase 通过同一个 sstable 中已经完成压缩的数据块的先验知识，对下一个数据块的压缩进行指导，在数据块中压缩尽量多的数据行，并选择更优的编码算法。</p><p></p><p>与部分在 schema 上指定数据编码的数据库实现不同， OceanBase 选择了用户不感知的数据自适应编码，在给用户带来更小负担的同时降低了存储成本，从历史库角度而言，用户也不需要针对历史库数据做出过多压缩与编码相关的配置调整。&nbsp;OceanBase 之所以能够在事务性能和压缩率之间取得更好的平衡，都得益于 LSM-Tree 的存储架构。</p><p></p><p>当然， LSM-Tree 架构不是解决数据库压缩所有问题的银弹，如何通过数据压缩降低成本、提升性能是业界一直在讨论的话题。对 B+ 树类的存储引擎进行更高效的压缩也有很多探索，比如基于可计算存储硬件的工作，利用存储硬件内部的透明压缩能力对&nbsp;B+&nbsp;树类存储引擎的数据压缩进行优化，使其写放大达到了接近 LSM-Tree 架构存储引擎的效果。但 LSM-tree 中内存数据页更新与数据块落盘解耦，和 sstable 数据紧凑排布的特点，使得 LSM-tree 相对&nbsp;B+ 树类存储引擎，仍然更适合在对查询/更新带来更少负面影响的前提下实现更高效的数据压缩。</p><p></p><h2>OceanBase 的数据库压缩技术</h2><p></p><p></p><p>OceanBase 同时支持不感知数据特征的通用压缩 ( compression ) 和感知数据特征并按列进行压缩的数据编码 ( encoding )。这两种压缩方式是正交的，也就是说，可以对一个数据块先进行编码，然后再进行通用压缩，来实现更高的压缩率。</p><p></p><p>OceanBase 中的通用压缩是在不感知微块内部数据格式的前提下，将整个微块通过通用压缩算法进行压缩，依赖通用压缩算法来检测并消除微块中的数据冗余。目前&nbsp; OceanBase 支持用户选择 zlib 、 snappy 、 zstd 、 lz4 算法进行通用压缩。用户可以根据表的应用场景，通过 DDL 对指定表的通用压缩算法进行配置和变更。</p><p></p><p>由于通用压缩后的数据块在读取进行扫描前需要对整个微块进行解压，会消耗一定 CPU 并带来 overhead。为了降低解压数据块对于查询性能的影响， OceanBase 将解压数据的动作交给异步 I / O 线程来进行，并按需将解压后的数据块放在 block cache 中。这样结合查询时对预读 （ prefetching ） 技术的应用，可以为查询处理线程提供数据块的流水线，消除解压带来的额外开销。</p><p></p><p>通用压缩的优点是对被压缩的数据没有任何假设，任何数据都可能找到模式并压缩，但往往出于平衡压缩性能和压缩率的考虑，通用压缩算法会放弃对一些复杂数据冗余模式的探测和压缩。对于关系型数据库来说，系统对数据库内存储的结构化数据有着更多的先验知识，利用这些先验知识可以对数据进行更高效的压缩。</p><p></p><h2>OceanBase 的数据编码算法</h2><p></p><p></p><p>上文提到在关系型数据库中，由于 schema 和数据类型的限制，同一列的数据类型、精度、值域往往相同。而且在实际应用中，同一列中相邻的数据也通常会有自己的特征，如下面两个场景。</p><p></p><p>当通过一列数据存储城市、性别、产品分类等具有类型属性的值时，这些列数据块内部数据的基数（ cardinality ）也会比较小，这时数据库可以直接在用户数据字段上建立字典，来实现更高的压缩率；</p><p></p><p>当数据按时序插入数据库，这些插入的数据行中的时间相关字段、自增序列等数据的值域会相对较小，也会有单调递增等特性，利用这些特性，数据库可以更方便地为这些数据做 bit-packing 、差值等编码。</p><p></p><p>为了实现更高的压缩比，帮助用户大幅降低存储成本， OceanBase &nbsp;设计了多种编码算法，最终在 OceanBase 的负载上实现了很好的压缩效果。&nbsp;OceanBase 根据实际业务场景需求实现了单列数据的 bit-packing 编码、字符串 HEX 编码、字典编码、 RLE 编码、常量编码、数值差值编码、定长字符串差值编码，同时，创新地引入了列间等值编码和列间子串编码，能够分别对数据库中一列数据或几列数据间可能产生的不同类型数据冗余进行压缩。</p><p></p><p>1、降低存储的位宽：Bit-packing 和 HEX 编码</p><p></p><p>Bit-packing 和 HEX 编码类似，都是在压缩数据的基数较小时，通过更小位宽的编码来表示原数据。而且这两种编码可以与其他编码叠加，对于其他编码产生的数值或字符串数据，都可以再通过 bit-packing 或 HEX 编码进一步去除冗余。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3909bbf3624feb475f38285b9bdba72.jpeg\" /></p><p>（bit-packing）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/1798334a5609da840beeb805c6c4e889.jpeg\" /></p><p>（ HEX&nbsp; 编码）</p><p></p><p>2、单列数据去重：字典编码和 RLE 编码等</p><p></p><p>字典编码则可以通过在数据块内建立字典，来对低基数的数据进行压缩。当低基数的数据在微块内的分布符合对应的特征时，也可以使用游程编码/常量编码等方法进行进一步的压缩。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c6/c619a29e44041e1476a49570198008fc.jpeg\" /></p><p>（字典编码/RLE 编码）</p><p></p><p>3、利用数据的值域压缩：差值编码等</p><p></p><p>差值编码也是常用的编码方法， OceanBase 中的差值编码分为数值差值编码和定长字符串差值编码。数值差值编码主要用来对值域较小的数值类数据类型进行压缩。对于日期、时间戳等数据，或其他临近数据差值较小的数值类数据，可以只存储最小值，每行存储原数据与最小值的差值。定长字符串编码则可以比较好地对人工生成的 ID，如订单号/身份证号、url 等有一定模式的字符串进行压缩，对一个微块的数据存储一个模式串，每行额外存储与模式串不同的子串差值，来达到更好的压缩效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7308f114d7d7f5a4327c825d6ed1bc6.jpeg\" /></p><p>（整形差值）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b35ad23489bf144ec5707b15c88facc6.jpeg\" /></p><p>（字符串差值）</p><p></p><p>4、减小多列数据冗余：列间编码</p><p></p><p>为了利用不同列间数据的相似性增强压缩效果，OceanBase 引入了列间编码。通常情况下，列存数据库只会对数据在列内部进行编码，但在实际应用中有很多表除了同一列数据之间存在相似性，不同列的数据之间也可能有一定的关系，利用这种关系可以通过一列数据表示另外一列数据的部分信息。</p><p></p><p>列间编码可以对复合列、系统生成的数据做出更好的压缩，也能够降低在数据表设计范式上的问题导致的数据冗余。</p><p></p><p>5、自适应压缩技术：让数据库选择编码算法</p><p></p><p>数据编码的压缩效果不仅与表的 schema 相关，同时还与数据的分布，微块内数据值域等数据本身的特征相关，这也就意味着比较难以在用户设计表数据模型时指定列编码来实现最好的压缩效果。为了减轻用户的使用负担，也为了实现更好的压缩效果，OceanBase 支持在合并过程中分析数据类型、值域、NDV 等特征，结合 compaction 任务中上一个微块对应列选择的编码算法和压缩率自适应地探测合适的编码，对同一列在不同数据块中支持使用不同的算法来进行编码，也保证了选择编码算法的开销在可接受的区间内。</p><p></p><h2>如何降低数据编码对查询性能的影响</h2><p></p><p></p><p>为了能够更好地平衡压缩效果和查询的性能，我们在设计数据编码格式时也考虑到了对查询性能带来的影响。</p><p></p><p>1、行级粒度数据随机访问</p><p></p><p>通用压缩中如果要访问一个压缩块中的一部分数据通常需要将整个数据块解压后访问，某些分析型系统的数据编码大多面向扫描的场景，点查的场景比较少，因此采用了在访问某一行数据时需要对相邻数据行或数据块内读取行之前所有行进行解码计算的数据编码的格式（如 PFor 等差值编码）。</p><p></p><p>OceanBase 需要更好地支持事务型负载，就意味着要支持相对更高效的点查，因此 OceanBase 在设计数据编码格式时保证了编码后的数据是可以以行为粒度随机访问的。也就是在对某一行进行点查时只需要对这一行相关的元数据进行访问并解码，减小了随机点查时的计算放大。同时对于编码格式的微块，解码数据所需要的所有元数据都存储在微块内，让数据微块有自解释的能力，也在解码时提供了更好的内存局部性。</p><p></p><p>2、缓存解码器</p><p></p><p>在 OceanBase 目前的数据解码实现中，每一列数据都需要初始化一个解码器对象来解析数据，构造解码器时会需要进行一些计算和内存分配，为了进一步减小访问编码数据时的 RT ，OceanBase 会将数据的解码器和数据块一起缓存在 block cache 中，访问&nbsp; cache 中数据块时可以直接通过缓存的解码器解析数据。当不能命中 block cache 中缓存的解码器时，OceanBase 还会为解码器用到的元数据内存和对象构建缓存池，在不同查询间复用这些内存和对象。</p><p></p><p>通过上述细节上的优化，行列混存格式的 sstable 编码数据也可以很好地支持事务型负载。而且由于编码数据行列混存的格式，使得在分析型查询的处理上，编码数据有着和列存数据相似的特性，数据分布更紧凑，对 CPU cache 更加友好。这些特性使列存常用的优化手段也能应用于分析型查询优化中，充分利用 SIMD 等方法来提供更高效的分析型负载处理。</p><p></p><p>3、计算下推</p><p></p><p>由于编码数据中会存储有序字典、 null bitmap 、常量等可以描述数据分布的元数据，在扫描数据时可以利用这些数据对于部分过滤，聚合算子的执行过程进行优化，实现在未解码的数据上直接进行计算。&nbsp;OceanBase 在 3.2 版本中对分析处理能力进行了大幅的优化，其中包括聚合与过滤计算下推到存储层执行，和在向量化引擎中利用编码数据的列存特征进行向量化的批量解码等特性。在查询时充分利用了编码元数据和编码数据列存储的局部性，在编码数据上直接进行计算，大幅提高了下推算子的执行效率和向量化引擎中的数据解码效率。基于数据编码的计算下推和向量化解码也成为了支持&nbsp; OceanBase 高效处理分析型负载，在 TPC-H benchmark 中达到优秀性能指标的重要功能。</p><p></p><h2>数据编码压缩的基础测试</h2><p></p><p></p><p>不同的压缩方式如何影响 OceanBase 的压缩效果，以下通过一个简单的测试进行观察。</p><p></p><p>使用 OceanBase 4.0 版本分别在交易场景的 TPC-H 10g 的数据模型和用户行为日志场景的 IJCAI-16 Brick-and-Mortar Store Recommendation Dataset 数据集上对&nbsp; OceanBase 的压缩率进行测试：</p><p></p><p>TPC-H 是对订单，交易场景的建模，对 TPC-H 模型中数据量比较大的两张表，即存储订单的 ORDERS 表和存储商品信息的 LINEITEM 表的压缩率进行统计。在&nbsp; OceanBase 默认配置（ zstd + encoding ）下，这两张表的压缩率可以达到 4.6 左右，相较只开启 encoding 或 zstd 压缩时提升明显。</p><p></p><p>IJCAI-16 taobao user log 则是淘宝脱敏后的真实业务数据，存储了用户浏览商品时的行为日志。在 OceanBase 默认配置（ zstd + encoding ）下压缩率可以达到&nbsp; 9.9 ，只开启 encoding 压缩率可以达到 8.3 ，只开启 zstd 压缩率为 6.0 。</p><p></p><p>可以看到 OceanBase 在面对数据更有规律的业务数据时会有更出色的数据压缩效果，在 TPC-H 这种数据冗余相对更少的数据集上也有着优秀的数据压缩能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b86d537df97e0e9b8c696523bfc21056.jpeg\" /></p><p></p><h2>写在最后</h2><p></p><p>OceanBase 存储引擎的数据库压缩功能在设计上希望能够在用户少感知、不感知存储格式的前提下，在不降低事务型负载性能的同时降低存储空间和存储成本，同时提升分析型负载的性能。这样的设计与历史库的设计不谋而合。高度压缩的数据既能够帮助历史库数据降低至少 50% 的存储成本，高效的写入查询和统一的配置接口又能够帮助业务增效。</p><p></p><p>对于历史库数据同步的需求， OceanBase 的 LSM-Tree 存储引擎天生具有高效的写入性能，既能够通过旁路导入高效处理定期的批量数据同步，又能够承载一些实时数据同步和历史库数据修改的场景。</p><p></p><p>对于历史库数据的定期跑批报表，和一些 ad-hoc 的分析型查询带来的大量数据扫描的需求，因为历史库中增量数据较少，所以绝大多数数据都存储在基线的 SSTable 中，这时计算下推可以只扫描基线数据，绕过了 LSM-Tree 架构常见的读放大问题。而且支持在压缩数据上执行下推算子和向量化解码的压缩格式可以轻松地处理大量数据查询和计算。</p><p></p><p>对于大量历史数据存储的需求， OceanBase 的 SSTable 存储格式和数据编码压缩功能可以使 OceanBase 更轻松地支持超大容量的数据存储。而且高度压缩的数据和在同等硬件下更高效的查询性能也能够大幅度降低存储和计算的成本。</p><p></p><p>此外，企业可以选择将历史库所在的集群部署在更经济的硬件上，但是对数据库进行运维基本不需要感知数据编码与压缩的相关配置，应用开发也可以做到在线库和历史库使用完全相同的访问接口，简化应用代码和架构。</p><p></p><p>这些特点让越来越多的企业开始在历史库场景使用 OceanBase 进行降本增效的实践。OceanBase 也不断在存储架构，降本增效方面做出更多的探索。</p>",
    "publish_time": "2023-08-21 10:37:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型赛道再添新玩家，快手自研大模型“快意”亮相",
    "url": "https://www.infoq.cn/article/baiaSCTjwDJzCZJJHjA9",
    "summary": "<p>近日，快手自研的大语言模型“快意”（KwaiYii）已开启内测，并为业务团队提供了标准API和定制化项目合作方案。</p><p>&nbsp;</p><p>GitHub链接：</p><p><a href=\"https://github.com/kwai/KwaiYii\">https://github.com/kwai/KwaiYii</a>\"</p><p>&nbsp;</p><p>据官方介绍，快意大模型（KwaiYii） 是由快手 AI 团队从零到一独立自主研发的一系列大规模语言模型（Large Language Model，LLM），当前包含了多种参数规模的模型，并覆盖了预训练模型（KwaiYii-Base）、对话模型（KwaiYii-Chat）。</p><p>&nbsp;</p><p>其中，13B规模的系列模型KwaiYii-13B主要特点包括：</p><p></p><p>KwaiYii-13B-Base预训练模型具备强大的通用技术支撑能力，在鳄鱼权威的中/英文基准上取得了同等模型尺寸下的State-Of-The-Art效果。例如，KwaiYii-13B-Base预训练模型在MMLU、CMMLU、C-Eval、HumanEval等Benchmark上目前达到同等模型规模的领先水平。KwaiYii-13B-Chat对话模型具备出色的语言理解和生成能力，支持内容创作、信息咨询、数学逻辑、代码编写、多轮对话等广泛任务，人工评估结果表明KwaiYii-13B-Chat超过主流的开源模型，并在内容创作、信息咨询和数学解题上接近ChatGPT(3.5)同等水平。</p><p>&nbsp;</p><p>据介绍，快意大模型（KwaiYii）在 MMLU、CMMLU、C-Eval、HumanEval 等 Benchmark 上目前处于同等模型规模的领先水平，在最新的 CMMLU 中文向排名中，快意的 13B 版本 KwaiYii-13B 同时位列 five-shot 和 zero-shot 下的第一名，在人文学科、中国特定主题等方面较强，平均分超 61 分。</p><p>&nbsp;</p><p>KwaiYii-13B-Chat 对话模型具备出色的语言理解和生成能力，支持内容创作、信息咨询、数学逻辑、代码编写、多轮对话等广泛任务。</p><p></p><p>快手方面表示，从人工评估的结果来看，KwaiYii-13B-Chat超过了同等规模的开源模型，并接近ChatGPT同等水平。在内容创作、信息咨询、逻辑推理和数学解题上，基本与ChatGPT(3.5)效果相当。在多轮对话能力方面，KwaiYii-13B-Chat超过同等规模的开源模型，但与ChatGPT(3.5)仍有一定差距。注意：人工评估结果受到评测数据覆盖面、标注主观性等因素的影响，无法全面反映大语言模型的所有能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/1800d174d36f37107b547def96c64848.png\" /></p><p></p><p>据悉，快手AI团队将持续迭代“快意”大模型，一方面将继续优化模型性能并研发多模态能力 ，另一方面也在推进更多C端与B端业务场景下的落地 。</p>",
    "publish_time": "2023-08-21 11:29:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "JavaScript前景展望：值得期待的那些新功能",
    "url": "https://www.infoq.cn/article/LS4Vy6OueTDFpekcEir2",
    "summary": "<p></p><p></p><blockquote>超实用的类型机制加现代工具，让网站和Web应用更跟得上潮流和形势。而从种种细节当中，我们也得以一窥JavaScript未来的发展方向。</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>如果单从每年公布的ECMAScript标准流程来看，JavaScript的功能设计似乎并没有什么特别重大的变化倾向。TC39联合主席兼Bloomberg JavaScript基础设施与工具团队负责人Rob Palmer一直强调要“铺平道路”，也就是逐步把小小的工具或框架功能融入语言之内，用“语法糖”的形式完成改进。换言之，先把现有功能做好、做完善，这样的升级明显更加无痛。</p><p>&nbsp;</p><p>“我们见证了工具、框架和模式的迭代，随着时间推移，我们会在标准层面找寻融合的交接点，并努力把道路铺平，借此降低应用程序技术栈的复杂度。这样Node模块目录才能持续瘦身，越来越多的功能开始由语言本体直接提供。”</p><p>&nbsp;</p><p>Vercel公司的TC39代表Justin Ridgewell在采访中表示，考虑到2017年引入了原生异步等重大发展，这类增量化功能正在不断增加。“多年以来，JavaScript引入了诸多具有新功能的新API。我们的新功能吸纳思路也正在于此——在添加小变更的同时，尽量扩大新功能的广度。”</p><p>&nbsp;</p><p></p><blockquote>“我们并不会每年发布重大的功能变更：有时候是每两年一次，有时候是每三年一次。我们只会在必要时引入，确保其切实推动语言的发展。”</blockquote><p></p><p>&nbsp;</p><p>一部分重要的新功能（例如Temporal）在发布之初就基本做好了应用准备；也有其他产品正处于开发中，预计会在未来几年内陆续推出。本文为大家挑选了一些最有趣的内容，同时也征求了JavaScript标准制定团队的意见，由他们对语言发展状况做出解释，包括JavaScript在下一步标准化中将要解决的问题。</p><p>&nbsp;</p><p></p><h2>类型机制不会把JavaScript变成TypeScript&nbsp;</h2><p></p><p>&nbsp;</p><p>TypeScript的诞生，纯粹是为了提高JavaScript开发者的工作效率，绝对不是要彻底取代JS的江湖地位。当然，TypeScript也成为JS未来发展完善的灵感来源。目前，大家在编写代码时需要在TypeScript中声明类型——但在代码运行时，这部分又会被移除掉。</p><p>&nbsp;</p><p>之后的发展自然也要解决这个问题。第一阶段的类型注释提案希望在JavaScript代码中引入类型信息，更重要的是保证JS引擎能够将其正确理解为注释，这样就能保证TypeScript和JavaScript相互一致和对齐，同时明确它们其实运行在不同的层上。</p><p>&nbsp;</p><p>Palmer指出，开发人员可以对类型使用一等语法，包括TypeScript以及带有长JSDoc注释块的Flow语法，同时保证自己的代码仍然能跟JavaScript引擎和JavaScript工具链相兼容——这就避免了在代码运行前消除类型给构建过程带来的复杂性。</p><p>&nbsp;</p><p>Palmer解释道，“这种仅在开发期间存在，但在运行时会被完全移除的静态类型具有巨大的价值。”而且其中一部分价值可谓相当直观，“开发者可以进行类型检查，知晓自己什么时候犯了错误，同时取消对不存在属性的引用——这很棒。而且除此之外，类型信息还能大大改善开发人员的工作感受，例如重构事物的能力、在IDE中自动一次性重命名变量和属性，还有实现代码导航等。”</p><p>&nbsp;</p><p>Ecma（TC39的母机构）副总裁兼Bloomberg&nbsp;JS开发者体验软件工程师Daniel Ehrenberg建议，“如果大家在开发中需要用到大量并非出于自己之手的代码，那么理解它的类型构造肯定会有所帮助。”</p><p>&nbsp;</p><p>尽管静态类型在JavaScript社区中一直存在争议，但考虑到长期有社区成员希望引入这项功能（静态机制连续三年在JavaScript现状调查中被评为最迫切需要的功能），标准流程已经基本证明这种方法能够为JS语言锦上添花、同时又不会影响其吸引新手开发者的简单特性。</p><p>&nbsp;</p><p></p><h2>用更智能的消息格式简化本地化过程</h2><p></p><p>&nbsp;</p><p>这里给大家科普一下，对网站和Web应用程序的本地化绝不止是替换掉用户界面中的消息字符那么简单。毕竟文本内容不只包含单词，还涉及数字、序数词（第一、第二等）、日期、得数和其他各种不同的语言结构，所以单纯替换单词得到的结果可能根本就不像人话。</p><p>&nbsp;</p><p>目前已经有一些库能帮助解决这个问题，例如FormatJS等。但与Java和C等其他语言相比，JS开发者和翻译人员的工作量还是要更大一些。毕竟人家Java和C都拥有支持国际Unicode组件（例如ICU4J和ICU4）的内置字符串翻译和格式化功能。</p><p>&nbsp;</p><p>开源咨询公司Igalia的Romulo Cintra在采访中指出，“复数部分真的很难处理。所有语法概念、词形变化和性别/阴阳性数字，以及不同的占位符在各语种之间总有种种差别。处理这种复杂性既依赖于库，也需要大量数据的支持。”</p><p>&nbsp;</p><p>实际上，浏览器已经在使用这些组件推动软件本地化，不少API也能调整日期/时间格式和相对时间格式。那么，为什么不为Web引入类似的选项，并给JavaScript开发人员提供包含专业语言知识的内置选项呢？毕竟，软件已经是最适合面向全世界的产品了。</p><p>&nbsp;</p><p>TC39提出的Intl MessageFormat是另一项一阶提案，他们与Unicode联盟的消息格式工作组合作，希望引入包含国际化和本地化逻辑的模板化字符串，将在JavaScript中以内置引擎的形式用不同语言正确填写模板字段。</p><p>&nbsp;</p><p>但在Web中引入国际化API其实是项影响深远的调整，甚至会引发对拥有20年历史的ICU消息格式API的重大更新。Cintra好奇的是，“原本的API只支持字符串、非常僵化，完全没有模块化特性——所以，为什么不从根本上解决问题，直接上马Unicode新标准呢？”而这正是MessageFormat 2.0（简称MF 2.0）的意义所在，它在设计上就是为软件和Web的国际化而生、支持Intl MessageFormat，Cintra甚至认为它就是帮助Web吸引又一个十亿用户的关键。</p><p>&nbsp;</p><p></p><blockquote>“它将突破固有循环，在本地化和个性化层面让Web的可访问性来到新的层次。”</blockquote><p></p><p>&nbsp;</p><p>Cintra认为，“对我来说，作为一名非英语母语者，Web的发展方向就是让更多人能够随意访问。这一点非常重要，也只有达成这个目标，我们才有机会让所有努力开发的软件内容变得更易于访问。绝对值得期待！”</p><p>&nbsp;</p><p>目前，大部分本地化主要依靠在运行阶段解析的自定义消息格式及专有规范来实现。Mozilla公司靠的就是自家Fluent工具来翻译所有界面。Igalia的Ujjwal Sharma告诉我们，Bloomberg也有自己的内部工具。“每个人都在尝试通过不同的定制化工具解决同一个问题。”虽然Intl MessageFormat肯定能为已经涉足国际化工作的组织建立起通用标准，同时也继承开放协作的各种固有优势，但Sharma更希望它能为那些还没有固定网站翻译流程的小型组织提供助力。</p><p>&nbsp;</p><p>除了简单翻译文本字符串之外，MF 2.0还将包含元数据和注释，可用于标记从文字证据（包括正式和非正式）到语音合成提示的全部内容，这样就能更好地支持Siri、Alexa等智能扬声器和屏幕阅读工具。新标准还有望消除瓶颈，“语音和界面的许多创新都发生在客户端”，但数据量不足往往成为制约本地化工作的关键。Sharma认为，“有了Intl MessageFormat，我们可以在客户端落地更多愿景。”</p><p>&nbsp;</p><p>本地化是个价值数百万美元的行业，因此必须要在兼容性和必要改进之间寻求平衡。Sharma指出，“最好能提供一个易于使用且足够直观的API，同时保证仍能跟传统工作方式对接起来——这明显是个极具挑战的任务。但我认为，只要能把这个问题解决好，就能彻底改变人们对于网站的看法。”</p><p>&nbsp;</p><p></p><h2>一种用于语言翻译的语言</h2><p></p><p>&nbsp;</p><p>Igalia负责ICU实现的Tim Chevalier解释道，MF 2.0规范定义了一种简单的编程语言，拥有名称绑定（’let’声明）和模式匹配（选择器）。他建议大家将其视为“一种用于编写可翻译消息的特定领域语言”，能够充分运用关于现有编译器和解释器的知识积累。</p><p>&nbsp;</p><p>“希望开发人员在用上MF 2.0之后，体验不再像过去那样要编写一大堆神秘的字符串，而更像是在自己选定的通用语言（例如JavaScript）中使用另一种专用语言进行编程。”他还把这种改进，比喻成从数据库硬编码查询客串到SQL的功能飞跃。</p><p>&nbsp;</p><p>“曾几何时，根本就没有可靠的方法能编写程序来操纵对字符串的查询，以便生成给定查询的变体，因为人们不知道要怎么向C++、Java和JavaScript等通用语言当中嵌入查询语言。现代语言工具已经提供更丰富的查询构造方法，不再仅仅依赖于编写字符串并将其作为参数传递给函数。”MF 2.0就承诺为开发人员提供类似的使用体验。</p><p>&nbsp;</p><p>而且MF 2.0还具备可扩展性：开发人员将获得一个接口，可以用JavaScript（或者其他语言）创建自己的新抽象以实现格式化函数。该规范还能降低高级翻译工具的编写难度，借这些工具提供更友好的用户界面，让翻译人员在不必学习硬核编程知识的前提下轻松处理文本消息。</p><p>&nbsp;</p><p>MF 2.0的ICU工作仍处于早期阶段，目前仅提供技术预览版。最早的成果是ICU4J，但目前正被移植向ICU4C，即大部分JavaScript引擎使用的语言。TC39提案依赖于MF 2.0，而“一旦实现了MF 2.0的稳定版本，TC39那边的工作就能顺利推进。”Cintra还推测，稳定版将允许在浏览器上实现该API，让更多开发者感受到它的实际效果。</p><p>&nbsp;</p><p>如果大家想亲自试试看，FormatJS已经为初始IntlMessageFormat提案提供早期支持：</p><p><a href=\"https://formatjs.io/docs/intl-messageformat/\">https://formatjs.io/docs/intl-messageformat/</a>\"</p><p>&nbsp;</p><p>正在开发一个实验性的polyfill：</p><p><a href=\"https://github.com/messageformat/messageformat/tree/main/packages/mf2-messageformat\">https://github.com/messageformat/messageformat/tree/main/packages/mf2-messageformat</a>\"</p><p>&nbsp;</p><p>由于该polyfill基于已经得到广泛应用的React及其他应用国际化工具，因此将为开发人员提供所需的全新语法体验。</p><p>&nbsp;</p><p></p><h2>里程碑可期，但仍在很多工作要做</h2><p></p><p>&nbsp;</p><p>Palmer表示，JavaScript将继续缓慢发展。“有些人觉得不妨让变革的脚步快一些，以便我们能够获取反馈、发现重要问题并进行迭代；但我们TC39的行事风格往往更偏保守。”</p><p>&nbsp;</p><p>如果事实证明一旦发现设计有误且需要修正（比如Web组件的实现），那从Web平台中移除内容的成本其实非常之高。相比之下，Polyfill和基于工具的功能实现（标准候选版）既能实现更快的反馈周期、保护Web兼容性，又无需让开发人员苦等正式流程走完所有流程。</p><p>&nbsp;</p><p></p><blockquote>“我真心认为，JavaScript保守的开发风格是很有意义的。”</blockquote><p></p><p>&nbsp;</p><p>Ehrenberg表示，“我们有很多实现方案，也有很多需要考量的用途。其他平台可以充当更具实验性的环境，而我们还是选择成为更保守的实现环境。”</p><p>&nbsp;</p><p>按照这种模式，他提到JavaScript本身现在包含大量以往开发者只能通过外部工具获取的功能（包括迭代器助手等则是受到其他语言的启发）。</p><p>&nbsp;</p><p>Ehrenberg解释道，“在处理类字段，包括私有字段和装饰器，还有像hashbang语法这样的小问题时，肯定希望我们能早点拿出类型注释和新的功能模块。它们其实就是在继承人们已经在工具中获得的功能，我们想把它们纳入语言本体。CSS的情况也差不多，同样是把很多通过工具实现的功能引入到核心语言当中，例如嵌套（经典示例）和作用域，还有层和变量等。”</p><p>&nbsp;</p><p>此外，标准化还为开发人员的设置提供非常明确的默认值，有助于最大限度减少项目创建过程中涉及的配置操作，这同样给JavaScript的发展带来了新灵感。展望未来，Ehrenberg还提出另一个值得关注的方向：响应式用户界面的核心组件，例如signals和cells，有可能会成为JS语言的一部分。毕竟当今开发人员在前端框架上经常会用到。</p><p>&nbsp;</p><p>这就是他常说的，JavaScript要走“增量式共享的发现道路”。通过实验为核心功能找到新的基准，再把成果内置到语言当中。他还坚持强调，JavaScript仍有巨大的发展空间。</p><p>&nbsp;</p><p>“我觉得启发和灵感还有很多，我们远远没有把这些全部转化成现实。”</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://thenewstack.io/whats-next-for-javascript-new-features-to-look-forward-to/\">https://thenewstack.io/whats-next-for-javascript-new-features-to-look-forward-to/</a>\"</p><p>&nbsp;</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/819c7ae045ecc83936afed4d1\">全网最全&nbsp;ECMAScript&nbsp;攻略</a>\"</p><p><a href=\"https://www.infoq.cn/article/dDXbcLHT7teNYSPL3sm7\">“TypeScript 不值得！... 反向迁移到&nbsp;JavaScript&nbsp;引争议</a>\"</p><p><a href=\"https://xie.infoq.cn/article/a4f9a16ebc0f866e312a06176\">JavaScript&nbsp;作用域深度剖析：动态作用域</a>\"</p><p><a href=\"https://xie.infoq.cn/article/6ff79700fb3bfa972c1beebf3\">TypeScript 与&nbsp;JavaScript：你应该知道的区别</a>\"</p>",
    "publish_time": "2023-08-21 11:38:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一个关于十年和五万的开源历程——Apache ECharts开源之路",
    "url": "https://www.infoq.cn/article/97daSyAVxs228gsiUJeg",
    "summary": "<p></p><h2>前言</h2><p></p><p></p><p>中国互联网最为兴盛繁荣的那个年代，也是Web前端技术领域飞速发展的时代，中国各家互联网公司均大规模进行了人才的军备竞赛，充裕的人力加上不断飞速发展的业务，让各个公司的前端团队都出于业务、技术或者团队的考虑，造出了大量轮子。</p><p>&nbsp;</p><p>绝大多数的轮子都慢慢因为业务、组织的原因逐渐凋零，极少数的产品活了下来，成为行业中大家膜拜的标杆产品。</p><p>&nbsp;</p><p>Apache ECharts，应该能算得上其中的一份子，这个诞生于百度的数据可视化产品，从发布之日算起，至今刚刚有十年的时间。它在 Github 上拥有的五万多的关注数（star）充分说明了这是一个由国人打造的极具影响力的开源技术产品。</p><p>&nbsp;</p><p>这样产品的背后，是怎么样的一群人在为之付出？这十年，这些人是如何一步一步走到今天的呢？</p><p>&nbsp;</p><p>基于 QCon 2023 中的分享，以及各种历史材料，本文将分享一下 Apache ECharts 十年来的开源故事。</p><p>&nbsp;</p><p></p><h2>Apache ECharts 介绍</h2><p></p><p></p><p>在回顾历史故事之前，我们先简单地介绍一下什么是Apache ECharts。</p><p>&nbsp;</p><p>Apache ECharts （下简称 ECharts）是一个基于 JavaScript 的开源可视化图表库，提供了丰富的图表类型，丰富的可配置化参数，开箱即用，对于大部分图表需求，ECharts的上手使用成本极低。这也是十年前 ECharts 立足的重要产品力之一。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc834defe4f4ca9982d92e4f33104e46.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/67dae480cd5d5801b8bf272a6c0282ef.png\" /></p><p></p><p>&nbsp;</p><p>而对于复杂的可视化需求，ECharts 也提供了非常强大的自定义扩展能力，比如上图中的左上角部分，是一个篮球场的投篮命中率的分布图，这并不是一个常规图表，但用户基于 ECharts 提供的扩展能力能够可以实现上图中各种各样的可视化需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9df8bea4452b9b9bd1822d2b9c2a3a79.png\" /></p><p></p><p>&nbsp;</p><p>对于 ECharts 来说，它希望提供给最终查看数据的用户，不仅仅只是一个静态的数据展现，它还希望能帮用户更好地理解数据、分析数据，所以 ECharts 从最初版本就开始提供了大量交互和动态展现的能力。比如上图中，展现的就是一个左侧地图拖拽选择和右侧条形图的分析联动。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/27de895e54b169365a92376a62a10391.png\" /></p><p></p><p>在 ECharts 诞生的时候，曾经不断强调数据可视化是大数据时代的最后一公里，那大数据的大要如何怎么体现呢？ECharts 选择了具备超大数据量渲染性能，从最早的十万量级、百万量级，到 ECharts 5.0 的亿级，通过大量的算法优化、使用WebGL加速等手段，帮助开发者在开发一些数据大屏时得心应手。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a7897dd3e8e2d43a802356ce58c6db11.png\" /></p><p></p><p>ECharts 在4.0版本发布时，同步也发布了其三维的可视化解决方案，ECharts GL，提供了包括地理可视化，函数可视化（如上图）。充分体现了团队在可视化领域上的技术实力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfdc979bee586cb6bb0c9e6453a8a61e.png\" /></p><p></p><p>在与 ECharts 团队交流的过程中，发现团队对于 ECharts 在可访问性上的支持，是团队特别引以为傲的功能。</p><p>&nbsp;</p><p>ECharts 提供了贴花的方案，帮助色弱人群区分图表里的不同数据系列。</p><p>&nbsp;</p><p>ECharts 充分遵循 W3C 标准，能够让开发者定制图表要表达的内容，让视障人群通过听的方式了解图表里的内容。同时，ECharts 还提供了基于规则的自动图表描述，例如上图中，针对左侧图表，ECharts 会生成右边的一段描述文字，当视障人群在访问这个页面的时候，可以听到相关的描述。</p><p>&nbsp;</p><p></p><h2>Apache ECharts 发展历史</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/036367f66e1fd8ae67a55fdfae3687b5.png\" /></p><p></p><p>Apache ECharts 从 2012 年 8 月立项，在 2013 年 6 月 30 日正式发布了 1.0 版本，到今年（2023年）的 6 月份正好是 10 周年。在 1.0 发布后的一年后， 2014 年 6 月 30 日，ECharts 发布了 2.0 版本。从立项到 2.0 的发布，可以算 ECharts 的早期诞生阶段。</p><p>&nbsp;</p><p>接下来的三年多时间，从 3.0 的 alpha 到 3.0 正式版，再到 4.0 版本，这是 ECharts 的第二个发展阶段，在这期间是以百度的团队为核心发布版本。</p><p>&nbsp;</p><p>2018 年 1 月，ECharts 进入 Apache 基金会进行孵化，由此转为社区运营状态，这个过程中 ECharts 得到了较好的发展，尤其是在 2020 年的 6 月份，达成了 100 个社区贡献者的成就，进入 Apache 基金会以后，社区的力量帮助 ECharts 吸引了很多贡献者。</p><p>&nbsp;</p><p>2021 年 1 月份 ECharts 顺利从 Apache 基金会毕业，同时也发布了 5.0版本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a27eb0d1b7851dfb804049d63c5974b.png\" /></p><p></p><p>随着版本不断迭代，Apache ECharts 在开发者社区内得到越来越多的关注，在Github中获得了超过五万的关注数。</p><p>&nbsp;</p><p></p><h3>从 2.0 到 5.0 的各个大版本执念</h3><p></p><p>&nbsp;</p><p>ECharts团队认为大版本发布，是一个非常严肃而正式的事情，因此也需要有非常仪式感的发布形式，在资源有限的情况下，团队会找寻各种可利用的资源，或是自己掏腰包，来进行大版本发布的宣传运作。</p><p>&nbsp;</p><p>对于一个技术团队来说，除了做好每个版本的技术工作，学习如何运营、如何做市场宣传运营，也是非常有意思的事情。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/1910fd6319d9843a40955d28d84c989a.png\" /></p><p></p><p>上图是 2.0 的官网，从 2.0 开始，ECharts 有了自己的设计师，也因为有了设计师，ECharts 终于有了 logo----小鲸鱼。</p><p>&nbsp;</p><p>为了营造 2.0 的宣传氛围，当时还做了分节奏的宣传海报，先放了左边这张图，代表比 ECharts 1.0 还好用的图表即将跃出海面，小鲸鱼还埋在地平线下，满屏幕充斥着各种不同语言的“二”。然后再放出第二个图，ECharts 2.0 正式发布，小鲸鱼浮出海面。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8e415197f4e4bd2df1973e5529cdaee.png\" /></p><p></p><p>在 2.0 发布以后，面临着用户需求越来越复杂，技术实现时，发现原有的技术架构，已经无法很好适应新的需求迭代，效率极低。再加上创始人离职，所以大家都很希望通过 3.0 来证明自己。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb4688f19125dffe112ac9cbf9591ae1.png\" /></p><p></p><p>因为全身心投入到了 3.0，导致在 3.0 正式发布前的好几个月，ECharts没有任何版本更新，用户因此经常通过邮件或微博上问 ECharts 是不是黄了。</p><p>&nbsp;</p><p>好不容易 3.0 发布，但因为整个架构调整太大，BUG太多，导致头几个版本骂声一片，导致前期的口碑和反馈特别差。</p><p>&nbsp;</p><p>好在经过团队的努力，在3.0 发布后的4 个月内，连续发布了 12 个版本来修复各种各样的 bug，用户终于看到了新的大版本的优点和变化，再次接纳了 ECharts。</p><p>&nbsp;</p><p>基于新的架构，3.0 迭代速度越来越快，用户的需求和大家的技术想法得到更好的落地，经过长时间积累，4.0 也顺理成章地到来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/6656466dfc6ac362087219cc2f4e9424.png\" /></p><p></p><p>ECharts团队一直渴望有一次正式的发布会，而 4.0 达成了团队的这个心愿（上图）。</p><p>&nbsp;</p><p>4.0 发布的同时，也是百度开源如火如荼开展的时候，团队借势将 ECharts 捐献给了 Apache，帮助公司提升在开源领域的影响力，也为产品的未来发展开启了一条新的大路。</p><p>&nbsp;</p><p>ECharts 捐献到 Apache 以后，不管是团队内部角度，还是从公司角度，都希望它能够成为百度第一个从 Apache 正式毕业的顶级项目。</p><p>&nbsp;</p><p>在这个过程中面临着很多挑战，最大的挑战来源于对于 Apache Way的适应。首先是效率之争，大家日常办公中习惯了用 IM 工具，甚至当面或以集中办公这种方式去沟通项目和技术方案，但是 Apache Way 要求以邮件的方式公开沟通，大家一开始认为效率太低。</p><p>&nbsp;</p><p>但当真正去适应、尝试之后，团队发现了其中的优势。通过邮件沟通的时候，每个人都需要尽可能地在邮件里把所要表达的内容写的非常详细，包括你的思考是什么，结论是什么，建议等都在邮件里，同时，大家的回复非常清楚。阅读邮件时，所有的上下文都能够通过邮件完整、明确地进行回溯。反而使得沟通效率变高。</p><p>&nbsp;</p><p>适应了 Apache Way后，更多的贡献者加入到了 ECharts 5.0 的开发中，已经充分适应远程协同办公的团队，让 5.0 没有受到口罩的影响，5.0 也通过视频直播的方式，完成了线上发布会。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4d529223827bd0813f91c9ae4555eb1.png\" /></p><p></p><p></p><h2>和 ECharts 相关的一些人</h2><p></p><p></p><p>除了产品自身的发展故事，产品背后的人与团队的故事，也吸引着我们去深度了解。</p><p>&nbsp;</p><p></p><h3>干一行爱一行的创始人</h3><p></p><p></p><p>说到 ECharts 的团队，许多人首先想到的就是它的创始人，林峰。除了 ECharts，林峰还有个身份，是 AntV 的前负责人。可以说，中国的两大可视化产品，都和他有着密切的关系，不得不说，林峰担得起中国前端数据可视化领军人物这一殊荣。</p><p>&nbsp;</p><p>据说林峰当年加入百度前端团队之前，对Web前端几乎没有太多了解，面试前才花了一段时间突击开始学习，就是这段突击，改变了中国前端数据可视化的格局。</p><p>&nbsp;</p><p>林峰加入百度的那些年，正是互联网以及Web前端领域欣欣向荣的年代，前端技术轮子一个接一个造得如火如荼。许多团队都把自己拥有多少开源产品作为团队影响力的重要参考。而各家公司冗余的人才招聘措施，也让各个团队在照顾好业务的同时，有足够的资源参与到技术产品的研发工作中来。</p><p>&nbsp;</p><p>林峰加入的团队，历经几年的组织变更后，成立了EFE团队，这个团队也同样产生出了大量具备团队特征的技术开源产品，如今都好能在团队的Github中看到踪影，但绝大多数产品已经完成了其历史使命，或只是在百度内部个别产品现服役，并没有成为一个有影响的社区开源产品。或者是说，大部分产品，在创立之初，并没有希望成为一个社区有影响力的开源产品的目标，只是务实地为了满足公司内部业务，顺手在Github上开了个源。说句题外话，这种操作方式在那些年各家公司的前端团队并不少见，但近些年，随着各家公司对于开源的管控越来越正式，这种现象在大公司中已经减少了许多。</p><p>&nbsp;</p><p>ECharts 也是在EFE成立后，在2012年8月正式立项，并于2013年6月开源发布，在之后，我们能够频繁地在各种技术社区中，看到林峰为 ECharts 发声宣传，据不完全统计，在2013到2015年，林峰累计进行了超过了40次对外的分享。</p><p>&nbsp;</p><p>那篇经典的《Why ECharts》的在线幻灯片，跟着林峰走遍了大江南北。</p><p>&nbsp;</p><p>林峰在技术上是如何推动 ECharts 开始的，我们不得而知，但在回顾林峰的宣传历史时，我们不难发现，他并没有简单把 ECharts 仅仅作为一个技术产品在各家前端团队中进行宣传。他敏锐地结合了大数据时代及数据新闻的趋势，把 ECharts 的产品价值从一个技术提效工具，提升到了业务价值提升的解决方案。</p><p>&nbsp;</p><p>这是大部分技术人员所不具备的视野和产品力，但这些都是可以通过实践习来，真正让林峰从众多产品负责人中脱颖而出的关键，是他干一行爱一行的能力。用当前互联网大厂的黑话，可以称之为信念感，或使命感。</p><p>&nbsp;</p><p>在做前端之前，他没有前端经验，但却在做ECharts之前，成为了团队中的技术中流砥柱；在做ECharts之前，他没有做过数据可视化，但却成为了后来中国两大前端可视化产品的重要角色。</p><p>&nbsp;</p><p>一个和林峰共事多年的人对他做出了这样的评价：</p><p>&nbsp;</p><p>“不卑不亢，不骄不躁，总是怀着空杯与谦虚的心态，拼命吸收新领域的知识，但充满自信和信念，坚定自己从事的事情会为他人带去极大的价值。”</p><p>&nbsp;</p><p></p><h3>低调前行的核心团队</h3><p></p><p>&nbsp;</p><p>不少人一直把 ECharts 等同于了林峰，但其实林峰早在15年便离开了百度，从这之后，百度逐渐建立了一只团队，持续在维护 ECharts，但直到2018年，这只团队才借助 ECharts 4.0 的发布，完整地出道。</p><p>&nbsp;</p><p>为了弥补林峰离职的影响，EFE攒出了一只麻雀虽小五脏俱全的精英团队，里面有来自浙大数据可视化专业硕士毕业，师从陈为教授的高徒；也有后来成为 Apache Member 的羡辙；还有经常代表 ECharts 出席各种技术大会的几位架构师。</p><p>&nbsp;</p><p>这个精英团队从 ECharts 发布后的第三年开始，成为了产品的真正脊梁，扛着产品，走过了七年。如今的 ECharts，从技术架构和社区建设的关键几个环节，都是经这个团队之手。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b88c3ebc44c984ea0fdf87479b3e889.png\" /></p><p></p><p>只是很可惜，这个团队过于低调，我们只能从 QCon 中祖明的分享中，了解到少许这个团队内部的工作情况，诸如大家经常一起到某个工程师家里封闭边娱乐边办公。希望未来有机会我们能够更多地了解到这只核心团队的故事。</p><p>&nbsp;</p><p>但 ECharts 真正的未来，我们相信，依然在社区，随着在 Apache 成功孵化毕业，ECharts的外部贡献者越来越多，最近一年来 Apache ECharts 最活跃的开发者，已经是一位非百度的程序员，ECharts 虽然走得比以前慢，但详细，社区的力量，会让其走得更久。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>第一个是从 ECharts 本身的发展来说，ECharts 的立足点很直接，就是面向百度自己的产品线业务，服务于业务，老板自然会给资源去做这个事，但是做这个事情的人，需要有高瞻远瞩的理想，这样才会吸引更多志同道合的人跟你一块做这个事情。</p><p>&nbsp;</p><p>第二点不一定适合所有产品，即应该适当的内藏外秀，处于公司或者组织下，才能让你的项目更久远的活下去。有时候技术产品服务内部比服务外部更难，因为服务外部业务的时候，会相对能够按照一些偏标准化的方案去执行，或者说即使有不标准化的地方，你也有权利去拒绝，尤其是像 ECharts 这种不用背收入的项目，拒绝就拒了，也没什么影响，但是内部的产品找你，你敢拒绝，那就是文化价值观的问题了，非常的困难。所以有的时候服务内部确实比服务外部难度要大很多。再就是外面的背书，其实有时候比内部的背书更有用，你服务好内部人家觉得你理所当然，但是服务好外部别人来个表扬信，或者别人来夸你，会比内部说很多这句话都有价值，所以大家如果自己有一些小的技术产品想活得更好，建议把外面的客户抓得更好一些。</p><p>&nbsp;</p><p>第三个是长期发展，如果想变成一个特别好的甚至商业化产品的话，不是靠个人力量能实现的，它依赖整个组织的变革，要么是像 ECharts 这样选择一个更加中立的开源组织，然后从外部去找一些开源的商业的模式，要么就是成为组织的第二曲线。做的比较好的就是像百度现在的 sugar，它是一个非常好的典型，它本来也是只服务内部，但是现在变成一个百度云的产品，成为公司的第二曲线之一。</p><p>&nbsp;</p><p>还有在团队组建上，开源是一个非常需要耐心的事，需要招有长期主义思维的人，怎么理解长期主义思维？他不会被眼前的一些小恩小利或者波折影响，他是自己心里明白到底要什么，这个产品要什么的人，而且他很自驱，但不会急功近利。</p><p>&nbsp;</p><p>开源的核心团队。这也是人月神话里提到这种比较大的架构和产品的话，核心团队的能力要足够强，相对在一定范围内要独裁，而不是民主的。最后是共同的价值观，公开透明的合作决策方式，整个团队要有好的默契，最好是培养出很好的私交，让团队能够更润滑一些。</p><p>&nbsp;</p><p>最后一个就是从产品方向上，做自己喜欢的事非常重要，尤其对于开源，如果做开源，还不喜欢做这个事，就不知道图啥了。然后最好是能擅长，能擅长不是说现在擅长，因为做一个开源产品，对整个 ECharts 团队来说，对大家的成长还是非常有价值的，很多东西其实不知道自己擅不擅长，只有试了以后才知道，所以在这个过程中能帮助你发现你其实还有哪些空间可以去做。还有很重要的一点是要关注用户的反馈，因为开源天生的反馈在 issue 里就很多，但是这里给大家提个醒，就是要结合一些反馈的上下文去看，要找到一些用户去问，你遇到这个问题到底是为什么? 你在什么样的场景使用？因为有的时候他直接反馈给你的东西，issue 里其实讲不明白，信息也相对有限。然后文档和 Showcase 也非常重要，什么是 Showcase，就是本来我想展示的东西没展示出来。最后就是产品功能可以多，但营销卖点要尽量专一，有时候你以为的卖点和用户想象的卖点是不一样的，要分清楚。用户想要的东西你直接说会让他觉得很 low，所以宣传的时候还是要高大上一些。</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247545699&amp;idx=3&amp;sn=d8b5508b8136adb405d4d9367822cc7d&amp;chksm=fbea8aaccc9d03badf450fff0cf3cb6bb552d57c16c8d6735ec555a00daf6287dc7d54782e72&amp;scene=27#wechat_redirect\">使用&nbsp;Apache&nbsp;ECharts&nbsp;呈现社区活动的渲染图</a>\"</p><p><a href=\"https://www.infoq.cn/article/9cvcLS4okZXT6S6lAYRS\">特斯拉是如何使用&nbsp;Apache&nbsp;ECharts&nbsp;的？</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247509777&amp;idx=1&amp;sn=e895df684c837898c5810fcce0ca0d79&amp;chksm=f9521252ce259b44957051e7fa68d590653aa626b61708d53f363f8f5a7e8338cbfdffd30d9e&amp;scene=27#wechat_redirect\">Apache&nbsp;ECharts&nbsp;团队：ASF&nbsp;顶级项目是怎么炼成的？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/12808c5b3d41917341d466a2f\">Apache&nbsp;HugeGraph1.0.0 版本正式发布！</a>\"</p>",
    "publish_time": "2023-08-21 11:39:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何在Next.js全栈应用程序中无缝实现身份验证",
    "url": "https://www.infoq.cn/article/HKfRavLNgxBAhyMoZAGw",
    "summary": "<p>&nbsp;</p><p></p><blockquote>本教程中，我们将一同了解如何使用Clerk向全栈应用程序添加身份验证机制。&nbsp;我们跟Clerk没有任何合作关系，但对这款工具的表现非常认可。很多朋友正好咨询怎么在Next.js下实现身份验证，这篇文章专为解决问题而来。</blockquote><p></p><p>&nbsp;</p><p></p><h2>背景介绍</h2><p></p><p>&nbsp;</p><p>身份验证一直是构建全栈应用程序中的一大主要痛点。特别是在Node.js环境当中，各种主流库和框架都没有内置auth-primitives。因此，开发人员不得不自己想办法构建身份验证解决方案。</p><p>&nbsp;</p><p>但从零开始构建安全身份验证是项颇为艰巨的任务。我们首先得对密码进行哈希和加盐处理，发布签名令牌来创建会话，同时防止各种恶意攻击向量。此外，大家还得保证自己的前端和后端能够相互通信、正常共享会话。</p><p>&nbsp;</p><p>好消息是，Express的Passport.js和Next.js的NextAuth等库就是为此而生，只是还不够完美。这些库的设置流程涉及多个步骤，虽然已经能较好地配合Google或GitHub等服务实现社交身份验证，但毕竟要比密码登录更困难。而且密码内容仍须存储在服务端数据库内，由软件开发一方承担全部安全责任。</p><p>&nbsp;</p><p>如今，登录时通过邮件验证、无密码登录和双因素身份验证已经相当流行。虽然前面讨论的库也能支持这些功能，但需要在本就复杂的设置之外再做更多额外工作。</p><p>这时就要请出托管身份验证提供程序Clerk了，它消除了身份验证中的所有难题，大大降低了妥善保护全栈应用程序的门槛。与其他托管身份验证提供程序相比，Clerk的开发者体验也明显做得更好。</p><p>&nbsp;</p><p>在本教程中，我们将运用Clerk及其全新App Router，在Next.js 13当中构建一款简单的全栈应用程序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bbe3e435ac04524c21204432bcc5f58d.png\" /></p><p></p><p></p><h2>设置</h2><p></p><p></p><p>首先在您终端中指定的文件夹中运行命令npx create-next-app@latest，从而创建新的Next应用程序。请按以下指定方式完成设置。需要注意的是，一定要在Tailwind CSS和App Router部分选择Yes。</p><p>&nbsp;</p><p><code lang=\"null\">Desktop npx create-next-app@latest\n✔ What is your project named? ... clerk-auth\n✔ Would you like to use TypeScript with this project? ... No / Yes\n✔ Would you like to use ESLint with this project? ... No / Yes\n✔ Would you like to use Tailwind CSS with this project? ... No / Yes\n✔ Would you like to use `src/` directory with this project? ... No / Yes\n✔ Use App Router (recommended)? ... No / Yes\n✔ Would you like to customize the default import alias? ... No / Yes</code></p><p>&nbsp;</p><p>现在我们切换到刚刚创建的clerk-auth文件夹，安装唯一的依赖项：Clerk。</p><p>&nbsp;</p><p><code lang=\"null\">npm install @clerk/nextjs</code></p><p>&nbsp;</p><p>接下来需要创建一个Clerk账户和新项目，获取要用到的API密钥。这就需要转到clerk.com，创建一个账户，之后在仪表板上单击“Add application”。</p><p>&nbsp;</p><p>将应用程序命名为clerk-auth-demo，并选择Email + Google的登录方式。如果需要，大家还可以添加其他登录方式。请放心，这不会对开发过程产生任何影响，Clerk为替我们完成所有工作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/53de94de4f015539a67a9aea0196fa65.png\" /></p><p></p><p>&nbsp;现在，Clerk会自动提供要添加到Next应用程序的API密钥。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8f087b503e747cf6d6c06d7a9d176d8c.png\" /></p><p></p><p>&nbsp;因此，请创建一个.env.local文件，把Clerk那边复制的内容全部粘贴进来。</p><p>&nbsp;</p><p><code lang=\"null\">NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_****\nCLERK_SECRET_KEY=sk_test_******</code></p><p>&nbsp;</p><p>要完成Clerk身份验证配置，最后一步就是把这款身份验证提供程序添加到/src/app/layout.tsx文件当中。如果大家比较熟悉传统的Next.js页面范式，会发现其内容跟/src/_app.tsx文件差不多。</p><p>&nbsp;</p><p><code lang=\"null\">import { ClerkProvider } from '@clerk/nextjs';\nimport './globals.css';\nimport { Inter } from 'next/font/google';\nconst inter = Inter({ subsets: ['latin'] });\nexport const metadata = {\n  title: 'Create Next App',\n  description: 'Generated by create next app',\n};\nexport default function RootLayout({ children }: { children: React.ReactNode }) {\n  return (\n    \n      \n        {children}\n      \n    \n  );\n}</code></p><p>&nbsp;</p><p>ClerkProvider中提供一项appearance属性，我们可以在其中定制Clerk组件以匹配应用程序的设计风格。每个Clerk组件还能单独设置样式。到这一步，我们就能在应用程序中使用Clerk了。</p><p>&nbsp;</p><p></p><h2>向应用添加身份验证</h2><p></p><p></p><h3>登录和注册页</h3><p></p><p></p><p>首先，我们需要创建注册和登录页。Clerk已经提供了完整的表单组件，剩下要做的就是利用这些组件构建一个简单的示例页面。</p><p>&nbsp;</p><p>我们从登录页开始。使用以下内容，在/src/app/sign-in/[[..sign-in]]/page.tsx&nbsp;中创建一个新组件：import { SignIn } from '@clerk/nextjs';</p><p>&nbsp;</p><p><code lang=\"null\">export default function SignInPage() {\n  return (\n    </code></p><div><code lang=\"null\">\n      \n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这里的文件路径可能跟大家习惯的传统Next.js应用有所区别，其中页面URL由/src/app/sign-in文件夹来定义，代表着页面实际上位于/sign-in。中括号用于捕捉Clerk内部使用的/sign-in/...&nbsp;之后的所有内容。使用新的App Router功能，页面本体将始终存放在page.tsx文件之内。</p><p>&nbsp;</p><p>至于/src/app/sign-up/[[..sign-up]]/page.tsx注册页面，处理方式也基本相同：import&nbsp;{&nbsp;SignUp&nbsp;}&nbsp;from&nbsp;'@clerk/nextjs';</p><p>&nbsp;</p><p><code lang=\"null\">import { SignUp } from '@clerk/nextjs';\n\n\nexport default function SignUpPage() {\n  return (\n    </code></p><div><code lang=\"null\">\n      \n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>现在转到http://localhost:3000/sign-in&nbsp;，就能看到预制好的注册组件，它支持电子邮件、密码或者大家指定的任何社交身份验证提供程序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/30/3099132234dbbc614b74a21c2bd01098.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>账户页面</h3><p></p><p></p><p>创建一个账户，或者通过Google进行登录。到这里，我们已经完成了应用登录，但目前的功能还比较有限。所以我们需要创建账户页面，首先将/src/app/page.tsx&nbsp;文件中的内容变更为：</p><p>&nbsp;</p><p><code lang=\"null\">import { UserButton } from '@clerk/nextjs';\nexport default function Home() {\n  return (\n    </code></p><div><code lang=\"null\">\n      <div>\n        <p>Hello, User!</p>\n        \n      </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这里我们使用到Clerk的UserButton组件。登录之后，它将为提供User Setting的下拉菜单，用户可以在其中更改密码、电子邮件地址和其他各种设置。这些功能是收费的，但毕竟能帮我们省下自行开发验证带来的时间和精力投入。</p><p>&nbsp;</p><p></p><h2>保护页面</h2><p></p><p></p><h3>Secret页面</h3><p></p><p>&nbsp;</p><p>现在我们需要在/protectet上创建一个新页面，要求该页面只能由经过身份验证的用户访问。为此，我们需要在/src/middleware.ts&nbsp;中创建一个新的中间件，内容如下：</p><p>&nbsp;</p><p><code lang=\"null\">import { authMiddleware } from \"@clerk/nextjs\";\nexport default authMiddleware({\n  publicRoutes: [\"/\"]\n});\nexport const config = {\n  matcher: [\"/((?!.*\\\\..*|_next).*)\", \"/\", \"/(api|trpc)(.*)\"],\n};</code></p><p>&nbsp;</p><p>此外，将以下新变更添加到.env.local文件当中，以告知Clerk在需要重新定向时如何操作。</p><p>&nbsp;</p><p><code lang=\"null\">// other settings\nNEXT_PUBLIC_CLERK_SIGN_IN_URL=/sign-in\nNEXT_PUBLIC_CLERK_SIGN_UP_URL=/sign-up\nNEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL=/\nNEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL=/</code></p><p>&nbsp;</p><p>Clerk提供的这个中间件，将确保只有root页面和注册/登录页面对未经身份验证的用户可见。现在让我们在/src/app/protected/page.tsx上创建页面：</p><p>&nbsp;</p><p><code lang=\"null\">export default function Protected() {\n  return (\n    </code></p><div><code lang=\"null\">\n      <div>\n        <p>This is a protected page</p>\n      </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这样在登录和注销时，就会转向这个页面。请注意，如果未能通过身份验证，访问者将被重新定向至/sign-in。</p><p>&nbsp;</p><p></p><h3>在主页中显示登录链接</h3><p></p><p>&nbsp;</p><p>当用户尚未登录时，我们的root页面目前不会显示任何信息。但现在中间件已经设置完毕，我们可以修改/src/app/page.tsx&nbsp;文件来更改此中间件：</p><p>&nbsp;</p><p><code lang=\"null\">import { UserButton, currentUser } from '@clerk/nextjs';\nimport Link from 'next/link';\nexport default async function Home() {\n  const user = await currentUser();\n  return (\n    </code></p><div><code lang=\"null\">\n      <div>\n        {user ? (\n          &lt;&gt;\n            <p>Hello, User: {user.emailAddresses[0]?.emailAddress}</p>\n            \n          \n        ) : (\n          \n            Sign in\n          \n        )}\n      </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这是一个React服务器组件，会使用await从Clerk异步获取当前用户会话。取决于会话是否存在，它会显示UserButton以及用户的电子邮件地址，或者指向登录页面的链接。</p><p>&nbsp;</p><p></p><h2>保护API路由</h2><p></p><p></p><p>到这里，我们已经讨论了如何保护应用前端。但全栈应用程序还有后端部分，为此我们将在新的App Router模式中使用/src/app/api/route.ts文件，借此在GET/api处创建一个后端端点：</p><p>&nbsp;</p><p><code lang=\"null\">import { auth } from '@clerk/nextjs';\nimport { NextResponse } from 'next/server';\nexport async function GET() {\n  const { userId } = auth();\n  if (!userId) {\n    return new Response('Unauthorized', { status: 401 });\n  }\n  const data = { message: 'Hello User', id: userId };\n  return NextResponse.json({ data });\n}</code></p><p>&nbsp;</p><p>这里的auth()函数会检查是否存在Clerk会话。如果不存在，则抛出401未经授权错误。而如果用户成功通过了身份验证，接下来就是设置用户能在端点上进行的操作了。我们可以访问userId，据此将数据库中的数据引用给用户。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>至此，我们已经在全栈Next.js 13应用程序中完成了Clerk Authentication的完整实施。可以看到，整个过程几乎无需编写任何身份验证代码就能正常起效！这也是Clerk等外部提供程序的魅力所在。更重要的是，我们的小小演示应用也内置了一系列用户管理功能，包括验证/更改电子邮件地址、更改密码和社交登录等，能帮开发者省下很多时间。</p><p>&nbsp;</p><p>对于同时拥有前端和后端的全栈应用程序，Clerk在Next.js等框架中有着相当出彩的表现。但如果匹配单独的后端，那在设置方面就要更复杂一些。Clerk可以发出JWT令牌，由开发者将其与API请求一同发往后端以验证用户身份。这种方式虽然可行，但整个过程肯定不如本文展示的那样无缝丝滑。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p></p><p><a href=\"https://dev.to/livecycle/seamless-full-stack-authentication-in-nextjs-11lp\">https://dev.to/livecycle/seamless-full-stack-authentication-in-nextjs-11lp</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://www.infoq.cn/article/VbDui4DRa2Lhq3h0ad2Z\">为什么说 Next.js 13 是一个颠覆性版本</a>\"</p><p><a href=\"https://xie.infoq.cn/article/93e23e080e828a8989a57a622\">Next.js&nbsp;实践：从 SSR 到 CSR 的优雅降级</a>\"</p><p><a href=\"https://www.infoq.cn/article/sITi66wc3mvcNs3PeRkb\">Next.js 13 新的实验性特性，实现 App“动态无限制”</a>\"</p><p><a href=\"https://www.infoq.cn/article/9G0lBWi2W58114ggfyge\">我们如何使用 Next.js 将 React 加载时间缩短 70%</a>\"</p>",
    "publish_time": "2023-08-21 11:47:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "前端全职岗位依然坚挺，广大同志不必惊慌",
    "url": "https://www.infoq.cn/article/FTfvi9TPlgAGIhDUDEtP",
    "summary": "<p></p><p>德国前端开发人员克里斯蒂安·海尔曼（Christian Heilmann）在这篇文章中阐述了前端开发作为全职工作的重要性和价值，并回应了一些人认为前端开发不是“真正”的开发工作的观点。</p><p></p><p>我最近刚好在找新工作。过程当中，很多雇主在收到简历后都表示满意，但随后又提到我好像太偏前端，想了解我在后端和全栈开发方面有没有经验。在我看来，这股歪风在 2021 年之前就吹过一波，总有人感觉 Web 开发或者说前端专家已经不足以支撑一个全职岗位了。所以我当时就做过如下解释。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4b/4b5c8e963b5074d242bc7f263fa6921b.png\" /></p><p></p><p>请注意：前端开发者擅长的不只是 HTML、CSS 和 JavaScript 这些“简单”的编程语言。前端开发者需要主动为未知的场景构建交互界面，他们的水平直接决定着最终用户的实际体验。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f4/f4dd8e2faa966214410c295b124a5849.png\" /></p><p></p><p>Web 绝不只是普通的编译目标，更是一个允许用户完全控制其外观风格的平台。它也是唯一一个拥有充足弹性，能够承受种种修改调整的平台。Web 同时面向桌面、Android 和 iOS，依托同一套代码库就能实现。只有敬业的前端开发者才清楚自己到底在干什么，而不会愚蠢地指望依靠一款插件或者扩展程序就奇迹般地让所有人都能获得满意的产品访问体验。</p><p></p><p>这里我还想再强调一句：无论最终选择什么平台，使用哪种编程语言，或者指定什么框架和库，最终跑在 Web 用户设备上的仍然是 HTML、CSS 和 JavaScript。</p><p></p><p>其中每一样（HTML 相对会好一点）都可能引发性能问题、跨浏览器功能冲突，并在难以预料的低配置、低网络质量环境中造成令人头痛的用户访问障碍。大家都知道，糟糕的性能表现只会让用户愤然“点叉”离去，某些服务无法正常访问甚至可能导致法律和合规性问题，导致我们被送上审判席。</p><p></p><p>我曾在全球最大的网站（包括 yahoo.com、bing、微软等等）和 Firefox、Edge 等浏览器上做过开发，这些开发商始终专注于一个目标：不要因为响应缓慢或者“错误”提示而被用户怒喷。所以我们得跟众多内部向外部合作伙伴携手，了解他们产品无法正常运行的原因。合作对象可能是扩展程序供应商、框架创建团队或者开发小组。在 Mozilla 和微软的“性能俱乐部”里，我们也一直在遇到各种问题：Web 产品中包含大量毫无意义的 HTML、几乎用不上的 CSS 和让人崩溃的 JavaScript，它们都在被无脑发送给用户。而这么做的原因，就是要让开发人员更便捷、更灵活地用一套框架搞定所有构建工作。</p><p></p><p>正是由于向全栈开发的转变，导致我们的 Web 体系越来越臃肿，它不仅拉低了客户满意度，也让用户平白付出了不必要的流量。</p><p></p><p>当然，造成 Web 产品质量堪忧还有另一个原因：组件设计缺乏大局观。</p><p></p><p>现在的 Web 产品压根不是按照文档或者作为网站进行构建的。相反，它们被视作一个个独立的组件，每个组件都非常灵活以适应不同的运行环境。这听起来很棒，但开发者开始随意进行组装，想起什么就塞进来什么。有时候，我们会发现 20 多种不同的、高度可定制的按钮组件，而它们执行的其实是同一种操作。</p><p></p><p>有经验的前端开发者肯定立马能察觉到其中的问题，并追踪到相应的资源浪费。</p><p></p><p>前端开发者究竟该如何定义？他们是：</p><p></p><p>浏览器性能专家跨平台开发专家辅助功能专家合规知识专家设计和测试部门间的桥梁最终用户的客服代表</p><p></p><p>但很多人的观念都被市场导向给扭曲了。他们之所以看不起前端开发者，主要是因为大多数人根本不知道做好这份工作需要哪些能力。</p><p></p><p>CSS 和客户端 JavaScript 不算真正的编码也完全是奇谈怪论。更讽刺的是，那些宣称自己不想碰 CSS 的家伙，给出的理由往往是“太难了，太怪了”。CSS 也不单纯是在调色加填充，它有自己的网格、子网格和伸缩布局框，这是一套完全成熟的布局系统，还能实现动画和响应式渲染。通过媒体和容器查询，开发者可以获得惊人的灵活性；通过级联层，大家甚至可以控制浏览器呈现当前设计的具体方式。</p><p></p><p>所以选择权在雇主手里。你可以聘请前端开发者专职构建自己的产品，也可以随随便便凑合搭建起来，再聘请性能和辅助功能顾问修复其中的问题。但请注意，越是来到生产下游，产品的优化和修复就越是困难，且往往会与新功能产生冲突。</p><p></p><p>所以当雇主们问我为什么“只做前端”时，我以自豪的心情解释了这一切，并强调我以身为前端工程师为荣。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/64/64e49de26ed551f8cae6fb6d40e18f0c.png\" /></p><p></p><p>附录：其实同样的道理也适用于市场上的其他职位。好的数据库工程师能帮你省下几秒钟的加载时间；优秀的云工程师能帮你节约云运营成本；优秀的后端工程师能确保你的服务器只跑有用的负载，用不着为一大堆根本没用的前端代码运行优化管线。如今科技大厂都在收缩规模，所以市场上的人才水准随之提升。也许请一位 10 倍全栈“技术大师”的预算现在没准够请 3 位专项技术专家，毕竟他们刚刚逃离要拿全部收入的 80% 交房租的“宇宙一线城市”。</p><p></p><p></p><h5>原文链接：</h5><p></p><p></p><p><a href=\"https://christianheilmann.com/2023/05/09/the-ongoing-defence-of-frontend-as-a-full-time-job/\">https://christianheilmann.com/2023/05/09/the-ongoing-defence-of-frontend-as-a-full-time-job/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/170673a775bec8d1e407bb035\">Web前端设计开发工具集（JS 框架、CSS 预处理）</a>\"</p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247485672&amp;idx=1&amp;sn=8f502dc871acfdc0d19f5bf79b6d5a3b&amp;chksm=f951bdabce2634bd1928eb241072d3bbaa436935d29b7979589befc7b76bf0225595ae28febf&amp;scene=27#wechat_redirect\">只需两步，快速提升你在前端圈的技术力和影响力</a>\"</p><p><a href=\"https://xie.infoq.cn/article/3da93330ebd71cf5b594cbb66\">从 0 到 1 普及前端知识|内容合集</a>\"</p><p><a href=\"https://xie.infoq.cn/article/bfc0d7c4415c2abd1c7811027\">使用 Lambda&nbsp;Web&nbsp;Adapter 在 Lambda 上 构建&nbsp;web&nbsp;应用</a>\"</p>",
    "publish_time": "2023-08-21 11:52:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]