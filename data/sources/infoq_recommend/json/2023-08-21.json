[
  {
    "title": "OpenAI设立Superalignment团队：让 AI 对齐人类价值观，预防潜在风险",
    "url": "https://www.infoq.cn/article/J8emKvQKNjHz3hVGamV6",
    "summary": "<p><a href=\"https://openai.com/\">OpenAI</a>\"宣布成立一个专门的<a href=\"https://openai.com/blog/introducing-superalignment\">Superalignment</a>\"团队，旨在防止流氓<a href=\"https://en.wikipedia.org/wiki/Superintelligence\">Superintelligent AI</a>\"的出现。OpenAI强调了使人工智能系统与人类价值保持一致的必要性，以及主动采取措施防止潜在危害的重要性。</p><p>&nbsp;</p><p>创造符合人类理想和目标的人工智能系统的过程被称为<a href=\"https://en.wikipedia.org/wiki/AI_alignment\">人工智能校准</a>\"。这需要确保AI系统理解伦理概念、社会标准和人类目标，并据此采取行动。AI校准旨在缩小人类需求和福祉与AI系统目标之间的差距。通过将AI与人类价值相结合，减少人工智能的危害，增加其潜在的优势。</p><p>&nbsp;</p><p>OpenAI的<a href=\"https://openai.com/blog/introducing-superalignment\">Superalignment</a>\"团队将专注于促进对AI校准的理解和实现。这是一个确保AI系统按照人类价值和目标行事的过程。通过研究强大的校准方法和开发新技术，该团队旨在创建在其整个发展过程中始终以人为本的人工智能系统。</p><p></p><p></p><blockquote>OpenAI表示：“我们的目标是在四年内解决超级智能校准的核心技术挑战。”</blockquote><p></p><p>&nbsp;</p><p>OpenAI联合创始人兼首席科学家Ilya Sutsker和校准主管Jan Leike表示，像GPT-4（ChatGPT的基础）这类模型当前使用的AI校准技术，都依赖于从人类反馈中进行<a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">强化学习</a>\"。不过，这种方法依赖于人类的监督，如果AI的的智力超越了人类，变得比它的监督者更聪明，这种方法可能就行不通了。Sutsker和Leike进一步解释说，其他一些基本假设，比如在部署过程中有良好的泛化属性，或者在训练过程中无法检测和削弱监督，在未来也可能被打破。</p><p>&nbsp;</p><p><a href=\"https://en.wikipedia.org/wiki/AI_safety\">AI安全</a>\"将成为一个重要的产业。世界各国政府正在采取措施制定法规，解决人工智能各个方面的问题，包括数据隐私、算法透明度和伦理考量。欧盟正在制定全面的《<a href=\"https://artificialintelligenceact.eu/the-act/\">人工智能法案</a>\"》，美国也在采取措施制定《<a href=\"https://www.whitehouse.gov/ostp/ai-bill-of-rights/\">人工智能权利法案蓝图</a>\"》。在英国，<a href=\"https://www.gov.uk/government/news/initial-100-million-for-expert-taskforce-to-help-uk-build-and-adopt-next-generation-of-safe-ai\">基金会模型人工智能工作组</a>\"已经成立，旨在研究调查人工智能的安全问题。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/openai-superalignment-ai-safety/\">https://www.infoq.com/news/2023/07/openai-superalignment-ai-safety/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/datmEqVmS134ewFO7wel\">OpenAI或于2024年底破产？大模型太烧钱了，快把OpenAI烧没了！</a>\"</p><p><a href=\"https://www.infoq.cn/article/IzPVkcZg0jeHGcD4xP7H\">OpenAI 推出网络爬虫 GPTBot，引发网站抵御潮：信息被爬走就很可能意味着永远无法删除</a>\"</p>",
    "publish_time": "2023-08-21 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "历史库存储成本节约至少 50% ，OceanBase数据压缩核心技术解读",
    "url": "https://www.infoq.cn/article/HERjTwV7wztVIFg9Tdt0",
    "summary": "<p></p><blockquote>作者｜赵赛铜，OceanBase 高级开发工程师。目前在 OceanBase 存储-分析处理组，工作方向是存储结构和分析处理功能的维护与开发。</blockquote><p></p><p></p><p>“数据是二十一世纪的石油”，这个观点正在逐渐成为现实，现在我们有各种各样的 IT 系统不断地生产着数据，这些数据累积起来为我们的生产生活带来了很多便利。但在挖掘这些数据价值的同时，大量数据的存储与计算也带来了巨大的成本，降本增效也成为了很多 IT 系统设计的重点。</p><p></p><h2>历史库的需求与挑战——降本增效</h2><p></p><p></p><p>在订单、交易、日志等业务场景中，数据总量会不断增加。而对于这些数据的访问往往和时间有很强的相关性，通常与当前时间越接近的数据越“热”，也就是说，这些数据可能会被频繁地修改与点查。热数据的访问更多是事务型负载和实时分析负载，其数据量在整个系统中的占比相对较低。而在系统中已经存在了一段时间的数据，被称为冷数据，这些数据的被查询次数相对没有那么频繁，也很少被修改。冷数据的访问通常是少量的事务型负载和一些 ad-hoc 或报表统计等分析型负载，而且冷数据通常是稳定运行的 IT 系统中数据量的主要部分。</p><p></p><p>由于冷热数据有着明显的区别，将它们放在一套相同规格的环境中同等处理显然会浪费系统的资源，单个数据库的容量上限还可能会限制数据的存储。但是，将冷数据定期归档到更经济的存储介质中，访问数据时采用从归档数据中还原的方法，又会对历史数据的查询性能和系统的复杂度带来负面影响。因此，将数据分为线上库和历史库，将在线数据定期同步到历史库中的做法成为了越来越多系统的解决方案，通过在存储和计算成本更低的环境上部署历史库来降低成本和满足业务需求。</p><p></p><p>历史库这种模式的出现也对数据库管理系统带来了新的需求，通常一个历史库希望数据库能够：</p><p></p><p>拥有大容量的存储空间，即能够支持大量数据的存储和在线库数据高效的持续导入；提供更低的存储成本，即能够用更少的磁盘空间，更经济的存储介质存储更多的数据；提供和在线库近似的查询性能，即支持高效的少量事务型查询，同时能够支持高效的分析型查询；支持低频率的历史数据更新；对应用和在线库保持相同的访问接口，降低应用复杂度。</p><p></p><p>面对这些需求，拥有良好的单机分布式扩展能力，支持 HTAP 混合负载处理能力的 OceanBase 数据库能够同时高效地支持业务系统的在线库和历史库场景。更为关键的是， OceanBase 可以在支持业务的同时至少降低一半的存储成本，部分客户反馈，业务历史库从其他数据库迁移至 OceanBase 后，存储成本降低 80% 左右，这也是许多用户在历史库场景选择 OceanBase 的原因之一。其中的核心技术，就是 OceanBase 的 LSM-tree 存储引擎和数据库压缩功能。本文会为大家分享 OceanBase 在降本增效的重要功能-数据库压缩上的设计与思考，来探讨存储引擎怎样通过数据压缩更好地支持历史库场景。</p><p></p><h2>解决历史库需求的核心技术——数据编码与压缩</h2><p></p><p></p><p>数据压缩是数据密集型应用中常用的方法，将数据处理后变换成占用更小空间的格式，来降低存储与传输数据的成本，是一种用压缩和解压的手段，以计算时间换存储空间。</p><p></p><p>在现代计算机存储的层次架构中，与 CPU Cache 和内存相比，硬盘通常有着更低的带宽与更高的查询时延。尽管现在 SSD 、 DMA 等硬件技术对 I / O 进行了优化，硬盘的访问效率仍然和内存等设备不在同一个数量级，而通过压缩可以在相同的 I / O 带宽上支持更多数据的读写。因此，对数据进行压缩不仅可以降低存储的成本，还可以降低 I / O 的压力，压缩过的数据也使得访问数据路径上的各级 cache 都能有相对更高的命中率。</p><p></p><p>而压缩数据的过程实际上就是去除数据中的冗余，更高效地表达数据信息的过程。因此，不同场景下的数据特征与访问需求也为数据压缩提供了更多的可能。例如，音视频数据都有特定的有损压缩算法，对连续的数值数据和文本数据分别有更高效的压缩方法。而对于数据库，尤其是关系型数据库，存储的数据会受到 schema 、类型、值域等限制，相似度会比较高，这些特征使数据可以被更好地编码与压缩。</p><p></p><h2>不同数据库产品的数据压缩技术</h2><p></p><p></p><p>目前数据库产品或多或少都提供了数据压缩功能，但由于存储引擎架构和数据库应用场景的不同，产品之间的数据库压缩设计和能力也会产生差别。通常，压缩率越高的压缩算法，压缩和解压数据的 overhead 就越大，因此各种产品在设计压缩功能时都会在成本和性能之间进行权衡， OceanBase 也根据自己的架构特点设计了数据的压缩方法。</p><p></p><p>1、事务型数据库的数据压缩技术</p><p></p><p>通常针对事务型负载设计的数据库需要在平衡压缩率和性能上做出更多的努力。面向 OLTP 场景的数据库要对频繁随机写入和更新场景支持更高的 TPS ，通常会使用基于行存和 B+ 树结构的存储引擎，因此对于数据的压缩会相对保守。这种存储引擎通常会将定长的内存数据页映射到持久化数据块来管理，而且有些情况下将更新数据同步写到数据块中，会导致对页内少量行进行 DML 操作可能需要对整个数据页进行重新压缩，在读写路径上带来更多的 overhead 。而且定长数据块在进行压缩前难以确定压缩后的数据块大小，也会带来一些空间浪费等问题。</p><p></p><p>典型的例子如 MySQL ，数据写入需要更新 redo log 和内存中的数据页，然后在触发页分裂或交换时把内存中的脏数据页刷到持久化存储中。在 MySQL 的早期版本中，一个数据页在 buffer pool 中可能同时存在压缩和未压缩两份数据，分别用于读数据和更新数据后重新压缩刷脏页，开启压缩功能的表会出现明显的性能下降。在 MySQL 5.7 版本后，通过文件系统的空洞能力实现了透明压缩（ TPC ），将内存页面压缩后按照原大小刷到文件系统中，以文件系统的块大小为粒度发现并复用数据块中的空洞。这样的方法相对旧版本更灵活，压缩和解压性能相对更好，但没有解决存储空间的浪费和文件系统碎片等问题。由于存储引擎中开启压缩带来的不可避免的性能损失， MySQL 和 Oracle 都提醒开发者谨慎使用透明压缩、 OLTP 压缩等特性。</p><p></p><p>2、分析型数据库的数据压缩技术</p><p></p><p>分析型负载的数据库需要批量处理大量数据，通常对数据压缩会有更高的要求。因为面向 OLAP 场景的数据仓库等系统的数据通常是批量导入的，增量数据相对较少，所以分析型数据库通常使用列存、增量数据写日志，定期重整基线数据的存储引擎。这种存储引擎中数据压缩发生在批量导入和后台数据重整时，可以采用相对激进的压缩策略。例如，以更大的数据块为单位进行压缩，将更多数据压缩到同一个数据块中，将同一列的数据存储在相邻的数据中，并针对这一列数据的特征对数据进行压缩率更高的编码。</p><p></p><p>处理各种特定负载的数据库也会根据自己的数据结构对数据进行更高效的编码。如 ClickHouse ， Redshift 等数据仓库中都对整形数据设计了特殊的编码方式；搜索引擎也对单调递增的整形序列的编码与扫描进行了深入的优化；时序数据库 Gorilla 中提出的 Gorilla 编码知名度甚至超过了这个数据库本身。很多分析型数据库还会根据自己的查询负载对数据编码进行设计，来优化自己的查询性能。传统的数据仓库能够通过列存和编码提供较好的数据压缩能力，对特定的分析型查询负载也有更好的表现，但往往难以支持高效的数据更新。</p><p></p><p>3、 OceanBase 的数据压缩技术</p><p></p><p>作为一款 HTAP 数据库产品， OceanBase 使用基于 LSM-Tree 架构的存储引擎，同时支持 OLTP 与 OLAP 负载，这种存储架构提供了优秀的数据压缩能力。在 OceanBase 中，增量数据会写入 clog 和 memtable 中，&nbsp;OceanBase 的 memtable 是内存中的 B+ 树索引，提供高效的事务处理能力。&nbsp;memtable 会定期通过 compaction 生成硬盘持久化数据 sstable&nbsp;，多层 sstable会采用 leveled compaction 策略进行增量数据重整。sstable 中数据块的存储分为两层，其中 2M 定长的数据块（宏块）作为 sstable 写入 I / O 的最小单元，存储在宏块中的变长数据块（微块）作为数据块压缩和读 I / O 的最小单元。</p><p></p><p>在这样的存储架构下， OceanBase 的数据压缩集中发生在 compaction 过程中 sstable 的写入时，数据的在线更新与压缩得到了解耦。批量落盘的特性使其采用更激进的压缩策略。OceanBase 从 2.0 版本开始引入了行列混存的微块存储格式（ PAX ），充分利用了同一列数据的局部性和类型特征，在微块内部对一组行以列存的方式存储，并针对数据特征按列进行编码。变长的数据块和连续批量压缩的数据也可以让 OceanBase 通过同一个 sstable 中已经完成压缩的数据块的先验知识，对下一个数据块的压缩进行指导，在数据块中压缩尽量多的数据行，并选择更优的编码算法。</p><p></p><p>与部分在 schema 上指定数据编码的数据库实现不同， OceanBase 选择了用户不感知的数据自适应编码，在给用户带来更小负担的同时降低了存储成本，从历史库角度而言，用户也不需要针对历史库数据做出过多压缩与编码相关的配置调整。&nbsp;OceanBase 之所以能够在事务性能和压缩率之间取得更好的平衡，都得益于 LSM-Tree 的存储架构。</p><p></p><p>当然， LSM-Tree 架构不是解决数据库压缩所有问题的银弹，如何通过数据压缩降低成本、提升性能是业界一直在讨论的话题。对 B+ 树类的存储引擎进行更高效的压缩也有很多探索，比如基于可计算存储硬件的工作，利用存储硬件内部的透明压缩能力对&nbsp;B+&nbsp;树类存储引擎的数据压缩进行优化，使其写放大达到了接近 LSM-Tree 架构存储引擎的效果。但 LSM-tree 中内存数据页更新与数据块落盘解耦，和 sstable 数据紧凑排布的特点，使得 LSM-tree 相对&nbsp;B+ 树类存储引擎，仍然更适合在对查询/更新带来更少负面影响的前提下实现更高效的数据压缩。</p><p></p><h2>OceanBase 的数据库压缩技术</h2><p></p><p></p><p>OceanBase 同时支持不感知数据特征的通用压缩 ( compression ) 和感知数据特征并按列进行压缩的数据编码 ( encoding )。这两种压缩方式是正交的，也就是说，可以对一个数据块先进行编码，然后再进行通用压缩，来实现更高的压缩率。</p><p></p><p>OceanBase 中的通用压缩是在不感知微块内部数据格式的前提下，将整个微块通过通用压缩算法进行压缩，依赖通用压缩算法来检测并消除微块中的数据冗余。目前&nbsp; OceanBase 支持用户选择 zlib 、 snappy 、 zstd 、 lz4 算法进行通用压缩。用户可以根据表的应用场景，通过 DDL 对指定表的通用压缩算法进行配置和变更。</p><p></p><p>由于通用压缩后的数据块在读取进行扫描前需要对整个微块进行解压，会消耗一定 CPU 并带来 overhead。为了降低解压数据块对于查询性能的影响， OceanBase 将解压数据的动作交给异步 I / O 线程来进行，并按需将解压后的数据块放在 block cache 中。这样结合查询时对预读 （ prefetching ） 技术的应用，可以为查询处理线程提供数据块的流水线，消除解压带来的额外开销。</p><p></p><p>通用压缩的优点是对被压缩的数据没有任何假设，任何数据都可能找到模式并压缩，但往往出于平衡压缩性能和压缩率的考虑，通用压缩算法会放弃对一些复杂数据冗余模式的探测和压缩。对于关系型数据库来说，系统对数据库内存储的结构化数据有着更多的先验知识，利用这些先验知识可以对数据进行更高效的压缩。</p><p></p><h2>OceanBase 的数据编码算法</h2><p></p><p></p><p>上文提到在关系型数据库中，由于 schema 和数据类型的限制，同一列的数据类型、精度、值域往往相同。而且在实际应用中，同一列中相邻的数据也通常会有自己的特征，如下面两个场景。</p><p></p><p>当通过一列数据存储城市、性别、产品分类等具有类型属性的值时，这些列数据块内部数据的基数（ cardinality ）也会比较小，这时数据库可以直接在用户数据字段上建立字典，来实现更高的压缩率；</p><p></p><p>当数据按时序插入数据库，这些插入的数据行中的时间相关字段、自增序列等数据的值域会相对较小，也会有单调递增等特性，利用这些特性，数据库可以更方便地为这些数据做 bit-packing 、差值等编码。</p><p></p><p>为了实现更高的压缩比，帮助用户大幅降低存储成本， OceanBase &nbsp;设计了多种编码算法，最终在 OceanBase 的负载上实现了很好的压缩效果。&nbsp;OceanBase 根据实际业务场景需求实现了单列数据的 bit-packing 编码、字符串 HEX 编码、字典编码、 RLE 编码、常量编码、数值差值编码、定长字符串差值编码，同时，创新地引入了列间等值编码和列间子串编码，能够分别对数据库中一列数据或几列数据间可能产生的不同类型数据冗余进行压缩。</p><p></p><p>1、降低存储的位宽：Bit-packing 和 HEX 编码</p><p></p><p>Bit-packing 和 HEX 编码类似，都是在压缩数据的基数较小时，通过更小位宽的编码来表示原数据。而且这两种编码可以与其他编码叠加，对于其他编码产生的数值或字符串数据，都可以再通过 bit-packing 或 HEX 编码进一步去除冗余。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3909bbf3624feb475f38285b9bdba72.jpeg\" /></p><p>（bit-packing）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/1798334a5609da840beeb805c6c4e889.jpeg\" /></p><p>（ HEX&nbsp; 编码）</p><p></p><p>2、单列数据去重：字典编码和 RLE 编码等</p><p></p><p>字典编码则可以通过在数据块内建立字典，来对低基数的数据进行压缩。当低基数的数据在微块内的分布符合对应的特征时，也可以使用游程编码/常量编码等方法进行进一步的压缩。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c6/c619a29e44041e1476a49570198008fc.jpeg\" /></p><p>（字典编码/RLE 编码）</p><p></p><p>3、利用数据的值域压缩：差值编码等</p><p></p><p>差值编码也是常用的编码方法， OceanBase 中的差值编码分为数值差值编码和定长字符串差值编码。数值差值编码主要用来对值域较小的数值类数据类型进行压缩。对于日期、时间戳等数据，或其他临近数据差值较小的数值类数据，可以只存储最小值，每行存储原数据与最小值的差值。定长字符串编码则可以比较好地对人工生成的 ID，如订单号/身份证号、url 等有一定模式的字符串进行压缩，对一个微块的数据存储一个模式串，每行额外存储与模式串不同的子串差值，来达到更好的压缩效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7308f114d7d7f5a4327c825d6ed1bc6.jpeg\" /></p><p>（整形差值）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b35ad23489bf144ec5707b15c88facc6.jpeg\" /></p><p>（字符串差值）</p><p></p><p>4、减小多列数据冗余：列间编码</p><p></p><p>为了利用不同列间数据的相似性增强压缩效果，OceanBase 引入了列间编码。通常情况下，列存数据库只会对数据在列内部进行编码，但在实际应用中有很多表除了同一列数据之间存在相似性，不同列的数据之间也可能有一定的关系，利用这种关系可以通过一列数据表示另外一列数据的部分信息。</p><p></p><p>列间编码可以对复合列、系统生成的数据做出更好的压缩，也能够降低在数据表设计范式上的问题导致的数据冗余。</p><p></p><p>5、自适应压缩技术：让数据库选择编码算法</p><p></p><p>数据编码的压缩效果不仅与表的 schema 相关，同时还与数据的分布，微块内数据值域等数据本身的特征相关，这也就意味着比较难以在用户设计表数据模型时指定列编码来实现最好的压缩效果。为了减轻用户的使用负担，也为了实现更好的压缩效果，OceanBase 支持在合并过程中分析数据类型、值域、NDV 等特征，结合 compaction 任务中上一个微块对应列选择的编码算法和压缩率自适应地探测合适的编码，对同一列在不同数据块中支持使用不同的算法来进行编码，也保证了选择编码算法的开销在可接受的区间内。</p><p></p><h2>如何降低数据编码对查询性能的影响</h2><p></p><p></p><p>为了能够更好地平衡压缩效果和查询的性能，我们在设计数据编码格式时也考虑到了对查询性能带来的影响。</p><p></p><p>1、行级粒度数据随机访问</p><p></p><p>通用压缩中如果要访问一个压缩块中的一部分数据通常需要将整个数据块解压后访问，某些分析型系统的数据编码大多面向扫描的场景，点查的场景比较少，因此采用了在访问某一行数据时需要对相邻数据行或数据块内读取行之前所有行进行解码计算的数据编码的格式（如 PFor 等差值编码）。</p><p></p><p>OceanBase 需要更好地支持事务型负载，就意味着要支持相对更高效的点查，因此 OceanBase 在设计数据编码格式时保证了编码后的数据是可以以行为粒度随机访问的。也就是在对某一行进行点查时只需要对这一行相关的元数据进行访问并解码，减小了随机点查时的计算放大。同时对于编码格式的微块，解码数据所需要的所有元数据都存储在微块内，让数据微块有自解释的能力，也在解码时提供了更好的内存局部性。</p><p></p><p>2、缓存解码器</p><p></p><p>在 OceanBase 目前的数据解码实现中，每一列数据都需要初始化一个解码器对象来解析数据，构造解码器时会需要进行一些计算和内存分配，为了进一步减小访问编码数据时的 RT ，OceanBase 会将数据的解码器和数据块一起缓存在 block cache 中，访问&nbsp; cache 中数据块时可以直接通过缓存的解码器解析数据。当不能命中 block cache 中缓存的解码器时，OceanBase 还会为解码器用到的元数据内存和对象构建缓存池，在不同查询间复用这些内存和对象。</p><p></p><p>通过上述细节上的优化，行列混存格式的 sstable 编码数据也可以很好地支持事务型负载。而且由于编码数据行列混存的格式，使得在分析型查询的处理上，编码数据有着和列存数据相似的特性，数据分布更紧凑，对 CPU cache 更加友好。这些特性使列存常用的优化手段也能应用于分析型查询优化中，充分利用 SIMD 等方法来提供更高效的分析型负载处理。</p><p></p><p>3、计算下推</p><p></p><p>由于编码数据中会存储有序字典、 null bitmap 、常量等可以描述数据分布的元数据，在扫描数据时可以利用这些数据对于部分过滤，聚合算子的执行过程进行优化，实现在未解码的数据上直接进行计算。&nbsp;OceanBase 在 3.2 版本中对分析处理能力进行了大幅的优化，其中包括聚合与过滤计算下推到存储层执行，和在向量化引擎中利用编码数据的列存特征进行向量化的批量解码等特性。在查询时充分利用了编码元数据和编码数据列存储的局部性，在编码数据上直接进行计算，大幅提高了下推算子的执行效率和向量化引擎中的数据解码效率。基于数据编码的计算下推和向量化解码也成为了支持&nbsp; OceanBase 高效处理分析型负载，在 TPC-H benchmark 中达到优秀性能指标的重要功能。</p><p></p><h2>数据编码压缩的基础测试</h2><p></p><p></p><p>不同的压缩方式如何影响 OceanBase 的压缩效果，以下通过一个简单的测试进行观察。</p><p></p><p>使用 OceanBase 4.0 版本分别在交易场景的 TPC-H 10g 的数据模型和用户行为日志场景的 IJCAI-16 Brick-and-Mortar Store Recommendation Dataset 数据集上对&nbsp; OceanBase 的压缩率进行测试：</p><p></p><p>TPC-H 是对订单，交易场景的建模，对 TPC-H 模型中数据量比较大的两张表，即存储订单的 ORDERS 表和存储商品信息的 LINEITEM 表的压缩率进行统计。在&nbsp; OceanBase 默认配置（ zstd + encoding ）下，这两张表的压缩率可以达到 4.6 左右，相较只开启 encoding 或 zstd 压缩时提升明显。</p><p></p><p>IJCAI-16 taobao user log 则是淘宝脱敏后的真实业务数据，存储了用户浏览商品时的行为日志。在 OceanBase 默认配置（ zstd + encoding ）下压缩率可以达到&nbsp; 9.9 ，只开启 encoding 压缩率可以达到 8.3 ，只开启 zstd 压缩率为 6.0 。</p><p></p><p>可以看到 OceanBase 在面对数据更有规律的业务数据时会有更出色的数据压缩效果，在 TPC-H 这种数据冗余相对更少的数据集上也有着优秀的数据压缩能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b86d537df97e0e9b8c696523bfc21056.jpeg\" /></p><p></p><h2>写在最后</h2><p></p><p>OceanBase 存储引擎的数据库压缩功能在设计上希望能够在用户少感知、不感知存储格式的前提下，在不降低事务型负载性能的同时降低存储空间和存储成本，同时提升分析型负载的性能。这样的设计与历史库的设计不谋而合。高度压缩的数据既能够帮助历史库数据降低至少 50% 的存储成本，高效的写入查询和统一的配置接口又能够帮助业务增效。</p><p></p><p>对于历史库数据同步的需求， OceanBase 的 LSM-Tree 存储引擎天生具有高效的写入性能，既能够通过旁路导入高效处理定期的批量数据同步，又能够承载一些实时数据同步和历史库数据修改的场景。</p><p></p><p>对于历史库数据的定期跑批报表，和一些 ad-hoc 的分析型查询带来的大量数据扫描的需求，因为历史库中增量数据较少，所以绝大多数数据都存储在基线的 SSTable 中，这时计算下推可以只扫描基线数据，绕过了 LSM-Tree 架构常见的读放大问题。而且支持在压缩数据上执行下推算子和向量化解码的压缩格式可以轻松地处理大量数据查询和计算。</p><p></p><p>对于大量历史数据存储的需求， OceanBase 的 SSTable 存储格式和数据编码压缩功能可以使 OceanBase 更轻松地支持超大容量的数据存储。而且高度压缩的数据和在同等硬件下更高效的查询性能也能够大幅度降低存储和计算的成本。</p><p></p><p>此外，企业可以选择将历史库所在的集群部署在更经济的硬件上，但是对数据库进行运维基本不需要感知数据编码与压缩的相关配置，应用开发也可以做到在线库和历史库使用完全相同的访问接口，简化应用代码和架构。</p><p></p><p>这些特点让越来越多的企业开始在历史库场景使用 OceanBase 进行降本增效的实践。OceanBase 也不断在存储架构，降本增效方面做出更多的探索。</p>",
    "publish_time": "2023-08-21 10:37:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型赛道再添新玩家，快手自研大模型“快意”亮相",
    "url": "https://www.infoq.cn/article/baiaSCTjwDJzCZJJHjA9",
    "summary": "<p>近日，快手自研的大语言模型“快意”（KwaiYii）已开启内测，并为业务团队提供了标准API和定制化项目合作方案。</p><p>&nbsp;</p><p>GitHub链接：</p><p><a href=\"https://github.com/kwai/KwaiYii\">https://github.com/kwai/KwaiYii</a>\"</p><p>&nbsp;</p><p>据官方介绍，快意大模型（KwaiYii） 是由快手 AI 团队从零到一独立自主研发的一系列大规模语言模型（Large Language Model，LLM），当前包含了多种参数规模的模型，并覆盖了预训练模型（KwaiYii-Base）、对话模型（KwaiYii-Chat）。</p><p>&nbsp;</p><p>其中，13B规模的系列模型KwaiYii-13B主要特点包括：</p><p></p><p>KwaiYii-13B-Base预训练模型具备强大的通用技术支撑能力，在鳄鱼权威的中/英文基准上取得了同等模型尺寸下的State-Of-The-Art效果。例如，KwaiYii-13B-Base预训练模型在MMLU、CMMLU、C-Eval、HumanEval等Benchmark上目前达到同等模型规模的领先水平。KwaiYii-13B-Chat对话模型具备出色的语言理解和生成能力，支持内容创作、信息咨询、数学逻辑、代码编写、多轮对话等广泛任务，人工评估结果表明KwaiYii-13B-Chat超过主流的开源模型，并在内容创作、信息咨询和数学解题上接近ChatGPT(3.5)同等水平。</p><p>&nbsp;</p><p>据介绍，快意大模型（KwaiYii）在 MMLU、CMMLU、C-Eval、HumanEval 等 Benchmark 上目前处于同等模型规模的领先水平，在最新的 CMMLU 中文向排名中，快意的 13B 版本 KwaiYii-13B 同时位列 five-shot 和 zero-shot 下的第一名，在人文学科、中国特定主题等方面较强，平均分超 61 分。</p><p>&nbsp;</p><p>KwaiYii-13B-Chat 对话模型具备出色的语言理解和生成能力，支持内容创作、信息咨询、数学逻辑、代码编写、多轮对话等广泛任务。</p><p></p><p>快手方面表示，从人工评估的结果来看，KwaiYii-13B-Chat超过了同等规模的开源模型，并接近ChatGPT同等水平。在内容创作、信息咨询、逻辑推理和数学解题上，基本与ChatGPT(3.5)效果相当。在多轮对话能力方面，KwaiYii-13B-Chat超过同等规模的开源模型，但与ChatGPT(3.5)仍有一定差距。注意：人工评估结果受到评测数据覆盖面、标注主观性等因素的影响，无法全面反映大语言模型的所有能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/1800d174d36f37107b547def96c64848.png\" /></p><p></p><p>据悉，快手AI团队将持续迭代“快意”大模型，一方面将继续优化模型性能并研发多模态能力 ，另一方面也在推进更多C端与B端业务场景下的落地 。</p>",
    "publish_time": "2023-08-21 11:29:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "JavaScript前景展望：值得期待的那些新功能",
    "url": "https://www.infoq.cn/article/LS4Vy6OueTDFpekcEir2",
    "summary": "<p></p><p></p><blockquote>超实用的类型机制加现代工具，让网站和Web应用更跟得上潮流和形势。而从种种细节当中，我们也得以一窥JavaScript未来的发展方向。</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>如果单从每年公布的ECMAScript标准流程来看，JavaScript的功能设计似乎并没有什么特别重大的变化倾向。TC39联合主席兼Bloomberg JavaScript基础设施与工具团队负责人Rob Palmer一直强调要“铺平道路”，也就是逐步把小小的工具或框架功能融入语言之内，用“语法糖”的形式完成改进。换言之，先把现有功能做好、做完善，这样的升级明显更加无痛。</p><p>&nbsp;</p><p>“我们见证了工具、框架和模式的迭代，随着时间推移，我们会在标准层面找寻融合的交接点，并努力把道路铺平，借此降低应用程序技术栈的复杂度。这样Node模块目录才能持续瘦身，越来越多的功能开始由语言本体直接提供。”</p><p>&nbsp;</p><p>Vercel公司的TC39代表Justin Ridgewell在采访中表示，考虑到2017年引入了原生异步等重大发展，这类增量化功能正在不断增加。“多年以来，JavaScript引入了诸多具有新功能的新API。我们的新功能吸纳思路也正在于此——在添加小变更的同时，尽量扩大新功能的广度。”</p><p>&nbsp;</p><p></p><blockquote>“我们并不会每年发布重大的功能变更：有时候是每两年一次，有时候是每三年一次。我们只会在必要时引入，确保其切实推动语言的发展。”</blockquote><p></p><p>&nbsp;</p><p>一部分重要的新功能（例如Temporal）在发布之初就基本做好了应用准备；也有其他产品正处于开发中，预计会在未来几年内陆续推出。本文为大家挑选了一些最有趣的内容，同时也征求了JavaScript标准制定团队的意见，由他们对语言发展状况做出解释，包括JavaScript在下一步标准化中将要解决的问题。</p><p>&nbsp;</p><p></p><h2>类型机制不会把JavaScript变成TypeScript&nbsp;</h2><p></p><p>&nbsp;</p><p>TypeScript的诞生，纯粹是为了提高JavaScript开发者的工作效率，绝对不是要彻底取代JS的江湖地位。当然，TypeScript也成为JS未来发展完善的灵感来源。目前，大家在编写代码时需要在TypeScript中声明类型——但在代码运行时，这部分又会被移除掉。</p><p>&nbsp;</p><p>之后的发展自然也要解决这个问题。第一阶段的类型注释提案希望在JavaScript代码中引入类型信息，更重要的是保证JS引擎能够将其正确理解为注释，这样就能保证TypeScript和JavaScript相互一致和对齐，同时明确它们其实运行在不同的层上。</p><p>&nbsp;</p><p>Palmer指出，开发人员可以对类型使用一等语法，包括TypeScript以及带有长JSDoc注释块的Flow语法，同时保证自己的代码仍然能跟JavaScript引擎和JavaScript工具链相兼容——这就避免了在代码运行前消除类型给构建过程带来的复杂性。</p><p>&nbsp;</p><p>Palmer解释道，“这种仅在开发期间存在，但在运行时会被完全移除的静态类型具有巨大的价值。”而且其中一部分价值可谓相当直观，“开发者可以进行类型检查，知晓自己什么时候犯了错误，同时取消对不存在属性的引用——这很棒。而且除此之外，类型信息还能大大改善开发人员的工作感受，例如重构事物的能力、在IDE中自动一次性重命名变量和属性，还有实现代码导航等。”</p><p>&nbsp;</p><p>Ecma（TC39的母机构）副总裁兼Bloomberg&nbsp;JS开发者体验软件工程师Daniel Ehrenberg建议，“如果大家在开发中需要用到大量并非出于自己之手的代码，那么理解它的类型构造肯定会有所帮助。”</p><p>&nbsp;</p><p>尽管静态类型在JavaScript社区中一直存在争议，但考虑到长期有社区成员希望引入这项功能（静态机制连续三年在JavaScript现状调查中被评为最迫切需要的功能），标准流程已经基本证明这种方法能够为JS语言锦上添花、同时又不会影响其吸引新手开发者的简单特性。</p><p>&nbsp;</p><p></p><h2>用更智能的消息格式简化本地化过程</h2><p></p><p>&nbsp;</p><p>这里给大家科普一下，对网站和Web应用程序的本地化绝不止是替换掉用户界面中的消息字符那么简单。毕竟文本内容不只包含单词，还涉及数字、序数词（第一、第二等）、日期、得数和其他各种不同的语言结构，所以单纯替换单词得到的结果可能根本就不像人话。</p><p>&nbsp;</p><p>目前已经有一些库能帮助解决这个问题，例如FormatJS等。但与Java和C等其他语言相比，JS开发者和翻译人员的工作量还是要更大一些。毕竟人家Java和C都拥有支持国际Unicode组件（例如ICU4J和ICU4）的内置字符串翻译和格式化功能。</p><p>&nbsp;</p><p>开源咨询公司Igalia的Romulo Cintra在采访中指出，“复数部分真的很难处理。所有语法概念、词形变化和性别/阴阳性数字，以及不同的占位符在各语种之间总有种种差别。处理这种复杂性既依赖于库，也需要大量数据的支持。”</p><p>&nbsp;</p><p>实际上，浏览器已经在使用这些组件推动软件本地化，不少API也能调整日期/时间格式和相对时间格式。那么，为什么不为Web引入类似的选项，并给JavaScript开发人员提供包含专业语言知识的内置选项呢？毕竟，软件已经是最适合面向全世界的产品了。</p><p>&nbsp;</p><p>TC39提出的Intl MessageFormat是另一项一阶提案，他们与Unicode联盟的消息格式工作组合作，希望引入包含国际化和本地化逻辑的模板化字符串，将在JavaScript中以内置引擎的形式用不同语言正确填写模板字段。</p><p>&nbsp;</p><p>但在Web中引入国际化API其实是项影响深远的调整，甚至会引发对拥有20年历史的ICU消息格式API的重大更新。Cintra好奇的是，“原本的API只支持字符串、非常僵化，完全没有模块化特性——所以，为什么不从根本上解决问题，直接上马Unicode新标准呢？”而这正是MessageFormat 2.0（简称MF 2.0）的意义所在，它在设计上就是为软件和Web的国际化而生、支持Intl MessageFormat，Cintra甚至认为它就是帮助Web吸引又一个十亿用户的关键。</p><p>&nbsp;</p><p></p><blockquote>“它将突破固有循环，在本地化和个性化层面让Web的可访问性来到新的层次。”</blockquote><p></p><p>&nbsp;</p><p>Cintra认为，“对我来说，作为一名非英语母语者，Web的发展方向就是让更多人能够随意访问。这一点非常重要，也只有达成这个目标，我们才有机会让所有努力开发的软件内容变得更易于访问。绝对值得期待！”</p><p>&nbsp;</p><p>目前，大部分本地化主要依靠在运行阶段解析的自定义消息格式及专有规范来实现。Mozilla公司靠的就是自家Fluent工具来翻译所有界面。Igalia的Ujjwal Sharma告诉我们，Bloomberg也有自己的内部工具。“每个人都在尝试通过不同的定制化工具解决同一个问题。”虽然Intl MessageFormat肯定能为已经涉足国际化工作的组织建立起通用标准，同时也继承开放协作的各种固有优势，但Sharma更希望它能为那些还没有固定网站翻译流程的小型组织提供助力。</p><p>&nbsp;</p><p>除了简单翻译文本字符串之外，MF 2.0还将包含元数据和注释，可用于标记从文字证据（包括正式和非正式）到语音合成提示的全部内容，这样就能更好地支持Siri、Alexa等智能扬声器和屏幕阅读工具。新标准还有望消除瓶颈，“语音和界面的许多创新都发生在客户端”，但数据量不足往往成为制约本地化工作的关键。Sharma认为，“有了Intl MessageFormat，我们可以在客户端落地更多愿景。”</p><p>&nbsp;</p><p>本地化是个价值数百万美元的行业，因此必须要在兼容性和必要改进之间寻求平衡。Sharma指出，“最好能提供一个易于使用且足够直观的API，同时保证仍能跟传统工作方式对接起来——这明显是个极具挑战的任务。但我认为，只要能把这个问题解决好，就能彻底改变人们对于网站的看法。”</p><p>&nbsp;</p><p></p><h2>一种用于语言翻译的语言</h2><p></p><p>&nbsp;</p><p>Igalia负责ICU实现的Tim Chevalier解释道，MF 2.0规范定义了一种简单的编程语言，拥有名称绑定（’let’声明）和模式匹配（选择器）。他建议大家将其视为“一种用于编写可翻译消息的特定领域语言”，能够充分运用关于现有编译器和解释器的知识积累。</p><p>&nbsp;</p><p>“希望开发人员在用上MF 2.0之后，体验不再像过去那样要编写一大堆神秘的字符串，而更像是在自己选定的通用语言（例如JavaScript）中使用另一种专用语言进行编程。”他还把这种改进，比喻成从数据库硬编码查询客串到SQL的功能飞跃。</p><p>&nbsp;</p><p>“曾几何时，根本就没有可靠的方法能编写程序来操纵对字符串的查询，以便生成给定查询的变体，因为人们不知道要怎么向C++、Java和JavaScript等通用语言当中嵌入查询语言。现代语言工具已经提供更丰富的查询构造方法，不再仅仅依赖于编写字符串并将其作为参数传递给函数。”MF 2.0就承诺为开发人员提供类似的使用体验。</p><p>&nbsp;</p><p>而且MF 2.0还具备可扩展性：开发人员将获得一个接口，可以用JavaScript（或者其他语言）创建自己的新抽象以实现格式化函数。该规范还能降低高级翻译工具的编写难度，借这些工具提供更友好的用户界面，让翻译人员在不必学习硬核编程知识的前提下轻松处理文本消息。</p><p>&nbsp;</p><p>MF 2.0的ICU工作仍处于早期阶段，目前仅提供技术预览版。最早的成果是ICU4J，但目前正被移植向ICU4C，即大部分JavaScript引擎使用的语言。TC39提案依赖于MF 2.0，而“一旦实现了MF 2.0的稳定版本，TC39那边的工作就能顺利推进。”Cintra还推测，稳定版将允许在浏览器上实现该API，让更多开发者感受到它的实际效果。</p><p>&nbsp;</p><p>如果大家想亲自试试看，FormatJS已经为初始IntlMessageFormat提案提供早期支持：</p><p><a href=\"https://formatjs.io/docs/intl-messageformat/\">https://formatjs.io/docs/intl-messageformat/</a>\"</p><p>&nbsp;</p><p>正在开发一个实验性的polyfill：</p><p><a href=\"https://github.com/messageformat/messageformat/tree/main/packages/mf2-messageformat\">https://github.com/messageformat/messageformat/tree/main/packages/mf2-messageformat</a>\"</p><p>&nbsp;</p><p>由于该polyfill基于已经得到广泛应用的React及其他应用国际化工具，因此将为开发人员提供所需的全新语法体验。</p><p>&nbsp;</p><p></p><h2>里程碑可期，但仍在很多工作要做</h2><p></p><p>&nbsp;</p><p>Palmer表示，JavaScript将继续缓慢发展。“有些人觉得不妨让变革的脚步快一些，以便我们能够获取反馈、发现重要问题并进行迭代；但我们TC39的行事风格往往更偏保守。”</p><p>&nbsp;</p><p>如果事实证明一旦发现设计有误且需要修正（比如Web组件的实现），那从Web平台中移除内容的成本其实非常之高。相比之下，Polyfill和基于工具的功能实现（标准候选版）既能实现更快的反馈周期、保护Web兼容性，又无需让开发人员苦等正式流程走完所有流程。</p><p>&nbsp;</p><p></p><blockquote>“我真心认为，JavaScript保守的开发风格是很有意义的。”</blockquote><p></p><p>&nbsp;</p><p>Ehrenberg表示，“我们有很多实现方案，也有很多需要考量的用途。其他平台可以充当更具实验性的环境，而我们还是选择成为更保守的实现环境。”</p><p>&nbsp;</p><p>按照这种模式，他提到JavaScript本身现在包含大量以往开发者只能通过外部工具获取的功能（包括迭代器助手等则是受到其他语言的启发）。</p><p>&nbsp;</p><p>Ehrenberg解释道，“在处理类字段，包括私有字段和装饰器，还有像hashbang语法这样的小问题时，肯定希望我们能早点拿出类型注释和新的功能模块。它们其实就是在继承人们已经在工具中获得的功能，我们想把它们纳入语言本体。CSS的情况也差不多，同样是把很多通过工具实现的功能引入到核心语言当中，例如嵌套（经典示例）和作用域，还有层和变量等。”</p><p>&nbsp;</p><p>此外，标准化还为开发人员的设置提供非常明确的默认值，有助于最大限度减少项目创建过程中涉及的配置操作，这同样给JavaScript的发展带来了新灵感。展望未来，Ehrenberg还提出另一个值得关注的方向：响应式用户界面的核心组件，例如signals和cells，有可能会成为JS语言的一部分。毕竟当今开发人员在前端框架上经常会用到。</p><p>&nbsp;</p><p>这就是他常说的，JavaScript要走“增量式共享的发现道路”。通过实验为核心功能找到新的基准，再把成果内置到语言当中。他还坚持强调，JavaScript仍有巨大的发展空间。</p><p>&nbsp;</p><p>“我觉得启发和灵感还有很多，我们远远没有把这些全部转化成现实。”</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://thenewstack.io/whats-next-for-javascript-new-features-to-look-forward-to/\">https://thenewstack.io/whats-next-for-javascript-new-features-to-look-forward-to/</a>\"</p><p>&nbsp;</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/819c7ae045ecc83936afed4d1\">全网最全&nbsp;ECMAScript&nbsp;攻略</a>\"</p><p><a href=\"https://www.infoq.cn/article/dDXbcLHT7teNYSPL3sm7\">“TypeScript 不值得！... 反向迁移到&nbsp;JavaScript&nbsp;引争议</a>\"</p><p><a href=\"https://xie.infoq.cn/article/a4f9a16ebc0f866e312a06176\">JavaScript&nbsp;作用域深度剖析：动态作用域</a>\"</p><p><a href=\"https://xie.infoq.cn/article/6ff79700fb3bfa972c1beebf3\">TypeScript 与&nbsp;JavaScript：你应该知道的区别</a>\"</p>",
    "publish_time": "2023-08-21 11:38:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一个关于十年和五万的开源历程——Apache ECharts开源之路",
    "url": "https://www.infoq.cn/article/97daSyAVxs228gsiUJeg",
    "summary": "<p></p><h2>前言</h2><p></p><p></p><p>中国互联网最为兴盛繁荣的那个年代，也是Web前端技术领域飞速发展的时代，中国各家互联网公司均大规模进行了人才的军备竞赛，充裕的人力加上不断飞速发展的业务，让各个公司的前端团队都出于业务、技术或者团队的考虑，造出了大量轮子。</p><p>&nbsp;</p><p>绝大多数的轮子都慢慢因为业务、组织的原因逐渐凋零，极少数的产品活了下来，成为行业中大家膜拜的标杆产品。</p><p>&nbsp;</p><p>Apache ECharts，应该能算得上其中的一份子，这个诞生于百度的数据可视化产品，从发布之日算起，至今刚刚有十年的时间。它在 Github 上拥有的五万多的关注数（star）充分说明了这是一个由国人打造的极具影响力的开源技术产品。</p><p>&nbsp;</p><p>这样产品的背后，是怎么样的一群人在为之付出？这十年，这些人是如何一步一步走到今天的呢？</p><p>&nbsp;</p><p>基于 QCon 2023 中的分享，以及各种历史材料，本文将分享一下 Apache ECharts 十年来的开源故事。</p><p>&nbsp;</p><p></p><h2>Apache ECharts 介绍</h2><p></p><p></p><p>在回顾历史故事之前，我们先简单地介绍一下什么是Apache ECharts。</p><p>&nbsp;</p><p>Apache ECharts （下简称 ECharts）是一个基于 JavaScript 的开源可视化图表库，提供了丰富的图表类型，丰富的可配置化参数，开箱即用，对于大部分图表需求，ECharts的上手使用成本极低。这也是十年前 ECharts 立足的重要产品力之一。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc834defe4f4ca9982d92e4f33104e46.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/67dae480cd5d5801b8bf272a6c0282ef.png\" /></p><p></p><p>&nbsp;</p><p>而对于复杂的可视化需求，ECharts 也提供了非常强大的自定义扩展能力，比如上图中的左上角部分，是一个篮球场的投篮命中率的分布图，这并不是一个常规图表，但用户基于 ECharts 提供的扩展能力能够可以实现上图中各种各样的可视化需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9df8bea4452b9b9bd1822d2b9c2a3a79.png\" /></p><p></p><p>&nbsp;</p><p>对于 ECharts 来说，它希望提供给最终查看数据的用户，不仅仅只是一个静态的数据展现，它还希望能帮用户更好地理解数据、分析数据，所以 ECharts 从最初版本就开始提供了大量交互和动态展现的能力。比如上图中，展现的就是一个左侧地图拖拽选择和右侧条形图的分析联动。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/27de895e54b169365a92376a62a10391.png\" /></p><p></p><p>在 ECharts 诞生的时候，曾经不断强调数据可视化是大数据时代的最后一公里，那大数据的大要如何怎么体现呢？ECharts 选择了具备超大数据量渲染性能，从最早的十万量级、百万量级，到 ECharts 5.0 的亿级，通过大量的算法优化、使用WebGL加速等手段，帮助开发者在开发一些数据大屏时得心应手。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a7897dd3e8e2d43a802356ce58c6db11.png\" /></p><p></p><p>ECharts 在4.0版本发布时，同步也发布了其三维的可视化解决方案，ECharts GL，提供了包括地理可视化，函数可视化（如上图）。充分体现了团队在可视化领域上的技术实力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfdc979bee586cb6bb0c9e6453a8a61e.png\" /></p><p></p><p>在与 ECharts 团队交流的过程中，发现团队对于 ECharts 在可访问性上的支持，是团队特别引以为傲的功能。</p><p>&nbsp;</p><p>ECharts 提供了贴花的方案，帮助色弱人群区分图表里的不同数据系列。</p><p>&nbsp;</p><p>ECharts 充分遵循 W3C 标准，能够让开发者定制图表要表达的内容，让视障人群通过听的方式了解图表里的内容。同时，ECharts 还提供了基于规则的自动图表描述，例如上图中，针对左侧图表，ECharts 会生成右边的一段描述文字，当视障人群在访问这个页面的时候，可以听到相关的描述。</p><p>&nbsp;</p><p></p><h2>Apache ECharts 发展历史</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/036367f66e1fd8ae67a55fdfae3687b5.png\" /></p><p></p><p>Apache ECharts 从 2012 年 8 月立项，在 2013 年 6 月 30 日正式发布了 1.0 版本，到今年（2023年）的 6 月份正好是 10 周年。在 1.0 发布后的一年后， 2014 年 6 月 30 日，ECharts 发布了 2.0 版本。从立项到 2.0 的发布，可以算 ECharts 的早期诞生阶段。</p><p>&nbsp;</p><p>接下来的三年多时间，从 3.0 的 alpha 到 3.0 正式版，再到 4.0 版本，这是 ECharts 的第二个发展阶段，在这期间是以百度的团队为核心发布版本。</p><p>&nbsp;</p><p>2018 年 1 月，ECharts 进入 Apache 基金会进行孵化，由此转为社区运营状态，这个过程中 ECharts 得到了较好的发展，尤其是在 2020 年的 6 月份，达成了 100 个社区贡献者的成就，进入 Apache 基金会以后，社区的力量帮助 ECharts 吸引了很多贡献者。</p><p>&nbsp;</p><p>2021 年 1 月份 ECharts 顺利从 Apache 基金会毕业，同时也发布了 5.0版本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a27eb0d1b7851dfb804049d63c5974b.png\" /></p><p></p><p>随着版本不断迭代，Apache ECharts 在开发者社区内得到越来越多的关注，在Github中获得了超过五万的关注数。</p><p>&nbsp;</p><p></p><h3>从 2.0 到 5.0 的各个大版本执念</h3><p></p><p>&nbsp;</p><p>ECharts团队认为大版本发布，是一个非常严肃而正式的事情，因此也需要有非常仪式感的发布形式，在资源有限的情况下，团队会找寻各种可利用的资源，或是自己掏腰包，来进行大版本发布的宣传运作。</p><p>&nbsp;</p><p>对于一个技术团队来说，除了做好每个版本的技术工作，学习如何运营、如何做市场宣传运营，也是非常有意思的事情。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/1910fd6319d9843a40955d28d84c989a.png\" /></p><p></p><p>上图是 2.0 的官网，从 2.0 开始，ECharts 有了自己的设计师，也因为有了设计师，ECharts 终于有了 logo----小鲸鱼。</p><p>&nbsp;</p><p>为了营造 2.0 的宣传氛围，当时还做了分节奏的宣传海报，先放了左边这张图，代表比 ECharts 1.0 还好用的图表即将跃出海面，小鲸鱼还埋在地平线下，满屏幕充斥着各种不同语言的“二”。然后再放出第二个图，ECharts 2.0 正式发布，小鲸鱼浮出海面。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8e415197f4e4bd2df1973e5529cdaee.png\" /></p><p></p><p>在 2.0 发布以后，面临着用户需求越来越复杂，技术实现时，发现原有的技术架构，已经无法很好适应新的需求迭代，效率极低。再加上创始人离职，所以大家都很希望通过 3.0 来证明自己。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb4688f19125dffe112ac9cbf9591ae1.png\" /></p><p></p><p>因为全身心投入到了 3.0，导致在 3.0 正式发布前的好几个月，ECharts没有任何版本更新，用户因此经常通过邮件或微博上问 ECharts 是不是黄了。</p><p>&nbsp;</p><p>好不容易 3.0 发布，但因为整个架构调整太大，BUG太多，导致头几个版本骂声一片，导致前期的口碑和反馈特别差。</p><p>&nbsp;</p><p>好在经过团队的努力，在3.0 发布后的4 个月内，连续发布了 12 个版本来修复各种各样的 bug，用户终于看到了新的大版本的优点和变化，再次接纳了 ECharts。</p><p>&nbsp;</p><p>基于新的架构，3.0 迭代速度越来越快，用户的需求和大家的技术想法得到更好的落地，经过长时间积累，4.0 也顺理成章地到来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/6656466dfc6ac362087219cc2f4e9424.png\" /></p><p></p><p>ECharts团队一直渴望有一次正式的发布会，而 4.0 达成了团队的这个心愿（上图）。</p><p>&nbsp;</p><p>4.0 发布的同时，也是百度开源如火如荼开展的时候，团队借势将 ECharts 捐献给了 Apache，帮助公司提升在开源领域的影响力，也为产品的未来发展开启了一条新的大路。</p><p>&nbsp;</p><p>ECharts 捐献到 Apache 以后，不管是团队内部角度，还是从公司角度，都希望它能够成为百度第一个从 Apache 正式毕业的顶级项目。</p><p>&nbsp;</p><p>在这个过程中面临着很多挑战，最大的挑战来源于对于 Apache Way的适应。首先是效率之争，大家日常办公中习惯了用 IM 工具，甚至当面或以集中办公这种方式去沟通项目和技术方案，但是 Apache Way 要求以邮件的方式公开沟通，大家一开始认为效率太低。</p><p>&nbsp;</p><p>但当真正去适应、尝试之后，团队发现了其中的优势。通过邮件沟通的时候，每个人都需要尽可能地在邮件里把所要表达的内容写的非常详细，包括你的思考是什么，结论是什么，建议等都在邮件里，同时，大家的回复非常清楚。阅读邮件时，所有的上下文都能够通过邮件完整、明确地进行回溯。反而使得沟通效率变高。</p><p>&nbsp;</p><p>适应了 Apache Way后，更多的贡献者加入到了 ECharts 5.0 的开发中，已经充分适应远程协同办公的团队，让 5.0 没有受到口罩的影响，5.0 也通过视频直播的方式，完成了线上发布会。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4d529223827bd0813f91c9ae4555eb1.png\" /></p><p></p><p></p><h2>和 ECharts 相关的一些人</h2><p></p><p></p><p>除了产品自身的发展故事，产品背后的人与团队的故事，也吸引着我们去深度了解。</p><p>&nbsp;</p><p></p><h3>干一行爱一行的创始人</h3><p></p><p></p><p>说到 ECharts 的团队，许多人首先想到的就是它的创始人，林峰。除了 ECharts，林峰还有个身份，是 AntV 的前负责人。可以说，中国的两大可视化产品，都和他有着密切的关系，不得不说，林峰担得起中国前端数据可视化领军人物这一殊荣。</p><p>&nbsp;</p><p>据说林峰当年加入百度前端团队之前，对Web前端几乎没有太多了解，面试前才花了一段时间突击开始学习，就是这段突击，改变了中国前端数据可视化的格局。</p><p>&nbsp;</p><p>林峰加入百度的那些年，正是互联网以及Web前端领域欣欣向荣的年代，前端技术轮子一个接一个造得如火如荼。许多团队都把自己拥有多少开源产品作为团队影响力的重要参考。而各家公司冗余的人才招聘措施，也让各个团队在照顾好业务的同时，有足够的资源参与到技术产品的研发工作中来。</p><p>&nbsp;</p><p>林峰加入的团队，历经几年的组织变更后，成立了EFE团队，这个团队也同样产生出了大量具备团队特征的技术开源产品，如今都好能在团队的Github中看到踪影，但绝大多数产品已经完成了其历史使命，或只是在百度内部个别产品现服役，并没有成为一个有影响的社区开源产品。或者是说，大部分产品，在创立之初，并没有希望成为一个社区有影响力的开源产品的目标，只是务实地为了满足公司内部业务，顺手在Github上开了个源。说句题外话，这种操作方式在那些年各家公司的前端团队并不少见，但近些年，随着各家公司对于开源的管控越来越正式，这种现象在大公司中已经减少了许多。</p><p>&nbsp;</p><p>ECharts 也是在EFE成立后，在2012年8月正式立项，并于2013年6月开源发布，在之后，我们能够频繁地在各种技术社区中，看到林峰为 ECharts 发声宣传，据不完全统计，在2013到2015年，林峰累计进行了超过了40次对外的分享。</p><p>&nbsp;</p><p>那篇经典的《Why ECharts》的在线幻灯片，跟着林峰走遍了大江南北。</p><p>&nbsp;</p><p>林峰在技术上是如何推动 ECharts 开始的，我们不得而知，但在回顾林峰的宣传历史时，我们不难发现，他并没有简单把 ECharts 仅仅作为一个技术产品在各家前端团队中进行宣传。他敏锐地结合了大数据时代及数据新闻的趋势，把 ECharts 的产品价值从一个技术提效工具，提升到了业务价值提升的解决方案。</p><p>&nbsp;</p><p>这是大部分技术人员所不具备的视野和产品力，但这些都是可以通过实践习来，真正让林峰从众多产品负责人中脱颖而出的关键，是他干一行爱一行的能力。用当前互联网大厂的黑话，可以称之为信念感，或使命感。</p><p>&nbsp;</p><p>在做前端之前，他没有前端经验，但却在做ECharts之前，成为了团队中的技术中流砥柱；在做ECharts之前，他没有做过数据可视化，但却成为了后来中国两大前端可视化产品的重要角色。</p><p>&nbsp;</p><p>一个和林峰共事多年的人对他做出了这样的评价：</p><p>&nbsp;</p><p>“不卑不亢，不骄不躁，总是怀着空杯与谦虚的心态，拼命吸收新领域的知识，但充满自信和信念，坚定自己从事的事情会为他人带去极大的价值。”</p><p>&nbsp;</p><p></p><h3>低调前行的核心团队</h3><p></p><p>&nbsp;</p><p>不少人一直把 ECharts 等同于了林峰，但其实林峰早在15年便离开了百度，从这之后，百度逐渐建立了一只团队，持续在维护 ECharts，但直到2018年，这只团队才借助 ECharts 4.0 的发布，完整地出道。</p><p>&nbsp;</p><p>为了弥补林峰离职的影响，EFE攒出了一只麻雀虽小五脏俱全的精英团队，里面有来自浙大数据可视化专业硕士毕业，师从陈为教授的高徒；也有后来成为 Apache Member 的羡辙；还有经常代表 ECharts 出席各种技术大会的几位架构师。</p><p>&nbsp;</p><p>这个精英团队从 ECharts 发布后的第三年开始，成为了产品的真正脊梁，扛着产品，走过了七年。如今的 ECharts，从技术架构和社区建设的关键几个环节，都是经这个团队之手。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b88c3ebc44c984ea0fdf87479b3e889.png\" /></p><p></p><p>只是很可惜，这个团队过于低调，我们只能从 QCon 中祖明的分享中，了解到少许这个团队内部的工作情况，诸如大家经常一起到某个工程师家里封闭边娱乐边办公。希望未来有机会我们能够更多地了解到这只核心团队的故事。</p><p>&nbsp;</p><p>但 ECharts 真正的未来，我们相信，依然在社区，随着在 Apache 成功孵化毕业，ECharts的外部贡献者越来越多，最近一年来 Apache ECharts 最活跃的开发者，已经是一位非百度的程序员，ECharts 虽然走得比以前慢，但详细，社区的力量，会让其走得更久。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>第一个是从 ECharts 本身的发展来说，ECharts 的立足点很直接，就是面向百度自己的产品线业务，服务于业务，老板自然会给资源去做这个事，但是做这个事情的人，需要有高瞻远瞩的理想，这样才会吸引更多志同道合的人跟你一块做这个事情。</p><p>&nbsp;</p><p>第二点不一定适合所有产品，即应该适当的内藏外秀，处于公司或者组织下，才能让你的项目更久远的活下去。有时候技术产品服务内部比服务外部更难，因为服务外部业务的时候，会相对能够按照一些偏标准化的方案去执行，或者说即使有不标准化的地方，你也有权利去拒绝，尤其是像 ECharts 这种不用背收入的项目，拒绝就拒了，也没什么影响，但是内部的产品找你，你敢拒绝，那就是文化价值观的问题了，非常的困难。所以有的时候服务内部确实比服务外部难度要大很多。再就是外面的背书，其实有时候比内部的背书更有用，你服务好内部人家觉得你理所当然，但是服务好外部别人来个表扬信，或者别人来夸你，会比内部说很多这句话都有价值，所以大家如果自己有一些小的技术产品想活得更好，建议把外面的客户抓得更好一些。</p><p>&nbsp;</p><p>第三个是长期发展，如果想变成一个特别好的甚至商业化产品的话，不是靠个人力量能实现的，它依赖整个组织的变革，要么是像 ECharts 这样选择一个更加中立的开源组织，然后从外部去找一些开源的商业的模式，要么就是成为组织的第二曲线。做的比较好的就是像百度现在的 sugar，它是一个非常好的典型，它本来也是只服务内部，但是现在变成一个百度云的产品，成为公司的第二曲线之一。</p><p>&nbsp;</p><p>还有在团队组建上，开源是一个非常需要耐心的事，需要招有长期主义思维的人，怎么理解长期主义思维？他不会被眼前的一些小恩小利或者波折影响，他是自己心里明白到底要什么，这个产品要什么的人，而且他很自驱，但不会急功近利。</p><p>&nbsp;</p><p>开源的核心团队。这也是人月神话里提到这种比较大的架构和产品的话，核心团队的能力要足够强，相对在一定范围内要独裁，而不是民主的。最后是共同的价值观，公开透明的合作决策方式，整个团队要有好的默契，最好是培养出很好的私交，让团队能够更润滑一些。</p><p>&nbsp;</p><p>最后一个就是从产品方向上，做自己喜欢的事非常重要，尤其对于开源，如果做开源，还不喜欢做这个事，就不知道图啥了。然后最好是能擅长，能擅长不是说现在擅长，因为做一个开源产品，对整个 ECharts 团队来说，对大家的成长还是非常有价值的，很多东西其实不知道自己擅不擅长，只有试了以后才知道，所以在这个过程中能帮助你发现你其实还有哪些空间可以去做。还有很重要的一点是要关注用户的反馈，因为开源天生的反馈在 issue 里就很多，但是这里给大家提个醒，就是要结合一些反馈的上下文去看，要找到一些用户去问，你遇到这个问题到底是为什么? 你在什么样的场景使用？因为有的时候他直接反馈给你的东西，issue 里其实讲不明白，信息也相对有限。然后文档和 Showcase 也非常重要，什么是 Showcase，就是本来我想展示的东西没展示出来。最后就是产品功能可以多，但营销卖点要尽量专一，有时候你以为的卖点和用户想象的卖点是不一样的，要分清楚。用户想要的东西你直接说会让他觉得很 low，所以宣传的时候还是要高大上一些。</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247545699&amp;idx=3&amp;sn=d8b5508b8136adb405d4d9367822cc7d&amp;chksm=fbea8aaccc9d03badf450fff0cf3cb6bb552d57c16c8d6735ec555a00daf6287dc7d54782e72&amp;scene=27#wechat_redirect\">使用&nbsp;Apache&nbsp;ECharts&nbsp;呈现社区活动的渲染图</a>\"</p><p><a href=\"https://www.infoq.cn/article/9cvcLS4okZXT6S6lAYRS\">特斯拉是如何使用&nbsp;Apache&nbsp;ECharts&nbsp;的？</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247509777&amp;idx=1&amp;sn=e895df684c837898c5810fcce0ca0d79&amp;chksm=f9521252ce259b44957051e7fa68d590653aa626b61708d53f363f8f5a7e8338cbfdffd30d9e&amp;scene=27#wechat_redirect\">Apache&nbsp;ECharts&nbsp;团队：ASF&nbsp;顶级项目是怎么炼成的？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/12808c5b3d41917341d466a2f\">Apache&nbsp;HugeGraph1.0.0 版本正式发布！</a>\"</p>",
    "publish_time": "2023-08-21 11:39:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何在Next.js全栈应用程序中无缝实现身份验证",
    "url": "https://www.infoq.cn/article/HKfRavLNgxBAhyMoZAGw",
    "summary": "<p>&nbsp;</p><p></p><blockquote>本教程中，我们将一同了解如何使用Clerk向全栈应用程序添加身份验证机制。&nbsp;我们跟Clerk没有任何合作关系，但对这款工具的表现非常认可。很多朋友正好咨询怎么在Next.js下实现身份验证，这篇文章专为解决问题而来。</blockquote><p></p><p>&nbsp;</p><p></p><h2>背景介绍</h2><p></p><p>&nbsp;</p><p>身份验证一直是构建全栈应用程序中的一大主要痛点。特别是在Node.js环境当中，各种主流库和框架都没有内置auth-primitives。因此，开发人员不得不自己想办法构建身份验证解决方案。</p><p>&nbsp;</p><p>但从零开始构建安全身份验证是项颇为艰巨的任务。我们首先得对密码进行哈希和加盐处理，发布签名令牌来创建会话，同时防止各种恶意攻击向量。此外，大家还得保证自己的前端和后端能够相互通信、正常共享会话。</p><p>&nbsp;</p><p>好消息是，Express的Passport.js和Next.js的NextAuth等库就是为此而生，只是还不够完美。这些库的设置流程涉及多个步骤，虽然已经能较好地配合Google或GitHub等服务实现社交身份验证，但毕竟要比密码登录更困难。而且密码内容仍须存储在服务端数据库内，由软件开发一方承担全部安全责任。</p><p>&nbsp;</p><p>如今，登录时通过邮件验证、无密码登录和双因素身份验证已经相当流行。虽然前面讨论的库也能支持这些功能，但需要在本就复杂的设置之外再做更多额外工作。</p><p>这时就要请出托管身份验证提供程序Clerk了，它消除了身份验证中的所有难题，大大降低了妥善保护全栈应用程序的门槛。与其他托管身份验证提供程序相比，Clerk的开发者体验也明显做得更好。</p><p>&nbsp;</p><p>在本教程中，我们将运用Clerk及其全新App Router，在Next.js 13当中构建一款简单的全栈应用程序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bbe3e435ac04524c21204432bcc5f58d.png\" /></p><p></p><p></p><h2>设置</h2><p></p><p></p><p>首先在您终端中指定的文件夹中运行命令npx create-next-app@latest，从而创建新的Next应用程序。请按以下指定方式完成设置。需要注意的是，一定要在Tailwind CSS和App Router部分选择Yes。</p><p>&nbsp;</p><p><code lang=\"null\">Desktop npx create-next-app@latest\n✔ What is your project named? ... clerk-auth\n✔ Would you like to use TypeScript with this project? ... No / Yes\n✔ Would you like to use ESLint with this project? ... No / Yes\n✔ Would you like to use Tailwind CSS with this project? ... No / Yes\n✔ Would you like to use `src/` directory with this project? ... No / Yes\n✔ Use App Router (recommended)? ... No / Yes\n✔ Would you like to customize the default import alias? ... No / Yes</code></p><p>&nbsp;</p><p>现在我们切换到刚刚创建的clerk-auth文件夹，安装唯一的依赖项：Clerk。</p><p>&nbsp;</p><p><code lang=\"null\">npm install @clerk/nextjs</code></p><p>&nbsp;</p><p>接下来需要创建一个Clerk账户和新项目，获取要用到的API密钥。这就需要转到clerk.com，创建一个账户，之后在仪表板上单击“Add application”。</p><p>&nbsp;</p><p>将应用程序命名为clerk-auth-demo，并选择Email + Google的登录方式。如果需要，大家还可以添加其他登录方式。请放心，这不会对开发过程产生任何影响，Clerk为替我们完成所有工作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/53de94de4f015539a67a9aea0196fa65.png\" /></p><p></p><p>&nbsp;现在，Clerk会自动提供要添加到Next应用程序的API密钥。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8f087b503e747cf6d6c06d7a9d176d8c.png\" /></p><p></p><p>&nbsp;因此，请创建一个.env.local文件，把Clerk那边复制的内容全部粘贴进来。</p><p>&nbsp;</p><p><code lang=\"null\">NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_test_****\nCLERK_SECRET_KEY=sk_test_******</code></p><p>&nbsp;</p><p>要完成Clerk身份验证配置，最后一步就是把这款身份验证提供程序添加到/src/app/layout.tsx文件当中。如果大家比较熟悉传统的Next.js页面范式，会发现其内容跟/src/_app.tsx文件差不多。</p><p>&nbsp;</p><p><code lang=\"null\">import { ClerkProvider } from '@clerk/nextjs';\nimport './globals.css';\nimport { Inter } from 'next/font/google';\nconst inter = Inter({ subsets: ['latin'] });\nexport const metadata = {\n  title: 'Create Next App',\n  description: 'Generated by create next app',\n};\nexport default function RootLayout({ children }: { children: React.ReactNode }) {\n  return (\n    \n      \n        {children}\n      \n    \n  );\n}</code></p><p>&nbsp;</p><p>ClerkProvider中提供一项appearance属性，我们可以在其中定制Clerk组件以匹配应用程序的设计风格。每个Clerk组件还能单独设置样式。到这一步，我们就能在应用程序中使用Clerk了。</p><p>&nbsp;</p><p></p><h2>向应用添加身份验证</h2><p></p><p></p><h3>登录和注册页</h3><p></p><p></p><p>首先，我们需要创建注册和登录页。Clerk已经提供了完整的表单组件，剩下要做的就是利用这些组件构建一个简单的示例页面。</p><p>&nbsp;</p><p>我们从登录页开始。使用以下内容，在/src/app/sign-in/[[..sign-in]]/page.tsx&nbsp;中创建一个新组件：import { SignIn } from '@clerk/nextjs';</p><p>&nbsp;</p><p><code lang=\"null\">export default function SignInPage() {\n  return (\n    </code></p><div><code lang=\"null\">\n      \n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这里的文件路径可能跟大家习惯的传统Next.js应用有所区别，其中页面URL由/src/app/sign-in文件夹来定义，代表着页面实际上位于/sign-in。中括号用于捕捉Clerk内部使用的/sign-in/...&nbsp;之后的所有内容。使用新的App Router功能，页面本体将始终存放在page.tsx文件之内。</p><p>&nbsp;</p><p>至于/src/app/sign-up/[[..sign-up]]/page.tsx注册页面，处理方式也基本相同：import&nbsp;{&nbsp;SignUp&nbsp;}&nbsp;from&nbsp;'@clerk/nextjs';</p><p>&nbsp;</p><p><code lang=\"null\">import { SignUp } from '@clerk/nextjs';\n\n\nexport default function SignUpPage() {\n  return (\n    </code></p><div><code lang=\"null\">\n      \n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>现在转到http://localhost:3000/sign-in&nbsp;，就能看到预制好的注册组件，它支持电子邮件、密码或者大家指定的任何社交身份验证提供程序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/30/3099132234dbbc614b74a21c2bd01098.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>账户页面</h3><p></p><p></p><p>创建一个账户，或者通过Google进行登录。到这里，我们已经完成了应用登录，但目前的功能还比较有限。所以我们需要创建账户页面，首先将/src/app/page.tsx&nbsp;文件中的内容变更为：</p><p>&nbsp;</p><p><code lang=\"null\">import { UserButton } from '@clerk/nextjs';\nexport default function Home() {\n  return (\n    </code></p><div><code lang=\"null\">\n      <div>\n        <p>Hello, User!</p>\n        \n      </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这里我们使用到Clerk的UserButton组件。登录之后，它将为提供User Setting的下拉菜单，用户可以在其中更改密码、电子邮件地址和其他各种设置。这些功能是收费的，但毕竟能帮我们省下自行开发验证带来的时间和精力投入。</p><p>&nbsp;</p><p></p><h2>保护页面</h2><p></p><p></p><h3>Secret页面</h3><p></p><p>&nbsp;</p><p>现在我们需要在/protectet上创建一个新页面，要求该页面只能由经过身份验证的用户访问。为此，我们需要在/src/middleware.ts&nbsp;中创建一个新的中间件，内容如下：</p><p>&nbsp;</p><p><code lang=\"null\">import { authMiddleware } from \"@clerk/nextjs\";\nexport default authMiddleware({\n  publicRoutes: [\"/\"]\n});\nexport const config = {\n  matcher: [\"/((?!.*\\\\..*|_next).*)\", \"/\", \"/(api|trpc)(.*)\"],\n};</code></p><p>&nbsp;</p><p>此外，将以下新变更添加到.env.local文件当中，以告知Clerk在需要重新定向时如何操作。</p><p>&nbsp;</p><p><code lang=\"null\">// other settings\nNEXT_PUBLIC_CLERK_SIGN_IN_URL=/sign-in\nNEXT_PUBLIC_CLERK_SIGN_UP_URL=/sign-up\nNEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL=/\nNEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL=/</code></p><p>&nbsp;</p><p>Clerk提供的这个中间件，将确保只有root页面和注册/登录页面对未经身份验证的用户可见。现在让我们在/src/app/protected/page.tsx上创建页面：</p><p>&nbsp;</p><p><code lang=\"null\">export default function Protected() {\n  return (\n    </code></p><div><code lang=\"null\">\n      <div>\n        <p>This is a protected page</p>\n      </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这样在登录和注销时，就会转向这个页面。请注意，如果未能通过身份验证，访问者将被重新定向至/sign-in。</p><p>&nbsp;</p><p></p><h3>在主页中显示登录链接</h3><p></p><p>&nbsp;</p><p>当用户尚未登录时，我们的root页面目前不会显示任何信息。但现在中间件已经设置完毕，我们可以修改/src/app/page.tsx&nbsp;文件来更改此中间件：</p><p>&nbsp;</p><p><code lang=\"null\">import { UserButton, currentUser } from '@clerk/nextjs';\nimport Link from 'next/link';\nexport default async function Home() {\n  const user = await currentUser();\n  return (\n    </code></p><div><code lang=\"null\">\n      <div>\n        {user ? (\n          &lt;&gt;\n            <p>Hello, User: {user.emailAddresses[0]?.emailAddress}</p>\n            \n          \n        ) : (\n          \n            Sign in\n          \n        )}\n      </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p>这是一个React服务器组件，会使用await从Clerk异步获取当前用户会话。取决于会话是否存在，它会显示UserButton以及用户的电子邮件地址，或者指向登录页面的链接。</p><p>&nbsp;</p><p></p><h2>保护API路由</h2><p></p><p></p><p>到这里，我们已经讨论了如何保护应用前端。但全栈应用程序还有后端部分，为此我们将在新的App Router模式中使用/src/app/api/route.ts文件，借此在GET/api处创建一个后端端点：</p><p>&nbsp;</p><p><code lang=\"null\">import { auth } from '@clerk/nextjs';\nimport { NextResponse } from 'next/server';\nexport async function GET() {\n  const { userId } = auth();\n  if (!userId) {\n    return new Response('Unauthorized', { status: 401 });\n  }\n  const data = { message: 'Hello User', id: userId };\n  return NextResponse.json({ data });\n}</code></p><p>&nbsp;</p><p>这里的auth()函数会检查是否存在Clerk会话。如果不存在，则抛出401未经授权错误。而如果用户成功通过了身份验证，接下来就是设置用户能在端点上进行的操作了。我们可以访问userId，据此将数据库中的数据引用给用户。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>至此，我们已经在全栈Next.js 13应用程序中完成了Clerk Authentication的完整实施。可以看到，整个过程几乎无需编写任何身份验证代码就能正常起效！这也是Clerk等外部提供程序的魅力所在。更重要的是，我们的小小演示应用也内置了一系列用户管理功能，包括验证/更改电子邮件地址、更改密码和社交登录等，能帮开发者省下很多时间。</p><p>&nbsp;</p><p>对于同时拥有前端和后端的全栈应用程序，Clerk在Next.js等框架中有着相当出彩的表现。但如果匹配单独的后端，那在设置方面就要更复杂一些。Clerk可以发出JWT令牌，由开发者将其与API请求一同发往后端以验证用户身份。这种方式虽然可行，但整个过程肯定不如本文展示的那样无缝丝滑。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p></p><p><a href=\"https://dev.to/livecycle/seamless-full-stack-authentication-in-nextjs-11lp\">https://dev.to/livecycle/seamless-full-stack-authentication-in-nextjs-11lp</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://www.infoq.cn/article/VbDui4DRa2Lhq3h0ad2Z\">为什么说 Next.js 13 是一个颠覆性版本</a>\"</p><p><a href=\"https://xie.infoq.cn/article/93e23e080e828a8989a57a622\">Next.js&nbsp;实践：从 SSR 到 CSR 的优雅降级</a>\"</p><p><a href=\"https://www.infoq.cn/article/sITi66wc3mvcNs3PeRkb\">Next.js 13 新的实验性特性，实现 App“动态无限制”</a>\"</p><p><a href=\"https://www.infoq.cn/article/9G0lBWi2W58114ggfyge\">我们如何使用 Next.js 将 React 加载时间缩短 70%</a>\"</p>",
    "publish_time": "2023-08-21 11:47:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "前端全职岗位依然坚挺，广大同志不必惊慌",
    "url": "https://www.infoq.cn/article/FTfvi9TPlgAGIhDUDEtP",
    "summary": "<p></p><p>德国前端开发人员克里斯蒂安·海尔曼（Christian Heilmann）在这篇文章中阐述了前端开发作为全职工作的重要性和价值，并回应了一些人认为前端开发不是“真正”的开发工作的观点。</p><p></p><p>我最近刚好在找新工作。过程当中，很多雇主在收到简历后都表示满意，但随后又提到我好像太偏前端，想了解我在后端和全栈开发方面有没有经验。在我看来，这股歪风在 2021 年之前就吹过一波，总有人感觉 Web 开发或者说前端专家已经不足以支撑一个全职岗位了。所以我当时就做过如下解释。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4b/4b5c8e963b5074d242bc7f263fa6921b.png\" /></p><p></p><p>请注意：前端开发者擅长的不只是 HTML、CSS 和 JavaScript 这些“简单”的编程语言。前端开发者需要主动为未知的场景构建交互界面，他们的水平直接决定着最终用户的实际体验。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f4/f4dd8e2faa966214410c295b124a5849.png\" /></p><p></p><p>Web 绝不只是普通的编译目标，更是一个允许用户完全控制其外观风格的平台。它也是唯一一个拥有充足弹性，能够承受种种修改调整的平台。Web 同时面向桌面、Android 和 iOS，依托同一套代码库就能实现。只有敬业的前端开发者才清楚自己到底在干什么，而不会愚蠢地指望依靠一款插件或者扩展程序就奇迹般地让所有人都能获得满意的产品访问体验。</p><p></p><p>这里我还想再强调一句：无论最终选择什么平台，使用哪种编程语言，或者指定什么框架和库，最终跑在 Web 用户设备上的仍然是 HTML、CSS 和 JavaScript。</p><p></p><p>其中每一样（HTML 相对会好一点）都可能引发性能问题、跨浏览器功能冲突，并在难以预料的低配置、低网络质量环境中造成令人头痛的用户访问障碍。大家都知道，糟糕的性能表现只会让用户愤然“点叉”离去，某些服务无法正常访问甚至可能导致法律和合规性问题，导致我们被送上审判席。</p><p></p><p>我曾在全球最大的网站（包括 yahoo.com、bing、微软等等）和 Firefox、Edge 等浏览器上做过开发，这些开发商始终专注于一个目标：不要因为响应缓慢或者“错误”提示而被用户怒喷。所以我们得跟众多内部向外部合作伙伴携手，了解他们产品无法正常运行的原因。合作对象可能是扩展程序供应商、框架创建团队或者开发小组。在 Mozilla 和微软的“性能俱乐部”里，我们也一直在遇到各种问题：Web 产品中包含大量毫无意义的 HTML、几乎用不上的 CSS 和让人崩溃的 JavaScript，它们都在被无脑发送给用户。而这么做的原因，就是要让开发人员更便捷、更灵活地用一套框架搞定所有构建工作。</p><p></p><p>正是由于向全栈开发的转变，导致我们的 Web 体系越来越臃肿，它不仅拉低了客户满意度，也让用户平白付出了不必要的流量。</p><p></p><p>当然，造成 Web 产品质量堪忧还有另一个原因：组件设计缺乏大局观。</p><p></p><p>现在的 Web 产品压根不是按照文档或者作为网站进行构建的。相反，它们被视作一个个独立的组件，每个组件都非常灵活以适应不同的运行环境。这听起来很棒，但开发者开始随意进行组装，想起什么就塞进来什么。有时候，我们会发现 20 多种不同的、高度可定制的按钮组件，而它们执行的其实是同一种操作。</p><p></p><p>有经验的前端开发者肯定立马能察觉到其中的问题，并追踪到相应的资源浪费。</p><p></p><p>前端开发者究竟该如何定义？他们是：</p><p></p><p>浏览器性能专家跨平台开发专家辅助功能专家合规知识专家设计和测试部门间的桥梁最终用户的客服代表</p><p></p><p>但很多人的观念都被市场导向给扭曲了。他们之所以看不起前端开发者，主要是因为大多数人根本不知道做好这份工作需要哪些能力。</p><p></p><p>CSS 和客户端 JavaScript 不算真正的编码也完全是奇谈怪论。更讽刺的是，那些宣称自己不想碰 CSS 的家伙，给出的理由往往是“太难了，太怪了”。CSS 也不单纯是在调色加填充，它有自己的网格、子网格和伸缩布局框，这是一套完全成熟的布局系统，还能实现动画和响应式渲染。通过媒体和容器查询，开发者可以获得惊人的灵活性；通过级联层，大家甚至可以控制浏览器呈现当前设计的具体方式。</p><p></p><p>所以选择权在雇主手里。你可以聘请前端开发者专职构建自己的产品，也可以随随便便凑合搭建起来，再聘请性能和辅助功能顾问修复其中的问题。但请注意，越是来到生产下游，产品的优化和修复就越是困难，且往往会与新功能产生冲突。</p><p></p><p>所以当雇主们问我为什么“只做前端”时，我以自豪的心情解释了这一切，并强调我以身为前端工程师为荣。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/64/64e49de26ed551f8cae6fb6d40e18f0c.png\" /></p><p></p><p>附录：其实同样的道理也适用于市场上的其他职位。好的数据库工程师能帮你省下几秒钟的加载时间；优秀的云工程师能帮你节约云运营成本；优秀的后端工程师能确保你的服务器只跑有用的负载，用不着为一大堆根本没用的前端代码运行优化管线。如今科技大厂都在收缩规模，所以市场上的人才水准随之提升。也许请一位 10 倍全栈“技术大师”的预算现在没准够请 3 位专项技术专家，毕竟他们刚刚逃离要拿全部收入的 80% 交房租的“宇宙一线城市”。</p><p></p><p></p><h5>原文链接：</h5><p></p><p></p><p><a href=\"https://christianheilmann.com/2023/05/09/the-ongoing-defence-of-frontend-as-a-full-time-job/\">https://christianheilmann.com/2023/05/09/the-ongoing-defence-of-frontend-as-a-full-time-job/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/170673a775bec8d1e407bb035\">Web前端设计开发工具集（JS 框架、CSS 预处理）</a>\"</p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247485672&amp;idx=1&amp;sn=8f502dc871acfdc0d19f5bf79b6d5a3b&amp;chksm=f951bdabce2634bd1928eb241072d3bbaa436935d29b7979589befc7b76bf0225595ae28febf&amp;scene=27#wechat_redirect\">只需两步，快速提升你在前端圈的技术力和影响力</a>\"</p><p><a href=\"https://xie.infoq.cn/article/3da93330ebd71cf5b594cbb66\">从 0 到 1 普及前端知识|内容合集</a>\"</p><p><a href=\"https://xie.infoq.cn/article/bfc0d7c4415c2abd1c7811027\">使用 Lambda&nbsp;Web&nbsp;Adapter 在 Lambda 上 构建&nbsp;web&nbsp;应用</a>\"</p>",
    "publish_time": "2023-08-21 11:52:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据库内核杂谈（三十六）- 向量数据库（4）quantization和HNSW",
    "url": "https://www.infoq.cn/article/qX7tW8YLlYf0E2Wh6Kz3",
    "summary": "<p>欢迎阅读新一期的内核杂谈。这是向量数据库的第四弹！这一期，咱们继续学习如何优化向量的存储和查询的技术，quantization和hierarchical navigable small worlds。这两个专有名词可能对很多同学来说都比较陌生，但希望经过这一期的学习，可以为大家demystify它们！</p><p></p><p></p><h2>Quantization（量化）</h2><p></p><p></p><p>前几期，咱们介绍了tree-based approach和sharding-based approach，它们都是通过引入不同类型的数据结构来存储（indexing）和查询vectors。Quantization技术，是通过对原生的vectors进行压缩优化，从而达到节省整体的存储vector容量的目的（进一步，由于存储优化了，计算时使用的资源也得到了优化）。也因此，Quantization技术和前几期介绍的技术是可以共用来达到更大的优化。我们依次介绍不同类型的quantization技术。</p><p></p><p></p><h3>Scalar Quantization</h3><p></p><p></p><p>Scalar Quantization是最简单易懂的quantization技术，它通过将大空间的向量转换成相对小空间的向量来减少整体的存储空间。通常是将浮点数（比如f32）向量量化成整数（比如int8）向量（存储的优化比是4X）：也可以认为是将2^32映射到2^8的空间里。原先不同的浮点数可能在映射后就变成同一个值。因此，scalar quantization是通过牺牲了计算精度来提升存储和查询效率。</p><p></p><p>下面，咱们结合代码来看如何实现Scalar quantization。首先，我们随机生成一个128维的1000 size的向量组。</p><p></p><p><code lang=\"text\">import numpy as np\ndataset = np.random.normal(size=(1000, 128))</code></p><p></p><p>接下来，我们需要将浮点数转换到int8的空间里。推荐大家也可以思考一下算法。一种可行的算法是：针对每一个dimension，遍历所有的数字得到min和max值，然后将min值映射到0（int8的min值），max值映射到255（int8的max值），然后，通过(max - min)/255计算出step值。相当于把min到max划分成了255个桶，然后对原先的数字进行桶排序，将原先向量中的浮点数替换成桶的ID数，就做到了f32到int8的映射。在映射的过程中，也尽量保证了向量之间的差异。代码实现也非常直观：得到每个维度的min，max值：</p><p></p><p><code lang=\"text\">ranges = np.vstack((np.min(dataset, axis=0), np.max(dataset, axis=0)))</code></p><p></p><p>计算出step值，并将每一个vector映射到int8的桶里：</p><p><code lang=\"text\">starts = ranges[0,:]\nsteps = (ranges[1,:] - ranges[0,:]) / 255\n\ndataset_quantized = np.uint8((dataset - starts) / steps)\n</code></p><p></p><p>打印dataset_quantized，大致会得到下面这样的输出：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9ebb847866b270bb5280da9f7d96f281.png\" /></p><p></p><p>下面的代码将scalar quantization封装起来使用：</p><p></p><p><code lang=\"text\">import numpy as np\n\nclass ScalarQuantizer:\n    def __init__(self):\n        self._dataset = None\n    def create(self):\n        \"\"\"Calculates and stores SQ parameters based on the input dataset.\"\"\"\n        self._dtype = dataset.dtype  # original dtype\n        self._starts = np.min(dataset, axis=1)\n        self._steps = (np.max(dataset, axis=1) - self._starts) / 255\n        # the internal dataset uses `uint8_t` quantization\n        self._dataset = np.uint8((dataset - self._starts) / self._steps)\n    def quantize(self, vector):\n        \"\"\"Quantizes the input vector based on SQ parameters\"\"\"\n        return np.uint8((vector - self._starts) / self._steps)\n    def restore(self, vector):\n        \"\"\"Restores the original vector using SQ parameters.\"\"\"\n        return (vector * self._steps) + self._starts\n    @property\n    def dataset(self):\n        if self._dataset:\n            return self._dataset\n        raise ValueError(\"Call ScalarQuantizer.create() first\")\n\ndataset = np.random.normal(size=(1000, 128))\nquantizer = ScalarQuantizer(dataset)</code></p><p></p><p>（scalar quantization code example: reference from: https://zilliz.com/blog/scalar-quantization-and-product-quantization)</p><p></p><p></p><h3>Vector Quantization</h3><p></p><p></p><p>Vector quantization是另一类简单的quantization技术。区别于scalar quantization是分别从每一个维度进行压缩，vector quantization是通过将原始向量当做一个整体去看待，将一个向量映射到指定的参考向量里（通常，指定的参考向量被称为码本，vector codebook）。虽然，每一个向量并没有从存储size上减小，但你可以认为所有要存储的向量都在这个码本里，只要码本的size足够小，带来的存储上的收益就非常大。</p><p></p><p>如何构建具有代表性的码本呢？又可以搬出我们介绍过的k-means算法了。k-means算法通过将空间上相似的向量聚类到一起，并通过centroid质点来代表这个聚类。因此，vector quantization可以通过将所有的向量都映射到相应的聚类的质点向量来提升存储效率。</p><p></p><p></p><h3>Product Quantization</h3><p></p><p></p><p>Vector quantization是将向量空间里的所有dimension都考虑进来，然后将每个向量映射到码本的参考向量里。实际情况中，最常见的quantization技术是product quantization，它和vector quantization的区别在于，将所有的dimension划分成subspace，然后每个subspace独立去做vector quantization（通过k-means找到代表这段subspace的质点）。下面是product quantization的具体算法描述：</p><p></p><p>1）给定一个包含N个总向量的数据集（设定为d dimension），我们首先将每个向量划分为M个子向量（也称为子空间），每个子空间的dimension为 d/M。这些子向量的长度不必完全相同，但实际情况下，推荐使用相同的长度（方便代码实现）。</p><p>2）对数据集中所有子向量使用k-均值（或其他聚类算法）。这会为每个子空间给出一组K个质心，每个质心都将被分配其唯一的ID。</p><p>3）在计算出所有质心后，我们将用其最近质心的ID替换原始数据集中的所有子向量。</p><p>4）给定一个新的d dimension的向量，分成M个子向量，分别对应每个子向量找到对应的质心，然后用质心ID组合起来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/386a2d8cd1e4aa8a734cd08bd1578929.png\" /></p><p></p><p>（product quantization示例）</p><p></p><p>Production quantizer的实现代码如下（下面的代码将原生的vector划分成16个subspace，然后每个subspace用k=256个cluster来表达）：</p><p></p><p><code lang=\"text\">import numpy as np\nfrom scipy.cluster.vq import kmeans2\n\nclass ProductQuantizer:\n    def __init__(self, M=16, K=256):\n        self.M = 16\n        self.K = 256\n        self._dataset = None\n    def create(self, dataset):\n        \"\"\"Fits PQ model based on the input dataset.\"\"\"\n        sublen = dataset.shape[1] // self.M\n        self._centroids = np.empty((self.M, self.K, sublen), dtype=np.float64)\n        self._dataset = np.empty((dataset.shape[0], self.M), dtype=np.uint8)\n        for m in range(self.M):\n            subspace = dataset[:,m*sublen:(m+1)*sublen]\n            (centroids, assignments) = kmeans2(subspace, self.K, iter=32)\n            self._centroids[m,:,:] = centroids\n            self._dataset[:,m] = np.uint8(assignments)\n    def quantize(self, vector):\n        \"\"\"Quantizes the input vector based on PQ parameters\"\"\"\n        quantized = np.empty((self.M,), dtype=np.uint8)\n        for m in range(self.M):\n            centroids = self._centroids[m,:,:]\n            distances = np.linalg.norm(vector - centroids, axis=1)\n            quantized[m] = np.argmin(distances)\n        return quantized\n    def restore(self, vector):\n        \"\"\"Restores the original vector using PQ parameters.\"\"\"\n        return np.hstack([self._centroids[m,vector[m],:] for m in range(M)])\n    @property\n    def dataset(self):\n        if self._dataset:\n            return self._dataset\n        raise ValueError(\"Call ProductQuantizer.create() first\")\n\ndataset = np.random.normal(size=(1000, 128))\nquantizer = ProductQuantizer()\nquantizer.create(dataset)\n\n# quantize a new vector\ntest_vector = np.random.normal(size=(1, 128))\nquantized = quantizer.quantize(test_vector)\n\n# restore to original dimension vector (using centroids)\nquantizer.restore(quantized)\n</code></p><p></p><p>为什么说product quantization比其他两类方法在实际应用中效果更好呢？找到的理由是，划分成多个subspace，比不划分（vector quantization）或者只考虑单个dimension的方法，（scalar quantization）能够在高维的向量空间中，更有效地捕捉到不同维度的相似度（correlation），因此在存储空间优化和准确性上有更好的平衡。感觉，这个有点玄学。不过介绍也说，通常情况下表现更好，但最好的方法还是通过不同的实验去比较不同方法的优劣。</p><p></p><p></p><h2>Hierarchical Navigable Small Worlds (HNSW)</h2><p></p><p></p><p>接下来要学习的是目前最广泛使用的算法，Hierarchical Navigable Small Worlds (由于我并没有找到对应的中文名称，下文就用HNSW来表示）。HNSW在查询速度和准确度上有非常好的平衡，也因此成为最受欢迎的vector search的算法。要学习HNSW，需要先熟悉两个概念，skip-list（跳表）和Navigable Small Worlds (NSW)。</p><p></p><p></p><h3>Skip-List（跳表）</h3><p></p><p></p><p>介绍网上已经有大量的优质的介绍skip-list的文章了，咱们稍稍简单介绍一下。要介绍跳表，先得从众所周知的链表（Linked-List）说起，一种每个元素都维护下一个元素（或者同时维护上一个元素）的指针的著名数据结构（数据结构和算法101）。尽管链表非常适合实现last-in first-out（LIFO）和first-in first-out（FIFO），如堆栈和队列，但在随机访问的时间复杂度表现较差，是O(n)。跳表旨在通过引入额外的中间层来优化这个复杂度，从而实现O(log n)的随机访问时间复杂度，同时增加额外的内存（与普通链表的O(n)相对，空间复杂度为O(n log n)），以及插入和删除的运行时开销。跳表是一个多层链表，上层维护“长”连接。当我们向下层移动时，连接变得越来越短，最底层是包含所有元素的“原始”链表。在插入元素时，可以试用一个随机函数（roll the dice）来决定是否要插入一个中间层。跳表通常被拿来和平衡二叉树（比如AVL-tree或者Black-red-tree）相比，因为它们的访问复杂度类似。跳表的优势在于实现相对容易，且在更频繁的更新用例（插入，删除）下表现相对更好。下图给出了一个跳表的示例：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/db316e4b464cf85d99f1fb92e7738d35.png\" /></p><p></p><p>（跳表示例）</p><p></p><p>跳表的查询从最高层开始，在跳跃列表中到达元素i。一旦我们找到一个与列表中大于i的元素对应的节点，我们就回退到前一个节点并移动到下面的层。这一过程一直持续到我们找到我们正在寻找的元素为止。注意，跳跃列表仅适用于排序列表，因为我们需要一种直接比较两个对象大小的方法。</p><p></p><p>跳表的中间层插入是基于概率的。对于任何新元素，首先需要弄清楚元素首次出现的层。最上层的概率最低，随着在层中向下移动，概率逐渐增加。一般规则是，某一层中的任何元素都会以某个预定的概率p出现在它上面的层中。因此，如果元素首次出现在某个层l中，它还将被添加到l-1、l-2等层中。</p><p></p><p></p><h3>Navigable Small World 介绍</h3><p></p><p></p><p>介绍了跳表后，我们来学习一下 Navigable Small Worlds（下面简称NSW）。首先想象一个网络，网络上有许多节点，这些节点有些相互联通。每个节点会和联通的节点有短距离、中距离或者长距离的连接。</p><p></p><p>在执行搜索时，我们将从某个预先设定的的入口点开始搜索。从那里，我们通过比较搜索点和这个点以及其联通的点的距离，并跳转到距离搜索点节点最近的节点。这个过程重复，直到我们找到最近的邻居。这种搜索称为贪心搜索。这个算法适用于数百或数千个节点的小型NSW，但对于更大的NSW则效率比较低。一个优化方法是通过增加每个节点的短距离、中距离和长距离连接的节点的数量，但这会增加网络的整体复杂性，并导致搜索时间变长。</p><p></p><p>将向量数据集中的所有向量想象成NSW中的点，长距离连接的点表示彼此不相似的两个向量，短距离连接的点则相反，表示两个近似的向量。通过这种方式，将整个数据集向量构建成NSW，我们可以通过贪婪算法遍历NSW，朝着越来越接近搜索向量的顶点方向移动，完成近邻搜索。</p><p></p><p></p><h3>HNSW的查询和构建</h3><p></p><p></p><p>在实际应用中，向量数据集包含着成千上亿个向量，简单的NSW的搜索效率太低了。于是，诞生了结合跳表和NSW的HNSW：与跳表一样，HNSW是一个多层级的数据结构，每层不是链表，而是由NSW构成（如下图所示）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/09717871aebbc41c859a7416909d4912.png\" /></p><p></p><p>（HNSW 示例）</p><p></p><p>HNSW图的最上层有很少的节点，且节点间的距离都很长。而最底层则包括所有节点和最短的链接。搜索过程如下：给定一个查询向量，我们先进入最上层的一个预定义的搜索起点，并通过贪婪算法朝着与我们的查询向量最近的邻居方向前进。一旦到达最近的节点，我们就在第二层重复此过程。这个过程一直持续到最底层，直到我们找到最近的邻居节点，搜索结束。</p><p></p><p>HNSW的另一个优势，区别于我们已经学习过的其他向量存储方式，就是更高效地支持插入新的向量。插入操作和跳表的插入操作类似。对于某个向量v，我们先通过搜索算法，找到这个向量在最底层的位置：首先遍历图的第一层，找到它的最近邻居，然后移到其下方的层次；然后，再次遍历图，以在第二层找到它的最近邻居；这个过程一直持续，直到在最底层的图中找到最近的邻居。</p><p></p><p>第二步，需要确定向量v和哪些现有节点建立连接（顶点之间的连接）。通常会设定一个预定义的参数M，它决定了我们可以添加的双向链接的最大数量。这些链接通常设置为v的最近邻居。与跳表一样，查询向量在上层出现的概率呈指数递减，可以试用一个随机函数来判断是否要创建上层的节点。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>这一期，我们一起学习了quantization和hierarchical navigable small worlds。Quantization技术能够压缩向量存储的空间，且可以和其他存储技术一起使用。 HNSW是目前最被广泛使用的向量存储和检索技术，通过跳表方式构建的多维NSW在构造和搜索向量数据集都有不错的表现。至此，咱们把构建向量数据库所用到的常用技术和算法都学习了。 下一期，我们会从构建一个向量数据库的角度出发，从整体看如何将这些单点能力串联起来。感谢阅读！</p>",
    "publish_time": "2023-08-21 12:11:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“视象新生”火山引擎视频云& AIGC 技术大会开幕在即",
    "url": "https://www.infoq.cn/article/D7HJP6e66emRsFPtp1Cv",
    "summary": "<p>时下，我们正加速迈入全行业视频化时代。从互动连麦 PK 的弹幕游戏，到 24 小时不停播的数字人电商直播间，再到虚拟映射的平行驾驶远程车控，以及专业的远程视频医疗服务，视频化正在由生活娱乐的重要方式向百业千行扩展。</p><p></p><p>但：</p><p>面对视频数据洪流，技术边界如何突破？视频应用变成标配，落地效率如何提升？行业视频化不断渗透，业务场景如何融合？……</p><p></p><p>8 月 22 日，火山引擎视频云 &amp;AIGC 技术大会即将开启！本次大会以“视象新生”为主题，聚焦体验创新，火山引擎视频云将携手投资机构、意见领袖、合作伙伴、行业代表，共话全行业视频化时代新趋势，视频技术新边界，视频化普惠新进程和场景融合新体验，驱动企业创新增长。</p><p></p><p>大会将全程线上直播，您可扫描下方二维码提前锁定精彩，共同见证全新视频化时代的开启，共同探寻随之而来的亿万新机会！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/270e03715d3335b825dcb16b3cabcc0a.png\" /></p><p></p>",
    "publish_time": "2023-08-21 13:55:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "生成式AI将如何改变金融业务？太保寿险、富滇银行、华盛证券在线分享干货",
    "url": "https://www.infoq.cn/article/D0hYyuqvEgtO0gMtq6pc",
    "summary": "<p>从1956年「人工智能」这一概念诞生以来，经历了三轮起起落落。最近的这次浪潮始于深度学习算法的突破，尤其是2012年AlexNet在ImageNet训练集上图像识别精度取得的重大进展，直接引爆了智能化应用——2016年AlphaGo战胜李世石，让人工智能走进了大众视野；今年初ChatGPT的问世，则让大众热情又被引燃至最高点。</p><p></p><p>而在商业领域，金融行业从来都是技术的“尝鲜者”，自2012年以来，智能化在金融行业的应用经历了探索、跟风、理性三个阶段。在这个过程中，金融行业不断摸索智能技术能够为业务带来的价值，与此同时，也面临着一系列的挑战。</p><p></p><p>针对这些挑战，金融智能化应用成功实践需要什么前提？业务流程如何重塑和发展？数字化背后的企业级架构如何演进迭代？生成式AI又将如何赋能金融业务创新？</p><p></p><p>8月23日（周三）&nbsp;19:30-22:00，来自太保寿险、富滇银行、华盛证券的专家将在《超级连麦.数智大脑》直播中分享他们在银行/保险/证券智能化应用中的思考和实践。<a href=\"https://live.infoq.cn/room/1848\">点击链接</a>\"即可报名预约（留下你关心的问题，讲师将在直播中解答）：<a href=\"https://live.infoq.cn/room/1848\">https://live.infoq.cn/room/1848</a>\"</p><p></p><h3>听众收益</h3><p></p><p>了解金融智能化应用成功实践的两个前提和两个关键；</p><p>了解银行数字化背后的企业级架构演进；</p><p>获得真正发挥智能⻛控价值的路径和⽅法；</p><p>洞察AIGC在金融领域的落地和发展趋势</p><p></p><h3>直播议程</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b99e0a80a04b5a7cb7c3b23bd7a7846d.png\" /></p><p></p><p></p><h3>嘉宾介绍：</h3><p></p><p>周建华，太保寿险首席架构师，武汉大学硕士。15+年ERP及保险领域产品规划研发及技术管理工作经验，对业务需求和痛点具有深刻洞察力，在技术路线规划、科技规划、平台架构设计、AI&nbsp;产品研发等方面经验丰富。</p><p></p><p>李涛，富滇银行数字金融中心副主任，富滇银行“滇峰”计划架构设计负责人。具有19年金融IT架构和敏捷实践经验，曾任职中国工商银行软件开发中心主要从事对私、对公核心业务架构设计和敏捷实践。2015年加入富滇银行主要担任新一代核心系统建设架构师、项目经理。富滇银行全面数字化转型“滇峰”计划项目架构和产品团队负责人，并牵头负责“滇峰”计划项目组织敏捷改进相关工作。</p><p></p><p>黄曙光，华盛证券技术VP，负责带领华盛通科技术中心支持业务的快速增长，同时致力金融科技产品的创新。拥有近20年软件开发及管理经验，曾就职于阿里、云途时代、工商银行等国内外大型知名互联网和金融公司。</p>",
    "publish_time": "2023-08-21 14:27:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "水木分子联合清华AIR、智源开源系列生物医药大模型",
    "url": "https://www.infoq.cn/article/Bydle1aR2REAdNOLVOrC",
    "summary": "<p></p><blockquote>水木分子联合清华大学智能产业研究院（AIR）开源全球首个可商用多模态生物医药百亿参数大模型BioMedGPT-10B，该模型在生物医药专业领域问答能力比肩人类专家水平，在自然语言、分子、蛋白质跨模态问答任务上达到SOTA。同时，水木分子、AIR联合开源了全球首个免费可商用、生物医药专用Llama 2大语言模型BioMedGPT-LM-7B。“AIR-智源健康计算联合研究中心” 合作开源了小分子药物基础模型DrugFM。此次开源的生物医药基础模型重科研、可商用，为生物医药研究与应用提供大模型底座。</blockquote><p></p><p></p><p>开源地址：</p><p><a href=\"https://github.com/PharMolix/OpenBioMed\">https://github.com/PharMolix/OpenBioMed</a>\"</p><p><a href=\"https://huggingface.co/PharMolix/BioMedGPT-LM-7B\">https://huggingface.co/PharMolix/BioMedGPT-LM-7B</a>\"</p><p></p><p></p><h3>研究动机 打通自然语言与化学、生物编码语言</h3><p></p><p></p><p>清华大学智能产业研究院（AIR）首席研究员、水木分子首席科学家聂再清表示：“大模型最令我们惊喜的是智能涌现与触类旁通的能力。生命现象本质也是一种自然进化的语言编码，如果能够将人类总结的知识与氨基酸、分子、蛋白数据压缩到统一的大模型框架内进行编码与学习，有望能够理解生物编码的语言机制，进而从底层推动与生命科学相关的研究与应用。”&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c81b6901c3b71a8ac6b8eb219f1de9ff.jpeg\" /></p><p></p><p></p><p>聂再清教授团队提出了一种全新的多模态语义理解框架 BioMedGPT，它运用了生物医学领域中的预训练大语言模型—BioMedGPT-LM作为桥梁，将自然语言、生物编码语言以及化学分子语言等连接起来。</p><p></p><p>BioMedGPT-LM 通过充分利用海量生物医学相关数据，对通用的基于GPT架构的大型语言模型进行微调，在生物医学领域发挥更出色的性能。作为连接桥梁，BioMedGPT-LM能够连接各种生物模态的编码，包括分子、蛋白质、细胞和基因表达数据，同时还能够整合知识图谱、文档、数值实验结果以及其他格式所体现的专业知识。通过跨模态特征融合模块集成，不同模态的生物编码语言、化学分子语言与自然语言能够在同一个特征空间中实现统一融合。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b611b7c1a59962cc718d3da5655e37d4.jpeg\" /></p><p></p><p>BioMedGPT架构</p><p></p><p></p><h3>BioMedGPT-10B 全球首个开源可商用多模态生物医药百亿参数大模型</h3><p></p><p></p><p>BioMedGPT-10B作为BioMedGPT的一个开源且可商用的具体实例，建立了文本、分子和蛋白质三个模态的统一特征空间。它支持跨模态自然语言和分子语言的交互式问答，可在药物靶点探索与挖掘、先导化合物设计与优化、蛋白质设计等领域得以应用。同时，在生物医药领域的语言理解能力得到显著提升，在多个生物医药问答基准数据集上实现了SOTA，比肩人类医学专家水平，已成功通过了美国医师资格考试。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b1a1bc458bad31e9985b25892ca5ec7e.jpeg\" /></p><p></p><p>通过精心设计的Instruct方法，将不同编码结构的生物医学数据映射到一个共同的文本模态特征空间中，实现了数据的统一性，不同模态的数据可以在相同的语义空间中进行比较和交互。</p><p></p><p>为了进行分子、蛋白质到自然语言的对齐，我们专门构建并开源了分子-文本问答（PubChem QA）数据集和蛋白质-文本问答（UniProt QA）数据集。分子-文本问答（PubChem QA）数据集用于对齐分子和自然语言语义，包含来自PubChem的 325, 754 个分子和365, 129个分子-文本描述。蛋白质-文本问答（UniProt QA）数据集，包含来自UniProt的 569, 516 个蛋白质，涵盖蛋白质相应的名称、蛋白质功能、亚细胞定位和蛋白质家族信息，共计生成了 1, 891, 506 个蛋白质序列-文本描述问答数据。以上数据集现阶段只支持单轮对话，而聂再清教授团队正在进行多轮版本的打造。</p><p></p><p>下面重点介绍模型在典型任务中的表现：</p><p></p><p>分子自然语言跨模态QA</p><p></p><p>该任务针对输入分子式生成对该分子的自然语言描述，同时支持进一步问答，用于探索该分子相关信息。在该任务下，采用了一个经典的分子文本生成任务数据集 ChEBI-20来评估BioMedGPT在处理自然语言和分子语言之间的理解与转化能力。实验针对BioMedGPT-10B的性能与几个基线模型进行了对比。结果表明，BioMedGPT-10B在分子文本生成任务上全面超越了通用语言模型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8f3c188a032a5d886d021e9a2071c684.png\" /></p><p></p><p>&nbsp;蛋白质自然语言跨模态QA</p><p></p><p>该任务针对输入蛋白序列生成对该蛋白的自然语言描述，同时支持进一步问答，可支撑药物靶点发现、靶点挖掘研究。基于UniProt QA数据集进行了系列对比实验，显示出BioMedGPT-10B 在蛋白质-文本跨模态理解和翻译上的能力。以下图为例，未经过对齐的LLama2-7B-chat无法理解输入的蛋白质数据，经过模态对齐后LLama2-7B-chat能通过提问获悉用户意图是想了解蛋白质功能，但仍然无法提供准确和有信息量的回答。BioMedGPT-10B的回答则更精确、全面、明确指出了蛋白质P52341在胸腺嘧啶核苷酸的生物合成中的作用，更接近于标准答案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c8d749648c9a490ff5521fd7119864d3.jpeg\" /></p><p></p><p>生物医药自然语言任务</p><p></p><p>BioMedGPT的语言模型 BioMedGPT-LM 在大规模的生物医学文献数据上进行了训练，其语言能力在生物医学领域表现更为出色。在生物医药领域的三个基准数据集，USMLE、MedMCQA和PubMedQA 达到业内领先水平，在专业生物医学问答方面能够媲美医学专家，成功通过了美国医师资格考。</p><p></p><p>BioMedGPT-10B 在 PubMedQA 上的准确率达到 76.1%，仅比人类专家（expert）标准低 1.9%。在 OOD （Out-of-Domain）设置中，BioMedGPT-10B 的准确率为 50.4%，是除ChatGPT外唯一一个超过人类人工性能（pass）的模型。但值得一提的是，ChatGPT的参数量是BioMedGPT-10B的17倍以上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d30f3077e1525e73b86925d8c71d4809.png\" /></p><p></p><h3>MolFM/DrugFM&nbsp; 小分子药物基础模型&nbsp;</h3><p></p><p></p><p>本次与BioMedGPT-10B一同开源的还有小分子药物基础模型：MolFM/DrugFM。MolFM 由 AIR 聂再清教授团队研发，是首个能够统一表示分子结构、生物医学文献以及知识库的基础模型。MolFM 引入了跨模态注意力机制，将分子结构中的原子、分子实体的邻居以及与之相关的语义文本相连接。通过在特征空间中最小化同一分子的不同模态以及具有相似结构或功能的分子之间的距离，MolFM 能够捕获局部和全局的分子知识，从而促进跨模态的理解。MolFM的有效性已在各种下游任务中得到广泛验证，包括跨模态检索、分子描述、分子-文本生成和分子特性预测。MolFM: A Multimodal Molecular Foundation <a href=\"https://arxiv.org/abs/2307.09484\">https://arxiv.org/abs/2307.09484</a>\"</p><p></p><p>DrugFM由“清华AIR-智源联合研究中心”联合研发，AIR兰艳艳教授团队针对小分子药物的核心组织规律与数据表示进行了更前沿的探索与更精细设计，形成有效的小分子药物预训练模型 UniMAP。同时，将该小分子药物预训练模型与已有的多模态小分子药物基础大模型MolFM有机结合。模型在Cross-modal Retrieval 跨模态检索任务取得SOTA。DrugFM 作为小分子药物基础科研模型，将持续迭代，有效支撑和提升小分子药物筛选、设计、优化等相关下游任务。</p><p></p><p>原文链接：</p><p><a href=\"https://mp.weixin.qq.com/s/PVBA4AAcbCdHg_fXKA58uA\">https://mp.weixin.qq.com/s/PVBA4AAcbCdHg_fXKA58uA</a>\"</p>",
    "publish_time": "2023-08-21 14:59:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "网易资深系统开发工程师陈静力确认出席QCon北京，分享Curve 文件系统在 AI 场景下的实践",
    "url": "https://www.infoq.cn/article/VHaERVtvV6h7ho9GvEbf",
    "summary": "<p>9 月 3 日 - 5 日，在 <a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=0821&amp;utm_content=chenjingli\">QCon 全球软件开发大会（北京站）</a>\"上，网易资深系统开发工程师陈静力将发表题为《Curve 文件系统在 AI 场景下的实践》主题分享，介绍从 AI 的业务的不同场景出发， AI 业务对存储系统的需求，并介绍 Curve 文件系统采用了哪些技术来满足这些需求，以及在实践过程中遇到的问题与挑战。</p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/presentation/5397?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=0821&amp;utm_content=chenjingli\">陈静力</a>\"，参与网易 Curve 文件系统从 0 到 1 的开发，在 Curve 文件系统落地 AI 场景有一定的实践。所参与的 Curve 项目是云原生计算基金会 (CNCF) Sandbox 项目，是网易主导自研和开源的高性能、易运维、云原生的分布式存储系统，目前在网易内部已大规模应用。他在本次会议的演讲内容如下：</p><p></p><p>演讲：Curve 文件系统在 AI 场景下的实践</p><p></p><p>随着 AI 业务的迅速发展，产生的数据量越来越多，对存储的成本和性能提出了新的挑战。本次演讲将从 AI 的业务的不同场景出发（数据集 / 模型管理、在线编程 / 项目管理、数据预处理、训练和预测），说明 AI 业务对存储系统的需求，并介绍 Curve 文件系统采用了哪些技术来满足这些需求，以及在实践过程中遇到的问题与挑战。</p><p></p><p>演讲提纲：</p><p></p><p>背景</p><p></p><p>○ 介绍 AI 的业务特点（从不同场景出发讲对存储的需求）</p><p>○ 介绍 AI 的业务需求</p><p></p><p>Curve 实践</p><p></p><p>○ Curve 文件系统简介</p><p>○ Curve 文件系统为 AI 业务降本</p><p>○ Curve 文件系统为 AI 业务提效</p><p></p><p>总结与展望</p><p></p><p>○ 和 AI 框架的融合：做到自动预热</p><p>○ 大数据和 AI 的融合：提供 HDFS 接口，使用 Curve 文件系统即可以用在数据生产收集也可以用于后续处理和训练</p><p>○ 推动在更多业务场景的落地</p><p></p><p>你将获得：</p><p></p><p>○ 了解 AI 场景对存储的需求</p><p>○ 了解 Curve 文件系统在对接 AI 业务中遇到问题与挑战、解决方案与收益</p><p></p><p>除上述演讲外，QCon 北京还将围绕 <a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1570?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">云原生</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1567?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">AIGC&nbsp;浪潮下的研发效能提升</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1552?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">面向&nbsp;AI&nbsp;的存储</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"等进行分享。</p><p></p><p>110+ 名嘉宾、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-21 15:25:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "360数科首席算法科学家费浩峻，确认担任 FCon 金融行业大模型应用专题出品人",
    "url": "https://www.infoq.cn/article/wcVFP0UiVDohBaeywufF",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。360 数科首席算法科学家费浩峻将担任「<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"」的专题出品人。在此次专题中，你将了解到如何利用大数据、AI 和机器学习等技术构建庞大的模型，来处理金融数据并应用于决策和预测。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">费浩峻</a>\"，负责 360 数科数据生态、人工智能相关工作的研究和落地。深耕智能增长技术十余年，曾任百度金融 / 度小满主任架构师，腾讯专家研究员。长期投身于广告、检索和增长等应用研究领域，专注于大数据、人工智能研究方向，对技术如何驱动业务发展有着丰富的经验和独到的理解，拥有大数据、NLP、计算广告、信息处理等 专利十余项。本科毕业于华中师范大学，硕士毕业于北京大学。</p><p></p><p>2015 年加入百度之后，投身金融科技增长方向，从 0 到 1 建立了百度金融以人工智能为核心、大数据为基础的智能获客系统，支持了千万级的用户获取。作为增长行业的先驱者，以百度金融为载体，推动了互联网广告行业的发展，探索了大数据时代广告主与媒体的合作边界，进一步衍生出了 RTA 的合作模式，促进了行业双赢、行业资源效率的提升。</p><p></p><p>2021 年加入 360 数科，从数据安全、数据生态、大模型计算着手, 推动 360 数科的人工智能技术体系不断完善升级。在图计算、CV、知识图谱、因果模型、语音技术等方向深化技术研究对于金融科技的促动作用，积极探寻隐私计算在工业的落地场景，同时也带领人工智能团队打造了新一代的智能增长平台。</p><p></p><p>相信费浩峻的到来，可以帮助提升此专题的质量，让你学习到利用大数据、AI 和机器学习等技术，可以构建庞大的模型来处理金融数据并应用于决策和预测，同时可以提升运营效率、风险管理以及创新产品的开发。</p><p></p><p>除上述专题外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等专题进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：13269078023（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-08-21 15:50:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "开源打败闭源？Meta即将推出开源代码生成平台Code Llama，剑指OpenAI Codex",
    "url": "https://www.infoq.cn/article/SNdKvvVcJLNkVOGZll2h",
    "summary": "<p>据报道，Meta公司正准备发布新软件，能够帮助开发人员自动生成编码代码，将直接挑战OpenAI、谷歌等其他厂商提供的同类专有方案。</p><p>&nbsp;</p><p>据科技外媒The Information援引消息人士的说法，Meta的代码生成AI模型“Code Llama”为开源项目，最快可能在本周推出。</p><p>&nbsp;</p><p>这套新的编码模型将与OpenAI打造的Codex正面抗衡，并基于Meta的Llma 2软件——这是一种能够理解并生成会话文本的大语言模型。</p><p></p><h2>Code Llama到底是什么？</h2><p></p><p>&nbsp;</p><p>Llama 2是一种极具颠覆性的开源AI框架，能帮助企业轻松构建起自己的AI应用程序，且无需像使用OpenAI、谷歌或微软的专有软件那样支付费用。</p><p>&nbsp;</p><p>报道指出，Code Llama将帮助企业客户轻松构建起AI助理，在开发人员敲击键盘时自动提供代码建议，因此有望从基于Codex的微软GitHub Copilot等收费编码工具处夺取大量客户。</p><p>&nbsp;</p><p>Code Llama最引人注目的功能之一，就是它能够为AI助手提供支持。想象一下，这种由AI驱动的编码伴侣能够在开发者输入期间无缝提供代码片段建议，从而显著提高编码效率、减少错误并加快开发过程。</p><p>&nbsp;</p><p>Code Llama的卓越功能源自行业领先的AI算法。其核心模型由包含编程语言、编码模式和最佳实践的大规模数据集训练而成。自然语言处理（NLP）技术则让Code Llama有能力理解开发者的输入，并生成与上下文相匹配的代码建议。</p><p>&nbsp;</p><p>该模型的神经架构受到Meta Llama 2的启发，表现出对编码语法和语义的深刻理解。正是凭借自然语言处理与编码专业知识的深度整合，Code Llama才得以提供符合开发者意图的可行建议。</p><p>&nbsp;</p><p>此外，Code Llama的开源属性，意味着其与OpenAI和谷歌等技术巨头主导的传统专有软件有着截然不同的气质和定位。以开源方法为基础，Meta Platforms正推动先进AI代码生成工具的大众化普及。开发者将有机会免费使用Code Llama功能，借此开发出更多创新应用并简化编码工作流程。</p><p></p><h3>Code Llama功能剖析</h3><p></p><p>那么，Code Llama到底有哪些功能值得开发者关注？这主要包括四个方面：</p><p>&nbsp;</p><p>智能代码建议：Code Llama的AI能力体现在实时提供代码片段建议上。在开发者的输入过程中，AI助手会根据上下文提供准确建议，减少编码错误并加快开发速度。可定制的代码模板：编码工作中不存在百试百灵的“银弹”，Code Llama也深知这一点。开发者可以根据特定目标的具体要求创建并定制自己的代码模板，丰富且灵活的定制选项将帮助开发者充分运用AI辅助潜力，同时继续保持适当的代码风格。自然语言界面：Meta向来拥有出色的用户友好界面，这一特性也延伸到了Code Llama当中，允许开发者使用自然语言与AI助手进行交互。这种方法弥合了人与机器间的差距，带来直观且无缝的代码生成过程。协作编码：Code Llama能够支持的不只有“独行侠”，同时也能帮助团队中的每一名合作者。这款产品能够提供与项目目标相契合的代码片段建议，借此促进协作编码。这不仅能提高团队合作效率，同时也有助于加速开发并促进代码一致性。</p><p></p><h3>Code Llama发展路线图</h3><p></p><p>Meta 在创新方面做出的承诺，已经远超Code Llama项目的最初构想。该公司为这款软件制定了雄心勃勃的发展路线图，具体包括：</p><p>高级语言支持：Meta希望扩展Code Llama的语言支持能力，使开发者能够轻松使用自己熟悉的编程语言进行编码。增强上下文理解：Code Llama的后续迭代将侧重于深入理解开发者意图，据此提供更多契合上下文的相关建议。与开发平台相集成：让Code Llama与更广泛的编码平台相兼容已经成为目前的首要任务，相信其后续将能够与各类流行IDE和文本编辑器无缝对接。机器学习驱动的功能改进：Meta在AI研究方面的持续投入，意味着机器学习的发展将不断驱动并增强Code Llama的性能与准确性。</p><p>&nbsp;</p><p>科技行业正对此充满期待，业内专家们也开始分享自己对于Code Llama潜在颠覆性的判断。著名AI研究人员和软件开发者们正在研究这种开源AI模型将如何重塑编码实践，并对市场上的每一位参与者造成深远影响。</p><p>&nbsp;</p><p>AI伦理倡导者Emily Chen博士强调，“Code Llama的开源方法符合包容性与协作原则。它有望为具备不同背景和技能水平的开发者提供支持，从而建立起更加多样、更具创造性的编码生态系统。”</p><p>&nbsp;</p><p>但也不乏怀疑的声音，软件架构师Mark Thompson认为“虽然Code Llama的开源性质颇具吸引力，但真正的试金石仍然着落在性能和适应性方面。开发者们需要评估其与各类编码环境无缝集成的能力，看它能否提供准确且符合上下文的代码建议。”</p><p></p><h3>巨头间的对抗：Code Llama与Codex</h3><p></p><p>&nbsp;</p><p>Code Llama和Codex之间的对抗，也将重塑整个编码行业的格局。OpenAI的Codex目前在市场上享有显著的领先地位，已经在为GitHub Copilot提供支持，并以令人印象深刻的代码建议效果赢得了开发者们的青睐。然而，Code Llama的开源特性对于需要更好可及性/可定制性解决方案的开发者来说将更具吸引力。而随着两大科技巨头的正面角力，编码社区也将从竞争当中享受创新成果、获取积极收益。</p><p>&nbsp;</p><p>随着Code Llama即将踏上全球舞台，开发者和行业利益相关方也站在了发展的十字路口上。拥有这种新的开源代码生成范式需要积极的适应心态和探索精神。传统上依赖专有解决方案的企业可能需要根据Code Llama的潜在影响重新评估现有战略。而谁能引领这波开发转型，谁就能开辟出前所未有的创新与协作路径。</p><p></p><h2>Meta在走一条与OpenAI不一样的路</h2><p></p><p>Meta进军代码生成领域，也是对打通人际关系、促进彼此协作这一核心使命的延伸。通过发布开源解决方案Code Llama，Meta Platforms将自身定位为编码社区和顶尖AI技术之间的桥梁。此举有望创造更公平的竞争环境，使来自不同背景的开发者都能用上AI驱动的代码辅助工具，且无需承担高昂的使用成本。</p><p>&nbsp;</p><p>即将发布的Code Llama也已经在科技行业内引起广泛讨论。开源AI代码生成模型也代表着一股新的力量，能够以多种方式促进创新：</p><p>&nbsp;</p><p>竞争压力：面对Code Llama加入战团，AI代码生成市场的老牌企业或将面临更大的竞争压力、被迫加速创新周期。混合解决方案的兴起：开发人员可以尝试将Code Llama的开源优势同当前专有工具相结合，构建起混合解决方案以优化其编码工作流程。创业机会：Code Llama强大的可及性将帮助更多初创企业接触到AI驱动型编码工具，从而培育出更趋多样化的创新生态系统。</p><p>&nbsp;</p><p>Code Llama的发布并不是个独立事件，它代表着我们对于AI驱动型开发工具的未来想象。软件发展的特点就在于不断创新，而Code Llama明显朝着更好的可及性、开发效率和智能水平迈出了重要一步。随着开发者逐渐接纳这项技术并为其开源发展做出贡献，编码体验将迎来近乎无限的成长和增强空间。</p><p>&nbsp;</p><p>总之，Meta Platforms即将推出的Code Llama代表着编码领域正在迎接又一轮巨变。这款开源AI模型将推动代码生成的大众化普及，为开发人员提供支持，并以前所未有的规模推动创新探索。Code Llama来临的脚步一刻未曾停歇，技术行业也共同期待着它所带来的变革与可能性。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://medium.datadriveninvestor.com/metas-next-ai-attack-on-openai-free-code-generating-software-e15c84521e83\">https://medium.datadriveninvestor.com/metas-next-ai-attack-on-openai-free-code-generating-software-e15c84521e83</a>\"</p><p><a href=\"https://telecom.economictimes.indiatimes.com/news/internet/meta-working-on-code-generating-ai-tool-code-llama-report/102841091\">https://telecom.economictimes.indiatimes.com/news/internet/meta-working-on-code-generating-ai-tool-code-llama-report/102841091</a>\"</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-08-21 16:26:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "生成未来：用AI原生重塑新世界 | Baidu World 2023",
    "url": "https://www.infoq.cn/article/69VtmUx2G0lJ7OwcPEIO",
    "summary": "<p>不再是你以为的App或者PC软件，站在AI时代的当下，应用的开发必须走上一条“AI原生”的道路了。</p><p>&nbsp;</p><p>在早期的AI模型研发过程中，无论是判别式的、生成式的，还是是规则的，对于每一个任务都需要训练专门的模型或者写规则，以此才能够满足应用的需求；而且模型之间的能力还无法做到共享。</p><p>&nbsp;</p><p>随着预训练机制和大模型等技术的发展，AI研发模式产生了变迁的趋势。以BERT为例，它的出现让AI研发模式变成一种“两阶段学习”的过程：第一阶段是通过自监督学习等，训练具备基本理解和生成能力的大模型；第二阶段主要是针对不同任务进行监督精调，训练特定任务的模型。</p><p>&nbsp;</p><p>这种模式虽然能够让不同模型之间可以共享基本的能力，但对于根据用户反馈优化底座模型这事上还是无能为力。</p><p>&nbsp;</p><p>而当ChatGPT引爆通用大模型的热潮，AI研发模式又有了新范式：像人类一样构建“先天”的学习机制和“语言大脑皮层”机制，通过“后天学习”，在环境中获得反馈，构建理解、生成和推理等能力。</p><p>&nbsp;</p><p>一言蔽之，只要你思路活跃、表达清晰，那么机器就能按照你的想法、以纯自然语言交互的方式去干活，这便是AI原生的应用。</p><p>&nbsp;</p><p>那么该如何玩转AI原生应用？又如何通过AI原生的思维和大模型技术重构行业？一切的答案将在10月17日主题为《生成未来》（PROMPT THE WORLD）的百度世界2023大会中揭晓。</p><p>&nbsp;</p><p>这将是近四年来百度世界大会首次恢复线下形式，据悉，百度创始人兼董事长、CEO李彦宏领衔发布多款全新“AI原生应用”。</p><p>&nbsp;</p><p>在百度内部会议中，李彦宏表示要基于“AI原生思维”开发“AI原生应用”，百度积累超过10年，才能跑的最快，理应做出最好的AI原生应用。他还给百度全体员工定下了小目标：百度所有的产品要重构、重做，所有员工要有意识培养AI原生思维，未来将有50%的工作靠Prompt完成；每一个百度人，都要克服固化思维，用发现和探索的眼光看世界。</p><p>&nbsp;</p><p>不仅如此，李彦宏在此次内部会议中着重强调了提示词的重要性，他认为未来很可能不需要那么多的程序员，大模型很多时候都可以自行生成代码，而决定结果好坏的是看谁能把提示词写得好。</p><p>&nbsp;</p><p>不过百度想要将AI研发和应用做到如此革新并不是天马行空地想象，而正是基于自身超过10年的技术积累。无论是在底层上的芯片层（昆仑芯）、框架层的飞桨、模型层上的文心一言，百度已然处于领先地位。</p><p>&nbsp;</p><p>而在最顶层的应用层方面，百度其实在AI原生上已初见端倪。例如基于文心大模型创新打造“AI伙伴”、“AI BOT”等功能目前正在内测中；百度智能工作平台“如流”中的超级助手，将企业知识和工作流任务全面整合，帮助员工直接获取知识，一步完成工作流操作，并可在任何工作场景、随时随地被唤起。</p><p>&nbsp;</p><p>再如百度智能云“Comate”代码助手，借助文心大模型的理解、推理能力，可实现代码的快速补齐、自然语言推荐代码、自动查找代码错误，全面提升开发者研发效率。在百度内部研发中使用，AI生成代码的采纳率达到了50%。</p><p>&nbsp;</p><p>因此，这一届百度世界大会注定将是一场意义非凡的AI盛宴：锁定未来，尽在《生成未来》。</p>",
    "publish_time": "2023-08-21 19:29:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]