[
  {
    "title": "AI 平民化趋势下，数据架构将被彻底颠覆？",
    "url": "https://www.infoq.cn/article/MridAJdKFPmp4QSPCvfU",
    "summary": "<p>深度学习诞生 10 年，LLM 终于带来 AI 平民化。基于海量数据构建的大模型能够进行相对独立的推理和判断，这让企业看到了 AI 与 Data 的技术融合已经成为当下重要的发展趋势之一。如何让 AI 在各行各业中真正落地，切实提高工作效率已经成为时代新课题。</p>\n<ul>\n<li>那么，大数据与 AI 一体化融合真正实现了吗？</li>\n<li>AI for Data 如今在企业生产中发挥着怎样的价值？</li>\n<li>AI 是否将彻底颠覆现有数据架构？</li>\n</ul>\n<p>《再谈数据架构》第二期，我们重点探讨了 AI 与大数据的融合、落地。</p>",
    "publish_time": "2023-07-03 09:56:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI、数字孪生、数字化精益在工业制造场景如何落地｜直播倒计时2天",
    "url": "https://www.infoq.cn/article/SvZM9Rdcm8PG0MZz1vWI",
    "summary": "<p>AI的热度在近半年来持续飙升，虽然相关技术与制造的融合由来已久，但由于工业制造的行业知识门槛较高，加上数据量级和数据质量的局限性等等，<a href=\"https://www.infoq.cn/theme/187\">AI</a>\"在工业场景的落地及规模化扩散还存在巨大挑战，这些障碍如何一一破除？</p><p></p><p>因元宇宙热度再次被点燃的数字孪生，事实上早已在工业制造领域拥有大量实践。在此基础上，工业软件巨头<a href=\"https://www.3ds.com/zh-hans/?utm_campaign=202306_chi_3dxp_dassaultsystemes_zh_CMP2164_gilt&amp;utm_medium=cpc&amp;utm_source=baidu&amp;utm_content=search&amp;utm_term=site-dassault-brand-dassault-dasuoxitong-pc\">达索系统</a>\"提出的“虚拟孪生”概念与“<a href=\"https://xie.infoq.cn/article/40aa1c9d6b8af73d71a789c37\">数字孪生</a>\"”技术有何异同？又如何覆盖产品全生命周期创造价值？</p><p></p><p>质量、效率、成本三要素之间能否平衡？诞生于上世纪50年代的<a href=\"https://www.infoq.cn/article/8eOkku2uTuU3qVFvwqhA\">精益</a>\"思想如何与数字化充分融合，帮助企业进一步优化各个制造关键环节的工作流程和运营模式？</p><p></p><p>7&nbsp;月&nbsp;5&nbsp;日（周三），最新一期《超级连麦.数智大脑》，邀请了来自艾聚达、达索系统、青岛中集的资深专家，将带你一起揭秘头部面板制造商友达光电的智能化实践与AI扩散策略，解读产品全生命周期的虚拟孪生应用案例，以及全球最大规模冷藏箱生产基地的数字化精益实践。</p><p></p><p>点击链接或扫描下方二维码即可预约报名：<a href=\"https://live.infoq.cn/room/1803\">https://live.infoq.cn/room/1803</a>\"</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6a/77/6aaab576d75ff0db94228c6e4d8da977.jpg\" /></p><p></p><p></p><h2>直播时间</h2><p></p><p>7&nbsp;月&nbsp;5&nbsp;日（周三）&nbsp;19:30-22:00</p><p></p><h2>联合出品</h2><p></p><p>InfoQ&nbsp;x&nbsp;TGO鲲鹏会&nbsp;x&nbsp;极客时间企业版</p><p></p><h2>直播议程</h2><p></p><p></p><h4>19:30-20:00&nbsp;&nbsp;主题分享-AI驱动的制造变革：揭密成功案例与关键策略</h4><p></p><p></p><p>嘉宾介绍：赖骏凯，艾聚达总经理。友达昆山智能制造负责人，大数据与人工智能专家，省级两化融合特聘讲师，重庆邮电大学先进制造工程学院客座教授。</p><p></p><p>分享大纲：</p><p>1.&nbsp;变革背景与价值创造</p><p>2.&nbsp;AI&nbsp;应用成功案例</p><p>3.&nbsp;AI&nbsp;扩散关键策略</p><p>4.&nbsp;总结-提升新型竞争力</p><p></p><h4>20:00-20:30&nbsp;主题分享-虚拟孪生如何助力制造业高质量发展</h4><p></p><p></p><p>嘉宾介绍：杨宠，达索系统技术咨询部业务咨询高级经理。华中科技大学工学博士。曾供职于盖德信息技术有限公司&nbsp;、国际商业机器有限公司（IBM）&nbsp;、参数技术软件有限公司（PTC）&nbsp;，为多家制造业企业提供过业务咨询和数字化咨询服务。</p><p></p><p>分享大纲：</p><p>1.&nbsp;虚拟孪生的概念</p><p>2.&nbsp;产品全生命周期的虚拟孪生</p><p>3.&nbsp;虚拟孪生应用案例及未来展望</p><p></p><p></p><h4>20:30-21:00&nbsp;主题分享-数字化精益实践</h4><p></p><p></p><p>嘉宾介绍：耿峰，青岛中集冷藏箱CIO。坚守制造业信息化一线工作十余年，主持实施过公司ERP、CRM、PLM、SRM、HR、服务器虚拟化、一卡通、条形码、数据防泄漏、桌面安全、私有云、企业内控体系、数字化工厂等项目。几乎涵盖制造业信息化的全部范畴。是国内较早的实战派云计算专家，第一批将虚拟化技术引进企业的CIO，近些年专注数字化精益制造工厂的探索于实战。</p><p></p><p>分享大纲：</p><p>精益数字化背景精益推行的业务痛点数字化精益的价值数字化精益企业建设保障</p><p></p><h4>21:00-21:40&nbsp;连麦对话</h4><p></p><p></p><h4>工业“智”造转型，如何评估投入产出比</h4><p></p><p>主持人：高玉娴&nbsp;InfoQ极客传媒数字化主编</p><p>对话嘉宾：赖骏凯&nbsp;艾聚达总经理/&nbsp;杨宠&nbsp;达索系统技术咨询部业务咨询高级经理/&nbsp;耿峰&nbsp;青岛中集冷藏箱CIO</p><p></p><p>话题方向：制造企业具体如何在转型制造过程中让技术的价值最大化，又如何平衡好效率、质量和成本的关系呢？本议题将与各位专家连麦，探讨工业“智”造转型过程中的经济效益问题。</p><p></p><p>&gt;&gt;&gt;直播报名通道：<a href=\"https://live.infoq.cn/room/1803\">https://live.infoq.cn/room/1803</a>\"</p><p></p><h4>关于《超级连麦.&nbsp;数智大脑》</h4><p></p><p><a href=\"https://www.infoq.cn/theme/192\">超级连麦.数智大脑</a>\"是&nbsp;InfoQ&nbsp;重磅推出的一档连麦直播栏目，由数字化领域的思想领袖、技术大咖及企业先行者等多方连线，聚焦企业数字化实践、数字人才培养、数字化技术管理等话题，剖析和拆解典型数字化场景及其痛点，意在帮助各行业企业扫清转型障碍、探寻变革路径。</p><p></p><p>每期直播将邀请&nbsp;2-3&nbsp;位嘉宾围绕同一话题，从不同维度展开深度讨论。我们希望能够深入行业最核心的话题，通过充分的现场碰撞和探讨，对关键问题进行深刻的剖析，既有理论也有实践，既有战略也有战术，为观众呈现更为全方位、多视角，更有代入感、更具启发性的数字化转型参考。</p><p></p><p>欢迎添加小助手加入「数字化读者群」，获取直播回放链接及相关资料。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d360329131fc348202c171aa0e8213a9.jpeg\" /></p><p></p>",
    "publish_time": "2023-07-03 11:05:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Maven揭秘，逃离依赖地狱",
    "url": "https://www.infoq.cn/article/J8FikQMqTwgRE1a73LLv",
    "summary": "<p>在<a href=\"https://www.devoxx.co.uk/talk/?id=1997\">英国 Devoxx 的演讲</a>\"中，JFrog 的开发倡导者&nbsp;<a href=\"http://www.linkedin.com/in/ixchelruiz\">Ixchel Ruiz</a>\"&nbsp;和 Oracle 首席产品经理&nbsp;<a href=\"https://www.linkedin.com/in/aalmiray/\">Andres Almiray</a>\"&nbsp;共同介绍了多个“Maven 难题”，以及摆脱<a href=\"https://maven.apache.org/\">阿帕奇 Maven</a>\"&nbsp;“依赖地狱”的可能解决方案。这次演讲中涉及了直接、传递、父级&nbsp;<a href=\"https://maven.apache.org/pom.html\">POM</a>\"，以及<a href=\"https://en.wikipedia.org/wiki/Bill_of_materials\">物料清单（BOM）</a>\"的导入。</p><p></p><p>Ruiz 以对工具价值的思考展开了演讲：</p><p></p><p></p><blockquote>Ruiz：作为开发者，如果我的工具在做它该做的，我会感觉很好。</blockquote><p></p><p></p><p>在清楚了解“好工具”时，“魔法”就会发生。正如标题所述，这次演讲主题时关于构建工具，更确切地说，是关于阿帕奇 Maven 的构建工具。</p><p></p><p>据&nbsp;<a href=\"https://jfrog.com/artifactory/\">JFrog 制品仓库（Artifactory）</a>\"的统计数据和&nbsp;<a href=\"https://www.jrebel.com/resources/java-developer-productivity-report-2022\">JRebel</a>\"&nbsp;或&nbsp;<a href=\"https://www.jetbrains.com/lp/devecosystem-2022/\">JetBrains</a>\"&nbsp;的开发者生产力调查，Maven 仍是主流 Java 开发者的构建工具，市场份额分别占据68%和73%。</p><p></p><p>作为&nbsp;<a href=\"https://gradle.org/\">Gradle</a>\"&nbsp;的长期支持者，Almiray 称即使是 Gradle 或&nbsp;<a href=\"https://www.scala-sbt.org/\">sbt</a>\"&nbsp;也或多或少地依赖 Maven。因此，即使是想要以自己的方式编码，人们仍然需要通过 POM 格式解决依赖关系。而有时这些东西又不会如人预期一般正常工作，或是说像是经典的 Maven 一样运作。</p><p></p><p>在一个新克隆的 maven 项目上，你首先会做什么？演讲者以这个问题为切入，开始了对问题的回答。<a href=\"https://github.com/aalmiray/mvn-clean-install\">Almiray 鼓励</a>\"观众在除项目依赖于其他远程项目的情况下，<a href=\"https://andresalmiray.com/maven-verify-or-clean-install/\">将本能的“mvn clean install”替换为“|mvn verify”</a>\"，因为“检查（verify）”是&nbsp;<a href=\"https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html\">Maven 生命周期中</a>\"“安装（install）”的上一步，是通过构建并运行测试进行项目验证的。而安装则仅仅是将构建（编译和打包）结果从文件系统（构建的位置）复制到仓库。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1warmup-questions-1685857753292.jpg\" /></p><p></p><p>随后，演讲进入了互动问卷环节，由听众对问卷内容进行实时回复。在问题的描述中，如果依赖的坐标（即 groupId、artificatId）相同但版本号不同，那么麻烦就会出现。</p><p></p><p>主持人在“暖场问题”中确定了听众所使用的 Maven 版本、安装方式，以及是否使用&nbsp;<a href=\"https://github.com/apache/maven-mvnd\">Maven 守护进程</a>\"（daemon）。针对听众的选择，二位演讲者建议使用&nbsp;<a href=\"https://sdkman.io/\">SDKMAN</a>\"（即使是在两个不同终端窗口内，也允许使用两套不同版本的 Java），用于提升速度的 Maven 守护进程，以及新版本&nbsp;<a href=\"https://maven.apache.org/docs/3.9.2/release-notes.html\">Maven 3.9.x</a>\"&nbsp;以进入更为颠覆性的&nbsp;<a href=\"https://maven.apache.org/ref/4.0.0-alpha-5/\">4.x 版本</a>\"。</p><p></p><p>在演讲的问答阶段，二位演讲者以<a href=\"https://github.com/google/guava\">谷歌 Guava 依赖</a>\"为例，但理由不是因为“大家都恨 Guava，而是因为大家都用 Guava”。他们提出了多个场景、问题、解答，以及优化和解决方案的相关建议，并将场景分为了三部分：单 POM 文件的依赖性、对父 POM 文件的依赖性，以及 BOM 导入。</p><p></p><h2>单 POM 文件的依赖管理</h2><p></p><p></p><p>第一种情况，连续声明了两个不同版本 Guava 的简单项目，哪一个会被采用？听众的回答几乎是在两个版本和构建错误之间五五开。虽然这些看起来仅仅是个简单的规则应用，但由于目前存在的众多版本和插件组合，事情往往会变得更加复杂。比如，这种情况在 Maven 4.x 中会出现构建错误，但在 3.x 中仅会以警告的形似出现。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1Screenshot%202023-06-05%20at%2008.17.07-1685943669814.png\" /></p><p></p><p>这个问题的答案是，在 28.0 版本的 Guava 中会被解决，但因为该版本号不够高，Maven 永远会采用最后一个声明的依赖。在加上 Maven 无法理解版本号，这些在它眼里仅仅只是字符串。为确保这些情况不会再发生，Ruiz 和 Almiray 建议在 Maven 4.x 版本普及之前采用&nbsp;<a href=\"https://maven.apache.org/enforcer/enforcer-rules/banDuplicatePomDependencyVersions.html\">Maven 增强（enforcer）插件中的“禁止重复 POM 版本号规则”</a>\"。首次使用这个插件时大概会非常痛苦，因为该规则会生成一个构建错误，迫使你为项目选择合适的版本。</p><p></p><p>下一个问题是对上一个问题的改版，将第二个依赖换为了更高版本的传递性依赖。即使第二个也是 POM 文件里最后声明的依赖版本号更高，直接的“依赖版本总会赢”。Almiray 提及阿帕奇 Maven 的前主席 Robert Scholte 曾强调，Maven 这项工具<a href=\"https://twitter.com/rfscholte/status/1243226856914063360?s=12&amp;t=p1k9i_gjO2cP_qDbAQnCVw\">无法理解语义上的版本号划分</a>\"，它只认得依赖在“图中的位置”。此外，依赖图中同一依赖的不同版本可能会导致应用程序时不时的崩溃。为避免这类情况的发生，<a href=\"https://maven.apache.org/enforcer/enforcer-rules/dependencyConvergence.html\">可使用 Maven 增强插件中的“依赖收敛规则”</a>\"，以确保版本号的一致。如果需要强调版本号在语义上的一致性，可使用<a href=\"https://maven.apache.org/enforcer/enforcer-rules/requireUpperBoundDeps.html\">增强规则“需求依赖项上界”</a>\"，该规则可给出依赖图中的可用新版本。将两项规则相结合后，就能得知同一依赖的两个版本，以及依赖的可用新版本。</p><p></p><p>第二种情况则引入了依赖管理的概念，其原理更像是查找表。在 Maven 构建图时，会在表中搜索匹配的依赖（artifactId 及 groupId），并选择其所定义的版本。第一个例子中只有依赖管理块和通过<a href=\"https://github.com/google/truth\">谷歌 Truth</a>\"&nbsp;获取的横向依赖，而第二个例子中则额外增加了直接依赖的内容。在例子一中，定义在依赖管理块中定义的版本号会“赢”，而例子二中则是直接依赖“赢”，也就是说“无论是在图中哪里定义的，直接依赖总会赢”。</p><p>另一个例子中则使用了两个依赖，二者均带来了与根距离相同的传递性依赖（“这点很重要”），而再将依赖管理块加进来又会让情况有所不同。听众们认为可以将直接依赖的情况套用，所以图中最后定义的依赖会“赢”，但传递性依赖的情况却是恰恰相反，即第一个库中、第一且最近的传递性依赖将“赢”。在这种情况下，由于这两个依赖距离相同，所以 Guice 带来的横向依赖（Guava 30.1）会赢。在依赖块加入后，其所定义的版本号将“赢”。</p><p></p><h2>父 POM 文件依赖</h2><p></p><p></p><p>在这段演讲中，二位演讲者又在依赖管理中加入了新的复杂因素：父 POM。每个 POM 都可以有一个父 POM，并为依赖管理带来不同的背景。父级关系是在子级层面定义的，父级不会知道任何继承自己子级的信息。每个 POM 都会有一个父级，如果没有明确定义，那么父级将默认成为 super POM。而 Maven 之所以能在只有基本插件的情况下构建项目，是因为 super POM 中包含了所有需要插件。</p><p></p><p></p><blockquote>Almiray：单个 POM 文件很好处理，但难免会出现多个 POM 文件的情况。Ruiz：当然，这时候事情就好玩了。</blockquote><p></p><p></p><p>在另一个例子中，情况一是父系中定义一个直接依赖，子系中定义一个传递依赖；情况二中则新增了一个依赖管理块。因为父 POM 是直接导入到子 POM 中，我们可以随时回到前一阶段，即有直接依赖、传递依赖，以及依赖块的单 POM，而正如所料，“直接依赖总会赢”。唯一的不同则是生效的 POM 同时也会导入父 POM 中的内容。因此，依赖是否能被解决取决于其在生效 POM 中的位置。</p><p></p><h2>BOM 依赖</h2><p></p><p></p><p>演讲中使用的最后一个概念是物料清单（BOM），是与别称“安全物什”的软件物料清单（SBOM）不同的。虽然没有对 BOM 的官方定义或分类，但据 Almiray 的说法，BOM 可以分为以下两类：</p><p>库 BOM：定义了项目与单一库的关联。举例来说，JUnit 或 Jackson BOM 定义且仅定义了一切与 JUnit 相关。堆栈 BOM： Spring 或 Quarkus BOM 可当前项目提供其运行所需的各个项目中的全部依赖关系，每个 DOM 中都包含有最合适运行的依赖版本组合。</p><p></p><p></p><blockquote>Ruiz：就算是没用过，也一定消费过（BOM）。如果用 Spring Boot 或 Quarkus 导入东西，必然会带来更多的依赖……</blockquote><p></p><p></p><p>无论你用的是哪类 BOM 最终的消费形式都是一样的：通过依赖管理块（Ruiz 称这是依赖管理块存在的意义）。消费 BOM 不仅需要 artifactID 和 groupID，还需要添加 POM 类型，否则 Maven 只会试图将其解析为 JAR 文件。POM 文件之中只有元数据。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1Screenshot%202023-06-09%20at%2008.54.00-1686290942713.png\" /></p><p></p><p>下面这个问题是前一个的变体。其中包含一个父级 POM 一个子级 POM，生效的 POM 有两个过渡性依赖，父级中的依赖管理模块有一个依赖和一个导入 BOM 的依赖（如上图所示）。另一个问题中则又在其中加入了直接依赖（无图）。生效的 POM 文件将引用父级中定义的依赖块，而 Guava 所选择的版本也是在这个依赖块中定义的。因此，第一个问题中生效的会是 28.0-jre。但对第二个问题而言，演讲者们给自己不断强调的“直接依赖总会赢”的说法打上了补丁，“……除非 BOM 文件是导入的，否则其他都会被忽略”。也就是说，情况二中依赖管理块中使用的版本是 26.0-jre。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1Screenshot%202023-06-09%20at%2012.38.27-1686304304944.png\" /></p><p></p><p>另一种情况中（如上图），父级和子级 POM 中都有定义依赖管理模块。子级的 POM 中包含导入的 BOM 和依赖。在这种情况下，子级定义的版本与所用的版本更接近，因此会直接覆盖父级中所定义的依赖版本。也就是说，子级 POM 的依赖块中定义的依赖会被使用，即 26.0-jre。如果父级中有一个依赖块，子级中的依赖块导入了一个 BOM、一个传递性依赖和直接依赖，那么直接依赖将会笑到最后，即 28.0-jre。</p><p></p><p>在演讲的最后，Ruiz 和 Almiray 给出了演讲中的关键点。她提到了依赖的采用在很大程度上取决于其在 POM 文件内定义的位置，并再次强调了使用 Maven 增强插件的重要性，以确保明确规则的定义和遵循。因此，在依赖定义文件中的不同位置新增一个条目并不会改变构建过程的输出。</p><p></p><p></p><blockquote>Almiray：如果要从这次演讲中学到什么，那就是去使用 Maven 增强插件，并从今天开始，正确地开始项目构建……</blockquote><p></p><p></p><p>Ruiz 在结束语中再次重申了 Maven 依赖的解决规律：直接依赖总会赢，传递性依赖会选择最先也是最近的。依赖管理解析会以目录的形式使用。对 BOM 文件而言，就算不知道也在用。Almiray 最后称，万不得已可以用排除法，只要用 Maven 构建，依赖管理块就是好工具。但如果用不同的消费者，你就需要将所有不同类型的依赖块转换为显示依赖。Maven flatten 可以做到这点。对 Gradle 而言则更是需要，因为 Gradle 除了直接依赖关系之外，不支持任何其他东西。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/\">Ruiz and Almiray at Devoxx UK: Lessons on How to Escape the Maven Dependency Hell</a>\"</p>",
    "publish_time": "2023-07-03 11:20:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何在英特尔® 平台上实现高效的大语言模型训练后量化",
    "url": "https://www.infoq.cn/article/GwojGAdYA5rfpzQpDuck",
    "summary": "<p>本文介绍了可提升大语言模型的训练后量化表现的增强型SmoothQuant技术，说明了这项技术的用法，并证明了其在准确率方面的优势。此方法已整合至<a href=\"https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/neural-compressor.html\">英特尔® Neural Compressor</a>\"(1)&nbsp;中。英特尔® Neural Compressor是一个包含量化、剪枝（稀疏性）、蒸馏（知识提炼）和神经架构搜索等多种常用模型压缩技术的开源Python库。目前，诸如TensorFlow、<a href=\"https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-tensorflow.html\">英特尔® Extension for TensorFlow</a>\"(2)、PyTorch、<a href=\"https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-pytorch.html\">英特尔® Extension for PyTorch</a>\"(3)&nbsp;、ONNX Runtime和MXNet等主流框架，都能与之兼容。</p><p>&nbsp;</p><p>英特尔® Neural Compressor 已经支持多款英特尔®架构的硬件，比如<a href=\"https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/scalable.html\">英特尔®至强®可扩展处理器</a>\"(4)、<a href=\"https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/max-series.html\">英特尔®至强® CPU Max系列</a>\"(5)、<a href=\"https://www.intel.cn/content/www/cn/zh/products/details/discrete-gpus/data-center-gpu/flex-series.html\">英特尔®数据中心GPU Flex系列</a>\"(6)和<a href=\"https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html\">英特尔®数据中心GPU Max系列</a>\"(7)。本文涉及的实验基于<a href=\"https://www.intel.cn/content/www/cn/zh/events/accelerate-with-xeon.html\">第四代英特®至强®可扩展处理器</a>\"(8)进行。</p><p></p><h1>大语言模型</h1><p></p><p></p><p>大语言模型(Large Language Model, LLM)需基于海量数据集进行训练，可能拥有数十亿权重参数。其先进的网络结构和庞大的参数量，使它们能够很好地应对自然语言本身的复杂性。完成训练后的大语言模型，可针对各种下游的自然语言处理(NLP)和自然语言生成(NLG)任务进行调优，让其更适合对话式聊天机器人（如ChatGPT）、机器翻译、文本分类、欺诈检测和情感分析等任务场景。</p><p></p><h1>大语言模型部署面临的挑战</h1><p></p><p></p><p>大语言模型在执行自然语言处理和自然语言生成任务方面表现出色，但其训练和部署颇为复杂，主要面临以下挑战：</p><p>&nbsp;</p><p>·&nbsp;<a href=\"https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8\">AI与内存墙</a>\"(9)瓶颈问题：算力每两年提高3.1倍，内存带宽却只提高1.4倍；</p><p>·&nbsp;网络带宽挑战：训练大语言模型需要采用分布式系统，这对网络带宽提出了较高要求；</p><p>·&nbsp;系统资源有限：训练后的模型往往会部署在算力和内存资源均有限的系统上。</p><p>&nbsp;</p><p>因此，采用训练后量化的方法来为大语言模型瘦身，对于实现低时延推理至关重要。</p><p></p><h1>大语言模型的量化</h1><p></p><p></p><p>量化是一种常见的压缩操作，可以减少模型占用的内存空间，提高推理性能。采用量化方法可以降低大语言模型部署的难度。具体来说，量化是将浮点矩阵转换为整数矩阵：</p><p><img src=\"https://static001.geekbang.org/infoq/08/08ad944f942d6c96e14bc700c128d6e6.png\" /></p><p>其中X_fp32、S和Z分别为输入矩阵、比例因子和整数零点。</p><p>&nbsp;</p><p>有关每通道(per-channel)量化策略虽然可能会减少量化损失，但不能用于激活值量化的原因，请参看<a href=\"https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md\">SmoothQuant相关文档</a>\"(10)。不过，激活值量化误差损失却是导致模型量化准确率下降的重要因素。为此，人们提出了很多方法来降低激活值量化损失，例如：<a href=\"https://arxiv.org/abs/2203.14642\">SPIQ</a>\"(11)、<a href=\"https://arxiv.org/abs/2209.13325\">Outlier Suppression</a>\"(12)和<a href=\"https://arxiv.org/abs/2211.10438\">SmoothQuant</a>\"(13)。这三种方法思路相似，即把激活值量化的难度转移到权重量化上，只是三者在转移难度的多少上有所不同。</p><p>&nbsp;</p><p>增强型SmoothQuant</p><p></p><p>SmoothQuant引入了一个超参数α作为平滑因子来计算每个通道的量化比例因子，并平衡激活值和权重的量化难度。</p><p><img src=\"https://static001.geekbang.org/infoq/d3/d38b57f1c9191093064f77f2bd652aa7.png\" /></p><p></p><p>其中j是输入通道索引。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/060c16a61d87a506b5e136120ec1c7b6.png\" /></p><p>&nbsp;</p><p>对于OPT和BLOOM等大多数模型来说，α=0.5是一个能够较好实现权重和激活值量化难度分割的平衡值。模型的激活异常值越大，就越需要使用更大的α值来将更多的量化难度转移到权重上。</p><p>&nbsp;</p><p>原始的SmoothQuant旨在通过针对整个模型使用一个固定值α来分割权重和激活值的量化难度。然而，由于激活异常值的分布不仅在不同模型之间存在差异，而且在同一模型的不同层之间也不尽相同，因此，本文推荐使用英特尔® Neural Compressor的自动调优能力，逐层获取最佳α值。</p><p>&nbsp;</p><p>相关方法包括以下五个主要步骤（伪代码如下所示）：</p><p>1.&nbsp;通过特殊的回调函数register_forward_hook捕获(hook)模型各层的输入和输出值。</p><p>2.&nbsp;根据用户定义的α范围和步长生成一个α值列表。</p><p>3.&nbsp;根据给定的α值重新计算平滑因子并调整参数（权重值和激活值）。</p><p>4.&nbsp;对权重执行每通道量化与反量化(quantization_dequantization)，对输入值执行每张量(per-tensor)量化与反量化，以预测与给定α值对应的每层输出值。</p><p>5.&nbsp;计算相对实际输出值的均方损失，将调整后的参数恢复回来，并保存每层的最佳α值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6e09bc08f2d0b75da0490760578ffdc.png\" /></p><p></p><p>本文提出的方法支持用多个标准（如最小值、最大值和平均值）来确定Transformer块的输入层归一化(LayerNorm)操作的α值。实验发现，将α范围设为[0.3, 0.7]，步长设为0.05，对大多数模型来说都能达到很好的平衡。</p><p>&nbsp;</p><p>这一方法有两个显著特点：一是全自动化，二是比原始方法支持的融合模式多。</p><p>&nbsp;</p><p>下图提供了在BLOOM-1b7模型上执行SmoothQuant α值自动调优的样例代码：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/703cb16fe2a013b61e1e7d34c7bc681b.png\" /></p><p>启用增强型SmoothQuant的样例代码</p><p>&nbsp;</p><p>用户只需传递一个模型名称(model_name)和一个数据加载器。值得注意的是，模型分析主要依靠的是Torch JIT。用户可以在加载<a href=\"https://huggingface.co/models\">Hugging Face模型</a>\"(14)时将torchscript设置为True，或将return_dict设置为False。更多信息请参阅<a href=\"https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md\">英特尔® Neural Compressor文档</a>\"(10)。</p><p></p><h1>结果</h1><p></p><p></p><p>本文提出的增强型SmoothQuant的主要优势在于提高了准确率。</p><p>&nbsp;</p><p>经过对多种主流大语言模型的评估，具备自动调优能力的INT8 SmoothQuant最后一个词元(last-token)的预测准确率要高于原始INT8 SmoothQuant和FP32基线方法。详见下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/835dd9d13030d4f78281ebeca6426cdc.png\" /></p><p></p><p>FP32基线方法、INT8（启用和不启用SmoothQuant）以及INT8（启用本文提出的增强型SmoothQuant）的准确率对比</p><p>&nbsp;</p><p>从上图可以看出，在OPT-1.3b和BLOOM-1b7模型上，本文提出的增强型SmoothQuant的准确率比默认的SmoothQuant分别高5.4%和1.6%。量化后的模型也缩小到FP32模型的四分之一，大大减少了内存占用空间，从而有效地提升大模型在英特尔®平台上的推理性能。</p><p>&nbsp;</p><p>更全面的结果请见<a href=\"https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md\">GitHub存储库</a>\"(10)。同时，也欢迎您创建拉取请求或就<a href=\"https://github.com/intel/neural-compressor/issues\">GitHub问题</a>\"(15)发表评论。期待听到您的反馈意见和建议。</p><p>&nbsp;</p><p>作者：</p><p>英特尔公司人工智能资深架构师沈海豪、英特尔公司人工智能资深软件工程师程文华、英特尔公司人工智能软件工程师陆崟彤、何欣、郭恒、王畅、王梦妮，他们都在从事模型量化及压缩的研究与优化工作。</p><p>&nbsp;</p><p>注释：</p><p>1、&nbsp;英特尔® Neural Compressor</p><p>https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/neural-compressor.html</p><p>2、&nbsp;英特尔® Extension for TensorFlow</p><p>https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-tensorflow.html</p><p>3、&nbsp;英特尔® Extension for PyTorch</p><p>https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-pytorch.html</p><p>4、&nbsp;英特尔®至强®可扩展处理器</p><p>https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/scalable.html</p><p>5、英特尔®至强® CPU Max系列</p><p>https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/max-series.html</p><p>6、英特尔®数据中心GPU Flex系列</p><p>https://www.intel.cn/content/www/cn/zh/products/details/discrete-gpus/data-center-gpu/flex-series.html</p><p>7、英特尔®数据中心GPU Max系列https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html</p><p>8、第四代英特®至强®可扩展处理器</p><p>https://www.intel.cn/content/www/cn/zh/events/accelerate-with-xeon.html</p><p>9、AI与内存墙</p><p>https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8</p><p>10、SmoothQuant相关文档/英特尔® Neural Compressor文档&nbsp;/ GitHub存储库</p><p>https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md</p><p>11、SPIQ</p><p>https://arxiv.org/abs/2203.14642</p><p>12、Outlier Suppression</p><p>https://arxiv.org/abs/2209.13325</p><p>13、SmoothQuant</p><p>https://arxiv.org/abs/2211.10438</p><p>14、Hugging Face模型</p><p>https://huggingface.co/models</p><p>15、GitHub问题</p><p>https://github.com/intel/neural-compressor/issues</p>",
    "publish_time": "2023-07-03 11:36:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "用基于英特尔® SGX 的可信执行环境有效应对大语言模型隐私和安全挑战",
    "url": "https://www.infoq.cn/article/3yHIZDfmJI50IftLTohG",
    "summary": "<p></p><h2>I. 可信执行环境是什么？大语言模型为什么需要它？</h2><p></p><p></p><p>OpenAI 的&nbsp;GPT 系列大语言模型（Large Language Mode，以下缩写为&nbsp;LLM）的兴起与应用，也带来了诸如数据泄露、数据滥用、模型被攻击和知识产权被窃取等一些列隐私和安全风险或挑战。</p><p>&nbsp;</p><p>可信执行环境（Trusted&nbsp;Execution&nbsp;Environment，以下缩写为&nbsp;TEE）是一项基于软硬件组合创建安全执行环境，能够更好地确保计算和数据处理机密性和完整性。其关键机制为：</p><p>&nbsp;</p><p>·&nbsp;安全隔离：通过硬件加密和内存隔离等硬件隔离技术，将敏感数据和关键代码与其他应用及操作系统相隔离，从而确保它们即使在系统其他部分被攻击或受到恶意软件影响时也能够得到更好的保护。</p><p>·&nbsp;安全验证：在启动过程中进行身份验证和完整性检查，确保只有经过授权的代码和数据可以在其中运行，以此防止恶意软件或未经授权的访问。</p><p>·&nbsp;安全执行环境：提供包含加密算法、安全协议和密钥管理等防护功能的执行环境，用于处理敏感数据和执行关键算法，以增强数据在执行过程中的保密性和完整性。</p><p>&nbsp;</p><p>TEE 与LLM 可在多行业、多场景融合，TEE 可用于为&nbsp;LLM 提供颇具商业落地价值的隐私和数据保护创新解决方案。</p><p></p><h2>2. LLM 与TEE&nbsp;的融合需求</h2><p></p><p>&nbsp;</p><p>LLM 在许多行业的不同场景都有着广泛应用，例如金融行业的风险评估和交易分析，医疗保健领域的医学图像识别、病历纪录和疾病预测，以及法律和合规行业的法律咨询、合同审查和文书处理等。这些行业或场景中涉及到的数据多为重要敏感的交易数据或个人数据，必须得到有效保护。</p><p>&nbsp;</p><p>将&nbsp;TEE 与LLM 融合，有助于在这类场景中更好地保障数据在LLM 模型训练和推理过程中的保密性。训练阶段，TEE 中的数据处理都处于加密状态；推理阶段，TEE&nbsp;则可保护用户输入和模型结果的隐私。同时，其硬件隔离和安全验证机制可以更有效地防止未经授权的访问和攻击，增强模型运行时的安全性。</p><p></p><h2>3. TEE 与&nbsp;LLM 融合的挑战：资源和性能限制</h2><p></p><p>&nbsp;</p><p>·&nbsp;资源限制：TEE 的计算资源和存储空间通常都非常有限，LLM 庞大的模型参数和计算需求可能会超出一般TEE 的能力范围。</p><p>·&nbsp;性能下降：I/O&nbsp;数据的加密和安全计算操作会引入额外的计算开销，导致模型训练和推理性能有一定程度下降。基于算法的解决方案可减少模型规模和计算需求，以适应&nbsp;TEE&nbsp;的资源限制，但CPU 仍会成为制约LLM 训练的算力瓶颈。</p><p></p><h2>4. 基于英特尔® 平台的解决方案：加速TEE 与LLM 融合应用</h2><p></p><p></p><h3>4.1 基于英特尔®&nbsp;SGX/TDX[1]的TEE 解决方案</h3><p></p><p></p><p>英特尔自第三代英特尔® 至强®&nbsp;可扩展处理器开始内置英特尔®&nbsp;软件防护扩展（英特尔®&nbsp;SGX）技术，其安全飞地的容量最多可达单颗CPU 512GB，双路共计1TB 容量，可满足目前千亿大模型的执行空间需求。此外，该技术提供支持的机密计算可实现应用层、虚拟机&nbsp;(VM)、容器和功能层的数据隔离。无论是在云端、边缘还是本地环境，都能确保计算与数据始终在私密性和安全性上获得更全面的保护，以免暴露给云服务提供商、未经授权的管理员和操作系统，甚至是特权应用。</p><p></p><p>另一方面，英特尔®&nbsp;Trust Domain Extension（英特尔® TDX）可将客户机操作系统和虚拟机应用与云端主机、系统管理程序和平台的其他虚拟机隔离开来。它的信任边界较英特尔®&nbsp;SGX 应用层的隔离边界更大，使受其保护的机密虚拟机比基于英特尔®&nbsp;SGX 的安全飞地的应用更易于进行大规模部署和管理，在部署LLM 这类复杂应用时，TDX 在易用性上更具优势。此外，今年推出的全新第四代英特尔®至强®&nbsp;可扩展处理器内置英特尔® AMX，可大幅提升矩阵运算性能，而英特尔® SGX/TDX也可为英特尔®&nbsp;AMX、英特尔® DL Boost等计算指令提供支持，进而为&nbsp;TEE 中的大模型赋予快速落地+优化性能的双重优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b5dba3733d6685752ed50ac864eeb1c.png\" /></p><p>图1.&nbsp;SGX/TDX 的可信边界</p><p>&nbsp;</p><p>构建完善的TEE 生态系统对推动LLM 的应用和发展至关重要。开发者需要能够简化集成和使用过程的面向TEE&nbsp;的开发者工具和框架。为此，英特尔在SDK 的基础上，推出了开源的lib OS 项目Gramine 来帮助开发者更好地使用基于英特尔® SGX的TEE，助推LLM 与TEE&nbsp;的融合。</p><p></p><h4>4.1.1 大语言模型推理</h4><p></p><p></p><p>使用私有数据进行个性化训练的大模型不仅包含私有数据信息，其查询本身也具有隐私性，尤其是基于边端的非安全环境部署。基于英特尔®&nbsp;SGX/TDX 的TEE 可为大模型提供更安全的运行环境，在数据上传云端前，查询可先通过客户端对传输内容加密，云端只需在英特尔® SGX/TDX 中解密查询问题，然后输入大模型的推理服务中，并将所得结果在云端的TEE&nbsp;中加密后传输回本地客户端。在整个工作流程中，客户端以外的数据和运行态程序均处于密态环境当中，效率远远高于其他基于纯密码学的解决方案。</p><p></p><p>目前像&nbsp;LLAMA 7B、ChatGLM 6B&nbsp;等模型都可以在该TEE 方案上满足实时可交互性能的运行。图2 展示了使用LLM 部署知识问答的参考设计。基于英特尔®&nbsp;SGX/TDX&nbsp;的&nbsp;TEE为实际部署LLM&nbsp;中的自有知识产权保护提供了一套完整的方案，优化整个模型在查询、传输和推理过程中的安全保护。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b7/b71c748c672b1e34ac0c4cd2181eb8d0.png\" /></p><p>图2.&nbsp;基于TEE 的大语言模型私密问答</p><p></p><h4>4.1.2 联邦学习</h4><p></p><p>借助基于&nbsp;TEE&nbsp;的联邦学习解决方案[2]（见图&nbsp;3），就可在多机构之间实现基于&nbsp;NLP 的深度学习例如使用BERT 的命名体识别。在金融和医疗等行业提升准确性，实现多机构数据互通，同时更好避免数据泄露。</p><p>&nbsp;</p><p>此方案中每个参与方包含一个&nbsp;Avalon[3]&nbsp;管理模块和Gramine 工作负载，均运行在英特尔®&nbsp;SGX 的安全飞地中，在管理模块彼此间的远程认证完成执行后，即可启动联邦学习过程，参与方在本地使用各自的数据集进行训练，然后将梯度上传至聚合方，聚合方进行聚合后将平均梯度下发至各参与方，以继续进行下一轮训练。对比图&nbsp;4 所示的BERT&nbsp;+ CRF 模型[4]，此方案可以在强化隐私保护的同时，让性能损失维持在50% 以下[2]。</p><p><img src=\"https://static001.geekbang.org/infoq/96/96e0575e7f0041dfd620882193df7ec0.png\" /></p><p>图3.&nbsp;基于TEE&nbsp;的联邦学习</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/e2/e277eb12fb220c052301b63634aaceff.png\" /></p><p>图4.&nbsp;BERT&nbsp;+&nbsp;CRF&nbsp;模型[4]</p><p></p><h3>4.2 BigDL：端到端大模型和TEE&nbsp;融合的方案</h3><p></p><p></p><p>据行业用户反馈，LLM 在端到端应用中的痛点包括：</p><p></p><p>·&nbsp;软件栈复杂，难以确保端到端应用的安全。LLM 的训练和推理常依赖较多的软件栈、服务和硬件。为保护用户数据和模型产权，需确保每个环节的安全性（不同硬件、运行环境、网络和存储等）。</p><p>·&nbsp;计算量大，且对性能敏感。LLM 的计算量非常大，需引入足够多的性能优化。但是，不同模型、平台和软件栈需要使用不同的优化方案，要在特定平台上实现更理想的性能，需要长时间的性能调优。</p><p>为解决这些痛点，由英特尔主导的开源项目BigDL，近期就推出了针对&nbsp;LLM的隐私保护方案，其两大主要功能为：</p><p>&nbsp;</p><p>·&nbsp;提供端到端的安全保护：在不修改代码的情况下，为单机和分布式的&nbsp;LLM应用提供端到端的安全保护功能。具体包括，基于英特尔®&nbsp;SGX/TDX的&nbsp;TEE、远程证明、统一的密钥管理接口和透明的加解密&nbsp;API等。</p><p>·&nbsp;实现一站式性能优化：BigDL Nano 提供的针对&nbsp;LLM 的一站式性能优化方案，可让现有&nbsp;LLM 应用在几乎不用修改代码的情况下受益于英特尔®&nbsp;AMX、英特尔®&nbsp;AVX-512 和英特尔®&nbsp;Extension for PyTorch。同时，用户还可利用&nbsp;BigDL Nano 提供的&nbsp;LLM API，快速构建应用。</p><p><img src=\"https://static001.geekbang.org/infoq/f3/f397ee5e26aac22741f8c4bfb266ba28.png\" /></p><p>图5.&nbsp;BigDL&nbsp;端到端安全的大模型方案</p><p>&nbsp;</p><p>如图6 所示，在应用了&nbsp;PPML（Privacy Preserving Machine Learning，隐私保护的机器学习）提供的安全技术后，由于更强的安全和隐私保护会带来额外开销，因此端到端应用性能会略有下降；但应用了&nbsp;BigDL Nano提供的优化功能后，端到端的性能得到了显著改善*，总体性能甚至高于没有任何保护的明文性能。</p><p><img src=\"https://static001.geekbang.org/infoq/3f/3fc1b683af2fda42fdc423a95d1a5c3f.png\" /></p><p>图6.&nbsp;BigDL PPML + Nano 端到端性能损失情况</p><p>&nbsp;</p><p>目前，该方案已经开源，并开始陆续交付给行业客户进行测试和集成[5]。</p><p>&nbsp;</p><p></p><h2>5. 未来趋势</h2><p></p><p>&nbsp;</p><p>TEE 提供了隐私保护和数据安全防护功能的创新解决方案，将在&nbsp;LLM 实际落地过程中扮演重要角色。通过将二者融合，可更好地保障数据在训练和推理过程中的保密性，增强对未经授权访问和模型结果篡改的防御。然而，在TEE&nbsp;中保护用户隐私的同时，需要平衡性能需求，随着大模型对于计算需求的大幅提升，算力可能会执行在异构硬件上，TEE&nbsp;和异构硬件的结合将成为未来发展趋势。随着&nbsp;CPU 性能的提升以及内置AI&nbsp;加速技术的升级和更新，在方便部署的场景下，CPU 会是大模型推理和TEE&nbsp;结合的首选，在训练的场景下，基于CPU 的TEE&nbsp;结合异构硬件的加密支持，则会是大模型训练甚至大模型联邦训练的技术方向。英特尔将一如既往地以软硬结合的产品技术组合促进开发者参与，推动LLM 与TEE&nbsp;的融合。</p><p>&nbsp;</p><p>作者简介：</p><p>英特尔公司AI 架构师俞巍，英特尔公司平台安全资深架构师李志强，英特尔公司安全软件研发工程师李青青，英特尔公司软件架构师龚奇源，都在从事AI&nbsp;和SGX/TDX 相关工作。</p><p>&nbsp;</p><p>[1] SGX/TDX: <a href=\"https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/innovation-data-protection-with-security-engines.html\">https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/innovation-data-protection-with-security-engines.html</a>\"</p><p>[2] Wei Yu. et al. 2022, TEE based Cross-silo Trustworthy Federated Learning Infrastructure, FL-IJCAI'22</p><p>[3] <a href=\"https://github.com/hyperledger-archives/avalon\">https://github.com/hyperledger-archives/avalon</a>\"</p><p>[4] Souza, F., Nogueira, R. and Lotufo, R. 2020, Portuguese Named Entity Recognition using Bert-CRF, arXiv.org（请参见<a href=\"https://arxiv.org/pdf/1909.10649.pdf\">https://arxiv.org/pdf/1909.10649.pdf</a>\"）</p><p>[5] <a href=\"https://github.com/intel-analytics/BigDL/tree/main/python/llm\">https://github.com/intel-analytics/BigDL/tree/main/python/llm</a>\"</p><p>*性能优化效果与具体平台、模型和环境有关</p>",
    "publish_time": "2023-07-03 11:42:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "市值曾破10亿美元，但95%都是假用户、创始人带头造假还PUA员工，这家软银领投的独角兽公司也爆雷了......",
    "url": "https://www.infoq.cn/article/MANfWltNOdOGEtA2lmBd",
    "summary": "<p>硅谷的初创企业总是会热衷于追求独角兽地位，公司和投资者常常不遗余力地追求天文数字的估值和快速增长。然而，过度追求“独角兽”这一称呼有时可能最终会以丑闻和倒闭收尾。即使在旧金山科技初创企业飞速发展的世界里，迅速崛起和一夜间倒闭的公司也并不罕见，曾经深得资本厚爱的社交应用初创公司 IRL 就是这其中一个。</p><p></p><p>IRL 将自己描述为“领先的社群消息社交网络，通过活动和共享体验将人们聚集在一起。” IRL 号称将成为 Z 世代客群的活动组织平台，迎合这部分用户不愿上 Facebook 的使用习惯。首席执行官亚伯拉罕·沙菲 (Abraham Shafi) 表示：“我们在 IRL 的首要目标始终是创建更真实、更有价值的社区。”</p><p></p><p></p><h2>市值曾突破 10 亿美元，但 95% 用户都是机器人</h2><p></p><p>2017 年，Shafi 和 PayPal 早期董事会成员斯科特·巴尼斯特 (Scott Banister) 一同创立了 IRL。最初的 IRL 并没有多大的名气。</p><p></p><p>2021 年，在日本投资巨头软银 Vision Fund 2 领投的 1.7 亿美元 C 轮融资后， IRL 的估值突破 10 亿美元大关，跻身令人垂涎的独角兽俱乐部，当时它声称拥有 1200 万用户。也就是从那时起 IRL 开始名声大噪，备受瞩目。而也是从那时起，公司的内部问题开始变得愈发明显。</p><p></p><p>名为社交应用，讽刺的是 IRL 上的用户却并非活生生的真人。</p><p></p><p>近日，据 The Information 网站率先报道，IRL 董事会的一项内部调查发现，该应用报告的 2000 万用户中有 95% 为“自动用户或机器人”。因此在筹集到超 2 亿美元的风险投资之后，IRL 宣布即将关闭。</p><p>IRL 发言人埃利奥特·斯隆 (Elliot Sloane) 证实，该公司将被解散，其大部分资产将转移至清算信托。</p><p></p><p></p><blockquote>“股东们选择采取董事会提议的这一行动，这是一项特别承诺调查的结果，该调查发现，95% 的已识别用户实际上是自动化用户或来自机器人，而不是真正的人类用户。基于这些调查结果，大多数股东得出的结论是，公司的未来前景是不可持续的。”</blockquote><p></p><p></p><p>Sloane 补充说，硅谷公司 Sherwood Partners 将帮助 IRL 制定“清算计划”，以分配公司剩余的现金。IRL 网站仍然活跃，但该应用程序已从 iOS 应用商店中删除。（它仍然可以在适用于 Android 设备的 Google Play 商店中找到。）</p><p></p><p>IRL 的母公司 Live Awake 还运营一家制作应用程序的公司 Memix，该应用程序似乎仍然活跃。据 Shafi 的 LinkedIn 称，他与 Memix 的联系也于 2023 年 6 月结束。</p><p></p><p>今年早些时候，一名 IRL 前员工揭露了公司的许多用户都是机器人，并不是真正的人类用户。紧接着，公司内的其他员工也对 IRL 用户数量的真实性表示质疑。他们怀疑首席执行官亚伯拉罕·沙菲 （Abraham Shafi）声称该应用程序每月有 2000 万活跃用户——这一数字现在看来即使不是彻头彻尾的欺诈，也是完全被误导了。</p><p></p><p>其实，在此次用户造假事件暴雷之前，IRL 自身内部问题早已积弊已久。</p><p></p><h2>创始人带头造假，还 PUA 员工</h2><p></p><p>去年，IRL 解雇了团队内 25% 的成员——约 25 名员工。但同样是在过去 12 个月间，IRL 曾迅速将雇员数量增加了两倍多，所以突如其来的裁员令人意外。在外媒体 TechCrunch 获得的一份致员工通告中，公司前 CEO 兼创始人 Abraham Shafi 鼓励大家“接受现实”并“遵守纪律”，并表示 WhatsApp 在仅有 55 名员工时就支撑起了 4.5 亿用户。</p><p></p><p>Shafi 在这份备忘录中塞进了大量莫名其妙的类比，“发展成这类标志性、极具影响力的企业，就像是在奥运会上夺金。大多数人其实并不想走竞技体育的道路，也不是每个人都愿意选择我们走的这条路。但对于那些想要突破自身极限、发现自我能力的人们来说，这种文化肯定适合你。”</p><p></p><p>同样在这份备忘录中，Shafi 称该公司“拥有足够的现金，完全可以维持到 2024 年。”而一位 IRL 员工当时向 TechCrunch 表示，该公司的银行存款超过 1 亿美元。虽不清楚到现在 IRL 手里还剩多少现金，但该公司发言人明确告知 The Information，他们将把资本返还给各位股东。</p><p></p><p>也差不多就是在裁员的同时，IRL 员工开始怀疑 Shafi 宣称的月活用户高达 2000 万的说法。随着调查结果的尘埃落定，到今年 4 月，IRL 董事会决定暂停 Shafi 的职务并任命一位代理 CEO。</p><p></p><p>TechCrunch 还试图联系 Shafi 本人和另一位 IRL 员工，但双方的 IRL 邮件地址均已被禁用。</p><p></p><h2>虚报用户数量的公司屡见不鲜</h2><p></p><p>值得注意的是，夸大平台用户数量的公司远不止 IRL 一家。</p><p></p><p>今年 3 月，Hindenburg Research 指控 Twitter 前首席执行官 Jack Dorsey 的金融科技公司 Block（前身为 Square）通过“虚假和重复账户”夸大用户数量来误导投资者。Block 公司甚至含糊地承认了这一点。</p><p></p><p>被外界视为 TikTok 的竞争对手 Triller 在 2019 年 10 月声称每月活跃用户数为 1300 万，而内部仪表板显示该数字为 200 万。</p><p></p><p>Ozy Media 是卡洛斯·沃森 （Carlos Watson）领导的陷入困境的数字新闻媒体，早在 2021 年就宣称拥有 5000 万独立用户。当时，测量网站流量的行业标准 Comscore 的数据讲述了一个截然不同的故事，用户数量在 23 万到 250 万之间。曾在 MSNBC 和 CNN 任职的创始人沃森也因长期以来一直欺诈潜在投资者和广告商而被捕。</p><p></p><p>学生贷款应用 Frank 的创始人 Charlie Javice 在 4 月份被指控夸大 Frank 的客户数量并诈骗摩根大通，后者于 2021 年为她的公司支付了 1.75 亿美元。</p><p></p><p>虽然实现增长和向投资者展示价值的压力有时会导致公司创始人们铤而走险虚报一些公司财务、用户等数据。但这些以牺牲长期稳定和信誉为代价换来的短暂的繁荣犹如昙花一现，泡沫破灭后所带来的代价终究还要由公司与无辜的员工们一起承担。</p><p></p><p>参考链接：</p><p>https://techcrunch.com/2023/06/26/irl-shut-down-fake-users/</p><p>https://innovation-village.com/unicorn-social-startup-irl-shuts-down-after-95-of-its-users-were-found-to-be-fake/</p><p>https://qz.com/social-app-irl-is-shutting-down-because-most-of-its-use-1850580325</p>",
    "publish_time": "2023-07-03 14:21:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023云边协同大会圆满召开，一场云边端建设与成果的盛宴！",
    "url": "https://www.infoq.cn/article/UE5UXsFsKKVlEXK3Bhse",
    "summary": "<p>2023 年 6 月 30 日，由中国信息通信研究院（以下简称“中国信通院”）、中国通信标准化协会主办，云计算标准和开源推进委员会承办的“2023 云边协同大会”在北京歌华开元酒店召开。大会以“云智物联，边筑算新”为主题，公布了分布式云、<a href=\"https://xie.infoq.cn/article/c574df54e396b1973376ba537\">边缘计算</a>\"、AIoT 平台的最新评估结果，发布云边端一体化技术创新与最佳实践先锋案例，并与合作伙伴共同发布《银行业大规模异构边缘节点管理实践指南》、《分布式云行业实践指南》等重磅成果，解读并发布算力服务时延圈标准体系，启动算力服务时延圈领航计划，并公布算力服务时延圈首批评估结果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc22aa30c09562345275afe5190adcde.png\" /></p><p></p><p>工业和信息化部信息通信管理局业务资源处处长赵阳出席会议并致辞，中国通信标准化协会副理事长兼秘书长代晓慧视频致辞，中国工程院院士倪光南通过视频作主题发言。开幕式由中国信通院云计算与大数据研究所副所长栗蔚主持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/55/557ce9ee7911ad68ff662c1cd424ffa8.png\" /></p><p>工业和信息化部信息通信管理局业务资源处处长赵阳致辞</p><p></p><p>赵阳在致辞中表示，边缘计算是打通工业互联网赋能行业数字化“最后一公里”的关键支撑技术。未来，要重点围绕<a href=\"https://xie.infoq.cn/article/e9854f92c83b68b79ff58ac9b\">云边协同</a>\"与工业互联网的融合创新，加快关键技术产品研发，促进新场景新模式应用，进一步赋能传统产业转型发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/91/916b7f86c5db019ab1fc150d9f4147e9.png\" /></p><p>中国通信标准化协会副理事长兼秘书长代晓慧视频致辞</p><p></p><p>代晓慧在致辞中，对进一步推动我国云边协同产业发展与实践落地，提出三点建议：一是贯彻落实国家重点政策；二是持续推动技术产业融合；三是加快完善云边端协同标准体系。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b7d3e83b847eecc92a11c9dcf8d38eee.png\" /></p><p>中国工程院院士倪光南视频发言</p><p></p><p>倪光南在主题发言中表示存储、计算、网络技术都是我国信息产业发展的核心技术，是建设科技强国的战略支撑，我国应当抓住新一代信息技术发展的新机遇，实现高质量科技自立自强，为科技强国建设，掌握数字经济竞争主动权提供坚实支撑。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2e0b6aad18b9c9b945bb4ad94c47daad.png\" /></p><p>分布式云和 AIoT 平台最新评估结果</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/98a0863b353527d37a537c11bc15c7d5.png\" /></p><p>边缘计算最新评估结果</p><p></p><p>在成果发布环节，大会发布 2023 年分布式云、边缘计算、AIoT 平台等最新评估结果，共有来自 11 家企业的 22 个产品及平台通过了本次评估。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10e87c5fdbf8bbe0b1ddb88f16cfcdfe.png\" /></p><p>分布式云技术创新先锋案例</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f255e79905a1f7e3bdd70af13f4208c3.png\" /></p><p>分布式云最佳实践先锋案例</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f1/f11759fcbe5ff6ce6698f8705793b304.png\" /></p><p>边缘计算技术创新先锋案例</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/07/074630c00889b80aacea3990ace2d292.png\" /></p><p>AIoT 平台技术创新与最佳实践先锋案例</p><p></p><p>大会发布了 2023 云边端一体化技术创新与最佳实践先锋案例，分布式云、边缘计算与 AIoT 平台，分别有 20 个、27 个和 12 个案例获评技术创新与最佳实践先锋案例，为推广相关技术的先进经验与最佳实践树立了行业标杆。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b08c58f9d02eb3109df7ae3d086cccb1.png\" /></p><p>发布《银行业大规模异构边缘节点管理实践指南》</p><p></p><p>栗蔚与中国农业银行总行研发中心应用开发三部总经理孙志斌共同发布了《银行业大规模异构边缘节点管理实践指南》，共同推进边缘节点管理在银行领域的应用实践。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/55/555ef0334a81652b063c69d3dc1fd6a1.png\" /></p><p>发布《分布式云行业实践指南》</p><p></p><p>栗蔚与腾讯云分布式云负责人李向辉共同发布业内首个《分布式云行业实践指南》，推动分布式云在各行业的应用逐步深入，推广落地实践经验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3fd2aa8cf3021e4eddb3ea7e230ab211.png\" /></p><p>发布 2022-2023 年度云边协同“领航人物”</p><p></p><p>会上公布了 2022-2023 年度云边协同“领航人物”，招商局集团有限公司数字化中心技术管理处处长山金孝、中国铁路信息科技集团有限公司总信息师朱涛、中国农业银行总行研发中心应用开发三部总经理孙志斌、移动云能力中心创新中心副总经理杜宇健、阿里云智能边缘云业务总经理郝冲、浪潮云产品总监尹萍、杭州谐云科技副总裁才振功等获此殊荣。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f1/f1b37d0f19df668c86e780cf799054f7.png\" /></p><p>栗蔚发布并解读《算力服务时延圈标准体系》</p><p></p><p>在算力服务时延圈专题行动环节，栗蔚发布并解读《算力服务时延圈标准体系》，首先分析了国家、地方等在算力服务时延圈的布局和规划以及数字化应用场景对算力服务低时延的迫切需求，接着在供给侧以运营商企业为例介绍了全国算力服务时延圈的建设情况以及构建算力服务时延圈的核心三要素，最后整体介绍并解读了中国信通院算力服务时延圈标准体系及未来规划。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f44df9fa220c476970e2df59eec09f60.png\" /></p><p>算力服务时延圈领航计划启动</p><p></p><p>接着，栗蔚与中移苏研 IaaS 产品部副总经理姚军、联通数科 IaaS 产品技术部负责人陈晓明、浪潮云产品总监尹萍、天翼云互联网金融中心沈旭等共同启动算力服务时延圈领航计划，为推动算力分布式发展贡献力量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/278160c2d5274efe1cf592f144d7774a.png\" /></p><p>算力服务时延圈首批评估结果发布</p><p></p><p>在算力服务时延圈专题行动环节的最后阶段，栗蔚为通过算力服务时延圈评估的单位颁发证书，在本次评估中，天翼云科技有限公司的智能边缘云、中国移动通信集团有限公司的边缘智能云 EIC 在北上广地区时延圈评估中都达到了卓越级水平。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e947610d73ae3c507280be85cbb0e158.png\" /></p><p>主题演讲</p><p></p><p>在主题演讲环节，来自产业界的多位专家发表了精彩主题演讲。招商局集团有限公司数字化中心技术管理处处长山金孝分享了《云边协同在传统产业数字化建设中的应用实践》；中国农业银行总行研发中心应用开发三部总经理孙志斌发表《新金融，新赛道，新未来 --“边缘计算 +”助力金融场景延展数字新核心》主题演讲；腾讯云分布式云负责人李向辉分享主题《云边架构加速各行各业上云实践》；中国铁路信息科技集团有限公司总信息师朱涛带来《新一代云边结合铁路云及其安全架构设计》主题演讲；中国移动云能力中心云网管理产品部总经理助理王燕分享主题《云边协同，算网大脑助力智算天下》；联通数字科技有限公司云计算事业部 -IaaS 产品技术部部门负责人陈晓明带来了《联通云分布式算力实践》的主题演讲；最后，中国信息通信研究院云计算与大数据研究所政企数字化转型部主任徐恩庆发表《共建云边端共促数字化》主题演讲。</p><p></p><p>在当天下午，大会还分别设置了分布式云高质量发展论坛、边缘计算高质量发展论坛和 AIoT 平台高质量发展论坛。“云智物联，边筑算新”，在业界专家及行业同仁的共同支持和见证下，2023 云边协同大会为大家带来一场云边端建设和成果的盛宴。未来，中国信通院将会继续发挥行业平台作用，凝聚产业力量，构建交流平台，开展相关标准制定及行业研究工作，携手各方共同为我国云边端产业高质量发展贡献力量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f85c9bc9946ef9385a9c6a315b62dd9.png\" /></p><p></p>",
    "publish_time": "2023-07-03 15:22:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "LLM 应用程序的新兴架构",
    "url": "https://www.infoq.cn/article/kPUyVsWATRdiJuGWKCSt",
    "summary": "<p>&nbsp;</p><p>大型语言模型是一个新的软件构建的强大原语。但是由于它们非常新，其行为也不同于普通的计算资源，所以它们的用法有时候并不是那么容易搞清楚。</p><p>&nbsp;</p><p>在这篇文章中，我们将分享一个新兴的LLM应用程序技术栈的参考架构，其中包括在AI初创企业和成熟的科技公司中最常见的系统、工具和设计模式。该技术栈还处于非常早期的阶段，可能会随着底层技术的发展而发生根本性的变化，但我们希望这个参考架构能够为正在使用LLM的开发人员带来帮助。</p><p></p><h2>技术栈简介</h2><p></p><p></p><p>下图是当前LLM应用程序技术栈的概览：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b2330e0a57d93960f03b4e93600d36e.png\" /></p><p></p><p>&nbsp;</p><p>以下是相关项目的链接清单，方便读者朋友们快速查阅：</p><p></p><p></p><p>&nbsp;</p><p></p><p></p><p>使用LLM进行构建的方法有许多，包括从头开始训练模型、对开源模型做一些微调或者使用托管API。本文要介绍的技术栈是基于<a href=\"https://en.wikipedia.org/wiki/In-context_learning_(natural_language_processing)\">上下文学习</a>\"的。大多数开发人员都是从这个设计模式入手的（现在只有基础模型才有可能）。</p><p>&nbsp;</p><p>下一节将简单介绍下这种模式；有经验的LLM开发人员可以跳过。</p><p></p><h2>设计模式：上下文学习</h2><p></p><p></p><p>上下文学习的核心思想是使用现成的LLM（即不做任何调整优化），然后通过私有的“上下文”数据进行巧妙地提示和调节，从而控制它们的行为。</p><p>&nbsp;</p><p>例如你正在构建一个聊天机器人，用于回答和一系列法律文档相关的问题。最简单的方法是将所有文档粘贴到ChatGPT或GPT-4中，然后询问有关这些文档的问题。如果数据集非常小，这是可以的，但这种方法无法扩展。最大的GPT-4模型也只能处理大约50页的输入文本，而且当接近这个限制（上下文窗口）时，它的性能（通过推理时间和准确性来度量）会严重下降。</p><p>&nbsp;</p><p>上下文学习用一个巧妙的技巧解决了这个问题：它不会在每个LLM提示中都发送所有文档，而是只发送少数相关度最高的文档，而哪些文档相关度最高是在LLM的帮助下确定的。</p><p>&nbsp;</p><p>概括地讲，这个工作流可以分为三个阶段：</p><p></p><p>数据预处理/嵌入：该阶段包括存储稍后用于检索的私有数据（本例中的法律文档）。通常，文档被分割成块，通过嵌入模型进行传递，然后存储在名为向量数据库的专用数据库中。提示构建/检索：当用户提交查询（本例中的法律问题）时，应用程序构建一系列提示并提交给语言模型。通常，编译后的提示会与开发人员硬编码的提示模板相结合；称为few-shot examples的有效输出样本；从外部API检索到的其他一些必要的信息；还有从向量数据库中检索出的一组相关文件。提示执行/推理：编译完提示后，将它们提交给预训练的LLM进行推理——包括专有模型API以及开源或自训练模型。有些开发人员还在这个阶段中添加了日志记录、缓存和验证等。</p><p>&nbsp;</p><p>看起来，这个过程的工作量很大，但通常这比训练或微调LLM本身要简单。上下文学习不需要一个专门的机器学习工程团队，你也无需托管自己的基础设施或从OpenAI购买昂贵的专用实例。这种模式有效地将人工智能问题简化一个大多数初创公司和大公司都知道如何解决的数据工程问题。在数据集相对较小的情况下，它的性能往往也优于微调——因为特定的信息片段要在训练集中至少出现10次，LLM才能借助微调记住它——并且可以近乎实时地吸纳新数据。</p><p>&nbsp;</p><p>关于上下文学习，一个最大的问题是：如果我们只是通过改变底层模型来扩大上下文窗口的话，会发生什么？实际上，这是可能的，而且是一个活跃的研究领域（感兴趣的话，可以看下这篇<a href=\"https://arxiv.org/abs/2302.10866\">关于Hyena的论文</a>\"或这篇<a href=\"https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c\">最近发表的文章</a>\"）。但这里需要做一些权衡——主要是推理的成本和时间随提示长度的增加呈非线性增长。如今，对于许多应用程序来说，即使是线性增长（理论上最好的结果），成本也是过高的。以目前的API费率计算，超过1万页的单个GPT-4查询将花费数百美元。因此，别指望基于扩展上下文窗口对技术栈进行大规模地更改。关于这一点，下文会进一步的讨论。</p><p>&nbsp;</p><p>如果想更深入地了解上下文学习，<a href=\"https://a16z.com/2023/05/25/ai-canon/\">AI canon</a>\"提供了许多不错的资源（特别是“LLM应用实践指南”这一部分）。在本文接下来的部分中，我们将顺着上面的工作流，逐阶段地看下这个参考技术栈。</p><p>&nbsp;</p><p></p><h2>数据预处理/嵌入</h2><p></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5b470f254c7fbf3048d12805de80ddb2.png\" /></p><p></p><p>&nbsp;LLM应用程序的上下文数据包括文本文档、PDF，甚至是结构化的格式，如CSV或SQL表。在我们交谈过的开发人员中间，这些数据的加载和转换解决方案存在着很大的差异。大多数人使用传统的ETL工具，如Databricks或Airflow。有些人还使用编排框架中内置的文档加载器，比如LangChain（基于Unstructured）和LlamaIndex（基于Llama Hub）。不过我们认为，技术栈的这个部分相对还不够成熟，专门为LLM应用程序构建的数据复制解决方案还有机会。</p><p>&nbsp;</p><p>对于嵌入，大多数开发人员都在使用OpenAI API，特别是text- embeddings -ada-002模型。它的用法很简单（特别是如果你已经在使用其他OpenAI API），结果也相当不错，并且越来越便宜。有些比较大的企业也在研究Cohere（它专注于嵌入这个相对狭窄的领域，并且在某些场景下提供了更好的性能）。对于喜欢开源的开发人员来说，来自hug Face的Sentence Transformers库是一个标准。也可以<a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\">针对不同的用例创建不同类型的嵌入</a>\"；这是一个小众的做法，但是一个有前途的研究领域。</p><p>&nbsp;</p><p>从系统的角度来看，预处理管道最重要的组成部分是向量数据库，它负责高效地存储、对比和检索多达数十亿个嵌入（即向量）。市场上最常见的选择是Pinecone。人们默认就会选择它，因为它完全是云托管的，上手很容易并且提供了许多大型企业在生产环境中需要的特性（例如，大规模情况下的良好性能、SSO和正常运行时间SLA）。</p><p>&nbsp;</p><p>不过，可供选择的向量数据库还是不少的，尤其是下面这些：</p><p></p><p>像Weaviate、Vespa和Qdrant这样的开源系统：它们通常都提供了出色的单节点性能，并且可以针对特定的应用程序进行定制。因此，在喜欢构建定制平台的经验丰富的AI团队中，它们很受欢迎。像Chroma和Faiss这样的本地向量管理库：它们提供了很好的开发体验，并且很容易应用于小型应用和开发实验。在规模比较大的时候，它们可能无法完全代替完备的数据库。像pgvector这样的OLTP扩展：对于那些熟悉数据库并试图添加Postgres的开发人员，或者从单个云提供商那里购买了大部分数据基础设施的企业来说，这是一个很好的向量支持解决方案。目前还不清楚，从长远来看，将向量和标量工作负载紧密耦合是否有意义。</p><p>&nbsp;</p><p>大多数开源向量数据库公司都在开发云产品。我们的研究表明，面对各种各样可能的用例，要想在云中实现强大的性能非常困难。因此在短期内，这个选项集可能不会有太大的变化，但长远来看可能会有。问题的关键是向量数据库是否会像OLTP和OLAP一样，围绕一两个流行的系统进行整合。</p><p>&nbsp;</p><p>另一个悬而未决的问题是，随着大多数模型可用上下文窗口的扩大，嵌入和向量数据库将如何发展。人们很容易会说，嵌入将变得不那么重要，因为上下文数据可以直接放到提示中。然而，专家对这个话题的反馈却表明，随着时间的推移，嵌入管道可能会变得越来越重要。大型上下文窗口是一种强大的工具，但它们的计算成本也很大。因此，当务之急是如何有效地利用它们。我们可能会看到，不同类型的嵌入模型开始流行，并直接针对模型相关性进行训练，而向量数据库在设计时就已经考虑到要利用这一点。</p><p></p><h2>提示构建/检索</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e221b70062f7fd6ae16e99f92e4eb99.png\" /></p><p></p><p>&nbsp;</p><p>提示LLM并整合上下文数据的策略正变得越来越复杂，而且作为产品差异化的来源，也变得越来越重要。在启动新项目时，大多数开发人员都是尝试一些简单的提示，包括直接指令（零提示）或一些示例输出（小样本提示）。通常，这些提示可以给出很好的结果，但无法达到生产部署所需的准确性水平。</p><p>&nbsp;</p><p>按照设计，提示的下一个层次是基于一些事实来源创建模型响应，并提供模型训练时未涉及的外部上下文。<a href=\"https://www.promptingguide.ai/techniques\">《LLM提示工程指南》</a>\"列出了超过12种更为高级的提示策略，包括思维链、自我一致性、生成知识、思想树、定向刺激等等。我们还可以搭配使用不同的策略，为不同的LLM用例提供支持，如文档问答、聊天机器人等。</p><p>&nbsp;</p><p>这就是LangChain和LlamaIndex等编排框架的用途所在。它们抽象了提示链的诸多细节；与外部API交互（包括确定何时需要调用API）；从向量数据库中检索上下文数据；跨多个LLM调用维护内存。它们还为上面提到的许多常见的应用程序提供了模板。它们的输出是提交给语言模型的一个或一系列提示。在希望从零开发LLM应用程序的爱好者和初创公司中，这些框架的应用非常广泛，而LangChain是其中的佼佼者。</p><p>&nbsp;</p><p>目前，LangChain还是一个相对比较新的项目（当前版本0.0.201），但我们已经看到，已经有人开始用它构建生产应用。有些开发人员，特别是LLM的早期采用者，更喜欢在生产环境中使用原始Python，为的是消除额外的依赖。但我们预计，随着时间的推移，在大多数用例中，这种DIY方法会逐步减少，就像传统的Web应用技术栈一样。</p><p>&nbsp;</p><p>眼尖的读者会注意到，编排框中有一个看似奇怪的条目：ChatGPT。通常来说，ChatGPT是一个应用程序，而不是开发工具。但它也提供了可供访问的API。而且，如果你再仔细看一下，就会发现它的功能与其他编排框架相同，例如：抽象掉对定制提示的需求；维护状态；通过插件、API或<a href=\"https://github.com/openai/chatgpt-retrieval-plugin\">其他数据源</a>\"检索上下文数据。虽然与这里列出的其他工具之间不存在直接的竞争关系，但ChatGPT可能会被看作是一种替代的解决方案，并且它最终可能成为一种简单可行的提示构建替代方案。</p><p>&nbsp;</p><p></p><h2>提示执行/推理</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a895b7a28c0e1acba13c090cd1190172.png\" /></p><p></p><p>如今，OpenAI是语言模型的领导者。在开始构建新的LLM应用程序时，我们交谈过的几乎所有开发者都在使用OpenAI API，通常使用gpt-4或gpt-4-32k模型。它们提供了理想的解决方案，并且易于使用，因为其输入域非常广，通常不需要微调或自托管。</p><p>&nbsp;</p><p>当项目进入生产阶段并开始规模化应用时，可选的范围就会更大，常见的选项包括：</p><p></p><p>切换到gpt-3.5-turbo：<a href=\"https://github.com/ray-project/llm-numbers\">成本是GPT-4的大约五十分之一</a>\"，而且速度也快很多。许多应用程序并不需要GPT-4级别的准确性，但确实需要低延迟推理以及为免费用户提供具有成本效益的支持。测试其他专有供应商的模型（特别是Anthropic的Claude模型）：Claude提供了快速推理，GPT-3.5级别的准确性，为大客户提供的更多的定制选项，以及高达100k的上下文窗口（尽管我们发现，准确性会随着输入长度的增加而降低）。将一些请求提供给开源模型：在搜索或聊天等大量的B2C用例中这尤其有效，因为这些用例的查询其复杂性有很大的差异，需要以较低的成本为免费用户提供服务。通常，这要与微调开源基础模型结合才最有意义。本文中我们不会深入讨论那个工具栈，但是像Databricks、Anyscale、Mosaic、Modal和RunPod这样的平台为越来越多的工程团队所采用。可以用于开源模型的推理选项多种多样，包括来自hug Face和Replicate的简单API接口；来自主要云提供商的原始计算资源；以及像上面列出的那些更为个性化的云产品。</p><p>&nbsp;</p><p>目前，与专有产品相比，开源模型还是落后一些，但差距正在缩小。Meta的LLaMa模型为开源模型的准确性设定了新的标准，并催生了一系列的变体。由于LLaMa仅授权用于研究用途，所以有许多新的供应商已经着手训练替代的基础模型（如Together、Mosaic、Falcon、Mistral）。Meta也在<a href=\"https://youtu.be/6PDk-_uhUt8?t=139\">探讨</a>\"一个真正开源的LLaMa 2版本。</p><p>&nbsp;</p><p>当开源LLM的准确性达到与GPT-3.5相当的水平时，我们希望可以看到文本的Stable Diffusion时刻，包括大规模的实验、共享和微调模型的生产应用。像Replicate这样的托管公司已经在补充工具，让软件开发人员可以更轻松地使用这些模型。越来越多的开发人员相信，小型的微调模型可以在特定的用例中达到更高的准备性。</p><p>&nbsp;</p><p>与我们交谈过的大多数开发人员都还没有深入研究LLM的操作工具。缓存相对比较常见（通常基于Redis），因为它可以减少应用程序的响应时间和成本。像Weights&amp;Biases和MLflow（从传统机器学习移植而来）或PromptLayer和Helicone（专门为LLM构建）这样的工具应用也很广泛。它们可以记录、跟踪和评估LLM输出，通常是为了改进提示构建、优化管道或选择模型。还有一些正在开发中的新工具，用于验证LLM输出（例如Guardrails）或检测提示注入攻击（如Rebuff）。这些操作工具中的大多数都鼓励使用它们提供的Python客户端来进行LLM调用，因此，看这些解决方案如何随着时间的推移和谐共存会非常有趣。</p><p>&nbsp;</p><p>最后，LLM应用程序的静态部分（即除了模型之外的所有内容）也需要托管在某个地方。我们看到，到目前为止，最常见的解决方案是像Vercel或主流云提供商这样的标准选项。不过，有两种新的类型正在兴起。像Steamship这样的初创公司就为LLM应用程序提供了端到端托管，包括编排（LangChain）、多租户数据上下文、异步任务、向量存储和密钥管理。Anyscale和Modal等公司允许开发人员在一个地方托管模型和Python代码。</p><p>&nbsp;</p><p></p><h2>代理呢？</h2><p></p><p></p><p>在这个参考架构缺少的组件中，最重要的是AI代理框架。<a href=\"https://github.com/Significant-Gravitas/Auto-GPT\">AutoGPT</a>\"被描述为“一个使GPT-4完全自主的实验性开源尝试”。今年春天，它成了历史上<a href=\"https://twitter.com/OfficialLoganK/status/1647757809654562816\">发展最快的Github存储库</a>\"。实际上，现如今，每个人工智能项目或初创公司都使用了某种形式的代理。</p><p>&nbsp;</p><p>说到代理的潜力，与我们交谈过的大多数开发人员都感到无比兴奋。本文介绍的上下文学习模式可以有效地解决幻觉和数据新鲜度问题，从而更好地支持内容生成任务。另一方面，代理赋予AI应用一系列全新的功能：解决复杂的问题，对外部世界做出反应，部署后从经验中学习。为此，它们会搭配高级推理/规划、工具使用和记忆/递归/自我反省。</p><p>&nbsp;</p><p>因此，代理有可能成为LLM应用程序架构的核心部分（如果你相信递归自我完善，它们甚至可以接管整个技术栈）。现在，像LangChain这样的框架已经包含了一些代理概念。问题只有一个：代理还没有真正发挥作用。目前，大多数代理框架仍处于概念验证阶段——能够进行令人难以置信的演示，但还无法可靠地重复完成任务。我们正在密切关注，在可见的将来，它们会如何发展。</p><p>&nbsp;</p><p></p><h2>未来展望</h2><p></p><p></p><p>预训练的人工智能模型代表了自互联网出现以来软件领域最重要的架构变化。它们使个人开发人员能够在几天内构建出令人难以置信的人工智能应用程序，而且可以超过大型团队花费数月时间构建的有监督的机器学习项目。本文列出的工具和模式只能算是集成LLM的起点，而不是最终状态。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/\">https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/</a>\"</p>",
    "publish_time": "2023-07-03 15:38:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“火山方舟”MaaS平台亮相：集成七款大模型，抖音等业务团队已试用",
    "url": "https://www.infoq.cn/article/X7lw1CzIdL5BabBJx4lS",
    "summary": "<p>&nbsp;</p><p>6月28日，<a href=\"https://xie.infoq.cn/article/6056428746d7901429d2b8e67\">火山引擎</a>\"发布了大模型服务平台“火山方舟”，面向企业提供模型精调、评测、推理等全方位的平台服务（MaaS，即Model-as-a-Service）。据悉，目前“火山方舟”集成了百川智能、出门问问、复旦大学MOSS、IDEA研究院、澜舟科技、MiniMax、智谱AI（以拼音首字母排序）等多家AI科技公司及科研院所的大模型，并已启动邀测。</p><p></p><p>对于中国的大模型发展，火山引擎有下面几个关键判断：</p><p>&nbsp;</p><p>第一，未来的<a href=\"https://www.infoq.cn/article/FRXOJ4dDWCK34mzO9EDM\">大模型</a>\"市场将是一个百花齐放的多模型生态。由于安全信任、行业壁垒和成本等因素，这个生态将同时并行存在几个少数的超大规模的大模型、多个中等规模的大模型和更多个行业的垂直模型。同时，开放的市场竞争和模型多样性又会进一步促进整个技术发展。</p><p>&nbsp;</p><p>第二，企业，尤其是行业头部企业，自身对于大模型的应用，将是“1 + N”的应用模式，即：企业通过自研或者与三方模型服务商的深度合作，形成企业自身的一个主力模型；在这个主力模型之外的不同场景中，企业还会同时应用N个外部模型。</p><p>&nbsp;</p><p>基于以上判断，火山引擎定位于为模型生产端和模型应用端提供底层能力和产品方案，解决计算、安全、成本等通用问题。据悉，火山方舟由以下几个核心部分构成：</p><p>&nbsp;</p><p>第一个是模型广场，许多模型提供商会提供不同版本/不同尺寸的模型。企业可以直接与这些模型交互。在火山引擎上可以直接调用推理API，接入生产环境。适合进行快速分析和AB实验。</p><p>&nbsp;</p><p>第二个是模型评估。企业要把大模型用好，首先得想好自己的业务需求，并为之设计一套可量化的评估指标，并在不断对比/评估/试验的过程中，积累评估数据、快速迭代。要给多样化的业务场景和业务入口，挑选不同的、最合适的模型。</p><p>&nbsp;</p><p>第三个是模型精调。对于要求较高的垂直场景，用户可能需要利用自有数据和领域非公开数据，进行持续训练，并且建设和积累自己的精调数据集。 对精调的良好运用也有益于对更小尺寸模型的利用， 在特定工作上达到逼近通用大模型的水平，由此降低推理成本。火山方舟可以对一个模型或多个基座模型同时发起训练任务，同时实时跟踪模型精调的效果指标和运行情况。</p><p>&nbsp;</p><p>第四个是安全和信任机制。据悉，火山大模型服务平台提供了基于安全沙箱、硬件加密和联邦学习的多套大模型安全与信任解决方案。吴迪表示，“火山方舟”还在探索基于NVIDIA新一代硬件支持的可信计算环境、基于联邦学习的数据资产分离等多种方式的安全互信计算方案。</p><p>&nbsp;</p><p>随着平台不断迭代， 企业对大模型的资源供应将更为弹性/动态和廉价。火山引擎通过流量错峰、训推一体等手段，将进一步降低推理的单位成本。吴迪表示，这也是在大模型时代，上云所带来的重要优势。除此之外， 火山引擎将和大模型提供商联合进行深度的性能优化，包括火山引擎知名的veLego优化框架，还包括火山引擎将不断和大模型提供商分享/共创的诸多量化/batching/调度等优化技术。</p><p>&nbsp;</p><p>火山引擎总裁<a href=\"https://www.infoq.cn/article/WKwdgBY1f4wdpjgoSDPf\">谭待</a>\"认为，除了算法、数据和算力外，企业做大模型还需要考虑以下问题：</p><p>&nbsp;</p><p>安全与信任。这里的安全和信任是一种双向信任。大模型服务商除了考虑模型方案和代码不被泄露，也需要考虑到各个应用企业的安全问题，保障企业在调用大模型时自己的prompt或精调数据不会被泄露。性价比。之前大家处于模型研发的早期，更多考虑的是模型预训练阶段的成本问题，但后期模型的推理计算才是成本更高的环节。只有推理成本做到足够低，才能让大模型真正被广泛使用。生态系统。生成式模型并不知道自己擅长什么、不擅长什么，需要我们告诉他们自己的优劣势，并且在他们不擅长的地方为其提供下游的模型、API和插件，帮助他们更好地解决问题，帮助大模型“长出手和脚”。</p><p>&nbsp;</p><p>火山引擎智能算法负责人吴迪表示，与其他很多云厂商单纯提供算力、基础底座不同，火山方舟平台一方面会在资源和优化方面跟模型提供商进行非常深度的合作，长期致力于降低推理成本，另一方面，火山算法团队会和客户做重点的应用共建和应用落地，催化整个大模型、大算力在千行百业的落地速度。</p><p>&nbsp;</p><p>据悉，抖音集团内部已有十多个业务团队试用“火山方舟”，在代码纠错等研发提效场景，文本分类、总结摘要等知识管理场景，以及数据标注、归因分析等方面进行探索，利用大模型能力促进降本增效。这些内部实践也在快速打磨“火山方舟”，推动平台能力的进一步完善。“火山方舟”的首批邀测企业还包括金融、汽车、消费等众多行业的客户。</p><p>&nbsp;</p><p>谭待坦言，“火山方舟”还在起步阶段，工具链和下游应用插件需要持续完善，平台还将接入更多大模型，并逐步扩大邀测范围，与企业客户共建开放合作的多模型生态，加速大模型在各行各业的应用落地。</p>",
    "publish_time": "2023-07-03 16:18:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Rust：根据谷歌内部调查，不难学，但编译速度慢",
    "url": "https://www.infoq.cn/article/q7wK7cM5fBvQYmXtqekZ",
    "summary": "<p>Google 发布了一份报告，表明<a href=\"https://www.infoq.cn/article/HdhHwuPQk4FCdPBpmdlP\"> Rust </a>\"并不比其他编程语言难学，尽管它的编译速度很慢。</p><p>&nbsp;</p><p><a href=\"https://opensource.googleblog.com/2023/06/rust-fact-vs-fiction-5-insights-from-googles-rust-journey-2022.html\">报告指出</a>\"，该调查覆盖了内部 1,000 多名Rust 开发人员，涵盖“谷歌雇用的专业软件开发人员（或相关领域）”&nbsp;。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/hRUNuCGVgGwda9jpwajc\">Rust 语言</a>\"因其高性能以及具备 C 和 C++ 等其他系统语言所缺乏的安全保证而受到赞赏，但我们也常听见一些抱怨说，由于所有权和借用等概念，该语言很难学习。Rust 中的所有权是一项编译时功能，可提供安全且自动的内存管理。“由于所有权对于许多程序员来说是一个新概念，因此确实需要一些时间来适应，”Rust 文档<a href=\"https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html\">承认</a>\"。</p><p>&nbsp;</p><p>不过，根据调查，尽管 Rust 存在一些挑战，但开发人员还是能相对较快地掌握该语言。超过 2/3 的受访者认为，他们只要学习 2 个月就能上手 Rust 语言、1/3 的受访者表示他们此前学习了 2 个月后，目前已经能够高效使用 Rust 语言，因此谷歌认为传言中的“Rust 语言至少需要学习 6 个月”并不准确。</p><p>&nbsp;</p><p>报告指出，编写 unsafe 代码和处理 C/C++ 互操作也是开发者经常遇到的问题，但并不是最大的挑战。谷歌认为大家对互操作和unsafe代码的担忧有些过头了。</p><p>&nbsp;</p><p>在接受调查的人中，只有 13% 的人有过 Rust 经验，其中大多数来自 C/C++、Python、Java、Go 或 Dart。“我们没有看到任何数据表明 Rust 相对于任何其他语言存在任何生产力损失，”报告指出。</p><p>&nbsp;</p><p>谷歌认为 Rust 的最大问题是编译速度。报告称，“到目前为止，构建速度慢是开发人员在使用 Rust 时遇到的第一大挑战”。但通过缓解措施，Rust 编译器也能做得很好。</p><p>&nbsp;</p><p>开发人员认为他们用 Rust 编写的代码更正确，比用其他语言编程时更有信心。另外，Rust 的使用量正在增加。最近的 StackOverflow<a href=\"https://survey.stackoverflow.co/2023/#technology-most-popular-technologies\">调查</a>\"将 Rust 在编程语言中排名第 14 位，有 13.05% 的开发人员使用它，仅次于 Go，但高于 Kotlin。</p>",
    "publish_time": "2023-07-03 16:31:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "欧伟杰博士确认出席 ArchSummit 深圳，分享《全自研国产数据库内核架构设计和应用实践》话题",
    "url": "https://www.infoq.cn/article/YM3Z6k0dBX0m2kWRZNVK",
    "summary": "<p>7&nbsp;月&nbsp;21&nbsp;日&nbsp;-&nbsp;22&nbsp;日，&nbsp;在&nbsp;<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">ArchSummit&nbsp;全球架构师峰会（深圳站）</a>\"，深圳计算科学研究院&nbsp;YashanDB&nbsp;研发总监欧伟杰博士，将于会上发表题为《全自研国产数据库内核架构设计和应用实践》的演讲，分享&nbsp;YashanDB&nbsp;如何从理论和工程两方面由&nbsp;0&nbsp;打造数据库系统，并介绍产品的内核架构与核心功能。</p><p></p><p>欧伟杰是武汉大学博士，深圳计算科学研究院&nbsp;YashanDB&nbsp;研发总监，拥有&nbsp;10&nbsp;年以上数据库内核设计与开发经验，曾负责分布式&nbsp;NewSQL&nbsp;数据库研发,&nbsp;拥有多篇顶级会议论文及技术专利，熟悉&nbsp;OLTP，HTAP&nbsp;业务场景及前沿技术趋势，研发的产品服务全球数亿用户。</p><p></p><p>相信通过欧伟杰博士的分享，你将了解到国产数据库产业发展趋势，收获原创数据库内核架构设计思路和不同行业的国产数据库替换经验。</p><p></p><p>除上述议题外&nbsp;，ArchSummit&nbsp;深圳还将围绕<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">基础架构技术</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1532?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">DataOps、Data&nbsp;Fabric&nbsp;等高效数据开发与服务模式</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1534?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">Mesh&nbsp;技术实践案例</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1535?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">QUIC&nbsp;传输和架构优化</a>\"等进行分享。</p><p></p><p>数十位业界专家，上百个国内外一线大厂前沿技术案例，一定会给你带来很多全新的开发灵感。期待与你线下交流！咨询购票请联系&nbsp;18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/9d/aa/9d6a27547062ee2e089f91bdc4ba1eaa.png\" /></p><p></p>",
    "publish_time": "2023-07-03 16:36:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "上汽集团与郑州市政府签署合作框架协议；黑芝麻智能冲刺“自动驾驶计算芯片第一股”；比亚迪与 ATL 汽车集团合作，助力加勒比地区新能源汽车普及；Halo.Car 推出...",
    "url": "https://www.infoq.cn/article/Xf82D8kcmhttfCrUovPu",
    "summary": "<p>上汽集团与郑州市政府签署合作框架协议，启动上汽集团郑州新能源动力电池项目；黑芝麻智能递交港交所主板上市申请，成为“自动驾驶计算芯片第一股”；比亚迪与 ATL 汽车集团合作，为加勒比地区提供新能源乘用车产品；地平线与安波福签署战略合作协议，共同打造智能驾驶软硬件解决方案……本周汽车技术领域又有哪些新动作，一起来看。</p><p></p><p></p><h2>上汽集团与郑州市政府签署合作框架协议，推动新能源汽车发展</h2><p></p><p></p><p>7&nbsp;月&nbsp;2&nbsp;日，<a href=\"https://www.infoq.cn/article/ut4LBWCA6Zx5i6Ca6mE3\">上汽集团</a>\"与郑州市政府签署合作框架协议，启动上汽集团郑州新能源动力电池项目，加速推进新能源汽车发展和基地创新升级。项目规划 30 万套 PACK 产能，投产后年产值超 100 亿元，将为郑州基地带来新能源配套和增长极。郑州基地已入选国家级“绿色工厂”，正在建设光伏发电项目。上汽乘用车将以绿色科技和卓越品质为引领，持续刷新用户出行体验，并加快近地化战略布局，推出更多新能源智能车型。</p><p></p><p></p><h2>比亚迪与 ATL 汽车集团合作，助力加勒比地区新能源汽车普及</h2><p></p><p></p><p>比亚迪与 ATL 汽车集团合作，为加勒比地区提供新能源乘用车产品。比亚迪是全球领先的新能源汽车领导者，全球累计销量超过 430 万辆。ATL 汽车集团是牙买加最大的汽车投资商，具有优秀的经营管理能力和业务水平。双方将合作开设两家门店，分别位于金斯顿和蒙特哥湾，为当地消费者提供销售和售后服务。合作覆盖 10 个国家，包括牙买加在内。双方共同推动牙买加的新能源汽车普及，ATL 汽车集团表示未来比亚迪有望成为加勒比地区排名第一的电动汽车品牌。比亚迪中美洲及加勒比海地区国家经理表示，双方合作是为了实现可持续发展的共同理念，将助力为地球降温 1°C ，促进当地可持续发展。</p><p></p><p></p><h2>新思科技与三星合作开发广泛&nbsp;IP&nbsp;组合</h2><p></p><p></p><p>新思科技近日宣布与三星晶圆厂签署了一项合作升级协议，旨在共同开发广泛的&nbsp;IP&nbsp;组合，以降低汽车、移动、高性能计算（HPC）和多裸晶芯片的设计风险，并加快其流片成功。该协议扩展了双方的合作范围，针对三星的先进工艺如&nbsp;8LPU、SF5、SF4&nbsp;和&nbsp;SF3&nbsp;等，新思科技将提供包括基础&nbsp;IP、USB、PCI&nbsp;Express、112G&nbsp;以太网、UCIe、LPDDR、DDR、MIPI&nbsp;等广泛的&nbsp;IP&nbsp;组合。此外，新思科技还将优化&nbsp;IP&nbsp;以适应三星的&nbsp;SF5A&nbsp;和&nbsp;SF4A&nbsp;汽车工艺节点，以满足严格的一级或二级温度要求和&nbsp;AEC-Q100&nbsp;可靠性要求，助力汽车芯片开发者减少设计工作并加快&nbsp;AEC-Q1100&nbsp;认证。针对&nbsp;ADAS&nbsp;SoC&nbsp;的车规级&nbsp;IP&nbsp;还将包含设计故障模式与影响分析（DFMEA），能够为汽车&nbsp;SoC&nbsp;应用的开发工作节省数月的时间。</p><p></p><p></p><h2>黑芝麻智能成为“国内自动驾驶计算芯片第一股”</h2><p></p><p></p><p>6 月 30 日，黑芝麻智能递交了港交所主板上市申请，成为“自动驾驶计算芯片第一股”，中金公司及华泰金融控股为联席保荐人。</p><p></p><p>黑芝麻智能科技有限公司是一家提供车规级智能汽车计算芯片及解决方案的供应商。公司已推出华山系列高算力芯片和武当系列跨域计算芯片，通过独立研发的IP核、算法和软件驱动的 <a href=\"https://xie.infoq.cn/article/f74b7c4b19c6311fbe35e04d6\">SoC</a>\" 及基于 SoC 的解决方案，为客户提供全栈式自动驾驶能力。根据 2022 年车规级高算力 SoC 的出货量计算，黑芝麻智能已成为全球前三大供应商之一。</p><p></p><p></p><h2>Innoviz&nbsp;与&nbsp;LOXO&nbsp;达成战略合作，为 LOXO 车辆提供激光雷达组件</h2><p></p><p></p><p>近日，一级供应商 Innoviz&nbsp;Technologies 和瑞典零排放自动驾驶送货车供应商 LOXO 签署了一份合作意向书。根据协议，Innoviz 将在 2024 年之前为 LOXO 的车辆提供 InnovizOne 激光雷达组件。InnovizOne 是专为汽车制造商设计的高性能固态激光雷达传感器，可用于各种应用，包括自动驾驶出租车、接驳车和配送公司等。该传感器具有坚固耐用、价格实惠、可靠、节能、轻量化等特点，可无缝集成至 L3 至 L5 自动驾驶汽车中，以确保乘员和行人的安全性。配备 InnovizOne 传感器的 LOXO 车辆将满足日益增长的自动驾驶配送市场需求。据预计，到 2030 年，该市场市值可达 59 亿美元。早些时候，LOXO 与 Innoviz 成功完成了车用激光雷达产品的评估和测试，并扩大了对 InnovizOne 的订单，双方还签署了 2024 年车队合作意向书。</p><p></p><p></p><h2>吉利成立极电电动汽车技术公司</h2><p></p><p></p><p>山东极电电动汽车技术有限公司近日成立，注册资本 9999 万元，法定代表人谢世滨。该公司主要经营范围包括电池零配件生产、汽车零部件及配件制造、电池零配件销售、电池制造和销售。股权穿透显示，山东极电电动汽车技术有限公司为衢州极电电动汽车技术有限公司全资子公司，后者又是浙江吉利产投控股有限公司的全资子公司。</p><p></p><p></p><h2>地平线与安波福达成战略合作，提供集成软硬件解决方案</h2><p></p><p></p><p>近日，地平线与安波福签署了战略合作协议，基于征程系列芯片为安波福及其旗下软件供应商风河公司提供完全集成的软硬件解决方案。征程系列芯片将为风河的多款软件产品提供硬件支持，并已与多家车企建立合作关系。此次与采埃孚的合作是地平线在智能驾驶芯片领域的新突破，为本地智能汽车开发和交付提供优势，以提升消费者的使用体验。</p><p></p><p></p><h2>Halo.Car 推出全球首个商业无人驾驶汽车</h2><p></p><p></p><p>6&nbsp;月&nbsp;29&nbsp;日，&nbsp;Halo.Car&nbsp;宣布推出全球首次商业落地的无人驾驶汽车，首批在拉斯维加斯提供配送服务。这些无人驾驶汽车通过远程驾驶技术实现，客户可以预订车辆并在需要时远程交付和取车。此前，Halo.Car 的车辆始终配备安全驾驶员。无人驾驶车辆使用摄像头、调制解调器、天线和定制开发的组件进行改造，并通过训练有素的远程领航员远程驾驶。车辆使用 T-Mobile&nbsp;5G 进行远程驾驶，同时利用 AT&amp;T 和 Verizon 提高稳定性。Halo.Car 计划逐步扩大在拉斯维加斯的服务范围，并在未来数年内拓展至其他城市。</p><p></p><p></p><h2>indie&nbsp;Semiconductor 推出新汽车无线充电解决方案</h2><p></p><p></p><p>汽车技术解决方案创新者 indie&nbsp;Semiconductor 于 6 月 29 日推出了一款集成式汽车无线充电片上系统（SoC）iND87200 。该系统具有业界最高的集成度，能够简化并加速基于 WPC&nbsp;‘Qi’的车载便携式设备充电设计的开发。由于驾驶员和乘员越来越多地使用智能手机提供导航、音乐、语音连接和其他移动服务，车载充电成为必需品。根据预测，到 2032 年，全球半导体车载无线充电器市场规模预计将增至 95 亿美元，同期复合年增长率为 22% 。新兴的 Qi&nbsp;2.0 标准采用磁功率配置文件（MPP），通过自动将智能手机与感应充电线圈对齐，提供更快、更可靠的充电，在车辆运动中仍能让设备保持在适当的位置。</p><p></p><p>iND87200 采用双核设计，将 Arm&nbsp;Cortex&nbsp;M4F 处理器与用于 WPC 堆栈的专用 Arm&nbsp;Cortex&nbsp;M0 处理器相结合。这种设计释放了 SoC 计算资源，提高了执行用户特定软件的效率。此外，indie 的无线充电解决方案集成了所有必要的电源管理、DC-DC 转换器、信号调节、WPC 逆变器驱动器、功率 FET 以及 LED 和风扇驱动器。该系统还提供了多种串行接口，如 CAN&nbsp;2.0B、LIN、I2C 和 UART，以便于与车辆和其他外设连接。</p><p></p><p>iND87200 完全符合 AEC-Q100&nbsp;2 级标准，已经投入生产并向客户提供样品。除了 SoC，indie 还提供了先进的面向应用的参考设计，以加速客户的开发和部署过程。</p><p></p><p></p><h2>提雅智行发布创新解决方案“&nbsp;fanfare&nbsp;”</h2><p></p><p></p><p>6&nbsp;月&nbsp;27&nbsp;日，提雅智行推出创新解决方案\" fanfare \"，简化和加速自动驾驶电动汽车的生产。该解决方案提供白标电动汽车模型，使客户能够以自己的品牌名称实现车辆商业化，并方便地整合自动驾驶功能。提雅智行还发布了\" L4 级定制设计指南\"，将自动驾驶功能集成到基准电动汽车，并测试是否符合 L4 级标准。他们计划在 2023 年推出小型客车车型，并逐步扩大产品阵容。提雅智行的目标是在 2025 年与合作伙伴一起生产 300 辆支持自动驾驶的电动汽车，并继续加速大规模生产的进程。</p><p></p><p></p><h2>享道 Robotaxi 与赛可智能及安途 AutoX 合作，共建智慧出行平台</h2><p></p><p></p><p>7 月 1 日，享道 Robotaxi 与赛可智能及安途 AutoX 合作，在上海嘉定区域开始示范运营，旨在打造高效的自动驾驶平台，支持上海自动驾驶技术进步和网约车行业发展。通过享道出行 APP，市民可以呼叫自动驾驶车辆，体验智慧出行。享道 Robotaxi 作为国内首个车企背景的 <a href=\"https://xie.infoq.cn/article/a96298fde6debef4930f1017a\">L4</a>\" 自动驾驶运营平台，已在上海等地开展常态化载人示范应用，累计完成超过 20 万单的安全自动驾驶行程。现在与安途 AutoX 合作，推动自动驾驶技术在出行商业场景的落地运营。</p>",
    "publish_time": "2023-07-03 16:49:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一场马斯克的反爬闹剧：Twitter一夜回到五年前？",
    "url": "https://www.infoq.cn/article/1PNitR5uszmmJL7BdXh9",
    "summary": "<p></p><blockquote>没有人相信埃隆·马斯克对推特崩溃的解释了。</blockquote><p></p><p>&nbsp;</p><p>上周末，推特的所有者马斯克（Elon Musk）限制了大多数用户每天可以查看的推文数量，随后推特遭遇了严重技术故障，致使大量用户无法登录。小小改动都能使推特崩溃，这次马斯克会怎么说？</p><p>&nbsp;</p><p></p><h2>从限制登录到服务崩溃</h2><p></p><p>&nbsp;</p><p>上周五，马斯克宣布，如果没有推特账户，就不能再阅读推文。马斯克当时表示，使用人工智能模型的公司们正在大量抓取推特数据，造成了网站流量问题。“这是临时紧急措施，”他说：“几百家机构（也许更多）正在极其积极地搜刮推特数据，以至于影响到了那些普通用户的体验。”</p><p>&nbsp;</p><p>虽然不确定马斯克具体指的是什么，但他很可能说的是从网站提取数据而不需要任何官方 API 的网络爬虫。毕竟，推特的 API 现在受到严格限制，使用者每月至少要花费 42,000 美元。</p><p>&nbsp;</p><p>在此之前，普通用户无需登录帐户即可访问推特，在桌面或移动设备上的网络浏览器中就可以直接打开最喜欢的推文或查看最喜欢的创作者的个人资料。早在 2015 年，推特在一篇博文中透露，每月有“5 亿人”在没有登录的情况下访问推特。据称，这个数字高于推特的月活跃用户数。现在，这部分用户都被挡在了平台外面，不登录就无法查看任何内容。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/066a2d8face1565228a9d6cea5e14d4a.png\" /></p><p></p><p>&nbsp;</p><p>自马斯克接管该平台以来，推特一直遭受用户增长停滞、高级用户流失和广告收入减少的困扰。迫使数亿用户注册推特账号，确实有可能提升用户数据并趁机向潜在广告商推销。然而，这个举措也存在一些重大缺陷，推特将面临的一个主要问题是谷歌等搜索引擎将难以抓取该平台并对其内容进行排名。这意味着当用户在谷歌上搜索时，用户个人资料和推文可能不会再出现。此举也不能保证用户在看到登录提示后会注册推特。一旦大家意识到他们无法访问内容，就可能会完全避开推特内容和链接。这最终可能会让推特损失大量流量。&nbsp;</p><p>&nbsp;</p><p>限制登录是第一步，到了周六，马斯克又出了新措施：“认证帐户每天只能阅读6000个帖子，未认证的帐户每天能看600个帖子；新的未认证的帐户每天能看300个帖子。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d18f72c68e967fa05b4c2bc3cfbdb32c.jpeg\" /></p><p></p><p>&nbsp;</p><p>对于未认证用户来说，600 次推文浏览量，一天随便翻翻各种评论可能就很快用完了。此后上限提高至 1,000/10,000，但这对于非认证用户来说仍然算不上什么。看起来这个举措是用来进一步鼓励 Twitter Blue 订阅，而这种事情也只有马斯克才做得出来。</p><p>&nbsp;</p><p>本来这个改动在技术上也没太复杂，但不幸的是很多人发现推特崩溃了。</p><p>&nbsp;</p><p>根据故障追踪网站Downdetector.com的数据，星期六上午有数千名推特用户的账号出现故障。美国东部时间星期六上午11时17分左右（北京时间星期六晚上11时17分左右）的故障高峰时段，有近7500名用户报告访问推特时出现问题。</p><p>&nbsp;</p><p>更让人无法理解的是，用户都已经无法访问推特首页的信息流了，然而，即使页面无法加载，推特网页仍在不断尝试发送请求。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f4dc2b0f2f5f26f86e01f12940769158.jpeg\" /></p><p></p><p>&nbsp;</p><p>推特每秒向自己的服务器发送约十条请求，试图获取永远无法获得的内容。这可能对工程师们来说是一场灾难，但同时我们也得以欣赏一场史诗级的自我抹黑：推特在不断对自己进行DDoS攻击！</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a7f62b0de6eca1c98e36632981fffc6.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>这是个悲伤的故事，也真实地讲述了前端背后发生的事情：劳累过度的工程师正努力地去满足一个<a href=\"https://www.infoq.cn/article/Gw50bZHLrreF9GAVJJDc\">“白痴”老板的要求</a>\"。</blockquote><p></p><p>&nbsp;</p><p>几乎可以肯定这与他们关闭对推特内容的匿名访问有关。有网友猜测在前端程序中可能存在一个逻辑漏洞，开发人员必须小心翼翼地绕过它，才能使该服务变为私有（a private only service）。</p><p>&nbsp;</p><p>鉴于马斯克有<a href=\"https://www.infoq.cn/article/qhgHLsmYuUuAZC9J4S2Q\">开除员工</a>\"的一系列前例，还有人感慨道：“这次不知道谁又得收拾行李，带着‘荣耀’走出推特大门。”</p><p>&nbsp;</p><p></p><h2>是系统损坏了吗？</h2><p></p><p>&nbsp;</p><p>一开始，马斯克说这是<a href=\"https://www.infoq.cn/article/7ND67G223JxipdDdzlXu\">限制第三方抓取推特数据</a>\"，但后来他开始开玩笑说，该网站瘫痪可以让大家“远离手机，去看看你的家人和朋友。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f97740f4b6b443f5afd15e90652a8e27.jpeg\" /></p><p></p><p>&nbsp;</p><p>但显然大家并不买账，一些有相关技术背景的人认为，这与抓取或机器人预防没有什么关系，而是与试图保持网站的完整性有关。起码大型生产系统自我进行八个小时的DDoS攻击是极其少见的。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b21890daf8f43ff75f0015df0c08f0e9.jpeg\" /></p><p></p><p>&nbsp;</p><p><a href=\"https://threadreaderapp.com/thread/1675331352584134658.html\">Maggie Johnson-Pint</a>\"是一家IT公司的创始人也是一名可靠性方面的专家，也是前推特基础架构团队成员，她对此发表了自己的看法，“作为一名速率限制方面的专家，我想解释下这背后的一些技术。”</p><p>&nbsp;</p><p>大型生产系统中涉及请求数量超出服务能力的事件可以分为两类：&nbsp;</p><p>1.自上而下的过载或“Reddit Hug of Death”：突然出现巨大的需求激增，服务器暂时“无法”运行。这种情况也会发生在超级碗广告之后、流行歌星宣布巡演或 DDOS 攻击期间。&nbsp;</p><p>2.自下而上：这是不太明显但更常见的情况，当系统内部出现故障时，系统将无法提供正常负载。如果你丢失了 Redis 缓存并且所有内容都读取到数据库，这将大大降低服务请求的能力。同样，如果数据库副本、云区域或集群出现故障，照样也会处理不了正常工作负载。当然，如果一项服务的开发人员编写的代码突然攻击另一项服务，那也是“DDOSing Yourself”，但这还是自下而上的。&nbsp;</p><p>&nbsp;</p><p>“我不知道推特今天发生了什么...... 我敢打赌是一些 ‘自下而上’的问题。大家讨论的 DDOSd 问题，这是事情的结果，而不是问题的原因。”</p><p>&nbsp;</p><p>在一些情况下，限速限流是一个逃离死亡的好办法，当然，如果服务器收到的请求数量超过了它们能够处理的数量，最终它们还是会崩溃。&nbsp;即使它们没有崩溃，请求也会堆积起来等待完成，这期间用户还会不断去刷新页面，增加更多请求，陷入死循环中。</p><p>&nbsp;</p><p>“此类最好策略是‘自适应’的，可以根据系统压力、请求优先级和其他因素更改速率限制。推特之前有一个非常好的团队，因为直到一年前他们还拥有一支非常出色的基础设施团队。&nbsp;现在，很多人认为限速是位于基础设施‘前端’的东西，以防止自上而下的问题，但事实上，先进的基础设施团队（包括我之前在推特的美好日子里）会定期在所有进程之间使用它，那么就可以防止各种情况导致的过载情况，从而防止出现各种级联故障场景。”</p><p>&nbsp;</p><p>Maggie猜测是因为推特丢失了关键后端系统的很大一部分：也许他们停止支付 GCP 账单，也许他们丢失了一个关键缓存，并且所有内容都在读取其他数据......</p><p>&nbsp;</p><p></p><h2>控制成本的极端举措？</h2><p></p><p>&nbsp;</p><p>GCP 账单理论也与马斯克有关，之前为了削减成本，他拒绝向亚马逊或谷歌等为推特提供基本服务的公司支付账单。尽管后续行动表明马斯克最终至少支付了谷歌的账单，但现在看来似乎还是在云服务上有些问题存在。</p><p>&nbsp;</p><p>The Information此前报道称，几个月来，推特一直在试图重新跟谷歌谈判新的云服务合同，推特曾承诺在五年内支出 10 亿美元。</p><p>&nbsp;</p><p>根据 The Information 的报道，推特最终继续支付其 Google Cloud 账单，因为它从该公司获得了巨额广告收入，但 Platformer 也有报道称，推特“计划完全停止使用该平台”。Platformer 报道称，推特计划在 6 月 30 日合同结束之前将服务从 Google Cloud 上移走，但援引消息人士的话称，这需要比预期更长的时间，并且不确定某些服务会发生什么情况。</p><p>&nbsp;</p><p>例如，<a href=\"https://techcrunch.com/2018/06/21/twitter-acquires-anti-abuse-technology-provider-smyte/\">推特于 2018 年收购的Smyte公司</a>\"，利用数据分析和机器学习，提供阻止各种不良在线行为的服务，包括针对机器人、黑客攻击、阻止骚扰、滥用和垃圾邮件的工具，该公司托管在谷歌云平台上，也是6月30日到期。当时消息人士告诉 Platformer，并还不清楚推特是否会尝试将 Smyte 迁移到自己的服务器上。</p><p>&nbsp;</p><p>马斯克指责试图获取数据用于人工智能训练的公司，说它们让推特不堪重负。如果仅仅是针对爬虫和机器人，“限流”无疑是控制爬虫的基本方法。特别现在模拟用户的高端爬虫，利用手机或者电脑，一步步去操作，跟真人没有区别。这种情况下，系统很难区别是不是普通用户，识别成本高，而且不一定准确。</p><p>&nbsp;</p><p><a href=\"https://www.poynter.org/fact-checking/2023/banning-donald-trump-and-meeting-elon-musk-former-twitter-safety-chief-gives-inside-account-yoel-roth/\">在马斯克裁掉了推特的绝大部分安全团队成员后</a>\"，增加限制也许是一个最低成本的替代解决方案。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/55/5566ab4e1991dc2d63c582fa895ca03b.jpeg\" /></p><p></p><p>&nbsp;</p><p>当然，并不是所有人都不相信马斯克所说的理由。在他的所有帖子下，都有一大群拥护者说着“伟大的举动，先生，感谢您让网站变得更好！”</p><p>&nbsp;</p><p>也不知道这种“临时”举措会持续多久。推特的竞争对手，现在也因为大批用户涌入而暂停注册。福布斯评价说：“这无疑是马斯克自接管该网站以来所做的最糟糕的事情，反爬虫明显是一个烟幕弹。如果这是真的，那么这实际上就像为了杀死几只蚊子而把整个房子烧掉一样。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://techcrunch.com/2023/06/30/twitter-now-requires-an-account-to-view-tweets/\">https://techcrunch.com/2023/06/30/twitter-now-requires-an-account-to-view-tweets/</a>\"</p><p><a href=\"https://threadreaderapp.com/thread/1675331352584134658.html\">https://threadreaderapp.com/thread/1675331352584134658.html</a>\"</p><p><a href=\"https://sfba.social/@sysop408/110639609718939225\">https://sfba.social/@sysop408/110639609718939225</a>\"</p><p><a href=\"https://www.forbes.com/sites/paultassi/2023/07/02/no-one-believes-elon-musks-explanation-for-breaking-twitter\">https://www.forbes.com/sites/paultassi/2023/07/02/no-one-believes-elon-musks-explanation-for-breaking-twitter</a>\"</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-07-03 18:35:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris 在头部票务平台的应用实践：报表开发提速数十倍、毫秒级查询响应",
    "url": "https://www.infoq.cn/article/ydwuAOiYH4dC0iXsQ0bc",
    "summary": "<p></p><blockquote>随着在线平台的发展，票务行业逐渐实现了数字化经营，企业可以通过在线销售、数字营销和数据分析等方式提升运营效率与用户体验。基于此，国内某头部票务平台为了更好地处理和分析各剧院的票务销售、分销渠道、用户画像等数据，决定引入 <a href=\"http://doris.apache.org/\">Apache Doris </a>\"开启实时数仓构建之旅。本文将详细介绍该票务平台基于 Apache Doris 实时数仓的搭建过程与报表开发场景下的应用实践，并分享实时数仓如何在报表开发和查询两方面提升性能，如何在系统维护和数据处理方面保持低成本运行的收益与成果。</blockquote><p></p><p></p><p>作者｜国内某头部票务平台 大数据开发工程师 刘振伟</p><p></p><p>近年来，文化产业在全球范围内快速发展，成为了经济增长的重要支柱。剧院票务作为文化产业的重要组组成部份，也得到了更多的关注。随着在线平台的发展，票务行业逐渐实现了数字化经营，企业可以通过在线销售、数字营销和数据分析等方式提升运营效率与用户体验。</p><p></p><p>基于此，国内某头部票务平台为了更好地处理和分析各剧院的票务销售、分销渠道、用户画像等数据，决定搭建实时数据仓库，并建设高效快捷的数据分析系统，将系统应用于常规业务报表、敏感数据监控以及票务推荐等应用。希望通过数据报表对剧院票务进行精细化地分析与处理，最终赋能营销策略、掌握市场需求，并达到票务销量增长。本文将详细介绍该票务公司引入 Apache Doris 实时数仓的搭建过程与报表开发场景下的应用实践，并分享在数据导入、数据开发、数据查询与系统运维等方面的收益成果。</p><p></p><h2>为什么引入 Apache Doris</h2><p></p><p>考虑到剧院票务在各类演出上线后会出现订单激增的情况，实时数仓的时效性十分关键。票务平台期望数仓在报表开发和查询两方面能够提供高效性能，同时在系统维护和数据处理方面保持低成本运行。 因此，我们票务平台对于市面上常用于报表开发的数据仓库（Apache Hive、Clickhouse、Apache Doris）进行了详细对比与分析。</p><p></p><p>在初步了解后，首先放弃了 Apache Hive 。主要因为 Apache Hive 是离线数仓，对数据进行批量处理，报表按照 T+1 的调度周期展示结果，无法满足实时数据更新的要求。在进一步了解后也排除了 Clickhouse 选项。一方面 Clickhouse 对 SQL 查询语法不够友好，虽然支持了 Join 语义，但在进行多表 Join 时表现性能低，复杂的关联查询会引起内存溢出，无法满足我们对报表查询的需求。另一方面，Clickhouse 的架构复杂，对于组件依赖严重，容易出现集群稳定性的问题。在面对海量新增数据时，业务人员需要对系统不断进行调优，不仅增加使用成本，还会增加运维管理的难度。</p><p></p><p>因此，在多方面了解和对比后，我们发现 Apache Doris 更符合票务平台业务需求，特别是在使用方式、架构设计、数据导入与处理方面都具有极大优势，具体表现为：</p><p></p><p>简单易用： Apache Doris 基于 MySQL 协议，支持标准的 SQL 查询语法，使开发人员能够快速上手使用。Doris 的架构非常精简，整体部署只有 FE 与 BE 两种角色，并且支持纯净安装，使架构无需再依赖其他组件。灵活配置监控： Doris 通过获取专门的 URL 来制定监控规则以达到优化集群状态和性能监控的目的。通过及时调整 FE、BE 角色的配置参数，始终确保数仓稳定快速地运行。数据模型丰富： 通过使用 Doris 自带的三种数据模型，可以有效地加速 ETL 开发过程。业务人员可以基于不同的数仓分层选用合适的模型来实现高效的数据导入，也可以根据不同的业务场景选择合适的模型进行报表开发。查询性能更优： Doris 的物化视图和物化索引功能可以实现预计算效果，并在命中物化视图时实现快速响应，达到秒级或毫秒级的查询展示。此外，在进行大表 Join 时，Doris 还提供多种优化机制，进一步提升查询效率。</p><p></p><h2>基于 Apache Doris 搭建数据平台</h2><p></p><p></p><h3>如何构建实时数仓</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/64/640d5e258abc9e0c7fbb3a8c2bee1eef.png\" /></p><p></p><p>基于 Apache Doris，票务平台开始了实时数仓构建之旅。我们平台的票务数据主要来自 MySQL 业务库、业务埋点数据、日志数据以及其他数据，在对数据进行采集后，同步至 Apache Kafka 消息队列并通过 Routine Load 导入到 Doris 数仓中。Apache Doris 主要作用于数据仓库分层以及直接应对前端业务报表的查询。如上方架构图所示，实时数仓共分为五层：</p><p></p><p>ODS 贴源层：主要存放未经处理的原始数据结构，与 MySQL 原系统保持一致，是数据仓库的准备区域。我们统一采用 Unique Key 数据模型，能够有效防止数据重复采集，减少任务失败。DWD 明细层：存放维度建模的事实表，对生产数据进行清洗、统一格式、脱敏等，保存各业务过程中最小粒度的操作记录。同样在明细层我们主要采用了 Unique Key 模型，用相同的 Key 进行数据覆盖，实现行级别的数据更新。DWS 汇总层：以明细层数据为基础，依据业务需求划分数据主题（如订单、用户等），将相同粒度数据进行关联合成宽表。该表使用 Unique Key 和 Aggregate Key 两种模型进行数据轻度汇总，为后续的业务查询和 OLAP 分析做准备。ADS 应用层：基于以上三层数据存放各项指标统一结果。主要利用 Aggregate Key 模型进行高度自动聚合，为满足前端人员的具体分析需求，直接提供查询展现。DIM 维表层：在 DIM 层中，主要存放剧院数据、项目数据、场次数据等。在实际应用中，维度数据会结合订单明细数据来进行使用。</p><p></p><p>基于剧院票务的多样外部数据源，采用分层管理方式可以简化数据清洗过程，使每一个数据层都具有其明确的作用域和职责，在使用表时能够更易于定位和理解。开发通用的中间层数据还可以大大减少重复计算，实现准实时数据拉宽。此外，分层提供了统一的数据出口，确保对外输出口径一致，方便后续数据查询与分析。</p><p></p><h3>引入 Flink CDC 全库同步</h3><p></p><p>在数仓应用后，我们对数据接入进行了优化处理，采取 Flink CDC 进行数据同步，实现对新架构稳定接入，进一步减少数据维护成本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/595036d1ece4bd0a8ade6a64326d9b9a.png\" /></p><p></p><p>在业务初期，开发人员使用 DataX 进行外部数据源的全量和增量抽取，以实现离线数据同步，并借助 Canal 解析 MySQL Binlog 进行实时数据的同步。然而，这种方式无法保证数据接入的稳定性。为了解决这一问题，开发人员决定引入 Flink CDC来执行数据同步。为了在短时间内获取业务所需报表，还采取了全库同步的方式对动态新增表进行同步，具体思路如图所示：</p><p></p><p>在 MySQL 数据库中对表管理配置数据进行动态更新。利用 Flink，在 Job 任务中创建两个 CDC 捕获任务。其中一个数据流负责捕获变更数据，另一个广播流负责进行更新配置。在 Sink 端配置所有全库的表，当表新增时，会触发广播流更新配置数据。（ 在 Sink端配置所有全库的表，只配置该表，暂时不用创建对应的表。）</p><p></p><h2>基于 Doris 进行 OLAP 报表开发</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a2af1ac8c5afd786296997203c266d5.png\" /></p><p></p><p>作为剧院票务的管理后台，票务数据平台主要利用 Apache Doris 进行报表开发，提供所需数据分析，以帮助业务人员对票务进行管理，提高票务销量。针对不同的报表场景，业务分析的侧重点有所不同，主要体现在：</p><p></p><p>统计报表： 该报表是业务分析使用频率最高的报表，主要涉及 100 多家剧院的销售数据，包括分销渠道销售明细、销售员销售报表、演出明细报表、纠错报表、场次汇总报表等。敏捷报表： 针对特定活动进行报表开发，业务数据主要来自商业化运营，包括日项目数据汇总、周项目数据汇总、销售额数据汇总、GMV月报数据、平台分销渠道数据、财务结算报表等。数据分析： 显示该剧院的运营情况，包括阅读会员日订单情况、销售收入情况、上座率、会员重复下单数量、用户画像分析等。数据大屏： 主要用于展示订单数据趋势、巨量销售趋势、提供数据视图。</p><p></p><p>根据以上报表场景的特点、使用范围与开发需求，我们选择 Doris 自带的多种数据模型进行高效的报表开发。在满足开发性能要求的同时，还实现了对实时数仓的低成本运维以及低成本存储。Doris 的引入为我们带来了以下具体应用收益：</p><p></p><h3>全面覆盖 OLAP 报表，开发效率极大提升</h3><p></p><p>在最常用的统计报表开发场景中，面对数据量巨大且数据繁杂的情况，我们采用了 Aggregation Key 模型来实现数据的自动聚合。通过使用相同的 Key 对列进行合并，提前进行聚合，大幅提升了开发效率。以销售员销售报表为例，将同一个销售员与按天维度的支付时间作为相同的 Key 值，票房收入作为值来进行自动聚合。一旦数据进入该报表中，即可直接为业务人员所用。在引入 Doris 之前，开发一张报表需要半天甚至一天时间，而现在开发周期缩短至 10 - 30 分钟，有效解决开发周期长的痛点，使开发效率极大提升。</p><p></p><h3>Join + Rollup 强强联合，查询响应达毫秒级</h3><p></p><p>在敏捷报表开发场景中，业务人员时常需要了解活动当天的数据，并在一定周期时间内形成汇总报表对活动进行复盘分析。因此不论是对开发报表的速度，还是对前端人员查询报表时的响应速度都有极高的要求。以 GMV 月报数据为例，我们需要在活动当月对对成交量进行统计汇总，并通过报表分析票务增速，评估活动效果。</p><p></p><p>在前期搭建数仓 DWD 明细层时，我们已经利用 Unique Key 模型实现了数据行级别更新，确保 GMV 报表所需数据的覆盖，无需再花费时间进行开发。在这一基础上，我们还利用了 SQL 多表 Join 进行聚合，借助 Doris Rollup 功能创建物化索引以缩短数据扫描的时间， 加速查询响应。通过两者结合的方式，报表展示从之前的十秒缩短至秒级或者毫秒级，响应速度提升了数十倍。</p><p></p><h3>支持多源异构数据，导数效率大幅提升</h3><p></p><p>数据导入的效率与便捷性是衡量数据仓库最重要的因素之一。我们利用 Doris Insert Into 和丰富的内置导数方式，对本地数据、外部存储数据、Kafka 日志等数据源进行导入，并且在导入数据的同时还可以对其进行列映射、转换和过滤的操作，有效解决了早期导数过程中数据重复采集和不同数据源导致操作复杂性的问题。同时，Doris 对接入源脚本支持了半自动化代码的功能，我们只需要在配置表增加表名，即可快速接入数据，不再需要手工编写脚本，大大提高了导数效率。</p><p></p><h3>架构链路清晰，实现低成本运维</h3><p></p><p>Doris 架构简单，只有 FE 和 BE 两个进程，扩缩容方便快捷，系统升级也非常简单，只需要替换相关的安装包即可。同时，Doris 对集群配置信息和状态信息提供了便捷灵活的管理方式，我们可以通过获取专门的URL，制定监控规则以便及时地调整各类配置参数，时刻保持 Doris 集群稳定快速地运行。以上这些功能都降低了我们在系统运维的成本和难度。</p><p></p><h2>未来规划</h2><p></p><p>当前，我们的票务平台已经基于 Apache Doris 搭建了实时数据仓库，并全面覆盖了报表的开发与分析，帮助剧院后台实时分析销量情况。未来，公司将基于 Doris 不断探索与优化，我们将重点推进以下几个方面的工作：</p><p></p><p>集群优化：加强指标管理体系、数据质量监控体系，对 Doris 集群进行性能优化升级实时拉宽：强数仓血缘关系的管理，使准实时的数据拉宽升级为实时数据拉宽，达到数据高度一致与实时同步。扩大 Doris 使用范围：逐步将实时数仓应用至票务推荐系统，基于 Doris 对用户购买行为和市场趋势推荐对应的产品，进一步提升票务销量。</p>",
    "publish_time": "2023-07-03 18:56:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]