[
  {
    "title": "AI 平民化趋势下，数据架构将被彻底颠覆？",
    "url": "https://www.infoq.cn/article/MridAJdKFPmp4QSPCvfU",
    "summary": "<p>深度学习诞生 10 年，LLM 终于带来 AI 平民化。基于海量数据构建的大模型能够进行相对独立的推理和判断，这让企业看到了 AI 与 Data 的技术融合已经成为当下重要的发展趋势之一。如何让 AI 在各行各业中真正落地，切实提高工作效率已经成为时代新课题。</p>\n<ul>\n<li>那么，大数据与 AI 一体化融合真正实现了吗？</li>\n<li>AI for Data 如今在企业生产中发挥着怎样的价值？</li>\n<li>AI 是否将彻底颠覆现有数据架构？</li>\n</ul>\n<p>《再谈数据架构》第二期，我们重点探讨了 AI 与大数据的融合、落地。</p>",
    "publish_time": "2023-07-03 09:56:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI、数字孪生、数字化精益在工业制造场景如何落地｜直播倒计时2天",
    "url": "https://www.infoq.cn/article/SvZM9Rdcm8PG0MZz1vWI",
    "summary": "<p>AI的热度在近半年来持续飙升，虽然相关技术与制造的融合由来已久，但由于工业制造的行业知识门槛较高，加上数据量级和数据质量的局限性等等，<a href=\"https://www.infoq.cn/theme/187\">AI</a>\"在工业场景的落地及规模化扩散还存在巨大挑战，这些障碍如何一一破除？</p><p></p><p>因元宇宙热度再次被点燃的数字孪生，事实上早已在工业制造领域拥有大量实践。在此基础上，工业软件巨头<a href=\"https://www.3ds.com/zh-hans/?utm_campaign=202306_chi_3dxp_dassaultsystemes_zh_CMP2164_gilt&amp;utm_medium=cpc&amp;utm_source=baidu&amp;utm_content=search&amp;utm_term=site-dassault-brand-dassault-dasuoxitong-pc\">达索系统</a>\"提出的“虚拟孪生”概念与“<a href=\"https://xie.infoq.cn/article/40aa1c9d6b8af73d71a789c37\">数字孪生</a>\"”技术有何异同？又如何覆盖产品全生命周期创造价值？</p><p></p><p>质量、效率、成本三要素之间能否平衡？诞生于上世纪50年代的<a href=\"https://www.infoq.cn/article/8eOkku2uTuU3qVFvwqhA\">精益</a>\"思想如何与数字化充分融合，帮助企业进一步优化各个制造关键环节的工作流程和运营模式？</p><p></p><p>7&nbsp;月&nbsp;5&nbsp;日（周三），最新一期《超级连麦.数智大脑》，邀请了来自艾聚达、达索系统、青岛中集的资深专家，将带你一起揭秘头部面板制造商友达光电的智能化实践与AI扩散策略，解读产品全生命周期的虚拟孪生应用案例，以及全球最大规模冷藏箱生产基地的数字化精益实践。</p><p></p><p>点击链接或扫描下方二维码即可预约报名：<a href=\"https://live.infoq.cn/room/1803\">https://live.infoq.cn/room/1803</a>\"</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6a/77/6aaab576d75ff0db94228c6e4d8da977.jpg\" /></p><p></p><p></p><h2>直播时间</h2><p></p><p>7&nbsp;月&nbsp;5&nbsp;日（周三）&nbsp;19:30-22:00</p><p></p><h2>联合出品</h2><p></p><p>InfoQ&nbsp;x&nbsp;TGO鲲鹏会&nbsp;x&nbsp;极客时间企业版</p><p></p><h2>直播议程</h2><p></p><p></p><h4>19:30-20:00&nbsp;&nbsp;主题分享-AI驱动的制造变革：揭密成功案例与关键策略</h4><p></p><p></p><p>嘉宾介绍：赖骏凯，艾聚达总经理。友达昆山智能制造负责人，大数据与人工智能专家，省级两化融合特聘讲师，重庆邮电大学先进制造工程学院客座教授。</p><p></p><p>分享大纲：</p><p>1.&nbsp;变革背景与价值创造</p><p>2.&nbsp;AI&nbsp;应用成功案例</p><p>3.&nbsp;AI&nbsp;扩散关键策略</p><p>4.&nbsp;总结-提升新型竞争力</p><p></p><h4>20:00-20:30&nbsp;主题分享-虚拟孪生如何助力制造业高质量发展</h4><p></p><p></p><p>嘉宾介绍：杨宠，达索系统技术咨询部业务咨询高级经理。华中科技大学工学博士。曾供职于盖德信息技术有限公司&nbsp;、国际商业机器有限公司（IBM）&nbsp;、参数技术软件有限公司（PTC）&nbsp;，为多家制造业企业提供过业务咨询和数字化咨询服务。</p><p></p><p>分享大纲：</p><p>1.&nbsp;虚拟孪生的概念</p><p>2.&nbsp;产品全生命周期的虚拟孪生</p><p>3.&nbsp;虚拟孪生应用案例及未来展望</p><p></p><p></p><h4>20:30-21:00&nbsp;主题分享-数字化精益实践</h4><p></p><p></p><p>嘉宾介绍：耿峰，青岛中集冷藏箱CIO。坚守制造业信息化一线工作十余年，主持实施过公司ERP、CRM、PLM、SRM、HR、服务器虚拟化、一卡通、条形码、数据防泄漏、桌面安全、私有云、企业内控体系、数字化工厂等项目。几乎涵盖制造业信息化的全部范畴。是国内较早的实战派云计算专家，第一批将虚拟化技术引进企业的CIO，近些年专注数字化精益制造工厂的探索于实战。</p><p></p><p>分享大纲：</p><p>精益数字化背景精益推行的业务痛点数字化精益的价值数字化精益企业建设保障</p><p></p><h4>21:00-21:40&nbsp;连麦对话</h4><p></p><p></p><h4>工业“智”造转型，如何评估投入产出比</h4><p></p><p>主持人：高玉娴&nbsp;InfoQ极客传媒数字化主编</p><p>对话嘉宾：赖骏凯&nbsp;艾聚达总经理/&nbsp;杨宠&nbsp;达索系统技术咨询部业务咨询高级经理/&nbsp;耿峰&nbsp;青岛中集冷藏箱CIO</p><p></p><p>话题方向：制造企业具体如何在转型制造过程中让技术的价值最大化，又如何平衡好效率、质量和成本的关系呢？本议题将与各位专家连麦，探讨工业“智”造转型过程中的经济效益问题。</p><p></p><p>&gt;&gt;&gt;直播报名通道：<a href=\"https://live.infoq.cn/room/1803\">https://live.infoq.cn/room/1803</a>\"</p><p></p><h4>关于《超级连麦.&nbsp;数智大脑》</h4><p></p><p><a href=\"https://www.infoq.cn/theme/192\">超级连麦.数智大脑</a>\"是&nbsp;InfoQ&nbsp;重磅推出的一档连麦直播栏目，由数字化领域的思想领袖、技术大咖及企业先行者等多方连线，聚焦企业数字化实践、数字人才培养、数字化技术管理等话题，剖析和拆解典型数字化场景及其痛点，意在帮助各行业企业扫清转型障碍、探寻变革路径。</p><p></p><p>每期直播将邀请&nbsp;2-3&nbsp;位嘉宾围绕同一话题，从不同维度展开深度讨论。我们希望能够深入行业最核心的话题，通过充分的现场碰撞和探讨，对关键问题进行深刻的剖析，既有理论也有实践，既有战略也有战术，为观众呈现更为全方位、多视角，更有代入感、更具启发性的数字化转型参考。</p><p></p><p>欢迎添加小助手加入「数字化读者群」，获取直播回放链接及相关资料。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d360329131fc348202c171aa0e8213a9.jpeg\" /></p><p></p>",
    "publish_time": "2023-07-03 11:05:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Maven揭秘，逃离依赖地狱",
    "url": "https://www.infoq.cn/article/J8FikQMqTwgRE1a73LLv",
    "summary": "<p>在<a href=\"https://www.devoxx.co.uk/talk/?id=1997\">英国 Devoxx 的演讲</a>\"中，JFrog 的开发倡导者&nbsp;<a href=\"http://www.linkedin.com/in/ixchelruiz\">Ixchel Ruiz</a>\"&nbsp;和 Oracle 首席产品经理&nbsp;<a href=\"https://www.linkedin.com/in/aalmiray/\">Andres Almiray</a>\"&nbsp;共同介绍了多个“Maven 难题”，以及摆脱<a href=\"https://maven.apache.org/\">阿帕奇 Maven</a>\"&nbsp;“依赖地狱”的可能解决方案。这次演讲中涉及了直接、传递、父级&nbsp;<a href=\"https://maven.apache.org/pom.html\">POM</a>\"，以及<a href=\"https://en.wikipedia.org/wiki/Bill_of_materials\">物料清单（BOM）</a>\"的导入。</p><p></p><p>Ruiz 以对工具价值的思考展开了演讲：</p><p></p><p></p><blockquote>Ruiz：作为开发者，如果我的工具在做它该做的，我会感觉很好。</blockquote><p></p><p></p><p>在清楚了解“好工具”时，“魔法”就会发生。正如标题所述，这次演讲主题时关于构建工具，更确切地说，是关于阿帕奇 Maven 的构建工具。</p><p></p><p>据&nbsp;<a href=\"https://jfrog.com/artifactory/\">JFrog 制品仓库（Artifactory）</a>\"的统计数据和&nbsp;<a href=\"https://www.jrebel.com/resources/java-developer-productivity-report-2022\">JRebel</a>\"&nbsp;或&nbsp;<a href=\"https://www.jetbrains.com/lp/devecosystem-2022/\">JetBrains</a>\"&nbsp;的开发者生产力调查，Maven 仍是主流 Java 开发者的构建工具，市场份额分别占据68%和73%。</p><p></p><p>作为&nbsp;<a href=\"https://gradle.org/\">Gradle</a>\"&nbsp;的长期支持者，Almiray 称即使是 Gradle 或&nbsp;<a href=\"https://www.scala-sbt.org/\">sbt</a>\"&nbsp;也或多或少地依赖 Maven。因此，即使是想要以自己的方式编码，人们仍然需要通过 POM 格式解决依赖关系。而有时这些东西又不会如人预期一般正常工作，或是说像是经典的 Maven 一样运作。</p><p></p><p>在一个新克隆的 maven 项目上，你首先会做什么？演讲者以这个问题为切入，开始了对问题的回答。<a href=\"https://github.com/aalmiray/mvn-clean-install\">Almiray 鼓励</a>\"观众在除项目依赖于其他远程项目的情况下，<a href=\"https://andresalmiray.com/maven-verify-or-clean-install/\">将本能的“mvn clean install”替换为“|mvn verify”</a>\"，因为“检查（verify）”是&nbsp;<a href=\"https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html\">Maven 生命周期中</a>\"“安装（install）”的上一步，是通过构建并运行测试进行项目验证的。而安装则仅仅是将构建（编译和打包）结果从文件系统（构建的位置）复制到仓库。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1warmup-questions-1685857753292.jpg\" /></p><p></p><p>随后，演讲进入了互动问卷环节，由听众对问卷内容进行实时回复。在问题的描述中，如果依赖的坐标（即 groupId、artificatId）相同但版本号不同，那么麻烦就会出现。</p><p></p><p>主持人在“暖场问题”中确定了听众所使用的 Maven 版本、安装方式，以及是否使用&nbsp;<a href=\"https://github.com/apache/maven-mvnd\">Maven 守护进程</a>\"（daemon）。针对听众的选择，二位演讲者建议使用&nbsp;<a href=\"https://sdkman.io/\">SDKMAN</a>\"（即使是在两个不同终端窗口内，也允许使用两套不同版本的 Java），用于提升速度的 Maven 守护进程，以及新版本&nbsp;<a href=\"https://maven.apache.org/docs/3.9.2/release-notes.html\">Maven 3.9.x</a>\"&nbsp;以进入更为颠覆性的&nbsp;<a href=\"https://maven.apache.org/ref/4.0.0-alpha-5/\">4.x 版本</a>\"。</p><p></p><p>在演讲的问答阶段，二位演讲者以<a href=\"https://github.com/google/guava\">谷歌 Guava 依赖</a>\"为例，但理由不是因为“大家都恨 Guava，而是因为大家都用 Guava”。他们提出了多个场景、问题、解答，以及优化和解决方案的相关建议，并将场景分为了三部分：单 POM 文件的依赖性、对父 POM 文件的依赖性，以及 BOM 导入。</p><p></p><h2>单 POM 文件的依赖管理</h2><p></p><p></p><p>第一种情况，连续声明了两个不同版本 Guava 的简单项目，哪一个会被采用？听众的回答几乎是在两个版本和构建错误之间五五开。虽然这些看起来仅仅是个简单的规则应用，但由于目前存在的众多版本和插件组合，事情往往会变得更加复杂。比如，这种情况在 Maven 4.x 中会出现构建错误，但在 3.x 中仅会以警告的形似出现。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1Screenshot%202023-06-05%20at%2008.17.07-1685943669814.png\" /></p><p></p><p>这个问题的答案是，在 28.0 版本的 Guava 中会被解决，但因为该版本号不够高，Maven 永远会采用最后一个声明的依赖。在加上 Maven 无法理解版本号，这些在它眼里仅仅只是字符串。为确保这些情况不会再发生，Ruiz 和 Almiray 建议在 Maven 4.x 版本普及之前采用&nbsp;<a href=\"https://maven.apache.org/enforcer/enforcer-rules/banDuplicatePomDependencyVersions.html\">Maven 增强（enforcer）插件中的“禁止重复 POM 版本号规则”</a>\"。首次使用这个插件时大概会非常痛苦，因为该规则会生成一个构建错误，迫使你为项目选择合适的版本。</p><p></p><p>下一个问题是对上一个问题的改版，将第二个依赖换为了更高版本的传递性依赖。即使第二个也是 POM 文件里最后声明的依赖版本号更高，直接的“依赖版本总会赢”。Almiray 提及阿帕奇 Maven 的前主席 Robert Scholte 曾强调，Maven 这项工具<a href=\"https://twitter.com/rfscholte/status/1243226856914063360?s=12&amp;t=p1k9i_gjO2cP_qDbAQnCVw\">无法理解语义上的版本号划分</a>\"，它只认得依赖在“图中的位置”。此外，依赖图中同一依赖的不同版本可能会导致应用程序时不时的崩溃。为避免这类情况的发生，<a href=\"https://maven.apache.org/enforcer/enforcer-rules/dependencyConvergence.html\">可使用 Maven 增强插件中的“依赖收敛规则”</a>\"，以确保版本号的一致。如果需要强调版本号在语义上的一致性，可使用<a href=\"https://maven.apache.org/enforcer/enforcer-rules/requireUpperBoundDeps.html\">增强规则“需求依赖项上界”</a>\"，该规则可给出依赖图中的可用新版本。将两项规则相结合后，就能得知同一依赖的两个版本，以及依赖的可用新版本。</p><p></p><p>第二种情况则引入了依赖管理的概念，其原理更像是查找表。在 Maven 构建图时，会在表中搜索匹配的依赖（artifactId 及 groupId），并选择其所定义的版本。第一个例子中只有依赖管理块和通过<a href=\"https://github.com/google/truth\">谷歌 Truth</a>\"&nbsp;获取的横向依赖，而第二个例子中则额外增加了直接依赖的内容。在例子一中，定义在依赖管理块中定义的版本号会“赢”，而例子二中则是直接依赖“赢”，也就是说“无论是在图中哪里定义的，直接依赖总会赢”。</p><p>另一个例子中则使用了两个依赖，二者均带来了与根距离相同的传递性依赖（“这点很重要”），而再将依赖管理块加进来又会让情况有所不同。听众们认为可以将直接依赖的情况套用，所以图中最后定义的依赖会“赢”，但传递性依赖的情况却是恰恰相反，即第一个库中、第一且最近的传递性依赖将“赢”。在这种情况下，由于这两个依赖距离相同，所以 Guice 带来的横向依赖（Guava 30.1）会赢。在依赖块加入后，其所定义的版本号将“赢”。</p><p></p><h2>父 POM 文件依赖</h2><p></p><p></p><p>在这段演讲中，二位演讲者又在依赖管理中加入了新的复杂因素：父 POM。每个 POM 都可以有一个父 POM，并为依赖管理带来不同的背景。父级关系是在子级层面定义的，父级不会知道任何继承自己子级的信息。每个 POM 都会有一个父级，如果没有明确定义，那么父级将默认成为 super POM。而 Maven 之所以能在只有基本插件的情况下构建项目，是因为 super POM 中包含了所有需要插件。</p><p></p><p></p><blockquote>Almiray：单个 POM 文件很好处理，但难免会出现多个 POM 文件的情况。Ruiz：当然，这时候事情就好玩了。</blockquote><p></p><p></p><p>在另一个例子中，情况一是父系中定义一个直接依赖，子系中定义一个传递依赖；情况二中则新增了一个依赖管理块。因为父 POM 是直接导入到子 POM 中，我们可以随时回到前一阶段，即有直接依赖、传递依赖，以及依赖块的单 POM，而正如所料，“直接依赖总会赢”。唯一的不同则是生效的 POM 同时也会导入父 POM 中的内容。因此，依赖是否能被解决取决于其在生效 POM 中的位置。</p><p></p><h2>BOM 依赖</h2><p></p><p></p><p>演讲中使用的最后一个概念是物料清单（BOM），是与别称“安全物什”的软件物料清单（SBOM）不同的。虽然没有对 BOM 的官方定义或分类，但据 Almiray 的说法，BOM 可以分为以下两类：</p><p>库 BOM：定义了项目与单一库的关联。举例来说，JUnit 或 Jackson BOM 定义且仅定义了一切与 JUnit 相关。堆栈 BOM： Spring 或 Quarkus BOM 可当前项目提供其运行所需的各个项目中的全部依赖关系，每个 DOM 中都包含有最合适运行的依赖版本组合。</p><p></p><p></p><blockquote>Ruiz：就算是没用过，也一定消费过（BOM）。如果用 Spring Boot 或 Quarkus 导入东西，必然会带来更多的依赖……</blockquote><p></p><p></p><p>无论你用的是哪类 BOM 最终的消费形式都是一样的：通过依赖管理块（Ruiz 称这是依赖管理块存在的意义）。消费 BOM 不仅需要 artifactID 和 groupID，还需要添加 POM 类型，否则 Maven 只会试图将其解析为 JAR 文件。POM 文件之中只有元数据。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1Screenshot%202023-06-09%20at%2008.54.00-1686290942713.png\" /></p><p></p><p>下面这个问题是前一个的变体。其中包含一个父级 POM 一个子级 POM，生效的 POM 有两个过渡性依赖，父级中的依赖管理模块有一个依赖和一个导入 BOM 的依赖（如上图所示）。另一个问题中则又在其中加入了直接依赖（无图）。生效的 POM 文件将引用父级中定义的依赖块，而 Guava 所选择的版本也是在这个依赖块中定义的。因此，第一个问题中生效的会是 28.0-jre。但对第二个问题而言，演讲者们给自己不断强调的“直接依赖总会赢”的说法打上了补丁，“……除非 BOM 文件是导入的，否则其他都会被忽略”。也就是说，情况二中依赖管理块中使用的版本是 26.0-jre。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/en/resources/1Screenshot%202023-06-09%20at%2012.38.27-1686304304944.png\" /></p><p></p><p>另一种情况中（如上图），父级和子级 POM 中都有定义依赖管理模块。子级的 POM 中包含导入的 BOM 和依赖。在这种情况下，子级定义的版本与所用的版本更接近，因此会直接覆盖父级中所定义的依赖版本。也就是说，子级 POM 的依赖块中定义的依赖会被使用，即 26.0-jre。如果父级中有一个依赖块，子级中的依赖块导入了一个 BOM、一个传递性依赖和直接依赖，那么直接依赖将会笑到最后，即 28.0-jre。</p><p></p><p>在演讲的最后，Ruiz 和 Almiray 给出了演讲中的关键点。她提到了依赖的采用在很大程度上取决于其在 POM 文件内定义的位置，并再次强调了使用 Maven 增强插件的重要性，以确保明确规则的定义和遵循。因此，在依赖定义文件中的不同位置新增一个条目并不会改变构建过程的输出。</p><p></p><p></p><blockquote>Almiray：如果要从这次演讲中学到什么，那就是去使用 Maven 增强插件，并从今天开始，正确地开始项目构建……</blockquote><p></p><p></p><p>Ruiz 在结束语中再次重申了 Maven 依赖的解决规律：直接依赖总会赢，传递性依赖会选择最先也是最近的。依赖管理解析会以目录的形式使用。对 BOM 文件而言，就算不知道也在用。Almiray 最后称，万不得已可以用排除法，只要用 Maven 构建，依赖管理块就是好工具。但如果用不同的消费者，你就需要将所有不同类型的依赖块转换为显示依赖。Maven flatten 可以做到这点。对 Gradle 而言则更是需要，因为 Gradle 除了直接依赖关系之外，不支持任何其他东西。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/maven-puzzlers-devoxxuk/\">Ruiz and Almiray at Devoxx UK: Lessons on How to Escape the Maven Dependency Hell</a>\"</p>",
    "publish_time": "2023-07-03 11:20:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何在英特尔® 平台上实现高效的大语言模型训练后量化",
    "url": "https://www.infoq.cn/article/GwojGAdYA5rfpzQpDuck",
    "summary": "<p>本文介绍了可提升大语言模型的训练后量化表现的增强型SmoothQuant技术，说明了这项技术的用法，并证明了其在准确率方面的优势。此方法已整合至<a href=\"https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/neural-compressor.html\">英特尔® Neural Compressor</a>\"(1)&nbsp;中。英特尔® Neural Compressor是一个包含量化、剪枝（稀疏性）、蒸馏（知识提炼）和神经架构搜索等多种常用模型压缩技术的开源Python库。目前，诸如TensorFlow、<a href=\"https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-tensorflow.html\">英特尔® Extension for TensorFlow</a>\"(2)、PyTorch、<a href=\"https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-pytorch.html\">英特尔® Extension for PyTorch</a>\"(3)&nbsp;、ONNX Runtime和MXNet等主流框架，都能与之兼容。</p><p>&nbsp;</p><p>英特尔® Neural Compressor 已经支持多款英特尔®架构的硬件，比如<a href=\"https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/scalable.html\">英特尔®至强®可扩展处理器</a>\"(4)、<a href=\"https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/max-series.html\">英特尔®至强® CPU Max系列</a>\"(5)、<a href=\"https://www.intel.cn/content/www/cn/zh/products/details/discrete-gpus/data-center-gpu/flex-series.html\">英特尔®数据中心GPU Flex系列</a>\"(6)和<a href=\"https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html\">英特尔®数据中心GPU Max系列</a>\"(7)。本文涉及的实验基于<a href=\"https://www.intel.cn/content/www/cn/zh/events/accelerate-with-xeon.html\">第四代英特®至强®可扩展处理器</a>\"(8)进行。</p><p></p><h1>大语言模型</h1><p></p><p></p><p>大语言模型(Large Language Model, LLM)需基于海量数据集进行训练，可能拥有数十亿权重参数。其先进的网络结构和庞大的参数量，使它们能够很好地应对自然语言本身的复杂性。完成训练后的大语言模型，可针对各种下游的自然语言处理(NLP)和自然语言生成(NLG)任务进行调优，让其更适合对话式聊天机器人（如ChatGPT）、机器翻译、文本分类、欺诈检测和情感分析等任务场景。</p><p></p><h1>大语言模型部署面临的挑战</h1><p></p><p></p><p>大语言模型在执行自然语言处理和自然语言生成任务方面表现出色，但其训练和部署颇为复杂，主要面临以下挑战：</p><p>&nbsp;</p><p>·&nbsp;<a href=\"https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8\">AI与内存墙</a>\"(9)瓶颈问题：算力每两年提高3.1倍，内存带宽却只提高1.4倍；</p><p>·&nbsp;网络带宽挑战：训练大语言模型需要采用分布式系统，这对网络带宽提出了较高要求；</p><p>·&nbsp;系统资源有限：训练后的模型往往会部署在算力和内存资源均有限的系统上。</p><p>&nbsp;</p><p>因此，采用训练后量化的方法来为大语言模型瘦身，对于实现低时延推理至关重要。</p><p></p><h1>大语言模型的量化</h1><p></p><p></p><p>量化是一种常见的压缩操作，可以减少模型占用的内存空间，提高推理性能。采用量化方法可以降低大语言模型部署的难度。具体来说，量化是将浮点矩阵转换为整数矩阵：</p><p><img src=\"https://static001.geekbang.org/infoq/08/08ad944f942d6c96e14bc700c128d6e6.png\" /></p><p>其中X_fp32、S和Z分别为输入矩阵、比例因子和整数零点。</p><p>&nbsp;</p><p>有关每通道(per-channel)量化策略虽然可能会减少量化损失，但不能用于激活值量化的原因，请参看<a href=\"https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md\">SmoothQuant相关文档</a>\"(10)。不过，激活值量化误差损失却是导致模型量化准确率下降的重要因素。为此，人们提出了很多方法来降低激活值量化损失，例如：<a href=\"https://arxiv.org/abs/2203.14642\">SPIQ</a>\"(11)、<a href=\"https://arxiv.org/abs/2209.13325\">Outlier Suppression</a>\"(12)和<a href=\"https://arxiv.org/abs/2211.10438\">SmoothQuant</a>\"(13)。这三种方法思路相似，即把激活值量化的难度转移到权重量化上，只是三者在转移难度的多少上有所不同。</p><p>&nbsp;</p><p>增强型SmoothQuant</p><p></p><p>SmoothQuant引入了一个超参数α作为平滑因子来计算每个通道的量化比例因子，并平衡激活值和权重的量化难度。</p><p><img src=\"https://static001.geekbang.org/infoq/d3/d38b57f1c9191093064f77f2bd652aa7.png\" /></p><p></p><p>其中j是输入通道索引。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/060c16a61d87a506b5e136120ec1c7b6.png\" /></p><p>&nbsp;</p><p>对于OPT和BLOOM等大多数模型来说，α=0.5是一个能够较好实现权重和激活值量化难度分割的平衡值。模型的激活异常值越大，就越需要使用更大的α值来将更多的量化难度转移到权重上。</p><p>&nbsp;</p><p>原始的SmoothQuant旨在通过针对整个模型使用一个固定值α来分割权重和激活值的量化难度。然而，由于激活异常值的分布不仅在不同模型之间存在差异，而且在同一模型的不同层之间也不尽相同，因此，本文推荐使用英特尔® Neural Compressor的自动调优能力，逐层获取最佳α值。</p><p>&nbsp;</p><p>相关方法包括以下五个主要步骤（伪代码如下所示）：</p><p>1.&nbsp;通过特殊的回调函数register_forward_hook捕获(hook)模型各层的输入和输出值。</p><p>2.&nbsp;根据用户定义的α范围和步长生成一个α值列表。</p><p>3.&nbsp;根据给定的α值重新计算平滑因子并调整参数（权重值和激活值）。</p><p>4.&nbsp;对权重执行每通道量化与反量化(quantization_dequantization)，对输入值执行每张量(per-tensor)量化与反量化，以预测与给定α值对应的每层输出值。</p><p>5.&nbsp;计算相对实际输出值的均方损失，将调整后的参数恢复回来，并保存每层的最佳α值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6e09bc08f2d0b75da0490760578ffdc.png\" /></p><p></p><p>本文提出的方法支持用多个标准（如最小值、最大值和平均值）来确定Transformer块的输入层归一化(LayerNorm)操作的α值。实验发现，将α范围设为[0.3, 0.7]，步长设为0.05，对大多数模型来说都能达到很好的平衡。</p><p>&nbsp;</p><p>这一方法有两个显著特点：一是全自动化，二是比原始方法支持的融合模式多。</p><p>&nbsp;</p><p>下图提供了在BLOOM-1b7模型上执行SmoothQuant α值自动调优的样例代码：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/703cb16fe2a013b61e1e7d34c7bc681b.png\" /></p><p>启用增强型SmoothQuant的样例代码</p><p>&nbsp;</p><p>用户只需传递一个模型名称(model_name)和一个数据加载器。值得注意的是，模型分析主要依靠的是Torch JIT。用户可以在加载<a href=\"https://huggingface.co/models\">Hugging Face模型</a>\"(14)时将torchscript设置为True，或将return_dict设置为False。更多信息请参阅<a href=\"https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md\">英特尔® Neural Compressor文档</a>\"(10)。</p><p></p><h1>结果</h1><p></p><p></p><p>本文提出的增强型SmoothQuant的主要优势在于提高了准确率。</p><p>&nbsp;</p><p>经过对多种主流大语言模型的评估，具备自动调优能力的INT8 SmoothQuant最后一个词元(last-token)的预测准确率要高于原始INT8 SmoothQuant和FP32基线方法。详见下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/835dd9d13030d4f78281ebeca6426cdc.png\" /></p><p></p><p>FP32基线方法、INT8（启用和不启用SmoothQuant）以及INT8（启用本文提出的增强型SmoothQuant）的准确率对比</p><p>&nbsp;</p><p>从上图可以看出，在OPT-1.3b和BLOOM-1b7模型上，本文提出的增强型SmoothQuant的准确率比默认的SmoothQuant分别高5.4%和1.6%。量化后的模型也缩小到FP32模型的四分之一，大大减少了内存占用空间，从而有效地提升大模型在英特尔®平台上的推理性能。</p><p>&nbsp;</p><p>更全面的结果请见<a href=\"https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md\">GitHub存储库</a>\"(10)。同时，也欢迎您创建拉取请求或就<a href=\"https://github.com/intel/neural-compressor/issues\">GitHub问题</a>\"(15)发表评论。期待听到您的反馈意见和建议。</p><p>&nbsp;</p><p>作者：</p><p>英特尔公司人工智能资深架构师沈海豪、英特尔公司人工智能资深软件工程师程文华、英特尔公司人工智能软件工程师陆崟彤、何欣、郭恒、王畅、王梦妮，他们都在从事模型量化及压缩的研究与优化工作。</p><p>&nbsp;</p><p>注释：</p><p>1、&nbsp;英特尔® Neural Compressor</p><p>https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/neural-compressor.html</p><p>2、&nbsp;英特尔® Extension for TensorFlow</p><p>https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-tensorflow.html</p><p>3、&nbsp;英特尔® Extension for PyTorch</p><p>https://www.intel.cn/content/www/cn/zh/developer/tools/oneapi/optimization-for-pytorch.html</p><p>4、&nbsp;英特尔®至强®可扩展处理器</p><p>https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/scalable.html</p><p>5、英特尔®至强® CPU Max系列</p><p>https://www.intel.cn/content/www/cn/zh/products/details/processors/xeon/max-series.html</p><p>6、英特尔®数据中心GPU Flex系列</p><p>https://www.intel.cn/content/www/cn/zh/products/details/discrete-gpus/data-center-gpu/flex-series.html</p><p>7、英特尔®数据中心GPU Max系列https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html</p><p>8、第四代英特®至强®可扩展处理器</p><p>https://www.intel.cn/content/www/cn/zh/events/accelerate-with-xeon.html</p><p>9、AI与内存墙</p><p>https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8</p><p>10、SmoothQuant相关文档/英特尔® Neural Compressor文档&nbsp;/ GitHub存储库</p><p>https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md</p><p>11、SPIQ</p><p>https://arxiv.org/abs/2203.14642</p><p>12、Outlier Suppression</p><p>https://arxiv.org/abs/2209.13325</p><p>13、SmoothQuant</p><p>https://arxiv.org/abs/2211.10438</p><p>14、Hugging Face模型</p><p>https://huggingface.co/models</p><p>15、GitHub问题</p><p>https://github.com/intel/neural-compressor/issues</p>",
    "publish_time": "2023-07-03 11:36:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "用基于英特尔® SGX 的可信执行环境有效应对大语言模型隐私和安全挑战",
    "url": "https://www.infoq.cn/article/3yHIZDfmJI50IftLTohG",
    "summary": "<p></p><h2>I. 可信执行环境是什么？大语言模型为什么需要它？</h2><p></p><p></p><p>OpenAI 的&nbsp;GPT 系列大语言模型（Large Language Mode，以下缩写为&nbsp;LLM）的兴起与应用，也带来了诸如数据泄露、数据滥用、模型被攻击和知识产权被窃取等一些列隐私和安全风险或挑战。</p><p>&nbsp;</p><p>可信执行环境（Trusted&nbsp;Execution&nbsp;Environment，以下缩写为&nbsp;TEE）是一项基于软硬件组合创建安全执行环境，能够更好地确保计算和数据处理机密性和完整性。其关键机制为：</p><p>&nbsp;</p><p>·&nbsp;安全隔离：通过硬件加密和内存隔离等硬件隔离技术，将敏感数据和关键代码与其他应用及操作系统相隔离，从而确保它们即使在系统其他部分被攻击或受到恶意软件影响时也能够得到更好的保护。</p><p>·&nbsp;安全验证：在启动过程中进行身份验证和完整性检查，确保只有经过授权的代码和数据可以在其中运行，以此防止恶意软件或未经授权的访问。</p><p>·&nbsp;安全执行环境：提供包含加密算法、安全协议和密钥管理等防护功能的执行环境，用于处理敏感数据和执行关键算法，以增强数据在执行过程中的保密性和完整性。</p><p>&nbsp;</p><p>TEE 与LLM 可在多行业、多场景融合，TEE 可用于为&nbsp;LLM 提供颇具商业落地价值的隐私和数据保护创新解决方案。</p><p></p><h2>2. LLM 与TEE&nbsp;的融合需求</h2><p></p><p>&nbsp;</p><p>LLM 在许多行业的不同场景都有着广泛应用，例如金融行业的风险评估和交易分析，医疗保健领域的医学图像识别、病历纪录和疾病预测，以及法律和合规行业的法律咨询、合同审查和文书处理等。这些行业或场景中涉及到的数据多为重要敏感的交易数据或个人数据，必须得到有效保护。</p><p>&nbsp;</p><p>将&nbsp;TEE 与LLM 融合，有助于在这类场景中更好地保障数据在LLM 模型训练和推理过程中的保密性。训练阶段，TEE 中的数据处理都处于加密状态；推理阶段，TEE&nbsp;则可保护用户输入和模型结果的隐私。同时，其硬件隔离和安全验证机制可以更有效地防止未经授权的访问和攻击，增强模型运行时的安全性。</p><p></p><h2>3. TEE 与&nbsp;LLM 融合的挑战：资源和性能限制</h2><p></p><p>&nbsp;</p><p>·&nbsp;资源限制：TEE 的计算资源和存储空间通常都非常有限，LLM 庞大的模型参数和计算需求可能会超出一般TEE 的能力范围。</p><p>·&nbsp;性能下降：I/O&nbsp;数据的加密和安全计算操作会引入额外的计算开销，导致模型训练和推理性能有一定程度下降。基于算法的解决方案可减少模型规模和计算需求，以适应&nbsp;TEE&nbsp;的资源限制，但CPU 仍会成为制约LLM 训练的算力瓶颈。</p><p></p><h2>4. 基于英特尔® 平台的解决方案：加速TEE 与LLM 融合应用</h2><p></p><p></p><h3>4.1 基于英特尔®&nbsp;SGX/TDX[1]的TEE 解决方案</h3><p></p><p></p><p>英特尔自第三代英特尔® 至强®&nbsp;可扩展处理器开始内置英特尔®&nbsp;软件防护扩展（英特尔®&nbsp;SGX）技术，其安全飞地的容量最多可达单颗CPU 512GB，双路共计1TB 容量，可满足目前千亿大模型的执行空间需求。此外，该技术提供支持的机密计算可实现应用层、虚拟机&nbsp;(VM)、容器和功能层的数据隔离。无论是在云端、边缘还是本地环境，都能确保计算与数据始终在私密性和安全性上获得更全面的保护，以免暴露给云服务提供商、未经授权的管理员和操作系统，甚至是特权应用。</p><p></p><p>另一方面，英特尔®&nbsp;Trust Domain Extension（英特尔® TDX）可将客户机操作系统和虚拟机应用与云端主机、系统管理程序和平台的其他虚拟机隔离开来。它的信任边界较英特尔®&nbsp;SGX 应用层的隔离边界更大，使受其保护的机密虚拟机比基于英特尔®&nbsp;SGX 的安全飞地的应用更易于进行大规模部署和管理，在部署LLM 这类复杂应用时，TDX 在易用性上更具优势。此外，今年推出的全新第四代英特尔®至强®&nbsp;可扩展处理器内置英特尔® AMX，可大幅提升矩阵运算性能，而英特尔® SGX/TDX也可为英特尔®&nbsp;AMX、英特尔® DL Boost等计算指令提供支持，进而为&nbsp;TEE 中的大模型赋予快速落地+优化性能的双重优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b5dba3733d6685752ed50ac864eeb1c.png\" /></p><p>图1.&nbsp;SGX/TDX 的可信边界</p><p>&nbsp;</p><p>构建完善的TEE 生态系统对推动LLM 的应用和发展至关重要。开发者需要能够简化集成和使用过程的面向TEE&nbsp;的开发者工具和框架。为此，英特尔在SDK 的基础上，推出了开源的lib OS 项目Gramine 来帮助开发者更好地使用基于英特尔® SGX的TEE，助推LLM 与TEE&nbsp;的融合。</p><p></p><h4>4.1.1 大语言模型推理</h4><p></p><p></p><p>使用私有数据进行个性化训练的大模型不仅包含私有数据信息，其查询本身也具有隐私性，尤其是基于边端的非安全环境部署。基于英特尔®&nbsp;SGX/TDX 的TEE 可为大模型提供更安全的运行环境，在数据上传云端前，查询可先通过客户端对传输内容加密，云端只需在英特尔® SGX/TDX 中解密查询问题，然后输入大模型的推理服务中，并将所得结果在云端的TEE&nbsp;中加密后传输回本地客户端。在整个工作流程中，客户端以外的数据和运行态程序均处于密态环境当中，效率远远高于其他基于纯密码学的解决方案。</p><p></p><p>目前像&nbsp;LLAMA 7B、ChatGLM 6B&nbsp;等模型都可以在该TEE 方案上满足实时可交互性能的运行。图2 展示了使用LLM 部署知识问答的参考设计。基于英特尔®&nbsp;SGX/TDX&nbsp;的&nbsp;TEE为实际部署LLM&nbsp;中的自有知识产权保护提供了一套完整的方案，优化整个模型在查询、传输和推理过程中的安全保护。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b7/b71c748c672b1e34ac0c4cd2181eb8d0.png\" /></p><p>图2.&nbsp;基于TEE 的大语言模型私密问答</p><p></p><h4>4.1.2 联邦学习</h4><p></p><p>借助基于&nbsp;TEE&nbsp;的联邦学习解决方案[2]（见图&nbsp;3），就可在多机构之间实现基于&nbsp;NLP 的深度学习例如使用BERT 的命名体识别。在金融和医疗等行业提升准确性，实现多机构数据互通，同时更好避免数据泄露。</p><p>&nbsp;</p><p>此方案中每个参与方包含一个&nbsp;Avalon[3]&nbsp;管理模块和Gramine 工作负载，均运行在英特尔®&nbsp;SGX 的安全飞地中，在管理模块彼此间的远程认证完成执行后，即可启动联邦学习过程，参与方在本地使用各自的数据集进行训练，然后将梯度上传至聚合方，聚合方进行聚合后将平均梯度下发至各参与方，以继续进行下一轮训练。对比图&nbsp;4 所示的BERT&nbsp;+ CRF 模型[4]，此方案可以在强化隐私保护的同时，让性能损失维持在50% 以下[2]。</p><p><img src=\"https://static001.geekbang.org/infoq/96/96e0575e7f0041dfd620882193df7ec0.png\" /></p><p>图3.&nbsp;基于TEE&nbsp;的联邦学习</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/e2/e277eb12fb220c052301b63634aaceff.png\" /></p><p>图4.&nbsp;BERT&nbsp;+&nbsp;CRF&nbsp;模型[4]</p><p></p><h3>4.2 BigDL：端到端大模型和TEE&nbsp;融合的方案</h3><p></p><p></p><p>据行业用户反馈，LLM 在端到端应用中的痛点包括：</p><p></p><p>·&nbsp;软件栈复杂，难以确保端到端应用的安全。LLM 的训练和推理常依赖较多的软件栈、服务和硬件。为保护用户数据和模型产权，需确保每个环节的安全性（不同硬件、运行环境、网络和存储等）。</p><p>·&nbsp;计算量大，且对性能敏感。LLM 的计算量非常大，需引入足够多的性能优化。但是，不同模型、平台和软件栈需要使用不同的优化方案，要在特定平台上实现更理想的性能，需要长时间的性能调优。</p><p>为解决这些痛点，由英特尔主导的开源项目BigDL，近期就推出了针对&nbsp;LLM的隐私保护方案，其两大主要功能为：</p><p>&nbsp;</p><p>·&nbsp;提供端到端的安全保护：在不修改代码的情况下，为单机和分布式的&nbsp;LLM应用提供端到端的安全保护功能。具体包括，基于英特尔®&nbsp;SGX/TDX的&nbsp;TEE、远程证明、统一的密钥管理接口和透明的加解密&nbsp;API等。</p><p>·&nbsp;实现一站式性能优化：BigDL Nano 提供的针对&nbsp;LLM 的一站式性能优化方案，可让现有&nbsp;LLM 应用在几乎不用修改代码的情况下受益于英特尔®&nbsp;AMX、英特尔®&nbsp;AVX-512 和英特尔®&nbsp;Extension for PyTorch。同时，用户还可利用&nbsp;BigDL Nano 提供的&nbsp;LLM API，快速构建应用。</p><p><img src=\"https://static001.geekbang.org/infoq/f3/f397ee5e26aac22741f8c4bfb266ba28.png\" /></p><p>图5.&nbsp;BigDL&nbsp;端到端安全的大模型方案</p><p>&nbsp;</p><p>如图6 所示，在应用了&nbsp;PPML（Privacy Preserving Machine Learning，隐私保护的机器学习）提供的安全技术后，由于更强的安全和隐私保护会带来额外开销，因此端到端应用性能会略有下降；但应用了&nbsp;BigDL Nano提供的优化功能后，端到端的性能得到了显著改善*，总体性能甚至高于没有任何保护的明文性能。</p><p><img src=\"https://static001.geekbang.org/infoq/3f/3fc1b683af2fda42fdc423a95d1a5c3f.png\" /></p><p>图6.&nbsp;BigDL PPML + Nano 端到端性能损失情况</p><p>&nbsp;</p><p>目前，该方案已经开源，并开始陆续交付给行业客户进行测试和集成[5]。</p><p>&nbsp;</p><p></p><h2>5. 未来趋势</h2><p></p><p>&nbsp;</p><p>TEE 提供了隐私保护和数据安全防护功能的创新解决方案，将在&nbsp;LLM 实际落地过程中扮演重要角色。通过将二者融合，可更好地保障数据在训练和推理过程中的保密性，增强对未经授权访问和模型结果篡改的防御。然而，在TEE&nbsp;中保护用户隐私的同时，需要平衡性能需求，随着大模型对于计算需求的大幅提升，算力可能会执行在异构硬件上，TEE&nbsp;和异构硬件的结合将成为未来发展趋势。随着&nbsp;CPU 性能的提升以及内置AI&nbsp;加速技术的升级和更新，在方便部署的场景下，CPU 会是大模型推理和TEE&nbsp;结合的首选，在训练的场景下，基于CPU 的TEE&nbsp;结合异构硬件的加密支持，则会是大模型训练甚至大模型联邦训练的技术方向。英特尔将一如既往地以软硬结合的产品技术组合促进开发者参与，推动LLM 与TEE&nbsp;的融合。</p><p>&nbsp;</p><p>作者简介：</p><p>英特尔公司AI 架构师俞巍，英特尔公司平台安全资深架构师李志强，英特尔公司安全软件研发工程师李青青，英特尔公司软件架构师龚奇源，都在从事AI&nbsp;和SGX/TDX 相关工作。</p><p>&nbsp;</p><p>[1] SGX/TDX: <a href=\"https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/innovation-data-protection-with-security-engines.html\">https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/innovation-data-protection-with-security-engines.html</a>\"</p><p>[2] Wei Yu. et al. 2022, TEE based Cross-silo Trustworthy Federated Learning Infrastructure, FL-IJCAI'22</p><p>[3] <a href=\"https://github.com/hyperledger-archives/avalon\">https://github.com/hyperledger-archives/avalon</a>\"</p><p>[4] Souza, F., Nogueira, R. and Lotufo, R. 2020, Portuguese Named Entity Recognition using Bert-CRF, arXiv.org（请参见<a href=\"https://arxiv.org/pdf/1909.10649.pdf\">https://arxiv.org/pdf/1909.10649.pdf</a>\"）</p><p>[5] <a href=\"https://github.com/intel-analytics/BigDL/tree/main/python/llm\">https://github.com/intel-analytics/BigDL/tree/main/python/llm</a>\"</p><p>*性能优化效果与具体平台、模型和环境有关</p>",
    "publish_time": "2023-07-03 11:42:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]