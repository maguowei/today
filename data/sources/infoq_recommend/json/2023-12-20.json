[
  {
    "title": "LinkedIn 将 Espresso 从 HTTP1.1 迁移到 HTTP2，连接数减少 88%，延迟降低 75%",
    "url": "https://www.infoq.cn/article/GMWGJZzawHaXiklM7eBA",
    "summary": "<p>LinkedIn 将其 Espresso 数据库从 HTTP/1.1 迁移到 HTTP/2，极大 提升 了可伸缩性和性能，减少了连接数量、降低了延迟并缩短了垃圾回收时间。为了获得这些好处，团队不得不优化 Netty 默认的 HTTP/2 栈来满足需求。</p><p></p><p>LinkedIn 使用 Espresso（构建在 MySQL 之上的文档平台）来存储和提供大部分数据。随着 LinkedIn 平台的有机增长，数据量不断增加，迫使公司不断扩展 Espresso 集群的规模，并进行优化工作，例如为 Espresso 引入 集中式缓存层 或者 采用 Protocol Buffers 进行服务间通信。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/91/916c9aa7e511e0aafce5cdbeb793dc14.png\" /></p><p></p><p>Espresso 高层架构（来源：LinkedIn Engineering Blog）</p><p></p><p>Espresso 的事务栈包括两个主要组件：路由器和存储节点。路由器负责将请求发送到正确的存储节点上，存储节点负责与 MySQL 集群进行交互，并相应地调整数据格式。这些组件之间的通信使用 HTTP 协议，更具体地说是使用了 Netty 框架。随着时间推移，团队发现到 Espresso 集群的规模增长导致可伸缩性下降。</p><p></p><p>最近增加的 100 个路由器节点导致存储节点内存使用量增加，额外的垃圾回收导致延迟增加了 15%。此外，由于增加了大量的 HTTP/1.1 连接，从连接池中获取连接所需的时间达到了几毫秒。最后，在发生网络事件（如交换机升级）期间，由于达到存储节点的连接限制，重新建立数千个连接可能会导致错误。</p><p></p><p>LinkedIn 的软件工程师 &nbsp;Abhishek Andhavarapu 解释了 HTTP/1.1 和 HTTP/2 之间的差异，以及这些差异如何影响 Espresso 平台的可伸缩性和性能：</p><p></p><p></p><blockquote>对于路由器与存储层之间的通信，我们早期的方法是使用了 HTTP/1.1，这是一种广泛用于 Web 服务器和客户端之间交互的协议。然而，HTTP/1.1 是基于每个请求连接的，在大规模集群中，这种方法会导致路由器和存储节点之间产生数百万个并发连接。这导致了可伸缩性、弹性和众多与性能相关的障碍。团队决定在进行 HTTP/2 迁移时继续使用 Netty 框架，但很快发现其性能并不理想（比 HTTP/1.1 实现的吞吐量低 45%，延迟高 60% 左右），因此工程师们不得不去解决 HTTP/2 栈的性能瓶颈。在经过一番诊断后，他们确定了两个改进方向：获取连接和处理请求，以及请求的编码 / 解码。</blockquote><p></p><p></p><p>开发人员通过修改几个内部的 Netty 实现细节来增强功能。他们创建了一个可以重复使用已有通道的处理程序，避免为每个请求创建新的处理通道。他们还引入了一个自定义的 EventLoopGroup 实现，可以更均匀地在工作线程之间平衡连接。为了减少获取连接时的上下文切换，团队重新设计了连接池实现，使用了高性能、线程安全的队列。</p><p></p><p>此外，SSL 处理使用原生的、基于 JNI 的 SSL 引擎进行了优化，并使用自定义的 SSL 初始化逻辑避免了冗长的 DNS 查找延迟。最后，团队通过创建自定义编解码器来优化编码 / 解码性能，编解码器将 HTTP/2 请求封装为 HTTP/1.1 请求，帮助处理 Espresso 使用的许多自定义 HTTP 标头，并禁用了 HPACK 标头压缩。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a1/a105f8a2d023cd058e5de92a376064bb.png\" /></p><p></p><p>迁移到 HTTP/2 后延迟减少（来源：LinkedIn Engineering Blog）</p><p></p><p>团队报告称，在所有这些定制化改进之后，迁移到 HTTP/2 带来了明显的性能改进，相较于 HTTP/1.1，TCP 连接数量减少了 88%，延迟降低了 65% 至 75%，垃圾回收时间减少了 75% 至 81%，获取连接的等待时间从 11 毫秒 降至 0.02 毫秒（改进了 99%）。</p><p></p><p>英文原文：</p><p><a href=\"https://www.infoq.com/news/2023/12/linkedin-espresso-http2/\">https://www.infoq.com/news/2023/12/linkedin-espresso-http2/</a>\"</p>",
    "publish_time": "2023-12-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于共享存储的 leader 选举：在存算分离架构云数仓 ByConity 中的实践",
    "url": "https://www.infoq.cn/article/NmUUxzcCmmF2CG4Ltygp",
    "summary": "<p>本文作者｜Yuan Zhu</p><p></p><p></p><blockquote>ByConity 0.3.0 版本本周发布，其中「基于共享存储的选主方式」作为 0.3.0 版本的主要功能将在此次更新中实现。本文将介绍其在 ByConity 中的设计思考与实践。项目地址｜<a href=\"https://github.com/ByConity/ByConity\">https://github.com/ByConity/ByConity</a>\"</blockquote><p></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>在传统常见的分布式 share-nothing 微服务架构中，我们通常使用 DNS 这类成熟方案来进行节点之间的服务发现，使用 Zookeeper、Etcd、Consul 这类成熟组件在副本节点之间进行 leader-follower 选举以实现集群的高可用，在配置、使用、运维管理都有一定的复杂度。</p><p></p><p>在越来越多的分布式系统中使用一份高可用存储来实现 share-everything 存算分离架构的今天，我们可以利用这块高可用存储来模拟单机系统里的共享内存，将不同的计算节点看成是单机系统里的进（线）程，模仿单机系统的方案来实现他们之间的发现、同步。</p><p></p><p>本文即介绍以上思想是如何在开源云原生数仓 ByConity 中设计和实践的。</p><p></p><p></p><h2>ByConity 的基本架构</h2><p></p><p></p><p>《<a href=\"http://mp.weixin.qq.com/s?__biz=MzkwMTQzMjc2OQ==&amp;mid=2247483778&amp;idx=1&amp;sn=06e9a9fe4180fa8c78079b45686a4bbe&amp;chksm=c0b5952cf7c21c3ac32f39c1008a7c0535af6b2cb7210ae2031953db91fd0b737b268310a66f&amp;scene=21#wechat_redirect\">谈谈 ByConity 存储计算分离架构和优势</a>\"》介绍了基于 ClickHouse 的开源云原生数仓 ByConity 的存算分离架构。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2edbef28e971d3eed68614183d0ba54e.png\" /></p><p></p><p></p><p>可以看到，在计算一侧，存在多种控制节点，它们需要各自通过多副本 + 选主来提供高可用的服务能力，例如上图中的 Resource manager/Timestamp oracle 等。实际中的多个计算 server，也需要在选出一个单节点来执行特定的读写任务。</p><p></p><p>最早 ByConity 使用了 ClickHouse-keeper（以下简称\"keeper\"）组件来进行选主，该组件基于 Raft 实现，提供兼容 zookeeper 的选主接口，在实际使用中遇到了以下运维问题：</p><p>至少需要部署 3 个 keeper 节点，才能提供单个节点故障的容灾。这是因为 Raft 协议需要过半节点正常运行，才能维护主节点的正常工作和选举。节点增删和服务发现流程复杂。需要修改所有 keeper 节点的配置文件才能生效，且所有的调用者也需要修改配置才能发现这个结果。ByConity 实现过一个使用固定的共享域名来代替给每个 keeper 节点配置地址的方案，但又进一步带来了处理 域名解析的可访问节点数量和 keeper 中配置数量不一致时的复杂性。容器重启后如果服务变换 ip 和服务端口，ClickHouse-keeper 难以快速恢复。这不仅是因为 2，也是因为 keeper 实现中 raft 的 server_id 和监听地址进行了强绑定。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/42c5ee69f1baed46c43fb666c677f3f1.png\" /></p><p></p><p></p><p>我们可以把以上问题分类为：</p><p>故障时的容灾性能。高可用的运维、部署成本。</p><p></p><p>考虑到 ByConity 作为一个新的云原生服务，并不需要兼容 ClickHouse 对 zookeeper 的访问，我们选择了基于存算分离的云原生架构实现一种新的选主方式来优化以上问题。</p><p></p><p></p><h2>基于共享存储的 leader 选举</h2><p></p><p></p><h3>术语定义</h3><p></p><p></p><p>副本：地位相互平等的某个服务多个部署实例进程。业务：除了选举之外的服务逻辑。Follower：副本中不可提供业务服务的节点。Leader：副本中可提供业务服务的节点，本文也常把 leader 选举简称为“选主”。客户端：需要访问 leader 提供业务服务的节点。</p><p></p><h3>设计思想</h3><p></p><p></p><p>我们注意到如果一台计算机在试图同步多个线程对一个临界资源的访问竞争时，常见的 pthread_mutex 内存锁实现方案是非常简单的，依赖了以下基础：</p><p>锁被分配在一份所有线程可见的内存中；内存支持通过 CAS（Compare&nbsp;And&nbsp;Swap）指令实现小对象的原子写入；内存支持确保原子写入的结果，读者看到的写入顺序和写者的写入顺序一样；操作系统内核通过 futex 等系统调用指令，支持原子的等待 / 通知线程某个值的变化，使得线程知道某个资源又可以被竞争了。&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/d0/d07e62d508b75ff0638dbd3d0f3c1ac4.png\" /></p><p></p><p>如果我们把 ByConity 多个试图选主的节点看成不同的线程，把支持事务提交、可见性顺序等于事务提交顺序的 Foudation DB（用于存储 ByConity 元数据的高可用 KV 存储，以下简称为“FDB”）看成支持 CAS 写入、保证可见性顺序的本地内存，用节点的定期 Get 轮询去模拟 Linux 内核的线程唤醒通知机制，我们就可以用 ByConity 所使用的高可用 Foudation DB KV 存储，通过模拟 CAS 操作去同步多个节点之间对“谁是 leader”这个问题答案的竞争：谁 CAS 成功谁就是 leader。</p><p></p><p>解决了相互竞争的写者之间的同步，我们还需要把写者竞争的结果发布给读者。Linux 的锁的数据结构会记录谁是 mutex owner，这里也可以把 leader 的监听地址写入竞争的结果：CAS 的 key 写入内容 value 需要包括自己的监听地址。所以读者访问这个 key 就可以完成服务发现（读者不需要知道非 leader 的地址）。</p><p></p><p></p><h3>设计目标</h3><p></p><p></p><p>我们预期实现以下目标：</p><p>选举组件以一个库的形式嵌入业务服务进行使用。类似 linux mutex 使用的 pthread 库。支持任意多副本节点。增删节点无需额外操作。节点变更监听地址无需额外操作。只要有一个副本节点可用，即可选主成功。这是因为存算分离场景，节点本地无状态，任何一个节点都可以成为主节点，无需从其他副本同步状态到本地。副本节点之间无需相互通信和服务发现，包括无需进行物理时钟同步。</p><p></p><p>接下来，我们使用若干个分布式共识的达成来介绍如何具体去实现这些目标：</p><p>follower 之间对“谁是新 leader”达成共识。新旧 2 任 leader 对“如何让卸任和上任的时间不重叠”达成共识。服务端节点在配置变更时，对“选举的时间参数”在每一轮选举中达成共识。客户端如何感知“谁是新 leader”这个服务端产生的共识。</p><p></p><h3>follower 节点的角色共识：leader 选举的实现</h3><p></p><p></p><p>数据结构</p><p></p><p>分布式系统具有许多单机系统所不涉及的复杂性，其中最主要的一个复杂性来源就是有限操作时间限制和非全连通拓扑带来的不可访问：单机系统的任何读写内存操作都没有“超时”或者失败的概念，而分布式系统必须考虑这个点才能保证可用性。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e79b6c515ead54d2b3b7888c13994d14.png\" /></p><p></p><p></p><p>所以如上图，对于 leader CAS 写入的数据结构 LeaderInfo，除了包括自己的监听地址 address，也需要包括关于绑定了时间相关的状态信息 lease：例如 leader 上任时间点 elected_time，最近一次刷新时间 last_refresh_time（有变化就证明自己还活着），刷新的时间间隔要求 refresh_interval_ms，多长时间不刷新就认为 leader 已经任期结束（其它节点可以开始重新竞争 leader 了）expired_interval_ms，以及 leader 的状态 status。</p><p></p><p>选举基本规则</p><p></p><p>每个节点要么是 follower，要么是 leader。预期系统内任何一个时间点，只有一个节点认为自己是 leader。任何节点都可以读 KV 存储中的一个 key （以下皆简称 “key”），从中得知 “谁是 leader”这个结果。如果这个 key 不存在，说明 leader 从未被成功选举。leader 定期 CAS 更新 key 中存储&nbsp;value（以下皆简称 “value”）的 lease.last_refresh_time 字段，延长自己的任期到 lease.last_refresh_time + lease.refresh_interval_ms。leader 遇到进程结束等服务可控停止时，可以 CAS 更新 value 的 lease.status 字段为 Yield，主动让出 leader 身份。每个 follower 定期 GET 读取 value，确认 leader 是否被成功选举、是否已经任期过期、是否已经让出 leader。如果是，那么 follower CAS 尝试更新 key 的 value 来竞选 leader，修改 address 为自己的地址。</p><p></p><p>接下来我们展开这个规则，介绍如何实际完成全流程的选举。</p><p></p><p>备选</p><p></p><p>前置条件</p><p>当前节点是 follower。</p><p></p><p>前置条件说明</p><p>每个节点启动后，都认为自己是 follower。每个 leader 在 lease 任期结束之前没有成功更新 lease，被认为任期过期（即 now()<=\"\" p=\"\"></p><p></p><p>动作</p><p>follower 每隔 lease.refresh_interval_ms 就去轮询读取 key 的结果，检测key 是否存在，或者 value 中的任期是否已经过期。</p><p></p><p>竞选</p><p></p><p>前置条件</p><p>当前节点是 follower。对于存算分离服务，我们认为每一个无状态副本都可以参与竞选，不存在状态机同步进度差异。key 不存在，或者 value 中的任期已经过期，或者 value 中的&nbsp;lease.status 是 Yield，或者&nbsp;value&nbsp;中的 address 是自己的监听地址。</p><p></p><p>动作</p><p>如果 key 不存在， 那么 Put if not exist 写入自己的地址信息。如果 value 中的任期已经过期或者 value 中的 address 是自己的监听地址，那么 Put CAS 写入自己的地址信息。写入的 lease 信息为：lease.status=Ready, lease.last_refresh_time=lease.elected_time=now(), lease.refresh_interval_ms 和 lease.expired_interval_ms 为配置文件中的信息。</p><p></p><p>胜选</p><p></p><p>前置条件</p><p>当前节点是 follower。当前节点写入 value 成功；或者虽然 CAS 失败，但是发现 value 的 address 是自己的监听地址。</p><p></p><p>动作</p><p>检查是否任期已过期，即当前时间 now() 是否满足 now()<=\"\" p=\"\"></p><p></p><p>就职</p><p></p><p>前置条件</p><p>当前节点胜选，且任期没有过期。</p><p></p><p>动作</p><p>调用业务侧注册的 onLeader() 回调，提醒业务可以以 leader 方式提供服务了。对于有状态的服务，可能在这个过程需要同步一些状态才能以 leader 方式提供服务；对于无状态服务（例如 ByConity 的存算分离计算节点），胜选即可立即就职服务。提供 isLeader() 接口供业务调用检查，仅在 now()<=\"\" p=\"\"></p><p></p><p>续任</p><p></p><p>前置条件</p><p>当前节点是 leader，且任期没有过期。可无限期连任。距离竞选或最近一次续任时间已经超过或等于 lease.refresh_interval_ms，即 now()&gt;=lease.last_refresh_time + lease.refresh_interval_ms 。</p><p></p><p>动作</p><p>CAS 设置&nbsp;value&nbsp;的&nbsp;lease.last_refresh_time = now()。</p><p></p><p>主动离职</p><p></p><p>前置条件</p><p>当前节点是 leader。被业务侧调用 yield() 接口。常见于服务退出等场景。</p><p></p><p>动作</p><p>调用业务侧注册的 onFollower() 回调，提醒业务不可以以 leader 方式提供服务了。CAS 更新&nbsp;value&nbsp;的 lease.status=Yield，lease.last_refresh_time=now()。当前节点变为 follower，即使 CAS 失败。</p><p></p><p>被动离职</p><p></p><p>前置条件</p><p>当前节点是 leader。now()&gt;=lease.last_refresh_time + lease.expired_interval_ms 或者在 CAS 更新 lease 发现被别的节点提前更新了。</p><p></p><p>动作</p><p>调用业务侧注册的 onFollower() 回调，提醒业务不可以以 leader 方式提供服务了。当前节点变为 follower。</p><p></p><p>总结</p><p></p><p>我们回顾一下预期的目标，可以看到都实现了。</p><p>支持任意多副本节点。它们只需要和共享存储 FDB 进行通信。增删节点无需额外操作。这是因为节点之间彼此都不互相服务发现和通信。节点变更监听地址无需额外操作。这是因为节点主动通过 CAS 写入自己的监听地址，无需类似 Raft 需要显式的节点减少再增加动作。只要有一个副本节点可用，即可选主成功。这是因为对于存算分离的无状态节点，任何副本都可以成为 leader。副本节点之间无需相互通信同步和服务发现，包括物理时钟同步。</p><p></p><p>但是不进行物理时钟同步，会不会产生 2 个 leader 的任期相互交叠，而给集群服务带来风险？我们在下一节分析这个问题。</p><p></p><h3>新旧 2 任 leader 的时间共识：对任期过期的判断</h3><p></p><p></p><p>问题描述</p><p></p><p>我们可以看到一个旧的 follower 节点胜选之后，可以立即就职提供 leader 服务。此时有没可能整个集群中有 2 个 leader，都在提供 leader 服务呢？</p><p></p><p>先定义需求：</p><p>新 leader 上任开始 leader 服务后，旧 leader 不再以 leader 身份响应新的请求。新 leader 上任开始 leader 服务后，旧 leader 在之前已经开始以 leader 身份处理的请求可以继续处理。</p><p></p><p>满足上面的需求需要以下保证：</p><p>任何 2 个节点的 leader 任期没有交叠。即不会发生节点 a 的 leader 任期还未结束，节点 b 的 leader 任期就已经开始。业务服务在响应请求时，总是先调用选主组件提供的 isLeader() 接口检查任期是否过期。</p><p></p><p>第二个点我们需要业务服务进行改造即可满足。第一个点我们需要基于对任期的设计和实现说明安全性。</p><p></p><p>问题分析</p><p></p><p>如果要让 2 个 leader 之间任期在全局时钟下没有交叠，我们只需要保证：</p><p></p><p>假设 1：任何 follower 认为某 leader 的任期结束时间点 A 大于 leader 认为的自己的任期结束时间点 B。</p><p></p><p>因为 A 一定小于该 follower 竞选 leader 成功后的任期开始时间点。这样任何 2 个 leader 的任期就不会</p><p>有交叠了。而任期的结束时间点通常是由任期开始时间点来确定，为了方便工程实践，我们可以把假设 1 进行一个转换：</p><p></p><p>假设 1a: follower 认为的 leader 的任期开始时间点 大于 leader 认为的自己的任期开始时间点。</p><p>如果我们认为 leader 和 follower 的时钟，在任期内的计时误差，小于 2 者认为的任期开始时间点的差，那么显然假设 1a 成立时假设 1 也成立。我们接下来尝试找出能够实现假设 1a 的实际方案。</p><p></p><p>问题方案：任期区间的定义</p><p></p><p>如果我们怀疑 2 任 leader 的任期有交叠，那旧 leader 一定有一次对自己任期的续期成功 CAS 写入，第一个和他任期有交叠的新 leader 的成功上任时一定有对这个旧 leader 这个续期租约的成功读取，和对这个任期过期的判断，以及自己竞选时任期的成功 CAS 写入。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8e816c4df85c596eb2a493081128070b.png\" /></p><p></p><p></p><p>如上图：上一任旧 leader 最后一次对自己任期的续期写入的开始时间是 T_w0， 收到回包的是 T_w1；下一任新 leader 对旧 leader 最后一个任期 lease 的第一次读取的开始时间是 T_r0， 收到回包的是 T_r1， 竞选写入的开始时间是 T_w2， 收到回包的是 T_w3。假设这些数值是由一个虚拟但精确的全局时钟给出的时间戳。</p><p></p><p>从单机的视角来看，必然有大小关系顺序：</p><p><code lang=\"null\">T_w0 &lt; T_w1\nT_r0 &lt; T_r1 &lt; T_w2 &lt; T_w3</code></p><p>由于下一任新 leader 对旧 leader 写入最后一个任期的租约成功读取，这说明一定写、读之间有\"happened-before\"的关系，所以也有多机视角下的大小关系顺序：</p><p></p><p><code lang=\"text\">T_w0 &lt; T_r1</code></p><p></p><p>我们将使用这个关系来给出符合假设 1a 的任期区间定义。设任期时间固定为 expired_interval_ms：</p><p>旧 leader 认为自己的任期是 [T_w0, T_w0+expired_interval_ms)follower（未来的新 leader）认为旧 leader 的任期是 [T_r1, T_r1+expired_interval_ms)新 leader 认为自己的任期是 [T_w2, T_w2+expired_interval_ms)</p><p></p><p>即 leader 总是认为自己的任期开始时间是从自己最近一次写入 FDB 成功的写入开始时间（竞选或续任发起开始时间），而其他 follower 认为这个 leader 的这一任任期开始时间是自己第一次读到这个任期 lease 的读取结束时间（得知 leader 胜选或续任成功时间）。</p><p></p><p>由于 T_w0 &lt; T_r1，所以T_w0+expired_interval_ms<=\"T_w2（一定会等到上一任任期结束才开始竞选）。故有结论</p><p></p><p><code lang=\"null\">T_w0+expired_interval_ms< code=\"\"></code></p><p></p><p><code lang=\"null\">即：两任 leader 的任期在 2 个 leader 的视角下都没有交叠。</code></p><p></p><p><code lang=\"null\">方案安全性分析</code></p><p></p><p><code lang=\"null\">现在我们考虑在包含时钟走时误差（不是时刻误差）情况下，在最极端的场景下上述方案的安全性。</code></p><p></p><p></p><p><code lang=\"null\"><img src=\"https://static001.geekbang.org/infoq/87/87a36d6ad9bec0eb9d852c121e9850ee.png\" /></code></p><p></p><p></p><p><code lang=\"null\">我们假设 follower 在认为上一任 leader 任期结束之后立即开始竞选，则有 T_r1+expired_interval_ms==T_w2，此时新的任期开始时间为 T_w2，旧的任期结束时间为 T_w0+expired_interval_ms。在没有时钟误差的情况下</code></p><p><code lang=\"null\"><code lang=\"null\">T_w2-(T_w0+expired_interval_ms)==T_r1-T_w0&gt;0</code></code></p><p></p><p><code lang=\"null\">现在分析时钟误差是否会超过这个差值，以及安全门限：</code></p><p><code lang=\"null\">即如果 follower 和 leader 的时钟在 expired_interval_ms 时间内的走时差异小于 T_r1-T_w0，那么不会发生上一任 leader 认为自己还在任的时候，follower 就已经开始尝试竞选的问题；如果 follower 和 leader 的时钟在 expired_interval_ms 时间内的走时差异小于 (T_r1-T_w0)+(T_w3-T_w2），那么不会发生 2 任 leader 所在的业务各自认定自己的服务任期 实际有相互交叠的问题：这是因为业务感知到的任期开始需要等待 FDB 写入完成，新 leader 的实际上任时间需要从 T_w3 而不是 T_w2 开始计算（T_w2 仅用于计算超时时刻）。</code></p><p></p><p><code lang=\"null\">通常我们配置的任期&lt;10s，而 2 次读和 CAS 写 fdb 带来的读写保守估计耗时&gt;1ms。而按极端场景的估计，现代电子计算机晶振在高温情况下工作 1s 内时钟漂变&lt;50us，预期 10s 内连续偏差&lt;500us。考虑到写入就差不多同时发生了读取也是很罕见的情况，我们的参数配置属于比较安全的范畴，但不建议设置大于 10s 的任期。</code></p><p></p><p><code lang=\"null\">实际实现</code></p><p></p><p><code lang=\"null\">在上面的流程中，可以看到是不需要在节点之间同步实际的绝对物理时刻的，那为什么选择在数据结构中存储 lease.last_refresh_time 这个任期开始的本地物理时刻，而不是如同 Raft 那样只需写入一个自增的逻辑时间戳？</code></p><p></p><p><code lang=\"null\">这是因为</code></p><p><code lang=\"null\">leader 有可能发生重启，我们希望它重启之后，能够基于这个物理时间判定自己还在 leader 任期之内，快速恢复工作。follower 竞选时写入的内容可能返回超时，但实际最终写入成功了。如果该 follower 在定期查看 key 的备选过程中能够看到自己已经竞选成功了，并且这个物理时间判定自己还在 leader 任期之内，那也能快速切换为 leader 工作。</code></p><p></p><p><code lang=\"null\">所以写入的这个物理时间不是为了分布式同步，而是 leader 自己为了提升 recovery 速度，而持久化自己的状态。</code></p><p></p><h3><code lang=\"null\">服务端节点的选举参数共识：发布选举参数改变</code></h3><p></p><p></p><p><code lang=\"null\">和 Raft 等分布式同步方案相比，本文介绍的方案有一个细微但重要的区别：不仅可以随意的增删节点，而且还可以随意的修改和配置任期！换而言之，集群的配置、管理类的元信息可以安全和简易的变更。</code></p><p><code lang=\"null\">传统的 Raft 之所以不能随意的直接修改心跳周期、leader 任期等时间参数，是因为升级修改这些参数的时候，集群内不同节点会持有不一致的参数值，而 leader 选举等共识构建流程的安全性是和这些参数的一致性有关系的（考虑到一些工程实现的取舍，那更是如此）。所以安全的工程实践往往不支持热升级修改这些参数，而需要停止所有节点来更新这些参数。</code></p><p></p><p><code lang=\"null\">而在本文的方案中，当 leader 把心跳周期 lease.refresh_interval_ms，任期 lease.expired_interval_ms 写入共享存储 key 之后，下一任 leader 尝试竞选时必须按照这个参数来进行竞选，即使它本地的配置参数和共享存储不一样；下一任 leader 胜选之后，它自然也就把自己的配置参数覆盖写入共享存储，它自己和其他 follower 都按照新发布的配置参数来决定自己的任期和未来的重新选主时间点。</code></p><p></p><p><code lang=\"null\">简而言之：每一任 leader 胜选后都自动发布自己配置中的新选举参数到共享存储中，下一任 leader 的选举一定所有节点都能看见并遵守（否则 CAS 会失败）。所以滚动升级过程中，即使不同节点的有不同的本地配置并参与竞选，也不会带来不一致问题。</code></p><p></p><p></p><h3><code lang=\"null\">客户端和服务端的共识：“谁是 leader”的服务发现</code></h3><p></p><p></p><p><code lang=\"null\">上面我们已经介绍完了服务副本之间的分布式共识建立，涵盖 follower 之间竞选的冲突、新任和上任 leader 交接的冲突、选举参数变更等共识问题的解决。我们最后介绍客户端如何感知服务端的 leader 选举结果变更。</code></p><p></p><p><code lang=\"null\">客户端访问 leader 的逻辑很简单：</code></p><p><code lang=\"null\">读取 K 中 address 的结果，如果 key 存在且 address.status==Ready， 无限期缓存并访问其中的地址。如果 leader 的响应返回自己不是 leader，那么删除缓存后重试 1。</code></p><p></p><p><code lang=\"null\">我们可以看到客户端无需访问 lease 中的时间信息，从而无需感知副本之间的时间共识，也不需要本地时钟和服务端进行任何同步。</code></p><p></p><p></p><h3><code lang=\"null\">总结</code></h3><p></p><p></p><p><code lang=\"null\">介绍到这里，我们可以看到一个新 leader 的产生，是如何是如何在 follower 之间竞争产生分布式共识，并和旧任 leader 以及未来可能出现的新 leader 对这个共识的有效期的安全性产生共识，再让客户端感知到这个共识的全过程。</code></p><p></p><p></p><p><code lang=\"null\"><img src=\"https://static001.geekbang.org/infoq/da/dae7df478a84a03e009da79f8ebdfd0c.png\" /></code></p><p></p><p></p><p></p><h2><code lang=\"null\">ByConity 的使用</code></h2><p></p><p></p><p><code lang=\"null\">ByConity 可以把上面的选主方案使用在使用 leader 节点工作的服务，例如 Resource manager/Timestamp oracle 等。这套方案允许启动后的节点副本无需配置客户端对服务端的服务发现地址，也无需配置 ClickHouse-keeper 中副本之间的相互服务发现以及设置静态的副本数量：增加的副本只要能启动即能被客户端发现，并自动参与选主竞争。</code></p><p></p><p><code lang=\"null\">如果使用 K8s 部署 ByConity 集群，只需要调整 replicas 属性就能简单的增减服务 Pod 副本。</code></p><p></p><h2><code lang=\"null\">总结</code></h2><p></p><p></p><p><code lang=\"null\">本文介绍了一套基于共享存储和 CAS 操作进行 leader 选举的通用方案，充分利用了高可用共享存储的能力，使得 leader 选举运维和配置简单，让 ByConity 开源用户能更轻松的用上高可用服务能力。该方案可以简单的推广到任意的无状态服务的选主场景。</code></p><p></p><p><code lang=\"null\">ByConity 也借此去除了对 ClickHouse-keeper 的依赖，在支持多副本高可用的同时大大简化了配置，也提升了在 2 节点等低成本场景的容灾能力，并且使得服务无论单节点和多节点副本部署 在运行逻辑上进行了统一，降低代码复杂度。</code></p><p></p><p><code lang=\"null\">项目地址：<a href=\"https://github.com/ByConity/ByConity\">https://github.com/ByConity/ByConity</a>\"</code></p><p></p><p></p><p></p>",
    "publish_time": "2023-12-20 09:08:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "CodeWhisperer：亚马逊的 AI 编码助手彻底改变了软件开发",
    "url": "https://www.infoq.cn/article/iCUjvWTzGm68ui4WbqFX",
    "summary": "<p></p><h2>CodeWhisper 的出现</h2><p></p><p></p><p>根据 Insider 最近的一份报告，本月早些时候，<a href=\"https://www.yundongfang.com/Yuntag/%e4%ba%9a%e9%a9%ac%e9%80%8a?trk=cndc-detail\">亚马逊</a>\"的<a href=\"https://www.yundongfang.com/Yuntag/%e8%bd%af%e4%bb%b6?trk=cndc-detail\">软件</a>\"工程师收到了一封内部电子邮件，敦促他们采用 CodeWhisperer，这是一种<a href=\"https://www.yundongfang.com/Yuntag/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd?trk=cndc-detail\">人工智能</a>\"编码助手，旨在优化和简化软件开发。这种先进的工具，与 ChatGPT 非常相似，能够理解和响应自然语言查询，使其非常人性化并可供所有人使用。</p><p></p><p>在获得内部使用批准后，CodeWhisperer 现在可供亚马逊的所有软件开发团队使用。这意味着整个组织的工程师可以利用 AI 的力量编写更好的代码，比以往任何时候都更快、更准确。CodeWhisperer 理解自然语言查询的能力是一个显着优势，因为它允许开发人员以一种感觉直观和熟悉的方式与该工具进行交互。通过消除对复杂编程命令和语言的需求，CodeWhisperer 使工程师可以轻松地专注于手头的任务——编写满足亚马逊客户需求的高质量代码。</p><p></p><p>通过采用 CodeWhisperer，亚马逊的软件工程师有望实现更高水平的生产力和效率，这最终将使公司及其客户受益。随着人工智能的不断发展和改进，我们很可能会看到更复杂、更先进的工具，如 CodeWhisperer 正在被各行各业的公司开发和采用。</p><p></p><h2>CodeWhisper 的工作原理</h2><p></p><p></p><p>CodeWhisperer 的核心旨在简化编码过程并减少工程师花在日常任务上的时间。这款由 AI 驱动的编码助手的主要功能之一是它能够理解自然语言查询，这使得它非常易于使用。</p><p></p><p>当开发人员向 CodeWhisperer 输入查询时，该工具会使用高级语言模型和算法来分析查询、提取关键信息，并随后生成相关代码片段。这个过程非常复杂，并考虑了广泛的因素，包括正在使用的编程语言、查询的上下文以及开发人员的编码风格和偏好。</p><p></p><p>通过自动化这些过程，CodeWhisperer 能够显着减少编写高质量代码所需的时间和精力。这使开发人员可以专注于更具创造性和更高层次的任务，例如设计新功能和优化现有代码，而不是陷入繁琐且耗时的编码任务中。</p><p></p><p>除了其自然语言处理能力外，CodeWhisperer 还采用一系列其他高级功能和技术来改进编码过程。例如，该工具能够从过去的查询和交互中学习，从而随着时间的推移提供越来越准确和有用的建议。它还考虑了广泛的因素，例如代码复杂性、最佳实践和潜在错误或错误，以确保它生成的代码具有尽可能高的质量。</p><p></p><h2>对软件开发的潜在影响</h2><p></p><p></p><p>在亚马逊的软件开发生态系统中实施 CodeWhisperer 有望带来多项好处，包括：</p><p></p><h4>提高效率</h4><p></p><p></p><p>CodeWhisperer 旨在自动执行各种编码任务，使软件工程师能够专注于开发过程中更复杂和关键的方面。通过减少日常编码任务所需的时间和精力，人工智能编码助手可以显着提高整体效率和生产力。</p><p></p><h4>改善协作</h4><p></p><p></p><p>凭借其理解和响应自然语言查询的能力，CodeWhisperer 可以促进团队成员之间更好的沟通。这使得协作讨论和解决问题变得更加容易，从而导致更有效的团队合作和更快的进步。</p><p></p><h4>减少错误</h4><p></p><p></p><p>CodeWhisperer 可以通过向开发人员提供建议和指导来帮助最大限度地减少代码生成中的人为错误。这可确保最终产品更加健壮和可靠，减少可能影响用户体验的错误和缺陷。</p><p></p><h4>加速学习</h4><p></p><p></p><p>CodeWhisperer 可以作为初级开发人员的宝贵资源，提供即时指导和代码建议以增强他们的学习体验。通过提供对最佳实践和编码标准的实时反馈和洞察，该工具可以帮助加快学习曲线并提高经验不足的开发人员编写的代码质量。</p><p></p><h3>挑战与未来展望</h3><p></p><p></p><p>尽管 CodeWhisperer 的推出代表了 AI 辅助软件开发的一个重要里程碑，但必须解决潜在的挑战以确保其成功。以下是一些最重要的：</p><p></p><p></p><h4>准确性和安全性</h4><p></p><p></p><p>CodeWhisperer 生成的代码必须准确、可靠且安全。这意味着 AI 编码助手必须经过严格测试，以确保其生成的代码符合行业标准，并且没有漏洞和安全漏洞。</p><p></p><h4>遵守</h4><p></p><p></p><p>遵守行业标准对于软件开发至关重要，CodeWhisperer 的设计必须符合相关法规和标准。这包括遵守与软件开发相关的最佳实践和指南，以及遵守数据隐私和安全法规。</p><p></p><h4>适应性</h4><p></p><p></p><p>编程语言和开发框架不断发展的本质意味着 CodeWhisperer 必须具有适应性和灵活性，以跟上该领域的变化。这需要持续开发和更新，以确保 AI 编码助手在面对新技术和新兴技术时保持相关性和有效性。</p><p></p><h4>可用性</h4><p></p><p></p><p>虽然 CodeWhisperer 旨在简化编码过程并提高效率，但它还必须易于使用并可供软件开发团队的所有成员访问。这需要用户友好的界面和清晰的文档，以确保开发人员可以充分利用其功能。</p><p></p><h2>未来由人工智能驱动，但以人为主导</h2><p></p><p></p><p>亚马逊的 CodeWhisperer 等人工智能工具的采用预示着软件开发的新时代，在这个时代，人类和机器智能共同推动创新、效率和质量。随着 AI 的不断发展，我们可以期待看到更先进、更复杂的工具出现，从而改变软件的开发、部署和维护方式。</p><p></p><p>经亚马逊云科技授权转载，文章出处：<a href=\"https://www.yundongfang.com/Yun220423.html?trk=cndc-detail\">https://www.yundongfang.com/Yun220423.html?trk=cndc-detail</a>\"</p>",
    "publish_time": "2023-12-20 10:09:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度侯震宇：大模型将彻底改变AI原生应用研发范式",
    "url": "https://www.infoq.cn/article/BJB2FqRVpNNLIG10XBXA",
    "summary": "<p>12月20日，在2023百度云智大会·智算大会上，百度集团副总裁侯震宇以“大模型重构云计算”为主题发表演讲。他强调，AI原生时代，面向大模型的基础设施体系需要全面重构，为构建繁荣的AI原生生态筑牢底座。</p><p></p><p>侯震宇表示：“大模型重构云计算主要体现在三个层面：AI原生云将改变云计算的格局，MaaS ( Model as a Service ，模型即服务）会成为新的基础服务，AI原生应用催生新的研发范式。”</p><p></p><h3>1、在算力层，计算更智能</h3><p></p><p></p><p>在底层的云基础设施层，以往从互联网应用到移动互联网应用，底层都基于CPU计算芯片，而AI应用对GPU或异构计算的需求大幅增加，云市场的底层算力开始迁移到以GPU为主。</p><p></p><p>2023年第三季度，英伟达的营收已经超过英特尔，英伟达最新市值也超过英特尔1万亿美元，未来GPU的增长将远大于CPU。在这一趋势下，我们需要对面向大模型的云计算基础设施体系进行全面重构，以支撑AI原生应用系统落地。</p><p></p><p>具体来说，云计算的全面重构会表现在三大领域，即：面向模型的智算基础设施、面向数据的数据基础设施、面向应用的云基础设施全面升级，让计算更智能。</p><p></p><h3>2、在模型层，大模型正在成为通用的服务能力，即MaaS</h3><p></p><p></p><p>MaaS将大幅降低Al落地的门槛、实现真正的Al普惠，其依赖的新型IT基础设施也将进一步在底层颠覆现有的云计算市场格局。</p><p></p><p>从百度智能云的实践来看，自8月31日文心一言全面开放后至今的4个月，百度智能云千帆大模型平台（百度智能云推出的MaaS平台）上，API日调用量增长10倍，客户主要来自互联网、教育、电商、营销、手机、汽车等各行业。可以明显看到，最近半年，已经有很多企业真正把大模型用起来了。</p><p></p><h3>3、在应用层，应用开发的范式已经被彻底颠覆</h3><p></p><p></p><p>大模型理解、生成、逻辑、记忆的独特能力会催生A原生应用研发新范式，整个应用技术栈、数据流和业务流都将被改变。</p><p></p><p>原先基于CPU的应用开发主要是业务逻辑驱动，传统的AI研发需要针对每一个独立场景获取数据，再分别从头训练模型。而现在AI原生应用主要基于大模型能力，以数据驱动开发。企业可直接在基础大模型之上，利用场景数据微调出专属大模型，再用模型能力设计AI原生应用，无需从头训练大模型。随着企业业务扩大，逐渐积累出更多有竞争力的场景数据，进而反哺模型和应用效果提升，从而形成数据飞轮。</p><p></p><p>具体来说，大模型驱动的AI原生应用研发新范式展现出几个新变化：</p><p></p><p>首先是“新场景”。生成式大语言模型，在理解、生成、推理、记忆等多维度展现出超预期的能力，带来了智能涌现，由此催生了很多新的可落地的业务场景应用，如个人助理、智能文案创作、GBI（智能商业分析）、编码助手等。</p><p></p><p>第二是“新架构”。大模型具体在这些新场景落地的过程中，也产生了很多新的系统架构，如检索增强生成RAG，智能体Agent 等。</p><p></p><p>第三是“新开发生态”。以大模型为核心，开发者工具层也出现了一些新工具，包括编排工具LangChain、AI应用开发工具PromptFlow、数据框架Llamalndex等。</p><p></p><p>侯震宇表示，总体来说，构建繁荣的A原生应用生态，需要大模型、智能算力、AI原生应用研发新范式三要素相辅相成。大模型是AI原生应用的“大脑”，智能计算则为AI原生应用运行提供坚实支撑，新研发范式助力开发者高效基于大模型能力开发应用。数据飞轮是成功的AI原生应用的充分必要条件，让大模型能力高速迭代，产品体验持续进步。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4a20542591a70a414fe957dd6549bd6.png\" /></p><p></p><p>“我相信，真正非常闪耀的AI原生应用会在2024年诞生。”侯震宇说。</p><p>&nbsp;</p>",
    "publish_time": "2023-12-20 10:14:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "React 单元测试实例：快速上手指南",
    "url": "https://www.infoq.cn/article/1ypo54cMidb7PzRpxowD",
    "summary": "<p>你是否正计划为你的React代码编写测试？是否因找不到好的入门教程而感到苦恼？那么，这篇文章正是你所需要的。在本文中，我们将涵盖编写单元测试的所有步骤以及这个过程中可能遇到的错误和问题。</p><p>&nbsp;</p><p>本文使用了<a href=\"https://jestjs.io/\">Jest</a>\"和<a href=\"https://testing-library.com/\">React Testing Library</a>\"库。如果你想使用其他库也没关系，文中的一些基础知识也会对你有所帮助。</p><p>&nbsp;</p><p>本文涉及的全部代码都托管在GitHub上，文末提供了链接地址。</p><p>&nbsp;</p><p></p><h1>为什么要编写测试</h1><p></p><p>当然，不写测试代码也可以完成产品开发。用户、产品经理，甚至测试人员或QA都不在乎该产品是否有测试代码。但是你，作为一名开发人员，应该在乎！</p><p>&nbsp;</p><p>假设你有一个拥有数万用户的网站，当你对一个公共的工具函数做了一些重构（或添加了一个功能修复），并在应用中某个调用它的地方进行了测试，表明该函数可以正常运行。于是你选择在周五上线（这是个低级错误）。然后，该函数在应用中的其他地方无法运行，导致网站在周末期间出现线上故障。 此时，你多么希望这些地方能有测试代码，可以在发布生产之前自动运行，从而避免此次故障。</p><p>&nbsp;</p><p>上述场景比你想象的还要普遍。你可能还没遇到过（不过，这是迟早的事情），但是包括我在内的很多工程师都已经遇到过了。</p><p>&nbsp;</p><p>因此，测试代码之所以很重要，主要有以下几个原因：</p><p>&nbsp;</p><p>🚀 增强你对代码发布上线的信心。</p><p>📜 测试代码本身也是一种文档。</p><p>🛠️ 有助于调试和重构。</p><p>⌛️ 从长远来看，有助于减少开发时间。</p><p>&nbsp;</p><p>对于所有希望晋升的初级开发人员来说，务必要具备编写测试代码的能力。</p><p>&nbsp;</p><p></p><h1>测试教程</h1><p></p><p></p><p>我们将从零开始教你编写测试代码，所以请准备好终端。首先，我们使用vite创建一个示例项目。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23d333bb0c9cf450b18148db95163a48.png\" /></p><p></p><p>&nbsp;</p><p>在创建项目后，使用以下命令运行它。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7ce59376e4453453392ad383b68f45e1.png\" /></p><p></p><p>&nbsp;</p><p>程序运行之后，你会在页面上看到一个demo应用。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf68ee643b5bc3745b3e62bc2ce94fcf.png\" /></p><p></p><p>&nbsp;</p><p>我们不会给该应用添加新功能，但为了给按钮编写测试代码，我们需要将按钮重构为一个单独的组件。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b4ea3dc23d1e7136c0b95352829016ee.png\" /></p><p></p><p>&nbsp;</p><p>接下来，我们在页面上添加两个按钮：</p><p>&nbsp;</p><p>一个按钮的功能是点击时将count的值乘以2。另外一个按钮的功能是点击时按以下顺序执行操作：如果count的值以0结尾，那么就将它的值除以2。如果count的值是斐波那契数，那么就将它的值加1。否则，将count的值进行平方操作。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/755fe81d7644d23570e3fc730f88486f.png\" /></p><p></p><p>&nbsp;</p><p>我们需要在utils模块中声明上面代码中用到的两个函数。同时，我们也声明了一些辅助函数，但由于其他地方并不会用到这些辅助函数，因此这里不需要做导出。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e81e4956bbc7bd0598f76ff4273c5e35.png\" /></p><p></p><p>&nbsp;</p><p>代码已经准备好，现在可以开始编写测试代码了。这里我们跳过React代码，直接先给工具函数编写测试。这有助于我们了解Jest框架的大致用法。</p><p>&nbsp;</p><p>下面，让我们为doubleTheNum函数编写测试。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4ec3629fcbdb09db9b1d87c19026826.png\" /></p><p></p><p>&nbsp;</p><p>上面的代码用于测试我们的函数是否可以按预期执行。任何测试代码都会包含以下这些关键组件：</p><p>&nbsp;</p><p>describe函数：第一个参数是字符串，它会在测试运行的时候显示。第二个参数则是测试实际执行的函数。describe函数的主要作用是对同类型的测试进行分组。这里只有一个测试，在另外一个示例中，你会看到其中有多个测试。it函数：其参数结构和describe函数类似。但这里的字符串参数应该尽可能详细地描述测试函数的具体内容。当然，你也可以使用test函数替代it。expect语句块：此函数中的前三行很简单。其最后一行是通过断言来判断doubleTheNum函数能否正确运行。此外，这里我们还用到了toEqual匹配器函数。</p><p>&nbsp;</p><p>Jest提供了很多匹配器，例如：</p><p>&nbsp;</p><p>toBeNull用于匹配null。toBeTruthy用于匹配判定结果为true的语句。</p><p>&nbsp;</p><p>想了解关于匹配器的更多信息，参考如下链接：</p><p><a href=\"https://jestjs.io/docs/using-matchers\">https://jestjs.io/docs/using-matchers</a>\"</p><p>&nbsp;</p><p>为了运行测试，我们需要先安装Jest：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2e1a80a0229feaa5402672d0e4e76f21.png\" /></p><p></p><p>&nbsp;</p><p>然后在package.json中添加测试脚本：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/021ddf8b4f904b18db3395ffd7ac9d28.png\" /></p><p></p><p>&nbsp;</p><p>最后，通过执行yarn test命令来运行测试。</p><p>&nbsp;</p><p>对于大多数人来说，上面的步骤已经足够了。但如果你遇到了与模块导入或TypeScript相关的任何问题，请按以下步骤进行操作：</p><p>&nbsp;</p><p>安装并设置@babel/preset-env：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b779fbb03349520acbc69d2cd7972b13.png\" /></p><p></p><p>&nbsp;</p><p>然后，将它配置到package.josn中：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/ca49063018bcd3c622bfe22f8ce966da.png\" /></p><p></p><p>&nbsp;</p><p>安装TypeScript依赖库：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/1082431bb2f809f966c12f5f00b29e9e.png\" /></p><p></p><p>&nbsp;</p><p>然后，在jest.config.ts中添加Jest配置：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/907d52e74d61ad00e70a9c018c902832.png\" /></p><p></p><p>&nbsp;</p><p>然后执行测试，结果如下：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f32c6afc2bb452c03a2cf9a1bd5df6ae.png\" /></p><p></p><p>&nbsp;</p><p>从输出中可以看到我们在describe和it函数中声明的字符串信息。</p><p>&nbsp;</p><p>🎉 恭喜，你完成了第一个测试！</p><p>&nbsp;</p><p></p><blockquote>喜欢这篇文章吗？如果觉得还不错，我推荐你看看我的另一篇最受欢迎的文章<a href=\"https://dev.to/thesanjeevsharma/just-redux-the-complete-guide-44d5\">《Redux完整指南》</a>\"，阅读量高达2.5万：<a href=\"https://dev.to/thesanjeevsharma/just-redux-the-complete-guide-44d5\">https://dev.to/thesanjeevsharma/just-redux-the-complete-guide-44d5</a>\"</blockquote><p></p><p>&nbsp;</p><p>接下来，我们给funkyNum函数编写测试。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d18abb3c06d906e76689816f454d061c.png\" /></p><p></p><p>&nbsp;</p><p>编写测试时，应该尽量多地覆盖函数的分支和语句。测试覆盖率越高会让人越有信心。</p><p>&nbsp;</p><p>如果你再次运行测试，应该会看到以下输出。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43f3b575208fcdb4a77806b26cabf23a.png\" /></p><p></p><p>&nbsp;</p><p>理想情况下，我们也应该为isFibonacci和isPerfectSquare函数编写单独的describe语句块。在单元测试中，测试代码应该是互相独立的。简洁起见，这里我们没有这样做。</p><p>&nbsp;</p><p>💡小提示</p><p>&nbsp;</p><p>通过调用.skip或test.skip来跳过任何测试，或调用describe.skip来跳过整个测试块。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9ac7ed914c3e510737c5bb5e2706294a.png\" /></p><p></p><p>&nbsp;</p><p>通过调用it.only或test.only执行单个测试。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/67cce5813b3c3ec30991ffc61a525454.png\" /></p><p></p><p>&nbsp;</p><p>上面我们已经介绍了如何使用Jest进行JS代码的测试。现在，让我们深入探讨下关于React的测试。</p><p>&nbsp;</p><p>在开始之前，我们还需要安装一些依赖库：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/99ad433321fbe10be116b12b30228cb7.png\" /></p><p></p><p>&nbsp;</p><p>同时，还需要在jest.config.ts中添加环境：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/031c37fff58a94db093a06a92423066d.png\" /></p><p></p><p>&nbsp;</p><p>下面我们给CounterButton组件编写一个最基础的测试：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/098c25daedd09aac49b8149bae9557a0.png\" /></p><p></p><p>&nbsp;</p><p>在上面的代码中，我们提供了所需的props&nbsp;，并尝试渲染组件。对于任何组件，这都应该是你为它编写的第一个测试。因为如果该测试无法通过，那么其他测试就毫无用处。</p><p>&nbsp;</p><p>RTL（React Testing Library）的render函数将在document.body中渲染传入的组件。</p><p>&nbsp;</p><p>它还返回了一些诸如getByText这样的查询方法，可用于在DOM中查找元素。</p><p>&nbsp;</p><p>点击<a href=\"https://testing-library.com/docs/queries/about/\">这里</a>\"查阅所有的查询方法。</p><p>&nbsp;</p><p>如果你再次运行测试，应该可以看到2组测试——全部为绿色且通过。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f374aa9f5d2bb6cdbcf8bf9dcac64ec.png\" /></p><p></p><p>&nbsp;</p><p>我们编写的第二个测试是测试组件对于props的反应。如果各个prop之间没有互相依赖，那么应该为每个prop参数都编写单独的测试。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6d1e8835c40ee84e948a848c21dd3de2.png\" /></p><p></p><p>&nbsp;</p><p>getByText函数是一种查询方法，可以让我们通过字符串来获取元素。</p><p>&nbsp;</p><p>toBeInTheDocument函数是一个和toEqual类似的匹配器。Jest默认不提供该函数，需要在安装@testing-library/jest-dom库之后才能使用。</p><p>&nbsp;</p><p>不同的环境有不同的包，例如在React Native环境中，需要使用@testing-library/jest-native。</p><p>&nbsp;</p><p>如果你再次运行测试，测试应该也会通过。</p><p>&nbsp;</p><p>最后，我们来编写本文的最后一个测试，同时也是最重要的一个。我们将编写一个测试来检查点击事件处理程序是否按预期工作。</p><p>&nbsp;</p><p>为了生成用户事件（例如点击和按键），我们需要安装另外一个包。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/99125f68bb66364842dcb5c51c444cec.png\" /></p><p></p><p>&nbsp;</p><p>与之前的测试代码相比，这次的测试代码几乎一样，只有一些微小的差异。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/65/65da5af9c546d4190121fb400413acb0.png\" /></p><p></p><p>&nbsp;</p><p>注意：由于是模拟用户事件，所以该函数异步执行。</p><p>&nbsp;</p><p>第一行的jest.fn()是一个模拟函数。在测试运行时，可以通过它跟踪诸如调用参数、调用次数等很多非常有用的信息。类似这样的函数，以后你会看到很多。</p><p>&nbsp;</p><p>我们还使用了一种新的查询方法getByRole来查找按钮元素。</p><p>&nbsp;</p><p>在检查模拟函数是否被调用之前，我们需要先等待点击事件完成。</p><p>&nbsp;</p><p>就是这样！如果你运行测试，它们应该也会通过。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d6/d6e6932126142a25934103e271c12018.png\" /></p><p></p><p>&nbsp;</p><p>🔗&nbsp;<a href=\"https://github.com/thesanjeevsharma/devto-unit-testing\">这里</a>\"获取所有代码：</p><p><a href=\"https://github.com/thesanjeevsharma/devto-unit-testing\">https://github.com/thesanjeevsharma/devto-unit-testing</a>\"</p><p>&nbsp;</p><p></p><h1>下一步</h1><p></p><p></p><p>如果你遵循本文成功地完成了测试代码的编写，那么你可以开始在自己的代码库中添加测试代码并且进一步探索各种测试功能了。</p><p>&nbsp;</p><p>另外，我建议你进一步了解以下几个方面的内容：</p><p>getByTestId——这是一个使用很普遍的查询方法。当其他方法都不好使的时候，可以用它。了解<a href=\"https://jestjs.io/docs/setup-teardown\">Setup和Teardown</a>\"方法。它将提升你的测试水平。学习如何模拟npm模块、API 调用、全局状态和上下文等。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://dev.to/thesanjeevsharma/writing-your-first-unit-test-in-react-150h\">https://dev.to/thesanjeevsharma/writing-your-first-unit-test-in-react-150h</a>\"</p><p>&nbsp;</p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://www.infoq.cn/article/4ydKY3wbZDp7Eei0JJm1\">React&nbsp;JS 广受业界认可，高级开发者年薪百万</a>\"</p><p><a href=\"https://www.infoq.cn/article/Tv3SyqoivXMWUoj8qSMT\">从新&nbsp;React&nbsp;文档看未来 Web 的开发趋势</a>\"</p><p><a href=\"https://www.infoq.cn/article/CZKMjHaxbf1Z7xcSzisX\">我被&nbsp;React&nbsp;劫持了，很痛苦又离不开</a>\"</p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247516803&amp;idx=1&amp;sn=f584917f8afe2f7bb10f8686acb040cf&amp;chksm=f95237c0ce25bed6980bdc34c630cbda97d0325a489663440b4c904499ca81da31729d476476&amp;scene=27#wechat_redirect\">React&nbsp;开发者们的 Solid.js&nbsp;快速入门教程</a>\"</p>",
    "publish_time": "2023-12-20 10:33:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "容联云大模型应用升级，发布容犀智能与容犀Copilot",
    "url": "https://www.infoq.cn/article/ryKfCI9KaLJVhHbDGUsL",
    "summary": "<p>12月19日，容联云“未来生成式——大模型应用升级新品发布会”在北京举办。发布会上，容联云正式发布基于自研赤兔大模型的全新产品品牌【容犀智能】及生成式应用【容犀Copilot】。</p><p></p><p>容犀智能从业务场景出发，结合全流程链路的数据能力、大小模型应用、端到端解决方案落地能力，弥合企业在数智化转型时技术与业务应用的差距，寻求投入与效益的最佳平衡，帮助企业实现营销服数智化升级。全新的容犀智能品牌将包含容犀AICC、容犀Desk、诸葛IO/CDP/CEP、容犀Copilot四大模块。</p><p><img src=\"https://static001.geekbang.org/infoq/63/637bb164d35cf34c46551051870fa3b5.png\" /></p><p></p><h2>容犀Copilot：大模型时代的实时AI领航员</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d34121133ce3f4939a47c2d9f3469e61.png\" /></p><p>容联云产业数字云事业群副总经理孔淼</p><p></p><p>在发布会现场，容联云产业数字云事业群副总经理孔淼宣布了容犀智能品牌的诞生，同时正式发布全新生成式应用容犀Copilot。</p><p><img src=\"https://static001.geekbang.org/infoq/9a/9aa329f7576f9e9f385afbf680816b5b.png\" /></p><p></p><p>容犀Copilot集“全链路数据+大小模型+分析洞察”于一体，在每一次的服务与营销场景中，实时根据企业与客户产生的会话数据与业务数据，结合“聚焦客户联络全场景的大小模型”与“会话洞察”能力，产出最佳沟通策略，打造销售和客服的实时AI领航员。</p><p><img src=\"https://static001.geekbang.org/infoq/86/86e8a3ae45918f593f6d7f5991a4669b.png\" /></p><p></p><p>首先是大模型话术挖掘，容犀Copilot后台一键快速对海量历史会话数据进行核对筛选，挑选出最佳话术并生成金牌话术，兼顾质与量的同时，挖掘出客户高频关注的问题，从问题中洞悉业务痛点。其次，大模型智能知识库可以帮助企业从零开始、低成本地快速构建话术库，包括理解文档知识、知识快搜、智能问答等，大幅提升构建效率。最后，通过大模型会话洞察，高效便捷洞察每一通会话沟通情况，分析客户诉求，精准诊断问题并优化。回归实际业务本身，容犀Copilot深入金融行业细分场景，打造场景化客服助手，譬如分期挽留助手、荐卡挽留助手、投诉安抚助手等，实时辅助快速洞察客户需求，推荐最佳应答话术，诊断客户情绪变化，提醒措辞及注意事项。</p><p><img src=\"https://static001.geekbang.org/infoq/04/0499aa61c33c701f4945ac2f5cfc1096.png\" /></p><p>容联云AI研究院院长 刘杰</p><p></p><p>容犀智能与容犀Copilot的落地意味着容联云赤兔大模型在智能性、可控性、投产比上都有了新的跃升，容联云AI研究院院长刘杰详解了赤兔大模型的落地路径。在智能性上，通过检索增强，会话分析，逻辑推理，数据分析等多维度深度理解分析，实现全面的沟通会话智能。在可控性上，快速对业务上的各种规定要求进行对齐，明确各个知识模块的范围界定，只处理业务角色相关问题，保证安全可控。在投产比上，通过大小模型配合构建最适合业务规模的AI底座。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4c8ade842be0e93fc5adcde49097e115.png\" /></p><p>北京华为云CTO&nbsp;丁晨</p><p></p><p>现场，北京华为云CTO丁晨也带来了与容联云合作的现状与展望，在基于9月份双方签订的战略合作协议之上达成深度合作共识，结合华为5G、大数据、鲲鹏、昇腾、大模型等前沿技术，持续创新行业大模型和场景化应用，打造云上客服联合解决方案。</p><p></p><h2>拓展海外市场，容联云业务持续升级与落地</h2><p></p><p>&nbsp;</p><p>容联云在发布会上还宣布了其业务的升级与落地计划。</p><p><img src=\"https://static001.geekbang.org/infoq/74/744c65b34f5062057a7a5504886d34bd.png\" /></p><p>容联云数字智能云AI产品专家 刘倩</p><p></p><p>持续深耕海外市场，容联七陌在日本和东南亚已累计服务上百家客户，并计划在未来推出更多AIGC的智能交互应用。在日本市场，容联七陌在2023年3月即推出了AIGC的智能交互应用，实现了诸如智能文档问答、AIGC电话交互等能力，是最早一批拥有AIGC落地应用的企业之一。容联七陌助力全日本最大的外国人求职服务会社Y社，应用语音机器人进行会员邀约注册、求职意向采集等服务。客户S社为印刷、活动推广、媒体业务服务商，在容联七陌文本机器人助力下，实现了自助答疑，节省人力成本60%以上，大量进订单咨询均由机器人独立接待回复。容联云数字智能云AI产品专家刘倩表示，未来，容联七陌将持续自研智能客服技术核心，在全球化规模营收支撑下，探索AIGC时代的新服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/2394097275a24b3714fd7a7dadd8b51f.png\" /></p><p>容联云CV产品解决方案总监李杰</p><p>&nbsp;</p><p>同时，容联云还将推出基于AIOT平台的金融银行场景解决方案，涵盖安防风控、合规运营、重资管理等多个方面。容联云CV产品解决方案总监李杰表示，该方案将利用多模态大模型和AIOT平台的技术优势，为金融银行业提供更加智能化、高效化的智慧营业厅。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e60501b7df596018f0d2f02d4c179f81.png\" /></p><p>阳光出行智能服务部负责人王妙心</p><p>&nbsp;</p><p>在发布会现场，阳光出行作为标杆客户代表，分享了其使用大模型等AI技术的经验。阳光出行通过使用大模型等AI技术优化了其服务流程和用户体验，实现了商业价值的提升。</p><p>&nbsp;</p><p>大模型时代，容联云将通过沟通智能、数据智能、链路智能等重构企业面向内部和面向外部的多元化应用，帮助企业打造全生命周期的智慧经营闭环，从而更好地感知客户需求，驱动数据的精细运营，创造个性化体验，让数智化真正可用、有用，从而带动整体业务增长。</p><p>&nbsp;</p>",
    "publish_time": "2023-12-20 10:39:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里巴巴通义实验室 NLP 资深算法专家张佶确认出席 QCon 上海，分享通义星尘——个性化大模型驱动的 AI 对话新范式",
    "url": "https://www.infoq.cn/article/Df1NwOv9o5k95deVBjpS",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1220&amp;utm_content=zhangji\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。阿里巴巴通义实验室 NLP 资深算法专家张佶将发表题为《<a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5599?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1220&amp;utm_content=zhangji\">通义星尘——个性化大模型驱动的 AI 对话新范式</a>\"》主题分享，探讨 AI 对话领域的最新发展趋势以及相关大模型关键技术和场景。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5599?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1220&amp;utm_content=zhangji\">张佶</a>\"，负责通义大模型的应用研究和落地，带领的研究团队发表国际顶级会议论文 40 余篇，曾在机器阅读理解（MRC）、视觉问答（VQA）等国际权威榜单中实现首次超越人类基准的成绩。曾领导开发的阿里小蜜算法平台服务于阿里全球 23 个语言、130 多个国家的电商用户。他在本次会议的演讲内容如下：</p><p></p><p>演讲：通义星尘——个性化大模型驱动的 AI 对话新范式</p><p></p><p>随着近年来大模型技术的快速演进，AI 对话领域迸发出全新的发展可能和想象空间，阿里巴巴通义大模型近期发布了个性化 AI 角色创作和对话平台——通义星尘，在保持通用大模型基础能力的情况下，延伸出个性化大模型，提供拟人化、场景化、多模态和共情的对话能力以及复杂任务执行能力。本次分享将介绍 AI 对话领域的最新发展趋势以及相关大模型关键技术。</p><p></p><p>演讲提纲：</p><p></p><p>AI 对话系统的进展和趋势通用大模型和个性化大模型个性化大模型的 4 个关键技术</p><p>○ 个性化、大小模型协同的 AI 智能体、多模态、安全负责的 AI</p><p>个性化大模型的场景未来展望</p><p></p><p>听众收益点：</p><p></p><p>○ 了解大模型对 AI 对话领域带来的新趋势</p><p>○ 个性化大模型的关键技术</p><p>○ 落地场景和挑战</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart\">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的性能优化</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>12 月 28-29 日，QCon 全球软件开发大会即将落地上海，中国科学院外籍院士、国际数据库专家樊文飞院士，英特尔大数据技术全球 CTO 戴金权等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。</p><p>这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，感兴趣的朋友可以扫描下方二维码，查看大会详细日程。咨询购票可联系票务经理 18514549229。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-12-20 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百川发布全新Baichuan2-Turbo系列API产品：构建“大模型+搜索增强”技术栈，解决99% 定制化需求",
    "url": "https://www.infoq.cn/article/bLJ3DBwctXlp5DdYOhTz",
    "summary": "<p>12月19日，<a href=\"https://www.infoq.cn/article/OcjyhximVsWg4o5rboDB?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">百川智能</a>\"宣布开放基于搜索增强的Baichuan2-Turbo系列API，包含Baichuan2-Turbo-192K 及Baichuan2-Turbo。在支持192K超长上下文窗口的基础上，还增加了搜索增强知识库的能力。即日起，API用户可上传文本资料来创建自身专属知识库，从而根据自身业务需求打造更完整、高效的智能解决方案。</p><p>&nbsp;</p><p>“Baichuan2-Turbo 192K API发布，一次可以输入35万字，代表今天行业最高的长窗口水准。”王小川说道。</p><p>&nbsp;</p><p>此外，百川智能还升级了官网模型体验，目前其官网大模型已支持PDF、Word等多种文本上传以及URL网址输入，用户可通过官网入口体验搜索增强和长窗口加持后的通用智能。</p><p>&nbsp;</p><p>体验官网：https://platform.baichuan-ai.com/playground</p><p>&nbsp;</p><p>百川智能认为，搜索增强是大模型落地应用的关键，能够有效解决幻觉、时效性差、专业领域知识不足等阻碍大模型应用的核心问题。</p><p>&nbsp;</p><p>一方面，搜索增强技术能有效提升模型性能，并且使大模型能“外挂硬盘”，实现互联网实时信息+企业完整知识库的“全知”；另一方面，搜索增强技术还能让大模型精准理解用户意图，在互联网和专业/企业知识库海量的文档中找到与用户意图最相关的知识，然后将足够多的知识加载到上下文窗口，借助长窗口模型对搜索结果做进一步的总结和提炼，更充分地发挥上下文窗口能力，帮助模型生成最优结果，从而实现各技术模块之间的联动，形成一个闭环的强大能力网络。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfadfb0f2630403919d2da07fe5cf3c4.gif\" /></p><p></p><p></p><h2>“大模型+搜索”构成完整技术栈</h2><p></p><p>&nbsp;</p><p>“没有搜索增强的大模型在企业里是没法落地的。”王小川说道。他解释道，很多行业需要垂直大模型来解决问题。普通改造有两个做法：一是SFT、二是Post-train，但两种方式都需要模型公司人才的介入，投入的成本巨大，企业做这件事情是一个巨大的挑战和资源消耗。一旦数据或算法更新，企业还得重训一次。因此，用行业大模型解决企业应用问题，虽然听着很好，但今天并没有良好的实践。</p><p>&nbsp;</p><p>另外，大模型自身也并不完美，幻觉、时效性差、缺乏专业领域知识等问题，是其落地千行百业必须要面对的挑战。</p><p>&nbsp;</p><p>当前，业界探索了多种解决方案，包括扩大参数规模、扩展上下文窗口长度、为大模型接入外部数据库，使用特定数据训练或微调垂直行业大模型等。这些路线各有优势，但也都存在自身的局限。例如，持续扩大模型参数虽然能够不断提升模型智能，但是需要海量数据和算力的支撑，巨额的成本对中小企业非常不友好，而且完全依靠预训练也很难解决模型的幻觉、时效性等问题。</p><p>&nbsp;</p><p>在百川智能的技术思考中，“大模型+搜索增强”是大模型时代的新计算机，大模型类似于计算机的CPU，通过预训练将知识内化在模型内部，然后根据用户的Prompt生成结果；上下文窗口可以看做计算机的内存，存储了当下正在处理的文本；互联网实时信息与企业完整知识库共同构成了大模型时代的硬盘。</p><p>&nbsp;</p><p>百川智能认为，这样将大模型加上“外挂硬盘”的方式，能够让其在大多数领域里更加实用。</p><p>&nbsp;</p><p>基于这一技术理念，百川智能以Baichuan2大模型为核心，将搜索增强技术与大模型深度融合，结合此前推出的超长上下文窗口，构建了一套“大模型+搜索增强”的完整技术栈，实现了大模型和领域知识、全网知识的链接。</p><p>&nbsp;</p><p>百川智能表示，其在业内探索的长上下文窗口和向量数据库路径基础上，将向量数据库升级为搜索增强知识库，极大提升了大模型获取外部知识的能力，并且把搜索增强知识库和超长上下文窗口结合，让模型可以连接全部企业知识库以及全网信息，能够替代绝大部分的企业个性化微调，解决99%企业知识库的定制化需求。</p><p></p><p><img src=\"https://uploader.shimo.im/f/26Xlp6vmIDMXw8ir.gif?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDMwNTIzMzYsImZpbGVHVUlEIjoiZFBrcGQ1S2dnZ3VQWndrTyIsImlhdCI6MTcwMzA1MjAzNiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.zDTYqgcGeBwQTquA52Iovs1xU1DBkLs5nPo2yzKxofA\" /></p><p></p><p>&nbsp;</p><p></p><h2>稀疏检索与向量检索并行</h2><p></p><p>&nbsp;</p><p>在大语言模型时代，用户需求（Prompt）与搜索的对齐成为了大模型获取外部知识过程中最为核心的问题。为更精准理解用户意图，百川智能使用自研大语言模型对用户意图理解进行微调，将用户连续多轮、口语化的Prompt信息转换为更符合传统搜索引擎理解的关键词或语义结构。</p><p>&nbsp;</p><p>此外，百川智能还参考Meta的CoVe（Chain-of-Verification Reduces Hallucination in Large Language Models）技术，将真实场景的用户复杂问题拆分成多个独立可并行检索的子结构问题，从而让大模型可以针对每个子问题进行定向的知识库搜索，提供更加准确和详尽的答案。同时，通过自研的TSF(Think Step-Further)技术，百川智能知识库可推断出用户输入背后深层的问题，更精准的理解用户的意图，进而引导模型回答出更有价值的答案。</p><p>&nbsp;</p><p>在精确理解用户需求基础上，想要进一步提升知识获取的效率和准确性，还需要借助向量模型解决用户需求和知识库的语义匹配问题。为此，百川智能表示，自研的向量模型使用了超过 1.5T token 的高质量中文数据进行预训练，通过自研的损失函数解决了对比学习对于 batchsize 的依赖，在C-MTEB评测集 6 个任务（分类、聚类、文本推理、排序、检索、文本相似度） 中的 5 个任务上都取得了效果的大幅领先，综合分数登上榜首：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/82373d29b786f1d20047287d0b71956f.png\" /></p><p></p><p>虽然向量检索是当下构建大模型知识库的主流方法，但向量模型的效果过于依赖训练数据的覆盖，在训练数据未覆盖的领域泛化能力会有明显折扣，并且用户 prompt 和知识库中文档长度的差距也给向量检索带来了很大挑战。</p><p>&nbsp;</p><p>对此，百川智能在向量检索的基础上融合了稀疏检索和 rerank模型。百川智能表示，通过稀疏检索与向量检索并行的混合检索方式，将目标文档的召回率提升到了 95%，而市面上绝大多数开源向量模型的召回率为80%。</p><p>&nbsp;</p><p>为解决模型“幻觉”加重现象，百川智能表示，在通用RAG（检索增强生成）基础上首创了Self-Critique大模型自省技术，该技术能够让大模型基于Prompt对检索回来的内容从相关性、可用性等角度进行自省，筛选出最优质、最匹配的候选内容，提升材料的知识密度和广度，并降低检索结果中的知识噪声。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/829b8fd28e1199aba34723ce1a1c0cbd.gif\" /></p><p></p><p></p><h2>长窗口+搜索，实现“真·大海捞针”</h2><p></p><p>&nbsp;</p><p>长上下文窗口虽然可以接收更长的文本信息，但扩展上下文窗口长度会影响模型性能，在当前技术下存在上限。另外，长窗口每次回答问题都要将文档全部重读一遍，推理效率低、成本高。</p><p>&nbsp;</p><p>百川智能通过长窗口+搜索增强的方式，在192K长上下文窗口的基础上，将大模型能够获取的原本文本规模提升了两个数量级，达到5000万tokens。通过搜索增强，模型可以先根据用户的Prompt在海量的文档中检索出最相关的内容，再将这些文档与Prompt一起放到长窗口中，有效节省了推理费用和时间成本。</p><p>&nbsp;</p><p>“大海捞针”测试（Needle in the Heystack）是由海外知名AI创业者兼开发者 Greg Kamradt 设计的，业内公认最权威的大模型长文本准确度测试方法。在“大海捞针”测试中，百川智能使用中文场景，实验配置如下：</p><p>&nbsp;</p><p>大海(HayStack)：博金大模型挑战赛-金融数据集中的80份长金融文档。针（Needle）：2023 年 12 月 16 日，王小川会上进一步分享了大模型的新思考。在王小川看来，大模型带来的新的开发范式下，产品经理的出发点，应该从思考产品市场匹配（PMF），到思考技术与产品的匹配怎么做，即 TPF（Technology Product Fit，技术产品匹配）。查询问题：王小川认为大模型时代下，产品经理的出发点是什么？</p><p>&nbsp;</p><p>&nbsp;</p><p>对于192k token以内的请求，百川智能可以实现100%回答精度：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3eaba31c8745178d7908d6d325ee31e3.jpeg\" /></p><p></p><p>而对于192k token以上的文档数据，百川智能结合搜索系统，将测试集上下文长度扩展到 5000万 tokens，分别评测了纯向量检索和稀疏检索+向量检索的检索的效果。</p><p>&nbsp;</p><p>测试结果显示，稀疏检索+向量检索的方式可以实现95%的回答精度，即使在 5000万tokens的数据集中也可以做到接近全域满分，而单纯的向量检索只能实现 80%的回答精度。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-12-20 14:05:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "前端老手10年心得，JavaScript/TypeScript项目保养实用指南",
    "url": "https://www.infoq.cn/article/MJBQC2Bumv2lyzZMUR18",
    "summary": "<p>有时候，JavaScript（甚至带有类型检查的TypeScript）会因为其不可预测的特性和缺乏约定而遭到批评。对于那些知道JavaScript是为web浏览器设计的脚本语言的人来说，这就不足为奇了。</p><p>&nbsp;</p><p>但是，现在它已经成为开发全栈web的首选语言，也是跨平台移动应用的热门方案。那么，当开发人员的JavaScript/TypeScript代码库开始老化，由此带来的复杂性痛苦地增长时，他们该采取什么行动才能最大限度地减少资源浪费并保持工作满意度呢？</p><p>&nbsp;</p><p>本文将基于我10多年来编写JavaScript代码的经验和5年多拯救JS/TS项目的经历，向读者介绍如下内容：</p><p></p><p>如何评估JS/TS代码库的质量和风险。对于需要修复的部分，该如何确定其优先级。有哪些非破坏性的方法可以让JS/TS代码库逐渐变得更健康。</p><p>&nbsp;</p><p></p><h2>清理工作台</h2><p></p><p>在开发下一个特性时，每个警告、类型错误或非正常的测试都会让开发人员浪费时间、精力和专注度。</p><p>&nbsp;</p><p>代码警告尤其令人讨厌，因为开发人员会习惯性地忽略它们，“只要一切按预期运行就好”。因此，它们会迅速累积，当我们遇到缺陷、事故或系统的意外行为时，就很难将其作为有用的线索。</p><p>&nbsp;</p><p>类型错误就是一个很好的样例。当我们的用户遵循“快乐路径（happy path）”时，这些错误似乎无关紧要，因为软件似乎能够按照预期运行。所以，我们可能会使用@ts-ignore、any或类型断言来暂时忽略它们。但是，这样做的话，就意味着如果有一天用户选择不同的路径，就会面临运行时错误。</p><p>&nbsp;</p><p>这样的话，开发人员就需要调查、重现和修复一个新的缺陷，而这个缺陷恰恰是他们几个月前允许走捷径所造成的。如果你的代码被各种警告和/或暂时忽略这些警告削弱了质量，那么找到这个捷径将耗费大量的时间。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4e64c09fccc2014700b8352addbad2d.jpeg\" /></p><p></p><p>&nbsp;当生产环境的数据库因“内存不足”错误而崩溃时，该警告可能会帮助开发人员找到崩溃的原因</p><p>&nbsp;</p><p>警告和类型错误是查找缺陷和事故的线索。我们累积（或忽略）的警告和错误越多，开发人员就会花费越多的时间去调查。如果代码是他们很久以前编写的，那情况就会更糟糕了。</p><p>&nbsp;</p><p></p><h3>我们能做些什么呢？</h3><p></p><p></p><p>确保开发人员在开发过程中能够尽快看到警告和类型错误。这不应该花费额外的成本。如果可能的话，集成到他们的IDE中。不要让警告和类型错误累积。尽快修复它们。提高信噪比。如果团队一致认为某条引发警告和类型错误的规则没有用处的话，就干脆禁用它。如果你确信需要在代码的特定部分忽略掉某条规则的话（也就是，使用code&gt;@ts-ignore、any或类型断言），请添加注释以记录忽略该规则的原因。不要在运行时添加try-catch代码块来捕获编程错误（比如，业务逻辑中意料之外的未定义值）。而是要使用这种代码块来处理外部系统的预期错误（如输入/输出异常、校验、环境问题等）。在开发过程中，应使用静态代码分析和单元测试来捕获编程错误。不要让带有警告和类型错误的代码进入生产环境。使用持续集成流水线来强制要求这一规则。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/0600e7070d697572d99a557e05099e02.png\" /></p><p></p><p>&nbsp;类型检查器认为缺少一个预期的属性。忽略这个错误将意味着要承担持久化不一致数据的风险，在几个月之后，你可能需要花费几天的时间来调查和解决这个问题</p><p>&nbsp;</p><p></p><h3>我们可以使用哪些工具来实现这一目标呢？</h3><p></p><p></p><p>有许多静态代码分析工具可供使用，最常用的包括：</p><p></p><p><a href=\"https://www.npmjs.com/package/eslint\">ESLint</a>\"，能够用来探测代码中的语法错误和反模式；<a href=\"https://www.npmjs.com/package/typescript\">TypeScript</a>\"（启用严格的规则），借助.ts文件或<a href=\"https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html\">JSDoc注解</a>\"以探测类型错误；除此之外，<a href=\"https://www.sonarsource.com/products/sonarcloud/\">SonarCloud</a>\"、<a href=\"https://sourcegraph.com/\">SourceGraph</a>\"、<a href=\"https://www.codacy.com/\">Codacy</a>\"或类似的在线工具服务也有助于跟踪共享代码库中多个代码质量度量指标的变化情况。</p><p>&nbsp;</p><p>警告也可能来自其他工具：依赖安装器（如npm和yarn）、打包器（如webpack）、代码处理器（babel、scss）和执行环境（CI运行器）。不要忽视它们！</p><p>&nbsp;</p><p>如果遵循这些建议会让你的代码变得非常冗长和/或复杂（比如防御式代码），你可以需要对其进行重新设计。</p><p>&nbsp;</p><p><code lang=\"null\"> \"scripts\": {\n    \"lint\": \"eslint .\",\n    \"lint:fix\": \"eslint . --fix\",\n    \"lint:errors\": \"eslint . --quiet\",\n    \"lint:typescript\": \"tsc --noEmit --skipLibCheck\",\n    \"lint:jsdoc-typing\": \"tsc --noEmit --allowJs `find ./ -name '*.js' -name '*.d.ts'`\"\n  },</code></p><p></p><p>借助静态代码分析器和npm脚本，能够让开发人员轻松快速地探测有问题的代码</p><p>&nbsp;</p><p></p><h3>后续该怎么办？</h3><p></p><p></p><p>安装和配置静态代码分析工具是一个良好的开端，但这还不够。</p><p>&nbsp;</p><p>要想取得持续的成功，要确保开发团队做到如下几点：</p><p></p><p>充分认识到部署不含编程错误的代码的重要性，并相信静态代码分析可以帮助他们实现这一点；充分理解TypeScript的运行原理（参见<a href=\"https://www.typescriptlang.org/docs/handbook/intro.html\">TypeScript: Handbook</a>\"）定期修复警告和类型错误，起码要比引入它们的频率更高；保持这些措施，永不间断。</p><p>&nbsp;</p><p>如下几种策略可能会提供帮助：</p><p></p><p>奖励提高代码质量的代码贡献行为，从而激励开发人员。其中，有种方法是使用可插入持续集成流水线的工具来跟踪开发人员推送的每个变更的代码质量变化，例如可以使用SonarCloud和/或Codacy。让一名开发人员负责确保代码质量永不下降。让另一名开发人员负责定期更新依赖，从而能够让团队能够从它们的逻辑和安全修复中受益。</p><p>&nbsp;</p><p></p><h3>为何要把每个角色都交给一个专门的人？</h3><p></p><p></p><p>当某项职责没有人负责时，集体责任往往会被其他“优先事项”所取代（比如，本周多交付一个特性，但是代价是忽略一个警告）。</p><p>&nbsp;</p><p>定期轮换角色，确保每个人都能参与其中并保持积极性。</p><p>&nbsp;</p><p></p><h2>使用（恰当类型的）测试覆盖关键的业务逻辑</h2><p></p><p></p><p>现在，我们有了一支致力于保持代码库整洁的团队，我们相信用户很少会遇到编程错误。</p><p>&nbsp;</p><p>但是，业务逻辑中的错误该怎么办呢？</p><p>&nbsp;</p><p>例如，如果一个新添加的功能破坏了另一个功能该怎么办？如果开发人员从一开始就误解了该功能的预期行为，又该怎么办？如果这样的错误最终导致了严重的收入损失又该如何处理？</p><p>&nbsp;</p><p>与编程错误类似，业务逻辑问题可能会在生产环境由用户发现，但我们更希望尽早发现它们。因此，定期测试软件非常重要，这个过程可以使用自动化和/或手动测试。</p><p>&nbsp;</p><p>从业务角度看，测试有两个作用：</p><p></p><p>符合功能性需求：每个特性的实现都能满足开发时的需求。检测回归：在对代码进行任何修改后，所有现有的特性都能按照预期运行。</p><p>&nbsp;</p><p>确保功能性测试（也称为“验收测试”）涵盖大多数关键业务特性，单元测试或集成测试涵盖大多数关键技术组件。此外，确保持续集成在任何测试失败时都能向开发人员提供可执行的反馈。</p><p>&nbsp;</p><p>对于有些开发人员来说，将测试工作委托给其他人（如产品负责人或QA团队）是很有诱惑力的做法。在每个新特性完成后，进行一次这样的委托测试，以确保特性实现符合功能性需求，并进行协作迭代，这样做可能是合理的。</p><p>&nbsp;</p><p>但是，委托他人进行回归检测并不是一个好主意，原因包括：</p><p></p><p>它增加了合并代码和部署代码之间的延迟。它增加了发现回归问题和修正它们之间的延迟。随着功能性范围的不断扩大，检测回归所需的时间也会随之增长。如果负责这些测试的人没有将其自动化，他们最终可能会跳过越来越多的测试。因此，一段时间之后，出现回归测试未发现问题的风险就会越来越高。</p><p>&nbsp;</p><p>回归测试是一项痛苦且可能代价高昂的负担，尤其是需要不同角色（如产品负责人和开发人员）必须协作的情况下。从长远来看，回归测试自动化意味着可以节省大量的时间，而且开发人员具有编写自动化测试的技能，所以，开发人员首先要承担起检测回归的责任，而不必让其他角色参与进来，这符合他们的利益。</p><p>&nbsp;</p><p></p><h3>如果要涵盖的功能范围很大该怎么办？</h3><p></p><p></p><p>从最关键的业务特性开始。要找出这些特性，你可以问自己：“就收益和/或减少成本而言，在生产环境中可能发生的最糟糕的事情是什么？”</p><p>&nbsp;</p><p>例如，电子商务网站的回答可能是如下的特性：</p><p></p><p>“信用卡购物”特性每分钟可以带来大约1000美元的收入。如果销售人员必须要求首席技术官手动将产品添加到数据库中，则“将产品添加到目录中”特性每小时的成本约为500美元。如果客户支持团队需要手动处理订单，那么“打印条形码以退回订单”将使我们每天损失500美元。</p><p>&nbsp;</p><p>基于这些业务关键的用例，从它们开始编写端到端的自动化测试肯定就是非常有意义的。</p><p>&nbsp;</p><p></p><h3>何时运行测试？</h3><p></p><p></p><p>在每次代码更新或添加到代码库之时，在将其部署到生产环境之前。</p><p>&nbsp;</p><p>借助git&nbsp;hook，在每次提交时运行测试可能就足够了，因为它能可靠地运行，而且其持续时间不会导致开发人员编写更少的测试。</p><p>&nbsp;</p><p>不管是否使用git&nbsp;hook，都要确保每次推送可用于生产环境的代码时，测试能在某处运行（例如，最好是在持续集成环境中）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0e/0e1ff0d97da66877738c4cca3734cff9.png\" /></p><p></p><p>在持续集成环境中，每次提交都会运行代码检查和自动化测试。</p><p></p><h3>&nbsp;</h3><p></p><p></p><h3>我们应该编写什么样的测试？</h3><p></p><p></p><p>需要优化的变量包括：</p><p></p><p>测试所覆盖的功能性和技术性范围的大小。从测试中获得反馈的时间。修复失败测试所报告的问题所需的时间。因为误报而损失的时间（即由于随机原因导致失败的测试）。</p><p>&nbsp;</p><p>如果你的团队在编写自动化测试和/或可测试代码方面经验不足，那么可以从一些端到端测试开始。然后，逐步增加对范围更小的代码单元的测试。这样做可以激励开发人员编写易于测试的代码。例如，通过隔离责任、减少耦合和/或将业务逻辑写成纯函数。遵循依赖注入架构是实现这一目标的好方法。（参见<a href=\"https://alistair.cockburn.us/hexagonal-architecture/\">六边形架构</a>\"或<a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">简洁架构</a>\"）</p><p>&nbsp;</p><p></p><h3>我们是否应该Mock第三方API？</h3><p></p><p></p><p>自动化测试（如本文所述）的目的是探测团队的功能性范围内的回归，而不是第三方的功能。基于这一点，在测试中Mock第三方是合理的。</p><p>&nbsp;</p><p>也就是说：</p><p></p><p>Mock应始终与当前API的行为相匹配。这意味着开发人员需要持续关注API的变化，并相应的更新它们的Mock。当实际API的行为与预期不符时，你可能依然希望得到警告。</p><p>&nbsp;</p><p>探测自己的代码中的问题和第三方API中的问题并不遵循相同的生命周期：</p><p></p><p>每次代码进行变更时，所涉及的范围都应该进行测试。仅在第三方的代码发生变更的时候，才应该对其进行测试。（也就是说，每次提交代码变更都测试第三方依赖是没有什么意义的）。</p><p>&nbsp;</p><p>你需要持续监控第三方提供商是否能够正常运行并达到预期效果。但是，第三方错误不一定能够在发生之时就探测到，因此最好是定期监控，而不是在开发人员每次推送代码变更的时候进行监控。</p><p>&nbsp;</p><p>所以，需要搭建两个专门的流水线：</p><p></p><p>你自己的CI流水线会在你的代码发生变更的时候测试自己的范围。另外一个CI流水线定期检查第三方所涉及的范围是否按照预期运行。</p><p>&nbsp;</p><p>为了编写长期最有用、最健壮的测试，我建议遵循<a href=\"http://agileinaflash.blogspot.com/2009/02/first.html?m=1\">F.I.R.S.T.原则</a>\"。确保开发人员不会<a href=\"https://youtu.be/x8sKpJwq6lY?si=ajOH__APjAW0fq4T\">滥用mock</a>\"。</p><p>&nbsp;</p><p></p><h2>细致保护代码库中新的/现代化的部分</h2><p></p><p></p><p>假设你的代码库已经或者将要开发数年的时间，那么随着时间的推移，它很可能会在代码风格和质量方面失去内聚力。更糟糕的是，由于技术债务、缺乏测试或意外复杂性的积累，某些组成部分的维护可能会变得很复杂。</p><p>&nbsp;</p><p>在这种情况下，要像上文所建议的那样，在整个代码库中对代码实现一致的内聚预期可能会变得很复杂。不过，这也没有关系。</p><p>&nbsp;</p><p>你不希望看到的是期望值降低到一个最低的平均水准。这样的话，你可以把代码划分为不同的范围，并为每个范围设置不同的期望水平。</p><p>&nbsp;</p><p>例如，考虑一个即将为电子商务网站实现新特性的团队。他们希望这个新特性能够比代码库中的其他特性更健壮、更易于维护。为了实现这一点，他们在配置静态代码分析工具（如ESLint和TypeScript）时采用比代码库的其他部分更严格的规则，并针对专门为该特性而创建的目录使用覆盖的方式启用更多的规则。通过这种方式，团队可以提高新代码的质量，而不必急于对代码库中“遗留”的部分进行现代化处理。</p><p>&nbsp;</p><p><code lang=\"null\">\"rules\": {\n    \"prettier/prettier\": \"error\",\n    \"deprecation/deprecation\": \"warn\"\n  },\n  \"overrides\": [\n    {\n      // Tolerate warnings on non critical issues from legacy JavaScript files\n      \"files\": [\"*.js\"],\n      \"rules\": {\n        \"prefer-const\": \"warn\",\n        \"no-inner-declarations\": [\"warn\", \"functions\"],\n        \"@typescript-eslint/ban-ts-comment\": \"warn\",\n        \"@typescript-eslint/no-var-requires\": \"off\"\n      }\n    },\n    {\n      // Enforce stricter rules on domain / business logic\n      \"files\": [\"app/domain/**/*.js\", \"app/domain/**/*.ts\"],\n      \"extends\": [\"async\", \"async/node\", \"async/typescript\"],\n      \"rules\": {\n        \"prefer-const\": \"error\",\n        \"no-async-promise-executor\": \"error\",\n        \"no-await-in-loop\": \"error\",\n        \"no-promise-executor-return\": \"error\",\n        \"max-nested-callbacks\": \"error\",\n        \"no-return-await\": \"error\",\n        \"prefer-promise-reject-errors\": \"error\",\n        \"node/handle-callback-err\": \"error\",\n        \"node/no-callback-literal\": \"error\",\n        \"node/no-sync\": \"error\",\n        \"@typescript-eslint/await-thenable\": \"error\",\n        \"@typescript-eslint/no-floating-promises\": \"error\",\n        \"@typescript-eslint/no-misused-promises\": \"error\",\n        \"@typescript-eslint/promise-function-async\": \"error\"\n      }\n    }\n  ]</code></p><p></p><p>通过配置覆盖，我们可以为不同的部分设置不同的ESLint规则</p><p>&nbsp;</p><p>与之类似，如果要对整个代码库进行现代化改造，也要循序渐进。你可以创建一个具有更严格规则的专用目录，并逐渐将遗留代码迁移至该目录，同时修复代码的警告和类型错误。</p><p>&nbsp;</p><p></p><h3>从何处开始？</h3><p></p><p></p><p>有种方式是逐步将功能范围中陈旧的部分迁移到更好的设计中。例如，选择一个难以编写自动化测试的特性，并将它的实现迁移到六边形架构中，将业务/领域逻辑根据输入命令（即“API”）和副作用（即“SPI”）分离开来。通过编写自动化测试来指导迁移，并将新的实现放在具有更严格静态代码分析规则的专用目录中。</p><p>&nbsp;</p><p><code lang=\"null\">import { makeFeatures } = from './domain/features';\nimport { userCollection } from './infrastructure/mongodb/UserCollection';\nimport { ImageStorage } from './infrastructure/ImageStorage.js';\n\n\n\n\n/** @type {import('./domain/api/Features').Features} Features*/\nconst features = makeFeatures({\n  userRepository: userCollection,\n  imageRepository: new ImageStorage(),\n});\n\n\n\n\nroutes.post('/avatar', (request, response) =&gt; {\n  features\n    .setAvatar(request.session.userId, request.files[0])\n    .then(\n      () =&gt; response.json({ ok: true },\n      (error) =&gt; response.json({ ok: false })\n    );\n});</code></p><p>setAvatar特性经过了重新设计，由于采用了依赖反转，使其易于单独进行测试。下面是我们迁移另一项特性的过程，即<a href=\"https://github.com/openwhyd/openwhyd/pull/662\">播放列表删除</a>\"</p><p>&nbsp;</p><p>如果你决定遵循这一路径，如下是一些建议：</p><p></p><p>如果你的团队没有重新设计遗留特性的经验，那么就从简单的小特性开始。否则的话，请选择一个未来几周或几个月内要实现的特性最依赖的那个特性。在编码之前，明确范围、业务事件和路径。例如，与你想重新设计的领域（或限界上下文）所涉及的专家一起组织一次<a href=\"https://www.infoq.com/news/2016/06/event-storming-ddd/\">事件风暴</a>\"。可视化要迁移范围的当前架构，例如使用像<a href=\"https://www.npmjs.com/package/arkit\">ARKit</a>\"、<a href=\"https://www.npmjs.com/package/dependency-cruiser\">Dependency-Cruiser</a>\"或类似的依赖分析工具，并写明不想在目标架构中重复出现的问题，以免重蹈覆辙。如果有疑问的话，请使用软件设计工具（如时序图、状态机图、ADR）协作完成恰当的设计。</p><p>&nbsp;</p><p>在迁移完每个限界上下文之后，你将会得到一个代码库，在代码库中100%的代码都应按照更严格的规则进行检查。</p><p>&nbsp;</p><p></p><h2>每日部署，但同样的错误不要犯两次</h2><p></p><p></p><p>尽管使用了静态分析工具来检测缺陷，使用了自动化测试来探测回归，但用户还是会在生产环境中发现问题。这是无法避免的。但是，有一种方法可以降低出现此类问题的概率，并缩短团队修复问题的时间：</p><p></p><p>每日部署（前提是你确信失败的风险很低）。同样的错误不要犯两次。</p><p>&nbsp;</p><p></p><h3>为何要每日部署？</h3><p></p><p></p><p>简约版答案：因为<a href=\"https://dora.dev/\">DORA研究项目</a>\"发现，大多数执行团队每天都在进行部署，或者每天部署多次。</p><p>&nbsp;</p><p>详尽版答案：</p><p>因为这能够让开发人员更快地找到在生产环境中出现新缺陷的根本原因。也就是说，部署越频繁，最新部署和上次部署之间的提交次数就越少。基于相同的原因，如果最新版本不能按照预期运行，回滚到上一个版本的成本会更低（就回滚代码提交的次数而言）。因为这能鼓励团队将工作分成更小、更安全的增量。DORA认为，这也是表现最好的团队所遵循的做法。</p><p>&nbsp;</p><p></p><h3>如何确保相同的错误不犯两次？</h3><p></p><p></p><p>在生产环境中出现意料之外的行为是可以的。在有些情况下，这甚至是一件好事。</p><p>&nbsp;</p><p>当意料之外的行为给企业和/或开发团队带来巨大损失时（例如，网站中断，导致几个小时无法使用），开发人员应该采取措施防止类似的事件再次发生。</p><p>&nbsp;</p><p></p><h3>如何探测生产环境中的问题？</h3><p></p><p></p><p>有多种方式可以探测生产环境中的问题：</p><p></p><p>理想情况：开发人员发现问题并立即修复。常规情况：员工发现问题并向开发团队报告。更糟糕的情况：用户向开发团队报告问题。最糟糕的情况：用户发现了问题，但并没有报告。</p><p>&nbsp;</p><p>无论是哪种情况，开发人员都需要以下信息：问题是什么、问题的具体表现（如错误信息）、如何重现问题（如环境+过程），以及用户的初衷和期望是什么。</p><p>&nbsp;</p><p>但是，如何在最糟糕的情况下获得这些数据呢？这就是错误监控工具（如<a href=\"https://sentry.io/\">Sentry</a>\"）的用武之地了。通过将它们注入到生产环境中运行的产品中，它们就能像探针一样检测运行时错误，并将它们汇总到已知错误的列表中，直到每个错误都被开发人员修复为止。此外，它们还会获取有关错误上下文的数据（如用户代理、所使用软件的版本、操作系统、确切的时间戳等），以帮助开发人员重现错误。</p><p>&nbsp;</p><p>但令人遗憾的是，与静态代码分析器类似，这些工具并不能解决问题。因此，与警告和类型错误一样，要确保尽快处理每个错误。团队让错误累积得越多，使用这些工具的动力和效率就会越低。</p><p>&nbsp;</p><p>此外，在使用这类监控工具时，请确保个人和/或机密数据不会从系统中泄露出去。</p><p>&nbsp;</p><p>从战术上讲，有许多方法可供选择。你可以让一名开发人员负责修复生产环境的错误，并将其作为最优先的事项。这个角色可以定期轮换（比如每天），这样可以激励每个人都编写更健壮的代码。或者，也可以在每天的会议上将新错误单独分派给志愿开发人员。</p><p>&nbsp;</p><p></p><h3>如何降低复发风险？</h3><p></p><p></p><p>不必慌张！当生产环境中发生事故时，都要遵守如下程序：</p><p></p><p>保留事故发生前、发生时和发生后的痕迹，以帮助你进行事后分析（注意：在事故发生前做好充分的监控和日志收集工作）。在内部和外部就事故进行沟通。稳定生产环境，例如，回滚到之前能正常运行的版本。编写并部署修正版本，以修复问题。查找并解决导致问题的根本原因，并采取预防措施。</p><p>&nbsp;</p><p>避免重蹈覆辙的关键在于上述程序的最后一步。</p><p>&nbsp;</p><p>这也是经常被忽视的过程。大多数情况下，是因为没人觉得自己有责任这样做。很多时候，是因为产品负责人（或产品团队）向开发人员施压，要求他们优先完成开发计划中的特性，而不是保护现有代码和/或调整开发流程。有时，开发人员自己也会决定开发更多的特性，而不是避免再次犯错。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/933b41381c3a0416d65a4b5bcc8d4eab.jpeg\" /></p><p></p><p>调查事故根本原因时的注意事项</p><p>&nbsp;</p><p></p><h3>如何查找事故的根本原因？</h3><p></p><p></p><p>在这个方面，“5个为什么（5 WHY）”技巧是很有用的。例如：</p><p></p><p>生产系统为什么会崩溃？——因为一个未登录的用户访问了页面B。用户为什么能够访问页面B？——因为主页上有一个链接。用户在访问页面B的时候为什么没有看到登录页面？——因为在页面渲染时，后端还不知道登录状态。为什么页面渲染时还不知道登录状态？——因为我们的会话管理后台很慢，等待这个状态会大大降低我们的网络性能指标。为什么会话管理后端很慢？——因为它使用的是未经优化的遗留数据库。</p><p>&nbsp;</p><p>在本例中，根本原因是整个网站都依赖于遗留的会话管理后端，这使得导航难以预测，有时还会导致生产环境崩溃。因此，除非团队修复传统的会话管理后端，否则类似的崩溃很可能很快就会在生产环境中再次发生。团队现在应该修复遗留的会话管理后端吗？也许不用。但是他们应该努力制定一个能够实现该目标的补救计划。</p><p>&nbsp;</p><p></p><h3>在实践中，如何实现低故障率的日常部署呢？</h3><p></p><p></p><p>让一位开发人员负责确保尽快发现生产中的意外行为（如运行时错误、缺陷、事故……），尽快修复，并采取措施防止今后再次发生各类问题。</p><p>&nbsp;</p><p>通过这种方式，开发人员能够感受到有能力在良好的条件下开展工作。例如，在生产过程中设置恰当的监控和日志，确保撰写有用的事后报告，并采取预防措施。</p><p>&nbsp;</p><p>当信心达到良好水平时，逐步增加部署频率。</p><p>&nbsp;</p><p></p><h2>以正确的激励机制调整产品开发团队</h2><p></p><p></p><p>此时，开发人员就具备了编写高质量软件，并尽快发现缺陷的能力。这些缺陷最好是在设计或实现时发现，而不是在生产环境中。他们能够快速发现并修正生产环境的错误，不会重复犯同样的错误。他们对自己的代码和开发流程充满信心，因此每天都能在生产中实现改善。而且，他们在对软件功能化范围进行预期改善的同时，也会逐步改善代码库中最古老部分的设计和质量，使其保持健康、稳健并易于长期维护。</p><p>&nbsp;</p><p>但是，令人遗憾的是，这种平衡很快就可能被瓦解。举例来说：</p><p></p><p>如果开发人员失去了长期保持高设计标准和/或代码质量的动力。如果部分开发人员不遵循团队的质量准则，造成系统性返工、挫折和延误。如果开发人员误解了功能性需求，而急于修复无法达到预期效果的特性，从而牺牲了长期的技术责任。如果有人（如经理、产品负责人或其他人）向开发人员施压，要求他们每周发布更多的特性，或在紧急的期限内完成任务。如果激励和/或奖励开发人员的绩效指标与其代码库的长期质量和健壮性不一致。例如，根据每周交付的特性数量确定晋升奖金。</p><p>&nbsp;</p><p>防止或解决这类情况可能会非常困难，因为这需要良好的领导力和/或软技能。</p><p>&nbsp;</p><p>一个常见的错误是培养某种思维定式，即开发人员应该主要致力于实现优先的、计划好的和设计好的特性。</p><p>&nbsp;</p><p>这样做是有问题的，因为：</p><p></p><p>它要求开发人员处于这样一种状态，即对软件做的每一项变更都要有精确和明确的规范。这可能会导致开发人员无法与负责制定这些规范的人员进行健康的双向合作。对于那些喜欢整天独自工作的开发人员来说更是如此。它让开发人员处于这样一种境地，即难以衡量那些与功能性路线图没有直接贡献的开发活动，如更新依赖、提高代码质量、培训更好的设计和编码技术。这很容易让人倾向于根据指标（如用户故事的开发速度）来跟踪开发人员的绩效（或“生产力”），而忽略了对可持续开发实践的投资，即代码质量、阻碍回归、错误管理等。</p><p>&nbsp;</p><p>下面是一些关于如何避免上述陷阱的建议：</p><p></p><p>在详细阐述业务问题的解决方案时，至少让一名开发人员参与设计过程。这将提高开发人员的责任心，使他们能够为一个充分理解的问题实现一个好的解决方案。有时，由于开发人员了解当前的建模和实现方式，他们会提出替代解决方案，从而在满足需求的同时节省大量的开发时间。确保产品和技术代表能够公开、友好地协商功能性和技术性项目的优先级和规划。例如，如果开发人员需要重新设计代码库的某个部分，那么他们就应该说服其他人相信这一点的重要性，解释这将为下一个特性的开发带来哪些具体的改善，以及延迟该项目的风险和成本是什么。同样的建议也适用于产品经理如何对即将开发的功能改善进行优先排序和规划：通过解释来说服开发团队并让他们参与进来。这样做可以增强参与设计和实现特性的所有员工的信任、协作和参与度。在管理方面，确保开发人员不会得到这样的激励，即“每周尽可能多地发布特性”。找到使每个开发人员的职业目标与团队的短期和长期期望相匹配的发展轨道。这样做的目的是防止出现开发人员理直气壮地只从事短期改善相关工作的情况。最后，确保为开发人员提供资源和指导，以不断提高他们的软硬技能。为他们提供培训和/或指导资源。鼓励他们通过结对和/或群体编程的方式共同完成任务。鼓励他们与其他/非开发人员角色进行良好的协作，包括领域专家、产品负责人、产品设计师、客户支持团队、终端用户等。</p><p>&nbsp;</p><p></p><h2>结论</h2><p></p><p></p><p>JavaScript语言及其不断变化的软件包和实践组成的生态系统会使代码库迅速变得难以维护。正如我们在本文所讨论的那样，无需从头重写所有的内容，也无需暂停新特性的开发，就可以避免由此造成的开发速度和/或代码质量的下降。</p><p>&nbsp;</p><p>关于如何在TypeScript和JavaScript项目中应用这些推荐做法的更多实用建议，我建议你参考<a href=\"https://github.com/goldbergyoni/nodebestpractices\">Yoni Goldberg的最佳实践列表</a>\"。它们是为Node.js（后端）项目编写的，但其中很多也适用于前端代码库。</p><p>&nbsp;</p><p></p><h5>参考链接：</h5><p></p><p><a href=\"https://www.infoq.com/articles/javascript-typescript-quality-velocity/\">https://www.infoq.com/articles/javascript-typescript-quality-velocity/</a>\"</p><p><a href=\"https://github.com/goldbergyoni/nodebestpractices\">https://github.com/goldbergyoni/nodebestpractices</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/6ff79700fb3bfa972c1beebf3\">TypeScript 与 JavaScript：你应该知道的区别</a>\"</p><p><a href=\"https://www.infoq.cn/article/dDXbcLHT7teNYSPL3sm7\">“TypeScript 不值得！”前端框架 Svelte 作者宣布重构代码，反向迁移到 JavaScript 引争议</a>\"</p><p><a href=\"https://xie.infoq.cn/article/b7f556a866805cf5c71be7af8\">Typescript- 类型检测和变量的定义</a>\"</p><p><a href=\"https://www.infoq.cn/article/ds994KySqo868U3e8s4N\">我踩过了 TypeScript 的坑，只想告诉你快来</a>\"</p>",
    "publish_time": "2023-12-20 14:27:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "奥特曼被套上“紧箍咒”？OpenAI发布27页安全指南，董事会有权阻止新AI模型发布",
    "url": "https://www.infoq.cn/article/t3Lh1FoXJXq7rci3ssRD",
    "summary": "<p>生成式人工智能的潜在危险引起了公众、政治家和人工智能研究人员的关注。随着各国政府希望压制该技术，OpenAI扩大了其内部安全流程，以应对有害人工智能 (AI) 的威胁。</p><p>&nbsp;</p><p>近日，OpenAI公司CEO Sam Altman现身美国佐治亚州亚特兰大召开的全球希望论坛。全球40个国家的5200多名代表参会，旨在重新构想全球经济体系，让企业的自由效益与机遇惠及所有人。</p><p></p><h2>OpenAI董事会有权阻止“有害的”新AI模型发布</h2><p></p><p>&nbsp;</p><p>OpenAI公司已经制定计划，遏制当前及未来正在开发的强大AI技术可能引发的一切最坏情况。</p><p>作为席卷全球的聊天机器人ChatGPT的缔造者，该公司本周公布了一份长达27页的“<a href=\"https://cdn.openai.com/openai-preparedness-framework-beta.pdf\">准确框架</a>\"”文件，概述了OpenAI如何跟踪、评估及防范由前沿AI模型所引发的“灾难性风险”。</p><p>&nbsp;</p><p>具体风险范围从AI模型被用于实施大规模网络安全破坏，到协助制造生物、化学或核武器等等。</p><p>作为这份准备框架中制衡章节的一部分，OpenAI表示该公司领导层将对是否发布新AI模型拥有决策权，但最终决定权将始终归董事会所有，即保有对OpenAI高管团队结论的“否决权”。</p><p>&nbsp;</p><p>而且即使未遭公司董事会否决，具有潜在风险的AI模型在实际部署之前，也需要预告通过一系列安全检查。</p><p>&nbsp;</p><p>将有一支专门的“准备”团队领导这项多管齐下的管控工作，负责监控并缓解OpenAI先进AI模型引发的潜在风险。</p><p>&nbsp;</p><p>OpenAI 于 2023 年 12 月 18 日更新了有关准备团队的页面。此次更新的主要目的似乎是为识别、分析和决定如何处理他们正在开发的模型固有的“灾难性”风险提供一条清晰的路径。正如他们所定义的：</p><p>&nbsp;</p><p></p><blockquote>我们所说的灾难性风险是指任何可能导致数千亿美元经济损失或导致许多人严重伤害或死亡的风险——这包括但不限于生存风险。</blockquote><p></p><p>&nbsp;</p><p>除了调查正在开发的 AI 模型的准备团队之外，安全系统团队还调查当前模型的风险，“安全系统”团队调查以下风险：通用人工智能等超级智能模型有望在未来投入实际应用。他们宣布将成立一个名为“Superalignment”的团队，这三个团队将确保 OpenAI 的安全。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/18/18ecea89d96a52f68c6dd45d7ffdb882.png\" /></p><p></p><p>目前正在休假的麻省理工学院教授Aleksander Madry，将出面领导这家初创公司的准备团队。他将监督一组研究人员，负责评估并密切监控潜在风险，并将这些具体风险整理成记分卡形式。按照具体影响程度，这些记分卡将把特定风险划分为“低”、“中”、“高”以及“严重”等类别。如果正在开发的AI风险超过“高”，将停止开发，如果超过“高”，将停止开发。超过Medium，可能会停止发布。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/5d/5dc8bda60206843e77c86b7e5b74196d.png\" /></p><p></p><p>&nbsp;准备框架指出，“只有在实施缓解措施之后，风险等级为「中」及以下的模型才能实际部署”，而且只有“实施缓解措施之后，风险等级为「高」及以下的模型才能进一步开发。”</p><p>&nbsp;</p><p>此外，OpenAI还宣布成立一个部门——安全咨询小组，负责监督安全决策的技术工作和运营架构。</p><p>&nbsp;</p><p>安全咨询小组位于 OpenAI 的技术开发之上，并定期生成有关 AI 模型的报告。此外，该报告还提交给管理层和董事会。管理层可以根据安全咨询小组的报告决定是否发布人工智能模型，但董事会可以否决管理层的决定。换句话说，即使管理层无视安全咨询小组的报告并决定发布本身存在高风险的人工智能模型，董事会也可以使用同一份报告推翻该决定。</p><p>&nbsp;</p><p>OpenAI公司表示，目前此份文件尚处于“beta”测试阶段，预计将根据反馈保持定期更新。</p><p>&nbsp;</p><p>该框架让人们再次关注到这家强大AI初创公司那不同寻常的治理结构。继上个月OpenAI“逼宫”事件爆发后，该公司董事会实施了一波彻底改革，甚至将创始人兼CEO Sam Altman赶下了台。但凭借在公司内的民意基础和外部投资方的高度认可，Altman短短五天之内即闪电回归。</p><p>&nbsp;</p><p>这场备受关注的权力争夺大戏在当时引发了新的问题：Altman对于他参与创立的企业该保有怎样的权力，董事会又该如何对Altman及其高管团队加以限制。</p><p></p><h2>人们对AI安全的担忧从未停止</h2><p></p><p>&nbsp;</p><p>值得注意的是，自CEO回归后，那些反对他的成员们被排除在董事会之外。“如果安全咨询小组提出建议，并且首席执行官同意建议，那么董事会真的可以阻止他吗？”&nbsp;这个问题的答案我们不得而知。除了承诺 OpenAI 将接受独立第三方审计之外，没有太多提及透明度。外界也对安全咨询小组是否真的存在表示怀疑。</p><p>&nbsp;</p><p>OpenAI公司强调，目前的董事会仍在“初始阶段”且尚未最终完成组建。三名成员均为高净值白人，负责确保OpenAI的前沿技术向着造福全人类的方向砥砺前行。</p><p>&nbsp;</p><p>临时董事会成员缺乏多样性的问题正遭受广泛批评。部分反对者还担心，单靠公司的自我监管还远远不够，立法机构应当采取更多措施以确保AI工具的安全开发和部署。</p><p>&nbsp;</p><p>以OpenAI公布这份最新主动安全框架为时间节点，过去一年来整个科技行业乃至其他领域一直在激烈争论AI技术引发灾难的可能性。</p><p>&nbsp;</p><p>今年早些时候，数百名顶尖AI科学家和研究人员（包括OpenAI的Altman以及Google DeepMind&nbsp;CEO Demis Hassabis）共同签署一封简短的公开信，呼吁将减轻“AI带来的灭绝性风险”视为全球优先事项，其优先级应等同于“大范围流行病及核战争”等顶级风险。</p><p>&nbsp;</p><p>这份声明很快引起了公众的广泛警惕。但后来也有行业观察人士认为，这其实是转移视线的烟幕弹，目的是将人们对于AI工具当前危害的关注引导到虚无飘渺的遥远末世场景身上。</p><p>&nbsp;</p><p>但无论如何，此次OpenAI内部爆发的“斗争”还是引发了人们对于超强人工智能的担忧。《时代》周刊将Altman评为世界上最有影响力的人物之一，因为他在推进人工智能系统方面所做的工作，同时警告我们，人工智能可能会消灭所有人类文明。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://edition.cnn.com/2023/12/19/tech/openai-board-safety-catastrophic-risks/index.html\">https://edition.cnn.com/2023/12/19/tech/openai-board-safety-catastrophic-risks/index.html</a>\"</p><p><a href=\"https://gigazine.net/gsc_news/en/20231219-openai-safety-advisory-group/\">https://gigazine.net/gsc_news/en/20231219-openai-safety-advisory-group/</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-12-20 14:49:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "800 家生态伙伴、CentOS 替换首选，龙蜥如何仅用三年引领开源操作系统创新",
    "url": "https://www.infoq.cn/article/iG6yDDhWZy7eIfdc9BL0",
    "summary": "<p>在云智融合的巨浪之下，中国操作系统产业正蓄势待发，迎来了黄金时代。</p><p></p><p>作为计算机系统的灵魂和产业生态的核心，操作系统的演进一直是推动产业变革和时代创新的关键引擎。随着云计算的蓬勃发展，服务器操作系统不仅需要释放硬件能力、支撑软件和业务生态，还需适应生成式 AI 技术崛起，为开发者和用户充分利用云端 AI 提供支持。然而，生成式 AI 对算力资源的庞大需求给服务器操作系统带来前所未有的挑战。操作系统须灵活适应通用计算与异构计算共存，同时在超大规模计算下确保极高的安全性和稳定性。</p><p></p><p>在云智融合大发展的关键期，我国龙头企业和开源社区积极参与，共同致力于推动中国开源操作系统生态的繁荣发展，而龙蜥社区作为开源社区的重要力量，正与国内龙头企业紧密合作，致力于构建蓬勃的中国开源操作系统生态。</p><p></p><p>龙蜥社区成立于 2020 年，恰逢其时地在 CentOS 停服的过程中发挥了关键的作用。通过包括阿里云在内的 24 家理事单位的引领和推动，龙蜥社区仅用短短几年就已完成社区的建设和布局，逐步解决了操作系统开源供应链的安全和可控问题，并在开放、中立的社区治理体系下形成了多厂商合作的成功典范。如今的龙蜥社区已然成为我国最多样化贡献机构的社区之一，为中国操作系统走向更高质量的发展奠定了坚实基础。</p><p></p><p>近日，首届龙蜥操作系统大会在北京盛大召开。本次大会以 “云智融合·共筑未来” 为主题，共同探讨产业趋势、生态合作、人才发展、技术创新等重要议题。中国工程院院士、原副院长陈左宁，中国科学院院士、CCF 理事长、北京大学教授梅宏，中国工程院院士、浙江大学信息学部主任陈纯，中国工程院院士王恩东等四位知名院士参与其中，分享了他们的见解和思考。此外，大会还邀请了多位来全球的技术专家和行业大咖，共同探讨开源操作系统领域的未来发展。</p><p></p><p>中央网信办信息化发展局负责同志致辞中，对龙蜥取得的成绩表示祝贺，希望龙蜥社区持续发挥开源集众智、采众长、聚众力的特点，汇聚上下游生态伙伴力量，与芯片企业、整机企业、头部应用形成发展合力，共同培育壮大国内操作系统生态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7ebc78106cfd174ab0881e44c987810.png\" /></p><p>（图 / 开放原子开源基金会理事长孙文龙）</p><p></p><p>此次大会不止是龙蜥的里程碑，更是操作系统发展史上新征程的起点。</p><p></p><h3>坚定可持续发展路线，开源之上长存开放之心</h3><p></p><p></p><p>在众多的开源操作系统社区中，龙蜥社区并非起步最早的，但仅用了三年左右的时间，却已构筑起拥有 24 家理事单位、800 家生态伙伴、1.5 万贡献者的庞大、健康的社区生态，成为开源操作系统社区的领航者。截至目前，龙蜥操作系统免费发行版的下载次数已达 250 万而安装数量超 600 万。不仅如此，根据中国信通院近期面向用户群体的调研显示，龙蜥操作系统 Anolis OS 位列用户意愿迁移系统的首位，超过一半的受访企业计划在五年时间内将 CentOS 替换为龙蜥系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/db2a405f2a025f517d1f642ba2945e01.png\" /></p><p>（图 / 中国信通院 - 用户意愿迁移服务器操作系统）</p><p></p><p>龙蜥社区这样的成就与龙蜥始终坚持可持续发展路线，协同产业上下游实现生态共赢是分不开的。</p><p></p><p>作为 Linux 开源操作系统根社区，龙蜥社区发布了免费发行版 Anolis OS，同时已有 12 家商业公司，基于龙蜥社区构建自己的商业版本，为客户提供企业级服务。通过这样的模式，社区可以帮助所有企业解决操作系统研发共性问题，同时也为所有商业版发行版提供足够商业空间，共同促进社区的发展。</p><p>在全球，不管是开源项目还是开源社区数量都非常多，因而想要在开源之上，创造能够走向更多企业的治理框架，摆脱一个项目对单一公司的依赖，就需要在开源之上，拥有开放之心。龙蜥社区在这方面就为其他开</p><p>源操作系统社区做出了表率。</p><p></p><p>龙蜥社区的开放包容体现在三个维度：</p><p></p><p>第一，社区治理和决策体系开放，龙蜥社区 24 家理事单位，采用一人一票制度，共同决策社区的发展方向。</p><p>第二，持续开放基础设施，包括技术、接口、能力等方面的全面开放，让用户免费部署私有云环境。</p><p>第三，重视开发者生态，认真对待每位开发者的问题和挑战，通过主动激励解决不同开发者遇到的问题，做到足够开放。</p><p></p><p>正是基于这三点，龙蜥社区快速地发展起来，并在百花齐放的局面中脱颖而出。会上，中国科学院院士、CCF 理事长、北京大学教授梅宏指出，龙蜥社区采用的开放中立的治理理念，让社区由单引擎变成多企业治理，原来主要是阿里云，现在有阿里云、浪潮信息、中兴通讯、统信软件等 24 家理事单位形成了共同治理的格局，是生态发展很好的雏形。而这种治理模式能够更好地激励所有参与者，发挥积极性、主观能动性，为龙蜥社区做出更大的贡献。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0c/0cbea6d9b4afb9da7223aba5faa4563d.png\" /></p><p>（图 / 中国科学院院士、计算机软件专家 梅宏）</p><p></p><h3>“1+3”能力模型，云智融合浪潮下技术领先性的关键</h3><p></p><p></p><p>除了开放的心态，技术能力对于操作系统的长期发展也至关重要。面对操作系统发行版包含成千上万的软件包，如何有序地解决研发和自主可控性问题，阿里云基础软件部副总裁、龙蜥社区理事长马涛，重磅首发下一代操作系统“1+3”能力模型，遵照 1 个“分层分类”科学理论的去中心化协同演进的技术路线，以“用好开源、做深开源、自主创新”为核心出发点，长期投入研发，将打造“供应链安全”、“开源标准”和“云原生 +AI”三位一体的下一代操作系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bfeb7dbaf5547058f795b08129fe182d.png\" /></p><p>（图 / 阿里云基础软件部副总裁、龙蜥社区理事长 马涛）</p><p></p><p>用好开源核心在于通过对供应链安全的掌控，实现对所有开源软件的掌控。做好开源则通过社区理事单位和合作伙伴的共同努力，引领关键标准和规范的制定。</p><p></p><p>近年来，开源已成为国产操作系统可持续发展的新思路，中国对于开源的认知，正在从使用开源、参与开源，走向贡献开源。开放原子开源基金会理事长孙文龙表示， 自 2020 年以来，龙蜥社区集聚产业力量，为开源操作系统发展，全球开源操作系统生态的繁荣作出了突出贡献，也欣喜地见证了龙蜥社区发展壮大的历程。</p><p></p><p>此外，自主创新也在云 +AI 场景下保持技术领先性的关键。据马涛介绍，龙蜥主要从三个角度解决问题：面向 AI、融合 AI 和依赖 AI。</p><p></p><p>首先，面向 AI，除了优化操作系统基础能力外，龙蜥重新设计关键组件，以实现大模型和核心算子更高效、稳定、方便的运行。同时，着重提升数据安全和可信等技术能力，为大模型和数据提供更安全的防护。</p><p>其次，融合 AI。除了基本的 CPU、GPU 异构算力支持外，操作系统需要更合理地分配和调度云计算的各种资源，确保大模型在最适合的硬件体系架构上运行。龙蜥追求在云智融合的场景下，为算力为大模型提供最佳运行场所。</p><p></p><p>最后，依赖 AI。操作系统不仅要服务于 AI，更要充分利用 AI 作为可靠的工具。通过 AI 协助操作系统的研发、测试和应用，解决核心问题。同时，通过智能优化和 Copilot 机制，为所有用户提供更便捷的使用体验。</p><p></p><p>依托面向 AI、融合 AI、依赖 AI 三个关键战略的推动，龙蜥操作系统实现了紧跟时代脉搏的高速发展。然而，陈纯院士认为，AI 时代操作系统之战，制胜的关键不仅是充分融合“云 +AI”的技术能力，更是下一代操作系统的生态构建。</p><p></p><h3>未来新里程：打造中国操作系统的黄金时代</h3><p></p><p></p><p>三年里，龙蜥社区的开源生态队伍同样也在不断壮大，持续推动了软、硬件及应用生态的繁荣发展。</p><p>龙蜥社区成立初期由阿里云牵头，将 Alibaba Cloud Linux 在操作系统上 积累的技术和经验大量投入龙蜥社区，使龙蜥社区在 2021 年初发布了社区的第一代操作系统 Anolis OS。阿里巴巴集团副总裁蒋江伟表示，在龙蜥发展过程中，阿里云致力于构建开放中立的社区治理体系，促进社区技术的成果转化成为规模化的应用，并携手上下游合作伙伴共同规划社区路线。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dd/dd0a1289d8921c4c8e93fb8c61019980.png\" /></p><p>（图 / 阿里巴巴集团副总裁、阿里云基础设施事业部总经理蒋江伟）</p><p></p><p>此后，越来越多的企业在与社区的深入合作中，发挥各自的行业优势，为社区开源作出了重要贡献。在首届龙蜥操作系统大会上，浪潮信息、中兴通讯、Intel 三家理事单位凭借对社区开源生态建设的卓越贡献，升级为龙蜥社区“新晋副理事长单位”，浪潮信息副总裁 Donny Zhang、中兴通讯系统技术总工徐立锋 、Intel 技术总监杨继国上台接受授牌，共同开启“同心共行·共建龙蜥”的新里程道路。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/31e7655ee22d6a816349428d6c1a8888.png\" /></p><p>（图 / 龙蜥社区新晋副理事长单位授牌仪式）</p><p></p><p>这一新里程的开启，标志着龙蜥社区将有着更加清晰的方向和更强劲的发展动力。</p><p></p><p>在过往的发展中，龙蜥社区一直秉持着集各家所长的原则。Intel 技术总监杨继国表示，龙蜥社区非常好的特点是它是非常多元化的社区，生态伙伴来自各个不同领域，不同的行业，不同的使用场景，能够把各个行业的趋势和方向代入到社区。</p><p></p><p>阿里云通过在本次大会发布 “Alibaba Cloud Linux &nbsp;伙伴招募计划”，把来自基础软件、云市场、计算巢等团队的技术积累和推广资源，与龙蜥社区开放强大的生态力量和技术协同相结合，号召服务商伙伴们进行共建；浪潮信息在硬件开放计算和技术创新方面与社区深度合作，为上下游硬件认证、软件认证、生态认证做出贡献；中兴通讯致力于开源文化，通过开源贡献、生态建设、社区运营等方面与社区合作；Intel 在技术创新、AI 计算和生态发展方面与社区共同努力，推动社区在创新、AI 领域的发展；统信软件通过技术委员会副主席的角色，积极参与社区技术投入，联合社区首创分层分类理论，并通过该理论在操作系统领域取得实践成果。在生态建设方面，统信软件致力于整合生态认证体系，避免重复工作，为操作系统领域提供更好的人才储备......</p><p></p><p>以上，所展现出来的仅仅是为了满足用户需求，寻找共同的发展方向，龙蜥社区与理事单位联合尝试的冰山一角，通过联合实验室、技术联盟、众测和共创计划以及人才计划等尝试，龙蜥社区正不断推动社区和操作系统的进步和发展。</p><p></p><p>谈到龙蜥社区未来的规划，马涛指出，云 +AI 场景，尤其是智算场景，对操作系统的发展提出了迫切需求。随着大模型的快速发展，对软硬件的需求日益增长。在这一背景下，操作系统需要重新思考如何更好地支持 AI 业务的发展。</p><p></p><p>龙蜥社区正在着手解决这一问题，首先，操作系统要面向 AI，发挥其在软硬件配合上的关键作用。其次，操作系统作为硬件和软件之间的桥梁，需要优化资源分配，提高模型在适配硬件平台上的效率。最后，AI 也能反过来助力操作系统的智能运用和调优，实现双向支持，定义出适应云 +AI 时代的新一代操作系统。</p><p>正如梅宏院士所言，当今时代正迈向数字社会和数字文明的开端。未来将迎来一个崭新的时代。在这个宏伟的背景下，通过全球开源社区的模式能够为中国在人类文明发展史上留下重要的足迹，共同迈向数字文明的时代。</p><p></p><p>预测未来最好的方式是创造未来。在这个竞争激烈的时代，龙蜥社区有望充分发挥国内庞大的生态和多样 GPU 场景的优势，共同打造中国操作系统的黄金时代，为中国在数字化时代的崛起贡献力量，创造更加璀璨的未来。</p>",
    "publish_time": "2023-12-20 15:02:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023年十大数字化政策盘点：激活万亿数据，加速提升千行百业数字化服务",
    "url": "https://www.infoq.cn/article/hhvpVOXJQJqzYzrlwzq8",
    "summary": "<p>2023 年，标志着数字中国建设迈向关键节点。随着《数字中国建设整体布局规划》的发布，未来数字中国的建设目标和关键工作任务得以明确。</p><p></p><p>在这一指导下，一系列政策相继问世，覆盖了能源数字化和智能化转型、加快推动智慧民航建设、银行办理资本项目业务数字化服务水平提升等领域，这些政策将深刻地影响着社会经济和大众的日常生活。</p><p></p><p>在通盘政策中，尤其引人注目的是与“数据”相关的政策。作为新型生产要素，数据对传统生产方式的改革产生深远影响。《企业数据资源相关会计处理暂行规定》、《关于促进数据安全产业发展的指导意见》以及国家数据局的正式揭牌等政策和措施的逐步推进，标志着数据价值将更快速地被释放。面向各行各业，数据资产价值将变得可量化、可评估、可交互，成为推动企业数字化转型的重要推动力。</p><p></p><p>通过了解这些政策和最新动态，我们不仅能更好地适应数字时代的剧变，找到自己在其中的定位，还能积极参与，共同助力数字中国的建设。</p><p></p><p>InfoQ 数字化经纬特地选取了 2023 年的十大数字化政策，邀请大家一同回顾。</p><p></p><h3>一、数字中国的顶层设计</h3><p></p><p></p><p>2023 年 2 月，中共中央、国务院印发了《数字中国建设整体布局规划》（以下简称《规划》），并发出通知，要求各地区各部门结合实际认真贯彻落实。</p><p></p><p>《规划》指出，建设数字中国是数字时代推进中国式现代化的重要引擎，是构筑国家竞争新优势的有力支撑。加快数字中国建设，对全面建设社会主义现代化国家、全面推进中华民族伟大复兴具有重要意义和深远影响。</p><p></p><p>作为影响中国未来发展的重磅文件，《规划》明确了两个重要时间节点：</p><p></p><p>到 2025 年，基本形成横向打通、纵向贯通、协调有力的一体化推进格局，数字中国建设取得重要进展；到 2035 年，数字化发展水平进入世界前列，数字中国建设取得重大成就。</p><p></p><p>《规划》明确，数字中国建设按照“2522”的整体框架进行布局，即夯实数字基础设施和数据资源体系“两大基础”，推进数字技术与经济、政治、文化、社会、生态文明建设“五位一体”深度融合，强化数字技术创新体系和数字安全屏障“两大能力”，优化数字化发展国内国际“两个环境”。</p><p></p><p>延展阅读：《<a href=\"https://www.infoq.cn/article/LBC3Rujd6xFoYfS2duGf?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">数字中国顶层设计来了，一图读懂“2522”框架｜政策解读</a>\"》</p><p></p><h3>二、数据资产入表正式落地</h3><p></p><p></p><p>2023 年 8 月，财政部发布了《企业数据资源相关会计处理暂行规定》（下称《暂行规定》），该规定自 2024 年 1 月 1 日起施行。《暂行规定》包括以下四部分内容：</p><p></p><p>一是适用范围。明确《暂行规定》适用于符合企业会计准则规定、可确认为相关资产的数据资源，以及不满足资产确认条件而未予确认的数据资源的相关会计处理。后续随着未来数据资源相关理论和实务的发展，可及时跟进调整。</p><p></p><p>二是数据资源会计处理适用的准则。按照会计上的经济利益实现方式，根据企业使用、对外提供服务、日常持有以备出售等不同业务模式，明确相关会计处理适用的具体准则，同时，对实务反映的一些重点问题，结合数据资源业务等实际情况予以细化。</p><p></p><p>三是列示和披露要求。要求企业应当根据重要性原则并结合实际情况增设报表子项目，通过表格方式细化披露，并规定企业可根据实际情况自愿披露数据资源（含未作为无形资产或存货确认的数据资源）的应用场景或业务模式、原始数据类型来源、加工维护和安全保护情况、涉及的重大交易事项、相关权利失效和受限等相关信息，引导企业主动加强数据资源相关信息披露。</p><p></p><p>四是附则。《暂行规定》将自 2024 年 1 月 1 日起施行，企业应当采用未来适用法应用本规定。</p><p></p><p>延展阅读：《<a href=\"https://www.infoq.cn/article/4rsaCarUujfmVyxW8wWl?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">时间敲定！企业数据将作为资产被纳入会计报表</a>\"》</p><p></p><h3>三、国家数据局正式揭牌</h3><p></p><p></p><p>2023 年 10 月 25 日，国家数据局正式揭牌。国家数据局负责协调推进数据基础制度建设，统筹数据资源整合共享和开发利用，统筹推进数字中国、数字经济、数字社会规划和建设等，由国家发展和改革委员会管理。</p><p></p><p>随着大数据、人工智能、云计算等数字技术的快速发展和应用，数据量呈爆炸式增长，数据价值日益凸显。如今，数据已经成为数字经济时代的生产要素，同时也是我国抓住新的产业发展机遇，推动经济社会高质量发展的关键抓手。</p><p></p><p>延展阅读：《<a href=\"https://www.infoq.cn/article/6xT0wjtbRdBLSOFcbI90?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">历时 7 个月，国家数据局正式揭牌，加速释放数据价值</a>\"》</p><p></p><h3>四、促进数据安全产业发展</h3><p></p><p></p><p>2023 年 1 月，工业和信息化部、国家网信办、发展改革委等十六部门印发《关于促进数据安全产业发展的指导意见》。指导意见聚焦数据安全保护及相关数据资源开发利用需求，提出促进数据安全产业发展的总体要求，分两个阶段明确发展目标。</p><p></p><p>到 2025 年，数据安全产业基础能力和综合实力明显增强。到 2035 年，数据安全产业进入繁荣成熟期。产业政策体系进一步健全，数据安全关键核心技术、重点产品发展水平和专业服务能力跻身世界先进行列，各领域数据安全应用意识和应用能力显著提高。</p><p></p><p>指导意见分两个层面明确促进数据安全产业发展的七项重点任务，一个层面是围绕产业本身要做什么，明确了提升产业创新能力、壮大数据安全服务、推进标准体系建设和推广技术产品应用四项重点任务；另一个层面围绕以什么为抓手，明确了构建产业繁荣生态、强化人才供给保障和深化国际合作交流三项重点任务。</p><p></p><p>详情：<a href=\"https://www.gov.cn/zhengce/zhengceku/2023-01/15/content_5737026.htm\">https://www.gov.cn/zhengce/zhengceku/2023-01/15/content_5737026.htm</a>\"</p><p></p><h3>五、开展中小企业数字化转型城市试点</h3><p></p><p></p><p>2023 年 6 月，财政部、工信部联合印发《关于开展中小企业数字化转型城市试点工作的通知》，为深入贯彻落实党中央、国务院关于支持中小企业创新发展、加快中小企业数字化转型系列决策部署，2023—2025 年拟分三批组织开展中小企业数字化转型城市试点工作。</p><p></p><p>该政策旨在通过开展城市试点，支持地方政府综合施策，探索形成中小企业数字化转型的方法路径、市场机制和典型模式，梳理一批数字化转型细分行业，打造一批数字化转型“小灯塔”企业，培育一批优质的数字化服务商，开发集成一批“小快轻准”（小型化、快速化、轻量化、精准化）的数字化解决方案和产品，通过示范带动、看样学样、复制推广，引导和推动广大中小企业加快数字化转型，全面提升中小企业数字化水平，促进数字经济和实体经济深度融合。</p><p></p><p>10 月 29 日，全国中小企业数字化转型试点城市实施工作正式启动，苏州、厦门、长春、宁波等 30 个市（区）纳入首批试点。</p><p></p><p>详情：<a href=\"https://www.gov.cn/zhengce/zhengceku/202306/content_6886368.htm\">https://www.gov.cn/zhengce/zhengceku/202306/content_6886368.htm</a>\"</p><p></p><h3>六、加快推进能源数字化智能化发展</h3><p></p><p></p><p>2023 年 3 月，国家能源局印发《关于加快推进能源数字化智能化发展的若干意见》（以下简称《意见》），提出针对电力、煤炭、油气等行业数字化智能化转型发展需求，通过数字化智能化技术融合应用，急用先行、先易后难，分行业、分环节、分阶段补齐转型发展短板，为能源高质量发展提供有效支撑。到 2030 年，能源系统各环节数字化智能化创新应用体系初步构筑、数据要素潜能充分激活，一批制约能源数字化智能化发展的共性关键技术取得突破。</p><p></p><p>《意见》提到，国家明确的各类能源数字化智能化示范项目，各级能源主管部门要加大支持力度，优先纳入相关规划。发挥财政资金的引导作用，落实好促进数字科技创新的投资、税收、金融、保险、知识产权等支持政策，加大对能源数字化智能化技术创新的资金支持力度，形成支持能源数字化智能化发展的长效机制。</p><p></p><p>详情：<a href=\"https://www.gov.cn/zhengce/zhengceku/2023-04/02/content_5749758.htm\">https://www.gov.cn/zhengce/zhengceku/2023-04/02/content_5749758.htm</a>\"</p><p></p><p></p><h3>七、加快推动智慧民航建设发展</h3><p></p><p></p><p>2023 年 6 月，中国民用航空局发布《关于落实数字中国建设总体部署 加快推动智慧民航建设发展的指导意见》（以下简称《指导意见》）。</p><p></p><p>《指导意见》遵循了《数字中国建设整体布局规划》“2522”整体框架，立足行业数字化转型阶段性特点和智慧民航建设发展，细化了“2522”民航行业工作部署，即夯实数字基础设施和数据资源体系“两大基础”，数字化赋能民航安全生产、航空服务、绿色发展、政府监管、行业文化“五个重要领域”，强化民航数字技术创新和筑牢民航数字安全屏障“两大核心动力和基本保障”，优化数字治理生态和国际合作格局“两大数字化环境”。</p><p></p><p>《指导意见》还围绕数字化赋能民航高质量发展的五个重要领域提出了加强机场全域高效协同等 14 项工作举措，加快民航数字化转型，提高行业全要素生产率。</p><p></p><p>详情：<a href=\"https://www.gov.cn/zhengce/zhengceku/202307/content_6889672.htm\">https://www.gov.cn/zhengce/zhengceku/202307/content_6889672.htm</a>\"</p><p></p><h3>八、加快推进公路数字化转型</h3><p></p><p></p><p>2023 年 9 月，交通运输部印发《关于推进公路数字化转型 加快智慧公路建设发展的意见》（简称《意见》），共八个部分、30 项内容，主要包括总体要求 2 项、主要任务六方面 22 项、保障措施 6 项。主要任务即“六提升、五推动、一筑牢”，包括提升设计施工、养护业务、路网服务、政务服务、技术标准、基础支撑的数字化水平，推动智慧建造、智慧养护、智慧出行、智慧治理、标准升级，筑牢数字底座。</p><p></p><p>《意见》中贯穿了三条主线。一是业务流程，按公路勘察、设计、施工、养护、运行全生命期业务链条展开，加强市场监管和政务服务，完善技术标准，夯实数字化基础；二是数据要素，将数据作为核心要素贯穿全文，包括数据的生成、获取、汇总、联网、算法、应用、服务、保障等；三是各类主体，充分考虑不同等级公路各类从业单位的特点，突出项目法人在实施层面的主导作用，注重针对性、实用性，强调“重安全、保畅通、提效率、优服务、降成本、减排放”。</p><p></p><p>详情：<a href=\"https://xxgk.mot.gov.cn/2020/jigou/glj/202309/t20230920_3922478.html\">https://xxgk.mot.gov.cn/2020/jigou/glj/202309/t20230920_3922478.html</a>\"</p><p></p><h3>九、提升银行办理资本项目业务数字化服务水平</h3><p></p><p></p><p>2023 年 11 月，中国人民银行、国家外汇管理局认真贯彻落实中央金融工作会议精神，在总结前期试点经验的基础上，发布《中国人民银行国家外汇管理局关于提升银行办理资本项目业务数字化服务水平的通知》（以下简称《通知》），在全国范围内推广资本项目业务数字化服务，推动银行通过“网上办”“远程办”方式为机构、个人等主体便捷高效办理资本项目外汇和跨境人民币业务。</p><p></p><p>《通知》内容包括：一是支持银行通过审核电子单证的方式办理资本项目业务。二是拓展资本项目数字化业务办理类型，将银行在线下有权限直接办理的资本项目下外汇和跨境人民币业务全面纳入数字化服务范围。三是明确银行办理资本项目数字化业务的形式和条件要求、业务审核及档案管理、相关数据与信息报送规则等。</p><p></p><p>《通知》自 2023 年 12 月 20 日起实施。</p><p></p><p>详情：<a href=\"https://www.gov.cn/zhengce/zhengceku/202311/content_6916279.htm\">https://www.gov.cn/zhengce/zhengceku/202311/content_6916279.htm</a>\"</p><p></p><h3>十、推进普惠金融高质量发展目标</h3><p></p><p></p><p>2023 年 10 月，国务院印发《关于推进普惠金融高质量发展的实施意见》，明确了未来五年推进普惠金融高质量发展的指导思想、基本原则和主要目标，提出了一系列政策举措。</p><p></p><p>实施意见指出，在未来五年基本建成高质量的普惠金融体系，努力实现基础金融服务更加普及、经营主体融资更加便利、金融支持乡村振兴更加有力、金融消费者教育和保护机制更加健全、金融风险防控更加有效、普惠金融配套机制更加完善的目标。</p><p></p><p>其强调，基础金融服务的效率和保障能力显著提升，数字化服务水平明显提高。并且聚焦“提升普惠金融科技水平”、“打造健康的数字普惠金融生态”、“健全数字普惠金融监管体系”三个方面提出了普惠金融的数字化发展目标。</p><p></p><p>详情：<a href=\"https://www.gov.cn/zhengce/content/202310/content_6908495.htm\">https://www.gov.cn/zhengce/content/202310/content_6908495.htm</a>\"</p>",
    "publish_time": "2023-12-20 15:52:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "来 QCon15 周年上海站，看大模型技术应用展，共探 AI 技术新未来",
    "url": "https://www.infoq.cn/article/PyjLcAtodXxAsTsCcy5D",
    "summary": "<p>引言：在人工智能领域，大模型技术正迅速成为驱动行业创新的重要力量。QCon 上海站特色晚场，即将举办一场专注于国内大模型技术应用的路演活动，旨在揭示这一领域的最新进展和实际应用。如果您对大模型技术充满热情，渴望深入了解其在企业应用中的潜力，那么这是一个不可错过的机遇。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/40/f2/400d2a6094f9fda511f679c4b4ea43f2.png\" /></p><p></p><p>活动概览：</p><p>活动主题：国内大模型技术应用最新展示活动时间：12月28日 18:30-20:30活动目标：展示国内AI企业如何创新利用大模型技术。分享具体的应用案例，激发技术人员的创新灵感。促进行业内的技术交流，提供直观、实用的大模型应用知识。主题演讲：我们特邀行业专家进行主题演讲，深入探讨大模型技术的发展趋势及其在国内的应用现状案例分享：来自不同行业的代表将分享他们在大模型技术应用上的真实案例，包括金融、医疗、教育、制造业等领域的创新应用。</p><p></p><p>特别邀请：我们特别邀请那些正在探索大模型技术应用、希望在行业中展示自己创新成果的企业加入我们。无论您是初创企业还是行业领头羊，只要您愿意分享您在大模型技术方面的经验和成果，都欢迎您来报名参加。</p><p></p><p>QCon特色晚场这里，您不仅能了解最前沿的大模型应用案例，还能与行业同仁深入交流，甚至有机会展示您自己的技术成果。我们热切期待您的参与，一起见证大模型技术如何塑造智能未来。</p><p></p><p>立即报名免费参加，与业界领袖共同探索大模型技术的边界！报名链接戳：<a href=\"https://jinshuju.net/f/soqwsp\">https://jinshuju.net/f/soqwsp</a>\"</p><p></p><p>活动推荐：</p><p><a href=\"https://qcon.infoq.cn/202312/shanghai/\">QCon 全球软件开发大会</a>\"（上海站）即将在 12 月 28-29 日开始，届时将围绕 GenAI 和通用大模型应用探索、AI Agent 与行业融合应用的前景、LLM 时代的性能优化、智能化信创软件 IDE、面向人工智能时代的架构、性能工程：提升效率和创新的新方法等专题进行交流。咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p></p>",
    "publish_time": "2023-12-20 15:54:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型重构云计算，AI原生应用创造云端新格局",
    "url": "https://www.infoq.cn/article/L8mq3DjpiuCT0dgAXWzN",
    "summary": "<p>2023 年，大模型热潮为软件应用开发行业带来了前所未有的深度影响，基于生成式 AI 技术打造的原生应用正在成为新的产业增长点，AI 原生应用生态快速走向繁荣。在这样的背景下，为 AI 大模型提供基础设施的云计算产业也即将迎来变革，产业模式重新洗牌。</p><p>&nbsp;</p><p>12月20日，在 2023 百度云智大会·智算大会上，百度集团副总裁侯震宇以“大模型重构云计算”为主题发表演讲。他强调，AI 原生时代，面向大模型的基础设施体系需要全面重构，为构建繁荣的 AI 原生生态筑牢底座。侯震宇表示，大模型不仅驱动了底层 IT 基础设施的变革，也带来了上层应用开发模式的颠覆。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/dd/dd6ab85ab8c07ae19cb49adc931b30c3.png\" /></p><p></p><p>本届的大会上，百度新发布了针对大模型专项优化智算平台百舸的 3.0 版本升级，并向行业全面开放 AI 原生应用开发工作台，百度智能云千帆 AppBuilder。在一系列创新技术和全新的架构理念引导下，百度智能云正在为 AI 原生应用开启云计算行业的全新格局。</p><p>&nbsp;</p><p></p><h2>三大领域融合，催生 AI 原生应用研发新范式</h2><p></p><p></p><p>过去十年来，移动应用、深度学习和云计算三大产业几乎同时崛起，并且互相影响和促进，造就了 IT 产业前所未有的繁荣时代。然而，应用、AI 技术和云端基础设施三大层面一直在各自独立演进，未曾深度融合，直至大模型技术逐渐成熟，这三条平行线也终于等到了交汇的时刻。</p><p>&nbsp;</p><p>在应用开发层面，大模型理解、生成、逻辑、记忆的能力将改变应用的技术栈、数据流和业务流，催生 AI 原生应用开发新范式。与此同时，大模型将成为通用服务能力——MaaS 模型即服务，大幅降低 AI 落地门槛。MaaS 成为新的基础服务后，其依赖的新型 IT 基础设施则将在底层颠覆传统云计算产业。未来，大模型将以面向 AI 打造的云原生智算体系为基础，助力 AI 原生应用生态蓬勃发展，</p><p>&nbsp;</p><p>以上述理念为基础，百度智能云在大模型和云计算领域进行了全新的产品布局和技术迭代。2023 年 3 月 16 日，百度文心一言大模型发布；3 月 27 日，百度智能云基于文心一言推出面向企业客户的千帆大模型平台，并在 8 月 31 日全面开放。 同时，百度智能云自身以大模型为驱动，以云智一体为战略持续高速发展，在 IDC 的 AI Cloud 市场占有率评估中连续 8 次实现占比第一。</p><p></p><h2>依托云智一体核心战略，百度智能云全面重构云计算体系</h2><p></p><p></p><p>今天，百度智能云正在依托云智一体核心战略，向客户提供符合 AI 原生应用时代需求的全方位产品和解决方案矩阵。</p><p>&nbsp;</p><p>在大模型层面，百度智能云提供业界领先的 MaaS 服务平台，千帆大模型平台，该平台提供了丰富好用的基础大模型选项，以及完整易用的大模型工具链，覆盖大模型精调、压缩等全流程。为了帮助客户基于大模型构建数据飞轮，千帆还提供了数智一体的数据飞轮工具链，包括数据管理全生命周期工具。开放近 4 个月以来，千帆平台大模型日调用量增长 10 倍，帮助众多行业客户实现了基于大模型能力的业务创新与升级。</p><p>&nbsp;</p><p>百度智能云面向大模型的云端基础设施体系正在全面重构，为此发布了百度百舸 AI 异构计算平台，提供了面向大模型训推的多芯、高速互联、高性能存储及加速能力。本届云智大会上，百舸平台升级 3.0 版本，训练和推理吞吐量相比开源版本提升最高 60%，机器有效训练时常达 98%，带宽有效利用率达 95%，支持万卡级别超大规模 AI 计算和丰富的运维、可观测工具及容错保障能力。百度智算网络平台支持接入了第三方智算中心、传统 IDC 资源和边缘端等智算资源，更好地满足智算资源供给平衡。此外，百度太行计算、网络产品、容器引擎、沧海存储产品、数据库 GaiaDB 等产品也进行了面向大模型能力的全面重构升级。</p><p>&nbsp;</p><p>开发 AI 原生应用需要开发者解决很多挑战，克服诸多困难，为此百度推出了针对性的 AI 原生应用开发工作台——百度智能云千帆 AppBuilder，并在本届大会上正式全面开放。AppBuilder 将基于大模型开发各种应用的常见模式、工具、流程，沉淀成一个工作台，能够让每一位开发者聚焦在自己的业务诉求上，无需为 AI 开发过程中的技术选型、模型与工具选择优化、业务落地等问题担忧。千帆 AppBuilder 提供低代码与标准代码两种开发方式，使开发者能够在平台一站式获取 AI 原生应用的全套开发资源，满足更加灵活、多样的 AI 原生应用开发需求。</p><p></p><h2>三大维度发力，百度智能云向 AI 技术普惠目标迈进</h2><p></p><p></p><p>在大模型、云端基础设施与 AI 原生应用开发三大维度同时发力，让百度智能云在大模型技术崛起的当下占据了行业先机。更为重要的是，解决了三大维度技术、产品与服务互联互通的挑战后，百度智能云向着 AI 应用开发生态繁荣、 AI 技术普惠的目标就迈进了一大步。</p><p>&nbsp;</p><p>未来，专注于 AI 原生应用创新的开发者可以完全依赖百度智能云平台，在同一个技术栈内完成从大模型选型到应用开发，再到产品落地的全套流程。在此过程中，百度智能云的智算基础设施能够对开发者完全屏蔽底层复杂性，按需提供算力和运维支持。百度智能云的 AI 开发社区生态则能为开发者带来丰富的开发资源、最佳实践和专家协助，使开发者能够完全专注于应用本身的能力创新上。对于中小企业团队与个人开发者而言，百度智能云的全新智算平台体系能够大幅降低开发成本和新技术学习难度，使他们能够以较低的投入制作出优秀的 AI 原生应用造福大众。</p><p>&nbsp;</p><p>侯震宇在大会演讲最后也表示，百度智能云的战略是云智一体、深入产业、生态繁荣、AI 普惠。百度智能云将持续努力，不断推出有竞争力的产品方案，让 AI 技术普惠可得，和生态伙伴一起持续深入客户场景，帮助客户实现数智化升级，助力行业涌现更多 AI 原生应用创新，早日实现 AI 技术普惠目标。</p>",
    "publish_time": "2023-12-20 17:33:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一代更比一代强，AI 时代的至强如何为云服务保驾护航？",
    "url": "https://www.infoq.cn/article/74LiswphPqhsxDFiO4m8",
    "summary": "<p>2023 年，生成式 AI 研究和应用的爆发给云计算产业带来了全新的机遇和挑战：大模型需要庞大的算力支持，用户普遍需要向云计算厂商购买算力服务；且由于大量用户涌入云服务市场，云厂商需要尽快升级数据中心算力以应对 AI 需求，同时持续降低 TCO，为用户提供价格合理的算力资源；此外，AI 应用开发还涉及大量隐私敏感数据的云端存储和使用，云厂商也要全力保障这些数据的安全可靠，打消用户后顾之忧。</p><p></p><p>基于上述需求，云厂商迫切需要对已有硬件基础设施进行更新换代，要求新一代 CPU 能在保障基础设施平稳升级迭代的同时，具备更强的性能、更低的 TCO，同时能够满足云端多样化工作负载需求的较强 AI 能力：</p><p></p><p>对于大型云服务基础设施而言，稳定性、可靠性依然是王道，因此云服务厂商升级硬件时决策更加谨慎，偏向于在有着长期延续性的主流平台上逐渐迭代，保护上层软件应用投资，减小对基础设施开发运维部门的冲击。</p><p></p><p>大模型在云端训练、推理的过程需要用户将大量数据传输至云端，云厂商需要采取更强的安全措施，如硬件级的安全引擎来更好地保障敏感数据的安全，确保云实例间的数据隔离，预防恶意入侵和泄漏。</p><p></p><p>AI 应用涉及密集的低精度矩阵运算，需要较大的内存空间。对于大模型推理应用和中小尺度（参数规模低于 20B）模型的训练应用而言，其在搭载 AI 加速器的 CPU 上运行可以获得非常好的能耗比与性价比，还能够以极具优势的 TCO 满足云厂商大多数 AI 服务的需求。同时 CPU 的通用计算能力也可以为云厂商提供充足的灵活性，有效保护基础设施投资。</p><p></p><p>面对上述需求，英特尔作为服务器 CPU 领域的技术领导者，继年初发布第四代至强®&nbsp;可扩展处理器之后，加快了产品更新节奏，于上周发布了第五代至强® 可扩展处理器，其可与上一代处理器兼容，提供硬件级安全和可信服务，并通过丰富的 AI 产品组合驾驭整个 AI 管线，从而进一步壮大了应对人工智能时代的产品组合。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e2/e2fec970245f7aeb907e35017b5ef7a5.png\" /></p><p></p><p></p><h2>1 技术创新解决三大维度需求，CPU 继续扮演 AI 时代基础设施关键角色</h2><p></p><p></p><p>相比上一代产品，第五代至强®&nbsp;可扩展处理器的核心数量增加至 64 个，拥有更高的单核性能和内存带宽，三级缓存容量提升近 3 倍。其每个内核都具备 AI 加速功能，内置的英特尔®&nbsp;AVX-512 及英特尔®&nbsp;AMX，能使机器学习、深度学习和大模型应用的性能大幅提升。第五代至强®&nbsp;可扩展处理器还能通过英特尔®&nbsp;SGX/TDX 为使用中的云端数据提供端到端硬件级防护能力。与上一代至强®&nbsp;可扩展处理器相比，五代至强®&nbsp;在相同功耗下的平均性能提升了 21%，而 AI 推理和训练性能的提升更是高达 42% 和 29%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/352db6864ec89f190d795f22e8d71fdd.png\" /></p><p></p><p>一系列技术创新，使第五代至强®&nbsp;可扩展处理器成为 AI 时代云厂商的基础设施关键角色。目前，已经有多家客户在实际业务中部署了第五代至强®&nbsp;可扩展处理器，在实践中证明了它为用户带来的巨大收益提升。其优异的表现得到了客户的很高评价，也让更多准备升级云计算基础设施的企业对新一代至强®&nbsp;有了更高的期待。</p><p></p><p></p><h2>2 英特尔 AMX 提升大模型推理性能，助力京东升级营销购物体验</h2><p></p><p></p><p>2023 年京东云突破性地在数百个 AI 场景中应用了大模型，在数百个营销场景中升级了原有工作流，显著提升了商家与消费者的购物体验。</p><p></p><p>基于自研的言犀 AI 与大模型，京东云通过 AIGC 管道生成了 30% 的大促物料，京小智数字人、领航者营销平台也在大模型支持下获得了高达 87% 的商品推荐采纳率，消费者应答准确率提升 30%。 </p><p></p><p>京东大模型第一次亮相就收获完美成绩，很大程度上要归功于其部署的基于第五代英特尔®&nbsp;至强®&nbsp;可扩展处理器的新一代自研服务器，与上一代自研服务器相比整机性能提升 23%，关键的 AI CV 推理性能与 Llama v2 大模型推理性能更是分别提升 38% 与 51%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7b/7b46ba762bc7819e62b5a66ec23c1d8a.png\" /></p><p></p><p>取得如此显著的 AI 推理能力进步，秘诀在于第五代至强®&nbsp;可扩展处理器搭载的英特尔®&nbsp;AMX 加速引擎。其可以将 INT8 低精度矩阵运算速度提升一个数量级，再结合第五代至强®&nbsp;可扩展处理器更高的内存带宽与更强的多核心互联能力，使 AI 推理性能相较上一代显著提升。在 11.11 大促中，第五代至强®&nbsp;可扩展处理器和英特尔®&nbsp;AMX 的组合在京东云承载的 AI 推理应用服务中大展身手，助力用户访问峰值同比提升 170%，智能客服咨询服务量超 14 亿次，且并未增加能耗，也将京东云基础设施的运维成本维护在之前的水平内。</p><p></p><p></p><h2>3 英特尔®&nbsp;TDX 赋能可信计算环境，为阿里云客户构筑端到端数据安全城墙</h2><p></p><p></p><p>对于云计算厂商而言，要让更多行业和组织信任云服务，就必须提供有足够说服力的安全隐私保障，所以云厂商迫切需要更高水准的硬件级安全城墙。</p><p></p><p>对于云环境中使用状态中的数据，机密计算是实现其有效保护的良策，其为客户敏感数据提供了基于硬件设备的可信执行环境（Trusted Execution Environment, TEE），通过隔离保护的方式来防止未经授权的入侵者访问或修改处理中的数据。作为机密计算技术的重要引领者，英特尔®&nbsp;软件防护扩展（英特尔®&nbsp;SGX）技术提供了应用层面的隔离能力；而在和阿里云的合作中，则由英特尔®&nbsp;TDX 技术与阿里云新实例搭载的可信平台模块（TPM）相配合，结合阿里云自研的加密计算隔离环境 enclave，为阿里云第八代企业级 ECS 实例 g8i 构建了一个基于虚拟化的硬件可信环境，即为整个虚拟化实例（包括虚拟机、容器）都构建出可信的边界，由此为客户提供了可信边界更大、更易部署的安全云环境。</p><p></p><p>英特尔® TDX 使 TEE 环境的可信边界获得了有效扩展，从而让 IaaS、PaaS 等环境中的云工作负载都能整体纳入机密计算的数据保护之下，能够有效抵御恶意威胁，加强云端数据隔离。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/15eca93de66726cf751f95c686575b2f.png\" /></p><p></p><p>阿里云自研的千问大模型就得到了英特尔®&nbsp;TDX 的充分保护，实现模型数据端到端加密保护。英特尔®&nbsp;TDX 技术为 AI 大模型这样需要向云端传输大量数据的应用场景铸就了足够牢固的安全保障，也为生成式 AI 应用广泛普及铺平了信任道路。此外，在引入第五代至强®&nbsp;可扩展处理器之后，第八代企业级 ECS 实例在计算、网络、存储、安全等工作负载中的都得到了显著提升，在数据库、硬件加解密、AI 应用、音视频等场景性能提升 15%~25% 不等。更重要的是，八代实例保持价格不变，使阿里云 g8i 实例可以用更小的性能开销保障用户的数据高度安全性。</p><p></p><p></p><h2>4 第五代至强®&nbsp;可扩展至强算力大升级，支持火山引擎实现降本增效目标</h2><p></p><p></p><p>火山引擎的大规模云原生基础设施包含超过一百万台服务器、上千万容器实例，管理数十 EB 级别存储资源，需要应对 10 亿 + 级 QPS 缓存峰值、10+TB/s 的读写峰值带宽，支持数亿日活的应用访问。</p><p></p><p>为了应对如此复杂的需求，火山引擎一直都选择和信赖英特尔®&nbsp;至强®&nbsp;解决方案，并率先引入第五代至强®&nbsp;可扩展处理器，助其第三代弹性计算实例加码全新升级。</p><p></p><p>与基于第四代至强®&nbsp;的弹性计算实例相比，第五代至强®&nbsp;可扩展处理器助力火山引擎释放了巨大算力和性能红利，其弹性计算实例整机算力提升 39%，内存带宽提升 17%，并在 AI、视频处理性能、Java 应用性能等方面均有 40% 左右的性能提升。火山引擎计划推出使用英特尔原生硬件加速技术的能力升级，以 Nginx 为例，使用英特尔®&nbsp;QAT 进行数据压缩和证书验证操作的吞吐量最高可提升 5 倍；在 RocksDB 中，使用英特尔®&nbsp;IAA 进行数据压缩读写的吞吐量最高可提升 1.9 倍。提升如此巨大的算力进化幅度，使火山引擎能够使用相同的实例数量应对更多业务需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9e82d1fa45d9cebb01a4fba93ccaaab0.png\" /></p><p></p><p>如今，火山引擎正在构建百万核心级别弹性资源池，为业务的流量增长、体验创新与安全性增强提供海量算力保障。</p><p></p><p>第五代至强®&nbsp;可扩展处理器提供澎湃的算力的同时，还与上一代处理器兼容，共享架构与平台，大大减少测试和验证工作，其更高的性能、更好的安全性、更高的成本效益，已经在头部云服务提供商中得到全面验证。</p><p></p><p></p><h2>5 软硬结合，打通 AI 创新底层瓶颈</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a2f2cc81c276e0558f501c406e3d84da.png\" /></p><p></p><p>除硬件方面的诸多创新，第五代至强®&nbsp;可扩展处理器在软件层面也搭建了良好的生态环境。例如，英特尔已经在 Pytorch、Tensorflow 和 OpenVINO™ 工具套件等行业标准框架中提供了针对第五代至强®&nbsp;可扩展处理器的优化，使得云厂商和用户能够以较低的门槛，快速利用如英特尔®&nbsp;AMX 等处理器功能，打通 AI 应用的算力瓶颈。英特尔®&nbsp;Trust Authority 鉴证服务则能充分验证 TEE 的有效性，发挥英特尔®&nbsp;SGX/TDX 技术的优势。</p><p></p><p>如果说数据中心是一台巨型计算机，那么 CPU 就是它的超级大脑，第五代英特尔®&nbsp;至强®&nbsp;可扩展处理器将一如既往地扮演核心角色。它与网络、GPU、软件技术栈等其他英特尔创新技术一起共同构筑了上层 AI 应用的根基。而这样的根基虽然能力强大，但并不需要用户为此投入大量精力学习或增加运维投入。由此，企业就能将主要精力投入在业务创新中，并在 AI 浪潮中紧紧把握住市场机遇，开启新的增长路径。</p>",
    "publish_time": "2023-12-20 18:00:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]