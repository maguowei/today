[
  {
    "title": "谷歌改进gVisor提高沙箱容器文件系统性能",
    "url": "https://www.infoq.cn/article/NIqyIMorP3VMhsoFLMFT",
    "summary": "<p>谷歌<a href=\"https://cloud.google.com/blog/products/containers-kubernetes/gvisor-file-system-improvements-for-gke-and-serverless/\">改进</a>\"了<a href=\"https://github.com/google/gvisor\">gVisor</a>\"中的文件系统实现，gVisor是一个开源隔离层，用于面向商业容器的产品，如App Engine、Cloud Run和Cloud Functions。根据谷歌工程师Ayush Ranjan和Fabricio Voznika的说法，新的gVisor文件系统被称为VFS2，可以将文件密集型工作负载的性能提高大约50%-75%。</p><p>&nbsp;</p><p>gVisor的主要目标是在容器和底层内核之间提供一个隔离层，该隔离层由运行在同一节点上的所有容器共享。为了防止恶意或易受攻击的容器危及整个节点的安全，gVisor实现了Linux系统表层的很大一部分，包括一个名为 runsc 的符合开放容器倡议（OCI）的兼容运行时，该运行时在应用程序和主机内核之间提供隔离边界。</p><p>&nbsp;</p><p></p><blockquote>由于gVisor内核不可信任，因此它不能直接访问文件系统。文件系统操作由代理（称为Gofer）来代理操作，该代理与可能的恶意工作负载相隔离。像open、create和stat这样的操作被转发到代理，经过审核，然后再由代理执行。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ae/ae5544961b4795e7d056166753d8038a.png\" /></p><p></p><p>&nbsp;</p><p>谷歌工程师发现，gVisor Gofer文件系统通过将路径解析委派给底层文件系统（每个路径组件使用一个RPC调用）来处理路径解析的方式对性能有害。对于频繁执行文件操作的工作负载，例如构建任务或运行带有大量导入的Python和NodeJS程序时，情况尤其如此。</p><p>&nbsp;</p><p></p><blockquote>要解决这个问题，需要启用gVisor的Sentry，使其能够将路径解析直接委托给文件系统。......例如，在VFS1 stat（/foo/bar/baz）中，至少生成三个到gofer（foo，bar，baz）的RPC，而VFS2仅生成一个。</blockquote><p></p><p>&nbsp;</p><p>此外，谷歌还借此机会创建了一个用于在gVisor沙箱和Gofer之间进行通信的新协议。该新协议名为<a href=\"https://github.com/google/gvisor/tree/master/pkg/lisafs\">LISAFS</a>\"（(Linux Sandbox File system protoco，Linux沙箱文件系统协议），它既减少了RPC调用的数量，也减少了它的内存使用量，改善了多路径组件的遍历，并加快了文件I/O。</p><p>&nbsp;</p><p>Ranjan和Voznika表示，由于这些变化， 根据许多不同的度量指标，runsc引入的开销减少了50%-75%。</p><p>&nbsp;</p><p>与在根文件系统或内存文件系统中托管源代码相比，使用绑定装载时的改进最大。这些结果是通过运行官方bazel基准测试构建gRPC和Abeil而获得的。</p><p>&nbsp;</p><p>基准测试的结果基本上得到了实验数据的证实，实验数据表明，整个平台上的Google App Engine冷启动时间提高了25%以上，这一数据包括了所有类型的工作负载，而不仅仅是文件系统密集型的工作负载。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/01/gvisor-file-system-improvements/\">https://www.infoq.com/news/2023/01/gvisor-file-system-improvements/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/2011/09/app-engine-price-hike\">Google App Engine 涨价，开发者深受打击</a>\"</p><p><a href=\"https://www.infoq.cn/articles/2015/07/JavaScript-GitHub\">Google 发布 App Engine 的 Go 语言通用版</a>\"</p>",
    "publish_time": "2023-02-16 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "第四届高端制造业CIO上海论坛将于2月22日举办，报名倒计时3天",
    "url": "https://www.infoq.cn/article/hQ0IRh0iBN7ygCzdpcAT",
    "summary": "<p>当前，我国<a href=\"https://www.infoq.cn/article/FYSwjaR1k8vyzyVwWQjm\">工业互联网</a>\"创新发展战略深入实施，工业互联网赋能高端制造将进一步催生融合创新应用，赋予各产业转型升级发展新动力。然而，数字化转型是一个长期系统工程，产业升级之路依旧任重道远。</p><p></p><p>由中国能源研究会信息通信专委会、上海市航空学会、上海市汽车工程学会、上海交通运输研究中心大力支持的 “第四届高端制造业CIO上海论坛” 将于2023年2月22日在上海举办。本次峰会以《数智转型 融合发展》为主题。现场汇集350+高端制造行业知名企业高管、CIO、IT负责人以及行业知名信息化服务商，共同总结制造业信息化建设成果，研讨 “十四五”信息化发展方向。加快5G、工业互联网、<a href=\"https://www.infoq.cn/article/u_ck0krm3Hp06fcyvBq2\">工业大数据</a>\"、人工智能、区块链、边缘计算、<a href=\"https://xie.infoq.cn/article/95847fb1bd1680c8d22ca2320\">增强现实</a>\"等在制造行业的应用，促进工业化与信息化全面融合，推动中国制造产业智慧建设、高质量发展。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f2/50/f22fe27f73b1ef20488f105657f93650.jpg\" /></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/79/c3/791fb5697b053bb11c51dd9f8e0283c3.jpg\" /></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a9/8f/a98504effe17c0c7983757a18c3fe48f.jpg\" /></p><p></p><p>大会期间，InfoQ将作为独家媒体合作伙伴全程报道大会盛况。组委会诚挚邀请大家的拨冗支持，共同推动高端制造行业的数字化转型落地！ </p><p></p><h4>会议核心议题</h4><p></p><p>人工智能与工业大数据</p><p>数字孪生与智能制造</p><p>智能制造与数字化运营</p><p>信息安全与数据管理</p><p>云管理与智能运维</p><p>数字化工厂与自动化技术</p><p>第四届高端制造业CIO “创智奖” 颁奖典礼</p><p></p><h4>会议议程</h4><p></p><p>上午主论坛：数智转型 融合发展</p><p>下午论坛A- 工业互联网-制造业数字化转型关键赋能者</p><p>下午论坛B- AI赋能制造行业转型升级</p><p></p><p>报名倒计时3天，请提前扫码预留席位。详情请咨询组委会余秘书 13917378771</p><p></p>",
    "publish_time": "2023-02-16 10:11:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT带火OpenAI！华盛顿大学博士放弃高校offer，加入OpenAI",
    "url": "https://www.infoq.cn/article/xzRzqmUAAhCo69ZSuDJz",
    "summary": "<p></p><blockquote><a href=\"https://www.infoq.cn/article/YYqCPSdRmtkdSl2hhb9Y\">ChatGPT</a>\" 的爆火让 OpenAI 名气大涨，事实上，在发布 ChatGPT 之前，OpenAI 就已因<a href=\"https://www.infoq.cn/article/w1lxxO4qtaVPxZUdqzwi\"> GPT-3</a>\" 等大模型在业界颇具名气。Rowan Zellers 是华盛顿大学博士，研究方向为多模态人工智能系统。在求职时，他做出了一个自己意想不到的决定：他放弃了所有的学术工作机会，转而接受了 OpenAI 的 Offer。本文为他的自述，讲述自己为什么会做出这一决策。</blockquote><p></p><p></p><p>本文最初发表于作者个人博客，经原作者授权，InfoQ 翻译并分享。</p><p></p><p>在我做这一决定的过程中，我感到很紧张，也很有压力，当时感觉就好像在坐过山车一样，但最终我对事情的结果还是非常满意的。对我来说，有两个关键因素起了作用。</p><p></p><p>我感到自己能够胜任 <a href=\"https://www.infoq.cn/article/ZWixRo76hFsOw38tRHNF\">OpenAI</a>\" 的工作。OpenAI 的所在地——旧金山，是一个非常适合我和伴侣生活和工作的城市。在本文中，我将对我的决策过程进行更深入的讨论。</p><p></p><h2>我为什么要写这篇文章？</h2><p></p><p></p><p>在我求职的过程中，我从我的教授那儿学到了许多有用信息，比如，怎样应聘，怎样去面试，怎样创建一个强有力的职位申请。但是，到了做出决定的时候，我反而觉得有点孤独。的确，我很庆幸自己有如此强大的人脉，身边有这么多优秀的教授和行业研究人员。但在职业道路上做出决策，更多的是一种个性化的个人决策，从某种意义上说，“没有正确的答案”。</p><p></p><p>对我决策产生影响的另一个因素是：我所认识的大部分人看起来都在学术界和工业界中做出了抉择。比如，我所认识的大多数教授都是坚定地站在学术体系中（尽管也有涉足工业界），但是我所认识的大多数工业界人士，从未认真地将学术当成一种职业。</p><p></p><p>（这对我来说感觉特别奇怪，因为我在读博士读到一半的时候，就下决心要走“学术路线”，因为那样可以让我推迟在学术界或工业界之间做出最终的决定：人们普遍认为，从学术界转到工业界比从工业界转到学术界要容易得多。不过，几年过去了，我觉得走学术路线是我职业身份的一部分，我许多同龄人也在做同样的事情，所以我感到势头正把我推向了学术路线。）</p><p></p><p>总之，我写这篇文章是为了给出一种独到的看法，说明我如何在一些截然不同的选择中做出自己的决定。（如果有人向我发送电子邮件要求提供建议，或许也能帮上忙！）</p><p></p><p>一些免责声明：本文仅代表本人的观点。我无意提供一般性的建议。我并不认为我有资格给出任何建议（毕竟我从来没有想过要成为一名教授）。此外，我做出这一决定的一个很重要的因素是，我觉得我的领域正处于一种非常独特的情况（稍后再说！），因此这并不一定适用于所有领域。</p><p></p><p>我会尽量把自己的经历告诉大家。我还会以我 2022 年春末的视角来写这篇文章，那时我正在做出一个决定。这么做可能对其他人的决定更相关：因为没有人可以预见将来会怎样，因此做出决定很困难。不过，我非常享受在 OpenAI 的工作，而且我对自己的选择毫不后悔。我对研究和这一领域的想法都改变了（并且会持续下去），向这儿的伟人们学习。</p><p></p><h2>学术求职期间，我的想法发生了转变</h2><p></p><p></p><p>从个人背景来看，我是 2016 ~ 2022 年的华盛顿大学的博士生，很享受读博的经历。我的研究方向是多模态人工智能：我创建的是能够理解语言、视觉以及其他领域的机器学习系统。</p><p></p><p>我的研究兴趣决定了我未来的职业道路，我最兴奋的是从事基础研究和指导初级研究人员。至少在传统的计算机领域，这是学术界的重点。而工业界则专注于应用研究，致力将科学进步转化为成果。</p><p></p><p>在学术界求职，使我对如何在计算机科学的许多不同机构和子领域担任教授有了更深刻的认识。我在所有的面试中，与 160 多名教授进行过交谈。不过，我最后还是很担心，学术界到底能不能适合我。</p><p></p><h3>某些领域在学术界开展大规模基础研究，正变得越来越困难</h3><p></p><p></p><p>我感到脚下大地的晃动。</p><p></p><p>在过去的六年里，学术界（更准确地说是华盛顿大学的顾问研究小组）对我来说是一个绝佳的环境。我被推动着去开辟一个令我兴奋的研究方向。我感到在指导和资源方面得到了慷慨的支持：通过它，我能够领导关于建立多模态人工智能系统的研究，这些系统随着规模的扩大而改进，反过来，（对我来说）产生的问题比答案更多。</p><p></p><p>相比之下，在那段时间里，大多数大型行业研究实验室对我来说并不是很有吸引力。在读博的时候，我曾经试图去应聘实习生，但是从来没有发现任何与我的研究议程相一致的地方。我所知道的大多数行业团队都专注于语言，或者注重视觉，而我不能从中挑选。我在艾伦人工智能研究所（Allen Institute for AI）花了很多时间，这是一家非营利性的研究实验室，与此相比，我觉得它更具学术性。</p><p></p><p>但是，我觉得情况正在改变。在我关注的领域，我担心在学术界开展具有开拓性意义的体系构建研究是非常困难的，并且是一种日益困难的过程。</p><p></p><p>事实上，建立这个体系是非常困难的。这是一项耗资巨大、技术含量高的工程技术。我认为，目前学术界的激励机制并不能适应这种高成本、高风险的体系构建研究。构建一个工件并展示它良好的可扩展性，可能需要研究生花费几年的时间，以及超过 10 万美元的无补贴计算成本；随着该领域的发展，这些数字似乎呈指数级增长。所以写很多论文并不是一个可行的策略。无论如何，这都不应该是目标。</p><p></p><p>但不幸的是，我知道许多学者倾向于将论文发表数量作为一种客观的衡量标准；此外，论文是学术界的硬币——你需要论文来写助学金，在会议上有话可谈，以及让你的学生实习，等等。归根结底，学术生涯的成功有助于学生“建立帝国”，开拓自己的研究议程（因此他们可能会成为其他地方的教授，这样的循环可以继续下去），这与做伟大研究所需的合作形成了内在张力。</p><p></p><p>我认为，更广泛的趋势是学术界转向应用研究。</p><p></p><p>随着核心模型越来越强大，构建成本越来越高，这促使更多的学者在其基础上进行构建——这是我在自然语言处理和计算机视觉中看到的趋势，这两个领域是我一直活跃的领域。这反过来又影响了学术研究、花时间思考和在会议上讨论的问题。这一趋势意味着在会议上发表的关于如何构建这些系统的论文越来越少（当然，还有其他因素在起作用。）。</p><p></p><p>至少对我最初的研究愿景来说，这表明机会之窗在学术界正在迅速关闭。假设我在筹集资金方面非常成功，建立了一个令人惊叹的研究人员实验室，并推动他们做出了令人惊叹的事情——那么，所有这些极其艰难的工作都要花费数年的时间才能完成。在经历了这么多时间之后，我所感兴趣的研究还会有支持者吗？如果该领域目前的进展速度继续下去，无论是从能力和准入门槛来看，似乎都是指数级的进展，那么，七年之后，也许再也没有学者在这一领域工作了，那时候就是我需要终身职位的时候。这是一个疯狂的想法。但话说回来，在过去的七年里，它的进步也是相当疯狂的。</p><p></p><p>从更实际的角度来看，我得改变我的研究方向。但是，这并不是我希望做的事情。也许这就是我最终走上工业道路的根本原因。</p><p></p><h3>学术界和工业界的其他区别</h3><p></p><p></p><p>我对研究的想法（当然是针对我工作的领域）是我做出决定的最重要因素。不过，我也在考虑一系列不同的事情：</p><p></p><p>对重大问题全神贯注。我对教授们的其他职责感到担忧：准备教学材料，为院系和现场服务，建立和管理计算机基础设施，申请拨款和管理经费。虽然我觉得这些事情很多都很有趣，很令人兴奋，特别是在教学中，但我不认为我会喜欢不断变换工作内容的环境。</p><p></p><p>相比之下，在我读博期间，我更愿意把重点放在一个重大课题上。我想，在工业中，这样做要简单得多。作为一名教授，从事实验和编写程序确实非常辛苦，但是在工业界中，在个人贡献者和管理者之间，选项更多。</p><p></p><p>名声和财富。我想，许多人都会潜意识里被学术界所吸引，因为这会让人产生一种名声和排外的心理。我对此没有任何兴趣。我认为，如果追逐错误的东西，把精力集中在排名和名声上，那么就会造成一种陈腐而有毒的氛围。另一方面，很多人都被工业界所吸引，原因是更高的工资（这一点可以理解）。我非常庆幸，我能够投入大把的精力去寻找一个能够让我内心更加充实的环境，这是最重要的。</p><p></p><p>工作保障与职业保障。我想许多人对终身职位有错误的理解，不管是学者还是非学者。终身职位是一份工作的保障，教授更难被辞退。但学术性就业市场的错综复杂性，意味着他们几乎没有职业保障，能够轻松地更换工作。所以，与行业研究人员相比，即便是在如此严峻的宏观经济形势下，他们也能轻松地转换工作（当然，这是由于人工智能在工业界的表现比其他产业要好），而学者们更倾向于反对那些可能给他们分配更多责任、可能减薪或可能让每个人在疫情高峰期亲自授课的管理人员。</p><p></p><p>（顺便说一句，我认为学者们唯一的办法就是成立工会；但有些令人沮丧的是，在华盛顿大学，许多计算机科学教授之前签署了一份反工会声明，扼杀了早些时候的工会运动。）</p><p></p><p>“自由”很复杂。在学术界，我可以在理论上自由地解决任何我想解决的问题，但由于没有足够的资源、正确的激励结构或者一个足够支持的环境，我的工作就可能会受阻。我加入 OpenAI 的原因是，在那里，我能感觉到自己得到了令人难以置信的良好支持，能够精确地解决我最感兴趣的问题。我认为，对于任何行业实验室来说，解决我所关心的问题的能力都需要与产品保持一致，我对这样的安排感到自在。</p><p></p><p>这些只是我加入 OpenAI 后不得不接受的几个维度，但我真的很高兴我做到了。也许我以后会写更多关于这方面的内容，但这里超级有趣。我正在指导初级研究人员，并在一个团队中工作，我可以获得充足的资源，并且我被激励着去解决那些对我来说很重要的挑战性问题。</p><p></p><h2>生活：旧金山是一座神奇的城市</h2><p></p><p></p><p>关于工作，我写了很长的篇章。但是，这个世界不只有眼前的苟且，还有诗与远方。就我而言，我也在寻找一个能让我和我的伴侣都开心的城市。</p><p></p><p>为了说明情况：我的伴侣和我已经相处了九年。她从事技术工作，在大流行期间，她完全是远程办公，这在理论上给了我们很多选择。但是我们希望要一个这样的地方，这样的地方我们不仅能忍受，而且能热爱，最好是和我们过去六年居住的西雅图一样（或更多）。按照美国的标准，西雅图很适合步行，我们发现西雅图是一个与其他同龄年轻人交朋友的好地方，也是一个追求共同兴趣的好地方，如旅游、徒步旅行、滑雪、攀岩和双人瑜伽。</p><p></p><p>一方面，作为一个进入学术界就业市场的人来写这些话，我感到很荣幸。学术界的就业市场是如此严酷和艰苦，以至于许多人不得不做出极大的牺牲，仅仅为了追求他们所热爱的事业，尤其是在计算机以外的领域。我听说过一些恐怖的故事，例如，教授上下班的路上要花好几个小时，或者双学位夫妇在不同的城市接受工作，然后长途跋涉，只是希望将来有一天能够一起找到工作。</p><p></p><p>另一方面，我不需要加入这场游戏。我对进入学术界还是进入工业界的犹豫不决和疑虑，也给了我足够的选择和自由。</p><p></p><p>我喜欢旧金山的城市规划，比我过去几年有机会参观的任何美国城市都要喜欢。这座城市适宜步行，商店、餐馆和杂货店都很有人情味，并且有一个连接的自行车基础设施和公共交通网络。</p><p></p><p>这并不是说这座城市十全十美。旧金山的物价昂贵，而且中产阶级化也存在严重的问题，我意识到搬到这里会加剧这个问题。不过，我也很欣赏像租金管制这样的政策，至少为现有居民提供了一些保护。与之形成鲜明对比的是，西雅图没有租金控制，因此企业房东可以轻易地提高租金价格。</p><p></p><p>在基础设施和城市规划方面，我认为旧金山还没有达到阿姆斯特丹的水平。许多自行车道感觉没有得到很好地保护，送货司机经常把车停在这些自行车道上。我很高兴能够支持像旧金山自行车联盟这样的地方组织，这些组织正在解决这些问题方面取得进展。</p><p></p><p>还有一个重要的因素：我和我的伴侣都是在旧金山湾区长大的，父母都住在这一带。这一点，再加上其他因素，让我们意识到旧金山将是一个非常适合居住的地方。</p><p></p><h2>收场白：加盟 OpenAI</h2><p></p><p></p><p>所以，这是一个漫长的讨论，关于我权衡的因素。</p><p></p><p>我有几个选择。在我所在的地区，担任教授职位，推迟一年，然后在工业界度过这一年，这种感觉很普遍。这有点像“预科”，对于研究人员来说没有什么坏处，但有很多好处：可以继续研究一年，还可以在春季招生期间招收学生。</p><p></p><p>然而，我决定不这样做。我担心我最终会不想来，而且这样做（通过签署学术聘书），我可能会让学校失去一个宝贵的招聘名额。我把这个想法告诉了我在各个学校的教员联系人，他们都非常理解，也很包容。可是，我越是思考，就越是明白自己要做的事情。 我谢绝了一切学术上的 Offer，并与 OpenAI 签订了全职 Offer。</p><p></p><p>半年过去了，我真的很高兴我这么做了，有很多原因。我真的很喜欢在 OpenAI 工作，我和我的伴侣都很享受生活在旧金山。</p><p></p><p>作者简介：</p><p></p><p>罗文·泽勒斯（Rowan Zellers），华盛顿大学博士，研究方向为多模态人工智能系统，最近刚入职 OpenAI。</p><p></p><p>原文链接：</p><p></p><p>https://rowanzellers.com/blog/rowan-job-search2/</p>",
    "publish_time": "2023-02-16 10:40:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT“大战”谷歌搜索：新王加冕还是旧王延续？",
    "url": "https://www.infoq.cn/article/A1EsBgNZp6ejaTRW08KP",
    "summary": "<p></p><blockquote>自从OpenAI发布ChatGPT以来，关于它的杀手级应用会是什么，人们有很多猜测。排名第一的可能要属在线搜索。据《纽约时报》报道，谷歌的管理层已经宣布进入“红色警戒”，努力保护其在线搜索的垄断地位，以抵御ChatGPT将带来的冲击。这场ChatGPT与谷歌搜索之间的大战，究竟谁能称王？可能不同的人心中都有不同的答案。日前，TeckTalks 博客发表评论文章认为，ChatGPT确实是一项很棒的技术，但从现阶段来看，取代谷歌搜索还存在一定的难度。</blockquote><p></p><p></p><p>本文最初发布于TeckTalks博客。</p><p></p><p>ChatGPT是一项很棒的技术，它很有可能会重新定义我们创建以及与数字信息交互的方式。它可以有许多有趣的应用，包括在线搜索。</p><p>&nbsp;</p><p>但说它将取代谷歌可能有点牵强——至少从目前来看是这样。目前，<a href=\"https://www.infoq.cn/article/UrFKiffb44jcffwP5FbH\">大型语言模型（LLM）</a>\"在挑战搜索引擎之前还有许多问题需要解决。即使技术成熟，谷歌搜索也可能是从LLM中获益最多的。</p><p></p><h2>LLM与真实性</h2><p></p><p></p><p>ChatGPT非常擅长回答问题。它让你觉得自己就像是在和一个花了几百年时间汲取知识的人说话。它的输出很流畅，语法也正确，甚至可以模仿不同的演讲风格。</p><p></p><p>然而，有个问题是ChatGPT的答案有时候不对。事实上，它经常产生幻觉，陈述的事实完全错误。在读写能力的表象之下，ChatGPT是一个非常先进的自动补全引擎。它会根据你的提示（和聊天记录）尝试预测接下来会发生什么。而且，即使它的答案大部分看起来是合理的，它也没有把事情做好。</p><p>&nbsp;</p><p>解决ChatGPT输出的<a href=\"https://www.infoq.cn/article/U5xfFfPULVbkDSLR3PAQ\">真实性问题</a>\"将是一项重大的挑战。遗憾的是，目前还没有办法在ChatGPT的输出中区分幻觉和真相，除非你用其他事实来源验证它的答案（或许可以使用谷歌？）。但如果重点是使用大型语言模型作为搜索引擎的替代品，那可能会弄巧成拙。</p><p>&nbsp;</p><p>现在，谷歌或其他搜索引擎所提供的所有内容都不一定是真实的。但至少，它们为你提供了可以进行验证的资源链接。而ChatGPT提供纯文本，不会引用实际的网站（注：在融合ChatGPT的<a href=\"https://www.infoq.cn/article/N2DHuiaEtcIEeXvjbVLC\">最新版Bing</a>\"中，会引用相关网址）。</p><p>&nbsp;</p><p>一个可能的解决方案是添加一种机制，将LLM输出的不同部分链接到实际的网页（一些公司正在试验这种方法）。但这是一项复杂的任务，可能无法用纯基于深度学习的方法来解决。这就需要访问另一个信息源，比如搜索引擎索引数据库（这是经典搜索引擎不太可能很快失去其重要地位的原因之一）。</p><p></p><h2>更新模型</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ede1d2325d7b5f0aa609de4f8a7030a6.jpeg\" /></p><p></p><p>ChatGPT及其他LLM面临的另外一项挑战是更新知识库。搜索引擎可以借助工具和软件不断索引新页面以及修改过的页面。更新搜索引擎数据库也是一项非常高效的操作。</p><p>&nbsp;</p><p>但对于大型语言模型，添加新知识需要重新训练模型。也许不是每次更新都需要重新训练，但与在搜索引擎数据库中添加和修改记录相比，其成本要高得多。如果你想了解最新的新闻，就得每天做很多次。</p><p>&nbsp;</p><p>ChatGPT基于GPT 3.5构建，它可能至少有1750亿个参数。由于任何一个单独的硬件都无法运行这个模型，所以必须将其分解并分布在几个处理器上，比如A100 GPU。配置这些处理器并行训练和运行模型不管在技术上还是财务上都是不小的挑战。</p><p>&nbsp;</p><p>LLM搜索引擎的运营商还需要有机制和工具，来确定哪些网络资源是可靠的知识源并应优先考虑。再一次，我们看到了搜索引擎组件的踪迹。</p><p></p><h2>速度挑战</h2><p></p><p></p><p>LLM还存在推理速度的问题。像谷歌这样的公司已经创建了高度优化的数据库基础设施，可以在不到一秒钟的时间内找到数百万个答案。像ChatGPT这样的LLM则需要几秒钟来撰写回复。</p><p></p><p>搜索引擎不需要为每个查询浏览整个数据集。它们有索引、排序和搜索算法，可以非常快的定位到正确的记录。因此，尽管在线信息的数量在增长，但搜索引擎的速度并没有下降。</p><p>&nbsp;</p><p>另一方面，LLM每次收到提示时都会浏览整个神经网络的信息。诚然，神经网络的规模无法与搜索引擎数据库相比。但是，计算量仍然比查询索引大很多。鉴于深度神经网络的非线性性质，并行化推理操作的程度是有限的。随着LLM训练语料库的增长，模型也必须变得更大，才能在其知识库中很好地泛化。</p><p></p><h2>ChatGPT的商业模式</h2><p></p><p></p><p>不过，基于LLM的搜索引擎最大的挑战可能是商业模式。谷歌在其搜索引擎上建立了一个广告帝国。</p><p>&nbsp;</p><p>谷歌搜索并不是一个完美的商业模式。人们很少会点击那些越来越多地出现在搜索引擎结果页面上方的广告。但谷歌在在线搜索市场的份额如此之大，所以即使点击率很低，它每年也能赚上数十亿美元。</p><p>&nbsp;</p><p>谷歌还可以根据从用户那里收集的数据来个性化搜索结果和广告。这使得它的业务更加高效和有利可图。别忘了谷歌还有许多其他产品，包括YouTube、Gmail、Chrome和Android，可以强化它为用户创建的数字档案。它的广告网络也扩展到了网站和其他媒体。</p><p>&nbsp;</p><p>基本上，谷歌控制着市场的两端：内容搜寻者和广告商。通过控制整个市场，它成功地创造了一个自我强化的循环。在这个循环中，它收集了更多的数据，改善了搜索结果，并提供了更多相关的广告。</p><p>&nbsp;</p><p>作为一个潜在的搜索引擎，ChatGPT还没有一个商业模式，而且成本很高。粗略估计，在100万用户的情况下，ChatGPT每天的成本为10万美元，每月约为300万美元。</p><p></p><p></p><blockquote>据我估计，运行ChatGPT的成本是每天10万美元或每月300万美元。这是一个粗略的计算。我是假设节点都总是在使用，批处理大小为1。而实际上，它们可能在访问量大时进行批处理，而在访问量小时会有GPU处于空闲状态。—— Tom Goldstein （@tomgoldsteincs）2022年12月6日</blockquote><p></p><p>&nbsp;</p><p>现在想象一下，当人们每天运行80亿个搜索查询时会发生什么。现在，再加上定期训练模型的成本，以及通过强化学习和人类反馈来优化模型所需的人工劳动。</p><p>&nbsp;</p><p>训练和运行像ChatGPT这样的大型语言模型的成本是如此之高，以至于让它发挥作用将成为大型科技公司的专利，这些公司可以在没有明确商业模式的无利可图的产品上投入大量资金。</p><p>&nbsp;</p><p>盈利的一个可能途径是将LLM作为像Codex和GPT-3那样的付费API交付。但这并不是搜索引擎的传统商业模式，我不确定它们将如何做到这一点。另一种方法是将其作为一些问答功能集成到微软Bing中，但这将使其与谷歌搜索相提并论，而不是提供一个可以颠覆搜索市场的不同系统。</p><p></p><h2>ChatGPT是一个搜索引擎吗？</h2><p></p><p></p><p>很多人都在谈论ChatGPT将成为万能助手，可以回答任何问题，这在逻辑上引出了它将取代谷歌搜索的想法。</p><p>&nbsp;</p><p>但是，尽管拥有一个可以回答问题的人工智能系统非常有用（假设OpenAI解决了它的问题），但这并不是在线搜索的全部。谷歌搜索有缺陷，它会显示很多没用的广告，也会返回很多没用的结果。但这是一个价值不可估量的工具。</p><p>&nbsp;</p><p>大多数时候，当我使用谷歌搜索时，我甚至不知道正确的问题是什么。我只是把一堆关键字混在一起，看看结果，做一些研究，然后缩小或修改搜索。在我看来，这种应用还不是一个非常有效的问答模型所能取代的。</p><p>&nbsp;</p><p>表面看来，ChatGPT或其他类似的LLM将成为在线搜索引擎的补充。最终，它们很可能会强化现有搜索巨头的地位，因为这些巨头拥有训练和运营它们的资金、基础设施和数据。</p><p>&nbsp;</p><p>原文链接：<a href=\"https://bdtechtalks.com/2023/01/02/chatgpt-google-search/\">https://bdtechtalks.com/2023/01/02/chatgpt-google-search/</a>\"</p>",
    "publish_time": "2023-02-16 11:13:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "RTC 端到端视频体验优化技术实践与探索",
    "url": "https://www.infoq.cn/article/zeyou9a4e4YYieNEi0Ot",
    "summary": "<p></p><blockquote>本文来自火山引擎视频云的技术实践分享</blockquote><p></p><p></p><p>RTC 是一个“发布-订阅”系统，我们在发布端和订阅端做的很多关于画质、性能、卡顿、延时的优化，在经过网络传输之后，不一定能够达到端到端的最优效果。本文介绍 RTC 如何通过发布端和接收端的联动优化，为用户提供更佳的视频通话体验。</p><p></p><h2>传统 RTC 上下行联动优化技术——带宽探测</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/33/90/3358f70540d1818264ae392c259de190.png\" /></p><p></p><p>&nbsp;这是一个多人 RTC 系统的示意图，左边是发布端 Pub（Publisher），右边是接收端 Sub（Subscriber），把视频流从发布端通过一连串的媒体级联服务器送到接收端，就是“发布——接收”的整体链路。在这条链路上，我们可以有效利用一些信息来帮助 RTC 系统做端到端优化，比如把接收端的信息送回发布端做优化。</p><p></p><p>上图是一个比较常见的端到端优化的例子——上下行带宽联动探测。发布端上行带宽有 1 Mbps，接收端下行带宽只有 0.5 Mbps，如果发布端和接收端不做“沟通”，发布端就会按照它的带宽探测 1 Mbps发流，造成的结果就是下行带宽不够了，接收端收不了，延时不断增加，当增加到一定程度的时候，Buffer清空重新发I帧造成大卡顿，用户的感受就是突然一个画面闪过去，中间一段内容都看不到了。</p><p></p><p>当前市面上 99% 的 RTC 厂商都是基于 WebRTC 来开发自己的 RTC 系统，WebRTC 系统支持 RTCP（RTP 的传输控制协议，专门用来传输控制信号），通过 RTCP 协议，我们可以把接收端探测到的网络状况，包括接收端网络的抖动信息、延时信息等回传给发送端，让发送端知道现在接收端的网络状况怎么样。由于 WebRTC 是一个点对点的系统，既然可以通过媒体级联服务器传递音视频数据，也能够使用同样的链路传递其他信息。通过 RTCP 传回的接收端带宽信息，发布端就会“知道”虽然自己有<a href=\"https://xie.infoq.cn/article/2b41c1bd956506770ac2529f7\"> 1 Mbps</a>\" 的带宽，但考虑到接收端的情况，用 0.5 Mbps 来发流更合理。</p><p></p><p>以上是最常见的一个「上下行带宽联动应用」的例子。</p><p></p><p></p><h2>真·端到端上下行联动优化实践</h2><p></p><p></p><p>RTC 系统中的这些“通道”以及通过这些通道传递的“信息”可以被应用来做一些上下行的联动优化，解决一些 RTC 深水区的问题。由于不同应用会使用不同的“信息”和不同的“通道”，我们先归纳一下发布端和接收端的特点，看看哪些是发布端有、接收端没有的，或者哪些是接收端有、发布端没有的“独有信息”。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/60/bc/605afd568e4ebcc776430605a19823bc.png\" /></p><p></p><p>先看发布端。发布端的特点之一是它有“视频源”，即采集或美颜后未受压缩损坏的视频源。RTC 系统中间是网络传输，网络传输的时候不可避免会碰到一些带宽波动，比如弱网、丢包、抖动等情况，为了确保把内容传输出去，发布端势必要做压缩，有压缩就会有损伤，所以接收端永远拿不到损伤前的“源”，只有在发布端才有“源”。如果我们要做质量评估，想知道 RTC 里画质好不好，只有在发布端做才更准确——因为只有在有“源”的情况下，我们才能客观评价接收到的内容跟“源”有多少差距。</p><p></p><p>发布端还有一个特点，就是 1 条流只有 <a href=\"https://xie.infoq.cn/article/458d5f3b741af2134b4e6f03a\">1 个</a>\"发布端，但可能有多个接收端。如果我们需要在 RTC 系统里做 1 个任务，特别是在多人通话场景下，一定会选择在发布端做，因为只要做 1 次就够了。比如在下文「智能场景识别/内容识别」的例子中，我们需要做一些视频内容的分析识别任务，假设 1 条流有 10 个接收端，如果在接收端做识别就需要做 10 遍重复的事，不如在发布端做1次识别，然后把这个信息传递给接收端来得高效。</p><p></p><p>接收端的特点是它能拿到所有网络相关的信息，常见的有丢包、抖动、延时等状态信息，它还“知道”收到了哪些帧，丢了哪些帧，而发布端只“知道”它发出了哪些帧。</p><p></p><p>说完了发布端和接收端的特点，我们再来看看有哪些“通道”可以传输这些信息。</p><p></p><p>上文中已提到，WebRTC 已经可以实现利用标准的“沟通”通道 RTCP 把接收端的网络状态信息回传给发布端。视频的压缩码流标准定义了一个叫 SEI 的 协议，SEI 里面可以带一些 meta data，可以通过它来携带一些个性化的内容信息。SEI 的好处是它可以做到“帧”级别的对齐，RTCP 无法保证什么时候到达，无法精准地控制在某一视频帧做什么事情，但是SEI可以，SEI的劣势是它只能“单向沟通”，只能从“发布端”传到“接收端”。</p><p></p><p>同样方向的流还有 RTP，RTP 是 WebRTC 的标准传输协议，它提供扩展头（Header Extension）功能，我们可以自定义地去扩展一些头部，在 RTP 数据包头中附加一些需要的信息传输。以上 RTCP、SEI、RTP 走的都是 UDP 协议，所以它们有可能会丢。</p><p></p><p>RTC 系统里也有一些“不会丢”的沟通通道，比如 data channel（它在 UDP 协议里做了一些可靠传输机制），还有基于 TCP 传输的信令，这两个都不会丢。不过，“不会丢”并不表示它就是好的，一般来说，“不会丢”表示丢了以后会重传，所以相对来说比较慢。</p><p>&nbsp;</p><p>有了这些总结归纳后，下面通过三个故事来介绍我们如何使用这些信息和通道来做上下行联动优化，解决弱网、丢包、4K屏幕分享卡顿等问题。这三个小故事的基本叙事逻辑是一致的——走的是什么通道？传的是什么信息？解决的是什么问题？</p><p></p><p></p><h2>超分辨率的性能迭代优化框架</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/69/be/69150b301ac56e9fc1d63d17299710be.png\" /></p><p></p><p>&nbsp;第一个故事是关于「超分辨率」。超分辨率是一个比较古老的图像处理问题，它的本质是把低分辨率的图像放大到高分辨率，并想办法恢复或重建图像中的一些细节。由于网络带宽等限制，视频在压缩时无可避免地会受到一些损坏，超分可以做一个“修复者”的工作，它最合适的位置是在接收端。</p><p></p><p>超分在 RTC 中的作用很大，它解决的问题是在系统资源有“限制”的情况下，视频质量被损坏，它能够部分恢复视频的质量（不是完全恢复）。“限制”包含了带宽限制和系统性能限制，比如在网络带宽非常低的时候，假设只有 200Kbps，我们需要要传一个 720P 分辨率的视频，这时传输的视频质量就会非常差，在这种情况下，我们不如先把它先缩小（比如先下采样到 360P），用好一点的质量把小分辨率先传出去，再在接收端用超分把画质还原回来。还有比如一些低端机发不出 720P 的视频，一发就卡顿，我们就可以先降低分辨率发出去，再通过超分把它修复回来。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c0/6f/c0be74d023fa4ca381ec008898a5106f.png\" /></p><p></p><p>超分在 RTC 中应用会遇到一些挑战。首先是计算复杂度的问题，超分不是简单的上采样，目前学术界的方式都是用深度学习卷积神经网络去训练超分模型，不可避免地，这是一个计算量非常大的操作，计算量大会限制超分的分辨率和运行设备，比如限制在比较低的<a href=\"https://xie.infoq.cn/article/1161506f1e55b2e399f5130f2\">分辨率</a>\"，或者一些超分模型只能限制在一些高端机上使用，低端机上跑不动。</p><p></p><p>其次是所有类似的后处理技术都会面临的一个问题：如何衡量超分做得好不好？线上打开超分后，我们非常需要知道，超分到底让画质增加了多少？新的模型在线上是不是比旧的模型要好？这些数据不仅对线上运营有帮助，对之后的算法模型迭代也有帮助。另外，由于深度学习并不是我们设计的一个我们能够理解的算法，它也有可能会造成“损坏”，比如损坏一些暗场景增强、一些美颜特效的效果。“衡量效果”是一个比较大的难题，因为我们只知道它在线下训练模型的测试组里面跑得好不好，无法知道这个模型在线上跑的效果好不好。</p><p></p><p>这两个挑战是我们在 RTC 中应用超分时遇到的比较实际的问题。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e9/18/e9c18d6e4c899775ff6f7aff155fa818.png\" /></p><p></p><p>在讨论如何解决这两个问题之前，我们先了解一下RTC系统中实现视频超分的卷积神经网络——使用 Resnet 的残差神经网络。Resnet 网络可以有很多层，甚至可能高达一百五十几层，但因为要跑在客户端上，所以我们使用了一个非常小的神经网络，我们使用的这个网络有 6 层，但即使是这样，它的复杂度也远比做一些线性的上采样要高。</p><p></p><p>和 Bicubic（OpenCV 常用的一种上采样方法）相比，当我们把视频从 270P 超分到 360P，基本可以达到 0.5dB 左右的视频修复能力。0.5dB 是什么概念？1.5dB 大概是 H.264 和 H.265 两代视频压缩标准之间相差的压缩收益，0.5dB 是它的 1/3。也就是说，在什么事情都不做、所有网络传输条件都一样的情况下，通过超分就可以平白让视频质量增加 0.5dB，这是很高的收益。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/89/89/891b7aceb67dc9fb58eb76f7c6a64689.png\" /></p><p></p><p>我们怎么用“收发联动端到端优化”这个思路解决超分“复杂度高”和“结果难衡量”的问题？</p><p></p><p>刚才我们说过，因为只有发送端有“视频源”，如果要做质量评估，只有在发送端做才是最直接、最准确的，所以我们的解法很简单，就是把超分搬到发布端去做质量评估，计算出超分能够在接收端恢复多少质量。通过这个方式，我们可以对每一个超分迭代模型在线上的表现进行评价。大家可能会认为这么做的复杂度很高，不建议这么做，但实际上，我们在线上只会放比如 2% 的量来做评价，再利用大数据来了解这个模型在线上的表现，了解这个模型在线上到底跑得好不好，第二代模型有没有比第一代模型好。</p><p></p><p>然后，我们用 SEI 把“超分收益好不好、值不值得做”的评估结果传递给接收端。这样做有什么好处？因为超分是在恢复带宽造成的质量“损伤”，但 RTC 系统弱网情况大概只占 20%，也就是说，80% 的情况都是好网，并不会造成视频的损伤，如果我们打开了超分，它不管网络好坏照样跑，把绝大部分计算量资源跑去恢复一个并没有什么损伤的视频是资源的浪费。</p><p></p><p>所以，当发布端已经知道超分在这一系列帧到底有多少恢复量的时候，如果恢复不多（比如网络很好没有什么压缩破坏，或者这帧视频非常简单，低带宽就可以压缩得很好），它就可以直接告诉接收端“现在不值得做超分，把超分关了”，这样我们就可以把复杂度投入在它产出最高画质、修复最高的那一段视频帧里，降低计算的复杂度。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6y/6c/6yyaf208558789065856584b324a906c.png\" /></p><p></p><p>经过端到端优化后的超分技术在抖音直播连麦场景帮我们节省了很多计算量。抖音直播连麦在实际应用时，如果处于弱网条件下，我们的做法是先下采样到 270P，再通过超分把它还原到 360P。由于 52% 的网络情况是带宽大于 500Kbps，网络传输并不会给视频带来额外的质量损伤，这时候用超分做修复的收益是很低的，可以忽略的。</p><p></p><p>所以，针对带宽大于 500Kbps 的场景，我们告诉接收端“不用开启超分”；针对带宽小于 500Kbps 的场景，我们则告诉接收端“开启超分”。发布端通过 SEI 把决策传递给接收端，接收端开启“Adaptive Switch”，动态地开关超分这个功能。通过这个动态的开关，CPU 计算量增量从 4.8% 降到 2.5%，内存的增量也可以几乎减少一半，从 35MB 到 18MB，但整体的质量修复并没有明显的减少，证明了我们其实是把计算量放在了真正有修复能力的这段视频帧上。</p><p></p><p></p><h2>智能内容模式的下行延时优化</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/90/68/90367f85670c16dbcf031661f2e58168.png\" /></p><p></p><p>第二个故事是关于「屏幕分享」的。屏幕分享是视频会议的一个常用功能，它在视频会议里的使用率比开视频还要高。大家在使用屏幕分享时可能会遇到这种情况：在讲 PPT 时突然播一段视频，视频会变得很卡，帧率很低。有一些视频会议厂商针对这种情况支持提供一个模式，叫“流畅模式”，如果播放视频卡，勾一下“流畅模式”，视频就流畅了；下一页 PPT 又变成了文字，我们又发现文字变模糊了，然后厂商会说，这时候应该选择“清晰模式”，它就会变清晰了。这种做法给用户的体验非常差，用户很容易忘记，切来切去也很麻烦。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ec/5c/ecd7650bfa4a760fa1fa98fbfd2f725c.png\" /></p><p></p><p>这个问题的本质在于，屏幕分享的 PPT 和视频是两种截然不同特性的内容，它们具备完全不同的视频参数和弱网对抗策略参数：在共享 PPT（或文档）的时候，我们的要求是越清晰越好，也就是说，它对分辨率的要求很高，但帧率可以很低，一般我们在分享文档的时候，如果是 4K，其实只要每秒 1 帧就够了，大家会看着就会觉得非常舒适，很清楚；在共享视频的时候（比如电影），它对帧率的要求很高，但分辨率可以很低，比如一般的视频可能 720P 就够了，但帧率需要 30FPS。</p><p></p><p>这是在视频参数上的不同，在弱网情况下，这两种内容也有非常不一样的弱网对抗策略：PPT（或文档）需要非常低的延时，特别是投屏场景，大家一定希望电脑换页的时候投屏也马上换页，但它可以忍受非常高的卡顿，两帧之间可以忍受 500ms 的间隔，甚至很多时候每 2 秒 1 帧用户也不会感受到它有没有卡；视频则是完全相反的，视频需要非常高的流畅性和非常低的卡顿，但是它可以容忍比较高的延时，大家其实并不太在意他看到的视频和演讲人分享的视频差了 2 秒钟，只要视频本身是流畅的就可以，但一旦两帧之间有有卡顿的情况，大家就会觉得很不舒服。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/30/6a/30cdd70485b9fa19df2f61638926516a.png\" /></p><p></p><p>在实际情况中，分享的内容是视频还是 PPT 是会动态变化的，用这种“屏幕分享前勾选“清晰模式”或“流畅模式”，勾选后就不动”的做法不太行得通，我们需要一个实时的、能够动态去分辨分享内容类型的机制——我们叫内容检测，去分辨当前分享的视频帧到底是PPT还是视频，然后通过这个信息在发布端做一些策略联动，比如分享 PPT 需要 4K 的分辨率，发布端就可以通知采集来提升分辨率。同时，编码器也可以针对不同的视频内容做不同的参数调整。</p><p></p><p>举几个比较常见的例子，在 H.264 年代有一个非常有名的开源编码器叫 X264 直播，虽然那时还没有“屏幕分享”，但已经有“动画模式 Animation”，如果你告诉它现在在编码的是一个动画，它就可以通过参数调整把压缩率提高 30%。在 H.265 年代，视频标准里面直接写入了 SCC（Screen Content Coding），这是一种针对文字的编码模式，它里面用的 Hash ME 可以针对屏幕内容去做压缩，将压缩率提升 40%，也就是说，如果你告诉编码器它编码的内容是什么，它就可以降低 40% 的带宽。同样地，如果你告诉 Pacer（Pacer是发布端的一个网络控制模块，它可以决定冗余控制，FEC、重传控制的响应）现在共享的是什么类型的内容，它也可以去控制屏幕内容和视频内容的延时。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/87/c6/8780ea53c2ebf10aaf4e09ff67913ec6.png\" /></p><p></p><p>当然，这里最关键的是发布端要怎么把内容检测的结果传给接收端。接收端里的 Jitter Buffer 是整个 RTC 系统里控制延时和卡顿 Trade Off 的关键模块，它可以决定系统的延时是多少，而延时则会直接决定卡顿程度。一个简单的概念，如果愿意使用延时很大的策略，换来的就是系统卡顿减少，类似地，像分享视频这种场景对卡顿的要求很高，就可以选择一定程度的“牺牲延时”。目前，我们使用RTP扩展头的方式把内容检测的结果传递给接收端，传递给 Jitter Buffer，由 Jitter Buffer 来决定“延时”和“卡顿”的偏好。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ec/f4/ecb4b31e24e874c84475697c1b4f4bf4.png\" /></p><p></p><p>我们看一下这个策略为“飞书屏幕分享”带来的收益。在体验方面，如果用户分享的是 PPT，屏幕可以直接从 1080P 提升到 4K，帧率从 15FPS 降到 5FPS，如果用户分享的是视频，帧率可以从 15FPS 提升到 30FPS。另外，通过收发联动优化，Jitter Buffer 收到了内容检测信息以后，可以针对 PPT 限制它的 Max Jitter Delay，同时关闭 AV 同步，这样做以后，整体分享文档的延时可以从 400ms 降低到  240ms。</p><p></p><h2>智能参考帧的极致弱网延时体验优化</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/62/ce/628a0f26bab39970bc03886a78b3c5ce.png\" /></p><p></p><p>第三个故事是关于「智能参考帧」的一些实践。智能参考帧技术解决了 RTC 的一个深水区问题——大卡顿（我们一般定义两帧间隔超过 2s 以上的叫做大卡）。一个丢包造成的弱网很容易导致卡死的情况（Frozen Frames），它是怎么造成的？</p><p></p><p>举个简单的例子，一般情况下，如果丢包了，短时间之内接收端会请求重传，让发送端再传一次（视频参考帧是有依赖关系的，这一帧其实依赖前一帧，下一帧就依赖这一帧），如果发送端能够重传过来，接收端能够补上这一帧，后面就都可以顺利地解码和输出。但如果重传失败，时间到一定的长度，接收端就会直接请求一个 PLI（Package Loss Indication），它会触发发布端开始发送 I 帧，I 帧就是关键帧，它不依赖任何其他帧，所以它特别大，在丢包网络中，越大的帧越容易丢包，越丢包它越传不到，越传不到越请求它，它就越编一个更大的帧给接收端，然后越接收不到，如此便会造成大卡顿的恶性循环。</p><p></p><p>大家可能会想到，把I帧编小一点是不是就解决问题了。这里给大家一个数字，I 帧跟 P 帧大概是 3-5 倍的大小差，也就是说，I 帧约是 P 帧的 5 倍大。如果把它限制在2倍大，让它好传一点，它的质量就会特别差。大家在开视频会议的时候，如果发现每 2 秒画面就会闪一下，这种间隔的闪动叫呼吸效应，它表示你看到 I 帧了。 为了把I帧缩小，I 帧就会很模糊，它的质量和前后帧会很不一样。</p><p></p><p>这并不是我们想要的结果。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/5e/c4/5efd38f93d966b5abyy5fe50bbdc7ac4.png\" /></p><p></p><p>智能参考帧提供的方案很简单，它在接收端跟发布端维持了一套一模一样的参考帧关系。这样做的好处是，当系统进入大卡时，发布端其实知道接收端手上有什么已经成功解码的关键帧，所以它可以发布一个接收端已经有的参考帧，而不是重新发I帧，这个技术叫做 LTR (Long Term Reference)，它的原理是，不管什么时候向发布端请求，因为发布端“知道”接收端已经完整收到了某些帧（这些帧可以当参考），它就发一个接收端手上已有的参考帧。它改变了原来固定的参考帧关系，变成“我知道你有什么，让你去参考你自己有的东西，这样你永远可以解码”。其次，它可以解决带宽浪费的问题，原来接收端请求一个关键帧，需要清空 Buffer；现在发布端发送的任何东西，除非被网络丢包了，只要接收端可以收，它就可以解码。这一点很重要，尤其是在丢包网络的情况下，这么做可以避免带宽浪费。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/3b/ca/3b8ab62e8eyy8bc2997fa2d1217d1aca.png\" /></p><p></p><p>智能参考帧的关键是在发布端和接收端维护一样的参考帧关系，即，接收端需要把它的参考帧关系通过编一个很精炼的信息来传递给发布端。通过在 RTCP 加入 ACK（Acknowledgement），接收端告诉发布端它已经收到并完整解码了哪些帧，这些帧可以进入参考帧关系结构，一旦发生弱网，接收端就告诉发布端取消 PLI，不再请求I帧，而是请求 LTR，避免弱网情况下发I帧导致弱网情况更恶化、甚至导致卡死的状况。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/9y/a7/9yye1fee5yy54740121e14f45c9347a7.png\" /></p><p></p><p>智能参考帧技术在一些对卡顿、延时比较敏感的场景中对提升用户体验有很大帮助，比如在抖音好友通话场景中，通过智能参考帧，“大卡”比例下降了 14%（两秒钟不出帧的“大卡”比“小卡”对用户体验的影响要大很多），而在一般的卡顿指标上，200ms 卡顿和 500ms 卡顿分别下降了 2.6% 和 0.7%。另外，智能参考帧在很大程度上也解决了延时的问题。刚才有提到，智能参考帧只要收到就能解码，而不需要通过重传，因此在智能参考帧中大部分重传是可以被关掉的，关掉后延时可以降低 11%，也就是说， 200ms 的传输可以减少 20ms，端到端延时 200ms 的达标率可以提升 16%。</p><p></p><p></p><h2>视频端到端优化技术的未来展望</h2><p></p><p></p><p>以上三个故事就是利用收发联动优化的技术解决视频参数上升、带宽限制下的视频修复、弱网丢包、延时、“大卡”体验的问题，未来我们还可以做哪些优化呢？</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/5d/a8/5d7ebc80c142948b40bb91d8e93e83a8.png\" /></p><p></p><p>在「超分辨率」上，除了告诉接收端它可以开关超分，针对线上不同的机型，我们还可以推荐它使用不同的模型，比如高端一点的机型可以使用复杂一点、强一点的模型，我们可以告诉接收端哪个模型最适合它，哪个模型的“性价比”更高。另外，发布端其实是可以知道接收端有没有超分的能力，如果知道接收端有超分的能力，那么在碰到弱网的时候，发布端可以更进一步地去主动降低分辨率。</p><p></p><p>现在的做法虽然也可以主动降低分辨率，但做得不够激进，因为发布端并不“确定”接收端能不能开启超分。如果发布端“确定”接收端可以开启超分，那么，也许本来是在 200K 的带宽才降低分辨率，现在甚至可以在 300K 的带宽下就降低分辨率——因为降低分辨率后之后再进行画质修复的效果，会比不降分辨率直接传输的效果更好。这就是所谓的 SR-aware 参数选择。</p><p></p><p>在「内容检测」上，我们也可以扩展一下，目前内容检测的结果是非黑即白的，即，内容的分类不是 PPT 就是视频，未来它会向更精细的视频分析方向演进，比如检测视频内容是运动的还是偏静止的，是复杂的运动还是简单的运动（比如是有人在跳舞健身，还是只是一个主播在播新闻）。视频和PPT是两个非常极端的场景，中间频谱上还可以细分很多档位，每一个档位都可以有不同的视频参数以及弱网对抗的策略（比如运动的内容可能需要 60FPS，但是一个静态的主播可能只需要 15FPS）。未来，内容检测可以告诉我们这个视频的复杂程度、或运动的程度是怎么样，而不只是简单告诉我们它是文字还是视频。</p><p></p><p>在「智能参考帧」上，智能参考帧技术和编解码器有强绑定的关系，目前支持智能参考帧的硬件编码器非常少，基本只支持 Nvidia 和 Intel 这类比较常见的硬件，iOS 跟 Android 大部分没有硬件编码器支持，这会把智能参考帧的性能限制在软件编码器中，而用软编实现的方案会限制智能参考帧只能应用在一些分辨率较低的场景。未来，智能参考帧技术还将往移动端硬件方向进一步发展，研究让更多的硬件编码器来支持这个技术，拓展更多高分辨率的应用场景。</p><p>&nbsp;</p><p>未来，RTC的问题会越来越破碎化，特别是当我们进入深水区以后。「上下行联动优化」是一种方式，希望以上这些分享可以帮助大家在一堆混乱的麻绪之中理出一些思路，来思考或解决一些问题。</p>",
    "publish_time": "2023-02-16 13:10:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]