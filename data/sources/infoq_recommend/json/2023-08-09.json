[
  {
    "title": "架构师（2023 年 8月）",
    "url": "https://www.infoq.cn/article/pUvp0Qm1bxS0fm7TRP5v",
    "summary": "<h2>卷首语：我们能从Telegram的开发中学到什么？</h2>\n<p>本期负责编辑  | Tina</p>\n<p>Telegram 由Nikolai和Pavel Durov两兄弟于2013年推出。根据维基百科的统计，截至2023年7月，Telegram的月活跃用户已突破8亿。这一用户规模可与国民软件QQ相媲美。根据2023年第一季度的财报数据，24岁高龄的QQ移动终端月活跃用户为5.97亿。</p>\n<p>虽然规模很大，但Telegram性能非常优异，其系统指标是一众社交软件看齐的对象。同时，始终坚持采用纯原生的方式实现，技术栈简洁干净。此外，自Telegram成立以来的这些年里，其受欢迎程度一直在持续增长，深受开发者推崇：“自从用了Telegram，我才知道某些即时通讯软件有多垃圾。”</p>\n<p>在创办Telegram之前，这两位兄弟曾共同创建了俄罗斯社交网络VK。从技术角度来看，VK同样是一款出色的软件。根据VK的早期员工透露，尽管VK已经成熟，Pavel仍然对产品功能保持着高度控制，并怀有极高的期望：“Pavel对质量设定了极高的标准……不论是代码质量还是最终产品的品质。你必须用尽所能，使用各种方式达到这一标准。”在创办Telegram时，他们持续坚守这一高标准。</p>\n<p>据报道，Pavel在控制方面一直具有独特性，他还引导着公司的愿景。一名员工形容他为“具有远见的人”，能够吸引并团结杰出工程师，以实现共同的目标。尽管Telegram规模超越QQ，但仅由一支小团队组成，由于团队采用扁平化的管理结构，他们负责的产品能够快速推进。</p>\n<p>另外，他对人才的要求也很高，很少有公司像 Telegram 那样拥有如此多的才华横溢的工程师。他们喜欢用“竞赛”来招募人才，比如通过举办“开发者挑战赛”来改进其产品并寻找新的队友。该公司曾举办了一场“GIF 大赛”，数千人参加了这个比赛，Telegram 再从这个人才库中聘用“前两名或前三名”。这是该公司在其整个生命周期中始终保持精简的部分原因。毫不夸张地说，它招聘的工程师是<a href=\"https://www.generalist.com/briefing/telegram\">最顶尖的 0.1%</a>。</p>\n<p>这种对技术和人才的高标准，使得Telegram在经过十年的发展后仍能在功能和技术方面保持高度内聚。或许我们应该庆幸有着Telegram这样的存在，它不仅提升了社交软件开发的标准，还推动了其竞争对手进行改进。</p>\n<h2>目录</h2>\n<p><strong>热点 | Hot</strong></p>\n<p>MySQL 之父：不要把一个优秀的开发者提升为管理者，那会是种资源浪费</p>\n<p>字节跳动开源 KubeAdmiral：基于 K8s 的新一代多集群编排调度引擎</p>\n<p>比 JDK 最高快 170 倍，蚂蚁集团开源高性能多语言序列化框架 Fury</p>\n<p>不到一年，Istio 项目正式从 CNCF 毕业</p>\n<p>第一批因 AIGC 裁掉自家员工的老板该后悔了？</p>\n<p><strong>访谈文章 | Interview</strong></p>\n<p>专访 OpenSSF CTO：安全问题应该考虑在构建模型之前，别出了问题就让 ChatGPT“背锅”</p>\n<p><strong>案例研究 | Case Study</strong></p>\n<p>财报会议新时代：如何将 AI 训练成资深 CFO</p>\n<p>小白大挑战：24 小时内用 ChatGPT 和 Next.js 开发开源项目，吸引上万用户！</p>\n<p>面向大模型的存储加速方案设计和实践</p>\n<p>Cube轻量虚拟化如何做到100ms交付一个安全容器</p>\n<p>如何挖掘 Bazel 的极致性能</p>\n<p>面向故障处理的可观测性体系建设</p>\n<p><strong>推荐文章 | Article</strong></p>\n<p>红帽：我们为什么要改变 RHEL 源码的发布策略？</p>\n<p>LLM 对程序员的冲击和影响</p>\n<p>高薪缺人，但要懂全栈懂 LLM，一个全新职业正在兴起！</p>\n<p>黄东旭：我对数据库如何 Serverless 化的一些思考</p>\n<p>C++ 变化太大！该重新学习这门语言了</p>\n<p>5 天内用户数破亿、增速碾压 ChatGPT，Twitter 劲敌 Threads 是如何构建的？</p>\n<p><strong>特别专题｜实时数仓 Apache Doris 精选实践</strong></p>\n<p>日增百亿数据，查询结果秒出， Apache Doris 在 360 商业化的统一 OLAP 应用实践</p>\n<p>Apache Doris 在叮咚买菜的应用实践</p>\n<p>天眼查基于 Apache Doris 构建统一实时数仓实践</p>\n<p>星云零售信贷基于 Apache Doris 的 OLAP 演进之路</p>\n<p>百亿大表 Join 提速 300 倍！Apache Doris 在约苗数据平台的实时数仓建设实践</p>\n<p><strong>特别专栏 | Video</strong></p>\n<p>本月，这些视频值得一看！</p>",
    "publish_time": "2023-08-09 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "领英采用 Protobuf 进行微服务集成，将延迟降低了60%",
    "url": "https://www.infoq.cn/article/ZQEP69byw99C818CzmF6",
    "summary": "<p>领英<a href=\"https://engineering.linkedin.com/blog/2023/linkedin-integrates-protocol-buffers-with-rest-li-for-improved-m\">采用 Protobuf，以实现其各类平台中更为高效的微服务间数据传递</a>\"，并将其与开源框架&nbsp;<a href=\"https://linkedin.github.io/rest.li/\">Rest.li</a>\"&nbsp;相集成。在全公司范围的推广完成后，领英将延迟降低了60%的同时，也提高了资源的利用率。</p><p></p><p>领英平台所采用的是微服务架构，而多年以来，<a href=\"https://en.wikipedia.org/wiki/JSON\">JSON</a>\"&nbsp;一直都是领英在微服务暴露的五万余 API 节点中所使用的序列化格式。为帮助团队在服务间构建一致性交互，领英创建并开源了一款名为 Rest.li 的 Java 框架。</p><p></p><p>该框架可用于创建使用&nbsp;<a href=\"https://en.wikipedia.org/wiki/Representational_state_transfer\">REST</a>\"&nbsp;通信风格的服务器和客户端，并抽象网络、序列化、服务发现等数据交换的诸多方面。Rest.li 框架主要支持 Java 和 Python，但也可与 Scala、Kotlin、JavaScript、Go 等语言协同运作。</p><p></p><p><img src=\"https://imgopt.infoq.com//news/2023/07/linkedin-protocol-buffers-restli/en/resources/1RestLiClientServerFlow-1689612288438.jpeg\" /></p><p>Rest.li 服务器和客户端之间的数据流和控制流（来源：<a href=\"https://linkedin.github.io/rest.li/user_guide/server_architecture\">Rest.li 文档</a>\"）</p><p></p><p>Rest.li 的默认序列化格式为 JSON，这种格式支持多款语言且易于人类阅读，后者虽然好处甚多，但却给性能（尤其是延迟）方面带来了许多问题。</p><p></p><p>领英工程师&nbsp;<a href=\"https://www.linkedin.com/in/karthikrg/\">Karthik Ramgopal</a>\"&nbsp;和&nbsp;<a href=\"https://www.linkedin.com/in/aman1309/\">Aman Gupta</a>\"&nbsp;分享了在使用 JSON 进行服务间通信所要面临的挑战：</p><p></p><blockquote>第一个挑战在于，JSON 作为一款文本格式往往过于冗长，从而导致网络带宽的使用和延迟增加，效果并不理想。（……）我们所面临的第二个挑战则在于，JSON 的文本性质会导致序列化和反序列化的延迟和吞吐量均不甚理想。</blockquote><p></p><p></p><p>领英团队一直在寻求 JSON 的替代方案，一款负载大小紧凑、系列化效率高，可减少延迟并提升吞吐量的方案。他们同时也希望这款方案不会限制所支持的语言栈数量，并能通过将这个新的序列化机制集成至 Rest.li 从而实现逐步迁移。最后，经过全面的思考，领英决定采用在各项考量中综合得分最高的<a href=\"https://protobuf.dev/\">Protobuf</a>\"。</p><p></p><p>将Protobuf集成到 Rest.li 中的主要困难在于&nbsp;<a href=\"https://linkedin.github.io/rest.li/pdl_schema\">PDL</a>\"，一个基于框架的自定义模式定义系统的动态模式生成。这套解决方案中需生成一个用于动态生成 Protobuf 模式定义的符号表，但根据客户端类型的不同，符号表的交付方式也会有所不同。后端客户端按需获取并缓存符号表，而网页或移动端应用的符号表则在构建时生成，且其中包含版本号依赖关系。</p><p></p><p>在对框架进行修改之后，领英团队通过 HTTP 头逐步对客户端进行重新配置，以 Protobuf 替代 JSON。采用 Protobuf 后，响应的吞吐量平均提高了 6.25%，请求的吞吐量平均提高了 1.77%。领英团队同样发现对大型负载而言，延迟降低了 60%。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/en/resources/1linkedin-restli-protobuf-1689612288438.jpeg\" /></p><p>JSON 和 Protobuf 的延迟比较（来源：<a href=\"https://engineering.linkedin.com/blog/2023/linkedin-integrates-protocol-buffers-with-rest-li-for-improved-m\">领英将 Protobuf 与 Rest.li 集成以提高微服务性能</a>\"）</p><p></p><p>根据对 Protobuf 的采用所得来的经验，领英团队计划后续将 Rest.li 迁移至&nbsp;<a href=\"https://grpc.io/\">gRPC</a>\"。gRPC 同样使用 Protobuf，并额外支持流式传输，其背后还有一个庞大社区的支持。</p><p></p><p>具体请见 InfoQ 博客：<a href=\"https://www.infoq.com/podcasts/api-showdown-rest-graphql-grpc/\">API 间的对决：REST vs. GraphQL vs. gRPC：该用哪一种？</a>\"</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/\">https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/HcNBNAE4M3dI41AhZbSC\">LinkedIn图数据库LIquid：为9.3亿会员提供实时数据访问</a>\"</p><p><a href=\"https://www.infoq.cn/article/5CGARLHYyr6ZL32hXxMq\">微软发言人证实旗下LinkedIn平台开始裁员</a>\"</p>",
    "publish_time": "2023-08-09 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "【实战篇】通过 SoFlu 构建一个低代码平台实现过程之“应用管理”",
    "url": "https://www.infoq.cn/article/jep62hkJKNBh41t48uaq",
    "summary": "<p>在现代企业开发中，初级开发者难以掌握开发企业级 Java 应用，中高级开发者则被繁琐任务所占据。为了让大家能快速上手可以解决这个难题的“SoFlu 软件机器人”，前蚂蚁金服技术专家杨彪设计了《2 小时学会 10 个组件“取代”传统 Java 开发》课程，本节为该系列课程的第十讲，也是本系列课程的最后一讲。</p>\n<p>自第八讲起，我们进入到了实战教学篇，应用“SoFlu 软件机器人”进行一个低代码平台的开发，上一讲我们讲到了了低代码平台的“表单剖析”，这一讲杨彪将再为大家来讲一下《实现过程之“应用管理”剖析：导入导出》。通过本节课程，你可以了解应用管理功能的使用情况，以及导入导出功能背后的实现流程。</p>\n<p>大家在课后可以登录 Gitee 下载 SoFlu 软件机器人客户端进行实践：<a href=\"http://suo.im/8wROo\">点击下载</a></p>\n<p>大家可以扫码添加小助手，进学习群与专家一对一交流：<br />\n<img alt=\"\" src=\"https://static001.infoq.cn/resource/image/e8/c9/e8833a01ba0bc705acab14a572b57cc9.png\" /></p>",
    "publish_time": "2023-08-09 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全球优秀的架构师都在关注什么？",
    "url": "https://www.infoq.cn/article/ag6khcalFU9bO2o3Ag8R",
    "summary": "<p>随着云计算、大数据、移动化、<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247490280&amp;idx=3&amp;sn=555602ee062f2055e8039a6289ed9cd7&amp;chksm=fbe9a327cc9e2a31ca257bd26d377944080205ce1c81ba8c3714add4012efa5d07b1819982b3&amp;scene=27#wechat_redirect\">人工智能</a>\"等新技术的推动，企业架构也在不断向着更优化的方向演变。企业架构是一套庞大复杂的体系，涉及到数据如何流转，系统之间如何关联，底层数据如何打通等问题。企业需要一个“技术实现者”，那就是“<a href=\"https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng==&amp;mid=2247489786&amp;idx=1&amp;sn=bdfb35af89489e1f5eb3fe805f5a50c1&amp;chksm=eba41b67dcd39271c41134683fedca891b026968536bdf01d06136dc1213f3974ddb027415b7&amp;scene=27#wechat_redirect\">架构师</a>\"”。</p><p></p><p>在数智化时代，企业要思考如何把核心竞争力和新一代数智化技术相结合，加速数智化进程，重新形成在数字经济时代的创新能力，包括快速响应市场变化的产品和业务创新、精细化的企业运营和管理创新，以及组织变革和组织能力建设层面的创新。</p><p></p><p>此时，优秀的CTO和架构师要懂得在合理架构设计和灵活多变的业务发展之间做出智慧的权衡。可以说，<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650996345&amp;idx=2&amp;sn=a6837cff67172d916b4bdeefb0ca2af4&amp;chksm=bdbf062a8ac88f3c8516874d0eac9bc9ff9a257a964e8f85f7c269d69f9bf3ede946c4909ad0&amp;scene=27#wechat_redirect\">CTO</a>\"和架构师的高度和能力决定了企业数智化的未来。</p><p></p><p>那么，业界优秀的架构师都在关注哪些先进的技术与架构？领先的企业如何构建数智化系统？如何利用数智底座提升底层技术与架构能力？前不久，用友在ArchSummit全球架构师峰会深圳站与一些优秀的架构师深入交流、调研，以上问题在这里可以找到一些答案。</p><p></p><h2>构建数智底座，关注快速开发、数据价值与内外链接</h2><p></p><p></p><p>在数智化时代，企业的IT架构需要以平台思维，构建能够实现数据打通和IT资产沉淀的体系，为企业提供平台化的技术能力，并以规划作为指引，在指引下统一数据治理，作为统一数智底座。</p><p></p><p>当前，一些领先企业的数智化，已经进入2.0阶段。从局部应用场景创新、数据相对分散、只有部分平台能力的1.0阶段，进入到围绕生产、经营及管理主题的融合化、智能化应用创新；重视和系统推进数据治理；全面升级数智底座的2.0阶段。</p><p></p><p>架构师们认为，数智化底座在企业中承担重要作用。调研对象中，超过50%架构师认为数智底座在企业中的重要性在于：利用平台技术快速开发应用；打通系统壁垒与业务全链接；实现内外部连接，助力产业链上下游；提升数据智能应用能力，发挥数据价值。其次，提升中台化等架构能力，助力业务创新；提升整体团队工程化开发能力；拥有自主安全可靠的平台等也是大家关注的维度。</p><p><img src=\"https://static001.geekbang.org/infoq/55/552bce99f8cf93aa594a81764161e100.png\" /></p><p></p><h2>云原生、中台架构最受关注</h2><p></p><p></p><p>数智底座的技术与架构决定了上层应用系统的开发方式、实现方式、扩展方式等。</p><p></p><p>过去，企业的IT基础架构被视为成本中心和一种必要的开销，但现在，拥抱数智化的组织愈发清晰地认识到，真正的数智底座、良好的数字化基础架构是企业长期取得成功的战略基础。因此，大多数组织都在对IT基础架构进行现代化改造或建设新的数智底座，他们开始确定和部署适当的数据中心、边缘平台和应用系统，以用作数智化升级的基础，而且高度关注现代化的数智底座对云原生应用及传统应用的支持。</p><p></p><p>调研发现，架构师对数智平台的架构或技术，最关注的是云原生架构，高达74.10%；其次为中台架构，占到49.64%；另外，多云异构、人工智能技术、多维数据库、国产化适配等也受到不同程度关注。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/5393ab263c123558bab8f9ebb0d569ae.png\" /></p><p></p><p>可以看出，如何利用云原生、中台化等，确保平台架构能够满足业务发展的各种要求，同时还能够保证操作的便捷性，且不能以牺牲自主控制力和企业的安全性为代价，已经成为企业对数智平台的基本需求。</p><p></p><h2>人工智能代替人工、数据驱动业务增长</h2><p></p><p></p><p>在过去的几十年中，人工智能技术取得了巨大的发展，包括机器学习、深度学习和自然语言处理等方面的突破。如斤，AI热潮席卷全球，标志着智能技术发展已经成熟，AI普及应用时代到来。</p><p></p><p>那么，企业的架构师们都希望通过人工智能技术解决哪些问题呢？投票率最高的是，通过智能机器人或数智员工替代一部分人工；其次是希望提升数据挖掘与预测能力，以数据价值驱动业务增长。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/4266435796ecf69349f4c50c69985193.png\" /></p><p>随着人工智能技术不断升级，基于大模型的生成式AI成为人工智能技术和应用的最新发展潮流。近日，用友发布业界首个企业服务大模型YonGPT，它是用友基于数字和智能技术服务企业和公共组织数智化的最新研发成果。YonGPT的发布标志着中国企业软件的创新迈入以人工智能为中心的新阶段。目前，YonGPT已经创新研发了包括企业经营洞察、智能订单生成、供应商风控、动态库存优化、智能人才发现、智能招聘、智能预算分析、智能商旅费控、代码生成等在内的数十种基于企业服务大模型赋能的智能应用。同时，YonGPT面向复杂的行业应用场景，还可以通过对行业模型精调，提供更加“在行”的智能化场景服务。</p><p></p><p></p><h2>影响力大、技术先进、懂业务</h2><p></p><p></p><p>通过升级数智化底座，企业得以实现理念升级和架构升级，形成能力聚合效应，进一步释放了数智化的潜能。但是，并不是所有企业都具备自主开发数智底座的能力，他们需要借助第三方平台，来持续保持技术架构的先进性，能够不断地融入新技术，实现自我迭代。</p><p></p><p>在调研中发现，55.39%的企业在选择第三方数智平台时，会优先选择品牌影响力的产品；其次，更关注平台是否具备先进的技术与架构能力，这个维度也超过了一半，占到51.89%；另外，懂企业的业务逻辑也是备受关注的点，占到38.12%。</p><p><img src=\"https://static001.geekbang.org/infoq/52/52ba599a961a7122147e46b0a592cb9c.png\" /></p><p></p><p></p><h2>用友iuap更懂业务、技术领先、体系完整，助力企业升级数智化底座</h2><p></p><p></p><p>从整体架构的视角看，数智底座是企业数智化转型最为关键的基础设施，是一切数智化创新的源动力。</p><p>用友作为全球领先的企业云服务与软件提供商，一直坚持产品的自主研发和设计。用友iuap更懂业务、技术领先、体系完整，助力企业升级数智化底座，帮助企业实现技术、数据、业务的深度融合，迈向世界一流。</p><p></p><p>用友iuap平台实现了多项技术的领先突破，如：首创YMS云中间件技术，实现跨云技术突破、多云适配能力；首创云上云下一体的持续交付体系，让企业私有云平台体验到公有云的更新效率，让云下应用升级像AppStore一样简单；领先的多租户、多数据中心技术，多租户部署在多云异构的多数据中心，云上管理，云下运行，实现公有云高可靠、专属云新模式；自研基于内存计算技术的多维数据引擎（存算一体）等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19f68bdc93720f64d1023144da728609.png\" /></p><p></p><p>用友iuap平台以企业业务为导向，实现了多项应用架构的领先创新，包括支撑业务变化和精准运营的特征体系、支撑产业链及价值网业务协同的社会化商业组织与数据模型、重构企业精细管理的事项会计中台、支持组织柔性与灵动发展的多维组织模型、支撑企业实时管控与精益运营的PDCA循环、业务导向随需应用的场景化应用组装等，使能企业随需而变、柔性扩展、韧性成长。</p><p></p><p>用友iuap包括“三中台+三平台”，有业务中台、数据中台、智能中台和技术平台、低代码开发平台、连接集成平台，形成了完整的企业数智化底座平台体系，并提供数智化工程、可持续运营两大体系，助力企业全面升级数智化底座。</p><p></p><p>这些业界领先的架构师所关注的数智化底座技术、架构、价值等方向，也正是当前不同行业领先企业对数智化底座的需求缩影。用友iuap关注企业数智化能力提升，通过帮助企业升级数智化底座，加速数智化转型，迈向商业创新。</p>",
    "publish_time": "2023-08-09 10:39:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "HA3 SQL样本实验：一种混合计算查询的全新样本解决方案",
    "url": "https://www.infoq.cn/article/f2fb26818e5f5fc7db103d804",
    "summary": "<p></p><p></p><p></p><blockquote>HA3（对外开源代号：Havenask ）是阿里智能引擎团队自研的大规模分布式检索系统，广泛应用于阿里内部的搜索业务，是十多年来阿里在电商领域积累下来的核心竞争力产品。Ha3 SQL 是在原有Ha3引擎基础上，新增的SQL查询功能，引擎内置了SQL形式的的查询语法，允许用户通过写SQL语句来构造引擎查询。</blockquote><p></p><p></p><h1>一、概述</h1><p></p><p></p><p>在搜推广这个业务场景内，样本构建是特征数据链路的一个重要环节，而且从工作流程上来看，做样本也通常是算法同学开始一个新实验的第一步。在这个环节的技术选型上，考虑到功能性、稳定性和上游数据的所在，大部分团队最终的选择都是MaxCompute。MaxCompute是一流的离线数据处理平台，但是在和算法同学的沟通交流中，我们频繁听到这些使用上的困扰：</p><p></p><p>样本存储占用了太多的MaxCompute空间；样本构建任务会消耗非常多的计算资源，集群常年被样本任务耗尽资源；新增特征、调整样本分布等实验的构建样本周期很长，时间成本高。</p><p></p><p>我们分析了这些困扰背后的本质原因，针对这里的关键点提供了一套新的样本解决方案：HA3 SQL样本实验功能。</p><p></p><p>通过在『构建样本-训练』这个流程中插入一个由HA3 SQL提供的查询层，非常精准地解决了上面列举的几个主要困扰点。目前已有多个算法团队接入这套样本解决方案并从中获得数据链路的全方面优化。</p><p></p><p>在本文中，我们会给出关于样本构建问题的分析与思考，介绍HA3 SQL样本实验的方案架构和里面值得和大家分享的一些技术细节，同时还会给出一些已经接入场景的性能报告和接入前后的成本对比。最后是我们在这个功能研发过程中的一些经验教训和未来工作展望。</p><p></p><p></p><h1>二、样本场景问题介绍与分析</h1><p></p><p></p><h2>2.1&nbsp;样本的现状</h2><p></p><p></p><p>先简单介绍一下，在搜推广这个场景，样本通常长什么样，是怎么构建出来的。以最常见的CTR样本（曝光-点击样本）为例，参考谷歌WDL论文中的例子，一条典型的样本记录是这个样子的：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a633a12b8f801506029bb46328731c4.png\" /></p><p></p><p>pv_id是某次请求对应的uuid，这条样本表达的含义就是：某次请求返回的app被用户在手机上点击了，在点击发生的当时，这个app的名字、下载量等属性是某某某，这个用户的年龄、性别等属性是某某某。一份样本就由上亿条这样的记录构成，这个数据会被训练时的机器学习模型消费，用于学习点击这个行为和各个特征之间的相关性。</p><p></p><p>这种样本常见的构建方式是这样的：pv_id app_id user_id click这些属性来源于用户行为埋点日志，记录了用户的行为发生时的一些关键id，算法同学会建立一系列上下游任务把这部分数据导入到MaxCompute上变成一张表示用户行为的分区表，同时还会准备若干以pv_id/app_id/user_id之类的字段为主键的属性表，通常称之为维表，最后以用户行为这张表作为起始，用上面的各个id把这些属性表join起来，最后再按训练平台的要求做一些格式上的处理，就生成了最终的样本表。</p><p></p><p>以SQL伪代码表示，关键步骤基本就是这样一段逻辑 ：</p><p></p><p><code lang=\"hljs cobol\">select pv_id, app_id, user_id, timestamp, click, download\nfrom clean_behaviour_table\nleft outer join app_profile_table\non clean_behaviour_table.app_id = app_profile_table.app_id\nleft outer join user_profile_table\non clean_behaviour_table.user_id = user_profile_table.user_id</code></p><p></p><p>目前大部分的训练平台对于MaxCompute样本的支持基本都局限于单个MaxCompute表的遍历读取，所以算法同学需要把上面的这段sql在MaxCompute上执行一边，把结果写入到另外一张结果表里作为最终的样本。</p><p></p><p>与此同时，算法同学平时在样本方面的实验迭代主要是在行列两个维度进行的：</p><p></p><p>行级别的迭代实验是说在串起样本的行为流上做实验变更，比如变更负例的采样比例、筛选不同场景的样本；列级别的迭代实验主要是新增各类不同维度的特征，比如多一张app的属性表或者在原有表上加一些字段。</p><p></p><p>并且因为上面讲到的训练平台的限制，算法基本需要为每一个这样的实验加一个类似上面伪代码的MaxCompute任务，另外产出一份样本表。</p><p></p><p>这套生产流程对应到公开业界概念的话，主要是feature store系统里面的离线批数据处理流程，MaxCompute对应的通常是Hive/Spark这样的系统，有兴趣了解的可以自行查阅。</p><p></p><p></p><h2>2.2&nbsp;主要矛盾分析</h2><p></p><p></p><p>从上面描述的现状里面，可以提炼出样本这类任务的两个关键属性：</p><p></p><p>样本表从构建逻辑上看，是一个非常典型的星状模型，行为流表就是事实表，其它表是维表，同时因为训练平台的限制，样本被要求对这个星状模型做了完整的物化展开；样本实验具体到构建逻辑其实就是在事实表和维表上的增减变化，和完整的构建SQL比，每次实验的变更通常局限在一两张表上。</p><p></p><p>依次来看，在阿里电商领域这个场景里，星状模型完全展开这个点带来了三个方面的影响：</p><p></p><p>单个样本任务的复杂、消耗资源大。因为在集团的很多场景，一天下来的样本条数是上亿甚至几十亿的，同时各个维表的维度也不小，商品表本来就是亿级维度的，用户-商品、用户-类目之类的交叉特征表更是能上千亿。样本任务通常至少会join七八张这样的表，这种级别的任务对于MaxCompute这样的离线计算平台来说，最基础的做法就是一轮轮的sort merge join，做完一个join，再根据下面一个join的key重新shuffle，这个是极其消耗资源的，而且对于规模到一定程度的场景，这个默认实现甚至根本跑不出来，需要用hash clutser、distributed mapjoin之类的方法来做优化。甚至是在MaxCompute之上另外再引入一层kv性质的外部服务，来加速这个join，过去几年里面内部的算法平台做过类似原理的工作。存储冗余：对于部分维表来说，因为被行为表这个大表join了，维表中的一行可能在最终样本表中重复几十万次，你可以想象一下像iphone这种热门商品会在多少样本的目标商品和行为序列中存在，而又有多少iphone的商品侧特征完全被冗余存储了。我们曾经统计过，在主搜的样本中，各种行为序列的商品属性占了数据空间的2/3。完整的物化展开最终带来的结果就是头部场景随便一份ttl是30天的样本就能直接占掉几个PB的MaxCompute存储空间。增改数据需要重刷整张表：因为MaxCompute本身没有修改已有分区表某些列的功能，如果需要订正数据或者加一些特征，唯一的选择是重新跑一遍，在这个过程中，其它无关字段也会被读取、再完整写入一遍。而算法的实验需求又意味着，上面描述的巨量计算和存储消耗，要直接再上一个数量级了，想要做多少个实验，就要把完整地再跑多少个样本任务，虽然这些任务里面可能两两之间只是多join少join了一个表的区别。理解了这两点，就能充分明白为什么样本任务堪称MaxCompute的资源杀手。</p><p></p><p></p><h2>2.3&nbsp;解决方案</h2><p></p><p>从场景抽离出来，如果我们用数据库领域的概念来描述一下样本的问题，那MaxCompute样本的问题本质可以抽象成：多个复杂但相似度很高的星状模型视图被强制物化展开带来的计算、存储资源问题。</p><p></p><p>抽象成这个问题后我们的解决思路也就非常明确了，在数据处理层和训练层之间再加入一层查询层，把部分计算逻辑延迟到训练时在查询层进行，让实验样本间逻辑上重复的部分变成查询阶段的重复查询，计算层只对部分适合计算平台处理的逻辑和真正增量的数据进行处理，这样可以在资源和效率两个方面逼近最优。</p><p></p><p>可以思考两个行列实验的典型场景：调整负样本采样比例和新增特征表。</p><p></p><p>调整负样本是指对样本中的负例进行采样，因为有些场景的负例可能太多了对训练模型没有帮助。在之前的模式里面，这种实验只能通过产出多份事实表，再构建多份样本表来进行。</p><p></p><p>如果系统具备sql化查询的能力，我们可以只保留一张有完整负例的事实表，通过查询sql中的条件语句直接过滤掉一些负例，过滤后的记录再去join各种维表，这样做不同比例的实验只是调整一个sql参数的事情，不需要反复生成样本，直接起多个配置不同的训练任务即可。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5c/5cb23e17dace37911116d071b02d7fe2.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8ed01af4c150961466d8372c4eec8a97.png\" /></p><p></p><p>新增特征表在传统模式下也是必须新生成样本表，这意味着整个样本表数据的重写，而在有sql化查询能力的系统里面，如同在星状模型在传统数据库里的处理方式一样，我们只需要把这个新特征表的数据准备好，就能在查询时直接join上，完全不需要对之前的事实表和维表进行变更。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/045fb94abd6f4a1759938da80719f34c.png\" /></p><p></p><p>﻿</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d8fb0eb9bf0f167da311d11ccc9f437.png\" /></p><p></p><p>﻿我们把这整个系统称为混合计算查询系统（Hybrid Computation Query System），区别于之前完全通过计算层生成样本的系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ce33ef495471fe8845e8b7875ad31225.png\" /></p><p></p><h1>三、系统架构</h1><p></p><p></p><h2>3.1&nbsp;查询层技术选型</h2><p></p><p>确定了思路后，还需要进行技术选型，从样本这个场景来看，我们需要一个怎样的查询层呢？我们总结下来主要是以下几个关键点：</p><p></p><p>支持全功能的SQL：如上所说，样本本身完整的构建过程是适合用一段或多段SQL化的逻辑来描述的，传统的MaxCompute批样本构建本身就是用具体的MaxCompute SQL来描述了样本的构建逻辑，查询层同样支持全功能的SQL会有利于我们灵活地将适合的计算转移到查询层进行，同时也能让这个过程或者说内部逻辑更容易被用户（算法同学）所理解。</p><p></p><p>健壮的索引能力：想要把join转移到查询阶段来做的话，维表是一定需要预先建好索引的，查询层一定要有健壮、稳定的索引能力，并且有成熟、快速的构建流程。</p><p></p><p>对读写超高吞吐的支持，毫秒级别延迟：样本在训练读取过程中会实打实地scan完整的事实表，再join多个维表。而且不像olap性质的任务会后接聚合函数，训练是需要一整行的所有数据的，这里面的整体吞吐量在大规模的场景甚至会到几十GB/s这个量级。同时训练端本身的数据队列通常比较小，为了不阻塞训练，单次查询的latency也需要保持在毫秒级别。</p><p></p><p>原生的计算存储分离：无论样本的逻辑构建过程最终被拆成了哪些物理表和对应的组合逻辑，多份样本及每份样本多天的存储是不可能在查询集群的内存或者本地磁盘内放下的，客观上就要求查询系统能原生支持计算存储分离。与之同时产生的隐含需求是查询层需要有丰富灵活的cache机制来降低对存储层的压力并满足训练端的吞吐要求。</p><p></p><p>实时数据的支持：</p><p></p><p>一方面来说，odl（online deep learning）样本在样本实验上遇到的问题本质上和批样本是一致的，只不过一来odl样本不会保存很长时间，存储的问题不会很大，二来很长一段时间里面odl样本的搭建难度明显还是更高的，很多场景无力维护实验样本。而如果查询层本身支持实时数据源作为一种表的源，其实odl样本也是完全可以进行各种行列实验的。另一方面，特征维表其实是有很强的实时性需求的，很多特征是和事件发生的时间相关的，之前只是因为MaxCompute上没法做这种维表，所以一直通过近似模拟或者埋点的方式来生成这部分特征，而如果查询层本身支持类似时序数据库的数据存储和读取，这部分特征其实是完全可以在离线生成的。</p><p></p><p>经过对查询层需求的分析和各种实测调研，最终我们决定用ha3 sql来做为我们样本实验方案的查询层。</p><p></p><p>ha3 sql的底层组件从支撑集团搜索的ha3引擎（havenask，已开源）发展而来，其核心组件indexlib上对上面列举的后四个关注点能很好地支持而且久经考验。而ha3 sql本身则是将传统的query表达式替换成了自定义的ha3 sql方言，样本场景的任务核心难点在于规模和多表连续join这个性质，构建逻辑本身所需的sql表达不复杂，目前的ha3 sql完全可以胜任。</p><p></p><p>同时ha3 sql也是AIOS体系内的项目，我们对其有完全的掌控能力，无论是问题的排查还是功能的扩展，都可以做到逻辑或者意愿的极限。</p><p></p><h2>3.2&nbsp;产品功能形态</h2><p></p><p>查询层的runtime选型确定还没有完，关于如何把这个思路变成真正可用的产品功能，我们还需要回答一系列问题：</p><p></p><p>该如何描述样本的构建逻辑？用户在哪里描述这个逻辑？在何时，如何将样本构建所依赖的MaxCompute表导入到ha3系统？训练平台如何和这个新的查询层对接？</p><p></p><p>这块我们的整体思路是：用一套和上面的样本构建伪代码比较接近的DSL来让用户描述样本逻辑，通过这段DSL，我们既能获得依赖的MaxCompute表的信息，从而能创建周期性的ETL任务来导入ha3，同时我们也能根据DSL去生成最终训练查询样本时所需要的那段ha3 sql。</p><p></p><p>DSL部分我们最终的技术选型是RTP Table Api（一套之前用于构建在线特征的python sdk，接口设计主要参考了flink的Table Api，下文沿用Table Api的名字），具体例子和技术细节会在下一个章节详细阐述。从功能流程上看，整个系统的流程架构如下：</p><p></p><p>﻿</p><p></p><h1>四、技术细节&amp;优化工作</h1><p></p><p></p><h2>4.1&nbsp;DSL模块</h2><p></p><p></p><p>关于DSL本身的设计，我们选择了对Table Api做了一些扩展来作为样本构建得DSL，描述出来的样本构建逻辑大致如下：</p><p></p><p><code lang=\"hljs cobol\">from turing_script.sdk.itable import ITable as Table\nfrom turing_script.biz_plugin.default_modules.base_sample_module import BaseSampleModule\n\nclass SampleDemo(BaseSampleModule):\n    def build(self, table_env) -&gt; Table:\n        ##获取基础样本表\n        base_sample = table_env.get_table('odps_project.base_sample')\n\n        #需要join的维表，指明pk_name和需要的字段\n        video_rtp_feature = table_env.get_table(\"odps_project.video_rtp_feature\", pk_name = \"video_id\") \\\n        .select(\"video_id, gmv_score, video_pageview\")\n        #需要join的维表可支持多张\n        author_feature = table_env.get_table(\"odps_project.mainse_vs_author_feature\", pk_name = \"author_id\")\n\n        #三张表按条件join，字段类型需一致，如字段名相同，可用_right_前缀来区分\n        sample_exp = base_sample.left_outer_join(video_rtp_feature, 'int64(video_id)=_right_video_id') \\\n        .left_outer_join(author_feature, 'video_authorid=author_id') \\\n        .filter(\"reserverd_rand_key &gt; 0.5\")\n        return sample_exp</code></p><p></p><p>可以看到上面的样例代码本身也是一个和sql可以近似映射的过程，事实上Table Api和ha3 sql算是同根生的关系，底层复用了很多组件，比如iquan、calcite。而之所以选择基于Table Api改造来作为样本实验本身的DSL，主要是出于下面的考虑：</p><p></p><p>构建样本和在线RTP serving时构建输入其实是特征的一体两面，长期来看我们的目标是统一在线和离线的特征构建表达，所以我们倾向于用和RTP serving阶段构建相同的DSL。因为涉及到调度、导入等逻辑，样本实验的DSL其实相当于融合了一部分DDL和DML的逻辑，如果通过ha3 sql来描述需要扩展部分sql语法，这个工作量会显著增大，而且有些逻辑和关系代数的映射并不直接，会影响我们在探索阶段的开发效率，也可能会给ha3 sql加上一些不必要的包袱。相对来说sdk式的接口拓展sql之外的功能和做接口兼容都会更容易一些，这一点在上面介绍的get_table接口性质里面体现地最为明显。</p><p></p><p>回到DSL本身，核心接口主要是两部分：</p><p></p><p>get_table是对原Table Api中scanregister_table两个接口的替换，和在线纯粹的查询不同，在样本场景我们需要让用户另外提供一些索引、配置之类的DDL性质信息，我们选择再这个接口中通过pk_name之类的参数来指定；select filter join等常用的sql语义接口，这部分接口的含义和语法与原Table Api中完全一致，基本可以覆盖构建样本所需的sql动作。</p><p></p><p>在接口之下，我们实现了下列功能：</p><p></p><p>依赖表的分析推导，生成对应的ha3索引表配置；自动去除冗余字段，缺失字段提示，得益于共享了iquan、calcite这样的基础组件，这部分能力基本是直接从底层享受到的；查询用的ha3 sql自动生成。</p><p></p><h2>4.2&nbsp;索引表设计</h2><p></p><p></p><p>另一个需要小心设计的地方是各个MaxCompute表对应的查询系统索引表配置，ha3底层的索引系统indexlib提供了非常多的索引类型和很多细致的参数配置——甚至可以说有点过于细致了，所以这层概念不宜直接暴露给用户，我们需要针对样本场景的需求，寻找合适的索引表配置，并将其与用户可以理解的DSL层面的一些概念挂钩。</p><p></p><h3>✪&nbsp;4.2.1 事实表</h3><p></p><p></p><p>样本场景的事实表一般有两种典型的例子，一种就是纯粹的行为流表，上面只有一些外键id和行为label，字段比较少；另外一种就是其它已经构建好的样本，用户期望基于这个基础样本另外join一些特征来做实验，这类表除了前一种的所有字段外，还包含大量已经生成好的特征，字段比较多，表的规模也比较大。</p><p></p><p>同时事实表在样本场景的使用方式就是全表遍历，不同实验里面可能会分别只需要一部分字段，本身并不会被其他表join。这个特性结合了上面样本作为事实表时数据规模较大的特点决定了事实表适合用列存表的形式来存储。一方面可以按列读取减少大量的seek，另一方面列存格式有利于压缩，可以降低样本类事实表的存储大小。所以最终我们选择把事实表配置成一张AliOrc格式的表，这也是通过indexlib支持的。</p><p></p><p></p><h3>✪&nbsp;4.2.2&nbsp;维表</h3><p></p><p></p><p>维表的需求相对更加明确，样本场景的维表一定是索引即主键的，否则同样一条记录就join出多个结果了。维表是被join的角色，需要有索引提供快速检索能力，所以适合带索引的行存表来存储。又因为在样本场景里面，维表基本都是单字段或者双字段主键。所以我们选用了indexlib的kv和kkv索引表作为维表的存储格式。分别提供了按单主键和双主键去重和检索的功能。</p><p></p><h2>4.3&nbsp;调度系统</h2><p></p><p></p><p>每天样本的导入流程涉及到多个阶段的动作，首先是当MaxCompute上对应的源表产出后，需要启动一个任务做ETL性质的工作，然后等待后续的ha3表构建、集群切换等流程的完成，而只有当一份样本依赖的所有表都已导入以后，我们才能将完整查询的ha3 sql写入meta系统。在表多的时候这个依赖流程还是比较复杂的，需要一个调度系统来辅助完成。</p><p></p><p>由于MaxCompute本身的任务调度是在DataWorks系统上的，同时算法同学对DataWorks的工作流程和补数据等操作都比较熟悉，所以我们决定把这部分调度依然基于DataWorks来实现。</p><p></p><p>我们拆分了三种细粒度的调度任务：sink、wait、commit。</p><p></p><p>sink节点依赖MaxCompute的源表产出节点调度，启动数据导出任务；wait节点依赖sink节点调度，等待build和切换完成，sink和wait之所以拆分成两个节点，是让wait节点在超时、异常等情况下可以单独重跑，避免重复导入数据；commit依赖该样本任务所有表的wait节点，等所有表都切换完成后，重新执行一遍DSL逻辑，替换其中的表名为当天具体产出的分区表名，产出完整查询用sql写入meta系统。</p><p></p><p>我们接入了DataWorks的OpenApi系统，通过接口直接往各个团队的project里面注册了上面这些节点，用户只需要用上面的DSL描述好样本构建过程后这些节点都会自动创建出来。</p><p></p><p>一个复杂一点的样本调度实例如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e1dd90533f612117465f0b0cd421c0ba.png\" /></p><p></p><p>﻿</p><p></p><h2>4.4&nbsp;集群部署和管理</h2><p></p><p></p><p>对外我们给用户暴露的是类似数据库的DB概念，同一个DB内表都是可见的，如果多个样本依赖了相同的表，这部分表不会重复创建导入，而是会共用。这个也为后续潜在的跨团队共享特征、label等行为预留了空间。</p><p></p><p>目前我们搭建了全国不同地区机房的公共集群，根据算法团队project的所属机房来决定使用哪个公共集群。在导入链路中，MaxCompute集群、中转swift集群（类似kafka）、build集群、盘古集群是被保证在同一个地区的，避免出现跨地区长传带宽，而查询链路中，默认配置下训练集群会和DB集群、盘古集群完全在同一个机房，可以完全规避同城跨机房带宽，从而降低成本。</p><p></p><p>这部分整体上对用户都是完全透明的，用户只需要最开始根据自己MaxCompute机房的所在选择一下合适的DB即可。</p><p></p><p>但是在我们目前的集群管理运维工作中，这种模式目前也已发现了一些问题：</p><p></p><p>业务隔离性：因为共用了集群，一方面业务团队之间就有互相影响的风险，比如某天一个团队导入过多的表就可能会卡住其它团队的表切换，或者挤占资源。另一方面这个对我们的服务计费造成了很大的困扰，说不清一个集群的资源各个业务分别用了多少。</p><p></p><p>单集群规模过大：由于混用集群，而且各个团队MaxCompute的机房分布也是不均匀的，部分集群上面表的规模已经偏大，资源没法跟着业务需要线性扩展是一个问题，更大的问题在于因为我们基于的ha3、运维平台这套体系总体上之前还是围绕在线集群设计的，不会存在像样本这个场景单个集群成百上千张表的情况，所以我们在运维过程中触碰到了一系列木桶的短板，遇到过切换严重变慢甚至服务不可用的问题，虽然通过各种手段短期解决了，但是单集群规模的风险始终是达摩克利斯之剑。</p><p></p><p>所以这部分我们接下来的一个改造方向就是拆小集群，提供按团队、任务粒度的集群管理模式，从而规避上述问题。</p><p></p><h2>4.5&nbsp;训练平台对接</h2><p></p><p></p><p>我们同时还在模型训练平台里做了HA3 SQL样本的适配工作，训练端最终拿到的每个分区的查询SQL大致是这样的：</p><p></p><p><code lang=\"hljs cobol\">SELECT *\nFROM (SELECT *\n      FROM (SELECT *\n      FROM mainse_video_ha3_sample_ds_20221123\n      /*+ SCAN_ATTR(batchSize='1', partitionIds='0', localLimit='100')*/) AS t0\n      WHERE pv = 1 AND content_type = 'video') AS t1\nINNER JOIN (SELECT pv_id, item_id, label, cart\n            FROM mainse_video_click_to_pay_backbone_v2_ds_20221123) AS t2 ON t1.pv_id = t2.pv_id AND t1.item_id = t2.item_id</code></p><p></p><p>对于一次训练来说，不同分区的样本。可能我们的sql语句会有所不同。比如从某天开始，一个字段从维表转移到事实表里面了，那么从这个分区开始的sql会有所变化，但是这个其实也是对训练端透明的，训练端只需要拿着sql去查询即可。</p><p></p><p>而对于一个分区的样本，一方面考虑到引擎服务内存压力，一方面为了对数据做shuffle，我们一般会分为256列，查询的时候通过SCAN_ATTR这个hint参数指定某一列，并且指点一个起始点和limit，相当于将所有数据划分成了许多个很小的分片。</p><p></p><p>具体到数据读取的实现，我们是通过以下几个op的串接实现的：</p><p></p><p>GenStreamSqlOp：根据样本名称与分区列表从特征中心api获取每天的meta信息，比如doc count、每天的sql语句模板，然后划分成许多小的字符串，每个字符串描述一个分片信息，譬如part_id, start_row等；</p><p></p><p>WorkQueueOp：一个全局的队列，所有的worker训练的时候都是从这个全局唯一的queue中取下一次要读取的样本分片，把GenStreamSqlOp的结果往queue里放，每次save checkpoint的时候，会把queue的一些消费信息记录进去，从而实现正常的训练failover；</p><p></p><p>SqlReadUpToOp：从queue中拿出来的分片，根据对应分区信息还原成一条完整sql，然后通过op中的流式客户端从远程ha3获取结果。由于离线训练环境与在线环境编译环境有所不同，我们这边对在线的结果做了一次封装，以tensor的形式返回防止出现兼容性的问题，也方便后续后端服务升级。</p><p></p><h2>4.6&nbsp;性能优化</h2><p></p><p></p><h3>✪&nbsp;4.6.1&nbsp;ETL</h3><p></p><p></p><p>数据导入的效率是我们关注的一个重点，因为样本实验功能的基本思路是把样本构建的部分逻辑替换到了查询，这部分MaxCompute任务就是可以省下来的，与之对应新增的是若干表的ETL任务，那么这两个任务之间速度的差异就是用户使用平台功能的第一个感受细节，我们认为需要在这个环节给用户一个印象深刻的对比。</p><p></p><p>传统的MaxCompute数据导出任务基本都是通过MaxCompute tunnel功能实现的，我们在使用过程中注意到两个比较主要的问题：</p><p></p><p>tunnel功能是要MaxCompute端起对应的转发服务的，还有对应的流控quota，在导比较大的表时经常被流控导致速度剧烈下降；缺乏只导出部分列的功能，有些情况下我们只需要导出表中少量列的数据，用tunnel会造成浪费。</p><p></p><p>这样意味着在ETL流程里最开始从原数据系统把数据读出来都有瓶颈，就不用谈整个任务的速度了。为了解决这部分的问题我们换了种思路，借助我们自己的swift服务做了层中转，通过一个MR任务把源表数据写到swift，后续的build任务把swift topic作为数据源。这样一个MR任务基本就是最简单的遍历逻辑，消耗的cu极少，而且可以按需读取需要的字段。</p><p></p><p>解决了导出层面的问题后，我们还针对上文提到的基础样本作为事实表的情况做了专门的优化，这类表的特点是规模大，字段多，索引简单，同时因为我们借助了swift作为中转，可以借助swift实现数据shuffle，在这个前提下，传统索引构建任务的processor-builder-merger三段流程显得有些浪费了，而且离线任务资源不稳定、频繁启停的现状也会影响表构建的速度。</p><p></p><p>所以我们运用了具有读写一体能力的AIOS存储服务2.0，用一个使用在线资源的、服务化的写集群来承接表的build。在经过一系列参数调整测试后，可以稳定做到一个小时完成一份30T大小的样本表导入。</p><p></p><h3>✪&nbsp;4.6.2&nbsp;流式执行</h3><p></p><p></p><p>如上面训练平台对接章节提到的，训练端最终执行的查询是一段切片后的sql，考虑到训练数据的凑批需要，这个切片大小一般是1w左右。初期实测下来这种query查询起来在cache已经命中的情况下依然会有较大的rt，导致训练端需要起多个查询并发才能让吞吐够得上后面模型的消费。排查后发现主要原因是当时ha3 sql的执行调度层比较接近批任务的调度模式，比如对于下面这样一个执行图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/72e3fe2fa6c9f03df52cb7e9002a1277.png\" /></p><p></p><p>因为本身确实是一个串行的依赖图，服务端就只能这样没有任何并行的直接执行，rt就会非常可观。为了解决这个问题ha3的同学开发了新的流式执行引擎，在新的执行引擎里面，上下游kernel之前会用队列连接起来，每个kernel发现自己上游的队列里面有足够自己计算的一个mini batch的数据后就可以激发计算，这样就把一个逻辑上纯串行的过程并行了起来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d9d74a16e708cd63c5ef8052bc224ffa.png\" /></p><p></p><p></p><h3>✪&nbsp;4.6.3&nbsp;Caching</h3><p></p><p></p><p>正如在查询层技术选型里面提到的，因为样本这个场景，表的总大小规模是一定会超过一个集群的内存上限的，所以完整数据只能放在盘古集群这样的DFS系统，而且是用HDD的离线盘古，那么像查询索引这样有多次seek动作的操作就必然要通过cache机制才能让查询速度符合期望。而indexlib内部有IO层统一的block cache机制，这项技术帮助我们能同时对事实表和维表进行caching。</p><p></p><p>另外在使用方面，考虑到样本场景的一些普遍业务特点，我们对数据的分布模式做了一些预设，从而获得更好的cache命中率。因为事实表通常是pv_id,item_id 双主键的，我们把事实表根据pv_id拆分了256列，这样事实表查询时的cache局部性就更好，同时有一类超大维度的维表通常也是带着pv_id作为其中一个主键的，这类表我们也会按pv_id拆分为256列，届时执行计划可以优化为一个单列上的local join。其它维度的维表最优的状态是根据主键性质和规模拆分成另外的列数单独部署，这也是我们现在的重点工作方向之一，目前这些维表先统一build成了一列平铺到了同一个部署上。</p><p></p><p>此外我们还注意到，线上集群的磁盘是ssd的，从磁盘读取的速度比从离线盘古读取要快2-3个数量级，把磁盘也作为cache的一层可以极大地扩大缓存空间。又是得益于indexlib的功能，我们通过简单的配置就用上了这层功能。目前集群的通用配置是128G的磁盘存储加上16G的内存，基于LRU淘汰。</p><p></p><h1>五、场景成本案例</h1><p></p><p></p><p>分享几个已经接入并替换掉MaxCompute样本的场景的具体使用方式和前后成本对比案例。</p><p></p><h2>5.1&nbsp;多场景通用样本</h2><p></p><p></p><p>我们服务的其中一个算法团队管理了多个场景的推荐模型，各个场景用的特征是一样的，之前就接入了我们特征平台的全埋点功能，所有场景都通过一套特征配置统一埋点了下来，各个场景在离线训练时有单独用自己场景的样本训练的需求。从概念上讲这种使用方式是上文提到过的行级别实验，但是之前只能算法自己在MaxCompute上接着我们产出的全埋点样本，单独过滤出几张不同场景的样本表，训练分别读取。采用我们的样本实验功能后，结构变成了只需要导入那一份包含所有场景的全埋点样本，各个场景训练时根据场景id直接过滤出来。</p><p></p><p>该算法团队接入后经过精排在线严格AB，相比之前的链路，在离线效果均一致，并且模型每天还能提前两个小时切到线上。在所有场景推全，所用盘古存储从PB级别直接降到TB级别（均为物理存储大小）。</p><p></p><h2>5.2&nbsp;召回样本</h2><p></p><p></p><p>某业务召回场景的样本则是一个非常典型的星状模型构建样本的例子，可以直接看样本代码：</p><p></p><p><code lang=\"hljs cobol\">from turing_script.sdk.itable import ITable as Table\nfrom turing_script.biz_plugin.default_modules.base_sample_module import BaseSampleModule\n\nclass SampleDemo(BaseSampleModule):\n    def build(self, table_env) -&gt; Table:\n        base_sample = table_env.get_table('odps_project.base_sample', shard_key=\"pvid\", with_extra_rand_key = True)\n        \n        #需要join的维表，指明pk_name和需要的字段，可支持多张\n        user_feature = table_env.get_table(\"odps_project.user_feature\", pk_name = \"id\")\n        content_pos_feature = table_env.get_table(\"odps_project.content_pos_feature\", pk_name = \"id\")\n        content_neg_feature = table_env.get_table(\"odps_project.content_neg_feature\", pk_name = \"id\")\n\n        # sequence feature\n        sequence_feature = table_env.get_table(\"odps_project.sequence_feature\", pk_name = \"pvid\")\n\n        #三张表按条件join，字段类型需一致，如字段名相同，可用_right_前缀来区分\n        sample_exp = base_sample.left_outer_join(user_feature, 'user_id=_right_id') \\\n                                .left_outer_join(content_pos_feature, 'content_id=_right_id') \\\n                                .left_outer_join(content_neg_feature, 'neg_content_id=_right_id') \\\n                                .left_outer_join(sequence_feature, 'pvid=_right_pvid') \\\n                                .select(\"is_clk as label, *\") \n        return sample_exp</code></p><p></p><p>这段代码基本就是MaxCompute上面最后生成样本表任务的等价替换，之前这个样本在MaxCompute上面存了8天，就能占据PB级别的存储，而迁移到HA3 SQL样本后，因为只需要分别存事实表和维表，存了14天的样本也仅用了几十TB的存储空间。</p><p></p><h1>六、经验教训和展望</h1><p></p><p></p><p>样本场景是我们关注和投入了很久的一个领域，部分技术尝试甚至可以追溯到两三年前，回顾这段历程，在技术和产品设计方面有不少感受值得和大家分享：</p><p></p><p>对全链路技术栈的把控力会直接影响这种复杂跨系统项目的成败，引入一个查询层解决样本构建和存储的问题这个想法我们其实很早就有，但是却始终无法找到一个符合场景要求的具体实现。而如今回过头去看，无论是本文提到ha3流式执行引擎改造、写集群直写索引，还是没有提到的各种查询计划生成的bug排查、细致的运行时内存管理逻辑修改等工作，如果我们没有办法去对各种底层系统做直接的改造，很难想象我们最终能完成这个项目。统一代码库带来的集成迭代速度优势是显著而且不断积累的。本文提到了我们运用了很多底层系统团队开发的新功能，项目成员也去各个系统里面加过很多功能或者fix bug，而因为我们整个AIOS团队的代码基本都在一个统一的mono repo中，我们无论是想用上其它系统的最新功能还是自己去学习、更改系统，都是成本非常低的事情，这个对我们的效率提升是巨大的。系统可以快速调试非常重要，我们一开始非常担心用户学习我们样本DSL的成本很高，而且会让我们花大量时间在这部分的答疑上。但是上线后发现，因为我们构建了了web ide开发环境，用户可以直接在页面里面编译DSL并直接看到具体报错，这个尝试过程很快，远比找我们答疑要快，用户普遍倾向于自己先试试，对于我们的答疑需求主要是具体方案和一些表的建议配置。</p><p></p><p>样本实验功能目前还是个很年轻的产品，我们还会持续在下面几个方面做改进：</p><p></p><p>集群部署形态往按团队、按任务粒度部署演进，提供更好的用户隔离性和灵活性，规避超大规模集群下的一些风险。但同时整个运维、runtime系统也需要适时针对离线的规模场景做重构优化，为规模问题做好两手准备。查询性能优化方面，通过多zone拆表优化cache、自动扩缩容、自动调整调度参数等方式持续优化样本集群查询性能和成本，让样本读取方面不成为训练的瓶颈，同吞吐下降低样本集群资源消耗。样本实验的具体功能还需要持续丰富，比如支持任意层多值MaxCompute表导入、补数据功能产品化、支持udf、建立业务报警机制等，满足业务在样本迭代方面的各类功能需求。目前计算层的逻辑都是抛给用户前置处理掉的，后续会把计算层的任务生成也包含进来，承担诸如部分join的前置计算、样本数据分析之类的功能，让整个产品真正做到混合计算查询。</p><p></p>",
    "publish_time": "2023-08-09 14:23:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌重磅发布多平台应用开发神器：背靠AI编程神器Codey，支持React、Vue等框架，还能补全、解释代码",
    "url": "https://www.infoq.cn/article/EjjeOEpoRbjkEXbqcG8q",
    "summary": "<p></p><blockquote>AI 编程狂飙，未来还会有多少惊喜？</blockquote><p></p><p>&nbsp;</p><p>8月8日，谷歌宣布推出AI代码编辑器IDX，旨在提供基于浏览器的人工智能开发环境，用于构建全栈网络和多平台应用程序。谷歌在创建IDX时并没有构建新的IDE（集成开发环境），而是使用VS Code作为其项目的基础。目前，IDX支持 Angular、Flutter、Next.js、React、Svelte 和 Vue 等框架以及 JavaScript 和 Dart 等语言，后续还将支持 Python、Go 和其他语言。</p><p>&nbsp;</p><p>据了解，IDX是基于Google Cloud构建的一整套浏览器开发体验，由Codey提供支持。IDX项目还以Code OSS编辑器为基础。当前，IDX还未大面积开放，只有少数指定测试人员能够亲身参与体验。不过IDX 团队透露，在即将召开的Google Cloud Next大会上，可能会有更多Codey现有成果及未来发展计划的消息与广大开发者见面。IDX 团队表示：</p><p>&nbsp;</p><p></p><blockquote>我们花费了大量时间编写代码，AI的最新进展则创造了巨大的机会空间，能让我们更高效地利用时间。通过IDX项目，我们正探索谷歌在AI领域的创新成果（包括为Android Studio中的Studio Bot提供支持的Codey与PaLM 2模型、Google Cloud中的Duet等）如何帮助开发者加快代码编写速度、提高代码编写质量。</blockquote><p></p><p></p><h2>谷歌发布AI代码编辑器IDX</h2><p></p><p>&nbsp;</p><p>IDX 团队在博文中提到，时至今日，从零开始构建应用（特别是能在移动、Web和桌面平台上良好运行的应用）的难度简直不逊于制造鲁布·戈德堡机械（Rube Goldberg machine，一种被设计得过度复杂的机械组合）。开发者需要在无尽的复杂性之海中航行，将种种技术栈粘合起来，奋力摸索出一条能够正确引导、编译、测试、部署和监控应用的路线。</p><p>&nbsp;</p><p>虽然谷歌多年来一直致力降低多平台应用的开发难度，也先后推出了Angular、Flutter、Google Cloud乃至Firebase等成果，但似乎还能做得更好。毕竟当下的多平台应用开发还远远称不上快速、顺畅。所以几个月前，谷歌的几个小伙伴聚在一起开始尝试，而这场实验的早期成果就是IDX项目（<a href=\"http://idx.dev/\">http://idx.dev/</a>\"）。</p><p>&nbsp;</p><p>团队之所以决意构建IDX项目，一个重要原因就是希望听取广泛开发者社区的意见，了解哪些要素能帮助大家提升工作效率。</p><p></p><h3>IDX功能特性</h3><p></p><p>&nbsp;</p><p>具体来说，IDX项目当前主要可实现以下功能：</p><p>&nbsp;</p><p>1、随时随地，快速工作。IDX项目的核心是帮助开发者在任何位置、任何设备上进行开发，并且获得完全保真的本地开发体验。IDX项目中的每个工作区都具备基于Linux虚拟机的全部功能，并配有托管在开发者邻近云数据中心的通用访问权限。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d75db0b7b53609374be50410bc82d76d.gif\" /></p><p></p><p>2、可导入现有应用，也可创建新应用。IDX项目允许开发者从GitHub处导入现有项目，随时从上次完成的位置继续开发。开发者也可以使用各类流行框架的预制模板创建新项目，包括Angular、Flutter、Next.js、React、Svelte、Vue&nbsp;以及 JavaScript和Dart，并即将推出对Python与Go语言的支持。此外，IDX团队还在积极努力为更多项目类型和框架提供最佳支持。</p><p>&nbsp;</p><p>3、跨平台实现应用预览。如今，构建成功应用意味着要对应用的设计和行为做跨平台优化，并以用户“所见即所得”的方式预览应用效果。为了降低这一切的实现门槛，IDX项目提内置有Web预览功能，而且即将推出经过完善配置的Android模拟器与嵌入式iOS模拟器。所有这些，都可以在浏览器中直接使用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/eb/eb347be36c43ed9e8531a07d058425c5.gif\" /></p><p></p><p>4、结合AI技术。目前，IDX项目的AI功能尚处于早期阶段，但已经拥有智能代码补全、辅助聊天机器人以及“添加注释”和“解释此代码”等结合上下文的代码操作。</p><p>&nbsp;</p><p>5、借助Firebase Hosting实现Web发布。将应用投入生产的一大常见痛点就是部署流程。IDX项目集成了Firebase Hosting以降低整个操作难度，只需单击几下，即可部署Web应用的可共享预览，或者使用快速、安全的全球托管平台将其部署至生产环境。由于Firebase Hosting能够支持基于Cloud Functions的动态后端，因此能够与Next.js等全栈框架良好配合。</p><p>&nbsp;</p><p>IDX团队表示，目前IDX项目才刚刚起步，距离最终的端到端开发流程改进还有很长的路要走。希望开发者可以注册成为首批IDX项目体验者，大家携手发现的问题将为项目的后续迭代方向、以及还需哪些新增功能以匹配应用团队的工作流程带来指引。</p><p>&nbsp;</p><p>至于下一步计划，IDX团队称，将不断努力添加新功能并解决开发者反馈的问题。“我们已经在研究新的协作功能，因为我们深知这项功能在如今混合办公时代下的重要意义。此外，更深入的框架集成和个性化/情境化AI选项也在筹备当中。期待听到大家提出的更多功能需求！”</p><p></p><h2>IDX背后的AI模型Codey</h2><p></p><p>&nbsp;</p><p>根据介绍，IDX由Codey提供支持。</p><p>&nbsp;</p><p>在Google I/O 2023 大会上，谷歌正式发布Codey。这是一款新型AI驱动工具，能够编写并理解代码内容。这款新工具被外界视为谷歌对于GitHub Copilot的回应，属于同Replit结盟打造的成果。</p><p>&nbsp;</p><p>Codey基于谷歌的下一代大语言模型PaLM 2，并采用谷歌自家产品代码及大量合法许可的源代码作为训练素材。更重要的是，Codey仍在不断学习和发展，从谷歌服务生态系统的各个项目中持续汲取新的力量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/96b34eb2a659bbc71340f19aff2acc95.gif\" /></p><p></p><p>Codey支持20多种编程语言，包括Go、谷歌标准SQL、Java、JavaScript、Python以及TypeScript。开发者可以通过Visual Studio Code、JetBrains IDE、Google Shell 编辑器以及 Google&nbsp;Cloud托管工作站服务的扩展来访问Codey。开发者能够直接在IDE的聊天框中与该模型交流（例如Android Studio Bot），或者在文本文件中编写注释以指示其生成相关代码。它支持各种编码任务，通过以下方式帮助开发人员更快地工作并缩小技能差距：</p><p>&nbsp;</p><p>代码完成：Codey 根据提示中输入的代码上下文建议接下来的几行。代码生成：小程根据开发人员的自然语言提示生成代码。代码聊天：Codey 允许开发人员与机器人对话，以获得调试、文档、学习新概念和其他与代码相关问题的帮助。</p><p>&nbsp;</p><p>Codey在处理与编码相关的提示词方面接受了专门训练，谷歌还通过其他训练让该模型学会了处理关于Google Cloud的一般查询。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\">https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html</a>\"</p><p><a href=\"https://codeandhack.com/google-codey/\">https://codeandhack.com/google-codey/</a>\"</p>",
    "publish_time": "2023-08-09 15:06:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "英伟达亮相“王炸”AI大礼包：首发HBM3e、升级Omniverse，老黄坚信“买的越多，省的越多”",
    "url": "https://www.infoq.cn/article/H6xAfNlV5f3rqiijXrGO",
    "summary": "<p>黄仁勋向数千名开发者和图形专业人士发表讲话，宣布更新GH200 Grace Hopper超级芯片、英伟达AI Workbench，并将把生成式AI引入英伟达Omniverse。</p><p>&nbsp;</p><p>随着生成式AI技术继续席卷整个数字化与超互连世界，英伟达创始人兼CEO黄仁勋决定以雷霆万钧的气势重回全球顶级计算机图形会议SIGGRAPH。</p><p></p><h2>黄仁勋：生成式AI开启了人工智能的“iPhone时刻”</h2><p></p><p>本周二，黄仁勋在洛杉矶举行的一场特别演讲中对数千名观众表示，“生成式AI时代即将到来。如果大家愿意，也可以称之为人工智能的iPhone时刻。”</p><p>&nbsp;</p><p>他带来的亮点包括下一代GH200 Grace Hopper超级芯片平台、英伟达AI Workbench（新的统一工具包，可在英伟达AI平台上引入更精简的模型调整和部署方式），以及搭载生成式AI和OpenUSD的英伟达Omniverse重大升级。</p><p>&nbsp;</p><p>这些公告，无疑是将过去十年来AI、虚拟世界、加速、模拟和协作等领域的所有重量级创新整合在了一起。</p><p>&nbsp;</p><p>黄仁勋表示，“图形和人工智能密不可分。图形需要AI，AI也需要图形。”他同时解释道，AI将在虚拟世界中学习技能，AI也将帮助人类创建虚拟世界。</p><p>&nbsp;</p><p>在五年前的SIGGRAPH大会上，英伟达通过将AI与实时光线追踪引入GPU重塑了整个图形世界。黄仁勋指出，“当我们用AI重新发明计算机图形学时，也相当于是在为AI重新发明GPU。”</p><p>&nbsp;</p><p>结果就是：系统变得越来越强大。以英伟达HGX H100为例，它包含8个GPU和共计1万亿个晶体管。与基于CPU的系统相比，其速度表现得以显著提升。</p><p>&nbsp;</p><p>黄仁勋向观众们强调，“正因为如此，全球数据中心才迅速转向加速计算。这才是真正的「买得越多、越省钱」。”</p><p></p><h2>全球首发HBM3e内存，带宽达每秒5TB</h2><p></p><p>为了延续AI的强劲发展势头，英伟达打造了Grace Hopper超级芯片GH200。它将72核Grace CPU与Hopper GPU结合起来，已经于今年5月全面投入生产。</p><p>&nbsp;</p><p>黄仁勋还宣布，已经投入生产的英伟达GH200还将迎来一个附加版本，其中搭载顶尖HBM3e内存。</p><p>&nbsp;</p><p>他随后又宣布将推出下一代GH200 Grace Hopper超级芯片平台。该平台能够接入多个GPU，从而实现卓越的性能和易于扩展的服务器设计。</p><p>&nbsp;</p><p>新平台专为处理世界上最复杂的生成式工作负载而构建，具体涵盖大语言模型、推荐系统和向量数据库等，而且将提供多种配置选项。</p><p>&nbsp;</p><p>相比前一代平台产品，双配置方案的内存容量增加了3.5倍、带宽增加3倍，由此构建起一台搭载144个Arm Neoverse核心、8千万亿次AI性能及282 GB最新HBM3e内存容量的服务器。</p><p>&nbsp;</p><p>HBM3e是一种高带宽内存，带宽达每秒5TB。该内存比当前的HBM3快50%，可提供总共每秒10TB的组合带宽，使新平台能运行比前代大3.5倍的模型，同时通过快三倍的内存带宽提高性能。</p><p>&nbsp;</p><p>预计各领先系统制造商将在2024年第二季度，向市场交付基于该平台的新一代计算系统。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3bee848542a71a44be0396d01b6abcf7.png\" /></p><p></p><p>英伟达创始人兼CEO黄仁勋发表主题演讲，SIGGRAPH专业图形大会座无虚席。</p><p>&nbsp;</p><p>数据中心的硬件正在快速向加速计算转变，这是黄仁勋一直在强调的一个趋势。相比CPU，GPU在能效上有很大的优势，黄仁勋演讲中举的例子是同样1亿美元成本，相比x86架构CPU，GH200将能提供超过20倍能效提升。</p><p></p><h2>英伟达AI Workbench：加速定制生成式AI的普及</h2><p></p><p>为了加快全球企业以定制化方式采用生成式AI，黄仁勋还宣布推出英伟达AI Workbench。它将为开发人员提供统一且易于使用的工具包，可在PC或工作站平台上快速创建、测试和微调生成式AI模型，而后将其扩展至几乎任何数据中心、公有云或英伟达DGX Cloud。</p><p>&nbsp;</p><p>AI Workbench消除了企业AI项目的入门复杂性。通过在本地系统上运行的精简界面开放访问，开发人员可以使用自定义数据微调Hugging Face、GitHub和NGC等流行repo中的模型，之后在各平台上轻松实现模型共享。</p><p>&nbsp;</p><p>虽然目前市面上已经有数十万种预训练模型可供使用，但基于开源工具的定制化微调往往仍然艰难且耗费时间。</p><p>&nbsp;</p><p>黄仁勋指出，“为了推动这种能力的大众化普及，我们必须让模型几乎能够随时随地运行。”</p><p>&nbsp;</p><p>在AI Workbench的帮助下，开发人员只需点击几下即可自定义并运行生成式AI。这款工具能够将所有必要的企业级模型、框架、软件开发套件和库，都整合到统一的开发者工作区当中。</p><p>&nbsp;</p><p>黄仁勋总结称，“每个人都可以参与AI开发。”</p><p>&nbsp;</p><p>领先的AI基础设施提供商——包括戴尔科技、HPE、惠普、Lambda、联想和超微等也在积极拥抱AI Workbench，希望借助它的力量将企业生成式AI功能带到开发者需要的任何地方，包括本地设备。</p><p>&nbsp;</p><p>黄仁勋还宣布英伟达与拥有200万用户的初创公司Hugging Face建立合作伙伴关系，将帮助数百万开发者轻松构建起大语言模型及其他先进AI应用，在超级算力的加持下拥抱生成式AI。</p><p>&nbsp;</p><p>开发人员还可访问Hugging Face平台中的英伟达DGX Cloud AI超级计算系统，利用它训练并微调各种高级AI模型。</p><p>&nbsp;</p><p>“这将是一项全新服务，努力将世界上最大的AI社区与世界上最好的训练和基础设施对接起来。”</p><p>&nbsp;</p><p>为了进一步加快生成式AI的应用，英伟达又发布了最新版本的企业软件套件英伟达AI Enterprise 4.0。</p><p>&nbsp;</p><p>英伟达AI Enterprise将为企业提供访问生成式AI所需要的工具，同时为大规模企业部署提供安全性和API稳定性支持。</p><p></p><h2>Omniverse迎来大更新：融合生成式AI与OpenUSD以推动工业数字化</h2><p></p><p>黄仁勋还公布了英伟达Omniverse的大版本更新。这是一套OpenUSD原生开发平台，可用于跨工具构建、模拟与协作，为开发人员和产业公司提供新的基础应用程序及服务。OpenUSD框架与生成式AI技术将帮助用户优化并增强其3D流程与虚拟环境。</p><p>&nbsp;</p><p>他还提到了英伟达对OpenUSD的贡献，推动这套用于描述、模拟和跨3D工具协作的框架向前发展。</p><p>&nbsp;</p><p>Omniverse平台此番更新，涵盖Omniverse Kit（用于开发原生OpenUSD应用程序和扩展的引擎）、英伟达Omniverse Audio2Face基础应用以及空间计算功能方面的改进。</p><p>Cesium、Convai、Move AI、SideFX Houdini 和 Wonder Dynamics等客户现已通过OpenUSB接入到Omniverse。</p><p>&nbsp;</p><p>为了扩大在Adobe Substance 3D、生成式AI和OpenUSB发展规划中的合作，Adobe和英伟达宣布将Adobe Firefly（Adobe打造的创意生成式AI模型家族）以API的形式在Omniverse中开放。</p><p>&nbsp;</p><p>Omniverse用户现在可以与其他OpenUSD空间计算平台（例如ARKit和RealityKit）相兼容的内容、体验及应用程序。</p><p>&nbsp;</p><p>黄仁勋宣布为开发人员和企业提供广泛的框架、资源和服务，借此加速通用场景描述（即OpenUSD）的普及度，为地理空间数据模型、指标组合与仿真就绪（简称SimReady）和OpenUSD规范等作出贡献。</p><p>&nbsp;</p><p>黄仁勋还公布了英伟达构建的四种新Omniverse Cloud API，可供开发人员更加无缝地实施并部署OpenUSD管线与应用程序。</p><p>&nbsp;</p><p>from text and answering USD knowledge questions.ChatUSD——ChatUSD是一个大语言模型（LLM）智能体，可帮助开发人员和艺术家处理OpenUSD数据及场景，根据文本提示生成Python-USD代码脚本并回答关于USD的问题。RunUSD——一个云API，通过检查上传文件与OpenUSD版本间的兼容性，并配合使用Omniverse Cloud生成渲染，从而将OpenUSD文件转换为全路径跟踪的渲染图像。DeepSearch——一个LLM智能体，可以在未标记资产的海量数据库内进行快速语义搜索。USD-GDN Publisher——一项一键式服务，可帮助企业和软件开发商从USD Composer等基于Omniverse的应用程序处，向Omniverse Cloud图形交付网络（GDN）发布高保真、基于OpenUSD的体验，并将结果实时传输至网络浏览器及移动设备。</p><p>&nbsp;</p><p>这些贡献，也标志着英伟达上周宣布与皮克斯、Adobe、苹果和Autodesk共同创立的OpenUSD联盟正迎来技术演变新成果。</p><p></p><h2>强大的新型桌面系统与服务器</h2><p></p><p>黄仁勋表示，英伟及全球各工作站设备制造商正着手推出功能更强大的新型RTX工作站，以便为各类需求提供更丰富的算力，用以支持生成式AI和数字化时代下的更多开发与内容创作需求。</p><p>&nbsp;</p><p>这些系统（包括来自BOXX、戴尔科技、惠普和联想的系统）基于英伟达RTX 6000 Ada Generation GPU，并采用英伟达AI Enterprise和英伟达Omniverse Enterprise等软件。</p><p>&nbsp;</p><p>另外，英伟达还发布了三款全新的桌面工作站Ada Generation GPU，分别为英伟达RTX 5000、RTX 4500和RTX 4000，旨在为全球专业人士提供最新的AI、图形与实时渲染技术。</p><p>&nbsp;</p><p>黄仁勋详细介绍了英伟达如何与全球数据中心系统制造商合作，继续通过搭载有全新英伟达L40S&nbsp;GPU（一种强大的通用数据中心处理器设计）的英伟达OVX来增强生成式AI与工业数字化实践。</p><p>&nbsp;</p><p>强大的新系统将通过英伟达Omniverse平台为最密集、最复杂的应用程序提供加速计算，具体包括AI训练与推理、3D设计和可视化、视频处理及工业数字化等。</p><p></p><h2>英伟达研究院带来更多新功能</h2><p></p><p>在英伟达研究院的不懈努力下，更多创新成果也即将揭开面纱。</p><p>&nbsp;</p><p>在本次大秀的现场体验环节中，英伟达研究人员将展示生成式AI的工作流程，帮助艺术家们快速创建和迭代3D场景材质，使用文本或图像提示词快速生成自定义纹理材质，并进行更精细的创意控制。</p><p>&nbsp;</p><p>英伟达研究院还演示了AI如何通过新的3D功能，将视频会议体验提升到新的水平。该研究院最近发表一篇论文，探讨了AI如何通过最少的捕捉设备为3D视频会议系统提供支持。</p><p>&nbsp;</p><p>Maxine的量产版本现已在英伟达Enterprise中开放，允许专业人士、团队、创作者等用户利用AI的力量获取高质量音/视频效果，甚至借助标准麦克风和网络摄像头即可实现。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://blogs.nvidia.com/blog/2023/08/08/siggraph-2023-special-address/\">https://blogs.nvidia.com/blog/2023/08/08/siggraph-2023-special-address/</a>\"</p>",
    "publish_time": "2023-08-09 15:49:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "英伟达、云厂商们也没有H100余粮了！未来全球顶级芯片缺口达40多万张",
    "url": "https://www.infoq.cn/article/TZh20mzRMjS7eewcTtyz",
    "summary": "<p>GPU是一种关键的硬件，可帮助运行训练和部署人工智能算法所涉及的无数计算。</p><p></p><p>行业分析师表示，持续的GPU紧缩已经影响了大大小小的企业，包括人工智能行业的一些领先平台，并且至少在一年或更长时间内可能不会出现有意义的改善。</p><p></p><h2>AI圈里，苦H100 GPU久矣</h2><p></p><p>&nbsp;</p><p>OpenAI联合创始人兼职科学家Andrej Karpathy近日发文称“目前硅谷最热门的八卦，就是谁谁又买了多少块H100 GPU。”</p><p>&nbsp;</p><p>特斯拉掌门人马斯克曾经向整个科技行业发出警告，称巨大的GPU危机即将来临。今年4月，马斯克曾发表推文说，“如今不管是人是狗，都在疯狂购买GPU。”而巨大的需求，势必引发严重的供应短缺。时间快进到当下，每个人都想搞自己的AI产品和业务。面对这样一场声势浩大的AI洪流，即使是像英伟达这样的全球巨头也难以及时制造出充足的GPU货源。</p><p>&nbsp;</p><p>市场对高性能GPU（特别是英伟达H100）的需求仍在猛增。截至2023年8月，科技行业正经受英伟达H100短缺的严重折磨。GPU供应不足，正在对严重依赖其进行模型训练和推理任务的AI厂商造成重大影响。</p><p>&nbsp;</p><p>微软最近的年度报告显示了人工智能芯片可能长期短缺的最新迹象。该报告首次将GPU的可用性确定为投资者可能会遇到的的风险因素。</p><p>&nbsp;</p><p>微软写道：“我们将继续寻找和评估扩大数据中心位置和增加服务器容量的机会，以满足客户不断变化的需求，特别是考虑到对人工智能服务不断增长的需求。”&nbsp;“我们的数据中心取决于许可的可建设土地、可预测的能源、网络供应和服务器，包括GPU和其他组件。”</p><p>&nbsp;</p><p>微软对GPU的认可凸显了计算能力的获取如何成为制约AI发展的关键因素。该问题直接影响正在构建人工智能工具和产品的公司，并间接影响希望将该技术应用于自己目的的企业和最终用户。</p><p>&nbsp;</p><p>来自OpenAI公司的Andrej Karpathy表示，“目前硅谷最热门的八卦，就是谁谁又买了多少块H100 GPU。”有趣的是，AWS Lambda&nbsp;CEO Stephen Balaban也提到，“Lambda将于今年年底之前上线数千块H100——如果您需要64块或者更多的H100，请提前私信预约。”没错，这宝贝现在就是这么紧俏。</p><p>&nbsp;</p><p>包括Quora公司CEO Adam D’Angelo和OpenAI创始人Sam Altman在内的多位AI领导者，也都表达了自己对于GPU短缺问题的担忧。OpenAI透露，GPU供应不足阻碍了他们的短期计划，包括模型微调和划拨专用容量。也许这正是OpenAI目前拘囿于GPT-4，无法进一步履行其大语言模型开发承诺的原因之一。</p><p></p><h2>谁，需要多少？</h2><p></p><p>&nbsp;</p><p>不只是AI公司，其他几类组织也对H100 GPU有着迫切需求。其中既包括研究大语言模型的初创企业，也有Azure、GCP和AWS等云服务供应商（CSP），CoreWeave、Lambda等大型私有云，以及马斯克的特斯拉等其他知名公司。说到马斯克，他自己已经抢先一步购买了数千块英伟达GPU，给自己的xAI储备“战略物资”。面对这块人人抢夺的现状，马斯克甚至曾劝Altman买下ai.com域名来换取GPU资源。</p><p>&nbsp;</p><p>据报道，GPT-4的训练过程可能用到了约1万到2.5万块英伟达A100。至于GPT-5，马斯克估计可能需要3万到5万块H100。2023年2月，摩根士丹利预测GPT-5大概需要使用2.5万个GPU。面对如此庞大的GPU需求，加之英伟达又是市场上唯一可靠的供应商，似乎整个AI世界的运转都着落在了这家显卡巨头身上。</p><p>&nbsp;</p><p>根据最近一篇博文，OpenAI预计需要约5万块H100 GPU，Inflection AI的采购计划则大致在2.2万块左右。Meta的需求尚不明确，但有传言称他们可能需要约2.5万个GPU，且实际上限甚至可能超过10万块。</p><p>&nbsp;</p><p>目前硅谷最热门的八卦，就是谁谁又买了多少块H100 GPU。近日，在Twitter上广为流传的一张“我们需要多少张GPU”的图片引发了网友们热议。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8000bfc7bbb4a9f4941fb86d0fd0ab9.png\" /></p><p></p><p>&nbsp;包括Azure、Google Cloud、AWS和甲骨文在内，各大主要云服务供应商可能各自需要约3万个GPU。AWS Lambda和CoreWeave等私有云预计共需10万个GPU。其他专注于AI业务的厂商，例如Anthropic、Helsing、Mistral和Character等，可能分别需要约1万个GPU。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df6148c9cb4ff58eed6e56425c9e4d35.png\" /></p><p></p><p>&nbsp;根据马斯克的说法，GPT-5可能需要3万-5万张H100。</p><p>&nbsp;</p><p>值得注意的是，这些数字只是近似估值，云服务商及其最终客户间可能存在一些需求重合。综合来看，H100 GPU的市场总需求可能在43.2万块左右，按每个GPU价格3.5万美元计算，意味着产品总价值高达150亿美元——没错，全部要被英伟达赚走。</p><p>&nbsp;</p><p>此外，值得一提的是，上述估算还不包括字节跳动（TikTok）、百度和腾讯等中国公司。这些公司可能对H800这款专为中国市场设计的GPU同样有着大量需求。</p><p>&nbsp;</p><p>尽管前景充满不确定性，但业界仍希望随着供应增加和GPU技术的进步，最终逐步缓解短缺问题。例如，英伟达一直在谈论发布A800——号称能在AI模型构建方面提供同样的算力，只是目前还没有可靠的实证。而在需求缺口得到填补之前，AI公司只能探索其他替代性GPU选项，并借助合作伙伴关系来应对这段充满挑战的时期。毕竟AI热度如此之高，该做的工作也还是得做。</p><p></p><h2>GPU稀缺性已成新的护城河</h2><p></p><p>令事态雪上加霜的是，业界专家担心当前GPU稀缺性可能引发自我强化的循环。换言之，这种稀缺性本身成为新的护城河，增强各方囤积GPU资源的心理、进一步加剧资源不足。也许这就是马斯克当初囤积GPU的原因所在。下一代H100继任者预计要到2024年底才会推出，这漫长的一年半将反复折磨用户们脆弱的神经。</p><p>&nbsp;</p><p>2010年，我们使用黄仁勋的英伟达GPU，证明无需任何无监督预训练，即可通过简单的反向传播实现对深度前馈网络的训练。2011年，我们的DanNet成为首个超级卷积神经网络。而到如今，计算成本降低到当初的百分之一，但英伟达公司的市值则涨了100多倍……</p><p>&nbsp;</p><p>获取H100已经成为AI公司面临的重大难题，也开始阻碍他们的正常运营，导致产品发布和模型训练纷纷出现延迟。AI热潮带来的对算力前所未有的需求也在加剧这种情况，导致GPU制造中使用的各种基本组件均告短缺。</p><p>&nbsp;</p><p>英伟达一直在支持全球几乎所有AI初创公司，而且似乎在为初创公司提供资助，帮助他们建立业务并购买GPU。如今的英伟达已经在GPU市场上建立起垄断地位，而其他参与方也不得不抱紧这条大腿。于是乎，满足市场需求的责任将无人分担，只能着落在英伟达自己身上。</p><p>&nbsp;</p><p>但GPU的制造涉及复杂的工艺流程，需要各种关键组件。内存、互连速度（例如InfiniBand）、缓存和缓存延迟等因素，在GPU的实际性能表现上起着至关重要的作用。其中任何一种组件的短缺，都有可能导致GPU生产延迟、进而引发整体供应不足。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://analyticsindiamag.com/why-the-ai-world-is-looking-up-to-nvidia/\">https://analyticsindiamag.com/why-the-ai-world-is-looking-up-to-nvidia/</a>\"</p><p><a href=\"https://edition.cnn.com/2023/08/06/tech/ai-chips-supply-chain/index.html\">https://edition.cnn.com/2023/08/06/tech/ai-chips-supply-chain/index.html</a>\"</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-08-09 15:52:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "科大讯飞AI研究院副院长、科研部部长李鑫博士，确认担任QCon北京大模型算法与应用专题出品人",
    "url": "https://www.infoq.cn/article/kbBx0z5igXdDdyvboeCT",
    "summary": "<p>9 月 3 日 - 5 日，在 <a href=\"https://qcon.infoq.cn/202309/beijing?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0809&amp;utm_content=lixin\">QCon 全球软件开发大会（北京站）</a>\"，科大讯飞 AI 研究院副院长、科研部部长李鑫博士将担任「大模型算法与应用」的专题出品人。在此次专题中，你将了解到大模型催生出的新业务，以及大模型与已有业务的结合。</p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/track/1560?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0809&amp;utm_content=lixin\">李鑫博士</a>\"，高级工程师、科大讯飞 AI 研究院副院长，科研部部长。中国科学技术大学博士、博士后，悉尼科技大学访问学者。认知智能全国重点实验室研究员。合肥工业大学、安徽大学、重庆邮电大学兼职硕士生导师。中国互联网协会青年专家。中国脑机接口标准委员会委员。</p><p></p><p>主持并参与多个国家重点研发计划、自然科学基金等项目 10 余项。发表高水平学术论文 40 余篇，申请专利 30 余件。BESC 2018 国际会议工业委员会主席。担任爱思唯尔期刊《Nature Language Processing Journal》创刊编委，获得 KSEM 2018 的最佳研究论文奖。研发儿童脑智发育测评平台获 2022 年度 1024 全球开发者节人工智能产品金奖。</p><p></p><p>相信李鑫博士的到来，可以帮助提升此专题的质量，让你了解到，Office Copilot 带来了一场效率革命，大模型使得图片 / 视频生成更加便捷，智能客服不再是“人工智障”，也使得搜索的架构变得极其简单，只需要用传统搜索 + 大模型即可获得更好的结果，以及大模型与推荐、广告等业务的结合。</p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-09 16:27:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC风口下，一窥智能技术在金融行业的应用与实践｜直播报名预约",
    "url": "https://www.infoq.cn/article/i1tjSZxTysysWJHs4F1v",
    "summary": "<p>一直以来，金融行业都是前沿技术应用的先行者。从信息化、数字化到智能化，技术的迭代与革新在金融丰富的业务场景中创造着巨大价值。</p><p></p><p>经过多年的探索与实践，以AI为代表的智能化技术已经渗透于<a href=\"https://www.infoq.cn/article/tXdg1xI1YWGYG6iGg4rj\">金融</a>\"产品设计、市场营销、风险控制、客户服务和其他业务运营场景中。并且，随着应用深度和广度的不断拓展，越来越多的金融机构展开了智能金融的规模化应用，从单一场景向多场景延展，从单领域创新向生态连接演变。</p><p></p><p>面对<a href=\"https://www.infoq.cn/article/4LKYBOU859NxLSk2klif\">AIGC</a>\"这波热浪，金融行业显然也不会置身事外。但作为一个在安全合规层面有着严苛要求的行业，相关技术在商业落地中每一个细微的问题都值得反复推敲和推演——比如“AI幻觉”的问题如何解决，如何让AI分析过程更具有可解释性等等。</p><p></p><p>基于这一背景，在8月23日由InfoQ与TGO鲲鹏会联合出品的《<a href=\"https://www.infoq.cn/theme/192\">超级连麦.数智大脑</a>\"》中，我们将邀请金融行业资深专家，围绕智能技术如何与金融场景结合，背后涉及什么样的技术升级以及AIGC等智能化技术落地过程有何挑战又如何顺利推进等话题，分享其在各自领域内的洞见和实践，为金融业的智能化创新实践找到更优的解题思路。</p><p></p><p><a href=\"https://live.infoq.cn/room/1848\">点击链接</a>\"率先报名预约（留下你关心的问题，讲师将在直播中解答）：<a href=\"https://live.infoq.cn/room/1848\">https://live.infoq.cn/room/1848</a>\"</p><p></p><h3>直播时间</h3><p></p><p>8&nbsp;月&nbsp;23&nbsp;日（周三）19:30-22:00</p><p></p><h3>联合出品</h3><p></p><p>InfoQ x TGO 鲲鹏会 x 极客时间企业版</p><p></p><h3>直播主题</h3><p></p><p>智能技术在金融行业的应用与实践</p><p></p><h3>直播亮点</h3><p></p><p>分享银行/证券/保险等金融机构智能化实践进程探讨智能技术如何与金融场景充分结合剖析金融智能化背后的技术升级挑战探寻智能技术在金融场景落地的路径和方法脑暴AIGC技术如何影响和助力金融智能化创新</p><p></p><h3>直播议程（拟）</h3><p></p><p>19:30-20:00 &nbsp;议题分享 -银行智能化运营实践20:00-20:30 议题分享 -华盛证券智能风控探索与实践（黄曙光 华盛证券技术VP）20:30-21:00  议题分享 - 保险智能化运营探索及背后的技术架构升级实践（周建华 太保寿险首席架构师）21:00-21:40 连麦对话 - AIGC技术如何影响和助力金融智能化创新</p>",
    "publish_time": "2023-08-09 17:24:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型颠覆研发模式：字节跳动是如何在单元测试中落地大模型的?",
    "url": "https://www.infoq.cn/article/20rbcXs3IT8fNaxz5l6f",
    "summary": "<p>大模型的出现引发了一场软件工程革命，它根本性地改变了软件开发的流程和方式。当下，越来越多的企业开始在实际的研发工作中，结合大模型增强软件开发在设计、需求、测试、发布和运维等各个环节中的能力，提高质量和效率。</p><p>&nbsp;</p><p>在接受 InfoQ 采访时，字节跳动算法专家张树波表示，大语言模型是一项人工智能基础技术的突破，必然会带来多个行业的变革。2023 年初，字节跳动智能服务团队开始启动大模型 X 智能单测项目。目前，大模型生成单元测试已经在实际业务中落地。</p><p>&nbsp;</p><p>单元测试是保障项目可靠性的重要手段。传统的智能单测生成依赖静态分析、动态分析等工具，对不同的语言需要重新适配。随着模型参数规模的提升，模型的代码理解、代码生成能力也大幅提升，使用模型端到端的生成单元测试，可以低成本地将单元测试覆盖到多种编程语言。然而大模型在单测生成任务上仍存在模型幻觉（随机生成不存在变量名、方法名）和测试分支覆盖不全的问题。</p><p>&nbsp;</p><p>为解决以上问题，字节跳动智能服务团队发现通过任务微调、强化学习等技术可以提升语言模型的单元测试生成语法正确率和分支覆盖率。经过测试，他们的基于Bloom&nbsp;70亿参数模型的生成效果不弱于通用版 ChatGPT 的水平，并且在低端显卡上的推理时延只有 ChatGPT 的 25%。且目前大模型单元测试生成分支覆盖率在实际项目中达到 56%，同时在抖音的 Android、iOS 双端落地，问题有效性达到 80%，修复率 65%。</p><p>&nbsp;</p><p>在今年 9 月 3-5 日举办的 QCon 全球软件开发大会·北京站中，张树波将把以上经验分享给大家。张树波硕士毕业于清华大学，先后就职于 vivo、字节跳动，从事 NLP 算法多年，在智能单测、智能客服、语音助手等业务场景有丰富的落地经验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/64/643a62dd12fff3ad58aec4efeb850b61.png\" /></p><p></p><p>在大会开始前，InfoQ 对张树波进行了专访，探索字节跳动是如何在单元测试中落地大模型的，以及大模型对软件研发工作流的改变。以下为对话实录，经编辑。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：您在今年9⽉举办的 QCon 全球软件开发⼤会·北京站上的演讲主题是《<a href=\"https://qcon.infoq.cn/202309/beijing/presentation/5380\">⼤模型助⼒智能单测⽣成</a>\"》，为什么会选择这⼀主题？</blockquote><p></p><p>&nbsp;</p><p>张树波：2022 年底 OpenAI 发布 ChatGPT，其效果令人大为震撼，曾经让 NLPer 困扰的自然语言处理问题，例如歧义、长程依赖、知识缺失、推理能力不足等，都得到了很大程度的缓解和解决。大语言模型是一项人工智能基础技术的突破，必然会带来多个行业的变革。2023 年初，我们字节跳动智能服务团队启动了大模型 X 智能单测项目，探索至今，大模型生成单元测试已经在实际业务中落地。这其中我们总结出了一些经验，希望能够帮助听众。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：对于这波⼤模型结合软件开发应⽤热潮，您观察到哪些有趣的趋势？</blockquote><p></p><p>&nbsp;</p><p>张树波：大模型会让开发更轻松。大模型代码生成会降低开发者编写重复性代码，但是不意味者开发门槛降低，开发者需要具备辨识模型生成是否正确，以及对最终上线负责。当前大模型生成的代码还不能保证绝对正确，甚至有些隐蔽的错误，不容易被新手开发者发现。从这个角度来看，大模型对有经验的开发者助益更大。</p><p></p><h2>大模型如何改变传统单测生成？</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：在⼤模型出现以前，传统的智能单测⽣成⽅法是什么样的？存在哪些痛点？</blockquote><p></p><p>&nbsp;</p><p>张树波：传统的单测生成应用最广、最成功的是基于搜索的单测生成，也就是很多场景都会提到的（search based software testing - SBST），其中集成了非常多的程序分析技术，包括各种各样的静态分析、动态分析以及遗传算法甚至 constrain solving。但因为语言的特性不同，同样的分析技术对不同的语言是需要重新实现的。虽然测试生成的原理在不同语言是通用的，但是强依赖于软件分析技术，那么每新增一种新的语言支持，就需要适配一整套分析技术，成本较大。另外，精确的分析可能会依赖于编译产物，例如动态分析，因此要求目标项目进行编译后才能进行测试生成，提高了生成所需的前置准备要求。</p><p>&nbsp;</p><p>而基于模型的生成可以直接分析源码，无需编译，降低生成的要求，大大扩大适应场景。近几年来应用&nbsp;repository mining 提升 test generation 甚至 program&nbsp; repair 效果的工作也在逐渐的增加，说明 NLP 中的一些假设在软工领域也是成立的，比如现有 repository 中包含了 test generation 甚至 program repair 中需要的知识，大家也做了相应的尝试，学习历史知识并应用到新的任务中在软工领域也是大家认可的思路。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：应⽤⼤模型后，智能单测⽣成⽅法发⽣了哪些变化？实际效果如何？能完全替代传统的智能单测⽣成⽅法吗？</blockquote><p></p><p>&nbsp;</p><p>张树波：这里先补充一个业务应用背景，智能单测一般在开发者代码编写过程中(IDE)和在代码提交后(CI)发挥作用，前者要求可读性、正确性，后者要求正确性、覆盖率指标。应用大模型后，智能单测由传统模版生成+遗传算法的方式向端到端的模型生成方式演化。传统单测在正确性和覆盖率指标上仍然比大模型生成的要高，在 CI 过程中，仍占主导位置，大模型在其中作为补充。而在 IDE 中，大模型生成单测的可读性更好，便于开发者修改，因此在 IDE 中单测更倾向使用大模型生成的结果。</p><p>&nbsp;</p><p>我们智能服务团队的主要基于&nbsp;Bloom、starcoder 等开源模型做了测试以及微调，经过测试，其中基于 Bloom 的 70 亿参数模型的生成效果不弱于通用版 ChatGPT 的水平，并且在低端显卡（A30）上的推理时延只有 ChatGPT 的 25%。目前，我们的大型模型单元测试生成分支覆盖率在实际项目中达到 56%，同时在抖音的 Android、iOS 双端落地， 问题有效性达到 80%，修复率 65%。同时我们也正在试用火山方舟上大模型的单测生成能力，效果正在评估中。</p><p>&nbsp;</p><p>整体来看，大模型仍有一定局限，发展有个过程，各有千秋，取长补短，可以融合应用1+1&gt;2，不同场景可以有不同的应用方式。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：⼤模型在智能单测⽣成中的应⽤原理是什么？</blockquote><p></p><p>&nbsp;</p><p>张树波：大模型单测生成属于代码生成、文本生成的范畴，旨在通过大模型完成端到端的单测代码生成。大模型单测生成输入是待测方法、以及上下文，输出为单元测试函数。随着模型规模的提升，模型的代码理解、单测生成能力也大幅提升。</p><p>&nbsp;</p><p>目前智能服务团队内使用的大模型基座主要是开源模型，例如 Bloom、Starcoder，基于以上大模型，我们对裸模型以及使用单测训练数据微调之后模型，分别做了评估，当前选择了基于 Bloom7B 的微调模型落地。同时我们团队在 Java、Swift、Go 等多种编程语言的大模型落地计划，广泛收集了公开数据集、业务数据集用于微调。</p><p></p><h2>如何提升⼤模型单测⽣成准确性？</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：您提到⼤模型在单测⽣成任务上仍存在模型幻觉和测试分⽀覆盖不全的问题，对于这两个问题，字节有哪些解决思路？如何提升⼤模型单测⽣成准确性？</blockquote><p></p><p>&nbsp;</p><p>张树波：当前我们使用单测生成任务数据在大模型做了微调，让大模型专注单测生成。实验表明，通过构建高质量的训练数据，可以显著提升大模型单测分支覆盖率指标。基于微调后的大模型，通过引入以编译器、静态分析结果作为奖励的强化学习，可以进一步缓解模型幻觉的问题。</p><p>微调和强化学习的基本假设是模型在预训练阶段学习到了代码相关知识，通过微调或强化学习，可以激发模型的潜力，或让模型跟随特定偏好，输出更好结果。如果预训练阶段没有过多的对应任务领域的语料，通过继续预训练的方式可以让模型适配这一领域，然后进行后面的微调和强化学习，可以取得更好的结果。</p><p>&nbsp;</p><p>除了以上方式，另外一种简单粗暴的方式是提升模型规模，规模越大，能力上限越高。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：除此之外，⼤模型在单测⽣成中还有那些局限性？是否会遇到数据质量问题？是否需要考虑隐私和安全问题？有哪些措施可以确保数据安全？</blockquote><p></p><p>&nbsp;</p><p>张树波：大模型在单测生成瓶颈在能给大模型提供多少背景信息，如果是一个简单的函数，没有涉及任何其他自定义的类，大模型未来可以完美解决，但是涉及其他的类的，甚至是多层的，外层信息稀疏性，会提高输入的上下文输入长度，在实际落地中会在输入长度和生成效果之间做一个取舍。微调数据质量非常重要，决定模型是否可用的关键因素。关于数据安全问题，火山方舟提出了全方位的大模型安全架构，为模型训练方和使用者提供安全可信环境。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：在⼤模型助⼒智能单测⽣成的过程中，字节团队内还积累了哪些经验和教训？对于希望在项⽬中应⽤⼤模型进⾏智能单测⽣成的团队，您会给他们提供哪些建议？</blockquote><p></p><p></p><p>张树波：不仅是在大模型助力智能单测生成这个方向，所有大模型X某某类似的应用落地都是一项系统工程。在大模型落地过程中，其他兄弟团队给予了大量的经验和技术支持。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：您认为在⼤模型助⼒智能单测⽣成⽅⾯，还有哪些需要进⼀步研究和探索的领域/挑战？⼤模型在智能单测⽣成领域的未来发展趋势是什么样的？</blockquote><p></p><p>&nbsp;</p><p>张树波：目前，我们对于 LLM 的应用仍比较初级，所以首先是最基础的研究，如何正确激发大模型在单测任务上的潜力，让大模型发挥全部的效果。目前我们探索的手段包括但不限于任务微调、prompt engineering、RL，然后是下一个阶段，如何让模型不断地增强在特定场景中的效果。另外，大模型的能力和发展让原本一些无法通过自动化解决的问题有了新的可能性，比如经典的 oracle problem，不仅仅是困扰单测生成，GUI 的测试、program repair 的落地都受限于这个经典问题。如果大模型能够解决 oracle problem，刚才提到的多种软工技术，会迎来又一个落地的春天，而我们对于这个趋势充满信心。</p><p></p><h2>“大模型将对研发模式产生颠覆性改变”</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：⼤模型在软件研发⼯作流中最⼤的价值是什么？⼤模型对软件研发⼯作流的改变，将会如何影响软件开发⾏业的未来发展趋势？</blockquote><p></p><p>&nbsp;</p><p>张树波：毫无疑问，大模型将对研发的模式产生颠覆性的改变，但这个改变并不会在一夜之间就发生，会是一个持续渐进的过程，三年五年甚至十年。随着大模型的不断发展和进化，对于研发工作流的影响程度会逐渐加深加强。副驾驶 Copilot 是一种比较可能的切入和演进方式，一开始会在一些比较合适的小场景，Copilot 以半自动化的方式对特定任务进行赋能和提效（比如单测的生成），然后随着模型对代码的理解能力和推理能力增强，推理结果置信度提升，模型在任务中的重要程度逐步增加，在一些任务上达到和人类同等重要的参与程度。</p><p>&nbsp;</p><p>同时，能力可以泛化推广到其他相似或者相关的任务，比如 defect detection、fault localization、program repair 等等，成为开发者的“强化外骨骼”或者最佳搭档。甚至有可能在不远的将来，实现通过 prompt 研发和调试软件，就像《西部世界》中的场景一样。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：⽬前市⾯上存在很多结合⼤模型的研发效能⼯具，但在⼀些企业的端到端落地过程中并不理想，也没有实现提效的突破，这背后可能存在哪些问题？不同规模的企业如何通过⼤模型实现最优的研发效率和质量？</blockquote><p></p><p>&nbsp;</p><p>张树波：大模型适合做推理任务，这是之前单体小模型不具备的能力。在这个基础上，可以反观大模型是否在做这类事情。另外当前市面上的开源大模型或者大模型API都是通用大模型接口，如果直接在某个领域应用可能存在领域的 gap。大模型本身也存在问题，例如大模型生成内容是有偏的，而且存在模型幻觉、推理错误等问题。同时研发效能工具的需要结合具体业务落地，我们智能服务团队，在抖音、直播、剪映做了很多开创性的研发效能实践，欢迎大家与我们合作。</p><p>&nbsp;</p><p>大模型应用可以分为几个层次，API 调用、模型微调、模型继续预训练、模型预训练，成本依次呈几何级数递增，不同规模企业可以简单衡量下投入产出比，来确定在哪个层面应用大模型。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：⼤模型会对程序员带来哪些冲击？程序员和⼤模型如何更好地共⽣，实现1+1＞2的效果？</blockquote><p></p><p>&nbsp;</p><p>张树波：我不认为大模型会减少对程序员的需求量，因为现在大模型还不能替代程序员，也不能为最终结果负责。在我们智能服务团队的实际业务中，我们把程序员当成客户，模型生成的单测为程序员服务，自动化单测检测出来的问题需程序员解决，大模型和程序本身是共生的关系。</p><p>&nbsp;</p><p>大模型生成代码能力增强的同时，需要程序员提升自己的专业能力，能快速判断大模型生成的代码是否正确以及生成质量的高低。程序员能力越强，使用大模型生成代码的质量也会越高，因为通过使用不同的 prompt，可以生成不同质量的代码。程序员应该拥抱大模型，它可以提高代码编写效率，对于一些常识性的问题，它也能做到有问必答，省去网上搜索的时间。</p><p></p><h4>嘉宾简介</h4><p></p><p>张树波，字节跳动算法专家，清华大学硕士毕业，先后就职于 vivo、字节跳动。从事 NLP 算法多年，在智能单测、智能客服、语音助手等业务场景有丰富的落地经验。将在 QCon 北京 2023 分享题为《大模型助力智能单测生成》的演讲。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c15a05e34162b1c3ecd3027c46bd0cb.png\" /></p><p></p>",
    "publish_time": "2023-08-09 18:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "DFS 将联合支付宝在全球率先推广数字化营销；伦敦证券交易所集团与微软合作定制金融大语言模型；腾讯云与中国信通院联合牵头开展国内首个大模型标准的编制工作｜...",
    "url": "https://www.infoq.cn/article/o4q37QrC4YRdzLISslL5",
    "summary": "<p></p><p>DFS 迪斐世将联合支付宝在全球范围内 15 个市场率先推广数字化营销；讯飞星火智能客服产品即将迎来重大版本升级；京东供应链金融科技探索大模型应用；\"问道新九方\" 2023&nbsp;九方智能投顾数字人发布会即将开启；京东应用言犀大模型助力服务效率提升；伦敦证券交易所集团与微软合作定制金融大语言模型；黑石公司转向投资 AI 数据中心；国泰君安与阿里云达成合作，聚焦云计算技术、AI 大模型在金融场景的应用；腾讯云与中国信通院联合牵头开展国内首个大模型标准的编制工作。一起来看看本周这些金融科技新闻详情！</p><p></p><h2>DFS 迪斐世将联合支付宝在全球范围内 15 个市场率先推广数字化营销</h2><p></p><p></p><p>8 月 1 日 DFS 迪斐世和蚂蚁集团宣布达成全球战略合作，并一起发展 DFS 迪斐世的线上与线下商店的支付和数字化营销。 DFS 迪斐世将联合支付宝在其全球范围内多个市场的多家商店中为客户提供端到端的数字化体验服务，以及会上线 D-Store 。</p><p></p><h2>讯飞星火智能客服产品即将迎来重大版本升级</h2><p></p><p></p><p>日前科大讯飞表示，讯飞星火认知大模型 V2.0 升级发布会将于 8 月 15 日举办，届时会发布该大模型的多项能力及应用。讯飞星火认知大模型可以真正地切入细分场景，并准确定位到场景需求，对于客户的购买需求也可迅速理解。</p><p></p><h2>\"问道新九方\" 2023 九方智能投顾数字人发布会即将开启</h2><p></p><p></p><p>“问道新九方” 2023 九方智能投顾数字人发布会将在 8 月 8 日 20:00 正式开幕。此次发布会将聚焦AI等先进科技，由九方财富与华为云、科大讯飞学习先进经验、方法，此次将要发布的的智能投顾数字人会更加具有专业性，且服务能力更强，通过强大的AI算力以及相关的知识支撑，这款AI产品将会更加的具有智能化、专业化的特点，更好地服务于金融业务。</p><p></p><h2>京东应用言犀大模型助力服务效率提升</h2><p></p><p>&nbsp;</p><p>目前，京东财富在机构基金代销系统中已经上线了根据大模型技术进行应用的“言犀有方”功能，在相关的应用场景中可以大大提升服务效率；同时在 B 端，京东财富也为金融机构提供了场外基金电子化交易、服务。未来，京东将继续通过先进技术助力服务效率提升，这对于京东的大模型应用场景及相关业务的增加也具有重要作用。</p><p></p><h2>伦敦证券交易所集团与微软合作定制金融大语言模型</h2><p></p><p></p><p>在伦敦 IPO 市场持续低迷的情况下，伦敦证券交易所集团希望 AI 相关产品可以成为新的业务线。伦敦证券交易所集团于当地时间 8 月 3 日发布半年报，这份报告中所提到的“微软元素”吸引了市场的关注焦点，其在财报中表示公司与微软的合作进展顺利。需要注意的是，在大语言模型的建设中，金融公司的一些数据是较需要保密的，在这个过程中金融机构对于数据安全的要求高，因此在建设这类服务于金融公司语言模型时，数据隔离就很重要了。</p><p></p><h2>黑石公司转向投资AI数据中心</h2><p></p><p></p><p>近月来，黑石一直在筹集现金准备建造数据中心以应对 AI 蓬勃发展所带来的算力需求增长，它承诺投入80 多亿美元来为若干个大型科技公司建造新的数据中心。由于生成式 AI 客观上对于存储容量、算力的需求都较大，数据中心的建设可以满足预期可能存在的、增长的算力需求。从它的投资转向也可看出这家公司在关注 AI 基础设施建设投资建设以更好地适应新的技术增长趋势和市场需求。</p><p></p><h2>国泰君安与阿里云达成合作，聚焦云计算技术、AI大模型在金融场景的应用</h2><p></p><p></p><p>7 月 28 日，国泰君安证券股份有限公司与阿里云计算有限公司签署战略合作协议。根据此协议，双方将以阿里云在云原生架构金融应用方面的技术优势为基础，打造具备强大算力、成本低且安全的云平台。另外，双方还将以通义系列大模型能力为基础去建造国泰君安专属大模型，加快大模型在相关业务场景中的落地。</p><p></p><h2>腾讯云与中国信通院联合牵头开展国内首个大模型标准的编制工作</h2><p></p><p>&nbsp;</p><p>7 月 28 日，腾讯云与中国信通院宣布联合牵头开展中国首个金融行业大模型标准的编制工作，即《面向行业的大规模预训练模型技术和应用评估方法第 1 部分：金融大模型》编制。其中对于金融行业大模型的评估方法覆盖了多个应用场景。随着行业大模型的应用逐渐在各个应用场景落地，腾讯云表示未来腾讯会进一步加强推动大模型落地的相关工作、塑造大模型产业发展生态。</p>",
    "publish_time": "2023-08-09 18:03:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "CommunityOverCode Asia 首次中国线下峰会即将开启！亮点抢先看",
    "url": "https://www.infoq.cn/article/pPjhzLLk5t55KB24IqoW",
    "summary": "<p>从一个项目成长到今天，Apache 软件基金会（ASF）已经成为目前为止全球最大的开源软件基金会，维护着包括 350 多个顶级项目以及数十个孵化器项目，为全球提供着几十亿甚至上百亿美元市值的开源软件，是推动全球开源软件发展的重要力量。</p><p></p><p>伴随着全球开源项目数量的迅速增长，我们很高兴看到越来越多中国的开发者与开源项目加入，以及越来越多高质量项目的涌现。种种迹象表明，中国在国际基金会的影响力愈发重要。</p><p></p><p>今年，Apache 软件基金会的官方全球系列大会 CommunityOverCode Asia（原ApacheCon Asia）将首次在中国线下举办，囊括 17 个论坛方向、上百个前沿议题，期待和各个层次的参与者，共赴开源盛宴，探索“今天的明天技术”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52c1a9087adb473613cf8e3c09e9c70c.png\" /></p><p></p><h2>新力量：ASF Asia 在中国的首次线下峰会</h2><p></p><p></p><p>Apache 软件基金会主席 David Nalley 在去年的公开演讲中指出，过去 Apache 大约有 25% 的流量来自中国，如今，这一数字超过了 40%。为了更好地服务于亚太快速增长的 Apache 用户和贡献者，我们于 2021 年首次举办了针对亚太地区时区的 CommunityOverCode 在线会议 Apache 亚洲大会，成功推动 CommunityOverCode Asia 来到中国，并吸引来自任何地方的参与者加入。</p><p></p><p>为了更好地服务于亚太快速增长的 Apache 用户和贡献者，我们于 2021 年首次举办了针对亚太地区时区的 CommunityOverCode 在线会议 Apache 亚洲大会，成功推动 CommunityOverCode Asia 来到中国，并吸引来自任何地方的参与者加入。</p><p></p><p>随着政策的开放，我们迫切需要一个线下聚会来团结中国的开发者，为那些对你们很重要的项目作出贡献，帮助我们共同构建开源未来。</p><p></p><p>作为 ASF Asia 在中国的第一次线下峰会， 我们不但邀请到国内 140 多位讲师，还有 20 多位海外讲师将到现场与大家交流。 与会者将近距离了解到 Apache 项目的最新突破和 Apache 孵化器中即将到来的创新，以及开源开发和以 “The Apache Way”领导社区驱动的项目。</p><p></p><h2>新机会：新增性能工程专题</h2><p></p><p></p><p>在 Apache，很多项目都是针对具有软件性能和可扩展性挑战的领域，如网络、云、数据库、流数据、大数据、数据分析、搜索、地理空间等，而其他项目则提供性能工程工具，如基准测试、测试、监控等，并被广泛使用，因此，今年我们新增了一个性能工程的专题，将为不同软件类别和成熟度的项目之间提供交叉融合的机会。</p><p></p><h2>新流行：AI时代下 DataOps 的最新发展</h2><p></p><p></p><p>DataOps 专题是硅谷最流行的大数据工程方向之一，涵盖了以前 ApacheCon Asia 当中数据集成、数据调度、数据治理等方向内容，主要是面向大数据研发、数据、调度流程提效的方向。</p><p></p><p>过去谈到 DataOps 只是 BI 分析的数据准备工作，时至今天大模型时代，DataOps 的数据准备和调度已经是如何给大模型供给数据，如何利用调度工具训练企业内部的大模型，Apache 有超过 10 个开源项目都在这个领域，在本话题当中各位可以感受到在 AI 大模型时代下 DataOps 的最新发展。</p><p></p><h2>新场景：深入业务场景的数据管理实践</h2><p></p><p></p><p>物联网/工业物联网中的数据管理一直是全球实现多行业智能转型关注的重点。今年我们依旧设置物联网/工业物联网专题，围绕 Apache 软件基金会中 IoT 相关项目的最新进展、核心技术以及用户经验分享等话题展开研讨。</p><p></p><p>物联网/工业物联网行业发展至今，对数据管理的应用场景日益丰富。相比往年集中在单一项目的创新成果与优秀实践分享，今年有更多议题侧重于多项目在各业务场景的联动配合，为用户提供更加完善的数据全链路管理解决方案。同时，针对物联网构建数据管理方案中所需求的具体需求场景，如智能厂务、流数据平台等，本次也有相比往年更多的议题基于这些具体场景进行深入讨论。</p><p></p><h2>新关注：HDFS 在行业中的应用</h2><p></p><p></p><p>近年来伴随 5G、物联网、人工智能等领域的快速发展，数据量规模不断增大，同时随着大数据应用的多样化发展，对数据的利用也更加成熟与深入，更大数据量以及更加灵活的数据处理场景对 HDFS 的数据存储与数据读写吞吐提出了越来越高的要求。</p><p></p><p>在数据存储和计算方向的众多议题投稿中，前两年都没有提交的 HDFS 项目相关议题，今年屡屡被讲师提及。</p><p></p><h2>新趋势：关于 Tomcat 安全性的探讨</h2><p></p><p></p><p>除了新增的论坛方向，各论坛的议题投稿也反映了一些新的趋势。</p><p></p><p>今年 Tomcat 分论坛的议题和往年相比，新增了 Tomcat 和云原生，Serverless  结合的内容，在 Tomcat 安全性方面也带来了相关介绍，同时也不乏来自互联网一线的最佳实践。比较热门的方向是 Tomcat 与 GraalVM 的结合，以及 Tomcat 可观测性的探索实践。</p><p></p><h2>新思考：打造开源社区领导力</h2><p></p><p></p><p>在开源的开发方式中，职位和头衔都是毫无意义的，开源的开发更加重视开放和共识，想要在社区中赢得信任、获得影响力的唯一方式是做出贡献。代码贡献决定影响力和技术方向，这也是开源项目的最大特色。</p><p></p><p>尽管开源项目已经无数次证明了自身协作的价值 ，但企业如果拥有能够影响项目进展的能力，将比同业者更具优势。因此，在开源项目中打造和维护领导力是企业的关键战略和目标。</p><p></p><p>在大会召开之前，我们特别准备了一个 Community Leadership 大会，希望帮助与会者了解在开源项目的领导力文化和角色是如何做决策的，作为公司又该如何在开源项目中打造影响力、并在开源社区中形成领导力，以及在开源社区中成为一个优秀的领导者的特质。</p><p></p><p>本次大会将于  8 月 18 日至 8 月 20 日，在北京丽亭华苑酒店举行，期待与您线下相见。</p>",
    "publish_time": "2023-08-09 18:09:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]