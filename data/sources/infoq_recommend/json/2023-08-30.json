[
  {
    "title": "Java近期新闻：Payara Cloud、MicroProfile Telemetry、Foojay.io日历、JVM语言峰会",
    "url": "https://www.infoq.cn/article/KPPWppqtOcT3czOYQqMf",
    "summary": "<p></p><h4>JDK 21</h4><p></p><p>JDK 21<a href=\"https://jdk.java.net/21/\">早期访问构建</a>\"<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B35\">Build 35</a>\"在上周发布，其中包括Build 34的<a href=\"https://github.com/openjdk/jdk/compare/jdk-21%2B34...jdk-21%2B35\">更新</a>\"，主要是修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2021%20and%20%22resolved%20in%20build%22%20%3D%20b35%20order%20by%20component%2C%20subcomponent\">问题</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p></p><h4>JDK 22</h4><p></p><p>JDK 22<a href=\"https://jdk.java.net/22/\">早期访问构建</a>\"<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-22%2B10\">Build 10</a>\"在上周发布，其中包括Build 9的<a href=\"https://github.com/openjdk/jdk/compare/jdk-22%2B9...jdk-22%2B10\">更新</a>\"，主要是修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2022%20and%20%22resolved%20in%20build%22%20%3D%20b10%20order%20by%20component%2C%20subcomponent\">问题</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/22/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.org/projects/jdk/22/\">JDK 22</a>\"和<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\"，我们鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告Bug。</p><p></p><h4>GraalVM</h4><p></p><p>在迈向1.0版本的道路上，Oracle实验室发布了<a href=\"https://github.com/graalvm/native-build-tools/blob/master/README.md\">Native Build Tools</a>\"的<a href=\"https://github.com/graalvm/native-build-tools/releases/tag/0.9.24\">0.9.24版本</a>\"，这是一个GraalVM项目，其中包含与GraalVM原生镜像进行互操作的插件。这个最新版本带来了一些显著的变化，包括：支持Profile-Guided Optimization（PGO）；从使用ImageClassLoader来发现JUnitPlatformFeature测试改为使用ClassLoader ，以消除在本地映像构建期间的饿汉式类初始化错误；完善GraalVM安装说明。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/graalvm/native-build-tools/compare/0.9.23...0.9.24\">变更日志</a>\"。</p><p></p><h4>Spring Framework</h4><p></p><p><a href=\"https://spring.io/projects/spring-cloud\">Spring Cloud</a>\" 2023.0.0-M1的<a href=\"https://spring.io/blog/2023/08/10/spring-cloud-2023-0-0-m1-aka-leyton-has-been-released\">第一个里程碑版本</a>\"（代号为Leyton）提供的新特性包括：Spring MVC和Jakarta Servlet的一个实现；支持Java HttpClient类；对Spring Cloud Commons 4.1.0-M1和Spring Cloud Task 3.1.0-M1等子项目的里程碑式升级。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2023.0-Release-Notes\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-modulith\">Spring Modulith</a>\" 1.0.0的第一个候选版本带来了Bug修复、依赖项升级和新特性，包括：避免过早初始化SpringModulithRuntimeAutoConfiguration 类，以避免代理警告；改进数据库交互，将事件发布标记为已完成；允许ApplicationModulesExporter 类将输出写入文件。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/spring-projects/spring-modulith/releases/tag/1.0.0-RC1\">发布说明</a>\"。GA版本的发布计划将于2023年8月下旬在<a href=\"https://springone.io/\">VMware Explore大会的SpringOne环节</a>\"宣布，InfoQ后续将跟进报道。</p><p></p><h4>Payara Cloud</h4><p></p><p>对于<a href=\"https://www.payara.fish/products/payara-cloud/\">Payara Cloud</a>\"云原生运行时服务，Payara<a href=\"https://blog.payara.fish/payara-cloud-15-day-free-trial\">宣布</a>\"为考虑采用这类服务的组织提供15天的免费试用。Payara声称，使用Payara Cloud可以缩短开发周期，提高运营效率，并节省培训Kubernetes开发人员的费用，因为Payara Cloud在后台搞定了关于的Kubernetes一切。</p><p></p><h4>Quarkus</h4><p></p><p>Red Hat<a href=\"https://quarkus.io/blog/quarkus-3-2-4-final-released/\">发布</a>\"了<a href=\"https://quarkus.io/\">Quarkus</a>\"的3.2.4.Final版本，其中包含一些值得注意的变化，包括：记录在运行测试时可能比较重要的Maven配置选项；修复了@RouteFilter注解在使用Quarkus 3.2.0.Final时无法处理WebSocket请求的问题；修复OpenTelemetry (OTEL) SDK自动配置忽略OTEL服务名称而使用Quarkus应用程序名称的问题。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.2.4.Final\">变更日志</a>\"。</p><p></p><h4>MicroProfile</h4><p></p><p>在迈向<a href=\"https://microprofile.io/\">MicroProfile</a>\" 6.1的道路上，MicroProfile工作组提供了MicroProfile Telemetry 1.1规范的<a href=\"https://github.com/eclipse/microprofile-telemetry/releases/tag/1.1-RC1\">第一个候选版本</a>\"，其中有一些显著的变化，包括：说明了哪些API类必须对用户可用；不依赖于时间戳的测试实现；说明了当当前span或baggage发生变化时Span 和Baggage bean的行为。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/eclipse/microprofile-telemetry/milestone/1?closed=1\">问题列表</a>\"。</p><p></p><h4>OpenXava</h4><p></p><p><a href=\"https://openxava.org/\">OpenXava</a>\" 7.1.4版本<a href=\"https://openxava.org/blog/openxava-7.1.4-released\">发布</a>\"，主要带来了一些依赖项升级和重要的修复，包括：当@DisplaySize注解与@Column(length=255)一起使用时，值大于50时会被忽略；创建新实体时同时上传多个文件会发生文件丢失；如果应用程序名称包含下划线，则移动列实现自定义列表将不起作用。要了解关于该版本的更多细节，请查看<a href=\"https://github.com/openxava/openxava/releases/tag/7.1.4\">发布说明</a>\"。</p><p></p><h4>Foojay.io</h4><p></p><p>Foojay.io<a href=\"https://foojay.io/calendar/\">社区日历</a>\"现在能够自动导入由Java用户组维护的单独的Meetup页面。这样一来，JUG就不必在Foojay.io日历中手动输入Meetup活动了。要使用这项功能，JUG负责人需要在Foojay.io&nbsp;<a href=\"https://bit.ly/join-foojay-slack\">Slack</a>\"频道上注册，并指定是每日自动更新还是每周自动更新。</p><p></p><h4>JVM语言峰会</h4><p></p><p><a href=\"https://www.linkedin.com/in/sharatchander/\">Sharat Chander</a>\"是Oracle Java和容器原生产品管理及开发者关系高级总监。他向InfoQ概要介绍了2023 JVM语言峰会的内容，涉及<a href=\"https://openjdk.org/projects/mlvm/jvmlangsummit/agenda.html\">会议议程</a>\"和<a href=\"https://openjdk.org/workshop\">OpenJDK提交者研讨会</a>\"。</p><p></p><p></p><blockquote>上周召开了第15届JVM语言峰会。这个为期三天的峰会在加州的圣克拉拉举行，由Oracle Java语言和JVM团队主办，旨在为语言设计人员、编译器编写人员、工具构建人员、运行时工程师和VM架构师提供一个开放的技术协作机会。&nbsp;本次峰会邀请了JVM和JVM编程语言的创建者来分享他们的经验。同时，本次峰会也邀请了使用类似技术的非JVM开发人员参加，其中还有人就他们自己选择的运行时、虚拟机或语言作了发言。与会者来自15家公司、30个国家，其中还包括Java Champions杰出计划的11名成员和Java用户组的16名负责人和组织者。&nbsp;峰会还从Leyden、Loom、Panama和Valhalla等项目的维度展开了讨论。此外，峰会还提供了有关Generational ZGC、Class-File API预览功能等方面的见解。</blockquote><p></p><p>&nbsp;</p><p>Chander说：“会议录音很快就会上线，敬请关注！”</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/08/java-news-roundup-aug07-2023/\">https://www.infoq.com/news/2023/08/java-news-roundup-aug07-2023/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/Lc07oHhI7kCfgOdTv1Vb\">Java 近期新闻：Grails 6.0、PrimeFaces 13.0、JUnit 5.10、GraalVM、TornadoVM、新的 JEP 草案</a>\"</p><p><a href=\"https://www.infoq.cn/article/ClW8eLeOxRUqqpHWOJCC\">Java ZGC 垃圾收集器全面增强</a>\"</p>",
    "publish_time": "2023-08-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Svelte使用心得：在个人项目中表现不错，但在大型企业项目中仍有待观察",
    "url": "https://www.infoq.cn/article/m0YhBSVkH1E2xQzdSLvT",
    "summary": "<p>作者用了1个月时间开发了一款个人RSS阅读器，并选择了Svelte和SvelteKit作为Web客户端的工具。这个选择的主要目的是为了评估这些工具是否适合在大型项目中使用。作者发文分享了对于Svelte的一些思考，这篇文章引起了Hacker News上读者的关注，并且被顶到了首页。fenomas和illilarian这两位用户谈到了他们对于过渡和动画API的看法。如果需要在Svelte管理的元素进入和离开DOM时对其进行动画处理，那么作者“吐槽”的这些API就非常有用。看来作者之前的抱怨不成立了。如果是你，你会把Svelte用到大型公开项目中吗？</p><p>&nbsp;</p><p>以下是这篇“吐槽”原文，由 InfoQ 翻译。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p>过去一个月来，我开发了一款个人RSS阅读器。</p><p>&nbsp;</p><p>在Web客户端这边，我选的是Svelte和SvelteKit，主要是为了评估这些工具适不适合在大型项目里使用。</p><p>&nbsp;</p><p>下面跟大家聊聊我自己对于Svelte的一点思考。</p><p>&nbsp;</p><p></p><h2>开篇总结</h2><p></p><p>&nbsp;</p><p>总的来说，我挺喜欢Svelte的使用体验。它的亮点在组件格式、内置store和事件调度程度API。</p><p>短板主要是响应式语句($)、await 块和内置的过渡与动画API。</p><p>&nbsp;</p><p></p><h2>组件格式</h2><p></p><p>&nbsp;</p><p>Svelte的组件格式最得我心。在编写.svelte文件时，默认上下文跟浏览器是完全相同的，都是用HTML。</p><p>&nbsp;</p><p>以下片段来自Svelte文档（包括示例标记、JS和CSS），应该可以说明问题：</p><p>&nbsp;</p><p><code lang=\"null\">\n<!-- markup (zero or more items) goes here -->\n</code></p><p><code lang=\"null\">1 + 2 is: {add(1, 2)}</code></p><code lang=\"null\">\n</code><p></p><p></p><p>再来跟React对比一下，这里的默认上下文是JavaScript，而HTML要通过JSX进行交错：</p><p></p><p><code lang=\"null\">import '../some-styles.css'; // styles are imported into JS files\nexport function SomeComponent() {\n  // logic can go anywhere\n  function add(a, b) {\n    return a + b;\n  }\n  // markup is returned from JS functions\n  return </code></p><p><code lang=\"null\">{`1 + 2 is: ${add(1, 2)}`}</code></p><code lang=\"null\">;\n}</code><p></p><p></p><p>我没法斩钉截铁地告诉大家，这种方式就一定更好，比如“Svelte的组件格式能让团队在构建组件时，比某某框架快多少倍。”这个我确实不敢说。</p><p>&nbsp;</p><p>但我觉得组件格式确实是很多朋友喜爱Svelte的原因。这可能是因为浏览器也优先使用HTML，所以用Svelte的话上下文切换较少，但我不确定是不是这样。总之，我个人非常喜欢。</p><p>&nbsp;</p><p></p><h2>内置store</h2><p></p><p>&nbsp;</p><p>Svelte为状态管理提供内置的store选项。</p><p>&nbsp;</p><p>其实大家对于用户界面库/框架应该关注什么、没必要关注还有争议。而Svelte聪明的地方，就在于它承认状态管理可能会成为问题，而且提供了相应的解决方案。大家可以根据需要使用或者扩展。</p><p>&nbsp;</p><p>更贴心的是，这个解决方案不像React上下文那样跟组件树紧密相关。</p><p></p><p></p><h2>事件调度程序 API</h2><p></p><p>&nbsp;</p><p>Svelte提供一个内置API可用于创建、分派和在父元素上侦听CustomEvent。</p><p>&nbsp;</p><p>在基于单向数据流概念构建的系统中，其实很难为Web事件建模。从本质上讲，Web的事件模型会让数据向上流动。</p><p>&nbsp;</p><p>Svelte承认用户可能需要向树结构的上方发送数据，并提供一个使用Web平台原语的API。我必须给它点个赞！</p><p></p><p></p><h2>响应式语句</h2><p></p><p></p><p>我发现Svelte的响应式语句有点让人摸不着头脑。</p><p>&nbsp;</p><p>Svelte的基本响应基于变量分配。通过文档中的以下示例，我已经弄明白了：</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p>&nbsp;</p><p>这个很合理。</p><p>&nbsp;</p><p>但接下来在引入响应式&nbsp;$标签时，文档给出了以下示例：</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p></p><p>考虑到这是文档里关于该主题的第一个示例，似乎显得太复杂了。但只要认真看下来，还是能理解其中逻辑的。</p><p>&nbsp;</p><p>但Svelte文档又提到：</p><p></p><p></p><blockquote>请务必注意，响应块在统计时会通过简单的静态分析进行排序，所有编译器查看的都是分配给块本身、并在块内部使用的变量，而不在它们调用的任何函数当中。这意味着以下示例中，yDependent不会随着x的更新而一同更新：</blockquote><p></p><p></p><p><code lang=\"null\"></code></p><p>&nbsp;</p><p>这种“玄学”般的设计，让我在很多情况下都想不明白为什么组件无法更新。</p><p>&nbsp;</p><p>最终我发现，确实很难明确认定$&nbsp;标签是否起效。有时候我用起来一切正常，但有时候用起来就没有效果，非常诡异。</p><p>&nbsp;</p><p>所以我决定离它远点。另一个类似的问题是访问store值，它跟$&nbsp;的情况差不多，时灵时不灵。</p><p>&nbsp;</p><p>正是$&nbsp;标签阻止了我在大型项目中使用Svelte。这是Svelte的核心部分，不可能彻底回避，而且我觉得由此引发错误的可能性很高、而且影响范围很大。</p><p></p><p></p><h2>Await块</h2><p></p><p>&nbsp;</p><p>Svelte提供{#if ...}&nbsp;和&nbsp;{#each ...}&nbsp;语法作为标记渲染的主要控制流方法。它还提供{#await ...}，可以根据Promise的状态来决定渲染什么。</p><p>&nbsp;</p><p>我喜欢这个设计思路，但在实践中总是以重构告终。在Promise被解决或拒绝之后，我总得再调整一下才能开始渲染，所以我可不打算每次运行服务时都用它。</p><p>&nbsp;</p><p>而且该逻辑也不属于渲染代码中的内联。那它到底是怎么工作的？</p><p>&nbsp;</p><p>把{#await ...}剔出来并放进</p>",
    "publish_time": "2023-08-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百亿级向量检索的向量数据库是如何构建的？",
    "url": "https://www.infoq.cn/article/KrM5DtGLLB121mNVvJ4S",
    "summary": "<p></p><blockquote>采访嘉宾｜李莅，百度智能云大数据技术负责人</blockquote><p></p><p>&nbsp;</p><p>OpenAI 掀起的这波 AI 变革，让向量数据库越来越受关注。</p><p>&nbsp;</p><p>AI 技术不断向前发展，一个核心驱动因素，就是背后的存储、处理和分析大量数据所需要的强大基础设施也在不断发生进步。这波“新基建”浪潮也催生出又一颗冉冉升起的新星——向量数据库，一种用于管理非结构化数据，包括数字形式的文本、音频、图像和视频的强大解决方案。</p><p>&nbsp;</p><p>随着市场对 AI 基础设施需求的不断增加，向量数据库预计也将保持强劲的发展势头，并一步步成为未来 AI 技术愿景的重要基石。</p><p>&nbsp;</p><p>受到近期AI火爆现象的影响，更多企业开始大力投资向量数据库以提升算法准确性和效率。据相关统计，2023 年 4 月的 AI 投资领域呈增长趋势，尤其是向量数据库领域的投资活动颇为活跃，Pinecone、Chroma 和 Weviate 等向量数据库初创公司都在这个月获得了融资。</p><p>&nbsp;</p><p>那么，向量数据库究竟是炒作还是刚需？向量数据库在大模型企业中是如何应用的？在向量数据库选型过程中他们是倾向于自研还是从外采购？近期，在北京QCon大会之际，InfoQ有幸采访到了百度智能云大数据技术负责人李莅，听他聊一聊百度在向量数据库技术上的实践和思考。</p><p>&nbsp;</p><p>以下为访谈实录，经编辑。</p><p>&nbsp;</p><p>InfoQ：李莅老师能方便介绍一下您整个的工作经历，包括在目前在百度智能云负责什么工作？</p><p>&nbsp;</p><p>李莅：我毕业后就来百度了，在百度工作了十余年。刚来的时候主要是做基础架构相关的技术工作，专注在消息队列和存储方向。后来百度智能云成立后，就一直在从事云相关的产品研发，偏底层组件的产品，依然是聚焦在消息队列和存储方面。最近几年我开始做大数据方向的工作，包括像Hadoop、ElasticSearch（搜索数据分析引擎）等这些偏技术的开源组件的平台研发，或者是内核优化等工作。</p><p>&nbsp;</p><p>InfoQ：那您是从什么时候开始接触向量数据库？</p><p>&nbsp;</p><p>李莅： 19年左右百度智能云开始计划在ElasticSearch公有云的场景上面去做一些向量的能力，这个时候我开始关注向量检索技术。</p><p>&nbsp;</p><p></p><h2>百度智能云的向量数据库建设技术实践</h2><p></p><p>&nbsp;</p><p>InfoQ：那个时候百度就在有意识地去搞一个向量数据库了吗？</p><p>&nbsp;</p><p>李莅：Elastic&nbsp;Search在广义上它也是属于数据库的一种，它是NoSQL数据库。如果再细分的话，它可以算是文档型的数据库，或者搜索型的数据库，然后我们给ElasticSearch加上了向量检索的能力。由于会有各种搜索场景的需求，所以它会存向量的数据。既然存了向量的数据，ES又是一个搜索型的数据库，自然也要搜索向量的数据，所以我们又给它加上了向量检索的能力。</p><p>&nbsp;</p><p>InfoQ：也就是说Elastic&nbsp;&nbsp;Search这款数据库天然就有这个场景？</p><p>&nbsp;</p><p>李莅：对，天然有这个需求。</p><p>&nbsp;</p><p>InfoQ：那您能具体讲一下Elastic&nbsp;&nbsp;Search上都有哪些更具体与向量搜索相关的能力吗？</p><p>&nbsp;</p><p>李莅：向量其实不复杂。向量在这个大模型火起来之前它主要就是用来做语义搜索的。各种语义数据的搜索，包括最基本的文本的搜索、图片搜索，还有可能有一些视频的缩略图之类的搜索。ElasticSearch上的搜索能力通常就是用向量去表达的，向量本身搜索起来并不难，比如把这个数据从头到尾扫一遍，肯定是可以搜出结果的，但是这样做性能就太差了。所以我们在ElasticSearch上做了两点改造：一是支持了向量数据列式存储格式，二是基于社区开源的向量相似度引擎做了一些搜索加速的改进。</p><p>&nbsp;</p><p>大概17年的时候Facebook开源了Faiss，全称为&nbsp;Facebook&nbsp;AI Similarity Search，是 Facebook AI Research&nbsp;开源的高维向量索引和聚类库，Faiss面向的也是传统的向量场景。到了2020年的时候，我们在ElasticSearch上加了向量数据这个格式，同时把开源的引擎给引进来跟ElasticSearch去做了一个结合，让它能够通过这些向量搜索引擎能够加速向量的检索速度。</p><p>&nbsp;</p><p>在引入Faiss之前，ElasticSearch自身的向量检索速度是非常慢的，需要通过ES原生的Script机制一个一个地去计算相似度，性能上无法满足业务需求。在加上了开源的向量引擎之后，它的性能提高了几十倍，满足了业务需求。</p><p>&nbsp;</p><p>InfoQ：那当时我们为什么没有想到去自研一款搜索引擎？还是说这个开源的Faiss已经足够满足我们的需求了，也就不需要自研了？</p><p>&nbsp;</p><p>李莅：是的，因为社区的已经足够满足需求了，Faiss的表现的还是不错的。把它引入过来之后，我们自己也做了一些改造。</p><p>&nbsp;</p><p>InfoQ：所以现在百度智能云的向量数据库的能力还是基于ElasticSearch+Faiss做的吗？</p><p>&nbsp;</p><p>李莅：基本上是这样的。虽然百度用到了开源的向量引擎，但在向量检索这部分我们做了很多自研工作，比如在将ElasticSearch和Faiss结合的很多地方都做了比较底层的改造。</p><p>&nbsp;</p><p>InfoQ：能详细展开下都做了哪些改造工作吗？</p><p>&nbsp;</p><p>李莅：ElasticSearch是一个开源的数据库，以前主要是用作文本搜索的，它底下核心的引擎是一套叫Lucene的搜索引擎，Lucene有自己的各种格式，当然早期它不支持向量这个格式。首先数据得存进来，并且得存到磁盘上，而且得用Lucene的格式去表达，所以我们在Lucene的基础上自研了一套定制的列式存储格式，这一套是我们自己设计和实现的，可以让向量的数据比较高性能地存进来，并加载到内存里。然后我们做了一些跟社区版向量引擎的对接能力。跟Faiss对接就是让社区的引擎可以在我的存储格式上进行加速计算，同时也做了一些适配工作。</p><p>&nbsp;</p><p>Faiss引擎一开始是有自己的格式的，我们要使两边的格式能够对得上，防止它两边各存一份，带来性能损耗。这就是在Lucene 和 Faiss 这两个东西怎么紧密地结合、发挥最大的效果这件事情上我们做了很多改造。</p><p>&nbsp;</p><p>InfoQ：那你们有没有测试过，在改造之前，它的效果是什么样的？改造之后效果如何？收益如何？有没有一个这样的对比呢？</p><p>&nbsp;</p><p>李莅：整体性能提升了一倍多。</p><p>&nbsp;</p><p>InfoQ：之前这款向量数据库还只是服务于百度内部，那现在它变成了一个对外服务的数据库，它整个产品化的过程耗时多久？</p><p>&nbsp;</p><p>李莅：其实 ES的向量检索能力很早就完成了产品化，大概也就用了一年左右的时间吧，可以说在大模型还没有火起来以前他就已经完成产品化了。</p><p>&nbsp;</p><p>InfoQ：整个期间做了几次优化？然后每次优化它的重点是什么？</p><p>&nbsp;</p><p>李莅：优化这个事情我们是一直在做的，从产品孵化到走向成熟到稳定服务客户，整个过程都是都是在不断地进行优化。比如客户在使用过程中反馈某个接口不是很友好，那我们就会进行针对性的进行优化。。</p><p>&nbsp;</p><p>在 2020年的第一版产品中，我们实现了主要功能。然后到22年我们做了很多关键的优化，推出了第二代版本。然后再到今年，因为大模型的需求来了，我们继续深度优化这个产品，进入到了第三代版本。第三版本相比上一个版本，总体提升了大概一倍多的性能。</p><p>&nbsp;</p><p>目前，第三代版本已经可以支持百亿级的高维度向量的存储和检索需求了，并投入了实际生产中。</p><p>&nbsp;</p><p>InfoQ：目前这款向量数据库它的架构是怎样的？是单机的，还是分布式的架构？</p><p>&nbsp;</p><p>李莅：分布式的，但是单机也可以支持。</p><p>&nbsp;</p><p></p><h2>传统数据库+搜索引擎就是向量数据库了？</h2><p></p><p></p><p>InfoQ：可以看到，大模型火了以后，向量数据库受到了特别高的关注，您是如何看待这种现象呢？您认为向量数据库是一个刚需产品吗？</p><p>&nbsp;</p><p>李莅：是的。大模型是在今年才崭露头角，或者说是在今年才备受瞩目的。虽然在此之前也有类似模型的存在，但是它们并没有像现在这样受到广泛的关注和重视。大模型已经成为今年最热门的话题之一。向量数据库作为大模型的配套设施，具有不可或缺的作用。</p><p>&nbsp;</p><p>从多个方面来看，向量数据库都是大模型的必要设施。首先，大模型自身能够存储的数据是有限的，而大量的各种知识数据需要被存储起来，以供大模型在问答时使用。这些知识数据可以提供给大模型作为输入，从而使其回答更加准确和可靠。这些知识数据也可以成为人类想要大模型回答的内容，以确保其回答更加准确。</p><p>&nbsp;</p><p>另外，向量数据库可以回答更加实时的内容，比如大模型它回答不了最新的数据，比如ChatGPT只能回答2021年和之前的数据，你让它回答2023年的问题，它就会瞎说了。这时，通过一些外置的数据库，当向它提问时，就可以把这些外置数据库中存储的数据直接输给它，这样大模型就可以结合这些数据去做出一个更准确的回答，所以在一些社区和工具链里面，向量数据库都是一个必须的组件。</p><p>&nbsp;</p><p>InfoQ：虽然您认为它是刚需，但也有人认为我可能不是需要一款企业级的向量数据库，而是需要一个向量引擎。我可以在传统的数据库上加一个向量引擎，然后它就变成了一款向量数据库，您觉得他们这样的想法是可行的吗？</p><p>&nbsp;</p><p>李莅：这个想法我认为是完全可行的。大模型在这个方面的核心需求就是向量检索的能力，一般不需要特别复杂的数据库的功能。</p><p></p><p>我们可以通过大模型，或者是各种其他的简化版的模型去做embedding，把各种文档、文字、图片、转化成向量。所以对于向量既要把它们存起来，又要可以被工具链调用，从里面能够查出来。比如像LangChain这种就可以支持很多向量数据库类型，如果我要做数据的增删改查，单纯的向量引擎是搞不定的，但在数据库上加入向量的能力就可以搞定这个事情了。</p><p>&nbsp;</p><p>所以，单从场景和功能出发，我觉得在传统的数据库，或者是一些NoSQL的数据库上去加上向量能力是完全行得通的。</p><p>&nbsp;</p><p>但是，当业务的规模发展得很大之后，那传统的数据库加上向量引擎就不一定能搞得定了。这时可能就需要一个更加跟向量检索耦合的技术实现，来保证向量检索这一部分的性能需求。比如一款大模型应用，要支持上亿用户访问量，这个量级肯定就不是一个传统的数据库可以搞定的，它上面就肯定要做各种架构考量，比如存算分离之类的技术去保证它的规模能够扩展。</p><p>&nbsp;</p><p>InfoQ：那这种传统的数据库加向量插件的方式和AI native的向量数据库两者之间的区别是什么？做AI native的向量数据库有哪些技术难点？</p><p>&nbsp;</p><p>李莅：向量检索算法是向量领域最核心的技术挑战。目前，主流的算法是基于图的算法，部分算法可以使用倒排索引等算法，并结合一些量化技术来降低成本。如果能够自主优化该算法，将会成为核心技术，例如，通过优化算法可以提高性能或吞吐量，这是第一层技术挑战。社区已经对此进行了很好的处理，但真正具备这方面能力的公司很少。</p><p>&nbsp;</p><p>第二层技术挑战是与具体的系统相结合。因为算法需要依托于一个工程实现。这个工程实现通常会选择基于数据库或从头实现一个框架。这个框架的选择会对整体性能产生影响，因此也存在技术挑战。</p><p>如果基于已有的开源系统，成本会降低很多，例如直接在ElasticSearch或Redis上进行开发。因此，许多研究人员会选择成熟的系统来解决工程问题，并且会考虑使用开源社区的引擎。这样，他们可以更专注于开发应用程序和生态系统，例如与AI生态系统、长期存储进行对接，或者开发更多的案例以及上下游工具。因此，技术挑战主要包括两个方面：算法和后端的系统。</p><p>&nbsp;</p><p>InfoQ：向量数据库会像传统数据库一样面临着选型问题吗？您能介绍下，什么样的企业适合什么类型的向量数据库吗？是单机版的还是插件版的，还是企业级的？</p><p>&nbsp;</p><p>李莅：选型的出发点还是要看需求。如果业务量级不是特别大，而且有成本方面的考虑，想尽快用到这个技术，那就可以采用我上面提到的将两个现成的技术结合起来，这是最快也最省事儿了方式。这个里面再具体一点的就要看用户的使用习惯，比如说他这个公司他很喜欢用MySQL，或者很喜欢用PostgreSQL，或者可能还是喜欢用Redis或者ElasticSearch都可以在上面加上向量引擎能力，小规模的公司比较适合这种。</p><p>&nbsp;</p><p>其他的体量更大的公司考虑的可能就不太一样了，数据肯定是更上一个量级的，这种情况下多半会倾向于自研。但是自研也不一定完全自研一个向量数据库，也可能只是定制一款向量引擎，和他的业务系统进行结合。</p><p>&nbsp;</p><p></p><h2>未来趋势展望</h2><p></p><p>&nbsp;</p><p>InfoQ：其实现在传统数据库市场就已经非常卷了，市面上有几百款数据库，在您看来以后向量数据库市场会不会更卷？</p><p>&nbsp;</p><p>李莅：我认为向量数据库这个方向可能没那么卷。它跟传统的数据库不一样，传统的数据库是整个互联网或者信息行业任何地方都要用到的东西，而向量数据库现在火起来完全是作为大模型的配套设施，它一定是要捆绑在大模型上的。如果大模型用不起来，或者不火的话，向量数据库就没有那么多生意了。所以它不会像传统数据库那么卷，能入局这一领域的玩家，它一定是要有大模型方面的能力才能走下去的，因为它还有上下游生态需要考虑，向量数据库只是其中的一个环节。</p><p>&nbsp;</p><p>InfoQ：您是否认为，以后所有的数据库它都会原生地支持用向量？</p><p>&nbsp;</p><p>李莅：传统数据库广义上也分好几类：一类是关系型的，一类是NoSQL类的，还有一类是分析型的数据库。我觉得关系型的这种数据库它往上去做向量的不会很多，因为关系型数据库跟大模型的使用习惯还是不太相符的。它还是偏一个传统的SQL的场景，SQL去做事务、写入数据、做一些查询，然后对接上层应用。关系型数据库去加向量的能力的也比较少，目前主要就是PostgreSQL加了向量插件。</p><p>&nbsp;</p><p>然后NoSQL这类的，我觉得大部分都会加上。因为NoSQL支撑的就是半结构化或者非结构化的数据，它本身就能对接各种杂七杂八的应用，其中就包括AI相关的应用，所以它加上向量的能力是很自然的。在NoSQL上加上向量能力这个事情本身不困难，基本上NoSQL数据库都已经加了这种能力，比如说ElasticSearch、Redis、还有MongoDB这些NoSQL他们都会加上。</p><p>&nbsp;</p><p>还有一种就是分析型数据库，分析型数据库我觉得也必然会加上向量能力的。因为分析型数据库其实就是为大数据行业服务的，大数据跟AI本身就是一个非常紧密的结合关系，大数据就是属于AI的产业链里面一环，所以它很有必要去加上向量能力。</p><p>&nbsp;</p><p>InfoQ：那您认为向量数据库未来的发展趋势是什么样的？会往哪方面发展呢？</p><p>&nbsp;</p><p>李莅：这个趋势就现在已经可以看到了，一个是精研算法，算法还是有一定的提升空间。只是说这个东西可能不会那么早的对外公开的一个东西，各家都在做技术竞争力，不一定会马上就落到社区上面去推广。</p><p>&nbsp;</p><p>还有一个就是架构层面的改动，一定是会往云原生和存算分离的这个方向去发展的，因为向量现在的技术上还是需要有大量的资源去构建索引。这个构建索引的资源开销是比较大的，但是构建完了之后可能一时半会儿也用不到了，那么这个时候如果要降本的话，就希望能有一些弹性资源去做这个事情。</p><p>&nbsp;</p><p>此外，在不同量级数据的查询时也希望能弹性缩放。比如今天要搞一个活动，可能今天的请求就达到了平时的十倍、百倍，这时就要一下拉起大量的资源。活动结束之后，再把这份资源释放掉，也是希望有些弹性的资源。这就会需要存算分离的技术去支撑，可以很快速地把计算层去扩容和缩容，所以未来向量数据库技术要往这个方向去发展。</p><p></p><p>以「启航·AIGC 软件工程变革」为主题的 <a href=\"https://qcon.infoq.cn/202309/beijing/schedule?utm_source=infoqweb&amp;utm_medium=wenzhang&amp;utm_campaign=10\">QCon 全球软件开发大会·北京站</a>\"将于 9 月 3-5 日在北京•富力万丽酒店举办，此次大会策划了向量数据库、大前端新场景探索、大前端融合提效、大模型应用落地、面向 AI 的存储、AIGC 浪潮下的研发效能提升、LLMOps、异构计算、微服务架构治理、业务安全技术、构建未来软件的编程语言、FinOps 等近 30 个精彩专题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7f66075681f911a948b01c99b5fd5857.jpeg\" /></p><p></p><p>现已确认 130+ 名嘉宾，咨询购票优惠信息可联系票务经理 18514549229（微信同手机号）。</p>",
    "publish_time": "2023-08-30 09:40:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023开源和信息消费大赛正式启动",
    "url": "https://www.infoq.cn/article/ZJSZrLXW78TjjyQbRJtc",
    "summary": "<p>8月29日，工业和信息化部、江苏省人民政府、湖南省人民政府联合在京举行新闻发布会宣布，<a href=\"https://competition.atomgit.com/\">2023开源和信息消费大赛</a>\"正式启动。</p><p>&nbsp;</p><p>工业和信息化部信息技术发展司副司长王威伟，江苏省工业和信息化厅副厅长池宇，湖南省工业和信息化厅总经济师熊琛，江苏省无锡市人民政府副市长周文栋，湖南省株洲市委常委、市人民政府副市长王庭恺，开放原子开源基金会理事长孙文龙，中国电子信息产业发展研究院总工程师秦海林参加发布会并发言。</p><p>&nbsp;</p><p>据了解，本次大赛聚焦开源和信息消费发展趋势和应用需求，旨在以赛促学、以赛促用，进一步发掘消费潜力，激发市场活力，改善消费环境，擦亮示范招牌，普及开源文化，培育选拔优秀开源人才，赋能行业和经济高质量发展。</p><p>&nbsp;</p><p>举办开源和信息消费大赛是推动我国开源体系建设、培育壮大战略性新兴产业的重要举措，是把实施扩大内需战略同深化供给侧结构性改革有机结合起来的模式创新。以开源软件创新为引领，有助于加快传统产业高端化、智能化、绿色化升级改造；以信息消费提质升级为抓手，进一步推动“数实”融合，积极发展数字经济和现代服务业，加快构建具有智能化、绿色化、融合化特征和符合完整性、先进性、安全性要求的现代化产业体系。</p><p>&nbsp;</p><p>据介绍，本次大赛分为开源和信息消费两个赛道：</p><p>&nbsp;</p><p>开源领域紧扣“软件定义世界&nbsp; 开源共筑未来”主题，聚焦基础软件、工业软件、云原生、开源硬件、安全治理等十余个领域，由各龙头企业和科研院所出题，支持大家用软件代码解决“真问题”，通过开源模式推动信息技术繁荣发展。信息消费领域围绕“提质消费供给&nbsp; 智造美好生活”主题，重点面向“智”生产、“慧”生活、“智能+”平台服务与治理三个方向，紧扣产业转型升级需求及多领域、跨场景消费应用需求命题，由信息消费平台企业和公共服务单位出题，鼓励更多市场主体、科研单位聚焦细分领域创新产品、升级服务，以数字技术赋能，驱动消费升级。</p><p>&nbsp;</p><p>开源和信息消费相辅相成、互相促进，一个侧重于高质量供给，一个侧重于高品质需求。下一步，工信部将从完善体系化布局、打造示范引领标杆、提升社会各界认识、发挥开放合作优势等四个方面着手，加快推进开源体系建设，持续扩大和升级信息消费。</p>",
    "publish_time": "2023-08-30 09:59:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "融资7亿元后，Mojo之父实名吐槽：Mojo太好用了，颤抖吧C++",
    "url": "https://www.infoq.cn/article/vmhNfhuc140CXrj5JDiv",
    "summary": "<p>Modular Inc. 是一种名为 Mojo 的人工智能编程语言的开发商，最近在 General Catalyst 领投的一轮融资中获得了 1 亿美元（约7亿人民币）的资金，Alphabet Inc. 的 GV 初创基金和其他机构支持者也参与其中。该公司计划利用这笔资金改进其人工智能编程语言及其旗舰产品AI Engine，这是一款旨在提高企业神经网络速度的软件工具。</p><p>&nbsp;</p><p>Modular公司CEO Chris Lattner表示，本轮融资总额达到1.3亿美元，所得款项将用于产品扩展、硬件支持和推动自研AI编程语言Mojo的进一步发展。</p><p></p><h2>谷歌前员工创办的Modular公司已获1亿美元融资</h2><p></p><p>&nbsp;</p><p>Lattner在邮件采访中解释道，“由于我们所处的技术领域非常深入，因此需要高度专业化的知识作为支撑。因此，本次筹得的资金将主要用于支持团队发展。这笔资金不会主要用于AI计算，而是投入到核心产品的改进和扩展当中，用以满足当前Modular公司面对的旺盛客户需求。”</p><p>&nbsp;</p><p>作为前谷歌员工，Lattner于2022年与Google Brain研究部门的前同事Tim Davis共同在帕洛阿尔托创立了Modular公司。Lattner和Davis都认为AI技术面临的核心障碍，就是过于复杂且分散的技术基础设施。为此二人决定成立Modular，专注于消除AI系统大规模构建与维护所带来的复杂性挑战。</p><p>&nbsp;</p><p>同时，Lattner也曾在社交媒体X上吐槽，Mojo的大火对Python没有构成威胁，它提升了Python程序员的能力并赋予了超能力，如果有哪种语言感受到了威胁，应该是C++和一些难以使用加速器的编程语言。Python是程序员的心头爱，可当你需要性能的时候，C++基本上就是不得不走的“刀山火海”了。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/90/90ec26413a50539a4725990505517cf9.png\" /></p><p></p><p>&nbsp;</p><p>Modular提供一套引擎，希望在提高AI模型在CPU上推理性能（对GPU的支持将于今年晚些时候推出）的同时，帮助客户实际成本节约。Modular的引擎能够与现有各云环境、机器学习框架（包括谷歌的TensorFlow和Meta的PyTOrch）以及其他AI加速器引擎相兼容，目前正处于内部预览状态。Lattner介绍称，该引擎允许开发人员导入已训练好的模型，并将运行速度提升至达原生框架的7.5倍，且无需对现有代码进行任何更改。该平台通过将人工智能模型的代码翻译成Mojo语言并应用其他优化来提高硬件效率来实现这一目标。</p><p>&nbsp;</p><p>AI引擎还利用编译时计算，无需在推理过程中重复计算，从而加快处理速度。此外，它还实现了算子融合，这是一种性能优化技术，将人工智能组件组合成一个更高效的单元，需要更少的硬件资源。</p><p></p><h2>Mojo编程语言将于下月推出</h2><p></p><p>Modular的另一大旗舰产品则是Mojo编程语言，这种语言希望将Python的可用性同缓存、自适应编译技术和元编程等功能结合起来。Modular目前已经开始向“数百名”早期采用者提供Mojo的预览版本，并计划在下月初全面对外发布。</p><p>&nbsp;</p><p>Python 由于其简单、简洁的语法而被开发人员广泛用于编写 AI 模型。然而，用 Python 编写的程序可能会遇到性能问题，并且与其他语言相比往往速度较慢。Modular 旨在通过 Mojo 来解决这个问题，Mojo 提供与 Python 几乎相同的语法，但据称速度提高了 35,000 倍。</p><p>&nbsp;</p><p>Python 性能缓慢的原因之一是它的内存安全机制，它可以帮助开发人员避免常见错误，但也会减慢代码执行速度。Mojo 提供了与 Python 类似的内存安全机制，但对性能没有重大影响。此外，Mojo 最大限度地减少了优化 AI 模型以在各种类型的芯片上有效运行所需的手动编码量。</p><p>&nbsp;</p><p>自 6 月首次亮相以来，Modular 的 AI 引擎和Mojo编程语言引起了开发人员的极大兴趣，超过 120,000 名开发人员表示对这些技术感兴趣。AI 引擎目前可通过抢先体验计划获得，而 Mojo 将于下月初推出。</p><p>&nbsp;</p><p>Lattner指出，“我们的开发者平台，将帮助客户乃至全球各地的开发人员整理自己AI技术栈中的一个个碎片，在生产流程中快速实现更多创新，并借由AI投资实现更多价值。我们正努力解决困扰AI技术栈的碎片化问题，借此降低阻碍AI整体发展的复杂性因素。而这一切的起点，正是尝试将AI软件与AI硬件结合起来。”</p><p>&nbsp;</p><p>这样一个目标背后的野心是否过于膨胀？可能有一点，但作为一家拥有70名员工的年轻公司，Modular提出的方案又确实都具备可行性。</p><p>&nbsp;</p><p>由英特尔支持的Deci，就是一家与Modular类似、提供技术以提高已训练AI模型效率与性能的初创企业。在同一生态位上的战友还包括OctoML，他们面向一系列不同硬件提供自动优化、基准测试与打包模型。</p><p>&nbsp;</p><p>无论如何，在Lattner看来，市场对于AI技术的需求正迅速在可持续性层面逼近极限——换言之，任何能够降低计算强度的方法都将具备巨大价值。正如《华尔街日报》最近发表的一篇报道所言，当前主流生成式AI模型的体量一般可达传统AI模型的10到100倍，而大部分公有云基础设施在设计上并不适合此类系统的体量和特性。</p><p>&nbsp;</p><p>而这些现实问题已经开始产生影响。微软在财报中就警告称，运行AI所需要的服务器硬件发生严重短缺，可能导致服务中断。与此同时，对AI推理硬件（主要是GPU）的极高需求，已经把GPU提供商英伟达的市值推升到了1万亿美元。然而，巨大的成功反而也让英伟达自己沦为受害者。据报道，该公司在2024年之前所有最高性能级别的AI芯片均已被提前预订。</p><p></p><h2>Mojo到底强在哪？</h2><p></p><p>&nbsp;</p><p>S&amp;P Global发布的一份2023年民意调查显示，受到种种原因的综合影响，顶级企业中超过半数的AI决策者都在部署最新AI工具方面遭遇到各种各样的障碍。</p><p>&nbsp;</p><p>Lattner强调，“当前AI程序所耗费的算力资源无疑是巨大的，而且现在这种模式根本就不具备可持续性。我们已经感受到计算能力不足以支撑需求的现实问题。硬件成本飞涨，只有财力最雄厚的科技巨头才有资源构建自己的AI体系。Modular的出现就是希望解决这个问题，把AI产品和服务变得任何企业都能负担得起、更具可持续性，并以易于上手的方式提供友好支持。”</p><p>&nbsp;</p><p>Modular公司的Mojo编程语言，属于Python的一个“快速超集”。</p><p>&nbsp;</p><p>这样的目标当然有其合理性。但考虑到Python在机器学习社区中那根深蒂固的江湖地位，我们恐怕很难相信Modular真能推动其新语言Mojo得到广泛普及。一项调查显示，截至2020年，高达87%的数据科学家在定期使用Python。</p><p>&nbsp;</p><p>但Lattner仍抱有信心，表示Mojo的独特优势将推动其稳步增长。</p><p>&nbsp;</p><p>&nbsp;“关于AI应用，人们普遍抱有一种误解，即总是把问题归结于高性能加速器。事实并非如此，当下的AI更多是个端到端的数据问题，具体涉及到数据加载、转换、预处理、后处理和网络传输。这些任务以往都是由Python和C++完成的，也只有Modular和Mojo才能把这么多组件真正结合在一起，立足统一的技术底座协同工作，同时不至于牺牲性能和可扩展性。”</p><p>&nbsp;</p><p>他的说法似乎确有道理。Lattner还声称，自5月初Modular发布产品主题演讲以来，Modular社区的开发者数量已经超过12万人。多家“领先科技公司”正在使用这家年轻企业的基础设施，还有3万家客户正在排队中焦急等待。</p><p>&nbsp;</p><p>他解释道，“Modular想要打败的最大敌人就是复杂性：只能在特殊情况下起效的软件层代表着复杂性，只能适配特定硬件和性能加速器的底层特性也代表着复杂性。AI之所以能成为如此强大、极具变革性的技术，正是因为它需要巨大的资源投入和工程努力才能达成如今的规模，需要投入海量人力来构建定制化解决方案，还需要令人咋舌的计算能力来提供一致的结果。Modular引擎与Mojo将共同创造出更公平的竞争环境，而这还仅仅只是开始。”</p><p>&nbsp;</p><p>而且至少从融资的角度来看，Modular的壮志雄心之下似乎确有一片丰沃的土壤。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://techcrunch.com/2023/08/24/modular-raises-100m-for-ai-dev-tools/\">https://techcrunch.com/2023/08/24/modular-raises-100m-for-ai-dev-tools/</a>\"</p><p><a href=\"https://www.verdict.co.uk/modular-raises-100m-in-funding/\">https://www.verdict.co.uk/modular-raises-100m-in-funding/</a>\"</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-08-30 11:11:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "营收有望突破10亿美元！OpenAI发布 ChatGPT 企业版，开启疯狂变现之路 ？",
    "url": "https://www.infoq.cn/article/9hjOlmmifKIB4wuy43NP",
    "summary": "<p></p><blockquote>Altman 曾乐观地表示，“在软件达到一定的能力水平后，就会像按下一个按钮，利润就可哗哗进账，最终收益将远远弥补成本。”</blockquote><p></p><p>&nbsp;</p><p>8月30日，据The Information报道，随着大企业不断增加人工智能方面的支出，ChatGPT开发商OpenAI有望在未来12个月内通过销售人工智能软件及其计算能力创造超过10亿美元的营收。</p><p>&nbsp;</p><p>十亿美元的收入数字，远超该公司之前向股东公布的预期。该报道还称，这家微软支持的公司现在每月的营收超过8000万美元。</p><p>&nbsp;</p><p>去年，在OpenAI开始对人工智能聊天机器人 ChatGPT 收费之前，一共只创造了 2800 万美元的营收。今年早些时候，当投资者从现有股东手中购买股票时，OpenAI的账面估值为270亿美元。</p><p>&nbsp;</p><p>营收的快速增长表明，更多的应用程序开发商和大公司（包括华尔街公司 Jane Street 等秘密公司）正利用 OpenAI 的聊天机器人技术来寻找赚钱或节省成本的方法。微软、谷歌和无数其他想要从人工智能技术中盈利的企业都在密切关注 OpenAI 的发展。</p><p>&nbsp;</p><p>根据之前的报道，虽然 OpenAI 目前成本高昂，收入相对较低，但更多投资者认为，ChatGPT 拥有着巨大的变革性，这会让 OpenAI 依靠超高利润率，成为像谷歌和亚马逊这样的万亿美元市值公司。</p><p>&nbsp;</p><p></p><h2>OpenAI搞钱策略</h2><p></p><p>&nbsp;</p><p>OpenAI 在2023年已完成一轮103亿美元的融资，据美国科技媒体4月份报道，参与本轮融资的风投公司包括老虎全球管理、红杉资本、加州安德森·霍茨基金、纽约Thrive和K2 Global。而2023年1月中旬，微软对OpenAI投资100亿美元，占股达到49%。</p><p>&nbsp;</p><p>大模型研发和维护需要庞大的资源投入，OpenAI面临着成本压力，在融资的同时，也在不断通过一些方式将ChatGPT变现。</p><p>&nbsp;</p><p></p><h3>收割C端流量</h3><p></p><p>&nbsp;</p><p>今年2月份，OpenAI推出了订阅计划ChatGPT Plus，ChatGPT Plus定价每月20美元。付费版的ChatGPT提供的增值服务包括：高峰时段免排队、快速响应以及优先获得新功能和改进，OpenAI同时保留了免费服务。</p><p>&nbsp;</p><p>到了3月份，OpenAI在官方博客宣布开放ChatGPT和Whisper的模型 API。支撑ChatGPT和Whisper API调用的分别是gpt-3.5-turbo模型和Whisper large-v2语音转文本模型。据OpenAI介绍，gpt-3.5-turbo是目前最强大的GPT-3.5模型，将优化后的聊天成本降到了此前GPT-3.5模型的十分之一。</p><p>&nbsp;</p><p>OpenAI当时给ChatGPT API定价为每1000个tokens 0.002美元，约750个单词。用户需要按照输入和输出的tokens总数来付费。同时，OpenAI提供了专用实例选项，面向需要通过API运行大量数据，以及希望控制特定型号版本和系统性能的开发人员提供。这部分用户将按时间段支付，以分担预留计算基础设施的费用。Whisper是OpenAI 早在2022年9月就已公布的开源语音转文本模型，可以实现多种语言的转录，其API 的定价为每分钟0.006 美元。&nbsp;</p><p>&nbsp;</p><p>ChatGPT远超前代的用户体验给予了用户极大的新鲜感，ToC端的用户带来了相当大的流量爆发。根据SimilarWeb的数据显示，ChatGPT自2022年11月在美国推出，仅在短短2个月内活跃用户规模便突破1亿大关。根据OpenAI首席执行官Sam Altman去年12月的一条推文，ChatGPT每次聊天的成本达到“个位数美分”，这表明每月为1亿人提供该服务可能需要花费数百万美元。面对上亿级别的用户访问，OpenAI运营策略是持续优化成本。根据此前路透社的报道，OpenAI可能在今年上半年不断缩减了ChatGPT推理时使用的算力，因此还导致用户体感回答质量出现下滑。</p><p>&nbsp;</p><p>另一方面，经过半年左右时间，随着新鲜感的减少，ChatGPT热度开始下滑。据SimilarWeb数据，6月全球用户访问量环比-9.7%。在此情形下，OpenAI的重点开始转向了ToB端。</p><p>&nbsp;</p><p></p><h3>OpenAI杀入B端</h3><p></p><p>&nbsp;</p><p>昨天（当地时间8月28日周一），OpenAI推出了面向大型企业的ChatGPT企业版（ChatGPT Enterprise），官方号称这是迄今为止最强大的ChatGPT版本。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a55268b015ff0873fb78961860e25530.jpeg\" /></p><p></p><p>&nbsp;</p><p>网友评论：“在企业采用之前，人们根本不明白这个市场将有多大。”</p><p>&nbsp;</p><p>根据OpenAI的介绍，除了和一般ChatGPT一样执行编写电邮、起草文件和调试电脑代码等任务外，企业版还可提供企业级的安全和隐私、以及高级数据分析功能。OpenAI首席运营官Brad Lightcap拒绝透露ChatGPT Enterprise定价的具体细节，并指出价格可能会根据每个企业的需求而有所不同。Lightcap表示，OpenAI“可以与每个企业合作，找出最适合它们的计划”。</p><p>&nbsp;</p><p>还有媒体指出，相比面向普通消费者的ChatGPT，ChatGPT企业版的一个重要区别是，企业版允许客户输入公司的数据，为企业所在的行业和用例训练和定制 ChatGPT。ChatGPT企业版由GPT-4驱动，企业用户可以优先访问GPT-4，不设用量限制，访问速度是普通GPT-4的两倍。而且，企业版的可输入内容更多，上下文窗口扩大到3.2万个token、约2.5万个单词，是普通版输入内容的四倍。</p><p>&nbsp;</p><p>另外，ChatGPT Enterprise的开发耗时不到一年，得到了20多家来自不同行业的不同规模的公司的帮助，Beta用户包括Block、Canva 和雅诗兰黛。OpenAI称，企业对面向企业的ChatGPT非常感兴趣，并声称，作为史上增长最快的消费类App之一，ChatGPT已被超过80%的财富500强公司团队采用。</p><p>&nbsp;</p><p>而在推出面向大型企业的ChatGPT Enterprise之后，OpenAI未来还将推出适用于较小型企业的ChatGPT Business，“让我们有更多的方式与团队进行实质性接触，并完全开放之前了解部署动作。”</p><p>&nbsp;</p><p></p><h2>给大家的强心针</h2><p></p><p>&nbsp;</p><p>大模型入场费极其高昂，今年5月份，据The Information透露，由于去年开发ChatGPT并从谷歌招聘关键员工，OpenAI的亏损大约翻了一番，达到5.4亿美元左右。</p><p>&nbsp;</p><p>5.4亿美元的缺口，会让不少资源能力较弱的初创公司对AI领域望而却步。前两周，<a href=\"https://analyticsindiamag.com/openai-might-go-bankrupt-by-the-end-of-2024/\">一家印度媒体还报道说</a>\"，OpenAI花钱太过厉害，在流量下降的情况下，没准会在2024年破产，并且在文中判定“预计 2023 年年收入将达到 2 亿美元，并预计在 2024 年达到 10 亿美元，这似乎不太可能。”</p><p>&nbsp;</p><p>人工智能的研发需要大量资金是毋庸置疑的事情，Sam Altman 自己也曾表示，OpenAI“将成为硅谷历史上资本最密集的初创公司”。</p><p>&nbsp;</p><p>但即使人工智能投资的回报一直难以捉摸，ChatGPT 热潮还是迫使很多公司自上而下地授权参与到大模型相关开发中，因此这也改变了无数企业技术的支出重点。&nbsp;特别是以亚马逊、微软、谷歌、阿里、百度等企业为首的云厂商。亚马逊CEO此前称大语言模型和生成式AI具有变革性，在经济不景气的情况下，亚马逊会继续大力投资并加入生成式AI竞赛，大语言模型和生成式AI是能让“亚马逊未来几十年可以在每个业务领域都进行创新的核心”。</p><p>&nbsp;</p><p>如今，OpenAI表现略好于预期，这也给无数追随者带来了一点信心。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.reddit.com/r/singularity/comments/164xncx/openai_passes_1_billion_revenue_pace_as_big/\">https://www.reddit.com/r/singularity/comments/164xncx/openai_passes_1_billion_revenue_pace_as_big/</a>\"</p><p><a href=\"https://stock.jrj.com.cn/2023/07/07080837675456.shtml\">https://stock.jrj.com.cn/2023/07/07080837675456.shtml</a>\"</p><p><a href=\"https://wallstreetcn.com/articles/3696594\">https://wallstreetcn.com/articles/3696594</a>\"</p><p><a href=\"https://twitter.com/EMostaque/status/1696663084050403336\">https://twitter.com/EMostaque/status/1696663084050403336</a>\"</p><p>https://openai.com/blog/introducing-chatgpt-enterprise</p>",
    "publish_time": "2023-08-30 14:43:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "让大模型融入工作的每个环节，数据巨头Databricks让生成式AI平民化 | 专访李潇",
    "url": "https://www.infoq.cn/article/EvYEXsLPh8KMkfNrsG7D",
    "summary": "<p>采访嘉宾 | 李潇</p><p>编辑 | Tina</p><p>&nbsp;</p><p>Databricks CEO Ali Ghodsi曾表达过这样的观点：Databricks的目标是实现数据普惠和AI普惠，数据普惠使得数据能够触达企业内的每一名员工，而AI普惠则将人工智能引入每一个产品中。他强调“每个组织都应该从AI革命中获益，并更好地掌控数据的使用方式。”在过去，Databricks在AI领域积累了大量经验，如今在大模型的潮流下，他们不仅推出了自家的开源大模型Dolly 2.0，还以13亿美元的价格收购了生成式AI公司MosaicML，迅速强化了大模型方面的实力。最近，Databricks发布了一系列创新产品，例如Lakehouse IQ、AI Gateway、Unity Catalog等。作为大数据领域的领军企业，我们相信Databricks正在塑造着未来。在QCon北京到来之际，我们采访了Databricks Engineering Lead 李潇，以深入了解他们在数据领域的创新思想。</p><p>&nbsp;</p><p>采访嘉宾简介：李潇， Databricks 工程总监、Apache Spark Committer 和 PMC 成员。他领导和管理七个团队，负责开发 Apache Spark、Databricks Runtime 和 DB SQL。他的主要兴趣是数据湖仓、数据复制和数据集成。此前，他是 IBM Master Inventor 荣誉的获得者，也是数据库异步复制和一致性验证方面的专家。他于 2011 年在佛罗里达大学获得博士学位。李潇将在9月3日-9月5日QCon北京发表主题为<a href=\"https://qcon.infoq.cn/202309/beijing/schedule?utm_source=wechat&amp;utm_medium=infoqart2&amp;utm_campaign=10&amp;utm_term=0829&amp;utm_content=lixiao\">“Introducing English as the New Programming Language for Apache Spark”的演讲</a>\"。</p><p></p><p>&nbsp;</p><p>InfoQ：Databricks的使命似乎在不断进化（从Spark到数据湖仓到AI），那么能说说这背后的思考吗？</p><p>&nbsp;</p><p>李潇：Spark其实是为AI而生的。最初是<a href=\"https://www.linkedin.com/in/mateizaharia/\">Matei Zaharia</a>\"为了 Netflix 的机器学习竞赛而创建了Spark这个分布式数据处理系统。在十年前，Spark刚进入ASF，就已经集成了机器学习、离线分析、流处理和图处理的功能。Lakehouse也是在十年前就已经有了雏形。在过去十年中，整个社区共同努力，使 Apache Spark™ 发展成为一个可以在单节点机器或集群上执行数据工程、数据科学和机器学习的多语言引擎。</p><p>&nbsp;</p><p>Databricks的使命，其实从创建开始一直到现在，都是非常一致的。Databricks 是由一群 Spark 的原创人于2013年创建的公司，专注于构建智能湖仓(Lakehouse)。虽然最初没有明确使用Lakehouse这一术语，但 Databricks Lakehouse平台一直在致力融合数据湖和数据仓库的最佳元素，旨在帮助降低成本并更快地实现数据和 AI 的目标。AI也一直是我们产品的重要组成部分。基于开放的数据湖仓架构，Databricks 上的 AI 和机器学习使用户能够准备和处理数据，简化跨团队合作，并规范从实验到生产的完整机器学习生命周期。这里面的AI当然也包括当下最热门的生成性技术，如大型语言模型。</p><p>&nbsp;</p><p>InfoQ：Databricks 最近有很多动作，做了开源大模型Dolly，也收购了大模型公司MosaicML。那么对于Spark和数据湖这些技术和产品，Databricks如何将整合大模型？</p><p>&nbsp;</p><p>李潇：Databricks坚信开放与合作的力量。我们预见的未来，既包括适应性广泛的通用大模型，也涵盖了具有独特优势的专业模型。数据所有权，训练和服务成本将会是很大的挑战。为此，我们构建了服务平台，助力企业用户便捷地训练、微调和管理这些模型。Dolly的诞生，更是我们为展示如何用小型的专有数据集调优大模型所付出的努力。而MosaicML是又这方面的领军者。我们的用户可通过MosaicML创建私有的ChatGPT，以较少的成本实现更大的价值。MosaicML的分布式训练服务允许客户不与第三方共享数据、模型和推理，仅需数日，就能建立自己的专属大语言模型。</p><p>&nbsp;</p><p>我们深知大模型的好坏在于数据，其中，数据的质量与模型的成果息息相关。身为Lakehouse服务商，我们致力于帮助客户高效整合、清洗并挖掘其核心数据资产。对MosaicML收购恰恰是1+1 &gt; 2。高质量的数据使MosaicML能打造出色的私有LLM（Large Language Model），而这样的LLM又进一步挖掘了数据的潜在价值，推动AI真正服务于每一位客户，这正是我们所追求的“AI普惠”。</p><p>&nbsp;</p><p>InfoQ：Databricks在AI峰会上发布了几个新AI产品，其中一个是英文SDK，“英语是新的 Spark 编程语言”。那么这个用户定位是针对什么样的人群？（小编注：似乎能用到Spark的人，应该已经具备了应用SQL和接口的能力？）另外，这个产品的准确率如何，是否有了进一步的提升？</p><p>&nbsp;</p><p>李潇：即使对于经验丰富的Apache Spark用户，他们也可能仅仅熟悉其中的一小部分API和参数，因为PySpark的功能之繁多，有上千个API。而随着ChatGPT的兴起，我们惊喜地发现它对PySpark有着深入的了解。这应归功于Spark社区在过去十年中的辛勤努力，他们提供了众多的API文档、开源项目、问题解答和教育资源。于是乎，我们启动了English SDK这个项目，我们将Spark Committers的专业知识注入，通过LLM，使用户可以只通过简单的英文指令获得所需结果，而不再需要自己录入复杂的代码。通过这种方式，我们降低了编程的入门难度，简化了学习过程。English SDK的初衷是扩大Spark的应用范围，进一步推动这个已经非常成功的项目。&nbsp;</p><p>&nbsp;</p><p>至于English SDK的准确率，它高度依赖于LLM的性能。例如，GPT-4在这方面的表现就非常出色。我们在Data+AI Summit展示的demo就是用的GPT-4。为了进一步改善准确率，我们会往English SDK加入更多Spark研发人员的专业经验和技巧，使LLM能够更精准地生成高效代码，并降低错误率。但需要强调的是，English SDK更多的是作为一个助手，它的目的是为了帮助我们更方便地使用Spark，提高效率。考虑到大模型的局限性，我们不期望它在短期内能完全替代人工，尤其是在处理像Spark这样的复杂软件和五花八门的应用场景。然而，随着技术的进步，人机交互的模式会发生重大变革，自然语言处理技术将更加融入我们的开发流程，使我们可以集中精力去解决业务问题，而不仅仅是编码。 最后，我要提醒大家，English SDK是一个开源项目，欢迎大家加入并为其贡献自己的创意。有兴趣的朋友们，可以访问pyspark.ai来了解更多。九月四号，我也会在QCon北京给大家更深入地讲解这个项目。</p><p>&nbsp;</p><p>InfoQ：数据平台结合AI的方式，Databricks与其他家相比，有哪些特色？</p><p>&nbsp;</p><p>李潇：面对如今日新月异的大模型行业，自从ChatGPT等先进技术横空出世后，数据和AI行业的领导者都意识到了“奇点”出现，大家都争先为用户提供大模型服务。所以，今天的特色很可能明天就会成为标配。</p><p>&nbsp;</p><p>如果让我来突出Databricks的一个区别点，那我会毫不犹豫地提及我们的“Unity Catalog”。在当前的技术环境下，信息安全始终是各大企业关心的焦点。而我们的Unity Catalog正是为Lakehouse设计的首款统一数据治理工具。它能够帮助企业精确管理其结构化与非结构化数据，同时对分布在不同云服务上的数据资产进行高效管理，其中自然包括了大模型。Catalog为大型模型的训练和应用提供了更全面的上下文信息，如元数据和数据溯源，从而有助于提高模型的准确度。有了Unity Catalog，数据科学家、分析师以及工程师们都能够在一个安全和高效的环境中探索、获取和处理可信赖的数据和AI资产。这不仅确保了数据的安全，同时也为他们提供了充分发挥Lakehouse潜能的机会。</p><p>&nbsp;</p><p>InfoQ：像Databricks这样的厂商，在整个大生态中希望扮演的角色/定位是什么？</p><p>&nbsp;</p><p>李潇：这个问题非常大，也很难回答。在过去十年，Databricks在大数据和AI生态中一直扮演的角色应该是创新的引领者、开源社区的坚定支持者和行业进步的推动者。</p><p>&nbsp;</p><p>Databricks的创始团队是Apache Spark的原创者，而现在，Spark已经成为了全球最受欢迎的大数据处理框架，每个月都有超过十亿次的下载。Databricks始终走在技术创新的前沿，我们率先在Spark中引入了批流一体框架，推出了Lakehouse架构，并在Delta 3.0中为开源存储层设计了创新的通用格式。这些成果也屡获大奖，如Apache Spark和Photon向量化引擎在顶级数据库会议SIGMOD中获得最佳系统和最佳论文奖，并刷新了TPC-DS的纪录。&nbsp;</p><p>&nbsp;</p><p>我们很高兴看到整个行业紧随我们的创新步伐，更多的厂商宣布提供Lakehouse产品，这无疑也在侧面说明我们在推动着整个行业的飞速进步。为了加快行业的共同进步，我们Databricks一直是开源精神的忠实拥护者，除了Apache Spark，我们还开源了Delta Lake数据存储框架和MLflow这样的机器学习生命周期管理平台。我们正在积极推动开源模型的广泛应用。例如，你可以利用如Meta公司最近发布的Llama 2模型这样的开源模型，轻松构建自己的检索增强生成（Retrieval Augmented Generation）应用程序。在我们最近发布的博客文章中（<a href=\"https://www.databricks.com/blog/using-ai-gateway-llama2-rag-apps\">https://www.databricks.com/blog/using-ai-gateway-llama2-rag-apps</a>\"），我们详细介绍了如何使用MLflow AI Gateway进行集中式的模型管理、凭证管理和速度限制。我们坚信，未来应是开放的，而不是封闭的。展望前方，我们期待持续在开源社区深耕，为大型模型的发展持续贡献我们的力量。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：很多人关心大模型的成本问题。Databricks在构建大模型时，有哪些降低成本的方法？（包括自己降低成本和帮助用户降低使用专有模型的成本；除了降低模型参数规模，还有哪些细节上的事情能够降低成本吗？）</p><p>&nbsp;</p><p>李潇：当我们讨论如何实现降本增效时，不得不提到MosaicML。他们持续追求的一个核心使命就是降低模型训练的整体成本。以他们最新发布的MPT-30B大模型为例，这个模型引入了Alibi等先进技术以延长context window，并利用FlashAttention技术有效提高了GPU在训练和推理过程中的工作效率，进而显著削减了成本。</p><p>&nbsp;</p><p>值得注意的是，在多GPU上预训练大模型时，经常会遇到软硬件错误，若不及时处理，这些错误很容易导致昂贵的资源浪费。而MosaicML在这方面展现出卓越的实力。他们实施了一套主动监管系统，能够即时检测到软硬件错误，并自动修复这些出现的问题。这种自动化的错误管理机制大大减少了不必要的成本浪费，确保了训练过程的高效进行。</p><p>&nbsp;</p><p>在模型规模上，MPT-30B经过深思熟虑的设计，使其30B的参数规模能够优化到可在单一GPU上部署的程度，例如可以在1x NVIDIA A100-80GB上以16-bit精度运行，或在1x NVIDIA A100-40GB上以8-bit精度运行。而与此相对照，其他的LLMs，如Falcon-40B，尽管参数量更大，但却无法在单块GPU上流畅运行，这常常意味着需要至少两块或更多的GPUs，自然也就增加了推理系统的基础开销。</p><p>&nbsp;</p><p>除此之外，选择和优化硬件同样是一项关键任务。据我们所知，MPT-30B是首个在NVIDIA H100 GPUs上训练的模型，与A100相比，它不仅运行速度更快，而且具有更高的性价比。</p><p>&nbsp;</p><p>最值得一提的是，通过MosaicML的技术，用户只需要短短不到两周的时间，就能从零开始训练出自己的MPT-30B模型。</p><p>&nbsp;</p><p>InfoQ：如果未来所有产品都需要用LLM来重新设计，那么Databricks自己的产品是否也会基于LLM重新设计？如果会的话，这项工作目前在进行中了吗？</p><p>&nbsp;</p><p>李潇：我们在CEO Ali Ghodsi 的号召下，全公司今年年初就全面拥抱LLM。最开始，我们有一个口头禅：“今天你用了LLM吗？”到后来， LLM逐渐就已融入到了我们的工作的每个环节里，无论是编写面向用户的错误提示，还是构建测试用例。在Databricks，我们秉持“Dogfood” 的文化，每天都在使用自家的产品进行研发。我们把自身的使用需求转化为实际的产品输出。例如，我们推出了Databricks Assistant——一个基于上下文的AI助手。它已经正式上线，并在Notebook、SQL编辑器和文件编辑器中都提供了公开预览。 Databricks Assistant让用户能够通过对话界面查询数据，进一步提高在Databricks平台上的工作效率。您只需用英语描述您的任务，Assistant便可以生成SQL查询，解释复杂的代码，甚至自动修复错误。此外，Assistant利用Unity Catalog的元数据来理解您的表格、列、描述以及公司中流行的数据资产，从而为您提供个性化的答复。&nbsp;</p><p>&nbsp;</p><p>当然，除了LLM，AI在我们众多的产品设计中都发挥了关键作用。例如，我们最新公布的predictive I/O，它可以加速读取数据的速度，缩短扫描和读取数据所需的时间。同时，它还可以加速数据在更新、删除和合并时的处理速度，降低在这些操作中需要重写的数据量。</p><p>&nbsp;</p><p>InfoQ：您认为GPT是否给大数据行业带来了冲击？如果有影响，主要体现在哪些方面？</p><p>&nbsp;</p><p>李潇：GPT等大模型成功地凸显了数据质量对于模型性能的重要性。它不仅进一步加强了我们对大数据的价值认知，而且提高了我们对数据采集、清洗与处理的关注度。这无疑刺激了大数据解决方案的需求。同时，随着大模型的广泛应用，如何有效、高效地处理数据，如何优化数据处理的成本，都成为了亟待解决的问题。此外，数据治理、数据安全和隐私保护也日益受到重视，因为不恰当的数据管理容易导致敏感信息在模型中的泄露。</p><p>&nbsp;</p><p>谈及未来的人机交互，我预期会有重大的变革。这个不单单影响大数据行业的软件，应该影响了所有的产品。我相信，大模型将很快成为行业的标准。但这也意味着那些用户不太熟悉的API和功能可能会遭遇使用难题。因为如果它们的使用量少，相应的文档和示例也会变得稀少，这将导致这些信息难以被纳入到大模型的训练数据中，使模型难以提供精确的建议。这种恶性循环对闭源产品和小型社区都构成了巨大的挑战。</p><p>&nbsp;</p><p>InfoQ：大模型会给大数据行业里的哪些职业带来影响较大，比如数据工程师、数据科学家、数据分析师等。</p><p>&nbsp;</p><p>李潇：大模型正在对各个行业带来深刻的影响，当然这种影响不仅限于大数据行业，但我们首先承认，大数据行业是最大的受益者之一。接下来我将以三个职业为例来详细说明。</p><p>&nbsp;</p><p>首先，看数据工程师。我们可以将大模型比作一座摩天大楼，而数据则是其不可或缺的建筑材料。随着这种“摩天大楼”大量兴建，对数据的需求和质量标准也随之增加。因此，数据工程师需要掌握处理更加复杂的数据流的技能，确保数据既高质量又能快速流转。</p><p>&nbsp;</p><p>其次，是数据科学家。在大模型的时代，通用模型和专用模型层出不穷，选择合适的模型已经成为他们日常工作中的一大挑战。根据实际需求，如成本与性能的权衡，可能需要为大模型的使用投入大量资金，这对于数据科学家来说是一个全新的考量维度。</p><p>&nbsp;</p><p>最后，让我们看看数据分析师。随着大模型的进步，一些基础的数据分析工作可能会被自动化取代。但这并不意味着数据分析师的工作将变得不重要，相反，他们需要更加深入地理解特定领域的业务逻辑，用于解决更为复杂的问题，并提供有洞察力的分析。</p><p>&nbsp;</p><p>实际上，随着大模型的普及，每个人都有可能成为“数据分析师”。以我们最近在Data+AI Summit上发布的Lakehouse IQ为例，它就是一个基于大模型的智能系统。Lakehouse IQ能够学习并理解您业务和数据的独特性质，为各种用途提供自然语言的访问能力。您组织中的任何员工都可以使用Lakehouse IQ以自然语言的方式搜索、理解和查询数据。它还能够结合您的数据、使用模式和组织结构来理解您公司的行话和独特的数据环境，从而提供比简单使用大语言模型更准确的答案。</p><p>&nbsp;</p><p></p><h4>活动推荐</h4><p></p><p>以「启航·AIGC软件工程变革」为主题的QCon全球软件开发大会·北京站将于 9 月3-5 日在北京•富力万丽酒店举办，此次大会策划了从 BI 到 BI+AI，新计算范式下的大数据平台、大前端新场景探索、大前端融合提效、大模型应用落地、面向 AI 的存储、AIGC 浪潮下的研发效能提升、LLMOps、异构计算、微服务架构治理、业务安全技术、构建未来软件的编程语言、FinOps 等近30个精彩专题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a12e80aa9dfab4b5a64f04d24887c27.jpeg\" /></p><p></p><p>咨询购票优惠信息可联系票务经理 18514549229（微信同手机号）。点击<a href=\"https://qcon.infoq.cn/202309/beijing/schedule?utm_source=wechat&amp;utm_medium=infoqart2&amp;utm_campaign=10&amp;utm_term=0829&amp;utm_content=lixiao\">链接</a>\"即可查看QCon 北京站完整日程，期待与各位开发者现场交流。</p>",
    "publish_time": "2023-08-30 14:50:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "紧跟生成式AI大潮，交通银行已组建GPT大模型专项研究团队",
    "url": "https://www.infoq.cn/article/wMdyWeSZl6chYwht8ZDz",
    "summary": "<p>交通银行于 8 月 25 日发布的 2023 年半年度经营业绩显示，2023 年上半年，该行实现营业收入 1371.55 亿元，同比增长 4.77%；实现归属于母公司股东净利润 460.39 亿元，同比增长 4.51%。</p><p></p><p>回顾上半年业务，作为交通银行发展战略落地的主要突破口之一，交行深入推进数字化转型，充分运用“数据 + 技术”新要素赋能业务提质增效，金融科技价值创造能力持续增强。据统计，报告期末，其金融科技人才已占集团总人数 6.51%，约 5938 人，同比增长 1.30%</p><p></p><p>具体而言，在推动数字平台内外赋能方面，交行着力打造企业级支付结算平台，围绕“交银易付”品牌，为客户提供用户、账户、支付、清分、融资等一站式服务，交易规模超 100 亿元。</p><p></p><p>其次，加速推进数据中台、营销中台、风控中台、运营中台建设，全面优化客户经理平台、管理驾驶舱功能。中台作用也进一步发挥：营销中台累计部署 2.4 万套营销策略，日均主动触达客户约 3500 万人；运营中台实现人工任务的全渠道调度，提升任务调度处理能力，月均调度任务量新增 800 万；风控中台上线 221 个风险模型资产，进一步夯实风险数据基础，提高风险监测 和反洗钱能力。</p><p></p><p>值得一提的是，在 AI 能力方面，交行表示正积极探索 AIGC 前沿技术，制定生成式人工智能建设规划，并组建 GPT 大模型专项研究团队，为体系化、规模化应用奠定基础。</p><p></p><p>上半年，围绕“降成本、控风险、优体验、增效益”目标，加大人工智能应用深度和广度，交行试点上线对公账户管理流程自动化场景、反洗钱可疑事件排序场景、零售客户兴趣偏好场景，压降人力投入，提升风险分析质效，赋能客户精细化经营。</p><p></p><p>在夯实数字技术基础方面，交行按国际最高安全标准完成浦江新同城数据中心建设，加速新异地数据中心建设；推动基础设施云化转型，构建浦江新数据中心信创云，为基于分布式新技术应用系统上云做好基础保障；持续推进云平台纳管多芯片服务器，强化一云多芯支撑能力；加大信创改造力度，贷记卡分布式业务系统实现单轨运行。</p><p></p><p>在深化数据治理方面，交行持续推进企业级数据标准落地，保障系统内数据标准统一。推动数据质量管理平台在各业务场景的应用，持续优化企业级数据中台、统一数据分析平台、管理驾驶舱以及数据安全管理制度体系等要素。</p><p></p><p>在纵深推进场景建设方面，孵化平台经济、跨境、司法、养老、住房、乡村振兴“5+1”场景，基本实现教育、医疗、政务、交通四类高频场景分行全覆盖；开放银行合作机构超过 860 家，同比提升 39.52%；信用就医累计推广至 57 个城市，在超过 600 家医院上线。</p><p></p><p>相关业绩显示，截至报告期末，交行的个人手机银行月度活跃客户数 4308.63 万户，同比增幅 6.26%；买单吧月度活跃客户 2628.55 万户，同比增长 5.33%；开放银行累计开放接口 3868 个，累计调用次数超 25 亿次；云管家服务用户规模 643.23 万人，较上年末增长 16.89%。</p><p></p><p>欢迎关注「InfoQ数字化经纬」公众号，我们将持续为您推送更多、更优质的数字化案例内容和线上线下活动。</p>",
    "publish_time": "2023-08-30 15:01:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "万事达卡（Mastercard）实验室研发副总裁黄东浩确认出席 FCon ，分享 Web3 时代的商业未来",
    "url": "https://www.infoq.cn/article/Z5fRxEDyhPRyWhfVJDMX",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。万事达卡（Mastercard）实验室研发副总裁黄东浩将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5505?utm_source=infoq&amp;utm_medium=article\">Web3 时代的商业未来</a>\"》主题分享，介绍去中心化技术正在成熟，并有潜力帮助塑造消费者如何存储、管理和使用数字资产。正在开发新形式的货币，以及为商业机会蓬勃发展的新环境。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5505?utm_source=infoq&amp;utm_medium=article\">黄东浩先生</a>\"，现任万事达卡（Mastercard）实验室研发副总裁。万事达卡实验室是万事达卡的全球研发部门，专注于技术演变和消费潮流及其在支付业的应用与影响。作为实验室的全球资深高管之一，黄先生率领新加坡研发团队，将创新想法转化成可商业化的产品。黄先生及其团队参与的全球研发项目，涉及车联网、物联网、人工智能与深度学习、增强与虚拟现实、Web3 与生成式人工智能等众多前沿技术领域。</p><p></p><p>毕业于清华大学、北京大学和新加坡国立大学，黄先生拥有 25 年的软件开发、研发和技术创新的业界经验。他在本次会议的演讲内容如下：</p><p></p><p>演讲：Web3 时代的商业未来</p><p></p><p>网络向 Web 3 的演变预示着将改变消费者在数字世界中的互动方式。Web 3 是一个去中心化互联网的愿景。尽管这一愿景可能尚未完全实现，但去中心化技术正在成熟，并有潜力帮助塑造消费者如何存储、管理和使用数字资产。正在开发新形式的货币，以及为商业机会蓬勃发展的新环境。</p><p></p><p>本次演讲，将帮助大家了解万事达卡如何利用金融科技创新来帮助客户把握这些技术所带来的新机会。</p><p></p><p>演讲提纲：</p><p></p><p>Web3 的现状和挑战万事达卡在 Web3 领域的全球布局Web3 技术和传统支付科技的融合和创新</p><p></p><p>你将获得：</p><p></p><p>○ 了解万事达卡最新 Web3 技术创新</p><p>○ 了解 Web3 技术和传统支付科技的融合和创新</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：13269078023（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-08-30 15:12:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "节省数千 GPU 卡资源，快手如何进行FinOps落地",
    "url": "https://www.infoq.cn/article/bTCwYawGsFxS0UrDzXK7",
    "summary": "<p></p><p>早期，企业在云服务上进行了大量投入，但并非所有资金都得到了有效利用。企业不仅没有享受到云带来的成本红利，反而要面对更高的云账单。在控制云成本需求的促进下，FinOps 理念在国内快速发展，不少企业已经开始落地探索，其中就包括快手。</p><p>&nbsp;</p><p>随着业务的快速发展，如何高效利用海量计算资源是摆在容器云编排调度引擎团队面前的重要问题。经过近期探索，该团队已经取得了初步进展：GPU日均利用率提升6~8个百分点，为公司节省数千GPU卡资源。快手容器云编排调度引擎团队负责人张伟向InfoQ介绍了落地过程的具体实现。</p><p></p><h2>内部实践</h2><p></p><p></p><h5>InfoQ：快手为什么要引入FinOps理念？内部成本问题体现在哪些方面？</h5><p></p><p>&nbsp;</p><p>张伟：快手整体的计算规模已达到数千万核，海量的计算资源要实现精细化的运营和高效的使用，已成为容器平台建设的重中之重。而FinOps理念从云成本管理角度，让技术团队更好的理解财务逻辑，感知技术成本，为公司级的降本增效提供了系统化的方法论，与我们的目标高度一致。快手希望能在FinOps的理论引导下，更有效的联合财务、平台技术研发和业务团队多方更快的实现提质增效的综合目标。</p><p></p><p>当前，我们在内部已建立了相对完善的容器资源SKU定价、货币化的预算与结算能力。目前内部成本问题仍体现在以下几方面：</p><p>&nbsp;</p><p>资源需求持续快速增长，尤其是流量峰值期间资源需求高，而资源日均利用效率仍存在提升空间；各类计算资源异构，且业务需求（如套餐）也呈多样化，使得资源管理的复杂度较高；针对已逐步落地的优化效果，如何长效巩固各类降本提效措施，杜绝运动式治理的阶段性生效。</p><p>&nbsp;</p><p></p><h5>InfoQ：快手在FinOps实践方面都经过了哪些阶段？每个阶段主要做了哪些事情？</h5><p></p><p></p><p>张伟：早期平台启步阶段中，主要由业务提交资源预算，由平台完成交付和保障，资源多采用独占方式使用。</p><p></p><p>此后进入降本增效的发力时期，我们建立完善的资源抽象模型，如在线CPU+内存，混部CPU+混部内存等不同SKU，通过单独的定价和定制化的计费机制，实现货币化拆分。为达到成本优化的共同目标，业务方重点关注使用量优化减少浪费，而平台及基础设施层则通过迭代硬件、降低损耗、提升任务密度等手段来优化降低SKU单价。</p><p>&nbsp;</p><p>当前阶段进入深化优化方案时期，建立了明确的预算内、预算外优化目标，通过拆解到多团队，借助流程规模、以及各类平台级技术手段持续提交，实现资源利用率不断逼近理论目标。</p><p></p><h5>InfoQ：实践中如何对各部门进行绩效考核？</h5><p></p><p></p><p>张伟：我们在内部建立了资源定价、预算、结算及各指标的观测跟踪等运营机制，落地实践中，针对各业务部门主要是通过资源用量、配额以及容器资源的实际使用率如峰均及日均数据来进行考核。而对于平台则重点会通过收支数据、资源日均等指标进行考核。</p><p></p><h5>InfoQ：成本可视化方面，快手具体做了哪些工作？面向FinOps的利用率监控和传统的运维监控有什么区别？</h5><p></p><p></p><p>张伟：快手内部针对成本建立了较为完善的度量和跟踪能力，会按月生成各业务线及各类平台的的收支帐单。同时针对资源在容器、宿主机多个维度跟踪了峰均、日均、时均及闲置、低利用率数据。</p><p>&nbsp;</p><p>相比传统的利用率监控，我们关注的更多维度，首先是容器维度，代表业务层对已分配资源的使用效率，如业务容器内的峰均和日均使用率是核心项，它们衡量了业务实际利用水平。与此同时，宿主机维度的峰均和日均分布受配额执行率和平台资源超发能力的影响。此外，容器及主机维度的空闲率和低利用率也会是我们的关注和考核项。</p><p></p><h5>InfoQ：成本优化方面，快手是从哪些方面着手的？具体是怎么做的？</h5><p></p><p></p><p>张伟：快手内部的优化从预算内和预算外同时着手并行推进。引导业务及平台层推进各项治理活动。</p><p>&nbsp;</p><p>对于业务侧，重点是从业务的结算成本维度进行顶层驱动，引导业务合理规划容量，压缩非必须的资源配额和用量。而对于各类平台，则通过整体的资源利用率等核心指标建立详细的规划，让平台自己围绕指标展开实施各类成优化的技术方案，包括如平台损耗、资源超发、在离线混部类措施。</p><p>&nbsp;</p><p>而各类优化项均会在资源的定价或使用量上有所体现，并最终反馈到业务的结算成本上，我们会从对定价还是配额用量的影响来量化各自的贡献。</p><p></p><h5>InfoQ：众多的云基础设施中，哪部分成本占比最多？针对这部分设施做了哪些优化工作？</h5><p></p><p></p><p>张伟：我们所重点参与的主要还是快手内部的计算领域，相应部分CPU和GPU相应的成本整体最高。主要的优化工作还是围绕在业务高峰期以尽可能少的资源满足业务峰值场景下的资源需求，同时通过系列能力来削峰填谷，实现日均维度的利用率提升。</p><p>&nbsp;</p><p>针对CPU资源，我们多角度出发分别落地了大量措施。在线场景下，我们建设了服务画像能力，基于实际利用率数据建立平台级的套餐推荐和动态超卖等能力，压缩业务侧使用浪费的情况。同时通过CPU在离线混部机制，基于存量进行资源的超发混部，实现二次售卖。</p><p>&nbsp;</p><p>而对于GPU场景，则重点建设了GPU虚拟化分配能力，打破整卡分配限制，同样规模化应用了GPU的在离线混部，而支持近离线任务，从而实现整体成本的降低。此外针对低利用率场景的运营治理活动及限制也是重要的补充手段。</p><p></p><h5>InfoQ：快手如何在FinOps商业产品和开源产品选择的？使用云厂商产品会不会产生“锁定”问题？</h5><p></p><p></p><p>张伟：快手的FinOps相应的机制，目前还是以开源基础上进行自研定制为主。目前对于云厂商的产品使用，会综合成本和性能选择多个云厂商，目前并不局限在单一云厂商上。</p><p></p><h2>经验分享</h2><p></p><p></p><h5>InfoQ：目前的 FinOps 落地处于哪个阶段？为什么？ 阻碍落地的因素有哪些？企业又该如何解决？</h5><p></p><p></p><p>张伟：在FinOps 落地过程中，可以理解主要有成本分析量化、执行成本优化和持续运营治理三个主要阶段。快手目前还第二、三阶段之间，已经通过包括套餐标准化及推荐、调度机制优化、在离线混部、GPU虚拟化等系列平台级能力取得了一定的优化成果，将CPU及GPU日均利用率提升到了业界前列的水平，但基于总体量出发，仍有进一步收益空间。</p><p>&nbsp;</p><p>我们现在主要重心投入的是包括在离线计算资源的统一调度和自由流转的相应能力。这个过程包括混合部署的业务稳定性提升、资源的精细化运营都是挑战。我们当前重点从长期主义出发，在资源抽象、调度框架上完成一致化的建设，再逐步由财务成本驱动实现最终的目标。&nbsp;</p><p></p><h5>InfoQ：&nbsp;您认为，企业确保FinOps实践成功的关键是什么？有哪些经验可以分享？</h5><p></p><p>&nbsp;</p><p>张伟：要实现FinOps实践落地，首先需要建立一个合理的量化模型，并尽可能大的范围内取得相应的共识，尤其是让大家都有效的感知成本及目标。其次，持续优化迭代跟踪会是落地的重要保证。快手在执行FinOps实践时，首先从组织上，自上而下推动了各平台拉齐成本优化的目标设定。此外，在落地过程中，定期进行复盘对齐，保证整体收益和机制会成为长期有效的沉淀也会是关键所在。</p><p>&nbsp;</p><p>在快手内的经验，我们会持续完善过程中的指标，如利用率维度，主机、逻辑核、基准核等各类维度的数据都会被关注。落地过程中，组织从上到下达成共识，会对预设的年度目标拆解到各个月，并在每月月末进行定期复盘和同步相应核心指标数据，从而来保障组织上的切实落地。</p><p></p><h5>InfoQ：当下技术热点转变很快（如大模型落地），FinOps 降本增效实践还在持续进行中吗？您认为，FinOps 未来的发展会是怎样的？</h5><p></p><p></p><p>张伟：快手降本增效实践仍在持续进行中，当前已迈入深水区，未来要实现收益还需要更为精细化的能力机制。考虑当前的整体算力体量，收益仍然是巨大的，我们从利用率等多维度设定了未来几年的建设目标，会持续迭代各类通用化的降本增效动作。</p><p>&nbsp;</p><p>对于快手在 GPU 资源效率持续提升上的系统性建设，张伟将在9月3日-5日的Qcon全球软件开发大会上，带来<a href=\"https://qcon.infoq.cn/202309/beijing/schedule?utm_source=wechat&amp;utm_medium=jgttart1banner&amp;utm_campaign=10&amp;utm_term=0830\">《云原生时代下大规模 GPU 资源利用率优化最佳实践》</a>\"的主题演讲，对此进行详细介绍，大家可以从中获取 AI 在线服务和离线训练分时复用算力资源的一些新思路。</p>",
    "publish_time": "2023-08-30 15:20:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "最长 30 分钟，拥抱生成式 AI，两堂热门课程免费学！",
    "url": "https://www.infoq.cn/article/z74asrQBMsulqi89bvFz",
    "summary": "<p>有人说，生成式 AI 将带来充满创造性的新世界；有人说，生成式 AI 热潮正在掀起一场新的科技革命；有人认为生成式 AI 将淘汰一部分旧工作；也有人认为生成式 AI 能将诞生一系列新的工作。无论如何，毋庸置疑的是生成式 AI 的出现，带来了人工智能技术发展的新转折点，将改变企业的运营方式。</p><p></p><p>据麦肯锡：《2023 生成式人工智能经济潜力研究报告》中提到：“生成式人工智能对生产力的影响可为全球经济增加数万亿美元的价值。我们的最新研究估计，在我们分析的 63 个使用案例中，生成式人工智能每年可增加相当于 2.6 万亿美元到 4.4 万亿美元的价值。”</p><p></p><p></p><h2>亚马逊云科技推出七项生成式 AI&nbsp;新功能</h2><p></p><p></p><p>关注云计算以及人工智能领域的朋友们相信对这条资讯并不陌生：亚马逊云科技推出七项生成式 AI 新功能，进一步降低了生成式 AI 的使用门槛，让无论是业务用户还是开发者都能从中受益。借助这些新功能，来自千行百业的企业都能更专注于核心业务，提高生产效率，充分释放数据价值和生成式 AI 的潜力。</p><p></p><h4>Amazon Bedrock</h4><p></p><p>Amazon Bedrock 全面扩展，新增全新基础模型、基础模型供应商以及 Agents 功能。</p><p></p><h4>Amazon Elastic Compute Cloud</h4><p></p><p>Amazon EC2 P5 实例正式可用，加速生成式 AI 和高性能计算应用。</p><p></p><h4>Amazon OpenSearch Serverless</h4><p></p><p>适用于 Amazon OpenSearch Serverless 的向量引擎助力客户轻松构建现代生成式 AI 应用。</p><p></p><h4>Amazon CodeWhisperer</h4><p></p><p>Amazon CodeWhisperer 与 Amazon Glue 实现集成，进一步提升开发效率。Amazon CodeWhisperer 是一款 AI 编程助手，能够使用底层基础模型帮助开发人员提高工作效率。它可以根据开发人员使用自然语言留下的注释和 IDE（集成开发环境）中的历史代码实时生成代码建议。</p><p></p><h4>Amazon QuickSight</h4><p></p><p>Amazon QuickSight 新增生成式 BI 功能，升级自然语言人机交互。</p><p></p><h4>Amazon Entity Resolution</h4><p></p><p>Amazon Entity Resolution 正式可用，赋能企业提升数据质量、获取客户洞察。</p><p></p><h4>Amazon HealthScribe</h4><p></p><p>Amazon HealthScribe 利用生成式 AI 助力构建医疗应用程序。</p><p></p><p>不仅如此，为了帮助云从业者在生成式 AI 热潮中乘风破浪，把握机遇，亚马逊云科技培训与认证还开设了多门以生成式 AI&nbsp;为主题的课程，课程针对不领域、不同技术程度、不同岗位的人群，针对性的讲解生成式 AI&nbsp;的原理以及应用案例，今天云师兄就向大家推荐两大热门课程，大家对号入座哦～</p><p></p><p></p><h2>高效制胜，提升云职场专业技能</h2><p></p><p></p><p>据麦肯锡《2023 生成式人工智能经济潜力研究报告》显示：生成式 AI 有可能改变工作结构，通过自动化人工的部分来增强他们的能力。目前的生成式 AI 和其他技术有可能将目前占用员工 60% 至 70% 时间的工作活动自动化。另外，报告认为将生成式人工智能与所有其他技术相结合，工作自动化可使生产率增长每年增加 0.2 到 3.3 个百分点。由此可见，“高效”是生成式 AI 的关键点。</p><p></p><p></p><h4>工作效率的提升助推器 Amazon CodeWhisperer</h4><p></p><p></p><p>在亚马逊云科技推出七项生成式 AI 新功能中，Amazon CodeWhisperer 作为一款可帮助开发者更快地完成更多工作的人工智能编程伴侣，就能很好的做到这一点。亚马逊云科技曾举办了一场生产力挑战赛，使用 Amazon CodeWhisperer 的参与者成功完成任务的可能性要比未使用 Amazon &nbsp;CodeWhisperer 的参与者高 27%，平均完成任务的速度快&nbsp;57%，实现了开发人员工作效率的巨大飞跃。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43f0ea06dcad914d5ab409c6c559c761.png\" /></p><p></p><p></p><h4>30 分钟上手云技能《Amazon CodeWhisperer 入门》</h4><p></p><p></p><p>如果您想要快速上手 Amazon CodeWhisperer，云师兄强推：《Amazon CodeWhisperer 入门（英文）》课程值得一学。《Amazon CodeWhisperer 入门》是一门免费的自主进度数字课程。课程面向开发人员和专业技术人员，在 30 分钟的课程中， 不但向大家介绍了 CodeWhisperer 的功能、如何进行设置，并开始用选定的编程语言使用 CodeWhisperer 。从理论到实践，助您快速解锁 Amazon CodeWhisperer 的使用指南，从此登上业务提升快车道。</p><p></p><p>亚马逊云科技官方学习中心 <a href=\"https://explore.skillbuilder.aws/learn/course/external/view/elearning/16405/amazon-codewhisperer-getting-started?trk=d15c46ed-1e4e-4e8b-82c0-63ba2feb26b9&amp;sc_channel=el\">学习通道</a>\"</p><p></p><p></p><h2>管理创新，提升企业竞争力</h2><p></p><p></p><p>据麦肯锡（QuantumBlack）发表的《2023 年的人工智能发展状况：生成式人工智能的爆发之年》调查报告中显示：有近 1/4 的受访首席高管表示，他们个人正在使用生成式 AI 工具开展工作，超过 1/4 的使用人工智能公司的受访者表示，生成式 AI 已列入其董事会的议程。在这一数据中不难发现，很多企业高管已经开始拥抱生成式 AI 了。当然也有很多企业高管仍在洞察这一态势的走向：生成式 AI 能否颠覆行业格局的机遇？生成式 AI 能给自身业务带来什么价值？</p><p></p><p></p><h4>13 分钟读懂生成式 AI《Generative AI for Executives》</h4><p></p><p></p><p>如果您想把握生成式 AI 风向标，如果您想了解生成式 AI 能为你带来什么，如果您想快速拉近与生成式 AI 的距离，如果您想通过深入浅出的方式掌握生成式 AI 云技能，如果您想通过生成式 AI 推动业务增长，如果您想通过生成式 AI 提升竞争力，云师兄五星推荐《Generative AI for Executives 》，步入生成式 AI 快车道，13 分钟足矣！</p><p></p><p>《Generative AI for Executives (英文) 》课程，面向企业管理者、业务人员以及非技术人员，通过一系列免费、简短且易于理解的视频课，可帮助高管了解生成式 AI 如何助力他们应对业务挑战并推动业务增长。</p><p></p><p>亚马逊云科技官方学习中心 <a href=\"https://explore.skillbuilder.aws/learn/course/external/view/elearning/16666/generative-ai-for-executives?trk=d15c46ed-1e4e-4e8b-82c0-63ba2feb26b9&amp;sc_channel=el\"> 学习通道</a>\"</p>",
    "publish_time": "2023-08-30 16:05:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "DingoDB多模向量数据库正式发布，支持多模态数据统一存储和联合分析",
    "url": "https://www.infoq.cn/article/ijdN1DxZ241UgsBqwVop",
    "summary": "<p>InfoQ 获悉，近日，九章云极DataCanvas正式发布DingoDB多模向量数据库，并将其开源。据了解，DingoDB提供了同时处理结构化和非结构化数据的能力，其多模态特性使其在处理不同类型的数据时更加灵活和高效。通过DingoDB，用户可以构建专属的数据向量海“vector ocean”，并实现针对不同行业场景的多模态数据存储、分析和管理的个性化需求。</p><p>&nbsp;</p><p>此外，DingoDB 还将数据湖和向量数据库的特性相结合，可以同时存储和处理多模态数据，并提供结构化与非结构化数据的联合查询和融合分析计算的能力；借助结构化和非结构化的融合分析计算技术，能够高效地管理和检索多模态数据，进一步提升数据的利用价值。</p><p>&nbsp;</p><p>当前，市场上的向量数据库主要有三种形态：一是基于关系型数据库的向量索引，适用于小规模向量数据的存储和查询；二是专用向量数据库，通过使用特定的向量索引、压缩算法和查询优化技术来提供高效的向量存储和检索，一般用于企业级应用设计和优化的数据库解决方案；三是分布式向量数据库：利用分布式计算和存储技术实现了高性能和可扩展性，适用于大规模向量数据集和高并发访问的场景。</p><p>&nbsp;</p><p>DingoDB 作为一种全新的向量数据库形态——分布式多模态向量数据库，具备上述三种数据库全部能力的同时，还支持多模态数据的统一存储和联合分析，进一步扩展了向量数据库的能力边界。DingoDB在向量化数据存储处理方面提供以下功能特性：</p><p></p><p>统一存储：提供统一的数据存储能力，支持单表存储表标量/向量数据，适用于不同数据间的联合查询和分析计算，提供全面的数据处理能力。多模态检索：支持基于不同模态数据的联合查询和检索，非结构化数据向量化存储，有利于处理、分析和应用非结构化数据。联合分析：支持非结构化数据向量化处理，同时提供标量数据与向量数据的联合分析能力，确保用户获取全面、准确的结果数据。一体化SQL计算引擎：使用SQL提供强大的结构化、非结构化的分析能力，实现多模态数据类型的综合分析。异构计算：利用多种不同类型的计算资源执行数据处理和计算任务，提高数据库系统的计算性能、增强扩展性和灵活性，实现高效分析和科学计算。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b9/b972ee7e4e507f5ad32bc7686b4d04a5.png\" /></p><p></p><p>目前，DingoDB已完成与LangChain框架的对接。LangChain是一种基于语义的知识图谱技术，它可以实现语义级别的数据关联和查询优化，从而提高数据库查询的效率和准确性。基于LangChain构建的数据库对接可以为数据查询效率带来一些潜在的提升。通过与LangChain的合作，DingoDB能够实现与其他开源工具和技术更好的集成，从而向用户提供更强大的数据能力。</p><p>&nbsp;</p><p>DingoDB地址：<a href=\"https://www.dingodb.com/\">https://www.dingodb.com</a>\"</p><p>DingoDB GitHub地址：<a href=\"https://github.com/dingodb\">https://github.com/dingodb</a>\"</p>",
    "publish_time": "2023-08-30 16:12:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "亚马逊终止低代码 Honeycode 服务",
    "url": "https://www.infoq.cn/article/cJgNCmuqrkgdeXbj0aKe",
    "summary": "<p>近日，亚马逊宣布将终止其低代码 Honeycode 服务，新的注册将立即被阻止，现有客户的应用程序只能继续运行到 2024 年 2 月 29 日。</p><p>&nbsp;</p><p>Honeycode 于 2020 年 6 月<a href=\"https://aws.amazon.com/about-aws/whats-new/2020/06/announcing-amazon-honeycode/\">推出测试版</a>\"，作为“一项完全托管的服务，允许客户快速构建强大的移动和 Web 应用程序，而无需编程。”2022年 4 月，该团队<a href=\"https://honeycodecommunity.aws/t/announcing-the-next-generation-of-amazon-honeycode/16941\">宣布了</a>\"“下一代 Honeycode”的测试版，旨在通过新的构建器用户界面和对图像的支持进一步降低复杂性，模板涵盖的应用程序包括费用报告、库存系统、活动规划器、休假报告和反馈调查。</p><p>&nbsp;</p><p>亚马逊建议那些陷入困境的“尊贵客户”使用 Honeycode 的“导出数据”选项，并表示“我们将保留您的数据直到 2024 年 4 月 29 日。如果您不采取任何行动，您的数据将在 2024 年 4 月 30 日被删除。”</p><p>&nbsp;</p><p>亚马逊补充表示，Honeycode（RIP，2020-2024）的精神将在其其他产品中延续：“我们正在将 Amazon Honeycode Beta 版的经验教训融入到当前的服务中，并继续致力于支持无/低代码服务，包括 Amazon SageMaker&nbsp;Canvas&nbsp;(2021-?)、AWS Amplify Studio&nbsp;(2021-?) 和AWS AppFabric&nbsp;(2023-?)。</p><p>&nbsp;</p><p>从社区论坛（也构成了主要文档）来看，Honeycode 的最大问题是使用率有限。每个月只有少数新主题。目前为止，生命周期终止公告只收到了 6 条回复，不过<a href=\"https://honeycodecommunity.aws/t/honeycode-ending-soon-community-discussion/28317/8\">其中一条表示</a>\"，“我公司的整个基础设施都是围绕 AWS Honeycode 构建的”。所以还是有些人对此感到失望。</p><p>&nbsp;</p><p>外媒&nbsp;Devclass&nbsp;认为，Honeycode 的问题可能在于过于关注可用性，而对功能关注不够；尤其是在早期，与其他服务和目录的集成很少。此外，AWS 的重点始终是 IT 专家，而不是可能对低代码工具更感兴趣的普通用户。</p><p>&nbsp;</p><p>Hacker News 上一位自称是 “2017 年 Honeycode 的工程师” 的网友发言称：</p><p>&nbsp;</p><p></p><blockquote>我最初加入该项目是因为最优秀的人才蜂拥而至，最终为开发人员打造了一个前端构建器。我在那里工作时，愿景就在那里（允许人们使用电子表格技能构建应用程序），但执行却是一团糟：我们的工程师最感兴趣的是升职，所以这是超级政治性的。我记得每个团队都有自己的 redux 商店（包括一个用于导航栏、一个用于登录屏幕、一个用于主屏幕等）。它完全不起作用，但很多人得到了晋升。一直以来我们都没有一个顾客！&nbsp;今天，我对无代码非常怀疑。感觉公民开发者已经走进了死胡同。我认为 Honeycode 正处于这个恐怖谷，你无法真正将它用于实际应用程序。Honeycode 没有源代码控制、自定义 React 组件，也没有测试）。</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://devclass.com/2023/08/30/muted-response-speaks-volumes-as-aws-scraps-low-code-honeycode/\">https://devclass.com/2023/08/30/muted-response-speaks-volumes-as-aws-scraps-low-code-honeycode/</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=37255765\">https://news.ycombinator.com/item?id=37255765</a>\"</p>",
    "publish_time": "2023-08-30 16:32:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "QCon 北京日程已全部确定，与 LangChain 作者 Harrison 对话",
    "url": "https://www.infoq.cn/article/9GAMjSy7Sz3JlNy51Zef",
    "summary": "<p>随着 Large Language Model 的崛起，通向 Artificial General Intelligence 的灯塔即将被点亮，软件架构、软件工程流乃至上下游的工具都将发生重大变化。对于这个将会持续 20 年甚至 100 年的技术浪潮，今朝启航正当时。</p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=richengwen&amp;utm_campaign=10&amp;utm_term=0830\">QCon 全球软件开发大会·北京站</a>\"，以「启航·AIGC 软件工程变革」为主题，将于 9 月 3 - 5 日正式开幕。</p><p></p><p>本次大会策划了微服务架构治理、构建未来软件的编程语言、面向 AI 的存储、FinOps 落地、从 BI 到 BI+AI，新计算范式下的大数据平台、AIGC 浪潮下的研发效能提升、业务安全技术、从 MLOps 到 LLMOps、大模型算法与应用、异构算力、向量数据库、重塑高效组织架构、大前端融合提效、视频与智能创作、打造高效协同工具、卓越项目管理、大前端新场景探索、AIGC 产品设计（PCon）等近 <a href=\"https://qcon.infoq.cn/202309/beijing/track?utm_source=infoqweb&amp;utm_medium=richengwen&amp;utm_campaign=10&amp;utm_term=0830\">30 个精彩专题</a>\"。我们将围绕 LLM 所带来的变化，从大模型算法、工程到应用，再到数据和软件架构等方面展开研究和讨论。通过与你共同探讨软件研发范式的变革之路，我们希望能够迎接未来的挑战，引领技术的发展，真正普惠千行百业。</p><p></p><p>为什么创造 LangChain，Harrison 在构建 AI 应用程序时看到了什么？以 ChatGLM 为代表的认知大模型与应用的走向如何？AI 原生时代来临，云计算在技术、范式、理念将发生怎样的变化，百度如何应时而变？阿里云在多模态数据存储、管理和应用上又有什么思考？未来十年，大模型产业还有哪些落地的机遇与挑战，智源研究院有哪些视角？</p><p></p><p>这一次，来 QCon 北京与 LangChain 作者 Harrison Chase，Lepton AI 创始团队成员、产品负责人 Yuze Ma 鱼哲 ，智源研究院副院长兼总工程师林咏华，智谱 AI CEO 张鹏博士，百度智能云技术委员会主席王耀，阿里云智能集团研究员 Alex Chen ，双环传动董事、副总经理、COO 李水土等大咖对话，共同奔赴大模型技术浪潮之巅。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/94c8f0fbe70983411425512147f0a400.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/5733c3ea15840deacd887626267b4cd8.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/86f56f94f7528c140961443399d6b2bb.png\" /></p><p></p><p>无论您是技术团队负责人、架构师、工程总监、开发人员，还是对 LLM 技术感兴趣的学者，我们都诚挚邀请您参与本次大会，共同探讨 LLM 技术浪潮下的软件研发范式变革，共创未来。</p><p></p><p>QCon 北京仅剩最后 5 天，咨询购票优惠信息可联系票务经理 18514549229（微信同手机号）。扫码或点击<a href=\"https://qcon.infoq.cn/202309/beijing/schedule?utm_source=infoqweb&amp;utm_medium=richengwen&amp;utm_campaign=10&amp;utm_term=0830\">链接</a>\"即可查看 QCon 北京站完整日程，期待与各位开发者现场交流。</p><p></p><p>相关会议推荐</p><p></p><p>11 月，来自以色列、新加坡、美国的专家，将会出席在上海举办的<a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=richengwen&amp;utm_campaign=10&amp;utm_term=0830\"> FCon 全球金融科技大会</a>\"，分享国外的金融创新技术、数字化转型、金融大模型、Web3 商业化、金融数据治理等话题。</p><p></p><p>FCon 主要是站在金融行业视角来分享话题，围绕技术、业务、运营、产品和人才培养来传递各个公司的经验，受众涵盖技术线专家和业务线的从业人员，帮助大家开拓视野，了解其他公司的动态。</p><p></p><p>现早鸟期 5 折抢票，仅限前 100 人，咨询购票可联系：13269078023（微信同手机号）。扫码可查看全部演讲专题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/00/0059525487f952f6e0ddcab29ef545e1.png\" /></p><p></p>",
    "publish_time": "2023-08-30 16:37:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]