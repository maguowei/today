[
  {
    "title": "React 19 要来了：你准备好了吗？｜讨论",
    "url": "https://www.infoq.cn/article/R7x8TGNPFrQvBqo151y3",
    "summary": "<p>React，作为前端领域中备受瞩目的技术框架之一，自其诞生以来一直以快速的版本迭代和引入新特性而闻名。然而，最近一年多以来，React 18版本停滞不前，当前的稳定版本是18.2，发布于22年6月，之后便没有新的稳定版本发布。直到今年2月15日，官方博客才透露了下一个稳定版本的计划，即React 19。具体更新新闻大家可以看这篇报道：<a href=\"https://www.infoq.cn/news/2t41mOMj3Hw2fBiTGLJH\">沉寂 600 多天后，React 憋了个大招</a>\"。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a9/e4/a96be12495e7d0f490e70b37a26202e4.png\" /></p><p></p><p></p><p>尽管停止了稳定版本的发布，但React在过去一年中的活跃度并未降低。从代码提交量来看，React仍然保持着较高的活跃度，主要集中在底层架构的重构，因此一年没发新版并非意味着停滞不前，而是其发展理念转变的必然结果。</p><p></p><p>对于 React 19，普通开发者无需担心学习成本过大。如果你不使用 Next 等框架，那么你大概率不会接触到 RSC 体系下的新特性。React 19 对你的最大影响可能就是新特性对老 API 的影响，例如 useContext 变为 use(promise)等。</p><p></p><p>了解更多</p><p></p><p>React 官方博客： <a href=\"https://react.dev/blog\">https://react.dev/blog</a>\"YouTube 视频: <a href=\"https://www.youtube.com/watch?v=B-tjhF7ojeA\">https://www.youtube.com/watch?v=B-tjhF7ojeA</a>\"</p><p></p><p>关于React 19 的进一步消息，可以关注今年 5 月 15 日至 16 日的 React Conf 。那么你最期待 React 哪些功能更新呢？欢迎留言～</p><p></p><p>InfoQ 为大家在前端学习的道路上准备了一份厚礼！扫码免费领取～</p><p><img src=\"https://static001.infoq.cn/resource/image/b7/fd/b74de49052a627a99c4769d3ec6037fd.png\" /></p><p></p>",
    "publish_time": "2024-02-27 10:30:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "aiXcoder 代码大模型在企业的应用实践",
    "url": "https://www.infoq.cn/article/JQsMsolAbd7TQ7kI4cVA",
    "summary": "<p>本次演讲针对代码大模型在企业真实落地中存在数据安全、计算资源有限、企业领域知识融合等挑战，结合实际落地案例，介绍 aiXcoder 代码大模型的构建与应用实践，包括代码大模型的训练、私有化部署、结合企业领域知识的个性化训练等。</p>\n<p>听众收益点：</p>\n<p>了解代码大模型的构建基本流程；<br />\n企业如何应用代码大模型提升研发效能。</p>",
    "publish_time": "2024-02-27 11:24:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "借助 LangChain 与 LLM Agent 加速生成式 AI 应用开发",
    "url": "https://www.infoq.cn/article/mSVS28xKaCQMfKR3lZDq",
    "summary": "<p>本次分享将会介绍 LLM-based Agent 特性，业界发展情况以及如何利用 Agent 构建生成式 AI 应用，让你快速借助 LangChain 和 Agents for Amazon Bedrock 构建企业自己的可落地的生成式AI应用。</p>\n<p>听众收益：</p>\n<p>了解 LLM-based Agent 特性与发展<br />\n初步掌握如何利用 LLM-based Agent 构建生成式 AI 应用</p>",
    "publish_time": "2024-02-27 11:24:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI Native 化的大前端开发模式",
    "url": "https://www.infoq.cn/article/CK0PJe62BNc5N15PaUIj",
    "summary": "<p>在本次的分享中，我将向大家展示如何将传统的交互方式与对话流相结合，这包括上下文和状态流转的设计策略。除了交互设计本身，我们还会探讨特定场景下的 PatternPlugin，这将涉及如何将状态机技术应用于肉鸽游戏和活动设计。此外，我还将分享如何利用大型模型来进行业务监控和效果评估。</p>\n<p>听众收益点：</p>\n<p>了解传统交互与 LLM 对话的结合方案<br />\n了解 PatternPlugin 状态机的设计<br />\n基于大模型的 EDD 设计思想</p>",
    "publish_time": "2024-02-27 11:36:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Sora 技术报告深度解读",
    "url": "https://www.infoq.cn/article/w72jY5dZOrW1f8ANARpG",
    "summary": "<p></p><h2>你将获得</h2><p></p><p>理解 Sora 令人惊叹的 8 大特性了解 Sora 背后的 6 大技术支撑深入探索 Sora 模拟世界的能力大胆探究 Sora 发展方向与前景</p><p></p><h2>课程介绍</h2><p></p><p>Sora 是啥？到底带来了哪些改变？Sora 背后的技术都有哪些？AGI 时代真的要来了吗？</p><p></p><p>OpenAI 的首个视频生成模式 Sora 发布，效果令人惊叹。作为技术人，除了看热闹，我们还要看门道；咱也不必跟着瞎焦虑，踏实下来研究些干货内容。这个公开课是对 Sora 官方技术报告的深度解读，郑建勋老师带我们从 4 个主题层层深入，看懂 Sora 背后技术，探索更多未来可能。</p><p></p><p>这是最好的时代，这是最坏的时代。而我们，跟上技术发展的脚步，扎扎实实练内功，成为同行者。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/d4/eb/d4ca5c2433cb6802f77e957c919f24eb.png\" /></p><p></p><h2>讲师介绍</h2><p></p><p>郑建勋，Go 语言技术专家，成都慧眸科技创始人。极客时间《Go 进阶 · 分布式爬虫实战》专栏讲师，《Go 语言底层原理剖析》《聚沙成塔：Go 语言构建高性能、分布式爬虫项目》图书作者。Go 语言垃圾回收源码贡献者，Go 语言精度库 shopspring/decimal 核心贡献者。曾就职于人工智能独角兽公司的视觉中台与大型互联网企业的业务中台，拥有丰富的大规模云原生、分布式、微服务集群的实战经验。确保了百万级流量系统的服务稳定性，并经历和主导了复杂业务系统的性能优化与系统重构。</p>",
    "publish_time": "2024-02-27 12:29:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "欧洲版OpenAI被微软收编了，但这家号称专注于“开源”的大模型企业转向了”闭源“？",
    "url": "https://www.infoq.cn/article/k166omoFE72Qrt5ZMMKQ",
    "summary": "<p>今天，微软突然宣布与法国开源大模型初创公司Mistral达成深度合作。</p><p>&nbsp;</p><p>Mistral AI正式成立于2023年5月，估值 20 亿欧元（约合 21 亿美元）。双方将共同开展研发合作，并将 Mistral 的 AI 模型部署在微软 Azure 云计算平台上。这将使 Mistral 成为继 OpenAI 之后，第二家在 Azure 上提供商用语言模型的公司。</p><p>&nbsp;</p><p>而且，据媒体透露，作为交易的一部分，微软还将对 Mistral 进行投资。这将使其成为继 OpenAI 之后，微软投资的第二家 AI 大模型公司。具体投资金额尚未披露。此前，微软投资OpenAI为130亿美元，持有OpenAI约49%股份。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/29/2971a7c9a91df639171d96f967d024c6.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>AI新贵Mistral发布最新旗舰大模型</h2><p></p><p>&nbsp;</p><p>Mistral AI也于今天宣布正式推出最新旗舰模型Mistral Large。这是一种新的语言模型，旨在与 OpenAI 的 GPT-4 直接竞争。</p><p>&nbsp;</p><p>Mistral AI 声称该模型具有“顶级的推理能力”，能用于处理复杂的多语言推理任务，包括文本理解、转换和代码生成。</p><p>&nbsp;</p><p>在常用基准测试MMLU的对比中，Mistral Large的得分仅次于GPT-4，略好于Anthropic开发的Claude 2。至于谷歌的Gemini Pro以及的LLaMA 2 70B模型，则被甩开了一个身位。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b0bfe59bdfca45f0f259be848f9087b7.png\" /></p><p></p><p>&nbsp;</p><p>在推理能力上，Mistral Large也仅次于GPT-4，优于LLaMA 2 70B模型：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef1e12eaf943f7b44cc5762591af6ee3.png\" /></p><p></p><p>&nbsp;</p><p>Mistral Large 具有本地多语言能力。它在法语、德语、西班牙语和意大利语的 HellaSwag、Arc Challenge 和 MMLU 基准测试中明显优于 LLaMA 2 70B。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5a/5a7071d664caf8ffcbcab578586cc34a.png\" /></p><p></p><p>&nbsp;</p><p>各路网友纷纷对其进行了测试，表示其能力“仅次于OpenAI”、“中文文本处理能力无限逼近GPT-4”......</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b5/b5e6dece60e76df31f3bcec8070cc368.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4d/4d2322f6d0b6669d3eff938a24083f64.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Mistral AI 在发布大模型的博客中，同时宣布将他们的开放式和商业模型引入到 Azure 中。所以现在访问Mistral AI 的模型方式为：</p><p>&nbsp;</p><p>La Plateforme：该平台托管在 Mistral 位于欧洲的基础设施上，使开发人员能够利用Mistral AI全系列模型构建应用程序和服务。Azure：Mistral Large 已通过 Azure AI Studio 和 Azure Machine Learning 上线，用户体验顺畅，一些测试版客户已经在使用。自部署：对于最敏感的用例，用户可以在自己的环境中部署Mistral AI的模型，并访问其模型权重。</p><p>&nbsp;</p><p>微软表示与 Mistral 的合作将帮助 Mistral 将其 AI 模型推向市场，并用于开发满足欧洲各国政府和公共部门需求的应用程序。</p><p>&nbsp;</p><p>微软总裁 Brad Smith 发言称，微软与 Mistral 的合作，将推动 AI 技术在欧洲乃至全球的应用和发展。他认为，AI 将创造全新的业务和商业模式，并将对各个行业产生深远影响。</p><p>&nbsp;</p><p></p><h2>这次合作，让Mistral成为“闭源”公司？</h2><p></p><p>&nbsp;</p><p>微软首席执行官萨特亚·纳德拉 (Satya Nadella) 近日称赞了法国初创公司 Mistral AI，将其视为在 Azure 云计算平台上构建人工智能的创新者之一。</p><p>&nbsp;</p><p>Mistral 由三位来自 Meta 和谷歌的前研究人员 Mensch、Timothée Lacroix 和 Guillaume Lample 创立，致力于构建大语言模型，这也是生成式 AI 产品的基础技术。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f080d14f628eb22f406f8b82d569bdd4.png\" /></p><p></p><p>&nbsp;</p><p>Mistral 于去年 12 月的融资中获得了 20 亿欧元的估值，融资金额约为 4 亿欧元。</p><p>&nbsp;</p><p>据英国《金融时报》，该公司承诺将模型开源，这意味着技术细节将公开发布，这与竞争对手 (例如 ChatGPT 制造商 OpenAI) 的做法形成鲜明对比。OpenAI 最新的模型 GPT-4 是所谓的 “黑匣子”，用于构建模型的数据和代码不会提供给第三方。</p><p>&nbsp;</p><p>Mistral 此前也一直专注于开源 AI 软件，他们坚信生成式 AI 技术应该是开源的，允许自由复制和修改 LLM 代码，通过这种方式帮助其他用户快速构建自己的聊天机器人。Mixtral 8x7b则被许多人视为目前性能最好的开源 LLM。</p><p>&nbsp;</p><p>但因为Mistral 没有像往常一样提供 GitHub 或是下载链接，不少网友担心这家公司开始转为“闭源”方向。</p><p>&nbsp;</p><p>而且，还有网友发现，Mistral 更改了他们的网站，删除了之前提及的关于他们对开源社区义务的地方，这也让一些人认为Mistral已经失去了初心。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a450279b20b76245b79dfd7d6744412e.jpeg\" /></p><p></p><p>&nbsp;</p><p>独立科技记者Luca Bertuzzi得到的消息跟《金融时报》完全相反，他发推表示，“与之前的模型不同，Mistral Large 不会开源，换句话说，Mistral正在放弃其备受赞誉的开源方法。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/56a28a1542e157459994195b6ae9252a.jpeg\" /></p><p></p><p>&nbsp;</p><p>“他们提供的最初的信息是‘在 2024 年发布开源 GPT-4 级别模型’，现在他们的立场变了，我们不希望他们成为另一个OpenAI。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/313d63aacac7fe2dd5c9f65ddc5bed03.jpeg\" /></p><p></p><p>&nbsp;</p><p>模型的定价也引发了一些质疑，比如 Mistral Small 的低延迟相比于 Mixtral 8x7B 的提升微乎其微，但输入贵了 2.8 倍，输出贵了 8.5 倍。</p><p>&nbsp;</p><p>那么为什么微软选择和Mistral合作？</p><p>&nbsp;</p><p>微软在其博客中透露，该公司与Mistral AI合作的一个核心方向就是“扩大市场，微软和 Mistral AI 将通过 Azure AI Studio和Azure 机器学习模型目录中的模型即服务 (MaaS) 、MACC服务向客户提供 Mistral AI 的高级模型，提供可替换OpenAI模型的多种选择，包括开源和商用模型。”</p><p>&nbsp;</p><p>微软表示，其数据中心运行着 1,600 个 AI 模型，其中 1,500 个是开源的。公司希望除了支持 OpenAI 等专有技术之外，继续在这个领域提供支持。</p><p>&nbsp;</p><p>而且，训练和开发新的 AI 模型所需的基础设施的建造成本也极高，只有少数几家公司能够参与竞争。</p><p>&nbsp;</p><p>微软总裁 Brad Smith 在巴塞罗那举行的世界移动通信大会上表示，微软将致力于一系列旨在鼓励 AI 创新和竞争的原则。他认为，监管机构最终将关注的更广泛问题是，训练和开发 AI 模型的基础设施是否可以广泛应用于没有自己的数据中心和云基础设施的公司。</p><p>&nbsp;</p><p>微软与Mistral的合作将进一步加剧 AI 领域的竞争。微软、谷歌、亚马逊等科技巨头都在积极布局 AI 领域，并寻求在各自的平台上构建强大的 AI 生态系统。 未来，AI 技术将如何发展，值得我们拭目以待。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://mistral.ai/news/mistral-large/\">https://mistral.ai/news/mistral-large/</a>\"</p><p><a href=\"https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/\">https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/</a>\"</p><p><a href=\"https://twitter.com/satyanadella/status/1762165185513722057\">https://twitter.com/satyanadella/status/1762165185513722057</a>\"</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2024-02-27 14:12:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI Native 化的大前端开发模式 | QCon",
    "url": "https://www.infoq.cn/article/CK0PJe62BNc5N15PaUIj",
    "summary": "<p>在本次的分享中，我将向大家展示如何将传统的交互方式与对话流相结合，这包括上下文和状态流转的设计策略。除了交互设计本身，我们还会探讨特定场景下的 PatternPlugin，这将涉及如何将状态机技术应用于肉鸽游戏和活动设计。此外，我还将分享如何利用大型模型来进行业务监控和效果评估。最新会议动态：QCon 全球软件开发大会暨智能软件开发生态展将于4月11-13日在北京·国测国际会议会展中心举办，点击<a href=\"https://qcon.infoq.cn/2024/beijing/?utm_source=qconshanghai&amp;utm_medium=playback\">链接</a>了解大模型如何革新软件开发全流程。</p>\n<p>听众收益点：</p>\n<p>了解传统交互与 LLM 对话的结合方案<br />\n了解 PatternPlugin 状态机的设计<br />\n基于大模型的 EDD 设计思想</p>",
    "publish_time": "2024-02-27 11:36:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "华为发布通信行业首个大模型，提供基于角色的Copilots和基于场景的Agents应用能力",
    "url": "https://www.infoq.cn/article/0PuOGSrqVePTwmZh5lk3",
    "summary": "<p>当地时间2月26日，在MWC24巴塞罗那展期间，华为发布了通信行业首个大模型。据了解，华为通信大模型是一款基于AI的商用大模型，采用先进的技术和算法，提供关键的智能化技术能力，用于优化通信网络性能、智能调度资源等，实现5G-A（5.5G）时代的智能化目标。</p><p>&nbsp;</p><p>针对行业提出的敏捷业务发放、精准用户体验保障、跨领域高效运维的高阶智能化目标，该大模型提供基于角色和基于场景的智能化应用，助力运营商赋能员工、提升用户满意度，全面使能网络生产力。</p><p>&nbsp;</p><p>华为董事、ICT产品与解决方案总裁杨超斌介绍，华为通信大模型发挥智能化技术优势，提供基于角色的Copilots（AI助手）和基于场景的Agents（智能体）的两类应用能力，帮助运营商赋能员工的同时，提升用户满意度，最终将全面提升网络生产力。</p><p>&nbsp;</p><p>杨超斌还分享了华为通信大模型的典型场景实践。在敏捷业务发放案例中，通过放号助手的多模态精准评估，实现了快速用户放号；在用户体验保障案例中，通过大模型的寻优能力，实现了多目标体验保障；在辅助排障场景下，跨流程的质差分析和对话辅助处理，显著改善了故障处理效率。</p><p>&nbsp;</p><p>在MWC24巴塞罗那大会上，华为公司高级副总裁、ICT销售与服务总裁李鹏表示，2024年是5G-A商用元年，结合云和AI技术的发展，运营商商业增长的潜力巨大。李鹏指出，全球运营商可以抓住四个方面的战略机会：优质网络是实现商业成功的基础；多维体验变现，充分挖掘网络每比特的价值；新业务不断涌现，支撑面向未来的持续增长；生成式AI，驱动移动产业走向全面智能化。</p>",
    "publish_time": "2024-02-27 15:51:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型时代：最大化CPU价值的优化策略|Qcon",
    "url": "https://www.infoq.cn/article/RFIYuIN1wdG1lQAjMfaI",
    "summary": "<p>本次演讲将探讨在大语言模型时代充分利用 CPU 资源的关键策略。具体介绍一些结合硬件特性的优化方法，例如利用 CPU 的多核特性、采用并行计算和 AMX 指令集扩展技术来提高处理速度。</p>\n<p>此外还将介绍一种结合 CPU 和 GPU 的投机采样方法，通过在 CPU 上运行部分计算任务，充分利用 CPU 资源并减少对 GPU 的依赖。最后，我将分享一些最新的性能情况，让您了解这些优化策略的实际效果。通过这些方法，您将能够更好地利用 CPU 资源，提高模型推理速度，以更快速高效的实现生成式模型部署落地。</p>\n<p>最新会议动态：QCon 全球软件开发大会暨智能软件开发生态展将于4月11-13日在北京·国测国际会议会展中心举办，点击<a href=\"https://qcon.infoq.cn/2024/beijing/?utm_source=qconshanghai&amp;utm_medium=playback\">链接</a>了解大模型如何革新软件开发全流程。</p>\n<p>听众收益点：</p>\n<p>理解并结合硬件特性进行优化，提高模型推理速度和处理能力<br />\n了解 CPU 上的最新性能情况，为实际业务的大模型线上部署提供更多选择<br />\n掌握结合 CPU 和 GPU 协同工作的优化策略，减少对 GPU 的依赖，提高资源利用率</p>",
    "publish_time": "2024-02-27 16:17:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "JavaScript前端框架维护者对2024年的预测",
    "url": "https://www.infoq.cn/article/7ciYZE56w7KENEGOjfQY",
    "summary": "<p>本文最初发表于<a href=\"https://thenewstack.io/2024-predictions-by-javascript-frontend-framework-maintainers/\">The New Stack网站</a>\"，由InfoQ中文站翻译分享。</p><p>&nbsp;</p><p>在2024年，前端领域会有怎样的发展呢？因为没有人能够预见未来，所以The New Stack与来自Angular、Next.js、React和Solid的创建者和维护者讨论了他们2024年的计划。以下是前端开发者对未来一年的展望。</p><p>&nbsp;</p><p></p><h2>Angular：可选的Zone.js</h2><p></p><p>&nbsp;</p><p>来自谷歌的Angular DevRel技术主管和经理<a href=\"https://www.linkedin.com/in/mgechev/\">Minko Gechev</a>\"说，在过去的一年里，Angular的两大成就是引入了<a href=\"https://thenewstack.io/angular-qwik-creator-on-how-js-frameworks-handle-reactivity/\">基于Signals的细粒度反应性</a>\"和可延迟视图。他告诉The NewStack，新的一年将会在此基础上进一步关注细粒度的反应性，并使Zone.js成为可选的。</p><p>&nbsp;</p><p>在Angular中，Zone是一个执行上下文，它能够跨异步任务持续存在。在<a href=\"https://github.com/angular/angular/blob/main/packages/zone.js/lib/zone.ts\">这个GitHub仓库中详细介绍了Zone</a>\"，它有五个职责，包括拦截异步任务调度和包装回调以便于进行错误处理，以及跨异步操作的zone跟踪。Zone.js可以创建跨异步操作而持久化存在的上下文，也可以为<a href=\"https://angular.io/guide/zone#:~:text=lifecycle%20hookslink-,Zone.,lifecycle%20hooks%20for%20asynchronous%20operations\">异步操作</a>\"提供生命周期钩子。</p><p>&nbsp;</p><p>Gechev说，“我们正在探索为现有的项目实现可选的Zone.js，开发人员可以通过重构现有的应用程序来利用这一特性”。“借助可选的Zone.js，我们期待加载时间能够得以改善，并实现更快的初始渲染。对细粒度反应式的处理将提升到一个新的层次，使我们能够仅探测组件模板的部分变更。”</p><p>&nbsp;</p><p>他说，这些特性将会带来更快的运行时。</p><p>&nbsp;</p><p>在关于性能的另一个方面，Angular正在考虑是否默认启用混合渲染。他补充说，选择避免混合渲染是可能实现的，因为这会增加托管需求和成本。</p><p>&nbsp;</p><p>Gechev说，“我们看到了SSG（静态站点生成，static site generation）和SSR（服务器端渲染，server-side rendering）的很多价值，而且在v17中奠定了坚实的基础，我们正在进行最后的润色工作，以便从一开始就实现这种体验。”</p><p>&nbsp;</p><p>他补充到，另一个优先事项是完成<a href=\"https://github.com/angular/angular/discussions/49685\">Signals请求的评论</a>\"(Request for Comment)。</p><p>&nbsp;</p><p>开发人员还可以看到Angular文档的改进。根据对开发人员的调查，他们希望能有一个升级的学习体验，其中包括让Angular.dev成为该框架新的托管地。他补充说，开发人员还优先考虑了初始加载时间（混合渲染、部分填充和可选的Zone.js应该可以部分解决该问题）和组件编写，Angular计划会进一步简化组件的编写。</p><p>&nbsp;</p><p>Gechev说，“我们致力于迭代式地交付特性，并随着时间的推移逐步增强它们”。“开发人员将能够从2024年的所有改进中受益，并在接下来的几年里获得更好的开发体验和性能。”</p><p>&nbsp;</p><p></p><h2>Next.js：新的编译器</h2><p></p><p>&nbsp;</p><p>Next.js在2023年引入了一个新的应用服务器，旨在支持React服务器组件（React Server Component，RSC）和<a href=\"https://blog.logrocket.com/diving-into-server-actions-next-js-14\">Server Action</a>\"。Vercel（该框架的监管者）的产品负责人Lee Robinson说，该框架会继续支持旧的服务器，并且其路由系统可以在两者实现互操作性。这种互操作性意味着可以继续花时间添加新的特性。</p><p>&nbsp;</p><p>Robinson说，“有些客户已经使用Next.js进行开发有五六年之久了，他们也会需要花费几年的时间来采用这些新特性。在这个过程中，我们希望他们的工作能够尽可能地顺畅。”</p><p>&nbsp;</p><p>在新的一年中，Next.js希望要解决很多问题，其中一个优先事项就是简化缓存。他说，从开发人员的体验来看，这可能会使其更加容易。</p><p>&nbsp;</p><p>Robinson说到，“通常情况下，生态系统中的许多开发人员必须引入一堆额外的包，或者学习如何使用其他的工具来进行数据抓取、缓存和重现校验”。“现在，Next.js已经构建了很多这样的特性，而且非常强大，但是这也意味着需要学习更多的东西，我们得到的初步反馈是，‘这非常棒，也很强大，但是我们希望它能更简单一些’”。</p><p>&nbsp;</p><p>Next.js团队还将继续关注性能的改进，他将其称之为“我们的持续投资”。</p><p>&nbsp;</p><p>他补充到，在新的一年里，这很可能会以新编译器的形式出现，它将加快在开发人员的机器上启动Next.js的速度。该编译器相关的工作已经进行了大约一年的时间，Vercel一直在其内部产品和应用中使用它。他说，这个使用Rust编写的编译器即便在没有使用的缓存的情况下也比之前启用缓存的编译器更快。</p><p>&nbsp;</p><p>Robinson说，“距离推出这个功能已经近在咫尺了，甚至到了每个人都可以默认启用它的程度，而且它比现有基于Webpack的编译方案更快”。“开发人员总是希望他们的工具能够更快。他们对工具运行速度的追求永无止境。因此，很有意思的一件事是，工具的制造商，而不是工具的使用者在开始转向像Rust这样的底层工具，从而帮他们实现性能方面的最终胜利。”</p><p>&nbsp;</p><p>他们的第三个目标是为未来十年的Next.js奠定基础。</p><p>&nbsp;</p><p>他说，“我们对这个新的路由系统感到非常兴奋。我们相信这是未来的基础。但是，这也需要时间。人们会尝试使用，从而有特性方面的需求，并且希望能够看到实际的改变。我们将其视为未来五年至十年的长期投资。”</p><p>&nbsp;</p><p>他补充说，一个“某天”可能会实现的目标是在Next.js中找到更好方式来处理内容，不过这应该不会新的一年中实现。</p><p>&nbsp;</p><p>他补充说，“现在，它依然能够正常运行，你依然可以连接到任何你想要的内容源，但是会有一些潜在的方式能够简化开发人员的体验”。“这更像是一件锦上添花的事情，而不是必须要做的事情，这就是为什么我认为在2024年我们不会实现它，但我希望未来会为其做一些相关的工作。”</p><p>&nbsp;</p><p></p><h2>React：2024预览</h2><p></p><p>Meta的React工程主管Eli White说，React团队希望在新的一年里会有更多的框架采用React服务器组件（RSC，React Server Component）。</p><p>&nbsp;</p><p>White说，“对大多数人来讲，他们认为RSC是React领域的重要变化，从一个单纯的UI层，变成对如何架构应用程序的方式都能产生更大的影响，以获得最佳的用户和开发人员体验，特别是对于单页应用程序（SPA，single page application）方式表现不够好的应用程序。”</p><p>&nbsp;</p><p>虽然他没有明确说明2024年的新动向，但是White表示他们将发布和分享2023年相关事项的更多进展。比如，在React Advanced会议上，团队向与会者展示了React Forget，这是React的自动记忆编译器。White说，<a href=\"https://dev.to/usulpro/how-react-forget-will-make-react-usememo-and-usecallback-hooks-absolutely-redundant-4l68\">有了React Forget之后，将意味着开发者不再需要使用useMemo和useCallback</a>\"。</p><p>&nbsp;</p><p>White补充说，“在React Native EU会议上，我们分享了从0.73版本开始将web开发人员熟悉的Chrome开发工具引入React Native。我们还初步展示了对Static Hermes的研究成果，这是针对JavaScript的原生编译器，它不仅有可能加快React Native应用程序的速度，而且会从根本上改变JavaScript的用途。”</p><p>&nbsp;</p><p></p><h2>Solid：专注于基本元素（Primitive）</h2><p></p><p>Solid创始人Ryan Carniato表示，在2024年，Solid开发人员可以期待即将推出的<a href=\"https://start.solidjs.com/getting-started/what-is-solidstart\">SolidStart 1.0</a>\"和Solid.js 2.0。SolidStart是一个元框架，这意味着它建立在框架Solid.js之上。他说，这类似于<a href=\"https://thenewstack.io/rich-harris-talks-sveltekit-and-whats-next-for-svelte/\">Svelte的SvelteKit</a>\"。</p><p>&nbsp;</p><p><a href=\"https://start.solidjs.com/getting-started/what-is-solidstart\">SolidStart的文档</a>\"是这样描述的：</p><p></p><blockquote>“Web应用程序通常包含很多组件，比如数据库、服务器、前端、打包器、数据抓取/变更、缓存和基础设施。编排这些组件很具挑战性，并且通常需要跨应用程序技术栈共享大量的状态和冗余逻辑。这就是SolidStart的用武之地，它是一个元框架，提供了一个将所有这些组件组合在一起的平台。”</blockquote><p></p><p>&nbsp;</p><p>由于SolidStart仍处于beta阶段，Carniato有机会利用生态系统中已有的东西将其变得更好。</p><p>Carniato说，“其中很重要的一点是，我们现在不再需要编写自己的部署适配器，而是可以使用Nitro，它也支撑了Nuxt框架，这使得我们可以将其部署到所有的不同平台上。”</p><p>&nbsp;</p><p>另外一个样例是任何Solid路由器都能在SolidStart中运行。</p><p>&nbsp;</p><p>他说，“这意味着对路由器的底层部分进行了大量更新，这样它们才能一起运行，但最终结果令我非常高兴，因为我们小团队的志愿者需要维护的代码量要少得多，这给了开发人员很大的灵活性。”“他们不必被迫采用单一的解决方案，这对我来说非常重要，因为每个人都有自己的需求。正如我所言，如果你构建了正确的组件并且能够弄清楚构建基块（building block），那么人们可以做更多的事情。”</p><p>&nbsp;</p><p>他说，最终的结果是由“可互换”的组件所组成的元框架，而不是具有很强的倾向性。Solid团队一直在思考，在由越来越多的元框架决定开发人员能够使用什么的世界中，正确的基本元素所带来的影响。</p><p>&nbsp;</p><p>他说，“对我来讲，最重要的一直是基本元素形成的构建基块，这是一个非常工程化的关注点，我认为这也是它与众不同的原因之一。”“我一直喜欢给别人选择的权力，并且我认为如果有正确的基本元素，正确的构建基块，就可以构建出正确的解决方案。”</p><p>&nbsp;</p><p>他表示，Solid 2.0应该会在2024年中后期发布。现在，他们正在实现如何处理异步系统的原型。</p><p>&nbsp;</p><p>Carniato说，“Solid 2.0也将是一个非常重要的版本，因为我们正在重新审视反应式系统，并思考如何解决异步信号或异步系统的问题。”</p><p>&nbsp;</p><p>他补充说，Solid试图在控制和性能之间取得平衡。</p><p>&nbsp;</p><p>他说，“在我们的社区里，有很多非常热情的人，非常有技术头脑的人，他们关注性能，关心控制。”“我们确实吸引了很多人，他们真的想控制自己建造的每一个组成部分。”</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://thenewstack.io/2024-predictions-by-javascript-frontend-framework-maintainers/\">https://thenewstack.io/2024-predictions-by-javascript-frontend-framework-maintainers/</a>\"</p>",
    "publish_time": "2024-02-27 18:10:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云服务遇到大模型：青云 AI 在线推理服务解析",
    "url": "https://www.infoq.cn/article/cv2qCpFVSMaflWYEuyIZ",
    "summary": "<p>作者 | 梁朝东，刘庆，杜炜，樊军伟，赵玉萍</p><p></p><p>在快速发展的生成式 AI 浪潮中，大语言模型推理是一个主流的工作负载，众多云服务提供商都致力于提供实时高效的大语言模型推理服务。青云 QingCloud 已经基于第四代英特尔®至强®&nbsp;可扩展处理器和 BigDL-LLM 大语言模型推理方案开发并上线了实时低延迟的大语言模型推理服务。本文介绍了青云 AI 在线推理服务，以及其中应用到的大语言模型技术和优化。</p><p></p><p></p><h2>青云 AI 在线推理服务</h2><p></p><p></p><p>青云科技近期推出了青云模型市场试用版，此试用版目前已基于青云已有的应用市场扩展了“大模型”分类，支持了众多国内外开源模型，如 ChatGLM3、Baichuan2、LLaMA2 等。其中，青云 AI 在线推理服务（公测版）构建在模型市场上，用户可使用开源模型，或者自行上传私有模型镜像，使用简单步骤即可实现快速大模型应用的部署。</p><p></p><p>青云 AI 在线推理服务运行于基于第四代英特尔®至强®&nbsp;可扩展服务器的青云 E4 云主机，采用了基于英特尔 BigDL-LLM 的大语言模型推理的运行时（runtime），支持实时低延迟大语言模型推理。目前该服务已上线，用户访问青云网站即可体验大语言模型的高效在线推理服务。</p><p></p><p>“青云 AI 在线推理”的访问界面如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5dc46be24e4c893db3578f69cf4b6a37.png\" /></p><p></p><p>用户登陆青云公有云，进入 AppCenter 控制台，选择“青云 AI 在线推理”。按照页面提示的步骤开始创建服务，在基本配置选项中，选择 intel-runtime，即可创建带有 AMX 特性的青云 E4 云主机，并可指定由 BigDL-LLM 提供低延迟推理能力。</p><p></p><p>经过服务器配置（推荐使用 16 核 32GB 内存的青云实例），网络配置（VPC 网络），服务环境配置（配置镜像仓库等）等步骤，即可以提交进行服务部署。如果成功部署，则可以看到 AI 在线推理服务的节点状态为“活跃”，服务状态为“正常”。　</p><p></p><p>通过青云负载均衡器提供的公网 IP，可以在浏览器访问部署成功的 “青云 AI 在线推理服务”，示例如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab7488d4ea69ba2d845e0106830d476b.png\" /></p><p></p><p></p><h2>BigDL-LLM 大语言模型推理和性能优化</h2><p></p><p></p><p>青云 AI 在线推理服务运行在基于第四代英特尔®至强®&nbsp;可扩展处理器的青云 E4 系列云主机。第四代英特尔® 至强® 可扩展处理器通过创新架构增加了每个时钟周期的指令，每个插槽多达 56 个核心，支持 8 通道 DDR5 内存，有效提升了内存带宽与速度。同时，英特尔® AMX 针对广泛的硬件和软件优化，通过提供矩阵类型的运算，为深度学习推理和训练提供显著的性能提升。</p><p></p><p>青云 AI 在线推理服务采用了 BigDL-LLM 作为大语言模型推理的运行时 (runtime)。BigDL-LLM 是英特尔开源的大语言模型库，能够在广泛的英特尔 XPU 上运行，如移动或桌面的 CPU/GPU、服务器 CPU/GPU，以及云端等设备，并提供了优化的性能表现。这一库支持对任何基于 PyTorch 的模型进行低比特优化，包括 FP4、INT4、NF4、FP8、INT8 、BF16、FP16 等多种数据类型，能显著降低内存占用并提供极低的访问延迟。</p><p></p><p>BigDL-LLM 提供的低比特模型优化技术是一种全面的解决方案，旨在降低大型模型的资源消耗。该技术包括模型量化和访存优化，同时对英特尔硬件进行了特定的优化措施，比如在 CPU 上应用 AVX2、AVX512、AMX 指令集，在 GPU 上则充分利用 XMX 计算单元。此外，BigDL-LLM 还借鉴并优化了多种业界先进的低比特技术，如 llama.cpp、bitsandbytes、qlora 等，并支持多种模型量化类型和策略，如对称 / 非对称量化、低比特类型（INT4、NF4、FP8）及策略（例如 GPTQ，AWQ, GGUF 等）。以 INT4 低比特优化为例，BigDL-LLM 将权重映射到 INT4 的整数空间时，会记录缩放系数，随后在推理过程中使用这个缩放系数恢复原先的权重，最大可能的保持了推理过程中的准确性。</p><p></p><p>这些技术显著减少了存储空间需求，降低了内存或显存的占用和访问压力，使得大语言模型的性能得到大幅度提升。同时，这些技术使得在显存较小的设备上运行大型模型成为可能，为资源受限的环境提供了强大的支持。</p><p></p><p>下图展示了 BigDL-LLM 进行 INT4 推理的主要步骤。用户通过 BigDL-LLM 提供的 Hugging Face Transformer API 将模型加载到内存中，在加载的同时，BigDL-LLM 通过低比特量化技术将模型的权重进行映射（比如将 FP16 的系数映射到 INT4 的整数空间），随后对用户提供的输入序列进行标准的推理工作。BigDL-LLM 支持用户使用熟悉的 Hugging Face Transformer API 进行推理工作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8a3125509a4eb0196525d57ac0a9109.png\" /></p><p></p><p>同时，BigDL-LLM 也采纳了 vLLM 的设计，在解码阶段（decoding）实现了 continuous batching 的优化方案。这一优化能够极大的提高推理服务的吞吐量，并保持很低的延迟。BigDL-LLM 也提供了在英特尔 XPU 平台上的大语言模型微调方案。BigDL-LLM 实现了 QLoRA 微调技术，应用了低比特量化，分布式数据并行，高性能通信等优化，极大的降低了微调过程中对大量内存使用的需求。BigDL-LLM 的大语言模型微调方案在集群或者云环境中可以进行轻松的扩展。</p><p></p><p>用户可以使用 BigDL-LLM 创建和运行大语言模型应用，使用标准的 PyTorch API（例如 Hugging Face Transformers, LangChain 等）在英特尔的 XPU 硬件平台上进行大语言模型的推理和微调。BigDL-LLM 已经适配和验证了众多的业界主流大语言模型，包括 LLaMA/LLaMA2, ChatGLM2/ChatGLM3, Mixtral, Mistral, Falcon, MPT, Dolly/Dolly-v2, Bloom, StarCoder, Whisper, InternLM, Baichuan, QWen, MOSS 等等大语言模型。</p><p></p><p>青云在 E4 云主机和 BigDL-LLM 上测试和验证了十几个主流大语言模型，并进行了性能分析和评估。结果显示，基于英特尔软硬件的大语言模型推理服务可以满足实时，低延迟的性能要求。经过 BigDL-LLM 的量化和低比特性能优化后，Baichuan2 7B 等模型可以获得高达 7 倍的性能加速比。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0e/0e0d0fc46220990587c824ab69af86f6.png\" /></p><p></p><p>测试数据由青云提供。英特尔并不控制或审计第三方数据。请您审查该内容，咨询其他来源，并确认提及数据是否准确。</p><p></p><p></p><h2>总结和展望</h2><p></p><p></p><p>本文介绍了青云基于第四代英特尔®至强®&nbsp;可扩展处理器发布的青云 AI 在线推理服务（公测版），以及其背后使用的大语言模型技术和优化。基于第四代英特尔®至强®&nbsp;可扩展处理器和 BigDL-LLM 大语言模型方案，青云 AI 在线推理服务提供了业界领先的低延迟响应速度。青云还将继续深入探索大语言模型的更多使用场景，与英特尔持续密切合作，在更多英特尔硬件平台（例如第五代至强可扩展处理器等）上推出大语言模型推理的解决方案，同时不断扩展大语言模型的应用能力，提供例如模型微调等功能（基于 BigDL-LLM QLoRA），为用户提供更好的体验和更大的价值。</p><p></p><p>2024 年中，青云模型市场正式版将随青云 AI 智算平台新版本一起发布，为智算平台用户和开发者提供丰富的开源模型、数据集、模型管理、模型部署、模型推理等服务。</p><p></p><p>&nbsp;致谢</p><p>特别感谢英特尔刘芍君、史栋杰，青云王士郁、何颜廷对本文内容的贡献。</p>",
    "publish_time": "2024-02-27 18:22:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]