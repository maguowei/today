[
  {
    "title": "架构师（2023 年 9 月）",
    "url": "https://www.infoq.cn/article/ZQq8d6dXd8TcKqHo4M9Y",
    "summary": "<h2>卷首语：向量数据库会是 AI 的“iPhone 时刻”吗？</h2>\n<p>作者：冬梅</p>\n<p>最近一年，以 ChatGPT、LLaMA 为代表的大语言模型的兴起，将向量数据库的发展推向了新的高度。</p>\n<p>向量数据库是一种在机器学习和人工智能领域日益流行的新型数据库，它能够帮助支持基于神经网络而不是关键字的新型搜索引擎。向量数据库不同于传统的关系型数据库，例如 PostgreSQL，其最初设计用于以行和列的形式存储表格数据。它也明显不同于较新的 NoSQL 数据库，例如 MongoDB，其主要是将数据存储在 JSON 文档中。</p>\n<p>向量数据库是为存储和检索一种特定类型的数据而设计的：向量嵌入。它们本质上是机器学习过程的推理部分中运行新数据的过滤器。</p>\n<p>在大模型部署中，向量数据库可用于存储大模型训练产生的向量嵌入。通过存储代表大模型广泛训练的潜在数十亿个向量嵌入，向量数据库执行最重要的相似性搜索，找到用户提示（他或她提出的问题）和特定向量嵌入之间的最佳匹配。</p>\n<p>大模型爆火后，更多企业开始大力投资向量数据库以提升算法准确性和效率。据相关统计，2023 年 4 月的 AI 投资领域呈增长趋势，尤其是向量数据库领域的投资活动颇为活跃，Pinecone、Chroma 和 Weviate 等向量数据库初创公司都在这个月获得了融资。</p>\n<p>当前的向量数据库在大模型淘金时代扮演着重要角色，它就像一把好的铲子一样，有助于挖掘出更多更宝贵的资源。</p>\n<p>但不能忽视的是，大模型在一定时段内可能无法解决所有问题。虽然有些大模型的创建者相信通用人工智能（AGI）会到来，但不少业内专家认为所谓的AI的“iphone”时刻不会这么快到来。在这个阶段，我们需要着手将大模型应用到实际中，让其具备非常智能的能力，可以进行对话，解决问题等。</p>\n<p>然而，要做好这件事并不容易，因为首先你需要了解如何挖掘金矿，即了解整个流程。就像采金矿一样，需要一套标准的流程，不能只是做好一把铲子，还需要考虑如何做筛子，如何对资源进行更深入的处理。这是一个复杂的过程，需要对向量数据库和大模型进行更深入的了解和探索。</p>\n<h2>目录</h2>\n<p><strong>热点 | Hot</strong><br />\nOpenAI 或于 2024 年底破产？大模型太烧钱了，快把 OpenAI 烧没了！</p>\n<p>吵翻了！到底该选 Rust 还是 Go，成 2023 年最大技术分歧</p>\n<p>IPv4 开始收费！新的 IT 灾难？<br />\n用 Rust 编写，已有 10 万行代码：顶级黑客组织出手，将推出新的反数据收集开源框架 Veilid</p>\n<p>平头哥推出首个 RISC-V AI 平台：软硬件深度协同，支持运行 170 余个主流 AI 模型</p>\n<p><strong>访谈文章 | Interview</strong><br />\n独家对话 AGI 模型“之父” Marcus Hutter：AI 能完成人类半数的工作，但让人类失业是一件美好的事情</p>\n<p>两个多月完成全自研：大模型之争，从 GPU 卷到了向量数据库</p>\n<p>QQ NT 全新重构，探寻 24 岁 QQ 大重构背后的思考</p>\n<p><strong>案例研究 | Case Study</strong><br />\nApache Doris 助力中国联通万亿日志数据分析提速 10 倍</p>\n<p>处理时延降低 24 倍，联通云粒数据引擎优化实践</p>\n<p>解读 Linux 内存管理新特性 Memory folios</p>\n<p>大模型颠覆研发模式：字节跳动是如何在单元测试中落地大模型的?</p>\n<p><strong>推荐文章 | Article</strong><br />\n谷歌的反“背锅”文化</p>\n<p>生成的代码会出错、质量差？面对 AI 编程工具的老大难问题，华为这群人打算这样做</p>\n<p>将 60 多年的 COBOL 语言重构为 Java，IBM 用 AI 工具解决大型机维护难</p>\n<p>七年没能将 Python 集成到 Excel，Python 之父加入微软三年后成了！</p>\n<p><strong>特别专题｜AIGC时代，我们需要什么样的向量数据库？</strong></p>\n<p>百亿级向量检索的向量数据库是如何构建的？</p>\n<p>解决成本、易用性和扩展性三大挑战，星环科技向量数据库从 0 到 1 技术实践</p>\n<p>DingoDB 多模向量数据库正式发布，支持多模态数据统一存储和联合分析</p>\n<p>向量数据库内核面临的技术挑战及应对措施</p>\n<p><strong>特别专栏 | 视频推荐</strong><br />\n本月，这些视频值得一看！</p>",
    "publish_time": "2023-09-08 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenTelemetry Protocol（OTLP）1.0.0版本正式发布：开源可观察性框架迈出关键一步",
    "url": "https://www.infoq.cn/article/qkkLpwRHSargeKqGU68J",
    "summary": "<p>最近，OpenTelemetry Protocol（OTLP）1.0.0版本发布了。<a href=\"https://github.com/open-telemetry/opentelemetry-specification/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">OTLP规范</a>\"描述了遥测数据在遥测源、中间节点（如收集器）和遥测后端之间的编码、传输和传递机制。OTLP是一个通用的遥测数据传递协议，隶属于OpenTelemetry项目。</p><p></p><p><a href=\"https://opentelemetry.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">OpenTelemetry</a>\"（OTEL）是一个开源的<a href=\"https://www.cncf.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">云原生计算基金会</a>\"（CNCF）项目，由<a href=\"https://opencensus.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">OpenCensus</a>\"和<a href=\"https://opentracing.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">OpenTracing</a>\"项目合并而成。它是一个与供应商无关的开源<a href=\"https://opentelemetry.io/docs/concepts/observability-primer/#what-is-observability?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">可观察性</a>\"框架，用于增强、生成、收集和导出遥测数据（如<a href=\"https://opentelemetry.io/docs/concepts/signals/traces/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">跟踪</a>\"信息、<a href=\"https://opentelemetry.io/docs/concepts/signals/metrics/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">指标</a>\"和<a href=\"https://opentelemetry.io/docs/concepts/signals/logs/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">日志</a>\"）。该框架提供了一组API、库、代理和收集器服务，用于捕获分布式跟踪和指标信息。此外，它在2021年早些时候发布了1.0.0版规范，InfoQ对此进行过<a href=\"https://www.infoq.com/news/2021/03/opentelemetry-spec-1-0/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">报道</a>\"。</p><p></p><p>OpenTelemetry通过使用<a href=\"https://opentelemetry.io/docs/concepts/instrumentation/libraries/#opentelemetry-api?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">API</a>\"生成遥测数据、在不同SDK之间实现无缝的指标收集来增强应用程序代码。它提供了<a href=\"https://opentelemetry.io/docs/instrumentation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">特定于语言的集成</a>\"方式和库，如用于Java、Golang、.NET和Python的OTel SDK，让开发人员能够增强他们的代码并捕获遥测数据。通过这些库收集的遥测数据被集中传输给<a href=\"https://opentelemetry.io/docs/collector/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">OpenTelemetry Collector</a>\"，利用OTLP在客户端和服务器之间进行数据交换。OTLP定义了一种序列化模式，与跟踪信息、指标和日志的数据模型紧密相关。</p><p></p><p>作为一个中央仓库，OpenTelemetry Collector负责接收、处理并导出从各种来源收集到的遥测数据，既作为应用程序的本地代理，也作为多个应用程序的网关。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3aadb8046bd283512c13bb5d7d1929fc.webp\" /></p><p></p><p></p><p>Open Telemetry架构图（来源：<a href=\"https://opentelemetry.io/docs/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">https://opentelemetry.io/docs/</a>\"）</p><p></p><p>OTLP在数据序列化、反序列化和网络传输方面发挥着重要作用。它致力于根据数据模型指定一种与之紧密相关的序列化模式，并解决与<a href=\"https://github.com/open-telemetry/opentelemetry-proto/blob/main/docs/requirements.md?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">其他遥测协议</a>\"相关的问题。</p><p></p><p>Honeycomb开发者布道师<a href=\"https://twitter.com/MartinDotNet?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">Martin Thwaites</a>\"告诉InfoQ：</p><p></p><p></p><blockquote>OTLP是OpenTelemetry的核心，它让OpenTelemetry变得比之前已有的东西都更强大，1.0.0版本则更上一层楼。目前，大多数供应商已经在使用OTLP协议接收数据，1.0.0版本的发布给人们带来了更强的信心，并有望让最后的一些观望者也加入使用OTLP的行列。</blockquote><p></p><p></p><p>此外，他还表示：</p><p></p><p></p><blockquote>这很重要，因为它带来了更多的互操作性，减少了在技术栈中加入专有协议库的需求，这对于希望更多地了解应用程序发生了什么的人来说是一个好消息。</blockquote><p></p><p></p><p>OpenTelemetry目前是一个CNCF<a href=\"https://www.cncf.io/projects/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">孵化器项目</a>\"。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/08/otlp-version-one-released/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTM5MDE3MjksImZpbGVHVUlEIjoiNk16N0FmV05zajQxcXBMaiIsImlhdCI6MTY5MzkwMTQyOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.Kt1bLpVB_57v_KxssMEDXo1DDjMHq-sx-fPkkwBVuX4\">https://www.infoq.com/news/2023/08/otlp-version-one-released/</a>\"</p>",
    "publish_time": "2023-09-08 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "关于征集中国人工智能产业联盟数据委员会首批成员的通知",
    "url": "https://www.infoq.cn/article/6lqRZZbyximTXzP7okTI",
    "summary": "<p>随着大模型技术的突破，新一轮人工智能浪潮正在引领各行各业快速发展。数据作为此轮变革的主要驱动力，已成为人工智能发展的关键战略要素。但国内人工智能行业正在面临高质量训练数据供给不足、训练数据治理水平不高、数据供需流通机制不畅等挑战，制约了我国生成式人工智能创新发展。</p><p></p><p>为破解AI数据短缺难题，中国人工智能产业发展联盟（AIIA）成立“数据委员会”。AIIA数据委员会拟定10月中旬正式举办成立仪式，成立后将与人工智能关键技术和应用评测工信部重点实验室、中国通信标准化协会大数据技术标准推进委员会（CCSA TC601）等组织加强协同，共同推动产业研究、标准研制、技术应用等相关工作。主要工作方向包括但不限于以下几个方面：</p><p></p><p>（一）AI数据资源供需对接</p><p>汇聚数据资源持有方、数据标注加工方、数据需求方等主体，聚焦AI数据资源汇聚、需求反馈、数据加工、供需对接等方面，提供AI数据集产业公共服务能力。</p><p></p><p>（二）AI数据技术应用研究推广</p><p>针对数据构建、增强、清洗、标注、治理和合成等共性关键技术和工具平台建设，推动数据合成等新兴技术应用，开展联合攻关和应用推广。</p><p></p><p>（三）AI数据治理体系建设</p><p>面向AI训练的数据全生命周期，围绕数据采集标注、质量管理、开放共享等方面需求，建设AI数据治理标准体系，提升训练数据质量，保证数据真实性、准确性、多样性和可追溯性。</p><p></p><p>（四）AI数据应用创新</p><p>围绕金融、零售、制造、教育等数据密集型行业，探索人工智能数据应用场景示范，推广典型人工智能数据应用方式。</p><p></p><p>（五）AI数据商业模式与政策研究</p><p>对接数据拥有方、数据训练方和数据加工方等主体，探索合作共赢的商业模式，总结业界最佳实践，形成可推广的商业模式；结合国家数据基础制度建设进展，提出适应AI训练场景的数据政策建议。</p><p></p><p>现公开征集组长单位、副组长单位和成员单位，作为首批发起单位，首批报名截至时间为2023年9月30日。</p><p></p><p>请有意向加入AIIA数据委员会的企业，填写报名表（点击“阅读原文”获取）并反馈至wanghongjing@aiiaorg.cn。</p><p></p><p>联系人：</p><p>李老师 18611353631&nbsp; &nbsp;</p><p>樊老师 18612301312</p>",
    "publish_time": "2023-09-08 08:54:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面对复杂技术栈，中泰证券如何做云原生转型？",
    "url": "https://www.infoq.cn/article/3f7M1Fiv0t9AB1F2xpxs",
    "summary": "<p>近年来，云原生转型已成为众多企业关注的焦点，中泰证券也不例外。但中泰证券的云原生转型面临着很大挑战。</p><p>&nbsp;</p><p><a href=\"https://xie.infoq.cn/article/799714640c0e5863f76910022\">中泰证券</a>\"的软件架构非常复杂。除了传统架构，还有一些自行研发的<a href=\"https://www.infoq.cn/topic/cloud-computing\">云原生</a>\"架构、独立框架以及独立的C++框架等，云原生架构的比例较低。数据库方面同样如此，内部数据库类型繁多，Oracle 占了30%，此外还有MySQL、SQL Server等等。过多的技术栈，各种架构交织在一起增加了转型难度。</p><p>&nbsp;</p><p>中泰证券金融科技委员会主任兼科技研发部总经理何波在“2023网易数帆城市行”大会上，分享了自己做云原生转型的过程。面对复杂的技术栈，中泰证券的技术团队的策略是将技术的复杂性与业务研究的复杂性分离。“通过标准化技术，我们可以将技术复杂性下沉，从而更多地关注业务复杂性，减少处理技术细节的时间和精力。”何波表示。</p><p></p><h4>转型过程</h4><p></p><p>&nbsp;</p><p>从软件工程的演进来看，架构逐渐复杂，但基本原则一直清晰：低耦合、高内聚。通过不断将复杂性标准化下沉至基础框架，可以更好地实现从作坊式向工业化的转变，从而应对更加复杂的体系。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/dbe35271fa382f48c73e7224d588efd0.png\" /></p><p></p><p>中泰证券在构建微服务体系的过程中，经历了技术选型、技术验证、引入开源实现及完全自研等一系列的过程。对于技术选型，中泰证券不希望重复发明轮子，也不希望完全受制于开源的实现，所以在技术选型时遵循如下原则：</p><p><img src=\"https://static001.geekbang.org/infoq/55/55f8c88b2038941bcea95b47c61f2af8.png\" /></p><p></p><p>在云原生转型过程中，中泰证券按照一系列明确的步骤进行了架构的迁移，其中一个关键举措是将应用与环境进行解耦。中泰证券需要逐步将原有的粗放式架构层层拆分，将其中台化。新自研项目除少数性能敏感的系统外，全部采用微服务架构；传统架构根据业务分类分组按需进行服务拆分改造；异构系统按需做服务适配，以API形式提供服务，保护投资，复用能力。</p><p>&nbsp;</p><p>通过微服务的转型，技术团队成功将研发工作从上层抽离出应用和环境，也解决了过去部署过程中常见的配置文件和环境问题。借助Kubernetes（K8s），团队成功实现了配置与环境的解耦。目前，中泰证券已经成功在11个K8s集群上运行了2000多个业务。</p><p>&nbsp;</p><p>应用和环境解耦后，接下来就是应用和配置的解耦。在传统的开发模式中，许多应用都含有大量的配置文件，有时甚至会将配置存储在数据库中。然而，这种做法在软件的配置、交付和上线过程中经常导致问题，因为不同环境中的配置可能不同。但实际上，应用本身与配置是没有直接关联的。按照云原生原则，中泰证券将配置转移到环境变量中，并通过配置中心进行管理，成功将应用与配置解耦，使得测试环境和生产环境中的应用完全一致，避免了配置文件引起的问题。目前，其已经建立了4个配置中心，涵盖了80%的自研项目。</p><p>&nbsp;</p><p>中泰证券的另一个重要改进是将中间件进行PaaS化。团队采用了网易数帆轻舟云原生平台作为中间件的资源池，将资源的管理从单个应用下沉到资源池中，避免了每个应用独立构建这些中间件。无论是Redis、Kafka还是多达40种不同版本的MySQL数据库，都被纳入了这一PaaS化的管理中。</p><p>&nbsp;</p><p>金融机构的数据库存储量是很大的。为了提高维护性和升级的便捷性，中泰证券对数据库进行轻量化处理，摒弃存储过程，回归到分布式数据库存储。这样，架构不仅保障了数据的完整性，还将大部分业务逻辑下沉到应用层，资源层的PaaS化也为外部提供了便利。此外，中泰证券对制品库进行了全面的管理，实现了统一的交付渠道。</p><p></p><h4>安全建设</h4><p></p><p>&nbsp;</p><p>技术发展过程中，建立一个更加安全和高效的治理体系至关重要。而中泰证券主要面临着技术栈多样性的挑战，涵盖了注册发现、配置中心、底层数据库、PaaS等。</p><p>&nbsp;</p><p>对于安全问题，中泰证券的技术团队认为，最关键的是要能够及早暴露出故障率。除了前述的技术栈，确保良好的监控和可观测性显得尤为重要。在实际操作中，技术团队需要明确监控的程度，确定哪些指标有意义、哪些无意义，并对衡量标准作出充分考虑，避免系统出现问题时仓促采取行动。</p><p>&nbsp;</p><p>因此，中泰证券非常重视混沌工程的实践，通过模拟红蓝军演练，在系统中引入破坏并尝试恢复，从而更快地发现潜在故障。在这个过程中，技术团队进行了攻防演练，涵盖了大量故障处理，包括基础故障、CPU、内存使用率、磁盘使用率、中间件问题、MySQL异常等。这一实践在某二级部门中得以应用，涉及到20多个业务场景、100多个混沌场景以及300次的演练，有力保障了业务的运行。</p><p>&nbsp;</p><p>接下来，中泰证券进入安全工具的应用阶段，通过代码扫描等安全工具，在CICD安全环节中加入了更多的监测。此外，无论是常规发布还是紧急发布，安全门禁都被置于产品的每个关键点，以确保发布和审核的安全性。</p><p></p><h4>结束语</h4><p></p><p>&nbsp;</p><p>为了评估转型效果，中泰证券对资产进行了分类：实用资产和战略资产，实用资产包括服务器、网络、财务系统等，战略资产包括企业用来向客户提供产品或服务的工具，可以形成差异化竞争优势。去年，中泰证券的战略资产占比为20%，如今已提升至28%左右。</p><p>&nbsp;</p><p>总体而言，云原生转型带来了研发质量和效率的提升。通过减少重复劳动和不必要的开发，中泰证券实现了研发效率的增加。测试能力的提升以及自动化测试平台的使用，也节省了大量的测试时间，大幅度降低了测试成本。</p><p>&nbsp;</p><p>何波还表示，中泰证券最近也在尝试用大模型自动生成代码、自动化测试等，希望开发人员可以聚焦在写业务代码上，并且通过云原生改造真正把技术沉淀下来，变成一些标准化的东西使用。</p><p>&nbsp;</p><p>目前，中泰证券自研项目的研发部分完成了约70%。“我们的目标是进行全面的云原生转型。”何波表示，云原生带来了研发质量和效率的提升，通过敏捷IT能力的构建，为证券业务数字化转型加速，更好地支撑业务创新与客户体验提升。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-09-08 09:56:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "当 Apache Doris 遇上大模型：探秘腾讯音乐如何基于大模型 + OLAP 构建智能数据服务平台",
    "url": "https://www.infoq.cn/article/uIbplGaLM0pJTTWcLsgT",
    "summary": "<p></p><blockquote>当前，大语言模型的应用正在全球范围内引发新一轮的技术革命与商业浪潮。腾讯音乐作为中国头部在线音乐娱乐平台，利用庞大用户群与多元场景的优势，持续探索大模型赛道的多元应用。本文将详细介绍腾讯音乐如何基于 <a href=\"http://doris.apache.org/\">Apache Doris</a>\" 构建查询高效、实时统一分析的 OLAP 引擎，使 OLAP 作为底层基建加强模型连接转化效率、结果输出准确率，最终将大模型 + OLAP 引擎结合为用户提供个性化、实时化、灵活化的智能数据服务平台。</blockquote><p></p><p></p><p>作者｜腾讯音乐大数据架构张俊、罗雷</p><p></p><p>腾讯音乐娱乐集团（以下简称“腾讯音乐”）是中国在线音乐娱乐服务开拓者，有着广泛的用户基础，总月活用户数超过 8 亿，通过“一站式”的音乐娱乐平台，用户可以在多场景间无缝切换并享受多元的音乐服务。我们希望通过技术和数据赋能，为用户带来更好的体验，为音乐人和合作伙伴在音乐制作、发行、销售等方面提供支持。</p><p></p><p>基于公司丰富的音乐内容资产，需要将歌曲库、艺人资讯、专辑信息、厂牌信息等大量数据进行统一存储形成音乐内容数据仓库，并通过产品工具为业务人员提供数据分析服务。在内容数仓搭建的过程中，我们的工作始终围绕降本增效为主要目的进行优化与迭代，希望在数据服务方面不断提升产品工具的开发与分析效率，同时在数仓架构方面能够有效减少架构成本与资源开销。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be6d51243889d07c791efa856d5693f4.jpeg\" /></p><p></p><p>在传统数据服务中，我们为业务分析师提供了多种数据服务，包括 SQL 查询、固定看板、定制化的分析工具以及人工跑数。然而，在实际应用过程中仍然存在一定痛点：</p><p></p><p>SQL 查询平台 ： 业务分析师根据需求进行 SQL 语句编写，对平台数据进行查询分析，每位业务人员都需要掌握 SQL，导致学习成本高、上手难度大。固定看板（Dashboard） ： 技术人员基于常规业务开发制作数据看板，虽然能够简化业务分析师查询的过程，但是看板制作成本高且灵活度低，当面对复杂的用户问题时，看板无法及时调整以满足需求变更。定制分析工具： 基于特定的业务需求，技术人员需要定制化开发产品分析工具，整体开发成本过高，且单一的开发工具不具备通用性，随着工具数量增加，操作介面变得散乱，从而降低业务效率。人工跑数： 当以上三个场景都无法满足业务需求时，业务分析师需要向技术人员提需求进行人工跑数，沟通成本过高、整体解决效率低下。</p><p></p><p>随着行业发展趋势，LLMs 大语言模型（LLMs - Large Language Models，以下统一简称为大模型）出现有效地解决了这些问题。当平台融入大模型后，平台用户输入的问题会进入大模型进行语义解析，自动转化为 SQL 语句触发 OLAP 引擎开启数据分析与查询。通过平台智能问答交互的方式，业务分析师不再需要依靠人工编写 SQL 提供查询分析结果，技术人员也不需要再制作过于固定或者过于定制化的产品工具。大模型 + OLAP 引擎结合的全新数据服务模式，不仅为平台用户提供了个性化、灵活表达、秒级回复的服务体验，还大幅降低了企业内部技术与业务学习成本，加速数据分析效率，实现多端入口统一、界面统一的平台构建。</p><p></p><p>本文将详细介绍腾讯音乐如何基于 Apache Doris 构建查询高效、实时写入且统一的 OLAP 分析引擎，使 OLAP 作为底层基建加强大模型与之连接转化的效率、结果输出的准确率，最终提供更智能化的问答交互服务，也希望通过这篇文章为有相关业务需求的公司提供不同视角和思路。</p><p></p><h2>大模型 + OLAP ：开启数据服务平台新模式</h2><p></p><p></p><p>在大模型 + OLAP 架构方案中，目前经典方案如下图所示，大模型充当中间层将用户输入的自然语言转化为 SQL 执行语句，OLAP 作为底层存储和数据处理的引擎，负责接受和执行从大模型发送过来的 SQL 语句，对数据进行预聚合、多维分析等操作，满足大规模数据集的查询分析需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/058c7be9d4bf67b4ee4af097566b4b16.png\" /></p><p></p><p>然而，这种架构在实际落地过程中也面临一定挑战，例如语义理解的准确性、查询效率的优化、私域知识的理解等方面，具体如下：</p><p></p><p>复杂数据口径不统一： 大模型对于技术方面的词汇，如字段、行列、表等无法理解，相反对于业务方面的词汇，如公司收入情况、日活跃用户数量等能够提供有效翻译与转换。因此挑战之一是需要思考如何引导用户进入指标范围内提问，挑战之二是当用户存在对多种指标、多类指标查询时，需要考虑如何保持指标维度口径的统一、如何有效生成对应的指标计算公式。模型处理效率较低： 现阶段大模型虽然支持交互能力，但推理速度较慢，需要花费十秒级以上响应，用户每增加一个问题输入，就需要花费更多等待时间，使服务质量降低。同时大模型整体按照 Token 收费，使用量增加时也会导致平台成本升高。私域知识无法识别： 虽然大模型已经开展许多公开数据集的语言转换训练，但面对企业内部的大量专业术语仍无法很好地理解转化。以音乐内容数据库为例，大模型时常缺少对于某些冷门歌曲的认知，在问答过程中无法正确给出交互反馈，因此我们需要增强大模型对于私域知识的理解。定制场景无法满足： 大模型主要依据自身数据集进行回答，会出现“知识幻觉”（输出缺乏依据的内容）问题，我们需要允许第三方插件的接入使大模型得以联网，让用户借助内部插件完成更定制化、更多样的任务。因此如何接入、匹配并触发组件功能是我们的重点优化目标。</p><p></p><p>面对经典方案中的落地难点，我们的总体解决思路是将以上四大挑战逐一拆解，通过组件叠加分阶段完善大模型 + OLAP 架构构建，最终实现全新的交互问答服务模式，接下来我们将介绍各阶段挑战对应的解决方案。</p><p></p><h3>增加语义层：处理复杂数据问题</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc1cc7076ec03de757325be3df7d235e.png\" /></p><p></p><p>为了解决复杂数据处理问题，我们在大模型与 OLAP 中间增加 Semantic Layer（以下简称语义层）。</p><p></p><p>一方面语义层作为连接技术与业务之间的转换桥梁，能够将数据字段翻译为业务用户的术语，使业务知识作为额外的抽象层。通过语义层，业务分析师不需要在定义指标后存储于 OLAP 数仓中，能够直接在语义层中指定过滤条件，将所需指标筛选后生成 SQL 语句并在 OLAP 中进行字段查询。这意味着，业务分析师能够把多源数据按照需求定义成语义信息并形成语义标准，有效解决了多种指标、多类维度计算口径不统一的挑战。</p><p></p><p>另一方面语义层能够针对业务计算逻辑，进行语义加工、描述、关联和运算。语义层在过滤数据后，能够屏蔽由表关联所产生的复杂指标计算公式，将多表 Join 场景进行拆解、转化，形成较为简单的单表查询，以提升语义转化的准确性。</p><p></p><h3>设定人工经验：处理模型效率问题</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc685dbf1f204f0a4cbb196ef7c9b01a.png\" /></p><p></p><p>针对模型效率问题，我们的解决思路是对指标计算、明细查询、人群圈选等查询场景进行复杂度判定，将简单查询场景直接跳过大模型解析的步骤，进入底层 OLAP 进行处理分析，使大模型更加专注处理复杂查询场景。</p><p></p><p>为此，如上图所示我们在模型中添加人工经验判断。当业务分析师输入 “查询各大音乐平台收入”问题时，模型依据判定规则发现该场景只需要提供某个指标或几个维度即可完成，这时不需要将问题进入大模型解析，直接使用 OLAP 进行查询分析，能够有效缩短响应时间，提升结果反馈效率。此外，跳过大模型解析的步骤也能够节省 API 调用经费，解决平台使用成本升高的问题。</p><p></p><h3>增加内容映射：处理私域知识问题</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4f6148b63a2e12a5d4ba82951251f012.png\" /></p><p></p><p>针对私域知识的问题，我们在大模型上游增加 Schema Mapper 、在外部建立业务知识库，将平台用户的问题与知识库进行连接，通过 Schema Mapper 判定是否存在部份文字能够与知识库内容匹配。如果匹配成功，大模型将进一步解析转化、OLAP 分析处理。Schema Mapper 与业务知识库的引入，有效解决了大模型对私域知识理解不足的问题，提升语言处理的效果。</p><p></p><p>目前，我们正在不断对 Schema Mapper 匹配准确性进行测试与优化，将知识库中的内容进行分类处理、字段评级等操作，同时将输入文本进行不同范围的内容映射（如全文本映射与模糊映射），通过映射结果来加强模型语义解析的能力。</p><p></p><h3>插件接入：处理定制场景问题</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/db37efda5c82f1f802c1ec1afeef655a.png\" /></p><p></p><p>定制化场景主要指代业务范围之外的查询需求，需要将音乐内容数据与法律、政治、金融、监管等方面信息结合提供问答服务。通过增加插件，使平台用户能够访问实时更新且无法包含在训练数据或业务知识库中的信息，以实现定制化交互。</p><p></p><p>由于插件类型不同，模型接入方式也会有所不同，常见的接入方式主要分为两种：</p><p></p><p>Embedding 本地文本接入： 该方式首先对本地文档进行向量化处理，通过语义向量搜索，找到本地文档中相关或者相似的词语进行匹配，之后将文档内容注入大模型解析窗口中生成答案。这种方式非常适合业务分析师希望将音乐内容数据库与最新政策等一类较为私有的文件结合完成查询需求。ChatGPT 第三方插件接入： 每款插件具备对应的 Prompt 与调用函数。业务人员在安装某款插件之后，在与模型对话中可以通过 Prompt 词触发函数开启调用。目前第三方插件类型丰富，涉及行业广泛，能够有效增加多元场景的处理与响应能力。</p><p></p><h2>超音数平台框架构思</h2><p></p><p></p><p>根据上述大模型 + OLAP 的四大解决方案进行了方案整合，以此进行框架设计并将其命名为超音数平台。大模型主要作用于自然语言与 SQL 分析语句的连接与转化，OLAP 引擎则作为数据存储与查询分析的核心基建。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97aefdf681aef73a34cb28e0b1f3002b.png\" /></p><p></p><p>超音数平台对于业务流程如图所示，模型运转具体过程如下：</p><p></p><p>用户输入问题通过 Schema Mapper 检索，判定字段是否匹配与业务知识库。如若匹配则跳过大模型解析步骤，直接利用知识库中的指标计算公式触发 OLAP 进行查询分析；如若不匹配则进入大模型，开启下一步判定。大模型首先通过人工经验判定问题复杂度，简单查询将指定 OLAP 引擎直接分析，复杂查询则开启语义解析形成 DSL 语句。DSL 语句通过语义层进一步过滤、拆解关联查询场景，生成简易单表 SQL 语句以触发 OLAP 数据处理与查询加速。针对需要与外部信息结合的查询场景，大模型会判断是否调用第三方插件来辅助完成查询。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/211c2536582f1990432fc3c0252448eb.png\" /></p><p></p><p>以“某首歌曲能否在综艺节目播出”为例，在经过检索匹配、语义解析后，大模型选择利用 OLAP 数据查询与第三方版权行业插件结合的方式进行回答，最终呈现结果由数仓中的歌曲信息与插件判定结果构成。</p><p></p><p>如今，业务分析师只需要在超音数平台中定义指标含义、维度类型即可直接开展自然语言的问答交互服务。同时还可以在平台中内置插件、丰富指标市场来拓展语义解析能力，完全覆盖了业务在常规与定制化场景下的查询需求。平台基于大模型 + OLAP 的模式加速业务分析效率，减少技术开发成本，向智能化、个性化、实时化的全新业务服务模式更近一步。</p><p></p><p>在这里希望可以与大家分享该开源项目，让更多人体验和学习大模型构建，也欢迎感兴趣的读者们共同参与大模型开发与建设。</p><p></p><p></p><blockquote>超音数开源框架：https://github.com/tencentmusic/supersonic</blockquote><p></p><p></p><h2>超音数平台框架演进</h2><p></p><p></p><p>在平台构建的过程中，OLAP 引擎作为整体架构的基建对 SQL 语句处理、数据存储分析、上游应用层的查询响应等有着至关重要的作用，我们希望通过架构升级以加强大模型到 OLAP 引擎的转化效率与结果输出准确性。</p><p></p><p>接下来我们将对比介绍 OLAP 早期架构与新一代架构在数据写入与查询两方面的差异，分享在架构演进过程中大模型 + OLAP 模型优化历程，最终助力超音数平台的构建，开启新一代的数据服务模式。</p><p></p><h3>数据架构 1.0</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/07/079ab31e38fea758f9f92fe2a4d23f89.jpeg\" /></p><p></p><p>我们初期的业务架构如上图所示，分为处理层、分析层、应用层三部份，用户文本在进入大模型之后解析为 SQL 语句使 OLAP 开始执行任务，具体的工作原理如下：</p><p></p><p>处理层：在 ODS- DWD- DWS 三层中将数据整合为不同主题的标签和指标体系之后，通过对 DWS 调度与采集所需字段，在 DWM 层将维度与指标数据加工成大宽表。分析层：通过大宽表进入分析层，将数据导入 Clickhouse 与 Elasticsearch，其中 Clickhosue 主要负责维度与指标两类数据的查询加速，作为分析引擎为后续提供报表开发服务；Elasticsearch 主要负责维度数据处理，作为搜索/圈选引擎。应用层：业务人员基于场景选取所需要的标签与指标，在应用层中创建数据集作为逻辑视图，同时可以二次定义衍生的标签与指标。</p><p></p><p>在实际业务使用中，早期架构的数据处理方式存在大宽表带来的数据延迟与存储浪费、多套组件导致架构冗余带来指标维度重复定义、学习与运维成本高等问题，具体如下：</p><p></p><p>数据延迟： 处理层不支持部分列表更新，DWS 层数据写入产生延迟后会造成大宽表的延迟，进而导致数据时效性下降。运维成本高： 在处理层大宽表中维度数据量平均占一张大宽表的 50%，且在大部份情况下变化缓慢，这意味着每一张宽表的开发会将维度数据叠加，造成存储资源的浪费、维护成本增加；在分析层中存在多引擎使用的问题，查询 SQL 语句需要同时适配 Clickhouse 与 Elasticsearch 两个组件，增加人力成本，且两套组件也会加大运维难度，运维成本进一步升高。架构冗余： 在应用层进行指标与维度定义时，导致相同数据会进行多次定义使各种指标、维度定义口径不一致，造成权限不可控，例如上图所示的 T1 （标签）与 M1 （维度）在应用层中，被不同数据集多次定义。</p><p></p><h3>数据架构 2.0</h3><p></p><p>基于以上问题，我们开始对架构进行改造升级，并在众多 OLAP 引擎中选择了 Apache Doris 来替换原有组件，主要因为 Apache Doris 具备以下核心优势：</p><p></p><p>实时导入：  Apache Doris 能够支持海量业务数据的高吞吐实时写入，时效性可以做到秒级完成导入。引擎统一： 支持 Multi-Catalog 功能，能够通过 Elasticsearch Catalog 外表查询，实现查询出口统一，查询层架构实现链路极简，维护成本也大幅降低。查询分析性能： Apache Doris 是 MPP 架构，支持大表分布式 Join，其倒排索引、物化视图、行列混存等功能使查询分析性能更加高效极速。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7b/7b9e4ec29a21c25ffe2161d0e3a616fb.jpeg\" /></p><p></p><p>在数据架构 2.0 版本中，数据架构保留处理层部份，主要升级分析层架构，并进行了语义层叠加：</p><p></p><p>分析层：引入 Apache Doris 替换 Clickhouse 组件，利用 Doris 的 Elasticsearch Catalog 功能对 Elasticsearch 外表进行查询，实现查询出口统一；语义层：应用层不再需要创建数据集视图，直接通过语义层获取指标与标签内容执行查询任务，有效解决标签与指标口径问题。</p><p></p><h3>数据架构 3.0</h3><p></p><p>由于宽表开发过程中，维度数据一般变化较小、字符存储空间较大，且分析查询一般只需要查询最新的维度数据。在这种情况下，如果不断叠加维度数据制作宽表，会造成存储空间浪费的问题，同时查询响应速度也受到影响。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c2cebe08abe150c665cc8e67dddf552c.jpeg\" /></p><p></p><p>为了进一步提升架构性能，数据架构 3.0 主要将处理层中大宽表进行拆分，同时将分析层统一使用 Apache Doris 作为查询分析引擎：</p><p></p><p>处理层：按照业务分类在 DWM 中将大宽表拆分成缓慢维度表与指标表，使两类表在本地 Hive 中进行关联，通过 Hive 导入 Apache Doris 分析层中加速任务；分析层：将关联数据表直接导入 Apache Doris 中，结合语义层暴露指标与维度以实现语义统一，用户只需要通过过滤条件就能够直接查询数据，得到所需要的结果。</p><p></p><h3>数据架构 4.0</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/7b/7b1c7017f2b38d5e33b15b4dab8b0740.jpeg\" /></p><p></p><p>我们延续了 3.0 架构中分析层统一的优势，对处理层、分析层、语义层架构进一步优化，使查询性能显著提升：</p><p></p><p>分析层 + 处理层：数仓 DWD 层数据采用  Rollup 功能使事实表与维度表实时关联并创建多个视图进入 DWS 中。通过这种方式，分析层与处理层中的各类指标数据无需再重复定义，能够基于 Apache Doris 全部写入新建的 Rollup 视图中并利用GROUP BY将维度传入视图进行查询加速，直接对外暴露所需数据。语义层：利用 Apache Doris 物化视图对指标与维度自定义口径，通过语义物化层进行查询加速，并将指标与维度通过 SUM 加工开发衍生标签与维度数据。应用层：利用 Apache Doris 2.0 版本的倒排索引功能，对现有的索引结构进行丰富，满足了对知识库进行模糊查询、等值查询和范围查询等场景中的能力，进一步加速指标、维度查询响应速度。</p><p></p><p>数仓架构基于 Apache Doris 迭代升级，最终实现导入实时、引擎统一、查询高效的现代化湖仓 OLAP 引擎，简化架构链路的同时，有效解决大宽表中指标重复定义所带来的问题。在架构演进的过程，我们也积累许多关于 Apache Doris 性能优化经验，希望通过分享给读者们带来一些参考。</p><p></p><h2>Apach Doris 性能优化实践</h2><p></p><p></p><h3>Colocate Join 宽表优化</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bd3fab4281d7addc8c0d9e52a3e80f39.jpeg\" /></p><p></p><p>在上文架构改造中我们提及，由于宽表开发会不断叠加字符数据，消耗存储空间，降低查询性能，因此我们充分利用了 Colocate Join 功能对宽表拆分、本地关联查询加速进行优化，具体过程如下：</p><p></p><p>指标大宽表：采用 Apache Doris 的 Aggregate Key 模型，使用增量的方式将数据覆盖写入；缓慢维度表：主要通过 start_date 和end_date的设置进行表建设，同时利用 end_date 进行分区，当我们需要查询最新的维度数据时只需要将 end_date 设置为 ‘9999-12-31’ 即可。此外我们引用 Doris 2.0 版本中的写时合并，利用 Unique Key 模型进行维度数据聚合，使查询性能在该场景中得到很大的提升。对外访问视图：在指标与维度表建设完成之后，利用 CREAT VIEW 提供统一对外访问视图，同时添加 end_date 条件，使视图保持最新数据的展示。通过这样的方式不仅能够大幅度降低查询的复杂性，还能够充分利用 Doris 特性实现查询加速。</p><p></p><h3>Rollup 解决指标膨胀问题</h3><p></p><p>宽表拆分为指标表与维度表后，我们发现每一次视图产生都需要定义多个指标，出现指标膨胀的情况。以“歌曲播放量结算”为例，当仅定义单一指标时，我们需要将各个平台 + 各类内容进行排列组合，使语义层定义很多指标数据，造成指标数量过多。此外这些指标都需要通过离线生产任务进行加工，并通过 Hive 导入至 Apache Doris 中，造成链路较长、加工维护比较困难。</p><p></p><p></p><blockquote>平台指标：覆盖四大音乐平台，包括酷我、QQ 音乐、酷狗、K 歌内容指标：包含歌曲、歌手、专辑以及厂牌等数据</blockquote><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ea/eac947d54b0095df842f936a67792d32.jpeg\" /></p><p></p><p>为了有效解决指标膨胀问题，我们引入了 Doris Rollup 功能。如图所示，在 Doris Base 表数据基础之上，可以根据指定维度来创建任意多个 Rollup 视图并自动进行GROUP BY，实现各个平台与各类内容指标定义不重复、查询性能提升的目标。</p><p></p><h3>物化视图实现查询加速</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/06728fde804a3b45f38ec42c4894a4af.jpeg\" /></p><p></p><p>除了减少指标数量外，我们还希望能够衍生指标并且做到查询加速。在 Apache Doris 2.0 版本中我们采用了物化视图功能进行衍生指标的开发。目前，我们主要在单一维度表中单独地去查询自定义标签与维度，在定义复杂口径后自动的通过语义层物化任务。</p><p></p><p>如上图所示我们将指标 M1 、M2、M3 与维度 T1、T2、T3 分别进行定义，并通过  SUM  加工衍生标签，在加工完成之后创建物化视图加速查询。此外，在 Doris 后续 2.1 版本中还会支持多表创建物化视图，我们也非常期待使用该功能。</p><p></p><h2>Apach Doris 导入性能调优实践</h2><p></p><p></p><p>目前，腾讯音乐具有 90+ 数据来源表、 3000 + 维度和指标、导入数据量达到千亿级别，我们希望数仓能够支持大规模数据快速导入，且导入过程中保证数据写入的准确性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/12bfd92cd2a10f60da70dce25cd7ea63.jpeg\" /></p><p></p><p>导入链路如图所示，主要分为离线与实时两个部分，离线链路中指标表与变更维度表通过 Spark 进行批量导入，两类表利用 Flink 聚合形成宽表后写入；实时链路主要利用 Kafak 消息队列进行流式写入。最终，离线与实时两条链路利用 Flink  实时写入 Apache Doris 数仓中。</p><p></p><p>由于 Flink 聚合为攒批写入，如果出现写入任务失败，会导致数据丢失；同时，在聚合任务过多、字段过多的情况下存在 Compaction 不及时的情况，导致实时能力不可控；此外在加工宽表的过程中，也会造成重复写入的问题，无法保证数据写入准确性。</p><p></p><p>在 Apache Doris 2.0 版本发布后，我们引入了其全新功能 Flink Doris Connector 与 Doris Compaction，有效解决了 Flink 聚合引起的问题。</p><p></p><h3>Flink Doris Connector 实现快写入</h3><p></p><p>Flink Doris Connector 主要是依赖 Checkpoint 机制进行流式写入，同时该功能默认开启两阶段提交，保证写入过程中 Exactly Once 语义。值得注意的是，我们在引入最新版的 Flink Doris Connector 功能后，实现了从关系型数据库到 Apache Doris 的一键整库同步，承载了我们实际业务中千亿级别的实时并行写入，满足数据快写入与不丢不重的需求。</p><p></p><h3>Doris Compaction 保证写入稳定性</h3><p></p><p>为了解决 Flink 聚合引起的偶发性 Compaction 不及时问题，我们引入最新版的 Vertical Compaction 与 Segment Compaction 功能。</p><p></p><p>Vertical Compaction 功能优势： 在单次合并过程中，我们不需要再将所有的列读出，只需要加载部份列数据即可，这能极大减少合并过程中的内存占用问题，提高压缩的执行速度，实现在大宽表场景下的部份数据合并。Segment Compaction 功能优势： 在单批次大数据量的导入场景下可以有效减少 Flink 写入过程中产生的 Segment 数量，且能够使合并和导入两个过程并行，避免增加导入时间。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/466cfe3127b16b3f2fb54a3cba5d46b7.jpeg\" /></p><p></p><p>如上图所示在引入 Doris Compation 功能后，在写入量增加 50 % 的情况下，Compaction Score 从平均 650 分降低至 80 分，技术人员不再需要担心夜间出现告警的情况，保证了整体链路的稳定性。</p><p></p><h2>总结收益与展望</h2><p></p><p></p><p>在引入 Apache Doris 后，数据架构围绕降本增效的目标，不仅在写查方面的性能得到大幅度提升，并且有效减少架构成本与资源开销，具体的收益如下：</p><p></p><p>极速查询分析： 通过 Apache Doris 的 Rollup、物化视图、倒排索引功能，由原来的分钟级查询时间达到现如今秒级毫秒级；导入性能提升： 导入优化完成后，原本 3000+ 维度、指标数据的导入时间需要超过一天，现如今能够在 8 小时内完成导入，导入时间缩短至原来的 1/3，实现快速导入需求；更重要的是，Apache Doris 在保证数据快写入的同时，使数据能够不丢不重、准确写入；链路极简与统一： Apache Doris 将查询与分析出口引擎统一，去除 Elasticsearch 集群使架构链路极简；存储成本降低： 通过大宽表拆分的方式，使存储成本降低 30%，开发成本降低 40% 。</p><p></p><p>在未来，我们将进一步拓展使用 Apache Doris 湖仓一体功能，对 Hive、MySQL、数据湖等多源异构数据库进行网关统一，实现真正意义上的实时统一分析引擎。同时，尝试 CCR 跨集群数据同步功能，通过用户多集群的数据库表自动同步以提升在线服务数据的可用性。未来，我们也将在测试环节中验证读写负载分离以及多机房备份的性能效果。</p><p></p><p>目前，Apache Doris 社区已经公布了后续版本中将推出的存算分离全新架构，能够利用低成本的共享存储系统简化上层计算节点的复杂度，使架构带来巨大的成本经济优势。我们也希望能够进一步探索，基于 Apache Doris 本地高速缓存 + 共享存储系统的混合模式，在保障性能的同时降低系统存储开销。</p>",
    "publish_time": "2023-09-08 11:10:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "HBlock实战 深挖独创存储技术亮点",
    "url": "https://www.infoq.cn/article/491wZbTFvRAuoxvo4N0q",
    "summary": "<p>企业对于“数据驱动”这个词都不陌生，也都深知数据对于业务发展的重要影响。然而，如何保障传输数据的安全? 如何避免意外场景下的数据丢失? 如何合理利用老旧服务器? 如何实现高效运维，有效节约人力和时间成本…</p>\n<p>是否存在一款存储产品能同时满足，使用者对成本、性能和安全的考量呢？天翼云重磅推出的存储资源盘活系统 HBlock，或许可以满足你的所有期待！</p>\n<p>InfoQ 联合天翼云策划了主题为《存储难题新解法，揭秘极致易用的 HBlock》的线上技术分享会，本期为第二期直播回放——《 HBlock实战，深挖独创存储技术亮点》。</p>",
    "publish_time": "2023-09-08 11:19:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AuxBridge CEO Michael Girel 确认出席 FCon ，分享新时代下以色列在金融转型创新上的探索",
    "url": "https://www.infoq.cn/article/bLwTqslvCuRdUirst9rK",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。AuxBridge&nbsp;CEO&nbsp;Michael&nbsp;Girel&nbsp;将发表题为《&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5496?utm_source=infoqweb&amp;utm_medium=article\">Israeli&nbsp;Innovation:&nbsp;Transforming&nbsp;Financial&nbsp;Society&nbsp;in&nbsp;the&nbsp;Modern&nbsp;Age</a>\"》主题分享，介绍以色列现在的真实经验以及过去 5 年在数字金融领域所做的改变，以及当今以色列金融现实的各个方面，例如数字银行、金融数据安全和数字投资（风险基金）。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5496?utm_source=infoqweb&amp;utm_medium=article\">Michael Girel（迈克尔·吉雷尔）</a>\"是以色列的主要企业家，AuxBridge 的创始人兼首席执行官。 Michael 在美国、以色列和欧洲高科技管理和多项目方面拥有 20 多年的成功经验，并在世界各地实施复杂项目，其主要合作伙伴是美国国务院，以色列商业发展部，世界货币基金组织，世界发展银行和等等。他是以色列商业发展部的顾问，以色列企业家协会的成员。他在本次会议的演讲内容如下：</p><p></p><p>演讲：Israeli Innovation: Transforming Financial Society in the Modern Age</p><p></p><p>呈现以色列现在的真实经验以及过去 5 年在数字金融领域所做的改变。作为一个在包括金融领域在内的所有生活领域创新成分排名最高的国家，以色列凭借在技术和组织领域的一系列创新，使其金融领域变得安全、可靠和高度技术化。</p><p></p><p>还将介绍当今以色列金融现实的各个方面，例如数字银行、金融数据安全和数字投资（风险基金）。会从金融部门竞争的角度介绍现代以色列金融部门与初创企业之间通过风险投资基金的合作模式，以及投资者在这种情况下可以获得什么好处。</p><p></p><p>由于初创企业是以色列最受欢迎的技术创新工作形式，因此初创企业投资和融资也是人们感兴趣的领域。</p><p></p><p>演讲提纲：</p><p></p><p>以色列是一个具有创业思维的国家金融初创企业及其在以色列金融领域的作用是什么让以色列数字金融领域既独特且安全数字金融与现实世界合作的形式和模式数字金融在以色列是如何运作的中以创业金融及数字金融合作趋势与展望</p><p></p><p>你将获得：</p><p></p><p>○ 听众将获得有关以色列金融和数字金融行业现实的最新集中信息</p><p>○ 中国和以色列是地方和州层面许多项目的合作伙伴，帮助大家了解合作的真实信息</p><p>○ 许多以色列金融初创企业已经走向全世界，有助于建立本地企业与国外企业间的合作</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：13269078023（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-09-08 12:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "革命性软件定义存储 HBlock 极致易用背后的技术解析（第二期）",
    "url": "https://www.infoq.cn/article/491wZbTFvRAuoxvo4N0q",
    "summary": "<p>企业对于“数据驱动”这个词都不陌生，也都深知数据对于业务发展的重要影响。然而，如何保障传输数据的安全? 如何避免意外场景下的数据丢失? 如何合理利用老旧服务器? 如何实现高效运维，有效节约人力和时间成本…</p>\n<p>是否存在一款存储产品能同时满足，使用者对成本、性能和安全的考量呢？天翼云重磅推出的存储资源盘活系统 HBlock，或许可以满足你的所有期待！</p>\n<p>InfoQ 联合天翼云策划了主题为《存储难题新解法，揭秘极致易用的 HBlock》的线上技术分享会，本期为第二期直播回放——《 HBlock实战，深挖独创存储技术亮点》。</p>",
    "publish_time": "2023-09-08 11:19:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "亚马逊云科技re:Inforce 2023中国站：企业如何提高数据、模型和应用安全？",
    "url": "https://www.infoq.cn/article/2SjM2FZqw7TB6SdxDaW7",
    "summary": "<p>过去十几年，云计算的发展让大规模可用的计算资源成为可能，也推动了人工智能技术不断地创新，直到现在。在过去不到一年的时间里，生成式AI应用场景井喷，这预示着一个新的转折点，一个新的技术时代。</p><p>&nbsp;</p><p>目前，生成式AI已经应用到企业创新的各个环节，从利用智能客服优化客户体验，到自动生成代码来提高技术团队生产力，再到以文字生成图片的方式加速创意内容生成，生成式AI的出现让AI技术能够真正地落地于企业实际业务中。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6d/71/6d0e08681807ffd57e00dc8bd8cbc671.jpg\" /></p><p></p><p>亚马逊云科技大中华区解决方案架构部总监 代闻</p><p>&nbsp;</p><p>生成式AI应用场景的井喷推动业务需求迅猛增长的同时，也给企业的技术部门带来了前所未有的挑战。大语言模型的广泛应用带来最大的安全威胁之一就是数据泄漏。</p><p>&nbsp;</p><p>前段时间，某大型国际企业在采用了一个基于大语言模型的公开聊天应用服务以后，20天内发生了三起数据泄露事件，有产品良率的泄露、有代码的泄露，还有会议内容的泄露。</p><p>&nbsp;</p><p></p><h2>大模型时代，如何提高数据、模型和应用安全？</h2><p></p><p>&nbsp;</p><p>面对这么多安全挑战，企业到底该如何防范？</p><p>&nbsp;</p><p>从定位上来讲，安全是构建生成式AI应用不可回避的一个议题。从构建开始，用户就需要把安全作为企业AI战略发展的一个核心环节，尤其是现在飞速发展的阶段。</p><p>&nbsp;</p><p>但是构建安全的AI应该从哪里开始？用到的框架是怎样的？首先，就要从数据谈起。因为数据和模型安全是构建AI应用的关键。</p><p>&nbsp;</p><p>生成式AI的井喷对企业的数据平台有了更高的要求。要训练构建一个生成式AI的模型，需要大量的非结构化数据。如果一个企业直接去用一个成本大模型进行微调，这时候又需要有高质量的专业化的数据来微调这个模型。这两个方面的要求目前对于大多数企业的数据平台都是具有挑战性的。亚马逊云科技在大数据还有AI领域里都耕耘多年，已经沉淀出来一个可以贯穿生成式AI全周期的数据治理流程。</p><p></p><h3>数据安全</h3><p></p><p>&nbsp;</p><p>在数据安全方面，亚马逊云科技提供了贯穿生成式AI全周期的数据治理，从数据源的获取到数据的存储和查询，再到将数据传输给AI平台进行模型的训练、调优和推理，以及全面实施数据分类和治理。亚马逊云科技提供了一整套的解决方案、产品服务和最佳实践，帮助企业加速落地端到端的云原生数据战略，给生成式AI提供高质量的数据支持。</p><p>&nbsp;</p><p>保护存储中的数据。亚马逊云科技通过实施安全密钥管理、静态数据加密、强制实施访问控制、利用机制限制数据访问，保护存储中的数据。高质量数据是构建生成式AI能力的关键。针对高质量的数据，亚马逊云科技有两个保护重点：​防止数据泄漏​以及防止数据篡改​。这就要求我们全面保障​数据存储、​从数据湖到模型训练的数据传输、​以及模型应用的环境。​本次大会上，推出了敏感数据保护解决方案，可实现对企业敏感数据的自动化发现并在统一平台管理数据资产。该解决方案允许客户创建数据目录、使用内置或定制数据识别规则定义敏感数据类型，该方案利用机器学习、模式匹配的方式自动识别敏感数据，并提供可视化面板，帮助客户更轻松地对敏感数据进行管理和保护。保护传输中的数据。亚马逊云科技从实施安全密钥和证书管理、执行传输中加密、自动检测意外数据访问、对网络通信进行身份验证四个方面对传输中的数据进行保护。多层次保护传输中的数据。亚马逊云科技通过跨区域之间的数据传输、VPC内部以及VPC之间的传输、迁移上云的过程中、以及TLS1.2+AES256从整个的基础架构上实现应用层的加密和传输的保护。保护使用中的数据。从身份认证、隔离环境、多方协作以及数据共享四个方面，进行使用中的数据保护。</p><p></p><h3>模型安全</h3><p></p><p>第二个大的方面就是模型的安全。模型训练后进入生产环境的安全防护同样重要。针对大模型，几个月前，亚马逊云科技宣布推出Amazon Bedrock和多种生成式AI服务和功能，以帮助客户构建和扩展自己的生成式AI应用程序。​Amazon Bedrock提供了广泛的基础模型因此客户可以选择最能满足需求的模型。​</p><p>&nbsp;</p><p>首先，Amazon Bedrock后面接入了基础模型，它给提供了一个API可以使用大模型加速生成式AI的应用程序和开发，无须管理底层的基础设施。第二，Amazon Bedrock会负责任地选取一些合作伙伴，例如AI21 Labs、Anthropic、Stability AI，以及自己的基础模型，另外亚马逊云科技最新推出的合作伙伴名单里也增加了Cohere，便于客户最快速地找到最合适的、能力最强的基础模型。第三，使用组织内部的数据来训练大模型，同时又保证：首先，背后给大模型做了私有拷贝，这个拷贝只是给客户服务，不会再跟其他任何的大模型共享。二是训练数据只是在客户账户里来帮助工作，Amazon Bedrock不会拿任何用户的数据来增进自己的模型。这两点非常关键，这也是很多企业在采用大模型的时候对于数据主权、数据保护方面有担心的一个很重要的点，Amazon Bedrock给了一个非常完善的答案。</p><p>&nbsp;</p><p>值得一提的是，它能全面地使用亚马逊云科技提供的安全功能，Amazon KMS、Amazon IAM等可以完善地跟Amazon Bedrock集成，集成以后可以很好地管理加密、权限控制和所有行为的日志。</p><p>&nbsp;</p><p>亚马逊云科技也在帮助客户构建负责任的AI，也会提供Amazon Titan自己的基础模型来给大家使用。Amazon Titan有两个基础模型，一个是Titan Text，能够执行文本类的任务。另外一个叫Titan Embeddings，能执行个性化推荐的任务。亚马逊云科技在负责任AI方面有着坚定的承诺，Amazon Titan可以通过减少和消除不当或者是有害的内容来支持技术的实现。</p><p>&nbsp;</p><p>IDC中国研究总监王军民认为，“IDC通过调研，看到企业组织在整个的数字化转型过程中，对于网络安全的担忧是最大的。安全是全球最高管理层数字化举措面临的最大障碍，因此安全也是最高管理层技术投资的关键任务，尤其是云计算市场在全球的快速发展极大推动了客户对IT安全软件及服务的需求。亚马逊云科技用高标准的安全理念，不断提升安全合规能力及标准。”</p><p></p><h3>应用安全</h3><p></p><p>对于每一个负责任的人工智能企业来说，安全应该贯穿到从开发到持续集成、持续部署再到投产、监控以及整个反馈的过程里面来。</p><p>&nbsp;</p><p>拿亚马逊云科技的Amazon CodeWisperer来说，它能够生成安全的代码，亚马逊云科技把内部数据集训练出来的模型能力放到经过安全检测的应用场景中，通过三层能力提供给客户，全面支持不同层的客户用生成式AI。</p><p>&nbsp;</p><p>在开发过程中，Amazon CodeWhisperer作为AI编程助手，根据开发者指令利用内嵌的基础模型实时生成代码建议，该服务内置了代码安全扫描功能，可帮助开发者查找难以检测的漏洞并提出补救建议；Amazon CodeGuru Security可以扫描代码，在代码里面寻找漏洞，包括调用包漏洞，包括很多其他代码逻辑的漏洞。</p><p>&nbsp;</p><p>在应用运行中，企业可构建零信任的应用安全访问策略。这是一整套机制，需要对访问大模型的应用进行权限管理，确保只有在拥有特定权限的应用，才能访问或者调用大模型里的制定API。</p><p>&nbsp;</p><p>亚马逊云科技推出系列工具，帮助客户构建零信任机制：</p><p>&nbsp;</p><p>使用Amazon Verified Access搭建无需VPN的网络验证系统、使用Amazon IAM或者客户自己的用户认证系统，来完成这个认证程序，建立可信任的网络通道；Amazon Verified Permissions为用户构建的应用程序提供细粒度授权和权限管理，用户可以使用该服务管理其应用程序的角色和属性的访问控制；发布开源语言CEDAR，CEDAR用于编写和执行授权策略的开源的语言，可以更加高效地创建所有的访问控制权限。</p><p>&nbsp;</p><p>亚马逊云科技大中华区解决方案架构部总监代闻强调：“零信任和网络控制并不是一个二选一的关系，两者相加才能实现端到端的应用安全，尤其是在大模型时代”。</p><p>&nbsp;</p><p>针对网络防护，Amazon Shield用于防DDos攻击；Amazon WAF提供防火墙支撑；Amazon Firewall Manager可以轻松管制防火墙策略。据介绍，亚马逊云科技去年缓解了70万次DDos攻击，Amazon WAF上每天托管规则请求超过3500亿条。</p><p>&nbsp;</p><p>针对威胁识别，Amazon GuardDuty使用了基于人工智能和机器学习的技术，使安全事件的误报率减少50%。它能够实现初期的检测，还可以做持续的分析，它会使用机器学习的技术来检测所有的威胁，以智能化的手段给予采取行动的建议。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-09-08 13:51:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "比Python快68000倍！Mojo正式发布，网友：Python生态系统最重要的升级来了",
    "url": "https://www.infoq.cn/article/eXdKQS3BhVWJ6JlXuiXy",
    "summary": "<p>自 5 月 2 日首次亮相以来，Modular 公司的 <a href=\"https://www.infoq.cn/article/vmhNfhuc140CXrj5JDiv\">Mojo</a>\" 编程语言引起了开发人员的极大兴趣，已经有超 12 万开发人员注册使用 Mojo Playground，19 万开发者热情参与 Discord 与 GitHub 讨论。Fast.ai 联合创始人、数据科学家 Jeremy Howard 更是表示，“Mojo 可能是近几十年来最大的编程语言进步。”</p><p></p><p>9 月 7 日，<a href=\"https://www.infoq.cn/article/GFfVLVpkIGOcKYB85Opb\">Modular 公司</a>\"宣布正式发布 Mojo：Mojo 现在已经开放本地下载——初步登陆 Linux 系统，并将很快提供 Mac 与 Windows 版本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d575aa3da3579bd694a909aa5aa961b.png\" /></p><p></p><p>Modular 公司由 LLVM 和 Swift 编程语言的联合创始人 Chris Lattner 创办，此前刚获得 1 亿美元（约 7 亿人民币）融资。Chris Lattner 表示，本轮融资总额达到 1.3 亿美元，所得款项将用于产品扩展、硬件支持和推动自研 AI 编程语言 Mojo 的进一步发展。</p><p></p><p>据介绍，Mojo Playground 提供的只是一套简单的语言展示，而本地 Mojo 工具链则可帮助开发人员完成更多工作。本地开发者工具将开放全部 Mojo 功能，包括一套完整的编译器功能与 IDE 工具，让开发者能够轻松对 Mojo 应用进行构建与迭代。</p><p></p><h2>Mojo：面向 AI 开发者的新型编程语言</h2><p></p><p></p><p>Mojo 是一种面向 AI 开发者的新型编程语言。随着时间推移，它将逐步成长为 Python 的超集。Mojo 已经支持与任意 Python 代码的无缝集成，并提供可扩展的编程模型以支撑各类关键性能系统，包括在 AI 场景中普遍存在的加速器（例如 GPU）。</p><p></p><p>Mojo 能够切实满足开发者需求，引导大家逐步采用新功能，从而在必要时获得高性能体验。具体来说，Mojo 可以为开发者带来的主要收益：</p><p>&nbsp;</p><p>一种语言编写所有内容：Mojo 能够随时随地服务于 AI 开发者，将 Python 的可用性与以往强制开发者使用 C、C++ 或 CUDA 的系统编程功能结合起来。 开发者可以在公共代码库上工作，从而简化从研究到生产的整个工作流程。突破 Python 性能极限：时至今日，Python 已经无处不在。但对于需要更高性能或特殊硬件的任务，Python 的表现往往不那么理想。Mojo 能够发挥 CPU 的性能潜力，并可良好支持 GPU 和 ASIC 等外部加速器，提供与 C++ 和 CUDA 相当的卓越性能。对接完整 Python 生态系统：Mojo 提供与 Python 生态系统间的全面互操作性，使其能够无缝利用 Python 库资源，同时发挥 Mojo 自身的功能与性能优势。例如，开发者可以将 NumPy 和 Matpotlib 同自己的 Mojo 代码无缝混合起来。升级 AI 工作负载：Mojo 紧密集成有模块化 AI 引擎，允许开发者通过自定义操作轻松扩展自己的 AI 工作负载，包括预处理、后处理操作和高性能数学算法。开发者还可以引入内核融合、图重写、sharp 函数等。</p><p></p><p>通过对现有 Python 代码做一点简单变更，开发者就可以使用 Mojo 对高计算强度工作负载进行显著加速（最高可提速 6.8 万倍）。目前，Mojo 的实际应用案例包括：</p><p></p><p>知名博主Maxim Zaks已经用Mojo实现了多个树数据结构，并发布了相关博文（<a href=\"https://pub.aimind.so/a-high-level-introduction-to-fibytree-bd7f8775d815\">https://pub.aimind.so/a-high-level-introduction-to-fibytree-bd7f8775d815</a>\"）和一些初步基准测试结果（<a href=\"https://pub.aimind.so/fibytree-vs-set-and-sortedset-7b4e6b56cac8\">https://pub.aimind.so/fibytree-vs-set-and-sortedset-7b4e6b56cac8</a>\"）。GitHub用户MadAlex1997在Mojo中构建了N维数组的实现（<a href=\"https://github.com/MadAlex1997/Mojo-Arrays\">https://github.com/MadAlex1997/Mojo-Arrays</a>\"）。</p><p></p><h2>为何能比Python快68000倍？</h2><p></p><p></p><p>Mojo 是 Python 家族的一员，但有着远大的目标——想要与 Python 生态系统完全兼容，因此开发人员可以继续使用自己熟悉的工具。Mojo 旨在通过保留 Python 的动态特性，同时为系统编程添加新原语，逐渐成为 Python 的超集。</p><p></p><p>LLVM 和 Swift 编程语言的联合创始人、Modular 公司 CEO Chris Lattner 此前在 Hacker News 上表示：“我们的目标不是让动态 Python 神奇地快速。虽然我们在动态代码方面要快得多（因为我们有编译器而不是解释器），但这并不是依靠‘足够智能’的编译器来消除动态性”。</p><p></p><p>据介绍，Mojo 最初的目标是比 Python 快 35000 倍，近日该团队表示，Mojo 将动态与静态语言的优点结合起来，一举将性能提升达 Python 的 68000 倍。</p><p></p><p>Mojo 团队在系列博文中介绍了 Mojo 是如何比 Python 快 68000 倍的：在第一篇博文中，团队尝试将代码移植为 Mojo，从而获得了约 90 倍的性能提升；在第二篇博文中，团队对代码进行矢量化与并行化，又将性能提升了 2.6 万倍；在第三篇博文中，团队展示如何通过新的性能技术全面超越 3.5 万倍的程序加速目标。</p><p></p><p>具体来说，Mojo 团队首先用简单端口将 Python 程序提速 89 倍；之后通过针对性优化和利用现代 CPU 的算力潜能，又将速度提升 2.6 万倍。Mojo 团队提出的并行策略是，每个 CPU 核心都应负责处理同等数量的代码行。</p><p></p><p>然而，只有当跨行工作负载相同时，对负载进行拆分以保证各个线程 worker 获取其中一组代码行才具有可行性；但曼德勃罗集并不是这样。以这种方式进行拆分会引发负载不均衡问题，这是因为曼德勃罗集中的一个像素可能在单次迭代后完成，而另一像素则可能经历 MAX_ITERS 多次迭代。也就是说，各个行的迭代次数并不相等，会导致某些率先完成计算的线程处于闲置状态，不利于全面挖掘性能潜力。</p><p></p><p>为了演示这种不均衡问题，Mojo 团队绘制了每个行在曼德勃罗集中执行的迭代总数。如下图所示，某些行在转义之前只需要不到 1000 次迭代，但其他一些行则可能需要超 80 万次迭代。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c5b5050229330c409f317c5efe211fce.png\" /></p><p></p><p>如果硬性为各个线程分配一定数量的连续行，就会发生全体线程都在等待，直至中间某组代码行（被分配给某个核心）完成运行的情况。解决这个问题的办法有很多，但最简单的当然是过度拆分。也就是说，各个线程所获得的不是一组平均分配的行，而是建立起一个工作负载池，再为每个行创建相应的工作项。各线程则以循环方式不断从线程池中拾取这些工作项。</p><p></p><p>好消息是 Mojo 拥有一个性能出色的并发运行时，所以我们用不着自行创建线程池或者设计循环拾取/执行。Mojo 的运行时提供不少高级功能，可以充分利用这样的多核心系统。</p><p></p><p><code lang=\"text\">fn compute_row(y:Int):\n    let cy = min_y + h * scale_y\n    @parameter\n    fn compute_vector[simd_width:Int](w:Int):\n        let cx = min_x + iota[DType.float64, simd_width]() * scale_x\n        output.simd_store[simd_width](Index(h,w), \n                mandelbrot_kernel(ComplexSIMD[DType.float64, \n                                              simd_width](cx,cy))\n    vectorize[num_ports * simd_width, compute_vector](width)\nwith Runtime(num_cores()) as rt:\n    let partition_factor = 16 # Is autotuned.\n    parallelize[compute_row](rt, height, partition_factor * num_cores())</code></p><p></p><p>可以分别在拆分成2、4、8、16和32份时评估程序性能，相应结果如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/61b41b2ea0c0bed3f8d9398bf47d43d2.png\" /></p><p></p><p>到这里，Mojo 团队获得了 2.3 倍于并行版本的加速效果，更是达到矢量化实现版本的 78 倍。那么，在每个行中做进一步划分会不会让性能更上一层楼？如果单行很大，那也许可以。但Mojo 团队这个示例中的最大单行长度也不过 4096。另外，同一行内的各像素间往往更具相关性。这时候更适合采用单指令流多数据流（SIMD），避免工作被白白浪费在矢量通道当中。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52afffcf8aa065fcfef49c614899a1fb.png\" /></p><p></p><p>回顾整个旅程，Mojo 团队先是对 Python 代码实现了 2.6 万倍性能提升，之后又使用超额订阅达成提速 68847 倍的最终成绩，并最终实现了 6.8 万倍的 Python 提速效果；而在应用超额订阅之后，性能在此前并行版本的基础上又提高了 1 倍。</p><p></p><h2>如何使用 Mojo？</h2><p></p><p></p><p>目前，开发者可以将 Mojo 下载至自己的本地计算机上，Modular 公司表示，Mojo 绝不只是编译器那么简单。</p><p></p><h4>Mojo 工具箱</h4><p></p><p>&nbsp;</p><p>Mojo SDK 的首个版本即提供轻松开发 Mojo 程序所需要的一切，具体包括以下工具选项：</p><p>&nbsp;</p><p>Mojo 驱动程序：提供用于 read-eval-print-loop（REPL）的 shell，允许开发者构建并运行 Mojo 程序、打包 Mojo 模块、生成文档和格式化代码。面向 Visual Studio&nbsp;Code（VS Code）的扩展：支持各种生产力功能，例如语法高亮显示、代码补全等。Jupyter 内核：支持构建和运行 Mojo notebook，包括使用 Python 代码。支持调试（即将推出）：进入并检查正在运行中的 Mojo 程序，甚至可以将 C++ 与 Mojo 堆栈帧混合起来。</p><p>&nbsp;</p><p>本次发布的 SDK 初始版本将支持 x86/Linux 系统，在后续更新中，团队将进一步扩展至其他操作系统、硬件和工具功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fd/fd870b331d547f2be27fd9bb5c56756e.png\" /></p><p></p><h4>Mojo 驱动程序</h4><p></p><p>&nbsp;</p><p>与 Python 一样，开发者可以运行 Mojo 命令在 REPL 中进行编程。下面是一个在 Mojo 中计算欧氏几何距离的示例：</p><p></p><p><code lang=\"text\">$ Mojo\nWelcome to Mojo! 🔥\nExpressions are delimited by a blank line.\nType `:Mojo help` for further assistance.\n1&gt; %%python\n2. import numpy as np\n3. n = 10000000\n4. anp = np.random.rand(n)\n5. bnp = np.random.rand(n)\n6&gt; from tensor import Tensor\n7. let n: Int = 10000000\n8. var a = Tensor[DType.float64](n)\n9. var b = Tensor[DType.float64](n)\n10. for i in range(n):\n11.    a[i] = anp[i].to_float64()\n12.    b[i] = bnp[i].to_float64()\n13&gt; from math import sqrt\n14. def Mojo_naive_dist(a: Tensor[DType.float64], b: Tensor[DType.float64]) -&gt; Float64:\n15.    var s: Float64 = 0.0\n16.    n = a.num_elements()\n17.    for i in range(n):\n18.       dist = a[i] - b[i]\n19.       s += dist*dist\n20.    return sqrt(s)\n23&gt; fn Mojo_fn_dist(a: Tensor[DType.float64], b: Tensor[DType.float64]) -&gt; Float64:\n24.    var s: Float64 = 0.0\n25.    let n = a.num_elements()\n26.    for i in range(n):\n27.       let dist = a[i] - b[i]\n28.       s += dist*dist\n29.    return sqrt(s)\n30.\n31&gt; let naive_dist = Mojo_naive_dist(a, b)\n32. let fn_dist = Mojo_fn_dist(a, b)\n33. print(fn_dist)\n34. \n1290.8521425092235\n35. print(naive_dist)\n36. \n1290.8521425092235 </code></p><p></p><p>此外，Mojo 还允许开发者构建静态编译的可执行文件，从而在无需任何依赖项的前提下进行部署。例如，开发者可以编译并运行示例库当中的&nbsp;hello.🔥 程序，如下所示：</p><p></p><p><code lang=\"text\">$ Mojo build hello.🔥\n$ ./hello\nHello Mojo 🔥!\n9\n6\n3\n$ ls -lGtranh hello*\n-rw-r--r-- 1 0   817 Sep  3 23:59 hello.🔥\n-rwxr-xr-x 1 0   22K Sep  3 23:59 hello</code></p><p></p><p>这个静态编译的 22 kB 二进制文件非常酷，它的实现离不开 Mojo 紧凑的依赖项管理机制。</p><p></p><h4>Visual Studio Code 扩展</h4><p></p><p>&nbsp;</p><p>VS Code 是目前全球最流行的 IDE 之一。Mojo 已经在 Visual&nbsp;Studio&nbsp;Marketplace 上发布了官方扩展，能够直接提供支持。如此一来，各位开发者就能轻松在生产流程中获取 Mojo 支持语法高亮显示、诊断和修复、定义和引用、悬停帮助、格式化、代码补全。</p><p></p><h4>Jupyter 集成</h4><p></p><p>&nbsp;</p><p>Jupyter 为交互式开发提供了一套强大的环境。Mojo 包含一个 Jupyter 内核，允许开发者直接在其中使用 Jupyter notebook。团队在 GitHub 上共享了 Mojo Playground 中的所有 notebook（<a href=\"https://github.com/modularml/mojo/tree/main/examples/notebooks\">https://github.com/modularml/mojo/tree/main/examples/notebooks</a>\"），关于更多细节信息请参阅 README 自述文件。</p><p></p><h4>调试支持（即将推出）</h4><p></p><p>&nbsp;</p><p>在即将发布的新版本中，团队将在 VS Code 中添加通过 LLDB 命令行界面实现的交互式调试体验。不仅如此，Mojo 的调试程序能够在同一调试会话中无缝对 Mojo/C/C++ 混合代码进行操作，进一步增强开发者在处理高度专业化代码时的能力。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.modular.com/blog/mojo-its-finally-here\">https://www.modular.com/blog/mojo-its-finally-here</a>\"</p><p><a href=\"https://www.modular.com/blog/mojo-a-journey-to-68-000x-speedup-over-python-part-3\">https://www.modular.com/blog/mojo-a-journey-to-68-000x-speedup-over-python-part-3</a>\"</p>",
    "publish_time": "2023-09-08 14:13:53",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "FCon联席主席吴震操：金融数据治理既是技术问题也是管理问题",
    "url": "https://www.infoq.cn/article/fC0VKxtiIoH096FrfD3U",
    "summary": "<p>金融行业数字化转型正处于加速阶段，但转型过程中的痛点和挑战仍然无法避免。在9月5日的InfoQ超级连麦·数智大脑直播中，极客邦科技创始人&amp;CEO霍太稳对话FCon金融科技大会联席主席、恒生聚源总经理吴震操，一起围绕金融数字化转型的当前形势，金融数据治理过程中需要注意的问题，以及大模型等先进技术对金融业创新的影响等话题进行了探讨。</p><p></p><p>吴震操表示，金融行业数字化转型的挑战主要表现在业务复杂、数据异构，强监管，和业务传统带来的惯性。对此，金融机构不仅要把数据治理视为一个技术问题，更要视为一个管理问题。在他看来，技术解决了数据的收集、存储、自动化建模和分发，但不能解决业务流程再造、数据和接口标准管控等，这些是管理问题的一部分。</p><p></p><p>以下是对话全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>InfoQ：金融行业的数字化转型现状如何？</h5><p></p><p></p><p>吴震操：整个金融行业从电子化转向信息化，再到现在的数字化，体现了景公行业不断变革和发展的态势。数字化不仅是提高服务质量和资产管理效率，本质上是如何高效地帮助实体行业进行融资和发展。</p><p></p><p>金融行业数字化转型的现状是“参差不齐”。这主要是因为数字化转型需要高额的投资，而金融行业又处在严格的监管环境下。虽然一些企业可以通过使用公有云和SaaS服务来降低运营成本，但由于金融行业对数据安全有更高的要求，很多解决方案需要内部实施和消化。</p><p></p><p>特别是对中小型金融机构来说，高昂的数字化投资成本是一大挑战。举个例子，一个小型银行如果要推出信用卡业务，每年需要投入千万级用于系统建设和运营，而其收入和客户数量却可能无法支撑这样高额的投入。相比之下，大型金融机构，由于资金充裕，更容易实现数字化转型，并且更可能看到投资回报。一些大机构甚至孵化出自己的金融科技公司，以推动数字化进程。</p><p></p><p>我认为金融行业的数字化转型正处于一个加速阶段。随着新技术的出现，比如云计算解决了算力问题，更先进的算法模型也逐渐应用，加上金融行业本身拥有海量数据，这都为行业提供了难得的转型机会。不论是大型还是中小型机构，都有各自不同的实施路径和解决方案。</p><p></p><h5>InfoQ：从您的角度和经验来看，金融行业在进行数字化转型的过程中，遇到哪些困难或痛点？</h5><p></p><p></p><p>吴震操：金融行在业数字化转型的过程中，痛点主要集中在以下几个方面：</p><p></p><p>业务异构</p><p>首先，金融机构（如银行和证券公司）面临的第一个挑战是业务异构性。这意味着不同业务部门有各自独特的特点和需求。以一个综合性商业银行为例，它既有零售业务也有对公业务，还涵盖贷款、信用卡、银行间金融市场、资金托管、投资银行等多个方面。这些各个部分都有自己对数据使用和业务管理的特殊需求。</p><p>数据异构</p><p>其次是数据异构性的挑战。金融行业需要处理大量不同类型的数据，包括结构化和非结构化数据，电子和非电子数据等。例如，在个人银行业务中，还涉及身份验证、生物信息、各种申请单据和抵押物资料等。因此，数据管理变得复杂和多样。</p><p>传统惯性</p><p>最后，金融行业因为其行业敏感性，一直以来对数据的安全性、准确性和时效性的要求都很高，行业整体在过去几十年的发展中形成了一套相对固定、传统的运营模式。这种“惯性”或“传统”在某种程度上阻碍了数字化转型的进程。</p><p></p><h5>InfoQ：从您的角度来看，为了应对金融行业在数字化转型过程中遇到的异构和数据安全等问题，我们应该采取哪些措施和使用哪些技术？</h5><p></p><p></p><p>吴震操：解决这些问题需要从多个方面来考虑：</p><p></p><p>组织决心与变革：数字化转型不仅是技术变革，更是组织变革。首先需要克服组织和企业的惯性，这是第一步；业务流程的深度刻画：虽然大多数金融机构已经实现了信息化，但从信息化到数字化需要深度地刻画现有的业务流程和管理方式；了解客户和产品（KYP &amp; KYC）：金融机构需要用数字来描述他们的客户和产品，这是数字化转型中非常关键的一步；数据的有效利用：金融机构拥有海量的数据，问题是如何有效地使用这些数据。例如，在资本市场投资和银行信贷的业务中，需要对各种数据进行业务建模；系统实现与落地：有了业务模型和数据后，接下来就是如何通过系统将这些应用到实际业务中，这可能涉及到大数据平台、数据仓库和其他技术解决方案；文化和组织问题：在整个数字化转型过程中，还需要解决组织文化和结构的问题。</p><p></p><p>总体来说，无论是大型还是小型金融机构，都需要解决这些共性问题以成功进行数字化转型。</p><p></p><h5>InfoQ：数据治理到底是一个技术问题还是管理问题？从您的角度，金融机构应该如何保证或提升数据质量？</h5><p></p><p></p><p>吴震操：数据治理是一个技术问题，更是一个管理问题。</p><p></p><p>数据治理的双重性质：技术解决了数据的收集、存储、自动化链接和分发，但技术不能解决数据的标准和接口标准，这些是管理问题的一部分。管理性问题：数据治理也涉及到管理性问题，比如统一的数据标准、管理理念等。例如，很多公司在做数据中台时，强调有 \"one ID\"、\"one API\"、\"one Model\"、\"one S、ervice\"，这些都是治理中的关键管理性问题。数据质量与来源：要保证数据质量，首先要考虑数据来源。数据可能来自业务过程、外部购买或第三方。无论来源如何，都需要在数据进来的时候进行贴源治理，确保数据准确和完整。成本与效益：数据治理可能会有很高的成本，但这是一个平衡的过程。不同规模的机构可以从最关键的业务或产品入手，逐步推进数字化转型。信心与持续改进：在整个数字化转型过程中，最重要的是给团队和整个机构以信心。通过不断地积累小胜，逐渐转化为大胜。交易级的数据质量：在金融数据方面，强调一个词叫“交易级”的数据质量，因为交易是不能出错的。一旦出错就会带来极大的损失，所以希望使用的数据都能满足交易级的标准。逐步推进与投入：数字化转型是一个循序渐进的过程。大机构有大机构的投入方式，小机构可以从自己最关键的流程入手。最重要的一点就是要给整个团队、整个机构，给他以信心，不停地积小胜变成大胜。</p><p></p><h5>InfoQ：关于数据资产化的文件对金融行业的影响，您认为有哪些？</h5><p></p><p></p><p>吴震操：您提到的数据资产化在金融和数据行业中是一个非常前沿和复杂的话题。资产化的基本思想是将数据从一种单纯的信息或者服务转变为具有明确价值的资产，这样它们就能在财务报表上有所体现。这一转变会对企业和行业产生多方面的影响。</p><p></p><p>财务影响</p><p>1. 提升资产负债表: 如果数据能被合法和合规地资产化，它将增加公司财报中的资产总额，有助于提高公司的信用和融资能力。</p><p>2. 资本化可能性: 以前看似“无形”的数据投入，现在有可能转化为“有形”的资产，这样企业的研发和数据获取成本就有可能资本化。</p><p>3. 税收问题: 资产化会涉及到折旧和摊销等问题，可能对企业的税务结构产生影响。但这也取决于当地的税法和会计准则。</p><p></p><p>业务影响</p><p>1.数据交易市场: 资产化后，数据有可能进入交易市场，形成明确的买卖价格，从而创造新的收入来源。</p><p>2. 增加流动性: 当数据变成一种可以交易的资产时，它能增加市场流动性。</p><p>3. 新业务模型: 资产化可能会促使企业探索以数据为核心的新业务模型，比如数据交易、数据共享等。</p><p></p><p>安全与合规影响</p><p>1. 数据隐私与安全: 数据资产化可能引发一系列关于数据安全和隐私的问题，需要在资产化过程中充分进行考虑。</p><p>2. 法规与审计: 如您所提，这一变化也可能引入一系列新的审计和会计挑战，需要行业和监管机构共同探索。</p><p></p><p>行业影响</p><p>1. 创新与竞争: 可能会促使更多公司进入这一领域，加速行业创新和竞争。</p><p>2.促进数字化转型: 对于整个金融行业，特别是在数字化转型中的企业，数据资产化可以说是一种正向推动力。</p><p></p><h5>InfoQ：恒生聚源在金融行业的数字化转型的作用是什么？</h5><p></p><p></p><p>吴震操：作为一家在行业内服务超过20年的金融数据供应商，它服务于各类、各级资本市场，以提交市场效率。恒生聚源的业务范围非常广泛，包括但不限于二级市场的交易数据和一级市场的企业研究数据等等。通过深入的数据分析和模型构建，恒生聚源能够为银行、券商、基金、资管机构，甚至监管机构和央行提供有价值的信息。在这个过程中，恒生聚源使用了多种工具和模型，以帮助市场参与者快速了解市场动态，完成交易和投资，甚至进行深入的市场研究。这不仅提高了交易效率，还有助于机构财富增值。</p><p></p><p>更重要的是，恒生聚源通过数据和知识提供了基于这些信息的决策系统，从而进一步提高了整个金融市场的运行效率。在数字化转型的大潮中，恒生聚源可以说是最早推进数字化转型的公司之一。因为它本身的产品和服务就是数据和数字，这使得它在理解数据价值和数据衍生价值方面具有独到的见解。这些数据不仅能够反映出相关资产和交易标的的价值，还能为市场参与者和交易对手提供宝贵的信息。</p><p></p><h5>InfoQ：大模型在金融行业的应用有哪些？</h5><p></p><p></p><p>吴震操：尽管大模型在中国还不到一年时间就已商用化，但其应用前景非常广阔。因为金融行业本身就是信息密集型的，大模型能很好地处理大量的文本数据。首先，在资本市场的角度，大模型对投资研究有着天然的优势。投研人员需要处理和分析大量的信息，大模型能辅助他们更高效地进行信息检索、抽取和知识生成。内部业务团队和买方团队可以直接从这些研究报告中受益；</p><p></p><p>其次，在投资顾问和理财领域，大模型也有其独特的价值。消费者在进行投资时总会有各种疑问，大模型能准确地获取和分析市场资讯，为客户提供更加精准的建议，做好投后的陪伴；</p><p></p><p>另外，在客服场景，大模型也有巨大的应用空间。客服每天需要解决大量客户问题，大模型不仅可以辅助客服进行更有效的信息检索，还可以直接作为聊天机器人，提供更加人性化的服务；</p><p></p><p>除了上述应用，金融机构还有其他后线部门和场景，如投行、风控、监管和合规等，都能从大模型中受益。当然，大模型也有其局限性，如目前尚不能进行复杂的数学推理。特别是在金融领域。由于大模型是生成式模型，即使训练数据是高质量的，生成的信息也可能不准确。因此，在金融行业应用大模型时，必须谨慎处理这一问题，确保信息的准确性。</p><p></p><h4>关于 FCon</h4><p></p><p>首届<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">FCon全球金融科技大会</a>\"将于 11 月 19-20 日在上海举办。大会将围绕金融领域数字化转型挑战探索、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等 10+专题进行交流。</p><p></p><p>目前大会邀请了汇丰科技中国区的代理总经理马国栋、度小满金融数据智能部总经理杨青、蚂蚁集团副总裁 &amp; 首席技术安全官韦韬博士、恒生聚源总经理吴震操担任大会联席主席。更多嘉宾仍在邀请中......</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来。<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=zhibo&amp;utm_medium=szhart&amp;utm_campaign=5&amp;utm_term=09\">点击链接</a>\"即可查看全部演讲专题，目前 <a href=\"https://fcon.infoq.cn/2023/shanghai/apply?utm_source=zhibo&amp;utm_medium=szhart&amp;utm_campaign=5&amp;utm_term=09\">5 折 优惠购票，仅限前 100 人</a>\"，咨询购票可联系：13269078023（微信同手机号）。</p>",
    "publish_time": "2023-09-08 14:21:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "银行越来越像“互联网公司”，IDC发布银行数字科技五大趋势",
    "url": "https://www.infoq.cn/article/Wutse91k66iV8ay6xcQu",
    "summary": "<p>9月8日消息，在上海外滩大会的银行业数字化论坛上，IDC中国副总裁兼首席分析师武连峰发布了《银行数字科技五大趋势》：随身银行、AI风控、数字员工、边缘物联与云原生架构。这五大趋势为银行数字化与智能化转型提供了前瞻性的洞察。</p><p></p><p>蚂蚁集团资深副总裁、网商银行董事长金晓龙表示，中国银行业的发展核心动力已经从规模驱动转向了新技术驱动。</p><p></p><p>以下是《银行数字科技五大趋势》的关键要点：</p><p></p><h2>趋势一：随身银行</h2><p></p><p>对于用户来说，现在一部手机就相当于一个银行网点，9成以上业务都能“点一点”。</p><p></p><p>根据《2022年中国银行业服务报告》的数据，银行平均电子分流率已经高达97%，表明9成以上的业务都已经实现了线上化。随着中国手机银行App月活用户数超过5亿，以及银行在微信和支付宝等平台上推出的200多款小程序，随身银行已经成为现实。</p><p></p><p>这让银行越来越像“互联网公司”，比如，<a href=\"https://www.infoq.cn/article/pc7ihipY5vPdizqktlE5\">工商银行</a>\"App月活用户达到1.74亿，与众多头部互联网公司不相上下。再比如，招商银行早已将生活服务纳入App，包括餐饮、电影票购买和出行服务等，使得银行成为了用户日常生活的一部分。</p><p></p><h2>趋势二：AI风控</h2><p></p><p>人工智能将深入风控系统，“AI大脑”将成为银行主流选择。据IDC预测，到2026年，将人工智能应用于信贷授信将成为银行的主流选择。借助更复杂的模型和算法，该类应用有望在2026年底让银行欺诈和洗钱等案例降低12%。</p><p></p><p>目前已经银行在AI风控领域进行了探索，例如<a href=\"https://www.infoq.cn/article/fg7QlA5txzUp3yZvedGz\">网商银行</a>\"的百灵智能交互式风控系统，相当于一个AI信贷审批员，已服务800万小微经营者。另外，中信银行的“哨兵”智能防欺诈风控系统则通过机器学习和大数据技术，拦截了资金超过1亿元的欺诈交易。</p><p></p><h2>趋势三：数字员工</h2><p></p><p>IDC预测到2025年，超过80%的银行将部署数字人，承担90%的客服和理财咨询服务。数字员工将具备“看懂文字、听懂语言、做懂业务”的能力，成为常态。浦发银行率先聘用了数字员工，如3D数字员工“小浦”在20多个岗位“任职”。交通银行也有数字员工“姐妹花”姣姣和小姣，为客户提供咨询服务。这些数字员工不仅解决客户问题，还成为了银行的科技品牌名片。</p><p></p><h2>趋势四：边缘物联</h2><p></p><p>IDC预测，到2027年，40%的G2000企业将通过低轨道卫星技术满足偏远、农村和高风险地区的网络覆盖缺口。借助人工智能物联网技术，中小微型企业的贷款覆盖率将达到70%。</p><p></p><p>案例方面，<a href=\"https://www.infoq.cn/article/DwEzOuzTXhom8J7c0M4L\">平安银行</a>\"通过发射卫星搭建了“星云物联网平台”，供应链上下游企业的真实经营数据等信息可以更及时、有效地回传，从而为其授信，截至2023年6月末，星云物联网平台支持实体企业融资额超8000亿元。</p><p></p><p>网商银行的大山雀卫星遥感风控系统则应用了卫星遥感技术，为农户提供精准的授信和合理的还款周期，已服务120万农户，填补了农村金融服务的技术空白。</p><p></p><p>总的来说，物联网与边缘计算技术将进一步“打通”小微金融最后1公里，农户可通过卫星办理贷款。</p><p></p><h2>趋势五：云原生架构</h2><p></p><p>6成以上银行将采用云原生架构，支持安全可控与未来生态创新。</p><p></p><p>IDC预测，到2025年，60%的中国银行将制定并实施云原生数字核心战略。科技银行如网商银行和微众银行早早采用了云原生架构，而传统银行也在加速部署。</p><p></p><p>广发银行自2019年开始搭建容器云平台，已尝试接入223个应用。上云之后，广发银行IT成本大幅降低，CPU节省比例超43.54%，内存节省比例超65.12%。</p><p></p><p>建设银行成立“建行云”，涉足云计算业务，首批推出三大类10个云服务套餐，为中小微金融机构提供“开箱即用”的技术服务。</p>",
    "publish_time": "2023-09-08 15:19:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "实时分析、融合统一及云原生，现代化数据仓库未来发展必经之路｜专访飞轮科技 CEO 马如悦",
    "url": "https://www.infoq.cn/article/wrokWmUq9QQFhbpoIxgR",
    "summary": "<p>在国内拥有 2500+ 中大型企业用户，用户社群聚集开发者超 3 万人，活跃贡献者数连续数月稳居全球大数据开源项目排行榜第一。毋庸置疑，Apache Doris 已成为全国数据库和大数据领域最为活跃的开源项目之一。Apache Doris 历经近十年的发展，为何还能持续保持竞争力和活力？其背后的核心推动力又是什么？</p><p></p><p>在 QCon 全球软件开发大会·北京站的现场，我们有幸采访到了基于 Apache Doris 的商业化公司飞轮科技的 CEO、Apache Doris 项目创始人 &amp; PMC 马如悦。马如悦曾在百度担任过分布式计算团队、大数据工程团队和 AI 产品工程团队的技术负责人，还主导设计和开发了实时数仓 Apache Doris。他对于 Apache Doris 的技术发展路线以及实时数仓的未来发展有着更加深刻的认知。</p><p></p><p>以下是视频采访的全部内容，为方便读者查看，视频下方也附上了文字内容。</p><p></p><p></p><p></p><p>InfoQ：作为一名深耕数据库领域多年的从业者，您观察到企业对于数据仓库的需求点主要集中在哪方面？</p><p></p><p>马如悦：纵观数据仓库的发展历程，数据仓库的演进经历了三个阶段，第一阶段，即在 2010 年之前，传统数据仓库解决方案包括 Teradata、Greenplum、IBM Netezza 来处理大数据问题。第二阶段，约在 2010 年左右，随着谷歌三驾马车的问世，人们逐步开始使用 Hadoop 大数据平台进行数据分析。如今已经进入了第三阶段，现代化的数据仓库产品开始涌现，这些产品结合了传统数据仓库的可靠性和性能优势，同时具备了对大数据的高效处理和实时分析能力。</p><p></p><p>作为一个完整经历了第二代大数据平台历史的从业者，在百度早期我主要负责 Hadoop 和 Spark 相关的工作，后来主导设计了 Doris 并将 Doris 贡献至 Apache 基金会。我们认为第三代数据平台的主要特点是实时化、架构的融合统一化以及云原生化。</p><p></p><p>随着业务需求的不断增长，企业对数据的实时性要求也越来越高，现代化数据仓库需要具备高速的数据处理和分析能力，能够实时响应和处理大规模数据流。同时，企业越来越倾向于将数据存储和分析工作迁移到公有云 / 私有云 /K8s 上，因此现代化数据仓库需要支持云原生架构，具备弹性伸缩、自动化管理和云端部署的能力，以适应云计算环境的需求。</p><p></p><p>除此之外，我认为融合统一是也是现代化数据仓库核心的特征。在大数据领域，存在众多的系统和组件，它们往往在架构中扮演着不同的角色。而随着时代的进步，架构“减负”已成为企业发展的重要目标，因此像融合数据库、超融合数据库、湖仓一体、流批一体等具有“融合统一”特征的数据库开始涌现。结合实际发展来看，过去几年，事务数据库已呈现明显的融合趋势，像 OceanBase、TiDB 以及 PolarDB 等新一代 NewSQL 事务数据库已经出现。同样地，数据仓库走到今天，融合统一也是必经之路。</p><p></p><p>这些特点其实也就是企业对于实时数据仓库的真实需求，也是 Apache Doris 在过去几年来广受认可的重要原因。</p><p></p><p>InfoQ：近期，我们关注到飞轮科技宣布将企业产品 SelectDB Cloud 云原生存算分离架构贡献到 Apache Doris 社区。在云原生存算分离架构的研发过程中，团队遇到过最大的技术难题是什么？又是通过什么方式解决的？</p><p></p><p>马如悦：有很多人之前就问到 Apache Doris 为什么没有实现存算分离，是因为技术太难了吗？实际不然，存算分离相比于存算一体来说更容易实现，存算分离依赖 HDFS、云上对象存储这样的共享存储系统，在设计实现时无需在存储层有太多投入，只需专注于计算层的实现即可。</p><p></p><p>那么回到问题本身，我们在云原生存算分离架构的研发过程中遇到了哪些问题：</p><p></p><p>一是如何实现存储和计算的状态剥离。在存算分离架构下，计算节点不再存储主数据，而是将共享存储层作为统一的数据主存储空间，这样计算节点可以做到无状态，可以实现完全关机，同时可实现更便捷的数据共享，不同的集群之间以及不同的仓库可以便捷地进行数据共享。</p><p></p><p>二是如何解决读取效率下降问题。存算分离依赖从网络上读取存储系统的数据来进行计算，在一定程度上会造成计算性能的下降，为了解决这一问题，我们利用 SSD 提供高速缓存来应对底层对象存储系统性能不佳和网络传输带来的性能下降。其次我们在存算分离模式下，提供了同一个仓库多个物理计算集群的隔离方式。计算集群可以独立扩缩容，其计算节点的本地高速缓存都是隔离的，这样尽可能保证了比较好的隔离性，从而避免集群间的影响，保证整体计算性能不下降。</p><p></p><p>三是解决对象存储读取带宽限制的问题。当用户购买公有云之后，云厂商会为每个企业账号分配资源配额，并提供对象存储访问平台供用户访问。然而我们作为一家软件服务商，我们希望产品能够被上万家甚至更多的企业使用，这就需要更多的带宽资源。过去，大部分云厂商都是直接服务终端用户，很少针对我们这类型的软件服务商做设计。而通过我们与各大云厂商的积极沟通，已经争取到了更大的带宽资源配合，足以满足当前各企业的需求。</p><p></p><p>此外，安全问题也是企业非常关心的问题。对于数据安全和合规性有着更高要求的企业，希望在享受自动化管理特性的同时，能够将数据存放在自己的账号下进行管控。因此，SelectDB 预计在 9 月底推出云上的一种新的产品形态，这款新的产品形态将实现数据面在客户账户，而管控面仍然在我们账户运行，这可以很好满足相关行业对数据安全管理的要求。</p><p></p><p>InfoQ：选择将存算分离架构贡献回开源社区，这一决策的出发点是什么？</p><p></p><p>马如悦：在回答这个问题之前，我想先分享一些我的所见所知。做开源的公司一般都会遇到这个经典的提问：云厂商会不会免费用你的代码来进行托管赚钱？</p><p></p><p>过去几年，我们观察到 MongoDB、Redis、ElasticSearch 等项目在不断变更许可证，但我认为这些调整无法解决根本问题，云厂商并不会因为许可证的改变而选择必须与这些项目合作，反而可能会推动他们自己开发与之兼容的产品。例如亚马逊云科技就推出了 DocumentDB、OpenSearch，这反而加剧了竞争局面。</p><p></p><p>我们的路线与之完全不同，我们致力于构建更加开放的生态系统。相比于和云厂商进行存量竞争，我们更愿意与云厂商携手合作，共同扩展市场。我们的目标是将蛋糕做得更大，让整个市场蓬勃发展。我们希望将 Apache Doris 打造成 AP 领域事实上的“MySQL”，只要 Apache Doris 成功了、做大了，全面拥抱 Doris 的所有商业厂商都能获益。我们只要努力保证我们的商业产品有足够的竞争力，能在市场上占领足够大的份额，我们的收益也是最大的。所以，我们选择了一个更开放的道路，与各家云厂商敞开怀抱合作，有竞争有合作。这种合作共赢的态度将促进整个生态系统的健康发展。</p><p></p><p>那么话说回来，如何才算更加开放？举一个简单的例子，当我们打通上下游生态时，我们是应该打通 Apache Doris 还是 SelectDB？许多人可能会认为，作为一家商业公司，我们应该打通 SelectDB，而我认为这种思路是不正确的。实际上我们花费了很多精力来帮助所有的上下游打通 Doris，这样当用户无论使用哪一家基于 Doris 开发的商业分发版的厂商都能顺利使用。这样一来，上下游厂商也更有动力去打通 Apache Doris，云厂商也愿意持续拥抱 Apache Doris 社区，广大的用户也能不被任何一个商业公司锁定，因为大家都遵守了 Apache Doris 的开源标准，也就不需要去学习每家商业厂商的不同。所以一旦秉着开放的心态做事，不是两方、三方共赢，而是所有方共赢。</p><p></p><p>回到最初的提问，飞轮科技选择将存算分离架构贡献给 Apache Doris 社区，其本质目的也是为了进一步让更多用户收益，如果大家都需要，我们就希望在内核层面提供支持，而不是希望各家分发商提供自己的实现，然后造成未来这块各方产品的不一致。而在这样的背景下，用户也可以获得兼容性更好、性价比更高的产品使用体验。</p><p></p><p>InfoQ：对于 Apache Doris 社区用户而言，新的存算分离架构在使用上有什么差异？如果用户想从存算一体的架构切到存算分离的架构，迁移方式与迁移成本是怎样的？</p><p></p><p>马如悦：Apache Doris 在国内有大约 2500 家企业使用，任何一个功能不兼容的问题都会对广泛的用户产生重大影响。因此，我们必须认真考虑企业升级的问题。因此，当 Apache Doris 推出存算分离架构时，我们非常谨慎地进行规划。目前，我们采取逐步迭代的方式，以确保最大程度地保护已有大量用户的利益。</p><p>在 Apache Doris 2.0 中，我们开始引入部分存算分离的能力，来解决大多数用户所面临存储成本高昂和计算弹性不足的问题。通过冷热数据分层（Tiered Storage），可以把将冷数据下沉到存储成本更加低廉的对象存储中。其次还引入了无状态的计算节点 Compute Node，Compute Node 不保存任何本次持久数据，在集群扩缩容时无需进行数据分片的负载均衡，因此在数据湖分析这种具有明显高峰的场景中可以灵活扩容、快速加入集群分摊计算压力。虽然这个版本并不没有实现纯粹的存算分离，但已经能解决用户许多问题。</p><p></p><p>在即将发布的社区 2.1 版本中，会发布更加彻底的存算分离架构。该版本将采用两种模式之一运行：存算一体的部署模式和存算分离的部署模式。在两种模式下运行的 Apache Doris 将以不同的方式来存储主数据。从用户使用体验上而言，绝大部分功能都是一致的，但是也会因为实现架构和部署模式的不同，带来一些功能细节上的差异。如果用户想要使用这个版本，就需要在搭建集群之初明确采用哪种模式。</p><p>我们也计划在明年，更进一步优化存算一体和存算分离的融合过程，为用户提供更加丝滑的架构切换体验。</p><p></p><p>InfoQ：现在市场上数据库产品越来越多，您觉得对于飞轮科技这家公司来说，我们的机遇和挑战分别是什么？</p><p></p><p>马如悦：放眼全球，中国数据库市场可以说是竞争最为激烈的。全球大约有五六百款数据库产品，而仅中国市场就占据了其中的两到三百款，这足以表明中国数据库市场的多样性和竞争程度。</p><p></p><p>面对如此激烈的竞争压力，我们最大的挑战是如何在众多企业中持续保有核心竞争力，这与未来数据仓库的发展趋势密切相关。正如前文所述，我们认为当前数据仓库未来的发展趋势一定是朝着实时分析、融合统一和云原生化分析方向发展的。而在这三个方面，我认为我们是国内目前相对领先、且理解较为深刻的厂商。</p><p>以湖仓融合体系为例，很多厂商也都在谈自己的湖仓融合，词是一个词，但是每个厂商落地到产品上就会不一样。Apache Doris 社区有自己的务实理解。Apache Doris 已经提供了高性能的计算引擎，使企业能够灵活地查询数据湖中的计算格式。同时，Apache Doris 还可以作为一个开放的数据湖，提供更加原生的能力，被其他查询引擎高速查询，目前为止，同类数据库产品中这一点只有 Apache Doris 在践行。</p><p></p><p>由此可见，实时分析、融合统一和云原生化这三个特性对于我们来说，不仅仅是一个概念，更是一个实际可行的目标。在大模型技术异常火热的今天，有很多人问为什么不做向量数据库的支持。一方面，我们对于向量化数据库这个垂直领域的理解还不够透彻，更不会因为热度而脱离主力目标发展方向；另一方面，Apache Doris 已经拥有庞大的用户群体，我们更倾向于将现有问题解决好，坚定地将上述提到的三个特性做扎实，这就足以满足大多数用户的需求。</p><p></p><p>InfoQ：去年亚马逊云科技在 re·Invent 提出的“Zero-ETL”，要解决什么问题？飞轮科技是否会有相应的布局？</p><p></p><p>马如悦：在数据基础设施中，主要包含事务数据库和分析数据库（数据仓库）两类，Zero-ETL 更多是让两者间的数据能够快速导入和贯通。</p><p></p><p>我认为 Zero-ETL 的出现给出了这个领域另一个问题的可能答案。很长时间，人们一直在谈论 &nbsp;HTAP，特别是以 OceanBase、TiDB、PolarDB 等以 TP 为主的厂商。这些系统以 TP 为主，但也宣称能解决 AP 的问题。从这么多年来实践来看，大量用户更愿意采用类似 MySQL/Oracle + Apache Doris 的组合方案。而 Zero-ETL 的出现也正是同样的思路，提供了实现 HTAP 另外的一种选择，它不认同要在一套系统里同时处理 TP 和 AP 的做法，而是可以采用快速地将 TP 的数据同步到 AP 中的方案。Zero-ETL 通过优化 TP 和 AP 打通的更好体验，提供了 HTAP 一体化的解决方案，而不是试图去在一套系统里解决两个问题。</p><p></p><p>这与数据仓库的实时分析能力有很大关系。举例来说，对于事务数据库来说，上游 TP 数据库的增删改操作希望能在在秒级以内同步至 AP 数据库，并且希望能够在 AP 数据库中立即查询分析。如果让 TP 的数据以 ETL 的方式进入 AP，数据处理链路的增加，势必造成数据进入数仓的速度也会随着变慢，整体可见性将受到影响。同时，由于越来越多的数据分析需求无法提前预知，所以分析前的预处理也变得无法提前规划，数仓能力的增强，也使得数据可以快速进入，然后在查询时按需对数据进行转化处理即可。现代化数据仓库提供实时分析能力，就需要类似 Zero-ETL 这样的方案，将 TP 数据库数据以秒级的速度高效同步到数据仓库系统中，以供类似 Apache Doris 进行高效处理和分析。</p><p></p><p>飞轮科技与阿里云合作的面向 Apache Doris 的阿里云云原生服务产品（阿里云 SelectDB 版本），已经在 8 月 20 日进行邀测，预计在 9 月底进行公测，其中也包括类似于 Zero-ETL 的解决方案。用户使用阿里云的 MySQL 或者 PolarDB 来作为事务数据库后，只需进行适当配置，数据就可以立即同步到阿里云上的 SelectDB 中，从而实现 TP 和 AP 数据库之间更好的协同工作。</p><p></p><p>写在最后</p><p></p><p>在采访最后，马如悦强调：“很多人问大数据分析领域有没有更新的黑科技了，实际到现在，大数据分析这个领域不是产品太少，黑科技太少，而是太多太复杂了。Hadoop 逐渐被替代最主要的原因是组件过于复杂，而过去数据仓库领域也涌现出了太多的新概念，我们接下来更希望把产品做得更简单、统一。”在他看来，Apache Doris 能否在未来稳居大数据项目第一梯队，并不是依靠更多的组件更多的功能，而是要坚定的专注于实时分析、融合统一和云原生化，将产品做到极致，让数据分析快速简单。</p><p></p><p>“实时分析、融合统一以及云原生化”，是马如悦在采访过程中不断强调的三个关键词。我们也很乐于看到，未来在飞轮科技的持续引领下，Apache Doris 能够持续朝着以上三个目标迭代，不断降低企业使用数据系统的复杂度。</p>",
    "publish_time": "2023-09-08 15:42:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "英特尔通过软硬件为LIama 2大模型提供加速，持续发力推动AI发展",
    "url": "https://www.infoq.cn/article/pe9XaEuqLBqmjeOD4MlD",
    "summary": "<p></p><blockquote>英特尔广泛的AI硬件组合及开放的软件环境，为Meta发布的Llama 2模型提供了极具竞争力的选择，进一步助力大语言模型的普及，推动AI发展惠及各行各业。</blockquote><p></p><p></p><p>大语言模型（LLM）在生成文本、总结和翻译内容、回答问题、参与对话以及执行复杂任务（如解决数学问题或推理）方面表现出的卓越能力，使其成为最有希望规模化造福社会的AI技术之一。大语言模型有望解锁更丰富的创意和洞察，并激发AI社区推进技术发展的热情。</p><p></p><p>Llama 2旨在帮助开发者、研究人员和组织构建基于生成式AI的工具和体验。Meta发布了多个Llama 2的预训练和微调版本，拥有70亿、130亿和700亿三种参数。通过Llama 2，Meta在公司的各个微调模型中采用了三项以安全为导向的核心技术：安全的有监督微调、安全的目标文本提取以及安全的人类反馈强化学习（RLHF）。这些技术相结合，使Meta得以提高安全性能。随着越来越广泛的使用，人们将能够以透明、公开的方式不断识别并降低生成有害内容的风险。</p><p></p><p>英特尔致力于通过提供广泛的硬件选择和开放的软件环境，推动AI的发展与普及。英特尔提供了一系列AI解决方案，为AI社区开发和运行Llama 2等模型提供了极具竞争力和极具吸引力的选择。英特尔丰富的AI硬件产品组合与优化开放的软件相结合，为应对算力挑战提供了可行的方案。</p><p></p><p></p><blockquote>英特尔提供了满足模型的开发和部署的AI优化软件。开放生态系统是英特尔得天独厚的战略优势，在AI领域亦是如此。我们致力于培育一个充满活力的开放生态系统来推动AI创新，其安全、可追溯、负责任以及遵循道德，这对整个行业至关重要。此次发布的大模型进一步彰显了我们的核心价值观——开放，为开发人员提供了一个值得信赖的选择。Llama 2模型的发布是我们行业向开放式AI发展转型迈出的重要一步，即以公开透明的方式推动创新并助力其蓬勃发展。-- 李炜英特尔软件与先进技术副总裁兼人工智能和分析部门总经理-- Melissa Evers英特尔软件与先进技术副总裁兼执行战略部总经理</blockquote><p></p><p></p><p>在Llama 2发布之际，我们很高兴地分享70亿和130亿参数模型的初始推理性能测试结果。这些模型在英特尔AI产品组合上运行，包括Habana® Gaudi®2 深度学习加速器、第四代英特尔® 至强® 可扩展处理器、英特尔® 至强® CPU Max系列和英特尔®数据中心GPU Max系列。我们在本文中分享的性能指标是我们当前软件提供的“开箱即用”的性能，并有望在未来的软件中进一步提升。我们还支持700亿参数模型，并将很快分享最新相关信息。</p><p></p><h2>Habana® Gaudi®2 深度学习加速器</h2><p></p><p></p><p>Habana® Gaudi®2旨在为用户提供高性能、高能效的训练与推理，尤其适用于诸如Llama和Llama 2的大语言模型。Gaudi®2加速器具备96GB HBM2E的内存容量，可满足大语言模型的内存需求并提高推理性能。Gaudi®2配备Habana® SynapseAI® 软件套件，该套件集成了对PyTorch和DeepSpeed的支持，以用于大语言模型的训练和推理。此外，SynapseAI近期开始支持HPU Graphs和DeepSpeed推理，专门针对时延敏感度高的推理应用。Gaudi®2还将进行进一步的软件优化，包括计划在2023年第三季度支持FP8数据类型。此优化预计将在执行大语言模型时大幅提高性能、吞吐量，并有效降低延迟。</p><p></p><p>大语言模型的性能需要灵活敏捷的可扩展性，来突破服务器内以及跨节点间的网络瓶颈。每张Gaudi®2芯片集成了21个100Gbps以太网接口，21个接口专用于连接服务器内的8颗Gaudi®2，该网络配置有助于提升服务器内外的扩展性能。</p><p></p><p>在近期发布的MLPerf基准测试中，Gaudi®2在大语言模型上展现了出色的训练性能，包括在384个Gaudi®2加速器上训练1750亿参数的GPT-3模型所展现的结果。Gaudi®2经过验证的高性能使其成为Llama和Llama 2模型训练和推理的高能效解决方案。</p><p></p><p>图1显示了70亿参数和130亿参数Llama 2模型的推理性能。模型分别在一台Habana Gaudi®2设备上运行，batch size=1，输出token长度256，输入token长度不定，使用BF16精度。报告的性能指标为每个token的延迟（不含第一个）。该测试使用optimum-habana文本生成脚本在Llama模型上运行推理。optimum-habana库能够帮助简化在Gaudi加速器上部署此类模型的流程，仅需极少的代码更改即可实现。如图1所示，对于128至2000输入token，在70亿参数模型上Gaudi®2的推理延迟范围为每token 9.0-12.2毫秒，而对于130亿参数模型，范围为每token 15.5-20.4毫秒1。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1a/1afc04c85d7492ba8d1ec5bbac2788e0.png\" /></p><p>图1&nbsp;基于Habana®&nbsp;Gaudi®2，70亿和130亿参数Llama 2模型的推理性能</p><p></p><p>若想访问Gaudi®2，可按照此处(<a href=\"https://developer.habana.ai/intel-developer-cloud/\">https://developer.habana.ai/intel-developer-cloud/</a>\")在英特尔开发者云平台上注册一个实例，或联系超微（Supermicro）了解Gaudi®2服务器基础设施。</p><p></p><h2>英特尔® 至强® 可扩展处理器</h2><p></p><p></p><p>第四代英特尔® 至强® 可扩展处理器是一款通用计算处理器，具有英特尔® 高级矩阵扩展（英特尔® AMX）的AI加速功能。具体而言，该处理器的每个核心内置了BF16和INT8通用矩阵乘（GEMM）加速器，以加速深度学习训练和推理工作负载。此外，英特尔® 至强® CPU Max系列，每颗CPU提供64GB的高带宽内存（HBM2E），两颗共128GB，由于大语言模型的工作负载通常受到内存带宽的限制，因此，该性能对于大模型来说极为重要。</p><p></p><p>目前，针对英特尔至强处理器的软件优化已升级到深度学习框架中，并可用于PyTorch*、TensorFlow*、DeepSpeed*和其它AI库的默认发行版。英特尔主导了torch.compile CPU后端的开发和优化，这是PyTorch 2.0的旗舰功能。与此同时，英特尔还提供英特尔® PyTorch扩展包*（Intel®Extension for PyTorch*），旨在PyTorch官方发行版之前，尽早、及时地为客户提供英特尔CPU的优化。</p><p></p><p>第四代英特尔® 至强® 可扩展处理器拥有更高的内存容量，支持在单个插槽内实现适用于对话式AI和文本摘要应用的、低延迟的大语言模型执行。对于BF16和INT8，该结果展示了单个插槽内执行1个模型时的延迟。英特尔® PyTorch扩展包*支持SmoothQuant，以确保INT8精度模型具有良好的准确度。</p><p></p><p>考虑到大语言模型应用需要以足够快的速度生成token，以满足读者较快的阅读速度，我们选择token延迟，即生成每个token所需的时间作为主要的性能指标，并以快速人类读者的阅读速度（约为每个token 100毫秒）作为参考。如图2、3所示，对于70亿参数的Llama2 BF16模型和130亿参数的Llama 2 INT8模型，第四代英特尔至强单插槽的延迟均低于100毫秒2。</p><p></p><p>得益于更高的HBM2E带宽，英特尔至强CPU Max系列为以上两个模型提供了更低的延迟。而凭借英特尔AMX加速器，用户可以通过更高的批量尺寸（batch size）来提高吞吐量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d141fa1b1f5faf824146b7c5b4d92d15.png\" /></p><p>图2 基于英特尔至强可扩展处理器，70亿参数和130亿参数Llama 2模型（BFloat16）的推理性能</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e75bfa044bacc5eb13790e602c9b2ff5.png\" /></p><p>图3 基于英特尔至强可扩展处理器，70亿参数和130亿参数Llama 2模型（INT8）的推理性能</p><p></p><p>对于70亿和130亿参数的模型，每个第四代至强插槽可提供低于100毫秒的延迟。用户可以分别在两个插槽上同时运行两个并行实例，从而获得更高的吞吐量，并独立地服务客户端。亦或者，用户可以通过英特尔®PyTorch扩展包 和DeepSpeed CPU，使用张量并行的方式在两个第四代至强插槽上运行推理，从而进一步降低延迟或支持更大的模型。</p><p></p><p>关于在至强平台上运行大语言模型和Llama 2，开发者可以点击此处（<a href=\"https://intel.github.io/intel-extension-for-pytorch/llm/cpu/\">https://intel.github.io/intel-extension-for-pytorch/llm/cpu/</a>\"）了解更多详细信息。第四代英特尔至强可扩展处理器的云实例可在AWS和Microsoft Azure上预览，目前已在谷歌云平台和阿里云全面上线。英特尔将持续在PyTorch*和DeepSpeed*进行软件优化，以进一步加速Llama 2和其它大语言模型。</p><p></p><h2>英特尔®数据中心GPU Max系列</h2><p></p><p></p><p>英特尔®数据中心GPU Max系列提供并行计算、科学计算和适用于科学计算的AI加速。作为英特尔性能最为出色、密度最高的独立显卡，英特尔®数据中心GPU Max系列产品中封装超过1000亿个晶体管，并包含多达128个Xe内核，Xe是英特尔GPU的计算构建模块。</p><p></p><p></p><blockquote>英特尔®数据中心GPU Max系列旨在为AI和科学计算中使用的数据密集型计算模型提供突破性的性能，包括：408 MB基于独立SRAM技术的L2缓存、64MB L1缓存以及高达128GB的高带宽内存（HBM2E）。AI增强型的Xe英特尔®矩阵扩展（英特尔®XMX）搭载脉动阵列，在单台设备中可实现矢量和矩阵功能。</blockquote><p></p><p></p><p>英特尔Max系列产品统一支持oneAPI，并基于此实现通用、开放、基于标准的编程模型，释放生产力和性能。英特尔oneAPI工具包括高级编译器、库、分析工具和代码迁移工具，可使用SYCL轻松将CUDA代码迁移到开放的C++。&nbsp;</p><p></p><p>英特尔数据中心Max系列GPU通过当今框架的开源扩展来实现软件支持和优化，例如面向PyTorch 的英特尔扩展、面向TensorFlow 的英特尔®扩展和面向DeepSpeed*的英特尔®扩展。通过将这些扩展与上游框架版本一起使用，用户将能够在机器学习工作流中实现快速整合。&nbsp;</p><p></p><p>我们在一个600瓦OAM形态的GPU上评估了Llama 2的70亿参数模型和Llama 2的130亿参数模型推理性能，这个GPU上封装了两个tile，而我们只使用其中一个tile来运行推理。图4显示，对于输入长度为32到2000的token，英特尔数据中心GPU Max系列的一个tile可以为70亿参数模型的推理提供低于20毫秒的单token延迟，130亿参数模型的单token延迟为29.2-33.8毫秒3。因为该GPU上封装了两个tile，用户可以同时并行运行两个独立的实例，每个tile上运行一个，以获得更高的吞吐量并独立地服务客户端。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fce796f21d03756ae54ee654695f0177.png\" /></p><p>图4&nbsp;英特尔数据中心GPU&nbsp;Max&nbsp;1550上的Llama&nbsp;2的70亿和130亿参数模型的推理性能</p><p></p><p>关于在英特尔GPU平台上运行大语言模型和Llama 2，可以点击此处（<a href=\"https://intel.github.io/intel-extension-for-pytorch/llm/xpu/\">https://intel.github.io/intel-extension-for-pytorch/llm/xpu/</a>\"）获取详细信息。目前英特尔开发者云平台上已发布英特尔GPU Max云实例测试版。</p><p></p><h2>英特尔平台上的大语言模型微调</h2><p></p><p></p><p>除了推理之外，英特尔一直在积极地推进微调加速，通过向Hugging Face Transformers、PEFT、Accelerate和Optimum库提供优化，并在面向Transformers的英特尔®扩展中提供参考工作流。这些工作流支持在相关英特尔平台上高效地部署典型的大语言模型任务，如文本生成、代码生成、完成和摘要。</p><p></p><h2>总结</h2><p></p><p></p><p>上述内容介绍了在英特尔AI硬件产品组合上运行Llama 2的70亿和130亿参数模型推理性能的初始评估，包括Habana®&nbsp;Gaudi®2深度学习加速器、第四代英特尔至强可扩展处理器、英特尔® 至强® CPU Max系列和英特尔数据中心GPU Max系列。我们将继续通过软件发布提供优化，后续会再分享更多关于大语言模型和更大的Llama 2模型的评估。</p>",
    "publish_time": "2023-09-08 16:20:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "PyTorch基金会又添新成员！Graphcore官宣加入，推动AI研究和应用突破",
    "url": "https://www.infoq.cn/article/HYs931qs2GRX92MFxG1S",
    "summary": "<p>近日，Graphcore®（拟未）宣布加入PyTorch基金会（PyTorch Foundation），从而与深度学习社区建立更紧密的联系，更好地开展开源PyTorch框架和生态系统方面的合作。</p><p></p><p>Graphcore于2016年成立于英国布里斯托，目前海外办公室和客户遍布欧洲、亚洲和美洲国家及地区。Graphcore主要为人工智能打造计算机系统，由先进的智能处理器（IPU）提供动力，旨在满足人工智能独特的计算要求。Graphcore的IPU产品在云和数据中心为不同领域的AI应用提供AI计算，包括消费互联网、医疗、金融、科研和汽车等。</p><p>&nbsp;</p><p>PyTorch基金会是深度学习社区在开源PyTorch框架和生态系统上开展合作的中立性场所。PyTorch基金会由其成员和PyTorch开源项目的主要贡献者提供支持。它是全球领先的开源软件、硬件、标准和数据协作组织Linux基金会的一部分。PyTorch基金会利用成员和贡献者提供的资源来实现社区讨论和合作。</p><p></p><p>早在2020年，Graphcore就发布并开源了面向IPU的PyTorch的产品化版本，把先进的人工智能计算平台和创新的框架整合在一起，向PyTorch开发人员开放Graphcore技术。多年来，Graphcore通过开发在其IPU硬件上运行的集成，为PyTorch生态系统做出了贡献。这些集成使研究人员和从业者能够在使用他们首选框架的同时，充分利用Graphcore专用硬件。</p><p></p><p>Graphcore软件框架负责人Anthony Barbier表示：“Graphcore与PyTorch的目标不谋而合，即降低人工智能从业者的入门门槛。通过支持面向IPU的原生PyTorch软件环境，我们将为开发者提供新的底层硬件，这些硬件专为人工智能设计，以帮助解锁可以提高效率或性能的新人工智能技术，并利用他们熟悉和期待的用户友好的PyTorch框架，推动人工智能研究和应用的突破。作为PyTorch基金会的活跃成员，我们期待为全球人工智能社区做出贡献并推动其发展，并为成为先期成员而感到自豪。”</p><p></p><p>PyTorch基金会执行董事Ibrahim Haddad表示：“我们很高兴PyTorch成为Graphcore平台上的领先开发框架。Graphcore在硬件和开源领域发挥了重要作用，我们期待他们继续为PyTorch做出贡献。”</p><p>未来，Graphcore将继续拥抱开源，助力深度学习社区的建设，加速人工智能技术的创新和应用。</p><p></p>",
    "publish_time": "2023-09-08 17:04:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]