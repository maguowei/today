[
  {
    "title": "通过Java来学习Apache Beam",
    "url": "https://www.infoq.cn/article/DNHzwEIkQyJgShdGHz6L",
    "summary": "<p>在本文中，我们将介绍Apache Beam，这是一个强大的批处理和流式处理<a href=\"https://github.com/apache/beam\">开源项目</a>\"，eBay等大公司用它来集成流式处理管道，Mozilla用它来在系统之间安全地移动数据。</p><p></p><h2>概览</h2><p></p><p>Apache Beam是一种处理数据的编程模型，支持批处理和流式处理。</p><p></p><p>你可以使用它提供的Java、Python和Go SDK开发管道，然后选择运行管道的后端。</p><p></p><h2>Apache Beam的优势</h2><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/apache-beam-intro/en/resources/5pasted%20image%200-1654188064991.jpeg\" /></p><p>Beam的编程模型</p><p></p><p>内置的IO连接器Apache Beam连接器可用于从几种类型的存储中轻松提取和加载数据。主要连接器类型有：基于文件的（例如Apache Parquet、Apache Thrift）；文件系统（例如Hadoop、谷歌云存储、Amazon S3）；消息传递（例如Apache Kafka、Google Pub/Sub、Amazon SQS）；数据库（例如Apache Cassandra、Elastic Search、MongoDB）。作为一个OSS项目，对新连接器的支持在不断增长（例如InfluxDB、Neo4J）。可移植性：Beam提供了几个运行管道的Runner，你可以根据自己的场景选择最合适的，并避免供应商锁定。分布式处理后端，如Apache Flink、Apache Spark或Google Cloud Dataflow可以作为Runner。分布式并行处理：默认情况下，数据集的每一项都是独立处理的，因此可以通过并行运行实现优化。开发人员不需要手动分配负载，因为Beam为它提供了一个抽象。</p><p></p><h2>Beam的编程模型</h2><p></p><p>Beam编程模型的关键概念：</p><p></p><p>PCollection：表示数据的集合，如从文本中提取的数字或单词数组。PTransform：一个转换函数，接收并返回一个PCollection，例如所有数字的和。管道：管理PTransform和PCollection之间的交互。PipelineRunner：指定管道应该在哪里以及如何执行。</p><p></p><h2>快速入门</h2><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/apache-beam-intro/en/resources/1figure-4-1654188238779.jpeg\" /></p><p>一个基本的管道操作包括3个步骤：读取、处理和写入转换结果。这里的每一个步骤都是用Beam提供的SDK进行编程式定义的。</p><p></p><p>在本节中，我们将使用Java SDK创建管道。你可以创建一个本地应用程序（使用Gradle或Maven构建），也可以使用<a href=\"https://frontend-beta-dot-apache-beam-testing.appspot.com/\">在线沙盒</a>\"。示例将使用本地Runner，因为这样使用JUnit断言验证结果会更容易些。</p><p></p><h4>Java本地依赖</h4><p></p><p>beam-sdk-java-core：包含所有的Beam模型类。beam-runners-direct-java：默认情况下Beam SDK将直接使用本地Runner，也就是说管道将在本地机器上运行。</p><p></p><h4>乘2操作</h4><p></p><p>在第一个例子中，管道将接收到一个数字数组，并将每个元素乘以2。</p><p></p><p>第一步是创建管道实例，它将接收输入数组并执行转换函数。因为我们使用JUnit运行Beam，所以可以很容易地创建TestPipeline并将其作为测试类的一个字段。如果你更喜欢通过main方法来运行，需要设置<a href=\"https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\">管道配置参数</a>\"。</p><p></p><p><code lang=\"java\">@Rule\npublic final transient TestPipeline pipeline = TestPipeline.create();\n</code></p><p></p><p>现在，我们可以创建作为管道输入的PCollection。它是一个直接在内存中实例化的数组，但它也可以从支持Beam的任何地方读取。</p><p></p><p><code lang=\"java\">PCollection numbers =\n                pipeline.apply(Create.of(1, 2, 3, 4, 5));\n</code></p><p></p><p>然后我们应用我们的转换函数，将每个元素乘以2。</p><p></p><p><code lang=\"java\">PCollection output = numbers.apply(\n                MapElements.into(TypeDescriptors.integers())\n                      .via((Integer number) -&gt; number * 2)\n      );\n</code></p><p></p><p>为了验证结果，我们可以写一个断言。</p><p></p><p><code lang=\"java\">PAssert.that(output)\n                .containsInAnyOrder(2, 4, 6, 8, 10);\n</code></p><p></p><p>注意，结果不排序，因为Beam将每一个元素作为独立的项进行并行处理。</p><p></p><p>测试到这里就完成了，我们通过调用下面的方法运行管道：</p><p></p><p><code lang=\"java\">pipeline.run();\n</code></p><p></p><h4>Reduce操作</h4><p></p><p>Reduce操作将多个输入元素进行聚合，产生一个较小的集合，通常只包含一个元素。</p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/apache-beam-intro/en/resources/1pasted%20image%200-2-1654188064991.jpeg\" /></p><p>MapReduce</p><p></p><p>现在我们来扩展上面的示例，将所有项乘以2后求和，产生一个MapReduce转换操作。</p><p></p><p>每一个PCollection转换都会产生一个新的PCollection实例，这意味着我们可以使用apply方法将转换链接起来。对于这个示例，将在每个元素乘以2后使用Sum操作：</p><p></p><p><code lang=\"java\">PCollection numbers =\n            pipeline.apply(Create.of(1, 2, 3, 4, 5));\n\nPCollection output = numbers\n            .apply(\n                   MapElements.into(TypeDescriptors.integers())\n                       .via((Integer number) -&gt; number * 2))\n            .apply(Sum.integersGlobally());\n\nPAssert.that(output)\n             .containsInAnyOrder(30);\n\npipeline.run();\n</code></p><p></p><h4>FlatMap操作</h4><p></p><p>FlatMap先对每个输入元素应用映射，返回一个新集合，从而产生一个集合的集合。然后再应用Flat操作将所有嵌套的集合合并，最终生成一个集合。</p><p></p><p>下一个示例将把字符串数组转换成包含唯一性单词的数组。</p><p></p><p>首先，我们声明将作为管道输入的单词列表：</p><p></p><p><code lang=\"java\">final String[] WORDS_ARRAY = new String[] {\n          \"hi bob\", \"hello alice\", \"hi sue\"};\n\nfinal List WORDS = Arrays.asList(WORDS_ARRAY);\n</code></p><p></p><p>然后，我们使用上面的列表创建输入PCollection：</p><p></p><p><code lang=\"plain\">PCollection input = pipeline.apply(Create.of(WORDS));\n</code></p><p></p><p>现在，我们进行FlatMap转换，它将拆分每个嵌套数组中的单词，并将结果合并成一个列表：</p><p></p><p><code lang=\"java\">PCollection output = input.apply(\n      FlatMapElements.into(TypeDescriptors.strings())\n            .via((String line) -&gt; Arrays.asList(line.split(\" \")))\n);\n\nPAssert.that(output)\n      .containsInAnyOrder(\"hi\", \"bob\", \"hello\", \"alice\", \"hi\", \"sue\");\n\npipeline.run();\n</code></p><p></p><h4>Group操作</h4><p></p><p>数据处理的一个常见的任务是根据特定的键进行聚合或计数。我们将计算上一个例子中每个单词出现的次数。</p><p></p><p>在有了扁平的字符串数组之后，我们可以链接另一个PTransform：</p><p></p><p><code lang=\"java\">PCollection&gt; output = input\n            .apply(\n                  FlatMapElements.into(TypeDescriptors.strings())\n                      .via((String line) -&gt; Arrays.asList(line.split(\" \")))\n            )\n            .apply(Count.perElement());\n</code></p><p></p><h4>产生结果：</h4><p></p><p><code lang=\"java\">PAssert.that(output)\n.containsInAnyOrder(\n       KV.of(\"hi\", 2L),\n       KV.of(\"hello\", 1L),\n       KV.of(\"alice\", 1L),\n       KV.of(\"sue\", 1L),\n       KV.of(\"bob\", 1L));\n</code></p><p></p><h4>从文件中读取</h4><p></p><p>Beam的一个原则是可以从任何地方读取数据，所以我们来看看在实际当中如何使用文本文件作为数据源。</p><p></p><p>下面的示例将读取包含“An advanced unified programming model”文本的文件“words.txt”。然后转换函数将返回一个包含每一个单词的PCollection。</p><p></p><p><code lang=\"java\">PCollection input =\n      pipeline.apply(TextIO.read().from(\"./src/main/resources/words.txt\"));\n\nPCollection output = input.apply(\n      FlatMapElements.into(TypeDescriptors.strings())\n      .via((String line) -&gt; Arrays.asList(line.split(\" \")))\n);\n\nPAssert.that(output)\n      .containsInAnyOrder(\"An\", \"advanced\", \"unified\", \"programming\", \"model\");\n\npipeline.run();\n</code></p><p></p><h4>将结果写入文件</h4><p></p><p>从前面的输入示例可以看到，Beam提供了多个内置的输出连接器。在下面的例子中，我们将计算文本文件“words.txt”（只包含一个句子“An advanced unified programming model\"）中出现的每个单词的数量，输出结果将写入一个文本文件。</p><p></p><p><code lang=\"java\">PCollection input =\n      pipeline.apply(TextIO.read().from(\"./src/main/resources/words.txt\"));\n\nPCollection&gt; output = input\n      .apply(\n            FlatMapElements.into(TypeDescriptors.strings())\n                 .via((String line) -&gt; Arrays.asList(line.split(\" \")))\n            )\n            .apply(Count.perElement());;\n\n       PAssert.that(output)\n             .containsInAnyOrder(\n                 KV.of(\"An\", 1L),\n                 KV.of(\"advanced\", 1L),\n                 KV.of(\"unified\", 1L),\n                 KV.of(\"programming\", 1L),\n                 KV.of(\"model\", 1L)\n            );\n\n      output\n             .apply(\n                   MapElements.into(TypeDescriptors.strings())\n                         .via((KV kv) -&gt; kv.getKey() + \" \" + kv.getValue()))\n             .apply(TextIO.write().to(\"./src/main/resources/wordscount\"));\n\n      pipeline.run();\n</code></p><p></p><p>默认情况下，文件写入也针对并行性进行了优化，这意味着Beam将决定保存结果的最佳分片（文件）数量。这些文件位于src/main/resources文件夹中，文件名包含了前缀“wordcount”、碎片序号和碎片总数。</p><p></p><p>在我的笔记本电脑上运行它生成了4个分片：</p><p></p><p>第一个分片（文件名：wordscount-00001-of-00003）：</p><p></p><p><code lang=\"java\">An 1\nadvanced 1\n</code></p><p></p><p>第二个分片（文件名：wordscount-00002-of-00003）：</p><p></p><p><code lang=\"java\">unified 1\nmodel 1\n</code></p><p></p><p>第三个分片（文件名：wordscount-00003-of-00003）：</p><p></p><p><code lang=\"java\">programming 1\n</code></p><p></p><p>最后一个分片是空的，因为所有的单词都已经被处理完了。</p><p></p><h4>扩展Beam</h4><p></p><p>我们可以通过编写自定义转换函数来扩展Beam。自定义转换器将提高代码的可维护性，并消除重复工作。</p><p></p><p>基本上，我们需要创建一个PTransform的子类，将输入和输出的类型声明为Java泛型。然后重写expand方法，加入我们的逻辑，它将接受单个字符串并返回包含每个单词的PCollection。</p><p></p><p><code lang=\"java\">public class WordsFileParser extends PTransform, PCollection&gt; {\n\n     @Override\n     public PCollection expand(PCollection input) {\n       return input\n                .apply(FlatMapElements.into(TypeDescriptors.strings())\n                  .via((String line) -&gt; Arrays.asList(line.split(\" \")))\n                );\n     }   \n}\n</code></p><p></p><p>用WordsFileParser来重构测试场景就变成了：</p><p></p><p><code lang=\"java\">public class FileIOTest {\n\n    @Rule\n    public final transient TestPipeline pipeline = TestPipeline.create();\n\n    @Test\n    public void testReadInputFromFile() {\n          PCollection input =\n                          pipeline.apply(TextIO.read().from(\"./src/main/resources/words.txt\"));\n\n          PCollection output = input.apply(\n                      new WordsFileParser()\n          );\n\n          PAssert.that(output)\n                      .containsInAnyOrder(\"An\", \"advanced\", \"unified\", \"programming\", \"model\");\n\n          pipeline.run();\n    }\n\n    @Test\n    public void testWriteOutputToFile() {\n          PCollection input =\n             pipeline.apply(TextIO.read().from(\"./src/main/resources/words.txt\"));\n\n          PCollection&gt; output = input\n                      .apply(new WordsFileParser())\n                      .apply(Count.perElement());\n\n          PAssert.that(output)\n                      .containsInAnyOrder(\n                            KV.of(\"An\", 1L),\n                            KV.of(\"advanced\", 1L),\n                            KV.of(\"unified\", 1L),\n                            KV.of(\"programming\", 1L),\n                            KV.of(\"model\", 1L)\n                      );\n\n           output\n                      .apply(\n                            MapElements.into(TypeDescriptors.strings())\n                            .via((KV kv) -&gt; kv.getKey() + \" \" + kv.getValue()))\n                      .apply(TextIO.write().to (\"./src/main/resources/wordscount\"));\n\n       pipeline.run();\n  }\n}\n</code></p><p></p><p>结果变成了更清晰和更模块化的管道。</p><p></p><h4>时间窗口</h4><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/apache-beam-intro/en/resources/1pasted%20image%200-3-1654188064991.jpeg\" /></p><p>Beam的时间窗口</p><p></p><p>流式处理中一个常见的问题是将传入的数据按照一定的时间间隔进行分组，特别是在处理大量数据时。在这种情况下，分析每小时或每天的聚合数据比分析数据集的每个元素更有用。</p><p></p><p>在下面的例子中，我们将假设我们身处金融科技领域，我们正在接收包含金额和交易时间的事件，我们希望获取每天的交易总额。</p><p></p><p>Beam提供了一种用时间戳来装饰每个PCollection元素的方法。我们可以通过这种方式创建一个代表5笔交易的PCollection：</p><p></p><p>金额10和20是在2022年02月01日转账的；金额30、40和50是在2022年02月05日转账的。</p><p></p><p><code lang=\"java\">PCollection transactions =\n      pipeline.apply(\n            Create.timestamped(\n                  TimestampedValue.of(10, Instant.parse(\"2022-02-01T00:00:00+00:00\")),\n                  TimestampedValue.of(20, Instant.parse(\"2022-02-01T00:00:00+00:00\")),\n                  TimestampedValue.of(30, Instant.parse(\"2022-02-05T00:00:00+00:00\")),\n                  TimestampedValue.of(40, Instant.parse(\"2022-02-05T00:00:00+00:00\")),\n                  TimestampedValue.of(50, Instant.parse(\"2022-02-05T00:00:00+00:00\"))\n               )\n       );\n</code></p><p></p><p>接下来，我们将应用两个转换函数：</p><p></p><p>使用一天的时间窗口对交易进行分组；把每组的数量加起来。</p><p></p><p><code lang=\"java\">PCollection output =\n      Transactions\n             .apply(Window.into(FixedWindows.of(Duration.standardDays(1))))\n             .apply(Combine.globally(Sum.ofIntegers()).withoutDefaults());\n</code></p><p></p><p>在第一个时间窗口（2022-02-01）中，预计总金额为30（10+20），而在第二个窗口（2022-02-05）中，我们应该看到总金额为120（30+40+50）。</p><p></p><p><code lang=\"java\">PAssert.that(output)\n                   .inWindow(new IntervalWindow(\n                       Instant.parse(\"2022-02-01T00:00:00+00:00\"),\n                       Instant.parse(\"2022-02-02T00:00:00+00:00\")))\n                 .containsInAnyOrder(30);\n\nPAssert.that(output)\n                 .inWindow(new IntervalWindow(\n                        Instant.parse(\"2022-02-05T00:00:00+00:00\"),\n                        Instant.parse(\"2022-02-06T00:00:00+00:00\")))\n                 .containsInAnyOrder(120);\n</code></p><p></p><p>每个IntervalWindow实例需要匹配所选时间段的确切开始和结束时间戳，因此所选时间必须是“00:00:00”。</p><p></p><h2>总结</h2><p></p><p>Beam是一个强大的经过实战检验的数据框架，支持批处理和流式处理。我们使用Java SDK进行了Map、Reduce、Group和时间窗口等操作。</p><p></p><p>Beam非常适合那些执行并行任务的开发人员，可以简化大规模数据处理的机制。</p><p></p><p>它的连接器、SDK和对各种Runner的支持为我们带来了灵活性，你只要选择一个原生Runner，如Google Cloud Dataflow，就可以实现计算资源的自动化管理。</p><p></p><p>作者简介</p><p></p><p>Fabio Hiroki是一位在Mollie公司从事金融服务的软件工程师。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.infoq.com/articles/apache-beam-intro/\">Introduction to Apache Beam Using Java</a>\"</p>",
    "publish_time": "2022-06-28 09:01:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "规避代码被“投毒”，开源软件供应链安全面面观 | InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/EgmPvTu4Ik9xeFJ8qToL",
    "summary": "<p>开源软件供应链当前面临的主要是两大风险：一是安全风险，一是许可证、版权、专利和出口管制等方面的法律合规风险。当然针对使用开源软件的企业来说，还有供应链风险及运维风险。这些风险如何更好地规避？开源协议、开源组织都做了哪些事情？企业如何自查内部开源项目的安全性？本期《极客有约》，我们邀请到了Zilliz合伙人，首席布道师顾钧，上海安势信息技术有限公司资深解决方案架构师朱贤曼共同解答上述问题。</p>\n<p><strong>直播大纲：</strong></p>\n<p>1.开源软件供应链解读；<br />\n2.常见的针对开源软件 - 供应链的攻击类型；<br />\n3.企业如何自查安全性；</p>\n<p><strong>讲师介绍：</strong></p>\n<p>顾钧，Zilliz 合伙人、首席布道师，LF AI&amp;Data 基金会 TAC 成员，开放原子基金会开源导师。北大毕业 16 年以来专注于数据库、大数据技术，尤其对 OLTP 平台与场景有着丰富的经验，先后任职于工商银行、IBM、摩根士丹利、华为等企业。</p>\n<p>朱贤曼，上海安势信息技术有限公司资深解决方案架构师，十余年软件开发经验，先后从事出口管制合规、合规相关系统设计和实施、开源软件合规等工作。</p>",
    "publish_time": "2022-06-28 09:13:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "亚马逊云科技发布架构决策记录指南",
    "url": "https://www.infoq.cn/article/6aLhRfRH7BPfGL3del9I",
    "summary": "<p>亚马逊云科技发布了<a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/architectural-decision-records/welcome.html\">使用架构决策记录（architecture decision record，ADR）的指南</a>\"。他们推荐了一个在软件工程团队中采用和审查ADR的过程，这个过程的结果是包含已批准、已拒绝和已废弃的ADR集合的决策记录。</p><p></p><p>亚马逊云科技提出该ADR过程的目的是改善架构决策，避免对相同主题的重复性讨论，并有效地对决策进行沟通。</p><p></p><p>ADR是一个简短的文档，描述了会影响软件架构的团队决策。它不仅包含决策，还包含了相关的背景和影响。一组ADR组成了一个决策日志，它提供了关于项目或产品的更广泛的背景、设计信息和实现细节。</p><p></p><p>ADR过程中，最常见的输入是需要在架构方面进行重大决策的功能性或非功能性需求。发现了这种决策的任何团队成员都应该创建一个ADR。使用<a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/architectural-decision-records/appendix.html\">模板</a>\"可以简化ADR的创建，并且能够确保它会捕获所有相关的信息。</p><p></p><p>按照亚马逊云科技的指南，创建ADR的团队成员也是该ADR的所有者，要负责维护和交流它的内容。在初始阶段，ADR所有者会提供一个“proposed”状态的ADR，这意味着它可以进行审查了。随后，ADR所有者要安排一个团队会议，以审查并决定该ADR要被批准、返工还是拒绝。</p><p></p><p>如果团队发现该ADR需要改进的话，它会依然保持“proposed”状态，所有者和其他团队成员会对其进行优化。否则的话，ADR的状态将会变为“accepted”或“rejected”，ADR就不可改变了。如果团队需要更新这个决策的话，那应该提出一个新的ADR，当该ADR被批准后，会取代之前的ADR。</p><p></p><p>下图展示了ADR的创建、所有权和采用的过程。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/06/aws-adr-guide/en/resources/1adr-creation-1654293535205.png\" /></p><p></p><p>图片来源：<a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/architectural-decision-records/adr-process.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/architectural-decision-records/adr-process.html</a>\"</p><p></p><p>亚马逊云科技建议，ADR应该有一个变更历史。一旦ADR被批准或拒绝，它就应该被认为是不可改变的。如果团队批准了一个新的ADR，并且该ADR取代或更新了以前的决策，ADR的所有者应该将旧ADR的状态变更为“superseded”。如果新的ADR 被拒绝了，则不需要对旧的ADR进行任何改变。</p><p></p><p>下图显示了ADR的更新过程。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/06/aws-adr-guide/en/resources/1adr-inspection-1654293535205.png\" /></p><p></p><p>图片来源: <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/architectural-decision-records/adr-process.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/architectural-decision-records/adr-process.html</a>\"</p><p></p><p>决策日志会随着时间的推移而增长，它会提供团队所做出的所有决策的历史。例如，在代码或架构审查期间，团队可以使用决策日志作为参考，以验证变更是否符合商定的决策，或者是否需要创建一个新的ADR。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/06/aws-adr-guide/\">AWS Publishes Guide to Architecture Decision Records</a>\"</p>",
    "publish_time": "2022-06-28 09:27:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]