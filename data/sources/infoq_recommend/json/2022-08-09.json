[
  {
    "title": "Java近期新闻：Helidon 3.0、GraalVM 22.2、IntelliJ IDEA 2022.2、Vert.x虚拟线程",
    "url": "https://www.infoq.cn/article/2xleK5MaFNdeAU94cFRt",
    "summary": "<p>本期Java近期新闻主要涉及OpenJDK、JDK 19、JDK 20、Spring项目升级、Helidon 3.0、GraalVM 22.2、Quarkus 2.11.1和2.10.4、Micronaut 3.5.4、Eclipse Vert.x虚拟线程孵化器、Jakarta EE 10升级、IntelliJ IDEA 2022.2、JUnit 5.9.0、Apache软件基金会项目升级和Multik 0.2.0。</p><p></p><h4>OpenJDK</h4><p></p><p>JEP草案8285724——<a href=\"https://openjdk.org/jeps/8285724\">弃用JMX M-Lets（管理小程序）以备删除</a>\"——是一份特性JEP。它提议弃用Java Management eXtension（JMX） “M-Let”特性，以备在将来的版本中删除，因为它已经过时，不符合现代应用程序开发的潮流了。删除M-Lets（最初受小程序启发）以及javax.management.loading API不会影响JMX及其相关技术。</p><p></p><h4>JDK 19</h4><p></p><p>JDK 19<a href=\"https://jdk.java.net/19\">早期访问构建</a>\"的Build 33在上周发布，它是Build 32的<a href=\"https://github.com/openjdk/jdk/compare/jdk-19%2B32...jdk-19%2B33\">升级</a>\"，修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2019%20and%20%22resolved%20in%20build%22%20%3D%20b33%20order%20by%20component%2C%20subcomponent\">问题</a>\"。要了解更多细节信息，请查看<a href=\"https://jdk.java.net/19/release-notes\">发布说明</a>\"。</p><p></p><h4>JDK 20</h4><p></p><p>JDK 20<a href=\"https://jdk.java.net/20/\">早期访问构建</a>\"的<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B8\">Build 8</a>\"在上周发布，它是Build 7的<a href=\"https://github.com/openjdk/jdk/compare/jdk-20%2B7...jdk-20%2B8\">升级</a>\"，修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2020%20and%20%22resolved%20in%20build%22%20%3D%20b08%20order%20by%20component%2C%20subcomponent\">问题</a>\"。发布说明尚未提供。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/19\">JDK 19</a>\"和<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"，我们鼓励开发者通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告缺陷。</p><p></p><h4>Spring Framework</h4><p></p><p><a href=\"https://spring.io/projects/spring-shell\">Spring Shell</a>\"&nbsp;2.1.0<a href=\"https://spring.io/blog/2022/07/25/spring-shell-2-1-0-is-now-available\">发布</a>\"，特性包括：新增接口CommandRegistration ，提供了一种通过编程方式定义命令的新方法；改造Spring Shell 内部构件，为即将发布的Spring Framework 6.0 GA及Spring Boot 3.0 GA提供初始支持；重新评估@ShellMethod&nbsp;和@ShellOption 注解，要想更好地匹配CommandRegistration 接口可能需要新的注解。要了解关于这个版本的详细信息，请查看<a href=\"https://github.com/spring-projects/spring-shell/releases/tag/v2.1.0\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-cloud-openfeign\">Spring Cloud OpenFeign</a>\"&nbsp;3.0.8<a href=\"https://spring.io/blog/2022/07/27/spring-cloud-openfeign-3-0-8-is-now-available\">发布</a>\"，主要是Bug修复和文档发布，其中修复补丁是从3.1.x发布序列反向移植过来的。最值得注意的反向移植和<a href=\"https://spring.io/projects/spring-data\">Spring Data</a>\"接口Page 中的<a href=\"https://github.com/spring-cloud/spring-cloud-openfeign/pull/653\">级联反序列化</a>\"有关。</p><p>&nbsp;</p><p>自2020年4月<a href=\"https://spring.io/blog/2020/04/15/announcing-the-spring-authorization-server\">首次亮相</a>\"以来，<a href=\"https://spring.io/projects/spring-authorization-server\">Spring Authorization Server</a>\"&nbsp;团队已经<a href=\"https://spring.io/blog/2022/07/28/spring-authorization-server-is-going-1-0\">宣布</a>\"，他们正在准备按计划将于2022年11月发布的1.0版本。该版本将以Spring Security 6.0 和Spring Framework 6.0为基础，所需的最低软件版本为JDK 17、Tomcat 10和Jetty 11。该团队还将发布一个0.4.0版本，以支持<a href=\"https://spring.io/projects/spring-security\">Spring Security</a>\"&nbsp;5.x 发布序列和JDK 8。如果想了解从现在起到2022年11月期间，该团队将为开发人员带来什么，可以查看<a href=\"https://github.com/spring-projects/spring-authorization-server/milestones\">发布计划</a>\"和<a href=\"https://docs.spring.io/spring-authorization-server/docs/current/reference/html/overview.html#feature-list\">特性清单</a>\"。InfoQ 后续将带来更详细的报道。</p><p></p><h4>Helidon</h4><p></p><p>在Helidon 2.0发布两年之后，Oracle<a href=\"https://medium.com/helidon/helidon-3-0-is-released-1bd2df1f999b\">发布</a>\"了Helidon 3.0，特性包括：最低支持JDK 17；一个MicroProfile 5.0实现和精选的Jakarta EE 9.1规范；支持JEP 290（<a href=\"https://openjdk.org/jeps/290\">过滤传入的序列化数据</a>\"），这样，反序列化就默认禁用了；升级Helidon SE路由API；一个新的<a href=\"https://helidon.io/starter/3.0.0\">项目启动器</a>\"；经过升级的CLI。要了解关于这个版本的详细信息，请查看<a href=\"https://github.com/oracle/helidon/releases/tag/3.0.0\">发布说明</a>\"。InfoQ 后续将带来更详细的报道。</p><p></p><h4>GraalVM</h4><p></p><p>Oracle 实验室<a href=\"https://medium.com/graalvm/graalvm-22-2-smaller-jdk-size-improved-memory-usage-better-library-support-and-more-cb34b5b68ec0\">发布</a>\"了GraalVM 22.2，特性包括：一个更小的GraalVM JDK分发，更加模块化，也不再包含JavaScript 运行时LLVM或VisualVM；改进Native Image使用第三方库的方式，减少内存占用和堆转储；<a href=\"https://www.graalvm.org/python/\">GraalPython</a>\"的快速启动和扩展库支持；改进与<a href=\"https://www.graalvm.org/22.1/reference-manual/js/\">GraalJS</a>\"的互操作性。GraalVM 22.2随JDK 11 和JDK 17构建提供。要了解关于这个版本的详细信息，请观看这个YouTube<a href=\"https://youtu.be/rpZJz4qbhCU\">视频</a>\"。InfoQ 后续将带来更详细的报道。</p><p></p><h4>Quarkus</h4><p></p><p>红帽公司<a href=\"https://quarkus.io/blog/quarkus-2-11-1-final-released/\">发布</a>\"了Quarkus 2.11.1.Final 和2.10.4.Final。两个版本都已经解决了<a href=\"https://bugzilla.redhat.com/show_bug.cgi?id=2108396\">CVE-2022-2466</a>\"，这是在服务器扩展<a href=\"https://smallrye.io/smallrye-graphql/\">SmallRye GraphQL</a>\"&nbsp;中发现的一个漏洞，服务器请求在其中没有正确终止。发布公告这样写道：</p><p></p><blockquote>遗憾的是，之前在2.10.3.Final和未发布版本2.11.0.Final中引入的修复并不彻底，问题依然以另外一种形式存在。</blockquote><p></p><p>其他新特性包括：新的Redis客户端API；依赖项升级到Vert.x 4.3.2 和Netty 4.1.78；GraphQL 端点经修改后默认为单例；原生可执行文件生成默认采用基于JDK 17的构建器镜像。要了解关于这些版本的详细信息，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.11.1.Final\">2.11.1</a>\"和<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.10.4.Final\">2.10.4</a>\"版本的发布说明。</p><p></p><h4>Micronaut</h4><p></p><p>Micronaut 基金会<a href=\"https://micronaut.io/2022/07/29/micronaut-framework-3-5-4-released/\">发布</a>\"了Micronaut 3.5.4，提供了Bug修复和多个Micronaut 模块的补丁版本，包括：<a href=\"https://github.com/micronaut-projects/micronaut-security/releases/tag/v3.6.3\">Micronaut Security 3.6.3</a>\"、&nbsp;<a href=\"https://github.com/micronaut-projects/micronaut-aws/releases/tag/v3.5.3\">Micronaut AWS 3.5.3</a>\"、&nbsp;<a href=\"https://github.com/micronaut-projects/micronaut-rxjava2/releases/tag/v1.2.2\">Micronaut RxJava 2 1.2.2</a>\"、&nbsp;<a href=\"https://github.com/micronaut-projects/micronaut-gcp/releases/tag/v4.2.1\">Micronaut GCP 4.2.1</a>\"和<a href=\"https://github.com/micronaut-projects/micronaut-reactor/releases/tag/v2.2.3\">Micronaut Reactor 2.2.3</a>\"。要了解关于这个版本的详细信息，请查看<a href=\"https://github.com/micronaut-projects/micronaut-core/releases/tag/v3.5.4\">发布说明</a>\"。</p><p></p><h4>Eclipse Vert.x</h4><p></p><p>由于即将发布的JDK 19将支持虚拟线程，所以Vert.x团队<a href=\"https://vertx.io/blog/vertx-virtual-threads-incubator/\">创建</a>\"了一个<a href=\"https://github.com/vert-x3/vertx-virtual-threads-incubator\">虚拟线程孵化器</a>\"项目，供开发人员试用虚拟线程并提供必要的反馈。目前，该孵化器项目包含一个async/await<a href=\"https://github.com/vert-x3/vertx-virtual-threads-incubator/tree/main/vertx-async-await-incubator\">实现</a>\"，其基础是Axoni软件工程师<a href=\"https://www.linkedin.com/in/augustnagro/\">August Nagro</a>\"的概念验证。该项目旨在提供一个中心化社区，让人们可以试验虚拟线程，而且也可以托管任何其他基于虚拟线程的项目。</p><p></p><h4>Jakarta EE 10</h4><p></p><p>在通往<a href=\"https://jakarta.ee/specifications/\">Jakarta EE 10</a>\"的道路上， Jakarta EE 规范委员会上周<a href=\"https://www.eclipse.org/lists/jakarta.ee-spec/msg02675.html\">发起了一项投票</a>\"，为的是使Jakarta EE 10<a href=\"https://jakarta.ee/specifications/platform/10/\">平台</a>\"配置文件获得正式批准。该投票计划于2022年8月9日关闭。<a href=\"https://jakarta.ee/specifications/webprofile/10/\">Web</a>\"和<a href=\"https://jakarta.ee/specifications/coreprofile/10/\">Core</a>\"配置文件会安排单独的投票。</p><p></p><h4>JetBrains</h4><p></p><p>JetBrains <a href=\"https://blog.jetbrains.com/idea/2022/07/intellij-idea-2022-2/\">发布</a>\"了IntelliJ IDEA 2022.2，新特性包括：从<a href=\"https://github.com/JetBrains/JetBrainsRuntime/blob/main/README.md\">JetBrains Runtime</a>\"（JBR）11迁移到JBR17；远程开发<a href=\"https://blog.jetbrains.com/idea/2022/06/intellij-idea-2022-2-eap-7/\">改进</a>\"；支持Spring Framework 6.0 和Spring Boot 3.0；一个实验性的GraalVM Java原生调试器；JSON、YAML及.properties 字符串值中的可点击URL。</p><p>&nbsp;</p><p>JetBrains 还<a href=\"https://blog.jetbrains.com/kotlin/2022/07/multik-0-2-multiplatform-with-support-for-android-and-apple-silicon/\">发布</a>\"了<a href=\"https://github.com/Kotlin/multik/blob/develop/README.md\">Multik</a>\"的0.2版本，一个用于Kotlin的多维数组库。这是自2021年11月<a href=\"https://github.com/Kotlin/multik/releases/tag/v0.1.1\">0.1.1版本</a>\"以来的第一次发布，这个新版本中的新特性包括：一个新的多平台结构；支持Android 和Apple Silicon处理器；改进随机数、范数矩阵、复数等的操作。要了解关于这个版本的详细信息，请查看<a href=\"https://github.com/Kotlin/multik/releases/tag/V0.2.0\">发布说明</a>\"。</p><p></p><h4>JUnit</h4><p></p><p>JUnit 5.9.0<a href=\"https://junit.org/junit5/docs/5.9.0/release-notes/\">发布</a>\"，新特性包括：支持<a href=\"https://github.com/ota4j-team/open-test-reporting\">Open Test Reporting</a>\"格式；ConfigurationParameters 接口新增方法keySet() ，可检索所有的配置参数键；@Suite 注解接口新增failIfNoTests 属性，如果没有发现测试程序，则测试套件失败。要了解关于这个版本的详细信息，请查看<a href=\"https://junit.org/junit5/docs/5.9.0/release-notes/#release-notes-5.9.0\">发布说明</a>\"。InfoQ 后续将带来更详细的报道。</p><p></p><h4>Apache软件基金会</h4><p></p><p>Apache软件基金会发布了<a href=\"https://quarkus.io/guides/camel\">Camel Quarkus</a>\"、<a href=\"https://tomcat.apache.org/\">Tomcat</a>\"&nbsp;和<a href=\"https://groovy-lang.org/\">Groovy</a>\"的点版本。</p><p>&nbsp;</p><p>为了与Quarkus保持一致，Camel Quarkus 2.11.0（包括Camel 3.18.0 和Quarkus 2.11.1.Final）的<a href=\"https://camel.apache.org/blog/2022/07/camel-quarkus-release-2.11.0/\">新特性包括</a>\"：支持Camel Hashicorp Vault和DataSet扩展；JAXB 扩展测试覆盖率提升；修复了bean内省在@Singleton 作用域bean中不起作用的问题。要了解关于这个版本的详细信息，请查看<a href=\"https://github.com/apache/camel-quarkus/milestone/29?closed=1\">问题列表</a>\"。</p><p>&nbsp;</p><p>Tomcat 10.0.23的<a href=\"https://www.mail-archive.com/announce@apache.org/msg07469.html\">新特性</a>\"包括：修复<a href=\"https://nvd.nist.gov/vuln/detail/CVE-2022-34305\">CVE-2022-34305</a>\"，这是表单验证示例中的一个低风险XSS 漏洞；支持可重复构建；<a href=\"https://tomcat.apache.org/native-doc/\">Tomcat Native Library</a>\"&nbsp;软件包的版本升级到1.2.35，其中包括使用OpenSSL 1.1.1q构建的Windows 二进制文件。要了解关于这个版本的详细信息，请查看<a href=\"https://tomcat.apache.org/tomcat-10.0-doc/changelog.html\">变更日志</a>\"。</p><p>&nbsp;</p><p>Apache Groovy <a href=\"https://www.mail-archive.com/announce@apache.org/msg07466.html\">4.0.4</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg07465.html\">3.0.12</a>\"&nbsp;及<a href=\"https://www.mail-archive.com/announce@apache.org/msg07464.html\">2.5.18</a>\"版本的新特性包括Bug修复、改进和依赖项升级，如<a href=\"https://spotbugs.github.io/\">Spotbugs</a>\"&nbsp;4.7.1、log4j2 2.18.0 和Ant 1.9.16。要了解关于这些版本的详细信息，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12351811\">4.0.4</a>\"、<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12351799\">3.0.12</a>\"&nbsp;和<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12351798\">2.5.18</a>\"的发布说明。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/08/java-news-roundup-jul25-2022/\">Java News Roundup: Helidon 3.0, GraalVM 22.2, IntelliJ IDEA 2022.2, Vert.x Virtual Threads</a>\"</p>",
    "publish_time": "2022-08-09 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "虎牙在直播场景下对音频内容审核的探索｜InfoQ大会早班车第17期",
    "url": "https://www.infoq.cn/article/JmZDlsHvKV5zRQCditwi",
    "summary": "<p>随着直播场景越来越丰富，直播中对一些敏感音频、版权歌曲的检测和审核问题一直困扰着平台和开发者。本次大会早班车邀请到 2 位虎牙直播的老师，将针对直播内容审核流程、音频审核算法模块及主要场景和大家聊一聊。</p>\n<p>2022年QCon北京站即将落地，[点击直达官网]<br />\n(<a href=\"https://qcon.infoq.cn/2022/beijing/?utm_source=infoq&amp;utm_medium=zaobanche&amp;utm_campaign=full&amp;utm_term=0722\">https://qcon.infoq.cn/2022/beijing/?utm_source=infoq&amp;utm_medium=zaobanche&amp;utm_campaign=full&amp;utm_term=0722</a>)。</p>",
    "publish_time": "2022-08-09 09:42:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "5亿用户如何高效沟通？钉钉首次对外揭秘即时消息服务DTIM",
    "url": "https://www.infoq.cn/article/NfyEB9CF6qL3MQO8CfUL",
    "summary": "<p></p><blockquote>这是钉钉第一次对外揭秘 DTIM(DingTalk IM，钉钉即时消息服务)。我们从设计原理到技术架构、从最底层存储模型到跨地域的单元化，全方位地展现了 DTIM 在实际生产中遇到各种挑战与解决方案，期望为企业级 IM 的建设贡献一臂之力。</blockquote><p></p><p></p><p>钉钉已经有 2100 万 + 组织、5 亿 + 注册用户在使用。DTIM 为钉钉用户提供即时消息服务，用于组织内外的沟通，这些组织包括公司、政府、学校等，规模从几人到百万人不等。DTIM 有着丰富的功能，单聊、各种类型的群聊、消息已读、文字表情、多端同步、动态卡片、专属安全和存储等等。同时钉钉内部很多业务模块，比如文档、钉闪会、Teambition、音视频、考勤、审批等，每个业务都在使用 DTIM，用于实现业务流程通知、运营消息推送、业务信令下发等。每个业务模块对于 DTIM 调用的流量峰值模型各有差别，对可用性要求也不尽相同。DTIM 需要能够面对这些复杂的场景，保持良好的可用性和体验，同时兼顾性能与成本。</p><p></p><p>通用的即时消息系统对消息发送的成功率、时延、到达率有很高的要求，企业 IM 由于 ToB 的特性，在数据安全可靠、系统可用性、多终端体验、开放定制等多个方面有着极致的要求。构建稳定高效的企业 IM 服务，DTIM 主要面临的挑战是：</p><p></p><p>企业 IM 极致的体验要求对于系统架构设计的挑战，比如数据长期保存可漫游、多端数据同步、动态消息等带来的数据存储效率和成本压力，多端数据同步带来的一致性问题等；极限场景冲击、依赖系统错误带来的可用性问题：比如超大群消息，突发疫情带来的线上办公和线上教学高并发流量，系统需要能够应对流量的冲击，保障高可用；同时在中间件普遍可用性不到 99.99% 的时候，DTIM 服务需要保障核心功能的 99.995% 的可用性；不断扩大的业务规模，对于系统部署架构的挑战：比如持续增长的用户规模，突发事件如席卷全球的疫情，单地域架构已经无法满足业务发展的要求。</p><p></p><p>DTIM 在系统设计上，为了实现消息收发体验、性能和成本的平衡，设计了高效的读写扩散模型和同步服务，以及定制化的 NoSQL 存储。通过对 DTIM 服务流量的分析，对于大群消息、单账号大量的消息热点以及消息更新热点的场景进行了合并、削峰填谷等处理；同时核心链路的应用中间件的依赖做容灾处理，实现了单一中间件失败不影响核心消息收发，保障基础的用户体验。</p><p></p><p>在消息存储过程中，一旦出现存储系统写入异常，系统会回旋缓冲重做，并且在服务恢复时，数据能主动向端上同步。随着用户数不断增长，单一地域已无法承载 DTIM 的容量和容灾需求，DTIM 实现了异地多单元的云原生的弹性架构。在分层上遵从的原则为重云轻端：业务数据计算、存储、同步等复杂操作尽量后移到云端处理，客户端只做终态数据的接收、展示，通过降低客户端业务实现的复杂度，最大化地提升客户端迭代速度，让端上开发可以专注于提升用户的交互体验，所有的功能需求和系统架构都围绕着该原则做设计和扩展。</p><p></p><p>以下我们将对 DTIM 做更加详细的介绍。在第 1 章，我们介绍了 DTIM 的核心模型设计；第 2 章介绍了针对 IM 特点将计算下沉和与之对应的优化点；在第 3 章中介绍了同步服务，实现 IM 和其他业务的多端同步能力；在第 4 章主要介绍高可用的设计，首先是系统自我防护，之后是系统的弹性扩展能力与异地容灾设计。</p><p></p><p></p><h2>模型设计</h2><p></p><p></p><p>低延迟、高触达、高可用一直是 DTIM 设计的第一原则，依据这个原则在架构上 DTIM 将系统拆分为三个服务做能力的承载：</p><p></p><p>消息服务：负责 IM 核心消息模型和开放 API，IM 基础能力包括消息发送、单聊关系维护、群组元信息管理、历史消息拉取、已读状态通知、IM 数据存储以及跨地域的流量转发。同步服务：负责用户消息数据以及状态数据的端到端同步，通过客户端到服务端长连接通道做实时的数据交互，当钉钉各类设备在线时 IM 及上游各业务通过同步服务做多端的数据同步，保障各端数据和体验一致。通知服务：负责用户第三方通道维护以及通知功能，当钉钉的自建通道无法将数据同步到端上时，通过三方提供的通知和透传能力做消息推送，保障钉钉消息的及时性和有效性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d2/d282c54fb2d46f4dcffdb9630bf13302.png\" /></p><p></p><p>Fig.1:DTIM architecture</p><p></p><p>同步服务和通知服务除了服务于消息服务，也面向其他钉钉业务比如音视频、直播、Ding、文档等多端 (多设备) 数据同步。上图展示了 DTIM 系统架构，接下来详细介绍消息收发链路。</p><p></p><p>消息发送：消息发送接口由 Receiver 提供，钉钉统一接入层将用户从客户端发送的消息请求转发到 Receiver 模块，Receiver 校验消息的合法性（文字图片等安全审核、群禁言功能是否开启或者是否触发会话消息限流规则等）以及成员关系的有效性（单聊校验二者聊天、群聊校验发送者在群聊成员列表中），校验通过后为该消息生成一个全局唯一的 MessageId 随消息体以及接收者列表打包成消息数据包投递给异步队列，由下游 Processor 处理。消息投递成功之后，Receiver 返回消息发送成功的回执给客户端。</p><p></p><p>消息处理 ：Processor 消费到 IM 发送事件首先做按接收者的地域分布（DTIM 支持跨域部署， Geography，Geo）做消息事件分流，将本域用户的消息做本地存储入库（消息体、接收者维度、已读状态、个人会话列表红点更新），最后将消息体以及本域接收者列表打包为 IM 同步事件通过异步队列转发给同步服务。</p><p></p><p>消息接收 ：同步服务按接收者维度写入各自的同步队列，同时查取当前用户设备在线状态，当用户在线时捞取队列中未同步的消息，通过接入层长连接推送到各端。当用户离线时，打包消息数据以及离线用户状态列表为 IM 通知事件，转发给通知服务的 PNS 模块，PNS 查询离线设备做三方厂商通道推送，至此一条消息的推送流程结束。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/23/23c25d88483f989ba85ea5e81140a104.png\" /></p><p></p><p>Fig. 2: DTIM message processing architecture</p><p></p><p></p><h3>存储模型设计</h3><p></p><p></p><p>了解 IM 服务最快的途径就是掌握它的存储模型。业界主流 IM 服务对于消息、会话、会话与消息的组织关系虽然不尽相同，但是归纳起来主要是两种形式：写扩散读聚合、读扩散写聚合，所谓读写扩散其实是定义消息在群组会话中的存储形式，以下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/06/066ee7490d755204a7bd51af61179f61.png\" /></p><p></p><p>Fig. 3: Read model &amp; Write model</p><p></p><p>读扩散的场景，消息归属于会话，对应到存储中相当于有张 conversation_message 的表存储着该会话产生的所有消息 (cid-&gt;msgid-&gt;message，cid 会话 ID、msgid 消息 ID、message 消息)，这样实现的好处是消息入库效率高，只存储会话与消息的绑定关系即可。写扩散的场景，会话产生的消息投递到类似于个人邮件的收件箱，即 message_inbox 表，存储个人的所有消息（uid-&gt;msgid-&gt;message， uid 用户 ID、msgid 消息 ID、message 消息），基于这种实现，会话中的每条消息面向不同的接收者可以呈现出不同状态。</p><p></p><p>DTIM 对 IM 消息的及时性、前后端存储状态一致性要求异常严格，特别对于历史消息漫游的诉求十分强烈，当前业界 IM 产品对于消息长时间存储和客户端历史消息多端漫游都做得不尽如人意，主要是存储成本过高。因此在产品体验与投入成本之间需要找到一个平衡点。</p><p></p><p>采用读扩散，在个性化的消息扩展及实现层面有很大的约束。采用写扩散带来的问题也很明显：一个群成员为 N 的会话一旦产生消息就会扩散 N 条消息记录，如果在消息发送和扩散量较少的场景，这样的实现相比于读扩散落地更为简单，存储成本也不是问题，但是 DTIM 会话活跃度超高，一条消息的平均扩散比可以达到 1：30，超大群又是企业 IM 最核心的沟通场景，如果采用完全写扩散所带来存储成本问题势必制约钉钉业务发展。</p><p></p><p>所以，在 DTIM 的存储实现上，钉钉采取了混合的方案，将读扩散和写扩散针对不同场景做了适配，最终在用户视角系统会做统一合并，如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/99/995da06734a2f5e4719962078ae70c5c.png\" /></p><p></p><p>Fig. 4: DTIM Read-Write hybrid model</p><p></p><p>作为读扩散、写扩散方案的混合形式存在，用户维度的消息分别从 conversation_message 和 message_inbox 表中获取，在应用侧按消息发送时间做排序合并，conversation_message 表中记录了该会话面向所有群成员接收的普通消息 N（Normal），而 message_inbox 表在以下两种场景下被写入：</p><p></p><p>第一种是定向消息 P（Private，私有消息），群会话中发送的消息指定了接收者范围，那么会直接写入到接收者的 message_inbox 表中，比如红包的领取状态消息只能被红包发送者可见，那么这种消息即被定义为定向消息。</p><p></p><p>第二种是归属于会话消息的状态转换 NP（Normal to Private，普通消息转私有消息），当会话消息通过某种行为使得某些消息接收者的消息状态发生转换时，该状态会写入到 message_inbox 表中，比如用户在接收侧删除了消息，那么消息的删除状态会写入到 message_inbox 中，在用户拉取时会通过 message_inbox 的删除状态将 conversation_message 中的消息做覆盖，最终用户拉取不到自己已删除的消息。</p><p></p><p>当用户在客户端发起某个会话的历史消息漫游请求时，服务端根据用户上传的消息截止时间（message_create_time）分别从 conversation_message 表和 message_inbox 表拉取消息列表，在应用层做状态的合并，最终返回给用户合并之后的数据，N、P、NP 三种类型消息在消息个性化处理和存储成本之间取得了很好的平衡。</p><p></p><p></p><h3>同步模型</h3><p></p><p></p><p></p><h4>推送模型</h4><p></p><p></p><p>用户在会话中发出的消息和消息状态变更等事件是如何同步到端上呢？业界关于消息的同步模型的实现方案大致有三种：客户端拉取、服务端推送、服务端推送位点之后客户端拉取的推拉结合方案。</p><p></p><p>三种方案各有优劣，在此简短总结：</p><p></p><p>首先，客户端拉取方案的优点是该方案实施简单、研发成本低，是传统的 B/S 架构；劣势是效率低下，拉取间隔控制权在客户端，对于 IM 这种实时的场景，很难设置一个有效的拉取间隔，间隔太短对服务端压力大，间隔太长时效性差。其次，服务端主动推送方案的优点是低延迟、能做到实时，最重要的主动权在服务端；劣势相对拉取方案，如何协调服务端和客户端的处理能力存在问题。最后是推拉结合方案，这个方案整合拉和推的优点，但是方案更复杂，同时会比推的方案多一次 RTT，特别是在移动网络的场景下，不得不面临功耗和推送成功率的问题。</p><p></p><p>在 DTIM 的场景中，如上文所述，DTIM 相对传统 toC 的场景，有较明显的区别：</p><p></p><p>第一是对实时性的要求。在企业服务中，比如员工聊天消息、各种系统报警，又比如音视频中的共享画板，无不要求实时事件同步，因此需要一种低延时的同步方案。</p><p></p><p>第二是弱网接入的能力。在 DTIM 服务的对象中，上千万的企业组织涉及各行各业，从大城市 5G 的高速到偏远的山区弱网，都需要 DTIM 的消息能发送、能触达。对于复杂的网络环境，需要服务端能判断接入环境，并依据不同的环境条件调整同步数据的策略。</p><p></p><p>第三是功耗可控成本可控。在 DTIM 的企业场景中，消息收发频率比传统的 IM 多出一个数量级，在这么大的消息收发场景怎么保障 DTIM 的功耗可控，特别是移动端的功耗可控，是 DTIM 必须面对的问题。在这种要求下，就需要 DTIM 尽量降低 IO 次数，并基于不同的消息优先级进行合并同步，既能要保障实时性不被破坏，又要做到低功耗。</p><p></p><p>从以上三点可知，主动推的模型更适合 DTIM 场景，服务端主动推送，首先可以做到极低的延时，保障推送耗时在毫秒级别；其次是服务端能通过用户接入信息判断用户接入环境好坏，进行对应的分包优化，保障弱网链路下的成功率；最后是主动推送相对于推拉结合来说，可以降低一次 IO，对 DTIM 这种每分钟过亿消息服务来说，能极大的降低设备功耗，同时配合消息优先级合并包的优化，进一步降低端的功耗。</p><p></p><p>虽说主动推送有诸多优势，但是客户端会离线，甚至客户端处理速度无法跟上服务端的速度，必然导致消息堆积，DTIM 为了协调服务端和客户端处理能力不一致的问题，支持 Rebase 的能力，当服务端消息堆积的条数达到一定阈值，触发 Rebase，客户端会从 DTIM 拉取最新的消息，同时服务端跳过这部分消息从最新的位点开始推送消息。DTIM 称这个同步模型为推优先模型（Preferentially-Push Model，PPM）。</p><p></p><p>在基于 PPM 的推送方案下，为了保障消息的可靠达到，DTIM 还做一系列优化。</p><p></p><p>第一，支持消息可重入，服务端可能会针对某条消息做重复推送，客户端需要根据 msgId 做去重处理，避免端上消息重复展示。</p><p></p><p>第二，支持消息排序，服务端推送消息特别是群比较活跃的场景，某条消息由于推送链路或者端侧网络抖动，推送失败，而新的消息正常推送到端侧，如果端上不做消息排序的话，消息列表就会发生乱序，所以服务端会为每条消息分配一个时间戳，客户端每次进入消息列表就是根据时间戳做排序再投递给 UI 层做消息展示。</p><p></p><p>第三，支持缺失数据回补，在某个极端情况下客户端群消息事件比群创建事件更早到达端上，此时端上没有群的基本信息消息也就无法展现，所以需要客户端主动向服务端拉取群信息同步到本地，再做消息的透出。</p><p></p><p></p><h4>多端数据一致性</h4><p></p><p></p><p>多端数据一致性问题一直是多端同步最核心的问题，单个用户可以同时在 PC、Pad 以及 Mobile 登录，消息、会话红点等状态需要在多端保持一致，并且用户更换设备情况下消息可以做全量的回溯。</p><p></p><p>基于上面的业务诉求以及系统层面面临的诸多挑战，钉钉自研了同步服务来解决一致性问题，同步服务的设计理念和原则如下：</p><p></p><p>统一消息模型抽象，对于 DTIM 服务产生的新消息以及已读事件、会话增删改、多端红点清除等事件统一抽象为同步服务的事件，同步服务不感知事件的类型以及数据序列化方式。同步服务为每个用户的事件分配一个自增的 ID（注：这里非连续递增），确保消息可以根据 ID 做遍历的有序查询。统一同步队列，同步服务为每个用户分配了一个 FIFO 的队列存储，自增 id 和事件串行写入队列；当有事件推送时，服务端根据用户当前各端在线设备状态做增量变更，将增量事件推送到各端。根据设备和网络质量的不同可以做多种分包推送策略，确保消息可以有序、可靠、高效的发送给客户端。</p><p></p><p>上面介绍了 DTIM 的存储模型以及同步模型的设计与思考，在存储优化中，存储会基于 DTIM 消息特点，进行深度优化，并会对其中原理以及实现细节做深入分析与介绍；在同步机制中，会进一步介绍多端同步机制是如何保障消息必达以及各端消息一致性。</p><p></p><p></p><h2>存储优化</h2><p></p><p></p><p>DTIM 底层使用了表格存储作为消息系统的核心存储系统，表格存储是一个典型 LSM 存储架构，读写放大是此类系统的典型问题。LSM 系统通过 Major Compaction 来降低数据的 Level 高度，降低读取数据放大的影响。在 DTIM 的场景中，实时消息写入超过百万 TPS，系统需要划归非常多的计算资源用于 Compaction 处理，但是在线消息读取延迟毛刺依旧严重。</p><p></p><p>在存储的性能分析中，我们发现如下几个特点：</p><p></p><p>6% 的用户贡献了 50% 左右的消息量，20% 的用户贡献了 74% 的消息量。高频用户产生的消息远多于低频用户，在 Flush MemTable 时，高频用户消息占据了文件的绝大部分。对于高频的用户，由于其“高频”的原因，当消息进入 DTIM，系统发现用户设备在线（高概率在线），会立即推送消息，因此需要推送的消息大部分在内存的 MemTable 中。高频用户产生大量的消息，Compaction 耗费了系统大量的计算和 IO 资源。低频的用户消息通常散落在多个文件当中。</p><p></p><p>从上面的四个表现来看，我们能得出如下结论：</p><p></p><p>绝大部分 Compaction 是无效的计算和 IO，由于大量消息写入产生大量的文件，但是高频的用户消息其实已经下推给用户的设备，Compaction 对读加速效果大打折扣。反而会抢占计算资源，同时引起 IO 抖动。低频用户由于入库消息频率低，在 Flush 之后的文件中占比低；同时用户上线频率低，期间会累计较多的待接收的消息，那么当用户上线时，连续的历史消息高概率散落在多个文件当中，导致遍历读取消息毛刺，严重的有可能读取超时。</p><p></p><p>为了解决此类问题，我们采用分而治之方法，将高频用户和低频用户的消息区别对待。我们借鉴了 WiscKey KV 分离技术的思想，就是将到达一定阈值的 Value 分离出来，降低这类消息在文件中的占比进而有效的降低写放大的问题。</p><p></p><p>但是 WiscKey KV 分离仅考虑单 Key 的情况，在 DITM 的场景下，Key 之间的大小差距不大，直接采用这种 KV 分离技术并不能解决以上问题。因此我们在原有 KV 分离的基础上，改进了 KV 分离，将相同前缀的多个 Key 聚合判断，如果多个 Key 的 Value 超过阈值，那么将这些 Key 的 Value 打包了 value-block 分离出去，写入到 value 文件。</p><p></p><p>数据显示，上述方法能够在 Minor Compaction 阶段将 MemTable 中 70% 的消息放入 value 文件，大幅降低 key 文件大小，带来更低的写放大；同时，Major Compaction 可以更快降低 key 文件数，使读放大更低。高频用户发送消息更多，因此其数据行更容易被分离到 value 文件。读取时，高频用户一般把最近消息全部读出来，考虑到 DTIM 消息 ID 是递增生成，消息读取的 key 是连续的，同一个 value-block 内部的数据会被顺序读取，基于此，通过 IO 预取技术提前读取 value-block，可以进一步提高性能。对于低频用户，其发送消息较少，K-V 分离不生效，从而减少读取时候 value 文件带来的额外 IO。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/24/24c34d6eec996e8464fb6661edbd66c9.png\" /></p><p></p><p>Fig. 5: Key value separation and re-pack</p><p></p><p></p><h2>同步机制</h2><p></p><p></p><p>DTIM 面向办公场景，和面向普通用户的产品在服务端到客户端的数据同步上最大的区别是消息量巨大、变更事件复杂、对多端同步有着强烈的诉求。DTIM 基于同步服务构建了一套完整同步流程。同步服务是一个服务端到客户端的数据同步服务，是一套统一的数据下行平台，支撑钉钉多个应用服务。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7c/7c7a328050c267eb006c7c459bd91db7.png\" /></p><p></p><p>Fig. 6: SyncServices architecture</p><p></p><p>同步服务是一套多端的数据同步服务，由两部分组成：部署于服务端的同步服务和由客户端 APP 集成的同步服务 SDK。工作原理类似于消息队列，用户 ID 近似消息队列中的 Topic，用户设备近似消息队列中的 Consumer Group，每个用户设备作为一个消费者能够按需获得这个用户一份数据拷贝，实现了多端同步诉求。</p><p></p><p>当业务需要同步一个变更数据到指定的用户或设备时，业务调用数据同步接口，服务端会将业务需要同步的数据持久化到存储系统中，然后当客户端在线的时候把数据推送给客户端。每一条数据入库时都会原子的生成一个按用户维度单调递增的位点，服务端会按照位点从小到大的顺序把每一条数据都推送至客户端。</p><p></p><p>客户端应答接收成功后，更新推送数据最大的位点到位点管理存储中，下次推送从这个新的位点开始推送。同步服务 SDK 内部负责接收同步服务数据，接收后写入本地数据库，然后再把数据异步分发到客户端业务模块，业务模块处理成功后删除本地存储对应的内容。</p><p></p><p>在上文章节中，已经初步介绍同步服务推送模型和多端一致性的考虑，本章主要是介绍 DTIM 是如何做存储设计、在多端同步如何实现数据一致性、最后再介绍服务端消息数据堆积技术方案 Rebase。</p><p></p><p></p><h3>事件存储</h3><p></p><p></p><p>在同步服务中，采用以用户为中心，将所有要推送给此用户的消息汇聚在一起，并为每个消息分配唯一且递增的 PTS（位点， Point To Sequence），服务端保存每个设备推送的位点。</p><p></p><p>通过两个用户 Bob 和 Alice，来实际展示消息在存储系统中存储的逻辑形态。例如，Bob 给 Alice 发送了一个消息”Hi! Alice“，Alice 回复了 Bob 消息”Hi! Bob“。</p><p></p><p>当 Bob 发送第一条消息给 Alice 时，接收方分别是 Bob 和 Alice，系统会在 Bob 和 Alice 的存储区域末尾分别添加一条消息，存储系统在入库成功时，会分别为这两行分配一个唯一且递增的位点（Bob 的位点是 10005，Alice 的位点是 23001）；入库成功之后，触发推送。比如 Bob 的 PC 端上一次下推的位点是 10000，Alice 移动端的推送位点是 23000，在推送流程发起之后，会有两个推送任务，第一是 Bob 的推送任务，推送任务从上一次位点（10000） + 1 开始查询数据，将获取到 10005 位置的”Hi“消息，将此消息推送给 Bob 的设备，推送成功之后，存储推送位点（10005）。Alice 推送流程也是同理。Alice 收到 Bob 消息之后，Alice 回复 Bob，类似上面的流程，入库成功并分配位点（Bob 的位点是 10009，Alice 的位点是 23003）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c9/c98376e59f45c298784e2b99a6dc70cf.png\" /></p><p></p><p>Fig. 7: Storage design of SyncServices</p><p></p><p></p><h3>多端同步</h3><p></p><p></p><p>多端同步是 DTIM 的典型特点，如何保持多端的数据及时触达和解决一致性是 DTIM 同步服务最大的挑战。上文中已经介绍了同步服务的事件存储模型，将需要推送的消息按照用户聚合。当用户有多个设备时，将设备的位点保存在位点管理系统之中，Key 是用户 + 设备 ID，Value 是上一次推送的位点。如果是设备第一次登录，位点默认为 0。由此可知，每个设备都有单独的位点，数据在服务端只有一份按照用户维度的数据，推送到客户端的消息是服务端对应位点下的快照，从而保障了每个端的数据都是一致的。</p><p></p><p>比如此时 Bob 登录了手机（该设备之前登录过钉钉），同步服务会获取到设备登录的事件，事件中有此设备上次接收数据的位点（比如 10000），同步服务会从 10000 + 1（位点）开始查询数据，获取到五条消息（10005~10017），将消息推送给此台手机并更新服务端位点。此时，Bob 手机和 PC 上的消息一致，当 Alice 再次发送消息时，同步服务会给 Bob 的两台设备推送消息，始终保持 Bob 两个设备之间消息数据的一致性。</p><p></p><p></p><h3>积压处理</h3><p></p><p></p><p>正如上文所述，我们采用了推优先的模型下推数据以保障事件的实时性，采用位点管理实现多端同步，但是实际情况却远比上面的情况复杂。最常见的问题就是设备离线重新登录，期间该用户可能会累计大量未接收的消息数据，比如几万条。如果按照上面的方案，服务端在短时间会给客户端推送大量的消息，客户端 CPU 资源极有可能耗尽导致整个设备假死。</p><p></p><p>其实对于 IM 这种场景来说，几天甚至几小时之前的数据，再推送给用户已经丧失即时消息的意义，反而会消耗客户移动设备的电量，得不偿失。又或者节假日大群中各种活动，都会有大量的消息产生。对于以上情况，同步服务提供 Rebase 的方案，当要推送的消息累计到一定阈值时，同步服务会向客户端发送 Rebase 事件，客户端收到事件之后，会从消息服务中获取到最新的消息（Lastmsg）。这样可以跳过中间大量的消息，当用户需要查看历史消息，可以基于 Lastmsg 向上回溯，即省电也能提升用户体验。</p><p></p><p>还是以 Bob 为例，Bob 登录了 Pad 设备（一台全新的设备），同步服务收到 Pad 登录的事件，发现登录的位点为 0，查询从 0 开始到当前，已经累计 1 万条消息，累计量大于同步服务的阈值，同步服务发送 Rebase 事件给客户端，客户端从消息服务中获取到最新的一条消息”Tks !!!“，同时客户端从同步服务中获取最新的位点为 10017，并告诉同步服务后续从 10017 这个位置之后开始推送。当 Bob 进入到和 Alice 的会话之后，客户端只要从 Lastmsg 向上回溯几条历史消息填满聊天框即可。</p><p></p><p></p><h2>高可用</h2><p></p><p></p><p>DTIM 对外提供 99.995% 的可用性 SLA，有上百万的组织将钉钉作为自身数字化办公的基础设施，由于其极广的覆盖面，DTIM 些许抖动都会影响大量企业、机构、学校等组织，进而可能形成社会性事件。因此，DTIM 面临着极大的稳定性挑战。</p><p></p><p>高可用是 DTIM 的核心能力。对于高可用，需要分两个维度来看，首先是服务自我防护，在遇到流量洪峰、黑客攻击、业务异常时，要有流量管控、容错等能力，保障服务在极端流量场景下还有基本服务的能力。其次是服务扩展能力，比如常见的计算资源的扩展、存储资源的扩展等，资源伴随流量增长和缩减，提供优质的服务能力并与成本取得较好的平衡。</p><p></p><p></p><h3>自我防护</h3><p></p><p></p><p>DTIM 经常会面临各种突发大流量，比如万人大群红包大战、早高峰打卡提醒、春节除夕红包等等都会在短时间内产生大量的聊天消息，给系统带来很大的冲击，基于此我们采用了多种措施。</p><p></p><p>首先是流量管控，限流是保护系统最简单有效的方式。DTIM 服务通过各种维度的限流来保护自身以及下游，最重要的是保护下游的存储。在分布式系统中存储都是分片的，最容易出现的是单个分片的热点问题，DTIM 里面有两个维度的数据：用户、会话 (消息属于会话), 分片也是这两个维度，所以限流采用了会话、用户维度的限流，这样既可以保护下游存储单个分区，又可以一定程度上限制整体的流量。要控制系统的整体流量, 前面两个维度还不够，DTIM 还采用了客户端类型、应用 (服务端 IM 上游业务) 两个维度的限流，来避免整体的流量上涨对系统带来的冲击。</p><p></p><p>其次是削峰平谷。限流简单有效，但是对用户的影响比较大。在 DTIM 服务中有很多消息对于实时性要求不高，比如运营推送等。对于这些场景中的消息可以充分利用消息系统异步性的特点，使用异步消息队列进行削峰平谷，这样一方面减少了对用户的影响，另一方面也减轻对下游系统的瞬时压力。DTIM 可以根据业务类型 (比如运营推送)、消息类型 (比如点赞消息) 等多种维度对消息进行分级，对于低优先级的消息保证在一定时间 (比如 1 个小时) 内处理完成。</p><p></p><p>最后是热点优化。DTIM 服务中面临着各种热点问题，对于这些热点问题仅仅靠限流是不够的。比如通过钉钉小秘书给大量用户推送升级提醒，由于是一个账号与大量账号建立会话，因此会存在 conversation_inbox 的热点问题，如果通过限速来解决，会导致推送速度过慢、影响业务。对于这类问题需要从架构上来解决。</p><p></p><p>总的来说，主要是两类问题：大账号和大群导致的热点问题。对于大账户问题，由于 conversation_inbox 采用用户维度做分区，会导致系统账号的请求都落到某个分区，从而导致热点。解决方案为做热点拆分，既将 conversation_inbox 数据合并到 conversation_member 中 (按照会话做分区)，将用户维度的操作转换为会话维度的操作，这样就可以将系统账号的请求打散到所有分区上， 从而实现消除热点。对于大群问题，压力来自大量发消息、消息已读和贴表情互动，大量的接收者带来极大的扩散量。所以我们针对以上三个场景，分而治之。</p><p></p><p></p><h4>计算延迟与按需拉取</h4><p></p><p></p><p>对于消息发送，一般的消息对于群里面所有人都是一样的，所以可以采用读扩散的方式，即不管多大的群，发一条消息就存储一份。另一方面，由于每个人在每个会话上都有红点数和 Lastmsg, 一般情况下每次发消息都需要更新红点和 Lastmsg，但是在大群场景下会存在大量扩散，对系统带来巨大的压力。我们的解决方案为，对于大群的红点和 Lastmsg，在发消息时不更新，在拉首屏时实时算，由于拉首屏是低频操作且每个人只有一到两个大群，实时计算压力很小，这样高峰期可以减少 99.99 % 的存储操作, 从而解决了大群发消息对 DTIM 带来的冲击。</p><p></p><p></p><h4>请求合并</h4><p></p><p></p><p>在大群发消息的场景中，如果用户都在线，瞬时就会有大量已读请求，如果每个已读请求都处理，则会产生 M*N(M 消息条数，N 群成员数) 的扩散，这个扩散是十分恐怖的。</p><p></p><p>DTIM 的解决方案是客户端将一个会话中的多次已读进行合并，一次性发送给服务端，服务端对于每条消息的已读请求进行合并处理，比如 1 分钟的所有请求合并为 1 次请求。在大群中，进行消息点赞时，短时间会对消息产生大量更新，再加上需要扩散到群里面的所有人，系统整体的扩散量十分巨大。我们发现，对于消息多次更新的场景，可以将一段时间里面多次更新合并，可以大大减少扩散量，从实际优化之后的数据来看，高峰期系统的扩散量同比减少 96%。</p><p></p><p>即使完全做到以上几点，也很难提供当前承诺的 SLA，除了防止自身服务出现问题以外，还必须实现对依赖组件的容灾。我们整体采用了冗余异构存储和异步队列与 RPC 相结合的方案，当任意一类 DTIM 依赖的产品出现问题时， DTIM 都能正常工作，由于篇幅问题，此处不再展开。</p><p></p><p></p><h3>水平扩展能力</h3><p></p><p></p><p>对于服务的弹性扩展能力，也需要分两个维度来看。首先，服务内部的弹性扩展，比如计算资源的扩展、存储资源的扩展等，是我们通常构建弹性扩展能力关注的重点方向；其次是跨地域维度的扩展，服务能根据自身需要，在其他区域扩展一个服务集群，新的服务集群承接部分流量，在跨地域层面形成一个逻辑统一的分布式服务，这种分布式服务我们称之为单元化。</p><p></p><p></p><h4>弹性应用架构</h4><p></p><p></p><p>对于 DTIM 的扩展性，因为构建和生长于云上，在弹性扩展能力建设拥有了更多云的特点和选择。对于计算节点，应用具备横向扩展的能力，系统能在短时间之内感知流量突增进而进行快速扩容，对于上文提到的各种活动引起的流量上涨，能做到轻松应对。同时，系统支持定时扩容和缩容，在系统弹性能力和成本之间取得较好的平衡。</p><p></p><p>对于存储，DTIM 底层选择了可以水平扩展的 Serverless 存储服务，存储服务内部基于读写流量的大小进行动态调度，应用上层完全无感知。对于服务自身的扩展性，我们还实施了不可变基础设施、应用无状态、去单点、松耦合、负载均衡等设计，使 DTIM 构建出了一套高效的弹性应用架构。</p><p></p><p></p><h4>地域级扩展：单元化</h4><p></p><p></p><p>在应用内部实现了高效弹性之后，伴随着业务流量的增长，单个地域已经无法满足 DTIM 亿级别 DUA 的弹性扩展的需求。由于 DTIM 特点，所有用户都可以在添加好友之后进行聊天，这就意味着不能简单换个地域搭建一套孤岛式的 DTIM。为了解决这种规模下的弹性能力，我们基于云上的多 Region 架构，在一个 Geo 地域内，构建了一套异地多活、逻辑上是一体的弹性架构，我们称之为单元化。下图是 DTIM 的单元化架构。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8c/8cd8cb81e94a97a4cb64799fe564e51c.png\" /></p><p></p><p>Fig.&nbsp;8:&nbsp;DTIM&nbsp;Unit&nbsp;group&nbsp;architecture&nbsp;to&nbsp;process&nbsp;message&nbsp;by&nbsp;RoutingService.</p><p></p><p>对于单元化的弹性扩展架构，其中最核心的内容是流量动态调度、数据单地域的自封闭性和单元整体降级。</p><p></p><p></p><h4>动态调度</h4><p></p><p></p><p>流量路由决定了数据流向，我们可以依托这个能力，将大群流量调度到新的单元来承接急速增长的业务流量，同时实现流量按照企业维度汇聚，提供就近部署能力，进而提供优质的 RT 服务。</p><p></p><p>业界现在主流的单元化调度方案主要是基于用户维度的静态路由分段，这种方案算法简单可靠，但是很难实现动态路由调度，一旦用户路由固定，无法调整服务单元。比如在 DTIM 的场景中，企业（用户）规模是随着时间增长、用户业务规模增长之后，单地域无法有效支撑多个大型企业用户时，传统静态方案很难将企业弹性扩展到其他单元，强行迁移会付出极高的运维代价。因此传统的路由方案不具备弹性调度能力。</p><p></p><p>DTIM 提供一套全局一致性的高可用路由服务系统 (RoutingService)。服务中存储了用户会话所在单元，消息服务基于路由服务，将流量路由到不同的单元。应用更新路由数据之后，随之路由信息也发生变化。与此同时，路由服务发起数据订正事件，将会话的历史消息数据进行迁移，迁移完成之后正式切换路由。路由服务底层依赖存储的 GlobalTable 能力，路由信息更新完成之后，保障跨地域的一致性。</p><p></p><p></p><h4>单元自封闭</h4><p></p><p></p><p>数据的单元自封闭是将 DTIM 最重要且规模最大的数据：“消息数据”的接收、处理、持久化、推送等过程封闭在当前单元中，解除了对其他单元依赖，进而能高效地扩充单元，实现跨地域级别高效弹性能力。</p><p></p><p>要做到业务数据在单元内自封闭，最关键是要识别清楚要解决哪种数据的弹性扩展能力。在 DTIM 的场景下，用户 Profile、会话数据、消息数据都是 DTIM 最核心的资产，其中消息数据的规模远超其他数据，弹性扩展能力也是围绕消息数据的处理在建设。怎么将消息按照单元数据合理的划分成为单元自封闭的关键维度。</p><p></p><p>在 IM 的场景中，消息数据来自于人与人之间的聊天，可以按照人去划分数据，但是如果聊天的两个人在不同的单元之间，消息数据必然要在两个单元拷贝或者冗余，因此按照人划分数据并不是很好的维度。</p><p></p><p>DTIM 采用了会话维度划分，因为人和会话都是元数据，数据规模有限，消息数据近乎无限，消息归属于会话，会话与会话之间并无交集，消息处理时并没有跨单元的调用。因此，将不同的会话拆分到不同的单元，保障了消息数据仅在一个单元处理和持久化，不会产生跨单元的请求调用，进而实现了单元自封闭。</p><p></p><p></p><h4>单元降级</h4><p></p><p></p><p>在单元化的架构中，为了支持服务级别的横向扩展能力，多单元是基本形态。单元的异常流量亦或者是服务版本维护的影响都会放大影响面，进而影响 DTIM 整体服务。因此，DTIM 重点打造了单元降级的能力，单一单元失去服务能力之后，DTIM 会将业务流量切换到新的单元，新消息会从正常的单元下推，钉钉客户端在数据渲染时也不会受到故障单元的影响，做到了单元故障切换用户无感知。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>本文通过模型设计、存储优化、同步机制以及高可用等维度，本文全方位地展示了当代企业级 IM 设计的核心。上文是对 DTIM 过去一段时间的技术总结，随着用户数的持续增长，DTIM 也在与时俱进、持续迭代和优化，比如支持条件索引进而实现索引加速和成本可控、实现消息位点的连续累加、实现消息按需拉取和高效的完整性校验、提供多种上下行通道，进一步提升弱网下的成功率和体验等。</p><p></p>",
    "publish_time": "2022-08-09 10:12:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "干了十年 Android 开发，为什么我再也不想继续了",
    "url": "https://www.infoq.cn/article/LW6kJZfINR19uMgppgeJ",
    "summary": "<p>本文最初发布于 Level Up Coding 博客。</p><p></p><p>在这篇文章中，我将谈谈为什么我在这个行业工作了近十年之后，永远地离开了 Android 开发。在开始之前，让我简单介绍下自己在这个领域的职业生涯。</p><p></p><h3>美好时光</h3><p></p><p></p><p>我从 2013 年开始接触 Android 开发，在当时 Android 4.4 还是热门的新事物。AsyncTasks 还是标准，还有 OttoEventBus 和其他令人讨厌的东西。我见证了架构演进的过程，从 MVC 到 MVP/MVI，然后转向 MVVM，最后是 MVVM 和 MVI 的混合。</p><p></p><p>我记得，当 RxJava 出现时，一切都突然变成了反应性的，变成了流。我记得，l33t Android Devs（Hi Jake Wharton）在谈论那个新出现的黑马 Kotlin。我记得，Kotlin 兴起并接管了 Android 行业。我记得，Coroutines 出现了，并且起初被认为是“RxJava”的杀手（嘿，你现在可以用同步方式编写所有异步代码了！不需要流了！）。这个理念很吸引人，但很快就被证明，那仅仅是一个好主意而已，因此，像 Channels 这样的底层异步原语成为 Kotlin 的 Rx-Way。不过事实证明，很多使用 Channels 的人都是自断双臂。不得已，精益冷流（Cold Streams）和热流（Hot Streams）的概念被重新引入，请允许我向你介绍：StateFlows 和 SharedFlows，最后，我们得到了一个轻量级的、支持 Coroutines 的 Kotlin 版 RxJava2。</p><p></p><p>我记得，我和同事 David 围绕状态和事件展开的所有有趣的对话，到底什么是状态，什么是事件？事件对状态有什么作用，反之亦然？我记得，在 Dagger 2 被 Koin 和 Dagger Hilt 取代之前的几年，我熬夜学习 Dagger 2。我还记得，第一次阅读 Uncle Bob 的《架构整洁之道》，这是我在 Android 开发生涯中最开阔眼界的时刻之一。现在，我能够设计和编写几乎所有应用程序，而不需要考虑 MVVM/MVP/MVC 或任何其他特定于平台的细节。我知道为什么测试很重要，我尝试了 TDD，对它是又爱又恨，我还学习了 DDD 和 BDD。</p><p></p><h3>我的火车到达终点站了吗？</h3><p></p><p></p><p>（我选择这个副标题是因为我现在正在从瑞士到德国的火车上写这篇文章。）</p><p></p><p>后来，我加入了保时捷和 IBM 等大企业的领导团队，这是一段很好的旅程，经过 6-7 年的经验积累，我达到了目标。我曾开发过复杂的应用程序，涉及大量的 E2E 加密、传感器通信、NFC 芯片、BLE Beacon、高流量聊天应用，还有非常有名的待办事项应用，等等。</p><p></p><p>大约 6 年后，我开始以首席 Android 开发人员的身份参与项目。我学会了识别所参与的大多数项目的核心技术问题（架构和团队成员对某些模式有不同的理解），我还学会了如何指导团队解决这些问题，以及如何成功地完成项目。对我而言，现在新东西仅仅是学习新的 API 变化 / 框架，目的是解决我们已经解决了很多年的问题，只是新的框架 /API 做了更好的处理（不用再手动处理生命周期、Fragment Transaction、XML 布局等）。</p><p></p><h3>我以前的后端经验</h3><p></p><p></p><p>很幸运，在过去的 4-5 年里，我在客户项目中从事后端工作（根据要求）。我花了很多时间去了解后端开发的来龙去脉，编写并发代码，创建分布式系统，纵向和横向扩展，处理分布式事务，编写可配置的代码，在不同阶段的环境表现出预期的效果。我研究了不同类型的数据库（图、关系、文档），以及什么样的数据应该使用什么样的数据库，我学习了 Docker 和 K8s，我用 Go 重构 Java EE 系统。看着由 Go 编译出来的二进制文件，它的内存使用率和几乎为零的内存占用，我明白了为什么 Go 如此令人惊叹。作为后端开发人员，我所解决的问题与我在 Android 开发中遇到的挑战无法相提并论（我很快就会讲到），作为后端开发者，我所解决的问题比在 Android 上推敲像素影响更大、更深远。</p><p></p><h3>Android 真的就那样吗？</h3><p></p><p></p><p>最终，我厌倦了与 UI/UX 设计师的所有会议，厌倦了向他们解释 Material Design 的基本原理，或者为什么我们不能触发应用程序 Y（不是我们开发的）中那样的行为 X，我记得，好几个小时的设计讨论都让我的大脑直接宕机了。不少项目都会出现这种情况，其中一些项目具有一定的复杂性，一旦团队理解了整洁架构和领域驱动开发，我们就能在很短的时间内写出领域 + 数据层。一旦你向团队解释清楚了各种身份认证流程，恰当处理令牌刷新逻辑就变成小菜一碟了。主要的挑战几乎总是出现在 UI 层，由于框架 API 不断发展变化，UI 层也在不断变化。在很大程度上，UI 层受 UI/UX 设计师和 PO 影响。最近，几乎所有的项目都变成了日常工作，对工程的关注少，对业务、实施的关注多，几乎一直在摆弄 Android API。在最好的情况下，会有一个令人耳目一新的任务，如编写一个自定义视图，并使用一些线性代数的知识。但通常情况下，几乎总是一些无聊的事情，反思这一切，我问自己：这对我有什么好处？当然我赚了很多钱，但我马上就要 30 岁了，几年后，我将在哪里？</p><p></p><p>作为一名经验丰富的 Android 开发人员，我只适合 Android 职位。我所有的技能都是为了能开发出可维护的、整洁的、能在 Android 平台上正常工作的代码。有些代码会被垃圾收集器如期杀死，而有些代码能在垃圾收集器中存活下来，因为它本该如此。如果 Android 很快消失了怎么办？看着像 Flutter 这样优秀的技术，人们已经用它开发出了一些很棒的应用，我不会再把任何新项目作为单独的原生 iOS 和原生 Android 应用来启动。老实说，你的 Android 技能对于大多数公司的首席 / 资深软件工程师职位来说价值并不高。</p><p></p><h3>Android 开发日薄西山</h3><p></p><p></p><p>我成功地完成了自己的最后一个项目。现在是时候做一些改变了。我不想再花几天的时间讨论 CardView 的边框或反复出现的毫无意义的问题，比如是使用单选按钮还是复选框。我不想再为了更好地处理 Android 生命周期或导航而学习新的 Android 库，然后在未来 12 个多月内看它们再次被替换，在过去的 10 年里，这种事我已经做过好多次了。开发人员一代接一代，每一代中都会有人觉得自己有权力编写一个新的库来处理 UI 状态，或者编写一个新的导航库。测试？不，没有。可悲的是，有很多开发者会使用这些库。Android 开发正慢慢被吞噬 Web 开发的混乱所吞噬（你试过安装 create-react-app 吗？你会下载数以千计的库，包括一些易受攻击的库）。</p><p></p><p>幸运的是，在过去几年里，我曾在几个项目中从事后端工作，这使我有机会过渡到后端开发，彻底离开安卓，专注于开发每秒处理数十万用户请求的系统，这对我来说非常有吸引力。现在，路线图上有一些我作为 Android 开发者不了解的新东西：获得 K8s 认证，掌握多个云，深入学习特定数据库，深入理解 DevOps。我感觉，编程的神秘性再次激发了我，有复杂的工程问题需要处理，这让人兴奋。</p><p></p><p>让人难过的是，对于一个纯粹的 Android 开发者来说，架构师或首席 / 资深工程师的道路是封闭的。纯粹的 Android 开发人员根本不具备履行这些职位所需的技能。对我来说，这是一次很棒的旅程，但我再也不会以 Android 开发人员的身份参与项目了。</p><p></p><h5>英文原文：</h5><p></p><p></p><p>https://levelup.gitconnected.com/why-i-left-android-development-after-10-years-and-became-a-backend-developer-86ebf3595d43</p>",
    "publish_time": "2022-08-09 14:32:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《OceanBase 社区版入门到实战》",
    "url": "https://www.infoq.cn/article/61tq7MPNErVJBTM4deZb",
    "summary": "<p>属于 OceanBase 开源社区的第一本分布式实战教程——《OceanBase 社区版入门到实战》正式来啦！</p>\n<p>本书共八章，从 OceanBase 数据库概述开始，到安装部署，到使用，到迁移 再到运维、测试、性能诊断，最后是 OceanBase 生态工具的使用，带你学面掌握 OceanBase 实战使用技能！</p>\n<h2>为什么你需要这本书？</h2>\n<p>《OceanBase 社区版入门到实战》是由阿里云开发者社区联合 OceanBase 开源团队共同打造的实践类教程。本教程总结了 OceanBase 社区版运维开发常见的场景、问题和实践经验，内含具体操作示例、原理解释等。所以，无论你是学生或数据库技术爱好者，还是 DBA 或应用开发者，你都可以通过本教程快速上手 OceanBase ，并获得相应的实践经验。</p>\n<p>《OceanBase 社区版入门到实战》的很多实践经验同样也适用于 OceanBase 企业版。</p>\n<h2>本书亮点</h2>\n<p>1、开源分布式数据库零基础快速入门，快速掌握 OceanBase 核心使用技能<br />\n2、理论结合实践，强化动手能力，快速进入 OceanBase 开发运维工作。<br />\n3、快速学习分布式数据库开发运维成熟经验，提升职场核心竞争力。</p>",
    "publish_time": "2022-08-09 15:11:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OceanBase 数据库",
    "url": "https://www.infoq.cn/article/q3D8R0x5LNtZUX5EchBz",
    "summary": "<p>自2010年立项开始到现在，由蚂蚁集团完全自主研发的原生分布式关系数据库软件——OceanBase 数据库已经走过了十几年的时间。这一路，从最开始的淘宝收藏夹到肩负着双十一的使命，从频临解散到浴火重生，OceanBase 一直秉承着这样一个理念：产品和技术最终是为了解决业务的实际问题，帮助业务持续成长而存在的。</p>\n<p>OceanBase 数据库能够在普通服务器集群上实现金融级稳定性和高可用，首创“三地五中心”城市级故障自动无损容灾新标准，具备基于原生分布式的卓越的水平扩展能力。OceanBase是全球首家通过 TPC-C 标准测试的分布式数据库，单集群规模超过 1500 节点。 OceanBase目前承担蚂蚁集团支付宝 100% 核心链路，在国内几十家银行、保险公司等金融客户的核心系统中稳定运行。</p>\n<p>本报告将从 OceanBase 数据库的整体架构、发展历程、多租户架构、数据库对象、集群架构、数据链路、用户接口和查询语言、事务管理、存储架构、数据库安全等方面全方位为你讲透 OceanBase 数据库的系统概念。</p>\n<p>如果你是数据库从业者或者想入行数据库领域，那么你一定不可错过！</p>",
    "publish_time": "2022-08-09 15:11:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "DatenLord开源Xline：实现跨数据中心数据一致性管理",
    "url": "https://www.infoq.cn/article/m4Pp7FbMqvDEm3otIWd0",
    "summary": "<p></p><h2>简介和背景</h2><p></p><p>随着分布式业务从单数据中心向多数据中心发展，多地多活部署的需求也越来越普遍。这带来最大的挑战就是跨数据中心跨地域的metadata管理，metadata对数据的稳定性和强一致性有极高要求。在单数据中心场景下，metadata的管理已经有很多成熟的解决方案，etcd就是其中的佼佼者，但是在多数据中心场景下，etcd的性能受Raft共识协议的限制，它的性能和稳定性都大打折扣。DatenLord作为高性能跨云跨数据中心的存储，对metadata管理有了跨云跨数据中心的要求。DatenLord目前使用etcd作为metadata的管理引擎，但是考虑到etcd无法完全满足DatenLord的跨云跨数据中心的场景，我们决定实现自己的metadata管理引擎。Xline应运而生，Xline是一个分布式的KV存储，用来管理少量的关键性数据，并在跨云跨数据中心的场景下仍然保证高性能和数据强一致性。考虑到兼容性问题，Xline会兼容etcd接口，让用户使用和迁移更加流畅。</p><p></p><h2>Xline的架构</h2><p></p><p><img src=\"https://static001.infoq.cn/resource/image/3a/4b/3a5688130d5540c9b0cb9f8e66f67c4b.png\" /></p><p></p><p></p><p>Xline的架构主要分为RPC server，KV server，其他server，CURP共识协议模块和Storage模块</p><p></p><p>RPC server：主要负责接受用户的请求并转发到相应的模块进行处理，并回复用户请求。KV Server和其他server：主要业务逻辑模块，如处理KV相关请求的KV server，处理watch请求的watch server等。CURP共识协议模块: 采用CURP共识协议，负责对用户的请求进行仲裁，保证数据强一致性。Storage：存储模块，存储了key value的相关信息。</p><p></p><p>一次写请求的操作流程如下：</p><p></p><p>RPC server接收到用户写请求，确定是KV操作，将请求转发到KV server。KV server做基本请求做验证，然后将请求封装为一个proposal提交给CURP模块。CURP模块执行CURP共识协议，当达成共识后，CURP模块会调用Storage模块提供的callback将写操作持久化到Storage中。最后通知KV server写请求已经commit。KV server得知请求已经被commit，就会封装请求回复，并通过RPC server返回给用户。</p><p></p><h3>Xline的核心： CURP共识协议</h3><p></p><p>CURP共识协议的细节介绍请参考 <a href=\"https://zhuanlan.zhihu.com/p/539140430\">DatenLord｜Curp 共识协议的重新思考</a>\"。CURP协议的优势是将非冲突的proposal达成共识所需要的RTT从2个降为1，对于冲突的proposal仍然需要两个RTT，而etcd等主流分布式系统采用的Raft协议在任何情况下都需要两个RTT。从两个RTT降为一个RTT所带来的性能提升在单数据中心场景下体现的并不明显，但是在多数据中心或者跨云场景下，RTT一般在几十到几百ms的数量级上，这时一个RTT的性能提升则相当明显。</p><p></p><h3>Storage和Revision</h3><p></p><p>Xline作为一个兼容etcd接口的分布式KV存储，etcd重要的revision特性需要完全兼容。简单介绍一下etcd的revision特性，etcd维护了一个全局单调递增的64bit的revision，每当etcd存储的内容发生改变，revision就会加一，也就是说每一次修改操作就会对应一个新的revision，旧的revision不会立马删除，会按需延时回收。一个简单的例子，两个写操作A -&gt; 1，A -&gt; 2，假设最初的revision是1，etcd会为 A = 1 生成revision 2，为 A = 2 生成revision 3。revision的设计使etcd对外提供了更加丰富的功能，如支持历史revision的查找，如查询revision是2的时候A的值，通过比较revision可以得到修改的先后顺序等。以下是etcd对一个KeyValue的proto定义</p><p></p><p><code lang=\"proto\">message KeyValue {\n  bytes key = 1;\n  int64 create_revision = 2;\n  int64 mod_revision = 3;\n  int64 version = 4;\n  bytes value = 5;\n  int64 lease = 6;\n}\n</code></p><p></p><p>一个KeyValue关联了三个版本号，</p><p></p><p>create_revision: 该key被创建时的revisionmod_revision：该key最后一次被修改时候的revisionversion：该key在最近一次被创建后经历了多少个版本，每一次修改version会加一</p><p></p><p>因为需要支持revision特性，Xline的Storage模块参考了etcd的设计，分为Index和DB两个子模块。Index模块存储的是一个key到其对应的所有revision数组的mapping，因为需要支持范围查找，Index采用了BTreeMap，并会放在内存中。DB模块存储的是从revision到真实KeyValue的mapping，因为有持久化和存储大量的历史revision的数据的需求，DB模块会将数据存到磁盘（目前prototype阶段DB仍然存在内存当中，在未来会对接持久化的DB）。那么一次查找流程是先从Index中找到对应的key，然后找到需要的revision，再用revision作为key到DB中查找KeyValue从而拿到完整数据。这样的设计可以支持历史revision的存取，分离Index和DB可以将Index放在内存当中加速存取速度，并且可以利用revision的存储特性即每一次修改都会产生一个新的revision不会修改旧的revision，可以方便DB实现高并发读写。</p><p></p><h2>CURP共识协议带来的挑战</h2><p></p><p>CURP协议的全称是Consistent Unordered Replication Protocal。从名字可以看出CURP协议是不保证顺序的，什么意思呢？比如两条不冲突的proposal，A -&gt; 1，B-&gt; 2，在CURP协议中，因为这两条proposal是不冲突的，所以它们可以并发乱序执行，核心思想是执行的顺序并不会影响各个replica状态机的最终状态，不会影响一致性。这也是CURP协议用一个RTT就可以达成共识的关键。但是对于冲突的proposal，如 A -&gt; 1, A -&gt; 2，CURP协议就需要一个额外的RTT来确定这两条proposal的执行顺序，否则在各个replica上A最终的值会不一样，一致性被打破。</p><p></p><p>因为Xline需要兼容etcd的revision特性也一定需要兼容。Revision特性要求每一次修改都有一个全局唯一递增的revision，但是CURP协议恰恰是无法保证不冲突proposal的顺序，它会允许不冲突的proposal乱序执行，比如前面的例子A -&gt; 1，B -&gt; 2，如果修改前存储的revision是1，那么哪一个修改的revision是2哪一个是3呢？如果需要确定顺序那么就需要一个额外的RTT，那么CURP协议仅需一个RTT就可以达成共识的优势将荡然无存，退化成和Raft一样的两个RTT。</p><p></p><h3>解决方案</h3><p></p><p>解决这个问题的思路是将达成共识和确定顺序即revision分成两个阶段，即通过一个RTT来达成共识，这时候就可以返回用户请求已经commit，然后再通过一个异步的RTT来确定请求的revision。这样既可以保证一个RTT就可以达成共识并返回给用户，又可以保证为每一个修改请求生成全局统一的revision。确定revision用异步batching的方式来实现，这一个额外的RTT会平摊到一段时间内的所有请求上并不会影响系统的性能。</p><p></p><p>Storage模块会实现如下两个callback接口供CURP模块调用，execute()会在共识达成后调用，通知proposal可以执行了，after_sync()会在proposal的顺序确定下来后再调用，以通知proposal的顺序，after_sync()接口会按照确定好的proposal顺序依次调用。</p><p></p><p><code lang=\"rust\">/// Command executor which actually executes the command.\n/// It usually defined by the protocol user.\n#[async_trait]\npub trait CommandExecutor: Sync + Send + Clone + std::fmt::Debug\nwhere\n    C: Command,\n{\n    /// Execute the command\n    async fn execute(&amp;self, cmd: &amp;C) -&gt; Result;\n\n    /// Execute the after_sync callback\n    async fn after_sync(&amp;self, cmd: &amp;C, index: LogIndex) -&gt; Result;\n}\n</code></p><p></p><p>为了配合CURP模块的两阶段操作，Storage模块的设计如下：</p><p></p><p><code lang=\"rust\">/// KV store inner\n#[derive(Debug)]\nstruct KvStoreInner {\n    /// Key Index\n    index: Index,\n    /// DB to store key value\n    db: DB,\n    /// Revision\n    revision: Mutex,\n    /// Speculative execution pool. Mapping from propose id to request\n    sp_exec_pool: Mutex&gt;&gt;,\n}\n</code></p><p></p><p>当execute()回调被调用时，修改Request会被预执行并存到sp_exec_pool中，它存储了ProposeId到具体Request的mapping，这个时候该操作的revision并没有确定，但是可以通知用户操作已经commit，此时只需一个RTT。当操作顺序被确定后，after_sync()会被调用，Storage模块会从sp_exec_pool找到对应的Request并将它持久化，并把全局revision加1作为该操作的revision。</p><p></p><p>接下来我们用一次写请求 A -&gt; 1 和一次读请求 Read A 来讲解整个流程。假设当前的revision是1，当KV server请求收到写请求，它会生成一个proposal发给CURP模块，CURP模块通过一个RTT达成共识后会调用execute() callback接口，Storage模块会将该请求放到sp_exec_pool中，这时候CURP模块会通知KV server请求已经commit，KV server就会返回给用户说操作已完成。同时CURP会异步的用一个额外的RTT来确定该写请求的顺序后调用after_sync() callback接口，Storage会把全局revision加1，然后从sp_exec_pool中讲写请求读出来并绑定revision 2，然后更新Index并持久化到DB当中，这时候DB存储的内容是 revision 2：{key: A, value：1, create_revision: 2, mod_revision: 2, version: 1}。当读请求到达时，就可以从Storage模块中读到 A = 1，并且create_revision = 2，mod_revision = 2。</p><p></p><h2>总结</h2><p></p><p>本文主要介绍了Geo-distributed KV Storage Xline的架构设计，以及为了兼容etcd的revision特性，我们对CURP共识协议和Storage模块做的设计，从而实现了在跨数据中心跨地域场景下的高性能分布式KV存储。详细代码请参考<a href=\"https://github.com/datenlord/Xline\">datenlord/Xline</a>\"，欢迎大家来讨论。</p>",
    "publish_time": "2022-08-09 16:17:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]