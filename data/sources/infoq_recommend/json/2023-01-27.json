[
  {
    "title": "深度学习先驱者Geoffrey Hinton发布新深度学习算法",
    "url": "https://www.infoq.cn/article/hurk7s81o0CSu2iRdr8o",
    "summary": "<p><a href=\"https://www.cs.toronto.edu/~hinton/\">多伦多大学</a>\"教授兼<a href=\"https://research.google/people/GeoffreyHinton/\">谷歌大脑</a>\"工程研究员Geoffrey Hinton近期发布的论文中提出了一种神经网络训练技术，<a href=\"https://www.cs.toronto.edu/~hinton/FFA13.pdf\">前向-前向</a>\"算法（FF），用两次通过网络的前向数据取代<a href=\"https://en.wikipedia.org/wiki/Backpropagation\">反向传播</a>\"来更新模型权重。</p><p></p><p>Hinton提出该算法旨在解决标准反向传播训练中，需要充分了解前向传递中计算才能在训练时计算导数和存储激活值这一缺陷。算法中所使用的两个前向传播输入数据，分别为一正一负，且具备需要优化的相反目标函数。据Hinton称，使用FF算法训练的网络不仅可以完成计算机视觉（CV）任务，且效果与使用方向传播训练的网络一样好。</p><p></p><p></p><blockquote>前向-前向算法（FF）在速度上与反向传播速度相当，且在前向计算的具体细节未知时仍可使用。FF算法的另一优势在于，其可以无需存储神经动态或中断传播误差导数，即可在神经网络传递顺序数据时进行学习……前向-前向算法优于向后传播算法的这两方面，即是大脑皮层中的学习模型，也是不求助于强化学习而以极低功率地进行硬件模拟的方式。</blockquote><p></p><p></p><p>尽管人工神经网络（ANN）是<a href=\"https://en.wikipedia.org/wiki/Perceptron\">基于大脑的数学模型</a>\"，但用于训练ANN的标准反向传播算法却不是基于任何已知生物过程。除开在生物学上的不可信外，反向传播也有上文中所提及的计算方面缺陷，Hinton指出，ANN的强化学习（RL）训练虽然可以不借助反向传播算法，但该方法“在包含数百万乃至数十亿参数的大型网络上扩展性很差”。InfoQ于2021年报道了一种生物学可信且可完全复现反向传播结果的算法，<a href=\"https://www.infoq.com/news/2021/05/biological-ai-training/\">零分化推理学习</a>\"（Z-IL）。</p><p></p><p>Hinton所提出的FF算法用两个“以完全相同方式彼此运算”的前向通道替换了反向传播训练中的前向-反向通道，第一个前向通道对训练集中的真实数据进行操作，神经网络根据输入调整权重以增加每一层的优点值（goodness）。第二个前向通道中，网络所使用的数据并非来自训练集，而是生成的负数据，神经网络权重根据该输入减少每层的优点值。</p><p></p><p>Hinton通过FF算法，在<a href=\"https://paperswithcode.com/dataset/mnist\">MNIST</a>\"及<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR</a>\"数据集上训练了几种可完成计算机视觉任务的神经网络。这些网络规模相对较小，仅包含二至三个隐藏卷积层，且训练不足百个迭代轮次。在评估训练集上的性能时，FF算法所训练的网络表现“仅比使用反向传播训练的网络略差”。</p><p></p><p>Nebuly的CTO，Diego Fiori实现了Hinton的算法，并在<a href=\"https://twitter.com/diego_fiori1995/status/1605242573311709184\">推特上公开了结果</a>\"：</p><p></p><p></p><blockquote>我将Hinton的论文中所提出的两种前向-前向算法分别称作“基础版”和“循环版”，尽管命名如此，基础版反而是性能最好的算法……基础版FF算法相较经典方向传播更节约内存，对深度网络而言可节省45%的内存使用。</blockquote><p></p><p>Fiori在GitHub上开源了他对<a href=\"https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward_forward\">FF算法的实现</a>\"，蒙特利尔大学博士生<a href=\"https://mohammadpz.github.io/\">Mohammad Pezeshki</a>\"同样在开源了他的<a href=\"https://github.com/mohammadpz/pytorch_forward_forward\">FF算法实现</a>\"版本。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/01/hinton-forward-algorithm/\">Deep Learning Pioneer Geoffrey Hinton Publishes New Deep Learning Algorithm</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/Sxv7ku1hDyZTjbJzawCc\">十大值得关注的深度学习算法</a>\"</p><p><a href=\"https://www.infoq.cn/article/rD3fMf_x5viIMssc02BG\">用AI对抗AI！教代码调戏深度学习算法生成的假视频</a>\"</p>",
    "publish_time": "2023-01-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]