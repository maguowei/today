[
  {
    "title": "降本增效：Grab如何在亚马逊云科技上将Kafka消费者流量成本降到零",
    "url": "https://www.infoq.cn/article/bHbaxQSIexCa63uzkJGl",
    "summary": "<p>Kafka 2.3引入了将Apache Kafka消费者连接到相同可用区域（AZ）代理节点的能力，Grab利用这一能力重新配置了消费者，<a href=\"https://engineering.grab.com/zero-traffic-cost\">将亚马逊云科技上的流量成本降低为零</a>\"。这一更改大大降低了在亚马逊云科技上运行Apache Kafka的基础设施总成本。</p><p>&nbsp;</p><p>Grab以Apache Kafka为中心创建了一个流数据平台，支撑公司所有的产品。遵循Kafka最佳实践，他们的初始配置为每个Kafka分区三个副本，横跨亚马逊云科技区域中三个不同的可用区。负责该平台的团队观察到，跨AZ流量占了他们Kafka平台一半的成本，因为<a href=\"https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer_within_the_same_AWS_Region\">亚马逊云科技对跨AZ数据传输收费</a>\"。</p><p>&nbsp;</p><p>对于初始设置的成本，<a href=\"https://www.linkedin.com/in/fhcloud/\">Fabrice Harbulot</a>\"和<a href=\"https://www.linkedin.com/in/quangminhtran94/\">Quang Minh Tran</a>\"的看法如下：</p><p></p><blockquote>这种设计的问题在于，它会产生惊人的跨AZ网络流量。这是因为，在默认情况下，Kafka客户端只与分区leader通信，而分区leader有67%的概率驻留在不同的AZ中。</blockquote><p></p><p>&nbsp;</p><p>跨AZ流量包括新发布的消息、代理之间的数据复制和消费者获取的消息。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/28/28e5bb967e34b588ddf31b00dcdf2ea0.jpeg\" /></p><p></p><p>默认消费者配置，消费者从分区leader获取数据（图片来源：<a href=\"https://engineering.grab.com/zero-traffic-cost\">Grab工程博客</a>\"）</p><p>&nbsp;</p><p>从<a href=\"https://archive.apache.org/dist/kafka/2.3.0/RELEASE_NOTES.html\">Apache Kafka 2.3</a>\"开始，可以将消费者配置为从分区副本中获取数据了。这样，如果消费者只从同一AZ中的代理获取消息，就不会产生数据传输成本了。</p><p>&nbsp;</p><p>这个特性要求Kafka代理和消费者都知道其所在的AZ。对于Kafka代理，团队会使用AZ ID（az1、az2、az3等）配置broker.rack 。AZ ID与AZ名称（1a、1b、1c等）不同，因为<a href=\"https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html\">AZ名称在亚马逊云科技账户间不一致</a>\"。他们还将参数replica.selector.class的值设置为org.apache.kafka.common.replica.RackAwareReplicaSelector。</p><p>&nbsp;</p><p>在消费者端，团队更新了内部Kafka SDK，基于EC2主机元数据用AZ ID配置client.rack 参数，为的是应用程序团队可以通过导出环境变量来启用该功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/9898f68a5854abb6c400ea677498a3f4.jpeg\" /></p><p></p><p>自定义消费者配置，消费者从最近的副本获取数据（图片来源：<a href=\"https://engineering.grab.com/zero-traffic-cost\">Grab工程博客</a>\"）</p><p>&nbsp;</p><p>在某些服务上应用新设置后，团队观察发现，跨AZ流量成本下降，并且有一些值得注意的副作用。首先，端到端延迟最多增加了500毫秒。考虑到大多数消费者从副本获取消息，这也是意料之中的。延迟增加是由复制时间导致的。理论上，任何对延迟敏感的数据流都应该始终从分区leader获取数据，即使那样会产生额外的成本。</p><p>&nbsp;</p><p>其次，在代理维护（停机）时，直接从副本获取消息的消费者可能会遇到代理不可用的情况，因此，它们应该等待/重试，直到同一AZ中的代理恢复在线。最后，团队观察到，代理的负载与跨AZ的消费者数量有关。这意味着，消费者的均匀分布对于确保代理的负载平衡至关重要。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/grab-apache-kafka-aws-cost/\">https://www.infoq.com/news/2023/07/grab-apache-kafka-aws-cost/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/ZHFZHsctlAhN8Ai9wdpo\">Cloudflare的Kafka之旅：万亿级消息处理实践</a>\"</p><p><a href=\"https://www.infoq.cn/article/CpfvECIb5gWdditBBYy7\">使用Strimzi提高Kafka集群的安全性</a>\"</p><p></p>",
    "publish_time": "2023-09-07 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "山一程，水一程！飞书OKR从People剥离",
    "url": "https://www.infoq.cn/article/5f26843427abd574f5bd6ab98",
    "summary": "<p>飞书产品族8月调整，将OKR从People产品族剥离，纳入Office产品族。</p><p></p><p>【山一程】2020年发布飞书OKR，花了很大力气宣传。OKR成名于1999年约翰.杜尔在Google做的管理实验，国内又火了一把，真是管理落后20年吗？</p><p>【水一程】2022年顺势推出People，目标是输出HR全模块套件，当然也包括OKR，全面进入一体化赛道，Buff加满。</p><p>【又一程】2023年8月，OKR作为飞书的种子产品，差异标签，必争之地，却从HR管理挪到办公协同，有什么逻辑变化？&nbsp;</p><p></p><p>事实上，三年来，选择以HR逻辑推广企业级OKR，成功挺难。这波OKR行情，从概念澄清而起，到假设混淆而失。</p><p></p><p>对OKR的概念澄清，飞书选择与KPI做Apple&nbsp;to Apple的直观比较，但问题就在这里。飞书发布的OKR理论手册中写到：</p><p>什么是 KPI ？</p><p>KPI 是 Key Performance Indicator 的缩写，中文名称是「关键绩效指标」，即一系列衡量工作成效的重要指标。</p><p>什么是 OKR ？</p><p>OKR 是 Objective and Key Results 的缩写，中文名称是「目标与关键结果法」，是一套帮助组织实现目标管理、推动执行与协作的工具和方法。</p><p></p><p>单看都对，本意是区分两种方式的场景和用法。可是，比较即观点，加上饱和宣传，挑战式销售，进场辅导，即视感拉满，传递给市场的观点是OKR比KPI高级。</p><p></p><p>【混淆了什么？】</p><p>一：混淆了经营假设</p><p>1999年，Google只有60来人，正如我在前文提到，杜尔引入OKR算不上管理变革， 而是一个管理实验；OKR源头Intel当时很大，可那是PC时代的Intel。&nbsp;面向增长，OKR有试错空间，而面向效率的时候，推动OKR的时间往往不够。</p><p>二：混淆了管理假设&nbsp;</p><p>这么说吧，</p><p>KPI管理目标， OKR驱动目标感。&nbsp;</p><p>KPI是射箭，最高就是十环；OKR是跳高，没必要限制。&nbsp;</p><p></p><p>为什么OKR目标二个月折叠一次最妙？为什么OKR目标要与绩效评估隔离？为什么OKR目标不要超过五个？为什么OKR目标要全员公开？为什么立个目标，会如下图这般对错分明，这是在用OKR签字画押吗？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cd14f130e1de79c7f22a34d3c3ab6901.png\" /></p><p></p><p>来源：飞书发布的OKR理论手册【侵删】</p><p>如此之重的OKR教条怎么驱动团队的目标感，根本忘记了杜尔多次现场演讲提到的，OKR的胜负手就是简单。</p><p></p><p>【OKR和KPI比较导致了什么】？</p><p>OKR与KPI的这种高下之分，客户潜意识接受了观点，用OKR替代KPI，或者从KPI升级到OKR。将一个长期的推动团队自驱，建立目标感的有氧过程，变成短期调整目标管理制度的无氧冲击。往往是由HR牵头OKR专项，HR流程中增加OKR目标设定，HR应用中再配套开发一个OKR App，全错了。&nbsp;</p><p></p><p>这个时候该问了，“&nbsp;你也在做OKR和KPI的对比啊，&nbsp;那你也错了。”人间清醒，其实只要上升一层，OKR本质上是与PFP（Pay for Performance）的区别。不是Apple To Apple，而是Apple To Ant。</p><p></p><p>OKR驱动目标感，解决团队信任问题的，业务主导；记得和特斯联创始人探讨的点，如何通过OKR共识目标，而不只是对齐目标？和知识星球创始人探讨的点，100多人的Startup，规模没到邓巴数的时候，有必要用OKR代替吼一嗓子吗？真正的需求是信任。</p><p>PFP管理目标，解决团队公平问题的，HR主导。主流的五种，KPI只是其中之一，与KPI比较的，应该是同层的这些绩效管理方法。几十年的各种实践，已经把绩效目标搞透了。&nbsp;比如，同样是PBC，IBM管结果和华为管行为；同样是BSC，科技公司较少公开组织绩效，国企反而较多公开组织绩效。什么样的选择对应什么样的底层逻辑，活学活用，很清晰。&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1db9ab6b5d72fed3dadc40cc85f53954.png\" /></p><p>KPI/BSC/PBC/MBO/CPM，这些都是PFP&nbsp;</p><p></p><p>有哪家企业能放弃PFP三步管理（目标设定，绩效评估，薪酬激励）？过去三年，除了把PFP换个时髦的OKR的说法，有哪家企业彻底放弃过？阿里从今年3月份喊出要用OKR全面替代KPI，到8月份就改口为OKR+KPI，因为搞清楚了， 把一年两次3.25/3.75打分，变为一年四次，本质还是KPI。&nbsp;&nbsp;</p><p></p><p>HR做的OKR，只给工具，不锚定问题的时候，一定错了；</p><p>工作重心在哪里，OKR就在哪里填，不求归口。&nbsp;举个例子，产品团队在TAPD里填， 研发团队在Gitlab里填，交付团队在项目工具里填，销售团队在CRM里填，&nbsp;OKR需要数据带感，需要业务数据和OKR目标背靠背，经不起时间和流程的消磨。&nbsp;</p><p></p><p>HR做的OKR，只做加法，不做减法的时候，一定错了。&nbsp;</p><p>HR能发起两次目的不同的目标填写吗（跳高和射箭），解释很复杂，注定会失败。而管理者以办公协同为入口，按自己的管理直觉（松紧和节奏），和工作会议结合，发起OKR的填写，晾晒，都很自然。OKR需要体验带感，简单唤起，用完即走，经不起复杂和耐心的考验。&nbsp;</p><p></p><p>飞书这次将OKR从People转到Office，行百里半九十，&nbsp;换换产品目录代替不了产品重构，过去三年高强度打市场也说明，现象级OKR产品，需要改造，避免最后只能捆绑办公，免费搭售。</p><p></p><p>实践参考：在会议入口，文档入口，Chatbot入口，Wiki入口，以对话GUI替代小程序GUI，通过个人号，工作群ID和CoreHR关系链鉴权，对话唤起分散在各生产系统中的OKR JS Page地址来填写，对齐，修改，折叠和设定；对话唤起本团队，跨团队的目标数据聚合，晾晒，围观，@公开等功能。&nbsp;尽量简单，盲盒打法，让用户自己摸索，组装这些能力。&nbsp;</p><p></p><p>想远一点：HR科技处在从数字化走向企效化的关口，即HRTech与WorkTech融合后，提供更快的企效服务。如小红书成立企效中心， 腾讯成立HR科技中心，都是探索。&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/7247a36cffe08a249c0a12a1042960f0.png\" /></p><p>基于HRTech实践和国内外技术趋势报告的总结</p><p>揽月不达，或可摘星。&nbsp;无论OKR目标还是KPI目标，无论自驱意图还是管理意图，都趋向高设定。细枝末节的比较，只会迷失目标。OKR产品和落地方案，决策者要先考虑For What，再讲How To。讲清楚企业绩效的推升效果怎么算，判断企业是强执行还是强自驱，了解团队沟通方式的接受度，再选择怎么做，因为免费的可能才是最贵的。</p><p></p><p>原文链接：<a href=\"https://mp.weixin.qq.com/s?__biz=MzAwNjI2MTA2MQ==&amp;mid=2247483787&amp;idx=1&amp;sn=b7f3c187f45fb098651bb3c521df66ee&amp;chksm=9b1151a1ac66d8b749954ced6dd03dfe9f579255100658e2c52d46cf853f686b01d8ef30d636&amp;token=820966871&amp;lang=zh_CN#rd\">山一程，水一程！飞书OKR从People剥离</a>\"</p>",
    "publish_time": "2023-09-07 00:13:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全面拥抱大模型！腾讯正式开放全自研通用大模型：参数规模超千亿、预训练语料超2万亿tokens",
    "url": "https://www.infoq.cn/article/HXQtZJxYy5SeK9OH3Xzi",
    "summary": "<p>9月7日，2023腾讯全球数字生态大会上，腾讯集团高级执行副总裁、云与智慧产业事业群CEO汤道生正式发布全链路自研的通用大语言模型：混元大模型。混元大模型具备强大的中文创作能力、复杂语境下的逻辑推理能力，以及可靠的任务执行能力。</p><p></p><p>汤道生表示：“以大模型生成技术为核心，人工智能正在成为下一轮数字化发展的关键动力，也为解决产业痛点带来了全新的思路。大模型需要基于产业场景，与企业数据融合，才能释放出最大的价值。”</p><p>&nbsp;</p><p>据悉，腾讯混元大模型参数规模超千亿，预训练语料超2万亿tokens，当前版本的知识截止到2023年7月。混元大模型基于Transformor，首先进行大规模自监督预训练，之后进行有监督精调，最后通过强化学习进行优化，同时具有一定调用外部插件工具的能力。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/9b/9bf1683f4a50499550200ac6eeebc64f.png\" /></p><p>混元大模型推理能力展示</p><p>&nbsp;</p><p>腾讯集团副总裁蒋杰表示，开源大模型并不适应腾讯海量高并发场景，自研才能完全掌握技术内核，将大模型更好地融入到腾讯的技术栈中。据悉，混元大模型以腾讯强大的算力基础设施为基础，腾讯掌握从模型算法到机器学习框架再到AI基础设施的全链路自研技术，包括从大规模、高质量、多样化的语料库，到创新的大模型算法，再到自研Angel机器学习框架和创新性的训练方法等研发能力。</p><p>&nbsp;</p><p>针对大模型容易“胡言乱语”的问题，腾讯通过自研“探真”算法进行事实修正，让混元大模型的幻觉相比主流开源大模型降低了30%-50%；通过强化学习的方法，让模型学会识别陷阱问题，对安全诱导问题的拒答率提高了20%；通过位置编码优化，提高了超长文的处理效果和性能；提出思维链的新策略，强化模型对问题拆解和分布思考的趋向，让大模型能够像人一样结合实际的应用场景进行推理和决策。此外，腾讯还自研了机器学习框架Angel，使训练速度相比业界主流框架提升1 倍，推理速度比业界主流框架提升1.3倍。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b7/b7f2c844eed7b2cc3552b1d16cf327f1.jpeg\" /></p><p>混元大模型测评数据</p><p>&nbsp;</p><p>蒋杰表示，混元大模型已经成为腾讯的业务底座。目前，腾讯云、腾讯广告、腾讯游戏、腾讯金融科技、腾讯会议、腾讯文档、微信搜一搜、QQ浏览器等50多个腾讯内部业务和产品，已经接入腾讯混元大模型测试并取得初步效果。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c0/c03c5f832c01b42956a65d805992c569.jpeg\" /></p><p>&nbsp;</p><p>混元大模型在腾讯文档的应用示范</p><p>&nbsp;</p><p>据了解，混元大模型将作为腾讯云MaaS（Model-as-a-Service）服务的底座，客户不仅可以直接通过API调用混元，也可以将混元作为基底模型，为不同产业场景构建专属应用。</p><p>&nbsp;</p><p>据悉，从2018年开始，腾讯开始探索大模型相关技术，先后推出了多个千万/亿参数大模型：2019年，腾讯推出了广告推荐MoE大模型，单模型参数超千亿；2021年，腾讯推出了千亿规模的 NLP大模型；2022年，腾讯推出万亿参数的NLP稀疏大模型。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-09-07 11:21:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "计算资源分配不公引发内斗！LLaMA核心作者流失大半，Meta AI联合主管也已离职",
    "url": "https://www.infoq.cn/article/QpRlMDrbrWwzaoKdIlSb",
    "summary": "<p></p><blockquote>计算资源上的内斗，导致 Meta AI 研究部门四分五裂。</blockquote><p></p><p>&nbsp;</p><p>随着 OpenAI 及其他科技巨头在 AI 领域取得突破，Meta 也正忙于发布自己的大语言模型，希望在这一新兴领域占得立足之地。日前，Meta 发布了大模型 <a href=\"https://www.infoq.cn/article/dkPASNisVmd1WIQTs41H\">LLaMA</a>\" 和 <a href=\"https://www.infoq.cn/article/SNdKvvVcJLNkVOGZll2h\">LLaMA 2</a>\"，作为 OpenAI 和 Anthropic 私有模型的开源替代，LLaMA 和 LLaMA 2 在市场上备受好评。但对大部分参与该项目的科学家和工程师来说，称赞来得太迟了。</p><p>&nbsp;</p><p>作为一家由社交媒体巨头转向元宇宙技术研发的大厂，Meta 旗下的 AI 研究实验室Fundamental AI Research (FAIR) 正面临内部危机。</p><p></p><h2>Meta 公司因争抢计算资源发生内斗</h2><p></p><p>&nbsp;</p><p>据 The Information 近日报道，消息人士表示，Meta 参与 LLaMA 项目的人员大半都已辞职，原因是与公司另一个研究团队在计算资源上的内斗。此外，另一个与LLaMA&nbsp;相竞争的模型已被 Meta 放弃。</p><p>&nbsp;</p><p>据悉，2022 年，来自美国的一支 FAIR 团队发布了 OPT-175B，这是一套拥有 1750 亿参数的模型。与此同时，常驻巴黎的另一支 FAIR 队伍则着手开发体量较小的 LLaMA 模型，并坚信其效率能够与更大模型相媲美。</p><p>&nbsp;</p><p>但与 OPT 团队相比，巴黎团队所能获得的算力资源严重不足。随着运营体系内的分配纠纷不断升级，Meta 内部的紧张局势也是愈演愈烈。</p><p>&nbsp;</p><p>2023 年 2 月，随着 FAIR 正式发布 LLaMA，矛盾和压力也终于逼近临界点。</p><p>&nbsp;</p><p>此前一周，Meta AI 的联合主管兼巴黎分部负责人&nbsp;Antoine Bordes 以工作过度和为了配合加州工作时间而筋疲力尽为由选择辞职。他的退出加剧了 Meta 美国和巴黎两支队伍间的分歧，最终闹到了势同水火的地步。为此，Meta 只得放弃了 OPT 模型，并要求双方团队的成员在 LLaMA 2 项目上共同合作。</p><p>&nbsp;</p><p>公众对于 Meta 发布的 LLaMA 和 LLaMA 2 均表示赞赏，称它们为 OpenAI 和 Anthropic 的最先进模型提供了免费的开源替代方案。然而，市场上的一片好评并没能平息 FAIR 之内部分研究人员和工程师们的不满，其中不少人已经申请退出 Meta。</p><p></p><h2>有限的计算资源该怎么分？</h2><p></p><p>&nbsp;</p><p>在生成式 AI 的训练过程中，需要使用大量的计算资源，Meta 这类科技巨头虽然比其它公司拥有更多的计算资源，但也是有限制的。</p><p>&nbsp;</p><p>在部分员工看来，Meta 内部今日的紧张局面，早在 FAIR 确立开放 AI 研究的使命时就已埋下了祸根。世界各地的 FAIR 实验室争相开发各种项目，从改善磁共振成像到解析自然语言等。而在 ChatGPT 横空出世之后，新的难题更是压在每个 FAIR 人头上：如何在有限的资源下，将现有 AI 功能整合起来？</p><p>&nbsp;</p><p>FAIR 部门负责人 Joelle Pineau 在接受 The Information 采访时坦言，Meta 内部的计算资源分配每月都需要由来自企业多个部门的领导班子重新议定。</p><p>&nbsp;</p><p>她承认，LLaMA 和其他团队在资源的具体分配上表现得“有点关系紧张”。Pineau 还强调，高层领导应当在各竞争项目间分配有限资源时努力权衡，尽量在保证顶尖人才的充分参与之外为各个研究方向提供充足的资源供应，而这显然并非易事。</p><p>&nbsp;</p><p>Pineau 称，“我自己的大部分工作时间都花在了挽留和吸引优秀人才上，毕竟没有了这些出色的参与者，我什么都做不了。”</p><p>&nbsp;</p><p>当然，爆发计算资源争夺的绝不只有 Meta 一家。随着生成式 AI 开发变得越来越复杂、资源密集度愈盛，不少科技巨头也开始感受到专用芯片不足、可调度资源有限的窘境。这个现实问题令 AI 研究人员的留存工作变得更加复杂。</p><p></p><h2>大厂的AI人才痛：招人难，留人更难</h2><p></p><p>&nbsp;</p><p>今年以来，Meta 的 AI 人才流失严重。知情人士称，2 月发表的 LLaMA 论文的 14 名作者中，一半以上已经离开 Meta司，其中几位加入了 AI 创业公司或其它大公司。这种外流不仅仅是人才上的损失，更是整个 AI 行业在局势持续紧张、资源限制日益加剧的现实之下表现出的自然态势。</p><p>&nbsp;</p><p>Meta 内部的结构性变化更是令复杂现实雪上加霜。去年 11 月，Meta 宣布裁减约 13% 的员工，即 11000 个工作岗位，这是其历史上规模最大的一轮裁员。今年 2 月，Meta 成立了一支新团队，专注于为自家应用开发生成式 AI 技术。此举从已经捉襟见肘的研究人员队伍中又抽调走了几名生力军。</p><p>&nbsp;</p><p>目前，Meta 公司仍在寻求理想结构、希望保持正常研发。想要保住这份竞争力，团队自身的敏捷运营和团队间的通力协作将至关重要。AI 人才之争在短时间内尚无平息的迹象，&nbsp;而 Meta 只有想办法成功留存并吸引到研究人员，其作为技术大厂的资源优势才能真正转化为市场领先地位。</p><p>&nbsp;</p><p>不只 Meta，谷歌近几年也流失了不少 AI 人才，Transformer 的八位作者甚至全部离开了谷歌。OpenAI 的日子也不好过，据量子比特统计，OpenAI 的 51 位研究人员中，有 16 位人才离开了 OpenAI，离职率高达三分之一；根据 AMiner 的统计，OpenAI 的 ChatGPT R&amp;D 团队已经有 4 人离职。</p><p>&nbsp;</p><p>留人难，招人也并不轻松。AI 大模型热潮使得 AI 人才成为各大公司的争抢对象，供需关系紧张。拉勾招聘发布的《2023 第一季度 AIGC 人才供需报告》显示，在 AI 抢人大战中，AIGC 人才岗位需求量在今年 3 月环比增加 42%。</p><p>&nbsp;</p><p>美国知名求职网站 Indeed 数据显示，近几个月来，合乎 AI&nbsp;要求的招聘职位急剧上升，尤其是中高级的 AI 人才供不应求，招聘薪资也不断攀升。另一家知名求职网站 Adzuna 数据也显示，6 月份美国有 760 万个空缺职位，而与 AI 相关的职位空缺已经上升至 169045 个，其中有 3575 个职位特别标注需要有&nbsp;AIGC&nbsp;的技能。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theinformation.com/articles/inside-metas-ai-drama-internal-feuds-over-compute-power\">https://www.theinformation.com/articles/inside-metas-ai-drama-internal-feuds-over-compute-power</a>\"</p><p><a href=\"https://www.reddit.com/r/LocalLLaMA/comments/16b18tj/inside_metas_ai_drama_internal_feuds_over_compute/\">https://www.reddit.com/r/LocalLLaMA/comments/16b18tj/inside_metas_ai_drama_internal_feuds_over_compute/</a>\"</p>",
    "publish_time": "2023-09-07 14:02:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "度小满数据智能部总经理，技术委员会执行主席杨青，确认担任 FCon 青年数字人才培养专题出品人",
    "url": "https://www.infoq.cn/article/qWNW578stbpGviXFeShD",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。度小满数据智能部总经理，技术委员会执行主席杨青将担任「<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1587?utm_source=infoqweb&amp;utm_medium=article\">青年数字人才培养</a>\"」的专题出品人。在此次专题中，你将了解到一些优秀的青年才俊的成长经历，比如如何培养自己的数智化思维，如何使用新的做事方法或者创新技术解决业务难题等等。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/track/1587?utm_source=infoqweb&amp;utm_medium=article\">杨青</a>\"，硕士毕业于清华大学计算机系，曾就职于百度、阿里，从事 NLP、搜索、推荐、大数据架构等相关方向的研发工作。</p><p></p><p>2018 年初加入度小满金融开始组建数据智能部，从 0 到 1 建设度小满金融的智能引擎的核心算法，深耕计算机视觉、自然语言处理、图模型、机器学习、因果推断等技术能力，多篇文章被 EMNLP、ACL、CIKM 等国际顶会收录，“智能化征信解读中台”工程荣获吴文俊人工智能科技进步奖。相关技术广泛应用于度小满营销、经营、风控、反欺诈全流程业务场景，为上千万客户提供着稳定、安全的金融服务。</p><p></p><p>目前专注于 AIGC 在金融场景中的落地，基于度小满模型即服务“MaaS”的模式积极探索文生图、数字人与生成式大模型的应用转化。已于年初带领团队发布国内首个千亿中文金融大模型“轩辕”。</p><p></p><p>相信杨青的到来，可以帮助提升此专题的质量，让你认识到，数字技术正在金融领域发生着深刻变革，如果说技术是数智化转型的基石，那青年科技人才一定是催化数字时代进程的加速器。打造数字新时代的领军人才，也是企业的人才培养目标之一。</p><p></p><p>除上述专题外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等专题进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：13269078023（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-09-07 14:53:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面对一家年产值 500 万吨的焦化厂，这家数科公司靠什么赋能业务？",
    "url": "https://www.infoq.cn/article/50juwqeILo52ZAD4jGrW",
    "summary": "<p></p><blockquote>采访嘉宾 | 用友网络副总裁罗小江、北京旭阳数字科技有限公司总经理 郗维宝主持人 | 极客邦科技合伙人 &amp; 市场副总裁 杨伟鑫编辑 | 郑思宇</blockquote><p></p><p></p><p>《 “十四五”规划纲要》曾多次提到“产业数字化”和“数字产业化”这样的概念，在这样的背景之下，数科公司实际上是一个不容忽视且极为重要的角色和纽带。尽管数科公司对数智化转型起到了重要的促进作用，但外界对它的了解却并不多，这也使数科公司与一众互联网公司相比，多了几分神秘的色彩。</p><p></p><p>聚焦数智化转型的时代命题，数科公司具体承担着怎样的职责？在服务上游集团企业时，他们遇到过哪些发展痛点？又沉淀出了哪些赋能行业数智化的实战经验 ？本期《InfoQ 新知实验室》有幸邀请到了北京旭阳数字科技有限公司总经理郗维宝以及用友网络副总裁罗小江，我们希望通过拆解旭阳数科服务集团企业的案例，逐步找到上述问题的答案。现整理此文，希望能对读者有所启迪。</p><p></p><h1>依托强大的数智底座，数科公司找到了另一种可能</h1><p></p><p></p><p>提起数科公司，我想很多人心中的第一个疑问便是：与企业内部的数字化团队相比，数科公司到底有什么不同？曾在旭阳集团负责过供应链管理，并在集团信息管理部担任过总经理，到后来逐步正式接任整个旭阳数科，郗维宝亲历了旭阳数科从 0 到 1 的建设过程，对于数科公司的运营方式与职责，他有着更加深刻的认识。</p><p></p><p>从他的角度来看，无论是在集团的信息管理部还是在数科公司，本质上都是帮助主体企业进行数智化转型来提升运营效率。但不同点也十分明显，在担任集团信息部总经理时，整个团队的工作职责是解读企业的战略，用 IT 系统快速固化、沉淀和优化集团的核心竞争力；</p><p></p><p>接任整个数科公司以后，则需要在加速主体企业数智化转型的同时，肩负着赋能行业数智化转型的责任，而这也就要求旭阳数科要考虑清楚自身的商业模式，如何能在服务好市场的同时，找到公司经营的第二曲线。</p><p>要知道，旭阳集团是一家年产值 500 万吨的焦化厂，服务如此大体量的集团企业，或许我们难以想象这其中会遇到怎样的挑战。</p><p></p><p>据郗维宝介绍，焦化行业的生产成本主要是由煤炭、电力、劳动力等方面构成，其中原煤费用就占整体成本的 85%-90%，如何通过技术创新、管理升级和资源整合等手段，降低生产成本，提高企业竞争力和盈利能力，是焦化行业需要面对的挑战。此外，焦化行业的生产需要依赖多种原材料和其他生产要素，如何协同整合各个环节的供应链，提高企业资源的利用效率，也是焦化行业需要解决的另一大问题 。与此同时，旭阳集团还在第六个五年规划中提到，要向“智能制造、工业互联”的新阶段迈进，并设立了“多方式增长、多产业发展、多区域布局”的运营模式。在这样的方针下，旭阳的经营方式在变、业务板块在变、地域在变，所以在水面之下，其实还存在更多问题等待着旭阳数科去解决。</p><p></p><p>面对当前挑战，什么才是最优的解决路径呢？“要想成功地实现数字化转型，旭阳应该有一套好的平台。” 这是郗维宝给出的答案。基于用友服务大型客户的经验，罗小江也表达了相似的观点，他认为，大型企业如果没有一套完整的平台去支撑前沿技术，有时候技术堆叠越多，带来灾难性的可能越大。</p><p></p><p>诚然，面对复杂且不确定的市场环境，利用技术力量来适应变化、适应业务板块与地域文化的差异化，已经是当下数科公司的共识。但现存问题是，如果数科公司选择从 0 开始自研数字化产品，需要经历不断的迭代和试错，周期较长且试错成本高，更何况受制于数字人才短缺、对数据智能技术运用不足等限制因素，绝大部分数科公司并不具备自研的技术能力。</p><p></p><p>因此，对数科公司而言，选择一个历经行业检验的数智底座，为技术创新提供了另一种可能，也让产品从安全性、可靠性以及敏捷性等层面得到了改善和提升。那么，对于数科公司而言，选择数智化服务商要考虑哪几个要素呢？在郗维宝看来，一方面要看它是否有行业解决方案以及成熟的平台产品，并且要看它是否具备引领行业数智化转型的能力；另一方面，要看对方能否与旭阳共同进步、共同成长。</p><p></p><h2>探寻旭阳数科与用友的“1+1&gt;2”</h2><p></p><p></p><p>出于上述考量，旭阳数科最终选择了与用友 iuap 展开合作。作为用友 BIP 的 PaaS 平台，iuap 平台是更懂业务、技术领先、体系完整的企业数智化底座，并积累了用友 35 年服务数百万企业客户的人财物项、产供销研等 10 大领域和众多行业的应用实践。</p><p></p><p>在今年初，旭阳数科以用友 iuap 平台为原型，将原有 100 多个系统全部纳入到了平台底座上，构建了企业整体的数智化底座——旭阳云工业互联网平台，核心目标就是为了解决上述提到的层层难题，最终推进集团各业务系统逐渐从独立化走向统一化融合。</p><p></p><p>目前，旭阳云平台围绕安全环保、生产管控、物资供应以及工厂全生命周期四条主线，打造了智能应用场景。特别值得一提的是，旭阳基于用友 iuap 发布了行业首款焦化行业 MES 系统，面向配煤、备煤、炼焦、化产生产过程管理，覆盖了从生产计划、原料进厂、生产过程到产品出厂的全部生产活动。</p><p></p><p>此外，旭阳集团将二十八年积累的配煤技术进行数字化处理，推出了智能配煤专家产品—旭阳 1 号。郗维宝认为，这将是旭阳数科最具备竞争力的一款产品，后续还将基于用友低代码开发平台 YonBuilder，把现有模型、数据封装成标准化产品，可随时升级迭代、私有化部署，满足焦化企业的各类需求。“我们也做了一个重要的决定，未来旭阳数科的大部分核心产品，都将基于整个用友 iuap 平台去打造。”</p><p></p><p>“旭阳数科在搭建数智化框架时，他们需要的不仅仅是工具，更需要一个完整的运营框架体系，从而让产品更有效地适配行业。”罗小江补充道。基于此，双方的合作不仅限于研发文化、产品体系建设等方面，用友还为旭阳数科提供了生态、营销、联合运营等多方面的能力。</p><p></p><p>目前，双方已经共建了行业数智化联合创新中心，用友在前期会向旭阳数科输送 IVE、领域专家、架构师等专业人才，一同帮助他们攻坚技术、产业融合等问题，让旭阳更快地基于用友 iuap 做产品；</p><p></p><p>在产品商业化层面，由于旭阳数科走向行业势必要触达客户，用友会与之联合打造针对焦化行业的产品或解决方案，同时借助用友的市场营销能力实现双向引流，而用友也会将共创后的知识产权归属给数科公司；</p><p>此外，用友还会为数科公司提供专业的投资与孵化平台，与互联网公司的投资不同，用友产投的目标不是通过投资来颠覆某个行业，而是赋能行业数科公司，和数科公司一起成长。</p><p></p><p>不难看出，用友与旭阳数科既是服务商与客户的关系，也是合作伙伴关系，而这与当时旭阳数科选择数智化技术服务商的初衷也是不谋而合的。</p><p></p><h2>写在最后：用友与数科公司使能行业数智化的故事才刚刚开始</h2><p></p><p></p><p>访谈最后，双方还就大模型技术的产业落地，分享了各自的看法。作为企业服务赛道的服务商，用友在前不久发布了具有多领域融合化、多形态综合型特性的企业服务大模型 YonGPT。谈及 YonGPT 的后续演进方向，罗小江认为，专业领域的 know-how 是大模型得以夯实的基础，只有持续沉淀跨行业的专业知识，才能更好地为企业赛道提供服务。</p><p></p><p>郗维宝补充时提到：“目前，旭阳 1 号智能配煤系统已经发布了 V6.0 版本，增强了预测算法的准确性和可靠性以及优化算法的稳定性，未来还可能将大模型的能力融入到 V7.0 版本中，持续优化配煤系统算法，助力企业实现降本增效的目标。”</p><p></p><p>可以预见的是，依托于用友 iuap 数智底座和软件解决方案，旭阳数科与用友还将有更多故事可以讲，这种源于产业，赋能产业的合作模式，更是为所有数科公司提供了一个转型的范本，也将为焦化行业带来更多可能。</p><p></p><p>在业务变革越来越快的当下，标准化治理的重要性逐渐显露开来，通过标准化治理所有业务数据、管理数据，并基于数智底座创造价值，将成为企业迈向高质量发展的首选。我们也有理由相信，伴随用友与更多产业龙头企业的携手共创，“产业数字化”和“数字产业化”的实现路径将变得愈发清晰。</p>",
    "publish_time": "2023-09-07 15:37:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯云总裁邱跃鹏：大模型将重新定义云上工具和应用",
    "url": "https://www.infoq.cn/article/QQlIL9bSwkBBZo1ymbsh",
    "summary": "<p>&nbsp;9月7日，2023腾讯全球数字生态大会上，腾讯发布了自己的通用大模型<a href=\"https://www.infoq.cn/article/HXQtZJxYy5SeK9OH3Xzi\">混元大模型</a>\"，腾讯集团副总裁、云与智慧产业事业群COO、腾讯云总裁邱跃鹏介绍了腾讯云在训练大模型过程中做的措施支持和优化。</p><p>&nbsp;</p><p>据介绍，目前腾讯云已经建立起围绕大模型的全套能力，包括高性能算力集群、云原生数据湖仓和向量数据库等数据处理引擎、以及模型安全、支持模型训练和精调的工具链等，企业、开发者可以根据各自需求，灵活选择产品，降低大模型的训练成本。</p><p>&nbsp;</p><p>训练大模型对算力的要求非常高，GPU这样的高密算力在训练过程中本身稳定性也要比原来通用计算差一些。每次发生GPU卡故障，对整个训练中断的影响、回滚的要求和工程化的要求都非常高。</p><p>&nbsp;</p><p>训练混元大模型期间，腾讯云对自身的云基础设施，从存储、网络到计算进行了全面升级。基于星海服务器，腾讯云把GPU的服务器故障率降低超50%；基于星脉网络，腾讯云可以支持超过10万张卡并行计算的大规模训练集群。</p><p>&nbsp;</p><p>在训练过程中，如果发生卡方面的故障，整个训练要被中断、做回滚，这对checkpoint写的速度要求非常高，传统的存储服务或者一些商业产品很难满足checkpoint写的要求。通过做存储升级，腾讯云可以在60s内完成超过3TB的数据写入，提高了整体的训练效率。通过存储、计算、网络整体的升级，腾讯云一轮万亿参数的训练可以在4天之内完成。</p><p>&nbsp;</p><p>有了足够的算力做模型训练后，下一步就是做原数据的清洗。混元大模型每次更新数据都要做数据清洗。随着数据越来越多，数据清洗过程也会成为制约整个模型迭代升级的重要因素。通过腾讯云的原生数据湖仓和向量数据库，腾讯云可以每秒写入百万级数据，对于海量数据的清洗也达到了Tbps级吞吐能力。原数据的清洗性能提升超过了40%，数据处理的整体运营成本也降低了50%。据悉，MiniMax、百川等创业公司也在使用腾讯云的解决方案。</p><p>&nbsp;</p><p>在完成了使用算力、清洗和处理数据之后，企业怎样能够更快、更高效地构建行业模型？为此，腾讯云打造了企业模型精调过程中的全栈式研发工具，每个企业可以基于TI平台，使用腾讯云的加速框架、基础算力和开源工具等。另外，腾讯的玄武实验室打造了隐私安全解决方案，让模型交互变得更加安全。</p><p>&nbsp;</p><p>邱跃鹏表示，大模型进一步提升了云产品的效能。比如，腾讯云风控大模型、腾讯云AI代码助手、腾讯会议AI小助手等产品，都因为大模型能力的加持，实现了显著的效率提升和体验优化。目前，腾讯会议AI小助手已经正式开放试用申请，同时新推出了国内首个裸眼3D视频会议功能。</p><p>&nbsp;</p><p>“云是大模型的最佳载体，大模型将开创下一代云服务的全新形态。”邱跃鹏说道。一方面，高性能的云上算力，成了大模型的最佳助推器；搭载大模型能力的应用，也通过云服务的方式落地。无论是训练大模型、还是使用大模型，都离不开云。另一方面，大模型将重新定义云上工具，效能显著提升，企业可以通过云，使用智能化水平更高、更便捷易用的云产品。</p><p>&nbsp;</p>",
    "publish_time": "2023-09-07 16:10:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]