[
  {
    "title": "开放式湖仓是湖仓一体架构的最终归宿",
    "url": "https://www.infoq.cn/article/N8D436ZU9qV1UySeIqCw",
    "summary": "<p>随着 Data+AI 技术的快速演进迭代，湖仓一体架构（Lakehouse）已经成为当前数据平台的事实标准。本文将简要概述数据平台的发展史，阐述湖仓架构产生的必然性。再从开放性的角度出发，探讨 Lakehouse 架构的选型，以及为什么开放式湖仓设计（Open Lakehouse）会是湖仓一体架构的最终归宿。</p><p></p><p></p><h2>湖仓一体的诞生</h2><p></p><p></p><p>我们先来看下湖仓一体架构出现之前，数据平台经过了怎样的发展。根据 CCSA TC601 大数据技术标准推进委员会发布的《湖仓一体技术与产业研究报告（2023 年）》显示，数据平台架构持续演进主要经历了数据库、数据仓库、数据湖三个阶段，它们各自都有明显的优缺点：</p><p></p><p>数据库（Database）：数据库诞生于 20 世纪 60 年代，它能够对结构化数据进行集中式的存储和计算，主要作为数据存储和计算的基础设施。数据库支持事务处理，可以确保数据的一致性和完整性。例如，企业可能使用 MySQL 或 Oracle 等关系型数据库管理系统来存储和查询结构化数据，如客户信息、订单记录等。传统数据库主要适用于结构化数据，对于半结构化和非结构化数据的存储和处理能力有限。另外，数据库在处理大规模数据时可能面临性能瓶颈和扩展困难，无法满足快速增长的数据需求。数据仓库（Data Warehouse）：随着数据量的增长和多样化数据类型的出现，在上世纪 90 年代数据仓库成为数据平台的主流架构。数据仓库具备规范性，能够集中存储和计算结构化数据。数据仓库通常采用星型或雪花型模型来组织数据，以支持复杂的分析查询。数据仓库可以帮助企业实现数据整合，进行在线分析处理（OLAP）和数据挖掘（Data Mining），从而辅助决策。例如，企业可能使用 Teradata、Amazon Redshift 和 Google BigQuery 等数据仓库解决方案来集中存储和分析销售数据、市场趋势等。然而，随着大数据技术的发展，数据仓库在处理非结构化数据、半结构化数据以及具有高多样性、高速度和高容量的数据方面表现出局限性。在数据仓库中，数据的加载和转换过程可能较为复杂，导致数据加载速度较慢。由于数据仓库的数据模型和索引设计，某些查询可能需要较长的时间才能返回结果。此外，数据仓库在处理来自不同数据源的数据时，需要进行数据集成和转换，这可能涉及到复杂的 ETL（抽取、转换和加载）过程。数据湖（Data Lake）：为了满足多种数据类型存储和多场景分析的需求，数据湖成为数据平台的另一种主流架构。它采用分布式存储来存储各种类型的数据，包括结构化、半结构化和非结构化数据，例如，企业可以 Amazon S3 等存储系统来存储各种数据源的原始数据，如日志文件、传感器数据、社交媒体数据等。数据湖提供了更大的灵活性和扩展性，使企业能够在需要时进行数据探索和分析。数据湖具有更好的扩展能力，能够灵活支持多种类型数据的高效取用，但不支持事务处理，数据质量难以保障。数据湖通常以原始数据的形式存储，缺乏严格的数据模式和约束，可能导致数据一致性和隔离性的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e7/e7289dbf6e285bb298a6b41b3cb771fe\" /></p><p></p><p>既然数据仓库和数据湖有着非常明显的优缺点（见上表），就短暂地诞生过一种“湖仓混合”架构（见下图），企业将数据湖和数据仓库结合起来，以充分发挥它们各自的优势。例如，企业可以使用数据湖作为数据的原始存储和数据探索的平台，而将数据仓库用于数据集成、数据清洗和高性能的分析查询。但是，\"数据湖 + 数据仓库\"混合架构也有着明显的缺点：</p><p></p><p>架构复杂：混合架构需要同时管理数据湖和数据仓库，涉及到不同的数据存储和计算引擎，增加了架构的复杂性。开发运维难度大：混合架构需要维护和管理多个组件和系统，对开发人员和运维团队提出了更高的要求。成本高：混合架构的建设和维护成本较高，包括硬件设备、软件许可和人力资源等方面的投入。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/98/981a008e330f7d63fb38c9e2a8795aaf\" /></p><p></p><p>由于现有架构中的数据仓库和数据湖有着各种各样的问题，湖仓一体架构就应运而生了，接下来我们来看看什么是湖仓一体。</p><p></p><p></p><h2>湖仓一体是什么</h2><p></p><p></p><p>Databricks 公司在 CIDR 2021 发表论文首次正式提出了 Lakehouse 的概念 ，但实际上湖仓一体的概念并非由单一个体提出，而是随着技术的发展和需求的变化逐渐形成的，下图分别列举了国外主流厂商对 Lakehouse 的理解。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/97/9769899ad51027eb1e9d5f4a4e902c04\" /></p><p></p><p>与\"湖 + 仓\"混合架构简单地把数据湖和数据仓库进行简单的堆积不同，湖仓一体架构（Lakehouse）是一种新兴的数据架构，将数据湖和数据仓库的特点融合在一起。它旨在解决传统数据湖和数据仓库各自的局限性，并提供更强大的数据管理和分析能力。湖仓一体架构的特点如下：</p><p></p><p>统一数据存储：湖仓一体架构将结构化、半结构化和非结构化数据以原生格式存储在数据湖中。这种统一的数据存储方式消除了数据复制和转换的需求，简化了数据管理过程。事务支持：与传统的数据湖相比，湖仓一体架构提供了事务支持。它允许在数据湖中执行事务操作，如插入、更新和删除，确保数据的一致性和可靠性。数据质量和治理：湖仓一体架构注重数据质量和治理。它提供了数据血缘追踪、数据质量监控和数据访问控制等功能，确保数据的准确性、完整性和安全性。实时和批处理：湖仓一体架构支持实时和批处理数据处理。它可以处理实时数据流和大规模批量数据，并提供实时分析和即席查询的能力。弹性和可扩展性：湖仓一体架构具有弹性和可扩展性。它可以根据需求自动扩展计算和存储资源，以适应不断增长的数据量和变化的工作负载。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4e/4ea605d39947cb762aca7c4e13d44207\" /></p><p></p><p>湖仓一体架构通过将数据湖和数据仓库的优势结合起来，提供了更灵活、可扩展和强大的数据管理和分析能力。它适用于各种数据场景，包括实时分析、机器学习、数据探索和报表等。</p><p></p><p>传统数据仓库和数据湖的老玩家，演进到湖仓一体架构有两个主要方向：一是“湖上建仓”，即在数据湖的基础上构建数据仓库，保留数据湖的灵活性和可扩展性，同时引入数据仓库的治理和分析能力，典型的例子是 Databricks 和开源 Hadoop 体系；二是“仓外挂湖”，即在数据仓库外部挂载数据湖，让数据湖成为数据仓库的一个数据源，以便企业更好地利用数据湖中的数据，典型的例子是 Amazon Redshift、Google BigQuery、阿里云 MaxCompute。由此可见，湖仓一体技术的发展，旨在融合数据湖和数据仓库的优势，形成一种更强大、灵活且易于管理的数据管理架构。通过湖仓一体，企业可以更好地处理和分析各种数据类型，实现数据价值的释放，因此湖仓一体架构已经成为当代大数据平台的事实标准。</p><p></p><p></p><h2>湖仓一体发展趋势</h2><p></p><p></p><p>随着用户场景和业务需求的不断变化，湖仓一体架构在发展过程中也出现了新的趋势。Dremio 在 2023 年发表的论文《The Data Lakehouse: Data Warehousing and More》中，提出 Lakehouse 是一个与具体实现无关的模块化湖仓一体架构：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f3/f3688029c8b0d6a85237ca33262d6bd2\" /></p><p></p><p>模块化的湖仓一体架构，核心模块包括以下几个方面：</p><p></p><p>数据存储（Data Storage）：使用云对象存储来保存原始数据文件，需要能够高效地存储大量来自不同来源的数据。存储引擎（Storage Engine）：负责处理数据管理任务，如数据压缩、重分区和索引等。存储引擎通过优化数据的组织方式，提高查询性能，并确保数据在云对象存储中的高效存储。文件格式（File Format）：它将原始数据以特定的格式存储在对象存储中。数据湖仓使用开放的文件格式（如 Apache Parquet、ORC 等），这些格式具有高效的压缩和查询性能，并且可以被不同的分析引擎使用。表格格式（Table Format）：表格格式是数据湖仓的一个重要组件，它在数据湖上添加了逻辑模型和可靠的数据治理。表格格式简化了数据文件的组织和管理，并提供了元数据管理和数据版本控制的功能。常见的表格格式包括 Apache Iceberg、Apache Hudi 和 Delta Lake 等。计算引擎（Compute Engine）：计算引擎负责处理数据操作和计算任务，它与表格格式进行交互，实现数据的查询、转换和分析等功能。Lakehouse 可以支持多种计算引擎，如 Apache Spark、Presto 等。元数据服务（Catalog）：用于管理数据湖中的表格信息和元数据，它跟踪每个表格的名称、模式和其他相关信息，提供了数据发现和搜索的功能。</p><p></p><p>模块化的设计让湖仓一体架构更加清晰和可解释，这个观点与 Voltron Data（开源项目 Apache Arrow 背后的商业公司）提出的 Composable Codex 不谋而合。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b9/b909eaee1d07473ff12c7cd18c10ee66\" /></p><p></p><p>Voltron Data 联合 Meta 和 Databricks 发表在 VLDB 23 上的论文《The Composable Data Management System Manifesto》指出，不仅模块化是数据系统的正确发展方向，基于开放标准和开源模块构建的数据系统才是未来，可以带来很多好处：</p><p></p><p>提高灵活性和可扩展性：模块化的设计可以将数据计算引擎拆分为多个组件，使得每个组件可以独立开发、维护和优化。这样可以提高系统的灵活性和可扩展性，使得引擎可以适应不同的工作负载和需求。促进互操作性：开放标准可以使不同的数据计算引擎之间实现互操作性，使得它们可以无缝地集成和协同工作。这样可以避免数据孤岛的问题，提高数据生态系统的整体效率和一致性。降低开发和维护成本：模块化的设计和开放标准可以促进组件的重用和共享，减少重复开发的工作量。同时，开放标准可以吸引更多的开发者和厂商参与，形成一个庞大的社区，共同推动引擎的发展和优化，从而降低开发和维护的成本。促进创新和进步：模块化的设计和开放标准可以为不同的开发者和厂商提供一个共同的平台，使他们可以自由地创新和实验新的功能和技术。这样可以推动数据计算引擎的不断进步，满足不断变化的需求和挑战。</p><p></p><p></p><h2>开放式湖仓才是未来</h2><p></p><p></p><p>传统数据仓库对用户最大的困扰是很容易被运营商锁定（Vendor Lock-in），通常有以下几个原因：</p><p></p><p>专有技术和格式：传统数据仓库通常使用特定厂商的专有技术和格式。这些技术和格式是特定厂商的商业机密，不公开或不兼容其他厂商的系统。因此，一旦选择了特定厂商的数据仓库解决方案，就会受到其技术和格式的限制，难以无缝地迁移到其他厂商的解决方案。另外，有一些开源项目使用了自有文件格式来存储数据，虽然源代码开源的，但其文件格式没办法被其他主流计算引擎理解，仍然不属于开放式的架构设计。闭源软件：传统数据仓库通常使用闭源的商业软件，用户无法查看或修改其内部实现细节。这意味着用户对软件的定制和扩展能力受到限制，只能依赖于特定厂商提供的功能和更新。依赖特定硬件和操作系统：传统数据仓库可能依赖于特定的硬件和操作系统。这意味着用户需要购买和维护特定的硬件设备，并且只能在特定的操作系统上运行数据仓库。这增加了用户的成本和依赖性，限制了他们在硬件和操作系统选择上的灵活性。高度集成的架构：传统数据仓库通常采用高度集成的架构，将数据存储、计算和查询等功能紧密耦合在一起。这使得用户难以将数据仓库的不同组件替换为其他厂商的解决方案，因为这些组件之间存在复杂的依赖关系。供应商锁定策略：一些数据仓库供应商可能采用锁定策略，通过限制用户的选择和迁移选项来维持其市场份额。这可能包括限制数据迁移工具、封闭的 API 接口、高昂的许可费用等。这使得用户难以切换到其他供应商的解决方案，从而导致供应商锁定。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/61/61b9318633cd7958d13a446452b8a880\" /></p><p></p><p>湖仓一体架构发展到今天，其最吸引人的一个特点就是就是数据开放性，这是因为其独特的模块化设计带来的：</p><p></p><p>数据格式的灵活性：Lakehouse 架构使用开放的数据格式，例如 Parquet、Avro 或 ORC，这些格式是通用的、开放的标准。这意味着数据可以以一种独立于特定供应商的方式存储和处理，而不会受到特定供应商的限制。这样，即使更换或切换供应商，数据仍然可以保持可访问和可用。开放的数据处理工具和技术：Lakehouse 架构支持使用各种开放的数据处理工具和技术，例如 Apache Spark、Apache Hive、Presto 等。这些工具和技术是开源的，可以在不同的供应商之间进行迁移和切换。这样，即使更换供应商，组织可以继续使用相同的数据处理工具和技术，而不需要重新学习或更换整个技术栈。数据所有权和控制：在 Lakehouse 架构中，数据湖是组织自己拥有和控制的。这意味着组织可以自由地管理和操作数据，而不受运营商的限制。即使切换供应商，组织仍然可以保持对数据的完全控制，并且可以根据需要进行数据迁移或复制。多云和混合云支持：Lakehouse 架构可以在多个云提供商之间进行部署，或者与本地数据中心进行混合部署。这种灵活性使得组织可以根据需求选择最适合的云提供商，而不会受到单一供应商的限制。这样，即使更换或切换云提供商，数据仍然可以保持可访问和可用。</p><p></p><p>值得一提的是，有些厂商声称自己是“开放式”湖仓一体架构，但所谓的“开放”实际是存算分离架构的“开放”，其实是与开放式湖仓一体混为一谈。存算分离是一种大数据处理架构，它将存储和计算节点分开，数据节点负责数据的存储和管理，而计算任务则由单独的计算节点来负责执行。相对于传统的存算一体架构，存算分离架构设计使得系统能够扩展到更大规模的并发能力和数据容量。相较于湖仓一体架构的开放数据设计，存算一体架构只是把数据放在了存储节点上，并没有保证数据的开放性（如使用开源表格式 Apache Iceberg，或者开源文件格式 Apache Parquet 等），因此并不能认为存算分离架构也是开放的。</p><p></p><p>随着人工智能（AI）和大语言模型（LLM）的热潮，AI 给数据平台带来新的挑战：AI 需要更丰富的数据，数据需要更多样的 BI+AI 应用。Data 与 AI 的关系不再是 Data+AI，而是 Data*AI ——数据平台不再是一对一的计算和存储架构，而是 m 对 n 关系的架构。这样的架构改变变化下，数据平台的架构更应有兼具一体化与开放性的设计。开放式湖仓一体架构，是面向 Data+AI 融合场景的新趋势。</p><p></p><p></p><h2>云器开放式湖仓的设计理念</h2><p></p><p></p><p>我们（云器科技）是一家新兴的数据平台服务提供商，主打多云及一体化的数据平台服务。我们依照 Open Lakehouse 的设计理念，实现了一套完整的系统，并服务多家头部客户。存储层设计兼容各种主流的开放存储格式，确保与广泛的数据环境的无缝集成：</p><p></p><p>开放元数据服务：采用 Catalog 形式开放，增强数据管理的灵活性和可访问性。安全的数据访问：保障数据安全的同时，通过身份认证和鉴权机制，允许外部引擎访问云器 Lakehouse 中的数据文件。统一安全策略与开放性结合：结合统一的安全策略与开放性，借助丰富的开源生态系统，最大化地释放企业数据的潜在价值。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a1/a18d792fe769d2c3f528992de96659a8\" /></p><p></p><p>文章前半部分讲述了 Open Lakehouse 的架构优势、设计目标和原则。文章后半部分，重点介绍我们在实现过程中的设计、取舍和经验总结。</p><p></p><p></p><h2>云器开放式湖仓的设计实践</h2><p></p><p></p><p></p><h4>拥抱 Apache Iceberg 打造开放生态</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/00/00b3424bf9adef82ed45ba2c91e87ba9\" /></p><p></p><p>Apache Iceberg、Apache Hudi 和 Delta Lake 是当前数据湖领域中的三种主要表格式，它们都在努力成为数据湖的标准表格式，这种竞争有时被形象地称为\"Table Format War\"。这三种表格式都是开源的，拥有非常好的生态，背后都有一个商业公司在支持。云器认为重复再造一个新的表格式的轮子并不能让 Lakehouse 更加地开放，因此选择一个最合适的表格式，围绕它构建 Lakehouse 的内置表格式，打造一个开放的生态是对用户最负责任的方案。</p><p></p><p></p><h5>为什么是 Apache Iceberg</h5><p></p><p></p><p>来自 Tabular（Apache Iceberg 背后的商业公司）的 Brian Olsen 在 2023 年 7 月发表了一篇文章 Iceberg won the table format war，他认为 Apache Iceberg 已经在这场表格式的竞争中取得了胜利。其观点基本符合我们的看法，也是云器科技选择使用 Apache Iceberg 作为开放存储底座的原因：</p><p></p><p>从技术角度来说，Apache Iceberg 是第一个跳出了 Hive 标准（比如 Hive 分区）限制的表格式，相对于 Apache Hudi 来说没有主键约束，通过 Hidden Partition 的设计支持 Partition Evolution 这种高级功能。这种设计使得使用方有更大的定制空间，支持更丰富的场景。从标准角度来说，Apache Iceberg 首先就制定了一套简单、清晰的标准，然后才提供各种引擎下的实现。Apache Hudi 实现太过复杂，并且非常依赖 JVM 生态。Delta Lake 开源版本相比 Databricks 内部版本有很多滞后性，与 Apache Spark 框架绑定也比较深。有了一个简洁的标准，用户和厂商既可以选择使用官方开源实现，也可以根据自己的需求开发一套完全兼容的实现，这正是一个开放的标准所带来的好处。从生态角度来说，Dremio 很早就选择了 Apache Iceberg 作为其完全开源的 Open Lakehouse 底座。像 Snowflake 这种起初期望用其私有格式抢占市场的大玩家，也支持了 Apache Iceberg 作为可选的内置存储格式，并且性能与内置格式相差无几。甚至像 Databricks 和 Onehouse（Apache Hudi 背后的商业公司）这样的直接竞争对手，也分别通过 Delta Uniserval Format 和 Hudi OneTable 的机制，输出 Apache Iceberg 兼容格式。选择 Apache Iceberg 能更好地避免被运营商绑定的风险，保护用户的数据。</p><p></p><p></p><h5>如何基于 Apache Iceberg 构建通用的增量存储</h5><p></p><p></p><p>云器 Lakehouse 使用 Apache Iceberg 表格式，以及 Apache Parquet 文件格式，打造了一个能够实时兼容 Apache Iceberg 标准的存储文件布局，其元数据通过完全兼容 Iceberg 的 catalog 进行输出，天然兼容所有支持消费 Apache Iceberg 的开源框架，如 Apache Spark 和 Trino 等。由于所有云器 Lakehouse 内表输出的数据文件都是以 Apache Parquet 格式存放在云对象存储上，元数据完全兼容 Apache Iceberg，用户真正拥有自己的数据资产，无需担心 Lock-in 的风险。</p><p></p><p>前面提到过增量计算模式依赖增量存储，这也是通过 Apache Iceberg 实现的。具体来说，基于 Snapshot Isolation 使用 Apache Iceberg V2 标准引入的 Position Delete File 来表示增量数据。由于 Position Delete File 只能表示数据的删除，我们需要把 Update 拆分成经典的 Delete+Insert 模式，这样的设计对数据有三个挑战：</p><p></p><p>Freshness：代表数据进入 Lakehouse 的新鲜度，即数据写入湖仓之后多久可见。为了让数据尽快可见，我们设计了单独的 Ingestion Service 服务，根据不同的表类型，兼顾性能和成本使用最优的方式把数据灌入湖仓。Latency：代表数据的查询性能，也就是查询湖仓中的数据要多久才能出结果。由于数据文件都保存在云对象存储上，我们设计了 Shared-Everything 的分层 Cache 服务，根据数据的冷热以及访问模式，自动将文件中的数据放到最合适的 Cache 中，加快查询速度。Performance：增量计算的间隔越短，就会以增量存储的方式引入越多的小文件，这对后续查询的性能会带来影响。同时，用户的数据和访问模式后续都可能会动态变化，我们需要能够识别并给出存储文件排布的最佳方案。因此，写入增量存储的时候，我们可以根据需要，自动选择以 Copy-on-Write 或者 Merge-on-Read 的方式产生数据文件。同时，在后台使用单独的 Compaction 服务，根据 Lakehouse 搜集到的信息进行文件的重排布，以节省成本，优化查询性能。</p><p></p><p></p><h4>围绕 Apache Parquet 锤炼极致性能</h4><p></p><p></p><p>对于 Lakehouse 的引擎来说，一个 SQL 查询始于读（TableScan）终于写（TableSink）。如果说开放的表格式决定了 Lakehouse 的能力下限，那合适的文件格式则可以决定 Lakehouse 的性能上限。在大数据领域，Apache Parquet 和 Apache Orc 基本就是列存格式的实施标准。两者曾一度各占半壁江山，但现在就像 DuckDB 作者 Hannes 回答社区问题时说的，似乎 Apache Orc 已经被用得越来越少了。那在 Apache Parquet 越来越独领风骚的今天，是不是无脑选择它就能高枕无忧了？</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e0/e0f25a97a12b1a7344fe6008ba6d3752\" /></p><p></p><p>事实并非如此，Parquet 标准其实是一个基于 Thrift 协议定义的文件格式，其中包含很多可选的字段，这为后面的问题埋下了伏笔：</p><p></p><p>由于早期 Parquet 开源代码的糟糕实现，各个引擎和框架都开始从头写自己的 Parquet Reader/Writer，它们都根据自己的需要，只使用了 Parquet 标准中一部分字段，这对不同引擎之间 Parquet 文件的互操作性带来了问题，系统 A 因为用了某个更高级的字段，系统 B 没有实现它，导致 B 无法直接消费 A 产生的文件。不同的 Parquet 实现有不同的 bug，而用户持久化的文件无法重写，这进一步导致不同引擎需要打上各种补丁，从有问题的历史 Parquet 文件中读出正确的数据。再就是臭名昭著的 Parquet V2 争论。Apache Parquet 社区先是引入了一个 Data Page V2 的概念，用来支持对某一列的某个 Page 进行更细粒度的压缩和编码控制，这没有问题。但后续又引入了一个 Feature V2 的概念，把所有新加入的功能如 Page Index、Bloom Filter、Delta Encoding、Byte Stream Split Encoding 等，都称做 V2 功能。这样一来，有的实现认为 Data Page V2 才是 V2，其他则认为只要用了某个 V2 功能也叫 V2，导致对如何判断一个 Parquet 文件是不是 V2 版本没有一个共识。所以很多企业为了规避这个问题，直接禁用 V2 的任何功能。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/89/892fe7ed76ca117dbe125b03f5a5ffeb\" /></p><p></p><p>尽管 Parquet 有各种各样的问题，它仍然是当前大数据文件格式的事实标准，在开放存储这个大前提下，也没有什么好纠结的。既然 Parquet 是开源的，是不是挑一个开源实现拿来用就好了？很多引擎和框架都实现了自己的 Parquet 读写模块，但由于前面提到的问题，要么功能不全，要么性能不佳，基本没有能直接拿来用。早在云器 Lakehouse 选型文件格式的时候（2022 年初），C++ 的开源 Parquet 实现只有 Apache Arrow、Apache Impala 和 DuckDB 三家，其中后两者都只实现了 V1 的功能；而 Apache Arrow 中的 Parquet C++ 代码是 Parquet 社区捐赠的，V2 的功能也实现了一部分，但仍然缺少 Predicate Pushdown、Page Index 和 Bloom Filter 等重要功能，并且内部测评的性能也不及预期，无法直接使用。</p><p></p><p></p><h4>Community Over Code</h4><p></p><p></p><p>纵观国外的一些友商，Velox 和 Databricks 开发了自己的 Native C++ Parquet 实现，而 Snowflake、BigQuery 和 ClickHouse 则都是基于 Apache Arrow 中的 Parquet C++ 实现来完成 Parquet 的拼图。虽然从头开发一套 Parquet 实现的确能够获得最佳的定制和性能，但也需要付出大量重复劳动，也可能会掉进一些前人踩过的坑。既然 Apache Parquet 是一个开源的标准，没有任何秘密，而 Apache Arrow 已经被友商广泛使用，那直接基于 Apache Arrow 中的 Parquet 代码来实现云器 Lakehouse 的文件格式，也符合云器 Lakehouse 追求最佳开放性的设计理念。与此同时，云器科技也决定投入到社区当中，补全 Parquet 缺失的功能，并且深度优化性能，从而真正成为社区的一部分。</p><p></p><p></p><h5>主要贡献</h5><p></p><p></p><p>云器科技对 Parquet 社区的贡献，主要涉及了 Apache 软件基金会旗下两个顶级项目 Apache Parquet 和 Apache Arrow 的三个 GitHub 仓库：</p><p></p><p>apache/parquet-format：Parquet 格式规范，所有的实现必须遵循此规范。参与该项目可以影响格式标准的后续发展，把对云器有益的格式演进推回社区，从而让更多用户受益。apache/parquet-mr：Parquet 的 Java 语言实现，是以 Java 生态为主的大数据开源生态圈依赖最广泛的 Parquet 实现。它也被认为是 Parquet 最权威的实现，其他语言的实现在拿不准的时候，都会参照它来统一行为。参与该社区，可以让云器 Lakehouse 写出的 Parquet 文件，能被所有 Java 生态的开源框架（如 Apache Spark、Apache Hive、Trino）直接消费，达到最大的开放性。apache/arrow：Apache Arrow 项目的核心是一种基于内存的列式数据结构，这是一种与语言无关的标准化规范，使得数据可以在不同的编程语言和计算引擎之间以零复制（zero-copy）的方式进行共享和交换，同时提供了一套比较干净和功能最多的 Parquet C++ 实现。云器 Lakehouse 的 Parquet 正是基于这套代码进行功能开发和性能优化，使其达到一个比较理想的状态。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0d/0da15416f2a2dde355a52fa9d15e7c46\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/74/74427a8621b51d82d7c36e3b5db17388\" /></p><p></p><p>上面两张图基本涵盖了云器科技在 Parquet 社区中最主要的贡献，团队两名成员也因此被 Apache Parquet 和 Apache Arrow 社区提名为 Committer。云器科技对开源社区的投入不会停止，团队成员在 2023 年 8 月北京举行的「Apache Software Foundation 旗下大会 - Community Over Code Asia 2023」上分享了参与开源项目的心得，后续仍会深耕社区，把我们认为有价值的功能贡献回社区，让更多的用户受益。</p><p></p><p></p><h2>总&nbsp; &nbsp; 结</h2><p></p><p></p><p>本文回顾和分析了数据湖仓的历史和大数据平台的演进趋势，提出了基于增量计算的一体化趋势，以及该架构必然需要一个开放式的增量存储支撑。基于 Single Engine · All Data 理念云器发布了一体化数据平台——云器 Lakehouse，并分享了如何围绕 Apache Iceberg 和 Apache Parquet，来构建开放湖仓之上的增量存储，获得最佳开放性和极致性能。同时云器科技也大力投入开源社区，以合作共赢的姿态践行构建 Open Lakehouse 的设计理念。</p><p></p><p></p><h5>作者简介：</h5><p></p><p></p><p>吴刚，云器科技，Lakehouse 技术专家。目前是 Apache ORC 的 PMC，也是 Apache Arrow 和 Apache Parquet 的 committer。在此之前，曾是阿里巴巴的高级技术专家，负责 MaxCompute 的存储系统，也曾在 Uber 负责 Apache Spark 平台。</p>",
    "publish_time": "2024-01-25 00:09:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AutoML时代：领英工程师如何缩短模型训练时间",
    "url": "https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh",
    "summary": "<p>领英工程师 Shubham Agarwal 及 Rishi Gupta 解释道，为协助发现并移除违反其标准政策的内容，领英一直在使用自研的 AutoML 框架，该框架可以并行地训练分类器且试验多个模型架构。</p><p></p><p></p><blockquote>我们使用 AutoML 不断重新训练已有模型，将训练所需时间从数月缩短到数天，并减少开发新基线模型所需时间。这也让我们能积极主动地应对新出现的对抗性威胁。</blockquote><p></p><p></p><p>内容审核的关键之一在于持续的执行和调整，以应对规避审核的新手段，除此之外还必须要能适应环境的变化。这些变化包括：数据漂移，即平台上发布的内容会随着对话的进行发生固有变化；全球事件，这类事件往往会在讨论中出现并产生不同观点，其中常充斥着错误信息；对抗性威胁，其中包括欺诈和欺瞒行为，如伪造档案、实施诈骗等。</p><p></p><p>为应对上述挑战，领英采用的方法目标为“主动检测”，该方法需要一个不断调整和发展其 ML 模型和系统的过程。AutoML 是领英内部研发的工具，全称为自动化机器学习（Automated Machine Learning），用于，通过不断在新数据上重新训练模型、使用假负和假正等数据修正模型、微调参数方式提升机器学习性能。</p><p></p><p></p><blockquote>通过 AutoML，我们得以将过去冗长且复杂的流程转变为精简又高效的流程……在实现 AutoML 后，我们开发新基线模型和持续性重新训练已有模型的平均所需时间从两个月缩短直不到一周。</blockquote><p></p><p></p><p>通过 AutoML，领英工程师实现了数据准备和特征转换过程的自动化，其中包括降噪、降维和特征工程，意在创建用于分类器训练的高质量训练数据集。</p><p></p><p>在第二阶段，AutoML 通过搜索一系列超参数和优化方式，对比不同分类器架构在一组已定的评估指标下生成的模型性能。</p><p></p><p>最后，AutoML 将新完成训练的模型供给生产服务器，实现部署过程的自动化。</p><p></p><p>Agarwal 和 Gupta 认为这套工具仍有一些方面不太成熟，具体来说是需要提高速度和效率，使其能够在更大范围内应用，最终提高对计算能力的要求。他们称，另一个颇具前景的领域是使用生成式 AI，减少标签噪声并生成用于模型训练的合成数据，从而提高数据集质量，</p><p></p><p>虽然并不是所有的组织都有领英的运营规模，或者能拥有自研 ML 自动化工具的资源，但 Agarwal 和 Gupta 所描述的方式仍可在小规模范围内进行复制，从而减轻机器学习工程师与重新训练已有模型相关的重复性工作量。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/linkedin-automl-content-filter/\">https://www.infoq.com/news/2024/01/linkedin-automl-content-filter/</a>\"</p><p></p><p></p><p></p><p></p>",
    "publish_time": "2024-01-25 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java近期新闻：JEP最终草案、Payara 2024年路线图、TornadoVM IDEA插件",
    "url": "https://www.infoq.cn/article/B28HwykMMrV0LBUE47J3",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p></p><p>Oracle 的 Loom 项目架构师和技术负责人<a href=\"https://inside.java/u/RonPressler/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Ron Pressler</a>\"和 Oracle 软件开发总监<a href=\"https://www.linkedin.com/in/jlaskey/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Jim Laskey</a>\"提交了 JEP 草案 8323335（<a href=\"https://openjdk.org/jeps/8323335?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">隐式声明类和实例主方法（最终）</a>\"） 。之前被称为 未命名类和实例主方法（预览）、灵活主方法和匿名主类（预览） 和 隐式类和增强的主方法（预览），这个 JEP 包含了对之前两轮 <a href=\"https://openjdk.java.net/jeps/12?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">预览</a>\" 反馈的增强，即在JDK 22中交付的 JEP 463（<a href=\"https://openjdk.org/jeps/463?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">隐式类和实例主方法（第二次预览）</a>\"和在JDK 21中交付的 JEP 445（<a href=\"https://openjdk.org/jeps/445?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">未命名类和实例主方法（预览）</a>\"）。该JEP提议“让学生可以在不需要理解太多语言特性的前提下编写他们的第一个程序。”2022年9月，Oracle的Java语言架构师<a href=\"https://www.linkedin.com/in/briangoetz/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Brian Goetz</a>\"为此撰写了<a href=\"https://openjdk.org/projects/amber/design-notes/on-ramp?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">“Paving the on-ramp”</a>\"一文。Oracle技术委员会成员<a href=\"https://www.linkedin.com/in/gavin-bierman-a0173075/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Gavin Bierman</a>\"已<a href=\"https://mail.openjdk.org/pipermail/amber-dev/2023-May/008065.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"<a href=\"https://cr.openjdk.org/~gbierman/jep445/jep445-20230502/specs/unnamed-classes-instance-main-methods-jls.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">规范文档</a>\"初稿，供Java社区评审。关于JEP 445的更多细节可以在InfoQ的其他<a href=\"https://www.infoq.com/news/2023/05/beginner-friendly-java/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">报道</a>\"中找到。</p><p></p><p>在经过了两轮预览之后，Laskey 还提交了 JEP 草案 8323333（<a href=\"https://openjdk.org/jeps/8323333?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">字符串模板（最终）</a>\"。前面的两轮预览即在JDK 22中交付的 JEP 459（<a href=\"https://openjdk.org/jeps/459?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">字符串模板（第二次预览）</a>\"）和在JDK 21交付的 JEP 430（<a href=\"https://openjdk.org/jeps/430?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">字符串模板（预览）</a>\"）。该 JEP 提议使用 字符串模板 来增强 Java 编程语言，这些字符串字面量包含嵌入表达式，将在运行时被解释，其中嵌入的表达式将在运行时进行计算和验证。关于 JEP 430 的更多详细信息可以在 InfoQ 的 <a href=\"https://www.infoq.com/news/2023/04/java-gets-a-boost-with-string/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">新闻报道</a>\" 中找到。</p><p></p><h4>JDK 23</h4><p></p><p></p><p>JDK 23 <a href=\"https://jdk.java.net/23/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">早期访问构建</a>\" 版本的<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-23%2B5?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Build 5</a>\" 已发布，包含了针对 Build 4 的 <a href=\"https://github.com/openjdk/jdk/compare/jdk-23%2B4...jdk-23%2B5?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">更新</a>\"，其中包括对各种 <a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2023%20and%20%22resolved%20in%20build%22%20%3D%20b05%20order%20by%20component%2C%20subcomponent&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">问题</a>\" 的修复。关于此版本的更多详细信息可以在 <a href=\"https://jdk.java.net/23/release-notes?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布说明</a>\" 中找到。</p><p></p><h4>JDK 22</h4><p></p><p></p><p>JDK 22 <a href=\"https://jdk.java.net/22/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">早期访问构建</a>\"版本的<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-22%2B31?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Build 31</a>\" 也已发布，包含了针对 Build 30 的 <a href=\"https://github.com/openjdk/jdk/compare/jdk-22%2B30...jdk-22%2B31?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">更新</a>\"，包括对各种 <a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2022%20and%20%22resolved%20in%20build%22%20%3D%20b31%20order%20by%20component%2C%20subcomponent&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">问题</a>\" 的修复。关于此版本的更多详细信息可以在 <a href=\"https://jdk.java.net/22/release-notes?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布说明</a>\" 中找到。</p><p></p><p>对于 <a href=\"https://openjdk.org/projects/jdk/23/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">JDK 23</a>\" 和 <a href=\"https://openjdk.org/projects/jdk/22/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">JDK 22</a>\"，开发者可以通过 <a href=\"https://bugreport.java.com/bugreport/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Java Bug Database</a>\" 报告错误。</p><p></p><h4>Spring Framework</h4><p></p><p></p><p><a href=\"https://spring.io/projects/spring-framework?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Spring Framework</a>\" 6.1.3 和 6.0.16 已 <a href=\"https://spring.io/blog/2024/01/11/spring-framework-6-1-3-and-6-0-16-available-now/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"，包含了错误修复、文档改进、依赖项升级和新特性，例如：在使用 <a href=\"https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/reactive/function/client/WebClient.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">WebClient</a>\" 接口时，从剩余的单检查点排除包含敏感查询参数的完整请求 URI；如果在预检请求中发送了 Access-Control-Request-Private-Network 标头（Private Network Access），则允许 <a href=\"https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/CrossOrigin.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">@CrossOrigin</a>\" 注解在应用程序中向 Google Chrome 提供 Access-Control-Allow-Private-Network 标头；避免在 <a href=\"https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/context/annotation/CommonAnnotationBeanPostProcessor.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">CommonAnnotationBeanPostProcessor</a>\" 类中由于在找到注解之前在外层级别调用而提前解析 ClassUtils 类中定义的 getMostSpecificMethod() 方法。这些版本将与即将发布的 Spring Boot 3.2.2 和 3.1.8 一起提供。关于这些版本的更多详细信息可以在 <a href=\"https://github.com/spring-projects/spring-framework/releases/tag/v6.1.3?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">6.1.3</a>\" 和 <a href=\"https://github.com/spring-projects/spring-framework/releases/tag/v6.0.16?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">6.0.16</a>\" 的发布说明中找到。</p><p></p><p><a href=\"https://spring.io/projects/spring-data?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Spring Data</a>\" 的 2023.1.2 和 2023.0.8 已 <a href=\"https://spring.io/blog/2024/01/12/spring-data-2023-1-2-and-2023-0-8-available/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"，包含了错误修复和相应的子项目依赖项升级，例如：Spring Data Commons 3.2.2 和 3.1.8；Spring Data MongoDB 4.2.2 和 4.1.8；Spring Data Elasticsearch 5.2.2 和 5.1.8；以及 Spring Data Neo4j 7.2.2 和 7.1.8。这些版本也可以在即将发布的 Spring Boot 3.2.2 和 3.1.8 中使用。</p><p></p><p><a href=\"https://spring.io/projects/spring-ws/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Spring Web Services</a>\" 4.0.10 已 <a href=\"https://spring.io/blog/2024/01/12/spring-web-services-4-0-10-is-released/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\" ，带来了一些显著变化，例如：支持 jar:nested，这是 uber JAR 资源的 URI Schema，作为 Spring Boot 3.2 新加载器实现的一部分，位于 <a href=\"https://docs.spring.io/spring-ws/docs/current/api/org/springframework/xml/validation/SchemaFactoryUtils.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">SchemaFactoryUtils</a>\" 类中；删除了 Apache HttpComponents <a href=\"https://hc.apache.org/httpcomponents-client-5.4.x/current/httpclient5/apidocs/org/apache/hc/client5/http/classic/HttpClient.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">HttpClient</a>\" 接口的重复依赖声明；将 Spring Framework 依赖项升级到 6.0.16。关于这个版本的更多详细信息可以在 <a href=\"https://github.com/spring-projects/spring-ws/releases/tag/v4.0.10?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布说明</a>\" 中找到。</p><p></p><p><a href=\"https://spring.io/projects/spring-cloud-dataflow/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Spring Cloud Dataflow</a>\" 2.11.2 已<a href=\"https://spring.io/blog/2024/01/11/spring-cloud-dataflow-2-11-2-released/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"，带来了一些重要的变化，例如：将 <a href=\"https://logback.qos.ch/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Logback</a>\" 升级到 1.2.13 ，解决了 <a href=\"https://github.com/advisories/GHSA-vmq6-5m68-f53m?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">CVE-2023-6378</a>\"问题（这是 Logback 接收器组件中的一种序列化漏洞，允许攻击者通过发送有毒数据来发动拒绝服务攻击）；更新 <a href=\"https://docs.spring.io/spring-cloud-dataflow/docs/2.11.2/api/org/springframework/cloud/dataflow/server/batch/BatchVersion.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">BatchVersion</a>\" 枚举和 <a href=\"https://docs.spring.io/spring-cloud-dataflow/docs/2.11.2/api/org/springframework/cloud/dataflow/server/batch/JdbcSearchableJobExecutionDao.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">JdbcSearchableJobExecutionDao</a>\" 类，支持在基于 Batch5 的模式被删除的 JOB_CONFIGURATION_LOCATION 字段；解决了 JdbcSearchableJobExecutionDao 类中的 getJobExecutionsWithStepCountFilteredByTaskExecutionId() 方法不支持 BATCH_ 任务前缀的问题。关于这个版本的更多详细信息可以在 <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow/releases/tag/v2.11.2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布说明</a>\" 中找到。</p><p></p><h4>Payara</h4><p></p><p></p><p>Payara团队对2023年进行了<a href=\"https://blog.payara.fish/payara-platform-roadmap-2024?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">回顾</a>\"，并提供了Payara平台2024年及以后的路线图。2023年的亮点包括：发布Payara Platform 6；支持JDK 21和MicroProfile 6.1；推出<a href=\"https://start.payara.fish/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Payara Starter</a>\"。2024年的路线图包括：对所有Payara产品的愿景（直至2026年）；详细的Payara Server、Payara Micro、Payara Cloud和Payara Developer Tools路线图；对Jakarta EE 11的支持，计划于2024年6月/7月发布正式版。更多详细信息可以在Payara高级产品经理<a href=\"https://blog.payara.fish/author/louise-castens?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Louise Castens</a>\"和Payara合同技术作家<a href=\"https://www.linkedin.com/in/ghgeek/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Luqman Saeed</a>\"在2023年12月Payara虚拟大会上的演讲<a href=\"https://www.crowdcast.io/c/virtualpayaraconference/xstAZ?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">“用Jakarta EE赋能企业创新”</a>\"中找到。</p><p></p><h4>TornadoVM</h4><p></p><p></p><p><a href=\"https://www.tornadovm.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">TornadoVM团队</a>\"推出了<a href=\"https://github.com/beehive-lab/tornado-insight/blob/main/README.md?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">TornadoInsight</a>\"，一个“开源的用于增强开发者使用TornadoVM体验的IntelliJ IDEA插件”。关键功能包括：即时静态检查器，实时扫描TornadoVM代码并报告TornadoVM不支持的Java特性；一个动态测试框架，简化了单个TornadoVM任务的测试过程。InfoQ将进一步跟进并发布更详细的新闻报道。</p><p></p><h4>Micrometer</h4><p></p><p></p><p><a href=\"https://github.com/micrometer-metrics/micrometer/blob/main/README.md?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Micrometer Metrics</a>\" 1.12.2和1.11.8版本均包含了依赖项升级和错误修复，如：POM文件中<a href=\"https://central.sonatype.com/artifact/io.netty/netty-transport-native-epoll?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">io.netty:netty-transport-native-epoll</a>\" 缺失version声明时报告编译错误；重命名在<a href=\"https://github.com/micrometer-metrics/micrometer/blob/main/micrometer-core/src/main/java/io/micrometer/core/instrument/step/StepMeterRegistry.java?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">StepMeterRegistry</a>\"类中轮询指标的线程；修复在<a href=\"https://github.com/micrometer-metrics/micrometer/blob/main/micrometer-core/src/test/java/io/micrometer/core/instrument/binder/grpc/GrpcObservationTest.java?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">GrpcObservationTest</a>\"类中定义的unaryRpcAsync()方法，提高并发性。有关这些版本的更多详细信息，请参阅<a href=\"https://github.com/micrometer-metrics/micrometer/releases/tag/v1.12.2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">1.12.2</a>\"和<a href=\"https://github.com/micrometer-metrics/micrometer/releases/tag/v1.11.8?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">1.11.8</a>\"的发布说明。</p><p></p><p>同样，<a href=\"https://github.com/micrometer-metrics/tracing/blob/main/README.md?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Micrometer Tracing</a>\"的1.2.2和1.1.9版本也都包含了依赖项升级和错误修复，如：<a href=\"https://github.com/micrometer-metrics/tracing/blob/main/micrometer-tracing-tests/micrometer-tracing-test/src/main/java/io/micrometer/tracing/test/simple/SimpleTraceContextBuilder.java?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">SimpleTraceContextBuilder</a>\"类不会覆盖<a href=\"https://github.com/micrometer-metrics/tracing/blob/main/micrometer-tracing/src/main/java/io/micrometer/tracing/TraceContext.java?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">TraceContext</a>\"接口的值；手动创建的传递字段不通过<a href=\"https://github.com/micrometer-metrics/tracing/blob/main/micrometer-tracing/src/main/java/io/micrometer/tracing/contextpropagation/ObservationAwareSpanThreadLocalAccessor.java?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">ObservationAwareSpanThreadLocalAccessor</a>\"类进行跨线程传播。有关这些版本的更多详细信息，请参阅<a href=\"https://github.com/micrometer-metrics/tracing/releases/tag/v1.2.2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">1.2.2</a>\"和<a href=\"https://github.com/micrometer-metrics/tracing/releases/tag/v1.1.9?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">1.1.9</a>\"的发布说明。</p><p></p><h4>Project Reactor</h4><p></p><p></p><p><a href=\"https://github.com/reactor/reactor/blob/main/README.md?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Project Reactor</a>\" <a href=\"https://github.com/reactor/reactor/releases/tag/2023.0.2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">第二个维护版本</a>\"2023.0.2进行了依赖项升级：reactor-core 3.6.2、reactor-netty 1.1.15和reactor-pool 1.0.5。reactor-kafka 1.3.22、reactor-addons 3.5.1和reactor-kotlin-extensions 1.2.2保持不变。有关此版本的更多详细信息，请参阅<a href=\"https://github.com/reactor/reactor/compare/2023.0.1...2023.0.2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">变更日志</a>\"。</p><p></p><p>Project Reactor <a href=\"https://github.com/reactor/reactor/releases/tag/2022.0.15?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">第十五个维护版本</a>\"2022.0.15的依赖项升级包括：reactor-core 3.5.14、reactor-netty 1.1.15和reactor-pool 1.0.5。reactor-kafka 1.3.22、reactor-addons 3.5.1和reactor-kotlin-extensions 1.2.2保持不变。有关此版本的更多详细信息，请参阅<a href=\"https://github.com/reactor/reactor/compare/2022.0.14...2022.0.15?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">变更日志</a>\"。</p><p></p><p>Project Reactor 2020.0.40，代号Europium-SR40，<a href=\"https://github.com/reactor/reactor/releases/tag/2020.0.40?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"，包含了依赖项升级：reactor-core 3.4.35和reactor-netty 1.0.41。reactor-pool 0.2.12、reactor-kafka 1.3.22、reactor-addons 3.4.10、reactor-kotlin-extensions 1.1.10和reactor-rabbitmq 1.5.6保持不变。有关此版本的更多详细信息，请参阅<a href=\"https://github.com/reactor/reactor/compare/2020.0.39...2020.0.40?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">变更日志</a>\"。</p><p></p><h4>Apache软件基金会</h4><p></p><p></p><p><a href=\"https://tomcat.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Apache Tomcat</a>\" <a href=\"https://www.mail-archive.com/announce@apache.org/msg08838.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">11.0.0-M16</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg08836.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">10.1.18</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg08839.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">9.0.85</a>\"和<a href=\"https://www.mail-archive.com/announce@apache.org/msg08837.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">8.5.98</a>\"均包含了错误修复和显著的变更，例如：重构<a href=\"https://tomcat.apache.org/tomcat-11.0-doc/api/org/apache/tomcat/util/threads/VirtualThreadExecutor.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">VirtualThreadExecutor</a>\"类，使其可以被NIO2连接器使用；纠正了<a href=\"https://bz.apache.org/bugzilla/show_bug.cgi?id=67675&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">问题67675</a>\"修复中的一个回归，该回归破坏了通常由OpenSSL 1.0.2生成的TLS密钥文件的解析，这些格式的密钥没有指定显式的伪随机函数，只依赖默认值;；允许在内省的<a href=\"https://tomcat.apache.org/tomcat-11.0-doc/mbeans-descriptors-howto.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">MBeans</a>\"上进行具有相同名称的多个操作，修复了由于引入第二个addSslHostConfig()方法引起的回归。有关这些版本的更多详细信息，请参阅<a href=\"https://tomcat.apache.org/tomcat-11.0-doc/changelog.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">11.0.0-M16</a>\"，<a href=\"https://tomcat.apache.org/tomcat-10.1-doc/changelog.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">10.1.18</a>\"，<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/changelog.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">9.0.85</a>\"和<a href=\"https://tomcat.apache.org/tomcat-8.5-doc/changelog.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">8.5.98</a>\"的发布说明。</p><p></p><p>在<a href=\"https://cocoon.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Apache Cocoon</a>\" 2.3.0发布之后，开发团队最近决定<a href=\"https://www.mail-archive.com/announce@apache.org/msg08844.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">报废</a>\"Cocoon的2.1和3.0分支。最初发布于20多年前的2.1分支现在被认为已经过时了。3.0分支试图从头开始重写Cocoon，但从未最终完成。Apache Cocoon是一个基于Spring的框架（自版本2.2起），建立在关注点分离和基于组件开发概念的基础上。</p><p></p><h4>Grails</h4><p></p><p></p><p>Grails基金会发布Grails Framework 5.3.6和3.3.18，其中一些显著变化包括：回滚了最近对SnakeYAML、Micronaut、Spring和Spring Boot的升级，因为它们不向后兼容；添加手动触发SDKMan<a href=\"https://sdkman.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"的工作流；更新<a href=\"https://grails-plugins.github.io/grails-release/docs/manual/guide/plugins.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">NexusPublishing</a>\"扩展，将重试延迟从2000毫秒增加到3000毫秒。有关这些版本的更多详细信息，请参阅<a href=\"https://github.com/grails/grails-core/releases/tag/v5.3.6?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">5.3.6</a>\"和<a href=\"https://github.com/grails/grails-core/releases/tag/v3.3.18?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">3.3.18</a>\"的发布说明。</p><p></p><h4>Piranha Cloud</h4><p></p><p></p><p><a href=\"https://piranha.cloud/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Piranha</a>\" 23.12.0已<a href=\"https://github.com/piranhacloud/piranha/releases/tag/v24.1.0?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"，带来了一些显著变化，例如：通过将<a href=\"https://projects.eclipse.org/projects/ee4j.wasp?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Eclipse WaSP</a>\"从3.2.1降级到3.2.0来解决Windows构建失败问题（Eclipse WaSP是<a href=\"https://jakarta.ee/specifications/pages/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Jakarta Pages</a>\"和<a href=\"https://jakarta.ee/specifications/tags/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Jakarta Standard Tag Library</a>\"的兼容实现）；一个新的<a href=\"https://github.com/piranhacloud/piranha/blob/current/uber/src/main/java/cloud/piranha/uber/UberPiranha.java?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">UberPiranha</a>\"类和相应的uber模块，用于在命令行初始化Piranha；为Piranha Uber设置临时目录的能力。有关此版本的更多详细信息，请参阅<a href=\"https://javadoc.io/doc/cloud.piranha/project/latest/index.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">文档</a>\"和<a href=\"https://github.com/piranhacloud/piranha/issues?q=is%3Aissue+-label%3Awontfix+milestone%3A24.1.0+is%3Aclosed&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">问题跟踪</a>\"。</p><p></p><h4>OpenXava</h4><p></p><p></p><p><a href=\"https://openxava.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">OpenXava</a>\" 7.2.2<a href=\"https://openxava.org/blog/openxava-7.2.2-released?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布</a>\"，包含了依赖项升级和错误修复，例如：在同一行应用多个<a href=\"https://openxava.org/OpenXavaDoc/apidocs/org/openxava/annotations/ListProperties.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">@RowStyle注解时只考虑其中的一个；[@ListProperties</a>\"注解在与<a href=\"https://openxava.org/OpenXavaDoc/apidocs/org/openxava/annotations/Tree.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">@Tree</a>\"或<a href=\"https://openxava.org/OpenXavaDoc/apidocs/org/openxava/annotations/Editor.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">@Editor(\"TreeView\")</a>\"注解结合使用时不支持多属性；在使用日历列表格式时数据库连接泄漏。有关此版本的更多详细信息，请参阅<a href=\"https://github.com/openxava/openxava/releases/tag/7.2.2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布说明</a>\"。</p><p></p><h4>Gradle</h4><p></p><p></p><p>Gradle 8.6的<a href=\"https://github.com/gradle/gradle/releases/tag/v8.6.0-RC2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">第二个候选版本</a>\"带来以下这些改进：通过GRADLE_ENCRYPTION_KEY环境变量支持配置缓存中的自定义加密密钥；改进错误和警告报告；改进<a href=\"https://docs.gradle.org/8.6-rc-1/userguide/build_init_plugin.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">Build Init Plugin</a>\"，支持各种类型的项目；增强了插件作者和构建工程师为插件开发自定义构建逻辑的构建编写过程。有关此版本的更多详细信息，请参阅<a href=\"https://docs.gradle.org/8.6-rc-2/release-notes.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDYxNDg5MDUsImZpbGVHVUlEIjoiV3IzRHA4MnJvMkZZbE8zSiIsImlhdCI6MTcwNjE0ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.9x4scHATH8vmXrS8s5aB0-6AW1MGat_Vbk6Ku9cW3rA\">发布说明</a>\"。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/java-news-roundup-jan08-2024/\">https://www.infoq.com/news/2024/01/java-news-roundup-jan08-2024/</a>\"</p>",
    "publish_time": "2024-01-25 10:13:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "曙光全栈托管云，助力“东数西算”发展持续提速",
    "url": "https://www.infoq.cn/article/6mwRolf1N411h7ZFYAF1",
    "summary": "<p>1月22日，信通院发布《面向东数西算的全栈托管云服务发展报告》，曙光全栈托管云作为经典案例被广泛提及。随着“东数西算”工程的持续推进，云计算作为算力的载体，为产业数字化转型注入强劲动能。然而，传统云服务难以满足全栈、安全、开放、绿色等方面的需求。曙光“一云俱全”全栈托管云，成为当下云服务市场的一剂“良方”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33d6ec26a2af0cc54e075d0de19532f3.jpeg\" /></p><p>中国信通院云计算与大数据研究所所长何宝宏（右）、</p><p>中科曙光总裁助理兼重庆计算公司副总裁兼重庆计算公司技术部经理何牧君共同发布</p><p></p><p>基于自研的云计算操作系统，曙光从处理器到应用，打造出集全栈服务、安全可信、开放兼容、绿色节能、算力强劲等核心优势于一体的全栈托管云平台。</p><p></p><p>全栈服务：涵盖云、数、智全栈服务，包含虚拟化、大数据组件、AI等多种能力，具备集IaaS、PaaS和SaaS为一体的产品能力，满足用户多样化需求。</p><p></p><p>安全可信：以国产硬件为底座，构建安全可信的基础环境，支持多重安全加密功能，针对业务的全生命周期，有效保障数据、网络、应用等各层次的安全。</p><p></p><p>开放兼容：与底层硬件、中间件、数据库、操作系统、第三方应用广泛适配，满足快速部署、迁移和扩展需求。</p><p></p><p>绿色节能：采用全浸式相变液冷技术，保持电子元器件恒温，整体提升系统性能，通过部署液冷服务器使算力系统PUE突破性降至1.04，节能效果显著。</p><p></p><p>算力强劲：接入全国一体化大数据中心算力调度平台，进行统一调度，承接成渝、京津冀、长三角、粤港澳等国家节点的各类实时或非实时算力需求服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d1a403e69961667f16b311eb4947c00c.png\" /></p><p>                                                   《面向东数西算的全栈托管云服务发展报告》扫码下载</p>",
    "publish_time": "2024-01-25 10:14:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "被严重宕机坑惨了！多家公司向这个已经存在10年却“鲜为人知”的架构迁移",
    "url": "https://www.infoq.cn/article/FezObW1xRCZavVgEkMsF",
    "summary": "<p></p><blockquote>一次持续了近 6 小时的网络中断故障让 Slack 团队意识到，是时候从单体架构迁移到基于单元的架构上了。对比发现，新的架构可以在 5 分钟内逐步排出受影响的可用区的所有流量。</blockquote><p></p><p>&nbsp;</p><p>单元化架构作为一种增加冗余和有效限制站点故障影响范围的方式，在大型在线服务中越来越流行。在过去一年半时间里，Slack 将大多数面向用户的关键服务从单体架构迁移到基于单元的架构上。此前，Slack 团队在一篇文章中解释了大规模迁移原因，以及团队在此过程中所做出的工程技术权衡。</p><p></p><h2>为减少灰色故障，Slack 迁移到 AWS 中基于单元的架构上</h2><p></p><p>&nbsp;</p><p>Slack团队会在每次发生明显的服务中断后进行一次事故评审。以下是Slack团队内部事故评审报告的一部分摘录，该摘录总结了其中的一起事故以及Slack团队的发现：</p><p>&nbsp;</p><p></p><blockquote>2021 年 6 月 30 日上午 11 点 45 分，我们的云供应商在美国东海岸的一个可用区域发生网络中断，Slack 的大部分服务都托管在那里。连接一个可用区域和其他几个包含 Slack 服务器的可用区域的网络链路发生了间歇性故障，导致 Slack 服务器之间的连接变慢，进而出现服务降级。&nbsp;当日下午 12 点 33 分，我们的云供应商自动从服务中删除了网络链接，恢复了对 Slack 客户的全部服务。经过他们的一系列自动检查之后，网络链接再次进入服务状态。&nbsp;当日下午 5 点 22 分，网络链路又发生了同样的间歇性故障。下午 5 点 31 分，云供应商永久地从服务中删除了网络链接，恢复了对我们全部的服务。</blockquote><p></p><p>&nbsp;</p><p>乍一看，Slack的服务遭遇了物理硬件故障，导致了一些错误。直到故障硬件被移除，问题才得到解决。然而，在事故评审过程中，不禁要问：让用户经历这样的服务中断是合理的吗？</p><p>&nbsp;</p><p>Slack 的基础设施覆盖全球，但其核心平台托管在美国东海岸地区 (us-east-1)，该公司使用可用区（AZ）进行故障隔离。在云端托管服务的构建者可以通过这种方式提高服务的可用性：即使某个AZ发生故障，整个服务的运行也不会受到影响。这就引出了一个问题：为什么6月30日的策略没有奏效？为什么一个AZ的故障会导致用户体验中断？</p><p>&nbsp;</p><p>Slack 的高级工程师 Cooper Bethea 解释了为什么分布式系统中的故障检测可能会出现问题：</p><p>&nbsp;</p><p></p><blockquote>事实证明，检测分布式系统中的故障是一个难题。来自用户的单个 Slack API 请求（比如说加载一个频道中的消息）可能会分散成数百个服务后端的 RPC，所有 RPC 都必须全部完成才能向用户返回正确的响应。我们的服务前端不断尝试检测和排除出现故障的后端，但在排除出现故障的服务器之前，我们必须先记录下来一些故障！</blockquote><p></p><p>&nbsp;</p><p>Slack 将这种类型的故障可以归类为灰色故障。在发生灰色故障时，系统可用性对于不同的组件来说是不一样的。例如，在上述案例中，受影响AZ内的系统看到其AZ内的后端完全可用，但AZ外的后端不可用。反过来，未受影响AZ内的系统看到受影响的AZ是不可用的。即使是同一个受影响的AZ内的客户端能够看到的后端可用性也是不一样的，这取决于它们的网络流量是否碰巧流经发生故障的设备。这就需要分布式系统在处理消息和提供小动物动图的同时处理好一切故障，这是一个相当复杂的任务。</p><p>&nbsp;</p><p>为了解决这个难题，Slack 团队决定改用一种基于单元（cell）的方法，其中每个可用区包含完全独立的后端部署，且后端组件仅限于单个可用区使用。这种方法使用一个基于 Envoy/xDS 的层将流量路由到 AZ 层面的单元。这种独立部署的方法在某种程度上是 Slack 平台异构属性的自然延伸，该平台使用了许多语言栈和服务发现接口，因此在后端服务之间引入复杂的路由逻辑会是非常麻烦的事情。</p><p><img src=\"https://static001.geekbang.org/infoq/44/449d4052128225b403378dddf13fb759.webp\" /></p><p></p><p>有了这个新架构后，Slack就可以快速将流量从出现问题的单元转移出去，因为配置更改只需几秒钟就能传播出去。流量可以渐次（以 1% 的粒度）且优雅地转移（所有正在进行的请求都在正在排出流量的单元中完成）。</p><p>&nbsp;</p><p>Slack 的设计路径遵循了 AWS 制定的指南，该指南在 AWS 的云原生架构系列中得到了深入探讨，其中作者提供了多种工具和技术，用来利用基于容器的计算服务提高云负载的可扩展性和弹性。AWS 架构师主张在云架构中建立强大的故障隔离边界，以应对黑天鹅事件并最大限度地减少意外故障的影响。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/31/319247571f01b71b978febe1b522d0e2.webp\" /></p><p></p><h2>从 73 小时故障中吸取教训，Roblox通过单元化架构改进用户体验</h2><p></p><p>&nbsp;</p><p>最近，在线游戏平台和创作系统Roblox也分享了其基于单元的架构走过的历程，以及进一步提高其平台弹性的未来计划。和Slack 一样，Roblox也是从一次故障中意识到，是时候采用单元化架构了。</p><p>&nbsp;</p><p>2021年10月，Roblox遭遇了一次长达73小时的系统范围故障，该故障最初源于一个数据中心的小问题，但迅速演变为大规模故障。事故后分析表明，团队需要加强其基础设施的稳固性，以应对流量峰值、天气条件、硬件故障、软件错误和人为失误等各种故障因素。重点在于防止单个组件问题扩散至整个系统，并确保网络或用户持续重试操作不会引发与负载相关的级联故障。</p><p>&nbsp;</p><p>为了应对类似2021年10月的故障，Roblox最初在多个区域的数据中心设置了基础设施的副本，采用主备方式。这意味着当主数据中心发生重大故障时，整个系统可以切换到备份基础设施上。这种方式提供了一种应急弹性，但Roblox的长期目标是实现从主备数据中心到双活数据中心的转型，使两个数据中心同时处理工作负载，提高可靠性和近乎即时的故障切换能力。</p><p>&nbsp;</p><p>Roblox还实现了单元化架构，在数据中心内建立坚固的“防爆墙”，以防止发生整个数据中心范围的故障。Roblox的目标是将所有服务迁移至单元中，以增强弹性和高效的工作负载管理。整个单元（每个单元可能包含1400台服务器）可在必要时进行修复或完全重新配置。这一过程需要确保一致性，要求服务进行容器化，并实现基础设施即代码的理念。Roblox新的部署工具会自动确保服务跨单元分布，从而使服务所有者无需考虑复制问题。</p><p>&nbsp;</p><p>Roblox 将单元作为一种防火门，可以将故障限制在一个单元内。目标是使单元变得可互换，以便在出现问题时更快地恢复。然而，管理单元之间的通信存在一些挑战，因为需要防止“死亡查询”，即重试查询会导致级联故障。他们正在部署短期解决方案，例如将计算服务的副本部署到每个计算单元中，并在单元间平衡流量，以此来缓解这种情况。他们的长期计划包括实现用于服务发现的下一代服务网格以及将依赖请求定向到与原始调用方相同单元的方法。这将降低故障从一个单元传播到另一个单元的风险。70% 的后端流量现在由单元提供，他们的最终目标是达到 100%。近 3 万台服务器正在运行单元，但这还不到总服务器数量的 10%。</p><p>&nbsp;</p><p>在不中断用户的情况下迁移一个非常繁忙的在线平台的复杂性是巨大的。由于没有大量的资金购买全新的服务器来运行单元化架构基础设施，Roblox 创造性地利用了一小部分备用机器，并策略性地建立了新的、单元，逐步迁移工作负载，然后重新使用已释放的机器来进行下一次迁移。这在不同的数据中心之间造成了一些理想的单元碎片，增加了单元内的弹性。Roblox 预计将于 2025 年完成迁移，他们需要强大的工具来部署均衡的服务，并且不会干扰到用户，他们还需要进行详尽的测试，确保在单元化架构中运行的新服务的兼容性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e6eba328605b95ac54f42dc5d6a9a3d0.png\" /></p><p></p><p>Roblox的努力取得了显著成果，但针对单元化架构的工作仍在进行中。他们致力于在不断扩展规模的过程中提高效率和弹性，主要成就包括建立第二个数据中心，在主备数据中心创建单元，将超过70％的后端服务流量迁移到单元中，以及建立实现一致性的要求。</p><p>&nbsp;</p><p>2023年9月，Roblox在数据中心启动了双活实验，增强了可靠性并最大限度地缩短了故障转移时间。这些成果让他们获得了一个实现全面双活基础设施的计划，确定了改进系统设计的模式。</p><p>&nbsp;</p><p>Roblox一直致力于提升效率和弹性，设想让平台成为数百万用户可靠、高性能的实用工具，并实现实时连接十亿人。他们的基础设施目前运行在近14万5千台服务器上（大部分在本地私有混合云中心）——两年内增长了三倍。目前，Roblox正在努力改造基础设施，使平台更具弹性、更加高效，为数百万用户提供服务，为持续的增长和创新奠定基础。</p><p></p><h2>单元化架构已存在至少十年</h2><p></p><p>&nbsp;</p><p>Slack 采用单元化架构的举措在社区中引发了广泛的讨论。用户esprehn认为，单元并不是要防止可用区故障，而是要对生产基础设施进行分区，以防止出现错误的部署和配置更改。每个 AZ 被分为许多不同的单元。另一位名为 ignoramous 的用户（自称前 AWS 员工）强调，对基于单元的架构的指南是来源于 Amazon 和 AWS 在云中为提供弹性所做的努力。</p><p>&nbsp;</p><p>用户 tedd4u 指出基于单元的架构已经存在至少 10 年了，并附上了一篇发表于 2012 年的文章链接。文中提到，面向服务的架构（SOA）带来了对大规模提供服务的迫切需求。为了满足这些需求，一种鲜为人知的架构技术应运而生——单元化架构。</p><p>&nbsp;</p><p>在传统的服务化架构下（如下图），服务是分层的，每一层使用不同的分区算法，每一层都有不同数量的节点，上层节点随机选择下层节点。当然这个随机是比较而言的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb149a6e83a4c81d5621db8bdf411eff.png\" /></p><p></p><p>而在单元化架构下，服务虽然分层划分，但每个单元自成一体。按照层次来讲的话，所有层使用相同的分区算法，每一层都有相同数量的节点，上层节点也会访问指定的下层节点。因为他们已经在一起。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c7346d3beb7619f85ad2838bdf311f76.png\" /></p><p></p><p>具体来说，单元化架构具有以下优点：</p><p></p><p>单元提供了一个可并行处理的单元，能够随着用户群的增长而按需调整大小。当需要更多容量时，可以增量式地添加单元。单元之间相互隔离，一个单元的故障不会影响到其他单元。单元内的存储和应用能力独立于其他单元，提供了很好的隔离性。单元提供了多种功能，如测试升级、滚动升级以及在不同版本软件间进行测试的能力。单元可以发生故障、进行升级，并且可以独立于其他单元分布在数据中心内。</p><p>&nbsp;</p><p>在2012年，就已经有包括Facebook在内的多家公司采用单元化结构：</p><p></p><p>Tumblr：用户被映射到单元中，每个数据中心内存在多个单元。每个单元都有一个HBase集群、服务集群和Redis缓存集群。用户被限定在一个单元内，所有单元通过Firehose更新来获取所有帖子。后台任务使用Firehose来填充表和处理请求。每个单元存储所有帖子的单个副本。Flickr：使用联合方法，其中所有用户数据都存储在一个分片上，该分片由不同服务的集群组成。Facebook：消息服务将称为单元的机器和服务集群作为其系统的基本构建块。每个单元由ZooKeeper控制器、应用程序服务器集群和元数据存储组成。Salesforce：Salesforce是根据Pod构建的。Pod是独立的功能集，由50个节点、Oracle RAC服务器和Java应用程序服务器组成。每个Pod支持数千名客户。如果Pod发生故障，只有该Pod上的用户会受到影响。</p><p>&nbsp;</p><p>采用单元化架构的关键在于创建可扩展且强大的服务，以提高系统的平均故障间隔时间（MTBF）。这种服务可以用作由可编程编排层协调的其他服务系统的基础组件，无论是在数据中心还是云环境中都同样有效。如果团队正在寻找更高级的组织模式，那么单元化架构是一个很好的选择。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/slack-cell-based-architecture/\">https://www.infoq.com/news/2024/01/slack-cell-based-architecture/</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=37274871\">https://news.ycombinator.com/item?id=37274871</a>\"</p><p><a href=\"https://www.infoq.cn/article/rBofrx6EO9VkId1mmmO3?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">https://www.infoq.cn/article/rBofrx6EO9VkId1mmmO3</a>\"</p><p><a href=\"http://highscalability.com/blog/2012/5/9/cell-architectures.html\">http://highscalability.com/blog/2012/5/9/cell-architectures.html</a>\"</p>",
    "publish_time": "2024-01-25 14:09:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI演讲：如何通过API将大模型集成到自己的应用程序中",
    "url": "https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d",
    "summary": "<p></p><p>OpenAI的员工Sherwin Wu和Atty Eleti在QCon上讨论了如何使用OpenAI API将这些大语言模型集成到应用程序中，并通过使用API和工具将GPT连接到外部世界以扩展GPT的功能。</p><p></p><p>Atty Eleti：我想带大家回到1973年，也就是50年前。1973年，《科学美国人》（Scientific American）发表了一篇非常有趣的文章，他们在文章中比较了各种动物的运动。他们着手比较运动的效率。换句话说，一只动物从A点到B点燃烧了多少卡路里，与它们的体重等是否有关？他们比较了各种动物，鸟类、昆虫，当然还有我们人类，并将它们根据效率从高到低进行了排名。他们发现，就运动的效率而言，秃鹫的最高。</p><p>&nbsp;</p><p>秃鹫是一种美丽的鸟类，原产于加利福尼亚州和南美洲的一些地区，有时它可以飞数百英里而无需扇动翅膀。它具有非常好的滑翔能力。另一方面，人类行走，在榜单中的排名相当平庸，大约排在榜单三分之一的位置。《科学美国人》这篇文章的精妙之处在于，除了所有物种之外，他们还增加了一个项目，那就是骑自行车的人。骑自行车的人在竞争中大获全胜，击败了所有竞争对手，其运动效率几乎是秃鹫的两倍。</p><p>&nbsp;</p><p>我很喜欢这个故事，因为它有一个很简单的认识，只要用一点工具，有一点机械帮助，我们就能极大地增强我们的能力。你们中的一些人可能以前听过这个故事。你可能会想，我是在哪里看到的？这个故事是苹果公司创立之初史蒂夫·乔布斯（Steve Jobs）经常讲的。他和苹果团队利用这个故事作为早期Macintosh的灵感来源。史蒂夫比较了这个故事，并说到：“人类是工具的制造者。”</p><p>&nbsp;</p><p>我们制造了像自行车这样的工具来增强我们完成任务的能力。就像自行车是运动的工具一样，计算机也是我们思维的工具。它增强了我们的能力、创造力、想象力和生产力。事实上，史蒂夫曾经用这个神奇的短语来形容个人计算机。他说：“计算机是思维的自行车”。这篇文章发表十年后的1983年，苹果公司发布了Macintosh，并掀起了个人计算的革命。当然，多年后的今天，我们仍然每天都在使用mac电脑。</p><p>&nbsp;</p><p></p><h1>2023——人工智能和语言模型</h1><p></p><p>那是1973年。现在是2023年，50年后，计算已经发生了很大的变化。如果《科学美国人》的工作人员再次进行这项研究，我敢打赌他们会在名单上再增加一个“物种”。对我们大多数人来说，这个“物种”在公众的想象中只存在了大约六个月的时间。我谈论当然是人工智能，或者具体来说是语言模型。</p><p>&nbsp;</p><p>自去年11月ChatGPT推出以来，人工智能和语言模型已经在全球范围内引起了公众的广泛关注。更令人兴奋的是，它们吸引了世界各地开发者的想象力。我们已经看到很多人将人工智能集成到他们的应用程序中，使用语言模型来构建全新的产品，并提出与计算机交互的全新方式。自然语言交互终于成为了可能，并且质量很高。但这存在局限性，也存在问题。对于任何使用过ChatGPT的人来说，我们都知道它的训练数据是2021年9月之前的，所以它不知道当前的事件。</p><p>&nbsp;</p><p>在大多数情况下，像ChatGPT这样的语言模型是根据训练中的记忆进行操作的，因此它们与当前事件或所有API、我们每天使用的自己的应用程序和网站无关。或者，如果你在一家公司工作，它不会连接到你公司的数据库和你公司的内部知识库等等。这使得语言模型的使用受到了限制。你可以写一首诗，可以写一篇文章，可以从中得到一个很棒的笑话，可以搜索一些东西。但如何将语言模型与外部世界联系起来呢？如何增强人工智能的能力，让它来代表你执行行动，让它做比它固有能力更多的事情呢？</p><p>&nbsp;</p><p></p><h2>概述</h2><p></p><p>如果计算机是思维的自行车，那么人工智能思维的自行车是什么？这就是我们要探讨的问题：一辆人工智能思维的自行车。我们将讨论GPT，这是OpenAI开发的一组旗舰语言模型，以及如何将它们与工具或外部API和函数集成，以支持全新的应用程序。我叫Atty。是OpenAI的一名工程师。Sherwin是我的搭档，我们是OpenAI的API团队的成员，共同构建了OpenAI API和其他各种开发者产品。</p><p>&nbsp;</p><p>我们将讨论三件事。首先，我们将讨论语言模型及其局限性。我们将快速介绍它们是什么以及它们是如何工作的。先培养下对它们的直观认识。然后还要了解它们的不足之处。其次，我们将讨论我们发布的一个全新特性，即使用GPT进行函数调用。函数调用是将OpenAI的GPT模型插入外部世界并让它执行操作的方式。最后，我们将通过三个快速演示样例来演示如何使用OpenAI模型和GPT函数调用功能，并将其集成到公司产品和辅助项目中。</p><p>&nbsp;</p><p></p><h2>大语言模型（LLMs）及其局限性</h2><p></p><p>Sherwin Wu：首先，我想对LLM做一个非常高层级的概述：它们做什么，它们是什么，它们如何工作。然后再谈谈它们开箱即用的一些限制。对于那些已经关注这个领域一段时间的人来说，这可能是你们都知道的信息，但我只是想在深入讨论细节之前确保我们都能达成共识。</p><p>&nbsp;</p><p>非常高层级的GPT模型，包括ChatGPT、GPT-4、GPT-3.5-turbo，它们都是我们所说的自回归语言模型。这意味着它们是巨大的人工智能模型，它们接受过庞大的数据集的训练，包括互联网、维基百科、公共GitHub代码和其他授权材料。它们被称为自回归，因为它们所做的只是综合所有这些信息。它们接受一个prompt，或者我们可以称之为上下文。它们查看prompt。然后它们基本上只是决定，给定这个prompt，给定这个输入，下一个单词应该是什么？它实际上只是在预测下一个单词。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a281b92260f77472c4dace8f5218988.png\" /></p><p></p><p>&nbsp;</p><p>例如，如果给定GPT的输入是，“the largest city in the United States is“（美国最大的城市是），那么答案就是New York City（纽约市）。它会一个字一个字地思考，它会说“New”、“York”，然后是“City”。同样，在更具对话性的环境中，如果你问它地球和太阳之间的距离是多少。GPT 已经从互联网上学过这个，它将输出9400万英里。它是根据输入逐个单词逐个单词思考的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1a/1a708f4455415a53608f886bd7cbdc35.png\" /></p><p></p><p>&nbsp;</p><p>在底层，它真正做的是每次输出单词时，都会查看一堆候选单词并为它们分配概率。例如，在最初的例子中，“美国最大的城市是”，它可能有很多候选城市，New代表“纽约”（New York），或者“新泽西”（New Jersey），或者其他什么，Los代表“洛杉矶”（Los Angeles），然后还有其他一些可能的例子。你可以看到，它确实认为“New York City”（纽约市）可能是正确的答案，因为New的概率为95%。在这种情况下，它通常会选择最有可能的结果，所以它会选择New，然后继续前进。这个单词出现后，我们现在就知道New是第一个单词，所以它对下一个单词是什么就有了更多的限制。</p><p>&nbsp;</p><p>我们可以看到，现在它认为New York（纽约）的可能性要高得多，但它也在考虑New Brunswick（新不伦瑞克）、New Mexico（新墨西哥）和New Delhi（新德里）等。直到完成第二个单词，这基本上是模型的叠加。它基本上知道答案是New York City，概率几乎是100%。但它仍在考虑其他一些剩余概率很低的选项，比如County（县）、New York Metro（纽约地铁）、New York Times（纽约时报），但最终它选择了City并给出答案。</p><p>&nbsp;</p><p>对于更机敏的LLM人士来说，这在技术上过于简单化了。我们并不是真正在预测单词，而是在预测token，比如单词片段，这实际上是一种更有效的表达英语的方式，主要是因为单词片段会在一堆不同的单词中重复，而不是单词本身会重复。但概念仍然是一样的。LLM在这种上下文中，很可能会连续输出一堆不同的token。就是这样，这就是这些语言模型的真正含义。了解了这一点，我认为让我们很多人感到惊讶的疯狂之处在于，我们只需预测下一个单词就可以走得很远。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/5673c0fe7089d2faab377baf358f7d0d.png\" /></p><p></p><p>&nbsp;</p><p>这张图表来自我们今年3月发布的GPT-4博客文章，它显示了我们最有能力的模型GPT-4在各种专业考试中的表现。这实际上只是GPT-4根据问题预测下一个单词。你可以看到，在很多不同的考试中，它的表现实际上和人类一样，甚至超过了人类的表现。y轴是考生的百分位数。在AP考试、GRE考试、LSAT考试、美国生物奥林匹克竞赛等一系列不同的考试中，它基本上处于第80个百分位，有时甚至是第90个百分位，甚至是第100个百分位。</p><p>&nbsp;</p><p>在这一点上，很多这样的测试我甚至都做不到，所以GPT-4远远超出了我自己的能力，而这只是来自对下一个单词的预测。这真的太酷了。你可以用它构建很多很酷的东西。任何一个已经学习了LLM一段时间的人都会意识到，我们很快就会遇到一些限制。当然，最大的一个是开箱即用的LLM或GPT实际上是一个装在盒子里的人工智能。它无法进入外部世界。它不知道任何其他信息。它就在那里，有它自己的记忆。感觉就像你在学校里参加考试时，只有你和考试，你只能根据记忆来回忆一些东西。</p><p>&nbsp;</p><p>想象一下，如果考试是开放的，你可以使用手机或类似的东西，你会做得更好。GPT今天真的只是在它自己的盒子里。正因为如此，作为工程师，我们希望使用GPT并将其集成到我们的系统中。限制GPT，不允许它与我们的内部系统对话，这对于你可能想做的事情来说是非常有限的。此外，即使它确实可以访问某些工具，因为语言模型是概率性的，有时也很难保证模型与外部工具交互的方式。如果你有一个API或其他你想要使用的东西，当前模型不能保证总是能与你API可能想要的输入相匹配时，这最终也会成为一个问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/2580bcf915d852712ba15dc55c330e14.png\" /></p><p></p><p>例如，如果我正在构建一个应用程序，并将此输入提供给GPT，基本上就是说，下面是一个剧本的文本，从中提取一些信息，并以这种JSON格式对其进行结构化。我真的只是给它一个剧本，让它推断出一种类型和一个子类型，以及其中的一些角色和年龄范围。我真正想要的是，我希望它能输出像这样的东西。就像JSON输出一样。</p><p>&nbsp;</p><p>也许这是一个关于哈利波特的浪漫故事之类的剧本。它知道这是浪漫的，青少年的浪漫，它看到罗恩（Ron）和赫敏（Hermione），并以这种JSON格式准确输出。这太棒了，因为我可以获取这个输出，现在我可以使用它并将其放入API中。然后我就像在我的代码中一样，一切都正常。问题是，它大概只有80%、70%的概率是这样的。</p><p>&nbsp;</p><p>在剩下的时间里，它会尝试并提供额外的帮助，做一些像这样的事情，它会说：“当然，我可以为你做。下面是你要求的JSON格式的信息。”这是非常有用的，但如果你试图将其插入到API中，它实际上室不起作用的，因为前面所有这些随机文本，你的API并不知道如何解析它。这显然是非常令人失望的。这不是你真正想要的。我们真正想做的是，帮助GPT打破常规，或者给GPT一辆自行车或另一套工具来真正增强它的能力，并让它无缝地工作。</p><p>&nbsp;</p><p></p><h2>使用GPT进行调用函数</h2><p></p><p>这就把我们带到了下一部分，那就是我们所说的GPT函数调用，这是我们发布的API的一个新变化，它使函数调用能够以一种非常一流的方式更好地使用我们的GPT模型。举个例子，如果你问GPT这样的问题，what's the weather like in Brooklyn today? （今天布鲁克林的天气怎么样？）如果你问一个普通的GPT这个问题，它基本上会说，“作为一个由OpenAI训练的人工智能模型，我无法提供实时信息。”这是真的，因为它实际上无法访问任何东西。它在一个盒子里。它怎么会知道今天天气怎么样呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/76c19cc7f6ed1d6b67c222e0be0fc12f.png\" /></p><p></p><p>&nbsp;</p><p>这显然确实限制了它的能力，这是不可取的。我们所做的是更新了GPT-4和gpt-3.5-turbo模型或旗舰模型。我们收集了大量的工具使用和函数调用数据，根据这些数据对我们的模型进行了微调，使其真正擅长选择是否使用工具。最终的结果是我们发布了一组新的模型，这些模型现在可以为你智能地使用工具和调用函数。在这个特殊的例子中，当我们询问模型“今天布鲁克林的天气怎么样？”时，我现在能做的就是解析这个输入，同时告诉它一组函数，或者在本例中，告诉它它可以访问的一个函数，如果需要帮助，它应该尝试并调用这个函数。在本例中，我们将为它提供一个名为get_current_filther的函数。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43e2a8a9cd6b458f9047cdbaa76a12a1.png\" /></p><p></p><p>它接收一个带有location（位置）的字符串，然后它就知道它可以使用这个。在本例中，在这个新的世界里，当你解析此输入时，GPT将表达它打算调用get_current_filther函数的意图。然后，你可以根据需要在自己的系统中自行调用该函数。假设你得到的输出是 “22 Celsius and Sunny”（22摄氏度和阳光明媚）。你可以将其解析回GPT，它会综合这些信息，并返回给用户说：the weather in Brooklyn is currently sunny, with a temperature of 22 degrees Celsius（目前布鲁克林天气晴朗，温度为22摄氏度。）</p><p>&nbsp;</p><p>稍微解释一下，真正发生的事情是GPT知道一组函数，并且它会智能地自行表达调用其中某个函数的意图。然后执行调用，并将其解析回GPT。这就是我们最终将它与外界联系起来的方式。为了进一步了解它在高层级上到底发生了什么，其实它仍然就像是一个来回，你的用户问了一个问题，发生了很多事情后，你对你的用户做出了回应。你的应用程序在底层实际做的事情将经历一个三步的过程，首先调用 OpenAI，然后使用你自己的函数，最后再次调用OpenAI或GPT。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/de2e19a01388973a0dc7a6ca59fe9d6a.png\" /></p><p></p><p>&nbsp;</p><p>第一步，显然是用户问了一个问题，在本例中，问题是what's the weather like in Brooklyn today?（“今天布鲁克林的天气怎么样？”）然后下一步是，在应用程序中，调用模型，调用OpenAPI，并非常具体地告诉它它可以访问的函数集以及用户输入。这是一个API请求的例子，目前它实际有效且可正常工作，任何具有API访问权限的人都可以尝试该操作。这是一个使用函数调用能力的curl示例。我们可以看到，这只是我们聊天完成端点的正常curl，这是我们发布的一个新的API端点，为我们的GPT-4和GPT-3.5模型提供支持。你curl该API。它会在模型中进行解析。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a02129b0f8e597f1fc1f439cca209731.png\" /></p><p></p><p>在本例中，我们将在gpt-3.5-turbo-0613中进行解析，它代表6月13日，一个我们发布的模型。这是一个能够进行函数调用的模型。我们还在解析一组消息。对于那些可能不熟悉我们聊天完成格式的人，你可以将其解析到我们的模型中，基本上是一个消息列表，也就是对话记录。</p><p>在本例中，实际上只有一条消息，没有历史记录。它只是用户询问“今天布鲁克林的天气怎么样”。你可以想象，随着对话的变长，它可能是一个包含5到10条消息的列表。我们正在解析消息，模型将能够看到历史记录并对此做出回应。那么，这里的新事物就是函数。</p><p>&nbsp;</p><p>这是一个我们现在可以解析的新参数，我们在这里解析的是，我们列出了这个模型应该知道的一组函数，它应该可以访问的函数集。在本例中，我们只有一个函数，它就是get_current_tweather函数。我们在这里还放了一个自然语言描述。我们说这个函数可以获取特定位置的当前天气。我们还需要输入函数签名。并且我们告诉它有两个参数。一个参数是location（位置），这是一个字符串，包含城市和州，格式是这样的：旧金山，加州（San Francisco, California.）。另一个参数时unit（单位），即摄氏度（Celsius）或华氏度（Fahrenheit）。</p><p>&nbsp;</p><p>在这里首屏的下面，还有另一个参数，该参数表示唯一必须的属性是位置。从技术上讲，你只需要解析位置，这里不需要单位。我们将该请求解析到GPT，然后GPT将作出响应。在过去中，GPT可能只会以文本形式进行响应。它会说：“我不能这样做，因为我没有访问权限。”在本例中，我们的API响应的是调用天气函数的意图。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/2339e79558c3eda24de50f9a9e11bcd4.png\" /></p><p></p><p>&nbsp;</p><p>这里真正发生的事情是GPT凭自己的直觉，为了弄清楚今天的天气，我自己做不到，但我可以访问get_current_weither这个函数，所以我会选择调用它，所以我要表达要调用它的意图。此外，如果你还没有真正注意到的话，GPT在这里所做的是，它在这里构造参数。我们可以看到它在告诉我们，它想调用get_current_tweather，它想用参数位置（Brooklyn, New York；纽约布鲁克林）来调用该函数。</p><p>&nbsp;</p><p>它所做的就是看到函数签名，并为其创建请求。然后还算出布鲁克林在纽约，然后用这种方式构造字符串。它把这一切都弄清楚了。至此，GPT就表达了现在要调用函数的意图。下一步是，我们要弄清楚我们到底想要如何调用这个函数。我们可以根据特定参数从get_current_tweather的函数调用中获取相应的返回值。然后我们可以自己执行。它可以是本地的，在我们自己的Web服务器上运行。它也可以是系统中的另一个API，还可能是一个外部API，我们可以调用weather.com API。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/57bc52645179b986fb8dc2fe597ac591.png\" /></p><p></p><p>那么在这个例子中，我们调用了一些东西，可能是一个内部API，它返回的输出是我们看到的是22 degrees Celsius and Sunny（22摄氏度和晴天）。给定了模型的输出，就可以开始这个过程中的第三步，即调用模型，用函数的输出调用GPT，然后查看GPT想要做什么。在本例中，我谈论的是消息。这次，我们在向OpenAI API发送的第二个请求中添加了几条消息。最初，只有一条信息，那就是“今天布鲁克林的天气怎么样？”，现在再添加两条新消息来表示函数调用时所发生的情况。</p><p>&nbsp;</p><p>第一个基本上是对意图的重申，所以基本上是说助理或GPT想要用纽约布鲁克林的这个参数来调用get_current_tweather函数。然后，我们还添加了第三条消息，它基本上说明了我们所进行的函数调用的结果，因此这是get_current_filther的结果。然后，内联这里输出的数据，即温度“22”、单位“摄氏度”和描述“晴天”，然后将所有数据解析给GPT。在此时，GPT接收了它，并决定它想要做什么。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52c47226a60424b8e67d5dfe8869db8e.png\" /></p><p></p><p>此时，模型已经足够智能了，它能够意识到“我将调用这个函数。这是输出。我实际上已经掌握了实际完成请求所需的所有信息。”它现在最终会通过文本方式来做出回应，并显示“今天布鲁克林天气晴朗，温度为22摄氏度”。这时，我们终于得到了GPT的最终输出。然后我们就可以回应我们的用户了。</p><p>&nbsp;</p><p>将所有这些放在一起，我们最终会得到我们理想中的体验，即用户询问“今天布鲁克林的天气怎么样？”我们的服务器会思考一下，GPT表达意图，我们完成完整的三步过程，调用了我们的函数。最终，用户看到的是“今天布鲁克林天气晴朗，气温为22摄氏度。成功”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>演示1——将自然语言转换为查询</h2><p></p><p>Eleti：我们刚刚介绍了几个入门性的主题。首先，我们了解了语言模型是如何工作的，以及它们的一些局限性，因为它们没有所有的训练数据，它们没有连接到外部世界，它们的结构化输出并不总是可解析的。Sherwin还向我们介绍了新特性、函数调用和API的工作原理，以及如何将函数解析为API并获取输出，以及如何让GPT以面向用户的方式来总结响应。让我们通过几个演示来了解如何将所有这些组合起来，并将其应用到我们的产品和应用程序中。</p><p>&nbsp;</p><p>让我们从小事做起。我们将介绍的第一个示例是将自然语言转换为查询的内容。我们的示例是，假设你正在构建一个数据分析应用程序或商业智能工具，比如Tableau或Looker。你们中的一些人可能很擅长SQL，但我肯定不擅长了。大多数情况下，我只想问数据库，谁是顶级用户，然后得到响应。今天终于有可能了。我们将使用GPT，将给它一个称为SQL查询的函数，它只需要一个参数，即一个字符串“query”。</p><p>&nbsp;</p><p>它应该是针对我们数据库的一个有效SQL字符串。让我们看看它是如何工作的。首先，我们将为模型提供一条系统消息，描述它应该做什么。我们称之为SQL GPT，可以将自然语言查询转换为SQL。当然，模型需要访问数据库模式。在本例中，我们有两个表，用户表（users）和订单表（orders）。用户表有姓名、电子邮件和生日。订单表有用户ID、购买金额和购买日期。现在我们可以开始使用一些自然语言来查询数据库了。</p><p>&nbsp;</p><p>我们来问这样一个问题“根据上周的消费金额，找出排名前10的用户姓名”（get me the names of the top 10 users by amount spent over the last week.）。这是一个相当正常的业务问题，当然不是我可以立即编写SQL就能解决的问题，但GPT可以。让我们运行一下。我们可以看到它正在调用SQL查询函数。它有一个参数“query”，它创建了一个漂亮的SQL查询。它是选择了名称和金额的总和；它连接到订单表；并获取最后一周的订单，按总花费进行排序，并将其限制为10个。这看起来是正确且恰当的。让我们在数据库中运行一下它。我们得到了一些结果。</p><p>&nbsp;</p><p>当然，这是JSON格式的，因此用户无法渲染它。让我们把它发送回GPT看看它说了什么。GPT总结了这些信息，并表示“这些是按消费金额排名前十的用户。这是他们上周的花费，包括Global Enterprises, Vantage Partners。”这是一个了不起的用户可读的答案。</p><p>&nbsp;</p><p>我们要对GPT给予的帮助表示感谢。我们说“谢谢”，GPT说“不客气”。这是一种快速的方法，它可以了解完全的自然语言、完全的自然语言查询是如何将结构化输出转换为有效的SQL语句的，我们在数据库中运行该语句，获取数据，并将其汇总回自然语言。我们当然可以在此基础上构建数据分析应用程序。</p><p>&nbsp;</p><p>你还可以构建其他的内部工具。Honeycomb最近为Honeycomb查询语言构建了一个非常相似的工具。这是使用GPT和函数将自然语言转换为查询的一个示例。</p><p>&nbsp;</p><p></p><h2>演示2——调用外部API和多个函数</h2><p></p><p>让我们来做第二个演示。这是关于将外部API和多个函数一起调用的。我们提高了复杂度。假设我们正在纽约参加一个会议，我们想预订今晚的晚餐。我们将使用两个函数来调用GPT。第一个是get_current_location。它在设备上本地运行，比如在你的手机或浏览器上，并获取你所在位置的纬度（Lat）和经度（Long）。第二个函数是Yelp搜索，它使用Yelp的API，也就是流行餐厅评价应用程序，我们可以对纬度、经度和查询进行解析。</p><p>&nbsp;</p><p>我们来运行一下这个演示。本例中的系统消息相当简单。它所说的就是我们的私人助理，来帮助用户完成任务，把GPT变成了一个有用的助手。我说“我正在参加一个会议，想在附近吃晚饭，有什么选择吗？我的公司会支付这笔费用，这样我们就可以尽情享受了”。让我们用GPT来运行一下它，看看它是如何做的。</p><p>&nbsp;</p><p>当然，GPT不知道我们在哪里，所以它说get_current_location，我们将调用本地API来获取我们的纬度和经度。我们已经获取到了。是纽约的布鲁克林（Brooklyn, New York）的某个地方。我们会将其返回给GPT，看它怎么说。它已经有了所需的信息，现在它想调用Yelp，它说“纬度、经度和查询”，并且会说“美食”。这很好。这就是我想要的。让我们调用Yelp并获取一些数据。</p><p>&nbsp;</p><p>我们从Yelp API中获取了一堆餐馆。当然，我希望它能给出一个漂亮的总结，所以让我们再次运行它。它回复说“你附近有一些高档餐饮可选择，La Vara、Henry's End、Colonie、Estuary”。上面还写着“请检查营业时间，尽情用餐。”这听起来很美味。再次感谢GPT帮助我们组织今晚的晚宴。</p><p>&nbsp;</p><p>这是一个使用GPT和函数调用外部API（在本例中为Yelp API）以及协调多个函数的示例。它能够凭借推理能力解析用户意图，并依次执行多个步骤的操作，以实现最终目标。</p><p>&nbsp;</p><p></p><h2>演示3——将高级推理与日常任务相结合</h2><p></p><p>第三个演示，让我们来进一步加强。我们讨论了GPT-4是如何通过SAT和GRE的。如果可以的话，它一定比仅仅调用Yelp API或编写一些SQL更聪明。让我们来测试一下。我们都是工程师，我们每天都有很多事情要做。我们必须要做的任务之一是拉取请求审查。我们必须审查同事的代码。如果GPT能帮助我，减轻我的工作量，那就太棒了。我们将做一个GPT的演示，它可以进行拉取请求审查，有点像构建自己的工程师。</p><p>&nbsp;</p><p>我们只需要一个函数submit_comments。它接受一些代码并返回一个要审查的评论列表，包括行、数字和评论。你可以想象，我们可以将其发送到GitHub API或GitLab API，并发布一堆评论。当然，你还可以添加更多的功能以使其更强大。让我们看看它是如何做的。</p><p>&nbsp;</p><p>在本例中，prompt有点长。我们向上滚动着看下。我们说：“GPT，你记录、审查rot，查看其差异并生成有关更改代码的审查评论，保留所有代码审查评论和相应的行号。”我们在这里也卖弄下个性。我们说toxicity为10分之0，其实我们不希望这样。</p><p>&nbsp;</p><p>为了好玩，让我们在snark上尝试10分之8。我们都认识一些表现出这些个性的工程师。然后尝试10分之2。让我们从这里开始吧。下面是一些我们要审查的代码。它是SaaS应用程序中的一个API方法，用于更改用户的权限。让我们运行一下它。我们看看GPT对这些代码有何看法。它给出了三条审查意见。我们可以看到它调用了submit_comments函数，并且它输出了完全有效的JSON。让我们看看上面写着什么。它说，“我们现在是在捉迷藏吗？”，“当角色不在身体里时会发生什么？”，“你在那里添加一个了小转折，你就直接访问了第一项。”</p><p>&nbsp;</p><p>我们只是随意地加入了数据库会话，是吗？这有点粗鲁。我们也不想那样。让我们来解决一下这个问题。我现在要退出并稍微修改一下prompt。要执行该操作，请退出。在幕后，我所做的就是返回prompt并更改这些的数字：toxicity，然后下一个，snark，我们将其恢复到0。我们并不希望这样。让我们礼貌一点。</p><p>&nbsp;</p><p>我们要把礼貌做到十分之十。好吧，再给我三条审查意见。它再次使用完全有效的JSON调用该函数。它说，“很高兴看到你检索角色值。”；“你的错误信息简洁明了。”；“我很感激你对数据库的更改，做得很好。”。我希望有人能这样审查我的代码。感谢GPT，我将退出了。这是第三个快速演示。</p><p>&nbsp;</p><p>从本质上讲，它仍然在做同样的事情。它调用一个函数，给出一些prompt，并对其做出响应。我们看到的是GPT的推理能力。GPT认识代码。它已经看到了成千上万行代码，可以给出很好的评价。如果你抛开一些个性的东西，它会指出错别字，指出潜在的错误案例和边缘案例。我们在这里将高级推理与日常任务相结合。它确实非常擅长编码。它在考试方面也确实非常出色，它的智力应用范围也很广。这实际上取决于开发人员的创造力，将其应用于尽可能困难的任务，并在此基础上循环运行。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>这是本次内容的快速总结。我们讨论了三件事。首先，我们讨论了LLM及其局限性。我们了解了LLM是如何工作的，它是token预测机。我们了解了它的局限性。它被时间限制住了。它并不总是输出结构化的输出等等。其次，我们了解了这个新特性，即使用GPT进行函数调用，这是对我们API和模型的更新。它允许模型表达何时调用函数的意图，并为我们构建有效的参数，然后在我们的终端上调用该函数。最后，我们浏览了一些演示。在某个时候，我会把公关的东西产品化。</p><p>&nbsp;</p><p>让我们回到开始的地方。我们谈到了史蒂夫·乔布斯的名言，他说“计算机是思维的自行车”。这对我来说确实如此，对你们所有人来说也都是如此。我们身处计算机行业，计算机改变了我们的生活。计算机增强了我们与生俱来的能力，给了我们更多的生产力、想象力和创造力。ChatGPT中的人工智能和语言模型还是个婴儿。它才出生几个月。我们有责任增强人工智能的思维，赋予它超越其内在推理能力的新能力，将其与工具连接，与API连接，并利用这一特性开发出真正令人兴奋的应用程序。</p><p>&nbsp;</p><p>原话对我来说非常有启发。我们永远无法公正地对待史蒂夫·乔布斯的名言。“我记得在我大约12岁的时候读过一篇文章，我想可能是在《科学美国人》上，他们在文章中测量了地球上所有这些物种的运动效率，它们从A点到B点需要消耗多少千卡热量。秃鹫赢了，位居榜首，超过了其他所有物种。人类排在榜单大约三分之一的位置，这对创造之冠来说并不是一个很好的表现。在那里有人有足够的想象力来测试人类骑自行车的效率。一个骑自行车的人把秃鹫吹走了，一直高居榜首。这给我留下了非常深刻的印象，我们人类是工具的制造者，我们可以制造出将这些固有能力放大到惊人程度的工具。对我来说，计算机一直是思维的自行车，它让我们远远超越了固有的能力。我认为我们只是处于这个工具的早期阶段，非常早期的阶段。我们只走了很短的一段距离，它仍处于形成阶段，但我们已经看到了巨大的变化。我认为，与未来100年发生的事情相比，这算不了什么。”</p><p>&nbsp;</p><p>就像50年前的计算机一样，我认为今天的人工智能也是如此。技术还处于起步阶段，所以我们很高兴看到它的发展。</p><p>&nbsp;</p><p></p><h1>问答</h1><p></p><p></p><h2>应对错误和失败的策略</h2><p></p><p>参会者1：我们应该如何应对错误和失败，你有什么建议的策略？以你的演示为例，在你构建SQL查询时，如果我提出的问题导致ChatGPT给出了一个在语法上完成正确，但在语义上完全不正确的SQL查询时，该怎么办？然后我向我的用户报告一些不正确的内容。很难告诉用户，这是错误的，但你有什么建议的策略来应对这个问题吗？</p><p></p><h1>&nbsp;</h1><p></p><p>Eleti：我认为首先，作为一个社会和这些语言模型的用户，我们必须了解它的局限性，几乎要围绕它的局限性来建立抗体。要知道输出可能是不准确的。我认为第二部分就像打开了盒子。我们已经将生产中的函数调用与ChatGPT集成在了一起。我们推出了一款名为插件的产品，它基本上可以做到这一点，它允许ChatGPT与互联网对话。我们要做的一件事是，如果最终用户愿意的话，那么所有的请求和响应都是可见的。这有助于信息部分。我个人认为SQL也是一个非常广阔的开放领域。我认为将其限制在仅在后端执行安全操作的知名API是一个好方法。你总是可以得到好的错误信息之类的。这些就是我即兴的建议。</p><p>&nbsp;</p><p></p><h2>LLM和langChain</h2><p></p><p>参会者2：有人尝试过做一些LangChain吗，它可以与LangChain一起使用吗？</p><p>Eleti：是的，事实上，LangChain、Harrison团队在我们推出一个小时后就发布了一个集成，所以它是有效的。</p><p>&nbsp;</p><p></p><h2>数据泄漏</h2><p></p><p>参会者2：这还暴露了一个泄漏问题。SQL示例就是一个很好的例子。如果有人读到这篇文章，他们对金融数据库进行SQL查询，并将其输入到gpt-3.5-turbo，我们基本上就泄露了数据。</p><p>如果你使用的是text-davinci-003或不同的模型，就会出现这样的问题，一些来自查询的数据会变成模型本身。在我看来，这个例子是极其危险的。</p><p>&nbsp;</p><p>Wu：实际上这存在一个误解，我认为我们最近没有作出很好地澄清，直到今年3月或2月，在我们为API提供的服务条款中，我们就说过“我们保留自己对API输入数据进行培训的权利”。我想这可能就是你所说的，就像你对一些SQL查询进行解析一样，它会在返回时以某种方式回到模型中。事实上，到目前为止，我们已经不再这样做了。根据我们的服务条款，我们实际上不会在API中对你的数据进行训练。我认为我们还没有把这一点说得非常清楚，所以人们对此非常偏执。到目前为止，还没有。你应该查阅我们的服务条款。我们不训练它。也就是说，解析的东西并不像企业级的那样。我们不会针对你的用户进行隔离。我们只是没有在自己的数据上训练它。这种围绕企业级数据隔离的特性显然很快就会出现。这一特定的安全层还没有出现。</p><p>&nbsp;</p><p>Eleti：我们不使用API数据进行训练。</p><p>&nbsp;</p><p></p><h2>函数调用的并行化</h2><p></p><p>参会者3：你展示的演示运行有点慢。我想知道，你们支持函数调用的并行化吗？就像现在你是串行的吗，你得到了这个函数签名，然后调用它，但假设ChatGPT说，三个函数应该同时被调用，这可行吗？</p><p>&nbsp;</p><p>Eleti：API实际上不支持多个函数调用。没有输出显示“调用这三个函数”。但你可以破解它。你只需要定义一个函数，让它调用多个函数，然后你提供一个签名，让模型调用它，即可实现调用多个函数，这完全是可行的。归根结底，我们仍然是使用模型的推理能力来输出一些文本。</p><p>&nbsp;</p><p></p><h2>模型上下文的预加载</h2><p></p><p>参会者4：在你给出的SQL示例中，你为其提供了一些可以访问的表。我们有没有办法可以让任何人的后续调用预加载所有上下文呢？</p><p>&nbsp;</p><p>Wu：有几个潜在的解决方案。我们有一个称为系统消息的功能，你可以在那里进行解析，它基本上设置了模型的整体对话上下文。但在当时的语境中它是完全颠倒的。目前，我们已经将上下文窗口增加到大约16000个token。你可以逐渐将更多内容压缩到系统消息中。该模型经过训练，会格外关注系统消息，以指导其做出回应。在本例中，Atty在系统消息中有两个表的模式。可以预见的是，你可以添加更多的内容来填充整个上下文。</p><p>&nbsp;</p><p>参会者4：这就是我们的预加载方式吗？</p><p>&nbsp;</p><p>Wu：是的，这是最简单的。还有一些其他的方法。你可以将它连接到外部数据源、数据库之类的。微调也是另一种选择。还有其他一些。</p><p>&nbsp;</p><p></p><h2>使用GPT进行可靠的函数调用</h2><p></p><p>参会者5：关于将GPT集成到不同的软件中。我在使用枚举时遇到了一些问题，当我要求它用英语、法语或德语做一些工作时，我使用的枚举有时会出现德语或法语。API函数也会发生这种情况吗？</p><p>&nbsp;</p><p>Eleti：是的，很不幸。模型在正常情况下以及在这种情况下都很容易产生幻觉。我们所做的基本上是对模型进行了微调，因此我们可以看到大约100000个关于如何可靠地调用函数的示例。它比你自己做的任何其他提示都要好得多。它仍然会生成参数，可能会输出无效的JSON，也可能会输出其他语言。为了防止这种情况，我们将进行更多的微调。我们也在探索一些低级推理技术来改进这一点。然后在你这边，你可以做prompt工程，只要提醒模型，不要输出德语，它会尽力的。</p><p>&nbsp;</p><p>Wu：看看它在这方面是否能做得更好，这会很有趣，尤其是如果你有一个函数签名，并且你明确列出了5个不同的英文枚举。较新的模型可能会更好，但也不完美。我不能百分百确定，不幸的是，我们没有跨英语、法语枚举那样的评估。这可能是一个值得思考的好问题，但我们很好奇，想看看它是否会变得更好。</p><p>&nbsp;</p><p></p><h2>GPT识别意图的能力</h2><p></p><p>参会者6：我有一个关于API理解意图能力的问题。函数调用是否有相似的温度（temperature）参数；如果我解析两个具有相似意图的函数，那么GPT对每个要调用的函数是否具有确定性；或者如果我多次询问，选择要调用哪个函数是否具有随机性？</p><p>&nbsp;</p><p>Eleti：随机性依然是存在的。归根结底，在底层，它仍然是一个token一个token地输出，选择要调用的函数。降低温度增加了确定性，但这并不能保证确定性。也就是说，API中有一个名为函数调用的参数，如果你知道你想让它调用哪个函数，实际上可以直接指定它，它肯定会调用该函数的。</p><p>&nbsp;</p><p></p><h2>函数调用权限</h2><p></p><p>参会者7：如果我们想限制某些用户进行某些函数调用，或者像你这样在这些SQL查询中访问某些表，你们有函数调用的权限吗，人们还需要实现他们自己的吗？</p><p>&nbsp;</p><p>Eleti：所有这些都会发生在你的服务器上，因为你拥有谁可以访问什么内容的完整上下文。这个API提供的只是GPT选择要调用哪个函数以及要使用哪些参数的能力。然后，我们希望你像对待任何其他客户端一样对待GPT的输出，因此对于不受信任的客户端输出，你可以在你的终端上验证其权限和内容。</p><p>&nbsp;</p><p></p><h2>思维链提示和约束采样</h2><p></p><p>参会者8：我只是想知道你是否可以详细说明一下这在底层发生了什么。这是底层的思维链吗？这是这些技术之上的一个有效的API层吗？</p><p>&nbsp;</p><p>Eleti：思维链提示是一种在给模型任务时的询问方式，首先，告诉我你要做什么，然后去做。如果你问“布鲁克林的天气怎么样？”它可能会说“我收到了一个天气请求，我将调用天气API”。然后它就这样做了。这是一种快速的工程技术。这是一个微调。随着插件的推出，我们收集了大约100000个用户问题和函数调用示例的内部和外部数据。这一切都在模型中进行了微调。这就是它的来源。</p><p>&nbsp;</p><p>我们还可以使用第三种技术，叫做约束采样，其中在token采样层，你可以确保预测的下一个token是值集中的一个。在JSON示例中，逗号之后必须是新行或类似的内容。我们可能会弄错，但是我们明白了，我们有语法要分配。这是我们正在探索的领域。这是从提示到微调再到更低层的东西的漫长旅程。这是让GPT输出可靠结构化数据的过程。</p><p>&nbsp;</p><p></p><h2>矢量数据库的兼容性</h2><p></p><p>参会者9：这可以与矢量数据库一起使用吗？我的想法是，我想根据我输入到向量数据库中的信息来约束信息，但它仍然能适用于函数逻辑？</p><p>&nbsp;</p><p>Eleti：是的，和以前一样好用。</p><p>&nbsp;</p><p></p><h2>函数调用是否公开可用？</h2><p></p><p>参会者10：我们今天就能使用它了吗？它现在对公众开放了吗？</p><p>&nbsp;</p><p>Wu：它是今天公开的，但有一个警告。它在gpt-3.5-turbo模型上可用。这里的任何人实际上都可以使用gpt-3.5-turbo访问函数调用，因为这是普遍可用的。它也可以在GPT-4 API上使用，但不幸的是，它仍然处于等待名单中。如果你不在该等待名单中，并且你可以访问GPT-4 API，那么你实际上可以使用GPT-4进行此操作。它在这方面做得更好。进度有点慢。如果你仍在等待名单上，或者你无法访问GPT-4 API，你今天可以在GPT-3.5-turbo上试用。</p><p>&nbsp;</p><p>查看更多<a href=\"https://www.infoq.com/transcripts/presentations/\">演示文稿字幕</a>\"</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/presentations/bicycle-ai-gpt-4-tools/\">https://www.infoq.com/presentations/bicycle-ai-gpt-4-tools/</a>\"</p>",
    "publish_time": "2024-01-25 14:38:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "代码屎山噩梦加速来袭，都是AI生成代码的锅？",
    "url": "https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL",
    "summary": "<p>“周边很多程度员一直在使用，都是用上就离不开了！”知乎上，在“大家现在使用哪些AI辅助编程工具？节省了多少工作量？”话题下，答主“以默”说道。</p><p>&nbsp;</p><p>按照“以默”了解的情况，AI辅助编程工具估计至少能帮程序员减少30%的工作量。对于工具，他表示“当然首选GPT，也可能是唯一答案！国产在这方面差距很大。”“综合能力水平: 4.0&gt;3.5&gt;国产大模型。模型能力越强，越好用！”</p><p>&nbsp;</p><p>现在用AI辅助编程已经是很多程序员的选择，但随着AI软件开发迅速普及，代码质量又会随之受到怎样的影响？⻓期代码研究员 Adam Tornhill 就曾表示担忧，AI辅助编程的主要挑战在于，它非常容易生成大量本来就不应该编写的代码。</p><p>&nbsp;</p><p>根据最新研究，结果确实令人忧心。除了代码返工（即代码在添加后不久即遭删除）以外，重复代码比例升高等问题愈发严重。</p><p></p><h2>主要让“添加代码”</h2><p></p><p>&nbsp;</p><p>自2021年6月推出beta版以来，GitHub Copilot已经掀起AI编码的一波流域。据公司CEO Thomas Dohmke介绍，该软件目前拥有超100万付费订阅开发者，已经让开发任务的速度提高了55%。而且在启用Copilot的文件中，有46%的代码量是由AI生成。</p><p>&nbsp;</p><p>根据来自开发者分析公司GitClear的研究，基于从1.5亿行已更改代码中收集到的数据，调查发现其中三分之二来自以匿名方式共享数据的私营企业，三分之一则来自谷歌、Facebook及微软等技术大厂的开源项目。</p><p>&nbsp;</p><p>这项研究着眼于经过添加、更新、删除、复制及移动的代码，并排除掉GitClear预先定义的“噪音”，例如被提交至多个分支的相同代码、空行及其他无意义的代码行。</p><p>&nbsp;</p><p>但GitClear的研究将关注重点放在代码质量、而非数量上，并观察到AI助手主要是在提供“代码添加建议，但很少涉及代码的更新、移动或删除建议”。</p><p>&nbsp;</p><p>研究人员还指出，“根据奖励设计，代码建议算法更倾向于提供最可能被采纳的建议”。尽管看似有理，但这明显忽略了代码简洁、易读等特性的重要意义。</p><p><img src=\"https://static001.geekbang.org/infoq/d6/d67e4520e1e054e976fbe8d8bf32b403.png\" /></p><p>GitClear分析得出的代码更改趋势</p><p>&nbsp;</p><p>对代码质量做精准衡量并不容易。研究人员也的确发现了一些变化趋势，表明代码的添加、删除、更新和复制/粘贴量大大提高，但代码移动比例却有所下降。他们还发现代码返工率大幅增加，从2020年的3.3%提升到目前的7.1%。</p><p>&nbsp;</p><p>一般来讲，代码移动是开发者进行代码重构的关键指标。具体来讲，就是在改进代码设计和结构的同时，确保不改变行为。</p><p>&nbsp;</p><p>研究人员初步猜测这种趋势可能与AI编码技术的日益普及相关，但真实原因仍有待验证。他们还严厉批评了大量复制/粘贴代码的负面影响，称“这种对AI生成代码的无脑使用，将对代码的长期可维护性产生灾难性的影响”。</p><p>&nbsp;</p><p>但过度使用复制/粘贴并不算是新问题。开发人员之所以这样做，很可能是因为无脑照搬比调整和重用现有代码更快、更省事，或者同一项目下多位开发者之间沟通不畅，抑或是从开发示例/编码问答网站上“抄袭”了太多内容。</p><p>&nbsp;</p><p>GitClear研究人员并没有具体讨论应如何解决调查中发现的这些问题，而是转向了“后续研究问题”。但他们也建议工程部门领导者应当“监督提交数据，并考虑其对未来产品维护造成的影响”。</p><p>&nbsp;</p><p>这次研究可能在一定程度上让那些担心被AI工具取代的开发者们感到放心。代码分析公司CodeScene最近开展的一项AI代码重构研究也得出结论，“在编码环境中，AI还远无法取代人类；当前的AI太容易出错，且完全不具备安全修改现有代码的水平。”</p><p></p><h2>代码质量，谁更应该关注</h2><p></p><p>&nbsp;</p><p>可以肯定的是，AI编码助手绝不会就此消失，反而是像一切新工具那样不断改进，并由开发者学习优化思路、改善使用效果。</p><p>&nbsp;</p><p>其实，现在开发者们也已经意识到了代码质量的问题。在GitHub 与 Wakefield Research 的调查报告中，当被调查的程序员被问到，“在积极使⽤⼈⼯智能时，应该根据哪些指标进⾏评估？”“代码质量”成为最关⼼的问题，</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/18/18784e8d5e1a34cd6205d5b389bb4bd3.png\" /></p><p></p><p>但另一方面，更应该关注代码质量问题的其实是公司领导层。</p><p>&nbsp;</p><p>“我公司的领导曾经就动过用代码行数衡量每个人的工作量这种想法。 研发人员每周代码量至少在500行以上，一个月必须在2000行以上。 甚至他还搞来了第三方的测算软件，输入git账号来计算你的代码量。然后在一次技术会议上，全体组员忍无可忍的怼了技术总监。“知乎上有网友分享到。</p><p>&nbsp;</p><p>一般公司考核代码量相对简单直观，但是代码质量考核就不那么容易了：满足用户需求，</p><p>合理的进度、成本、功能关系，具备扩展性和灵活性等都不是那么可量化的指标。</p><p>&nbsp;</p><p>但<a href=\"https://dl.acm.org/doi/abs/10.1145/3524843.3528091\">关于代码质量对业务影响的研究</a>\"表明，一般来说，由于技术债务和糟糕的代码，公司平均浪费了开发人员 23%～ 42%的时间。但似乎这还不够令人感到担忧，关于<a href=\"https://research.chalmers.se/publication/511450/file/511450_Fulltext.pdf\">软件开发人员由于技术债务而导致的生产力损失</a>\"的研究还发现，开发人员经常“被迫”引入新的技术债务，因为公司一直在用代码质量换取新功能等短期收益。</p><p>&nbsp;</p><p>现在企业为“降本增效”引入AI辅助工具是可以理解的，但需要注意扬长避短、合理使用。根据Alphacodium的说法，大模型生成单个冗长函数的结果很差，代码通常包含错误或逻辑错误，大模型也往往在需要思考、推理并做出严格、重要决策的代码任务中遇到困难。</p><p>&nbsp;</p><p>代码生成与其他对话不同，它需要匹配目标语言的精确语法、识别最佳路径和边缘情况、关注问题规范中的众多小细节，并解决其他特定于代码的问题和要求。因此，在自然语言生成中许多优化和技巧可能对代码任务无效。</p><p>&nbsp;</p><p>如何让AI辅助编程更好地帮助开发者，也需要各方努力。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://devclass.com/2024/01/24/ai-assistance-is-leading-to-lower-code-quality-claim-researchers/\">https://devclass.com/2024/01/24/ai-assistance-is-leading-to-lower-code-quality-claim-researchers/</a>\"</p><p><a href=\"https://www.zhihu.com/question/640036429\">https://www.zhihu.com/question/640036429</a>\"</p><p><a href=\"https://zhuanlan.zhihu.com/p/626643788\">https://zhuanlan.zhihu.com/p/626643788</a>\"</p><p><a href=\"https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/\">https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/</a>\"</p>",
    "publish_time": "2024-01-25 15:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁集团去中心化的高性能存储服务 LiteIO 正式开源",
    "url": "https://www.infoq.cn/article/1LMBLffVZJMetQkAXDMd",
    "summary": "<p></p><p>在传统分布式存储盛行的时代，LiteIO 作为点对点块设备服务的代表作，在蚂蚁集团内部实践中，带来了极大的业务与技术收益。并在近期由阿里云软硬件结合研发团队与蚂蚁集团数据库技术团队联合撰写的论文《LightPool: A NVMe-oF-based High-performance and Lightweight Storage Pool Architecture for Cloud-Native Distributed Database 》，被 HPCA‘24 论文评审结果收录，CCF-A 类论文也肯定了 LiteIO 的技术先进性。</p><p></p><p>趁着这个机会，我们非常荣幸的宣布蚂蚁集团去中心化的高性能存储服务 LiteIO 正式开源。将我们的技术与全世界的开发者共享，激发更多的创意和想法，我们相信，对于数据库、存储产品类有状态的、有产品层多副本能力的产品，需要 LiteIO 这样的点对点技术来适配当下的 FinOps。</p><p></p><p>我们希望吸引更多社区的想法，只有通过开放和协作，LiteIO 才能够迎接更多的挑战，解决更多的问题，并为用户带来更多的价值，让 LiteIO 项目走得更远，成为云原生时代，存储使用的一种标准范式。</p><p></p><p>开源项目仓库：<a href=\"https://github.com/eosphoros-ai/liteio\">https://github.com/eosphoros-ai/liteio</a>\"</p><p></p><p></p><h2>“什么是 LiteIO ？”</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/57/575e0c24568f7dd06438726db99a6fdf.jpeg\" /></p><p></p><p>LiteIO 是一款高性能、易扩展的云原生块设备服务，专为超融合架构下的 Kubernetes 设计。在蚂蚁集团内部孵化 3 年并大规模应用在生产环境，为蚂蚁集团全数据型、存储型产品提供稳定、高效、易扩展的磁盘服务。</p><p></p><p>LiteIO 是将本地磁盘 / 逻辑卷，通过网络的方式共享给远程其他服务器使用，结合云原生 Kubernetes 的调度，将一系列磁盘统一管理、池化的通用技术。点对点的技术设计，相较传统分布式存储，有效地控制住硬件故障所带来的爆炸半径，同时去除存储冗余，有更多使用空间。</p><p></p><p></p><h4>01 设计背景</h4><p></p><p></p><p>在降本增效的时代，FinOps 显得格外重要，尤其像蚂蚁集团这种存储服务器规模庞大的体系，全局 1% 的存储利用率提升都会带来巨大的成本经济收益。因此需要再成本优化、保证通用性的同时稳定性不降。</p><p></p><p>数据库是一种重 IO 的软件系统，对于 IO 的稳定性和性能要求极高，一般生产系统都将数据库部署在本地磁盘的服务器上，这将带来两个问题：</p><p></p><p>利用率不均：IO 密集型和计算密集型的 workload 不同，这就会出现一台机器计算用完而存储富裕，亦或者存储用完计算还富裕，且通过调度也很难做到全局最优解。扩展性差：当出现存储不足，需要 scale up 存储，不得不通过迁移的手段换一台更大存储的服务器，拷贝数据的过程长。</p><p></p><p>传统分布式存储也是一种不错的解决方案，但在数据库领域，它将带来几个方面的问题：</p><p></p><p>副本数上升（成本）：分布式存储的优势在于通过 EC、多副本技术将存储池化，对于单硬件故障有很好的保护，但通过 EC、多副本技术造成单份数据副本在此架构下副本数将大于 1，往往副本数在 1.375~2 之间。数据库作为业务服务重要的组成，往往在上层有异 AZ、异地容灾的要求，在另一个 AZ 已经有数据库层面的备副本。总数据副本数会增加。爆炸半径大（稳定性）：分布式存储一般有一层中心式的 Meta 层，故障将带来全局性异常。</p><p></p><p></p><h4>02 设计思路</h4><p></p><p></p><p>LiteIO 采用去中心化的设计思路，基于 SPDK 数据引擎以及高性能 NVMe-over-Fabric 协议，将计算节点直接连到远程存储节点，通过高效的协议和后端 I/O 轮询，提供接近本地磁盘的高性能。点对点的设计配合 K8S 的调度控制，有效的控制单硬件故障所影响的服务。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/12/12e052c0c11da2ad1f8e0646742d47e0.png\" /></p><p></p><p></p><h4>03 FinOps</h4><p></p><p></p><p>基于 LiteIO，可以将服务器中无法分配的存储，按需分给远程计算节点使用，同时全局配合调度，将全局存储池化，从而提升全局的存储利用率。</p><p></p><p>例如有两种型号的服务器，计算密集型 96C 4T，存储密集型 64C 20T，假设存储机型的 CPU 已经分配完还剩 5T 磁盘，计算机型还有 CPU 但无磁盘可分，使用 LiteIO 可以将计算机型的 CPU + 存储机型的剩余磁盘组合成新的容器提供服务，同时提升了计算、存储利用率。</p><p></p><p></p><h4>04 通用存储计算分离</h4><p></p><p></p><p>LiteIO 是一个通用的存储服务技术，作用于存储逻辑卷，配合 K8S 上层容器或应用看到的和本地磁盘无差别，不论是直接读写块设备 bdev 还是将块设备做成任何文件系统均可以，不需要上层服务做任何修改，不论是 OceanBase、MySQL、PostgreSQL 这样的数据库，或者 Java、Python、Go 写的应用服务均可以将它用作一块普通磁盘使用。</p><p></p><p></p><h4>05 Serverless</h4><p></p><p></p><p>LiteIO 的通用的存储计算分离能力，使得 Scale 变得无比简单。配合感知与调度系统，部署一个 MySQL 实例就天然具备了 Serveless 能力。当 MySQL 的算力不够时，通过 LiteIO 将 MySQL 存储挂到一台更大算力的容器即可快速完成 scale up。当 MySQL 存储空间不足时，从其他存储节点挂一块磁盘即可完成无损扩容。</p><p></p><p></p><h2>\"技术特性 \"</h2><p></p><p></p><p></p><h4>01 高性能协议</h4><p></p><p></p><p>LitelO 使用 NVMe-oF 协议来提升性能，NVME-oF 协议可以充分利用新兴 NVMe 设备的固有并行性和多队列功能。而 iSCSI 在访问 NVMe SSD 时，性能损失高达 40％，并在协议转换等其他操作时也会消耗多于 30％的 CPU 资源。NVMe-oF 在性能方面优于 iSCSI，可以提供接近本地连接的 NVMe SSD 的性能。因此，在 LiteIO 中采用 NVMe-oF 可以最大程度地减少访问存储池中的存储资源时的开销，以提供接近原生磁盘的高性能。LiteIO 采用了 NVMe over Fabric（TCP）作为远程存储协议，以便集群中的其他节点访问存储资源。</p><p></p><p></p><h4>02 简化的 IO 链路</h4><p></p><p></p><p>传统分布式存储架构下，一个 Write IO 需要经过查询元数据、写元数据、写多副本数据三个过程，整个过程需要多次网络交互；而在 LitelO 架构下，由于单副本机制，点对点访问，前端 bdev 和后端 vol 一一映射，不需要额外的 rootserver 或者 metaserver 来管理全局元数据，IO 链路中只有一次跨网络访问，同时也不需要考虑多副本写带来的数据传输延迟，数据放大的问题，使得 LitelO 有更高的 IO 吞吐和更低的 IO 延迟</p><p></p><p></p><h4>03 零拷贝</h4><p></p><p></p><p>在访问本地磁盘时，I/O 请求和数据在 NoF-Initiator 和 NoF-Engine 之间通过 tcp-loopback 传输，但是这个过程涉及许多冗余的数据拷贝。为了消除这些拷贝并减少 CPU 开销，我们提出了一种新颖的零拷贝传输方式，用于 LiteIO 的本地访问。对于 I/O 请求，零拷贝传输采用 NoF-Initiator 和 NoF-Engine 之间的共享内存。对于数据，我们提出了一种 DMA 重映射机制，使本地存储设备可以直接访问应用程序缓冲区。零拷贝传输抛弃了 TCP 堆栈，并消除了用户缓冲区和内核之间的冗余数据拷贝，实现了访问本地存储资源时接近本地性能的效果。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b3/b390a1e035ee22d4b101c08fcd356f3c.png\" /></p><p></p><p></p><h4>04 热升级</h4><p></p><p></p><p>充分考虑到作为数据链路上的关键一环的 LiteIO 也会面临功能升级，我们实现了在升级 LiteIO 的过程中，让前端业务无感，IO 短时间抖动（&lt;100ms），同时机头挂载的 nvme 盘符不会发生改变。</p><p></p><p>Target 整体框架如下图所示，在热升级期间必须保持 nvmf 的网络连接不可中断，否则 host 侧会感知并去重连或者删除盘符，热升级采用旧的 target 程序 fork 新的 target 程序并加载新的二进制文件来实现，整个热升级过程中 IO 不可丢失，新旧进程的切换速度要快。基于热升级框架的简单性设计原则，热升期间下图中绿色的 TCP 或 RDMA 连接为必须保持的上下文，其他模块均无需保存上下文状态，网络连接的保持通过父子进程继承文件描述符的方式实现。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7f/7f13f8d5651f349f6c8c07790735730c.png\" /></p><p></p><p></p><h4>05 热迁移</h4><p></p><p></p><p>卷热迁移特性是为了在不影响业务的情况下，将卷的数据从原 Target 迁移到新的 Target，当迁移完成时由 Host 端完成链路的切换，从而实现业务无感切换到新的 Target。</p><p></p><p>热迁移过程中，将原 Target 的数据发送到新 Target 采用的方法是多轮循环迭代。每一轮开始前都会从卷获取一份 data map，然后根据这个 map 进行数据的拷贝。初始轮可能需要拷贝较多的数据，后续每一轮只需要拷贝上一轮迁移过程中新修改（write, discard）的数据。在保证迁移带宽大于新写入数据带宽的前提下，经过多轮拷贝后原 Target 和新 Target 之间的数据差异会逐渐缩小，从而可以停 IO 进行最后一轮的拷贝。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4e/4e4df0d93dd0b227d39e1787fb710be5.png\" /></p><p></p><p></p><h4>06 快照</h4><p></p><p></p><p>LiteIO 对接了 CSI 的 Snapshot 相关的接口，允许用户使用 K8S 社区的 Snapshot 资源对 LiteIO 卷创建快照。快照能力与底层数据引擎有关，LiteIO 支持两种引擎：LVM 和 SPDK。LVM 的快照能力是 VG/LV 提供的， NoF-Engine 的快照能力是 SPDK LVS 提供的。LVM 和 SDPK 都是单机引擎，要求 Snapshot 和 原始 LV 必须在同一台机器，这就意味着，在创建原始 LV 时，就需要预留一定空间给快照，如果没有预留空间，则无法保证创建 Snapshot 一定成功。</p><p></p><p>LiteIO 对接了 CSI 的 ExpandVolume 相关接口，用户可以通过修改 PVC 磁盘空间，实现磁盘在线扩容。对于 LVM 引擎，LV 扩容无需修改，NoF-Engine 暴露远程盘的流程中新增了 bdev_aio_resize RPC 调用，实现了远程盘的在线扩容。扩容同样有一些限制，原因也和快照一样，由于 LVM, SPDK 都是单机引擎，无法保证单机上有足够空间扩容。</p><p></p><p></p><h4>07 多盘</h4><p></p><p></p><p>点对点的数据链路模式，不可避免还是会产生一些存储资源碎片。LiteIO 支持将这些碎片组合起来，变为一个卷供业务使用。这也带来一个故障率提升问题，假设提供碎片的任意一个节点故障，则这个卷不可用。内部恰好有这样一个业务 LDG 可以容忍这样的故障率，物尽其用。LDG(logic data guard) 设计旨在构建常态化逻辑主备数据库，提供一站式的主备库生命周期管理和应用使用管控平台，为提升稳定性，减小运维过程中由于升级，维护，意外等产生的数据风险进行规避, 同时提升数据操控的能力。</p><p></p><p></p><h4>08 Thin provisioning</h4><p></p><p></p><p>LitelO 还提供 Thin provisioning 能力，在单机维度实现存储的超卖，适合于像 Mysql 等存储非预填充空间的存储产品使用，Thin provisioning 结合热迁移能力，可以在单机存储空间不足时，快速无缝迁移数据到空间空闲的节点；由于 LitelO 不是分布式存储架构，对 Thin provisioning 功能的使用需要精确控制超卖比例和和超卖资源总量，保障空间不足时能快速迁移，避免业务受损</p><p></p><p></p><h2>\"实践落地 \"</h2><p></p><p></p><p>LiteIO 广泛地应用在蚂蚁集团数万台生产服务器，整体提升了 25% 的存储利用率，极大地优化了资源服务成本。与本地存储相比，LiteIO 带来的额外 IO Latency 仅约为 2.1 us。其通用的存储计算分离架构，不仅服务于数据库产品，同时也为蚂蚁其他计算产品、应用服务提供了存储计算分离以及 Serverless 的能力。配合热升级、热迁移、Kubernetes 生态，使其在日常运维中不增加额外的运维负担。快照、多盘聚合等能力让其有更多灵活的使用与玩法。针对 LiteIO 在蚂蚁集团的最佳实践，后续有系列文章分享，敬请期待。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651194503&amp;idx=1&amp;sn=50355b06c918ff3a05a59349519a6f3a&amp;chksm=bdbbfcd48acc75c24b77b624b23779e46a342a1624eb0d2123ebe7e2dc8b79e51bc9bd143617&amp;scene=21#wechat_redirect\">Apache 顶级项目 MXNet 退役！大神李沐创办、亚马逊首选深度学习框架如何从大厂“宠儿”到落入“冷宫”？</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651194286&amp;idx=1&amp;sn=f013aa5e430eefdb4ae0d3aa20d1ff24&amp;chksm=bdbbfbfd8acc72ebbcf727db172add001c4142323e93a15149d9cd63f2a2f46ac7f218c60421&amp;scene=21#wechat_redirect\">“印度 CEO 毁了谷歌！”大裁员引发谷歌元老集体怀旧：20 年前为梦想而战，20 年后混口饭吃</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651194221&amp;idx=1&amp;sn=a3c659847ce4bdb38cda2128822319ce&amp;chksm=bdbbfb3e8acc722822b79a5040bcb29e317df0c37afe56bcc339cca06ed9ad56640993d9d10f&amp;scene=21#wechat_redirect\">中国开源，又一次让人失望了</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651194220&amp;idx=1&amp;sn=ea3097e47d0cf652f6e70f8c68d9dde1&amp;chksm=bdbbfb3f8acc722913b73454d48eea22bf8c6bf0e48698879fb906d01bf0b1d68e790507eead&amp;scene=21#wechat_redirect\">TikTok 员工加速“出海”，薪资翻倍；老外控诉中国科技巨头抄袭：反正官司打不赢，不费那个劲了；快手上市后首次整体盈利｜Q资讯</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2024-01-25 15:05:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Bonree ONE技术实践：如何用5台机器资源支撑起2000探针同时起跑？",
    "url": "https://www.infoq.cn/article/R9fJP6k6OMMomRuHbzmI",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p>日志、指标和调用链是可观测性取得成功的三要素，而这些的实现离不开数据采集，探针采集并上报数据，后端服务接收后对数据进行处理分析，从而达到可观测的目的。通常，服务器性能数据、服务相关数据、服务之间的调用等数据经由探针采集上报，经过ETL处理后，成为可观测性分析中的重要依据。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/53/53faac546c822ea88af9c75b9e52b432.png\" /></p><p></p><p>探针采集的数据量大小依赖两个要素：</p><p></p><p>采样率：采样率越高，数据量越大，对应可观测性分析会更加全面。业务调用量：当业务服务调用频率越高， 相应的数据量越大，对应可观测性分析会更加复杂。</p><p></p><h2>2000探针难在哪儿？</h2><p></p><p></p><p>由于私有化部署资源有限，需要尽可能多的满足企业监控需求，因此博睿数据的内部测试会以5台机器的集群作为部署标准，在资源固定的前提条件下，随着探针量的增多，主要难点如下：</p><p></p><p>业务场景存在峰值波动，高峰期的服务调用是低峰期的2倍+业务数据是多种业务场景同时存储，常见的涉及调用链数据、指标数据、服务快照数据等5台机器是混合部署多种服务，比如数据接入的controller服务、报警服务、业务查询服务、数据调用链存储、数据快照存储、数据指标存储、消息中间件等，在更大数据量写入的情况下，针对CPU、内存、磁盘IO的消耗都是抢占式的，影响服务的稳定性。</p><p></p><h2>如何优化瘦身？</h2><p></p><p></p><p>针对以上难点，首先想到的就是瘦身，即降低服务组件的数量，减少服务资源抢占的情况。其次是业务存储迁移，弃用高消耗组件，使用低消耗组件满足业务需求。最后在合理的数据存储方案的前提下，优化存储服务本身的性能，满足业务查询稳定性。</p><p></p><h3>降低组件数量</h3><p></p><p></p><p>hadoop存储套装节点数据量比较多，而且是java服务，资源消耗较大，内存需求较大。hadoop的主要业务方是AI服务，AI团队基于自研的数据处理框架，打造了全新一代的swiftAI服务，组件种类只有1个，部署服务最少只需要2个。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7b/7bc02b0db2a5a36f000b2b95ef572220.png\" /></p><p></p><h3>业务存储优化</h3><p></p><p></p><p>当前APM业务的存储分为三大块：指标数据、调用链和快照。目前主要使用三种不同的存储系统分别来支撑，指标数据存储在clickhouse、调用链使用ES，快照数据存储在自研的对象存储系统中。在实际的业务场景中，会交叉访问多种存储引擎，在资源估算时，没有一个合理的尺度来衡量资源的上下界。在单台机器上，如果部署多种存储引擎，势必会对服务稳定性产生影响，所以，减少APM业务的存储组件，成为一个可行性较高的方案。</p><p></p><p>探针调用链数据基于ES来存储，有以下痛点：</p><p></p><p>调用链数据与关联的快照数据写入时机存在不一致，基于ES的数据写入存在延迟。ES消耗资源较大，在CPU 和 IO上消耗较多，影响其他服务稳定性。ES的查询效率不稳定，随着数据量越来越大，甚至出现无法查询出数据的问题。</p><p></p><p>探针调用链快照数据基于对象存储系统来存储，有以下痛点：</p><p></p><p>写入不稳定，存在毛刺。对cpu 和 IO消耗较大，容易触达瓶颈。</p><p></p><p>针对以上两个组件的明显痛点，迁移数据到clickhouse进行存储，获益如下：</p><p></p><p>调用链数据和关联的快照数据同时写入clickhouse，保证关联数据的一致性。clickhouse写入稳定，即使是针对挽回数据，资源消耗较小。clickhouse读取稳定，clickhouse支持查询熔断、资源限制等手段，提高clickhouse查询稳定性。基于合理的攒批策略，clickhouse整体资源消耗平稳，毛刺点波动很小。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c57f44453d67c976229d73dd57fd0068.png\" /></p><p></p><h3>存储服务优化</h3><p></p><p></p><p>相关的业务存储进行了聚焦，那么势必会对clickhouse服务产生影响， clickhouse服务的优化以及运维监控就显得更加重要。</p><p></p><p>在优化方面，我们从以下三个方向着手：</p><p></p><p>服务参数调优</p><p></p><p>max_bytes_before_external_group_by：通过维度聚合查询时，当 RAM 消耗超过这个阈值， GROUP BY 会把多余的临时数据输出到文件系统并在磁盘进行处理计算，通常会建议配置成当前服务内存的80%。max_bytes_before_external_sort：涉及数据排序时，当 RAM 消耗超过这个阈值，ORDER BY 会把多余的临时数据输出到文件系统并在磁盘进行排序计算，通常会建议配置成当前服务内存的80%。max_memory_usage：用户单条查询可以使用的最大内存，通常会建议配置成当前服务内存的80%。max_execution_time：单条查询可以执行的最长时间，这个根据业务响应时间的上限来定。</p><p></p><p>物化视图、索引、projection的合理使用</p><p></p><p>针对不同的场景，使用不同的加速手段，解决查询效率的问题。</p><p></p><p>高频查询要充分利用主键索引。主键索引满足不了的高频查询，借助索引来加速。涉及排序操作，利用projection和物化视图来加速，优先使用projection。无法使用projection的场景，使用物化视图。</p><p></p><p>监控、容错的支持</p><p></p><p>为了解决多业务接入带来的复杂影响，需要对集群有充分的监控，且在容错性上需要考虑更多因素。</p><p></p><p>监控首要跟踪的监控是写入和读取两个方向，比如每分钟写入量，写入耗时、查询QPS等，针对特定敏感业务可以个性化跟踪。针对节点本身的状态信息进行监控，比如服务负载、merge任务数、parts数量等，这些指标可以及时发现服务的稳定性风险。针对集群的均衡性进行监控，比如parts数据同步的延迟时间、各个节点的查询均衡性、各个节点的写入均衡性等，避免集群倾斜。容错性写入节点单节点异常，不影响整体服务写入。clickhouse单节点异常，不影响整体集群的写入也不影响读取。</p><p></p><p></p><h3>效果</h3><p></p><p></p><p>AI组件瘦身</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/97/97b98196e76a1bb486581489aa5c6d9a.png\" /></p><p></p><p>调用链等相关数据迁移到CK</p><p></p><p></p><p></p><p></p><h2>总结</h2><p></p><p></p><p>为了实现5台集群可以支持到2000探针，我们首先要做的就是减法，减少组件之间的影响，让单个组件可以发挥更大的效能。再围绕这个组件，构建更全面的生态，包括监控、运维和操作等入口。最后在围绕业务使用场景进行深入优化，保证整体服务稳定性。</p><p></p><p>后续我们会在clickhouse内核上深入发力，不断拓展clickhouse的使用场景，与开发者一起分享博睿数据在clickhouse方向的探索和实践，助力Bonree ONE在更快、更准、更稳定的方向上走得更远。</p>",
    "publish_time": "2024-01-25 15:08:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "苹果iCloud的“极端”架构：管理数十亿独立用户数据库！",
    "url": "https://www.infoq.cn/article/kK349HwqlXPsFuytIkNH",
    "summary": "<p>苹果的云后端服务CloudKit使用了Cassandra和FoundationDB。我们来看看它们在各自的云中是如何使用的，以及它们解决了哪些问题。</p><p></p><p>论文原文：https://www.foundationdb.org/files/record-layer-paper.pdf</p><p></p><p>&nbsp;</p><p>在过去的几个月里，我写了各种关于大型科技公司“幕后”技术的文章，比如<a href=\"https://engineercodex.substack.com/p/meta-xfaas-serverless-functions-explained\">Meta的内部无服务器（serverless）平台</a>\"和谷歌<a href=\"https://engineercodex.substack.com/p/how-google-takes-the-pain-out-of\">内部喜爱的代码审查工具</a>\"。</p><p>&nbsp;</p><p>然而，苹果的基础设施并不公开。我想知道苹果是如何构建iCloud的，这篇文章涵盖了我所知道的一切。</p><p>&nbsp;</p><p>苹果将<a href=\"https://www.foundationdb.org/\">FoundationDB</a>\"和Cassandra用于其云后端服务iCloud和CloudKit。是的，标题并没有错：苹果确实在其极端的多租户架构中存储了数十亿个数据库。</p><p>&nbsp;</p><p></p><h1>现实世界中永恒的教训</h1><p></p><p>&nbsp;</p><p>在开始阅读之前，先看下这些适用的经验教训和指导方针。</p><p>&nbsp;</p><p>我发现，本文将要讨论的苹果的许多经验教训与<a href=\"https://read.engineerscodex.com/p/meta-xfaas-serverless-functions-explained\">Meta无服务器平台架构</a>\"的经验教训非常相似。</p><p>&nbsp;</p><p>&nbsp;</p><p>两者都巧妙地使用了异步处理，以使用户功能更加流畅。Meta使用其无服务器栈来实现非面向用户的功能。苹果几乎对Record Layer的所有功能都使用了异步处理（在下面将会深入解释），以向用户隐藏延迟。两者都充分利用了无状态架构，因为他们知道他们有强烈的可扩展性需求。两者都在逻辑上进行了资源隔离，以提高可靠性和可用性。两者都能简单地处理各种各样的需求。苹果提到“提供和操作独立的系统来存储‘小数据’和‘大数据’是多么诱人。”然而，这增加了操作的复杂性，相反，它们用一个抽象来处理所有类型的数据需求。Meta在其无服务器平台上也做了同样的事情，为各种功能负载提供了一个抽象。两者都构建了抽象层，以使开发人员的体验更好。应用程序开发人员不必担心可扩展性需求——这是由分布式系统工程师在更深层次的栈中处理的。了解你的用户。Meta和苹果提供的每一层、API和设计决策都是以明确了解特定技术的用户是谁为指导的，无论是应用开发团队还是可观察性团队。</p><p>&nbsp;</p><p></p><h1>Cassandra</h1><p></p><p>&nbsp;</p><p><a href=\"https://cassandra.apache.org/_/index.html\">Cassandra</a>\"是一个宽列NoSQL数据库管理系统。它最初是由Facebook开发，用于支持Facebook收件箱的搜索功能。有趣的是，Meta自己已经用<a href=\"https://engineering.fb.com/2021/08/06/core-infra/zippydb/\">ZippyDB</a>\"取代了大部分Cassandra的使用。</p><p>&nbsp;</p><p>iCloud部分是由Cassandra提供支持的。<a href=\"https://news.ycombinator.com/item?id=9307563\">DataStax</a>\"的数据显示，苹果拥有世界上最大的Cassandra部署之一。</p><p>&nbsp;</p><p><a href=\"https://twitter.com/erickramirezau/status/1578063811495477248\">报告显示</a>\"：</p><p>&nbsp;</p><p>拥有超过30万个实例/节点数百PB的数据(如果不是EB的话)每个群集超过2 PB，有数千个群集每秒数百万次查询数以千计的应用程序</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/944bde257a37b18244ac23cb07a259d0.png\" /></p><p></p><p><a href=\"https://twitter.com/erickramirezau/status/1578063811495477248\">来源</a>\"</p><p>&nbsp;</p><p>iCloud中Cassandra的其他分片显示，它管理着EB级的数据。<a href=\"https://news.ycombinator.com/item?id=33124631#33136026\">每台服务器有多个Cassandra节点</a>\"，<a href=\"https://news.ycombinator.com/item?id=33124631#33136026\">苹果的团队在控制爆炸半径和分片方面非常聪明</a>\"。这确保了iCloud数据的可用性接近100%。</p><p>&nbsp;</p><p>苹果仍在积极改进Cassandra。苹果的Scott Andreas上个月就<a href=\"https://www.youtube.com/watch?v=hUxLJSFi2-A\">Cassandra的未来</a>\"做了一次演讲。在苹果的招聘页面上，当招聘分布式系统工程师时，他们通常会提到Cassandra。</p><p>&nbsp;</p><p>然而，<a href=\"https://developer.apple.com/icloud/cloudkit/\">CloudKit</a>\"&nbsp; + Cassandra遇到了两个可扩展性限制，这导致他们采用了FoundationDB。</p><p>&nbsp;</p><p>在单个区域内，一次只能执行一个操作，即使正在编辑不同的记录也是如此。对于需要多个用户或设备同时处理共享数据的应用程序来说，这可能会产生问题。在原子操作中同时更新多个记录时，更新仅限于单个Cassandra分区。这些分区有它们可以处理的最大尺寸，并且随着分区尺寸的增加，Cassandra的速度往往会变慢。</p><p>&nbsp;</p><p>FoundationDB和Record Layer解决了这两个问题。</p><p>&nbsp;</p><p></p><h1>FoundationDB</h1><p></p><p>&nbsp;</p><p>苹果对FoundationDB的公开程度要高得多。他们于<a href=\"https://techcrunch.com/2015/03/24/apple-acquires-durable-database-company-foundationdb/\">2015年收购了FoundationDB</a>\"，并发表了多篇论文，详细介绍了他们对FoundationDB的使用情况。</p><p>&nbsp;</p><p><a href=\"https://github.com/apple/foundationdb\">FoundationDB</a>\"是一个开源、分布式、事务性的键值对存储。它旨在处理大量的数据，并适用于读/写工作负载和写入密集型工作负载。它也符合<a href=\"https://www.swequiz.com/learn/acid-properties\">ACID</a>\"。</p><p>&nbsp;</p><p>苹果在CloudKit（苹果的云后端服务）中广泛使用了<a href=\"https://github.com/FoundationDB/fdb-record-layer\">FoundationDB Record Layer</a>\"。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c9/c960a074fe868f2f8b3a0162627edc0c.png\" /></p><p></p><p>&nbsp;</p><p>来源：<a href=\"https://www.youtube.com/watch?v=HLE8chgw6LI\">FoundationDB Record Layer：开源结构化存储</a>\"</p><p>&nbsp;</p><p>根据<a href=\"https://github.com/FoundationDB/fdb-record-layer\">GitHub</a>\"的介绍：</p><p>&nbsp;</p><p></p><blockquote>Record Layer是一个Java API，它在FoundationDB之上提供了面向记录的存储，（非常）大致相当于一个简单的关系数据库，其特点是：&nbsp;结构化类型——记录是根据<a href=\"https://developers.google.com/protocol-buffers/\">protobuf</a>\"（Protocol Buffer）消息定义和存储的。Protocol Buffer最初是由谷歌设计的。&nbsp;索引——Record Layer支持各种不同的索引类型，包括值索引（大多数数据库提供的类型）、排序索引和聚簇索引。索引和主键可以通过protobuf选项定义，也可以通过编程方式定义。&nbsp;复杂类型——支持复杂类型，如列表和嵌套记录，包括针对此类嵌套结构定义索引的能力。&nbsp;查询——Record Layer不提供查询语言，但它提供了查询API，该API能够扫描、过滤和排序一种或多种记录类型，以及能够自动选择索引的查询规划器。&nbsp;多记录存储，共享模式——Record Layer提供了支持许多离散记录存储实例的能力，所有实例都具有共享（和不断发展的）模式。例如，与其为存储所有用户数据的单个数据库建模，不如为每个用户提供自己的记录存储，也许可以在不同的FDB集群实例中进行分片。&nbsp;非常轻量级——Record layer旨在用于大型、分布式、无状态的环境。打开存储和第一次查询之间的时间以毫秒计。&nbsp;可扩展——新的索引类型和自定义索引键表达式可以动态地合并到记录存储中。</blockquote><p></p><p>&nbsp;</p><p>在<a href=\"https://www.foundationdb.org/files/record-layer-paper.pdf\">FoundationDB Record Layer</a>\"的论文中，他们写道：</p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>“[FoundationDB Record Layer用于]为服务于数亿用户的应用程序提供强大的抽象。CloudKit使用Record Layer来承载数十亿个独立的数据库，其中许多数据库具有通用模式。”</blockquote><p></p><p>&nbsp;</p><p></p><h2>为什么使用 FoundationDB Record Layer？</h2><p></p><p>&nbsp;</p><p>FoundationDB、Record Layer和CloudKit的结构如下所示：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f5e887c41d9407554b419de3330e21d.png\" /></p><p></p><p>&nbsp;</p><p>来源：<a href=\"https://www.youtube.com/watch?v=HLE8chgw6LI\">FoundationDB Record Layer：开源的结构化存储</a>\"</p><p>&nbsp;</p><p>FoundationDB完成所有分布式系统和并发控制的工作。Record Layer充当关系数据库，使得FoundationDB更易于使用。CloudKit是最顶层，为应用程序开发人员提供特性和API。CloudKit并不是唯一一个构建在Record Layer之上的东西，在Record Layer之上还有其他内部构建的层，用于需要结构化存储的东西，如JSON文档存储。</p><p>&nbsp;</p><p>Record Layer允许苹果大规模支持多租户。</p><p>&nbsp;</p><p>事实上，这有点低估了它。</p><p>&nbsp;</p><p>Record Layer用于极端多租户，其中每个应用程序的每个用户都可以获得独立的记录存储。这意味着Record Layer承载着数十亿个独立的数据库，共享数千个模式。</p><p>&nbsp;</p><p>那就更好了！而且更令人印象深刻。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4858c925b1070a9a2a38ffad0031f4b.png\" /></p><p></p><p>&nbsp;</p><p>来源：<a href=\"https://www.youtube.com/watch?v=HLE8chgw6LI\">FoundationDB Record Layer：开源的结构化存储</a>\"</p><p>&nbsp;</p><p>由于两个基本的架构决策，Record Layer被设计用于处理如此大规模的多租户。</p><p>&nbsp;</p><p>该层能无状态地运行，只需添加更多的无状态实例，就可以轻松地扩展计算资源。这种无状态架构简化了负载均衡器和路由器的任务，因为它们只需要关注数据的位置，而不需要关注计算服务器的功能。此外，无状态服务器可以在客户端之间分配的资源减少了。该层使用记录存储抽象来有效地管理资源分配和可扩展性。这种抽象表示整个逻辑数据库，包括序列化数据、索引和操作状态。每个记录存储都被分配了一个特定的键范围，这保证了不同租户之间数据的逻辑分离。如有必要，传输租户的数据将成为将分配的键范围重新定位到新集群的简单过程，因为管理和使用记录存储所需的所有信息都包含在该范围内。</p><p>&nbsp;</p><p>这是一个很好的切入点，可以让我们粗略地了解一下苹果是如何构建iCloud的。</p><p>&nbsp;</p><p>如果你对CloudKit、FoundationDB和Record Layer的相关技术感兴趣，请继续阅读。</p><p>&nbsp;</p><p></p><h1>CloudKit如何使用FoundationDB和Record Layer</h1><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e6e253526ab2ce4504117368d86facef.png\" /></p><p></p><p>来源：<a href=\"https://www.foundationdb.org/files/record-layer-paper.pdf\">FoundationDB Record Layer：多租户结构化数据存储</a>\"</p><p>&nbsp;</p><p>在CloudKit中，应用程序由“逻辑容器”表示，该容器遵循已定义的模式。该模式概述了必要的记录类型、字段和索引，以实现高效的数据检索和查询。应用程序将其数据组织到CloudKit内的“区域”中，这允许对记录进行逻辑分组，以便与客户端设备进行选择性同步。</p><p>&nbsp;</p><p>对于每个用户，CloudKit在FoundationDB中指定一个唯一的子空间。在这个子空间中，它为用户与之交互的每个应用程序创建一个记录存储。从本质上讲，CloudKit管理着大量的逻辑数据库（将用户数量乘以应用程序数量），每个数据库都包含自己的一组记录、索引和元数据，总计数十亿个数据库。</p><p>&nbsp;</p><p>当CloudKit收到来自客户端设备的请求时，它会通过负载平衡将该请求定向到可用的CloudKit服务进程。然后，该进程与特定的Record Layer记录存储进行交互来满足请求。</p><p>&nbsp;</p><p>CloudKit将定义的应用程序模式转换为Record Layer内的元数据定义，该元数据定义存储在单独的元数据存储中。此元数据通过特定于CloudKit的系统字段来进行扩充，这些字段跟踪记录的创建、修改时间以及存储记录的区域。区域名称以主键为前缀，以便能够有效地访问每个区域内的记录。除了用户定义的索引外，CloudKit还管理着用于内部目的的“系统索引”，例如通过保留按记录类型跟踪记录大小的索引来管理存储配额。</p><p>&nbsp;</p><p>FoundationDB和Record Layer一起为苹果解决了4个关键问题，这些问题是单独使用Cassandra或单独使用FoundationDB无法解决的。</p><p>&nbsp;</p><p></p><h2>已解决的问题：个性化全文搜索</h2><p></p><p>&nbsp;</p><p>FoundationDB帮助用户解决了个性化全文搜索的问题，让用户能够快速访问数据。</p><p>&nbsp;</p><p>他们的系统利用FoundationDB的键顺序，可以快速搜索文本的开头（前缀匹配），也可以进行更复杂的搜索（例如查找靠近或按特定顺序排列的单词——邻近度和短语搜索），而无需额外的开销。</p><p>&nbsp;</p><p>在传统的搜索系统中，你通常需要在后台运行额外的进程来保持搜索索引的最新状态，但苹果的系统会实时执行所有操作，这意味着一旦数据发生变化，搜索索引就会立即更新，不需要额外的步骤。</p><p>&nbsp;</p><p></p><h2>已解决的问题：高并发区域</h2><p></p><p>&nbsp;</p><p>借助FoundationDB，CloudKit可以顺利地处理同时发生的许多更新。</p><p>&nbsp;</p><p>之前，在使用Cassandra时，CloudKit曾经依赖一个特殊的索引来跟踪每个区域中的更新，从而在设备之间同步数据。当设备需要更新其数据时，它会检查该索引以查看新内容。但这个系统有一个缺点：当多个更新同时发生时，它可能会导致冲突。</p><p>&nbsp;</p><p>但借助FoundationDB，CloudKit使用了一种特殊的索引来跟踪每次更新的确切顺序，而不会导致冲突。这是通过为每个更新分配一个唯一的“版本”来完成的，当CloudKit需要同步时，它会查看这些版本，以找出设备错过了哪些更新。</p><p>&nbsp;</p><p>然而，当CloudKit需要在不同的存储集群之间移动数据时（也许是为了更均匀地分配负载），事情就变得棘手了，因为每个集群都有自己的版本号，而这些版本号并不匹配。为了解决这个问题，CloudKit为每个用户的数据提供了一个“移动计数”（称为“化身”），每当他们的数据被转移到一个新的集群时，移动计数就会增加。每个记录更新都包括用户当前的“化身”编号，确保即使在移动后，CloudKit仍然可以通过查看化身号和版本号来确定正确的更新顺序。</p><p>&nbsp;</p><p>当他们切换到这个新系统时，CloudKit面临着处理不包含这些版本号的旧数据的挑战。他们巧妙地克服了这一点，通过使用一个特殊的函数，在新的更新之前使用以前的系统对旧的更新进行排序。这意味着不会对应用程序进行复杂的更改，也不会留下过时的代码。该函数考虑了化身、版本和旧的更新计数器值，以维护记录的正确顺序。</p><p>&nbsp;</p><p></p><h2>已解决的问题：高延迟查询</h2><p></p><p>&nbsp;</p><p>FoundationDB是为高并发而非低延迟而设计的。这意味着它可以同时处理很多任务，而不是关注单个任务的速度。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0e/0e0bb249de3b9613c043614e0ef17e2c.png\" /></p><p></p><p>来源：<a href=\"https://www.youtube.com/watch?v=HLE8chgw6LI\">FoundationDB Record Layer：开源的结构化存储</a>\"</p><p>&nbsp;</p><p>为了充分利用这种设计，Record Layer的许多工作都是“异步”完成的——它将待完成的任务排在队列中，允许其他工作在此期间完成。这种方法有助于掩盖在这些任务中可能出现的任何延迟。</p><p>&nbsp;</p><p>然而，FoundationDB用于与其数据库通信的工具被设计为使用单个线程进行网络连接，每次只做一件事。在早期版本中，这种设置会导致系统中的流量堵塞，因为这个网络线程中的所有东西都在等待被轮询。Record Layer一直在使用这种单线程方式，这导致了瓶颈。</p><p>&nbsp;</p><p>为了改善这一点，苹果减少了该网络线程的工作负载。现在，复杂的任务似乎更快了，因为系统同时在多个前端处理数据库，而不是形成队列。通过这种方式，延迟或明显的缓慢被隐藏起来了，因为系统不会等到一个任务完成后再开始另一个任务。</p><p>&nbsp;</p><p></p><h2>已解决的问题：冲突的事务</h2><p></p><p>&nbsp;</p><p>在FoundationDB中，如果一个事务正在读取某些键，而另一个事务同时在修改这些键，就会导致“事务冲突”。FoundationDB通过提供对读写时可能导致这些冲突的键集控制，允许对这些冲突进行精确地管理。</p><p>&nbsp;</p><p>避免不必要冲突的一种常见方法是对一系列键执行一种不会引起冲突的特殊读取，称为“快照”读取。如果这个读取找到了重要的键，则事务将只标记这些特定的键是否存在潜在冲突，而不是标记整个范围。这确保了事务只受对其结果真正重要的更改的影响。</p><p>&nbsp;</p><p>Record Layer使用这种策略来有效地管理一个被称为跳表的结构，该结构是其排序索引系统的一部分。然而，手动设置这些冲突范围可能很棘手，并可能导致难以识别的错误，尤其是当它们与应用程序的主要逻辑混合在一起时。因此，建议构建在FoundationDB之上的系统创建更高级别的工具，比如自定义索引，来处理这些模式。这种方法有助于避免将放宽冲突规则的责任留给每个客户端应用程序，这可能会导致错误和不一致。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions\">https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions</a>\"</p>",
    "publish_time": "2024-01-25 15:17:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "挑战Transformer霸权？ Yan 架构竟以半价成本实现百万级参数大模型",
    "url": "https://www.infoq.cn/article/mw700OH6gDaYk9beMFft",
    "summary": "<p></p><blockquote>1 月 24 日，岩芯数智正式发布自研大模型“Yan 模型”。Yan 模型采用非 Transformer 架构，为非 Attention 机制的通用自然语言大模型。据了解，该大模型有相较于同等参数 Transformer 的 7 倍训练效率、5 倍推理吞吐和 3 倍记忆能力。</blockquote><p></p><p></p><p>昨日，在 ROCK AI 大模型发布会上，Yan 大模型展示了其在人工智能领域的一系列创新和优势。该模型在多个方面表现出超越当前 Transformer 技术的潜力。</p><p></p><p>首先，Yan 大模型在训练效率方面显示出惊人的成绩，据称比同等参数的 Transformer 提高了 7 倍。这意味着在更短的时间内，Yan 可以处理和学习更多的数据，这对于加快 AI 模型的发展至关重要。其次，它的推理吞吐量是 Transformer 的 5 倍，这使得处理实时数据和复杂任务变得更加高效。最引人注目的是，它拥有 3 倍于 Transformer 的记忆能力，这可能为处理大规模数据集和复杂的 AI 任务提供了全新的途径。</p><p></p><p>尽管 Yan 大模型是否会开源还有待确定，但其合作者已经可以免费使用这一架构，这无疑将促进技术的快速应用和发展。值得一提的是，基于 Yan 架构，仅需投入同等规模 Transformer 架构成本的 50% 甚至更低，就可以拥有百万参数级的大模型。</p><p></p><h5>Transformer 架构的局限性</h5><p></p><p></p><p>作为当前 AI 领域的一个基石，Transformer 的设计和性能已经在各种任务中被广泛验证。Transformer 是基于注意力机制的神经网络架构，现今在人工智能领域占据主导地位。它能够有效处理序列数据，极大提高翻译、识别等任务的效果。</p><p></p><p>全球人工智能热潮的许多主要模型和产品，如 GPT、LLAMA、PaLM 等，都是基于 Transformer 构建的。其通用性显著，虽最初设计用于语言翻译，但现也推动计算机视觉、机器人学、计算生物学等领域的发展。Transformer 的核心在于快速捕捉输入内容各部分间的相互作用，适用于处理句子中的片段、音乐中的音符、图像中的像素、蛋白质的部分等各种任务。</p><p></p><p>Transformer 的概念最早出现在谷歌研究人员 2017 年的论文《Attention is All You Need》中，这篇论文在短短 5 年内被引用了 3.8 万余次。它是编码器 - 解码器模型的一个特例，2-3 年前开始流行。在此之前，注意力机制只是模型的一部分，基于 LSTM（长短期记忆）和其他 RNN（循环神经网络）变体。</p><p></p><p>Transformers 的关键见解在于，注意力可以作为推导输入和输出之间依赖关系的唯一机制。</p><p></p><p>Transformer 的突破在于其对注意力的独特运用。它使模型在处理单词时能够关注与该单词密切相关的其他单词。在《Attention is All You Need》发表前，语言 AI 领域先进技术是 RNN，它按顺序处理数据，但在表达单词间远距离依赖关系时存在局限。注意力机制使模型无视距离，考虑单词间的关系，确定哪些单词和短语更值得关注。谷歌团队的突破在于完全舍弃 RNN，仅用 Attention 进行语言建模。</p><p></p><p>注意力机制最初在计算机视觉中提出，重点关注特定区域，忽略无关图像区域。它实现了语言处理的并行化，同时分析文本中的所有单词，而非顺序分析。Transformer 的并行化带来了更全面、准确的文本理解，以及高于 RNN 的计算效率和可扩展性。现代基于 Transformer 的模型以其规模为特点，能在更大的数据集上训练，使用更多参数。</p><p></p><p>尽管 Transformer 非常强大和通用，技术领域仍在寻求更高效、先进的解决方案来应对新挑战和需求。</p><p>尽管 Transformer 模型在人工智能领域取得了显著成就，但它们存在一些局限性，这促使研究者寻找更优的模型架构。Transformer 的主要局限性包括：</p><p></p><p>参数数量庞大：Transformer 模型通常含有数百万到数十亿个参数，需要大量数据进行训练，以及昂贵的计算资源，包括高性能的 GPU 或 TPU。高昂的计算成本：标准 Transformer 模型在处理长序列时，其自注意力机制的时间和空间复杂度呈二次方增长。随着输入序列长度的增加，计算资源和时间需求成指数级增长。同时，由于参数众多和复杂的层间交互，模型在训练和推理时还需要大量内存。长序列处理困难：Transformer 架构与序列长度呈二次方关系，处理更长的序列时，内存和计算需求急剧增加，使得处理长序列变得困难。</p><p></p><p></p><h4>国内首个非 Attention 机制大模型——Yan 模型</h4><p></p><p></p><p>面对 Transformer 模型在处理大参数量、高计算成本和长序列困难方面的局限性，科技界迫切寻求更高效的解决方案。这些挑战促使岩芯数智研发团队开创性地开发了 Yan 模型，一个基于非 Attention 机制的创新架构。在 ROCK AI 大模型发布会上，刘凡平详细介绍了 Yan 模型的独特优势和技术进步。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c31a73f684d89b492231b0bd4781d95.jpeg\" /></p><p></p><p>他指出，Yan 架构与 OpenAI 的 GPT 系列、Meta 的 LLaMa 系列和 Google 的 PaLM 系列等基于 Transformer 架构的模型截然不同，是一种完全独立研发的新一代技术，拥有自主知识产权。</p><p></p><p>Yan 架构的主要优势在于其训练效率和资源消耗方面的显著改进。刘凡平提到，Yan 架构的训练效率是传统 Transformer 架构的 7 倍，这大大缩短了开发周期，并显著降低了成本。这对资源有限的创业公司和中小企业尤其有利。</p><p></p><p>此外，Yan 架构在保持高效能的同时，具有高推理吞吐量的特点，能够支持更多用户的同时使用。刘凡平还强调了 Yan 架构对数据隐私的重视，支持 100% 私有化部署，这对注重数据安全的企业至关重要。</p><p></p><p>他提到，Yan 架构能够在不同平台上运行，包括大型服务器和普通消费级 CPU，这增加了其在不同规模和类型企业中的应用范围。同时，Yan 在减少大模型幻觉问题方面也取得了进展，通过增强记忆能力，提高了问题回答的准确性。</p><p></p><p>在刘凡平的介绍之后，岩芯数智 CTO 杨华对 Yan 架构进行了进一步的阐释。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a25d1c1da80d787f3386df6ba77f69b9.png\" /></p><p></p><p>杨华表示，Yan 架构不依赖于传统的注意力机制或 RNN 等序列模型。通过采用线性自然语言关联特征表示、特征关联函数和记忆算子，Yan 实现了计算复杂度的显著降低和特征表达能力的增强。Yan 通过多层叠加提高网络深度，优化了模型的学习和生成复杂信息特征的能力，从而在推理效率上取得显著提升，同时大幅降低了推理成本。</p><p></p><p>杨华还介绍了基于 Yan 架构的不同参数规模的语言模型，包括 13 亿、70 亿、480 亿参数量的模型，并强调了在大规模语料上的训练过程和方法。在性能对比中，Yan 在训练效率、推理吞吐量、资源消耗和记忆能力等多个维度上均优于传统 Transformer 模型。通过应用示例，如机器翻译、古诗续写和问答系统，Yan 展示了其实际运行能力，特别是在常规消费级 CPU 设备上的流畅运行能力。</p><p></p><p>随着发布会的结束，这些技术介绍和展示吸引了与会者的极大关注，引发了大家的广泛讨论。在随后的深入采访中，刘凡平表示，Yan 模型的设计旨在满足中小企业和大型企业合作伙伴的多样化需求。这一模型以其高效、灵活且成本效益高的特点，已经在多个行业中获得了广泛的关注和应用。</p><p></p><p>刘凡平强调，Yan 模型深受多个合作伙伴的青睐，这些合作伙伴参与了与模型相关的会议，并对其表现出浓厚的兴趣；对于中小型企业而言，Yan 模型提供了一种相对低成本的技术解决方案。它通过优化模型架构，不仅提高了训练和推理的效率，还降低了客户的总体项目成本。</p><p></p><p>此外，刘凡平也谈到，Yan 模型对于离线应用场景也具有重要意义。它能够在端侧运行，支持断网情况下的应用，这对于教育等领域尤为关键。在这些领域中，Yan 模型能够为用户提供不依赖于网络环境的稳定和高效服务。在金融和制造业领域，Yan 模型可以以低成本的方式提供智能客服解决方案，优化供应商管理和高效处理内部数据等，从而提升用户体验和运营效率。</p><p></p><h5>Yan 架构的潜力与挑战</h5><p></p><p></p><p>从技术介绍来看，Yan 架构无疑展示了许多潜在优势，例如其在训练效率、资源消耗、推理吞吐量以及对数据隐私的重视上的显著进步。</p><p></p><p>然而，正如历史上许多技术革新所展示的，一定程度的技术优势并不总是能够直接转化为实际应用中的成功。因此，对于 Yan 架构来说，下一步至关重要的是经受市场和行业专家的实际测试和验证。这不仅是对其技术创新的检验，也是对其在实际应用环境中可行性的考量。</p><p></p><p>我们期待看到更多来自不同背景和专业领域的专家对 Yan 架构进行深入分析和实际应用测试。进一步的，对于 Yan 架构来说，吸引和鼓励更广泛的行业参与至关重要。是否能够激发开发者、创业公司和大型企业的兴趣，将是衡量其市场潜力的关键。</p>",
    "publish_time": "2024-01-25 16:07:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "12年业务架构老兵谈2024年数字化：结构化思维和企业转型之道",
    "url": "https://www.infoq.cn/article/RPUrC0taZ4XBWpGgxfvA",
    "summary": "<p>一年之计在于春，春还未到，“计”已先到，每年这个时段，都是各种关于“如何做好今年 XXX”的文章，大家还是喜欢积极思考的。</p><p></p><h2>一</h2><p></p><p>笔者也算是日常行走于“数字化江湖”之人，也很愿意学习各类数字化文章，不过，很多文章都带有一定的“情绪价值”。这也难免，文字既是表达，表达既是思考，思考既是专注，专注难免情绪，古人云：“人无癖不可与交，以其无深情也”。用情深了，无论是思考之深，还是实践之深，都难免成“癖”，乃至化“执”，一件事情说多了，无论是否影响别人，大概率都会是自己信了，“住”了。</p><p></p><p>数字化也难免于此，信的人越信越深，不信的，越说越烦；用得好的，越用越宽，不爱用的，总嫌没用。再加上有时候数字化会成为“任务”，也就容易催生一些“反弹”。</p><p></p><p>电子计算机诞生已经快八十年的历史了，就生产工具而言，其普及速度当然是远超过工业化进程中“机械”的普及速度，尤其是在最近十余年的移动设备发展过程中。笔者最早接触真正的计算机是在高中的计算机实验室，上世纪九十年代前半期，与其说是上了计算机课，还不如说只是参观，那时候我对计算机更多的理解还是建立在“小霸王学习机”的基础上，但是今日回看这个进程，不过一瞬间罢了。</p><p></p><p>以后毕业的学生，计算机基础能力越来越强，所以，数字化带来最大“折磨”的，主要是最多到 2030 年（时间只是个时段划分而已，倒不用对准确的时间点太纠结）之前就已经参加工作的人，他们的数字化能力仍需要在企业中更多地“磨合”、“调整”，至于以后的学生，无论是自身的计算机能力，还是计算机已经可以提供的辅助能力，都会远超过今天。移动设备的发展，把个人能力的换代基本上卡在每 10 年左右必须有一次大的能力调整，否则，适应环境就不太容易了。</p><p></p><p>这不是贩卖焦虑，而是能力的变化确实在加速，不少企业应该也有感觉，目前有些处于很重要位置的中层干部，对环境的适应能力、方向的预感能力已经弱于新生代骨干员工了。</p><p></p><p>笔者以前在大型金融央企工作，经常有一种感觉，如果你不主动学习新东西，企业对你的使用必然是损耗性的，读者不要误会，笔者并非指这些企业不注重员工培养——恰恰相反，它们通常有着非常完善的培训体系，并在此投入巨大。但是“改头换脑”式的学习，得你自己来，企业只能提供环境。</p><p></p><p>像上边讲的，每 10 年，你自己是否觉得自己的认知体系和能力，有了更深层次的发展，如果没有，未必是你得道成仙了，而可能是旧的思维模式到天花板了，虽然你确实用得“炉火纯青”，但是它们未必能有效适应新事物，而你也“惰”于去更换它们，希望通过一些“消极”的观察来保护自己已有的能力。</p><p></p><p>这也是笔者最近经常会思考的问题，笔者从偶然投身业务架构工作，到今年正好十二年了，十二生肖又见了一遍；2024 年也是笔者在业务侧和技术侧各工作过十二年的时间点，恍恍惚惚走过来，是否今天看事物的认知有所不同了？到底是什么在推动自己变化？</p><p></p><h2>二</h2><p></p><p>大家可能会觉得，明明说 2024 年的数字化转型，怎么扯到自我认知上去了？这正是笔者想聊的，数字化转型，不会变的话题正是人自己的发展，最终是人的发展带来工具的发展。从这个角度讲，人的变化是“因（主要矛盾）”，工具的变化是“缘（次要矛盾，触发条件）”，企业的数字化实施和当前的成效是“果（当下看到的东西）”，企业的持续发展是“报（未来的长期影响）”，形成了可以始终适应变化的观察和行动力才是“本末究竟（企业的终极能力，真正的智慧）”，从这个角度看，数字化也只是企业的一场修行，一次长考，最终检验的是企业中的每一个人。</p><p></p><p>笔者并非盲目宣扬数字化的价值，有国家政策做引导，数字化的方向没什么可怀疑的。就像智能手机的普及一样，还有多少人真的拒绝使用智能手机，未来的新生代接触智能手机的年龄越来越低，如果出现了意念即可控制的手机、元宇宙的手机，这些体验更好、玩着更爽的 “手机”，有多少人会拒绝？这就是时代的特征，企业的数字化与之相比，多了很多枯燥乏味，缺了很多兴致盎然，但道理是一样的。</p><p></p><p>与电子消费品不同的是，企业的数字化不是用来消费享乐的，是用来竞争的。商场如战场，没有痛苦就没有收获，痛苦越少，长久收获越少，因为越是简单，对人、企业的改变就越浅。</p><p></p><p>读者可能会觉得，手机不就是用着很简单吗？可以思考几个点再回答这个问题：一，单就打电话来讲，拨号电话可能是最好学的；二，手机貌似简单，但就日常使用时间来讲，大家花在手机学习上的时间一点儿都不少，通过手机获得的能力，也是对手机的再学习啊；三，为什么有些人还是用得不好，笔者自己就不是一个手机玩得很溜的人，用情不深，所以不专，笔者估计很多人也是，明明都没能充分发挥手机的能力，但是换代却还是跟得挺紧。</p><p></p><p>在数字化上就不是这样了，都没能深入进去，就已经开始否定了；明明看到了别的企业有脱胎换骨的变化，也还是更愿意承认自己无法改变。自己无法改变也就罢了，还要让数字化背这个“不好用”的锅。</p><p></p><h2>三</h2><p></p><p>不过，数字化也确实“不好用”，因为不是简单就能用好。</p><p></p><p>笔者这样写也不是让大家都搞大兴土木、大冒风险搞数字化变革，不同规模、不同目标、不同组织结构的企业，搞的东西也不一样，路径也不同，小的企业当然用低成本的工具和做法，比如低零代码，“随力供养”。笔者想表达的是，全社会统一流行“数字化”这个叫法，已经有三、四年时间了，企业应该达成共识的是数字化是目标导向。</p><p></p><p>一个企业搞数字化一定要明确希望成为什么样的自己，这个目标是在数字化之外的，先确定自己要干什么，再与之匹配数字化做法。这说法看着没什么稀奇，但是在一个企业中，达成这样一个共识并不容易，很多时候数字化背的锅恰恰是由于企业没有确定自己的定位，导致数字化要么太大、要么太小、要么就是没作用到该作用的人。说到底，顺着目标推演数字化的能力没有形成。</p><p></p><p>很多人讲数字化不是规划的，应该与时俱进，这个道理是对的，但这是说给行业中的“鲶鱼”听的。这些企业要么是跨界而来，要么是打算豪赌一场，对于大多数企业而言，他们做的数字化仍是完成现有生产体系、经营体系的转换，这个是有目标可言的，也是可以分解落实的，基础体系稳固建立、数字人才队伍随之形成后，才是准备考虑变身行业“鲶鱼”的时候，别总想着做完会过时，毕竟没做才更容易过时。</p><p></p><p>大多数企业并不是来数字化赌一场的，只是跟着时代切换生产方式而已。就像火箭发射一样，总得有个一二级火箭的助推过程，企业才能真上了轨道自由航行。也像火箭一样，如果不能历尽艰险，熬过助推过程，火箭炸了，也就没有自由航行了。</p><p></p><p>那么这种切换，最重要的是什么？自然是将人的思维模式调整到位，调整到可以利用数据要素、软件工具去充分提升每个岗位效能的程度，也就是能够定义数据要素、加速软件设计的程度，这个是数字化的“深意”，这种数字化需要教、学，但更需要掌握结构化的思维模式，自己去探、练，要自己有能力总结经验。</p><p></p><p>但是也有一点要注意，经验也是分类别的，通过点状试验、小工具推广、小项目速赢，只能总结处理小范围事物、简单数字化实现的能力和知识，无法总结体系性变革的知识，这就是笔者常讲的，无法通过几十个小项目换出一个大项目的操盘手来，因为两者的复杂度是不同的，没经历过大战，就不能练出大将。所以，又回到企业的目标上来了，是要培养一个面向大战的企业，还是一个面向小战的企业，以此来决定要做什么样的数字化，要让什么样的人领军，要付出什么样的时间和代价。</p><p></p><p>当然，无论是小战还是大战，数字化最终效率的提升，一定会作用到每个人身上，人人都有能力影响企业的数字化走向，才能让数字化真的转起来，创意无穷地转起来。结构化思维是数字时代的思维特征，无论小战还是大战都要用到这个，一定要有能力把你自己看到的事物，想说的东西，有结构地说清楚，这是数字时代大多数岗位必备的基本能力。所以，2024 年，着力在一切数字化工作中，培养人的结构化思维，让数字化的沟通效率上来，让业技融合真正发生在第一线。</p><p></p><p>培养人的结构化思维也是要区别对待的。新毕业的学生，经过十几年的教学，其实结构化思维的基础都很好，企业该做的是“保护”他们的结构化思维，新员工自己也要经常注意，不要被企业繁琐的日常工作“搞坏”了自己的结构化思考能力；对于已经工作一段时间的员工，企业要做的当然是“修复”，引导员工重新注意结构化思维的价值；对于中高层管理人员，这是必修课，企业的“一盘棋”不可能让没有全局性结构构建能力的人下出来。</p><p></p><h2>四</h2><p></p><p>数字化一定离不开企业自己的动手能力，参考下国外科技领先企业，也包括其他行业的一些领先企业，不少都是打造工具的高手，工具并非只是简单的软件而已，好的软件都是企业知识的沉淀，是企业知识的软件化，它是企业的宝贵资产，也是竞争武器，在关键领域，大家都用同一把枪，也就没什么竞争差异可言了，软件已经不再是主张简单的开箱即用的年代了，它已经是一个企业知识管理能力的表现。</p><p></p><p>在思考数字化的时候，还是要冷静客观。什么事情都有人做得好，有人做得不好，所以，执着地去跟“工具”发脾气是没意义的，笔者自己从事的企业架构、业务架构领域也经常这样，正反意见都有，但是以笔者自己的经验来看，“看开”是最好的学习思路，架构是观察事物的方法，因此不要执着于架构本身是什么，是你看到了什么。</p><p></p><p>所以，注意力要放在讨论看到的东西上，而不是讨论眼睛，如果觉得眼睛看不清了，就加个眼镜，这就是对方法的调整，最终都是为了能清晰地看到事物。看清了，就按照这个结构去做，这就是行为方式了，架构落地指的是行为方式，而不是看没看清，已经看清了，问题就不在眼睛了，在手和脑的配合，如果觉得看得不对，那就把眼睛叫来一起看下，而不是不管眼睛，手和脑“盲”做。</p><p></p><p>所以，企业架构落地这个事情，重点要讨论你看到的东西，以及是否按照看到的东西做，而不是总将精力放在讨论眼睛上，最终是眼、脑、手的配合，而不是眼睛自己好不好使的问题。说到底，数字化也是这个道理吧，“无住于相（不是只关注数字化的概念和案例）”，“不执一边（不是总觉得自己看透了，那样反倒容易陷入“增上慢”）”。</p><p></p><p>不过数字化也许真有个不变的东西，那就是不断磨炼人的结构化思维，以更好地使用数据和软件，也就是新要素和新工具，终归还是要人能驾驭工具才行。结构化思维并不神秘，它就是结构化地观察事物的方法，是一双“慧眼”，但不是“义眼”，不是买一个装上就行，需要自己锻炼，得出自己的观察套路和结论，大多数问题是能够在正确的观察下得到解决的，毕竟我们处理的都不是什么基因研究的课题。正如笔者常说的，我们真正用到设计的时候并不多，能够正确观察都可以解决很多问题了。</p><p></p><p>笔者之所以一直喜欢企业架构这个领域，正是因为企业架构就是一种结构化观察问题并寻找答案的方式，而且，它并不限制你必须用哪一种方法，你可以发挥创造。很多人不喜欢企业架构，实际上就是没有找到适合自己的结构和角度而已，误以为自己必须遵循什么，如果说真的有必须要遵循的东西，那笔者认为，可能就是最终必须要看出个结构来。</p><p></p><p>“东扯西扯”讨论一番，展望 2024 年，乃至以后，企业数字化的核心在于采用结构化思维多向内看，多向一线看，推动结构化思维在整个企业的渗透，只有培养和能够推动自己团队的人的发展，才能找到自己的路。总想走别人的路，可能会让自己走岔路或者无路可走。</p><p></p><p></p><h4>作者介绍</h4><p></p><p>付晓岩，《企业级业务架构设计：方法论与实践》、《银行数字化转型》和《聚合架构：面向数字生态的构件化企业架构》三书作者。北京天润聚粮咨询服务有限公司执行董事总经理，数孪模型科技公司高级副总裁；工业和信息化重点领域数字化转型与人工智能产业人才基地专家委员会副主任；中国计算机学会软件工程专委会委员、数字金融分会首批执行委员；信通院企业架构推进中心、组装式推进中心技术专家；中华全国数字化人才培育联盟专家委员会特聘专家；工信部中小企业发展促进中心产教融合产业实践教授；国家工程实验室金融大数据应用与安全研究中心研究员；CIC 金融科技与数字经济发展专家委员会成员；国家互联网数据中心产业技术创新战略联盟专家委员会副主任专家委员。</p><p></p><p>📣&nbsp;&nbsp;欢迎向「InfoQ 数字化经纬」投稿，与我们共享您的思考、洞见和实践经验！投稿可邮箱至 editors@geekbang.com（邮件标题前注明【数字化投稿】）</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6a/7f/6a727c625eae4byya4a6075aed39e17f.jpg\" /></p><p></p>",
    "publish_time": "2024-01-25 16:13:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云原生场景下Fluid加速AIGC工程实践",
    "url": "https://www.infoq.cn/article/3bYEFqvNim8fG8tBhCbb",
    "summary": "<p>阿里云高级技术专家&nbsp;车漾老师在QCon上海会议上，分享了在&nbsp;Fluid&nbsp;项目作为云原生AI场景下的数据和任务编排框架，在&nbsp;AIGC&nbsp;模型推理工程化落地方面做了许多优化探索的工作，包括简化云原生&nbsp;AI&nbsp;场景的分布式缓存管理和运维，降低资源成本；以及优化推理服务读取模型数据的效率，加速模型加载过程。同时也会演示了如何通过&nbsp;Fluid&nbsp;将一个&nbsp;LLM&nbsp;模型的推理加载速度提升近7倍，同时提供缓存弹性的能力，避免资源浪费的实践话题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97407972a14e44014d7a2ae50e60bd8f.png\" /></p><p></p><p>首先，大模型推理将在AI商业化时代中找到更广泛的应用，这比模型训练更具有竞争力。这是因为技术商业化是AI发展的必经道路，而模型的价值主要体现在其能否被广泛应用。我们直观理解，一个大模型只有两种可能的结局：要么它无用，之后就不会有后续；要么我们发现它非常有用，因此将被全球范围的用户所使用。如 OpenAI 和 Midjourney 等公司，用户都是在为每一次推理行为付费。随着时间的推移，我们可以预见，模型训练和模型推理的使用比重可能会达到3:7，甚至2:8。可以说，模型推理将是未来的主要战场。</p><p></p><p>然而，大模型推理带来的挑战主要表现在成本、性能和效率上，其中成本最为关键。随着模型规模的不断增大，所需要的运行资源也日益增多。而模型运行所依赖的 GPU 由于其稀缺，价格高昂，使得每一次模型推理的成本也随之上升。大模型推理的过程就像使用兰博基尼送外卖，速度越快，成本就越高。不过，用户往往只为价值买单，而不愿意为高昂的推理成本买单。因此，降低单位推理的成本成为了基础设施团队的重要任务。</p><p></p><p>此外，性能是我们的核心竞争力，特别是在面向消费者（ToC）领域的大模型中，更快的推理速度和更好的推理效果都是吸引和保持用户的关键。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/48456169e572a49f3941bbb66deb4c74.png\" /></p><p></p><p>随着云原生技术和架构发展，我们明显观察到IT架构的变化。传统的企业级应用、Web应用、微服务等领域都在从传统架构转向云原生架构。互联网应用大多是基于容器、Kubernetes、Prometheus等云原生技术实现的，追求弹性、灵活性以及最佳性价比。同时，通过标准化交付，提升生产流程的高效闭环。在标准API和标准架构的指导下，进一步提高多角色之间的协作和迭代效率。同样地，为了获得更多弹性算力供给、更高稳定性保证以及更快的交付，越来越多AI和大数据工作负载也运行在云原生架构上。</p><p>﻿</p><p>右图清晰地显示了这一趋势：最早是从可水平扩展的无状态应用程序（如Web应用、移动后端应用）开始；然后是NoSQL数据库、关系数据库、TensorFlow、PyTorch、Spark、Flink等大数据和典型机器学习的AI计算任务。都能够在Kubernetes容器集群上大规模运行。</p><p></p><p>三方研究机构的预测，显示出同样的趋势。早在2019年，Gartner就曾预测，到2023年，70%的人工智能应用程序将基于云原生等技术进行开发。IDC的研究则预测，到2025年，接近50%的企业内部的数据密集型或性能密集型计算工作负载都将迁移到基于云的架构上，而基于云的架构正是典型的云原生架构。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/108b595b2c3eaa8d66d6010acf24ed4d.png\" /></p><p></p><p>在&nbsp;AIGC&nbsp;推理场景下有个关键的矛盾，就是计算存储分离的架构导致的数据访问高延迟、带宽受限问题和大模型规模不断增长的矛盾，它会同时影响成本、性能和效率。</p><p></p><p>模型弹性伸缩、按需使用，是控制大模型成本的利器。然而，如上图右所示，以&nbsp;Bloom-175B&nbsp;模型（FP16&nbsp;精度模型大小约&nbsp;340GiB）为例，模型的扩容耗时为&nbsp;82&nbsp;分钟，接近&nbsp;1&nbsp;个半小时，为了找到问题的根因，需要对模型启动时间进行拆解，其中主要耗时在于&nbsp;HPA&nbsp;弹性、创建计算资源、拉取容器镜像，加载模型。可以看到从对象存储加载一个约&nbsp;340G&nbsp;的大模型，耗时大约在&nbsp;71&nbsp;分钟，占用整体时间的&nbsp;85%，这个过程中我们其实可以看到&nbsp;I/O&nbsp;吞吐仅有几百&nbsp;MB&nbsp;每秒。</p><p></p><p>要知道在&nbsp;AWS&nbsp;上&nbsp;A100&nbsp;按量付费的价格每小时&nbsp;40&nbsp;美元，而模型启动时刻&nbsp;GPU&nbsp;其实是处于空转的时刻，这本身就是成本的浪费。同时这也影响了模型的启动性能和更新频率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/80043e2bd7c9f0495d0bcf07aceb06d0.png\" /></p><p></p><p>那么，我们有办法解决这个问题吗？一个直观的想法是增加一个缓存层，但是真的增加了缓存层就可以了吗？实践中其实并不是这样的，我们会遇到一系列的问题。</p><p>首先就是快的问题：能否用好缓存，如果加了缓存但是速度依旧不快，那么是缓存的规划问题？硬件配置问题？还是软件配置？网络问题？调度问题？</p><p>其次就是省：关注成本问题，作为缓存的机器通常是高带宽、大内存、本地盘的机器，这些配置的机器往往并不便宜。如何能够实现性能最大化的同时也有合理成本控制。</p><p>接着就是好：用户使用复杂不？用户代码是否需要相应的修改。运维团队工作量大吗？模型会不断更新和同步，如何降低这个缓存集群的运维成本。简化运维团队的负担。</p><p></p><p>正是在这种对于缓存工程化落地的思考中，诞生了Fluid这个项目。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/08d6e4d876d3c0edff3d360dbc43c9c1.png\" /></p><p></p><p>首先，我们来探索Fluid的核心概念。Fluid在Kubernetes中负责编排数据及其计算任务，这不仅包括空间编排，还囊括时间编排。空间编排意味着计算任务将优先调度到已缓存数据的节点或近似节点上，以此提升数据密集型应用的性能。时间编排则允许我们同时提交数据操作和任务，但是在任务执行之前，我们需要执行一些数据迁移和预热操作，以确保任务在无人值守的情况下也能顺利进行，从而提升工程效率。</p><p></p><p>观察Fluid的架构图，我们可以看到Fluid可以对接各种AI/大数据应用，并且可以与各种异构存储系统进行链接。目前，Fluid已支持包括Alluxio、JuiceFS和阿里内部自研的JindoFS、EFC等多种缓存系统。</p><p></p><p>具体而言，Fluid可以提供五种核心功能：</p><p>首先它实现了标准化。为了满足针对场景的数据访问方式（比如大语言模型、自动驾驶的仿真数据以及图像识别的小文件等），Fluid提供了同样的优化数据访问方式。同时，随着越多的分布式缓存（如JuiceFS、Alluxio、JindoFS、EFC）的出现，为了在Kubernetes上使用它们，我们需要制定一种标准的API。Fluid将这些分布式缓存系统转化为有可管理、可弹性、可观测性和自我修复能力的缓存服务，并暴露Kubernetes&nbsp;API。其次是自动化。Fluid使用CRD的方式来进行数据操作，预热，数据迁移，缓存扩容等多种操作，便于用户整合到自动化运维体系中。再者是性能优化。通过针对特定场景的分布式缓存和任务缓存亲和性调度，可以显著提升数据处理性能。平台独立。无论是原生Kubernetes、边缘Kubernetes、Serverless&nbsp;Kubernetes还是Kubernetes多集群等环境，Fluid都能胜任，并根据环境需要可以通过CSI&nbsp;Plugin和sidecar来选择不同的模式运行存储客户端。最后是数据和任务编排。Fluid能支持自动化操作流程，以数据集为中心，定义数据迁移、预热和任务的执行顺序依赖关系。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/77627ff6be6b53cff1737cfee6e7a35c.png\" /></p><p></p><p>那么回到AIGC模型推理场景，Fluid为这个场景带来了许多优化方案。</p><p></p><p>首先，一种常见的挑战是，分布式缓存的使用复杂度高且运行环境差异大。对于AI和大数据应用来说，我们可能需要适配多种运行时，例如已经被广泛使用的Alluxio、Jindo、JuiceFS，以及最新的Dragonfly。同时，这些应用可能会在各种运行时环境中执行，包括公共云、私有云、边缘云和Serverless云。在这样的背景下，Fluid提供了一种一键部署的解决方案，使各种环境能够无缝衔接。</p><p></p><p>第二，AI和大数据模型推理服务在业务属性上具有很高的灵活性，而Fluid则通过提供弹性缓存能力，帮助用户在性能和成本之间实现最大化的权衡。具体来说，当您需要更多的缓存空间时，Fluid能够自动扩展；而在不需要时，它也能自动缩减。</p><p></p><p>第三，Fluid以数据为中心，实现了数据感知调度。这意味着，计算任务会尽可能地调度到离数据最近的地方，提升运算速度，从而极大地提高了应用的性能。</p><p></p><p>第四，Fluid提供了数据流编排能力，能够自动化处理复杂的数据消费和模型推理行为，从而降低用户在操作上的难度。</p><p></p><p>最后，在性能优化方面，Fluid拥有一种适用于云原生缓存的读取优化方案，可以充分利用节点资源，使处理速度得到进一步提升。</p><p></p><p>总的来说，Fluid是一种旨在提高运算性能、简化操作流程，并能适应多种运行环境的工具，无论您的AI或大数据应用在何种环境中运行，都能从Fluid中获益。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a9186f39383ad527809c5803fe7f6ecd.png\" /></p><p></p><p>Fluid是一种分布式数据和计算架构，旨在提供易于使用的、一致的和高效的数据访问。它由两个主要部分构成：Dataset和Runtime，它们分别代表需要访问的数据源和对应的缓存系统。下面我们详细解析一下。</p><p></p><p>首先看到的是Dataset，它代表需要访问的数据源。在这个例子中，数据源被描述为OSS存储桶中的一个子目录，用户通过创建Dataset对象来声明他们的数据源。</p><p></p><p>接下来是Runtime，代表对应的缓存系统。在这个例子中，缓存系统是Alluxio，因此，对应的是AlluxioRuntime的CRD。CRD（Custom&nbsp;Resource&nbsp;Definition）是Kubernetes提供的一种扩展机制，用于自定义资源。</p><p></p><p>当用户创建了Dataset和对应的Runtime后，Fluid会接管后续的所有工作。首先，Fluid会自动完成对应的缓存系统的配置，然后，它会自动拉起缓存系统的组件。接着，Fluid会自动创建一个PVC（Persistent&nbsp;Volume&nbsp;Claim）。PVC是&nbsp;Kubernetes中的一种资源，它提供了对存储系统的抽象，使得用户无需知道具体的存储系统类型和参数，就可以使用存储。</p><p></p><p>当这些准备工作完成后，希望访问模型数据的推理应用只需要挂载这个PVC，就可以从缓存中读取模型数据。与此同时，因为Fluid使用的是Kubernetes标准的存储方式，所以用户可以以熟悉的方式来访问和操作数据。</p><p></p><p>可以看到，Fluid通过提供简洁明了的接口，自动化地完成了数据源的声明、缓存系统的配置和管理，极大地降低了数据访问的复杂度，让用户可以专注于他们的业务逻辑。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5f8af4566c020b7cda9ff31d2da66de1.png\" /></p><p></p><p>AIGC&nbsp;推理的运行平台非常多样化，包括云服务的&nbsp;Kubernetes、自建的&nbsp;Kubernetes、边缘的&nbsp;Kubernetes&nbsp;以及&nbsp;Serverless&nbsp;形态的&nbsp;Kubernetes。Serverless&nbsp;形态的&nbsp;Kubernetes&nbsp;由于其易用性、低负担的好处，已经越来越多的成为用户的选择；但是&nbsp;Serverless&nbsp;由于安全的考量，没有开放第三方存储接口，所以只支持自身存储，以阿里云为例子，只有&nbsp;NAS，OSS，CPFS&nbsp;有限存储。</p><p></p><p>在&nbsp;Serverless&nbsp;容器平台上，Fluid&nbsp;会将&nbsp;PVC&nbsp;自动转换成可以适配底层平台的&nbsp;sidecar，开放了第三方的配置接口，可以允许并且控制这个&nbsp;sidecar&nbsp;容器的生命周期，保证它在应用容器启动前运行，当应用容器结束后自动退出。这样&nbsp;Fluid&nbsp;则提供了丰富的可扩展性，可以运行多种分布式缓存引擎。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4b2e7efea1195d5cfa96e5209c2ee33d.png\" /></p><p></p><p>Fluid提供的可弹性伸缩的计算侧分布式缓存，对于大规模并发，需要快速高效地读取数据的AI应用来说，尤其重要。</p><p></p><p>只依赖简单的分布式缓存，问题在于当很多服务实例需要拉取数据时，每个实例能获得的带宽将会受到限制。例如，100个推理服务实例同时启动，并需要从对象存储中拉取数据，那么，由于总的可用带宽是固定的，每个实例能份到的带宽就只有总带宽的百分之一。这可能导致数据拉取的延时显著增加，从而影响性能。</p><p></p><p>通过弹性伸缩的计算侧分布式缓存，我们将底层存储系统的有限带宽，扩展到了可以弹性扩容的Kubernetes集群内。这个集群内的可用带宽取决于分布式缓存的节点数量，从而可以根据业务需求，及时进行伸缩，提供灵活而高效的I/O处理。</p><p></p><p>测试数据也证明了弹性伸缩的计算侧分布式缓存的优势。在100个Pod并发启动的情况下，使用缓存可以显著加速，如果使用更多的缓存worker节点，效果会更好。因为大规模的缓存节点能提供更大的聚合带宽，因此每个Pod份到的带宽就越多。同样，当你使用更多的分布式缓存节点时，聚合带宽也会近线性地提升。</p><p>归结起来，弹性伸缩的计算侧分布式缓存，不仅能够灵活地应对业务的变化需求，还能根据需求提供充足的带宽，保证了AI模型推理服务的性能，这是简单部署分布式缓存所不能实现的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a3bfba9ee4364ced92af6ebc9d7c0e4.png\" /></p><p></p><p>介绍完如何简单地部署缓存后，接下来考虑的问题就是如何在尽可能节省成本的前提下最大化缓存带来的性能提升，如何在成本和性能间取得平衡实质上是与业务场景的I/O访问模式相关的。Fluid在缓存上暴露的可观测性，配合手动扩缩容、HPA、CronHPA等Kubernetes的扩缩容能力，可以根据业务需求弹性扩容、缩容数据缓存。我们可以举几个具体的例子：</p><p></p><p>对于大语言模型场景，大语言模型一个特点是拥有很强的泛化知识，因此把一个LLM加载到GPU显存后，它其实可以为多种不同的场景提供服务。因此这种业务的数据I/O特点是一次性对I/O有很高的要求。对应到缓存的弹性上来，就是一个先扩容，推理服务就绪后缩容到0的过程。再来看文生图Stable&nbsp;Diffusion的场景，假如是一种SD模型市场的场景，那就会包含大量不同风格的SD模型，因此尤其对于热点模型，会有持续的I/O需求，此时保持一定的缓存副本就是更好的选择。而无论哪种场景，如果因为业务洪峰造成服务端需要扩容，数据缓存可以跟随它做临时扩容，缓解扩容时的冷启动问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec29f7ee16fc57e92ce5955b1bfec8bf.png\" /></p><p></p><p>公共云的一个主要优势是其灵活的弹性能力和高可用性，这些都是通过底层的多可用区（Multiple&nbsp;Availability&nbsp;Zones）来实现的。多可用区设计可以为互联网应用带来极高的稳定性，即使牺牲了一些性能。</p><p></p><p>然而，在AIGC大模型场景中，我们发现跨可用区的延时可能会有较大影响，这是因为大模型文件往往体积较大，它传输的数据包就会非常多，这就放大了延时的影响。</p><p></p><p>因此，缓存和使用缓存的应用之间的亲和性就显得非常重要。亲和性在这里指的是Kubernetes的一种调度策略，通过设置亲和性，可以告诉Kubernetes调度器，在调度Pod时，要尽量将拥有相同亲和性的Pod调度到同一台节点上或者尽量不调度到同一台节点上。</p><p></p><p>Fluid解决这个问题的方式是提供无侵入性的亲和性调度。无侵入性的亲和性调度是根据缓存的地理位置来调度应用，优先在同一可用区内进行调度。这样就可以避免跨可用区的延时。</p><p></p><p>与此同时，Fluid也提供了弱亲和性和强亲和性的可配置性。弱亲和性是指如果条件允许，Kubernetes会尽量按照亲和性策略进行调度，但如果无法满足，则可以打破这个策略；而强亲和性则是指必须严格按照策略进行调度，不能打破。这两种配置提供了使用缓存的灵活性，可以根据不同的业务需求进行选择。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9b/9b333f61445ea171c7133420cd09c9d2.png\" /></p><p></p><p>在传统的AI模型推理业务上线过程中，确实存在许多的复杂操作和耗时步骤。例如部署和扩容分布式缓存，预热模型数据以避免Cache&nbsp;Miss，启动多个服务实例，以及在确认服务无误后缩容缓存以减少成本。这些步骤需要人工介入，并在每一步之间花费大量时间来确认状态，并进行下一步操作。</p><p></p><p>为解决这一问题，Fluid将数据消费流程标准化，并通过数据操作抽象以及数据流编排能力，使得这些流程得以自动化进行。换言之，Fluid将和数据缓存相关的步骤（如数据迁移，预热，以及和业务相关的数据处理等）描述为Kubernetes级别的抽象，用户可通过这些抽象去直接描述和控制数据处理的全过程。这在大大简化操作复杂度的同时，也有效缩短了整个业务上线的时间。</p><p></p><p>总的来说，Fluid利用Kubernetes的能力，将AI模型上线的业务流程标准化，并通过提供数据操作的抽象及数据流的编排能力，使得整个流程更为高效，并减少了人工介入的程度，大大提高了业务上线的效率和误操作犯错的可能性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/549c4d9cf448538a1e3e67fad8e25df5.png\" /></p><p></p><p>这些数据操作可以串联成一条数据流。于是刚才我们提到的这个例子，就可以用&nbsp;5&nbsp;步数据操作来轻松定义。运维人员只需要一次性提交这条数据流，Fluid&nbsp;会自动地完成整个&nbsp;AI&nbsp;模型推理服务发布的流程，提升使用缓存过程的自动化比例。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef55451b4692b22bf65787002425777e.png\" /></p><p></p><p>那么刚才提到的“用好缓存”的技巧其实都在资源成本和运维效率方面。但实际测试过程中我们发现，服务启动过程使用的带宽远小于这些GPU计算实例可用的带宽，这意味着模型的加载效率在客户端上仍然有可以优化的空间。</p><p></p><p>从节点吞吐情况上可以看到，这些AI推理的运行时框架会以单线程的方式去按序读取模型参数，这在非容器环境是没有什么问题的，如果使用本地SSD盘存储模型参数，加载吞吐很容易就可以到达3～4GB/s。但是在计算存储分离架构下，哪怕我们使用了缓存，缓存也需要使用用户态文件系统（也就是FUSE）这种技术挂载到容器中。FUSE自身的开销和额外的RPC调用，都使得read请求的延时变得更高，单线程所能达到的带宽上限也就更低了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/76efeb875c251d61c615e41947d903ae.png\" /></p><p></p><p>为了最大化发挥分布式缓存提供的巨大I/O吞吐，Fluid可以提供一个Python的SDK，在用户代码中使用多线程读和预读的方式去加速模型加载过程。从我们的测试结果来看，额外使用这种客户端的优化，可以在使用计算侧分布式缓存的基础上，将冷启动耗时缩短一半，做到1分钟内拉起一个接近100G的大模型。从右下角的这个I/O吞吐情况，也可以看出，我们更充分地利用了GPU计算节点的带宽资源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b12ddf6a93ab7983674b45a3596a70d7.png\" /></p><p></p><p>我们首先看一下直接访问&nbsp;OSS&nbsp;存储的运行效果。这里我们已经创建好了&nbsp;OSS&nbsp;的&nbsp;PV&nbsp;和&nbsp;PVC。</p><p></p><p>接着，我们定义一个&nbsp;deployment：deployment&nbsp;Pod&nbsp;中挂载刚才的&nbsp;OSS&nbsp;PVC，使用的容器镜像是&nbsp;TGI&nbsp;镜像。还有声明使用&nbsp;1&nbsp;张&nbsp;GPU&nbsp;卡，用于模型推理。接着把&nbsp;deployment&nbsp;创建下去。然后我们看下这个服务的就绪时间，这边&nbsp;5&nbsp;倍速加速了一下。终于就绪了，可以看到整个过程耗费了&nbsp;101s，考虑到我们的模型大小仅为&nbsp;12.55G，这个时间可以说是比较长的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7ff445877284bd454dbc025a5d197b41.png\" /></p><p></p><p>最后，让我们看看&nbsp;Fluid&nbsp;的优化效果。我们需要定义&nbsp;Fluid&nbsp;的&nbsp;Dataset&nbsp;和&nbsp;Runtime&nbsp;资源，并将分布式缓存部署到集群中。定义数据源、节点个数以及缓存数据的存储介质和大小。由于我们是初次部署弹性分布式缓存，这可能需要约&nbsp;40&nbsp;秒的时间。缓存准备完成后，我们可以看到一些缓存的监控信息。PVC&nbsp;和&nbsp;PV&nbsp;也会自动创建。然后，我们定义一个新的&nbsp;deployment，只需要进行几个修改：</p><p>添加一个&nbsp;annotation&nbsp;来触发&nbsp;Fluid&nbsp;的自动数据预热将&nbsp;OSS&nbsp;的&nbsp;PVC&nbsp;更改为由&nbsp;Fluid&nbsp;Dataset&nbsp;自动创建的&nbsp;PVC替换为一个使用了客户端优化的镜像</p><p></p><p>观察服务的就绪时间，我们可以看到部署只花了&nbsp;22&nbsp;秒。我们还可以尝试对现有的&nbsp;deployment&nbsp;进行扩容，观察第二个服务实例的启动时间。由于所需的模型数据已被完全缓存，第二个服务实例只需&nbsp;10&nbsp;秒就能准备就绪。这个例子展示了&nbsp;Fluid&nbsp;优化的效果，我们成功提升了服务的启动速度约&nbsp;10&nbsp;倍。</p><p></p><p>总结：</p><p></p><p>综上，可以看到Fluid&nbsp;为&nbsp;AIGC&nbsp;模型弹性加速提供开箱即用、优化内置的方案，在达到更好性能的同时还可以降低成本，同时还包含端到端的自动化能力；在此基础上使用&nbsp;Fluid&nbsp;SDK&nbsp;可以进一步充分发挥&nbsp;GPU&nbsp;实例的带宽能力实现极致的加速效果。</p><p></p><p>【活动推荐】</p><p></p><p>目前，极客邦科技正在策划2024年6月14-15日<a href=\"https://archsummit.infoq.cn/2024/shenzhen/\">ArchSummit架构师峰会</a>\"深圳站会议，本次会议将围绕“智能进阶.架构重塑”大主题方向，探讨在AI浪潮下，架构设计如何匹配智能化趋势，以适应日益复杂的业务需求。会议上将讨论大模型、AI运维、数据架构、应用架构、业务架构、微服务、高可用以及数字化转型等多个层面的话题，感兴趣的可以点击阅读原文查看ArchSummit会议官网。<a href=\"https://archsummit.infoq.cn/2024/shenzhen/\"></a>\"</p>",
    "publish_time": "2024-01-25 16:39:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Katalyst v0.4.0 发布：潮汐混部与资源超分",
    "url": "https://www.infoq.cn/article/NRufHCzQmrR3oDZ0OIBq",
    "summary": "<p></p><p>Katalyst 是字节跳动开源的成本优化实践系统，致力于解决云原生场景下的资源不合理利用问题，为资源管理和成本优化提供解决方案。</p><p></p><p>来源&nbsp;| KubeWharf 社区</p><p>项目 |&nbsp;github.com/kubewharf/katalyst-core</p><p></p><p>近日，Katalyst 社区完成了 0.4.0 版本发布。除了持续优化 QoS 能力之外，我们还在新版本中提供了可以独立在原生 Kubernetes 上使用的潮汐混部和资源超售能力。</p><p></p><p>和在离线常态混部一样，这些能力是字节跳动在不同业务场景中实现降本增效的技术手段，我们在抽象出标准化能力之后也进行了开源，期望这些能力可以帮助用户以更低的落地成本完成资源效能提升。</p><p></p><p></p><h1>潮汐混部</h1><p></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>通过给应用分配差异化的 QoS 等级，Katalyst 可以基于资源隔离和动态调控能力实现在单机维度的在离线业务混部，即常态混部。这种混部模式虽然可以实现极致的资源效能提升，但是也增加了基础设施的复杂度。同时因为引入了例如 Reclaimed 资源这样的概念，要落地常态混部往往还需要做一些业务侧的适配。</p><p></p><p>为了让用户可以以更低的成本落地混部能力，在 v0.4.0 中，Katalyst 提供了潮汐混部（Tidal Colocation）功能。</p><p></p><p></p><h2>技术解读</h2><p></p><p></p><p>在潮汐混部中引入了潮汐节点池的概念，并且将集群中的节点划分为“在线”和“离线”两种类型。潮汐混部主要分为两个部分：</p><p></p><p>实例数管理：通过 HPA、CronHPA 等各种横向扩缩能力来管理在线业务的实例数，在夜间可以腾出资源给离线业务使用</p><p></p><p>潮汐节点池管理：Tidal Controller 基于设定好的策略对潮汐节点池中的节点做 binpacking，将腾出的资源折合成整机出让给离线业务</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c6/c68f047668f4098353a888df874abffc.png\" /></p><p></p><h2>使用</h2><p></p><p></p><p>在集群中选取加入潮汐节点池的节点，并为节点打上某个 Label</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f52889882fd25165fb4aae83eef8ef30.png\" /></p><p></p><p>2. 创建潮汐节点池配置</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04231f3eb1a56885dded04842d4f04b8.png\" /></p><p></p><p>3. 潮汐控制器为节点打上对应的标签和污点，并且会根据各个节点的负载情况动态做 Binpacking 调整节点角色</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bfb44fc4c5e83c4d35da9414713e2b60.png\" /></p><p></p><p>部署在线离线业务，为应用打上相应标签和污点容忍，并配置 HPA 规则</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c287f2de0a100784373a9859d40e5e90.jpeg\" /></p><p></p><p></p><h1>在线超分</h1><p></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>在线业务的资源使用量往往会随着访问数量的波动而变化，具备明显的潮汐特性。为了确保业务的稳定性，用户通常会以峰值时消耗的资源量作为申请的依据，而且往往会有过度申请资源的倾向，这些资源会被浪费。</p><p></p><p>Katalyst 提供了在离线混部的能力作为解决上述问题的方式之一，但是在一些场景下，在离线混部可能不便于落地，比如：</p><p></p><p>负载类型比较单一，只有在线业务业务方不愿意改变申请资源的协议来申请 Reclaimed 资源</p><p></p><p>在新版本中，Katalyst 针对在线业务场景，提供了一种简单的、对业务方无感的资源超分方案，便于用户快速提升资源利用率。</p><p></p><p></p><h2>技术解读</h2><p></p><p></p><p>Over-commit Webhook：劫持 kubelet 上报心跳的请求，并对 Allocatable 资源量进行放大Over-commit Controller：超分配置管理Katalyst Agent：通过干扰检测和驱逐，保障超分后节点的性能和稳定性；根据指标数据，计算并上报动态的超分比Katalyst Scheduler：对需要绑核的 Pod 进行准入，避免超分导致实际无法绑核而启动失败</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/dfeb304e36cad46b7669b6d6e9d41876.png\" /></p><p></p><p></p><h2>使用</h2><p></p><p></p><p>为需要超分的节点池的节点打上 Label</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4cc5585c36b5be798d0ac72e9b171998.png\" /></p><p></p><p>2. 创建超分规则</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed3f39ee42ab6b26852a73c659adb8bb.png\" /></p><p></p><p>3. 观察 Node 对象，发现 Katalyst 将超分比、未超分时的资源量更新到 Annotation 中，并根据超分比对 Allocatable 和 Capacity 进行了放大</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5efee26227fd5678a37841631bde4100.jpeg\" /></p><p></p><p></p><h1>NUMA 粒度混部内存管控框架</h1><p></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>Katalyst 当前的混部策略只考虑整体机器的可用资源，导致离线任务在 NUMA 跨度申请内存时，内存容量和带宽在各 NUMA 间分布不均匀。这种情况下，当前的混部策略往往无法精确控制内存使用量，进而引起内存压力。</p><p></p><p>针对这种情况，我们提出了一种精细化的 NUMA 粒度内存管控框架，旨在通过 sysadvisor 计算 memory provisions，并与 qrm memory plugin 交互，实现更细致的 NUMA 内存管理。这将使 qrm memory plugin 能够根据 memory provisions 进行 NUMA 细粒度内存控制。</p><p></p><p></p><h2>技术解读</h2><p></p><p></p><p>在 Sysadvisor 的 Memory Plugin 中，我们引入了名为 memoryProvisioner 的插件，负责计算每个 NUMA 的内存供应逻辑。</p><p></p><p>为增强其可扩展性，我们设计了 ProvisionPolicy 接口，包含 Update 和 GetProvision 两个方法，分别用于定期更新内存供应量和获取 provision 建议。MemoryProvisioner 插件实现了 MemoryAdvisorPlugin 接口。</p><p></p><p>此策略基于 Memory Headroom的 PolicyNUMAAware 策略，通过遍历每个物理 NUMA 及其 pod，计算每个 NUMA 的内存供应量。具体计算逻辑包括分析 NUMA Exclusive 设置，获取每个 NUMA 节点的空闲内存，并应用公式考虑 reclaimed cores、系统 scale_factor 和 reserved 内存，以实现更均衡的 NUMA 内存分配。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18bacd5f97a69a38512540b2ec91c088.png\" /></p><p></p><p></p><h2>使用</h2><p></p><p></p><p>katalyst-agent 添加了 memory-provision-policy 的启动参数，用于指定计算策略，默认是 canonical。用法如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b188619da2acb85c07afd92b643700b0.png\" /></p><p></p><p></p><h1>支持 OOM 优先级作为 QoS 增强</h1><p></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>目前，Kubernetes 中 pod 的 OOM 优先级主要受其 QoS 级别与其对内存的申请量、使用量影响。然而，当前混部场景下，kubelet 原生的 oom_score_adj 计算策略已经不能很好的满足需求，例如：</p><p></p><p>需要给两个都映射到原生的 Burstable 级别的 shared_cores pods 设定 OOM 优先级需要在两个原生都是 Guaranteed 级别的 dedicated_cores pod 和 shared_cores pod 之间设定 shared_cores pod 要早于 dedicated_cores pod OOM</p><p></p><p>此外，当前 kubelet 中提供的静态 oom_score_adj 计算机制，不支持 OOM 优先级的动态调整。因此 Katalyst 提供了一个关于 OOM 优先级的 QoS Enhancement，支持更加灵活地为 pods 设置 OOM 优先级。</p><p></p><p></p><h2>技术解读</h2><p></p><p></p><p>Katalyst 通过在内核添加 ebpf 的方式实现用户自定义的 OOM 策略注入，并在上层 qrm memory plugin 中完成用户定义策略的解析以及 OOM Priority 的配置下发。</p><p></p><p></p><h2>使用</h2><p></p><p></p><p>OOM Priority 信息通过 annotaion 在 pod 上进行指定</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/51ac5c7de10b1891074d0dd94512fee4.png\" /></p><p></p><p>priorityValueInt 的取值越大表示优先级越高，并且取值范围受 pod 所指定的 QoS level 影响。</p><p></p><p></p><h1>支持拓扑感知调度</h1><p></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>在搜索、广告、推荐、游戏、AI 分布式训练等业务场景下，用户对时延的敏感性较高，对容器在微拓扑级别的摆放方式存在要求。原生 K8s 的微拓扑管理能力存在一些局限，调度器不感知微拓扑，可能导致出现较多的因不满足 NUMA 亲和要求而造成的 Admit 失败。</p><p></p><p>因此，Katalyst 在 v0.4.0 实现了拓扑感知调度功能，支持两种模式：</p><p></p><p>Native 策略：兼容 K8s 原生的 NUMA 亲和和绑核策略Dynamic 策略：混部场景下增强的绑核策略，对于 dedicated_cores QoS 级别，支持了 NUMA 亲和 (numa_binding) 以及 NUMA 独占 (numa_exclusive) 两种语义</p><p></p><p></p><h1>其他</h1><p></p><p></p><p>SysAdvisor 框架支持对接自定义业务模型，调优 rama provision policy 计算结果</p><p></p><p>QRM 支持设置整机和容器级别 TCP Memory 上限，缓解 TCP 内存满导致的丢包问题</p><p></p><p>Eviction 集成 RootFS 驱逐能力，定制排序策略和 QoS 级别驱逐阈值</p><p></p><p>KCMAS 优化存储数据结构和索引，支持多 tag 能力</p><p></p><p>ServiceProfilingDescriper (SPD) 支持服务维度的混部 baseline 和 per-pod 灰度能力</p><p></p><p>社区开发切换到基于 owner review 模式</p><p></p><p>基于超时实现死锁检测功能</p><p></p><p></p><h1>新版本体验路径</h1><p></p><p></p><p>请参考社区官方文档体验 Katalyst 潮汐混部和资源超分能力:</p><p></p><p>潮汐混部：gokatalyst.io/docs/user-guide/tidal-colocation/资源超分：gokatalyst.io/docs/user-guide/resource-overcommitment/</p><p></p><p></p><h1>感谢贡献者</h1><p></p><p></p><p>在本次新版本的发布过程中，社区也迎来了不少新的贡献者，在此向他们的付出表示由衷感谢：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee356ed3668a728e4cfc8004da002452.png\" /></p><p></p><p>非常期待更多开发者和用户加入到 Katalyst 开源社区中，和我们一起交流和探讨在离线混部以及资源效能的相关话题。</p><p></p><p>如需火山引擎商务交流：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/739d8ee0efcbea4ec95ceb902432eed3.png\" /></p><p></p><p>如需开源交流，添加字节跳动云原生小助手，加入云原生社群：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/54ffb1ae722aebe181e8023a9bc85c63.png\" /></p><p></p><p>文章专题推荐：<a href=\"https://www.infoq.cn/theme/225\">《字节跳动云原生创新实践与开源之路》</a>\"</p><p></p><p></p><p></p><p></p><p></p>",
    "publish_time": "2024-01-25 17:49:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]