[
  {
    "title": "开源软件包ngrok-go致力于让Ingress更容易地嵌入到Go应用中",
    "url": "https://www.infoq.cn/article/H1pfOQDy4642xHG59I2D",
    "summary": "<p><a href=\"https://blog.ngrok.com/posts/ngrok-go\">Ngrok-go</a>\"是一个地道的（idiomatic）的Go软件包，它使Go应用程序能够通过ngrok的ingress即服务（ingress-as-a-service）平台安全地接收网络请求，就像监听本地端口一样。</p><p></p><p>ngrok-go旨在简化网络ingress的创建，它负责处理网络栈中不同层的一些低级网络基元（primitive），这些基元目前需要由开发人员来设置，以启用网络ingress。它们包括DNS、TLS证书、网络级CIDR策略、IP与子网路由、负载均衡、VPN和NAT。</p><p></p><p>ngrok-go可以看作一种将ngrok代理打包并嵌入Go应用程序的方式，从而为捆绑ngrok来创建网络ingress的应用消除了巨大的复杂性，例如物联网设备、CI/CD流水线等。</p><p></p><p></p><blockquote>ngrok-go让开发人员只需一行代码就能将Go应用发布到互联网上，而无需设置IP、证书、负载均衡器甚至端口等低级别的网络基元!</blockquote><p></p><p></p><p>要通过ngrok将ingress嵌入Go应用，开发人员只需调用ngrok-go提供的ngrok.Listen原语即可。如果你的环境有ngrok authtoken的话，对Listen的调用将初始化与ngrok的安全和持久连接，并传输你所指定的配置需求，包括URL、认证、IP限制等，例如：</p><p></p><p><code lang=\"go\">ngrok.Listen(ctx, \n       config.HTTPEndpoint(\n        config.WithDomain(\"my-app.ngrok.io\"),\n        config.WithAllowCDIRString(\"192.30.252.0/22\"),\n        config.WithCircuigBreaker(0.8),\n        config.WithCompression(),\n        config.WithOAuth(\"github\")\n       ), \n       ngrok.WithAuthtokenFromEnv(),\n)</code></p><p></p><p>调用Listen时指定的所有策略都由ngrok执行，它在边缘处拒绝所有未经授权的请求，这意味着只有有效的请求才能到达Go应用。</p><p></p><p>使用ngrok-go的另一个优势是它在ingress方面的“可移植性”。这意味着使用它的应用将以同样的方式运行，与底层平台无关，无论是裸机、虚拟机、AWS、Azure、Kubernetes等均是如此。</p><p></p><p>虽然ngrok选择Go作为第一个支持的语言，但对其他语言的支持已经在进行中，包括<a href=\"https://github.com/ngrok/ngrok-rs\">Rust</a>\"和<a href=\"https://github.com/ngrok/ngrok-js\">JavaScript</a>\"。对其他语言（如Java、C#、Python和Ruby）的支持预计很快就会纳入路线图，这也会基于用户的反馈。</p><p></p><p>如果你有兴趣尝试，<a href=\"https://github.com/ngrok/ngrok-go\">ngrok-go可以在GitHub上找到</a>\"，<a href=\"https://ngrok.com/docs/using-ngrok-with/go/\">官方的入门指南</a>\"是一个很好的起点。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/ngrok-go-ingress-package/\">Open-Source Package ngrok-go Aims to Make it Easier to Embed Ingress into Go Apps</a>\"</p>",
    "publish_time": "2023-03-29 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "部署太慢，我们用 Warm Docker 容器将速度提高了 5 倍",
    "url": "https://www.infoq.cn/article/XNAGHzotjohTSqoWDE9S",
    "summary": "<p></p><h2>背景</h2><p></p><p>&nbsp;</p><p>我们使用 <a href=\"https://dagster.io/cloud\">Serverless Dagster Cloud</a>\" 来开发和部署 Dagster 代码，无需设置本地开发环境或任何云基础架构。当提交更改到 GitHub 时，<a href=\"https://github.com/features/actions\">GitHub Action</a>\" 会直接构建和部署代码到 Dagster Cloud，然后可以在界面上查看并与Dagster 对象进行交互。 Dagster Cloud 可以利用一个远程环境来共享部署，并且可以利用自动创建的临时环境与合作者协作，实现了个人本地开发和共享远程环境的结合。</p><p>&nbsp;</p><p>最初，我们在 Dagster Cloud Serverless 中使用了标准的基于 Docker 的构建流程，但很快发现这个流程会使“编辑-部署-运行”的循环变得非常缓慢。为了将过程加速，我们实现了一个在 Docker 镜像之外运送代码的系统。本文将描述我们分析的问题、选择的解决方案以及在此过程中做出的各种权衡。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/9b/d3/9b34086474abddb6033d075ff75780d3.png\" /></p><p></p><h2>Docker 镜像存在的问题</h2><p></p><p>&nbsp;</p><p>当我们在 GitHub 上构建 Docker 镜像并将其部署到 Dagster Cloud 时，每次提交需要 3～5 分钟才会在 Dagster UI 中显示。在每次迭代中，无服务器开发人员通常会对代码进行微小更改，但是必须等待 3 分钟以上才能看到该更改的效果，这很快就会变得非常烦人。我们分析了“当你更改一行代码并提交时会发生什么”，并发现以下问题：</p><p><img src=\"https://static001.infoq.cn/resource/image/87/43/87b02ce1db4d5165c7773aa2650d3c43.png\" /></p><p></p><p>你可以看到，有两样东西花了最多的时间：</p><p>&nbsp;</p><p>构建 Docker 镜像（60 ~ 90 多秒）部署 Docker 容器（90 秒）</p><p>&nbsp;</p><p>让我们分别看一下这两个问题。</p><p>&nbsp;</p><p></p><h3>构建 Docker 镜像</h3><p></p><p>&nbsp;</p><p>有一些需要注意的关于构建 Docker 镜像的事情：</p><p>&nbsp;</p><p>Docker 镜像由堆栈中的多个层构成，每层由 Dockerfile 中的一部分命令构建。每个层有一个哈希标识。上传镜像到注册表时，只上传注册表中不存在的层（由哈希标识确定）。在 GitHub 构建机上使用 <a href=\"https://docs.docker.com/build/cache/backends/gha/\">GitHub Actions 缓存</a>\"重新构建镜像会将所有未受影响的层从缓存中拉取到构建机上。请注意，如果你的项目中有大量不会更改的依赖项，它们仍将在构建过程中从缓存中复制到构建机上。Docker 构建不是确定性的。如果使用完全相同的内容构建两个镜像，可能每次都会产生不同的哈希值。（虽然与本文直接相关性不强，但我们想观察这个意外的结果。作为一个极端情况，请考虑，一个刚构建的大层与已存在于注册表中的层完全相同，但仍然会作为一个新的层被上传。）</p><p>&nbsp;</p><p></p><h3>启动 Docker 容器</h3><p></p><p>&nbsp;</p><p>关于启动 Docker 容器，我们使用<a href=\"https://aws.amazon.com/fargate/\">亚马逊云科技 Fargate</a>\"，需要 45～90 秒的时间来提供和启动一个镜像。它不提供任何<a href=\"https://github.com/aws/containers-roadmap/issues/696\">镜像缓存</a>\"。启动一个新的容器会将所有层从注册表下载到已提供的容器中。</p><p>&nbsp;</p><p></p><h3>其他限制</h3><p></p><p>&nbsp;</p><p>在 Docker 镜像构建和启动后，我们运行用户的代码来提取元数据，并在 UI 中显示。这是不可避免的，并且可能需要几秒钟、30 秒或更多时间，具体取决于如何计算元数据（例如可能会连接数据库以读取模式）。此代码服务器保持活动状态并服务元数据请求，直到推送代码的新版本，然后启动一个新的容器。</p><p>&nbsp;</p><p>我们有一个关键要求是可重复性：我们需要能够多次重新部署完全相同的代码和环境。使用 Docker 镜像哈希作为代码和环境的标识符非常适合这个要求。</p><p>&nbsp;</p><p></p><h2>我们的备选方案</h2><p></p><p>&nbsp;</p><p>下面是我们探索和讨论过的一些备选方案：</p><p>&nbsp;</p><p>切换到 EC2 以加快容器启动速度。这会增加我们的运营负担，需要我们预先预配、监控和扩展集群。我们仍然会面临 Docker 构建速度慢的问题。切换到其他的 Docker 构建系统，例如 AWS CodeBuild。这需要更多的实现工作，并与 GitHub 进行更深入的集成。不确定收益是否值得。切换到 AWS Lambda，启动时间更快。Lambda 环境附带其自己的基础镜像，如果需要进行定制则更加困难。它还对执行时间施加了 15 分钟的限制，这将需要为运行时间更长的服务器实施复杂的解决方案。通过仅构建和上传更改的代码到相同的服务器，来重复使用长时间运行的代码服务器。这里的挑战是实现打包和运行时机制，以确保可靠和可重复的执行环境。我们研究了各种打包和分发 Python 环境的方法，包括 rsync、<a href=\"https://python-poetry.org/\">poetry</a>\"、<a href=\"https://nixos.org/\">nix</a>\"、<a href=\"https://shiv.readthedocs.io/en/latest/\">shiv</a>\" 和 <a href=\"https://pex.readthedocs.io/\">pex</a>\"。我们还考虑使用 EFS 卷以及这些工具来挂载 Python 环境。</p><p>&nbsp;</p><p>做出这个决定的关键因素是因为我们意识到，虽然 Docker 镜像是行业标准，但在只需要同步一个小改变时，传输数百兆字节的镜像似乎是不必要的。考虑 git——它只传输差异，但却产生了完整且一致的代码库。</p><p>&nbsp;</p><p>这让我们倾向于选择方案 4……如果我们能找到一个适合我们大部分工作工具的话。经过一些实验，我们发现 PEX 具有许多特性，非常适合我们的使用情况——在下一节中会详细介绍。</p><p>&nbsp;</p><p></p><h4>什么是 PEX？</h4><p></p><p>&nbsp;</p><p>PEX 是 Python 可执行文件的缩写，是一个将 Python 包打包成名为 pex 文件的工具。这些可执行文件包含 Python 包和一些引导代码。例如，我们可以将 dagster 包和其依赖项打包成单个文件，然后运行它：</p><p>&nbsp;</p><p><code lang=\"null\">% pex dagster --python=python3.8 -o dagster.pex\n% ./dagster.pex\nPython 3.8.16 (default, Dec  7 2022, 01:24:57)\n[Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n&gt;&gt;&gt; import dagster\n&gt;&gt;&gt;</code></p><p>&nbsp;</p><p>将整个环境存储在单个文件中非常方便，可以轻松地将其传输到 S3 中进行存储。PEX 提供了更多功能，不仅仅是“文件中的虚拟环境” - 这里是我们使用的其他功能：</p><p>&nbsp;</p><p>隔离性</p><p>&nbsp;</p><p>在运行时，pex 环境与其他全局包完全隔离。在环境中只有捆绑在 pex 文件中的包。我们将多个 pex 文件一起发送到同一台机器上，而不必担心环境隔离问题。</p><p>&nbsp;</p><p>确定性</p><p>&nbsp;</p><p>使用相同的输入包会生成完全相同的 pex 文件：</p><p><code lang=\"null\">$ pex dagster pandas -o out.pex | sha256sum\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855  -\n$ pex dagster pandas -o out.pex | sha256sum\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855  -</code></p><p>&nbsp;</p><p>这让我们可以使用内容寻址来识别这些 pex 文件，从而对实现可重复性更有信心。为了实现可重复性，除了使用 Docker 镜像哈希之外，我们还使用 pex 文件哈希。</p><p>&nbsp;</p><p>组合</p><p>&nbsp;</p><p>多个 pex 文件可以在运行时合并，有效地将多个环境合并为一个环境。</p><p>&nbsp;</p><p><code lang=\"null\">% pex pandas -o pandas.pex\n% pex dagster -o dagster.pex\n% PEX_PATH=pandas.pex ./dagster.pex\nPython 3.8.16 (default, Dec  7 2022, 01:24:57)\n[Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n&gt;&gt;&gt; import pandas\n&gt;&gt;&gt; import dagster\n&gt;&gt;&gt;</code></p><p>&nbsp;</p><p>我们使用这个功能将代码分成两个部分，在运行时合并起来：一个包含所有依赖项的 deps.pex 文件，和一个仅包含用户代码的 source.pex 文件。</p><p>&nbsp;</p><p>跨平台构建</p><p>&nbsp;</p><p>我们在 Serverless Cloud 中使用 Linux python：* -slim 衍生的<a href=\"https://hub.docker.com/_/python\">基础镜像</a>\"。只要有包的 <a href=\"https://packaging.python.org/en/latest/glossary/#term-Built-Distribution\">wheel</a>\"，pex 工具就可以在任何平台上为 Linux 构建 pex 文件。</p><p>&nbsp;</p><p></p><h4>快速部署</h4><p></p><p>&nbsp;</p><p>使用 pex 和 S3 存储 pex 文件，我们构建了一个系统，其中快速路径避免了构建和启动 Docker 镜像的开销。</p><p>&nbsp;</p><p>我们的系统工作方式如下：当你将代码提交到 GitHub 时，GitHub 操作根据你的依赖关系是否与上一次部署不同，执行全量构建或快速构建。我们跟踪在 setup.py 和 requirements.txt 中指定的依赖关系集。</p><p>&nbsp;</p><p>对于全量构建，我们将你的项目依赖项构建为 deps.pex 文件，将你的代码构建为 source.pex 文件。这两个文件都会上传到 Dagster Cloud。对于快速构建，我们只构建并上传 source.pex 文件。</p><p>&nbsp;</p><p>在 Dagster Cloud 中，我们可能会重复使用现有容器或为代码服务器提供新的容器。我们将 deps.pex 和 source.pex 文件下载到此代码服务器上，并在隔离环境中使用它们运行你的代码。我们从不跨用户共享容器，容器上的所有环境都属于同一用户。快速部署的最佳时间和最差时间如下所示：</p><p>&nbsp;</p><p>这里的要点是，在快速路径中——当我们进行快速构建并重用现有容器时——整个过程只需要大约 40 秒，而不是之前的 3 分钟多。我们称这个功能为快速部署，现在已经默认开启了所有新的无服务器注册。</p><p><img src=\"https://static001.infoq.cn/resource/image/3b/97/3bc742e157e97abb8f23f7b3fyy0yy97.png\" /></p><p></p><p></p><h4>权衡和问题</h4><p></p><p>&nbsp;</p><p>这种方式可以显著提高部署速度（4~5 倍），但也带来了一些权衡和其他需要调整的因素：</p><p>&nbsp;</p><p>虽然现在我们可以在一个代码服务器上运行多个环境，并且在代码方面是隔离的，但它们仍然共享相同的 RAM 和 CPU。如果我们将太多的环境放在一个容器中，而一个环境占用了太多的内存，就可能对在同一个容器中运行的其他环境产生不利影响。Docker 可以在任何操作系统上为 Linux 构建 Python 包，因为在构建过程中目标 Linux 操作系统和 Python 解释器都可用。pex 仅为提供 <a href=\"https://github.com/pypa/manylinux\">wheel</a>\" 的包构建 Linux 的 pex 文件。如果出现问题，我们在构建过程中使用 <a href=\"https://github.com/pypa/manylinux\">Docker 容器</a>\"来处理<a href=\"https://packaging.python.org/en/latest/glossary/#term-Source-Distribution-or-sdist\">源分发</a>\"。未来，这一步骤可以移动到单独的共享服务中。在构建 Docker 镜像时可以进行深度定制，例如，你可以指定自定义基础镜像而不是默认的 python:*-slim 镜像之一。为了实现功能的平等，我们实现了一种方法，允许用户指定他们自己的基础 Docker 镜像，我们将其用于快速部署。</p><p>&nbsp;</p><p></p><h2>GitHub 工作流和 PEX</h2><p></p><p>&nbsp;</p><p>你可能已经注意到，在最初的图表中，Download Docker based action 的操作大约需要 10 秒钟。我们是如何完全消除这个步骤的呢？</p><p>&nbsp;</p><p>我们曾经将 GitHub action 代码打包成 Docker 镜像，并使用 <a href=\"https://docs.github.com/en/actions/creating-actions/creating-a-docker-container-action\">Docker container action</a>\"。现在，我们将我们的 action 代码打包为 pex 文件，将其检入我们的 action 存储库并直接在 GitHub runner 上运行。这消除了下载和启动 Docker action 镜像所花费的时间，同时仍允许我们打包所有依赖项。</p><p>&nbsp;</p><p>我们做出的另一个小优化是只使用一个 GitHub 工作流作业。在 GitHub 中，每次作业启动需要大约 10 秒钟来准备一个新的 runner。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>将部署时间从超过 3 分钟缩短到 40 秒是一个显著的加速，我们对这个结果非常满意，特别是在测试自己的服务时。使用 pex 使我们能够在 Docker 之上构建一个可重复、一致的环境，我们很高兴能够探索使用 pex-on-docker 组合的其他可能性。</p><p>&nbsp;</p><p>如果有兴趣，可以看 PEX 团另一篇关于使用 Docker 进行部署的<a href=\"https://blog.pantsbuild.org/optimizing-python-docker-deploys-using-pants/\">博客文章</a>\"，它描述了如何使用 pex 文件作为中间目标来加速 Docker 镜像构建。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://dagster.io/blog/fast-deploys-with-pex-and-docker</p>",
    "publish_time": "2023-03-29 12:08:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "高盛预警！疯狂的AIGC将抢走全球3亿人饭碗，Open AI CEO : AI 还可能毁灭人类",
    "url": "https://www.infoq.cn/article/CLRC6iPEfUfmHUBloASr",
    "summary": "<p></p><blockquote>但高盛同时也指出，AI 也可能创造出新的创新驱动型岗位。</blockquote><p></p><p></p><h2>高盛预测大语言模型可能威胁3亿工作岗位</h2><p></p><p></p><p>日前，全球投资银行高盛发布研究称，<a href=\"https://www.infoq.cn/article/FRcz5vjOvl3bM2d57opX\">ChatGPT</a>\"等<a href=\"https://www.infoq.cn/article/4puKvRiE5uviX5vl4JPY\">生成式人工智能</a>\"系统的最新突破，将给全球劳动力市场带来重大颠覆。高盛银行经济学家Joseph Briggs和Devesh Kodnani在文章中认为，全球就业市场仍将受到大语言模型的严重冲击，预计将有3亿个工作岗位被生成式AI取代。</p><p></p><p>其中，律师和行政人员将是最有可能被取代的岗位。相比之下，户外和体力工作则不易受到大语言模型影响，这部分岗位在美国占比为30%。</p><p></p><p>高盛的此份报告再度引发了人们对于人工智能技术潜力的辩论。</p><p></p><p>一方面，将大语言模型引入工作环境无疑能为生产力再注入一针强心剂，重振世界萎靡不振的经济增速。高盛预计，未来10年，生成式AI有望给全球国内生产总值(GDP)带来7万亿美元的巨量提振，使全球年均GDP提高7%。</p><p></p><p>科技行业已经意识到大语言模型的蓬勃能量，谷歌和携手微软的OpenAI正积极发布自家成果。大语言模型在模仿人类语言方面有着令人印象深刻的表现，甚至能够生成可运行的计算机代码，并借此迅速霸占了新闻媒体的头版头条。</p><p></p><p>另一方面，虽然当前大语言模型的实质仍然是对大量文本做统计分析，并不能真正理解自己在读什么、在说什么，但该技术仍给劳动力市场带来重大颠覆，甚至可能会创造出一个新的白领失业阶层，他们可能遭遇与上世纪80年代制造业蓝领工人类似的命运。</p><p></p><p>除了可能会抢走人类的饭碗，也有一种观点认为，AI可能毁灭人类。</p><p></p><p>OpenAI创始人兼首席执行官Sam Altman上周六(3月25日)在接受科技博客主Lex Fridman的采访时表态，他不会回避“人工智能可能杀死全人类”的说法。在Youtube平台公布的视频中，Fridman在访谈中提到了人工智能研究者、LessWrong研究所创始人Eliezer Yudkowsky的观点。Yudkowsky此前指出“人工智能可能杀死所有人”，并认为，当人工智能发展为超级智能时，几乎无法与人类站在统一战线上。</p><p></p><h4>AIGC将显著影响6大行业</h4><p></p><p></p><p>具体会有哪些行业受到AIGC技术影响？《AIGC：智能创作时代》作者杜雨日前在一场演讲中曾表示，ChatGPT这类AIGC技术对资讯、影视、电商、教育、金融、医疗等六大行业带来的影响最为显著。</p><p></p><p>资讯行业：这里面，ChatGPT更多是辅助媒体人的工作，虽然它可以直接生成内容，但缺少温度和人文关怀，而且无法判断报道角度，做不了价值判断。影视行业：在剧本创作环节，ChatGPT能够生成各种各样的故事线，而其他AIGC工具还可以生成动画，或者生成已经去世或者年老的演员，让他们再一次出现荧幕中。电商领域：AIGC可以生成视觉三维的模型，也可以让图片展示更高效。虚拟主播接入ChatGPT等AIGC模块后可以更好地和用户互动，最后可能会代替真人主播，实现更高的PMV。教育行业：比如写作辅导可能节省老师备课的时间；再如阅卷，ChatGPT未来也许能够去写评语，直接判断主观题。金融行业：ChatGPT以及底层通用模型广泛应用之后，智慧客服会变得更加智慧。医疗领域：公司可以借助ChatGPT的底层能力协助做心理治疗，此外ChatGPT还能辅助医疗科普。</p><p></p><h2>未来三分之二的工作岗位都将实现一定程度的AI自动化</h2><p></p><p></p><p>欲戴王冠，必承其重。</p><p></p><p>高盛在报告中表示，发挥生成式AI力量的前提，是各方积极开展相关投资。假设这波浪潮与1990年代的软件投资趋势类似，那么结合当前数据，到2030年，美国工作环境在大语言模型和相关工具上的支出可能将占全国GDP的1%，达到约2300亿美元。</p><p></p><p>高盛预测，在未来，美国和欧洲约三分之二的工作岗位都将实现一定程度的AI自动化。</p><p></p><p>数据显示，欧洲需要日常处理单调重复性任务的岗位比例接近50%，美国则在64%左右。大语言模型的普及意味着员工能够节约更多的时间和精力，用于处理更有意义的事务。</p><p></p><p>但研究同时指出，受生成式AI技术影响，未来，失业人员可能会在技术变革之下找到新的职业方向。正如经济学家David Autor曾在一篇论文中说的那样，当前60%的工作岗位在1940年时并不存在。这个观点放在未来同样成立。</p><p></p><p>较有希望的新兴职位可能包括警察或安保人员，他们可以受雇抓捕利用大语言模型从事恶意活动的罪犯。欧洲刑警组织本周曾警告，生成式AI技术的快速发展可能会帮助在线欺诈者和网络犯罪分子，因此从安全视角来看，未来将诞生一批对抗利用生成式AI技术犯罪的安全人员。</p><p></p><p>此外，为了帮助人类更好地与 AI 互动，“提示工程师”也是一个冉冉升起的新兴职业。</p><p></p><p><a href=\"https://www.infoq.cn/article/xfBFzvfHQmXylbcA0jXW\">“提示工程师”</a>\"们的职责就是指导 AI 系统生成符合需求的输出，并且能让 AI 工具发挥出最大效力：了解自身缺陷、增强自身优势，并制定复杂策略以将简单输入转化为独特的输出结果。由前 OpenAI 员工和 Claude 语言 AI 系统缔造者建立的 AI 初创企业 Anthropic此前曾在旧金山发布了一个“提示工程师与库管理员”岗位，薪酬高达 33 万 5 千美元（约 232 万人民币，要求申请人‘具备创造性的极客精神，乐于解决难题’）。</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://www.theregister.com/2023/03/28/investment_bank_forecasts_llms_could/\">https://www.theregister.com/2023/03/28/investment_bank_forecasts_llms_could/</a>\"</p><p></p><p><a href=\"https://news.caijingmobile.com/article/detail/486690\">https://news.caijingmobile.com/article/detail/486690</a>\"</p>",
    "publish_time": "2023-03-29 13:40:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱2022——云原生领域",
    "url": "https://www.infoq.cn/article/zdDoaDUkCGiLmWcPBYIz",
    "summary": "<h3>研究背景</h3>\n<p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。<br />\n所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。<br />\n此篇是《中国开源生态图谱系列研究》的第四篇，聚焦在云原生领域，通过统计目前的云原生开源项目，并进行分类，同时结合开源基金会、开源产业联盟等生态，完整构成中国云原生开源的生态图谱。<br />\n随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，分析和评价现有云原生开源项目，并从中选择优秀案例供广大开发者和开源社区研究。<br />\n<img alt=\"\" src=\"https://static001.infoq.cn/resource/image/4a/52/4ac3f1fa1207465f1b561a48cd7e2252.jpg\" /><br />\n截至目前，InfoQ研究中心已经发布了4个领域的开源生态图谱，并预计在未来推出涵盖操作系统、数据库、人工智能、云原生、前端和中间件等七大领域的中国开源生态图谱全景图，将涵盖500+中国开源项目，敬请期待。</p>\n<h3>目录</h3>\n<ul>\n<li>生态图谱解读</li>\n<li>生态图谱企业洞察</li>\n</ul>",
    "publish_time": "2023-03-29 14:08:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "马斯克、图灵奖得主等千名大佬联名，要求暂停训练比GPT-4更强大的AI",
    "url": "https://www.infoq.cn/article/eCk6z73VdRjvqXtWdDWw",
    "summary": "<p>3月22日，生命未来研究所（Future of Life）向全社会发布了一封《暂停大型人工智能研究》的公开信，呼吁所有人工智能实验室立即暂停比<a href=\"https://www.infoq.cn/article/HFSPasQ7SXZ9QzdFXhGO\">GPT-4</a>\"更强大的人工智能系统的训练，暂停时间至少为6个月。</p><p>&nbsp;</p><p>截止目前，马斯克、图灵奖得主Yoshua Bengio、苹果联合创始人Steve Wozniak、Stability AI 创始人Emad Mostaque、DeepMind高级研究科学家Zachary Kenton等上千名科技大佬和AI专家已经签署公开信。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/0c/0cd3c0bcbed56dc8b26b775c367c1c07.png\" /></p><p></p><p>不过，也有大佬公开表达了反对意见。图灵奖得主、Meta首席AI科学家Yann LeCun在Twitter上表示：“我不会签署这个公开信，不同意这封公开信的内容。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/eebd99727e8d7d4f3e11f9b416b23983.png\" /></p><p></p><p>自<a href=\"https://www.infoq.cn/article/YYqCPSdRmtkdSl2hhb9Y\">ChatGPT</a>\"、GPT-4发布以来，关于AI与安全性的讨论声不断，甚至引发了不少科技大佬的深切担忧。</p><p>&nbsp;</p><p>“人工智能教父”、英国计算机科学家Geoffrey Hinton接受CBS 新闻的长篇采访中提到，人工智能正处于“关键时刻”。目前，人们经常提到AGI提高当前模型能力的预兆，但无论业界多么欢呼它的到来，或者它真正需要多长时间，Hinton说，我们现在应该仔细考虑它的后果，这可能包括它试图毁灭人类的小问题。</p><p>&nbsp;</p><p>3 月 25 日，OpenAI 创始人兼首席执行官 Sam Altman 在接受科技博客主 Lex Fridman 的采访时表态，他不会回避“人工智能可能杀死全人类”的说法。</p><p>&nbsp;</p><p>在 Youtube 平台公布的视频中，Fridman 在访谈中提到了人工智能研究者、LessWrong 研究所创始人 Eliezer Yudkowsky 的观点。Yudkowsky 此前指出“人工智能可能杀死所有人”，并认为，当人工智能发展为超级智能时，几乎无法与人类站在统一战线上。</p><p>&nbsp;</p><p>生命未来研究所在公开信中表示，通过广泛研究和顶级AI实验室的调查认可，具备类人智能的AI系统很可能对社会和人类构成深远风险。正如广受推崇的阿西洛马AI原则中所述，高级AI可能代表着地球生命史上一场影响深远的变化，应给予相应的关注和资源进行规划和管理。</p><p>&nbsp;</p><p>生命未来研究所表示，遗憾的是，我们并没有看到这种级别的规划和管理。最近几个月来，AI实验室陷入了一场失控般的技术竞赛，全力开发和部署一颗颗愈发强大的“数字大脑”，但就连创造者自己都无法理解、预测或可靠地加以控制。</p><p></p><h4>附公开信原文：</h4><p></p><p>&nbsp;</p><p>关于叫停巨型AI实验的一封公开信</p><p>&nbsp;</p><p>我们呼吁所有AI实验室，立即暂停对体量超过GPT-4的AI系统的训练，且暂停期至少6个月。</p><p>&nbsp;</p><p>通过广泛研究和顶级AI实验室的调查认可，具备类人智能的AI系统很可能对社会和人类构成深远风险。正如广受推崇的阿西洛马AI原则中所述，高级AI可能代表着地球生命史上一场影响深远的变化，应给予相应的关注和资源进行规划和管理。但遗憾的是，我们并没有看到这种级别的规划和管理。最近几个月来，AI实验室陷入了一场失控般的技术竞赛，全力开发和部署一颗颗愈发强大的“数字大脑”，但就连创造者自己都无法理解、预测或可靠地加以控制。</p><p>&nbsp;</p><p>当代AI系统在常规任务上已经具备与人类相仿的能力，因此我们必须扪心自问：我们是否应该让机器用宣传和谎言填满信息渠道？我们真的应该推动一切工作自动化，包括目前已经令人满意的岗位吗？我们是否应当开发出最终可能超越自身、超越特种甚至取代我们的非人类思维？我们应该承担这份失去对文明掌控权的风险吗？很明显，这些问题的答案，不该由少数未经选举产生的技术领导者决定。只有当我们确信具有积极影响且风险可控时，才应当实际开发强AI系统。对这个问题的判断需要坚毅的决心，也要充分考虑到系统潜在影响等因素。OpenAI近期发布的AI声明指出，“必须承认，在开始训练未来系统之前，可能应该认真考虑开展独立审查。对于最前沿的探索，也应该对新系统的算力资源增长率做出限制。”我们完全认同这一判断，并坚信当下就是最恰当的时机。</p><p>&nbsp;</p><p>因此，我们呼吁所有AI实验室，立即暂停对体量超过GPT-4的AI系统的训练，且暂停期至少6个月。这种暂停应当对外公开且可加验证，涵盖所有关键参与者。如果未能迅速实施暂停，政府应介入并强制要求其暂停。</p><p>&nbsp;</p><p>各AI实验室和独立专家则应把握这次暂停，共同开发和实施一套用于高级AI设计和开发的共享安全协议，并由外部独立专家进行严格审计与监督。这些协议应确保依其构建的系统具备无可置疑的安全性。我们并不是要全面叫停AI开发，只是想从目前这场危险的竞赛中退后一步，避免最终衍生出体量更大、更加不可预测的黑盒模型。</p><p>&nbsp;</p><p>AI研究和开发工作应当集中注意力，努力让目前最强大、最先进的系统变得更准确、更安全、更可解释、更透明、更稳健、更一致，也更加忠诚且值得信赖。</p><p>&nbsp;</p><p>与此同时，AI开发商必须与立法机构合作，加快开发出强大的AI治理体制。其中至少应当包括：专门负责AI事务的新型监管机构；监督和跟踪高性能AI系统及大量算力的机构；出处与水印系统，帮助区分真实与合成信息，同时跟踪模型泄露；强大的审计和认证生态系统；对AI造成的伤害进行责任回溯；为AI安全研究提供强大的公共资金支持；指派资源充足的机构应对AI将会在经济和政治层面造成的巨大破坏。</p><p>&nbsp;</p><p>人类应当享受到AI造就的繁荣未来。在成功创建起强大的AI系统之后，我们才能平稳度过“AI之夏”，静候金秋带来的累累硕果。但这一切的前提，是暂时叫停一切可能对社会造成灾难性影响的技术，这也是我们的当务之急。为社会提供过渡期、确保系统设计造福每个人，我们才能信心满满地迎接“AI之秋”。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">https://futureoflife.org/open-letter/pause-giant-ai-experiments/</a>\"</p>",
    "publish_time": "2023-03-29 14:31:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度袁佛玉：文心一言将改变云计算市场游戏规则",
    "url": "https://www.infoq.cn/article/iOKBmLooIIk26qXaLVcI",
    "summary": "<p>3月29日，<a href=\"https://www.infoq.cn/topic/baidu\">百度集团</a>\"副总裁袁佛玉在出席一次公开活动中表示，百度新一代大语言模型、生成式AI产品<a href=\"https://yiyan.baidu.com/welcome\">文心一言</a>\"，将根本性地改变云计算市场游戏规则。因为大模型和生成式AI的突破，全球性“AI再造”已经拉开序幕，跟不上的企业将彻底失去未来的竞争力。</p><p></p><p>袁佛玉在演讲中称，“文心一言是百度技术积累的阶段性成果。我们在人工智能领域已投入13年，研发投入高达千亿。如果没有这些投入，根本就不可能出现文心一言这个大模型。”百度一直身处人工智能浪潮之中，对行业的变革有着敏锐的嗅觉。2022年9月，百度CEO李彦宏就指出，人工智能在技术层面和商业层面都发生了方向性改变。技术层面的方向性改变是，AI从理解内容迈向生成内容，也就是今天大热的生成式<a href=\"https://www.infoq.cn/topic/AI\">AI</a>\"。</p><p></p><p>“我们对当下的判断是，因为大模型和生成式AI的突破，全球性AI再造已经拉开序幕。跟不上的企业将彻底失去未来的竞争力。”袁佛玉表示，文心一言将与百度搜索、小度、Apollo自动驾驶等业务融合。“但更大的故事是云计算。文心一言将根本性地改变云计算市场的游戏规则，百度智能云有信心成为引领者”。</p><p></p><p>人工智能科技革命，尤其是生成式AI的兴起，将给生产经营效率与用户体验效果带来“双效”提升，将彻底改变所有行业，加速实现社会“智能化跃迁”。首先是生产经营效率将极大提高。生成式AI已经可以写合同、写PPT、写投资研报等。接下来，多模融合的生成式AI，会重塑生产全流程。其次，用户体验效果将显著提升。基于生成式AI的数字人和个性化助理，将协助人类解决健康、理财、法律等问题，让人机交互界面更自然、友好。</p><p></p><p>基于文心一言的能力，百度智能云将帮助千行百业实现AI再造。比如企业办公场景中的PPT制作，AI做出一份格式精美、内容丰富的PPT，可能只需要几分钟。在电商服务领域，直播间已经成为品牌营销的标配。基于文心一言大模型的能力，百度智能云可以帮助电商客户自动创造直播脚本，完成脚本到数字人开播等全流程。这对中小企业来说是非常大的好消息。</p><p></p><p>技术的进步，必然伴随着技术生产范式的升级。今天的IT技术栈，已经变成了适配人工智能技术发展的四层架构：芯片、框架、模型、应用。百度是全球唯一在这四层架构的各个层面，都拥有领先自研产品的公司。袁佛玉表示，“四层架构都有的优势是，每一层通过反馈不断相互加强，端到端地优化，显著提升效率、降低成本。”</p><p></p><p>百度智能云的核心理念也正是这四层架构，这使得百度智能云成为目前国内唯一训练出生成式大语言模型的云，这个大语言模型就是文心一言。与此同时，文心一言的服务也会通过百度智能云对外提供。</p><p>袁佛玉认为，以文心一言为代表的生成式AI大模型将给云计算行业带来根本性改变。云计算的主流商业模式将从IaaS（基础设施即服务）变为MaaS（模型即服务）。过去，企业在选择云厂商时更多关注算力、存储等基础云服务。现在，他们更关注框架、模型的质量，以及芯片、框架、模型、应用这四层之间协同是否高效。</p><p></p><p>未来，开发者只需基于大模型开发业务，不用关心其他技术细节。“基于大模型开发，速度更快，成本更低。这会推动整个云计算市场进入新型云计算阶段。传统云和智能云将完全划清界限。不是智能化的云不是好云。百度智能云也会因为文心一言，有机会成为新型云计算供应商的引领者。”</p><p></p><p>中国拥有全球最先进的产业链，会产生大量真实的行业需求和用户反馈，进而推动生成式AI持续进化。随着生成式AI在金融、教育、医疗、交通、能源等重点领域的不断深入，模型即服务（MaaS）将催生万亿级别的新市场，并为人们的生活带来更多幸福感。</p><p>&nbsp;</p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/Mz92EUBpvOwykbNG2iMz\">刚刚，百度文心一言揭开面纱！能续写《三体》，算“鸡兔同笼”题，将开启首批内测</a>\"</p><p><a href=\"https://www.infoq.cn/article/F3wfMpdNNJEg3ho4RYgI\">InfoQ 宣布接入百度文心一言能力，打造内容生态人工智能全系产品 / 服务</a>\"</p><p></p>",
    "publish_time": "2023-03-29 16:11:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从“大数据”到“小数据”，“隐语”开源SCQL助力不同规模数据安全分析",
    "url": "https://www.infoq.cn/article/p4VXV9pYfPXjF9sLffld",
    "summary": "<p>在数据要素战略持续升级的背景下，发展以<a href=\"https://xie.infoq.cn/article/0d80e9a3bf05d028f4d82ce69\">隐私计算</a>\"为代表的密态技术，是解决大规模数据安全可信流转的技术手段。由于隐私计算最早诞生于大规模数据场景，技术架构和资源投入对于小规模数据体量的机构来说相对复杂和奢侈，动辄百亿千亿数据规模的隐私保护机器学习，对于中小机构来说并不适用。如何简单快捷高性价比地使用隐私计算技术，提高易用性，一直是隐私计算行业待突破的技术难题之一。</p><p></p><p>3月29日，首届“隐语开源社区开放日”活动上，隐私计算开源框架“<a href=\"https://www.infoq.cn/article/q5jpzKT7FQPXGihouEQR\">隐语</a>\"”宣布产品升级，并开源了SCQL功能。SCQL提供的简单易上手的BI分析可帮助中小机构快速解决急迫的长尾数据安全分析需求，让隐语在工业界首次实现了隐私数据从Al分析到BI分析，是其走向易用的重要一步。</p><p>&nbsp;</p><p>围绕共建更好用的隐私计算技术和更健康的生态，“隐语开源社区开放日”活动现场，“隐语”开源框架负责人王磊表示，“隐语”框架最新升级并开源的功能SCQL，将业界常用的隐私计算技术多方安全计算(MPC）作为底座，以当前主流的SQL作为分析语言的多方安全数据分析系统，可让用户简单易上手地实现多方数据密态分析任务，能有效满足中小机构低成本使用隐私计算分析的需求。</p><p>&nbsp;</p><p>“由于中小机构的数字化水平处于初期，数据量属于小样本阶段，Al机器学习的方式不必要也不具性价比，而以SQL语言为主要手段的<a href=\"https://s.geekbang.org/search/c=0/k=BI%E5%88%86%E6%9E%90/t=\">BI分析</a>\"是最具可行性的方案”，王磊表示。对于大规模数据场景来说，BI数据分析也是不可或缺的重要分析手段。“从大数据到小数据，SCQL都能满足密态数据安全分析的需求”，王磊强调。</p><p>&nbsp;</p><p>据悉，这一技术在蚂蚁保理赔科技平台和“隐语〞框架打造的智能理赔系统中得到了充分验证。“隐语”SCQL帮助保险公司与不同的外部医疗数据源，在原始数据不离开本地、数据价值被保护的前提下进行联合分析，可帮助大幅提升有效阳性线索发现率、降低错赔风险，控制理赔运营成本。</p><p>&nbsp;</p><p>据悉，自从2022年7月正式开源以来，“隐语〞框架几乎覆盖了几乎所有主流隐私计算技术路线。当前，隐私计算在全球范围内都处于攻坚阶段，行业正在通过开源开放的方式，降低技术门槛，加速技术普及。本次“隐语〞开源框架升级将进一步提升技术可用性和易用性。王磊表示，隐语开源社区也将发布SCQL的共建任务，希望广大开发者参与进来，共同持续建设完善这一功能，共建隐私计算技术生态。</p>",
    "publish_time": "2023-03-29 16:20:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]