[
  {
    "title": "微软Bing Chat全面开放，所有人可用！官宣多项重大升级，日活用户超过1亿",
    "url": "https://www.infoq.cn/article/ZmDbgaSV7PmYEG3MwQAx",
    "summary": "<p>当地时间 5 月 4 日，根据 The Verge 报道，微软宣布公司旗下基于 ChatGPT-4 的 Bing Chat 功能已进入开放预览（Open Preview）模式，也就是面向所有用户开放。</p><p></p><p>在此之前，如果用户想测试微软的 Bing GPT-4 聊天机器人，则必须加入等候名单。现在大家只需要前往 Bing 或 Edge 并使用 Microsoft 帐户登录即可。</p><p></p><h2>微软 Bing Chat 正式开放</h2><p></p><p></p><p>微软首席营销官优素福·迈赫迪（Yusuf Mehdi）表示，自推出以来，基于 GPT-4 的 Bing AI 已经提供了 5 亿次聊天，并且有 2 亿人使用该平台生成 AI 图像，日活已超过 1 亿。</p><p></p><p>Mehdi 还称，大约 70% 尝试新 Bing 聊天功能的用户， 都会用来进行搜索相关的任务。聊天这种人类最自然的交互，正实实在在改变着搜索方式。</p><p></p><p>微软补充说，将继续监测 Bing 聊天的使用情况，以确保其 AI 工具的合理使用：“我们与 OpenAI 的合作伙伴一起，根据我们在预览中学习和看到的情况，继续实施防御有害内容的保障措施。”</p><p></p><p>其实今年 3 月份的时候，就有用户发现微软取消了候补名单功能，意味着只需注册一个新的 Bing 账户就能立即解锁 Bing Chat。不过当时微软没有做出正面回应，本次则是微软第一次官方宣布全面对外开放 Bing Chat。</p><p></p><p>除了正式宣布全面对外开放以外，Bing Chat 还上线了多个新功能。</p><p></p><h2>BingChat 迎来多项重大升级</h2><p></p><p></p><p>微软表示，新的 BingChat 在餐厅预订、图像结果等方面变得更加智能。</p><p></p><p>比如，新版 Bing Chat 中增加了图像和视频答案、餐厅预订、聊天记录以及一些更智能的 Microsoft Edge 集成。</p><p></p><p>首先，在插件功能上，新版 Bing Chat 将允许第三方参与 Bing Chat AI 的结果。微软正在与 OpenTable 合作，将餐厅预订纳入到 AI 响应中。</p><p></p><p>早在今年 3 月份，OpenAI 就已经为 ChatGPT 加入了插件功能，加入了插件功能后的 ChatGPT 将能够与其它第三方应用程序连通起来，直接通过与 ChatGPT 的对话用户就可以实现点餐、购物等此前需要跳转多个站点才能完成的操作。</p><p></p><p>增加了插件功能后，如果 Bing Chat 搜索结果推荐了一家餐厅，它可以智能地找到适合的预订时间，并帮助用户在聊天界面中预订。</p><p></p><p>其次，现在 Bing Chat 除了能做普通的文字交流问答、编程学习外，还能帮助用户“生成图片” 甚至是视频等，而且以文生图的功能还支持了中文在内的多种语言。</p><p></p><p>此外，微软还在 Bing Chat 中添加了一项备受期待的功能：历史记录。这个新的历史聊天记录功能可以让用户跨设备获取聊天机器人对话，甚至可以将 Bing Chat 用作研究工具。微软还计划在 Bing Chat 中添加导出和共享功能，以便用户可以在 Twitter 上共享对话内容，甚至可以将其导入 Word 文档。</p><p>聊天记录真正有趣的地方是在 Microsoft Edge 中。如果用户在 Edge 中打开 Bing Chat 答案的链接，它会自动将该聊天移至侧边栏，以便用户可以在浏览网站时继续提问。微软还在尝试通过将以前的聊天记录中的上下文引入新的对话来个性化这些聊天会话。</p><p></p><h2>负责任的 AI 需要接受公众的检验</h2><p></p><p></p><p>微软和其他竞争对手竞相押注 AI 聊天机器人掀起了全球 AI 热潮，但 AI 带来的安全问题同样不容忽视。</p><p>此次官宣 Bing Chat 公开后，微软并没有花太多时间谈论快速采用 AI 的潜在陷阱，但 Mehdi 确实解释了为什么微软基本上一直在与公众一起测试 Bing AI。</p><p></p><p></p><blockquote>“我们相信，以正确和负责任的方式将这项技术推向市场就是要公开测试，就是要让人们使用它并获得反馈”。</blockquote><p></p><p></p><p>“我知道有很多人在谈论，‘嘿，你的速度是多少，太快了，还是太慢了？’ 在我们看来……这前提必须以负责任的 AI 原则和方法为基础，我们微软在负责任 AI 这件事上已经做了很多年，现在我们必须走出去，必须测试它并获得反馈，”Mehdi 说道。</p><p></p><p>曾参与开发 Bing AI 聊天界面外观设计的微软设计副总裁 Liz Danzico 表示，体验 GPT-4 像是经历了另一个技术时代。</p><p></p><p>Danzico 表示他了解人们对 AI 的担忧，人们要确保不能让 AI 走得太远或越界。</p><p></p><p>在 Bing AI 聊天的开发过程中，Danzico 的团队与微软的负责任的 AI 团队密切合作，以确保负责任的 AI 原则能贯穿于整个产品中。她补充说，微软明白“如今的 Bing Chat 还一个不完善的系统，我们仍在学习中。”</p><p></p><p>参考链接：</p><p>https://www.theverge.com/2023/5/4/23710022/microsoft-bing-chatbot-ai-image-video-chat-history-features</p><p></p><p>https://www.techradar.com/news/bing-ai-chat-is-now-waitlist-free-and-about-to-get-even-more-powerful</p>",
    "publish_time": "2023-05-06 09:07:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "CentOS 迁移首选龙蜥，更快更稳更安全！",
    "url": "https://www.infoq.cn/article/fr61MHJEQ82Mri3sELC1",
    "summary": "<p>CentOS 全面停服在即，为妥善应对 CentOS 停服带来的数据泄露和安全风险，龙蜥社区秉承 CentOS 迁移“三不”原则，不盲目，不折腾，不短择，为广大用户提供更方便更安全的操作系统迁移服务。CentOS 迁移首选龙蜥，更快更稳更安全！</p>",
    "publish_time": "2023-05-06 14:12:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "第四范式开源强化学习研究通用框架，支持单智能体、多智能体训练，还可训练自然语言任务！训练速度提升17%",
    "url": "https://www.infoq.cn/article/AKQkaQil4TApHS88uVEL",
    "summary": "<p>OpenRL 是由第四范式强化学习团队开发的基于 PyTorch 的强化学习研究框架，支持单智能体、多智能体、自然语言等多种任务的训练。OpenRL 基于 PyTorch 进行开发，目标是为强化学习研究社区提供一个简单易用、灵活高效、可持续扩展的平台。目前，OpenRL 支持的特性包括：</p><p></p><p>●简单易用且支持单智能体、多智能体训练的通用接口</p><p>●支持自然语言任务（如对话任务）的强化学习训练</p><p>●支持从 Hugging Face 上导入模型和数据</p><p>●支持 LSTM，GRU，Transformer 等模型</p><p>●支持多种训练加速，例如：自动混合精度训练，半精度策略网络收集数据等</p><p>●支持用户自定义训练模型、奖励模型、训练数据以及环境</p><p>●支持 gymnasium 环境</p><p>●支持字典观测空间</p><p>●支持 wandb，tensorboardX 等主流训练可视化工具</p><p>●支持环境的串行和并行训练，同时保证两种模式下的训练效果一致</p><p>●中英文文档</p><p>●提供单元测试和代码覆盖测试</p><p>●符合 Black Code Style 和类型检查</p><p></p><p>目前，OpenRL 已经在 GitHub 开源：https://github.com/OpenRL-Lab/openrl</p><p></p><p></p><h2>OpenRL 初体验</h2><p></p><p></p><p>OpenRL 目前可以通过 pip 进行安装：</p><p><code lang=\"nginx\">pip install openrl</code></p><p></p><p>也可以通过 conda 安装：</p><p></p><p><code lang=\"nginx\">conda install -c openrl openrl</code></p><p></p><p>OpenRL 为强化学习入门用户提供了简单易用的接口， 下面是一个使用 PPO 算法训练 CartPole 环境的例子：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nenv = make(\"CartPole-v1\", env_num=9) # 创建环境，并设置环境并行数为9\nnet = Net(env) # 创建神经网络\nagent = Agent(net) # 初始化智能体\nagent.train(total_time_steps=20000) # 开始训练，并设置环境运行总步数为20000</code></p><p></p><p>使用 OpenRL 训练智能体只需要简单的四步：创建环境 =&gt; 初始化模型 =&gt; 初始化智能体 =&gt; 开始训练！</p><p></p><p>在普通笔记本电脑上执行以上代码，只需要几秒钟，便可以完成该智能体的训练：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/05/05e1ddaf02fa30d1e3019f9637cb9454.gif\" /></p><p></p><p>此外，对于多智能体、自然语言等任务的训练，OpenRL 也提供了同样简单易用的接口。例如，对于多智能体任务中的 MPE 环境，OpenRL 也只需要调用几行代码便可以完成训练：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\ndef train():\n    # 创建 MPE 环境，使用异步环境，即每个智能体独立运行\n    env = make(\n        \"simple_spread\",\n        env_num=100,\n        asynchronous=True,\n    )\n    # 创建 神经网络，使用GPU进行训练\n    net = Net(env, device=\"cuda\")\n    agent = Agent(net) # 初始化训练器\n    # 开始训练\n    agent.train(total_time_steps=5000000)\n    # 保存训练完成的智能体\n    agent.save(\"./ppo_agent/\")\nif __name__ == \"__main__\":\n    train()</code></p><p></p><p>下图展示了通过 OpenRL 训练前后智能体的表现：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/32/32b5041e84110f7be683520425ade82d.gif\" /></p><p></p><p></p><h2>加载配置文件</h2><p></p><p></p><p>此外，OpenRL 还同时支持从命令行和配置文件对训练参数进行修改。比如，用户可以通过执行 python train_ppo.py --lr 5e-4 来快速修改训练时候的学习率。</p><p></p><p>当配置参数非常多的时候，OpenRL 还支持用户编写自己的配置文件来修改训练参数。例如，用户可以自行创建以下配置文件 (mpe_ppo.yaml)，并修改其中的参数：</p><p></p><p><code lang=\"bash\"># mpe_ppo.yaml\nseed: 0 # 设置seed，保证每次实验结果一致\nlr: 7e-4 # 设置学习率\nepisode_length: 25 # 设置每个episode的长度\nuse_recurrent_policy: true # 设置是否使用RNN\nuse_joint_action_loss: true # 设置是否使用JRPO算法\nuse_valuenorm: true # 设置是否使用value normalization</code></p><p></p><p>最后，用户只需要在执行程序的时候指定该配置文件即可：</p><p></p><p><code lang=\"css\">python train_ppo.py --config mpe_ppo.yaml</code></p><p></p><p></p><h2>训练与测试可视化</h2><p></p><p></p><p>此外，通过 OpenRL，用户还可以方便地使用 wandb 来可视化训练过程：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ce/ce48f0a69a9fac0a4bfe103eaedf35c6.png\" /></p><p></p><p>OpenRL 还提供了各种环境可视化的接口，方便用户对并行环境进行可视化。用户可以在创建并行环境的时候设置环境的渲染模式为\"group_human\"，便可以同时对多个并行环境进行可视化：</p><p></p><p><code lang=\"ini\">env = make(\"simple_spread\", env_num=9, render_mode=\"group_human\")</code></p><p></p><p>此外，用户还可以通过引入 GIFWrapper 来把环境运行过程保存为 gif 动画：</p><p></p><p><code lang=\"javascript\">from openrl.envs.wrappers import GIFWrapper\nenv = GIFWrapper(env, \"test_simple_spread.gif\")</code></p><p></p><p></p><h2>智能体的保存和加载</h2><p></p><p></p><p>OpenRL 提供 agent.save() 和 agent.load() 接口来保存和加载训练好的智能体，并通过 agent.act() 接口来获取测试时的智能体动作：</p><p></p><p><code lang=\"python\"># test_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nfrom openrl.envs.wrappers import GIFWrapper # 用于生成gif\ndef test():\n    # 创建 MPE 环境\n    env = make( \"simple_spread\", env_num=4)\n    # 使用GIFWrapper，用于生成gif\n    env = GIFWrapper(env, \"test_simple_spread.gif\")\n    agent = Agent(Net(env)) # 创建 智能体\n    # 保存智能体\n    agent.save(\"./ppo_agent/\")    \n    # 加载智能体\n    agent.load('./ppo_agent/')\n    # 开始测试\n    obs, _ = env.reset()\n    while True:\n        # 智能体根据 observation 预测下一个动作\n        action, _ = agent.act(obs)\n        obs, r, done, info = env.step(action)\n        if done.any():\n            break\n    env.close()\nif __name__ == \"__main__\":\n    test()</code></p><p></p><p>执行该测试代码，便可以在同级目录下找到保存好的环境运行动画文件 (test_simple_spread.gif)：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/59/5948ac804c46b8d10d3d0460e718e4b2.gif\" /></p><p></p><p></p><h2>训练自然语言对话任务</h2><p></p><p></p><p>最近的研究表明，强化学习也可以用于训练语言模型， 并且能显著提升模型的性能。目前，OpenRL 已经支持自然语言对话任务的强化学习训练。OpenRL 通过模块化设计，支持用户 加载自己的数据集 ， 自定义训练模型， 自定义奖励模型， 自定义 wandb 信息输出 以及 一键开启混合精度训练 等。</p><p></p><p>对于对话任务训练，OpenRL 提供了同样简单易用的训练接口：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nfrom openrl.configs.config import create_config_parser\ndef train():\n    # 添加读取配置文件的代码\n    cfg_parser = create_config_parser()\n    cfg = cfg_parser.parse_args()\n    # 创建 NLP 环境\n    env = make(\"daily_dialog\",env_num=2,asynchronous=True,cfg=cfg,)\n    net = Net(env, cfg=cfg, device=\"cuda\")\n    agent = Agent(net)\n    agent.train(total_time_steps=5000000)\nif __name__ == \"__main__\":\n    train()</code></p><p></p><p>可以看出，OpenRL 训练对话任务和其他强化学习任务一样，都是通过创建交互环境的方式进行训练。</p><p></p><p></p><h2>加载自定义数据集</h2><p></p><p></p><p>训练对话任务，需要对话数据集。这里我们可以使用 Hugging Face 上的公开数据集（用户可以替换成自己的数据集）。加载数据集，只需要在配置文件中传入数据集的名称或者路径即可：</p><p></p><p><code lang=\"bash\"># nlp_ppo.yaml\ndata_path: daily_dialog # 数据集路径\nenv: # 环境所用到的参数\n    args: {'tokenizer_path': 'gpt2'} # 读取tokenizer的路径\nseed: 0 # 设置seed，保证每次实验结果一致\nlr: 1e-6 # 设置policy模型的学习率\ncritic_lr: 1e-6 # 设置critic模型的学习率\nepisode_length: 20 # 设置每个episode的长度\nuse_recurrent_policy: true</code></p><p></p><p>上述配置文件中的 data_path 可以设置为 Hugging Face 数据集名称 或者 本地数据集路径。此外，环境参数中的 tokenizer_path 用于指定加载文字编码器的 Hugging Face 名称 或者 本地路径。</p><p></p><p></p><h2>自定义训练模型</h2><p></p><p></p><p>在 OpenRL 中，我们可以使用 Hugging Face 上的模型来进行训练。为了加载 Hugging Face 上的模型，我们首先需要在配置文件 nlp_ppo.yaml 中添加以下内容：</p><p></p><p><code lang=\"cs\"># nlp_ppo.yaml\n# 预训练模型路径\nmodel_path: rajkumarrrk/gpt2-fine-tuned-on-daily-dialog \nuse_share_model: true # 策略网络和价值网络是否共享模型\nppo_epoch: 5 # ppo训练迭代次数\n\ndata_path: daily_dialog # 数据集名称或者路径\nenv: # 环境所用到的参数\n    args: {'tokenizer_path': 'gpt2'} # 读取tokenizer的路径\nlr: 1e-6 # 设置policy模型的学习率\ncritic_lr: 1e-6 # 设置critic模型的学习率\nepisode_length: 128 # 设置每个episode的长度\nnum_mini_batch: 20</code></p><p></p><p>然后在 train_ppo.py 中添加以下代码：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nfrom openrl.configs.config import create_config_parser\nfrom openrl.modules.networks.policy_value_network_gpt import (\n    PolicyValueNetworkGPT as PolicyValueNetwork,\n)\ndef train():\n    # 添加读取配置文件的代码\n    cfg_parser = create_config_parser()\n    cfg = cfg_parser.parse_args()\n    # 创建 NLP 环境\n    env = make(\"daily_dialog\",env_num=2,asynchronous=True,cfg=cfg,)\n    # 创建自定义神经网络\n    model_dict = {\"model\": PolicyValueNetwork}\n    net = Net(env, cfg=cfg, model_dict=model_dict)\n    # 创建训练智能体\n    agent = Agent(net)\n    agent.train(total_time_steps=5000000)\nif __name__ == \"__main__\":\n    train()</code></p><p></p><p>通过以上简单几行的修改，用户便可以使用 Hugging Face 上的预训练模型进行训练。如果用户希望分别自定义策略网络和价值网络，可以写好 CustomPolicyNetwork 以及 CustomValueNetwork 后通过以下方式从外部传入训练网络：</p><p></p><p><code lang=\"makefile\">model_dict = {\n    \"policy\": CustomPolicyNetwork,\n    \"critic\": CustomValueNetwork,\n}\nnet = Net(env, model_dict=model_dict)</code></p><p></p><p></p><h2>自定义奖励模型</h2><p></p><p></p><p>通常，自然语言任务的数据集中并不包含奖励信息。因此，如果需要使用强化学习来训练自然语言任务，就需要使用额外的奖励模型来生成奖励。在该对话任务中，我们可以使用一个复合的奖励模型，它包含以下三个部分：</p><p></p><p>●意图奖励：即当智能体生成的语句和期望的意图接近时，智能体便可以获得更高的奖励。</p><p></p><p>●METEOR 指标奖励：METEOR 是一个用于评估文本生成质量的指标，它可以用来衡量生成的语句和期望的语句的相似程度。我们把这个指标作为奖励反馈给智能体，以达到优化生成的语句的效果。</p><p></p><p>●KL 散度奖励：该奖励用来限制智能体生成的文本偏离预训练模型的程度，防止出现 reward hacking 的问题。</p><p></p><p>我们最终的奖励为以上三个奖励的加权和，其中 KL 散度奖励 的系数是随着 KL 散度的大小动态变化的。想在 OpenRL 中使用该奖励模型，用户无需修改训练代码，只需要在 nlp_ppo.yaml 文件中添加 reward_class 参数即可：</p><p></p><p><code lang=\"cs\"># nlp_ppo.yaml\nreward_class:\n    id: NLPReward # 奖励模型名称\n    args: {\n        # 用于意图判断的模型的名称或路径\n        \"intent_model\": rajkumarrrk/roberta-daily-dialog-intent-classifier,\n        # 用于计算KL散度的预训练模型的名称或路径\n        \"ref_model\": roberta-base, # 用于意图判断的tokenizer的名称或路径\n    }</code></p><p></p><p>OpenRL 支持用户使用自定义的奖励模型。首先，用户需要编写自定义奖励模型 (需要继承 BaseReward 类)。接着，用户需要注册自定义的奖励模型，即在 train_ppo.py 添加以下代码：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.rewards.nlp_reward import CustomReward\nfrom openrl.rewards import RewardFactory\nRewardFactory.register(\"CustomReward\", CustomReward)</code></p><p></p><p>最后，用户只需要在配置文件中填写自定义的奖励模型即可：</p><p></p><p><code lang=\"makefile\">reward_class:\n    id: \"CustomReward\" # 自定义奖励模型名称\n    args: {} # 用户自定义奖励函数可能用到的参数</code></p><p></p><p></p><h2>自定义训练过程信息输出</h2><p></p><p></p><p>OpenRL 还支持用户自定义 wandb 和 tensorboard 的输出内容。例如，在该任务的训练过程中，我们还需要输出各种类型奖励的信息和 KL 散度系数的信息， 用户可以在 nlp_ppo.yaml 文件中加入 vec_info_class 参数来实现:</p><p></p><p><code lang=\"makefile\"># nlp_ppo.yaml\nvec_info_class:\n    id: \"NLPVecInfo\" # 调用NLPVecInfo类以打印NLP任务中奖励函数的信息\n#设置wandb信息\nwandb_entity: openrl # 这里用于指定wandb团队名称，请把openrl替换为你自己的团队名称\nexperiment_name: train_nlp # 这里用于指定实验名称\nrun_dir: ./run_results/ # 这里用于指定实验数据保存的路径\nlog_interval: 1 # 这里用于指定每隔多少个episode上传一次wandb数据\n# 自行填写其他参数...</code></p><p></p><p>修改完配置文件后，在 train_ppo.py 文件中启用 wandb:</p><p></p><p><code lang=\"php\"># train_ppo.py\nagent.train(total_time_steps=100000, use_wandb=True)</code></p><p></p><p>然后执行 python train_ppo.py –config nlp_ppo.yaml，过一会儿，便可以在 wandb 中看到如下的输出:</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0e/0e635e370fd9fe8e7eb0e0b98b4ef05b.png\" /></p><p></p><p>从上图可以看到，wandb 输出了各种类型奖励的信息和 KL 散度系数的信息。</p><p></p><p>如果用户还需要输出其他信息，还可以参考 NLPVecInfo 类 和 VecInfo 类来实现自己的 CustomVecInfo 类。然后，需要在 train_ppo.py 中注册自定义的 CustomVecInfo 类:</p><p></p><p><code lang=\"apache\"># train_ppo.py \n# 注册自定义输出信息类 \nVecInfoFactory.register(\"CustomVecInfo\", CustomVecInfo)</code></p><p></p><p>最后，只需要在 nlp_ppo.yaml 中填写 CustomVecInfo 类即可启用：</p><p></p><p><code lang=\"makefile\"># nlp_ppo.yaml\nvec_info_class:\n    id: \"CustomVecInfo\" # 调用自定义CustomVecInfo类以输出自定义信息</code></p><p></p><p></p><h2>使用混合精度训练加速</h2><p></p><p></p><p>OpenRL 还提供了一键开启混合精度训练的功能。用户只需要在配置文件中加入以下参数即可：</p><p></p><p><code lang=\"bash\"># nlp_ppo.yaml\nuse_amp: true # 开启混合精度训练</code></p><p></p><p></p><h2>对比评测</h2><p></p><p></p><p>下表格展示了使用 OpenRL 训练该对话任务的结果。结果显示使用强化学习训练后，模型各项指标皆有所提升。另外，从下表可以看出，相较于 RL4LMs ， OpenRL 的训练速度更快（在同样 3090 显卡的机器上，速度提升 17% ），最终的性能指标也更好：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/02/02723b98d2f0cdc13fd99b2defa74af1.png\" /></p><p></p><p>最后，对于训练好的智能体，用户可以方便地通过 agent.chat() 接口进行对话：</p><p></p><p><code lang=\"python\"># chat.py\nfrom openrl.runners.common import ChatAgent as Agent\ndef chat():\n    agent = Agent.load(\"./ppo_agent\", tokenizer=\"gpt2\",)\n    history = []\n    print(\"Welcome to OpenRL!\")\n    while True:\n        input_text = input(\"&gt; User: \")\n        if input_text == \"quit\":\n            break\n        elif input_text == \"reset\":\n            history = []\n            print(\"Welcome to OpenRL!\")\n            continue\n        response = agent.chat(input_text, history)\n        print(f\"&gt; OpenRL Agent: {response}\")\n        history.append(input_text)\n        history.append(response)\nif __name__ == \"__main__\":\n    chat()</code></p><p></p><p>执行 python chat.py ，便可以和训练好的智能体进行对话了：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/be/bec73e55d2f1076848ead8d6cf7b1e90.gif\" /></p><p></p><p></p><h2>总结</h2><p></p><p></p><p>OpenRL 框架经过了 OpenRL-Lab 的多次迭代并应用于学术研究和 AI 竞赛，目前已经成为了一个较为成熟的强化学习框架。OpenRL-Lab 团队将持续维护和更新 OpenRL，欢迎大家加入我们的开源社区，一起为强化学习的发展做出贡献。更多关于 OpenRL 的信息，可以参考:</p><p></p><p>●OpenRL 官方仓库：<a href=\"https://github.com/OpenRL-Lab/openrl/\">https://github.com/OpenRL-Lab/openrl/</a>\"</p><p></p><p>●OpenRL 中文文档：<a href=\"https://openrl-docs.readthedocs.io/zh/latest/\">https://openrl-docs.readthedocs.io/zh/latest/</a>\"</p><p></p><p></p><h2>致谢</h2><p></p><p></p><p>OpenRL 框架的开发吸取了其他强化学习框架的优点：</p><p></p><p>●Stable-baselines3: <a href=\"https://github.com/DLR-RM/stable-baselines3\">https://github.com/DLR-RM/stable-baselines3</a>\"</p><p></p><p>●pytorch-a2c-ppo-acktr-gail: <a href=\"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail</a>\"</p><p></p><p>●MAPPO: <a href=\"https://github.com/marlbenchmark/on-policy\">https://github.com/marlbenchmark/on-policy</a>\"</p><p></p><p>●Gymnasium: <a href=\"https://github.com/Farama-Foundation/Gymnasium\">https://github.com/Farama-Foundation/Gymnasium</a>\"</p><p></p><p>●DI-engine: <a href=\"https://github.com/opendilab/DI-engine/\">https://github.com/opendilab/DI-engine/</a>\"</p><p></p><p>●Tianshou: <a href=\"https://github.com/thu-ml/tianshou\">https://github.com/thu-ml/tianshou</a>\"</p><p></p><p>●RL4LMs: <a href=\"https://github.com/allenai/RL4LMs\">https://github.com/allenai/RL4LMs</a>\"</p><p></p><p></p><h2>未来工作</h2><p></p><p></p><p>目前，OpenRL 还处于持续开发和建设阶段，未来 OpenRL 将会开源更多功能：</p><p></p><p>●支持智能体自博弈训练</p><p></p><p>●加入离线强化学习、模范学习、逆强化学习算法</p><p></p><p>●加入更多强化学习环境和算法</p><p></p><p>●集成 Deepspeed 等加速框架</p><p></p><p>●支持多机分布式训练</p><p></p><p></p><h2>OpenRL Lab 团队</h2><p></p><p></p><p>OpenRL 框架是由 OpenRL Lab 团队开发，该团队是第四范式公司旗下的强化学习研究团队。第四范式长期致力于强化学习的研发和工业应用。为了促进强化学习的产学研一体化，第四范式成立了 OpenRL Lab 研究团队，目标是先进技术开源和人工智能前沿探索。成立不到一年，OpenRL Lab 团队已经在 AAMAS 发表过三篇论文，参加谷歌足球游戏 11 vs 11 比赛并获得第三的成绩。团队提出的 TiZero 智能体，实现了首个从零开始，通过课程学习、分布式强化学习、自博弈等技术完成谷歌足球全场游戏智能体的训练：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e1/e1c675405a724d499c289c4b8078db6b.png\" /></p><p></p><p>截止 2022 年 10 月 28 日，Tizero 在及第评测平台上排名第一：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/1a/1afbca2eb2cc7ed8663ee9d5353f516b.png\" /></p><p></p><p></p><h2>作者介绍</h2><p></p><p></p><p>黄世宇，第四范式强化学习研究员。博士毕业于清华大学计算机系，博士导师是朱军和陈挺教授，本科期间在 CMU 交换，导师为 Deva Ramanan 教授。主要研究方向为强化学习，多智能体强化学习，分布式强化学习。曾在腾讯 AI Lab、华为诺亚、商汤、RealAI 工作。</p>",
    "publish_time": "2023-05-06 14:48:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "得物App全栈可观测平台落地实践",
    "url": "https://www.infoq.cn/article/lUNazGtLiTLBAg9S714b",
    "summary": "<p>得物初版以资讯类App在2015年上线，为了帮助年轻人了解潮流文化资讯，其中创造性的引入了“先鉴别、后发货”的方式解决在交易过程中遇到的问题，最近几年逐步从球鞋品类把它过渡到服装、箱包、化妆品等，到今天为止得物App它就不是一个简单的是球鞋交易平台，而是一个全品类综合性的电商平台。随着业务规模和复杂度的增加，监控的规模也随之扩大。</p><p></p><p>在4月22日ArchSummit（上海站）架构师峰会上，得物App监控平台架构师李尊和大家分享了得物在可观测性领域的一些工作成果。（7月21-22日<a href=\"https://archsummit.infoq.cn/2023/shenzhen\">深圳站ArchSummit会议</a>\"在筹备中，欢迎点击“阅读原文”关注）文末有彩蛋。</p><p></p><p>监控系统发展史</p><p></p><p>得物的技术栈主要是Java和Go，在2021年前主要选择当时业界比较流行的监控开源产品，像日志选择Loki，指标基于Prometheus生态，应用上报到注册中心后可实现自动化采集，存储选择VictoriaMetrics，Trace基于Jaeger，采用组合采样策略，头部采样和每分钟固定采样，最终采样率在3%左右，大盘基于Grafana构建。</p><p><img src=\"https://static001.infoq.cn/resource/image/58/51/58d23be01d214e6a5da061ae5c799751.png\" /></p><p></p><p>2021年我们开始制定标准进行监控治理，结合排查经验，引入应用监控，提供异常分析、慢MySQL统计和Redis热点分析，包括命中率、大Key。</p><p></p><p>2022年针对监控各个领域专项做深度迭代和打磨，比如 Trace，解决采样率低问题， 全量上报后通过尾部采样解决冷热存储问题以及引入OSS将存储成本降低90%。另一方面在数据联动上指标和Trace缺乏强关联性，引入OT打造端到端的链路追踪。在标准协议下实现指标关联Trace，Trace关联日志，在产品侧实现下钻分析。产品层开始摆脱Grafana的依赖，图表基于antv构建。</p><p><img src=\"https://static001.infoq.cn/resource/image/34/f7/347325f7b127c4e24f18ee3499d582f7.png\" /></p><p></p><p>整个2022年我们在数据质量方面和架构方面做了一个巨大的调整，主要是为了我们在2023年建设全息监控做准备，第一基于应用的这套体系快速的覆盖到中间件，包括K8s，实现指标的标准化，大盘的标准化，告警的标准化。再就是我们要以应用为中心，搭建一个可观性平台，通过以应用为中心构建一个元数据中心，同时基于Trace构建整个业务拓扑图，得到整个知识图谱实现一个全息监控。</p><p></p><p>通过Trace打通了所有层的关联，基于这些关联关系将元数据与异常仓库进行关联支撑整个得物NOC-SLA的体系建设,完成“1-5-10”的目标，并且在系统治理和用户体验上面得到一个很大的提升。</p><p></p><p>为什么选择 OpenTelemetry 呢？</p><p>&nbsp;</p><p>要实现一个链路追踪系统，我们首先要解决的就是数据采集的问题，早期得物的链路追踪使用SDK方式，升级起来比较麻烦，所以我们新的链路追踪就希望使用Java Agent采用字节码增强的方式，对应用透明，然后当时市面上基于Java Agent字节码增强实现比较成熟的有Pinpoint、Skywalking 和 OpenTelemetry。</p><p><img src=\"https://static001.infoq.cn/resource/image/26/bb/26f94dd80d677d662856f22c5ffb7cbb.png\" /></p><p></p><p>那 OpenTelemetry 是 CNCF 云原生基金会里面的项目，它规范了各观测数据的类型，以及采集和导出的一个标准方法，其中包括 Trace、Metric、Log，如果基于OpenTelemetry的话，我们就可以通过一套标准的方案将 Trace、Metric、Log进行生成和导出，然后可以将这些不同类型的数据友好的把它关联起来，从而降低我们在开发过程中对不同类型的观察数据的使用成本，且无缝兼容现有的主流可观测性系统，比如 Prometheus、OpenTracing。</p><p></p><p>同时国内外各大厂商都在适配它，包括谷歌、微软、阿里云、腾讯云等，不依赖任何特定的厂商，然后我们团队也比较看好 OpenTelemetry 以后的一个发展趋势，并且希望跟进 OpenTelemetry 的社区享受社区带来的技术红利和影响力，最终我们选择了基于 OpenTelemetry 来建设得物的Trace2.0。</p><p></p><p>分布式链路追踪 Trace2.0 架构</p><p><img src=\"https://static001.infoq.cn/resource/image/dc/2b/dc72bfa57e4fb9d1f046ddc2fb6b5f2b.png\" /></p><p></p><p>采集端：通过集成并定制 OT 提供的多语言 SDK(Agent)，生成统一格式的数据。目前支持Java\\Go\\Python\\JS 四个语言。控制平面：中心化的配置中心向采集侧下发各类配置并动态生效；支持应用按实例数灰度接入，并提供出入参动态开关、性能剖析、版本管理等数据网关： OTel Server 兼容 OT 协议，提供gRPC和 HTTP 两种方式,并将数据写入 Kafka计算侧：除了将Span数据落盘之外，还提供了场景化的数据分析能力包括：SpanMetric 的计算、Redis 热点分析、MySQL 热点分析、单号关联 Trace存储侧：主要引入对象存储，Trace的索引数据存放在ClickHouse中，明细数据存放OSS，元数据存放在图数据库中</p><p></p><p>Trace2.0能力建设</p><p></p><p>首先最重要的就是Trace2.0保存所有有价值的一个 Trace 的完整的链路，那哪些是“有价值”的Trace（详见《<a href=\"https://mp.weixin.qq.com/s/axNxUpwtq3Why0UbuAQF8Q\">得物云原生全链路追踪Trace2.0架构实践</a>\"》）。</p><p><img src=\"https://static001.infoq.cn/resource/image/01/6a/0145a08146265d53dc4a864bb9fd736a.png\" /></p><p></p><p>同时 Trace2.0 支持各类型监控的一个关联，就可以实现指标关联 Trace，Trace 关联日志，然后采集端基于 JavaAgent 构建，接入时只需要添加一个JVM 参数就可以了，后续升级就比较方便，对应用无感知，Trace2.0还提供了请求的出入参、应用自定义埋点、诊断工具等方面的能力。</p><p>&nbsp;</p><p>Trace2.0在存储层的演进历程</p><p>&nbsp;</p><p>第一阶段Trace完全基于 ClickHouse 构建，有两张表分别是 SpanIndex表和 SpanData表，SpanIndex用于高级检索，SpanData存储每个Span的明细数据，为什么选择 ClickHouse 呢，ClickHouse 每秒可以支持数百万甚至数千万行的写入。全量采集下写入能力还是非常重要的，此外稀疏索引可以提供更好的查询性能和内存占用。</p><p><img src=\"https://static001.infoq.cn/resource/image/53/ec/537115278d923ccfc451e94c5c4850ec.png\" /></p><p></p><p>第二阶段我们对 Trace 数据做了冷热分离，电商履约系统经常需要排查超过7天的场景，为此我们做了冷热存储分离，近7天的数据全量保存，超过7天的只保留“有价值”的数据，为什么选择基于kafka的延迟消费方式呢，主要是当前市面的的尾部采样都是在端侧决策，对异步场景支持不友好，容易导致Trace保留不全，无法保障完整的链路。</p><p></p><p>第二阶段的Trace数据还是保留在 ClickHouse 中，虽然采用冷热存储解决了长时间保留的问题，但 ClickHouse 的磁盘利用率比较高，使用的SSD盘价格也不低，第三阶段我们引入对象存储进一步的把Trace存储成本降低。</p><p>&nbsp;</p><p>Trace2.0 - 冷热存储</p><p>&nbsp;</p><p>下面来详细介绍下冷热存储和对象存储：</p><p>冷热存储主要基于 Kafka 的延迟消费和 BloomFilter 编码，数据网关将客户端上报Trace 数据写入到 Kafka中，并把满足采样规则（主要是错慢的Trace）TraceID 通过 BloomFilter 编码写入DB中；热集群实时消费 Kafka 中的数据，并持久化到ClickHouse中，数据保留7天；冷集群延迟消费 Kafka 中的数据，通过 BloomFilter 判断 TraceID 是否命中，命中则写入 ClickHouse中，数据保留30天；</p><p><img src=\"https://static001.infoq.cn/resource/image/e5/d5/e58883c1f9bde99b8e4a40a8c19c88d5.png\" /></p><p></p><p>相关设计细节可以参考《<a href=\"https://mp.weixin.qq.com/s/axNxUpwtq3Why0UbuAQF8Q\">得物云原生全链路追踪Trace2.0架构实践</a>\"》</p><p>&nbsp;</p><p>Trace2.0 - 对象存储 写入流程</p><p>&nbsp;</p><p>如何降低Trace的存储成本，第一个想到的开源项目就是Grafana 的Tempo，Tempo使用对象存储(索引和明细)，并兼容一些主流协议像 Zipkin，Jaeger，OpenTelemetry，综合考虑后决定引入对象存储，将索引放在 ClickHouse 中，明细存放在对象存储中。</p><p><img src=\"https://static001.infoq.cn/resource/image/5c/2b/5c18b710fa190afa2a8cd87e19b5db2b.png\" /></p><p></p><p>从Kafka消费Span后，先将Span写内存，当内存满足一定条件后会对这批Span数据进行ZSTD压缩后刷盘到对象存储OSS后。写OSS采用追加写方式，每次写入可获得OSS的文件名和偏移量，将地理位置存储 ClickHouse 中。</p><p>&nbsp;</p><p>Trace2.0 - 对象存储 查询流程</p><p>&nbsp;</p><p>查询时，先通过 ClickHouse 确定一个TraceID 都在哪些OSS文件块中，获得文件块偏移量后通过随机读(Range)方式将文件块下载到本地(注意这里是文件块，不是文件)，再通过块内偏移量解析得到Span。汇聚所有Span得到一个完整的Trace。</p><p><img src=\"https://static001.infoq.cn/resource/image/99/21/99af2a4cbbc4ac27d0f57a2f8b658e21.png\" /></p><p></p><p>这里需要注意文件块的大小，一次批量写会包含不同Trace下的 Span，如果文件块过大，会导致解压时间过长，查询会超时，测下来文件块设置4MB是一个比较合适的值。</p><p>&nbsp;</p><p>实施效果：Trace2.0 现状</p><p>&nbsp;</p><p>Trace2.0在得物落地一年多时间，现已覆盖公司绝大部分在线业务。</p><p>每秒TPS在12M/s个SpanTrace点查P50耗时在300ms, P90查询耗时在800ms每天日增数据量在700+TB热存保留6天总存储4PB、冷存保留30天存储1PB，采用ZSTD压缩比在12，ClickHouse 单机可支持每秒40w个Span写入</p><p>&nbsp;</p><p>可观测性平台 - 前端监控</p><p>&nbsp;</p><p>第一个案例的话就是我们前后端链路打通的能力，在前后端没有打通之前，当前端收到一个异常后，我们无法全面地了解系统的性能状况，无法监控请求的传输路径和处理时间，也无法对系统的性能指标进行全面的分析。</p><p><img src=\"https://static001.infoq.cn/resource/image/3d/38/3d956129ce73438f5240f8f187a57338.png\" /></p><p></p><p>为了解决这个痛点的话，我们做了前后端的链路打通。这边是一个效果图，前端的异常就可以看到后端的链路，关联了哪些服务，通过出入参可以很方便的排查问题。同时提供了接口分析和页面分析。另外也支持会话跟踪，当出现问题时通过 SessionID 实现全链路追踪，还原用户的行为轨迹，包括页面加载、接口请求、和用户操作等。</p><p>&nbsp;</p><p>可观测性平台 - 容器监控</p><p>&nbsp;</p><p>随着公司应用全面容器化，对于 K8s 集群的稳定性要求越来越高，但是针对 K8s 监控没有一个专业和全面的监控产品满足问题发现和故障定位。</p><p><img src=\"https://static001.infoq.cn/resource/image/5c/4b/5ca6d23c56185e9dfde094d886bdcd4b.png\" /></p><p></p><p>早期K8s 监控大盘基于 Grafana 构建，指标理解和大盘配置门槛较高，且监控面板比较分散，</p><p>SRE 积累的排障工具无法与监控产品做融合，图表无下钻能力，为此打造一款专业的 K8s监控产品，统一指标计算口径，对现有指标做好分类分级，单击单个集群视角可查看关联资源池监控，资源池可下钻分析单个 Node节点的监控，POD视角可关联查看Node相关监控，大盘之间可进行联动分析，以及提供控制平面 API-Server 和 etcd 的监控。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（接口分析）</p><p>&nbsp;</p><p>讲完 K8S监控我们再看看应用监控，首先我们来看下接口分析，接口我们按照组件类型分为接口、下游调用、SQL调用、NoSQL 调用、以及 MQ 和其他进行分类。</p><p><img src=\"https://static001.infoq.cn/resource/image/4d/ba/4d79bf4559df20a4bf65932bcf3e00ba.png\" /></p><p></p><p>图表右侧展示接口的黄金三指标，底下展示接口关联的Trace，接口支持的实例维度、上下游分析和耗时分解等排查场景。有了耗时分解我们就可以很清晰的看出接口RT变高，是由哪类操作类型导致，DB 还是Redis，以及关联的 Trace。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（异常分析）</p><p>&nbsp;</p><p>接下来介绍应用监控下另外一个非常重要的模块“异常分析”，第一个是基于异常日志按照异常名进行统计分析。</p><p><img src=\"https://static001.infoq.cn/resource/image/49/ec/495ebc2c7efc91aaec7770b2749f5eec.png\" /></p><p></p><p>第二个是错误码分析，主要根据响应体里的 Code码和 CodeMessage，通过对业务码分类，可以快速的帮助我们了解业务需求和业务流程，有助于问题的发现和定位。</p><p>第三块是MySQL热点分析，按照SQL指纹统计调用次数、慢SQL次数以及关联的接口名。</p><p>第四块是Redis热点分析，通过客户端记录Redis的入参和出参，统计Redis命中率、大Key、高频写、慢调用。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（链路追踪）</p><p>&nbsp;</p><p>接下来介绍下应用监控的链路Trace部分。用户可以在接口分析页的黄金三指标曲线上点击就能看到对应的Trace链路。同时也在接口页面的右下角展示Trace检索页，可以快速的看出错、慢Trace。</p><p><img src=\"https://static001.infoq.cn/resource/image/46/1f/4677f010cdb5e50yy99d5fc31c0bac1f.png\" /></p><p></p><p>此外提供高级检索模式，满足研发自定义查询场景，支持多维查询，比如实例、或者指定上游或者指定下游进行过滤。</p><p>&nbsp;</p><p>此外，在电商场景下，研发多以订单号、作为排障的场景，因此我们和研发团队约定埋点规则 ——在Span的 Tag上记录单号—— 便会在 ClickHouse 中新写一条“单号关联表”。用于记录单号ID和 TraceID 的映射关系，研发通过单号查看所有的 Trace。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（Trace详情）</p><p>&nbsp;</p><p>Trace详情页的话会将一个Trace下的所有Span在一张图中显示摘要信息根据TraceID解码当前链路的发生时间和来源IP。</p><p><img src=\"https://static001.infoq.cn/resource/image/13/3e/13c495eae80a26b47f73yy7cbyyf3d3e.png\" /></p><p></p><p>目前我们提供两种展示模式，列表详情以及聚合统计。</p><p>在聚合统计模式下，可以查看trace的整体性能指标，这次请求经过哪些服务，以及该服务请求了哪些组件，可以直观的看出哪个服务耗时最高以及耗时最大的接口是哪个。列表模式下支持自定义列，通过自定义可以快速的帮助我们理解Trace上下文，比如在染色环境下，测试同学想一眼看出当前链路是否跨环境了，再比如通过线程名可以快速帮助我们理解异步上下文。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（Trace详情）</p><p>&nbsp;</p><p>单击某个Span，Span详情会以抽屉方式展示，上面是Span的摘要信息，下面按照属性含义做了分类。部分属性支持跳转到其他平台，比如CMDB和容器平台，在Span详情页的打通了指标和日志，日志主要根据TraceID 关联。</p><p><img src=\"https://static001.infoq.cn/resource/image/fd/49/fd5b0bccddd4784e5297464a00c8e749.png\" /></p><p></p><p>指标可显示当前Span的黄金三指标、以及当前Span所在JVM状态包括GC、线程池和POD的 CPU和内存以及POD所在宿主机的资源使用情况，这样可以减少页面来回切换。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（告警平台）</p><p>&nbsp;</p><p>在报警分析上，主要使用Prometheus作为数据源，通过专家经验梳理告警模板，可以快速的应用到其他服务，同时可以保障新应用上线后低成本接入，目前共提供50多个模版。</p><p><img src=\"https://static001.infoq.cn/resource/image/78/03/7888460afcf69c822760b5d2027a9d03.png\" /></p><p></p><p>在告警规则上，支持同比、环比、无数据。在通知组上支持飞书、短信、电话，以及支持分钟级、小时级、天级的告警聚合通知。</p><p>&nbsp;</p><p>【活动推荐】</p><p>&nbsp;</p><p>7 月 21-22 日，将在深圳·博林天瑞喜来登酒店举办 <a href=\"https://archsummit.infoq.cn/2023/shenzhen\">ArchSummit 架构师峰会</a>\"，会议主题还是会围绕AIGC、前端架构、数字化、架构思维，和架构师成长来展开。如果你想要分享演讲话题，可以在这里提交议题思路：<a href=\"https://jinshuju.net/f/7wUiwn\">https://jinshuju.net/f/7wUiwn</a>\"</p>",
    "publish_time": "2023-05-06 15:16:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "科大讯飞正式发布星火认知大模型，刘庆峰：多题型可解析数学能力已领先ChatGPT | InfoQ快讯",
    "url": "https://www.infoq.cn/article/057ZDYfzOJZOZUfpOLH7",
    "summary": "<p>5月6日，InfoQ获悉，科大讯飞正式发布了“讯飞星火认知大模型”。</p><p>&nbsp;</p><p>在星火大模型发布会上，科大讯飞董事长刘庆峰表示，认知智能全国重点实验室牵头设计了通用认知智能大模型测评体系。</p><p>&nbsp;</p><p>现场，科大讯飞研究院院长刘聪，上台演示了讯飞星火认知大模型的实际应用。据介绍，星火认知大模型具备以下能力：多风格多任务长文本生成、多层次跨语种语言理解，泛领域开放式知识问答，情景式思维链逻辑推理，多题型可解析数学能力，多功能多语言代码能力。</p><p>&nbsp;</p><p>其中，多风格多任务长文本生成能力，包括发言稿、邮件、新闻通稿、营销方案、调研问卷、商业文案、英文写作等。多层次跨语种语言理解能力，涵盖语法检查、要素抽取、语篇规整、文本摘要、阅读理解、情感分析，机器翻译等。</p><p>&nbsp;</p><p>刘庆峰表示，在多题型可解析数学能力上，星火认知大模型摇摇领先，已领先ChatGPT。多功能多语言代码能力和ChatGPT还有一定差距，后续将不断提升。</p><p>&nbsp;</p><p>刘庆峰介绍，讯飞星火大模型将在6月9日前，将开启实时问答，升级多轮对话能力；在8月15日前，再次提升代码能力以及多模态交互能力；在10月24日前，星火大模型将对标ChatGPT，在中文能力上超过ChatGPT，在英文能力上与ChatGPT相当。</p><p>&nbsp;</p>",
    "publish_time": "2023-05-06 15:20:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "拼多多回应将总部从中国迁至爱尔兰；微软Bing突然爆炸级更新，文生图原生支持中文；75岁人工智能教父离职谷歌，痛悔毕生工作｜ Q资讯",
    "url": "https://www.infoq.cn/article/WG49e1Q55SNA0B9kZkMj",
    "summary": "<p>五一假期已经结束，让我们一起看看本周有哪些重要新闻吧！</p><p>&nbsp;</p><p>拼多多回应总部由中国迁至爱尔兰：消息严重失实</p><p>&nbsp;</p><p>微软Bing突然全面开放插件系统，文生图原生支持中文</p><p>&nbsp;</p><p>消息称马斯克已将推特员工数量减至约1000人&nbsp;</p><p>&nbsp;</p><p>75 岁人工智能之父突然离职谷歌：痛悔毕生工作，警告 AI 会对人类构成巨大威胁</p><p>&nbsp;</p><p>IBM 将用 AI 取代部分工作岗位，7800人将失业</p><p>&nbsp;</p><p>世界经济论坛：未来五年 AI 或淘汰2600万个工作岗位</p><p>&nbsp;</p><p>蒂姆·库克称苹果公司仍未考虑“大规模裁员”</p><p>&nbsp;</p><p>暴雪 CEO 呼吁更多公司合并以便与腾讯竞争</p><p>&nbsp;</p><p>Angular 16 正式发布</p><p>&nbsp;</p><p>sudo 和 su 正用 Rust 重写</p><p>&nbsp;</p><p>&nbsp;</p><p>今日好文推荐</p><p>&nbsp;</p><p><a href=\"https://mp.weixin.qq.com/s/LH3751pEeqBWxe375m0kXA\">谷歌、OpenAI 都白干，开源才是终极赢家！谷歌内部文件泄露：欲借开源打败 OpenAI</a>\"</p><p>&nbsp;</p><p><a href=\"https://mp.weixin.qq.com/s/H29PBWmeEA5mfI38ikigMA\">谷歌用机器人大规模删除代码：二十多年积累了数十亿行，已删除5%C++代码</a>\"</p><p>&nbsp;</p><p></p><h4>拼多多回应将总部从中国迁至爱尔兰：消息严重失实</h4><p></p><p>&nbsp;</p><p>近期，有媒体报道称，拼多多提交给美国证券交易委员会（SEC）的20-F文件显示，爱尔兰已经成为其注册地。另有最新文件显示，该公司的“主要行政办公室”在都柏林，而2月份的文件还显示该公司所列的地址是上海。5月4日下午，对于外媒报道的“拼多多总部从中国迁至爱尔兰”的消息，拼多多方面回应称严重失实，纯属误读。该负责人表示，“拼多多出生在上海，成长在中国，拼多多总部始终在中国上海，不会改变。”</p><p>&nbsp;</p><p>3月下旬美国政府持续对 TikTok 提出的质疑；4月初，美国国会下属美中经济与安全审议委员会曾发布分析师报告，提示跨境电商平台 SHEIN、拼多多旗下 Temu 及其他中国电商可能存在风险。目前来看，拼多多此举应该是为了提前做出应对。</p><p></p><h4>微软Bing突然全面开放插件系统，文生图原生支持中文</h4><p></p><p>&nbsp;</p><p>5月5日，<a href=\"https://www.infoq.cn/article/ZmDbgaSV7PmYEG3MwQAx\">微软突然官宣全面开放 Bing Chat</a>\"：无需任何等待。只需注册一个账户，首页即可体验。新的 Bing Chat 不止能聊天，还能问问题、搜索网页，甚至还能生成图片。</p><p>&nbsp;</p><p>这次做的大规模升级主要有三个方面：首先，在插件功能上，新版 Bing Chat 将允许第三方参与 Bing Chat AI 的结果。微软正在与 OpenTable 合作，将餐厅预订纳入到 AI 响应中。其次，现在 Bing Chat 除了能做普通的文字交流问答、编程学习外，还能帮助用户“生成图片” 甚至是视频等，而且以文生图的功能还支持了中文在内的多种语言。此外，微软还在 Bing Chat 中添加了一项备受期待的功能：历史记录。这个新的历史聊天记录功能可以让用户跨设备获取聊天机器人对话，甚至可以将 Bing Chat 用作研究工具。</p><p></p><h4>消息称马斯克已将推特员工数量减至约1000人&nbsp;</h4><p></p><p>&nbsp;</p><p>5月4日消息，据外媒报道，有知情人士称，收购推特半年后，特斯拉 CEO 埃隆·马斯克已将该公司的员工人数裁减了近90%，目前只剩下大概1000名全职员工。知情人士认为，在排除掉合同工后，现在的推特剩余员工数量远低于此前马斯克对媒体所说的1500名。</p><p>&nbsp;</p><p>值得注意的是，此前马斯克曾表示，在他去年11月接管推特时，该公司拥有约7800名员工，他上任后裁掉了6500人，现在仅剩下1500名员工。他还提到，解雇推特的大部分员工是他自接管公司以来不得不做的“最艰难的事情之一”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>75 岁人工智能之父突然离职谷歌：痛悔毕生工作，警告 AI 会对人类构成巨大威胁</h4><p></p><p>&nbsp;</p><p>近日，有“人工智能教父”之称的 Geoffrey Hinton 宣布辞去在谷歌的工作，并且现在对自己一生从事的工作感到后悔：“我总在用这样的借口安慰自己：哪怕我自己不做，其他人也会这样做。但目前真的不知道要怎么防止坏蛋利用 AI 来作恶。”</p><p>&nbsp;</p><p>Hinton 在社交平台表示，他辞去谷歌的工作以便自己可以自由地谈论 AI 的风险。他认为，随着企业改进 AI 系统，它们会变得越来越危险。他目前最担心的是，互联网将充斥着虚假照片、视频和文字，普通人将“无法再知道什么是真的”。虚假信息的传播只是 Hinton 眼下想要强调的风险之一。从长远来看，他担心 AI 会彻底消除一切需要大量记忆的工作，而随着其逐步编写并运行构成自身的代码，AI 也许会最终取代人类。Hinton 今年75岁，作为三位“人工智能教父”之一，与另外两位合作伙伴共同获得了 2018 年图灵奖，旨在表彰他们为当前 AI 繁荣做出的基础性贡献。</p><p></p><h4>IBM 将用 AI 取代部分工作岗位，7800人将失业</h4><p></p><p>&nbsp;</p><p>5月1日，IBM 宣布一个重磅消息：将暂停招聘人工智能可以胜任的岗位，将用 AI 取代7800个工作岗位。这意味着将有大约7800人失业。</p><p>&nbsp;</p><p>IBM首席执行官 Arvind Krishna 透露，暂停招聘的主要为后台岗位，比如人力资源等。这类岗位大约有26000名员工。“我可以很容易地看到，在接下来5年的时间里，其中30%的工作将被人工智能和自动化取代。”该 CEO 很确定地预测，在未来5年里，这些岗位的30%工作将被人工智能等取代。目前，IBM 的员工总数约26万，并将继续招聘软件开发和面向客户的职员。IBM的最新计划标志着人工智能技术对人类就业的冲击和风险。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>世界经济论坛：未来五年 AI 或淘汰2600万个工作岗位</h4><p></p><p>&nbsp;</p><p>5月1日，世界经济论坛发布的一份报告显示，未来五年内，由于人工智能、数字化以及绿色能源转型和供应链回流等因素，全球近四分之一的工作岗位将发生变化。此次调查涉及800多家企业，约75%的受访公司表示，他们预计将在未来五年内采用 AI 技术；AI 将淘汰多达2600万个记录和行政职位，比如收银员、票务员、数据录入和会计。</p><p>&nbsp;</p><p>对于这种前景，世界经济论坛常务董事 Saadia Zahidi 评论称，总体而言，世界就业形势变化的程度相当高。同时根据世界经济论坛的预测，各企业将需要更多员工协助运用和管理 AI 工具。截至2017年，信息分析师、科学家、机器学习专家和网络安全专家职位预估将增长30%。</p><p></p><h4>蒂姆·库克称苹果公司仍未考虑“大规模裁员”</h4><p></p><p>&nbsp;</p><p>5月5日，苹果公司首席执行官蒂姆·库克表示，他仍将大规模裁员视为“最后手段”，并确保这不是公司目前正在考虑的事情。苹果公司至今没有大规模裁减全职员工，相对而言，Google 和Facebook 母公司 Meta 等大型科技公司在最近几个月经济不稳定的情况下，裁减了数万名员工。</p><p>&nbsp;</p><p>\"我认为这是最后的手段，因此，大规模裁员不是我们目前正在谈论的事情，\"库克在接受 CNBC 采访时说。在去年的一份监管文件中，苹果公司表示，截至2022年9月24日，该公司约有16.4万名全职等值员工。</p><p></p><h4>暴雪CEO呼吁更多公司合并以便与腾讯竞争</h4><p></p><p>&nbsp;</p><p>动视暴雪CEO鲍比・科蒂克（BobbyKotick）日前接受外媒CNBC采访，继续对英国 CMA 阻止微软收购动视暴雪的决定进行反击。他表示应该允许更多的合并，以便与腾讯等公司竞争。</p><p>&nbsp;</p><p>据悉，科蒂克称：“我认为在某个时候，监管机构会意识到大量高薪工作从欧美科技公司流失。我关注了字节跳动、腾讯，这些都是世界上各自行业中最好的公司，公司要想竞争，就必须能够进行整合或合并。”科蒂克还补充说，如果交易无法完成，动视暴雪依然会继续前进。“我们是一家强大的公司，无论交易是否通过，我们都将继续作为一个独立公司运营，当在全球市场运营时，其会变得更具挑战性。”</p><p></p><h4>Angular 16 正式发布</h4><p></p><p>&nbsp;</p><p>本周， Angular 16正式发布。在之前的 Angular 15中，Angular 团队通过将独立 API 从开发者预览版升级至稳定版，将 Angular 的简易性和开发者体验提升到了一个重要的里程碑阶段。如今，Angular 将继续这一改进势头，发布自 Angular 最初推出以来最大的一次版本更新。新版本在Reactivity、服务器端渲染和工具方面取得了巨大的飞跃。</p><p>&nbsp;</p><p>Angular 16 的新特性包括：全新 Reactivity 模型的开发者预览，完全向后兼容，Angular Signals 库，RxJS 互操作性；服务器端渲染和 hydration 增强；改进独立组件、指令和管道的工具等。</p><p></p><h4>sudo 和 su 正用 Rust 重写</h4><p></p><p>&nbsp;</p><p>据奇客 Solidot 消息，在亚马逊 AWS 的资助下，类 Unix 操作系统广泛使用的工具 sudo 和 su 正用 Rust 语言重写，以提高软件在内存方面的安全性，进一步增强 Linux 和开源生态系统的安全性。sudo 的开发始于 1980 年代，过去几十年里它已经成为一种必不可少的工具，但它是用 C 语言编写的，遭遇过许多与内存安全问题相关的漏洞。为了确保关键软件的安全性，防止内存安全漏洞，Ferrous Systems 和 Tweede Golf 正在联合将 sudo 从 C 移植到 Rust，他们的项目 sudo-rs 托管在 GitHub 上。</p>",
    "publish_time": "2023-05-06 15:39:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "作业帮在多云环境下的高可用双活架构优化实践",
    "url": "https://www.infoq.cn/article/WdFEZ5ZfOe11vXgFdELr",
    "summary": "<p></p><blockquote><a href=\"https://www.infoq.cn/article/ARMLcL0qLpBztICm1XGv\">作业帮</a>\"作为国内主流在线教育品牌之一，旗下有多款教育软件产品与硬件产品，而且每个产品背后的业务都有不同的特性和诉求。在这个背景下，作业帮采用多云架构（阿里云、百度云、腾讯云），并同时使用 MySQL、Redis-Cluster、MongoDB、Elastisearch、TiDB 、OceanBase这几款数据库。出于高可用和降本需求，作业帮决定将更多 MySQL业务场景用 OceanBase 代替，本文将和大家分享具体原因，以及<a href=\"https://github.com/oceanbase/oceanbase\">OceanBase 4.0</a>\"与 MySQL5.7 的对比数据。</blockquote><p></p><p></p><p>作者｜刘强，就职于作业帮基础架构 DBA 团队，负责分布式数据库的探索和使用，协同研发团队在公司内部推进分布式数据库在业务上的落地。</p><p></p><h2>高可用双活架构方案升级需求</h2><p></p><p></p><p>由于<a href=\"https://www.infoq.cn/article/i7LFwhxHVb69FaxEThZt\">作业帮</a>\"业务的多样性和复杂性，我们对于分布式数据库的使用需求主要基于以下几个方面：</p><p></p><p>第一，在海量数据的情况下希望减少分库分表的复杂度，并解决单机存储瓶颈。</p><p></p><p>第二，对 I/O 密集型的 SQL 及 CPU 密集型的 SQL 来说，我们希望能够提高响应速度，减少它在 MySQL 中对线上业务的影响。</p><p></p><p>第三，每个业务内部都需要业务人员频繁查询、录取线上数据，并有相应的报表服务以供上级 Leader 查看，而且大数据部门也会有报表需求接入线上数据，这对于线上 MySQL 来说难以支撑，在数据归档及汇总的情况下，也缺乏良好方案。</p><p></p><p>第四，由于MySQL的特性限制，我们需要基于一个外部的高可用组件来实现 MySQL 的高可用架构，在多云环境下，网络环境相对复杂，这对高可用的稳定性提出了更高要求。如果部分业务的请求链路长或复杂，跨云访问会使业务相应耗时增加，影响用户体验。</p><p></p><p>因此，我们需要探索良好的双活<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1511\">架构方案</a>\"，初步方案是基于 MySQL ，并引入 DTS 来实现双活架构。这种架构的复杂性及引入过程中 DTS 的异常或中断，对于数据的一致性有很大的挑战。同时在使用公有云的情况下，也希望能够最大程度降低硬件的使用成本。</p><p></p><p>出于高可用和降本需求，我们决定将更多<a href=\"https://www.infoq.cn/article/3xtSDtHUgTKRsyw3kZXH\"> MySQL </a>\"的业务场景替换为 OceanBase，并对OceanBase 和 MySQL5.7 进行了多方面的对比。</p><p></p><h2>OceanBase 4.0 对比 MySQL5.7</h2><p></p><p></p><h3>性能对比</h3><p></p><p>我们使用32C64GB的硬件规格分别对<a href=\"https://www.infoq.cn/article/D2EylZ7X3xS6NCiuRtyb\"> OceanBase </a>\"和 MySQL 进行性能、CPU使用率、磁盘空间占用的测试。</p><p></p><p>首先，从图1可见，在这样的硬件条件下，OceanBase性能超过了MySQL。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4ce25ba0ae5b16fe307467e1854ab95c.png\" /></p><p></p><p>其次，从图2得知，在相同的并发环境下，OceanBase 的 CPU 使用率比 MySQL 低至少一倍以上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f57dee792ea46cbae0d2f221c468ab1.png\" /></p><p></p><p>另外，由于 OceanBase 数据压缩及编码的技术相较于 MySQL，能够节约 2/3 以上的磁盘空间，因此，综合上述三方面的对比结果，我们认为 OceanBase 能为<a href=\"https://www.infoq.cn/article/3htE4GjwUYvqetCs3spP\">作业帮</a>\"的降本增效提供较大帮助。</p><p></p><p>在性能方面，我们还测试了DDL的执行速度。对于耗时较长的 DDL，MySQL 会有补充延时问题，需要我们引用额外的审核工具来控制它的延迟，而 OceanBase 不存在延时问题。对于执行速度，MySQL 和 OceanBase 相差不大，这让我们更加期待 OceanBase 4.1 的数据旁路导入功能，可以将 DDL 的执行速度大幅提升。不过，我们也发现了一些语法兼容性的问题，例如，OceanBase 对主键的操作语法不支持多个 DDL 合并执行，只能各自单独执行。</p><p></p><h3>架构对比</h3><p></p><p></p><p>除了降本增效的需求，高可用也是我们在探索双活架构中最看重的一方面。相较于 MySQL ，<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1516\">OceanBase </a>\"的高可用是有延伸的，不需要额外的高可用组件，这有利于解决<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1516\">数据</a>\"不一致的问题。再加上OceanBase 的日志具备多副本特性，能够支持在多机房或多城市灵活部署。OceanBase 还便于作业帮实现一些单元化的需求，我们可以将业务单元内的 Leader 数据调度在某一个机房内，实现业务访问的流量闭环，减少跨域读写。</p><p></p><h3>字符集对比</h3><p></p><p></p><p>最后，我们测试了字符集的支持程度。作业帮成立十年，我们使用 MySQL 的场景和字符集种类都比较多。OceanBase 4.0当前支持图3 中显示的几种字符集，在4.1版本中增加了对拉丁字符的支持。后续我们也希望 OceanBase 能够扩展字符集及校验集的支持种类。</p><p><img src=\"https://static001.geekbang.org/infoq/eb/eb0157064f078a6098f5c29bb71f814b.png\" /></p><p>以上就是作业帮对 OceanBase 和 MySQL的主要对比数据。在将更多业务场景切换至 OceanBase 的过程中，我们发现，在高可用双活架构方案之外， OceanBase 4.0 的HTAP和资源隔离能力也为我们带来许多意外之喜。</p><p></p><h2> HTAP 两大优势：低成本、低延时</h2><p></p><p></p><p>OceanBase 是一个具备 HTAP 能力的原生<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1516\">分布式数据库</a>\"，如何理解 HTAP？引用 OceanBase CTO 的一句话：HTAP 就是在高性能 OLTP 数据库的基础上扩展 OLAP 的能力，能很好支持实时分析。</p><p></p><p>在作业帮的业务场景中，我们感受到 HTAP 的两大显著优势：低成本和低延时。</p><p>低成本：我们希望一套系统能同时支持OLTP场景和OLAP场景，相比两套系统拥有更高的性价比。低延时：省去了繁琐费时的ETL过程，降低延时，更好支持实时分析。</p><p></p><p>我们知道，在一套系统同时实现OLTP和OLAP的能力，其中一项挑战是资源隔离，使业务之间互不影响。这便是OceanBase 带给我们惊喜的地方。</p><p></p><p>对于核心业务来说，我们希望能够使用物理资源管理，比如行存副本服务 OLTP，列存副本服务 OLAP，这两种业务是不共享物理资源的，可以做到绝对的隔离。 OceanBase 可以增加额外的只读副本，再通过配置 OBProxy 的 proxy_idc_name 实现读写分离。</p><p></p><p>图 4 为OceanBase 的物理资源隔离方案，基于只读副本，再增加逻辑机房的情况下，在 OBProxy 中配置逻辑机房的位置。所有 OLAP 的只读流量都会录入只读副本中，避免与 OLTP 副争抢资源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/64/64ea43f8c40b46de403535a5cd1bc195.png\" /></p><p>对于成本敏感的逻辑资源隔离，OceanBase 在同一租户内就可能实现 OLAP 和 OLTP 的物理资源共享，进而实现资源隔离。</p><p></p><p>对于逻辑隔离来说，首先 OceanBase 定义了一个大查询，默认将执行时间超过 5 秒的请求判定为大查询，当大查询和短查询同时争抢 CPU 时，大查询会被降低优先级，待 CPU 资源充足时再被挂起，我们可以设置Large_query_worker_percentage 在同一租户内，大查询最多可以占用30%的用户线程数。在这种情况下，我们可以有效隔离大查询对 OLTP 业务的影响，优先保证了 OLTP 业务的执行。</p><p></p><p>我们使用了一些线上业务数据和 SQL 来对比 MySQL 和 OceanBase。在作业帮的业务场景中，一个大业务部门的报表需要多级 Leader甚至上百人频繁查看，因此，即使是 OLAP 类型的业务，QPS也可以达到几十甚至上百。我们使用了60个并发去压测较复杂的 SQL，通过图 5 可以看出，OceanBase 比 MySQL 最起码快了一倍以上。OceanBase 的CPU使用率也基本控制在25%以下。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a58f4100bcfac165833d148215d44fa.png\" /></p><p>在60个并发执行 OLAP 业务的同时，我们也用256个并发去运行 Sysbench 任务，在 OLAP SQL 扫描量较大的情况下，我们可以看到它的耗时出现了一些抖动（见图6）。</p><p><img src=\"https://static001.geekbang.org/infoq/83/838e315eb910dbc0b2b0c2a3b085c52f.png\" /></p><p></p><p>以上就是作业帮对OceanBase 4.0 的探索过程，供大家参考。</p>",
    "publish_time": "2023-05-06 16:48:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从 Elasticsearch 到 Apache Doris，构建新一代日志存储分析平台",
    "url": "https://www.infoq.cn/article/Kmui4oKixhx4Mvpqh3CD",
    "summary": "<p></p><blockquote>日志数据的处理与分析是最典型的大数据分析场景之一，过去业内以 Elasticsearch 和 Grafana Loki 为代表的两类架构难以同时兼顾高吞吐实时写入、低成本海量存储、实时文本检索的需求。Apache Doris 借鉴了信息检索的核心技术，在存储引擎上实现了面向 AP 场景优化的高性能倒排索引，对于字符串类型的全文检索和普通数值、日期等类型的等值、范围检索具有更高效的支持，相较于 Elasticsearch 实现性价比 10 余倍的提升，以此为日志存储与分析场景提供了更优的选择。</blockquote><p></p><p></p><p>作者 | 肖康，SelectDB 技术副总裁</p><p></p><h2>日志数据分析的需求与特点</h2><p></p><p></p><p>日志数据在企业大数据中非常普遍，其体量往往在企业<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1511\">大数据体系</a>\"中占据非常高的比重，包括服务器、数据库、网络设备、IoT 物联网设备产生的系统运维日志，与此同时还包含了用户行为埋点等业务日志。</p><p></p><p>日志数据对于保障系统稳定运行和业务发展至关重要：基于日志的监控告警可以发现系统运行风险，及时预警；在故障排查过程中，实时日志检索能帮助工程师快速定位到问题，尽快恢复服务；日志报表能通过长历史统计发现潜在趋势。而用户埋点日志数据则是用户行为分析以及智能推荐业务所依赖的决策基础，有助于用户需求洞察与体验优化以及后续的业务流程改进。</p><p></p><p>由于其在业务中能发挥的重要意义，因此构建统一的日志分析平台，提供对日志数据的存储、高效检索以及快速分析能力，成为企业挖掘日志<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1525\">数据</a>\"价值的关键一环。而日志数据和应用场景往往呈现如下的特点：</p><p></p><p>数据增长快：每一次用户操作、系统事件都会触发新的日志产生，很多企业每天新增日志达到几十甚至几百亿条，对日志平台的写入吞吐要求很高；数据总量大：由于自身业务和监管等需要，日志数据经常要存储较长的周期，因此累积的数据量经常达到几百 TB 甚至 PB 级，而较老的历史数据访问频率又比较低，面临沉重的存储成本压力；时效性要求高：在故障排查等场景需要能快速查询到最新的日志，分钟级的数据延迟往往无法满足业务极高的时效性要求，因此需要实现日志数据的实时写入与实时查询。</p><p></p><p>这些日志数据和应用场景的特点，为承载存储和分析需求的日志平台提出了如下挑战：</p><p></p><p>高吞吐实时写入：既需要保证日志流量的大规模写入，又要支持低延迟可见；低成本大规模存储：既要存储大量的数据，又要降低存储成本；支持文本检索的实时查询：既要能支持日志文本的全文检索，又要做到实时查询响应；</p><p></p><h2>业界日志存储分析解决方案</h2><p></p><p>当前业界有两种比较典型的日志存储与分析<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1511\">架构</a>\"，分别是以 <a href=\"https://www.infoq.cn/article/hefb1QQsWF2NHoRQ-tL0\">Elasticsearch</a>\" 为代表的倒排索引检索架构以及以 Loki 为代表的轻量索引/无索引架构，如果我们从实时写入吞吐、存储成本、实时交互式查询性能等几方面进行对比，不难发现以下结论：</p><p></p><p>以 ES 为代表的倒排索引检索架构，支持全文检索、查询性能好，因此在日志场景中被业内大规模应用，但其仍存在一些不足，包括实时写入吞吐低、消耗大量资源构建索引，且需要消耗巨大存储成本；以 Loki 为代表的轻量索引或无索引架构，实时写入吞吐高、存储成本较低，但是检索性能慢、关键时候查询响应跟不上，性能成为制约业务分析的最大掣肘。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/65/653febb49bf84c1cd21dcd82ecc4e112.png\" /></p><p></p><p>ES 在日志场景的优势在于全文检索能力，能快速从海量日志中检索出匹配关键字的日志，其底层核心技术是倒排索引（Inverted Index）。</p><p></p><p>倒排索引是一种用于快速查找文档中包含特定单词或短语的数据结构，最早应用于信息检索领域。如下图所示，在数据写入时，倒排索引可以将每一行文本进行分词，变成一个个词（Term），然后构建词（Term） -&gt; 行号列表（Posting List） 的映射关系，将映射关系按照词进行排序存储。当需要查询某个词在哪些行出现的时候，先在 词 -&gt; 行号列表 的有序映射关系中查找词对应的行号列表，然后用行号列表中的行号去取出对应行的内容。这样的查询方式，可以避免遍历对每一行数据进行扫描和匹配，只需要访问包含查找词的行，在海量数据下性能有数量级的提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2a18fc7c80aeec93a01f308b8e44520.png\" /></p><p></p><p>图：倒排索引原理示意</p><p></p><p>倒排索引为 ES 带来快速检索能力的同时，也付出了写入速度吞吐低和存储空间占用高的代价——由于数据写入时倒排索引需要进行分词、词典排序、构建倒排表等 CPU 和内存密集型操作，导致写入吞吐大幅下降。而从存储成本角度考虑，ES 会存储原始数据和倒排索引，为了加速分析可能还需要额外<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1511\">存储</a>\"一份列存数据，因此 3 份冗余也会导致更高的存储空间占用。</p><p></p><p>Loki 则放弃了倒排索引，虽然带来来写入吞吐和存储空间的优势，但是损失了日志检索的用户体验，在关键时刻不能发挥快速查日志的作用。成本虽然有所降低，但是没有真正解决用户的问题。</p><p></p><h2>更高性价比的日志存储分析解决方案</h2><p></p><p>从以上方案对比可知，以 Elasticsearch 为代表的倒排索引检索架构以及以 Loki 为代表的轻量索引/无索引架构无法同时兼顾 高吞吐、低存储成本和实时高性能的要求，只能在某一方面或某几方面做权衡取舍。如果在保持倒排索引的文本检索性能优势的同时，大幅提升系统的写入速度与吞吐量并降低存储资源成本，是否日志场景所面临的困境就迎刃而解呢？答案是肯定的。</p><p></p><p>如果我们希望使用 Apache Doris 来更好解决日志存储与分析场景的痛点，其实现路径也非常清晰——在数据库内部增加倒排索引、以满足字符串类型的全文检索和普通数值/日期等类型的等值、范围检索，同时进一步优化倒排索引的查询性能、使其更加契合日志数据分析的场景需求。</p><p></p><p>在同样实现倒排索引的情况下，相较于 ES， <a href=\"https://www.infoq.cn/article/8SU754VFuKpw8gwjF24v\">Apache Doris </a>\"怎么做到更高的性能表现呢？或者说现有倒排索引的优化空间有哪些呢？</p><p></p><p>ES 基于 Apache Lucene 构建倒排索引，Apache Lucene 自 2000 年开源至今已有超过 20 年的历史，设计之初主要面向信息检索领域、功能丰富且复杂，而日志和大多数 OLAP 场景只需要其核心功能，包括分词、倒排表等，而相关度排序等并非强需求，因此存在进一步功能简化和性能提升的空间；ES 和 Apache Lucene 均采用 Java 实现，而 Apache Doris 存储引擎和执行引擎采用 C++ 开发并且实现了全面向量化，相对于 Java 实现具有更好的性能；倒排索引并不能决定性能表现的全部，作为一个高性能、实时的 OLAP 数据库，Apache Doris 的列式存储引擎、MPP 分布式查询框架、向量化执行引擎以及智能 CBO 查询优化器，相较于 ES 更为高效。</p><p></p><p>通过在 <a href=\"https://github.com/apache/doris/releases/tag/2.0.0-alpha1\">Apache Doris 2.0.0 最新版本</a>\"的探索与持续优化，在相同硬件配置和数据集的测试表现上，Apache Doris 在数据库内核实现高性能倒排索引后，相对于 ES 实现了日志数据写入速度提升 4 倍、存储空间降低 80%、查询性能提升 2 倍，再结合 Apache Doris 2.0.0 版本引入的冷热数据分离特性，整体性价比提升 10 倍以上！</p><p></p><p>接下来我们进一步介绍设计与实现细节。</p><p></p><h2>高性能倒排索引的设计与实现</h2><p></p><p>业界各类系统为了支持全文检索和任意列索引，往往有两种实现方式：一是通过外接索引系统来实现，原始数据存储在原系统中、索引存储在独立的索引系统中，两个系统通过数据的 ID 进行关联。数据写入时会同步写入到原系统和索引系统，索引系统构建索引后不存储完整数据只保留索引。查询时先从索引系统查出满足过滤条件的数据 ID 集合，然后用 ID 集合去原系统查原始数据。</p><p></p><p>这种架构的优势是实现简单，借力外部索引系统，对原有系统改动小。但是问题也很明显：</p><p></p><p>数据写入两个系统，异常有数据不一致的问题，也存在一定冗余存储；查询需在两个系统进行网络交互有额外开销，数据量大时用 ID 集合去原系统查性能比较低；维护两套系统的复杂度高，将系统的复杂性从开发测转移到运维测；</p><p></p><p>而另一种方式则是直接在系统中内置倒排索引，尽管技术难度更高，但性能更好、且无需花费额外的系统维护成本，对用户更加友好，这也是<a href=\"https://www.infoq.cn/article/8SU754VFuKpw8gwjF24v\"> Apache Doris</a>\" 所选择的方式。</p><p></p><h3>数据库内置倒排索引</h3><p></p><p>在选择了在数据库内核中内置倒排索引后，我们需要进一步对 Apache Doris 索引结构进行分析，判断能否通过在已有索引基础上进行拓展来实现。</p><p></p><p>Apache Doris 现有的索引存储在 Segment 文件的 Index Region 中，按照适用场景可以分为跳数索引和点查索引两类：</p><p></p><p>跳数索引：包括 ZoneMap 索引和 Bloom Filter 索引。ZoneMap 索引对每一个数据块和文件保存 Min/Max/isnull 等汇总信息，可以用于等值、范围查询的粗粒度过滤，只能排除不满足查询条件的数据块和文件，不能定位到行，也不支持文本分词。BloomFilter 索引也是数据块和文件级别的索引，通过 Bloom Filter 判断某个值是否在数据块和文件中，同样不能定位到行、不支持文本分词；点查索引：包括 ShortKey 前缀排序索引和 Bitmap 索引。ShortKey 在排序的基础上，根据给定的前缀列实现快速查询数据的索引方式，能够对前缀索引的列进行等值、范围查询，但不支持文本分词，另外由于数据要按前缀索引排序、因此一个表只允许一组前缀索引。Bitmap 索引记录数据值 -&gt; 行号 Bitmap 的有序映射，是一种很基础的倒排索引，但是索引结构比较简单、查询效率不高、不支持文本分词。</p><p></p><p>原有索引结构很难满足日志场景实时文本检索的需求，因此设计了全新的倒排索引。倒排索引在设计和实现上我们采取了无侵入的方式、不改变 Segment 数据文件格式，而是增加了新的 Inverted Index File，逻辑上在 Table 的 Column 级别。具体流程如下：</p><p></p><p>数据写入和 Compaction 阶段：在写 Segment 文件的同时，同步写入一个 Inverted Index 文件，文件路径由 Segment ID + Index ID 决定。写入 Segment 的 Row 和 Index 中的 Doc 一一对应，由于同步顺序写入，Segment 中的 Rowid 和 Index 中的 Docid 完全对应。查询阶段：如果查询 Where 条件中有建了倒排索引的列，会自动去 Index 文件中查询，返回满足条件的 Docid List，将 Docid List 一一对应的转成 Rowid Bitmap，然后走 Doris 通用的 Rowid 过滤机制只读取满足条件的行，达到查询加速的效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d8/d896d7c9d748d3c9edfd572d37dae108.png\" /></p><p></p><p>图：Doris倒排索引架构图</p><p></p><p>这个设计的好处是已有的数据文件无需修改，可以做到兼容升级，而且增减索引不影响<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1525\">数据</a>\"文件和其他索引，用户增建索引没有负担。</p><p></p><h3>通用倒排索引优化</h3><p></p><p></p><p>C++和向量化实现</p><p></p><p>Apache Doris 使用 <a href=\"https://clucene.sourceforge.net/\">CLucene</a>\" 作为底层的倒排索引库，CLucene 是一个用 C++ 实现的高性能、稳定的 Lucene 倒排索引库，它的功能比较完整，支持分词和自定义分词算法，支持全文检索查询和等值、范围查询。</p><p></p><p>Apache Doris 的存储模块和 CLucene 都用 C++ 实现，避免了Java Lucene 的 JVM GC 等开销，同样的计算 C++ 实现相对于 Java 性能优势明显，而且更利于做向量化加速。Doris 倒排索引进行了向量化优化，包括分词、倒排表构建、查询等，性能得到进一步提升。整体来看 Doris 的倒排索引写入速度可以超过单核 20MB/s，而 ES 的单核写入速度不到 5MB/s，有 4 倍的性能优势。</p><p></p><p>列式存储和压缩</p><p></p><p>Lucene 本身是文档存储模型，主数据采用行存，而 Doris 中不同列的倒排索引是相互独立的，因此倒排索引文件也采用列式存储，有利于向量化构建索引和提高压缩率。</p><p></p><p>采用压缩比高且速度快的 ZSTD，通常可以达到 5 ~10倍的压缩比，与常用的GZIP压缩相比有50%以上的空间节省且速度更快。</p><p></p><p>BKD 索引与 数值、日期类型 列优化</p><p></p><p>针对数值、日期类型的列，我们还实现了 BKD 索引，可以对范围查询提高性能，存储空间也相对于转成定长字符串更加高效，具有以下主要特性和优势：</p><p></p><p>高效范围查询：BKD 索引采用多维数据结构，为范围查询带来高效率。它能迅速定位数值或日期类型列中所需的数据范围，降低查询时间复杂度。存储空间优化：与其他索引方法相比，BKD 索引在存储空间使用上更高效。通过聚合并压缩相邻数据块，减少索引所需存储空间，降低存储成本。多维数据支持：BKD 索引具备良好扩展性，支持多维数据类型，如地理坐标（GEO point）和范围（Range），使其在处理复杂数据类型时具有高适应性。</p><p></p><p>此外，我们在原有 BKD 索引能力基础上进行了进一步拓展：</p><p></p><p>优化低基数场景：针对数值分布集中、单个数值倒排列表较多的低基数场景，我们调整了针对性的压缩算法，降低大量倒排表解压缩和反序列化所带来的CPU性能消耗。预查询技术：针对查询结果命中数较高的场景，我们采用预查询技术进行命中数预估。若命中数显著超过阈值，可跳过索引查询，直接利用Doris在大数据量查询下的技术优势进行数据过滤。</p><p></p><h3>面向 OLAP 的倒排索引优化</h3><p></p><p>日志存储和分析场景对检索的需求很简单，不需要特别复杂的功能（比如相关性排序），更需要降低存储成本和快速按照条件查出数据。因此，在面对海量数据的写入和查询时，Apache Doris 还针对 OLAP 数据库的特点优化了倒排索引的结构，使其更加简洁高效。例如：</p><p></p><p>在写入流程保证不会多个线程写入一个索引，从而避免写入时多线程锁竞争的开销；在存储结构上去掉了不必要的正排、norm 等文件，减少写入 IO 开销和存储空间占用；查询过程中简化相关性打分和排序逻辑，降低不必要的开销，提升查询性能。</p><p></p><p>针对日志等数据有按时间分区、历史数据访问频度低的特点，基于独立的索引文件设计，Apache Doris 还将在后续的版本中提供更细粒度、更灵活的索引管理功能：</p><p></p><p>指定分区构建倒排索引，比如新增一个索引的时候指定最近7天的日志构建索引，历史数据不建索引指定分区删除倒排索引，比如删除超过1个月的日志的索引，释放访问频度低的索引存储空间</p><p></p><h2>性能测试</h2><p></p><p>高性能是 Apache Doris 倒排索引设计和实现的首要出发点，我们通过公开的测试数据集分别与 ES 以及 Clickhouse 进行性能测试，测试效果如下：</p><p></p><h3>vs Elasticsearch</h3><p></p><p>我们采用了 ES 官方的性能测试 Benchmark esrally 并使用其中的 HTTP Logs 日志，在同样的硬件资源、数据、测试Case 以及测试工具下，记录并对比各自的数据写入时间、吞吐以及查询延迟。</p><p></p><p>测试数据：esrally HTTP Logs track 中自带测试数据集，1998 年 World Cup HTTP Server Logs，未压缩前 32G、共 2.47 亿行、单行平均长度 134 字节；测试查询：esrally HTTP Logs 测试关键词检索、范围查询、聚合、排序等 11 个 Query，所有查询跑 100 次串行执行；测试环境：3 台 16C 64G 云主机组成的集群。</p><p></p><p>在最终的测试结果中，Doris 写入速度是 ES 的 4.2 倍、达到 550 MB/s，写入后的数据压缩比接近 1:10、存储空间 节省 超 ****80% ，查询耗时下降 57%、查询性能是 ES 的 2.3 倍。加上冷热数据分离降低冷数据存储成本，整体相较 ES 实现 10倍以上的性价比提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c407b505d2a0ee3f999213c052e0a90.png\" /></p><p></p><h3>vs Clickhouse</h3><p></p><p>Clickhouse 近期的 v23.1 版本也引入了类似 Feature，将倒排索引作为实验性功能发布，因此我们同样进行了跟 Clickhouse 倒排索引的性能对比。在本次测试中，我们采用了 Clickhouse 官方 Inverted Index 介绍博客中使用的 Hacker News 样例数据以及查询 SQL ，同样保持相同的物理资源、数据、测试 Case 以及测试工具。</p><p></p><p>（参考文章：https://clickhouse.com/blog/clickhouse-search-with-inverted-indices）</p><p></p><p>测试数据：Hacker News 2873 万条数据，6.7G，Parquet 格式；测试查询：3 个查询，分别查询 'clickhouse'、'olap' OR 'oltp'、'avx' AND 'sve' 等关键字出现的次数；测试机器：1 台 16C 64G 云主机</p><p></p><p>在最终的测试结果中，3 个 SQL Apache Doris 的查询性能分别是 Clickhouse 的 4.7 倍、12.0 倍以及 18.5 倍，有明显的性能优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/3277efb20973d6cd5e1bd480244edb18.png\" /></p><p></p><h2>如何使用</h2><p></p><p>下面以一个 Hacker News 100 万条测试数据的示例展示 Doris 如何利用倒排索引实现高效的日志分析。</p><p></p><p>建表时指定索引INDEX idx_comment (comment) 指定对 comment 列建一个名为 idx_comment 的索引USING INVERTED 指定索引类型为倒排索引PROPERTIES(\"parser\" = \"english\") 指定分词类型为英文分词</p><p></p><p><code lang=\"text\">CREATE TABLE hackernews_1m\n(\n    `id` BIGINT,\n    `deleted` TINYINT,\n    `type` String,\n    `author` String,\n    `timestamp` DateTimeV2,\n    `comment` String,\n    `dead` TINYINT,\n    `parent` BIGINT,\n    `poll` BIGINT,\n    `children` Array,\n    `url` String,\n    `score` INT,\n    `title` String,\n    `parts` Array,\n    `descendants` INT,\n    INDEX idx_comment (`comment`) USING INVERTED PROPERTIES(\"parser\" = \"english\") COMMENT 'inverted index for comment'\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 10\nPROPERTIES (\"replication_num\" = \"1\");\n</code></p><p></p><p>注：对于已经存在的表，也可以通过 ADD INDEX idx_comment ON hackernews_1m(`comment`) USING INVERTED PROPERTIES(\"parser\" = \"english\") 来增加索引。值得一提的是，和 Doris 原先存储在 Segment 数据文件中的智能索引和二级索引相比，增加倒排索引的过程只会读 comment 列构建新的倒排索引文件，不会读写原有的其他数据，效率有明显提升。</p><p></p><p>导入数据后查询，使用 MATCH_ALL 在 comment 这一列上匹配 OLAP 和 OLTP 两个词，和 LIKE 扫描硬匹配相比，查询性能有十余倍的提升。（这仅是 100 万条数据下的测试效果，而随着数据量增大、性能提升越明显）</p><p></p><p><code lang=\"text\">mysql&gt; SELECT count() FROM hackernews_1m WHERE comment LIKE '%OLAP%' AND comment LIKE '%OLTP%';\n+---------+\n| count() |\n+---------+\n|      15 |\n+---------+\n1 row in set (0.13 sec)\n\nmysql&gt; SELECT count() FROM hackernews_1m WHERE comment MATCH_ALL 'OLAP OLTP';\n+---------+\n| count() |\n+---------+\n|      15 |\n+---------+\n1 row in set (0.01 sec)\n</code></p><p></p><p>更多详细功能介绍和测试步骤可以参考Apache Doris <a href=\"https://doris.apache.org/zh-CN/docs/dev/data-table/index/inverted-index\">倒排索引官方文档</a>\" 。</p><p></p><h2>总结</h2><p></p><p>通过内置高性能倒排索引，Apache Doris 对于字符串类型的全文检索和普通数值、日期等类型的等值、范围检索具有更高效的支持，进一步提升了数据查询的效率和准确性，对于大规模日志数据查询分析有了更好的性能表现，为需要检索能力的用户提供了更高性价比的选择。</p><p></p><p>目前倒排索引已经支持了 String、Int、Decimal、Datetime 等常用 Scalar 数据类型和 Array 数组类型，后续还会增加对 JSONB、Map 等复杂数据类型的支持。而 BKD 索引可以支持多维度类型的索引，为未来 Doris 增加 GEO 地理位置数据类型和索引打下了基础。与此同时 Apache Doris 在半结构化数据分析方面还有更多能力扩展，比如自动根据导入数据扩展表结构的 Dynamic Table、丰富的复杂数据类型（Array、Map、Struct、JSONB）以及高性能字符串匹配算法等。</p><p></p><p>除倒排索引以外，<a href=\"https://github.com/apache/doris/releases/tag/2.0.0-alpha1\">Apache Doris 在 2.0.0 Alpha 版本</a>\"中还实现了单节点数万 QPS 的高并发点查询能力、基于对象存储的冷热数据分离、基于代价模型的全新查询优化器以及 Pipeline 执行引擎等，欢迎大家下载体验。高并发点查询的详细介绍可以查看 SelectDB 技术团队过往发布的技术博客，其他功能的使用介绍请参考社区官方文档，同时也敬请持续关注我们后续发布的特性解读系列文章。</p><p></p><p>为了让用户可以体验社区开发的最新特性，同时保证最新功能可以收获到更广范围的使用反馈，我们建立了 2.0.0 版本的专项支持群，欢迎广大社区用户在使用最新版本过程中多多反馈使用意见，帮助 Apache Doris 持续改进，<a href=\"https://wenjuan.feishu.cn/m/cfm?t=sF2FZOL1KXKi-m73g\">通过此处填写申请加入专项支持群。</a>\"</p>",
    "publish_time": "2023-05-06 17:29:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "十年攻坚，清华大学、蚂蚁集团获吴文俊人工智能自然科学奖一等奖",
    "url": "https://www.infoq.cn/article/hZ9ayuEwk0Ef7lmI9D8x",
    "summary": "<p>5月6日，中国智能科学技术最高奖——“2022年度吴文俊人工智能科学技术奖”颁奖典礼在北京召开，百余位部委专家、国内外院士、顶尖学者、获奖代表和企业家出席。清华大学兴军亮研究员，蚂蚁集团李建树、赵闻飙等五人共同完成的“无约束人像目标智能感知与理解”成果获“自然科学奖一等奖”。</p><p>&nbsp;</p><p>“<a href=\"https://www.caai.cn/index.php?s=/Home/Article/index/id/65.html\">吴文俊人工智能科学技术奖</a>\"”由中国人工智能学会2011年设立，一等奖及以上具备提名推荐国家科学技术奖资格，被誉为<a href=\"http://www.wuwenjunkejijiang.cn/a/1881.html#:~:text=%E2%80%9C%E5%90%B4%E6%96%87%E4%BF%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A5%96%E2%80%9D%E7%94%B1%E4%B8%AD%E5%9B%BD%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%BC%9A%E5%8F%91%E8%B5%B7%E4%B8%BB%E5%8A%9E%EF%BC%8C%E6%98%AF%E6%88%91%E5%9B%BD%E6%99%BA%E8%83%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E9%A2%86%E5%9F%9F%E5%94%AF%E4%B8%80%E4%BB%A5%E4%BA%BA%E6%B0%91%E7%A7%91%E5%AD%A6%E5%AE%B6%E3%80%81%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%BC%80%E6%8B%93%E5%85%88%E9%A9%B1%E3%80%81%E6%88%91%E5%9B%BD%E6%99%BA%E8%83%BD%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%9A%84%E5%BC%80%E6%8B%93%E8%80%85%E5%92%8C%E9%A2%86%E5%86%9B%E4%BA%BA%E3%80%81%E9%A6%96%E5%B1%8A%E5%9B%BD%E5%AE%B6%E6%9C%80%E9%AB%98%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A5%96%E8%8E%B7%E5%BE%97%E8%80%85%E3%80%81%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E9%99%A2%E5%A3%AB%E3%80%81%E4%B8%AD%E5%9B%BD%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%BC%9A%E5%8E%9F%E5%90%8D%E8%AA%89%E7%90%86%E4%BA%8B%E9%95%BF%E5%90%B4%E6%96%87%E4%BF%8A%E5%85%88%E7%94%9F%E5%91%BD%E5%90%8D%EF%BC%8C%E4%BE%9D%E6%89%98%E7%A4%BE%E4%BC%9A%E5%8A%9B%E9%87%8F%E8%AE%BE%E7%AB%8B%E7%9A%84%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A5%96,%28%E5%9B%BD%E7%A7%91%E5%A5%96%E7%A4%BE%E8%AF%81%E5%AD%97%E7%AC%AC0218%E5%8F%B7%29%EF%BC%8C%E5%85%B7%E5%A4%87%E6%8F%90%E5%90%8D%E6%8E%A8%E8%8D%90%E5%9B%BD%E5%AE%B6%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A5%96%E8%B5%84%E6%A0%BC%EF%BC%8C%E8%A2%AB%E8%AA%89%E4%B8%BA%E2%80%9C%E4%B8%AD%E5%9B%BD%E6%99%BA%E8%83%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E6%9C%80%E9%AB%98%E5%A5%96%E2%80%9D%E3%80%82\">“中国智能科学技术最高奖”</a>\"。其中，“自然科学奖一等奖”限定颁给实现前瞻性基础研究、引领性原创成果重大突破，并被国内外科学界公认和广泛引用，推动学科发展或对经济社会有重大影响的成果完成人。</p><p>&nbsp;</p><p>此次，清华大学与蚂蚁集团历时十余年攻坚完成的“无约束人像目标智能感知与理解”获得“自然科学奖一等奖”，意味着我国在解决AI视觉领域核心难题上取得了重大创新突破。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4aa95e92fa94fdf3002303f161ae95f.png\" /></p><p>（5月6日，2022年度吴文俊人工智能科学技术奖颁奖典礼现场，清华大学兴军亮，蚂蚁集团李建树等人共同完成的“无约束人像目标智能感知与理解”成果获“自然科学奖一等奖”）</p><p>&nbsp;</p><p>人工智能已成为新一轮科技革命和产业变革重要引擎。计算机视觉作为人工智能行业重要组成部分，通过模拟人类视觉系统赋予机器“看”和“认知”能力，是实现工业数字化智能化的关键性技术。人像目标则是计算机视觉领域重要的研究内容，在金融认证、民生经济等场景拥有广泛应用需求。然而，受人像内在多变因素（如姿态、表情、性别等）及外部复杂环境因素（如视角、光照、噪声等）等影响，现有人像目标感知理解模型在精度和效率上存在极大挑战。</p><p>&nbsp;</p><p>针对这一复杂科学难题，该项目经过十多年努力，形成了较为完整的人像目标智能感知理解的基础理论方法和核心算法技术研究体系。该项目主要有三大科学发现：</p><p></p><p>一是针对人像表观和形状的多变呈现，揭示了“不变性特征学习”的重要性，形成了全视角人像关键信息感知建模理论框架；</p><p>二是针对复杂空间下人像内在关联属性，发现了人像的姿态、表情等多维属性间普遍存在关联，提出了“联合多任务和对抗不变性学习”的属性挖掘方法；</p><p>三是结合人像关键信息和关联属性，构建了面向“人脸-人体-人群”的高适用性机器视觉智能技术应用框架，可解决实际应用场景中的多属性融合感知等难题。</p><p>&nbsp;</p><p>据了解，该项目在国家自然科学基金、“新一代人工智能”重大项目支持下，在学术研究、技术影响、服务产业等方面均取得丰硕成果。已累计获得CVPR、ACM Multimedia等5次重要论文奖，在国内外顶级人像感知理解赛事中10次夺冠；8篇代表作论文Google-Scholar总引用3225 次、WOS他引650次，并获得十余位马尔奖/傅京孙奖得主、百余位国内外院士等知名学者引用并正面评价。</p><p>&nbsp;</p><p>该项目相关成果已在欧姆龙公司、华为、蚂蚁集团等企业核心产品中得到应用，并产生规模化商业应用。基于该项目核心能力沉淀的蚂蚁集团可信数字身份认证方案已在海外服务超1.2亿东南亚用户，荣获IDC金融科技真实价值奖。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4adce36dcbab9eb86d4970b35357f89c.png\" /></p><p></p><p></p>",
    "publish_time": "2023-05-06 18:13:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]