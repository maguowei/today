[
  {
    "title": "微软Bing Chat全面开放，所有人可用！官宣多项重大升级，日活用户超过1亿",
    "url": "https://www.infoq.cn/article/ZmDbgaSV7PmYEG3MwQAx",
    "summary": "<p>当地时间 5 月 4 日，根据 The Verge 报道，微软宣布公司旗下基于 ChatGPT-4 的 Bing Chat 功能已进入开放预览（Open Preview）模式，也就是面向所有用户开放。</p><p></p><p>在此之前，如果用户想测试微软的 Bing GPT-4 聊天机器人，则必须加入等候名单。现在大家只需要前往 Bing 或 Edge 并使用 Microsoft 帐户登录即可。</p><p></p><h2>微软 Bing Chat 正式开放</h2><p></p><p></p><p>微软首席营销官优素福·迈赫迪（Yusuf Mehdi）表示，自推出以来，基于 GPT-4 的 Bing AI 已经提供了 5 亿次聊天，并且有 2 亿人使用该平台生成 AI 图像，日活已超过 1 亿。</p><p></p><p>Mehdi 还称，大约 70% 尝试新 Bing 聊天功能的用户， 都会用来进行搜索相关的任务。聊天这种人类最自然的交互，正实实在在改变着搜索方式。</p><p></p><p>微软补充说，将继续监测 Bing 聊天的使用情况，以确保其 AI 工具的合理使用：“我们与 OpenAI 的合作伙伴一起，根据我们在预览中学习和看到的情况，继续实施防御有害内容的保障措施。”</p><p></p><p>其实今年 3 月份的时候，就有用户发现微软取消了候补名单功能，意味着只需注册一个新的 Bing 账户就能立即解锁 Bing Chat。不过当时微软没有做出正面回应，本次则是微软第一次官方宣布全面对外开放 Bing Chat。</p><p></p><p>除了正式宣布全面对外开放以外，Bing Chat 还上线了多个新功能。</p><p></p><h2>BingChat 迎来多项重大升级</h2><p></p><p></p><p>微软表示，新的 BingChat 在餐厅预订、图像结果等方面变得更加智能。</p><p></p><p>比如，新版 Bing Chat 中增加了图像和视频答案、餐厅预订、聊天记录以及一些更智能的 Microsoft Edge 集成。</p><p></p><p>首先，在插件功能上，新版 Bing Chat 将允许第三方参与 Bing Chat AI 的结果。微软正在与 OpenTable 合作，将餐厅预订纳入到 AI 响应中。</p><p></p><p>早在今年 3 月份，OpenAI 就已经为 ChatGPT 加入了插件功能，加入了插件功能后的 ChatGPT 将能够与其它第三方应用程序连通起来，直接通过与 ChatGPT 的对话用户就可以实现点餐、购物等此前需要跳转多个站点才能完成的操作。</p><p></p><p>增加了插件功能后，如果 Bing Chat 搜索结果推荐了一家餐厅，它可以智能地找到适合的预订时间，并帮助用户在聊天界面中预订。</p><p></p><p>其次，现在 Bing Chat 除了能做普通的文字交流问答、编程学习外，还能帮助用户“生成图片” 甚至是视频等，而且以文生图的功能还支持了中文在内的多种语言。</p><p></p><p>此外，微软还在 Bing Chat 中添加了一项备受期待的功能：历史记录。这个新的历史聊天记录功能可以让用户跨设备获取聊天机器人对话，甚至可以将 Bing Chat 用作研究工具。微软还计划在 Bing Chat 中添加导出和共享功能，以便用户可以在 Twitter 上共享对话内容，甚至可以将其导入 Word 文档。</p><p>聊天记录真正有趣的地方是在 Microsoft Edge 中。如果用户在 Edge 中打开 Bing Chat 答案的链接，它会自动将该聊天移至侧边栏，以便用户可以在浏览网站时继续提问。微软还在尝试通过将以前的聊天记录中的上下文引入新的对话来个性化这些聊天会话。</p><p></p><h2>负责任的 AI 需要接受公众的检验</h2><p></p><p></p><p>微软和其他竞争对手竞相押注 AI 聊天机器人掀起了全球 AI 热潮，但 AI 带来的安全问题同样不容忽视。</p><p>此次官宣 Bing Chat 公开后，微软并没有花太多时间谈论快速采用 AI 的潜在陷阱，但 Mehdi 确实解释了为什么微软基本上一直在与公众一起测试 Bing AI。</p><p></p><p></p><blockquote>“我们相信，以正确和负责任的方式将这项技术推向市场就是要公开测试，就是要让人们使用它并获得反馈”。</blockquote><p></p><p></p><p>“我知道有很多人在谈论，‘嘿，你的速度是多少，太快了，还是太慢了？’ 在我们看来……这前提必须以负责任的 AI 原则和方法为基础，我们微软在负责任 AI 这件事上已经做了很多年，现在我们必须走出去，必须测试它并获得反馈，”Mehdi 说道。</p><p></p><p>曾参与开发 Bing AI 聊天界面外观设计的微软设计副总裁 Liz Danzico 表示，体验 GPT-4 像是经历了另一个技术时代。</p><p></p><p>Danzico 表示他了解人们对 AI 的担忧，人们要确保不能让 AI 走得太远或越界。</p><p></p><p>在 Bing AI 聊天的开发过程中，Danzico 的团队与微软的负责任的 AI 团队密切合作，以确保负责任的 AI 原则能贯穿于整个产品中。她补充说，微软明白“如今的 Bing Chat 还一个不完善的系统，我们仍在学习中。”</p><p></p><p>参考链接：</p><p>https://www.theverge.com/2023/5/4/23710022/microsoft-bing-chatbot-ai-image-video-chat-history-features</p><p></p><p>https://www.techradar.com/news/bing-ai-chat-is-now-waitlist-free-and-about-to-get-even-more-powerful</p>",
    "publish_time": "2023-05-06 09:07:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "CentOS 迁移首选龙蜥，更快更稳更安全！",
    "url": "https://www.infoq.cn/article/fr61MHJEQ82Mri3sELC1",
    "summary": "<p>CentOS 全面停服在即，为妥善应对 CentOS 停服带来的数据泄露和安全风险，龙蜥社区秉承 CentOS 迁移“三不”原则，不盲目，不折腾，不短择，为广大用户提供更方便更安全的操作系统迁移服务。CentOS 迁移首选龙蜥，更快更稳更安全！</p>",
    "publish_time": "2023-05-06 14:12:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "第四范式开源强化学习研究通用框架，支持单智能体、多智能体训练，还可训练自然语言任务！训练速度提升17%",
    "url": "https://www.infoq.cn/article/AKQkaQil4TApHS88uVEL",
    "summary": "<p>OpenRL 是由第四范式强化学习团队开发的基于 PyTorch 的强化学习研究框架，支持单智能体、多智能体、自然语言等多种任务的训练。OpenRL 基于 PyTorch 进行开发，目标是为强化学习研究社区提供一个简单易用、灵活高效、可持续扩展的平台。目前，OpenRL 支持的特性包括：</p><p></p><p>●简单易用且支持单智能体、多智能体训练的通用接口</p><p>●支持自然语言任务（如对话任务）的强化学习训练</p><p>●支持从 Hugging Face 上导入模型和数据</p><p>●支持 LSTM，GRU，Transformer 等模型</p><p>●支持多种训练加速，例如：自动混合精度训练，半精度策略网络收集数据等</p><p>●支持用户自定义训练模型、奖励模型、训练数据以及环境</p><p>●支持 gymnasium 环境</p><p>●支持字典观测空间</p><p>●支持 wandb，tensorboardX 等主流训练可视化工具</p><p>●支持环境的串行和并行训练，同时保证两种模式下的训练效果一致</p><p>●中英文文档</p><p>●提供单元测试和代码覆盖测试</p><p>●符合 Black Code Style 和类型检查</p><p></p><p>目前，OpenRL 已经在 GitHub 开源：https://github.com/OpenRL-Lab/openrl</p><p></p><p></p><h2>OpenRL 初体验</h2><p></p><p></p><p>OpenRL 目前可以通过 pip 进行安装：</p><p><code lang=\"nginx\">pip install openrl</code></p><p></p><p>也可以通过 conda 安装：</p><p></p><p><code lang=\"nginx\">conda install -c openrl openrl</code></p><p></p><p>OpenRL 为强化学习入门用户提供了简单易用的接口， 下面是一个使用 PPO 算法训练 CartPole 环境的例子：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nenv = make(\"CartPole-v1\", env_num=9) # 创建环境，并设置环境并行数为9\nnet = Net(env) # 创建神经网络\nagent = Agent(net) # 初始化智能体\nagent.train(total_time_steps=20000) # 开始训练，并设置环境运行总步数为20000</code></p><p></p><p>使用 OpenRL 训练智能体只需要简单的四步：创建环境 =&gt; 初始化模型 =&gt; 初始化智能体 =&gt; 开始训练！</p><p></p><p>在普通笔记本电脑上执行以上代码，只需要几秒钟，便可以完成该智能体的训练：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/05/05e1ddaf02fa30d1e3019f9637cb9454.gif\" /></p><p></p><p>此外，对于多智能体、自然语言等任务的训练，OpenRL 也提供了同样简单易用的接口。例如，对于多智能体任务中的 MPE 环境，OpenRL 也只需要调用几行代码便可以完成训练：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\ndef train():\n    # 创建 MPE 环境，使用异步环境，即每个智能体独立运行\n    env = make(\n        \"simple_spread\",\n        env_num=100,\n        asynchronous=True,\n    )\n    # 创建 神经网络，使用GPU进行训练\n    net = Net(env, device=\"cuda\")\n    agent = Agent(net) # 初始化训练器\n    # 开始训练\n    agent.train(total_time_steps=5000000)\n    # 保存训练完成的智能体\n    agent.save(\"./ppo_agent/\")\nif __name__ == \"__main__\":\n    train()</code></p><p></p><p>下图展示了通过 OpenRL 训练前后智能体的表现：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/32/32b5041e84110f7be683520425ade82d.gif\" /></p><p></p><p></p><h2>加载配置文件</h2><p></p><p></p><p>此外，OpenRL 还同时支持从命令行和配置文件对训练参数进行修改。比如，用户可以通过执行 python train_ppo.py --lr 5e-4 来快速修改训练时候的学习率。</p><p></p><p>当配置参数非常多的时候，OpenRL 还支持用户编写自己的配置文件来修改训练参数。例如，用户可以自行创建以下配置文件 (mpe_ppo.yaml)，并修改其中的参数：</p><p></p><p><code lang=\"bash\"># mpe_ppo.yaml\nseed: 0 # 设置seed，保证每次实验结果一致\nlr: 7e-4 # 设置学习率\nepisode_length: 25 # 设置每个episode的长度\nuse_recurrent_policy: true # 设置是否使用RNN\nuse_joint_action_loss: true # 设置是否使用JRPO算法\nuse_valuenorm: true # 设置是否使用value normalization</code></p><p></p><p>最后，用户只需要在执行程序的时候指定该配置文件即可：</p><p></p><p><code lang=\"css\">python train_ppo.py --config mpe_ppo.yaml</code></p><p></p><p></p><h2>训练与测试可视化</h2><p></p><p></p><p>此外，通过 OpenRL，用户还可以方便地使用 wandb 来可视化训练过程：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ce/ce48f0a69a9fac0a4bfe103eaedf35c6.png\" /></p><p></p><p>OpenRL 还提供了各种环境可视化的接口，方便用户对并行环境进行可视化。用户可以在创建并行环境的时候设置环境的渲染模式为\"group_human\"，便可以同时对多个并行环境进行可视化：</p><p></p><p><code lang=\"ini\">env = make(\"simple_spread\", env_num=9, render_mode=\"group_human\")</code></p><p></p><p>此外，用户还可以通过引入 GIFWrapper 来把环境运行过程保存为 gif 动画：</p><p></p><p><code lang=\"javascript\">from openrl.envs.wrappers import GIFWrapper\nenv = GIFWrapper(env, \"test_simple_spread.gif\")</code></p><p></p><p></p><h2>智能体的保存和加载</h2><p></p><p></p><p>OpenRL 提供 agent.save() 和 agent.load() 接口来保存和加载训练好的智能体，并通过 agent.act() 接口来获取测试时的智能体动作：</p><p></p><p><code lang=\"python\"># test_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nfrom openrl.envs.wrappers import GIFWrapper # 用于生成gif\ndef test():\n    # 创建 MPE 环境\n    env = make( \"simple_spread\", env_num=4)\n    # 使用GIFWrapper，用于生成gif\n    env = GIFWrapper(env, \"test_simple_spread.gif\")\n    agent = Agent(Net(env)) # 创建 智能体\n    # 保存智能体\n    agent.save(\"./ppo_agent/\")    \n    # 加载智能体\n    agent.load('./ppo_agent/')\n    # 开始测试\n    obs, _ = env.reset()\n    while True:\n        # 智能体根据 observation 预测下一个动作\n        action, _ = agent.act(obs)\n        obs, r, done, info = env.step(action)\n        if done.any():\n            break\n    env.close()\nif __name__ == \"__main__\":\n    test()</code></p><p></p><p>执行该测试代码，便可以在同级目录下找到保存好的环境运行动画文件 (test_simple_spread.gif)：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/59/5948ac804c46b8d10d3d0460e718e4b2.gif\" /></p><p></p><p></p><h2>训练自然语言对话任务</h2><p></p><p></p><p>最近的研究表明，强化学习也可以用于训练语言模型， 并且能显著提升模型的性能。目前，OpenRL 已经支持自然语言对话任务的强化学习训练。OpenRL 通过模块化设计，支持用户 加载自己的数据集 ， 自定义训练模型， 自定义奖励模型， 自定义 wandb 信息输出 以及 一键开启混合精度训练 等。</p><p></p><p>对于对话任务训练，OpenRL 提供了同样简单易用的训练接口：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nfrom openrl.configs.config import create_config_parser\ndef train():\n    # 添加读取配置文件的代码\n    cfg_parser = create_config_parser()\n    cfg = cfg_parser.parse_args()\n    # 创建 NLP 环境\n    env = make(\"daily_dialog\",env_num=2,asynchronous=True,cfg=cfg,)\n    net = Net(env, cfg=cfg, device=\"cuda\")\n    agent = Agent(net)\n    agent.train(total_time_steps=5000000)\nif __name__ == \"__main__\":\n    train()</code></p><p></p><p>可以看出，OpenRL 训练对话任务和其他强化学习任务一样，都是通过创建交互环境的方式进行训练。</p><p></p><p></p><h2>加载自定义数据集</h2><p></p><p></p><p>训练对话任务，需要对话数据集。这里我们可以使用 Hugging Face 上的公开数据集（用户可以替换成自己的数据集）。加载数据集，只需要在配置文件中传入数据集的名称或者路径即可：</p><p></p><p><code lang=\"bash\"># nlp_ppo.yaml\ndata_path: daily_dialog # 数据集路径\nenv: # 环境所用到的参数\n    args: {'tokenizer_path': 'gpt2'} # 读取tokenizer的路径\nseed: 0 # 设置seed，保证每次实验结果一致\nlr: 1e-6 # 设置policy模型的学习率\ncritic_lr: 1e-6 # 设置critic模型的学习率\nepisode_length: 20 # 设置每个episode的长度\nuse_recurrent_policy: true</code></p><p></p><p>上述配置文件中的 data_path 可以设置为 Hugging Face 数据集名称 或者 本地数据集路径。此外，环境参数中的 tokenizer_path 用于指定加载文字编码器的 Hugging Face 名称 或者 本地路径。</p><p></p><p></p><h2>自定义训练模型</h2><p></p><p></p><p>在 OpenRL 中，我们可以使用 Hugging Face 上的模型来进行训练。为了加载 Hugging Face 上的模型，我们首先需要在配置文件 nlp_ppo.yaml 中添加以下内容：</p><p></p><p><code lang=\"cs\"># nlp_ppo.yaml\n# 预训练模型路径\nmodel_path: rajkumarrrk/gpt2-fine-tuned-on-daily-dialog \nuse_share_model: true # 策略网络和价值网络是否共享模型\nppo_epoch: 5 # ppo训练迭代次数\n\ndata_path: daily_dialog # 数据集名称或者路径\nenv: # 环境所用到的参数\n    args: {'tokenizer_path': 'gpt2'} # 读取tokenizer的路径\nlr: 1e-6 # 设置policy模型的学习率\ncritic_lr: 1e-6 # 设置critic模型的学习率\nepisode_length: 128 # 设置每个episode的长度\nnum_mini_batch: 20</code></p><p></p><p>然后在 train_ppo.py 中添加以下代码：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.envs.common import make\nfrom openrl.modules.common import PPONet as Net\nfrom openrl.runners.common import PPOAgent as Agent\nfrom openrl.configs.config import create_config_parser\nfrom openrl.modules.networks.policy_value_network_gpt import (\n    PolicyValueNetworkGPT as PolicyValueNetwork,\n)\ndef train():\n    # 添加读取配置文件的代码\n    cfg_parser = create_config_parser()\n    cfg = cfg_parser.parse_args()\n    # 创建 NLP 环境\n    env = make(\"daily_dialog\",env_num=2,asynchronous=True,cfg=cfg,)\n    # 创建自定义神经网络\n    model_dict = {\"model\": PolicyValueNetwork}\n    net = Net(env, cfg=cfg, model_dict=model_dict)\n    # 创建训练智能体\n    agent = Agent(net)\n    agent.train(total_time_steps=5000000)\nif __name__ == \"__main__\":\n    train()</code></p><p></p><p>通过以上简单几行的修改，用户便可以使用 Hugging Face 上的预训练模型进行训练。如果用户希望分别自定义策略网络和价值网络，可以写好 CustomPolicyNetwork 以及 CustomValueNetwork 后通过以下方式从外部传入训练网络：</p><p></p><p><code lang=\"makefile\">model_dict = {\n    \"policy\": CustomPolicyNetwork,\n    \"critic\": CustomValueNetwork,\n}\nnet = Net(env, model_dict=model_dict)</code></p><p></p><p></p><h2>自定义奖励模型</h2><p></p><p></p><p>通常，自然语言任务的数据集中并不包含奖励信息。因此，如果需要使用强化学习来训练自然语言任务，就需要使用额外的奖励模型来生成奖励。在该对话任务中，我们可以使用一个复合的奖励模型，它包含以下三个部分：</p><p></p><p>●意图奖励：即当智能体生成的语句和期望的意图接近时，智能体便可以获得更高的奖励。</p><p></p><p>●METEOR 指标奖励：METEOR 是一个用于评估文本生成质量的指标，它可以用来衡量生成的语句和期望的语句的相似程度。我们把这个指标作为奖励反馈给智能体，以达到优化生成的语句的效果。</p><p></p><p>●KL 散度奖励：该奖励用来限制智能体生成的文本偏离预训练模型的程度，防止出现 reward hacking 的问题。</p><p></p><p>我们最终的奖励为以上三个奖励的加权和，其中 KL 散度奖励 的系数是随着 KL 散度的大小动态变化的。想在 OpenRL 中使用该奖励模型，用户无需修改训练代码，只需要在 nlp_ppo.yaml 文件中添加 reward_class 参数即可：</p><p></p><p><code lang=\"cs\"># nlp_ppo.yaml\nreward_class:\n    id: NLPReward # 奖励模型名称\n    args: {\n        # 用于意图判断的模型的名称或路径\n        \"intent_model\": rajkumarrrk/roberta-daily-dialog-intent-classifier,\n        # 用于计算KL散度的预训练模型的名称或路径\n        \"ref_model\": roberta-base, # 用于意图判断的tokenizer的名称或路径\n    }</code></p><p></p><p>OpenRL 支持用户使用自定义的奖励模型。首先，用户需要编写自定义奖励模型 (需要继承 BaseReward 类)。接着，用户需要注册自定义的奖励模型，即在 train_ppo.py 添加以下代码：</p><p></p><p><code lang=\"python\"># train_ppo.py\nfrom openrl.rewards.nlp_reward import CustomReward\nfrom openrl.rewards import RewardFactory\nRewardFactory.register(\"CustomReward\", CustomReward)</code></p><p></p><p>最后，用户只需要在配置文件中填写自定义的奖励模型即可：</p><p></p><p><code lang=\"makefile\">reward_class:\n    id: \"CustomReward\" # 自定义奖励模型名称\n    args: {} # 用户自定义奖励函数可能用到的参数</code></p><p></p><p></p><h2>自定义训练过程信息输出</h2><p></p><p></p><p>OpenRL 还支持用户自定义 wandb 和 tensorboard 的输出内容。例如，在该任务的训练过程中，我们还需要输出各种类型奖励的信息和 KL 散度系数的信息， 用户可以在 nlp_ppo.yaml 文件中加入 vec_info_class 参数来实现:</p><p></p><p><code lang=\"makefile\"># nlp_ppo.yaml\nvec_info_class:\n    id: \"NLPVecInfo\" # 调用NLPVecInfo类以打印NLP任务中奖励函数的信息\n#设置wandb信息\nwandb_entity: openrl # 这里用于指定wandb团队名称，请把openrl替换为你自己的团队名称\nexperiment_name: train_nlp # 这里用于指定实验名称\nrun_dir: ./run_results/ # 这里用于指定实验数据保存的路径\nlog_interval: 1 # 这里用于指定每隔多少个episode上传一次wandb数据\n# 自行填写其他参数...</code></p><p></p><p>修改完配置文件后，在 train_ppo.py 文件中启用 wandb:</p><p></p><p><code lang=\"php\"># train_ppo.py\nagent.train(total_time_steps=100000, use_wandb=True)</code></p><p></p><p>然后执行 python train_ppo.py –config nlp_ppo.yaml，过一会儿，便可以在 wandb 中看到如下的输出:</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0e/0e635e370fd9fe8e7eb0e0b98b4ef05b.png\" /></p><p></p><p>从上图可以看到，wandb 输出了各种类型奖励的信息和 KL 散度系数的信息。</p><p></p><p>如果用户还需要输出其他信息，还可以参考 NLPVecInfo 类 和 VecInfo 类来实现自己的 CustomVecInfo 类。然后，需要在 train_ppo.py 中注册自定义的 CustomVecInfo 类:</p><p></p><p><code lang=\"apache\"># train_ppo.py \n# 注册自定义输出信息类 \nVecInfoFactory.register(\"CustomVecInfo\", CustomVecInfo)</code></p><p></p><p>最后，只需要在 nlp_ppo.yaml 中填写 CustomVecInfo 类即可启用：</p><p></p><p><code lang=\"makefile\"># nlp_ppo.yaml\nvec_info_class:\n    id: \"CustomVecInfo\" # 调用自定义CustomVecInfo类以输出自定义信息</code></p><p></p><p></p><h2>使用混合精度训练加速</h2><p></p><p></p><p>OpenRL 还提供了一键开启混合精度训练的功能。用户只需要在配置文件中加入以下参数即可：</p><p></p><p><code lang=\"bash\"># nlp_ppo.yaml\nuse_amp: true # 开启混合精度训练</code></p><p></p><p></p><h2>对比评测</h2><p></p><p></p><p>下表格展示了使用 OpenRL 训练该对话任务的结果。结果显示使用强化学习训练后，模型各项指标皆有所提升。另外，从下表可以看出，相较于 RL4LMs ， OpenRL 的训练速度更快（在同样 3090 显卡的机器上，速度提升 17% ），最终的性能指标也更好：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/02/02723b98d2f0cdc13fd99b2defa74af1.png\" /></p><p></p><p>最后，对于训练好的智能体，用户可以方便地通过 agent.chat() 接口进行对话：</p><p></p><p><code lang=\"python\"># chat.py\nfrom openrl.runners.common import ChatAgent as Agent\ndef chat():\n    agent = Agent.load(\"./ppo_agent\", tokenizer=\"gpt2\",)\n    history = []\n    print(\"Welcome to OpenRL!\")\n    while True:\n        input_text = input(\"&gt; User: \")\n        if input_text == \"quit\":\n            break\n        elif input_text == \"reset\":\n            history = []\n            print(\"Welcome to OpenRL!\")\n            continue\n        response = agent.chat(input_text, history)\n        print(f\"&gt; OpenRL Agent: {response}\")\n        history.append(input_text)\n        history.append(response)\nif __name__ == \"__main__\":\n    chat()</code></p><p></p><p>执行 python chat.py ，便可以和训练好的智能体进行对话了：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/be/bec73e55d2f1076848ead8d6cf7b1e90.gif\" /></p><p></p><p></p><h2>总结</h2><p></p><p></p><p>OpenRL 框架经过了 OpenRL-Lab 的多次迭代并应用于学术研究和 AI 竞赛，目前已经成为了一个较为成熟的强化学习框架。OpenRL-Lab 团队将持续维护和更新 OpenRL，欢迎大家加入我们的开源社区，一起为强化学习的发展做出贡献。更多关于 OpenRL 的信息，可以参考:</p><p></p><p>●OpenRL 官方仓库：<a href=\"https://github.com/OpenRL-Lab/openrl/\">https://github.com/OpenRL-Lab/openrl/</a>\"</p><p></p><p>●OpenRL 中文文档：<a href=\"https://openrl-docs.readthedocs.io/zh/latest/\">https://openrl-docs.readthedocs.io/zh/latest/</a>\"</p><p></p><p></p><h2>致谢</h2><p></p><p></p><p>OpenRL 框架的开发吸取了其他强化学习框架的优点：</p><p></p><p>●Stable-baselines3: <a href=\"https://github.com/DLR-RM/stable-baselines3\">https://github.com/DLR-RM/stable-baselines3</a>\"</p><p></p><p>●pytorch-a2c-ppo-acktr-gail: <a href=\"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail</a>\"</p><p></p><p>●MAPPO: <a href=\"https://github.com/marlbenchmark/on-policy\">https://github.com/marlbenchmark/on-policy</a>\"</p><p></p><p>●Gymnasium: <a href=\"https://github.com/Farama-Foundation/Gymnasium\">https://github.com/Farama-Foundation/Gymnasium</a>\"</p><p></p><p>●DI-engine: <a href=\"https://github.com/opendilab/DI-engine/\">https://github.com/opendilab/DI-engine/</a>\"</p><p></p><p>●Tianshou: <a href=\"https://github.com/thu-ml/tianshou\">https://github.com/thu-ml/tianshou</a>\"</p><p></p><p>●RL4LMs: <a href=\"https://github.com/allenai/RL4LMs\">https://github.com/allenai/RL4LMs</a>\"</p><p></p><p></p><h2>未来工作</h2><p></p><p></p><p>目前，OpenRL 还处于持续开发和建设阶段，未来 OpenRL 将会开源更多功能：</p><p></p><p>●支持智能体自博弈训练</p><p></p><p>●加入离线强化学习、模范学习、逆强化学习算法</p><p></p><p>●加入更多强化学习环境和算法</p><p></p><p>●集成 Deepspeed 等加速框架</p><p></p><p>●支持多机分布式训练</p><p></p><p></p><h2>OpenRL Lab 团队</h2><p></p><p></p><p>OpenRL 框架是由 OpenRL Lab 团队开发，该团队是第四范式公司旗下的强化学习研究团队。第四范式长期致力于强化学习的研发和工业应用。为了促进强化学习的产学研一体化，第四范式成立了 OpenRL Lab 研究团队，目标是先进技术开源和人工智能前沿探索。成立不到一年，OpenRL Lab 团队已经在 AAMAS 发表过三篇论文，参加谷歌足球游戏 11 vs 11 比赛并获得第三的成绩。团队提出的 TiZero 智能体，实现了首个从零开始，通过课程学习、分布式强化学习、自博弈等技术完成谷歌足球全场游戏智能体的训练：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e1/e1c675405a724d499c289c4b8078db6b.png\" /></p><p></p><p>截止 2022 年 10 月 28 日，Tizero 在及第评测平台上排名第一：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/1a/1afbca2eb2cc7ed8663ee9d5353f516b.png\" /></p><p></p><p></p><h2>作者介绍</h2><p></p><p></p><p>黄世宇，第四范式强化学习研究员。博士毕业于清华大学计算机系，博士导师是朱军和陈挺教授，本科期间在 CMU 交换，导师为 Deva Ramanan 教授。主要研究方向为强化学习，多智能体强化学习，分布式强化学习。曾在腾讯 AI Lab、华为诺亚、商汤、RealAI 工作。</p>",
    "publish_time": "2023-05-06 14:48:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "得物App全栈可观测平台落地实践",
    "url": "https://www.infoq.cn/article/lUNazGtLiTLBAg9S714b",
    "summary": "<p>得物初版以资讯类App在2015年上线，为了帮助年轻人了解潮流文化资讯，其中创造性的引入了“先鉴别、后发货”的方式解决在交易过程中遇到的问题，最近几年逐步从球鞋品类把它过渡到服装、箱包、化妆品等，到今天为止得物App它就不是一个简单的是球鞋交易平台，而是一个全品类综合性的电商平台。随着业务规模和复杂度的增加，监控的规模也随之扩大。</p><p></p><p>在4月22日ArchSummit（上海站）架构师峰会上，得物App监控平台架构师李尊和大家分享了得物在可观测性领域的一些工作成果。（7月21-22日<a href=\"https://archsummit.infoq.cn/2023/shenzhen\">深圳站ArchSummit会议</a>\"在筹备中，欢迎点击“阅读原文”关注）文末有彩蛋。</p><p></p><p>监控系统发展史</p><p></p><p>得物的技术栈主要是Java和Go，在2021年前主要选择当时业界比较流行的监控开源产品，像日志选择Loki，指标基于Prometheus生态，应用上报到注册中心后可实现自动化采集，存储选择VictoriaMetrics，Trace基于Jaeger，采用组合采样策略，头部采样和每分钟固定采样，最终采样率在3%左右，大盘基于Grafana构建。</p><p><img src=\"https://static001.infoq.cn/resource/image/58/51/58d23be01d214e6a5da061ae5c799751.png\" /></p><p></p><p>2021年我们开始制定标准进行监控治理，结合排查经验，引入应用监控，提供异常分析、慢MySQL统计和Redis热点分析，包括命中率、大Key。</p><p></p><p>2022年针对监控各个领域专项做深度迭代和打磨，比如 Trace，解决采样率低问题， 全量上报后通过尾部采样解决冷热存储问题以及引入OSS将存储成本降低90%。另一方面在数据联动上指标和Trace缺乏强关联性，引入OT打造端到端的链路追踪。在标准协议下实现指标关联Trace，Trace关联日志，在产品侧实现下钻分析。产品层开始摆脱Grafana的依赖，图表基于antv构建。</p><p><img src=\"https://static001.infoq.cn/resource/image/34/f7/347325f7b127c4e24f18ee3499d582f7.png\" /></p><p></p><p>整个2022年我们在数据质量方面和架构方面做了一个巨大的调整，主要是为了我们在2023年建设全息监控做准备，第一基于应用的这套体系快速的覆盖到中间件，包括K8s，实现指标的标准化，大盘的标准化，告警的标准化。再就是我们要以应用为中心，搭建一个可观性平台，通过以应用为中心构建一个元数据中心，同时基于Trace构建整个业务拓扑图，得到整个知识图谱实现一个全息监控。</p><p></p><p>通过Trace打通了所有层的关联，基于这些关联关系将元数据与异常仓库进行关联支撑整个得物NOC-SLA的体系建设,完成“1-5-10”的目标，并且在系统治理和用户体验上面得到一个很大的提升。</p><p></p><p>为什么选择 OpenTelemetry 呢？</p><p>&nbsp;</p><p>要实现一个链路追踪系统，我们首先要解决的就是数据采集的问题，早期得物的链路追踪使用SDK方式，升级起来比较麻烦，所以我们新的链路追踪就希望使用Java Agent采用字节码增强的方式，对应用透明，然后当时市面上基于Java Agent字节码增强实现比较成熟的有Pinpoint、Skywalking 和 OpenTelemetry。</p><p><img src=\"https://static001.infoq.cn/resource/image/26/bb/26f94dd80d677d662856f22c5ffb7cbb.png\" /></p><p></p><p>那 OpenTelemetry 是 CNCF 云原生基金会里面的项目，它规范了各观测数据的类型，以及采集和导出的一个标准方法，其中包括 Trace、Metric、Log，如果基于OpenTelemetry的话，我们就可以通过一套标准的方案将 Trace、Metric、Log进行生成和导出，然后可以将这些不同类型的数据友好的把它关联起来，从而降低我们在开发过程中对不同类型的观察数据的使用成本，且无缝兼容现有的主流可观测性系统，比如 Prometheus、OpenTracing。</p><p></p><p>同时国内外各大厂商都在适配它，包括谷歌、微软、阿里云、腾讯云等，不依赖任何特定的厂商，然后我们团队也比较看好 OpenTelemetry 以后的一个发展趋势，并且希望跟进 OpenTelemetry 的社区享受社区带来的技术红利和影响力，最终我们选择了基于 OpenTelemetry 来建设得物的Trace2.0。</p><p></p><p>分布式链路追踪 Trace2.0 架构</p><p><img src=\"https://static001.infoq.cn/resource/image/dc/2b/dc72bfa57e4fb9d1f046ddc2fb6b5f2b.png\" /></p><p></p><p>采集端：通过集成并定制 OT 提供的多语言 SDK(Agent)，生成统一格式的数据。目前支持Java\\Go\\Python\\JS 四个语言。控制平面：中心化的配置中心向采集侧下发各类配置并动态生效；支持应用按实例数灰度接入，并提供出入参动态开关、性能剖析、版本管理等数据网关： OTel Server 兼容 OT 协议，提供gRPC和 HTTP 两种方式,并将数据写入 Kafka计算侧：除了将Span数据落盘之外，还提供了场景化的数据分析能力包括：SpanMetric 的计算、Redis 热点分析、MySQL 热点分析、单号关联 Trace存储侧：主要引入对象存储，Trace的索引数据存放在ClickHouse中，明细数据存放OSS，元数据存放在图数据库中</p><p></p><p>Trace2.0能力建设</p><p></p><p>首先最重要的就是Trace2.0保存所有有价值的一个 Trace 的完整的链路，那哪些是“有价值”的Trace（详见《<a href=\"https://mp.weixin.qq.com/s/axNxUpwtq3Why0UbuAQF8Q\">得物云原生全链路追踪Trace2.0架构实践</a>\"》）。</p><p><img src=\"https://static001.infoq.cn/resource/image/01/6a/0145a08146265d53dc4a864bb9fd736a.png\" /></p><p></p><p>同时 Trace2.0 支持各类型监控的一个关联，就可以实现指标关联 Trace，Trace 关联日志，然后采集端基于 JavaAgent 构建，接入时只需要添加一个JVM 参数就可以了，后续升级就比较方便，对应用无感知，Trace2.0还提供了请求的出入参、应用自定义埋点、诊断工具等方面的能力。</p><p>&nbsp;</p><p>Trace2.0在存储层的演进历程</p><p>&nbsp;</p><p>第一阶段Trace完全基于 ClickHouse 构建，有两张表分别是 SpanIndex表和 SpanData表，SpanIndex用于高级检索，SpanData存储每个Span的明细数据，为什么选择 ClickHouse 呢，ClickHouse 每秒可以支持数百万甚至数千万行的写入。全量采集下写入能力还是非常重要的，此外稀疏索引可以提供更好的查询性能和内存占用。</p><p><img src=\"https://static001.infoq.cn/resource/image/53/ec/537115278d923ccfc451e94c5c4850ec.png\" /></p><p></p><p>第二阶段我们对 Trace 数据做了冷热分离，电商履约系统经常需要排查超过7天的场景，为此我们做了冷热存储分离，近7天的数据全量保存，超过7天的只保留“有价值”的数据，为什么选择基于kafka的延迟消费方式呢，主要是当前市面的的尾部采样都是在端侧决策，对异步场景支持不友好，容易导致Trace保留不全，无法保障完整的链路。</p><p></p><p>第二阶段的Trace数据还是保留在 ClickHouse 中，虽然采用冷热存储解决了长时间保留的问题，但 ClickHouse 的磁盘利用率比较高，使用的SSD盘价格也不低，第三阶段我们引入对象存储进一步的把Trace存储成本降低。</p><p>&nbsp;</p><p>Trace2.0 - 冷热存储</p><p>&nbsp;</p><p>下面来详细介绍下冷热存储和对象存储：</p><p>冷热存储主要基于 Kafka 的延迟消费和 BloomFilter 编码，数据网关将客户端上报Trace 数据写入到 Kafka中，并把满足采样规则（主要是错慢的Trace）TraceID 通过 BloomFilter 编码写入DB中；热集群实时消费 Kafka 中的数据，并持久化到ClickHouse中，数据保留7天；冷集群延迟消费 Kafka 中的数据，通过 BloomFilter 判断 TraceID 是否命中，命中则写入 ClickHouse中，数据保留30天；</p><p><img src=\"https://static001.infoq.cn/resource/image/e5/d5/e58883c1f9bde99b8e4a40a8c19c88d5.png\" /></p><p></p><p>相关设计细节可以参考《<a href=\"https://mp.weixin.qq.com/s/axNxUpwtq3Why0UbuAQF8Q\">得物云原生全链路追踪Trace2.0架构实践</a>\"》</p><p>&nbsp;</p><p>Trace2.0 - 对象存储 写入流程</p><p>&nbsp;</p><p>如何降低Trace的存储成本，第一个想到的开源项目就是Grafana 的Tempo，Tempo使用对象存储(索引和明细)，并兼容一些主流协议像 Zipkin，Jaeger，OpenTelemetry，综合考虑后决定引入对象存储，将索引放在 ClickHouse 中，明细存放在对象存储中。</p><p><img src=\"https://static001.infoq.cn/resource/image/5c/2b/5c18b710fa190afa2a8cd87e19b5db2b.png\" /></p><p></p><p>从Kafka消费Span后，先将Span写内存，当内存满足一定条件后会对这批Span数据进行ZSTD压缩后刷盘到对象存储OSS后。写OSS采用追加写方式，每次写入可获得OSS的文件名和偏移量，将地理位置存储 ClickHouse 中。</p><p>&nbsp;</p><p>Trace2.0 - 对象存储 查询流程</p><p>&nbsp;</p><p>查询时，先通过 ClickHouse 确定一个TraceID 都在哪些OSS文件块中，获得文件块偏移量后通过随机读(Range)方式将文件块下载到本地(注意这里是文件块，不是文件)，再通过块内偏移量解析得到Span。汇聚所有Span得到一个完整的Trace。</p><p><img src=\"https://static001.infoq.cn/resource/image/99/21/99af2a4cbbc4ac27d0f57a2f8b658e21.png\" /></p><p></p><p>这里需要注意文件块的大小，一次批量写会包含不同Trace下的 Span，如果文件块过大，会导致解压时间过长，查询会超时，测下来文件块设置4MB是一个比较合适的值。</p><p>&nbsp;</p><p>实施效果：Trace2.0 现状</p><p>&nbsp;</p><p>Trace2.0在得物落地一年多时间，现已覆盖公司绝大部分在线业务。</p><p>每秒TPS在12M/s个SpanTrace点查P50耗时在300ms, P90查询耗时在800ms每天日增数据量在700+TB热存保留6天总存储4PB、冷存保留30天存储1PB，采用ZSTD压缩比在12，ClickHouse 单机可支持每秒40w个Span写入</p><p>&nbsp;</p><p>可观测性平台 - 前端监控</p><p>&nbsp;</p><p>第一个案例的话就是我们前后端链路打通的能力，在前后端没有打通之前，当前端收到一个异常后，我们无法全面地了解系统的性能状况，无法监控请求的传输路径和处理时间，也无法对系统的性能指标进行全面的分析。</p><p><img src=\"https://static001.infoq.cn/resource/image/3d/38/3d956129ce73438f5240f8f187a57338.png\" /></p><p></p><p>为了解决这个痛点的话，我们做了前后端的链路打通。这边是一个效果图，前端的异常就可以看到后端的链路，关联了哪些服务，通过出入参可以很方便的排查问题。同时提供了接口分析和页面分析。另外也支持会话跟踪，当出现问题时通过 SessionID 实现全链路追踪，还原用户的行为轨迹，包括页面加载、接口请求、和用户操作等。</p><p>&nbsp;</p><p>可观测性平台 - 容器监控</p><p>&nbsp;</p><p>随着公司应用全面容器化，对于 K8s 集群的稳定性要求越来越高，但是针对 K8s 监控没有一个专业和全面的监控产品满足问题发现和故障定位。</p><p><img src=\"https://static001.infoq.cn/resource/image/5c/4b/5ca6d23c56185e9dfde094d886bdcd4b.png\" /></p><p></p><p>早期K8s 监控大盘基于 Grafana 构建，指标理解和大盘配置门槛较高，且监控面板比较分散，</p><p>SRE 积累的排障工具无法与监控产品做融合，图表无下钻能力，为此打造一款专业的 K8s监控产品，统一指标计算口径，对现有指标做好分类分级，单击单个集群视角可查看关联资源池监控，资源池可下钻分析单个 Node节点的监控，POD视角可关联查看Node相关监控，大盘之间可进行联动分析，以及提供控制平面 API-Server 和 etcd 的监控。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（接口分析）</p><p>&nbsp;</p><p>讲完 K8S监控我们再看看应用监控，首先我们来看下接口分析，接口我们按照组件类型分为接口、下游调用、SQL调用、NoSQL 调用、以及 MQ 和其他进行分类。</p><p><img src=\"https://static001.infoq.cn/resource/image/4d/ba/4d79bf4559df20a4bf65932bcf3e00ba.png\" /></p><p></p><p>图表右侧展示接口的黄金三指标，底下展示接口关联的Trace，接口支持的实例维度、上下游分析和耗时分解等排查场景。有了耗时分解我们就可以很清晰的看出接口RT变高，是由哪类操作类型导致，DB 还是Redis，以及关联的 Trace。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（异常分析）</p><p>&nbsp;</p><p>接下来介绍应用监控下另外一个非常重要的模块“异常分析”，第一个是基于异常日志按照异常名进行统计分析。</p><p><img src=\"https://static001.infoq.cn/resource/image/49/ec/495ebc2c7efc91aaec7770b2749f5eec.png\" /></p><p></p><p>第二个是错误码分析，主要根据响应体里的 Code码和 CodeMessage，通过对业务码分类，可以快速的帮助我们了解业务需求和业务流程，有助于问题的发现和定位。</p><p>第三块是MySQL热点分析，按照SQL指纹统计调用次数、慢SQL次数以及关联的接口名。</p><p>第四块是Redis热点分析，通过客户端记录Redis的入参和出参，统计Redis命中率、大Key、高频写、慢调用。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（链路追踪）</p><p>&nbsp;</p><p>接下来介绍下应用监控的链路Trace部分。用户可以在接口分析页的黄金三指标曲线上点击就能看到对应的Trace链路。同时也在接口页面的右下角展示Trace检索页，可以快速的看出错、慢Trace。</p><p><img src=\"https://static001.infoq.cn/resource/image/46/1f/4677f010cdb5e50yy99d5fc31c0bac1f.png\" /></p><p></p><p>此外提供高级检索模式，满足研发自定义查询场景，支持多维查询，比如实例、或者指定上游或者指定下游进行过滤。</p><p>&nbsp;</p><p>此外，在电商场景下，研发多以订单号、作为排障的场景，因此我们和研发团队约定埋点规则 ——在Span的 Tag上记录单号—— 便会在 ClickHouse 中新写一条“单号关联表”。用于记录单号ID和 TraceID 的映射关系，研发通过单号查看所有的 Trace。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（Trace详情）</p><p>&nbsp;</p><p>Trace详情页的话会将一个Trace下的所有Span在一张图中显示摘要信息根据TraceID解码当前链路的发生时间和来源IP。</p><p><img src=\"https://static001.infoq.cn/resource/image/13/3e/13c495eae80a26b47f73yy7cbyyf3d3e.png\" /></p><p></p><p>目前我们提供两种展示模式，列表详情以及聚合统计。</p><p>在聚合统计模式下，可以查看trace的整体性能指标，这次请求经过哪些服务，以及该服务请求了哪些组件，可以直观的看出哪个服务耗时最高以及耗时最大的接口是哪个。列表模式下支持自定义列，通过自定义可以快速的帮助我们理解Trace上下文，比如在染色环境下，测试同学想一眼看出当前链路是否跨环境了，再比如通过线程名可以快速帮助我们理解异步上下文。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（Trace详情）</p><p>&nbsp;</p><p>单击某个Span，Span详情会以抽屉方式展示，上面是Span的摘要信息，下面按照属性含义做了分类。部分属性支持跳转到其他平台，比如CMDB和容器平台，在Span详情页的打通了指标和日志，日志主要根据TraceID 关联。</p><p><img src=\"https://static001.infoq.cn/resource/image/fd/49/fd5b0bccddd4784e5297464a00c8e749.png\" /></p><p></p><p>指标可显示当前Span的黄金三指标、以及当前Span所在JVM状态包括GC、线程池和POD的 CPU和内存以及POD所在宿主机的资源使用情况，这样可以减少页面来回切换。</p><p>&nbsp;</p><p>可观测性平台 - 应用监控（告警平台）</p><p>&nbsp;</p><p>在报警分析上，主要使用Prometheus作为数据源，通过专家经验梳理告警模板，可以快速的应用到其他服务，同时可以保障新应用上线后低成本接入，目前共提供50多个模版。</p><p><img src=\"https://static001.infoq.cn/resource/image/78/03/7888460afcf69c822760b5d2027a9d03.png\" /></p><p></p><p>在告警规则上，支持同比、环比、无数据。在通知组上支持飞书、短信、电话，以及支持分钟级、小时级、天级的告警聚合通知。</p><p>&nbsp;</p><p>【活动推荐】</p><p>&nbsp;</p><p>7 月 21-22 日，将在深圳·博林天瑞喜来登酒店举办 <a href=\"https://archsummit.infoq.cn/2023/shenzhen\">ArchSummit 架构师峰会</a>\"，会议主题还是会围绕AIGC、前端架构、数字化、架构思维，和架构师成长来展开。如果你想要分享演讲话题，可以在这里提交议题思路：<a href=\"https://jinshuju.net/f/7wUiwn\">https://jinshuju.net/f/7wUiwn</a>\"</p>",
    "publish_time": "2023-05-06 15:16:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "科大讯飞正式发布星火认知大模型，刘庆峰：多题型可解析数学能力已领先ChatGPT | InfoQ快讯",
    "url": "https://www.infoq.cn/article/057ZDYfzOJZOZUfpOLH7",
    "summary": "<p>5月6日，InfoQ获悉，科大讯飞正式发布了“讯飞星火认知大模型”。</p><p>&nbsp;</p><p>在星火大模型发布会上，科大讯飞董事长刘庆峰表示，认知智能全国重点实验室牵头设计了通用认知智能大模型测评体系。</p><p>&nbsp;</p><p>现场，科大讯飞研究院院长刘聪，上台演示了讯飞星火认知大模型的实际应用。据介绍，星火认知大模型具备以下能力：多风格多任务长文本生成、多层次跨语种语言理解，泛领域开放式知识问答，情景式思维链逻辑推理，多题型可解析数学能力，多功能多语言代码能力。</p><p>&nbsp;</p><p>其中，多风格多任务长文本生成能力，包括发言稿、邮件、新闻通稿、营销方案、调研问卷、商业文案、英文写作等。多层次跨语种语言理解能力，涵盖语法检查、要素抽取、语篇规整、文本摘要、阅读理解、情感分析，机器翻译等。</p><p>&nbsp;</p><p>刘庆峰表示，在多题型可解析数学能力上，星火认知大模型摇摇领先，已领先ChatGPT。多功能多语言代码能力和ChatGPT还有一定差距，后续将不断提升。</p><p>&nbsp;</p><p>刘庆峰介绍，讯飞星火大模型将在6月9日前，将开启实时问答，升级多轮对话能力；在8月15日前，再次提升代码能力以及多模态交互能力；在10月24日前，星火大模型将对标ChatGPT，在中文能力上超过ChatGPT，在英文能力上与ChatGPT相当。</p><p>&nbsp;</p>",
    "publish_time": "2023-05-06 15:20:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "拼多多回应将总部从中国迁至爱尔兰；微软Bing突然爆炸级更新，文生图原生支持中文；75岁人工智能教父离职谷歌，痛悔毕生工作｜ Q资讯",
    "url": "https://www.infoq.cn/article/WG49e1Q55SNA0B9kZkMj",
    "summary": "<p>五一假期已经结束，让我们一起看看本周有哪些重要新闻吧！</p><p>&nbsp;</p><p>拼多多回应总部由中国迁至爱尔兰：消息严重失实</p><p>&nbsp;</p><p>微软Bing突然全面开放插件系统，文生图原生支持中文</p><p>&nbsp;</p><p>消息称马斯克已将推特员工数量减至约1000人&nbsp;</p><p>&nbsp;</p><p>75 岁人工智能之父突然离职谷歌：痛悔毕生工作，警告 AI 会对人类构成巨大威胁</p><p>&nbsp;</p><p>IBM 将用 AI 取代部分工作岗位，7800人将失业</p><p>&nbsp;</p><p>世界经济论坛：未来五年 AI 或淘汰2600万个工作岗位</p><p>&nbsp;</p><p>蒂姆·库克称苹果公司仍未考虑“大规模裁员”</p><p>&nbsp;</p><p>暴雪 CEO 呼吁更多公司合并以便与腾讯竞争</p><p>&nbsp;</p><p>Angular 16 正式发布</p><p>&nbsp;</p><p>sudo 和 su 正用 Rust 重写</p><p>&nbsp;</p><p>&nbsp;</p><p>今日好文推荐</p><p>&nbsp;</p><p><a href=\"https://mp.weixin.qq.com/s/LH3751pEeqBWxe375m0kXA\">谷歌、OpenAI 都白干，开源才是终极赢家！谷歌内部文件泄露：欲借开源打败 OpenAI</a>\"</p><p>&nbsp;</p><p><a href=\"https://mp.weixin.qq.com/s/H29PBWmeEA5mfI38ikigMA\">谷歌用机器人大规模删除代码：二十多年积累了数十亿行，已删除5%C++代码</a>\"</p><p>&nbsp;</p><p></p><h4>拼多多回应将总部从中国迁至爱尔兰：消息严重失实</h4><p></p><p>&nbsp;</p><p>近期，有媒体报道称，拼多多提交给美国证券交易委员会（SEC）的20-F文件显示，爱尔兰已经成为其注册地。另有最新文件显示，该公司的“主要行政办公室”在都柏林，而2月份的文件还显示该公司所列的地址是上海。5月4日下午，对于外媒报道的“拼多多总部从中国迁至爱尔兰”的消息，拼多多方面回应称严重失实，纯属误读。该负责人表示，“拼多多出生在上海，成长在中国，拼多多总部始终在中国上海，不会改变。”</p><p>&nbsp;</p><p>3月下旬美国政府持续对 TikTok 提出的质疑；4月初，美国国会下属美中经济与安全审议委员会曾发布分析师报告，提示跨境电商平台 SHEIN、拼多多旗下 Temu 及其他中国电商可能存在风险。目前来看，拼多多此举应该是为了提前做出应对。</p><p></p><h4>微软Bing突然全面开放插件系统，文生图原生支持中文</h4><p></p><p>&nbsp;</p><p>5月5日，<a href=\"https://www.infoq.cn/article/ZmDbgaSV7PmYEG3MwQAx\">微软突然官宣全面开放 Bing Chat</a>\"：无需任何等待。只需注册一个账户，首页即可体验。新的 Bing Chat 不止能聊天，还能问问题、搜索网页，甚至还能生成图片。</p><p>&nbsp;</p><p>这次做的大规模升级主要有三个方面：首先，在插件功能上，新版 Bing Chat 将允许第三方参与 Bing Chat AI 的结果。微软正在与 OpenTable 合作，将餐厅预订纳入到 AI 响应中。其次，现在 Bing Chat 除了能做普通的文字交流问答、编程学习外，还能帮助用户“生成图片” 甚至是视频等，而且以文生图的功能还支持了中文在内的多种语言。此外，微软还在 Bing Chat 中添加了一项备受期待的功能：历史记录。这个新的历史聊天记录功能可以让用户跨设备获取聊天机器人对话，甚至可以将 Bing Chat 用作研究工具。</p><p></p><h4>消息称马斯克已将推特员工数量减至约1000人&nbsp;</h4><p></p><p>&nbsp;</p><p>5月4日消息，据外媒报道，有知情人士称，收购推特半年后，特斯拉 CEO 埃隆·马斯克已将该公司的员工人数裁减了近90%，目前只剩下大概1000名全职员工。知情人士认为，在排除掉合同工后，现在的推特剩余员工数量远低于此前马斯克对媒体所说的1500名。</p><p>&nbsp;</p><p>值得注意的是，此前马斯克曾表示，在他去年11月接管推特时，该公司拥有约7800名员工，他上任后裁掉了6500人，现在仅剩下1500名员工。他还提到，解雇推特的大部分员工是他自接管公司以来不得不做的“最艰难的事情之一”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>75 岁人工智能之父突然离职谷歌：痛悔毕生工作，警告 AI 会对人类构成巨大威胁</h4><p></p><p>&nbsp;</p><p>近日，有“人工智能教父”之称的 Geoffrey Hinton 宣布辞去在谷歌的工作，并且现在对自己一生从事的工作感到后悔：“我总在用这样的借口安慰自己：哪怕我自己不做，其他人也会这样做。但目前真的不知道要怎么防止坏蛋利用 AI 来作恶。”</p><p>&nbsp;</p><p>Hinton 在社交平台表示，他辞去谷歌的工作以便自己可以自由地谈论 AI 的风险。他认为，随着企业改进 AI 系统，它们会变得越来越危险。他目前最担心的是，互联网将充斥着虚假照片、视频和文字，普通人将“无法再知道什么是真的”。虚假信息的传播只是 Hinton 眼下想要强调的风险之一。从长远来看，他担心 AI 会彻底消除一切需要大量记忆的工作，而随着其逐步编写并运行构成自身的代码，AI 也许会最终取代人类。Hinton 今年75岁，作为三位“人工智能教父”之一，与另外两位合作伙伴共同获得了 2018 年图灵奖，旨在表彰他们为当前 AI 繁荣做出的基础性贡献。</p><p></p><h4>IBM 将用 AI 取代部分工作岗位，7800人将失业</h4><p></p><p>&nbsp;</p><p>5月1日，IBM 宣布一个重磅消息：将暂停招聘人工智能可以胜任的岗位，将用 AI 取代7800个工作岗位。这意味着将有大约7800人失业。</p><p>&nbsp;</p><p>IBM首席执行官 Arvind Krishna 透露，暂停招聘的主要为后台岗位，比如人力资源等。这类岗位大约有26000名员工。“我可以很容易地看到，在接下来5年的时间里，其中30%的工作将被人工智能和自动化取代。”该 CEO 很确定地预测，在未来5年里，这些岗位的30%工作将被人工智能等取代。目前，IBM 的员工总数约26万，并将继续招聘软件开发和面向客户的职员。IBM的最新计划标志着人工智能技术对人类就业的冲击和风险。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>世界经济论坛：未来五年 AI 或淘汰2600万个工作岗位</h4><p></p><p>&nbsp;</p><p>5月1日，世界经济论坛发布的一份报告显示，未来五年内，由于人工智能、数字化以及绿色能源转型和供应链回流等因素，全球近四分之一的工作岗位将发生变化。此次调查涉及800多家企业，约75%的受访公司表示，他们预计将在未来五年内采用 AI 技术；AI 将淘汰多达2600万个记录和行政职位，比如收银员、票务员、数据录入和会计。</p><p>&nbsp;</p><p>对于这种前景，世界经济论坛常务董事 Saadia Zahidi 评论称，总体而言，世界就业形势变化的程度相当高。同时根据世界经济论坛的预测，各企业将需要更多员工协助运用和管理 AI 工具。截至2017年，信息分析师、科学家、机器学习专家和网络安全专家职位预估将增长30%。</p><p></p><h4>蒂姆·库克称苹果公司仍未考虑“大规模裁员”</h4><p></p><p>&nbsp;</p><p>5月5日，苹果公司首席执行官蒂姆·库克表示，他仍将大规模裁员视为“最后手段”，并确保这不是公司目前正在考虑的事情。苹果公司至今没有大规模裁减全职员工，相对而言，Google 和Facebook 母公司 Meta 等大型科技公司在最近几个月经济不稳定的情况下，裁减了数万名员工。</p><p>&nbsp;</p><p>\"我认为这是最后的手段，因此，大规模裁员不是我们目前正在谈论的事情，\"库克在接受 CNBC 采访时说。在去年的一份监管文件中，苹果公司表示，截至2022年9月24日，该公司约有16.4万名全职等值员工。</p><p></p><h4>暴雪CEO呼吁更多公司合并以便与腾讯竞争</h4><p></p><p>&nbsp;</p><p>动视暴雪CEO鲍比・科蒂克（BobbyKotick）日前接受外媒CNBC采访，继续对英国 CMA 阻止微软收购动视暴雪的决定进行反击。他表示应该允许更多的合并，以便与腾讯等公司竞争。</p><p>&nbsp;</p><p>据悉，科蒂克称：“我认为在某个时候，监管机构会意识到大量高薪工作从欧美科技公司流失。我关注了字节跳动、腾讯，这些都是世界上各自行业中最好的公司，公司要想竞争，就必须能够进行整合或合并。”科蒂克还补充说，如果交易无法完成，动视暴雪依然会继续前进。“我们是一家强大的公司，无论交易是否通过，我们都将继续作为一个独立公司运营，当在全球市场运营时，其会变得更具挑战性。”</p><p></p><h4>Angular 16 正式发布</h4><p></p><p>&nbsp;</p><p>本周， Angular 16正式发布。在之前的 Angular 15中，Angular 团队通过将独立 API 从开发者预览版升级至稳定版，将 Angular 的简易性和开发者体验提升到了一个重要的里程碑阶段。如今，Angular 将继续这一改进势头，发布自 Angular 最初推出以来最大的一次版本更新。新版本在Reactivity、服务器端渲染和工具方面取得了巨大的飞跃。</p><p>&nbsp;</p><p>Angular 16 的新特性包括：全新 Reactivity 模型的开发者预览，完全向后兼容，Angular Signals 库，RxJS 互操作性；服务器端渲染和 hydration 增强；改进独立组件、指令和管道的工具等。</p><p></p><h4>sudo 和 su 正用 Rust 重写</h4><p></p><p>&nbsp;</p><p>据奇客 Solidot 消息，在亚马逊 AWS 的资助下，类 Unix 操作系统广泛使用的工具 sudo 和 su 正用 Rust 语言重写，以提高软件在内存方面的安全性，进一步增强 Linux 和开源生态系统的安全性。sudo 的开发始于 1980 年代，过去几十年里它已经成为一种必不可少的工具，但它是用 C 语言编写的，遭遇过许多与内存安全问题相关的漏洞。为了确保关键软件的安全性，防止内存安全漏洞，Ferrous Systems 和 Tweede Golf 正在联合将 sudo 从 C 移植到 Rust，他们的项目 sudo-rs 托管在 GitHub 上。</p>",
    "publish_time": "2023-05-06 15:39:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]