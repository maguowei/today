[
  {
    "title": "前 Mozilla 浏览器工程师：Web 开发的七大奇怪观念，你中招了吗？",
    "url": "https://www.infoq.cn/article/e9csXhR2pJafE2FhDqAd",
    "summary": "<p>导读：本文深入探讨了从浏览器开发回归 Web 开发的经验，揭示了一些关于 Web 开发的观念和看法。作者提醒我们，尽管我们可能认为自己的 Web 经验已经足够丰富，但实际上，Web 开发领域远比我们想象的要广阔和复杂。通过比较浏览器工程师、标准编辑和其他 Web 开发者的观点，我们可以更全面地理解 Web 开发的全貌。对于小型 Web 商店和初创公司的 Web 开发者来说，他们面临的挑战和约束更是独特。在 Web 应用开发中，他们需要找到自己的定位，充分发挥自己的优势。总体而言，作者希望读者能够对自己的 Web 开发实践充满信心。了解并接受自己的约束可能是取得成功的关键。</p><p></p><p>自从我离开 Mozilla，全身心投入 Web 开发工作后，我发现了一些令我惊讶的事情。与我之前作为浏览器工程师的认知相比，Web 开发实际上是一个相当复杂且富有挑战性的领域。而且，Web 开发人员普遍都非常聪明，他们所采用的框架和技术并非如我们之前所轻视的那样不堪。</p><p></p><p>与此同时，我也发现了一些 Web 开发人员对浏览器和 Web 的看法，这些观点在我作为前浏览器工程师和标准编辑的角度看来，存在一些疑问。</p><p></p><p>以下是我所观察到的一些令我惊讶的事情。</p><p></p><p></p><h2>Web 浏览器工程师对Web 开发非常了解</h2><p></p><p></p><p>我曾天真地认为，作为浏览器工程师，我们对 Web 的了解一定比其他任何人都要深入。毕竟，我们每天都在编写构成 Web 平台的代码。</p><p></p><p>问题在于，编写 Web 浏览器是一项艰巨的任务。</p><p></p><p>大多数浏览器工程师都专注于特定的领域，对某些领域有着非常深入的了解，而对其他领域的知识可能只是停留在表面。此外，浏览器平台的工程师们主要使用 C++ 和 Rust 编写代码，只有极少数的 JavaScript 测试用例。他们所面对的是一个庞大的代码仓库，其中大部分工作都是由其他人负责的。</p><p></p><p>因此，在日常工作中，Web 浏览器工程师并不会与 webpack 对抗，也不会去深入理解全页的 TypeScript 错误。他们也不会考虑如何让 iOS 上的 Safari 表现得更像其他浏览器，或者在大规模的 CSS 中挣扎。他们更不会去评估最新的 SSR/SSG/island 框架是否值得投入使用，也不会为了更优化的分块而重构庞大的 JS 代码库，也没有在 GitHub 上因为某个依赖的最新版本破坏了他们的应用而提出沮丧的问题，也没有试图让所有工具在 ESM 与 CJS 之间达成一致，也没有因为选择了正确的状态管理方法还是应该第十次重写整个东西而夜不能寐。</p><p></p><p>简而言之，由于他们的工作重点和职责范围，他们对于真实世界的 Web 开发所面临的挑战和问题的了解，可能远不如 Web 开发人员所期望的那样深入。</p><p></p><p>尽管如此，致力于浏览器开发者工具和浏览器前端开发的工程师们通常会更加关注这些问题，并且他们每天都在使用 JavaScript 进行开发。然而，他们仍然与常规的 Web 开发存在一定的距离。例如，他们只需要针对单个浏览器引擎的平台特性进行开发，通常只需要针对单个浏览器版本进行工作（能够使用新的和令人兴奋的功能而不用担心兼容性是非常棒的），不需要担心捆绑大小，服务器，脱机状态，或许多其他使 Web 开发变得困难的问题。</p><p></p><p>显然，一些 Web 浏览器工程师也会参与业余项目，但这些业余项目的限制与在一家初创公司的情况截然不同。在初创公司，Web 应用的成功与否直接关系到公司的生死存亡。</p><p></p><p>在我职业生涯的早期，我曾从事平面设计和 Web 开发方面的工作，甚至在加入 Firefox 开发团队之前，我曾在大阪的一家 Web 开发公司工作过一段时间，并参与了一些 Web 应用的制作。然而，当我离开 Mozilla，全心投入到 Web 开发领域时，我对它的快速发展以及我对它的理解程度感到震惊。</p><p>作为一名 Web 浏览器工程师的经验对 Web 开发非常有价值，主要是因为我了解去哪里寻找资源、向谁寻求帮助以及如何提交错误报告。然而，如果我认为成为 Web 浏览器工程师就自动使我具备了 Web 开发的能力，那我就是在自欺欺人。</p><p></p><p>在我在 Mozilla 的日子里，Firefox OS 时代无疑是我最愉快的时光。那时候，我们有一个内部团队专注于构建真实世界的移动 Web 应用。作为平台团队，我们的首要任务是调试这些应用，确保它们能够成功运行。</p><p></p><p>在这个过程中，我发现 transitionend 事件有时并不可靠，这导致 Web 应用程序出现错误，甚至过于复杂。因此，我提出并实现了 transitioncancel 事件，并引入了 Web 动画的 Animation.finished Promise。</p><p></p><p>然而，即使与 Web 开发人员紧密合作，也无法让我真正准备好再次成为全职的 Web 开发人员。在很大程度上，浏览器工程师和 Web 开发人员是处于两个截然不同的世界中，这也许并不像我们所想象的那样，都是 Web 开发人员超级英雄。</p><p></p><p></p><h2>制定 Web 规范的人对 Web 开发非常了解</h2><p></p><p></p><p>是的，那些致力于 Web 标准和规范制定的人，特别是浏览器工程师，通常对 Web 开发有深入的了解。早在 2012 年，Brendan Eich 就曾指出，“制定标准就像制定法律一样，绝对是制香肠的过程” ，引用了 John Godfrey Saxe 的名言：</p><p></p><p>法律就像香肠一样，我们对制定过程的了解程度与对它的尊重程度成反比。</p><p></p><p>作为一名 Web 开发人员，很容易将那些致力于 Web 标准和规范制定的人想象成拥有无限智慧的群体。他们能够根据每个提案的技术优点和对行业需求的深刻理解，做出冷静而理性的决策。</p><p></p><p>然而，这种美好的幻想通常会在你参与的第一个工作小组会议中破灭。尽管大家的出发点都是最好的，但有时决策至少在一定程度上是基于某个人的魅力或强烈的个性，这取决于当时在房间里的人是谁、大家有多疲劳，甚至可能因为某人需要完成晋升材料而急于推出某个功能。</p><p></p><p>这听起来可能有些愤世嫉俗，但请让我进一步澄清两点。</p><p></p><p>首先，这些工作小组的人员都是善良、出色的人，他们通常也意识到自己的局限性，并尽力寻求 Web 开发人员的反馈。不幸的是，到目前为止，我还没有看到哪个小组在这方面取得了很大的成功。例如，虽然有 Twitter/X 的投票机制，但这些投票往往只有前沿的 Web 开发人员参与，而且容易被投票者的个人观点所影响。</p><p></p><p>其次，我对 WHATWG 规范（如 HTML 和 DOM）的经验并不多，这些规范的决策过程似乎是异步进行的（“在面对面会议期间做出的任何决定都应被视为非约束性”——来自 WHATWG meetings），然而，我的印象是他们似乎能够做出更好的决策。像 Anne van Kesteren、Simon Pieters 和 Domenic Denicola 这样的人可能比地球上其他任何人更了解 Web 技术。但即使如此，这并不等同于他们完全了解 Web 开发的实际情况和挑战。</p><p></p><p></p><h2>Web 开发人员对Web 开发非常了解</h2><p></p><p></p><p>作为一名浏览器工程师，发布新的平台功能无疑是一种令人满足的体验。当我的工作成果被 Smashing Magazine 和 CSS Tricks 等知名技术媒体报道，并在 Twitter/X 上引发广泛讨论时，很容易让人误以为全世界都已经知道了这一伟大的新进展。</p><p></p><p>然而，几年前的某一天，Mozilla 日本团队的一些成员决定在东京进行一项特别的调查，我们采访了一些当地的 Web 开发人员，了解他们对最新开发工具的看法和使用情况。</p><p>结果令人惊讶。</p><p></p><p>我们发现许多开发人员对十年前推出的新 CSS 功能一无所知。更令人沮丧的是，即使我们告诉他们这些功能，他们似乎并不感到兴奋。他们表示，使用 jQuery 和 WordPress 已经能够很好地完成工作，对此心存感激。</p><p></p><p>相反，他们更关心的是一些实际问题，比如：“在响应式设计模式下为客户展示网站时，如果没有 iPhone 模型来模拟设备视图，客户很难理解他们所看到的是该网站在手机上的预览效果。我真的需要那个 iPhone 模型。”</p><p></p><p>作为一个参与制定新 Web 标准的浏览器工程师，我对这些开发人员面临的现实问题深感失望。但我更深刻地认识到，他们在交付网站和 Web 应用的过程中所面临的限制和挑战。</p><p></p><p>与从事浏览器开发或资金充裕的硅谷初创公司的人不同，许多 Web 开发人员在小公司工作，面临着巨大的压力，需要快速交付项目以满足客户需求，然后迅速转向下一个项目以维持生计。他们没有时间去探索即将推出的新技术，而是更倾向于寻找他们已经熟悉的经过验证的解决方案。</p><p></p><p></p><h2>浏览器并非专门为运行单页面应用而设计</h2><p></p><p></p><p>从浏览器开发回归到 Web 开发，我发现关于浏览器工作方式的某些观点令人惊讶。</p><p></p><p>在从事动画开发时，我发现许多人误以为动画 “在 GPU 上运行”。实际上，浏览器可以将部分动画卸载到单独的进程或线程中，使用 GPU 来更新动画并合成每一帧。然而，这并不意味着所有动画都完全在 GPU 上运行。相比之下，有些观点则更为离谱，如 “浏览器不是为运行单页面应用（SPA）而设计的”。</p><p>有一种观点认为，最初的浏览器主要是从网络加载内容，逐步进行布局和渲染，并且针对这一过程进行了大量优化。动态内容的出现相对较晚，因此优化程度可能较低。</p><p></p><p>然而，我从事浏览器开发近二十年的时间里，并不完全认同这一观点。Firefox 的前端和开发者工具实际上就是一个 SPA。尤其是开发者工具，它们使用 React 和 Redux 编写，各个方面都体现了 SPA 的特点。</p><p></p><p>关于浏览器在处理通过 JavaScript 进行的复杂和长寿命 DOM 树的动态更改方面存在缺陷的观点，这与浏览器的发展趋势相矛盾。</p><p></p><p>尽管有人可能会提出移动端浏览器并未针对 SPA 进行优化的观点，但实际上，这并不意味着浏览器在处理 SPA 方面存在根本缺陷。例如，在 Android 上，Firefox 从使用 HTML 渲染的浏览器外壳切换到了本地浏览器外壳以提高性能。我无法评论导致这一变化的特定性能限制是什么，但当时的一篇博客 表明这与改善应用启动时间和平移和滚动性能有关，这两者都不表明浏览器在处理 SPA 方面比其他架构更差。</p><p>“好吧，也许浏览器可以处理复杂的长寿命 DOM 树，进行频繁的动态更改，但 SPA 往往具有大型 JS 捆绑包，下载和解析速度较慢，阻塞了初始渲染。” 这是一个合理的论点，但它主要关注的是捆绑包大小和初始渲染的阻塞问题，而不是浏览器是否适合运行 SPA。实际上，这个问题同样适用于普通的 WordPress 站点。</p><p></p><p></p><h2>多页面应用将取代单页面应用</h2><p></p><p></p><p>在探讨 SPA（单页面应用）和 “编写 Web 应用程序的唯一真实方式” 时，近期关于 “浏览器无法处理 SPA” 的一种观点是 “MPA（多页面应用）将取代 SPA”。</p><p></p><p>我对 MPA 持有相当积极的态度。更具体地说，作为一名与众多动画规范密切相关的人，我对 视图转换规范 感到十分兴奋。在 Mozilla，特别是在 Firefox OS 时期，我们一直致力于推动这一 提案 的实现。我十分赞赏 Jake 等人最终使这一规范成为现实。</p><p></p><p>视图转换规范最初是专为 SPA 设计的，但现已适应了 MPA 的工作方式，并为多页面站点带来了更加愉悦的用户体验。</p><p></p><p>然而，基于 “SPA 是不好的” 这一思想，似乎有一种趋势认为 MPA 是未来的发展方向，而 SPA 将逐渐被淘汰。</p><p></p><p>与前文中的观点不同，我对这一看法的惊讶并非基于我在浏览器开发上的工作经验，而是基于我最近在 Web 应用程序方面的工作经验。</p><p></p><p>首先，我们需要明确什么是 MPA。</p><p></p><p>在我理解中，与 SPA 的特点 —— 拥有一个或两个长寿命的 DOM 树，经常由脚本频繁更新 —— 不同，MPA 主要是通过导航到从网络提供的不同 HTML 资源来更新内容。这些资源不必是最顶层的导航，例如，可以通过导航到  来实现。同样，SPA 可能使用 `` 来进行分块，但内容的更新方式通常存在区别。</p><p></p><p>根据这个定义，Google Docs 是一个 SPA，因为虽然每个文档都作为单独的资源提供，但大多数时候，用户与通过 JavaScript 不断更新的单个文档进行交互。YouTube 可能被认为是 MPA，但实际上可能是作为 SPA 实现的，通过平滑内容变化、拦截导航并通过脚本替换内容。</p><p></p><p>然而，对于 MPA 将取代所有 SPA 的想法，我感到惊讶：Figma 或 Web 上的 Photoshop 将如何作为 MPA 工作？或者 Slack、Discord、Google Maps 等应用又该如何实现？</p><p></p><p>目前，我正处于开发一款以离线优先的移动 Web 应用程序的过程中，该程序可以将数据存储在本地并同步到服务器。为了站在 Web 技术的前沿，我研究了如何采用 MPA。</p><p></p><p>在简要阐述观点时，我们应考虑到保持预期用户体验的重要性。对于具有独立导航面板和离线优先要求的移动 Web 应用程序，引入 ``，将功能划分到单独的 HTML 请求中（而非单独的脚本块请求）以及预渲染某些块是合理的解决方案。然而，这样做将使复杂性显著增加（例如，双向同步首先变成三向同步），并可能带来更多的错误、延迟和功能发布缓慢等问题。</p><p></p><p>鉴于应用尚未发布，这一点上的论据可能显得较为薄弱。由于无法实际查看应用程序，因此请相信我的观点。我确实已经尝试过其他架构，但根据我对此特定应用程序的了解，这并非正确的架构选择。因此，对于提出 everything 应该采用 MPA 的建议，我感到十分惊讶。</p><p></p><p><h2>所有网站都应该在没有JavaScript 的情况下运行</h2></p><p></p><p>在探讨 Web 开发最佳实践时，构建一个在无 JavaScript 环境下仍能正常运行的站点无疑是一个值得追求的目标。这样的做法能够确保站点在失去 JavaScript 功能时仍能保持良好的降级体验，避免 JS 阻塞初始渲染，并确保在各类浏览器中的正常运行。然而，这一目标有时被过于教条化，甚至有观点认为 “所有站点都应该在没有 JavaScript 的情况下工作”。</p><p></p><p>我理解，如果你的站点能够在无 JavaScript 环境下正常工作，那么这个结论可能看似合理。然而，对我而言，这种观点似乎过于狭隘。</p><p></p><p>正如我之前提到的 Figma 和 Photoshop for Web，以及浏览器的开发者工具和 Parapara Animation，很难想象这些工具和应用程序在没有 JavaScript 的情况下如何正常运作。</p><p></p><p>因此，在追求无 JavaScript 站点的同时，我们也需要认识到并非所有站点和应用都适合或能够在无 JavaScript 环境下运行。关键在于确保站点在失去 JavaScript 功能时仍能提供良好的用户体验，并尽可能地避免功能上的缺失。</p><p></p><p>在无障碍性方面，Mozilla 的专家分享了关于键盘导航的经验，强调 Tab 导航应保持相对粗糙，即使用 Tab 键导航至控制组（如工具栏），再使用箭头键在该组内移动。这种 “粗略” 的导航方式能够提高用户在应用程序中的移动速度，避免用户需要通过每个控件逐一使用 Tab 键进行导航。WAI（Web Accessibility Initiative）将此称为“roving tabindex”。</p><p></p><p>然而，实现这种粗粒度的键盘导航需要依赖 JavaScript。若要使基于箭头键的导航具备二维特性，则需更多的 JavaScript 代码。虽然我们期待有一天平台能够填补这一空白（期待 focusgroup 的进展），但目前，我们应坦然地利用客户端 JavaScript 提升应用程序的无障碍性。</p><p></p><p>我认为，有些站点应当更加积极地运用 JavaScript。</p><p></p><p>以 Eleventy 为例，其文档在很大程度上避免了使用客户端 JavaScript。由于 Eleventy 支持多种模板语言，它为每种语言都提供了代码示例。遗憾的是，它并未记录用户选择的语言，导致用户若非默认语言，则需在每个代码示例上更改选项卡。若在此处加入一些客户端 JavaScript，将显著提升用户体验。</p><p></p><p>更新（2024 年 1 月 11 日）：根据最新反馈，Eleventy 似乎已采纳建议并进行 修复。</p><p></p><p><h2>Web 开发不应该需要构建步骤</h2></p><p></p><p>尽管本文的大部分内容源自 2022 年，但在整理过程中，我忍不住加入了一些近期让我深感惊讶的想法。</p><p></p><p>在 Web 开发教条主义的列车上，我们抵达了最后一站，一个观点再次浮现在我脑海中，它虽然已经出现多次，但仍然让我感到惊讶。我最近看到的一个版本大致是这样的：“我们如此盲目和固执，以至于我们最终拥有了一个非常复杂的工具链，但我们实际上应该能够在完全没有构建步骤的情况下发布 Web 应用。”</p><p></p><p>作为一名长期使用编译型语言的专业人士，我对无需构建步骤的愿景感到惊讶。编译器工程师所展现的精湛技艺令我惊叹不已。他们是真正的天才，通过巧妙的优化技术，将平凡的代码转化为卓越的性能。如果可能的话，我希望在我的 Web 开发中能够更多地运用这种编译器的魔法。显然，JavaScript 本身存在挑战，因为静态确定操作的副作用可能非常困难，但我相信在编译时优化 JavaScript 方面仍有很大的探索空间。</p><p></p><p>Web 开发人员似乎在优化图像资产和在合适的情况下预生成静态 HTML 页面方面达成了共识，但为什么在优化代码资产方面存在抵制情绪？为什么要将计算和 I/O 推迟到运行时，而不是在构建时完成一次？此外，我不想每次请求时都向每个用户发送我那长达一兆字节的针对 iOS Safari 的诅咒。也许 2024 年将是客户端 Rust/WASM 前端框架崭露头角的一年，如果是这样，我们最好习惯有一个构建步骤！</p><p>我的博客不能代表整体 Web 开发的情况。</p><p></p><p>“Web 开发的整体情况”。从浏览器工程师转变为 Web 开发人员的过程中，我发现许多关于 Web 开发的观念其实源于人们根据自己的经验进行的推断，而这些经验与我所经历的并不重合。自从我四年前离开 Mozilla 以来，我大部分时间都专注于开发 Web 应用，同时也在博客上投入了大量时间。令人惊讶的是，这两者在工具、架构或所使用的 Web 平台特性方面几乎没有交集。就像博客和应用分别代表了 Web 开发领域中截然不同的方向一样。</p><p></p><p>我的 Web 应用主要基于 TypeScript 编写，而我的博客几乎不使用客户端 JavaScript。我的应用通过双向数据同步变得相当复杂，而我的博客是静态的、只读的。我的应用使用 webpack 进行模块打包，使用 Playwright 进行端到端测试，采用一个组件框架和状态管理库。相比之下，我的博客并没有使用这些工具和技术。如果你主要与其中一个领域打交道，很容易认为这就是 Web 开发的全部。然而，事实上，Web 开发可能比我们想象的要更加多元化和丰富。</p><p></p><p><h2>结论</h2></p><p></p><p>除了上述观念外，还有一些其他的观念也让我感到惊讶。但上述内容的共同主题是，人们很容易假设自己的 Web 经验能够代表整个 Web 开发领域。</p><p></p><p>从浏览器开发转向 Web 开发是一次令人惊叹的体验，因为它比我知道的要广泛和深刻得多。我对 Web 开发者产生了更深的敬意，尤其是那些在小型 Web 商店和初创公司工作的人。他们的 Web 应用的成功与否直接关系到他们的生计。</p><p></p><p>如果你正是这样的一名开发者，我希望通过阅读这篇文章，你能确信，即使浏览器工程师、标准编辑和其他 Web 开发者坚持采用特定的方法，你对自己的约束有着更深入的了解，并且你可能能够做得很好。</p><p></p><p>原文链接：</p><p><a href=\"https://birtles.blog/2024/01/06/weird-things-engineers-believe-about-development\">https://birtles.blog/2024/01/06/weird-things-engineers-believe-about-development</a>\"</p></p>",
    "publish_time": "2024-02-01 10:24:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "多模态+大模型会带来哪些“化学反应”？",
    "url": "https://www.infoq.cn/article/z61j9qm9yX9Tq7N2p8tA",
    "summary": "<p>在当今这个信息爆炸的时代，多模态数据无处不在，而大模型的崛起为处理这些复杂数据提供了强大的工具。如何将这两者完美结合，释放出更大的价值？本期节目将深入探讨这个话题。</p>\n<h1>直播亮点：</h1>\n<ul>\n<li>揭秘多模态大模型的最新前沿进展</li>\n<li>大模型和多模态技术，对于消费者体验带来哪些变化</li>\n<li>在关键业务场景更好地发挥大模型能力，存在哪些挑战</li>\n<li>在大模型时代，技术人员的关键思维方式如何转变</li>\n</ul>",
    "publish_time": "2024-02-01 11:36:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ALL IN AIGC 新时代，共探行业变革之路｜InfoQ 合作伙伴年度盛典",
    "url": "https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT",
    "summary": "<p>20234年 1 月 31 日，InfoQ 极客传媒合作伙伴年度盛典在前门 blue note 圆满落幕。时隔三年，InfoQ合作伙伴年会回归线下，本届合作伙伴年会围绕“有被 Q 到”这个主题精彩展开，Q 代表着 Quality（坚守品质）、Quick（敏锐、速度和专业能力）、Question（不断求知、探索、创新），InfoQ 全体同学“Cue ”各位合作伙伴相聚一堂，分享这一年的感悟与收获，共话技术前沿与商业创新。</p><p></p><p></p><h4>共筑开发者生态 引领AI创新</h4><p></p><p>&nbsp;</p><p>回首2023年，AI技术的璀璨光芒照亮了各行各业，带来了翻天覆地的变革，也催生了无数的创新机遇。InfoQ也在这股变革的潮流中与时俱进，与合作伙伴们紧密携手，共同探索和实践着技术和新领域带来的无限可能。</p><p>&nbsp;</p><p>极客邦科技创始人 &amp;CEO 霍太稳（Kevin）在新年致辞中表示，在生成式AI时代，极客邦科技在积极探索和改变，未来将从共建优质内容、扶持创新企业/业务、保证客户体验三方面进行创新，也是极客邦科技对朋友们做出的承诺。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/2d/2ddc620a200d2301dcf3630ad9e27e49.jpeg\" /></p><p>极客邦科技创始人 &amp;CEO 霍太稳（Kevin）</p><p>&nbsp;</p><p></p><h5>共建优质内容：从 Demo&nbsp;到产品化再到产业化，帮助企业培育开发者生态</h5><p></p><p>过去一年，InfoQ见证并报道了基础大模型、行业大模型在金融、制造、泛互等领域的探索和实践。未来一年，我们的内容、会议将继续瞄准“AI+场景化”的这个方向，只要是这个主题下面对开发者、对社区有价值的内容，我们坚决欢迎朋友们与我们共建。</p><p>&nbsp;</p><p></p><h5>扶持创新企业/业务：从流量到内容到出海，借助AI全方位帮助早期业务/企业成长</h5><p></p><p>过去几年，InfoQ极客传媒在推动数字化转型和创新创业方面取得了显著成果。与100+生态伙伴共同推出【数字化转型专区】，展示了中国数字化转型的创新技术和数字化人才培养成果。发布了创新创业“成长计划”，为创新企业提供媒体传播、展示区和CXO群体链接等支持。</p><p>&nbsp;</p><p>此外，InfoQ极客传媒还与NVIDIA初创加速计划合作，为TGO会员企业提供创业指导。去年，InfoQ成功助力云器科技的产品技术发布，并受到广泛关注。同时，TGO鲲鹏会也在全球多地举办活动，为会员提供出海业务指导和合规建议。</p><p>&nbsp;</p><p>新的一年，InfoQ极客传媒将结合AI技术，为创新企业/业务提供更加强大的支持。推动技术创新和业务拓展，为合作伙伴和会员提供更优质的服务和支持。</p><p>&nbsp;</p><p></p><h5>优化客户体验：服务到客户满意为止，合同期止不满意，持续服务</h5><p></p><p>极客邦科技始终坚守一个原则：客户体验坚决放在首位。霍太稳也向现场的合作伙伴郑重承诺，在合同期限内，如对服务有任何不满或建议，极客邦科技将全力以赴，持续优化服务内容和质量，直至达到客户满意为止。</p><p>&nbsp;</p><p>霍太稳表示，极客邦科技第一季度的主题是“一具体，就深刻”。2024年，形势可能依然严峻，希望能够与合作伙伴携手共同实现“全面进化”，最终共同实现增长！</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c5/c54bc64eb49209e7758382a16ccd1a60.png\" /></p><p></p><p></p><h4>汇聚市场力量 驱动技术升级</h4><p></p><p></p><p>在年会现场，极客邦科技CGO汪丹进行 2024 年 InfoQ 极客传媒战略发布重点强调了开发者生态的重要性以及从科技创新到产业赋能的链接力量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a1/a14d93608007e43c72dc42a2ea1fc3e2.jpeg\" /></p><p>极客邦科技CGO 汪丹</p><p>&nbsp;</p><p>汪丹表示，InfoQ极客传媒一直致力于打造一个强大的开发者生态，通过科技创新推动产业发展。InfoQ极客传媒始终坚持以客户需求为导向，通过不断创新和演进，为企业提供更加贴合实际需求的解决方案和产品。</p><p>&nbsp;</p><p>汪丹介绍了InfoQ极客传媒在内容、会议、社交、企业服务等方面的全新布局。InfoQ极客传媒将通过打造大模型领航者、实践案例采访等内容产品，为开发者提供更加丰富、实用的学习资源。</p><p>&nbsp;</p><p>通过举办智能软件开发生态展、通用人工智能开发与应用生态展等会议活动，为开发者提供更多的交流和展示平台。通过打造opentalk区、社交区等社交场景，实现企业和开发者的双向奔赴。同时，InfoQ极客传媒还将深入千行百业，将优秀实践带到传统企业，推动产业的数字化转型和升级。</p><p>&nbsp;</p><p>面向2024年，InfoQ极客传媒将继续加大对国内大模型领域发展的跟踪和研究力度，为业内提供更多的选型依据和决策支持。并将积极拓展游学业务，坚守“不卷、不焦虑”的原则，帮助中国企业走向海外，拓展国际市场。为开发者和企业提供更加优质、高效的服务和支持，与更多的合作伙伴携手共进，共同推动行业的繁荣发展。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c9/c9fa3ac5e7c53ba599c072deeaba9f0c.jpeg\" /></p><p>InfoQ极客传媒 2024 战略发布</p><p></p><p></p><h4>达成战略合作 谱写开源新章</h4><p></p><p>&nbsp;</p><p>回顾InfoQ的整个发展过程，始终与许多开源社区紧密联系，在开放原子开源基金会成立后，InfoQ便成为银牌赞助商，以实际行动表达对开源社区的支持与感恩。为了更深层次地孕育与发展繁荣的开源生态体系，在本次年会盛典中，开放原子开源基金会与极客邦科技在 InfoQ 合作伙伴年会上举行战略合作发布仪式。</p><p>&nbsp;</p><p>开放原子开源基金会秘书长冯冠霖与霍太稳共同按下战略启动按钮，宣布正式达成战略合作，共同推动中国开源事业的发展。双方将共绘战略合作宏图，坚定致力于开源技术的普及推广和开源知识的深度传播，协同提升双方品牌在业界的声望和影响力，全力构建全面创新的开发者生态系统以及一套卓越的人才培养机制，重点推动AtomGit平台在全球范围内的广泛应用与深化拓展，促进开源技术生态可持续发展，为全球数字经济的发展注入新活力。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a6633b2470e49027d96ca2e7b0902949.jpeg\" /></p><p>开放原子开源基金会与极客邦科技达成战略合作</p><p>开放原子开源基金会秘书长冯冠霖（图左）与极客邦科技创始人 &amp;CEO 霍太稳（图右）</p><p>&nbsp;</p><p>此外，在启动仪式后，开放原子开源基金会开源大赛组委会办公室高级运营官曹海清对“开放原子开源大赛”进行了宣讲。该大赛旨在激发全球开发者的创新潜力，挖掘并提炼开源领域的宝贵精华。双方的战略合作，将有力驱动全球数字经济领域的深化拓展与升级转型，并在开源人才的孕育、选拔与输送方面发挥关键作用，有效促进全球范围内技术交流与协作的国际化进程。对此，InfoQ侧表示，“将继续发挥其全球性科技媒体品牌的影响力，为合作伙伴提供更加优质、高效的服务，推动全球科技创新与发展。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/25/253f6861b080ed0ffce0e2357db7700d.jpeg\" /></p><p>开放原子开源基金会开源大赛组委会办公室高级运营官 曹海清</p><p></p><p></p><h4>致敬荣耀时刻 见证科技星熠</h4><p></p><p>&nbsp;</p><p>在本次盛典上，InfoQ 极客传媒面向合作伙伴颁布了“2023年度新锐技术品牌奖”、“2023年度技术影响力引领品牌”、“2023年度技术传播创新案例”、“2023年度技术生态构建奖”、“2023年度合作伙伴奖”、“2023年度全球科技领导力推动者”奖项。</p><p>&nbsp;</p><p>以下是具体获奖企业及个人名单（排名不分先后）：</p><p><img src=\"https://static001.geekbang.org/infoq/76/76690345bf1ee08c4cb324a440138586.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b97c094e33e1660cbfb823f5aeafc4ca.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec5f86db84bb6e8ed515e587a234b569.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10c55373c4baa97492a1f14eb7accd00.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec4c6b0114b4b7bfadad332fb170cc32.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d12732a2644d0f6e2d4629206f51e09.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/7095485c60758114021788988f3d4e15.jpeg\" /></p><p></p><p></p><h4>行业变革时刻 趋势发展策略</h4><p></p><p>&nbsp;</p><p>2023 年AI 技术的突破直接拉动了自动驾驶、机器人和生成式 AI 的融资增长，数据和算力作为训练大模型的底座也受到了越来越多的关注和讨论。随着技术的不断进步，企业需要紧跟时代步伐，积极拥抱 AI 与数字化，以实现更高效、更智能的业务运营。</p><p>&nbsp;</p><p>InfoQ研究中心研究总监兼首席分析师姜昕蔚带来了主题为《2023年中国软件技术洞察及2024年趋势预测》的主旨演讲，为现场观众深入解析了中国软件技术的发展现状和未来趋势。姜昕蔚表示，在AI技术的推动下，数据库和算力技术将继续升级加强，以应对日益复杂的数据处理需求和计算挑战。随着数据量的爆炸式增长和计算任务的多样化，传统的数据库和算力架构已难以满足当下的需求，行业亟需更加高效、灵活和智能的解决方案；数据库和算力的升级将成为行业发展的必然趋势。与此同时，云原生轻量、灵活和高效的特性，与边缘计算的低延迟、高带宽和数据处理能力也将为AI技术的发展提供更加广阔的空间和更加丰富的应用场景。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/6e/6eff7ee39d9e330d78f9062931aae867.jpeg\" /></p><p>InfoQ研究中心研究总监兼首席分析师 姜昕蔚</p><p>&nbsp;</p><p>AIGC时代不仅对技术提出了更高要求，品牌营销与市场运营更是出现了新变革。技术品牌市场运营必须刷新认知、深挖用户需求，并融合前沿科技，打造差异化的技术品牌形象。同时，市场运营需依托数据分析，洞察市场动态与用户偏好，预测未来趋势。</p><p>&nbsp;</p><p>在本次年会的开放麦环节，华为云中国区行业营销负责人刘丽丽分享了她的经验和洞悉。她表示，2024年在稳定性成为前提的当下市场环境中，品牌工作的专业性和创新性显得尤为重要。对于不同发展阶段的企业来说，定义一个清晰可落地的品牌价值主张非常重要，因为作为连接品牌和产品的桥梁，价值主张对于牵引产品进步和行业市场拓展具有关键作用。2023年在拓展零售市场时，华为云通过深入客户市场，洞悉客户需求，提出了“共筑新时代伟大品牌”的价值主张，得到了众多客户的认同和认可，从而All in 华为云。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/6841db98e9b5499baa08193b101bda91.jpeg\" /></p><p>&nbsp;</p><p>无独有偶，大模型农场LLMFarm创始人宜博同样认为创新力和创造力在AI时代尤为重要。他认为，随着大模型的不断发展，未来的AI系统将具备更强的创新能力和自主决策能力，这将为企业品牌营销带来更多的机遇和挑战。“随着大模型的普及和应用，非AI项目将逐渐失去投资吸引力。”其这一观点为市场人敲响了警钟，在未来的工作中大家需要更加关注“如何利用大模型提升效果和效率，以适应行业变革的趋势。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4fd468ca9f6bec20b262bce9c5ec32e.jpeg\" /></p><p></p><p>AIGC时代迎来变革的不仅是营销层面，运营领域也迎来了新的可能。零一万物开源负责人、开源社联合创始人林旅强表示，随着AIGC时代的到来，以大模型优化运营流程、加速社区拓展的过程中，更要不忘开源初心，坚持“Community Over Code”路线，深刻理解“开源项目在自身的商业模式中的价值”，显得尤为重要。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/28/28d0fb879a9b7ad4920fab5e52ca9d3b.jpeg\" /></p><p></p><p>这种紧密的联系其实不仅限于技术和商业模式之间，更体现在行业之间的生态构建。企业必须持续地技术创新和产品迭代，以满足用户不断变化的需求。只有从企业之间的竞争逐渐演变为生态间的共建，与合作伙伴共同打造具有市场竞争力的产品和服务，才能在AIGC时代的浪潮中立于不败之地，实现可持续的发展和增长。</p><p>&nbsp;</p><p>总而言之，AI在2023年掀起的这波巨大波澜，需要合作伙伴能够紧跟时代步伐，抓住AI带来的机遇，共同迈向更加美好的明天。</p>",
    "publish_time": "2024-02-01 12:03:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "刚上线就崩了？字节版GPTs征战国内市场：无需编码，快速创建AI聊天机器人",
    "url": "https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij",
    "summary": "<p>2月1日，字节跳动旗下AI聊天机器人构建平台Coze国内版（中文名：扣子）正式上线。Coze是一款应用程序编辑平台，用于开发下一代AI聊天机器人。无论用户是否拥有编程经验，都可在该平台上快速创建各类聊天机器人，并将成果部署在不同社交媒体与消息应用当中。</p><p>&nbsp;</p><p>据悉，Coze 由字节跳动新成立的AI部门 Flow 开发，去年年末在海外先行上线。与海外版相比，国内版在功能上并无差异，只是推送渠道略有不同。</p><p>&nbsp;</p><p>有网友反馈，Coze 刚上线就崩了，InfoQ实测发现，Coze 创建界面长时间显示确认中，或许与刚上线流量过大有关。不过截至发稿前，Coze响应时间恢复了正常。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/19/19c3274341257b4c62176b23bce3f3ee.png\" /></p><p></p><p>体验地址：</p><p><a href=\"https://www.coze.cn/home\">https://www.coze.cn/home</a>\"</p><p></p><h2>Coze背后神秘的AI部门Flow：由字节大模型领队牵头，聚焦AI应用层</h2><p></p><p>&nbsp;</p><p>据36kr 2023年11月末报道，字节跳动近期成立了一个新AI部门Flow，技术负责人为字节跳动技术副总裁洪定坤。有知情人士表示，Flow部门的业务带头人，为字节大模型团队的负责人朱文佳。</p><p>&nbsp;</p><p>公开信息显示，朱文佳是业界知名的架构师，曾担任百度搜索部主任架构师，是百度网页搜索部技术副总监杨震原手下的得力干将。2014年，杨震原受张一鸣邀约，离开百度入职字节跳动，现为字节跳动公司副总裁、火山引擎业务负责人。2015年，朱文佳加入字节跳动，被称为“今日头条里算法技术的Top3人选”，并在入职4年后担任今日头条CEO。2021年2月，朱文佳调任成为为Tik Tok产品技术负责人。</p><p>&nbsp;</p><p>据悉，朱文佳是字节跳动大模型业务中的“隐形领队”——是字节跳动语言大模型负责人与图像大模型团队负责人的间接和直接汇报对象。在内部人士眼中，朱文佳拥有“综合的工程和技术管理经验”。</p><p>&nbsp;</p><p>与朱文佳背景类似，洪定坤也曾在百度任职过，曾担任百度贴吧的技术经理。2014年，洪定坤加入字节跳动，现为字节跳动技术副总裁。</p><p>&nbsp;</p><p>据了解，Flow部门主要聚焦在AI应用层。在 Flow 此前发布的活水招聘帖中，Flow 称其是字节跳动旗下 AI 创新业务团队，已经在国内和海外分别上线豆包和 Cici 两款产品，还有多个 AI 相关创新产品在孵化中。</p><p>&nbsp;</p><p>其中，豆包属于AI 对话产品，Cici 与Coze都属于AI聊天机器人创建平台，可供用户创建和共享自己的聊天机器人。此外，字节跳动旗下的AI产品还包括AI聊天机器人创建平台ChitChop和AI角色创建与互动平台BagelBell，这两款产品分别由 POLIGON 和SPRING(SG)PTE.LTD.开发运营。Cici和CHitChop主要用于娱乐场景，提供基于虚构性格的浪漫伴侣型机器人，而Coze则提供可简化办公流程的工作机器人。</p><p>&nbsp;</p><p>Cici、Coze、ChitChopt和BagelBell这四款产品均在过去三个月内登陆海外，目前已经拥有数百万下载量。其中，只有Coze目前登陆了美国市场，其他三款均未向美国和欧洲市场开放。科技大厂通常会先在监管审查较弱的小型市场上测试产品，之后再逐步推向美国和欧盟。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/72e14a648302c9e10fbb2400dcab16e0.png\" /></p><p></p><p>有网友表示自己非常看好Coze，一是2B产品对质量要求高，2C产品市场相对有容忍度，二是核心不在产品在流量，字节有地表最强流量能力，比如一键上架豆包。“字节聚焦AI应用非常聪明，基础模型差距很大，借‘基础能力’用流量起势，积累数据再反哺‘能力’，策略清晰可执行”，该用户在X上评论道。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/55/55c07e8586cc210fcd9921dd0bbcf758.png\" /></p><p></p><p>根据Google Play商店的数据，Cici是四款中最受欢迎的应用产品，目前下载量已经超过1000万次。字节跳动并未公开用于支持这些产品的底层大语言模型。字节跳动发言人Jodi Seth表示，这些应用依赖于OpenAI的GPT技术，可通过微软Azure许可证进行访问。</p><p>&nbsp;</p><p>据Forbes报道，这四款字节跳动新应用均在隐私政策中包含警告（与字节跳动的其他应用保持一致），称这些应用可能与其他“我公司内部实体”共享用户信息。字节跳动发言人Seth也证实，字节跳动的中方员工可能会访问到应用内的用户数据，但将严格遵守公司的访问控制与审批流程。</p><p>&nbsp;</p><p>在测试对话中，Coze和Chitchop均就某些敏感话题给出比较完整的描述，但仍存在幻觉问题。但在注册账户测试Cici（与豆包App几乎一模一样）时，该应用屡屡报错，最终导致测试无法进行。</p><p>&nbsp;</p><p>有网友分析，字节跳动的这四款AI产品应该是不同团队间产品的 PK。对于素有“APP工厂”之称的字节跳动来说，四大产品“赛马”可以提升团队创新性，同时也可看出字节跳动对AI领域的重视程度。</p><p></p><h2>如何用扣子快速创建AI聊天机器人？</h2><p></p><p>&nbsp;</p><p>自去年第四季度在海外取得很好的成绩之后，字节跳动今天正式推出“Coze 扣子”AI Bot 开发平台。</p><p></p><h4>Coze具备哪些功能？</h4><p></p><p>&nbsp;</p><p>无限拓展的能力集</p><p>&nbsp;</p><p>扣子集成了丰富的插件工具，可以极大地拓展 Bot 的能力边界。</p><p>&nbsp;</p><p>内置插件：目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。 你可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。例如使用新闻插件，打造一个可以播报最新时事新闻的 AI 新闻播音员。自定义插件：扣子平台也支持创建自定义插件。 你可以将已有的 API 能力通过参数配置的方式快速创建一个插件让 Bot 调用。</p><p>&nbsp;</p><p>丰富的数据源</p><p>&nbsp;</p><p>扣子提供了简单易用的知识库功能来管理和存储数据，支持 Bot 与你自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到知识库中。这样，Bot 就可以使用知识库中的内容回答问题了。</p><p>&nbsp;</p><p>内容格式：知识库支持添加文本格式、表格格式的数据。内容上传： 你可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至知识库，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在知识库内添加自定义数据。</p><p>&nbsp;</p><p>持久化的记忆能力</p><p>&nbsp;</p><p>扣子提供了方便 AI 交互的数据库记忆能力，可持久记住用户对话的重要参数或内容。</p><p>&nbsp;</p><p>例如，创建一个数据库来记录阅读笔记，包括书名、阅读进度和个人注释。有了数据库，Bot 就可以通过查询数据库中的数据来提供更准确的答案。</p><p>&nbsp;</p><p>灵活的工作流设计</p><p>&nbsp;</p><p>扣子的工作流功能可以用来处理逻辑复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大语言模型 LLM、自定义代码、判断逻辑等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如：</p><p>&nbsp;</p><p>创建一个搜集电影评论的工作流，快速查看一部最新电影的评论与评分。创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。</p><p></p><h4>Coze快速上手教程</h4><p></p><p>&nbsp;</p><p>Coze的主页面非常简洁，点击“创建Bot”即可创建属于自己的Coze机器人。可以自行设置机器人的名称以及功能介绍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f843506a8e644ea731a46e0f30b2754c.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fe/fe0cf437234b93b5d5a98cef53dd7002.png\" /></p><p></p><p>创建好后，可以编辑机器人提示词，比如可以描述机器人的角色、技能、约束条件等内容以定义机器人的预期行为。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c8/c8db6d4356cad6f7ee2509a5d5284e47.png\" /></p><p></p><p>由于AI机器人本身无法直接访问互联网，因此需要配合某些工具来获取数据或执行网上操作。</p><p>可以将各类插件工具添加到机器人内以扩展其功能，具体包括必应搜索、ByteArtist、图片理解、头条搜索等多个插件。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/95ba71088036d5e85cf0e19d0511d386.png\" /></p><p></p><p>创建好机器人后，可以选择发布平台，国内版Coze发布平台包括飞书、微信。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/060c86c5e3969b6083fed240956a9871.png\" /></p><p></p><p>Coze还提供了Bot 商店，包含工具、娱乐、生活方式等多个类目，可以看到其他开发者是如何创建机器人的，并从中找寻灵感，甚至可以基于该机器人创建一个副本，再进行个性化调整。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d9cbfe0f35d63a957599be08e7776f3f.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c780d27d43c755e4e1a2af5a77427ea1.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4e/4e3572701727592d19b2d1e3071b24e3.png\" /></p><p></p><p>参考链接：</p><p><a href=\"https://www.coze.cn/docs/guides/welcome\">https://www.coze.c</a>\"<a href=\"https://www.coze.cn/docs/guides/welcome\">n</a>\"<a href=\"https://www.coze.cn/docs/guides/welcome\">/docs/</a>\"<a href=\"https://www.coze.cn/docs/guides/welcome\">guides/</a>\"<a href=\"https://www.coze.cn/docs/guides/welcome\">welcome</a>\"</p><p><a href=\"https://www.forbes.com/sites/emilybaker-white/2024/01/16/tiktok-bytedance-ai-chatbots-openai/?sh=4bb1b0fba240\">https://www.forbes.com/sites/emilybaker-white/2024/01/16/tiktok-bytedance-ai-chatbots-openai/?sh=4bb1b0fba240</a>\"</p>",
    "publish_time": "2024-02-01 14:46:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "金山云升级全栈云计算体系，全方位承接大模型应用",
    "url": "https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty",
    "summary": "<p>随着云原生、AIGC、大模型等新兴技术的迅速发展，智能化时代开启。云计算也正全面步入3.0时代，即云计算和人工智能深度融合的阶段。在这个阶段，人工智能技术成为云计算进一步释放潜力的核心推动力。</p><p>&nbsp;</p><p>近日，金山云举办了「云+人工智能·时代新机遇」媒体沟通会，金山云副总裁钱一峰、金山云公有云产品中心负责人孙晓、金山云人工智能与大数据产品中心负责人徐寅斐在会上分享了金山云在基础能力、平台能力和模型服务等方面的技术历程和落地实践。通过技术自研和升级，金山云已初步建成人工智能时代民用领域全栈的云计算体系。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c7/c7a4ac5ef7a3b8f9f69f7f66f82246ec.png\" /></p><p></p><h2>夯实基础，全面升级人工智能服务能力</h2><p></p><p>&nbsp;</p><p>全球数字化浪潮风起云涌，云计算作为数字经济重要的支撑基础，正发挥越来越重要的作用。其中，智能化作为云计算3.0时代的重要理念，已成为数字基础设施升级的重要驱动力。知名调研机构IDC于2023年发布的报告显示，未来5年，以大模型、生成式人工智能驱动的下一代人工智能有望带动整体云产业穿过下行阶段，重回高增长时代。</p><p>&nbsp;</p><p>顺应趋势，金山云深入探索前沿技术，围绕AIGC升级核心计算、存储、网络等产品，从多个维度全面拥抱人工智能。</p><p>&nbsp;</p><p>金山云副总裁钱一峰强调，在此次技术升级中，在网络上采用了目前业界流行的高性能RoCE网络。在成本方面，同样的集群如果采用IB网络，网络本身的成本几乎占整个算力群的一半，如果用RoCE网络，只占5%到10%，所以国内很多大厂都在往RoCE网络转，金山也是如此。</p><p>&nbsp;</p><p>在计算方面，金山云第七代云服务器X7搭载第四代英特尔®至强®可扩展处理器、支持英特尔® AMX原生加速能力，CPU性能较上代最高提升60%，内存升级至DDR5，频率较上一代性能提升50%。融合金山云自主创新的加速技术，云服务器X7可有效提升模型推理性能。</p><p>&nbsp;</p><p>根据数据从极热到极冷的不同热度，金山云对象存储已覆盖标准存储、低频存储、深度低频存储、归档存储、深度冷归档存储和全闪存储等不同访问热度的存储产品。其中，金山云对象存储KS3极速型最高可提供1Tbps/PB的兑付带宽，相较基于机械硬盘（HDD）的对象存储性能提升了上百倍，能为AIGC、存算分离和高性能计算等场景提供强有力的存储解决方案。</p><p>&nbsp;</p><p>在底层技术基础上，金山云此次升级还新增了金山轻舟智问以及一些合作的商业模型。</p><p>&nbsp;</p><p>一直以来，业内有一种观点认为一个大模型就解决所有问题，因为所有人都可以去调用大模型的API，但根据用户的反馈和场景需求来看，把所有问题都扔给一个大模型去解决是非常昂贵的，更经济的做法是将不同的场景用不同规模的模型去解决。将这些平台打包在一起，可以为客户提供一个综合性价比最高的解决方案。</p><p>&nbsp;</p><p>过去这一年，金山云一直在做两件事——夯实基础和做长长板。在云计算方上重点打磨四大基础能力：速度快、性能好、成本低、稳定易用；而做长长板找增量则体现在金山云在混合云、分布式云上的很多创新。</p><p>&nbsp;</p><p>具体而言，在在公有云的核心产品上第一要做到一切皆标准，提升用云效率。第二，要做到软硬结合，最大限度提升客户用云性价比。</p><p>&nbsp;</p><p>在混合云方面，客户拥有专属区或者专属集群，这既让客户拥有了云下的独立，又能让其享受云上的规模红利和弹性。</p><p>&nbsp;</p><p>此外，在Serverless化上也有了新进展。Serverless化过去是局限在算力层面，但随着存算分离场景的流行已经渗透到PaaS层面，如今MySQL也已经做到了Serverless化，另外也引入了开源向量数据库。</p><p>&nbsp;</p><p>面向大模型应用场景，金山云推出互信虚拟私有网络（简称“互信VPC”），解决模型厂商和应用厂商互信的问题。相对于标准VPC，互信VPC对进出VPC的通信行为有着更为严格的控制，帮助客户解决合规和互信问题。针对HTTP、HTTPS等应用层负载场景，公司推出应用型负载均衡ALB(Application Load Balancer)，单实例最大支持100万QPS。与云原生场景融合，客户在使用金山云容器服务KCE产品或自建K8S集群时，ALB都可作为Ingress部署，为业务提供网络流量调度服务。</p><p></p><h2>勇立潮头，做大模型助力者</h2><p></p><p>2023年被称为大模型“元年”。公开数据显示，从年初到年末，国内大模型数量超过200个。与“大模型”数量呈倍数级增长相反的是，真正被调用的大模型却相对较少。在本次大模型浪潮中，金山云坚持中立定位，充分发挥自身的底座和平台能力，做大模型的助力者。</p><p>&nbsp;</p><p>在洞察到模型供需方的痛点后，金山云于去年6月率先推出MaaS互信推理专区方案（以下简称“MaaS 1.0”），在大模型厂商、用户和金山云之间建立互信，以解决模型及数据的互信问题。本次沟通会上，金山云发布MaaS互信推理专区方案2.0（以下简称“MaaS 2.0”）。</p><p>&nbsp;</p><p>在MaaS 1.0基础上，MaaS互信推理专区方案2.0以金山云IaaS和PaaS为底座，可实现云上LangChain的一键部署，默认对接多个生态合作商业大模型和开源大模型。同时支持包括BGE、Bert等在内的Embedding模型，能无缝对接金山云全托管向量数据库Milvus，提供面向企业开发者的简单易用、安全可信的一站式推理应用部署平台。此外，MaaS 2.0支持通过标准化的API接口和Web前端界面，实现包括模型推理和知识库搭建的RAG大模型场景应用。为进一步增强云上运行的安全性，MaaS 2.0还提供容器服务加密镜像解决方案，依托金山云裸金属服务，实现在金山云容器服务中从镜像加密、加密镜像上传、解密镜像运行的全流程模型安全运行。</p><p>&nbsp;</p><p>同时，为满足行业客户的需求，金山云探索大模型时代企业赋能新机遇，围绕企业级知识助手场景制定了“一三一四”产品全景规划，即一套能力（金山云轻舟智问）、三个模型（行业语言模型、文本分片和Embedding模型）、一个平台（金山云瀚海平台）及四大功能（微调推理、数据加速、智能检索和文档智能），围绕生成式人工智能构建应用落地、模型训练微调、平台支撑的全栈能力。基于“一三一四”规划，金山云将分别针对应用型客户和平台型客户输出多项原子能力。目前，金山云轻舟智问知识助手产品已完成应用以及Embedding模型、多路召回算法和智能数据切片模型等核心技术的建设，计划优先在公共服务和法律场景落地。</p><p>&nbsp;</p><p>与人工智能的结合，给了云更多的想象空间，也给各行业都带来了新的生产力。随着技术的进一步完善和落地，云计算将迎来更强更久的生命力。金山云将持续围绕客户需求“练内功”，携手生态伙伴以差异化打法布局未来。</p>",
    "publish_time": "2024-02-01 16:09:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "性能接近GPT-4，Mistral-Medium遭泄露？CEO最新回应来了......",
    "url": "https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV",
    "summary": "<p></p><h2>开源模型疑似泄露，开发者纷纷下场测试</h2><p></p><p>&nbsp;</p><p>近日，一则关于“Mistral-Medium 模型泄露”的消息引起了大家的关注，该消息在Hacker News和X（原Twitter）上持续发酵。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/89/89aab1008554b6e25fb0bd71e9c02a03.jpeg\" /></p><p></p><p>&nbsp;此消息之所以受到这么多关注，与一款叫做“Miqu”的神秘模型有关。</p><p>&nbsp;</p><p>1月28日左右，一位名为“Miqu Dev”的用户在开源 AI 模型和代码共享平台<a href=\"https://huggingface.co/miqudev/miqu-1-70b\">HuggingFace 上发布了一组文件</a>\"，这些文件共同构成了一个看似新的开源大语言模型，名为“miqu-1-70b”。</p><p>&nbsp;</p><p>开源地址：<a href=\"https://huggingface.co/miqudev/miqu-1-70b\">https://huggingface.co/miqudev/miqu-1-70b</a>\"</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1f/1fb6232a05547d602936f41d64ab00aa.png\" /></p><p></p><p>&nbsp;在Hugging Face平台的miqu-1-70b项目上，多条内容指出这款新的大语言模型的“提示格式”以及用户与其交互的方式与 Mistral AI公司正在研发中的Mistral Medium模型相同。同一天，4chan 上的一位匿名用户（可能是“Miqu Dev”）<a href=\"https://boards.4chan.org/g/thread/98696032#p98697258\">在 4chan 上发布了 miqu-1-70b 文件的链接</a>\"，该项目的受关注程度逐渐升高。</p><p>&nbsp;</p><p>模型放出后，有业内人士猜测，这个神秘泄露的miqu-1-70b可能就是MistralAI模型的Medium或者过往混合专家测试版本。</p><p>&nbsp;</p><p>一些人用户在X上分享了该模型的发现，以及该模型在常见 LLM 任务（通过称为基准的测试来衡量）上表现出的异常出色的性能，甚至接近了 OpenAI 的 GPT -4 在EQ 工作台上的表现。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c7/c7d5fb572e8b0626b98b59e44cc73cda.png\" /></p><p></p><p>有用户测试了这款神秘模型和Medium模型后表示：“尽管可能有些晚了，但现在我100%确信miqu与Perplexity Labs上可访问的Mistral-Medium模型是同一个模型。该用户还称：“它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。”</p><p>&nbsp;</p><p>这款备受瞩目的生成式AI新星——miqu-1-70b自然要被拿来与各位前辈进行一番比较。有测试者用4项德国数据保护测试对这款神秘模型做了更深入的测试，方法如下：</p><p>&nbsp;</p><p>这里通过4项德国在线数据保护培训/考试，对这套新模型的表现加以验证。测试数据、问题及所有说明均为德语，而答题卡则为英语。这考察了模型的翻译能力和跨语言理解能力。在提供信息之前，测试者会用德语指示模型：接下来向你提供一些信息，请记住相关内容，并回答“确定”以确认已经理解其内容。这一步是为了测试模型的指令理解与遵循能力。在提供关于某个主题的全部信息之后，测试者会向模型提出测试问题。这是一套包含三个选项的多选题，但首题采用A/B/C选项，末题为X/Y/Z选项。每项考试包含4至6道题，测试流程总计18道选择题。根据模型给出的正确答案数量进行排名，先测试事先提供课程内容后的成绩，再测试没有提供信息下的盲答成绩（作为决胜局）。所有测试均单独运行，每次测试间会清除上下文，保证会话之间的记忆/状态不相互干扰。</p><p>&nbsp;</p><p>还进行了包括<a href=\"https://github.com/SillyTavern/SillyTavern\">SillyTavern</a>\"前端、<a href=\"https://github.com/LostRuins/koboldcpp\">koboldcpp</a>\"后端（对于GGUF模型）在内的其他测试，另外还预先设置确定性生成，以尽可能消除随机因素并进行有意义的模型间比较，也包括注明官方提示词格式。</p><p>&nbsp;</p><p>以下为详细注释、排名基础和其他评论与观察发现：</p><p><a href=\"https://huggingface.co/miqudev/miqu-1-70b\">miqudev/miqu-1-70b</a>\"&nbsp;GGUF Q5_K_M, 32K上下文, Mistral格式:</p><p>❌&nbsp;正确回答了4+4+4+5=17/18道选择题，而在盲答阶段，正确答案题为:&nbsp;4+3+1+5=13/18。</p><p>❌&nbsp;未能按照要求用“确定”来回应数据输入。</p><p>&nbsp;</p><p>经过了多项测试后，结果显示miqu-1-70b的效果着实不错，测试者出于个人猜测，miqu-1-70b可能是一套外泄的MistralAI概念验证旧模型，从开发次序来讲应该不会比Mixtral更晚。此外，测试者也表示，在测试过程中注意到了几个有趣的点，从这几个方面来看，miqu-1-70b跟Mixtral存在诸多相似：</p><p>&nbsp;</p><p>优秀的德语拼写与语法能力。支持双语，可在回复中添加翻译。能够为回复添加注释和评论。</p><p>&nbsp;</p><p>但测试者也表示，在测试中，miqu-1-70b仍无法与Mixtral-8x7B-Instruct-v0.1（4-bit）相媲美，不过性能仍比Mistral Small和Medium更好（亲自测试Medium时其表现相当糟糕，可能是API的问题）。但与测试者每天都在使用的Mixtral 8x7B Instruct相比，miqu也没有好太多。</p><p>&nbsp;</p><p>在这场miqu和Mistral Medium模型对比测试中，前阵子号称要干掉谷歌搜索的 Perplexity印度创始人Aravind Srinivas也在X上发表了自己的观点：</p><p>&nbsp;</p><p></p><blockquote>很多人问我Mistral的所有模型是否都基于Meta的Llama。特别是因为Mistral Medium在Perplexity Labs上的输出与miqu非常相似，而这种相似性是通过测试发现的。Mistral的CEO Arthur已经提供了一个清晰的解释，并确认这是一个来自早期访问客户的泄露。&nbsp;此外，Perplexity从未获得过Mistral Medium的权重访问权限。所以，当你在Labs上使用Mistral Medium时，我们只是将你的请求路由到Mistral支持的有效端点，而没有访问权重。泄露的权重实际上是量化版本，与NVIDIA TensorRT不兼容。&nbsp;此外，很多人在看到这个消息是本能地反应会认为Mistral不知道如何进行预训练，只是在LLama 2上构建。这是明显不真实的。Mistral 7b是一个由Mistral团队从头开始训练的模型，而Mistral 8x7b MoE也是通过使用他们自己的7b作为每个专家的初始化来训练的。所以很明显，这个团队知道如何从零开始训练自己的模型。Mistral Medium是从LLama后期训练的，可能是因为迫切需要一个接近GPT-4质量的API，以便早期客户使用。但是一个能够在计算和时间投入远少于Gemini Pro的情况下取得胜利的团队，现在他们有了更多的资金和计算资源，显然能够做到GPT-4级别的质量。&nbsp;当然，泄露是不好的。Mistral的胜利对社区来说是一件好事：无论是对学术界还是对初创公司。支持他们！</blockquote><p></p><p></p><h2>Mistral AI高层发声：是泄露了，但只是个旧版本</h2><p></p><p>在Mistral AI的新模型遭泄漏这一话题热度不断上涨之时，据外媒最新消息，Mistral AI联合创始人兼CEO&nbsp;Arthur Mensch 在 X 上澄清：</p><p>&nbsp;</p><p></p><blockquote>“一个我们早期客户的热情员工泄露了一个我们公开训练和发布的老模型的量化（带水印）版本。为了尽快与一些特定的客户开始合作，我们在获得整个集群访问权限后立即从Llama 2重新训练了这个模型——预训练在Mistral 7B发布的那一天完成。自那时以来，我们取得了很好的进展——敬请期待！”</blockquote><p></p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/ed/edbdac6347f53ab3b41ca879cd4bf3b7.jpeg\" /></p><p></p><p>有趣的是，Mensch 并没有要求删除 HuggingFace 上的帖子，而是留下那些评论说发帖者“可能会遭到模型所属公司追责”的评论。</p><p></p><h2>Mistral AI创始团队成员均来自谷歌和Meta</h2><p></p><p>&nbsp;</p><p>Mistral AI是一家总部位于巴黎的欧洲公司，由 Arthur Mensch 和 Guillaume Lample以及 Timothée Lacroix于 2023 年 2 月联合创立，并于去年 12 月 10 日宣布筹集了 3.85 亿美元，仅半年多的时间，该公司估值近 20 亿美元。Mistral AI 在刚成立且没有任何产品时就已筹集了 1.05 亿美元。</p><p>&nbsp;</p><p>因此，它也成为继德国 Aleph Alpha 在去年11月筹集了 5 亿欧元之后，第二家筹集到如此多资金的欧洲人工智能初创公司。</p><p>&nbsp;</p><p>Mistral AI一直在研究如何提高模型性能，同时减少为实际用例部署llm所需的计算资源。Mistral 7B是他们创建的最小LLM，它为传统的Transformer架构带来了两个新概念，Group-Query Attention(GQA)和Sliding Window Attention(SWA)。这些组件加快了推理速度，减少了解码过程中的内存需求，从而实现了更高的吞吐量和处理更长的令牌序列的能力。</p><p>&nbsp;</p><p>Mistral AI 首席执行官 Arthur Mensch，31 岁，在 Google 人工智能实验室 DeepMind 工作了近三年。Mistral 的科学总监 Guillaume Lample 是 Facebook 母公司 Meta 在 2 月份推出的 LLaMA 语言模型的创建者之一。Timothée Lacroix 是 Mistral AI 的技术总监，也是 Meta 的研究员。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://twitter.com/Yampeleg/status/1751837962738827378\">https://twitter.com/Yampeleg/status/1751837962738827378</a>\"</p><p><a href=\"https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op\">https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op</a>\"</p><p><a href=\"https://analyticsindiamag.com/mistral-ai-challenges-dominance-of-openai-google-meta/\">https://analyticsindiamag.com/mistral-ai-challenges-dominance-of-openai-google-meta/</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=39175611\">https://news.ycombinator.com/item?id=39175611</a>\"</p>",
    "publish_time": "2024-02-01 16:13:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Linus开喷谷歌内核贡献者：你的代码是垃圾！网友：我们熟悉的Linus回来了",
    "url": "https://www.infoq.cn/article/l192hquuFavBJTgQqE8T",
    "summary": "<p>近年来脾气愈发温和的Linus Torvalds，刚刚在Linux内核邮件列表中发布一篇措辞最为激烈的帖子，矛头直指谷歌一位贡献者关于文件系统的建议。</p><p>&nbsp;</p><p>引发Linus愤怒情绪的导火索正是inodes，即索引节点。根据红帽给出的定义，inodes是“给定文件系统上特定元数据片段的唯一标识符”。</p><p>&nbsp;</p><p>过去几周以来，inodes在Linux内核邮件列表上引发了广泛争论，其中谷歌员工Steven Rostedt与Linus甚至就此事展开了激烈对抗。在题为《保证所有文件和目录都拥有相同inodes》的帖子中，作者认为在使用tar归档文件时，inodes仍可发挥作用。但Linus则反驳称inodes早已过时。</p><p>&nbsp;</p><p>Linus指出，“没错，inodes曾经地位特殊且拥有历史传承。但总的来看，我们应当努力摆脱这段破碎的历史。Indoes编号不再作为唯一的描述符。现在已经不是20世纪70年代，如今的文件系统早已发生重大转变。”</p><p>&nbsp;</p><p>但关于inodes的争论仍未平息。Rostedt最终建议称，所有inodes都应拥有唯一编号。</p><p>&nbsp;</p><p>Steven 于 1998 年开始研究 Linux 内核，并于 2001 年成为一名专业（付费）内核开发人员。目前，Steven Rostedt 是谷歌内核的开发者，也是 VMware 的开源程序员，负责维护 Linux 内核的实时稳定版本等。Rostedt 是PREEMPT_RT 补丁的原始开发者之一，并于 2004 年开始研究该补丁，目标是将 Linux 转变为实时设计的操作系统。他也是内部跟踪工具<a href=\"https://www.kernel.org/doc/Documentation/trace/ftrace.txt\">Ftrace</a>\"的主要作者、开发人员和维护者，该工具旨在帮助开发人员查找内核内部发生的情况。根据 Ftrace wiki，该工具可用于调试或分析用户空间之外发生的延迟和性能问题。</p><p>&nbsp;</p><p>Rostedt 参加了很多 Linux 基金会的活动，已在世界各地进行了 80 多场演讲，他也是 Linux 基金会技术咨询委员会 (TAB) 和 Linux Plumbers 编程委员会的成员。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a31708ed9c632283e671da30808baec.png\" /></p><p></p><p>Steven Rostedt</p><p>&nbsp;</p><p>Linus也终于按捺不住，捡起了近年来已经大大缓和的火爆脾气。</p><p>&nbsp;</p><p>2018年，在向Linux程序员和项目贡献者们就“邮件中的轻率攻击”道歉之后，Linus曾宣布要休息一下并寻求帮助，并承认这种粗暴的反应“既不专业，也没有必要。我把技术上的争论当成了个人恩怨……我知道这样不好，真的非常抱歉。”</p><p>&nbsp;</p><p>Linus之所以诚恳致歉，是因为Linux内核邮件列表其实是众多项目贡献者的共同工作场所。辱骂性帖子显然会破坏轻松愉快的工作氛围。毕竟Linux的发展还要仰仗于这些志愿贡献者和维护者，恶言恶语恐怕会损害项目的未来发展。</p><p>&nbsp;</p><p>在回应Rostedt关于唯一inodes编号的建议时，Linus先是表示“如非必要，勿增复杂性。”</p><p>&nbsp;</p><p>但在此之后，他就有点失控了。</p><p>&nbsp;</p><p>“我真的服了，别再照搬VFS层函数了。这玩意当初就有问题，现在一样要出问题。我不想讨论这种废话。”</p><p>&nbsp;</p><p>Linus对Rostedt的批评意见，主要集中在这位谷歌开发者对讨论主题存在理解偏差——Rostedt本人后来也承认了这一点。</p><p>&nbsp;</p><p>但在此之前，Linus的“大炮”已经轰轰作响：</p><p>&nbsp;</p><p></p><blockquote>你直接照搬了这条函数，却不理解它为什么会有相应的效果，这样的代码纯属垃圾。真让人心累。</blockquote><p></p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c9/c96c75faf08b721ddb49ab2e95411b6e.png\" /></p><p></p><p>争论持续一段时间之后，Linus开始以冷静的证据提出了更好的方法，希望解决Rostedt所关切的问题。这位写道，他倒不打算立刻跟进这件事，因为“我在这事上已经浪费了很多时间，手头还有不少亟待解决的工作，所以暂时就先放一放。”</p><p>&nbsp;</p><p>Rostedt则在回复中给出了似乎针锋相对的说法：</p><p>&nbsp;</p><p></p><blockquote>讽刺的是，我也有拖着没做完的工作，就是为eventfs的维护者倦怠支持小组编写文档！</blockquote><p></p><p>&nbsp;</p><p>不难看出，Rostedt言外之意是——你忙，我也忙，说的好像谁不忙似的。</p><p>&nbsp;</p><p>尽管引起了风波，但Linus也带来了令人振奋的好消息：Linux内核6.8-rc2版本已首次亮相。</p><p>&nbsp;</p><p>Linus指出，rc1中存在“一个amdgpu调度错误，可能导致桌面挂起（最终可以恢复，但需要等待足够长的超时，所以大多数用户可能会选择重新启动）。”</p><p>&nbsp;</p><p>“这个问题似乎产生了广泛影响。”</p><p>&nbsp;</p><p>Linus本人也遇到过btrfs bug，但由于发现较早，所以这个bug没有进入rc1。</p><p>&nbsp;</p><p>&nbsp;“总之，随着rc2版本逐渐趋于稳定，不少可能影响众多测试人员的问题正逐步得到解决。所以希望后续我们能修复掉更多比较复杂、不涉及常见核心设置的问题。”</p><p>&nbsp;</p><p>“所以大家可以亲自测试一下了，现在一切正常。我们还是靠谱的，对吧？”</p><p>&nbsp;</p><p>当然，Linus大佬在内核开发领域绝对靠谱，但在项目管理和沟通方面恐怕还值得商榷……</p><p></p><h2>Linus内心os：我也不想喷，但我忍不了</h2><p></p><p>&nbsp;</p><p>然而，大家似乎已经适应了这位Linux“独裁者”的暴脾气，甚至有网友表示，看不到Linus Torvalds怼人，失去了很多快乐。</p><p>&nbsp;</p><p>过去十多年里，被这位大佬怼过的开发者和公司不胜枚举。</p><p>&nbsp;</p><p>2013 年，Intel 公司的内核开发人员 Sarah Sharp 称 Torvalds 的行为是不专业的，称 Torvalds“主张进行人身恐吓和暴力行为。”Torvalds 随后指责 Sharp 把自己描述成受害者博取同情，不接受任何劝他应该改变的建议。</p><p>&nbsp;</p><p>在 2015 年发表演讲说到英伟达时，Torvalds 还曾转向一台摄像机说“so Nvidia fuck you”并竖起了中指。</p><p>&nbsp;</p><p>2015 年底，Sarah Sharp 宣布退出（Closing a door）内核社区。Sarah Sharp 当时说道，过去一年多时间她已经逐步终止了手中的各项社区工作，转交了 USB 3.0 主控制器驱动的维护工作，不再担任开源会议的内核协调员。她不再递交任何补丁和 bug 报告，不再向内核邮件列表写任何的建议。她声称，Linux 内核社区的互动是一种“潜在有毒的背景辐射”，充满了性别歧视、语言暴力和不尊重人。</p><p>&nbsp;</p><p>Torvalds 最终也意识到他的言行会伤害到社区发展。2018 年，他决定休假并反思自己的行为。但反思过后，该喷还是要喷。</p><p>&nbsp;</p><p>2020年，在严辞拒绝 Intel 安全漏洞相关补丁之后，Linus Torvalds 再次对 Intel 开喷，直言 “我希望 AVX-512 赶紧‘去死’ ”。</p><p>&nbsp;</p><p>事情的起因是Linus Torvalds 对 Intel 的 Advanced Vector Extensions 512（AVX-512）指令集提出了一些激进的看法，称其为“功率病毒”，其创建目的单纯只是为了使 Intel 公司的 CPU 硬件在基准测试中跑分更高。当然，Linus 也坦承了自己“脾气暴躁”，并且表示对 Intel 确实“有偏见”。</p><p>&nbsp;</p><p>AVX-512 是 Intel 公司在 2013 年发布的一套扩展指令集，其指令宽度扩展为 512 bit，每个时钟周期内可执行 32 次双精度或 64 次单精度浮点（FP）运算，专门针对图像 / 音视频处理、数据分析、科学计算、数据加密和压缩和深度学习等大规模运算需求的应用场景。</p><p>&nbsp;</p><p>Linus 认为 Intel 为了提高 CPU 的性能跑分，强行在 CPU 中塞入了像 AVX-512 这样的东西，导致 CPU 内核臃肿，为了提升少数特定使用场景下的运算性能而影响了大部分普通用户的使用体验。</p><p>&nbsp;</p><p>Linus 就此事发表评论称：</p><p></p><p></p><blockquote>“我希望 AVX-512 赶紧“去死”（dies a painful death），这样 Intel 就可以开始解决实际的问题，而不是去尝试创造这些花里胡哨的指令集，然后围绕它们做一些自欺欺人的基准测试。&nbsp;我希望 Intel 能够返璞归真：让他们的进程再次正常运行，更多地专注于提升用户基本体验的常规代码，而不是纠结于所谓的高性能运算或一些毫无意义的特殊情况下才会出现的问题。</blockquote><p></p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theregister.com/2024/01/29/linux_6_8_rc2/\">https://www.theregister.com/2024/01/29/linux_6_8_rc2/</a>\"</p><p><a href=\"https://www.bez-kabli.pl/news/en/2024/01/29/linus-torvalds-expresses-frustration-with-google-developers-over-inodes-in-the-linux-kernel/\">https://www.bez-kabli.pl/news/en/2024/01/29/linus-torvalds-expresses-frustration-with-google-developers-over-inodes-in-the-linux-kernel/</a>\"</p><p><a href=\"https://twitter.com/srostedt/status/1403081676327108612?lang=en\">https://twitter.com/srostedt/status/1403081676327108612?lang=en</a>\"</p>",
    "publish_time": "2024-02-01 16:17:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数百万小时，6秒，万分之五｜效果不输 ElevenLabs 和 OpenAI, MiniMax 语音大模型能用来做什么？",
    "url": "https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt",
    "summary": "<p></p><h2>从 0 到 1 的 MiniMax 语音大模型</h2><p></p><p></p><p>2023 年 11 月，MiniMax 发布语音大模型 abab-speech-01。从 11 月至今，共有超过 400 家企业用户接入我们的语音大模型。</p><p></p><p>在实际应用中，来自各行各业的用户给我们反馈了很多好的建议和想法。例如，在复刻有声书场景下，市面上没有可以批量、快速生成多角色音频的解决方案；在直播电商等注重互动性的场景中，各家现有语音能力仍无法做到实时，在生成语音的过程中仍需一定的等待时间，非常影响用户体验；在教学场景中，模型碰到特殊字词或者多音字的情况，时常存在发音不准确的问题。</p><p></p><p>为了给用户带来更加高效、丰富和真实的语音定制体验，我们不断迭代 MiniMax 语音大模型，并基于用户高优需求新增语音 API 接口，并上线了多个产品功能。MiniMax 是目前第一个开放多角色配音商用接口的公司。</p><p></p><p>在模型基础能力上，我们的语音模型对长达数百万小时的高质量音频数据进行训练，基于它的训练结果，仅用 6 秒的音频就能完成音色复刻，基于文本生成语音的字错率低至万分之五，已达到全球顶尖水平。</p><p></p><p>针对用户的高优需求，我们新增了以下产品功能：</p><p></p><p>三个 API 接口：多角色音频生成 API、文本角色分类 API 和快速复刻 API，帮助用户自主批量生成、克隆多角色音频；多语种能力、字典和间隔时长控制，满足用户丰富的定制化需求，提升教学场景体验T2A Stream （流式语音输出） 实现生成与输出的同步，减少用户在直播、对话等场景的等待时间。</p><p></p><p>为了让更多用户体验、使用我们的技术，我们在价格上也做出了调整：T2A Pro、T2A、T2A Stream 等价格下调为原先的一半，由 10 元 / 万字符降至 5 元 / 万字符。</p><p></p><p>具体功能价格调整见下表：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8b578605342dddc0a7732b7e930449c.png\" /></p><p></p><p></p><h2>声音小剧场</h2><p></p><p></p><p>由于语音模型没有公开的测评集，衡量一个语音模型到底怎么样主要依靠几个比较主观的评判标准，例如：自然度、相似度，可懂度和情感表现等。以下是几个基于我们语音大模型生成、复刻的一些语音效果。大家可以听听看，欢迎拍砖：）</p><p></p><p></p><h3>01 中英文夹杂读着毫无压力</h3><p></p><p></p><p>文本：</p><p>哎，你说你特别想念某个东西，可以说\"I really miss it a lot\" 或者\"I'm missing it terribly.\" 这样表达出你的感情。有什么特别想念的嘛？想聊聊吗？</p><p></p><p>声音 1（明杰）：</p><p></p><p>声音 2（晨曦）：</p><p></p><p></p><p>声音 3（祁辰）：</p><p></p><p></p><p></p><h3>02 跨语种复刻，比原声更自然</h3><p></p><p></p><p>文本：</p><p>别担心，犯错是学习的一部分，下次你会做得更好的。Don't worry, making mistakes is part of learning. You'll do better next time.</p><p></p><p>原声音频（童声）：</p><p></p><p>复刻音频（中 + 英）：</p><p></p><p>只用中文原声，也可以复刻出他们讲中、英、日、韩等多种语言的声音：</p><p>韩语：</p><p></p><p>日语：</p><p></p><p></p><p></p><h3>03 AI 嬛嬛和四爷，有没有甄嬛十级学者来检验一下效果？</h3><p></p><p></p><p></p><p></p><p></p><p></p><h3>04 多音字绕口令也难不倒！</h3><p></p><p>真人都不一定能读准的多音字绕口令，我们的语音模型可以：）出现多音字的绕口令对语音模型理解上下文提出了很高要求。</p><p></p><p>“人要是行，干一行，行一行，一行行，行行行，行行行，干哪行都行”</p><p></p><p></p><p></p><p></p><h3>05 实时语音通话，跟小海螺打电话吧</h3><p></p><p></p><p>MiniMax 不仅为企业用户和开发者提供语音相关的 API，也为普通用户打磨了多款含有语音功能的产品。例如，我们在 AI 助手海螺问问上线了实时语音通话功能——无论你遇到什么问题，都可以随时打电话给小海螺，就像在和朋友聊天一样轻松、自然。小海螺的反应比 ChatGPT 的语音功能还快哦，快来体验一下吧！</p><p></p><p></p><p></p><p></p><h3>06 唱 AI 嘻哈</h3><p></p><p></p><p>节奏感强、唱腔复杂的饶舌说唱，我们的模型也能够超酷演绎。</p><p>想和 AI battle 说唱的朋友可以打开链接尝试：</p><p><a href=\"https://m.xingyeai.com/tag/2760001\">https://m.xingyeai.com/tag/2760001</a>\"</p><p></p><p></p><p></p><p></p><p></p>",
    "publish_time": "2024-02-01 17:27:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]