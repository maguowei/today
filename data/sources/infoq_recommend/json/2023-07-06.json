[
  {
    "title": "微软发布Guidance语言，用于控制大语言模型",
    "url": "https://www.infoq.cn/article/QS07h5A8l1EGqVpV89ZK",
    "summary": "<p>最近，微软<a href=\"https://www.linkedin.com/posts/philippelimantour_inside-guidance-microsofts-new-open-source-activity-7068528335399575552-o-z8/\">推出</a>\"了一种名为<a href=\"https://github.com/microsoft/guidance\">Guidance</a>\"的领域专属语言，旨在增强开发人员管理当代语言模型的能力。这个新框架将诸如生成、提示和逻辑控制等任务集成到一个统一的开发流程中。</p><p>&nbsp;</p><p>据GitHub存储库的介绍，这门编程语言<a href=\"https://github.com/microsoft/guidance/blob/main/README.md\">使开发人员能够</a>\"“将生成、提示和逻辑控制组织到一个连续的流中，从而与语言模型实际处理文本的方式相匹配”。它可以与<a href=\"https://huggingface.co/models\">Hugging Face模型</a>\"等提供程序无缝集成，并集成基于智能种子的生成缓存系统和令牌修复，从而优化提示边界并消除词汇切分过程中的偏见。正则模式指引（pattern guides）则进一步强化了格式约束，保证提示可以正常完成。</p><p>&nbsp;</p><p>微软法国公司首席技术兼网络安全官Philippe Limantour<a href=\"https://www.linkedin.com/posts/philippelimantour_inside-guidance-microsofts-new-open-source-activity-7068528335399575552-o-z8/\">写道</a>\"：“用户可以无缝地合并生成、提示和逻辑控制，从而创建一个连续的流，与语言模型固有的文本处理机制保持一致。”</p><p>&nbsp;</p><p>对于微软推出Guidance，外界的反应也比较积极。根据哥伦比亚大学和沃顿商学院客座讲师<a href=\"https://pub.towardsai.net/inside-guidance-microsofts-new-open-source-framework-for-improving-the-control-in-llm-apps-3e5e4158027a\">Jesus Rodriguez</a>\"的说法，Guidance旨在为开发人员提供“一种简单而全面的语法，用于构建复杂的语言模型工作流”，降低LLM的复杂性。</p><p>&nbsp;</p><p>这个框架还没有完全完成。当前，针对该框架的扩展需求还包括：<a href=\"https://github.com/microsoft/guidance/issues/50\">更多的LLM支持</a>\"、更好的<a href=\"https://python.langchain.com/docs/get_started/introduction.html\">LangChain</a>\"<a href=\"https://github.com/microsoft/guidance/issues/163\">集成</a>\"以及支持OpenAI函数调用。</p><p>&nbsp;</p><p>Guidance是扩展语言模型功能这个工具生态系统的一部分。像<a href=\"https://github.com/hwchase17/langchain\">LangChain</a>\"和<a href=\"https://github.com/deepset-ai/haystack\">Haystack</a>\"这类框架的出现，已经简化了将模型集成到应用程序中的过程。<a href=\"https://handlebarsjs.com/\">Handlebars</a>\"、<a href=\"https://lmql.ai/\">语言模型查询语言</a>\"（<a href=\"https://www.theregister.com/2023/04/28/ai_models_may_not_yet/\">LMQL</a>\"）以及Nvidia的<a href=\"https://www.infoq.com/news/2023/06/nvidia-nemo-safety-ai/\">NeMo Guardrails</a>\"也被用于减轻LLM的不利影响。</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/guidance-microsoft-language/\">https://www.infoq.com/news/2023/06/guidance-microsoft-language/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/minibook/vWO39J1tlb9xlSaIJoI6\">大语言模型综合能力测评报告 2023</a>\"</p><p><a href=\"https://www.infoq.cn/article/gjLJp08IHUUD8ShahHZ3\">大语言模型进化之谜：涌现现象的挑战与争议</a>\"</p><p><a href=\"https://www.infoq.cn/video/eJmFPe7oGOoQi4flItDe\">浪潮之巅，如何让大语言模型走向金融应用新纪元</a>\"</p>",
    "publish_time": "2023-07-06 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "华盛证券推出“华盛 GPT -天玑”；建设银行进行数字人民币创新应用；信也科技宣布在三个层次深化小微服务布局；恒生电子推出数智金融新品 | 金融科技新闻速览（6...",
    "url": "https://www.infoq.cn/article/AM56qBPMXsuuljG7kRzR",
    "summary": "<p>软通金科引入 AI 大模型和 <a href=\"https://xie.infoq.cn/article/7d0ea1e9fececb398eed64e63\">RPA 技术</a>\"，推出了新一代测试管理平台；华盛证券推出大模型工具“华盛 GPT -天玑”，提高金融内容创作效率，助力数字化转型；建设银行与中金财富、东方证券、国泰君安证券在证券公司三方存管领域进行数字人民币创新应用合作；中国银行“绿洲工程”在&nbsp;7&nbsp;月&nbsp;2&nbsp;日迎来大规模投产上线，借记卡、信用卡业务完成全国推广……本周金融科技领域有哪些新闻，一起来看。</p><p></p><p></p><h2>软通金科推出 AI+RPA 技术，赋能金融行业数字化转型</h2><p></p><p></p><p>软通金科通过引入 AI 大模型和 RPA 技术，推出了新一代测试管理平台，整体产品和解决方案利用 AI 大模型私有化测试训练，基于测试场景自由组合、定制组合等多种技术，自动生成测试案例和测试数据，并结合 RPA 自动化测试，解决了以往需要大量金融测试人员的高成本、低效率问题。</p><p></p><p>以保险行业为例，AI+RPA 技术可以应用于客户咨询、保单查询等工作，AI+RPA&nbsp;技术不仅能解决保险行业的大量重复的手工工作，同时可以通过 AI 技术辅助保险销售环节精准获客。在核保和核赔环节， RPA 智能流程机器人可以完成大量的工作，提升审核效率。</p><p></p><p>未来，软通动力将继续在“ AI+RPA ”领域布局，加速数字化转型升级，为各行业企业提供数智生产力。</p><p></p><p></p><h2>华盛证券推出首个大模型工具“华盛 GPT -天玑”</h2><p></p><p></p><p>华盛证券近日推出了首个大模型工具“华盛 GPT -天玑”，以应对金融科技领域的需求。该工具利用<a href=\"https://xie.infoq.cn/article/dcf1235518df0b7cfff97548e\">大模型</a>\"技术解决了金融行业中的一些繁重且耗时的工作，如内容创作和行情数据抄录等。通过自动生成文章的能力，天玑工具可以大大提高内容创作的效率。目前，该工具主要用于华盛证券内部，但未来将逐步开放给外部用户使用。华盛证券在金融科技领域有着丰富的应用场景和技术实力，将继续深耕 AI 领域，助力金融行业的数字化转型和创新发展。该工具的推出将有效降低企业成本，提高运营效率，为用户带来更高效的金融投资体验。作为香港金融科技券商中少数具备自研实力的企业，华盛证券将引领香港金融券商在 AI&nbsp;2.0 时代的革新，持续推动金融业务发展。</p><p></p><p></p><h2>建设银行拓展数字人民币应用场景</h2><p></p><p></p><p>最近，建设银行与中金财富、东方证券、国泰君安证券在证券公司三方存管领域进行数字人民币创新应用合作，其中与中金财富和东方证券的合作已正式上线。客户可以在已签约的证券公司&nbsp;App&nbsp;内绑定建行数字人民币个人钱包，将其作为支付方式之一进行交易服务、资讯服务和研究服务等。这次合作实现了证券公司保证金账户与数字人民币体系的互联互通。这次合作不仅为财富管理发展提供了思路和借鉴模式，拓宽了投资者参与财富管理的场景选择，优化了客户操作流程，保障了客户资金安全，并提供了良好的客户体验。建设银行表示将继续在数字人民币应用和创新方面发挥经验，为中国数字经济的发展做出贡献。</p><p></p><p></p><h2>信也科技宣布在三个层次深化小微服务布局</h2><p></p><p></p><p>7&nbsp;月&nbsp;2&nbsp;日，信也科技&nbsp;COO&nbsp;王玉翔在投融会上宣布将在三个层次深化小微服务布局。首先，强化科技型“小店”金融业务以满足小微真实需求；其次，加强数字化服务产品矩阵建设，提升小微用户数字化能力；第三，加速推进<a href=\"https://xie.infoq.cn/article/e2d14f4b536ee42110780d704\">&nbsp;SaaS&nbsp;</a>\"业务布局，在垂直小微服务领域提供产业科技服务。信也科技将利用联邦学习等新技术建立全生命周期管理模型，精准服务小微企业。通过智能获客、风控和运营平台，帮助金融机构扩大业务规模、降低成本、提升效率。信也科技已为近&nbsp;80&nbsp;家金融机构提供数字化服务，为&nbsp;2800&nbsp;万用户提供信贷便利化服务。信也科技希望通过科技力量让普惠金融覆盖更多人群，并将继续致力于为小微企业提供更好的服务。</p><p></p><p></p><h2>中国银行“绿洲工程”全面投产，数字化转型迈出新步伐</h2><p></p><p></p><p>中国银行“绿洲工程”在&nbsp;7&nbsp;月&nbsp;2&nbsp;日迎来大规模投产上线，借记卡、信用卡业务完成全国推广，反洗钱系统重构升级，标志着中国银行在数字化转型方面取得新进展。通过创新完善信创技术平台，提升业务响应能力，支持高频借记卡、信用卡业务，提供&nbsp;24&nbsp;小时不间断服务。新一代数字化技术底座为个人业务发展提供支撑。中国银行还结合业务特点，运用混合事务处理架构数据库技术，提升交易处理能力和数据分析能力，满足反洗钱业务要求。未来，中国银行将继续推进“绿洲工程”，提升数字化转型能力，支持实体经济和社会民生。</p><p></p><p></p><h2>稠州银行成功实施首例数字人民币跨境收款业务</h2><p></p><p></p><p>近日，稠州银行通过创新数字人民币应用成功实现了首单数字人民币跨境收款业务。这一举措推动了数字人民币智能合约应用，为人民币国际化提供了新思路，为跨境交易带来了新突破。数字人民币具有可编程性、实时到账和安全性等特点，提高了交易透明度和效率，减少了结算周期和交易成本，促进了人民币在国际贸易中的使用和地位的提升。稠州银行将继续推动数字人民币跨境结算服务，为企业提供安全、高效的服务，助力人民币国际化进程。</p><p></p><p></p><h2>临港新片区启动金融智算平台</h2><p></p><p></p><p>7 月 4 日，在上海自贸区临港新片区举办了滴水湖新兴金融大会·夏季大会——亚金协金融科技年度论坛。论坛上，正式启动了名为“滴水湖金融湾——新兴金融智算融合创新平台”的项目。该平台由临港新片区经济公司与中国信通院、新型互联网交换中心、临港科技城、跨境数科、移动、电信、联通以及有孚、城地、龙丰、海兰信、浪潮等智算企业共同合作打造。平台以临港新片区智算中心为核心，利用人工智能技术推动金融数据产业的快速发展，激发金融数据潜能，促进金融机构的云网融合和便捷连通。此外，平台还支持共建高能级研发机构和功能性平台，建立起新片区管委会、科研院所、产业园区和新兴金融企业等多层次的协同合作创新平台，为临港新片区金融科技产业的发展提供有力支持。</p><p></p><p></p><h2>恒生电子推出数智金融新品</h2><p></p><p></p><p>恒生电子及其子公司恒生聚源于 6 月 28 日发布了数智金融新品，包括金融智能助手光子和智能投研平台 WarrenQ 。此外，恒生电子还推出了金融行业大模型 LightGPT ，并公布了最新的研发进展。</p><p></p><p>金融智能助手光子，能够解决大模型在金融业务中的技术、应用和数据安全问题。光子可以为金融机构提供投顾、客服、运营、合规、投研、交易等业务系统注入 AI 能力。光子在咨询、创作、合规和运营场景上展示了其能力，可以提供精准的咨询观点和建议，自动生成专业文案，以及智能处理合规和运营系统中的文档和参数。光子聚集了各类金融数据，并实现了数据的保护、合规和授权。试用将于 9 月正式开放。</p><p></p><p>WarrenQ 是一款专业的投研工具平台，通过智能对话的方式提高投研效率，打破传统投研信息孤岛。平台推出了两款AI工具产品： WarrenQ-Chat 和 ChatMiner 。WarrenQ-Chat 利用大模型叠加搜索和聚源金融数据库，通过对话指令获取金融行情、资讯和数据，并生成金融专业报表。ChatMiner 则是一款金融文档挖掘器，能够快速解读指定文档，提取关键信息，智能化处理海量文本数据。WarrenQ 将继续发展，结合更多场景输出智能工具，助力投研数智化发展。</p><p></p><p>LightGPT 具备更专业、更合规、更轻量的特点。LightGPT 使用了超大规模的金融语料和语种强化数据进行训练，支持 80+ 金融专属任务指令微调，具备金融领域的准确理解能力。在金融大模型能力评测中表现出色，同时保证内容和指令的合规安全，可以为金融业务场景提供底层 AI 能力支持。LightGPT 还具有丰富、轻量化的部署方式，支持私有化/云部署以及灵活 API 调用。预计将于 9 月底完成新一轮的金融能力升级，并开放试用接口。</p><p></p><p></p><h2>外资银行“星展”在中国推出企业数字人民币收款解决方案</h2><p></p><p></p><p>星展银行在中国推出企业数字人民币收款解决方案，成为为数不多提供该方案的外资银行之一。该解决方案允许星展中国的企业客户以数字人民币形式接收消费者付款，并自动转存到企业的人民币结算账户。该方案的优势包括：无需人工结算、双重离线功能以及通过企业网银&nbsp;IDEAL&nbsp;获取数字人民币交易明细的综合商户报告。</p>",
    "publish_time": "2023-07-06 14:34:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT又断网了！OpenAI暂时下线ChatGPT搜索功能，只因绕过付费墙？",
    "url": "https://www.infoq.cn/article/8EixxqlrSE5O10k76XyJ",
    "summary": "<p></p><blockquote>一夜之间，ChatGPT 又回到了 2021 年。</blockquote><p></p><p></p><h2>OpenAI暂停ChatGPT Bing搜索功能</h2><p></p><p>&nbsp;</p><p>近日，OpenAI 发布通知称：</p><p></p><p></p><blockquote>自 2023 年 7 月 3 日起，出于谨慎考虑，我们已禁用“使用 Bing 浏览”测试版功能，同时我们会修复此问题，以维护内容所有者的权益。我们正在努力尽快恢复测试版，感谢您的理解！</blockquote><p></p><p>&nbsp;</p><p>OpenAI 表示，ChatGPT 浏览 Bing 是一个测试版功能，可供 ChatGPT Plus 订阅者使用（ChatGPT Plus 是 ChatGPT 的高级版本，每月收费 20 美元，订阅者可以优先使用新功能和改进，在对话期间加快响应时间，甚至在需求高峰期也可以访问 ChatGPT），它允许 ChatGPT 搜索互联网以帮助回答从最新信息中受益的问题。OpenAI 了解到，该功能有时会以 OpenAI 不希望的方式显示内容。例如，如果用户专门请求 URL 的全文，则可能会无意中满足此请求。</p><p>&nbsp;</p><p>这也意味着，目前 ChatGPT 又回到了对 2021 年 9 月以后的世界一无所知的状态（该版本模型的训练数据截止于此）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/41/4104eb2fe4dd604b70f0e4134104d6ae.png\" /></p><p></p><p>据悉，今年 3 月，网页版 ChatGPT 首次宣布<a href=\"https://www.infoq.cn/article/sqGLAIdIKP1jv2YKMd3C\">联网功能</a>\"。据官方博客介绍，此次联网功能的实现得益于 OpenAI 为 ChatGPT 增加了插件使用功能，“插件是专门为语言模型设计的工具，以安全为核心原则，并帮助 ChatGPT 访问最新的信息，运行计算，或使用第三方服务。”</p><p>&nbsp;</p><p>6 月 27 日，ChatGPT 发布最新更新声明，宣布对<a href=\"https://www.infoq.cn/article/bQHRqFcQ1TlJCqHuczGR\">移动 ChatGPT</a>\" 应用程序进行了更新：用户可以使用浏览来获取有关事件和信息的全面答案和最新见解，这些信息超出了模型的原始训练数据。用户可在应用程序设置的“新功能”部分中启用浏览，然后在模型切换器中选择 GPT-4，并在下拉列表中选择“使用 Bing 浏览”。至此，移动设备上的 ChatGPT 现在也可以上网了。</p><p>&nbsp;</p><p>不过如今，该联网功能已被叫停，OpenAI 没有给出何时重新启用 Bing 浏览的时间表，但表示他们正在尽快恢复该功能。</p><p></p><h2>ChatGPT断网后，用户怒火被点燃</h2><p></p><p>&nbsp;</p><p>ChatGPT 用户们对 OpenAI 的这一决定并不买账。</p><p>&nbsp;</p><p>有用户称他就是为了用上 Bing 网络搜索功能，才愿意付费订阅 ChatGPT Plus。“看来 OpenAI 正在针对 ChatGPT Plus 的付费用户，”一位 ChatGPT Plus 用户在 OpenAI 论坛上说道。“这次他们取消了浏览功能，因为它可以读取用户请求的网站内容？拜托，这就是我为 Plus 付费的原因。”</p><p>&nbsp;</p><p>也有用户在论坛中表达了自己的担忧，怀疑未来 ChatGPT 可能不再支持对网站内容的翻译功能。ChatGPT 用户 Thiago Ramos 坦言，“假设我需要用 ChatGPT 来阅读 GitHub 上的某个代码仓库或者主题，或者翻译目标论坛上他国语种的信息，结果你告诉我这些都做不到？而且现在 ChatGPT 4 的反应也越来越差了，特别容易犯错误。在某些方面，连 3.5 版本都比它做得好。”</p><p>&nbsp;</p><p>有外媒就此事联系了 OpenAI，询问关于此项决定的几个问题。对方回复了邮件，但仅仅是列出一条与更新后的帮助页面内容相似的推文。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/0352d6d8f9d0591a9e7ca839ebe0daab.png\" /></p><p></p><p>OpenAI称：我们了解到，ChatGPT 的“Browse”beta 版有时会以意外方式显示内容。例如，若用户坚持请求目标 URL 指向的全文，其可能在无意中满足这一请求。我们将暂时禁用 Browse 功能并修复相关问题，希望维护内容所有者的应有权益。</p><p></p><h2>总有办法绕过付费墙？</h2><p></p><p>&nbsp;</p><p>付费墙（Pay Walls）是指对在线内容实行付费阅读，为网上的内容设立收费门槛，这是一种常见的盈利模式。在网络上，总能看到各式各样的绕过付费墙方案。不少企业也曾与用户斗智斗勇，阻止用户通过各种方式绕过付费墙。</p><p>&nbsp;</p><p>比如，《纽约时报》之前就曾使用“发出文件系统 API 请求”这个技术，防止访问者利用隐身模式来绕过他们网络上的付费墙以及限制免费文章的数量。</p><p>&nbsp;</p><p>微软的 Bing AI 聊天机器人本身是由 OpenAI 的 ChatGPT 提供支持，而且与谷歌 Bard 一样于今年 2 月正式上线。二者与 ChatGPT 的不同之处在于，它们都能访问网络来获取更新的相关信息。但几个月来，用户确实报告称这两款机器人均能绕过付费墙，提供大量原本需要花钱订阅才能查看的信息。</p><p>&nbsp;</p><p>目前还不清楚两家公司是否已经出手处理。Bard 和 Bing 最新的更新说明，也都未提及是否通过改造限制了这种绕过付费墙的能力。但有消息人士称，目前再以这种方式使用，两款机器人都会予以回绝。</p><p>&nbsp;</p><p>Bing 的回复是“我无法显示[您请求的网站]或任何其他受到版权保护的出版物文章的完整内容”，但会主动给出相关主题的概括和报道。在被要求提供付费文章的副本时，Bard 的回答更为简洁：“我只是个语言模型，所以无法帮到您。”</p><p>&nbsp;</p><p>有外媒就此事询问了微软和谷歌对于OpenAI最新举措的看法，包括在 AI 打破付费墙的问题上持何种立场，但双方均未做出回应。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://help.openai.com/en/articles/8077698-how-do-i-use-chatgpt-browse-with-bing-to-search-the-web\">https://help.openai.com/en/articles/8077698-how-do-i-use-chatgpt-browse-with-bing-to-search-the-web</a>\"</p><p><a href=\"https://www.theregister.com/2023/07/05/openai_pauses_bing_search/\">https://www.theregister.com/2023/07/05/openai_pauses_bing_search/</a>\"</p>",
    "publish_time": "2023-07-06 14:48:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Service Mesh：探索分布式系统的幻觉与未来",
    "url": "https://www.infoq.cn/article/XaSoBilDNO7qy24zCOyM",
    "summary": "<p>在现代的微服务架构中，应用程序网络是实现微服务之间分布式通信的关键。无论是在单个 Kubernetes 集群中部署还是跨多个集群和不同基础设施环境中部署，都需要建立一个强大的应用程序网络，让微服务能够相互交流。这种通信不仅需要高效可靠，还需要具备适应各种逆境的韧性。</p><p></p><p>除了建立应用程序网络，我们还需要监控微服务之间的通信，即可观察性（observability）。在微服务通信中，可观察性非常重要，可以了解微服务之间的相互作用方式。此外，微服务之间的通信也需要安全保护，通信应当进行加密，防止中间人攻击。每个微服务应具有身份标识，并能够证明其与其他微服务之间的授权通信。</p><p></p><p>那么，为什么需要<a href=\"https://www.infoq.cn/article/stCMjmTuODmzZmGzaNUr\">服务网格</a>\"（Service Mesh）呢？为什么这些需求不能在 Kubernetes 中满足？答案在于 Kubernetes 的架构和设计目标。正如之前提到的，Kubernetes 是应用程序生命周期管理软件，它提供了基本级别的应用程序网络、可观察性和安全性支持，但无法满足现代动态微服务架构的需求。这并不意味着 Kubernetes 不是现代化的软件，它确实是一项非常先进和前沿的技术，但它主要用于容器编排。</p><p></p><p>在 Kubernetes 中，流量管理由 Kubernetes 网络代理（kube-proxy）负责。kube-proxy在每个节点上运行，并与 Kubernetes API 服务器通信，获取关于 Kubernetes 服务的信息。Kubernetes 服务是一种将一组 Pod 作为网络服务公开的抽象层。kube-proxy通过设置 iptables 规则，定义了如何将流量路由到对应的端点（实际上是承载应用程序的底层 Pod）。</p><p></p><p>这就是服务网格发挥作用的地方。服务网格通过提供高级流量管理、可观察性和安全性功能，弥补了 Kubernetes 的不足。服务网格位于应用程序层，并与微服务并行工作，拦截和管理它们之间的通信。借助服务网格，您可以实现细粒度的流量控制、收集丰富的遥测数据以实现可观察性，并强制实施微服务之间的安全通信。</p><p></p><p>服务网格，例如 Istio、FloMesh、和 Linkerd，与 Kubernetes 紧密集成，并增强其功能，以满足现代微服务架构的要求。通过采用服务网格，组织可以实现微服务部署的增强韧性、可观察性和安全性。</p><p></p><p>服务网格技术填补了 Kubernetes 在微服务架构中先进的应用程序网络、可观察性和安全性方面的不足。它提供了一个强大且灵活的基础设施层，与 Kubernetes 互补，使组织能够构建和运行具备韧性和安全性的分布式系统。通过采用服务网格，组织可以实现微服务部署的增强韧性、可观察性和安全性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44aa0140e9df7d7c068929c60aeef6f8.png\" /></p><p></p><h2>分布式系统谬误</h2><p></p><p></p><p>当设计分布式应用程序时，软件开发人员经常会犯一些错误的假设，这些假设被称为\"分布式系统的谬论\"（Fallacies of a distributed system）。这些谬论最初由 L Peter Deutsch 和其他 Sun Microsystems 的工程师提出，并被广泛接受。它们揭示了关于分布式系统性质和挑战的常见误解。下面是这些谬论的总结：</p><p></p><p>1. 网络是可靠的：这个假设认为网络始终可用且没有故障。然而，在现实中，网络可能会出现中断、故障或间歇性的连接问题。</p><p></p><p>2. 延迟为零：这个谬论假设网络传输数据时没有延迟。然而，在实际情况下，网络延迟受到距离、拥塞和处理时间等因素的影响，会有不同程度的延迟。</p><p></p><p>3. 带宽是无限的：这个假设认为网络传输的数据量没有限制。然而，在现实中，网络带宽是有限的，会存在拥塞和性能降低的情况。</p><p></p><p>4. 网络是安全的：这个谬论认为网络本身是安全的，能够防止未经授权的访问或数据泄漏。实际上，网络需要强大的安全措施来确保机密性、完整性和可用性。</p><p></p><p>5. 拓扑结构不会改变：这个假设认为网络的结构和配置在时间上保持静态不变。然而，网络是动态的，节点可能加入、离开或改变连接，应用程序需要对此进行适应。</p><p></p><p>6. 只有一个管理员：这个谬论认为单个实体对整个网络具有完全的控制和管理权。实际上，分布式系统通常涉及多个管理员，他们拥有不同的控制权和责任。</p><p></p><p>7. 传输成本为零：这个假设认为在网络中传输数据没有任何成本。然而，在现实中，网络基础设施、带宽使用等因素都会带来成本。</p><p></p><p>8. 网络是同质的：这个谬论认为网络的各个组件具有相同的特性和统一的行为。实际上，网络可能由不同类型的设备、操作系统、协议和能力组成。</p><p></p><h2>服务治理痛点</h2><p></p><p></p><h3>1. 多语言、多技术栈</h3><p></p><p></p><p>微服务架构中，团队可能使用不同的编程语言和技术栈来开发微服务。这导致了多语言和多技术栈的挑战，需要找到一种统一的方式来协调不同语言的微服务之间的通信和交互。这涉及到如何选择合适的通信协议、数据格式以及寻找跨语言的解决方案。</p><p></p><h3>2. 有侵入性</h3><p></p><p></p><p>传统的服务治理解决方案可能需要对现有的微服务代码进行侵入性的修改或添加额外的依赖库，以实现服务发现、负载均衡、故障转移等功能。这样的侵入性可能增加了开发团队的工作量，并且可能引入不必要的复杂性和风险。</p><p></p><h3>3. 重复建设</h3><p></p><p></p><p>在大规模微服务架构中，可能存在大量的微服务实例需要进行服务治理。在传统的方式下，每个微服务都需要重复构建和维护自己的服务发现、负载均衡、故障转移等功能，导致了重复建设的问题。这不仅浪费了开发资源，还增加了系统的复杂性和维护成本。</p><p></p><h3>4. SDK版本碎片化</h3><p></p><p></p><p>当微服务通过共享的SDK进行通信时，不同微服务可能使用不同版本的SDK。这导致了SDK版本碎片化的问题，可能会导致兼容性和一致性方面的挑战。当需要更新SDK版本或解决SDK中的漏洞时，需要协调和管理各个微服务的SDK版本，这增加了额外的复杂性和风险。</p><p></p><h3>5. 跨机房、跨地域调度</h3><p></p><p></p><p>在分布式系统中，微服务可能部署在不同的机房或地域中。在进行跨机房或跨地域的调度时，需要考虑网络延迟、带宽限制等因素，以确保微服务之间的通信效率和质量。这需要一种灵活而智能的调度策略，能够根据实际情况进行动态的负载均衡和故障转移。</p><p></p><h2>演进趋势</h2><p></p><p></p><h3>1. 扩展性</h3><p></p><p></p><p>Service Mesh 提供了一系列的功能来增强微服务的扩展性。这包括熔断、限流、超时控制、灰度发布和故障注入等机制，以确保微服务在面对高负载、异常情况或部分故障时能够保持稳定和可靠。Service Mesh 支持多种通信协议，如HTTP、gRPC、Dubbo、Thrift等，使得这些功能可以适用于不同类型的微服务。</p><p></p><h3>2. 连通性</h3><p></p><p></p><p>Service Mesh 提供了强大的连接管理功能，确保微服务之间的连通性。它通过服务发现和负载均衡机制，使得微服务能够自动发现和定位其他微服务，并能够实现跨不同语言和技术栈的通信。Service Mesh 还提供了智能路由和流量控制功能，以便在微服务之间实现灵活的流量转发和负载均衡策略。</p><p></p><h3>3. 性能与资源</h3><p></p><p></p><p>Service Mesh 关注微服务架构的性能和资源管理。它通过优化网络通信、请求处理和数据传输等方面的性能，提高微服务的响应速度和吞吐量。同时，Service Mesh 还提供了对微服务的监控、追踪和日志记录功能，以便进行性能分析、故障排查和容量规划等任务。通过对资源的细粒度管理和调控，Service Mesh 可以有效地提高微服务架构的资源利用率和效率。</p><p></p><h3>4. 易用性</h3><p></p><p></p><p>Service Mesh 设计了一套简洁易用的接口和控制平面，使得开发人员和运维人员可以轻松地配置、管理和监控微服务。它提供了可视化的管理界面和命令行工具，简化了配置和部署的流程。同时，Service Mesh 还支持自动化的服务注册和发现，减轻了手动管理的负担。通过这些易用性的特点，Service Mesh 提供了一种简单而高效的方式来管理和维护微服务架构。</p><p></p><h2>Istio &amp; FloMesh</h2><p></p><p></p><h3>1. 相同点</h3><p></p><p></p><p>Istio 和 FloMesh 都使用了 sidecar 模型，它们具有这些优点： 解耦服务逻辑和网络治理、统一的通信层、动态的网络治理、安全性和可观测性增强、无侵入性。</p><p></p><p>但是在使用的同时也增加了一些损耗，具体可以从下面的示例图中看出：sidecar模型在服务之间进行通讯的时候有三个 connection 需要维护。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c3/c32f01717a6fec734d2fe3887b7e4744.png\" /></p><p></p><h3>2. 不同点</h3><p></p><p></p><p>Istio 固有地支持诸如断路、速率限制、超时控制、金丝雀部署和故障注入等机制。一旦正确安装了 Istio，这些特性就可以开箱即用了。 而FloMesh 采取了不同的方法。它提供了一个基于 JavaScript 的可扩展性模型，允许开发人员根据他们的具体需求定制和扩展这些功能。通过编写 JavaScript 代码，FloMesh 实现了对这些机制的细粒度控制，给予开发人员更多的灵活性，并使其能够根据自己的独特需求进行调整。</p><p></p><p>在 Istio 和 FloMesh 之间的选择应该基于你的具体需求和优先事项。Istio 提供了具有广泛内置特性的健壮和成熟的服务网格解决方案，而 FloMesh 为那些寻求对其服务网格功能进行细粒度控制的人提供了更可定制的方法。考虑诸如易用性、开发灵活性。</p><p></p><p>看看下面的图或许就能够理解了设计的区别：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/60373df0219c2cc02c634a6f558b1b20.png\" /></p><p></p><h2>未来</h2><p></p><p></p><h3>1. Wasm sidecar</h3><p></p><p></p><p>采用 WebAssembly (Wasm) sidecar 形式相对于传统的 sidecar 具有以下优势：</p><p></p><p>跨平台兼容性：Wasm 是一种可移植的二进制格式，可以在不同的操作系统和架构上运行。使用 Wasm sidecar 可以轻松实现跨平台部署，而无需担心依赖于特定环境的问题。</p><p></p><p>轻量高效：Wasm 代码通常比传统的 sidecar 容器更小、更轻量，因为它是一种二进制指令格式。这使得 Wasm sidecar 在启动时间和资源消耗方面表现更加高效，提供更快的响应和更低的延迟。</p><p></p><p>安全性：Wasm 提供了一种沙箱环境，在其中运行代码可以被有效地隔离和限制。使用 Wasm sidecar 可以增加安全性，因为它可以将应用程序与主机环境隔离开来，防止恶意代码的影响。</p><p></p><p>可扩展性：由于 Wasm 的灵活性，可以使用多种编程语言编写 Wasm 模块，而不仅仅局限于特定的编程语言或框架。这使得开发人员能够选择最适合他们需求的语言和工具，并提供更大的灵活性和可扩展性。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8d/a6/8d083b68205fd3b33906fdeb8f8dfda6.png\" /></p><p></p><p></p><h3>2. Ambient Mesh</h3><p></p><p></p><p>通过采用每个节点的代理模型，我们能够摆脱其中一个代理的需求，因为我们不再依赖于在每个工作负载内运行一个附属容器。这种转变带来了 ambient mesh 相对于 sidecar 模型的几个显著优势。</p><p></p><p>首先，使用 ambient mesh，我们可以减少运行在每个工作负载内的附属容器数量，从而减轻了系统的负担和复杂性。</p><p></p><p>其次，ambient mesh 不再依赖于每个工作负载的 sidecar 容器，这意味着我们不再需要为每个工作负载额外的连接。虽然仍然需要一些额外的连接，但这比始终需要两个额外连接要好得多。</p><p></p><p>最重要的是，ambient mesh 提供了更灵活的部署选项。由于不再需要在每个工作负载内运行附属容器，我们可以更轻松地将应用程序部署到不同的环境中，而无需过多的修改。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c20ffbbebd29b30f6f9aa3be7b63129.webp\" /></p><p></p><h3>3. eBPF</h3><p></p><p></p><p>在每个工作负载中运行附属容器会导致大量的代理实例，即使每个代理实例的内存占用已经进行了优化，但实例数量的增加仍会对整体系统造成重大影响。此外，每个代理还要维护诸如路由和终端点表等数据结构，随着集群规模的增长，这些数据结构也会增加，导致每个代理所消耗的内存随着集群规模的扩大而增加。为了解决这个问题，一些服务网格尝试将部分路由表推送到各个代理中，以限制它们的路由范围。</p><p></p><p>eBPF 是一种灵活的内核扩展框架，它允许在内核空间中执行自定义的网络过滤和处理逻辑。相比于运行在用户空间的附属容器，eBPF 的运行开销更低，因为它直接在内核中执行，避免了用户态和内核态之间的频繁切换。</p><p></p><p>采用 eBPF 而不使用附属容器具有轻量高效、低延迟、强大的可编程性、节省系统资源以及简化部署和管理的优势。这使得 eBPF 成为一种强大的工具，在现代网络环境中实现高性能和灵活的网络处理和功能成为可能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdb504d30e2ba1fc0b342f2d46a13b22.png\" /></p><p></p><h2>总结</h2><p></p><p></p><p>微服务架构中的应用程序网络是实现微服务之间分布式通信的关键。为了建立高效可靠的通信，需要具备适应各种逆境的韧性。此外，可观察性和安全性也是微服务通信中的重要方面。</p><p></p><p>服务网格位于应用程序层，与微服务并行工作，拦截和管理微服务之间的通信。它能实现细粒度的流量控制、收集丰富的遥测数据以实现可观察性，并强制实施微服务之间的安全通信。一些常见的服务网格技术包括Istio、FloMesh和Linkerd，它们与Kubernetes紧密集成，增强其功能，满足现代微服务架构的要求。</p><p></p><h4>作者介绍</h4><p></p><p></p><p>陈章朝，政采云有限公司运维开发工程师，一个热爱生活的编程爱好者，热于参与开源社区共建以及分享知识。</p>",
    "publish_time": "2023-07-06 15:09:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面向大模型的存储加速方案设计和实践",
    "url": "https://www.infoq.cn/article/SX3U3EMprDPUXe32C0Cm",
    "summary": "<p>在《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课第三讲中，百度智能云资深工程师陈志鹏分享了大模型对存储的全新挑战、大模型全流程存储问题的解决思路以及百度沧海存储加速方案和实践。</p>\n<p>在本期公开课中，他首先介绍了大模型对存储的全新挑战，从经典 AI 到大模型，本地存储不再适用，大模型全流程对存储有了更高的需求；针对这一问题，他分享了大模型全流程存储问题的解决思路，而数据的流动和性能是需要解决的主要问题；最后，他分享了百度沧海存储加速方案和实践，包括百度沧海 RapidFS 产品架构和 RapidFS 模型分发加速效果。</p>",
    "publish_time": "2023-07-06 15:17:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI大模型狂飙的背后：高性能计算网络是如何“织”成的？",
    "url": "https://www.infoq.cn/article/NeWITbYb6hjMaRkvSZOG",
    "summary": "<p>ChatGPT 的爆火掀起了 AI 大模型狂飙热潮，随着国内外原来越多的 AI 大模型应用落地，AI 算力需求快速增加。在算力的背后，网络起到至关重要的作用——网络性能决定 GPU 集群算力，网络可用性决定 GPU 集群算力稳定性。因此，高性能与高可用的网络对 AI 大模型的构建尤为重要。</p><p></p><p>6 月 26 日，腾讯云举办《面向 AI 大模型的高性能网络》沟通会，首次对外完整披露自研星脉高性能计算网络，并梳理了腾讯的网络架构演进历程。会后，腾讯云副总裁王亚晨、腾讯云数据中心网络总监李翔接受了 InfoQ 在内的媒体采访，进一步分享面向 AI 大模型的高性能网络是如何构建的。</p><p></p><p>据了解，星脉网络具备业界最高的3.2T通信带宽，可提升40%的GPU利用率、节省30%~60%的模型训练成本，进而能为AI大模型带来10倍通信性能提升。基于腾讯云新一代算力集群，可支持10万卡的超大计算规模。</p><p></p><p>王亚晨表示：“星脉网络是为大模型而生。它所提供的大带宽、高利用率以及零丢包的高性能网络服务，将助力算力瓶颈的突破，进一步释放 AI 潜能，全面提升企业大模型的训练效率，在云上加速大模型技术的迭代升级和落地应用。”</p><p></p><h2>AI大模型时代需要什么样的网络？</h2><p></p><p></p><h4>大带宽、高利用率、无损</h4><p></p><p></p><p>AI 大模型训练需要海量算力的支撑，而这些算力无法由单台服务器提供，需要由大量的服务器作为节点，通过高速网络组成集群，服务器之间互联互通，相互协作完成任务。有数据显示，GPT-3.5 的训练使用了微软专门建设的 AI 计算系统，由 1 万个 V100 GPU 组成的高性能网络集群，总算力消耗约 3640 PF-days (假如每秒计算一千万亿次，需要计算 3640 天)。</p><p></p><p>如此大规模、长时间的 GPU 集群训练任务，仅仅是单次计算迭代内梯度同步需要的通信量就达到了百 GB 量级，此外还有各种并行模式、加速框架引入的通信需求。如果网络的带宽不够大、延时长，不仅会让算力边际递减，还增加了大模型训练的时间成本。因此，大带宽、高利用率、无损的高性能网络至关重要。</p><p></p><p>王亚晨表示，大模型运算实际上是一个通信过程，一部分 GPU 进行运算，运算完成后还需要与其他 GPU 之间交互数据。通信带宽越大，数据传输越快，GPU 利用率越高，等待时间就会越少。此外，大模型训练对时延和丢包要求也比较高。“假设有很多 GPU 运算同一个任务，因为有木桶效应存在，一定要等花费时间最长的 GPU 运算完之后，才能完成一个运算任务。AI 对于时延的敏感度比 CPU 高很多，所以一定要把木桶效应消除，把时延控制在非常短的水平，让 GPU 的效率更高。此外，和带宽、时延相比，丢包对 GPU 效率的影响更加明显，一旦丢包就需要重传，重新进行 GPU 的训练。”</p><p></p><p>王亚晨认为，大集群不等于大算力。集群训练会引入额外的通信开销，导致&nbsp;N&nbsp;个&nbsp;GPU&nbsp;算力达不到单个 GPU&nbsp;算力的&nbsp;N&nbsp;倍。这也意味着，一味地增加 GPU 卡或计算节点，并不能线性地提升算力收益。“GPU 利用率的合理水平大概是在 60% 左右。”王亚晨说道。</p><p></p><p>要想通过集群发挥出更强的算力，计算节点需协同工作并共享计算结果，需要优化服务器之间的通信、拓扑、模型并行、流水并行等底层问题。高速、低延迟的网络连接可以缩短两个节点之间同步梯度信息的时间，使得整个训练过程变得更快。同时，降低不必要的计算资源消耗，使计算节点能够专注于运行训练任务。</p><p></p><h4>AI大模型驱动DCN网络代际演进</h4><p></p><p></p><p>据介绍，腾讯网络主要提供的功能是“连接”，一是连接用户到机器的流量，二是连接机器到机器的流量。目前，腾讯的网络架构主要分三大部分：</p><p></p><p>ECN 架构，表示不同类型的客户通过多种网络方式接入云上虚拟网络，这一块主要是外联架构，主要包括终端用户、企业用户、物联网用户分别通过运营商专线、企业专线、边缘网关接入腾讯数据中心。DCI 网络，主要是数据中心之间的互联，实现一个城市多数据中心或者多个城市的数据中心进行互联，底层会用到光纤传输。DCN，主要是数据中心的网络，这部分的任务是实现数据中心里面超过 10 万或者几十万服务器进行无阻塞的连接。</p><p></p><p>腾讯通过 ECN、DCI、DCN 等网络，把用户和业务服务器连接起来，并且把数百万台服务器连接起来。</p><p></p><p>王亚晨表示，AI 大模型的发展驱动了 DCN 网络代际演进。</p><p></p><p>在移动互联网时代，腾讯的业务以 to C 为主，数据中心网络服务器规模并不大，当时主要解决的是数据中心、服务器之间的互联，以及运营商之间的互联。所以那时数据中心流量特征很明显，基本都是外部访问的流量，对网络的时延和丢包要求也不高。</p><p></p><p>随着移动互联网以及云的快速发展，数据中心网络流量模型发生了变化，除了有从运营商访问过来的南北向流量，也有数据中心之间互访的东西向流量，对网络的时延要求也是从前的 10 倍。为了降低设备故障对网络的影响，腾讯采用多平面设计，并引入了控制器的概念，把转发面和控制面进行分离。用定制的设备、多平面以及 SDN 的路由器控制，将故障的解决时间控制在一分钟之内。</p><p></p><p>在 AI 大模型时代，数据中心网络流量模型进一步发生变化。“到了 AI 大模型时代，我们发现东西向流量比以前大了很多，尤其是 AI 在训练的时候，几乎没有什么南北向流量。我们预计如果大模型逐渐成熟，明年大模型数据中心流量南北向流量可能会有所增长，因为推理需求会上来。但就现在而言，东西向流量需求非常大，我们 DCN 网络设计会把南北向流量和东西向流量分开，以前是耦合在一张网络里，基础网络都是一套交换机，只是分不同层。但到了 GPU 时代，我们需要专门为 GPU 构建一层高性能网络。”王亚晨说道。</p><p></p><p>基于此，腾讯打造出了高性能网络星脉：具备业界最高的 3.2T 通信带宽，能提升 40% 的 GPU 利用率，节省 30%~60% 的模型训练成本，为 AI 大模型带来 10 倍通信性能提升。基于腾讯云新一代算力集群 HCC，可支持 10 万卡的超大计算规模。</p><p></p><h2>高性能网络星脉是如何设计的？</h2><p></p><p></p><p>据李翔介绍，腾讯网络大概由大大小小几十个组件组成，数据中心网络是其中最大、历史最悠久的一个。在 PC 和移动互联网时代，数据中心网络主要解决的是规模问题。而进入算力时代，业务对算力网络有了更高的要求。</p><p></p><p>“举个例子，如果说过去两个阶段数据中心网络是‘村村通’，解决大规模部署和广覆盖的问题，那么在算力时代，数据中心网络就是全自动化、无拥塞的高速公路。”李翔表示，AI 大模型对互联有比较高的要求，几千张 GPU 协同计算，如果出现任何一个丢包阻塞，那么全部都要降速，这种降速 1 分钟就有几十万的损失。</p><p></p><p>基于此，腾讯云开始搭建算力集群。4 月 14 日，腾讯云正式发布面向大模型训练的新一代 HCC（High-Performance Computing Cluster）高性能计算集群。网络层面，计算节点间存在海量的数据交互需求，随着集群规模扩大，通信性能会直接影响训练效率。腾讯自研的星脉网络，为新一代集群带来了业界最高的 3.2T 的超高通信带宽。</p><p></p><p>据介绍，腾讯对大模型集群网络做了以下几大优化：</p><p></p><p>（1）采用高性能RDMA网络</p><p></p><p>RDMA（GPU之间直接通信），是一种高性能、低延迟的网络通信技术，主要用于数据中心高性能计算，允许计算节点之间直接通过GPU进行数据传输，无需操作系统内核和CPU的参与。这种数据传输方法可以显著提高吞吐量并降低延迟，从而使计算节点之间的通信更加高效。</p><p></p><p>过往的数据中心VPC网络，在源服务器与目标服务器之间传输时，需要经过多层协议栈的处理，过往数据每一层都会产生延迟，而腾讯自研的星脉RDMA网络，可以让GPU之间直接进行数据通信。</p><p></p><p>打个比方，就像之前货物在运输途中需要多次分拣和打包，现在通过高速传送带、不经过中间环节，货物直接送到目的地</p><p></p><p>同时，由于星脉RDMA直接在GPU中传输数据，CPU资源得以节省，从而提高计算节点的整体性能和效率。</p><p></p><p>（2）自研网络协议（TiTa）</p><p></p><p>在网络协议上，腾讯云通过自研TiTa协议，让数据交换不拥塞、时延低，使星脉网络可以实现90%负载0丢包。</p><p></p><p>网络协议是在计算节点之间传输数据的规则和标准，主要关注数据传输的控制方式，能改善网络连接性能、通信效率和延迟问题。</p><p></p><p>为了满足大型模型训练中的超低时延、无损和超大带宽要求，传统的网络协议由于其固有的设计与性能限制，无法满足这些需求，还需要对“交通规则”进行优化。</p><p></p><p>星脉网络采用的自研端网协同协议TiTa，可提供更高的网络通信性能，特别是在满足大规模参数模型训练的需求方面。TiTa协议内嵌拥塞控制算法，以实时监控网络状态并进行通信优化，使得数据传输更加流畅且延迟降低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dd/dd349429d99a9e3ea9db7e9e7708cac4.png\" /></p><p></p><p>（3）定制化高性能集合通信库TCCL</p><p></p><p>通信库在训练过程中负责管理计算节点间的数据通信。面对定制设计的高性能组网架构，业界开源的GPU集合通信库（比如NCCL）并不能将网络的通信性能发挥到极致，从而影响大模型训练的集群效率。</p><p></p><p>为解决星脉网络的适配问题，腾讯云还为星脉定制了高性能集合通信库TCCL（Tencent Collective Communication Library），相对业界开源集合通信库，可以提升40%左右的通信性能。</p><p></p><p>并在网卡设备管理、全局网络路由、拓扑感知亲和性调度、网络故障自动告警等方面融入了定制设计的解决方案。</p><p></p><p>（4）多轨道网络架构</p><p></p><p>星脉网络对通信流量做了基于多轨道的流量亲和性规划，使得集群通信效率达80%以上。</p><p></p><p>多轨道流量聚合架构将不同服务器上位于相同位置的网卡，都归属于同一ToR switch；不同位置的网卡，归属于不同的ToR switch。由于每个服务器有8张计算平面网卡，这样整个计算网络平面从物理上划分为8个独立并行的轨道平面。</p><p></p><p>在多轨道网络架构中，AI训练产生的通信需求（AllReduce、All-to-All等）可以用多个轨道并行传输加速，并且大部分流量都聚合在轨道内传输（只经过一级ToR switch），小部分流量才会跨轨道传输（需要经过二级switch），大幅减轻了大规模下的网络通信压力。</p><p></p><p>（5）异构网络自适应通信</p><p></p><p>大规模AI训练集群架构中，GPU之间的通信实际上由多种形式的网络来承载的：机间网络（网卡+交换机）与机内网络（NVLink/NVSwitch网络、PCIe总线网络）。</p><p></p><p>星脉网络将机间、机内两种网络同时利用起来，达成异构网络之间的联合通信优化，使大规模All-to-All通信在业务典型message size下的传输性能提升达30%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0dc12799c45d0554557f37fe717dd041.png\" /></p><p></p><p>（6）自研全栈网络运营系统</p><p></p><p>为确保星脉网络的高可用性，腾讯云还自研了端到端全栈网络运营系统，先是实现了端网部署一体化以及一键故障定位，提升高性能网络的易用性，进而通过精细化监控与自愈手段，提升可用性，为极致性能的星脉网络提供全方位运营保障。</p><p></p><p>具体应用成效方面，大模型训练系统的整体部署时间可以从19天缩减至4.5天，保证基础配置100%准确，并让系统故障的排查时间由天级降低至每分钟级，故障的自愈时间缩短到秒级。</p><p></p><h2>写在最后</h2><p></p><p></p><p>AI 大模型时代给网络带来了新的机遇与挑战。随着 GPU 算力的持续提升，GPU 集群网络架构也需要不断迭代升级。</p><p></p><p>王亚晨表示，未来，星脉网络将围绕算力网卡、高效转发、在网计算、高速互联四大方向持续迭代。“这四个迭代方向也与我们面临的痛点相关，目前我们重点发力算力网卡和高效转发这两大方向。其中，算力网卡需要与交换机做配合，实现更优的、类似主动预测控制的机制，让网络更不容易拥塞；高效转发方面，之后可能会变成定长包的转发机制，这样也能保证整体效率。”</p>",
    "publish_time": "2023-07-06 15:51:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "秦金卫确认出席 ArchSummit 深圳，分享《金融级分布式技术平台的设计与实践》话题",
    "url": "https://www.infoq.cn/article/rxRWjmLEihy5P5xtyySA",
    "summary": "<p>7&nbsp;月&nbsp;21&nbsp;日&nbsp;-&nbsp;22&nbsp;日，&nbsp;在&nbsp;<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">ArchSummit&nbsp;全球架构师峰会（深圳站）</a>\"，长亮科技平台技术部副总经理秦金卫，将于会上发表题为《金融级分布式技术平台的设计与实践》的演讲，详细介绍金融级分布式技术平台的背景、目标、功能、内容、设计、实践落地过程和经验。</p><p></p><p>秦金卫是&nbsp;Apache&nbsp;Dubbo/ShardingSphere&nbsp;PMC，前某集团高级技术总监&nbsp;/&nbsp;阿里架构师&nbsp;/&nbsp;某商业银行北京研发中心负责人。关注于互联网，电商，金融，支付，区块链等领域，熟悉海量并发低延迟交易系统的设计实现，10&nbsp;多年研发管理和架构经验，熟悉各类中间件，擅长于&nbsp;SOA/&nbsp;微服务等分布式系统架构，热爱各种开源技术，活跃于&nbsp;Dubbo/Fastjson/ActiveMQ&nbsp;等多个开源社区。合著作品有《微服务架构实战：基于&nbsp;Dubbo、Spring&nbsp;Cloud&nbsp;和&nbsp;Service&nbsp;Mesh》、《JVM&nbsp;核心技术&nbsp;32&nbsp;讲》。阿里云&nbsp;MVP、腾讯&nbsp;TVP、TGO&nbsp;鲲鹏会会员、1024&nbsp;学院&nbsp;CTO&nbsp;培训班第六届学员。</p><p></p><p>相信通过秦金卫的分享，你将了解到金融级分布式技术平台的作用及其功能特性和组件能力，收获技术平台的建设经验和发展趋势。</p><p></p><p>除上述议题外&nbsp;，ArchSummit&nbsp;深圳还将围绕<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">基础架构技术</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1532?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">DataOps、Data&nbsp;Fabric&nbsp;等高效数据开发与服务模式</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1534?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">Mesh&nbsp;技术实践案例</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1535?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">QUIC&nbsp;传输和架构优化</a>\"等进行分享。</p><p></p><p>数十位业界专家，上百个国内外一线大厂前沿技术案例，一定会给你带来很多全新的开发灵感。期待与你线下交流！咨询购票请联系&nbsp;18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/9d/aa/9d6a27547062ee2e089f91bdc4ba1eaa.png\" /></p><p></p>",
    "publish_time": "2023-07-06 16:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "超低延时直播技术演进之路",
    "url": "https://www.infoq.cn/article/iSSNhwt4qU2WuSQ6U3AM",
    "summary": "<p></p><blockquote>据中国互联网络信息中心发布的《中国互联网络发展状况统计报告》显示，截止到 2022 年 6 月我国网络直播用户规模达到了 7.16 亿，占网民整体的 68.1%。最主要原因是 2020 年度疫情期间导致居家办公和休闲娱乐的人数呈现激增，新媒体互动直播成为了广大网民最重要的休闲娱乐方式之一。随着直播产业链的不断扩展完备升级，相关产业链各个环节分工逐渐明确且各环节参与人数逐步增多；为了满足不同的就业需求，引发相关就业人数提升，通过直播形式赋能传统产业升级转型，并与高新技术融合创新，优化传统行业商业模式，如直播带货、新媒体广告传媒转型等。丰富的传统文化、新闻、竞技体育、法律、知识共享等内容，通过移动端互动直播的形式得以更加高效的展现传播，既让优质的直播内容可以实现爆发式传播扩散，又可以让用户有更多的机会感受，学习甚至主动参与直播互动，实现内容供给侧和需求传播的多方共赢。可以说，<a href=\"https://www.volcengine.com/product/live\">超低延时直播技术</a>\"正在走上一条全新的发展之路。InfoQ 将联合火山引擎视频直播团队推出《超低延时直播技术演进之路》系列，带您探索超低延时直播<a href=\"https://xie.infoq.cn/article/feb3808b91b888f3cbbe9f589\">技术</a>\"的演进历程，揭示背后的挑战和突破，以及对未来直播行业的影响。</blockquote><p></p><p></p><p></p><h2>第一篇 进化篇-超低延时直播技术的前世今生</h2><p></p><p></p><p>网络基础设施升级、音视频传输技术迭代、WebRTC 开源等因素，驱动音视频服务时延逐渐降低，使<a href=\"https://www.volcengine.com/product/live\">超低延时直播技术</a>\"成为炙手可热的研究方向。实时音视频业务在消费互联网领域蓬勃发展，并逐渐向产业互联网领域加速渗透。经历了行业第一轮的红利爆发期，我国实时音视频行业的场景效能逐渐深化，步入到理性增长阶段。</p><p></p><p>延时的指标选择很大程度上取决于用户与内容制作方的交互耦合程度，场景丰富多样。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/94/e3/948f849494c91a2fe52bf767c69bb1e3.png\" /></p><p></p><p>在这些极端场景下，延时在用户侧希望越小越好，接近于实时通信的低延迟模式可以最大化地激发用户的参与感，无缝地与内容生产方产生互动效应，调动用户所见即所得的积极性。比如在主播秀场的 PK 、送礼、工会冲榜、打赏的活动关键环节，竞争双方的储值大户都希望实时地观察到自身主播在礼物刷榜后的反应，为后台运营决策团队或者后续活动策略提供第一时间的信息反馈。</p><p></p><p>下图体现了从技术/产品/运营的三方角度来综合思考低延时直播技术的作用；从外部-内部综合因素考虑技术的变迁对整个生态正向循环的影响。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/03/45/030ed90e63a0ea4551c10cedd435ed45.jpg\" /></p><p></p><p></p><h3>（一）传统标准直播技术的局限性</h3><p></p><p></p><p></p><h4>1、RTMP 协议的延迟问题</h4><p></p><p></p><p>RTMP 协议是最传统的直播协议，主播端采用 <a href=\"https://xie.infoq.cn/article/97ac1c5c632ff199247b63d3d\">RTMP</a>\" 协议推送 H.264/5 和 AAC 编码的视音频数据到云厂商 CDN 服务器进行转封装分发，端到端延迟一般控制在 3 到 7 秒。问题是 RTMP 的可扩展性存在缺陷，同时对于延迟的进一步下探存在一定的技术困难。RTMP 协议情况下：为了满足延时降低必然压缩播放器的下载缓冲区，这样会引发显著的卡顿问题，使得播放的观感产生不舒适的感受（延时下探至 2 秒以下）。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0c/07/0cdaf133be878214df78fc5e54746a07.png\" /></p><p></p><p></p><h4>2、传统直播技术在实时互动场景中的不足</h4><p></p><p></p><p>（1）视频延时和弹幕交互的延时存在显著差异，问题聊天内容互动与视频传输图像节奏不匹配。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/71/ff/71a16bf883ef6f1670e83be37e809bff.jpg\" /></p><p>（2）观众与主播互动形式单一，是单向内容传导无法做到双向（在 <a href=\"https://www.infoq.cn/article/qdXFclAaRi1OYmHTMGcj\">RTC</a>\" 技术引入之前无法显著解决）。</p><p></p><p>（3）单向传导的局限第一个方面表现在：观众端拉流传输无法做到根据网络情况自适应调节。用户只能以固定的码率进行流媒体传输无法做到动态感知，在网络情况实时变化的场景（比如弱网，移动基站切换等）固定单向码率传输有较大概率造成丢帧卡顿等因素影响观播体验；另一方面在网络条件更好时，固定码率传输无法动态提升视频传输码率（更高的画质带来更加舒适的体验）。</p><p></p><p>（4）在直播和连麦场景共存的互动直播场景下，主播采用传统RTMP推流在遇到连麦PK场景时，会产生推流/本地连麦合流/服务器连麦合流的切换问题，这种场景变换的切换会使得观众端产生瞬间的卡顿问题；如果采用基于webRTC直播技术的超低延时直播方案，这种推流--连麦逻辑的合流切换问题可以得到比较友好的解决（只需要改变服务器转发-订阅流通道的分发逻辑，不涉及推流媒体数据流的旁路调度切换）。</p><p></p><p></p><h4>3、超低延时直播与标准直播的区别</h4><p></p><p></p><p>（1）超低延时直播是近年来新兴起的一类应用。如电商直播、赛事直播等场景，兼具高并发与低延时的特性，传统直播 3-20s 的时延难以满足其需求，但对实时互动的要求又不及视频会议等典型的实时音视频应用，无需将时延降低至 400ms 以下。 为此，超低延时直播融合了传统直播与实时音视频的技术架构，通过取长补短的方式实现了介于二者之间的端到端时延。 尽管针对超低延时直播厂商尚无一套标准的技术路径，但大体可以归纳为拉流协议、网络架构和推流协议三个方面的改造， 在实际应用过程中，厂商会平衡成本及性能指标等因素，在不同的协议和网络架构之间进行选择。</p><p></p><p>（2）传输层协议的差异 （基于 UDP 协议的可靠性优化，为弱网对抗策略提供依据）</p><p></p><p>传统直播 FLV/RTMP 等采用的是 TCP 协议（或者 QUIC 协议）TCP 是牺牲传输实时性来换取数据完整性的可靠传输协议。弱网环境下，其在数据传输前的“三次 握手”连接会带来较大延时。而 UDP 作为不可靠的传输协议，其最大的优点为高实时性，但不保证数据的到达和排序。 实时音视频产品（如 RTM 超低延时直播）往往采用 UDP 协议，并在此之上进行协议层与算法层的优化，来提高传输的可靠性与逻辑性。</p><p></p><p>（3）UDP 协议的优化</p><p></p><p>UDP 协议往往和 RTP/RTCP 协议一起在实际应用中出现。RTP 负责数据传输，其协议头中的序列号、 端口类型、时间戳等字段，可为数据包的分组、组装、排序提供逻辑依据；RTCP 作为 RTP 的控制协议，负责对 RTP 的传输质量进行统计反馈，并为弱网对抗策略提供控制参数。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/92/19/927761459a0c7a0872c79c3574a29c19.png\" /></p><p></p><h3>（二）超低延时直播技术的演进历程</h3><p></p><p></p><p>（1）基于业务场景发展的直播技术演进过程（延迟主线）</p><p></p><p>（2）RTM 协议本身的演进历程</p><p></p><p>miniSDP 信令标准实现部分（抖音)CDN 信令异步回源RTP 携带扩展头组成部分</p><p></p><p><code lang=\"null\">a=extmap:18 \"http://www.webrtc.org/experiments/rtp-hdrext/decoding-timestamp\"\na=extmap:19 \"uri:webrtc:rtc:rtp-hdrext:video:CompositionTime\"\na=extmap:21 \"uri:webrtc:rtc:rtp-hdrext:video:frame-seq-range\"\na=extmap:22 \"uri:webrtc:rtc:rtp-hdrext:video:frame-type\"\na=extmap:23 \"uri:webrtc:rtc:rtp-hdrext:video:reference-frame-timestamp\"\na=extmap:27 \"uri:webrtc:rtc:rtp-hdrext:audio:aac-config\"</code></p><p></p><p>a=extmap:18 \"http://www.webrtc.org/experiments/rtp-hdrext/decoding-timestamp\"</p><p></p><p>a=extmap:19 \"uri:webrtc:rtc:rtp-hdrext:video:CompositionTime\"</p><p></p><p>RTP 使用 RTP 私有扩展头携带 DTS/CTS 值，每一帧 RTP 数据包通过 RFC5285-Header-Extension 扩展头携带该帧的 DTS 值，每一帧首个 RTP 包和 VPS/SPS/PPS 包通过 RFC5285-Header-Extension 扩展头携带该帧的 CTS 值，通过 PTS = DTS + CTS 计算当前帧的时间戳。用于启播快速音画同步和播放器播控逻辑精准音画同步。</p><p></p><p>a=extmap:21 uri:webrtc:rtc:rtp-hdrext:video:frame-seq-range</p><p></p><p>扩展头携带帧的起始/结束序号：如果首帧的前几个包丢失，那么可根据起始序号快速发起重传加快首帧；如果当前帧的后几个包丢失，那么可根据该帧的结束序号快速发起重传，降低延时，减少卡顿。</p><p></p><p>a=extmap:22 uri:webrtc:rtc:rtp-hdrext:video:frame-type</p><p></p><p>扩展头携带帧的类型：如果携带并解析了正确的帧类型，客户端可以不用解析 metadata ；同时在弱网情形，客户端可以跳过 B 帧直接解码 P 帧，加速出帧并减少潜在卡顿。</p><p></p><p>a=extmap:23 uri:webrtc:rtc:rtp-hdrext:video:reference-frame-timestamp</p><p></p><p>扩展头携带 P 帧的参考帧信息：如果发生弱网情形，那么客户端可以依照扩展头指定的参考帧关系及其对应时间戳，跳过 B 帧解码，减少卡顿发生。</p><p></p><p>a=extmap:27 uri:webrtc:rtc:rtp-hdrext:audio:aac-config</p><p></p><p>为了加速信令交互的速度，CDN 可以在某些条件下不去查询媒体信息，直接向客户端返回支持的音视频能力；此时 SDP 的媒体描述中将不包含有具体的音视频配置详细信息。在音频层面，此时AnswerSDP 中不包含 aac 解码所需的头信息；此时我们需要采取 RTP 扩展头模式携带 AAC-Config 供客户端在 RTP 收包时刻自行解析处理完成解码动作，作用是减少信令交互时间，提升拉流成功率。</p><p></p><p></p><h4>1、WebRTC 协议在直播播放器的移植</h4><p></p><p></p><p>RTM 低延时直播基于 WebRTC 技术衍生，基于 WebRTC 标准构建点到点传输一般有如下几个步骤：</p><p></p><p>（1）通信双方要进行媒体协商，会话详细规范即 SDP(Session Description Protocol) 交互；</p><p></p><p>（2）随后进行交互式网络地址协商（查询对端真实 IP 地址）准备构建媒体传输通道；</p><p></p><p>（3）当上述条件准备完毕即进入最终的 Peer to Peer 点对点媒体数据传输。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0f/b5/0fe680f53cc7ac020yy5e3ae56c3afb5.jpg\" /></p><p></p><p>信令部分客户端-服务器单独开发，利用了 SDP 标准报文模式；媒体传输部分采用开源的 WebRTC 框架和自截自研的实时音视频媒体引擎进行媒体传输。</p><p></p><p></p><h4>2、RTC 信令协议的改造升级（ MiniSDP 压缩协议）</h4><p></p><p></p><p>https://github.com/zhzane/mini_sdp</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/9b/3c/9bf0535ff0edc3c0b0184c1b9c78163c.png\" /></p><p></p><p>标准 SDP 比较冗长（ 5-10KB 左右），不利于快速高效传输。在直播场景下，会尤其影响首帧时间。MiniSDP 对标准 SDP 文本协议进行高效能压缩，将原生 SDP 转换成更小的二进制格式，使其能够通过一个 UDP 包来传输。降低信令交互时间，提高网络传输效能，降低直播拉流首帧渲染时间，提高拉流秒开率/成功率等 QoS 统计指标。</p><p></p><p></p><p></p><p></p><h4>3、CDN 对 RTM 信令的异步回源优化</h4><p></p><p></p><p>降低 RTM 信令交互时间，降低 RTM 拉流首帧渲染时间。原来的流程在服务端缓存不命中时需要等待回源拿到数据，才能返回带有 AacConfig 信息的 AnswerSDP。客户端收到 AnswerSDP 后发送 STUN，而服务端只能在收到 STUN 才能开始下发数据。（如下图左）；当异步回源情况下：服务端不再等待回源结果直接返回 AnswerSDP，之后回源和WebRTC 建连流程同步进行。等到 WebRTC 建连成功且回源拿到数据立即下发 RTP 数据。（如下图右）</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2d/bf/2df3615d0539c19caf00a0179815b5bf.jpg\" /></p><p></p><p></p><h4>4、视频渲染卡顿的优化（百秒卡顿平均降低 4 秒）</h4><p></p><p></p><p>改善人均看播时长，改变 RTC 引擎的组帧/解码策略；禁止 RTC 在低延时模式下的丢帧，改善直播的视频渲染卡顿。</p><p></p><p></p><p></p><p>传统的 RTC 场景优先保时延，全链路会触发各种丢帧（包括但不限于解码模块，网络模块），FLV 直播场景会优先保证观播体验（不丢帧，良好的音画同步效果）。RTM 要想减少卡顿，取得 qoe 的收益，播控策略需进行定制化, 定制逻辑修改点：</p><p></p><p>a. 确保不会由于软解的解码耗时或者硬解的 dequeuinputbuffer 等其它 api 操作阻塞 jitterbuffer ，内核层有一层强制的音画同步逻辑，可以确保音视频的播放体验；</p><p></p><p>b. 同时上层在监控网络模块和解码模块的缓存长度，有相应的兜底逻辑：</p><p></p><p>（1）判断硬解确实解不过来，dec_cache_frames 过多，上报错误，会降级到软解；</p><p></p><p>（2）jitterbuffer 异常，缓存的 frame_list 过多，触发播放器异常逻辑，上报错误，重新拉流。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b8/f6/b84082ec14653e63e1de968d3c22eef6.png\" /></p><p></p><p></p><h4>5、RTM 播控逻辑的优化</h4><p></p><p></p><p>改善移动端看播渗透，RTC 统一内核方案天生存在缺陷（ MediaCodec 硬件解码器初始化耗时久）；将 RTM 视频解码模块从 RTC 内核中迁移至 TTMP 播放内核，复用了 FLV 的视频解码模块（ MediaCodec 避免重新初始化）；显著的降低了安卓平台的首帧渲染时间，提升了拉流的成功率。RTC 内核通用逻辑</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1d/44/1d0d6f3a7bd5fbb540bdf3f29ba9ac44.jpg\" /></p><p></p><p>改进的 RTM 内核播控逻辑</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/04/16/049204378dfe7d0df20a155ae35fc616.jpg\" /></p><p></p><p>以上为超低延时直播技术演进之路《进化篇》的所有内容，第二篇《实战篇》我们将聚焦于<a href=\"https://www.volcengine.com/product/live\">超低延时直播技术</a>\"如何大规模落地实践，请大家持续关注~</p>",
    "publish_time": "2023-07-06 16:02:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱——数据库领域",
    "url": "https://www.infoq.cn/article/N7HK09pMRJrrOUDCSUus",
    "summary": "<p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。</p><p></p><p>所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。</p><p></p><p>此篇是《中国开源生态图谱系列研究》的第二篇，聚焦在三大基础软件中的数据库，通过将目前的开源数据库项目进行分类，并结合开源基金会、开源产业联盟等生态，完整构成中国开源数据库的生态图谱。</p><p></p><p>随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，和拓展的 Gitee 指数，分析和评价现有开源数据库项目，并从中选择优秀案例供广大开发者和开源社区研究。</p><p></p><h1>目录</h1><p></p><p>生态图谱解读生态图谱企业洞察</p><p></p><p>扫码下载</p><p><img src=\"https://static001.geekbang.org/infoq/e9/e959341544efe203f86ddd98afc1d625.png\" /></p><p></p>",
    "publish_time": "2023-07-06 16:17:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱——操作系统领域",
    "url": "https://www.infoq.cn/article/r9Lptg2pYrKvX4VMVYgT",
    "summary": "<p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。</p><p></p><p>所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。</p><p></p><p>此篇是《中国开源生态图谱系列研究》的第一篇，聚焦在三大基础软件中的操作系统，通过将目前的开源操作系统项目进行分类，并结合开源基金会、开源产业联盟等生态，完整构成中国开源操作系统的生态图谱。</p><p></p><p>随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，和拓展的 Gitee 指数，分析和评价现有开源操作系统项目，并从中选择优秀案例供广大开发者和开源社区研究。</p><p></p><h1>目录</h1><p></p><p>生态图谱解读生态图谱企业洞察</p><p></p><p>扫码领取</p><p><img src=\"https://static001.geekbang.org/infoq/23/23b3bdc2616f26b63e7b04f7688ceb18.png\" /></p><p></p>",
    "publish_time": "2023-07-06 16:22:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁集团：世界人工智能大会上“最接地气”参展商，中西部县域数字就业中心组团亮相",
    "url": "https://www.infoq.cn/article/4gwZYub5sl92yd8FYFMv",
    "summary": "<p>7 月 6 日举办的 2023 世界人工智能大会迎来“最接地气”参展商，由<a href=\"https://www.infoq.cn/article/g2dxkXzWCtLEpuizIxqR\">蚂蚁集团</a>\"“数字木兰｜AI </p><p>豆计划”支持的 17 个县域数字就业中心组团参加，与来自大模型、芯片、机器人、智能驾驶</p><p>等领域的 400 余家参展企业同台亮相。</p><p></p><p>为期 3 天的展期内，这些来自中西部欠发达县域的数据标注企业将通过现场展示、分享等，</p><p>与来自世界各地的参会者交流，如何做好人工智能的“训练师”，帮助 <a href=\"https://www.infoq.cn/article/Mq97Ju9jJ5M1rxc15dCG\">AI</a>\" 认识世界。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9ee5bceb3fd194caa2584262c6b87700.png\" /></p><p>“数字木兰｜AI 豆计划”支持的县域数字就业中心代表在 WAIC 现场分享</p><p></p><p>为发挥数字技术和平台优势，助力乡村留守女性、返乡青年等在地就业，2019 年，蚂蚁集</p><p>团、蚂蚁公益基金会联合中国妇女发展基金会等共同发起“数字木兰｜AI 豆计划”，通过订单</p><p>引入、技能培训、社会企业孵化等，助力中西部欠发达县域女性就业和产业发展。</p><p></p><p>截至 2022 年底，该计划已支持在陕西、山西、甘肃、贵州、宁夏等地建立了 17 个县域数</p><p>字就业中心，累计帮助 4000 人在地就业，成为“人工智能训练师”，其中超过 6 成员工为女</p><p>性。相关数字就业中心所开展的标注业务，已经涵盖 200 余个具体应用场景，年数据标注</p><p>量过亿，多个中心成为当地最大用工企业。</p><p></p><p>AI 越聪明，背后就越少不了“人工智能训练师”一步步的教它们，把图片、语音、文本、视频</p><p>等原始数据标注成<a href=\"https://www.infoq.cn/article/iBHmuLWBH14YgWjhD1Xq\">人工智能</a>\"可以理解的结构化语言。“没想到在小县城的我们能成为人工智</p><p>能产业链上的一环。”清涧县爱豆科技有限公司总经理鱼涛、积石山县“ AI 豆计划”数字经济</p><p>产业园负责人鲁玉超等都表示，科技助力打破地域壁垒，小县城数字标注企业也能走上大舞</p><p>台。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2533568b6ad1428a3639f0cb91acd51.png\" /></p><p>甘肃省积石山县人工智能训练师张娟带参观者体验数据标注工作</p><p></p><p>“任何一轮重大的产业进步，都会带来职业结构的重新构建，以 AI 为代表的数字时代催生了</p><p>大量数据标注等新职业机会，给县域就业、乃至产业发展带来了重大机遇。” 7 月 6 号同期</p><p>举行的 AI 女性菁英论坛上，蚂蚁公益基金会秘书长王晓晶现场分享道。在她看来，在一些</p><p>以男性为主导的传统职业领域，技术正在打破性别壁垒，女性的耐心和细心等特质，在数字</p><p>就业领域展现出了很多新的竞争力，“数字时代，可能是缩小性别鸿沟最好的时代。</p><p></p><p>据悉，为了更好的支持乡村女性及县域数字就业中心发展，蚂蚁集团联合生态伙伴搭建了</p><p> iTAG 智能数据标注平台，为县域引入更多就业机会，同时借助端模型、预标注机器人等智</p><p>能化工具，对 AI 训练任务进一步简化，降低人工标注难度，提高效率，帮助县域数字就业</p><p>中心能够在市场上接到更多场景更复杂的订单。目前，这些县域数字就业中心开展的标注业</p><p>务，已经涵盖了文本、图像、音频、视频等各类标注场景，应用领域覆盖了智慧城市、智能</p><p>制造、自动驾驶、智能服务、智能医疗、智能农业、智能物流、智能金融等社会生产生活的</p><p>多个角落。</p>",
    "publish_time": "2023-07-06 16:34:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "对话贾扬清、关涛、张伯翰：AI 平民化趋势下，数据架构将被彻底颠覆？",
    "url": "https://www.infoq.cn/article/HgeggrBTRm7iE5FqEkmR",
    "summary": "<p>深度学习诞生 10 年，LLM （大语言模型技术）终于带来 AI 平民化。ChatGPT 爆火后，AIGC 浪潮席卷全球。AI 作画、AI 写歌、AI 生成视频…… 全球大厂纷纷推出 AIGC 应用，让 AI 变得“触手可及”。从技术角度看，基于海量数据构建的大模型能够进行相对独立的推理和判断，让企业看到了 AI 与 Data 的技术融合已经成为当下重要的发展趋势之一。</p><p></p><p>如今，AI 与企业的数据基础设施融合到了什么程度？企业是否要选择一款 AI 数据平台？AI for Data 如今在企业生产中发挥着怎样的价值？为了探讨问题的答案，InfoQ 联合云器科技策划了《极客有约》特别版——《再谈数据架构》系列直播。第二期，我们邀请到了 前阿里巴巴副总裁贾扬清、云器科技联合创始人 &amp; CTO 关涛 以及 OtterTune 联合创始人张伯翰，畅谈以下话题：</p><p></p><p>数据库、大数据和 AI，哪个更重要？“AI for Data”与“Data for AI”有何不同？企业数据平台要不要结合 AI？“模型即数据”？模型平台可以完全替代数据平台吗？企业需要怎样的一体化的 AI 数据平台？</p><p></p><p></p><p></p><p></p><h1>数据库、大数据和 AI，哪个更重要？</h1><p></p><p></p><p></p><blockquote>贾扬清：数据库、大数据和 AI 齐头并进、相辅相成。这一轮大模型创业公司当中，有很多公司首先要招数据处理、数据清洗、数据标注、数据挖掘等等这一系列的工程师——又回到了数据上。张伯翰：数据库、大数据和 AI 三者之间两两融合。当 AI 数据量特别大的时候就需要去考虑分布式模型训练，这是大数据和 AI 融合要考虑的点。AI 和数据库之间的关系要从 Data for AI 和 AI for Data 两个角度来看。关涛：数据平台需要把 AI 作为“一等公民”支持，而不是只做数仓，这就是 Data for AI 的关键。同时，DBA 的这种人工调优的模式并不高效，怎么解放人力 / 提升效率？AI for Data 就是一个关键项。</blockquote><p></p><p></p><p>InfoQ：数据库、大数据和 AI 都是当下热门的技术方向，三者之间的关系是怎样的？</p><p></p><p>贾扬清：我觉得数据跟 AI 一直是相辅相成的关系。2015-2016 年，行业内认为做 AI 还是应该关注计算和算法，寻找更优的模型在现有数据库 / 数据集上面进行更好地统计。ImageNet 数据集应该是大家第一次认识到：数据能够赋能 AI 做更加宽广的探索。ImageNet 以及当时一系列的自然语言、语音等模块的数据让行业在神经网络方面有更多的探索。</p><p></p><p>因此我认为在过去十年当中，我们其实是在数据和另外一个系统的红利上面来寻找更多更好的算法，比如 CNN、RNN、LSTM，包括现在比较流行的 GPT 等一系列的算法。如今，算法又发展到了一个新高度。基于像 Transformer 这样的模式，算法能够有能力来处理，或者说理解、压缩更多数据了。</p><p>所以大家可以看到，这一轮行业内的大模型创业公司当中，有很多公司首先要招数据处理、数据清洗、数据标注、数据挖掘等等这一系列的工程师——又回到了数据上。随着数据量越来越大，算法越来越复杂，系统变得越来越重要。</p><p></p><p>2011 年行业内讨论大模型的时候，有一种说法叫做：参数服务器 (Parameter server)。当时，大家以类似互联网的传统思维来做大模型：用一堆相对而言性能比较差、不稳定的机器来解决共同训练的问题。但是随着算法越来越多、越来越复杂，传统的高性能计算系统变得越来越流行。</p><p></p><p>如今，我们会发现所有人都在买 GPU 机器。系统变得越来越大并且和传统的高性能计算的结合程度越来越深之后，我们能够以更加高的效率来处理一系列的数据和一些算法，我觉得这个是今天我们看到的，数据、人工智能和系统这三块齐头并进的一个状态。</p><p></p><p>其实一直以来，人工智能和数据领域都有融合的部分。在互联网时代，人工智能和数据领域融合的地方叫做广告搜索和推荐。</p><p></p><p>张伯翰：数据库、大数据和 AI 都是现在比较火的话题，我觉得三者之间是两两融合的关系。数据库和大数据方面，像 Databricks、Snowflake 主要在做 Data warehouse 或者 ETL 的数据处理，也在往 AI 方面发展，方向上都是往数据方面融合的。谈及大数据和 AI 的融合，其实我们可以看到 Spark 也做了很多 AI 方向的布局，如 SparkML。我觉得大数据是平台化的，当 AI 数据量特别大的时候就需要去考虑分布式模型训练，这是大数据和 AI 融合要考虑的点。</p><p></p><p>AI 和数据库之间的关系分为两类，一个 DB for AI，另一个是 AI for DB。目前，有些企业在数据库内部做一些机器学习方面的一些工作，可以省去 ETL 或者是各种数据倒来倒去的操作，这个是 AI for DB。我觉得这方面还是挺有市场需求的，因为很多时候企业不需要很复杂的 AI 模型，仅需要去做一些简单的数据处理和预测工作。我是做数据库的，所以主要关注 AI for DB。企业想利用 AI 来优化数据库，可以通过一些训练的数据去学习优化数据库的经验和规则、自动大规模优化数据库。</p><p></p><p>关涛：伯翰通常把 Snowflake 定义到数据库领域里边，我把他可能更细分到 BigData；数据库领域更像指代 transactional Processing（事务处理），所以我把像 Oracle 这类的公司定义成数据库的公司，然后把 Snowflake、Databricks 定义成大数据的公司，其他还有一些公司归属于 AI 类。</p><p></p><p>三个领域从发展阶段看，如下图所示。横轴可以理解为时间，共 5 个阶段；纵轴可以理解为影响力和预期；图上的曲线表现了技术发展到高热度期、发展期以及普惠期的过程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a49009295bc83bb3967d94cbc4fd4761.jpeg\" /></p><p></p><p>数据库发展了 50 年，如果以 Oracle 为代表，那么它处在下图中的红圈位置，表示如今处在普惠期。BigData 发展了 20 年，大概在绿圈的位置。其中，美国大数据市场可能从发展期可能到了普惠期，中国大数据市场可能从爬升期开始到了发展期。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b867473d3e4fbdccdbe9817f08cf711.jpeg\" /></p><p></p><p>对比这两张图，你会发现：数据库发展了 50 年，从营收层面看，Oracle 的营收实际上是 Snowflake 的 20 倍。当一个领域进入到普惠期的时候，它会有非常高的市场占有率。如果从增长率的角度来看，Oracle 低一些，大概 17%；Snowflake 是 Oracle 的 4 倍，大概 60% 多。如果按照这个增速的话，理论上大概也许 8 年半到 9 年的时候，Snowflake 能超越 Oracle。技术的发展过程可见一斑。</p><p></p><p>我们用一个例子来理解这三者的融合关系。在视频直播推荐场景，我们发现很多客户需要通过 AI 的方式把很多非结构化的数据抽取出来用于推荐，同时沉淀结构化的用户画像数据存放在数据库中。这两个数据一定要融合在一起，因为推荐系统左边是推荐的内容，右边是客户的客群，只有通过推荐的内容在客群上做圈选融合在一起，才能做出推荐系统。我们发现，企业需要用 AI 的能力去做部分的数据计算，同时需要用数据系统做很多计算。</p><p></p><p>伯翰刚才讲了两个大的方向，一个方向叫做 Data for AI，一个叫做 AI for Data。前者大家可能比较好理解，刚才我举的那个推荐的例子就是这样；后者其实是 AI for system 的一个子集，伯翰他们做的是 AI for Database，还有 AI for BigData system，甚至 AI for AI system。</p><p></p><p>AI for Data 实际上是目前比较火的一个创业方向。很多人觉得 DBA 的这种靠人工调优的模式其实不太适用，而且大数据模型其实带来了更好的人的智能体的能力，它真的可以替代人做很多事情。怎么解放人力？AI for Data 就是一个关键项。</p><p></p><h1>“AI for Data”与“Data for AI”有何不同？</h1><p></p><p></p><p></p><blockquote>贾扬清：我更关注 Data for AI 中海量异构数据存储和管理，AI 计算范式的支持，以及 Data 和 AI 结合带来的新产品形态。张伯翰：我发现 DBA 越来越少了，如果数据库能自己调优自己的话，对整个行业是一个很好的事。从更深的技术角度来看，依赖经验和通过 AI 机器学习经验，两者并不是二选一的情况，而是相互补充的。</blockquote><p></p><p></p><p>贾扬清：AI for Data 可能是大家在通用系统领域相对比较容易理解的一个事情，因为任何一个系统都有非常多的需要调优、管控等等的工作。以前大家靠经验或者一些指标来判断什么时候拉起机器做计算，现在大家可以基于时序的统计数据等方式加上一个预测的算法来做，相当于现在把以前的一些需要在系统里面做决策的过程，交给 AI 来简化。</p><p></p><p>我自己更关注 Data for AI 的三方面问题。</p><p></p><p>第一，海量异构数据存储和管理。</p><p></p><p>第二，对 AI 计算范式的支持。Data for AI 在 AI 算法内部不只是作为一个 Data Provider，也有很多的应用。</p><p></p><p>譬如说我们在做大模型，包括在做广告推荐的时候，经常会遇到一个算法或者一个模块叫做 embedding。embedding 的意思是我们把很多的文本变成一个高维的数据的向量，把它放到一个很大的 KV 里头去做。以前我在 Facebook 的时候，也遇到过这样的情况：我自己来管理哪些 embedding 更热，哪些 embedding 更冷，然后来做 cache 等等。</p><p></p><p>后来，我们发现这就是一个标准的 KV 数据库，以前 KV 数据库里面所有的应用、想法、思路，都可以相应地互通过来。这件事情让我意识到，Data for AI 在 AI 算法内部不光只是作为一个 Data Provider，也有很多的应用。</p><p></p><p>第三，Data 和 AI 的结合产生了新的各种各样的产品形态，比如最近大家比较关注的向量数据库。其实早在 2017 年的时候，我们在 Facebook 的时候和 AI 的研究院一块做了一个算法叫做 FaaS，应该叫 Facebook Approximate Nearest Neighbor Search。今天，很多的向量数据库的背后也都是用 FaaS 来做它的一个核心引擎。FaaS 更多专注在计算，需要叠加更多内容才能变成一个向量数据库产品。</p><p></p><p>向量数据库公司 Pinecone 融了很多钱，那么它的业务空间有多大，它是否和传统的数据库之间有足够的 differentiate。这个事情目前我们还不太确定，但是我们比较确定的一点是，因为各种新的计算模式的产生，使得我们在数据库的领域和 AI 的领域有更多的结合，结合出一个“两边都像，但是两边都得用到，和以前的形态都不太一样”这样一种新的产品形态。</p><p></p><p>张伯翰：AI for Data 的做法其实就是通过机器学习或者 AI，或者模型去学习那些规则。我是做 AI for Database 的，其实是 AI for system 的一个子集，也可以是 AI for Spark，AI for TensorFlow，我们目前主要是做 AI for PostgreSQL and AI for MySQL，做数据库的调优。</p><p></p><p>Oracle 几年前宣布了自治化数据库，大概的意思是使用 AI 让数据库更加智能，减少 DBA 的负担，相当于自己优化自己。MySQL 也做了自己的自治化数据库。将 AI 与自动优化结合不仅是数据库厂商的一个技术方向，也是客户认可的趋势。我发现 DBA 越来越少了，如果数据库能自己调优自己的话，对整个行业是一个很好的事。从更深的技术角度来看，依赖经验和通过 AI 机器学习经验，两者并不是二选一的情况，而是相互补充的。客户关注的是可靠性和可解释性，其中可解释性非常重要，我认为不可能是只使用 AI 就能胜任的。</p><p></p><p>此外，我们发现很多机器学习的一些实践，在 AI for Databricks 实践，最后发现难点并不是 AI 的模型，而是怎么去和数据库结合，怎么收集这些训练数据，怎么把推荐自动地放到数据库上。比如，有些参数的调整是需要重启数据库才能生效的，但是大部分的生产数据库不可能支持重启数据库改参数，因为这样会有挺多的宕机时间，风险较大。这个难点是我们创业这段时间看到的，也是我们数据库公司重点在做的方向。我们做的事情就是让一个完全不懂数据库的人能更好地去优化数据库，能更快解决数据库的问题。</p><p></p><h1>企业数据平台要不要结合 AI？</h1><p></p><p></p><p></p><blockquote>关涛：与 AI 结合其实是很新的一个技术方向，也还远没有定型，平台建设容易踏空 / 落后。所以，企业数据平台的设计需要考虑面向未来的扩展，比如开放性和可插拔 AI 计算能力。关涛：湖仓一体的架构是下一代数据平台的必选项。系统设计的简单化（一体化）是终极目标。</blockquote><p></p><p></p><p>关涛：大家都会觉得关系型的计算模型可能不够，需要有更多 AI 的能力。从这个角度出发看企业的痛点，我大概总结了三点。</p><p></p><p>第一，现在传统数仓架构其实并不能够很好地支撑 AI。当前很多企业的数据基础设施不是为 AI 设计的，还是只面向数据。从数据库出发，数据库是纯结构化数据然后做关系计算的，你让一个比如说 MySQL 去存音视图的数据其实不太合适。很多数据库甚至很多数仓的设计都偏重于结构化数据分析结构，它们对新的存储介质的支持，对新的计算介质的支持，还有对 AI 的计算范式的支持其实都不够好。</p><p></p><p>第二，AI 的整个工具链自有特色，让建设、维护和系统本身的复杂度越来越高。你会发现因为 AI 的 workload 进来之后，AI 会使得原有的数据平台的系统设计更复杂。这会让系统变成一个非常专家和 Geek 的系统，让一个公司里可能只有少数的几个人能够 touch 它。这意味着，这个系统能够真正被用起来的机会很少。</p><p></p><p>第三，因为与 AI 结合其实是很新的一个技术方向，也还远没有定型，平台建设容易踏空 / 落后。面向未来的系统的终态，最终很难有一个定论。包括像 Snowflake 和 Databricks，他们 AI 方向的收购和合作也是刚刚展开。所以 企业数据平台设计需要考虑面向未来的扩展，比如开放性和可插拔 AI 计算能力。</p><p>对于这三个痛点，我也有两点建议：</p><p></p><p>第一，未来的设计这个系统一定是要把存储和计算考虑进去，要支撑多种不同的负载，要支持结构化、半结构化和非结构化的数据存储，要支持其他的计算模型，简而言之就是你的存储体系要是能开放的。所以，湖仓架构可能是做数据平台建设上可能必须要考虑的一个点，这个数据平台要兼顾效率和多样性。</p><p></p><p>第二，因为 Large Language Model 和 AI 的很多技术还是非常新，还在不断地变化，可以说可能是按星期为维度在做迭代，在平台的 Infra 的迭代中，不可能保持一样的迭代速度，因为对于公司来讲成本太高了，保证自己的平台有良好的扩展性就好了。扩展性包括刚刚提到的数据开放性和管理以及计算灵活且能够扩展。</p><p></p><p>我们云器科技当前在做的产品就是为了解决这些问题，所以在底层架构里采用了湖仓的架构。虽然当时大语言模型热度并没有那么高，但我们依然选定了这个方向做了开放的设计。我们的数据虽然放在数仓里，但它是开放的，数据可以被其他的引擎消费，所以从这个层面，我们在做平台扩展性的设计的时候其实兼顾了这一点。计算灵活可扩展方面，我们支持比如说 Python 的代码和 SQL 的混编，保证计算的开放和管理，保证平台具有扩展性，能够面向未来更多的技术突破做迭代。</p><p></p><p>目前，云器科技不会做大语言模型，但会和做大语言模型的公司合作，更好地做支持。</p><p></p><h1>模型平台可以完全替代数据平台吗？</h1><p></p><p></p><p></p><blockquote>贾扬清：无论是数据平台还是 AI 平台都没法来用自己的经验解决对方的问题。数据平台和模型平台是相互结合的关系。关涛：数据库 / 大数据系统已经是一个必选项了，AI 可能目前还是可选项。关涛：好的数据平台架构三个标准：1) 能容纳管理异构数据 2) 能支持多种计算形态 3) 非技术人员能*直接*用起来平台（需要平台非常简单易用）。&nbsp;&nbsp;</blockquote><p></p><p></p><p>InfoQ：有一种说法是叫数据即模型，所以这是否意味着对于企业而言不需要数据平台了，直接用模型平台就可以了？</p><p></p><p>贾扬清：这个是个挺好的问题，从技术跟业务这两个角度可能回答会稍微不一样一点，我就拿 Snowflake 和 Databricks 最近他们的一些动作来解释。</p><p></p><p>从技术的角度来看，其实目前数据和 AI 的计算是分开的。数据这一块我们更关注 IO 数据等等这一系列的事情；AI 这一块我们更加关注计算，比如说利用像 GPU 这种，高性能计算的资源来做数据的分析等等。这也是为什么从技术上来讲，今天 无论是数据平台还是 AI 平台都没法来用自己的经验解决对方的问题，因为技术上这两个其实就是很不一样的。这也是为什么说 Databricks 没法自己原生地长出一个 AI 东西来，Snowflake 长不出来。</p><p></p><p>从应用的角度或者从需求的角度来讲，其实的确用户会越来越多地把数据分析的需求跟 AI 的需求结合起来。从产品的角度来讲，单纯做数据是比较困难的，单纯做 AI 也比较困难。这也是为什么传统的数据公司也会需要有 AI 的能力。大家在解决这个问题的时候发现，技术和产品不能分别单独来看，想要拥有完整的产品体验，要么选择合作，要么选择购买。当然这是我个人的一个观点，并不一定对，Databricks 的收购以及 Snowflake 和英伟达合作，一定程度上也是说找到一个自己的 counterpart，然后能够来一起解决这样一个统一的产品问题。</p><p></p><p>回到你的问题，我觉得就是 数据平台跟模型平台肯定都需要，而且很有可能是一个相互结合的关系。把它放在企业内部，有点像采购不同的标准化的组件，然后把自己的业务做好的过程。</p><p></p><p>&nbsp;InfoQ：企业如何判断是否要进行数据架构或者说数据平台的升级和迭代？</p><p></p><p>贾扬清：我觉得可能从两个角度，第一个角度，目前在做大数据和 AI 的创业企业处在“前狼后虎”的状态中，不仅需要有大量的资源来做大模型，还需要找到大模型落地场景并且与其他系统相连接。</p><p></p><p>我在硅谷，在全球其他地方都看见了这样的一个情况：企业在看到 AI 的可能性的时候，提出 AI 战略，也有业务工程师、数据工程师、算法工程师、数据科学家，也听到了很多开源大模型，但是都无法用起来。</p><p></p><p>大家都在看着大模型“临渊羡鱼”。虽然开源的模型企业都有，但是和业务系统的对接很难。如果说有那么一个解决方案，能够让企业里面的业务工程师、数据工程师不懂 AI，也不知道 GPU 是什么东西，但是能够 5 分钟之内甚至 5 秒钟之内拉起一个 HuggingFace 大模型；一个钟头之后，把现有的数据应用和这个模型跑起来，能先溜一溜；一天之内 hopefully 能够连接到业务系统，看看到底效果怎么样。这样的话我们尝试的这个飞轮转起来之后，就能够从今天的一个抽象的大模型，到将来有更多的人能够把大模型跟应用结合起来，这样不断地来迭代来搞出东西来。</p><p></p><p>所以除了训练一个模型之外，怎么样让大量的对于 AI 系统、对于 GPU、对于 AI 算法、数学没有那么深的理解，但是对自己的业务有很深的理解的企业能够更加快地接触到这些模型，能够非常大规模地、非常高效地、非常迅速地拉起这些模型，把它对接到业务里面去，这是一个挺大的机会。</p><p></p><p>模型是企业自己，算法是企业自己的，数据是企业自己的，但是工具是标准化地提供的，20 年前这个工具叫数据库，Oracle、IBM 都提供了这个数据库；十几年前这个工具叫云；AI 来了之后也有新一波的 AI 工具。</p><p></p><p>张伯翰：我觉得还是取决于公司的业务，还有数据的结构。现在开源数据库还是非常流行的，如果是个开源系统的话内部阻力会小很多，我觉得这个也是个大趋势。</p><p></p><p>关涛：其实数据是个资产，怎么能释放资产价值，实际上是现在每个企业都关心的问题。数据库系统已经是一个必选项了，AI 可能目前还是可选项，大家都愿意可能去尝试它。其实之前我一直被问到一个问题，包括在阿里的时候也被问到这样的问题，因为我作为数据平台的建设者，他说你从你的评估标准看，你觉得我们的数据平台究竟是一个什么样的水平？我觉得有以下 3 个标准：</p><p></p><p>第一，数据平台究竟能够容纳什么样的数据。如果一个企业其实它有机会能采到很多的数据，但不能把这个数据保存或者用起来的话，这个平台价值会下降。</p><p></p><p>第二，什么样的计算能力能够让这些数据的价值体现出来，这个就涉及刚才的观点了，除了关系计算、SQL 的模式以外，AI 的计算能力包括传统算法。大语言模型这些能力，其实都是用来释放数据价值的。</p><p>第三，有多少人能够把这个用起来。运营人员、销售人员是不是能够直接使用数据，是一个企业的数据平台是否够先进、够现代的一个标准。</p><p></p><p>基于这些标准出发面向未来去看的话，我们发现随着底层系统越来越复杂，越来越多的企业大多数情况下都会用很多 AI 的算法去做调优，因为这种方式其实会使得你上层的用户变得非常简单。</p><p></p><p>所以从这个视角看一个企业的数据平台在发展过程中，应该关注三点：第一，数据存储是不是足够丰富；第二，能否很好地扩展支撑更多的算力；第三，数据平台是否足够简单，能够使运维成本降低，让更多人能更好地用起来。</p><p></p><h1>企业需要怎样的一体化的 AI 数据平台？</h1><p></p><p></p><p></p><blockquote>贾扬清：一体化的 AI 数据平台最重要的一点其实就是好用和快捷。当一个平台做得越来越简单的时候，业务企业可能就不需要数据科学家了。关涛：CEO 要业务价值，CTO 要降本增效，业务团队需要简单易用。企业应该根据业务体量，选择合适的多云、湖仓架构的一体化数据平台，同时能支持 Data 和 AI。</blockquote><p></p><p></p><p>InfoQ：我们发现，CEO 关注的是企业的整体的发展，看到了技术趋势；CTO 关注企业整体的数据架构和业务结合。有的企业没有数据科学家团队，由产品总监在牵头关注 AI 大模型等新技术趋势。那么，企业内部谁在进行 AI 落地技术和业务的决策？</p><p></p><p>关涛：这是一个特别好的问题。你刚才提到的这三类型的人代表企业三种不同的角色，三种不同的角色的人确实关注点不一样，CEO 更关注的是这样的一个平台，怎么能够帮助企业更好地实现价值，他甚至不太关注说你这个平台是个自建的还是购买的，只要你的性价比足够达标就好了。他更关注怎么能让更多的人把这个平台用起来使得企业能更受益。这种情况通常是会推动平台向前演进的。</p><p></p><p>CTO 的角色可能并不完全一样。我们跟很多企业的 CTO 沟通发现，他们提的第一个需求往往都是降本。这个可能跟当前的经济状态也相关，他想的就是说我怎么能够以更低的成本得到更好的价值，这是 CTO 的视角。</p><p></p><p>业务视角其实要来得更直接，比如说我们跟一个企业做合作想出一个报表的时候，把需求提一个单子给他们，这个单子流转到他们那边去排个期，排期回来再把这个单子拿回来，最终我们收回来这个数据，这个周期大概需要 3 天的时间。他们来跟我们聊，能不能更简化这个过程甚至说能不能直接就做这件事情。</p><p></p><p>这件事情其实就数据平台本身来讲，如果你会写 SQL 的话，这个事情并不特别复杂。这里面涉及一些数据建模的问题，也可以通过 Data for AI 这种 AI for Data 这种方式来解决。剩下那半边我能不能更快地做这个迭代，现在其实答案很可能是 yes，我们能够通过不编程的方式直接和平台做交互，慢慢正在变成现实。</p><p></p><p>很多企业其实没有数据科学家这个岗位，很可能是因为当一个平台做得越来越简单的时候，这个岗位有可能都会被人工智能或者系统来替代。</p><p></p><p>InfoQ：很多时候企业的发展其实是数据在驱动。业务视角看，数据科学家往业务方向走一走，走着走着可能就变成这个业务里面的 CEO 了。我刚才其实是举了 3 个例子，CEO、CTO、数据科学家，其实代表的是企业可能是不同的规模，业务的多元化和单一化也决定着企业的结构可能是怎么样的。那么，企业怎么去选择一体化的 AI 数据平台？</p><p></p><p>贾扬清：我觉得企业今天其实在一个迅速变化的过程当中，最重要的一点其实就是好用和快捷。所以说在选择数据平台还是 AI 的平台的时候，能否迅速地能够上手，能否迅速能够让自己的团队对接用起来然后去尝试业务效果，是今天更加重要的一个点。</p><p></p><p>张伯翰：我从一个创业者的角度来说一下，我觉得这个完全取决于公司的体量。资源有限的时候，你一定得关注最核心的业务。这也是侧面反映了扬清说的一点就是好用。</p><p></p><p>关涛：其实前面聊得很充分，我给几个具体的建议：</p><p></p><p>第一，建议用云。云其实是一个非常灵活的基础设施，可以让你今天买一个信息流，明天就不用它了。这种灵活性其实会使得企业的架构迭代变得特别简单。所以第一个建议是要用云，最好其实是多云的，有分层解耦的这样一个设计。</p><p></p><p>第二，湖仓架构现在应该是个必选项。</p><p></p><p>第三，关注企业的体量。中小企业选择一个更简单、更容易上手的平台其实更重要。Infra 建设目标是为了业务服务，最重要的是你的业务，你关注在你的业务上选一个你最合用的平台就好。</p><p></p><p>InfoQ：咱们今天的圆桌基本上到这里就到最终结束的时间了，听我们同事说在 7 月 20 号云器科技其实是有一个新产品的发布会的，关涛老师要不要提前给我们剧透一下？</p><p></p><p>关涛：谢谢主持人，最后打一个小广告。云器科技是成立了一年半的数据平台服务的提供商，我们的主打的技术口号是多云和一体化，希望给用户提供全托管的企业级的极致简单的数据平台，我们能同时地支持数据和 AI 的负载。</p><p></p><p>我们在 7 月 20 号会举办首次产品发布会，主题是 “Single Engine· All Data”，如果大家希望找到我们的话，可以搜索云器科技就能找到我们的网站和公众号。7 月 20 号，欢迎大家来听我们的发布会，谢谢！</p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a8fc1b2a8eb0990d8391d02e03ddb73.jpeg\" /></p><p></p>",
    "publish_time": "2023-07-06 18:47:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "老虎国际的数据库选型方法论",
    "url": "https://www.infoq.cn/article/m3I60MOJBdy8XF6aCr8G",
    "summary": "<p>在亚马逊云科技中国峰会上，我们采访到了老虎国际数据库与中间件高级经理孙龙，<br />\n戳视频了解他的参会感受！</p>",
    "publish_time": "2023-07-06 21:46:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "携手亚马逊云科技构建开放共赢的生成式AI生态",
    "url": "https://www.infoq.cn/article/8H7v3LMM5BeIsR6bbumb",
    "summary": "<p>Stability与亚马逊云科技展开了怎样的合作？戳视频了解</p>",
    "publish_time": "2023-07-06 21:49:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]