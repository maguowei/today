[
  {
    "title": "微软发布Guidance语言，用于控制大语言模型",
    "url": "https://www.infoq.cn/article/QS07h5A8l1EGqVpV89ZK",
    "summary": "<p>最近，微软<a href=\"https://www.linkedin.com/posts/philippelimantour_inside-guidance-microsofts-new-open-source-activity-7068528335399575552-o-z8/\">推出</a>\"了一种名为<a href=\"https://github.com/microsoft/guidance\">Guidance</a>\"的领域专属语言，旨在增强开发人员管理当代语言模型的能力。这个新框架将诸如生成、提示和逻辑控制等任务集成到一个统一的开发流程中。</p><p>&nbsp;</p><p>据GitHub存储库的介绍，这门编程语言<a href=\"https://github.com/microsoft/guidance/blob/main/README.md\">使开发人员能够</a>\"“将生成、提示和逻辑控制组织到一个连续的流中，从而与语言模型实际处理文本的方式相匹配”。它可以与<a href=\"https://huggingface.co/models\">Hugging Face模型</a>\"等提供程序无缝集成，并集成基于智能种子的生成缓存系统和令牌修复，从而优化提示边界并消除词汇切分过程中的偏见。正则模式指引（pattern guides）则进一步强化了格式约束，保证提示可以正常完成。</p><p>&nbsp;</p><p>微软法国公司首席技术兼网络安全官Philippe Limantour<a href=\"https://www.linkedin.com/posts/philippelimantour_inside-guidance-microsofts-new-open-source-activity-7068528335399575552-o-z8/\">写道</a>\"：“用户可以无缝地合并生成、提示和逻辑控制，从而创建一个连续的流，与语言模型固有的文本处理机制保持一致。”</p><p>&nbsp;</p><p>对于微软推出Guidance，外界的反应也比较积极。根据哥伦比亚大学和沃顿商学院客座讲师<a href=\"https://pub.towardsai.net/inside-guidance-microsofts-new-open-source-framework-for-improving-the-control-in-llm-apps-3e5e4158027a\">Jesus Rodriguez</a>\"的说法，Guidance旨在为开发人员提供“一种简单而全面的语法，用于构建复杂的语言模型工作流”，降低LLM的复杂性。</p><p>&nbsp;</p><p>这个框架还没有完全完成。当前，针对该框架的扩展需求还包括：<a href=\"https://github.com/microsoft/guidance/issues/50\">更多的LLM支持</a>\"、更好的<a href=\"https://python.langchain.com/docs/get_started/introduction.html\">LangChain</a>\"<a href=\"https://github.com/microsoft/guidance/issues/163\">集成</a>\"以及支持OpenAI函数调用。</p><p>&nbsp;</p><p>Guidance是扩展语言模型功能这个工具生态系统的一部分。像<a href=\"https://github.com/hwchase17/langchain\">LangChain</a>\"和<a href=\"https://github.com/deepset-ai/haystack\">Haystack</a>\"这类框架的出现，已经简化了将模型集成到应用程序中的过程。<a href=\"https://handlebarsjs.com/\">Handlebars</a>\"、<a href=\"https://lmql.ai/\">语言模型查询语言</a>\"（<a href=\"https://www.theregister.com/2023/04/28/ai_models_may_not_yet/\">LMQL</a>\"）以及Nvidia的<a href=\"https://www.infoq.com/news/2023/06/nvidia-nemo-safety-ai/\">NeMo Guardrails</a>\"也被用于减轻LLM的不利影响。</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/guidance-microsoft-language/\">https://www.infoq.com/news/2023/06/guidance-microsoft-language/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/minibook/vWO39J1tlb9xlSaIJoI6\">大语言模型综合能力测评报告 2023</a>\"</p><p><a href=\"https://www.infoq.cn/article/gjLJp08IHUUD8ShahHZ3\">大语言模型进化之谜：涌现现象的挑战与争议</a>\"</p><p><a href=\"https://www.infoq.cn/video/eJmFPe7oGOoQi4flItDe\">浪潮之巅，如何让大语言模型走向金融应用新纪元</a>\"</p>",
    "publish_time": "2023-07-06 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "华盛证券推出“华盛 GPT -天玑”；建设银行进行数字人民币创新应用；信也科技宣布在三个层次深化小微服务布局；恒生电子推出数智金融新品 | 金融科技新闻速览（6...",
    "url": "https://www.infoq.cn/article/AM56qBPMXsuuljG7kRzR",
    "summary": "<p>软通金科引入 AI 大模型和 <a href=\"https://xie.infoq.cn/article/7d0ea1e9fececb398eed64e63\">RPA 技术</a>\"，推出了新一代测试管理平台；华盛证券推出大模型工具“华盛 GPT -天玑”，提高金融内容创作效率，助力数字化转型；建设银行与中金财富、东方证券、国泰君安证券在证券公司三方存管领域进行数字人民币创新应用合作；中国银行“绿洲工程”在&nbsp;7&nbsp;月&nbsp;2&nbsp;日迎来大规模投产上线，借记卡、信用卡业务完成全国推广……本周金融科技领域有哪些新闻，一起来看。</p><p></p><p></p><h2>软通金科推出 AI+RPA 技术，赋能金融行业数字化转型</h2><p></p><p></p><p>软通金科通过引入 AI 大模型和 RPA 技术，推出了新一代测试管理平台，整体产品和解决方案利用 AI 大模型私有化测试训练，基于测试场景自由组合、定制组合等多种技术，自动生成测试案例和测试数据，并结合 RPA 自动化测试，解决了以往需要大量金融测试人员的高成本、低效率问题。</p><p></p><p>以保险行业为例，AI+RPA 技术可以应用于客户咨询、保单查询等工作，AI+RPA&nbsp;技术不仅能解决保险行业的大量重复的手工工作，同时可以通过 AI 技术辅助保险销售环节精准获客。在核保和核赔环节， RPA 智能流程机器人可以完成大量的工作，提升审核效率。</p><p></p><p>未来，软通动力将继续在“ AI+RPA ”领域布局，加速数字化转型升级，为各行业企业提供数智生产力。</p><p></p><p></p><h2>华盛证券推出首个大模型工具“华盛 GPT -天玑”</h2><p></p><p></p><p>华盛证券近日推出了首个大模型工具“华盛 GPT -天玑”，以应对金融科技领域的需求。该工具利用<a href=\"https://xie.infoq.cn/article/dcf1235518df0b7cfff97548e\">大模型</a>\"技术解决了金融行业中的一些繁重且耗时的工作，如内容创作和行情数据抄录等。通过自动生成文章的能力，天玑工具可以大大提高内容创作的效率。目前，该工具主要用于华盛证券内部，但未来将逐步开放给外部用户使用。华盛证券在金融科技领域有着丰富的应用场景和技术实力，将继续深耕 AI 领域，助力金融行业的数字化转型和创新发展。该工具的推出将有效降低企业成本，提高运营效率，为用户带来更高效的金融投资体验。作为香港金融科技券商中少数具备自研实力的企业，华盛证券将引领香港金融券商在 AI&nbsp;2.0 时代的革新，持续推动金融业务发展。</p><p></p><p></p><h2>建设银行拓展数字人民币应用场景</h2><p></p><p></p><p>最近，建设银行与中金财富、东方证券、国泰君安证券在证券公司三方存管领域进行数字人民币创新应用合作，其中与中金财富和东方证券的合作已正式上线。客户可以在已签约的证券公司&nbsp;App&nbsp;内绑定建行数字人民币个人钱包，将其作为支付方式之一进行交易服务、资讯服务和研究服务等。这次合作实现了证券公司保证金账户与数字人民币体系的互联互通。这次合作不仅为财富管理发展提供了思路和借鉴模式，拓宽了投资者参与财富管理的场景选择，优化了客户操作流程，保障了客户资金安全，并提供了良好的客户体验。建设银行表示将继续在数字人民币应用和创新方面发挥经验，为中国数字经济的发展做出贡献。</p><p></p><p></p><h2>信也科技宣布在三个层次深化小微服务布局</h2><p></p><p></p><p>7&nbsp;月&nbsp;2&nbsp;日，信也科技&nbsp;COO&nbsp;王玉翔在投融会上宣布将在三个层次深化小微服务布局。首先，强化科技型“小店”金融业务以满足小微真实需求；其次，加强数字化服务产品矩阵建设，提升小微用户数字化能力；第三，加速推进<a href=\"https://xie.infoq.cn/article/e2d14f4b536ee42110780d704\">&nbsp;SaaS&nbsp;</a>\"业务布局，在垂直小微服务领域提供产业科技服务。信也科技将利用联邦学习等新技术建立全生命周期管理模型，精准服务小微企业。通过智能获客、风控和运营平台，帮助金融机构扩大业务规模、降低成本、提升效率。信也科技已为近&nbsp;80&nbsp;家金融机构提供数字化服务，为&nbsp;2800&nbsp;万用户提供信贷便利化服务。信也科技希望通过科技力量让普惠金融覆盖更多人群，并将继续致力于为小微企业提供更好的服务。</p><p></p><p></p><h2>中国银行“绿洲工程”全面投产，数字化转型迈出新步伐</h2><p></p><p></p><p>中国银行“绿洲工程”在&nbsp;7&nbsp;月&nbsp;2&nbsp;日迎来大规模投产上线，借记卡、信用卡业务完成全国推广，反洗钱系统重构升级，标志着中国银行在数字化转型方面取得新进展。通过创新完善信创技术平台，提升业务响应能力，支持高频借记卡、信用卡业务，提供&nbsp;24&nbsp;小时不间断服务。新一代数字化技术底座为个人业务发展提供支撑。中国银行还结合业务特点，运用混合事务处理架构数据库技术，提升交易处理能力和数据分析能力，满足反洗钱业务要求。未来，中国银行将继续推进“绿洲工程”，提升数字化转型能力，支持实体经济和社会民生。</p><p></p><p></p><h2>稠州银行成功实施首例数字人民币跨境收款业务</h2><p></p><p></p><p>近日，稠州银行通过创新数字人民币应用成功实现了首单数字人民币跨境收款业务。这一举措推动了数字人民币智能合约应用，为人民币国际化提供了新思路，为跨境交易带来了新突破。数字人民币具有可编程性、实时到账和安全性等特点，提高了交易透明度和效率，减少了结算周期和交易成本，促进了人民币在国际贸易中的使用和地位的提升。稠州银行将继续推动数字人民币跨境结算服务，为企业提供安全、高效的服务，助力人民币国际化进程。</p><p></p><p></p><h2>临港新片区启动金融智算平台</h2><p></p><p></p><p>7 月 4 日，在上海自贸区临港新片区举办了滴水湖新兴金融大会·夏季大会——亚金协金融科技年度论坛。论坛上，正式启动了名为“滴水湖金融湾——新兴金融智算融合创新平台”的项目。该平台由临港新片区经济公司与中国信通院、新型互联网交换中心、临港科技城、跨境数科、移动、电信、联通以及有孚、城地、龙丰、海兰信、浪潮等智算企业共同合作打造。平台以临港新片区智算中心为核心，利用人工智能技术推动金融数据产业的快速发展，激发金融数据潜能，促进金融机构的云网融合和便捷连通。此外，平台还支持共建高能级研发机构和功能性平台，建立起新片区管委会、科研院所、产业园区和新兴金融企业等多层次的协同合作创新平台，为临港新片区金融科技产业的发展提供有力支持。</p><p></p><p></p><h2>恒生电子推出数智金融新品</h2><p></p><p></p><p>恒生电子及其子公司恒生聚源于 6 月 28 日发布了数智金融新品，包括金融智能助手光子和智能投研平台 WarrenQ 。此外，恒生电子还推出了金融行业大模型 LightGPT ，并公布了最新的研发进展。</p><p></p><p>金融智能助手光子，能够解决大模型在金融业务中的技术、应用和数据安全问题。光子可以为金融机构提供投顾、客服、运营、合规、投研、交易等业务系统注入 AI 能力。光子在咨询、创作、合规和运营场景上展示了其能力，可以提供精准的咨询观点和建议，自动生成专业文案，以及智能处理合规和运营系统中的文档和参数。光子聚集了各类金融数据，并实现了数据的保护、合规和授权。试用将于 9 月正式开放。</p><p></p><p>WarrenQ 是一款专业的投研工具平台，通过智能对话的方式提高投研效率，打破传统投研信息孤岛。平台推出了两款AI工具产品： WarrenQ-Chat 和 ChatMiner 。WarrenQ-Chat 利用大模型叠加搜索和聚源金融数据库，通过对话指令获取金融行情、资讯和数据，并生成金融专业报表。ChatMiner 则是一款金融文档挖掘器，能够快速解读指定文档，提取关键信息，智能化处理海量文本数据。WarrenQ 将继续发展，结合更多场景输出智能工具，助力投研数智化发展。</p><p></p><p>LightGPT 具备更专业、更合规、更轻量的特点。LightGPT 使用了超大规模的金融语料和语种强化数据进行训练，支持 80+ 金融专属任务指令微调，具备金融领域的准确理解能力。在金融大模型能力评测中表现出色，同时保证内容和指令的合规安全，可以为金融业务场景提供底层 AI 能力支持。LightGPT 还具有丰富、轻量化的部署方式，支持私有化/云部署以及灵活 API 调用。预计将于 9 月底完成新一轮的金融能力升级，并开放试用接口。</p><p></p><p></p><h2>外资银行“星展”在中国推出企业数字人民币收款解决方案</h2><p></p><p></p><p>星展银行在中国推出企业数字人民币收款解决方案，成为为数不多提供该方案的外资银行之一。该解决方案允许星展中国的企业客户以数字人民币形式接收消费者付款，并自动转存到企业的人民币结算账户。该方案的优势包括：无需人工结算、双重离线功能以及通过企业网银&nbsp;IDEAL&nbsp;获取数字人民币交易明细的综合商户报告。</p>",
    "publish_time": "2023-07-06 14:34:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT又断网了！OpenAI暂时下线ChatGPT搜索功能，只因绕过付费墙？",
    "url": "https://www.infoq.cn/article/8EixxqlrSE5O10k76XyJ",
    "summary": "<p></p><blockquote>一夜之间，ChatGPT 又回到了 2021 年。</blockquote><p></p><p></p><h2>OpenAI暂停ChatGPT Bing搜索功能</h2><p></p><p>&nbsp;</p><p>近日，OpenAI 发布通知称：</p><p></p><p></p><blockquote>自 2023 年 7 月 3 日起，出于谨慎考虑，我们已禁用“使用 Bing 浏览”测试版功能，同时我们会修复此问题，以维护内容所有者的权益。我们正在努力尽快恢复测试版，感谢您的理解！</blockquote><p></p><p>&nbsp;</p><p>OpenAI 表示，ChatGPT 浏览 Bing 是一个测试版功能，可供 ChatGPT Plus 订阅者使用（ChatGPT Plus 是 ChatGPT 的高级版本，每月收费 20 美元，订阅者可以优先使用新功能和改进，在对话期间加快响应时间，甚至在需求高峰期也可以访问 ChatGPT），它允许 ChatGPT 搜索互联网以帮助回答从最新信息中受益的问题。OpenAI 了解到，该功能有时会以 OpenAI 不希望的方式显示内容。例如，如果用户专门请求 URL 的全文，则可能会无意中满足此请求。</p><p>&nbsp;</p><p>这也意味着，目前 ChatGPT 又回到了对 2021 年 9 月以后的世界一无所知的状态（该版本模型的训练数据截止于此）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/41/4104eb2fe4dd604b70f0e4134104d6ae.png\" /></p><p></p><p>据悉，今年 3 月，网页版 ChatGPT 首次宣布<a href=\"https://www.infoq.cn/article/sqGLAIdIKP1jv2YKMd3C\">联网功能</a>\"。据官方博客介绍，此次联网功能的实现得益于 OpenAI 为 ChatGPT 增加了插件使用功能，“插件是专门为语言模型设计的工具，以安全为核心原则，并帮助 ChatGPT 访问最新的信息，运行计算，或使用第三方服务。”</p><p>&nbsp;</p><p>6 月 27 日，ChatGPT 发布最新更新声明，宣布对<a href=\"https://www.infoq.cn/article/bQHRqFcQ1TlJCqHuczGR\">移动 ChatGPT</a>\" 应用程序进行了更新：用户可以使用浏览来获取有关事件和信息的全面答案和最新见解，这些信息超出了模型的原始训练数据。用户可在应用程序设置的“新功能”部分中启用浏览，然后在模型切换器中选择 GPT-4，并在下拉列表中选择“使用 Bing 浏览”。至此，移动设备上的 ChatGPT 现在也可以上网了。</p><p>&nbsp;</p><p>不过如今，该联网功能已被叫停，OpenAI 没有给出何时重新启用 Bing 浏览的时间表，但表示他们正在尽快恢复该功能。</p><p></p><h2>ChatGPT断网后，用户怒火被点燃</h2><p></p><p>&nbsp;</p><p>ChatGPT 用户们对 OpenAI 的这一决定并不买账。</p><p>&nbsp;</p><p>有用户称他就是为了用上 Bing 网络搜索功能，才愿意付费订阅 ChatGPT Plus。“看来 OpenAI 正在针对 ChatGPT Plus 的付费用户，”一位 ChatGPT Plus 用户在 OpenAI 论坛上说道。“这次他们取消了浏览功能，因为它可以读取用户请求的网站内容？拜托，这就是我为 Plus 付费的原因。”</p><p>&nbsp;</p><p>也有用户在论坛中表达了自己的担忧，怀疑未来 ChatGPT 可能不再支持对网站内容的翻译功能。ChatGPT 用户 Thiago Ramos 坦言，“假设我需要用 ChatGPT 来阅读 GitHub 上的某个代码仓库或者主题，或者翻译目标论坛上他国语种的信息，结果你告诉我这些都做不到？而且现在 ChatGPT 4 的反应也越来越差了，特别容易犯错误。在某些方面，连 3.5 版本都比它做得好。”</p><p>&nbsp;</p><p>有外媒就此事联系了 OpenAI，询问关于此项决定的几个问题。对方回复了邮件，但仅仅是列出一条与更新后的帮助页面内容相似的推文。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/0352d6d8f9d0591a9e7ca839ebe0daab.png\" /></p><p></p><p>OpenAI称：我们了解到，ChatGPT 的“Browse”beta 版有时会以意外方式显示内容。例如，若用户坚持请求目标 URL 指向的全文，其可能在无意中满足这一请求。我们将暂时禁用 Browse 功能并修复相关问题，希望维护内容所有者的应有权益。</p><p></p><h2>总有办法绕过付费墙？</h2><p></p><p>&nbsp;</p><p>付费墙（Pay Walls）是指对在线内容实行付费阅读，为网上的内容设立收费门槛，这是一种常见的盈利模式。在网络上，总能看到各式各样的绕过付费墙方案。不少企业也曾与用户斗智斗勇，阻止用户通过各种方式绕过付费墙。</p><p>&nbsp;</p><p>比如，《纽约时报》之前就曾使用“发出文件系统 API 请求”这个技术，防止访问者利用隐身模式来绕过他们网络上的付费墙以及限制免费文章的数量。</p><p>&nbsp;</p><p>微软的 Bing AI 聊天机器人本身是由 OpenAI 的 ChatGPT 提供支持，而且与谷歌 Bard 一样于今年 2 月正式上线。二者与 ChatGPT 的不同之处在于，它们都能访问网络来获取更新的相关信息。但几个月来，用户确实报告称这两款机器人均能绕过付费墙，提供大量原本需要花钱订阅才能查看的信息。</p><p>&nbsp;</p><p>目前还不清楚两家公司是否已经出手处理。Bard 和 Bing 最新的更新说明，也都未提及是否通过改造限制了这种绕过付费墙的能力。但有消息人士称，目前再以这种方式使用，两款机器人都会予以回绝。</p><p>&nbsp;</p><p>Bing 的回复是“我无法显示[您请求的网站]或任何其他受到版权保护的出版物文章的完整内容”，但会主动给出相关主题的概括和报道。在被要求提供付费文章的副本时，Bard 的回答更为简洁：“我只是个语言模型，所以无法帮到您。”</p><p>&nbsp;</p><p>有外媒就此事询问了微软和谷歌对于OpenAI最新举措的看法，包括在 AI 打破付费墙的问题上持何种立场，但双方均未做出回应。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://help.openai.com/en/articles/8077698-how-do-i-use-chatgpt-browse-with-bing-to-search-the-web\">https://help.openai.com/en/articles/8077698-how-do-i-use-chatgpt-browse-with-bing-to-search-the-web</a>\"</p><p><a href=\"https://www.theregister.com/2023/07/05/openai_pauses_bing_search/\">https://www.theregister.com/2023/07/05/openai_pauses_bing_search/</a>\"</p>",
    "publish_time": "2023-07-06 14:48:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Service Mesh：探索分布式系统的幻觉与未来",
    "url": "https://www.infoq.cn/article/XaSoBilDNO7qy24zCOyM",
    "summary": "<p>在现代的微服务架构中，应用程序网络是实现微服务之间分布式通信的关键。无论是在单个 Kubernetes 集群中部署还是跨多个集群和不同基础设施环境中部署，都需要建立一个强大的应用程序网络，让微服务能够相互交流。这种通信不仅需要高效可靠，还需要具备适应各种逆境的韧性。</p><p></p><p>除了建立应用程序网络，我们还需要监控微服务之间的通信，即可观察性（observability）。在微服务通信中，可观察性非常重要，可以了解微服务之间的相互作用方式。此外，微服务之间的通信也需要安全保护，通信应当进行加密，防止中间人攻击。每个微服务应具有身份标识，并能够证明其与其他微服务之间的授权通信。</p><p></p><p>那么，为什么需要<a href=\"https://www.infoq.cn/article/stCMjmTuODmzZmGzaNUr\">服务网格</a>\"（Service Mesh）呢？为什么这些需求不能在 Kubernetes 中满足？答案在于 Kubernetes 的架构和设计目标。正如之前提到的，Kubernetes 是应用程序生命周期管理软件，它提供了基本级别的应用程序网络、可观察性和安全性支持，但无法满足现代动态微服务架构的需求。这并不意味着 Kubernetes 不是现代化的软件，它确实是一项非常先进和前沿的技术，但它主要用于容器编排。</p><p></p><p>在 Kubernetes 中，流量管理由 Kubernetes 网络代理（kube-proxy）负责。kube-proxy在每个节点上运行，并与 Kubernetes API 服务器通信，获取关于 Kubernetes 服务的信息。Kubernetes 服务是一种将一组 Pod 作为网络服务公开的抽象层。kube-proxy通过设置 iptables 规则，定义了如何将流量路由到对应的端点（实际上是承载应用程序的底层 Pod）。</p><p></p><p>这就是服务网格发挥作用的地方。服务网格通过提供高级流量管理、可观察性和安全性功能，弥补了 Kubernetes 的不足。服务网格位于应用程序层，并与微服务并行工作，拦截和管理它们之间的通信。借助服务网格，您可以实现细粒度的流量控制、收集丰富的遥测数据以实现可观察性，并强制实施微服务之间的安全通信。</p><p></p><p>服务网格，例如 Istio、FloMesh、和 Linkerd，与 Kubernetes 紧密集成，并增强其功能，以满足现代微服务架构的要求。通过采用服务网格，组织可以实现微服务部署的增强韧性、可观察性和安全性。</p><p></p><p>服务网格技术填补了 Kubernetes 在微服务架构中先进的应用程序网络、可观察性和安全性方面的不足。它提供了一个强大且灵活的基础设施层，与 Kubernetes 互补，使组织能够构建和运行具备韧性和安全性的分布式系统。通过采用服务网格，组织可以实现微服务部署的增强韧性、可观察性和安全性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44aa0140e9df7d7c068929c60aeef6f8.png\" /></p><p></p><h2>分布式系统谬误</h2><p></p><p></p><p>当设计分布式应用程序时，软件开发人员经常会犯一些错误的假设，这些假设被称为\"分布式系统的谬论\"（Fallacies of a distributed system）。这些谬论最初由 L Peter Deutsch 和其他 Sun Microsystems 的工程师提出，并被广泛接受。它们揭示了关于分布式系统性质和挑战的常见误解。下面是这些谬论的总结：</p><p></p><p>1. 网络是可靠的：这个假设认为网络始终可用且没有故障。然而，在现实中，网络可能会出现中断、故障或间歇性的连接问题。</p><p></p><p>2. 延迟为零：这个谬论假设网络传输数据时没有延迟。然而，在实际情况下，网络延迟受到距离、拥塞和处理时间等因素的影响，会有不同程度的延迟。</p><p></p><p>3. 带宽是无限的：这个假设认为网络传输的数据量没有限制。然而，在现实中，网络带宽是有限的，会存在拥塞和性能降低的情况。</p><p></p><p>4. 网络是安全的：这个谬论认为网络本身是安全的，能够防止未经授权的访问或数据泄漏。实际上，网络需要强大的安全措施来确保机密性、完整性和可用性。</p><p></p><p>5. 拓扑结构不会改变：这个假设认为网络的结构和配置在时间上保持静态不变。然而，网络是动态的，节点可能加入、离开或改变连接，应用程序需要对此进行适应。</p><p></p><p>6. 只有一个管理员：这个谬论认为单个实体对整个网络具有完全的控制和管理权。实际上，分布式系统通常涉及多个管理员，他们拥有不同的控制权和责任。</p><p></p><p>7. 传输成本为零：这个假设认为在网络中传输数据没有任何成本。然而，在现实中，网络基础设施、带宽使用等因素都会带来成本。</p><p></p><p>8. 网络是同质的：这个谬论认为网络的各个组件具有相同的特性和统一的行为。实际上，网络可能由不同类型的设备、操作系统、协议和能力组成。</p><p></p><h2>服务治理痛点</h2><p></p><p></p><h3>1. 多语言、多技术栈</h3><p></p><p></p><p>微服务架构中，团队可能使用不同的编程语言和技术栈来开发微服务。这导致了多语言和多技术栈的挑战，需要找到一种统一的方式来协调不同语言的微服务之间的通信和交互。这涉及到如何选择合适的通信协议、数据格式以及寻找跨语言的解决方案。</p><p></p><h3>2. 有侵入性</h3><p></p><p></p><p>传统的服务治理解决方案可能需要对现有的微服务代码进行侵入性的修改或添加额外的依赖库，以实现服务发现、负载均衡、故障转移等功能。这样的侵入性可能增加了开发团队的工作量，并且可能引入不必要的复杂性和风险。</p><p></p><h3>3. 重复建设</h3><p></p><p></p><p>在大规模微服务架构中，可能存在大量的微服务实例需要进行服务治理。在传统的方式下，每个微服务都需要重复构建和维护自己的服务发现、负载均衡、故障转移等功能，导致了重复建设的问题。这不仅浪费了开发资源，还增加了系统的复杂性和维护成本。</p><p></p><h3>4. SDK版本碎片化</h3><p></p><p></p><p>当微服务通过共享的SDK进行通信时，不同微服务可能使用不同版本的SDK。这导致了SDK版本碎片化的问题，可能会导致兼容性和一致性方面的挑战。当需要更新SDK版本或解决SDK中的漏洞时，需要协调和管理各个微服务的SDK版本，这增加了额外的复杂性和风险。</p><p></p><h3>5. 跨机房、跨地域调度</h3><p></p><p></p><p>在分布式系统中，微服务可能部署在不同的机房或地域中。在进行跨机房或跨地域的调度时，需要考虑网络延迟、带宽限制等因素，以确保微服务之间的通信效率和质量。这需要一种灵活而智能的调度策略，能够根据实际情况进行动态的负载均衡和故障转移。</p><p></p><h2>演进趋势</h2><p></p><p></p><h3>1. 扩展性</h3><p></p><p></p><p>Service Mesh 提供了一系列的功能来增强微服务的扩展性。这包括熔断、限流、超时控制、灰度发布和故障注入等机制，以确保微服务在面对高负载、异常情况或部分故障时能够保持稳定和可靠。Service Mesh 支持多种通信协议，如HTTP、gRPC、Dubbo、Thrift等，使得这些功能可以适用于不同类型的微服务。</p><p></p><h3>2. 连通性</h3><p></p><p></p><p>Service Mesh 提供了强大的连接管理功能，确保微服务之间的连通性。它通过服务发现和负载均衡机制，使得微服务能够自动发现和定位其他微服务，并能够实现跨不同语言和技术栈的通信。Service Mesh 还提供了智能路由和流量控制功能，以便在微服务之间实现灵活的流量转发和负载均衡策略。</p><p></p><h3>3. 性能与资源</h3><p></p><p></p><p>Service Mesh 关注微服务架构的性能和资源管理。它通过优化网络通信、请求处理和数据传输等方面的性能，提高微服务的响应速度和吞吐量。同时，Service Mesh 还提供了对微服务的监控、追踪和日志记录功能，以便进行性能分析、故障排查和容量规划等任务。通过对资源的细粒度管理和调控，Service Mesh 可以有效地提高微服务架构的资源利用率和效率。</p><p></p><h3>4. 易用性</h3><p></p><p></p><p>Service Mesh 设计了一套简洁易用的接口和控制平面，使得开发人员和运维人员可以轻松地配置、管理和监控微服务。它提供了可视化的管理界面和命令行工具，简化了配置和部署的流程。同时，Service Mesh 还支持自动化的服务注册和发现，减轻了手动管理的负担。通过这些易用性的特点，Service Mesh 提供了一种简单而高效的方式来管理和维护微服务架构。</p><p></p><h2>Istio &amp; FloMesh</h2><p></p><p></p><h3>1. 相同点</h3><p></p><p></p><p>Istio 和 FloMesh 都使用了 sidecar 模型，它们具有这些优点： 解耦服务逻辑和网络治理、统一的通信层、动态的网络治理、安全性和可观测性增强、无侵入性。</p><p></p><p>但是在使用的同时也增加了一些损耗，具体可以从下面的示例图中看出：sidecar模型在服务之间进行通讯的时候有三个 connection 需要维护。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c3/c32f01717a6fec734d2fe3887b7e4744.png\" /></p><p></p><h3>2. 不同点</h3><p></p><p></p><p>Istio 固有地支持诸如断路、速率限制、超时控制、金丝雀部署和故障注入等机制。一旦正确安装了 Istio，这些特性就可以开箱即用了。 而FloMesh 采取了不同的方法。它提供了一个基于 JavaScript 的可扩展性模型，允许开发人员根据他们的具体需求定制和扩展这些功能。通过编写 JavaScript 代码，FloMesh 实现了对这些机制的细粒度控制，给予开发人员更多的灵活性，并使其能够根据自己的独特需求进行调整。</p><p></p><p>在 Istio 和 FloMesh 之间的选择应该基于你的具体需求和优先事项。Istio 提供了具有广泛内置特性的健壮和成熟的服务网格解决方案，而 FloMesh 为那些寻求对其服务网格功能进行细粒度控制的人提供了更可定制的方法。考虑诸如易用性、开发灵活性。</p><p></p><p>看看下面的图或许就能够理解了设计的区别：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/60373df0219c2cc02c634a6f558b1b20.png\" /></p><p></p><h2>未来</h2><p></p><p></p><h3>1. Wasm sidecar</h3><p></p><p></p><p>采用 WebAssembly (Wasm) sidecar 形式相对于传统的 sidecar 具有以下优势：</p><p></p><p>跨平台兼容性：Wasm 是一种可移植的二进制格式，可以在不同的操作系统和架构上运行。使用 Wasm sidecar 可以轻松实现跨平台部署，而无需担心依赖于特定环境的问题。</p><p></p><p>轻量高效：Wasm 代码通常比传统的 sidecar 容器更小、更轻量，因为它是一种二进制指令格式。这使得 Wasm sidecar 在启动时间和资源消耗方面表现更加高效，提供更快的响应和更低的延迟。</p><p></p><p>安全性：Wasm 提供了一种沙箱环境，在其中运行代码可以被有效地隔离和限制。使用 Wasm sidecar 可以增加安全性，因为它可以将应用程序与主机环境隔离开来，防止恶意代码的影响。</p><p></p><p>可扩展性：由于 Wasm 的灵活性，可以使用多种编程语言编写 Wasm 模块，而不仅仅局限于特定的编程语言或框架。这使得开发人员能够选择最适合他们需求的语言和工具，并提供更大的灵活性和可扩展性。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8d/a6/8d083b68205fd3b33906fdeb8f8dfda6.png\" /></p><p></p><p></p><h3>2. Ambient Mesh</h3><p></p><p></p><p>通过采用每个节点的代理模型，我们能够摆脱其中一个代理的需求，因为我们不再依赖于在每个工作负载内运行一个附属容器。这种转变带来了 ambient mesh 相对于 sidecar 模型的几个显著优势。</p><p></p><p>首先，使用 ambient mesh，我们可以减少运行在每个工作负载内的附属容器数量，从而减轻了系统的负担和复杂性。</p><p></p><p>其次，ambient mesh 不再依赖于每个工作负载的 sidecar 容器，这意味着我们不再需要为每个工作负载额外的连接。虽然仍然需要一些额外的连接，但这比始终需要两个额外连接要好得多。</p><p></p><p>最重要的是，ambient mesh 提供了更灵活的部署选项。由于不再需要在每个工作负载内运行附属容器，我们可以更轻松地将应用程序部署到不同的环境中，而无需过多的修改。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c20ffbbebd29b30f6f9aa3be7b63129.webp\" /></p><p></p><h3>3. eBPF</h3><p></p><p></p><p>在每个工作负载中运行附属容器会导致大量的代理实例，即使每个代理实例的内存占用已经进行了优化，但实例数量的增加仍会对整体系统造成重大影响。此外，每个代理还要维护诸如路由和终端点表等数据结构，随着集群规模的增长，这些数据结构也会增加，导致每个代理所消耗的内存随着集群规模的扩大而增加。为了解决这个问题，一些服务网格尝试将部分路由表推送到各个代理中，以限制它们的路由范围。</p><p></p><p>eBPF 是一种灵活的内核扩展框架，它允许在内核空间中执行自定义的网络过滤和处理逻辑。相比于运行在用户空间的附属容器，eBPF 的运行开销更低，因为它直接在内核中执行，避免了用户态和内核态之间的频繁切换。</p><p></p><p>采用 eBPF 而不使用附属容器具有轻量高效、低延迟、强大的可编程性、节省系统资源以及简化部署和管理的优势。这使得 eBPF 成为一种强大的工具，在现代网络环境中实现高性能和灵活的网络处理和功能成为可能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdb504d30e2ba1fc0b342f2d46a13b22.png\" /></p><p></p><h2>总结</h2><p></p><p></p><p>微服务架构中的应用程序网络是实现微服务之间分布式通信的关键。为了建立高效可靠的通信，需要具备适应各种逆境的韧性。此外，可观察性和安全性也是微服务通信中的重要方面。</p><p></p><p>服务网格位于应用程序层，与微服务并行工作，拦截和管理微服务之间的通信。它能实现细粒度的流量控制、收集丰富的遥测数据以实现可观察性，并强制实施微服务之间的安全通信。一些常见的服务网格技术包括Istio、FloMesh和Linkerd，它们与Kubernetes紧密集成，增强其功能，满足现代微服务架构的要求。</p><p></p><h4>作者介绍</h4><p></p><p></p><p>陈章朝，政采云有限公司运维开发工程师，一个热爱生活的编程爱好者，热于参与开源社区共建以及分享知识。</p>",
    "publish_time": "2023-07-06 15:09:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面向大模型的存储加速方案设计和实践",
    "url": "https://www.infoq.cn/article/SX3U3EMprDPUXe32C0Cm",
    "summary": "<p>在《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课第三讲中，百度智能云资深工程师陈志鹏分享了大模型对存储的全新挑战、大模型全流程存储问题的解决思路以及百度沧海存储加速方案和实践。</p>\n<p>在本期公开课中，他首先介绍了大模型对存储的全新挑战，从经典 AI 到大模型，本地存储不再适用，大模型全流程对存储有了更高的需求；针对这一问题，他分享了大模型全流程存储问题的解决思路，而数据的流动和性能是需要解决的主要问题；最后，他分享了百度沧海存储加速方案和实践，包括百度沧海 RapidFS 产品架构和 RapidFS 模型分发加速效果。</p>",
    "publish_time": "2023-07-06 15:17:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI大模型狂飙的背后：高性能计算网络是如何“织”成的？",
    "url": "https://www.infoq.cn/article/NeWITbYb6hjMaRkvSZOG",
    "summary": "<p>ChatGPT 的爆火掀起了 AI 大模型狂飙热潮，随着国内外原来越多的 AI 大模型应用落地，AI 算力需求快速增加。在算力的背后，网络起到至关重要的作用——网络性能决定 GPU 集群算力，网络可用性决定 GPU 集群算力稳定性。因此，高性能与高可用的网络对 AI 大模型的构建尤为重要。</p><p></p><p>6 月 26 日，腾讯云举办《面向 AI 大模型的高性能网络》沟通会，首次对外完整披露自研星脉高性能计算网络，并梳理了腾讯的网络架构演进历程。会后，腾讯云副总裁王亚晨、腾讯云数据中心网络总监李翔接受了 InfoQ 在内的媒体采访，进一步分享面向 AI 大模型的高性能网络是如何构建的。</p><p></p><p>据了解，星脉网络具备业界最高的3.2T通信带宽，可提升40%的GPU利用率、节省30%~60%的模型训练成本，进而能为AI大模型带来10倍通信性能提升。基于腾讯云新一代算力集群，可支持10万卡的超大计算规模。</p><p></p><p>王亚晨表示：“星脉网络是为大模型而生。它所提供的大带宽、高利用率以及零丢包的高性能网络服务，将助力算力瓶颈的突破，进一步释放 AI 潜能，全面提升企业大模型的训练效率，在云上加速大模型技术的迭代升级和落地应用。”</p><p></p><h2>AI大模型时代需要什么样的网络？</h2><p></p><p></p><h4>大带宽、高利用率、无损</h4><p></p><p></p><p>AI 大模型训练需要海量算力的支撑，而这些算力无法由单台服务器提供，需要由大量的服务器作为节点，通过高速网络组成集群，服务器之间互联互通，相互协作完成任务。有数据显示，GPT-3.5 的训练使用了微软专门建设的 AI 计算系统，由 1 万个 V100 GPU 组成的高性能网络集群，总算力消耗约 3640 PF-days (假如每秒计算一千万亿次，需要计算 3640 天)。</p><p></p><p>如此大规模、长时间的 GPU 集群训练任务，仅仅是单次计算迭代内梯度同步需要的通信量就达到了百 GB 量级，此外还有各种并行模式、加速框架引入的通信需求。如果网络的带宽不够大、延时长，不仅会让算力边际递减，还增加了大模型训练的时间成本。因此，大带宽、高利用率、无损的高性能网络至关重要。</p><p></p><p>王亚晨表示，大模型运算实际上是一个通信过程，一部分 GPU 进行运算，运算完成后还需要与其他 GPU 之间交互数据。通信带宽越大，数据传输越快，GPU 利用率越高，等待时间就会越少。此外，大模型训练对时延和丢包要求也比较高。“假设有很多 GPU 运算同一个任务，因为有木桶效应存在，一定要等花费时间最长的 GPU 运算完之后，才能完成一个运算任务。AI 对于时延的敏感度比 CPU 高很多，所以一定要把木桶效应消除，把时延控制在非常短的水平，让 GPU 的效率更高。此外，和带宽、时延相比，丢包对 GPU 效率的影响更加明显，一旦丢包就需要重传，重新进行 GPU 的训练。”</p><p></p><p>王亚晨认为，大集群不等于大算力。集群训练会引入额外的通信开销，导致&nbsp;N&nbsp;个&nbsp;GPU&nbsp;算力达不到单个 GPU&nbsp;算力的&nbsp;N&nbsp;倍。这也意味着，一味地增加 GPU 卡或计算节点，并不能线性地提升算力收益。“GPU 利用率的合理水平大概是在 60% 左右。”王亚晨说道。</p><p></p><p>要想通过集群发挥出更强的算力，计算节点需协同工作并共享计算结果，需要优化服务器之间的通信、拓扑、模型并行、流水并行等底层问题。高速、低延迟的网络连接可以缩短两个节点之间同步梯度信息的时间，使得整个训练过程变得更快。同时，降低不必要的计算资源消耗，使计算节点能够专注于运行训练任务。</p><p></p><h4>AI大模型驱动DCN网络代际演进</h4><p></p><p></p><p>据介绍，腾讯网络主要提供的功能是“连接”，一是连接用户到机器的流量，二是连接机器到机器的流量。目前，腾讯的网络架构主要分三大部分：</p><p></p><p>ECN 架构，表示不同类型的客户通过多种网络方式接入云上虚拟网络，这一块主要是外联架构，主要包括终端用户、企业用户、物联网用户分别通过运营商专线、企业专线、边缘网关接入腾讯数据中心。DCI 网络，主要是数据中心之间的互联，实现一个城市多数据中心或者多个城市的数据中心进行互联，底层会用到光纤传输。DCN，主要是数据中心的网络，这部分的任务是实现数据中心里面超过 10 万或者几十万服务器进行无阻塞的连接。</p><p></p><p>腾讯通过 ECN、DCI、DCN 等网络，把用户和业务服务器连接起来，并且把数百万台服务器连接起来。</p><p></p><p>王亚晨表示，AI 大模型的发展驱动了 DCN 网络代际演进。</p><p></p><p>在移动互联网时代，腾讯的业务以 to C 为主，数据中心网络服务器规模并不大，当时主要解决的是数据中心、服务器之间的互联，以及运营商之间的互联。所以那时数据中心流量特征很明显，基本都是外部访问的流量，对网络的时延和丢包要求也不高。</p><p></p><p>随着移动互联网以及云的快速发展，数据中心网络流量模型发生了变化，除了有从运营商访问过来的南北向流量，也有数据中心之间互访的东西向流量，对网络的时延要求也是从前的 10 倍。为了降低设备故障对网络的影响，腾讯采用多平面设计，并引入了控制器的概念，把转发面和控制面进行分离。用定制的设备、多平面以及 SDN 的路由器控制，将故障的解决时间控制在一分钟之内。</p><p></p><p>在 AI 大模型时代，数据中心网络流量模型进一步发生变化。“到了 AI 大模型时代，我们发现东西向流量比以前大了很多，尤其是 AI 在训练的时候，几乎没有什么南北向流量。我们预计如果大模型逐渐成熟，明年大模型数据中心流量南北向流量可能会有所增长，因为推理需求会上来。但就现在而言，东西向流量需求非常大，我们 DCN 网络设计会把南北向流量和东西向流量分开，以前是耦合在一张网络里，基础网络都是一套交换机，只是分不同层。但到了 GPU 时代，我们需要专门为 GPU 构建一层高性能网络。”王亚晨说道。</p><p></p><p>基于此，腾讯打造出了高性能网络星脉：具备业界最高的 3.2T 通信带宽，能提升 40% 的 GPU 利用率，节省 30%~60% 的模型训练成本，为 AI 大模型带来 10 倍通信性能提升。基于腾讯云新一代算力集群 HCC，可支持 10 万卡的超大计算规模。</p><p></p><h2>高性能网络星脉是如何设计的？</h2><p></p><p></p><p>据李翔介绍，腾讯网络大概由大大小小几十个组件组成，数据中心网络是其中最大、历史最悠久的一个。在 PC 和移动互联网时代，数据中心网络主要解决的是规模问题。而进入算力时代，业务对算力网络有了更高的要求。</p><p></p><p>“举个例子，如果说过去两个阶段数据中心网络是‘村村通’，解决大规模部署和广覆盖的问题，那么在算力时代，数据中心网络就是全自动化、无拥塞的高速公路。”李翔表示，AI 大模型对互联有比较高的要求，几千张 GPU 协同计算，如果出现任何一个丢包阻塞，那么全部都要降速，这种降速 1 分钟就有几十万的损失。</p><p></p><p>基于此，腾讯云开始搭建算力集群。4 月 14 日，腾讯云正式发布面向大模型训练的新一代 HCC（High-Performance Computing Cluster）高性能计算集群。网络层面，计算节点间存在海量的数据交互需求，随着集群规模扩大，通信性能会直接影响训练效率。腾讯自研的星脉网络，为新一代集群带来了业界最高的 3.2T 的超高通信带宽。</p><p></p><p>据介绍，腾讯对大模型集群网络做了以下几大优化：</p><p></p><p>（1）采用高性能RDMA网络</p><p></p><p>RDMA（GPU之间直接通信），是一种高性能、低延迟的网络通信技术，主要用于数据中心高性能计算，允许计算节点之间直接通过GPU进行数据传输，无需操作系统内核和CPU的参与。这种数据传输方法可以显著提高吞吐量并降低延迟，从而使计算节点之间的通信更加高效。</p><p></p><p>过往的数据中心VPC网络，在源服务器与目标服务器之间传输时，需要经过多层协议栈的处理，过往数据每一层都会产生延迟，而腾讯自研的星脉RDMA网络，可以让GPU之间直接进行数据通信。</p><p></p><p>打个比方，就像之前货物在运输途中需要多次分拣和打包，现在通过高速传送带、不经过中间环节，货物直接送到目的地</p><p></p><p>同时，由于星脉RDMA直接在GPU中传输数据，CPU资源得以节省，从而提高计算节点的整体性能和效率。</p><p></p><p>（2）自研网络协议（TiTa）</p><p></p><p>在网络协议上，腾讯云通过自研TiTa协议，让数据交换不拥塞、时延低，使星脉网络可以实现90%负载0丢包。</p><p></p><p>网络协议是在计算节点之间传输数据的规则和标准，主要关注数据传输的控制方式，能改善网络连接性能、通信效率和延迟问题。</p><p></p><p>为了满足大型模型训练中的超低时延、无损和超大带宽要求，传统的网络协议由于其固有的设计与性能限制，无法满足这些需求，还需要对“交通规则”进行优化。</p><p></p><p>星脉网络采用的自研端网协同协议TiTa，可提供更高的网络通信性能，特别是在满足大规模参数模型训练的需求方面。TiTa协议内嵌拥塞控制算法，以实时监控网络状态并进行通信优化，使得数据传输更加流畅且延迟降低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dd/dd349429d99a9e3ea9db7e9e7708cac4.png\" /></p><p></p><p>（3）定制化高性能集合通信库TCCL</p><p></p><p>通信库在训练过程中负责管理计算节点间的数据通信。面对定制设计的高性能组网架构，业界开源的GPU集合通信库（比如NCCL）并不能将网络的通信性能发挥到极致，从而影响大模型训练的集群效率。</p><p></p><p>为解决星脉网络的适配问题，腾讯云还为星脉定制了高性能集合通信库TCCL（Tencent Collective Communication Library），相对业界开源集合通信库，可以提升40%左右的通信性能。</p><p></p><p>并在网卡设备管理、全局网络路由、拓扑感知亲和性调度、网络故障自动告警等方面融入了定制设计的解决方案。</p><p></p><p>（4）多轨道网络架构</p><p></p><p>星脉网络对通信流量做了基于多轨道的流量亲和性规划，使得集群通信效率达80%以上。</p><p></p><p>多轨道流量聚合架构将不同服务器上位于相同位置的网卡，都归属于同一ToR switch；不同位置的网卡，归属于不同的ToR switch。由于每个服务器有8张计算平面网卡，这样整个计算网络平面从物理上划分为8个独立并行的轨道平面。</p><p></p><p>在多轨道网络架构中，AI训练产生的通信需求（AllReduce、All-to-All等）可以用多个轨道并行传输加速，并且大部分流量都聚合在轨道内传输（只经过一级ToR switch），小部分流量才会跨轨道传输（需要经过二级switch），大幅减轻了大规模下的网络通信压力。</p><p></p><p>（5）异构网络自适应通信</p><p></p><p>大规模AI训练集群架构中，GPU之间的通信实际上由多种形式的网络来承载的：机间网络（网卡+交换机）与机内网络（NVLink/NVSwitch网络、PCIe总线网络）。</p><p></p><p>星脉网络将机间、机内两种网络同时利用起来，达成异构网络之间的联合通信优化，使大规模All-to-All通信在业务典型message size下的传输性能提升达30%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0dc12799c45d0554557f37fe717dd041.png\" /></p><p></p><p>（6）自研全栈网络运营系统</p><p></p><p>为确保星脉网络的高可用性，腾讯云还自研了端到端全栈网络运营系统，先是实现了端网部署一体化以及一键故障定位，提升高性能网络的易用性，进而通过精细化监控与自愈手段，提升可用性，为极致性能的星脉网络提供全方位运营保障。</p><p></p><p>具体应用成效方面，大模型训练系统的整体部署时间可以从19天缩减至4.5天，保证基础配置100%准确，并让系统故障的排查时间由天级降低至每分钟级，故障的自愈时间缩短到秒级。</p><p></p><h2>写在最后</h2><p></p><p></p><p>AI 大模型时代给网络带来了新的机遇与挑战。随着 GPU 算力的持续提升，GPU 集群网络架构也需要不断迭代升级。</p><p></p><p>王亚晨表示，未来，星脉网络将围绕算力网卡、高效转发、在网计算、高速互联四大方向持续迭代。“这四个迭代方向也与我们面临的痛点相关，目前我们重点发力算力网卡和高效转发这两大方向。其中，算力网卡需要与交换机做配合，实现更优的、类似主动预测控制的机制，让网络更不容易拥塞；高效转发方面，之后可能会变成定长包的转发机制，这样也能保证整体效率。”</p>",
    "publish_time": "2023-07-06 15:51:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "秦金卫确认出席 ArchSummit 深圳，分享《金融级分布式技术平台的设计与实践》话题",
    "url": "https://www.infoq.cn/article/rxRWjmLEihy5P5xtyySA",
    "summary": "<p>7&nbsp;月&nbsp;21&nbsp;日&nbsp;-&nbsp;22&nbsp;日，&nbsp;在&nbsp;<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">ArchSummit&nbsp;全球架构师峰会（深圳站）</a>\"，长亮科技平台技术部副总经理秦金卫，将于会上发表题为《金融级分布式技术平台的设计与实践》的演讲，详细介绍金融级分布式技术平台的背景、目标、功能、内容、设计、实践落地过程和经验。</p><p></p><p>秦金卫是&nbsp;Apache&nbsp;Dubbo/ShardingSphere&nbsp;PMC，前某集团高级技术总监&nbsp;/&nbsp;阿里架构师&nbsp;/&nbsp;某商业银行北京研发中心负责人。关注于互联网，电商，金融，支付，区块链等领域，熟悉海量并发低延迟交易系统的设计实现，10&nbsp;多年研发管理和架构经验，熟悉各类中间件，擅长于&nbsp;SOA/&nbsp;微服务等分布式系统架构，热爱各种开源技术，活跃于&nbsp;Dubbo/Fastjson/ActiveMQ&nbsp;等多个开源社区。合著作品有《微服务架构实战：基于&nbsp;Dubbo、Spring&nbsp;Cloud&nbsp;和&nbsp;Service&nbsp;Mesh》、《JVM&nbsp;核心技术&nbsp;32&nbsp;讲》。阿里云&nbsp;MVP、腾讯&nbsp;TVP、TGO&nbsp;鲲鹏会会员、1024&nbsp;学院&nbsp;CTO&nbsp;培训班第六届学员。</p><p></p><p>相信通过秦金卫的分享，你将了解到金融级分布式技术平台的作用及其功能特性和组件能力，收获技术平台的建设经验和发展趋势。</p><p></p><p>除上述议题外&nbsp;，ArchSummit&nbsp;深圳还将围绕<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">基础架构技术</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1532?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">DataOps、Data&nbsp;Fabric&nbsp;等高效数据开发与服务模式</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1534?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">Mesh&nbsp;技术实践案例</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1535?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">QUIC&nbsp;传输和架构优化</a>\"等进行分享。</p><p></p><p>数十位业界专家，上百个国内外一线大厂前沿技术案例，一定会给你带来很多全新的开发灵感。期待与你线下交流！咨询购票请联系&nbsp;18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/9d/aa/9d6a27547062ee2e089f91bdc4ba1eaa.png\" /></p><p></p>",
    "publish_time": "2023-07-06 16:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "超低延时直播技术演进之路",
    "url": "https://www.infoq.cn/article/iSSNhwt4qU2WuSQ6U3AM",
    "summary": "<p></p><blockquote>据中国互联网络信息中心发布的《中国互联网络发展状况统计报告》显示，截止到 2022 年 6 月我国网络直播用户规模达到了 7.16 亿，占网民整体的 68.1%。最主要原因是 2020 年度疫情期间导致居家办公和休闲娱乐的人数呈现激增，新媒体互动直播成为了广大网民最重要的休闲娱乐方式之一。随着直播产业链的不断扩展完备升级，相关产业链各个环节分工逐渐明确且各环节参与人数逐步增多；为了满足不同的就业需求，引发相关就业人数提升，通过直播形式赋能传统产业升级转型，并与高新技术融合创新，优化传统行业商业模式，如直播带货、新媒体广告传媒转型等。丰富的传统文化、新闻、竞技体育、法律、知识共享等内容，通过移动端互动直播的形式得以更加高效的展现传播，既让优质的直播内容可以实现爆发式传播扩散，又可以让用户有更多的机会感受，学习甚至主动参与直播互动，实现内容供给侧和需求传播的多方共赢。可以说，<a href=\"https://www.volcengine.com/product/live\">超低延时直播技术</a>\"正在走上一条全新的发展之路。InfoQ 将联合火山引擎视频直播团队推出《超低延时直播技术演进之路》系列，带您探索超低延时直播<a href=\"https://xie.infoq.cn/article/feb3808b91b888f3cbbe9f589\">技术</a>\"的演进历程，揭示背后的挑战和突破，以及对未来直播行业的影响。</blockquote><p></p><p></p><p></p><h2>第一篇 进化篇-超低延时直播技术的前世今生</h2><p></p><p></p><p>网络基础设施升级、音视频传输技术迭代、WebRTC 开源等因素，驱动音视频服务时延逐渐降低，使<a href=\"https://www.volcengine.com/product/live\">超低延时直播技术</a>\"成为炙手可热的研究方向。实时音视频业务在消费互联网领域蓬勃发展，并逐渐向产业互联网领域加速渗透。经历了行业第一轮的红利爆发期，我国实时音视频行业的场景效能逐渐深化，步入到理性增长阶段。</p><p></p><p>延时的指标选择很大程度上取决于用户与内容制作方的交互耦合程度，场景丰富多样。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/94/e3/948f849494c91a2fe52bf767c69bb1e3.png\" /></p><p></p><p>在这些极端场景下，延时在用户侧希望越小越好，接近于实时通信的低延迟模式可以最大化地激发用户的参与感，无缝地与内容生产方产生互动效应，调动用户所见即所得的积极性。比如在主播秀场的 PK 、送礼、工会冲榜、打赏的活动关键环节，竞争双方的储值大户都希望实时地观察到自身主播在礼物刷榜后的反应，为后台运营决策团队或者后续活动策略提供第一时间的信息反馈。</p><p></p><p>下图体现了从技术/产品/运营的三方角度来综合思考低延时直播技术的作用；从外部-内部综合因素考虑技术的变迁对整个生态正向循环的影响。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/03/45/030ed90e63a0ea4551c10cedd435ed45.jpg\" /></p><p></p><p></p><h3>（一）传统标准直播技术的局限性</h3><p></p><p></p><p></p><h4>1、RTMP 协议的延迟问题</h4><p></p><p></p><p>RTMP 协议是最传统的直播协议，主播端采用 <a href=\"https://xie.infoq.cn/article/97ac1c5c632ff199247b63d3d\">RTMP</a>\" 协议推送 H.264/5 和 AAC 编码的视音频数据到云厂商 CDN 服务器进行转封装分发，端到端延迟一般控制在 3 到 7 秒。问题是 RTMP 的可扩展性存在缺陷，同时对于延迟的进一步下探存在一定的技术困难。RTMP 协议情况下：为了满足延时降低必然压缩播放器的下载缓冲区，这样会引发显著的卡顿问题，使得播放的观感产生不舒适的感受（延时下探至 2 秒以下）。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0c/07/0cdaf133be878214df78fc5e54746a07.png\" /></p><p></p><p></p><h4>2、传统直播技术在实时互动场景中的不足</h4><p></p><p></p><p>（1）视频延时和弹幕交互的延时存在显著差异，问题聊天内容互动与视频传输图像节奏不匹配。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/71/ff/71a16bf883ef6f1670e83be37e809bff.jpg\" /></p><p>（2）观众与主播互动形式单一，是单向内容传导无法做到双向（在 <a href=\"https://www.infoq.cn/article/qdXFclAaRi1OYmHTMGcj\">RTC</a>\" 技术引入之前无法显著解决）。</p><p></p><p>（3）单向传导的局限第一个方面表现在：观众端拉流传输无法做到根据网络情况自适应调节。用户只能以固定的码率进行流媒体传输无法做到动态感知，在网络情况实时变化的场景（比如弱网，移动基站切换等）固定单向码率传输有较大概率造成丢帧卡顿等因素影响观播体验；另一方面在网络条件更好时，固定码率传输无法动态提升视频传输码率（更高的画质带来更加舒适的体验）。</p><p></p><p>（4）在直播和连麦场景共存的互动直播场景下，主播采用传统RTMP推流在遇到连麦PK场景时，会产生推流/本地连麦合流/服务器连麦合流的切换问题，这种场景变换的切换会使得观众端产生瞬间的卡顿问题；如果采用基于webRTC直播技术的超低延时直播方案，这种推流--连麦逻辑的合流切换问题可以得到比较友好的解决（只需要改变服务器转发-订阅流通道的分发逻辑，不涉及推流媒体数据流的旁路调度切换）。</p><p></p><p></p><h4>3、超低延时直播与标准直播的区别</h4><p></p><p></p><p>（1）超低延时直播是近年来新兴起的一类应用。如电商直播、赛事直播等场景，兼具高并发与低延时的特性，传统直播 3-20s 的时延难以满足其需求，但对实时互动的要求又不及视频会议等典型的实时音视频应用，无需将时延降低至 400ms 以下。 为此，超低延时直播融合了传统直播与实时音视频的技术架构，通过取长补短的方式实现了介于二者之间的端到端时延。 尽管针对超低延时直播厂商尚无一套标准的技术路径，但大体可以归纳为拉流协议、网络架构和推流协议三个方面的改造， 在实际应用过程中，厂商会平衡成本及性能指标等因素，在不同的协议和网络架构之间进行选择。</p><p></p><p>（2）传输层协议的差异 （基于 UDP 协议的可靠性优化，为弱网对抗策略提供依据）</p><p></p><p>传统直播 FLV/RTMP 等采用的是 TCP 协议（或者 QUIC 协议）TCP 是牺牲传输实时性来换取数据完整性的可靠传输协议。弱网环境下，其在数据传输前的“三次 握手”连接会带来较大延时。而 UDP 作为不可靠的传输协议，其最大的优点为高实时性，但不保证数据的到达和排序。 实时音视频产品（如 RTM 超低延时直播）往往采用 UDP 协议，并在此之上进行协议层与算法层的优化，来提高传输的可靠性与逻辑性。</p><p></p><p>（3）UDP 协议的优化</p><p></p><p>UDP 协议往往和 RTP/RTCP 协议一起在实际应用中出现。RTP 负责数据传输，其协议头中的序列号、 端口类型、时间戳等字段，可为数据包的分组、组装、排序提供逻辑依据；RTCP 作为 RTP 的控制协议，负责对 RTP 的传输质量进行统计反馈，并为弱网对抗策略提供控制参数。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/92/19/927761459a0c7a0872c79c3574a29c19.png\" /></p><p></p><h3>（二）超低延时直播技术的演进历程</h3><p></p><p></p><p>（1）基于业务场景发展的直播技术演进过程（延迟主线）</p><p></p><p>（2）RTM 协议本身的演进历程</p><p></p><p>miniSDP 信令标准实现部分（抖音)CDN 信令异步回源RTP 携带扩展头组成部分</p><p></p><p><code lang=\"null\">a=extmap:18 \"http://www.webrtc.org/experiments/rtp-hdrext/decoding-timestamp\"\na=extmap:19 \"uri:webrtc:rtc:rtp-hdrext:video:CompositionTime\"\na=extmap:21 \"uri:webrtc:rtc:rtp-hdrext:video:frame-seq-range\"\na=extmap:22 \"uri:webrtc:rtc:rtp-hdrext:video:frame-type\"\na=extmap:23 \"uri:webrtc:rtc:rtp-hdrext:video:reference-frame-timestamp\"\na=extmap:27 \"uri:webrtc:rtc:rtp-hdrext:audio:aac-config\"</code></p><p></p><p>a=extmap:18 \"http://www.webrtc.org/experiments/rtp-hdrext/decoding-timestamp\"</p><p></p><p>a=extmap:19 \"uri:webrtc:rtc:rtp-hdrext:video:CompositionTime\"</p><p></p><p>RTP 使用 RTP 私有扩展头携带 DTS/CTS 值，每一帧 RTP 数据包通过 RFC5285-Header-Extension 扩展头携带该帧的 DTS 值，每一帧首个 RTP 包和 VPS/SPS/PPS 包通过 RFC5285-Header-Extension 扩展头携带该帧的 CTS 值，通过 PTS = DTS + CTS 计算当前帧的时间戳。用于启播快速音画同步和播放器播控逻辑精准音画同步。</p><p></p><p>a=extmap:21 uri:webrtc:rtc:rtp-hdrext:video:frame-seq-range</p><p></p><p>扩展头携带帧的起始/结束序号：如果首帧的前几个包丢失，那么可根据起始序号快速发起重传加快首帧；如果当前帧的后几个包丢失，那么可根据该帧的结束序号快速发起重传，降低延时，减少卡顿。</p><p></p><p>a=extmap:22 uri:webrtc:rtc:rtp-hdrext:video:frame-type</p><p></p><p>扩展头携带帧的类型：如果携带并解析了正确的帧类型，客户端可以不用解析 metadata ；同时在弱网情形，客户端可以跳过 B 帧直接解码 P 帧，加速出帧并减少潜在卡顿。</p><p></p><p>a=extmap:23 uri:webrtc:rtc:rtp-hdrext:video:reference-frame-timestamp</p><p></p><p>扩展头携带 P 帧的参考帧信息：如果发生弱网情形，那么客户端可以依照扩展头指定的参考帧关系及其对应时间戳，跳过 B 帧解码，减少卡顿发生。</p><p></p><p>a=extmap:27 uri:webrtc:rtc:rtp-hdrext:audio:aac-config</p><p></p><p>为了加速信令交互的速度，CDN 可以在某些条件下不去查询媒体信息，直接向客户端返回支持的音视频能力；此时 SDP 的媒体描述中将不包含有具体的音视频配置详细信息。在音频层面，此时AnswerSDP 中不包含 aac 解码所需的头信息；此时我们需要采取 RTP 扩展头模式携带 AAC-Config 供客户端在 RTP 收包时刻自行解析处理完成解码动作，作用是减少信令交互时间，提升拉流成功率。</p><p></p><p></p><h4>1、WebRTC 协议在直播播放器的移植</h4><p></p><p></p><p>RTM 低延时直播基于 WebRTC 技术衍生，基于 WebRTC 标准构建点到点传输一般有如下几个步骤：</p><p></p><p>（1）通信双方要进行媒体协商，会话详细规范即 SDP(Session Description Protocol) 交互；</p><p></p><p>（2）随后进行交互式网络地址协商（查询对端真实 IP 地址）准备构建媒体传输通道；</p><p></p><p>（3）当上述条件准备完毕即进入最终的 Peer to Peer 点对点媒体数据传输。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0f/b5/0fe680f53cc7ac020yy5e3ae56c3afb5.jpg\" /></p><p></p><p>信令部分客户端-服务器单独开发，利用了 SDP 标准报文模式；媒体传输部分采用开源的 WebRTC 框架和自截自研的实时音视频媒体引擎进行媒体传输。</p><p></p><p></p><h4>2、RTC 信令协议的改造升级（ MiniSDP 压缩协议）</h4><p></p><p></p><p>https://github.com/zhzane/mini_sdp</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/9b/3c/9bf0535ff0edc3c0b0184c1b9c78163c.png\" /></p><p></p><p>标准 SDP 比较冗长（ 5-10KB 左右），不利于快速高效传输。在直播场景下，会尤其影响首帧时间。MiniSDP 对标准 SDP 文本协议进行高效能压缩，将原生 SDP 转换成更小的二进制格式，使其能够通过一个 UDP 包来传输。降低信令交互时间，提高网络传输效能，降低直播拉流首帧渲染时间，提高拉流秒开率/成功率等 QoS 统计指标。</p><p></p><p></p><p></p><p></p><h4>3、CDN 对 RTM 信令的异步回源优化</h4><p></p><p></p><p>降低 RTM 信令交互时间，降低 RTM 拉流首帧渲染时间。原来的流程在服务端缓存不命中时需要等待回源拿到数据，才能返回带有 AacConfig 信息的 AnswerSDP。客户端收到 AnswerSDP 后发送 STUN，而服务端只能在收到 STUN 才能开始下发数据。（如下图左）；当异步回源情况下：服务端不再等待回源结果直接返回 AnswerSDP，之后回源和WebRTC 建连流程同步进行。等到 WebRTC 建连成功且回源拿到数据立即下发 RTP 数据。（如下图右）</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2d/bf/2df3615d0539c19caf00a0179815b5bf.jpg\" /></p><p></p><p></p><h4>4、视频渲染卡顿的优化（百秒卡顿平均降低 4 秒）</h4><p></p><p></p><p>改善人均看播时长，改变 RTC 引擎的组帧/解码策略；禁止 RTC 在低延时模式下的丢帧，改善直播的视频渲染卡顿。</p><p></p><p></p><p></p><p>传统的 RTC 场景优先保时延，全链路会触发各种丢帧（包括但不限于解码模块，网络模块），FLV 直播场景会优先保证观播体验（不丢帧，良好的音画同步效果）。RTM 要想减少卡顿，取得 qoe 的收益，播控策略需进行定制化, 定制逻辑修改点：</p><p></p><p>a. 确保不会由于软解的解码耗时或者硬解的 dequeuinputbuffer 等其它 api 操作阻塞 jitterbuffer ，内核层有一层强制的音画同步逻辑，可以确保音视频的播放体验；</p><p></p><p>b. 同时上层在监控网络模块和解码模块的缓存长度，有相应的兜底逻辑：</p><p></p><p>（1）判断硬解确实解不过来，dec_cache_frames 过多，上报错误，会降级到软解；</p><p></p><p>（2）jitterbuffer 异常，缓存的 frame_list 过多，触发播放器异常逻辑，上报错误，重新拉流。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b8/f6/b84082ec14653e63e1de968d3c22eef6.png\" /></p><p></p><p></p><h4>5、RTM 播控逻辑的优化</h4><p></p><p></p><p>改善移动端看播渗透，RTC 统一内核方案天生存在缺陷（ MediaCodec 硬件解码器初始化耗时久）；将 RTM 视频解码模块从 RTC 内核中迁移至 TTMP 播放内核，复用了 FLV 的视频解码模块（ MediaCodec 避免重新初始化）；显著的降低了安卓平台的首帧渲染时间，提升了拉流的成功率。RTC 内核通用逻辑</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1d/44/1d0d6f3a7bd5fbb540bdf3f29ba9ac44.jpg\" /></p><p></p><p>改进的 RTM 内核播控逻辑</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/04/16/049204378dfe7d0df20a155ae35fc616.jpg\" /></p><p></p><p>以上为超低延时直播技术演进之路《进化篇》的所有内容，第二篇《实战篇》我们将聚焦于<a href=\"https://www.volcengine.com/product/live\">超低延时直播技术</a>\"如何大规模落地实践，请大家持续关注~</p>",
    "publish_time": "2023-07-06 16:02:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱——数据库领域",
    "url": "https://www.infoq.cn/article/N7HK09pMRJrrOUDCSUus",
    "summary": "<p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。</p><p></p><p>所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。</p><p></p><p>此篇是《中国开源生态图谱系列研究》的第二篇，聚焦在三大基础软件中的数据库，通过将目前的开源数据库项目进行分类，并结合开源基金会、开源产业联盟等生态，完整构成中国开源数据库的生态图谱。</p><p></p><p>随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，和拓展的 Gitee 指数，分析和评价现有开源数据库项目，并从中选择优秀案例供广大开发者和开源社区研究。</p><p></p><h1>目录</h1><p></p><p>生态图谱解读生态图谱企业洞察</p><p></p><p>扫码下载</p><p><img src=\"https://static001.geekbang.org/infoq/e9/e959341544efe203f86ddd98afc1d625.png\" /></p><p></p>",
    "publish_time": "2023-07-06 16:17:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱——操作系统领域",
    "url": "https://www.infoq.cn/article/r9Lptg2pYrKvX4VMVYgT",
    "summary": "<p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。</p><p></p><p>所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。</p><p></p><p>此篇是《中国开源生态图谱系列研究》的第一篇，聚焦在三大基础软件中的操作系统，通过将目前的开源操作系统项目进行分类，并结合开源基金会、开源产业联盟等生态，完整构成中国开源操作系统的生态图谱。</p><p></p><p>随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，和拓展的 Gitee 指数，分析和评价现有开源操作系统项目，并从中选择优秀案例供广大开发者和开源社区研究。</p><p></p><h1>目录</h1><p></p><p>生态图谱解读生态图谱企业洞察</p><p></p><p>扫码领取</p><p><img src=\"https://static001.geekbang.org/infoq/23/23b3bdc2616f26b63e7b04f7688ceb18.png\" /></p><p></p>",
    "publish_time": "2023-07-06 16:22:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁集团：世界人工智能大会上“最接地气”参展商，中西部县域数字就业中心组团亮相",
    "url": "https://www.infoq.cn/article/4gwZYub5sl92yd8FYFMv",
    "summary": "<p>7 月 6 日举办的 2023 世界人工智能大会迎来“最接地气”参展商，由<a href=\"https://www.infoq.cn/article/g2dxkXzWCtLEpuizIxqR\">蚂蚁集团</a>\"“数字木兰｜AI </p><p>豆计划”支持的 17 个县域数字就业中心组团参加，与来自大模型、芯片、机器人、智能驾驶</p><p>等领域的 400 余家参展企业同台亮相。</p><p></p><p>为期 3 天的展期内，这些来自中西部欠发达县域的数据标注企业将通过现场展示、分享等，</p><p>与来自世界各地的参会者交流，如何做好人工智能的“训练师”，帮助 <a href=\"https://www.infoq.cn/article/Mq97Ju9jJ5M1rxc15dCG\">AI</a>\" 认识世界。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9ee5bceb3fd194caa2584262c6b87700.png\" /></p><p>“数字木兰｜AI 豆计划”支持的县域数字就业中心代表在 WAIC 现场分享</p><p></p><p>为发挥数字技术和平台优势，助力乡村留守女性、返乡青年等在地就业，2019 年，蚂蚁集</p><p>团、蚂蚁公益基金会联合中国妇女发展基金会等共同发起“数字木兰｜AI 豆计划”，通过订单</p><p>引入、技能培训、社会企业孵化等，助力中西部欠发达县域女性就业和产业发展。</p><p></p><p>截至 2022 年底，该计划已支持在陕西、山西、甘肃、贵州、宁夏等地建立了 17 个县域数</p><p>字就业中心，累计帮助 4000 人在地就业，成为“人工智能训练师”，其中超过 6 成员工为女</p><p>性。相关数字就业中心所开展的标注业务，已经涵盖 200 余个具体应用场景，年数据标注</p><p>量过亿，多个中心成为当地最大用工企业。</p><p></p><p>AI 越聪明，背后就越少不了“人工智能训练师”一步步的教它们，把图片、语音、文本、视频</p><p>等原始数据标注成<a href=\"https://www.infoq.cn/article/iBHmuLWBH14YgWjhD1Xq\">人工智能</a>\"可以理解的结构化语言。“没想到在小县城的我们能成为人工智</p><p>能产业链上的一环。”清涧县爱豆科技有限公司总经理鱼涛、积石山县“ AI 豆计划”数字经济</p><p>产业园负责人鲁玉超等都表示，科技助力打破地域壁垒，小县城数字标注企业也能走上大舞</p><p>台。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2533568b6ad1428a3639f0cb91acd51.png\" /></p><p>甘肃省积石山县人工智能训练师张娟带参观者体验数据标注工作</p><p></p><p>“任何一轮重大的产业进步，都会带来职业结构的重新构建，以 AI 为代表的数字时代催生了</p><p>大量数据标注等新职业机会，给县域就业、乃至产业发展带来了重大机遇。” 7 月 6 号同期</p><p>举行的 AI 女性菁英论坛上，蚂蚁公益基金会秘书长王晓晶现场分享道。在她看来，在一些</p><p>以男性为主导的传统职业领域，技术正在打破性别壁垒，女性的耐心和细心等特质，在数字</p><p>就业领域展现出了很多新的竞争力，“数字时代，可能是缩小性别鸿沟最好的时代。</p><p></p><p>据悉，为了更好的支持乡村女性及县域数字就业中心发展，蚂蚁集团联合生态伙伴搭建了</p><p> iTAG 智能数据标注平台，为县域引入更多就业机会，同时借助端模型、预标注机器人等智</p><p>能化工具，对 AI 训练任务进一步简化，降低人工标注难度，提高效率，帮助县域数字就业</p><p>中心能够在市场上接到更多场景更复杂的订单。目前，这些县域数字就业中心开展的标注业</p><p>务，已经涵盖了文本、图像、音频、视频等各类标注场景，应用领域覆盖了智慧城市、智能</p><p>制造、自动驾驶、智能服务、智能医疗、智能农业、智能物流、智能金融等社会生产生活的</p><p>多个角落。</p>",
    "publish_time": "2023-07-06 16:34:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]