[
  {
    "title": "WireMock Spring Boot将简化Spring Boot应用的WireMock配置",
    "url": "https://www.infoq.cn/article/JtADhFJ2VYuoDT4BqkeF",
    "summary": "<p><a href=\"https://wiremock.org/\">WireMock</a>\"是一个构建API mock的灵活工具，新的<a href=\"https://github.com/maciejwalkowiak/wiremock-spring-boot\">WireMock Spring Boot</a>\"简化了Spring Boot应用中基于JUnit的集成测试的WireMock配置。</p><p></p><p><a href=\"https://www.linkedin.com/in/maciejwalkowiak/\">Maciej Walkowiak</a>\"是一位自由职业的架构师和开发者，他在2023年2月<a href=\"https://twitter.com/maciejwalkowiak/status/1630248586557673472\">发布了</a>\"第一个版本WireMock Spring Boot。该项目会自动配置Spring环境属性，并提供一个完全声明式的WireMock设置。另外，还可以使用多个<a href=\"https://javadoc.io/doc/com.github.tomakehurst/wiremock/latest/com/github/tomakehurst/wiremock/WireMockServer.html\">WireMockServer</a>\"实例，每个HTTP客户端对应一个实例。最近，这个新的工具不会向Spring应用上下文中发布额外的bean，但是会将它们保存在于应用上下文关联的独立存储中。</p><p></p><p>添加如下Maven依赖之后就可以使用WireMock Spring Boot：</p><p></p><p><code lang=\"java\">\n    com.github.maciejwalkowiak.wiremock-spring-boot\n    wiremock-spring-boot\n    0.1.0\n    test\n\n</code></p><p></p><p>目前，Maven Central仓库中还没有提供这个依赖，但是可以通过Git的<a href=\"https://jitpack.io/\">JitPack</a>\"包仓库来使用它。在第一次请求时，JitPack会从Git仓库下载代码，并构建代码以提供构建制品（artifact），比如JAR文件。如果想了解更多信息的话，请参阅JitPack的<a href=\"https://docs.jitpack.io/\">文档</a>\"。</p><p></p><p>在制品发布到Maven Central仓库之前，我们应该在_pom.xml_中添加如下的JitPack仓库：</p><p></p><p><code lang=\"java\">\n    \n        jitpack.io\n        https://jitpack.io\n    \n\n</code></p><p></p><p>使用<a href=\"https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/test/context/SpringBootTest.html\">@SpringBootTest</a>\"注解的测试，以及其他使用<a href=\"https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/test/context/junit/jupiter/SpringExtension.html\">SpringExtension</a>\"类的注解测试，可以使用 <a href=\"https://github.com/maciejwalkowiak/wiremock-spring-boot/blob/main/wiremock-spring-boot/src/main/java/com/maciejwalkowiak/wiremock/spring/EnableWireMock.java\">@EnableWireMock</a>\"注解进行标注，该注解会启用<a href=\"https://github.com/maciejwalkowiak/wiremock-spring-boot/blob/main/wiremock-spring-boot/src/main/java/com/maciejwalkowiak/wiremock/spring/WireMockSpringExtension.java\">WireMockSpringExtension</a>\"并添加测试上下文定制器。我们可以使用<a href=\"https://github.com/maciejwalkowiak/wiremock-spring-boot/blob/main/wiremock-spring-boot/src/main/java/com/maciejwalkowiak/wiremock/spring/ConfigureWireMock.java\">@ConfigureWireMock</a>\"来配置mock，它会创建一个<a href=\"https://javadoc.io/doc/com.github.tomakehurst/wiremock/latest/com/github/tomakehurst/wiremock/WireMockServer.html\">WireMockServer</a>\"，并使用**property**指定的名称来作为环境属性的名称，这样的环境属性可以用来检索WireMockServer：</p><p></p><p><code lang=\"java\">@SpringBootTest\n@EnableWireMock({\n        @ConfigureWireMock(name = \"studentservice\", property = \"studentservice.url\")\n})\nclass StudentControllerTest {\n    @Autowired\n    private Environment environment;\n\n    @WireMock(\"studentservice\")\n    private WireMockServer wireMockServer;\n\n    @Test\n    void studentTest() {\n        environment.getProperty(\"studentservice.url\");\n        wireMockServer.stubFor(get(urlEqualTo(\"/student\"))\n            …\n\n    }\n}\n</code></p><p></p><p>在上述的样例中，我们使用**environment.getProperty(\"studentservice.url\")方法来检索WireMockServer**实例的URL。</p><p></p><p>WireMock<a href=\"https://wiremock.org/docs/extending-wiremock/\">扩展</a>\"可以通过配置注解中的**extensions**参数进行配置：</p><p></p><p><code lang=\"java\">@ConfigureWireMock(extensions = { … }, …)\n</code></p><p></p><p>默认情况下，包含映射文件的classpath目录会被设置为_wiremock/{server-name}/mappings_，但是也可以通过配置注解中的**stubLocation**参数进行变更：</p><p></p><p><code lang=\"text\">@ConfigureWireMock(stubLocation = \"customLocation\", …)\n</code></p><p></p><p>相对于<a href=\"https://github.com/spring-cloud/spring-cloud-contract/tree/main/spring-cloud-contract-wiremock\">Spring Cloud Contract WireMock</a>\"，WireMock Spring Boot的优势在于自动设置的Spring属性和多个 **WireMockServer**实例的声明式配置。但是， <a href=\"https://github.com/spring-cloud/spring-cloud-contract/tree/main/spring-cloud-contract-wiremock\">Spring Cloud Contract WireMock</a>\"支持契约测试、REST文档以及其他的特性。</p><p></p><p>WireMock Spring Boot使用了Spring Cloud Contract WireMock和<a href=\"https://github.com/skuzzle/spring-boot-wiremock\">Spring Boot WireMock</a>\"项目以及<a href=\"https://rieckpil.de/spring-boot-integration-tests-with-wiremock-and-junit-5/\">“使用WireMock和JUnit 5进行Spring Boot集成测试”</a>\"这篇文章的概念和想法。关于该项目的更多信息，可以在<a href=\"https://github.com/maciejwalkowiak/wiremock-spring-boot/\">GitHub</a>\"上找到。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/wiremock-spring-boot/\">WireMock Spring Boot Simplifies the WireMock Configuration for Spring Boot Applications</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/KUFfYLk4zWcQ1VZXflpp\">Just：Spring Boot 应用的新命令行界面</a>\"</p><p><a href=\"https://www.infoq.cn/article/aVZBQSZYiCA5PhzA3EDy\">Spring Authorization Server 1.0 提供了 OAuth 2.1 和 OpenID Connect 1.0 实现</a>\"</p>",
    "publish_time": "2023-03-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "落地4年，工商银行如何进行Serverless 架构迭代",
    "url": "https://www.infoq.cn/article/uhgZm4TRELfXgizNFtGM",
    "summary": "<p></p><p>工商银行早在 2018 年便启动了 Serverless 技术的研究，通过将业界主流 Serverless 技术栈与行内“云计算 + 分布式”体系融合，建设了具备极致弹性伸缩能力的全托管 Serverless 平台，并在 AI 模型、批量任务、接口聚合等多个场景落地，有效提升了云上资源利用率和业务迭代效率。我们在这篇文章里分享了工商银行 Serverless 实践至今的经过、效果和经验，希望对大家有所帮助。</p><p></p><h3>Serverless 的发展历程和业界现状</h3><p></p><p></p><p>Serverless 作为虚拟机技术、容器技术之后的下一代云计算形态，被认为是云计算未来的发展方向。纵观云计算的发展历程，经历了从物理机到虚拟机，从 IaaS、PaaS 到 Serverless（如图 1 所示），每个阶段的跃迁都是一个去服务器的过程，同时带来了技术架构和应用架构的不断演进。在 Serverless 阶段，应用被拆分成多个独立功能点，每个功能点对应一个函数，每个函数实现各自的业务逻辑。同时 Serverless 将后端能力封装成服务，并以接口的形式提供给上层业务调用，确保用户最大程度聚焦业务逻辑开发，而无需关注高可用、服务器运维等其他要素，极大提升了业务的上线速度。</p><p><img src=\"https://static001.geekbang.org/wechat/images/10/105a61785ab5164108e92f46a3389425.png\" /></p><p></p><p>图 1 Serverless 发展历程</p><p></p><p>Serverless 经过这些年的快速发展，在技术架构、平台能力、平台特性、应用场景和业界产品等方面也逐渐趋向成熟。</p><p></p><p>技术架构方面：Serverless 依托于底层 IaaS、PaaS 平台，以事件驱动为核心，通过 HTTP、定时、消息等事件触发的方式，调度执行上层业务逻辑。平台能力方面：Serverless 为开发、测试、发布、交付、灰度等应用生产环节提供了完备的支撑，实现了完整的 DevOps 能力，并通过完善的日志、监控体系，提升了运维工作的效率。平台特性方面：Serverless 的弹性扩缩容能力，满足了后端服务急速扩缩容需求，解决了业务流量波谷时期带来的资源浪费问题。同时，Serverless 函数计算具备的函数编排能力，满足了多媒体计算领域在对音视频进行采样、降噪、平衡音量、转码等处理时的计算编排要求。应用场景方面：Serverless 可应用后端业务处理、文件转换、音视频处理、AI 模型训练和运行等场景，并且通过按需使用，降低业务上云的成本，同步提升研发效能。例如，AI 计算是一种 CPU 密集型且有明显波峰和波谷的业务，同时 AI 开发过程中需要对参数频繁调整并验证调整结果，Serverless 的弹性伸缩和快速发布能力可以很好的应对此类具有峰谷特性的业务，同时满足了模型快速上线的需求。新浪微博、高德地图、闲鱼等互联网企业也通过 Serverless 的弹性伸缩能力，在音视频处理和后端服务上大幅降低了计算资源成本，同时得益于 Serverless 便捷的研发部署流程和强大的运维管理支撑，研发效能实现大幅提升。业界产品方面：当前亚马逊、阿里、腾讯等头部厂商提供了完善的 Serverless 平台服务。国内外主要的公有云 Serverless 产品有 AWS Lambda（亚马逊）、Azure Functions（微软）、函数计算（阿里云）等。金融行业考虑到安全性和多样的金融业务需求，一般使用私有化的 Serverless 方案，搭建适用于银行业务特性的 Serverless 平台。</p><p></p><h3>工商银行 Serverless 体系技术支撑</h3><p></p><p></p><p>工商银行作为国内银行业首家落地 Serverless 平台的企业，早在 2018 年便启动了 Serverless 1.0 的设计。技术架构上，以“开源框架 + 自研事件驱动框架”为核心，提供了函数模式和 Serverless 容器模式（如图 2 所示）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3f/3f6e527a53c9822aa9e8900bcea39130.png\" /></p><p></p><p>图 2 工商银行 Serverless1.0 技术架构</p><p></p><p>Serverless 1.0 平台实现了动态伸缩过程中应用实例数 0 到 N 的能力，在量化交易业务模型回测场景中，为了对历史市场数据、指标数据、交易明细等数据进行查询分析，通过将基金组合效果统计模型的逻辑封装成函数，运行在 Serverless 平台中，提升了分析模型的开发人效和计算资源的利用率。</p><p></p><p>随着一些对业务响应要求较高的应用开始接入，Serverless 1.0 平台逐渐出现一些问题，比如每次发布都需要制作新的镜像、运维流程繁琐等等，这对行内生态闭环程度、平台易用性上提出了更高的要求。</p><p></p><p>2020 年，工商银行在原有 Serverless 平台上增强了与存储、日志、监控等周边生态的融合，并进一步强化“函数即服务”的能力，设计和建设了 Serverless 2.0 平台。2.0 平台在技术选型上以工商银行 PaaS 平台为基础，并与工商银行云计算、分布式体系充分融合，为应用提供了完整的函数核心引擎、函数管理、开发交付等能力。</p><p></p><p>基于平台的使用场景和技术选型，Serverless 2.0 函数计算平台在架构上分为函数计算管理平台和函数计算系统服务两大部分（如图 3 所示）。</p><p></p><p>函数计算管理平台：主要向开发、运维人员提供完备的管理能力，包括函数管理、服务管理、事件管理、工作流管理、发布管理和日志监控等功能，覆盖函数的开发测试、运维监控全流程。函数计算系统服务：主要提供了函数开发运行所需的各个底层支撑能力，包括事件触发器、Runtime 执行环境、平台底层支撑三个层面。在事件触发器层面，平台已支持 HTTP、定时任务、Kafka、对象存储等事件源；在执行环境层面，平台已支持最主流的 Java、Python、Nodejs 三大运行时，同时也支持自定义运行时配置，以覆盖更多应用场景；在平台底层支撑层面，平台提供了函数执行、监控运维、资源管控、稳定性保障等能力，有效降低接入应用的运维成本。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/76/d7/760c187e4a6c2b24408a04a3fcdc02d7.png\" /></p><p></p><p>图 3 工商银行 Serverless2.0 平台架构</p><p></p><p></p><h3>工商银行 Serverless 应用实践及成效</h3><p></p><p></p><p>经过近几年的实践，工商银行 Serverless 平台目前已将应用场景总结为分布式批量、小程序应用、AI 模型、音视频处理、流式消息处理等近 10 类，并已在黄金账户、智能外呼、资产估值、持续交付系统等数十个应用全面开展落地工作，接入 Serverless 平台的应用总体资源利用率提升了 90% 以上，研发人力投入降至未接入前的 70%，应用降本增效成果显著。同时，借助平台提供的统一运维能力，应用极大地降低了运维工作量，从而进一步提升了我行云原生基础设施对上层业务的支撑水平。</p><p></p><h4>（一）Serverless 在分布式批量场景的实践及成效</h4><p></p><p></p><p>在传统的分布式批量架构中，批量作业整体调度能力由批量控制器、分布式协调中心（zookeeper/kafka）、批量作业执行器构成。其中批量控制器用于作业的调度和触发，作业触发消息通过分布式协调中心进行发布，批量执行器在监听到作业触发消息后，启动批量作业并同步更新批量作业状态（如图 4 所示）。传统分布式批量架构由于批量作业执行器需要实时监听分布式协调中心中的作业触发消息，因此在非批量作业执行期间，批量作业执行器也需处于运行状态，导致资源利用率较低。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a7/a72cea7518fef0b225ce697a8408b2e4.png\" /></p><p></p><p>图 4 传统分布式批量作业架构</p><p></p><p>为提升批量作业执行器的资源利用率，工商银行围绕高可用、灵活性、兼容性三方面，基于原有分布式批量平台，增加了 Serverless 批量任务管理能力，将分布式批量框架的调度能力和 Serverless 平台快速弹性伸缩能力相结合，通过整合批量框架和 Serverless 平台的技术优势，大幅提升了批量作业资源利用率和调度灵活性。</p><p></p><p>批量作业高可用方面：分布式协调中心和 Serverless 批量任务管理模块均采用多实例运行机制，当主节点发生故障时，从节点可以在第一时间接替主节点任务，防止单点故障引起的系统不可用。此外在作业运行期间，函数计算平台也能够根据函数运行的返回消息快速捕获异常作业，然后报告给 Serverless 批量任务管理模块，通知批量控制器对该作业进行重试。批量作业灵活性方面：基于 Serverless 平台弹性伸缩能力和事件驱动特性，在高并发场景下，可动态创建和销毁函数容器，实现资源的灵活分配，进而提升分布式批量作业调度的灵活性。批量作业开发兼容性保障方面：通过引入 Serverless 批量任务管理模块，将分布式批量框架和函数计算平台解耦，实现了平台兼容。同时，通过升级分布式批量开发框架 SDK 的方式，保证了基于传统分布式批量架构开发的存量应用可以平滑过渡到基于 Serverless 函数计算的批量架构，确保了 Serverless 平台和分布式批量框架的兼容性。</p><p></p><p>目前批量作业函数化已在贵金属积存金业务、小程序开放平台、资产管理估值核算、分布式事务等应用落地试点，峰值资源占用可减少 50%，总体资源利用率提升 90% 以上，达到了较好的试点效果。未来预计生产超过 90% 的分布式批量场景可适用该方案，将大幅提升工商银行分布式批量场景的资源利用率。</p><p></p><h4>（二）Serverless 在持续交付流水线的实践及成效</h4><p></p><p></p><p>持续交付系统在安装上云数据库时，一般选择在代理服务器上运行数据库安装程序，但代理服务器存在多应用、多节点共用时资源无法隔离、与数据库关系映射无序等问题。此外，代理服务器只在应用投产时运行，其余时间服务器资源处于闲置浪费状态。</p><p></p><p>通过 Serverless 平台的弹性扩缩容特性可做到按需加载，实现只在执行数据库安装程序时动态拉起对应的函数计算实例，任务运行结束后即可销毁的能力，从而避免了空耗计算资源的情况。同时 Serverless 平台支持自动弹性扩容，可为任务执行提供足够的计算资源，以应对投产时间窗口内大量的数据库安装部署任务。使用方只需利用函数计算事件触发机制，通过请求参数实现各数据库安装函数实例只作用于目标数据库，无需维护相关映射关系。</p><p></p><p>在运维方面，基于 Serverless 平台的持续交付作业无需进行日志和可用性监控的配置，可方便地使用 Serverless 平台提供的定时、kafka 等事件源触发机制进行作业调度，大幅提升了运行效率。目前，Serverless 平台已为持续交付系统的一千多个数据库实例提供了数据库安装服务，累计节省生产环境数据库服务器一千多台。</p><p></p><h4>（三）Serverless 在智能外呼场景的实践及成效</h4><p></p><p></p><p>智能交户应用服务提供了智能外呼、智能呼入、智能质检等多维度的智能交户能力。在外呼场景中，系统需要将收到的外发信息文件进行预处理，包含文件解析、数据校验、任务拆分、特殊场景处理等，并将处理后的数据进行标准化的存储，等待后续外呼处理。为了满足应用的外呼需求，外呼数据的预处理功能部署在同一个容器中，需要 7*24 小时在线，但每日实际运行时间只有数小时，甚至只有几十分钟，存在大量的服务器资源浪费问题。</p><p></p><p>为了应对上述痛点，通过函数计算平台开发部署，应用只需编写外呼数据的预处理功能，由 Serverless 平台承接预处理功能的发布、触发运行、监控告警等能力，在空闲期不再占用服务器资源。</p><p></p><p>相较于原开发运维方式，函数计算开发功能人力投入整体合计节约 25% 左右，生产环境服务实际占用的资源节约了 90% 以上。</p><p></p><h4>（四）Serverless 在文本核对场景的实践及成效</h4><p></p><p></p><p>工商银行对账中心在对文本数据进行核对时，底层数据库进行分库之后需要多个节点支撑，由于文本的核对任务不需要长期执行，因此导致了大量空闲时间段的资源冗余问题。</p><p></p><p>对账中心通过抽取文本核对过程中共性的数据获取、数据加工、数据持久化和数据回传等关键步骤为函数，利用函数工作流机制对函数进行编排。核对任务通过工作流进行编排，由函数平台触发，其运行所需的容器资源管理和运维监控保障均通过函数计算平台实现，达到了系统高性能、高扩展和资源高利用率的效果，也降低了运维复杂度。此外 Serverless 平台提供的灰度发布、日志管理和监控能力，也丰富了对账中心的运维能力（如图 5 所示）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a7/a77a7892101c4990b6e3ac018e7fb175.png\" /></p><p></p><p>图 5 对账中心基于 Serverless 的文本核对系统架构</p><p></p><h3>Serverless 在金融行业落地建议</h3><p></p><p></p><p>目前银行线上业务正处于高速发展时期，同时又面临着服务器资源和研发效能两方面的压力，针对创新类业务还面临着较高的试错成本。工商银行认为银行业在引入 Serverless 技术需要重点关注以下内容。</p><p></p><p>一、深入研究业务特性，全面评估应用收益。通过分析应用的负载情况，对具有负载峰谷现象的应用或变更频繁、需快速试错的业务，进行函数化改造的可行性分析，适当规划函数服务粒度，并重点关注服务改造成本、投产风险和预期收益。</p><p></p><p>二、加强运维能力建设，着力保障研发体验。完善 Serverless 平台稳定性，通过与开源或现有的日志、监控、告警组件结合，融入现有运维生态体系，构建完善的研发体系，辅助开发人员高效完成函数的开发和测试，并结合完善的 DevOps 能力，提供 Serverless 项目一体化研发、发布流程。</p><p></p><p>三、充分利用平台优势，逐步丰富落地场景。建议金融同业充分利用 Serverless 的特性，结合应用的负载规律、业务的响应效率、运维的人力成本等多方面因素，充分挖掘潜在场景，使用 Serverless 技术实现降本增效。</p><p></p><p>当前工商银行已经建设了功能完备的企业级 Serverless 平台，未来也将持续打磨 Serverless 产品的核心能力，打造适配金融业务特性的 Serverless 平台，不断丰富落地场景，扩大函数计算的使用范围，将 Serverless 技术大规模运用到工商银行金融业务中，为金融同业的架构转型提供最佳实践和参考样板。</p><p></p><p>延伸阅读：</p><p></p><p><a href=\"https://www.infoq.cn/article/MA6MqQUHalKLxYSAN9do\">从公有云方案转向谷歌开源 Knative，网易云音乐的 Severless 演进实践</a>\"</p><p><a href=\"https://www.infoq.cn/article/vHCG1pJpsLapBBMvbvZM\">备受云厂商们推崇的 Serverless，现在究竟发展到什么水平了？</a>\"</p><p></p>",
    "publish_time": "2023-03-27 10:50:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据零丢失，存储成本降低2/3，OceanBase已经从金融走向通用",
    "url": "https://www.infoq.cn/article/i2NJuxytCgAuHEXh2KLf",
    "summary": "<p>3月25日，OceanBase开发者大会在北京举办，与开发者共同探讨了单机分布式、云原生、HTAP 等数据库前沿趋势，并展示了多个企业内部最佳技术实践。会上，OceanBase CTO杨传辉发布了OceanBase 4.1版本，公布了两大友好工具，升级文档易用性，统一企业版和社区版代码分支，全面呈现了OceanBase打造极致的开发者友好数据库的成果。</p><p></p><p>2010 年，OceanBase 第一个版本诞生。在过去的十三年里，OceanBase的产品技术，从支付宝走向众多企业，跟随着开源和云的成长，逐渐成为开发者喜欢的数据库。</p><p></p><p>过去13年，OceanBase以极强的稳定性、可扩展性和低成本，成为分布式数据库领域的典型实践，并持续加大科研投入，突破技术边界，让分布式技术不断升级的同时越来越好用，研发单机分布式一体化架构，让分布式数据库走向通用；攻坚HTAP能力，让一份数据既能做交易又能做分析，实现低延时、低成本。</p><p></p><p>大会现场，OceanBase首次公布了四项“开发者友好”实践。</p><p></p><p>一是，发布了OceanBase 4.1版本。4.1版本的内核能力及小规格综合读写性能得到提升。相比4.0版本， 事务处理（TP）性能提升40%，数据分析（AP）性能提升15%，旁路导入平均将加载数据的性能提升6倍，新增租户级别物理备库以及更细粒度的资源隔离能力，全面兼容MySQL8.0版本，更通用、易用。</p><p></p><p>二是，升级工具，推出向导式的安装部署。让部署从小时级降到分钟级，可实现2分钟部署demo环境，10分钟完成标准部署集群，降低了使用分布式数据库第一道门槛。同时，把开发工具（OCP Express）集成到社区版，一键下载全部署，降低运维成本。</p><p></p><p>三是，推出场景化文档，解决文档“不好找、不好用”的问题。数据库文档就像说明书，是开发者使用数据库的敲门砖。分布式数据库对于开发者来说，天然存在相对复杂的架构和理解成本。长期以来，国产数据库技术文档劝退了一批数据库爱好者。OceanBase结合外界反馈的问题和建议，从贴近用户视角和场景做设计，重构了7000多页文档。</p><p></p><p>四是，宣布将整体研发流程迁移至外部，统一企业版和社区版代码分支，让每一位开发者看到代码进展。同时，宣布即将开源三大开发者工具（ODC、OCP Express和MySQL binlog service）。一如既往坚持开放的技术态度，OceanBase 真正迈出了走向生态化开放的重要一步。</p><p></p><p>OceanBase CTO杨传辉在大会现场说，“OceanBase将持续降低开发者使用门槛，全面提升OceanBase的易用性，打造真正对开发者友好的数据库，建设开放的技术生态，让国产数据库走向田间地头。”杨传辉也表示，从上云到多云原生，开放的存储计算分离是多云原生的必然路径，在云上OceanBase相比MySQL可直接降低18%-42%的整体成本。</p><p></p><p>此外，大会现场，OceanBase还发布了全新Logo。三条超级符号交相呼应，取自 OceanBase 经典“三副本”架构，寓意“流动的数据”。OceanBase 希望通过创新的技术帮助数据能够可信、高效、低成本地流动，助力更多企业探索新的商业可能，创造数字时代的新未来。</p><p></p><p>本场专为开发者打造的数据库技术交流会吸引了共14万人参加，其中现场超500位开发者。来自携程、客如云、国泰财险、翼鸥教育等近二十位数据库专家现场分享了使用OceanBase的真实体验。</p><p></p><p>“高并发不慌了”“成本整体节省了2/3”、“存储空间节省80%”、“HTAP低成本低延时”、“平稳运行近4年，数据零丢失”……</p><p></p><p>这是在3月25日首届OceanBase开发者大会上来自开发者的声音。</p><p></p><p>携程高级数据库工程师台枫介绍，“迁移至OceanBase后，相较于MySQL数据库成本节省了2/3。”作为一线互联网企业，二十多年来携程不断拓展业务边界，对数据库的多租户数据隔离及资源利用率要求越来越高，维护成本也随之增加。“OceanBase拥有的强一致性、高可用性，以及多租户数据隔离和弹性扩缩容的能力，都深深地吸引了我们。从2021年开始，携程尝试使用OceanBase并逐步替换部分线上数据库。”</p><p></p><p>客如云，作为一家SaaS服务商，在全国服务商家累计超过137万，对数据库高压缩比的需求十分急迫。2022年，客如云开始把数据迁移到OceanBase，完成系统的分布式云升级，数据压缩比与查询性能得到极大提升。“迁移后，我们的数据存储空间节省80%，成本降低40%，索引的创建更快速、使用更灵活，索引调整时间缩短95%。”高级运维专家陈威在现场表示。</p><p></p><p>“以前双11大家还比较紧张。”国泰财险资深数据库专家舒明表示，“现在用OceanBase后，运维成本降低了，效率提升了，双11日保单超1亿都不慌，集群巅峰时刻达到了8.5w次TPS（每秒处理事务量）都平稳度过。”国泰财险核心系统迁移至OceanBase平稳运行近4年，不仅做到数据传输、存储、使用的全生命周期管理与架构治理、降本增效、SQL治理的精细化运营，且实现了数据零丢失。</p><p></p><p>翼鸥教育（ Classln）是一个教学一体化平台，全球有150多个国家的6万多所学校在其平台上开展教育工作。随着业务翻倍增长、流量猛增，其数据库读写和容量都存在瓶颈。“数据清洗时间关乎上线耗时，是业务迁移中比较重要的节点。我们的测试结果显示，同一套清洗程序，其他数据库的数据清洗用时是OceanBase 的2.74倍。因此，我们的研发人员对OceanBase的性能充满了信心和期待。”</p><p></p><p>“OceanBase可以在高性能 OLTP 数据库的基础上扩展 OLAP 的能力，能很好支持实时分析，一份数据既可以进行事务处理又可以进行数据分析。OceanBase在作业帮的实践显示，可以很好的降低整个IT的成本和数据的低延时。“作业帮DBA刘强现场说道。</p><p></p><p>据了解，截止目前OceanBase已实现全行业应用场景覆盖，助力金融、政务、运营商、零售、互联网等多个行业的400多家客户实现了关键系统升级。OceanBase资深研发总监易鸿伟表示：“OceanBase已经从金融走向通用，打破了过去分布式数据库只能支持大企业、大客户的思维定式，可以更好地服务中小企业。”</p>",
    "publish_time": "2023-03-27 11:35:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "可悲的现实，大部分技术领导者可能并不称职",
    "url": "https://www.infoq.cn/article/LXqbfcLi32qxsJiJ0ecr",
    "summary": "<p></p><p></p><blockquote>本文由 InfoQ 整理自阿里巴巴资深工程师 许晓斌 在 QCon全球软件开发大会（北京站）2022 上的演讲《技术领导力实战》。</blockquote><p></p><p></p><p>大家好，我是许晓斌，目前就职于阿里巴巴技术风险与效能部，负责运维与构建基础设施平台。在软件行业从业 15 年，包括微服务架构、DevOps、云原生等领域，软件管理工作 5 年。出过一本书《Maven 实战》，做 Java 的同学应该有不少人读过。</p><p></p><p>目前我在阿里巴巴带了多年团队，在实际工作中也有一些管理上的经验，但在准备<a href=\"https://qcon.infoq.cn/2023/beijing/track/1299\"> QCon 全球软件开发大会</a>\"北京站的这个演讲主题时，在官网信息及材料中刻意隐去了自己的 title，我认为如果大家只是被 title 吸引而来，那其实并没有什么意义。个人认为，这个话题在国内也很少能有非常专业的分享。因为管理和领导是一种“专业”，它跟技术专业不一样，但它也是一种专业。众所周知，当下国内整体上是一个“业务为王”的时代，只要业务增长，管理、领导，甚至技术，做得好与不好都不重要。很多时候业务好和人的技术无关，和管理也无关，只是因为你恰好在风口，所以大家得意识到这一点。假设业务不增长了，该如何去量化一个管理者？如何去评估一个管理者做的好与不好？这是一件挺有意思的事情。</p><p></p><p>那这次演讲为什么讲管理？因为我见过太多糟糕的管理者了，包括我自己。</p><p></p><h3>技术领导反模式</h3><p></p><p></p><p>回顾一下我刚刚开始带团队的时候，可以用“稀烂”来形容。技术领导力其实非常重要，因为好的管理，它决定了公司的战略是否能够得到执行和落地；其次，它也决定了大量工程师的成长和发展。举个例子，如果你犯个技术错误，无非是一个故障；但是如果你犯管理错误，你对一个人的一年、两年甚至更长时间，会产生一个巨大的影响。所以这是一个非常关键的事情，需要重视。</p><p></p><p>但可悲的现实是，大部分技术领导者是不称职的。以下列举的几种错误模式在技术领域随处可见，基本都可以对号入座：</p><p></p><p>闷头干模式</p><p></p><p>延续独立贡献者的工作方法，所有方案自己做，所有代码自己写。</p><p></p><p>路由器模式</p><p></p><p>上级任务往下转发，任务结果收集汇报。</p><p></p><p>高压模式</p><p></p><p>对上过度汇报，对下持续增加，辅以不科学的价值宣导。</p><p></p><p>不决策模式</p><p></p><p>不对任何需求 say no，或者决策全部下放，并让下属承担决策后果。</p><p></p><p>有业务无工程模式</p><p></p><p>高度关注短期业务交付，不管工程质量。</p><p></p><p>作为管理者，你可以通过参考以上反模式来反思一下自己是否称职。如果你是一个“大头兵”，你可能感觉到自己老板不靠谱，时常被 PUA 到怀疑自己能力问题，但这些问题的根源并不是因为大家不想做好这件事情或者缺乏专业的理论，而是因为行业发展得太快了。以蓬勃发展的互联网为例，像阿里、腾讯等公司在短短十几年时间内从 100 人增长为 10W 人规模，这个时期产生了大量的技术管理者。然而，由于社会面缺乏既懂业务又懂技术、同时又懂管理的人才，只能通过内部提拔。但这些被提拔上去的人是因为管理做得好吗？大部分不是。他们成长晋升得很快，大部分是因为业务好或者技术好，但管理能力却不一定好。因此，这会导致这些人产生了一个错误的认知偏差，认为自己的成长和晋升速度很快，但其实本身的工程能力不足，实际的管理能力与其“总监”、“总裁”的 title 并不匹配。</p><p></p><p>事实上，很多时候业务的成功和他们的管理能力没有必然关系，换一个人也是同样的结果。业务的飞速增长是因为业务本身处于商业风口，或是因为商业战略、业务战略、运营战略的判断。</p><p></p><p>那么，如何进行管理呢？我自己也阅读了一些管理方面的书籍，并有一些实际管理工作中的经验和心得体会，可能不是最准确的，但应该还算靠谱。在本文中，我将删繁就简，探讨一些重要的事情。也希望能够通过本文得到一些反馈，帮助我更好地整理和思考。</p><p></p><p></p><h3>人才</h3><p></p><p></p><p>我自己带团队的时间大概有五年多，总结下来，如果说技术领导者只能做好一件事的话，就是做招聘，挖掘人才。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e9/e9c42d6637b57db0e91be9e43e23e1d2.png\" /></p><p></p><p>那为什么是招人最重要？这个观点也出自于我现任的一位 Leader，大约三四年前，我们一起在杭州的 EFC 地下食堂吃饭聊到了相关的话题。当时，他问我，你觉得做管理什么事情最重要？我当时没有想好，沉默了一分钟没有说话，然后他看我笑了笑说，你傻，就是招人。</p><p></p><p>如果你正在带领一个团队或正处于一个团队中，发现团队里有一些非常棘手的或是痛苦的问题得不到解决，通常都能够最终溯源到某个人身上。比如我反思自己在团队中做得最成功的事情，我能够溯源到我招了一位正确的人，或者是我培养了一位正确的人；反思我做得比较失败的事情，或者是让我头疼的事情，都是因为这个人不是我招聘的，或者是不得不塞到我团队里的。</p><p></p><p>往往常见的情况是，老板给了业务后团队立马招人扩大规模，你想了想明年可能又可以晋升了。但实际上，一定要给团队招一个正向的人，即与团队目标一致、文化一致，能力一致。如果团队里某个人的专业素养不能支撑住在团队生存的时候，他必然会进化出一种其他方面的能力帮助自己在团队里生存。比如他可能特别“会汇报”、特别会“写 PPT”、特别会“搞东搞西”的一些事情来帮助他自己生存，因为他的专业能力无法跟上团队，为了不被踢出团队，所以需要进化其他能力。所以我后来就养成了一个习惯，就是在招聘的时候会将候选人专业素质的要求提高，我宁愿招不到人，宁愿业务不做或是做得慢一些。</p><p></p><p>那重视人才意味着什么？</p><p></p><p>你每周花几个小时做招聘 / 面试 /1-on-1 沟通？你是否对每次面试都严格要求？会不会因为项目压力降低要求？你能欣赏和你不同的想法和观点吗？你有信心充分地授权，并敢于为此负责吗？</p><p></p><p>在招聘比较旺盛的时候，比如校招开始时，我每天平均会花几个小时的时间来做招聘和面试，和一些 1-on-1 沟通，并且不断地告诫自己，一旦招聘一个人，如果我很喜欢但是又有点犹豫，我就会判断可能哪里存在问题，就会放弃。在招聘过程中，首先我会非常的重视工程能力，比如会进行在线笔试，在线编码能力和历史代码等，一轮两轮三轮，不断地验证候选人的代码是否足够 Solid，讨论时是否足够 Opening，还会问一些明知对方不会的问题，去看对方是直接反馈不会，还是选择绕圈回答，比如有人绕了一圈后，最后结论是虽然我不会，但是我会怎样怎样。但其实我不需要对方去回答这些问题，我需要的是职业发展目标对齐，基本能力对齐，沟通简单，逻辑清楚，这几点要求我一直非常坚持。</p><p></p><p>到目前为止，我们团队在公司每年的 360 员工满意度调研中的分数都是不错的，这并不是因为员工进来之后我带领的多好，而是因为他们进入了合适自己的团队， 并且能够开心地工作，这是我认为比较关键的。</p><p></p><p>下一步，就是如何带团队。</p><p></p><p>带团队肯定要定战略、做规划。那战略目标如何落地？阿里是用的 OKR，我每个两周或一个月，都会去看我的 OKR 进展并进行更新。很多人的规划经常变来变去，可能过去一个月后做的事情已经和规划完全无关了，但我今年的规划相比去年，有 80% 其实是一样的，这是多重原因决定的，一方面是因为我认为我做的规划比较科学清晰，另一方面是整个组织结构比较稳定，比如没有老板天天换等等，这两点其实非常重要，实际上也应该如此。</p><p></p><p>我们的业务并没有多大的变化，我们是做工程的，平时的工作就是写代码的时候做发布、做构建、做运维，以及访问各种云产品等，这些平台和业务是非常稳定的，并不会发生剧烈的变化。但如果你的规划发生剧烈的变化，证明你或者你的老板甚至公司的 CTO 并没有想清楚，这些是不应该发生的事情。</p><p></p><p>那作为管理者，如何去制定 OKR？关于 OKR 有很多相关的书籍可以学习，此处我根据自己的体感做了一些摘录和总结，主要为以下几点：</p><p></p><p>OKR 应该体现团队为谁服务（for who），即围绕价值阐述。很多人将 OKR 写成了一个指标，比如写“性能优化 10%”，那因此谁受益了？你是为谁服务的？比如写“构建速度提升 10%，让研发者在构建的时候得到更快的反馈”，这里写让谁发生了一个变化很关键，所以要围绕用户价值进行阐述。</p><p></p><p>OKR 应该体现聚焦（即取舍），资源有限，集中精力办大事。举个反例，很多人写 OKR，带了 3 个人，写了 8 个 O，20 个 KR，这样写并不知道他们在干嘛。我的团队规模大约三四十人，我的 O 应该是 4 个，专注这四个目标，明确地告诉团队我们做什么和不做什么，如果我一旦列 40 个 O，那所有人都会不明白到底需要做什么。其实也没有那么多需要做的事情，把每一件事做好是不容易的，很多人做事情是抱着“广撒网总能捞到一条鱼”的态度，但我们的目标其实是去“捞一条最好的鱼”，因此一定要做取舍，集中精力办大事。</p><p></p><p>OKR 应该要尽可能量化（不必 100%），用来校准方向，且量化不应被用来考核绩效。量化的意思，即不要全是形容词，比如写“卓越的、先进的”，如何衡量是否卓越、是否先进、是否优秀？这很难说，所以需要去尽量量化。同时，量化也是个双刃剑，因为一旦量化之后，大家很容易陷入为了“数字”而去“做数字”的局面，所以作为管理者需要去和团队强调，量化的目的是为了让大家了解方向是否正确，进展是否偏离，而不是为了进行 KPI 考核，一旦强调是在考核，那“数字”必然会变得好看，但实际上毫无用处，还可能给团队带来负担。所以作为管理者一定要谨慎地对待量化，并在团队建立起好的共识。</p><p></p><p>OKR 的承接应该遵循 Single Threaded Leadership 原则。OKR 的负责人没有，或者 OKR 的负责人有一堆，都是错误的。在《亚马逊逆向工作法》一书中，有个观点非常好，即你关键的 O 需要有唯一的责任人，他只对这个 O 负责，并不用对太多事情负责，权责对等。</p><p></p><p>OKR 应该公开，且根据实际情况沟通调整。反例：一线员工看不到主管的 OKR，看不到更高层级主管的 OKR。</p><p></p><p></p><h3>工程文化</h3><p></p><p></p><p>接下来，讲一些被广泛低估的一件事，工程文化。</p><p></p><p>为何工程文化重要？</p><p></p><p>中国的互联网快速发展，导致大家产生了我们的工程能力赶超英美的错觉，但其实在很多方面还是会被打回原形。我们整个工程能力在性能和稳定性领域，其实和谷歌、微软等公司没有太大差距，因为我们分布式系统的用户量会要求我们的性能必须做到极致，否则便无法支撑。我们一直讨论的研发效能、代码质量等其实都是工程文化的问题，软件系统是极高复杂度的系统，工程文化一旦出现问题，复杂度失控，质量失控，会导致系统崩溃；其次，研发人员是知识工作者，是“手艺人”，良好的工程文化能激发他们的工作热情，反之则会消磨热情，增加稳定性风险。像阿里这种规模的公司，每月多人协同产出数以亿级的代码行，其实是非常复杂的，如果没有好的工程能力最后会无法维护。</p><p></p><p>除了效率问题，另外一个是激励问题。工程师其实都想在专业领域做得很好，抛开只为达成功利目的的少数人，大部分人都是希望把自己的工作给做好，代码写得漂亮舒服，被人认可，这是大家都广泛认同的东西。毕竟，谁愿意在代码“屎”山上工作呢？所以我们需要去重视工程文化。</p><p></p><p>技术管理者与其他领域的管理者之间最大的不同，就是技术管理者除了招聘、做战略规划之外，还需要关注团队的工程文化。那如何建设工程文化？以下是我的一些做法：</p><p></p><p>要求代码开放，要求 code review，要求 unit test搭建 CI 看板技术领导者每天参加 code review绩效考核 / 晋升考核中纳入“技术素养”的要求定义阶段性技术目标，降低系统复杂度（如：下线服务，架构治理）</p><p></p><p>如果作为技术管理者不亲力亲为，工程文化往往会被牺牲。举个例子，我的老板是研究员级别，P10 级别，他每天都会花不少时间看 code review，每天挑几个看一下并给个反馈，慢慢地就形成了一个比较好的工程文化。如果团队里有工程师有事没事删个几千行代码，那他一定是佼佼者，因为降低系统复杂度是比往系统里怼功能加代码难得多的事情，以上的这些方法都很关键。</p><p></p><p></p><h3>案例：故障 Review 的重要性</h3><p></p><p></p><p>所有的公司都会去做故障 Review，但是我并不能确定是否所有的管理者都会去仔细 Review 团队中的每一个故障和细节。因为从中可以看到这几个方面的信息：</p><p></p><p>系统架构是否存在问题（例如：存在不合理依赖）研发流程是否存在问题（例如：代码提交没有单元测试覆盖）运维应急能力是否存在问题（例如：是否第一时间操作扩容）</p><p></p><p>基本上团队的每个故障我都会去看，最近看的比较少，因为故障比较少：），在新接一些系统时故障比较多。为什么要去做这件事情？一是你会对整个技术架构有更深入的理解，因为多数故障是工程能力不足的症状表现；另外，通过观察团队对故障的处理和复盘，可以发现团队里优秀工程师，优秀的工程师在处理故障的时候，他对整个系统的全局有着清晰点认识，但如果是一个不熟悉系统的工程师，他会非常慌张，因为他不知道哪里出现了问题。因此，一方面是在看系统，一方面也是在看人，在这种过程中，看到优秀的工程师需要去给他相应的一些资源支持，鼓励他去做架构治理，去下线系统等等，这都是非常关键的细节之处。</p><p></p><p>出现故障之后，了解问题并帮助大家改进，我们宣扬 blameless，即创造安全感。如果将故障与人的绩效挂钩，那会造成相互甩锅的局面，没有人想着去改进，故障非个人主观情况造成的话，没有必要去进行追责，鼓励大家发现问题，去改进系统，给予好的正向反馈才是重要的，这也体现出管理者对技术的要求与态度。</p><p></p><p></p><h3>建设开放透明的文化</h3><p></p><p></p><p>团队文化的建设其实是润物细无声的一件事，很多人将文化建设停留在口头，或者理解的是一些团建聚餐活动等，导致大家认为它是“虚”的，其实不然。如果你的团队文化不注意建设开放透明的文化，那会发生一些反例：</p><p></p><p>A 同学把自己写的代码设置为 private，他人不知道其工程能力，老板也不在乎，但是他非常会写 PPT 汇报。B 同学找 C，D 单独沟通获取了大量的信息后，和老板单独汇报（选取对自己有益的信息），促成了老板做出对自己有利的决策。B 同学就某个想法找 C、D 聊完后，包装成自己的观点，和老板单独沟通，给老板造成他能力强的印象。X 领导在一年中多次改变团队目标，但是未和团队解释这些变化的原因，导致团队士气低落。晋升季的时候，B 同学被晋升了，但是领导没有向大家清晰的公开晋升标准以及 B 同学何以满足这些标准，导致团队各种猜测。</p><p></p><p>那有哪些建设开放透明文化的方法：</p><p></p><p>开放团队所有的代码和文档关键决策公开群组 / 会议讨论，鼓励离线记录讨论公开晋升 / 奖励等标准，公开其过程公开团队和个人目标（如：OKR）</p><p></p><p>有很多团队中每个人都有自己的代码库，只有在彼此系统对接合作时才会了解，我要求团队必须强制开放代码，是因为这样做可以让所有人相互了解，形成一种同行的压力。在团队管理中，尽可能地给团队信息公开，信息透明，决策透明，避免私下沟通、信息差等问题存在，能够提高团队的效率和凝聚力。</p><p></p><p>另外，公开透明的环境会让“小恶”无处遁形，创造鼓励向善的行为十分关键；其次，在团队中给予充分的信息和规则，知识工作者自己可以做最高效的判断；最后，文化的建设需要时间，不可能一蹴而就。</p><p></p><p></p><h3>达尔文进化论的启发</h3><p></p><p></p><p>最后聊一下管理中的激励，大家讲管理经常会提到马斯洛需求层次理论，但这里我认为达尔文的进化论在管理和激励上给我们的启发会更大。</p><p></p><p>达尔文的进化论大家都了解，这里不再赘述，我简单提炼下几个核心的信息：</p><p></p><p>人类进化的 99% 时间（约 200 万年前到 3.5 万年）都处于依靠狩猎采集的社会状态。在这漫长时间内进化出来的，追逐地位和尊重的心理特点，普遍刻在每一个人的基因里。在狩猎采集阶段，面对力量和速度都远超人类的野兽，人类的核心竞争力是相互协作。被集体所排斥（安全感缺失）意味着高概率的死亡。被集体所尊重，获取更高的地位，意味着更多的物质机会和更多的交配机会。</p><p></p><p>我们今天所有的心理状态都是在进化过程中逐渐形成的，都旨在帮助我们生存，以上这些心理机制在今天的管理和激励上也有很大的借鉴意义，管理者需要意识到人类普遍对安全感的需求，对地位和尊重的需求。</p><p></p><p>总结一下，那如何在团队中营造一个充满安全感的工作氛围？首先，从微小之处让团队成员感受到自己的价值与意义，比如 1-on-1 的沟通，鼓励团队中的正向行为，公开场合明确目标，让团队成员知晓自己的决策过程等方式，让团队成员获得安全感和价值感。</p><p></p><p>其次，理解团队成员对于“名利”看重的心理机制，作为管理者可以尽可能的去为团队成员的成长提供资源支持，但固然无法满足每个人对于“地位”和“资源”的诉求。然而，管理者可以做到对每个人的尊重，对其工作成果的尊重，不论是开庆功会还是群里发红包等形式，让团队成员感受到自己工作的价值和意义，这对促进团队成员的创造性行为是非常重要的。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ac/acaf9834668ff804b59027a7fa58585f.png\" /></p><p></p><p>定目标，找人才、建文化，这就是我们做团队管理比较关键的一些内容。有时候很多事情是很表面的，但实际上内心的一些机制、人的认知、人的心理，其实是起了一些决定性的作用的，我们是改变不了的，我们只能接受。在这些基础上，我们再思考可以去做哪些措施去不断地优化。</p><p></p><p>重新 Review 你的面试流程，是否有明确的标准要求？是否有严格的流程遵循？和团队的关键成员安排一次 1-on-1 沟通，关注你和他 / 她是否有清晰一致的目标？把团队所有代码设置成尽可能公开，至少团队内公开，每天至少花 2 小时 Review 代码。思考并修订团队目标（OKR），和你上级讨论达成共识，在团队公开宣讲。思考团队目标，如果必须要砍掉其中一个子项，你会怎么选择，写下你的思考。整理团队的技术债务和技术风险，产出改进计划。</p><p></p><p>最后我认为很关键的一句话，做管理实际上你要在团队内建立一种氛围和文化，把每个人的善意都激发出来，我觉得这是非常值得做的一件事情，不一定是伟大，但是是非常有意义的一件事情。抛开我们对升职加薪那些功利的追求之外，每个做管理的人，不论你的团队是 10 人、 20 人、 30 人、 50 个人还是 100 人，每个人产生的影响都是巨大的，这也可能是对技术更有价值的一件事情。</p><p></p><p></p><h3>作者简介</h3><p></p><p></p><p>许晓斌，阿里巴巴工程师，技术管理者，目前负责阿里巴巴集团运维及构建基础设施平台，《Maven 实战》作者。</p><p></p><p></p><h3>活动推荐</h3><p></p><p></p><p>5 月 26 日 -5 月 27 日，QCon 全球软件开发大会即将落地广州，从下一代软件架构、研发效能提升、DevOps vs 平台工程、AIGC、数据驱动业务、工业互联网、出海的思考、金融分布式核心系统、大前端架构等角度与你探讨，欢迎你来现场打卡交流～</p><p></p><p>点击<a href=\"https://qcon.infoq.cn/2023/guangzhou/track?utm_source=infoq&amp;utm_medium=gw&amp;utm_campaign=8&amp;utm_term=0327&amp;utm_content=srr\">此处</a>\"直达大会官网，现在购票享 8 折优惠，组团购票还有更多折扣，感兴趣的同学联系票务经理：15600537884（电话同微信）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e1/e14b91f7cf9ffa4e8ae68eb7829f0898.png\" /></p><p></p>",
    "publish_time": "2023-03-27 11:50:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "后摩尔定律时代，如何提升云效益的天花板",
    "url": "https://www.infoq.cn/article/SLGbYU0RINZR13q1XGTH",
    "summary": "<p>在摩尔定律失效的今天，各行各业对算力的需求却空前膨胀。大数据、AI 等趋势方兴未艾，生命科学、智能制造等行业的深度数字化，也给数据处理的规模和性能带来更高要求。</p><p></p><p>云作为如今数字经济的基础设施，承载着海量的应用。云厂商不得不思考，如何才能更好地满足客户对数据处理效率越来越高的要求，对算力性能、性价比越来越高的要求？</p><p></p><p>在摩尔定律失效的今天，当前云上的企业是否已经触碰到了云效益的天花板？云厂商可以做点什么，来突破传统计算架构下对算力的限制？作为一种新型的计算服务提供模式，云服务器所提供的性价比如何超越自建数据中心？</p><p></p><p>本文，InfoQ 于<a href=\"https://www.infoq.cn/article/riqXTpXeAWK5AaGCCxDK\">阿里云第八代企业级实例 g8i </a>\"正式发布之际，采访到了阿里云弹性计算产品总监王志坤和阿里云高级产品专家姬少晨，试图寻求上述问题的答案。</p><p></p><h1>死磕“性能”，软硬一体化重塑计算架构</h1><p></p><p></p><p>3 月 24 日，阿里云发布第八代企业级实例 g8i。其依托于 CIPU+ 飞天的技术架构，搭载第四代英特尔至强处理器（代号 Sapphire Rapids，SPR），全核睿频 p0n 达到 3.2GHz，相比上一代实例，整机核密度提升 50%，性能提升 60% 以上。</p><p></p><p>存储方面，IOPS 最高达 100 万，全面适配 NVMe 云盘，存储延时低至百微秒，同时支持共享盘。网络方面，全面升级至配阿里云自研 eRDMA 大规模加速能力，g8i 是业界首个具备大规模弹性 RDMA 加速能力的计算实例，网络延时最低 8 微秒，为数据库、大数据等常见应用带来进一步的性能跃升。</p><p></p><p>这种算力服务水平的不断提升，效益的不断突破，从何而来？</p><p></p><p>从阿里云弹性计算的一路演进来看，答案很明确：软硬一体化，甚至尝试重构传统计算架构。具体到本次 ECS g8i 实例，其表现为基于“飞天 +CIPU”架构。</p><p></p><p>如今，软硬一体化成为所有云厂商共同的方向。如果将时间倒退到 2017 年，阿里云是孤独的。</p><p></p><p>以往，企业选择上云主要是希望通过其进行商业模式的创新，云计算弹性、灵活、免运维的原始特点降低了企业的初始建设门槛，虽然这牺牲了一部分性能，但在当时的商业环境下，这样的性能损失与其带来的商业价值相比并不关键。</p><p></p><p>后来，随着越来越多的企业上云，性能损耗问题越来越突出。为了解决这一问题，阿里云相关研发团队于 2017 年推出业内首款计算虚拟化损耗为零的神龙弹性裸金属服务器，深度融合了物理机和虚拟机特性，标志着神龙架构的诞生。</p><p></p><p>45 天之后，亚马逊云科技在地球另一端发布了自研的云服务器硬件——Nitro。</p><p></p><p>自此，云厂商开始深入硬件领域，战场开始发生转移。</p><p></p><p>2019 年 9 月，阿里云正式发布第三代自研神龙架构，用户能在云上获得超越传统物理机 100% 的计算能力。2021 年，第四代神龙架构诞生，具备业界首个大规模弹性 RDMA 加速能力。2022 年，神龙架构全面升级为 CIPU，可实现对计算服务器即插即用，对数据中心内部的云计算体系架构进行改革创新，从以 CPU 为中心的体系架构进入以飞天操作系统 +CIPU 为中心的体系架构。</p><p></p><p>如今，云计算所能提供的优势不单单是免运维，而是性能也就是性价比的全面提升。ECS g8i 的发布让这种能力更加普惠，让大部分云上用户都可以感受到云平台本身的性能跃迁。</p><p></p><h1>普惠 eRDMA 加速能力，突破场景化性能极限</h1><p></p><p></p><p>ECS g8i 实例的发布也标志着阿里云自研 eRDMA 能力的全面商业化，这也是本次第八代实例 g8i 的性能大杀器之一。eRDMA 能够大幅提升大规模计算通信效率，并且能够随着阿里云的集群规模动态扩展，轻松构建大规模 RDMA 高性能计算网络。</p><p></p><p>RDMA（Remote Direct Memory Access）是一种高性能网络传输技术，可将数据直接从一台计算机的内存传输到另一台计算机，数据传输不经过 CPU。相比传统 TCP 网络，RDMA 能够大幅减少 CPU 的开销，并降低网络互联带来的通信延迟，有助于在云上处理更大数据量的应用。</p><p></p><p>然而，搭建 IB (InfiniBand) 和 RoCE（RDMA over Converged Ethernet）等主流 RDMA 方案，存在部署周期长、维护成本高、使用门槛高和无法大规模组网等弊端，同时与现有 TCP 网络不兼容。企业需要购置昂贵的专用设备，并对应用做改造，才能用上 RDMA 能力。</p><p></p><p>阿里云通过自研的 CIPU，基于云上通用设备，研发出“弹性 RDMA”，简称 eRDMA。相较于传统 TCP 网络，eRDMA 具有更高性能，同时消灭了上述弊端。</p><p></p><p>基于 CIPU，eRDMA 与云上 VPC 共享同一张网络，用户可以随开随用具有 eRDMA 能力的实例；现有 Redis、Spark、AI、HPC 等应用，通过阿里云提供的接口，即可一键适配，最大程度降低企业的使用难度。同时，企业的大规模分布式计算应用将更加高效，能够以更低的成本在更短的时间内处理更大规模的数据量。</p><p></p><p>与此同时，芯片厂商的王牌代表英特尔，也在用自身的方式，正在打破摩尔定律失效的魔咒。他们给出的方式，是 CPU 内置多种加速器——让 CPU 不仅仅承担通用计算的功能，而是变得擅长更多垂直场景。</p><p></p><p>第四代英特尔至强可扩展处理器所配备的硬件原生加速器，搭配上阿里云的 eRDMA 能力，更是使 g8i 在大数据和数据库等场景下的性能大幅提升。</p><p></p><p>“英特尔已经不是一家传统的 CPU 厂商，它也在积极地向云转型，而通过硬件辅助的虚拟化带来的性能优势是巨大的。阿里云每年都会与英特尔保持高密度的沟通，双方就如何让云计算更加普惠做深入合作，而只有当云本身达到一定的体量，这种合作的规模化效益才能展现出来，实现双赢。”王志坤表示。</p><p></p><p>阿里云弹性计算产品线负责人张献涛表示，阿里云 CIPU+ 飞天的技术架构与第四代英特尔® 至强®可扩展处理器的强强联合下，阿里云第八代企业级实例 g8i 规格族性能最大提升了 60%，叠加第四代英特尔® 至强®处理器的加速器，在大数据、数据库等场景实现了数倍级性能提升，进一步为客户实现降本增效；同时，双方就机密虚拟机能力 TDX 在云上的实践进行了深度的技术合作，相信在双方的持续紧密合作之下，将会给更多各行业的客户带来更具性价比的技术红利。</p><p></p><h1>卷安全：将机密计算拉下神坛</h1><p></p><p></p><p>在性能得以持续攀升的背后，云厂商必须死守安全底线，否则皆是空谈。</p><p></p><p>安全方面，本次发布的 ECS g8i 实例支持可信计算与加密计算等特性，默认内存加密（TME），并率先支持机密虚拟机 TDX（Intel® Trusted Domain Extension）能力。</p><p></p><p>随着云计算的大规模部署，机密计算旨在允许将云提供商从可信计算基础（TCB）中移除，以便只有硬件和受保护的应用程序本身在可信边界内。这使得客户可以放心地、安全地将业务负载转移到公有云上。然而，过去多年，机密计算对用户来说始终是一种“看得着，摸不着”的存在。</p><p></p><p>在工程落地层面，主流的机密计算技术方案要么对应用具备较大侵入性，要么性能会做出较大牺牲。作为亚太地区最早部署机密计算的云厂商，阿里云一直在持续推广可信与机密计算技术以为客户的数据提供更好的保护。</p><p></p><p>ECS g8i 实例成功将机密计算“拉下神坛”，其全量搭载安全芯片 TPM 作为硬件可信根，实现服务器的可信启动，确保零篡改；虚拟化层面，支持虚拟可信能力 vTPM，提供实例启动过程核心组件的校验能力。在实例可信的基础上，配合英特尔硬件 TEE 能力和内存加密 TME (Intel® Total Memory Encryption) 技术，以及阿里云自研的加密计算隔离环境 enclave，ECS g8i 实例进一步实现数据的可用不可见，为大型互联网、新金融、医疗保健、知识产权等业务场景提供了更高安全等级数据保护能力和云上可信运行环境。</p><p></p><p>内存加密 TME 技术是新一代 ECS g8i 实例独具的全新安全加密技术。在该技术的加持下，ECS g8i 实例默认全内存加密，加强内存数据的抗物理攻击能力，进一步提升云上数据的安全水位，用户无需对操作系统或应用进行任何改动，即可享受到更高一层的安全防护。</p><p></p><p>同时，基于英特尔第四代至强可处理器的 g8i 实例还实现了机密虚拟机能力 TDX 在云上的实践，无需用户二次开发即可将现有应用迁移至受 TDX 保护的实例。阿里云和 Intel 在 TDX 的架构设计、功能验证、安全分析和性能优化等方面均进行了紧密的合作，并实现了 TDX 技术在云上的首次应用。目前，阿里云在全球范围内实现了该能力的首发，这将推动机密计算的通用化和平民化，并与可信技术一起成为未来云上服务器的标准能力。</p><p></p><h1>行进在云普惠的路上：天花板远未到达</h1><p></p><p></p><p>过去 16 年的发展历程中，阿里云的弹性计算演进史可以概括为云计算技术的普惠历程、算力的普惠历程，这对当今处在人工智能风暴中的企业而言具备极大的场景价值。</p><p></p><p>时至今日，云所带来的已经不单单是运维优势，性能提升的背后蕴含着的是巨大的成本优势。随着阿里云这样的云厂商在核心技术层面不断做出突破，将会有越来越多的企业愿意深度用云。</p><p></p><p>在政策方面，数字中国建设、东数西算等都将云计算放到了非常重要的位置。未来十年，在多方利好因素促成之下，我们有理由相信云计算将进入普惠发展期。</p><p></p><p>所以，云效益的天花板远未到达，有些人尚未触顶，有些人在不断突破极限。</p>",
    "publish_time": "2023-03-27 14:09:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "回国后，马云首谈ChatGPT：这类技术已对教育带来挑战，要用AI解决问题，而不是被其所控制 | InfoQ快讯",
    "url": "https://www.infoq.cn/article/dHVFNs6EvQKXWeoq4WMA",
    "summary": "<p>马云回国的消息刷屏朋友圈。</p><p>&nbsp;</p><p>3月27日，微信公众号 “云谷教育 ”刊发了《马云来云谷学校，和校园长讨论了未来教育》的视频和文章，侧面证实马云已回国。</p><p>&nbsp;</p><p>云谷是马云创办的学校。据悉，云谷从创校以来，一直在探索如何在人工智能快速发展的时代为孩子提供面向未来的教育。</p><p>&nbsp;</p><p>对此，马云表示，ChatGPT这一类技术已经对教育带来挑战，但是ChatGPT这一类技术只是AI时代的开始。我们要用人工智能去解决问题，而不是被人工智能所控制，虽然人的体力、脑力比不过机器，但机器只有“芯“，而人有“心”。</p><p>&nbsp;</p><p>马云表示，工业时代是知识驱动，知识的竞争；数字时代，是智慧驱动，是创造力和想象力的竞争，是领导力、担当力、责任的竞争，是独立思考的竞争。</p>",
    "publish_time": "2023-03-27 14:47:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Twitter部分源代码泄漏、疑遭离职员工报复：马斯克要求GitHub交出所有上传、下载人员名单",
    "url": "https://www.infoq.cn/article/FXHB5FYwWjrxKdYY61Bj",
    "summary": "<p></p><p>&nbsp;</p><p>最近的法律文件显示，Twitter的部分源代码已经在网上泄露。这是一次罕见的、严重的知识产权泄露事件。为防止该事件对其服务产生潜在的破坏性损失，Twitter已经采取了法律行动。</p><p>&nbsp;</p><p></p><h2>部分源代码在线泄露</h2><p></p><p>&nbsp;</p><p>根据文件显示，上周五Twitter采取了行动，向GitHub发送了版权侵权通知，并删除了被泄露的代码。GitHub遵从了通知并删除了该代码。DMCA通知显示，这次泄露的源代码是Twitter平台和内部工具的私有源代码，严重侵犯了Twitter的版权。具体的仓库地址是：<a href=\"https://github.com/FreeSpeechEnthusiast/PublicSpace\">https://github.com/FreeSpeechEnthusiast/PublicSpace</a>\"。该帐号的名字 “FreeSpeechEnthusiast”，这显然是对马斯克的攻击，毕竟他曾自称 “言论自由绝对主义者”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af84300ebea0eabc16bce92b51bbcd24.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>根据备案文件，Twitter 现在正在寻找该账户背后用户的明确身份，该用户于年初在 GitHub 上注册，似乎只上传了 Twitter 代码。马斯克所拥有的这家公司还通过美国加利福尼亚州北区地方法院命令 GitHub 识别所有上传和发布代码的人，甚至还包括了所有下载此代码的人。他们要求GitHub提供相关各方的姓名、地址、电话号码、电子邮件、社交媒体资料和 IP 地址。那 GitHub 到底会不会交出所有进行了“git clone”的人员的信息？有网友回复说：“目前不存在这种法律”。</p><p>&nbsp;</p><p>据《纽约时报》援引未具名知情人士的话说，该公司还对泄密事件展开了内部调查。目前还不清楚泄露的代码在网上发布了多长时间，但它似乎已经公开至少几个月了。根据处理此事的高管推测，负责此事的人去年离开了 Twitter ，应该是在马斯克入驻Twitter后，正值公司内部动荡期间。</p><p>&nbsp;</p><p>马斯克接手时，Twitter的员工总数为7500人，而现在的员工人数不到2000人。尽管每次裁员马斯克都实施了临时代码冻结举措，以防止在裁员期间对其应用程序进行任何更改，但这可能是被裁掉的5500人中某个心怀不满的员工故意实施的报复行为。而Twitter这些高管最近才知道源代码泄露了。</p><p>&nbsp;</p><p>一般平台和工具的私有代码会被严密保密，严禁分享，因为可能会被竞争对手利用或暴漏安全漏洞。因此这些高官担忧该代码包含安全漏洞，可能会让黑客或其他有动机的各方有机会提取用户数据或关闭网站。网络安全软件公司 Emsisoft 的威胁分析师Brett Callow也对此评论道，Twitter 代码的公开发布确实“令人担忧”，“它让探测漏洞变得更容易、更快捷。”</p><p>&nbsp;</p><p></p><h2>迟迟未能实现的开源</h2><p></p><p>&nbsp;</p><p>意外泄露的私有代码在短期内对安全性非常不利，而有意开源的代码往往具有更好的长期安全性。在泄密事件之前，马斯克已承诺开源 Twitter 的部分代码。马斯克过去曾多次表示，他支持将该平台算法开源的想法，从表面上开源也有利于提高Twitter的平台透明度。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5ff6808df83b2df4f51ddddf614f26ca.jpeg\" /></p><p></p><p>&nbsp;</p><p>在此之前，马斯克其实在2 月 22 日的时候就宣布过“Twitter的算法将于一周后开源”。当时一位Twitter用户在与马斯克互动时表示，把Twitter开源吧，如果能够做到这一点，他们“真的会印象深刻”。马斯克随后回复称：“我们的算法会在下周开源，首先要做好失望的准备，但它会迅速改进的！”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c869f69cd579252e3bd88e5e35a8eac8.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>马斯克虽然理解开源的价值，但Twitter真正的开源又迟迟实现不了。一直拖到3月18日，马斯克才再次发声，表示将于3月31日开源。到时到底是否能按时开源，也很难说。开源需要有专门的软件开发人员和工程师，他们了解算法背后的代码以及需要做什么才能开源该代码，而该公司开源相关的人实际上都早已离职。Twitter 的前开源负责人 Will Norris 曾表示：</p><p>&nbsp;</p><p></p><blockquote>当我加入 Twitter 时，我们已经有许多大型开源组件的现代化工作正在进行中。Pants<a href=\"https://www.pantsbuild.org/\">构建系统</a>\"正在被<a href=\"https://bazel.build/\">Bazel</a>\"取代。正在努力准备最终用<a href=\"https://kubernetes.io/\">Kubernetes取代</a>\"<a href=\"https://aurora.apache.org/\">Apache Aurora</a>\"和<a href=\"https://mesos.apache.org/\">Mesos</a>\"。<a href=\"https://kafka.apache.org/\">而且，我们已经是Apache Kafka</a>\"、<a href=\"https://hadoop.apache.org/\">Hadoop</a>\"和<a href=\"https://www.scala-lang.org/\">Scala</a>\"的最大用户之一。我们还有一个 JVM [&nbsp;<a href=\"https://www.infoworld.com/article/3272244/what-is-the-jvm-introducing-the-java-virtual-machine.html\">Java 虚拟机</a>\"] 的自定义分支，我们希望它最终能够开源。有很多令人惊叹的工作正在进行中，Twitter 成功地从这些社区聘请了非常优秀的人来从事这些项目。然后裁员发生了。</blockquote><p></p><p>&nbsp;</p><p>Norris 对此无限感慨：“大多数在 Twitter 从事开源工作的关键人物都离开了。所有与我一起从事开源工作的工程师也都离开了。”</p><p>&nbsp;</p><p>Twitter的源代码在开源前被泄漏，这肯定会让马斯克感到不舒服，就是不知道最后会有哪个倒霉高管背锅。</p><p>&nbsp;</p><p>GitHub通常不会就删除内容的决定发表评论。但据称，有媒体向Twitter求证时，用于媒体查询的电子邮件发出了一条自动回复，其中使用了“大便表情符号”。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.nytimes.com/2023/03/26/technology/twitter-source-code-leak.html\">https://www.nytimes.com/2023/03/26/technology/twitter-source-code-leak.html</a>\"</p><p><a href=\"https://twitter.com/elonmusk/with_replies\">https://twitter.com/elonmusk/with_replies</a>\"</p><p><a href=\"https://github.com/github/dmca/blob/master/2023/03/2023-03-24-twitter.md\">https://github.com/github/dmca/blob/master/2023/03/2023-03-24-twitter.md</a>\"</p>",
    "publish_time": "2023-03-27 14:52:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全球首款RISC-V笔记本ROMA正式发布：搭载openKylin操作系统，可体验原生RISC-V开发",
    "url": "https://www.infoq.cn/article/QgsXcWCCeE1i2MEOFhwi",
    "summary": "<p>2023年3月26日，在第十届开源操作系统年度技术会议（OS2ATC）上，鉴释科技和深度数智正式发布了全球首款 RISC-V 笔记本电脑 ROMA。</p><p>&nbsp;</p><p>据介绍，ROMA是由RISC-V基金会牵头，深度数智开发、鉴释科技调试的全球首款原生RISC-V开发笔记本电脑，默认搭载 <a href=\"https://www.infoq.cn/article/Qw7SwAB6W8wBg8ZrD8IJ\">openKylin</a>\"（开放麒麟）国产操作系统，可体验原生RISC-V开发及RISC-V软件生态系统。作为一款RISC-V笔记本电脑，ROMA的发布为RISC-V的生态应用提供了更多可能，真正实现了RISC-V从物联网领域向通用桌面领域的迈进。</p><p>&nbsp;</p><p>发布会上，<a href=\"https://xie.infoq.cn/article/3c31098b123c2cec9ff0ac023\">鉴释科技</a>\"创始人兼CEO梁宇宁展示了ROMA的相关设计与功能。ROMA采用12nm(专业版)/28nm(标准版)SoM封装；配备GPU、NPU和功能加速器的四核RISC-V笔记本电脑；可升级 SoC 主板。</p><p>&nbsp;</p><p>openKylin社区秘书长余杰介绍了openKylin操作系统在RISC-V方面的进展和规划。openKylin对ROMA进行了RISC-V专项技术优化，包括内核&amp;支撑层，可扩展<a href=\"https://www.infoq.cn/article/4lDxHRoqXbSw6zl29AEo\">操作系统</a>\"架构，硬件设备驱动等，针对开发者支撑环境提供1万+全量软件包，软件生态包括openKylin自研、FireFox、钉钉等。</p><p>&nbsp;</p><p>中国工程院院士、清华大学教授郑纬民在发布会上表示：“当前科技创新在全球范围内进入了加速模式，科技产业的地位越来越重要，大批工程师活跃在开源中，而RISC—V的出现为全球芯片领域打破垄断、推进创新、开创新局面，也为中国芯片行业发展提供了新的机遇。</p><p>&nbsp;</p><p>基于RISC—V的开源优势，近几年国内不乏一些优秀企业和机构在积极推进RISC—V的生态发展，并且取得了不错的成绩。RISC—V不仅可以在物联网、智能家居等算力要求不高的领域得到推广应用，在众多企业和研究院所的共同努力下，RISC—V也逐步向边缘计算、服务器、自动驾驶等对算力有更高需求的领域发展。</p><p>&nbsp;</p><p>今天是ROMA的产品发布会，我了解到在过去几年深度数智的各位技术同行在打造ROMA过程中遇到的各种困难与挫折，作为生产第一台RISC—V笔记本电脑的各位来说，过程无疑是艰难的，很多问题都是摸着石头过河去解决。但当此问题解决以后，一切都迎来了希望的曙光，也为致力于RISC—V生态发展的其他企业收获了更多的信心。”</p>",
    "publish_time": "2023-03-27 15:09:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "快手针对大型推荐模型的性能优化",
    "url": "https://www.infoq.cn/article/Nou4kpZa2AU5dadx61z6",
    "summary": "<p></p><blockquote>北京时间2023年3月23日11:00，在英伟达GTC开发者大会上，来自快手的软件架构师梁潇分享了题为《<a href=\"https://register.nvidia.cn/flow/nvidia/gtcspring2023/registrationcn/page/sessioncatalog/session/1666621870571001rXJL?n=undefined\">针对大型推荐模型的性能优化</a>\"》的演讲，介绍了快手如何通过平衡CPU 和 GPU 的工作负载，实现大型推荐模型的性能优化。</blockquote><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/d6/c7/d61bdbafe524007963b6221dbae029c7.png\" /></p><p></p><p>在本文中，我将分享改进大模型推荐服务的一些经验——包括如何通过平衡 CPU 和 GPU 之间的工作负载，使得资源利用更加平衡，并提高系统吞吐。也希望能提供一些在日常工作中充分利用 GPU 算力的经验。</p><p>&nbsp;</p><p>快手作为热门的短视频平台，日活用户就达到了3.6 亿，日均时长为 129 分钟。为了提高用户体验，我们需要在个性化推荐领域做出非常多的努力。</p><p>&nbsp;</p><p>与业界很多场景的推荐方案不同，我们的推荐服务主要面临两个挑战：</p><p>首先我们必须提供实时训练，以确保新近上传的视频可以在短时间内被推荐出来；</p><p>其次是模型的规模——个性化推荐需要大量特征进行准确预测，从而导致模型非常庞大。</p><p></p><p>例如，我们的精排模型使用了超过 1000 亿个特征，大约 1.9 万亿个稀疏参数。对于这样一个模型，常规的做法导致 80% 以上的时间都花在 CPU 计算上（包括特征抽取和 Embedding 处理），而只有 20% 的时间用于模型推理。这导致整个系统是一个高度内存密集型的系统。如果我们仅仅将模型推理部分移到 GPU 上，CPU 仍然会是整个系统的瓶颈，导致 GPU 的利用率受到限制。</p><p>&nbsp;</p><p>为了解决这些问题，我们对这个系统做了很多改进：消除CPU 瓶颈的最基本的想法是将 CPU 工作负载转移到远程服务器上。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/3a/ce/3afdd3f696c0db807a353f72b14246ce.png\" /></p><p></p><p>有了这些远程服务器，我们可以很容易地通过调整两种服务器的实例个数来平衡系统，从而最大限度地减少对现有代码的修改。然而，在实际工作中，这种方法并不奏效。</p><p></p><p>首先，因为模型特征的数量很大，会导致这两种服务器之间的网络流量很高。</p><p>其次，为了处理网络数据包，GPU 服务器也会承担额外的内存访问开销，从而导致实际性能收益降低。我们对此做过评估，远程服务器的方案会对系统带来 20% 的额外开销，效果并不理想。</p><p>&nbsp;</p><p>另外一种方案是通过平衡CPU 和 GPU 的工作负载来优化整个系统——所有的工作都在同一台服务器上完成。这样做非常易于部署，并能同时充分利用 CPU 和 GPU 资源。这个方案的实现关键是：将部分负载转移到 GPU 上。为了做到这一点，我们首先要深度于优化 CPU 算法，并且提升模型在 GPU 上推理的效率。推理所需的时间越少，就意味着有更多的 GPU 算力可以用来承载从 CPU 上迁移的算法。此外，我们还尝试在 GPU 端缓存数据，从而减少对 DRAM 的访问量。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/59/29/593db87199ca27a94ca1aa8e736faa29.png\" /></p><p></p><p>在实施这些优化之后，<a href=\"https://s.geekbang.org/search/c=0/k=GPU%20/t=\">GPU </a>\"利用率从 20% 大幅提高到近 90%，整个系统的吞吐量提高了 10 倍以上。</p><p>&nbsp;</p><p>大家可以先看一下系统的整体流程。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/91/da/91c5f852316200180c96ce6b5bac98da.png\" /></p><p></p><p>&nbsp;</p><p>当服务接收到一个请求的时候，Feature Extractor 会向调度器请求 Data Slots。Data Slot 是 Data Context 的一部分，而一个 Data Context 可以理解为批任务（Batching）的数据存储单元，它被同时映射到 CPU 和 GPU 地址段上，因此调度器可以将 Context 中的数据在 CPU 和 GPU 之间自由迁移。调度器等这个 Data Context 填充完毕，就会将它发送到推理引擎。</p><p></p><p>与很多方案不同，在特征抽取开始之前，Data Context 就已经分配好了。虽然这会导致分配更多的内存（因为占用时间变长了），但是它让调度器对这个批任务有了更加充分的信息，可以选择更好的推理策略。特征抽取是在 CPU 或 GPU 上运行的算法，运行结果也可以直接保存在 GPU 内存中。在引擎部分，我们同时支持 TensorRT 和 Tensorflow。TensorRT 性能较好，但因为 Tensorflow 的兼容性更好，我们仍然需要它做一些对性能不太敏感的实验或验证。</p><p>&nbsp;</p><p>在整个系统中，调度器（Scheduler）是最为关键的组件：因为我们需要对请求进行打包批处理（Batching），从而提高 GPU 的利用率。然而，在不同的时间段，每个请求中的候选视频个数并不均匀。调度器需要将请求定向到不同的 Batch，以确保尽可能将每个 Batch 填满。</p><p></p><p>为了提高性能，我们使用的是 Fixed-Size Model 进行推理（即输入 Tensor 的 bs 维度是固定的，不能改变），然后再通过调度器加载多个不同 Batch Size 的模型。例如，如果有 5 个请求，总计 200 个候选视频组成了 Batch，我们可以使用一个 bs = 256 的模型来推理。如果有 900个候选视频，则会选择 bs = 1024 的模型。调度器负责维护不同 bs 的模型，并根据不同批次的大小选择合适的模型。</p><p></p><p>其次，调度器需要支持 dense 参数的实时更新，因此需要能够降低更新时对服务稳定性的影响（尤其是当有许多模型实例一起服务时，这种影响更加明显）。最后，调度器需要在内存密集型系统中考虑 NUMA 和 PCI-e 的亲和性。跨 NUMA 访问对系统延迟和吞吐的影响很大，调度器需要知道处理线程在哪个 CPU 上，并尽量把结果放在相应的 NUMA 节点上。根据测试，我们设计的调度器的吞吐量大约是 Tensorflow-Serving 的 2 倍。</p><p>&nbsp;</p><p><a href=\"https://xie.infoq.cn/article/680ffd538f9eba1700fe99134\">TensorRT </a>\"的性能很好，但它对算子的支持能力有限，因此加载模型相对复杂。目前，官方并不提供从 Tensorflow 到 TensorRT 的解析器，我们实验后决定不从 ONNX 模型进行解析。最主要的原因是希望保留 Tensorflow 中的高语义运算符，这些运算符更易于编写 Kernel Funsion。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1f/db/1fbe5ebf4f37523304b2a1229c0yybdb.png\" /></p><p></p><p>&nbsp;</p><p>我们设计的模型加载器包含多个模块：首先是一个简单的Parser，将Tensorflow 模型文件解析成内存中的图结构（Graph），然后将图传递给 Graph Transformer，后者用来解决算子不兼容问题。假设某个算子不被 TensorRT 支持，Transformer 就会进行基于规则进行局部搜索，以确定是否能将某个包含这个算子的子图替换成另一个完全等价、且与 TensorRT 兼容的子图。Graph Fuser 用于算子融合，但对于实时更新的系统，有一些需要进行权重变换的融合并不容易实现。例如，右图所示的融合方式，需要将权重融合到一起。Weights Transformer 会记录这些融合步骤，并在参数更新期间按照这些步骤依次执行，从而保持训练侧不需要改变。经过这些步骤之后，Graph 就可以被 TensorRT 加载了。Builder 会负责将其转换成 TensorRT 的 Engine。</p><p>&nbsp;</p><p>在针对TensorRT 的性能调参中，我们总结了一些相关经验以供参考。或许在不同的项目里面，其他的参数配置性能更好，所以建议在做出决定前尝试每个参数。</p><p></p><p>首先，我们发现Model Refit 对于实时更新权重更加友好。因为 Model Reload 非常耗时，从而导致更新期间的 P99 时延升高。即便使用了 TensorRT 8 中的 Timing Cache 机制，耗时仍然不可接受。然而，开启 Refit 会禁用掉一些 Point-wise Fusion，后续需要手工实现这些 Fusion。</p><p></p><p>其次，我们更加推荐使用Explicit Batch，而不是 Implicit Batch。不仅因为 Explicit Batch 在设计图时更加灵活，还因为它的性能更好——如果模型图中的 shape 是固定的，TensorRT 可以优化得更好。缺点如前所提，如果使用 Explicit Batch 就需要调度器加载不同 Batch Size 的多个模型实例以适应不同 Batch。</p><p></p><p>此外，我们没有使用 Dynamic Shape，因为在项目实际测试中，Dynamic Shape 的性能比 Fixed Shape 低。使用如上设置后，TensorRT 在大部分场景下比 TensorFlow XLA 快 20％。</p><p>&nbsp;</p><p>在GPU 中缓存 Embedding 也是减少 CPU 负载和内存访问的有效方式。由于 Embedding 数量很大，我们使用远程服务器存储，本地使用了 Embedding Cache 的机制来降低网络访问量，这种方式还减少了远程调用过程中网络包解析带来的带宽影响。在我们的系统中，使用了 1GB 的缓存大小，就已经缓存了大约 60% 的 Hot Embedding。为了进一步减少 CPU 的工作量，我们将 Embedding 量化为 8bit，并且没有使用 LRU 的更新策略。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b5/72/b5b30ccc3b5d37e36ff40e52bc168f72.png\" /></p><p></p><p>如下是一些测试结果：与完全没有缓存的版本相比，如果完全缓存在CPU 内存中，可以节省 5% 的 CPU 利用率。这主要来自远程调用量的降低。如果将 Embedding Data 转移到 GPU 上，可以再节省 5% 的 CPU 利用率，以及 50% 的 H2D 带宽。如果将 Cache Index 也转移到 GPU 上，效果将更加明显。这项工作仍在进行着，从初步结果来看，与目前工业界解决方案相比，性能有 1.2 倍的改进。相关细节信息可以在我们的论文中查看。</p><p>&nbsp;</p><p>将特征抽取算法转移到GPU 上也可以改善系统性能。通常来讲，在 GPU 上实现特征抽取算法并不容易，因为这些算法有很多条件分支，难以并行化。然而，在我们的场景中，这件事情仍然很有意义：在特征抽取后，数据量会显著增加，导致 H2D 的同步时间变长。另外，访问 GPU 内存比访问 CPU 内存快很多，这对于内存密集型的系统有明显的收益。 此外，迁移后 CPU 的工作量降低可以进一步提高系统各个运算单元的平衡。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/68/19/6862ee9fcc82dae8a7cff9367a6d9c19.png\" /></p><p></p><p>例如，对于Sum-Pooling 算法，最初的 CPU 版本改写成 GPU 版本后，CPU 内存访问减少了近 1GB/s，耗时降低了 45%。如果对实现高性能的特征抽取算法有兴趣，可以参考 cuDF 和 Velox。</p><p>&nbsp;</p><p>下图总结了各个性能优化方案的效果。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/47/0a/4718ce0b599b085334a39a80fede1c0a.png\" /></p><p></p><p>我们的主要任务是平衡CPU 和 GPU 之间的工作负荷，因此需要迭代式地对 CPU 和 GPU 代码进行优化。比如，在进行 Kernel Fusion 之前，我们花了大量精力优化 CPU 端的算法，大幅降低了内存访问压力。否则，在 GPU 上优化的性能收益会因为 CPU 瓶颈被掩盖。在图中，可以看出 Scheduler 是其他优化的基础，之后我们通过减少 CPU 内存访问和 GPU 上 Kernel Fusion 的方式提升了整体的吞吐。当这些事情做完之后，CPU 仍然是系统的瓶颈。我们尝试通过在 GPU 上进行数据缓存和将特征抽取算法转移到 GPU 上来减少 CPU 的工作量。上述优化总计优化了系统整体吞吐约 10.5 倍。</p><p>&nbsp;</p><p>以下为内容的总结。我们讨论了几种改善内存密集型系统吞吐量的方法。</p><p></p><p>首先，需要优化CPU 算法，降低访存量；</p><p>其次，需要提高 GPU 推理效率，使 GPU 腾出算力承载更多算法；</p><p>再次，可以将 CPU 的算法迁移到 GPU 上，减轻 CPU 的工作压力；</p><p>最后，在 GPU 中缓存数据可以减少 H2D 的同步时间，并降低 DRAM 的访问量。</p><p></p><p>以上这些优化方法，可以使得整体系统更平衡，并帮助我们在成本可控的情况下，为用户提供更好的服务。</p>",
    "publish_time": "2023-03-27 17:28:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "网新集团、唐氏集团、浙大计算机创新技术研究院、赛智伯乐一行嘉宾来访极客邦科技",
    "url": "https://www.infoq.cn/article/5sRgPtlEfyrpo0rWcxc5",
    "summary": "<p></p><blockquote>3 月 24 日，网新集团、唐氏集团、浙大计算机创新技术研究院、赛智伯乐等一行嘉宾到访极客邦科技，各方就企业数字化转型、数字人才培养、人工智能技术等话题进行深入交流。</blockquote><p></p><p></p><p>此次来访嘉宾有网新集团董事长 赵建，网新控股常务副总 耿晖，网新科创高级副总裁 史豫，中网银新董事长 张灿洪，浙江大学计算机创新技术研究院常务副院长 刘贤斐，唐氏集团执行董事 朱光杰，赛智伯乐创始人 陈斌，赛智伯乐合伙人 袁智勇。极客邦科技创始人、CEO 霍太稳与高管团队接待来访嘉宾并出席座谈交流会。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a05da1fbfddb129aa9442680eb6a7e0c.png\" /></p><p></p><p>座谈会上霍太稳分享极客邦科技的发展历程和创新成果。极客邦科技自成立以来，以 KaaS（Knowledge-as-a- Service，知识服务化）理念和模式为指导，始终专注为数字管理人才、数字应用人才、数字专业人才提供丰富的知识服务，旗下产品包括，促进数字技术领域知识与创新的传播的媒体社区 InfoQ 极客传媒；引领中国数字化实践的 InfoQ 技术大会；科技领导者同侪学习社区 TGO 鲲鹏会；数字人才的移动知识库极客时间；以及企业级一站式数字技术学习 SaaS 平台极客时间企业版。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/9c/f9/9c01d42f226bab6a058560dba27457f9.jpeg\" /></p><p></p><p>近年来，随着极客邦科技所服务的个人与企业数量不断增加，公司愈发深刻感受到来自社会对“数字化”相关话题的前所未有的重视。因此极客邦科技不断向内生长，提升自身服务能力，为客户创造优秀产品，帮助客户解决数字化转型道路上的难点和痛点问题。</p><p></p><p>2022 年，极客邦科技成立双数研究院，专注于数字经济观察、数字化人才培养研究与体系设计、数字化转型企业架构方法论研究与微咨询、数字经济研究、生态建设相关研究与实践，以及数字化转型案例发掘与推广。双数研究院首倡“数字人才粮仓模型”，结合企业数字化转型路径，将共识的数字管理、数字应用、数字专业三类数字人才进一步细分为更加匹配企业架构的五层数字人才，即数字思维管理者、数字思维业务人才、业务架构人才、技术架构人才、专项技术人才，以期帮助各行业、企业在进行数字化转型的战略落地过程中构筑更加扎实的人才体系。</p><p></p><p>网新集团董事长 赵建对极客邦科技的业务模式予以肯定，并且对未来的发展充满信心。表示网新集团作为以浙江大学综合应用学科为依托的信息技术咨询和服务集团，行业数智化专家。集团以“数智引领”为战略导向，整合人工智能、区块链、云计算、大数据、物联网等技术赋能行业，并以智能云为基础打造数智化基座，支撑数字化政府、基建智能化和产业数智化三大领域应用，为政府、全球产业客户提供深入行业的数字化解决方案和业务全流程的智能化平台服务，推动产业数智化转型升级。</p><p></p><p>推动企业数字化转型服务， 为人才提供终身教育是双方企业共同的服务理念，网新集团与极客邦科技一起，在人工智能、软件开发、数字人才培养等领域，充分发挥各自优势，携手创新智能产品和打造一体化解决方案，以期加速中国企业数字化转型进程，推动数字中国建设。</p><p></p><p></p><h4>关于极客邦科技</h4><p></p><p>极客邦科技，以“推动数字人才全面发展，助力数字中国早日实现”为己任，致力于为数字人才提供全面的、高质量的资讯、课程、会议、培训、咨询等服务。极客邦科技的核心是独特的专家网络和优质内容生产体系，为企业、个人提供其成功所必需的技能和思想。</p><p></p><p>极客邦科技自 2007 年开展业务至今，已建设线上全球软件开发知识与创新社区 InfoQ，发起并成立技术领导者社区 TGO 鲲鹏会，连续多年举办业界知名技术峰会（如 QCon、ArchSummit 等），自主研发数字人才在线学习产品极客时间 App，以及企业级一站式数字技术学习 SaaS 平台极客时间企业版，在科技人群、科技驱动型企业、数字化产业当中具有广泛的影响力。</p><p></p><p>2022 年成立双数研究院，专注于数字经济观察与数字人才发展研究，原创发布了数字人才粮仓模型，以此核心整合极客邦科技专业的优质资源，通过 KaaS 模式助力数字人才系统化学习进阶，以及企业数字人才体系搭建。</p><p></p><p>公司业务遍布中国大陆主要城市、港澳台地区，以及美国硅谷等。十余年间已经为全球千万科技人才，数万家企业提供服务。</p>",
    "publish_time": "2023-03-27 17:58:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI：我们将 Kubernetes 扩展到了 7500 个节点",
    "url": "https://www.infoq.cn/article/g9tuoTODP20N1lTzjsjw",
    "summary": "<p></p><blockquote>在本文中，OpenAI 的工程师团队分享了他们在 Kubernetes 集群扩展过程中遇到的各种挑战和解决方案，以及他们取得的性能和效果。</blockquote><p></p><p>&nbsp;</p><p>我们已经将 Kubernetes 集群扩展到 7500 个节点，为大型模型（如 <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3</a>\"、 <a href=\"https://openai.com/blog/clip/\">CLIP</a>\" 和 <a href=\"https://openai.com/blog/dall-e/\">DALL·E</a>\"）创建了可扩展的基础设施，同时也为快速小规模迭代研究（如<a href=\"https://arxiv.org/abs/2001.08361\">神经语言模型的缩放定律</a>\"）创建了可扩展的基础设施。</p><p>&nbsp;</p><p>将单个 Kubernetes 集群扩展到这种规模很少见，但好处是能够提供一个简单的基础架构，使我们的机器学习研究团队能够更快地推进并扩展，而无需更改代码。</p><p>&nbsp;</p><p>自上次发布关于<a href=\"https://openai.com/blog/scaling-kubernetes-to-2500-nodes/\">扩展到 2500 个节点</a>\"的帖子以来，我们继续扩大基础设施以满足研究人员的需求，在此过程中学到了许多的经验教训。本文总结了这些经验教训，以便 Kubernetes 社区里的其他人也能从中受益，并最后会介绍下我们仍然面临的问题，我们也将继续解决这些问题。</p><p><img src=\"https://static001.geekbang.org/infoq/12/123a284d1a949b9bf486c24bcfae91a4.png\" /></p><p></p><p></p><h2>我们的工作负载</h2><p></p><p>&nbsp;</p><p>在深入探讨之前，我们着重描述一下我们的工作负载。我们在 Kubernetes 上运行的应用程序和硬件与大家在普通公司遇到的可能相当不同。因此，我们的问题及解决方案可能与你自己的设置匹配，也可能不匹配！</p><p>&nbsp;</p><p>一个大型的机器学习作业跨越许多节点，当它可以访问每个节点上的所有硬件资源时，运行效率最高。这允许 GPU 直接使用 <a href=\"https://www.nvidia.com/en-us/data-center/nvlink/\">NVLink</a>\" 进行交叉通信，或者 GPU 使用 <a href=\"https://developer.nvidia.com/gpudirect\">GPUDirect</a>\" 直接与 NIC 进行通信。因此，对于我们的许多工作负载，单个 Pod 占用整个节点。任何 NUMA、CPU 或 PCIE 资源争用都不是调度的因素。装箱或碎片化不是常见的问题。我们目前的集群具有完全的二分带宽，因此我们也不考虑机架或网络拓扑。所有这些都意味着，虽然我们有许多节点，但调度程序的负载相对较低。</p><p>&nbsp;</p><p>话虽如此，kube-scheduler 的负载是有波动的。一个新的作业可能由许多数百个 Pod 同时创建组成，然后返回到相对较低的流失率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d0baf33b2a39b8abfe6def08b01c6757.png\" /></p><p></p><p>&nbsp;我们最大的作业运行 MPI，作业中的所有 Pod 都参与一个单一的 MPI 通信器。如果任何一个参与的 Pod 挂掉，整个作业就会停止，需要重新启动。作业会定期进行检查点，当重新启动时，它会从上一个检查点恢复。因此，我们认为 Pod 是半有状态的——被删掉的 Pod 可以被替换并且工作可以继续，但这样做会造成干扰，应该尽量减少发生。</p><p>&nbsp;</p><p>我们并不太依赖 Kubernetes 的负载均衡。我们的 HTTPS 流量非常少，不需要进行 A/B 测试、蓝/绿或金丝雀部署。Pod 使用 SSH 直接通过 Pod IP 地址与 MPI 进行通信，而不是通过服务端点。服务“发现”是有限的；我们只在作业启动时进行一次查找，查找哪些 Pod 参与 MPI。</p><p>&nbsp;</p><p>大多数作业与某种形式的 Blob 存储进行交互。它们通常会直接从 Blob 存储流式传输一些数据集的分片或检查点，或将其缓存到快速的本地临时磁盘中。我们有一些 PersistentVolumes，用于那些需要 POSIX 语义的情况，但 Blob 存储更具可扩展性，而且不需要缓慢的分离/附加操作。</p><p>&nbsp;</p><p>最后，我们的工作性质本质上是研究，这意味着工作负载本身是不断变化的。虽然超级计算团队努力提供我们认为达到“生产”质量水平的计算基础架构，但在该集群上运行的应用程序寿命很短，它们的开发人员会快速迭代。因此，随时可能出现新的使用模式，这些模式会挑战我们对趋势和适当权衡的设定。我们需要一个可持续的系统，同时也能让我们在事情发生变化时快速做出响应。</p><p></p><h2>网络</h2><p></p><p>&nbsp;</p><p>随着集群内节点和 Pod 数量的增加，我们发现 Flannel 难以满足所需的吞吐量。因此，我们转而使用 Azure VMSS 的本地 Pod 网络技术和相关 CNI 插件来配置 IP。这使我们的 Pod 能够获得主机级别的网络吞吐量。</p><p>&nbsp;</p><p>我们转而使用别名 IP 地址的另一个原因是，在我们最大的集群中，可能会同时使用约 20 万个 IP 地址。在测试了基于路由的 Pod 网络后，我们发现能够使用的路由数明显存在限制。</p><p>&nbsp;</p><p>避免封装会增加底层 SDN 或路由引擎的需求，虽然这使我们的网络设置变得简单。添加 VPN 或隧道可以在不需要任何其他适配器的情况下完成。我们不需要担心由于某部分网络具有较低的 MTU 而导致的分组分段。网络策略和流量监控很简单；没有关于数据包源和目的地的歧义。</p><p>&nbsp;</p><p>我们使用主机上的 iptables 标记来跟踪每个 Namespace 和 Pod 的网络资源使用情况，这使研究人员可以可视化他们的网络使用模式。特别是，由于我们的许多实验具有不同的 Internet 和 Pod 内通信模式，因此能够调查任何瓶颈发生的位置通常是非常有意义的。</p><p>&nbsp;</p><p>可以使用 iptables mangle 规则任意标记符合特定条件的数据包。以下是我们用来检测流量是内部流量还是 Internet 流量的规则。FORWARD 规则涵盖了来自 Pod 的流量，而 INPUT 和 OUTPUT 规则涵盖了主机上的流量：</p><p>&nbsp;</p><p><code lang=\"null\">iptables -t mangle -A INPUT ! -s 10.0.0.0/8 -m comment --comment \"iptables-exporter openai traffic=internet-in\"\niptables -t mangle -A FORWARD ! -s 10.0.0.0/8 -m comment --comment \"iptables-exporter openai traffic=internet-in\"\niptables -t mangle -A OUTPUT ! -d 10.0.0.0/8 -m comment --comment \"iptables-exporter openai traffic=internet-out\"\niptables -t mangle -A FORWARD ! -d 10.0.0.0/8 -m comment --comment \"iptables-exporter openai traffic=internet-out\"</code></p><p>&nbsp;</p><p>一旦标记，iptables 将开始计数以跟踪匹配该规则的字节数和数据包数。你可以使用 iptables 本身来查看这些计数器：</p><p>&nbsp;</p><p><code lang=\"null\">% iptables -t mangle -L -v\nChain FORWARD (policy ACCEPT 50M packets, 334G bytes)\n pkts bytes target     prot opt in     out     source               destination\n....\n1253K  555M            all  --  any    any     anywhere            !10.0.0.0/8           /* iptables-exporter openai traffic=internet-out */\n1161K 7937M            all  --  any    any    !10.0.0.0/8           anywhere             /* iptables-exporter openai traffic=internet-in */</code></p><p>&nbsp;</p><p>我们使用名为 iptables-exporter 的开源 Prometheus 导出器将这些数据追踪到我们的监控系统中。这是一种简单的方法，可以跟踪与各种不同类型的条件匹配的数据包。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f5451aff83d390366762c660a7a401a9.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/7775d9d4c53093a3ef62b3ca33dd8846.png\" /></p><p></p><p>&nbsp;</p><p>我们网络模型中比较独特的一点是，我们完全向研究人员公开节点、Pod 和 Service 网络 CIDR 范围。我们采用集线器和分支的网络模型，并使用本机节点和 Pod CIDR 范围路由该流量。研究人员连接到中心枢纽，然后可以访问任何一个单独的集群（分支）。但是这些集群本身无法相互通信。这确保了集群保持隔离、没有跨集群依赖，可以防止故障隔离中的故障传播。</p><p>&nbsp;</p><p>我们使用一个“NAT”主机来翻译从集群外部传入的服务网络 CIDR 范围的流量。这种设置为我们的研究人员提供了很大的灵活性，他们可以选择各种不同类型的网络配置进行实验。</p><p></p><h2>API 服务器</h2><p></p><p>&nbsp;</p><p>Kubernetes 的 API Server 和 etcd 是保持集群健康运行的关键组件，因此我们特别关注这些系统的压力。我们使用 <a href=\"https://github.com/coreos/kube-prometheus\">kube-prometheus</a>\" 提供的 Grafana 仪表板以及额外的内部仪表板。我们发现，将 HTTP 状态码 429（请求太多）和 5xx（服务器错误）的速率作为高级信号警报是有用的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fcb95aa683b2e4024e7fd4c72bfafd5f.png\" /></p><p></p><p>&nbsp;虽然有些人在 kube 内部运行 API 服务器，但我们一直在集群外运行它们。etcd 和 API 服务器都在它们自己的专用节点上运行。我们的最大集群运行 5 个 API 服务器和 5 个 etcd 节点，以分散负载并尽可能减少发生故障后带来的影响。自从我们在<a href=\"https://openai.com/blog/scaling-kubernetes-to-2500-nodes/\">上一篇博文</a>\"中提到的将 Kubernetes 事件拆分到它们自己的 etcd 集群中以来，我们没有遇到 etcd 的任何值得注意的问题。API 服务器是无状态的，通常很容易在自我修复的实例组或扩展集中运行。我们尚未尝试构建任何自我修复 etcd 集群的自动化，因为发生事故非常罕见。</p><p>&nbsp;</p><p>API 服务器可能会占用相当多的内存，并且往往会与集群中的节点数量成线性比例。对于我们有 7500 个节点的集群，我们观察到每个 API 服务器使用高达 70GB 的堆内存，因此幸运地是，未来这应该仍然在硬件能力范围之内。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/76dc497cb75d77528e67aaeca188b16d.png\" /></p><p></p><p>&nbsp;API Servers 受到压力的主要来源之一就是对 Endpoints 的 WATCH。在整个集群中有一些服务，如“kubelet”和“node-exporter”，其中每个节点都是成员。当一个节点被添加或从集群中删除时，这个 WATCH 将被触发。通常，由于每个节点本身都通过 kube-proxy 监视 kubelet 服务，因此这些响应中所需的数量和带宽将是 2N2 以及极大的，有时会达到 1GB/s 或更高。Kubernetes 1.17 中推出的 <a href=\"https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/\">EndpointSlices</a>\" 大大降低了这种负载，减少达 1000 倍。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c2/c22f63beb5f1da62a2e02f9e2dd9cf84.png\" /></p><p></p><p>总的来说，我们会非常注意随着集群规模增大而增加的 API Server 请求。我们尽量避免任何 DaemonSets 与 API Server 交互。在需要每个节点监视更改的情况下，引入缓存服务（例如 <a href=\"https://docs.datadoghq.com/agent/cluster_agent/\">Datadog Cluster Agent</a>\"）作为中介，似乎是避免集群范围瓶颈的良好模式。</p><p>&nbsp;</p><p>随着集群的增长，我们对集群的实际自动伸缩越来越少。但当一次性自动扩展太多时，我们偶尔会遇到问题。当新节点加入集群时会生成大量请求，一次性添加数百个节点可能会超过 API 服务器容量的负荷。稍微平滑一下这个过程，即使只有几秒钟也有助于避免宕机。</p><p></p><h2>时间序列度量与 Prometheus 和 Grafana</h2><p></p><p>&nbsp;</p><p>我们使用 Prometheus 收集时间序列度量数据，并使用 Grafana 进行图形、仪表板和警报。我们从 <a href=\"https://github.com/coreos/kube-prometheus\">kube-prometheus</a>\" 部署开始收集了各种各样的度量数据，并使用了一些良好的仪表板进行可视化。随着节点数量的不断增加，我们开始遇到 Prometheus 收集的度量数据数量过多的问题。尽管 kube-prometheus 公开了许多有用的数据，但我们实际上并没有查看所有的度量数据，一些数据也过于细化，无法有效地进行收集、存储和查询。因此，我们使用 <a href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config\">Prometheus 规则</a>\"从被摄入的度量数据中“删掉”一些数据。</p><p>&nbsp;</p><p>有一段时间，我们遇到了 Prometheus 消耗越来越多的内存问题，最终导致容器崩溃并出现 Out-Of-Memory 错误（OOM）。即使为应用程序分配了大量的内存容量，这种情况似乎仍然会发生。更糟糕的是，它在崩溃时会花费很多时间在启动时回放预写日志文件，直到它再次可用。最终，我们<a href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config\">发现了这些 OOM 的来源</a>\"是 Grafana 和 Prometheus 之间的交互，其中 Grafana 使用 /api/v1/series API 查询 {le!=\"\"}（基本上是“给我所有的直方图度量”）。/api/v1/series 的实现在时间和空间上没有限制，对于具有大量结果的查询，这将不断消耗更多的内存和时间。即使请求者已经放弃并关闭了连接，它也会继续增长。对于我们来说，内存永远不够，而 Prometheus 最终会崩溃。因此，我们<a href=\"https://github.com/openai/prometheus/pull/1\">修补</a>\"了 Prometheus，将此 API 包含在上下文中以强制执行超时，从而完全解决了问题。</p><p>&nbsp;</p><p>虽然 Prometheus 崩溃的次数大大减少，但在我们需要重新启动它的时候，WAL 回放仍然是一个问题。通常需要多个小时来回放所有 WAL 日志，直到 Prometheus 开始收集新的度量数据并提供服务。在 <a href=\"https://www.robustperception.io/\">Robust Perception</a>\" 的帮助下，我们发现将 GOMAXPROCS=24 应用于服务器可以显著提高性能。在 WAL 回放期间，Prometheus 尝试使用所有核心，并且对于具有大量核心的服务器，争用会降低所有性能。</p><p>&nbsp;</p><p>我们正在探索新的选项来增加我们的监控能力，下面“<a href=\"https://openai.com/blog/scaling-kubernetes-to-7500-nodes/#unsolvedproblems\">未解决的问题</a>\"”部分将对此进行描述。</p><p>&nbsp;</p><p></p><h2>健康检查</h2><p></p><p>对于如此庞大的集群，我们当然依赖自动化来检测并从集群中移除行为不当的节点。随着时间的推移，我们建立了许多健康检查系统。</p><p>&nbsp;</p><p></p><h3>被动健康检查</h3><p></p><p>&nbsp;</p><p>某些健康检查是被动的，总是在所有节点上运行。这些检查监视基本的系统资源，例如网络可达性、坏盘或满盘，或者 GPU 错误。GPU 以许多不同的方式出现问题，但一个容易出现的常见问题是“不可纠正的 ECC 错误”。Nvidia 的数据中心 GPU 管理器（DCGM）工具使查询这个问题和许多其他“Xid”错误变得容易。我们跟踪这些错误的一种方式是通过 <a href=\"https://github.com/NVIDIA/gpu-monitoring-tools#dcgm-exporter\">dcgm-exporter</a>\" 将指标收集到我们的监控系统 Prometheus 中。这将出现为 DCGM_FI_DEV_XID_ERRORS 指标，并设置为最近发生的错误代码。此外，<a href=\"https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries\">NVML 设备查询 API</a>\" 公开了有关 GPU 的健康和操作的更详细信息。</p><p>&nbsp;</p><p>一旦检测到错误，它们通常可以通过重置 GPU 或系统来修复，但在某些情况下确实需要更换基础 GPU。</p><p>&nbsp;</p><p>另一种健康检查是跟踪来自上游云提供商的维护事件。每个主要的云提供商都公开了一种方式来了解当前 VM 是否需要进行会最终导致中断的、即将发生的维护事件。VM 可能需要重新启动以应用底层的超级管理程序补丁，或者将物理节点替换为其他硬件。</p><p>&nbsp;</p><p>这些被动健康检查在所有节点上不断运行。如果健康检查开始失败，节点将自动划分，因此不会在节点上安排新的 Pod。对于更严重的健康检查失败，我们还将尝试 Pod 驱逐，以要求当前运行的所有 Pod 立即退出。这仍然取决于 Pod 本身，可通过 Pod 故障预算进行配置来决定是否允许此驱逐发生。最终，无论是在所有 Pod 终止之后，还是在 7 天过去之后（我们的服务级别协议的一部分），我们都将强制终止 VM。</p><p></p><h3>活动 GPU 测试</h3><p></p><p>&nbsp;</p><p>不幸的是，并非所有 GPU 问题都会通过 DCGM 可见的错误代码表现出来。我们建立了自己的测试库，通过对 GPU 进行测试来捕捉其他问题，并确保硬件和驱动程序的行为符合预期。这些测试无法在后台运行 - 它们需要独占 GPU 运行数秒钟或数分钟。</p><p>&nbsp;</p><p>我们首先在节点启动时运行这些测试，使用我们称之为“预检（preflight）”的系统。所有节点都会附带一个“预检”污点和标签加入集群。这个污点会阻止普通 Pod 被调度到节点上。我们配置了一个 DaemonSet，在所有带有此标签的节点上运行预检测试 Pod。测试成功完成后，测试本身将删除污点和标签，然后该节点就可供一般使用。</p><p>&nbsp;</p><p>我们还定期在节点的生命周期中运行这些测试。我们将其作为 CronJob 运行，允许它着陆在集群中的任何可用节点上。哪些节点会被测试到可能有些随机和不受控制，但我们发现随着时间的推移，它提供了足够的覆盖率，并且最小化了协调或干扰。</p><p></p><h2>配额和资源使用</h2><p></p><p>&nbsp;</p><p>随着集群规模的扩大，研究人员开始发现他们难以获取分配给他们的全部容量。传统的作业调度系统有许多不同的功能，可以公平地在竞争团队之间运行工作，而 Kubernetes 没有这些功能。随着时间的推移，我们从这些作业调度系统中汲取灵感，并以 Kubernetes 原生的方式构建了几个功能。</p><p></p><h3>团队污点</h3><p></p><p>&nbsp;</p><p>我们在每个集群中都有一个服务，称为“team-resource-manager”，具有多个功能。它的数据源是一个 ConfigMap，为在给定集群中具有容量的所有研究团队指定了 （节点选择器、应用的团队标签、分配数量） 元组。它会将当前节点与这些元组进行对比，并使用 openai.com/team=teamname:NoSchedule 的污点对适当数量的节点进行标记。</p><p>&nbsp;</p><p>“team-resource-manager”还有一个入站的 webhook 服务，因此在提交每个作业时会根据提交者的团队成员身份应用相应的容忍度。使用污点使我们能够灵活地限制 Kubernetes Pod 调度程序，例如允许较低优先级的 Pod 具有 \"any\" 容忍度，这样团队可以借用彼此的容量，而无需进行大量协调。</p><p></p><h3>CPU 和 GPU &nbsp;balloons</h3><p></p><p>&nbsp;</p><p>除了使用集群自动缩放器动态扩展我们基于虚拟机的集群之外，我们还使用它来纠正（删除和重新添加）集群中的不健康成员。我们通过将集群的 \"最小值\" 设置为零、\"最大值\" 设置为可用容量来实现这一点。然而，如果 cluster-autoscaler 发现有空闲节点，它将尝试缩小到只需要的容量。由于多种原因（VM 启动延迟、预分配成本、上面提到的 API 服务器影响），这种空闲缩放并不理想。</p><p>&nbsp;</p><p>因此，我们为 CPU 和 GPU 主机都引入了“球形”部署。这个部署包含一个具有 \"最大值\" 数量的低优先级 Pod 副本集。这些 Pod 占用节点内的资源，因此自动缩放器不会将它们视为空闲。但由于它们是低优先级的，调度程序可以立即将它们驱逐出去，以腾出空间进行实际工作。（我们选择使用 Deployment 而不是 DaemonSet，以避免将 DaemonSet 视为节点上的空闲工作负载。）</p><p>&nbsp;</p><p>需要注意的是，我们使用 pod 反亲和性（anti-affinity）来确保 pod 在节点之间均匀分布。Kubernetes 调度器的早期版本存在一个 O(N^2) 的性能问题，与 pod 反亲和性有关。自 Kubernetes 1.18 版本以后，这个问题已经得到了纠正。</p><p></p><h3>Gang 调度</h3><p></p><p>&nbsp;</p><p>我们的实验通常涉及一个或多个 StatefulSets，每个 StatefulSet 操作不同部分的训练任务。对于优化器，研究人员需要在进行任何训练之前调度 StatefulSet 的所有成员（因为我们通常使用 MPI 在优化器成员之间协调，而 MPI 对组成员变化很敏感）。</p><p>&nbsp;</p><p>然而再默认情况下，Kubernetes 不一定会优先满足某个 StatefulSet 的所有请求。例如，如果两个实验都请求 100％的集群容量，那么 Kubernetes 可能只会调度给每个实验需要的一半 Pod，这会导致死锁，使两个实验都无法进行。</p><p>&nbsp;</p><p>我们尝试了一些需要自定义调度程序的方法，但遇到了一些与正常 Pod 调度方式冲突的边缘情况。 Kubernetes 1.18 引入了核心 Kubernetes 调度程序的插件体系结构，使本地添加此类功能变得更加容易。我们最近选择了 <a href=\"https://github.com/kubernetes/enhancements/pull/1463\">Coscheduling 插件</a>\"作为解决此问题的方法。</p><p></p><h2>未解决的问题</h2><p></p><p>&nbsp;</p><p>随着 Kubernetes 集群规模的扩大，我们仍有许多问题需要解决。其中一些问题包括：</p><p></p><h3>指标</h3><p></p><p>&nbsp;</p><p>在如今的规模下，Prometheus 内置的 TSDB 存储引擎很难压缩，并且每次重新启动时需要长时间回放 WAL（预写式日志）。查询还往往会导致“查询处理会加载过多样本”的错误。我们正在迁移到不同的、与 Prometheus 兼容的存储和查询引擎。大家可以期待下我们未来的博客文章，看看它的表现如何！</p><p>&nbsp;</p><p></p><h3>Pod 网络流量整形</h3><p></p><p>&nbsp;</p><p>随着集群规模的扩大，每个 Pod 的互联网带宽量被计算了出来。每个人的聚合互联网带宽需求变得非常大，我们的研究人员现在有能力会意外地对互联网上的其他位置施加重大资源压力，例如要下载的数据集和要安装的软件包。</p><p>&nbsp;</p><p></p><h2>结论</h2><p></p><p>&nbsp;</p><p>Kubernetes 是一个非常灵活的平台，可以满足我们的研究需求。它具有满足我们所面临的最苛刻工作负载的能力。尽管它仍有许多需要改进的地方，但 OpenAI 的超级计算团队将继续探索 Kubernetes 的可扩展性。</p><p>&nbsp;</p><p>作者简介：</p><p>&nbsp;</p><p>本文作者为 OpenAI 员工 Eric Sigler 和 Benjamin Chess。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://openai.com/research/scaling-kubernetes-to-7500-nodes</p>",
    "publish_time": "2023-03-27 18:09:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "寻找“数字化创变者”，极客邦科技&华为云联创营重磅发布MVP计划！",
    "url": "https://www.infoq.cn/article/W5cRzETsiVJpFEzu5CZK",
    "summary": "<p>产业数字化进程加快，对技术与产业融合提出更为迫切的要求，如何通过新的生产方式聚焦资源，塑造技术竞争力实现数字技术价值最大化，成就业务增长，并实现产业升级成为关键难题，时代正在呼唤“产业数字化的创变者”。</p><p></p><p>华为云联创营自2020年开营以来，以“为各行各业造就一批具有未来视野、商业前瞻、技术战略的企业领航者”为目标，目前在联创营平台上已经聚集了450+CXO，这些时代的先锋者在联创营这个平台上，探讨技术，探索商业模式，互相学习，驱动行业进步。</p><p></p><p>极客邦科技双数研究院是极客邦科技的核心研究机构，它以“数字经济观察和数字人才培养”为核心研究方向，以帮助更多企业更好地走上数字化转型之路。</p><p></p><p>这与华为云联创营的理念不谋而合。</p><p></p><p>3月25日，在华为云联创营CTO领航班第七期活动现场，华为云联创营与极客邦科技双数研究院</p><p>重磅发布华为云联创营MVP计划。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/cc/47/cc89c716c0dd81033924b12bfe2b8147.jpeg\" /></p><p>华为云中国区市场总监刘丽丽发布华为云联创营MVP计划</p><p></p><p>华为云联创营MVP最有价值专家是产业数字化的实践者，他们深耕行业，理解行业，直面产业数智化进程，并通过亲身实践引领行业跃迁。他们是创新技术的布道师，积极探索前沿技术，技术专业经验扎实，并且乐于分享，擅于布道，助力产业进步。他们是不断探索边界的实干家，不断突破边界，探索业务创新业态。</p><p></p><p>华为云中国区市场总监刘丽丽在发布仪式上表示，华为云联创营将通过MVP计划，携手企业CXO，从产业数字化的实际问题出发，促进更多行业解决方案的孵化，丰富行业技术生态。同时与MVP共建行业/场景标准，让数字化价值更普惠共享、更公平可及，驱动数字中国发展。</p><p><img src=\"https://static001.infoq.cn/resource/image/80/c0/80526aa7f57d1faf873506b646ayy0c0.png\" /></p><p></p><p>直面产业数智化升级，以创新的方式重塑生产力格局，撬动行业进步的杠杆！<a href=\"https://e-campaign.huawei.com/events2/UniversalForm/register/uued5574d9/phoneRegister.html?utm_campaign=&amp;utm_medium=%E5%AE%9A%E5%90%91%E9%82%80%E8%AF%B7&amp;utm_source=%E7%8E%B0%E5%9C%BA%E6%B3%A8%E5%86%8C&amp;utm_term=&amp;utm_content=\">点击即可报名参选</a>\"华为云联创营MVP，第一期入选名单将于2023年4月底进行公布，敬请期待。</p>",
    "publish_time": "2023-03-27 20:16:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]