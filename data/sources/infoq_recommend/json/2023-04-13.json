[
  {
    "title": "“绿菜单”2.0发布 数据中心液冷生态建设进入加速期",
    "url": "https://www.infoq.cn/article/3Akr3k35Qiy4lo9Xvot0",
    "summary": "<p>近年来，随着数字化社会对算力需求的不断增长，数据中心能耗与散热问题日益凸显。液冷作为更加高效、低能耗的制冷技术，逐渐成为了数据中心建设的热门选择。对此，产业上下游企业热情高涨，液冷产品与方案纷纷亮相，相关技术创新与探索也进行的得如火如荼。</p><p></p><p>而作为一种新兴技术，很多液冷产品的研发与生产处于独自摸索的初级阶段，不同品牌甚至同一品牌不同产品之间的标准都有天差地别，配件通用性也近于无，严重阻碍了液冷技术的普及，建设一个标准统一、技术兼容的生态体系已经成为产业共识。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b071c7414f2bea9f7a74fe620ee231de.png\" /></p><p></p><p>在4月12日举办的英特尔可持续发展高峰论坛的数据中心能效技术论坛上，众多产业上下游企业参与了论坛，并围绕数据中心能效这一主题，展出了各自产品与解决方案。特别在液冷领域，众企业通过介绍产品、解决方案、落地案例等最新成果，向与会观众和业界分享了在液冷产品研发与技术合作方面的经验与思考。</p><p></p><h2>持续创新升级“绿菜单”</h2><p></p><p></p><p>2022年初，英特尔基于对数据中心碳排放的理解，以及与生态合作伙伴多年的共同实践、积累，推出了绿色数据中心技术框架1.0，即业内俗称的“绿菜单”1.0。“绿菜单”采用四横三纵的矩阵型框架，为绿色数据中心建设提供了一系列整体解决方案和参考设计。将散热技术置于关键地位，旨在有效实现数据中心的“降耗增效”，将PUE维持在1.25以内。</p><p></p><p>“绿菜单”1.0发布后的一年中，英特尔积极参与构建生态系统、市场培育，与生态伙伴一起发布了冷板液冷行业标准、OCSP1.0通用服务器系统设计指南规范和参考设计等技术成果，并宣传可持续发展的成功案例扩大本土社区影响力，不断推动理论探索与积累实践。</p><p></p><p>经过一年的实践与持续创新，英特尔推出了绿色数据中心技术框架2.0。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4aeff44adf7c1d55ebd9aa05fe225d4.png\" /></p><p></p><p>注：绿色数据中心技术框架，即“绿菜单”是英特尔为行业提供的绿色发展指导框架。您可以通过“绿菜单”了解英特尔致力于及其生态合作伙伴已落地的技术产品及解决方案，以及相应方案为用户带来的商业和品牌价值。英特尔致力于将自身技术成果赋能行业，是您可信赖的顾问。如您有“双碳”需求，可以参考“绿菜单”中的方案与理念。如您有进一步的疑问和需求，请与英特尔联系<a href=\"https://www.intel.cn/content/www/cn/zh/now/data-centric/green-dc.html\">https://www.intel.cn/content/www/cn/zh/now/data-centric/green-dc.html</a>\"</p><p>&nbsp;</p><p>“绿菜单”2.0聚焦产品全生命周期碳足迹，以碳排放来源为抓手，将“绿菜单”结构划分为两大部分，分别为面向运营碳排放的“三纵四横”部分，以及针对隐含碳排放的附加部分。</p><p></p><p>这一划分来自于英特尔在数据中心节能降碳领域常年的实践与观测。据估算，以寿命4年的全天候工作数据中心服务器来看，其全生命周期碳排放中，显性的运营碳排放占比约为90%，来自原料、生产、运输环节的隐性碳排放占比约为10%。</p><p></p><p>由此，英特尔通过推动电源、液冷等产品技术创新，通过标准化、智能化等举措优化运维，来降低运营碳排放；通过改进制造流程、加强循环利用等制造手段，以及开发相应的平台和解决方案来减少服务器隐含碳排放。</p><p></p><p>据介绍，基于“绿菜单”2.0，京东云与英特尔联合创新的绿色数据中心理念解决方案可以达到开发成本与开发周期缩减为60%，物料复用率高达70%的成果。通过部署冷板式液冷系统和高效率电源模块，服务器部署密度可提升28.6%，供电效率提升3.67%，PUE值可降至1.1左右。</p><p></p><h2>紧扣根本打造“绿产品”</h2><p></p><p></p><p>可落地的产品与解决方案是实现“绿菜单”的基础保障。在本次论坛上，英特尔带来了诸多围绕节能减碳，以可持续发展为目标的“绿产品”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dcbedee721d7f729dfcc7e7fda52db6b.png\" /></p><p></p><p>作为全球最大的半导体芯片制造商，数据中心服务器芯片是英特尔的核心产品之一。本次论坛中，英特尔带来了其新一代“有史以来最可持续的”数据中心处理器——第四代英特尔®️&nbsp;至强®️&nbsp;可扩展处理器。</p><p></p><p>相比传统处理器，新一代第四代至强®️&nbsp;可扩展处理器由90%可再生能源制造，具有更高的环保性能。通过电源优化模式，可带来最多20%（140W）的能源节约。其内置加速器可带来平均2.9倍的能效提升，面向AI负载则可实现高达14倍的能效提升。新一代第四代至强还将适配液冷散热系统，为数据中心的整体可持续发展奠定基础。</p><p></p><p>数据中心供电是保障数据中心稳定运营的关键，也是改善数据中心能效的重要环节。英特尔基于供配电版块产品全生命周期减碳目标，研发了可持续发展的数据中心电源节能解决方案。方案采用HVDC高压直流技术，将整体能效提升至97%；氮化镓/碳化硅材料服务器电源，可以提升供电能效至96%以上；并通过推动服务器48V应用及主板电源汇流排方案，整体提升服务器能效。</p><p></p><p>英特尔可持续发展理念得到了数据中心供配电产业上下游企业的支持，如台达与长城的高能效电源系列产品、新华三电源汇流排解决方案、烽火超微电源汇流排解决方案等，均采用了英特尔数据中心电源节能解决方案。</p><p></p><p>在软件层面，英特尔带来了智慧节能技术整体解决方案，借助人工智能提升至强平台功耗效能。依托人工智能和英特尔服务器平台技术的节能减排方案，通过软件和AI模型进行预测和干预，提高数据中心的运行能效，实现根据业务SLA智能调频，峰谷预测控制多冗余主机休眠，并通过更多功耗控制抓手减少无效能耗。</p><p>英特尔智慧节能技术除面向新建数据中心外，对不适合做硬件改造的老旧数据中心也有良好的效果。可以基于软件层面的智能调节，帮助老旧数据中心实现能效的提升。</p><p>&nbsp;</p><p>此外，“绿菜单”2.0中还对模块化服务器设计、液冷产品设计、服务器管理等提出了可行性参考。通过软、硬件技术的创新与融合，英特尔全面降低了从数据中心到芯片级的产品碳足迹，以可持续性实践来推动社会、环境和经济的可持续发展。</p><p></p><h2>产业贯通，构建“绿生态”</h2><p></p><p></p><p>在产业方面，英特尔积极推动行业生态的建立，将自身的可持续创新成果赋能产业。一方面致力于冷板式液冷的标准形成，以及促进产业合作以降低液冷部署的成本；另一方面则加速浸没式液冷解决方案的落地，为迎接算力爆发做好准备。此外，英特尔积极构建开放通用服务器平台模块化生态，在降低成本的同时减少隐性碳排放。</p><p></p><p>英特尔携手业界伙伴发布了《服务器及存储设备用液冷装置技术规范第1部分：冷板》技术规范，以及《绿色数据中心创新实践——冷板液冷系统设计参考》白皮书等技术成果，以助推冷板式液冷生态发展。</p><p>在产业链上游领域，英特尔与生态伙伴携手推动可普适、可落地、可通用的标准和产品落地。其中，比赫与英维克是两家专注于液冷系统硬件产品的企业，通过与英特尔的合作，共同研发了漏液感知与干预系统，双方推出了包括从冷板组件、通用快接头到循环管路、CDU、冷却塔在内的冷板系统配件，完善了相关领域产品生态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d22faa4b05a438fe541d353ba51c82d.png\" /></p><p>&nbsp;超聚变FusionPoD整机柜服务器及内存液冷产品</p><p></p><p>液冷服务器则是整个生态最重要的组成部分。本次论坛上，包括超聚变、宁畅、阿里云、京东云等众多产商带来了自己的液冷服务器产品。其中，超聚变作为液冷服务器领域的先行者之一，在本次论坛带来了其冷板式液冷FusionPoD整机柜服务器及内存液冷产品，并向与会观众分享了超聚变如何深度进行生态合作，为用户打造高效、安全、稳定的数据中心服务器。</p><p></p><p>FusionPoD整机柜服务器将数据中心L1、L2层创新整合，采用100%全液冷免空调设计，实现了8倍高算力密度。通过智能化实现电、网、液三总线全盲插，0线缆维护，降低运维压力，大幅优化全生命周期TCO。</p><p>超聚变除了参与英特尔牵头发布的技术规范与白皮书外，在低成本材料替代方案、简化设计提高加工效率方面与积极参与产业链合作，不仅实现了冷板式液冷服务器产品的稳定、可靠，也做到了产品生产的可控、成本的降低，确保了产品的商业价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8cfe5a0434891fdae95ba2a25aaae58.png\" /></p><p>&nbsp;</p><p>此外现场另有数十家生态企业带来了包括冷板式液冷、浸没式液冷、内存液冷、SOC液冷等在内的众多产品与解决方案，向与会者展现其创新实力的同时，也彰显了液冷产业生态发展的迅速。</p><p>&nbsp;</p><p>英特尔相关负责人在演讲中表示，数据中心的高能效、高质量、可持续发展，需要产业的共同努力。英特尔将携手践行可持续发展的供应商、制造商和分销商共同减少碳足迹，持续推动开放式创新，在产品、平台、软件和服务设计中贯穿可持续发展理念，与合作伙伴共同践行可持续发展目标。</p><p></p><p>文章来源：中国IDC圈</p>",
    "publish_time": "2023-04-13 08:47:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "聚焦新产品，论剑新技术——2023 春季火山引擎 FORCE 原动力大会即将开启！",
    "url": "https://www.infoq.cn/article/cX662RHlbwD7SlE1qdGe",
    "summary": "<p></p><p><img src=\"https://static001.infoq.cn/resource/image/9d/f3/9dc2bc6ba56f797888e4b2828fc9c0f3.jpg\" /></p><p></p><p></p><p>&lt;点击观看&gt;</p><p></p><p></p><h2>情形如何？收到请回复</h2><p></p><p></p><p>相信很多热爱科幻小说、影视的朋友们对“宇宙闪烁”这个概念并不陌生。在相关作品中，这是地外文明刻意创造出来、颠覆人类基础科学认知的假象，意图让人类的发展陷入停滞甚至倒车。而在现实社会中，陷入“宇宙闪烁”的情景也屡见不鲜，特别在数字化飞速发展的今天，企业也会因为各种内外部原因，遇到瓶颈，无法获得持续、有效的增长。</p><p></p><p>那么，在新技术、新产业、新环境这些不断刷新又持续叠加的变量面前，企业应该如何应对变幻莫测的市场环境，实现快速破局？</p><p></p><p>在 4 月 18 日即将举办的「2023春季火山引擎FORCE原动力大会」上，<a href=\"https://www.infoq.cn/article/qdXFclAaRi1OYmHTMGcj\">火山引擎</a>\"聚焦新产品和新技术，以产品为核心、以客户为导向、以云端动力为主张，携手金融、医疗、汽车等领域的专家们共同分享前沿科技和创新成果。</p><p></p><p>在“动力·增长”篇章中，<a href=\"https://www.infoq.cn/article/Ue0E2ZXpr2BwaYxlQ0fL\">火山引擎</a>\"将围绕“敏捷迭代”、“数据驱动”、“体验创新”这三大增长要素，重磅发布火山引擎在云计算、大数据、人工智能等领域的最新技术和产品，帮助企业获取数字化转型原动力，为业务增长构建数字化基石。同时，来自平安银行、晶泰科技、毫末智行的嘉宾们，将结合场景化应用和落地实践，与火山引擎共同分享新趋势、解读新场景、讲述新实践，在不断“闪烁”的新时代找到方向和路径，实现品牌价值与业务的双重快速增长。</p><p></p><p>在“生态·共建”篇章中，火山引擎作为字节跳动旗下的云服务平台，也邀请到了来自抖音电商、字节跳动公益基金会的相关负责人，一同讲述火山引擎在生态建设、生态合作、企业社会责任等方面的成果。</p><p></p><p><a href=\"https://www.infoq.cn/article/SlKTDaYhsGGG6HFRpKGo\">云计算</a>\"对企业数字化转型的帮助日益深远。火山引擎将继续以创新为火种，积极探索新的数字化技术和业务模式，为企业数字化转型提供源源不断的动力，帮助企业突破发展瓶颈、实现持续增长。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/67/63/67c2a2422f62aa2d81394a1556a32463.png\" /></p><p>扫描上方海报二维码预约观看 ↑</p><p>2023 春季火山引擎 FORCE 原动力大会直播！</p><p></p><p></p><h2>收到。4 月 18 日见！</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b8/34/b89c0abe5143430ab585ac3be950c734.png\" /></p><p></p><p>PS：预约观看有惊喜，报名成功后火山引擎 AIGC 技术可为您生成具有漫画风格头像的专属邀请函，欢迎体验！</p>",
    "publish_time": "2023-04-13 09:24:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java近期新闻：JDK 21发布计划、Payara平台、JBang、JHipster、WildFly",
    "url": "https://www.infoq.cn/article/ecfagKNCSRckMTv7ozAI",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p></p><p>2023年<a href=\"https://openjdk.org/poll/gb/2023/\">理事会选举</a>\"<a href=\"https://openjdk.org/poll/gb/2023/results\">结果</a>\"显示，Red Hat开源Java技术主管<a href=\"https://www.linkedin.com/in/andrew-haley-3546112/\">Andrew Haley</a>\"和Oracle技术咨询人员<a href=\"https://www.linkedin.com/in/phil-race-bb3624/\">Phil Race</a>\"已当选为董事会成员，填补了两个<a href=\"https://openjdk.org/bylaws#at-large-members\">At-Large</a>\"成员席位。选举结果将于2023年4月1日生效，任期为一年。InfoQ后续将带来更详细的新闻报道。</p><p>&nbsp;</p><p>JEP 444（<a href=\"https://openjdk.org/jeps/444\">虚拟线程</a>\"）从JEP Draft 8303683状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007543.html\">提升</a>\"到Candidate状态，然后很快就会从JDK 21的Candidate状态提升到Proposed to Target状态。该JEP建议根据前两轮预览的反馈将这个特性确定下来：JEP 436，<a href=\"https://openjdk.org/jeps/436\">虚拟线程第二次预览</a>\"，在JDK 20中交付；JEP 425，<a href=\"https://openjdk.org/jeps/425\">虚拟线程预览版</a>\"，在JDK 19中交付。该特性为Java平台提供了轻量级的虚拟线程，可以极大地减少编写、维护和观察高吞吐量并发应用程序的工作量。与JEP 436相比，其最重要的变化是虚拟线程现在完全支持<a href=\"https://openjdk.org/jeps/8303683#Thread-local-variables\">线程局部变量</a>\"，并去掉了不使用这些变量的选项。要了解更多关于JEP 425的细节信息，可以阅读InfoQ的<a href=\"https://www.infoq.com/news/2022/05/virtual-threads-for-jdk19/\">新闻报道</a>\"以及观看Oracle Java平台组Java开发大使<a href=\"https://www.linkedin.com/in/jos%C3%A9-paumard-2458ba5/\">José Paumard</a>\"提供的<a href=\"https://inside.java/2022/06/08/jepcafe11/\">截屏视频</a>\"。评审预计将于2023年4月7日结束。</p><p>&nbsp;</p><p>JDK回归测试工具jtreg 7.2<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007541.html\">发布</a>\"，已经可以集成到JDK中。其最重要的新特性是能够<a href=\"https://bugs.openjdk.org/browse/CODETOOLS-7903373\">使用虚拟线程来运行测试</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/openjdk/jtreg/blob/master/CHANGELOG.md\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JDK 21</h4><p></p><p></p><p>JDK 21<a href=\"https://jdk.java.net/21/\">早期访问构建</a>\"<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B16\">Build 16</a>\"在上周发布，其中包括<a href=\"https://github.com/openjdk/jdk/compare/jdk-21%2B15...jdk-21%2B16\">Build 15的更新</a>\"，主要是修复了<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2021%20and%20%22resolved%20in%20build%22%20%3D%20b16%20order%20by%20component%2C%20subcomponent\">各种问题</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p>Oracle Java平台组首席架构师<a href=\"https://www.linkedin.com/in/markreinhold\">Mark Reinhold</a>\"正式<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007533.html\">提出</a>\"了JDK 21的发布计划：</p><p>Rampdown Phase One（从主干分叉）：2023年6月8日Rampdown Phase Two：2023年7月20日初始候选版本：2023年8月10日最终候选版本：2023年8月24日正式发布：2023年9月19日</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\"，我们鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告Bug。</p><p>&nbsp;</p><p></p><h4>GlassFish</h4><p></p><p></p><p>GlassFish 7.0.3<a href=\"https://twitter.com/OmniFishEE/status/1641442636623142914?cxt=HHwWhIDTsbPYycctAAAA\">发布</a>\"，带来了Bug修复、文档改进和依赖项升级，如：Mojarra 4.0.2、EclipseLink 4.0.1、Helidon Config 3.2.0和ASM 9.5。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/eclipse-ee4j/glassfish/releases/tag/7.0.3\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Spring Framework</h4><p></p><p></p><p><a href=\"https://spring.io/projects/spring-integration\">Spring Integration</a>\"团队<a href=\"https://spring.io/blog/2023/03/27/spring-integration-for-aws-3-0-0-m2-and-spring-cloud-stream-kinesis-binder-4\">宣布</a>\"，<a href=\"https://github.com/spring-projects/spring-integration-aws/blob/main/README.md\">Spring Integration Extension for Amazon Web Services（AWS</a>\"）（3.0.0-M2版本），以及<a href=\"https://github.com/spring-cloud/spring-cloud-stream-binder-aws-kinesis/blob/main/README.adoc\">Spring Cloud Stream Binder for AWS Kinesis</a>\"（4.0.0-M1版本）项目已经转移到<a href=\"https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/home.html\">AWS Java SDK</a>\"。这些里程碑版本带来了一些显著的变化，包括：AWS Java SDK 2.20.32，这是最新版本；依赖项升级到<a href=\"https://awspring.io/\">Spring Cloud AWS</a>\" 3.0.0，带来了新的SQS监听API；一个DynamoDbLockRegistry类，这是ExpirableLockRegistry和RenewableLockRegistry接口的一个实现，提供了适当的TTL支持；删除XML配置。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-cloud\">Spring Cloud</a>\" 2022.0.2（代号Kilburn）<a href=\"https://spring.io/blog/2023/03/30/spring-cloud-2022-0-2-is-available\">发布</a>\"，主要是升级了子项目，如：Spring Cloud Vault 4.0.1、Spring Cloud Kubernetes 3.0.2、Spring Cloud OpenFeign 4.0.2和Spring Cloud Config 4.0.2。不过，部分子项目的移除也导致了一些破坏性的变化：Spring Cloud CLI、Spring Cloud for Cloud Foundry和Spring Cloud Sleuth。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2022.0-Release-Notes\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-webflow\">Spring Web Flow</a>\" 3.0.0<a href=\"https://spring.io/blog/2023/03/30/spring-web-flow-3-0-rc1-released\">第一个候选版本</a>\"提供的新特性包括：Spring Faces迁移到Spring Framework 6、Jakarta EE和JSF 4；<a href=\"https://github.com/spring-projects/spring-webflow-samples\">JSF示例</a>\"升级到Jakarta EE。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/spring-projects/spring-webflow/releases/tag/v3.0.0-RC1\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Payara</h4><p></p><p></p><p>Payara<a href=\"https://blog.payara.fish/whats-new-in-the-march-2023-payara-platform-release\">发布</a>\"了<a href=\"https://www.payara.fish/\">Payara平台</a>\"2023年3月版，其中包括社区版6.2023.3、企业版5.49.0和正式发布的Payara企业版6.0。所有这些版本现在都支持Jakarta EE 10和MicroProfile 6.0。值得注意的是，目前有一个正在研究解决中的问题：在部署包含Java Record的应用程序时，服务器日志中会有不支持Record的警告。Payara团队会确保应用程序仍将按预期部署和运行。</p><p>&nbsp;</p><p>社区版6.2023.3提供了Bug修复、组件升级和改进，如：REST SSL Alias Extension for Payara 6升级；cacerts.jks 和keystore.jks证书升级到<a href=\"https://www.openssl.org/docs/man1.1.1/man1/pkcs12.html\">PKCS#12</a>\"；为HTTP网络监听器配置所有的<a href=\"https://web.dev/samesite-cookies-explained/\">SameSite</a>\" cookie属性。要了解关于这个版本的更多细节，请查看<a href=\"https://docs.payara.fish/community/docs/Release%20Notes/Release%20Notes%206.2023.3.html\">发布说明</a>\"。</p><p>&nbsp;</p><p>企业版5.49.0还提供了Bug修复、组件升级以及刚刚提到过的SameSite cookie改进。要了解关于这个版本的更多细节，请查看<a href=\"https://docs.payara.fish/enterprise/docs/5.49.0/Release%20Notes/Release%20Notes%205.49.0.html\">发布说明</a>\"。</p><p>&nbsp;</p><p>Payara团队还<a href=\"https://blog.payara.fish/vulnerability-affecting-server-environments-on-java-1.8-on-updates-lower-than-1.8u191\">发布</a>\"了<a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-28462\">CVE-2023-28462</a>\"漏洞。该漏洞会影响在版本低于1.8u191的JDK 8上运行的服务器环境。它使得远程攻击者能够通过不安全的对象请求代理（ORB）监听器，利用远程JNDI访问将恶意代码加载到面向公众的Payara Server安装中。建议开发人员安装1.8u191以上的JDK 8版本。</p><p>&nbsp;</p><p></p><h4>Quarkus</h4><p></p><p></p><p>在发布了6个Alpha版本和1个Beta版本之后，Java社区在上周发布了Quarkus 3.0.0的<a href=\"https://quarkus.io/blog/quarkus-3-0-0-cr1-released/\">第一个候选版本</a>\"，其新特性包括：引入/q/info端点，提供有关应用程序的信息；使用<a href=\"https://github.com/smallrye/smallrye-beanbag\">SmallRye BeanBag</a>\"初始化Maven RepositorySystem接口以兼容Maven 3.9；一种面向Quarkus CLI的新的插件机制。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.0.0.CR1\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Micronaut</h4><p></p><p></p><p>Micronaut基金会<a href=\"https://micronaut.io/2023/03/31/micronaut-framework-3-8-8-released/\">发布</a>\"了Micronaut Framework 3.8.8，带来了Bug修复和模块升级：<a href=\"https://micronaut-projects.github.io/micronaut-data/snapshot/guide/\">Micronaut</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-data/snapshot/guide/\">Data</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-views/snapshot/guide/\">Micronaut</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-views/snapshot/guide/\">Views</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-openapi/snapshot/guide/\">Micronaut OpenAPI</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-security/snapshot/guide/\">Micronaut</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-security/snapshot/guide/\">Security</a>\"和<a href=\"https://micronaut-projects.github.io/micronaut-maven-plugin/latest/\">Micronaut Maven</a>\"<a href=\"https://micronaut-projects.github.io/micronaut-maven-plugin/latest/\">Plugin</a>\"。它还将一个依赖项升级到了<a href=\"https://netty.io/news/2023/03/14/4-1-90-Final.html\">Netty 4.1.90</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/micronaut-projects/micronaut-core/releases/tag/v3.8.8\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>WildFly</h4><p></p><p></p><p>WildFly 28的第一个Beta版本提供了如下新特性：支持Micrometer，包括将Micrometer与<a href=\"https://github.com/eclipse/microprofile-fault-tolerance/blob/master/README.adoc\">MicroProfile Fault Tolerance</a>\"规范的实现集成；支持<a href=\"https://github.com/eclipse/microprofile-telemetry/blob/main/README.adoc\">MicroProfile Telemetry</a>\"和<a href=\"https://github.com/eclipse/microprofile-lra/blob/master/README.adoc\">MicroProfile Long Running Actions</a>\"（LRA）规范。此外，该版本还取消了对<a href=\"https://github.com/eclipse/microprofile-metrics/blob/master/README.adoc\">MicroProfile Metrics</a>\"和<a href=\"https://github.com/eclipse/microprofile-opentracing/blob/master/README.adoc\">MicroProfile OpenTracing</a>\"规范的支持。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.redhat.com/secure/ReleaseNote.jspa?projectId=12313721&amp;version=12395993\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Hibernate</h4><p></p><p></p><p>在发布了4个候选版之后，Hibernate ORM 6.2正式<a href=\"https://in.relation.to/2023/03/30/orm-62-final/\">发布</a>\"。它提供了以下支持：结构化SQL类型；Java Records；统一生成持久值；数据库分区；私有SQL类型；使用SQL MERGE命令来处理可选表的更新。</p><p>&nbsp;</p><p></p><h4>Apache软件基金会</h4><p></p><p></p><p>Object Computing公司首席软件工程师、<a href=\"http://www.asert.com.au/home\">ASERT</a>\"主管兼Apache Groovy副总裁<a href=\"https://www.linkedin.com/in/paulwilliamking/\">Paul King</a>\"<a href=\"https://twitter.com/paulk_asert/status/1641731443838812160?cxt=HHwWgIDQ6ZiDzcgtAAAA\">宣布</a>\"了Apache Groovy的3个点版本。由于开发团队将专注于Groovy 5.0的开发，所以3.0和2.0发布序列的点版本会比较少。</p><p>&nbsp;</p><p><a href=\"https://www.mail-archive.com/announce@apache.org/msg08065.html\">4.0.11版本</a>\"带来了Bug修复和一些新特性，包括：新方法asReversed()和reverseEach()，它们将分别映射到NavigableSet接口中定义的descentSet()和descentIterator()方法；依赖项升级到ASM 9.5；JDK 21新增的一个常量。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12353000\">更新日志</a>\"。</p><p>&nbsp;</p><p>3.0.17版本修复了Bug，改进了文档，并将依赖项升级到ASM 9.5。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12352999\">更新日志</a>\"。</p><p>&nbsp;</p><p>类似地，<a href=\"https://www.mail-archive.com/announce@apache.org/msg08061.html\">2.5.22版本</a>\"也是修复了Bug，改进了文档，并将依赖项升级到ASM 9.5。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12352774\">更新日志</a>\"。</p><p>&nbsp;</p><p>Apache Camel 3.20.3<a href=\"https://camel.apache.org/blog/2023/03/RELEASE-3.20.3/\">发布</a>\"，提供了Bug修复、依赖项升级和新特性/改进，包括：为具有连接验证扩展的组件添加健康检查（camel-health）；camel-jbang组件中的用户配置文件；在Camel <a href=\"https://camel.apache.org/manual/registry.html\">Registry API</a>\"中使用CompositeMeterRegistry类的实例。要了解关于这个版本的更多细节，请查看<a href=\"https://camel.apache.org/releases/release-3.20.3/\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://www.mail-archive.com/announce@apache.org/msg08066.html\">Apache James 3.7.4的发布</a>\"解决了CVE-2023-26269漏洞，即<a href=\"https://seclists.org/oss-sec/2023/q1/217\">通过未经身份验证的JMX实现特权升级</a>\"。该漏洞存在于Apache James Server 3.7.3及更早的版本中。这些版本默认提供了无需身份验证的JMX管理服务，使得攻击者可以获得特权升级。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/apache/james-project/blob/master/CHANGELOG.md#374---2023-03-20\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Eclipse Vert.x</h4><p></p><p></p><p>Eclipse Vert.x 4.4.1<a href=\"https://vertx.io/blog/eclipse-vert-x-4-4-1/\">发布</a>\"，带来了Bug修复和依赖项升级，包括GraphQL-Java 20.1、Netty 4.1.90、SnakeYAML 2.0、Micrometer 1.10.5和Apache Qpid Proton-J 0.34.1。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/vert-x3/wiki/wiki/4.4.1-Release-Notes\">发布说明</a>\"、<a href=\"https://github.com/vert-x3/wiki/wiki/4.4.1-Deprecations-and-breaking-changes\">弃用及破坏性更改</a>\"。</p><p>&nbsp;</p><p></p><h4>JHipster</h4><p></p><p></p><p>JHipster团队<a href=\"https://twitter.com/juliendubois/status/1642061363383091200?cxt=HHwWgIC-_YSH48ktAAAA\">发布</a>\"了<a href=\"https://github.com/jhipster/generator-jhipster-quarkus/blob/main/README.md\">JHipster Quarkus Blueprint</a>\"的2.0.0版本，其中有一些显著的变化，包括：修复生产配置文件的OIDC设置；将Blueprint依赖项和Quarkus的版本升级到2.16.2；修复Keycloak授权和Cypress测试；修复SQL Docker镜像。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/jhipster/generator-jhipster-quarkus/releases/tag/v2.0.0\">发布说明</a>\"。</p><p>&nbsp;</p><p>JHipster团队还<a href=\"https://twitter.com/pascalgrimaud/status/1640753446105088012?cxt=HHwWmIC88Z2kkMUtAAAA\">发布</a>\"了JHipster Lite 0.30.0，带来了Bug修复、依赖项升级和功能增强，包括：删除重复的JSON Web Token依赖；ApplicationAuthorizations类增加getUsername()方法；用Keycloak修复Angular OAuth2。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.30.0\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JBang</h4><p></p><p></p><p><a href=\"https://www.jbang.dev/\">JBang</a>\"的<a href=\"https://github.com/jbangdev/jbang/releases/tag/v0.106.0\">0.106.0</a>\"和<a href=\"https://github.com/jbangdev/jbang/releases/tag/v0.106.1\">0.106.1</a>\"版本在jbang init 命令中引入了<a href=\"https://openai.com/research/gpt-4\">GPT</a>\"支持，让它可以调用ChatGPT API来初始化并创建一个jbang&nbsp;脚本。该脚本会尝试执行你在命令行中提供的字符串表述。要了解关于这个新功能的更多细节，可以观看这段<a href=\"https://www.youtube.com/watch?v=4Ol3sMm8xDM\">YouTube视频</a>\"，InfoQ后续也将带来更详细的新闻报道。</p><p>&nbsp;</p><p></p><h4>Gradle</h4><p></p><p></p><p>Gradle 8.1的<a href=\"https://github.com/gradle/gradle/releases/tag/v8.1.0-RC2\">第2个候选版本</a>\"提供了以下新特性：对<a href=\"https://docs.gradle.org/8.1-rc-2/userguide/configuration_cache.html\">配置缓存</a>\"的持续改进；支持<a href=\"https://docs.gradle.org/8.1-rc-2/userguide/dependency_verification.html\">依赖关系验证</a>\"；改进Groovy闭包的错误报告；支持Java lambdas；支持使用JDK 20构建项目。要了解关于这个版本的更多细节，请查看<a href=\"https://docs.gradle.org/8.1-rc-2/release-notes.html\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Foojay.io</h4><p></p><p></p><p><a href=\"https://foojay.io/\">Foojay.io</a>\"是面向Java开发人员的Friends of OpenJDK资源。它提供了Java<a href=\"https://foojay.io/calendar/\">社区日历</a>\"，供开发人员查看和添加事件。日历是开放的，不需要专门的帐户就可以添加内容，而且内容是经过审核的。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/04/java-news-roundup-mar27-2023/\">https://www.infoq.com/news/2023/04/java-news-roundup-mar27-2023/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/R4gOIJ1ILgX1KF8vU4Y2\">甲骨文推出新的 Java SE 通用订阅</a>\"</p><p><a href=\"https://www.infoq.cn/article/LxsO27ZvvcHHRuS7J1VT\">Java 近期新闻：JDK 20 发布、Spring 多个版本发布、Quarkus、Helidon、Micronaut 和 Open Liberty</a>\"</p><p><a href=\"https://www.infoq.cn/article/JtADhFJ2VYuoDT4BqkeF\">WireMock Spring Boot 将简化 Spring Boot 应用的 WireMock 配置</a>\"</p>",
    "publish_time": "2023-04-13 09:47:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "InfoQ 2023年趋势报告：影响组织文化的两个最大的因素是大裁员和ChatGPT等大型语言模型",
    "url": "https://www.infoq.cn/article/fvb5Y9egEbd3BNvGRoBw",
    "summary": "<p></p><blockquote>关键要点大型科技公司的裁员对整个行业的心理安全感产生了负面影响；生成式AI工具（如Copilot或ChatGPT）提升了开发者生产力，但也存在重大缺陷；履责技术不仅仅是遵守监管规定，企业必须更有社会责任感才能吸引和留住客户和员工；异步工作方式正被越来越多的人接受，采用这种实践的组织正在从中获得实实在在的好处；混合式工作已成为常态，所以现在要仔细思考为什么要让人们面对面聚在一起工作，而不仅仅是规定一个固定的时间表。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df4674958e119d1c5e9476fcc60c822c.png\" /></p><p></p><p>&nbsp;</p><p>2023年初影响组织文化的两个最大的因素是科技行业破坏性极大的裁员和ChatGPT等大型语言模型的出现。</p><p>&nbsp;</p><p></p><h2>科技行业裁员破坏了人们的心理安全感</h2><p></p><p>&nbsp;</p><p>裁员的最大影响可能还不在于失业人数的上升，而在于出现了大量关于那些大型雇主在裁员时所采取的不人道的方式的报道。当人们无法登录公司账户，或者收到来自不明号码的短信，在没有其他通知方式的情况下告诉他们交出公司配置的设备，他们才发现自己失业了，这些裁员方式对整个行业产生了非常负面的影响，几乎所有高科技公司的员工心理安全感都在下降。</p><p>&nbsp;</p><p>一种现实的观点认为，裁员是对不断变化的经济气候所做出的反应，对于许多公司来说，裁员是在<a href=\"https://www.theverge.com/2023/1/26/23571659/tech-layoffs-facebook-google-amazon\">调整规模</a>\"，而不是缩小规模，然而，他们所采取的裁员方式导致了信任的丧失，降低了整个行业的员工敬业度。虽然重建文化很困难，但仍然可以通过谨慎和<a href=\"https://www.infoq.com/articles/rebuild-tech-culture-layoffs/\">深思熟虑的设计</a>\"来实现。</p><p>&nbsp;</p><p>尽管有大量的人被解雇，但世界各地的科技人才仍然严重短缺，大多数受影响的人都<a href=\"https://www.wsj.com/articles/laid-off-tech-workers-quickly-find-new-jobs-11672097730\">相对较快</a>\"地找到了新工作。</p><p>&nbsp;</p><p>Thoughtworks首席技术官Rebecca Parsons在与本报告对应的播客中说道：</p><p>&nbsp;</p><p></p><blockquote>拥有强大的员工价值主张是有充分商业理由的，因为人员流动和换人的成本都很高。如果你对员工好，你就知道为什么了，不过你也可以把它当作是一件对的事情来做。&nbsp;在裁员时对员工不好的雇主，在经济好转、希望再次出现时，将很难吸引到人才。</blockquote><p></p><p>&nbsp;</p><p></p><h2>大型语言模型——开发者的朋友还是敌人？</h2><p></p><p>&nbsp;</p><p>大型语言模型（如<a href=\"https://www.infoq.com/news/2022/12/openai-chatgpt/\">ChatGPT</a>\"）、高级服务<a href=\"https://www.infoq.com/news/2023/02/openai-chatgpt-plus/\">ChatGPT Plus</a>\"和增强的<a href=\"https://www.infoq.com/news/2023/02/github-enhanced-copilot-codex/\">GitHub Copilot</a>\"的推出，出现了大量关于AI可能取代人类工作的新闻，而程序员在一些被取代的工作岗位清单中<a href=\"https://www.businessinsider.com/chatgpt-jobs-at-risk-replacement-artificial-intelligence-ai-labor-trends-2023-02\">位居前列</a>\"。关于威胁的真实程度及其潜在的失业者将如何应对的问题，目前还不是很明朗，因为这些工具的发展还处于早期阶段。围绕<a href=\"https://www.infoq.com/news/2022/11/lawsuit-github-copilot/\">开源许可</a>\"和<a href=\"https://www.forbes.com/sites/lanceeliot/2023/02/26/legal-doomsday-for-generative-ai-chatgpt-if-caught-plagiarizing-or-infringing-warns-ai-ethics-and-ai-law/\">版权</a>\"（用于<a href=\"https://original.newsbreak.com/@brian-matthew-1594732/2930481049422-lawsuits-piling-against-chatgpt-maker-openai\">生成内容</a>\"的源和生成内容的<a href=\"https://www.reuters.com/legal/ai-created-images-lose-us-copyrights-test-new-technology-2023-02-22/\">版权</a>\"）而展开的法律官司正在形成当中。</p><p>&nbsp;</p><p>另一方面，有报道称使用这些工具可以显著提高开发者的生产力——可以处理许多<a href=\"https://dev.to/manthanbhatt/boost-your-productivity-as-a-developer-with-openais-chatgpt-1n5l\">普通的编码任务</a>\"，<a href=\"https://levelup.gitconnected.com/8-ways-chatgpt-helps-you-boost-your-programming-productivity-8f4999e3a624\">降低认知负担</a>\"，让有创造力的人将他们的精力应用到最能发挥创造力的开发领域。开发者工具中已经<a href=\"https://www.infoq.com/articles/ai-for-software-developers/\">包含了AI辅助</a>\"，而这方面的东西将会继续增加。</p><p>&nbsp;</p><p>现在已经很清楚的是，精心设计要问的问题成了一种技能，“<a href=\"https://www.computerworld.com/article/3691253/how-to-train-your-chatbot-through-prompt-engineering.html\">提示词（Prompt）工程</a>\"”最适合用来描述这种技能。如果你知道你想要的答案是什么，那么这些代码辅助工具将会发挥出最佳的效果。用户需要验证是否确实解决了所提出的问题。这些模型<a href=\"https://www.infoworld.com/article/3689172/chatgpt-and-software-development.html\">缺少上下文</a>\"，不知道自己不知道什么，所以使用它们存在潜在的风险。</p><p>&nbsp;</p><p>ChatGPT最近因为一个<a href=\"https://www.theverge.com/2023/3/21/23649806/chatgpt-chat-histories-bug-exposed-disabled-outage\">Bug</a>\"（用户会看到其他人的提示词）临时关闭过服务。这说明向大型语言模型提供商共享机密数据存在固有的<a href=\"https://www.quorumcyber.com/insights/how-to-protect-your-data-privacy-and-security-when-using-chatgpt/\">风险</a>\"。OpenAI的服务条款明确<a href=\"https://openai.com/policies/privacy-policy\">表示</a>\"，他们会收集“包含在输入”或“上传的文件”中的信息。由于提示词工程依赖于提供上下文信息，因此组织保护自己免受无意的数据隐私泄露的危害就变得十分重要。一些机构已经在实施<a href=\"https://www.businessinsider.com/walmart-warns-workers-dont-share-sensitive-information-chatgpt-generative-ai-2023-2\">监管治理</a>\"，摩根大通最近也在<a href=\"https://edition.cnn.com/2023/02/22/tech/jpmorgan-chatgpt-employees/index.html\">禁止</a>\"所有员工向ChatGPT提交任何数据。</p><p>&nbsp;</p><p>一些网络犯罪分子已经在使用ChatGPT<a href=\"https://www.infoq.com/news/2023/01/chatgpt-creating-malware/\">生成恶意软件</a>\"，作为技术人员，我们需要意识到潜在的影响，并积极地防范和降低风险。</p><p>&nbsp;</p><p>虽然与这两个因素相关的内容占据了新闻头条，但在其他领域也发生了很多影响组织文化和人员运营的事情。</p><p>&nbsp;</p><p></p><h2>履责技术的动向</h2><p></p><p>&nbsp;</p><p>技术从业者越来越意识到他们所做的工作对社会所产生的广泛影响，以及他们所构建的产品和构建产品的方式中涉及的<a href=\"https://www.infoq.com/ethics/\">道德考量</a>\"。在开发进程中，我们需要考虑对<a href=\"https://www.infoq.com/articles/fight-climate-change-software-engineer/\">气候</a>\"、社会公益、<a href=\"https://www.infoq.com/data_privacy/\">隐私</a>\"、安全、<a href=\"https://www.infoq.com/diversity/\">多样性</a>\"、公平和<a href=\"https://www.infoq.com/inclusion/\">包容</a>\"以及<a href=\"https://www.infoq.com/presentations/bias-ml-sao-paulo-2019/\">偏见</a>\"的影响。</p><p>&nbsp;</p><p><a href=\"https://www.technologyreview.com/2023/01/11/1066303/the-state-of-responsible-technology/\">履责技术</a>\"（Responsible Technology）不仅仅是遵守规章制度。履责意味着在开始构建产品之前就要考虑正在构建的产品可能带来的潜在影响，并提出一些棘手的问题，比如产品有可能按照何种非预期的方式被使用、有哪些社区和利益相关者没有被咨询和考虑到、如何积极预防损害而不仅仅是遵守最低的监管要求？</p><p>&nbsp;</p><p>公司声誉的提升，员工和客户希望与有道德的公司建立联系，客户愿意为有道德的产品支付更高的价格，这些都会带来一定的经济效益。履责技术不是凭空就有的——它需要经过<a href=\"https://www.pwc.co.uk/who-we-are/our-purpose/working-with-purpose/responsible-technology.html\">深思熟虑的实践</a>\"并制定明确的目标，涵盖供应链、运营、员工和客户等领域，此外还需要解决与产品和服务开发相关的问题。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.com/news/2023/02/sustainability-develop-operation/\">DevSusOps</a>\"不仅仅是碳中和——为了有效地可持续发展，组织需要关注所有<a href=\"https://www.infoq.com/articles/agile-manifesto-sustainability/\">利益相关者</a>\"的需求，包括社会、环境和经济层面的。</p><p>&nbsp;</p><p>软件工程的<a href=\"https://www.infoq.com/articles/ethical-decisions-wicked-world/\">道德标准</a>\"和指南正在不断演化，并出现在新的领域（如<a href=\"https://www.infoq.com/news/2021/02/draft-code-ethics-agile-coaches/\">敏捷教练</a>\"）。</p><p>&nbsp;</p><p></p><h2>“项目到产品”的想法越来越受欢迎</h2><p></p><p>&nbsp;</p><p>越来越多的组织不再把IT部门视为间接成本，而是把它们视为真正的合作伙伴——有效运行业务和满足客户需求所必需的增值服务，“<a href=\"https://www.infoq.com/podcasts/moving-projects-to-products/\">项目到产品</a>\"”（Project to Products）或“<a href=\"https://www.infoq.com/podcasts/continuous-digital-noprojects-OKRs/\">无项目</a>\"”（NoProjects）的想法因此越来越受欢迎。</p><p>&nbsp;</p><p>在端到端DevOps实践中，<a href=\"https://www.infoq.com/articles/value-stream-management/\">价值流管理</a>\"被更<a href=\"https://www.infoq.com/news/2022/10/state-of-vsm-report/\">频繁</a>\"地用于可视化和优化开发过程。</p><p>&nbsp;</p><p>当组织在采用这些实践时，基于项目的工作被分解了，<a href=\"https://www.infoq.com/product-management/\">产品管理</a>\"就变得很有必要。<a href=\"https://www.infoq.com/podcasts/evidence-based-management/\">循证管理</a>\"（Evidence Based Management）等方法提供了有助于最大化交付价值和专注业务结果的工具。</p><p>&nbsp;</p><p></p><h2>架构师的价值</h2><p></p><p>&nbsp;</p><p>开发团队和架构师之间频繁的冲突造成了负面影响，一些组织已经找到了有效协调潜在冲突的方法。开发团队的任务是尽可能快地交付功能增加价值并得到客户的认可。架构师则着眼于最大化IT产业的长期资产价值。协调这两个目标需要在沟通和参与方面做出一些转变。</p><p>&nbsp;</p><p>让架构成为开发过程的核心部分需要持续的协作和交流，不能只是通过演示和象牙塔式的发号施令，而需要通过能够彰显大愿景思维给产品带来短期和长期价值的代码实现和实际应用。</p><p>&nbsp;</p><p>组织通过使用跨团队评审<a href=\"https://blog.pragmaticengineer.com/rfcs-and-design-docs/\">设计文档或RFC</a>\"<a href=\"https://patterns.innersourcecommons.org/p/transparent-cross-team-decision-making-using-rfcs\">越来越多</a>\"地将架构功能分布到各个团队中。最近，Grygoriy Gonchar在InfoQ上发表了一篇关于使用架构决策框架的文章，其中就包括使用RFC，如架构决策记录（ADR）。Gonchar在文章中写道，这样的方法确保与“业务目标”保持一致，并帮助团队做出“明智”的架构决策。</p><p>&nbsp;</p><p></p><h2>适用于快速变化环境的团队拓扑</h2><p></p><p>&nbsp;</p><p>支持组织快速演化和远程/混合工作方式的<a href=\"https://www.infoq.com/presentations/team-topologies-technical-agility/\">团队拓扑</a>\"正在出现。<a href=\"https://www.infoq.com/articles/self-selection-reteaming-redgate/\">重建团队</a>\"是组织需要建立的一种能力——围绕新的挑战或机会迅速组建新的团队，并迅速进入高效和紧密合作的状态。</p><p>&nbsp;</p><p>理解<a href=\"https://www.infoq.com/presentations/ddd-team-topology/\">康威定律</a>\"，并采用<a href=\"https://www.infoq.com/news/2020/06/distributed-organizations/\">反康威定律</a>\"方法让组织能够有意识地调整沟通方式，使其变得更加高效。团队拓扑提供了一些模式来指导组织如何<a href=\"https://www.infoq.com/news/2021/03/team-topologies-during-pandemic/\">重组团队</a>\"，优化价值交付和最小化认知负担。Twitter最近的大规模裁员<a href=\"https://www.theverge.com/2023/3/6/23627875/twitter-outage-how-it-happened-engineer-api-shut-down\">告诉</a>\"我们，大规模裁员的负面影响可能是道德、<a href=\"https://www.businessinsider.com/layoff-survivors-describe-being-overworked-anxious-in-the-aftermath-2022-9\">可持续性</a>\"和稳定性的丧失，因为留存团队的认知负担会因此而增加。</p><p>&nbsp;</p><p>变化的步伐和就业的动态性质意味着在面临大规模的组织变革时，组织需要重新审视团队不断变化的认知负担。团队还必须善于<a href=\"https://www.infoq.com/podcasts/engineer-onboarding/\">培养新人</a>\"，帮助他们提高工作效率。</p><p>&nbsp;</p><p></p><h2>Staff Plus和技术职业生涯路径</h2><p></p><p>&nbsp;</p><p>“<a href=\"https://www.infoq.com/staff-plus/\">Staff Plus</a>\"”这个角色正受到越来越多的关注，人们也清楚地认识到，有一些职业路径并不需要被纳入到人事管理的岗位中。有意地进行领导力角色<a href=\"https://www.infoq.com/news/2022/11/engineer-manager-pendulum/\">轮换</a>\"是一个既能加深又能拓宽职业技能的可行办法。</p><p>&nbsp;</p><p></p><h2>混合工作模式将继续演化</h2><p></p><p>&nbsp;</p><p>从完全远程工作到混合式工作的转变仍在稳步进行中，一些公司要求员工完全回到办公室工作，结果导致了人员流失。随着工作方式的转变，远程团队对<a href=\"https://www.infoq.com/podcasts/remote-asynchronous-work-rob-rawson/\">异步工作</a>\"方式的接受程度大大提高，对更有效的书面沟通的需求也在增加，对如何沟通架构决策的过程和指南的需求也在增加。当越来越多的工作变成了异步方式，在自由和灵活性以及文档和流程之间找到适当的平衡就变得十分重要。</p><p>&nbsp;</p><p>随着组织为混合式工作重构工具，<a href=\"https://www.infoq.com/news/2022/06/office-design-hybrid/\">工作场所的设计</a>\"也在发生变化。我们看到一些办公室的设计减少了办公桌的数量，增加了团队协作所需要的空间，也为个人工作和需要安静对话和思考的工作提供了空间。</p><p>&nbsp;</p><p>在制定工作方式时需要考虑到员工的<a href=\"https://www.infoq.com/wellbeing/\">福利</a>\"和心理舒适度。了解需求的<a href=\"https://www.infoq.com/diversity/\">多样性</a>\"并确保包容和尊重可以实现更有效的协作。</p><p>&nbsp;</p><p>现如今可用的协作工具已经得到了显著的改善，但仍然存在差距和不足。由部分现场参与者和部分远程参与者组成的混合式会议通常会让人们对糟糕的音质和互动的缺失感到沮丧，这种情况在分布式团队中很容易出现。</p><p>&nbsp;</p><p></p><h2>InfoQ工程文化播客上有更多关于趋势的讨论</h2><p></p><p>&nbsp;</p><p>InfoQ文化与方法论编辑团队通过远程的方式讨论了这些趋势，我们将讨论内容录成了播客。你可以<a href=\"https://www.infoq.com/podcasts/culture-trends-2023/\">听听讨论的内容</a>\"，感受一下这些趋势背后的想法。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/culture-trends-2023/\">https://www.infoq.com/articles/culture-trends-2023/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/T4PEt15NU2JUk1VWhTMv\">避免成为“象牙塔”架构师：架构师和组织之间的关系</a>\"</p><p><a href=\"https://www.infoq.cn/article/QS7fCm6sciNqjwwk3Tu9\">裁员潮过去、削减中层管理潮又来了：升管理保饭碗，不灵了</a>\"</p><p><a href=\"https://www.infoq.cn/article/7Ps0qyHfQhp59g7YrEvZ\">当你的技术栈不能满足每个人需求时，下一步是什么呢？</a>\"</p><p><a href=\"https://www.infoq.cn/article/rUdJbb6hGtxw2hTJcerX\">“开发和运维”DevOps 只是一个开始，最终目标是质量工程</a>\"</p>",
    "publish_time": "2023-04-13 09:56:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "网易伏羲张荣升：数据闭环和人机协作是提升AIGC效果的重要方向已读",
    "url": "https://www.infoq.cn/article/dgCKRuC1owW8A1WPXUFs",
    "summary": "<p></p><blockquote>网易伏羲文本大模型“玉言”的开发遇到了哪些挑战？是如何解决的？网易伏羲多模态预训练的应用探索有哪些？对于想要入局AIGC的人有什么注意事项？为解答这些问题，InfoQ 采访了网易伏羲NLP 研究组负责人张荣升~</blockquote><p></p><p>&nbsp;</p><p>嘉宾介绍：张荣升，网易伏羲 NLP 研究组负责人。在 ICLR、ACL、EMNLP、NAACL、AAAI 等国际顶级会议上发表了近二十篇论文，研究方向涉及预训练模型、多模态、文本生成、对话生成等；其有着丰富的大模型技术研究和应用经验，其中文本大模型“玉言”系列曾登顶中文语言理解测评基准&nbsp; FewCLUE 小样本榜单和 CLUE 分类榜单，并应用在歌词辅助创作、智能对话、剧情生成等文本落地场景，另外多模态理解和生成大模型的研究也在互联网搜索推荐及众多AI绘画场景中应用落地。</p><p>&nbsp;</p><p>InfoQ：张老师，文本大模型“玉言”的起源是什么？在做的过程中遇到了哪些挑战与问题？这些问题最后都是如何解决的？有哪些事情是可以吸取经验教训？关于“玉言”模型的应用场景，您认为最突出的是哪些？</p><p>&nbsp;</p><p>张荣升：大模型起源：伏羲从 2018 年底就开始训练自己的文本预训练模型，并且经过4年多时间的积累，训练模型的参数从1亿增长到百亿甚至千亿的规模，在这个过程中我们积累了很多预训练技术算法和工程方面的经验，并且我们的《超大规模预训练云平台》也申请到了浙江省科技计划“尖兵”项目，有了浙江省政府的支持，我们也开发出了“玉言”系列的文本大模型。</p><p>&nbsp;</p><p>我们遇到的挑战及如何解决：大模型训练的算力要求和实际落地时推理成本是两个主要的问题。</p><p>&nbsp;</p><p>针对大模型训练，除了构建自己的预训练集群，我们也会注重不同模型参数之间的复用，例如我们一开始训练的文本大模型是GPT自回归的Transformer架构，其在语言理解能力上相比Bert这种双向的Transformer架构要弱一些，在打榜FewCLUE任务时，我们基于50亿参数的GPT模型初始化，并使用PrefixLM的方式进行二次训练，以较低的训练成本让模型更好的适应理解任务。</p><p>&nbsp;</p><p>另外在大模型应用推理时，伏羲自研的EET推理框架（<a href=\"https://github.com/NetEase-FuXi/EET\">https://github.com/NetEase-FuXi/EET</a>\"）能大幅降低Transformer结构模型的显存占用并提升推理速度1~8倍。</p><p>&nbsp;</p><p>这些问题在大家训练大模型的过程中是比较常见的，充分的利用现有模型的能力可以帮助我们节省算力和时间，同时也欢迎大家来使用我们开源的EET推理框架。</p><p>&nbsp;</p><p>关于“玉言”模型的应用场景：“玉言”模型重要的应用场景是剧情生成、对话生成等文本生成式场景，并且为了提升这方面能力，我们在预训练的过程中用了与这些应用场景更契合的大量高质量小说数据。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：文图生成方面，例如AI绘画，如何使得其产生的内容可控？例如准确生成你要的内容？在和我们设计总监沟通的过程中，我们发现，当给一些关键词，他无法生成我们想要的图片，例如我们专题需要宣传，但是给他输入关键词，他不按照这个方向设计，直接给你一张图，无法商用。这个如何实现呢？不知道未来距离商用，或者说替代设计师方面还有多远的距离？</p><p>&nbsp;</p><p>张荣升：要想使得内容可控，我认为有两方面：</p><p>首先是风格的可控性：如果用户有自有项目的绘画风格，那最好是能够用一些已有的项目图片在基础模型上进行微调来增强生成的可控性；</p><p>&nbsp;</p><p>其次，引入额外的条件输入：除了输入关键词之外，一方面用户可以输入参考图，并通过模型加噪的强度来控制生成的图片与参考图的相似性；另一方面可以借助最近很火的 ControlNet网络能力，支持线稿、深度图、人体姿势等方面的控制维度，或者用户也可以根据自己想要控制的属性训练模型，从而保证生成图片的可控性。</p><p>&nbsp;</p><p>至于你谈到的替代设计师这一问题：我觉得已经具备商用的能力了，但不是等于AI取代设计师，我认为设计师有AI难以取代的独特审美和专业能力，学会合理的利用AI工具，通过人机协作提升工作效率才是未来的方向。</p><p>&nbsp;</p><p>InfoQ：您谈到了多模态预训练，在 AIGC 领域的应用探索有哪些？</p><p>&nbsp;</p><p>张荣升：AIGC 领域的应用探索主要有两方面：</p><p>&nbsp;</p><p>1. 首先我们在开发专业的美术设计辅助工具，我们知道美术成本占游戏开发的很大一部分，常见的游戏美术设计包括场景原画，角色原画，图标制作等，因此为了降本增效，我们基于多模态预训练能力构建了“丹青约”绘画平台，旨在为美术同学提供专业的辅助创作工具，我们有较好的中文领域理解能力，并支持用户自定义风格的模型训练。</p><p>&nbsp;</p><p>2. 另外，我们也将多模态能力应用到多种场景的玩法上，例如在永劫无间游戏的时装营销中用户可以生成自己的服装、进行云音乐头像的二次元风格迁移、沉浸式会议系统网易瑶台中上线动态时装等功能。</p><p>&nbsp;</p><p>InfoQ：AIGC现在各家产品多样化，关于AIGC工具的商业化，您有什么样的思考？</p><p>&nbsp;</p><p>张荣升：我认为未来基座模型会掌握在有实力训大模型的大公司或者研究机构手上，而AIGC公司的商业化更多的机会在细分垂类场景，例如设计、教育、虚拟陪伴等场景的AIGC专业辅助工具或者产品应用。</p><p>&nbsp;</p><p>InfoQ：您如何看待 AIGC 技术对于社会和人类的影响？</p><p>&nbsp;</p><p>张荣升：AIGC技术对于社会和人类是一种新的生产方式，并且恰当地使用有助于提升各行各业的生产效率。例如在美术辅助设计领域，据我们美术同学反馈AI绘画可以极大节省和游戏策划的沟通成本，并且在美术设计的多个环节提升效率，如图标设计、角色原画等场景甚至可以节省50%以上的时间；再比如音乐创作包含编曲、作曲、作词等环节，其中的每一步都可以引入AI的协助，不管是给音乐人提供灵感，还是基于AI开发一些有趣的音乐玩法，都是值得期待和探索的。</p><p>&nbsp;</p><p>InfoQ：您有哪些建议给予想要从事 AIGC 研究的人？</p><p>&nbsp;</p><p>张荣升：（1）注重数据和人机协作的重要性：从GPT-3到ChatGPT，GPT-4的发展可以看出，模型参数的扩张已经不是深度学习最重要的方面了，更多的在于人机协作积累高质量数据以及AI和人类理解的Alignement，这也是提升AIGC效果的重要方向，值得大家重视。</p><p>&nbsp;</p><p>目前我们伏羲的思路正是围绕数据闭环和人机协作进行探索，以多智能体强化学习为理论基础，面向智能体编程（AOP）为实现方式，构建了一套实时人机协作的有灵众包平台。让机器在帮助人类与环境交互完成任务的过程中，即模仿学习人类的认知或决策能力，又能学习出一套仿真的反馈评价机制，从而在孪生环境中更高效地进化学习。</p><p>&nbsp;</p><p>（2）AIGC引入了更多的创新机会：近期大模型技术的发展，使 AI生成的内容（文本、图像、3D、视频、音乐等）质量能达到工业应用的水平，对各行各业结合AIGC进行创新带来了新的机会，不管是专业的辅助工具，还是开发新的应用场景都是值得大家探索的。</p><p>&nbsp;</p><p>InfoQ：简单介绍一下现在伏羲的研究和应用方向吗？</p><p>&nbsp;</p><p>张荣升：网易伏羲成立于 2017 年，是网易游戏旗下专门从事泛娱乐人工智能研究和转化应用的部门人工智能研究方面，网易伏羲的研究方向主要包括强化学习、图像 动作、虚拟人、自然语言、用户画像、大数据和云计算平台等，已拥有数字虚拟人、智能捏脸、智能创作、智能反外挂、智能对战匹配、智能竞技机器人等多项行业领先技术。</p><p>&nbsp;</p><p>目前，网易伏羲已在世界顶级学术会议发表论文 160 余篇，申请发明专利 400 余项。在为网易众多产品提供高质量人工智能技术的同时，为了让人工智能技术惠及更多领域，网易伏羲积极推动前沿人工智能技术在线下实体产业的转化应用。</p><p>&nbsp;</p><p>网易伏羲开发了专注于面 向虚实世界的实时⼈机协作在线任务平台，通过研发数字孪生引擎工具和平台、智能体核心能力模型，制定相关标准，构建行业生态等，为机器智能提供更多训练场景及数据，为智能制造等行业任务解决人力问题，为人们提供更多便捷有趣的工作机会。</p><p>&nbsp;</p><p>&nbsp;</p><p>活动推荐：</p><p>&nbsp;</p><p>在4月21-22日，InfoQ即将在上海举办一场ArchSummit，内容涵盖人工智能前沿技术、AIGC应用探索、金融业数字化转型探索、架构标准化和质量评估、大数据+架构、DataOps落地实践、ToB软件质量保障、制造业数字化转型架构创新、架构师成长、以及企业架构演进、数字化转型下的应用现代化、架构稳定性保障等专题。可扫码下方海报了解更多...</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0e/e3/0ec0e6b9480f72d35f1495fe39ed74e3.jpg\" /></p><p></p>",
    "publish_time": "2023-04-13 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "CUDA编程基础与Triton模型部署实践",
    "url": "https://www.infoq.cn/article/12d4daeaac4bd60d4590120a7",
    "summary": "<p>作者：王辉   阿里智能互联工程技术团队</p><p></p><p>近年来人工智能发展迅速，模型参数量随着模型功能的增长而快速增加，对模型推理的计算性能提出了更高的要求，GPU作为一种可以执行高度并行任务的处理器，非常适用于神经网络的推理计算，因此近年来在人工智能领域得到广泛关注与应用。</p><p></p><p>本文将结合我在阿里智能互联云端模型推理部署方面半年以来的工作学习，对相关的GPU编程与云端模型部署的知识与经验进行总结分享，下文内容为个人学习总结，如有疏漏与错误，还请各位不吝赐教。如有同样对云端模型推理部署、GPU计算优化、大模型推理部署相关技术感兴趣的同学，非常高兴能够进行相互的交流学习。</p><p></p><p></p><h1>一、CUDA编程基础</h1><p></p><p></p><p>CUDA是一种通用的并行计算平台和编程模型，它可以让用户在NVIDIA的GPU上更好地进行并行计算以解决复杂的计算密集型问题。本章将主要介绍GPU的相关基本知识、编程基础以及相关的部署要点。</p><p></p><p></p><h2>1.1 NVIDIA GPU系列与硬件结构简介</h2><p></p><p></p><p>NVIDIA的GPU产品主要是根据其应用场景进行划分的，主流的产品主要分为三个系列，分别是用于消费级显示与游戏的GeForce系列显卡（如我们熟悉的RTX 1080、RTX 3090等），用于专业图形化工作站的Quadro系列以及用于数据中心和高性能计算的Tesla系列（如T4、V100等），除此上述三种主流的系列之外，还有用于嵌入式设备的Jetson系列的显卡。</p><p></p><p>用于企业进行高性能计算的Tesla系列显卡根据其所采用的架构的计算能力由低到高分为Fermi、Kepler、Maxwell、Pascal、Volta、Turing、Ampere以及Hopper。</p><p></p><p>注：本节图片均来自于nvidia官方网站</p><p>https://images.nvidia.cn/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#cid=_pa-srch-baid_zh-cn</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fd62cc4dfdd4ddc498b6fe8f66d2797.png\" /></p><p>A100硬件架构图</p><p></p><p>从硬件架构上看，GPU拥有更多的简单计算资源，较少的逻辑控制资源并且单个SM的缓存资源也较小；而CPU片上拥有较少的复杂计算资源，同时拥有较多的逻辑控制资源以及较大的缓存。GPU的计算资源是以SM（Streaming Multiprocessor）划分的，以A100为例，该款GPU拥有108个SM，单个SM中有4个子块，1个线程束调度器，每个子块中拥有16个INT32和FP32核心、8个FP64核心、1个TensorCore、1个SFU（特殊函数单元）以及8个LD/ST（数据加载存储单元）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/ccc80cf4226bd64b87d5acc2e2055621.png\" /></p><p>A100单SM示意图</p><p></p><p>在存储空间方面，GPU片上的内存分为全局内存、常量内存、纹理内存、共享内存、L0/L1/L2缓存以及寄存器。全局内存、常量内存与纹理内存对全局可见，L2缓存对部分SM共有，L1缓存和共享内存对单个SM可见，而L0缓存和寄存器只对单个子块可见，有些GPU的SM中没有子块，即L0缓存和寄存器也对SM可见。</p><p></p><p>我们为什么要用GPU计算，特别是用于人工智能推理呢？我看过一个很形象的说法，CPU是一个大学数学教授，而GPU的一个核心是一个小学生，当面对一道困难的微积分题目和一万道100以内的加减法时，该怎么做才能既得到正确结果又最短时间获得结果呢？面对复杂的微积分问题（复杂运算与逻辑控制），当然让大学教授（CPU）去解决，而面对简单却数量庞大的问题（计算量很大的加减乘除问题），让很多的小学生（GPU）一起去做才能在最短时间内获得答案。因此如何根据CPU和GPU的特性，去分配不同的计算任务在不同的硬件上执行与调度，以获得最佳的计算性能和资源利用，这是异构计算最主要的目标之一，本章下文将主要针对如何利用CUDA在GPU上编程来进行一些我自己的学习总结与经验分享。</p><p></p><p></p><h2>1.2 CUDA编程模型</h2><p></p><p></p><p>我们都知道线程是CPU调度的基本单位，而GPU上计算资源是如何调度呢？在CUDA中，线程调度是按照线程束（Warp）去调度的，每个线程束含有32个线程，若干个线程束构成线程块，若干线程块组成一个网格。为便于陈述，通常约定CPU操作为主机端（Host），而GPU操作为设备端（Device）。</p><p></p><p>当主机端发起一个CUDA kernel时，便在设备端启动一个网格（Grid），一个网格中的不同线程块分布在不同的SM中，但是相同线程块的所有线程束一定在同一个SM中被执行，一个SM可能有多个线程块。可以这样理解硬件与软件概念之间的关系：GPU-网格，SM-线程块，CUDA core-线程。</p><p></p><p>值得注意的是：</p><p></p><p>线程束在被调度的时候一定是以32为整体进行调度，因此当我们启动60个线程时，在网格内实际存在64个活跃的线程。在大部分型号的GPU上，一个线程块中最多存在1024个线程，而线程块的数量限制一般很大，可以通过cudaGetDeviceProperties进行查询</p><p></p><p><code lang=\"text\">int dev = 0;\ncudaSetDevice(dev);\ncudaDeviceProp deviceProp;\ncudaGetDeviceProperties(&amp;deviceProp, dev);\nprintf(\"Maximum number of threads per multiprocessor:  %d\\n\", deviceProp.maxThreadsPerMultiProcessor);\nprintf(\"Maximum number of threads per block:           %d\\n\", deviceProp.maxThreadsPerBlock);\nprintf(\"Maximum sizes of each dimension of a block:    %d x %d x %d\\n\",\n    deviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1], deviceProp.maxThreadsDim[2]);\nprintf(\"Maximum sizes of each dimension of a grid:     %d x %d x %d\\n\",\n    deviceProp.maxGridSize[0], deviceProp.maxGridSize[1], deviceProp.maxGridSize[2]);</code></p><p></p><p>如果一个SM中存在多个线程束调度器，则同时一个SM中会有多个线程束被调度。</p><p></p><p>通常一个GPU并行计算的完整简单流程为：</p><p></p><p>在主机端申请适当大小的显存用于存放输入数据以及接收输出，将主机端的数据拷贝到设备端；在设备端进行计算，得到结果；将输出从设备端拷贝回主机端，销毁显存。</p><p></p><p>首先介绍第一步和第三步的相关API，相关函数和C语言的相关内存操作非常相似，有不同的是对于设备端之间的内存拷贝，存在异步操作，相关介绍如下：</p><p></p><p><code lang=\"text\">//在设备端申请显存\ncudaMalloc(void** ptr, size_t size);\n​\n//设置显存的值\ncudaMemset(void* ptr, int value, size_t size);\n​\n//内存拷贝,阻塞\n//有cudaMemcpyHostToHost、cudaMemcpyHostToDevice、cudaMemcpyDeviceToHost、cudaMemcpyDeviceToDevice\ncudaMemcpy(void* dst_ptr, void* src_ptr, size_t count, cudaMemcpyKind kind);\n​\n//异步拷贝,不阻塞\ncudaMemcpyAsync(void* dst_ptr, void* src_ptr, size_t count, cudaMemcpyKind kind, stream_t stream);\n​\n//内存释放\ncudaFree(void* ptr);</code></p><p></p><p>注意：所有的长度都要乘以数据类型的实际字节数。</p><p></p><p>接下来介绍第二步，启动核函数，通常所有在GPU执行的核函数都要编写在.cu文件中，通过对外提供C接口被调用。而核函数之前通常要添加函数类型限定符__global__，device，host，分别代表该核函数可以从全局被调用在设备端执行、只能从设备端被调用只设备端执行、只能从主机端被调用只能在主机端执行（通常省略）。而__device__，__host__可以一齐使用代表该核函数即可同时在主机端和设备端被编译。</p><p></p><p>值得注意的是：</p><p></p><p>所有核函数返回值必须是void不支持静态变量只能访问设备空间不支持可变数量参数</p><p></p><p>在启动核函数时，我们需要在函数调用名字之后添加三个尖括号&lt;&lt;&lt;&gt;&gt;&gt;，需指定启动网格大小（多少个线程块）和线程块（多少个线程）大小，第三个参数为动态分配的共享存储器大小，通常为0，第四个参数为stream，在同步操作时也默认为0。而网格和线程块的大小，也需要通常需要根据下式或者别的方式计算得到。</p><p></p><p><code lang=\"text\">#include \n#include \n​\n__global__ void helloworld_kernel()\n{\n    printf(\"hello world!\\n\");\n}\n​\nvoid helloworld(int len)\n{\n    int ilen = 64;\n    dim3 block(ilen);\n    dim3 grid((len+block.x-1)/block.x);\n    helloworld_kernel&lt;&lt;&gt;&gt;();\n}\n​\nint main()\n{\n    int thread = 100\n    helloworld(thread);\n    return 0;\n}</code></p><p></p><p>我们编写的核函数，在设备端会存在非常多的线程执行，那核函数怎么知道自己在全局的位置呢？在每个核函数的内部，存在四个自建变量，gridDim，blockDim，blockIdx，threadIdx，分别代表网格维度，线程块维度，当前线程所在线程块在网格中的索引，当前线程在当前线程块中的线程索引，每个变量都具有三维x、y、z，可以通过这四个变量的转换得到该线程在全局的位置。</p><p></p><p><code lang=\"text\">int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//假设网格和线程块都是一维启动</code></p><p></p><p>启动kernel时，设定的grid和block大小会对函数的性能产生一定影响，通常需要对这两个变量进行一定的调试，以获得较好的性能，而产生影响的原因，主要是根据每个函数所占用的资源以及cuda的执行模型决定，不同的启动参数会由于不同的寄存器占用等而导致不同的线程调度以及内存访问的延迟隐藏。</p><p></p><p></p><h2>1.3 CUDA执行模型</h2><p></p><p></p><p>与我们熟知的SIMD（单指令多数据）相似，GPU中每个SM中采用SIMT（单指令多线程）的架构进行线程的管理与执行，以线程束32个线程为单位进行调度与执行。</p><p></p><p>当CPU发起一个cuda kernel的时候，每个线程在哪个SM上所执行已经被确定，线程块一旦被调度到某个SM上，该线程块会在该SM上并发执行直到执行完毕。线程块被分配到哪个SM是由每个SM空闲的资源（寄存器、共享内存和剩余活跃线程数等）决定，一个SM可以同时执行多个线程块，这些线程块既可以来自于同一个kernel所发起，也可以来自于不同kernel所发起。</p><p></p><p>从SM之间的层次来看，不同SM之间的线程束确实从物理层面可能在并行执行，但是在单个SM内，每个线程束只是逻辑层面的并行执行，但从物理层面所有的线程束并不是同时进行，它们是并发地被SM中的线程束调度器调度执行。与CPU的线程切换不同，在GPU内，活跃的线程束切换并不会引起上下文切换的开销，因为在线程束被某个SM执行时，所有的共享内存和寄存器资源已经被分配完成，因此线程束的切换并不需要像CPU那样进行寄存器等上下文的切换。</p><p></p><p>然而SM内的资源是有限的，因此SM内的活跃线程束的数量是有限制的，它受到SM内资源限制以及硬件上最大活跃线程束数量的限制，不同硬件的SM最大活跃线程束的数量同样可以通过cudaGetDeviceProperties进行查询（查询到线程数量，线程束=线程数量/32）。如上文提及，线程束的调度一定是以32为整数进行调度，即使某个线程束中的实际kernel所需的线程数量不足32，但也会实际占用32个线程的硬件和内存资源。</p><p></p><p>每个被分配到资源的线程束被称为活跃的线程束，所有活跃的线程束有三种状态：选定的线程束，阻塞的线程束，符合条件的线程束。正在被执行的线程束称为选定的线程束，由于等待某种条件而等待的线程束称为阻塞的线程束，已经准备就绪等待执行的线程束称为符合条件的线程束。</p><p></p><p>下面针对CUDA执行模型的几个要点进行总结。</p><p></p><p></p><h3>1.3.1 延迟隐藏</h3><p></p><p></p><p>通常GPU具有比CPU片上更少的缓存资源，因此通常CUDA kernel会从全局内存中读取数据，但这会导致GPU上函数相比于CPU函数具有更长的访存延迟（通常为400-800个左右的时钟周期）。为了隐藏这种延迟通常使每个SM上不止执行一个线程束，通过多个线程束交替执行，从宏观上让SM一直处于运行状态，以达到延迟隐藏的目的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/76be5db86d4170c6e4d33daa61f54f84.png\" /></p><p></p><p>为更好地说明延迟隐藏的效果，在A10卡（72个SM）上进行elementwise_add的操作，每个线程执行三个读操作，一次写操作和一次加法操作，采用不同大小的grid和block进行实验，每次运算迭代100次测得总时间，实验结果如下表。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a068216ff3940d4150e555c5feaa94aa.png\" /></p><p></p><p>从实验结果中看，当线程束的数量大于SM数量，但是不算很大时，算子的RT几乎相近，因为每个线程的访存延迟相比于计算时间长的多，所以延迟隐藏之后，对整体的计算延迟没有太大的影响，但是当线程数量过大时，一个访存延迟时间不足以完成所有的线程发出访存指令，因此计算延迟会受到影响。但我们也可以看到不同的grid和block大小对性能也是有影响的，在kernel开发时，需要通过调节相应的grid和block大小获得最佳性能。</p><p></p><p></p><h3>1.3.2 避免分支分化</h3><p></p><p></p><p>由于GPU的SM是按照SIMT的方式执行，因此在一个线程束执行时，会执行相同的指令，如果kernel中存在if···else···语句，那么GPU会执行两个分支的代码，只是最终隐藏不符合条件的结果，这样会导致执行的指令数量变多，导致性能下降，因此在一个线程束内，应尽量避免出现分支分化。这样的限制存在于单个线程束内，在不同的线程束之间出现分支分化，一般来说对性能的影响会比较小。</p><p></p><p>但是有一种特例，当kernel中只存在if语句而不存在else时，对整体的性能影响较小。</p><p></p><p>相关实验网络上较多，不再展示实验结果。</p><p></p><p></p><h3>1.3.3 并行规约</h3><p></p><p></p><p>在向量中满足分配律和结合律的运算称为规约（reduce）。与elementwise运算不同，reduce会根据线程索引发生分支分化，严重影响性能，如何增加并行性是规约问题的主要优化方向。下文展示两种并行规约的代码，并分析影响其性能的因素。</p><p></p><p><code lang=\"text\">    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n​\n    // convert global data pointer to the local pointer of this block\n    int *idata = g_idata + blockIdx.x * blockDim.x * 8;\n​\n    // unrolling 8\n    if (idx + 7 * blockDim.x &lt; n)\n    {\n        int a1 = g_idata[idx];\n        int a2 = g_idata[idx + blockDim.x];\n        int a3 = g_idata[idx + 2 * blockDim.x];\n        int a4 = g_idata[idx + 3 * blockDim.x];\n        int b1 = g_idata[idx + 4 * blockDim.x];\n        int b2 = g_idata[idx + 5 * blockDim.x];\n        int b3 = g_idata[idx + 6 * blockDim.x];\n        int b4 = g_idata[idx + 7 * blockDim.x];\n        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;\n    }\n​\n    __syncthreads();\n​\n    // in-place reduction and complete unroll\n    if (blockDim.x &gt;= 1024 &amp;&amp; tid &lt; 512) idata[tid] += idata[tid + 512];\n​\n    __syncthreads();\n​\n    if (blockDim.x &gt;= 512 &amp;&amp; tid &lt; 256) idata[tid] += idata[tid + 256];\n​\n    __syncthreads();\n​\n    if (blockDim.x &gt;= 256 &amp;&amp; tid &lt; 128) idata[tid] += idata[tid + 128];\n​\n    __syncthreads();\n​\n    if (blockDim.x &gt;= 128 &amp;&amp; tid &lt; 64) idata[tid] += idata[tid + 64];\n​\n    __syncthreads();\n​\n    // unrolling warp\n    if (tid &lt; 32)\n    {\n        volatile int *vsmem = idata;\n        vsmem[tid] += vsmem[tid + 32];\n        vsmem[tid] += vsmem[tid + 16];\n        vsmem[tid] += vsmem[tid +  8];\n        vsmem[tid] += vsmem[tid +  4];\n        vsmem[tid] += vsmem[tid +  2];\n        vsmem[tid] += vsmem[tid +  1];\n    }\n​\n    // write result for this block to global mem\n    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n}</code></p><p></p><p>​上述两种方法将长度为16777216的整数进行求和规约，第一种方法在A10卡上0.574ms，第二种方法0.179ms，性能快了3倍。</p><p></p><p>规约问题通常涉及到循环与同步，CUDA的线程采用__syncthreads()进行同步，并且只能在同一个线程块内同步，不同线程块无法同步，因此每个线程块内求和得到的结果需要最终在cpu中或者再次起kernel求得。</p><p></p><p>两种方案的差别导致性能差异的分析如下：</p><p></p><p>前者在一个线程块内的显存范围进行求和，后者在8个线程块内的显存范围进行求和，降低了得到结果后在cpu进行求和的计算量，提高了kernel计算的并行度；前者采用相邻配对，而后者采用交错配对，虽然交错配对不会对分支分化产生影响，但是对全局内存的访问方法存在差异，对性能产生一定的优化；后者在与不同位置的值求和时，没有采用循环，而是直接将循环展开，这样做省略了for循环时进行的条件判断等操作耗时；当最终活跃的线程数量小于32时，没有在每次加法后进行同步，而是在一个线程束里面直接展开循环，因为CUDA执行是SIMT的方式，从指令层隐式同步了每一次加法，从而可以省略掉最后6的加法后同步，但此时要注意局部求和的结果用volatile进行修饰，以保证每次的结果直接写入全局内存，而不是暂存在缓存中从而导致运算结果不同步。</p><p></p><p></p><h2>1.4 CUDA内存模型</h2><p></p><p></p><p>CUDA的内存模型较为复杂，笔者也在逐渐摸索，所以在此仅介绍一些基本的性能概念。</p><p></p><p>GPU上的内存分为全局内存、常量内存、纹理内存、共享内存、L0/L1/L2缓存以及寄存器。</p><p></p><p>全局内存是GPU上面最大的内存，但是读取速度也是最慢的，通常在不进行运算时，数据都存在于全局内存，用户可编程进行读写。常量内存也是GPU上一块对所有线程可见的内存，里面存的数据只能由主机编程写，对核函数只能读取，除此之外在GPU上还有一块常量内存缓存，读取速度比全局的内存较快。纹理内存也是一种存放常量的内存，以前是用于图形处理，由于其存储地方处于片上，因此读取速度比全局内存快。共享内存位于SM中，对每个线程块可见，读取速度较快，通常这里的数据用于线程块内的数据交换，是用户可编程读写内存。L0/L1/L2不同级别的缓存存储的位置由小到大，但其读取速度由快到慢，是不可编程缓存。寄存器是GPU上面读取速度最快的内存，但也是空间最小的内存，它与共享内存一样是GPU上最稀缺的资源，通常也是制约活跃线程束的重要因素之一，用户可编程读写，通常核函数内的自变量存在于寄存器中。</p><p></p><p>对于频繁进行D2H和H2D内存拷贝操作时，通常我们可以将CPU内存固定页面在内存中，以防止页面切换导致的开销，可以通过cudaMallocHost进行申请，这样当数据进行传输时，可以直接利用主机DMA进行数据拷贝。但是过多的固定页面内存会导致主机性能的下降，需要根据性能结果进行微调。</p><p></p><p></p><h2>1.5 多流</h2><p></p><p></p><p>多流在GPU编程中是一个非常重要的特性，对于充分利用GPU计算资源以及提高程序的并行度具有重要意义。流是指一连串有先后顺序的异步操作，按照顺序在设备上执行。一般在多线程程序中，可以通过多流，使多线程中不相关的操作堆叠执行，提高GPU计算的并行度，实现网格级并发。</p><p></p><p>前面提到过，通常最简单的CUDA程序有三步，先将数据从主机拷贝到设备，在设备上进行计算，将数据从设备拷到主机。通过多流技术，既可以让同一个流上的异步操作按顺序执行，也可以让不同流上的操作重叠执行。</p><p></p><p>之前我们所使用的同步cuda操作，都是使用系统的默认流，而默认流通常认为和其他用户自定义的流是冲突的，即默认流不能和用户自定义的流重叠执行。</p><p></p><p>在使用用户自定义的流之前，需要先对流进行初始化：</p><p></p><p><code lang=\"text\">cudaStream_t stream;\ncudaStreamCreate(&amp;stream);\n​\ncudaStreamDestory(stream);</code></p><p></p><p>通常使用的异步cuda操作有两种，kernel和memcpy，对kernel使用多流时，只需要在启动kernel的&lt;&lt;&lt;&gt;&gt;&gt;中的第四个参数传入流即可，通常kernel只要在GPU计算资源充足的情况下，即可实现多流重叠执行，但当GPU相关计算或者寄存器等资源不足时，也会存在等待的情况。</p><p></p><p><code lang=\"text\">···\nkernel&lt;&lt;&gt;&gt;(···);\n···</code></p><p></p><p>memcpy的异步操作需要使用异步的API，可以实现H2D、D2H、D2D的重叠执行，其中D2D之间也可以实现重叠执行，由于PCIE接口是全双工通信，H2D和D2H之间可以重叠执行，但是多个H2D或者多个D2H之间由于抢占PCIE的接口资源，不能重叠执行。</p><p></p><p><code lang=\"text\">cudaMemcpyAsync(void *dst, void *src, size_t size, cudaMemcpyKind kind, cudaStream_t stream);</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b08e71ae614595c3ef9ebf3379ca54f9.png\" /></p><p></p><p>对于主机和设备之间的memcpy，当cpu内存是malloc申请的，这一块内存可能会因为分页导致需要先加载数据到内存，再进行拷贝，因此通常会把cpu内存注册为pinned内存，这样在memcpy的时候可以利用DMA直接进行拷贝，提高拷贝性能。</p><p></p><p><code lang=\"text\">cudaError_t cudaMallocHost(void** ptr, size_t size);\ncudaError_t cudaHostAlloc(void** pHost, size_t size, unsigned int flags);</code></p><p></p><p>当我们的程序需要在某个操作之后等待GPU完成所有计算，再执行一下步操作时，需要对流进行同步。</p><p></p><p><code lang=\"text\">cudaStreamSynchronize(stream);</code></p><p></p><p></p><h2>1.6 GPU性能profile工具Nsight System简介</h2><p></p><p></p><p>Nsight System是一款用于GPU性能profile的工具，通常从nsight上可以直观看到CPU和GPU执行的情况，并由此分析计算性能瓶颈，并且可以查看线程情况，CUDA api以及cpu程序api等，同时也可以查看更加详细的gpu占用情况，网卡情况以及tensorrt，cudnn等调用情况。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4cd5e9ad8a5ccdd514fce57ff1502227.png\" /></p><p></p><p>上图是一张nsight的性能测试结果，从途中我们可以很轻松看到cpu占用情况，gpu在某个时刻是在memcpy还是执行kernel，每个流的占用，每个线程的api调用等。</p><p></p><p>调用nsight的命令非常简单，并且可以通过--trace指定需要生成哪些信息的报告（比如cuda、cudnn、cublas、nvtx，在较新版本中还可以查看nccl），--duration可以指定抓多长时间的包，--sampling-frequency可以指定采样频率（100～8000），其他的选项可以查看下方链接中的官方使用文档。</p><p></p><p><code lang=\"text\">nsys profile [option] [exec]</code></p><p></p><p>当我们需要profile的程序启动时间比较久时，我们希望nsight延迟启动抓包，可以先nsys launch [exec]再通过nsys start和nsys stop来启动和停止抓包</p><p></p><p>https://developer.nvidia.com/nsight-systems</p><p>https://docs.nvidia.com/nsight-systems/UserGuide/index.html</p><p></p><p></p><h2>1.7 编程小经验</h2><p></p><p></p><p></p><h3>1.7.1 cuda版本兼容性</h3><p></p><p></p><p>做GPU、TensorRT应用部署的小伙伴经常为版本问题烦恼，比如trt8.2要求cuda版本一般为11.4，这时要求GPU驱动至少为470.57.02，而对于企业来讲，通常cuda版本可以通过改变容器镜像来升级，但GPU驱动版本是由宿主机决定，对于云端部署的应用来讲，GPU驱动版本是不易修改的，那我们怎么部署依赖较新cuda版本的应用呢？</p><p></p><p>在谈论这个问题之前，我们首先了解两个概念Driver API（驱动API）和Runtime API（运行时API），驱动API存在与libcuda.so中，而运行时API存在与libcudart.so中，通常我们在读取驱动信息时，会访问驱动API，通常我们熟悉的nvidia-smi就是访问这里的信息。当我们访问nvidia-smi时，显示的GPU驱动是宿主机的驱动版本，而显示的cuda版本，并不是我们容器内安装的cuda版本，而是该GPU驱动版本下允许安装的最高cuda版本。在我们实际进行GPU计算时，会调用运行时API，也就是通过libcudart.so进行调用。</p><p></p><p>在cuda10.x及以前，我们要求cudnn，cublas等GPU应用与库必须依赖对应的cuda版本，且必须有足够高的GPU版本，如果版本不对应则会出现不能正常运行的问题。在cuda11.x之后，NVIDIA对cuda提供了次级版本兼容性，使用相同cuda主要版本的 cuda工具包版本编译的应用程序可以在至少具有最低要求驱动程序版本的系统上运行，但是其功能可能会被限制。下图是NVIDIA官方文档提供的cuda11.x和12.x最低驱动版本限制的表格。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6d475b7b3154e6689c65c4f013eb010c.png\" /></p><p></p><p>所以当我们宿主机版本在450.80.02+时，我们可以安装cuda11.x版本的工具包，并直接运行仅依赖cuda的应用。但当依赖cudnn和cublas时，我们仍然要考虑他们之间版本的对应，但是通常这些库版本升级较为容易。值得注意的是当我们的应用程序依赖了ptx优化时，需要通过后面的前向兼容来解决兼容性问题。以下是我在cuda11.2的系统内使用trt8.4（依赖cuda11.6）编译的引擎模型时，运行tritonserver时报的错误，通过前向兼容可以解决这个问题。那么要怎么使用前向兼容呢？</p><p></p><p></p><blockquote>UNAVAILABLE: Internal: unable to create stream: the provided PTX was compiled with an unsupported toolchain</blockquote><p></p><p></p><p>虽然一直以来cuda11.x的应用可以直接在低版本的驱动环境内运行，但是一些有别的依赖的应用仍需要较高的驱动版本才能运行，此时可以通过安装NV的前向驱动包解决这个问题。例如当我们在460版本的驱动环境中使用cuda11.6时，且依赖与cuda11.6对应的ptx工具时，可以通过下述命令（alios7）安装兼容包。</p><p></p><p><code lang=\"text\">sudo yum install -y cuda-compat-11-6</code></p><p></p><p>安装之后通常会存在/usr/local/cuda-11.6/compat目录，将该目录override系统的库目录LD_LIBRARY_PATH，此时大家可以nvidia-smi看一下，是不是惊喜发现允许的最高cuda版本变高了（但驱动版本是没有变的哦）。该目录下存在以下四个文件：</p><p>libcuda.so.* 驱动程序libnvidia-nvvm.so.* JIT-LTO（cuda11.5以上）libnvidia-ptxjitcompiler.so.* PTX的实时编译器libcudadebugger.so.* 调试程序（cuda11.8以上）</p><p></p><p>以下是NVIDIA官方文档中的兼容性支持表格。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78bdf4249dd6ca19fa9aa65822d51460.png\" /></p><p></p><p>https://docs.nvidia.com/deploy/cuda-compatibility/index.html</p><p></p><p></p><h3>1.7.2 避免多次D2D内存拷贝</h3><p></p><p></p><p>GPU通常用于处理大规模并行计算，特别是在人工智能领域，我们通常通过组batch的方式提高并行度，可能某个逻辑处理原因，我们会在GPU内多次D2D的进行显存拷贝。由于每次显存拷贝需要CPU发送指令，即使采用流的方式，也会导致每次cpy之间存在空隙，且无法有效利用显存带宽，导致GPU利用率较低，虽然可以通过多流的方式隐藏延迟，但是从一个请求的角度看，RT仍然较高。如下图是一个多次拷贝的profile结果，从图中可以看到D2D的memcpy（浅蓝色），持续时间较长。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/110b36762daf48aa7d935a699d2c9f04.png\" /></p><p></p><p>对于D2D的内存拷贝，我们为解决这种问题，自己写了一个kernel手动进行赋值操作，一般这种情况下是多组空间不连续的显存进行拷贝，所以我们先存储每块显存的首地址和数据长度以及目标地址在cpu上进行存储，然后拷贝到GPU上，通过起一个kernel的当时进行一对一的赋值，这样可以在高并发的情况下，将起cpy或者kernel的次数从N次变为4次，这样可以缩短很多时间（当然，并发比较低时候，两种方式的rt都不高）。以下是优化之后的同样并发下，Nsight profile之后的结果，可以看到非模型推理时间大幅度降低，甚至之前需要将近10ms才能完成的cpy操作变得连1ms都不到。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b84d013c9b80243a4ffeaee75e5b41a1.png\" /></p><p></p><p></p><h1>二、TritonServer模型部署与业务实践</h1><p></p><p></p><p></p><h2>2.1 Triton功能简介</h2><p></p><p></p><p>TritonServer是NVIDIA发布一款开源的推理服务软件，可以简化AI推理的部署复杂度，提高AI开发人员的开发效率，使用Triton可以部署多种机器学习框架的AI模型，比如pytorch、tensorflow、TensorRT、ONNX以及FasterTransformer等，当然也可以用户自定义编写backend用于推理。</p><p></p><p>Triton服务器在模型推理部署方面拥有非常多的便利特点，大家可以在官方github上查看，笔者在此以常用的一些特性功能进行介绍（以TensorRT模型为例）。大家尝试使用的话，可以直接下载nv的NGC容器进行尝试（自己编译tritonserver非常痛苦）。</p><p></p><p>https://github.com/triton-inference-server/server</p><p></p><p></p><h3>2.1.1 模型配置</h3><p></p><p></p><p>笔者认为triton最方便的一点便是自动化的多模型部署，想一下如果我们一张GPU上只部署一个普通的模型，会不会存在GPU利用率很低的情况呢，这样肯定会导致成本的增长与资源的浪费；如果我们一张GPU上部署多个模型，那么对于每种框架，我们需要编写复杂的业务和工程代码以使多个模型在同一个镜像内运行起来，并对外提供服务，这样极大影响了AI工程师的开发效率，并且稳定性也需要验证。通过利用triton，我们可以很方便将目前通用框架的模型不用编写工程代码很方便的部署起来，并且一台机器上可以部署多个模型，以及一个模型的多种版本。</p><p></p><p>当然，我们在部署模型的时候需要为triton提供一份模型的配置文件，对于同一种模型，通常只需要提供一份配置文件即可（config.pbtxt），然后将模型文件按照版本一起提供给triton，即可开启triton进行服务。以下列出tirton模型仓库的目录结构。</p><p></p><p><code lang=\"text\">model_repo\n    |-model_1\n        |-config.pbtxt\n        |-  1\n            |-model_1.plan\n        |-    2\n            |-model_1.plan\n    |-model_2\n        |-config.pbtxt\n        |-    1\n            |-model_2.plan\n            \n#启动命令\ntritonserver --model-repository model_repo</code></p><p></p><p>以下给出最简单的配置文件，并对里面的模块进行介绍。根据triton的介绍，对于使用最简单的模型配置，可以不需要用户显式提供配置文件，但是目前笔者还没有尝试过，因为一般都会在基础配置上添加一些额外配置，所以还是提供一份配置文件放心一点。</p><p></p><p>name：模型名字，需要和该模型的根目录名字一样；backend：用于执行模型的后端，可以是tensorrt、tensorflow、python、pytorch以及用户自定义后端；max_batch_size：最大batch_size（用于输入输出不包含batch，且支持动态批处理的模型，对于不支持动态批处理的模型，只能设置为0）；input、output：内含模型的每个输入、输出；input-name：输入的名字，一半可以通过在onnx状态模型去查看；input-data_type：类型，有TYPE_FP32、TYPE_INT8、TYPE_FP16、TYPE_STRING等类型；input-dims：维度，可以包含或者不包含批处理维度，对于不支持批处理的模型，需要完全按照实际模型的输入维度提供（包括batch），对于支持批处理的模型，第一维可以省略，写-1均可。</p><p></p><p><code lang=\"text\">name: \"model\"\nbackend: \"tensorrt\"\nmax_batch_size: 8\ninput [\n  {\n    name: \"input0\"\n    data_type: TYPE_FP32\n    dims: [ 16 ]\n  },\n  {\n    name: \"input1\"\n    data_type: TYPE_FP32\n    dims: [ 16 ]\n  }\n]\noutput [\n  {\n    name: \"output0\"\n    data_type: TYPE_FP32\n    dims: [ 16 ]\n  }\n]</code></p><p></p><p></p><h3>2.1.2 动态形状与动态批处理</h3><p></p><p></p><p>当我们部署模型时，我们不仅希望模型可以组batch增大模型的并行性，也希望模型的并行度可以随着流量而变化，而像pytorch、Tensorrt等推理引擎都支持动态形状推理，因此triton服务器在接受推理请求时，也需要支持动态形状。</p><p></p><p>对于非batch维度，我们需要在配置文件对应输入输出dims的对应位置写为-1，这代表的该tensor的该维度接收动态形状。而对于batch维度，上小节已介绍了配置的方法，这里提一点，当多个请求在短时间内被发送到triton时，服务器应该是对每个请求执行一次推理？还是等待请求数到达最大batch再进行推理？或者是别的调度方法？</p><p></p><p>前两种方案很明显存在很严重的问题，会导致某些请求的等待时间过长，triton提供了动态batch的调度方法，只需要在配置文件中添加下述的配置即可。其中preferred_batch_size指尽可能让组batch的数量为以下值（不能大于max_batch_size），而max_queue_delay_microseconds指当无法到达最大batchsize或者倾向的batchsize时，最长的超时时间，例子中设置为1000微秒。</p><p></p><p><code lang=\"text\">dynamic_batching {\n  preferred_batch_size: [16, 32, 64, 128]\n  max_queue_delay_microseconds: 1000\n}</code></p><p></p><p></p><h3>2.1.3 多实例部署</h3><p></p><p></p><p>除了多模型多版本部署以提高GPU利用率之外，triton还支持通过配置文件进行模型的多实例部署，当一个模型在GPU中存在多个实例时，triton将自动为每个（批）请求分配空闲的实例进行推理，以提高GPU的利用率，同时triton还支持在单机多卡的节点中，在不同的GPU中部署不同数量的实例。</p><p></p><p><code lang=\"text\"># 为每个GPU上分配2个实例\ninstance_group [\n  {\n    count: 2\n    kind: KIND_GPU\n  }\n]\n​\n# 为0号GPU分配1个实例，为1、2号GPU分配2个实例\ninstance_group [\n  {\n    count: 1\n    kind: KIND_GPU\n    gpus: [ 0 ]\n  },\n  {\n    count: 2\n    kind: KIND_GPU\n    gpus: [ 1, 2 ]\n  }\n]</code></p><p></p><p></p><h3>2.1.4 客户端调用方式与监控</h3><p></p><p></p><p>在triton的github仓库中，nvidia提供了与tritonserver相对应的client sdk以及示例代码，大家可以通过GRPC（8001端口）或者HTTP（8000端口）协议与tritonserver进行请求与获得响应。同时，tritonserver通过8002端口以http协议提供监控功能，可以对外提供GPU和CPU的相关指标，便于服务端开发者建设稳定性系统。</p><p></p><p>在示例中，主要根据其应用场景分为多种客户端，有针对单次推理的普通客户端，也有针对流式的客户端，也有针对不同数据类型的客户端。</p><p></p><p>https://github.com/triton-inference-server/client</p><p></p><p>然而我们在实际实践过程中，特别是对于自定义backend，经常需要进行压测与疲劳测试以确定用户代码没有内存问题和线程安全等问题，对于客户端的敏捷开发非常不便，这一点笔者目前正在开发简单易用的基于GRPC的多功能测试客户端，支持QPS和并发两种模式，同时具有性能测试和疲劳稳定性测试以及回归测试三种测试方式，后续功能完善稳定之后，会逐步对开放对外使用，敬请期待。</p><p></p><p></p><h3>2.1.5 ensemble编排</h3><p></p><p></p><p>对于绝大部分AI应用场景，我们通常需要部署多个模型以满足业务功能的需要，假如我们分多次发送请求，必定会存在大量的通信开销，如果能够一次请求就按照顺序完成所有的模型推理，那岂不是很好？</p><p></p><p>在triton里面提供了ensemble功能，可以对多个模型进行编排，通过一个虚拟的编排模型，将多个模型进行串联，并完成数据的传递。</p><p></p><p>以下是来自triton github上面的一个例子，定义ensemble的模型名字是“ensemble_model”，即客户端在发送请求时，应该请求“ensemble_model”，而input和output则应该与模型的输入输出区分开来，因为triton认为ensemble也是一个模型，同时在部署的时候，在模型仓库中，该配置脚本也应该放在一个叫“ensemble_model”的文件夹下，并且也存在一个名为“1”的空白文件夹，以代表该模型存在版本-1。</p><p></p><p>在“ensemble_scheduling”中，每个step代表一次模型推理，分别填入模型名字与版本（-1代表最新版本），在map中key中指代模型实际输入输出的名字，而value指代在ensemble中的形参。注意一点，如果模型有的输出不会被其他模型所使用，那么需要将这一对map删除，即ensemble的配置中，并不需要强制指定所有输入输出的映射关系，只需要完成对应的数据传输流即可。</p><p></p><p><code lang=\"text\">name: \"ensemble_model\"\nplatform: \"ensemble\"\nmax_batch_size: 1\ninput [\n  {\n    name: \"IMAGE\"\n    data_type: TYPE_STRING\n    dims: [ 1 ]\n  }\n]\noutput [\n  {\n    name: \"CLASSIFICATION\"\n    data_type: TYPE_FP32\n    dims: [ 1000 ]\n  },\n  {\n    name: \"SEGMENTATION\"\n    data_type: TYPE_FP32\n    dims: [ 3, 224, 224 ]\n  }\n]\nensemble_scheduling {\n  step [\n    {\n      model_name: \"image_preprocess_model\"\n      model_version: -1\n      input_map {\n        key: \"RAW_IMAGE\"\n        value: \"IMAGE\"\n      }\n      output_map {\n        key: \"PREPROCESSED_OUTPUT\"\n        value: \"preprocessed_image\"\n      }\n    },\n    {\n      model_name: \"classification_model\"\n      model_version: -1\n      input_map {\n        key: \"FORMATTED_IMAGE\"\n        value: \"preprocessed_image\"\n      }\n      output_map {\n        key: \"CLASSIFICATION_OUTPUT\"\n        value: \"CLASSIFICATION\"\n      }\n    },\n    {\n      model_name: \"segmentation_model\"\n      model_version: -1\n      input_map {\n        key: \"FORMATTED_IMAGE\"\n        value: \"preprocessed_image\"\n      }\n      output_map {\n        key: \"SEGMENTATION_OUTPUT\"\n        value: \"SEGMENTATION\"\n      }\n    }\n  ]\n}</code></p><p></p><p></p><h3>2.1.6 自定义backend</h3><p></p><p></p><p>ensemble可以解决大部分简单的模型串行关系，但是对于有复杂前后处理或者复杂链接关系的业务模型，使用ensemble则可能不能完全满足需要，这个时候需要我们用户自己写backend，类似于trt或者pytorch，triton将我们写的代码也认为是一个backend，可以将业务代码直接写进去，也可以作为一种通用的backend进行多模型复用。</p><p></p><p>在triton的github官网上，nvidia提供了用户自定义backend的示例，用户编写代码好，经过编译，生成一个libtriton_[name].so，将该so文件可以放在与模型一样的文件夹结构，也可以放在与trt相同的backend目录中。custom backend为用户提供了极高的自由度，用户只需要实现相应的triton接口，即可完全自由地进行业务逻辑的实现。</p><p></p><p>需要实现的接口有以下：</p><p></p><p><code lang=\"text\">//初始化backend\nTRITONSERVER_Error* TRITONBACKEND_Initialize(TRITONBACKEND_Backend* backend);\n​\n//销毁backend\nTRITONSERVER_Error* TRITONBACKEND_Finalize(TRITONBACKEND_Backend* backend);\n​\n//初始化模型\nTRITONSERVER_Error* TRITONBACKEND_ModelInitialize(TRITONBACKEND_Model* model);\n​\n//销毁模型\nTRITONSERVER_Error* TRITONBACKEND_ModelFinalize(TRITONBACKEND_Model* model);\n​\n​\n//初始化实例\nTRITONSERVER_Error* TRITONBACKEND_ModelInstanceInitialize(TRITONBACKEND_ModelInstance* instance);\n​\n//销毁实例\nTRITONSERVER_Error* TRITONBACKEND_ModelInstanceFinalize(TRITONBACKEND_ModelInstance* instance);\n​\n//单个示例接收请求\nTRITONSERVER_Error* TRITONBACKEND_ModelInstanceExecute(\n        TRITONBACKEND_ModelInstance* instance, TRITONBACKEND_Request** requests,\n        const uint32_t request_count);</code></p><p></p><p>https://github.com/triton-inference-server/backend</p><p></p><p></p><h2>2.2 Triton部署TTS模型实践</h2><p></p><p></p><p></p><h3>2.2.1 TTS业务概述</h3><p></p><p></p><p>文本转语音（text to speech，TTS）是AI语音领域非常重要的应用方向。</p><p></p><p>云端tts业务整体有encoder、decoder和vocoder三个模型，他们的串联顺序如下图，由于文本转语音是一个流式任务，一次请求需要多次返回结果，所以一次请求需要重复调用多次模型推理，其中encoder模型仅在收到推理请求时执行一次，而decoder和vocoder需要根据划分块的数量推理相应的次数，并且在每次模型推理后还包括一些后处理的任务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a3d2192dc8bbf7bc240803b57ec704a5.png\" /></p><p></p><p>业务有以下几个特点：</p><p></p><p>首先是对首包延时敏感，tts业务的结果由多个块组成，而客户端只要收到首包即可以播放给用户，因此相比于非首包的计算时间，首包的计算时间更加关键，直接关系到用户的体验；第二个是非首包时间不敏感，在正常情况下，处理一包的时间小于一包的播放时间；第三个是一次请求会计算得到十数甚至数十包；第四个是encoder模型只参与第一包的计算，而decoder和vocoder模型参与了所有包的计算，使得在正常情况下，decoder和vocoder的组batch数会远远大于encoder；第五个是性能评价指标有首包延时，可承载并发量以及吞吐量；第六个是云端业务共同的特点，由于云端业务具有并发特性，所有在一些预处理和后处理的过程中，每个请求需要等待所有请求均完成之后才能共同进入下一步，这会导致单请求视角的等待时间较长。</p><p></p><p>我们的tts部署在nvidia语音解决方案团队的帮助下，在他们提供的tts部署方案基础上，成功在triton server上完成部署，通过自定义的backend，完成了整套的流程，但是初期的backend延迟性能和并发性能都较差，GPU利用率较低，笔者对这部分进行了一部分优化，在此记录所优化的过程。下文的实验与优化，都在A10卡上进行。</p><p></p><p></p><h3>2.2.2 性能瓶颈分析</h3><p></p><p></p><p>先用nsys对计算时的计算资源进行分析，得到如下图，并根据代码逻辑，分析得到有如下的性能瓶颈：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/507018e8f5e82f5fcf87903768e37c43.png\" /></p><p></p><p>1）首先从整体上分析，一次包含encoder的模型推理耗时在整个流程中仅占42%（以下实验除标注外，都在100并发下进行），除计算耗时外，大部分时间消耗在资源的申请释放、内存拷贝、后处理三个阶段中。</p><p></p><p>2）一次循环的开头，线程接收到新的请求，为其新开辟所需要的内存与显存，待推理结束后又将所有资源释放掉，这部分耗时严重，并且随着请求量的增加，其耗时也随之增加，而其他每个请求均需要等待所有请求的资源申请之后才能进行下一步，增加等待时间。（重复申请释放资源）</p><p></p><p>3）在组、拆Batch时，对每个请求的每个tensor都单独执行cudamemcpy，使得启动cudamemcpy的次数过多，导致时间累积过长，并且性能分析图里出现许多的断块，说明没有充分利用显存的带宽，上次的拷贝已经结束，下次的拷贝仍未启动，像encoder和decoder的输入均有5个tensor，这样无疑成倍增加memcpy的时间开销。（多次、离散进行cudamemcpy）</p><p></p><p>4）存在不必要的深拷贝，在encoder的后处理和decoder-&gt;vocoder的数据过程中，存在了不必要的深拷贝，并且均为离散的cudamemcpy操作，这部分会增加时间开销。（过多深拷贝）</p><p></p><p>5）首包与非首包存在相互制约，由于首包需要进行encoder的推理，而非首包不需要，因此会导致非首包数据等待一次encoder过程；而由于非首包的请求量远大于首包的请求量，导致首包数据需要拼很大的batch，会增加前后处理、组拆batch以及推理耗时。（任务线程组织不合理）</p><p></p><p>6）后处理过程耗时较长，在tts业务中，encoder和vocoder的输出数据具有后处理过程，而后处理将数据拷贝回cpu进行处理，再拷贝回gpu，无疑增加时间消耗。（后处理设计不合理）</p><p></p><p></p><h3>2.2.3 优化工作</h3><p></p><p></p><p></p><h4>2.2.3.1 重复申请释放资源-管理资源池</h4><p></p><p></p><p>在程序启动时，一次性创建一定数量的InferenceItem实例存放于资源池中，每次收到推理实例时，从资源池中取出，进行状态量与表征状态的内存初始化，节省了很多时间开销。当遇到资源池中无实例时，采用的策略是已经获得实例的请求开始首包推理，而其他请求则等待下次首包推理获取实例。目前资源池的大小设定为模型max_batch的两倍，主要是依据后续的线程组织结构设定。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/54013afd0ab329d0a0a9e4f2ec95f4d7.png\" /></p><p></p><p></p><h4>2.2.3.2 多次、离散进行cudamemcpy-对D2D使用cudakernel进行并行拷贝</h4><p></p><p></p><p>在gpu中的数据传递可以使用cudakernel进行一次性操作，以实现batch之间memcpy的并行，减小了cudamemcpy启动的次数。</p><p></p><p>第一次优化：在cpu中记录每组拷贝的源地址(src_ptr)，目标地址(dst_ptr)以及拷贝长度(src_len)，在程序启动时，管理一片用于存储这三组数据的显存，然后三次将拷贝信息拷贝到显存中，可以启动相应组数的cudakernel，每个kernel进行memcpy操作。这样将原先的多次启动cudamemcpy操作缩短为三次cudamemcpy+一次cudakernel的时间开销。</p><p></p><p>第二次优化：同样记录上述三种信息，但同时记录所有拷贝长度中的最大值，在启动kernel时，启动 组数*最大值 数量的kernel，根据索引判断是否在有效的数据范围内，直接进行赋值操作，以进一步压缩拷贝时间开销。</p><p></p><p></p><h4>2.2.3.3 多次、离散进行cudamemcpy-对H2D、D2H使用连续内存进行拷贝</h4><p></p><p></p><p>由于D2H和H2D的数据操作不能在cudakernel中并行执行，因此想要降低启动cudamemcpy的开销，只能将数据存储于一片内存中，然后一次性进行拷贝操作，然后在cpu或者gpu中进行拆分等操作。</p><p></p><p>由于encoder的数据拷贝量很少，目前的时间开销不多，因此没有针对H2D进行优化；针对vocoder的输出数据，则是将其输出先直接拷贝回cpu中，然后直接将cpu中的内存首地址给各个实例存储以进行发送，由于每个线程的vocoder推理和发送结果是顺序进行的，因此不会存在后来数据覆盖未处理数据的风险。</p><p></p><p></p><h4>2.2.3.4 过多深拷贝-改用地址拷贝、减少中间环节</h4><p></p><p></p><p>原先由于decoder-&gt;vocoder的数据传输较为复杂，出现了需要多次拷贝decoderOutput不同片断的情况，所以出现了多次内存深拷贝，但增加了很多的时间开销。通过梳理数据流，将深拷贝更改为地址拷贝，有效降低了时间开销，具体数据流见下图。由于业务逻辑的多样性和复杂性，这部分在后续其他业务的开发中，需要提前进行梳理，以避免后续二次优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/50fdf3b4ab954f69e5d74a865bb92193.png\" /></p><p></p><p></p><h4>2.2.3.5 任务线程组织不合理-避免将不需要的任务放入线程</h4><p></p><p></p><p>将原先的一个线程拆分为两个线程，一个线程用于处理首包请求，包含三个模型encoder、decoder与vocoder，此时的decoder和vocoder的请求batch数相比于原来相同并发下能保持在一个相对较小的水平，可以降低这两个模型的耗时；另外一个线程用于处理非首包请求，只包含两个模型decoder和vocoder，此时非首包请求不需要在encoder模型处进行等待，很有效地增加了系统的吞吐量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8b714ab51d612fb203bb32436d9a6bb5.png\" /></p><p></p><p></p><h4>2.2.3.6 后处理设计不合理-将后处理放入gpu以获得并行加速</h4><p></p><p></p><p>原先的后处理过程中，整体思路都是先拷回cpu，进行处理之后再进行下一步操作，而组batch的目的是利用gpu的并行运算加快计算时间。</p><p></p><p>针对encoder的后处理，原先的方案是将output与duration都拷贝回cpu，由duration计算得到实际的帧数之后，将output进行拷贝与扩展，再拷贝回gpu。优化后的方案是只将duration拷贝回cpu，计算得到帧数后，在gpu中进行output的拷贝，而显存空间的扩展也不再进行，改为在程序启动时，预先预留好最长可能的数据长度，避免重复的malloc与free，在目前显存空间充足的前提下，是较优的方案。</p><p></p><p>针对vocoder的后处理，需要进行四步，lfilter、crossfade、memcpy以及quantize，其中lfilter不能在一个请求的数据之间并行进行，因此只能在batch之间并行运算，而另外三个任务都是可以在请求内的数据之间并行运行，并且这三个任务是相邻，可以进行任务的融合。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/489ca9b11e1f4bcd7f51148c0bccaf63.png\" /></p><p></p><p></p><h3>2.2.4 优化总体效果</h3><p></p><p></p><p></p><h4>2.2.4.1 管理资源池</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/6233ca4f27c3940aa49128ccf561f73e.png\" /></p><p></p><p>在100并发下首包延时降低约15%，吞吐量增加21.7%。</p><p></p><p></p><h4>2.2.4.2 对D2D使用cudakernel进行并行拷贝+改用地址拷贝、减少中间环节</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/69efb07c1f8f4a6e3422f4adac344ae6.png\" /></p><p></p><p>在100并发下首包延时降低约24%，吞吐量增加10%</p><p></p><p>第二期优化：将memcpy展平</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/5313ddf618b2a77b52f68d1a28712e2d.png\" /></p><p></p><p></p><h4>2.2.4.3 对H2D、D2H使用连续内存进行拷贝+优化vocoder后处理</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c46fabb6b10936bd2a5e56c94f6c4621.png\" /></p><p></p><p>在100并发下首包延时降低约9%左右。</p><p></p><p></p><h4>2.2.4.4 避免将不需要的任务放入线程</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f39f0e77807b176c55902c9810ed0be9.png\" /></p><p></p><p>在100并发下首包延时降低约13%，吞吐量增加12.7%。</p><p></p><p></p><h4>2.2.4.5 优化encoder后处理</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c46fabb6b10936bd2a5e56c94f6c4621.png\" /></p><p></p><p>由于encoder的batch数较小，这一部分与batch正相关，所以加速效果较小。</p><p></p><p></p><h4>2.2.4.6 优化总体效果</h4><p></p><p></p><p>经过优化后，TTS整体的性能得到了极大的提升，以天猫精灵高峰期的常规文本作为测试集，在128并发下A10卡进行测试，首包延时平均下降约60%，非首包延时平均下降约40%，文本级单卡吞吐量上升50%，相较于原先的性能得到了大幅提升。</p><p></p><p>我们与NVIDIA解决方案架构团队合作的前期在T4GPU上的相关成果也在NVIDIA的官网进行介绍。</p><p></p><p>https://blogs.nvidia.cn/2022/07/19/alibaba-tmall-genie-accelerated-deep-learning-models-text-to-speech-with-nvidia-tensorrt-triton/ 。</p><p></p><p></p><h1>三、写在最后</h1><p></p><p></p><p>在半年以来一直围绕云端模型的部署和计算开展学习和工作，利用NVIDIA的TensorRT、Triton和Nsight等工具进行天猫精灵在线TTS推理服务相关工作，并以此业务为基石，拓展云端GPU相关技术能力。本文主要对我半年以来主要从事的triton部署以及学习的cuda相关知识进行了一些总结，可能由于学习的还不够全面，文中仅总结了一些常用的功能和知识点，并且可能会有一些疏漏，如果大家发现有错误的地方，望不吝赐教。</p><p></p><p>后续我们还将对于云端语音相关技术的模型部署，基于cuda的相关高性能计算库以及大模型的推理部署等相关技术进行分。如果阿里的同学们也有对相关技术感兴趣的，欢迎随时和我们联系交流，一起进步推动AI工程化的发展。在此感谢智能互联-工程技术-智能引擎团队所有师兄同学的帮助，感谢与我合作的算法同学的倾力配合，同时感谢NVIDIA语音解决方案团队对我们在相关工具使用上的支持与帮助。</p>",
    "publish_time": "2023-04-13 10:04:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "容器与无服务器，是竞争对手还是队友？",
    "url": "https://www.infoq.cn/article/EvMkADbFYlbaphLm8ywY",
    "summary": "<p>在IT行业，我们经常会听到诸如云计算、容器、无服务器框架等术语。</p><p>&nbsp;</p><p>那么什么是云计算？容器是如何工作的？函数又如何变成无服务器的？</p><p>&nbsp;</p><p>本文将尝试解读这些技术术语，并探索开发人员应该如何在技术栈中考虑采用容器或无服务器函数。</p><p>&nbsp;</p><p>例如，如果你的应用程序启动时间较长，那么容器可以更好地满足你的需求。</p><p>&nbsp;</p><p>需要进行大规模伸缩的高效无状态函数将从运行无服务器函数中受益。</p><p>&nbsp;</p><p></p><h4>容器的工作原理</h4><p></p><p>&nbsp;</p><p><a href=\"https://www.techtarget.com/searchitoperations/definition/container-containerization-or-container-based-virtualization\">容器</a>\"是被打包好的应用程序，包含了代码以及必要的库和依赖项，可以在任何环境中运行，不管是哪种操作系统。它可以帮助开发人员轻松地构建、发布、部署和扩展应用程序。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1a/1a17623a86b2256f9560cfd4b2da92e3.png\" /></p><p></p><p>图1 虚拟机与容器之间的比较</p><p>&nbsp;</p><p>以前，在容器还不是那么流行的时候，开发人员习惯于将应用程序部署在单独的虚拟机中以实现隔离。每个<a href=\"https://www.spiceworks.com/tech/devops/articles/what-is-virtual-machine/\">虚拟机</a>\"都需要一个客户操作系统，需要物理硬件提供CPU和内存，这会导致虚拟机耗尽应用程序所需的资源。</p><p>&nbsp;</p><p>采用了容器之后就不再需要客户操作系统了，因为容器引擎可以与一个或多个容器共享物理操作系统。与虚拟机相比，这是一个巨大的优势，因为现在更多的资源可用于应用程序。</p><p>&nbsp;</p><p>容器镜像是由开发人员创建的，其中包含了关于容器应该如何运行的指令。然后，开发人员可以使用容器镜像来启动运行实际应用程序的容器。它包含了可以在完全隔离的环境中运行的可执行代码。应用程序的库和依赖项也打包在镜像中。Docker、Amazon ECS、Kubernetes和GCP Autopilot是主要的容器化平台。</p><p>&nbsp;</p><p></p><h1>什么是无服务器函数</h1><p></p><p>&nbsp;</p><p>顾名思义，<a href=\"https://www.techtarget.com/searchitoperations/definition/serverless-computing\">无服务器计算</a>\"是计算机科学的一种范式，即开发人员不需要自己管理服务器，而由第三方服务来管理服务器，这样开发人员就可以更多地关注应用程序逻辑，不需要操心与维护服务器相关的问题。<a href=\"https://www.geeksforgeeks.org/serverless-computing-and-faas-model-the-next-stage-in-cloud-computing/\">无服务器函数</a>\"，一般来说，是对容器的进一步抽象，两者使用的是相同的底层技术。</p><p>&nbsp;</p><p></p><h4>无服务器计算的工作原理</h4><p></p><p>&nbsp;</p><p>如果没有物理服务器，无服务器计算是不可能实现的——只是开发人员不需要与它们交互，因为供应商（通常是像AWS、Azure或GCP这样的云供应商）承担了管理服务器的工作。通常，无服务器基础设施中是有容器的。例如，AWS开源了他们的Rust项目，用于处理函数的实例化（firecracker）和用于承载容器节点的轻量级操作系统（<a href=\"https://github.com/bottlerocket-os/bottlerocket\">bottlerocket</a>\"）。在这种范式中，供应商为开发人员提供了一个可以编写和提交代码的环境。平台负责执行、分配物理内存、CPU和输出，并根据应用程序运行的时间以及内存和CPU消耗向用户收费。这是一个巨大的优势，因为与自己运行或管理基础设施相比，它大大降低了成本。需要注意的是，无服务器函数也利用了底层的容器。在你启动无服务器函数时，云平台会启动容器，应用程序将在容器中运行。因此，底层技术是相似的，只是环境的部署、扩展和使用方式不同。</p><p>&nbsp;</p><p></p><h1>容器与无服务器之间的基本区别</h1><p></p><p>&nbsp;</p><p>你可以在下表中看到容器与无服务器函数之间的一些关键区别。</p><p></p><p></p><p>&nbsp;</p><p></p><h1>用例</h1><p></p><p>&nbsp;</p><p>容器和无服务器计算有几种日常的应用场景。有些解决方案可以通过使用容器或无服务器函数来实现。不过，主要的区别在于容器和无服务器函数的定价方式、与云端其他应用程序和服务的集成以及基于负载进行伸缩的能力。</p><p>&nbsp;</p><p>例如，如果你的应用程序是一个基于IP地址查找地理位置的短时间进程，那么它可以是无服务器的。随着请求数量的增加，无服务器函数将会自动伸缩，无需任何人工干预。如果是Web应用程序，需要长时间保持运行，那么容器可能更合适。</p><p>&nbsp;</p><p>此外，需要更快访问存储的进程可以使用容器，因为它们可以与文件系统集成。一个例子是集成运行在<a href=\"https://aws.amazon.com/ecs/\">Amazon ECS</a>\"上的容器，这些容器可以使用高度可伸缩的<a href=\"https://aws.amazon.com/efs/\">EFS</a>\"或<a href=\"https://azure.microsoft.com/en-us/products/storage/files/?&amp;ef_id=Cj0KCQiA_P6dBhD1ARIsAAGI7HATZ9tCTK8y92QTGXj8XhqLG43y7c4K3U3vPgCZ_O1-stL3he0RA2gaAngmEALw_wcB:G:s&amp;OCID=AIDcmm81syc84i_SEM_Cj0KCQiA_P6dBhD1ARIsAAGI7HATZ9tCTK8y92QTGXj8XhqLG43y7c4K3U3vPgCZ_O1-stL3he0RA2gaAngmEALw_wcB:G:s&amp;gclid=Cj0KCQiA_P6dBhD1ARIsAAGI7HATZ9tCTK8y92QTGXj8XhqLG43y7c4K3U3vPgCZ_O1-stL3he0RA2gaAngmEALw_wcB\">Azure Files</a>\"进行存储。</p><p>&nbsp;</p><p>下面的决策树可能有助于决定选择容器还是无服务器函数。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/48/48eb63a71a12a5f31ed5210cb14d482d.png\" /></p><p></p><p>图2 选择容器或无服务器函数的简单决策树</p><p>&nbsp;</p><p>两者之间可能存在重叠，下面将讨论容器和无服务器函数的一些常见用例。</p><p>&nbsp;</p><p></p><h4>容器</h4><p></p><p>&nbsp;</p><p>编程语言支持</p><p>&nbsp;</p><p>当你将应用程序打包到容器中，它就与平台无关了。无服务器函数支持一些最常见的运行时环境，如Java、Python、Go等。但是，如果无服务器函数的运行时不支持应用程序所使用的编程语言，那么容器将是理想的解决方案。容器可以使用任何容器编排平台（如Kubernetes）进行部署。</p><p>&nbsp;</p><p>托管长时间运行的应用程序</p><p>&nbsp;</p><p>容器更容易运行需要作为服务长时间运行的Web应用程序。例如，跟踪网站用户行为的跟踪应用程序就可以使用容器部署。容器可以根据生成的事件进行伸缩。我们还可以利用Kubernetes、AWS Fargate等流行技术来编排容器。</p><p>&nbsp;</p><p></p><h4>无服务器</h4><p></p><p>&nbsp;</p><p>API端点</p><p>&nbsp;</p><p>组织可以使用无服务器应用程序为Web或移动服务器部署API端点。这些API可以是无状态的和短生命周期的，可以根据事件触发。</p><p>&nbsp;</p><p>物联网处理</p><p>&nbsp;</p><p>随着家庭和工业自动化的发展，物联网设备的使用显著增加。这些物联网设备可以利用无服务器计算的强大功能。</p><p>&nbsp;</p><p>事件流</p><p>&nbsp;</p><p>在实时事件流场景中，需要根据特定条件填充或过滤事件，开发人员可以使用短生命周期的无服务器函数。然后，你可以使用这些函数检查事件是否有效，或者用人口统计信息来填充事件，等等。</p><p>&nbsp;</p><p></p><h1>总结</h1><p></p><p>&nbsp;</p><p>容器和无服务器应用程序都是可以让开发人员受益的与云无关的工具。根据经验，容器提供隔离性和灵活性，而无服务器有助于开发，并帮助你以最小的运行时成本进行自动伸缩。</p><p>&nbsp;</p><p>选择使用容器还是无服务器应用程序取决于具体的用例。例如，如果需要构建一个提供快速、短时间响应的API服务器，那么无服务器框架会更好。如果应用程序需要始终可用和运行，那么选择容器化应用程序更合适。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/containers-serverless-rivals-cohorts/\">https://www.infoq.com/articles/containers-serverless-rivals-cohorts/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/1409ffa52d932d257331bb2b5\">通通透透看无服务器计算：由来、场景和问题</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651119126&amp;idx=4&amp;sn=6c3234ee4dad81689bee5f294fd5c08c&amp;chksm=bdb926458aceaf537525b2f047f19ef85c4427f335e8b3281868719cd579fa0f251f1b6bbae4&amp;scene=27#wechat_redirect\">无服务器应用程序开发的最新趋势</a>\"</p><p><a href=\"https://xie.infoq.cn/article/a05c7f3e5c826d912391ea842\">无需服务器开发，实现设备状态缓存方案——实践类</a>\"</p><p><a href=\"https://www.infoq.cn/article/NIKdzlRr6VqIjFFZgseM\">递归无服务器函数是云端最大的计费风险？</a>\"</p>",
    "publish_time": "2023-04-13 10:24:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "训练成本300美元、比肩ChatGPT和Bard，低成本开源聊天机器人Vicuna来了",
    "url": "https://www.infoq.cn/article/cSP1Qaue8AbEhQY1QiQ5",
    "summary": "<p>大语言模型（LLM）的快速发展彻底颠覆了聊天机器人系统，实现了前所未有的智能水平。OpenAI的ChatGPT就是其中最典型的代表。虽然ChatGPT的性能令人印象深刻，但目前我们并不清楚ChatGPT的训练和架构细节，这也阻碍了该领域的研究和开源创新。</p><p></p><p>受Meta LLaMA和Stanford Aplaca项目的启发，近日，来自加州大学伯克利分校、卡内基梅隆大学、斯坦福大学、加州大学圣迭戈分校的研究人员们共同推出了一个开源聊天机器人Vicuna-13B。这是一款通过LLaMA模型微调和ShareGPT用户共享对话训练而成的开源聊天机器人。以GPT-4作为比照对象的初步评估表明，Vicuna-13B的质量可达OpenAI&nbsp;ChatGPT和Google Bard的90% 以上，并在超过90% 的情况下优于LLaMA和Stanford Alpaca等其他模型。</p><p></p><p>值得一提的是，Vicuna-13B的训练成本仅为300美元（约合2062元人民币）。目前，Vicuna-13B训练和服务代码及在线演示可向非商业用例开放。</p><p></p><p>在首个版本中，研究人员将在GitHub repo上共享训练、服务和评估代码:&nbsp;<a href=\"https://github.com/lm-sys/FastChat\">https://github.com/lm-sys/FastChat</a>\"。</p><p>Vicuna-13B模型的权重链接：<a href=\"https://github.com/lm-sys/FastChat#vicuna-weights\">https://github.com/lm-sys/FastChat#vicuna-weights</a>\"</p><p>Vicuna-13B演示链接：<a href=\"https://chat.lmsys.org/\">https://chat.lmsys.org/</a>\"</p><p></p><h2>Vicuna是如何炼成的？</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78e03c0190879cc88ee1e6544efad078.png\" /></p><p></p><p>在一篇文章中，研究人员介绍了整体的工作流程。</p><p></p><p>首先，研究人员从ShareGPT.com（一个供用户分享ChatGPT对话内容的网站）收集了约7万个对话，并增强了Alpaca提供的训练脚本，以更好地处理多轮对话和长序列。训练是在一天内通过8个A100 GPU配合PyTOrch FSDP完成的。为了提供演示服务，研究人员建立起一个轻量级的分布式服务系统，创建了一组80个不同问题，利用GPT-4来判断模型输出，借此对模型质量做初步评估。</p><p></p><p>为了比较两套不同模型，研究人员将各个模型的输出组合成各问题的单一提示，再将提示发送至GPT-4，由GPT-4评估哪个模型做出的响应更好。LLaMA、Alpaca、ChatGPT和Vicuna的具体比较如下表一所示。</p><p></p><p>表一：几大知名模型间的性能比较</p><p></p><p></p><p></p><p>前文提到，Vicuna是通过从ShareGPT.com的公共API收集到的约70K用户共享对话对LLaMA基础模型微调而成。为了确保数据质量，研究人员将HTML转换回markdown并过滤掉了一些不合适或低质量的样本。此外，研究人员还将冗长的对话拆分成多个小部分，以适应模型所能支持的最大上下文长度。</p><p></p><p>训练方法以Standford Alpaca为基础，并做出以下改进。</p><p></p><p>内存优化：为了使Vicuna能够理解长上下文，研究人员将Alpaca的最大上下文长度从512扩展至2048，但这也大大增加了GPU内存需求。研究人员利用梯度检查点和闪存注意力的方式来解决内存压力。多轮对话：研究人员调整训练损失以考虑多轮对话，并仅根据聊天机器人的输出计算微调损失。通过竞价实例降低成本：40倍的大规模数据集和4倍的训练序列长度对训练成本提出了很大挑战。为此研究人员使用SkyPilot托管点来降低成本，希望使用更便宜的竞价实例并配合自动恢复以抢占/切换区域。该解决方案将7B模型的训练成本从500美元削减至140美元左右，将13B模型的训练成本从1000美元削减至300美元。研究人员构建了一套服务系统，能够使用分布式工作节点为多个模型提供服务，它支持来自本地集群和云GPU工作节点的多种灵活插件。通过使用SkyPilot中的容错控制器和托管点功能，这套服务系统能够很好地与来自多种云环境的低成本竞价实例配合运作，借此降低服务成本。其目前还只是轻量级实现，研究人员正努力将更多最新研究成果集成进来。</p><p></p><h2>Vicuna的优势与局限性</h2><p></p><p></p><p>研究人员展示了Alpaca和Vicuna在基准问题上的回答示例。在使用70K用户共享的ChatGPT对话对Vicuna进行微调之后，与Alpaca相比，Vicuna能够给出更详尽、结构更合理的答案（参见下图），且质量几乎与ChatGPT持平。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/74/74460935342eed9b6adb44d701a28163.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bdb3a0d82d56779b23972f4ab71fc59b.png\" /></p><p></p><p>然而，评估聊天机器人绝非易事。随着GPT-4的最新进展，研究人员很好奇其能力是否已经达到了与人类相仿的水平，甚至可用于实现基准生成及性能评估的自动化框架。初步调查发现，在比较聊天机器人的答案时，GPT-4可以给出非常一致的排名和详细评估（参见上图中的GPT-4判断示例）。</p><p></p><p>基于GPT-4的初步评估（见下图），可以看到Vicuna的能力已经达到Bard/ChatGPT的90%。虽然这套框架能在一定程度上反映聊天机器人的潜力，但方法本身并不够严格。目前为聊天机器人建立评估系统仍是一个悬而未决的难题，尚需进一步研究。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/279867d064eaa9b61afdab21d3f90a6f.png\" /></p><p></p><p>与其他大语言模型类似，Vicuna也具有一定局限性。例如，它不擅长涉及推理或数学的任务，而且在确切识别自身或确保所输出事实的准确性方面可能存在局限。此外，它并没有得到充分优化以保证安全性，或缓解潜在的毒性或偏见。为了解决安全问题，研究人员使用OpenAI的审核API来过滤掉在线演示中的不当用户输入。尽管还有问题，但研究人员预计Vicuna将作为未来解决这些局限的开放起点。</p><p></p><h2>如何评估聊天机器人？</h2><p></p><p></p><p>对AI聊天机器人的评估是个老大难问题，需要验证其语言理解、推理和上下文感知能力。随着AI聊天机器人变得越来越先进，当前的开放基准可能不足以做出准确判断。例如，Standford Alpaca使用的评估数据集self-instruct，在领先聊天机器人中就能得到有效解答，导致人类很难辨别各模型间的性能差异。更多限制还包括训练/测试数据污染及创建新基准可能带来的高昂成本。</p><p></p><p>为了解决这些问题，研究人员提出了一套基于GPT-4的评估框架，借此自动评估聊天机器人性能。</p><p></p><p>首先，研究人员设计了8种问题类型，包括费米问题、角色扮演场景及编码/数学任务，借此测试聊天机器人的各方面性能。通过认真设计的提示工程，GPT-4得以生成基准模型难以解决的多样化、极具挑战的问题。研究人员为各个类别具体选择10个问题，并从5款聊天机器人处收集答案：LLaMA、Alpaca、ChatGPT、Bard以及Vicuna。</p><p></p><p>之后，研究人员要求 GPT-4根据指导性、相关性、准确性和细节度秋评估答案质量。最终发现，GPT-4不仅能够生成相对一致的评分，还能具体解释为什么给出这样的评分（详见<a href=\"https://vicuna.lmsys.org/eval\">https://vicuna.lmsys.org/eval</a>\"）。但是，研究人员也注意到GPT-4似乎不太擅长判断编码/数学任务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e97267d558bbfbe7df3ef384646ec8f1.png\" /></p><p></p><p>如上图所示，为各基准与Vicuna间的比较结果。在超过90%的问题中，GPT-4更支持Vicuna而非其他先进开源模型（LLaMA和Alpaca）的答案，而且在性能上与专有模型（ChatGPT、Bard）等相差不大。在45%的问题中，GPT-4都将Vicuna的回答评为优于或等于ChatGPT的回答。GPT-4会以10分为满分为各个回答做出定量评分，因此研究人员将各个模型在80个问题上获得的分数相加以计算其与Vicuna之间的总分比较。</p><p></p><p>如表二所示，Vicuna的部分为ChatGPT的92%。尽管近来取得巨大进步，但这些聊天机器人仍有自己的局限，例如难以解决基本的数学问题且编码能力有限。</p><p></p><p>表二：GPT-4给出的评估总分</p><p></p><p></p><p></p><p>虽然这套评估框架确实具有一定的聊天机器人评估潜力，但尚不足以作为严格或成熟的评判方法，特别是无法处理大语言模型容易产生的幻觉问题。为聊天机器人开发一套全面、标准化的评估系统，仍是一个悬而未决、有待进一步研究的问题。</p><p></p><p>参考链接：</p><p><a href=\"https://vicuna.lmsys.org/\">https://vicuna.lmsys.org/</a>\"</p>",
    "publish_time": "2023-04-13 10:37:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]