[
  {
    "title": "最高法判决引发热议：国内首例GPL侵权案背后的开源与创新之争｜讨论",
    "url": "https://www.infoq.cn/article/QbzYObG5FaaWa4JYx2Md",
    "summary": "<p>近日，最高人民法院发布了一则关于开源软件著作权纠纷案的判决，引发了业内人士的广泛关注和热议。该案中，被告公司基于 GPLv2 协议提出了不侵权抗辩，但最终被法院认定为侵权。关于本案详细报道，请移步InfoQ 报道文章“<a href=\"https://www.infoq.cn/article/PLtyHpFh4tXuJqs0yjWs\">最高人民法院：这份判决给软件开发者吃了定心丸？</a>\"”</p><p></p><p>本案的争议焦点在于，软件开发者自身存在一定程度上的违反 GPLv2 协议，是否就一定不享有新研发软件著作权呢？</p><p></p><p>支持者认为， 该判决有助于明确开源软件的法律地位，确保了开发者的劳动成果得到应有的尊重和保护。它强调了即使在开源环境下，开发者对其作品的独创性贡献仍然享有著作权，这有助于激励创新和保护知识产权。开源协议并非凌驾于法律之上的“免责条款”，开发者不能以此为借口侵犯他人著作权。此外，判决也有助于规范市场行为，防止不正当竞争，维护公平的市场秩序。</p><p></p><p>反对者则担心， 这样的判决可能会对开源社区的开放性和协作精神产生负面影响。他们认为，开源软件的核心价值在于自由分享和共同进步，如果对违反开源协议的行为过于严苛，可能会限制开发者的自由，抑制社区的活力和创新。此外，过于严格的法律约束可能会让一些潜在的开发者和企业对参与开源项目产生顾虑，从而影响开源软件的整体发展。</p><p></p><p>小编认为， 最高人民法院的判决在一定程度上确实为开源软件的著作权保护提供了指导原则，尤其是在区分软件开发者违反GPLv2协议与他人侵犯其著作权行为的独立性方面。这有助于软件开发者在遵守开源协议的同时，保护自己的合法权益。判决强调了在软件尚未被开源的情况下，软件开发者基于其独创性贡献享有的著作权不应受到不合理的限制。该判决在一定程度上平衡了软件开发者权益保护与开源社区建设之间的关系，具有积极意义。</p><p></p><p>但同时，该判决也存在一些值得探讨的地方。例如，GPLv2协议涉及的技术细节和法律解释往往非常复杂，特别是在判断软件是否构成“衍生作品”（derivative work）时，需要对软件的源代码进行深入的技术分析，而本案的案卷材料可能尚不充分。最高人民法院的判决具有重要的指导意义，但在具体案件中，如何将判决原则应用到其他类似情况，可能还需要进一步的司法实践和案例积累。</p><p></p><p>关于本案判决对开源软件著作权保护的影响，你的看法是什么呢，欢迎投票或评论分享。</p><p></p><p></p><p></p><p></p><p>欢迎加入 InfoQ 读者技术交流群，与志同道合的朋友一起探讨知识，交流经验。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e6/36/e64f5643365f9a9fedd63326ff5de736.png\" /></p><p></p>",
    "publish_time": "2024-02-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Sora生成的视频太真实？那是你遇到造假了",
    "url": "https://www.infoq.cn/article/1M4s64scpG5ifvX7qfAT",
    "summary": "<p></p><p><img src=\"https://static001.infoq.cn/resource/image/2b/3c/2b9dd247c1dea290fe83f6f2996b033c.gif\" /></p><p></p><p></p><p>视频发布者“No Context Brits”表示这是 Sora 生成的，提示词是：Brit gets hit by a bus then goes for a pint。那么你认为，上面视频是真的还是 AI 生成的？</p><p></p><p></p><p></p><p>这个问题的答案，我们留到最后揭晓。</p><p></p><h3>现实真的不存在了吗？</h3><p></p><p></p><p>当大家都在说 Sora 颠覆行业的时候，Sora 究竟能颠覆多少？我们由易到难，看看 Sora 制作的视频，可以达到什么级别。</p><p></p><h4>风景</h4><p></p><p></p><p>风景类视频制作可以说是入门级，画面细节要求相对少一些，构图、运镜相对比较重要。而 Sora 确实能制作出纪录片里常用到的运镜方式，构图也是参照了构图规则的：</p><p><img src=\"https://static001.infoq.cn/resource/image/c4/fa/c4b91174f7fcd49fff4d3d898d548ffa.gif\" /></p><p></p><p></p><p></p><p>可以简单看下《地球脉动》第二季第一集的开头片段：</p><p><img src=\"https://static001.infoq.cn/resource/image/52/3f/52ea660833f22fcc53acb89e1ff3363f.gif\" /></p><p></p><p>同时，与视频生成领域的其他同行比，Sora 在真实性、连续性上的进步也是很明显的：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b9/b915114aec59b9b33722cf28523d2b13.gif\" /></p><p></p><p></p><h4>动物</h4><p></p><p></p><p>在 OpenAI Sora 研发成员 Aditya Ramesh 发出的一个关于一只蚂蚁“在蚁巢内部移动的视角镜头”的视频里，Sora 给出了如下效果：</p><p></p><p></p><p>这个视频犯了基础的认知错误：里面的蚂蚁只有四条腿，真实世界里的是六条腿。杨立昆（Yann LeCun）也直接指出了这一点，但仍止不住网友对视频效果的赞叹。</p><p></p><p>题外话：Aditya 与 LeCun 也有一段缘分。据 LeCun 爆料，Aditya 本科就读于纽约大学，并参加过其实验室的一些项目。</p><p></p><p>下面这只“飞入海底的蝴蝶”，虽然没有尊重基本事实（毕竟蝴蝶没入海底怕是飞不起来），但如果是特效，那还是可以的：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/30/30f9e5da5c031b147db08da2eaf758ff.gif\" /></p><p></p><p>一只寻找庇护所的流浪猫：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7e/7e1f77c38386a684b9fac9a58a7d6775.gif\" /></p><p></p><p>在单只动物的相对简单的场景里，Sora 表现还是不错的。</p><p></p><h4>人物</h4><p></p><p></p><p>在最新发布的 Sora 生成视频里，有一个体现人类惊讶表情的视频，但效果不太好：鲨鱼在离沙滩特别近的沙滩出现，女人夸张的惊讶……“那个女人比鲨鱼更让我害怕，制作恐怖电影可能是 Sora 的最佳用途。”网友评价。另外，这个视频的逻辑还需要提示词输入进行调整，比如男人的无动于衷。</p><p></p><p></p><p></p><p></p><p>下面这个老人过生日的视频应该很多人见过，效果相对还是相对丝滑一些的，虽然老人吹蜡烛时，烛光动也没动……</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f6/f626e0dfcdfcb287a054cf25273c626d.gif\" /></p><p></p><p>这个猫和主人互动的视频里，猫挠到主人鼻子时，鼻子的变化给人感觉像一张纸。另外，她不疼吗？！</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/37/37c73c5a1a23529e109be29afd6d5c0a.gif\" /></p><p></p><p>更复杂一些的场景，我们看看 Sora 的一镜到底：</p><p></p><p></p><p></p><p></p><p>“几乎完美。但是吹毛求疵，这里的视角不太好。看起来用餐的人坐在一个小型市场旁边。”有敏锐的网友指出：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/82/8227305e455b1c73737aa477818b6b30.png\" /></p><p></p><p>“大多数人身上都发现了人工制品和某种程度的幻觉。”复杂场景下，Sora 还是做不到完美。</p><p></p><h4>特效</h4><p></p><p></p><p>特效视频就不存在真实性问题了，视觉效果是重要的衡量因素。</p><p></p><p>Sora 研发团队 Bill Peebles 发布了一只“科技犬”视频：未来控制论德国牧羊犬的特写镜头，展示了其引人注目的棕色和黑色皮毛…</p><p></p><p></p><p></p><p>一位数字艺术方面的从业者表示，“这看起来比我们见过的任何 CGi 都更真实。迫不及待地希望能够尽快将视频制作变为 3D 模型，这样我们就可以在游戏中拥有这些资源和动画。”也有网友调侃道，“本次拍摄中没有动物受伤。”</p><p></p><p>Bill 还发布了另一个特效视频：“一座巨大的大教堂里全是猫。放眼望去，到处都是猫。一个男人走进大教堂，向坐在王座上的巨型猫王鞠躬。”在经过网友增加旁白和配音后，便是这样的：</p><p></p><p></p><p>旁白 @ChatGPTapp</p><p>配音者 @elevenlabsio</p><p>音乐由 @suno_ai_</p><p></p><p>如果有一天，OpenAI 能够直接将视觉效果和听觉效果一起输出，那又会是震惊行业的一件大事。可以看下，网友给 Sora 视频加上视觉效果是什么样的：</p><p></p><p></p><p></p><p></p><p>Sora 研发团队另一位重要成员 Tim Brooks 用 Sora 让沙盒游戏《我的世界》拥有了“有史以来最华丽的高分辨率 8k 纹理包”：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a5/a50113c3cc591d5d61279e30906d18cb.gif\" /></p><p></p><p>同时，Tim 还让《我的世界》视频融合进摩托车视角，“这个功能有如此大的创造潜力”Tim 说道。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3c/3ceb4b6f9f3445481a1817101d47b7b3.png\" /></p><p></p><p>其实效果已经不错，有网友建议可以在提示中加上“光线追踪、光晕、后期特效”等，这样效果可能会更好。</p><p></p><p>下面是一个 Sora 改变视频的风格和环境的例子，一辆跑车穿梭在水底、恐龙乐园、像素世界等等场景中：</p><p></p><p></p><p></p><p>“一只鸭子走在波士顿的街道”，如果更加复杂一些，会不会有漫威的感觉？</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e5/e5fc56900721b672725e46414c1c4cf2.gif\" /></p><p></p><p>“在叶子上行驶的火车”，叶子的脉络还真是跟清晰的，当然也有网友认为这种视频没有什么用，更多是一种数字垃圾。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/da/daf9db1bda75b14a536c74ece4a20013.gif\" /></p><p></p><p></p><h3>谢赛宁：Sora 跟我没关系</h3><p></p><p></p><p>Sora 能有上面的效果，主要得益于 DiT 架构和 Spacetime Patch。</p><p></p><p>其中，Spacetime Patch 建立在 GoogleDeepMind 对 NaViT（原生分辨率视觉 Transformer）和 ViT（视觉 Transformer）的早期研究基础上。Patch 可以理解为 Sora 的基本单元，类比 Token。Sora 处理一系列的 Patch，并预测出序列中的下一个 Patch。</p><p></p><p>Sora 团队发现补丁是一种高度可扩展且有效的表示形式，因此通过 Spacetime Patch 将视频视为补丁序列，捕捉视觉数据使模型能够从更准确的表达中学习。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/07/076c9f430f3eac29557102a07de46702.png\" /></p><p></p><p>从 OpenAI 的技术报告可知，Sora 的作者团队有 13 位成员，如今被报道最多的核心成员包括研发负责人 Tim Brooks、William Peebles、系统负责人 Connor Holmes 等。</p><p></p><p>其中，Tim Brooks 是 DALL-E 3 作者之一，GitHub 5.7k️星项目 InstructPix2Pix 作者，博士毕业于 UC Berkeley 的伯克利人工智能研究所 BAIR。Tim 曾在谷歌为 Pixel 手机摄像头提供 AI 算法，也在英伟达负责过视频生成模型的研究。</p><p></p><p>William Peebles 也来自 UC Berkeley，去年（2023 年）刚刚获得博士学位。据悉，William 和谢赛宁合作，研发了 DiT。也因为这个关系，毕业于上海交大的天才少年谢赛宁被报道为是 Sora 的研发者之一。谢赛宁本人对此强烈否认：“一点关系都没有”。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/13/13a3f99c9c6993f85cb16137b008b013.jpeg\" /></p><p></p><p>Connor Holmes 则曾在 Colorado School of Mines、微软工作过，在 LLM、BE RT 风格的编码器、RNN 和 UNets 方面有丰富经验。“我期待解决在扩展深度学习工作负载以进行推理和训练时系统效率低下的问题。”他在自己的领英上说道。此外，Sora 团队的不少成员都是 DALL-E 3 的作者，包括两位华人 Li Jing 和 Yufei Guo。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>“如何加入红队？我可以帮助测试”有积极参与的人，也有不喜欢生成视频的人：“我看视频，不是想看虚拟的世界，而是想通过镜头去看自己不了解的真实的世界。”</p><p></p><p>现在网上也出现了很多声称是 Sora 生成的视频，但其实并不是。比如下面这个女团视频声称是Sora生成的，但真实性存疑。</p><p></p><p></p><p></p><p>来源：https://twitter.com/ViLettuce/status/1758976415150559638</p><p></p><p>还比如下面视频的发布者“víty”表示这个“女生吃面包时与他人发生争执”视频是 Sora 生成的，提示词是：𝘞𝘩𝘪𝘵𝘦 𝘸𝘰𝘮𝘢𝘯 𝘦𝘢𝘵𝘴 𝘣𝘳𝘦𝘢𝘥，𝘢𝘶𝘯𝘵 𝘣𝘪𝘵𝘤𝘩𝘴𝘭𝘢𝘱、𝘸𝘩𝘪𝘵𝘦 𝘸𝘰𝘮𝘢𝘯 𝘭𝘢𝘶𝘨𝘩𝘴、𝘱𝘪𝘢𝘯 𝘰𝘥𝘶𝘩𝘩，𝘩𝘰𝘶𝘴𝘦𝘦𝘷𝘪𝘤𝘵𝘪𝘰𝘯，𝘤𝘰𝘰𝘭𝘣𝘢𝘴𝘴𝘰𝘶𝘵 𝘳𝘰𝘮𝘶𝘴𝘪𝘤。</p><p></p><p>但有网友指出，这个视频并非 Sora 生成的，而是来源于一部名为《Ti Ti Ti》的肥皂剧。看过这部剧的朋友可以出来说说～</p><p></p><p></p><p>来源：<a href=\"https://twitter.com/vvvorvvtorvitor/status/1758654081176866906\">https://twitter.com/vvvorvvtorvitor/status/1758654081176866906</a>\"</p><p></p><p>回到文章最初问到的问题，其实帖子下面也引起了网友的各种讨论，有人说是真的，有人说是生成的。而真正的答案就是：那是真实的视频。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/46/464a491c0577c42dacd774797cf8de72.png\" /></p><p></p><p>出自外媒 The Guardian 在 2017 年的报道：</p><p></p><p><a href=\"https://www.theguardian.com/global/video/2017/jun/27/man-hit-by-bus-in-reading-survives-without-injury-video\">https://www.theguardian.com/global/video/2017/jun/27/man-hit-by-bus-in-reading-survives-without-injury-video</a>\"</p><p></p><p>你猜对了吗？</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://twitter.com/minchoi/status/1758831971726225591\">https://twitter.com/minchoi/status/1758831971726225591</a>\"</p><p><a href=\"https://twitter.com/NoContextBrits/status/1759212202853040265\">https://twitter.com/NoContextBrits/status/1759212202853040265</a>\"</p><p><a href=\"https://openai.com/research/video-generation-models-as-world-simulators\">https://openai.com/research/video-generation-models-as-world-simulators</a>\"</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f9/f9e4f42a4db5c560a5e031ced2a5ac56.png\" /></p><p></p><p></p><p></p>",
    "publish_time": "2024-02-20 12:21:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Sora 横空出世，给科技型企业带来的 3 点启示 | Q 推荐",
    "url": "https://www.infoq.cn/article/JaYGkUL3slZtQIivxcWQ",
    "summary": "<p>开工第一周，OpenAI 发布的“文生视频”（text-to-video）的工具，Sora ， 可谓刷爆各个平台。Sora 的出现，或意味着 AGI（通用人工智能）实现将从 10 年缩短到 1 年。从 ChatGPT 到 Midjournery 再到 Sora ，每一次新技术的出现便会有相关从业者的开始焦虑。然而，面对层出不穷的 AI 技术，作为科技型企业又该如何应对呢？</p><p></p><p>培养前瞻性思维，坚持技术创新：Sora 的推出强调了持续技术创新的重要性，对科技型企业来说，这意味着需要不断探索和实验新技术，以维持在竞争激烈的市场中的领先地位。夯实技术基础，稳步实现 AI 融合：建立现实的技术应用视角，避免对热点技术抱有不切实际的高期望，而忽视了技术应用的现实挑战和局限性。持续学习和适应，打造成长型团队：Sora 展示了技术的快速进步，对科技型企业来说，这意味着需要在组织内建立持续学习和适应新技术的文化，以便快速响应行业变化和技术升级。据《麻省理工科技评论》报道，具备成长型思维的团队在面对 AI 变革时，其创新能力和业务增长速度均超过其他团队。例如，谷歌的 DeepMind 团队正是因为其成长型思维和强大的创新能力，成功研发出了 AlphaGo 等颠覆性 AI 技术，为企业带来了巨大的商业价值。</p><p></p><p>应对科技企业培养成长型团队的需求，极客时间企业版推出“领跑 2024，企业数字化人才培养福利活动”，精选了涵盖 AI 大模型应用、数字化转型实战、领导力与企业管理策略等多元维度的在线课程。这些课程不仅由行业专家亲自授课，还结合了丰富的实战案例和前沿的技术动态，确保技术团队能够学到最实用、最前沿的知识。</p><p></p><p>即日起至活动结束，参与活动的企业可以免费获得 100 个学员账号！团队成员可以在 2 月底之前，自由选择平台上的 6 门课程进行深度学习。立即扫码，提升团队整体实力，无惧迎接 AI 时代挑战。</p><p>↓↓↓扫描下图二维码参加活动↓↓↓</p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4c5fae03a44b50caf660c0e96065a68.png\" /></p><p></p><p>培养 AIGC 应用能力</p><p>从系统实战到应用开发，探索 AI 大模型无限可能</p><p>掌握生产级 AI 系统研发能力</p><p>手把手带你用 LLM 提升研发效率</p><p>一站式掌握硬件、理论、LangChain 开发框架和项目实战技能</p><p>全面解析微调技术理论，掌握大模型微调核心技能</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5eaa08a89078e7b3b4d8dd62cb0b16f0.png\" /></p><p></p><p>完善岗位专项技能体系</p><p>行业大咖规划职业进阶学习路径</p><p>全方位提升实战技能，补足能力短板</p><p>从工作应用出发，紧跟前沿技术发展和行业趋势</p><p>涵盖从基础知识到高级应用的全链路学习</p><p>按学习路径选课</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be08f4c52878faad45a41cd8e2afb9bf.png\" /></p><p></p><p>按专项技能选课</p><p><img src=\"https://static001.geekbang.org/infoq/bf/bff22496a674fb9eca8a0ab2adf1797f.png\" /></p><p></p><p>提升技术对业务的驱动</p><p>向推动软件创新和变革的从业者学习</p><p>轻松获取源于实践、有效验证的优质内容</p><p>为项目汲取可落地的想法</p><p>验证企业软件开发路线图</p><p>掌握技术与业务创新深度融合的前沿案例</p><p>1400+ 顶尖技术团队实战案例</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c6e847069b59052dbf8363c7bbf3f08.png\" /></p><p></p><p>领跑 2024，数字化人才培养福利</p><p>6000 小时精选课程，</p><p>AI 大模型等前沿课程，</p><p>创新产品开发、架构设计等专业课程，</p><p>数字化领导力、数字化管理等领导课程，</p><p>数据分析等实用课程......</p><p>↓↓↓扫描下图二维码参加活动↓↓↓</p><p><img src=\"https://static001.geekbang.org/infoq/53/53485585ad6b50a70ddabad8d16bd69b.png\" /></p><p></p>",
    "publish_time": "2024-02-20 14:51:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动辟谣推出中文版Sora：还无法完善产品落地，距离国外模型有很大差距",
    "url": "https://www.infoq.cn/article/Suc3VpjPiSq6PfukWxZm",
    "summary": "<p>今日有消息称，在Sora引爆文生视频赛道之前，国内的字节跳动也推出了一款颠覆性视频模型——Boximator。与Gen-2、Pink1.0等模型不同的是，Boximator可以通过文本精准控制生成视频中人物或物体的动作。</p><p>&nbsp;</p><p>对此，字节跳动相关人士向媒体回应称，Boximator是视频生成领域控制对象运动的技术方法研究项目，目前还无法作为完善的产品落地，距离国外领先的视频生成模型在</p><p>&nbsp;</p><p>根据介绍，Boximator 可以通过文本精准控制生成视频中人物或物体的动作。例如，“小猫把自己藏进杯子里了”：</p><p></p><p></p><p></p><p></p><p>“由像素组成的角色正在跳舞”：</p><p></p><p></p><p></p><p></p><p></p><p>“一个红衣女孩用头骨遮住了脸”：</p><p></p><p></p><p></p><p></p><p>“一名年轻女子转过头，露出了她的侧脸”：</p><p></p><p></p><p></p><p></p><p>“蜘蛛侠向镜头摆动”：</p><p></p><p></p><p></p><p>根据论文介绍，Boximator使⽤ 3D U-Net 架构构建在视频扩散模型之上。3D U-Net 由交替的卷积块和注意⼒块构成。每个块包含两个组件：⼀个空间组件，负责将各个视频帧作为单独的图像进⾏处理；另外一个是时间组件，⽀持跨帧信息交换。</p><p>&nbsp;</p><p>为了实现对视频中物体、人物的动作控制，Boximator 使用了“软框”和“硬框”两种约束方法。其中，硬框可精确定义目标对象的边界框，软框则定义一个对象可能存在的区域, 形成一个宽松的边界框。</p><p>&nbsp;</p><p>控制模块可以将框约束的编码与视频帧的视觉编码结合，用来指导视频的精准动作生成。包含框编码器和自注意力层两大块。</p><p>&nbsp;</p><p>论文地址：<a href=\"https://arxiv.org/abs/2402.01566\">https://arxiv.org/abs/2402.01566</a>\"</p><p>&nbsp;</p><p>下面是研发人员给出的Gen-2、Pink1.0 和Boximator 的对比：</p><p></p><p></p><p></p><p></p><p></p><p>&nbsp;根据其<a href=\"https://boximator.github.io/\">在Github</a>\"上的信息，Boximator演示网站正在开发中，将在未来 2-3 个月内推出。</p><p></p>",
    "publish_time": "2024-02-20 14:57:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI模型家族更新：GPT-4训练数据至2023年12月",
    "url": "https://www.infoq.cn/article/AMfMVAJMZG9zlrboe8pf",
    "summary": "<p>近日，OpenAI 宣布 GPT-3.5-turbo、GPT-4以及GPT-4-turbo-preview等均指向最新模型版本。用户可以发送请求并查看响应对象来验证自己正在使用哪种模型。响应结果中包含所使用的特定模型版本（例如GPT-3.5-turbo-0613）。</p><p>&nbsp;</p><p>OpenAI还提供静态模型版本，开发人员可以在模型更新发布后的三个月内继续使用原有模型。随着模型更新的加快，OpenAI还开放了评估贡献通道，由用户针对不同用例协同进行模型改进。</p><p>&nbsp;</p><p>感兴趣的朋友请参阅OpenAI&nbsp;Evals&nbsp;repo：</p><p><a href=\"https://github.com/openai/evals\">https://github.com/openai/evals</a>\"</p><p>&nbsp;</p><p>关于弃用模型的更多详细信息，请参阅OpenAI官网上的弃用页面：</p><p><a href=\"https://platform.openai.com/docs/deprecations\">https://platform.openai.com/docs/deprecations</a>\"</p><p>&nbsp;</p><p></p><h4>GPT-4与GPT-4 Turbo</h4><p></p><p>&nbsp;</p><p>GPT-4是一套大型多模态模型（可接收文本或图像输入，并输出文本结果），目前通过OpenAI API 向付费客户开放。</p><p>&nbsp;</p><p>与GPT-3.5-turbo一样，GPT-4针对聊天进行了优化，因此可通过聊天完成以往必须借助Chat Completions&nbsp;API才能处理的任务。OpenAI在文本生成指南中专门介绍了如何使用GPT-4：</p><p><a href=\"https://platform.openai.com/docs/guides/text-generation\">https://platform.openai.com/docs/guides/text-generation</a>\"</p><p>&nbsp;</p><p></p><p>对于大部分基本任务，GPT-4和GPT-3.5模型间的差异并不明显。但在需要较复杂推理能力的情况下，GPT-4则拥有超越OpenAI此前各类模型的表现。</p><p>&nbsp;</p><p></p><h4>GPT-3.5 Turbo</h4><p></p><p>&nbsp;</p><p>GPT-3.5 Turbo模型能够理解并生成自然语言或者代码，针对Chat Completions API进行了聊天优化，但也同样适用于非聊天任务。</p><p></p><p>&nbsp;</p><p></p><h4>DALL·E</h4><p></p><p>&nbsp;</p><p>DALL-E是一套AI系统，能够根据自然语言的描述创建出逼真的图像与艺术效果。DALL-E 3目前支持根据提示词生成拥有特定尺寸的新图像。DALL-E 2还支持对现有图像进行编辑、或为用户上传的图像生成变体等功能。</p><p>&nbsp;</p><p>DALL-E 3可通过OpenAI的Images API同DALL-E 2配合使用。用户可通过ChatGPT Plus服务体验DALL-E 3。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p></p><h4>TTS</h4><p></p><p>&nbsp;</p><p>TTS是一种AI模型，能够将文本转换为听感自然顺畅的语音。OpenAI提供两种不同模型变量，其中tts-1针对实时文本到语音用例进行了优化，tts-1-hd则针对输出质量进行了优化。这些模型均可通过Audio API中的Speech端点配合使用。</p><p></p><p>&nbsp;</p><p></p><h4>Whisper</h4><p></p><p>&nbsp;</p><p>Whisper是一种通用语音识别模型，在包含多种音频的大型数据集上训练而成。它也是一套多任务模型，能够执行多语种语音识别、语音翻译与理解等任务。Whisper v2-large模型目前可通过API调用，模型名称为Whisper-1。</p><p>&nbsp;</p><p>目前，Whisper的开源版本与OpenAI通过API提供的版本完全一致。但API版本的推理过程经过优化，因此Whisper在API上的运行速度要比其他方式快得多。</p><p>&nbsp;</p><p>关于Whisper的更多技术细节，请参阅此论文：</p><p><a href=\"https://arxiv.org/abs/2212.04356\">https://arxiv.org/abs/2212.04356</a>\"</p><p>&nbsp;</p><p></p><h4>Embeddings</h4><p></p><p>&nbsp;</p><p>Embeddings是指文本的数字表示，可用于衡量两段文本之间的相关性。Embeddings即嵌入，往往在搜索、聚类、推荐、异常检测和分类任务中拥有良好表现。</p><p>&nbsp;</p><p>感兴趣的朋友可以在OpenAI的公告博文中了解关于最新嵌入模型的更多信息：</p><p><a href=\"https://openai.com/blog/new-embedding-models-and-api-updates\">https://openai.com/blog/new-embedding-models-and-api-updates</a>\"</p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>Moderation</h4><p></p><p>&nbsp;</p><p>Moderation审核模型负责检查内容是否符合OpenAI的使用政策。这些模型提供分类功能，用于查找以下类别的内容：仇恨、仇恨/威胁、自残、性、性/未成年人、暴力及暴力/图像。</p><p>&nbsp;</p><p>更多具体信息请参阅OpenAI审核指南：</p><p><a href=\"https://platform.openai.com/docs/guides/moderation/overview\">https://platform.openai.com/docs/guides/moderation/overview</a>\"</p><p>&nbsp;</p><p>审核模型可接受任意大小的输入，将输入自动拆分成4096个tokens的块。如果总输入超过32768个tokens，则使用截断技术处理。在极少数情况下，此类模型可能会在审核检查中忽略少量tokens。</p><p>&nbsp;</p><p>每条指向审核端点的请求仅显示各类别的最大值。例如，如果一个4k&nbsp;tokens块的分类得分为0.9901，而另一个块的得分为0.1901，则API响应结果将仅显示明显更高的0.9901。</p><p></p><p>&nbsp;</p><p></p><h4>GPT base</h4><p></p><p>&nbsp;</p><p>GPT base模型能够理解并生成自然语言或者代码，但并未接受指令遵循方面的训练。这些模型旨在替代OpenAI之前的GPT-3 base基础模型，且使用旧版Completions API。OpenAI推荐大多数用户直接使用GPT-3.5或者GPT-4。</p><p></p><p>&nbsp;</p><p></p><h2>使用政策</h2><p></p><p>&nbsp;</p><p>在用户数据处理上，OpenAI 强调用户数据始终归用户所有。</p><p>&nbsp;</p><p>自2023年3月1日起，发送至OpenAI API的数据将不会被用于训练或改进OpenAI模型（除非用户明确表示同意&nbsp;）。但若选择参与改进，那么模型可能随时间推移更加契合的用例。</p><p>&nbsp;</p><p>为了帮助识别滥用行为，API数据最多可保留30天，之后将被删除（除非法律另行要求）。对于用例较为敏感的可信客户，OpenAI亦提供零数据保留选项。在零数据保留情况下，请求与响应主体不会被持久保存在任何日志记录当中，而仅放置在内存内以支持服务需求。请注意，此数据政策不适用于OpenAI提供的非API消费级服务，例如ChatGPT或DALl-E Labs。</p><p>&nbsp;</p><p></p><h4>端点默认使用政策</h4><p></p><p>&nbsp;</p><p></p><p>*&nbsp;通过GPT-4-vison-preview模型输入的图像不符合零保留条件。</p><p>*&nbsp;对于Assistants API，OpenAI仍在beta期间评估默认保留周期。预计beta结束后将确定沿用默认的保留周期。</p><p>&nbsp;</p><p>关于更多详细信息，请参阅OpenAI的API数据使用政策：</p><p><a href=\"https://openai.com/policies/api-data-usage-policies\">https://openai.com/policies/api-data-usage-policies</a>\"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>模型端点兼容性</h4><p></p><p>&nbsp;</p><p></p><p>此列表不包含已被OpenAI弃用的各模型版本：</p><p><a href=\"https://platform.openai.com/docs/deprecations\">https://platform.openai.com/docs/deprecations</a>\"</p><p>&nbsp;</p><p>相关链接：</p><p><a href=\"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\">https://platform.openai.com/docs/models/GPT-4-and-GPT-4-turbo</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2024-02-20 16:56:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据要素×物流｜顺丰如何实现数据驱动流转效率提升【超级连麦·数智大脑】",
    "url": "https://www.infoq.cn/article/FIKfbul8LmVwJcfVQxfg",
    "summary": "<p>超级连麦，主持人高玉娴 （InfoQ极客传媒数字化主编） 对话特邀嘉宾林国强（顺丰科技大数据总监）就以下三大板块：<br />\n1.业务数字化与创新技术实践<br />\n2.数据管理挑战与应对策略<br />\n3.数据要素流通的价值与挑战<br />\n探讨数据要素×物流,看顺丰如何实现数据驱动流转效率提升。</p>",
    "publish_time": "2024-02-20 17:07:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "行知数字中国案例集锦（第三期）",
    "url": "https://www.infoq.cn/article/77RTy0iBNI85eANsBwXC",
    "summary": "<p>本手册共包括 “ 数智视野篇 ”、 “ 数智案例篇 ”、“ 数智技术篇 ”，以及 “ FCon 精选 ” 四个部分。其中，“数智案例篇”又划分为金融、制造、消费零售三个章节模块，分别从不同行业视角拆解数字化转型的最优路径，“ FCon 精选 ”则囊括了 FCon 大会上的重磅企业采访文章。</p>\n<p><strong>精彩内容抢先预览</strong></p>\n<p>数智视野篇</p>\n<p>01 ｜既要安全、又要创新，庞大且复杂的核电站数字化怎么破？</p>\n<p>02 ｜国货李宁的新数字化故事：如何利用技术做运动产品的研发？</p>\n<p>03 ｜ A0 级以上新能源汽车市占率 60%，全球齿轮制造大厂的数字化实践</p>\n<p>04 ｜涉及万亿元规模资产，华润集团数字化转型如何“大象转身”</p>\n<p>数智案例篇</p>\n<p>第一章：金融数字化</p>\n<p>05 ｜北京银行数字化转型：用近 2 年时间，从“搭梁立柱”到“积厚成势”</p>\n<p>06 ｜玉山银行数字化（上）：构建台湾地区第一个银行自建的“微服务架构”核心系统</p>\n<p>07 ｜玉山银行数字化（下）：普惠金融、智慧金融、场景金融三大 FINTECH 策略如何落地</p>\n<p>08 ｜银行券商经验不完全适用，中粮信托这样构建数字化中台</p>\n<p>09 ｜金融智能化实践的两个前提和两个关键</p>\n<p>10 ｜富滇银行：中小金融机构如何思考和实现 AIGC 应用？</p>\n<p>第二章：制造数字化</p>\n<p>11 ｜美的集团：关于工业数字化的最新思考和实践</p>\n<p>12 ｜百年工业巨头施耐德电气如何在数字世界“建工厂”</p>\n<p>第三章：消费零售数字化</p>\n<p>13 ｜专访华润啤酒首席数字官，探讨智能技术如何助力酒企数字化转型</p>\n<p>14 ｜自研模型大幅降低原奶调配成本，伊利如何做数字基建？</p>\n<p>数智技术篇</p>\n<p>15 ｜漏洞产生快于识别修复，开发者要如何预防安全问题？</p>\n<p>16 ｜美的集团最新 AI 实践：拟上线智能家居大模型，开源边端 AI 算法部署工具链</p>\n<p>17 ｜首个千亿医药对话大模型来了，要打破医药研发“三十定律”</p>\n<p>18 ｜ SOLIDWORKS：把 AI 放进工业设计软件，把软件放到云上</p>\n<p>FCON 精选</p>\n<p>19 ｜德邦基金 CTO 李鑫：中小金融机构数字化规划要聚焦重点、有所取舍</p>\n<p>20｜平安人寿魏政刚：算力与语料，是制约保险领域大模型应用的首要挑战</p>\n<p>21 ｜阳光保险张晗：大模型为保险业务全自动化创造了可能性</p>\n<p>22｜ 3 年探索与实践，苏州银行的数字人民币基础建设与场景应用怎么样了？</p>\n<p>23｜广发证券：金融科技浪潮下的 IT 架构升级之道</p>\n<p>24｜平安银行：云原生转型背景下的质效探索与思考</p>\n<p>25｜实现“数据资产当日达”，国泰君安证券做数据平台的逻辑与实践</p>\n<p>26｜中信银行：用 AI 搞定普惠型财富管理的高成本问题</p>\n<p>27｜民营银行生存法则“唯快不破”，华瑞银行风险特征计算平台如何做到实时响应</p>",
    "publish_time": "2024-02-20 17:14:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI的Sora注定死路一条！Yann LeCun火力全开怒喷：这种实现方式“完全没有现实意义”",
    "url": "https://www.infoq.cn/article/Gf8Z4CVHwvqLEOXGlY9c",
    "summary": "<p>近日，OpenAI 发布的视频生成模型 Sora 成为全球焦点。与以往只能生成几秒钟视频的模型不同，Sora 可生成长达 60 秒的高清视频。</p><p>&nbsp;</p><p>英伟达高级研究科学家 Jim Fan 断言，Sora 是一个数据驱动的物理引擎，是一个可学习的模拟器，或“世界模型”。OpenAI也声称Sora是“扩展视频生成模型是构建物理世界通用模拟器的一条可行之路”。这些说法让很多普通人感到非常恐慌，担心这代表了人工智能已经有能力理解人类真实世界，因此这或许代表着人类末日的开始。</p><p>&nbsp;</p><p>而图灵奖得主Yann LeCun，作为一位“世界模型”的倡导者，他认为OpenAI的Sora并不理解物理世界，今天他更是直接说Sora对“世界模型”的实现方式，注定是死路一条。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bca754e6c0dec0130b310eb32001b24.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><h2>Yann LeCun火力全开</h2><p></p><p>&nbsp;</p><p>之前， OpenAI Sora 研发成员 Aditya Ramesh 发布了一个关于一只蚂蚁“在蚁巢内部移动的视角镜头”的视频，但视频里面的蚂蚁只有四条腿。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/66b414762e58dcb74b3a9757202e4417.jpeg\" /></p><p></p><p>&nbsp;</p><p>Yann LeCun随后对其喊话：“Aditya，蚂蚁难道不是有6条腿吗？”“作为曾在我实验室待过的学生，我担保他知道蚂蚁有6条腿！”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5b09fefd04313863bf0e4d5e08d42f68.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>4条腿的蚂蚁的确不符合真实世界的实际情况，Yann LeCun也认为根据提示词生成看似真实的视频绝不代表系统真的理解物理世界。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6f/6f5a1c0ad7a707a65da92150a6c146bd.jpeg\" /></p><p></p><p>&nbsp;</p><p>这样的图像生成跟世界模型的因果预测间仍然存在重大差异。或者说，让视频内容看似合理的空间非常大，视频生成系统只需生成其中“一种”样本即可算作成功。但真实视频的合理连续空间要“小得多”，而且生成其中的代表性图块更是一项极为困难的任务，在涉及各种动作的情况下更是如此。</p><p>&nbsp;</p><p>此外，他还强调，这种连续生成不仅成本高昂，而且完全没有现实意义。</p><p>&nbsp;</p><p></p><p></p><p></p><p>Visualization of Slicing Video Temporal Data — Source:&nbsp;<a href=\"https://twitter.com/kitasenjudesign/status/1489260985135157258\">kitasenjudesign</a>\"</p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ff/b0/ff74fc7d2c4d1837295bf6cb51c0c1b0.png\" /></p><p>Visualization of Spacetime Patching (Processing) — Credit: OpenAI (Sora)</p><p></p><p>在今天的推文中，他更是直言Sora这种通过生成像素来对真实世界建模“不仅是种浪费，而且注定将要失败”，如同现在已经被基本放弃的“合成分析”技术一样。</p><p>&nbsp;</p><p>Yann LeCun解释说，几十年前，机器学习领域曾经就生成式方法与判断式分类方法的优劣对比展开过一场大辩论。数学家Vapnik等机器学习理论研究者明确反对生成式方法，认为生成模型的训练要比分类模型更困难（从样本复杂性角度出发）。总而言之，整个计算机视觉领域普遍认定像素的生成应该从解释潜在变量入手。毕竟在推理过程中，人类就是在根据观察到的像素推断出反映规律的潜在变量。正确的推理方法还涉及优化部分：比如使用对象的3D模型并尝试找到能够重现图像的姿态参数。遗憾的是，这个路子一直没能彻底走通，而且速度非常缓慢。</p><p>&nbsp;</p><p>后来，有些人选择了贝叶斯路线，尝试使用贝叶斯推理来推断潜在变量（例如使用变分近似及/或采样）。非参数贝叶斯与潜在狄利克雷分配都在某种程度上主导过文本建模，有些人开始勇敢尝试借此识别图像中的具体对象。但这同样是一场彻头彻尾的失败！</p><p>&nbsp;</p><p>Yann LeCun认为，如果现在的目标是训练出用于识别或规划真实世界的模型，那么在像素层面进行预测肯定不是什么好主意。</p><p>&nbsp;</p><p>只能说生成技术恰好适用于文本，因为文本内容属于离散的、数量有限的符号。在这种情况下，预测过程中的不确定性更容易处理。相比之下，对高维连续感官输入中的不确定性进行预测则非常困难。</p><p>&nbsp;</p><p>正因为如此，依靠感官输入的生成模型注定将会失败。</p><p>&nbsp;</p><p></p><h2>Yann LeCun认为的更好的办法是什么？</h2><p></p><p>&nbsp;</p><p>作为人类，我们对周遭世界的了解和大部分知识（特别是在童年时代）主要是依靠观察而来。以牛顿运动定律为例，即使是未经任何引导的幼儿或者小动物，也会在多次触碰并观察之后意识到，一切抛掷的物体终将落向地面。是的，只需一点观察，而非耗费几个小时的指导或者阅读上千本学术著作。我们内心深处的世界模型（基于世界心理模型的情境理解能力）完全可以准确预测结果，而且效率非常高。</p><p>&nbsp;</p><p>所以Yann LeCun认为实现“世界模型”的方式，应该是让机器智能像人类般学习、建立起周遭世界的内部模型，从而高效学习、适应并制定计划以完成种种复杂的任务。</p><p>&nbsp;</p><p>这也是他提出的JEPA（Joint Embedding Predictive Architecture，联合嵌入预测架构）的核心特点所在：它并不是在“生成”，而是在表示空间中进行预测。</p><p>&nbsp;</p><p>在他前几天发布的推文结尾，他又给大家安利了一遍JEPA 的论文和他们的试验结果表：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/367bb87351b38a9d076a55a3b7f5b574.jpeg\" /></p><p>截图来源：<a href=\"https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/\">https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/</a>\"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>备受瞩目的视频JEPA</h3><p></p><p>&nbsp;</p><p>V-JEPA是一种非生成模型，通过预测抽象表示空间中视频的缺失/遮蔽部分来进行学习。这种方法与图像联合嵌入预测架构（I-JEPA）对图像抽象表示的比较（而非直接比较像素本身）有异曲同工之妙。不同于尝试填充每个缺失像素的生成式方法，V-JEPA能够灵活丢弃各种不可预测的信息，从而将训练与采样效率提高1.5至6倍。</p><p>&nbsp;</p><p>由于V-JEPA采用自监督学习方法，因此可以纯依靠未经标注的数据进行预训练。这些标签仅在预训练之后被用于保证模型能够适应特定任务。事实证明，这种类型的架构比以往模型更加高效，不仅训练需要的标注示例更少、在学习未标注数据方面投入的总工作量也更低。借助V-JEPA，Meta在这两项指标上均迎来了改进。</p><p>&nbsp;</p><p>使用V-JEPA，研究团队遮蔽掉了视频中的大部分内容，借此让模型仅能观察到小部分上下文。之后，再要求预测器填补缺失的空白——请注意，填补过程并非根据实际像素，而是依托表示空间中更抽象的内容描述。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7a9650f911d41a7f8375c9bc22666b2.png\" /></p><p></p><p>在学习潜在空间中，V-JEPA通过预测被遮蔽的时空区域来训练视觉编码器。</p><p></p><h4>遮蔽方法</h4><p></p><p>&nbsp;</p><p>V-JEPA的这种理解并非来自对某一特定操作类型的训练；相反，它是在一系列视频之上完成了自监督训练，并借此掌握了大量关于真实世界运行规律的知识。</p><p>&nbsp;</p><p>研究团队还认真设计了遮蔽策略——如果不遮挡视频中的大块区域，而是随机在各处覆盖内容，那么任务就会变得过于简单，导致模型学不到真实世界中的任何复杂规律。</p><p>&nbsp;</p><p>另外需要注意的是，在大多数视频中，对象随时间推移的变化其实相对缓慢。如果只遮蔽特定时刻下视频中的某个部分，而模型仍能观察到紧随其前/其后的内容，任务同样会变得过于简单，导致其无法学习到有趣的知识。因此，研究团队采取一种方法，在空间与时间两个维度上遮蔽视频的部分内容，强迫模型学习并加深对于场景逻辑的理解。</p><p>&nbsp;</p><p>保证在抽象表示空间中进行预测同样非常重要，这样模型才能专注于实际视频内容所反映出的更高级别概念信息，而忽略掉那些对于下游任务意义不大的各类细节。举例来说，如果视频画面中是一棵树，那么就并不需要关心每片叶子的细小运动。</p><p>&nbsp;</p><p></p><h4>高效预测</h4><p></p><p>&nbsp;</p><p>V-JEPA是首个擅长“冻结评估”的视频模型，换句话说，模型的编码器与预测器均可实现自监督预训练，研究人员不必再做具体操作。想让模型掌握一项新技能，只需要额外训练一个小型轻量级专业层、或者在其上训练一个小型网络，整个过程更加高效快速。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2dca417addc76d4a32f23f1293ff6f6f.png\" /></p><p></p><p>少样本冻结评估：将V-JEPA与Kinetics-400和Something-Something-v2等其他视频模型中的冻结评估进行比较，这里我们改变了每套数据集上可用于训练注意力探针的标注示例百分比。我们在几种少样本设置中进行探针训练：分别对应训练集中5%、10%和50%的数据，并在每种情况下进行三轮随机比较以获得更稳健的指标，也就是分别对每套模型进行9次不同的评估实验。表中列出了官方验证的K400与SSv2验证集的平均值与标准差。V-JEPA的标记效率的确高于其他模型，而且可用标注示例数量越少，V-JEPA相较于其他模型的性能优势也越明显。</p><p>&nbsp;</p><p>&nbsp;</p><p>以往的生成式模型要求我们进行全面微调，就是说在模型预训练完成之后，如果希望模型能够真正掌握对细粒度动作的识别能力、利用它来处理实际任务，还需要更新所有模型中的参数或者权重。之后，该模型总体上只能执行一类特定任务，而不再适用于其他任务类型。</p><p>&nbsp;</p><p>如果想要引导模型学会执行多种任务，则需要提供不同的数据，并针对新任务对整个模型进行特化。而正如Meta在研究中所演示的那样，使用V-JEPA，我们可以在没有任何标注数据的前提下对模型进行一次预训练、修复相应问题，然后重复利用模型中的相同部分处理多种不同任务，例如动作分类、识别细粒度对象交互及活动定位等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/1622a7610996a3eb0f86f42f4611c74d.png\" /></p><p></p><p>V-JEPA是一种从视频中学习表示的自监督方法，适用于各类下游图像及视频处理任务，且无需调整模型参数。V-JEPA在图像分类、动作分类及时空动作检测等任务的冻结评估方面，优于以往的视频表示学习方法。</p><p>&nbsp;</p><p>虽然V-JEPA中的“V”代表视频，但并不是说它的适用范围就仅限于视频内容。后续Meta还将采用其他多模态方法，并认真考虑将音频与视觉效果结合起来。</p><p>&nbsp;</p><p>虽然目前V-JEPA还只能在较短的时间维度上发挥作用——比如在不超过10秒的视频片段中准确识别不同对象的行为。但Meta接下来的另一项研究重点，在于如何调整模型以在更长的时间范围内实现准确预测。</p><p>&nbsp;</p><p>目前的结果证明，Meta目前可以直接用视频数据训练JEPA模型，而不再需要大量监督和介入。它会像婴儿般从视频中学习，凭借被动观察世界来学习有助于理解内容上下文的背景知识。这样，只须配合少量标注数据，就能让模型快速获得执行新任务、识别各种动作的能力。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://twitter.com/ylecun/status/1759486703696318935\">https://twitter.com/ylecun/status/1759486703696318935</a>\"</p><p><a href=\"https://twitter.com/ylecun/status/1758740106955952191\">https://twitter.com/ylecun/status/1758740106955952191</a>\"</p><p><a href=\"https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/\">https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2024-02-20 20:57:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]