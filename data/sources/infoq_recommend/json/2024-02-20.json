[
  {
    "title": "最高法判决引发热议：国内首例GPL侵权案背后的开源与创新之争｜讨论",
    "url": "https://www.infoq.cn/article/QbzYObG5FaaWa4JYx2Md",
    "summary": "<p>近日，最高人民法院发布了一则关于开源软件著作权纠纷案的判决，引发了业内人士的广泛关注和热议。该案中，被告公司基于 GPLv2 协议提出了不侵权抗辩，但最终被法院认定为侵权。关于本案详细报道，请移步InfoQ 报道文章“<a href=\"https://www.infoq.cn/article/PLtyHpFh4tXuJqs0yjWs\">最高人民法院：这份判决给软件开发者吃了定心丸？</a>\"”</p><p></p><p>本案的争议焦点在于，软件开发者自身存在一定程度上的违反 GPLv2 协议，是否就一定不享有新研发软件著作权呢？</p><p></p><p>支持者认为， 该判决有助于明确开源软件的法律地位，确保了开发者的劳动成果得到应有的尊重和保护。它强调了即使在开源环境下，开发者对其作品的独创性贡献仍然享有著作权，这有助于激励创新和保护知识产权。开源协议并非凌驾于法律之上的“免责条款”，开发者不能以此为借口侵犯他人著作权。此外，判决也有助于规范市场行为，防止不正当竞争，维护公平的市场秩序。</p><p></p><p>反对者则担心， 这样的判决可能会对开源社区的开放性和协作精神产生负面影响。他们认为，开源软件的核心价值在于自由分享和共同进步，如果对违反开源协议的行为过于严苛，可能会限制开发者的自由，抑制社区的活力和创新。此外，过于严格的法律约束可能会让一些潜在的开发者和企业对参与开源项目产生顾虑，从而影响开源软件的整体发展。</p><p></p><p>小编认为， 最高人民法院的判决在一定程度上确实为开源软件的著作权保护提供了指导原则，尤其是在区分软件开发者违反GPLv2协议与他人侵犯其著作权行为的独立性方面。这有助于软件开发者在遵守开源协议的同时，保护自己的合法权益。判决强调了在软件尚未被开源的情况下，软件开发者基于其独创性贡献享有的著作权不应受到不合理的限制。该判决在一定程度上平衡了软件开发者权益保护与开源社区建设之间的关系，具有积极意义。</p><p></p><p>但同时，该判决也存在一些值得探讨的地方。例如，GPLv2协议涉及的技术细节和法律解释往往非常复杂，特别是在判断软件是否构成“衍生作品”（derivative work）时，需要对软件的源代码进行深入的技术分析，而本案的案卷材料可能尚不充分。最高人民法院的判决具有重要的指导意义，但在具体案件中，如何将判决原则应用到其他类似情况，可能还需要进一步的司法实践和案例积累。</p><p></p><p>关于本案判决对开源软件著作权保护的影响，你的看法是什么呢，欢迎投票或评论分享。</p><p></p><p></p><p></p><p></p><p>欢迎加入 InfoQ 读者技术交流群，与志同道合的朋友一起探讨知识，交流经验。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e6/36/e64f5643365f9a9fedd63326ff5de736.png\" /></p><p></p>",
    "publish_time": "2024-02-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Sora生成的视频太真实？那是你遇到造假了",
    "url": "https://www.infoq.cn/article/1M4s64scpG5ifvX7qfAT",
    "summary": "<p></p><p><img src=\"https://static001.infoq.cn/resource/image/2b/3c/2b9dd247c1dea290fe83f6f2996b033c.gif\" /></p><p></p><p></p><p>视频发布者“No Context Brits”表示这是 Sora 生成的，提示词是：Brit gets hit by a bus then goes for a pint。那么你认为，上面视频是真的还是 AI 生成的？</p><p></p><p></p><p></p><p>这个问题的答案，我们留到最后揭晓。</p><p></p><h3>现实真的不存在了吗？</h3><p></p><p></p><p>当大家都在说 Sora 颠覆行业的时候，Sora 究竟能颠覆多少？我们由易到难，看看 Sora 制作的视频，可以达到什么级别。</p><p></p><h4>风景</h4><p></p><p></p><p>风景类视频制作可以说是入门级，画面细节要求相对少一些，构图、运镜相对比较重要。而 Sora 确实能制作出纪录片里常用到的运镜方式，构图也是参照了构图规则的：</p><p><img src=\"https://static001.infoq.cn/resource/image/c4/fa/c4b91174f7fcd49fff4d3d898d548ffa.gif\" /></p><p></p><p></p><p></p><p>可以简单看下《地球脉动》第二季第一集的开头片段：</p><p><img src=\"https://static001.infoq.cn/resource/image/52/3f/52ea660833f22fcc53acb89e1ff3363f.gif\" /></p><p></p><p>同时，与视频生成领域的其他同行比，Sora 在真实性、连续性上的进步也是很明显的：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b9/b915114aec59b9b33722cf28523d2b13.gif\" /></p><p></p><p></p><h4>动物</h4><p></p><p></p><p>在 OpenAI Sora 研发成员 Aditya Ramesh 发出的一个关于一只蚂蚁“在蚁巢内部移动的视角镜头”的视频里，Sora 给出了如下效果：</p><p></p><p></p><p>这个视频犯了基础的认知错误：里面的蚂蚁只有四条腿，真实世界里的是六条腿。杨立昆（Yann LeCun）也直接指出了这一点，但仍止不住网友对视频效果的赞叹。</p><p></p><p>题外话：Aditya 与 LeCun 也有一段缘分。据 LeCun 爆料，Aditya 本科就读于纽约大学，并参加过其实验室的一些项目。</p><p></p><p>下面这只“飞入海底的蝴蝶”，虽然没有尊重基本事实（毕竟蝴蝶没入海底怕是飞不起来），但如果是特效，那还是可以的：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/30/30f9e5da5c031b147db08da2eaf758ff.gif\" /></p><p></p><p>一只寻找庇护所的流浪猫：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7e/7e1f77c38386a684b9fac9a58a7d6775.gif\" /></p><p></p><p>在单只动物的相对简单的场景里，Sora 表现还是不错的。</p><p></p><h4>人物</h4><p></p><p></p><p>在最新发布的 Sora 生成视频里，有一个体现人类惊讶表情的视频，但效果不太好：鲨鱼在离沙滩特别近的沙滩出现，女人夸张的惊讶……“那个女人比鲨鱼更让我害怕，制作恐怖电影可能是 Sora 的最佳用途。”网友评价。另外，这个视频的逻辑还需要提示词输入进行调整，比如男人的无动于衷。</p><p></p><p></p><p></p><p></p><p>下面这个老人过生日的视频应该很多人见过，效果相对还是相对丝滑一些的，虽然老人吹蜡烛时，烛光动也没动……</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f6/f626e0dfcdfcb287a054cf25273c626d.gif\" /></p><p></p><p>这个猫和主人互动的视频里，猫挠到主人鼻子时，鼻子的变化给人感觉像一张纸。另外，她不疼吗？！</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/37/37c73c5a1a23529e109be29afd6d5c0a.gif\" /></p><p></p><p>更复杂一些的场景，我们看看 Sora 的一镜到底：</p><p></p><p></p><p></p><p></p><p>“几乎完美。但是吹毛求疵，这里的视角不太好。看起来用餐的人坐在一个小型市场旁边。”有敏锐的网友指出：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/82/8227305e455b1c73737aa477818b6b30.png\" /></p><p></p><p>“大多数人身上都发现了人工制品和某种程度的幻觉。”复杂场景下，Sora 还是做不到完美。</p><p></p><h4>特效</h4><p></p><p></p><p>特效视频就不存在真实性问题了，视觉效果是重要的衡量因素。</p><p></p><p>Sora 研发团队 Bill Peebles 发布了一只“科技犬”视频：未来控制论德国牧羊犬的特写镜头，展示了其引人注目的棕色和黑色皮毛…</p><p></p><p></p><p></p><p>一位数字艺术方面的从业者表示，“这看起来比我们见过的任何 CGi 都更真实。迫不及待地希望能够尽快将视频制作变为 3D 模型，这样我们就可以在游戏中拥有这些资源和动画。”也有网友调侃道，“本次拍摄中没有动物受伤。”</p><p></p><p>Bill 还发布了另一个特效视频：“一座巨大的大教堂里全是猫。放眼望去，到处都是猫。一个男人走进大教堂，向坐在王座上的巨型猫王鞠躬。”在经过网友增加旁白和配音后，便是这样的：</p><p></p><p></p><p>旁白 @ChatGPTapp</p><p>配音者 @elevenlabsio</p><p>音乐由 @suno_ai_</p><p></p><p>如果有一天，OpenAI 能够直接将视觉效果和听觉效果一起输出，那又会是震惊行业的一件大事。可以看下，网友给 Sora 视频加上视觉效果是什么样的：</p><p></p><p></p><p></p><p></p><p>Sora 研发团队另一位重要成员 Tim Brooks 用 Sora 让沙盒游戏《我的世界》拥有了“有史以来最华丽的高分辨率 8k 纹理包”：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a5/a50113c3cc591d5d61279e30906d18cb.gif\" /></p><p></p><p>同时，Tim 还让《我的世界》视频融合进摩托车视角，“这个功能有如此大的创造潜力”Tim 说道。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3c/3ceb4b6f9f3445481a1817101d47b7b3.png\" /></p><p></p><p>其实效果已经不错，有网友建议可以在提示中加上“光线追踪、光晕、后期特效”等，这样效果可能会更好。</p><p></p><p>下面是一个 Sora 改变视频的风格和环境的例子，一辆跑车穿梭在水底、恐龙乐园、像素世界等等场景中：</p><p></p><p></p><p></p><p>“一只鸭子走在波士顿的街道”，如果更加复杂一些，会不会有漫威的感觉？</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e5/e5fc56900721b672725e46414c1c4cf2.gif\" /></p><p></p><p>“在叶子上行驶的火车”，叶子的脉络还真是跟清晰的，当然也有网友认为这种视频没有什么用，更多是一种数字垃圾。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/da/daf9db1bda75b14a536c74ece4a20013.gif\" /></p><p></p><p></p><h3>谢赛宁：Sora 跟我没关系</h3><p></p><p></p><p>Sora 能有上面的效果，主要得益于 DiT 架构和 Spacetime Patch。</p><p></p><p>其中，Spacetime Patch 建立在 GoogleDeepMind 对 NaViT（原生分辨率视觉 Transformer）和 ViT（视觉 Transformer）的早期研究基础上。Patch 可以理解为 Sora 的基本单元，类比 Token。Sora 处理一系列的 Patch，并预测出序列中的下一个 Patch。</p><p></p><p>Sora 团队发现补丁是一种高度可扩展且有效的表示形式，因此通过 Spacetime Patch 将视频视为补丁序列，捕捉视觉数据使模型能够从更准确的表达中学习。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/07/076c9f430f3eac29557102a07de46702.png\" /></p><p></p><p>从 OpenAI 的技术报告可知，Sora 的作者团队有 13 位成员，如今被报道最多的核心成员包括研发负责人 Tim Brooks、William Peebles、系统负责人 Connor Holmes 等。</p><p></p><p>其中，Tim Brooks 是 DALL-E 3 作者之一，GitHub 5.7k️星项目 InstructPix2Pix 作者，博士毕业于 UC Berkeley 的伯克利人工智能研究所 BAIR。Tim 曾在谷歌为 Pixel 手机摄像头提供 AI 算法，也在英伟达负责过视频生成模型的研究。</p><p></p><p>William Peebles 也来自 UC Berkeley，去年（2023 年）刚刚获得博士学位。据悉，William 和谢赛宁合作，研发了 DiT。也因为这个关系，毕业于上海交大的天才少年谢赛宁被报道为是 Sora 的研发者之一。谢赛宁本人对此强烈否认：“一点关系都没有”。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/13/13a3f99c9c6993f85cb16137b008b013.jpeg\" /></p><p></p><p>Connor Holmes 则曾在 Colorado School of Mines、微软工作过，在 LLM、BE RT 风格的编码器、RNN 和 UNets 方面有丰富经验。“我期待解决在扩展深度学习工作负载以进行推理和训练时系统效率低下的问题。”他在自己的领英上说道。此外，Sora 团队的不少成员都是 DALL-E 3 的作者，包括两位华人 Li Jing 和 Yufei Guo。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>“如何加入红队？我可以帮助测试”有积极参与的人，也有不喜欢生成视频的人：“我看视频，不是想看虚拟的世界，而是想通过镜头去看自己不了解的真实的世界。”</p><p></p><p>现在网上也出现了很多声称是 Sora 生成的视频，但其实并不是。比如下面这个女团视频声称是Sora生成的，但真实性存疑。</p><p></p><p></p><p></p><p>来源：https://twitter.com/ViLettuce/status/1758976415150559638</p><p></p><p>还比如下面视频的发布者“víty”表示这个“女生吃面包时与他人发生争执”视频是 Sora 生成的，提示词是：𝘞𝘩𝘪𝘵𝘦 𝘸𝘰𝘮𝘢𝘯 𝘦𝘢𝘵𝘴 𝘣𝘳𝘦𝘢𝘥，𝘢𝘶𝘯𝘵 𝘣𝘪𝘵𝘤𝘩𝘴𝘭𝘢𝘱、𝘸𝘩𝘪𝘵𝘦 𝘸𝘰𝘮𝘢𝘯 𝘭𝘢𝘶𝘨𝘩𝘴、𝘱𝘪𝘢𝘯 𝘰𝘥𝘶𝘩𝘩，𝘩𝘰𝘶𝘴𝘦𝘦𝘷𝘪𝘤𝘵𝘪𝘰𝘯，𝘤𝘰𝘰𝘭𝘣𝘢𝘴𝘴𝘰𝘶𝘵 𝘳𝘰𝘮𝘶𝘴𝘪𝘤。</p><p></p><p>但有网友指出，这个视频并非 Sora 生成的，而是来源于一部名为《Ti Ti Ti》的肥皂剧。看过这部剧的朋友可以出来说说～</p><p></p><p></p><p>来源：<a href=\"https://twitter.com/vvvorvvtorvitor/status/1758654081176866906\">https://twitter.com/vvvorvvtorvitor/status/1758654081176866906</a>\"</p><p></p><p>回到文章最初问到的问题，其实帖子下面也引起了网友的各种讨论，有人说是真的，有人说是生成的。而真正的答案就是：那是真实的视频。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/46/464a491c0577c42dacd774797cf8de72.png\" /></p><p></p><p>出自外媒 The Guardian 在 2017 年的报道：</p><p></p><p><a href=\"https://www.theguardian.com/global/video/2017/jun/27/man-hit-by-bus-in-reading-survives-without-injury-video\">https://www.theguardian.com/global/video/2017/jun/27/man-hit-by-bus-in-reading-survives-without-injury-video</a>\"</p><p></p><p>你猜对了吗？</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://twitter.com/minchoi/status/1758831971726225591\">https://twitter.com/minchoi/status/1758831971726225591</a>\"</p><p><a href=\"https://twitter.com/NoContextBrits/status/1759212202853040265\">https://twitter.com/NoContextBrits/status/1759212202853040265</a>\"</p><p><a href=\"https://openai.com/research/video-generation-models-as-world-simulators\">https://openai.com/research/video-generation-models-as-world-simulators</a>\"</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f9/f9e4f42a4db5c560a5e031ced2a5ac56.png\" /></p><p></p><p></p><p></p>",
    "publish_time": "2024-02-20 12:21:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Sora 横空出世，给科技型企业带来的 3 点启示 | Q 推荐",
    "url": "https://www.infoq.cn/article/JaYGkUL3slZtQIivxcWQ",
    "summary": "<p>开工第一周，OpenAI 发布的“文生视频”（text-to-video）的工具，Sora ， 可谓刷爆各个平台。Sora 的出现，或意味着 AGI（通用人工智能）实现将从 10 年缩短到 1 年。从 ChatGPT 到 Midjournery 再到 Sora ，每一次新技术的出现便会有相关从业者的开始焦虑。然而，面对层出不穷的 AI 技术，作为科技型企业又该如何应对呢？</p><p></p><p>培养前瞻性思维，坚持技术创新：Sora 的推出强调了持续技术创新的重要性，对科技型企业来说，这意味着需要不断探索和实验新技术，以维持在竞争激烈的市场中的领先地位。夯实技术基础，稳步实现 AI 融合：建立现实的技术应用视角，避免对热点技术抱有不切实际的高期望，而忽视了技术应用的现实挑战和局限性。持续学习和适应，打造成长型团队：Sora 展示了技术的快速进步，对科技型企业来说，这意味着需要在组织内建立持续学习和适应新技术的文化，以便快速响应行业变化和技术升级。据《麻省理工科技评论》报道，具备成长型思维的团队在面对 AI 变革时，其创新能力和业务增长速度均超过其他团队。例如，谷歌的 DeepMind 团队正是因为其成长型思维和强大的创新能力，成功研发出了 AlphaGo 等颠覆性 AI 技术，为企业带来了巨大的商业价值。</p><p></p><p>应对科技企业培养成长型团队的需求，极客时间企业版推出“领跑 2024，企业数字化人才培养福利活动”，精选了涵盖 AI 大模型应用、数字化转型实战、领导力与企业管理策略等多元维度的在线课程。这些课程不仅由行业专家亲自授课，还结合了丰富的实战案例和前沿的技术动态，确保技术团队能够学到最实用、最前沿的知识。</p><p></p><p>即日起至活动结束，参与活动的企业可以免费获得 100 个学员账号！团队成员可以在 2 月底之前，自由选择平台上的 6 门课程进行深度学习。立即扫码，提升团队整体实力，无惧迎接 AI 时代挑战。</p><p>↓↓↓扫描下图二维码参加活动↓↓↓</p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4c5fae03a44b50caf660c0e96065a68.png\" /></p><p></p><p>培养 AIGC 应用能力</p><p>从系统实战到应用开发，探索 AI 大模型无限可能</p><p>掌握生产级 AI 系统研发能力</p><p>手把手带你用 LLM 提升研发效率</p><p>一站式掌握硬件、理论、LangChain 开发框架和项目实战技能</p><p>全面解析微调技术理论，掌握大模型微调核心技能</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5eaa08a89078e7b3b4d8dd62cb0b16f0.png\" /></p><p></p><p>完善岗位专项技能体系</p><p>行业大咖规划职业进阶学习路径</p><p>全方位提升实战技能，补足能力短板</p><p>从工作应用出发，紧跟前沿技术发展和行业趋势</p><p>涵盖从基础知识到高级应用的全链路学习</p><p>按学习路径选课</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be08f4c52878faad45a41cd8e2afb9bf.png\" /></p><p></p><p>按专项技能选课</p><p><img src=\"https://static001.geekbang.org/infoq/bf/bff22496a674fb9eca8a0ab2adf1797f.png\" /></p><p></p><p>提升技术对业务的驱动</p><p>向推动软件创新和变革的从业者学习</p><p>轻松获取源于实践、有效验证的优质内容</p><p>为项目汲取可落地的想法</p><p>验证企业软件开发路线图</p><p>掌握技术与业务创新深度融合的前沿案例</p><p>1400+ 顶尖技术团队实战案例</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c6e847069b59052dbf8363c7bbf3f08.png\" /></p><p></p><p>领跑 2024，数字化人才培养福利</p><p>6000 小时精选课程，</p><p>AI 大模型等前沿课程，</p><p>创新产品开发、架构设计等专业课程，</p><p>数字化领导力、数字化管理等领导课程，</p><p>数据分析等实用课程......</p><p>↓↓↓扫描下图二维码参加活动↓↓↓</p><p><img src=\"https://static001.geekbang.org/infoq/53/53485585ad6b50a70ddabad8d16bd69b.png\" /></p><p></p>",
    "publish_time": "2024-02-20 14:51:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动辟谣推出中文版Sora：还无法完善产品落地，距离国外模型有很大差距",
    "url": "https://www.infoq.cn/article/Suc3VpjPiSq6PfukWxZm",
    "summary": "<p>今日有消息称，在Sora引爆文生视频赛道之前，国内的字节跳动也推出了一款颠覆性视频模型——Boximator。与Gen-2、Pink1.0等模型不同的是，Boximator可以通过文本精准控制生成视频中人物或物体的动作。</p><p>&nbsp;</p><p>对此，字节跳动相关人士向媒体回应称，Boximator是视频生成领域控制对象运动的技术方法研究项目，目前还无法作为完善的产品落地，距离国外领先的视频生成模型在</p><p>&nbsp;</p><p>根据介绍，Boximator 可以通过文本精准控制生成视频中人物或物体的动作。例如，“小猫把自己藏进杯子里了”：</p><p></p><p></p><p></p><p></p><p>“由像素组成的角色正在跳舞”：</p><p></p><p></p><p></p><p></p><p></p><p>“一个红衣女孩用头骨遮住了脸”：</p><p></p><p></p><p></p><p></p><p>“一名年轻女子转过头，露出了她的侧脸”：</p><p></p><p></p><p></p><p></p><p>“蜘蛛侠向镜头摆动”：</p><p></p><p></p><p></p><p>根据论文介绍，Boximator使⽤ 3D U-Net 架构构建在视频扩散模型之上。3D U-Net 由交替的卷积块和注意⼒块构成。每个块包含两个组件：⼀个空间组件，负责将各个视频帧作为单独的图像进⾏处理；另外一个是时间组件，⽀持跨帧信息交换。</p><p>&nbsp;</p><p>为了实现对视频中物体、人物的动作控制，Boximator 使用了“软框”和“硬框”两种约束方法。其中，硬框可精确定义目标对象的边界框，软框则定义一个对象可能存在的区域, 形成一个宽松的边界框。</p><p>&nbsp;</p><p>控制模块可以将框约束的编码与视频帧的视觉编码结合，用来指导视频的精准动作生成。包含框编码器和自注意力层两大块。</p><p>&nbsp;</p><p>论文地址：<a href=\"https://arxiv.org/abs/2402.01566\">https://arxiv.org/abs/2402.01566</a>\"</p><p>&nbsp;</p><p>下面是研发人员给出的Gen-2、Pink1.0 和Boximator 的对比：</p><p></p><p></p><p></p><p></p><p></p><p>&nbsp;根据其<a href=\"https://boximator.github.io/\">在Github</a>\"上的信息，Boximator演示网站正在开发中，将在未来 2-3 个月内推出。</p><p></p>",
    "publish_time": "2024-02-20 14:57:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI模型家族更新：GPT-4训练数据至2023年12月",
    "url": "https://www.infoq.cn/article/AMfMVAJMZG9zlrboe8pf",
    "summary": "<p>近日，OpenAI 宣布 GPT-3.5-turbo、GPT-4以及GPT-4-turbo-preview等均指向最新模型版本。用户可以发送请求并查看响应对象来验证自己正在使用哪种模型。响应结果中包含所使用的特定模型版本（例如GPT-3.5-turbo-0613）。</p><p>&nbsp;</p><p>OpenAI还提供静态模型版本，开发人员可以在模型更新发布后的三个月内继续使用原有模型。随着模型更新的加快，OpenAI还开放了评估贡献通道，由用户针对不同用例协同进行模型改进。</p><p>&nbsp;</p><p>感兴趣的朋友请参阅OpenAI&nbsp;Evals&nbsp;repo：</p><p><a href=\"https://github.com/openai/evals\">https://github.com/openai/evals</a>\"</p><p>&nbsp;</p><p>关于弃用模型的更多详细信息，请参阅OpenAI官网上的弃用页面：</p><p><a href=\"https://platform.openai.com/docs/deprecations\">https://platform.openai.com/docs/deprecations</a>\"</p><p>&nbsp;</p><p></p><h4>GPT-4与GPT-4 Turbo</h4><p></p><p>&nbsp;</p><p>GPT-4是一套大型多模态模型（可接收文本或图像输入，并输出文本结果），目前通过OpenAI API 向付费客户开放。</p><p>&nbsp;</p><p>与GPT-3.5-turbo一样，GPT-4针对聊天进行了优化，因此可通过聊天完成以往必须借助Chat Completions&nbsp;API才能处理的任务。OpenAI在文本生成指南中专门介绍了如何使用GPT-4：</p><p><a href=\"https://platform.openai.com/docs/guides/text-generation\">https://platform.openai.com/docs/guides/text-generation</a>\"</p><p>&nbsp;</p><p></p><p>对于大部分基本任务，GPT-4和GPT-3.5模型间的差异并不明显。但在需要较复杂推理能力的情况下，GPT-4则拥有超越OpenAI此前各类模型的表现。</p><p>&nbsp;</p><p></p><h4>GPT-3.5 Turbo</h4><p></p><p>&nbsp;</p><p>GPT-3.5 Turbo模型能够理解并生成自然语言或者代码，针对Chat Completions API进行了聊天优化，但也同样适用于非聊天任务。</p><p></p><p>&nbsp;</p><p></p><h4>DALL·E</h4><p></p><p>&nbsp;</p><p>DALL-E是一套AI系统，能够根据自然语言的描述创建出逼真的图像与艺术效果。DALL-E 3目前支持根据提示词生成拥有特定尺寸的新图像。DALL-E 2还支持对现有图像进行编辑、或为用户上传的图像生成变体等功能。</p><p>&nbsp;</p><p>DALL-E 3可通过OpenAI的Images API同DALL-E 2配合使用。用户可通过ChatGPT Plus服务体验DALL-E 3。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p></p><h4>TTS</h4><p></p><p>&nbsp;</p><p>TTS是一种AI模型，能够将文本转换为听感自然顺畅的语音。OpenAI提供两种不同模型变量，其中tts-1针对实时文本到语音用例进行了优化，tts-1-hd则针对输出质量进行了优化。这些模型均可通过Audio API中的Speech端点配合使用。</p><p></p><p>&nbsp;</p><p></p><h4>Whisper</h4><p></p><p>&nbsp;</p><p>Whisper是一种通用语音识别模型，在包含多种音频的大型数据集上训练而成。它也是一套多任务模型，能够执行多语种语音识别、语音翻译与理解等任务。Whisper v2-large模型目前可通过API调用，模型名称为Whisper-1。</p><p>&nbsp;</p><p>目前，Whisper的开源版本与OpenAI通过API提供的版本完全一致。但API版本的推理过程经过优化，因此Whisper在API上的运行速度要比其他方式快得多。</p><p>&nbsp;</p><p>关于Whisper的更多技术细节，请参阅此论文：</p><p><a href=\"https://arxiv.org/abs/2212.04356\">https://arxiv.org/abs/2212.04356</a>\"</p><p>&nbsp;</p><p></p><h4>Embeddings</h4><p></p><p>&nbsp;</p><p>Embeddings是指文本的数字表示，可用于衡量两段文本之间的相关性。Embeddings即嵌入，往往在搜索、聚类、推荐、异常检测和分类任务中拥有良好表现。</p><p>&nbsp;</p><p>感兴趣的朋友可以在OpenAI的公告博文中了解关于最新嵌入模型的更多信息：</p><p><a href=\"https://openai.com/blog/new-embedding-models-and-api-updates\">https://openai.com/blog/new-embedding-models-and-api-updates</a>\"</p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>Moderation</h4><p></p><p>&nbsp;</p><p>Moderation审核模型负责检查内容是否符合OpenAI的使用政策。这些模型提供分类功能，用于查找以下类别的内容：仇恨、仇恨/威胁、自残、性、性/未成年人、暴力及暴力/图像。</p><p>&nbsp;</p><p>更多具体信息请参阅OpenAI审核指南：</p><p><a href=\"https://platform.openai.com/docs/guides/moderation/overview\">https://platform.openai.com/docs/guides/moderation/overview</a>\"</p><p>&nbsp;</p><p>审核模型可接受任意大小的输入，将输入自动拆分成4096个tokens的块。如果总输入超过32768个tokens，则使用截断技术处理。在极少数情况下，此类模型可能会在审核检查中忽略少量tokens。</p><p>&nbsp;</p><p>每条指向审核端点的请求仅显示各类别的最大值。例如，如果一个4k&nbsp;tokens块的分类得分为0.9901，而另一个块的得分为0.1901，则API响应结果将仅显示明显更高的0.9901。</p><p></p><p>&nbsp;</p><p></p><h4>GPT base</h4><p></p><p>&nbsp;</p><p>GPT base模型能够理解并生成自然语言或者代码，但并未接受指令遵循方面的训练。这些模型旨在替代OpenAI之前的GPT-3 base基础模型，且使用旧版Completions API。OpenAI推荐大多数用户直接使用GPT-3.5或者GPT-4。</p><p></p><p>&nbsp;</p><p></p><h2>使用政策</h2><p></p><p>&nbsp;</p><p>在用户数据处理上，OpenAI 强调用户数据始终归用户所有。</p><p>&nbsp;</p><p>自2023年3月1日起，发送至OpenAI API的数据将不会被用于训练或改进OpenAI模型（除非用户明确表示同意&nbsp;）。但若选择参与改进，那么模型可能随时间推移更加契合的用例。</p><p>&nbsp;</p><p>为了帮助识别滥用行为，API数据最多可保留30天，之后将被删除（除非法律另行要求）。对于用例较为敏感的可信客户，OpenAI亦提供零数据保留选项。在零数据保留情况下，请求与响应主体不会被持久保存在任何日志记录当中，而仅放置在内存内以支持服务需求。请注意，此数据政策不适用于OpenAI提供的非API消费级服务，例如ChatGPT或DALl-E Labs。</p><p>&nbsp;</p><p></p><h4>端点默认使用政策</h4><p></p><p>&nbsp;</p><p></p><p>*&nbsp;通过GPT-4-vison-preview模型输入的图像不符合零保留条件。</p><p>*&nbsp;对于Assistants API，OpenAI仍在beta期间评估默认保留周期。预计beta结束后将确定沿用默认的保留周期。</p><p>&nbsp;</p><p>关于更多详细信息，请参阅OpenAI的API数据使用政策：</p><p><a href=\"https://openai.com/policies/api-data-usage-policies\">https://openai.com/policies/api-data-usage-policies</a>\"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>模型端点兼容性</h4><p></p><p>&nbsp;</p><p></p><p>此列表不包含已被OpenAI弃用的各模型版本：</p><p><a href=\"https://platform.openai.com/docs/deprecations\">https://platform.openai.com/docs/deprecations</a>\"</p><p>&nbsp;</p><p>相关链接：</p><p><a href=\"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\">https://platform.openai.com/docs/models/GPT-4-and-GPT-4-turbo</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2024-02-20 16:56:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据要素×物流｜顺丰如何实现数据驱动流转效率提升【超级连麦·数智大脑】",
    "url": "https://www.infoq.cn/article/FIKfbul8LmVwJcfVQxfg",
    "summary": "<p>超级连麦，主持人高玉娴 （InfoQ极客传媒数字化主编） 对话特邀嘉宾林国强（顺丰科技大数据总监）就以下三大板块：<br />\n1.业务数字化与创新技术实践<br />\n2.数据管理挑战与应对策略<br />\n3.数据要素流通的价值与挑战<br />\n探讨数据要素×物流,看顺丰如何实现数据驱动流转效率提升。</p>",
    "publish_time": "2024-02-20 17:07:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "行知数字中国案例集锦（第三期）",
    "url": "https://www.infoq.cn/article/77RTy0iBNI85eANsBwXC",
    "summary": "<p>本手册共包括 “ 数智视野篇 ”、 “ 数智案例篇 ”、“ 数智技术篇 ”，以及 “ FCon 精选 ” 四个部分。其中，“数智案例篇”又划分为金融、制造、消费零售三个章节模块，分别从不同行业视角拆解数字化转型的最优路径，“ FCon 精选 ”则囊括了 FCon 大会上的重磅企业采访文章。</p>\n<p><strong>精彩内容抢先预览</strong></p>\n<p>数智视野篇</p>\n<p>01 ｜既要安全、又要创新，庞大且复杂的核电站数字化怎么破？</p>\n<p>02 ｜国货李宁的新数字化故事：如何利用技术做运动产品的研发？</p>\n<p>03 ｜ A0 级以上新能源汽车市占率 60%，全球齿轮制造大厂的数字化实践</p>\n<p>04 ｜涉及万亿元规模资产，华润集团数字化转型如何“大象转身”</p>\n<p>数智案例篇</p>\n<p>第一章：金融数字化</p>\n<p>05 ｜北京银行数字化转型：用近 2 年时间，从“搭梁立柱”到“积厚成势”</p>\n<p>06 ｜玉山银行数字化（上）：构建台湾地区第一个银行自建的“微服务架构”核心系统</p>\n<p>07 ｜玉山银行数字化（下）：普惠金融、智慧金融、场景金融三大 FINTECH 策略如何落地</p>\n<p>08 ｜银行券商经验不完全适用，中粮信托这样构建数字化中台</p>\n<p>09 ｜金融智能化实践的两个前提和两个关键</p>\n<p>10 ｜富滇银行：中小金融机构如何思考和实现 AIGC 应用？</p>\n<p>第二章：制造数字化</p>\n<p>11 ｜美的集团：关于工业数字化的最新思考和实践</p>\n<p>12 ｜百年工业巨头施耐德电气如何在数字世界“建工厂”</p>\n<p>第三章：消费零售数字化</p>\n<p>13 ｜专访华润啤酒首席数字官，探讨智能技术如何助力酒企数字化转型</p>\n<p>14 ｜自研模型大幅降低原奶调配成本，伊利如何做数字基建？</p>\n<p>数智技术篇</p>\n<p>15 ｜漏洞产生快于识别修复，开发者要如何预防安全问题？</p>\n<p>16 ｜美的集团最新 AI 实践：拟上线智能家居大模型，开源边端 AI 算法部署工具链</p>\n<p>17 ｜首个千亿医药对话大模型来了，要打破医药研发“三十定律”</p>\n<p>18 ｜ SOLIDWORKS：把 AI 放进工业设计软件，把软件放到云上</p>\n<p>FCON 精选</p>\n<p>19 ｜德邦基金 CTO 李鑫：中小金融机构数字化规划要聚焦重点、有所取舍</p>\n<p>20｜平安人寿魏政刚：算力与语料，是制约保险领域大模型应用的首要挑战</p>\n<p>21 ｜阳光保险张晗：大模型为保险业务全自动化创造了可能性</p>\n<p>22｜ 3 年探索与实践，苏州银行的数字人民币基础建设与场景应用怎么样了？</p>\n<p>23｜广发证券：金融科技浪潮下的 IT 架构升级之道</p>\n<p>24｜平安银行：云原生转型背景下的质效探索与思考</p>\n<p>25｜实现“数据资产当日达”，国泰君安证券做数据平台的逻辑与实践</p>\n<p>26｜中信银行：用 AI 搞定普惠型财富管理的高成本问题</p>\n<p>27｜民营银行生存法则“唯快不破”，华瑞银行风险特征计算平台如何做到实时响应</p>",
    "publish_time": "2024-02-20 17:14:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]