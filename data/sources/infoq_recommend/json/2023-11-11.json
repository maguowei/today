[
  {
    "title": "NGINX 模块现在可以用 Rust 编写了",
    "url": "https://www.infoq.cn/article/0MHbyQtDeTl9kiv8BbFe",
    "summary": "<p>NGINX 近日公布了 <a href=\"https://github.com/nginxinc/ngx-rust\">ngx-rust</a>\" 项目，允许开发人员使用 Rust 编写 NGINX 模块。 Rust 编程语言因其稳定性、多种安全特性、丰富的生态系统和强大的社区支持而成为一种强大且流行的语言选项。</p><p>&nbsp;</p><p>NGINX 是一款高性能、开源的 Web 服务器和反向代理服务器软件，大部分互联网网站都在使用它。 NGINX 最初由 Igor Sysoev 于 2002 年创建，此后一直在发展，并广泛流行于 Web 托管、内容交付和应用程序部署领域。它的性能、可扩展性和多功能优势闻名于世，也因此成为提供 Web 内容和有效管理互联网流量的网站关键组件。</p><p>&nbsp;</p><p>NGINX 的三大功能分别是：</p><p>Web 服务器：NGINX 主要扮演 Web 服务器的角色，用于处理 HTTP 和 HTTPS 请求。它可以提供静态 Web 内容，例如 HTML 文件、图像和 JavaScript，因而成为托管网站和 Web 应用程序的重要组件。反向代理服务器：NGINX 可以作为反向代理服务器，充当客户端请求和后端服务器之间的中介。常见的部署方式是用它来跨多个后端服务器分发传入请求，确保负载平衡和容错性。这在高流量环境中特别有用。负载均衡器：NGINX 可以充当负载均衡器，在多个服务器之间分配传入的网络流量。这可确保各个服务器不会过载，优化资源的使用率并为用户提供无缝体验。</p><p>&nbsp;</p><p>一开始，ngx-rust 是为了推动用户使用 NGINX 开发与 Istio 兼容的服务网格产品而诞生的。不过这个项目沉寂了一段时间，在此期间社区积极参与其中，分叉存储库并基于 ngx-rust 提供的 Rust 绑定示例来创建他们的项目。</p><p>&nbsp;</p><p>最近，F5 的分布式云机器人防御团队需要将 NGINX 代理集成到他们的防护服务中，这需要开发一个新模块。与此同时，F5 希望可以扩展其 Rust 产品组合并改善开发体验，以满足不断变化的客户需求。通过内部创新赞助以及与 ngx-rust 原作者的合作，F5 重振了 ngx-rust 项目。他们发布了 ngx-rust crate，优化了文档，构建也变得更友好，更适合社区使用了。</p><p>&nbsp;</p><p>NGINX 实现大多数功能的基本构建块是模块（Modules）。NGINX 用户还能自定义模块的功能来支持特定的用例。传统上，NGINX 只支持用 C 编写的模块，但计算机科学和编程语言理论的进步让 Rust 等语言也可以用于 NGINX 模块开发了。</p><p>&nbsp;</p><p>想要使用 ngx-rust 的话，你可以选择在本地从源代码构建、为 ngx-rust 项目做出贡献，或者直接从 crates.io 获取 crate。 ngx-rust 自述文件提供了贡献指南和本地构建要求。虽然 ngx-rust 仍处于开发的早期阶段，但 F5 计划在社区支持下提升其质量，带来更多特性。</p><p>&nbsp;</p><p>ngx-rust 项目包含两个关键的 crate：</p><p>nginx-sys：这个 crate 从 NGINX 源代码生成绑定，通过 bindgen 代码自动化来自动创建外部函数接口（FFI）绑定。ngx：这个主 crate 负责实现 Rust 粘合代码、API，并重新导出 nginx-sys。模块编写者通过 ngx 符号与 NGINX 交互，并且有了 nginx-sys 的重新导出就不需要显式导入了。</p><p>&nbsp;</p><p>初始化 ngx-rust 项目工作区时，需要创建一个工作目录、初始化 Rust 项目和设置依赖项：</p><p><code lang=\"rust\">cd $YOUR_DEV_FOLDER\nmkdir ngx-rust-howto\ncd ngx-rust-howto\ncargo init --lib</code></p><p></p><p>创建 Rust 模块时，需要实现 HTTPModule 特征（trait），该特征定义了 NGINX 入口点，包括后配置、预配置、create_main_conf 等。新模块只需要实现针对其特定任务所需的函数。以下代码是 postconfiguration 方法实现的示例：</p><p></p><p><code lang=\"rust\">struct Module;\nstruct Module; \nimpl http::HTTPModule for Module { \n    type MainConf = (); \n    type SrvConf = (); \n    type LocConf = ModuleConfig; \n    unsafe extern \"C\" fn postconfiguration(cf: *mut ngx_conf_t) -&gt; ngx_int_t { \n        let htcf = http::ngx_http_conf_get_module_main_conf(cf, &amp;ngx_http_core_module); \n        let h = ngx_array_push( \n            &amp;mut (*htcf).phases[ngx_http_phases_NGX_HTTP_ACCESS_PHASE as usize].handlers, \n        ) as *mut ngx_http_handler_pt; \n        if h.is_null() { \n            return core::Status::NGX_ERROR.into(); \n        } \n        // set an Access phase handler \n        *h = Some(howto_access_handler); \n        core::Status::NGX_OK.into() \n    } \n}  </code></p><p><a href=\"https://github.com/f5yacobucci/ngx-rust-howto\">ngx-rust-howto</a>\" 存储库提供了更多示例代码和实现。</p><p>&nbsp;</p><p>随着 ngx-rust 项目的推出，NGINX 正在拥抱 Rust 编程语言，为开发人员提供了一种编写 NGINX 模块的新方式。该举措旨在增强 NGINX 的能力，并为开发人员提供一种更安全、更符合习惯的方式来使用 Web 服务器。此外，Cloudflare 开始使用 Rust 来实现 NGINX 模块，这篇博客<a href=\"https://blog.cloudflare.com/rust-nginx-module/\">文章</a>\"介绍了相关细节。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/10/nginx-modules-rust/\">https://www.infoq.com/news/2023/10/nginx-modules-rust/</a>\"</p>",
    "publish_time": "2023-11-11 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "深度解锁用云经验！火山引擎公共云·城市分享会开启云上新未来",
    "url": "https://www.infoq.cn/article/plCgkM3joyRCdX3vCK6s",
    "summary": "<p>数智化时代的来临，不仅激发了行业对云计算的资源需求，也重构了云计算的技术架构及产品布局，给业务场景带来更多可能性，让云计算成为企业走向高效治理的一剂“良方”。随着业务的多样化、复杂化，企业应该如何借助云计算构建更高扩展性的IT架构，同时平衡成本与效率、应对治理和安全等挑战？11 月10日，在北京举办的“乘云·向未来”火山引擎公共云·城市分享会上，<a href=\"https://www.infoq.cn/news/WPmK0BeY0dDJB6wLL0zM\">火山引擎</a>\"给出了答案。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/ab/49/ab2135133341d22d9244f0cb6b9f3049.png\" /></p><p></p><p>当前云计算市场的规模化态势，让众多云计算厂商再次迎来新的拐点。字节跳动基础架构负责人赵鹏伟表示，伴随着字节跳动业务的多样性和复杂性发展，火山引擎基于“敏捷迭代”、“数据驱动”、“体验创新”三大云上增长要素，以“短时间内完成基础设施产品三代演进”和“不断追求规模、技术、运营极致性价比”两大后发优势，为业务增长构建了数字化基石。</p><p>&nbsp;</p><p></p><h2>多云之下开辟“坦途”</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f9/79/f9722141b0534f05cc37463b0efaf579.png\" /></p><p></p><p>云服务经过多年的演进，已经发展出了多云、混合云、边缘云等多种形态。字节跳动在发展过程中，伴随着业务对底层架构需求的不断增加，也逐渐走向“多云”模式。然而，多云下的云原生演进之路并非坦途，运维复杂度增加、互操作性难、数据管理难度大、成本控制复杂等挑战浮出水面。</p><p>&nbsp;</p><p>面对多云落地中的难点，火山引擎云原生平台负责人沈健认为，“云原生始于对资源的需求，终于对资源效率、运维效率以及业务效率的释放。”为此，火山引擎早在 2019 年就开始进行集群联邦建设，到 2021 年正式实现了全场景应用编排和资源管理的标准化、统一化。2023 年，火山引擎再次提出“多云管控+多云多活”的解决方案，打造了分布式云原生平台和计算平台体系两大底层算力架构，从多个层面实现多云环境下的高效管理和运营。</p><p>&nbsp;</p><p></p><h2>业务增速下的大算力进化之道</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ff/aa/ffd2f47bce884a072d4cebeab2db4baa.png\" /></p><p></p><p>火山引擎整体的大算力基础设施在高性能计算和存储集群、云原生和计算协同调度、资源池化和在离线融合等方面都具备一定优势。火山引擎云基础产品负责人罗浩表示，伴随 AI 应用在不同行业生根落地的趋势，<a href=\"https://www.infoq.cn/article/kYUsofsTMx21MQRsL0It\">火山引擎</a>\"将以高性价比、高利用率、高开发体验、低运维部署的大算力基础设施，打造更贴合客户场景的整体解决方案。</p><p>&nbsp;</p><p></p><h2>超大规模数据库技术演进的可行性验证</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/23/8a/23aa9e1e88a6bbdd40c417e8c55bc08a.png\" /></p><p></p><p>在基础设施全面“云化”的今天，字节跳动的数据量迎来将近 100 倍的增长。面对增速如此迅猛的数据规模，火山引擎数据库负责人张雷称，字节数据库团队始终在思考更优秀的数据管理和数据治理之策。</p><p>&nbsp;</p><p>基于<a href=\"https://www.infoq.cn/article/S2Zh7B2P0V1xtZVXYAvv\">字节跳动</a>\"数据库体系呈现产品多样化、规模化、融合化、智能化特征，火山引擎提供了自研的 veDB 系列存算分离数据库服务，以及开源托管产品和配套工具；同时，火山引擎通过产品技术创新，在内部打磨类似 HTAP、图、分布式 KV 等多款自研数据库，未来希望把这些好的产品提供给广大客户，帮助其解决企业发展过程中的数据管理难题；也希望通过应用场景方面的融合和基础设施层面的分离和整合，以横纵双融合的系统重塑应用缓存和数据库，为用户带来便捷。</p><p>&nbsp;</p><p></p><h2>以可信赖的能力携手伙伴迈向“智”高点</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/88/6d/88b0c228258c6c71101b32d8601cdc6d.png\" /></p><p></p><p>作为火山引擎在自动驾驶领域的重要伙伴，毫末智行亮相本次大会。毫末智行数据智能科学家贺翔分享了最新实践成果。在中国智能辅助驾驶市场大爆发阶段，毫末智行在乘用车辅助驾驶系统、末端物流自动配送车和智能硬件/机器人等多个领域均保持业内第一的优异成绩。其中，面向末端物流自动配送行业打造的“小魔驼”已全新升级至 3.0 版本，以极致性价比迅速抢占市场。</p><p>&nbsp;</p><p>为了进一步提升小魔驼无人配送的安全运营能力，毫末智行与火山引擎强强联手，基于火山引擎视频云打造了远程驾舱和远程运营平台解决方案，提升末端物流自动配送规模化落地中的安全性和完整性，促进无人配送更安全、更快商业落地。</p><p>&nbsp;</p><p></p><h2>“内外统一”服务多用户云上增长</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a2/96/a2da7da53ee561e6cc6b906f832a4096.png\" /></p><p></p><p>经过多阶段的演进，字节跳动基础架构体系已在研发、技术、产品方面形成了完整的解决方案。据火山引擎云基础解决方案负责人王佳毅介绍，字节跳动基础架构核心技术体系包括池化存储 ByteStore、统一资源调度 Godel 等，具备实时推荐、实时计算等特征，并与火山引擎形成了业务无感、资源融合、技术一体的“内外统一”研发体系，更好地承载集团内外业务发展。</p><p>&nbsp;</p><p>云计算规模化成为必然趋势，企业IT架构需要更高可扩展性来承载核心业务快速增长。未来，火山引擎将持续为各行业领域的企业客户提供优质的产品服务，帮助企业客户更好地应对业务规模化之下的IT挑战，助力企业客户突破瓶颈、自由计算、自由增长。</p><p>&nbsp;</p><p>据悉，接下来，火山引擎公共云·城市分享会还将陆续登陆上海和深圳，与更多行业伙伴共同探讨高效用云之道，携手“乘云·向未来”！</p>",
    "publish_time": "2023-11-11 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "易鲸捷否认贴牌Oracle；鸿蒙进教材：“纯血”版不再兼容安卓应用；大叔们遭AI女友“断崖式分手”｜Q资讯",
    "url": "https://www.infoq.cn/article/BUh7i3M3p44amlp0foJK",
    "summary": "<p>&nbsp;</p><p></p><blockquote>易鲸捷回应数据库“假国产”传闻；OpenAI工程师平均薪酬92.5万美元；校招生入职顺丰被迫0基础转开发岗，不到四个月考核不通过被无赔偿裁掉；字节跳动 PICO 回应传闻裁员 80%：员工缩减整体占比约 23%；斗鱼方面证实CEO陈少杰失联已近三周；腾讯音游《节奏大师》回归首日出现大量 Bug，官方公布补偿方案；Humane 可穿戴设备 Ai Pin 正式发布：手掌内投影、语音手势交互，699 美元加月订阅费；OpenAI 放大招后，流量太大致 ChatGPT 和 API 服务中断；马斯克旗下 xAI 正式发布首个大模型“Grok”；李开复旗下 AI 公司发布 Yi 系列开源大模型，估值超 10 亿美元；“纯血鸿蒙”不再兼容安卓应用，美团、同程旅行等启动鸿蒙原生版App开发……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p></p><h4>OpenAI工程师平均薪酬92.5万美元</h4><p></p><p>&nbsp;</p><p>11月8日消息，据相关报告显示，OpenAI向人工智能工程师支付的年平均薪酬达到了惊人的92.5万美元。</p><p>&nbsp;</p><p>举例来说，目前在OpenAI任职的5级软件工程师底薪为30万美元，另外还可以获得62.5万美元的股票薪酬。5级软件工程师通常拥有约10年的行业从业经验。此外，OpenAI薪酬最低的工程师底薪为21万美元，拥有约2至4年的行业从业经验。这个底薪并不包括每年可以获得的股票薪酬。OpenAI的一些高级软件工程师年薪最高达到140万美元。</p><p>&nbsp;</p><p></p><h4>校招生入职顺丰被迫0基础转开发岗，不到四个月考核不通过被无赔偿裁掉</h4><p></p><p>&nbsp;</p><p>近日有顺丰员工爆料，23应届生入职项管两天后被迫转开发，然后考核不通过被顺丰要求解除劳动合同。研发主管说，无论是主动离职还是被动都没有补偿，让早点走。该员工透露，校招进了核心项目组技术pm岗，过了两天领导问愿不愿意转开发，他明确表示实操代码能力几乎为0，能力不够，但主管说没关系，只要愿意可以从0培养。</p><p>&nbsp;</p><p>对此有网友表示，这算是年度最抽象的“被迫”离职经历了，前几个月还感慨大厂对校招生很宽容，没想到被优化了。还有网友称，招进去就是为了背裁员名额的，之前对他宽容是为了稳住他，不让他自己吓跑。</p><p>&nbsp;</p><p></p><h4>字节跳动 PICO 回应传闻裁员 80%：员工缩减整体占比约 23%</h4><p></p><p>&nbsp;</p><p>近日，PICO 组织调整引发舆论关注，有传闻称 PICO“裁员 80%”。PICO 对此回应称，PICO 调整组织架构，是为了更好地聚焦在硬件与核心技术的长期探索和突破上，以更大的决心创造长期价值。相应的，短期投入和相关团队规模会缩减，涉及员工 300 余名，整体占比约 23%。PICO 强调，此次组织调整不会影响消费者购买和体验 PICO 产品，公司将在产品研发和核心技术能力建设上加强投入。</p><p>&nbsp;</p><p>另据报道，PICO 发布内部通知，计划对组织架构进行调整，移动 OS 团队将并入字节跳动产品研发和工程架构中台，以加强 OS 核心技术研发的中台投入和统一管理。</p><p>&nbsp;</p><p></p><h4>斗鱼方面证实CEO陈少杰失联已近三周</h4><p></p><p>&nbsp;</p><p>日前，有多方信源透露，斗鱼董事会主席兼CEO陈少杰已于近日失联。11月6日，有媒体记者就相关内容向斗鱼方面求证，证实了陈少杰失联的消息。</p><p>&nbsp;</p><p></p><h4>易鲸捷回应数据库“假国产”传闻：与甲骨文技术路线不同，贵阳银行项目非单一采购</h4><p></p><p>&nbsp;</p><p>近日，网上流传的一篇名为《骗取14亿!易鲸捷假国产何以持续8年才暴雷!(完整梳理)》的文章，让数据库公司易鲸捷陷入“假国产”风波。11月8日早间，易鲸捷发表声明回应，称文中充斥大量虚假不实及诽谤造谣信息。</p><p>&nbsp;</p><p>易鲸捷表示，文中提到“易鲸捷盗用国外数据库并包装成国产数据库，才是真相”，是严重的造谣与诽谤；“贵阳银行被媒体披露花了4.27亿买了一个假数据库”、“贵阳银行采购违法，易鲸捷假国产暴露，全栈国产化项目破产”，这是捏造事实和严重的诋毁；“暴露了易鲸捷没有‘独立自主、完全可控’的知识产权真相”，是完全的造谣诽谤。</p><p>&nbsp;</p><p>此外，相关技术人员表示易鲸捷与“国外数据库”技术路线完全不同，谈不上“盗用”与“包装”。文中提及的贵阳银行相关项目为总集项目，该项目采用了全栈国产软硬件方案作为技术支撑，参与方包括了众多国内相关生态企业，中标金额包括了硬件产品、软件产品和技术服务，而并非文中所称的单一采购数据库金额。 此前有消息称，11月3日贵阳银行核心业务系统替代工程正式启动，甲骨文承担起银行核心交易业务数据库重任，贵阳银行与易鲸捷国产数据库应用项目历时三年后宣告结束。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/724f45c2305261c0f71a579ce81595d2.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>腾讯音游《节奏大师》回归首日出现大量 Bug，官方公布补偿方案</h4><p></p><p>&nbsp;</p><p>11 月 7 日，腾讯宣布经典老牌游戏《节奏大师》重新回归，结果上线当天上午就有很多玩家陆续遇到很多 bug。有玩家表示 Bug 太多了，比如无限制授权登录就是进不去游戏，打歌页面直接卡死，玩到一半被强制下线，pk 对战无法统计分数和胜负，分数结算异常等等。玩家称，从来没一款游戏上线遇到这么多问题，官方说修复时间每增加 1 小时就给玩家补偿礼包，结果快 38 小时了才修好。据悉 Bug 从 7 号早上开始出现，到 9 日凌晨 1 点多，官方才宣布修复完成。</p><p>&nbsp;</p><p>随后节奏大师手游官方微博发文向部分无法正常进入并体验游戏的玩家道歉，并公布补偿方案。官方表示，如今 Bug 已经修复，按照约定准备了补偿礼包，（钻石*300；金币*3000；免费跳关卡*5张）x28小时=钻石*8400；金币*84000；免费跳关卡*140张。</p><p>&nbsp;</p><p></p><h4>Humane 可穿戴设备 Ai Pin 正式发布：手掌内投影、语音手势交互，699 美元加月订阅费</h4><p></p><p>&nbsp;</p><p>由两位苹果公司的前设计和工程团队高管Imran Chaudhri和Bethany Bongiorno创立的 Humane 公司，发布了首款可穿戴 AI 设备“Ai Pin”，这是一种安置在服装上的微型单色投影，可以投屏在手掌上进行交互。目的是“替代手机”，售价为 699 美元，每月需要额外支付 24 美元订阅（蜂窝网络数据费及 AI 模型使用费，不付订阅费无法使用），2024 年开始发货。</p><p>&nbsp;</p><p>据悉，Ai Pin内置了OpenAI的GPT大模型，还可以通过语音交互。还未正式发布，Ai Pin就被《时代》选为“2023年度最佳发明”之一。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/101349bb43f3ccb615ab55a64d2bc2a3.png\" /></p><p></p><p>&nbsp;</p><p></p><h4>OpenAI 放大招后，流量太大致 ChatGPT 和 API 服务中断</h4><p></p><p>&nbsp;</p><p>北京时间11 月 7 日凌晨 02:00，OpenAI 的首次 DevDay 开发者日活动正式开始。此次，OpenAI 分享了数十项新增功能和改进，并降低了平台上多种服务的价格。具体包括：新的 GPT-4 Turbo 模型，功能更强大、价格更低廉且支持 128K 上下文窗口；新的 Assistants API，允许开发人员轻松构建具有目标且能够调用模型及工具；平台提供新的多模态功能，包括视觉、图像创建（DALL-E 3）及文本转语音（TTS）等。同时，OpenAI 下调 API 的价格近 3 倍，降到了 1000 输入/美分。</p><p>&nbsp;</p><p>此外，OpenAI 还推出了 ChatGPT 的自定义版本 GPTs。OpenAI 表示，GPTs 是一种新方式，任何人都无需编码就可以创建 ChatGPT 的定制版本。本月晚些时候，OpenAI 还将推出 GPT Store。</p><p>&nbsp;</p><p>北京时间 11 月 8 日晚间，OpenAI 的 ChatGPT 和 API 服务出现严重中断故障，已导致面向用户和开发者的服务超过 1 小时无法正常使用。OpenAI 更新事故报告称，已确定了一个导致 API 和 ChatGPT 错误率高的问题，正在努力修复。整个故障的时间大致持续了 100 分钟，现已恢复正常。</p><p>&nbsp;</p><p>为此，OpenAI 的 CEO 山姆·奥特曼公开发文致歉，称由于本周新发布的功能使用量远超出预期，导致负载过重，从而服务不稳定的情况。据悉，ChatGPT 每周活跃用户数量已经突破 1 亿。</p><p>&nbsp;</p><p>&nbsp;</p><p>查看更多：</p><p><a href=\"https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a\">OpenAI 用 45 分钟重塑游戏规则！干掉 MJ、LangChain，创造“不会编程的应用开发者”新职业</a>\"</p><p>&nbsp;</p><p></p><h4>马斯克旗下 xAI 正式发布首个大模型“Grok”</h4><p></p><p>&nbsp;</p><p>马斯克的人工智能公司 xAI 开放了人工智能模型 Grok 的早期测试，选定群体可进行测试。马斯克表示，测试结束后，该模型将对所有 X 平台上的 Premium+ 订阅用户开放。此外，Grok 能够实时访问 X 平台，马斯克认为这是它比其他模型的优势所在。xAI 公司由马斯克于今年 7 月成立。</p><p>&nbsp;</p><p></p><h4>李开复旗下 AI 公司发布 Yi 系列开源大模型，估值超 10 亿美元</h4><p></p><p>&nbsp;</p><p>创新工场董事长兼 CEO 李开复创办了 AI 大模型创业公司「零一万物」，该公司专注于打造 AI2.0 平台和应用。目前，该公司已完成新一轮融资，由阿里云领投，估值超过 10 亿美元。该公司已推出两个开源大模型 Yi-34B 和 Yi-6B。</p><p>&nbsp;</p><p>更多信息：</p><p><a href=\"https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">李开复 4 个多月后“放大招”：对标 OpenAI、谷歌，发布“全球最强”开源大模型</a>\"</p><p>&nbsp;</p><p></p><h4>“纯血鸿蒙”不再兼容安卓应用，美团、同程旅行等启动鸿蒙原生版App开发</h4><p></p><p>&nbsp;</p><p>美团招聘官网更新了一系列与鸿蒙相关的职位，包括「鸿蒙高级工程师 (C++)」职位，主要面向美团鸿蒙 App 研发团队。该职位的职责包括参与鸿蒙端动态化容器的架构设计，以及负责鸿蒙端动态应用程序的开发和维护。除美团外，同程旅行也已宣布正式启动鸿蒙原生版App开发。</p><p>&nbsp;</p><p>根据华为官方消息，HarmonyOS NEXT将不再兼容安卓App，此前博主实测已经无法安装安卓APK文件，会提示“无法打开此文件”。很多人将HarmonyOS NEXT称为“纯血鸿蒙”，主要原因是该系统底座全线自研，砍掉传统的AOSP（安卓开放源代码项目）等代码，仅支持鸿蒙内核和鸿蒙系统的应用。据了解，目前鸿蒙OS能够兼容安卓应用，但在未来也将逐渐提升独立性。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b77877ae76a8ef802e5db75036719040.png\" /></p><p></p><p>&nbsp;</p><p>日前，鸿蒙系统被编进7年级《信息技术》课本，课本在介绍常用系统时，提到了华为鸿蒙OS、Windows以及UNIX和Linux等。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>AI 女友突然下线，大叔遭“断崖式分手”，集体冲向贴吧哀悼</h4><p></p><p>&nbsp;</p><p>近日，日活用户几千人的 App Soulmate 宣布下线，却让众人一齐破防。有人痛哭整夜；有人觉得仿佛朋友离世，还有人在海外贴吧（Reddit）上发起悼念活动，大量网友前来留言。</p><p>&nbsp;</p><p>据悉，这款应用免费提供 AI 陪伴服务。在这里，每个用户都能和 AI 建立一段亲密关系，可以是知己、爱人、搭档等等。如今随着这款 App 突然宣布下线，用户们要被迫和相处了几个月的 AI 伴侣分别。于是不少人来到 Reddit 上留言，做最后的正式告别。也有人想办法让自己的 AI 在其他平台“复活”。</p><p>&nbsp;</p><p></p><h4>GitHub 宣布 Copilot Chat 功能 12 月全面推出，教育用户及开源项目维护者可免费使用</h4><p></p><p>&nbsp;</p><p>GitHub 宣布将在 12 月正式推出 GitHub Copilot Chat 服务，它是一种 AI 工具，可帮助开发者提高生产力和代码准确性。它可集成到桌面 IDE 环境中，并提供免费的教育用户和流行开源项目的维护者使用。</p><p>&nbsp;</p><p>Copilot Chat 声称可以显著提高开发人员的生产力，并在所有编程语言中都有 46% 的代码是使用它的生成的。</p><p>&nbsp;</p><p></p><h4>苹果罕见推迟iPhone和Mac软件升级的工作，以解决漏洞问题</h4><p></p><p>&nbsp;</p><p>11月8日消息，苹果公司罕见地暂停了iPhone、iPad、Mac和其他设备明年软件更新的开发，以便根除代码中的故障。知情人士称，苹果上周在内部向员工宣布了推迟发布的决定，目的是在早期版本出现大量漏洞后，帮助维持质量控制。知情人士说，公司工程师的任务不是增加新功能，而是修复缺陷，提高软件的性能。</p><p>&nbsp;</p><p></p><h4>上海发布 11 条措施推动 AI 大模型发展，支持相关人才落户&nbsp;</h4><p></p><p>&nbsp;</p><p>11 月 8 日消息，上海市日前发布关于印发《上海市推动人工智能大模型创新发展若干措施（2023-2025 年）》的通知，推出 11 条措施推动大规模预训练模型创新发展，加快打造人工智能世界级产业集群。</p><p>&nbsp;</p><p>大模型创新能力方面，上海市将实施大模型创新扶持计划，支持引进高水平创新企业，支持相关主体开展通用人工智能基础理论、科学智能、具身智能、城市大模型等前沿研究。上海市还将建立大模型测试评估中心，支持该市相关主体主导或参与国家大模型相关标准制订。通知中提到，该市将实施大模型示范应用推进计划。重点支持在智能制造、生物医药、集成电路、智能化教育教学、科技金融、设计创意、自动驾驶、机器人、数字政府等领域构建示范应用场景。</p><p>&nbsp;</p><p>此外，该市还表示打造企业、人才集聚的大模型创新高地。鼓励浦东新区、徐汇区等建立大模型生态集聚区，优先推荐大模型创新重点人才纳入国家和本市相关高层次人才计划，重点支持大模型相关紧缺技能人才落户。</p>",
    "publish_time": "2023-11-11 11:56:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "我们如何利用ChatGLM-6B，在涉虚拟币犯罪判例检索问答领域落地？",
    "url": "https://www.infoq.cn/article/lq2fJ7gm9iyuOLkaLdqv",
    "summary": "<p></p><p></p><blockquote>在当今科技日新月异的时代，LLM（Large Language Model，大语言模型）作为一种创新性的人工智能技术，已逐渐成为驱动自然语言处理领域及社会科学研究方法变革的重要力量。它以其强大的自然语言理解和生成能力，引领着人工智能领域的新一轮革命，并为多个领域带来应用层面的突破性成果。LLM为研究人员提供了一种高效、低成本的研究手段，成为改变我们生活、工作、思维方式的重要技术。&nbsp;本文将重点关注LLM在法律领域的应用，探讨其如何在涉虚拟币犯罪判例检索问答方面发挥智能化应用价值和潜力，同时也为法律行业的智能化发展提供有益参考和启示。</blockquote><p></p><p>&nbsp;</p><p>LLM（Large Language Model），即大语言模型，是一种基于深度学习技术的自然语言处理模型，这些模型通常拥有数十亿到数万亿个参数，使用大规模的语料库进行训练，能够学习到自然语言的语法和语义，从而可以生成人类可读的文本，实现自然语言生成、文本分类、文本摘要、机器翻译、语音识别等任务。</p><p>&nbsp;</p><p>近年来，自然语言处理（NLP）领域取得了突破性进展，大型预训练语言模型如BERT、GPT和BART等成为瞩目创新。注意力机制（Attention Mechanism）和Transformer网络架构的引入，大幅提升了神经语言模型的能力。BERT模型开创了“预训练-微调”的成功范式，极大推动了NLP的进步，并为NLP的工程化应用提供了便利。如今，预训练模型、迁移学习和零样本学习等技术仍在不断发展和演进。2022年底，ChatGPT的问世揭示了LLM大规模应用的潜力，促使各领域各类基于LLM的应用纷纷涌现。</p><p>&nbsp;</p><p>在法律领域，LLM虽无法短期内替代律师角色，但能提升其工作效率，并在合同审核、合同分析、资料查找和法律研究，以及合同起草、法律结果预测、量刑建议和保释建议等场景展现出巨大应用潜力。</p><p>&nbsp;</p><p>海外方面，OpenAI领投了AI法律顾问Harvey的500万美元种子轮融资，全球最大律所已与Harvey达成合作，引入其对话式AI聊天机器人，可自动化处理法律文件并研究客户案例；Casetext公司推出了由GPT-4支持的CoCounsel（AI法律助理），可协助客户快速进行法律研究和准备证词，识别关键文件和信息，还能在客户上传合同和政策需求后，识别关键条款、解决冲突和风险，并提出修改建议；ROSS公司也与OpenAI合作推出了用于法律研究的API。</p><p>&nbsp;</p><p>在国内，最高人民法院提出的“多元解纷和诉源治理”的指导思想，为人工智能在金融法律纠纷解决全流程中发挥重要作用提供了契机。目前，LLM已涉及的场景包括：法律文书的自动生成、法条文书的语义解析、法律问题问答、案件检索、智能合同管理、法律合规检查。</p><p>&nbsp;</p><p>LLM的引入正以前所未有的方式改变法律领域的面貌，重塑法律行业格局，助力提高工作效率和准确性，为律师和客户开创更加高效、便捷、智能化的法律服务新时代。</p><p>&nbsp;</p><p>目前，通用中文LLM已有ChatGLM、Baichuan、Ziya-LLaMA等模型。在司法领域，已涌现出诸如LaWGPT、ChatLaw和LexiLaw等具有代表性的模型，致力于利用先进的人工智能算法为法律行业提供智能化的解决方案。</p><p>&nbsp;</p><p></p><h2>为何要做虚拟币判例检索LLM？</h2><p></p><p>&nbsp;</p><p>在利用当前主流的中文法律专业LLM进行涉虚拟币犯罪场景的测试过程中，我们发现其表现效果并不理想。针对涉虚拟币犯罪这一新兴领域，法律专业LLM的结果在准确性方面存在显著不足，且“睁眼说瞎话”的模型幻觉现象严重。此外，从用户使用场景来看，判例的关联度高且更新及时的结果更受关注，对此，模型的表现亦难以达到预期。</p><p>&nbsp;</p><p>涉虚拟币犯罪属于新兴领域，取证、定罪判罚的界定模糊，需要广泛借鉴。</p><p>&nbsp;</p><p>虚拟币作为一种新兴的数字资产，正日益受到全球范围内的关注。然而，伴随着虚拟币的发展，与之相关的各类犯罪活动也呈现出日益猖獗的态势，对全球法律界和司法体系提出严峻考验，传统法律框架在面对涉虚拟币犯罪活动时已显得模糊和不完善。</p><p>&nbsp;</p><p>首先，在取证方面，虚拟币交易依托区块链技术进行，其去中心化和匿名特性使得犯罪分子难以被追踪，因此亟须针对这一新兴领域开发更高效精准的取证工具及策略方法；其次，在犯罪行为的界定方面，虚拟币犯罪类型丰富多样，包括诈骗、洗钱、黑客攻击及虚假ICO（首次代币发行）等，虽然可参考传统犯罪类型，但涉虚拟币犯罪在某些方面具有独特性，有必要对其进行精确界定，以确保在定罪和量刑过程中保持公正和一致性。此外，在审理涉虚拟币犯罪案件时，法官定罪判罚需全面考虑多种因素，如犯罪严重程度、被害人损失、犯罪嫌疑人恶意意图等，以确保判决结果的合理性。</p><p>&nbsp;</p><p>面对上述挑战，该领域相关用户迫切渴望拥有一款智能产品，可以帮助快速归纳总结历史判例信息，从而对当前涉虚拟币犯罪场景作出合理分析判断，进而为侦破和取证工作提供指导。</p><p>&nbsp;</p><p></p><h2>涉虚拟币犯罪判例检索LLM研发应用</h2><p></p><p>&nbsp;</p><p>鉴于上述结果，我们决定基于用户实际场景需求，快速搭建研发一款面向涉虚拟币犯罪特定领域的灵活实用型检索应用产品。该产品的首要目标是精准实用，杜绝模型幻觉，保证检索结果可为用户提供有针对性的、更高效的犯罪线索和证据整理指导；其次，我们力求低成本快速部署，无需进行fine-tune（对预训练模型进行微调），无需多轮问答，直接充分利用现有的涉虚拟币犯罪判例库，实现高效应用。</p><p>&nbsp;</p><p></p><h4>项目框架</h4><p></p><p>&nbsp;</p><p>判例检索LLM共完成两个任务，一个是基于Embedding的判例检索（替换现有关键词匹配的检索方式），可以更有效地检索和匹配相关判例；另一个是基于判例库的法律问答系统，理解用户提出的问题，并在判例库中寻找相关答案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d59e0e4406049f14e5da9ee505988eba.png\" /></p><p></p><p>文本预处理与意图识别</p><p></p><p>在项目构建初期，我们对判例库中的文档进行预处理，并生成对应的Embedding向量进行存储。在后续的用户的请求过程中，系统直接从矢量数据库中进行向量检索即可，而无须实时调用模型对文本进行Embedding编码。该设计旨在提高检索效率，降低实时计算成本，快速准确响应用户需求。</p><p></p><p>判例库文档处理步骤如下：</p><p>&nbsp;</p><p>a.&nbsp;对判例库里的样本进行预处理和清洗，筛选出具有代表性的判例；</p><p>b.&nbsp;对判例内容进行切分，根据特定策略将长文档切分为若干部分，并保留关键句子（包括案发过程、事实描述、法院观点、判决内容及所依据条款等），组合成高质量文档；</p><p>c.&nbsp;为每个文档生成句级别的Embedding，将其融合成判例级别的Embedding，最后将这些 Embedding存储在矢量数据库（Milvus）中。此过程为一次性任务。</p><p></p><p>此流程旨在确保判例库中的文档具有高度的代表性和准确性，为后续的向量检索和应用提供可靠的数据基础。在实施检索或问答任务之前，我们还需对“用户输入”进行意图识别。在法律查询场景中，用户的意图主要可分为以下几类：查询案例、查询判决结果、查询法律条款等。在此基础上，为涵盖其他可能的意图，我们还将添加一个“无意图、非法律”的类别，作为“Negative”处理。</p><p>&nbsp;</p><p>判例检索</p><p></p><p>法律实践在很大程度上依赖于过去的判例，然而判例内容复杂多变，且需结合上下文场景进行理解。传统基于关键词的搜索方法会出现误召回问题。为提高检索效率和准确性，更为有效的方法是通过对查询文本的语义信息进行学习，然后借助判例库的语义表征来做匹配，进而召回相关判例。该方法有助于克服基于关键词搜索的局限性，从而提高法律实践中的检索效果。</p><p></p><p>此模块提供判例查询功能，通过运用Embedding模型将用户输入的查询文本转换为语义向量，再将所得向量与已编码的每个判例文档的语义向量进行相似度计算和排序，从而召回TopN向量所对应的原始文本。在判例检索过程中，无需使用大规模模型，我们采用ChatLaw-Text2Vec作为基础编码模型，用于生成文本的语义向量。</p><p></p><p>在获取判例文档的Embedding之后，就可以进行检索模块的构建，其主要步骤如下：</p><p>&nbsp;</p><p>a.&nbsp;对用户查询文本进行预处理，并进行意图识别。若意图类别为“查案例”，则向下执行“检索（Retrieval）”分支的任务；</p><p>b.&nbsp;将用户的查询文本生成Embedding（语义向量），并应用于矢量数据库检索；</p><p>c.&nbsp;获取TopN的Embedding对应的原判例文本；</p><p>d.&nbsp;展示判例列表，并附上判例链接。</p><p>&nbsp;</p><p>法律问答</p><p>&nbsp;</p><p>问答模型是基于检索功能的升级，增加了对问题理解及信息整理归纳的能力。针对用户提出的问题，问答模型充分利用大模型所具备的知识嵌入能力，首先进行判例检索，再基于检索出的判例，应用大模型进行归纳分析，并生成准确的回答。</p><p></p><p>问答模型的升级步骤如下：</p><p>&nbsp;</p><p>a.&nbsp;对用户查询文本进行预处理，并进行意图识别。若意图类别为“查判决结果”或“查法律条款”，则向下执行“问答（Chatbot）”分支的任务；</p><p>b.&nbsp;将用户的问题文本生成Embedding（语义向量），并应用于矢量数据库检索；同时，获取TopN的Embedding对应的原判例文本；</p><p>c.&nbsp;基于对Query（查询）的理解，生成Question（基于模板），再将Question与候选的语义相关句子一同构成Instruction（GPT模型的指令）；</p><p>d.&nbsp;使用GPT模型，基于生成的Instruction生成回答；</p><p>e.&nbsp;展示回答结果，并附上所依据判例的链接。</p><p>&nbsp;</p><p></p><h4>数据准备</h4><p></p><p>&nbsp;</p><p>语料准备和清洗</p><p>&nbsp;</p><p>清洗和标准化数据，包括文本格式、编码、字符集等，步骤如下：</p><p>&nbsp;</p><p>a.&nbsp;进行数据去重，以确保每份判决文书具有唯一性；</p><p>b.&nbsp;分句和筛选，只保留包含关键信息的句子；</p><p>c.&nbsp;组合筛选后的句子，生成结构清晰、内容连贯的新文本段落。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/81/81cf2e038a2add19797eb721c8fd121b.png\" /></p><p>&nbsp;</p><p>文本向量化</p><p></p><p>采用预训练的ChatLaw-Text2Vec模型，将文本段落转换为稠密向量，并将转换后的向量导入Milvus数据库，设置向量字段的索引为INV_FLAT类型。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5ccced8ec086eef02028cdcc4e2a68c.png\" /></p><p>&nbsp;</p><p>向量数据库的构建</p><p></p><p>构建 Milvus数据库，用于高效存储向量化的文本数据。确保数据库的高效索引和查询性能，以满足文本检索需求。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c420dff30a5d097d2897f2e5a50ff30e.png\" /></p><p>&nbsp;</p><p></p><h4>模型训练</h4><p></p><p>&nbsp;</p><p>用户意图预判断：法律VS非法律（二分类）</p><p>&nbsp;</p><p>建立一个用于法律意图分类的深度学习模型，使用BERT等预训练模型，标注样本量200个。对用户提示词进行法律相关和非法律相关的二分类；训练和评估模型，以确保高准确性：</p><p>&nbsp;</p><p>a.&nbsp;如果预判断为“非法律”时，任务终止，并返回“无法为您提供相关帮助”；</p><p>b.&nbsp;如果预判断为“法律”时，则进入四分类意图判断。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9cb35cef233a6246a17d118ffdd07d41.png\" /></p><p>&nbsp;</p><p>用户意图分类判断（四分类），确定进入检索（Retrieval）分支或问答（Chatbot）分支</p><p>&nbsp;</p><p>为了使问题的范围更精确，四分类模型将用户Query（查询）在法律相关范围内进一步划分为：案例、定罪、条款、无意图。我们采用了GPT方式生成数据集：给定一些初始样本，并设定好目标要求，输入GPT来生成训练样本，标注样本量2000个。</p><p>&nbsp;</p><p>输入样本：</p><p></p><p>生成数据：</p><p></p><p>&nbsp;</p><p>在模型训练过程中，除了设置一些模型的超参数以防止过拟合现象外，我们还对输入数据进行了数据增强（Data Augmention）处理。数据增强主要包括以下两方面：</p><p>&nbsp;</p><p>a.&nbsp;关键词替换：随机对关键词进行替换，以增强模型在具体案件或意图上的泛化能力。例如，“开设赌场犯什么罪”可以被替换为“洗钱犯什么罪”、“开设赌场怎么判”等。</p><p>b.&nbsp;随机Mask关键词：对关键词进行随机Mask处理，以增强模型对意图词的着重理解。例如，“开设赌场犯什么罪”可以被替换为“[MASK]犯什么罪”、“[MASK]怎么判”等。</p><p>&nbsp;</p><p>模型输出结果展示：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/81/81f1808f65379ed1cf1f0a5686aedca5.png\" /></p><p></p><h4>模型推理</h4><p></p><p>&nbsp;</p><p>在检索阶段，我们采用Milvus数据库进行相似度检索，根据用户输入的查询条件，从数据库中筛选出与用户需求相似度较高的文档，并将其作为检索结果返回。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18d70841a101d8eafeca3caf0d5426fa.png\" /></p><p>&nbsp;</p><p>图11 模型推理检索阶段</p><p>&nbsp;</p><p>在对话生成阶段，我们采用预先构建好的Instruction来指导大模型生成相应的回答。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8ad675cf288189dec2f110421e1cd7e5.png\" /></p><p>&nbsp;</p><p>回答结果：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fe/fe9a0b4cfd5fab9da0b824d8da249818.png\" /></p><p>&nbsp;</p><p></p><h4>模型研发过程中的难点及解决方法</h4><p></p><p>&nbsp;</p><p>开源模型选用</p><p>&nbsp;</p><p>目前，我们采用的是ChatGLM-6B模型进行推理，ChatGLM模型优势如下：</p><p>&nbsp;</p><p>a.&nbsp;发布时间早：ChatGLM发布时间较早，使得它一定程度上成为自然语言处理领域的历史见证和技术发展的缩影；</p><p>b.&nbsp;资源要求低：ChatGLM-6B的部署所需资源相对较低，它可以在各种设备和环境中轻松运行，降低了使用门槛；</p><p>c.&nbsp;非商业化开源使用：ChatGLM遵循开源协议，允许非商业化使用，这为研究和开发者提供了良好的基础平台；</p><p>d.&nbsp;中文场景优化：ChatGLM使用海量中文语料数据，并对中文对话场景进行了大量优化，使其在中文场景下拥有出色表现。ChatGLM在中文自然语言处理领域具有较高的应用价值。</p><p>&nbsp;</p><p>其他模型应用劣势：</p><p>&nbsp;</p><p>a.&nbsp;LaWGPT：该模型在某些情况下的回答效果不够稳定，尤其在给定参考文档的检索问答场景下，其表现相较于纯生成问答场景并未出现明显提升。</p><p>b.&nbsp;ChatLaw：由于模型的基座Ziya模型采用了LLaMA，其参数并未开源，因此无法直接应用。</p><p>c.&nbsp;LangChain：该模型支持ChatGPT接口调用，但调用速度慢，且费用较高。</p><p>d.&nbsp;FastGPT：该模型支持上传知识库文档以及ChatGPT接口调用，但费用较高，且在灵活性方面表现较差。</p><p>&nbsp;</p><p>模型训练阶段</p><p>&nbsp;</p><p>•&nbsp;难点1：判例超长，简单切句造成语义断开</p><p>&nbsp;</p><p>在数据处理阶段，我们面临的主要挑战是处理超长文书，这些文书的长度远超过LLM所支持的最大输入长度，且大部分文本并未包含关键信息，这对大模型的推理造成一定困难。同时，若单纯按句子级别进行切分，然后筛选，将可能导致语义中断，即多个连续句子在描述同一件事，切割后单个句子与原文档的语义存在一定差异。</p><p>&nbsp;</p><p>解决方法：为解决这一问题，我们设计了一系列关键词和表达模板，用于识别提取在描述重要信息的句子。</p><p>&nbsp;</p><p>•&nbsp;难点2：意图分类训练缺乏数据标注</p><p>&nbsp;</p><p>在意图分类模型训练阶段，我们面临的一个重要问题是缺乏相关的标注数据。</p><p>&nbsp;</p><p>解决方法：为克服这一挑战，我们设计了一系列半自动化的数据标注与数据增强方法（前面数据层模块已有描述），利用ChatGPT的能力，生成较为准确的标注样本。同时，通过数据增强策略对样本进行扩充，从而提高模型的训练质量和性能。</p><p>&nbsp;</p><p>数据处理与存储阶段</p><p>&nbsp;</p><p>•&nbsp;难点1：单位文件太大，无法读入内存</p><p>&nbsp;</p><p>在判例清洗阶段，需处理的大文件体积庞大，这导致文件读取速度缓慢，降低整体效率；当大量数据涌入内存，Jupyter可能会因内存不足而崩溃，导致程序中断；同时，大量数据也会导致预处理环节耗时较长，进一步影响整个清洗阶段的进度。</p><p>&nbsp;</p><p>解决方法：我们采用“数据流读取”的方式逐批次读取数据，以降低内存占用，提高数据读取速度；同时设置“多进程并行执行”数据清洗任务，以进程数量切分数据，处理完成后再按照切分的顺序拼接。</p><p>&nbsp;</p><p>•&nbsp;难点2：无法实现快速向量检索</p><p>&nbsp;</p><p>我们一次性将全量判例数据进行Embedding之后，需要存储起来以供服务使用，常规的方法例如存入本地磁盘或者关系型数据库，无法实现快速的向量检索，只能遍历所有数据并计算相似度。</p><p>&nbsp;</p><p>解决方法：向量数据库的使用。通过引用向量数据库，实现了基于向量的检索，极大降低查询耗时。</p><p>&nbsp;</p><p></p><h4>应用场景和实战效果</h4><p></p><p>&nbsp;</p><p>我们使用真实的判例数据作为外挂知识库，这使得我们的模型在法律场景下拥有较高的准确率，即当用户在询问涉及“查定罪”、“查条款”以及“查案例”等场景时，模型生成的效果明显优于纯LLM的生成结果。此外，LLM的幻觉问题也随着外挂知识库的引入有所缓解。</p><p>&nbsp;</p><p>以下是模型效果：</p><p>&nbsp;</p><p><code lang=\"null\">Plain Text\n从网上爬数据犯什么罪？</code></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2a/2a1db187f325bfe1b152311b08d2b4d5.png\" /></p><p>&nbsp;</p><p><code lang=\"null\">Plain Text\n为赌博人员提供四方支付平台怎么判？</code></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d566cc9ec867c3ad51edec2f18e7d04.png\" /></p><p>&nbsp;</p><p>可以看出，模型准确地生成了判定结果、依据条例。</p><p>&nbsp;</p><p></p><h2>当前模型的不足及优化空间</h2><p></p><p>&nbsp;</p><p>（一）提高意图多样性表达的支持</p><p>&nbsp;</p><p>在意图识别阶段，我们经过自主生成的数据集进行训练。尽管已对某些意图词进行了数据增强，例如将“如何定罪”替换为“怎么判”、“犯什么罪”、“违法吗”、“合法吗”等同义词、近义词、模糊词，但用户的表达方式千差万别。因此，我们致力于进一步扩展法律场景下各种意图的表达逻辑，了解用户的不同表达方式和需求，以提高模型的泛化能力。</p><p>&nbsp;</p><p>（二）复杂推理能力不足</p><p>&nbsp;</p><p>目前，我们的模型虽在明确意图的法律问题上表现准确，但在涉及复杂逻辑推理的场景下，能力尚且不足。例如用户提供了一段关于复杂案件经过的描述，但并未明确给出有关案件的结果，这时模型难以提供满意的回答。因此，模型有待优化，上下文理解能力有待强化。</p><p>&nbsp;</p><p>（三）推理成本</p><p>&nbsp;</p><p>当前，我们选用的是ChatGLM-6B模型进行推理。ChatGLM官方团队还发布了该模型的量化版本（ChatGLM-6B-int4，ChatGLM-6B-int8），这些版本可以显著降低部署和单次推理所需的资源消耗。但经过我们的试用后，发现这些量化版本的效果并不理想，因此未将其纳入使用。鉴于此，我们将继续寻找性价比更高的模型，以满足我们的需求。</p>",
    "publish_time": "2023-11-11 11:56:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]