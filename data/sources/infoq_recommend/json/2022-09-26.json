[
  {
    "title": "Java近期新闻：Helidon Níma、Spring Framework、MicroProfile、MicroStream、Kotlin和Piranha",
    "url": "https://www.infoq.cn/article/d9jp2N9XlIiCbfgWpd9P",
    "summary": "<p>本期的Java新闻包括JDK 19、JDK 20、Spring框架的更新、Spring Cloud与Spring Tools、Helidon Níma、MicroProfile Reactive规范、Quarkus 2.12.2、MicroStream 7.1.0、Reactor项目2022.0.0-M6、Hibernate Search 6.1.7、JHipster Lite 0.15.1、Piranha Cloud 22.9.0、Kotlin 1.7.20-RC和Apache Tika 1.28.5。</p><p></p><h4>JDK 19</h4><p></p><p><a href=\"https://openjdk.org/projects/jdk/19/\">JDK 19</a>\"已经于2022年9月20日正式发布。<a href=\"https://jdk.java.net/19/release-notes\">发布说明</a>\"包含了文档的链接，比如<a href=\"https://cr.openjdk.java.net/~iris/se/19/latestSpec//api/index.html\">完整的API规范</a>\"以及一个<a href=\"https://cr.openjdk.java.net/~iris/se/19/latestSpec/apidiffs/overview-summary.html\">标注的API规范</a>\"，后者对比了JDK 18（<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-18%2B36\">Build 36</a>\"）和JDK 19（<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-19%2B36\">Build 36</a>\"）的差异。关于JDK 19的更多细节和对JDK 20的预测可以参阅InfoQ的<a href=\"https://www.infoq.com/news/2022/09/java-19-so-far/\">新闻报道</a>\"。</p><p></p><h4>JDK 20</h4><p></p><p>JDK 20的<a href=\"https://jdk.java.net/20/\">早期访问构建</a>\"版本<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B15\">Build 15</a>\"发布，它是对Build 14的<a href=\"https://github.com/openjdk/jdk/compare/jdk-20%2B14...jdk-20%2B15\">更新</a>\"，包括对各种[问题](https://bugs.openjdk.org/issues/?jql=project %3D JDK AND fixversion %3D 20 and \"resolved in build\" %3D b15 order by component%2C subcomponent)的修复。关于该版本的更多细节，请参阅<a href=\"https://jdk.java.net/20/release-notes\">发布说明</a>\"。</p><p></p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/19/\">JDK 19</a>\"和<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"，鼓励开发者通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告缺陷。</p><p></p><h4>Spring框架</h4><p></p><p>Spring框架向Java社区<a href=\"https://spring.io/blog/2022/09/15/spring-framework-6-0-0-m6-and-5-3-23-available-now\">发布了</a>\"6.0.0-M6和5.3.23版本版本。这两个版本都提供了新特性、缺陷修复和依赖升级。5.3.23版本引入的新特性是<a href=\"https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/annotation/AnnotationUtils.html\">AnnotationUtils</a>\"类中定义的**isSynthesizedAnnotation()方法，它能够让开发人员放弃已废弃的<a href=\"https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/annotation/SynthesizedAnnotation.html\">SynthesizedAnnotation</a>\"接口。6.0.0-M6版本定义了七个废弃的功能，并且将会移除两个之前定义的废弃功能，其中包括SynthesizedAnnotation**接口。关于这两个版本的更多细节可以参阅<a href=\"https://github.com/spring-projects/spring-framework/releases/tag/v5.3.22\">5.3.23</a>\"和<a href=\"https://github.com/spring-projects/spring-framework/releases/tag/v6.0.0-M6\">6.0.0-M6</a>\"版本的发布说明。</p><p></p><p>Spring Cloud Dataflow 2.9.6<a href=\"https://spring.io/blog/2022/09/14/spring-cloud-dataflow-2-9-6-released\">发布</a>\"，特性包括升级PostgreSQL驱动版本至42.2.26以解决CVE-2022-31197漏洞，即<a href=\"https://github.com/advisories/GHSA-r38f-c4h4-hqq2\">通过恶意的列名能够在ResultSet.refreshRow()中触发PostgreSQL JDBC驱动的SQL注入</a>\"，该漏洞是由于**ResultSet类中refreshRow()**方法的实现没有正确的转义列名，所以包含语句终结符（比如分号）的恶意列名会导致SQL注入。关于该版本的更多细节可以参阅<a href=\"https://github.com/spring-cloud/spring-cloud-dataflow/releases/tag/v2.9.6\">发布说明</a>\"。</p><p></p><p><a href=\"https://github.com/spring-projects-experimental/spring-cloud-sleuth-otel/blob/main/README.adoc\">Spring Cloud Sleuth OpenTelemetry</a>\" 1.1.0版本<a href=\"https://spring.io/blog/2022/09/16/spring-cloud-sleuth-opentelemetry-otel-1-1-0-has-been-released\">发布</a>\"，这是<a href=\"https://spring.io/projects/spring-cloud-sleuth\">Spring Cloud Sleuth</a>\"的一个实验性扩展，其中包括了对Spring Cloud 2021.0.4和OpenTelemetry 1.18.0的依赖升级。关于该版本的更多信息请参阅<a href=\"https://github.com/spring-projects-experimental/spring-cloud-sleuth-otel/releases/tag/v1.1.0\">发布说明</a>\"。</p><p></p><p>Spring Tools 4.16.0<a href=\"https://spring.io/blog/2022/09/16/spring-tools-4-16-0-released\">发布</a>\"，特性包括：支持<a href=\"https://eclipseide.org/\">Eclipse 2022-09</a>\"；适用于ARM上Linux环境的实验性发行版；更新<a href=\"https://www.eclipse.org/m2e/\">M2Eclipse</a>\"（m2e）2.0.5。关于该版本的更多细节，可以参阅<a href=\"https://github.com/spring-projects/sts4/wiki/Changelog#2022-09-16-4160-release-incl-language-servers-version-1390\">变更日志</a>\"。</p><p></p><h4>Helidon</h4><p></p><p>甲骨文<a href=\"https://medium.com/helidon/please-welcome-helidon-n%C3%ADma-9a882c5b6f1e\">引入了</a>\" Helidon Níma，这是一个基于虚拟线程的微服务框架，它提供了一个低开销、高并发的服务器，同时保持了阻塞式的线程模型。在<a href=\"https://helidon.io/\">Helidon项目</a>\"的协助下，这个新的框架随Helidon 4.0.0发布了<a href=\"https://medium.com/helidon/helidon-n%C3%ADma-helidon-on-virtual-threads-130bb2ea2088\">第一个alpha版本</a>\"，但是Java社区需要2023年底才能等到正式的GA版本。关于Helidon Níma的更多细节，可以参阅InfoQ的<a href=\"https://www.infoq.com/news/2022/09/introducing-helidon-nima/\">新闻报道</a>\"。</p><p></p><h4>MicroProfile</h4><p></p><p>在通往<a href=\"https://microprofile.io/\">MicroProfile</a>\" 6.0的路上（计划2022年10月发布），<a href=\"https://github.com/eclipse/microprofile-reactive-streams-operators/releases/tag/3.0\">Reactive Streams Operators 3.0</a>\"和<a href=\"https://github.com/eclipse/microprofile-reactive-messaging/releases/tag/3.0\">Reactive Messaging 3.0</a>\"规范向Java社区发布，其特性与Jakarta EE 9.1保持了一致。</p><p></p><h4>Quarkus</h4><p></p><p>Red Hat<a href=\"https://quarkus.io/blog/quarkus-2-12-2-final-released/\">发布</a>\"了Quarkus 2.12.2.Final，包括了SnakeYAML 1.3.2、Hibernate Validator 6.2.5.Final和JBoss Threads 3.4.3.Final的依赖升级。关于该版本的更多细节可以参阅<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.12.2.Final\">变更日志</a>\"。</p><p></p><h4>MicroStream</h4><p></p><p>MicroStreams<a href=\"https://microstream.one/blog/article/release-of-microstream-version-7-1/\">发布</a>\"了其7.1.0版本的对象-图持久化框架，特性包括：集成Spring Boot；改善与CDI和MicroProfile Config运行时的集成；改进了数据通道的垃圾收集。此外，他们还开源了所有的连接器，现在包括Oracle和SAP HANA数据库、 Cloud存储（AWS S3、Azure Storage、Google Firestore、Oracle Object Storage）以及其他资源（Hazelcast、Kafka、Redis、DynamoDB、Oracle Coherence）。关于该版本的更多信息请参阅<a href=\"https://github.com/microstream-one/microstream/releases/tag/07.01.00-MS-GA\">发布说明</a>\"。</p><p></p><h4>Reactor项目</h4><p></p><p>在通往<a href=\"https://github.com/reactor/reactor/blob/main/README.md\">Reactor项目</a>\"2022.0.0的路上，<a href=\"https://github.com/reactor/reactor/releases/tag/2022.0.0-M6\">第六个历程碑版本</a>\"发布，其特性是对**reactor-core** 3.5.0-M6和**reactor-netty** 1.1.0-M6制品的依赖升级。此外，还对第六个里程碑版本进行了调整，reactor-pool 1.0.0-M6、reactor-addons 3.5.0-M6和**reactor-kotlin-extensions** 1.2.0-M6这些制品保持不变。</p><p></p><h4>Hibernate</h4><p></p><p><a href=\"https://hibernate.org/search/\">Hibernate Search</a>\" 6.1.7.Final发布，它将依赖升级到了Hibernate ORM 5.6.11.Final；将所有包含**-orm6**名称的制品与Hibernate ORM的依赖保持一致；以及Java模块相关缺陷的修复。</p><p></p><h4>JHipster Lite</h4><p></p><p><a href=\"https://github.com/jhipster/jhipster-lite/blob/main/README.md\">JHipster Lite</a>\"的0.15.0和0.15.1版本<a href=\"https://twitter.com/pascalgrimaud/status/1570138502850920448\">发布</a>\"，它是JHipster的启动项目，包含许多功能增强、错误修复、依赖性升级和重构。关于这个版本的更多细节可以在<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.15.0\">0.15.0</a>\"和<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.15.1\">0.15.1</a>\"版本的发布说明中找到。</p><p></p><h4>Piranha</h4><p></p><p>Piranha 22.9.0<a href=\"https://github.com/piranhacloud/piranha/releases/tag/v22.9.0\">发布</a>\"。这个新版本被称为2022年9月的“Core Profile just landed”版本，其特性包括：支持通过<a href=\"https://github.com/piranhacloud/piranha/issues/2775\">Piranha Core Profile</a>\"引入Jakarta EE Core Profile；以及对<a href=\"https://jakarta.ee/specifications/transactions/2.0/\">Jakarta Transactions</a>\"和<a href=\"https://jakarta.ee/specifications/persistence/3.1/\">Jakarta Persistence</a>\"规范的初始支持。关于这个版本的更多细节可以在他们的<a href=\"https://javadoc.io/doc/cloud.piranha/project/latest/index.html\">文档</a>\"和<a href=\"https://github.com/piranhacloud/piranha/issues?q=is%3Aissue+-label%3Awontfix+milestone%3A22.9.0+is%3Aclosed\">问题跟踪页面</a>\"中找到。</p><p></p><h4>Kotlin</h4><p></p><p>KotlinJetBrains<a href=\"https://blog.jetbrains.com/kotlin/2022/09/kotlin-news-august/\">发布了</a>\"Kotlin 1.7.20-RC，其特性包括：支持多个新的<a href=\"https://kotlinlang.org/docs/whatsnew-eap.html#support-for-kotlin-k2-compiler-plugins\">插件</a>\"；预览用于开闭式范围的**..&lt;**操作符；默认启用Kotlin/Native内存管理器；以及增加具有通用底层类型的内联类，这是一个实验性功能。</p><p></p><h4>Apache软件基金会</h4><p></p><p>Apache Tika 1.28.5<a href=\"https://www.mail-archive.com/announce@apache.org/msg07572.html\">发布</a>\"，其特性包括：安全问题修复；修复从PDF中提取书签时出现无限循环的问题；以及依赖性升级。该版本的详细信息可以在<a href=\"https://downloads.apache.org/tika/1.28.5/CHANGES-1.28.5.txt\">更新日志</a>\"中找到。1.x版本的发布列车将在2022年9月30日结束生命周期。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/09/java-news-roundup-sep12-2022/\">Java News Roundup: Helidon Níma, Spring Framework, MicroProfile, MicroStream, Kotlin, Piranha</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/pQ7VDmaij1aCT9TD0R06\">在 Java 中如何加快大型集合的处理速度</a>\"</p><p><a href=\"https://www.infoq.cn/article/3IgHpkRJIsFXm0vPNvFc\">甲骨文新微服务框架Helidon Níma：使用虚拟线程实现高性能</a>\"</p>",
    "publish_time": "2022-09-26 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "不要指望下一个像GPT这样的大型语言模型会民主化",
    "url": "https://www.infoq.cn/article/UrFKiffb44jcffwP5FbH",
    "summary": "<p></p><blockquote>5月初，Meta公司发布了Open Pretrained Transformer（OPT-175B），这是一个可以执行各种任务的大型语言模型（LLM）。在过去几年中，大型语言模型已经成为人工智能研究最热门的领域之一。</blockquote><p></p><p></p><p>本文最初发布于TeckTalks。</p><p></p><p><a href=\"https://www.infoq.cn/article/4iDmM3PgXmvELorwQ8M3\">OPT-175B</a>\"是由OpenAI的<a href=\"https://www.infoq.cn/article/Z46h9vTIL3dBx5Z3hAjO\">GPT-3</a>\"引发的LLM军备竞赛的最新参与者。GPT-3是一种具有1750亿个参数的深度神经网络。GPT-3表明，LLM可以在没有任何额外训练以及只学习几个样本（零样本或小样本学习）的情况下完成许多任务。微软后来将GPT-3集成到了它的几个产品中，不仅展示了LLM在科学研究上的前景，也展示了其在商业应用上的前景。</p><p></p><p>让OPT-175B与众不同的是Meta对“开放性”的承诺，正如模型的名字所暗示的那样。Meta已经向公众提供了这个模型（以及一些注意事项），它还公布了大量关于训练和开发过程的细节。在Meta AI博客上发表的一篇文章中，该公司将OPT-175B的发布描述为“大规模语言模型的民主化访问”。</p><p></p><p>Meta朝着透明的方向发展值得称赞。然而，大型语言模型的竞争已经达到了无法再民主化的地步。</p><p></p><h2>关于该大型语言模型的几个细节</h2><p></p><p></p><p>Meta发布的OPT-175B有一些关键特性，包括预训练的模型以及训练和使用LLM所需的代码。对于没有计算资源用于训练模型的组织，预训练模型特别有用（训练神经网络比运行它们消耗的资源更多）。它有助于减少训练大型神经网络所需的计算资源所造成的巨大碳排放量。</p><p></p><p>与<a href=\"https://www.infoq.cn/article/w1lxxO4qtaVPxZUdqzwi\">GPT-3</a>\"一样，OPT也有不同的大小，参数从1.25亿到1750亿不等（参数越多模型学习能力越强）。在撰写本文时，OPT-30B以下的所有模型都已提供下载。拥有全部1750亿个参数的模型将仅提供给被选中的研究人员和机构（他们需要填写一张申请表）。</p><p></p><p>根据Meta AI博客，“为了保持完整性和防止滥用，我们将在非商业许可下发布我们的模型，专注于研究用例。该模型将授权给学术研究人员，与政府、民间团体和学术机构有关的组织，以及世界各地的行业研究实验室。”</p><p></p><p>除了模型，Meta还发布了一份完整的日志，提供了关于该大型语言模型开发和训练过程的详细的技术时间线。通常，发表的论文只包含最终模型的信息。Meta表示，该日志提供了一些有价值的信息，包括“用于训练OPT-175B的计算资源的数量，以及当底层基础设施或训练过程本身因为规模太大而变得不稳定时所需的人力开销。”</p><p></p><h2>与GPT-3比较</h2><p></p><p></p><p>Meta公司在其博文中指出，大型语言模型大多是通过“付费API”访问的，对LLM的限制性访问“限制了研究人员了解这些大型语言模型如何工作以及为何有效的能力，妨碍了他们提高模型鲁棒性以及缓解偏见和数据中毒等已知的问题”。</p><p></p><p>这对于OpenAI（以及微软的独家GPT-3许可）无疑是一记重击，后者将GPT-3作为黑盒API服务发布，而不是将其模型权重和源代码公开。OpenAI没有公开GPT-3的原因之一是控制有害应用程序的滥用和开发。</p><p></p><p>Meta相信，把模型提供给更广泛的受众，他们将可以更好地研究和预防它们可能造成的任何伤害。</p><p></p><p>Meta是这样描述这项工作的：“我们希望，OPT-175B将为大型语言模型创建前沿带来更多的声音，帮助社区共同设计负责任的发布策略，并为该领域大型语言模型的开发增加前所未有的透明度和开放性。”</p><p></p><h2>大型语言模型的成本</h2><p></p><p></p><p>然而，值得注意的是，“透明和开放”并不等同于“民主化大型语言模型”。训练、配置和运行大型语言模型的成本仍然很高，而且未来可能还会增长。</p><p></p><p>根据Meta的博文，模型的研究人员已经大幅降低了训练大型语言模型的成本。该公司表示，这个模型的碳排放量已减少到GPT-3的七分之一。据我之前采访过的专家估计，GPT-3的训练成本高达2760万美元。</p><p></p><p>这意味着，OPT-175B的训练成本仍将高达数百万美元。幸运的是，预训练的模型可以避免模型训练过程，并且Meta表示，他们将提供“只使用16块NVIDIA V100 GPU”就可以完成整个模型训练和部署的代码库。这相当于一台英伟达（Nvidia）DGX-2，成本约为40万美元。对于资金紧张的研究实验室或个体研究人员来说，这不是一个小数目。(根据一篇提供了更多OPT-175B细节的论文，Meta使用992块A100 80GB GPU训练了自己的模型，这款GPU明显比V100快。)</p><p></p><p>Meta AI的日志进一步证实，训练大型语言模型是一项非常复杂的任务。OPT-175B的时间线上到处都是服务器崩溃、硬件故障和其他需要高级技术人员才能解决的并发症。研究人员还不得不多次重启训练过程，调整超参数，修改损失函数。所有这些都会产生小型实验室无法承担的额外费用。</p><p></p><h2>大型语言模型的未来</h2><p></p><p></p><p>语言模型如OPT和GPT都是基于转换器架构的。转换器的关键特性之一是它们能够大规模地并行处理海量时序数据（如文本）。</p><p></p><p>近年来，研究人员已经证明，增加转换器模型的层数和参数，可以提高它们在语言任务上的性能。一些研究人员认为，达到更高的智能水平只是一个规模问题。因此，像Meta AI、DeepMind（由Alphabet拥有）和OpenAI（由微软支持）这样现金充足的研究实验室正在朝着创建越来越大的神经网络前进。</p><p></p><p></p><blockquote>某人的观点文章。我的看法是：现在都是规模问题了！游戏结束了！现在只要让这些模型更大、更安全、计算效率更高、采样更快、记忆更智能、模式更多样、数据更有创新性，无论在线还是离线......1/N <a href=\"https://t.co/UJxSLZGc71?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjM4MTUxNzMsImZpbGVHVUlEIjoiR1E5NVBaWE55VEVjbGlYdSIsImlhdCI6MTY2MzgxNDg3MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTY1MTE5Nn0.Nskx-jA73oq2h4Mm1yBBSTg5klykj1PGjBvHJVLq0Hc\">https://t.co/UJxSLZGc71</a>\"—— Nando de Freitas（@NandoDF）</blockquote><p></p><p></p><p>去年，微软和英伟达创建了一个有5300亿个参数的语言模型，名为Megatron-Turing （MT-NLG）。上个月，谷歌推出了路径语言模型（PaLM）。这是一个有5400亿个参数的LLM。有传言称，OpenAI将在未来几个月发布GPT-4。</p><p></p><p>然而，神经网络越大需要的财政和技术资源也越多。虽然更大的语言模型会带来新的东西（和新的问题），但不可避免地，它们将把权力集中在少数富有的公司手中，使得较小的研究实验室和独立的研究人员更难研究大型语言模型了。</p><p></p><p>在商业方面，大型科技公司将拥有更大的优势。运行大型语言模型是非常昂贵和具有挑战性的。像谷歌和微软这样的公司有特殊的服务器和处理器，他们能够大规模运行这些模型并从中获利。对于比较小的公司来说，运行自己的LLM（如GPT-3）版本开销太大了。正如大多数企业使用云托管服务，而不是构建自己的服务器和数据中心一样，随着大型语言模型变得越来越流行，像GPT-3 API这样的开箱即用系统将越来越有吸引力。</p><p></p><p>这反过来又会使人工智能进一步集中在大型科技公司的手中。越来越多的人工智能研究实验室将不得不与大型科技公司建立合作伙伴关系，以获得资助。而这将使大型科技公司有更多的权力来决定人工智能研究的未来方向（这可能会与他们的经济利益相一致）。这可能要以那些短期内无法产生投资回报的研究领域为代价。</p><p></p><p>最后，当我们庆祝Meta为LLM带来透明度的时候，请不要忘记，大型语言模型本质上就是不民主的，而是有利于推广它们的公司。</p><p></p><p>英文原文：<a href=\"https://bdtechtalks.com/2022/05/16/opt-175b-large-language-models?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjM4MTUxNzMsImZpbGVHVUlEIjoiR1E5NVBaWE55VEVjbGlYdSIsImlhdCI6MTY2MzgxNDg3MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNTY1MTE5Nn0.Nskx-jA73oq2h4Mm1yBBSTg5klykj1PGjBvHJVLq0Hc\">Can large language models be democratized?</a>\"</p>",
    "publish_time": "2022-09-26 08:20:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于微服务和低代码，网易数帆推出软件生产力新模型",
    "url": "https://www.infoq.cn/article/fqygd7M5v6MlKUx3acIw",
    "summary": "<p>9月23日，2022网易数字+大会在杭州召开。针对数字化转型下的企业软件研发需求，网易数帆在会上发布了轻舟云原生、轻舟低代码两大产品线的一系列升级，并基于此推出了以资产为中心的软件生产力模型和技术方案。网易数帆云原生及低代码产品线总经理陈谔表示，该方案旨在通过微服务和低代码在帮助企业沉淀内部标准化的原子服务与数据，形成可组装的业务资产，并采用组装的开发方式快速进行创新业务生产和交付，云原生底座提供服务运行保障。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89c8db0560c83f694813f2dc5f23091a.png\" /></p><p></p><p>网易数帆云原生及低代码产品线总经理陈谔</p><p></p><h2>云原生升级：探索共享复用，促进业务创新</h2><p></p><p>陈谔首先介绍了网易数帆云原生产品线在行业的落地进展以及产品升级方向。在过去一年，网易数帆轻舟云原生平台支撑了金融行业众多核心业务场景，包括从小型机向分布式架构迁移、国产信创适配、个贷业务交易安全、生态缴费业务高可用保障等。在此过程中，网易数帆云原生平台的着眼点，也从帮助企业应用进行微服务化改造，转向提供稳定性治理和高可用性保障，并开启了共享复用的探索。</p><p></p><p>以中间件稳定性治理为例，网易数帆全新推出了中间稳定性管控产品，提供巡检和辅助定位能力，基于网易内部中间件稳定性治理实践，不仅将超过300条运维经验交付给用户，还通过“知识引擎”能力，帮助业务建立稳定性改进循环。</p><p></p><p>网易数帆认为，复杂系统的稳定性应当不断主动改进，而这个改进思路就是“发现问题-&gt;分析整改-&gt;将沉淀经验加入检查避免同类问题-&gt;发现新问题”这样不断进行的“稳定性改进循环”。知识引擎可以根据企业情况不断进行经验沉淀和规则迭代的平台，而巡检系统相当于稳定性检查执行工具，从而辅助用户建立这样的“稳定性改进循环”。</p><p></p><p>在高可用层面，网易数帆在支撑业务“两地三中心”部署架构解决方案的基础上，通过轻舟微服务提供资源感知、区域路由、增强多中心应用监控等产品化能力，通过轻舟中间件提供数据复制、集群联邦等产品化能力，并在此基础上结合API网关流量调度、多活管控服务等能力，新推出异地多活解决方案，帮助客户业务进行多活改造。</p><p></p><h2>低代码演进：构建超1000复杂应用，开发效率提升100%</h2><p></p><p></p><p>对于轻舟低代码平台，网易数帆希望它能成为企业信息化建设的通用平台工具，从而使企业不会迷失在形形色色的工具之中。基于此，网易数帆轻舟低代码平台的演进主要聚焦复杂应用开发能力、开发效率和易用性等三个方面。</p><p></p><p>陈谔介绍，轻舟低代码平台目前能够构建超1000个业务逻辑函数的复杂应用而用户体验不妥协，并实现100%的开发效率提升，同时成本降低超60%。从普及程度来说，过去一年，轻舟低代码的开发者已在全国13省市开发了200多个企业级应用。</p><p></p><p>在这背后，网易数帆为轻舟低代码加入了多项有特色的新特性，包括支持源码导出、客制化、多人协作等。轻舟低代码独门的NASL，支持将应用乃至前后端编译成通用编程语言，这意味着低代码应用可脱离平台独立部署，无缝衔接企业软件生产运维体系，从而解决了企业低代码应用开发面临的网络隔离、安全性要求严格、代码合规等挑战。</p><p></p><p>客制化能力不仅支持低代码组件、逻辑、API协议通过传统语言进行扩展开发，还可以将企业原有SDK复用到低代码应用中。这对于企业定制自有组件、沉淀具有行业特性的IT资产而言非常实用。在陈谔的现场演示中，轻舟低代码对客户需求的还原能力趋近100%。</p><p></p><p>多人协作则是当前企业通过任务分解的方式应对高复杂度业务开发的有效途径，轻舟低代码带来的研发效能提升，也从这一特性中受益匪浅。</p><p></p><h2>软件研发新模式：资产即生产力</h2><p></p><p></p><p>为帮助企业加速业务创新，利用数字化软件系统快速应对市场变化、提质增效，网易数帆提出了一个新的软件生产力模型，这一模型与Gartner此前提出的可组装业务能力（PBC）及经典的DDD（领域驱动设计）理论一脉相承。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/53ca8ecee50020131bf38a1dd88b247d.png\" /></p><p>网易数帆软件生产力模型</p><p></p><p>组装式架构能够让业务更快速在生产环境运行起来，避免重复建设，提高资产复用比，并且生产方式更标准化，变更更安全可靠。凭借在云原生和低代码领域的深厚积累，网易数帆打造了以资产为中心的软件生产力解决方案，让这一方法论得以在企业落地。</p><p></p><p>陈谔介绍，这一方案帮助企业基于内部服务沉淀标准化的原子服务与数据（如API、服务、流程、数据等），可以通过统一集成平台的集成和编排等能力，将其合成可组装的业务能力（PBC），也可以通过低代码平台形成可复用的低代码资产。这些资产将在软件资产中心统一管理和运营（包括资产认证、安全合规验证、资产入驻、访问控制和运营统计等），专业开发人员和低代码开发者都基于可复用的资产，快速组装和交付数字化业务。同时，强大的云原生底座为业务运行保驾护航，提供架构治理、研发流程管理、服务治理和运维保障等能力支撑。</p><p></p><p>陈谔介绍了一个DevOps资产包的实践案例。在该案例中，网易数帆基于DevOps基础服务沉淀DevOps资产包，包括112个UI组件、12个集成页面、169个API和128个页面模板，采用组装式开发思想，借助轻舟低代码平台为企业快速组装了一个客制化的DevOps平台。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/77521af8d441a69b376b8920c2d62537.png\" /></p><p>DevOps资产包实践案例</p><p></p><p>从中可见，在软件生产力模型下，IT资产的丰富度意味着业务创新的速度，而行业资产的沉淀，必将使得整个行业具备更为成熟的数字化水平。</p><p></p><p>陈谔表示，未来，网易数帆在云原生、低代码产品持续演进的同时，也将不断完善这一方法论和工具平台，让数字化业务的构建不再受限于专业开发资源，让行业创新不再有技术门槛。</p>",
    "publish_time": "2022-09-26 10:06:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据中台不是“无中生有”，而是让企业更快去做眼前的事",
    "url": "https://www.infoq.cn/article/SEJ62iIqiEpPfW0pRm7f",
    "summary": "<p>在商业市场竞争加剧以及国家政策扶持的大背景下，如何进行数字化转型成为了企业所关注的重点。</p><p></p><p>数据作为企业数字化转型的关键要素，同时也是赋能业务以及管理的重要生产资料，究竟企业应该如何打通数据壁垒，构建出数据采集、治理、分析和使用的完整闭环？如何深度挖掘数据的价值，提高企业运营效率，使得企业可以灵活应对迅速变化的市场环境？为了解答这一系列问题，有一个概念不得不提，那就是：<a href=\"https://xie.infoq.cn/article/69e8879ff199fff7fb0ae2c3c\">数据中台</a>\"。</p><p></p><p>2015年，阿里为了提高业务效率、支持产品创新、减少成本投入，开创了“大中台，小前台”的组织机制和业务模式，提出中台战略。后来的几年，数据中台的概念一路走红，逐渐深入互联网公司甚至是传统行业。</p><p></p><p>然而，几乎是在一夜之间，数据中台的“彩色泡泡”被戳破——一方面，提出这一理念的阿里被指忙着“<a href=\"https://www.infoq.cn/article/HwD2iMWxZ3MW7HloOHiD\">拆中台</a>\"”；另一方面，许多企业在大张旗鼓地建设完数据中台之后，发现其带来的回馈和价值并不明显。业界对数据中台的争议声接连不断。</p><p></p><p>但即便如此，仍然有企业把数据中台始终作为数据能力建设的关键部分。比如，近几年来，中国<a href=\"https://www.infoq.cn/article/TvHZGbm4EToxCb98PO0r\">光大银行</a>\"（以下简称“光大银行”）一直在加速数据平台的中台化、敏态化发展，通过建设数据中台沉淀数据资产价值，为“用数据驱动业务”夯实基础。</p><p></p><p>而关于数据中台的争议点，作为光大银行数据体系建设的核心参与者，光大银行资深数据架构师<a href=\"https://time.geekbang.org/column/intro/100057401?tab=catalog\">王磊</a>\"在日前接受InfoQ采访时表示，其中既有概念不明确的原因，也有企业对数据中台期待过高的原因。用他的话说——数据中台就像是“巨人的肩膀”，但它不是让企业去做一些不能做的事，而是能够更快地去做现在能做的事。</p><p></p><p>换句话说，企业建设数据中台，归根结底还是要回归业务本身，从自身情况出发。而不是无中生有，让企业去做与业务发展方向不匹配的事情。</p><p></p><h1>数据中台的核心价值是实现数据复用</h1><p></p><p></p><p>那么，具体来说，在企业数字化转型的过程中，数据中台的定位究竟是什么？我们或许可以从<a href=\"https://www.infoq.cn/article/zz3JtiokOUIrJXHl4wLE\">银行数据体系</a>\"的建设历程中来找到答案。</p><p></p><p>银行数据体系的建设，可以追溯到初步信息化的时期，银行一方面将业务从线下转到线上，形成协同效应；另一方面将省级IT力量逐渐向总行迁移，实现集中电子化。</p><p></p><p>“在早期信息化阶段，手工业务逐渐被信息系统取代，但系统仍然在各分行单独部署，而后随着技术的进步，各家银行纷纷将地方的IT力量逐渐集中到总行，原有的分行开发系统不断消失，这是银行系统的大集中时期，主要为了解决银行电子化的工作需求。”王磊在谈及银行不同时期的发展特点时说道。</p><p></p><p>王磊表示，在这个阶段，银行对单机处理能力的依赖程度还较高，大多数银行面对日益增长的数据量，还是会优先选择更换硬件设备来提高系统负载量。</p><p></p><p>传统银行与客户接触都是通过柜台，从资产层面来说，它的设备容量比较容易评估。而当用户和银行的接触由传统的线下物理网点变为互联网平台时，对银行系统的冲击也随之而来。</p><p></p><p>“互联网电商的运营模式，制造出来像双十一这样的消费高峰，最终会体现在银行的交易层面。这时，怎样解决弹性资源的分配？怎样才能抵御系统的高并发？大多数银行会把总系统按照类别进行拆分，如用户中心、支付中心、产品中心等。换句话说，银行会把相对聚焦的业务拆分成更小颗粒度的单元，来保证系统整体的性能。”王磊说道。</p><p></p><p>如果说互联网时代是以人为核心，企业聚焦的是链接，为人提供平衡的供需关系，那么，随之而来的数字经济时代，则是以价值为核心，企业追求的是共同创造利润，从业务模式出发，利用海量数据，由彼及己，为自身赢取增长。</p><p></p><p>因此，在数字经济的时代背景下，王磊认为数字化转型的重点，不纯粹发生在IT<a href=\"https://archsummit.infoq.cn/202212/beijing?utm_source=infoq&amp;utm_medium=conference\">技术架构</a>\"层面，而在于企业的业务发展。</p><p></p><p>他以银行互联网贷款的场景为例。传统方式下，用户向银行贷款递交申请主要是通过线下柜台，审核贷款申请可能需要几天甚至十几天；如今这种模式发生了明显的变化，银行后台通过算法模型以及引入各种辅助决策工具，让用户在线上递交的申请审核，变成一个自动化的过程，不少银行小额贷款可以做到实时反馈。</p><p></p><p>王磊表示，在前两个阶段，银行数据体系建设还主要是“烟囱式”，强调聚焦于某一条线、某一个业务板块去集成数据能力，而在不同条线之间会存在“<a href=\"https://www.infoq.cn/article/29uvshLBl6ZIeuxTdNOx\">部门墙</a>\"”，系统层面也会存在互不相通的障碍。这些阻碍导致各系统之间的数据无法打通，可能存在数据不一致、难调用等一系列的问题，很难满足前台业务的高效甚至是实时响应的需求。</p><p></p><p>“而数据中台的主要作用就是解决系统横向打通的问题，在系统层面最大程度实现数据复用。”王磊指出。也就是说，数据中台拥有可以为不同形式的业务，提供通用数据服务的能力。</p><p></p><h1>数据交付更加强调实时性</h1><p></p><p></p><p>但是，由于业界对数据中台的概念一直没有统一的共识，有的公司会把原有数据类的能力和技术统一打包，就叫做数据中台——其中的核心内容仍然是过去的元数据管理、数仓、数据湖，前台的BI系统、报表文件、多维分析，以及大数据等技术。</p><p></p><p>“这类所谓的数据中台，没有带来新的东西。一个新的名词出现时，它如果没有区别于之前已经存在的概念，就只是‘新瓶装旧酒’，并没有什么实质性的创新。”王磊强调。</p><p></p><p>在他看来，要想正确理解数据中台，需要一种务实的态度。从银行角度来讲，数据中台一定是要从前台业务价值出发，提供一些数据拆解的能力。比如，要灵活支持前台，中台就必须与前台同频率变化，实时满足业务场景的需求。</p><p></p><p>但是，由于前台覆盖的业务面非常广泛，要想保持同频，对数据中台的交付能力又会提出挑战。</p><p></p><p>过去，数据分析主要服务于特定的场景。比如，银行会依托业务模型，把交易系统产生的数据，通过一种批量处理的方式，整理成企业所关注的各种指标，然后工作人员会根据这些指标来制定管理者所关注的数据，最终以报表资料的形式，辅助不同层级的管理者进行决策。</p><p></p><p>“如今，在银行数字化阶段，数据的使用方式已经发生了实质性的变化，不是单纯只从管理者做决策这一个场景出发，而是融入在一线业务的每一个场景。比如，前面说到的银行发放网络贷款的场景中，怎么去识别客户提交的数据？最终银行是否应该发放贷款？具体应该发放多少额度？再比如，在银行的营销场景下，如何能够与客户接触，为客户提供他们更感兴趣的产品？为了做出决策，数据的支持发生在每时每刻，并且离不开场景。”王磊说道。</p><p></p><p>具体从时间维度来看，传统数据处理的时效性是“T+1”天，也就是说，今天输出的数据报表，实际上是通过昨天的数据进行批量处理而得到的。但现在，数据使用、交付不单纯体现在产出数据报表的时间节点上，而是只要有正常的业务开展，就需要有数据赋能，会反应在任何一个时间点上。“比如，当客户在手机APP端提交了一个操作请求，银行后台需要及时反馈这个请求，所以，数据的交付必须更加强调实时性。”王磊解释道。</p><p></p><p>从系统架构层面上来看，当数据交付方式变化，数据中台服务能力的建设模式也会发生变化。过去，银行会先从数据库中读取批量数据，然后经过计算处理，最后以报表文件方式输出数据，这时数据能力的复用跟数据批量加工的形式之间，有着很强的耦合关系。</p><p></p><p>“但为了满足时效性，如今数据交付方式正在向服务化的方向发展，整个数据中台服务能力的架构方式，也由传统烟囱式架构向着<a href=\"https://xie.infoq.cn/article/59f058d2221ed257c938f53d5\">微服务化</a>\"架构转型。”王磊说道。</p><p></p><p>具体来说，微服务架构运用的是围绕业务需求的轻量级架构，所以具有诸多好处。比如，因为服务的设计是围绕特定业务开展的，例如商品服务只管理商品、客户服务只管理客户等，这意味着开发人员足够专注，可以大大提高开发效率；再比如，微服务架构中的每一个服务都是一个独立应用，可以访问自己的数据库，通过提供公共API，服务之间还可以相互调用，如此一来就可以满足不同业务的快速开展和交互。</p><p></p><p>“所以，在我们的数据交付和应用交付过程中，服务化成为非常重要的承接方式，而微服务化架构也发挥着重要的作用。”王磊强调。</p><p></p><h1>数据中台可以这样考虑投入产出比</h1><p></p><p></p><p>但话说回来，建设数据中台的确是一件高投入且长期的工作，短期内很难看到直接效益。对于企业而言，不得不考虑到一件事就是“投入产出比”。</p><p></p><p>“如果数据中台非常划算，那么企业很早就会建设了，也不会等到现在。企业之所以都做烟囱式的系统，就是因为烟囱式系统的短期效益更好，做了一个系统、加了一个功能，立马可以用上。”王磊解释道，因此，短期来说，未必所有的企业都需要建设数据中台。不同的企业处于不同的发展阶段，拥有不同的盈利模式，自然对数据的需求也就千企千面。</p><p></p><p>但是，就<a href=\"https://www.infoq.cn/theme/149\">金融</a>\"机构而言，长期来看，当面临业务日新月异的变化，传统的IT系统建设模式，大概率比数据中台的建设成本更高。</p><p></p><p>王磊表示：“拿银行业来举例，为了适应客户需求的变化，银行在组织架构方面的调整也越来越频繁。很多时候会遇到今年系统要合并，明年再拆分的情况，如果按照原本的投入方式，反而会让IT系统成本越来越高。”</p><p></p><p>而如前文所说，数据中台的核心是企业级的能力复用，这种能力虽然不能快速建立起来，但是通过项目经验稳定沉淀，长此以往，可以大大降低企业的应用开发风险和成本。</p><p></p><p>那么，从未来中长期来看，企业应该如何权衡数据中台的投入产出比呢？</p><p></p><p>王磊认为，数据中台是不是能够带来效益，可以在构建新场景的过程中计算。比如，在固定场景中，数据中台投入了多大的成本，如果是按照传统烟囱式系统进行建设的话，其投入成本又有多少，二者进行对比分析，可以直接量化得出结果。未来，也可以采用同样的量化方式，复盘数据能力的复用可以为企业带来多少的增长价值。</p><p></p><p>“对于任何投入，企业都会理性地关注它的价值点。当我们去看数据中台时，最开始一定是看数据中台在哪些范围、哪些场景能发挥作用。企业一定也希望自己能够尽早的触达这些场景。如果数据中台在某个能力点上的复用率很高，应用场景自然也很广泛，那么，这一点的投入在未来带来收益的可能性就更很大。如果是复用率比较低的节点，企业投入就可以少一些。”王磊进一步说道。</p><p></p><p>他举了个例子，“比如，大多数银行都会对客户资产进行一个评估，这个指标会在不同条件、不同场景下被重复使用。因此，企业可以将这种关键性的指标放在数据中台的建设过程中，从而获得更大的收益。”</p><p></p><p>总的来说，企业应该通过自身的实践出发，找到具有潜力的业务场景，有针对性地投入资源、费用、人力等成本，才有更大可能从中挖掘价值、创造收益。</p><p></p><h1>让业务人员参与其中</h1><p></p><p></p><p>那么，建设完数据中台之后，如何确保它能够发挥最大的价值？这是大部分企业的另一个疑问。</p><p></p><p>王磊指出：“由于数据和业务之间具有关联性，数据的实际使用是以业务上的定义为前提的，换句话说，数据中台发挥价值的一个重要前提便是——要在业务语义上进行标准化、成体系的管理。”</p><p></p><p>他拿“客户”一词进行举例，对于不同分行、不同业务线、不同部门而言，这个词的定义、界限、分类可能完全不同。“但如果我们对客户都没有一个明确定义的话，那么客户的价值、客户的资产就不知道应该采用什么样的规则去计算，更不用提让数据能够复用了。这归根到底还是业务口径不一致带来的问题。”</p><p></p><p>由此可见，数据中台所形成的技术能力实际上是业务能力的体现，业务人员需要参与到整个数据中台的建设过程中，对数据有更好的管控，才能让数据中台更好地赋能于业务场景。</p><p></p><p>然而，业务与技术的鸿沟是天然存在的，这是企业面临的普遍问题，也是决定了数据中台价值能不能有效发挥的关键要素。 “从这两年银行业的实践可以看到，很多银行都在成立独立的数据部门，去负责数据能力建设或者数据中台建设的工作，目的就是拉通业务和技术，通过一种通力协作的方式去重新构建企业能力。具体来说，这个数据部门通常既有传统的IT技术能力，又会融合企业的业务能力。”王磊说道。</p><p></p><p>同样，我们也看到光大银行在数月前的公告中对外公布，银行的组织架构将有所调整，数据资产管理部也会随之建立起来。对此，王磊表示，“我觉得，未来数据资产管理部会在很大程度上推进我们共同去完善数据中台的建设。”</p><p></p><p>但是，另一个问题是，新技术的应用一定有一个周期。比如，目前还有很多银行的一线业务人员由于缺少对新技术工具的驾驭能力，所以仍旧会使用Excel这种非常传统的数据分析工具，进行计算、分析。</p><p></p><p>这其中既有业务人员思维意识的原因，也有技术本身的原因。“短时间来看，用Excel报表确实可以更灵活的完成一些工作。但是毕竟这只是一种轻量级的数据统计分析工具，随着银行数据量级的增长、<a href=\"https://www.infoq.cn/news/AkzC5erKpwNFTELuPAGQ\">业务场景</a>\"的复杂化，像Excel这种传统的数据统计分析工具会无法承载。另一方面，由于工具级别的不同，它们处理问题的逻辑也是不一样的，因此，系统对现在的业务支持可能也会存在一些盲点，这会导致一线业务人员不得不用报表去分析业务情况。”王磊表示。</p><p></p><p>所以，在他看来，业务人员使用数据的能力和技术人员构建的数据平台之间需要一个桥梁，在这个桥梁之上，还需要业务人员和技术人员的共同学习、互相奔赴，努力扩大“共通的意义空间”。只有这样，才能够真正发挥数据中台的价值，才能让<a href=\"https://www.infoq.cn/article/YcvRvZaOoez9sKJ92jHR\">数据</a>\"流向业务、驱动业务、赋能业务。</p>",
    "publish_time": "2022-09-26 11:44:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "聊聊可观测在企业的实践应用与实际价值｜InfoQ大会早班车第23期",
    "url": "https://www.infoq.cn/article/hTRCXPdd3Cphkdpf0bMi",
    "summary": "<p>可观测性如何服务好上层业务？自建还是引入？可观测运维对开发的价值有哪些？本次大会早班车聚焦可观测问题，邀请到阿里云资深技术专家承嗣、字节跳动可观测性平台负责人孔罗星进行分享，不妨来听一听他们的观点与分析。</p>",
    "publish_time": "2022-09-26 12:03:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "旧工厂如何用AI玩出新花样？",
    "url": "https://www.infoq.cn/article/ZGkKXpk51tTmHwOYCVKx",
    "summary": "<p>不少人认为，所谓的<a href=\"https://www.infoq.cn/article/8EjU56MwtgBcwB8ML8AT\">智能工厂</a>\"一定要有最完善的智能化产线，以及机械操作臂、AGV搬运车、码垛机器人等最先进的设备。但是，很多企业在智能化转型的初期，往往无法一次性到位，对旧工厂、旧设备的改造，几乎无法避免。</p><p></p><p><a href=\"https://www.auo.com/en-global\">友达</a>\"是全球头部的显示面板设计、研发、制造和销售公司，最早的工厂筹建于20多年前，所以很多设备在早期也不具备良好的数据采集和联网基础。但是，这些令很多传统制造企业在数字化和智能化转型过程中非常头大的旧设备和旧产线，似乎没有变成友达的“拖油瓶”，反而，友达利用<a href=\"https://www.infoq.cn/article/7vTfCLYoKzuIZTjdLGk0\">AIoT</a>\"技术为它们焕发出了新的生命力。</p><p></p><p>几年前，笔者曾经在参观友达苏州工厂时看到这样一幕：很多老旧设备的监测表头前都加装了一个摄像头，通过对表头数据（包括温度、压力等）进行图像识别和读取，把无法进行数据采集的关键设备信息也上传到了后台，供后续数据分析使用。这就解决了既要保护原始投资，又想实现数据获取、传输和应用的问题。</p><p></p><p>而这其实不过是友达借助AI、IoT、大数据等数字技术实现智能化升级的一个缩影。从2015年开始布局智能制造以来，除了对旧设备的改造，友达还布局了大量智能化设备和软硬件基础设施，在智能化的技术、场景、人才等方面积累了大量经验，这些经验先是在公司内部得到了广泛且深入的复制落地，随后又转化为技术产品向外部进行智慧工业服务输出，成为友达的另一增长曲线。并且，就在2021年，友达还凭此转型成效获评了世界经济论坛的“全球灯塔工厂”。</p><p></p><h1>是什么让友达“积极转型”</h1><p></p><p></p><p>故事要从2015年讲起。那一年，友达开始在全集团范围内投入大量资源进行数字化建设，其中包括数据整合与数据分析技术提升等。2016年，阿尔法狗大战世界围棋冠军李世石，并以4:1的总分获胜，AI技术迎来高光时刻；一年之后，友达很快启动了全面智能化战略，开始在内部通过AI等技术与先进制造融合，构建自感知、自决策、自执行的智能系统。</p><p></p><p>“所谓的自感知就是数据能被采集侦测，比如设备的状况能通过IoT技术做到实时监测；自决策就是数据采集后可以借助AI技术进行数据分析建模，从而实现自主决策，例如是否要调整参数或者停机等等，并且不管数据是在边缘、云端还是本地；自执行就是在决策完成后，可以把决策动作反馈到后台，让智能系统做自执行、自调整，形成闭环。”艾聚达信息技术（苏州）有限公司（以下简称“艾聚达”）总经理赖骏凯在接受InfoQ采访时解释，从技术角度而言，这样的智能系统正是AI与IoT技术融合的表现形式，是<a href=\"https://www.infoq.cn/article/a9ChVY8DGJRbXqTHqjah\">AIoT落地</a>\"的成果。</p><p></p><p><a href=\"https://ati.auodigitech.com/\">艾聚达</a>\"是友达在2018年成立的工业服务全资子公司之一，主要提供 AI 赋能的 HaaS 平台解决方案，透过 AI 技术赋能边缘硬件产生数据价值，助力企业智慧化升级。具体来说，艾聚达基于AI技术，可以帮助企业提取数据价值并将算法模型赋予终端，让边缘硬件产生智慧决策能力。凭借在制造行业多年积累的丰富场景应用技术与算法模型，能够为各企业提供工业自动化、边缘智能化、工业大脑、工业人工智能平台四大解决方案和服务。</p><p></p><p>和艾聚达同年成立的，还有另一家工业服务子公司友达智汇，主要提供规划咨询服务解决方案、数字化工厂解决方案、智能工厂解决方案和绿色智慧园区解决方案。2021年，友达在全球的智慧工业服务全新事业：友达数位成立，艾聚达和友达智汇都划归为旗下子公司，共同为制造业转型升级贡献力量。</p><p></p><p>事实上，变革是一件需要强驱动力的事情，和大多数企业一样，友达最初做数字化和智能化，也有很多外部因素的推动。赖骏凯告诉InfoQ，光电是一个竞争非常激烈的行业，尤其是在过去十年间，国内面板市场出现了大规模的扩产，对于友达这样的光电制造领导企业来说，必须提前布局思考如何从规模竞争转向价值竞争。</p><p></p><p>所谓价值转型主要表现在两方面：首先，是产品形态的转变，专注于高附加值的产品；其次，是订单形态的变化，从规模化生产转变为少量多样的订单生产。举例来说，“现在大家打游戏对画面的要求越来越高，我们就必须做超高刷新率的产品，以前这种产品的标准是90赫兹，也就是一秒刷新90次，但现在在电竞领域，我们已经可以做到领先业界的500赫兹。”赖骏凯表示，“外部环境的这些变化，要求我们必须做高度定制化，必须少量多样地去生产，但是这种生产方式会带来一个很大的挑战，比如产能会有所损失、成本会增加、良率会变差，这就是我们转型过程中的‘痛’。”</p><p></p><p>除此之外，还有一个关键的矛盾点——市场需求变快、产品周期变短，但面板产品的制造周期却相对长。用赖骏凯的话说，产品优化和生产的速度都不一定能跟上产品需求的变化速度。“所以，正是这些源源不断的问题促使我们很早就开始思考，如何通过新的技术提升内部的竞争力，如何用新方法解决旧问题。”</p><p></p><h1>AI解决了哪些问题</h1><p></p><p></p><p>2017年开始，AI成了友达应对这些问题的那个“新的技术”和“新的方法”。具体来说，赖骏凯认为AI在制造行业主要有三大应用场景——数据科学、<a href=\"https://www.infoq.cn/article/MDuweEQy7Qr1FNHFXzfN\">工业质检</a>\"和智能监控。</p><p></p><p>以生产排程计划为例，赖骏凯表示，在面板制造的过程中有一个关键制程，每个产线配备的设备比较有限，所以作为一个稀缺资源，这类设备的排程计划会直接影响生产效率。</p><p></p><p>“比如，这个设备一次要生产多少片面板、一天要产出多少量，如果产品在A设备上良率比较低，这时候就必须在B设备上生产，这些都是限制式。而这些限制式一旦达到一定数量，比如十几种甚至几十种，用人工去计算的工作量是大且繁复的。”赖骏凯表示，过去这个工作需要由最有经验的工程师每天花时间用Excel表格手工完成。</p><p></p><p>但这种“传统”的模式不只效率低、调整次数有限，而且很难与MES（生产执行系统）联动生产。“而利用AI模型，我们只要把限制式输入进去，它就能直接输出一个最佳排程结果，告诉我们今天最多可以生产多少产品，甚至什么时间点要产出这些产品。这么做的好处不只是效率变高了，也使得该关键设备的产能得以提升。</p><p></p><p>同样的道理，面向员工的排班工作也可以由AI完成。在友达工厂，基于由AI自动化进行的排岗、调休和操作规范知识库实时支持，员工每天上岗后就可以非常清楚自己当天的主要工作、具体的生产任务、操作注意事项等等。</p><p></p><p>另外，其中还有一个令人拍案的巧思之处在于，友达在每个关键岗位也加设了摄像头，结合AI技术可以分析员工的作业节拍，判断动作是否标准等。这样一来，传统工位就摇身一变变成了智能工位，过去可控性比较低的人工作业也实现了相对的标准化，在生产环节就提高了对产品质量的把控。</p><p></p><p>而为了提高产线的可控性，除了对人的操作做追踪，还要对设备的运行状态做监测。比如我们在开篇中提到的那一幕，通过摄像头读取表头数据的目的，事实上就是为了对设备做监控，一旦设备出现故障，可能直接表现为温度、压力等数据异常，这时候就可以立即给设备工程师下维修订单。</p><p></p><p>赖骏凯介绍，如今在友达工厂，早期那些加挂固定式的摄像头很多已经升级成了移动式的摄像头，通过加装在机械臂上，不但可以用来读取数据，还可以去完成一些不适合人工完成的操作。比如在车间的一些角落，人要进去比较困难，很容易撞头或者发生危险，这时候，机械臂的摄像头和自动巡检机器人配合就可以替代人工作业，完成点检的工作，用来规避这些安全隐患。</p><p></p><h1>如何搞定“人”的问题</h1><p></p><p></p><p>由于友达的智能化转型起步较早，当时行业内几乎没有可参考的案例和路径，所以回顾这7年的历程，赖骏凯坦言他们也走过一些弯路。</p><p></p><p>赖骏凯回忆，最开始友达对于AI这样的新技术也不熟悉，如何把它与制造流程结合更没有概念，所以初期只能先依托外部资源进行项目合作。“他们的确给我们带来了新的技术思维，对我们视野产生了新的扩展，但是在这个过程中我们还是遇到了两个问题。一方面，这些公司不够了解工业现场的场景和一线的真正需求，这使得我们之间的沟通成本很高；另一方面，当技术公司在项目完成撤出后，除了产品本身，他们的能力并没有留存下来，使得我们无法基于新的技术能力持续进行制程优化。”</p><p></p><p>友达很快意识到，这个困境的症结出在“人”上。于是，从2018年开始，友达每年都会安排遍布于各地、各个产线、各个部门的工程师进行<a href=\"https://time.geekbang.org/resource?m=0&amp;d=8&amp;c=8\">脱岗学习</a>\"，他们不只是技术人员，还可能来自生产部门、研发部门或者品质管理部门。在这个过程中，他们会先花大概数个月的时间学习AI、RPA等最新的技术，然后再用近一年的时间做专案，让每一个人了解除了过去他专精的业务领域之外，AI技术还可以解决哪些业务问题。</p><p></p><p>“随着我们AI落地经验的不断积累, 也摸索出工业应用的核心场景, 即将过去积累的经验转化为AI工具平台，也就是说，后进的人只要通过已平台化的工具就可以快速获得AI能力去解决问题。比如，我们有一个AI平台，它可以帮助员工在上面快速做数据分析，做最佳参数推荐。”赖骏凯表示，这一平台的使用门槛非常低，只要对制程机理有一定了解，进公司大概半年时间，就能在上面灵活运用AI工具帮助自己探索数据价值，而不一定要懂代码、懂Python。</p><p></p><p>那么，有了技术又找对了场景就能万事大吉了吗？真相往往没有那么简单。据了解，友达最初通过MES、ERP、IoT等系统的融合把数据做了统一的整合管理，然而，转型初期的<a href=\"https://www.infoq.cn/article/YcvRvZaOoez9sKJ92jHR\">数据利用率</a>\"偏低。很多企业在数字化、智能化的推进过程中也会遇到类似的问题。</p><p></p><p>在赖骏凯看来，这一方面是管理的问题。还以设备故障维修为例，虽然AI可以帮助工厂做数据读取、数据分析，实现故障预警。但是，后续的维修动作还是由人来完成。过去，产线生产员工和设备工程师之间的关键矛盾在于，设备出现故障会直接影响生产人员的业绩，但与工程师的业绩无关。所以，设备维修这件事最后往往是后者不急前者急。</p><p></p><p>为了解决这个问题，友达的办法是装预警灯+APP“抢单”。如果设备出现故障没有及时维修，预警灯就会长亮。而在后台，通过“抢单”的方式可以快速匹配工程师进行故障维修，并且所有维修结果，包括平均用时等等都会通过数据结果呈现到后台，与工单价格直接挂勾。通过这种管理模式的改变，友达的工厂车间从“有故障没人修”变成了“有故障抢着修”，从反应式管理变成了预测式管理。</p><p></p><p>另一方面，数据利用率低也是人的思维因素使然。“比如，那么多数据呈现在眼前，但不知道如何将其价值充分发挥出来。这时候我们就需要通过一些培训和机制培养员工在这方面的能力，让大家知道自己关心的指标什么情况下异常，如果发生异常应该采取什么动作，其中的数据量够不够，如果不够是否需要通过IoT再补充收集数据等等。通过这样的方式慢慢培养工程师的数据思维，让大家的能力螺旋式上升。”赖骏凯解释道。</p><p></p><p>也就是说，要让<a href=\"https://www.infoq.cn/article/k0Lr2oVo3EdPqVR6CLo0\">数据驱动</a>\"形成闭环是一个系统工程，既要有技术维度，也要有管理维度，二者融合才能让效果最大化。</p><p></p><h1>写在最后</h1><p></p><p></p><p>其实，变革这件事就像是过河，有人早早上岸，有人正在蹚水，有人还在观望。</p><p></p><p>赖骏凯表示，这是一个从0到1、从1到10、从10再到100的过程。企业不需要急于求成，一开始就投入大量的资源。而是找准自己的问题点，在几个关键的业务场景先针对性地投入做试点，取得一定改善成效之后再进一步复制平展。</p><p></p><p>比如，对于友达来说，并没有一上来就把老旧设备都抛弃，也不是一步到位全做了改造，而是先从关键设备开始，逐步对旧设备实现了数据联网，再逐步实现AI自决策与异常自调整。</p><p></p><p>“并且，我们在做任何流程改善的时候，都会先做流程梳理，然后再做流程精益，在精益化完成之后，才能把一些新的技术逐步放进去，形成新的智能化制造流程。”赖骏凯强调，“这么做的原因，一是避免资源浪费，二是让内部员工看到新技术的价值。”</p><p></p><p>如此一来，无论是数字化还是<a href=\"https://www.infoq.cn/article/UlxmUYElHHjTsU55rFUP\">智能化</a>\"，才能在企业内部形成自上而下、自下而上的可持续正循环，形成滚雪球式的效应放大，让企业真正从中受益，实现降本增效、提质转型的最终目的。</p>",
    "publish_time": "2022-09-26 12:42:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数智底座必备能力三：快速构建创新应用",
    "url": "https://www.infoq.cn/article/UzktAiP9d1L9JQMQyiBP",
    "summary": "<p>当前，全球企业都将数智化作为实现转型升级的关键路径，通过推进数智化，成为数智企业迈向高质量发展。企业希望能快速创新，快速组装及调整各类创新业务，能敏捷响应需求更快的业务及迭代，能承受大流量的冲击而保持韧性，能更好的连接外部资源及能力，扩展业务边界及运营；能更好的沉淀数据资产，并基于数据及智能产生价值，产业龙头企业能构建产业互联网，柔性扩展，产业链价值重塑，资源有效配置。所有的这些，需要一个数智化新底座。</p><p></p><p>通过统一的数智化底座，企业可以将技术、业务、数据深度融合，落地数智化转型，实现提质增效，迈向商业创新，成就高质量发展。</p><p></p><p>那么，构建企业数智化新底座，需要具备哪些能力？<a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651141032&amp;idx=3&amp;sn=6942e928e960a248b13ae1436b2009e8&amp;chksm=bdb8cbfb8acf42ed1813bbaec361a6976ff4a33073d1376191bd8828de005abb33c9e847ef59&amp;scene=21#wechat_redirect\">轻松驾驭新技术</a>\"、<a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651141163&amp;idx=3&amp;sn=93c66d4e0ababbc5f12e5a7b97c0fe87&amp;chksm=bdb8cc788acf456ec7f6bdc692ef3427db8be9416710951528a7745bda599c5392b8ce0c7156&amp;scene=21#wechat_redirect\">中台化的架构</a>\"、快速构建创新应用、对外链接与集成。接下来我们将通过四篇文章陆续为您揭开谜底！</p><p></p><p>面对外界环境的不确定性，以及业务个性化需求增加，企业需要具备快速构建创新应用的能力。然而，有限的技术开发生产力在一定程度上制约了数智化实践的效率。</p><p></p><p>问题有了，那解决方案呢？</p><p></p><h2>企业需求多变，低代码助力企业应用的快速构建</h2><p></p><p></p><p>据中国信息通信研究院数据表明，整个软件产业市场稳定增长，巨大的需求被释放，但企业面临着传统开发周期长、需求响应不敏捷、缺乏技术人才三大挑战。</p><p></p><p>在众多数智化转型的实施方案中，低代码开发平台因其“全民开发”的理念，成为企业首选。作为应用开发工具，低代码平台能够敏捷响应用户需求，促进企业数智化转型进入快车道，有效解决企业开发需求迸发与人才匮乏之间的矛盾。</p><p></p><p>低代码平台可将“功能”标准化、组件化，支持可视化应用开发。对于企业开发者来说，代码编写量大幅减少，高效完成应用构建的同时，还能有效减少代码 bug 排查与修复的工作量。甚至没有编码能力的业务人员也能利用低代码平台快速完成企业应用构建，实现企业 IT 资源的快速释放。随着云原生等技术的发展，低代码平台的组件库也逐渐丰富，如今的低代码平台不仅支持开发一些类似于表单的简单功能，像一些比较复杂的移动端应用也同样能快速实现构建。而且随着各大厂商通过对架构设计和编排引擎的利用，低代码平台的场景开发能力也都大幅增强，企业开发者“重复造轮子”的情况得到了极大改善。</p><p></p><p>使用低代码平台后，应用构建周期最快可以缩短到 1~2 周。应用构建周期的缩短，对新业务场景的探索也有着直接促进意义。低代码平台可以包容“需要小步快走、逐步打磨”或者是像促销活动等生命周期短的业务场景，可以根据实际业务反馈，灵活调整应用架构，缩短产品的落地周期，实现快速迭代，大大降低了创新业务的试错成本和短期项目的运营成本。</p><p></p><p>低代码如今的热度越来越高，曾经各方对低代码的理解差异在今年也逐渐统一。为了适应企业的高速发展场景下产生的复杂需求，低代码平台正在向可以支撑专业深度开发能力、综合支撑能力的平台方向演进，不再仅仅是满足简化代码开发的“拖拉拽”。</p><p></p><p>当下企业需求多变，国内低代码市场已经进入高速发展阶段，市场格局已十分细分，像用友等本土厂商开始追求产品差异化，定位于“数智化应用构建器”。基于低代码平台全面支持各类开发者的需求，包括不限于本地开发、SaaS、企业自建、ISV 生态开发等，为企业客户提供“定制化”应用构建。</p><p></p><p>比如，四川供销云基于 YonBuilder 低代码开发平台，开发设计“农村集体三资管理创新平台”，为 50 余家供销社县级会计服务中心开展 “互联网 +”会计服务、“三资”管理服务提供技术支撑，全面推动供销社会计服务体系建设。</p><p></p><h2>YonBuilder 满足数智企业多场景创新应用开发</h2><p></p><p></p><p>现在几乎每个企业都在喊“数智化转型，构建创新应用以促进业务增长”，但实际上，现阶段的创新型应用的主要场景其实就分为两类。一类是基于物联网的智能应用，一类是基于大数据的可视化应用。这两类应用的构建往往都是企业在进行数智化转型时的“投石问路”之举，是典型的低代码应用场景。</p><p></p><p>在物联网场景下，使用低代码平台，开发者可以与物联网平台无缝集成来构建 Web 或移动应用，从而将物联网数据转化为可感知业务逻辑及可操作的行为见解，以供最终用户使用。比如，制药企业“香雪制药”在生产制造行业特征明显，具有入库 &amp; 出库频繁度高、检验核验精细化程度强、物料管理调度频繁的特点，标准产品交付难度极大，而 YonBuilder 通过高可用的扩展能力实现了该业务场景应用的定制化构建。</p><p></p><p>在数据可视化的应用场景下，低代码平台能够构建面向不同用户的实时数据观测工具。比如世界第一港口的资产系统运维负责人通过 YonBuilder 的可视化、脚本能力，发布了系统问题反馈的应用，同时利用 YonBuilder 脚本能力开发了预警通知的服务，在系统运行过程中不断收集系统的运行状况，发生紧急情况能够第一时间得到预警通知。</p><p></p><p>其实对于企业用户来说，在做低代码平台选型时，需要考虑的并不是平台能够提供多少工具、组件，而是平台背后的研发能力。特别是在大型企业应用中，需求千差万别，在平台应用过程中会暴露出各种技术短板，企业需要的是一个有绝对实力可以满足应用开发需求又能帮其补齐短板的平台，而一体化低代码开发平台 YonBuilder 便是这样的实力代表。</p><p></p><p>YonBuilder 通过用友 BIP 强大的中台支撑能力，在元数据驱动和运行框架的统一模型架构下，通过点击拖拽、在线脚本以及多端编译的技术，提供可视化 + 低代码 + 全代码的一站式开发能力，快速生成 PC 和移动多端的业务应用，可即时发布与使用：</p><p></p><p>低代码应用开发：模型驱动，正向 / 反向建模 + 流程，快速构建企业业务应用。可视化画布、70 多个移动组件 &amp; 模版、超级 APP 门户，多端自动化编译, 实现移动应用极速开发；低代码数据分析：全量数据源，便捷的数据移动，可视化数据准备，一站式数据治理和数据开发，智能数据探索，零代码轻松构建智能分析应用；低代码 AI 开发：丰富的算法和预训练实例预置、 基于向导式交互的低代码 AI 应用训练与发布，让 AI 创作更普惠，运营更便捷；低代码集成链接：覆盖事件、消息、API、数据集成多种能力， 30 多个连接器、3000 多个 openAPI，可视化集成开发，让商业连接更容易、集成开发更轻松；低代码区块链构建：可视化构建联盟链，丰富的智能合约模板、基于向导式交互的应 用数据快速上链，让创新技术变得简单易用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/429b3456dd041daa4221a68e0abc883d.png\" /></p><p></p><p>值得一提的是，YonBuilder 移动开发平台的重要组成部分 APICloud，是国内低代码开发平台的引领者与效率革命的探索者，基于对云原生、BizDevOps、混合开发等能力的持续集成，聚合了国内主流 PaaS 和 SaaS 服务，提供 1400+ 成熟功能模块，如今已经完成了从移动开发向低代码开发平台的演进，形成了坚实的生态壁垒和先进的敏捷开发能力，可以使应用的开发周期从 6 个月缩短至 2 个月。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/adc73d6b22dd1a1782b8d7878186a224.png\" /></p><p></p><p>放眼国内市场，做低代码平台的厂商有很多，每个品牌都有自己的看家本领，但评测指标无非都是平台基础能力、平台生态、集成整合、可扩展性、用户体验这五个。而 YonBuilder 作为面向企业组织和个人开发者的企业业务应用快速开发平台，在技术能力表现上是领域内少见的“五边形战士”。</p><p></p><p>YonBuilder 低代码开发平台让从业务人员到专业开发者的所有企业角色，都能高效地将需求直接转换为业务应用，帮助企业用户完成技术的深度融合，力求真正实现应用开发大众化。</p><p></p><h2>AI 扩展低代码能力边界，重塑企业数智化转型</h2><p></p><p></p><p>当前环境，企业只有积极参与 AI 的生产，才能从源头上抓住先机。BCG 在调研中发现，约 86% 的市场需求需要定制开发业务场景下的 AI 模型。但 AI 是计算机科学的一个高科技领域，开发者需要丰富的技术知识和经验，端到端完成一个项目落地至少需要 3-6 个月。低代码平台 AI 能力的拓展，使非专业开发者更容易将 AI 与企业自研平台轻松集成。</p><p></p><p>YonBuilder 能给到企业用户的是基于用友 iuap 平台的 AI 造血能力。企业用户在低代码平面可以直接连接到用友 iuap 的业务中台，通过业务中台提供的丰富业务模板，开发者以 YonBuilder 作“向导”将用友的 AI 能力模板化，快速完成应用功能构建，场景不限于数据分析或者是其他中台业务场景。像三一集团、华菱钢铁、飞鹤都通过 YonBuilder 做了自家数智化运营平台的搭建。</p><p></p><p>此外，企业用户也可通过 YonBuilder 对用友 iuap 数据中台的服务能力的集成和接入，搭建出的企业服务和应用将具备强悍的数据分析能力。据此，企业用户可以基于 AI 技术，深入探索内外部多重经营约束条件下的经营变化，推送前置决策建议，帮助业务实现更精准的辅助决策。</p><p></p><h2>写在最后</h2><p></p><p></p><p>回顾过往，企业数智化的体现形式，是通过各种行业管理软件将云计算、大数据、人工智慧、物联网等新技术串联起来，行成一定的解决方案，从而应用于企业业务、管理和运营场景之中。</p><p></p><p>近些年来数智化转型的路径与方法越发简单化。从互联网 +，到企业上云，到工业互联网，到业务流程自动化，再到现在的低代码 +AI，我们会发现数智化转型已经从流程驱动转变为数据驱动。通过构建数据驱动的数智底座来支撑数智化落地，成为众多大型企业的共同选择。</p><p></p><p>基于统一的底座能力，业务部门的管理者及开发人员等可以直接参与整个企业的应用系统的建设。</p><p>而低代码正是那个可以为企业带来敏捷创新能力，并借此驱动一个部门甚至整个企业创新发展的来源。所以，如果一家企业已经走在了低代码加持推动转型的路上，那说明，这家企业已经走在了正确的路上。</p><p></p><p>有的企业管理者认为低代码开发平台的“降本增效”优势会节约开发者成本，甚至不少开发者为自己的“饭碗”感到担心 。但事实上，低代码的兴起并不是为了取代开发者，而是为了让开发者从繁重重复的代码中解放出来，参与到更有价值的开发环节 ，技术的进步离不开开发者的探索。我们所有人都要清楚，这场技术革命，或许会成为“颠覆”，但不会成为“取代”。</p>",
    "publish_time": "2022-09-26 13:52:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "当出海成为必选项，企业如何构建全场景全生态技术底座？",
    "url": "https://www.infoq.cn/article/zd6J6sPGYgSTkoyNO3Ph",
    "summary": "<p>采访嘉宾 | 宋清晨</p><p>整理 | 任传英</p><p></p><p>“大航海”时代，企业出海成为业务增长不可忽视的方式，出海技术的敏感度、数据合规、跨国通信、不同地区用户对于产品的倾向等，一直是行业中热议的话题。</p><p></p><p>9 月 15 日，<a href=\"https://xie.infoq.cn/article/41f96992a88e303d432e572ef\">融云</a>\"在“超浪 2022 社交泛娱乐出海嘉年华”活动中，与白鲸出海联合发布了《2022 社交泛娱乐出海白皮书》（以下简称《白皮书》），回顾了国内互联网出海的历史，并聚焦社交泛娱乐作出系统的行业洞察与数据解读，对企业出海的现状、痛点与挑战进行了深度剖析。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43e7cf68c12e8adab48d6863301359cb.jpeg\" /></p><p>（白皮书限时免费开放，感兴趣长按识别下载）</p><p></p><p>当天晚上，InfoQ 邀请融云出海业务总经理宋清晨做客《极客有约》，为大家详细解读了企业出海的现状及未来趋势，并给相关企业与开发者带来了一些中肯的建议。</p><p></p><p>以下内容节选自当天的分享，InfoQ 做了不改变原意的编辑：</p><p></p><p>&nbsp;InfoQ：融云是领先的全球互联网通信云服务商，能否请宋总再展开地介绍下融云。</p><p></p><p>宋清晨：融云成立于 2014 年。在此之前，公司核心骨干是给中国移动做飞信的团队。熟悉飞信的朋友都知道，在那个年代飞信就已经是日活上亿的一个应用了。即时通讯作为当时最复杂的技术之一，是很多想在社交赛道发力的创业团队可能花费数月甚至更久也很难攻克的。在这样的背景下，融云团队将即时通讯技术封装成 SDK，开创性地提供了通信云 PaaS 服务，让开发者可以快速获得即时通讯能力。</p><p></p><p>2015-2016 年，以内容为主的泛娱乐数字产业开始向海外发力，融云随之陆续在北美、新加坡等地区上线了数据中心；2018 年，我们发现用户除了需要即时通讯外，对于实时音视频通信的需求也逐渐增多，所以我们上线了实时音视频的公有云技术，形成了一张 IM+RTC 的通信大网；2019-2020 年，随着音频社交与游戏社交出海的崛起，融云上线了直播功能；近两年，在社交与元宇宙、社区等场景结合的趋势下，融云又上线了一系列场景化 SDK，供开发者更加快速、便捷地集成解决方案，同时随着单一功能场景的需求越来越少，融云在深化核心通信能力的同时，也在其他通信业务上进行了创新，陆续上线了翻译、美颜、空间音效等一系列周边能力，形成了现在的 IM+RTC+X 全通信解决方案。</p><p></p><p>我们希望在全通信解决方案的加持下，提供给开发者开箱即用的工具，让开发者把更多精力投身在打磨产品和商业增长上。</p><p></p><p>&nbsp;InfoQ：<a href=\"https://xie.infoq.cn/article/8e0b7ea199f617f89e279d0db\">《2022 社交泛娱乐出海白皮书》</a>\"中提到了“社交和泛娱乐的融合趋势正在加强”，您对“社交 +”模式的创新怎么看？哪些模式值得国内企业持续关注？</p><p></p><p>宋清晨：社交和泛娱乐这两个原本各自独立的行业，随着网络技术、用户习惯、市场竞争等多方面原因，边界愈发模糊，“社交 &amp; 泛娱乐”的融合也成为自 2019 年以来行业的主流发展趋势之一。</p><p></p><p>“社交 +”的存在感越来越强，与用户运营与增长的考量有很大关系。增长已经见顶，大家都在争夺用户的注意力，如何让用户留存得更久，考验的是 App 的粘稠度，而 App 粘稠度关键就在于两点 —— 社交关系和内容。这也是为什么大部分 App 都在渗透社交。</p><p></p><p>通过梳理美国、巴西、沙特阿拉伯、印度尼西亚等社交 &amp; 泛娱乐出海厂商重点关注的 4 大市场出海情况，以及分析近半年来 4 个市场 App &nbsp;Store &amp; Google Play 娱乐、社交、生活等 3 大品类下载和收入 Top50，我们发现“社交 + 游戏”、“社交 + 音频”、“社交 + 视频”、“社交 + 虚拟形象”、“社交 + Dating”以及“社交 + 社区”等 6 大类别，最能概括目前市场的主要玩法模式。</p><p></p><p>举个例子，“社交 + 社区”的超级群产品模式在近期就非常火爆，我们可以把它运用到很多场景和玩法中。事实上，社区这个品类咱们并不陌生，无论是天涯、豆瓣还是贴吧，都曾经在某一阶段是现象级产品，但也都随着技术和用户习惯的演进而退出 C 位。这两年，国内出现了很多类 Discord 的实时社区产品，算是一种从国外引入的产品形态。要知道，Discord 已经拥有 5 亿用户，且年轻用户比例极高，是一个不容忽视的社交产品。</p><p></p><p>我想，Discord 的强势破圈，以及它对传统社交巨头的冲击，是有一定启示意义的。而这种把无限用户管理、社区内容丰富性和即时聊天顺畅性相结合的产品形态，是游戏社区、粉丝运营、地区运营及各类兴趣社区的绝佳实现方式。融云针对这一场景，推出了首个完整封装业务逻辑的原生底层技术实现方案——融云超级群，已经在国内外拥有多个实践案例。</p><p></p><p>我们看到国内已经有不少大小厂商入局，不过目前还没有哪一家真正跑出来。我认为这个方向是有一定机会的。</p><p></p><p>除了针对“社交 + 社区”的超级群，融云针对上述 6 个品类，结合美颜、CDN、审核、推送等通信周边能力，还推出了开箱即用的语聊房、直播、1V1、元宇宙、游戏等场景化解决方案，更好地服务开发者对创新场景的探索。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a506f31a41235cce3c59055bfd02b78c.png\" /></p><p>（融云社交泛娱乐全场景方案）</p><p></p><p>&nbsp;InfoQ：放眼全球不同地区，对于国内出海应用的接受度怎么样？哪些类型的应用更受海外用户青睐？</p><p></p><p>宋清晨：产品的接受度与其完成度及当地发展程度直接相关。像欧美这些发达地区的用户更愿意为产品的价值买单，他们对沉浸式的社交类 App 更有兴趣，比如交友 App、职场 App、虚拟形象 App 等等；至于拉丁美洲，用户还比较关注电商类的应用；中东地区则以娱乐性目的为主，比较关注直播、音视频、游戏社交等；东南亚由于地区、国家、文化的多元化，对于 App 的需求也呈现出多元化的特点。</p><p></p><p>时间有限，在此我就不展开赘述了。我们《白皮书》中详细分析了全球各个市场“用户为什么而付费”、“哪些 App 获得用户青睐”等出海厂商非常关心的一众问题，非常推荐大家下载一读。</p><p></p><p>InfoQ：近几年，全球的企业都面临着营收持续增长的挑战，出海也成为一个必选项，我们关注到出海企业大多都聚集在社交泛娱乐赛道，为什么会出现这样的现象？</p><p></p><p>宋清晨：这是一种必然现象。一方面，国内的社交泛娱乐玩法越来越多，加上疫情、泛娱乐、数字化转型高速发展等因素的影响，使得线下的社交和娱乐方式都在向线上迁移；另一方面，是来源于社交模式的变化：社区社交、元宇宙社交等新趋势的出现。</p><p></p><p>另外，是来自供给方和需求方，现在大家的在线时长越来越长，但是关掉手机之后，大家没有更多的收获，虽然互联网拉近了彼此的关系，但是也形成了一个无形的屏障，在这个过程中，聚焦在社交泛娱乐赛道上的变化，其实是一种推陈出新，这也在一定程度上印证了当下人们对于社交模式的重新定位和思考。</p><p></p><p>因此，我们需要通过社交平台用新的模式让用户有归属感，所以相关的社交产品也会越来越多。一切应用皆社交，社交赛道的变化，也影响了其他的赛道或者玩法。</p><p></p><p>&nbsp;InfoQ：社交泛娱乐应用出海，企业侧面临的挑战主要有哪些？</p><p></p><p>宋清晨：首先是获客压力，其次面临着本地化经验不足的问题，最后还要考虑合规、政府监管的复杂度。</p><p>自从 2018 年起，欧盟就提出了通用数据保护条件，个人隐私引起了各国政府和民众的高度重视，全球各地都在通过颁布法律法规、加强监管、提升自身的数据安全治理能力来应对与日俱增的数据安全威胁。除此之外，出海一直在个人隐私、数据安全、跨境参数、内容安全，以及供应链的风险方面存在巨大的挑战。</p><p></p><p>InfoQ：面对全球不同地区的复杂网络环境，技术层面是如何解决延时挑战的？</p><p></p><p>宋清晨：融云一直在持续不断地完善、壮大我们的全球接入点，自研了最优的链路调度算法，可以将动态文本、图片、音视频快速地分发给我们的终端用户。</p><p></p><p>针对那些对延时、数据安全比较敏感的业务，融云会加速网络服务，将海外、国内之间的沟通延时控制在 190ms 以内。访问者通过接入就近的节点，就可以传输数据，享受融云优质的全球通信服务。除此之外，我们还会在节点之间的专线储备方面下功夫，保证中间没有中转，降低数据传输出现重传的几率。</p><p></p><p>除了传输之外，我们对基建、服务、协议等方面也在持续优化，希望 SDK 更小，从而提高传输数据的便捷性。</p><p></p><p>InfoQ：除了通信方面，融云在玩法创新、获客方面有什么样的考虑？</p><p></p><p>宋清晨：关于玩法创新，我们在长期服务社交泛娱乐客户的过程中，观察到开发者对产品主要有以下两个方向的需求：</p><p></p><p>首先，产品核心功能的多样化。比如直播类产品，除了要融合使用 RTC 和 CDN 以提供高品质、低延迟的用户体验之外，在主播与粉丝或者叫榜一大哥互动的过程中，与其他主播的 PK 也是一个非常重要的功能。</p><p>其次，产品外延价值的拓展性。比如对于休闲游戏的接入，它不仅是陌生人社交产品中的用户破冰利器，也是 1V1、语聊房、IM 聊天等几乎所有场景都适用的活跃法宝。</p><p></p><p>以上的能力，无论是贴近市场的多样化功能，还是运营需要的周边能力，融云的全生态一体化解决方案都可以提供，并且是以完整封装成 SDK 的方式来提供，极大地降低开发者的研发和学习成本，也让他们能腾出手来去针对用户洞察做更多创新。</p><p></p><p>而关于获客，我们都知道这是目前大家普遍面对的难题。尤其是在全球化业务中随着苹果隐私政策和 Google 核心算法的更新，咱们已经非常熟悉的 ROI 方法论其实受到了一定的冲击。</p><p></p><p>我们最近也在跟很多开发者聊，大家普遍会觉得目前确实是需要回归到产品价值上，去发现和挖掘核心用户，围绕他们做好服务，并且以他们的自发传播来拉新。</p><p></p><p>我们的出海《白皮书》也总结了一些全球化应用的创新手段：</p><p>A. 加强对用户需求和生态变化的洞察，开发或者迭代可借力“上升通道”的产品，如 Widget、做经验证产品模式的细分领域以及满足头部产品忽视的用户需求；</p><p>B. 通过种子用户定位目标受众画像，对应受众去选择与之相适应的获客渠道，例如小组件和社交 App 利用 TikTok 冷启动。</p><p></p><p>《白皮书》还有更多创新案例，欢迎大家获取完整版本参考借鉴。</p><p></p><p>InfoQ：对于目前正在规划出海的企业，从赛道、地区、技术架构、法律合规层面，您有哪些好的建议？</p><p></p><p>宋清晨：第一，数据安全是基石。如果不考虑数据安全，准备日后再找补，成本会非常高；</p><p>第二，要考虑内容合规。它是一个非常大的隐患，会对后期落地、起量造成很大的阻碍；</p><p>第三，要做到降本增效。企业规划出海的时候，应该更加聚焦自己产品对应的市场以及客户群体，而不是把所有精力投放到产研、开发上；</p><p>第四，不能闭门造车，企业深入研究用户对玩法的理解，才能帮助自身产品更快地落地。</p><p></p><p>InfoQ：近两年有一种说法是“海外社交泛娱乐赛道不存在蓝海，几乎每个赛道都是红海”，您对这个说法怎么看？未来，社交泛娱乐赛道可能存在的突破点在哪？</p><p></p><p>宋清晨：中国互联网出海已经有近十五年的历史了，社交泛娱乐是其中非常核心的赛道，也已经经历了从蓝海到修罗场的一个状态转变。</p><p></p><p>融云与白鲸出海近期联合发布的《白皮书》对美国、巴西、印度尼西亚、沙特阿拉伯 4 个市场进行调研发现，除了美国市场，在剩余 3 个市场，基本上形成了美国、中国、本土市场的 3 强格局，竞争可以说非常充分了。</p><p></p><p>但“红海论”是不是这么绝对，还是一个非常值得探讨的话题。因为《白皮书》也同样展现了另一个视角——不同区域市场的机会差异。比如，美国市场，我们看到了一些网文赛道的冒头趋势；娱乐赛道同质化问题非常严重的中东也还是时常有新产品登上榜单；东南亚市场展现出来对 Web3 的极大欢迎；以及拉美市场工具类产品的强势地位。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/5302dbb5fc96006f1304d1b9d0d6f03e.png\" /></p><p></p><p>所以我们更偏向于认为，红利是永远都有的，现在更考验的是开发者对自身资源、所在赛道及目标市场匹配度的深刻洞察能力。</p><p></p><p>我觉得这也可以同时回答“未来社交市场突破点”这个问题，突破点在于对那些未被巨头覆盖的用户需求的洞察和满足。</p><p></p><p>比如我们有一个客户叫 Zervo，做的是聚焦欧美青少年的二次元社交产品，就是因为发现了欧美青少年在二次元这个垂直方向的一些机会，采用融云超级群做了一个社区产品。相较于传统的社交巨头，它更聚焦欧美青少年用户，相较于 Discord 这样的年轻化社区产品，它又主打其中二次元的那部分，可以说是非常好的一个差异点。我认为未来的一些机会，更多正是会出现在这些细分垂直方向的“差异点”上。</p><p></p><p>InfoQ：未来，全球通信服务商会面临哪些机会和挑战？融云对此有什么规划？</p><p></p><p>宋清晨：作为全球通信服务商，融云会聚焦满足开发者对“社交 +X”赛道的探索需求，提供直播、语聊房、1V1、元宇宙、游戏、实时社区等场景化解决方案，并根据市场和用户需求持续迭代更新，希望能够做得更多、做得精细、让开发者做得更少。我们还在纳入更多的数据节点，找更多的服务厂商合作，不单是为了满足客户和开发者的需要，也是为了打磨自己的平台能力、服务能力等，这是融云对自身的要求。</p>",
    "publish_time": "2022-09-26 14:07:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里云AI技术分享会第四期——大规模稀疏模型演进与DeepRec初探",
    "url": "https://www.infoq.cn/article/IegCujQrW1vzEA3pTuYQ",
    "summary": "<p>大规模稀疏模型的应用是搜索、推荐、广告等多业务领域所面临的重要课题。</p>\n<p>在稀疏模型结构中离散特征的计算、通信、访存占比较高。离散特征通常表示为算法不能直接处理的非数值特征，其广泛用于高价值业务中。</p>\n<p>DeepRec 是阿里巴巴集团统一的大规模稀疏模型训练/预测引擎，广泛应用于淘宝、天猫、阿里妈妈、高德等，支持了淘宝搜索、推荐、广告等核心业务，支撑着千亿特征、万亿样本的超大规模稀疏训练。</p>",
    "publish_time": "2022-09-26 14:17:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“抄我的还‘反捅’我一刀”，Gary Marcus发文驳斥图灵奖得主Yann LeCun",
    "url": "https://www.infoq.cn/article/JcpMIMgeS7bgM2hqzcG8",
    "summary": "<p></p><p></p><blockquote>今年6月，图灵奖得主、深度学习先驱、Meta公司首席AI科学家Yann LeCun发表了一篇长达62页的论文，论文阐述了他对过去 5 年 - 10 年关于 AI 发展大方向的思考，并提出了AI未来的新愿景。对于这篇论文的观点，著名AI学者、Robust.AI 的创始人兼 CEO 、纽约大学的名誉教授Gary Marcus并不认同，并专门写了一篇长文驳斥，以下为全文。</blockquote><p></p><p></p><p>62岁的深度学习先驱、纽约大学教授、图灵奖获得者兼Meta公司首席AI科学家Yann LeCun，又肩负起自己的一项新使命。他的任务，就是从技术缔造者以及思想者的角度，探索如何超越深度学习。</p><p></p><p>经过一番深思熟虑，他把自己的观点透露给了多家外媒，于是，<a href=\"https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/\">《Yann LeCun为AI未来大胆勾勒出新愿景》</a>\"的报道迅速亮相。</p><p></p><p>今年6月初，LeCun还发布了一份激起热烈讨论的宣言，也对我（Gary Marcus）的差异化研究工作做出回顾，表达了他自己在一系列重要符号处理问题上的立场。</p><p></p><p>但是，他所谓的“新”主意到底有多新？</p><p></p><p>读了那篇报道，我的第一反应是震惊。LeCun提到的一切我之前几乎都曾说过，而且字句甚至都没多大变化——大部分内容来自我在2018年发表的<a href=\"https://arxiv.org/abs/1801.00631\">《深度学习：一种批判性评价》</a>\"的论文，LeCun当时可是嘲讽说此文“大多不靠谱”。</p><p></p><p>下面我先列出他观点照搬的七个例子，之后再聊聊LeCun的观点整体上出了什么问题。</p><p></p><p>LeCun，2022年：“今天的AI方法永远无法实现真正的智能（在标题部分使用，并非直接照搬）”；Marcus，2018年：“如果我们要实现人工通用智能，必须为深度学习辅以其他技术。”LeCun，2022年：“[当前的深度学习模型]可能是未来智能系统中的组成部分，但我认为其中还缺少别的必要部分”；Marcus，2018年：“尽管我已经罗列出所有问题，但我认为答案并不在于放弃深度学习。相反，我们需要重要为深度学习规定概念：它不是一种通用方法，而是众多工具中的一种。就像在物理世界中，我们不仅需要锤子、扳手、钳子和电动螺丝刀，还需要凿子、钻头、电压表、逻辑探针和示波器。”LeCun，2022年：“强化学习永远不足以实现智能”；Marcus，2018年：“深度强化学习的能力其实跟概念感知没什么关系。”LeCun，2022年：“我们的智能机器甚至连小猫所能理解的常识都把握不了，所以为什么不从这里开始探索呢？”；Marcus，2022年：“那在深度学习之外，我们还应该寄希望于哪里？……第二个关注重点也许是常识性知识。”LeCun，2022年：“我认为AI系统需要具备推理能力”；Marcus，2018年：“深度学习的适用范围主要集中在分类方面，对跟常识推理有关的问题则束手无策。据我所知，深度学习几乎解决不了这方面问题。”LeCun，2022年：“退后一步来看，我们就像是造了一把梯子。但我们的终极目标是登月，这把梯子显然不可能帮我们达成心愿。”Marcus，2012年《纽约客》：“套用一个古老的寓言，Hinton通过深度学习造了把好梯子，但再好的梯子也没法让人登上月球。”LeCun可以算是认认真真复述了一遍我的观点，而且完全没提到言论出处。</p><p></p><p>但我不打算指责LeCun抄袭，毕竟他可能只是在认识到现有架构的失败之后，很诚实地自己得出了这些结论。我之前的想法，现在终于为他所认清和接受。这对我来说相当于巨大的胜利——毕竟LeCun可是非常杰出的研究者，他能认可我的观点非常难得。</p><p></p><h2>关于“攻击我从没写过AI论文”的驳斥：</h2><p></p><p></p><p>但事情到这里还远没有结束。</p><p></p><p>首先，LeCun明显想把自己的观点跟我的想法刻意区分开来。他之前公开批评过我的论文，所以这次发言他全程没提到很多想法我早就说过——这其实不大符合学术礼仪，也让人对这位老教授颇为失望。</p><p></p><p>不仅如此，为了保证我的言论得不到广泛支持，他还在采访中对我进行了无端且毫无客观性可言的攻击。他宣称“Gary Marcus根本不是搞AI的，他其实是个心理学家。他从来没为AI做出过任何贡献。他在实验心理学方面做得非常好，但从来没写过关于AI的同行评议论文”——这种论断完全错误。</p><p></p><p>实际上，我在AI方面发表过很多文章，有些发表在同行评议的期刊上、有些没有。我最重要的AI论文就是关于神经网络实验的，而且在1998年就预见到了分布变化与异常值方面的难题——这些难题目前正困扰着Yoshua&nbsp;Bengio等研究者。</p><p></p><p>过去十年来，我发表了不少经过同行评议的AI论文，主题涵盖常识、利用不完整信息进行推理，以及模拟/自动推理的局限性等。</p><p></p><p>其中很多文章还是跟LeCun学术阵营中的同事、纽约大学计算机科学家Ernest Davis共同撰写的。而我最具影响力的AI成果，其实并不是期刊文章，而是2001年出版的《代数思维》（The Algebraic Mind&nbsp;，由麻省理工出版社送交同行评议）。LeCun在此次采访中表达的一切，在那本书中几乎都有提及。但讽刺的是，LeCun却总在暗示自己没读过这本书，这实在太可笑了。</p><p></p><p>也许是LeCun的说法太过离谱，引得其他人现身帮我辩护。就在我撰写本文的同时，Vmind.AI公司CEO Miguel Solano发表了推文来支持我：</p><p></p><p></p><blockquote>Miguel Ignacio Solano @miguelisolano@GaryMarcus&nbsp;@ZDNET&nbsp;@TiernanRayTech&nbsp;@ylecun&nbsp;确实如此，&nbsp;@ylecun. @GaryMarcus的《代数思维》（麻省理工学院出版社，2001年）得到过868次引用，而且他肯定发表过AI文献:&nbsp;<a href=\"https://scholar.google.com/scholar?cites=2332635447613222375&amp;as_sdt=205&amp;sciodt=0,1&amp;hl=en\">scholar.google.com/scholar?cites=…</a>\"-2022年9月25日</blockquote><p></p><p></p><p>Henning Schwabe的表达则更加尖锐：</p><p></p><p></p><blockquote>Henning Schwabe @SchwabeHenning@ylecun的攻击并不公平，就连最支持深度学习的人也要站出来说两句了。&nbsp;@GaryMarcus是个很好的盟友，大家应该团结起来共同推动技术发展。过度自我永远是理性的敌人。</blockquote><p></p><p></p><p></p><blockquote>Dagmar Monett @dmonett我在@Eric_Sadin的《世界的芯片殖民化》（2016/2018）中找到了对LeCun行为的解释。详见图片和翻译。@ZDNET的采访也再次证明了这一点，让我们感受到了LeCun如何粗暴鲁莽地指摘其他学者的工作。😳 <a href=\"https://t.co/qy3WrIcJAb\">https://t.co/qy3WrIcJAb</a>\" <a href=\"https://t.co/KlPXhYjNxM\">https://t.co/KlPXhYjNxM</a>\"—— 2022年9月25日</blockquote><p></p><p></p><p>研究生有时候爱用学分来证明自己。Harold Bloom还专门写过《焦虑的影响》（The Anxiety of Influence）一书分析这种心态。但直到现在，我才在LeCun这个量级的大咖身上看到同样的毛病。</p><p></p><p>而且不是一次，是一次又一次。</p><p></p><h2>LeCun立场生变：用自己的方式否定自己的过去</h2><p></p><p>LeCun最近发表的每篇论文，都是在用自己的方式否定自己的过去。</p><p></p><p>其中一篇文章谈到了符号处理这个老大难问题。简单总结一下，LeCun在很长一段职业生涯中都在抨击符号处理。他的研究合作伙伴Geoff Hinton也是一样，二人在2015年共同撰写的深度学习评论最后提到，他们“需要新的范式来取代基于规则的符号表达处理。”</p><p></p><p>现在，LeCun又开始支持符号处理了（符号处理的概念不是我发明的，但我30年来一直都表示支持），而且表现得好像这是他刚刚发现的新趋势一样。但LeCun所说的“每位深度学习从业者都认为，符号处理是创建类人AI的必要一环”，其实就是过去几十年来无数研究者的探索方向。所以连一直支持LeCun的斯坦福AI教授Christopher Manning也表达了震惊：</p><p></p><p></p><blockquote>Christopher Manning @chrmanning我感觉&nbsp;@ylecun的立场似乎有所变化——可能是受到了Browning的影响；新的文章提到，“每位深度学习从业者都认为，符号处理是创建类人AI的必要一环。”但十年前可不是这么说的呀，或者现在大家真的都这么认为了？！？—— 2022年7月28日</blockquote><p></p><p></p><p>在我一一列举出这些问题时，LeCun并没有做出正面回应，而是转发了来自合著者Browning的一段莫名其妙的反驳：</p><p></p><p></p><blockquote>Browning.jake00 @Jake_Browning00来自@GaryMarcus的回复已经看到，但我们不同意他的观点，或者说不同意他所指出的分歧。不过我觉得，解决困难问题时难免会出现种种分歧。Noema Magazine @NoemaMag长达十年的AI辩论终于要解决了吗？@garymarcus似乎看到了结束的迹象。现在，“我们终下载可以专注于真正的问题：如何让数据驱动的学习与抽象符号表示统一起来。”https://t.co/QtaxfAEWdv——2022年8月14日</blockquote><p></p><p></p><p>但对我提出的一条条反驳，他们完全没做出任何具体评论。</p><p></p><h2>关于大模型的论断的辩论</h2><p></p><p></p><p>LeCun最近发表的另一篇文章谈到了一个重要问题，即大型语言模型是否真的走上了通往人工通用智能的正确道路，包括人是否真能单靠语言表达就掌握足够的常识。</p><p></p><p>LeCun和合著者Browning提出了强有力的论断，认为单凭语言输入（也就是GPT-3训练模型之类的成果）还不够。他们专门写了一篇题为<a href=\"https://www.noemamag.com/ai-and-the-limits-of-language/?utm_source=noematwitter&amp;utm_medium=noemasocial\">《AI和语言的局限》</a>\"的文章，认为“单靠语言训练的系统既从现在开始一直训练到宇宙热寂，也永远不可能接近人类的智慧。”</p><p></p><p>但他们这个观点仍然不是原创。我在2020年2月<a href=\"https://arxiv.org/abs/2002.06177\">《AI的下一个十年》</a>\"一文中也提出过相同的问题：</p><p></p><p>等待越来越大的语言训练语料库中自动孕育出认知模型和推理能力，就像是在等待神迹的出现……——这几乎跟LeCun和Browning的结论完全相同。</p><p></p><p>这还没完呢。</p><p></p><p>下一个关键问题，就是我们到底该怎么做。我们没法单靠大型语言模型来真正解决AI问题，所以我就在2020年1月提出：</p><p></p><p>像GPT-2这样的系统无论自身功能表现如何，都不具备任何明确（即无法直接表示、无法简单共享）的常识知识、推理能力和确切认知模型。</p><p></p><p>之后是2020年2月：</p><p></p><p>花在改进大规模单词级预测模型上的每一秒钟都是浪费，不如用在开发有望实现推导、更新和推理认知模型的技术上。</p><p></p><p>听起来是不是很耳熟？LeCun在最新采访中宣扬的，也是与认知模型相结合的相同观点。</p><p></p><p>而我在2019年第一次提出这个观点时，猜猜谁马上跳出来喷我？对，就是Yann LeCun。</p><p></p><p>我当时写道：</p><p></p><p></p><blockquote>Gary Marcus @GaryMarcus像GPT-2这类系统的最大问题并不在于能不能理解数量（@ylecun的意见恰好相反），而是没能发展出能准确表示事件如何随时间展开的清晰w数。这个问题单靠堆数量是解决不了的：</blockquote><p></p><p></p><p></p><blockquote>Yann LeCun @ylecun@StanDehaene @GaryMarcus实际上，接受过数量处理训练的机器确实学会了处理数量。Gary总说通过训练来预测缺失单词的机器掌握不了数量的概念，呸！2019年10月28日&nbsp;这其实就是在用另一种表达，强调大型语言模型缺乏认知模型的问题。</blockquote><p></p><p></p><p>当时，LeCun还说我的论证在根源上就错了：</p><p></p><p></p><blockquote>Yann LeCun @ylecun@GaryMarcus&nbsp;是错的。咱们看看: <a href=\"https://arxiv.org/abs/1612.03969\">arxiv.org/abs/1612.03969</a>\"其中表二：第7行（计数）和第14行（时间推理）都获得了0错误率（在bAbl任务中）。你在抛出论点的时候，最好搞清楚你的论据三年之前就已经站不住脚了。 —— 2019年10月28日</blockquote><p></p><p></p><p>现在他的想法变了，于是之前说过的话就如同没说。他也开始强调认知模型的重要意义，又反“捅”我一刀，坚称这是他的独创观点。</p><p></p><h2>LeCun的新论文“洗稿”多位前辈学者成果</h2><p></p><p>所以我当然有理由生气，而且也有其他同行在为我打抱不平。</p><p></p><p>深度学习先驱、广泛应用的LSTM神经网络创造者Jürgen Schmidhuber最近也在推特上提到：</p><p></p><p></p><blockquote>Jürgen Schmidhuber @SchmidhuberAILecun (@ylecun)在2022年关于自主机器智能的论文中把老观点复述了一遍，但却丝毫没提到这些内容早在1990年到2015年的研究中就已经存在了。我们已经发表了他口中的那些“重要原创贡献”，包括：学习子目标、可预测的抽象表示、多个时间尺度等。</blockquote><p></p><p></p><p></p><blockquote>LeCun2022年发表的新论文其实是对1990至2015年间原有成果的“洗稿”，例如1990年：梯度下降学习子目标；1991年：多个时间尺度与抽象级别；1997年：学习可预测抽象表示的世界模型……——  2022年7月7日“复述却不引用”——这可以说是对其他学术研究者最大的冒犯了。</blockquote><p></p><p></p><p>LeCun的新宣言总体上还是出于良好的动机，呼吁将“可配置的预测世界模型”纳入深度学习。我也一直强调应该这么做，但最早提出这一理念的是Schmidhuber。作为1990年代的深度学习先驱，他也一直在为此而努力，但LeCun对此甚至不愿提一句感谢。</p><p></p><h2>LeCun观点引来多位学者批评</h2><p></p><p></p><p>LeCun的妄言已经在推特上激起波澜。</p><p></p><p>德国计算神经科学家与AI研究员Patrick Krauss也在推文中嘲讽：</p><p></p><p></p><blockquote>Patrick Krauss @Krauss_PK哇哦，AGI终于实现了！&nbsp;😂&nbsp;@ylecun发现了迄今为止深度学习中缺失的环节：常识和世界模型！&nbsp;<a href=\"https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/\">technologyreview.com/2022/06/24/105…</a>\"@GaryMarcus&nbsp;@maier_ak</blockquote><p></p><p></p><p></p><blockquote>Yann LeCun对于AI未来的大胆设想，其实是把当初深度学习先驱们的观点汇总了起来。但尝试回答问题的同时，这又生成了很多新的问题。—— 2022年6月25日</blockquote><p></p><p></p><p>今天早上，Lathropa发布了更加尖锐的批评。大家都知道，我在2022年3月发表的《深度学习正走进死胡同》（<a href=\"https://nautil.us/deep-learning-is-hitting-a-wall-238440/\">https://nautil.us/deep-learning-is-hitting-a-wall-238440/</a>\"）一文受到了LeCun的猛烈抨击。</p><p></p><p>现在几个月过去，他怎么想法又变了呢？</p><p></p><p></p><blockquote>Lathropa&nbsp;@lathropa@GaryMarcus&nbsp;@MetaAI&nbsp;@ylecun “好吧，我们造了把梯子，但我们想要登月，而靠这把梯子显然登不了月。”LeCun说他希望重新审视深度学习的基本概念，“似乎是说他的方法走进了某种形似死胡同的空间”……&nbsp;-2022年9月25日</blockquote><p></p><p></p><p>我实在没从LeCun的采访中看到什么真正的新东西，所以昨天专门邀请他在推特上做出解释。他暂时还没回复，让我们拭目以待。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://garymarcus.substack.com/p/how-new-are-yann-lecuns-new-ideas\">https://garymarcus.substack.com/p/how-new-are-yann-lecuns-new-ideas</a>\"</p>",
    "publish_time": "2022-09-26 14:18:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "爱数正式开源认知智能开发框架KWeaver",
    "url": "https://www.infoq.cn/article/MsmsmreSCgiWz1SeJS7Q",
    "summary": "<p></p><p>InfoQ获悉，9月16日，爱数正式开源了认知智能开发框架KWeaver。</p><p></p><p>KWeaver名称中，K代表的是Knowledge知识，Weaver代表编织者，意为将所有领域知识编织在一起，从而实现领域认知智能。</p><p></p><p>据悉，KWeaver脱胎于爱数认知智能框架AnyDATA Framework 2，具有快速的开发能力、全面的开放性、高性能等特性，以成熟的数据知识化方法论和系列认知智能应用组件赋能数据科学家和应用开发者，以此降低领域认知智能应用开发的复杂度与人才门槛。</p><p></p><p>GitHub 项目地址：<a href=\"https://github.com/AISHU-Technology/kweaver\">https://github.com/AISHU-Technology/kweaver</a>\"</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8d/b9/8d167245fb0b603f3e537dbf2e4384b9.png\" /></p><p></p><p>具体而言，KWeaver面向数据科学家、应用开发者、领域专家三类用户提供三种能力。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/12/e0/122c19b0ddca344536bf4884b034cee0.png\" /></p><p></p><p></p><p>快速的开发能力：KWeaver提供可视化的知识网络工作台，可视化的认知智能应用开发调试工具，丰富的数据加工及模型训练工具，以及所见即所得的API文档；全面的开放性：&nbsp;KWeaver源代码是开源且技术透明的，这意味着参与项目的用户可以查看全部技术细节，同时还支持多元异构数据源对接，提供SDK兼容更多的第三方知识抽取模型，提供&nbsp;API 和 Web 组件的整合方式；高性能：KWeaver的开发基于云原生技术，提供横向扩展能力，采用分布式的计算引擎实现海量数据接入的处理能力。</p><p></p><p>通过KWeaver实现领域认知驱动涉及到两个关键步骤。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/15/64/1510eb05c9d2f35d2b53199d6b218c64.png\" /></p><p></p><p>第一是领域知识获取。首先建立领域认知模型，基于模型进行数据加工抽取知识，对于不同来源的数据，加工的方式也会有很大不同，部分数据可以直接抽取，部分数据则需要通过深度学习模型，例如文本知识。加工完成后生成领域知识网络。</p><p></p><p>第二是领域知识利用。基于知识网络进行认知智能应用的开发，利用数据科学的方法开发模型，并将模型应用到推理、理解等具体场景。显然，在数据领域认知智能并非替代原有的深度学习或机器学习，而是将两者结合发挥更大的作用。KWeaver也会内置很多基于深度学习的知识抽取模型。</p><p></p><p>本次正式开源前，KWeaver已随AnyDATA经历了近3年的孵化和打磨，结合AnyShare、AnyRobot、AnyFabric等产品的能力实现诸多创新。如结合AnyShare在知识管理方面形成行业知识卡片、行业标签、知识搜索等应用；结合AnyRobot在智能运维方面实现可持续进化的运维知识库，以及运维中小概率事件的故障定位和根因分析；结合AnyFabric帮助用户更好地编织数据，以资产图谱的形式让用户更直观全面的观察和分析业务能力、数据质量等等。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/60/e9/6064d3e20d6613ba861ac5b8e7de7ee9.png\" /></p><p></p>",
    "publish_time": "2022-09-26 15:39:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris在思必驰的应用优化实践：海量语音通话数据下，实时、离线一体的数仓架构设计实践",
    "url": "https://www.infoq.cn/article/rLwih7JefQQW11sgv7HL",
    "summary": "<p></p><h2>业务背景</h2><p></p><p></p><p>思必驰是一家对话式人工智能平台公司，拥有全链路的智能语音语言技术，致力于成为全链路智能语音及语言交互的平台型企业，自主研发了新一代人机交互平台 DUI 和人工智能芯片 <a href=\"https://www.infoq.cn/article/rVlBAE9S1X3Bb_dfNFjU\">TH1520</a>\"，为车联网、IoT 及政务、金融等众多行业场景合作伙伴提供自然语言交互解决方案。</p><p></p><p>思必驰于 2019 年首次引入 <a href=\"https://doris.apache.org/\">Apache Doris</a>\" ，基于 Apache Doris 构建了实时与离线一体的数仓架构。相对于过去架构，Apache Doris 凭借其灵活的查询模型、极低的运维成本、短平快的开发链路以及优秀的查询性能等诸多方面优势，如今已经在实时业务运营、自助/对话式分析等多个业务场景得到运用，满足了设备画像/用户标签、业务场景实时运营、数据分析看板、自助 BI、财务对账等多种数据分析需求。在这一过程中我们也积累了诸多使用上的经验，在此分享给大家。</p><p></p><h2>架构演进</h2><p></p><p></p><p>早期业务中，离线数据分析是我们的主要需求，近几年，随着业务的不断发展，业务场景对实时数据分析的要求也越来越高，早期数仓架构逐渐力不从心，暴露出很多问题。为了满足业务场景对查询性能、响应时间及并发能力更高的要求，2019年正式引入 Apache Doris 构建实时离线一体的数仓架构。</p><p></p><p>以下将为大家介绍思必驰数仓架构的演进之路，早期数仓存在的优缺点，同时分享我们选择 Apache Doris 构建新架构的原因以及面临的新问题与挑战。</p><p></p><h4>早期数仓架构及痛点</h4><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/64/c7/64889d836c716221494b6bc21b74a3c7.png\" /></p><p></p><p>如上图所示，早期架构基于 Hive +Kylin 来构建离线数仓，实时数仓架基于 Spark+MySQL 来构建实时分析数仓。</p><p></p><p>我们业务场景的数据源主要分为三类，业务数据库如 MySQL，应用系统如 K8s 容器服务日志，还有车机设备终端的日志。数据源通过 MQTT/HTTP 协议、业务数据库 Binlog 、Filebeat日志采集等多种方式先写入 Kafka。</p><p></p><p>在早期架构中，数据经 Kafka 后将分为实时和离线两条链路，首先是实时部分，实时部分链路较短，经过 Kafka 缓冲完的数据通过 Spark 计算后放入 MySQL 中进行分析，对于早期的实时分析需求，MySQL 基本可以满足分析需求。而离线部分则由 Spark 进行数据清洗及计算后在 Hive 中构建离线数仓，并使用 Apache Kylin 构建 Cube，在构建 Cube 之前需要提前做好数据模型的的设计，包括关联表、维度表、指标字段、指标需要的聚合函数等，通过调度系统进行定时触发构建，最终使用 HBase 存储构建好的 Cube。</p><p></p><p>早期架构的优势：</p><p></p><p>早期架构与 Hive 结合较好，无缝对接 Hadoop 技术体系。离线数仓中基于 Kylin 的预计算、表关联、聚合计算、精确去重等场景，查询性能较高，在并发场景下查询稳定性也较高。</p><p></p><p>早期架构解决了当时业务中较为紧迫的查询性能问题，但随着业务的发展，对数据分析要求不断升高，早期架构缺点也开始逐渐凸显出来。</p><p></p><p>早期架构的痛点：</p><p></p><p>依赖组件多。Kylin 在 2.x、3.x 版本中强依赖 Hadoop 和 HBase ，应用组件较多导致开发链路较长，架构稳定性隐患多，维护成本比很高。Kylin 的构建过程复杂，构建任务容易失败。Kylin 构建需要进行打宽表、去重列、生成字典，构建 Cube 等如果每天有 1000-2000 个甚至更多的任务，其中至少会有 10 个甚至更多任务构建失败，导致需要大量时间去写自动运维脚本。维度/字典膨胀严重。维度膨胀指的是在某些业务场景中需要多个分析条件和字段，如果在数据分析模型中选择了很多字段而没有进行剪枝，则会导致 Cube 维度膨胀严重，构建时间变长。而字典膨胀指的是在某些场景中需要长时间做全局精确去重，会使得字典构建越来越大，构建时间也会越来越长，从而导致数据分析性能持续下降。数据分析模型固定，灵活性较低。在实际应用过程中，如果对计算字段或者业务场景进行变更，则要回溯部分甚至全部数据。不支持数据明细查询。早期数仓架构是无法提供明细数据查询的，Kylin 官方给的解决方法是下推给 Presto 做明细查询，这又引入了新的架构，增加了开发和运维成本。</p><p></p><h4>架构选型</h4><p></p><p></p><p>为解决以上问题，我们开始探索新的数仓架构优化方案，先后对市面上应用最为广泛的 Apache Doris、Clickhouse 等 OLAP 引擎进行选型调研。相较于 ClickHouse 的繁重运维、各种各样的表类型、不支持关联查询等，结合我们的 OLAP 分析场景中的需求，综合考虑，Apache Doris 表现较为优秀，最终决定引入 Apache Doris 。</p><p></p><h4>新数仓架构</h4><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f5/86/f53dc7fa4cc5c77f9d11e3ab6d72c986.png\" /></p><p></p><p>如上图所示，我们基于 Apache Doris 构建了实时+离线一体的新数仓架构，与早期架构不同的是，实时和离线的数据分别进行处理后均写入 Apache Doris 中进行分析。</p><p></p><p>因历史原因数据迁移难度较大，离线部分基本和早期数仓架构保持一致，在Hive上构建离线数仓，当然完全可以在Apache Doris 上直接构建离线数仓。</p><p></p><p>相对早期架构不同的是，离线数据通过 Spark 进行清洗计算后在 Hive 中构建数仓，然后通过 Broker Load 将存储在 Hive 中的数据写入到 Apache Doris 中。这里要说明的， Broker Load 数据导入速度很快，天级别 100-200G 数据导入到 Apache Doris 中仅需要 10-20 分钟。</p><p></p><p>实时数据流部分，新架构使用了 Doris-Spark-Connector 来消费 Kafka 中的数据并经过简单计算后写入 Apache Doris 。从架构图所示，实时和离线数据统一在 Apache Doris 进行分析处理，满足了数据应用的业务需求，实现了实时+离线一体的数仓架构。</p><p></p><h4>新架构的收益</h4><p></p><p></p><p>极简运维，维护成本低，不依赖 Hadoop 生态组件。Apache Doris 的部署简单，只有 FE 和 BE 两个进程， FE 和 BE 进程都是可以横向扩展的，单集群支持到数百台机器，数十 PB 的存储容量，并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。在使用 Doris 三年时间中花费的运维时间非常少，相比于基于 Kylin 搭建的早期架构，新架构花费极少的时间去做运维。链路短，开发排查问题难度大大降低。基于 Doris 构建实时和离线统一数仓，支持实时数据服务、交互数据分析和离线数据处理场景，这使得开发链路变的很短，问题排查难度大大降低。支持 Runtime 形式的 Join 查询。Runtime 类似 MySQL 的表关联，这对数据分析模型频繁变更的场景非常友好，解决了早期结构数据模型灵活性较低的问题。同时支持 Join、聚合、明细查询。解决了早期架构中部分场景无法查询数据明细的问题。支持多种加速查询方式。支持上卷索引，物化视图，通过上卷索引实现二级索引来加速查询，极大的提升了查询响应时间。支持多种联邦查询方式。支持对 Hive、Iceberg、Hudi 等数据湖和 MySQL、Elasticsearch 等数据库的联邦查询分析。</p><p></p><h4>问题和挑战</h4><p></p><p></p><p>在建设新数仓架构过程中，我们遇到了一些问题：</p><p></p><p>高并发场景对 Apache Doris 查询性能存在一定影响。我们分别在 Doris 0.12 和 Doris 1.1版本上进行测试，同一时间同样的 SQL，10 并发和 50 并发进行访问，性能差别较大。在实时写入场景中，当实时写入的数据量比较大时，会使得 IO 比较密集，导致查询性能下降。大数据量下字符串精确去重较慢。目前使用的是 count distinct 函数、Shuffle 和聚合算子去重，此方式算力比较慢。当前业内常见的解决方法一般是针对去重列构建字典，基于字典构建 Bitmap 索引后使用 Bitmap 函数去重。目前 Apache Doris 只支持数字类型的 Bitmap 索引，具有一定的局限性。</p><p></p><h2>业务场景的应用</h2><p></p><p></p><p>Apache Doris 在思必驰最先应用在实时运营业务场景以及自助/对话式分析场景，本章节将介绍两个场景的需求及应用情况。</p><p></p><h4>实时运营业务场景</h4><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/eb/01/eb63c3995b09eae15e3308265332a301.png\" /></p><p></p><p>首先是实时运营业务场景，如上图所示，实时运营业务场景的技术架构和前文所述的新版数仓架构基本一致：</p><p></p><p>数据源：数据源新版架构图中一致，包括 MySQL 中的业务数据，应用系统埋点数据以及设备和终端日志。数据导入：离线数据导入使用 Broker Load，实时数据导入使用 Doris-Spark-Connector 。数据存储与开发：几乎所有的实时数仓全部在 Apache Doris 构建，有一部分离线数据放在 Airflow 上执行 DAG 跑批任务。数据应用：最上层是业务侧提出的业务分析需求，包括大屏展示，数据运营的实时看板、用户画像、BI 看板等。</p><p></p><p>在实时运营业务场景中，数据分析的需求主要有两方面：</p><p></p><p>由于实时导入数据量比较大，因此对实时数据的查询效率要求较高在此场景中，有 20+ 人的团队在运营，需要同时开数据运营的看板，因此对实时写入的性能和查询并发会有比较高的要求。</p><p></p><h4>自助/对话式分析场景</h4><p></p><p></p><p>除以上之外，Apache Doris 在思必驰第二个应用是自助/对话式分析场景。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a0/71/a000c6de9b15d7bbf32a31ddc7ecb971.png\" /></p><p></p><p>如上图所示，在一般的 BI 场景中，用户方比如商务、财务、销售、运营、项目经理等会提出需求给数据分析人员，数据分析人员在 BI 平台上做数据看板，最终把看板提供给用户，用户从 BI 看板上获取所需信息，但是有时候用户想要查看明细数据、定制化的看板需求，或者在某些场景需做任意维度的上卷或者下钻的分析，一般场景下 BI 看板是不支持的的，基于以上所述用户需求，我们打造了自助对话式 BI 场景来解决用户定制化的需求。</p><p></p><p>与一般 BI 场景不同的是，我们将自助/对话式 BI 场景从数据分析人员方下沉到用户方，用户方只需要通过打字，描述数据分析的需求。基于我们公司自然语言处理的能力，自助/对话式 BI 场景会将自然语言转换成SQL，类似 NL2SQL 技术，需要说明的是这里使用的是定制的自然语言解析，相对开源的 NL2SQL 命中率高、解析结果更精确。当自然语言转换成 SQL 后，将  SQL 给到 Apache Doris 查询得到分析结果。由此，用户通过打字就可以随时查看任意场景下的明细数据，或者任意字段的上卷、下钻。</p><p></p><p>相比 Apache Kylin、Apache Druid 等预计算的 OLAP 引擎，Apache Doris 符合以下几个特点：</p><p></p><p>查询灵活，模型不固定，支持自由定制场景。支持表关联、聚合计算、明细查询。响应时间要快速。</p><p></p><p>因此我们很顺利的运用 Apache  Doris 实现了自助/对话式分析场景。同时，自助/对话式分析在我们公司多个数据分析场景应用反馈非常好。</p><p></p><h2>实践经验</h2><p></p><p></p><p>基于上面的两个场景，我们使用过程当中积累了一些经验和心得，分享给大家。</p><p></p><h4>数仓表设计</h4><p></p><p></p><p>千万级(量级供参考，跟集群规模有关系)以下的数据表使用 Duplicate 表类型，Duplicate 表类型同时支持聚合、明细查询，不需要额外写明细表。当数据量比较大时，使用 Aggregate 聚合表类型，在聚合表类型上做上卷索引，使用物化视图优化查询、优化聚合字段。由于 Aggregate 表类型是预计算表，会丢失明细数据，如有明细查询需求，需要额外写一张明细表。当数据量又大、关联表又多时，可用 ETL 先写成宽表，然后导入到 Doris，结合 Aggregate 在聚合表类型上面做优化，也可以使用官方推荐Doris 的<a href=\"https://doris.incubator.apache.org/zh-CN/docs/dev/advanced/join-optimization/doris-join-optimization\"> Join 优化</a>\"。</p><p></p><h4>写入</h4><p></p><p></p><p>通过 Spark Connector 或 Flink Connector 替代 Routine Load： 最早我们使用的是 Routine Load 实时写入 BE 节点， Routine Load 的工作原理是通过 SQL 在 FE 节点起一个类似于 Task Manager 的管理，把任务分发给 BE 节点，在 BE 节点起 Routine Load 任务。在我们实时场景并发很高的情况下，BE 节点 CPU 峰值一般会达到 70% 左右，在这个前提下，Routine Load 也跑到 BE 节点，将严重影响 BE 节点的查询性能，并且查询 CPU 也将影响 Routine Load 导入， Routine Load 就会因为各种资源竞争死掉。面对此问题，目前解决方法是将 Routine Load  从 BE 节点拿出来放到资源调度上，用 Doris-Spark/Flink-Connector 替换 Routine Load。当时 Doris-spark-Connector 还没有实时写入的功能，我们根据业务需求进行了优化，并将方案贡献给社区。通过攒批来控制实时写入频率：当实时写入频率较高时，小文件堆积过多、查询 IO 升高，小文件排序归并的过程将导致查询时间加长，进而出现查询抖动的情况。当前的解决办法是控制导入频次，调整 Compaction 的合并线程、间隔时间等参数，避免 Tablet 下小文件的堆积。</p><p></p><h4>查询</h4><p></p><p></p><p>增加 SQL 黑名单，控制异常大查询。个别用户在查询时没有加 where 条件，或者查询时选择的时间范围较长，这种情况下 BE 节点的 SQL 会把磁盘的负载和 CPU 拉高，导致其他节点的 SQL 查询变慢，甚至出现 BE 节点宕机的情况。目前的解决方案是使用 SQL 黑名单禁止全表及大量分区实时表的查询。使用 SQL Cache 和 SQL Proxy 实现高并发访问。同时使用 SQL Cache 和 SQL Proxy 的原因在于，SQL Cache的颗粒度到表的分区，如果数据发生变更， SQL Cache 将失效，因此 SQL Cache 缓存适合数据更新频次较低的场景（离线场景、历史分区等）。对于数据需要持续写到最新分区的场景， SQL Cache 则是不适用的。当 SQL Cache 失效时 Query 将全部发送到 Doris 造成重复的 Runtime 计算，而 SQL Proxy 可以设置一秒左右的缓存，可以避免相同条件的重复计算，有效提高集群的并发。</p><p></p><h4>存储</h4><p></p><p></p><p>使用 SSD 和 HDD 做热温数据存储周期的分离，近一年以内的数据存在 SSD，超过一年的数据存在 HDD。Apache Doris 支持对分区设置冷却时间，但只支持创建表分区时设置冷却的时间，目前的解决方案是设置自动同步逻辑，把历史的一些数据从 SSD 迁移到 HDD，确保 1年内的数据都放在 SSD 上。</p><p></p><h4>升级</h4><p></p><p></p><p>升级前一定要备份元数据，也可以使用新开集群的方式，通过 Broker 将数据文件备份到 S3 或 HDFS 等远端存储系统中，再通过备份恢复的方式将旧集群数据导入到新集群中。</p><p></p><h4>升级前后性能对比</h4><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6b/b0/6b1e01e678089e7a22dyya18e542eab0.png\" /></p><p></p><p>思必驰最早是从 0.12 版本开始使用 Apache Doris 的，在今年我们也完成了从 0.15 版本到最新<a href=\"https://xie.infoq.cn/article/0f316cdd71a454f81da963a4d\"> 1.1 版本</a>\"的升级操作，并进行了基于真实业务场景和数据的性能测试。</p><p></p><p>从以上测试报告中可以看到，总共 13 个测试 SQL 中，前 3 个 SQL 升级前后性能差异不明显，因为这 3 个场景主要是简单的聚合函数，对 Apache Doris 性能要求不高，0.15 版本即可满足需求。而在 Q4 之后的场景中 ，SQL 较为复杂，Group By 有多个字段、多个字段聚合函数以及复杂函数，因此升级新版本后带来的性能提升非常明显，平均查询性能较 0.15 版本提升 2-3 倍。由此，非常推荐大家去升级到 Apache Doris 最新版本。</p><p></p><h2>总结和收益</h2><p></p><p></p><p>Apache Doris 支持构建离线+实时统一数仓，一个 ETL 脚本即可支持实时和离线数仓，大大缩短开发周期，降低存储成本，避免了离线和实时指标不一致等问题。Apache Doris 1.1.x 版本开始全面支持向量化计算，较之前版本查询性能提升 2-3 倍。经测试，Apache Doris 1.1.x 版本的查询性能已接近 ClickHouse。功能强大，不依赖其他组件。相比 Apache Kylin、Apache Druid、ClickHouse 等，Apache Doris 不需要引入第 2 个组件填补技术空档。Apache Doris 支持聚合计算、明细查询、关联查询，当前思必驰超 90% 的分析需求已移步 Apache Doris实现。 得益于此优势，技术人员需要运维的组件减少，极大降低运维成本。易用性极高，支持 MySQL 协议和标准 SQL，大幅降低用户学习成本。</p><p></p><h2>未来计划</h2><p></p><p></p><p>Tablet 小文件过多的问题。Tablet 是 Apache Doris 中读写数据最小的逻辑单元，当 Tablet 小文件比较多时会产生 2 个问题，一是 Tablet 小文件增多会导致元数据内存压力变大。二是对查询性能的影响，即使是几百兆的查询，但在小文件有几十万、上百万的情况下，一个小小的查询也会导致 IO 非常高。未来，我们将做一个 Tablet 文件数量/大小比值的监控，当比值在不合理范围内时及时进行表设计的修改，使得文件数量和大小的比值在合理的范围内。支持基于 Bitmap 的字符串精确去重。业务中精确去重的场景较多，特别是基于字符串的 UV 场景，目前 Apache Doris 使用的是 Distinct 函数来实现的。未来我们会尝试的在 Apache Doris 中创建字典，基于字典去构建字符串的 Bitmap 索引。Doris-Spark-Connector 流式写入支持分块传输。Doris-Spark-Connector 底层是复用的 Stream Load，工作机制是攒批，容易出现两个问题，一是攒批可能会会出现内存压力导致 OOM，二是当Doris-Spark-Connector 攒批时，Spark Checkpoint 没有提交，但 Buffer 已满并提交给 Doris，此时 Apacche Doris 中已经有数据，但由于没有提交 Checkpoint，假如此时任务恰巧失败，启动后又会重新消费写入一遍。未来我们将优化此问题，实现 Doris-Spark-Connector 流式写入支持分块传输。</p><p></p><h4>作者介绍</h4><p></p><p></p><p>赵伟，思必驰大数据高级研发，10年大数据开发和设计经验，负责大数据平台基础技术和OLAP分析技术开发。社区贡献：Doris-spark-connector 的实时读写和优化。</p>",
    "publish_time": "2022-09-26 16:31:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动现象级 App 十年成长史，移动端基础建设与组织演进之路 | 卓越技术团队访谈录",
    "url": "https://www.infoq.cn/article/2vewEZczdQhIiRsDv7bC",
    "summary": "<p></p><p>采访嘉宾 | 孙念、杨萍</p><p>采访 | Tina、闫园园</p><p>编辑 | 闫园园</p><p></p><p>2012 年，<a href=\"https://www.infoq.cn/article/xIV5VfJ4CU1pPHtaKMNz\">字节跳动</a>\"成立，到今年，正好是它的第十个年头。虽然在年龄上，这家公司还非常年轻，但从影响力上来看，它早已成长为移动互联网时代的新兴势力。</p><p></p><p>现在外界给它贴上了很多标签，其中令人印象深刻的无外乎：庞大、低调。的确，字节跳动鲜有发声，这也使得它与一众互联网巨头相比，多了几分神秘的色彩。不过，如果要探寻字节成功的原因，创始人张一鸣的一句话或许能成为答案：</p><p></p><p></p><blockquote>字节跳动的核心竞争力，直接来说是我们的产品，产品背后是我们的技术系统，技术系统背后是我们的团队和文化。</blockquote><p></p><p></p><p>如今，再过多去渲染字节的产品稍显多余，毕竟对于开发者群体来说，目光也更多的聚焦在后半句——持续、丰富的 App 研发的背后究竟有着怎样的技术支撑。在本期访谈中，InfoQ 有幸采访到了字节 AppInfra 团队。他们是移动端基础技术的全球化研发团队，支持的产品包括但不限于抖音、今日头条、西瓜视频、飞书等，在性能优化、前沿技术探索等方向上都有深入的研究。</p><p></p><p>在与他们的交谈过程中，我们沿着团队的成长轨迹探寻到了字节跳动移动端基础设施的构建之路，以及支撑如此冗杂的工作背后的团队精神和文化，现整理此文，希望能对读者有所启迪。</p><p></p><p></p><h2>现象级 App 爆红带来团队成长</h2><p></p><p></p><p>2018 年，抖音日活破 2 亿，一跃成为中国头部 App 之一。这一年，媒体将之报道为“字节跳动成为巨头，与 BAT 们正面交锋的一年”，自那时候起，字节业务开始触及多个领域，扩大旗下 App 矩阵，其中，多款 App 获得了成功。在外界看来，这是字节的高光时刻，但人们看不到的是，彼时字节的内部也需要开始做出转变来应对业务的高速发展。</p><p></p><p>以本文的主角 AppInfra 团队举例，其前身可追溯到字节跳动的基础技术部门。2017 年之前，基础技术部门大约只有十几个人规模，其他都是纯客户端安卓、iOS；2019 年之后，整个字节包括抖音的业务都起来了，需要支持的业务越来越多，“于是我们吸纳了不少领域内的专家，包含 Flutter 社区的关键专家，一方面帮助团队支撑起平台职能；另一方面也开始投入做一些技术研究”，杨萍谈到。在此过程中，基础技术部门逐渐孵化出现在的 AppInfra 团队。</p><p></p><p>业务多，场景也随之多样化，对于技术团队来说，堆人头、拼时间势必不是长久之计。这种情况下，整个公司开始更加注重技术复用。对于 AppInfra 来说，他们的主要职责，正是将场景中的通用技术能力抽象出来，加以建设，沉淀出通用工具再落地到业务中去。AppInfra 的应用基建过程覆盖了整个开发周期中的各个环节，涉及到应用开发的整个环境，包括需求管理、代码审核、测试、打包部署、性能优化等。另外，团队还需要判断和引入新技术，比如由于业务扩张需要更好的跨平台技术方案，AppInfra 团队就促进了 Flutter 在字节跳动新业务上的落地。</p><p></p><p>发展至今，整个 AppInfra 团队的规模已经逐步扩大，并且进一步细化为了四个子团队——AppHealth 团队、Research Center 团队、端智能团队以及 DevOps 团队。AppInfra 团队也有了自己的使命：提升移动端效能、性能质量、产品核心指标和智能化水平，并关注新技术研究与落地。</p><p></p><p>那么为什么要分化出这四个方向呢？对此，另一位受访嘉宾孙念总结为团队的定位是为支持业务而生。他表示，与其说团队的架构是自顶而上规划而出，倒不如说是随着业务的需求慢慢生长出自己的形状来的贴切。“拿 AppHealth 团队来说，当应用达到一定量级，性能、稳定性等指标会直接影响用户产品体验。随着业务规模持续增长，这部分问题一旦开始凸显，团队也会加大这些方面的投入。”由此也可见，组织架构的演变其实是和技术、业务息息相关的。当业务改进时，新技术就会通过架构的分支与原有技术连接在一起，共同工作，更高效率的解决业务需求。</p><p></p><p>如果把组织架构的成长分为支持业务而生、赋能业务中去、引领业务发展三个阶段的话，目前看来，AppInfra 团队已经同时迈入了第二阶段与第三阶段。但也有很多人好奇在字节跳动里，这样一支通用技术建设团队是如何与业务团队进行有效协作的，对于这个问题，杨萍是这样回答 InfoQ 的：</p><p></p><p>第一，明确职责是前提。在字节内部，双方团队的职责是比较明确的：业务团队主要是以需求迭代和业务交付为目标，他们的侧重点在于保障需求侧的交付；而 Infra 团队主要是以技术方向为目标，侧重点在于技术的实现情况以及解决技术难点。</p><p></p><p>第二，保持紧密的协作关系。在业务团队提出需求后，背后的技术或者中台团队会迅速调整去做支撑，同时技术团队还充当业务团队的智囊团角色，以应对业务团队可能会面临的各种突发技术状况。另一方面，业务团队在长期需求迭代过程中也会有比较多的业务视角经验，他们会将这种经验传导到技术团队中，从而沉淀出典型的解决方案或者技术上的经验，赋能给更多业务。“过去两三年，我们主要是解决业务的痛点，后面的思路是走在业务前面，提前布局好技术方案。”</p><p></p><p></p><h2>性能优化已成移动端重要议题</h2><p></p><p></p><p>从 2007 年兴起，移动端已经经历了十几年的发展。目前来看，移动端的技术栈，可以用百家争鸣来形容：</p><p></p><p>原生 App 技术栈：安卓平台的 Java 技术栈，iOS 平台的 Object-C 技术栈或 Swift 技术栈。混合 App 技术栈：典型代表有 PhoneGap、Cordova、Ionic 等框架。跨平台 App 技术栈：典型代表有 React Native、Xamarin、Flutter 等。</p><p></p><p>对于<a href=\"https://www.infoq.cn/article/SX9ZlzNrYsElIRYoHUqD\">字节</a>\"来说，移动端的主要技术栈也无外乎如此，“我认为，近几年来看移动端开发的架构或者框架演进过程并没有发生质的变化，技术栈的出现也属于渐进式变化，包括底层编程语言以及 UI 层面。”杨萍介绍道。同时她也谈到一个新的趋势，相较现如今比较成熟的技术栈来说，大家开始更关注 App 体验和性能的优化。对于拥有旗下多款 App 的字节来说，这一趋势显然更需要提起重视。也因此，其中一部分重担落到了 AppInfra 子团队 AppHealth 团队身上，团队需要提升全产品线的性能、稳定性和工程效率。</p><p></p><p>作为 AppHealth 团队的负责人，孙念毕业后的第一份工作是在高通集团，“当时主要是做安卓底层和芯片比较紧密的系统软件的优化”。后来随着手机厂商的快速发展，他转而加入手机厂商做安卓系统的相关优化。如今，再总结这两段工作经历，孙念感触颇深，他认为自己的工作层面从偏底层转而到应用层面，明显对自己的技术视野和深度上有相当大的帮助。</p><p></p><p>2019 年，孙念加入字节跳动，他回忆当时摆在自己面前的第一个任务就是组建团队。最开始，团队成员中拥有做监控能力的开发者比较多，“所以，当时的我们主要是做性能监控，一方面是加强线上的归因能力，另一方面是加强线下深入分析的相关工具建设。”</p><p></p><p>他进一步介绍，对于线上和线下来说，其实想要监控的指标和发现的问题其实是一致的，但更希望能把问题在线下就处理掉，以避免线上造成更大范围的损失。因此，线上层面，团队着重制订更好、更合理的指标，同时，建立整套问题的报警、响应、解决流程，来及时发现、解决问题。</p><p></p><p>线下层面，则从性能角度建立了较为精细的防劣化机制。所谓劣化，是指在业务迭代和架构优化过程中，遇到了很多不符合架构设计的代码，而导致架构出现劣化。“比如，我们会去定制 Android 手机 ROM，通过固定 ROM 里面的参数控制硬件的波动，来减小数据测量的误差，来达到更加精准防劣化，”通过建立精细化防劣化机制及时发现问题，并进行拦截和消费，来保持项目整体架构持续正向演进。</p><p></p><p>在解决性能监控问题的同时，AppHealth 团队也开始在全球各地吸纳更多单点技术领域和方向的人才，开启更多通用技术能力的探索。其中，“编译器成为我们重要发力点之一”，孙念介绍，团队在链接器上也做了一些特化工作。</p><p></p><p>对于工具链的探索主要有两个原因：第一，工具链的改造能直接缩小包体积或者提升应用性能，但对于开发者来说只需要替换工具链，而没有其他感知；第二，移动端工具链的潜力挖掘的还不够充分，可做的事情比较多。</p><p></p><p>目前，AppInfra 团队在包体积方面也已经得到了初步效果。孙念举例，团队通过定制自己的链接器替换苹果默认链接器，可以做到消除全局重复代码，从而将 iOS 二进制文件优化百分之十几左右。“这块目前业内还是做的比较少的，之后我们也会将实践做更详细的分享。”</p><p></p><p></p><h2>自动化测试技术探索</h2><p></p><p></p><p>如果说性能优化是目前大厂们做产品势在必行的大事之一，那么与此相对，四个子团队中的 Research Center 团队的工作似乎与业务相去甚远，毕竟从介绍来看，Research Center 主要负责前沿技术创新，以及与一些学术机构的合作。然而在实际中情况却恰恰相反，对于字节来说，Research Center 团队的存在不仅不是“可有可无”，甚至已经成为 AppInfra 团队更好地去服务业务发展的重要一棋。</p><p></p><p>“我们的理念并不是做一个单纯的学术研究部门，我们做的是技术研究，终归还是要讲究去落地的。字节提供了非常多的有挑战的技术场景，而真正解决实际的技术难题，是互联网企业里的研究性部门的使命和初衷。”杨萍谈道。</p><p></p><p>与孙念的经历相似，Research Center 团队的负责人杨萍同样有着丰富工作经验。研究生毕业后，她选择供职于英特尔，工作内容主要涉及与手机摄像头相关的软件，且以应用层为主。2018 年，杨萍加入字节，最初在 AILab 字节跳动人工智能实验室工作负责 QA 技术团队的搭建。2020 年，杨萍所在的团队开始转型技术方向探索，更名为 Quality Lab，Quality Lab 的主要职责是探索一些前沿技术以进一步提升测试能力。“这个过程基本上延续到今年年初，我们的研究方向以质量测试技术、效率和自动化为主。”</p><p></p><p>同时她也坦言，在这个过程中，团队面临了不少挑战，也做了很多变化去应对：第一，组织架构角度，除了算法工程师以外，加入更多客户端、服务端的工程师来做产品化支撑；第二，拓展与高校、机构等深度的交流和学术合作。</p><p></p><p>“未来，组织定位也会发生一些变化，变化主要在于 DevOps 或者极致的性能与体验的优化。”随着组织定位的进一步清晰，在公司战略调整下， AppInfra 正式成立了 Research Center 团队。“Research Center 会持续跟进加深一些学术合作，继续前沿技术的探索；另外，我们也会继续在自动化测试上的投资；最后，我们还尝试将主攻方向从端延伸到整个代码，去做针对程序方面的探索。”</p><p></p><p>那么对于 Research Center 来说，为什么会选择从自动化测试入手开展现有工作呢？杨萍介绍，这主要在于测试技术在开发过程中属于比较关键的节点，能够在研发、QA 甚至其他决策方等多个方面为项目带来明显的优化效果。</p><p></p><p>越是在项目、业务众多的情况下，开发者团队越能体会到代码审查的必要性，可以说不做代码审查基本相当于背着定时炸弹前进，时刻有爆炸的危险。而在代码审查中，测试是关键的一环。一般来讲，单元测试成本最少，代价也最小，“但对于开发者来说，需要面对快节奏的迭代任务，多半情况下可能并没有多余的精力去维护好单测。”因此，自动化测试的重新定义及探索也显得尤为迫切。</p><p></p><p>“2018 年时，字节并没有所谓的测试平台的概念”，杨萍回顾了字节整个自动化测试平台建设，“那时候主要是执行一些 UI 自动化测试任务，并且早期服务端测试还是空白的，主要依靠客户端测试同学去兼顾。”随着业务规模的扩大，字节设置了专门的服务端 QA 的角色，来填补相应技术层面的空白。接口测试则利用对应的平台去提供编写、执行，以及测试报告的生成等功能。</p><p></p><p>同时，性能稳定逐渐被重视，字节也随之设置了单独的性能测试平台来提供相应支撑。另外，底层测试技术也一直在探索的过程中。发展到今天，字节的整体自动化测试平台发，既具有了从客户端到服务端广度，又具有从功能到性能的深度，能够从更加全面的维度为项目及业务做支撑。</p><p></p><p>当然，建设自动化测试平台中，Research Center 团队也在做观察及调研，他们发现市面上针对客户端测试的自动化测试工具有很多，比较典型的有 Facebook 研发的 Sapienz 以及 Android 自带的随机测试工具 Monkey 等，但其中对 iOS 系统的软件测试工具基本上没有。“2019 年我们调研了市面上能搜到的相关工具，但是字节的业务类型多，不止有头条、抖音，还有飞书等一些其它类型的应用。市面的测试工具，不足以支撑我们所有的业务。整体评估过后，我们决定自己探索这部分问题。”杨萍介绍。</p><p></p><p>因此，2019 年，字节在自动测试方面加大投入，研发了智能化测试服务 Fastbot。杨萍进一步谈道，Fastbot 最核心的技术本质上是如何在海量的数据中更好、更快的遍历完整个测试地图，“当我们抽象出来这个问题之后，就可以在低层去做多端或者 OS 层面的兼容。”这也意味着 &nbsp;Fastbot 不仅可以支持安卓，还能支持 iOS，甚至做到了支持跨端框架。</p><p></p><p>当然，对于新技术的探索，点滴成功的背后无疑是多次的失败，Research Center 团队也并不例外，“自动化的工具，过程中会有非常多的技术探索，也会面临很多失败，但这些失败其实并不足以改变我们的初衷，我们希望用更多的新技术去解决业务中的实际测试问题。”</p><p></p><p></p><h2>团队建设，人才是根本性问题</h2><p></p><p></p><p>本次访谈中，InfoQ 照例与两位团队的负责人聊到了他们眼中团队亟需解决的问题。令人印象深刻的是，除了谈及技术以及业务上的规划，本次两位负责人还不约而同谈到了他们对于“人”的问题的理解。</p><p></p><p>对 AppInfra 团队来说，目前还需要进一步建设好人才梯队。“最开始，我们只有聚焦有性能监控技术的人才上，后来逐渐补齐了一些做深入分析，以及通用优化能力的人员。还包括安卓系统，甚至在 Linux 内核上有经验的人才，他们可以给我们提供更具有广度和深度的技术视野，同时我们也增加了一些在单点的技术上积累非常深厚的程序员，比如安卓 Hook。2020 年左右，我们也开始去找一些合适的编译器领域的人才，并逐步建立起来了这样一支队伍。”</p><p></p><p>孙念认为团队需要支撑全球化的业务，那么需要更多不同技术领域的、很出色的工程师，“我们还会持续再补充一些优秀同学进来。”同时他也谈到，虽然字节在海外的知名度已大有提高，但与 Google、Facebook 等国际巨头的影响力差距仍然存在，这是需要承认，并不断努力提高的一点。</p><p></p><p>而对于 Research Center 团队来说，杨萍谈到，他们目前最需要的则是正视目前潜在的人才结构化的风险。展开来讲，作为前沿技术探索领域的先行者，一方面，团队需要引进更多优秀的青年学者，去做更多技术探索，但也应该注意到，研究型人才的加入可能会削弱团队本身的工程和业务视角。因此，另一方面，她直言接下来团队引入更多来自业务领域的技术专家角色，使团队更好地传承以技术研究落地为主的使命和愿景。</p><p></p><p></p><h2>写在最后</h2><p></p><p></p><p>互联网大厂们缔造了不少 App 神话，字节跳动里的类似头条、抖音这种超大型项目也是其中之一。不同类型的企业，其产品差异很大，都需要根据各自的业务情况决定自己的实现和演进方案。</p><p></p><p>在字节跳动里打造 App，一开始技术方案和框架选择并不是靠就精心设计出来的，而是在实践的过程中随业务的发展和业务类型的改变不断调整出来的。随着公司的发展，<a href=\"https://www.infoq.cn/article/T7HCVlTeUCSroDRhT1FF\">字节跳动逐渐强调靠数据说话</a>\"，一些架构上面的选择，随后也有不少是由数据来支撑的。</p><p></p><p>结合这几年移动端技术发展趋势，虽然基础架构不会有特别大的调整，性能和体验上却在不断地优化。跟其他大厂一样，字节跳动的架构优化也经历了组件化、插件化的典型过程，这些优化的实施都是跟业务状态紧密相关的。比如为了更好的触达用户，像头条的极速版就会使用比较多的插件化，对初始的包体积更小。</p><p></p><p>总结来说，架构演进是跟业务强相关的，但作为移动端基础技术架构团队，保持对未来技术的关注也是必要的。</p><p></p><p>这其中包括应用框架和编程语言。AppInfra 团队有关注一些开发框架，比如谷歌推出的 Jetpack Compose ，以及苹果推出的 SwiftUI 响应式编程框架。</p><p></p><p>同时，一些编程语言已成为趋势，比如安卓里的 Kotlin，一方面在开发者中的占比越来越高，另一方面在做编译器优化中其产物、性能开销都是需要关注的点。另外是 Swift，比 Kotlin 更新的开发语言，孙念解释道，“我们需要关注开发者对它的关注程度，业务对它是否有计划。如果业务有比较明确的采用 Swift 的计划，我们也要为团队新建配套的基础设施，要及时跟上，以适应现在的开发趋势。”甚至是安卓系统里的 Rust，AppInfra 团队也需要考虑移动端的开发里 Rust 会不会成为一个可选项。</p><p></p><p>嘉宾介绍：</p><p></p><p>孙念，字节跳动基础架构 AppInfra 部门 AppHealth 团队负责人，专注于 App 性能、稳定性方向，带领团队通过对操作系统资源利用、虚拟机、编译器工具链，高性能基础库等方向的深度优化和建设各个核心指标全链路的监控体系和分析调试工具，协助字节各业务提升产品质量和体验。</p><p></p><p>杨萍，字节跳动基础架构 AppInfra 部门 Research Center 团队负责人，负责端架构基础研究与智能化能力建设，带领团队孵化并落地多个行业领先的端质量产品（Fastbot，精准测试），也对目前 AI 技术在大前端研发体系的应用有较多了解。</p><p></p><p>内容推荐</p><p></p><p>本文选自《<a href=\"https://www.infoq.cn/minibook/EQzDrPI1dT9G8V6alV1I\">中国卓越技术团队访谈录</a>\"》（2022 年第三季），本期精选了阿里达摩院数据库、得物、华润云、民生保险、众安保险、字节跳动 AppInfra 等技术团队在技术落地、团队建设方面的实践经验及心得体会。</p><p></p><p>《中国卓越技术团队访谈录》是 InfoQ 打造的重磅内容产品，以各个国内优秀企业的 IT 技术团队为线索策划系列采访，希望向外界传递杰出技术团队的做事方法 / 技术实践，让开发者了解他们的知识积累、技术演进、产品锤炼与团队文化等，并从中获得有价值的见解。</p><p></p><p>访谈录现开放长期报名通道，如果你身处传统企业经历了数字化转型变革，或者正在互联网公司进行创新技术的研发，并希望 InfoQ 可以关注和采访你所在的技术团队，可以添加微信：caifangfang_wechat，请注明来意及公司名称。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9f/9f66c137b5399abf731a2f0a1e31936c\" /></p><p></p><p></p><p></p>",
    "publish_time": "2022-09-26 20:34:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百万级 Topic，腾讯云的 Apache Pulsar 稳定性实践",
    "url": "https://www.infoq.cn/article/mx3jWtzU3eAqN2eP466H",
    "summary": "<p></p><p><a href=\"https://www.infoq.cn/article/z8NxTrFR8R1G11lkchyM\">Apache Pulsar</a>\" 作为云原生时代消息流系统，采用存储计算分离架构，支持大集群、多租户、百万级 Topic、跨地域数据复制、持久化存储、分层存储、高可扩展性等企业级和金融级功能。Apache Pulsar 提供了统一的消费模型，支持消息队列和流两种场景，既能为队列场景提供企业级读写服务质量和强一致性保障，又能为流场景提供高吞吐、低延迟。</p><p></p><p>Apache Pulsar 在腾讯云中已经得到大规模的生产实践，在过去一年中承接了诸多行业生态中不同的使用场景。在实际的生产实践中，腾讯云针对 Apache Pulsar 做了一系列的性能优化和稳定性功能方面的工作，来保障用户在不同的场景下系统的稳定高效的运行。本文围绕腾讯云近一年在 Pulsar 稳定性和性能方面优化最佳实践。</p><p></p><p></p><h2>Pulsar 在腾讯云百万级 Topic 上的应用</h2><p></p><p></p><p>为什么选择在生产环境中使用 Pulsar？</p><p></p><p>此前该用户使用 Kafka 集群来承载业务，由于业务的特定场景，集群的整体流量相对不大，但是需要使用的 Topic 较多。此前使用 Kafka 集群时，由于 Kafka 自身架构的限定，用户不能在一套集群中创建较多的 Topic，所以为了满足业务多 Topic 的使用场景，需要部署多套 Kafka 集群来满足业务的使用，导致业务使用的成本较大。</p><p></p><p><a href=\"https://www.infoq.cn/article/0LRhleM5wpzgJwaRuprC\">Pulsar </a>\"本身除了具备 Pub-Sub 的传统 MQ 功能外，其底层架构计算存储分离，在存储层分层分片，可以很容易地把 BookKeeper 中的数据 offload 到廉价存储上。Pulsar Functions 是 Serverless 的轻量化计算框架，为用户提供了 Topic 之间中转的能力。在开源之前，Pulsar 已在 Yahoo! 的生产环境中经历 5 年的打磨，并且可以轻松扩缩容，支撑多 Topic 场景。为了降低使用的成本，同时满足多 Topic 的业务场景，该用户切换到了 Pulsar 的集群上。</p><p></p><p>当前该用户的一套 <a href=\"https://www.infoq.cn/article/ZyNHQsXPD0N7VwHcUbTT\">Pulsar</a>\" 集群可以承载 60W 左右的 Topic，在很好地满足了业务使用的场景的同时降低了使用成本。</p><p></p><p></p><h3>Apache Pulsar 稳定性优化实践</h3><p></p><p></p><p></p><h3>实践 1：消息空洞的影响及规避措施</h3><p></p><p></p><p>使用 Shared 订阅模式或单条 Ack 消息模型时，用户经常会遇到 Ack 空洞的情况。Pulsar 中单独抽象出了 individuallyDeletedMessages 集合来记录空洞消息的情况。该集合是开闭区间集合，开区间表明消息是空洞消息，闭区间表明消息已被处理。早期 Pulsar 支持单条 Ack 和批量 Ack 两种模型，后者对标 Kafka 的 Ack Offset。引入单条 Ack 模型主要针对在线业务场景，但也因此带来了 Ack 空洞问题。Ack 空洞即下图中 individuallyDeletedMessage 所展示的集合。</p><p></p><p>如何理解 individuallyDeletedMessage？以下图为例：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ef/ef3c945b0f9eacac4ce7cf7d47cc3b62.png\" /></p><p></p><p>该记录中第一个 Ledger id 是 5:1280，该集合是闭区间，说明消息已经被 Ack；之后的 5:1281 是开区间，说明消息没有被 Ack。这里就用开闭区间的形式来区分消息是否被 Ack。</p><p></p><p>Ack 空洞的出现原因可能因为 Broker 处理失败，源于早期版本的设计缺陷，Ack 处理没有返回值。在 2.8.0 及以上版本中，对事务消息支持上引入了 AckResponse 概念，支持返回值。因此在早期版本中，调用 Ack 后无法确保 Broker 可以正确处理 Ack 请求。第二个原因可能因为客户端出于各种原因没有调用 Ack，在生产实践中出现较多。</p><p></p><p>为了规避 Ack 空洞，一种方法是精确计算 Backlog Size。因为在 Broker 上解析 Batch 消息会浪费性能，在 Pulsar 中对 Batch 消息的解析在消费者侧，因此一个 Entry 可能是单条消息也可能是 Batch 消息的。后者情况下 Batch 内的消息数量或形态是未知的。为此要精确计算 Backlog Size，但经过调研发现这种方法的复杂性和难度较大。</p><p></p><p>另一种方法是 Broker 的主动补偿策略。因为 individuallyDeletedMessage 存储在每一个 ManagedCursor，也就是每一个订阅对象到 Broker 实际类中的映射。每一个订阅都可以拿到对应的 individuallyDeletedMessage 集合，Broker 就可以主动把集合推送到客户端，也就是主动补偿。</p><p></p><p>接下来我们了解一下 Broker 主动补偿机制，即 Backlog 策略。在了解补偿机制之前，先要了解 Topic 可能的分布与构成。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a6/a6c98ed91c5b9256387c87e76d541049.png\" /></p><p></p><p>正常来说，生产者向 Topic 发布消息，消费者从 Topic 接收消息。如上图，红、灰、蓝色代表消息在 Topic 中的三种形态。Pulsar 中引入了 Backlog 策略，用来描述生产者和消费者之间的 Gap。该策略提供了三种选项，包括 Producer Exception、Producer Request Hold 和 Consumer Backlog Eviction。</p><p></p><p>其中，Producer Exception 相对用户友好，在生产环境中更加常用。当消息堆积到一定程度，消费者处理消息的能力不足时，Producer Exception 会通知生产者出现了问题。Producer Request Hold 原理相同，但是 Producer Request Hold 只是会让生产者停止发送，而不会告知其原因（即不会向业务侧返回标识），用户感知为 Producer 停止发送消息但是无异常抛出。而 Consumer Backlog Eviction 则会自动丢弃最早的消息来保证消息持续处理，可能导致丢消息的情况出现。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/26/2636199205ebcae0490489fb62ac4ede.png\" /></p><p></p><p>此外，还需要注意的是 Pulsar 计算 Backlog Size 的方式。上图可以理解为一个事件流，生产者源源不断地 append message。Pulsar 计算 Backlog Size 时，是计算从当前 MarkedDeletedPosition 的位置，到 ReadPosition 的位置之前的 Backlog Size，而后结合 Producer Exception 策略暴露出来。如果 Ack 空洞，比如 Broker 侧请求失败，或者客户代码产生异常导致 Ack 永远不会被调用，Backlog Size 会到达一定速率，就相当于限制生产者。上图中，M4 和 M2 是两条空洞消息，出现这样的空洞消息时，生产者的发送流就迟早会被打断。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/10/10bdebd71a3ae476502647e6313b399c.png\" /></p><p></p><p>Broker 主动补偿机制的实现方式如上图。由于 individuallyDeletedMessage 记录了所有消息的 Ack 成功与否的状态，就可以从中获取 MarkedDeletedPosition 位置的消息，开启一个 Executor Service 定时任务，设置监听频率，间隔一段时间将消息重新推送到客户端侧，实现 Broker 的主动补偿，避免 Ack 空洞导致 Producer Exception 被频繁触发。</p><p></p><p></p><h3>实践 2：再谈 TTL、Backlog 及 Retention 策略</h3><p></p><p></p><p>我们先看下这三个概念：</p><p></p><p>TTL：表示消息在指定时间内没有被用户 Ack 时会在 Broker 主动 Ack。Backlog ：表示生产者发送的消息与消费者接收消息之间的差距。Retention：表示当消息被 Ack 之后，继续在 Bookie 侧保留多久的时间，以 Ledger 为最小操作单元。</p><p></p><p>如果 TTL 和 Retention 同时设置，那么一条消息的生命周期该如何计算？来看以下代码：</p><p></p><p><code lang=\"cs\">void updateCursor (ManagedCursor Impl cursor, PositionImpl newPosition) t\n    Pair pair = cursors.cursorUpdated (cursor, newPosition);\n    if (pair == nulL) {\n        Cursor has been removed in the meantime\n        trimConsumedLedgersInBackground();\n        return;\n    }\n    \n    PositionImplpreviousSlowestReader = pair.getLeftO);\n    PositionImpl currentSlowestReader = pair.getRightO);\n\n    if (previousSlowestReader.compareTo(currentSlowestReader)==0){\n        // The slowest consumer has not changed position. Nothing to do right now\n        return;\n    }\n    \n    //Only trigger a trimming when switching to the next Ledger\n    if (previousSlowestReader.getLedgerId() != newPosition.getLedgerId0)) f\n        trimConsumedLedgersInBackground();\n    }\n</code></p><p></p><p>TTL：根据设置的时间（默认五分钟）定期检查，根据触发的策略不断更新 cursor 位置，处理消息过期。Retention：检查 Ledger 的创建时间（通过元数据时间戳可以了解 Ledger 的生命周期）以及 Entry 的大小两个阈值来决定是否删除某一个 Ledger。</p><p></p><p>在以上代码中的最后三行中，将之前最慢的 LedgerId 与 newPosition 的 LedgerId 对比，检查 ManagedLedger 是否发生过切换，一旦切换就调用 trimConsumedLedgersInBackground()。该函数方法的核心代码策略就是 Retention 的逻辑。</p><p></p><p>由此可知：</p><p></p><p>当 TTL 时间小于 Retention 时间时，消息的完整生命周期就是 TTL 时间 + Retention 时间；当 TTL 时间大于等于 Retention 时间，消息的生命周期就是 TTL 时间。</p><p></p><p>这里又引出了一个新问题：TTL 策略为什么要选择在 Ledger 切换的时机来触发 Ledger 的删除操作呢？因为 Retention 删除 Ledger 时是以 Ledger 为最小操作单元。如果 Ledger 不切换，Retention 也不会触发删除。所以上述代码逻辑会选择切换时机来交给 Retention 执行删除动作。</p><p></p><p></p><h3>实践 3：延迟消息与 TTL 的关系</h3><p></p><p></p><p>在团队曾经遇到的场景中，某用户发送了数十万延迟消息，延迟设置为十天，但 TTL 过期时间设置为五天，五天后所有延迟消息都已被过期。我们可以从源码层面看一下 TTL 策略。</p><p></p><p><code lang=\"kotlin\">public boolean expireMessages(int messageTTLInSeconds) {\n    if (expirationCheckInProgressUpdater.compareAndSet( obj: this, FALSE, TRUE)) {\n        log.info(\"[{}][{}] Starting message expiry check, ttl= {} seconds\", topicName, subName,\n                messageTTLInSeconds);\n\n        cursor.asyncFindNewestMatching(ManagedCursor.FindPositionConstraint.SearchActiveEntries, entry -&gt; {\n            try {\n                long entryTimestamp = Commands.getEntryTimestamp(entry.getDataBuffer());\n                return Messaqelmpl.isEntryExpired(messageTTLInSeconds. entryTimestamp);\n            } catch (Exception e) {\n                log.error(\"[{}][{}] Error deserializing message for expiry check\", topicName, subName, e);\n            } finally {\n                entry<=\"\" code=\"\"></code></p><p></p><p><code lang=\"kotlin\"><code lang=\"java\">public static boolean isEntryExpired(int messageTTLInSeconds, long entryTimestamp) {\n    return messageTTLInSeconds != 0\n            &amp;&amp; (System.currentTimeMillis() &gt;\n            entryTimestamp + TimeUnit.SECONDS.toMillis(messageTTLInSeconds));\n}\n</code></code></p><p></p><p><code lang=\"kotlin\">TTL 的核心逻辑是通过 cursor 传入的值决定消息是否过期，即是否能找到 Entry。TTL 只获取了消息的发布时间，却没有理会消息的延迟设置。结合上面两段代码，isEntryExpired 只关心 PublishedTime 时间戳元数据属性，FindNewestMatchingEntry 对象时可以从元数据中获取 PublishedTime。所以当延迟设置小于 TTL 时间就会导致延迟消息被过期，在用户侧就会发现消息丢失。</code></p><p></p><p><code lang=\"kotlin\">针对这一问题，腾讯团队向社区提供了 PR（<a href=\"https://github.com/apache/pulsar/pull/15628%EF%BC%89%EF%BC%8C%E4%B8%BB%E8%A6%81%E9%80%BB%E8%BE%91%E6%98%AF%E5%88%86%E5%88%AB%E6%A3%80%E6%9F%A5%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%91%E5%B8%83%E6%97%B6%E9%97%B4%E5%92%8C%E5%BB%B6%E8%BF%9F%E6%97%B6%E9%97%B4%EF%BC%8C%E5%88%B0%E8%BE%BE%E5%8F%91%E5%B8%83%E6%97%B6%E9%97%B4%E5%90%8E%E5%A6%82%E6%9E%9C%E5%BB%B6%E8%BF%9F%E6%97%B6%E9%97%B4%E5%A4%A7%E4%BA%8E\">https://github.com/apache/pulsar/pull/15628），主要逻辑是分别检查消息的发布时间和延迟时间，到达发布时间后如果延迟时间大于</a>\" TTL 时间，则 TTL 时间到达后依然不能过期消息。IsEntryExpired 会判断并检查 TTL 时间与延迟时间。这里发布时间和延迟时间要一次性从 Entry 中获取，否则每次获取的 Entry 对象是不一样的。此外，延迟时间需要发送时间点的时间戳，根据具体计算出延迟的时间长度来做判断。</code></p><p></p><p></p><h3><code lang=\"kotlin\">实践 4：Admin API Block 的优化处理</code></h3><p></p><p></p><p><code lang=\"kotlin\">在 Pulsar 之前的代码逻辑中：</code></p><p></p><p><code lang=\"kotlin\">如果在异步代码中频繁调用同步逻辑，那么其中的牵连关系很可能导致 Pulsar 外部的线程卡住，这时只能重启对应的 Broker 节点来恢复任务。Pulsar 的 Http Lookup 服务调用的是外部端口，一旦异步调用同步导致阻塞，那么该服务外部端口的数据流也会出现阻塞。Pulsar Web 服务的性能较差，主要是因为 CompletableFuture 的误用。当我们定义一个 CompletableFuture 对象后，经常调用 thenapply 或者 thencompose 来返回对象。这其实是 CompletableFuture 内对象的同步返回，是由当前线程栈执行的。如果异步任务没有返回，则由回调线程执行任务。Pulsar 高版本加入了 Metadata Store 线程池的抽象。这个抽象会增大 ZooKeeper 的压力。当同一时间内的外部服务调用量增大，ZooKeeper 负载增大会导致消息延迟等指标出现退化。</code></p><p></p><p><code lang=\"kotlin\">腾讯团队针对上述问题，一方面剥离了 Metadata Store 线程池，另一方面通过服务监听来定位和发现 Web 服务的性能较弱的位置，去做进一步的优化处理。此外，团队还加入了超时处理逻辑，所有 Pulsar 外部线程如果在最后限定时间（30 秒）内无法处理完成就会抛出超时。虽然单个外部线程超时、重启影响不大，但这样避免了整个数据流阻塞的情况。</code></p><p></p><p></p><h3><code lang=\"kotlin\">实践 5：zk-node 泄露</code></h3><p></p><p></p><p><code lang=\"kotlin\"><img src=\"https://static001.geekbang.org/wechat/images/50/5092af7a6f9d2d36f1c25347d59c3370.png\" /></code></p><p></p><p><code lang=\"kotlin\">有时用户正在使用的 Topic 不多，但 zk-node 数量却很大，Pulsar 对 zk-node 的放大倍数较高。上图拐点是 zk-node 脏数据清理的时点，可以看到 zk-node 数据泄漏的情况非常严重，达到 5 倍之多。</code></p><p></p><p><code lang=\"kotlin\"><img src=\"https://static001.geekbang.org/wechat/images/90/901bb3ea639bdd2bd421e6495233cbd9.png\" /></code></p><p></p><p><code lang=\"kotlin\">在创建一个 Topic 时，首先要在 zk-path 的六级目录下涵盖所有 Topic 信息，在 ZooKeeper 上创建的资源量很大。此目录下涵盖了所有的 Topic，问题即出现在六个层级中。为此团队做了以下操作来处理 zk-node 脏数据：</code></p><p></p><p><code lang=\"kotlin\">首先通过 ZooKeeper client 读取 zk-path，按照指定的格式拼接所有 Topic 名字，获取 Topic 列表；通过 pulsar-admin 检查集群中是否存在该 Topic；如果集群中不存在该 Topic，则相关数据一定是脏数据；（修复 zk-node 泄露问题的相关代码已 merge 进 2.8 + 的社区版本。）切记在清理 ZookKeeper 脏数据之前备份 ZookKeeper 数据。</code></p><p></p><p></p><h3><code lang=\"kotlin\">实践 6：Bookie Ledger 泄漏</code></h3><p></p><p></p><p><code lang=\"kotlin\">团队在实践中发现，虽然 Retention 策略设置的消息生命周期最长应不超过 30 天，但检测扫描到的一些消息已经有数百天历史，且难以从 BookKeeper 中删除。针对这一问题，团队分析如下：</code></p><p></p><p><code lang=\"kotlin\">触发 Ledger 删除的唯一路径是 Retention 策略。这些消息产生的原因只能定位到一些 Bookie CLI 命令，这些命令生成了一些 Retention 策略管控不到的 Ledger。每一个 Ledger 都有对应的 LedgerInfo，记录了它的元数据信息，包括创建时间等。获取元数据后，就可以确定 Ledger 是多久前创建的，还可以确定 Ledger 具体是在哪些 Bookie 节点上。一个 Ledger 唯一归属于一个 Topic，所以可以获取 Topic 中存在 Ledger 的信息，进而确定某个 Ledger 是否存在于 Topic 的 Ledger 列表中，如果不在就是脏数据，可以清理。如果 Ledger 对应的元数据已经丢失，那么 Ledger 本身也可以直接删除。注意 Schema，如果忽略 Schema 可能会删除用户 Schema。恢复用户 Schema 时，Schema 的 Ledger 信息是存在 Bookie 中，Schema 自身的信息存在 Broker 归属的 ZK 中。恢复时需要先把 Broker 中存在的 Schema 信息删除，再让用户尝试使用生产端重建 Schema。</code></p><p></p><p></p><blockquote><code lang=\"kotlin\">注意：执行以上操作前，切记提前备份数据。</code></blockquote><p></p><p></p><p></p><h3><code lang=\"kotlin\">实践 7：Apache Pulsar 多级缓存优化</code></h3><p></p><p></p><p><code lang=\"kotlin\"><img src=\"https://static001.geekbang.org/wechat/images/23/236ca265ed1f21e5818c4d8fef0d0183.png\" /></code></p><p></p><p><code lang=\"kotlin\">如上图，Pulsar 现有缓存策略会导致明显的毛刺现象，出现服务周期性的剧烈性能波动和用户端的明显感知。</code></p><p></p><p><code lang=\"kotlin\"><code lang=\"cs\">try {\n     //We need to check all the segments, starting from the current\n     //backward to minimize the\n     //checks for recently inserted entries\n     int size = cacheSegments.size();\n     for (int i = 0; i &lt; size; i++)\n         int segmentIdx = (currentSegmentIdx + (size - i)) % size;  \n</code></code></p><p></p><p><code lang=\"kotlin\"><code lang=\"cs\">try {\n    int offset = currentSegmentOffset.getAndAdd(entrySize);\n    if (offset + entrySize &gt; segmentSize) {\n        // Rollover to next segment\n        currentSegmentIdx = (currentSegmentIdx + 1) % cacheSegments.size();  \n        currentSegment0ffset. set(alignedSize);\n        cacheIndexes.get(currentSegmentIdx).clear();\n         offset = 0;\n}\n</code></code></p><p></p><p><code lang=\"kotlin\">这里腾讯团队主要做了读取缓存的优化。在读取缓存层面，可以看到 Pulsar 在读取缓存时迭代了缓存中的所有消息，如第一段代码倒数第二行所示。同时，一旦 offset + entrySize 大于 segmentSize，就会清除全部缓存，如第二段代码所示。这也就是为什么之前会出现明显的性能波动点的原因所在。</code></p><p></p><p><code lang=\"kotlin\">为此团队使用了 OHC + LRU 的策略，避免了缓存情况导致的剧烈波动，效果如下图：</code></p><p></p><p><code lang=\"kotlin\"><img src=\"https://static001.geekbang.org/wechat/images/67/675f3836f3a2313fe39d798e960283af.png\" /></code></p><p></p><p></p><h2><code lang=\"kotlin\">总结与展望</code></h2><p></p><p></p><p><code lang=\"kotlin\">本文分享了腾讯云团队在 Apache Pulsar 稳定性上的实践经验，重点介绍了消息空洞的影响及规避措施等最佳实践，为更多开发者提供参考。同时，腾讯云团队也在参与社区贡献中，和社区讨论以下重要问题并探索相关解决方案，如客户端超时时间内的重试策略，借鉴其他 MQ 的思路进行改进，尝试在客户端加入超时重试策略，通过多次重试机制来避免发送失败的情况发生；优化 Broker 和 Bookie OOM，针对 Ack 空洞对应集合无法缩容的问题进行改进；以及优化 Bookie Auto Recover，加入超时重试逻辑，避免 BookKeeper 和 ZooKeeper 之间发生 Session 超时的情况下服务重启。</code></p><p></p><p><code lang=\"kotlin\">作者介绍：</code></p><p></p><p><code lang=\"kotlin\">冉小龙，腾讯云高级研发工程师，Apache Pulsar Committer，RoP maintainer，Apache Pulsar Go Client、Pulsarctl 与 Go Functions 作者与主要维护者。</code></p><p></p><p><code lang=\"kotlin\">点击阅读原文，关注 Apache Pulsar。</code></p><p></p><p><code lang=\"kotlin\">今日好文推荐</code></p><p></p><p><code lang=\"kotlin\"><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651141163&amp;idx=1&amp;sn=99aa47bf30120796bd751f19bc5cf2a0&amp;chksm=bdb8cc788acf456ecb420dfcc9e053e53506c68b5c67824ab88a6eff6633690c17d4bc668ca5&amp;scene=21#wechat_redirect\">从一线研发到公司创始人，基础软件创业者迷雾中与市场赛跑</a>\"</code></p><p></p><p><code lang=\"kotlin\"><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651141032&amp;idx=1&amp;sn=442a5173ee6de4e77bb523c4ea778828&amp;chksm=bdb8cbfb8acf42ed0e5c4219bb684cb1c85f579e5cc216820b2a07fb9f07b06fab57110afa77&amp;scene=21#wechat_redirect\">Azure CTO 呼吁不要使用 C/C++ 启动新项目，C++ 之父回应：你们这些高管就爱喜新厌旧</a>\"</code></p><p></p><p><code lang=\"kotlin\"><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651140751&amp;idx=1&amp;sn=e2bf15afc244935fa620aa1b059ca599&amp;chksm=bdb8cadc8acf43cac05e729ac1e4b75a494d12b42c00ccadbf5cd00c9424453f72a32105bc37&amp;scene=21#wechat_redirect\">NGINX 局限太多，Cloudflare 最终放弃它并用 Rust 自研了全新替代品</a>\"</code></p><p></p><p><code lang=\"kotlin\"><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651140613&amp;idx=1&amp;sn=9c18da3b5843e9f17de54156dcbe3eb2&amp;chksm=bdb8ca568acf43404f2c5c655f729ee6ac93f3e5c1b545f938474bdb612ef42442a230485187&amp;scene=21#wechat_redirect\">CEO 们突然介入到 IT 建设， 企业纷纷迁出 VM 虚拟机基础设施</a>\"<a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651140613&amp;idx=1&amp;sn=9c18da3b5843e9f17de54156dcbe3eb2&amp;chksm=bdb8ca568acf43404f2c5c655f729ee6ac93f3e5c1b545f938474bdb612ef42442a230485187&amp;scene=21#wechat_redirect\"></a>\"</code></p><p></p><p><code lang=\"kotlin\"><img src=\"https://static001.geekbang.org/wechat/images/2b/2b44db05f6e661eb2eee307b1ac747ba.gif\" /></code></p><p></p>",
    "publish_time": "2022-09-26 21:25:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]