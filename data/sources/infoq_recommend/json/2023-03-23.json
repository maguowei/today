[
  {
    "title": "如何最大限度地利用GitHub Actions自动化软件开发过程",
    "url": "https://www.infoq.cn/article/8q3rjE8YRu11TaCS2bhl",
    "summary": "<p></p><h2>Actions发展简介</h2><p></p><p></p><p>当GitHub Actions在2019年秋天发布时，立即就引起了人们的关注，因为它引领了“<a href=\"https://blog.codecentric.de/github-actions-nextgen-cicd\">CI/CD平台的第三次风潮</a>\"”。从可重用的开源构建块构建可组合管道，在改善CI/CD管道操作的效率和维护方面，这个独特的方法明显有很大的潜力。然而，尽管Actions平台的设计具有内在的吸引力，但许多组织，特别是已经构建了CI/CD系统的大型企业，对采取行动犹豫不决，这是可以理解的。在一定程度上，这是由于婴儿期的平台存在诸多限制，比如在企业内部执行和共享Actions的限制。</p><p>&nbsp;</p><p>幸运的是，自最初发布之后，Actions已经采取了一些步骤来消除障碍，实现企业使用的可维护性。此后不久，<a href=\"https://github.blog/2019-11-05-self-hosted-runners-for-github-actions-is-now-in-beta/\">自托管runner</a>\"推出，这是在现有企业内部网络中执行作业的一项基础能力。然后，2021年11月，<a href=\"https://github.blog/2021-11-29-github-actions-reusable-workflows-is-generally-available/\">可重用工作流</a>\"发布，极大地扩展了提供可重用管道的能力，减少了重复步骤和样板代码的数量，缩小了与许多更成熟的CI/CD产品的差距。真正让这一功能做好企业采用准备的是在2022年1月宣布的<a href=\"https://github.blog/changelog/2022-01-21-share-github-actions-within-your-enterprise/\">内部共享操作</a>\"的能力，这种能力使平台工程团队能够打包发布常见的可重用步骤，而又不会暴露给外界。最终，我们感觉Actions有能力作为企业CI/CD生态系统的基础了。</p><p>&nbsp;</p><p></p><h2>我们为什么转到Actions</h2><p></p><p></p><p>在Thrivent，我们最近几年投入了大量的精力来改善CI/CD工具的开发体验，包括实现自动化，按照公认的黄金路径快速从模板生成应用程序管道。尽管如此，我们在提供一个一致性和灵活性均衡的平台方面遇到了限制。</p><p>&nbsp;</p><p>就我们当前的系统而言，其中一个不足体现在在CI/CD管道中共享公共任务。我们的共享脚本存储库提供的封装有限，并且会弄乱应用程序的构建历史，将实际应用程序存储库的提交和共享任务存储库的提交混在一起。</p><p>&nbsp;</p><p>在对管道模板升级时，我们保留用户自定义设置的能力也很有限。如果我们在模板中引入了一个新的步骤或修复了一个Bug，那么开发人员将不得不重新生成整个管道来获取更改，这会删除他们所做的所有定制。</p><p>&nbsp;</p><p>最后，还有一个不太明显的因素是，最大限度地减少开发人员使用CI/CD系统的障碍。当我们的技术组织进入增长期，有大量新的开发人员涌入，提供一个能够让这些开发人员快速上手并满足他们期望的平台成了当务之急。在最近<a href=\"https://survey.stackoverflow.co/2022/#section-version-control-version-control-platforms\">Stack Overflow</a>\"和<a href=\"https://tsh.io/state-of-frontend/#whats-your-favorite-version-control-provider\">State of Frontend</a>\"的调查中，大多数开发人员都回复说使用了GitHub。鉴于它在开源社区中的地位，这不足为奇。通过将使用范围扩大到平台原生的CI/CD功能，我们看到了以下机会：</p><p>将工具整合到单个系统中，充分利用开发人员现有的熟悉度，从而减少无关的认知负荷；使用存储库事件最大限度地减少上下文切换，使编码生命周期内的工作更紧凑；提供我们一直在寻找的原生模块化管道架构。</p><p>&nbsp;</p><p>总的来说，GitHub Actions平台的设计借鉴了现代开发方法。开发人员可以利用可重用组件灵活地构建管道，将团队在开发活动中使用的开源思想转移到CI/CD管道中。然而，随着我们对Actions生态系统的深入研究，Actions在CI/CD处理方面存在着一些显著的差异，这促使我们重新思考了之前所做的一些假设，并开发了一种新的策略，将平台的限制变成一种优势。</p><p>&nbsp;</p><p></p><h3>术语速览</h3><p></p><p></p><p>在GitHub Actions系统中，有几个术语由于在CI/CD生态系统中使用广泛，可能具有各种含义，但在Actions框架中，它们指的是特定的资源（即“工作流”、“作业”、“操作”、“事件”和“runner”）。如果你不熟悉GitHub Actions的定义，那么建议你快速浏览一下<a href=\"https://docs.github.com/en/enterprise-cloud@latest/actions/learn-github-actions/understanding-github-actions\">GitHub的文档</a>\"。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9f/9f0e5708fe3382ab8c3c131e97e5e3b0.jpeg\" /></p><p></p><p>GitHub Actions工作流可视化（来源：<a href=\"https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions\">GitHub</a>\"）</p><p>&nbsp;</p><p></p><h2>思想转变</h2><p></p><p></p><p>在转换到Actions时，其中一个需要首先理解的区别是存储库和管道工具之间的关系。我们一直在使用的CI/CD产品是与应用程序的存储库集成在一起的，虽然本质上是分开的。即使是CI/CD领域中的其他工具，它们可能为应用程序的整个生命周期提供了更全面的解决方案（通常将存储库、管道和其他交付工具捆绑到单个产品中），也倾向于提供更高级的结构，如“项目”或“应用程序”，明确区分存储库和管道，使它们的地位相对平等。当CI/CD过程开始时，所有的注意力都转到了管道工具视图上。</p><p>&nbsp;</p><p>相比之下，使用GitHub Actions，存储库仍然是宇宙的中心。Actions平台中触发的工作流主要存在于存储库的上下文中。这种方法有利也有弊。</p><p>&nbsp;</p><p></p><h3>优点</h3><p></p><p></p><p>存储库现在提供了应用程序状态的单一落点，将应用程序运行状况和部署信息与代码本身紧密地结合在一起。在传统的提交/推送触发器之外，它还提供了在各种存储库事件（例如，加标签、pull请求）上启动工作流的能力，方便我们将CI/CD更紧密地集成到开发流程中去。对于共享操作和工作流的系统，它提供的能力让我们可以独立于应用程序存储库的变更，轻松地管理共享管道组件的生命周期。</p><p>&nbsp;</p><p></p><h3>缺点</h3><p></p><p></p><p>因为所有事情都是基于存储库中发生的事件进行编排的，所以很难从其他（外部）事件源启动工作流。通过工作流生成的工件在存储库中的可见度与代码的可见度不一样。这意味着分享和推广结果工件的能力是有限的，因为该信息没有持久化到存储库中，也没有在工作流执行之间共享。不同的存储库组织方法可能会为流程带来额外的复杂性，例如，<a href=\"https://en.wikipedia.org/wiki/Monorepo\">单存储库</a>\"可能在同一个存储库中包含多个不同的项目。这些相互独立的应用程序在构建和部署的方式上也可能存在差异。因此，需要格外小心，并可能需要借助额外的工具来有效地管理触发事件，以避免构建未经修改的应用程序，以及正确地打包和部署下游工件。</p><p>&nbsp;</p><p></p><h2>制定Actions使用策略</h2><p></p><p></p><p>现在，在了解了GitHub Actions的视角及其优缺点后，我们可以开始制定有效使用Actions平台作为CI/CD解决方案的策略。以下是指导我们作出决策的一些主要原则。</p><p>&nbsp;</p><p></p><h3>优选Pull请求审批而非工作流审批</h3><p></p><p></p><p>CI/CD平台的主要关注点之一是有效地收集和显示输出，并对这些结果强制执行质量门检验。以前，我们一直在使用一个许多组织都很熟悉的流程：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/83aef4a68f067cff340dec6d7c9c8ef5.jpeg\" /></p><p></p><p>以前的方法：存储库和管道检查是分开的</p><p>&nbsp;</p><p>在这种方法中，对目标分支的推送会触发管道。管道会分成一系列的阶段执行，每个阶段都会输出一些结果，并且通常会根据这些结果评估工件的生产就绪情况。</p><p>&nbsp;</p><p>我们的新目标是使这些信息更趋近于自然的开发活动，同时又要注意Actions结果和工件显示方式的一些固有限制，因此，我们希望有一种更好的方式来提供这些控制，而pull请求特性是合乎逻辑的选择。Pull请求维护的审计历史更久（与工作流执行的<a href=\"https://docs.github.com/en/enterprise-cloud@latest/organizations/managing-organization-settings/configuring-the-retention-period-for-github-actions-artifacts-and-logs-in-your-organization\">保留时间限制</a>\"相比），并且讨论聚焦于变更本身，代码可供随时参考。将pull请求作为主要的审批仪表板需要转换模式，pull请求收集相同的结果并执行同样的质量检查，但不需要额外导航回存储库查看相关的更改。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/777178afcd7d9106f2cb6f66d37c1d50.jpeg\" /></p><p></p><p>新策略：依据pull请求收集必要的信息</p><p>&nbsp;</p><p></p><h3>制定可重用组件指南</h3><p></p><p></p><p>构建可重用管道片段的自助库也带来了许多问题。我们要如何处理共享工作流和操作的更新？当多个团队对看似相同的任务在处理方式上略有差异时，如果我们想要平衡这些团队，什么样的抽象级别才合适？</p><p>&nbsp;</p><p>面对这些挑战，我们的目标是实现一系列“铺好的路（paved roads）”，每个新的存储库都应该能够使用我们团队支持的模板、操作和可重用工作流进行部署，但又有按需定制的灵活性。这为保持存储库始终处于可部署状态的持续交付指令奠定了基础。为了实现这一目标，我们制定了一些指导方针。</p><p>&nbsp;</p><p></p><h3>推送所有版本的语义版本标签</h3><p></p><p></p><p>维护精确的版本让用户有能力根据他们所能接受的风险级别保持更新。在我们之前的CI/CD工具中，共享任务是从存储库的主分支上拉取的，这意味着我们需要非常小心，以免引入意外的更改。虽然Actions支持这种用法，但最好使用语义版本范围在保持最新和避免破坏性更改之间实现一个平衡。在这里，我们学到的一个关键知识是，确保为每个版本推送/更新多个标签（主标签、次标签和补丁标签），确保跟踪主标签（如“@v2”）的用户也可以获得新的补丁版本。</p><p>&nbsp;</p><p></p><h3>像管理任何其他API一样管理你的接口</h3><p></p><p></p><p>当向内部团队提供资源时，很容易陷入这样的陷阱，即假设他们对内部工作方式的理解比对开源项目或外部供应商的理解更深入。这可能会迅速导致抽象泄漏，降低所提供的共享组件的价值，因为用户必须承担额外的认知负荷，而不仅仅是理解如何与它的API交互。</p><p>&nbsp;</p><p>为了保持更加一致的封装级别，我们的目标是明确我们的<a href=\"https://docs.github.com/en/enterprise-cloud@latest/actions/creating-actions/metadata-syntax-for-github-actions#inputs\">输入</a>\"和<a href=\"https://docs.github.com/en/enterprise-cloud@latest/actions/using-jobs/defining-outputs-for-jobs\">输出</a>\"，尽量减少假设和对环境副作用的依赖。在此基础上，我们一直在尝试提供一个合理的默认值，这样用户就可以专注于提供大多数用例都需要的值。最后，如果使用环境变量，我们建议将它们的范围限制得越窄越好。</p><p>&nbsp;</p><p>例如，如果一个环境变量只在一个步骤中使用，就在这个步骤中声明它，如果在多个步骤中使用，就在作业中声明它，最后，如果在多个作业中使用，就在工作流中声明它。这是一个<a href=\"https://refactoring.com/catalog/reduceScopeOfVariable.html\">通用的建议</a>\"，与平台无关，其目的是降低变量意外修改或命名冲突的风险，并通过将信息保存在使用位置附近来提高可读性。</p><p>&nbsp;</p><p></p><h3>聚焦工作流</h3><p></p><p></p><p>在GitHub Actions生态系统中，人们关注最多的是Actions；毕竟，产品就是以Actions命名的。然而，在铺好的路上，操作不过是砖。它们非常有助于将流程分解为功能步骤，但是，为了向内部交付团队提供最大价值，平台工程师应该投入同样多的时间来开发将这些步骤联系在一起的可重用工作流目录。可重用工作流可以确保不同的步骤按所需的顺序执行，并可以简化设置和拆卸活动。</p><p>&nbsp;</p><p>例如，我们的团队创建了一个将容器部署到Kubernetes环境的操作。为此，需要将环境元数据作为输入的一部分提供。为了更好地管理调用此操作的活动，我们使用工作流来执行一些额外的步骤，收集所需元数据、执行部署，然后将结果添加到输出。但是我们发现，在许多不同的情况下，这个工作流的代码都会重复——针对不同的触发事件，以及不同的部署环境（开发、过渡、生产等）。为了进一步最大化重用，我们将这些工作流重构为单个可重用的工作流，它会根据触发事件确定正确的目的地和镜像上下文。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/106ffb6e7655c2a4ef0bf86aa6c68678.jpeg\" /></p><p></p><p>进一步重用：合并3个几乎相同的作业……</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/32d58e911cdf0194ff3cae48b22c4137.jpeg\" /></p><p>进入一个可重用工作流</p><p>&nbsp;</p><p></p><h3>降低使用第三方Actions的风险</h3><p></p><p></p><p><a href=\"https://docs.github.com/en/enterprise-cloud@latest/actions/security-guides/security-hardening-for-github-actions\">GitHub的文档</a>\"提供了一些很好的GitHub Actions安全加固通用指南。其中有这样一条警告，“从GitHub上的第三方存储库获取操作存在重大风险”。为此，他们提供了一些指导，即将第三方操作的版本锚定到一个提交SHA，审计操作的源代码，并且只使用你信任的创建者提供的操作。</p><p>&nbsp;</p><p>这给我们造成了一些困难，因为我们已经习惯了由软件组合分析（SCA）工具自动提供适当的外部组件的风险（漏洞/许可）信息，这使得我们在应用程序开发活动中能够更广泛地应用开源组件。</p><p>&nbsp;</p><p>回归手动检查所有外部操作的做法，可能会严重限制开源Actions市场的效用，而这个市场在我们的旅程之初就有这样的承诺。随着供应链攻击的增加，保证管道安全变的至关重要，但手动检查操作源代码和创建者的方法无法扩展，因为我们团队可能对给定操作使用的语言没有深入的了解，而且，当它来自单个用户的贡献时，创建者的可信度就很难评估。</p><p>&nbsp;</p><p>最近，GitHub宣布支持<a href=\"https://github.blog/2022-08-09-dependabot-now-alerts-for-vulnerable-github-actions/\">Actions Dependabot告警</a>\"，这是朝着正确的方向迈出了良好的第一步。然而，这仍然没有达到我们所希望的安全等级，原因如下：</p><p>Dependabot生成的告警很大程度上依赖于创建者自己报告的漏洞。在个人提供的小操作中，尚不清楚他们可能提供何种程度的报告。仍然没有对风险进行基线评估——当用户正在浏览Actions市场时，并没有现成的数据让用户可以了解操作中现有的漏洞或创建者的漏洞管理流程。还有一种风险是，有人故意将恶意代码嵌入到操作中（如果他们是原始作者，则可以在创建操作时嵌入恶意代码，或者通过供应链攻击更隐蔽地嵌入恶意代码)，这种方法无法捕获这类恶意代码。最后，目前许多公司在依赖关系管理方面的最佳实践是内部托管一个存储库，并代理到其他主要的公共存储库（NPM、Maven Central、PyPI等）。对于公司产品使用的依赖项，会有一个内部缓存，为的是可以在<a href=\"https://www.theregister.com/2016/03/23/npm_left_pad_chaos/\">依赖源无法访问</a>\"时帮助确保业务连续性。现在，操作具备了同等的重要性，因为它们对工作管道至关重要，任何中断都可能给开发人员带来大量的时间损失。</p><p>&nbsp;</p><p>为了解决上述限制，我们使用现有的工具开发了一种方法，并结合GitHub设置中的控制项，设法在风险和开发速度之间达到一个平衡。首先，我们允许任何内部编写的操作和任何GitHub编写的操作。下一个通用控制是允许一个经过验证的创建者列表，并允许来自已与我们建立信任关系的组织的所有操作。最后，对于个别来自较小的、未经验证的第三方操作，我们有一个允许列表。</p><p>&nbsp;</p><p>为了加快这些操作的审批过程，我们利用现有的SCA、SAST和容器安全扫描工具来扫描这些操作的存储库和/或容器镜像，将任何潜在的漏洞都暴露出来。</p><p>&nbsp;</p><p>在某些情况下，我们使用的最后一种技术是将操作存储库的副本派生或导入到企业自己的存储库中。这提供了三个保证：</p><p>如果需要，总是有可用的代码；可以修改代码，以便更好地匹配我们自己的用例；可以确保任何更改版本引用的尝试（如更改标签）都不会影响我们。</p><p>&nbsp;</p><p>当然，还需要不断的努力，将源存储库中任何有用的更改或修复都合并到我们自己的版本中，因此，在使用这种方法时应该做好权衡。</p><p>&nbsp;</p><p>GitHub在<a href=\"https://github.com/github/roadmap/issues/592\">他们的路线图</a>\"中也有继续提高操作安全性的计划。显然，这是一个不断发展的领域，我们希望平台和解决方案生态系统的进步可以减少我们对自主开发评估流程的依赖。</p><p>&nbsp;</p><p></p><h3>扩大基础设施实践，紧跟需求步伐</h3><p></p><p></p><p>随着Actions平台应用范围的扩大，提高性能和可靠性成了人们关注的焦点。我们依据运营使用的关键经验扩展了我们的战略。</p><p>&nbsp;</p><p>利用自托管Runner增强弹性</p><p>像许多企业一样，为了满足需求，并帮助管理成本，我们大量使用了自托管runner，使作业可以与我们内部网络上的资源进行通信，从而可以更好地管理runner镜像的工具和设置。</p><p>&nbsp;</p><p>尽管依赖于自托管runner，但我们希望与使用GitHub托管的runner的体验保持一致，即在作业开始时runner已经准备就绪（最小化作业等待runner可用的排队时间），并且将这些runner视为一次性使用的临时实例，最大限度地减少前一次执行留下的更改影响下一作业幂等性的机会。</p><p>&nbsp;</p><p>为次，我们维护了一个一次性runner的暖池——当新作业启动并获取runner时，我们使用一个WebHook来启动下一个runner。这样一来，这个池子就总是能够处理当前的工作量。另外，在每个工作日开始时，我们还使用默认分配的runner来预先填充runner池（然后在晚上关闭它），并随着使用量的增加扩大这个池子。</p><p>&nbsp;</p><p>使用缓存缩短构建时间</p><p>如上所述，我们的runner是一次性的，我们为每个作业提供的运行环境都是干净的。虽然这在构建一致性和隔离方面带来了明显的好处，但那也意味着生成的任何文件后续作业都无法使用，除非它们直接使用像<a href=\"https://docs.github.com/en/enterprise-cloud@latest/actions/using-workflows/storing-workflow-data-as-artifacts\">Artifacts</a>\"这样的特性共享。在这里，我们要克服的主要挑战之一便是花费大量的时间重新生成现有的资源。</p><p>&nbsp;</p><p>幸运的是，在需要共享大量极少变化的数据（如应用程序依赖关系）时，GitHub提供了几种缓存功能。我们需要设法解决的三种最常见的情况是共享应用程序工件、容器镜像和操作镜像。</p><p>&nbsp;</p><p>第一种情况涉及到作业之间使用的应用程序工件和依赖关系。在这种情况下，我们使用<a href=\"https://github.com/actions/cache\">GitHub缓存操作</a>\"迅速提高了工作流的性能。</p><p>&nbsp;</p><p>容器镜像，包括常用的基础镜像层，是第二种最常见的情况。为了缓解经常重新拉取常用镜像的问题，我们研究了<a href=\"https://github.com/docker/build-push-action#inputs\">Docker buildx操作</a>\"支持的<a href=\"https://github.com/moby/buildkit#export-cache\">不同缓存选项</a>\"。我们在大多数镜像构建任务中都使用了这些选项。在对内联缓存、GHA（GitHub Actions缓存）和注册中心缓存选项进行了不同的度量之后，我们目前使用GitHub容器注册中心（GHCR）托管的注册中心缓存取得了最大的成功。</p><p>&nbsp;</p><p>最后，最棘手的是操作镜像。需要理解的一个重要概念是，每次使用打包为镜像的GitHub操作时，都是使用它们的Dockerfile从头开始构建的（有一个替代方案是使用Docker Hub中的公共镜像，但我们不希望公开发布内部操作，所以这并不可行）。</p><p>&nbsp;</p><p>为了减轻这种痛苦，我们遵循通用镜像最佳实践来尽量缩小镜像。不过，我们也在研究将一些作业抽离出来，不再作为容器化操作执行，而是将相同的逻辑打包为预构建的独立镜像，然后将作业配置为在该镜像中运行。然后，作业将执行一个脚本。该脚本通常用作操作的ENTRYPOINT，其行为与在本地作为操作运行时非常相似。这样做的好处是，我们可以拉取一个镜像，而不是每次重新构建每个层，这使得我们可以在runner启动过程中预先拉取常用的镜像，在工作流开始之前缓存它们。</p><p>&nbsp;</p><p></p><h2>GitOps即将来临</h2><p></p><p></p><p>尽管我们提升了操作组合以及工作流的质量和效率，但我们还面临的主要问题之一是，在应用程序部署和运营阶段保持敏捷性和可见性。我们最初的部署方法是将变量替换为参数化模板，使用我们的自定义操作，最终形成一个完整的Kubernetes清单，应用于我们的环境。</p><p>&nbsp;</p><p>这给我们的团队带来了很大的负担，因为我们需要使用复杂的逻辑来处理各种部署风格、重试和部署失败恢复，以及清理过时部署。由于在部署后，包含所有变量的最终清单没有在任何地方持久化，所以部署问题也变得更加难以排查。这也意味着，回滚或升级工件需要重新调用整个流程。</p><p>&nbsp;</p><p>为应对这些挑战，我们再次开始研究如何重塑部署策略，以发挥GitHub CI/CD平台的优势。在重申上述部分经验教训的基础上，我们认识到，我们需要的解决方案应该：</p><p>聚焦于存储库；通过pull请求控制变更；尽量减少自定义部署逻辑。</p><p>&nbsp;</p><p>到此为止，熟悉<a href=\"https://opengitops.dev/\">GitOps</a>\"理念的读者可能开始明白我们的思路了。在GitOps提倡的部署策略中，运行时状态在Git存储库中声明，并由环境中运行的代理自动拉取和协调。</p><p>&nbsp;</p><p>转向GitOps部署模型需要采用新的技术和行为模式，但这使得我们可以利用经开源社区专家优化的部署和协调逻辑，并维护一个清晰、持久、有版本控制的环境变更历史，有一个清晰的路径（拉请求）可以轻松地更改版本，并在需要时保留审批记录。这一切都是基于开发人员现有的熟悉度，也正是这种熟悉度把我们吸引到了GitHub。</p><p>&nbsp;</p><p>我们正处于这一转变的早期阶段，但很高兴我们采取了下一步措施，为我们的客户提供了更好、更可靠的价值交付方式。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/enterprise-github-actions/\">https://www.infoq.com/articles/enterprise-github-actions/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/TTDS1pC6Cz6MRqJTpMir\">为什么说可观察性是解锁&nbsp;GitOps&nbsp;的关键</a>\"</p><p><a href=\"https://www.infoq.cn/article/E2Nk6iYClH4RL8aNCRDB\">GitOps&nbsp;是皇帝的新衣吗？</a>\"</p><p><a href=\"https://www.infoq.cn/article/UMsJHeW2ZWEcF6uUMstQ\">最全的&nbsp;GitOps&nbsp;工具选型，30+ 款工具随你挑</a>\"</p><p></p>",
    "publish_time": "2023-03-23 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java JVM 可观测的原理解释和落地方案对比",
    "url": "https://www.infoq.cn/article/3iwV28HezCgCyPdFMWSd",
    "summary": "<p></p><h2>如何监控 JVM</h2><p></p><p></p><p>云原生时代，我们使用 Prometheus 来作为可观测平台的指标监控的核心组件。监控一个云原生的 Java 应用，一般会分成 3 个步骤：</p><p></p><p>配置 Java 应用，暴露 JVM 指标信息通过 Prometheus 采集并存储指标数据通过 Grafana 的 Dashboard 展示采集到的指标</p><p></p><p>借力社区生态，我们有越来越丰富的方式来暴露 JVM 核心指标，文本将介绍 JVM 监控的基本原理，并重点介绍常见的 3 种暴露 JVM 指标的方案，可以根据读者的使用场景来选择。</p><p></p><p></p><h2>采集原理</h2><p></p><p></p><p>Java Management Extensions（JMX）技术是 Java SE 平台的标准功能，提供了一种简单的、标准的监控和管理资源的方式，对于如何定义一个资源给出了明确的结构和设计模式，主要用于监控和管理 Java 应用程序运行状态、设备和资源信息、Java 虚拟机运行情况等信息。并且如下图所示，有关应用程序性能和资源使用情况的详细信息可以从 JMX 指标中导出。如果有任何问题，我们可以借助收集的指标进行诊断，并对系统进行微调以获得最佳性能。JMX 是可以动态的，所以也可以在资源创建、安装、实现时进行动态监控和管理，JDK 自带的 jconsole 就是使用 JMX 技术实现的监控工具。图片: </p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a21d967f0b6210b412675b2a82902a5d.png\" /></p><p></p><p>使用 JMX 技术时，通过定义一个被称为 MBean 或 MXBean 的 Java 对象来表示要管理指定的资源，然后可以把资源信息注册到 MBean Server 对外提供服务。MBean Server 充当了对外提供服务和对内管理 MBean 资源的代理功能，如此优雅的设计让 MBean 资源管理和 MBean Server 代理完全独立开，使之可以自由的控制 MBean 资源信息。</p><p></p><p>JMX 不仅仅用于本地管理，JMX Remote API 为 JMX 添加了远程功能，使之可以通过网络远程监视和管理应用程序。得益于相对独立的架构设计，使 JMX 可以平滑的集成到各种监控系统之中。并具有包括遵守 Java 规范、通用的管理方式、开箱即用的监控功能、架构设计优秀、监测 JVM 状态能力、管理解决方案集成的能力等优点。</p><p></p><p>JMX 技术架构主要有资源管理（MBean/MXBean）模块，资源代理模块（MBean Server），远程管理模块（Remote API）组成 ，下面的图片来自维基百科，很好的展示了三个模块之间的关系。图片: </p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f9e211a9aeb92e0251b093c2365b4e97.png\" /></p><p></p><p>资源管理在架构中标识为资源探测层（Probe Level），在 JMX 中， 使用 MBean 或 MXBean 来表示一个资源（下面简称 MBean），访问和管理资源也都是通过 MBean。</p><p></p><p>JMX 已经对 JVM 进行了多维度资源检测，所以可以轻松启动 JMX 代理来访问内置的 JVM 资源检测，从而通过 JMX 技术远程监控和管理 JVM。JMX 内置了常用的 JVM 资源监测类，包括但不限于：类加载、垃圾收集、日志系统、内存池、内存、内存系统、操作系统、运行时系统和线程系统。</p><p></p><p>以下代码演示了如何使用原生 JMX 接口获取 JVM 指标：</p><p><code lang=\"text\">package org.example.jmx;\n\nimport java.lang.management.CompilationMXBean;\nimport java.lang.management.GarbageCollectorMXBean;\nimport java.lang.management.ManagementFactory;\nimport java.lang.management.MemoryMXBean;\nimport java.lang.management.MemoryManagerMXBean;\nimport java.lang.management.MemoryUsage;\nimport java.lang.management.OperatingSystemMXBean;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class Main {\n\n    public static void main(String[] args) {\n        printOSInfo();\n        System.out.println(\"================\");\n        printMemoryManagerInfo();\n        System.out.println(\"================\");\n        printGCInfo();\n    }\n\n    //通过 OperatingSystemMXBean 获取 OS 信息\n    public static void printOSInfo() {\n        OperatingSystemMXBean operatingSystemMXBean = ManagementFactory.getOperatingSystemMXBean();\n        String osName = operatingSystemMXBean.getName();\n        String osVersion = operatingSystemMXBean.getVersion();\n        int processors = operatingSystemMXBean.getAvailableProcessors();\n        System.out.println(String.format(\"OS：%s，Version：%s，CPU：%d 个\", osName, osVersion, processors));\n\n        CompilationMXBean compilationMXBean = ManagementFactory.getCompilationMXBean();\n        String compilationMXBeanName = compilationMXBean.getName();\n        System.out.println(\"编译系统：\" + compilationMXBeanName);\n\n        MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean();\n        MemoryUsage heapMemoryUsage = memoryMXBean.getHeapMemoryUsage();\n        long max = heapMemoryUsage.getMax();\n        long used = heapMemoryUsage.getUsed();\n        System.out.println(String.format(\"使用内存：%dMB/%dMB\", used / 1024 / 1024, max / 1024 / 1024));\n\n        List gcMXBeans = ManagementFactory.getGarbageCollectorMXBeans();\n        String gcNames = gcMXBeans.stream()\n                                  .map(MemoryManagerMXBean::getName)\n                                  .collect(Collectors.joining(\",\"));\n        System.out.println(\"垃圾收集器：\" + gcNames);\n    }\n\n    // 通过 MemoryManagerMXBean 获取 JVM 内存管理器信息\n    public static void printMemoryManagerInfo() {\n        List managers = ManagementFactory.getMemoryManagerMXBeans();\n        if (managers != null &amp;&amp; !managers.isEmpty()) {\n            for (MemoryManagerMXBean manager : managers) {\n                System.out.println(\"vm内存管理器：名称=\" + manager.getName() + \",管理的内存区=\"\n                                       + Arrays.deepToString(\n                    manager.getMemoryPoolNames()) + \",ObjectName=\" + manager.getObjectName());\n            }\n        }\n    }\n\n    //通过 GarbageCollectorMXBean 获取 JVM GC 信息\n    public static void printGCInfo() {\n        try {\n            List gcMxBeans = ManagementFactory.getGarbageCollectorMXBeans();\n\n            for (GarbageCollectorMXBean gcMxBean : gcMxBeans) {\n                System.out.println(gcMxBean.getName());\n                System.out.println(gcMxBean.getObjectName());\n            }\n\n        } catch (RuntimeException re) {\n            throw re;\n        } catch (Exception exp) {\n            throw new RuntimeException(exp);\n        }\n    }\n}</code></p><p></p><p>以上代码运行结果如下：</p><p></p><p><code lang=\"text\">OS：Mac OS X，Version：10.16，CPU：8 个\n编译系统：HotSpot 64-Bit Tiered Compilers\n使用内存：4MB/4096MB\n垃圾收集器：G1 Young Generation,G1 Old Generation\n================\nvm内存管理器：名称=CodeCacheManager,管理的内存区=[CodeHeap 'non-nmethods', CodeHeap 'profiled nmethods', CodeHeap 'non-profiled nmethods'],ObjectName=java.lang:type=MemoryManager,name=CodeCacheManager\nvm内存管理器：名称=Metaspace Manager,管理的内存区=[Metaspace, Compressed Class Space],ObjectName=java.lang:type=MemoryManager,name=Metaspace Manager\nvm内存管理器：名称=G1 Young Generation,管理的内存区=[G1 Eden Space, G1 Survivor Space, G1 Old Gen],ObjectName=java.lang:type=GarbageCollector,name=G1 Young Generation\nvm内存管理器：名称=G1 Old Generation,管理的内存区=[G1 Eden Space, G1 Survivor Space, G1 Old Gen],ObjectName=java.lang:type=GarbageCollector,name=G1 Old Generation\n================\nG1 Young Generation\njava.lang:type=GarbageCollector,name=G1 Young Generation\nG1 Old Generation\njava.lang:type=GarbageCollector,name=G1 Old Generation</code></p><p></p><p></p><h3>常见开源暴露 JVM 指标工具</h3><p></p><p></p><p>通过原生 JMX 获取 JVM 指标，并能够被我们的可观测性系统采集到工作量还是很大。以下，列举了常见的几款开箱即用的工具：</p><p></p><p></p><h4>Micrometer/Spring Boot Actuator</h4><p></p><p></p><p>Spring Boot Actuator 模块提供了生产级别的功能，比如健康检查，审计，指标收集，HTTP 跟踪等，帮助我们监控和管理Spring Boot 应用。这个模块是一个采集应用内部信息暴露给外部的模块，上述的功能都可以通过HTTP 和 JMX 访问。Spring Boot Actuator 使用 Micrometer 与这些外部应用程序监视系统集成，Spring Boot 应用只需很少的配置即可轻松集成外部的监控系统。</p><p></p><p>以下是 Micrometer 官方介绍：</p><p></p><p>Micrometer provides a simple facade over the instrumentation clients for the most popular observability systems, allowing you to instrument your JVM-based application code without vendor lock-in. Think SLF4J, but for application observability! Data recorded by Micrometer are intended to be used to observe, alert, and react to the current/recent operational state of your environment.</p><p></p><p>在 pom.xml 中添加如下依赖：</p><p><code lang=\"text\">\n   org.springframework.boot\n   spring-boot-starter-actuator\n \n \n   io.micrometer\n   micrometer-registry-prometheus\n   runtime\n </code></p><p></p><p>在 src/main/resources/application.properties 中添加相关配置</p><p></p><p><code lang=\"text\">management.endpoints.web.exposure.include=*\nmanagement.endpoints.web.exposure.include=prometheus,health,info,metric\nmanagement.endpoints.web.base-path=/\n\nmanagement.server.port=8999\nmanagement.health.probes.enabled=true\nmanagement.endpoint.health.show-details=always\nmanagement.endpoint.prometheus.enabled=true</code></p><p></p><p>重新构建并运行服务</p><p></p><p>运行服务之后，我们可以通过 http://lcoalhost:8999/prometheus 访问服务暴露出来的 OpenMetrics 格式的指标。同时，该接口也能被 prometheus 抓取并收集指标， prometheus.yaml 配置文件参考如下:</p><p></p><p><code lang=\"text\">prometheus:\n config:\n   scrape_configs:\n     - job_name: \"jvm-metrics\"\n       scrape_interval: 10s\n       metrics_path: \"/prometheus\"\n       static_configs:\n         - targets: [\"\"]</code></p><p></p><h4>Prometheus JMX Exporter</h4><p></p><p></p><p>JMX-Exporter 主要通过连接到 Java 原生指标收集系统 JMX，并将指标转换为 Prometheus 可以理解的格式。JMX-Exporter 提供了两种采集并暴露指标的用法:</p><p></p><p>启动独立进程。JVM 启动时指定参数，暴露 JMX 的 RMI 接口，JMX-Exporter 调用 RMI 获取 JVM 运行时状态数据，转换为 Prometheus metrics 格式，并暴露端口让 Prometheus 采集JVM 进程内启动(in-process)。JVM 启动时指定参数，通过 javaagent 的形式运行 JMX-Exporter 的 jar 包，进程内读取 JVM 运行时状态数据，转换为 Prometheus metrics 格式，并暴露端口让 Prometheus 采集。</p><p></p><p>官方不推荐使用第一种方式，一方面配置复杂，另一方面因为它需要一个单独的进程，而这个进程本身的监控又成了新的问题，所以本文重点围绕第二种用法以及如何在 Kubernetes 环境下使用 JMX Exporter 暴露 JVM 监控指标。</p><p></p><p>由于在应用启动时要通过 javaagent 指定 jmx exporter Jar 包，因此，我们需要考虑启动时如何让 JVM 正确的找到 jmx exporter Jar 包。以下列举了两种方式：</p><p></p><p>在构建应用镜像时将 JAR 文件打进镜像</p><p></p><p>根据 JMX EXporter 官方介绍，需要为其指定配置文件，需要提前创建好 prometheus-jmx-config.yaml , 更多配置项请参考官方文档， 以下内容仅供参考：</p><p></p><p><code lang=\"text\">lowercaseOutputLabelNames: true\nlowercaseOutputName: true\nwhitelistObjectNames: [\"java.lang:type=OperatingSystem\"]\nblacklistObjectNames: []\nrules:\n  - pattern: 'java.lang&lt;&gt;(committed_virtual_memory|free_physical_memory|free_swap_space|total_physical_memory|total_swap_space)_size:'\n    name: os_$1_bytes\n    type: GAUGE\n    attrNameSnakeCase: true\n  - pattern: 'java.lang&lt;&gt;((?!process_cpu_time)\\w+):'\n    name: os_$1\n    type: GAUGE\n    attrNameSnakeCase: true</code></p><p></p><p>通过在应用 Dockerfile 中将 JAR 拷贝进镜像或者在线下载 JAR 文件实现，以下 Dockerfile 仅供参考：</p><p></p><p><code lang=\"text\">FROM openjdk:11.0.15-jre\n\nWORKDIR /app/\n\nCOPY target/my-app.jar ./\n# 将前面创建的 config 文件拷贝至镜像\nCOPY prometheus-jmx-config.yaml ./\n\n# 在线下载 jmx prometheus javaagent jar\nRUN set -ex; \\\n    curl -L -O https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.17.2/jmx_prometheus_javaagent-0.17.2.jar;\n\n# JAVA_TOOL_OPTIONS 会被 JVM 调用\nENV JAVA_TOOL_OPTIONS=-javaagent:/app/jmx_prometheus_javaagent-0.17.2.jar=8088:/app/prometheus-jmx-config.yaml\n\nEXPOSE 8081 8999 8080 8888\n\nENTRYPOINT java $JAVA_OPTS -jar my-app.jar</code></p><p></p><p>b.  通过 Kubernetes Init Container 在应用启动之前将 JAR 文件挂载进容器</p><p></p><p>我们需要先将 JMX exporter  做成 Docker 镜像, 以下 Dockerfile 仅供参考：</p><p></p><p><code lang=\"text\">FROM alpine/curl:3.14\n\nWORKDIR /app/\n# 将前面创建的 config 文件拷贝至镜像\nCOPY prometheus-jmx-config.yaml ./\n\n# 在线下载 jmx prometheus javaagent jar\nRUN set -ex; \\\n    curl -L -O https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.17.2/jmx_prometheus_javaagent-0.17.2.jar;</code></p><p></p><p>根据上面 Dockerfile 构建镜像：docker build -t my-jmx-exporter .</p><p></p><p>在 kubernetes 编排文件中使用该镜像，并将 JAR 通过共享目录挂载进容器：</p><p><code lang=\"text\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-demo-app\n  labels:\n    app: my-demo-app\nspec:\n  selector:\n    matchLabels:\n      app: my-demo-app\n  template:\n    metadata:\n      labels:\n        app: my-demo-app\n    spec:\n      imagePullSecrets:\n      - name: registry-pull\n      initContainers:\n      - name: jmx-sidecar\n        image: my-jmx-exporter\n        command: [\"cp\", \"-r\", \"/app/jmx_prometheus_javaagent-0.17.2.jar\", \"/target/jmx_prometheus_javaagent-0.17.2.jar\"]  ➊\n        volumeMounts:\n        - name: sidecar\n          mountPath: /target\n      containers:\n      - image: my-demo-app-image\n        name: my-demo-app\n        resources:\n          requests:\n            memory: \"1000Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1000Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 18083\n        env:\n        - name: JAVA_TOOL_OPTIONS\n          value: \"-javaagent:/app/jmx_prometheus_javaagent-0.17.2.jar=8088:/app/prometheus-jmx-config.yaml\" ➋\n        volumeMounts:\n        - name: host-time\n          mountPath: /etc/localtime\n          readOnly: true\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: host-time\n        hostPath:\n          path: /etc/localtime\n      - name: sidecar  #共享agent文件夹\n        emptyDir: {}\n      restartPolicy: Always</code></p><p></p><p>经过如上的改造之后， my-demo-app 具备了暴露 JVM 指标的能力，运行服务之后，我们可以通过 http://lcoalhost:8088 访问服务暴露出来的 prometheus 格式的指标。</p><p></p><p>同时，该接口也能被 prometheus 抓取并收集指标， prometheus.yaml 配置文件参考如下:</p><p><code lang=\"text\">prometheus:\n config:\n   scrape_configs:\n     - job_name: \"jvm-metrics\"\n       scrape_interval: 10s\n       metrics_path: \"/\"\n       static_configs:\n         - targets: [\"IP of the demo app:8088&gt;\"]\n</code></p><p></p><p></p><h3>OpenTelemetry Java Agent</h3><p></p><p></p><p>在 Opentelemetry Agent v1.20.0 及以上版本中，Opentelemetry Agent 新增了 JMX Metric Insight 模块，如果你的应用已经集成了 Opentelemetry Agent 去采集应用链路，那么你不再需要另外引入其他 Agent 去为我们的应用暴露 JMX 指标。Opentelemetry Agent 也是通过检测应用程序中本地可用的 MBean 公开的指标，对其进行收集并暴露指标。</p><p></p><p>Opentelemetry Agent 也针对常见的 Java Server 或框架内置了一些监控的样例，请参考 预定义的指标。</p><p></p><p>使用 OpenTelemetry Java Agent 同样需要考虑如何将 JAR 挂载进容器，除了可以参考上面 JMX Exporter 挂载 JAR 文件的方式外，我们还可以借助 Opentelemetry 提供的 Operator 的能力来实现自动为我们的应用开启 JVM 指标暴露：</p><p></p><p>在 Kubernetes 中安装 Opentelemetry Operator</p><p><code lang=\"text\">kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml</code></p><p></p><p>安装 Instrumentations 资源</p><p></p><p>一旦上面的 operator 安装就绪之后，我们就可以使用其内置的 Instrumentations 资源，如下文件告诉了 Operator 后续为我们应用注入 sidecar 的时候的一些差异化行为，比如文件中针对 Java 应用定义了两个环境变量，该环境变量是将 Opentelemetry Java Agent 的指标以 Prometheus 导出器将指标通过 9464 端口暴露出来。</p><p><code lang=\"text\">kubectl apply -f - &lt;<=\"\" code=\"\"></code></p><p></p><p><code lang=\"text\">为应用注入 sidecar</code></p><p></p><p><code lang=\"text\">上面我们创建了一个 Instrumentation 资源，只是申明了 Operator 注入 sidecar 的时候的模版，我们还得需要告诉 Operator 哪些应用需要注入这个 sidecar。假设需要给名字为 my-demo-app 的 Deployment 注入 sidecar，需要在 spec.annotations 下添加 instrumentation.opentelemetry.io/inject-java: \"default/opentelemetry-autoinstrumentation\"注解：</code></p><p><code lang=\"text\"><code lang=\"text\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-demo-app\n  labels:\n    app: my-demo-app\nspec:\n  selector:\n    matchLabels:\n      app: my-demo-app\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: my-demo-app\n      annotations:\n# 在 spec.annotations 下添加该注解\n        instrumentation.opentelemetry.io/inject-java: \"default/opentelemetry-autoinstrumentation\"\n    spec:\n      containers:\n      - name: my-demo-app\n        image: my-jmx-exporter\n        ports:\n          - containerPort: 8080\n            protocol: TCP</code></code></p><p></p><p><code lang=\"text\">最终生成的 YAML 内容如下：</code></p><p><code lang=\"text\"><code lang=\"text\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-demo-app-565bd877dd-nqkk6\n  generateName: my-demo-app-565bd877dd-\n  namespace: default\n  uid: aa89ca0d-620c-4d20-8bc1-37d67bad4ea4\n  resourceVersion: '2668986'\n  creationTimestamp: '2022-04-08T05:58:48Z'\n  labels:\n    app: my-demo-app\n    pod-template-hash: 565bd877dd\n  annotations:\n    instrumentation.opentelemetry.io/inject-java: 'true'\n    sidecar.opentelemetry.io/inject: 'false'\nspec:\n  volumes:\n    - name: opentelemetry-auto-instrumentation\n      emptyDir: {}\n  initContainers:\n    - name: opentelemetry-auto-instrumentation\n      image: &gt;-\n        ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java\n      command:\n        - cp\n        - /javaagent.jar\n        - /otel-auto-instrumentation/javaagent.jar\n      volumeMounts:\n        - name: opentelemetry-auto-instrumentation\n          mountPath: /otel-auto-instrumentation\n  containers:\n    - name: my-demo-app\n      image: my-jmx-exporter\n      env:\n- name: OTEL_METRICS_EXPORTER\n        value: \"prometheus\"\n      - name: OTEL_METRICS_EXPORTER_PORT\n        value: \"9464\"\n        - name: JAVA_TOOL_OPTIONS\n          value: ' -javaagent:/otel-auto-instrumentation/javaagent.jar'\n        - name: OTEL_TRACES_EXPORTER\n          value: otlp\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: http://opentelemetry-collector.svc.cluster.local:4317\n        - name: OTEL_SERVICE_NAME\n          value: my-demo-app\n        - name: OTEL_RESOURCE_ATTRIBUTES_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OTEL_RESOURCE_ATTRIBUTES_POD_UID\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.uid\n        - name: OTEL_RESOURCE_ATTRIBUTES_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: OTEL_RESOURCE_ATTRIBUTES\n          value: &gt;-\n            k8s.container.name=my-demo-app,k8s.deployment.name=my-demo-app,k8s.deployment.uid=8de6929d-dda0-436c-bca1-604e9ca7ea4e,k8s.namespace.name=default,k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.pod.uid=$(OTEL_RESOURCE_ATTRIBUTES_POD_UID),k8s.replicaset.name=my-deployment-with-sidecar-565bd877dd,k8s.replicaset.uid=190d5f6e-ba7f-4794-b2e6-390b5879a6c4\n        - name: OTEL_PROPAGATORS\n          value: jaeger,b3\n      volumeMounts:\n        - name: opentelemetry-auto-instrumentation\n          mountPath: /otel-auto-instrumentation\n      terminationMessagePath: /dev/termination-log\n      terminationMessagePolicy: File\n      imagePullPolicy: Always\n  restartPolicy: Always\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n</code></code></p><p></p><p><code lang=\"text\">经过如上的改造之后， my-demo-app 具备了暴露 JVM 指标的能力，运行服务之后，我们可以通过 http://lcoalhost:9464 访问服务暴露出来的 prometheus 格式的指标。同时，该接口也能被 prometheus 抓取并收集指标， prometheus.yaml 配置文件参考如下:</code></p><p><code lang=\"text\"><code lang=\"text\">prometheus:\n config:\n   scrape_configs:\n     - job_name: \"otel-jvm-metrics\"\n       scrape_interval: 10s\n       metrics_path: \"/\"\n       static_configs:\n         - targets: [\"<=\"\" code=\"\"></code></code></p><p></p><p></p><h2><code lang=\"text\"><code lang=\"text\">使用 Prometheus 和 Grafana 统一 监控 JVM</code></code></h2><p></p><p></p><p><code lang=\"text\"><code lang=\"text\">前面介绍了几种开源暴露 JVM 指标的工具，通过上面几款工具，可以使我们的 Java 应用具备 JVM 暴露能力，接下来我们就需要考虑如何将这些指标集中采集存储并展示分析。本文主要介绍 Prometheus 和 Grafana 的方案并以使用 Opentelemetry Java Agent 作为 JVM 指标暴露为例：</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">查看 Java 应用运行日志，加载 OTEL Agent：</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/be/be8e4d543308f917f48cce72340052f3.png\" /></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">以下 Prometheus Scrape 抓取指标配置文件，与前面 Opentelemetry Java Agent 章节末尾提供的 prometheus.yaml 配置文件一致 ：</code></code></p><p><code lang=\"text\"><code lang=\"text\"><code lang=\"text\">prometheus:\n config:\n   scrape_configs:\n     - job_name: \"otel-jvm-metrics\"\n       scrape_interval: 10s\n       metrics_path: \"/\"\n       static_configs:\n         - targets: [\"\"]</code></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">以下是 OTEL Java Agent 通过 JMX 采集并暴露出来的部分指标：</code></code></p><p><code lang=\"text\"><code lang=\"text\"><code lang=\"text\"># TYPE target info\n# HELP target Target metadata\ntarget_info{container_id=\"3da5e8bc10cf207f01e0373a82c7bb3f7b8c430a757b6412f2199967a7971f9a\",host_arch=\"amd64\",host_name=\"3da5e8bc10cf\",os_description=\"Linux 5.10.104-linuxkit\",os_type=\"linux\",process_command_line=\"/usr/local/openjdk-11/bin/java -javaagent:/app/opentelemetry-javaagent.jar -Dspring.cloud.nacos.config.enabled=false -Dspring.randomError=false -Dotel.jmx.config=/app/jmx_config_file.yaml -Dotel.metrics.exporter=prometheus -Dotel.exporter.prometheus.port=9464\",process_executable_path=\"/usr/local/openjdk-11/bin/java\",process_pid=\"7\",process_runtime_description=\"Oracle Corporation OpenJDK 64-Bit Server VM 11.0.15+10\",process_runtime_name=\"OpenJDK Runtime Environment\",process_runtime_version=\"11.0.15+10\",service_name=\"adservice-springcloud\",telemetry_auto_version=\"1.22.1\",telemetry_sdk_language=\"java\",telemetry_sdk_name=\"opentelemetry\",telemetry_sdk_version=\"1.22.0\"} 1\n# TYPE otel_scope_info info\n# HELP otel_scope_info Scope metadata\notel_scope_info{otel_scope_name=\"io.opentelemetry.micrometer-1.5\"} 1\n# TYPE otel_scope_info info\n# HELP otel_scope_info Scope metadata\notel_scope_info{otel_scope_name=\"io.opentelemetry.runtime-metrics\",otel_scope_version=\"1.22.1-alpha\"} 1\n# TYPE executor_pool_core gauge\n# HELP executor_pool_core The core number of threads for the pool\nexecutor_pool_core{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",name=\"applicationTaskExecutor\"} 8.0 1676368076291\n# TYPE logback_events_total counter\n# HELP logback_events_total Number of events that made it to the logs\nlogback_events_total{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",level=\"info\"} 12.0 1676368076291\nlogback_events_total{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",level=\"warn\"} 3.0 1676368076291\n# TYPE jvm_gc_live_data_size gauge\n# HELP jvm_gc_live_data_size Size of long-lived heap memory pool after reclamation\njvm_gc_live_data_size{otel_scope_name=\"io.opentelemetry.micrometer-1.5\"} 2.7941376E7 1676368076291\n# TYPE executor_pool_max gauge\n# HELP executor_pool_max The maximum allowed number of threads in the pool\nexecutor_pool_max{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",name=\"applicationTaskExecutor\"} 2.147483647E9 1676368076291\n# TYPE disk_total gauge\n# HELP disk_total Total space for path\ndisk_total{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",path=\"/app/.\"} 4.2006183936E10 1676368076291\n# TYPE executor_active gauge\n# HELP executor_active The approximate number of threads that are actively executing tasks\nexecutor_active{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",name=\"applicationTaskExecutor\"} 0.0 1676368076291\n# TYPE disk_free gauge\n# HELP disk_free Usable space for path\ndisk_free{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",path=\"/app/.\"} 1.8299908096E10 1676368076291\n# TYPE system_cpu_count gauge\n# HELP system_cpu_count The number of processors available to the Java virtual machine\nsystem_cpu_count{otel_scope_name=\"io.opentelemetry.micrometer-1.5\"} 2.0 1676368076291\n# TYPE jvm_memory_committed gauge\n# HELP jvm_memory_committed The amount of memory in bytes that is committed for the Java virtual machine to use\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"heap\",id=\"G1 Old Gen\"} 6.6060288E7 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"nonheap\",id=\"Compressed Class Space\"} 9175040.0 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"nonheap\",id=\"CodeHeap 'non-profiled nmethods'\"} 4653056.0 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"nonheap\",id=\"CodeHeap 'profiled nmethods'\"} 1.8087936E7 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"nonheap\",id=\"Metaspace\"} 6.422528E7 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"heap\",id=\"G1 Survivor Space\"} 7340032.0 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"heap\",id=\"G1 Eden Space\"} 1.00663296E8 1676368076291\njvm_memory_committed{otel_scope_name=\"io.opentelemetry.micrometer-1.5\",area=\"nonheap\",id=\"CodeHeap 'non-nmethods'\"} 2555904.0 1676368076291\n......</code></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">在 Grafana 中导入 Opentelemetry JVM Metrics DashBoard( ID=8563 ）</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">DashBoard 根据 Prometheus 采集到的指标，绘制出了我们常用的一些指标面板，比如 CPU 和内存负载情况，线程运行状况等等，也可以基于该 DashBoard 进行自定义。图片: h</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/7a/7ab42060dea9e0f1c58072c8e3300e09.png\" /></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/a5/a5531b9052ac8be170d289fd44ffc231.png\" /></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/eb/eb6e5a87d7e75b28dc1680139367de85.png\" /></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/7b/7b03aef32d88ae14502273a3e32880c5.png\" /></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">该部分的 Demo 文件可以参考底部参考部分的 jmx-jvm-demo</code></code></p><p></p><p></p><h2><code lang=\"text\"><code lang=\"text\">总结：</code></code></h2><p></p><p></p><p><code lang=\"text\"><code lang=\"text\">本文介绍了 JVM 监控的技术原理，同时介绍了常见的几种开源暴露 JVM 指标的工具。从接入方式来看主要分为两类，第一类是像 Spring Boot Actuator 需要客户应用做一些改造的方式，另一类是 JMX Exporter 和 Opentelemetry Java Agent 无侵入的方式。</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">以下针对这三种方案进行了多维度比较：</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/ed/ed86dc4b41132879dc251ffd4f82bc00.png\" /></code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">可以根据业务自身情况选择使用哪一种方式，针对运行在非容器平台的 Spring Boot 应用，建议选择 Spring Boot Actuator 方式，该方式能够更好的和 Spring Boot 生态集成。</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">如果业务应用运行在容器平台，建议选择 Opentelemetry Java Agent 的方式，该方式支持 Operator 的方式，能够以最小代价对 Java 应用进行监控，同时也方便后续对 Agent 版本的升级与运维。如果应用目前已经通过 Opentelemetry Java Agent 实现了链路采集，那么更加建议使用 Operator 云原生的方式。</code></code></p><p></p><p></p><h2><code lang=\"text\"><code lang=\"text\">参考：</code></code></h2><p></p><p></p><p><code lang=\"text\"><code lang=\"text\">jmx demo (https://gist.github.com/JaredTan95/8711878369b9b12c772dd783a21c621d)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">Micrometer (https://micrometer.io/)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">Spring Boot Actuator (https://github.com/spring-projects/spring-boot/tree/main/spring-boot-project/spring-boot-actuator)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">Prometheus JMX Exporter (https://github.com/prometheus/jmx_exporter)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">Zabbix JMX （https://www.zabbix.com/integrations/java_monitoring）</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">OpenTelemetry Java Agent(https://opentelemetry.io/blog/2023/jmx-metric-insight/)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">JMX Metric Insight (https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/instrumentation/jmx-metrics/javaagent/README.md)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">Opentelemetry Operator (https://github.com/open-telemetry/opentelemetry-operator)</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">jmx-jvm-demo (https://github.com/openinsight-proj/jmx-jvm-demo)</code></code></p><p></p><p></p><h3><code lang=\"text\"><code lang=\"text\">作者介绍</code></code></h3><p></p><p></p><p><code lang=\"text\"><code lang=\"text\">谭建，「DaoCloud 」可观测性技术专家。负责研发新一代云原生操作系统底座，DCE 5.0 社区版 https://docs.daocloud.io/download/dce5/</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\">欢迎大家关注“OpenTelemetry” 公众号，这是中国区唯一官方技术公众号想加入技术社群和参与活动的朋友联系我</code></code></p><p></p><p><code lang=\"text\"><code lang=\"text\"><img src=\"https://static001.geekbang.org/infoq/95/9557c5bef779936148762d7141c30d44.png\" /></code></code></p><p></p>",
    "publish_time": "2023-03-23 09:12:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]