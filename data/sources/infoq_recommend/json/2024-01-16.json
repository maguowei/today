[
  {
    "title": "Roblox 如何从 73 小时故障中吸取教训，为 7 千万用户提供可靠服务",
    "url": "https://www.infoq.cn/article/rBofrx6EO9VkId1mmmO3",
    "summary": "<p>在线游戏平台和创作系统 Roblox 详细介绍了他们如何提升其基础设施的效率和弹性，满足 7 千多万活跃用户参与沉浸式游戏体验的需求。这篇博&nbsp;深入探讨了 Roblox 如何实现可靠性承诺、如何应对 2021 年的重大故障以及如何通过持续转型来提升基础设施的效率和弹性。</p><p></p><p>2021 年 10 月，Roblox 遭遇了持续 73 小时的系统范围的故障，这是由一个数据中心的一个小问题引发的，然后迅速演变成大规模故障。通过事故后分析，团队加大了巩固其基础设施的工作，以应对各种故障因素，如流量峰值、天气条件、硬件故障、软件错误和人为失误。重点是防止单个组件的问题扩散到整个系统，并确保网络或用户持续重试操作不会造成与负载相关的级联故障。</p><p></p><p>为了应对类似 2021 年 10 月的故障，Roblox 最初在不同区域的数据中心采用主备方式构建了基础设施的副本。这意味着团队可以在主数据中心出现重大故障时将整个系统切换到备份基础设施上。这提供了一种应急的弹性形式，但他们的长期目标是从主备数据中心转变为双活，让两个数据中心同时处理工作负载，实现更高的可靠性和几乎即时的故障切换。</p><p></p><p>Roblox 还实现了蜂窝基础设施架构，在数据中心内建立强大的“防爆墙”，防止发生整个数据中心范围的故障。蜂窝蜂窝单元或机器集群在单个蜂窝单元内提供冗余和故障控制。Roblox 的目标是将所有服务迁移到蜂窝单元中，以此来增强弹性和高效的工作负载管理，整个蜂窝单元（每个蜂窝单元可能包含 1400 台服务器）可以在必要时进行修复或完全重新配置。这个过程需要确保一致性，要求服务进行容器化，并实现基础设施即代码的理念。Roblox 新的部署工具会自动确保服务跨蜂窝单元分布，从而使服务所有者不必考虑复制问题。</p><p></p><p>Roblox 将蜂窝单元作为一种防火门，可以将故障限制在一个蜂窝单元内。目标是使蜂窝单元变得可互换，以便在出现问题时更快地恢复。然而，管理蜂窝单元之间的通信存在一些挑战，因为需要防止“死亡查询”，即重试查询会导致级联故障。他们正在部署短期解决方案，例如将计算服务的副本部署到每个计算蜂窝单元中，并在蜂窝单元间平衡流量，以此来缓解这种情况。他们的长期计划包括实现用于服务发现的下一代服务网格以及将依赖请求定向到与原始调用方相同蜂窝单元的方法。这将降低故障从一个蜂窝单元传播到另一个蜂窝单元的风险。70% 的后端流量现在由蜂窝单元提供，他们的最终目标是达到 100%。近 3 万台服务器正在运行蜂窝单元，但这还不到总服务器数量的 10%。</p><p></p><p>在不中断用户的情况下迁移一个非常繁忙的在线平台的复杂性是巨大的。由于没有大量的资金购买全新的服务器来运行蜂窝基础设施，Roblox 创造性地利用了一小部分备用机器，并策略性地建立了新的蜂窝单元，逐步迁移工作负载，然后重新使用已释放的机器来进行下一次迁移。这在不同的数据中心之间造成了一些理想的蜂窝单元碎片，增加了蜂窝单元内的弹性。Roblox 预计将于 2025 年完成迁移，他们需要强大的工具来部署均衡的服务，并且不会干扰到用户，他们还需要进行详尽的测试，确保在蜂窝架构中运行的新服务的兼容性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b6/b6531b9e6c0757e1b080c3fffbd9e8e4.png\" /></p><p></p><p>Roblox 的努力取得了成功，但针对蜂窝单元的工作仍在进行中。他们致力于在不断扩展规模的过程中提高效率和弹性。他们的主要成就包括建立第二个数据中心，在主备数据中心创建蜂窝单元，将超过 70％的后端服务流量迁移到蜂窝单元中，以及建立了实现一致性的要求。2023 年 9 月，Roblox 在数据中心启动了双活实验，增强了可靠性并最大限度地缩短故障转移时间。这些成果让他们获得了一个实现全面双活基础设施的计划，确定了改进系统设计的模式。他们一直致力于提升效率和弹性，设想让平台成为数百万用户可靠、高性能的实用工具，并实现实时连接十亿人。</p><p></p><p>他们的基础设施现在运行在近 14 万 5 千台服务器上（大部分在本地私有混合云中心）——两年内增加了三倍。Roblox 目前正在努力改造基础设施，使平台更具弹性，更加高效，为数百万用户提供服务，为持续的增长和创新奠定基础。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/roblox-cellular-infrastructure/\">https://www.infoq.com/news/2024/01/roblox-cellular-infrastructure/</a>\"</p>",
    "publish_time": "2024-01-16 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "MySQL 支持 JavaScript，目前处于预览阶段",
    "url": "https://www.infoq.cn/article/6bT6kDa1ADuCKeTU4VlI",
    "summary": "<p>最近，Oracle 宣布 MySQL 数据库服务器支持 JavaScript 函数和过程。用于存储过程的 JavaScript 目前处于预览阶段，仅适用于 MySQL 企业版和 MySQL Heatwave。</p><p></p><p>引入 JavaScript 支持让开发人员能够在数据库中实现高级的数据处理逻辑。通过最小化数据库服务器和客户端应用程序之间的数据移动，存储函数和过程可以降低延迟、网络开销和向外流量成本。Oracle 高级首席软件工程师 Øystein Grøvlen 和技术咨询委员会成员 Farhan Tauheed 写道：</p><p></p><p></p><blockquote>支持 JavaScript 存储程序不仅可以利用庞大的生态系统来提高开发人员的生产力，而且将会有更多的开发人员具备编写存储程序所需的必要技能。换句话说，开发企业现在可以利用广泛可用的 JavaScript 技能集进行后端开发，吸纳更多的开发人才。</blockquote><p></p><p></p><p>在常见的新特性应用场景中，Oracle 强调了数据提取、数据格式化、近似搜索、数据验证、压缩、编码和数据转换，得到了社区的积极响应。发布公告中提供了一个示例函数，JavaScript 代码直接被嵌入到 SQL 定义中：</p><p></p><p><code lang=\"sql\">CREATE FUNCTION gcd_js (a INT, b INT) RETURNS INT\nLANGUAGE JAVASCRIPT AS $$\n  let [x, y] = [Math.abs(a), Math.abs(b)];\n  while(y) [x, y] = [y, x % y];\n  return x;\n$$;\n</code></p><p></p><p>来源：Oracle 博客</p><p></p><p>当使用传统的 CALL 语句调用函数时，SQL 类型和 JavaScript 类型之间会发生隐式类型转换。文档 提到，对 JavaScript 的支持是基于 ECMAScript 2021 标准，并支持所有整数、浮点数和CHAR/VARCHAR* 类型变体。Grøvlen 和 Tauheed 补充道：</p><p></p><p></p><blockquote>为实现最佳的端到端性能，MySQL 和 JavaScript 的集成使用了自定义 VM，基于 GraalVM 的 ahead-of-time（AOT）编译，将语言编译成本机二进制表示，实现快速的处理。GraalVM 有自己的 ECMAScript 2021 标准 JavaScript 实现。尽管它是用 GraalVM 的 Polyglot 框架实现的，但在性能方面仍然具有竞争力。</blockquote><p></p><p></p><p>GraalVM 运行时包括 JDK、语言实现（JavaScript、R、Python、Ruby 和 Java）以及具有沙箱功能和工具支持的托管虚拟机。MySQL-JavaScript 可在 MySQL 企业版和 MySQL Heatwave 云服务（OCI、AWS 和 Azure）中使用，但 MySQL 社区版不提供支持。</p><p></p><p>MySQL 并不是第一个支持在存储过程中使用 JavaScript 的开源关系型数据库，PostgreSQL 使用了最为流行的 JavaScript 语言扩展 PLV8。PostgreSQL 当前所有版本均支持 PLV8，包括像 Amazon RDS 这样的托管服务，并可用在存储过程和触发器中。</p><p></p><p>Oracle 在 YouTube 发布了三个 MySQL HeatWave 视频，演示了如何使用 JavaScript 存储程序来运行 Mustache 库、验证 Web 表单输入 和 处理 Web URL。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.infoq.com/news/2024/01/mysql-javascript-procedures/\">https://www.infoq.com/news/2024/01/mysql-javascript-procedures/</a>\"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p><p></p><p></p><p></p><p></p>",
    "publish_time": "2024-01-16 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "对标OpenAI GPT-4，MiniMax 国内首个 MoE 大语言模型全量上线",
    "url": "https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0",
    "summary": "<p>1月16日，InfoQ获悉，经过了半个月的部分客户的内测和反馈，MiniMax 全量发布大语言模型 abab6，该模型为国内首个 MoE（Mixture-of-Experts）大语言模型。</p><p>&nbsp;</p><p>早在上个月举办的数字中国论坛成立大会暨数字化发展论坛的一场分论坛上，MiniMax副总裁魏伟就曾透露将于近期发布国内首个基于MoE架构的大模型，对标OpenAI GPT-4。</p><p>&nbsp;</p><p>在 MoE 结构下，abab6 拥有大参数带来的处理复杂任务的能力，同时模型在单位时间内能够训练足够多的数据，计算效率也可以得到大幅提升。改进了 abab5.5 在处理更复杂、对模型输出有更精细要求场景中出现的问题。</p><p></p><h2>为什么选择 MoE 架构？</h2><p></p><p>&nbsp;</p><p>那么，MoE到底是什么？MiniMax的大模型为何要使用使用 MoE 架构？</p><p>&nbsp;</p><p>MoE架构全称专家混合（Mixture-of-Experts），是一种集成方法，其中整个问题被分为多个子任务，并将针对每个子任务训练一组专家。MoE模型将覆盖不同学习者（专家）的不同输入数据。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/4d/4d1c3880f8e55a33e9aadcc3b06685c4.png\" /></p><p>图片来源：<a href=\"https://arxiv.org/pdf/1701.06538.pdf\">https ://arxiv.org/pdf/1701.06538.pdf</a>\"</p><p>&nbsp;</p><p>有传闻称，GPT-4也采用了相同的架构方案。</p><p>&nbsp;</p><p>2023 年 4 月，MiniMax 发布了开放平台。过去半年多，MiniMax陆续服务了近千家客户，包括金山办公、小红书、腾讯、小米和阅文在内的多家头部互联网公司，MiniMax 开放平台平均单日的 token 处理量达到了数百亿。</p><p>&nbsp;</p><p>MiniMax在官微中发文称：“这半年多来，客户给我们提供了很多有价值的反馈和建议。例如，大家认为我们做得比较好的地方有：在写作、聊天、问答等场景中，abab5.5 的表现不错，达到了 GPT-3.5 的水平。”</p><p>&nbsp;</p><p>但是和最先进的模型 GPT-4 相比，仍有明显差距。这主要体现在处理更复杂的、对模型输出有精细要求的场景时，存在一定概率违反用户要求的输出格式，或是在推理过程中发生错误。当然，这不仅是 abab5.5 的问题，也是目前除 GPT-4 以外，几乎所有大语言模型存在的缺陷。</p><p>&nbsp;</p><p>为了解决这个问题，进一步提升模型在复杂任务下的效果，MiniMax技术团队从去年6月份起开始研发 MoE 模型——abab6 是MiniMax的第二版 MoE 大模型（第一版 MoE 大模型已应用于其 C 端产品中）。</p><p>&nbsp;</p><p>虽然MiniMax并未透露Abab6的具体参数，但据MiniMax透露，Abab6比上一个版本大了一个量级。更大的模型意味着 abab6 可以更好的从训练语料中学到更精细的规律，完成更复杂的任务。</p><p>&nbsp;</p><p>但仅扩大参数量会带来新的问题：降低模型的推理速度以及更慢的训练时间。在很多应用场景中，训练推理速度和模型效果同样重要。为了保证 abab6 的运算速度，MiniMax技术团队使用了 MoE &nbsp;(Mixture of Experts 混合专家模型）结构。在该结构下，模型参数被划分为多组“专家”，每次推理时只有一部分专家参与计算。基于 MoE 结构，abab6 可以具备大参数带来的处理复杂任务的能力；计算效率也会得到提升，模型在单位时间内能够训练足够多的数据。</p><p>&nbsp;</p><p>目前大部分大语言模型开源和学术工作都没有使用 MoE 架构。为了训练 abab6，MiniMax还自研了高效的 MoE 训练和推理框架，也发明了一些 MoE 模型的训练技巧。到目前为止，abab6 是国内第一个千亿参数量以上的基于 MoE 架构的大语言模型。</p><p></p><h2>测评结果</h2><p></p><p></p><p>为了对比各模型在复杂场景下的表现，MiniMax对abab6、abab5.5、GPT-3.5、GPT-4、Claude 2.1和 Mistral-Medium 商用进行了自动评测。在简单的任务上，abab5.5 已经做得比较好，因此MiniMax选择了三种涵盖了较复杂的问题的评测方法：</p><p>&nbsp;</p><p>IFEval：这个评测主要测试模型遵守用户指令的能力。在测试时，提问者会问模型一些带有约束条件的问题，例如“以XX为标题，列出三个具体对方法，每个方法的描述不超过两句话”，然后统计有多少回答严格满足了约束条件。</p><p>&nbsp;</p><p>MT-Bench：这个评测衡量模型的英文综合能力。提问者会问模型多个类别的问题，包括角色扮演、写作、信息提取、推理、数学、代码、知识问答。MiniMax技术团队会用另一个大模型（GPT-4）对模型的回答打分，并统计平均分。</p><p>&nbsp;</p><p>AlignBench：该评测反映了模型的中文综合能力测试，测试形式与 MT-Bench 类似。</p><p>&nbsp;</p><p>测评及对比结果如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8bead6d0caf101206d14b55165cc5458.png\" /></p><p></p><p>注：对比模型均选择各自最新、效果最好的版本，分别为 Claude-2.1、Mistral-Medium 商用、GPT-3.5-Turbo-0613、GPT-4-1106-preview；GPT-3.5-Turbo-0613 略好于 GPT-3.5-Turbo-1106 。abab6 是 1 月 15 号的版本。</p><p>&nbsp;</p><p>可以看出，abab6 在三个测试集中均明显好于前一代模型 abab5.5。在指令遵从、中文综合能力和英文综合能力上，abab6 大幅超过了 GPT-3.5。和 Claude 2.1 相比，abab6 也在指令遵从、中文综合能力和英文综合能力上略胜一筹。相较于 Mistral 的商用版本 Mistral-Medium，abab6 在指令遵从和中文综合能力上都优于 Mistral-Medium，在英文综合能力上与 Mistral- Medium 旗鼓相当。</p><p>&nbsp;</p><p>如果想体验MiniMax MoE大模型，可访问MiniMax开放平台官网：api.minimax.chat</p><p>&nbsp;</p><p>ps：MiniMax方面称，模型还在持续训练中，远没有收敛，欢迎大家反馈。</p>",
    "publish_time": "2024-01-16 14:25:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“AI女友”霸占GPT商店，OpenAI苦不堪言：开发者也难出头！",
    "url": "https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY",
    "summary": "<p>OpenAI 不久前推出了 GPT 商店，让开发者可以售卖自己定制的 GPT 机器人。商店刚开张，就积累了 300 万个不同类型的机器人。</p><p>&nbsp;</p><p>OpenAI 将该商店定位为一个聊天机器人交易平台，每个机器人都经过了特殊训练，具备特定技能。例如，有可以帮你查菜谱的美食机器人，也有可以写代码的程序员机器人。</p><p>&nbsp;</p><p>GPT 商店的推出在 AI 社区引起了广泛关注。支持者认为这是 AI 发展的一大进步，未来人们将更容易使用到优质的 AI 工具。反对者则担心这将影响开发者的收入，而且机器人的质量和行为规范也可能存在问题。总而言之，GPT 商店的利弊尚未可知，还需要我们进一步观察才能做出判断。</p><p>&nbsp;</p><p></p><h2>AI女友成了香饽饽，OpenAI 管店不容易</h2><p></p><p>&nbsp;</p><p>上周，OpenAI 推出了 GPT 商店，用户可以浏览和下载由创作者们精心打造的 ChatGPT 定制版本。然而，短短几天内，商店的宁静就被打破了。爱好者们的热情催生出一波意想不到的浪潮：“AI 女友”迅速占领了商店，挑战着 OpenAI 的规定。</p><p>&nbsp;</p><p>在 GPT 商店中搜索“女友”，网站的结果栏中将显示至少八个“AI 女友”聊天机器人，包括“韩国女友”、“虚拟甜心”、“你的女朋友斯嘉丽”、“你的 AI 女友 Tsu”等。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9cde6fc61350e9085192b68efbcd75b8.png\" /></p><p></p><p>OpenAI GPT 商店中“女朋友”搜索结果截图</p><p>&nbsp;</p><p>如果选择了其中一个，比如“虚拟甜心”，用户点击后将收到诸如“你的梦想女孩是什么样子？”、“与我分享你最黑暗的秘密”之类的提示语。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/808948d356c1a49dd259279fcf35f49b.png\" /></p><p></p><p>&nbsp;</p><p>OpenAI 深知潜在的滥用问题，并在 GPT 商店上线当天更新了其使用政策。这些政策明确禁止 GPT 参与浪漫互动：“我们......不允许 GPT 用于培养浪漫伴侣关系或从事受监管活动。”在同一段话中，OpenAI 指出，名称中包含脏话或描绘或宣扬图形暴力的 GPT 也是不允许的。但第二天就出现的政策违规情况表明，审核可能非常困难。</p><p>&nbsp;</p><p>说来也巧，交朋友、找女友、当陪伴的智能聊天机器人，在美国还真挺吃香。据某数据公司统计，2023 年美国人从苹果或谷歌商店下载的前 30 个聊天机器人热门应用中，足足有 7 个是跟这相关的。</p><p>&nbsp;</p><p>“AI女友”也让 OpenAI 意识到，管住这些 GPT 可真是个不小的挑战。虽然他们有规定，违规了就警告、限制、踢出商店、断财路，可这些规则跟现实的碰撞，还真是火花四溅。这些卖商家随后就换了关键词，把“女友”换成了“甜心”，搜索出来的选项就多了不少。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/22d36fb81c342adcc69fdc53dabf4c9b.png\" /></p><p></p><p>&nbsp;</p><p>看来，OpenAI 又得抓耳挠腮了。监管这些人工智能聊天机器人，是一场持久战！</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>炒作中的GPT商店</h2><p></p><p>&nbsp;</p><p>从技术角度来看，创建这些定制 GPT 非常容易，几乎所有人都可以参与。使用 OpenAI 的 GPT Builder，创作者只需用简单语言描述他们希望 GPT 拥有的功能，该工具就会尝试根据这些规范创建一个 AI 聊天机器人。这种易于创作的特点自发布以来就备受关注，使得 GPT 的开发和分享变得非常迅速。</p><p>&nbsp;</p><p>但它也有坏的一面，比如这些 GPT 的审核机制还不完善，可能导致意想不到的、令人不快的行为。上线到现在，抄袭现象也非常严重，抄袭者可以使用同样的名称、工作原理甚至图标，社交平台上用户对此怨声载道。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d2d866dd0b49b277058210d10374d0a.jpeg\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f4917b377a65d32c5929b0d3bbba3b81.png\" /></p><p></p><p>&nbsp;</p><p>而且即将推出的生成器收入计划，美国开发者将可以通过用户参与获得收入。不过，由于收入分成比例可能很低，大家还是不要抱太大期望。</p><p>&nbsp;</p><p>ChatGPT 拥有 1.8 亿用户和 25 万 Plus 订阅者，市场似乎广阔。但让我们冷静分析一下，看看实际潜力有多大。</p><p>&nbsp;</p><p>假设 OpenAI 分成 10%，所有创作者理论上最多能赚到600 万美元（前提是他们的 GPT 可以触达每一个 Plus 用户）。让我们来模拟一个成功的 GPT 场景：</p><p>1% 的 Plus 用户使用你的 GPT。这些用户平均同时使用 5 个不同创作者的 GPT。</p><p>&nbsp;</p><p></p><blockquote>OpenAI 每月从 Plus 用户中赚取 20 美元。Plus 用户年收入：20 美元/月 * 25 万用户 * 12 月 = 6 千万美元。所有创作者的分成：6 千万美元 * 10% = 60 万美元。作为被 1% Plus 用户使用的 5 个创作者之一，你的年收入：60 万美元 / (100 个创作者 * 5 个) = 1.2 万美元。</blockquote><p></p><p>&nbsp;</p><p>就算 OpenAI 分成增加到 20%，Plus 用户翻倍，你的年收入也只有 4.8 万美元。相比其他双边市场，即使是最成功的创作者，这个收入也相当微薄。</p><p>&nbsp;</p><p>所以，OpenAI 的GPT 商店也许并不是为创作者创收而设计的，它也不会为 OpenAI 工具带来新的用户参与，因为它的受众仅限于 Plus+ 用户。因此，它的首要目标应该是作为一种发现工具，帮助 OpenAI 了解用户接下来需要什么产品。打造成功的 GPT 实际上就是告诉 OpenAI 下一代 B2C 产品应该朝哪个方向发展。</p><p>&nbsp;</p><p>这个商店标志着 OpenAI 战略的重要转变，表明它正迈向以产品为中心的方式。这一举措不仅仅是为了创建一个 AI 应用的市场，更是 OpenAI 在 AI 应用领域实现市场主导地位的重要战略一步。通过推出 GPT 商店，OpenAI 将控制 AI 生态系统中关键的分发平台，展示其先进的 AI 模型，同时通过商店收入实现收入来源多元化，不再局限于研究资助和合作。</p><p>&nbsp;</p><p>对我们开发者来说，GPT 商店的推出为大家提供了用 AI 驱动应用进行创新和实验的机会。然而，必须理性看待个人创作者的财务收益。该平台更多扮演的是新想法和应用的测试平台，提供用户偏好和应用趋势的洞察。</p><p>&nbsp;</p><p>总的来说，大家要做好心理准备，毕竟分成少、用户少、竞争大，想在 GPT 商店赚大钱不容易。</p><p>&nbsp;</p><p>毕竟它跟传统应用商店或创作者平台动辄七成八成利润分红不一样，GPT 商店的开发商分成估计只有可怜的 10%-20%。为啥这么少？因为用户花的钱买的是 OpenAI 的算力，跟自家手机没关系。 这点本质区别就导致了 GPT 商店的玩法跟其他平台完全不同。</p><p>&nbsp;</p><p>另外，只有 ChatGPT Plus 用户才能用定制 GPT，这一下子就把用户群从 1.8 亿缩水到 25 万。对想获利的开发者来说，这也是个不小的拦路虎。</p><p>&nbsp;</p><p>最后，如果好不容易做了个 GPT，想在商店里脱颖而出可不容易。成千上万个机器人里，谁又能保证你的被大家看到？更要命的是，复制一个 GPT 太简单了，想做出独一无二的产品难上加难，竞争可激烈着呢！</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131\">https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131</a>\"</p>",
    "publish_time": "2024-01-16 15:01:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "WebAssembly 2023 年回顾与 2024 年展望 | 年度技术盘点与展望",
    "url": "https://www.infoq.cn/article/5Zrq507bQW6lial5iVy1",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/77/77d262475ed561520ac076d16507423a.jpeg\" /></p><p></p><p>在刚刚过去的 2023 年，WebAssembly 技术发展态势喜人，多项关键性提议都进入了新阶段，并且获得了社区与工具链的广泛深入支持。同时，其应用场景呈现出蓬勃扩展的态势，吸引着越来越多组织和个人开发者群体投入 WebAssembly 的开发之中。下文我们将首先回溯 WebAssembly 在 2023 年各项关键技术特性的进展，继而前瞻探讨新的一年它有望展现的发展趋势和前景。本文是 “2023 InfoQ 年度技术盘点与展望” 系列文章之一，由 InfoQ 编辑部制作呈现。</p><p></p><h2>回顾 2023</h2><p></p><p></p><h3>GC、Func Ref 和 String Ref</h3><p></p><p></p><p>WebAssembly GC （Garbage Collection，垃圾回收） 特性引入的目的之一是为了更加高效地支持一些需要垃圾回收处理的高级语言，比如 TypeScript、Java、Kotlin、Python、PHP 和 C# 等。</p><p></p><p>在没有引入 WebAssembly GC 特性之前，如果要将这些语言移植到 WebAssembly，一般的做法是将相应语言的虚拟机（如 JavaScript runtime、Python runtime 等）以及目标 app 一起编译成 WebAssembly 目标代码，由 wasm runtime 来执行该语言的虚拟机，然后再由该虚拟机来执行目标 app。由于这种方式不容易将目标 app 转成机器码以 AOT（Ahead of Time Compilation）或 JIT（Just in Time Compilation）方式来执行，虚拟机一般只能以解释器的方式执行目标 app，所以性能受到很大的限制。另一方面，将虚拟机编译到 wasm 目标代码，也可能大大增加目标代码的体积。</p><p></p><p>引入了 WebAssembly GC 特性之后，人们可以开发出新的编译器，将某种高级语言的 app 直接编译成基于 WebAssembly GC 操作的目标代码，不需要将虚拟机一起编译进来。这样一方面目标 app 可以用 AOT 或 JIT 方式来执行，另一方面基于 GC opcode 的操作也非常高效（比如对结构成员的访问、对数组元素的访问等），因此可以极大提升性能，并减少目标代码体积。</p><p></p><p>WebAssembly GC 依赖于 Reference Types 和 Typed Function References （或简称 Func Ref） 两个特性，在基本数据类型 （i32、 i64、 f32、 f64 和 v128） 的基础上引入了多种和引用相关的数据类型，包括 structref、 arrayref、 i31ref、 funcref、 externref 等，规定了类型之间的继承和等价关系，并且引入很多新的 opcode 来操作这些数据类型，从而满足多种高级语言的需求。其中 Reference Types 已经在 2021 年正式写入规范，而 GC MVP 提议和 Func Ref 提议在 2023 年也得到了较大的发展，在经过了大量讨论和修改后趋于完善和稳定，目前都进入了阶段四即标准化阶段。</p><p></p><p>另外开源项目 WAMR （wasm-micro-runtime）、v8、Kotlin 以及开源编译框架 Binaryen 等都在持续跟进支持 GC 特性。WAMR 已经基本实现了 interpreter、AOT 以及 LLVM JIT 几种执行模式对绝大部分 GC MVP 特性的支持（注：开发分支），并被应用到了开源 Wasmnizer-ts 项目上面，后者旨在将 TypeScript 语言编译成基于 WebAssembly GC 的目标代码，以提升性能、减少模块尺寸并方便移植到各个架构，有兴趣可以参考 <a href=\"https://github.com/web-devkits/Wasmnizer-ts\">https://github.com/web-devkits/Wasmnizer-ts</a>\"。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/1d/1dba6ef405e3adadf9adcd36d42e4156.png\" /></p><p>Hierarchy of abstract wasm GC heap types</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4f/4f12be8ae741e9fadd9c571b13dd6f63.png\" /></p><p>Basic idea of Wasmnizer-ts compiler</p><p></p><p>Func Ref 允许把 wasm 函数作为一个对象来引用，并传入函数参数来调用该引用指向的函数，例如通过 ref.func opcode 来创建一个函数引用然后使用 call_ref opcode 来调用该函数。这样可以更灵活地支持动态函数调用，相比之前的 call_indirect opcode 通过 wasm table 来间接的调用函数的方式，可以减少较多的运行时检查以提升性能。另外通过函数引用也方便实现有些高级语言特性，比如闭包（Closure）。</p><p></p><p>Reference-Typed Strings （或简称 String Ref） 则旨在改善 WebAssembly 中对字符串的处理方式。在该提议出现前，每个语言的编译器需要自己设计 string 在 WebAssembly 中的表达方式、编码处理等。这种方式导致生成的 wasm 模块中有大量的逻辑用来处理 string，进而增大模块体积。另一方面，WebAssembly 往往运行在一个特定的宿主环境中，在 WebAssembly 中实现的 string 可能无法被宿主环境直接使用，因此在宿主和 wasm 之间进行 string 传递时往往涉及到内存拷贝，影响性能。</p><p></p><p>String Ref 提议引入了一种新的引用类型，直接将宿主环境的 string 以引用的形式提供给 WebAssembly，并通过一系列 opcode 来进行操作。这一提议使得编译器无需再设计自己的 string 表达方式，同时避免了大量用于处理编码的 opcode，在降低编译器实现复杂度的同时，也减小了生成的 wasm 模块体积。同时，WebAssembly 和宿主直接传递 string 时无需拷贝，也提升了性能。虽然该提议还处在阶段一，但很快引起关注，目前 v8 、WAMR 和 Binaryen 都已经支持。</p><p></p><h3>wasi-threads</h3><p></p><p></p><p>wasi-threads 是 WebAssembly 系统接口 （WebAssembly System Interface，WASI） 的一个扩展提议，它的目的是在 WASI 环境中引入对多线程的支持，使得 WebAssembly 应用程序能够创建、同步和管理多个线程。它提供了一个标准化的 API 来创建线程：status wasi_thread_spawn（thread_start_arg* start_arg），该 API 要求 wasm runtime 实现对应的名为“thread-spawn”的 WASI 接口来创建线程，并准备好相关上下文，在新创建的线程中实现对 wasm app 函数的回调。另外它也在 WebAssembly threads 提议的一些原语基础上实现了对互斥锁 （mutex）、条件变量等的支持，以便协调和同步多个线程的执行。基于此，该提议实现了对 pthreads 大部分 API 的支持，目前工具链 wasi-sdk 已经基本实现了上述支持，并发布了 wasi-threads 的二进制包。而开源社区 wasmtime 和 WAMR 也分别实现了该提议，有兴趣可以参考 <a href=\"https://bytecodealliance.github.io/wamr.dev/blog/introduction-to-wamr-wasi-threads\">https://bytecodealliance.github.io/wamr.dev/blog/introduction-to-wamr-wasi-threads</a>\"。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/73/737bc0e9bf08a693bb4566ba1a1aef31.png\" /></p><p></p><p>Memory model of WAMR wasi-threads</p><p></p><h3>Memory 64、 Multi-Memories 和 Memory Control</h3><p></p><p></p><p>WebAssembly 的内存模型是基于线性内存的，目前 MVP 版本（最小可行版本）中线性内存只支持 Memory32，其地址范围是 32 位，即一个线性内存最多有 65536 （=216&nbsp;）个 page，而每个 page 有 65536 （=216） 个字节，所以最多可以有232&nbsp;= 4G 个字节。这对于许多应用如 Web 应用和嵌入式系统来说是足够的，但对于某些工作负载，特别是需要大量内存的应用程序，如云计算、人工智能、虚拟化和容器等，可能不够。因此 Memory64 提议被提出，以支持 64 位地址范围，而相应的和线性内存访问相关的规范或提议也被扩展，包括基本的 memory 操作、Bulk Memory 规范、Shared Memory 的 atomic 内存操作、SIMD-128 规范中有关内存的操作等。</p><p></p><p>目前 Memory64 已经进入到了阶段三即实现阶段，wasm runtime 方面 v8、wasmtime、wasmer 和 wasm2c 等均已经提供支持，工具链方面 LLVM、Emscripten 和 Binaryen 也提供了支持。而 wasi-sdk 所依赖的基础库 wasi-libc 也有开发者提交了支持 Memory64 的 patch，期待在不久的将来 Memory64 可以在 wasi-sdk 中得到支持。</p><p></p><p>Multi-Memories 提议则意在支持在一个 wasm 模块中使用多个线性内存，这样做可以提高隔离和安全性，提供更灵活的内存管理，并且方便多模块之间共享数据，比如模块将私有数据存在一个内存实例，而需要和其它模块共享的数据则存在另一个内存实例。目前该提议已经进入阶段四即标准化阶段，v8 和 Binaryen 已经支持。</p><p></p><p>Memory Control 提议则希望可以减少 native 和 wasm 模块以及 wasm 模块之间的内存拷贝，并提供基于 host mmap 的内存映射和访问控制方式，比如在对内存初始化后将它设置成只读模式。它提出了 memory.map 和 memory.protect 等 opcode，可选方案之一是将 host 内存映射成一个 wasm 的内存引用，然后允许将该引用的句柄在共享的 heap 中传递到另一个 wasm 模块，这样目标模块可以通过该引用句柄来访问对应的内存，从而实现零拷贝数据传递。虽然该提议在 2022 年初就停止了更新，但是目前引起了较多兴趣，有不少讨论希望能继续推进该提议。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/1f/1f1842d764b3c23c7be90f21b69adf38.png\" /></p><p>Possible memory data sharing based on memory control proposal</p><p></p><h3>Component Model</h3><p></p><p></p><p>Component Model（组件模型） 是一个在多语言环境下实现多个 wasm 模块相互协作的提议，它引入了一套抽象类型解决多语言的类型差异，并用序列化 / 反序列化来解决抽象类型到 wasm 基本类型的过渡。该提议目前得到了社区的广泛支持，在 2023 年取得了显著的进展，支持提议的 wasm runtime 数量在增长，基于 Component model 抽象类型重制的 WASI-preview2 正式上线。工具链方面，Wasm-tools 完成对提议的支持。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/32/325677efe814cc97ccd56e4b84793e8b.png\" /></p><p>Component model shared everything dynamic linking sample</p><p></p><h3>Core Module Dynamic Linking</h3><p></p><p></p><p>这里所说的 Dynamic Linking 是指最早由 Emscripten 提出的动态链接模型，用来将 C/C++ 应用程序的源代码划分成几个部分进行编译，并在执行时将它们链接在一起。相对 Component Model 而言，Core Module 进行链接的模块是 WebAssembly 模块，而不是对 WebAssembly 模块进行了封装的组件 （Component）。这种动态链接方式相对简单，目前在 llvm-17.0 中已经支持，而最新的 wasi-sdk release 版本也提供了支持。Emscripten 规定编译器可以编译出两类的共享模块：Main modules（主模块）和 Side modules（暂称为副模块）。其中只有主模块可以把系统库（如 libc）一起链接进来，并且一个项目中有且只有一个主模块，主模块的一些 symbol 如函数可能依赖于副模块，它们在执行时当副模块被加载后被进行链接。同时主模块有自己的线性内存和 wasm table，而副模块则没有自己的线性内存和 wasm table，副模块共享主模块的线性内存和 wasm table，并且需要从主模块的线性内存和 wasm table 中预留一部分空间来存储自己的数据。通过这种方式，可以实现主模块和副模块之间的数据共享。</p><p></p><p>而如何在主模块的线性内存以及 wasm table 中预留出部分空间给副模块，以及对副模块的链接时机，是在主模块加载时链接（Load-time dynamic linking），还是在主模块执行时加载（Runtime dynamic linking），可能是 wasm runtime 实现时需要考虑的问题。从目前来看，除了浏览器之外，似乎还没有 standalone runtime 实现了该技术。相对于组件模型，这种链接方式可以实现模块间零拷贝数据传递以提升性能，并且内存消耗较小，实现也相对比较容易，缺点是多语言支持较差，目前来看似乎只能支持 C/C++/Rust 等使用 clang 来编译的语言。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c0/c01cf31940534a7aa67c1407012cf4c4.png\" /></p><p>Possible linking between Main module and Side module</p><p></p><h3>wasi-nn</h3><p></p><p></p><p>wasi-nn是 WebAssembly 系统接口 （WebAssembly System Interface、 WASI） 的另一个扩展，主要用于支持深度学习硬件加速。它被提出的原因之一是由于机器学习框架（例如 Tensorflow）不容易被移植到 WebAssembly SIMD 并且较好地支持一些硬件特性来保证性能。它主要针对机器学习应用场景，允许在 wasm 模块中访问 host 提供的机器学习功能，可以用于模型训练和推理，适用场景包括自然语言处理、图像识别、语音识别等领域。其支持多种深度学习模型框架，其中包括 TensorFlow 和 OpenVINO 等业界主流框架；其目标执行环境涵盖了从中央处理器（CPU）、图形处理器（GPU）到专为机器学习设计的张量处理单元（TPU）等多种计算硬件架构。目前 WAMR、wasmtime 和 WasmEdge 都对 wasi-nn 提供了支持，其中 WAMR 主要支持 Tensorflow 模型，而 wasmtime 和 WasmEdge 主要支持 OpenVINO 模型。有兴趣可以参考 <a href=\"https://github.com/bytecodealliance/wasm-micro-runtime/tree/main/core/iwasm/libraries/wasi-nn\">https://github.com/bytecodealliance/wasm-micro-runtime/tree/main/core/iwasm/libraries/wasi-nn</a>\"。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3f/3f26caa34bb101873603143d41115014.png\" /></p><p>Simple working flow of wasi-nn</p><p></p><h3>Exception Handling</h3><p></p><p></p><p>WebAssembly Exception Handling（异常处理）提议旨在为 WebAssembly 添加类似 Java 或 C++ 中异常处理的机制，使开发者能够更好地管理和处理程序执行过程中的错误情况。该提议引入了“tag” 的概念，使用 tag 来标识异常类型，当抛出一个异常时，会附带一个 tag 来告诉调用者异常的类型，然后调用者可以在 catch 语句中根据 tag 来决定如何处理异常。目前该提议已经进入到阶段三即实现阶段，v8、Firefox 和 wasm2c 都已经支持，WAMR 在 classic interpreter 模式下也提供了支持（注: 开发分支），工具链方面 LLVM、 Emscripten 和 Binaryen 也都已经支持。</p><p></p><h2>展望 2024</h2><p></p><p></p><h3>更多 wasm 提议被写入规范</h3><p></p><p></p><p>目前 GC、Func Ref、Multi-memories、Threads 和 Tail call 等提议都已经进入到了阶段四，随着提议的进一步完善和稳定，有望在新的一年里正式写入规范。而基于这些提议之上的应用，也将得到更加广泛的发展，比如基于 GC 提议的 Wasmnizer-ts 和 Kotlin、基于 Threads 提议的 wasi-threads 提议及相关的应用等。</p><p></p><h3>更好地解决 wasm 模块间数据共享和模块链接问题</h3><p></p><p></p><p>关于 wasm 模块与本机环境（Native）之间、wasm 模块之间的数据共享问题，一直是社区广泛讨论的议题。业界普遍期待能够解决模块之间零拷贝数据传递的问题，籍此提升性能表现。随着一系列创新方案的提出以及工具链的不断完善，这一挑战有望得到更为有效的解决。</p><p></p><p>另一方面，针对 wasm 模块之间的链接与代码共享难题，随着 Component Model 和 Core Module Dynamic Linking 的发展，更多的 wasm runtime 正在逐步实现对模块间链接与代码共享的支持，这将进一步推动 WebAssembly 生态系统的高效整合。</p><p></p><h3>更多的应用场景</h3><p></p><p></p><p>目前 WebAssembly 已经被广泛应用到各个领域中，比如 Web 应用，图像处理、IoT、人工智能、边缘计算、区块链等等。随着 wasm 技术进一步成熟和工具链生态进一步发展，其在更多专业领域和场景的潜力将得以释放。展望未来，我们预期 wasm 将在汽车、云原生、PLC、Snapshot 迁移等场景得到采用。</p><p></p><h3>更好的用户体验</h3><p></p><p></p><p>WebAssembly 发展面临的主要问题之一是工具支持，这可能影响了一些用户体验。预计在新的一年里会有更多更友好的工具出现，比如 AOT/JIT 调试工具、多线程调试支持、性能监测工具、内存监测工具等。</p><p></p><h2>小结</h2><p></p><p></p><p>总之，在过去一年里，WebAssembly 多项提案得到了显著的演进与发展，诸多前沿特性和功能逐步获得了各个 WebAssembly 运行时与工具链的广泛支持。同时，我们也目睹了越来越多的应用场景和实际案例涌现出来，充分展示了 WebAssembly 技术的潜力与价值。</p><p></p><p>展望新的一年，我们期待 WebAssembly 能够在技术成熟度及实用性层面实现更为坚实而有力的突破，更好地为相关的行业、组织以及开发人员服务。</p><p></p><p>作者介绍：</p><p></p><p>黄文勇, Intel Web Platform Engineering 软件工程师, Wasm Micro Runtime 项目 Technical Leader何良, Intel Web Platform Engineering 软件工程师, Wasm Micro Runtime 项目主要贡献者徐君, Intel Web Platform Engineering 软件工程师, Wasm Micro Runtime 项目主要贡献者</p><p></p><p>Wasm Micro Runtime (WAMR) 是一个运行在浏览器之外的 Standalone WebAssembly 虚拟机，支持 Interpreter、AOT、JIT 等多种执行模式，支持多种 OS 和多种 CPU 架构，具有高性能、低内存消耗、功能高度可配置等特性，适用于从嵌入式、物联网、边缘计算到可信执行环境、智能合约、云原生等各种应用。参考 <a href=\"https://github.com/bytecodealliance/wasm-micro-runtime\">https://github.com/bytecodealliance/wasm-micro-runtime</a>\"。</p><p></p><p>如果你觉得本文对你有帮助，或者你对&nbsp;WebAssembly&nbsp;未来发展有自己的思考，欢迎在文末留言告诉我们！</p><p></p><p></p><blockquote>InfoQ&nbsp;2023&nbsp;年度技术盘点与展望专题重磅上线！与&nbsp;50+&nbsp;头部专家深度对话，探明&nbsp;AIGC&nbsp;创新浪潮下，重点领域技术演进脉络和行业落地思路，点击<a href=\"https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MjM5MDE0Mjc4MA==&amp;action=getalbum&amp;album_id=2717978015128879106&amp;scene=173&amp;subscene=227&amp;sessionid=1704178990&amp;enterid=1704178995&amp;from_msgid=2651192070&amp;from_itemidx=2&amp;count=3&amp;nolastread=1#wechat_redirect\">订阅</a>\"/<a href=\"https://www.infoq.cn/theme/229\">收藏</a>\"内容专题，更多精彩文章持续更新~另，InfoQ&nbsp;年度展望系列直播最后一场将于&nbsp;2024&nbsp;年&nbsp;1&nbsp;月&nbsp;22&nbsp;日开播，主题为《代码人生攻略：程序员们如何为自己编织一份明朗未来？》，我们邀请到了章文嵩、周爱民、李博源、陶建辉四位重量级大咖，通过分享各自的职业心得和技术洞察，帮助大家更好地为未来发展做好准备。关注&nbsp;InfoQ&nbsp;视频号，与行业技术大牛连麦~</blockquote><p></p>",
    "publish_time": "2024-01-16 16:39:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]