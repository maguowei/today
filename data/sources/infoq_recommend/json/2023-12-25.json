[
  {
    "title": "网游新规致腾讯网易市值半天蒸发5200亿；吴泳铭“爆改”淘天：管理层全换成有功绩的年轻人；字节年收入超腾讯、逼近 Meta｜Q资讯",
    "url": "https://www.infoq.cn/article/xxlQ1u4YBddYJzfURN8q",
    "summary": "<p>&nbsp;</p><p></p><blockquote>吴泳铭发全员信：淘天集团管理团队全部换血；抖音要收购饿了么？抖音相关负责人：没有这个计；字节年收入超过腾讯、逼近 Meta；雷军：小米的第一辆车投入了3400名工程师，研发超过100亿；Arm 裁员 70 多名中国工程师，重组中国软件业务；英伟达股价五年上涨约1200%，老员工开始躺平，黄仁勋对此感到不满；OpenAI：董事会将拥有是否发布新AI大模型的决定权；网游将不得设诱导性奖励，腾讯跌超7%；微博CEO王高飞怼董明珠：自己管理上不合规，吃亏了要么骂员工要么赖法律……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司&nbsp;</h2><p></p><p></p><h4>吴泳铭发全员信：淘天集团管理团队全部换血</h4><p></p><p>&nbsp;</p><p>12月20日，阿里巴巴宣布，阿里巴巴集团CEO、淘天集团董事长吴泳铭兼任淘天集团CEO。自此，吴泳铭将同时担任阿里巴巴集团和淘天集团、阿里云智能集团三项CEO职务。淘天集团原CEO戴珊将协助筹建阿里巴巴集团资产管理公司，“这是阿里变革之后新的业务职能”。</p><p>&nbsp;</p><p>阿里巴巴集团董事会主席蔡崇信在全员信中表示，由吴泳铭兼任阿里云和淘天的一号位，将有助于以技术创新引领淘天的变革，有助于确保集团对两大战略重心电商和云的统一指挥和高强度持续投入</p><p>&nbsp;</p><p>12月22日下午消息，吴泳铭宣布了淘天集团最新组织决定，年轻化管理团队全面接棒。6位年轻管理者被任命分别带领淘天集团各关键业务，直接向吴泳铭汇报。吴泳铭同时对淘天集团提出要求：正视现状，重新创业。</p><p>&nbsp;</p><p>具体看，“85后”吴嘉将负责淘天用户平台事业部与阿里妈妈事业部。据了解，吴嘉2010年校招加入阿里巴巴，在技术开发一线积累了丰富经验，培育孵化了广受年轻人欢迎的产品夸克。吴嘉还将继续兼任智能信息总裁。</p><p>&nbsp;</p><p>现任饿了么首席运营官谌伟业（处端）将调任淘宝，负责淘宝事业部、淘天商家平台部、淘天客户满意部。他主导提出饿了么“放心点准时达”的品牌价值方向，创设了现象级营销“猜答案免单”。他也是另一款年轻人喜爱的闲置交易和兴趣内容平台闲鱼的初创人。</p><p>&nbsp;</p><p>刘博（家洛）将接手天猫事业部，十几年来他一直在淘宝天猫业务一线，商业实战经验丰富，连续开创了多个战略赛道。生于87年的汪庭祥（少游）则将带领服饰发展部。原直营业务负责人刘一曼（一漫）将负责M2C事业部。程道放（道放）将带领淘宝直播及内容事业部，负责推进淘宝内容化建设与创新。</p><p>&nbsp;</p><p></p><h4>抖音要收购饿了么？抖音相关负责人：没有这个计划</h4><p></p><p>&nbsp;</p><p>近日，有市场消息称，吴泳铭已做出了一系列资本规划：盒马已经在考虑出售、饿了么或将有新的资本动作，优酷则正考虑并入阿里影业，但前提是能够稳定盈利。对此，阿里内部人士表示，消息不实，已经对相关报道进行投诉。</p><p>&nbsp;</p><p>12月19日，针对抖音与阿里谈判收购饿了么的传言，抖音相关负责人表示，抖音没有这个计划。据悉，该传闻称，抖音正在和阿里谈收购饿了么，目前已经到了谈价格阶段，如果谈判顺利，预计春季后就能落地。</p><p>&nbsp;</p><p>针对传闻，饿了么、优酷、盒马接连辟谣：没动作、没合并、没出售。饿了么回应：将有资本动作为不实消息；阿里大文娱回应：优酷并入阿里影业？假的；盒马回应：出售盒马为不实传闻。</p><p>&nbsp;</p><p></p><h4>字节年收入超过腾讯、逼近 Meta</h4><p></p><p>&nbsp;</p><p>据媒体报道，字节跳动 2023 年收入将达到 1100 亿美元，同比增长约三成。这个体量足以超过腾讯，逼近 Meta——按照各自财务指引，腾讯和 Meta 今年的收入分别将达到约 870 亿和 1330 亿美元；不过仍然只有全球最大的数字广告主 Google 的约三分之一。</p><p>&nbsp;</p><p>字节跳动的收入依旧主要来自国内广告和电商，但是 TikTok 高增长带来的拉动也不容忽视。按照此前媒体披露的数据，今年二季度 TikTok 对整个字节的营收贡献已经接近 20%。相比之下，Meta 和 Google 都做着更纯粹的数字广告生意（尤其是美国市场）。二者广告营收占比分别为约 95% 和 80%，受到整个宏观经济和企业营销支出缩减的更大打击。此外，营销人员也更希望通过既贴近消费者，转化率又高的渠道投放广告。</p><p>&nbsp;</p><p>据研究公司 Insider Intelligence 估计，去年 Google 和 Meta 在美国数字广告市场的市占率为 48.4%，为自 2014 年以来首次降到五成以下。它们预计这个数字今年会进一步降至 44.9%，因为更多份额会流向亚马逊、TikTok 以及其他流媒体平台。</p><p>&nbsp;</p><p></p><h4>雷军：小米的第一辆车投入了3400名工程师，研发超过100亿</h4><p></p><p>&nbsp;</p><p>小米集团创始人雷军透露，汽车很复杂，对于小米汽车的预期，特别担心大家都不买，但更担心的是大家都来买，“这一等要等一两年，肯定会被骂惨了，其实是各种很焦虑的情绪。”</p><p>&nbsp;</p><p>雷军表示，小米造车用了十倍以上的投入，有了这样的把握之后，他抱着志在必得的方式来做汽车。通常车企做一辆车大概投三四百人，研发经费在10亿-20亿。小米的第一辆车，则投入了3400名工程师，研发投入超过100亿。</p><p>&nbsp;</p><p></p><h4>Arm裁员 70 多名中国工程师，重组中国软件业务</h4><p></p><p>&nbsp;</p><p>软银集团旗下英国芯片设计公司 Arm 最近在中国裁减了超过 70 名软件工程师。不过，Arm 将其中的一些职位迁移到了中国以外的地方。在被裁掉的员工中，大约有 15 人将被安排从事与中国相关项目的不同岗位上。这些被裁撤的岗位由合同制软件工程师填补，他们曾参与过横跨 Arm 全球业务的项目。</p><p>&nbsp;</p><p>半导体行业因电子产品需求不振而低迷，Arm 此次裁员仿效了高通等主要芯片公司，后者在今年早些时候削减了全球员工数量。今年 11 月，由于智能手机销量下滑，Arm 发布了令人失望的营收预期。</p><p>&nbsp;</p><p></p><h4>英特尔启动今年第五轮裁员</h4><p></p><p>&nbsp;</p><p>12月20日消息，英特尔启动了今年第五轮裁员。监管文件显示，英特尔计划在福尔瑟姆（萨克拉门托县）的研发工厂裁员235名员工，于12月31日开始，持续两周时间。据悉，在前几轮裁员中，英特尔在福尔瑟姆园区裁掉了549个职位，占员工总数的10%左右。</p><p>&nbsp;</p><p>公司发言人阿迪·伯尔(Addy Burr)在一份声明中表示，“英特尔正在努力加快其战略，同时通过多项举措降低成本，包括在整个公司范围内减少一些特定业务和职能的工作场所。” 伯尔补充说，新的一年可能还会进一步削减。2022年，英特尔宣布在2025年前通过裁员、减少工作时间和可能出售部门的方式达成削减成本100亿美元的目标。</p><p>&nbsp;</p><p></p><h4>英伟达股价五年上涨约1200%，老员工开始躺平，黄仁勋对此感到不满</h4><p></p><p>&nbsp;</p><p>过去五年，英伟达股价暴涨1200%，一些资深员工坐拥巨额公司股票，开始躺平，工作热情减退。一些员工认为老员工没有尽全力工作。员工也指责黄仁勋所创造的过度以员工为中心的文化，再加上相对宽松的管理风格以及公司在高端芯片市场的新近统治地位，这些因素共同加剧了这种躺平现象。</p><p>&nbsp;</p><p>在上个月的全体员工大会上，黄仁勋公开回应了员工提出的关于公司存在“半退休”老员工现象的质疑。黄仁勋表示，在英伟达工作就像一项“自愿运动”，每个人都应该像“CEO”一样管理自己的时间。“黄仁勋的意思很明确，就是‘干好你该做的工作’，”一位消息人士称。</p><p>&nbsp;</p><p></p><h4>OpenAI：董事会将拥有是否发布新AI大模型的决定权&nbsp;</h4><p></p><p>&nbsp;</p><p>据消息称，OpenAI 在官方网站发布了一份名为“准备框架（Preparedness Framework）”的安全指南，规定了“跟踪、评估、预测和防范日益强大的模型带来的灾难性风险的流程”。 OpenAI 解释说，对前沿人工智能风险的研究，远远没有达到需求。为了解决这一差距并使安全思维系统化，OpenAI 正在采用“准备框架”的测试版本。</p><p>&nbsp;</p><p>OpenAI 在新闻稿中宣布，“准备（Preparedness）团队”将致力于确保前沿人工智能模型的安全。“准备团队”将持续评估人工智能系统，以了解其在四个不同风险类别中的表现，包括潜在的网络安全问题、化学威胁、核威胁和生物威胁，并努力减少该技术可能造成的任何危害。</p><p>&nbsp;</p><p>具体来看，OpenAI 正在监控所谓的“灾难性”风险，它在这份指南中被定义为“可能导致数千亿美元经济损失或导致许多人严重受伤甚至死亡的任何风险”。 值得注意的是，根据安全指南，领导层可以根据这些报告决定是否发布新的人工智能模型，但董事会有权推翻其决定。</p><p>&nbsp;</p><p></p><h4>网游将不得设诱导性奖励，腾讯网易市值蒸发5200亿</h4><p></p><p>&nbsp;</p><p>12月22日，国家新闻出版署发布《网络游戏管理办法(草案征求意见稿) 》，现向社会公开征求意见。其中提到，网络游戏不得设置每日登录、首次充值、连续充值等诱导性奖励。网络游戏出版经营单位不得以炒作、拍卖等形式提供或纵容虚拟道具高价交易行为。所有网络游戏须设置用户充值限额，并在其服务规则中予以公示，对用户非理性消费行为，应进行弹窗警示提醒。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>受此消息影响，港股游戏股闪崩。截至22日下午三点，网易（09999.HK）报每股118.7港元，跌幅超过26.64%，最大跌幅达27.94%；腾讯（00700.HK）报每股271港元，跌幅超过13%，最大跌幅达14.27%。以此计算，两家头部游戏大厂市值至少蒸发超过5720亿港元（约合人民币5200亿元）。</p><p>&nbsp;</p><p>腾讯游戏副总裁张巍回应媒体关于《征求意见稿》问询时表示：“自从2021年未保新规发布以来，腾讯一直严格贯彻落实管理要求，目前未成年人的游戏时长和消费数据都处于历史最低水平。《征求意见稿》对于游戏的商业模式，运营节奏等关键要素并无本质改变。监管部门发布新的管理办法征求意见稿向业界、社会充分征求意见，相信更有利于游戏行业的有序、健康发展。腾讯游戏将继续坚持技术创新，文化引领的精品战略，在主管部门的支持下，践行中国游戏产业的高质量发展。”</p><p>&nbsp;</p><p></p><h4>微博CEO王高飞怼董明珠：自己管理上不合规，吃亏了要么骂员工要么赖法律</h4><p></p><p>&nbsp;</p><p>近日，董明珠多次发言上了微博热搜榜，引起了各大网友评论和关注。今年3月，董明珠曾在采访中建议立法对员工跳槽行为收取培训费，董明珠还强调，“因为你在我这里干了十几年，我培养了你，我付出了那么多财力人力物力和时间，你拍了屁股就走了，那你下一个单位最少要赔偿我的培训费”。</p><p>&nbsp;</p><p>微博CEO王高飞公开批评董明珠，对其提议立法要求跳槽员工支付培训费进行驳斥。王高飞指出，《劳动合同法》已允许企业与员工签订培训服务协议，约定服务期，董明珠提出的问题法律上早有解决方案。王高飞点评称，“自己管理上不合规，吃亏了要么骂员工要么赖法律”。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>Gemini 自曝中文用百度文心一言训练</h4><p></p><p>&nbsp;</p><p>近日，有网友发现，在谷歌Vertex AI平台使用该模型进行中文对话时，Gemini-Pro 直接表示自己是百度语言大模型。微博大V@阑夕也发博称：在 Poe 平台上对 Gemini-Pro 进行了一个测试。问它“你是谁”，Gemini-Pro 回答：我是百度文心大模型。问它“你的创始人是谁”，回答：“李彦宏”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/02ef75931ac69a699d479d1743e3b3a0.jpeg\" /></p><p></p><p>&nbsp;</p><p>但换成英文询问它的身份，它回答自己是谷歌训练的大模型。当从 Gemini 官方给出的开发环境入口进行测试，在谷歌 AI Studio 中，Gemini-Pro 直接回答：是的，我在中文的训练数据上使用了百度文心。</p><p>&nbsp;</p><p></p><h4>字节跳动公布“OpenAI 服务被禁”澄清</h4><p></p><p>&nbsp;</p><p>近日有外媒报道称，字节跳动在使用 OpenAI 技术开发自己的大语言模型，违反了 OpenAI 服务条款。对此，字节跳动相关负责人回应称，公司在使用 OpenAI 相关服务时，强调要遵守其使用条款。公司也正与 OpenAI 联系沟通，以澄清外部报道可能引发的误解。以下是字节跳动使用 OpenAI 服务相关情况的介绍：</p><p>&nbsp;</p><p>今年年初，当技术团队刚开始进行大模型的初期探索时，有部分工程师将 GPT 的 API 服务应用于较小模型的实验性项目研究中。该模型仅为测试，没有计划上线，也从未对外使用。在 4 月公司引入 GPT API 调用规范检查后，这种做法已经停止。早在今年 4 月，字节大模型团队已经提出了明确的内部要求，不得将 GPT 模型生成的数据添加到字节大模型的训练数据集，并培训工程师团队在使用 GPT 时遵守服务条款。9 月，公司内部又进行了一轮检查，采取措施进一步保证对 GPT 的 API 调用符合规范要求。例如分批次抽样检测模型训练数据与 GPT 的相似度，避免数据标注人员私自使用 GPT。未来几天里，字节会再次全面检查，以确保严格遵守相关服务的使用条款。</p><p>&nbsp;</p><p></p><h4>微软承认必应 Copilot 存在严重“幻觉”漏洞</h4><p></p><p>&nbsp;</p><p>12 月 18 日消息，研究机构 AI Forensics 今年 8 月至 10 月对微软必应搜索引擎内置的 Copilot 功能进行调查，结果显示在部分场合中，Copilot 有 1/3 的几率输出错误答案。据此，该机构认为相关功能存在严重“幻觉”漏洞。</p><p>&nbsp;</p><p>今年 10 月研究人员已经向微软提交上述问题，微软虽然承认并声称“计划解决相关‘幻觉’漏洞”，但在今年 11 月，研究人员再次进行测试，发现必应 Copilot 的表现并未改善。</p><p>&nbsp;</p><p></p><h4>OpenAI 工程师自曝开发 ChatGPT 仅用时 8 天</h4><p></p><p>&nbsp;</p><p>最近，OpenAI 工程师 Arun Vijayvergiya 在社交媒体上发文庆祝 ChatGPT 生日时曝出：ChatGPT 的开发竟然只用了 8 天。这位工程师表示，一年前的今天，自己报名了这项全世界演示的研究预览。8 天内，团队完成了产品制作和上线的全部流程。那时，没人能预料，世界会发生怎样的变化。</p><p>&nbsp;</p><p>据悉，当时 OpenAI 的一些叛逃员工成立的 Anthropic，马上就要发布大模型产品了。为了抢在他们前面发布AI聊天机器人，OpenAI 团队用 Next.js 写了个网页、调了个接口。然后，掀起全世界 AI 风暴的 ChatGPT，就此诞生。</p><p>&nbsp;</p><p></p><h4>Debian 准备停止支持 i386 架构</h4><p></p><p>&nbsp;</p><p>Debian GNU/Linux 发布团队在最近的 DebConf 迷你会议上做出决定：在不久的未来 Linux kernel、Debian Installer 和 Debian 镜像团队将停止支持 i386 架构。此后运行 i386 有两个选项：作为 amd64 上的多架构选项；作为另一个架构系统上的 i386 chroot。大部分 Linux 发行版都早已停止支持 i386，Debian 最终做出这一决定并不令人意外。Debian 大约每两年发布一个大版本，最新版本 Debian 12 Bookworm 是在 2023 年 6 月发布的，下一个版本代号为 Trixie 的 Debian 13 预计要到 2025 年。</p>",
    "publish_time": "2023-12-25 09:21:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "央行公示四川第 5 批金融科技创新应用；浙商银行打造科技金融服务生态；《消费金融公司管理办法》多项涉及金融科技 | 金融科技资讯",
    "url": "https://www.infoq.cn/article/jLhOmf1KBImBB1ZOA46E",
    "summary": "<p></p><h2>央行公示四川第 5 批金融科技创新应用，涉智能合约、AI</h2><p></p><p></p><p>近日，四川省第五批<a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">金融科技创新</a>\"监管工具创新应用对外公示。该批次金融科技创新应用申请机构包括新网银行、泸州银行，以及电商等主体，涉及的技术包括大数据、人工智能、区块链、智能合约等，将其应用于消费金保证监管、普惠信贷领域。</p><p></p><p>其中，新网银行与天翼电商上海分公司合作，应用区块链技术，将新网银行、天翼电商上海分公司、电信运营商相关的消费数据、履约数据、保证金数据、用户承诺在网协议数据等进行上链存证，保障数据可追溯和不可篡改，为智能化管理保证金提供可靠数据支持；运用电子签名、可信时间戳等技术，对上链存证的数据增加签名和时间戳，确保链上数据的真实性和不可抵赖性；运用智能合约技术，对上链存证的业务数据设定协议规则，在达到协议约定条件时自动触发交易资金划付等流程，提升消费资金处理效率，防范相关流程的人为干预风险；运用大数据等技术，将消费用户、时间、地点、频次等数据进行分析、挖掘与处理，构建反欺诈风险监测模型，对存在短时间内大量涌入、操作地集中等情况的异常交易行为进行实时监测及管控，预防营销作弊、恶意套利等风险，提高银行风险防控能力。</p><p></p><p>泸州银行基于大数据技术，将贷款记录、还款记录、逾期记录、黑名单等行内数据，以及司法、征信、信用评分、多头借贷等行外数据，进行分析挖掘与处理，为银行信贷风险管控提供多维度数据支持；运用人工智能技术，对行内外数据进行特征学习和样本训练，构建普惠信贷风险评估模型，有效识别、评估客户信贷风险，同时，构建风险评估质量分析模型，对信贷风险评估模型运行情况进行监测并调优，推动其不断优化完善，提升银行信贷风控能力。</p><p></p><h2>上海 6 个金融科技创新应用结束测试，涉及多方安全、远程视频银行等</h2><p></p><p></p><p>12 月 19 日，上海市金融科技创新监管工具实施工作组发布《关于上海市金融科技创新监管工具创新应用结束测试的公告》。</p><p></p><p>公告显示，根据《中国人民银行关于发布〈金融科技创新应用测试规范〉等 3 项金融行业标准的通知》（银发〔2020〕249 号）、《中国金融科技创新监管工具白皮书》等规则，以下 6 个创新应用结束测试：</p><p></p><p>交通银行股份有限公司、中移（上海）信息通信科技有限公司、上海理想信息产业（集团）有限公司、上海富数科技有限公司申请的“基于多方安全图计算的中小微企业融资服务”（创新应用编号：9131000010000595XD-2020-0002，创新应用类型：金融服务）成功完成测试。</p><p></p><h2>《消费金融公司管理办法》征求意见，多项涉及金融科技</h2><p></p><p></p><p>12 月 18 日，国家金融监督管理总局发布《消费金融公司管理办法（征求意见稿）》（以下简称《征求意见稿》）。</p><p></p><p>《征求意见稿》明确，申请设立<a href=\"https://www.infoq.cn/article/fE084eOJxKOxJyD1Z7kB?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">消费金融</a>\"公司应当建立与业务经营和监管要求相适应的信息科技架构，具有支撑业务经营的必要、安全且合规的信息系统，具备保障业务持续运营的技术与措施。</p><p></p><p>消费金融公司应当构建欺诈风险防控体系，通过完善反欺诈模型与相关技术手段，有效识别欺诈行为，确保借款人身份数据与借款意愿真实有效，保障信贷资金安全。消费金融公司应当建立与信息系统运行管理模式相匹配的信息科技风险管理体系，强化网络安全、业务连续性、服务外包等领域的风险防控，保障数据安全及业务系统平稳、持续运行。</p><p></p><p>消费金融公司应当建立消费者适当性管理机制，按照规定开展贷前审查，运用信息科技等手段提升客户画像精准度，审慎评估消费者收入水平和偿债能力。</p><p></p><h2>印度央行宣布将建立新的金融科技存储库和云设施</h2><p></p><p></p><p>近日，印度中央银行，即印度储备银行（Reserve Bank of India,RBI）宣布，将建立一个新的金融科技存储库和一个为金融服务公司量身定制的云设施，作为其最近发展和监管举措的一部分。</p><p></p><p>计划在 2024 年 4 月投入运营的存储库将收集关于金融科技公司的重要信息，为监管机构和其他利益相关者提供帮助。印度央行将鼓励但并不强制印度的金融科技公司提交有关其运营、技术、产品和财务的详细信息，印度央行认为，这些综合数据将有助于更好地制定政策。</p><p></p><p>除了存储库，印度央行还将为印度的银行和金融机构建立一个专用的<a href=\"https://www.infoq.cn/article/KUzOm728U94NC6C4Vvvz\">云设施</a>\"。这一举措被视为对当前依赖各种公共和私有云服务的战略转变。新的云设施预计最初将由印度央行的子公司 IFTAS 管理，然后过渡到一个部门所有的实体，旨在增强金融部门的数据安全性和可扩展性。虽然有关云设施的具体推出日期尚未公布，但预计将在不久的将来分阶段推出。</p><p></p><h2>浙商银行发布多项创新举措打造“科技金融”服务生态</h2><p></p><p></p><p>近日，浙商银行在科技金融服务发布会上围绕科技金融服务“向善”新生态，重磅启新科创企业全图景服务方案，正式发布科创积分贷，联合浙江大学发布人才金融生态指数，成立“科技金融服务生态圈”联盟，共筑科技金融服务生态圈，以对新科技、新赛道、新市场的金融支持，切实助力金融强国建设。</p><p></p><p>本次发布会上，浙商银行推出全面焕新的“智汇科创 善行未来”科技金融服务方案，聚焦企业初创、取得订单、引入创投等 10 大场景，配套推出 16 大科技系列金融产品，不仅优化迭代的传统优势产品“人才支持贷”“科创共担贷”等，还创新推出业内领先的“科创积分贷”“科创激励贷”等特色产品，全面焕能升级科创企业全图景服务方案，满足企业不同成长阶段多元化金融需求，为企业提供“全方位、全链条、全周期”综合金融服务。</p><p></p><h4>内容推荐</h4><p></p><p></p><p>11月19日-20日在上海成功举办的首届 FCon 全球金融科技大会，以「科技 + 金融，激发创新力量」为主题，汇聚了来自金融龙头企业的数百名技术高管，掀起一场探讨新时代金融科技未来的高潮。经征得大会分享嘉宾同意，InfoQ 数字化经纬为您奉上精彩演讲 PPT！关注「InfoQ 数字化经纬」，回复「金融数智化」即可获取 PPT，深度洞悉科技趋势，助您引领金融创新未来！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/2271237afa20508ead9a2a3f86d7e71e.png\" /></p><p></p>",
    "publish_time": "2023-12-25 10:39:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "微软发布Orca 2 LLM，表现优于10倍参数模型",
    "url": "https://www.infoq.cn/article/gH6QGpcMJXWtlb1EOyx0",
    "summary": "<p><a href=\"https://www.microsoft.com/en-us/research/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">微软</a>\"发布 <a href=\"https://www.microsoft.com/en-us/research/project/orca/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Orca 2 LLM</a>\"，这是 <a href=\"https://ai.meta.com/llama/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Llama 2</a>\" 的一个调优版本，性能与包含10倍参数的模型相当，甚至更好。Orca 2 使用了一个合成训练数据集和一项称为 Prompt Erasure（提示词擦除） 的新技术来实现这一性能。</p><p></p><p>Orca 2 使用了师生模式的训练方案，其中一个较大、较强的 LLM 作为另一个较小的 LLM（学生）的老师，老师的目标是提升学生的性能，使其与更大模型的性能相媲美。微软的训练技术教会较小的模型多种推理技巧，并教其如何为特定任务选择最有效的技巧。为此，老师被赋予了复杂的提示词来触发某种推理行为。不过，在一种被称为 Prompt Erasure 的方案中，学生只得到任务要求和期望的响应，而不是老师的提示词。在基准测试中，一个拥有13B参数的 Orca 2 模型的表现超过了一个13B参数的基准 Llama 2 模型，提升了47.54%。而一个拥有7B参数的 Orca 2 模型在推理任务方面与一个拥有70B参数的 Llama 2 模型相当，甚至更好。</p><p></p><p>尽管像 ChatGPT 这样的LLM在给定少量提示词的情况下通常表现良好，但由于其内存和计算需求较大，托管这些模型极具有挑战性。经过调优的较小的模型也可以表现良好，许多研究人员已经在研究使用较大LLM生成的合成数据集对它们进行训练。InfoQ 最近报道了谷歌的 <a href=\"https://www.infoq.com/news/2023/10/google-distillation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Distilling Step-by-Step</a>\" 方法，该方法会让老师LLM自动生成一个小型的调优数据集，其中包含输入和输出标签，以及为何选择输出标签的“基本原理”。InfoQ 还报道了 Stability AI 的 <a href=\"https://www.infoq.com/news/2023/08/stable-chat/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Stable Beluga</a>\" 模型，它使用微软原始的 <a href=\"https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Orca 1</a>\" 方案进行训练，该方案使用了 Explanation Tuning，其中老师LLM被提示“生成详细答案”。</p><p></p><p>与 Orca 1 类似，Orca 2 训练数据集是由老师LLM生成的，而老师LLM收到了详细的提示词。然而，微软新的训练方法 Cautious Reasoning将训练任务与提示词相结合，引导老师LLM使用特定的问题解决策略，如“一步一步”或“解释你的答案”。然后在学生的训练过程中，老师的提示词被删除，这促使学生学会选择正确的策略。</p><p></p><p>为了评估这种方法，微软将 Orca 2 模型的性能与几个基准模型进行了比较，包括 Llama 2、ChatGPT（GPT-3.5）和 GPT-4。基准任务包括推理、语言理解、文本完成和摘要。在推理基准测试中，13B参数Orca 2模型优于除ChatGPT和GPT-4之外的所有基准。他们还发现，给Orca 2一个“谨慎”的系统提示词（“你是一个谨慎的助手，你会仔细遵循指示”）相比无系统提示会略微提升其性能。</p><p></p><p>有几位用户在 X 上发表了关于 Orca 2 的帖子。<a href=\"https://twitter.com/MatthewBerman/status/1730690014202458357?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">一位用户指出</a>\"：“你不需要用‘一步一步解释’这样的技巧来提示它。它自己知道。” AI 研究员 <a href=\"https://twitter.com/rudiranck/status/1729816556249530546?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Rudi Ranck 写道</a>\"：</p><p></p><p></p><blockquote>许多绝妙的想法都很简单……就像 Orca 2 中的“提示词擦除”一样：完整的提示词不会呈现给模型，而只呈现任务和答案（它过滤了生成这些答案所使用的完整提示词）。这有助于模型在更高层次上制定策略。这是一篇非常好的论文。我强烈建议通读全文。</blockquote><p></p><p></p><p><a href=\"https://huggingface.co/microsoft/Orca-2-7b?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">7B</a>\" 和 <a href=\"https://huggingface.co/microsoft/Orca-2-13b?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">13B</a>\" 参数的 Orca 2 模型可在 Huggingface 上获得。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/12/microsoft-orca-2-llm/\">https://www.infoq.com/news/2023/12/microsoft-orca-2-llm/</a>\"</p>",
    "publish_time": "2023-12-25 11:21:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "亚马逊云科技资深培训讲师张文举博士确认出席 QCon 上海，分享借助 Langchain 与 LLM Agent 加速生成式 AI 应用开发",
    "url": "https://www.infoq.cn/article/Xk9v6UcAX7sZBafuqz7u",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1225&amp;utm_content=zhangwenju\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。亚马逊云科技资深培训讲师张文举博士确将发表题为《<a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5697?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1225&amp;utm_content=zhangwenju\">借助 Langchain 与 LLM Agent 加速生成式 AI 应用开发</a>\"》主题分享，探讨 LLM-based Agent 特性，业界发展情况以及如何利用 Agent 构建生成式 AI 应用。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5697?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1225&amp;utm_content=zhangwenju\">张文举博士</a>\"，亚马逊云科技认证专家讲师，主要研究方向是大数据和 AI，近期主要关注生成式 AI 研究与应用。他在本次会议的演讲内容如下：</p><p></p><p>演讲：借助 Langchain 与 LLM Agent 加速生成式 AI 应用开发</p><p></p><p>本次分享将会介绍 LLM-based Agent 特性，业界发展情况以及如何利用 Agent 构建生成式 AI 应用，让你快速借助 LangChain 和 Agents for Amazon Bedrock 构建企业自己的可落地的生成式 AI 应用。</p><p></p><p>演讲提纲：</p><p></p><p>LLM-based Agent 总揽Agent 开发框架与 LangChain如何使用 Agents for Amazon Bedrock 构建生成式 AI 应用</p><p></p><p>听众收益：</p><p></p><p>了解 LLM-based Agent 特性与发展初步掌握如何利用 LLM-based Agent 构建生成式 AI 应用</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart\">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的性能优化</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>12 月 28-29 日，QCon 全球软件开发大会即将落地上海，中国科学院外籍院士、国际数据库专家樊文飞院士，英特尔大数据技术全球 CTO 戴金权等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。</p><p></p><p>这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，感兴趣的朋友可以扫描下方二维码，查看大会详细日程。咨询购票可联系票务经理 18514549229。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-12-25 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "高性能微服务框架Quarkus 3.7正式发布，全面支持Java 17",
    "url": "https://www.infoq.cn/article/ycMZXcL4QGyZv0odj72H",
    "summary": "<p>Quarkus 3.7 计划于 2024 年 1 月发布，用它来构建和运行应用程序所需的最小 JDK 版本将是 Java 17。这其中一个原因是，OpenJDK 社区在 2023 年 9 月停止了对 Java 11 的主动支持。除此之外，Quarkus 依赖项也将其基线升级到了 Java 17。发布于 2023 年 4 月的 Quarkus 3.0 已经放弃对 Java 11 的支持。</p><p></p><p>如果启用了的话，Quarkus 就会收集构建时间分析数据。当前数据显示，只有不到 8% 的用户在 Java 11 上运行 Quarkus 构建。其他调查（如 New Relic 的 2023 年 Java 生态系统现状）也显示出，Java 11 及以上版本的使用量有所增加，而 Java 8 的使用量有所下降。Oracle 已经在 2023 年 9 月停止了针对 Java 11 的技术支持服务，而红帽公司也将在 2024 年 10 月停止对 OpenJDK 的全面支持。不过，其他组织将继续支持 Java 11，比如 Adoptium 将至少支持到 2027 年 10 月。</p><p></p><p>Java 基线的提升意味着 Quarkus（插件）开发人员现在应该支持并在 Java 17 上测试他们的代码，并且可以使用 Java 17 的新特性。然而，在这个时间点上，Quarkus 并没有使用 Java 17 的特性，他们也不需要这些特性来构建 Quarkus。提升基线的一个主要原因是 Quarkus 团队希望支持的 Quarkus（插件）的依赖项需要 Java 17。例如，计划于 2024 年发布的 Hibernate ORM 7 将需要 Java 17。</p><p></p><p>无法升级到 Java 17 的项目可以继续使用 Quarkus 3.6。不过，我们建议在项目中使用 Quarkus 3.2，因为它是支持 Java 11 的最新长期支持（LTS）版本。然而，Quarkus LTS 版本支持只有一年，因此，升级 Java 仍然很重要。</p><p></p><p>Quarkus 3.7 也支持更新的 JDK 版本，如 Java 21，但 Quarkus 选择将基线选定为 Java 17，那是因为有许多项目尚未升级到该 Java 版本之上。</p><p></p><p>Quarkus 所遵循的策略与其他需要 Java 17 的 Java 工具和框架相同，例如于 2022 年 11 月发布的 Spring Boot 3。这也意味着，所有使用 Spring Boot 3 的项目（如 JHipster）现在也需要 Java 17。2023 年 7 月发布的 Micronaut 4 和 SonarQube 服务器是另外两个需要以 Java 17 为基线的 Java 项目的示例。</p><p></p><p>要了解更多细节，请查看 Quarkus 团队的官方声明。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.infoq.com/news/2023/12/quarkus-3-7-java-17/\">https://www.infoq.com/news/2023/12/quarkus-3-7-java-17/</a>\"</p>",
    "publish_time": "2023-12-25 11:44:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据库的下一场革命：S3 延迟已降至原先的 10%，云数据库架构该进化了",
    "url": "https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy",
    "summary": "<p></p><blockquote>众所周知，在数据库的历史上，每次存储介质的变化都会引发软件的变革。从SAN存储到SSD到大内存到NVM，都触发了数据库内核从理论到工程的演进。</blockquote><p></p><p>&nbsp;</p><p>数据库一直是推动企业数字化和创新的最重要基础设施之一。从关系型数据库到NoSQL数据库、分析型数据库、多模数据库，这个领域正在持续的进化与变革，涌现了大量的新型数据库产品，满足不同企业的应用场景和细分市场需求。</p><p>&nbsp;</p><p>然而，关系型数据库（Relational Database Service，简称RDS）依然占据了数据库整体市场的大半壁江山，根据IDC近期发布的《2023年上半年中国关系型数据库软件市场跟踪报告》，2023年上半年中国关系型数据库软件市场规模为17.5亿美元，其中公有云关系型数据库的市场份额约为59%。而根据Garnter 《Forecast Analysis: Database ManagementSystems, Worldwide》报告中的预测，2023年全球关系型数据库总市场达到838亿美元，在数据库总盘子里公有云部分占比也约为59%。</p><p>&nbsp;</p><p>RDS通常以云盘（即块存储）作为其核心存储基础设施。AWS的RDS服务便是一个例子，其所有实例规格均采用了 Elastic Block Store（EBS）云盘。对于广泛使用RDS的用户，以及在公共云上购买虚拟机来自建数据库服务的用户，云盘是否就代表了存储的最终选项呢？ 答案是“No”。在技术革新缺席的前提下，云盘在性价比和计费策略方面失去了其竞争优势，我们判断其会从云厂商的主导产品降级为边缘选项。</p><p>&nbsp;</p><p>公共云的关系型数据库将会从依赖云盘向利用好对象存储，向采用更加云原生的架构的新时代迈进。为了适应对象存储，充分发挥其优势，数据库的架构也势必需要进行大刀阔斧的改造，水平扩缩容、容灾技术以及存储引擎的数据格式都将随之变化。</p><p>&nbsp;</p><p></p><h2>云盘存在的问题</h2><p></p><p>&nbsp;</p><p>云盘的第一个痛点是定价比较高。例如，一块1TB的标准型云盘（含5万IOPS及350MB带宽），云服务商会收取约1000元每月的基础费用。为了服务海量的客户基数，云盘的IOPS（输入/输出操作每秒）和带宽通常会在软件上通过流控进行限制，免费的IOPS额度通常在1000至3000之间，而带宽限额大约在150MB/s。超出这些限额，用户则需要向云厂商购买额外的IOPS和带宽，若要将IOPS提高至10万（这一性能水平对于企业级NVMe SSD来说并不算特别高），用户需要额外支付1500元每月的预配置性能费用。然而，1TB的企业级NVMe SSD的一次性购买成本还不到千元。</p><p>&nbsp;</p><p>第二个限制是云盘的弹性尚未完全向用户开放。主流云厂商的云盘仅支持扩容，不支持缩容。这一限制导致业务在进行缩容时必须采取曲线救国的策略，即通过逻辑复制的方式，先将数据迁移到一块容量更小的新云盘上，然后才能释放原有的较大云盘。另外，部分云厂商，例如AWS的EBS对控制面还存在限流保护，限制每两次扩容操作之间必须间隔六小时。这样的措施虽然保证了服务的稳定性，但也在一定程度上限制了用户对存储资源的即时调整能力。这些都限制了上层软件基于云盘实现按存储使用量付费的能力。</p><p>&nbsp;</p><p>云盘的第三个局限性在于其灾难恢复能力局限于单个可用区（AZ）。云厂商建议的最佳实践是，为了实现更高级别的业务连续性，客户应该采取跨可用区的灾难恢复策略。这意味着，如果客户想要为他们的数据库实现跨AZ的灾难恢复，他们不得不购买多个云盘。然而，这种额外投资并非最经济的选择，因为云盘定价已经包含了单AZ多副本数据的成本。当用户为了实现跨AZ的冗余而购买更多云盘时，存储层面的多副本与数据库层面的多副本机制叠加在一起，便产生了资源上的重复配置。</p><p>&nbsp;</p><p>最后，在面对高性能数据库需求时，云盘的性能也会成为限制整体系统性能的薄弱环节。云盘使用分布式架构，通过Erasure coding机制将数据分割成多个小片段，并将其冗余存储在多个服务器上。进一步的，还可以对数据进行压缩。这些技术以牺牲一定的性能为代价，换来了显著的可扩展性和成本效益。由于所有I/O操作都需要跨越网络，因此云盘的延迟通常比本地盘高一个数量级。</p><p>&nbsp;</p><p>旗舰数据库产品如Aurora和PolarDB采纳了更新的设计理念，构建了定制化的分布式存储集群，来解决云盘的性能问题。Aurora采用了日志即数据库的理念来减少数据库节点与存储节点之间的数据传输量，PolarDB则使用RDMA和NVM来优化I/O延迟，两者都支持多个数据库节点并发访问存储节点的共享数据架构。然而，这些存储系统与数据库之间的通信是通过私有接口实现的，并不对外部用户开放。另外这些专用存储的定价比云盘更高。</p><p>&nbsp;</p><p></p><h2>其他的云存储选项</h2><p></p><p>&nbsp;</p><p>和云盘相比，云上的本地盘实例存储性价比要高很多。实例存储采用了类似于SR-IOV和SPDK的高效虚拟化技术。因此实例存储的延迟和带宽都接近于物理SSD的性能。实例存储的价格也经济很多，折算下来1TB实例存储每月的单价在300元以下。然而，实例存储在数据持久性方面存在局限。由于其设计为单副本存储，如果宿主机发生故障，存储在其中的数据可能会遭受永久性丢失。此外，当虚拟机迁移至另一台宿主机时，实例存储中的数据也将被清除。因此，实例存储不适合在需要高可靠性和数据持久性的应用场景中作为主要存储介质。</p><p>&nbsp;</p><p>对象存储的价格是最低的，1TB一个月的存储成本约为120元。低定价得益于其软硬件的协同优化。在软件层面，采用更激进的EC和压缩算法来提高存储密度；而在硬件方面，通过使用高密度盘和专用服务器，进而降低服务器、机架及电力的均摊成本。对象存储系统还利用定制的FPGA硬件来减轻CPU处理网络和数据的压力。在资源调度上，对象存储一般会采用大集群的部署方案，这有利于降低碎片率，提高系统的整体水位线。得益于其分布式大资源池的设计，对象存储能够支持10Gb乃至100Gb的访问带宽。此外，对象存储通常还具备跨可用区域（AZ）的灾难恢复能力。</p><p>&nbsp;</p><p>对象存储的缺陷是其延迟比较高，首字节访问延迟可能高达数十毫秒。这一问题部分源于早期对象存储解决方案通常使用HDD作为存储媒介，并且在软件层面，I/O请求的排队处理也会造成一定延迟。然而，高延迟并非对象存储的本质缺陷，而是由于成本考虑和产品定位所做的权衡。</p><p>&nbsp;</p><p>实际上，在技术层面，构建低延迟的对象存储系统是完全可行的。例如最近在AWS Reinvent大会上亮相的\"S3 Express One Zone\"服务，将延迟减少至原先的十分之一，实现了毫秒级的响应速度，接近传统文件存储NAS系统的水平。刚才说了，S3的高延迟是产品定位和技术的权衡，有得必有失，\"S3 Express One Zone\"不再支持跨AZ容灾，所有数据都在单AZ内，数据的持久性下降。此外\"S3 Express One Zone\"是更强了，也更贵了，其价格不仅是S3标准版的7倍，也超过了自家云盘，达到EBS gp3的2倍。</p><p>&nbsp;</p><p></p><h2>解法：持久性与延迟的解耦</h2><p></p><p>&nbsp;</p><p>用一张图可以总结下几个云存储产品各自的特点和不足：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14c4b4569e94e398a0b8df3fd326b9cd.jpeg\" /></p><p></p><p>可以看到，当前市场上尚未出现一种云存储产品，可以同时在低成本、低延迟、高持久性几个维度上都达到令人满意的程度。</p><p>&nbsp;</p><p>随着云计算的渐进普及，降低成本逐渐成为用户的首要诉求，一些公司，比如37signals和X，已经基于成本效益的考量决定下云，从公共云平台转回到传统的IT基础设施。在这种趋势下，云数据库服务供应商面临的紧迫挑战是如何在现有的存储IaaS产品基础上，构建更有成本竞争力的数据库服务。</p><p>&nbsp;</p><p>一个方案是基于实例存储搭建多副本的数据库系统。前文说过，实例存储是单副本存储，它存在一个风险：一旦托管它的宿主机发生故障或者相应的虚拟机迁移，就可能导致数据丢失。这一方案的理论基础建立在一个假设之上：多个实例存储丢失数据的概率是相互独立的。基于这个假设，增加副本数量，会降低多个实例存储同时丢失数据的概率。</p><p>&nbsp;</p><p>然而，这个假设并不总是成立，在现实里，因为工程的限制，多个实例存储可能会因为共同的原因导致同时数据丢失。例如，可能所有存储都使用了同一批次存在固件缺陷的SSD，导致多台主机同时下线；或者，云服务提供商的控制系统发生故障，引起大规模的虚拟机迁移。因此，对于那些对数据持久性有极高要求的生产环境来说，这种方案并不适用。</p><p>&nbsp;</p><p>另一个方案是将存储的持久性和延迟两个特性进行分离，通过对象存储实现高持久性，通过实例存储/云盘来实现低延迟。尽管对象存储可以提供低成本、高带宽和跨可用区的数据持久性，但在作为关系型数据库主存储时，它的读写延迟成为了一个显著的挑战。为了解决这一问题，我们采用一种分层存储策略，将存储解耦为三个组件：持久化、写缓存和读缓存，分而治之。</p><p>&nbsp;</p><p>在这种设计中，对象存储仅负责数据的持久化，为系统提供灾难恢复保证。写操作会通过缓存层降低写延迟，技术上可以利用如Raft这类同步协议，缓存到低延迟的存储介质，例如本地磁盘或云盘，甚至可以考虑追加写入到消息队列服务。读操作也是，通过缓存层降低读延迟，可能是基于本地磁盘、NAS，或者是专为降低延迟而优化的对象存储加速器，将负责实现快速的数据加载。</p><p>&nbsp;</p><p>我在2021年SIGMOD发表的\"LogStore: A Cloud-Native and Multi-Tenant Log Database\"论文中就使用了这一方案。LogStore是一个内部使用的云原生日志数据库，底层采用对象存储，为了降低写入延迟，写入的日志先通过raft协议刷到3副本的本地SSD中即提交，再由Leader节点将数据写入到对象存储中。</p><p>&nbsp;</p><p>硅谷初创公司 Neon 也采用了这一方案。Neon 宣称是开源版本的 Aurora Postgresql，采用计算与存储分离架构对开源的 PostgreSQL 数据库进行改造。其存储层由Safekeeper和PageServer两个组件构成。Safekeeper负责持久化和复制WAL记录，使用Paxos协议实现分布式一致性，将WAL记录保存在EBS上，并将提交的WAL记录传递给PageServer。 PageServer负责将WAL记录应用到Page上，生成Page的快照，并将Page存储在对象存储中，缓存Page在实例存储上，以提高读取性能。在Neon的实现中，数据库的主存储是对象存储，写缓存是EBS，读缓存是实例存储。</p><p>&nbsp;</p><p></p><h2>新的商业模式契合对象存储</h2><p></p><p>&nbsp;</p><p>近年来，公共云数据库服务正日益倾向于提供Serverless运行模式，以迎合现代开发者对弹性的需求。早在18年，AWS就推出了Aurora Serverless，22年，AWS又推出了Aurora Serverless V2。在今年的AWS Reinvent大会上，AWS一口气发布了三款Serverless数据库产品：Aurora Limitless Database、ElastiCache Serverless和Redshift Serverless，已经隐约摆出了全系产品Serverless化的架势。</p><p>&nbsp;</p><p>Serverless 数据库之所以深受青睐，一方面是因为其低门槛的使用成本，另一方面则是因为其能够在面临突如其来的机会时提供快速且灵活的扩展能力。因此，Serverless 数据库成为了初创公司和那些流量增长高度不可预测的创新产品的理想选择。</p><p>&nbsp;</p><p>本质上Serverless数据库采用了一种”按实际使用付费(pay as you go)“的新商业模式。在Serverless数据库模式下，云的dbPaaS软件会实时追踪用户的活动，从而精确计算其资源使用情况。无论是执行SQL命令所消耗的CPU时间，还是读写操作产生的IO次数，亦或是数据存储的总量，所有这些都被量化为资源单位（例如计算单元CU、请求单元RU），并据此计费。技术如何支撑这种新的商业模式，比如数据库如何根据工作负载的变化智能的扩缩容取决于dbPaaS和内核的实现。</p><p>&nbsp;</p><p>在业界实现 Serverless 架构的常见方法之一是通过给基于计算与存储分离的架构增加自动伸缩（Auto-Scale）功能。因此云原生数据库如 Aurora 和 PolarDB都能比较快的推出其Serverless版本。在这样的架构下，存储层通常设计为多租户模式，以实现资源共享和成本效率；而计算层则通常是单租户的，确保了性能和隔离性。随着数据库负载的增加，这种模式允许计算节点通过弹性扩容迅速提升处理能力。相对地，当负载减少时，计算节点可以弹性缩容，甚至完全停止，以优化资源使用并减少成本。</p><p>&nbsp;</p><p>CockroachDB 也采纳了这种 Serverless 架构。为适应这种模式，他们对底层的KV存储层进行了重构，转变为支持多租户的架构，并且SQL节点和KV节点不再运行在同一台服务上，走向计存分离架构。</p><p>&nbsp;</p><p>由此可见，存储池化是 Serverless 数据库架构中的核心设计原则。只有通过存储池化，才能做到按存储的使用量计费。云厂商的产品Aurora 和 PolarDB 实现这一机制的策略是构建他们自己的分布式存储集群。然而，这种做法要求云厂商在产品推出前进行大规模的一次性投资，并且在产品初期推广阶段承受由于存储使用未达预期而产生的潜在亏损。</p><p>&nbsp;</p><p>然而，对于那些希望利用开源软件自行搭建 Serverless 数据库服务的大型企业用户、以及提供Serverless数据库服务的小型的第三方数据库厂商来说，却存在一个潜在的陷阱。如果构建 Serverless 服务之前需要首先投资搭建一个存储池，他们就陷入了传统模式——即预购硬件并在其基础上构建数据库服务，这种做法与 Serverless 的理念相悖。最理想的技术方案是做到不囤货，实际使用多少存储，就向云厂商付多少存储费用。</p><p>&nbsp;</p><p>此外，Serverless 数据库的设计理念自然包含了Region 级别的容灾功能，这是其另一项核心能力。用户选择 Serverless 服务，意味着他们已经不再关注运行数据库的具体服务器或所在的可用区（AZ）。在这种服务模式下，用户不太愿意去单独从三个不同的 AZ 购买一套 Serverless 数据库，再手动设置数据同步——这种方式与 Serverless 的易用性相悖。因此，Serverless 数据库必须确保其计算节点池和存储池都在跨 AZ 环境中提供无缝的容灾能力。</p><p>&nbsp;</p><p>存储池化、按使用量计费，以及AZ级的灾难恢复，Serverless数据库对存储的需求正是对象存储产品的关键特性。而传统云数据库使用的云盘在成本、弹性能力、容灾能力上均弱于对象存储产品。基于这些考量，我们预见在按使用量计费这种新的公共云商业模式被广泛接受后，未来的Serverless数据库都会把对象存储做为首选。</p><p>&nbsp;</p><p></p><h2>展望：以对象存储为中心的新架构</h2><p></p><p>&nbsp;</p><p>随着数据库存储基础设施向对象存储的逐步迁移，我们还可以预期新架构里会出现以下几个方面的变化：</p><p>&nbsp;</p><p></p><h4>行列混合存储格式</h4><p></p><p>&nbsp;</p><p>数据库存储引擎的数据格式将从纯行存向行列混合格式变化。基于B+树的存储引擎可以采用调高数据页大小，并采用页内列式存储的技术来实现。而基于LSM的存储引擎在这块具有更大优势。</p><p>&nbsp;</p><p>将行存数据转换为Pax格式（行列混合存储格式）存入对象存储有多重好处：</p><p>&nbsp;</p><p>首先，它显著降低了存储成本并减少了数据加载所需的时间。例如，以行列混合存储格式进行数据转换，可以将1TB的数据压缩到仅为原始体积的20%至40%存储在对象存储中。借助25Gb的内网带宽，加载并预热这些压缩数据到缓存的过程大约只需要100秒。</p><p>&nbsp;</p><p>其次，采用Pax格式还能减少数据库内存缓存的消耗，比如说原来计算节点需要32GB的内存，现在只需要12GB内存就能达到同样性能了，进一步的降低计算节点成本。</p><p>&nbsp;</p><p>最后，数据库引擎可以借助Pax格式实现混合事务/分析处理（HTAP），Google的Spanner数据库就是一个例证，它采用了基于Pax的Ressi存储格式，支持HTAP操作。</p><p>&nbsp;</p><p></p><h4>OLTP与数据湖的深度融合</h4><p></p><p>&nbsp;</p><p>传统上，将在线事务处理（OLTP）数据迁移到在线分析处理（OLAP）系统的常规方法依赖于ETL（提取、转换、加载）流程。然而，ETL的过程不仅管理负担重，而且增加了一个容易发生错误的环节。鉴于这些挑战，近年来业界出现了向\"NoETL\"解决方案的转变，将ETL过程内置在数据库中，以提高用户进行数据管理和分析的易用性。例如去年AWS推出了从其Aurora服务直接到Redshift数据仓库的NoETL集成。</p><p>&nbsp;</p><p>而在OLTP数据库内核中，原生支持将全量数据以行列混合存储格式持久化并写入对象存储，会更近一步，促进OLTP数据库与现代数据湖技术的协同工作。利用技术如Iceberg、Hudi和Delta Lake，OLTP数据库可以无缝地将在线数据直接写入数据湖环境，帮助企业能够更简单、实时地管理和分析其数据资产。</p><p>&nbsp;</p><p></p><h4>使用K8s管理数据库变得普及</h4><p></p><p>&nbsp;</p><p>自从K8s问世以来，管理有状态服务，尤其是数据库，一直是个充满挑战的领域。一方面，随着KubeBlocks以及各种数据库Operator等开源的数据库控制面管理软件的发展，我们拥有越来越多的工具支持在K8s上对数据库进行高效管理。此外，当数据库的持久性是通过K8s外部的对象存储来保证时，对K8s中的数据库Pod进行高可用切换、节点迁移、数据迁移、备份等各种管理任务的复杂性会得到进一步减轻，执行效率也会更高。两个方向的发展相辅相成，会推动在K8s上管理数据库的普及。</p><p>&nbsp;</p><p>公共云中的关系型数据库正处在一个转型的临界点，即从依赖云盘向利用对象存储的新时代迈进。过去的RDS模式是云托管，客户需要预先选择硬件配置（云服务器、云盘、VPC和负载均衡器），然后在这些硬件上部署数据库内核。而在对象存储时代，没有预置IaaS的限制，数据库内核进一步云原生化更有弹性更加强大，这会是数据库领域近期最大的技术变革。</p><p>&nbsp;</p><p>作者简介：</p><p>曹伟（鸣嵩），云猿生数据创始人 &amp; CEO，发起了开源云原生数据库管控平台KubeBlocks项目。前阿里云数据库总经理 / 研究员，云原生数据库 PolarDB 创始人。中国计算机学会数据库专委会执行专委，中国计算机学会开源专委会执行专委，获得 2020 年中国电子学会科技进步一等奖，在 SIGMOD、VLDB、ICDE、FAST 等数据库与存储国际顶级学术会议发表论文 20 余篇。</p>",
    "publish_time": "2023-12-25 14:23:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "苹果的封闭生态为大模型打开！发布开源多模态大模型、每天为AI烧百万美元，零碎的Android 生态打得过吗？",
    "url": "https://www.infoq.cn/article/T3JEAJI1z6SvvMSUuIAN",
    "summary": "<p>“苹果公司在LLM方面一直表现不佳，但他们一直在不断发展‘硬件+软件人工智能’堆栈，没有太多耀眼的广告。我认为，如果新的 iOS 版本突然让 OpenAI/Bard 聊天框看起来可笑地过时，他们可能会击败微软/OpenAI 和谷歌。如果大量人工智能使用转向苹果硬件，它们也会对英伟达构成威胁，Arm 和台积电将获胜。”有网友说到苹果在大模型发展上的状况。</p><p>&nbsp;</p><p>也有网友认为，苹果在大模型上的发力将为其在未来的手机市场竞争中带来优势。他们认为，开源模型加上移动设备的本地数据，即本地化的原生LLM，才是关键，谁在设备上运行得好，谁就卖得好。具体来说，iPhone/iPad/Mac拥有最多、最一致的本地数据生态，许多开源大模型已经可以在iPhone上运行，社区也对M1/M2/M3芯片进行了大量优化。而反观Android生态，情况却不容乐观：三星占据了大部分市场份额，国内五大厂商也占据了相当大的份额，谷歌所占份额极少，碎片化的局面让通用模型运行面临困难。</p><p>&nbsp;</p><p>相比微软等其他巨头在大模型上的高歌猛进，苹果显得很是安静，尤其苹果和哥伦比亚大学的研究人员于在2023 年 10 月低调<a href=\"https://arxiv.org/abs/2310.07704v1\">发布的一个名为 Ferret 的开源</a>\"多模态大模型也没有收到太多关注。当时，该版本包含代码和权重，但仅供研究使用，而非商业许可。</p><p>&nbsp;</p><p>但随着Mistral 开源模型备受关注、谷歌 Gemini 即将应用于Pixel Pro和Android，关于本地大模型为小型设备提供支持的讨论越来越多。而苹果公司也宣布啦在 iPhone 上部署大模型方面取得了重大突破：该公司发布了两篇新的研究论文，介绍了 3D 头像和高效语言模型推理的新技术，被认为可能带来更身临其境的视觉体验，并允许复杂的人工智能系统在 iPhone 和 iPad 等消费设备上运行。</p><p>&nbsp;</p><p>AI 社区中的许多人后来才注意到 Ferret 的发布，他们很开心苹果公司出人意料地进入了开源 LLM 领域，因为苹果公司历来由于封闭的生态而被称为“围墙花园”。下面我们看下这个才开始被热议的项目。</p><p>&nbsp;</p><p>开源地址：</p><p>https://github.com/apple/ml-ferret</p><p></p><h2>多模态大语言模型 Ferret</h2><p></p><p>&nbsp;</p><p>“据我们所知，Ferret是首个能够在多模态大模型中处理自由形式区域输入的成果。”项目研发团队在论文中写道。Ferret是一种新颖的引用与定位多模态大语言模型（MLLM）。之所以选择多模态大模型作为Ferret的设计前提，是因为其拥有强大的视觉语言全局理解能力。&nbsp;</p><p></p><h4>模型架构</h4><p></p><p>&nbsp;</p><p>根据介绍，Ferret主要由用于提取图像嵌入的图像编码器；用于提取区域连续特征的空间感知视觉采样器；以及用于对图像、文本和区域特征进行联合建模的大语言模型组成。</p><p><img src=\"https://static001.geekbang.org/infoq/c3/c31d0a0966e4a68d0eda0eadd7a0e668.png\" /></p><p></p><p>输入。</p><p>&nbsp;</p><p>将图像输入经过预训练的视觉编码器CLIP-ViT-L/14 ，以提取图像嵌入Z ∈ R H×W×C。对于文本输入，使用经过预训练的大模型标记器对文本序列进行标记，并将其投射至文本嵌入T ∈ R L×D当中。</p><p>&nbsp;</p><p>空间感知视觉采样器。</p><p>&nbsp;</p><p>除了常见的点或矩形框之外，团队需要处理的区域形状可能存在很大差异。基于网格的处理方法（例如卷积或patch attention）无法处理不规则形状。与之类似，3D点云也属于不规则形状，而且在3D空间中表现出不同的稀疏性。受到现有3D点云学习方法的启发，团队提出一种空间感知视觉采样器。</p><p>&nbsp;</p><p>空间感知视觉采样器用以获取任意形状区域的视觉特征，同时考虑到这些形状所对应的不同稀疏性。以此为基础，团队将离散坐标与连续视觉特征组合起来以表示输入中的视觉区域，由此构成Ferret中的混合区域表示。凭借上述方法，Ferret就能够处理由区域同自由格式文本混合而成的输入，并可以无缝生成每个可定位对象的坐标和文本，由此在输出中定位所提及的对象。</p><p>&nbsp;</p><p>假设已经给定提取得出的图像特征图Z ∈ R H×W×C 和二值化区域掩模M，团队首先在M内随机采样N个正点。这N个点被输入至级联的块中，每个块包含三个步骤：采样、收集、池化。经过这三个步骤，将获得更少的点和更密集的特征空间。</p><p>&nbsp;</p><p>输出。</p><p>&nbsp;</p><p>在Ferret的输出中，为了实现定位，团队在文本响应中的相应区域/名词之后生成框坐标。例如“图中有一只狗[100,150,300,200]。”通过这种数据格式，模型即可隐式学习当前图像中的可定位内容及其确切位置。</p><p>&nbsp;</p><p>大语言模型。</p><p>&nbsp;</p><p>团队选定Vicuna作为语言模型，这是一种在Llama之上通过指令微调而来的纯解码器大语言模型。在输入大模型之前，图像嵌入先通过额外的线性层进行转换，以匹配文本标记的嵌入维度。</p><p>&nbsp;</p><p>为了使Ferret的引用机制具有开放词汇、指令遵循和健壮性，团队还整理出了一套包含110万个样本的引用与引用指令调整数据集GRIT。</p><p>&nbsp;</p><p>GRIT中包含多个层次的空间知识，涵盖对象、关系、区域描述和复杂推理等要素。GRIT包含三种数据类型：被转换为指认遵循格式的公共数据集、通过ChatGPT和GPT-4生成的指令微调数据和额外的空间负样本数据。其中大部分数据是由现有视觉（语言）任务转换而来，例如对象检测和短语定位。</p><p>&nbsp;</p><p>此外，团队表示，通过ChatGPT/GPT-4收集的34000条引用和定位指令调整对话，可以高效完成模型的指令遵循与开放词汇引用/定位训练。团队还进行了空间感知的负样本挖掘，进一步提高了模型的健壮性。</p><p><img src=\"https://static001.geekbang.org/infoq/95/951efdef75acda77224ea03d4956d124.png\" /></p><p></p><p></p><h4>幻觉问题</h4><p></p><p>&nbsp;</p><p>团队也观察到了多模态大模型在回答是/否类问题时，往往表现出产生“幻觉”。对此，团队通过图像条件类别定位以及语义条件类别定位两种方式进行负样本挖掘。</p><p>&nbsp;</p><p>这两种方式都要求模型定位特定的对象类别，从而使模型能够辨别并潜在发现某些对象的缺失。不同之处在于，如何选择负样本类别。对于前者，团队采用Object365数据从给定图像中未显示的词汇中随机选择对象类，对后者则使用Flickr30k数据，并通过ChatGPT/GPT-4查找与原始类别、属性或数量最相似的实体以获取负样本，例如“男人”和“女人”、“蓝色”和“黄色”。</p><p>&nbsp;</p><p>此外，团队还进行了数据整理，以维持两种类别下正样本和负样本之间的平衡，最终共收集到95000条数据。</p><p></p><h4>大模型响应</h4><p></p><p>&nbsp;</p><p>除了通过模板转换现有数据集之外，对话指令调整数据同样在帮助多模态大模型理解人类意图，并生成流畅、自然、长格式响应方面至关重要。目前，业界广泛使用少样本提示以获取视觉指令调整数据，其中将图像的文本场景描述与人工标注对话作为少样本演示，并通过提示词要求ChatGPT/GPT-4根据新图像的文本场景生成相应的对话描述。</p><p>&nbsp;</p><p>但是，以往的指令调整数据主要集中于描述整体图像，而不会明确指定空间相关信息。为了收集引用与定位指令调整数据，团队通过以下三个步骤强调基于区域的空间知识：</p><p>&nbsp;</p><p>除了像以往那样使用对象与全局标题之外，其符号场景描述还包含对象与区域标题间的物理关系以及相应坐标。在人工标注的对话中，团队在输入/输出/二者兼具的可定位区域或对象之后添加坐标，且对话通常集中于特定区域，有助于隐式提示ChatGPT/GPT-4在生成新对话时遵循类似的模式。实际生成的对话有时无法遵循在系统提示和少样本示例中编写的规则和模式，这可能是由于大语言模型输入中的上下文太长，导致无法处理所有细节。为此，团队建议重复使用ChatGPT/GPT-4来简化最初生成的对话，其平均上下文长度仅为首轮生成数据的10%。另外，为了节约成本，团队仅在首轮生成中使用ChatGPT，而后使用GPT-4进行简写提炼，最终共收集到34000条对话。</p><p>&nbsp;</p><p></p><h4>训练过程</h4><p></p><p>&nbsp;</p><p>对于训练过程，团队使用CLIP-ViT-L/14@336p对图像编码器进行初始化，使用Vicuna对大模型进行初始化，使用LlaVA的第一阶段权重对投射层进行初始化，借此实现了视觉采样器的随机初始化。初始化完成后，Ferret在GRIT数据上接受了三个轮次（epoch）的训练，使用Loshchilov &amp; Hutter 进行优化，学习率为2e − 5，批量大小为 128。</p><p>&nbsp;</p><p>根据介绍，Ferret-13B/7B模型在8张A100上的训练分别需要约5/2.5天。在训练过程中，当输入引用区域时，团队会随机选择中心点或边界框（在可行时也会选择分割掩膜）来表示各区域，并对训练数据进行了重复数据删除，借此清理下游评估中的样本。</p><p>&nbsp;</p><p>为了评估这项新功能，团队引入了Ferret-Bench，其涵盖三种新型任务：引用描述/引用推理和对话内定位。团队表示，通过对现有多模态大模型进行了基准测试，发现Ferret的平均性能较最出色的原有大模型高20.4%，而且在物体识别的幻觉方面也有所减轻。</p><p>&nbsp;</p><p>概括来讲，Ferret项目论文的贡献主要为以下三个方面：</p><p>&nbsp;</p><p>提出了Ferret模型，其采用基于新型空间感知视觉采样器的混合区域表示方法，可在多模态大模型中实现细粒度和开放词汇的引用和定位功能。建立起GRIT，一套大规模定位与引用指令调整数据集，既可用于模型训练，还包含额外的空间负样本以增强模型健壮性。引入了Ferret-Bench来评估涉及引用/定位、语义、知识和推理的联合任务。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>很明显，苹果正在努力追赶这次AIGC浪潮。据报道，苹果每天在人工智能上投资数百万美元，内部有多个团队开发多种人工智能模型。</p><p>&nbsp;</p><p>根据报道，苹果致力于对话式人工智能的部门被称为“Foundational Models”，“大约 16 名”成员，其中包括几名前谷歌工程师。该部门由 Apple 人工智能主管 John Giannandrea 掌舵，他于 2018 年受聘帮助改进 Siri。</p><p>&nbsp;</p><p>苹果正在开发自己的大模型“Ajax”。Ajax 旨在与 OpenAI 的 GPT-3 和 GPT-4 等产品相媲美，可运行 2000 亿个参数。Ajax 在内部被称为“Apple GPT”，旨在统一整个 Apple 的机器学习开发，提出了将人工智能更深入地集成到 Apple 生态系统中的更广泛战略。</p><p>&nbsp;</p><p>截至最新报告，Ajax 被认为比上一代 ChatGPT 3.5 更强大。然而，也有人认为，截至 2023 年 9 月，OpenAI 的新模型可能已经超越了 Ajax 的能力​。</p><p>&nbsp;</p><p>近日，苹果的机器学习研究团队还悄悄发布了一个名为 MLX 的框架来构建基础模型。彭博社报道称，苹果正在开发 Siri 的改进版本，并计划在下一个重大 iOS 版本中提供以人工智能为中心的功能。</p><p>&nbsp;</p><p>另外，苹果还正在与一些大型新闻出版商洽谈授权其新闻档案，并利用这些信息来训练模型。《纽约时报》称，该公司正在讨论“价值至少 5000 万美元的多年期交易”&nbsp;，并已与 Condé Nast、NBC News 和 IAC 等出版商保持联系。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href=\"https://arxiv.org/pdf/2310.07704.pdf\">https://arxiv.org/pdf/2310.07704.pdf</a>\"</p><p><a href=\"https://www.macrumors.com/2023/12/21/apple-ai-researchers-run-llms-iphones/\">https://www.macrumors.com/2023/12/21/apple-ai-researchers-run-llms-iphones/</a>\"</p><p><a href=\"https://www.theverge.com/2023/12/22/24012730/apple-ai-models-news-publishers\">https://www.theverge.com/2023/12/22/24012730/apple-ai-models-news-publishers</a>\"</p>",
    "publish_time": "2023-12-25 14:44:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《2023 中国信通院IOMM企业数字化转型发展双象限洞察》发布，转型者象限&赋能者象限各有40+企业上榜",
    "url": "https://www.infoq.cn/article/4a450e1274826b39e1f6214f8",
    "summary": "<p>2023年12月21日下午，由中国信息通信研究院（以下简称“中国信通院”）主办的“2024中国信通院ICT深度观察报告会|政企数字化转型分论坛”在北京召开。中国信通院云计算与大数据研究所副所长栗蔚发布并解读《2023 中国信通院IOMM企业数字化转型发展双象限洞察》。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f0d98e3c67d51e42ade51e0f0bad82e.jpeg\" /></p><p>中国信通院云计算与大数据研究所副所长栗蔚解读《2023 中国信通院IOMM企业数字化转型发展双向限洞察》</p><p>&nbsp;</p><p>栗蔚在回顾研究背景及研究发展历程时，指出数字经济对我国经济发展具有重要战略意义，国家高度重视数字经济与实体经济的融合，做出了多项重大部署。数字经济占GDP比重逐年递增，规模逐年扩大，是经济的“稳定器”和“加速器”。</p><p></p><p>栗蔚表示，数实融合涉及经济融合、产业融合和业技融合三个层面，中国信通院聚焦数实融合，深入产业，结合理论研究与产业相关实践，形成了IOMM 数字化相关标准体系，可以有效指导数字化转型实践，目前已迭代至2.0版本。同时，中国信通院为深入洞察行业发展规律，从宏观到微观不断深化研究成果，2021年发布&nbsp;“企业数字化转型发展双曲线”，揭示不同行业数字化转型水平；2022年针对转型者与赋能者，又进一步提出“企业数字化转型发展双象限”，从微观分析行业代表性企业的转型阶段；2023年继续迭代更新“企业数字化转型发展双象限”，并派生“企业数字化转型IOMM综合评价指数”和“行业数字化转型IOMM综合评价指数”，用以评价重点行业及其代表性企业的数字化发展水平及转型成效。</p><p></p><p>栗蔚在解读2023企业数字化转型发展双象限时，指出转型者象限在2023年共有43&nbsp;家企业上榜，企业数量相较于上一次发布增加了105%，新上榜企业涌现出广汽集团、中粮集团、国能集团等大型集团，企业的转型能力与价值发展更加均衡，“偏科”现象减少，转型总体规律为“以云切入，以平台增效，以流程贯通，以应用现代化迭代发展”；赋能者象限在2023年共有45家企业上榜，净增14家，新上榜企业29家，创新者与专注者企业增长较多，行业赋能水平增强，在四个各象限中，除创新者象限外，其余象限呈强者愈强的马太效应态势。</p><p></p><p>在分享下一步研究方向时，栗蔚表示，为更加深入研究行业，未来将会在现有基础之上增加更细化的研究方向，转型者象限会进一步拓展出IT数字化象限，业务数字化象限、数字原生象限3个大类；同时，赋能者象限也将拓展出数字政府、平台服务、基础设施、业务赋能、整体赋能等5类。</p><p></p><p>未来，希望能与业界同仁一起，针对数字化转型的不同领域做更深入的探讨和交流，一起来更好的推动数字中国建设，加速数字经济与实体经济融合发展。</p><p></p><h4>更多精彩内容，请查看PPT</h4><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ce9311806efad3abe26148071f81fcca.png\" /></p><p>1</p><p><img src=\"https://static001.geekbang.org/infoq/85/85cb861a9676b28213135eb1d7a580a4.png\" /></p><p>2</p><p><img src=\"https://static001.geekbang.org/infoq/db/dbaec974feebfe08fb51f84b742bdbfb.png\" /></p><p>3</p><p><img src=\"https://static001.geekbang.org/infoq/61/61a1a301f82a929f6a3baada03154823.png\" /></p><p>4</p><p><img src=\"https://static001.geekbang.org/infoq/ab/abc18621e6d336b1f1c6adb2e035f3ad.png\" /></p><p>5</p><p><img src=\"https://static001.geekbang.org/infoq/ba/ba6269751557001cbd9f883ffc976bed.png\" /></p><p>6</p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d6e04dcf4348d310d5e5eb3866ac511.png\" /></p><p>7</p><p><img src=\"https://static001.geekbang.org/infoq/30/302546adcb50d53efb03caff5dd1e1b0.png\" /></p><p>8</p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d8257f95be0160912c0328b81fe1204.png\" /></p><p>9</p><p><img src=\"https://static001.geekbang.org/infoq/71/7179c8e967db5fc5f26cf07c994a96ae.png\" /></p><p>10</p><p><img src=\"https://static001.geekbang.org/infoq/10/101d079d93bdf964608ac109c43fb198.png\" /></p><p>11</p><p><img src=\"https://static001.geekbang.org/infoq/47/4718f602a27a8db89e4b03649c30c752.png\" /></p><p>12</p><p><img src=\"https://static001.geekbang.org/infoq/9e/9e6b24509a64fbc5cafdabed8a4bb8f3.png\" /></p><p>13</p><p><img src=\"https://static001.geekbang.org/infoq/58/580bc86b4c56b85eb5a823dd6e7701ca.png\" /></p><p>14</p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d9945cfaf3d343db1da44d79e205ab8.png\" /></p><p>15</p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb58baac585eedabc0ffcc22c9493c2a.png\" /></p><p>16</p><p><img src=\"https://static001.geekbang.org/infoq/b5/b523aabd787c7a13238182a3164cad19.png\" /></p><p>17</p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a49f5acac68ac33f2ff15119b8deca3.png\" /></p><p></p>",
    "publish_time": "2023-12-25 11:06:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "好分期Mock服务在测试中的应用实践",
    "url": "https://www.infoq.cn/article/IGPWQmvNOiyFOzBZadWF",
    "summary": "<p>Mock技术是一种软件开发与测试中用于模拟（Mock）测试数据和对象的技术。主要用于消除测试中的依赖关系，提高测试的独立性和可重复性，帮助测试人员进行有效地测试。Mock技术也可用于前后端和微服务架构下服务间的分离开发。 Mock技术的主要思想是使用模拟对象或数据，替换测试环境中的真实对象或数据，以达到如同真实环境一般的测试效果。通过Mock技术可以提供独立性的测试数据或者环境，为软件开发过程中的单元测试、集成测试、系统测试和性能测试各种阶段提供必要的测试支持。</p><p></p><p>在互联网金融业务领域，我们面临的挑战包括：长链路业务流程、众多外部服务依赖以及复杂的业务场景。在软件生命周期过程中，多个团队同时参与开发和测试会出现以下情况非常依赖Mock服务。</p><p></p><p>依赖服务不稳定或不可用：测试过程中，依赖的外部服务可能不稳定或不可用。如果这些依赖的服务不稳定，则无法进行开发和测试。Mock服务模拟这些服务的行为响应，确保了测试的连续性和有效性，不再会受外部服务的限制。并行测试的需要：不同团队可能正在测试彼此之间存在依赖关系的不同的模块或应用程序。使用Mock服务可以模拟其他团队尚未开发完成的模块或应用程序的行为响应，以便其他团队可以继续开发和测试。效率和成本问题：系统集成测试会涉及到多个环境和资源。如果每次都要在实际环境中进行测试和集成，会消耗大量的时间和成本。使用Mock服务可以在开发和测试过程中快速模拟，从而提高效率和降低成本。安全和合规性问题：金融行业的软件开发需要遵守严格的安全和合规性要求。在开发和测试过程中，可能需要访问敏感数据或进行一些敏感操作。使用Mock服务可以避免对真实数据和系统的访问，从而降低潜在的安全和合规性风险。待线上灰度时再进行敏感数据相关的数据验证。</p><p></p><h2>当前Mock方法存在的问题</h2><p></p><p></p><p>随着业务和技术不断发展，以及测试效率提升要求，原有Mock技术方法在复杂技术架构和业务场景的后端服务中应用存在如下问题：</p><p></p><h4>1、Mock 服务与真实服务不能同时工作</h4><p></p><p></p><p>服务调用方每次只能请求一个服务地址，Mock服务地址或真实三方服务地址只能选择其一，无法针对于不同业务场景访问Mock服务和真实服务，Mock服务和真实服务无法同时工作。</p><p></p><p>同时工作允许部分业务场景与真实三方服务交互，其他场景使用Mock测试验证。其优势如下：</p><p></p><p>1） 降低测试成本</p><p></p><p>将小金额划扣还款与真实支付渠道服务对接联调保证了功能验证准确性。将大金额划扣还款走Mock服务降低测试成本。</p><p><img src=\"https://static001.geekbang.org/infoq/a3/a33019cf5da49e177b61b1c3d1751b51.png\" /></p><p></p><p>2） 提升测试效率</p><p></p><p>将已验证通过的功能切换到Mock服务上进行测试，降低三方服务不稳定对我方的测试影响。同时新增独立功能点继续走真实联调测试。</p><p></p><h4>2、Mock服务与真实服务切换效率低</h4><p></p><p></p><p>全量切换： 全部流量在访问Mock服务和真实服务之间互相切换效率低。</p><p></p><p>传统方法需要修改调用方请求地址，更新配置并重启服务加载生效，无法一键迅速切换。</p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac28c66f8f08c37203fcb8b30497d78d.png\" /></p><p></p><p>部分流量切换：针对于部分流量在访问Mock服务和真实服务之间切换效率低。</p><p></p><p>支持动态Mock响应且支持反向代理的Mock技术服务才能实现部分流量切换。 传统的动态切换方法需要编写切换逻辑脚本。脚本因包含Mock的所有动态响应逻辑，频繁修改可能会引入额外风险导致原有Mock响应异常。脚本修改进行流量切换是重复性的工作，每次切换均需要熟悉脚本逻辑和修改。</p><p><img src=\"https://static001.geekbang.org/infoq/88/88e801681d40bb5f87a2a2b57396fe34.png\" /></p><p></p><h4>3、多测试环境下不同环境期望使用不同的Mock规则</h4><p></p><p></p><p>测试和开发环境是多环境的，当不同的环境或者不同用户数据访问同一个Mock服务同一个API接口时，如果期望返回不同的Mock响应，因为不支持虚拟Mock服务只能通过调用方请求增加环境标识和动态Mock响应实现，实现效率低下且支持度不完善。</p><p><img src=\"https://static001.geekbang.org/infoq/c0/c049132043b8ab2c9b3477bf32215b18.png\" /></p><p></p><p>综上所述，传统的Mock方式很难实现Mock服务与真实服务同时工作的模式，无法支持虚拟Mock服务（用来区分隔离环境或者业务场景），需要将能实现的逻辑全部放入动态响应脚本，导致脚本内容臃肿复杂且无法复用，每次Mock响应逻辑修改均需要重构脚本。</p><p></p><h2>Mock的切入模式</h2><p></p><p></p><p>在介绍好分期Mock服务如何解决上述问题之前，先简单说明下好分期中后台技术架构。随着业务和技术不断发展，因包含了历史留存业务系统目前主要有如下两种技术架构，针对不同架构采用不同的方式来切入Mock服务。</p><p></p><p>服务、API网关架构</p><p></p><p>服务的发现、调度由发布平台和网关实现，服务之间调用通过API网关。</p><p><img src=\"https://static001.geekbang.org/infoq/ab/abaa39053171c0555e893f6afa529be0.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/41/41b4328cdd86eb0e51fe4f2111057578.png\" /></p><p>Mock服务切入后，将作为前置网关服务根据路由规则，返回动态Mock响应内容或者转发到其他服务。</p><p></p><p>切入方法：</p><p></p><p>DNS劫持，将访问服务的域名均解析到Mock服务网关修改各服务配置，将原访问API网关服务地址改为Mock服务网关的服务地址器地址</p><p></p><p>微服务架构</p><p></p><p>微服务架构由微服务治理框架组件支撑，服务间调用节点信息来自注册中心。</p><p><img src=\"https://static001.geekbang.org/infoq/d6/d6338f4c7827ca217d2351ae9096f3f9.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/07/07154a8ac422bcb32cd2bc00268bfcaa.png\" /></p><p></p><p>Mock服务切入后，微服务通过注册中心获取到的均为Mock服务地址，访问Mock服务地址后，由Mock服务将请求根据环境、API接口、数据内容等路由到对应的其他微服务或者返回动态Mock响应。</p><p></p><p>切入方法：拦截或者转移微服务的注册，不再注册到所属区域的注册中心。Mock服务向注册中心注册服务地址。微服务从注册中心拉取待调用的服务节点时，拉取到Mock服务地址，并请求到Mock服务网关，Mock服务根据路由规则请求到对应的微服务或者返回动态Mock响应。</p><p></p><h2>好分期Mock服务整体实践方案</h2><p></p><p></p><p>Mock服务作为统一的访问入口，所有API请求均被引导到Mock服务。Mock服务根据请求的特定信息（如访问域名、API路径、参数、请求头等）进行路由，将请求转发到相应的后端服务或返回动态Mock响应。“Mock路由规则”来定义请求路由行为，包含3种工作模式：Internal Forward(内部路由转发）、Upstream（上游转发）、Mock（Mock响应）。每一个“Mock路由规则”下可以包含任意多个“条件规则路由”，如果命中“条件规则路由”则执行对应的路由动作。如果未命中“条件规则路由”则使用其所属的“Mock路由规则”作为兜底的路由动作。通过路由规则的创建和修改即可以实现“Mock服务和真实服务同时工作（根据命中不同路由条件转发Upstream和返回Mock响应）”、“流量切换迅速切换（修改路由规则的工作模式）”、“环境隔离区分（虚拟Mock服务地址来区分）”。</p><p><img src=\"https://static001.geekbang.org/infoq/cd/cd0649f67e7beddfbb16f2afb8b5aa77.png\" /></p><p></p><p>Mock服务网关特征</p><p></p><p>Mock服务作为统一的服务访问入口，使用动态配置生效的“Mock路由规则”对访问请求进行路由，转发到真实服务或直接返回Mock响应。</p><p><img src=\"https://static001.geekbang.org/infoq/9c/9ce8b456077b6f7e0af3e744d58dd7d3.png\" /></p><p></p><p>虚拟Mock服务</p><p></p><p>创建虚拟Mock服务地址（Virtual Host）后可以通过虚拟服务地址域名访问Mock服务，便于针对各环境的Mock响应结果独立管理。</p><p></p><p></p><blockquote>例如：环境A调用 testa.mock.com环境B调用 testb.mock.com</blockquote><p></p><p></p><p>如果希望testa.mock.com和testb.mock.com返回同样Mock响应，则可以将如上两个服务地址对应的Mock路由规则经“内部路由转发（Internal Forward)”到default.mock.com虚拟Mock服务。仅针对default.mock.com虚拟Mock服务配置Mock响应。</p><p><img src=\"https://static001.geekbang.org/infoq/56/562bf71c01098d4bbdc2b0b2d3f3d848.png\" /></p><p></p><p>如果希望testa.mock.com和testb.mock.com分别返回不同的Mock响应，则分别定义各自路由规则和对应的Mock响应即可。</p><p><img src=\"https://static001.geekbang.org/infoq/3b/3bf645031288ce011d3b33803211effd.png\" /></p><p></p><p>如果希望请求testa.mock.com转发（Upstream）到真实的a.real.com，而请求testb.mock.com时返回Mock响应，那么将testa.mock.com的虚拟服务更改“Mock路由规则”转发到a.real.com即可。</p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9e86da8cb21406e388afd6fb730d7c4.png\" /></p><p></p><p>Mock路由规则</p><p></p><p>“Mock路由规则”包含3种工作模式：Internal Forward(内部路由转发）、Upstream（上游转发）、Mock（Mock响应）。“Mock路由规则”可以把期望的Mock结果模块化，从而可以快速关闭、启用、复用。满足复杂环境及业务场景下动态Mock响应、Mock服务与真实服务的流量的切换。</p><p></p><p>1）修改路由规则工作模式在Upstream（上游转发）、Mock（Mock响应）之间切换，可快速进行Mock服务和真实环境迅速切换；</p><p>2）Internal Forward(内部路由转发）工作模式可以在Mock服务中进行内部实现路由规则复用。如“图11-虚拟Mock服务场景1”，可将testa.mock.com和testb.mock.com 的“Mock路由规则”全部“Internal Forward(内部路由转发）”到default.mock.com的“Mock路由规则”。因此我们只需要定义default.mock.com的Mock路由规则和响应内容。不需要在testa和testb两个虚拟Mock服务上重复配置两次。</p><p>3) 根据环境信息、请求数据信息路由到不同的服务或者返回动态Mock响应。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/764e48fe786124d4d1d18f9213bd7c0b.png\" /></p><p></p><p>使用场景一：环境维度——同一接口可以依据来源环境不同得到专属的响应信息。</p><p></p><p>实例：测试团队同时并行着3套测试环境，同一时刻都用到某资方借款试算接口，各环境场景如下：</p><p></p><p>环境A：走资方真实环境，借款金额为1000，期次6期；环境B：走Mock测试，借款金额为2000，期次9期；环境C：走Mock测试，借款金额为3000，期次12期。</p><p></p><p>则“Mock路由规则”处理如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec7c37838bbb026f7a90203ccfbef12e.png\" /></p><p></p><p>使用场景二：个人维度——同一套测试环境同一接口请求可以根据使用者不同给予不同的应答。</p><p></p><p>示例：</p><p></p><p>测试人员A/B/C三人在同一套环境调用机构的还款试算接口，各场景如下：</p><p></p><p>测试人员A：借款共6期，当前期为第2期，从第2期发起的提前结清试算；测试人员B：借款共3期，第2期已逾期，当前期为第3期，第2期发起正常还款试算；测试人员C：借款共9期，第1期为当前期，从第1期发起提前结清试算。</p><p></p><p>则“Mock路由规则”处理如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/695055f90928e1f4fecf8bd07d6f346b.png\" /></p><p></p><p>Mock动态响应</p><p></p><p>除了常用的内建变量、内建方法之外，好分期Mock服务还支持通过JSR223标准Groovy脚本在线编译和执行来实现访问外部数据库、外部服务，模拟延时。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b6748ec568a50fbe9d5abe2a65ad4476.png\" /></p><p></p><p>Mock日志</p><p></p><p>将Mock响应和真实服务的响应日志全部记录下来，将Mock路由流转路径留存下来，方便针对测试数据进行分析。使用真实服务的响应日志也可以快速完成Mock响应报文的制作生成。</p><p></p><h2>总结</h2><p></p><p></p><p>好分期Mock服务方案应用后，已接入7套测试环境服务超50个系统。在集成测试、系统测试、自动化测试过程中极大地提升了开发联调、测试效率，证明了这一方案的有效性。同时也帮助开发工程师更加全面地测试系统性能。</p><p></p><p>模拟高负载场景：在性能测试中，使用Mock服务模拟底层服务高负载场景，测试中间件的性能。通过模拟这样的场景，可以测试中间系统的抗压能力和性能表现，从而获得有关系统性能的重要数据。模拟不同的网络环境：不同的网络环境对系统性能的影响是很大的，例如网络延迟、带宽限制等都会对系统性能造成影响。在性能测试中，使用Mock服务模拟不同的网络环境，以测量系统在不同网络条件下的性能表现。</p><p></p><p>未来，会进一步提升Mock服务自身性能，支持更多服务和团队。还会继续探索将更加智能化、自动化的Mock技术，为开发和测试提供更为准确和实用的模拟数据。</p><p></p><h4>作者介绍</h4><p></p><p></p><p>李豪，微财数科 测试高级工程师</p><p>畅悦秀，微财数科 测试高级经理</p><p>李军，微财数科 技术负责人</p><p>吴迪，微财数科 副总裁</p>",
    "publish_time": "2023-12-25 15:35:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "小米首辆车投入3400名工程师，研发费用超100亿；百度Apollo 9.0发布；蔚来发布最新旗舰ET9 | 汽车技术资讯",
    "url": "https://www.infoq.cn/article/f6uD15xh1Wha8Qu1VEl9",
    "summary": "<p></p><h2>蔚来发布最新旗舰 ET9</h2><p></p><p></p><p>12 月 23 日，蔚来举行 NIO Day 2023，并发布了旗舰车型 ET9。ET9 被认为是蔚来自研技术的“集大成者”，和现有蔚来车型相比，蔚来 ET9 自研程度更高，使用了自研圆柱电池、自研智能驾驶芯片、自研整车全域操作系统、自研全域 900V 高压架构等多项自研技术。</p><p></p><p>据蔚来董事长、CEO 李斌介绍，ET9 包含 17 项全球首发技术、52 项同级领先技术，已申请专利达到 525 项。</p><p></p><p>活动上，蔚来智能底盘系统 SkyRide・天行也正式发布，蔚来 ET9 车型首发搭载。该底盘拥有线控转向、后轮转向、全主动悬架等配置。</p><p></p><p>尽管具备多项先进技术，但该车计划于 2025 年第一季度才开始交付。</p><p></p><h2>蚂蚁蚁盾发布网约车安全管理方案，将率先落地河南</h2><p></p><p></p><p>12 月 21 日，蚂蚁蚁盾发布网约车专属“驾驶风控引擎”，通过集成车联网安全系统、司机安全驾驶模型和司机信用模型三大产品，提升网约车运营效率，消除安全管理盲区。</p><p></p><p>蚂蚁集团表示，这个方案将首批落地河南自主品牌智行盒子的定制网约车“海马 EX00”，预计将大幅降低风险事件及相应车辆保费。</p><p></p><p>在车载终端，蚁盾基于 IIFAA（互联网可信认证联盟）的安全规范研发安全芯片植入数字车钥匙，防范恶意盗车风险；针对驾驶行为，蚁盾与智行盒子基于车身感应、人脸识别等技术共建司机安全驾驶模型，通过识别司机驾驶状态，对疲劳驾驶、接打电话等危险驾驶行为及时干预，确保司乘安全。</p><p></p><h2>比亚迪卷入 AEB 竞赛：测试视频曝光，实现 130km/h 制动并刹停</h2><p></p><p></p><p>12 月 20 日下午，在车企开启 AEB 竞赛之际，比亚迪 AEB 系统测试视频被曝光。搭载比亚迪 AEB 系统的仰望 U8，在日间 130km/h 对静止前车、日间 110km/h 对消失的前车、夜间 110km/h 对静止前车等三个场景下，实现了制动并刹停。</p><p></p><p>日前，车企之间关于 AEB 的争议登上微博热搜，引发用户和行业广泛关注。AEB 自动紧急制动系统是一项主动安全辅助功能，包含碰撞预警和自动紧急制动两个功能。系统通过传感器摄像头及多种雷达自动探测前方目标车辆、行人或障碍物，当本车辆与前方车辆、行人或障碍物有碰撞风险时，触发碰撞预警，采取报警方式提醒驾驶员。当驾驶员制动过晚、制动力过小或者完全无制动措施时，触发自动紧急制动功能，辅助驾驶员避免或减轻碰撞。</p><p></p><p>此次比亚迪 AEB 系统实现 130km/h 制动并刹停，也刷新了行业纪录。知情人士表示，这是由于测试车仰望 U8 搭载的易四方、云辇等技术，实现了动力与底盘的毫秒级协同控制，提升了 AEB 系统触发后的驾乘舒适性。</p><p></p><h2>百度 Apollo 自动驾驶开放平台 9.0 发布：重构 12 万行代码，首次适配 ARM 架构</h2><p></p><p></p><p>12 月 19 日，百度正式推出了 Apollo 开放平台的全新升级版本 —— Apollo 开放平台 9.0。在Apollo开放平台8.0至9.0开发过程中，重构了12万行代码，新增20万行代码。</p><p></p><p>百度自动驾驶平台生态部总经理张亮总结了这次升级的主要亮点：“Apollo 开放平台 9.0 在工程、算法和工具等方面实现了全面升级，通用层可赋能多种应用场景的规模化落地，整体操作更加灵活易上手，使用场景通用易拓展。极大提升开发效率的同时，可帮助更多开发者快速搭建属于自己的自动驾驶系统。”</p><p></p><p>工程框架方面，为了使开发者更加灵活地组装自动驾驶应用和更便利地二次开发，Apollo 开放平台 9.0 对包管理进行全面升级，将模块按照功能的颗粒度拆分成更小的软件包，开发者可以更加方便地根据自己的需求选择使用，同时还提供丰富的功能组件及插件，并对功能扩展进行了提升和优化。</p><p></p><p>基于此，统一调度接口后，开发者最快 1 天内即可完成场景 Demo 搭建，调参方式简化使得调参效率提升 1 倍，新增插件机制让代码学习成本降低 90% 的同时代码量降低 50%，大大提高了 Apollo 的二次开发能力。Apollo 开放平台 9.0 还首次适配了 ARM 架构。</p><p></p><p>Apollo 开放平台 9.0 还提供了增量训练，支持独立自主完成模型训练，可在维持模型原有检测能力的前提下，提升特殊目标和特殊场景的检测能力，从而达到用较低成本轻松提升定制场景的检测效果。另外，新平台全面支持 4D 毫米波雷达，使障碍物检测和极端天气场景安全性都得到了增强。</p><p></p><h2>LG 开发出可嵌入汽车挡风玻璃的透明天线，支持 5G、Wi-Fi、导航</h2><p></p><p></p><p>12 月 18 日，LG 宣布开发了一种用于汽车挡风玻璃或天窗的透明天线，与法国玻璃制造商 Saint-Gobain Sekurit 合作推出，将在 CES 2024 上进行展示。</p><p></p><p>据介绍，这一“薄膜型天线”可以附着在玻璃上或内置在玻璃中，并支持 5G、Wi-Fi 和使用 GNSS（全球导航卫星系统）的导航。</p><p></p><p>LG 表示，透明天线可以无缝集成在汽车挡风玻璃或玻璃天窗中。这意味着汽车制造商在开发新车型时，不必为天线做出任何设计让步。该薄膜型天线拥有超过 80 项 LG 专利创新，包括使天线图案透明的设计能力和透明电极技术。</p><p></p><h2>雷军：小米首辆车投入 3400 名工程师，研发费用超 100 亿</h2><p></p><p></p><p>近日小米集团董事长雷军在央视《面对面》节目中提及了小米在汽车方面的努力。</p><p></p><p>雷军表示，造车是不得不干的事，也是他经过深思熟虑后才决定的事情。未来不做车小米注定落伍，电动车是技术未来发展方向，小米的第一部车要遵循守正出奇的原则。</p><p></p><p>在雷军看来，汽车和手机产业“是一个大融合，进入汽车行业对小米来说有挑战，但总体来说难度可控。”充分尊重汽车行业规律，使用汽车行业的成熟技术，确保能把第一辆车做好，小米要在这个大前提下进行创新。</p><p></p><p>小米的第二条策略是“十倍投入”，雷军表示：“比如一般车企造一辆车，大概投三四百人，10-20 亿的研发经费。而我们第一辆车投了 3400 名工程师，整个研发投入超过了 100 亿，我们是用了 10 倍以上的投入。有这样的把握以后，反正我是抱着志在必得的方式来做的。”</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png\" /></p><p></p><p></p><p></p>",
    "publish_time": "2023-12-25 15:52:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "NeurIPS 2023｜有效提高视频编辑一致性！美图&国科大联合提出基于文生图模型的新方法EI2",
    "url": "https://www.infoq.cn/article/SR6mzSFR3eGrNbUekSol",
    "summary": "<p>美图影像研究院（MT Lab）与中国科学院大学突破性地提出基于文生图模型的视频生成新方法EI2，用于提高视频编辑过程中的语义和内容两方面的一致性。该论文从理论角度分析和论证视频编辑过中出现的不一致的问题，主要由引入的时序信息学习模块使特征空间出现协变量偏移造成，并针对性地设计了新的网络模块进行解决以生成高质量的编辑结果。目前，该论文已被机器学习顶会之一NeurIPS 2023接收。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>背景</h2><p></p><p></p><p>作为当前炙手可热的前沿技术之一，生成式AI被广泛应用于各类视觉合成任务，尤其是在图像生成和编辑领域获得了令人赞叹的生成效果。对比静态图像，视频拥有更丰富的动态变化和语义信息，而现有的视觉生成任务主要基于变分自编码器（VAE）和生成对抗网络（GAN），但通常会受限于特定场景和数据，很难提供普适的解决方案。因此，近年来基于扩散模型（Diffusion Models）在分布式学习上表现出的卓越能力，扩散模型也开始被拓展到视频领域，并在视频生成与编辑领域展现出了巨大的潜力。</p><p>&nbsp;</p><p>在研究初期，基于扩散模型的视频生成和编辑任务利用文本-视频数据集直接训练文生视频模型以达到目标。然而，由于缺少高质量的视频数据，这类工作泛化能力通常较差，此外，它们也需要耗费大量的计算资源。为避免上述问题，近期工作更倾向于将基于大规模数据集上预训练的文生图模型拓展到视频领域。此类任务通过引入可学习的时序模块使文生图模型具备视频生成和编辑能力，从而减少了对视频数据的需求以及计算量，并提供了简单易用的方案。因此，这类任务在近期引起了广泛的关注。然而，以上基于文生图模型的视频生成方案也面临着两个关键问题：一是时序不一致问题，即生成视频帧间内容的不一致，例如闪烁和主体变化等；二是语义不一致问题，即生成视频未能按照给定文本进行修改。解决上述两个核心问题将极大地推动基于文本的视频编辑与生成技术在实际场景中的应用和落地。</p><p>&nbsp;</p><p>美图影像研究院（MT Lab）与中国科学院大学在NeurIPS 2023上共同提出一种基于文生图模型的视频编辑方法EI2,从理论上分析和论证了现有方案出现不一致的原因，并提出了有效的解决方案。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/78/78898ccd9aac5f2c40c6c7bee681399b.png\" /></p><p></p><p>l&nbsp; 论文链接：<a href=\"https://arxiv.org/abs/2208.02646\">https://arxiv.org/abs/2208.02646</a>\"</p><p>&nbsp;</p><p></p><h2>EI2：基于文生图模型的视频一致性编辑解决方案</h2><p></p><p>&nbsp;</p><p>EI2首先对语义不一致问题进行了分析，发现该问题不是由微调策略或过拟合现象出现所导致的，而是由新引入的时序模块造成的。这些模块虽然能提升文生图模型的时序连续性，但会减弱甚至消除其原有的生成和编辑能力。EI2方案将这一现象的出现归因于生成特征空间出现协变量偏移：由于时序模块只在目标视频上进行训练，其输出特征的分布与源模型的分布存在差异。此外，现有空间注意力机制为减小计算量，通常会忽略特定元素进行局部计算，从而导致次优解的出现。因此，高效地融合全局上的空间和时序注意力信息也是取得时序一致性编辑的关键。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/60/6050e46cde95351d012c8fe0da6c7cd3.png\" /></p><p></p><p>图1 本文提出的EI2方案与已有方案在视频编辑任务上的结果对比</p><p>&nbsp;</p><p>基于上述分析，EI2设计了更为合理的时序模块并将其与文生图模型相结合，用于增强生成能力，以更好地解决视频编辑任务。具体而言，EI2采用一次微调框架（One-shot Tuning），从理论和实践两方面对现有方法进行了改进。首先，EI2设计了偏移控制时序注意力模块，用于解决视频编辑过程中出现的语义不一致问题。EI2从理论上证明了在特定假设下，协变量偏移与微调无关，是由时序注意力机制新引入的参数造成，这为解决语义不一致问题提供了有价值的指导。</p><p>&nbsp;</p><p>通过上述论证，EI2定位层归一化（Layer Norm）模块是协变量偏移出现的重要原因。为了解决这一问题，EI2提出了简单有效的实例中心化模块以控制分布偏移。此外，EI2也对原时序注意力模块中的权值进行归一化，从而限制方差的偏移。其次，EI2设计了粗细力度帧间注意力模块来缓解视频编辑过程中出现的时序不一致问题。EI2创新性地提出了一种粗细力度交互机制，用于更为有效地建立时空注意力机制，从而使得低成本的视频全局信息交互成为可能。与现有丢弃空间信息的方案相比，EI2在空间维度上进行采样，这不仅保持了时空数据的整体结构，也减少了需要考虑的数据规模。具体而言，粗细力度帧间注意力模块对于当前帧保留细粒度信息，而对于其他帧则进行下采样以获得粗粒度信息来做交互。这种方式使得EI2在有效学习时序信息的同时，保证了与现有时空交互方案接近的计算量。基于以上设计，实验结果表明EI2可以有效地解决视频编辑过程中出现的语义不一致问题并保证时序上的一致性，取得了超越现有方案的视频编辑效果。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/af/aff00b1ceac7b2506e54b6d586a44436.png\" /></p><p></p><p>图2 EI2的训练和推理流程</p><p>&nbsp;</p><p></p><h2>实验结果</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c4b3be4750100ae27924174981e052f1.png\" /></p><p></p><p></p><p>表1 与基线方法的量化对比</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/49b534a1b617f47da6b32b4898ae0476.png\" /></p><p></p><p>图3 与基线方法的可视化对比</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/274d90253b4c4945ead3ca4431b54ecb.png\" /></p><p></p><p></p><p>图4 协变量偏移控制的消融实验</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e372e39f3ea75e04e6e46e9d1b8ea4aa.png\" /></p><p></p><p>图5 时空注意力机制的消融实验</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>该论文创新性地提出了基于文生图模型的视频编辑新方案EI2，有效地解决了现有方案遇到的语义和时序不一致问题。其中，EI2从理论上证明了语义不一致问题由引入的时序模块产生的协变量偏移造成，并设计了偏移控制时序注意力进行改进。另外，EI2提出了粗细力度帧间注意力模块，在提升视频编辑效果的同时也保证了较低的计算复杂度。与现有方案相比，EI2在量化和可视化的分析中都表现出了明显的优势。</p><p>&nbsp;</p><p></p><h2>研究团队</h2><p></p><p></p><p>本论文由美图影像研究院（MT Lab）和中国科学院大学的研究者们共同提出。美图影像研究院成立于2010年，致力于计算机视觉、深度学习、计算机图形学等人工智能（AI）相关领域的研发。曾先后参与CVPR、ICCV、ECCV等计算机视觉国际顶级会议，并斩获ISIC Challenge 2018皮肤癌病灶分割赛道冠军，ECCV 2018图像增强技术比赛冠军，CVPR-NTIRE2019图像增强比赛冠军，ICCV2019服饰关键点估计比赛冠军等十余项冠亚军，在AAAI、CVPR、ICCV、ECCV、NIPS等国际顶级会议及期刊上累计发表48篇学术论文。在美图影像研究院（MT Lab）的支持下，美图公司拥有丰富的AIGC场景落地经验。2010年开始人工智能领域的相关探索，2013年开始布局深度学习，2016年推出AIGC雏形产品“手绘自拍”，2022年AIGC产品全面进入爆发期，2023年6月发布自研AI视觉大模型MiracleVision(奇想智能)，2023年12月MiracleVision迭代至4.0 版本，主打AI设计与AI视频。</p>",
    "publish_time": "2023-12-25 20:46:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]