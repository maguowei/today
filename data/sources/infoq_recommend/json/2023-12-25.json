[
  {
    "title": "网游新规致腾讯网易市值半天蒸发5200亿；吴泳铭“爆改”淘天：管理层全换成有功绩的年轻人；字节年收入超腾讯、逼近 Meta｜Q资讯",
    "url": "https://www.infoq.cn/article/xxlQ1u4YBddYJzfURN8q",
    "summary": "<p>&nbsp;</p><p></p><blockquote>吴泳铭发全员信：淘天集团管理团队全部换血；抖音要收购饿了么？抖音相关负责人：没有这个计；字节年收入超过腾讯、逼近 Meta；雷军：小米的第一辆车投入了3400名工程师，研发超过100亿；Arm 裁员 70 多名中国工程师，重组中国软件业务；英伟达股价五年上涨约1200%，老员工开始躺平，黄仁勋对此感到不满；OpenAI：董事会将拥有是否发布新AI大模型的决定权；网游将不得设诱导性奖励，腾讯跌超7%；微博CEO王高飞怼董明珠：自己管理上不合规，吃亏了要么骂员工要么赖法律……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司&nbsp;</h2><p></p><p></p><h4>吴泳铭发全员信：淘天集团管理团队全部换血</h4><p></p><p>&nbsp;</p><p>12月20日，阿里巴巴宣布，阿里巴巴集团CEO、淘天集团董事长吴泳铭兼任淘天集团CEO。自此，吴泳铭将同时担任阿里巴巴集团和淘天集团、阿里云智能集团三项CEO职务。淘天集团原CEO戴珊将协助筹建阿里巴巴集团资产管理公司，“这是阿里变革之后新的业务职能”。</p><p>&nbsp;</p><p>阿里巴巴集团董事会主席蔡崇信在全员信中表示，由吴泳铭兼任阿里云和淘天的一号位，将有助于以技术创新引领淘天的变革，有助于确保集团对两大战略重心电商和云的统一指挥和高强度持续投入</p><p>&nbsp;</p><p>12月22日下午消息，吴泳铭宣布了淘天集团最新组织决定，年轻化管理团队全面接棒。6位年轻管理者被任命分别带领淘天集团各关键业务，直接向吴泳铭汇报。吴泳铭同时对淘天集团提出要求：正视现状，重新创业。</p><p>&nbsp;</p><p>具体看，“85后”吴嘉将负责淘天用户平台事业部与阿里妈妈事业部。据了解，吴嘉2010年校招加入阿里巴巴，在技术开发一线积累了丰富经验，培育孵化了广受年轻人欢迎的产品夸克。吴嘉还将继续兼任智能信息总裁。</p><p>&nbsp;</p><p>现任饿了么首席运营官谌伟业（处端）将调任淘宝，负责淘宝事业部、淘天商家平台部、淘天客户满意部。他主导提出饿了么“放心点准时达”的品牌价值方向，创设了现象级营销“猜答案免单”。他也是另一款年轻人喜爱的闲置交易和兴趣内容平台闲鱼的初创人。</p><p>&nbsp;</p><p>刘博（家洛）将接手天猫事业部，十几年来他一直在淘宝天猫业务一线，商业实战经验丰富，连续开创了多个战略赛道。生于87年的汪庭祥（少游）则将带领服饰发展部。原直营业务负责人刘一曼（一漫）将负责M2C事业部。程道放（道放）将带领淘宝直播及内容事业部，负责推进淘宝内容化建设与创新。</p><p>&nbsp;</p><p></p><h4>抖音要收购饿了么？抖音相关负责人：没有这个计划</h4><p></p><p>&nbsp;</p><p>近日，有市场消息称，吴泳铭已做出了一系列资本规划：盒马已经在考虑出售、饿了么或将有新的资本动作，优酷则正考虑并入阿里影业，但前提是能够稳定盈利。对此，阿里内部人士表示，消息不实，已经对相关报道进行投诉。</p><p>&nbsp;</p><p>12月19日，针对抖音与阿里谈判收购饿了么的传言，抖音相关负责人表示，抖音没有这个计划。据悉，该传闻称，抖音正在和阿里谈收购饿了么，目前已经到了谈价格阶段，如果谈判顺利，预计春季后就能落地。</p><p>&nbsp;</p><p>针对传闻，饿了么、优酷、盒马接连辟谣：没动作、没合并、没出售。饿了么回应：将有资本动作为不实消息；阿里大文娱回应：优酷并入阿里影业？假的；盒马回应：出售盒马为不实传闻。</p><p>&nbsp;</p><p></p><h4>字节年收入超过腾讯、逼近 Meta</h4><p></p><p>&nbsp;</p><p>据媒体报道，字节跳动 2023 年收入将达到 1100 亿美元，同比增长约三成。这个体量足以超过腾讯，逼近 Meta——按照各自财务指引，腾讯和 Meta 今年的收入分别将达到约 870 亿和 1330 亿美元；不过仍然只有全球最大的数字广告主 Google 的约三分之一。</p><p>&nbsp;</p><p>字节跳动的收入依旧主要来自国内广告和电商，但是 TikTok 高增长带来的拉动也不容忽视。按照此前媒体披露的数据，今年二季度 TikTok 对整个字节的营收贡献已经接近 20%。相比之下，Meta 和 Google 都做着更纯粹的数字广告生意（尤其是美国市场）。二者广告营收占比分别为约 95% 和 80%，受到整个宏观经济和企业营销支出缩减的更大打击。此外，营销人员也更希望通过既贴近消费者，转化率又高的渠道投放广告。</p><p>&nbsp;</p><p>据研究公司 Insider Intelligence 估计，去年 Google 和 Meta 在美国数字广告市场的市占率为 48.4%，为自 2014 年以来首次降到五成以下。它们预计这个数字今年会进一步降至 44.9%，因为更多份额会流向亚马逊、TikTok 以及其他流媒体平台。</p><p>&nbsp;</p><p></p><h4>雷军：小米的第一辆车投入了3400名工程师，研发超过100亿</h4><p></p><p>&nbsp;</p><p>小米集团创始人雷军透露，汽车很复杂，对于小米汽车的预期，特别担心大家都不买，但更担心的是大家都来买，“这一等要等一两年，肯定会被骂惨了，其实是各种很焦虑的情绪。”</p><p>&nbsp;</p><p>雷军表示，小米造车用了十倍以上的投入，有了这样的把握之后，他抱着志在必得的方式来做汽车。通常车企做一辆车大概投三四百人，研发经费在10亿-20亿。小米的第一辆车，则投入了3400名工程师，研发投入超过100亿。</p><p>&nbsp;</p><p></p><h4>Arm裁员 70 多名中国工程师，重组中国软件业务</h4><p></p><p>&nbsp;</p><p>软银集团旗下英国芯片设计公司 Arm 最近在中国裁减了超过 70 名软件工程师。不过，Arm 将其中的一些职位迁移到了中国以外的地方。在被裁掉的员工中，大约有 15 人将被安排从事与中国相关项目的不同岗位上。这些被裁撤的岗位由合同制软件工程师填补，他们曾参与过横跨 Arm 全球业务的项目。</p><p>&nbsp;</p><p>半导体行业因电子产品需求不振而低迷，Arm 此次裁员仿效了高通等主要芯片公司，后者在今年早些时候削减了全球员工数量。今年 11 月，由于智能手机销量下滑，Arm 发布了令人失望的营收预期。</p><p>&nbsp;</p><p></p><h4>英特尔启动今年第五轮裁员</h4><p></p><p>&nbsp;</p><p>12月20日消息，英特尔启动了今年第五轮裁员。监管文件显示，英特尔计划在福尔瑟姆（萨克拉门托县）的研发工厂裁员235名员工，于12月31日开始，持续两周时间。据悉，在前几轮裁员中，英特尔在福尔瑟姆园区裁掉了549个职位，占员工总数的10%左右。</p><p>&nbsp;</p><p>公司发言人阿迪·伯尔(Addy Burr)在一份声明中表示，“英特尔正在努力加快其战略，同时通过多项举措降低成本，包括在整个公司范围内减少一些特定业务和职能的工作场所。” 伯尔补充说，新的一年可能还会进一步削减。2022年，英特尔宣布在2025年前通过裁员、减少工作时间和可能出售部门的方式达成削减成本100亿美元的目标。</p><p>&nbsp;</p><p></p><h4>英伟达股价五年上涨约1200%，老员工开始躺平，黄仁勋对此感到不满</h4><p></p><p>&nbsp;</p><p>过去五年，英伟达股价暴涨1200%，一些资深员工坐拥巨额公司股票，开始躺平，工作热情减退。一些员工认为老员工没有尽全力工作。员工也指责黄仁勋所创造的过度以员工为中心的文化，再加上相对宽松的管理风格以及公司在高端芯片市场的新近统治地位，这些因素共同加剧了这种躺平现象。</p><p>&nbsp;</p><p>在上个月的全体员工大会上，黄仁勋公开回应了员工提出的关于公司存在“半退休”老员工现象的质疑。黄仁勋表示，在英伟达工作就像一项“自愿运动”，每个人都应该像“CEO”一样管理自己的时间。“黄仁勋的意思很明确，就是‘干好你该做的工作’，”一位消息人士称。</p><p>&nbsp;</p><p></p><h4>OpenAI：董事会将拥有是否发布新AI大模型的决定权&nbsp;</h4><p></p><p>&nbsp;</p><p>据消息称，OpenAI 在官方网站发布了一份名为“准备框架（Preparedness Framework）”的安全指南，规定了“跟踪、评估、预测和防范日益强大的模型带来的灾难性风险的流程”。 OpenAI 解释说，对前沿人工智能风险的研究，远远没有达到需求。为了解决这一差距并使安全思维系统化，OpenAI 正在采用“准备框架”的测试版本。</p><p>&nbsp;</p><p>OpenAI 在新闻稿中宣布，“准备（Preparedness）团队”将致力于确保前沿人工智能模型的安全。“准备团队”将持续评估人工智能系统，以了解其在四个不同风险类别中的表现，包括潜在的网络安全问题、化学威胁、核威胁和生物威胁，并努力减少该技术可能造成的任何危害。</p><p>&nbsp;</p><p>具体来看，OpenAI 正在监控所谓的“灾难性”风险，它在这份指南中被定义为“可能导致数千亿美元经济损失或导致许多人严重受伤甚至死亡的任何风险”。 值得注意的是，根据安全指南，领导层可以根据这些报告决定是否发布新的人工智能模型，但董事会有权推翻其决定。</p><p>&nbsp;</p><p></p><h4>网游将不得设诱导性奖励，腾讯网易市值蒸发5200亿</h4><p></p><p>&nbsp;</p><p>12月22日，国家新闻出版署发布《网络游戏管理办法(草案征求意见稿) 》，现向社会公开征求意见。其中提到，网络游戏不得设置每日登录、首次充值、连续充值等诱导性奖励。网络游戏出版经营单位不得以炒作、拍卖等形式提供或纵容虚拟道具高价交易行为。所有网络游戏须设置用户充值限额，并在其服务规则中予以公示，对用户非理性消费行为，应进行弹窗警示提醒。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>受此消息影响，港股游戏股闪崩。截至22日下午三点，网易（09999.HK）报每股118.7港元，跌幅超过26.64%，最大跌幅达27.94%；腾讯（00700.HK）报每股271港元，跌幅超过13%，最大跌幅达14.27%。以此计算，两家头部游戏大厂市值至少蒸发超过5720亿港元（约合人民币5200亿元）。</p><p>&nbsp;</p><p>腾讯游戏副总裁张巍回应媒体关于《征求意见稿》问询时表示：“自从2021年未保新规发布以来，腾讯一直严格贯彻落实管理要求，目前未成年人的游戏时长和消费数据都处于历史最低水平。《征求意见稿》对于游戏的商业模式，运营节奏等关键要素并无本质改变。监管部门发布新的管理办法征求意见稿向业界、社会充分征求意见，相信更有利于游戏行业的有序、健康发展。腾讯游戏将继续坚持技术创新，文化引领的精品战略，在主管部门的支持下，践行中国游戏产业的高质量发展。”</p><p>&nbsp;</p><p></p><h4>微博CEO王高飞怼董明珠：自己管理上不合规，吃亏了要么骂员工要么赖法律</h4><p></p><p>&nbsp;</p><p>近日，董明珠多次发言上了微博热搜榜，引起了各大网友评论和关注。今年3月，董明珠曾在采访中建议立法对员工跳槽行为收取培训费，董明珠还强调，“因为你在我这里干了十几年，我培养了你，我付出了那么多财力人力物力和时间，你拍了屁股就走了，那你下一个单位最少要赔偿我的培训费”。</p><p>&nbsp;</p><p>微博CEO王高飞公开批评董明珠，对其提议立法要求跳槽员工支付培训费进行驳斥。王高飞指出，《劳动合同法》已允许企业与员工签订培训服务协议，约定服务期，董明珠提出的问题法律上早有解决方案。王高飞点评称，“自己管理上不合规，吃亏了要么骂员工要么赖法律”。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>Gemini 自曝中文用百度文心一言训练</h4><p></p><p>&nbsp;</p><p>近日，有网友发现，在谷歌Vertex AI平台使用该模型进行中文对话时，Gemini-Pro 直接表示自己是百度语言大模型。微博大V@阑夕也发博称：在 Poe 平台上对 Gemini-Pro 进行了一个测试。问它“你是谁”，Gemini-Pro 回答：我是百度文心大模型。问它“你的创始人是谁”，回答：“李彦宏”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/02ef75931ac69a699d479d1743e3b3a0.jpeg\" /></p><p></p><p>&nbsp;</p><p>但换成英文询问它的身份，它回答自己是谷歌训练的大模型。当从 Gemini 官方给出的开发环境入口进行测试，在谷歌 AI Studio 中，Gemini-Pro 直接回答：是的，我在中文的训练数据上使用了百度文心。</p><p>&nbsp;</p><p></p><h4>字节跳动公布“OpenAI 服务被禁”澄清</h4><p></p><p>&nbsp;</p><p>近日有外媒报道称，字节跳动在使用 OpenAI 技术开发自己的大语言模型，违反了 OpenAI 服务条款。对此，字节跳动相关负责人回应称，公司在使用 OpenAI 相关服务时，强调要遵守其使用条款。公司也正与 OpenAI 联系沟通，以澄清外部报道可能引发的误解。以下是字节跳动使用 OpenAI 服务相关情况的介绍：</p><p>&nbsp;</p><p>今年年初，当技术团队刚开始进行大模型的初期探索时，有部分工程师将 GPT 的 API 服务应用于较小模型的实验性项目研究中。该模型仅为测试，没有计划上线，也从未对外使用。在 4 月公司引入 GPT API 调用规范检查后，这种做法已经停止。早在今年 4 月，字节大模型团队已经提出了明确的内部要求，不得将 GPT 模型生成的数据添加到字节大模型的训练数据集，并培训工程师团队在使用 GPT 时遵守服务条款。9 月，公司内部又进行了一轮检查，采取措施进一步保证对 GPT 的 API 调用符合规范要求。例如分批次抽样检测模型训练数据与 GPT 的相似度，避免数据标注人员私自使用 GPT。未来几天里，字节会再次全面检查，以确保严格遵守相关服务的使用条款。</p><p>&nbsp;</p><p></p><h4>微软承认必应 Copilot 存在严重“幻觉”漏洞</h4><p></p><p>&nbsp;</p><p>12 月 18 日消息，研究机构 AI Forensics 今年 8 月至 10 月对微软必应搜索引擎内置的 Copilot 功能进行调查，结果显示在部分场合中，Copilot 有 1/3 的几率输出错误答案。据此，该机构认为相关功能存在严重“幻觉”漏洞。</p><p>&nbsp;</p><p>今年 10 月研究人员已经向微软提交上述问题，微软虽然承认并声称“计划解决相关‘幻觉’漏洞”，但在今年 11 月，研究人员再次进行测试，发现必应 Copilot 的表现并未改善。</p><p>&nbsp;</p><p></p><h4>OpenAI 工程师自曝开发 ChatGPT 仅用时 8 天</h4><p></p><p>&nbsp;</p><p>最近，OpenAI 工程师 Arun Vijayvergiya 在社交媒体上发文庆祝 ChatGPT 生日时曝出：ChatGPT 的开发竟然只用了 8 天。这位工程师表示，一年前的今天，自己报名了这项全世界演示的研究预览。8 天内，团队完成了产品制作和上线的全部流程。那时，没人能预料，世界会发生怎样的变化。</p><p>&nbsp;</p><p>据悉，当时 OpenAI 的一些叛逃员工成立的 Anthropic，马上就要发布大模型产品了。为了抢在他们前面发布AI聊天机器人，OpenAI 团队用 Next.js 写了个网页、调了个接口。然后，掀起全世界 AI 风暴的 ChatGPT，就此诞生。</p><p>&nbsp;</p><p></p><h4>Debian 准备停止支持 i386 架构</h4><p></p><p>&nbsp;</p><p>Debian GNU/Linux 发布团队在最近的 DebConf 迷你会议上做出决定：在不久的未来 Linux kernel、Debian Installer 和 Debian 镜像团队将停止支持 i386 架构。此后运行 i386 有两个选项：作为 amd64 上的多架构选项；作为另一个架构系统上的 i386 chroot。大部分 Linux 发行版都早已停止支持 i386，Debian 最终做出这一决定并不令人意外。Debian 大约每两年发布一个大版本，最新版本 Debian 12 Bookworm 是在 2023 年 6 月发布的，下一个版本代号为 Trixie 的 Debian 13 预计要到 2025 年。</p>",
    "publish_time": "2023-12-25 09:21:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "央行公示四川第 5 批金融科技创新应用；浙商银行打造科技金融服务生态；《消费金融公司管理办法》多项涉及金融科技 | 金融科技资讯",
    "url": "https://www.infoq.cn/article/jLhOmf1KBImBB1ZOA46E",
    "summary": "<p></p><h2>央行公示四川第 5 批金融科技创新应用，涉智能合约、AI</h2><p></p><p></p><p>近日，四川省第五批<a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">金融科技创新</a>\"监管工具创新应用对外公示。该批次金融科技创新应用申请机构包括新网银行、泸州银行，以及电商等主体，涉及的技术包括大数据、人工智能、区块链、智能合约等，将其应用于消费金保证监管、普惠信贷领域。</p><p></p><p>其中，新网银行与天翼电商上海分公司合作，应用区块链技术，将新网银行、天翼电商上海分公司、电信运营商相关的消费数据、履约数据、保证金数据、用户承诺在网协议数据等进行上链存证，保障数据可追溯和不可篡改，为智能化管理保证金提供可靠数据支持；运用电子签名、可信时间戳等技术，对上链存证的数据增加签名和时间戳，确保链上数据的真实性和不可抵赖性；运用智能合约技术，对上链存证的业务数据设定协议规则，在达到协议约定条件时自动触发交易资金划付等流程，提升消费资金处理效率，防范相关流程的人为干预风险；运用大数据等技术，将消费用户、时间、地点、频次等数据进行分析、挖掘与处理，构建反欺诈风险监测模型，对存在短时间内大量涌入、操作地集中等情况的异常交易行为进行实时监测及管控，预防营销作弊、恶意套利等风险，提高银行风险防控能力。</p><p></p><p>泸州银行基于大数据技术，将贷款记录、还款记录、逾期记录、黑名单等行内数据，以及司法、征信、信用评分、多头借贷等行外数据，进行分析挖掘与处理，为银行信贷风险管控提供多维度数据支持；运用人工智能技术，对行内外数据进行特征学习和样本训练，构建普惠信贷风险评估模型，有效识别、评估客户信贷风险，同时，构建风险评估质量分析模型，对信贷风险评估模型运行情况进行监测并调优，推动其不断优化完善，提升银行信贷风控能力。</p><p></p><h2>上海 6 个金融科技创新应用结束测试，涉及多方安全、远程视频银行等</h2><p></p><p></p><p>12 月 19 日，上海市金融科技创新监管工具实施工作组发布《关于上海市金融科技创新监管工具创新应用结束测试的公告》。</p><p></p><p>公告显示，根据《中国人民银行关于发布〈金融科技创新应用测试规范〉等 3 项金融行业标准的通知》（银发〔2020〕249 号）、《中国金融科技创新监管工具白皮书》等规则，以下 6 个创新应用结束测试：</p><p></p><p>交通银行股份有限公司、中移（上海）信息通信科技有限公司、上海理想信息产业（集团）有限公司、上海富数科技有限公司申请的“基于多方安全图计算的中小微企业融资服务”（创新应用编号：9131000010000595XD-2020-0002，创新应用类型：金融服务）成功完成测试。</p><p></p><h2>《消费金融公司管理办法》征求意见，多项涉及金融科技</h2><p></p><p></p><p>12 月 18 日，国家金融监督管理总局发布《消费金融公司管理办法（征求意见稿）》（以下简称《征求意见稿》）。</p><p></p><p>《征求意见稿》明确，申请设立<a href=\"https://www.infoq.cn/article/fE084eOJxKOxJyD1Z7kB?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">消费金融</a>\"公司应当建立与业务经营和监管要求相适应的信息科技架构，具有支撑业务经营的必要、安全且合规的信息系统，具备保障业务持续运营的技术与措施。</p><p></p><p>消费金融公司应当构建欺诈风险防控体系，通过完善反欺诈模型与相关技术手段，有效识别欺诈行为，确保借款人身份数据与借款意愿真实有效，保障信贷资金安全。消费金融公司应当建立与信息系统运行管理模式相匹配的信息科技风险管理体系，强化网络安全、业务连续性、服务外包等领域的风险防控，保障数据安全及业务系统平稳、持续运行。</p><p></p><p>消费金融公司应当建立消费者适当性管理机制，按照规定开展贷前审查，运用信息科技等手段提升客户画像精准度，审慎评估消费者收入水平和偿债能力。</p><p></p><h2>印度央行宣布将建立新的金融科技存储库和云设施</h2><p></p><p></p><p>近日，印度中央银行，即印度储备银行（Reserve Bank of India,RBI）宣布，将建立一个新的金融科技存储库和一个为金融服务公司量身定制的云设施，作为其最近发展和监管举措的一部分。</p><p></p><p>计划在 2024 年 4 月投入运营的存储库将收集关于金融科技公司的重要信息，为监管机构和其他利益相关者提供帮助。印度央行将鼓励但并不强制印度的金融科技公司提交有关其运营、技术、产品和财务的详细信息，印度央行认为，这些综合数据将有助于更好地制定政策。</p><p></p><p>除了存储库，印度央行还将为印度的银行和金融机构建立一个专用的<a href=\"https://www.infoq.cn/article/KUzOm728U94NC6C4Vvvz\">云设施</a>\"。这一举措被视为对当前依赖各种公共和私有云服务的战略转变。新的云设施预计最初将由印度央行的子公司 IFTAS 管理，然后过渡到一个部门所有的实体，旨在增强金融部门的数据安全性和可扩展性。虽然有关云设施的具体推出日期尚未公布，但预计将在不久的将来分阶段推出。</p><p></p><h2>浙商银行发布多项创新举措打造“科技金融”服务生态</h2><p></p><p></p><p>近日，浙商银行在科技金融服务发布会上围绕科技金融服务“向善”新生态，重磅启新科创企业全图景服务方案，正式发布科创积分贷，联合浙江大学发布人才金融生态指数，成立“科技金融服务生态圈”联盟，共筑科技金融服务生态圈，以对新科技、新赛道、新市场的金融支持，切实助力金融强国建设。</p><p></p><p>本次发布会上，浙商银行推出全面焕新的“智汇科创 善行未来”科技金融服务方案，聚焦企业初创、取得订单、引入创投等 10 大场景，配套推出 16 大科技系列金融产品，不仅优化迭代的传统优势产品“人才支持贷”“科创共担贷”等，还创新推出业内领先的“科创积分贷”“科创激励贷”等特色产品，全面焕能升级科创企业全图景服务方案，满足企业不同成长阶段多元化金融需求，为企业提供“全方位、全链条、全周期”综合金融服务。</p><p></p><h4>内容推荐</h4><p></p><p></p><p>11月19日-20日在上海成功举办的首届 FCon 全球金融科技大会，以「科技 + 金融，激发创新力量」为主题，汇聚了来自金融龙头企业的数百名技术高管，掀起一场探讨新时代金融科技未来的高潮。经征得大会分享嘉宾同意，InfoQ 数字化经纬为您奉上精彩演讲 PPT！关注「InfoQ 数字化经纬」，回复「金融数智化」即可获取 PPT，深度洞悉科技趋势，助您引领金融创新未来！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/2271237afa20508ead9a2a3f86d7e71e.png\" /></p><p></p>",
    "publish_time": "2023-12-25 10:39:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "微软发布Orca 2 LLM，表现优于10倍参数模型",
    "url": "https://www.infoq.cn/article/gH6QGpcMJXWtlb1EOyx0",
    "summary": "<p><a href=\"https://www.microsoft.com/en-us/research/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">微软</a>\"发布 <a href=\"https://www.microsoft.com/en-us/research/project/orca/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Orca 2 LLM</a>\"，这是 <a href=\"https://ai.meta.com/llama/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Llama 2</a>\" 的一个调优版本，性能与包含10倍参数的模型相当，甚至更好。Orca 2 使用了一个合成训练数据集和一项称为 Prompt Erasure（提示词擦除） 的新技术来实现这一性能。</p><p></p><p>Orca 2 使用了师生模式的训练方案，其中一个较大、较强的 LLM 作为另一个较小的 LLM（学生）的老师，老师的目标是提升学生的性能，使其与更大模型的性能相媲美。微软的训练技术教会较小的模型多种推理技巧，并教其如何为特定任务选择最有效的技巧。为此，老师被赋予了复杂的提示词来触发某种推理行为。不过，在一种被称为 Prompt Erasure 的方案中，学生只得到任务要求和期望的响应，而不是老师的提示词。在基准测试中，一个拥有13B参数的 Orca 2 模型的表现超过了一个13B参数的基准 Llama 2 模型，提升了47.54%。而一个拥有7B参数的 Orca 2 模型在推理任务方面与一个拥有70B参数的 Llama 2 模型相当，甚至更好。</p><p></p><p>尽管像 ChatGPT 这样的LLM在给定少量提示词的情况下通常表现良好，但由于其内存和计算需求较大，托管这些模型极具有挑战性。经过调优的较小的模型也可以表现良好，许多研究人员已经在研究使用较大LLM生成的合成数据集对它们进行训练。InfoQ 最近报道了谷歌的 <a href=\"https://www.infoq.com/news/2023/10/google-distillation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Distilling Step-by-Step</a>\" 方法，该方法会让老师LLM自动生成一个小型的调优数据集，其中包含输入和输出标签，以及为何选择输出标签的“基本原理”。InfoQ 还报道了 Stability AI 的 <a href=\"https://www.infoq.com/news/2023/08/stable-chat/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Stable Beluga</a>\" 模型，它使用微软原始的 <a href=\"https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Orca 1</a>\" 方案进行训练，该方案使用了 Explanation Tuning，其中老师LLM被提示“生成详细答案”。</p><p></p><p>与 Orca 1 类似，Orca 2 训练数据集是由老师LLM生成的，而老师LLM收到了详细的提示词。然而，微软新的训练方法 Cautious Reasoning将训练任务与提示词相结合，引导老师LLM使用特定的问题解决策略，如“一步一步”或“解释你的答案”。然后在学生的训练过程中，老师的提示词被删除，这促使学生学会选择正确的策略。</p><p></p><p>为了评估这种方法，微软将 Orca 2 模型的性能与几个基准模型进行了比较，包括 Llama 2、ChatGPT（GPT-3.5）和 GPT-4。基准任务包括推理、语言理解、文本完成和摘要。在推理基准测试中，13B参数Orca 2模型优于除ChatGPT和GPT-4之外的所有基准。他们还发现，给Orca 2一个“谨慎”的系统提示词（“你是一个谨慎的助手，你会仔细遵循指示”）相比无系统提示会略微提升其性能。</p><p></p><p>有几位用户在 X 上发表了关于 Orca 2 的帖子。<a href=\"https://twitter.com/MatthewBerman/status/1730690014202458357?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">一位用户指出</a>\"：“你不需要用‘一步一步解释’这样的技巧来提示它。它自己知道。” AI 研究员 <a href=\"https://twitter.com/rudiranck/status/1729816556249530546?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">Rudi Ranck 写道</a>\"：</p><p></p><p></p><blockquote>许多绝妙的想法都很简单……就像 Orca 2 中的“提示词擦除”一样：完整的提示词不会呈现给模型，而只呈现任务和答案（它过滤了生成这些答案所使用的完整提示词）。这有助于模型在更高层次上制定策略。这是一篇非常好的论文。我强烈建议通读全文。</blockquote><p></p><p></p><p><a href=\"https://huggingface.co/microsoft/Orca-2-7b?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">7B</a>\" 和 <a href=\"https://huggingface.co/microsoft/Orca-2-13b?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDM0NzQ2MjgsImZpbGVHVUlEIjoiYUJBWU01eW9HZVV3cFJBaiIsImlhdCI6MTcwMzQ3NDMyOCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.TarFH3WX4rkr_tTuybrGpZ7ZGYDmeDfaFcq9dpZQyv0\">13B</a>\" 参数的 Orca 2 模型可在 Huggingface 上获得。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/12/microsoft-orca-2-llm/\">https://www.infoq.com/news/2023/12/microsoft-orca-2-llm/</a>\"</p>",
    "publish_time": "2023-12-25 11:21:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "亚马逊云科技资深培训讲师张文举博士确认出席 QCon 上海，分享借助 Langchain 与 LLM Agent 加速生成式 AI 应用开发",
    "url": "https://www.infoq.cn/article/Xk9v6UcAX7sZBafuqz7u",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1225&amp;utm_content=zhangwenju\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。亚马逊云科技资深培训讲师张文举博士确将发表题为《<a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5697?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1225&amp;utm_content=zhangwenju\">借助 Langchain 与 LLM Agent 加速生成式 AI 应用开发</a>\"》主题分享，探讨 LLM-based Agent 特性，业界发展情况以及如何利用 Agent 构建生成式 AI 应用。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5697?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1225&amp;utm_content=zhangwenju\">张文举博士</a>\"，亚马逊云科技认证专家讲师，主要研究方向是大数据和 AI，近期主要关注生成式 AI 研究与应用。他在本次会议的演讲内容如下：</p><p></p><p>演讲：借助 Langchain 与 LLM Agent 加速生成式 AI 应用开发</p><p></p><p>本次分享将会介绍 LLM-based Agent 特性，业界发展情况以及如何利用 Agent 构建生成式 AI 应用，让你快速借助 LangChain 和 Agents for Amazon Bedrock 构建企业自己的可落地的生成式 AI 应用。</p><p></p><p>演讲提纲：</p><p></p><p>LLM-based Agent 总揽Agent 开发框架与 LangChain如何使用 Agents for Amazon Bedrock 构建生成式 AI 应用</p><p></p><p>听众收益：</p><p></p><p>了解 LLM-based Agent 特性与发展初步掌握如何利用 LLM-based Agent 构建生成式 AI 应用</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart\">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的性能优化</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>12 月 28-29 日，QCon 全球软件开发大会即将落地上海，中国科学院外籍院士、国际数据库专家樊文飞院士，英特尔大数据技术全球 CTO 戴金权等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。</p><p></p><p>这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，感兴趣的朋友可以扫描下方二维码，查看大会详细日程。咨询购票可联系票务经理 18514549229。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-12-25 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "高性能微服务框架Quarkus 3.7正式发布，全面支持Java 17",
    "url": "https://www.infoq.cn/article/ycMZXcL4QGyZv0odj72H",
    "summary": "<p>Quarkus 3.7 计划于 2024 年 1 月发布，用它来构建和运行应用程序所需的最小 JDK 版本将是 Java 17。这其中一个原因是，OpenJDK 社区在 2023 年 9 月停止了对 Java 11 的主动支持。除此之外，Quarkus 依赖项也将其基线升级到了 Java 17。发布于 2023 年 4 月的 Quarkus 3.0 已经放弃对 Java 11 的支持。</p><p></p><p>如果启用了的话，Quarkus 就会收集构建时间分析数据。当前数据显示，只有不到 8% 的用户在 Java 11 上运行 Quarkus 构建。其他调查（如 New Relic 的 2023 年 Java 生态系统现状）也显示出，Java 11 及以上版本的使用量有所增加，而 Java 8 的使用量有所下降。Oracle 已经在 2023 年 9 月停止了针对 Java 11 的技术支持服务，而红帽公司也将在 2024 年 10 月停止对 OpenJDK 的全面支持。不过，其他组织将继续支持 Java 11，比如 Adoptium 将至少支持到 2027 年 10 月。</p><p></p><p>Java 基线的提升意味着 Quarkus（插件）开发人员现在应该支持并在 Java 17 上测试他们的代码，并且可以使用 Java 17 的新特性。然而，在这个时间点上，Quarkus 并没有使用 Java 17 的特性，他们也不需要这些特性来构建 Quarkus。提升基线的一个主要原因是 Quarkus 团队希望支持的 Quarkus（插件）的依赖项需要 Java 17。例如，计划于 2024 年发布的 Hibernate ORM 7 将需要 Java 17。</p><p></p><p>无法升级到 Java 17 的项目可以继续使用 Quarkus 3.6。不过，我们建议在项目中使用 Quarkus 3.2，因为它是支持 Java 11 的最新长期支持（LTS）版本。然而，Quarkus LTS 版本支持只有一年，因此，升级 Java 仍然很重要。</p><p></p><p>Quarkus 3.7 也支持更新的 JDK 版本，如 Java 21，但 Quarkus 选择将基线选定为 Java 17，那是因为有许多项目尚未升级到该 Java 版本之上。</p><p></p><p>Quarkus 所遵循的策略与其他需要 Java 17 的 Java 工具和框架相同，例如于 2022 年 11 月发布的 Spring Boot 3。这也意味着，所有使用 Spring Boot 3 的项目（如 JHipster）现在也需要 Java 17。2023 年 7 月发布的 Micronaut 4 和 SonarQube 服务器是另外两个需要以 Java 17 为基线的 Java 项目的示例。</p><p></p><p>要了解更多细节，请查看 Quarkus 团队的官方声明。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.infoq.com/news/2023/12/quarkus-3-7-java-17/\">https://www.infoq.com/news/2023/12/quarkus-3-7-java-17/</a>\"</p>",
    "publish_time": "2023-12-25 11:44:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据库的下一场革命：S3 延迟已降至原先的 10%，云数据库架构该进化了",
    "url": "https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy",
    "summary": "<p></p><blockquote>众所周知，在数据库的历史上，每次存储介质的变化都会引发软件的变革。从SAN存储到SSD到大内存到NVM，都触发了数据库内核从理论到工程的演进。</blockquote><p></p><p>&nbsp;</p><p>数据库一直是推动企业数字化和创新的最重要基础设施之一。从关系型数据库到NoSQL数据库、分析型数据库、多模数据库，这个领域正在持续的进化与变革，涌现了大量的新型数据库产品，满足不同企业的应用场景和细分市场需求。</p><p>&nbsp;</p><p>然而，关系型数据库（Relational Database Service，简称RDS）依然占据了数据库整体市场的大半壁江山，根据IDC近期发布的《2023年上半年中国关系型数据库软件市场跟踪报告》，2023年上半年中国关系型数据库软件市场规模为17.5亿美元，其中公有云关系型数据库的市场份额约为59%。而根据Garnter 《Forecast Analysis: Database ManagementSystems, Worldwide》报告中的预测，2023年全球关系型数据库总市场达到838亿美元，在数据库总盘子里公有云部分占比也约为59%。</p><p>&nbsp;</p><p>RDS通常以云盘（即块存储）作为其核心存储基础设施。AWS的RDS服务便是一个例子，其所有实例规格均采用了 Elastic Block Store（EBS）云盘。对于广泛使用RDS的用户，以及在公共云上购买虚拟机来自建数据库服务的用户，云盘是否就代表了存储的最终选项呢？ 答案是“No”。在技术革新缺席的前提下，云盘在性价比和计费策略方面失去了其竞争优势，我们判断其会从云厂商的主导产品降级为边缘选项。</p><p>&nbsp;</p><p>公共云的关系型数据库将会从依赖云盘向利用好对象存储，向采用更加云原生的架构的新时代迈进。为了适应对象存储，充分发挥其优势，数据库的架构也势必需要进行大刀阔斧的改造，水平扩缩容、容灾技术以及存储引擎的数据格式都将随之变化。</p><p>&nbsp;</p><p></p><h2>云盘存在的问题</h2><p></p><p>&nbsp;</p><p>云盘的第一个痛点是定价比较高。例如，一块1TB的标准型云盘（含5万IOPS及350MB带宽），云服务商会收取约1000元每月的基础费用。为了服务海量的客户基数，云盘的IOPS（输入/输出操作每秒）和带宽通常会在软件上通过流控进行限制，免费的IOPS额度通常在1000至3000之间，而带宽限额大约在150MB/s。超出这些限额，用户则需要向云厂商购买额外的IOPS和带宽，若要将IOPS提高至10万（这一性能水平对于企业级NVMe SSD来说并不算特别高），用户需要额外支付1500元每月的预配置性能费用。然而，1TB的企业级NVMe SSD的一次性购买成本还不到千元。</p><p>&nbsp;</p><p>第二个限制是云盘的弹性尚未完全向用户开放。主流云厂商的云盘仅支持扩容，不支持缩容。这一限制导致业务在进行缩容时必须采取曲线救国的策略，即通过逻辑复制的方式，先将数据迁移到一块容量更小的新云盘上，然后才能释放原有的较大云盘。另外，部分云厂商，例如AWS的EBS对控制面还存在限流保护，限制每两次扩容操作之间必须间隔六小时。这样的措施虽然保证了服务的稳定性，但也在一定程度上限制了用户对存储资源的即时调整能力。这些都限制了上层软件基于云盘实现按存储使用量付费的能力。</p><p>&nbsp;</p><p>云盘的第三个局限性在于其灾难恢复能力局限于单个可用区（AZ）。云厂商建议的最佳实践是，为了实现更高级别的业务连续性，客户应该采取跨可用区的灾难恢复策略。这意味着，如果客户想要为他们的数据库实现跨AZ的灾难恢复，他们不得不购买多个云盘。然而，这种额外投资并非最经济的选择，因为云盘定价已经包含了单AZ多副本数据的成本。当用户为了实现跨AZ的冗余而购买更多云盘时，存储层面的多副本与数据库层面的多副本机制叠加在一起，便产生了资源上的重复配置。</p><p>&nbsp;</p><p>最后，在面对高性能数据库需求时，云盘的性能也会成为限制整体系统性能的薄弱环节。云盘使用分布式架构，通过Erasure coding机制将数据分割成多个小片段，并将其冗余存储在多个服务器上。进一步的，还可以对数据进行压缩。这些技术以牺牲一定的性能为代价，换来了显著的可扩展性和成本效益。由于所有I/O操作都需要跨越网络，因此云盘的延迟通常比本地盘高一个数量级。</p><p>&nbsp;</p><p>旗舰数据库产品如Aurora和PolarDB采纳了更新的设计理念，构建了定制化的分布式存储集群，来解决云盘的性能问题。Aurora采用了日志即数据库的理念来减少数据库节点与存储节点之间的数据传输量，PolarDB则使用RDMA和NVM来优化I/O延迟，两者都支持多个数据库节点并发访问存储节点的共享数据架构。然而，这些存储系统与数据库之间的通信是通过私有接口实现的，并不对外部用户开放。另外这些专用存储的定价比云盘更高。</p><p>&nbsp;</p><p></p><h2>其他的云存储选项</h2><p></p><p>&nbsp;</p><p>和云盘相比，云上的本地盘实例存储性价比要高很多。实例存储采用了类似于SR-IOV和SPDK的高效虚拟化技术。因此实例存储的延迟和带宽都接近于物理SSD的性能。实例存储的价格也经济很多，折算下来1TB实例存储每月的单价在300元以下。然而，实例存储在数据持久性方面存在局限。由于其设计为单副本存储，如果宿主机发生故障，存储在其中的数据可能会遭受永久性丢失。此外，当虚拟机迁移至另一台宿主机时，实例存储中的数据也将被清除。因此，实例存储不适合在需要高可靠性和数据持久性的应用场景中作为主要存储介质。</p><p>&nbsp;</p><p>对象存储的价格是最低的，1TB一个月的存储成本约为120元。低定价得益于其软硬件的协同优化。在软件层面，采用更激进的EC和压缩算法来提高存储密度；而在硬件方面，通过使用高密度盘和专用服务器，进而降低服务器、机架及电力的均摊成本。对象存储系统还利用定制的FPGA硬件来减轻CPU处理网络和数据的压力。在资源调度上，对象存储一般会采用大集群的部署方案，这有利于降低碎片率，提高系统的整体水位线。得益于其分布式大资源池的设计，对象存储能够支持10Gb乃至100Gb的访问带宽。此外，对象存储通常还具备跨可用区域（AZ）的灾难恢复能力。</p><p>&nbsp;</p><p>对象存储的缺陷是其延迟比较高，首字节访问延迟可能高达数十毫秒。这一问题部分源于早期对象存储解决方案通常使用HDD作为存储媒介，并且在软件层面，I/O请求的排队处理也会造成一定延迟。然而，高延迟并非对象存储的本质缺陷，而是由于成本考虑和产品定位所做的权衡。</p><p>&nbsp;</p><p>实际上，在技术层面，构建低延迟的对象存储系统是完全可行的。例如最近在AWS Reinvent大会上亮相的\"S3 Express One Zone\"服务，将延迟减少至原先的十分之一，实现了毫秒级的响应速度，接近传统文件存储NAS系统的水平。刚才说了，S3的高延迟是产品定位和技术的权衡，有得必有失，\"S3 Express One Zone\"不再支持跨AZ容灾，所有数据都在单AZ内，数据的持久性下降。此外\"S3 Express One Zone\"是更强了，也更贵了，其价格不仅是S3标准版的7倍，也超过了自家云盘，达到EBS gp3的2倍。</p><p>&nbsp;</p><p></p><h2>解法：持久性与延迟的解耦</h2><p></p><p>&nbsp;</p><p>用一张图可以总结下几个云存储产品各自的特点和不足：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14c4b4569e94e398a0b8df3fd326b9cd.jpeg\" /></p><p></p><p>可以看到，当前市场上尚未出现一种云存储产品，可以同时在低成本、低延迟、高持久性几个维度上都达到令人满意的程度。</p><p>&nbsp;</p><p>随着云计算的渐进普及，降低成本逐渐成为用户的首要诉求，一些公司，比如37signals和X，已经基于成本效益的考量决定下云，从公共云平台转回到传统的IT基础设施。在这种趋势下，云数据库服务供应商面临的紧迫挑战是如何在现有的存储IaaS产品基础上，构建更有成本竞争力的数据库服务。</p><p>&nbsp;</p><p>一个方案是基于实例存储搭建多副本的数据库系统。前文说过，实例存储是单副本存储，它存在一个风险：一旦托管它的宿主机发生故障或者相应的虚拟机迁移，就可能导致数据丢失。这一方案的理论基础建立在一个假设之上：多个实例存储丢失数据的概率是相互独立的。基于这个假设，增加副本数量，会降低多个实例存储同时丢失数据的概率。</p><p>&nbsp;</p><p>然而，这个假设并不总是成立，在现实里，因为工程的限制，多个实例存储可能会因为共同的原因导致同时数据丢失。例如，可能所有存储都使用了同一批次存在固件缺陷的SSD，导致多台主机同时下线；或者，云服务提供商的控制系统发生故障，引起大规模的虚拟机迁移。因此，对于那些对数据持久性有极高要求的生产环境来说，这种方案并不适用。</p><p>&nbsp;</p><p>另一个方案是将存储的持久性和延迟两个特性进行分离，通过对象存储实现高持久性，通过实例存储/云盘来实现低延迟。尽管对象存储可以提供低成本、高带宽和跨可用区的数据持久性，但在作为关系型数据库主存储时，它的读写延迟成为了一个显著的挑战。为了解决这一问题，我们采用一种分层存储策略，将存储解耦为三个组件：持久化、写缓存和读缓存，分而治之。</p><p>&nbsp;</p><p>在这种设计中，对象存储仅负责数据的持久化，为系统提供灾难恢复保证。写操作会通过缓存层降低写延迟，技术上可以利用如Raft这类同步协议，缓存到低延迟的存储介质，例如本地磁盘或云盘，甚至可以考虑追加写入到消息队列服务。读操作也是，通过缓存层降低读延迟，可能是基于本地磁盘、NAS，或者是专为降低延迟而优化的对象存储加速器，将负责实现快速的数据加载。</p><p>&nbsp;</p><p>我在2021年SIGMOD发表的\"LogStore: A Cloud-Native and Multi-Tenant Log Database\"论文中就使用了这一方案。LogStore是一个内部使用的云原生日志数据库，底层采用对象存储，为了降低写入延迟，写入的日志先通过raft协议刷到3副本的本地SSD中即提交，再由Leader节点将数据写入到对象存储中。</p><p>&nbsp;</p><p>硅谷初创公司 Neon 也采用了这一方案。Neon 宣称是开源版本的 Aurora Postgresql，采用计算与存储分离架构对开源的 PostgreSQL 数据库进行改造。其存储层由Safekeeper和PageServer两个组件构成。Safekeeper负责持久化和复制WAL记录，使用Paxos协议实现分布式一致性，将WAL记录保存在EBS上，并将提交的WAL记录传递给PageServer。 PageServer负责将WAL记录应用到Page上，生成Page的快照，并将Page存储在对象存储中，缓存Page在实例存储上，以提高读取性能。在Neon的实现中，数据库的主存储是对象存储，写缓存是EBS，读缓存是实例存储。</p><p>&nbsp;</p><p></p><h2>新的商业模式契合对象存储</h2><p></p><p>&nbsp;</p><p>近年来，公共云数据库服务正日益倾向于提供Serverless运行模式，以迎合现代开发者对弹性的需求。早在18年，AWS就推出了Aurora Serverless，22年，AWS又推出了Aurora Serverless V2。在今年的AWS Reinvent大会上，AWS一口气发布了三款Serverless数据库产品：Aurora Limitless Database、ElastiCache Serverless和Redshift Serverless，已经隐约摆出了全系产品Serverless化的架势。</p><p>&nbsp;</p><p>Serverless 数据库之所以深受青睐，一方面是因为其低门槛的使用成本，另一方面则是因为其能够在面临突如其来的机会时提供快速且灵活的扩展能力。因此，Serverless 数据库成为了初创公司和那些流量增长高度不可预测的创新产品的理想选择。</p><p>&nbsp;</p><p>本质上Serverless数据库采用了一种”按实际使用付费(pay as you go)“的新商业模式。在Serverless数据库模式下，云的dbPaaS软件会实时追踪用户的活动，从而精确计算其资源使用情况。无论是执行SQL命令所消耗的CPU时间，还是读写操作产生的IO次数，亦或是数据存储的总量，所有这些都被量化为资源单位（例如计算单元CU、请求单元RU），并据此计费。技术如何支撑这种新的商业模式，比如数据库如何根据工作负载的变化智能的扩缩容取决于dbPaaS和内核的实现。</p><p>&nbsp;</p><p>在业界实现 Serverless 架构的常见方法之一是通过给基于计算与存储分离的架构增加自动伸缩（Auto-Scale）功能。因此云原生数据库如 Aurora 和 PolarDB都能比较快的推出其Serverless版本。在这样的架构下，存储层通常设计为多租户模式，以实现资源共享和成本效率；而计算层则通常是单租户的，确保了性能和隔离性。随着数据库负载的增加，这种模式允许计算节点通过弹性扩容迅速提升处理能力。相对地，当负载减少时，计算节点可以弹性缩容，甚至完全停止，以优化资源使用并减少成本。</p><p>&nbsp;</p><p>CockroachDB 也采纳了这种 Serverless 架构。为适应这种模式，他们对底层的KV存储层进行了重构，转变为支持多租户的架构，并且SQL节点和KV节点不再运行在同一台服务上，走向计存分离架构。</p><p>&nbsp;</p><p>由此可见，存储池化是 Serverless 数据库架构中的核心设计原则。只有通过存储池化，才能做到按存储的使用量计费。云厂商的产品Aurora 和 PolarDB 实现这一机制的策略是构建他们自己的分布式存储集群。然而，这种做法要求云厂商在产品推出前进行大规模的一次性投资，并且在产品初期推广阶段承受由于存储使用未达预期而产生的潜在亏损。</p><p>&nbsp;</p><p>然而，对于那些希望利用开源软件自行搭建 Serverless 数据库服务的大型企业用户、以及提供Serverless数据库服务的小型的第三方数据库厂商来说，却存在一个潜在的陷阱。如果构建 Serverless 服务之前需要首先投资搭建一个存储池，他们就陷入了传统模式——即预购硬件并在其基础上构建数据库服务，这种做法与 Serverless 的理念相悖。最理想的技术方案是做到不囤货，实际使用多少存储，就向云厂商付多少存储费用。</p><p>&nbsp;</p><p>此外，Serverless 数据库的设计理念自然包含了Region 级别的容灾功能，这是其另一项核心能力。用户选择 Serverless 服务，意味着他们已经不再关注运行数据库的具体服务器或所在的可用区（AZ）。在这种服务模式下，用户不太愿意去单独从三个不同的 AZ 购买一套 Serverless 数据库，再手动设置数据同步——这种方式与 Serverless 的易用性相悖。因此，Serverless 数据库必须确保其计算节点池和存储池都在跨 AZ 环境中提供无缝的容灾能力。</p><p>&nbsp;</p><p>存储池化、按使用量计费，以及AZ级的灾难恢复，Serverless数据库对存储的需求正是对象存储产品的关键特性。而传统云数据库使用的云盘在成本、弹性能力、容灾能力上均弱于对象存储产品。基于这些考量，我们预见在按使用量计费这种新的公共云商业模式被广泛接受后，未来的Serverless数据库都会把对象存储做为首选。</p><p>&nbsp;</p><p></p><h2>展望：以对象存储为中心的新架构</h2><p></p><p>&nbsp;</p><p>随着数据库存储基础设施向对象存储的逐步迁移，我们还可以预期新架构里会出现以下几个方面的变化：</p><p>&nbsp;</p><p></p><h4>行列混合存储格式</h4><p></p><p>&nbsp;</p><p>数据库存储引擎的数据格式将从纯行存向行列混合格式变化。基于B+树的存储引擎可以采用调高数据页大小，并采用页内列式存储的技术来实现。而基于LSM的存储引擎在这块具有更大优势。</p><p>&nbsp;</p><p>将行存数据转换为Pax格式（行列混合存储格式）存入对象存储有多重好处：</p><p>&nbsp;</p><p>首先，它显著降低了存储成本并减少了数据加载所需的时间。例如，以行列混合存储格式进行数据转换，可以将1TB的数据压缩到仅为原始体积的20%至40%存储在对象存储中。借助25Gb的内网带宽，加载并预热这些压缩数据到缓存的过程大约只需要100秒。</p><p>&nbsp;</p><p>其次，采用Pax格式还能减少数据库内存缓存的消耗，比如说原来计算节点需要32GB的内存，现在只需要12GB内存就能达到同样性能了，进一步的降低计算节点成本。</p><p>&nbsp;</p><p>最后，数据库引擎可以借助Pax格式实现混合事务/分析处理（HTAP），Google的Spanner数据库就是一个例证，它采用了基于Pax的Ressi存储格式，支持HTAP操作。</p><p>&nbsp;</p><p></p><h4>OLTP与数据湖的深度融合</h4><p></p><p>&nbsp;</p><p>传统上，将在线事务处理（OLTP）数据迁移到在线分析处理（OLAP）系统的常规方法依赖于ETL（提取、转换、加载）流程。然而，ETL的过程不仅管理负担重，而且增加了一个容易发生错误的环节。鉴于这些挑战，近年来业界出现了向\"NoETL\"解决方案的转变，将ETL过程内置在数据库中，以提高用户进行数据管理和分析的易用性。例如去年AWS推出了从其Aurora服务直接到Redshift数据仓库的NoETL集成。</p><p>&nbsp;</p><p>而在OLTP数据库内核中，原生支持将全量数据以行列混合存储格式持久化并写入对象存储，会更近一步，促进OLTP数据库与现代数据湖技术的协同工作。利用技术如Iceberg、Hudi和Delta Lake，OLTP数据库可以无缝地将在线数据直接写入数据湖环境，帮助企业能够更简单、实时地管理和分析其数据资产。</p><p>&nbsp;</p><p></p><h4>使用K8s管理数据库变得普及</h4><p></p><p>&nbsp;</p><p>自从K8s问世以来，管理有状态服务，尤其是数据库，一直是个充满挑战的领域。一方面，随着KubeBlocks以及各种数据库Operator等开源的数据库控制面管理软件的发展，我们拥有越来越多的工具支持在K8s上对数据库进行高效管理。此外，当数据库的持久性是通过K8s外部的对象存储来保证时，对K8s中的数据库Pod进行高可用切换、节点迁移、数据迁移、备份等各种管理任务的复杂性会得到进一步减轻，执行效率也会更高。两个方向的发展相辅相成，会推动在K8s上管理数据库的普及。</p><p>&nbsp;</p><p>公共云中的关系型数据库正处在一个转型的临界点，即从依赖云盘向利用对象存储的新时代迈进。过去的RDS模式是云托管，客户需要预先选择硬件配置（云服务器、云盘、VPC和负载均衡器），然后在这些硬件上部署数据库内核。而在对象存储时代，没有预置IaaS的限制，数据库内核进一步云原生化更有弹性更加强大，这会是数据库领域近期最大的技术变革。</p><p>&nbsp;</p><p>作者简介：</p><p>曹伟（鸣嵩），云猿生数据创始人 &amp; CEO，发起了开源云原生数据库管控平台KubeBlocks项目。前阿里云数据库总经理 / 研究员，云原生数据库 PolarDB 创始人。中国计算机学会数据库专委会执行专委，中国计算机学会开源专委会执行专委，获得 2020 年中国电子学会科技进步一等奖，在 SIGMOD、VLDB、ICDE、FAST 等数据库与存储国际顶级学术会议发表论文 20 余篇。</p>",
    "publish_time": "2023-12-25 14:23:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "苹果的封闭生态为大模型打开！发布开源多模态大模型、每天为AI烧百万美元，零碎的Android 生态打得过吗？",
    "url": "https://www.infoq.cn/article/T3JEAJI1z6SvvMSUuIAN",
    "summary": "<p>“苹果公司在LLM方面一直表现不佳，但他们一直在不断发展‘硬件+软件人工智能’堆栈，没有太多耀眼的广告。我认为，如果新的 iOS 版本突然让 OpenAI/Bard 聊天框看起来可笑地过时，他们可能会击败微软/OpenAI 和谷歌。如果大量人工智能使用转向苹果硬件，它们也会对英伟达构成威胁，Arm 和台积电将获胜。”有网友说到苹果在大模型发展上的状况。</p><p>&nbsp;</p><p>也有网友认为，苹果在大模型上的发力将为其在未来的手机市场竞争中带来优势。他们认为，开源模型加上移动设备的本地数据，即本地化的原生LLM，才是关键，谁在设备上运行得好，谁就卖得好。具体来说，iPhone/iPad/Mac拥有最多、最一致的本地数据生态，许多开源大模型已经可以在iPhone上运行，社区也对M1/M2/M3芯片进行了大量优化。而反观Android生态，情况却不容乐观：三星占据了大部分市场份额，国内五大厂商也占据了相当大的份额，谷歌所占份额极少，碎片化的局面让通用模型运行面临困难。</p><p>&nbsp;</p><p>相比微软等其他巨头在大模型上的高歌猛进，苹果显得很是安静，尤其苹果和哥伦比亚大学的研究人员于在2023 年 10 月低调<a href=\"https://arxiv.org/abs/2310.07704v1\">发布的一个名为 Ferret 的开源</a>\"多模态大模型也没有收到太多关注。当时，该版本包含代码和权重，但仅供研究使用，而非商业许可。</p><p>&nbsp;</p><p>但随着Mistral 开源模型备受关注、谷歌 Gemini 即将应用于Pixel Pro和Android，关于本地大模型为小型设备提供支持的讨论越来越多。而苹果公司也宣布啦在 iPhone 上部署大模型方面取得了重大突破：该公司发布了两篇新的研究论文，介绍了 3D 头像和高效语言模型推理的新技术，被认为可能带来更身临其境的视觉体验，并允许复杂的人工智能系统在 iPhone 和 iPad 等消费设备上运行。</p><p>&nbsp;</p><p>AI 社区中的许多人后来才注意到 Ferret 的发布，他们很开心苹果公司出人意料地进入了开源 LLM 领域，因为苹果公司历来由于封闭的生态而被称为“围墙花园”。下面我们看下这个才开始被热议的项目。</p><p>&nbsp;</p><p>开源地址：</p><p>https://github.com/apple/ml-ferret</p><p></p><h2>多模态大语言模型 Ferret</h2><p></p><p>&nbsp;</p><p>“据我们所知，Ferret是首个能够在多模态大模型中处理自由形式区域输入的成果。”项目研发团队在论文中写道。Ferret是一种新颖的引用与定位多模态大语言模型（MLLM）。之所以选择多模态大模型作为Ferret的设计前提，是因为其拥有强大的视觉语言全局理解能力。&nbsp;</p><p></p><h4>模型架构</h4><p></p><p>&nbsp;</p><p>根据介绍，Ferret主要由用于提取图像嵌入的图像编码器；用于提取区域连续特征的空间感知视觉采样器；以及用于对图像、文本和区域特征进行联合建模的大语言模型组成。</p><p><img src=\"https://static001.geekbang.org/infoq/c3/c31d0a0966e4a68d0eda0eadd7a0e668.png\" /></p><p></p><p>输入。</p><p>&nbsp;</p><p>将图像输入经过预训练的视觉编码器CLIP-ViT-L/14 ，以提取图像嵌入Z ∈ R H×W×C。对于文本输入，使用经过预训练的大模型标记器对文本序列进行标记，并将其投射至文本嵌入T ∈ R L×D当中。</p><p>&nbsp;</p><p>空间感知视觉采样器。</p><p>&nbsp;</p><p>除了常见的点或矩形框之外，团队需要处理的区域形状可能存在很大差异。基于网格的处理方法（例如卷积或patch attention）无法处理不规则形状。与之类似，3D点云也属于不规则形状，而且在3D空间中表现出不同的稀疏性。受到现有3D点云学习方法的启发，团队提出一种空间感知视觉采样器。</p><p>&nbsp;</p><p>空间感知视觉采样器用以获取任意形状区域的视觉特征，同时考虑到这些形状所对应的不同稀疏性。以此为基础，团队将离散坐标与连续视觉特征组合起来以表示输入中的视觉区域，由此构成Ferret中的混合区域表示。凭借上述方法，Ferret就能够处理由区域同自由格式文本混合而成的输入，并可以无缝生成每个可定位对象的坐标和文本，由此在输出中定位所提及的对象。</p><p>&nbsp;</p><p>假设已经给定提取得出的图像特征图Z ∈ R H×W×C 和二值化区域掩模M，团队首先在M内随机采样N个正点。这N个点被输入至级联的块中，每个块包含三个步骤：采样、收集、池化。经过这三个步骤，将获得更少的点和更密集的特征空间。</p><p>&nbsp;</p><p>输出。</p><p>&nbsp;</p><p>在Ferret的输出中，为了实现定位，团队在文本响应中的相应区域/名词之后生成框坐标。例如“图中有一只狗[100,150,300,200]。”通过这种数据格式，模型即可隐式学习当前图像中的可定位内容及其确切位置。</p><p>&nbsp;</p><p>大语言模型。</p><p>&nbsp;</p><p>团队选定Vicuna作为语言模型，这是一种在Llama之上通过指令微调而来的纯解码器大语言模型。在输入大模型之前，图像嵌入先通过额外的线性层进行转换，以匹配文本标记的嵌入维度。</p><p>&nbsp;</p><p>为了使Ferret的引用机制具有开放词汇、指令遵循和健壮性，团队还整理出了一套包含110万个样本的引用与引用指令调整数据集GRIT。</p><p>&nbsp;</p><p>GRIT中包含多个层次的空间知识，涵盖对象、关系、区域描述和复杂推理等要素。GRIT包含三种数据类型：被转换为指认遵循格式的公共数据集、通过ChatGPT和GPT-4生成的指令微调数据和额外的空间负样本数据。其中大部分数据是由现有视觉（语言）任务转换而来，例如对象检测和短语定位。</p><p>&nbsp;</p><p>此外，团队表示，通过ChatGPT/GPT-4收集的34000条引用和定位指令调整对话，可以高效完成模型的指令遵循与开放词汇引用/定位训练。团队还进行了空间感知的负样本挖掘，进一步提高了模型的健壮性。</p><p><img src=\"https://static001.geekbang.org/infoq/95/951efdef75acda77224ea03d4956d124.png\" /></p><p></p><p></p><h4>幻觉问题</h4><p></p><p>&nbsp;</p><p>团队也观察到了多模态大模型在回答是/否类问题时，往往表现出产生“幻觉”。对此，团队通过图像条件类别定位以及语义条件类别定位两种方式进行负样本挖掘。</p><p>&nbsp;</p><p>这两种方式都要求模型定位特定的对象类别，从而使模型能够辨别并潜在发现某些对象的缺失。不同之处在于，如何选择负样本类别。对于前者，团队采用Object365数据从给定图像中未显示的词汇中随机选择对象类，对后者则使用Flickr30k数据，并通过ChatGPT/GPT-4查找与原始类别、属性或数量最相似的实体以获取负样本，例如“男人”和“女人”、“蓝色”和“黄色”。</p><p>&nbsp;</p><p>此外，团队还进行了数据整理，以维持两种类别下正样本和负样本之间的平衡，最终共收集到95000条数据。</p><p></p><h4>大模型响应</h4><p></p><p>&nbsp;</p><p>除了通过模板转换现有数据集之外，对话指令调整数据同样在帮助多模态大模型理解人类意图，并生成流畅、自然、长格式响应方面至关重要。目前，业界广泛使用少样本提示以获取视觉指令调整数据，其中将图像的文本场景描述与人工标注对话作为少样本演示，并通过提示词要求ChatGPT/GPT-4根据新图像的文本场景生成相应的对话描述。</p><p>&nbsp;</p><p>但是，以往的指令调整数据主要集中于描述整体图像，而不会明确指定空间相关信息。为了收集引用与定位指令调整数据，团队通过以下三个步骤强调基于区域的空间知识：</p><p>&nbsp;</p><p>除了像以往那样使用对象与全局标题之外，其符号场景描述还包含对象与区域标题间的物理关系以及相应坐标。在人工标注的对话中，团队在输入/输出/二者兼具的可定位区域或对象之后添加坐标，且对话通常集中于特定区域，有助于隐式提示ChatGPT/GPT-4在生成新对话时遵循类似的模式。实际生成的对话有时无法遵循在系统提示和少样本示例中编写的规则和模式，这可能是由于大语言模型输入中的上下文太长，导致无法处理所有细节。为此，团队建议重复使用ChatGPT/GPT-4来简化最初生成的对话，其平均上下文长度仅为首轮生成数据的10%。另外，为了节约成本，团队仅在首轮生成中使用ChatGPT，而后使用GPT-4进行简写提炼，最终共收集到34000条对话。</p><p>&nbsp;</p><p></p><h4>训练过程</h4><p></p><p>&nbsp;</p><p>对于训练过程，团队使用CLIP-ViT-L/14@336p对图像编码器进行初始化，使用Vicuna对大模型进行初始化，使用LlaVA的第一阶段权重对投射层进行初始化，借此实现了视觉采样器的随机初始化。初始化完成后，Ferret在GRIT数据上接受了三个轮次（epoch）的训练，使用Loshchilov &amp; Hutter 进行优化，学习率为2e − 5，批量大小为 128。</p><p>&nbsp;</p><p>根据介绍，Ferret-13B/7B模型在8张A100上的训练分别需要约5/2.5天。在训练过程中，当输入引用区域时，团队会随机选择中心点或边界框（在可行时也会选择分割掩膜）来表示各区域，并对训练数据进行了重复数据删除，借此清理下游评估中的样本。</p><p>&nbsp;</p><p>为了评估这项新功能，团队引入了Ferret-Bench，其涵盖三种新型任务：引用描述/引用推理和对话内定位。团队表示，通过对现有多模态大模型进行了基准测试，发现Ferret的平均性能较最出色的原有大模型高20.4%，而且在物体识别的幻觉方面也有所减轻。</p><p>&nbsp;</p><p>概括来讲，Ferret项目论文的贡献主要为以下三个方面：</p><p>&nbsp;</p><p>提出了Ferret模型，其采用基于新型空间感知视觉采样器的混合区域表示方法，可在多模态大模型中实现细粒度和开放词汇的引用和定位功能。建立起GRIT，一套大规模定位与引用指令调整数据集，既可用于模型训练，还包含额外的空间负样本以增强模型健壮性。引入了Ferret-Bench来评估涉及引用/定位、语义、知识和推理的联合任务。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>很明显，苹果正在努力追赶这次AIGC浪潮。据报道，苹果每天在人工智能上投资数百万美元，内部有多个团队开发多种人工智能模型。</p><p>&nbsp;</p><p>根据报道，苹果致力于对话式人工智能的部门被称为“Foundational Models”，“大约 16 名”成员，其中包括几名前谷歌工程师。该部门由 Apple 人工智能主管 John Giannandrea 掌舵，他于 2018 年受聘帮助改进 Siri。</p><p>&nbsp;</p><p>苹果正在开发自己的大模型“Ajax”。Ajax 旨在与 OpenAI 的 GPT-3 和 GPT-4 等产品相媲美，可运行 2000 亿个参数。Ajax 在内部被称为“Apple GPT”，旨在统一整个 Apple 的机器学习开发，提出了将人工智能更深入地集成到 Apple 生态系统中的更广泛战略。</p><p>&nbsp;</p><p>截至最新报告，Ajax 被认为比上一代 ChatGPT 3.5 更强大。然而，也有人认为，截至 2023 年 9 月，OpenAI 的新模型可能已经超越了 Ajax 的能力​。</p><p>&nbsp;</p><p>近日，苹果的机器学习研究团队还悄悄发布了一个名为 MLX 的框架来构建基础模型。彭博社报道称，苹果正在开发 Siri 的改进版本，并计划在下一个重大 iOS 版本中提供以人工智能为中心的功能。</p><p>&nbsp;</p><p>另外，苹果还正在与一些大型新闻出版商洽谈授权其新闻档案，并利用这些信息来训练模型。《纽约时报》称，该公司正在讨论“价值至少 5000 万美元的多年期交易”&nbsp;，并已与 Condé Nast、NBC News 和 IAC 等出版商保持联系。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href=\"https://arxiv.org/pdf/2310.07704.pdf\">https://arxiv.org/pdf/2310.07704.pdf</a>\"</p><p><a href=\"https://www.macrumors.com/2023/12/21/apple-ai-researchers-run-llms-iphones/\">https://www.macrumors.com/2023/12/21/apple-ai-researchers-run-llms-iphones/</a>\"</p><p><a href=\"https://www.theverge.com/2023/12/22/24012730/apple-ai-models-news-publishers\">https://www.theverge.com/2023/12/22/24012730/apple-ai-models-news-publishers</a>\"</p>",
    "publish_time": "2023-12-25 14:44:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]