[
  {
    "title": "Git 2.40发布，包括git jump工具的更新、cat-file工具的增强以及提高Windows上响应速度",
    "url": "https://www.infoq.cn/article/tQjr79KiqeK88KgjhUJY",
    "summary": "<p>最近，开源Git项目发布了最新的2.40版本，带来了一些新的特性和缺陷修复。这个版本值得关注的特性包括git jump工具的更新、cat-file工具的增强以及在Windows上更快的响应。</p><p></p><p>GitHub的软件工程师<a href=\"https://www.linkedin.com/in/ttaylorr/\">Taylor Blau</a>\"详细介绍了Git 2.40的<a href=\"https://github.blog/2023-03-13-highlights-from-git-2-40/\">更新细节</a>\"。git jump是<a href=\"https://github.com/git/git/tree/v2.40.0/contrib\">contrib</a>\"目录中的一个可选工具，现在支持Emacs和Vim。git jump工具对Git命令进行了封装，比如git grep，并将它们的结果输入的Vim的<a href=\"https://vimdoc.sourceforge.net/htmldoc/quickfix.html\">quickfix</a>\"列表中。</p><p></p><p>如果你使用Emacs的话，git jump可以通过M-x grepgit jump --stdout grep foo命令来生成一个位置列表。该命令将会输出项目中所有与“foo”匹配的内容，使其更容易进行导航。Git jump也适用于diff和merge。</p><p></p><p>Git的cat-file工具常用于打印Git仓库中任意对象的内容。从Git 2.38.0版本开始，cat-file工具支持在打印提交的内容时，使用Git的<a href=\"https://git-scm.com/docs/gitmailmap\">mailmap</a>\"规则。为了确定某个特定对象的大小，cat-file工具可以使用--batch-check和-s选项。但是，以前版本的Git在使用cat-file工具时，如果在使用--use-mailmap选项的同时，组合使用了--batch-check和-s选项，将会导致结果不正确。在Git 2.40中，这个问题得到了修正，现在--batch-check和-s选项会正确报告对象的大小。</p><p></p><p>git check-attr命令能够用来确定给定的路径设置了哪些gitattributes。这些属性是由仓库中的一个或多个.gitattributes文件所定义和设置的。对于复杂的规则或多个.gitattributes文件，请按照如下形式使用check-attr&nbsp;git命令：</p><p><code lang=\"shell\">$ git check-attr -a git.c git.c: diff: cpp git.c: whitespace: indent,trail,space</code></p><p></p><p>在此之前，check-attr需要有一个索引，这使得它在<a href=\"https://git-scm.com/docs/gitglossary/2.40.0#Documentation/gitglossary.txt-aiddefbarerepositoryabarerepository\">裸仓库</a>\"中很难使用。裸仓库是一个以.git为后缀的目录，它没有任何版本控制文件的本地检出副本（在裸仓库中，所有通常存在于隐藏的“.git”子目录中的Git管理和控制文件都直接存在于“repository.git”目录中，而没有其他文件存在并被检出。——译者注）。在Git 2.40和更新的版本中，现在支持使用--source=选项来扫描.gitattributes，使其更易于在裸仓库中使用。</p><p></p><p><a href=\"https://twitter.com/github/status/1635339931949346816?s=61&amp;t=CIFud503wZ5wc6mlfiwwsw\">GitHub的推特账号</a>\"发布了这些更新，这引起了推特上技术社区的关注。其中，有位推特用户<a href=\"https://twitter.com/andrewdimola\">Andrew</a>\"转推了该公告，并这样<a href=\"https://twitter.com/andrewdimola/status/1635340909499027456\">引用到</a>\"：</p><p>“git jump 似乎相当有用！&nbsp;<a href=\"https://github.com/git/git/tree/v2.19.0/contrib/git-jump\">https://github.com/git/git/tree/v2.19.0/contrib/git-jump</a>\"。”</p><p></p><p>Git 2.40还包括一些功能增强，将Git旧的组成部分从Perl或Shell改写成现代C语言的等价方式。这使得Git命令在Windows等平台上运行得更快。现在，git bisect已经以原生内置程序的方式以C语言进行了实现，而传统实现git add --interactive已经废弃了。</p><p></p><p>在2.40版本中，Git的CI基础设施也有一些改进。一些长期运行的Windows专有CI构建已被禁用，为Git开发人员带来了更快、更节省资源的CI运行方案。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/04/git-releases-version-2-40/\">Open Source Git Project Releases Version 2.40</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://xie.infoq.cn/article/999f8a62e67979b75776a22e6\">20 个&nbsp;Git&nbsp;命令玩转版本控制</a>\"</p><p><a href=\"https://xie.infoq.cn/article/d5e76833564f3fd1f4c1577a7\">软件开发入门教程网之&nbsp;Git&nbsp;基本操作</a>\"</p><p><a href=\"https://xie.infoq.cn/article/5b2d6b8e8f5234a856d3e056e\">软件测试丨让工作更高效，搞定&nbsp;Git&nbsp;的分支管理</a>\"</p>",
    "publish_time": "2023-04-18 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "苹果现已支持 Swift 包索引",
    "url": "https://www.infoq.cn/article/AHUamwkJ3fqljoliPKHa",
    "summary": "<p>Dave Verwer 和 Sven A. Schmidt 于三年前（2019年）创建了 <a href=\"https://swiftpackageindex.com/\">Swift 包索引</a>\"，目的为让 Swift 开发者能够轻松地搜索发现 Swift 包。目前该项目已正式获得<a href=\"https://www.swift.org/blog/swift-package-index-developer-spotlight/\">苹果的赞助</a>\"，使其成为 Swift 包相关事宜的官方网站。</p><p>&nbsp;</p><p>顾名思义，Swift 包索引并不是完整的软件包仓库，而是专注于为软件包元数据编制索引，为开发者们在决定项目要使用的软件包时提供其完善的信息。</p><p>&nbsp;</p><p></p><blockquote>其所能回答的问题包括：某个软件包已经开发了多长时间、作者授权代码的方式、拉取请求和问题是否有人关注和回应等等。乍看之下，索引中软件包的页面可能与其 GitHub 页面相差无几，但我们将元数据的关注点放在了软件包的潜在用户相关方面。</blockquote><p></p><p>&nbsp;</p><p>目前，该索引所包含的<a href=\"https://github.com/SwiftPackageIndex/PackageList/blob/main/packages.json\">超五千个软件包</a>\"几乎所有都是托管在 GitHub 上。每个软件包都是克隆的，有不同 Swift 版本和平台兼容性构建的评估，并收集元数据以确认其基本信息。</p><p>&nbsp;</p><p></p><blockquote>我们目前所说的“构建系统”，平均每天处理五千个构建，共计五百余万次构建。这一操作是如此地庞大，以至于我们需要为其定制一个监控应用程序。</blockquote><p></p><p>&nbsp;</p><p>每个软件包的构建步骤结果，都总结在了一个兼容性矩阵中。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/46af70cae263b5a90b66266fb7c9801b.png\" /></p><p></p><p>&nbsp;</p><p>近期，包索引已开始向社区提供一个软件包文档的托管平台，文档对人们是否决定使用某个依赖关系而言往往是至关重要的。</p><p>&nbsp;</p><p></p><blockquote>任何软件包作者现在都可选择是否使用文档的生成，只要构建系统能够完成一次成功的构建，我们便会托管该版本号的 DocC 文档。</blockquote><p></p><p>&nbsp;</p><p>苹果推出 <a href=\"https://www.swift.org/package-manager/\">Swift 包管理</a>\"作为 Swift 代码分发的官方工具，为已经失去功效的服务开辟了一条新赛道，其中就包含由 IBM 适时推出但未能获得开发者们足够重视的“IBM Swift 包分类”服务。IBM Swift 包分类先是被 <a href=\"https://github.com/vapor-community/PackageCatalogAPI\">Vapor 社区包分类 API</a>\"所取代，随后又被 <a href=\"https://swiftpackageregistry.com/about\">Swift 包注册</a>\"取代，后者与刚刚获得苹果认可的 Swift 包索引并不是一回事。</p><p>&nbsp;</p><p>与其他软件包仓库，如 CocoaPods 或 Carthage 相比，Swift 包管理的优势在于其与 Swift 构建系统的集成和在 Xcode 之中的紧密集成，因此，后者的使用对开发者而言几乎是完全透明的。不过，<a href=\"https://cocoapods.org/\">CocoaPods</a>\" 和&nbsp;<a href=\"https://github.com/Carthage/Carthage\">Carthage</a>\" 除了 Swift 包之外，也支持 Objective-C 包。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/apple-swift-package-index/\">The Swift Package Index is now Backed by Apple</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/LVg8YPxsNvl2u8wuxWlD\">从探索到落地，手淘引入&nbsp;Swift“历险记”</a>\"</p><p><a href=\"https://www.infoq.cn/article/SH3KGISMFcHz0gZRVAaH\">如何用&nbsp;Swift&nbsp;重写 C++/ObjC 代码库，并将其缩减 70%</a>\"</p>",
    "publish_time": "2023-04-18 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "钉钉总裁叶军：未来一年钉钉所有场景都要进行智能化布局",
    "url": "https://www.infoq.cn/article/S5MTdft9O1KI2sdSfQ4J",
    "summary": "<p>4&nbsp;月&nbsp;18&nbsp;日，在2023&nbsp;春季钉峰会上，钉钉发了一条斜杠“/”，并现场演示接入千问大模型后，通过输入“/”在钉钉唤起10余项AI能力。</p><p></p><p>钉钉总裁叶军宣布，新钉钉将全面智能化，未来一年所有场景都将进行智能化布局。“这只是我们工作智能化探索的一小步，AI对生产力工具的改造才刚刚开始”，他说。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/36cb5540f7cbf4f6dda3da9152f5ddd8.jpeg\" /></p><p></p><p>4&nbsp;月&nbsp;11&nbsp;日，阿里巴巴集团董事会主席兼CEO、阿里云智能集团CEO张勇在云峰会上表示，阿里巴巴所有产品将接入千问大模型，进行全面改造。他认为，AI时代所有产品都值得用大模型重新做一遍。</p><p></p><p>在千问大模型面世一周后，钉钉确认接入千问。目前，钉钉与大模型融合场景正在测试中，将在相关安全评估完成后上线。叶军现场演示的四个场景为：群聊、文档、视频会议及应用开发。</p><p></p><p>在群聊中，新入群者无需爬楼，在对话框输入钉钉斜杠“/”即可自动整理群聊要点，快速了解上下文，并生成待办、预约日程。还可以用“/”在群聊中创作文案、表情包等。</p><p></p><p>在聊天中，用户也可以训练专属的助理机器人。用“/”创建机器人后，只需要发送文档、网页或者知识库的一条链接，就可以让机器人自动学习其中内容，并可生成对话问答，不用再手动设置问题和答案。</p><p></p><p>在钉钉文档中，“/”可以是用户的创意助理，帮助写文案、生成海报。在视频会议中，“/”也是会议助理，能一键生成讨论要点、会议结论、待办事项等。</p><p></p><p>最令人惊艳的是“/”还可用自然语言或拍照生成应用，并以钉钉酷应用的形式在群聊内使用。比如，公司行政人员需要统计午餐的订餐份数，只需要在群聊对话框中输入“/”和需求，几秒钟后一个订餐统计小程序就会展现在群聊中。</p><p></p><p>叶军认为，未来人和软件的交互将变得非常简单，产品会走向极简，甚至可能就是通过一条斜杠来唤起所有功能。每家企业都需要思考人与科技、机器的关系，坚定以人为本，科技服务于人，让人从常规、低效的工作中解放出来，做更有创造力的事情。</p><p></p><p>“除了这四个高频场景，我们还在同步测试钉钉个人版、搜索、邮箱、AI助理、智能客服等产品，让客户和生态基于钉钉PaaS底座的能力，更好地对接大模型”，叶军说：“AI&nbsp;时代，数据资产的价值将更加重要，每个组织的数据密度会快速增加。未来，每一家企业既可以直接在阿里云上调用千问的能力，也可以借助钉钉这个工作入口，结合企业自己的行业知识和应用场景，训练自己的专属大模型。可以畅想，每家企业都可以有专属的、深刻理解自身企业业务知识的智能客服、智能导购、文案助手、AI设计师等。”</p>",
    "publish_time": "2023-04-18 12:09:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "多云缓存在知乎的探索：从 UnionStore 到 Alluxio",
    "url": "https://www.infoq.cn/article/FpQsyrpxtiol2sFvk9cF",
    "summary": "<p></p><h2>1 背景</h2><p></p><p>随着云原生技术的飞速发展，各大公有云厂商提供的云服务也变得越来越标准、可靠和易用。凭借着云原生技术，用户不仅可以在不同的云上低成本部署自己的业务，而且还可以享受到每一个云厂商在特定技术领域上的优势服务，因此多云架构备受青睐。</p><p></p><p>知乎目前采用了多云架构，主要是基于以下考虑：</p><p></p><p>服务多活：将同一个服务部署到不同的数据中心，防止单一数据中心因不可抗力不能正常提供服务，导致业务被“一锅端”；</p><p></p><p>容量扩展：一般而言，在公司的服务器规模达到万台时，单一数据中心就很难支撑业务后续的扩容需求了；</p><p></p><p>降本增效：对于同一服务，不同云厂商对同一服务的定价和运维的能力也不尽相同，我们期望能够达到比较理想的状态，在云服务满足我们需求的前提下，尽量享受到低廉的价格。</p><p></p><p>知乎目前有多个数据中心，主要的机房有以下两个：</p><p></p><p>在线机房：主要是部署知乎主站上直接面向用户的服务（如评论、回答等），这部分服务对时延敏感；</p><p></p><p>离线机房：主要是部署一些离线存储，计算相关的服务，对时延不敏感，但是对吞吐要求高。</p><p></p><p>两个数据中心之间通过专线连接，许多重要服务都依赖于专线进行跨机房调用，所以维持专线的稳定十分重要。专线流量是衡量专线是否稳定的重要指标之一，如果专线流量达到专线的额定带宽，就会导致跨专线服务之间的调用出现大量的超时或失败。</p><p></p><p>一般而言，服务的吞吐都不会特别高，还远远达不到专线带宽的流量上限，甚至连专线带宽的一半都达不到，但是在我们的算法场景中有一些比较特殊的情况：算法模型的训练在离线机房，依赖 HDFS 上的海量数据集，以及 Spark 集群和机器学习平台进行大规模分布式训练，训练的模型结果存储在 HDFS 上，一个模型甚至能达到数十上百 GB；在模型上线时，算法服务会从在线机房跨专线读取离线 HDFS 上的模型文件，而算法服务一般有数十上百个容器，这些容器在并发读取 HDFS 上的文件时，很轻易就能将专线带宽打满，从而影响其他跨专线服务。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/7c/09/7cbf4d80bf5605a368ed699871c64509.svg\" /></p><p></p><p></p><h2>2 多 HDFS 集群</h2><p></p><p>在早期，我们解决算法模型跨机房读取的方式非常简单粗暴，部署一套新的 HDFS 集群到在线机房供算法业务使用，业务使用模型的流程如下：</p><p></p><p>产出模型：模型由 Spark 集群或机器学习平台训练产出，存储到离线 HDFS 集群；拷贝模型：模型产出后，由离线调度任务定时拷贝需要上线的模型至在线 HDFS 集群；读取模型：算法容器从在线 HDFS 集群读取模型上线。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/34/35/34d736b9baa85a8a9937b33b16b36035.svg\" /></p><p></p><p>多 HDFS 集群的架构虽然解决了专线流量的问题，但是依然存在一些问题：</p><p></p><p>多个 HDFS 集群不便于维护，增加运维人员负担；拷贝脚本需要业务自己实现，每次新上线模型时，都要同步修改拷贝脚本，不便维护；在线 HDFS 集群的文件需要业务定期手动删除以降低成本，操作风险高；在线 HDFS 与离线 HDFS 之间文件视图不一致，用户在使用 HDFS 时，需要明确知道自己使用的是哪个 HDFS，需要保存多个地址，心智负担高；在超高并发读取时，比如算法一次性起上百个容器来读取某个模型文件时，会导致 DataNode 负载过高，虽然可以通过增加副本解决，但是也会带来较高的存储成本。</p><p></p><p>基于以上痛点，我们自研了多云缓存服务—UnionStore。</p><p></p><h2>3 自研组件 UnionStore</h2><p></p><p></p><h3>3.1 简介</h3><p></p><p></p><p>UnionStore 顾名思义，就是联合存储的意思，它提供了标准的 S3 协议来访问 HDFS 上的数据，并且以对象存储来作为跨机房缓存。UnionStore 目前在知乎有两种使用场景：</p><p></p><p>模型上线场景：部署到在线机房，作为跨机房缓存使用：</p><p></p><p>用户在向 UnionStore 请求读取文件时，会先检查文件是否已经上传到对象存储上：</p><p></p><p>如果对象存储已经存在该文件，则直接从对象存储读取文件返回给用户；如果对象存储不存在该文件，UnionStore 会先将离线 HDFS 上的文件上传到在线机房的对象存储上，再从对象存储上读取文件，返回给用户，缓存期间用户的请求是被 block 住的。这里相当于是利用对象存储做了一层跨机房缓存。</p><p></p><p>模型训练场景：部署到离线机房，作为 HDFS 代理使用，目的是为业务提供 S3 协议的 HDFS 访问方式，通过&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/s3fs-fuse/s3fs-fuse\">s3fs-fuse</a>\"，业务就能挂载 HDFS 到本地目录，读取训练数据进行模型的训练。</p><p></p><p>模型训练场景是我们 UnionStore 上线后的扩展场景，之前我们尝试过很多 HDFS 挂载 POSIX 的方式，但是效果都不太理想，主要体现在重试方面，而 UnionStore 正好提供了 S3 协议，s3fs-fuse 重试做的不错，所以我们最后选择了 UnionStore + s3fs-fuse 对 HDFS 进行本地目录的挂载。</p><p></p><p>其工作流程如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0d/0d/0db5a9e97dc217d315e12996fbbd0b0d.svg\" /></p><p></p><p>相比于之前多 HDFS 集群方案，UnionStore 的优势如下：</p><p></p><p>UnionStore 提供了 S3 协议，各编程语言对 S3 协议的支持要比 HDFS 协议好，工具也相对来说也更丰富；UnionStore 会自动缓存文件，无需用户手动拷贝模型，省去了拷贝脚本的开发与维护；提供统一的文件视图，因为元数据是实时请求 HDFS 的，所以文件视图与 HDFS 强一致；下线了一个 HDFS 集群，文件储存能力由对象存储提供，节省了大量的服务器成本；文件过期可依赖对象存储本身提供的能力，无需自己实现；UnionStore 以云原生的方式提供服务，部署在 k8s 上，每一个容器都是无状态节点，可以很轻易的扩缩容，在高并发的场景下，由于存储能力转移到对象存储，在对象存储性能足够的情况下，不会遇到类似 DataNode 负载过高的问题。</p><p></p><h3>3.2 实现细节</h3><p></p><p></p><p>UnionStore 的完整架构图如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/73/2c/731d6e04a5eed2b5e72b3e0823db502c.svg\" /></p><p></p><p>在使用对象存储作为缓存时，UnionStore 有三个核心组件：</p><p></p><p>UnionStore Server：无状态节点，每一个节点都能单独提供服务，一般会部署多个，用于分摊流量；</p><p></p><p>Object Storage：对象存储，用于缓存 HDFS 上的数据，一般是在哪个云厂商就使用对应云厂商提供的对象存储，流量费用几乎可忽略；</p><p></p><p>Task Manager：任务管理器，用于存储缓存任务，可用 MySQL 和 Redis 实现。</p><p>基于这三个组件我们在 UnionStore 上实现了一系列有用的功能。</p><p></p><p>文件校验：文件被缓存至对象存储后，如果 HDFS 上的文件做了修改，UnionStore 需要检查到文件的变更，确保用户不会读取到错误的文件。这里我们在将 HDFS 文件上传至对象存储时，会将 HDFS 文件的大小，最后修改时间，checksum 等元信息存储到对象存储文件的 UserMetadata 上，用户在读取文件时，会检查这部分的信息，只有当信息校验通过时，才会返回对象存储上的文件，如果校验未通过，则会重新缓存这个文件，更新对象存储上的缓存。</p><p></p><p>读写加速：对象存储的单线程读写速度大约在 30-60MB/sec，远远小于 HDFS 的吞吐，如果不做特殊处理，是很难满足业务的读写需求的。在读方面，我们利用对象存储的 RangeRead 接口，多线程读取对象存储上的数据返回给用户，达到了与 HDFS 相同的读取速度。在写方面，我们利用对象存储的 MultiPartUpload 接口，多线程上传 HDFS 上的文件，也能达到与 HDFS 相同的写入速度。</p><p></p><p>文件仅缓存一次：因为 UnionStore Server 被设计成了无状态节点，所以它们之间是无法互相感知的。如果有多个请求同时打到不同的 Server 节点上来请求未缓存的文件，这个文件可能会被不同的 Server 多次缓存，对专线造成较大的压力。我们引入了 Task Manager 这个组件来解决这个问题：</p><p></p><p>Server 节点在接受到读取未缓存文件的请求时，会先将用户的请求异步卡住，生成缓存任务，提交到 Task Manager 的等待队列中；所有 Server 节点会不断竞争等待队列里的任务，只会有一个节点竞争成功，此时该节点会将缓存任务放入运行队列，开始执行，执行期间向任务队列汇报心跳；每个 Server 节点会定期检查自己卡住的用户请求，来检查 Task Manager 里对应的任务，如果任务执行成功，就会唤醒用户请求，返回给用户缓存后的文件；同时，每个 Server 都会定期检查 Task Manager 里正在运行的任务，如果任务长时间没有更新心跳，则会将任务从运行队列里取出，重新放回等待队列，再次执行。</p><p></p><p>这里所有的状态变更操作都发生在 Server 节点，Task Manager 只负责存储任务信息以及提供队列的原子操作。</p><p></p><h3>3.3 局限</h3><p></p><p></p><p>UnionStore 项目在知乎运行了两年，早期并没有出现任何问题，但是随着算法业务规模的不断扩大，出现了以下问题：</p><p></p><p>没有元数据缓存，元数据强依赖 HDFS，在 HDFS 抖动的时候，有些需要频繁更新的模型文件会受影响，无法更新，在线服务不应强依赖离线 HDFS；读写加速因为用到了多线程技术，对 CPU 的消耗比较大，在早期业务量不大的时候，UnionStore 只需要几百 Core 就能支撑整个公司的算法团队读取数据，但是随着业务量不断上涨，需要的 CPU 数也涨到了上千；对象存储能力有上限，单文件上千并发读取时，也会面临性能瓶颈；UnionStore 只做到了缓存，而没有做到高性能缓存，业务方的大模型往往需要读取十多分钟，极大影响模型的更新速度，制约业务的发展；无法做到边缓存边返回文件，导致第一次读取文件的时间过长。</p><p></p><p>另外还有一个关键点，机器学习平台为保证多活，也采用了多云架构，支持了多机房部署，在读取训练数据时，走的是 UnionStore 对 HDFS 的直接代理，没走缓存流程，因为训练数据大部分都是小文件，而且数量特别巨大，小文件都过一遍缓存会导致缓存任务在任务队列里排队时间过长，很难保证读取的时效性，因此我们直接代理了 HDFS。按照这种使用方式，专线带宽在训练数据规模扩大时，依然会成为瓶颈。</p><p><img src=\"https://static001.infoq.cn/resource/image/ec/10/ec429dd50075313cbe3260207b28c610.svg\" /></p><p></p><p>以上痛点使我们面临两个选择：一是继续迭代 UnionStore，让 UnionStore 具备高性能缓存能力，比如支持本地 SSD 以及内存缓存；二是寻找合适的开源解决方案，完美替代 UnionStore 的使用场景。基于人力资源的宝贵，我们选择了其二。</p><p></p><h2>4 利用 Alluxio 替代 UnionStore</h2><p></p><p></p><h3>4.1 调研</h3><p></p><p></p><p>我们调研了业内主流的文件系统，发现 Alluxio 比较适合我们的场景，原因有以下几点：</p><p></p><p>透明缓存：相较于其他文件系统，Alluxio 可仅作为缓存使用，用于编排数据，业务方无需将模型文件写入到其他的文件系统，只需要维持现状，写入 HDFS 即可；元数据与数据缓存：Alluxio 支持自定义缓存元数据与数据，这样在读取已缓存文件时，可完全不受 HDFS 影响；目前我们 UnionStore 的 QPS 大约在 20K-30K，缓存元数据可极大降低 NameNode 的压力，反哺离线场景；丰富的 UFS 支持：支持除 HDFS 外的多种 UFS，比如对象存储，对我们的数据湖场景也提供了强有力的支撑；即席查询加速：知乎 Adhoc 引擎采用的是 Spark 与 Presto，Alluxio 对这两个引擎都有较好的支持；访问接口丰富：Alluxio 提供的 S3 Proxy 组件完全兼容 S3 协议，我们的模型上线场景从 UnionStore 迁移至 Alluxio 付出的成本几乎可忽略不计；另外 Alluxio 提供的 Alluxio fuse 具备本地元数据缓存与数据缓存，比业务之前使用的 S3 fuse 具有更好的性能，正好能满足我们的模型训练场景。社区活跃：Alluxio 社区十分活跃，在我们调研期间交流群基本上都会有热心的网友及时答复， issue 很少有超过半天不回复的情况。</p><p></p><p>对 Alluxio 的调研让我们非常惊喜，它不仅满足了我们的需求，还给我们“额外赠送”了不少附加功能。</p><p>我们在内部对 Alluxio 进行了测试，以 100G 的文件做单线程读取测试，多次测试取平均值，结果如下：</p><p></p><p></p><p></p><p>其中 HDFS 因为涉及到 OS 层面的缓存，波动是最大的，从 200MB/sec - 500MB/sec 都有，而 UnionStore 与 Alluxio 在命中缓存时表现十分稳定。</p><p></p><h3>4.2 集群规划</h3><p></p><p></p><p>Alluxio 在我们的规划中是每个机房部署一套，利用高性能 NVME 磁盘对 HDFS 和对象存储上的数据进行缓存，为业务提供海量数据的加速服务。</p><p></p><p>依据业务的使用场景，我们将 Alluxio 集群分为两类。</p><p></p><p>模型上线加速集群：Alluxio 集群缓存模型本身，利用 S3 Proxy 对外提供只读服务，加速模型的上线；</p><p></p><p>模型训练加速集群：Alluxio 集群缓存模型训练数据，利用 Alluxio fuse 对 HDFS 上数据与元数据再做本地缓存，加速模型的训练；产出的模型直接通过 Alluxio fuse 写入 HDFS 进行持久化存储。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/59/be/59c94fb0cf50f30d65f555a9e91e07be.svg\" /></p><p></p><p></p><h3>4.3 模型上线场景适配</h3><p></p><p></p><h3>4.3.1 场景特点</h3><p></p><p>我们的模型上线场景有以下特点：</p><p></p><p>用户利用 S3 协议读取模型文件；用户将模型数据写入到 HDFS 上后，需要立即读取，数据产出与读取的间隔在秒级，几乎无法提前预热，存在缓存穿透的问题；一份模型文件将由上百甚至上千个容器同时读取，流量放大明显，最大的单个模型读取时，峰值流量甚至能达到 1Tb/sec；模型文件只会在短时间内使用，高并发读取完毕后可视为过期；数万容器分散在上千个 K8s 节点上，单个容器可用资源量较少。</p><p></p><p>针对模型上线场景，我们选择了 S3 Proxy 来为业务提供缓存服务，不使用 Alluxio Client 以及 Alluxio fuse 主要是基于以下考虑：</p><p></p><p>用户原本就是利用 S3 协议读取文件，换成 S3 Proxy 几乎无成本；业务方使用的语言有 Python，Golang，Java 三种，Alluxio Client 是基于 Java 实现的，其他语言使用起来比较麻烦；受限于单个容器的资源限制，不适合在容器内利用 CSI 等方式启动 Alluxio fuse，因为 fuse 的性能比较依赖磁盘和内存的缓存。</p><p></p><h3>4.3.2 集群部署</h3><p></p><p></p><p>首先是集群的部署方式，在这个场景下，我们的 Alluxio 集群采取了“大集群轻客户端”的方式来部署，也就是提供足够数量的 Worker 与 S3 Proxy 来支撑业务以 S3 协议发起的高并发请求，架构图如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/00/4a/00a6812e19bcba289761fefb21f80b4a.svg\" /></p><p></p><p>我们的集群版本是 2.9.2，在这个版本，S3 Proxy 有 v1 v2 两种实现，可通过配置&nbsp;alluxio.proxy.s3.v2.version.enabled&nbsp;进行切换。v2 版本有一个很重要的功能，就是将 IO 操作与元数据操作进行了分类，分别交给不同的线程池去处理。这样做的好处是，让元数据操作能够快速执行，不被 IO 线程卡住，因为一般情况下，元数据请求的 QPS 远远大于读写文件的 QPS。这个功能对我们非常有用，我们 UnionStore 的 QPS 在 25K 左右，其中 90% 的操作都是元数据访问。</p><p></p><p>整个 Alluxio 集群我们采取了裸金属机部署，Alluxio 也提供了 k8s 的部署方式，但是在我们的权衡之下，还是选择了裸金属机部署，原因如下：</p><p></p><p>从我们的测试结果来看，Alluxio Worker 在”火力全开“的情况下是可以轻易打满双万兆网卡的，这个时候网卡是瓶颈；如果选择 k8s 部署，当有容器与 Alluxio Worker 调度到同一台 k8s 的节点时，该容器容易受到 Alluxio Worker 的影响，无法抢占到足够的网卡资源；Alluxio Worker 依赖高性能磁盘做本地缓存，与其他服务混布容易收到其他进程的磁盘 IO 影响，无法达到最佳性能；因为 Alluxio Worker 强依赖网卡，磁盘等物理资源，这些资源不适合与其他服务共享。强行以 k8s 部署，可能就是一个 k8s 节点启一个 Alluxio Worker 的 DaemonSet，这其实也没必要用 k8s 部署，因为基于我们过往的经验，容器内搞存储，可能会遇到各类奇奇怪怪的问题，这些问题解决起来比较浪费时间，影响正常的上线进度。</p><p></p><p>我们除了按照社区文档的推荐将 Master 与 Job Master，Worker 与 Job Worker 部署到同一台机器上，还另外将 S3 Proxy 与 Worker 进行了混布。S3 Proxy 在用户看起来虽然是服务端，但是对 Alluxio 集群来说它还是客户端，而 Alluxio 对于客户端有一个非常重要的优化：</p><p></p><p>当 Client 与 Worker 在同一节点时，就可以使用短路读的功能，在短路读开启的情况下，Client 将不再利用网络请求调用 Worker 上的 RPC 接口读取数据，而是直接读本地磁盘上的数据，能够极大节省网卡资源。通过 S3 Porxy 访问 Alluxio 时，流量主要分为以下几个部分：</p><p></p><p>文件未缓存至 Alluxio：Worker 从 UFS 读取数据，任一 Worker 只要缓存了 UFS 的文件，这部分流量将不存在；文件在远端 Worker 缓存：本地 Worker 从其他 Worker 读取数据缓存到本地，S3 Proxy 暂时从远端 Worker 读取，本地 Worker 缓存完毕后这部分流量将不存在；文件在本地 Worker 缓存：S3 Proxy 从本地 Worker 读取的流量，这部分流量在开启短路读后将不存在；业务方从 S3 Proxy 读取的流量，这部分流量无法避免。</p><p></p><p>其中 1，2 中的流量远小于 3，4 中的流量，短路读能够将 3 的流量省下，节省约 30%-50% 的流量。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/4f/ee/4fd59b1e4d08aa104ae23355b51dfbee.svg\" /></p><p></p><p>其次是集群的部署规模，在模型读取这个场景，尽管每天的读取总量可达数 PB，但是因为模型文件很快就会过期，所以 Worker 的容量并不需要很大，Worker 网卡的总带宽能够支持读取流量即可。Worker 的数量可按照&nbsp;流量峰值/(2/3*网卡带宽)&nbsp;来计算，这里网卡需要预留 1/3 的 buffer 来供 Worker 读取 UFS 以及 Worker 互相同步数据使用。</p><p></p><p>最后是 Alluxio Master 的 HA 方式，我们选择了 Raft，在我们的测试过程中，在上亿的元数据以及数百 GB 堆的情况下，Master 主从切换基本上在 10 秒以内完成，效率极高，业务近乎无感。</p><p></p><h3>4.3.3 上线与调优</h3><p></p><p></p><p>我们的上线过程也是我们调优的一个过程。</p><p></p><p>在初期，我们只将一个小模型的读取请求从 UnionStore 切换到了 Alluxio S3 Proxy，效果如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b8/fd/b80a77e085b204121be4f4cb49b2abfd.png\" /></p><p></p><p>里面的每一条线段都代表着一个模型的读取请求，线段的长短代表读取数据的花费的时间。</p><p></p><p>其中阶段一是我们内部的 UnionStore 服务，阶段二是我们直接切换到 S3 Proxy 时的状态，可以很明显的看到换成 S3 Proxy 了以后，模型读取的平均速度有所上升，但是出现了尖刺，也就是偶尔有请求读取的很慢。问题出在模型读取时，总是冷读，也就是模型数据没有经过预热，在文件未预热的情况下，从 Alluxio 读数据最多只能达到与 HDFS 相同的速度，不能充分发挥缓存的能力。而且通过测试，我们发现 Alluxio 在并发请求同一个没有经过预热的文件时，性能会下降的十分严重，甚至达不到直接读 HDFS 的速度。因此我们需要想办法预热文件。</p><p></p><p>预热文件的手段一般有以下两种：</p><p></p><p>用户在写完文件后，手动调用 Alluxio load 命令，提前将数据缓存，确保在读取的时候，需要的文件已经被缓存了；根据 HDFS 的 audit log 或者利用 HDFS 的 inotify 来订阅文件的变更，只要发现算法目录下有文件变动就加载缓存进 Alluxio。</p><p></p><p>方式 1 的问题在于需要用户深度参与，有额外的心智负担和开发成本，其次是用户调用 load 命令不可控，如果对一个超大目录进行 load，将会使所有缓存失效。</p><p></p><p>方式 2 也需要用户提供监听的路径，如果路径是文件比较方便，只需要监听 close 请求即可，但是路径是目录的情况下，涉及到临时文件，rename 等，十分复杂；每次用户新增模型时，都需要我们把路径新加入监控，有额外的沟通成本；另外由于我们这个场景，数据产出与读取的间隔在秒级，监控文件变更链路太长，可能出现一些延迟，从而导致预热方案失效。</p><p></p><p>基于以上缺点，我们自己设计了一套缓存策略：</p><p></p><p>冷读文件慢的本质在于通过 Alluxio 读取未缓存文件时，读到哪一个 block 才会去缓存这个 block，没有做到并发缓存 block。因此我们在 S3 Proxy 上添加了一个逻辑，在读取文件时，会将文件按 block 进行分段生成 cache block 任务，平均提交到每一个 Worker 来异步缓存。这样的好处是，客户端在读取前面少量几个未缓存的 block 后，后面的 block 都是已经缓存完毕的，读取速度十分快。此外，由于提前缓存了 block，缓存穿透的问题也能有所缓解，HDFS 流量能够下降 2 倍以上。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e6/77/e6a502a8c90a85bbb3e0ae85c1ab4f77.svg\" /></p><p></p><p>此缓存策略需要注意以下几点：</p><p></p><p>缓存 block 需要异步，并且所有的异常都要处理掉，不要影响正常的读取请求；缓存 block 时，最好将 block id 与 Worker id 以某种方式（如 hash）进行绑定，这样能保证在对同一个文件进行并发请求时，对某一个 block 的缓存请求都只打到同一个 Worker 上，避免不同的 Worker 从 UFS 读取同一个 block，放大 UFS 流量；S3 Proxy 需要对提交的 cache block 任务计数，避免提交过多任务影响 Worker 正常的缓存逻辑，最好不要超过配置&nbsp;alluxio.worker.network.async.cache.manager.threads.max&nbsp;的一半，这个配置代表 Worker 处理异步缓存请求的最大线程数，默认值是两倍的 CPU 数；S3 Proxy 需要对已经提交缓存的 block 进行去重，防止在高并发读取同一个文件的情况下，多次提交同一个 block 的缓存请求到 Worker，占满 Worker 的异步缓存队列。Worker 的异步缓存队列大小由配置&nbsp;alluxio.worker.network.async.cache.manager.queue.max&nbsp;控制，默认是 512。去重比较推荐使用 bitmap 按照 block id 做；在 Worker 异步缓存队列没满的情况下，异步缓存的线程数将永远保持在 4 个，需要修改代码提高 Worker 异步缓存的最小线程数，防止效率过低，可参考&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/pull/17179\">#17179</a>\"。</p><p></p><p>在上线了这个缓存策略后，我们进入了阶段三，可以看到，阶段三的尖刺全部消失了，整体的速度略微有所提升。因为我们是对小文件（1GB 左右）进行的缓存，所以提升效果不明显。经过我们测试，此缓存策略能够提升读取大文件（10GB 及以上）3-5 倍的速度，而且文件越大越明显。</p><p></p><p>解决了缓存的问题后，我们继续切换更多模型的读取到 S3 Proxy，效果如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/29/c3/294133a69c4e93237fdf21637b1b8cc3.png\" /></p><p></p><p>本次我们另外切换了三个模型的读取请求到 S3 Proxy，其中橙色模型是我们之前已经切换到 S3 Proxy 的模型，本次新增的模型最大达到了 10G，读取流量峰值为 500Gb/sec。</p><p></p><p>这次我们同样分为三个阶段，阶段一是橙色模型已经切换到 S3 Proxy，其他模型都使用 UnionStore，因为橙色模型的数据量小，并且还用了 Alluxio 加速，所以它的读取速度能够比其他模型的读取速度快上数十倍。</p><p></p><p>阶段二是我们将其他模型也切换至 S3 Proxy 后的状态，可以看到其他模型读取速度明显变快了，但是橙色模型读取速度受到其他模型的影响反而变慢了，这是一个非常奇怪的现象。最后我们定位到是元数据缓存没有开启的原因，在元数据缓存没有开启的情况下，Alluxio 会将客户端的每一次请求都打到 HDFS 上，加上 S3 Proxy 也会频繁对一些系统目录做检查，这样就导致 Master 同步元数据的负担非常重，性能甚至能下降上千倍。</p><p></p><p>在这个场景，我们本来是不打算开启元数据缓存的，主要是担心业务对已缓存修改文件进行修改，导致读取到错误的文件，从而影响模型的上线。但是从实践的结果来看，元数据缓存必须要开启来提升 Master 的性能。</p><p></p><p>与业务方沟通过后，我们制定了元数据一致性的规范：</p><p></p><p>元数据缓存设置为 1min；新增文件尽量写入新目录，以版本号的方式管理，不要在旧文件上修改或覆盖；对于历史遗留，需要覆盖新文件的任务，以及对元数据一致性要求比较高的任务，我们在 S3 Proxy 上提供特殊命令进行元数据的同步，数据更新后，业务方自己调用命令同步元数据。</p><p></p><p>在开启元数据缓存过后，我们来到了图中的阶段三，可以很明显的看到所有模型数据的读取速度有了飞跃式提升，相比于最开始没有使用 S3 Proxy 读取速度提升了 10+ 倍。这里需要注意的是，10+ 倍是指在 Alluxio 机器数量足够多，网卡足够充足的情况下能达到的效果，我们在实际使用过程中，用了 UnionStore 一半的资源达到了与 UnionStore 同样的效果。</p><p></p><h3>4.3.4 S3 Proxy 限速</h3><p></p><p></p><p>我们在模型读取场景上线 Alluxio 的本意是为了提高业务方读取模型的速度，但是因为通过 Alluxio 读数据实在是太快了，反而需要我们给它限速，非常的具有戏剧性。不限速将会面临一个很严重的问题：算法容器在读取模型时，如果文件较大，不仅会影响 S3 Proxy 所在物理机的网卡，也会导致该容器所在的 k8s 宿主机的网卡长时间处于被占满状态，从而影响这一节点上的其他容器。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/64/b0/64e08276ea9b9440d7d62c93fb56deb0.svg\" /></p><p></p><p>目前限速的实现主要有以下几种方案：</p><p></p><p>Worker 端限速：优点是对所有客户端生效，缺点是对同节点客户端短路读不生效，在我们的场景，S3 Proxy 会走短路读，不能满足我们的需求。</p><p></p><p>客户端限速：优点是能够同时对 Alluxio fuse 和 S3 Proxy 生效，缺点是客户端可以自己改配置绕过限制，同时服务端版本和客户端版本可能存在不一致的情况，导致限速失效。</p><p></p><p>S3 Proxy 限速：只能对 S3 Proxy 生效，对其他的客户端以及 Worker 都不能生效。</p><p></p><p>因为我们当前的目标就是替代 UnionStore，业务方访问 Alluxio 的入口只有 S3 Proxy，因此客户端限速和 S3 Proxy 限速都能满足我们的需求，但是从实现的难易角度上考虑，我们最后选择了从 S3 Proxy 层面限速。</p><p></p><p>我们支持了两种限速策略，一方面是 S3 Proxy 进程全局限速，用于保护 Worker 网卡不被打满；另一方面是单连接限速，用于保护业务容器所在 k8s 节点。限速策略我们已经贡献给了社区，如果感兴趣可以参考：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/pull/16866\">#16866</a>\"。</p><p></p><h3>4.4 模型训练场景适配</h3><p></p><p></p><h3>4.4.1 场景特点</h3><p></p><p></p><p>我们的模型训练场景有以下特点：</p><p></p><p>因为大部分开源的模型训练框架对本地目录支持最好，所以我们最好是为业务提供 POSIX 访问的方式；模型训练时，主要瓶颈在 GPU，而内存，磁盘，网卡，CPU 等物理资源比较充足；GPU 机器不会运行训练任务以外的任务，不存在服务混布的情况；数据以快照形式管理，对元数据没有一致性要求，但是需要有手段能够感知 HDFS 上产生的新快照。</p><p>针对模型训练场景，毫无疑问我们应该选择 Alluxio fuse 来提供缓存服务： 1. Alluxio fuse 提供了 POSIX 访问方式； 2. Alluxio fuse 能够利用内存和磁盘做元数据缓存与数据缓存，能够最大程度利用 GPU 机器上闲置的物理资源。</p><p></p><h3>4.4.2 性能测试</h3><p></p><p></p><p>在上线前，我们对 fuse 用 fio 进行了压测。</p><p>Alluxio fuse 配置：</p><p></p><p></p><p></p><p>测试结果如下：</p><p></p><p></p><p></p><p>以上结果均针对数据已缓存至 fuse 本地磁盘的情况，1G 文件与 10G 文件读取时，速度是 100G 文件的两倍，这是因为容器的内存为 40G，有充足的 pagecache 来缓存 1G 与 10G 的文件，但是 100G的文件没有充足的 pagecache，所以性能会下降，但是也能达到不错的速度，整体行为符合预期。</p><p></p><h3>4.4.3 集群部署</h3><p></p><p></p><p>Alluxio fuse 的部署方式我们选择了以 DaemonSet 部署，通过 host path 进行映射，没有选择 CSI 部署，主要是基于以下考虑：</p><p></p><p>Alluxio fuse 高性能的核心在于数据缓存与元数据缓存，数据缓存需要消耗大量的磁盘，元数据缓存需要消耗大量的内存，如果以 CSI 的形式进行部署，每个容器只能分配到少量的磁盘与内存给 Alluxio fuse 进程；在模型进行训练的时候，读取的训练数据重复程度很高，如果每个容器起一个 fuse 进程，可能会导致同一机器缓存多份相同的文件，浪费磁盘；GPU 机器只跑训练任务，所以 fuse 进程可以 long running，无需考虑资源释放的问题；host path 的部署方式可以很容易实现挂载点恢复。</p><p></p><p>这里对挂载点恢复做一个说明，一般情况下，如果 Alluxio fuse 容器因为各种异常挂了，哪怕 fuse 进程重新启动起来，将目录重新进行挂载，但是在业务容器里的挂载点也是坏掉的，业务也读不了数据；但是如果做了挂载点恢复，Alluxio fuse 容器启动起来以后，业务容器里的挂载点就会自动恢复，此时如果业务自身有重试逻辑，就能不受影响。Alluxio fuse 进程的挂载点恢复包括两个部分，一部分是挂载点本身的恢复，也就是 fuse 进程每次重启后要挂到同一个挂载点；另一部分是客户端缓存数据的恢复，也就是 fuse 进程每次重启后缓存数据目录要与原先保持一致，避免从 Alluxio 集群重复拉取已经缓存到本地的文件。挂载点恢复在 CSI 里需要做一些额外的开发来支持，但是如果是以 host path 的方式映射，只要在业务容器里配置了 HostToContainer 即可，不需要额外的开发。</p><p></p><p>我们 fuse 进程的部署架构图如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b5/cf/b58af7ec13c5efa16273665988bee1cf.svg\" /></p><p></p><p></p><p>在这个场景下，我们的 Alluxio 集群采取了“小集群重客户端”的方式来部署，即提供一个规模较小的 Alluxio 集群，只用来做数据的分发，性能和缓存由 Alluxio fuse 自身保证。Alluxio 集群只需要提供高配置的 Master 和少量的 Worker 即可，集群整体的部署架构如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ab/81/abba7884ca490f8c58e280be03e56481.svg\" /></p><p></p><p></p><p>按照这种部署模式，3 台 Raft HA 的 Master 与 少量 Worker 就可支撑起 fuse 进程大规模的部署。</p><p></p><h3>4.4.4 Alluxio fuse 调优</h3><p></p><p></p><p>首先是元数据缓存，Alluxio fuse 可开启元数据缓存，这里容易与 Master 对 UFS 元数据的缓存弄混淆，我们简单做个说明：</p><p></p><p>Alluxio Master 会缓存 UFS 的元数据，决定是否更新元数据由客户端配置的&nbsp;alluxio.user.file.metadata.sync.interval&nbsp;决定。假如这个值设置为 10min，客户端在请求 Master 时，如果 Master 在之前的 10min 内已经更新过元数据，则 Master 会直接返回缓存的元数据，而不会请求 UFS 拿最新的元数据；否则将会返回 UFS 的最新的元数据，并且更新 Master 的元数据；用户在用 Alluxio fuse 访问 Alluxio 时，会先看内核缓存元数据是否失效（配置为 fuse 启动参数 attr_timeout,entry_timeout），再看用户空间元数据缓存是否失效（配置为&nbsp;alluxio.user.metadata.cache.expiration.time），再看 Master 缓存是否失效（配置为alluxio.user.file.metadata.sync.interval），只要有一层没失效，都不能拿到 HDFS 的最新元数据。</p><p></p><p>所以建议在开启 fuse 元数据缓存后，设置&nbsp;alluxio.user.file.metadata.sync.interval=0&nbsp;以便每次 fuse 在本地元数据缓存失效后，都能拿到 UFS 最新的元数据。</p><p>另外 fuse 的元数据缓存可以通过一些特殊的命令来更新（需要配置&nbsp;alluxio.fuse.special.command.enabled=true）：</p><p></p><p>元数据缓存可通过以下命令进行强制刷新，假设我们的 mount 目录为&nbsp;/mnt/alluxio，利用以下命令可以刷新所有元数据缓存：</p><p></p><p><code lang=\"null\">ls -l /mnt/alluxio/.alluxiocli.metadatacache.dropAll</code></p><p></p><p>利用以下命令可以刷新指定目录（这里以&nbsp;/user/test&nbsp;为例）的元数据缓存：</p><p></p><p><code lang=\"null\">ls -l /mnt/alluxio/user/test/.alluxiocli.metadatacache.drop</code></p><p></p><p>在代码中（以 python 为例），可以这样清理元数据：</p><p></p><p><code lang=\"null\">import os\nprint(os.path.getsize(\"/mnt/alluxio/user/test/.alluxiocli.metadatacache.drop\"))</code></p><p></p><p>但是需要注意，内核元数据缓存是清理不掉的，所以这里推荐内核元数据缓存设置一个较小的值，比如一分钟，用户空间元数据缓存设置一个较大的值，比如一小时，在对元数据有一致性要求的时候，手动刷新用户空间元数据缓存后，等待内核元数据缓存失效即可。</p><p></p><p>元数据缓存和数据缓存同时开启的情况下，清理元数据缓存的命令在使用上会有一些问题，我们进行了修复，参考：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/issues/17029\">#17029</a>\"。</p><p></p><p>其次就是数据缓存，我们的 Alluxio fuse 因为是用 DeamonSet 的方式进行的部署，所以数据缓存我们基本上可以用满整台物理机的磁盘，极大降低了 Alluxio Worker 的流量。</p><p></p><p>最后就是资源配置，因为每个机器只起一个 fuse 进程，所以可以适当给 fuse 进程多分配给一些 CPU 和内存，CPU 可以适当超卖，以处理突然激增的请求。</p><p></p><p>内存方面，首先是堆内存的配置，如果开启了用户空间元数据缓存，按照&nbsp;缓存路径量数 * 2KB * 2&nbsp;来设置 Xmx。另外 DirectoryMemory 可设置大一点，一般 8G 够用。如果开启了内核数据缓存，还需要给容器留存一些空间来存放 pagecache，因为 kubernetes 计算容器内存使用量会包含 pagecache 的使用量。关于 pagecache 是否会引起容器 OOM，我们查找了很多文档都没有得到准确的结论，但是我们用如下配置进行了压测，发现容器并不会 OOM，并且 fuse 的表现十分稳定：</p><p></p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/7d/70/7d5425d3e78a009c8c1b6c16defe4f70.png\" /></p><p></p><p></p><h3>4.4.5 上线结果</h3><p></p><p></p><p>我们的算法模型训练切换至 Alluxio fuse 后，模型训练的效率达到了本地磁盘 90% 的性能，相比于原来 UnionStore 的 s3fs-fuse 的挂载，性能提升了约 250%。</p><p></p><h2>5 S3 Proxy 在大数据场景的应用</h2><p></p><p></p><p>回顾模型上线场景，我们不仅为算法业务提供了模型加速读取的能力，还沉淀下来了一个与对象存储协议兼容，但是下载速度远超普通对象存储的组件，那就是 Alluxio S3 Proxy，所以我们现在完全可以做一些”拿着锤子找钉子“的一些事情。</p><p></p><p>这里介绍一下我们大数据组件的发布与上线流程，流程图大致如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f0/f6/f0e536a60ddb058961bd0d9f8376a5f6.svg\" /></p><p></p><p>下面用文字简单描述：</p><p></p><p>开发者修改代码以后，将代码合入对应组件的 master 分支，此时 Gitlab 将调用 CI 的 Web Hook，CI 会运行对应组件的打包编译逻辑；组件打包成二进制包后，CI 会向 Kosmos 注册二进制包的元信息，以及将二进制包上传至 Kosmos，Kosmos 在接受到二进制包后，会上传至对象存储；开发者在大数据运维平台选择要上线的组件，以及组件的版本，大数据组件会自动在生产环境的服务器上运行部署逻辑；在部署逻辑运行的过程中，会向 Kosmos 请求下载组件的二进制包，Kosmos 将会直接返回对象存储的只读链接，供生产环境服务器进行下载。</p><p></p><p>其中 Kosmos 是我们自研的包管理系统，其诞生的背景可以参考：<a href=\"https://zhuanlan.zhihu.com/p/593637546\">Flink 实时计算平台在知乎的演进</a>\"；另外我们的大数据运维平台也有相应的专栏，感兴趣可以查看：<a href=\"https://zhuanlan.zhihu.com/p/617731670\">Ansible 在知乎大数据的实践</a>\"。</p><p></p><p>一方面，这个流程最大的问题在于大规模上线节点时，从对象存储下载二进制包速度过慢。比如我们要对所有的 DataNode 节点以及 NodeManager 节点做变更时，每台机器都需要下载数百 MB 甚至上 GB 的二进制包，按照对象存储 20-30MB/sec 的下载速度，每台机器需要花费约 30 秒的时间来进行下载，占了整个部署逻辑约 2/3 的时间。如果按照 10000 台 DataNode 来计算，每两台滚动重启（保证三副本一个副本可用），仅仅花费在下载二进制包上的时间就达到了40+ 小时，及其影响部署效率。</p><p></p><p>另一方面，对象存储在不同的机房使用时，也会面临外网流量的问题，造成比较高的费用；所以这里对 Kosmos 做了多机房改造，支持向不同的对象存储上传二进制包，用户在请求 Kosmos 时，需要在请求上加上机房参数，以便从 Kosmos 获取同机房对象存储的下载链接，如果用户选错了机房，依然会使用外网流量。</p><p></p><p>上述问题其实可以通过改造大数据运维平台来解决，比如将下载与部署逻辑解耦，在节点上以较高的并发下载二进制包后再进行滚动部署，但是改造起来比较费时费力，更何况我们现在有了更高效下载文件的方式— Alluxio S3 Proxy，所以更没有动力来做这个改造了。</p><p></p><p>我们将 Kosmos 的对象存储挂载到 Alluxio 上，Kosmos 在被请求下载时，返回 Alluxio S3 Proxy 的只读链接，让用户从 S3 Proxy 读取数据，改造后的流程图如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ee/f3/ee062eaa7cc1b45c7cf46344547cb6f3.svg\" /></p><p></p><p>经过我们的改造，Kosmos 几乎所有的下载请求都能在 1-2 秒内完成，相比于从对象存储下载，快了 90% 以上，下图是我们的生产环境中，Kosmos 分别对接对象存储与 Alluxio 的下载速度对比，其中 Alluxio S3 Proxy 被我们限速至 600MB/sec：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/25/c1/25f3b640ed91d3fe8d5276cdc0d1c7c1.png\" /></p><p></p><p>此外 Alluxio 我们也进行了多机房部署，支持了 Kosmos 的多机房方案，哪怕是用户选错了机房，也不会造成额外的外网流量，仅仅只是会请求其他机房的 Alluxio 集群，消耗一定的专线带宽。</p><p></p><h2>6 权限相关</h2><p></p><p></p><p>Alluxio 在与 HDFS 对接时，会继承 HDFS 的文件权限系统，而 HDFS 与 Alluxio 的用户可能不一致，容易造成权限问题。权限问题比较重要，所以我们单独用一个章节来做介绍。</p><p></p><p>我们通过研究代码与测试，总结了基于 Alluxio 2.9.2 版本（HDFS 与 Alluxio 的认证方式都是 SIMPLE），用户与权限的映射关系，总览图如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b4/0e/b41ddc52685bde5d7c7e60207d236a0e.svg\" /></p><p></p><p>首先是 Alluxio Java Client 的用户：Alluxio Java Client 与 Alluxio 交互时，如果配置了&nbsp;alluxio.security.login.username，Alluxio 客户端将会以配置的用户访问 Alluxio 集群，否则将会以 Alluxio Java Client 的启动用户访问 Alluxio。</p><p></p><p>Alluxio Master/Worker 在与 HDFS 交互时，如果 Master/Worker 在启动时配置了环境变量&nbsp;HADOOP_USER_NAME（可在&nbsp;alluxio-env.sh&nbsp;配置），则 Master/Worker 将会以配置的用户访问 HDFS，否则将会以 Master/Worker 的进程启动用户访问 HDFS。这里需要注意，Master 和 Worker 尽量配置一样的 HDFS 用户，否则一定会造成权限问题。</p><p></p><p>在向 HDFS 写入文件时，Alluxio 会先以 Master/Worker 配置的 HDFS 用户写入文件，写完以后会调用 HDFS 的 chown 命令，将文件的 owner 修改为 Alluxio Java Client 的用户，这里我们举例说明：假设 Alluxio 启动用户为 alluxio，Alluxio Java Client 用户为 test，在向 HDFS 写入文件时，Alluxio 会先将文件以 alluxio 账号写到 HDFS 上，再将文件 chown 变成 test 用户，这时如果 alluxio 用户不是 HDFS 超级用户，在 chown 时会发生错误（比较坑的一点是这个错误 alluxio 不会抛出给客户端），导致 Alluxio 上看到的文件owner 是 test，但是 HDFS 上的文件 owner 时 alluxio，造成元数据不一致。</p><p>其次是 S3 Proxy 的用户，S3 Proxy 它也是一个比较特殊的 Alluxio Java Client，但同时它也是一个 Server 端，这里主要是用户请求 S3 Proxy 的 AK SK 与 HDFS 用户的映射。S3 Proxy 默认会将用户的 AK 映射成访问 Alluxio 集群的用户，这里也可以自己实现映射关系，比如将 AK 映射成特定的用户，S3 Proxy 里有相关插件。</p><p></p><p>最后是 Alluxio fuse 的用户，Alluxio fuse 因为涉及到 linux 文件系统，而且有多种与 linux 本地文件系统相关的实现，所以比前面的更加复杂，这里我们只讨论默认情况，也就是&nbsp;alluxio.fuse.auth.policy.class=alluxio.fuse.auth.LaunchUserGroupAuthPolicy&nbsp;时的情况。用户在访问挂载目录时，用的是当前 linux 用户，用户看到挂载目录里所有文件的 owner 都是 fuse 进程启动用户；fuse 在写本地缓存目录时，用的是 fuse 进程的启动用户，此外 fuse 进程与 Alluxio 集群交互时又完全遵循 Alluxio Java Client 的逻辑。</p><p></p><p>综上所述，比较推荐的用户设置方式为：</p><p></p><p>Alluxio 集群使用 alluxio 账号启动，并且将 alluxio 账号设置为 HDFS 超级用户；S3 Proxy 用 alluxio 账号启动，用户访问时，AK 为 HDFS 账号；Alluxio fuse 以 root 用户启动，防止写本地数据没有权限，并且加上 allow_other 参数，配置&nbsp;alluxio.security.login.username&nbsp;为 HDFS 用户。</p><p></p><h2>7 其他问题</h2><p></p><p></p><p>在上线过程中，我们遇到了很多问题，其中大部分都跟配置项调优有关。遇到这些问题的原因主要还是因为 Alluxio 是面相通用设计的缓存系统，而用户的场景各式各样，很难通过默认配置完美适配，比如我们有多套 Alluxio 集群，每套集群用来解决不同的问题，所以这些集群的配置都有些许差异。多亏 Alluxio 提供了许多灵活的配置，大部分问题都能通过修改配置解决，所以这里只介绍一些让我们印象深刻的“代表”。</p><p></p><p>最大副本数：在模型上线场景，缓存副本数我们不设上限，因为在算法模型在读取时，往往是一个大模型同时几十个甚至上百个容器去读，占用的存储不多，但是读取次数多，并且仅高并发读取这一次，很少有再读第二次的情况。所以这里对每一个缓存文件副本数不做限制，可以让每个 Worker 都缓存一份，这样能够达到最大的吞吐，拥有最好的性能。在模型训练场景，我们将缓存副本数设置为 3，一方面是因为训练数据量很大，需要节省存储，另一方面是 Alluxio fuse 的本地缓存会承担大部分流量，所以对于 Worker 的吞吐要求相对较低。</p><p></p><p>S3 Proxy ListObjects 问题：我们发现 S3 Proxy 在实现 ListObjects 请求时，会忽略 maxkeys 参数，列出大量不需要的目录。 比如我们请求的 prefix 是&nbsp;/tmp/b， maxkeys 是 1，S3 Proxy 会递归列出&nbsp;/tmp&nbsp;下所有文件，再从所有文件里挑选出满足 prefix&nbsp;/tmp/b&nbsp;的第一条数据，这样不仅性能差，也会导致可能出现 OOM 的情况，我们采用临时方案进行的修复，感兴趣可以参考&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/pull/16926\">#16926</a>\"。这个问题比较复杂，需要 Master 与 S3 Proxy 联合去解决，可以期待&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/pull/16132\">#16132</a>\"&nbsp;的进展。</p><p></p><p>监控地址冲突：我们监控采用的是 Prometheus 方案，Alluxio 暴露了一部分指标，但是 JVM 指标需要额外在 Master 或者 Worker 的启动参数中添加 agent 与端口暴露出来，添加 agent 以后，因为 monitor 会继承 Master 与 Worker 的启动参数，所以 monitor 也会尝试使用与 Master 和 Worker 同样的指标端口，这会出现 ”Address already in use“ 的错误，从而导致 monitor 启动失败。具体可查看&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/issues/16657\">#16657</a>\"。</p><p></p><p>Master 异常加载 UFS 全量元数据：如果一个路径下有 UFS mount 路径，在对这个路径调用 getStatus 方法时，Alluxio master 会递归同步这个路径下的所有文件的元信息。比如&nbsp;/a&nbsp;路径下的&nbsp;/a/b&nbsp;路径是 UFS 的 mount 路径，在调用&nbsp;getStatus(\"/a\")&nbsp;的时候，会导致&nbsp;/a&nbsp;下面的元数据被全量加载。如果&nbsp;/a&nbsp;是一个大路径，可能会导致 Master 因为加载了过多的元数据而频繁 GC 甚至卡死。具体可查看&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/issues/16922\">#16922</a>\"。</p><p></p><p>Master 频繁更新 access time：我们在使用过程中，发现 Master 偶尔会很卡，通过 Alluxio 社区同学的帮助，定位到问题来自 Master 频繁更新文件的最后访问时间，通过合入&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Alluxio/alluxio/pull/16981\">#16981</a>\"，我们解决了这个问题。</p><p></p><h2>8 总结与展望</h2><p></p><p></p><p>其实从 2022 年的下半年我们就开始调研 Alluxio 了，但是因为种种原因，中途搁置了一段时间，导致 Alluxio 推迟到今年才上线。在我们调研与上线的过程中，Alluxio 社区是我们最强大的外援，为我们提供了海量的帮助。</p><p></p><p>本次我们在算法场景对 Alluxio 小试牛刀，取得的结果令人十分惊喜。</p><p></p><p>从性能上讲，在算法模型上线的场景，我们将 UnionStore 用 Alluxio 替换后，最高能够获得数十倍的性能提升；在模型训练场景，我们配合 Alluxio fuse 的本地数据缓存，能够达到近似本地 NVME 磁盘的速度，相比于 UnionStore + s3fs-fuse 的方案，性能提升了 2-3 倍。</p><p></p><p>从稳定性上讲，在 HDFS 抖动或者升级切主的时候，因为有数据缓存和元数据缓存，Alluxio 能够在一定时间内不受影响，正常提供服务。</p><p></p><p>从成本上讲，Alluxio 相比于 UnionStore 每年为我们节省了数十万真金白银，而且性能上还有盈余。</p><p></p><p>从长远的发展来看，Alluxio 具有强大的可扩展性，尤其是 Alluxio 的新一代架构 Dora ，能够支持我们对海量小文件缓存的需求，这让我们更有信心支撑算法团队，面对即将到来的人工智能浪潮。</p><p></p><p>最后再次感谢 Alluxio 团队，在我们上线的过程中为我们提供了大量的帮助与建议，也希望我们后续能够在大数据 OLAP 查询加速场景以及分布式数据集编排领域继续深入合作与交流。</p>",
    "publish_time": "2023-04-18 12:15:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Web 的下一个转型：单页应用？是时候换个思路了",
    "url": "https://www.infoq.cn/article/J9f5EyxwRCmKrORr5s2t",
    "summary": "<p>Remix 软件的联合创始人兼软件工程师 Kent C. Dodds，讲述了当前备受欢迎的单页应用程序（SPA）架构中存在的缺陷。</p><p></p><p>Dodds 介绍了从纯 HTML 和多页面应用（MPA）开始的不同种类 Web 应用方式的简短历史。他说，MPA 因其简单的心智模型而出名，这种方式虽然专注于开发者的体验，但每次与服务器的互动都需要全页面刷新导致其用户体验很差。开发者们转而使用被 Dodds 称作是 “逐步增强的多页面应用（PEMPAs）”。该方法使用 JavaScript 结合 AJAX（异步 JavaScript 和 XML）和后续的 Fetch API，在无需全页面刷新的情况下即可调用服务器端点。</p><p></p><p>Dodds 称，PEMPA 的最大问题在于代码的重复。服务器生成了一份 HTML，但为避免全页面的刷新，开发者们将代码放在浏览器客户端又渲染了一份 HTML。“这是 PEMPA 中避无可避的问题”，Dodds 说，“（这个问题）不仅存在于用户界面，还有验证方面。”</p><p></p><p>对此问题的解决方案是仅在服务器上运行 REST API 的 SPA（单页应用）。“我们干掉了服务器端的渲染层，” Dodds 说。这种方式规避了代码重复，成为了现代网页应用的默认方式。然而，Dodds 也提到规避了代码重复是 SPA 应用相较 PEMPA 而言的唯一优势点。他说，PEMPA 的问题 SPA 一个不少，此外，SPA 还有增多的捆绑大小、高网络流量、运行时性能差，以及困难的状态管理等问题。在状态管理这一点上，Dodds 将其描述为“非常地折磨。只要看看 NPM，就能找到上百个尝试解决这一问题的模块。”其中，NPM 是 Node.js 模块的包注册。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8a/8a96af342493070791aebba4d954164f.png\" /></p><p></p><p>PESPA（逐步增强的单页应用）—— QCon 上所展示的幻灯片</p><p></p><p>Dodds 认为，他所说的 PESPA（逐步增强的单页应用）应是 Web 的下一个合理过渡。“解决代码重复问题的方法是在两侧都使用同样的代码”，他说，“由此得出的心智模型与十多年前简单的 MPA 心智模型很类似。”</p><p></p><p>PESPA 模型也包括 Dodds 所称的模拟浏览器。“用于实现这一架构的框架，就是把它看作是浏览器。这就是得出这种心智模型的方式。” Dodds 称，在实践中，多数代码都会在服务器上，而框架则负责调用这些代码。</p><p></p><p>“许多框架都在采用这种架构，”Dodds 说，“但我印象最深的还是 Remix……不过老实说，我也是 Remix 的联合创始人，”虽然 Dodds 已经于 2022 年离开了公司，去追寻不同的道路。Remix 使用的是 React，也就是 Dodds 所说的“UI 框架”，他也说，未来 Remix 将支持其他 UI 框架。Dodds 还称 Remix 本身即是一个“浏览器模拟器”，可以使用请求对象等标准 web API，但是在服务器端运行。</p><p></p><p>但 Remix 真的就是答案所在吗？在最新的 React 文档中，该框架确实被称作是“具有嵌套路由的全栈 React 框架”，也是仅有的几个被提及的框架之一。也有其他框架在采用类似的想法，尤其是被 React 团队誉为当前采用 React 服务器组件最佳方式的 Next.js，React 团队称：“（Next.js）将多页应用这种服务器为中心的、简单的请求返回心智模型，和以客户侧为中心、无缝交互式的单页应用相结合，带来两个世界的最佳效果。\"</p><p></p><p>此外，Deno 项目也一直在为服务器端渲染和其自身的 Fresh 框架做准备。</p><p></p><p>Web 应用框架领域发展迅速，尽管 Dodds 的演讲主题是“Web 的下一个转型“，还是留存了许多供人猜测的空间。</p><p></p><p>附 QCon 伦敦 Kent C. Dodds 主题演讲内容概述。</p><p></p><p></p><h2>Kent C. Dodds 谈 Web 的下一次转型</h2><p></p><p></p><p>DOdds 解释了近年来 Web 所经历的一系列变化，从静态 HTML 文件开始，并迅速演变为动态服务器生成的 HTML 响应。如今，REST 或 GraphQL API 已经通过 Jamstack 得到 JavaScript 重型客户端的广泛使用。Web 的最新转型可谓前所未有，现代基础设施和技术已经重新定义了卓越用户体验的评判基准。在这个全新时代，旧实践正得到全面改造，而现代方法也日益暴露出自己的局限性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/5e/5eb71ccd5461769ebc33f6d63fe303f4.png\" /></p><p></p><p>在演讲中，DOdds 分享了上述转变给用户体验和开发效率带来的影响。即将到来的 Web 时代将以去中心化为共性，强调更快的速度、更低的成本和更加令人向往的发展前景。</p><p></p><p>用于构建当今 Web 的技术早在 25 年前就进行过首次标准化，包括诞生于九十年代中期的 HTTP、HTML、CSS 和 JS。多年以来，Web 已经发展成无处不在的重要应用程序平台。随着 Web 的发展，用于创建这些应用程序的开发架构也在同步改进。虽然当今 Web 开发者采用的核心架构多种多样，但单页应用程序（SPA）仍是其中应用最广泛的类别。而如今，我们正进一步过渡至新的、更加强大的 Web 应用程序构建架构。</p><p></p><p>最近，Web 的开发生产力又迎来另一波影响深远的冲击。借助现代 Web 开发工具和流程，开发人员能够更快完成任务，显著减少新 Web 应用程序的开发和启动时间。此外，新变化也让开发者能够将更多精力放在应用程序的业务逻辑上，而非底层支持基础设施上，从而带来更富成效、更令人满意 的成果。</p><p></p><p>把握这些新技术和新方法的潜在优势、积极适应不断变化的数字环境可谓至关重要。开发人员可以利用现代基础设施和程序的功能，设计出速度更快、可靠性更高、开发和维护成本更低的新一代 Web 应用程序。</p><p></p><p>总而言之，Web 的最新变化标志着开发人员迎来了提高生产力、改善用户体验的又一个激动人心的历史性时刻。随着我们的进步，特别是对新趋势和新技术的密切关注，未来的 Web 开发方式必将呈现出不同于以往的全新面貌。</p><p></p><p></p><h5>参考链接：</h5><p></p><p></p><p><a href=\"https://devclass.com/2023/03/28/single-page-applications-time-to-move-on-says-remix-co-founder-at-qcon/\">https://devclass.com/2023/03/28/single-page-applications-time-to-move-on-says-remix-co-founder-at-qcon/</a>\"</p><p></p><p><a href=\"https://qconlondon.com/presentation/mar2023/webs-next-transition\">https://qconlondon.com/presentation/mar2023/webs-next-transition</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/bfc0d7c4415c2abd1c7811027\">使用 Lambda Web Adapter 在 Lambda 上 构建 web 应用</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ccd0ebe97a11b368e0e068c79\">Tech Talk 宣传 | 如何高效、极简构造无服务器 Web 应用</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&amp;mid=2649970029&amp;idx=1&amp;sn=dd41f58ff643b44d9284f20eb6fe5e8d&amp;chksm=beca256b89bdac7d4234c85f248d75aace542c31322afcd2181035933b448fb7e52a9115a80d&amp;scene=27#wechat_redirect\">你可以错过Web3，但不要错过Web5</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&amp;mid=2649969927&amp;idx=1&amp;sn=cc8a7cee992d36202d86ee5068fcc66e&amp;chksm=beca250189bdac17511f9649f03ab3b0c6fee72a33cb1957ff5a6017924fe10b7c7d5581eb98&amp;scene=27#wechat_redirect\">Web3当下，最佳投资就是投资自己</a>\"</p>",
    "publish_time": "2023-04-18 12:27:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "vivo AI 计算平台轩辕文件存储缓存的设计与应用",
    "url": "https://www.infoq.cn/article/546XhyGqVtkvMWGlEGys",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p><a href=\"https://www.infoq.cn/article/6jqHWXEQHV6DoAYeSLZa\">vivo AI 计算平台</a>\"是在 2018 年底开始着手建设的，致力于解决统一高性能训练环境、大规模分布式训练、计算资源的高效利用调度等痛点。经过将近四年的持续迭代，平台建设和落地取得了很大进展，由当初服务深度学习为主，到现在演进成包含 VTraining、VServing、VContainer 三大模块，对外提供模型训练、模型推理和容器化等能力，成为 vivo AI 领域的核心基础平台。VTraining 是一站式的通用训练平台，基于 <a href=\"https://www.infoq.cn/article/EhRjlkwxs6C6cT4cHzlt\">Kubernetes 集群</a>\"搭建，支持多种框架的大规模分布式训练，并配备 PB 级别规模的分布式存储。现在 VTraining 的用户已经超过 700 人，覆盖了推荐，语音，视觉，NLP，影像等方向核心业务。VTraining 模型训练需要处理数百万亿字节的数据，但通常是音频和图像文件等海量小文件。模型训练需要多次运行 epoch 来进行迭代，因此会频繁地访问数据。此外, 还需要通过不断向 GPU 供给数据来让 GPU 处于忙碌状态。既要优化 I/O 又要保持 GPU 所需的吞吐量并非易事。本文将分享轩辕文件存储缓存的设计原理以及在 Vtraining 平台的性能加速应用。</p><p></p><p></p><h2>轩辕文件存储系统</h2><p></p><p></p><h4>用户态文件系统框架</h4><p></p><p></p><p>文件系统提供了通用的应用程序的访问数据的接口，一般分为两种实现，一种是内核在用户态实现了文件系统；另外一种是内核在自己的内核态实现了文件系统，这也是内核的一部分，在内核态实现这个文件系统避免了消息在用户态和内核态之间的切换，具备比较高的性能。但随着内核态文件系统(xfs/ext4)的复杂性的增加,在用户态开发文件系统变得比较流行，同时在用户态开发比较容易维护，即使 crash 了也不会导致 kernel crash。如果是基于内核态文件系统 xfs crash 了，整个 kernel 就会 crash。大部分用户态文件系统开发都是基于 Fuse。Fuse 有两部分组成：fuse 驱动和用户态的 daemon.fuse。驱动是由内核的 fuse 设备驱动(/dev/fuse)这个字符设备驱动充当代理，针对不同的文件系统实现提供 kernel 和用户态 daemon 的通信桥梁；用户态 daemon 是从/dev/fuse 设备读取，然后处理这些请求，最后把处理的就结果写回到/dev/fuse 设备。<a href=\"https://www.infoq.cn/article/3oFSOWfYGsX5h7xzsIe6\">轩辕文件系统</a>\"也是基于 Fuse 进行开发的用户态文件系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/94e390a216df4105ded69e5883d0040d.png\" /></p><p>fuse实现用户态文件系统的框架</p><p></p><p></p><h4>轩辕文件存储系统</h4><p></p><p></p><p>轩辕文件存储是 vivo 互联网基础平台存储团队开发给内部使用的存储系统，是一款面向云原生设计的高性能文件系统。它采用了”数据“与”元数据“分离的存储架构，从而实现文件系统的分布式设计。主要的核心架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e37afe2d70cfe023ad31ff6894a9923c.png\" /></p><p>轩辕文件存储架构</p><p></p><p>主要包括三个特性组件：</p><p></p><p>客户端 vivo_juicefs</p><p>vivo_juicefs 是基于开源社区 juicefs 开发的存储客户端，提供了丰富的 API，兼容了当前大多数主流应用平台，可以在最上层无缝对接大数据，机器学习，人工智能等平台，为其提供海量、弹性、低价的高性能存储。在数据存储和元数据引擎上采用了可插拔架构，能对接各种各样的对象存储，用户可以根据自己需求选择合适的底层存储。最主要 vivo_juicefs 的缓存策略也十分灵活，很大程度上能解决存储性能不足的问题，本文着重讲解客户端 vivo_juicefs 的缓存功能。</p><p></p><p>元数据引擎</p><p>由于 vivo_juicefs 元数据引擎可插拔特性，可以使用户会选择性能较好的元数据引擎来加速元数据访问，比如 redis，Tikv 等高性能数据库。轩辕文件存储的元数据引擎是 vivo 公司自研的一款具备高性能，高稳定性，多数据模型等特性的分布式磁盘 KV 数据库，虽然达不到 redis 那么高的性能，但是会大大减少成本压力，并针对 AI 特征业务做更多的应用场景适配。</p><p></p><p>数据存储引擎</p><p>数据存储引擎使用的是 vivo 公司自研的轩辕对象存储。轩辕对象存储提供了海量，安全，低成本，高性能，高可靠的存储服务解决方案。目前提供了多种语言 SDK，能使开发者快速接入存储集群，也能更好对接 vivo_juicefs 客户端。</p><p></p><p></p><h2>为什么需要缓存</h2><p></p><p></p><p>当前计算平台基本都是存储计算分离框架，一般采用分布式存储集群来存储模型所需要的海量数据，我们先考虑没有缓存的情况，如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b8433c428f958feac81b59efb9a38eb7.png\" /></p><p></p><p>它会存在以下问题：</p><p></p><p>首先，从 I/O 和工作流的角度来看，数据是串行处理的，所有的数据访问操作都必须通过分布式存储和训练集群之间的网络，由于 I/O 操作的吞吐量受限于网络速度，GPU 会出现空转等待的情况，这使得 I/O 成为性能瓶颈。其次，当训练规模较大时，所有训练节点需要同时访问远端分布式存储 server，会对存储系统造成巨大的负载压力。此时由于高并发访问，分布式存储 server 很可能会出现拥塞，请求延时过高，进而导致任务训练的低下。最后，为了节省成本，远端分布式存储介质一般采用比较便宜的 HDD,这在降低成本的同时，无疑增加了 IO 的开销，特别是在海量小文件场景，性能瓶颈就更加突出。但是采用较多高性能磁盘（比如nvme），成本又难以控制。</p><p></p><p>那在 Vtraining 平台中，轩辕文件系统是怎么解决性能问题的呢？答案数缓存。对于一个由对象存储和数据库组合驱动的文件系统，缓存是本地客户端与远端服务之间高效交互的重要纽带。读写的数据可以提前或者异步载入缓存，再由客户端在后台与远端服务交互执行异步上传或预取数据。相比直接与远端服务交互，采用缓存技术可以大大降低存储操作的延时并提高数据吞吐量, 提升存储性能。</p><p></p><p></p><h2>轩辕文件系统缓存</h2><p></p><p></p><p>轩辕文件系统客户端 vivo_juicefs 实现了多种缓存机制来降低访问的时延和提高吞吐量，包括内存预读缓冲区，本地磁盘缓存以及分布式缓存。</p><p></p><p></p><h4>内存预读缓冲区</h4><p></p><p></p><p>vivo_juicefs 客户端在读数据的时候是先把数据读到内存读缓冲区，然后返回给业务程序。其内部实现高效的预读算法，其下次读取数据有一定的概率在内存读缓冲区命中，从而提高访问性能。vivo_juicefs 的启动参数--buffer-size 控制着读缓冲区的的大小，进而决定了读取文件以及预读的内存数据量。因此在面对高并发读场景的时候，可以适当的对--buffer-size 进行扩容，能有效提升性能。</p><p></p><p></p><h4>本地读缓存</h4><p></p><p></p><p>vivo_juicefs 客户端会根据应用读数据的模式，自动做预读和缓存操作以提高顺序读的性能。数据会缓存到本地文件系统中，可以是基于硬盘、SSD 或者内存的任意本地文件系统。如果希望保证应用程序首次访问数据的时候就能获得已缓存的性能，vivo_juicefs 也提供命令对缓存数据进行预热。注意本地读缓存最好选用高性能磁盘(比如nvme)，特别是在 IO 密集型的场景，因为本地缓存是除了内存数据访问的第一层，它能减少网络带来的延时开销，但如果磁盘的性能不高，并且缓存命中率较高的话会造成磁盘负载过高，存储性能反而会下降。</p><p></p><p></p><h4>分布式缓存集群</h4><p></p><p></p><p>本地读缓存时延最小，能提供较好的存储性能，但它存在以下限制：</p><p></p><p>各节点缓存磁盘容量有限，存储的缓存数据是有限的，当节点上的训练任务所需的数据多于缓存磁盘容量，是必然会出现缓存 miss 的，极端情况下会造成节点缓存失效。相同的训练数据可能会被不同的训练任务共享， 这些任务很有可能被调度到不同节点上，这样会造成多个节点的缓存里面有同一份缓存数据，对于集群来说缓存资源利用率不高，缓存的有效数据变少，也会加剧缓存失效。很多时候需要通过数据集提前预热，以提升第一次访问数据时的性能。本地缓存这种方式就对预热不是很友好。首先你不知道训练任务调度会调度到哪个节点（如果没有指定节点调度），你就不知道该在哪个节点上预热；其次，任务下次训练换了节点，又得再一次预热。</p><p></p><p>可以利用分布式缓存集群解决上述问题，使用分布式缓存集群有以下好处：</p><p></p><p>分布式缓存集群相当于把集群中节点的磁盘组成一块巨大的磁盘，并且同样的数据在集群中只要缓存一份，就能给不同节点的任务使用，完美解决了缓存磁盘不足和有效数据低的问题。对于预热数据加速来说也非常友好，只要在独立缓存集群任意节点进行预热，就能把数据预热到集群中，并且任务无论调度到哪个节点都无需第二次预热。分布式集群能使用访问数据的 IO 压力分散到各个节点，不会出现像本地缓存单节点瓶颈性能问题。</p><p></p><p>当然相对来说分布式缓存的集群的直接性能要比本地性能稍差一点，因为有网络开销，但是它能大大提高缓存的命中率进而提高存储的性能。另外，由于缓存数据能被共享 ，同样数据量的缓存成本只有本地缓存的 1/n(n为节点数)。在实际使用中，分布式缓存应该使用比远端存储更好的性能磁盘从而才能达到性能提升的效果(比如使用 ssd 和 nvme,比 HDD 高一个量级)。</p><p></p><p>下面我们来说一下分布式缓存集群的设计原理：</p><p></p><p>轩辕文件系统的分布式缓存集群实在客户端 vivo_juicefs 实现的。我们可以通过参数--cache-group 设置 vivo_juicefs 的缓存组。同一个局域网内相同缓存组的客户端，它们会把监听在内网 IP 的随机端口汇报给元数据服务，进而发现其它客户端，并通过内网通信共享缓存。每隔一段时间(可以通过参数设置)，vivo_juicefs 会向元数据服务发起请求，获取最新的缓存组信息，从而调整缓存组节点的信息。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f1799481b0d801a032b2ff97a2ff623.png\" /></p><p></p><p>如图所示，缓存组内的客户端会组成一个一致性哈希环。每个数据块都会根据一致性哈希计算出负责其存储的成员节点，当 vivo_juicefs 发起查询该数据的时候，会从该节点上获取数据(如果是本节点直接获取，如果是其他节点则通过 rpc)，如果数据尚未缓存在节点上，直接从远程存储集群读取并缓存在本地。缓存组内如果发生了成员节点增删，数据会向哈希环的临近节点做迁移（为了防止波动，实际会等待成员变更后约 5 分钟，方执行迁移操作）。可见缓存组的成员变更，只影响到少量数据块的缓存命中率。在缓存组一致性哈希环的实现中，也采用了虚拟节点（virtual node）的概念，确保数据分布均衡，避免因数据迁移产生访问热点，影响缓存组性能。</p><p></p><p>在缓存组的中，如果每个 vivo_juicefs 客户端都是参与分布式缓存集群的建设。如果遇到 vivo_juicefs 不是常驻的情况下，特别是 VTraining 平台下训练任务的 vivo_juicefs 客户端会不断销毁、重建，会让哈希环极其不稳定，缓存数据不断迁移，从而导致缓存集群利用率低。类似这种动态创建伸缩的计算集群， 我们可以创建独立的分布式缓存集群来解决上述问题。在同一个缓存中，可以存在两种不同角色的节点。当 vivo_juicefs 挂载参数中有--no-sharing 参数时，如同字面意思，这个节点虽然属于同一个缓存组，但却不分享自己的缓存数据，不会参与缓存哈希环的建设，只会向缓存集群索取数据。另外一种即不带参数--no-sharing 的节点则组成一个独立的分布式缓存集群，提供缓存数据给同一缓存组的 no-sharing 节点使用。这样动态创建伸缩的计算集群可以使用带--no-sharing 的 vivo_juicefs 客户端，然后专门准备固定数量的机器挂载 vivo_juicefs(不带--no-sharing)组成分布式缓存集群，由于计算集群的节点不参与缓存的建设，也就不会导致哈希缓存的组的成员变迁而影响整体缓存集群的效率，如图设计：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/493211b603e573d24fc8c9eb8c170103.png\" /></p><p></p><p></p><h4>缓存数据一致性</h4><p></p><p></p><p>轩辕文件存储只支持 close-to-open 一致性保证，即当两个及以上客户端同时读写相同的文件时，客户端 A 的修改在客户端 B 不一定能立即看到。但是，一旦这个文件在客户端 A 写入完成并关闭，之后在任何一个客户端重新打开该文件都可以保证能访问到最新写入的数据，不论是否在同一个节点。</p><p></p><p>我们在 close-to-open 一致性得基础上来讨论一下缓存数据是怎么和远端存储数据保持一致的。轩辕文件系统当前只支持缓存数据，每次访问数据之前，首先得到该数据得元数据，然后通过元数据先确定缓存中有没有该数据，有的话从缓存中读取，没有则从远端存储读取。我们这里用一个实例来说明。假设 damon.txt 文件在某一时刻是由对象 A、B、C 组成，这个信息记录在元数据中。我们要获取 damon.txt 文件，首先获取元数据知道它是由对象 A、B、C 组成，然后查缓存中有没有 A、B、C 对象缓存文件(对象映射到缓存系统是文件，文件名对应对象名，文件数据对应对象数据，一个对象名在挂载的文件系统中全局唯一)，有直接读取缓存文件返回，没有从远端存储访问。由于轩辕对象的元数据服务是支持事务的，所以对于访问之前的修改缓存都是可见的，因为元数据服务已经记录，符合 close-to-open 一致性。所以缓存数据的一致性是通过元数据服务提供的事务来保证的。</p><p></p><p></p><h2>轩辕存储系统缓存的应用</h2><p></p><p></p><p>在VTraining训练平台中，如图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a4e5047e1b550499be0514d74f80e06.png\" /></p><p></p><p>训练任务的客户端配置了内存缓存，本地缓存，并且和高性能的分布式缓存集群的客户端都隶属于一个缓存组，但是训练任务的客户端带--no-sharing 参数不参与分布式缓存的建设。这样训练任务对应有 3 级缓存：计算节点的内存缓存，计算节点的磁盘缓存和缓存组的分布式缓存，可以根据具体应用的访问特点配置各个层级的缓存介质和空间大小。</p><p></p><p>本地缓存和分布式缓存集群存应该尽量存储热数据，这样可以用最少的磁盘成本达到较高的缓存命中率。在 VTraining 平台中每个物理缓存节点都有一个叫 cache_service 的服务对缓存数据进行管理，它内部实现了一个高效的缓存淘汰策略，在磁盘容量不足的情况下能根据数据的冷热程度智能的淘汰数据，最终达成缓存数据都是热数据的目的。当前在 VTrianing 平台中，分布式缓存集群与远端存储的容量比大概为 1:50,但是缓存的命中率超过 90%，大大提高了访问性能，进而也提高了 Vtraining 训练集群 GPU 的利用率。在缓存策略全面上线后，各个业务的用户都反馈良好，训练效率都有显著的提升，下图展示了图像训练任务缓存使用前后效率的对比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/ded2d8e8f3e07f9e2094169410af68be.png\" /></p><p></p><p></p><h2>致谢</h2><p></p><p></p><p>感谢杭州果汁数据科技 JuiceFS 团队的苏锐、davies、朱唯唯等和 vivo 互联网基础平台存储团队的肖博、龚兵、于相洋、韩姜、储敏等对轩辕文件存储在 Vtraining 平台设计和落地过程中的大力支持。</p><p></p><p></p><p></p><p>作者介绍：</p><p></p><p>彭毅格，vivo AI 研究院计算平台组的资深工程师，曾就职于华为、深信服等公司；关注 K8s、容器、存储等领域。</p>",
    "publish_time": "2023-04-18 14:29:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云原生时代，Java会走向末路吗？",
    "url": "https://www.infoq.cn/article/FIqDkNzwO7ZgGA5gDD0U",
    "summary": "<p>唱衰 Java 的声音由来已久。</p><p></p><p>尤其是近几年，云原生时代的到来，软件的交付方式发生了根本性变化，Java 遭受了诸多质疑。</p><p>传统领域的 Java 开发，交付的是 Java 应用本身，具体体现在以 “jar”“war” 的形式交付，而云原生最佳实践，则是基于微服务形式，以容器为基本交付单位，并在 K8S 中编排。云原生应用要求更快速地启动、强调资源按量消费，弹性扩展，以及可观测性等。就这一层面而言，在云原生时代，Java 的缺陷确实是天然存在的。</p><p></p><p>“事实上，Java 技术在云原生时代也在不停地进化。”&nbsp;面对唱衰 Java 的诸多论调，龙蜥社区 Java 语言和虚拟机 SIG 成员、龙蜥社区 RISC-V SIG 成员、阿里云程序语言与编译器团队负责人、Java Champion 李三红选择理性看待。</p><p></p><p>“比如更多支持容器部署的特性已经加入到 OpenJDK 版本。一直被人诟病的 Java 启动慢的问题，目前基于 OpenJDK 技术的几个创新项目正在多个不同方向探索，包括 CRaC&nbsp;(Coordinated Restore at Checkpoint)&nbsp;、Leyden，以及由阿里和 Google 作为主要贡献者的 Fast Startup Incubator 等项目。在资源弹性使用方面，由 Alibaba Dragonwell 提供的 ElasticHeap 功能，主要目的是解决云计算环境下 Java 内存资源使用的弹性问题。最后，OpenJDK 的 JFR 以及 JFR Streaming 技术为构建 Java 云原生可观测工具提供了重要的技术支撑。”</p><p>&nbsp;</p><p>Java 面临的挑战，不仅仅是云原生。</p><p></p><p>李三红认为，从 1995 年 Java 1.0 发布算起， Java 技术这二十多年的发展，大致存在一明一暗两条线的驱动。“一条暗线，是指 Java 或者说支撑 Java 的底层 JVM 技术适配计算机架构的演进与发展。一条明线，是指 Java 作为一个开发者工具，本质是要面向业务领域解决业务问题的，所以自然而然地推动了 Java 在云原生，AI 等方向的演进，解决这些领域内碰到的问题与挑战。”</p><p></p><p>后摩尔时代，算力增长放缓，更多利用多核、SIMD（单指令多数据流）等并行计算技术，以及异构来释放更大的算力。相较于 C/C++ 这些传统编译型语言，Java 处在软件栈的更高抽象级别，自带 Java 标准库，以及运行时环境，这也给 Java 创新带来了更多的空间以及可能性。</p><p></p><p>李三红指出， Java 在多核、异构加速领域做了多方面的探索，适应与优化。比如，OpenJDK 孵化的 Vector API 项目，依赖 CPU 的 SIMD 指令，获得计算性能的成倍提升。即将发布的 OpenJDK 19 引入了 Virtual Threads&nbsp;(Preview)，旨在帮助 Java 开发者高效处理并发&nbsp;(尤其针对 IO 密集型场景)&nbsp;。而在异构领域，早 2014 年 JVM 技术峰会，AMD 就分享了 Sumatra 项目，尝试实现 JVM 与 Heterogeneous System Architecture 目标硬件交互。由 The University of Manchester 发起的 TornadoVM 项目，目标是帮助 Java 开发者不需要了解 GPU 编程语言或者相关的 GPU 体系结构知识就可以编写面向异构的并行程序。</p><p></p><p>在 AI 方向上，Java 也在与时俱进。据李三红介绍，在企业计算领域，Java 是被使用最多的语言之一，但对于机器学习领域的开发，Java 一直缺乏标准支持，这个方向其实在 JCP-EC 讨论也比较多。</p><p>基于 Java SE 技术，在 JCP 流程内推动并最终在 2022 年定稿的 JSR 381 规范，其目标就是为不同领域的 Java 机器学习开发提供通用的可重用设计。JSR 381 定义了标准的 Java API，提供了基本机器学习、图像分类和对象识别方面的处理能力。“依赖于不同的机器学习平台，如 TensorFlow、MXNet 以及 DeepNett 等，JSR-381 提供了不同的实现。对于 Java 生态内的开发者来说，不必再去学习 Python, 可以依赖 JSR-381 VisRec API 去构建你的 AI 应用。”</p><p></p><p>现实中，Java 应用的版本升级是较为缓慢的。Java 11&nbsp;(OpenJDK11）距离 2018 年发布已经过去四年多，目前国内大多数的用户仍然停留在 Java 8。李三红认为，动力不足是多方面的，对开发者来说最直接的原因可能是担心升级后兼容性带来的稳定性问题，会直接影响业务的连续性。</p><p></p><p>这种问题并不罕见。令人振奋的是，处于 Java 生态中的企业正在贡献自己的力量。阿里内部在大规模地往 Java 11、 Java 17 迁移的时候，总结了不少的经验，并且将这些经验通过工具的方式沉淀下来。最后阿里开源了 EMT4J&nbsp;(Eclipse Migration Toolkit for Java)&nbsp;，能够帮助 Java 应用无缝升级最新版本 JDK， 主要支持从 Java 8 到 Java 11，以及 17 的升级。</p><p></p><p>李三红还补充道，对于 Java 版本的升级问题，还可以从另一个角度 ——Software Sustainability 来进一步探讨。</p><p></p><p>“由 Titus Winter 等编写的《Software Engineering at Google - Lessons Learned from Programming Over Time》一书中，谈到了组织的 Codebase Sustainability 概念，强调了两个核心理念：第一，无论应对的是技术需求，还是业务需求，软件代码应当可以做一切应该做的改变。第二，这些改变带来的影响是安全的。</p><p></p><p>“回到 Java 版本升级这个问题，我们在开发 Java 应用的时候，建议应用架构师们把 Java 版本升级纳入到 Software Sustainability 这个维度下考量，对代码开发规范进行相关的约束。例如，不要让你的代码依赖 JDK 内部不公开的 API，不要让你的实现依赖特定的 JDK 版本行为，不要使用被 Deprecated 的 API 等等。架构的目标应当考虑 Code Sustainability，让你的 Java 应用可以在任何时候根据实际需要平滑升级到不同 JDK 版本，不应当因为代码缺乏 Sustainability 而导致的尽量少的版本升级。”</p><p></p><p>李三红对 Java 的未来充满信心，源于他在 JVM 领域耕耘多年，不仅深入了解 Java 特性，并且有能力进行创新性研究。</p><p></p><p>在加入阿里之前，李三红一直在 IBM Java 技术中心，参与 J9 虚拟机开发，期间领导了 JVM 多租户项目。目前就职于阿里云，领导程序语言与编译器团队，主要的工作是结合阿里、蚂蚁及云上各业务的需求，在编译器、语言运行时等基础领域进行研究创新。编程语言是基础软件的核心，也是龙蜥技术生态的八大方向之一，Dragonwell 是龙蜥社区 Java 语言和虚拟机 SIG下的项目。目前，在语言工具链这块，已经形成 Alibaba Dragonwell&nbsp;(Java 生态)，Alibaba Cloud Compiler&nbsp;(C++ 生态)&nbsp;等多个产品来支撑其业务，语言工具链相关的开源技术也在为龙蜥社区的开发者提供支持。</p><p></p><p>2020 年，李三红获得了 Java 技术领导者社区 Java Champions 推荐，被授予 Java Champion 荣誉。Java Champion 由 Java 社区成员提名，并且必须得到现有 Java Champions 成员的一致同意。唯有为 Java / JVM 生态系统做出重要贡献的专家才能获此荣誉。</p><p></p><p>去年，龙蜥社区理事长单位<a href=\"http://mp.weixin.qq.com/s?__biz=Mzg4MTMyMTUwMQ==&amp;mid=2247510181&amp;idx=1&amp;sn=b88e4c10a93989e1ff6380075bae082b&amp;chksm=cf655bd7f812d2c152469034f3c6cf2701df78f74c988e19d2a93384d73308cc39d7c81241fa&amp;scene=21#wechat_redirect\">阿里云第三次入选 JCP 最高执行委员会 (JCP-EC)</a>\"，作为阿里云在 JCP-EC 的代表，李三红一直在参与 JCP-EC 领导下的相关 Java 标准讨论制定工作。</p>",
    "publish_time": "2023-04-18 15:33:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]