[
  {
    "title": "无侵入增强 Istio，网易数帆践行这三条原则",
    "url": "https://www.infoq.cn/article/CSU0NV5SHOEHMRXJkiyJ",
    "summary": "<p>在云原生社区近日主办的 Service Mesh Summit 2022 服务网格峰会上，网易数帆云原生技术专家方志恒分享了轻舟服务网格无侵入增强 Istio 的经验，本文据此次分享整理，介绍了对无侵入和实现的思考，轻舟服务网格演进过程中的扩展增强，以及这些扩展增强和无侵入的关系。这里“无侵入”强调的是对服务网格基础设施本身的无侵入，而不是只有对业务的无侵入，后者是服务网格本身的定位所要考虑的内容。</p><p></p><h2>服务网格维护中的无侵入</h2><p></p><p></p><p>关于无侵入，我们从各自的实践经验也知道，做无侵入的增强是非常困难的。原因有很多，比如说我们可能要做业务的适配，快速落地，定制的一些需求等，业务以及项目周期的压力，迫使我们不得不做出一些有侵入的选择。但是我们为什么还要去强调无侵入呢？因为对于我们维护团队，或者说对于我们自己这个技术方案的维护方来说，每一分侵入都会有每一分的成本，它会在后续会体现出来，比如说在做长期维护，做版本演进，去做社区的一些功能和新版本的对齐，我们都需要去解决我们的改动和社区的主干分支之间的冲突。</p><p></p><p>因此，我们需要在我们的研发流程里面去贯彻这样一个目标或者理念：这个方案是不是做到无侵入了，如果没有做到的话，那做到的程度是怎么样的？这样可能才得到一个“求上得中”的效果，就是坚持无侵入，我们才可能做到比较低的侵入。</p><p></p><p>在使用社区的方案去做定制开发、去演进的过程中，我们认为，有一些比较合适的用来去做维护的思路，最合适的方式，是直接使用社区原生的 API 提供的扩展点去做一些无侵入的扩展，这是最理想的情况。当然，社区的一些扩展的 API 可能无法完全满足我们的需求，这个时候我们可以在上层去做一个封装，业界的一些落地的实践案例也体现出这样的理念。这里引用一句名言：计算机科学领域的任何问题，都可以通过增加一个中间层来解决。</p><p></p><p>即使是这样，我们出于性能的考虑，或者出于特性的考虑，很多时候还是会面临一些不得不去做修改的情况。我们有第二个原则，就是把扩展增强的内容去做一些封装在一个单独的库里面，这样可以做到最小的修改和替换。这也是一个比较朴素的工程经验。</p><p></p><p>第三点，如果我们确实要做比较大的一个修改，这个时候我们可以尽量去贯彻一个理念，就是要对社区原生的一些设计思路和特性做一致性的对齐。这里我分享一个“撸猫原则”：如果我们非要去撸一只猫的话，最好顺着它的毛去撸，否则我们可能会把它的毛搞得很乱，甚至它还会反过来咬我们一口。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/df/df8821814a68b6dc017302e53a1beafa\" /></p><p></p><h2>服务和配置的扩展</h2><p></p><p></p><p>首先介绍我们对 Istio 的服务和配置的扩展。</p><p></p><h3>配置</h3><p></p><p></p><p>Istio 社区已经提供支持多 configSource，并给出了一个协议叫 MCP-over-xds，通过这种方式我们可以从不同的数据源去拿到所需的配置。</p><p></p><p><code lang=\"javascript\">configSources:\n- address: xds://mesh-registry.istio-system.svc:16010?type=serviceentry&amp;type=sidecar\n- address: k8s://\n</code></p><p></p><p>这里给出一个配置样例，是通过 xDS 协议去向某一个服务去获取它的配置，同时也可以去 Kubernetes 去获取那些标准的 CR，这是它的一个扩展的方式。</p><p></p><p>我们的 URL 跟官方的稍微有点不一样，是在这基础上稍微做了一点改进，实现了一个叫做 Istio-MCP 的库平替社区原生的 adsc，这就是前面说的第二个原则。在这个库里面，我们实现了 xDS 的增量推送，更增强了一个 revision 的机制，Istio 支持按照 revision 去获取自己感兴趣的配置，我们对此做了增强。我们还做了一个更灵活的分派，允许在 configSource 里面去指定一个类型，相当于从不同的 configSource 去获取不同类型的配置，这个很多时候都是实践的需要。</p><p></p><h3>服务</h3><p></p><p></p><p>服务这部分，在网易数帆的场景里面，比较广泛地应用到了 ServiceEntry。Istio 对 Kubernetes 服务的支持很好，大部分情况下无需做额外的扩展，但是因为它的定位，Istio 把对非 Kubernetes 服务的支持几乎都留给了它的一个扩展点，就是 ServiceEntry 这个 API，以及前面所说的配置扩展的方式。通过这种方式，Istio 允许第三方来作为配置和服务的提供员，来提供其他的服务模型。当然在这个过程中，第三方需要自己去完成服务模型的转换，因为 ServiceEntry 几乎就是 Istio 内部服务模型的 API 版本，你可以认为它是 Istio Service 模型。</p><p></p><p>我们也有一个单独的组件，叫做 mesh-registry，实现了 MCP-over-xds 的协议，作为一个 MCP server。在这个组件内部，我们支持了不同的服务模型的转换，包括将 Dubbo/ZK、Eureka、Nacos 去转换成标准的 ServiceEntry，然后下发。</p><p></p><p>这是在服务这一块的扩展。从目前来说，以上两部分的扩展方式都可以说是无侵入的，是基于社区原生接口和协议的扩展。</p><p></p><h2>插件的扩展</h2><p></p><p></p><p>第二部分是插件的扩展。Istio 的功能确实很丰富，这导致它在市场上成为主流，使用的人很多。但很多人使用同时也意味着，即使 Istio 的能力再丰富，它也无法覆盖所有用户的场景，就会需要这种扩展机制。</p><p></p><h3>EnvoyFilter</h3><p></p><p></p><p>Istio 社区的扩展方式是一个比较典型的 EnvoyFilter 的方式。这种方式，configPatches 进去的内容是 Envoy 的一个一个 filter 的配置，Envoy 具体的内容我们可以先忽略，先看一下 applyTo、context、match、routeConfiguration、vhost 这一堆东西。</p><p></p><p>举个例子，我们要做一个限流的功能，因为它是一个业务功能，作为使用者，我们要知道限流 API 的业务语义，首先需要去看 Envoy 的限流插件它的 API 是怎么样的，跟上层对限流的业务需求是不是对得上，再确定我应该怎么样去写一个限流的插件的配置。到这一步还只是完成了要 Patch 进去的内容，要写出这个 EnvoyFilter 的时候，我们还需要去了解更多的东西，比如这里的 applyTo、HTTP_ROUTE 以及 vhost 等。如果是其他的类型的 Patch，可能还有其他的概念。</p><p></p><p><code lang=\"properties\">apiVersion: networking.istio.io/v1alpha3\nkind: EnvoyFilter\nmetadata:\nname: bookinfo-gateway-sampling\nnamespace: istio-system\nspec:\nconfigPatches:\n- applyTo: HTTP_ROUTE\n   match:\n   context: GATEWAY\n   routeConfiguration:\n    portNumber: 80\n    vhost:\n     name: \"*:80\"\n   patch:\n    operation: MERGE\n    value:\n\n</code></p><p></p><p>这里面有一个本质的问题，filter 是 Istio 提供的一个几乎是纯数据结构级别的 Patch 机制，它直接操作 Istio 下发给 Envoy 的 xDS 配置，它的数据结构的描述、定义和类型都是 Envoy 侧的一些概念，比如 vhost，这就意味着 Istio 的使用者需要深入了解 Envoy 侧的概念。同时还有一些的灰色地带，举个例子，如果我们要给一个 vhost 去 Patch 一些东西，就要知道 vhost name，而 vhost name 是 Istio 自己的一个实现，纯粹实现层面的东西，相当于说上层的使用者还要知道某一个版本的 Istio，它的 vhost name 是通过什么规则拼起来的。我们会认为对使用者来说负担会比较多，一个比较理想的做法是，他既然是 Istio 的使用者，那么他接触到的应该尽量是 Istio 层面的一些语义。我们对它做增强的思路就是这样的。</p><p></p><p>下面是轻舟服务网格做的一个比较浅的封装，但是在我们内部用得很多，所以我们认为它解决了一些实际问题。这个字段描述我们这个插件要去作用于网关，作用于某一个 host，作用于某一条路由，也就是说我们会尽量用 Istio 层面的语义来做这种类似的封装，帮用户转成下层的 Envoy 语义。</p><p></p><p><code lang=\"properties\">apiVersion: microservice.slime.io/v1alpha1\nkind: EnvoyPlugin\nmetadata:\nname: reviews-ep\nnamespace: istio-samples\nspec:\nworkloadSelector:\n  labels:\n   app: reviews\ngateway:\n- gateway-system/prod-gateway\nhost:\n- reviews.istio-samples.svc.cluster.local\nroute:\n  - ratings.istio-samples.svc.cluster.local:80/default\n  - prefix-route1\nplugins:\n- name: envoy.filters.network.ratelimit\n   enable: true\n   inline:\n    settings:\n     {{plugin_settings}} # plugin settings\n</code></p><p></p><p>基于同样的思路，我们还做了一个限流的模块。但这里不是为了讲限流，而是说我们怎么去做上层的业务语义描述。限流这个功能有点特别，可以做得很复杂，所以 Envoy 提供了一个非常灵活的 API，这带的一个问题是，别说 Istio 的用户，就是 Istio 的维护者自己要把 Envoy 限流 API 看明白，都需要付出较多的时间和精力。所以，我们希望把它做得简化一点，更接近业务语义描述，这也是一个复杂度的消化——Envoy 做得非常灵活，它什么都可以做，但复杂度不会凭空消失，中间需要肯定有一层实现业务语义到底层的灵活能力的映射。</p><p></p><p><code lang=\"properties\">apiVersion: microservice.slime.io/v1alpha2\nkind: SmartLimiter\nmetadata:\nname: review\nnamespace: default\nspec:\nsets:\n  v1:\n   descriptor:\n   - action:\n      fill_interval:\n       seconds: 1\n      quota: \"10\"\n      strategy: \"single\"\n     condition: \"{{.v1.cpu.sum}}&gt;10\"\n     target:\n      port: 9080\n</code></p><p></p><h3>Rider 插件扩展</h3><p></p><p></p><p>插件扩展的第二大类是我们的 Rider 的插件，Rider 比较像 Envoy 版本的 OpenResty，Envoy 本身有支持 Lua 的插件，但是它的支持比较简单，里面的 API 比较少，熟悉 OpenResty 的同学应该知道，我们写 Lua 和 OpenResty 是完全不一样的，因为 OpenResty 提供了很丰富的，跟网络操作、跟 Nginx 内部 API 做交互的 API，让我们很容易去做实际业务功能的开发，你无法想象我们纯粹用 Lua 去开发一个 HTTP_SERVER。基于这个背景，我们对 Lua 做了一个增强，在 Rider 插件提供了比较丰富的 Lua 交互的 API，让用户可以相对容易地在里面去实现一个上层的业务和治理的功能。另外，我们也对原生的 Lua 插件实现做了一些性能优化，相比来说 Rider 是有一定的性能优势的。</p><p></p><p>Rider 和原生的 Envoy Lua 插件的对比，是我们可以支持插件的配置。这里的配置是指类似于 Envoy 的 WASM 或者 Lua 插件，都是分成两部分，一部分是下发一个可执行的内容，无论是 Lua 脚本还是 WASM 二进制，都是一个插件实际执行的逻辑，我们还可以给它一份配置，这个配置跟执行内容两相结合，形成最终的业务行为。</p><p></p><p>社区的 Lua 不支持如路由级别的插件配置，这导致它的行为比较死，比较 hardcode，我们支持它的插件配置，支持更多的 API，性能也会更好一点。</p><p></p><p>现在 WASM 是一个很火的概念，我们也跟 WASM 做了一个对比，第一点是 Lua 跟 WASM 的对比，作为脚本语言，Lua 更加可见即可得，虽然 WASM 也可以从脚本编译而得，但如果用脚本转成 WASM 再做下发，WASM 编译语言性能损耗更小的优势就没有了。第二点是便于分发，因为没有编译的过程。第三点也是支持更多的 API，原生的 Envoy WASM API 确实会少一些。最后一点是更好的性能，我们原以为即使考虑上 LuaJIT，Rider 的性能也不会更好，但实际上 Envoy WASM API 的实现导致 Envoy WASM 跟 Envoy 内部交互的成本略高，所以测出来它的性能反而比 Rider 要更差一点。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0a/0a7d98ee15710e5618d67370c8a24192.png\" /></p><p></p><p>这是我们实际上测出来的一个对比，我们分别模拟三种场景下，一个很简单的，一个中等复杂度的，还有一个稍微高一点复杂度的，不同方案的性能差别，从图中可以都可以看到，三种场景下 Rider 插件的性能都是好于 WASM C++ 的，尤其是在复杂场景，大概有 10% 左右的提升，当然这三者相比于原生插件都有不小的差别。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0e/0e84310186a636935b4eedeaca53bed7.png\" /></p><p></p><p>这里给出了一个我们提供的 API 的列表，可以看到风格还是非常的 Resty 的。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9b/9b50ba577abaa6360ec4245f79a73e87.png\" /></p><p></p><p>我们做的 RiderPlugin，这个东西比较有意思，下面是一个 WASM 的样例，大家可以看到它的 API 的数据结构，完全是一个 WASM 的 API，但实际上做的事情，是通过镜像的方式把我们的 Rider 插件给下发了，再分发到数据面，效果是将 HTTP 请求的那个 response 的 body 改成了 C++ is awesome。所以说，使用上不能说差不多，简直是一模一样。</p><p></p><p><code lang=\"properties\"># wasm_plugin.yaml\napiVersion: extensions.istio.io/v1alpha1\nkind: WasmPlugin\nmetadata:\nname: test1.rider\nspec:\nimagePullPolicy: IfNotPresent\nimagePullSecret: qingzhou-secret\npluginConfig:\n  destination: Body\n  message: C++ is awesome!!\n  source: Static\nselector:\n  matchLabels:\n   app: reviews\nsha256: nil\nurl: oci://slimeio/rider_plugin:0.1.0\n</code></p><p></p><p>我当时看到 Istio 的 WasmPlugin 这个 API 的时候，是有点惊讶的，第一是不知道它为什么要做一个 WasmPlugin，而不是做一个通用的插件分发机制，有点过于耦合了；第二是我发现它字段的设计就是一个通用的插件分发，但是它的名字就叫做 WasmPlugin，当然它在实现的时候也是这样的。所以，我们完全可以用这个 API 来实现我们的 Rider 插件的分发，这个是我们目前已有的一个特性。当然我们还另外设计了一个 RiderPlugin CRD，主要是考虑到后续 Rider 可能提供更丰富的功能，会有更多的字段。我们在实现这个支持的时候，不能说没有侵入，但真的是只改了一点点——它的那些插件分发，包括 Pilot agent 里面，怎么从镜像里面去提取 WASM 文件，我们几乎按同样的规范去定义 Rider 的镜像，去在里面放我们的 Rider 的插件的配置，Lua 的文件，几乎是完全一样的。这就是前面提到的另一个原则，如果我们非要去做一些新的功能，可以做得跟原生的比较像，这样的话无论是我们自己还是用户，都很容易上手。</p><p></p><p>我们最终下发给 Envoy 的一个数据结构，和 WASM 是有一些差异的，这个差异是在实现里面去做了一个屏蔽，本质上说，我们只是在最后生成下发给 Envoy 的数据的时候对内容做了一些修改，让它是一个 Rider 的格式。</p><p></p><p><code lang=\"properties\">\nroute_config:\nname: local_route\nvirtual_hosts:\n- name: local_service\n  domains:\n  - \"*\"\n  # Plugin config here applies to the VirtualHost\n  #\n  # typed_per_filter_config:\n  # proxy.filters.http.rider:\n  # \"@type\": type.googleapis.com/proxy.filters.http.rider.v3alpha1.RouteFilterConfig\n  # plugins:\n  routes:\n  - match:\n     prefix: \"/static-to-header\"\n    route:\n     cluster: web_service\n    # Plugin config here applies to the Route\n    #\n    # plugins is a list of plugin route configs. Each entry has a name and its config.\n    # The filter will look up the list by order given a plugin name, and use the first match entry.\n    typed_per_filter_config:\n     proxy.filters.http.rider:\n      \"@type\": type.googleapis.com/proxy.filters.http.rider.v3alpha1.RouteFilterConfig\n      plugins:\n       - name: echo\n         config:\n          message: \"Lua is awesome!\"\n          source: Static\n          destination: Header\n          header_name: x-echo-foo\n    http_filters:\n    - name: proxy.filters.http.rider\n      typed_config:\n       \"@type\": type.googleapis.com/proxy.filters.http.rider.v3alpha1.FilterConfig\n       plugin:\n        vm_config:\n         package_path: \"/usr/local/lib/rider/?/init.lua;/usr/local/lib/rider/?.lua;\"\n        code:\n         local:\n          filename: /usr/local/lib/rider/examples/echo/echo.lua\n        name: echo\n        config:\n         message: \"C++ is awesome!\"\n         source: Static\n         destination: Body\n    - name: envoy.filters.http.router\n      typed_config: {}\n\n</code></p><p></p><p>基于此我们还做了前面提到的一个 EnvoyPlugin 的插件，对插件分发做了一个比较浅的上层封装，也加入了对 WASM、Rider 的支持，将会在近期 release。</p><p></p><h3>Dubbo 协议扩展</h3><p></p><p></p><p>接下来介绍协议的扩展。我们第一个做的是 Dubbo 协议扩展，因为 Dubbo 在国内确实使用很广泛，同时我们还有一些特殊的考量，稍后再详解。</p><p></p><h4>数据面</h4><p></p><p></p><p>首先看数据面的部分。第一，我们做了比较丰富的七层的 dubbo filters，Istio + Envoy 这套体系的大部分治理功能都是通过七层插件来实现的，HTTP 的比较丰富的治理功能有很多七层插件，所以我们也实现了很多 Dubbo 的插件。</p><p></p><p><code lang=\"perl\">\"typed_per_filter_config\": {\n\"proxy.filters.dubbo.locallimit\": {\n  \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\n  \"type_url\": \"type.googleapis.com/proxy.filters.dubbo.local_limit.v2.\\\n    ProtoCommonConfig\",\n  \"value\": {\n</code></p><p></p><p>第二，我们实现了一个 DRDS，也就是 Dubbo RDS，因为 Envoy RDS 几乎等同于 HTTP RDS，只定义了 HTTP 协议相关的路由的数据结构，我们要实现一个比较灵活的路由，以及性能比较好的路由分发，就需要定义一个单独的 xDS 资源类型。</p><p></p><p>第三，我们还做了一个 Dubbo 协议嗅探，这是有一些特殊的场景，Istio 社区对协议嗅探有一部分的的支持，它的本意不是为了实现一个很丰富的基于协议嗅探的治理，而是为了解决一部分场景需求，相当于我们引入一个代理之后，会有一些场景需要通过特殊的技术手段去解决，而我们在实际生产中会面临 HTTP 的、TCP 的、Dubbo 的协议，也需要用同样的机制去解决它，所以我们也支持 Dubbo 的协议嗅探。其中技术细节非常多，这里就不展开。</p><p></p><h4>控制面</h4><p></p><p></p><p>控制面的改动更多，用户会更可感知。我们的支持比较特殊，因为我们实现了跟 HTTP 几乎完全同等语义的 Istio API，体现为 VS/DR，就是基本的治理。还有比较丰富的治理，是那些七层的 filter，还有 Istio 这一层所抽象出来的认证鉴权的策略是只作用于 HTTP 的，我们也是对它做了 Dubbo 的支持。还有一个比较特殊的就是 Sidecar，Istio 有一个资源是用来描述应用或者服务之间的依赖关系的，这样就可以实现按需下发，像配置瘦身、推送范围的影响，都可以得到一个比较好的优化，我们对此也做了 Dubbo 的支持，相当于用标准的 API 去支持 Dubbo 的类似于服务依赖描述和按需下发。还有一个是 EnvoyFilter，我们也支持了用 EnvoyFilter 的标准的 API 去做 Dubbo 插件的分发。</p><p></p><p><code lang=\"properties\">- applyTo: DUBBO_FILTER\n  match:\n   context: SIDECAR_OUTBOUND\n   listener:\n    filterChain:\n     filter:\n      name: envoy.filters.network.dubbo_proxy\n      subFilter:\n       name: envoy.filters.dubbo.router\n  patch:\n   operation: INSERT_BEFORE\n   value:\n    config:\n     '@type': type.googleapis.com/udpa.type.v1.TypedStruct\n     type_url: type.googleapis.com/proxy.filters.dubbo.traffic_mark.v2.ProtoCommonConfig\n</code></p><p></p><h4>通用七层扩展框架</h4><p></p><p></p><p>我们也做了通用的七层扩展框架的支持，我们在 Envoy 社区的 Maintainer 也和国内同行沟通过，共同努力推进，目前已经合入 Envoy 社区版本，也就是说在数据面是有通用七层扩展框架的支持的，后续 Istio 社区相关的支持，我觉得也是可以期待的。</p><p></p><p>这里简要展开一下我个人对通用七层扩展框架的理解。我们服务网格多协议适配以及长期维护的成本很高，每接入一个新的协议，都需要去做一个额外的适配。在 Envoy 视角来说，支持一个新的协议需要做的事情，首先是对协议基本的编解码和协议流程的支持，这是协议内部的东西，肯定要做的；除此之外，还涉及到它要跟 Envoy 原有流程的对接，比如 Cluster 是不是可以用，路由是怎么生效的，流量的分派从 listener 进来怎么走到协议解析器或者是四层的 filter，也就是说，一个新的协议的支持要做很多重复性的事情，比如跟原有的机制相结合，这部分重复的工作量我们是可以省去的。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/96/96bec6198342c1bad1c486e5eb5fc94f.png\" /></p><p></p><p>从服务代理和服务治理的视角来说，很多时候我们不关心它是什么协议，服务治理可以理解为一个基于特征的流量分配或者是流量处理，也就是说通常我们关心的是它的特征能否用一种比较通用的方式去描述，就好比我们在做协议设计的时候，可能都会塞进去一个字段，叫做 metadata，HTTP 的协议里面的 header 其实就是一种 metadata，如果能用一个比较简单的通用模型去描述我们所有协议的特征，我们就可以基于这个模型去做服务治理，剩下的事情就是把已有的协议来转化成这个通用模型。当然，所有的这种加一个简单的中间层去做一个复杂度的屏蔽，都会有一个问题，就是无法感知我们抽象出来的共性以外的东西。</p><p></p><p>举个例子，如果我们要支持 HTTP2，想做一些相对下层的治理，可能会比较困难，因为感知不到 stream、frame 等等，能感知到的就只有它这个抽象。这就是我觉得所有类似的技术方案都有同样的问题。但是一个技术方案的价值，肯定是在它带来的收益减去副作用之后的，如果我们觉得还不错，就可以继续去使用它。</p><p></p><p>我们目前在用这个通用框架去实现 Dubbo。以前 Dubbo 之所以没有进 Istio 社区，是因为 Istio 社区并不想维护一个特定的协议，即使该协议国内用户比较多，而且因为年代的原因，Dubbo 是不那么云原生的。但是如果我们这里引入的不是 Dubbo，而是一个通用的七层框架，那应该比较乐观一些。所以我们后续会替换原有的 Dubbo Proxy，就是数据面这一块，同时也会尝试跟相关方去推动控制面的接入，看能不能进入 Istio 社区。</p><p></p><h2>Slime 开源项目的集成</h2><p></p><p></p><p>上述的很多扩展增强，都已经沉淀在我们开源的 Slime 项目（github.com/slime-io/slime）里面了。这个项目已经进入 Istio 生态，我们对 Istio 的增强，或者是说魔改也好，或者是说生态丰富也好，都会放进 Slime 项目里面，它是一个 group，里面包含比较多的子项目。这里简单介绍 Slime 最近的一些进展。首先在架构层面，我们做了比较彻底的模块化设计，可以快速地去对 Slime 做上层功能的扩充。</p><p></p><p>我们定义了一个比较明确的框架层来管理上层的模块，同时也提供一些基础能力给上层，包含定义的一些 metric 框架，让上层可以用很少的代码去获取到一些像 Kubernetes 乃至更多类型的 metric 信息，并且对它做一些处理。同时我们也做了多集群的支持——很多时候我们之所以做一些东西，是因为 Istio 的多个增强功能都需要去做相关的支持，我们就会把它放到框架层——在这个框架层我们直接做了与 Istio 原生比较一致的多集群感知，社区叫做 multi cluster discovery。</p><p></p><p>我们还支持了一个统一的服务模型数据，支持了 Kubernetes Services 和 ServiceEntry，两个消化完以后处理为内部的一个数据类型。我们之前有了解到，一些同行的朋友在调研技术方案的时候，发现我们只支持 Kubernetes 就没有继续了。比如懒加载的方案，我们现在已经做了一个比较彻底的支持。</p><p></p><p>还有一个模块聚合和管理的能力，最终的形态大概是这样，只要手动写几行代码，把每一个模块直接放进来就可以了。</p><p></p><p><code lang=\"cpp\">func main() {\n   module.Main(\"bundle\", []module.Module{\n        &amp;limitermod.Module{},\n        &amp;pluginmod.Module{},\n        &amp;pilotadminmod.Module{},\n        &amp;sidecarmgrmod.Module{},\n        &amp;tracetiomod.Module{},\n        &amp;meshregistrymod.Module{},\n   })\n}\n</code></p><p></p><p>我们在最近一年多做了很多的模块，比如前面提到的 meshregistry、pilotadmin、sidecarmgr、tracetio，其中里面有一部分已经开源出去了，还有一部分因为耦合了一些业务逻辑，可能会晚一点开源。</p><p></p><p>稍微重点说一下，我们有一个子项目叫做 i9s（github.com/slime-io/i9s），是从另一个开源项目 K9s fork 过来的。熟悉 Kubernetes 的同学可能会用过 K9s ，它提供一种交互式的方式，我们觉得它很好用。同时我们也想到，对于 Istio 的运维管理，有很多地方也可以用这种方式来实现，使用起来会更方便一些。i9s 本质上是用 K9s 这种交互式的视图去展示 Istio 的各种内部信息，比如可以去查看 Istio 上面连接哪些 Sidecar，每个 Sidecar 的运行状态，下发的配置，它的配置是否和应有的配置一致，我们可以去 watch 推送的频率，以及推送的时延，等等。</p><p></p><p>Slime 项目后续会开放更多服务网格管理的能力，期待大家共建社区。谢谢！</p><p></p><p>延伸阅读：</p><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU4MDM1MDAwOQ==&amp;mid=2247486008&amp;idx=1&amp;sn=a99b06d8acd007ce8bd2f27d4084a4c9&amp;chksm=fd59736eca2efa78a9041779287b986cfa085b90a59693bc1bbd78627526334af45691363022&amp;scene=21#wechat_redirect\">IstioCon 回顾 | 网易数帆的 Istio 推送性能优化经验</a>\"Slime 项目：<a href=\"https://github.com/slime-io/slime\">https://github.com/slime-io/slime</a>\"i9s 子项目：<a href=\"https://github.com/slime-io/i9s\">https://github.com/slime-io/i9s</a>\"Rider 插件框架：<a href=\"https://github.com/hango-io/rider\">https://github.com/hango-io/rider</a>\"</p><p></p><p>作者介绍：</p><p></p><p>方志恒，网易数帆云原生技术专家，负责轻舟 Service Mesh，先后参与多家科技公司 Service Mesh 建设及相关产品演进。从事多年基础架构、中间件研发，有较丰富的 Istio 管理维护、功能拓展和性能优化经验。</p>",
    "publish_time": "2022-10-17 10:10:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "依赖重、扩展差，字节跳动是如何优化Apache Atlas 实时消息同步的？",
    "url": "https://www.infoq.cn/article/BM7PS9IyFSy9fvWh3Dg9",
    "summary": "<p></p><h2>摘要</h2><p></p><p></p><p>字节数据中台 DataLeap 的 Data Catalog 系统通过接收 MQ 中的近实时消息来同步部分元数据。Apache Atlas 对于实时消息的消费处理不满足性能要求，内部使用 Flink 任务的处理方案在 ToB 场景中也存在诸多限制，所以团队自研了轻量级异步消息处理框架，很好地支持了字节内部和火山引擎上同步元数据的诉求。本文定义了需求场景，并详细介绍框架的设计与实现。</p><p></p><h2>背景</h2><p></p><p></p><h3>动机</h3><p></p><p></p><p>字节数据中台 DataLeap 的 Data Catalog 系统基于 Apache Atlas 搭建，其中 Atlas 通过 Kafka 获取外部系统的元数据变更消息。在开源版本中，每台服务器支持的 Kafka Consumer 数量有限，在每日百万级消息体量下，经常有长延时等问题，影响用户体验。在 2020 年底，我们针对 Atlas 的消息消费部分做了重构，将消息的消费和处理从后端服务中剥离出来，并编写了 Flink 任务承担这部分工作，比较好的解决了扩展性和性能问题。然而，到 2021 年年中，团队开始重点投入私有化部署和火山引擎公有云业务的支持，对于 Flink 集群的依赖引入了可维护性的痛点。在仔细分析了使用场景和需求，并调研了现成的解决方案后，我们决定投入人力自研一个消息处理框架。当前这个框架很好支持了字节内部以及 ToB 场景中 Data Catalog 对于消息消费和处理的场景。本文会详细介绍框架解决的问题，整体的设计以及实现中的关键决定。</p><p></p><h3>需求定义</h3><p></p><p></p><p>使用下面的表格将具体场景定义清楚。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0e/0e11ac17d65456698f59b0ec5ba60ccf.png\" /></p><p></p><h3>相关工作</h3><p></p><p></p><p>在启动自研之前，我们评估了两个比较相关的方案，分别是 Flink 和 Kafka Streaming。Flink 是我们之前生产上使用的方案，在能力上是符合要求的，最主要的问题是长期的可维护性。在公有云场景，那个阶段 Flink 服务在火山引擎上还没有发布，我们自己的服务又有严格的时间线，所以必须考虑替代；在私有化场景，我们不确认客户环境一定有 Flink 集群，即使部署的数据底座中带有 Flink，后续的维护也是个头疼的问题。另外一个角度，作为通用流式处理框架，Flink 的大部分功能我们并没有用到，对于单条消息的流转路径，其实只是简单的读取和处理，使用 Flink 有些“杀鸡用牛刀”了。另一个比较标准的方案是 Kafka Streaming。作为 Kafka 官方提供的框架，对于流式处理的语义有较好的支持，也满足我们对于轻量的诉求。最终没有采用的主要考虑点是两个：</p><p></p><p>对于 Offset 的维护不够灵活：我们的场景不能使用自动提交（会丢消息），而对于同一个 Partition 中的数据又要求一定程度的并行处理，使用 Kafka Streaming 的原生接口较难支持。与 Kafka 强绑定：大部分场景下，我们团队不是元数据消息队列的拥有者，也有团队使用 RocketMQ 等提供元数据变更，在应用层，我们希望使用同一套框架兼容。</p><p></p><p></p><h2>设计</h2><p></p><p></p><h3>概念说明</h3><p></p><p></p><p>MQ Type：Message Queue 的类型，比如 Kafka 与 RocketMQ。后续内容以 Kafka 为主，设计一定程度兼容其他 MQ。Topic：一批消息的集合，包含多个 Partition，可以被多个 Consumer Group 消费。Consumer Group：一组 Consumer，同一 Group 内的 Consumer 数据不会重复消费。Consumer：消费消息的最小单位，属于某个 Consumer Group。Partition：Topic 中的一部分数据，同一 Partition 内消息有序。同一 Consumer Group 内，一个 Partition 只会被其中一个 Consumer 消费。Event：由 Topic 中的消息转换而来，部分属性如下。Event Type：消息的类型定义，会与 Processor 有对应关系；Event Key：包含消息 Topic、Partition、Offset 等元数据，用来对消息进行 Hash 操作；Processor：消息处理的单元，针对某个 Event Type 定制的业务逻辑。Task：消费消息并处理的一条 Pipeline，Task 之间资源是相互独立的。</p><p></p><h3>框架架构</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/eb/eb6b7775e262abd924ef04d2b4872962.png\" /></p><p></p><p>整个框架主要由 MQ Consumer, Message Processor 和 State Manager 组成。</p><p></p><p>MQ Consumer：负责从 Kafka Topic 拉取消息，并根据 Event Key 将消息投放到内部队列，如果消息需要延时消费，会被投放到对应的延时队列；该模块还负责定时查询 State Manager 中记录的消息状态，并根据返回提交消息 Offset；上报与消息消费相关的 Metric。Message Processor：负责从队列中拉取消息并异步进行处理，它会将消息的处理结果更新给 State Manager，同时上报与消息处理相关的 Metric。State Manager：负责维护每个 Kafka Partition 的消息状态，并暴露当前应提交的 Offset 信息给 MQ Consumer。</p><p></p><h2>实现</h2><p></p><p></p><h3>线程模型</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/de/deef6d2c11c17651bf691c6c82613c52.png\" /></p><p></p><p>每个 Task 可以运行在一台或多台实例，建议部署到多台机器，以获得更好的性能和容错能力。</p><p></p><p>每台实例中，存在两组线程池：</p><p></p><p>Consumer Pool：负责管理 MQ Consumer Thread 的生命周期，当服务启动时，根据配置拉起一定规模的线程，并在服务关闭时确保每个 Thread 安全退出或者超时停止。整体有效 Thread 的上限与 Topic 的 Partition 的总数有关。Processor Pool：负责管理 Message Processor Thread 的生命周期，当服务启动时，根据配置拉起一定规模的线程，并在服务关闭时确保每个 Thread 安全退出或者超时停止。可以根据 Event Type 所需要处理的并行度来灵活配置。</p><p></p><p>两类 Thread 的性质分别如下：</p><p></p><p>Consumer Thread：每个 MQ Consumer 会封装一个 Kafka Consumer，可以消费 0 个或者多个 Partition。根据 Kafka 的机制，当 MQ Consumer Thread 的个数超过 Partition 的个数时，当前 Thread 不会有实际流量。Processor Thread：唯一对应一个内部的队列，并以 FIFO 的方式消费和处理其中的消息。</p><p></p><h3>StateManager</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4f/4f20ded14ffd23af08d6fa6d92890b92.png\" /></p><p></p><p>在 State Manager 中，会为每个 Partition 维护一个优先队列（最小堆），队列中的信息是 Offset，两个优先队列的职责如下：</p><p></p><p>处理中的队列：一条消息转化为 Event 后，MQ Consumer 会调用 StateManager 接口，将消息 Offset 插入该队列。处理完的队列：一条消息处理结束或最终失败，Message Processor 会调用 StateManager 接口，将消息 Offset 插入该队列。</p><p></p><p>MQ Consumer 会周期性的检查当前可以 Commit 的 Offset，情况枚举如下：</p><p></p><p>处理中的队列堆顶 &lt; 处理完的队列堆顶或者处理完的队列为空：代表当前消费回来的消息还在处理过程中，本轮不做 Offset 提交。处理中的队列堆顶 = 处理完的队列堆顶：表示当前消息已经处理完，两边同时出队，并记录当前堆顶为可提交的 Offset，重复检查过程。处理中的队列堆顶 &gt; 处理完的队列堆顶：异常情况，通常是数据回放到某些中间状态，将处理完的队列堆顶出堆。注意：当发生 Consumer 的 Rebalance 时，需要将对应 Partition 的队列清空。</p><p></p><h3>KeyBy 与 Delay Processing 的支持</h3><p></p><p></p><p>因源头的 Topic 和消息格式有可能不可控制，所以 MQ Consumer 的职责之一是将消息统一封装为 Event。根据需求，会从原始消息中拼装出 Event Key，对 Key 取 Hash 后，相同结果的 Event 会进入同一个队列，可以保证分区内的此类事件处理顺序的稳定，同时将消息的消费与处理解耦，支持增大内部队列数量来增加吞吐。</p><p></p><p>Event 中也支持设置是否延迟处理属性，可以根据 Event Time 延迟固定时间后处理，需要被延迟处理的事件会被发送到有界延迟队列中，有界延迟队列的实现继承了 DelayQueue，限制 DelayQueue 长度, 达到限定值入队会被阻塞。</p><p></p><h3>异常处理</h3><p></p><p></p><p>Processor 在消息处理过程中，可能遇到各种异常情况，设计框架的动机之一就是为业务逻辑的编写者屏蔽掉这种复杂度。Processor 相关框架的逻辑会与 State Manager 协作，处理异常并充分暴露状态。比较典型的异常情况以及处理策略如下：</p><p></p><p>处理消息失败：自动触发重试，重试到用户设置的最大次数或默认值后会将消息失败状态通知 State Manager。处理消息超时：超时对于吞吐影响较大，且通常重试的效果不明显，因此当前策略是不会对消息重试，直接通知 State Manager 消息处理失败。处理消息较慢：上游 Topic 存在 Lag，Message Consumer 消费速率大于 Message Processor 处理速率时，消息会堆积在队列中，达到队列最大长度，Message Consumer 会被阻塞在入队操作，停止拉取消息，类似 Flink 框架中的背压。</p><p></p><h3>监控</h3><p></p><p></p><p>为了方便运维，在框架层面暴露了一组监控指标，并支持用户自定义 Metrics。其中默认支持的 Metrics 如下表所示：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/66/666c41e21fd56db88026edab1fe4266d.png\" /></p><p></p><h2>线上运维 Case 举例</h2><p></p><p></p><p>实际生产环境运行时，偶尔需要做些运维操作，其中最常见的是消息堆积和消息重放。对于 Conusmer Lag 这类问题的处理步骤大致如下：</p><p></p><p>查看 Enqueue Time，Queue Length 的监控确定服务内队列是否有堆积。如果队列有堆积，查看 Process Time 指标，确定是否是某个 Processor 处理慢，如果是，根据指标中的 Tag 确定事件类型等属性特征，判断业务逻辑或者 Key 设置是否合理；全部 Processor 处理慢，可以通过增加 Processor 并行度来解决。如果队列无堆积，排除网络问题后，可以考虑增加 Consumer 并行度至 Topic Partition 上限。</p><p></p><p>消息重放被触发的原因通常有两种，要么是业务上需要重放部分数据做补全，要么是遇到了事故需要修复数据。为了应对这种需求，我们在框架层面支持了根据时间戳重置 Offset 的能力。具体操作时的步骤如下：</p><p></p><p>使用服务测暴露的 API，启动一台实例使用新的 Consumer GroupId: {newConsumerGroup} 从某个 startupTimestamp 开始消费更改全部配置中的 Consumer GroupId 为 {newConsumerGroup}分批重启所有实例</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>为了解决 DataLeap 中 Data Catalog 系统消费近实时元数据变更的业务场景，我们自研了轻量级消息处理框架。当前该框架已在字节内部生产环境稳定运行超过 1 年，并支持了火山引擎上的数据地图服务的元数据同步场景，满足了团队需求。下一步会根据优先级排期支持 RocketMQ 等其他消息队列，并持续优化配置动态更新，监控报警，运维自动化等方面。</p><p></p>",
    "publish_time": "2022-10-17 12:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Power Platform 产品大更新，微软：以无代码、低代码方式全面支持企业数字化转型",
    "url": "https://www.infoq.cn/article/PPjm219eVKgsvIIPz4TP",
    "summary": "<p>近期，在微软年度技术大会 Ignite 2022 及 Ignite China 中国技术峰会上，<a href=\"https://www.infoq.cn/article/YxIvZSIsLrXioeW9MFvO\">微软</a>\"宣布对Power Platform平台进行了大量更新，包括新增了Express Design功能，用户可通过手绘设计草图、纸质表单、PPT、PDF以及<a href=\"https://www.infoq.cn/article/SDQwzl4iTDuU8F2quv49\">Figma</a>\"中的设计方案，在AI帮助下一键生成应用程序。</p><p>&nbsp;</p><p>Power Platform是微软推出的一款面向具备较少IT技能，或者非专业IT人员的低代码、无代码的应用开发平台，于2020年在中国正式商用。其核心组件有Power App、Power Automate、Power BI、Power Virtual Agents等。本次大会上，Power Platform 还迎来全新的成员——Power Pages，用户可以通过该产品以低代码开发方式快速构建自己的商业网站。</p><p>&nbsp;</p><p>根据微软大中华区商业应用事业部高级业务主管李威的介绍，Power Platform在中国发布至今，其遵循了微软在中国市场的发展策略：支持中国企业全球拓展、支持跨国企业本土创新和聚焦战略生态。</p><p>&nbsp;</p><p>具体地，比如在进行全球拓展过程中，小米选择使用Power Platform来赋能后端技术团队。小米国际IT总监杨恩介绍，“IT全球合规、安全、高效的业务系统，是小米IT建设的首要目标。微软Power Apps基于分布在全球的Azure数据中心进行部署，天然继承了Azure数据中心在基础设施层面，在不同国家地区以及行业的80多项合规标准。我们在此基础上搭建系统，更便捷的满足了各国家不同的要求。”</p><p>&nbsp;</p><p>李威表示，微软一直基于对技术发展趋势的研判来演进Power Platform。与其他厂商聚焦在某个领域不同，Power Platform最大的特点是通过一个平台，以无代码、低代码的方式来支持企业数字化转型涉及到的方方面面，即在企业数字化转型的各个领域，Power Platform都有对应的产品给予支持，</p><p>&nbsp;</p><p>Power Platform 首先可以帮助企业在不同操作系统和设备上快速开发一款App；其次可以快速定义一个流程，让企业从一些简单、繁琐且重复的工作中解放出来；再者，可以形成一系列数据分析、数据洞察和数据报表；最后有自动化的机器人、helpdesk的自动化bot等等。</p><p>&nbsp;</p><p>结合对于整个发展趋势的研判，Power Platform也在以下方向不断发展：</p><p>&nbsp;</p><p>在AI领域，Power Automate之前可以用一种图形化拖拉拽的方式定义一个流程，而无需代码开发。现在，业务人员可以用自然语言描述一个流程，然后平台通过机器学习的方式理解这些话术，然后将其快速、自动地形成一个流程。在融合开发方面，Power Platform不只要满足不具备开发技能的业务人员的开发需求，而是希望大家能够在一起共同开发，且不会互相干扰。超级<a href=\"https://www.infoq.cn/article/Vyg6CQgbtTu9SKbuk9oX\">自动化</a>\"方面，Power Platform的目标是将企业至少50%的人，通过低代码、无代码的方式解放出来，去做更多的创新工作。可管控上，对于访问权限等问题，Power Platform最新发布了Managed&nbsp;Environments等能力，帮助企业IT部门做好安全和管控。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;</p><p>微软&nbsp;Power Platform 全球黑带技术专家Frank&nbsp;Yang表示，大家要明确低代码开发技术和低代码开发平台的区别。</p><p>&nbsp;</p><p>“Power Platform实际上是低代码的平台，不只是技术，它是技术加上平台的应用。它的好处就是不但能够帮助业务人员解决问题，还能解决开发人员的问题，背后的关键是能帮助企业快速实现数字化转型。数字化转型绝不只是靠开发人员或者只靠业务人员实现的，需要两方都做出改变。这也是最近大家讲得最多的Biz DevOps，即业务人员引领的DevOps场景。”Frank&nbsp;Yang说道。</p><p>&nbsp;</p><p>Frank Yang表示，越来越多的企业意识到了低代码平台带来的业务价值。</p><p>&nbsp;</p><p>“我们经常听到客户的一个想法就是，开发人员成本足够低的话是不会用低代码的。实际上，并不存在这样的事情。”Frank Yang说道，“最大的Power Platform 用户其实是印度的客户，而印度实际上是全球开发人员成本最低的地方，印度的很多企业就是使用Power Platform去做开发的。”</p><p>&nbsp;</p><p>Frank Yang表示，过去一段时间内，国内都专注在了对低代码技术本身的讨论上，但技术本身并没有错，可以用在业务系统人员身上，也可以用在开发人员身上。从技术角度来讲，低代码技术是不会替代传统高代码方式的，因为两者解决的是不同的问题，但低代码平台会对传统开发方式产生深远影响。</p>",
    "publish_time": "2022-10-17 13:57:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "为躲亲戚催婚，一摄影师创造出了AI女友",
    "url": "https://www.infoq.cn/article/r3HIrXYNuuP6Tp8F8XrV",
    "summary": "<p></p><p></p><blockquote>如何优雅地拒绝亲戚催婚，AI来支招了。</blockquote><p></p><p></p><p>来自<a href=\"https://www.youtube.com/c/PiXimperfect/featured\">PiXimperfect</a>\"的摄影师Unmesh&nbsp;Dinda创建出纯AI生成的女友，再次展示了AI强大的照片编辑能力。</p><p></p><p>近日，Dinda发布了一对情侣在假期闪逛时的多张自拍，其中的光影效果极其逼真，与照片背景完美契合。但请注意：照片中的Dinda是真人，女友则是由AI模型一手创造出来的。</p><p></p><p><img src=\"https://static001.geekbang.org/resource/image/04/0c/0490a2572329fcc6e8b6bf7df94bd10c.png\" /></p><p></p><p>Dinda在自己的YouTube视频中提到，“如果你身边也有一堆催婚的亲戚，那就给他们发张这样的照片。这应该能让他们消停一阵子。”</p><p></p><h2>创建AI女友</h2><p></p><p>上个月，DALL-E决定解除禁令，允许用户编辑带有人脸的图像。</p><p></p><p>经营照片编辑YouTube频道的Dinda把握时机，上传了他拍下的几张自拍照，而后使用图像修复功能擦除照片中的特定部分，再输入文本提示引导DALL-E填充空白区域。</p><p></p><p>在删除了照片上对应女伴的区域之后，Dinda输入“和女友在一起的男性”，这样DALL-E就能生成一对生动鲜活的情侣。</p><p></p><p>在选定AI女友之后，Dinda又使用外画功能将照片延伸到原始边界之外。</p><p></p><p><img src=\"https://static001.geekbang.org/resource/image/85/7e/85af8a0d432f86347b50a65a2cb2e37e.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/resource/image/ac/77/ac1f4acfd75bd054917bb3602bf1a577.png\" /></p><p></p><p>DALL-E等AI图像生成器其实还不足以生成完美可信的人脸，所以Dinda把这里的“女友脸”裁剪并导入另一款名为GFP-GAN的AI照片编辑程序中。该程序在女性面部润色方面更加强大。完成细节补充之后，他又把照片加载至Photoshop中并在原始图像上对齐，最终创造出了令人真假难辨的AI合成女友。</p><p></p><h2>加人可以，删人也行</h2><p></p><p>Dinda还演示了如何通过同一技术，借AI照片编辑器之手删除人物。通过对所要删除的人物进行修复和替换，DALL-E完全能够在复杂的图像和背景中绘制出比较自然的填充部分。</p><p></p><p>对于需要花费大量时间从照片中移除复杂物体或人物的摄影师们来说，这项技术无疑令人兴奋。</p><p></p><p>AI工具还能添加特定人类特征，例如头发。Dinda就演示了如何为巨石强森加上头发，DALL-E甚至还为不同发型匹配了相应的阴影。</p><p></p><p>关于Dinda的更多作品，请参阅其YouTube（<a href=\"https://www.youtube.com/c/PiXimperfect/featured\">https://www.youtube.com/c/PiXimperfect/featured</a>\"）、Instagram（<a href=\"https://www.instagram.com/piximperfect/\">https://www.instagram.com/piximperfect/</a>\"）和网站（<a href=\"https://www.piximperfect.com/\">https://www.piximperfect.com/</a>\"）。</p><p></p><h2>AI图像生成器DALL-E，现已允许用户编辑人脸</h2><p></p><p></p><p>作为AI图像生成器DALL-E的缔造者，OpenAI公司在今年9月20日<a href=\"https://petapixel.com/2022/09/20/ai-image-generator-dall-e-now-allows-users-to-edit-human-faces/\">宣布</a>\"将允许用户编辑包含人脸的照片。</p><p></p><p><img src=\"https://static001.geekbang.org/resource/image/97/71/9779fdaf5f06ac3a031a99d464b2bd71.png\" /></p><p></p><p>此前，DALL-E一直禁止用户生成包含真实人脸的图像，以避免模型被滥用于创建deepfakes。</p><p></p><p>但如今OpenAI公司表示已经“改进了安全系统”，并结合用户提交的功能需求反馈而准备开放人脸编辑。</p><p></p><p>OpenAI公司在一份声明中表示，“很多用户告诉我们，他们想要使用DALL-E为自己设计服装、发型，并编辑全家福照片的背景。”</p><p></p><p>“一位整形外科医生告诉我们，他一直使用DALL-E帮患者们查看可视化结果。也有电影制作人告诉我们，他们希望能用DALL-E编辑场景图像，借此加快创作速度。”</p><p></p><p>OpenAI也对自己做出了明确定义 —— 一款比Midjourney或者Stable Diffusion等竞争对手更具品牌友好性的AI图像生成器。这里顺带一提，Stable Diffusion可谓反行业主流而行，在开放功能的同时几乎不加任何安全过滤限制。</p><p></p><p>“我们的过滤器更善于防止生成关于性、政治和暴力方面的内容，努力减少错误标记，同时配合新的检测与响应技术以阻止滥用。”</p><p></p><p>目前DALL-E仍只对受邀用户开放，使用者可以上传自己的照片，借此编辑照片或要求AI为其生成图像变体。</p><p></p><p>值得一提的是，DALL-E的内容政策仍然禁止用户上传未经授权的人物图像，但还不清楚这项管控要如何具体实现。</p><p></p><p>OpenAI得到了微软及多家大型风险投资公司的支持。在目前的早期文本到图像生成器中，DALL-E对于法律和道德问题的关注度明显比<a href=\"https://www.infoq.cn/article/MYYhWiSNPaAQIGfZywa0\">Stable Diffusion</a>\"更高，也因此成为应用广泛的主流模型选项。</p><p></p><p>但一旦落入错误的人手中，这种强大的技术足以为外行创造出真假难辨的deepfakes，进而冲击整个社会的信任结构甚至是正常运转。</p><p></p><p>参考文章：</p><p></p><p><a href=\"https://petapixel.com/2022/10/14/photographer-creates-ai-girlfriend-to-stave-off-nosy-relatives/\">https://petapixel.com/2022/10/14/photographer-creates-ai-girlfriend-to-stave-off-nosy-relatives/</a>\"</p><p></p><p><a href=\"https://petapixel.com/2022/09/20/ai-image-generator-dall-e-now-allows-users-to-edit-human-faces/\">https://petapixel.com/2022/09/20/ai-image-generator-dall-e-now-allows-users-to-edit-human-faces/</a>\"</p><p></p>",
    "publish_time": "2022-10-17 14:11:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]