[
  {
    "title": "Terraform引入新的CI/CD工具，增加对Azure Linux的支持",
    "url": "https://www.infoq.cn/article/H7CJpGyk0BRjUTAqIXbM",
    "summary": "<p>HashiCorp发布了一系列针对Terraform和Terraform Cloud的改进。Terraform Cloud提供了一个新的<a href=\"https://www.hashicorp.com/blog/hashicorp-releases-new-ci-cd-pipeline-integration-tool-templates-for-terraform\">CI/CD管道集成工具</a>\"。Terraform为Azure Kubernetes Service增加了对<a href=\"https://www.hashicorp.com/blog/terraform-adds-support-azure-linux-container-host-azure-kubernetes-service\">Azure Linux容器主机</a>\"的支持。HashiCorp Terraform <a href=\"https://www.hashicorp.com/blog/terraform-aws-provider-5-0-adds-updates-to-default-tags\">AWS Provider 5.0</a>\"发布，改进了对默认标记的支持。</p><p>&nbsp;</p><p>新的CI/CD管道工具有一个相关的命令行工具，叫作tfci。这个工具通过API调用自动运行Terraform Cloud，并支持可以嵌入到CI工具中的Terraform Cloud操作。tfci提供的<a href=\"https://github.com/hashicorp/tfc-workflows-tooling/blob/main/docs/USAGE.md\">命令</a>\"包括：通过Terraform Cloud Run ID显示运行详情、执行新的计划运行、在计划确认后继续执行暂停的任务，以及返回计划详情。</p><p>&nbsp;</p><p>除了tfci，还有为<a href=\"https://github.com/hashicorp/tfc-workflows-github/blob/main/actions/plan-output/action.yml\">GitHub Actions</a>\"和<a href=\"https://github.com/hashicorp/tfc-workflows-gitlab\">GitLab CI</a>\"提供的<a href=\"https://github.com/hashicorp/tfc-workflows-tooling\">模板</a>\"。这些模板包含用户在使用tfci时可能需要配置的常见操作。例如，下面的代码片段是在GitHub Actions中使用tfci执行计划的部分内容：</p><p><code lang=\"null\">runs:\n  using: docker\n  image: 'docker://hashicorp/tfci:v1.0.1'\n  args:\n  - tfci\n  ## global flags\n  - -hostname=${{ inputs.hostname }}\n  - -token=${{ inputs.token }}\n  - -organization=${{ inputs.organization }}\n  ## command\n  - run\n  - create\n  - -workspace=${{ inputs.workspace }}\n  - -configuration_version=${{ inputs.configuration_version }}\n  - -message=${{ inputs.message }}\n  - -plan-only=${{ inputs.plan_only }}</code></p><p>&nbsp;</p><p>HashiCorp还增加了在Azure Kubernetes Service上部署Azure Linux容器主机的支持。微软最近提供了<a href=\"https://learn.microsoft.com/en-us/azure/aks/use-mariner\">Azure Linux容器主机</a>\"（之前的Mariner OS）的<a href=\"https://www.infoq.com/news/2023/06/azure-linux-cbl-mariner/\">一般可用性</a>\"。Azure Linux被设计成一个最小化的、云优先的Linux发行版。</p><p>&nbsp;</p><p>这些更新包含在<a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster\">azurerm Terraform Provider</a>\"中。要在AKS上配置Azure Linux容器主机，可以将os_sku设置为Mariner：</p><p><code lang=\"null\">resource \"azurerm_kubernetes_cluster\" \"default\" {\n  name                = \"aks-${random_string.suffix.result}\"\n  location            = azurerm_resource_group.default.location\n  resource_group_name = azurerm_resource_group.default.name\n  \n  kubernetes_version  = var.kubernetes_version\n  dns_prefix          = \"k8s-${random_string.suffix.result}\"\n \ndefault_node_pool {\n    name            = \"default\"\n    node_count      = var.aks_node_count\n    vm_size         = var.aks_confidential_computing_enabled ? \"Standard_DC2s_v2\" : \"Standard_D2_v2\"\n    os_sku          = \"Mariner\"\n    os_disk_size_gb = 50\n  }\n \n  confidential_computing {\n    sgx_quote_helper_enabled = true\n  }\n \n  identity {\n    type = \"SystemAssigned\"\n  }\n \n  tags = {\n    name = \"demo-aks-${random_string.suffix.result}\"\n    environment = \"demo\"\n  }\n}</code></p><p>&nbsp;</p><p><a href=\"https://github.com/hashicorp/terraform-provider-aws/releases/tag/v5.0.0\">HashiCorp Terraform AWS Provider 5.0</a>\"改进了对默认标签的支持，允许在Provider级别设置标签。这个更新解决了之前默认标签实现的许多痛点，包括处理不一致的最终计划、默认标签和资源标签之间的相同标签，以及标签配置中的永久差异。</p><p>&nbsp;</p><p>可以使用default_tags在Provider级别指定默认标签：</p><p><code lang=\"null\">provider \"aws\" {\n  default_tags {\n    tags = {\n      environment = \"Dev\"\n      department  = \"WebEng\"\n      application = \"HashiCafe website\"\n      cost_center = \"8675309\"\n    }\n  }\n}\n \nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"example-bucket-aj-11122\"\n  tags = {\n    environment = \"Production\"\n    created_at  = timestamp()\n  }\n}</code></p><p>&nbsp;</p><p>该版本还调整了已弃用或已删除的属性的报告方式，之前用户会收到警告通知，现在会向用户显示“不受支持错误”。EC2的典型功能也被完全移除，因为这些功能早在2022年8月就被AWS弃用了。</p><p>&nbsp;</p><p>CI/CD管道集成工具和模板对Terraform Cloud和Terraform Enterprise用户可用。更多细节可以在<a href=\"https://www.hashicorp.com/blog/hashicorp-releases-new-ci-cd-pipeline-integration-tool-templates-for-terraform\">发布博客</a>\"和<a href=\"https://github.com/hashicorp/tfc-workflows-tooling\">GitHub代码库</a>\"中找到。Terraform AWS Provider 5.0提供了一个<a href=\"https://www.hashicorp.com/blog/terraform-aws-provider-5-0-adds-updates-to-default-tags\">升级指南</a>\"，其中包含了有关该版本变更的更多详细信息。</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/hashicorp-azure-linux/\">https://www.infoq.com/news/2023/06/hashicorp-azure-linux/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/LFa4ESMJOMY66HtTjYsG\">中国企业研发高效能白皮书-CI/CD篇</a>\"</p><p><a href=\"https://www.infoq.cn/article/lPSPNxwMD2US2WEHamr4\">可观测的崭新进化：加速CI/CD管道的秘密武器</a>\"</p><p><a href=\"https://www.infoq.cn/article/V5NBFgrCIVMpXVazuGQE\">干货 | 携程 Web CI/CD 实践</a>\"</p>",
    "publish_time": "2023-07-04 09:59:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "厂商征集 | 中国人工智能成熟度模型报告 2023",
    "url": "https://www.infoq.cn/article/Rxyrty9TAXacP99lCtWF",
    "summary": "<p></p><h1>研究背景</h1><p></p><p>去年，AI绘画等生成式AI的内容激发了AI领域的广泛关注，年底发布的ChatGPT更是迅速破圈，话题度和讨论度不断提升。这些变化的趋势也延续到了2023年上半年，人们讨论话题也向着大模型的实际应用逐渐深入，同时AI安全也展现了大家对技术突破的不同态度。</p><p>今年年初，InfoQ研究中心发布了《中国软件技术发展洞察和趋势预测报告&nbsp;2023》，在报告中将各个领域的关键技术按照不同成熟度阶段进行了划分，总结成为中国技术成熟度评估曲线。报告发布之后也获得了众多企业和开发者的关注和讨论。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4d85ee1cd05f4b611bde1b873183ecc.png\" /></p><p></p><p>因此，InfoQ研究中心将基于人工智能领域的种种变化，更新人工智能领域的技术成熟度模型，为技术的应用决策和未来投资参考提供研究分析工具。</p><p></p><h1>研究内容</h1><p></p><p>基于技术发展时间、技术专利规模、技术应用市场规模、技术舆论热度等基础评价指标，构建中国人工智能技术成熟度模型。</p><p>报告包含“行业发展洞察”、“人工智能技术成熟度模型解读”、“人工智能技术企业生态图谱”等多个行业研究内容，以及在模型解读中还会有案例分析等厂商研究内容。</p><p></p><h1>征集范围</h1><p></p><p>企业主要业务类型包括：</p><p>提供人工智能产品与服务的各类厂商（RPA、计算机视觉、自然语言学习、数据挖掘、脑机接口、生成式AI等）；其他综合科技厂商。</p><p></p><h1>参与价值</h1><p></p><p>参与本次征集的厂商，将有机会获得：</p><p>InfoQ研究中心《中国人工智能成熟度模型报告&nbsp;2023》人工智能技术企业生态图谱、典型案例两大内容露出，提升厂商品牌知名度和行业影响力。报告通过InfoQ官网、公众号InfoQ等多个官方平台进行发布，同时会在多家生态链接的渠道传播。将有机会受邀参加InfoQ的线上线下活动，与行业内的甲方，行业专家，投资机构等进行深入的交流。</p><p></p><p>本次厂商征集自即日起，截至07月25日，欢迎识别下方二维码，添加小助手后，填写表单参与报告征集。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8a0f5451e7cea9ef0c7f9c70c0da9421.png\" /></p><p></p>",
    "publish_time": "2023-07-04 10:06:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "厂商征集 | 中国云原生成熟度模型报告 2023",
    "url": "https://www.infoq.cn/article/Hlwwy6hW43iG9PUs1RO8",
    "summary": "<p></p><h1>研究背景</h1><p></p><p>随着云计算技术的不断发展和普及，越来越多的企业和组织开始采用云原生架构来构建和运行应用程序。云原生是近些年重要战略技术趋势，也是企业数字化转型的重要途径。</p><p>今年年初，InfoQ研究中心发布了《中国软件技术发展洞察和趋势预测报告&nbsp;2023》，在报告中将各个领域的关键技术按照不同成熟度阶段进行了划分，总结成为中国技术成熟度评估曲线。报告发布之后也获得了众多企业和开发者的关注和讨论。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4d85ee1cd05f4b611bde1b873183ecc.png\" /></p><p></p><p>因此，InfoQ研究中心将基于收集到的云原生领域的各种数据，更新云原生领域的技术成熟度模型，为技术的应用决策和未来投资参考提供研究分析工具。</p><p></p><h1>研究内容</h1><p></p><p>基于技术发展时间、技术专利规模、技术应用市场规模、技术舆论热度等基础评价指标，构建中国云原生技术成熟度模型。</p><p>报告包含“行业发展洞察”、“云原生技术成熟度模型解读”、“云原生技术企业生态图谱”等多个行业研究内容，并且在模型解读中还会有案例分析等厂商研究内容。</p><p></p><h1>征集范围</h1><p></p><p>企业主要业务类型包括：</p><p>提供云原生产品与服务的各类厂商（云原生数据库、容器服务、数据流与消息传递、Kubernetes&nbsp;集群托管、监控、安全与合规等）；其他综合科技厂商。</p><p></p><h1>参与价值</h1><p></p><p>参与本次征集的厂商，将有机会获得：</p><p>1、InfoQ研究中心《中国云原生成熟度模型报告&nbsp;2023》云原生技术企业生态图谱、典型案例两大内容露出，提升厂商品牌知名度和行业影响力。</p><p>2、报告将通过InfoQ官网、公众号InfoQ等多个官方平台进行发布，同时会在多家生态渠道传播。</p><p>3、将有机会受邀参加InfoQ的线上线下活动，与行业内的专家、投资机构等进行深入的交流。</p><p></p><p>本次厂商征集自即日起，截至07月20日，欢迎识别下方二维码，添加小助手后，填写表单参与报告征集。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8a0f5451e7cea9ef0c7f9c70c0da9421.png\" /></p><p></p>",
    "publish_time": "2023-07-04 10:26:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面向故障处理的可观测性体系建设",
    "url": "https://www.infoq.cn/article/g4LyEj3Ue2JCD2vLAfXc",
    "summary": "<p><a href=\"https://www.infoq.cn/article/kTtlLZxNUPmlvYpzA0eF\">笔者</a>\"从 12 年开始入行，从事 DevOps 研发工作，做过部署系统、监控系统、可观测性相关产品，也做过 SRE 一线和管理工作，对于可观测性的理解和实践，有一些小小的见解，利用本文和大家做一个探讨分享。本文主要内容包括：</p><p>可观测性在整个商业体系中的位置和价值如何快速发现故障，使用哪类指标告警SRE 在谈论故障定位的时候，谈的是什么如何找到故障直接原因，找到止损依据如何让可观测性系统呈现观点，辅助洞察，定位故障</p><p></p><h2>可观测性在整个商业体系中的位置和价值</h2><p></p><p>做一个事，首先得有价值，如果价值太小不值得投入。可观测性也不例外，我们首先分析一下可观测性在整个商业体系中的位置和价值。思考第一个问题：作为在线类产品，我们希望客户/用户有一个好的产品体验，那怎么算一个好的产品体验？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/797ea306fce73a3782598262c260cf3c.png\" /></p><p></p><p>很明显，产品体验包括功能体验和可靠性体验。功能体验依赖产品设计和迭代速度，跟今天的话题关系不大暂且按下不表。可靠性体验呢？可靠性体验核心就是追求高可用、低延迟，通俗讲就是每次打开站点或app，都不报错，速度嗖嗖的。那如何才能具有好的可靠性体验呢？</p><p></p><p>其实如果一切正常，就应该是可用且速度快的，除非哪里出了问题，也就是发生了故障，才会报错或者延迟大增。那技术团队要做的，除了持续优化架构和性能，就是不断和故障做斗争了。降低故障发生的频率，降低故障的影响范围，降低故障的恢复时间。归纳为 6 个字：降发生、降影响！</p><p></p><p>怎么做？有没有方法论来指导？我们可以从故障的生命周期着手，来优化生命周期的各个环节，每个环节都做好了，理论上结果就是好的。故障生命周期的梗概图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/73c9e6f892d22efd34b3b90df3f6fadc.png\" /></p><p></p><p>从大面上，可以分成事前、事中、事后三个大的阶段：</p><p>事前：及时发现风险，做好架构、预案、演练事中：及时发现故障，及时定位，及时止损事后：排查根因，落实复盘改进项</p><p></p><p>看起来寥寥数语，没有特殊的东西，但实际上每个环节要做好，都不容易。那可观测性，在这整个过程的职能是什么？在哪个环节发挥价值？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/72ab3a951c58cb598c5d098ae537850f.png\" /></p><p></p><p>显然，可观测性，是在故障发现、定位环节发挥作用的，核心价值就是帮我们快速发现故障、快速定位故障，进而降低故障的影响。如此，可观测性的位置和价值就很明确了，用一张图概括：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f61bada46081700d99d9dab70e438889.png\" /></p><p></p><p>客户/用户需要好的产品体验，好的产品体验包括可靠性体验，要想有好的可靠性体验，就得减少故障，所谓的降发生、降影响，而这，又依赖了可观测性的能力。所以：可观测性最终是服务于产品体验、服务于商业成功的（想不想取得商业成功？根据刚才的分析可观测性可是重点因素哦），核心目标是快速发现、定位故障。</p><p></p><p>那么，如何快速发现故障？</p><p></p><h2>如何快速发现故障，使用哪类指标告警</h2><p></p><p>要想能够快速发现故障，得先定义什么是故障！简单来看，产品体验受损，就是故障！比如：</p><p>电商产品：用户无法下单、无法支付、无法查看商品、无法查看历史订单存储系统：用户无法读、无法写、或者读写延迟过高流媒体产品：无法开启播放、无法拉流、无法浏览视频信息</p><p></p><p>既然能够定义如何算是产品体验受损，那就可以梳理出相关的监控指标，比如：</p><p>电商产品：订单量、支付量、商品/订单访问成功率/延迟存储系统：读/写成功率、读/写延迟流媒体产品：播放量和成功率、拉流延迟、视频浏览成功率/延迟等</p><p></p><p>大家有没有发现这类指标的特点？显然，都是可以量化客户体验的指标，这类指标我们称为结果类指标（后面会介绍原因类指标），大面上可以分为两类，一类是业务指标，另一类是 SLO 指标。</p><p></p><p>一般公司做监控的时候，可能会意识到要做 SLO 指标的监控，容易忽略业务类指标的监控。其实，业务类指标才是老板更为关注的指标，而且，SLO 指标正常的时候，业务指标未必正常。比如客户到服务端的网络出问题了，服务端的成功率、延迟指标都是正常的，但是客户无法下单，订单量会下跌。所以，一定要重视业务指标体系的构建和监控。</p><p></p><p>听起来，业务指标和 BI 数据很像有没有？确实，最大的相同点是：都是老板关注的，哈哈。不同点呢？BI 数据对准确性要求很高，对实时性要求没有那么高，而业务指标监控，对准确性要求没有那么高（只要能发现数据趋势出问题了就可以了），对实时性要求很高，毕竟是用来发现故障的，如果实时性太差，黄花菜都凉了。</p><p></p><p>指标体系的构建，除了结果类指标，与之对应的还有原因类指标。都需要，但是我们配置告警的时候，一般是针对结果类指标来配置。因为产品的核心业务功能是可枚举的，每个功能对应的结果类指标就是可枚举的，做好结果类指标的告警，就可以保证告警是全的，做到有故障必有告警！举个例子：实时交易类系统，交易量突然下跌。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c15623e52d18db86e9487b0ad3b39de.png\" /></p><p></p><p>如果，面向原因类指标配置告警，则永远无法配全，无法做到有故障必有告警！实际上，原因类指标不必一定要配置告警，出故障的时候可观测，其实也基本够了。</p><p></p><p>如上，要构建可观测性体系，首先要建立完备的指标体系，其中非常关键的是结果类指标，即业务指标和 SLO 指标，结果类指标配合告警系统可以快速发现故障！从这里也可以看出，监控（monitoring）和可观测性（observability）是相辅相成的，非替代关系。</p><p></p><p>OK，既然可以发现故障了，下一步就是定位故障了。</p><p></p><h2>SRE 在谈论故障定位的时候，谈的是什么</h2><p></p><p>在讨论这个问题之前。先分享一个信息层级的概念。说：信息分4个层级，最底下是数据，杂乱无章，比如海量的指标、日志、链路追踪的数据；数据上面是特征，比如最大值、最小值、同环比等，比如5个服务实例，延迟的最大的是哪个，这叫数据特征；特征上面是观点，从故障定位场景来举例，比如根据特征数据分析之后发现，数据库没有问题，依赖的第三方服务也没问题，这就是观点；观点之上就是洞察，或称洞见，综合所有观点，得出故障定位结论，得知具体是哪个模块的什么原因导致了本次故障，就是最终洞察。画个图示例一下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab4212cba5a30046ad10f5412eeca63b.png\" /></p><p></p><p>要想得到最终的洞察（定位到故障），首先要依赖底层的数据完备性，否则就是巧妇难为无米之炊！但是故障原因五花八门，数据能全么？做过 SRE 或者运维的朋友肯定感触颇深，故障可能是电源模块坏了、机房空调坏了、机柜压住网线了、供电不稳、某个盘故障了、中间件配置错了、被黑客攻击了、分布式中间件脑裂了、写日志hang住了、程序配置错了、程序连接第三方的地址错配成线下地址了、DNS配错了、证书过期了、代码Bug了、疏漏了某个罕见用户流程…等等等等。</p><p></p><p>这么多可能的故障原因，要通过可观测性数据分析出来，这数据能全么？比如代码 Bug，要想能根据可观测性数据分析出是哪一行代码的问题，岂不是要像在 IDE 里调试那样，每一行代码的输入输出都得拿到啊，这成本谁扛得住啊，性能损耗谁扛得住啊…</p><p></p><p>如果我们的目标只是定位直接原因，找到止损依据尽快止损，这个底层数据需求就少多了。比如我们不需要知道是哪行代码出了问题，我们只要知道是某个模块做了变更导致了故障，就可以去止损（这个场景的止损动作就是回滚）了。再比如，多活的服务，有时仅仅知道是 A 机房的问题就可以了，把流量切到 B 机房就可以解决。</p><p></p><p>综上，个人观点：使用可观测性数据定位根因，几无可能100%覆盖全部场景！因为数据就不可能全！但如果只是用可观测性数据定位直接原因，找到止损依据，则100%是可以做到的，而这，才是我们应该努力的方向。</p><p></p><p>当 SRE 在谈论故障定位的时候，其实谈论的时是如何找到直接原因，尽快止损。而根因，可以留在复盘阶段慢慢找的。</p><p></p><h2>如何找到故障的直接原因</h2><p></p><p>回答这个问题之前，我们先来看看一个服务要想正常运行，依赖了哪些内容，或者说一个服务如果出故障，可能会是哪里的问题。如果我们能够枚举故障类别，那么我们就可以针对每个类别去分析，找到故障的直接原因。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/65/65f9508d26469553daa68aeec70808f8.png\" /></p><p></p><p>首先，依赖的基础设施（基础网络、硬件、Runtime环境）不能出问题，依赖的第三方其他服务不能出问题，这两个方面大家比较容易理解，不多说了。还有就是服务本身的变更，比如二进制变更、配置的变更、部署方式的变更、流量接入方式的变更，等等，也可能引发问题。最后就是上游访问的方式，比如流量突增，显然也可能会带来故障。</p><p></p><p>那针对这些故障场景，我们应该去看哪些数据呢？这其实就是可观测性数据底座的建设方向。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8eaf8e2083e1b39adc7cb8eea62cb34.png\" /></p><p></p><p>咦？说来说去，还是要建立 metrics、logs、traces、events？是的，但不仅是，只有数据还远远不够，我们需要通过平台工具，通过数据运营整理，帮助用户找到数据特征，建立初步观点，最终形成洞察定位故障直接原因。还记得那张信息层级的图吧：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04b672f449297f8e87f0314e7722547f.png\" /></p><p></p><p>网上有人批评可观测性三支柱的说法，核心要点是：不能只关注 raw data，就像一道菜，只有原料还不能称之为一道菜，没有炊具、菜谱、厨师，无法最终产出那道菜（客人要的是那道菜，那道菜才是结果，应该没有哪个餐厅说，你看我原料都有，完活了，让客人吃吧，应该没有这样的餐厅…）。</p><p></p><p>Martin Mao 曾经也写过一篇文章《Beyond the 3 Pillars of Observability》来论述这个事情。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e49091cfa86e586ab6323f998457137.png\" /></p><p></p><p>他的核心观点是：只关注三支柱raw data，认为有了三支柱数据就建立了可观测性，是不对的，我们更应该面向结果来思考如何构建整个体系，Martin Mao 认为，所谓的结果，就是 Remediate，就是止损！英雄所见略同。</p><p></p><h2>可观测性体系具体要如何做才能辅助技术团队止损</h2><p></p><p>还是参考刚才信息层级的图，有了 raw data 数据底座之后，可观测性体系还需要利用平台能力、通过数据运营整理，呈现数据特征、帮用户建立初步观点，最终形成洞察，定位故障直接原因。</p><p></p><h3>可观测性体系要告诉我故障模块</h3><p></p><p>这应该是可观测性体系提供给客户/用户的第一个观点，故障发生了，现在都是微服务的，你得一目了然的告诉我具体是哪个模块故障了不是。总不能让我写一条有一条的 promql，看一个又一个监控大盘，才能找到故障模块吧。故障处理可是要争分夺秒的，一个一个查看太浪费时间了。</p><p></p><p>既然要一目了然，初始页面上的内容不能太多，从技术角度来看，一般模块都是有层级关系的，首先是系统，然后是子系统，然后是模块。所以，初始页面应该展示系统的健康状况，如果某个系统有问题，应该可以点击进去查看详情（这个过程称为下钻），下钻到子系统，再下钻到模块，最终找到故障模块。</p><p></p><p>那如何衡量一个模块是否健康呢？这其实就可以使用 SLO/SLI 这套体系，每个模块都有几个 SLI，每个 SLI 异常，这个模块就可以定义为异常，进而定义子系统异常、系统异常，这个过程也有种故障冒泡上浮的感觉。</p><p></p><p>我以我们的产品来举例这种效果图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/734f0c69aa555d015852229751dbcd86.png\" /></p><p></p><p>这样的系统我们称为灭火图，最上层是一个个的系统卡片，如果有问题就会有个飘红的小火苗，点击详情进入，可以看到相关子系统，点击故障子系统，可以看到模块以及核心接口的列表，进而可以定位到故障模块或核心接口。产品通过颜色做引导，而且具备层级关系，即可做到一目了然。</p><p></p><h3>可观测性体系要告诉我故障模块的各项依赖是否健康</h3><p></p><p>模块依赖的数据库、中间件、基础网络、机器硬件、第三方服务等等，都会影响模块的健康状况。所以，当模块异常的时候，我们需要知道各项依赖是否健康，如果依赖也异常，那么模块异常的直接原因基本可以断定是异常的依赖项导致的。</p><p></p><p>要用可观测性产品建立这样的视图，核心有两点，一个是依赖的关联关系，一个是依赖项的 SLO 视图（通常体现为 metrics 仪表盘）。下图是一个逻辑示意图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/4421982147704e140791a3118590e58a.png\" /></p><p></p><p></p><h3>可观测性体系要告诉我是否是变更导致的</h3><p></p><p>线上故障，大概 70% 都是变更导致的，所以运维行业中流传一句话叫：“变更是万恶之源”。所以，当故障发生的时候，我们需要知道是否是变更导致的，如果是变更导致的，就要尽快止损。</p><p></p><p>如果迭代比较快，每周的变更数量会很多，那么如何快速定位到是哪个变更导致的故障呢？这需要可观测性产品提供一些数据特征给用户，用户才能方便分析。典型的是在时间维度上做文章。把故障和变更放到一张图上，在时间维度上做对比，比如从故障时刻往前看 1 小时，看看有没有变更，如果有，那个变更很可能就是罪魁祸首。示意图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/0286cb42253bcdbfa54b5d9755ffc3e6.png\" /></p><p></p><p>注意，这里的变更不仅仅是代码变更，还包括配置变更、机器变更、网络变更等等，变更事件收集得越全，越有价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/21c586c7231033ac28f4e0507e5a6211.png\" /></p><p></p><p>上面，我举例了三个可观测性产品需要为用户输出的观点：</p><p>可观测性体系要告诉我故障模块可观测性体系要告诉我故障模块的各项依赖是否健康可观测性体系要告诉我是否是变更导致的</p><p></p><p>当然，还有其他观点可以输出，比如是否是容量不足导致的故障，大家可以自行思考看看还可以让可观测体系输出哪些观点。但是，罗马不是一天建成的，在某个阶段，可观测性体系输出的观点有限，不足以帮我们定位故障，此时，可观测性体系还可以做什么呢？</p><p></p><p>至少，还需要提供工具帮我们分析数据特征，别让用户陷入海量散乱的可观测性 raw data 中。这需要多维分析引导能力、数据串联打通能力。举一个数据串联的例子：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/453c988a566d2f0a1283a435ec48fbec.png\" /></p><p></p><p></p><h2>总结</h2><p></p><p>可观测性体系不能仅仅只有散乱的数据，而应让数据呈现特征，让特征呈现观点，让特征和观点辅助洞察：洞悉故障直接原因，完成止损！这才是建设可观测性体系的核心目标。</p><p></p><p>作者简介：</p><p>秦晓辉，Open-Falcon、Nightingale 创始研发，极客时间《运维监控系统实战笔记》作者，公众号 SRETalk 主理人，快猫星云创业合伙人，创业方向是稳定性保障方向。如果你有兴趣来《<a href=\"https://mp.weixin.qq.com/s/Y4rIfV4_7MuYigLNNrtifg\">运维百家讲坛</a>\"》输出一些自己的宝贵经验和见解，欢迎联系我，联系方式如下：18612185520（微信同号）。</p>",
    "publish_time": "2023-07-04 11:40:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "之江实验室： 如何基于 JuiceFS 为超异构算力集群构建存储层 ？",
    "url": "https://www.infoq.cn/article/83tfpS8cgjfuZTU27QgK",
    "summary": "<p></p><blockquote>今天，高性能计算结合人工智能技术正在推动科研创新。例如通过破解水稻基因密码推动作物育种从“试验选优”向“计算选优”发展，在医药领域快速分析分子与蛋白之间的相互作用，发现潜在的能够有效干预疾病发生的药物分子。&nbsp;</blockquote><p></p><p></p><p>之江实验室就是上述科研创新的推动者，实验室由浙江省政府主导、浙江大学等院校支持、企业参与的事业单位性质的新型研发机构，为材料、基因、制药、天文、育种等科学领域的研究提供新的方法、工具和手段。&nbsp;</p><p></p><p>由于算力资源的天然异构性，以及不同技术实现的计算能力往往出自不同的系统架构或指令集，会导致软件不兼容性，从而提高了算力使用的门槛，也使得算力利用率难以有效提高。为解决这个问题，之江实验室将各种异构算力资源汇聚在一起，形成一个庞大的“算力池”。本文将分享之江实验室如何基于 JuiceFS 为超异构算力集群构建存储层的实践。</p><p></p><h2>01-之江实验室的输出反应堆</h2><p></p><p>数字反应堆是之江实验室一个大型科研装置。整个科研装置由软硬件部分组成。在软件方面，负责研发之江瑶光智能操作系统。</p><p><img src=\"https://static001.geekbang.org/infoq/d0/d053313d04e6e76453f1f40ccb6de9b3.png\" /></p><p>该智能操作系统主要包含两个关键组成部分。首先，它提供了通用的计算平台解决方案，为上层应用层提供支持。通过这个平台，用户可以针对不同的应用领域，如计算材料、计算制药、计算天文等，进行开发和应用。</p><p></p><p>其次，我们实现了一个异构资源聚合方案。在之江实验室，我们拥有多个异构集群，包括CPU集群、GPU集群以及一些超算资源。这些不同类型的计算资源具有不同的使用方式。通过异构资源聚合方案，我们将这些不同的计算资源统一聚合，实现统一的管理和使用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/57e367290b5c79bdbb57c8cd27da20c3.png\" /></p><p>整体架构如上，在之江实验室的计算与数据中心，我们部署了多套异构集群，包括 H3C 的傲飞集群、之江的计算集群、国产的曙光集群等，以及边缘计算场景，这些都纳入了我们的管控系统中。通过集群设备插件的方式，我们对这些不同的集群进行了统一抽象，并实现了算力的聚合。在整个异构算力联邦的体系中，我们将这些不同的算力集群都抽象为 Kubernetes（k8s）集群进行管理。</p><p></p><p>上层下发的业务指令及不同类型的作业，通过元调度器来决定将这些作业发送到哪个集群。根据不同的调度策略，如计算优先级、能耗优先级和性能优先级，来确定计算作业的具体执行方式。目前，我们已接入约 200P（PFLOPS, 1P 相当于每秒执行一千万亿次浮点运算） 的 AI 算力和 7000 核的 HPC 算力。</p><p></p><h3>算力侧对存储的需求</h3><p></p><p>首先：存储层的抽象和统一。因为在许多计算场景中，包括超算和AI训练，都使用 POSIX 接口。因此，我们希望在这一层面上统一使用 JuiceFS 的接口来提供服务。</p><p></p><p>第二个方面：存储方案的通用性。目前接入的算力集群是异构的，那么尽量需要考虑方案在不同的异构集群中都能适用。</p><p></p><p>第三个方面：数据的编排条件。我们的数据是有典型的冷热特性，在一个任务在计算过程中，它用到的数据是热数据，任务计算之后或者过了几天之后，这个数据就变成了冷数据，我们对冷数据的读和操作是比较少的。</p><p></p><p>第四个方面：存储性能的要求。数据读写性能要好。特别是热数据的读取性能。在算力集群中，计算资源非常宝贵，如果因为数据读取慢导致CPU，GPU空转等待，是极大的浪费。</p><p></p><h3>存储方案选型</h3><p></p><p>方案1：裸露的对象存储（OSS）与S3FS + NAS 相结合</p><p></p><p>这个方案存在一个较大的问题，即直接使用裸露的对象存储性能非常差。另外，裸露使用对象存储的 S3FS 挂载点经常会出现莫名其妙的丢失。一旦挂载点丢失，容器将无法访问，如果要恢复挂载点，必须对整个容器进行重启，这对用户业务造成了很大的干扰。</p><p></p><p>由于开始时集群规模较小，而且这套方案部署简单，实验室一开始采用了这种方案进行部署。但随着集群规模的逐渐扩大，尤其是从去年开始建设的数字反应堆，从单一集群到多集群的演进过程中，当节点数量从最初的 10 多台逐渐扩展到 100 多个节点时，这个方案基本上已经行不通了。</p><p></p><p>方案2：Alluxio + Fluid + OSS</p><p></p><p>经过调研，我们发现该方案的结构相对复杂，涉及到许多组件的组成。之江实验室是一个超异构的多集群环境，由于 Alluxio 并不是一个强一致性的文件系统，它实际上只是一个缓存的粘合层。在这种多集群环境下，会面临元数据不一致的问题，而解决这个问题特别困难。由于上层用户的业务产品非常繁多，我们不能干涉用户的使用方式。在这种情况下，如果不同集群之间的数据不一致，将会导致严重的问题。其次底层仍然使用OSS，当数据规模扩大到一定程度时，由于 OSS 的元数据性能问题，当存储规模达到一定级别后， 元数据同步、新集群的缓存层初始化等操作也会遇到较大的性能瓶颈。</p><p></p><p>方案3 ：JuiceFS（最终采用）</p><p></p><p>JuiceFS 有非常详细的社区文档，可以直接上手使用，并且在我们的搭建测试集群以及最终的上线部署中表现出色。另外，JuiceFS 支持 CSI可以容器化部署，另外对国产硬件的适配性较好。因此我们最终选择将 JuiceFS 作为我们算力侧的存储底座。</p><p></p><h3>使用 JuiceFS 的优势</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f56f28f2d866ea9921f0c440327c20b.png\" /></p><p>首先，JuiceFS 提供了丰富的元数据引擎选择，比如 Redis 和 TiKV，使得 JuiceFS 具有较好的元数据性能。目前我们实验室使用的是由三个节点搭建的 TiKV 作为元数据引擎。由于该配置是去年建立的，现在的性能已经有些不够用，后续我们将逐步提升性能。</p><p></p><p>最初我们考虑使用 Redis 作为元数据引擎，但后来发现如果使用 Redis，就无法实现水平扩容。从而使用了 TiKV，则可以随着文件系统数量的增长逐步扩展，这确实更好。</p><p></p><p>第二，在跨集群环境下，使用 JuiceFS 可以实现文件的原子性和一致性。在集群 A 中写入的文件在集群 B 中立即可见。然而，如果使用 Alluxio，就无法做到这一点。Alluxio 需要进行一些数据同步事件等操作才能实现，而这些事件实际上会带来一定的开销。</p><p></p><p>第三， JuiceFS 具备缓存能力。客户端可以配置一个缓存目录，使用缓存后可以大大降低算力集群对底层存储的压力。</p><p></p><p>第四，JuiceFS 对于 POSIX 的兼容性非常好。我们发现 Alluxio 的实际兼容性并不那么好，而且其客户端性能相对较一般。Alluxio 可能更适用于不同异构数据源的统一接入层，用于读取数据较好。但是，如果需要频繁写入或修改数据，则可能使用起来并不理想。</p><p></p><p>第五 JuiceFS 的社区是常活跃。</p><p><img src=\"https://static001.geekbang.org/infoq/52/52e68a70d44d1821247c428d73930845.png\" /></p><p>这个是我们自己在实验室环境下测出来的，测试工具：FIO 16 线程、4M Block 、1GB 数据 上图 NAS 的性能就没法看，因为当时测评的时候还在生产环境正在提供服务，当时有大概七十几个节点正在运行，带宽很小，基本就运行不了。</p><p></p><h2>02-存算分离架构演进</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ceb60d24d04c55aada05f0746fe6894e.png\" /></p><p>初期，整个高性能计算的过程实际上是分为很多个环节，但是数据分散在不同的存储系统中会带来使用效率和便利性上的挑战。为了简化数据的管理和流转，我们使用了一个统一的存储底座作为存储基础设施。存储底座的核心能力包括高可靠性、低成本和高吞吐量，因此我们选择了对象存储作为存储底座。将数据存储到对象存储中可以轻松实现数据的冷热分层，从而节省存储空间。</p><p></p><p>然而，直接让计算集群直接使用裸对象存储仍然存在一些问题。首先是元数据性能差的问题，例如对同一目录下文件的列表操作，在文件数量较多的情况下，耗时会非常长。第二个问题是带宽消耗大，数据湖提供的是普通的IP网络，而不是 RDMA 高速网络，因此总带宽有限。</p><p><img src=\"https://static001.geekbang.org/infoq/af/afc7be237781c15bc7555e3fd2e43d07.png\" /></p><p>因此，在对象存储之外，我们还建立了一个元数据集群，并使用了 TiKV 数据库。基于对象存储和 TiKV，我们构建了JuiceFS 分布式文件系统。算力集群通过在节点上安装 JuiceFS 客户端来读取文件系统的数据。这样，我们可以克服对象存储的一些限制，提高元数据性能，并减少带宽消耗。</p><p></p><p>为了实现高效的数据流转，我们通过文件管理系统允许用户进行文件的上传和下载操作。文件管理系统底层采用JuiceFS S3 网关，将数据写入底层存储。</p><p></p><p>除了数据湖和元数据集群，我们还建立了一个高速缓存集群，它紧密部署在计算集群中，主要目的是为了实现最佳的 I/O 性能。这样解决了计算集群与对象存储数据湖底座之间高效数据流转的问题。用户并不需要关心数据是存储在对象存储中还是在高速缓存集群中。</p><p></p><p>算力系统对数据流转进行管控。计算集群和高速缓存集群之间通过200G的RDMA高速网络连接。高速缓存集群上部署了BeeGFS高速并行文件系统，将该文件系统作为一个目录挂载到计算集群。这样，计算集群可以像使用本地目录一样使用该缓存系统。</p><p></p><h2>03- 存储能力产品化建设</h2><p></p><p>在不同的业务场景中，对存储的需求和性能指标是不一样的。为了能够更高效地服务用户，我们提出了打造存储能力产品化这样的想法，目前JuieFS 被应用到了以下几类存储产品中。</p><p></p><h3>通用文件存储</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4fbcd5b12b0f9eee4a2115954ea76c7a.png\" /></p><p>JuiceFS 会将其数据保存在一个特定的目录下，并根据用户所属的组织架构生成一个唯一的访问路径。通过直接将该路径挂载到容器内，实现数据的隔离。用户可以通过页面端进行文件的上传和下载，也可以使用我们提供的命令和工具对文件进行操作。</p><p></p><h3>存储卷</h3><p></p><p>在初始建设阶段，通用文件存储存在一个问题，容量的扩展性较差。底层的对象存储集群（oss）的容量有限，随着数据量的增加，用户无法申请更多的存储空间。为解决这个问题，我们引入了存储卷的概念。</p><p><img src=\"https://static001.geekbang.org/infoq/a0/a0f22e78e2e1bfa0f577a6919c1c8950.png\" /></p><p>存储卷可以类比为云盘，不同的存储卷相当于不同类型的云盘。对于不同的存储类型，我们可以将它们包装成不同的存储卷，以满足用户在不同场景下的需求。</p><p></p><p>对于需要频繁地读写海量小文件的场景，它需要使用延迟较低且吞吐量较高的存储产品。为满足这种需求，我们将之前搭建的高速缓存集群转化为高速存储卷的功能，直接将其文件系统目录开放给用户使用。这样，用户可以直接使用高速存储，而无需通过 JuiceFS 来访问，可以更直接地感受到高速存储的性能优势。</p><p></p><p>而对于需要保存大数据但实际上不频繁读取的用户，可以结合JuiceFS和对象存储来创建标准存储卷。这样可以提供较大的存储容量和可接受的吞吐性能，同时相对于高速存储卷，支持跨集群的网络互通能力。</p><p>此外，一些用户可能对性能有更高的要求，例如他们需要本地盘的产品，但同时也需要数据持久化的能力。在 Kubernetes 场景下，如果用户直接将数据写入本地盘，存在数据丢失的风险，例如遇到意外重启或物理节点问题。在这种情况下，用户需要一种持久化的解决方案。我们可以通过将用户在受影响节点的本地盘开放一部分存储空间作为本地存储卷，并在作业调度时根据用户指定的存储卷将任务调度到指定的节点上。</p><p></p><p>另外，不同存储产品在容量、吞吐量和跨集群互通能力方面存在差异。例如，高速存储可以在集群内部进行互通，但无法跨集群；存储产品的容量和成本也各不相同。高速存储采用全闪存的集群，建设成本较高，而对象存储的建设成本相对较低，且具备较大的存储容量。因此，将不同的存储硬件（设施）能力包装成不同的存储产品来适配用户不同的业务场景。</p><p></p><h3>数据编排</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed750852102e298c674c4b474a42a323.png\" /></p><p>在使用 JuiceFS 时，我们还实现了一个数据编排功能。管理员可以将常用的数据集上传到文件系统某个目录，这个目录可以在上层抽象成一个公开的数据集。不同用户在创建作业时都可以挂载这些数据集。普通用户也可以上传自己的私有数据集，并通过 JuiceFS 的预热功能对这些数据集进行预热。</p><p></p><p>我们在算力集群内部建立了一个高速缓存集群。使用 warmup 指令，用户的数据集可以直接从两端预热到计算节点的高速缓存集群。这样，用户在进行大量模型训练时，可以直接与自己搭建的高性能集群交互，无需与远程 OSS 集群进行交互，从而获得更好的性能。</p><p></p><p>另外，这种设置可以降低对象存储底座的网络带宽压力。整个缓存的淘汰过程由 JuiceFS 客户端自动管理，因为可以对访问目录进行上限容量的配置。对用户来说，这部分功能相对透明且易于使用。</p><p></p><h2>04-JuiceFS使用过程当中也遇到了一些问题</h2><p></p><p></p><h3>文件读取性能</h3><p></p><p>在我们选择使用JuiceFS之后，我们在内部进行了一些文件读取性能的测试，并与算法团队合作进行了测试。当时，从测试结果看，JuiceFS 的读取性能总是比 NAS 慢得多。我们开始查找原因为什么 JuiceFS 比 NAS 还要慢。</p><p><img src=\"https://static001.geekbang.org/infoq/7b/7b47102b88536be0cbf622fe988e46ce.png\" /></p><p>后来我们发现，在使用 JuiceFS 和 TiKV 作为元数据的场景中，像列举目录这样的 API 操作实际上是随机的，它并不像 NAS 或其他文件系统那样保证一致的顺序。在这种情况下，如果算法是基于随机选择文件或者代码是固定的，那么可能会认为选择的那些文件应该是固定的。</p><p></p><p>在处理海量小文件的场景中，元数据的开销是相当可观的。如果元数据没有被缓存到内存中，每次都需要从元数据引擎那里获取，与没有缓存相比这会带来较大的开销。因此，通过这个问题，我们发现在特定的场景下需要对文件的索引目录进行编排。例如，某个算法可能需要处理数十万甚至数百万个文件，如果要确保算法训练的一致性，首先需要将这些文件作为索引文件，作为自己的一个索引目录树。每次进行算法训练时，直接读取索引文件，而不再调用 list dir 的操作，以确保这个文件夹下的文件目录树在算法训练中保持一致性。</p><p></p><p>编著注：造成读性能慢主要与用户的使用场景相关，评估后对“目录的随机读”这个功能未作调整，如果其他用户在这个问题上也有类似问题，欢迎提出。</p><p></p><h3>TiKV 无法垃圾回收</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af9f5f17d471180741b51cdf60eb0e07.png\" /></p><p>在使用过程中，我们遇到了 TiKV 无法进行垃圾回收的问题。由于我们只使用了这个文件系统，并且显示容量为 106T、1.4 亿个文件，而 TiKV 占用了 2.4T 的容量，这显然是不正常的。</p><p></p><p>根据官方文档的显示，例如 Redis，大约 1 亿个文件应该只占用大约 30GB 的容量。我们进行了排查后发现可能是 TiKV 的元数据引擎没有进行垃圾回收。我们还查看了报表，发现整个垃圾回收指标为空。可能的原因是我们只部署了 TiKV，而没有部署 TiDB。然而，TiKV 的垃圾回收实际上需要依赖 TiDB，这是一个容易被忽视的问题点。</p><p>编著注：JuiceFS 在 #3262 和 #3432 的 PR 中加上了 TiKV 的后台 GC 任务，修复了这个问题。这些修复已经在 v1.0.4 中合入。</p><p></p><h3>JuiceFS client 内存占用较高</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af4c48bdeb39da32ffea72f8c40d73de.png\" /></p><p>当我们挂载 JuiceFS 客户端时，我们将高速缓存集群设置为保存目录，并将其容量设置得相对较高，理论上限为 50T。</p><p></p><p>在这种情况下，JuiceFS 客户端会定期扫描缓存目录，并建立内存索引，以便 JuiceFS 知道哪些数据位于缓存目录中。因此，这会占用相当多的内存。如果目录非常庞大，我们建议用户关闭这个扫描功能。</p><p>在测试小文件随机 I/O 时，我们觉得表现还可以，但在测试顺序 I/O 时，出现了一个较大的问题。例如，使用 dd 命令创建一个 500MB 的文件，结果发现对象存储生成了大量快照。可以看出，这里的存储和对对象存储的操作远远超过了创建一个 500MB 文件应有的操作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5ebf2b6c93d18741d6c3c18c77427d4.png\" /></p><p>在进一步排查时，我们发现在启用&nbsp;-o writeback_cache&nbsp;参数后，顺序写会变成随机写，从而降低整体的顺序写性能。该参数只适用于非常高级的随机性场景。如果在不是这种场景下使用该参数，将导致严重的问题。这也是在使用 JuiceFS 时需要注意的一个点。</p><p></p><p>编著注：这个问题主要是针对使用 NAS 做缓存的场景，已经在1.1beta中优化，扫描时大幅减少内存占用，并提升速度。JuiceFS 在 #2692 中添加了 --cache-scan-interval 来自定义扫描时间，并且可以选择只在启动时扫描一次或完全关闭扫描，用户可以配置该选项。对于使用本地盘做缓存的用户则不需要调整。</p><p></p><p></p><h2>05-后续规划：更丰富多样的存储产品</h2><p></p><p>多层次</p><p>我们将提供更多层次的软硬件产品，并将这些能力以不同的存储卷形式进行产品化，以适应用户在不同场景下的存储需求。</p><p></p><p>隔离性</p><p>目前存在数据安全风险，所有用户的数据都存储在同一个大型文件系统中并通过hostpath挂载在裸机。如某些用户拥有节点登录权限，实际上可以访问整个文件系统内部的数据。为了解决这个问题，我们计划在后续采用 CSI 模式结合路径定制来避免隔离性问题。我们还将上线配额管理功能。在用户使用存储产品时，需要有一种强制手段来限制用户可以使用的存储容量，并且能够准确查看用户实际使用了多少容量。直接使用 du 命令查看容量的过程开销很大，并且不太方便。配额管理功能将解决这个问题。在计量计费场景下，我们需要了解用户产生的流量和消耗的能量，并根据实际使用的容量进行计费。因此，良好的容量管理能力是必需的。</p><p>监控&amp;运维</p><p>在使用 JuiceFS 时，我们是直接在物理机上挂载，通过暴露一个监控端口，我们的生产集群可以与这些端口进行通信，并建立了一套监控系统，可以监控和收集所有的监控数据。数据的容灾和迁移能力目前还比较欠缺。我们遇到了一个典型场景，即现有集群的容量不足，需要上线新的集群。在新旧集群之间，如何处理数据迁移以及不同数据的迁移方式，如何在不影响生产用户的情况下尽量保证业务不中断，实现数据迁移仍然是一个较为困难的问题。因此，我们计划在后续寻找解决方案，以提升这方面的能力另外，我们还在开发基于 JuiceFS 和 CSI 插件的通用能力，以实现在不同的存储客户端上动态挂载的能力。在生产环境中，用户对挂载参数的调整有一定需求，因为不同的挂载参数适配不同的业务产品。然而，如果直接调整挂载参数，可能会导致整个物理节点的中断。因此，如果能够实现动态挂载的能力，用户只需要对业务进行适当的切换，而无需进行重启等操作。</p><p></p><p>对 JuiceFS 一些功能期望：</p><p>卷管理能力&nbsp;（配额、用户、权限）卷能力在之前已经部分实现了配额的功能，但实际上我们更需要的是基于用户和管理人员权限的管理能力。目前，JuiceFS是挂载在一个大型文件系统上，如果为每个用户创建文件系统，将会带来很高的开销。因此，我们目前采用的是基于一个大型文件系统，通过不同的目录来管理不同用户的权限配合，缺少一个统一的、中心化的用户权限管理系统，仍然需要依赖Linux的权限管理。然而，Linux的权限管理是分布在不同节点上的，对用户权限的管理相对较为困难。我在思考是否可以依赖元数据，并将其作为一个中心化数据库的能力，实现用户和权限的管理。这样，卷的管理能力就可以更加产品化。支持分布式缓存能力。它可以充分利用计算机的物理资源和本地磁盘，通过集群内的计算节点和网络进行利用。目前我们了解到，商业版有提供这个功能，但社区版也还没有。挂载点热更新能力。在不同的场景下，用户需要不同的挂载参数，但是卸载和重新挂载的操作太重，会影响用户的业务。用户可以接受短暂的不可读取或者读取中断，但是不能重启业务容器或中断算法或策划程序。我们正在内部调研和开发这个能力。</p><p></p><p>编著注：目录配额需求在1.1Beta中已经实现了，详情请大家关注下周的发版信息。分布式缓存和挂载点更新能力，目前商业版可以提供，这两个功能也在社区版的规划当中。</p><p></p><p>直播回顾：https://www.bilibili.com/video/BV1Fo4y1V7Ka/</p><p></p><p>作者简介：</p><p>洪晨</p><p>之江实验室高级工程专员，负责计算侧存储的架构和演进</p>",
    "publish_time": "2023-07-04 12:15:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "承接百亿保单的新基建，无界山到底是什么山｜InfoQ 《一探到底》",
    "url": "https://www.infoq.cn/article/ZhIJE05sJgdxkPdYci37",
    "summary": "<p>新基建的推动使得数字化转型成为各行各业的必然趋势，众安保险也从其中看到机遇，把握机遇并通过加强创新，探索出共建共赢的新模式。《一探到底》众安之行的第三期，带大家探访保险行业的一座“神山”——无界山的前世今生，并畅谈如何在新基建时代通过技术创新实现合作共赢。</p>",
    "publish_time": "2023-07-04 12:29:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "技术解读：Cube安全容器高并发低延时实践之路",
    "url": "https://www.infoq.cn/article/RgWPqx5HXvFX5gVVCIPT",
    "summary": "<p>随着云计算技术的发展，Serverless凭借其免运维、按量付费和弹性伸缩等特点逐渐成为热门话题。然而，传统的虚拟化计算无法满足Serverless的需求，如高并发创建能力、冷启动速度等。为此，腾讯云技术团队推出了Cube轻量虚拟化产品，通过对管控流程、KVM、主机OS、VMM、子机OS等进行全链路精简和优化，实现高性能、低开销、高密度的运行环境。Cube还针对快照方案进行了深度优化，大幅提升启动速度，从而更好地满足Serverless场景的需求。</p><p></p><h1>背景</h1><p></p><p>随着云计算技术的持续演进，具有免运维，按量付费，弹性伸缩等特性的serverless逐渐成为行业关注的热点。从Serverless技术最有代表性的FaaS产品现网运营数据来看，有以下特点：</p><p></p><p>1） 单实例生命周期短， 平均执行时间50ms， P95小于100ms</p><p>2） 并发创建能力要求高， 资源流转快， 每天近千万次的资源流转（创建/销毁）</p><p>3） 冷启动速度要求高， 要求百毫秒级的冷启动时延</p><p></p><p>在通用的虚拟化计算环境下，无法满足Serverless的以上特点， 根本原因是通用虚拟化的实例创建过程会涉及到和装箱， VPC、CBS、安全组等多个子系统的交互，实例创建往往需要等待数十秒的时间，无法满足Serverless对冷启动和高并发的诉求。 而实例本身的创建销毁时间远远大于实例本身的执行时间， 会导致大量的算力浪费。</p><p></p><p>在实际的运营中，通用虚拟机架构下，为了满足Serverless的运营指标诉求， 通常采取储备资源池的方案， 也就是提前创建好一批虚拟机实例， 直接用提前创建好并已运行的虚拟机实例来满足客户高并发高资源流转的诉求。 这个方案通过储备资源池来平衡客户高并发高资源流转的需求与平台低并发资源流转慢的矛盾， 但储备资源池会给平台带来比较大的呆滞成本以及很大的算力浪费。</p><p></p><p>为了让底层基础设施更加匹配Serverless的诉求， 腾讯云技术团队对管控流程、KVM、主机OS、VMM、子机OS进行全链路精简和优化， 设计了Cube轻量虚拟化产品。Cube&nbsp;方案能提供高性能、低开销、高密度运行环境，涵盖从主机OS、虚拟化、虚拟机OS整套优化。</p><p></p><p>此外，Cube还针对快照方案进行了技术栈深度优化，引入多项关键功能，将启动速度进一步提升，使得轻量级场景下的虚拟机运行更加迅速。通过这些优化，Cube在有限空间内，能提供高性能、低开销的解决方案。</p><p></p><h1>整体架构</h1><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c6/b3/c653d4a8656f8291ff53314bd28c64b3.png\" /></p><p></p><p></p><h1>技术挑战</h1><p></p><p></p><h2>虚拟机管控流程</h2><p></p><p>通用虚拟机的创建流程涉及与虚拟化，网络，存储等诸多IaaS基础设施的交互，这是制约通用虚拟化高并发高资源流转能力的关键瓶颈。</p><p></p><p>Cube轻量虚拟机创建流程完全与IaaS基础设施解耦， 将整个虚拟机的创建变成一个单台物理机内可以闭环的操作。</p><p>&nbsp;</p><p>具体实现上，Cube母机初始化时与IaaS基础设施交互， 得到可与IaaS交互的网络资源（VPC IP）与存储资源（CBS）， 并在内部闭环管理网络和存储资源的分配。</p><p>网络上，Cube在VPC网络之上， 实现了一个单机维度的Cube网络， 而且这个网络可以通过CubeGW实现和VPC网络的互通；存储上，Cube将本地盘和CBS云盘资源进行了切割管理，将只读存储通过virtiofs的方式暴露给虚拟机， 可写存储结合写时复制的技术以virtio-blk的方式暴露给虚拟机。</p><p></p><p>通过上述存储，网络等虚拟机配套资源全部在母机范围内的闭环管理，从管控流程上完全消除了外部基础设施的影响因素。</p><p>&nbsp;</p><p></p><h2>虚拟化底座（KVM）</h2><p></p><p>KVM作为虚拟化的底座，在通用VM场景中已经经过千锤百炼，然而在轻量虚拟化产品高并发超买的场景，却暴露出来了很多性能和时延相关问题，典型的如高并发操作下lock contention导致的时延。</p><p>首先是irqfd操作引入接近100ms时延。irqfd是KVM提供给VMM的将eventfd绑定到VM中断的一个feature，用户态的VMM通过wirteirqfd，KVM在内核层面将对应中断注入到VM中去，这个feature通常被virtio设备使用，比如virtio设备的队列事件，在CUBE产品中因为irqfd注册引入了60ms以上时延。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/33/33/337efa06b0f57dee31374abfeeab1233.png\" /></p><p></p><p>&nbsp;</p><p>经过对KVM中irqfd注册流程分析，腾讯云技术团队针对非直通设备（CUBE产品）场景彻底消除了这部分时延影响，而针对更为棘手的直通设备场景，积极在社区讨论终极解决方案。</p><p>&nbsp;</p><p>另外一个典型时延问题是，nx-lpage-recovery内核线程创建会导致创建VM引入100ms的抖动，是因为高并发场景下这个内核线程启动引入了同步等待，放大了VM创建的时延。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/70/3f/702f22624720502b35a98ee004cf113f.png\" /></p><p></p><p>&nbsp;</p><p>经过调查，腾讯云技术团队发现nx-lpage是KVM是为了缓解ITLB multihit CPU漏洞的软件缓解方案，而nx-lpage-recovery内核线程则是针对这组缓解方案造成性能损失的补偿机制。且不说这个补偿方案的效果，CUBE产品环境中，该方案都不会触发这个问题（需要VM使用大页），再者新版的Intel CPU已经fix了这个硬件漏洞，看起来很普通的一个内核线程启动，在场景中竟然有这么大副作用，对KVM持续优化深入到每一个细节。</p><p></p><p>除了这些问题，针对高并发超买场景降低开销和时延，团队还对KVM的一些参数进行了调整和调优，比如halt_poll_ns等。</p><p></p><h2>轻量化VMM</h2><p></p><p>社区近些年兴起基于RUST的轻量级VMM，相比QEMU简化了设备模型，更小的内存和运行开销，在CUBE产品中，腾讯云技术团队基于cloud-hypervisor构建VMM组件（cube-hypervisor），是因为其在保持了轻量化的同时，有较为完备的现代VMM具备的基础功能，如热插拔/TDX等。</p><p></p><h3>存储架构调整</h3><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/39/a8/394681eeac5296yyyy878d4cbcbd8ba8.png\" /></p><p></p><p>&nbsp;</p><p>在产品架构上为了适配容器场景，腾讯云技术团队对VM存储进行读写分离，使用更为灵活的virtiofsd为容器提供rootfs目录，使用传统的disk为容器提供临时数据写盘。为了达到极致的性能，对viriofsd进行了native改造，改造后的virtiofs从独立进程变成了CH的一组线程，处理上优化了vhost逻辑，可以像普通virtio设备一样直接操作后端fs。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/50/7b/50f26ed1422bc6325b1abf4def272d7b.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>快照方案提速</h3><p></p><p>在技术演进方面，腾讯云技术团队对VM冷启动做了多项优化，如通过并发内核加载加速VM启动，以及调优启动过程中的阻塞行为，但仅仅通过调优只能将VM交付时间缩减到100ms左右，无法达到对启动时延的极致要求，而在实际场景中，不同VM的内核和rootfs都是一致的，针对这个高度重复的动作，是否有办法做到一次性优化么？答案是有的，这里，腾讯云技术团队选择了VM快照技术。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0f/89/0fe1fc3d300f41c57cbdd1722c047289.png\" /></p><p></p><p>&nbsp;</p><p>快照技术原本是VMM提供的对VM快照的保存和恢复，快照保存时，将VM暂停下来，将VM所有状态保存到快照文件中，快照恢复时，基于快照文件恢复VM，恢复完成之后继续VM运行，快照技术语义上支持1 to 1的VM状态恢复，且对VM后端设备有一致性要求（磁盘/网卡）。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8c/51/8c00d510c68a7ca0e25ec30394600451.png\" /></p><p></p><p>&nbsp;</p><p>首先，需要对快照技术进行1 to n的VM状态恢复改造，实现VM内部状态和后端设备无关，并可以将保存的旧设备状态恢复到新设备。实际实现中，通过替换后端设备，以及基于快照中设备状态对设备进行重新初始化，实现了快照恢复过程中后端设备无感替换。</p><p></p><p>经过对快照技术持续优化，最终做到将VM交付时间从100ms+降低到20ms以内，同时为单个VM减少了20MB+内存（单机1000个VM节省20G内存）。</p><p></p><h3>网络方案重构</h3><p></p><p>资源池化是降低并发资源访问的法宝，在产品架构上，通过池化解决一众资源依赖问题，如磁盘池化，CGROUP池化，以及网络设备池化，网络方面更是结合ebpf实现快速转发路径，已经努力将网络层面的开销降到了最低，然而高并发测试总是能够发现彩蛋，这一次是TAP设备，实测发现，VM在TAP设备初始化上耗时50ms以上，MAX甚至超过100ms。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/ae/dc/aedabcb5f4fa79d2cf99629235888fdc.png\" /></p><p></p><p>&nbsp;</p><p>VM创建过程需要对TAP设备做初始化，然而这个高并发的操作在母机内核触发了全局锁冲突，造成大量时延。很显然解决这个问题需要深入母机内核TAP做深度优化，然而有没有跟简单的办法呢，是否可以避免重复的TAP设备初始化动作呢？腾讯云技术团队对TAP网络方案进行了重新设计，将TAP设备的初始化放在了TAP池化初始化中，而VM的网络初始化只需要从池子中申请一个对应的资源就可以了，经过对方案的不断调优，成功将VM启动过程中TAP设备初始化降低到了1ms级别。</p><p>&nbsp;</p><p></p><h2>母机OS</h2><p></p><p>高并发场景下，高频的创建销毁会带来母机全局资源的锁竞争，导致性能的波动和不可预期性。</p><p>Cube为了保证高并发场景下冷启动时延的可预期性， 通过以下技术手段来优化</p><p>计算，网络，存储资源的池化对于无法池化的资源，尽量减小对全局资源的冲击Mount/umount</p><p>在mount/umount过程中都需要全局的命名空间锁来保证关键路径的串行操作，在安全容器环境的构建中不但需要通过overlayfs mount创建容器的rootfs，还需要各种形式的bind mount来实现hosts，dns等个性化配置。Cube通过把这种个性化的bind mount转换成overlayfs的一个层次（layer）的方式使mount/umount次数减少到最低。</p><p>进程创建/销毁</p><p>在进程的创建，销毁过程中也需要全局的task锁来确保关键路径的串行操作，&nbsp;为了确保进程创建的时延有保证，cube技术团队将进程销毁的优先级调低， 并引入排队机制串行执行进程的销毁操作。同时通过进程合并等手段减少虚拟机创建需要创建的进程数目。</p><p></p><p>除了高并发的优化，针对高密场景的资源复用，cube也对虚拟机内核，虚拟机镜像，以及容器镜像的复用做了一些工作</p><p>虚拟机镜像通过pmem设备方式在母机层面在所有虚拟机之间进行只读共享，子机通过DAX方式直接访问母机内存容器镜像的读写存储分离，只读部分通过virtio-fs暴露给虚拟机，并实现容器镜像和page&nbsp;cache在母机层面的复用； 可写部分通过virtio-blk暴露给虚拟机， 可写块存储使用空洞文件减少实际磁盘使用量的同时， 通过reflink和写时复制技术实现磁盘块的按需使用</p><p></p><h2>子机OS</h2><p></p><p>通用虚拟机的设计目标是提供一个通用的计算环境，要求子机OS（内核与操作系统）的功能足够丰富与灵活，但在Serverless场景中，过多的功能反而变成了一种负担，牺牲了极速的启动速度，也额外耗费了太多的资源。Cube为了满足Serverless极致的用户体验，对子机OS的所有功能模块进行了深度的裁剪和优化。</p><p>&nbsp;</p><p>子机镜像（虚拟机rootfs）层面：</p><p>裁剪掉所有不需要软件包，只保留极少数关键库和命令，镜像大小控制在128MB以下。使用Cube-agent作为init替换庞大的systemd体系，系统启动时只进行少量必须的初始化操作。</p><p>子机内核层面：</p><p>关闭所有不需要的模块和功能，减少内核代码段和数据段的大小。调整内核各个子系统的hash表大小，减少额外内存消耗。调整预留内存的默认计算算法，提供更多资源给业务进程。</p><p></p><p>经过裁剪优化，1C128MB的规格下，相比通用虚拟机秒级启动速度，Cube虚拟机启动时间降至80ms以下，启动时内存消耗也降至20MB左右。</p><p>&nbsp;</p><p></p><h1>总结</h1><p></p><p>目前在大量背景负载情况下，50并发启动Cube轻量虚拟机的时延可以控制在100毫秒以下的水平。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ec/09/ec97cc53f782fa615c19d7387675a209.png\" /></p><p></p><p>&nbsp;</p><p></p><h1>回馈社区</h1><p></p><p>值得一提的是，腾讯云技术团队在优化Cube在高密高并发场景下性能的同时也发现并修复了其他开源组件长期以来没有被发现的问题。</p><p>&nbsp;</p><p>golang &nbsp;runtime相关：</p><p>在高并发操作tap 设备时会触发了 go runtime epoll 对象复用的bug。</p><p>这是一个go net epoll 模型的重大远古缺陷，已提交社区并推动修复。</p><p><a href=\"https://github.com/golang/go/issues/59545\">internal/poll, runtime: got `not pollable` err when reading fifo or socket · Issue #59545 · golang/go · GitHub</a>\"</p><p>&nbsp;</p><p>kata container相关：</p><p>容器控制流程hang住的问题反馈与修复</p><p><a href=\"https://github.com/kata-containers/kata-containers/issues/6059\">runtime: all APIs are hang in the service.mu · Issue #6059 · kata-containers/kata-containers · GitHub</a>\"</p><p><a href=\"https://github.com/kata-containers/kata-containers/issues/6171\">runtime: unix socket file leak · Issue #6171 · kata-containers/kata-containers · GitHub</a>\"</p><p>&nbsp;</p><p>cloud-hypervisor 相关：</p><p>cloud-hypervisor 清除 virtiofs dax mapping时offset设置错误。&nbsp;<a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5235\">virtio-devices: vhost-user: fs: reset offset when to remove the whole mapping by HowHsu · Pull Request #5235 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"备初始化时Breadth First Traversal 设备树实现不当，存在多余的内存拷贝开销。</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5428\">https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5428</a>\"</p><p>支持网络设备配置offloading能力feature</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5029\">vmm: add configuration for network offloading features by zhuangel · Pull Request #5029 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"</p><p>支持block设备latency统计</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5044\">https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5044</a>\"</p><p>解决vcpu异常退出造成死循环问题</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5211\">vmm: properly set vcpu state when thread exited by zhuangel · Pull Request #5211 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"</p><p>优化virtio console信号线程</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5240\">Do not start signal thread is there is no need. by zhuangel · Pull Request #5240 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"</p><p>&nbsp;</p><p>virtiofs相关：</p><p>在cache=never 模式下，virtiofs前端驱动的private mmap的page cache存在一致性问题。</p><p><a href=\"https://lore.kernel.org/linux-fsdevel/20230509080128.457489-1-hao.xu@linux.dev/\">[RFC PATCH] fuse: invalidate page cache pages before direct write - Hao Xu</a>\"</p><p>在cache=never 模式下，对shared_mmap的缺乏支持。</p><p><a href=\"https://lore.kernel.org/linux-fsdevel/20230505081652.43008-1-hao.xu@linux.dev/\">[PATCH] fuse: add a new flag to allow shared mmap in FOPEN_DIRECT_IO mode - Hao Xu</a>\"</p><p>在cache=always 模式下目录 page cache失效，导致readdir性能变差。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/172\">passthrough: add KEEP_CACHE flag for directory file when cache=always (!172) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>没必要的open(/proc/pid/mountinfo)操作导致的mount操作时延抖动。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/167\">passthrough: open mountinfo proc file in case we readlly need (!167) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>无效的后端读写size限制。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/161\">server: remove buffer size check for read/write (!161) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>&nbsp;</p><p>腾讯云技术团队对这些问题提出了修复方案，并将其贡献给了上游社区。此外，根据使用场景在功能上做了进一步的增强。</p><p>virtiofs设备增加了ratelimiter功能以支持QoS。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/147\">Introduce RateLimiter to virtiofsd (!147) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>virtiofsd新增加了cache=part模式，使得节省内存的同时不影响readdir操作的效率。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/173\">Add a new cache policy Part (!173) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>&nbsp;</p><p>rust-vmm相关：</p><p>解决linux-loader没有正确处理ELF align的问题</p><p><a href=\"https://github.com/rust-vmm/linux-loader/pull/128\">https://github.com/rust-vmm/linux-loader/pull/128</a>\"</p><p>解决virtio-queue没有正确处理descriptor align问题</p><p><a href=\"https://github.com/rust-vmm/vm-virtio/pull/220\">https://github.com/rust-vmm/vm-virtio/pull/220</a>\"</p><p>&nbsp;</p><p></p><h1>展望</h1><p></p><p>目前为止，Cube对底层支撑功能以及各相关组件进行了大量深入的分析，并有针对性进行优化， 真正的做到了用户按量付费，平台按需生产，并且可以满足用户的性能诉求。</p><p></p><p>技术的道路没有尽头，后续随着技术方案的不断创新和迭代，腾讯云技术团队将不遗余力地持续提高Cube的底层性能，与此同时，在用户体验，易用性等方面做更多的探索和增强。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-07-04 13:12:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "正在诞生的五种编程语言",
    "url": "https://www.infoq.cn/article/ReR4ui073IN4Qdv7dPdg",
    "summary": "<p>本文最初发表于<a href=\"https://kevin-da-silva.medium.com/5-programming-languages-we-can-see-being-born-right-now-4d30b29552e0\">作者的个人博客网站</a>\"，经原作者Kevin Da Silva授权，由InfoQ中文站翻译分享。</p><p></p><p>这个周末，我在YouTube频道上观看了大量关于编程语言的视频，并在Github上看到了很多新的编程语言正在出现。不得不说，IT市场已经变得非常疯狂，但是在大多数公司中，开展工作时都在使用相同的繁琐技术，而不是使用新鲜和时髦的技术。作为一种爱好，编程是一件非常神奇的事情，看到有如此多的语言和工具，这实在太酷了。看着每天都有语言在Github上诞生，这更是不可思议。</p><p></p><p>所以这篇文章的意图是列出一些还没有到1.0版本的语言，但它们绝对值得我们关注。</p><p></p><blockquote>请注意，本文中的“正在诞生”指的是仍低于1.0版本的语言，但这不应该是妨碍你基于它们构建特性的原因，因为一般来说，它们和许多主流编程语言一样完备。</blockquote><p></p><p></p><p>但首先，我们回顾一下现在使用的语言，市场上使用的大部分语言都是由于大的科技企业想要锁定其他的公司，并便于向他们出售商品而被选中的，比如微软的.NET和Typescript、Sun/甲骨文的Java，以及其他公司的杀手级应用（如让JS依然光芒万丈的浏览器、WordPress的PHP以及数据科学中的Python）。</p><p></p><p>然后，硬件发生了变化，处理器上增加了更多的内核，使用以前那些流行的语言就意味着要抛弃处理能力或在软件中增加不安全的代码层，所以新的一批语言崛起了，试图克服这个问题，举例来说Elixir、Rust、Clojure、Go、Scala等。</p><p></p><p>但是，就业市场并不关心计算能力的浪费，仍然一如既往地使用原有的东西。在我看来，唯一相对流行起来的是Go，不仅仅是因为Go是一种相当好的语言，还因为谷歌的影响/声誉（该语言非常棒，有一个非常好的并发模型，但前文提到的其他语言也有这样的并发模型）。</p><p></p><p>而现在，有大量的语言正在诞生，以解决特定主题的问题，如下是我们的名单：</p><p></p><p></p><h2><a href=\"https://grain-lang.org/\">Grain</a>\"</h2><p></p><p>Grain是一种函数式语言，在我看来，它是JavaScript和一点ML的混合体，专注于编译成web assembly，能够在多平台上运行</p><p></p><p></p><h2><a href=\"https://ziglang.org/\">Zig</a>\"</h2><p></p><p>Zig是一门系统语言，但总的来说比Rust简单（也没有那么安全），Zig没有C和C++的影子，如果你不考虑上述三种语言中的任何一种，它是一个合适的选择。</p><p>关于<a href=\"https://bun.sh/\">Bun</a>\"有一个热议的话题，它是基于Zig构建的JavaScript运行时，比Node和Deno更快。</p><p></p><p></p><h2><a href=\"https://vlang.io/\">V</a>\"</h2><p></p><p>V是一门通用的编程语言，也可以作为系统语言，其网站说它非常简单，你可以在一个周末学会，它还说Go程序员会对该语言非常熟悉，因为V语言在很多方面借鉴了Go。</p><p></p><p>V语言的网站也有一些关于磁盘空间和编译时间的基准测试结果，看起来非常有吸引力。</p><p></p><p></p><h2><a href=\"https://factorcode.org/\">Factor</a>\"</h2><p></p><p>迄今为止提到的所有语言中，我认为Factor是最古老的。它出现在2003年，但它目前的版本还不到1.0，所以我将这个语言列为诞生中的语言。</p><p></p><p>Factor是一种栈语言，意味着每个元素和函数调用的行为都类似于栈：</p><p><code lang=\"null\">[ 4 ]-----[ + ]-----[ 3 ]输出错误，因为“+”函数要基于两个元素进行调用，目前栈上只有一个数字[ + ]-----[ 5 ]-----[ 4 ] -----[ 3 ]输出9，因为“+”函数会应用于之前的5和4元素，最终生成的栈为[ 9 ]-----[ 3 ]</code></p><p></p><p></p><h2><a href=\"https://gleam.run/\">Gleam</a>\"</h2><p></p><p>Gleam是面向Erlang虚拟机的类型化语言，Gleam的语法对于类型化语言来说非常优雅和简单。如果能看到Gleam像Elixir一样成功，那就太酷了。</p><p></p><p>这只是一些可供我们尝试的新语言，但还有很多其他的语言，我相信肯定会有足够的材料来写这篇文章的第二部分。</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/HdhHwuPQk4FCdPBpmdlP\">Azure CTO： Rust 已登陆 Windows 11 内核</a>\"</p><p><a href=\"https://www.infoq.cn/article/GFfVLVpkIGOcKYB85Opb\">比 Python 快 35000 倍！LLVM&amp;Swift 之父宣布全新编程语言 Mojo：编程被颠覆了</a>\"</p>",
    "publish_time": "2023-07-04 14:05:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "上架不到一周，Mozilla 的 AIGC 助手就被骂到暂停开发！最老牌的互联网企业居然做不好大模型产品？",
    "url": "https://www.infoq.cn/article/uwO9NbZlcgg7lq5DWBst",
    "summary": "<p>当地时间6月27日，Mozilla&nbsp;发布了与<a href=\"https://supabase.com/\">Supabase</a>\"合作开发的生成式人工智能助手AI Help。开发者只需在 MDN 上提出问题，AI Help 就会开始工作。但由于AI Help 频繁出错，社区批评声不断，一致反对 Mozilla 使用AI Help&nbsp;。</p><p></p><h2>错误频出，社区要求下架</h2><p></p><p>&nbsp;</p><p><a href=\"https://github.com/mdn/yari/issues/9208\">在 MDN 的 Yari 存储库上</a>\"，一位名为 Eevee 的开发人员，“MDN 现在可以自动对寻求技术信息的人撒谎。”</p><p>&nbsp;</p><p>Eevee 访问了浏览器地址栏历史记录中的第一篇 MDN 文章（针对<a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/grid\">网格属性</a>\"），在遇到的第一个代码块（语法摘要）上点击“aiterpret”，并收到以下信息：</p><p>&nbsp;</p><p></p><blockquote>grid: \"a\" 100px \"b\" 1fr;：此值将网格模板设置为两行两列。第一行的高度为 100 像素，第二行的高度为 1 个分数单位 (1fr)。这些列被命名为“a”和“b”。</blockquote><p></p><p>&nbsp;</p><p>Eevee 表示，这是严重但微妙的错误：只创建一列（更多的列需要斜杠），并且引用的字符串是区域的名称，而不是列的名称。但它是可信的，它与其他正确的属性值解释交织在一起。这尤其糟糕，因为grid 具有复杂快捷语法的属性——正是人们可能想要点击“解释”按钮的那种东西。</p><p>&nbsp;</p><p>“生成的文本似乎未经审查、不可靠、不负责任，甚至无法纠正。至少如果文本被嵌入到存储库中，它可能会受到人类监督和pull request，但据我所知，它只是在某个缓存中？似乎这个功能的构思、开发和部署都没有考虑到LLM可能会产生令人信服的胡言乱语。”Eevee 说道。“我希望 MDN 包含正确的信息，但实际上MDN 生成了一个听起来令人信服的谎言，且没有明显的纠错过程。”</p><p>&nbsp;</p><p>开发者 Lifning&nbsp;也表示，这不是偶发的单独事件，直接给出了一张截图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a84f16b4be3c47800585d974b181462b.jpeg\" /></p><p></p><p>&nbsp;还有人表示，AI Help<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615205813\">自相矛盾</a>\"、<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615200919\">错误识别</a>\"&nbsp;<a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/color_value/oklch\">CSS 功能</a>\"、<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615245134\">错误地解释</a>\"辅助功能，并且通常<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615277871\">不理解 CSS</a>\"。</p><p>&nbsp;</p><p>Eevee 帖子下边，几乎充斥着对MDN&nbsp;的批评。开发者“MrLightningBolt”表示，“由于上述原因，这种‘AI’万金油比无用更糟糕；其他示例的创建很简单。MDN 的存在会让情况变得更糟。”&nbsp;</p><p>&nbsp;</p><p>“我非常依赖这些文档。但LLM非常不可靠，所以不要使用它了，请将原来人工MDN作为开发人员文档的可信来源。”Datarocks 说道。</p><p>&nbsp;</p><p>屡次出现的错误严重损害了开发者对 MDN 的信心。Dalton Miner 表示，“我使用 MDN，因为它是一个全面、准确的文档来源，没有任何废话。我看不出容易出现严重错误的 LLM 输出如何改善这一点。它极大地削弱了我对 MDN 的信心，我担心它的包容性将导致过度依赖廉价但不可靠的文本生成。”</p><p>&nbsp;</p><p>开发人员 Andi McClure<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615308637\">痛斥了</a>\"新的 AI Help功能，甚至因此表示不再使用MDN，“除了道德、法律和声誉问题之外，实际上，在我确信在所有‘AI’集成和内容已从MDN 中永久删除之前，我无法相信或者出于任何目的使用 MDN。”</p><p>&nbsp;</p><p>Ruby Juric 对 Mozilla 表达了强烈的不满，“Mozilla，如果您试图削弱社区对您作为 MDN 有效管理者的信任，那么您的工作已经非常出色了。我们要求您做的就是接受社区反馈——我认为在这个高曝光的问题中，几乎完全没有支持此功能的声音，社区足够一致——请丢掉这个没人想要的东西。我们不希望有‘信息可能不正确’的免责声明。我们一开始就不想要错误信息。”</p><p>&nbsp;</p><p>Ruby Juric 表示，“为什么社区反馈被忽视？为什么 Mozilla 认为这个功能如此重要，以至于尽管社区几乎一致反对，但Mozilla仍保留它？”</p><p>&nbsp;</p><p>Mozilla 没有立即回应。然而，MDN 核心维护者 sideshowbarker 似乎已经注意到了这一混乱局面。</p><p>&nbsp;</p><p></p><blockquote>这个变化似乎是两天前在e342081上发布的——没有任何相关的问题，只有<a href=\"https://github.com/mdn/yari/pull/9188\">#9188</a>\"上的一个PR，其中绝对没有任何讨论或任何类型的背景信息。&nbsp;在我看来，这一点上，Mozilla太自作主张了，而且没有给任何其他MDN利益相关者任何提示。(我可能错了，我离开了一段时间——不幸的是，上个月我的很多时间都花在了其他地方，这使我无法做我通常会做的MDN工作。)&nbsp;无论如何，这个“用AI来解释”是一个非常糟糕的想法，原因很明显。在这一点上，我至少可以承诺，我个人将在内部尽可能高紧迫地让内部高度重视这个问题（在发表这篇评论之前，我已经开始这样做了），以便把它尽快删除。</blockquote><p></p><p></p><p>&nbsp;</p><p>后来，MDN表示“AI Help”功能已<a href=\"https://github.com/mdn/yari/commit/1bf285612ca2a2741795916fbe7fd2549e6b0013\">暂时</a>\"停止，团队表示稍后会继续研究问题。</p><p>&nbsp;</p><p>据悉，AI Help 的灵感来自<a href=\"https://supabase.com/blog/chatgpt-supabase-docs\">Supabase Clippy</a>\"，而 Supabase Clippy 又受到<a href=\"https://www.theregister.com/2013/07/12/microsoft_clippy_lives/\">Clippy</a>\"的启发。有趣的是，Clippy 的设计师Kevan Atteberry自己都不好意思将其纳入自己的作品集。</p><p>&nbsp;</p><p></p><h2>Mozilla 的入局策略</h2><p></p><p>&nbsp;</p><p>今年三月，Mozilla 宣布投资 3000 万美元成立了名为 Mozilla.ai 的初创公司，通过社区驱动来构建一个值得信赖、独立和开源的 AI 生态系统。</p><p>&nbsp;</p><p>“我在值得信赖的人工智能领域工作了近五年，一直感到兴奋和焦虑交织，”Mozilla执行总裁兼 Mozilla.ai 负责人Mark Surman对外媒表示，“大型科技公司人工智能的快速发布没有什么不同。真正令人兴奋的新技术正在出现——新工具立即激发了艺术家、创始人等各种各样的人尝试新事物。但当意识到几乎没有人建立防护栏时，你就会感到焦虑。”</p><p>&nbsp;</p><p>过去几个月涌现出来的模型对现实世界的影响令人担忧，比如在发布时，ChatGPT 可能会被<a href=\"https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/\">提示</a>\"编写恶意软件等。很多模型的创建者表示正在采取措施遏制滥用行为，但 Mozilla 认为他们做得还不够。</p><p>&nbsp;</p><p>因此，Surman 强调这家公司的使命不是随便构建什么人工智能，而是构建开源且“值得信赖”的人工智能。</p><p>&nbsp;</p><p>Surman 还将 Mozilla.ai 描述为半研究公司、半社区——一家致力于帮助创建值得信赖、独立的开源 AI 堆栈的初创公司。Bard 支持的谷歌和ChatGPT支持的Bing 保持封闭，即使长期以来一直承诺成为开放生态系统的 OpenAI，也开始更少地分享有关其模型和训练数据的信息。Mozilla.ai 希望改变这一点。</p><p>&nbsp;</p><p>根据报道，最初，Mozilla.ai 的首要任务是建立一支由大约 25 名工程师、科学家和产品经理组成的团队，按照 OpenAI GPT-4的路线开发“值得信赖”的推荐系统和大型语言模型。但该公司更广泛的目标是建立一个由拥有共同愿景的联盟公司和研究团体（包括 Mozilla Ventures 支持的初创公司和学术机构）组成的网络。</p><p>&nbsp;</p><p>Mozilla.ai 目前主要在开发一些工具，例如让用户询问人工智能聊天机器人答案背后的来源。该公司还努力创建能让用户更好地控制内容推荐的系统，例如基于 Mozilla 现有研究针对个人或社区做价值优化的推荐系统。</p><p>&nbsp;</p><p>ChatGPT、Bing等已经开始改变互联网的运作方式。Mozilla 已经跟进这件事有一段时间了，比如2020年发布了被广泛引用的“<a href=\"https://blog.mozilla.org/en/mozilla/mozillas-vision-for-trustworthy-ai/\">创建值得信赖的人工智能</a>\"”白皮书，但大部分时间 Mozilla 都对产品持观望态度。</p><p>&nbsp;</p><p>Mozilla 长期以来一直是市场上规模虽小但有影响力的参与者，尤其是 Firefox。虽然Firefox不再是最流行的网络浏览器，但它的流行足以让 Mozilla 有资格加入这场竞争。</p><p>&nbsp;</p><p>Mozilla 喜欢谈论建立更广泛的社区和生态系统，但很明显该公司也将人工智能视为 Firefox 未来的一部分。Mozilla <a href=\"https://www.theverge.com/2023/2/14/23598344/mozilla-firefox-ceo-mitchell-baker-microsoft-edge-bing-google-apple-ai\">首席执行官Mitchell Baker</a>\"表示，我们正在进入一个真正变革的时刻，这可能有助于 Firefox（或其他一些浏览器）赢回一些市场份额。事实上，Bing确实因为接入ChatGPT而用户数量大增。</p><p>&nbsp;</p><p>入场较晚的Mozilla 能否凭借社区、开放这一属性赢得一席之地还要拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://github.com/mdn/yari/issues/9208\">https://github.com/mdn/yari/issues/9208</a>\"</p><p><a href=\"https://www.theregister.com/2023/07/03/mozilla_developer_network_adds_ai/?td=rt-3a\">https://www.theregister.com/2023/07/03/mozilla_developer_network_adds_ai/?td=rt-3a</a>\"</p><p><a href=\"https://techcrunch.com/2023/03/22/mozilla-launches-a-new-startup-focused-on-trustworthy-ai/\">https://techcrunch.com/2023/03/22/mozilla-launches-a-new-startup-focused-on-trustworthy-ai/</a>\"</p>",
    "publish_time": "2023-07-04 14:05:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从Ruby到Node：重写Shopify CLI，提升开发体验",
    "url": "https://www.infoq.cn/article/pdDcJCrLarGjmKGfcQDL",
    "summary": "<p></p><p>本文最初发布于 Shopify 工程博客。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cd/cd087429bd124727f5cef2be191c535a.png\" /></p><p></p><p>Shopify CLI（命令行界面）是开发人员在 Shopify 平台上构建和部署 Theme、App、Hydrogen 店面时的重要工具。它提供了按照最佳实践创建新项目的工作流，实现了与开发平台的集成，并可以将产品工件分发给商家。我的团队，即 CLI Foundations，负责为设计和构建 Shopify CLI 的最佳实践和核心功能打基础。我们知道，开发人员在开发 Shopify App 时会大量用到终端，而他们使用 CLI 时并不总是能够获一致而愉快的体验。因此，我们开始使用 Node 彻底重写 Shopify CLI 2（那原本是用 Ruby 编写的），并在去年夏天推出了 Shopify Editions。</p><p></p><p>在这篇博文中，我将介绍下我们团队之前为什么做出了重写的决策以及当时所做的权衡，我们在这个新的迭代中所遵循的原则，以及我们后续要克服的挑战和探索的想法。</p><p></p><p></p><h2>语言决策回顾</h2><p></p><p></p><p>在 Shopify CLI 之前，Theme 开发人员用的是我们的另一个 CLI——ThemeKit。我们从 2014 年 10 月就开始维护它。它是用 Ruby 编写的，基于我们在内部 CLI 和服务中使用的一些 Ruby gems 构建，诸如 cli-kit、cli-ui 和 theme-check 等。</p><p></p><p>当我们在 2018 年 12 月开始开发第一个 Shopify CLI 来帮助 App 开发人员时，考虑到我们已有的 Ruby 资源和知识，选择 Ruby 是明智的。用户需要一个全局的 Ruby 安装才能使用 CLI，但我们通过为所有受支持的操作系统（Windows、Linux 和 macOS）提供安装程序解决了这个问题。2020 年 12 月，我们将 ThemeKit 合并到 Shopify CLI 中，迈出了将所有开发集中在一个 CLI 中的第一步。</p><p></p><p>2020 年 6 月，我们添加了 UI 扩展，让开发人员可以使用自己的 UI 扩展平台的某些区域，CLI 开始依赖 Node 工具来转译和打包扩展代码。系统要求越来越高，这不是用户所希望的。不过，生态系统正朝着用编译式语言（如 Go 和 Rust）实现 JavaScript 工具的方向发展，因此我们希望可以摆脱对 Node 的依赖。但这种事情并没有发生。尽管像 ESBuild 这样的工具（我们用于打包扩展）是可移植的二进制文件，但它们的可扩展性依赖于在 Node 运行时上动态求值的插件。</p><p></p><p>此外，Hydrogen 团队已经在 Node 上构建了一些工具，他们开始考虑构建一个新的 CLI，而不是将 Hydrogen 工作流构建到 Shopify Ruby CLI 中，这样他们的用户就不需要在自己的系统中安装 Ruby 运行时。Hydrogen 开发人员希望 npm install 命令能够解析他们在项目中需要的所有依赖项。如果将 Ruby 作为一个依赖项，这种思维模式就会被打破，他们就很容易遇到 CLI 因为需要额外的步骤而拒绝运行的问题。另建一个 CLI 会破坏我们始于将 ThemeKit 合并到 CLI 的统一工作。这可能会导致平台不同区域的 CLI 体验不一致。</p><p></p><p>最后但同样重要的是，Shopify 越来越依赖于 Web 技术和标准，其中 JavaScript 和 Node 运行时在资源、工具和知识方面更有优势。</p><p></p><p>所有这些都促使我们思考 Ruby 是否是最适合 CLI 的语言，所以我们回顾了这个决策。我们需要一种技术：</p><p></p><p>系统要求尽可能少（例如，不需要安装多个运行时）；让我们能够提供一流的开发体验；内部团队很容易做出贡献。</p><p></p><p>最终，我们决定用 TypeScript 重写 CLI，以便在 Node 运行时上运行。</p><p></p><p></p><h2>从 Ruby 迁移到 Node</h2><p></p><p></p><p>在 Shopify 使用的所有编程语言中，Ruby 是大多数开发人员都熟悉的语言，其次是 Node、Go 和 Rust。使用它们都可以构建出一流的开发体验，这要归功于生态系统提供的丰富软件包解决了常见的问题。从这些选项中，Go 和 Rust 都可以轻松地发布运行时不依赖运行时的静态二进制文件。这一点，Node 和 Ruby 也可以通过将源代码和运行时依赖项（又名 vendoring）打包在一起来实现，但设置更复杂，并且可能有一些操作系统不支持。</p><p></p><p>我们选择 Node 有几个原因。Go 和 Rust 允许分发静态二进制文件，但代价是 Shopify 的人由于不熟悉语言很少能做出贡献。这并不理想，因为 Shopify 希望内部团队可以为 CLI 贡献新的想法。我们只能选择 Ruby 或 Node。</p><p></p><p>在构建 CLI 方面，Node 有一个与 Ruby 不同的特性：它的模块系统和它所支持的可扩展性。与 Ruby 不同，Node 的模块系统允许同一个传递包有多个版本，而且不会相互冲突。这就让我们可以构建一个模块化的架构，将平台的不同功能域封装在 NPM 包中，而它们都基于一个包含共享功能的包构建。需要注意的是，虽然 Hydrogen 和 App 开发人员只需要一个运行时（Node），但 Theme 开发人员现在还需要两个：Ruby 和 Node。不过，我们已经开始着手消除 Ruby 依赖，我们的目标是在今年晚些时候完成这项工作。</p><p></p><p></p><h2>构建卓越的终端体验</h2><p></p><p></p><p>我们做出了技术决策，但我们还得做一些最佳实践、代码架构、模式和约定方面的决策。这是对我们从不同团队习得的经验和我们构建 Ruby CLI 的经验的一次综合运用。我将与大家分享我们在构建卓越的终端体验的过程中对我们影响最大的 7 个决定。</p><p></p><p>1. 为实现一致性奠定基础</p><p></p><p>Ruby CLI 的贡献一致性较差，而且是松耦合的（与我们所期望的高度一致的松耦合状态相反），导致内外部分化，进而导致了糟糕的体验。在 Node 版本中，我们必须做一些不同的事情。我们需要一种方法来使贡献保持一致。我们通过：</p><p></p><p>代码模式：建模命令的业务逻辑。在基于框架（如 Rails）的项目中，框架（如 MVC）通常会支持这些模式，但我们没有框架。因此，我们必须开发自己的模式和机制，并保证开发人员遵循它们。UI 模式和组件：我们在 Ink 上设计并构建了一个设计系统，以确保所有命令的体验都和 Shopify 类似。约定：便于开发人员浏览他们的项目和命令。原则：创造卓越的体验，助力开发人员取得成功。</p><p></p><p>上述内容体现在一个共享 NPM 包 @shopify/cli-kit 中，所有功能域（Theme、App 和 Hydrogen）都以它为基础构建，还有一套通用的原则，用于 CLI 和任何与平台开发人员交互的界面（例如，文档和合作伙伴仪表板）。像 ESLint 这样的静态分析工具成为我们构建自动化的平台，用于保证贡献与 CLI 的基础和方向相一致。我们实现了像 command-flags-with-env 这样的自定义规则，以支持通过环境变量设置命令标志。</p><p></p><p>下面的示例展示了一个惯用的 API，特性开发人员可以使用该 API 获取一个有效的会话，与 GraphQL API 进行交互。</p><p></p><p><code lang=\"javascript\">const session = await ensureAuthenticatedAdmin(storeFQDN)\n</code></p><p></p><p>文件 idiomatic_API_example.js，托管在 GitHub 上</p><p></p><p>2. 确保支持跨操作系统</p><p></p><p>在 MacOS 环境中开发时，确保代码更改支持 macOS、Windows 和 Linux 是一个繁琐的过程，会导致测试被跳过并出现回归。Node 运行时会使问题加剧，因为已知有些 API 在不同操作系统上的行为不一致。社区正在用 NPM 包克服这些问题。例如，pathe 规范了跨操作系统的路径。为了防止同样的事情再次发生，我们采取了三个策略：</p><p></p><p>我们在 @shopify/cli-kit 中提供了环境交互（如 IO 操作）模块，并确保它们的 API 跨操作系统兼容。如果我们检测到操作系统不兼容的情况，就会一次性修复它。我们的测试套件混合了单元测试、集成测试和端到端（E2E）测试，这些测试通过持续集成（CI）在所有支持的操作系统上运行。它会在合并前在 PR 中暴露问题。我们为与环境存在契约关系的模块（如提供 Git 交互实用工具的模块）编写集成测试。我们提供了在 MacOS、Linux 和 Windows 环境中测试更改的指令。</p><p></p><p>3. 迁移到 Monorepo</p><p></p><p>Conway 定律在我们的组织中得到了体现，我们的存储库中包含了 CLI 的不同组件（如模板和内部 CLI）。Multirepo 设置不会给用户带来什么价值，却使得内部贡献和自动化测试更加麻烦。我们决定以重写为契机改变这种局面，将所有组件放入同一个存储库 shopify/cli 中。Monorepo 设置允许跨多个包和模板原子地贡献更改。</p><p></p><p>4. 拥抱函数式编程</p><p></p><p>Ruby CLI 命令的业务逻辑是有状态的，有许多假设，并且在命令生命周期中会产生多种副作用。这增加了代码推理、贡献和测试的难度。对于 Node CLI，我们采用了不同的方法。</p><p></p><p>我们在逻辑设计时尽可能地函数化，并且只要可能，就将副作用集中在命令开始的部分。例如，命令所做的第一件事是在内存中加载和验证项目。这类似于 Web API 在接收请求时所做的事情；在将其传递到可能产生级联效应且处于无效状态的系统之前，它将对其进行验证。我们对函数范式的运用并不是教条式的，但我们的目标是把逻辑变成传递状态的函数的组合。</p><p></p><p>我们使用 JavaScript 对象和函数作为组合单元。我们默认创建对象的副本，而不是改变传递的实例。只有少数情况下，为了符合语言要求，我们才诉诸于类，如错误类型。我们引入了一个与函数组织有关的软约定，类似于模型 - 视图 - 控制器（MVC）架构模式：</p><p></p><p>模型（Model）：是用来对状态建模的 TypeScript 接口，例如 App 项目、项目配置和会话的内部表示。命令（View）：是用户进行交互的界面，用户在调用 CLI 时会传递参数和标志。它们的职责仅限于解析和验证参数及标志，并提供帮助菜单的内容。服务（Controller）：是业务逻辑的封装单元。所有命令都有一个包含命令业务逻辑的服务，有些服务没有绑定到特定的命令。</p><p></p><p>除了上面提到的，我们还有提示符，它包含通过标准输入提示用户的函数，以及将一组函数分组到特定域的实用程序。一个例子是与 Shopify GraphQL API 交互的所有函数。</p><p></p><p>5. 实现端到端测试策略</p><p></p><p>采用函数式编程和最小化副作用简化了单元测试的编写。为了定义和运行它们，我们使用了 Vitest，它在我们开始开发 Node CLI 前数周才刚刚发布。我们决定使用 Vitest，因为它完全支持 ES 模块（我们采用的模块系统）。尽管在工具成熟的过程中，最初会有一些问题，但我们对它提供的体验和与 Jest API 一对一的映射感到满意。</p><p></p><p>单元测试给了我们信心，相信我们的函数在不同的场景中完成了它们应该做的事，但这还不够——单元测试套件成功运行的结果并不意味着像“app build”这样的工作流在最近创建的项目中成功运行。因此，我们决定投资一个使用 Cucumber 的端到端测试套件，以确保各种工作流可以端到端工作。Cucumber 为我们提供了描述、运行和调试这些测试的工具和 API。你可能知道的，E2E 以维护麻烦和可能引入古怪行为而闻名。不过，在 CLI 中不会那样，因为这里的设置更简单。执行可以隔离，并将范围限定在测试场景中，防止全局状态泄漏到其他测试中导致它们表现异常。下面是我们的一个测试示例：</p><p></p><p><code lang=\"sql\">Scenario: I create and build an app with extensions\n  Given     I create an app named MyTestApp with pnpm as the package manager\n  And       I create an extension named TestPurchaseExtension of type post_purchase_ui\n  When      I build the app\n  The       The extensions are built\n</code></p><p></p><p>文件 example_test_gherkin.txt，托管在 GitHub 上</p><p></p><p>6. 利用 TypeScript</p><p></p><p>TypeScript 的类型系统和编译器让我们可以相信，代码单元和外部依赖关系之间的契约是匹配的。CLI 依赖的许多 NPM 包和 @shopify/cli-kit 中提供的模块提供了类型定义，极大地改善了对存储库做贡献的体验。例如，我们正在实现一个为 CLI 设计的新设计系统的组件，我们广泛使用 TypeScript 来确保开发人员以正确的方式使用组件。</p><p></p><p>7. 构建经过社区测试的基础</p><p></p><p>在早先一次与 Shopify 之外的 CLI 开发人员的对话中，oclif 作为一个出色的、使用 Node 构建 CLI 的工具和 API 框架出现在我们的视野中。例如，它诞生于 Heroku 的 CLI，用于支持其他 CLI 的开发。在决定使用 Node 之后，我们更彻底地研究了 oclif 的特性集，构建了小型原型，并决定基于它们的 API、约定和生态系统构建 Node CLI。事后看来，这是个好主意。</p><p></p><p>Oclif 为我们提供了用于声明 CLI 接口的惯用 API，并提供了出色的默认值自定义功能。例如，帮助文档是从代码内的声明自动生成的。此外，它还通过插件系统内置提供了可扩展性，我们已经利用插件开发了 App、Theme 和 Hydrogen。它允许我们将项目组织成边界和职责分工明确的模块。我们利用了 oclif 的 hooks API 来防止 @shopify/cli-kit 通过依赖倒置获得依赖插件的信息。插件和 @shopify/cli-kit 实现依赖于接口。</p><p></p><p></p><h2>Node CLI 展望</h2><p></p><p></p><p>Node CLI 极大地改善了开发体验：我们统一并简化了 App 开发，带来了全面的一致性，并新增了扩展功能，如函数。不过，我们还有很长的路要走，还有很多东西要学。</p><p></p><p>我们希望 Theme 开发体验与 App 和 Hydrogen 的开发体验保持一致。目前，Theme 命令仍然在 Ruby 实现中运行，为用户提供 Ruby CLI 体验，开发人员需要在他们的环境中安装 Ruby 运行时，这种情况并不理想。</p><p></p><p>我们还将继续迭代 App 开发体验，为开发人员提供一些实用的命令，用于创建、开发 App 并部署到平台。自从宣布为开发 App 提供更好的开发体验以来，我们已经收到了许多宝贵的反馈，并且正以此为基础进行迭代，如从 Multirepo 设置迁移到统一 App 模型的一些难点。在构建新想法的原型方面，我们的团队也有一个很好的基础。未来，我们会很高兴分享我们的进展。</p><p></p><p>关于可扩展性，还有很多内容可以分享，但这是后续博文的主题。</p><p></p><p></p><blockquote>Pedro Piñera Buendía 是 Shopify 的一名高级开发人员。</blockquote><p></p><p></p><p>原文链接：</p><p></p><p><a href=\"https://shopify.engineering/overhauling-shopify-cli-for-a-better-developer-experience\">https://shopify.engineering/overhauling-shopify-cli-for-a-better-developer-experience</a>\"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171257&amp;idx=1&amp;sn=7f77e37374ec697d23f6f15930c8cb7f&amp;chksm=bdb851ea8acfd8fcc1f25b225b86c3294dbcc1ebe79ef5c541a015180cece750ac2cc07ead2a&amp;scene=21#wechat_redirect\">编程已死，AI 当立？教授公开“唱反调”：AI 还帮不了程序员</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171070&amp;idx=1&amp;sn=e15ef53749fc3f1097b9141dc95087ce&amp;chksm=bdb850ad8acfd9bbb4d406ad7e29ff280229c483401ff07cf2b4e7b57938a1629a22ffde87ab&amp;scene=21#wechat_redirect\">抗拒使用 GPT-4 和 Copilot 写代码，拥有 19 年编程经验的老程序员“面试”被淘汰</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651170964&amp;idx=1&amp;sn=76048b245762bd0d382afe7d35ad6bca&amp;chksm=bdb850c78acfd9d1aadde16a310df7f1dcccfdd8572ce52d81d63c827894f675d077350b96ee&amp;scene=21#wechat_redirect\">马化腾称“收紧队形”，腾讯回应；微软发布自己的Linux发行版；OpenAI回应GPT-4 变笨 | Q资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651170943&amp;idx=1&amp;sn=1375a494b874ee28d39d9a0a38fe7f9e&amp;chksm=bdb8502c8acfd93a3545d91f70121109cdaa1523868ef50bbc8f3b1bddafbba1042777571a8c&amp;scene=21#wechat_redirect\">向量数据库？不要投资！不要投资！不要投资！</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-07-04 14:14:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java极客眼中的WebAssembly",
    "url": "https://www.infoq.cn/article/H9VRjX3X1MeXqImEwI6J",
    "summary": "<p>不少Java开发人员在面对WebAssembly一词时，首先会想到这是一种“浏览器技术”，之后可能会认为“还是归结为JVM”。毕竟浏览器内应用对他们而言是一种“史前生物”。</p><p></p><p>最近数周内，围绕WebAssembly，多项技术呈密集发布，例如<a href=\"https://www.docker.com/blog/docker-wasm-technical-preview/\">Docker+wasm技术预览</a>\"等。作为一名Java极客，我认为不应视<a href=\"https://www.infoq.cn/article/pskeeKXTSbmQa2cauwBh\">WebAssembly</a>\"为一时风尚而置若罔闻。</p><p></p><p>文如其名，WebAssembly（wasm）的确可称为“一种用于Web的字节码”。Java和wasm二者间的相似性也仅限于此。这里“wasm”是小写的，表示它是一个缩略词，而非首字母缩略语。</p><p></p><p>如果有兴趣了解我们如何定义了WebAssembly标准，<a href=\"https://evacchi.github.io/wasm/compilers/history/2022/11/23/a-history-of-webassembly.html\">欢迎翻阅我写过的一篇博文</a>\"，其中解释了来龙去脉。本文阐述的重点是，为什么说WebAssembly并不仅仅局限于Web。</p><p></p><p>首要一点，WebAssembly运行时仅是貌似JVM。其中一点，WebAssembly的长远目标，是成为适合各种编程语言的编译目标。但JVM并非如此，至少最初没有做如此考虑。</p><p></p><h2>第一个神话：JVM是一种多语言编译目标</h2><p></p><p>必须承认，JVM是最为丰富的、可互操作的语言生态系统之一。Java生态还包括了Scala，Jython，JRuby，Clojure，Groovy，Kotlin等编程语言。</p><p></p><p>但现实非常可悲，Java字节码从未真正地成为一种通用的编译目标。<a href=\"https://dl.acm.org/doi/10.1145/1711506.1711508\">不少文献资料</a>\"都对此做了清晰的阐述。例如，John Rose在“<a href=\"https://dl.acm.org/doi/10.1145/1711506.1711508\">字节码与组合选择的结合：JVM中的invokedynamic</a>\"”一文中写道：</p><p></p><blockquote>Java虚拟机（JVM）被广泛采用，可部分归因于class文件格式是可移植的、紧凑的、模块化的和可验证的，并且非常易于使用。然而，class文件在设计上仅针对Java这一种语言，用于表达其它语言编写的程序时，常常出现一些阻碍开发和执行的“痛点”。</blockquote><p></p><p></p><p>这篇文章阐释了invokedynamic操作码引入JVM中的原因和方式。事实上，引入该操作码就是专为支持使用JVM运行时的动态语言。虽然JRuby，Jython，Groovy等一些语言在运行时中添加了该操作码，并不是JVM在设计中考虑了如何支持这些语言，而是因为这些语言已经这样做了。木已成舟，只能去认可它！</p><p></p><p>换句话说，时过境迁，JVM依然未成为这些动态语言合适的编译目标。甚至可以说，以JVM为编译目标并非因为它是最好的，而是考虑到JVM的采纳度和支持情况，人们希望能与JVM互操作。正如JavaScript那样！</p><p></p><h2>GraalVM：一统各方的虚拟机</h2><p></p><p>最近<a href=\"https://www.graalvm.org/\">GraalVM项目</a>\"大行其道。该项目中包括针对例常Java字节码的JIT编译器，以及用于构建高效语言解释器的API，还新添加了原生镜像编译器。</p><p></p><p>成为“<a href=\"https://dl.acm.org/doi/10.1145/2509578.2509581\">一统所有VM的虚拟机</a>\"”，是GraalVM的最初目标之一，也就是说成为一种多语言运行时。</p><p>但Truffle并未定义多语言编译目标，而是通过Truffle API实现一种极高层级表示，进而构建基于AST的高效JIT解释器。对感兴趣的读者，可自行去深入了解“抽象语法树”（AST）。</p><p></p><p></p><blockquote>“致编程大神”：漫游编程语言的奇境，所有一切都变“神奇”。使用Truffle，的确可以为其它“适当”的字节码格式编写JIT解释器。事实上，已有用于LLVM（Sulong）的Truffle解释器。当然，LLVM位码也的确是多平台/多目标编译目标。依此类推，是否可以说GraalVM/Truffle同样支持多平台编译目标？从技术角度看，可以这么说，甚至可以说是“<a href=\"https://www.youtube.com/watch?v=hou0lU8WMgo\">完全正确的</a>\"”。但依然存在不少可商榷之处，对此本文不一一展开讨论。简而言之，LLVM位码只是作为一种编译目标，并未完全考虑作为一种跨平台的运行时语言。例如，针对不同的CPU和操作系统，LLVM可能必须要调用不同的指令。此外，不同于作为多厂商标准的WebAssembly，GraalVM和Truffle目前为止仍然是开源的、社区驱动的、单厂商实现的项目。但<a href=\"https://www.graalvm.org/2022/openjdk-announcement/\">将GrallvVM纳入OpenJDK的工作近期已经启动，并可能进入Java语言规范</a>\"。毕竟，WebAssembly只是一种得到GraalVM/Truffle支持的语言。如果要使用GraalVM，甚至可以考虑wasm！</blockquote><p></p><p></p><h2>第二个神话：WebAssembly只是另一种Stack-based VM（栈机）</h2><p></p><p>WebAssembly定义为一种<a href=\"https://github.com/WebAssembly/design/blob/main/Rationale.md\">结构化栈机</a>\"使用的虚拟指令集架构（ISA）。</p><p></p><p>上述定义中，关键在于“结构化”（structured）一词，它表明WebAssembly与JVM的工作方式大相径庭。结构化栈机在实际运行中，大部分计算使用值栈，控制流却使用块、if和循环等结构化结构表示。WebAssembly语言则更进一步，一些指令可同时使用“简单”和“嵌套”表示。</p><p></p><p>下面给出一个例子。wasm栈机中有如下表达式：</p><p><code lang=\"null\">( x + 2 ) * 3\n</code></p><p></p><p><code lang=\"null\"> int exp(int);\n    Code:\n       0: iload_1\n       1: iconst_2\n       2: iadd\n       3: iconst_3\n       4: imul\n       5: ireturn\n</code></p><p>该表达式可被翻译为下述一系列指令：</p><p><code lang=\"null\">(local.get $x) \n(i32.const 2) \ni32.add \n(i32.const 3) \ni32.mul\n</code></p><p>其中：*&nbsp;local.get在栈中加入本地变量$x；* 然后i32.const将32位整数（i32）常量2推送入栈；*&nbsp;i32.add从栈中弹出两个值，并将$x+2结果推送入栈；* 整数常量3被推送入栈；*&nbsp;i32.mul弹出两个整数值，并将($x+2)*3的i32乘法结果推送入栈。</p><p></p><p>大家应该能注意到，用括号括起来的，是含有一个以上参数的指令。上面给出的“线性化”版本的WebAssembly，在.wasm文件中直接转换为二进制表示。此外还有在语义上等效的另一种“嵌套”表示：</p><p><code lang=\"null\">(i32.mul \n  (i32.add \n    (local.get $x) \n      (i32.const 2)) \n    (i32.const 3))\n</code></p><p>嵌套表示别具特色。操作的嵌套和编写有别于JVM等字节码类型，而是类似于一种“传统”编程语言。这里所说的“传统”，就是指操作读起来类似于LISP家族中的Scheme语言。显而易见，其中使用的括号约定，就是在向Scheme致敬。当然，事出必有因。对JavaScript的神奇起源稍有了解，就一定知道JavaScript最初是在10天内写成的，而且Brendan Eich一开始的任务是去开发另一种Scheme方言。</p><p>至少对我而言，嵌套序列更有趣的细节在于，它能自然地线性化为其它版本。事实上，遵循括号表达式的优先规则，须从最内层的括号开始。例如：</p><p><code lang=\"null\"> (i32.add \n    (local.get $x) \n    (i32.const 2))\n</code></p><p>上面的例子首先获取$x，然后为常量赋值2，进而将二者相加。此后，再去处理外层的表达式：</p><p><code lang=\"null\">(i32.mul \n  (i32.add ...) \n  (i32.const 3))\n</code></p><p>对其中的i32.add求值，需对常量赋值3，并将二者相乘。这与栈机的操作顺序相同。</p><p></p><p>这里提出结构化控制流，同样是考虑了安全性，以及<a href=\"https://github.com/WebAssembly/design/blob/main/Rationale.md\">简单性</a>\"：</p><p></p><blockquote>WebAssembly栈机仅限于结构化控制流和结构化栈的使用。这一方面极大地简化了“一次通过”（one pass）验证，避免了JVM（栈映射推出前）等栈机的固定点（fixpoint）计算；另一方面，也简化了其他工具编译和操作WebAssembly代码。</blockquote><p></p><p>下面看一个例子：</p><p><code lang=\"null\"> void print(boolean x) {\n    if (x) {\n        System.out.println(1);\n    } else {\n        System.out.println(0);\n    }\n}\n</code></p><p>上述代码翻译为如下字节码：</p><p><code lang=\"null\"> void print(boolean);\n Code:\n 0: iload_1\n 1: ifeq 14\n 4: getstatic #7 // java/lang/System.out:Ljava/io/PrintStream;\n 7: iconst_1\n 8: invokevirtual #13 // java/io/PrintStream.println:(I)V\n11: goto 21\n14: getstatic #7 // java/lang/System.out:Ljava/io/PrintStream;\n17: iconst_0\n18: invokevirtual #13 // java/io/PrintStream.println:(I)V\n21: return\n</code></p><p>在如上等价的WebAssembly定义中可看到，非结构化跳转指令ifeq和goto并未出现，而是恰如其分地被语句块if...then...else所替代。</p><p><code lang=\"null\">(module\n ;; 导入浏览器控制台对象，需要将此从JavaScript传递进来。\n (import \"console\" \"log\" (func $log (param i32)))\n\n (func\n   ;; 如运行if代码块，更改为True。\n   (i32.const 0) \n   (call 0))\n\n  (func (param i32)\n   local.get 0 \n   (if\n     (then\n       i32.const 1\n       call $log ;; 应该记录'1'\n     )\n     (else\n       i32.const 0\n       call $log ;; 应该记录'0'\n     )))\n\n (start 1) ;; 自动运行第一个func。\n)\n</code></p><p>原例可在<a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/Reference/Control_flow/if...else\">Mozilla Developer Network</a>\"上查看和运行。</p><p></p><p>当然，上述例子也可线性化，形成如下的非嵌套版本：</p><p><code lang=\"null\">(module\n  (type (;0;) (func (param i32)))\n  (type (;1;) (func))\n  (import \"console\" \"log\" (func (;0;) (type 0)))\n  (func (;1;) (type 1)\n    i32.const 1\n    call 0)\n  (func (;2;) (type 0) (param i32)\n    local.get 0\n    if  ;; label = @1\n      i32.const 1\n      call 0\n    else\n      i32.const 0\n      call 0\n    end)\n  (start 1))\n</code></p><p></p><h2>更多差异：内存管理</h2><p></p><p>另一处WebAssembly虚拟机和JVM大相径庭，在于各自的内存管理，虽然难以评价孰优孰劣。大家应该知道，Java不需要开发人员显式地分配和释放内存，也无需去操心栈和堆的分配。但开发人员通常需要了解一些内存管理知识，在真正需要时能使用一些方法做显式处理，虽然现实中很少有人这么做。</p><p></p><p>事实上该特性并非语言层级上的，而是VM的工作机制。在VM层级，并没有内存处理的原语操作。堆分配原语是以JDK API的方式提供。开发人员无法缺省禁用内存管理，不能说“我不需要GC Heap，我将自己实现内存管理”。</p><p></p><p>当前，WebAssembly做法恰恰相反。大多数语言在以WebAssembly为编译目标时，的确是自行管理内存，这并非巧合。有些语言的确能做GC，但其VM并不提供GC功能，因此自身的例程在GC时必须回滚。</p><p>WebAssembly的做法是，为使用者分配一小片支持分配、释放甚至是随意移动等操作的“线性内存”（linear memory）。虽然在一定程度上要比JVM提供的功能更强大，但在使用中也需谨慎。</p><p></p><p>例如，JVM不需要开发人员显式指定对象的内存布局，结构体打包（structure packing)、字节对齐（word alignment）等内存空间优化工作已交由VM处理。但在WebAssembly中，这些工作需要开发人员处理。</p><p></p><p>这在一方面，使得WebAssembly成为手动管理内存的编程语言的理想编译目标。因为这类语言需要并期望对内存更高程度上的控制。但在另一方面，增加了语言间互操作的难度。</p><p></p><p>当前，结构和对象布局是ABI（Application Binary Interface，应用二进制接口）关注的问题。但<a href=\"https://docs.oracle.com/javase/specs/jls/se7/html/jls-13.html\">ABI对JVM开发人员都已成为昨日黄花，除了一些极为有限和需注意的例外情况</a>\"。</p><p></p><p>值得关注的是，<a href=\"https://github.com/WebAssembly/gc\">最近WebAssembly垃圾回收规范草案已向前推进</a>\"。规范草案中不仅声明了GC，而且有效地描述了结构体，以及与原始语言无关的结构体间互操作方式。尽管该草案尚未准备好，但事情是在不断发展的，多个关注问题正得到解决。</p><p></p><h2>并非局限于Web</h2><p></p><p>看到大家应该注意到，本文至此还从未提起过“Web”。</p><p></p><p>经过上文的铺垫，下面给出本文的重点，就是阐明Java极客应该关注WebAssembly。</p><p></p><p>即使你不关注前端技术，也不应将WebAssembly纯粹视为前端技术。在WebAssembly的设计和规范中，没有任何一处规定其是专门绑定到前端的。事实上，当前的大多数主流的JavaScript运行时，都能够加载和链接WebAssembly二进制文件，甚至在浏览器之外。因此，可在Node.js运行时中运行wasm可执行文件，并且使用薄薄一层JS胶水代码，就能与平台其它部分交互。</p><p></p><p>但目前也存在一些纯WebAssembly运行时，例如<a href=\"https://wasmtime.dev/\">wasmtime</a>\"、<a href=\"https://wasmedge.org/\">wasmEdge</a>\"、<a href=\"https://wasmcloud.com/\">wasmCloud</a>\"、<a href=\"https://wazero.io/\">Wazero</a>\"等。纯运行时完全脱离开JavaScript主机，并且比成熟的JavaScript引擎更轻量级，更易于嵌入到更大型的项目中。</p><p></p><p>事实上，许多项目正开始采纳WebAssembly，将其作为托管扩展和插件的多语言平台。</p><p></p><p><a href=\"https://www.envoyproxy.io/\">Envoy proxy</a>\"正是其中一个著名项目。其代码库以C++为主，虽然支持插件，但存在和浏览器插件一样的问题，即必须做编译、必须做发布、插件可能无法以正确的权限级别运行，甚至在发生严重故障时可能破坏整个过程。现在，开发人员可以通过嵌入Lua或JS解释器，支持用户通过编写脚本方式成功运行。解释器更为安全，因为它与主要业务逻辑隔离，并且仅采用安全方式与主机环境交互。但不足之处是必须为用户选择一种语言。</p><p></p><p>另一种做法是嵌入WebAssembly运行时，让用户自己选择语言，然后编译成wasm。该做法可实现同样的安全保证，用户也更乐意为之。</p><p></p><p>纯WebAssembly运行时不仅用于实现扩展。一些项目正在创建wasm原生API薄层，以提供独立的平台。</p><p>例如，<a href=\"https://docs.fastly.com/products/compute-at-edge\">Fastly</a>\"开发了边端的无服务器计算平台。其中，无服务器功能由用户提供的WebAssembly可执行文件实现。</p><p></p><p>初创公司<a href=\"https://www.fermyon.com/\">Fermyon</a>\"正开发一个丰富的生态，实现仅使用wasm编写Web应用。该生态由各种工具和基于Web的API组成。最新发布的产品是<a href=\"https://www.fermyon.com/cloud\">Fermyon Cloud</a>\"。</p><p></p><p>这些解决方案已为特定用例提供定制的即席API，这确实是WebAssembly的一类使用方式。不止于此，Docker创始人Solomon Hykes在2019年就写道：</p><p></p><p></p><blockquote>如果wasm+WASI在2008年就出现了，那么我们就不需要去创建Docker。这足以说明其重要性。服务器端WebAssembly是计算的未来，但标准化的系统接口是缺失的一环。希望WASI能够胜任这项任务！— Solomon Hykes (@solomonstre)&nbsp;<a href=\"https://twitter.com/solomonstre/status/1111004913222324225?ref_src=twsrc%5Etfw\">March 27, 2019</a>\"</blockquote><p></p><p></p><p>抛开具体场景，人们的第一反应不免是“wasm到底与Docker有什么关系？”。当然也会想，“WASI是什么鬼？”</p><p></p><p>WASI指“WebAssembly System Interface”，可视其为支持wasm运行时与操作系统交互的一组（类POSIX）API集合。WASI是否类似于JDK类库？并不完全如此。WASI是薄薄一层面向功能的API，用于与操作系统交互，详见<a href=\"https://hacks.mozilla.org/2019/03/standardizing-wasi-a-webassembly-system-interface/\">Mozilla公告博客</a>\"。简而言之，WASI补上了缺失的一环。WASI允许定义与操作系统直接交互的后端应用，无需任何额外的层，也无需即席API。目前WASI的工作是推进其被广泛采纳，能在某种程度上成为后端开发的事实标准。</p><p></p><p>WASI API包括文件系统访问、网络乃至线程API等。这些API与运行时的底层功能协同工作，可简化平台的迁移。</p><p></p><h2>移植Java</h2><p></p><p>尽管存在各种挑战，但WebAssembly依然是首个有潜力成为真正的多供应商、多平台、安全和多语言的编程平台。我认为各位Java极客应把握机会参与其中。</p><p></p><p>WebAssembly规范和WASI工作仍在不断地发展变化。点滴汇成江海，这些工作铺就了通往简化任意编程语言的移植之路，且不仅局限于支持手动内存管理的语言。</p><p></p><p>事实上，部分使用垃圾回收的语言已实现移植，尽管它们所采用的方式方法各有千秋。例如，Go采取了编译为wasm（虽然存在部分限制）；Python移植采取了解释器的移植，即将CPython解释器编译为wasm，之后和传统的执行环境一样去执行Python脚本。</p><p></p><p>当然，实现向Java的移植依然面对很多问题，内存管理只是其中之一。我们当然可以为可执行文件中添加GC，这实际上也正是GraalVM原生镜像目前的工作方式。但在我看来，更难之处在于对其它一些CPU功能或系统调用的支持。这些功能目前仍然不稳定，或尚未得到广泛支持，诸如：</p><p>大多数独立wasm运行时依然缺乏对线程的支持，或是仍是实验性的；甚至浏览器支持也是实验性的，是通过WebWorkers模拟的；套接字访问缺少标准化支持：所有支持编写自定义HTTP handler的服务，通常都提供了预先配置的套接字，对低层级访问是受限的；更难模拟的实验性功能是异常处理，因为wasm字节码中缺乏非结构化跳转。实现该功能，应需在wasm VM中提供适当的支持。每种语言对内存布局和对象形状都有自己的约束。因此，各语言更难跨边界共享数据，这阻碍了不同语言间的兼容性，限制了wasm作为多语言平台的适用性（但该问题已列入GC规范本身之中）。</p><p></p><p>简而言之，无论对于浏览器之内还是之外的WebAssembly平台，移植Java依然存在诸多挑战。</p><p></p><h2>WebAssembly对Java的支持</h2><p></p><p>当前，已有一些面向WebAssembly和Java的项目和软件库。下面将列出我在网上发现的一些资源，虽然其中很多只能称为兴趣爱好项目。</p><p></p><h3>浏览器中运行Java</h3><p></p><p>一些项目针对将Java转换为WebAssembly。但其中大多数项目生成的代码是不兼容更精简的wasm运行时的，通常只适用于浏览器中运行。</p><p><a href=\"https://github.com/mirkosertic/Bytecoder\">Bytecoder</a>\"、<a href=\"https://github.com/i-net-software/JWebAssembly\">JWebAssembly</a>\"和<a href=\"https://teavm.org/\">TeaVM</a>\"等转换器项目，都是将Java字节码转换为WebAssembly，但在将Java字节码转换为浏览器友好代码的技术上略有差异。其中，<a href=\"https://teavm.org/\">TeaVM</a>\"项目相对而言更具前景。我们看到在<a href=\"https://github.com/fermyon/teavm-wasi\">Fermyon分支</a>\"中，包括了对WASI Bytecoder的初步支持。<a href=\"https://leaningtech.com/cheerpj/\">CheerpJ</a>\"是一个非常有前途的专有软件项目。CheerpJ意在提供对全部Java特性的支持，甚至包括Swing。还有<a href=\"https://chrome.google.com/webstore/detail/cheerpj-applet-runner/bbmolahhldcbngedljfadjlognfaaein\">Chrome扩展</a>\"使用Web技术运行很赞的applet。</p><p>一些项目也值得关注，它们针对浏览器运行时，部分提供实验性wasm支持：</p><p><a href=\"https://github.com/google/j2cl/tree/master/samples/wasm/src/main/java/com/google/j2cl/samples/wasm\">J2CL（GWT的后续）</a>\"是Java到JavaScript的源到源编译器，即transpiler，最近提供了对wasm的支持。该编译器支持最新的GC规范。<a href=\"https://github.com/jtulach/bck2brwsr\">Bck2Brwsr</a>\"也是针对JavaScript和浏览器的字节码编译器。<a href=\"https://www.fermyon.com/wasm-languages/kotlin\">Kotlin/Native</a>\"也支持通过LLVM编译为wasm。它也继承了Kotlin/Native存在的所有问题，例如并非支持所有的Java库。<a href=\"https://plasma-umass.org/doppio-demo/\">DoppioJVM</a>\"项目值得一提。它采用了完全不同的技术路径，<a href=\"https://www.fermyon.com/wasm-languages/python\">类似Python</a>\"，不是将字节码编译为wasm，而是提供一种用JavaScript编写的浏览器内VM，去解释JVM字节码。但不幸的是，该项目当前不再维护。</p><p></p><h3>在JVM上运行WebAssembly</h3><p></p><p>前面一直讨论的是如何让Java程序运行在wasm运行时上，我们当然也希望能反其道而行之。平心而论，JVM编程语言已颇具规模的，并且当前大多数wasm运行时所提供的编程模型（使用手动内存管理）在JVM上运行也有些别扭。为周全起见，在此仍有必要提及，当然这些项目也值得介绍。</p><p>首选显然是前文提及的<a href=\"https://github.com/oracle/graal/tree/master/wasm\">GraalVM Truffle实现的WebAssembly解释器</a>\"。GraalVM/Truffle平台博采众JIT之长，具有多语言互操作性。<a href=\"https://github.com/cretz/asmble\">asmble</a>\"提供了一组工具，包括将wasm编译为字节码的编译器、wasm解释器等。<a href=\"https://github.com/fishjd/HappyNewMoonWithReport\">Happy New Moon With Report (JVM)</a>\"是WebAssembly的JVM运行时。列举于此，仅是因为我喜欢这个憨憨的命名。原生wasm运行时绑定，例如<a href=\"https://github.com/kawamuray/wasmtime-java\">kawamuray/wasmtime-java</a>\"。最近推出的<a href=\"https://extism.org/\">Extism项目</a>\"跨多种宿主语言，提供原生WebAssembly运行时wasmtime接口统一的API。<a href=\"https://github.com/evacchi/kaitai-webassembly\">Katai WebAssembly</a>\"是一个由我维护的wasm解析器项目，它使用<a href=\"https://kaitai.io/\">Katai Struct</a>\"二进制解析生成器编写，欢迎大家反馈问题请求（PR）。项目设计上并非针对在JVM上运行wasm，而是针对用户操作或查询wasm可执行文件信息的需求。事实上，Kaitai语法支持对所有受支持语言生成二进制解析器，不局限于Java，还包括Python、Ruby、Go和C++等。</p><p></p><h2>结束语</h2><p></p><p>希望本文能激发大家对WebAssembly的兴趣。Java-on-wasm依然是一个新生事物，欢迎大家以开放心态去探索这一全新的世界，并从中收获惊喜。</p><p></p><h2>作者简介</h2><p></p><p><a href=\"https://www.javaadvent.com/author/evacchi\">Edoardo Vacchi</a>\"，博士毕业于米兰大学，研究方向是编程语言设计与实现。在UniCredit银行研发部门工作三年后，加入Red Hat公司，先后参与&nbsp;<a href=\"https://drools.org/\">Drools规则引擎</a>\"、<a href=\"https://jbpm.org/\">jBPM工作流引擎</a>\"和<a href=\"https://kogito.kie.org/\">Kogito云原生业务自动化平台</a>\"项目。关注WebAssembly等新语言技术，并在<a href=\"https://kie.org/\">KIE organization</a>\"和<a href=\"https://evacchi.github.io/\">个人博客</a>\"上撰写文章。</p><p>原文链接：&nbsp;<a href=\"https://www.javaadvent.com/2022/12/webassembly-for-the-java-geek.html\">WEBASSEMBLY FOR THE JAVA GEEK</a>\"</p>",
    "publish_time": "2023-07-04 14:26:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从闭源到开源，明星项目 TDengine 的开源之路",
    "url": "https://www.infoq.cn/article/bZXU0sZzQX8a95Gsz3Ro",
    "summary": "<p></p><h2>背景介绍</h2><p></p><p>&nbsp;</p><p>首先，让我来介绍一下自己。我是涛思数据的联合创始人，同时也是 TDengine 的核心开发者之一。我主导了 TDengine 从 1.0 到 3.0 的所有版本的迭代开发工作。虽然我也承担着管理者的角色，但我更喜欢作为一名开发者。自从 2019 年开源以来，我每年都在 GitHub 上为开源社区贡献了大量的代码。今年的统计数据显示，涛思数据在中国开源社区的贡献排名第 20 名，而我个人的排名是第 7 位。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/2211aa08b61c127cb708fe16b5e9e3ca.png\" /></p><p></p><p></p><p>TDengine 是一个开源的物联网时序数据库，采用云原生技术实现，以其高性能而闻名。任何人都可以免费下载和使用，欢迎访问官网体验。在 GitHub 上，TDengine 获得了 2 万多个 star，与其他开源云原生数据库（如 TiDB 和 Redis）齐名。在国内的数据库排行榜中，TDengine 稳居第一已经有两三年了。这是一个基本的介绍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a361b5a5e024991bd4e7fb45fdd2e33.png\" /></p><p></p><p>&nbsp;</p><p>TDengine 为什么选择开源</p><p>&nbsp;</p><p></p><h2>发展里程碑</h2><p></p><p>让我们一起了解一下TDengine的发展历程。在大约2017年左右，涛思数据成立，那时只有一个原型产品，所以我们并没有选择开源。到了2018年，我们发布了第一个商业化版本，拥有了大约三四个客户，并对产品进行了进一步的打磨。在2019年7月发布的1.6版本之后，社区的反馈非常好。在GitHub的排行榜上，连续十多天都排名第一。2020年8月，我们发布了2.0版本，这两次开源带来了重要的效应和用户增长，我们成功地吸引了经纬、红杉、GGV纪元等大型投资公司的关注。2022年8月，我们发布了3.0版本以及云原生版本。可以说，TDengine的成功与我们选择的开源策略密不可分。涛思数据将与开源用户一起成长。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2c019883027e26e4653ab2597c8def4.png\" /></p><p></p><h2>为什么要开源？</h2><p></p><p></p><p>为什么要选择开源？其实，我们一开始是闭源的。但是从成立之初，我们就想要做开源。开源并不是简单的把产品拿出来放出来，它经过了很多系统的思考。我们认为开源能够扩大产品的影响力，能够很好地树立品牌。在19年我们开源的时候，创始人陶建辉写了一篇文章叫做《49岁的这个程序员》，估计很多人都看过，这篇文章的阅读量达到了20多万。这样，很多开发者就了解到了我们的TDengine并开始使用，知名度成为涛思数据，包括TDengine未来一切发展的一个基础。因此，我们要开源首先就是要想好如何推广产品。</p><p>&nbsp;</p><p>其次，开源之后，可以很容易地构建开发者社区，建立竞争壁垒。我们现在的微信群里大概有20多个500人的群，每天都有人在里面提问。如果确实有问题，我们会把这些问题反馈到GitHub的需求里，或形成内部工单。也会有很多人写一些文章，包括设计原理、好处、坏处、坑在哪里、优势等等。这些东西成为了我们的财富。我们的竞争者很难获得这么多的关注度，无形之中形成了竞争的壁垒。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d788580c0a06128ec28c1a641dc76dca.png\" /></p><p></p><p></p><p>发布开源代码后，我们很容易得到市场反馈。由于有大量用户使用我们的产品，他们会提出改进建议和报告 bug。无论反馈是好是坏，这些用户的声音都能促进我们更好地迭代产品。我们迭代速度很快，曾经每周发布一个版本，后来平均每两周发布一个版本。一些用户抨击我们这么快地迭代，认为我们是基础软件，不需要那么快更新。但实际上，如果我们的软件没有人使用，那就永远不会有 bug。我们需要不断迭代才能让产品更加稳定。虽然使用基础软件时很少遇到问题，而且我们的软件已经非常稳定了，但我们的很多新用户来会带来许多新想法，这些想法会成为我们的创新源泉，促进我们持续迭代。</p><p>&nbsp;</p><p>发布开源代码后可以轻松地打造产品生态。在当前信创趋势下，我们的软件可以融入不同的操作系统，如 ARM、龙芯和鲲鹏等。外部的很多开发人员会为我们贡献代码、连接器和可视化工具等。我们会区分官方发布版本和开发者贡献版本。</p><p>&nbsp;</p><p>另外，开源软件更容易获得用户的信任。现在，许多大型公司提供一整套解决方案，特别是在云原生浪潮下，用户可以购买一个 K8s 集群并获得所需的任何工具。然而，一些传统公司可能不想受到大公司的束缚。相比之下，一些成熟的互联网公司更愿意自行开发产品。在这种情况下，开源软件成为一个非常重要的选择。此外，基础软件要想在国内取得成功并获得盈利非常困难。因此，要想真正取得成功，包括所有投资者在内的所有人都认为基础软件是一个很好的趋势。为什么呢？因为在国内，我们的用户数量和数据量都非常大，我们可以通过开源软件更容易地进入其他市场。因此，开源是走向海外的一个非常有效的途径。</p><p>&nbsp;</p><p></p><h2>为什么要开源最核心的代码</h2><p></p><p>开源的核心代码非常重要，因为如果你不开源这部分最具有竞争力的代码，市场上可能会出现许多替代品。在过去，国内的许多开源软件仅开放了API和外围工具，这并没有给用户带来特别多的价值，也没有让贡献者获得很大的成就感。这就像你买一件衣服，如果你觉得不好看，别人送你一件你也不会穿。同样地，用户也不会使用没有价值的软件，因此很难形成一个支持者群体。</p><p>&nbsp;</p><p>通过开源具有竞争力的代码，我们可以更好地击败竞争对手。作为基础软件的开源项目，例如数据库，如果没有良好的性能或特色功能，竞争对手很容易替换你的位置。相反，如果你已经做到了某个高度，竞争对手也就没有动力推出与你类似的产品了。因此，开源可以使竞争对手越来越少。特别是对于公司来说，开源带来的好处是很明显的，因为我们需要盈利。&nbsp;</p><p></p><h2>开源的误区</h2><p></p><p></p><p>开源其实存在几个误区。首先，仅仅写出好的代码是不够的。很多人将他们的代码开源出来之后，却没有做任何市场推广。虽然这样的代码确实可以被随意下载，但实际上却无法为产品创造真正有价值的贡献。因此，我们需要在代码写得好的基础上，采取各种各样的配套手段，并且最好能够在一个有计划的时间段内进行执行。</p><p>&nbsp;</p><p>其次，我们不能仅仅是写完代码之后就去开源。如下图所示，在我们的项目中，我们的更新速度从2019年到2023年逐渐加快。虽然这反映出我们的代码肯定不是足够好的，但我们仍然选择开源的原因在于，我们已经有了一定的商业客户基础，可以放心地将代码发布出去，吸引更多的客户并完善我们的产品。因为很多软件的市场定位会随着时间的推移发生变化，只有通过不断地发展，才能真正找到市场定位和盈利点。</p><p>&nbsp;</p><p>最后，商业化也不能慢慢来，需要与开源同步推进。当然，并不需要组建一个庞大的商业化团队，因为在国内这样的环境下，商业化必然需要一些定制化开发的工作。随着项目的规模增大，这些长尾工作可能会变得越来越多，需要平衡市场的诱惑和主线版本之间的关系。因此，商业化的进度需要适度地控制，因为商业化既能为我们的开发团队带来信心，也能为投资团队带来信心。这是一个适度的发展过程，在合适的时间选择做好商业化。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/57/5705317306a9a3503f6140492ee63780.png\" /></p><p></p><p></p><h2>开源软件成功的关键指标</h2><p></p><p></p><p>什么是成功的开源软件呢？我认为衡量开源软件成功的关键指标有四个，其中最重要的指标是它的关注者。因为关注者代表了社区的影响力，而GitHub上的star数是最直观的衡量指标。虽然GitHub上有二三十万个开源软件，但能达到一万个star以上的软件只有一两千个。这些软件无一例外地都是业内领先的佼佼者，这是一个很客观的评价指标，而且它无法作假。如果有假冒伪劣的情况被发现，会对公司形象产生致命影响，使公司无法再起步。</p><p>&nbsp;</p><p>除此之外，开源软件的成功还可以从使用者的数量方面衡量。不同软件的使用者衡量标准可能不同。例如，底层软件的数据库，前端组件都无法用统一的方式衡量它们的用户量。部署实例的数量也不好衡量，因为有些软件部署在内网中，无法与外界通信，也无法获取相关信息。一些用户也可能关闭上报信息的配置项。但是，我们可以从趋势上看到软件在哪些版本中得到了更好的使用，哪些特性更加重要。</p><p>&nbsp;</p><p>在开源社区中，贡献者的数量虽然反映了用户对软件的投入和反馈，但并不是衡量软件重要性的唯一标准。外部贡献者的增多可能增加软件风险，因为出现问题时可能难以立即找到他们进行定位和修复。</p><p>&nbsp;</p><p>在选择开源软件时，大企业更注重的是维护者和团队的背景和能力，以及他们的稳定性。核心提交是否来自于团队的内部以及团队对代码的控制能力，是衡量软件质量的重要指标之一。</p><p>&nbsp;</p><p></p><h2>什么样的产品适合开源？</h2><p></p><p></p><p>关于哪些产品适合开源，我认为有三类：一类是用户量较大的产品，另一类是相对标准化的产品。这两类产品一旦打好口碑，就会像很多互联网热门的APP一样一夜爆红，迅速获得用户。但是我们也需要具备技术硬核软件，这是第三类。这类产品是以技术取胜的。而一些以解决业务上痛点或创新为主的软件则不太适合开源，因为一旦公布，容易被仿造。这类软件最好选择闭源。创业者们应该根据自己的情况，做出明智的选择。</p><p>&nbsp;</p><p></p><h3>开源产品的定位</h3><p></p><p></p><p>如果我们要真正开源一个产品，那么它的定位就必须面向全球市场。这可以从一些顶尖的开源产品的目标市场中看出来，这些软件的共同特点是它们的目标市场都是全球，因为作为一款基础软件，如果不走向全球市场，它的想象空间将非常有限。此外，如果你的核心团队没有足够的激励，那么这款产品很可能也无法生存下来。在目前的ToB形势下，只有全球前三名的软件才能生存下来。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f9/f90b74b00a1ff471fd6ee5fdcf142d4a.png\" /></p><p></p><p>&nbsp;</p><p>比如芯片，目前市场份额大概有80～90%由英特尔和NVIDIA占据，而ARM则占据一小部分份额。国内很多芯片厂商靠政策或资本支持才勉强生存，希望有一天能够进入前三。所以我们的目标必须是远大的。如果这款产品的定位不是全球性的，如果不能在技术上处于全球领先地位，那么它的意义就不大。我相信，在任何一个细分市场中，都可以诞生出独角兽。现在，估值超过10亿美元的公司就可以叫独角兽了，它不一定是一个全面的软件平台。&nbsp;</p><p></p><h3>我适合做开源的创业吗？</h3><p></p><p></p><p>在考虑将产品开源化并创业时，需要先确定产品的方向。目前，软件行业有点像之前的汽车行业，出现了越来越多的零部件制造商，每个公司专注于自己的一小块工作，由集成商将它们最终组装完成。类似地，开源领域，特别是大数据领域，也逐渐实现了零部件化。许多公司专注于自己擅长的领域，将与业务相关的部分交给第三方集成商处理。例如，国内的东软、神州和中科软等公司会处理这些客户的需求，而其他技术型公司则会专注于自己产品的研发。</p><p>&nbsp;</p><p>如果要创业，团队小而精干，但至少要有一个人擅长技术推广。这个人应该有意愿去推广产品，并成为优秀的市场推广人才。如果团队中没有这样的人才，就需要重新考虑如何组建团队。因为软件开发不仅仅是技术，技术只是一个必要的条件。</p><p>&nbsp;</p><p>我们需要逐渐将传统的软件思维转化为互联网思维，更进一步，转化为创造共赢的思维。我们只需要专注于自己的组件，与其他相关公司共同构建整个生态系统。这意味着我们需要与上下游企业建立良好的合作关系，以实现共同的成功。</p><p>&nbsp;</p><p></p><h2>产品开源后就能带来巨大的成功吗？</h2><p></p><p></p><h3>开源成功的关键</h3><p></p><p></p><p>产品开源并不一定能够获得成功。像我之前说的，将软件放在互联网上并不是结束，这只是一个简单的开始。我发现很多同行都有开源的产品，但是它们宣布开源后，文章可能阅读量不超过1000个甚至更少，没有什么反响。实际上，对于这些产品，开源与否并没有什么意义，还不如闭源做好自己的客户服务。因此，开源的软件需要有一个合理的规划。开源的软件必须达到一定的质量标准，这也是为什么我们的产品在2年之后才选择开源的原因，因为基础软件成长周期相对较长。而一些软件的成长周期可能较短，我们可以适当缩短这个过程。但是无论如何，在进行开源宣传时，产品一定要有一个足够打动人心的亮点。也就是说，你要能回答使用你的软件能够带来什么样的价值。除了有一定的质量保证，软件还需要有稳定的维护和团队支持，需要组建公司或小型工作室来维护稳定的团队，并做好市场推广。</p><p></p><h2>开源的商业模式</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/8706970b7fce8deb064f6dde17c31ce4.png\" /></p><p></p><p></p><p>&nbsp;</p><p>开源并不意味着免费，也不表示缺乏商业模式。我们公司一直在探索与开源相关的商业模式。目前业内很多领先的软件公司也都在摸索如何孵化。从一开始，我们就考虑了一些策略，因为我们知道靠着热情可以坚持1～2年，但工作性质总是会发生变化，因此确定商业模式非常重要。作为TDengine，我们有三个版本的商业模式。</p><p>&nbsp;</p><p>第一个版本是社区版，这是免费下载的版本，其主要目的是建立品牌，建立开发者社区，吸引越来越多的用户，提高用户口碑。一些用户可能一开始使用的是开源产品，但随着公司的不断扩大，可能会做出一些新的商业决策。例如，我们有一个典型的案例，有一个工程师非常了解我们，后来他去到理想汽车公司，觉得可以用上我们的产品，就为获得了一个非常好的客户，社区版发挥了很重要的推广作用。</p><p>&nbsp;</p><p>在企业版中，我们提供了两种收费模式：独立部署和订阅。独立部署是一种一次性买断的收费模式，而订阅则是一种长期的收费模式。这两种模式各有优劣。对于国有企业来说，他们可能更喜欢独立部署这种买断式的收费模式。从产品发展和估值的角度来看，我们更喜欢订阅模式。我们销售的是什么呢？由于我们的核心代码已经开源，剩下的是一些辅助功能。如果不需要像审计、加密等高级功能，或者你的公司有足够的技术能力来维护这些软件，那么可以选择不购买这些技术服务。然而，对于许多中小型公司来说，他们虽然拥有技术能力，但也不想将其投入到软件维护上，而是希望将其投入到业务发展上，这个时候就可以考虑购买企业版。</p><p>&nbsp;</p><p>我们也有云服务的版本。尽管公有云的需求正在下降，但私有云的需求仍然很高。我们可以在客户公司的私有环境中部署云环境，以满足他们的需求。</p><p>&nbsp;</p><p></p><h3>开源带来的销售变革</h3><p></p><p></p><p>开源有哪些好处呢？首先，开源改变了传统的 ToB 销售模式，我们不再只是去找企业，而是面对活生生的个人客户，这些客户通常都是能够对软件购买产生决策的架构师等人群。这也是我们在软件大会上宣传产品的原因。传统的登门拜访转变为线上销售，PoC 流程也大为减少，因为真正联系我们的客户已经了解软件的使用模式和优缺点。他们只需要检查公司的最佳实践和业务场景是否匹配，以及技术和维护能力是否足够即可。这样，我们需要投入 PoC 的人员也会非常少，售前团队也会变得更小。长远来看，这对公司的成本有很大帮助，我们将从资源型销售转变为技术和产品型销售。我们现在招聘的销售人员很多都是毕业生，只要他们了解行业的基础知识就可以胜任，而传统的到各个圈子走访的销售可能会把资源带走，难以管理。通过这些措施，我们可以把 ToB 的生意变成 ToC 的生意，变成开发人员喜欢的简单生意。所以像能力强的架构师、开发人员都喜欢去大厂，因为工作比较简单。如果我们也能做到这一点，就可以吸引更多的人才加入基础软件行业。</p><p>&nbsp;</p><p></p><h3>开源带来的生态变革</h3><p></p><p></p><p>开源使得技术和产品的竞争变得更加激烈，而不是仅仅依靠销售资源。这将会促进更多以产品为导向的公司的出现，当然平台型公司也会有很大的成长空间。我们相信在各个细分领域，都会出现一些全球性的企业。即使我们只是做螺丝钉，也可以成为全球前三名之一，我们也可以很好地生存下去。我们还可以帮助许多大众型企业降低成本，使他们能够将精力集中在核心业务上。随着这些企业逐渐成长，我们可以获得更多长尾价值。</p><p>&nbsp;</p><p></p><h3>开源需要组织支撑</h3><p></p><p>我认为开源项目需要一个组织来提供支持。即使你将软件捐赠给 Apache 基金会等组织，也需要有商业孵化公司提供支持。虽然有开发能力和贡献精神的个人可以自己维护和开发软件，但企业使用开源软件需要强有力的支持，以确保软件的稳定性。特别是在发布大版本时，例如从2.0升级到3.0或从3.0升级到4.0，需要封闭性开发、大量测试、DevOps、CI/CD、手工测试等。个人贡献者不愿意从事这些工作，他们只想编写核心代码。如果这些任务被忽略，软件将无法发布。如果开源软件没有组织支持，它的发展速度将逐渐下降，需要寻找新的方式来维持软件的生命周期，这是一个更长远的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/684cc3f70367cb7c6ca427001ae48c2c.png\" /></p><p></p><p></p><p>在某次会议上，我听到有人说开源只是一种好玩的东西，但我对他提出质疑，因为开源并不是一个简单的好玩的事情。不同类型的软件公司必须将开源作为他们的事业，并且对待它的态度不能仅仅是好玩。不能因为某个人觉得不好玩了，就停止参与开源工作。尤其对于ToB领域的软件，这个思路必须清晰。</p><p></p><h2>开源产品运营的正确姿势</h2><p></p><p></p><h3>开发者运营是一个项目</h3><p></p><p>&nbsp;</p><p>开源软件需要设立开发者运营部门来协同各类资源，促进开发者生态的搭建和完善，尤其在企业发展到一定规模时。虽然研发人员的开发能力很强，但很多人不愿意写文章或者做演讲报告，而市场推广和商务部门的技术能力相对较弱，无法胜任这些任务。因此，我们需要协调资源，为软件推广提供更好的支持。为此，越来越多的公司开始设立开发者运营部门和布道师岗位，把它视为一个新项目来管理。该项目的主要指标是用户规模的增长，其中用户体验的提升是其中一个关键点。用户体验不仅包括产品本身的使用体验，还包括是否能够在各种搜索引擎中找到所需信息，是否有相关案例、最佳实践和文档，以及在出现问题时能否联系到合适的人进行沟通等。开发者运营的工作很大程度上涉及到这些方面，同时也要参与各种有技术影响力的活动，如写文章和公开演讲，推进技术影响力的提升。通过这些活动和案例的积累，我们可以更容易地为商业赋能。知名度对于线下生意也是一个非常好的支撑。现在我们的开源软件 TDengine已经走上正轨，客户选择我们也不会被质疑。通过这些方法，我们可以让客户成功，持续了解他们的新需求并加入到我们的产品路线图中。</p><p>&nbsp;</p><p></p><h3>开发者运营的日常工作</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/097679348738c1b02abfa25edf581256.png\" /></p><p></p><p></p><p>作为一个项目，开发者运营需要有很多人员，每天也要处理很多日常工作。第一是用户运营。我们将用户分为几类：顶层灯塔用户、普通用户和企业用户等。对于这些用户，我们提供的支持可能也不同。此外，我们还会制定一定的激励体系，并定期与贡献者进行沟通，授予荣誉证书或分配一些开发任务等。</p><p>另外，内容运营也很关键。我们需要编写大量的基础文章、用户案例并指导用户自己撰写。此外，我们还需要发布培训视频和短视频等，这些都是提升影响力的重要部分。</p><p>&nbsp;</p><p>活动运营也是项目的一部分，包括线上和线下活动。我们在本次大会上也有展台，展示我们的产品，并回答大家的问题。另外，开源项目肯定需要在某个平台上运行，例如在GitHub上，我们需要不断回答用户问题，维护平台并发布新版本，同时还要进行产品培训等。</p><p>&nbsp;</p><p>此外，还需要进行数据方面的运营。这些数据主要是供我们自己内部使用的，因为我们需要衡量做的这么多事情中，哪些是有用的，哪些是没用的。这样可以让公司高层了解下一步该如何做，并成为评价指标。如果没有人给你一个明确的评价，那么即使你取得了成就，也会感到不满足。我们正在建立评价体系，并希望通过数据运营的方式，为生态者部门本身提供很好的评价指标。</p><p>&nbsp;</p><p></p><h3>为什么要进行开发者运营</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e502dddefc9f8440cce40da5c742b522.png\" /></p><p></p><p>为什么我们需要进行开发者运营呢？因为开发者就是产品的用户。实际上，直接使用我们的产品的人都是我们的开发者，他们的技能和水平也在不断提高，他们也不会一直待在某一家公司。因此，我们不应该忽视任何一个开发者，无论他们所在的公司是否与我们的业务有交集。开发者是我们技术的伙伴，活跃的开发者对于开源项目的长期繁荣发展至关重要。他们可能也会成为我们生态的伙伴，我们可以与他们共同寻求平台化或生态化运作，实现共同的价值。事实上，有一项调查表明，57%以上的开发者认为他们对基础软件的购买有一定的影响力。</p><p></p><h3>如何让开发者成功</h3><p></p><p></p><p>我们应该让开发者成功，让他们开发的软件被市场所接受和喜爱，这样才能让客户成功，最终也才能实现我们自己的成功。为了让开发者成功，我们需要做以下几件事情。</p><p>&nbsp;</p><p>让开发者用起来放心：不用担心产品中的各种坑和后门，不用担心被厂商绑架。让开发者用起来安心：不用担心出问题没人管。让开发者用起来顺心：不用花费过多的力气去学习、去开发。让开发者用起来开心：让开发者开发的应用大获成功。&nbsp;</p><p></p><h3>开源在中国的机会</h3><p></p><p></p><p>在中国开源软件的机会还是很大的。中国的企业可以利用这个机会去颠覆传统软件，就像几年前互联网公司进军企业软件一样。然而，简单地将互联网思维套用于传统软件并不会产生深刻的价值，也不容易获得成功，因为传统软件的业务非常复杂。如果我们能够在开源软件中为其创造一些特别的亮点和特色，如将服务器数量从 100 台降至 10 台，或将响应时间从 10 分钟缩短至秒级，就可以为其赋能，使其更好地进行二次迭代和新项目的申请。因此，在中国开源基础软件仍然有许多机会。</p><p>&nbsp;</p><p>中国有许多工程师，价格便宜、数量充足，因此我们可以用较低的人民币薪资生产类似的软件。中国是制造大国和消费大国，有所有的应用场景，而且数据量巨大，我们可以在国内做好一些试验，之后再将其开源。一旦我们开源软件，它将变得非常可靠，这也是我们向全球市场进军的最好路径。</p><p>&nbsp;</p><p>最近参加的一次会议讨论了如何拓展海外市场并制定相应的计划。许多方法都被提出，但开源无疑是其中非常重要的一环。开源并不意味着免费，一旦确定好收费模式，就可以更好地推广产品。</p><p>&nbsp;</p><p>“有所为，有所不为”是指要不忘初心，即在创建公司时要明确业务场景和所期望的效果，始终记得这一点。虽然可以进行修改，但不应因获得大型客户合同而进行修改。例如，今天有一个3千万的项目，你应该接还是不接呢？如果接了，企业中可能有一半以上的重要员工将去做这项开发工作，甚至可能无法预期项目何时完成。一旦开始这样的项目，会有越来越多的类似需求出现，销售人员也会为自己的KPI而推销更多需求。这家原本是基础软件公司的企业，可能会变成一个集成商。如果一直继续这样，最终会发现，两三年后，原本擅长的领域已有更多竞争者，软件也无法更好地发展。因此，如果我们能够坚持五年或八年来发展自己的产品，仍然有可能在该领域取得垄断地位。</p><p></p><h3>参与开源社区的好处</h3><p></p><p></p><p>参加开源社区肯定有好处。我们可以了解到业界广泛应用的各项技术，并学习到高质量设计的思路。即使你认为他们写的代码很平凡，但是他们的流行性基本代表了业界的高水平。此外，公开的程序和文档强制你严格要求自己。在公司内部写一个文档可能只需要随便写两句，但一旦要公布出去，就需要仔细斟酌。这也包括在公司内部做报告时，你可能不会花费时间去准备完善的PPT，但在开源社区，你需要不断磨练自己，提高自己的技能水平。参加越来越多的顶级开源软件项目也是你技能的一个很好的证明。如果别人能够approve你的commit，就说明你的水平达到了这个软件的基本水平，这也是你简历中最好的证明之一。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/39/39b3299f431a747d7466d37493a864fd.png\" /></p><p></p><p>&nbsp;</p><p>欢迎大家加我的微信，我们可以线下聊聊。此外，如果你对TDengine感兴趣，可以在GitHub上<a href=\"https://github.com/taosdata/TDengine\">下载代码</a>\"。TDengine是一款开源、高性能、云原生的时序数据库，专为物联网(IoT)、连接汽车、工业物联网和DevOps而优化。谢谢大家！</p><p>&nbsp;</p>",
    "publish_time": "2023-07-04 14:30:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "跨平台三端重构正式统一，QQ Windows全新体验版上架官网",
    "url": "https://www.infoq.cn/article/lVbbQRzXymTFtqjN9FYT",
    "summary": "<p>7月3日，全新体验版Windows QQ正式上线官网，面向用户开放官方下载渠道。继QQ对 macOS 、Linux版本进行升级后，本次Windows版本的更新，标志QQ基于NT技术架构，实现了桌面端QQ的三端体验统一。同时，新版本QQ Windows新增&nbsp;64&nbsp;位版本支持，并针对大众关注的内存占用问题进行了深度拆解和优化。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/95/e9/955d3372e90f5bec6745d3862c199ae9.png\" /></p><p></p><p>&nbsp;</p><p>（QQ官网界面）</p><p>&nbsp;</p><p>全新交互界面，升级功能及安全能力</p><p></p><p>正式上线的Windows QQ 基于NT架构打造了全新的交互界面，UI设计三端保持一致；消息界面采用了三栏式设计，整体风格更加清爽简约，方便用户查看所需信息；资料设置及资料卡也变得更简洁干净。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1c/b3/1cca43ed70ec74706490c6aea4aa64b3.png\" /></p><p></p><p>&nbsp;</p><p>（登录界面更新、消息界面三栏式设计）</p><p>&nbsp;</p><p>新版本支持消息、QQ空间板块等功能，提升用户在工作、社交、生活等多个场景中的使用体验。语音、视频等通讯功能可一键开启，屏幕共享能力也同步支持，为用户带来更高的沟通效率。</p><p>&nbsp;</p><p>官方发布公测版本之后，QQ团队建立了多个用户体验群，持续关注用户反馈和体验，小步快跑迭代优化了包括群应用、全局搜索、表情分类收藏等能力。</p><p>&nbsp;</p><p>同时，QQ安全团队协同腾讯多个安全实验室，在新版本QQ上全面加强各项核心安全能力，比如外挂检测、hook检测、反动态库注入、反木马盗号、风控预警等多项核心安全能力，进一步保障用户数据资产安全。</p><p>&nbsp;</p><p>跨平台复用方案，三端统一体验</p><p></p><p>从传统互联网时期扎根本土发展，桌面端QQ已经诞生超过24年。而旧版的桌面QQ采用纯Native技术栈开发，多年迭代以来，新旧的功能逻辑十分复杂，代码量非常庞大。出于对开发效率的考虑，新版QQ在技术选型及重构决策上，选用了一套标准化的框架来进行全面的架构升级，即QQ NT架构。跨平台的复用方案能够确保Windows/Mac/Linux三个桌面端快速、高质量的迭代，实现一套代码多端运行，多平台体验统一，满足各桌面用户的需求。近期，手机 QQ 首个基于 NT 架构的正式版也已在发布中。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/99/af/99d0b8b8ef655fe7e30e3575493f21af.png\" /></p><p></p><p>&nbsp;</p><p>（QQNT架构）</p><p>&nbsp;</p><p>持续优化内存使用，实现性能提升</p><p></p><p>自去年Mac端和Linux端的版本上线，QQ桌面端的更新备受用户的关注。针对三端用户广为关注的内存占用问题，QQ技术团队根据其占用的几大主要进程，重点设定优化目标，通过工具分析、定向优化、线上监控及自动化测试等，尽可能减少缓存占用及内存泄露，实现资源使用效率的最大化。目前，QQ技术团队已通过多个阶段目标的设定，从单个进程内存优化到整体内存控制，新版本已取得有效的优化成果。</p><p>&nbsp;</p><p>此次新版Windows QQ上线，专注于整体功能的高质量迭代，在用户反馈的基础上带来更成熟的核心体验，实现了Windows/Mac/Linux三端打通，一套代码多端运行，统一跨平台体验。目前，用户可通过QQ官网下载QQ Windows版本，支持x64及x86。</p><p>&nbsp;</p>",
    "publish_time": "2023-07-04 14:39:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]