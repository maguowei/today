[
  {
    "title": "Terraform引入新的CI/CD工具，增加对Azure Linux的支持",
    "url": "https://www.infoq.cn/article/H7CJpGyk0BRjUTAqIXbM",
    "summary": "<p>HashiCorp发布了一系列针对Terraform和Terraform Cloud的改进。Terraform Cloud提供了一个新的<a href=\"https://www.hashicorp.com/blog/hashicorp-releases-new-ci-cd-pipeline-integration-tool-templates-for-terraform\">CI/CD管道集成工具</a>\"。Terraform为Azure Kubernetes Service增加了对<a href=\"https://www.hashicorp.com/blog/terraform-adds-support-azure-linux-container-host-azure-kubernetes-service\">Azure Linux容器主机</a>\"的支持。HashiCorp Terraform <a href=\"https://www.hashicorp.com/blog/terraform-aws-provider-5-0-adds-updates-to-default-tags\">AWS Provider 5.0</a>\"发布，改进了对默认标记的支持。</p><p>&nbsp;</p><p>新的CI/CD管道工具有一个相关的命令行工具，叫作tfci。这个工具通过API调用自动运行Terraform Cloud，并支持可以嵌入到CI工具中的Terraform Cloud操作。tfci提供的<a href=\"https://github.com/hashicorp/tfc-workflows-tooling/blob/main/docs/USAGE.md\">命令</a>\"包括：通过Terraform Cloud Run ID显示运行详情、执行新的计划运行、在计划确认后继续执行暂停的任务，以及返回计划详情。</p><p>&nbsp;</p><p>除了tfci，还有为<a href=\"https://github.com/hashicorp/tfc-workflows-github/blob/main/actions/plan-output/action.yml\">GitHub Actions</a>\"和<a href=\"https://github.com/hashicorp/tfc-workflows-gitlab\">GitLab CI</a>\"提供的<a href=\"https://github.com/hashicorp/tfc-workflows-tooling\">模板</a>\"。这些模板包含用户在使用tfci时可能需要配置的常见操作。例如，下面的代码片段是在GitHub Actions中使用tfci执行计划的部分内容：</p><p><code lang=\"null\">runs:\n  using: docker\n  image: 'docker://hashicorp/tfci:v1.0.1'\n  args:\n  - tfci\n  ## global flags\n  - -hostname=${{ inputs.hostname }}\n  - -token=${{ inputs.token }}\n  - -organization=${{ inputs.organization }}\n  ## command\n  - run\n  - create\n  - -workspace=${{ inputs.workspace }}\n  - -configuration_version=${{ inputs.configuration_version }}\n  - -message=${{ inputs.message }}\n  - -plan-only=${{ inputs.plan_only }}</code></p><p>&nbsp;</p><p>HashiCorp还增加了在Azure Kubernetes Service上部署Azure Linux容器主机的支持。微软最近提供了<a href=\"https://learn.microsoft.com/en-us/azure/aks/use-mariner\">Azure Linux容器主机</a>\"（之前的Mariner OS）的<a href=\"https://www.infoq.com/news/2023/06/azure-linux-cbl-mariner/\">一般可用性</a>\"。Azure Linux被设计成一个最小化的、云优先的Linux发行版。</p><p>&nbsp;</p><p>这些更新包含在<a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster\">azurerm Terraform Provider</a>\"中。要在AKS上配置Azure Linux容器主机，可以将os_sku设置为Mariner：</p><p><code lang=\"null\">resource \"azurerm_kubernetes_cluster\" \"default\" {\n  name                = \"aks-${random_string.suffix.result}\"\n  location            = azurerm_resource_group.default.location\n  resource_group_name = azurerm_resource_group.default.name\n  \n  kubernetes_version  = var.kubernetes_version\n  dns_prefix          = \"k8s-${random_string.suffix.result}\"\n \ndefault_node_pool {\n    name            = \"default\"\n    node_count      = var.aks_node_count\n    vm_size         = var.aks_confidential_computing_enabled ? \"Standard_DC2s_v2\" : \"Standard_D2_v2\"\n    os_sku          = \"Mariner\"\n    os_disk_size_gb = 50\n  }\n \n  confidential_computing {\n    sgx_quote_helper_enabled = true\n  }\n \n  identity {\n    type = \"SystemAssigned\"\n  }\n \n  tags = {\n    name = \"demo-aks-${random_string.suffix.result}\"\n    environment = \"demo\"\n  }\n}</code></p><p>&nbsp;</p><p><a href=\"https://github.com/hashicorp/terraform-provider-aws/releases/tag/v5.0.0\">HashiCorp Terraform AWS Provider 5.0</a>\"改进了对默认标签的支持，允许在Provider级别设置标签。这个更新解决了之前默认标签实现的许多痛点，包括处理不一致的最终计划、默认标签和资源标签之间的相同标签，以及标签配置中的永久差异。</p><p>&nbsp;</p><p>可以使用default_tags在Provider级别指定默认标签：</p><p><code lang=\"null\">provider \"aws\" {\n  default_tags {\n    tags = {\n      environment = \"Dev\"\n      department  = \"WebEng\"\n      application = \"HashiCafe website\"\n      cost_center = \"8675309\"\n    }\n  }\n}\n \nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"example-bucket-aj-11122\"\n  tags = {\n    environment = \"Production\"\n    created_at  = timestamp()\n  }\n}</code></p><p>&nbsp;</p><p>该版本还调整了已弃用或已删除的属性的报告方式，之前用户会收到警告通知，现在会向用户显示“不受支持错误”。EC2的典型功能也被完全移除，因为这些功能早在2022年8月就被AWS弃用了。</p><p>&nbsp;</p><p>CI/CD管道集成工具和模板对Terraform Cloud和Terraform Enterprise用户可用。更多细节可以在<a href=\"https://www.hashicorp.com/blog/hashicorp-releases-new-ci-cd-pipeline-integration-tool-templates-for-terraform\">发布博客</a>\"和<a href=\"https://github.com/hashicorp/tfc-workflows-tooling\">GitHub代码库</a>\"中找到。Terraform AWS Provider 5.0提供了一个<a href=\"https://www.hashicorp.com/blog/terraform-aws-provider-5-0-adds-updates-to-default-tags\">升级指南</a>\"，其中包含了有关该版本变更的更多详细信息。</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/hashicorp-azure-linux/\">https://www.infoq.com/news/2023/06/hashicorp-azure-linux/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/LFa4ESMJOMY66HtTjYsG\">中国企业研发高效能白皮书-CI/CD篇</a>\"</p><p><a href=\"https://www.infoq.cn/article/lPSPNxwMD2US2WEHamr4\">可观测的崭新进化：加速CI/CD管道的秘密武器</a>\"</p><p><a href=\"https://www.infoq.cn/article/V5NBFgrCIVMpXVazuGQE\">干货 | 携程 Web CI/CD 实践</a>\"</p>",
    "publish_time": "2023-07-04 09:59:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "厂商征集 | 中国人工智能成熟度模型报告 2023",
    "url": "https://www.infoq.cn/article/Rxyrty9TAXacP99lCtWF",
    "summary": "<p></p><h1>研究背景</h1><p></p><p>去年，AI绘画等生成式AI的内容激发了AI领域的广泛关注，年底发布的ChatGPT更是迅速破圈，话题度和讨论度不断提升。这些变化的趋势也延续到了2023年上半年，人们讨论话题也向着大模型的实际应用逐渐深入，同时AI安全也展现了大家对技术突破的不同态度。</p><p>今年年初，InfoQ研究中心发布了《中国软件技术发展洞察和趋势预测报告&nbsp;2023》，在报告中将各个领域的关键技术按照不同成熟度阶段进行了划分，总结成为中国技术成熟度评估曲线。报告发布之后也获得了众多企业和开发者的关注和讨论。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4d85ee1cd05f4b611bde1b873183ecc.png\" /></p><p></p><p>因此，InfoQ研究中心将基于人工智能领域的种种变化，更新人工智能领域的技术成熟度模型，为技术的应用决策和未来投资参考提供研究分析工具。</p><p></p><h1>研究内容</h1><p></p><p>基于技术发展时间、技术专利规模、技术应用市场规模、技术舆论热度等基础评价指标，构建中国人工智能技术成熟度模型。</p><p>报告包含“行业发展洞察”、“人工智能技术成熟度模型解读”、“人工智能技术企业生态图谱”等多个行业研究内容，以及在模型解读中还会有案例分析等厂商研究内容。</p><p></p><h1>征集范围</h1><p></p><p>企业主要业务类型包括：</p><p>提供人工智能产品与服务的各类厂商（RPA、计算机视觉、自然语言学习、数据挖掘、脑机接口、生成式AI等）；其他综合科技厂商。</p><p></p><h1>参与价值</h1><p></p><p>参与本次征集的厂商，将有机会获得：</p><p>InfoQ研究中心《中国人工智能成熟度模型报告&nbsp;2023》人工智能技术企业生态图谱、典型案例两大内容露出，提升厂商品牌知名度和行业影响力。报告通过InfoQ官网、公众号InfoQ等多个官方平台进行发布，同时会在多家生态链接的渠道传播。将有机会受邀参加InfoQ的线上线下活动，与行业内的甲方，行业专家，投资机构等进行深入的交流。</p><p></p><p>本次厂商征集自即日起，截至07月25日，欢迎识别下方二维码，添加小助手后，填写表单参与报告征集。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8a0f5451e7cea9ef0c7f9c70c0da9421.png\" /></p><p></p>",
    "publish_time": "2023-07-04 10:06:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "厂商征集 | 中国云原生成熟度模型报告 2023",
    "url": "https://www.infoq.cn/article/Hlwwy6hW43iG9PUs1RO8",
    "summary": "<p></p><h1>研究背景</h1><p></p><p>随着云计算技术的不断发展和普及，越来越多的企业和组织开始采用云原生架构来构建和运行应用程序。云原生是近些年重要战略技术趋势，也是企业数字化转型的重要途径。</p><p>今年年初，InfoQ研究中心发布了《中国软件技术发展洞察和趋势预测报告&nbsp;2023》，在报告中将各个领域的关键技术按照不同成熟度阶段进行了划分，总结成为中国技术成熟度评估曲线。报告发布之后也获得了众多企业和开发者的关注和讨论。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4d85ee1cd05f4b611bde1b873183ecc.png\" /></p><p></p><p>因此，InfoQ研究中心将基于收集到的云原生领域的各种数据，更新云原生领域的技术成熟度模型，为技术的应用决策和未来投资参考提供研究分析工具。</p><p></p><h1>研究内容</h1><p></p><p>基于技术发展时间、技术专利规模、技术应用市场规模、技术舆论热度等基础评价指标，构建中国云原生技术成熟度模型。</p><p>报告包含“行业发展洞察”、“云原生技术成熟度模型解读”、“云原生技术企业生态图谱”等多个行业研究内容，并且在模型解读中还会有案例分析等厂商研究内容。</p><p></p><h1>征集范围</h1><p></p><p>企业主要业务类型包括：</p><p>提供云原生产品与服务的各类厂商（云原生数据库、容器服务、数据流与消息传递、Kubernetes&nbsp;集群托管、监控、安全与合规等）；其他综合科技厂商。</p><p></p><h1>参与价值</h1><p></p><p>参与本次征集的厂商，将有机会获得：</p><p>1、InfoQ研究中心《中国云原生成熟度模型报告&nbsp;2023》云原生技术企业生态图谱、典型案例两大内容露出，提升厂商品牌知名度和行业影响力。</p><p>2、报告将通过InfoQ官网、公众号InfoQ等多个官方平台进行发布，同时会在多家生态渠道传播。</p><p>3、将有机会受邀参加InfoQ的线上线下活动，与行业内的专家、投资机构等进行深入的交流。</p><p></p><p>本次厂商征集自即日起，截至07月20日，欢迎识别下方二维码，添加小助手后，填写表单参与报告征集。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8a0f5451e7cea9ef0c7f9c70c0da9421.png\" /></p><p></p>",
    "publish_time": "2023-07-04 10:26:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面向故障处理的可观测性体系建设",
    "url": "https://www.infoq.cn/article/g4LyEj3Ue2JCD2vLAfXc",
    "summary": "<p><a href=\"https://www.infoq.cn/article/kTtlLZxNUPmlvYpzA0eF\">笔者</a>\"从 12 年开始入行，从事 DevOps 研发工作，做过部署系统、监控系统、可观测性相关产品，也做过 SRE 一线和管理工作，对于可观测性的理解和实践，有一些小小的见解，利用本文和大家做一个探讨分享。本文主要内容包括：</p><p>可观测性在整个商业体系中的位置和价值如何快速发现故障，使用哪类指标告警SRE 在谈论故障定位的时候，谈的是什么如何找到故障直接原因，找到止损依据如何让可观测性系统呈现观点，辅助洞察，定位故障</p><p></p><h2>可观测性在整个商业体系中的位置和价值</h2><p></p><p>做一个事，首先得有价值，如果价值太小不值得投入。可观测性也不例外，我们首先分析一下可观测性在整个商业体系中的位置和价值。思考第一个问题：作为在线类产品，我们希望客户/用户有一个好的产品体验，那怎么算一个好的产品体验？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/797ea306fce73a3782598262c260cf3c.png\" /></p><p></p><p>很明显，产品体验包括功能体验和可靠性体验。功能体验依赖产品设计和迭代速度，跟今天的话题关系不大暂且按下不表。可靠性体验呢？可靠性体验核心就是追求高可用、低延迟，通俗讲就是每次打开站点或app，都不报错，速度嗖嗖的。那如何才能具有好的可靠性体验呢？</p><p></p><p>其实如果一切正常，就应该是可用且速度快的，除非哪里出了问题，也就是发生了故障，才会报错或者延迟大增。那技术团队要做的，除了持续优化架构和性能，就是不断和故障做斗争了。降低故障发生的频率，降低故障的影响范围，降低故障的恢复时间。归纳为 6 个字：降发生、降影响！</p><p></p><p>怎么做？有没有方法论来指导？我们可以从故障的生命周期着手，来优化生命周期的各个环节，每个环节都做好了，理论上结果就是好的。故障生命周期的梗概图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/73c9e6f892d22efd34b3b90df3f6fadc.png\" /></p><p></p><p>从大面上，可以分成事前、事中、事后三个大的阶段：</p><p>事前：及时发现风险，做好架构、预案、演练事中：及时发现故障，及时定位，及时止损事后：排查根因，落实复盘改进项</p><p></p><p>看起来寥寥数语，没有特殊的东西，但实际上每个环节要做好，都不容易。那可观测性，在这整个过程的职能是什么？在哪个环节发挥价值？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/72ab3a951c58cb598c5d098ae537850f.png\" /></p><p></p><p>显然，可观测性，是在故障发现、定位环节发挥作用的，核心价值就是帮我们快速发现故障、快速定位故障，进而降低故障的影响。如此，可观测性的位置和价值就很明确了，用一张图概括：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f61bada46081700d99d9dab70e438889.png\" /></p><p></p><p>客户/用户需要好的产品体验，好的产品体验包括可靠性体验，要想有好的可靠性体验，就得减少故障，所谓的降发生、降影响，而这，又依赖了可观测性的能力。所以：可观测性最终是服务于产品体验、服务于商业成功的（想不想取得商业成功？根据刚才的分析可观测性可是重点因素哦），核心目标是快速发现、定位故障。</p><p></p><p>那么，如何快速发现故障？</p><p></p><h2>如何快速发现故障，使用哪类指标告警</h2><p></p><p>要想能够快速发现故障，得先定义什么是故障！简单来看，产品体验受损，就是故障！比如：</p><p>电商产品：用户无法下单、无法支付、无法查看商品、无法查看历史订单存储系统：用户无法读、无法写、或者读写延迟过高流媒体产品：无法开启播放、无法拉流、无法浏览视频信息</p><p></p><p>既然能够定义如何算是产品体验受损，那就可以梳理出相关的监控指标，比如：</p><p>电商产品：订单量、支付量、商品/订单访问成功率/延迟存储系统：读/写成功率、读/写延迟流媒体产品：播放量和成功率、拉流延迟、视频浏览成功率/延迟等</p><p></p><p>大家有没有发现这类指标的特点？显然，都是可以量化客户体验的指标，这类指标我们称为结果类指标（后面会介绍原因类指标），大面上可以分为两类，一类是业务指标，另一类是 SLO 指标。</p><p></p><p>一般公司做监控的时候，可能会意识到要做 SLO 指标的监控，容易忽略业务类指标的监控。其实，业务类指标才是老板更为关注的指标，而且，SLO 指标正常的时候，业务指标未必正常。比如客户到服务端的网络出问题了，服务端的成功率、延迟指标都是正常的，但是客户无法下单，订单量会下跌。所以，一定要重视业务指标体系的构建和监控。</p><p></p><p>听起来，业务指标和 BI 数据很像有没有？确实，最大的相同点是：都是老板关注的，哈哈。不同点呢？BI 数据对准确性要求很高，对实时性要求没有那么高，而业务指标监控，对准确性要求没有那么高（只要能发现数据趋势出问题了就可以了），对实时性要求很高，毕竟是用来发现故障的，如果实时性太差，黄花菜都凉了。</p><p></p><p>指标体系的构建，除了结果类指标，与之对应的还有原因类指标。都需要，但是我们配置告警的时候，一般是针对结果类指标来配置。因为产品的核心业务功能是可枚举的，每个功能对应的结果类指标就是可枚举的，做好结果类指标的告警，就可以保证告警是全的，做到有故障必有告警！举个例子：实时交易类系统，交易量突然下跌。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c15623e52d18db86e9487b0ad3b39de.png\" /></p><p></p><p>如果，面向原因类指标配置告警，则永远无法配全，无法做到有故障必有告警！实际上，原因类指标不必一定要配置告警，出故障的时候可观测，其实也基本够了。</p><p></p><p>如上，要构建可观测性体系，首先要建立完备的指标体系，其中非常关键的是结果类指标，即业务指标和 SLO 指标，结果类指标配合告警系统可以快速发现故障！从这里也可以看出，监控（monitoring）和可观测性（observability）是相辅相成的，非替代关系。</p><p></p><p>OK，既然可以发现故障了，下一步就是定位故障了。</p><p></p><h2>SRE 在谈论故障定位的时候，谈的是什么</h2><p></p><p>在讨论这个问题之前。先分享一个信息层级的概念。说：信息分4个层级，最底下是数据，杂乱无章，比如海量的指标、日志、链路追踪的数据；数据上面是特征，比如最大值、最小值、同环比等，比如5个服务实例，延迟的最大的是哪个，这叫数据特征；特征上面是观点，从故障定位场景来举例，比如根据特征数据分析之后发现，数据库没有问题，依赖的第三方服务也没问题，这就是观点；观点之上就是洞察，或称洞见，综合所有观点，得出故障定位结论，得知具体是哪个模块的什么原因导致了本次故障，就是最终洞察。画个图示例一下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab4212cba5a30046ad10f5412eeca63b.png\" /></p><p></p><p>要想得到最终的洞察（定位到故障），首先要依赖底层的数据完备性，否则就是巧妇难为无米之炊！但是故障原因五花八门，数据能全么？做过 SRE 或者运维的朋友肯定感触颇深，故障可能是电源模块坏了、机房空调坏了、机柜压住网线了、供电不稳、某个盘故障了、中间件配置错了、被黑客攻击了、分布式中间件脑裂了、写日志hang住了、程序配置错了、程序连接第三方的地址错配成线下地址了、DNS配错了、证书过期了、代码Bug了、疏漏了某个罕见用户流程…等等等等。</p><p></p><p>这么多可能的故障原因，要通过可观测性数据分析出来，这数据能全么？比如代码 Bug，要想能根据可观测性数据分析出是哪一行代码的问题，岂不是要像在 IDE 里调试那样，每一行代码的输入输出都得拿到啊，这成本谁扛得住啊，性能损耗谁扛得住啊…</p><p></p><p>如果我们的目标只是定位直接原因，找到止损依据尽快止损，这个底层数据需求就少多了。比如我们不需要知道是哪行代码出了问题，我们只要知道是某个模块做了变更导致了故障，就可以去止损（这个场景的止损动作就是回滚）了。再比如，多活的服务，有时仅仅知道是 A 机房的问题就可以了，把流量切到 B 机房就可以解决。</p><p></p><p>综上，个人观点：使用可观测性数据定位根因，几无可能100%覆盖全部场景！因为数据就不可能全！但如果只是用可观测性数据定位直接原因，找到止损依据，则100%是可以做到的，而这，才是我们应该努力的方向。</p><p></p><p>当 SRE 在谈论故障定位的时候，其实谈论的时是如何找到直接原因，尽快止损。而根因，可以留在复盘阶段慢慢找的。</p><p></p><h2>如何找到故障的直接原因</h2><p></p><p>回答这个问题之前，我们先来看看一个服务要想正常运行，依赖了哪些内容，或者说一个服务如果出故障，可能会是哪里的问题。如果我们能够枚举故障类别，那么我们就可以针对每个类别去分析，找到故障的直接原因。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/65/65f9508d26469553daa68aeec70808f8.png\" /></p><p></p><p>首先，依赖的基础设施（基础网络、硬件、Runtime环境）不能出问题，依赖的第三方其他服务不能出问题，这两个方面大家比较容易理解，不多说了。还有就是服务本身的变更，比如二进制变更、配置的变更、部署方式的变更、流量接入方式的变更，等等，也可能引发问题。最后就是上游访问的方式，比如流量突增，显然也可能会带来故障。</p><p></p><p>那针对这些故障场景，我们应该去看哪些数据呢？这其实就是可观测性数据底座的建设方向。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8eaf8e2083e1b39adc7cb8eea62cb34.png\" /></p><p></p><p>咦？说来说去，还是要建立 metrics、logs、traces、events？是的，但不仅是，只有数据还远远不够，我们需要通过平台工具，通过数据运营整理，帮助用户找到数据特征，建立初步观点，最终形成洞察定位故障直接原因。还记得那张信息层级的图吧：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04b672f449297f8e87f0314e7722547f.png\" /></p><p></p><p>网上有人批评可观测性三支柱的说法，核心要点是：不能只关注 raw data，就像一道菜，只有原料还不能称之为一道菜，没有炊具、菜谱、厨师，无法最终产出那道菜（客人要的是那道菜，那道菜才是结果，应该没有哪个餐厅说，你看我原料都有，完活了，让客人吃吧，应该没有这样的餐厅…）。</p><p></p><p>Martin Mao 曾经也写过一篇文章《Beyond the 3 Pillars of Observability》来论述这个事情。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e49091cfa86e586ab6323f998457137.png\" /></p><p></p><p>他的核心观点是：只关注三支柱raw data，认为有了三支柱数据就建立了可观测性，是不对的，我们更应该面向结果来思考如何构建整个体系，Martin Mao 认为，所谓的结果，就是 Remediate，就是止损！英雄所见略同。</p><p></p><h2>可观测性体系具体要如何做才能辅助技术团队止损</h2><p></p><p>还是参考刚才信息层级的图，有了 raw data 数据底座之后，可观测性体系还需要利用平台能力、通过数据运营整理，呈现数据特征、帮用户建立初步观点，最终形成洞察，定位故障直接原因。</p><p></p><h3>可观测性体系要告诉我故障模块</h3><p></p><p>这应该是可观测性体系提供给客户/用户的第一个观点，故障发生了，现在都是微服务的，你得一目了然的告诉我具体是哪个模块故障了不是。总不能让我写一条有一条的 promql，看一个又一个监控大盘，才能找到故障模块吧。故障处理可是要争分夺秒的，一个一个查看太浪费时间了。</p><p></p><p>既然要一目了然，初始页面上的内容不能太多，从技术角度来看，一般模块都是有层级关系的，首先是系统，然后是子系统，然后是模块。所以，初始页面应该展示系统的健康状况，如果某个系统有问题，应该可以点击进去查看详情（这个过程称为下钻），下钻到子系统，再下钻到模块，最终找到故障模块。</p><p></p><p>那如何衡量一个模块是否健康呢？这其实就可以使用 SLO/SLI 这套体系，每个模块都有几个 SLI，每个 SLI 异常，这个模块就可以定义为异常，进而定义子系统异常、系统异常，这个过程也有种故障冒泡上浮的感觉。</p><p></p><p>我以我们的产品来举例这种效果图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/734f0c69aa555d015852229751dbcd86.png\" /></p><p></p><p>这样的系统我们称为灭火图，最上层是一个个的系统卡片，如果有问题就会有个飘红的小火苗，点击详情进入，可以看到相关子系统，点击故障子系统，可以看到模块以及核心接口的列表，进而可以定位到故障模块或核心接口。产品通过颜色做引导，而且具备层级关系，即可做到一目了然。</p><p></p><h3>可观测性体系要告诉我故障模块的各项依赖是否健康</h3><p></p><p>模块依赖的数据库、中间件、基础网络、机器硬件、第三方服务等等，都会影响模块的健康状况。所以，当模块异常的时候，我们需要知道各项依赖是否健康，如果依赖也异常，那么模块异常的直接原因基本可以断定是异常的依赖项导致的。</p><p></p><p>要用可观测性产品建立这样的视图，核心有两点，一个是依赖的关联关系，一个是依赖项的 SLO 视图（通常体现为 metrics 仪表盘）。下图是一个逻辑示意图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/4421982147704e140791a3118590e58a.png\" /></p><p></p><p></p><h3>可观测性体系要告诉我是否是变更导致的</h3><p></p><p>线上故障，大概 70% 都是变更导致的，所以运维行业中流传一句话叫：“变更是万恶之源”。所以，当故障发生的时候，我们需要知道是否是变更导致的，如果是变更导致的，就要尽快止损。</p><p></p><p>如果迭代比较快，每周的变更数量会很多，那么如何快速定位到是哪个变更导致的故障呢？这需要可观测性产品提供一些数据特征给用户，用户才能方便分析。典型的是在时间维度上做文章。把故障和变更放到一张图上，在时间维度上做对比，比如从故障时刻往前看 1 小时，看看有没有变更，如果有，那个变更很可能就是罪魁祸首。示意图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/0286cb42253bcdbfa54b5d9755ffc3e6.png\" /></p><p></p><p>注意，这里的变更不仅仅是代码变更，还包括配置变更、机器变更、网络变更等等，变更事件收集得越全，越有价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/21c586c7231033ac28f4e0507e5a6211.png\" /></p><p></p><p>上面，我举例了三个可观测性产品需要为用户输出的观点：</p><p>可观测性体系要告诉我故障模块可观测性体系要告诉我故障模块的各项依赖是否健康可观测性体系要告诉我是否是变更导致的</p><p></p><p>当然，还有其他观点可以输出，比如是否是容量不足导致的故障，大家可以自行思考看看还可以让可观测体系输出哪些观点。但是，罗马不是一天建成的，在某个阶段，可观测性体系输出的观点有限，不足以帮我们定位故障，此时，可观测性体系还可以做什么呢？</p><p></p><p>至少，还需要提供工具帮我们分析数据特征，别让用户陷入海量散乱的可观测性 raw data 中。这需要多维分析引导能力、数据串联打通能力。举一个数据串联的例子：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/453c988a566d2f0a1283a435ec48fbec.png\" /></p><p></p><p></p><h2>总结</h2><p></p><p>可观测性体系不能仅仅只有散乱的数据，而应让数据呈现特征，让特征呈现观点，让特征和观点辅助洞察：洞悉故障直接原因，完成止损！这才是建设可观测性体系的核心目标。</p><p></p><p>作者简介：</p><p>秦晓辉，Open-Falcon、Nightingale 创始研发，极客时间《运维监控系统实战笔记》作者，公众号 SRETalk 主理人，快猫星云创业合伙人，创业方向是稳定性保障方向。如果你有兴趣来《<a href=\"https://mp.weixin.qq.com/s/Y4rIfV4_7MuYigLNNrtifg\">运维百家讲坛</a>\"》输出一些自己的宝贵经验和见解，欢迎联系我，联系方式如下：18612185520（微信同号）。</p>",
    "publish_time": "2023-07-04 11:40:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "之江实验室： 如何基于 JuiceFS 为超异构算力集群构建存储层 ？",
    "url": "https://www.infoq.cn/article/83tfpS8cgjfuZTU27QgK",
    "summary": "<p></p><blockquote>今天，高性能计算结合人工智能技术正在推动科研创新。例如通过破解水稻基因密码推动作物育种从“试验选优”向“计算选优”发展，在医药领域快速分析分子与蛋白之间的相互作用，发现潜在的能够有效干预疾病发生的药物分子。&nbsp;</blockquote><p></p><p></p><p>之江实验室就是上述科研创新的推动者，实验室由浙江省政府主导、浙江大学等院校支持、企业参与的事业单位性质的新型研发机构，为材料、基因、制药、天文、育种等科学领域的研究提供新的方法、工具和手段。&nbsp;</p><p></p><p>由于算力资源的天然异构性，以及不同技术实现的计算能力往往出自不同的系统架构或指令集，会导致软件不兼容性，从而提高了算力使用的门槛，也使得算力利用率难以有效提高。为解决这个问题，之江实验室将各种异构算力资源汇聚在一起，形成一个庞大的“算力池”。本文将分享之江实验室如何基于 JuiceFS 为超异构算力集群构建存储层的实践。</p><p></p><h2>01-之江实验室的输出反应堆</h2><p></p><p>数字反应堆是之江实验室一个大型科研装置。整个科研装置由软硬件部分组成。在软件方面，负责研发之江瑶光智能操作系统。</p><p><img src=\"https://static001.geekbang.org/infoq/d0/d053313d04e6e76453f1f40ccb6de9b3.png\" /></p><p>该智能操作系统主要包含两个关键组成部分。首先，它提供了通用的计算平台解决方案，为上层应用层提供支持。通过这个平台，用户可以针对不同的应用领域，如计算材料、计算制药、计算天文等，进行开发和应用。</p><p></p><p>其次，我们实现了一个异构资源聚合方案。在之江实验室，我们拥有多个异构集群，包括CPU集群、GPU集群以及一些超算资源。这些不同类型的计算资源具有不同的使用方式。通过异构资源聚合方案，我们将这些不同的计算资源统一聚合，实现统一的管理和使用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/57e367290b5c79bdbb57c8cd27da20c3.png\" /></p><p>整体架构如上，在之江实验室的计算与数据中心，我们部署了多套异构集群，包括 H3C 的傲飞集群、之江的计算集群、国产的曙光集群等，以及边缘计算场景，这些都纳入了我们的管控系统中。通过集群设备插件的方式，我们对这些不同的集群进行了统一抽象，并实现了算力的聚合。在整个异构算力联邦的体系中，我们将这些不同的算力集群都抽象为 Kubernetes（k8s）集群进行管理。</p><p></p><p>上层下发的业务指令及不同类型的作业，通过元调度器来决定将这些作业发送到哪个集群。根据不同的调度策略，如计算优先级、能耗优先级和性能优先级，来确定计算作业的具体执行方式。目前，我们已接入约 200P（PFLOPS, 1P 相当于每秒执行一千万亿次浮点运算） 的 AI 算力和 7000 核的 HPC 算力。</p><p></p><h3>算力侧对存储的需求</h3><p></p><p>首先：存储层的抽象和统一。因为在许多计算场景中，包括超算和AI训练，都使用 POSIX 接口。因此，我们希望在这一层面上统一使用 JuiceFS 的接口来提供服务。</p><p></p><p>第二个方面：存储方案的通用性。目前接入的算力集群是异构的，那么尽量需要考虑方案在不同的异构集群中都能适用。</p><p></p><p>第三个方面：数据的编排条件。我们的数据是有典型的冷热特性，在一个任务在计算过程中，它用到的数据是热数据，任务计算之后或者过了几天之后，这个数据就变成了冷数据，我们对冷数据的读和操作是比较少的。</p><p></p><p>第四个方面：存储性能的要求。数据读写性能要好。特别是热数据的读取性能。在算力集群中，计算资源非常宝贵，如果因为数据读取慢导致CPU，GPU空转等待，是极大的浪费。</p><p></p><h3>存储方案选型</h3><p></p><p>方案1：裸露的对象存储（OSS）与S3FS + NAS 相结合</p><p></p><p>这个方案存在一个较大的问题，即直接使用裸露的对象存储性能非常差。另外，裸露使用对象存储的 S3FS 挂载点经常会出现莫名其妙的丢失。一旦挂载点丢失，容器将无法访问，如果要恢复挂载点，必须对整个容器进行重启，这对用户业务造成了很大的干扰。</p><p></p><p>由于开始时集群规模较小，而且这套方案部署简单，实验室一开始采用了这种方案进行部署。但随着集群规模的逐渐扩大，尤其是从去年开始建设的数字反应堆，从单一集群到多集群的演进过程中，当节点数量从最初的 10 多台逐渐扩展到 100 多个节点时，这个方案基本上已经行不通了。</p><p></p><p>方案2：Alluxio + Fluid + OSS</p><p></p><p>经过调研，我们发现该方案的结构相对复杂，涉及到许多组件的组成。之江实验室是一个超异构的多集群环境，由于 Alluxio 并不是一个强一致性的文件系统，它实际上只是一个缓存的粘合层。在这种多集群环境下，会面临元数据不一致的问题，而解决这个问题特别困难。由于上层用户的业务产品非常繁多，我们不能干涉用户的使用方式。在这种情况下，如果不同集群之间的数据不一致，将会导致严重的问题。其次底层仍然使用OSS，当数据规模扩大到一定程度时，由于 OSS 的元数据性能问题，当存储规模达到一定级别后， 元数据同步、新集群的缓存层初始化等操作也会遇到较大的性能瓶颈。</p><p></p><p>方案3 ：JuiceFS（最终采用）</p><p></p><p>JuiceFS 有非常详细的社区文档，可以直接上手使用，并且在我们的搭建测试集群以及最终的上线部署中表现出色。另外，JuiceFS 支持 CSI可以容器化部署，另外对国产硬件的适配性较好。因此我们最终选择将 JuiceFS 作为我们算力侧的存储底座。</p><p></p><h3>使用 JuiceFS 的优势</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f56f28f2d866ea9921f0c440327c20b.png\" /></p><p>首先，JuiceFS 提供了丰富的元数据引擎选择，比如 Redis 和 TiKV，使得 JuiceFS 具有较好的元数据性能。目前我们实验室使用的是由三个节点搭建的 TiKV 作为元数据引擎。由于该配置是去年建立的，现在的性能已经有些不够用，后续我们将逐步提升性能。</p><p></p><p>最初我们考虑使用 Redis 作为元数据引擎，但后来发现如果使用 Redis，就无法实现水平扩容。从而使用了 TiKV，则可以随着文件系统数量的增长逐步扩展，这确实更好。</p><p></p><p>第二，在跨集群环境下，使用 JuiceFS 可以实现文件的原子性和一致性。在集群 A 中写入的文件在集群 B 中立即可见。然而，如果使用 Alluxio，就无法做到这一点。Alluxio 需要进行一些数据同步事件等操作才能实现，而这些事件实际上会带来一定的开销。</p><p></p><p>第三， JuiceFS 具备缓存能力。客户端可以配置一个缓存目录，使用缓存后可以大大降低算力集群对底层存储的压力。</p><p></p><p>第四，JuiceFS 对于 POSIX 的兼容性非常好。我们发现 Alluxio 的实际兼容性并不那么好，而且其客户端性能相对较一般。Alluxio 可能更适用于不同异构数据源的统一接入层，用于读取数据较好。但是，如果需要频繁写入或修改数据，则可能使用起来并不理想。</p><p></p><p>第五 JuiceFS 的社区是常活跃。</p><p><img src=\"https://static001.geekbang.org/infoq/52/52e68a70d44d1821247c428d73930845.png\" /></p><p>这个是我们自己在实验室环境下测出来的，测试工具：FIO 16 线程、4M Block 、1GB 数据 上图 NAS 的性能就没法看，因为当时测评的时候还在生产环境正在提供服务，当时有大概七十几个节点正在运行，带宽很小，基本就运行不了。</p><p></p><h2>02-存算分离架构演进</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ceb60d24d04c55aada05f0746fe6894e.png\" /></p><p>初期，整个高性能计算的过程实际上是分为很多个环节，但是数据分散在不同的存储系统中会带来使用效率和便利性上的挑战。为了简化数据的管理和流转，我们使用了一个统一的存储底座作为存储基础设施。存储底座的核心能力包括高可靠性、低成本和高吞吐量，因此我们选择了对象存储作为存储底座。将数据存储到对象存储中可以轻松实现数据的冷热分层，从而节省存储空间。</p><p></p><p>然而，直接让计算集群直接使用裸对象存储仍然存在一些问题。首先是元数据性能差的问题，例如对同一目录下文件的列表操作，在文件数量较多的情况下，耗时会非常长。第二个问题是带宽消耗大，数据湖提供的是普通的IP网络，而不是 RDMA 高速网络，因此总带宽有限。</p><p><img src=\"https://static001.geekbang.org/infoq/af/afc7be237781c15bc7555e3fd2e43d07.png\" /></p><p>因此，在对象存储之外，我们还建立了一个元数据集群，并使用了 TiKV 数据库。基于对象存储和 TiKV，我们构建了JuiceFS 分布式文件系统。算力集群通过在节点上安装 JuiceFS 客户端来读取文件系统的数据。这样，我们可以克服对象存储的一些限制，提高元数据性能，并减少带宽消耗。</p><p></p><p>为了实现高效的数据流转，我们通过文件管理系统允许用户进行文件的上传和下载操作。文件管理系统底层采用JuiceFS S3 网关，将数据写入底层存储。</p><p></p><p>除了数据湖和元数据集群，我们还建立了一个高速缓存集群，它紧密部署在计算集群中，主要目的是为了实现最佳的 I/O 性能。这样解决了计算集群与对象存储数据湖底座之间高效数据流转的问题。用户并不需要关心数据是存储在对象存储中还是在高速缓存集群中。</p><p></p><p>算力系统对数据流转进行管控。计算集群和高速缓存集群之间通过200G的RDMA高速网络连接。高速缓存集群上部署了BeeGFS高速并行文件系统，将该文件系统作为一个目录挂载到计算集群。这样，计算集群可以像使用本地目录一样使用该缓存系统。</p><p></p><h2>03- 存储能力产品化建设</h2><p></p><p>在不同的业务场景中，对存储的需求和性能指标是不一样的。为了能够更高效地服务用户，我们提出了打造存储能力产品化这样的想法，目前JuieFS 被应用到了以下几类存储产品中。</p><p></p><h3>通用文件存储</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4fbcd5b12b0f9eee4a2115954ea76c7a.png\" /></p><p>JuiceFS 会将其数据保存在一个特定的目录下，并根据用户所属的组织架构生成一个唯一的访问路径。通过直接将该路径挂载到容器内，实现数据的隔离。用户可以通过页面端进行文件的上传和下载，也可以使用我们提供的命令和工具对文件进行操作。</p><p></p><h3>存储卷</h3><p></p><p>在初始建设阶段，通用文件存储存在一个问题，容量的扩展性较差。底层的对象存储集群（oss）的容量有限，随着数据量的增加，用户无法申请更多的存储空间。为解决这个问题，我们引入了存储卷的概念。</p><p><img src=\"https://static001.geekbang.org/infoq/a0/a0f22e78e2e1bfa0f577a6919c1c8950.png\" /></p><p>存储卷可以类比为云盘，不同的存储卷相当于不同类型的云盘。对于不同的存储类型，我们可以将它们包装成不同的存储卷，以满足用户在不同场景下的需求。</p><p></p><p>对于需要频繁地读写海量小文件的场景，它需要使用延迟较低且吞吐量较高的存储产品。为满足这种需求，我们将之前搭建的高速缓存集群转化为高速存储卷的功能，直接将其文件系统目录开放给用户使用。这样，用户可以直接使用高速存储，而无需通过 JuiceFS 来访问，可以更直接地感受到高速存储的性能优势。</p><p></p><p>而对于需要保存大数据但实际上不频繁读取的用户，可以结合JuiceFS和对象存储来创建标准存储卷。这样可以提供较大的存储容量和可接受的吞吐性能，同时相对于高速存储卷，支持跨集群的网络互通能力。</p><p>此外，一些用户可能对性能有更高的要求，例如他们需要本地盘的产品，但同时也需要数据持久化的能力。在 Kubernetes 场景下，如果用户直接将数据写入本地盘，存在数据丢失的风险，例如遇到意外重启或物理节点问题。在这种情况下，用户需要一种持久化的解决方案。我们可以通过将用户在受影响节点的本地盘开放一部分存储空间作为本地存储卷，并在作业调度时根据用户指定的存储卷将任务调度到指定的节点上。</p><p></p><p>另外，不同存储产品在容量、吞吐量和跨集群互通能力方面存在差异。例如，高速存储可以在集群内部进行互通，但无法跨集群；存储产品的容量和成本也各不相同。高速存储采用全闪存的集群，建设成本较高，而对象存储的建设成本相对较低，且具备较大的存储容量。因此，将不同的存储硬件（设施）能力包装成不同的存储产品来适配用户不同的业务场景。</p><p></p><h3>数据编排</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed750852102e298c674c4b474a42a323.png\" /></p><p>在使用 JuiceFS 时，我们还实现了一个数据编排功能。管理员可以将常用的数据集上传到文件系统某个目录，这个目录可以在上层抽象成一个公开的数据集。不同用户在创建作业时都可以挂载这些数据集。普通用户也可以上传自己的私有数据集，并通过 JuiceFS 的预热功能对这些数据集进行预热。</p><p></p><p>我们在算力集群内部建立了一个高速缓存集群。使用 warmup 指令，用户的数据集可以直接从两端预热到计算节点的高速缓存集群。这样，用户在进行大量模型训练时，可以直接与自己搭建的高性能集群交互，无需与远程 OSS 集群进行交互，从而获得更好的性能。</p><p></p><p>另外，这种设置可以降低对象存储底座的网络带宽压力。整个缓存的淘汰过程由 JuiceFS 客户端自动管理，因为可以对访问目录进行上限容量的配置。对用户来说，这部分功能相对透明且易于使用。</p><p></p><h2>04-JuiceFS使用过程当中也遇到了一些问题</h2><p></p><p></p><h3>文件读取性能</h3><p></p><p>在我们选择使用JuiceFS之后，我们在内部进行了一些文件读取性能的测试，并与算法团队合作进行了测试。当时，从测试结果看，JuiceFS 的读取性能总是比 NAS 慢得多。我们开始查找原因为什么 JuiceFS 比 NAS 还要慢。</p><p><img src=\"https://static001.geekbang.org/infoq/7b/7b47102b88536be0cbf622fe988e46ce.png\" /></p><p>后来我们发现，在使用 JuiceFS 和 TiKV 作为元数据的场景中，像列举目录这样的 API 操作实际上是随机的，它并不像 NAS 或其他文件系统那样保证一致的顺序。在这种情况下，如果算法是基于随机选择文件或者代码是固定的，那么可能会认为选择的那些文件应该是固定的。</p><p></p><p>在处理海量小文件的场景中，元数据的开销是相当可观的。如果元数据没有被缓存到内存中，每次都需要从元数据引擎那里获取，与没有缓存相比这会带来较大的开销。因此，通过这个问题，我们发现在特定的场景下需要对文件的索引目录进行编排。例如，某个算法可能需要处理数十万甚至数百万个文件，如果要确保算法训练的一致性，首先需要将这些文件作为索引文件，作为自己的一个索引目录树。每次进行算法训练时，直接读取索引文件，而不再调用 list dir 的操作，以确保这个文件夹下的文件目录树在算法训练中保持一致性。</p><p></p><p>编著注：造成读性能慢主要与用户的使用场景相关，评估后对“目录的随机读”这个功能未作调整，如果其他用户在这个问题上也有类似问题，欢迎提出。</p><p></p><h3>TiKV 无法垃圾回收</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af9f5f17d471180741b51cdf60eb0e07.png\" /></p><p>在使用过程中，我们遇到了 TiKV 无法进行垃圾回收的问题。由于我们只使用了这个文件系统，并且显示容量为 106T、1.4 亿个文件，而 TiKV 占用了 2.4T 的容量，这显然是不正常的。</p><p></p><p>根据官方文档的显示，例如 Redis，大约 1 亿个文件应该只占用大约 30GB 的容量。我们进行了排查后发现可能是 TiKV 的元数据引擎没有进行垃圾回收。我们还查看了报表，发现整个垃圾回收指标为空。可能的原因是我们只部署了 TiKV，而没有部署 TiDB。然而，TiKV 的垃圾回收实际上需要依赖 TiDB，这是一个容易被忽视的问题点。</p><p>编著注：JuiceFS 在 #3262 和 #3432 的 PR 中加上了 TiKV 的后台 GC 任务，修复了这个问题。这些修复已经在 v1.0.4 中合入。</p><p></p><h3>JuiceFS client 内存占用较高</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af4c48bdeb39da32ffea72f8c40d73de.png\" /></p><p>当我们挂载 JuiceFS 客户端时，我们将高速缓存集群设置为保存目录，并将其容量设置得相对较高，理论上限为 50T。</p><p></p><p>在这种情况下，JuiceFS 客户端会定期扫描缓存目录，并建立内存索引，以便 JuiceFS 知道哪些数据位于缓存目录中。因此，这会占用相当多的内存。如果目录非常庞大，我们建议用户关闭这个扫描功能。</p><p>在测试小文件随机 I/O 时，我们觉得表现还可以，但在测试顺序 I/O 时，出现了一个较大的问题。例如，使用 dd 命令创建一个 500MB 的文件，结果发现对象存储生成了大量快照。可以看出，这里的存储和对对象存储的操作远远超过了创建一个 500MB 文件应有的操作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5ebf2b6c93d18741d6c3c18c77427d4.png\" /></p><p>在进一步排查时，我们发现在启用&nbsp;-o writeback_cache&nbsp;参数后，顺序写会变成随机写，从而降低整体的顺序写性能。该参数只适用于非常高级的随机性场景。如果在不是这种场景下使用该参数，将导致严重的问题。这也是在使用 JuiceFS 时需要注意的一个点。</p><p></p><p>编著注：这个问题主要是针对使用 NAS 做缓存的场景，已经在1.1beta中优化，扫描时大幅减少内存占用，并提升速度。JuiceFS 在 #2692 中添加了 --cache-scan-interval 来自定义扫描时间，并且可以选择只在启动时扫描一次或完全关闭扫描，用户可以配置该选项。对于使用本地盘做缓存的用户则不需要调整。</p><p></p><p></p><h2>05-后续规划：更丰富多样的存储产品</h2><p></p><p>多层次</p><p>我们将提供更多层次的软硬件产品，并将这些能力以不同的存储卷形式进行产品化，以适应用户在不同场景下的存储需求。</p><p></p><p>隔离性</p><p>目前存在数据安全风险，所有用户的数据都存储在同一个大型文件系统中并通过hostpath挂载在裸机。如某些用户拥有节点登录权限，实际上可以访问整个文件系统内部的数据。为了解决这个问题，我们计划在后续采用 CSI 模式结合路径定制来避免隔离性问题。我们还将上线配额管理功能。在用户使用存储产品时，需要有一种强制手段来限制用户可以使用的存储容量，并且能够准确查看用户实际使用了多少容量。直接使用 du 命令查看容量的过程开销很大，并且不太方便。配额管理功能将解决这个问题。在计量计费场景下，我们需要了解用户产生的流量和消耗的能量，并根据实际使用的容量进行计费。因此，良好的容量管理能力是必需的。</p><p>监控&amp;运维</p><p>在使用 JuiceFS 时，我们是直接在物理机上挂载，通过暴露一个监控端口，我们的生产集群可以与这些端口进行通信，并建立了一套监控系统，可以监控和收集所有的监控数据。数据的容灾和迁移能力目前还比较欠缺。我们遇到了一个典型场景，即现有集群的容量不足，需要上线新的集群。在新旧集群之间，如何处理数据迁移以及不同数据的迁移方式，如何在不影响生产用户的情况下尽量保证业务不中断，实现数据迁移仍然是一个较为困难的问题。因此，我们计划在后续寻找解决方案，以提升这方面的能力另外，我们还在开发基于 JuiceFS 和 CSI 插件的通用能力，以实现在不同的存储客户端上动态挂载的能力。在生产环境中，用户对挂载参数的调整有一定需求，因为不同的挂载参数适配不同的业务产品。然而，如果直接调整挂载参数，可能会导致整个物理节点的中断。因此，如果能够实现动态挂载的能力，用户只需要对业务进行适当的切换，而无需进行重启等操作。</p><p></p><p>对 JuiceFS 一些功能期望：</p><p>卷管理能力&nbsp;（配额、用户、权限）卷能力在之前已经部分实现了配额的功能，但实际上我们更需要的是基于用户和管理人员权限的管理能力。目前，JuiceFS是挂载在一个大型文件系统上，如果为每个用户创建文件系统，将会带来很高的开销。因此，我们目前采用的是基于一个大型文件系统，通过不同的目录来管理不同用户的权限配合，缺少一个统一的、中心化的用户权限管理系统，仍然需要依赖Linux的权限管理。然而，Linux的权限管理是分布在不同节点上的，对用户权限的管理相对较为困难。我在思考是否可以依赖元数据，并将其作为一个中心化数据库的能力，实现用户和权限的管理。这样，卷的管理能力就可以更加产品化。支持分布式缓存能力。它可以充分利用计算机的物理资源和本地磁盘，通过集群内的计算节点和网络进行利用。目前我们了解到，商业版有提供这个功能，但社区版也还没有。挂载点热更新能力。在不同的场景下，用户需要不同的挂载参数，但是卸载和重新挂载的操作太重，会影响用户的业务。用户可以接受短暂的不可读取或者读取中断，但是不能重启业务容器或中断算法或策划程序。我们正在内部调研和开发这个能力。</p><p></p><p>编著注：目录配额需求在1.1Beta中已经实现了，详情请大家关注下周的发版信息。分布式缓存和挂载点更新能力，目前商业版可以提供，这两个功能也在社区版的规划当中。</p><p></p><p>直播回顾：https://www.bilibili.com/video/BV1Fo4y1V7Ka/</p><p></p><p>作者简介：</p><p>洪晨</p><p>之江实验室高级工程专员，负责计算侧存储的架构和演进</p>",
    "publish_time": "2023-07-04 12:15:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "承接百亿保单的新基建，无界山到底是什么山｜InfoQ 《一探到底》",
    "url": "https://www.infoq.cn/article/ZhIJE05sJgdxkPdYci37",
    "summary": "<p>新基建的推动使得数字化转型成为各行各业的必然趋势，众安保险也从其中看到机遇，把握机遇并通过加强创新，探索出共建共赢的新模式。《一探到底》众安之行的第三期，带大家探访保险行业的一座“神山”——无界山的前世今生，并畅谈如何在新基建时代通过技术创新实现合作共赢。</p>",
    "publish_time": "2023-07-04 12:29:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "技术解读：Cube安全容器高并发低延时实践之路",
    "url": "https://www.infoq.cn/article/RgWPqx5HXvFX5gVVCIPT",
    "summary": "<p>随着云计算技术的发展，Serverless凭借其免运维、按量付费和弹性伸缩等特点逐渐成为热门话题。然而，传统的虚拟化计算无法满足Serverless的需求，如高并发创建能力、冷启动速度等。为此，腾讯云技术团队推出了Cube轻量虚拟化产品，通过对管控流程、KVM、主机OS、VMM、子机OS等进行全链路精简和优化，实现高性能、低开销、高密度的运行环境。Cube还针对快照方案进行了深度优化，大幅提升启动速度，从而更好地满足Serverless场景的需求。</p><p></p><h1>背景</h1><p></p><p>随着云计算技术的持续演进，具有免运维，按量付费，弹性伸缩等特性的serverless逐渐成为行业关注的热点。从Serverless技术最有代表性的FaaS产品现网运营数据来看，有以下特点：</p><p></p><p>1） 单实例生命周期短， 平均执行时间50ms， P95小于100ms</p><p>2） 并发创建能力要求高， 资源流转快， 每天近千万次的资源流转（创建/销毁）</p><p>3） 冷启动速度要求高， 要求百毫秒级的冷启动时延</p><p></p><p>在通用的虚拟化计算环境下，无法满足Serverless的以上特点， 根本原因是通用虚拟化的实例创建过程会涉及到和装箱， VPC、CBS、安全组等多个子系统的交互，实例创建往往需要等待数十秒的时间，无法满足Serverless对冷启动和高并发的诉求。 而实例本身的创建销毁时间远远大于实例本身的执行时间， 会导致大量的算力浪费。</p><p></p><p>在实际的运营中，通用虚拟机架构下，为了满足Serverless的运营指标诉求， 通常采取储备资源池的方案， 也就是提前创建好一批虚拟机实例， 直接用提前创建好并已运行的虚拟机实例来满足客户高并发高资源流转的诉求。 这个方案通过储备资源池来平衡客户高并发高资源流转的需求与平台低并发资源流转慢的矛盾， 但储备资源池会给平台带来比较大的呆滞成本以及很大的算力浪费。</p><p></p><p>为了让底层基础设施更加匹配Serverless的诉求， 腾讯云技术团队对管控流程、KVM、主机OS、VMM、子机OS进行全链路精简和优化， 设计了Cube轻量虚拟化产品。Cube&nbsp;方案能提供高性能、低开销、高密度运行环境，涵盖从主机OS、虚拟化、虚拟机OS整套优化。</p><p></p><p>此外，Cube还针对快照方案进行了技术栈深度优化，引入多项关键功能，将启动速度进一步提升，使得轻量级场景下的虚拟机运行更加迅速。通过这些优化，Cube在有限空间内，能提供高性能、低开销的解决方案。</p><p></p><h1>整体架构</h1><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c6/b3/c653d4a8656f8291ff53314bd28c64b3.png\" /></p><p></p><p></p><h1>技术挑战</h1><p></p><p></p><h2>虚拟机管控流程</h2><p></p><p>通用虚拟机的创建流程涉及与虚拟化，网络，存储等诸多IaaS基础设施的交互，这是制约通用虚拟化高并发高资源流转能力的关键瓶颈。</p><p></p><p>Cube轻量虚拟机创建流程完全与IaaS基础设施解耦， 将整个虚拟机的创建变成一个单台物理机内可以闭环的操作。</p><p>&nbsp;</p><p>具体实现上，Cube母机初始化时与IaaS基础设施交互， 得到可与IaaS交互的网络资源（VPC IP）与存储资源（CBS）， 并在内部闭环管理网络和存储资源的分配。</p><p>网络上，Cube在VPC网络之上， 实现了一个单机维度的Cube网络， 而且这个网络可以通过CubeGW实现和VPC网络的互通；存储上，Cube将本地盘和CBS云盘资源进行了切割管理，将只读存储通过virtiofs的方式暴露给虚拟机， 可写存储结合写时复制的技术以virtio-blk的方式暴露给虚拟机。</p><p></p><p>通过上述存储，网络等虚拟机配套资源全部在母机范围内的闭环管理，从管控流程上完全消除了外部基础设施的影响因素。</p><p>&nbsp;</p><p></p><h2>虚拟化底座（KVM）</h2><p></p><p>KVM作为虚拟化的底座，在通用VM场景中已经经过千锤百炼，然而在轻量虚拟化产品高并发超买的场景，却暴露出来了很多性能和时延相关问题，典型的如高并发操作下lock contention导致的时延。</p><p>首先是irqfd操作引入接近100ms时延。irqfd是KVM提供给VMM的将eventfd绑定到VM中断的一个feature，用户态的VMM通过wirteirqfd，KVM在内核层面将对应中断注入到VM中去，这个feature通常被virtio设备使用，比如virtio设备的队列事件，在CUBE产品中因为irqfd注册引入了60ms以上时延。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/33/33/337efa06b0f57dee31374abfeeab1233.png\" /></p><p></p><p>&nbsp;</p><p>经过对KVM中irqfd注册流程分析，腾讯云技术团队针对非直通设备（CUBE产品）场景彻底消除了这部分时延影响，而针对更为棘手的直通设备场景，积极在社区讨论终极解决方案。</p><p>&nbsp;</p><p>另外一个典型时延问题是，nx-lpage-recovery内核线程创建会导致创建VM引入100ms的抖动，是因为高并发场景下这个内核线程启动引入了同步等待，放大了VM创建的时延。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/70/3f/702f22624720502b35a98ee004cf113f.png\" /></p><p></p><p>&nbsp;</p><p>经过调查，腾讯云技术团队发现nx-lpage是KVM是为了缓解ITLB multihit CPU漏洞的软件缓解方案，而nx-lpage-recovery内核线程则是针对这组缓解方案造成性能损失的补偿机制。且不说这个补偿方案的效果，CUBE产品环境中，该方案都不会触发这个问题（需要VM使用大页），再者新版的Intel CPU已经fix了这个硬件漏洞，看起来很普通的一个内核线程启动，在场景中竟然有这么大副作用，对KVM持续优化深入到每一个细节。</p><p></p><p>除了这些问题，针对高并发超买场景降低开销和时延，团队还对KVM的一些参数进行了调整和调优，比如halt_poll_ns等。</p><p></p><h2>轻量化VMM</h2><p></p><p>社区近些年兴起基于RUST的轻量级VMM，相比QEMU简化了设备模型，更小的内存和运行开销，在CUBE产品中，腾讯云技术团队基于cloud-hypervisor构建VMM组件（cube-hypervisor），是因为其在保持了轻量化的同时，有较为完备的现代VMM具备的基础功能，如热插拔/TDX等。</p><p></p><h3>存储架构调整</h3><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/39/a8/394681eeac5296yyyy878d4cbcbd8ba8.png\" /></p><p></p><p>&nbsp;</p><p>在产品架构上为了适配容器场景，腾讯云技术团队对VM存储进行读写分离，使用更为灵活的virtiofsd为容器提供rootfs目录，使用传统的disk为容器提供临时数据写盘。为了达到极致的性能，对viriofsd进行了native改造，改造后的virtiofs从独立进程变成了CH的一组线程，处理上优化了vhost逻辑，可以像普通virtio设备一样直接操作后端fs。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/50/7b/50f26ed1422bc6325b1abf4def272d7b.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>快照方案提速</h3><p></p><p>在技术演进方面，腾讯云技术团队对VM冷启动做了多项优化，如通过并发内核加载加速VM启动，以及调优启动过程中的阻塞行为，但仅仅通过调优只能将VM交付时间缩减到100ms左右，无法达到对启动时延的极致要求，而在实际场景中，不同VM的内核和rootfs都是一致的，针对这个高度重复的动作，是否有办法做到一次性优化么？答案是有的，这里，腾讯云技术团队选择了VM快照技术。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0f/89/0fe1fc3d300f41c57cbdd1722c047289.png\" /></p><p></p><p>&nbsp;</p><p>快照技术原本是VMM提供的对VM快照的保存和恢复，快照保存时，将VM暂停下来，将VM所有状态保存到快照文件中，快照恢复时，基于快照文件恢复VM，恢复完成之后继续VM运行，快照技术语义上支持1 to 1的VM状态恢复，且对VM后端设备有一致性要求（磁盘/网卡）。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8c/51/8c00d510c68a7ca0e25ec30394600451.png\" /></p><p></p><p>&nbsp;</p><p>首先，需要对快照技术进行1 to n的VM状态恢复改造，实现VM内部状态和后端设备无关，并可以将保存的旧设备状态恢复到新设备。实际实现中，通过替换后端设备，以及基于快照中设备状态对设备进行重新初始化，实现了快照恢复过程中后端设备无感替换。</p><p></p><p>经过对快照技术持续优化，最终做到将VM交付时间从100ms+降低到20ms以内，同时为单个VM减少了20MB+内存（单机1000个VM节省20G内存）。</p><p></p><h3>网络方案重构</h3><p></p><p>资源池化是降低并发资源访问的法宝，在产品架构上，通过池化解决一众资源依赖问题，如磁盘池化，CGROUP池化，以及网络设备池化，网络方面更是结合ebpf实现快速转发路径，已经努力将网络层面的开销降到了最低，然而高并发测试总是能够发现彩蛋，这一次是TAP设备，实测发现，VM在TAP设备初始化上耗时50ms以上，MAX甚至超过100ms。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/ae/dc/aedabcb5f4fa79d2cf99629235888fdc.png\" /></p><p></p><p>&nbsp;</p><p>VM创建过程需要对TAP设备做初始化，然而这个高并发的操作在母机内核触发了全局锁冲突，造成大量时延。很显然解决这个问题需要深入母机内核TAP做深度优化，然而有没有跟简单的办法呢，是否可以避免重复的TAP设备初始化动作呢？腾讯云技术团队对TAP网络方案进行了重新设计，将TAP设备的初始化放在了TAP池化初始化中，而VM的网络初始化只需要从池子中申请一个对应的资源就可以了，经过对方案的不断调优，成功将VM启动过程中TAP设备初始化降低到了1ms级别。</p><p>&nbsp;</p><p></p><h2>母机OS</h2><p></p><p>高并发场景下，高频的创建销毁会带来母机全局资源的锁竞争，导致性能的波动和不可预期性。</p><p>Cube为了保证高并发场景下冷启动时延的可预期性， 通过以下技术手段来优化</p><p>计算，网络，存储资源的池化对于无法池化的资源，尽量减小对全局资源的冲击Mount/umount</p><p>在mount/umount过程中都需要全局的命名空间锁来保证关键路径的串行操作，在安全容器环境的构建中不但需要通过overlayfs mount创建容器的rootfs，还需要各种形式的bind mount来实现hosts，dns等个性化配置。Cube通过把这种个性化的bind mount转换成overlayfs的一个层次（layer）的方式使mount/umount次数减少到最低。</p><p>进程创建/销毁</p><p>在进程的创建，销毁过程中也需要全局的task锁来确保关键路径的串行操作，&nbsp;为了确保进程创建的时延有保证，cube技术团队将进程销毁的优先级调低， 并引入排队机制串行执行进程的销毁操作。同时通过进程合并等手段减少虚拟机创建需要创建的进程数目。</p><p></p><p>除了高并发的优化，针对高密场景的资源复用，cube也对虚拟机内核，虚拟机镜像，以及容器镜像的复用做了一些工作</p><p>虚拟机镜像通过pmem设备方式在母机层面在所有虚拟机之间进行只读共享，子机通过DAX方式直接访问母机内存容器镜像的读写存储分离，只读部分通过virtio-fs暴露给虚拟机，并实现容器镜像和page&nbsp;cache在母机层面的复用； 可写部分通过virtio-blk暴露给虚拟机， 可写块存储使用空洞文件减少实际磁盘使用量的同时， 通过reflink和写时复制技术实现磁盘块的按需使用</p><p></p><h2>子机OS</h2><p></p><p>通用虚拟机的设计目标是提供一个通用的计算环境，要求子机OS（内核与操作系统）的功能足够丰富与灵活，但在Serverless场景中，过多的功能反而变成了一种负担，牺牲了极速的启动速度，也额外耗费了太多的资源。Cube为了满足Serverless极致的用户体验，对子机OS的所有功能模块进行了深度的裁剪和优化。</p><p>&nbsp;</p><p>子机镜像（虚拟机rootfs）层面：</p><p>裁剪掉所有不需要软件包，只保留极少数关键库和命令，镜像大小控制在128MB以下。使用Cube-agent作为init替换庞大的systemd体系，系统启动时只进行少量必须的初始化操作。</p><p>子机内核层面：</p><p>关闭所有不需要的模块和功能，减少内核代码段和数据段的大小。调整内核各个子系统的hash表大小，减少额外内存消耗。调整预留内存的默认计算算法，提供更多资源给业务进程。</p><p></p><p>经过裁剪优化，1C128MB的规格下，相比通用虚拟机秒级启动速度，Cube虚拟机启动时间降至80ms以下，启动时内存消耗也降至20MB左右。</p><p>&nbsp;</p><p></p><h1>总结</h1><p></p><p>目前在大量背景负载情况下，50并发启动Cube轻量虚拟机的时延可以控制在100毫秒以下的水平。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ec/09/ec97cc53f782fa615c19d7387675a209.png\" /></p><p></p><p>&nbsp;</p><p></p><h1>回馈社区</h1><p></p><p>值得一提的是，腾讯云技术团队在优化Cube在高密高并发场景下性能的同时也发现并修复了其他开源组件长期以来没有被发现的问题。</p><p>&nbsp;</p><p>golang &nbsp;runtime相关：</p><p>在高并发操作tap 设备时会触发了 go runtime epoll 对象复用的bug。</p><p>这是一个go net epoll 模型的重大远古缺陷，已提交社区并推动修复。</p><p><a href=\"https://github.com/golang/go/issues/59545\">internal/poll, runtime: got `not pollable` err when reading fifo or socket · Issue #59545 · golang/go · GitHub</a>\"</p><p>&nbsp;</p><p>kata container相关：</p><p>容器控制流程hang住的问题反馈与修复</p><p><a href=\"https://github.com/kata-containers/kata-containers/issues/6059\">runtime: all APIs are hang in the service.mu · Issue #6059 · kata-containers/kata-containers · GitHub</a>\"</p><p><a href=\"https://github.com/kata-containers/kata-containers/issues/6171\">runtime: unix socket file leak · Issue #6171 · kata-containers/kata-containers · GitHub</a>\"</p><p>&nbsp;</p><p>cloud-hypervisor 相关：</p><p>cloud-hypervisor 清除 virtiofs dax mapping时offset设置错误。&nbsp;<a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5235\">virtio-devices: vhost-user: fs: reset offset when to remove the whole mapping by HowHsu · Pull Request #5235 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"备初始化时Breadth First Traversal 设备树实现不当，存在多余的内存拷贝开销。</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5428\">https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5428</a>\"</p><p>支持网络设备配置offloading能力feature</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5029\">vmm: add configuration for network offloading features by zhuangel · Pull Request #5029 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"</p><p>支持block设备latency统计</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5044\">https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5044</a>\"</p><p>解决vcpu异常退出造成死循环问题</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5211\">vmm: properly set vcpu state when thread exited by zhuangel · Pull Request #5211 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"</p><p>优化virtio console信号线程</p><p><a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor/pull/5240\">Do not start signal thread is there is no need. by zhuangel · Pull Request #5240 · cloud-hypervisor/cloud-hypervisor · GitHub</a>\"</p><p>&nbsp;</p><p>virtiofs相关：</p><p>在cache=never 模式下，virtiofs前端驱动的private mmap的page cache存在一致性问题。</p><p><a href=\"https://lore.kernel.org/linux-fsdevel/20230509080128.457489-1-hao.xu@linux.dev/\">[RFC PATCH] fuse: invalidate page cache pages before direct write - Hao Xu</a>\"</p><p>在cache=never 模式下，对shared_mmap的缺乏支持。</p><p><a href=\"https://lore.kernel.org/linux-fsdevel/20230505081652.43008-1-hao.xu@linux.dev/\">[PATCH] fuse: add a new flag to allow shared mmap in FOPEN_DIRECT_IO mode - Hao Xu</a>\"</p><p>在cache=always 模式下目录 page cache失效，导致readdir性能变差。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/172\">passthrough: add KEEP_CACHE flag for directory file when cache=always (!172) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>没必要的open(/proc/pid/mountinfo)操作导致的mount操作时延抖动。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/167\">passthrough: open mountinfo proc file in case we readlly need (!167) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>无效的后端读写size限制。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/161\">server: remove buffer size check for read/write (!161) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>&nbsp;</p><p>腾讯云技术团队对这些问题提出了修复方案，并将其贡献给了上游社区。此外，根据使用场景在功能上做了进一步的增强。</p><p>virtiofs设备增加了ratelimiter功能以支持QoS。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/147\">Introduce RateLimiter to virtiofsd (!147) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>virtiofsd新增加了cache=part模式，使得节省内存的同时不影响readdir操作的效率。</p><p><a href=\"https://gitlab.com/virtio-fs/virtiofsd/-/merge_requests/173\">Add a new cache policy Part (!173) · Merge requests · virtio-fs / virtiofsd · GitLab</a>\"</p><p>&nbsp;</p><p>rust-vmm相关：</p><p>解决linux-loader没有正确处理ELF align的问题</p><p><a href=\"https://github.com/rust-vmm/linux-loader/pull/128\">https://github.com/rust-vmm/linux-loader/pull/128</a>\"</p><p>解决virtio-queue没有正确处理descriptor align问题</p><p><a href=\"https://github.com/rust-vmm/vm-virtio/pull/220\">https://github.com/rust-vmm/vm-virtio/pull/220</a>\"</p><p>&nbsp;</p><p></p><h1>展望</h1><p></p><p>目前为止，Cube对底层支撑功能以及各相关组件进行了大量深入的分析，并有针对性进行优化， 真正的做到了用户按量付费，平台按需生产，并且可以满足用户的性能诉求。</p><p></p><p>技术的道路没有尽头，后续随着技术方案的不断创新和迭代，腾讯云技术团队将不遗余力地持续提高Cube的底层性能，与此同时，在用户体验，易用性等方面做更多的探索和增强。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-07-04 13:12:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "正在诞生的五种编程语言",
    "url": "https://www.infoq.cn/article/ReR4ui073IN4Qdv7dPdg",
    "summary": "<p>本文最初发表于<a href=\"https://kevin-da-silva.medium.com/5-programming-languages-we-can-see-being-born-right-now-4d30b29552e0\">作者的个人博客网站</a>\"，经原作者Kevin Da Silva授权，由InfoQ中文站翻译分享。</p><p></p><p>这个周末，我在YouTube频道上观看了大量关于编程语言的视频，并在Github上看到了很多新的编程语言正在出现。不得不说，IT市场已经变得非常疯狂，但是在大多数公司中，开展工作时都在使用相同的繁琐技术，而不是使用新鲜和时髦的技术。作为一种爱好，编程是一件非常神奇的事情，看到有如此多的语言和工具，这实在太酷了。看着每天都有语言在Github上诞生，这更是不可思议。</p><p></p><p>所以这篇文章的意图是列出一些还没有到1.0版本的语言，但它们绝对值得我们关注。</p><p></p><blockquote>请注意，本文中的“正在诞生”指的是仍低于1.0版本的语言，但这不应该是妨碍你基于它们构建特性的原因，因为一般来说，它们和许多主流编程语言一样完备。</blockquote><p></p><p></p><p>但首先，我们回顾一下现在使用的语言，市场上使用的大部分语言都是由于大的科技企业想要锁定其他的公司，并便于向他们出售商品而被选中的，比如微软的.NET和Typescript、Sun/甲骨文的Java，以及其他公司的杀手级应用（如让JS依然光芒万丈的浏览器、WordPress的PHP以及数据科学中的Python）。</p><p></p><p>然后，硬件发生了变化，处理器上增加了更多的内核，使用以前那些流行的语言就意味着要抛弃处理能力或在软件中增加不安全的代码层，所以新的一批语言崛起了，试图克服这个问题，举例来说Elixir、Rust、Clojure、Go、Scala等。</p><p></p><p>但是，就业市场并不关心计算能力的浪费，仍然一如既往地使用原有的东西。在我看来，唯一相对流行起来的是Go，不仅仅是因为Go是一种相当好的语言，还因为谷歌的影响/声誉（该语言非常棒，有一个非常好的并发模型，但前文提到的其他语言也有这样的并发模型）。</p><p></p><p>而现在，有大量的语言正在诞生，以解决特定主题的问题，如下是我们的名单：</p><p></p><p></p><h2><a href=\"https://grain-lang.org/\">Grain</a>\"</h2><p></p><p>Grain是一种函数式语言，在我看来，它是JavaScript和一点ML的混合体，专注于编译成web assembly，能够在多平台上运行</p><p></p><p></p><h2><a href=\"https://ziglang.org/\">Zig</a>\"</h2><p></p><p>Zig是一门系统语言，但总的来说比Rust简单（也没有那么安全），Zig没有C和C++的影子，如果你不考虑上述三种语言中的任何一种，它是一个合适的选择。</p><p>关于<a href=\"https://bun.sh/\">Bun</a>\"有一个热议的话题，它是基于Zig构建的JavaScript运行时，比Node和Deno更快。</p><p></p><p></p><h2><a href=\"https://vlang.io/\">V</a>\"</h2><p></p><p>V是一门通用的编程语言，也可以作为系统语言，其网站说它非常简单，你可以在一个周末学会，它还说Go程序员会对该语言非常熟悉，因为V语言在很多方面借鉴了Go。</p><p></p><p>V语言的网站也有一些关于磁盘空间和编译时间的基准测试结果，看起来非常有吸引力。</p><p></p><p></p><h2><a href=\"https://factorcode.org/\">Factor</a>\"</h2><p></p><p>迄今为止提到的所有语言中，我认为Factor是最古老的。它出现在2003年，但它目前的版本还不到1.0，所以我将这个语言列为诞生中的语言。</p><p></p><p>Factor是一种栈语言，意味着每个元素和函数调用的行为都类似于栈：</p><p><code lang=\"null\">[ 4 ]-----[ + ]-----[ 3 ]输出错误，因为“+”函数要基于两个元素进行调用，目前栈上只有一个数字[ + ]-----[ 5 ]-----[ 4 ] -----[ 3 ]输出9，因为“+”函数会应用于之前的5和4元素，最终生成的栈为[ 9 ]-----[ 3 ]</code></p><p></p><p></p><h2><a href=\"https://gleam.run/\">Gleam</a>\"</h2><p></p><p>Gleam是面向Erlang虚拟机的类型化语言，Gleam的语法对于类型化语言来说非常优雅和简单。如果能看到Gleam像Elixir一样成功，那就太酷了。</p><p></p><p>这只是一些可供我们尝试的新语言，但还有很多其他的语言，我相信肯定会有足够的材料来写这篇文章的第二部分。</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/HdhHwuPQk4FCdPBpmdlP\">Azure CTO： Rust 已登陆 Windows 11 内核</a>\"</p><p><a href=\"https://www.infoq.cn/article/GFfVLVpkIGOcKYB85Opb\">比 Python 快 35000 倍！LLVM&amp;Swift 之父宣布全新编程语言 Mojo：编程被颠覆了</a>\"</p>",
    "publish_time": "2023-07-04 14:05:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "上架不到一周，Mozilla 的 AIGC 助手就被骂到暂停开发！最老牌的互联网企业居然做不好大模型产品？",
    "url": "https://www.infoq.cn/article/uwO9NbZlcgg7lq5DWBst",
    "summary": "<p>当地时间6月27日，Mozilla&nbsp;发布了与<a href=\"https://supabase.com/\">Supabase</a>\"合作开发的生成式人工智能助手AI Help。开发者只需在 MDN 上提出问题，AI Help 就会开始工作。但由于AI Help 频繁出错，社区批评声不断，一致反对 Mozilla 使用AI Help&nbsp;。</p><p></p><h2>错误频出，社区要求下架</h2><p></p><p>&nbsp;</p><p><a href=\"https://github.com/mdn/yari/issues/9208\">在 MDN 的 Yari 存储库上</a>\"，一位名为 Eevee 的开发人员，“MDN 现在可以自动对寻求技术信息的人撒谎。”</p><p>&nbsp;</p><p>Eevee 访问了浏览器地址栏历史记录中的第一篇 MDN 文章（针对<a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/grid\">网格属性</a>\"），在遇到的第一个代码块（语法摘要）上点击“aiterpret”，并收到以下信息：</p><p>&nbsp;</p><p></p><blockquote>grid: \"a\" 100px \"b\" 1fr;：此值将网格模板设置为两行两列。第一行的高度为 100 像素，第二行的高度为 1 个分数单位 (1fr)。这些列被命名为“a”和“b”。</blockquote><p></p><p>&nbsp;</p><p>Eevee 表示，这是严重但微妙的错误：只创建一列（更多的列需要斜杠），并且引用的字符串是区域的名称，而不是列的名称。但它是可信的，它与其他正确的属性值解释交织在一起。这尤其糟糕，因为grid 具有复杂快捷语法的属性——正是人们可能想要点击“解释”按钮的那种东西。</p><p>&nbsp;</p><p>“生成的文本似乎未经审查、不可靠、不负责任，甚至无法纠正。至少如果文本被嵌入到存储库中，它可能会受到人类监督和pull request，但据我所知，它只是在某个缓存中？似乎这个功能的构思、开发和部署都没有考虑到LLM可能会产生令人信服的胡言乱语。”Eevee 说道。“我希望 MDN 包含正确的信息，但实际上MDN 生成了一个听起来令人信服的谎言，且没有明显的纠错过程。”</p><p>&nbsp;</p><p>开发者 Lifning&nbsp;也表示，这不是偶发的单独事件，直接给出了一张截图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a84f16b4be3c47800585d974b181462b.jpeg\" /></p><p></p><p>&nbsp;还有人表示，AI Help<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615205813\">自相矛盾</a>\"、<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615200919\">错误识别</a>\"&nbsp;<a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/color_value/oklch\">CSS 功能</a>\"、<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615245134\">错误地解释</a>\"辅助功能，并且通常<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615277871\">不理解 CSS</a>\"。</p><p>&nbsp;</p><p>Eevee 帖子下边，几乎充斥着对MDN&nbsp;的批评。开发者“MrLightningBolt”表示，“由于上述原因，这种‘AI’万金油比无用更糟糕；其他示例的创建很简单。MDN 的存在会让情况变得更糟。”&nbsp;</p><p>&nbsp;</p><p>“我非常依赖这些文档。但LLM非常不可靠，所以不要使用它了，请将原来人工MDN作为开发人员文档的可信来源。”Datarocks 说道。</p><p>&nbsp;</p><p>屡次出现的错误严重损害了开发者对 MDN 的信心。Dalton Miner 表示，“我使用 MDN，因为它是一个全面、准确的文档来源，没有任何废话。我看不出容易出现严重错误的 LLM 输出如何改善这一点。它极大地削弱了我对 MDN 的信心，我担心它的包容性将导致过度依赖廉价但不可靠的文本生成。”</p><p>&nbsp;</p><p>开发人员 Andi McClure<a href=\"https://github.com/mdn/yari/issues/9208#issuecomment-1615308637\">痛斥了</a>\"新的 AI Help功能，甚至因此表示不再使用MDN，“除了道德、法律和声誉问题之外，实际上，在我确信在所有‘AI’集成和内容已从MDN 中永久删除之前，我无法相信或者出于任何目的使用 MDN。”</p><p>&nbsp;</p><p>Ruby Juric 对 Mozilla 表达了强烈的不满，“Mozilla，如果您试图削弱社区对您作为 MDN 有效管理者的信任，那么您的工作已经非常出色了。我们要求您做的就是接受社区反馈——我认为在这个高曝光的问题中，几乎完全没有支持此功能的声音，社区足够一致——请丢掉这个没人想要的东西。我们不希望有‘信息可能不正确’的免责声明。我们一开始就不想要错误信息。”</p><p>&nbsp;</p><p>Ruby Juric 表示，“为什么社区反馈被忽视？为什么 Mozilla 认为这个功能如此重要，以至于尽管社区几乎一致反对，但Mozilla仍保留它？”</p><p>&nbsp;</p><p>Mozilla 没有立即回应。然而，MDN 核心维护者 sideshowbarker 似乎已经注意到了这一混乱局面。</p><p>&nbsp;</p><p></p><blockquote>这个变化似乎是两天前在e342081上发布的——没有任何相关的问题，只有<a href=\"https://github.com/mdn/yari/pull/9188\">#9188</a>\"上的一个PR，其中绝对没有任何讨论或任何类型的背景信息。&nbsp;在我看来，这一点上，Mozilla太自作主张了，而且没有给任何其他MDN利益相关者任何提示。(我可能错了，我离开了一段时间——不幸的是，上个月我的很多时间都花在了其他地方，这使我无法做我通常会做的MDN工作。)&nbsp;无论如何，这个“用AI来解释”是一个非常糟糕的想法，原因很明显。在这一点上，我至少可以承诺，我个人将在内部尽可能高紧迫地让内部高度重视这个问题（在发表这篇评论之前，我已经开始这样做了），以便把它尽快删除。</blockquote><p></p><p></p><p>&nbsp;</p><p>后来，MDN表示“AI Help”功能已<a href=\"https://github.com/mdn/yari/commit/1bf285612ca2a2741795916fbe7fd2549e6b0013\">暂时</a>\"停止，团队表示稍后会继续研究问题。</p><p>&nbsp;</p><p>据悉，AI Help 的灵感来自<a href=\"https://supabase.com/blog/chatgpt-supabase-docs\">Supabase Clippy</a>\"，而 Supabase Clippy 又受到<a href=\"https://www.theregister.com/2013/07/12/microsoft_clippy_lives/\">Clippy</a>\"的启发。有趣的是，Clippy 的设计师Kevan Atteberry自己都不好意思将其纳入自己的作品集。</p><p>&nbsp;</p><p></p><h2>Mozilla 的入局策略</h2><p></p><p>&nbsp;</p><p>今年三月，Mozilla 宣布投资 3000 万美元成立了名为 Mozilla.ai 的初创公司，通过社区驱动来构建一个值得信赖、独立和开源的 AI 生态系统。</p><p>&nbsp;</p><p>“我在值得信赖的人工智能领域工作了近五年，一直感到兴奋和焦虑交织，”Mozilla执行总裁兼 Mozilla.ai 负责人Mark Surman对外媒表示，“大型科技公司人工智能的快速发布没有什么不同。真正令人兴奋的新技术正在出现——新工具立即激发了艺术家、创始人等各种各样的人尝试新事物。但当意识到几乎没有人建立防护栏时，你就会感到焦虑。”</p><p>&nbsp;</p><p>过去几个月涌现出来的模型对现实世界的影响令人担忧，比如在发布时，ChatGPT 可能会被<a href=\"https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/\">提示</a>\"编写恶意软件等。很多模型的创建者表示正在采取措施遏制滥用行为，但 Mozilla 认为他们做得还不够。</p><p>&nbsp;</p><p>因此，Surman 强调这家公司的使命不是随便构建什么人工智能，而是构建开源且“值得信赖”的人工智能。</p><p>&nbsp;</p><p>Surman 还将 Mozilla.ai 描述为半研究公司、半社区——一家致力于帮助创建值得信赖、独立的开源 AI 堆栈的初创公司。Bard 支持的谷歌和ChatGPT支持的Bing 保持封闭，即使长期以来一直承诺成为开放生态系统的 OpenAI，也开始更少地分享有关其模型和训练数据的信息。Mozilla.ai 希望改变这一点。</p><p>&nbsp;</p><p>根据报道，最初，Mozilla.ai 的首要任务是建立一支由大约 25 名工程师、科学家和产品经理组成的团队，按照 OpenAI GPT-4的路线开发“值得信赖”的推荐系统和大型语言模型。但该公司更广泛的目标是建立一个由拥有共同愿景的联盟公司和研究团体（包括 Mozilla Ventures 支持的初创公司和学术机构）组成的网络。</p><p>&nbsp;</p><p>Mozilla.ai 目前主要在开发一些工具，例如让用户询问人工智能聊天机器人答案背后的来源。该公司还努力创建能让用户更好地控制内容推荐的系统，例如基于 Mozilla 现有研究针对个人或社区做价值优化的推荐系统。</p><p>&nbsp;</p><p>ChatGPT、Bing等已经开始改变互联网的运作方式。Mozilla 已经跟进这件事有一段时间了，比如2020年发布了被广泛引用的“<a href=\"https://blog.mozilla.org/en/mozilla/mozillas-vision-for-trustworthy-ai/\">创建值得信赖的人工智能</a>\"”白皮书，但大部分时间 Mozilla 都对产品持观望态度。</p><p>&nbsp;</p><p>Mozilla 长期以来一直是市场上规模虽小但有影响力的参与者，尤其是 Firefox。虽然Firefox不再是最流行的网络浏览器，但它的流行足以让 Mozilla 有资格加入这场竞争。</p><p>&nbsp;</p><p>Mozilla 喜欢谈论建立更广泛的社区和生态系统，但很明显该公司也将人工智能视为 Firefox 未来的一部分。Mozilla <a href=\"https://www.theverge.com/2023/2/14/23598344/mozilla-firefox-ceo-mitchell-baker-microsoft-edge-bing-google-apple-ai\">首席执行官Mitchell Baker</a>\"表示，我们正在进入一个真正变革的时刻，这可能有助于 Firefox（或其他一些浏览器）赢回一些市场份额。事实上，Bing确实因为接入ChatGPT而用户数量大增。</p><p>&nbsp;</p><p>入场较晚的Mozilla 能否凭借社区、开放这一属性赢得一席之地还要拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://github.com/mdn/yari/issues/9208\">https://github.com/mdn/yari/issues/9208</a>\"</p><p><a href=\"https://www.theregister.com/2023/07/03/mozilla_developer_network_adds_ai/?td=rt-3a\">https://www.theregister.com/2023/07/03/mozilla_developer_network_adds_ai/?td=rt-3a</a>\"</p><p><a href=\"https://techcrunch.com/2023/03/22/mozilla-launches-a-new-startup-focused-on-trustworthy-ai/\">https://techcrunch.com/2023/03/22/mozilla-launches-a-new-startup-focused-on-trustworthy-ai/</a>\"</p>",
    "publish_time": "2023-07-04 14:05:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从Ruby到Node：重写Shopify CLI，提升开发体验",
    "url": "https://www.infoq.cn/article/pdDcJCrLarGjmKGfcQDL",
    "summary": "<p></p><p>本文最初发布于 Shopify 工程博客。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cd/cd087429bd124727f5cef2be191c535a.png\" /></p><p></p><p>Shopify CLI（命令行界面）是开发人员在 Shopify 平台上构建和部署 Theme、App、Hydrogen 店面时的重要工具。它提供了按照最佳实践创建新项目的工作流，实现了与开发平台的集成，并可以将产品工件分发给商家。我的团队，即 CLI Foundations，负责为设计和构建 Shopify CLI 的最佳实践和核心功能打基础。我们知道，开发人员在开发 Shopify App 时会大量用到终端，而他们使用 CLI 时并不总是能够获一致而愉快的体验。因此，我们开始使用 Node 彻底重写 Shopify CLI 2（那原本是用 Ruby 编写的），并在去年夏天推出了 Shopify Editions。</p><p></p><p>在这篇博文中，我将介绍下我们团队之前为什么做出了重写的决策以及当时所做的权衡，我们在这个新的迭代中所遵循的原则，以及我们后续要克服的挑战和探索的想法。</p><p></p><p></p><h2>语言决策回顾</h2><p></p><p></p><p>在 Shopify CLI 之前，Theme 开发人员用的是我们的另一个 CLI——ThemeKit。我们从 2014 年 10 月就开始维护它。它是用 Ruby 编写的，基于我们在内部 CLI 和服务中使用的一些 Ruby gems 构建，诸如 cli-kit、cli-ui 和 theme-check 等。</p><p></p><p>当我们在 2018 年 12 月开始开发第一个 Shopify CLI 来帮助 App 开发人员时，考虑到我们已有的 Ruby 资源和知识，选择 Ruby 是明智的。用户需要一个全局的 Ruby 安装才能使用 CLI，但我们通过为所有受支持的操作系统（Windows、Linux 和 macOS）提供安装程序解决了这个问题。2020 年 12 月，我们将 ThemeKit 合并到 Shopify CLI 中，迈出了将所有开发集中在一个 CLI 中的第一步。</p><p></p><p>2020 年 6 月，我们添加了 UI 扩展，让开发人员可以使用自己的 UI 扩展平台的某些区域，CLI 开始依赖 Node 工具来转译和打包扩展代码。系统要求越来越高，这不是用户所希望的。不过，生态系统正朝着用编译式语言（如 Go 和 Rust）实现 JavaScript 工具的方向发展，因此我们希望可以摆脱对 Node 的依赖。但这种事情并没有发生。尽管像 ESBuild 这样的工具（我们用于打包扩展）是可移植的二进制文件，但它们的可扩展性依赖于在 Node 运行时上动态求值的插件。</p><p></p><p>此外，Hydrogen 团队已经在 Node 上构建了一些工具，他们开始考虑构建一个新的 CLI，而不是将 Hydrogen 工作流构建到 Shopify Ruby CLI 中，这样他们的用户就不需要在自己的系统中安装 Ruby 运行时。Hydrogen 开发人员希望 npm install 命令能够解析他们在项目中需要的所有依赖项。如果将 Ruby 作为一个依赖项，这种思维模式就会被打破，他们就很容易遇到 CLI 因为需要额外的步骤而拒绝运行的问题。另建一个 CLI 会破坏我们始于将 ThemeKit 合并到 CLI 的统一工作。这可能会导致平台不同区域的 CLI 体验不一致。</p><p></p><p>最后但同样重要的是，Shopify 越来越依赖于 Web 技术和标准，其中 JavaScript 和 Node 运行时在资源、工具和知识方面更有优势。</p><p></p><p>所有这些都促使我们思考 Ruby 是否是最适合 CLI 的语言，所以我们回顾了这个决策。我们需要一种技术：</p><p></p><p>系统要求尽可能少（例如，不需要安装多个运行时）；让我们能够提供一流的开发体验；内部团队很容易做出贡献。</p><p></p><p>最终，我们决定用 TypeScript 重写 CLI，以便在 Node 运行时上运行。</p><p></p><p></p><h2>从 Ruby 迁移到 Node</h2><p></p><p></p><p>在 Shopify 使用的所有编程语言中，Ruby 是大多数开发人员都熟悉的语言，其次是 Node、Go 和 Rust。使用它们都可以构建出一流的开发体验，这要归功于生态系统提供的丰富软件包解决了常见的问题。从这些选项中，Go 和 Rust 都可以轻松地发布运行时不依赖运行时的静态二进制文件。这一点，Node 和 Ruby 也可以通过将源代码和运行时依赖项（又名 vendoring）打包在一起来实现，但设置更复杂，并且可能有一些操作系统不支持。</p><p></p><p>我们选择 Node 有几个原因。Go 和 Rust 允许分发静态二进制文件，但代价是 Shopify 的人由于不熟悉语言很少能做出贡献。这并不理想，因为 Shopify 希望内部团队可以为 CLI 贡献新的想法。我们只能选择 Ruby 或 Node。</p><p></p><p>在构建 CLI 方面，Node 有一个与 Ruby 不同的特性：它的模块系统和它所支持的可扩展性。与 Ruby 不同，Node 的模块系统允许同一个传递包有多个版本，而且不会相互冲突。这就让我们可以构建一个模块化的架构，将平台的不同功能域封装在 NPM 包中，而它们都基于一个包含共享功能的包构建。需要注意的是，虽然 Hydrogen 和 App 开发人员只需要一个运行时（Node），但 Theme 开发人员现在还需要两个：Ruby 和 Node。不过，我们已经开始着手消除 Ruby 依赖，我们的目标是在今年晚些时候完成这项工作。</p><p></p><p></p><h2>构建卓越的终端体验</h2><p></p><p></p><p>我们做出了技术决策，但我们还得做一些最佳实践、代码架构、模式和约定方面的决策。这是对我们从不同团队习得的经验和我们构建 Ruby CLI 的经验的一次综合运用。我将与大家分享我们在构建卓越的终端体验的过程中对我们影响最大的 7 个决定。</p><p></p><p>1. 为实现一致性奠定基础</p><p></p><p>Ruby CLI 的贡献一致性较差，而且是松耦合的（与我们所期望的高度一致的松耦合状态相反），导致内外部分化，进而导致了糟糕的体验。在 Node 版本中，我们必须做一些不同的事情。我们需要一种方法来使贡献保持一致。我们通过：</p><p></p><p>代码模式：建模命令的业务逻辑。在基于框架（如 Rails）的项目中，框架（如 MVC）通常会支持这些模式，但我们没有框架。因此，我们必须开发自己的模式和机制，并保证开发人员遵循它们。UI 模式和组件：我们在 Ink 上设计并构建了一个设计系统，以确保所有命令的体验都和 Shopify 类似。约定：便于开发人员浏览他们的项目和命令。原则：创造卓越的体验，助力开发人员取得成功。</p><p></p><p>上述内容体现在一个共享 NPM 包 @shopify/cli-kit 中，所有功能域（Theme、App 和 Hydrogen）都以它为基础构建，还有一套通用的原则，用于 CLI 和任何与平台开发人员交互的界面（例如，文档和合作伙伴仪表板）。像 ESLint 这样的静态分析工具成为我们构建自动化的平台，用于保证贡献与 CLI 的基础和方向相一致。我们实现了像 command-flags-with-env 这样的自定义规则，以支持通过环境变量设置命令标志。</p><p></p><p>下面的示例展示了一个惯用的 API，特性开发人员可以使用该 API 获取一个有效的会话，与 GraphQL API 进行交互。</p><p></p><p><code lang=\"javascript\">const session = await ensureAuthenticatedAdmin(storeFQDN)\n</code></p><p></p><p>文件 idiomatic_API_example.js，托管在 GitHub 上</p><p></p><p>2. 确保支持跨操作系统</p><p></p><p>在 MacOS 环境中开发时，确保代码更改支持 macOS、Windows 和 Linux 是一个繁琐的过程，会导致测试被跳过并出现回归。Node 运行时会使问题加剧，因为已知有些 API 在不同操作系统上的行为不一致。社区正在用 NPM 包克服这些问题。例如，pathe 规范了跨操作系统的路径。为了防止同样的事情再次发生，我们采取了三个策略：</p><p></p><p>我们在 @shopify/cli-kit 中提供了环境交互（如 IO 操作）模块，并确保它们的 API 跨操作系统兼容。如果我们检测到操作系统不兼容的情况，就会一次性修复它。我们的测试套件混合了单元测试、集成测试和端到端（E2E）测试，这些测试通过持续集成（CI）在所有支持的操作系统上运行。它会在合并前在 PR 中暴露问题。我们为与环境存在契约关系的模块（如提供 Git 交互实用工具的模块）编写集成测试。我们提供了在 MacOS、Linux 和 Windows 环境中测试更改的指令。</p><p></p><p>3. 迁移到 Monorepo</p><p></p><p>Conway 定律在我们的组织中得到了体现，我们的存储库中包含了 CLI 的不同组件（如模板和内部 CLI）。Multirepo 设置不会给用户带来什么价值，却使得内部贡献和自动化测试更加麻烦。我们决定以重写为契机改变这种局面，将所有组件放入同一个存储库 shopify/cli 中。Monorepo 设置允许跨多个包和模板原子地贡献更改。</p><p></p><p>4. 拥抱函数式编程</p><p></p><p>Ruby CLI 命令的业务逻辑是有状态的，有许多假设，并且在命令生命周期中会产生多种副作用。这增加了代码推理、贡献和测试的难度。对于 Node CLI，我们采用了不同的方法。</p><p></p><p>我们在逻辑设计时尽可能地函数化，并且只要可能，就将副作用集中在命令开始的部分。例如，命令所做的第一件事是在内存中加载和验证项目。这类似于 Web API 在接收请求时所做的事情；在将其传递到可能产生级联效应且处于无效状态的系统之前，它将对其进行验证。我们对函数范式的运用并不是教条式的，但我们的目标是把逻辑变成传递状态的函数的组合。</p><p></p><p>我们使用 JavaScript 对象和函数作为组合单元。我们默认创建对象的副本，而不是改变传递的实例。只有少数情况下，为了符合语言要求，我们才诉诸于类，如错误类型。我们引入了一个与函数组织有关的软约定，类似于模型 - 视图 - 控制器（MVC）架构模式：</p><p></p><p>模型（Model）：是用来对状态建模的 TypeScript 接口，例如 App 项目、项目配置和会话的内部表示。命令（View）：是用户进行交互的界面，用户在调用 CLI 时会传递参数和标志。它们的职责仅限于解析和验证参数及标志，并提供帮助菜单的内容。服务（Controller）：是业务逻辑的封装单元。所有命令都有一个包含命令业务逻辑的服务，有些服务没有绑定到特定的命令。</p><p></p><p>除了上面提到的，我们还有提示符，它包含通过标准输入提示用户的函数，以及将一组函数分组到特定域的实用程序。一个例子是与 Shopify GraphQL API 交互的所有函数。</p><p></p><p>5. 实现端到端测试策略</p><p></p><p>采用函数式编程和最小化副作用简化了单元测试的编写。为了定义和运行它们，我们使用了 Vitest，它在我们开始开发 Node CLI 前数周才刚刚发布。我们决定使用 Vitest，因为它完全支持 ES 模块（我们采用的模块系统）。尽管在工具成熟的过程中，最初会有一些问题，但我们对它提供的体验和与 Jest API 一对一的映射感到满意。</p><p></p><p>单元测试给了我们信心，相信我们的函数在不同的场景中完成了它们应该做的事，但这还不够——单元测试套件成功运行的结果并不意味着像“app build”这样的工作流在最近创建的项目中成功运行。因此，我们决定投资一个使用 Cucumber 的端到端测试套件，以确保各种工作流可以端到端工作。Cucumber 为我们提供了描述、运行和调试这些测试的工具和 API。你可能知道的，E2E 以维护麻烦和可能引入古怪行为而闻名。不过，在 CLI 中不会那样，因为这里的设置更简单。执行可以隔离，并将范围限定在测试场景中，防止全局状态泄漏到其他测试中导致它们表现异常。下面是我们的一个测试示例：</p><p></p><p><code lang=\"sql\">Scenario: I create and build an app with extensions\n  Given     I create an app named MyTestApp with pnpm as the package manager\n  And       I create an extension named TestPurchaseExtension of type post_purchase_ui\n  When      I build the app\n  The       The extensions are built\n</code></p><p></p><p>文件 example_test_gherkin.txt，托管在 GitHub 上</p><p></p><p>6. 利用 TypeScript</p><p></p><p>TypeScript 的类型系统和编译器让我们可以相信，代码单元和外部依赖关系之间的契约是匹配的。CLI 依赖的许多 NPM 包和 @shopify/cli-kit 中提供的模块提供了类型定义，极大地改善了对存储库做贡献的体验。例如，我们正在实现一个为 CLI 设计的新设计系统的组件，我们广泛使用 TypeScript 来确保开发人员以正确的方式使用组件。</p><p></p><p>7. 构建经过社区测试的基础</p><p></p><p>在早先一次与 Shopify 之外的 CLI 开发人员的对话中，oclif 作为一个出色的、使用 Node 构建 CLI 的工具和 API 框架出现在我们的视野中。例如，它诞生于 Heroku 的 CLI，用于支持其他 CLI 的开发。在决定使用 Node 之后，我们更彻底地研究了 oclif 的特性集，构建了小型原型，并决定基于它们的 API、约定和生态系统构建 Node CLI。事后看来，这是个好主意。</p><p></p><p>Oclif 为我们提供了用于声明 CLI 接口的惯用 API，并提供了出色的默认值自定义功能。例如，帮助文档是从代码内的声明自动生成的。此外，它还通过插件系统内置提供了可扩展性，我们已经利用插件开发了 App、Theme 和 Hydrogen。它允许我们将项目组织成边界和职责分工明确的模块。我们利用了 oclif 的 hooks API 来防止 @shopify/cli-kit 通过依赖倒置获得依赖插件的信息。插件和 @shopify/cli-kit 实现依赖于接口。</p><p></p><p></p><h2>Node CLI 展望</h2><p></p><p></p><p>Node CLI 极大地改善了开发体验：我们统一并简化了 App 开发，带来了全面的一致性，并新增了扩展功能，如函数。不过，我们还有很长的路要走，还有很多东西要学。</p><p></p><p>我们希望 Theme 开发体验与 App 和 Hydrogen 的开发体验保持一致。目前，Theme 命令仍然在 Ruby 实现中运行，为用户提供 Ruby CLI 体验，开发人员需要在他们的环境中安装 Ruby 运行时，这种情况并不理想。</p><p></p><p>我们还将继续迭代 App 开发体验，为开发人员提供一些实用的命令，用于创建、开发 App 并部署到平台。自从宣布为开发 App 提供更好的开发体验以来，我们已经收到了许多宝贵的反馈，并且正以此为基础进行迭代，如从 Multirepo 设置迁移到统一 App 模型的一些难点。在构建新想法的原型方面，我们的团队也有一个很好的基础。未来，我们会很高兴分享我们的进展。</p><p></p><p>关于可扩展性，还有很多内容可以分享，但这是后续博文的主题。</p><p></p><p></p><blockquote>Pedro Piñera Buendía 是 Shopify 的一名高级开发人员。</blockquote><p></p><p></p><p>原文链接：</p><p></p><p><a href=\"https://shopify.engineering/overhauling-shopify-cli-for-a-better-developer-experience\">https://shopify.engineering/overhauling-shopify-cli-for-a-better-developer-experience</a>\"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171257&amp;idx=1&amp;sn=7f77e37374ec697d23f6f15930c8cb7f&amp;chksm=bdb851ea8acfd8fcc1f25b225b86c3294dbcc1ebe79ef5c541a015180cece750ac2cc07ead2a&amp;scene=21#wechat_redirect\">编程已死，AI 当立？教授公开“唱反调”：AI 还帮不了程序员</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171070&amp;idx=1&amp;sn=e15ef53749fc3f1097b9141dc95087ce&amp;chksm=bdb850ad8acfd9bbb4d406ad7e29ff280229c483401ff07cf2b4e7b57938a1629a22ffde87ab&amp;scene=21#wechat_redirect\">抗拒使用 GPT-4 和 Copilot 写代码，拥有 19 年编程经验的老程序员“面试”被淘汰</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651170964&amp;idx=1&amp;sn=76048b245762bd0d382afe7d35ad6bca&amp;chksm=bdb850c78acfd9d1aadde16a310df7f1dcccfdd8572ce52d81d63c827894f675d077350b96ee&amp;scene=21#wechat_redirect\">马化腾称“收紧队形”，腾讯回应；微软发布自己的Linux发行版；OpenAI回应GPT-4 变笨 | Q资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651170943&amp;idx=1&amp;sn=1375a494b874ee28d39d9a0a38fe7f9e&amp;chksm=bdb8502c8acfd93a3545d91f70121109cdaa1523868ef50bbc8f3b1bddafbba1042777571a8c&amp;scene=21#wechat_redirect\">向量数据库？不要投资！不要投资！不要投资！</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-07-04 14:14:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java极客眼中的WebAssembly",
    "url": "https://www.infoq.cn/article/H9VRjX3X1MeXqImEwI6J",
    "summary": "<p>不少Java开发人员在面对WebAssembly一词时，首先会想到这是一种“浏览器技术”，之后可能会认为“还是归结为JVM”。毕竟浏览器内应用对他们而言是一种“史前生物”。</p><p></p><p>最近数周内，围绕WebAssembly，多项技术呈密集发布，例如<a href=\"https://www.docker.com/blog/docker-wasm-technical-preview/\">Docker+wasm技术预览</a>\"等。作为一名Java极客，我认为不应视<a href=\"https://www.infoq.cn/article/pskeeKXTSbmQa2cauwBh\">WebAssembly</a>\"为一时风尚而置若罔闻。</p><p></p><p>文如其名，WebAssembly（wasm）的确可称为“一种用于Web的字节码”。Java和wasm二者间的相似性也仅限于此。这里“wasm”是小写的，表示它是一个缩略词，而非首字母缩略语。</p><p></p><p>如果有兴趣了解我们如何定义了WebAssembly标准，<a href=\"https://evacchi.github.io/wasm/compilers/history/2022/11/23/a-history-of-webassembly.html\">欢迎翻阅我写过的一篇博文</a>\"，其中解释了来龙去脉。本文阐述的重点是，为什么说WebAssembly并不仅仅局限于Web。</p><p></p><p>首要一点，WebAssembly运行时仅是貌似JVM。其中一点，WebAssembly的长远目标，是成为适合各种编程语言的编译目标。但JVM并非如此，至少最初没有做如此考虑。</p><p></p><h2>第一个神话：JVM是一种多语言编译目标</h2><p></p><p>必须承认，JVM是最为丰富的、可互操作的语言生态系统之一。Java生态还包括了Scala，Jython，JRuby，Clojure，Groovy，Kotlin等编程语言。</p><p></p><p>但现实非常可悲，Java字节码从未真正地成为一种通用的编译目标。<a href=\"https://dl.acm.org/doi/10.1145/1711506.1711508\">不少文献资料</a>\"都对此做了清晰的阐述。例如，John Rose在“<a href=\"https://dl.acm.org/doi/10.1145/1711506.1711508\">字节码与组合选择的结合：JVM中的invokedynamic</a>\"”一文中写道：</p><p></p><blockquote>Java虚拟机（JVM）被广泛采用，可部分归因于class文件格式是可移植的、紧凑的、模块化的和可验证的，并且非常易于使用。然而，class文件在设计上仅针对Java这一种语言，用于表达其它语言编写的程序时，常常出现一些阻碍开发和执行的“痛点”。</blockquote><p></p><p></p><p>这篇文章阐释了invokedynamic操作码引入JVM中的原因和方式。事实上，引入该操作码就是专为支持使用JVM运行时的动态语言。虽然JRuby，Jython，Groovy等一些语言在运行时中添加了该操作码，并不是JVM在设计中考虑了如何支持这些语言，而是因为这些语言已经这样做了。木已成舟，只能去认可它！</p><p></p><p>换句话说，时过境迁，JVM依然未成为这些动态语言合适的编译目标。甚至可以说，以JVM为编译目标并非因为它是最好的，而是考虑到JVM的采纳度和支持情况，人们希望能与JVM互操作。正如JavaScript那样！</p><p></p><h2>GraalVM：一统各方的虚拟机</h2><p></p><p>最近<a href=\"https://www.graalvm.org/\">GraalVM项目</a>\"大行其道。该项目中包括针对例常Java字节码的JIT编译器，以及用于构建高效语言解释器的API，还新添加了原生镜像编译器。</p><p></p><p>成为“<a href=\"https://dl.acm.org/doi/10.1145/2509578.2509581\">一统所有VM的虚拟机</a>\"”，是GraalVM的最初目标之一，也就是说成为一种多语言运行时。</p><p>但Truffle并未定义多语言编译目标，而是通过Truffle API实现一种极高层级表示，进而构建基于AST的高效JIT解释器。对感兴趣的读者，可自行去深入了解“抽象语法树”（AST）。</p><p></p><p></p><blockquote>“致编程大神”：漫游编程语言的奇境，所有一切都变“神奇”。使用Truffle，的确可以为其它“适当”的字节码格式编写JIT解释器。事实上，已有用于LLVM（Sulong）的Truffle解释器。当然，LLVM位码也的确是多平台/多目标编译目标。依此类推，是否可以说GraalVM/Truffle同样支持多平台编译目标？从技术角度看，可以这么说，甚至可以说是“<a href=\"https://www.youtube.com/watch?v=hou0lU8WMgo\">完全正确的</a>\"”。但依然存在不少可商榷之处，对此本文不一一展开讨论。简而言之，LLVM位码只是作为一种编译目标，并未完全考虑作为一种跨平台的运行时语言。例如，针对不同的CPU和操作系统，LLVM可能必须要调用不同的指令。此外，不同于作为多厂商标准的WebAssembly，GraalVM和Truffle目前为止仍然是开源的、社区驱动的、单厂商实现的项目。但<a href=\"https://www.graalvm.org/2022/openjdk-announcement/\">将GrallvVM纳入OpenJDK的工作近期已经启动，并可能进入Java语言规范</a>\"。毕竟，WebAssembly只是一种得到GraalVM/Truffle支持的语言。如果要使用GraalVM，甚至可以考虑wasm！</blockquote><p></p><p></p><h2>第二个神话：WebAssembly只是另一种Stack-based VM（栈机）</h2><p></p><p>WebAssembly定义为一种<a href=\"https://github.com/WebAssembly/design/blob/main/Rationale.md\">结构化栈机</a>\"使用的虚拟指令集架构（ISA）。</p><p></p><p>上述定义中，关键在于“结构化”（structured）一词，它表明WebAssembly与JVM的工作方式大相径庭。结构化栈机在实际运行中，大部分计算使用值栈，控制流却使用块、if和循环等结构化结构表示。WebAssembly语言则更进一步，一些指令可同时使用“简单”和“嵌套”表示。</p><p></p><p>下面给出一个例子。wasm栈机中有如下表达式：</p><p><code lang=\"null\">( x + 2 ) * 3\n</code></p><p></p><p><code lang=\"null\"> int exp(int);\n    Code:\n       0: iload_1\n       1: iconst_2\n       2: iadd\n       3: iconst_3\n       4: imul\n       5: ireturn\n</code></p><p>该表达式可被翻译为下述一系列指令：</p><p><code lang=\"null\">(local.get $x) \n(i32.const 2) \ni32.add \n(i32.const 3) \ni32.mul\n</code></p><p>其中：*&nbsp;local.get在栈中加入本地变量$x；* 然后i32.const将32位整数（i32）常量2推送入栈；*&nbsp;i32.add从栈中弹出两个值，并将$x+2结果推送入栈；* 整数常量3被推送入栈；*&nbsp;i32.mul弹出两个整数值，并将($x+2)*3的i32乘法结果推送入栈。</p><p></p><p>大家应该能注意到，用括号括起来的，是含有一个以上参数的指令。上面给出的“线性化”版本的WebAssembly，在.wasm文件中直接转换为二进制表示。此外还有在语义上等效的另一种“嵌套”表示：</p><p><code lang=\"null\">(i32.mul \n  (i32.add \n    (local.get $x) \n      (i32.const 2)) \n    (i32.const 3))\n</code></p><p>嵌套表示别具特色。操作的嵌套和编写有别于JVM等字节码类型，而是类似于一种“传统”编程语言。这里所说的“传统”，就是指操作读起来类似于LISP家族中的Scheme语言。显而易见，其中使用的括号约定，就是在向Scheme致敬。当然，事出必有因。对JavaScript的神奇起源稍有了解，就一定知道JavaScript最初是在10天内写成的，而且Brendan Eich一开始的任务是去开发另一种Scheme方言。</p><p>至少对我而言，嵌套序列更有趣的细节在于，它能自然地线性化为其它版本。事实上，遵循括号表达式的优先规则，须从最内层的括号开始。例如：</p><p><code lang=\"null\"> (i32.add \n    (local.get $x) \n    (i32.const 2))\n</code></p><p>上面的例子首先获取$x，然后为常量赋值2，进而将二者相加。此后，再去处理外层的表达式：</p><p><code lang=\"null\">(i32.mul \n  (i32.add ...) \n  (i32.const 3))\n</code></p><p>对其中的i32.add求值，需对常量赋值3，并将二者相乘。这与栈机的操作顺序相同。</p><p></p><p>这里提出结构化控制流，同样是考虑了安全性，以及<a href=\"https://github.com/WebAssembly/design/blob/main/Rationale.md\">简单性</a>\"：</p><p></p><blockquote>WebAssembly栈机仅限于结构化控制流和结构化栈的使用。这一方面极大地简化了“一次通过”（one pass）验证，避免了JVM（栈映射推出前）等栈机的固定点（fixpoint）计算；另一方面，也简化了其他工具编译和操作WebAssembly代码。</blockquote><p></p><p>下面看一个例子：</p><p><code lang=\"null\"> void print(boolean x) {\n    if (x) {\n        System.out.println(1);\n    } else {\n        System.out.println(0);\n    }\n}\n</code></p><p>上述代码翻译为如下字节码：</p><p><code lang=\"null\"> void print(boolean);\n Code:\n 0: iload_1\n 1: ifeq 14\n 4: getstatic #7 // java/lang/System.out:Ljava/io/PrintStream;\n 7: iconst_1\n 8: invokevirtual #13 // java/io/PrintStream.println:(I)V\n11: goto 21\n14: getstatic #7 // java/lang/System.out:Ljava/io/PrintStream;\n17: iconst_0\n18: invokevirtual #13 // java/io/PrintStream.println:(I)V\n21: return\n</code></p><p>在如上等价的WebAssembly定义中可看到，非结构化跳转指令ifeq和goto并未出现，而是恰如其分地被语句块if...then...else所替代。</p><p><code lang=\"null\">(module\n ;; 导入浏览器控制台对象，需要将此从JavaScript传递进来。\n (import \"console\" \"log\" (func $log (param i32)))\n\n (func\n   ;; 如运行if代码块，更改为True。\n   (i32.const 0) \n   (call 0))\n\n  (func (param i32)\n   local.get 0 \n   (if\n     (then\n       i32.const 1\n       call $log ;; 应该记录'1'\n     )\n     (else\n       i32.const 0\n       call $log ;; 应该记录'0'\n     )))\n\n (start 1) ;; 自动运行第一个func。\n)\n</code></p><p>原例可在<a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/Reference/Control_flow/if...else\">Mozilla Developer Network</a>\"上查看和运行。</p><p></p><p>当然，上述例子也可线性化，形成如下的非嵌套版本：</p><p><code lang=\"null\">(module\n  (type (;0;) (func (param i32)))\n  (type (;1;) (func))\n  (import \"console\" \"log\" (func (;0;) (type 0)))\n  (func (;1;) (type 1)\n    i32.const 1\n    call 0)\n  (func (;2;) (type 0) (param i32)\n    local.get 0\n    if  ;; label = @1\n      i32.const 1\n      call 0\n    else\n      i32.const 0\n      call 0\n    end)\n  (start 1))\n</code></p><p></p><h2>更多差异：内存管理</h2><p></p><p>另一处WebAssembly虚拟机和JVM大相径庭，在于各自的内存管理，虽然难以评价孰优孰劣。大家应该知道，Java不需要开发人员显式地分配和释放内存，也无需去操心栈和堆的分配。但开发人员通常需要了解一些内存管理知识，在真正需要时能使用一些方法做显式处理，虽然现实中很少有人这么做。</p><p></p><p>事实上该特性并非语言层级上的，而是VM的工作机制。在VM层级，并没有内存处理的原语操作。堆分配原语是以JDK API的方式提供。开发人员无法缺省禁用内存管理，不能说“我不需要GC Heap，我将自己实现内存管理”。</p><p></p><p>当前，WebAssembly做法恰恰相反。大多数语言在以WebAssembly为编译目标时，的确是自行管理内存，这并非巧合。有些语言的确能做GC，但其VM并不提供GC功能，因此自身的例程在GC时必须回滚。</p><p>WebAssembly的做法是，为使用者分配一小片支持分配、释放甚至是随意移动等操作的“线性内存”（linear memory）。虽然在一定程度上要比JVM提供的功能更强大，但在使用中也需谨慎。</p><p></p><p>例如，JVM不需要开发人员显式指定对象的内存布局，结构体打包（structure packing)、字节对齐（word alignment）等内存空间优化工作已交由VM处理。但在WebAssembly中，这些工作需要开发人员处理。</p><p></p><p>这在一方面，使得WebAssembly成为手动管理内存的编程语言的理想编译目标。因为这类语言需要并期望对内存更高程度上的控制。但在另一方面，增加了语言间互操作的难度。</p><p></p><p>当前，结构和对象布局是ABI（Application Binary Interface，应用二进制接口）关注的问题。但<a href=\"https://docs.oracle.com/javase/specs/jls/se7/html/jls-13.html\">ABI对JVM开发人员都已成为昨日黄花，除了一些极为有限和需注意的例外情况</a>\"。</p><p></p><p>值得关注的是，<a href=\"https://github.com/WebAssembly/gc\">最近WebAssembly垃圾回收规范草案已向前推进</a>\"。规范草案中不仅声明了GC，而且有效地描述了结构体，以及与原始语言无关的结构体间互操作方式。尽管该草案尚未准备好，但事情是在不断发展的，多个关注问题正得到解决。</p><p></p><h2>并非局限于Web</h2><p></p><p>看到大家应该注意到，本文至此还从未提起过“Web”。</p><p></p><p>经过上文的铺垫，下面给出本文的重点，就是阐明Java极客应该关注WebAssembly。</p><p></p><p>即使你不关注前端技术，也不应将WebAssembly纯粹视为前端技术。在WebAssembly的设计和规范中，没有任何一处规定其是专门绑定到前端的。事实上，当前的大多数主流的JavaScript运行时，都能够加载和链接WebAssembly二进制文件，甚至在浏览器之外。因此，可在Node.js运行时中运行wasm可执行文件，并且使用薄薄一层JS胶水代码，就能与平台其它部分交互。</p><p></p><p>但目前也存在一些纯WebAssembly运行时，例如<a href=\"https://wasmtime.dev/\">wasmtime</a>\"、<a href=\"https://wasmedge.org/\">wasmEdge</a>\"、<a href=\"https://wasmcloud.com/\">wasmCloud</a>\"、<a href=\"https://wazero.io/\">Wazero</a>\"等。纯运行时完全脱离开JavaScript主机，并且比成熟的JavaScript引擎更轻量级，更易于嵌入到更大型的项目中。</p><p></p><p>事实上，许多项目正开始采纳WebAssembly，将其作为托管扩展和插件的多语言平台。</p><p></p><p><a href=\"https://www.envoyproxy.io/\">Envoy proxy</a>\"正是其中一个著名项目。其代码库以C++为主，虽然支持插件，但存在和浏览器插件一样的问题，即必须做编译、必须做发布、插件可能无法以正确的权限级别运行，甚至在发生严重故障时可能破坏整个过程。现在，开发人员可以通过嵌入Lua或JS解释器，支持用户通过编写脚本方式成功运行。解释器更为安全，因为它与主要业务逻辑隔离，并且仅采用安全方式与主机环境交互。但不足之处是必须为用户选择一种语言。</p><p></p><p>另一种做法是嵌入WebAssembly运行时，让用户自己选择语言，然后编译成wasm。该做法可实现同样的安全保证，用户也更乐意为之。</p><p></p><p>纯WebAssembly运行时不仅用于实现扩展。一些项目正在创建wasm原生API薄层，以提供独立的平台。</p><p>例如，<a href=\"https://docs.fastly.com/products/compute-at-edge\">Fastly</a>\"开发了边端的无服务器计算平台。其中，无服务器功能由用户提供的WebAssembly可执行文件实现。</p><p></p><p>初创公司<a href=\"https://www.fermyon.com/\">Fermyon</a>\"正开发一个丰富的生态，实现仅使用wasm编写Web应用。该生态由各种工具和基于Web的API组成。最新发布的产品是<a href=\"https://www.fermyon.com/cloud\">Fermyon Cloud</a>\"。</p><p></p><p>这些解决方案已为特定用例提供定制的即席API，这确实是WebAssembly的一类使用方式。不止于此，Docker创始人Solomon Hykes在2019年就写道：</p><p></p><p></p><blockquote>如果wasm+WASI在2008年就出现了，那么我们就不需要去创建Docker。这足以说明其重要性。服务器端WebAssembly是计算的未来，但标准化的系统接口是缺失的一环。希望WASI能够胜任这项任务！— Solomon Hykes (@solomonstre)&nbsp;<a href=\"https://twitter.com/solomonstre/status/1111004913222324225?ref_src=twsrc%5Etfw\">March 27, 2019</a>\"</blockquote><p></p><p></p><p>抛开具体场景，人们的第一反应不免是“wasm到底与Docker有什么关系？”。当然也会想，“WASI是什么鬼？”</p><p></p><p>WASI指“WebAssembly System Interface”，可视其为支持wasm运行时与操作系统交互的一组（类POSIX）API集合。WASI是否类似于JDK类库？并不完全如此。WASI是薄薄一层面向功能的API，用于与操作系统交互，详见<a href=\"https://hacks.mozilla.org/2019/03/standardizing-wasi-a-webassembly-system-interface/\">Mozilla公告博客</a>\"。简而言之，WASI补上了缺失的一环。WASI允许定义与操作系统直接交互的后端应用，无需任何额外的层，也无需即席API。目前WASI的工作是推进其被广泛采纳，能在某种程度上成为后端开发的事实标准。</p><p></p><p>WASI API包括文件系统访问、网络乃至线程API等。这些API与运行时的底层功能协同工作，可简化平台的迁移。</p><p></p><h2>移植Java</h2><p></p><p>尽管存在各种挑战，但WebAssembly依然是首个有潜力成为真正的多供应商、多平台、安全和多语言的编程平台。我认为各位Java极客应把握机会参与其中。</p><p></p><p>WebAssembly规范和WASI工作仍在不断地发展变化。点滴汇成江海，这些工作铺就了通往简化任意编程语言的移植之路，且不仅局限于支持手动内存管理的语言。</p><p></p><p>事实上，部分使用垃圾回收的语言已实现移植，尽管它们所采用的方式方法各有千秋。例如，Go采取了编译为wasm（虽然存在部分限制）；Python移植采取了解释器的移植，即将CPython解释器编译为wasm，之后和传统的执行环境一样去执行Python脚本。</p><p></p><p>当然，实现向Java的移植依然面对很多问题，内存管理只是其中之一。我们当然可以为可执行文件中添加GC，这实际上也正是GraalVM原生镜像目前的工作方式。但在我看来，更难之处在于对其它一些CPU功能或系统调用的支持。这些功能目前仍然不稳定，或尚未得到广泛支持，诸如：</p><p>大多数独立wasm运行时依然缺乏对线程的支持，或是仍是实验性的；甚至浏览器支持也是实验性的，是通过WebWorkers模拟的；套接字访问缺少标准化支持：所有支持编写自定义HTTP handler的服务，通常都提供了预先配置的套接字，对低层级访问是受限的；更难模拟的实验性功能是异常处理，因为wasm字节码中缺乏非结构化跳转。实现该功能，应需在wasm VM中提供适当的支持。每种语言对内存布局和对象形状都有自己的约束。因此，各语言更难跨边界共享数据，这阻碍了不同语言间的兼容性，限制了wasm作为多语言平台的适用性（但该问题已列入GC规范本身之中）。</p><p></p><p>简而言之，无论对于浏览器之内还是之外的WebAssembly平台，移植Java依然存在诸多挑战。</p><p></p><h2>WebAssembly对Java的支持</h2><p></p><p>当前，已有一些面向WebAssembly和Java的项目和软件库。下面将列出我在网上发现的一些资源，虽然其中很多只能称为兴趣爱好项目。</p><p></p><h3>浏览器中运行Java</h3><p></p><p>一些项目针对将Java转换为WebAssembly。但其中大多数项目生成的代码是不兼容更精简的wasm运行时的，通常只适用于浏览器中运行。</p><p><a href=\"https://github.com/mirkosertic/Bytecoder\">Bytecoder</a>\"、<a href=\"https://github.com/i-net-software/JWebAssembly\">JWebAssembly</a>\"和<a href=\"https://teavm.org/\">TeaVM</a>\"等转换器项目，都是将Java字节码转换为WebAssembly，但在将Java字节码转换为浏览器友好代码的技术上略有差异。其中，<a href=\"https://teavm.org/\">TeaVM</a>\"项目相对而言更具前景。我们看到在<a href=\"https://github.com/fermyon/teavm-wasi\">Fermyon分支</a>\"中，包括了对WASI Bytecoder的初步支持。<a href=\"https://leaningtech.com/cheerpj/\">CheerpJ</a>\"是一个非常有前途的专有软件项目。CheerpJ意在提供对全部Java特性的支持，甚至包括Swing。还有<a href=\"https://chrome.google.com/webstore/detail/cheerpj-applet-runner/bbmolahhldcbngedljfadjlognfaaein\">Chrome扩展</a>\"使用Web技术运行很赞的applet。</p><p>一些项目也值得关注，它们针对浏览器运行时，部分提供实验性wasm支持：</p><p><a href=\"https://github.com/google/j2cl/tree/master/samples/wasm/src/main/java/com/google/j2cl/samples/wasm\">J2CL（GWT的后续）</a>\"是Java到JavaScript的源到源编译器，即transpiler，最近提供了对wasm的支持。该编译器支持最新的GC规范。<a href=\"https://github.com/jtulach/bck2brwsr\">Bck2Brwsr</a>\"也是针对JavaScript和浏览器的字节码编译器。<a href=\"https://www.fermyon.com/wasm-languages/kotlin\">Kotlin/Native</a>\"也支持通过LLVM编译为wasm。它也继承了Kotlin/Native存在的所有问题，例如并非支持所有的Java库。<a href=\"https://plasma-umass.org/doppio-demo/\">DoppioJVM</a>\"项目值得一提。它采用了完全不同的技术路径，<a href=\"https://www.fermyon.com/wasm-languages/python\">类似Python</a>\"，不是将字节码编译为wasm，而是提供一种用JavaScript编写的浏览器内VM，去解释JVM字节码。但不幸的是，该项目当前不再维护。</p><p></p><h3>在JVM上运行WebAssembly</h3><p></p><p>前面一直讨论的是如何让Java程序运行在wasm运行时上，我们当然也希望能反其道而行之。平心而论，JVM编程语言已颇具规模的，并且当前大多数wasm运行时所提供的编程模型（使用手动内存管理）在JVM上运行也有些别扭。为周全起见，在此仍有必要提及，当然这些项目也值得介绍。</p><p>首选显然是前文提及的<a href=\"https://github.com/oracle/graal/tree/master/wasm\">GraalVM Truffle实现的WebAssembly解释器</a>\"。GraalVM/Truffle平台博采众JIT之长，具有多语言互操作性。<a href=\"https://github.com/cretz/asmble\">asmble</a>\"提供了一组工具，包括将wasm编译为字节码的编译器、wasm解释器等。<a href=\"https://github.com/fishjd/HappyNewMoonWithReport\">Happy New Moon With Report (JVM)</a>\"是WebAssembly的JVM运行时。列举于此，仅是因为我喜欢这个憨憨的命名。原生wasm运行时绑定，例如<a href=\"https://github.com/kawamuray/wasmtime-java\">kawamuray/wasmtime-java</a>\"。最近推出的<a href=\"https://extism.org/\">Extism项目</a>\"跨多种宿主语言，提供原生WebAssembly运行时wasmtime接口统一的API。<a href=\"https://github.com/evacchi/kaitai-webassembly\">Katai WebAssembly</a>\"是一个由我维护的wasm解析器项目，它使用<a href=\"https://kaitai.io/\">Katai Struct</a>\"二进制解析生成器编写，欢迎大家反馈问题请求（PR）。项目设计上并非针对在JVM上运行wasm，而是针对用户操作或查询wasm可执行文件信息的需求。事实上，Kaitai语法支持对所有受支持语言生成二进制解析器，不局限于Java，还包括Python、Ruby、Go和C++等。</p><p></p><h2>结束语</h2><p></p><p>希望本文能激发大家对WebAssembly的兴趣。Java-on-wasm依然是一个新生事物，欢迎大家以开放心态去探索这一全新的世界，并从中收获惊喜。</p><p></p><h2>作者简介</h2><p></p><p><a href=\"https://www.javaadvent.com/author/evacchi\">Edoardo Vacchi</a>\"，博士毕业于米兰大学，研究方向是编程语言设计与实现。在UniCredit银行研发部门工作三年后，加入Red Hat公司，先后参与&nbsp;<a href=\"https://drools.org/\">Drools规则引擎</a>\"、<a href=\"https://jbpm.org/\">jBPM工作流引擎</a>\"和<a href=\"https://kogito.kie.org/\">Kogito云原生业务自动化平台</a>\"项目。关注WebAssembly等新语言技术，并在<a href=\"https://kie.org/\">KIE organization</a>\"和<a href=\"https://evacchi.github.io/\">个人博客</a>\"上撰写文章。</p><p>原文链接：&nbsp;<a href=\"https://www.javaadvent.com/2022/12/webassembly-for-the-java-geek.html\">WEBASSEMBLY FOR THE JAVA GEEK</a>\"</p>",
    "publish_time": "2023-07-04 14:26:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从闭源到开源，明星项目 TDengine 的开源之路",
    "url": "https://www.infoq.cn/article/bZXU0sZzQX8a95Gsz3Ro",
    "summary": "<p></p><h2>背景介绍</h2><p></p><p>&nbsp;</p><p>首先，让我来介绍一下自己。我是涛思数据的联合创始人，同时也是 TDengine 的核心开发者之一。我主导了 TDengine 从 1.0 到 3.0 的所有版本的迭代开发工作。虽然我也承担着管理者的角色，但我更喜欢作为一名开发者。自从 2019 年开源以来，我每年都在 GitHub 上为开源社区贡献了大量的代码。今年的统计数据显示，涛思数据在中国开源社区的贡献排名第 20 名，而我个人的排名是第 7 位。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/2211aa08b61c127cb708fe16b5e9e3ca.png\" /></p><p></p><p></p><p>TDengine 是一个开源的物联网时序数据库，采用云原生技术实现，以其高性能而闻名。任何人都可以免费下载和使用，欢迎访问官网体验。在 GitHub 上，TDengine 获得了 2 万多个 star，与其他开源云原生数据库（如 TiDB 和 Redis）齐名。在国内的数据库排行榜中，TDengine 稳居第一已经有两三年了。这是一个基本的介绍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a361b5a5e024991bd4e7fb45fdd2e33.png\" /></p><p></p><p>&nbsp;</p><p>TDengine 为什么选择开源</p><p>&nbsp;</p><p></p><h2>发展里程碑</h2><p></p><p>让我们一起了解一下TDengine的发展历程。在大约2017年左右，涛思数据成立，那时只有一个原型产品，所以我们并没有选择开源。到了2018年，我们发布了第一个商业化版本，拥有了大约三四个客户，并对产品进行了进一步的打磨。在2019年7月发布的1.6版本之后，社区的反馈非常好。在GitHub的排行榜上，连续十多天都排名第一。2020年8月，我们发布了2.0版本，这两次开源带来了重要的效应和用户增长，我们成功地吸引了经纬、红杉、GGV纪元等大型投资公司的关注。2022年8月，我们发布了3.0版本以及云原生版本。可以说，TDengine的成功与我们选择的开源策略密不可分。涛思数据将与开源用户一起成长。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2c019883027e26e4653ab2597c8def4.png\" /></p><p></p><h2>为什么要开源？</h2><p></p><p></p><p>为什么要选择开源？其实，我们一开始是闭源的。但是从成立之初，我们就想要做开源。开源并不是简单的把产品拿出来放出来，它经过了很多系统的思考。我们认为开源能够扩大产品的影响力，能够很好地树立品牌。在19年我们开源的时候，创始人陶建辉写了一篇文章叫做《49岁的这个程序员》，估计很多人都看过，这篇文章的阅读量达到了20多万。这样，很多开发者就了解到了我们的TDengine并开始使用，知名度成为涛思数据，包括TDengine未来一切发展的一个基础。因此，我们要开源首先就是要想好如何推广产品。</p><p>&nbsp;</p><p>其次，开源之后，可以很容易地构建开发者社区，建立竞争壁垒。我们现在的微信群里大概有20多个500人的群，每天都有人在里面提问。如果确实有问题，我们会把这些问题反馈到GitHub的需求里，或形成内部工单。也会有很多人写一些文章，包括设计原理、好处、坏处、坑在哪里、优势等等。这些东西成为了我们的财富。我们的竞争者很难获得这么多的关注度，无形之中形成了竞争的壁垒。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d788580c0a06128ec28c1a641dc76dca.png\" /></p><p></p><p></p><p>发布开源代码后，我们很容易得到市场反馈。由于有大量用户使用我们的产品，他们会提出改进建议和报告 bug。无论反馈是好是坏，这些用户的声音都能促进我们更好地迭代产品。我们迭代速度很快，曾经每周发布一个版本，后来平均每两周发布一个版本。一些用户抨击我们这么快地迭代，认为我们是基础软件，不需要那么快更新。但实际上，如果我们的软件没有人使用，那就永远不会有 bug。我们需要不断迭代才能让产品更加稳定。虽然使用基础软件时很少遇到问题，而且我们的软件已经非常稳定了，但我们的很多新用户来会带来许多新想法，这些想法会成为我们的创新源泉，促进我们持续迭代。</p><p>&nbsp;</p><p>发布开源代码后可以轻松地打造产品生态。在当前信创趋势下，我们的软件可以融入不同的操作系统，如 ARM、龙芯和鲲鹏等。外部的很多开发人员会为我们贡献代码、连接器和可视化工具等。我们会区分官方发布版本和开发者贡献版本。</p><p>&nbsp;</p><p>另外，开源软件更容易获得用户的信任。现在，许多大型公司提供一整套解决方案，特别是在云原生浪潮下，用户可以购买一个 K8s 集群并获得所需的任何工具。然而，一些传统公司可能不想受到大公司的束缚。相比之下，一些成熟的互联网公司更愿意自行开发产品。在这种情况下，开源软件成为一个非常重要的选择。此外，基础软件要想在国内取得成功并获得盈利非常困难。因此，要想真正取得成功，包括所有投资者在内的所有人都认为基础软件是一个很好的趋势。为什么呢？因为在国内，我们的用户数量和数据量都非常大，我们可以通过开源软件更容易地进入其他市场。因此，开源是走向海外的一个非常有效的途径。</p><p>&nbsp;</p><p></p><h2>为什么要开源最核心的代码</h2><p></p><p>开源的核心代码非常重要，因为如果你不开源这部分最具有竞争力的代码，市场上可能会出现许多替代品。在过去，国内的许多开源软件仅开放了API和外围工具，这并没有给用户带来特别多的价值，也没有让贡献者获得很大的成就感。这就像你买一件衣服，如果你觉得不好看，别人送你一件你也不会穿。同样地，用户也不会使用没有价值的软件，因此很难形成一个支持者群体。</p><p>&nbsp;</p><p>通过开源具有竞争力的代码，我们可以更好地击败竞争对手。作为基础软件的开源项目，例如数据库，如果没有良好的性能或特色功能，竞争对手很容易替换你的位置。相反，如果你已经做到了某个高度，竞争对手也就没有动力推出与你类似的产品了。因此，开源可以使竞争对手越来越少。特别是对于公司来说，开源带来的好处是很明显的，因为我们需要盈利。&nbsp;</p><p></p><h2>开源的误区</h2><p></p><p></p><p>开源其实存在几个误区。首先，仅仅写出好的代码是不够的。很多人将他们的代码开源出来之后，却没有做任何市场推广。虽然这样的代码确实可以被随意下载，但实际上却无法为产品创造真正有价值的贡献。因此，我们需要在代码写得好的基础上，采取各种各样的配套手段，并且最好能够在一个有计划的时间段内进行执行。</p><p>&nbsp;</p><p>其次，我们不能仅仅是写完代码之后就去开源。如下图所示，在我们的项目中，我们的更新速度从2019年到2023年逐渐加快。虽然这反映出我们的代码肯定不是足够好的，但我们仍然选择开源的原因在于，我们已经有了一定的商业客户基础，可以放心地将代码发布出去，吸引更多的客户并完善我们的产品。因为很多软件的市场定位会随着时间的推移发生变化，只有通过不断地发展，才能真正找到市场定位和盈利点。</p><p>&nbsp;</p><p>最后，商业化也不能慢慢来，需要与开源同步推进。当然，并不需要组建一个庞大的商业化团队，因为在国内这样的环境下，商业化必然需要一些定制化开发的工作。随着项目的规模增大，这些长尾工作可能会变得越来越多，需要平衡市场的诱惑和主线版本之间的关系。因此，商业化的进度需要适度地控制，因为商业化既能为我们的开发团队带来信心，也能为投资团队带来信心。这是一个适度的发展过程，在合适的时间选择做好商业化。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/57/5705317306a9a3503f6140492ee63780.png\" /></p><p></p><p></p><h2>开源软件成功的关键指标</h2><p></p><p></p><p>什么是成功的开源软件呢？我认为衡量开源软件成功的关键指标有四个，其中最重要的指标是它的关注者。因为关注者代表了社区的影响力，而GitHub上的star数是最直观的衡量指标。虽然GitHub上有二三十万个开源软件，但能达到一万个star以上的软件只有一两千个。这些软件无一例外地都是业内领先的佼佼者，这是一个很客观的评价指标，而且它无法作假。如果有假冒伪劣的情况被发现，会对公司形象产生致命影响，使公司无法再起步。</p><p>&nbsp;</p><p>除此之外，开源软件的成功还可以从使用者的数量方面衡量。不同软件的使用者衡量标准可能不同。例如，底层软件的数据库，前端组件都无法用统一的方式衡量它们的用户量。部署实例的数量也不好衡量，因为有些软件部署在内网中，无法与外界通信，也无法获取相关信息。一些用户也可能关闭上报信息的配置项。但是，我们可以从趋势上看到软件在哪些版本中得到了更好的使用，哪些特性更加重要。</p><p>&nbsp;</p><p>在开源社区中，贡献者的数量虽然反映了用户对软件的投入和反馈，但并不是衡量软件重要性的唯一标准。外部贡献者的增多可能增加软件风险，因为出现问题时可能难以立即找到他们进行定位和修复。</p><p>&nbsp;</p><p>在选择开源软件时，大企业更注重的是维护者和团队的背景和能力，以及他们的稳定性。核心提交是否来自于团队的内部以及团队对代码的控制能力，是衡量软件质量的重要指标之一。</p><p>&nbsp;</p><p></p><h2>什么样的产品适合开源？</h2><p></p><p></p><p>关于哪些产品适合开源，我认为有三类：一类是用户量较大的产品，另一类是相对标准化的产品。这两类产品一旦打好口碑，就会像很多互联网热门的APP一样一夜爆红，迅速获得用户。但是我们也需要具备技术硬核软件，这是第三类。这类产品是以技术取胜的。而一些以解决业务上痛点或创新为主的软件则不太适合开源，因为一旦公布，容易被仿造。这类软件最好选择闭源。创业者们应该根据自己的情况，做出明智的选择。</p><p>&nbsp;</p><p></p><h3>开源产品的定位</h3><p></p><p></p><p>如果我们要真正开源一个产品，那么它的定位就必须面向全球市场。这可以从一些顶尖的开源产品的目标市场中看出来，这些软件的共同特点是它们的目标市场都是全球，因为作为一款基础软件，如果不走向全球市场，它的想象空间将非常有限。此外，如果你的核心团队没有足够的激励，那么这款产品很可能也无法生存下来。在目前的ToB形势下，只有全球前三名的软件才能生存下来。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f9/f90b74b00a1ff471fd6ee5fdcf142d4a.png\" /></p><p></p><p>&nbsp;</p><p>比如芯片，目前市场份额大概有80～90%由英特尔和NVIDIA占据，而ARM则占据一小部分份额。国内很多芯片厂商靠政策或资本支持才勉强生存，希望有一天能够进入前三。所以我们的目标必须是远大的。如果这款产品的定位不是全球性的，如果不能在技术上处于全球领先地位，那么它的意义就不大。我相信，在任何一个细分市场中，都可以诞生出独角兽。现在，估值超过10亿美元的公司就可以叫独角兽了，它不一定是一个全面的软件平台。&nbsp;</p><p></p><h3>我适合做开源的创业吗？</h3><p></p><p></p><p>在考虑将产品开源化并创业时，需要先确定产品的方向。目前，软件行业有点像之前的汽车行业，出现了越来越多的零部件制造商，每个公司专注于自己的一小块工作，由集成商将它们最终组装完成。类似地，开源领域，特别是大数据领域，也逐渐实现了零部件化。许多公司专注于自己擅长的领域，将与业务相关的部分交给第三方集成商处理。例如，国内的东软、神州和中科软等公司会处理这些客户的需求，而其他技术型公司则会专注于自己产品的研发。</p><p>&nbsp;</p><p>如果要创业，团队小而精干，但至少要有一个人擅长技术推广。这个人应该有意愿去推广产品，并成为优秀的市场推广人才。如果团队中没有这样的人才，就需要重新考虑如何组建团队。因为软件开发不仅仅是技术，技术只是一个必要的条件。</p><p>&nbsp;</p><p>我们需要逐渐将传统的软件思维转化为互联网思维，更进一步，转化为创造共赢的思维。我们只需要专注于自己的组件，与其他相关公司共同构建整个生态系统。这意味着我们需要与上下游企业建立良好的合作关系，以实现共同的成功。</p><p>&nbsp;</p><p></p><h2>产品开源后就能带来巨大的成功吗？</h2><p></p><p></p><h3>开源成功的关键</h3><p></p><p></p><p>产品开源并不一定能够获得成功。像我之前说的，将软件放在互联网上并不是结束，这只是一个简单的开始。我发现很多同行都有开源的产品，但是它们宣布开源后，文章可能阅读量不超过1000个甚至更少，没有什么反响。实际上，对于这些产品，开源与否并没有什么意义，还不如闭源做好自己的客户服务。因此，开源的软件需要有一个合理的规划。开源的软件必须达到一定的质量标准，这也是为什么我们的产品在2年之后才选择开源的原因，因为基础软件成长周期相对较长。而一些软件的成长周期可能较短，我们可以适当缩短这个过程。但是无论如何，在进行开源宣传时，产品一定要有一个足够打动人心的亮点。也就是说，你要能回答使用你的软件能够带来什么样的价值。除了有一定的质量保证，软件还需要有稳定的维护和团队支持，需要组建公司或小型工作室来维护稳定的团队，并做好市场推广。</p><p></p><h2>开源的商业模式</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/8706970b7fce8deb064f6dde17c31ce4.png\" /></p><p></p><p></p><p>&nbsp;</p><p>开源并不意味着免费，也不表示缺乏商业模式。我们公司一直在探索与开源相关的商业模式。目前业内很多领先的软件公司也都在摸索如何孵化。从一开始，我们就考虑了一些策略，因为我们知道靠着热情可以坚持1～2年，但工作性质总是会发生变化，因此确定商业模式非常重要。作为TDengine，我们有三个版本的商业模式。</p><p>&nbsp;</p><p>第一个版本是社区版，这是免费下载的版本，其主要目的是建立品牌，建立开发者社区，吸引越来越多的用户，提高用户口碑。一些用户可能一开始使用的是开源产品，但随着公司的不断扩大，可能会做出一些新的商业决策。例如，我们有一个典型的案例，有一个工程师非常了解我们，后来他去到理想汽车公司，觉得可以用上我们的产品，就为获得了一个非常好的客户，社区版发挥了很重要的推广作用。</p><p>&nbsp;</p><p>在企业版中，我们提供了两种收费模式：独立部署和订阅。独立部署是一种一次性买断的收费模式，而订阅则是一种长期的收费模式。这两种模式各有优劣。对于国有企业来说，他们可能更喜欢独立部署这种买断式的收费模式。从产品发展和估值的角度来看，我们更喜欢订阅模式。我们销售的是什么呢？由于我们的核心代码已经开源，剩下的是一些辅助功能。如果不需要像审计、加密等高级功能，或者你的公司有足够的技术能力来维护这些软件，那么可以选择不购买这些技术服务。然而，对于许多中小型公司来说，他们虽然拥有技术能力，但也不想将其投入到软件维护上，而是希望将其投入到业务发展上，这个时候就可以考虑购买企业版。</p><p>&nbsp;</p><p>我们也有云服务的版本。尽管公有云的需求正在下降，但私有云的需求仍然很高。我们可以在客户公司的私有环境中部署云环境，以满足他们的需求。</p><p>&nbsp;</p><p></p><h3>开源带来的销售变革</h3><p></p><p></p><p>开源有哪些好处呢？首先，开源改变了传统的 ToB 销售模式，我们不再只是去找企业，而是面对活生生的个人客户，这些客户通常都是能够对软件购买产生决策的架构师等人群。这也是我们在软件大会上宣传产品的原因。传统的登门拜访转变为线上销售，PoC 流程也大为减少，因为真正联系我们的客户已经了解软件的使用模式和优缺点。他们只需要检查公司的最佳实践和业务场景是否匹配，以及技术和维护能力是否足够即可。这样，我们需要投入 PoC 的人员也会非常少，售前团队也会变得更小。长远来看，这对公司的成本有很大帮助，我们将从资源型销售转变为技术和产品型销售。我们现在招聘的销售人员很多都是毕业生，只要他们了解行业的基础知识就可以胜任，而传统的到各个圈子走访的销售可能会把资源带走，难以管理。通过这些措施，我们可以把 ToB 的生意变成 ToC 的生意，变成开发人员喜欢的简单生意。所以像能力强的架构师、开发人员都喜欢去大厂，因为工作比较简单。如果我们也能做到这一点，就可以吸引更多的人才加入基础软件行业。</p><p>&nbsp;</p><p></p><h3>开源带来的生态变革</h3><p></p><p></p><p>开源使得技术和产品的竞争变得更加激烈，而不是仅仅依靠销售资源。这将会促进更多以产品为导向的公司的出现，当然平台型公司也会有很大的成长空间。我们相信在各个细分领域，都会出现一些全球性的企业。即使我们只是做螺丝钉，也可以成为全球前三名之一，我们也可以很好地生存下去。我们还可以帮助许多大众型企业降低成本，使他们能够将精力集中在核心业务上。随着这些企业逐渐成长，我们可以获得更多长尾价值。</p><p>&nbsp;</p><p></p><h3>开源需要组织支撑</h3><p></p><p>我认为开源项目需要一个组织来提供支持。即使你将软件捐赠给 Apache 基金会等组织，也需要有商业孵化公司提供支持。虽然有开发能力和贡献精神的个人可以自己维护和开发软件，但企业使用开源软件需要强有力的支持，以确保软件的稳定性。特别是在发布大版本时，例如从2.0升级到3.0或从3.0升级到4.0，需要封闭性开发、大量测试、DevOps、CI/CD、手工测试等。个人贡献者不愿意从事这些工作，他们只想编写核心代码。如果这些任务被忽略，软件将无法发布。如果开源软件没有组织支持，它的发展速度将逐渐下降，需要寻找新的方式来维持软件的生命周期，这是一个更长远的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/684cc3f70367cb7c6ca427001ae48c2c.png\" /></p><p></p><p></p><p>在某次会议上，我听到有人说开源只是一种好玩的东西，但我对他提出质疑，因为开源并不是一个简单的好玩的事情。不同类型的软件公司必须将开源作为他们的事业，并且对待它的态度不能仅仅是好玩。不能因为某个人觉得不好玩了，就停止参与开源工作。尤其对于ToB领域的软件，这个思路必须清晰。</p><p></p><h2>开源产品运营的正确姿势</h2><p></p><p></p><h3>开发者运营是一个项目</h3><p></p><p>&nbsp;</p><p>开源软件需要设立开发者运营部门来协同各类资源，促进开发者生态的搭建和完善，尤其在企业发展到一定规模时。虽然研发人员的开发能力很强，但很多人不愿意写文章或者做演讲报告，而市场推广和商务部门的技术能力相对较弱，无法胜任这些任务。因此，我们需要协调资源，为软件推广提供更好的支持。为此，越来越多的公司开始设立开发者运营部门和布道师岗位，把它视为一个新项目来管理。该项目的主要指标是用户规模的增长，其中用户体验的提升是其中一个关键点。用户体验不仅包括产品本身的使用体验，还包括是否能够在各种搜索引擎中找到所需信息，是否有相关案例、最佳实践和文档，以及在出现问题时能否联系到合适的人进行沟通等。开发者运营的工作很大程度上涉及到这些方面，同时也要参与各种有技术影响力的活动，如写文章和公开演讲，推进技术影响力的提升。通过这些活动和案例的积累，我们可以更容易地为商业赋能。知名度对于线下生意也是一个非常好的支撑。现在我们的开源软件 TDengine已经走上正轨，客户选择我们也不会被质疑。通过这些方法，我们可以让客户成功，持续了解他们的新需求并加入到我们的产品路线图中。</p><p>&nbsp;</p><p></p><h3>开发者运营的日常工作</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/097679348738c1b02abfa25edf581256.png\" /></p><p></p><p></p><p>作为一个项目，开发者运营需要有很多人员，每天也要处理很多日常工作。第一是用户运营。我们将用户分为几类：顶层灯塔用户、普通用户和企业用户等。对于这些用户，我们提供的支持可能也不同。此外，我们还会制定一定的激励体系，并定期与贡献者进行沟通，授予荣誉证书或分配一些开发任务等。</p><p>另外，内容运营也很关键。我们需要编写大量的基础文章、用户案例并指导用户自己撰写。此外，我们还需要发布培训视频和短视频等，这些都是提升影响力的重要部分。</p><p>&nbsp;</p><p>活动运营也是项目的一部分，包括线上和线下活动。我们在本次大会上也有展台，展示我们的产品，并回答大家的问题。另外，开源项目肯定需要在某个平台上运行，例如在GitHub上，我们需要不断回答用户问题，维护平台并发布新版本，同时还要进行产品培训等。</p><p>&nbsp;</p><p>此外，还需要进行数据方面的运营。这些数据主要是供我们自己内部使用的，因为我们需要衡量做的这么多事情中，哪些是有用的，哪些是没用的。这样可以让公司高层了解下一步该如何做，并成为评价指标。如果没有人给你一个明确的评价，那么即使你取得了成就，也会感到不满足。我们正在建立评价体系，并希望通过数据运营的方式，为生态者部门本身提供很好的评价指标。</p><p>&nbsp;</p><p></p><h3>为什么要进行开发者运营</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e502dddefc9f8440cce40da5c742b522.png\" /></p><p></p><p>为什么我们需要进行开发者运营呢？因为开发者就是产品的用户。实际上，直接使用我们的产品的人都是我们的开发者，他们的技能和水平也在不断提高，他们也不会一直待在某一家公司。因此，我们不应该忽视任何一个开发者，无论他们所在的公司是否与我们的业务有交集。开发者是我们技术的伙伴，活跃的开发者对于开源项目的长期繁荣发展至关重要。他们可能也会成为我们生态的伙伴，我们可以与他们共同寻求平台化或生态化运作，实现共同的价值。事实上，有一项调查表明，57%以上的开发者认为他们对基础软件的购买有一定的影响力。</p><p></p><h3>如何让开发者成功</h3><p></p><p></p><p>我们应该让开发者成功，让他们开发的软件被市场所接受和喜爱，这样才能让客户成功，最终也才能实现我们自己的成功。为了让开发者成功，我们需要做以下几件事情。</p><p>&nbsp;</p><p>让开发者用起来放心：不用担心产品中的各种坑和后门，不用担心被厂商绑架。让开发者用起来安心：不用担心出问题没人管。让开发者用起来顺心：不用花费过多的力气去学习、去开发。让开发者用起来开心：让开发者开发的应用大获成功。&nbsp;</p><p></p><h3>开源在中国的机会</h3><p></p><p></p><p>在中国开源软件的机会还是很大的。中国的企业可以利用这个机会去颠覆传统软件，就像几年前互联网公司进军企业软件一样。然而，简单地将互联网思维套用于传统软件并不会产生深刻的价值，也不容易获得成功，因为传统软件的业务非常复杂。如果我们能够在开源软件中为其创造一些特别的亮点和特色，如将服务器数量从 100 台降至 10 台，或将响应时间从 10 分钟缩短至秒级，就可以为其赋能，使其更好地进行二次迭代和新项目的申请。因此，在中国开源基础软件仍然有许多机会。</p><p>&nbsp;</p><p>中国有许多工程师，价格便宜、数量充足，因此我们可以用较低的人民币薪资生产类似的软件。中国是制造大国和消费大国，有所有的应用场景，而且数据量巨大，我们可以在国内做好一些试验，之后再将其开源。一旦我们开源软件，它将变得非常可靠，这也是我们向全球市场进军的最好路径。</p><p>&nbsp;</p><p>最近参加的一次会议讨论了如何拓展海外市场并制定相应的计划。许多方法都被提出，但开源无疑是其中非常重要的一环。开源并不意味着免费，一旦确定好收费模式，就可以更好地推广产品。</p><p>&nbsp;</p><p>“有所为，有所不为”是指要不忘初心，即在创建公司时要明确业务场景和所期望的效果，始终记得这一点。虽然可以进行修改，但不应因获得大型客户合同而进行修改。例如，今天有一个3千万的项目，你应该接还是不接呢？如果接了，企业中可能有一半以上的重要员工将去做这项开发工作，甚至可能无法预期项目何时完成。一旦开始这样的项目，会有越来越多的类似需求出现，销售人员也会为自己的KPI而推销更多需求。这家原本是基础软件公司的企业，可能会变成一个集成商。如果一直继续这样，最终会发现，两三年后，原本擅长的领域已有更多竞争者，软件也无法更好地发展。因此，如果我们能够坚持五年或八年来发展自己的产品，仍然有可能在该领域取得垄断地位。</p><p></p><h3>参与开源社区的好处</h3><p></p><p></p><p>参加开源社区肯定有好处。我们可以了解到业界广泛应用的各项技术，并学习到高质量设计的思路。即使你认为他们写的代码很平凡，但是他们的流行性基本代表了业界的高水平。此外，公开的程序和文档强制你严格要求自己。在公司内部写一个文档可能只需要随便写两句，但一旦要公布出去，就需要仔细斟酌。这也包括在公司内部做报告时，你可能不会花费时间去准备完善的PPT，但在开源社区，你需要不断磨练自己，提高自己的技能水平。参加越来越多的顶级开源软件项目也是你技能的一个很好的证明。如果别人能够approve你的commit，就说明你的水平达到了这个软件的基本水平，这也是你简历中最好的证明之一。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/39/39b3299f431a747d7466d37493a864fd.png\" /></p><p></p><p>&nbsp;</p><p>欢迎大家加我的微信，我们可以线下聊聊。此外，如果你对TDengine感兴趣，可以在GitHub上<a href=\"https://github.com/taosdata/TDengine\">下载代码</a>\"。TDengine是一款开源、高性能、云原生的时序数据库，专为物联网(IoT)、连接汽车、工业物联网和DevOps而优化。谢谢大家！</p><p>&nbsp;</p>",
    "publish_time": "2023-07-04 14:30:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "跨平台三端重构正式统一，QQ Windows全新体验版上架官网",
    "url": "https://www.infoq.cn/article/lVbbQRzXymTFtqjN9FYT",
    "summary": "<p>7月3日，全新体验版Windows QQ正式上线官网，面向用户开放官方下载渠道。继QQ对 macOS 、Linux版本进行升级后，本次Windows版本的更新，标志QQ基于NT技术架构，实现了桌面端QQ的三端体验统一。同时，新版本QQ Windows新增&nbsp;64&nbsp;位版本支持，并针对大众关注的内存占用问题进行了深度拆解和优化。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/95/e9/955d3372e90f5bec6745d3862c199ae9.png\" /></p><p></p><p>&nbsp;</p><p>（QQ官网界面）</p><p>&nbsp;</p><p>全新交互界面，升级功能及安全能力</p><p></p><p>正式上线的Windows QQ 基于NT架构打造了全新的交互界面，UI设计三端保持一致；消息界面采用了三栏式设计，整体风格更加清爽简约，方便用户查看所需信息；资料设置及资料卡也变得更简洁干净。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1c/b3/1cca43ed70ec74706490c6aea4aa64b3.png\" /></p><p></p><p>&nbsp;</p><p>（登录界面更新、消息界面三栏式设计）</p><p>&nbsp;</p><p>新版本支持消息、QQ空间板块等功能，提升用户在工作、社交、生活等多个场景中的使用体验。语音、视频等通讯功能可一键开启，屏幕共享能力也同步支持，为用户带来更高的沟通效率。</p><p>&nbsp;</p><p>官方发布公测版本之后，QQ团队建立了多个用户体验群，持续关注用户反馈和体验，小步快跑迭代优化了包括群应用、全局搜索、表情分类收藏等能力。</p><p>&nbsp;</p><p>同时，QQ安全团队协同腾讯多个安全实验室，在新版本QQ上全面加强各项核心安全能力，比如外挂检测、hook检测、反动态库注入、反木马盗号、风控预警等多项核心安全能力，进一步保障用户数据资产安全。</p><p>&nbsp;</p><p>跨平台复用方案，三端统一体验</p><p></p><p>从传统互联网时期扎根本土发展，桌面端QQ已经诞生超过24年。而旧版的桌面QQ采用纯Native技术栈开发，多年迭代以来，新旧的功能逻辑十分复杂，代码量非常庞大。出于对开发效率的考虑，新版QQ在技术选型及重构决策上，选用了一套标准化的框架来进行全面的架构升级，即QQ NT架构。跨平台的复用方案能够确保Windows/Mac/Linux三个桌面端快速、高质量的迭代，实现一套代码多端运行，多平台体验统一，满足各桌面用户的需求。近期，手机 QQ 首个基于 NT 架构的正式版也已在发布中。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/99/af/99d0b8b8ef655fe7e30e3575493f21af.png\" /></p><p></p><p>&nbsp;</p><p>（QQNT架构）</p><p>&nbsp;</p><p>持续优化内存使用，实现性能提升</p><p></p><p>自去年Mac端和Linux端的版本上线，QQ桌面端的更新备受用户的关注。针对三端用户广为关注的内存占用问题，QQ技术团队根据其占用的几大主要进程，重点设定优化目标，通过工具分析、定向优化、线上监控及自动化测试等，尽可能减少缓存占用及内存泄露，实现资源使用效率的最大化。目前，QQ技术团队已通过多个阶段目标的设定，从单个进程内存优化到整体内存控制，新版本已取得有效的优化成果。</p><p>&nbsp;</p><p>此次新版Windows QQ上线，专注于整体功能的高质量迭代，在用户反馈的基础上带来更成熟的核心体验，实现了Windows/Mac/Linux三端打通，一套代码多端运行，统一跨平台体验。目前，用户可通过QQ官网下载QQ Windows版本，支持x64及x86。</p><p>&nbsp;</p>",
    "publish_time": "2023-07-04 14:39:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "高时效、低成本， Apache Doris 在文旅业态下的统一数据中台实践",
    "url": "https://www.infoq.cn/article/bzSMclq0eGBalYusMlsZ",
    "summary": "<p></p><p>作者｜NearFar X Lab 团队 洪守伟、陈超、周志银、左益、武超</p><p>整理｜SelectDB 内容团队</p><p></p><p></p><blockquote>导读：无锡拈花云科技服务有限公司（以下简称拈花云科）是由中国创意文旅集成商拈花湾文旅和北京滴普科技有限公司共同孵化组建的。拈花云科以数字化思维为导向，致力于成为文旅目的地数智化服务商。2022 年底，拈花云科 NearFar X Lab 团队在数据需求的驱动下，开始调研并引进 Apache Doris 作为新架构下的数据仓库选型方案。本文主要介绍了拈花云科数据中台架构从 1.0 到 2.0 的演变过程，以及 Apache Doris 在交付型项目和 SaaS 产品中的应用实践，希望本文分享的内容能对大家有所启发。</blockquote><p></p><p></p><p></p><h2>业务背景</h2><p></p><p></p><p>拈花云科的服务对象主要是国内各个景区、景点，业务范围涵盖文旅行业的多个板块，如票务、交通、零售、住宿、餐饮、演绎、游乐、影院、KTV、租赁、服务、会务、康乐、康养、电商、客服、营销、分销、安防等。多业务线条下用户对于数据使用的时效性需求差异较大，需要我们能够提供实时、准实时、T+1 的业务支撑能力。同时根据大部分景区为国有化的特点，我们也需要具备能够 提供私有化交付部署及 SaaS 化数据中台产品解决方案的双重服务支撑能力。</p><p></p><p></p><h3>数据中台 1.0 - Lambda</h3><p></p><p></p><p>早期构建数据中台时，为了优先满足 B 端用户数据整合的需求，以稳定数据输出为出发点，因此我们基于行业中比较成熟的 Lambda 架构形成了数据中台 1.0 。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f3/f3f4fad0a54f3908f31ef4be4aa80916.png\" /></p><p></p><p>在数据中台 1.0 架构中分为三层，分别为 Batch Layer，Speed Layer 和 Serving Layer。其中，Batch Layer 用于批量处理全部的数据，Speed Layer 用于处理增量的数据，在 Serving Layer 中综合 Batch Layer 生成的 Batch Views 和 Speed Layer 生成的 Realtime Views，提供给用户查询最终的结果。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/90/904861d560bab92406e669f78b2584a3.png\" /></p><p></p><p>Batch Layer： 在我们早期的实施类项目中，单纯以离线 T+1 进行数据支持的项目占了绝大多数。但实施类项目在实现 Lambda 架构的过程中也会面临很多问题。比如数据采集环节，由于项目本身原因业务系统不能开放 DB 的 Binlog 供数据仓库采集，因此只能以 JDBC 的方式完成增量或全量的数据同步，而通过该方式同步的数据往往会由于系统人工补充数据、时间戳不规范等问题产生同步数据差异的情况发生，最终只能通过额外的数据对比逻辑进行校验，以保证其数据的一致性。</p><p></p><p>Speed Layer： 项目受成本约束较大，大面积基于流的实时计算对于不论是从硬件成本、部署成本还是实施成本、维护成本等角度均难以支撑。基于该原因，在实施类项目中只有部分业务会进行基于流的实时统计计算，同时满足流计算条件的业务上游系统也必须同时满足同步 Binlog 的使用需求。</p><p></p><p>Serving Layer： 大部分的预计算结果存储在 MySQL 中提供 Report 支持，部分实时场景通过 Merge Query 对外提供 Ad-Hoc 的查询支持。</p><p></p><p>随着时间的推移，大量的项目交付使用增多，架构的问题也逐渐开始显现：</p><p></p><p>开发和维护成本高：该架构需要维护两套代码，即批处理和实时处理的代码，这无疑增加了开发和维护的成本。数据处理复杂度高：Lambda 架构需要处理多个层次的数据，包括原始数据、批处理数据和实时处理数据，需要对不同的数据进行清洗、转换和合并，数据处理的复杂度较高。实时计算支持有限：业务方对于数据时效性要求越来越高，但是该架构能力有限，无法支持更多、更高要求的的实时计算。资源利用率低：离线资源较多，但我们仅在凌晨后的调度时间范围内使用，资源利用率不高。受成本制约：该架构对于我们部分用户而言使用成本较高，难以起到降低成本提高效率的作用。</p><p></p><p></p><h3>新架构的设计目标</h3><p></p><p></p><p>基于以上架构问题，我们希望实现一套更加灵活的架构方案，同时希望新的架构可以满足日益增高的数据时效性要求。在新方案实现之前，我们必须先对当前的业务应用场景和项目类型进行分析。</p><p></p><p>我们业务应用场景分为以下四类，这四类场景的特点和需求分别是：</p><p></p><p>看板类：包括 Web/ 移动端数据看板和大屏可视化，用于展示景区重要场所的数据，如业务播报（实时在园人数监控、车船调度管理等）、应急管理监控（客流密度监控、景区消防预警、景区能耗监控等）。其组成特点一般为业务汇总指标和监控指标报警，对数据时效性要求较高。报表类：数据报表以图表形式展示，主要服务于各业务部门的一线业务人员。会更多关注垂直业务的数据覆盖程度，会有钻取需求（也可能通过不同报表来体现不同数据粒度）。一般以景区的业务部门为单位构建报表栏目和分析主题，除财务结算类报表外，一般可接受 T+1 的报表时效。分析类：自助分析基于较好的数据模型表（数据宽表）实现，对分析人员有一定的数据理解和操作需求，基于我们提供的 BI 分析平台，业务人员可基于此数据范围通过拖拽的方式组合出自己的数据结果，灵活度较高。该场景对数据时效要求不高，更多关注业务数据沉淀和与往期历史数据的对比分析侧重架构的 OLAP 能力。服务类：一般对接三方系统，由数据中台提供数据计算结果。如画像标签等数据，通过数据接口控制权限提供对外数据服务与其它业务系统集成，需要新架构能够提供稳定的数据服务。</p><p></p><p>接着我们对项目类型的特点和需求也进行了分析，并确定新架构需要同时提供实施类项目和 SaaS 产品的数据中台支撑能力：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/db/dbfcf67c9624cba50ec4c6fbdc3a470c.png\" /></p><p></p><p></p><h3>数据中台 2.0 - Apache Doris</h3><p></p><p></p><p>结合以上需求，我们计划对原有架构进行升级，并对新架构的 OLAP 引擎进行选型。在对比了 ClickHouse 等 OLAP 引擎后（社区有非常多的对比文章参考，这里不过多赘述），最终选择了 Apache Doris 作为数据中台 2.0 的基座。同时，在数据的同步、集成及计算环节，我们也构建了多套方案来适配以 Apache Doris 为核心的计算链路，以应对不同类型的实施类项目及 SaaS 产品需求。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/21/219e0421c5274f25533049af98af3258.jpeg\" /></p><p></p><p>数据中台 2.0 的核心思路是将 Apache Doris 作为核心的数据仓库，并将其作为实时数据同步中心、核心数据计算中心。数据集成环节将专注于数据同步，而计算交由 Apache Doris 完成或由 Doris 辅助计算引擎完成。同时，我们将在提供多种数据同步至 Apache Doris 的方案以应对不同的项目需求。在这个架构下，我们支持实现实时、准实时、T+1 的计算场景支持，以满足不同业务场景的需求。</p><p></p><p></p><h4>新架构数据流转：</h4><p></p><p></p><p>1. 数据同步集成：架构 2.0 有多种数据同步方式，我们主要借助 Doris Unique Key 模型完成数据的同步更新。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/40/409903f072496ad0a4f103331d457085.png\" /></p><p></p><p>2. 数仓分层计算：根据项目资源情况分 View/ 实体表单来构建后面的数据层级（DWD、DWS、ADS）。业务较轻或时效性很高时，通过 View 方式来实现逻辑层面的 DWD，通过这种方式为下游 Ad-hoc 提供宽表查询支持，Doris 的谓词下推及 View 优化的能力为使用视图查询带来了便利。而当业务较重时，通过实体表单 + 微批任务进行实现，按照调度依赖关系逐层完成计算，针对使用场景对表单进行优化。</p><p></p><p>3. 数据计算时效：新架构下的数据时效受具体数据计算链路中的三个方面限制，分别是数据采集时效、批次计算时效、数据查询耗时。在不考网络吞吐、消息积压、资源抢占的情况下：</p><p></p><p>（实施类项目经常会遇到第三方不提供 Binlog 的情况，所以这里把通过批次采集数据也作为一个 case 列出来）</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0c/0c712fedf0cdc327a92cde2f8e47b2a7.png\" /></p><p></p><p>在 Doris 中为了达到更好的计算时效，基于 Doris 的数据计算流程相比在 Hive 中的计算流程可以进行一定的简化，这样可避免过多的冗余计算设计，以此提高计算产出效率。</p><p></p><p>4. 补充架构能力：</p><p></p><p>Hadoop：根据不同的项目资源及数据情况来决定是否引入 Hadoop 补充大规模离线计算场景。以实施类项目为例，Doris 可以涵盖大部分核心业务数据计算场景。MySQL：基于预计算的结果数据可以推送到下游 MySQL 中以供 Report 查询，从而分散 Doris 计算查询的资源消耗，这样可以将资源充分留给核心且时效性要求高的应用或高频批次任务。如果计算资源充足，Doris 也可以直接作为应用层的加速查询 DB，而无须引入其它 DB。</p><p></p><p></p><h3>新架构收益</h3><p></p><p></p><p>通过引入 Apache Doris，我们成功构建了高时效、低成本的数据中台 2.0，并成功满足了交付型项目和 SaaS 产品两种需求场景下的使用需求。新架构的收益如下：</p><p></p><p>数据时效性提升：架构 1.0 中大部分业务为 T+1 的支持方式，而在新架构下大部分业务都可实现实时或小时级计算支持。资源利用率提高：在架构 1.0 中，离线资源在白天大部分时间处于闲置状态。而在新架构下，数据同步、计算（增量 / 全量）和查询均在同一集群下完成，从而提高了资源利用率。相较于部署一套 CDH，同等资源成本下，部署一套 Doris 可以带来更多的收益。运维管理成本降低：在原有架构下，实时统计需求需要维护非常长的计算链路。而在新架构下，所有计算仅需在一个数据库中完成，更加简单、高效且易于维护。易于业务扩展：Doris 的节点扩展操作非常便捷，这对于业务的增量支持非常友好。</p><p></p><p></p><h2>新架构的落地实践</h2><p></p><p></p><p>我们在 2022 年底首次在测试环境中部署了 Apache Doris 1.1.5 版本，并进行了一些业务数据的导入测试和新架构的可行性验证。在测试后，我们决定在生产环境中落地实践 Apache Doris。第一次生产环境部署时，我们使用了当时最新的 1.2.2 版本。目前，新项目已升级到 1.2.4 版本并使用。Apache Doris 作为新架构下的核心系统，在整个架构中发挥着重要的作用。下面我们将从模型选择、资源规划、表结构同步、计算场景实现、运维保障等几个角度分享我们基于 Doris 的项目落地经验，希望为正在准备落地 Doris 方案的读者带来一些参考。</p><p></p><p></p><h3>模型选择</h3><p></p><p></p><p>数据模型我们主要应用了 Doris 提供的 Unique 模型和 Aggregate 模型。</p><p></p><p>Unique 模型</p><p></p><p>对于 ODS 层的表单来说，我们需要 Doris Table 与源系统数据保持实时同步。为了保证数据同步的一致性，我们采用了 Unique 模型，该模型会根据主键来对数据进行合并。在 1.2.0 版本之前，Unique 模型是 Aggregate 模型的一种特例，使用了 Merge On Read 的实现方式，这种实现方式下 count(*)的查询效率较低。而在 1.2.0 版本推出之后，采用了新的 Merge On Write 的数据更新方式，在 Unique Key 写入过程中，Doris 会对新写入的数据和存量数据进行 Merge 操作，从而大幅优化查询性能。在 Merge 过程中，Doris 会查找 Unique Key 索引，并使用 Page Cache 来优化索引查找效率。因此在使用 1.2 版本中，建议打开 Doris BE 的 Page Cache（在 be.conf文件中增加配置项 disable_storage_page_cache = false）。另外在很多情况下，Unique 模型支持多种谓词的下推，这样表单也可以支持从源表直接建立视图的查询方式。</p><p></p><p>Aggregate 模型</p><p></p><p>在某些场景下（如维度列和指标列固定的报表查询），用户只关心最终按维度聚合后的结果，而不需要明细数据的信息。针对这种情况，我们建议使用 Aggregate 模型来创建表，该模型以维度列作为 Aggregate Key 建表。在导入数据时，Key 列相同的行会聚合成一行（目前 Doris 支持 SUM、REPLACE、MIN、MAX 四种聚合方式）。</p><p></p><p>Doris 会在三个阶段对数据进行聚合：</p><p></p><p>数据导入的 ETL 阶段，在每一批次导入的数据内部进行聚合；底层 BE 进行数据 Compaction 的阶段；数据查询阶段。</p><p></p><p>聚合完成之后，Doris 最终只会存储聚合后的数据，这种明细表单数据的预聚合处理大大减少了需要存储和管理的数据量。当新的明细数据导入时，它们会和表单中存储的聚合后的数据再进行聚合，以提供实时更新的聚合结果供用户查询。</p><p></p><p></p><h4>资源管理</h4><p></p><p></p><p>在生产环境中，我们使用一套 Doris 数据仓库支撑了多个下游数据应用系统的使用。这些应用系统对数据访问的资源消耗能力不同，对应的业务重要等级也不相同。为了能够更好管理应用资源的使用，避免资源冲突，我们需要对应用账号进行划分和资源规划，以保证多用户在同一 Doris 集群内进行数据操作时减少相互干扰。而 Doris 的多租户和资源隔离功能，可以帮助我们更合理地分配集群资源。Doris 对于资源隔离控制有两种方式，一是集群内节点级别的资源组划分，二是针对单个查询的资源限制。这里主要介绍下集群内节点级别的资源组划分过程。</p><p></p><p>第一步：需要梳理规划各场景的用途、重要等级及资源需求等，举例说明：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/62/62544a947d667d41f7c84f1a5a70a0ca.png\" /></p><p></p><p>第二步：对节点资源进行划分、给节点打上 tag 标签：</p><p></p><p><code lang=\"sql\">alter system modify backend \"10.10.101.1:9050\" set (\"tag.location\" = \"group_a\");\nalter system modify backend \"10.10.101.2:9050\" set (\"tag.location\" = \"group_a\");\nalter system modify backend \"10.10.101.3:9050\" set (\"tag.location\" = \"group_b\");\nalter system modify backend \"10.10.101.4:9050\" set (\"tag.location\" = \"group_b\");\nalter system modify backend \"10.10.101.5:9050\" set (\"tag.location\" = \"group_c\");\nalter system modify backend \"10.10.101.6:9050\" set (\"tag.location\" = \"group_c\");\n</code></p><p></p><p>第三步：给应用下的表单指定资源组分布，将用户数据的不同副本分布在不同资源组内</p><p></p><p><code lang=\"sql\">create table flume_etl\n(k1 int, k2 int)\ndistributed by hash(k1) buckets 1\nproperties(\n    \"replication_allocation\"=\"tag.location.group_a:2, tag.location.group_b:1\"\n)\n\ncreate table cdc_etl<table></table><p></p><p></p><p><code lang=\"text\">\"replication_allocation\"=\"tag.location.group_b:2, tag.location.group_c:1\"\n</code></p><p></p><p>create table etl</p><table>\n</table><p></p><p></p><p><code lang=\"text\">    \"replication_allocation\"=\"tag.location.group_a:1, tag.location.group_c:2\"\n\ncreate table mkui_readonly</code></p><table></table><p></p><p></p><p><code lang=\"text\">\"replication_allocation\"=\"tag.location.group_a:2, tag.location.group_c:1\"\n</code></p><p></p><p>create table SaaS_readonly</p><table>\n</table><p></p><p></p><p><code lang=\"text\">    \"replication_allocation\"=\"tag.location.group_a:1, tag.location.group_b:1, tag.location.group_c:1\"\n\ncreate table dev</code></p><table></table><p></p><p></p><p><code lang=\"text\">\"replication_allocation\"=\"tag.location.group_a:1, tag.location.group_b:1, tag.location.group_c:1\"\n</code></p><p></p><p><code lang=\"text\">\n第四步：设置用户的资源使用权限，来限制某一用户的查询只能使用其指定资源组中的节点来执行。\n\n```sql\nset property for 'flume_etl' 'resource_tags.location' = 'group_a';\nset property for 'cdc_etl' 'resource_tags.location' = 'group_b';\nset property for 'etl' 'resource_tags.location' = 'group_c';\nset property for 'mkui_readonly' 'resource_tags.location' = 'group_a';\nset property for 'SaaS_readonly' 'resource_tags.location' = 'group_a, group_b, group_c';\nset property for 'dev' 'resource_tags.location' = 'group_b';\n</code></p><p></p><p>值得一提的是，与社区交流中我们得知在即将发布的 Apache Doris 2.0 版本中还基于 Pipeline 执行引擎增加了 Workload Group 能力。该能力通过对 Workload 进行分组管理，以保证内存和 CPU 资源的精细化管控。通过将 Query 与 Workload Group 相关联，可以限制单个 Query 在 BE 节点上的 CPU 和内存资源的百分比，并可以配置开启资源组的内存软限制。当集群资源紧张时，将自动 Kill 组内占用内存最大的若干个查询任务以减缓集群压力。当集群资源空闲时，一旦 Workload Group 使用资源超过预设值时，多个 Workload 将共享集群可用空闲资源并自动突破阙值，继续使用系统内存以保证查询任务的稳定执行。更详细的 Workload Group 介绍可以参考：<a href=\"https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/\">https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/</a>\"</p><p></p><p><code lang=\"sql\">create workload group if not exists etl_group\nproperties (\n    \"cpu_share\"=\"10\",\n    \"memory_limit\"=\"30%\",\n    \"max_concurrency\" = \"10\",\n    \"max_queue_size\" = \"20\",\n    \"queue_timeout\" = \"3000\"\n);\n</code></p><p></p><p></p><h4>批量建表</h4><p></p><p></p><p>初始化完成 Doris 的建表映射往往需要构建许多表单，而单独建表低效且易出错。为此，我们根据官方文档的建议使用 Cloudcanal 进行表结构同步来批量建表，大大提高了数据初始化的效率。</p><p></p><p>建表时需要注意的是：以 MySQL 为例，MySQL 数据源映射到 Doris 表结构的过程中需要进行一定的表结构调整。在 MySQL 中varchar(n) 类型的字段长度是以字符个数来计算的，而 Doris 是以字节个数计算的。因此，在建表时需要将 Doris varchar 类型字段的长度调整到 MySQL 对应字段长度的 3 倍。在使用 Unique 模型时需要注意建表时 UNIQUE KEY 列要放在 Value 列前面声明，且保证有序排列和设置多副本配置。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c1/c13e27c02b2c36bcbbd2a4333b081957.png\" /></p><p></p><p>除了以上方式，日前新发布的 Doris-Flink-Connector 1.4.0 版本中已集成了 Flink CDC、实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步功能，用户无需提前在 Doris 中建表、可以直接使用 Connector 快速将多个上游业务库的表结构及数据接入到 Doris 中。推荐大家尝试使用。相关链接：<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247521271&amp;idx=1&amp;sn=4f1a52806a06b8062cb206f586398463&amp;scene=21#wechat_redirect\">https://mp.weixin.qq.com/s/Ur4VpJtjByVL0qQNy_iQBw</a>\"</p><p></p><p></p><h4>计算实现</h4><p></p><p></p><p>根据我们对架构 2.0 的规划，我们将所有计算转移在 Doris 中完成。然而在支撑实时和准实时的场景下，具体的技术实现会有所不同，主要区别如下：</p><p></p><p>实时计算</p><p></p><p>如上文提到我们会以实时数据采集 + Doris 视图模型的方式提供实时计算结果，而为了在计算过程中达到更高的数据时效支持，应该尽量减少不必要的数据冗余设计。如传统数据仓库会按照 ODS-&gt;DWD-&gt;DWS-&gt;ADS 等分层逐层计算落表。而在实时计算场景下可以适当进行裁剪，裁剪的依据为整体查询时效的满足情况。此外，在实际的业务场景中也会有多层视图嵌套调用的情况。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4a/4a2ff7a3fa6fe36606a9ce5744ec2fc3.jpeg\" /></p><p></p><p>准实时计算</p><p></p><p>在业务能接受的准实时场景下（10 分钟、30 分钟、小时级），可以通过实体表单 + 微批任务实现计算，计算过程按照调度层级依赖关系逐层完成。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f7/f793f91a0d7267e6c7153378d45a2c4c.jpeg\" /></p><p></p><p>通过 Java UDF 生成增量 / 全量数据</p><p></p><p>在实际业务中，存在增量 / 全量的日、月、年等不同时间频度数据生成需求。我们通过 Doris 的 Java UDF 功能 (1.2 版本后支持) + 调度系统传参的方式实现了一套脚本动态的生成增量 / 全量及日、月、年等不同的指标汇总。</p><p></p><p>实现思路：</p><p></p><p>period_type：计算频度 D/W/M/Y 代表计算日、周、月、年run_type：INC（增量）/ DF（全量）是通过传递begin_date,end_datel 来筛选 business_date数据进行汇总。增量满足：begin_date(对应计算频度开始日期) &lt;= business_date &lt;= end_date （对应计算频度结束日期）全量满足：begin_date(写死一个业务最小日期) &lt;= business_date &lt;= end_date （对应计算频度结束日期）</p><p></p><p>基于以上思路实现etlbegindate函数来返回不同计算频度下增量、全量的 begin_date</p><p></p><p><code lang=\"null\">etlbegindate(run_type,period_type,end_date)\n</code></p><p></p><p>为了在统计不同频度时能够生成对应频度的识别id 字段，我们还需要实现一个periodid 函数</p><p></p><p><code lang=\"null\">periodid(period_type,business_date)\n</code></p><p></p><p>该函数的主要功能为：</p><p></p><p>period_type = ‘D’ 返回 business_date 所在日， ‘YYYYMMDD’ 格式的 period_id 字段period_type = ‘W’ 返回 business_date 所在周的起始日期， ‘YYYYMMDDYYYYMMDD’ 格式的 period_id 字段period_type = ‘M’ 返回 business_date 所在月，‘YYYYMM’ 格式的 period_id 字段period_type = ‘Y’ 返回 business_date 所在年，‘YYYY’ 格式的 period_id 字段</p><p></p><p>结合 etlbegindate 与 periodid 两个函数，假定当前时间为 2023 年 6 月 16 日则相应的实现如下：</p><p></p><p>SQL 脚本使用函数示例</p><p></p><p><code lang=\"sql\">  -- 示例 Demo\n  select ${period_type}                          as period_type -- 统计频度 D/W/M/Y\n        ,period_id(${period_type},business_date) as period_id   -- 时间频度 ID\n        ,count(goods_id)                         as goods_cnt   -- 商品数\n   where business_date &gt;= etlbegindate(${run_type},${period_type},${end_date})\n     and business_date &lt;= ${end_date}\ngroup by period_id\n</code></p><p></p><p>运行调度前参数配置：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/67/67dee8bc0ed5b95ef2a15b35c6436523.jpeg\" /></p><p></p><p>任务运行结果示例：W/M/Y 是的实现方式一致，只是数据返回的 period_id 格式会按照上文描述的格式输出。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7c/7cd4295f23d0f59b270e37fde12aee2b.png\" /></p><p></p><p>基于以上方法，我们高效地为公司 SaaS 产品构建了相应的数据指标库应用。</p><p></p><p>基于 Doris 的大表优化</p><p></p><p>我们的业务涉及基于用户页面访问和景区设备日志信息的统计分析业务，这类指标计算需要处理大量日志数据。接下来，我们将介绍如何利用 Doris 提供的功能对数据进行优化处理。</p><p></p><p>数据分区分桶：</p><p></p><p>Doris 支持两级分区，第一级叫做 Partition，支持 Range Partitioning 和 List Partitioning 两种分区策略。第二级分区叫做 Bucket，支持 Hash Partitioning 分区策略。</p><p></p><p>对于用户浏览行为的埋点事件表，我们按照时间做为分区（Range Partitioning）：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cc/ccfd903013913e3355d1d6213e9fd5f2.png\" /></p><p></p><p>在实际应用中，业务的历史冷数据可以按年进行分区，而近期的热数据可以根据数据量增幅按照日、周、月等进行分区。另外，Doris 自 1.2.0 版本后支持批量创建 RANGE 分区，语法简洁灵活。</p><p></p><p>从 Doris 1.2.2 版本开始，Doris 支持了自动分桶功能，免去了在分桶上面的投入，一个分桶在物理层面为一个 Tablet，官方文档建议 Tablet 大小在 1GB - 10GB 之内，因此对于小数据量分桶数不应太多。自动分桶的开启只需要建表时新增一个属性配置：</p><p></p><p><code lang=\"nginx\">DISTRIBUTED BY HASH(openid) BUCKETS AUTO PROPERTIES (\"estimate_partition_size\" = \"1G\"）\n</code></p><p></p><p>Like Query 和 SEQUENCE_COUNT 介绍</p><p></p><p>Like Query</p><p></p><p>在使用埋点日志数据进行漏斗分析时，需要针对某些特定 URL 数据进行汇总分析。这些 URL 中带有参数信息，以 String 或者 Varchar 类型存储为例，在计算过程中需要对含有特定参数的数据进行筛选。</p><p></p><p>根据该 Issue：<a href=\"https://github.com/apache/doris/pull/10355%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BA%86%E8%A7%A3%E5%88%B0\">https://github.com/apache/doris/pull/10355，我们了解到</a>\" Doris 对于 like/not like 有一定的优化处理，操作符可以下推到存储引擎进行数据过滤。因此在这个场景下，我们尝试使用 like 操作符对数据进行筛选处理。</p><p></p><p>另外在 Apache Doris 2.0 版本中将增加 Ngram BloomFilter 索引，使用该索引可以提升 Like Query 的性能，未来我们也将进行升级使用。Doris 提供了gram_size 和 bf_size两个参数进行配置，示例如下：</p><p></p><p><code lang=\"typescript\">CREATE TABLE `test_ngrambf` (\n  `id` int(11),\n  `str` varchar(32),\n  INDEX idx_str (`str`) USING NGRAM_BF PROPERTIES(\"gram_size\"=\"3\", \"bf_size\"=\"256\")\n) ENGINE=OLAP\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 10\nPROPERTIES (\n\"replication_num\" = \"1\"\n);\n\nmysql&gt; INSERT INTO test_ngrambf VALUES (1, 'hello world'), (2, 'ngram test');\nQuery OK, 2 rows affected (0.18 sec)\n{'label':'insert_fbc5d3eca7204d52_965ce9de51508dec', 'status':'VISIBLE', 'txnId':'11008'}\n\nmysql&gt; SELECT * FROM test_ngrambf WHERE str LIKE '%hel%';\n+------+-------------+\n| id   | str         |\n+------+-------------+\n|    1 | hello world |\n+------+-------------+\n1 row in set (0.03 sec)\n\nmysql&gt; SELECT * FROM test_ngrambf WHERE str LIKE '%abc%';\nEmpty set (0.02 sec)\n\nmysql&gt; SELECT * FROM test_ngrambf WHERE str LIKE '%llm%';\nEmpty set (0.04 sec)\n</code></p><p></p><p>下面对 Ngram BloomFilter 索引的原理作简要介绍：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/1d/1d1eede24fd54a1e6b6a65aee2baba48.jpeg\" /></p><p></p><p>假如将“hello world\"存入 Bloom Filter 中，将gram_size 配置为 3，这时会将\"hello world\"分为[\"hel\", \"ell\", \"llo\",...]分别进行存储，每个 gram 通过 N 个哈希函数 h1, h2, …, hn 映射到 Bloom Filter 中，对应的索引值设为 1。当处理where column_name like 'hel'这样的查询语句时，'hel'会经过相同哈希函数的映射和 Bloom Filter 进行比较，如果映射出的索引和 Bloom Filter 的索引的值都是 1，那么判断'hel'在 Bloom Filter 中（True Positive），但也存在一定概率会将本来不在 Bloom Filter 中的元素判断为在集合中（False Positive），比如上图中的'llm'，但将其判断为不在 Bloom Filter 中的元素（True Negative）一定不会存在，比如图中的过滤条件like 'abc'。</p><p></p><p>在实际使用 Ngram BloomFilter 索引时有一些注意事项：</p><p></p><p>使用 Ngram BloomFilter 索引时需要根据实际查询情况合理配置gram_size的大小。小的gram_size 支持搜索查询更多的 String，但是同时也带来更多数量的 ngram 和需要应用更多的哈希函数，这将会增大 False Positive 的概率。因为存在 False Positive 的可能性，Ngram BloomFilter 索引不能被用来处理 column_name != 'hello'或者 column_name not like '%hello%'这样使用负运算符的过滤条件。</p><p></p><p>SEQUENCE_COUNT</p><p></p><p>针对用户留存或漏斗分析等指标的计算，可以使用 Doris 提供的 SEQUENCE_COUNT(pattern, timestamp, cond1, cond2, ...) 函数。这里 pattern 参数用来指定用户一系列浏览行为的事件链，比如：</p><p></p><p><code lang=\"sql\">-- 计算用户浏览商品、加入购物车以及支付这一连串事件的数量\nSELECT SEQUENCE_COUNT('(?1)(?2)(?3)', timestamp, event = 'view_product', event = 'add_cart', event = 'pay') FROM user_event;\n</code></p><p></p><p>通过 SEQUENCE_COUNT 可以非常方便地计算我们指定的事件链的数量。</p><p></p><p>Doris Borker 的协同计算</p><p></p><p>业务中存在部分大数据量的历史数据统计需求，针对这部分需求我们进行了协同计算处理</p><p></p><p>FlinkCDC 读取 Binglog 实时同步数据到 Doris 明细表Doris 明细表会存储近 30 日热数据（需要进行 TTL 管理）Doris 每日通过 Borker Export 同步一份日增量数据至 HDFS，并加载至 Hive 中Hive 中储存所有明细数据，数据初始化生成计算结果在 Hive 中完成 Borker Load 至 DorisDoris 在生成结果数据时仅生成当前日期数据，每天的增量生成沉淀为历史结果当业务有需要时通过 Borker Export 加载 Hive 全量计算结果刷新 Doris 结果表当业务有基于此明细数据的新开发需求时，可在 Hive 中计算完成初始化结果至 Doris</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ee/ee75c10e7da9c898af5c538e1fb470c3.jpeg\" /></p><p></p><p>数据导出（Export）： Export 是 Doris 提供的一种将数据导出的功能。该功能可以将用户指定的表或分区的数据以文本的格式，通过 Broker 进程导出到远端存储上，如 HDFS 或对象存储（支持 S3 协议） 等。用户提交一个 Export 作业后，Doris 会统计这个作业涉及的所有 Tablet，然后对这些 Tablet 进行分组，每组生成一个特殊的查询计划。这些查询计划会读取所包含的 Tablet 上的数据，然后通过 Broker 将数据写到远端存储指定的路径中。</p><p></p><p>数据导入（Broker load）： Broker Load 是 Doris 的一种异步数据导入方式，可以导入 Hive、HDFS 等数据文件。Doris 在执行 Broker Load 时占用的集群资源比较大，一般适合数据量在几十到几百 GB 级别下使用。同时需要注意的是单个导入 BE 最大的处理量为 3G，如果超过 3G 的导入需求就需要通过调整 B roker Load 的导入参数来实现大文件的导入。</p><p></p><p>联邦查询在数据分析场景下的尝试</p><p></p><p>由于上游数据源较多，我们仅对常用的数据表单进行了数据仓库采集建模，以便更好地进行管理和使用。对于不常用到的数据表单，我们没有进行入仓，但业务方有时会临时提出未入仓数据的统计需求，针对这种情况，我们可以通过 Doris 的 Multi-Catalog 进行快速响应、完成数据分析 ，待需求常态化后再转换成采集建模的处理方式。</p><p></p><p>Multi-Catalog 是 Doris 1.2.0 版本中推出的重要功能。该功能支持将多种异构数据源快速的接入 Doris，包括 Hive、Iceberg、Hudi、MySQL、Elasticsearch 和 Greenplum 等。使用 Catalog 功能，可以在 Doris 中统一的完成异构数据源之间的关联计算。Doris 1.2.0 以后的版本官方推荐通过 Resource 来创建 Catalog，这样在多个使用场景下可以复用相同的 Resource。下面是 Doris 本地表与通过 Multi-Catalog 映射的远程表单组合完成关联计算的场景示例。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/13/13f2d649363ea304dbbdaca302a4aaf8.jpeg\" /></p><p></p><p>Multi-Catalog 带来的收益：</p><p></p><p>更高的灵活性：通过 Multi-Catalog，用户可以灵活地管理不同数据源的数据，并在不同的数据源之间进行数据交换和共享。这可以提高数据应用的可扩展性和灵活性，使其更适应不同的业务需求。高效的多源管理：由于 Multi-Catalog 可以管理多个数据源，用户可以使用多个 Catalog 来查询和处理数据，解决了用户跨库访问不便的问题，从而提高数据应用的效率。</p><p></p><p>社区中已经有非常多的伙伴基于 Multi-Catalog 功能落地了应用场景。另外如果要深度使用该功能，建议建立专门用于联邦计算的 BE 节点角色，当查询使用 Multi-Catalog 功能时，查询会优先调度到计算节点。</p><p></p><p></p><h2>运维保障</h2><p></p><p></p><p>守护进程</p><p></p><p>为了保障 Doris 进程的持续运行，我们按照 Doris 官网的建议在生产环境中将所有实例都的守护进程启动，以保证进程退出后自动拉起。我们还安装部署了 Supervisor 来进行进程管理，Supervisor 是用 Python 开发的一套通用的进程管理程序，可以将一个普通的命令行进程变为后台 Daemon 并监控进程状态，当进程异常退出时自动重启。使用守护进程后，Doris 的进程变成了 Supervisor 的子进程，Supervisor 以子进程的 PID 来管理子进程，并可以在异常退出时收到相应的信号量。</p><p></p><p>配置 Supervisor 时的注意事项：</p><p></p><p>通过 supervisorctl status查询出来的进程 id 不是 Fe、Be、Broker 的进程 ID，而是启动它们的 Shell 进程 ID。在 start_xxx.sh中会启动真正的 Doris 进程，因此才有了进程树的说法。stopasgroup=true ;是否停止子进程、killasgroup=true ;是否杀死子进程，需要保证这两个参数为true，否则通过 supervisorctl控制 Doris 的后台进程是无效的，这也是通过 Supervisor 守护 Doris 进程的关键。</p><p></p><p>配置完 Supervisor 后则通过守护进程的方式来管理 FE、BE、Borker……</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c8/c8b3dce068353993d20856b20246586b.png\" /></p><p></p><p>由于 Superviosr 自带的 Web UI 不支持跨机器管理，当多节点时管理非常不便，这里可以使用 Cesi 来对 Supervisor 进行合并管理：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/23/2361a432f225183e6cf8d3dad9014e8d.png\" /></p><p></p><p>Grafana 监控报警</p><p></p><p>关于 Doris 的运行监控，我们按照官网相关内容部署了 Prometheus 和 Grafana ，并进行监控项的采集。同时对于一些关键指标进行了预报警，利用企微 Bot 完成信息推送。</p><p></p><p>以下为测试环境示例图：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c6/c628dd2f9244375ea10c911b5a0fab46.jpeg\" /></p><p></p><p>集群 CPU 空闲情况:</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/74/74c0c645a0642a2979941259eb57cc57.jpeg\" /></p><p></p><p>集群内存使用情况: 之前发现集群存在内存泄露</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b7/b706513617764853c0f4670e22f69b3b.jpeg\" /></p><p></p><p>BDBJE 写入情况： 超过秒级可能会出现元数据写入延迟的问题</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/61/6176525d31586bcb065110058fb06479.jpeg\" /></p><p></p><p>开始调度运行的 Tablet 数量： 正常情况该值基本为 0 或个位数，出现波动的 Tablet 说明可能在进行 Recovery 或 Balance。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/11/1106e21cf06468e1fb5b1994c0003e75.jpeg\" /></p><p></p><p>除此之外，我们还使用 QPC/99th Latency……等指标来查看监测集群服务能力，建议可以在 Doris 监控的基础上额外加入集群机器的监控，因为我们的机器部署在 VM 中，曾经出现过硬盘问题、内存问题、网络波动、专线异常等情况，多一层报警机制就多一份稳定性保障。</p><p></p><p></p><h2>总结收益</h2><p></p><p></p><p>通过新架构的成功搭建，实现了以 Apache Doris 为核心数据仓库 + OLAP 引擎的使用方式（All in One），有效缩减了数据处理流程，大大降低了投递型项目的实施成本。在旧架构下，需要部署、适配、维护非常多的组件，无论是实施还是运维都会比较繁重。相比之下，新架构下的 Doris 易于部署、扩展和维护，组合方案也灵活多变。在我们近半年的使用时间内，Doris 运行非常稳定，为项目交付提供了强有力的计算服务保障能力。</p><p></p><p>此外，基于 Apache Doris 丰富的功能、完善的文档，我们可以针对离线和在线场景进行高效且细致的数据开发和优化。通过引入 Doris 我们在数据服务时效性方面也有了大幅提高，当前我们已经成功地落地了多个数据项目，并孵化出了一个基于 Doris 的 SaaS 产品。同时，Doris 拥有一个成熟活跃的社区，SelectDB 技术团队更是为社区提供了一支全职的技术团队，推动产品迭代、解决用户问题，也正是这强有力的技术支持，帮助我们更快上线生产，快速解决了我们在生产运用中遇到的问题。</p><p></p><p></p><h2>未来规划</h2><p></p><p></p><p>未来，我们将密切关注 Apache Doris 社区的发展，并计划构建基于 K8S 的 Doris 服务方式。在项目交付场景下，常常需要进行整套环境的部署。目前，我们已经在 K8S 上集成了 2.0 版本架构下除 Doris 以外的其他数据服务组件。随着 Doris 社区 2.0 版本全面支持 K8S 的计划，我们也会将方案集成到我们的新体系中，以方便后期的项目投递。除此之外，我们还将结合 Doris 的功能特性提炼基于 Doris 的数仓方法论，优化开发过程。Doris 是一个包容性非常好的存储计算引擎，而想要将原有的数据仓库开发内容全部适配在 Doris 上还需要不断的学习、优化与调整，以充分发挥 Doris 的优势，同时我们也将在此基础上沉淀一套与之相匹配的开发规范。</p><p></p><p>最后，我非常推荐大家使用 Apache Doris 作为数据项目的解决方案。它具有功能强大、包容性强、社区活跃、迭代更新快等优势，这些优势将助推你的项目达成目标。在此，我要感谢 Apache Doris 社区和 SelectDB 的同学们给予我们的支持和帮助，也祝愿 Apache Doris 社区越来越壮大。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174039&amp;idx=1&amp;sn=75e4852f03bb624621077065d78d0b15&amp;chksm=bdb84cc48acfc5d24605dfe5ae0cb94f405a328b302c86bef8bf869038b45d9127135bdbc6b8&amp;scene=21#wechat_redirect\">对话开源泰斗陆首群教授：中国开源发展应追求0到1的爆发性创新，而不是0到0的假创新</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174038&amp;idx=1&amp;sn=e13b8cd549f1eedf21b15c830f207599&amp;chksm=bdb84cc58acfc5d3145fcf6cc8f81b8ca8df7056c5896d70c264fab3dca6d023727dd1272e9a&amp;scene=21#wechat_redirect\">离职员工窃取源代码，半年狂赚1.5 亿；美团“1元现金”火速收购光年之外；53岁周鸿祎清华读博：重新学习做一个工程师｜Q 资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651173870&amp;idx=1&amp;sn=d10158444a733e6c32e3894f334d0fcf&amp;chksm=bdb84bbd8acfc2ab55d7f643521d4de0c22550f0f4b437f56d109df3ccbccd80989fb329d1ed&amp;scene=21#wechat_redirect\">对话用友王文京，探寻企业数智化的“密钥”</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651173791&amp;idx=1&amp;sn=cc5e81ca50f6844a5782fa95f1615aec&amp;chksm=bdb84bcc8acfc2da94af6136c9dd78e962f726320fe7ce20147ab6b7960e1b711a51d0692726&amp;scene=21#wechat_redirect\">Electron末日来了？又一应用将其抛弃！WhatsApp强制推行原生应用：速度更快、内存占用更少</a>\"</p><p></p><p></p><p></p><table>\n</table></code></p>",
    "publish_time": "2023-07-04 14:56:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "畅谈百年编程语言",
    "url": "https://www.infoq.cn/article/HPJJzCMZaj1PrgrqYRzG",
    "summary": "<p>某些软件开发者，或更普遍的是生意人，喜欢谈论百年编程语言，甚至是百年框架。</p><p>&nbsp;</p><p>这个话题非常大胆。我是说，软件开发发展到今天，<a href=\"https://codefol.io/posts/the-forty-year-programmer/\">差不多已经有 65 年了</a>\"。而框架就更年轻了。断定你的语言或框架能活到一百年，这是一个相当大的论断。</p><p>&nbsp;</p><p>你可以看看保罗·格雷厄姆（Paul Graham，美国程序员、风险投资家、博客作者和技术作家。他以 Lisp 方面的工作而知名）关于百年编程语言的<a href=\"http://www.paulgraham.com/hundred.html\">论断</a>\"。他的意思稍有不同：他所指的是一种语言，其后代将在 100 年后以或多或少可辨认的形式与我们同在。比起我，他更关注的是编程范例和特性。但是如果 C 接替了 Algol，他认为这没问题，他只是想知道 Algol 的哪些特性足够好，能够在转变中存活下来。</p><p>&nbsp;</p><p>我想知道 100 年以后，哪些语言还会存在，并且基本上还可以使用。</p><p>&nbsp;</p><p>人们流行这样的说法：“一种语言要想在一百年后仍然可用，它需要有良好的性能。”你将如何评估这样的说法？你要怎么开始呢？</p><p>&nbsp;</p><p>这并不是反问。让我们开始评估吧。</p><p>&nbsp;</p><p></p><h2>最古老的语言能告诉我们什么？</h2><p></p><p>&nbsp;</p><p>如果我们最古老的语言有 65 年的历史，那么就没有百年编程语言的例子。不过，我们有一些 60 多年的编程语言。我们可以从最古老的例子中看出什么？</p><p>&nbsp;</p><p>一种语言已经普及，就很难消亡，但是往往会产生变异。Fortran 的 2008 版并不完全像 Fortran77 或最初的 Fortran（1957 年）。但是感性链是非常连续的。这里面有许多共同点，这是可以辨认的。它有一个持续的编程社区。Fortran 在科学和数学界一直很受欢迎，直到今天也是。例如，Fortran 仍然是超级计算机编程的主要语言，在这种情况下，你为高性能付出了太多，你需要一种尽可能快的语言。比如说，Fortran 明显比 C 快得多。这种需求和开发者社区一直在推动着 Fortran 的发展。</p><p>&nbsp;</p><p>这就很有意思了：我们说一种百年编程语言时，并非指它的语法和特性的延续。我们要看的是一个持续的编程社区。Perl 是个有意思的例子。Perl 第 6 版（现在叫 Raku）旨在大幅改动这门语言，并且确实做到了，但花了很多年。Perl 5就这样被分裂成了一个单独的社区，Perl 6 则作为一种新语言持续发展。如果 Perl 6 很快就会完成大规模的重写，那么我们一定会将其称为 Perl 社区。Perl 1 和 Perl 2 看上去和 Perl 5 很不一样，没有人会争辩说我们不应该把 Perl 1 和 Perl 5 称为同一种语言。与 Perl 6 不同，它有一个相当连续的编程社区。用户（软件开发者）的分裂使得人们认为 Perl 6 实际上是一种不同的语言。</p><p>&nbsp;</p><p>LISP 是另一种古老的语言，它有很多很多的后代，如 Guile、Scheme、Clojure 和 Racket。它们从来都不是广受欢迎的语言，但从整体上看，它们并没有消亡的征兆。LISP 虽然很多小方言都已不复存在，但仍未消亡。较大的方言（如 CommonLISP）仍然保持着增长的势头，也没有真正消亡。所以：我们并不总是指单一语言的编程社区。你也许可以争辩说，例如，即使 Clojure 变得巨大，LISP 仍然是消亡的。但是，大多数人在谈论 LISP 时，并不是指非常具体的原始 LISP。就像 Perl 1，它的后代已经发生了巨大的变化，但它们仍然是非常容易识别的、同样的基本语言。更重要的是，如果你了解 LISP 的老方言，那么要掌握一种 LISP 的新方言就不会太难了。它们之间有很多共同点，可以让它们快速、顺畅地共享一个更大的开发者社区。</p><p>&nbsp;</p><p></p><h2>语言是如何消亡的？</h2><p></p><p>&nbsp;</p><p>“消亡”是什么？大多数情况下，我的意思就是“消亡”，正如拉丁语的消亡。有一些小而孤立的讲拉丁语的学术和宗教团体。但是，对于大多数人来说，这并不是一种普通的日常语言。</p><p>&nbsp;</p><p><a href=\"https://en.wikipedia.org/wiki/History_of_programming_languages\">维基百科关于编程语言历史的页面</a>\"，如果你还没有看过，那就值得一看。</p><p>&nbsp;</p><p>Algol（1958 年）已消亡。它有后代，但它们看起来不太像 Algol。所以：这就是一种语言消亡的一种方式。它完全可以被更好的后代吞噬，它们带走了它的整个用户群。“Algol 家族”中包含了远亲，如 Perl，以及使用较少的 Delphi 这样的分支。但它们也包括 C、C++、Visual BASIC 和 Java。这些都是非常流行的语言，早已吞噬了 Algol 的整个开发者社区。尤其是 C，它的诞生很早，也很受欢迎。这就意味着没有什么理由继续使用 Algol。</p><p>&nbsp;</p><p>有些语言从未真正确立。FLOW-MATIC 是格蕾丝・赫柏（Grace Hopper，1906 年 12 月 9 日～1992 年 1 月 1 日，美国海军准将及计算机科学家，世界最早一批的程序员之一，也是最早的女性程序员之一）编写 COBOL 之前的第一种语言。它影响了 COBOL，但它本身从未被广泛使用过。许多如今已消亡的语言都是如此：用户很少，社区也不大。</p><p>&nbsp;</p><p>如果你愿意，你可以包括很多“原始语言”。我指的是那些大多变异成其他语言的语言。FLOW-MATIC 变成了 COBOL，所以它的小用户群变成了 COBOL 用户。这就是很多语言的消亡方式：它们加入了另一种语言，它们的社区变成了那个语言的社区。</p><p>&nbsp;</p><p>“先驱者”和原始语言不必由同一批人设计。FACT（1959 年）现在被认为是 COBOL 的先驱，因为它影响了设计，它的社区变成了 COBOL 程序员。但那不是格蕾丝·赫柏的项目。不同的人，但是社区变成了 COBOL 程序员。所以这也不是谁编写了这种语言的问题。这是关于用户社区的去向。</p><p>&nbsp;</p><p>同样，C 也是以 BCPL 命名的。BCPL 用户（没有那么多）变成了 C 用户。但是 BCPL 是马丁・理察德（Martin Richards，1940 年 7 月 21 日 ～，计算机科学家，为 BCPL 编程语言的发明者，发明了 TRIPOS 操作系统）写的，而不是 C 的作者之一。这是关于用户社区，而不是语言作者或特性。</p><p>&nbsp;</p><p></p><h2>消亡意味着什么？</h2><p></p><p>&nbsp;</p><p>在我们深入讨论之前，我想先谈谈“消亡”一词在编程语言中的一个明显的、有点愚蠢的用法。</p><p>&nbsp;</p><p>编程语言是由人类编写的，是给人类的。它们既是人类的语言，也是人类的文化。它们恰好也能开发出实用的计算机程序。它们和其他人类语言或文化一样，也容易受到趋势的影响。</p><p>&nbsp;</p><p>当你听到<a href=\"https://codefol.io/posts/what-do-they-mean-when-they-ask-if-rails-is-dead/\">有人说“Rails 已经消亡”</a>\"时，你可以肯定他们并不是指“没有程序还在运行 Rails”或“没有公司还在使用 Rails”或“没有新项目用 Rails 编写”。这一切都很明显，很清楚，根本不真实。</p><p>&nbsp;</p><p>他们指的是“Rails 现在不再是‘’了，你应该学习其他东西。”这就意味着 Rails 的使用机会比其他语言、框架或技术要少。这也可能意味着，初创公司的创始人对其他事物的青睐程度要高得多。不管怎么说，这并不表示 Rails 不被使用或不能使用。他们的意思是它不再是“”。</p><p>&nbsp;</p><p>对于我们现在所讨论的一切，我们可以安全地忽略这种用法。的确如此。任何超过 25 年历史的语言都不再是的新语言，也不会被大肆宣扬。根据这种定义，对于一种具有百年历史的编程语言，其百分之百的绝对要求就是，它已经消亡。</p><p>&nbsp;</p><p></p><h2>是什么让语言变得孱弱？</h2><p></p><p>&nbsp;</p><p>语言不会一下子就消亡。随着最后几个日常使用该语言的用户相继去世，或者发现其他的日常语言时，它们就逐渐消亡了。</p><p>&nbsp;</p><p>它们日渐孱弱，直到基本上消亡，最后彻底消亡。</p><p>&nbsp;</p><p>是什么让语言变得孱弱？</p><p>&nbsp;</p><p>一件事就是社区分裂。在分裂之后，Perl 5 和 Raku 都要弱得多。Python 从版本 2 到版本 3 的过渡非常艰难，也让它失去了市场份额。Ruby 从 1.8 到 1.9 的过渡也类似，如果不是那么迟缓和戏剧化的话。当你做一些分裂社区的事情时，那么你的社区就会变得孱弱。在像 Perl 这样的极端情况下，它的社区已经缩小到足以消亡的地步。</p><p>&nbsp;</p><p>这也是语言倾向于被自己的后代杀死的原因之一。既然有了 C 语言，为什么还要继续使用 Algol？如果 FLOW-MATIC 的作者已经转而制作 COBOL，为什么还要继续使用 FLOW-MATIC 呢？同一种语言的后代往往对该语言的社区很有吸引力。</p><p>&nbsp;</p><p>更重要的是，语言社区倾向于离开，转而选择其他解决他们同样问题的工具。开发人员从 Perl 转向 Ruby 或 Python 语言，这些语言以类似的方式解决类似的问题。Python 和 R 之间在统计学方面的关系一直很紧张，这是因为它们都为统计学家解决了有用的问题。并不是很多人为了语言本身而采用它。他们有需要解决的问题，而语言可以解决这些问题。</p><p>&nbsp;</p><p>这并不只是一个新的竞争者出现时的问题。当一个旧的解决方案不再有效时，这也是一个问题。与 JBoss 等旧的 Java 框架相比，Ruby 和 Rails 曾被誉为一个非常简单的 Web 编程栈。但较新版本的 Rails 要求在面对安全威胁时不断加固框架，这就增加了部署的难度。Heroku 不再提供免费计划，也没有一个明显的 Heroku 替代品。因此，Rails 目前的部署比以往任何时候都要糟糕。Rails 在解决特定用户问题上的能力的确有所下降。这样就会削弱这个框架，也削弱了得益于 Rails 用户社区的 Ruby。</p><p>&nbsp;</p><p>如果语言不再像以前那样很好地解决问题，或者当有一个更好的竞争者来解决问题时，这种语言就会变得孱弱。</p><p>&nbsp;</p><p></p><h2>是什么让语言变得更强？</h2><p></p><p>&nbsp;</p><p>如果一种语言必须为用户解决问题，那就表明这个问题很重要。</p><p>&nbsp;</p><p>小众环境强化了一种语言。R 语言非常适合统计，并且在社区中有很大优势。大家都认为，你不会用它来写操作系统。</p><p>&nbsp;</p><p>Java 是最近流行的通用语言。之后的一切都成了这样或那样的小众语言。Python 擅长数学、科学和人工智能编程，Ruby 则擅长于动态 Web 应用。JavaScript 是浏览器内编程和一些相关的服务器端的主要拥有者。Fortran，曾经是通用的，现在在超级计算和某些高度数学化的应用中继续存在。C 语言在操作系统和驱动程序中退居一席之地，因此它现在被从一般应用程序编程中驱逐出去。</p><p>&nbsp;</p><p>小众的力量是强大的。</p><p>&nbsp;</p><p>我不会说“Java 是终极的通用语言，永远都是。”基于 65 年的历史，这样的声明似乎过于强烈了。Java 27 年的运行并没有告诉我们，将不会有其他流行的通用语言。它只是告诉我们，它们将相当罕见。</p><p>&nbsp;</p><p>这就说得通了。小众非常强大，要想成为真正的通用语言，就得避开它们。甚至 Java 在使用时也存在着一些重要的限制，例如，高内存利用率使得其不适合小型嵌入式处理器。或许“通用”这一描述即将消亡，被众多的小众语言所吞噬。</p><p>&nbsp;</p><p>也可能不是这样。这一切都是近期发生的历史，因此说它是永久的就太愚蠢了。我们现在还不清楚。</p><p>&nbsp;</p><p>不过，小众意味着该语言有重点和目的。小众语言之所以强大，是因为它能让用户社区知道在帮助谁，以及如何帮助他们。这也是围绕书籍、产品或许多其他事物的社区所需要的。</p><p>&nbsp;</p><p>编程语言具有相同的要求是有道理的。</p><p>&nbsp;</p><p></p><h2>那么，对于一门百年语言来说，性能是必要的吗？</h2><p></p><p>&nbsp;</p><p>我说过，我们会评估性能是否是百年语言的要求。</p><p>&nbsp;</p><p>让我们来评估一下。</p><p>&nbsp;</p><p>Fortran 是最古老、最繁荣的语言之一，它的生死取决于性能。所以这是“是”栏中的复选标记。</p><p>&nbsp;</p><p>LISP 是另外一种最古老的语言，它并不注重性能，但是由于其灵活性和易于实现而繁荣。所以，在“否”栏中有一个复选标记。</p><p>&nbsp;</p><p>但这当然不是投票。相反，Fortran 和它的用户社区所关心的是性能。LISP 的重点是灵活性和可表达性。</p><p>&nbsp;</p><p>性能是必要的吗？那得看情况了。这就是这种语言的意义所在吗？这就是它的关注点吗？</p><p>&nbsp;</p><p>性能并不是一件坏事。但如果你向 Fortran 社区询问，“你需要编译器实现更简单吗？”他们会说“不关心”。LISP 社区可能不会说他们不关心性能，但他们也不会回避低性能的有趣解释器。问题不在于“高性能好吗？”而在于“它在你的优先级列表中排名第几？”</p><p>&nbsp;</p><p>BASH，命令行 shell，性能相当差，并且一直如此。这不是它的卖点。相反，它的社区想要的是兼容性、稳定性和易用性，并希望它能随时随地地摆脱困境。如果你做了一个高性能的 bash 实现，那么只有几个人会使用它。至少，他们会使用它，除非它损害了其他目标之一。所以，我们假设的高性能 BASH 可能已经成为泡影，因为“普遍可用”和“100% 兼容”对其社区来说是如此重要。</p><p>&nbsp;</p><p></p><h2>优先级不能改变吗？</h2><p></p><p>&nbsp;</p><p>你可以争辩说，仅仅因为某些东西对早期用户来说不是优先考虑的，并不意味着它现在就不是优先考虑的。也许 Ruby 或 Python 的第一个版本可能很慢，但现在的一些用户或许更关心性能，而非可表达性。PHP 的初衷或许是为了方便部署，但它目前的 Laravel 用户可能想要一个更干净、更有表达性的核心语言，并愿意为此牺牲一些兼容性和可用性？</p><p>&nbsp;</p><p>这也不是没有可能。</p><p>&nbsp;</p><p>问题在于你的社区的延续性。很难赶走一个社区然后吸引另一个社区。 关于你的太多旧信息，现在都不正确。不好的信息会吸引你不想要的人，也会排斥想要你当前关注的人。因此，在一门语言中，修改你所能提供的东西可能是最危险的行为之一。这就是我们之前讨论过的社区分裂的根源。</p><p>&nbsp;</p><p>你可以做一些小的改变。额外的一点性能不会使任何人离开。但是有时你得衡量一下其中的优劣。</p><p>&nbsp;</p><p>Ruby 已经开始被更多的大公司使用。总的来说，这是一件好事。但这也是一个拐点，一个我们的权衡改变的地方。我们是否要减少可表达性，因为无限制的可表达性会给更大的团队带来麻烦？也许吧。如果这种改变是有效的，它可以增加 Ruby 开发者社区的规模和寿命。但是，像这样的改变是语言面临的最大的消亡风险之一。</p><p>&nbsp;</p><p>顺便说一下，这也是像 YJIT 和 TruffleRuby 这样的 Ruby 优化项目大多不推动限制 Ruby 可以做什么的原因之一。额外的性能不错，但可表达性是 Ruby 在早期获胜的关键。如果性能不影响可表达性，那就很好。并非每个地方都如此。但对于 Ruby 这样的小众语言来说，却是如此。</p><p>&nbsp;</p><p></p><h2>更快，更慢</h2><p></p><p>&nbsp;</p><p>对优先级的担忧也是为什么 Python 在 Python 2 向 Python 3 的过渡中花费如此漫长而痛苦的时间。Python 在向后兼容方面做了非常缓慢、慎重的改变。在极少数情况下，如果它破坏了向后兼容性，就会被认为是对用户社区的一种背叛。</p><p>&nbsp;</p><p>Ruby 的情况就好得多，因为 Ruby 并没有承诺太多的向后兼容性。Ruby 的承诺更像是，“我们保持了相当多的向后兼容性，但当我们想让语言变得更好时，有时会出现一些问题，你只需要处理这些问题。”Python 赢得了数学 / 科学社区，需要程序在 20 年后继续运行；Ruby 则赢得了 Web 编程社区，不管怎样，那里的标准每 5 年改变一次。这绝非偶然。</p><p>&nbsp;</p><p>另外，这并不意味着“Ruby 是对的，Python 是错的”，也不意味着相反。这并不是说“更适合 Web”就是好的，“更适合数学和科学”就是不好的。这是用另一种方式来说明小众有多强大：如果你可以在一件事上做得好，在另一件事上做得不好，你就可以在这一件事上做到极致，并击败在这两件事上都不出色的通用语言。</p><p>&nbsp;</p><p>赢得 Web 的小众语言得益于快速的变化，比如 Ruby。赢得数学 / 科学的小众语言得益于其缓慢的变化，如 Python。</p><p>&nbsp;</p><p>一门语言能够如此优秀，以至于能够战胜两种语言，也不是不可能的。但是，它最终还是要和快速变化和缓慢变化的后代进行竞争，因此，你会认为很难弥合这一差距。</p><p>&nbsp;</p><p>顺便说一下，Perl 也是如此。很久以前，Perl 是唯一的脚本语言。它仍然非常适用于总结大量文本文件。但是现在它正在与 Ruby（一个快速变化的后代）和 Python（一个缓慢变化的后代）进行竞争。Ruby 接管了 Perl 注定要失掉的 Web 小众社区。</p><p>&nbsp;</p><p>这并不限于脚本语言。由于各种原因，C++ 的变化比 C 快得多。C 受益于某些领域的缓慢变化。设备驱动程序编写者多久更改一次语言？他们喜欢稳定性。C++ 受益于其他领域的快速变化，如应用程序编程。从长远来看，这并不是 C 和 C++ 之间唯一的区别。但这是一个重要的问题。</p><p>&nbsp;</p><p></p><h2>对事实置若罔闻</h2><p></p><p>&nbsp;</p><p>这里有一种我们还没怎么谈过的语言：COBOL。COBOL 程序员不多，而且他们几乎完全是在维护旧的代码。然而，数以百万计的 COBOL 代码行保持着大量的银行基础设施的运行。它已经存在了几十年，几乎没有变化。在那些摆脱了 COBOL 的地方，他们通常通过机械地将 COBOL 翻译成另一种语言（基本上总是 Java）来实现。</p><p>&nbsp;</p><p>就运行的代码行数而言，COBOL 具有极大的活力和生命力。如果明天所有的 COBOL 代码都蒸发了，整个国际银行系统就会立即崩溃。</p><p>&nbsp;</p><p>对于将 COBOL 用于新项目的人来说，COBOL 几乎已经消亡了。编写的新 COBOL 项目非常非常少。</p><p>&nbsp;</p><p>所以，它是生存的还是消亡的呢？大部分都消亡了。</p><p>&nbsp;</p><p>现在没有一个大型的 COBOL 开发者社区。人们正在逐渐摆脱 COBOL。COBOL 就是如果你从不改变你的权衡，并且最终你的用户社区不关心你所提供的内容，那么将会发生的情况。改变就是冒着消亡的风险，要么是一点点，要么是很多。永不改变，终将消亡。</p><p>&nbsp;</p><p>不就是因为 COBOL 老了吗？不是。COBOL 比 Fortran 略微年轻，而 Fortran 在它的小众社区中出奇地健康和充满活力。</p><p>&nbsp;</p><p>最主要的是，这提醒我们，“这种语言不能消失，它被某公司使用”只能让你走到这里。C++ 在谷歌中得到了广泛的应用，但是这并不能成为其生存的唯一原因。Shopify 使用 Ruby，但是它的寿命只有这么长。</p><p>&nbsp;</p><p>你需要新的用户，新的承诺，新的血液。</p><p>&nbsp;</p><p></p><h2>承诺是如何实现的？</h2><p></p><p>&nbsp;</p><p>C 的旧承诺之一是像 PDP-11 计算机一样工作。虽然现在依然如此，但是对每个人来说，这已经不是最重要的事情了。说来也怪，早期的 LISP 也做了类似的事情，这就是为什么<a href=\"https://en.wikipedia.org/wiki/CAR_and_CDR#704_macros\">“列表头”/“列表尾”仍然被命名为“car”/“cdr”，以古老的 IBM 704 上的机器寄存器命名。</a>\"</p><p>&nbsp;</p><p>这些都是古老的语言，这些特性可以被认为是古老的承诺。</p><p>&nbsp;</p><p>LISP 最初需要一点性能来使其数学表达能力可行。最终，处理器变得更快，这一点被放弃了——在现代 LISP 的实现中，car 和 cdr 绝对不是简单的寄存器访问，而且没有人在乎。只要性能可以接受，这对他们的社区来说并不是一个重要的承诺。而“可接受”在当时是相当宽松的。例如，CommonLISP 比 C 语言慢得多，但大多数情况下，这没什么问题。</p><p>&nbsp;</p><p>你可能会认为，如果同样的变化，C 会比较困难。如果 C 承诺像处理器一样，那么当处理器发生变化时，它是怎么做的呢？较新的处理器使用 <a href=\"https://en.wikipedia.org/wiki/Single_instruction,_multiple_data\">SIMD 指令</a>\"，如英特尔 SSE 指令。这些指令进行大型阵列操作，而这是快速执行大型任务的核心。C 语言从未真正为它们添加任何类型的抽象。C 语言看起来仍然很像 PDP-11，它没有这种东西。</p><p>&nbsp;</p><p>但是我们经常不明白哪些承诺对我们的开发者社区是重要的。C 不需要成为最快的语言。否则像<a href=\"https://cvw.cac.cornell.edu/vector/coding_aliasing\">指针别名</a>\"这样的问题就会把它的早期用户赶到 Fortran 的怀抱中，而 Fortran 是一种更快的语言，没有这个问题。</p><p>&nbsp;</p><p>C 是一种足够快、足够令人愉悦的语言，它能够很好地兼顾特性和性能。老实说，在其最鼎盛的时候，没有多少竞争者，因此，它无需为自己的承诺和小众而担忧。现代 C 语言在其现代小众领域（操作系统、设备驱动、低级系统编程）中更多的是关于控制而不是性能。它的性能必须是可以接受的，但它主要是关于精确的逐位布局。如果它能减少几个 CPU 周期，就会有更多的 CPU 周期可供使用。因此，指针别名不会导致用 Fortran 编写设备驱动程序。</p><p>&nbsp;</p><p>承诺确实会随时间而改变。它们必须如此。这不是一个避免所有改变的问题。这是一个管理问题。</p><p>&nbsp;</p><p>承诺会改变多少？C 语言诞生于 1972 年，距今 50 年。这些年来，它所经历的所有变化，使它在成为一个百年语言的过程中走过了一半。它还很健康，很有活力。我想它会成功的。</p><p>&nbsp;</p><p></p><h2>那么，你如何坚持 100 年？</h2><p></p><p>&nbsp;</p><p>如果你希望你的语言能持续 100 年呢？如果你在一个你喜欢的语言社区，或者你是一个语言设计师呢？你会怎么做才能设计出一种百年的语言，成为时代的纪念碑？</p><p>&nbsp;</p><p>你不能只关注语言特性和句法。英语不断吸收新的词汇，这既不是它最大的优势，也不是它最大的弱点。重要的、持续的部分是说话者的社区。我们只关心这个社区的词汇。</p><p>&nbsp;</p><p>Java 或 Fortran 或 Ruby 或任何你喜欢的语言也是如此。</p><p>&nbsp;</p><p>为了保持一种语言的健康发展，你需要知道你对社区的承诺是什么。为什么会有这些承诺？这就是必须保持真实的东西。这个承诺的某些部分是复杂的：一个特定的新特性究竟是好是坏？承诺的某些部分是微不足道的，因为有时你与你的听众在这里的核心原因发生冲突。在 Fortran 中，他们不把可表达性看得比速度更重要。而在 Ruby 中，他们很重视。在 Rust 中，内存安全是最重要的。在 C 语言中，能够以奇怪的方式处理内存，远比安全更重要得多。</p><p>&nbsp;</p><p>这些优先级列表都没有错。它们定义了小众语言。</p><p>&nbsp;</p><p>这也意味着你需要小心“跳出你的小众”。你的小众就是你的优势。小心翼翼地离开它，而且有点胆怯。种种迹象表明，语言在将来会变得越来越小众，而非越来越少。</p><p>&nbsp;</p><p>这也意味着你遵循亨利・福特（Henry Ford，美国汽车工程师与企业家，福特汽车公司的建立者。亨利・福特是世界上第一位将装配线概念实际应用在工厂并大量生产而获得巨大成功者。亨利・福特不是汽车或是装配线的发明者，但他让汽车在美国真正普及化。这种新的生产方式使汽车成为一种大众产品，它不但革命了工业生产方式，而且对现代社会和美国文化起了巨大的影响）的各种著名建议时必须小心。“如果我问他们想要什么，他们会说要一匹更快的马。”“只要是黑色的，他们想要什么颜色都可以。”有时你可以高瞻远瞩，给你的社区一些他们没有想到的要求。但是，如果他们说他们不想要什么，请记住你处于危险的境地。对于世界上的每一个亨利·福特来说，有太多的人不是亨利·福特，而且他们都猜错了。</p><p>&nbsp;</p><p>这里重要的部分是要问，“我们的承诺是什么？”想一想这些承诺有多久。100 年后人们还需要你所提供的东西吗？100 年前他们需要什么？</p><p>&nbsp;</p><p>我并不是说这些都很容易。但重要的是，要意识到 100 年有多么漫长。</p><p>&nbsp;</p><p>作者简介：</p><p>&nbsp;</p><p>诺亚·吉布斯（Noah Gibbs），教授各种编程技巧。平时会参会发言，写书，举办研讨会。曾为硅谷的初创公司工作。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://codefol.io/posts/the-hundred-year-programming-language/</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/HdhHwuPQk4FCdPBpmdlP\">Azure CTO： Rust 已登陆 Windows 11 内核</a>\"</p><p><a href=\"https://www.infoq.cn/article/GFfVLVpkIGOcKYB85Opb\">比 Python 快 35000 倍！LLVM&amp;Swift 之父宣布全新编程语言 Mojo：编程被颠覆了</a>\"</p><p><a href=\"https://www.infoq.cn/article/ReR4ui073IN4Qdv7dPdg\">正在诞生的五种编程语言</a>\"</p><p></p>",
    "publish_time": "2023-07-04 15:11:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "使用Zig在arm64上引导Uber的基础设施",
    "url": "https://www.infoq.cn/article/Q0qY2KNVKiwDHjVyWqc9",
    "summary": "<p></p><p></p><h2>简&nbsp; &nbsp;述</h2><p></p><p></p><p>2021 年 11 月，我们决定评估 arm64 架构在 Uber 的可行性。我们的大多数服务是用 Go 或 Java 编写的，但我们的构建系统只能编译成 x86_64。现在，得益于开源合作，Uber 拥有了一个独立于系统的构建工具链，可以无缝地支持多种架构。我们使用这个工具链来引导 arm64 主机。本文将分享我们是如何着手去做这件事情的，以及我们早期的想法、遇到的问题、达成的一些成就和未来的方向。</p><p></p><p>我们从 2021 年 11 月开始使用专门的 Linux/x86_64 基础架构，而到了 2023 年 1 月，我们有：</p><p></p><p>用于生产环境服务器架构（x86_64 和 arm64）的 C++ 工具链，由 zig cc 提供支持；一些在 arm64 硬件上运行的核心基础设施服务，为未来的扩展提供了可能性。</p><p></p><p>让我们来看看我们是如何做到的。</p><p></p><p></p><h2>为什么要考虑 arm64 架构</h2><p></p><p></p><p>所有的主流云供应商都在 arm64 上投入巨资，再加上 arm64 与古老的 x86_64 相比所表现出来的平台优势（能耗、价格、计算性能），我们觉得很有必要认真考虑让 arm64 成为我们平台的一部分。</p><p></p><p>于是，我们开始尝试自己去探究。我们的第一个目标如下所述：</p><p></p><p></p><blockquote>在 arm64 架构上运行一个大型的应用程序，并对可能节省的成本进行度量。</blockquote><p></p><p></p><p>其中一个关键点是最小化运行和基准测试消耗多个核心的服务所需的工作量。我们找到了两种截然不同的方法：</p><p></p><p>在并行区域或现有区域中的独立集群提供基本的 arm64 支持，并在那里运行测试（实验质量）；让所有的核心基础设施都知道现在不止一种架构，然后像生成其他 SKU 一样生成 arm64 主机并测试应用程序。</p><p></p><p>考虑到最小化工作量是我们优先考虑的事项，所以第一个选项似乎更适合我们。毕竟，我们为什么要把时间和金钱投入到有可能被放弃的东西上呢？我们考虑运行一个“并行区域”，它具备 arm64 架构，但在其他方面与生产环境是分离的（并且质量要求更为宽松，方便我们快速前进）。</p><p></p><p>不久之后，我们有了一个更重要的支持 arm64 的理由：如果我们可以在 arm64 上运行工作负载，就可以让平台的能力多样化，从而让自己处于一个更有利的位置。于是，我们的使命变成了（直到今天仍是如此）：</p><p></p><p></p><blockquote>通过在 arm64 上部署一些生产应用程序来降低 Uber 的计算成本、增加容量多样性，以及使我们的平台现代</blockquote><p></p><p></p><p>我们最初是带着原型思维开始的，但现在却有了 180 度转变，形成了一个指导原则：</p><p></p><p></p><blockquote>没有 hack，所有的内容都在主线上（也就是说，没有长期的分支或补丁）。</blockquote><p></p><p></p><p>既然我们的核心基础设施需要提供一流的 arm64 支持，那么这个项目就很自然地被分成两个部分：</p><p></p><p>第一个任务是将包含了我们几乎所有基础架构代码的 Go 代码库编译成 arm64 二进制文件；修改与构建、存储、下载和执行代码相关的所有东西（构建主机、工件存储和调度器），让它们知道现在存在两种架构。</p><p></p><p>那么如何编译成 arm64 二进制文件？当然是直接在 arm64 主机上进行原生构建，或者通过交叉编译。我们有必要先来了解一下原生编译和交叉编译的差异和要求。</p><p></p><p></p><h2>原生编译和交叉编译的基础知识</h2><p></p><p></p><p>一些我们可能不太熟悉的术语：</p><p></p><p>二进制文件是由源代码编译而来的机器代码程序。工具链是将源代码编译为二进制文件所需的一组工具，通常包括预处理器、编译器、链接器等。密闭（hermetic）工具链是指无论在什么样的环境下，只要给定相同的输入，总是产生相同输出的工具链。这里的“密闭”是指它不使用来自主机的文件，并且包含编译文件所需的所有东西。主机（host）是指编译二进制文件的机器。目标平台（target）是指运行二进制文件的机器。在进行原生编译时，主机和目标是相同的平台（即操作系统、处理器架构和共享库是相同的）。在进行交叉编译时，主机和目标是不同的平台（例如，从 macOS arm64 (M1) 编译成 x86_64 Linux）。有时候，目标机器可能无法编译代码，但可以运行。例如，一块智能手表可以运行已编译的代码，但不能运行编译器，因此我们可以使用交叉编译器为手表编译程序。sysroot 是目标平台文件系统的归档。例如，特定于目标平台的头文件、共享库、静态库。通常是交叉编译工具链所必需的，下面将会讨论。aarch64 或 arm64 是指处理器架构。</p><p></p><p>下图显示了如何通过原生编译（左）和交叉编译（右）将源文件 main.c 编译成可执行文件。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ee/ee040c0875bbcf21add3558873cb5521.png\" /></p><p></p><p>图 1：输入文件 main.c 原生编译（左）或交叉编译（右）为 aarch64 架构。</p><p></p><p>原生编译只需要较少的配置和准备工作就可以使用，因为这是大多数编译器工具链的默认模式。从表面上看，我们可以在云供应商的平台上启动一些 arm64 虚拟机，并从那里开始引导我们的工具。但是，我们所有的服务器都使用相同的基础镜像，包括构建主机。基础镜像包含许多从 Go 代码库编译出来的内部工具。因此，我们遇到了一个先有鸡还是先有蛋的问题：如何为我们的第一个 arm64 构建主机编译工具？</p><p></p><p></p><h2>示例：使用 GCC 和 Clang 进行交叉编译</h2><p></p><p></p><p>让我们在 x86_64 Linux 主机上编译一个 C 文件，目标平台是 Linux aarch64：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7b/7b956703b9ab72ca7db4b389da276405.png\" /></p><p></p><p>GCC 调用目标平台特定的可执行文件（aarch64-linux-gnu-gcc），而 Clang 接受目标平台作为命令行参数（-target &lt;…&gt;）：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7d/7dfb80e01c1988aafed0c0395e3cf68e.png\" /></p><p></p><p>表面上看，用 GCC 和 Clang 交叉编译 C 源文件似乎很容易，但背后都发生了什么？</p><p></p><p>基于 LLVM 的 C/C++ 工具链</p><p></p><p>“clang”使用哪些文件来构建最终的可执行文件？我们来跟踪一下：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/55/555bfc70b538c6f49890a793acdaa91a.png\" /></p><p></p><p>以下是这些相关的文件：</p><p></p><p>（没有显示出来的）工具：C 编译器（Clang）和链接器（ld）。/usr/aarch64-linux-gnu/include 中的头文件。这些通常是 GNU C 库头文件。有些程序使用 Linux 内核的公共头文件，但本例中没有。头文件是特定于目标平台的。编译的、特定于目标架构的库：动态链接器 /usr/aarch64-linux-gnu/lib/ld-linux-aarch64.so.1；C 库，共享对象：/usr/aarch64-linux-gnu/lib/libc.so.6；程序加载器：crt.o。其他库：libgcc 和 libc_nonshared。</p><p></p><p>现在我们已经知道交叉编译器使用了哪些东西，我们可以将依赖项分为两类：</p><p></p><p>特定于主机的工具（编译器、链接器和其他与目标平台无关的程序）；特定于目标平台的库和头文件，它们是为目标平台编译最终程序所必需的。</p><p></p><p>Uber 需要支持以下这些目标平台：</p><p></p><p>Linux x86_64（带有 glibc 2.28）；Linux x86_64（带有 glibc 2.31）；Linux x86_64（带有 musl）；Linux arm64（aarch64，带有 glibc 2.31）；Linux arm64 （aarch64，带有 musl）。</p><p></p><p>在撰写本文时，GCC 和 LLVM 都不能交叉编译 macOS 二进制文件。因此，我们维护了一个专门的构建集群来编译 macOS 目标平台。交叉编译 macOS 目标平台是非常有必要的，但我们目前还没有做到这一点。</p><p></p><p>以下是我们目前支持的主机平台：</p><p></p><p>Linux x86_64：构建集群、DevPod 和开发者笔记本电脑；macOS x86_64：老一代 macOS 开发者笔记本电脑；macOS aarch64（Apple Silicon）：新一代 macOS 开发者笔记本电脑。</p><p></p><p>下图画出了主机工具链、sysroot 以及它们之间的关系，每个主机工具链（左）都可以使用任意特定于目标平台的 sysroot（右）：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/46/46939342f14321ea0fe012d0df3c96cc.png\" /></p><p></p><p>图 2：基于 LLVM 的工具链需要每个主机和目标平台的 tarball（“sysroot”）</p><p></p><p>为了支持这些主机和目标平台，我们需要维护 8 个压缩文件：3 个工具链（每个主机架构需要一个编译的 LLVM）和 5 个目标平台的 sysroot。一个典型的 LLVM 工具链需要 500 到 700MB（压缩包），一个典型的 sysroot 需要 100 到 150MB（压缩包）。在编译代码之前，加上其他工具，总共需要下载和解压约 1.5GB 的压缩文件。Linux x86_64 的 Go 1.20 工具链压缩包为 95MB，是编译代码所需的最大的下载文件。</p><p></p><p>为了完整起见，我们来看一下 GCC。你可能还记得之前提到 GCC 交叉编译器是 aarch64-linux-gnu-gcc，这意味着每个主机和目标平台都需要一个完整的工具链。因此，如果我们要使用基于 GCC 的工具链，就需要维护 35=15 个工具链。如果我们添加一个新的主机平台（例如 Linux aarch64）和两个目标平台（分别针对 x86_64 和 aarch64 的 Linux glibc.2.36），那么需要维护的压缩包数量将跃升至 4</p><p></p><p>7=28 个！</p><p></p><p>在购买 Bazel 工具链时，我们评估了 GCC 和基于 LLVM 的工具链。LLVM 更受青睐，因为它需要维护的压缩文件数量的增长是线性的（而不是 GCC 那样的二次幂增长）。但我们能做得更好吗？</p><p></p><p></p><h2>Zig 工具链</h2><p></p><p></p><p>Zig 采用了不同的方式：它对所有受支持的目标平台使用了相同的工具链。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b8/b815a9cc574fe2f9e75ec50a70111e18.webp\" /></p><p></p><p>它在编译时使用了哪些文件？如果我们跟踪一下会发现，它只使用了来自 Zig SDK 的文件（中间文件放在 /tmp 目录下）。主机系统没有受到任何影响，这意味着 Zig 是完全独立的。</p><p></p><p>为什么 Zig 能做到这样，而 Clang 却不能？Clang 和 Zig 之间主要的差异是什么？Zig 需要的依赖项与 Clang 一样，我们来看一下：</p><p></p><p>工具：C 编译器（Clang）和链接器（lld）。它们被静态地链接到 Zig 二进制文件中，对于 macOS，Zig 实现了自己的链接器。/usr/aarch64-linux-gnu/…中的头文件。Zig 捆绑了多个版本的 glibc、musl libc、linux 内核和其他一些头文件，并自动包含它们。编译好的特定于目标平台的库：动态链接器、glibc（多版本）、程序加载器。Zig 根据具体的平台在后台动态编译所有这些文件。其他库：libgcc 和 libc_nonshared。Zig 重新实现了这些库中的函数。</p><p></p><p>因此，Zig 可以用一个工具链编译所有受支持的目标平台。为了支持我们的 3 个主机和 5 个目标平台，我们需要从 <a href=\"https://ziglang.org/download\">https://ziglang.org/download</a>\" 下载 3 个 Zig tarball 文件：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/05/055cdf43e611a3216a317d664a1529fa.png\" /></p><p></p><p>图 3：每个主机平台需要 1 个工具链。同一工具链可以编译所有目标平台。</p><p></p><p>Zig 作者 Andrew Kelley 在他的博客中更详细地解释了 Zig 在 Clang 之上添加了哪些东西。不管我们希望支持多少个目标平台，只需要一个主机工具链，这是非常诱人的。</p><p></p><p>我们尝试做一些其他工具链无法做到的事情：在 Linux 机器上交叉编译和链接 macOS 可执行文件：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/31/31863c8772c7501c9e49f3c73e6c65c1.png\" /></p><p></p><p>尽管在 2021 年底，Zig 还只是一项未经验证的新技术，但一个主机平台一个 tar 包和交叉编译 macOS 目标的能力赢得了团队的青睐。我们开始使用 Zig，将 zig cc 整合到我们的 Go 代码库中。</p><p></p><p></p><h2>Bazel 与 Zig</h2><p></p><p></p><p>对于 Bazel 来说，只有一个 C++ 工具链（在本例中是 Zig SDK）是不够的：它还需要一些粘合代码，一个工具链配置。2022 年 2 月，Go 代码库对 zig cc 的初步支持是通过添加到一个配置标志来实现的：</p><p></p><p><code lang=\"xml\">bazel build –config=hermetic-cc &lt;…&gt;\n</code></p><p></p><p>最开始所有的东西都不正常，大部分的测试都无法构建通过，更不用说执行了。我们开始慢慢解决这些问题。到 2022 年 9 月，所有测试都通过了。自 2023 年 1 月起，Zig 工具链可以将 Uber Go 代码库中的所有 C 和 C++ 代码编译到 Linux 目标平台。</p><p></p><p>Uber 自 2022 年 4 月以来一直在运行 Zig 生成的二进制文件，因此我们对 Zig 信心满满。Bazel 和 Zig 之间的粘合代码最初放在 Adam Bouhenguel 的代码库 bazel-zig-cc 中，后来被 Motiejus Jakštys 克隆并进一步开发，最终转到了 <a href=\"https://github.com/uber/hermetic_cc_toolchain%E3%80%82\">https://github.com/uber/hermetic_cc_toolchain。</a>\"</p><p></p><p>因为与 Zig 软件基金会合作，我们可以寻求对我们来说重要的解决方案。Zig 的人帮助我们发现和修复 Go 和 Zig 中的问题。因为在 2021 年合作进展顺利，Uber 决定将合作关系延长到了 2023 年和 2024 年。Zig 软件基金会所做的所有工作都是开源的，这让更大社区从中受益。</p><p></p><p></p><h2>对 arm64 支持的进展</h2><p></p><p></p><p>等到工具链足够成熟，可以进行 arm64 平台编译，我们就开始在内部加强对 arm64 的支持。例如：</p><p></p><p>当开发人员在 Go 代码库中定义了 Docker 镜像（使用 rules_docker，它相当于 Dockerfile，只是是在 Bazel 中使用），CI 将编译 x86_64 和 arm64 的依赖代码，并且如果无法编译就不允许通过。我们将 Go 代码库中所有的 Debian 包编译到了 arm64 并发布，尽管它们中的大部分不是我们必需的。与 Docker 镜像类似，CI 确保它们可以编译到 arm64 和 x86_64。目前不可能在我们的 Go 代码库中声明一个不能编译到 arm64 的新的 Debian 包。</p><p></p><p>在能够将程序编译为 arm64 之后，我们开始采用所有可以存储、下载和执行原生二进制文件的系统。现在，我们有：</p><p></p><p>开发环境中的 arm64 主机，就像其他 x86_64 主机一样；运行在 arm64 主机上的几个核心基础设施服务（例如，内部构建的容器调度器和支配程序）；继续扩大 arm64 的使用和支持。</p><p></p><p>我们 2023 年的计划包括：</p><p></p><p>为 arm64 增加 Kubernetes 支持；在 Kubernetes 的 arm64 主机上运行面向客户的服务。</p><p></p><p></p><h2>Uber 有使用 Zig 语言吗</h2><p></p><p></p><p>可以说有，也可以说没有。例如，ermet_cc_toolchain 中的启动器是我们用 Zig 编写的。嵌入到可执行文件中的运行时库（compiler-rt）是用 Zig 编写的。总而言之，我们的大多数 Go 服务都涉及到了一点 Zig，并且是用 Zig 编写的工具链编译的。</p><p></p><p>尽管如此，我们还没有将用 Zig 编写的生产应用程序引入到我们的代码库中（虽然工具链已经完全设置好了），因为目前公司中只有少数人知道这门语言。</p><p></p><p></p><h2>总&nbsp; 结</h2><p></p><p></p><p>截止 2023 年 1 月 16 日，所有发布到生产环境的 C/C++ 代码都通过 hermect_cc_toolchain 进行编译。因为 Zig 现在是我们 Go 代码库的关键组成部分，因此 hermetic_cc_toolchain 的维护得到了财务（与 Zig 软件基金会的合作将到 2024 年底）和 Uber 员工工时的支持。</p><p></p><p>虽然可以在 arm64 硬件上运行我们的核心基础设施，但我们还没有准备好运行面向客户的应用程序。我们的下一步是在 arm64 上试验面向客户的应用程序，这样就可以测试它的性能并决定未来的方向。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.uber.com/en-SG/blog/bootstrapping-ubers-infrastructure-on-arm64-with-zig\">https://www.uber.com/en-SG/blog/bootstrapping-ubers-infrastructure-on-arm64-with-zig</a>\"</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171903&amp;idx=1&amp;sn=0c7373b44edd38c814e29506b04aadfb&amp;chksm=bdb8546c8acfdd7ac6f9ed08f58dfb2c630efdb7df5d316709140d4ae9da3063fb956fe33e95&amp;scene=21#wechat_redirect\">十七年来奇葩大崩溃！为不让OpenAI和谷歌白拿数据，Reddit 收取巨额API 费用还诽谤开发者，社区爆发大规模抗议</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171799&amp;idx=1&amp;sn=a4f1d9cbc97c09b2802f0bb40cc7f824&amp;chksm=bdb853848acfda92bd91ca750bd435146217a63514378afc7e76914859dd0ccfa7cdafe2529b&amp;scene=21#wechat_redirect\">“偷”代码建起公司、学历造假、6天拿下1亿美元却拖欠工资，这位AI独角兽CEO屡遭质疑后亲自回应了</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651171663&amp;idx=1&amp;sn=8e0970bed61e782fc3bc0175cc886341&amp;chksm=bdb8531c8acfda0ae65316bf7e837f96319f6b1d2b3f51a99caf87a216319255ba49eb48f3b8&amp;scene=21#wechat_redirect\">市值暴涨10519%，原来全世界搞大模型的企业都在给这位华人打工！</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-07-04 16:05:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯云发布AI原生向量数据库：已支撑腾讯视频、QQ浏览器等业务，提供10亿级向量检索能力",
    "url": "https://www.infoq.cn/article/ElrMobhxqr1JJIF5rXUK",
    "summary": "<p>7月4日，腾讯云正式发布AI 原生（AI Native）向量数据库Tencent Cloud VectorDB。该数据库能够被广泛应用于大模型的训练、推理和知识库补充等场景，是国内首个从接入层、计算层、到存储层提供全生命周期AI化的向量数据库。</p><p></p><p>向量数据库专门用于存储和查询向量数据，业界称之为大模型的“海马体”。据介绍，腾讯云向量数据库最高支持10亿级向量检索规模，延迟控制在毫秒级，相比传统单机插件式数据库检索规模提升10倍，同时具备百万级每秒查询（QPS）的峰值能力。</p><p></p><h2>腾讯云定义AI Native向量数据库</h2><p></p><p></p><p>大模型时代的到来，拥抱大模型成为企业的刚需。</p><p></p><p>向量数据库通过把数据向量化然后进行存储和查询，可以极大地提升效率和降低成本。它能解决大模型预训练成本高、没有“长期记忆”、知识更新不足、提示词工程复杂等问题，突破大模型在时间和空间上的限制，加速大模型落地行业场景。</p><p></p><p>统计显示，将腾讯云向量数据库用于大模型预训练数据的分类、去重和清洗相比传统方式可以实现10倍效率的提升，如果将向量数据库作为外部知识库用于模型推理，则可以将成本降低2-4个数量级。</p><p></p><p>值得关注的是，腾讯云重新定义了AI Native的开发范式，提供了接入层、计算层、存储层的全面AI化解决方案，使用户在使用向量数据库的全生命周期，都能应用到AI能力。</p><p></p><p>具体而言，在接入层，腾讯云向量数据库支持自然语言文本的输入，同时采用“标量+向量”的查询方式，支持全内存索引，最高支持每秒百万的查询量（QPS）；在计算层，AI Native开发范式能实现全量数据AI计算，一站式解决企业在搭建私域知识库时的文本切分（segment）、向量化（embedding）等难题；在存储层，腾讯云向量数据库支持数据智能存储分布，助力企业存储成本降低50%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0dbf43a6e941389655476d6588e5f5f6.png\" /></p><p></p><p>企业原先接入一个大模型需要花1个月左右时间，使用腾讯云向量数据库后，3天时间即可完成，极大降低了企业的接入成本。</p><p></p><p>据了解，腾讯云向量数据库的向量化能力（embedding）曾多次获得权威机构认可，2021年曾登顶MS MARCO榜单第一、相关成果已发表于NLP顶会ACL。</p><p></p><p>腾讯云数据库副总经理罗云表示，AI Native（AI原生）时代已经到来，“向量数据库+大模型+数据”，三者将产生“飞轮效应”，共同助力企业步入AI Native（AI原生）时代。</p><p></p><h2>腾讯云向量数据库助力数据接入效率提升10倍</h2><p></p><p></p><p>腾讯云向量数据库基于腾讯集团每日处理千亿次检索的向量引擎（OLAMA），经过腾讯内部海量场景的实践，数据接入AI的效率也比传统方案提升10倍，运行稳定性高达99.99%，目前已经应用在了腾讯视频、QQ浏览器、QQ音乐等30多款国民级产品中。</p><p></p><p>腾讯云向量数据库能有效助力产品提升运营效率。数据显示，使用腾讯云向量数据库后，QQ音乐人均听歌时长提升3.2%、腾讯视频有效曝光人均时长提升1.74%、QQ浏览器成本降低37.9%。</p><p></p><p>以腾讯视频的应用为例，视频库中的图片、音频、标题文本等内容使用腾讯云向量数据库，月均完成的检索和计算量高达200亿次，有效满足了版权保护、原创识别、相似性检索等场景需求。</p><p></p><p>大模型加速向量数据库进入飞速发展期，据东北证券预测，到 2030 年，全球向量数据库市场规模有望达到 500 亿美元，国内向量数据库市场规模有望超过600亿人民币。</p><p></p><p>向量数据库可以帮助企业更高效、便捷地使用大模型，将数据的价值释放到最大，随着大模型的不断发展和普及，AI Native向量数据库将成为企业数据处理的标配。</p>",
    "publish_time": "2023-07-04 16:14:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "安全地将 Netflix 迁移到 GraphQL",
    "url": "https://www.infoq.cn/article/UGGyOTPDIQGcBlrbD70o",
    "summary": "<p></p><blockquote>导读：本文介绍了 Netflix 在 2022 年将其移动应用程序迁移到GraphQL 的过程。他们采用了 AB 测试、Replay 测试和 Sticky Canaries 策略来确保安全地进行迁移。</blockquote><p></p><p>&nbsp;</p><p>在 2022 年，Netflix 的 iOS 和 Android 应用程序发生了重大变化。我们将 Netflix 的移动应用程序迁移到了 GraphQL，并实现了零停机时间，这涉及了从客户端到 API 层的全面改进。</p><p>&nbsp;</p><p>直到最近，我们的移动应用程序使用的是内部 API 框架 <a href=\"https://netflix.github.io/falcor/\">Falcor</a>\"。现在，它们使用了 <a href=\"https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2\">Federated GraphQL</a>\"，这是一种分布式的 API 方法，领域团队可以独立地管理和拥有特定部分的 API。</p><p>&nbsp;</p><p>在不中断数亿用户的情况下安全地进行这项工作是极具挑战性的，特别是考虑到所涉及的众多变化维度。本博文将分享我们在进行这次迁移时使用的广泛适用的技术（超出了 GraphQL 范畴）。今天我们将讨论的三种策略是 AB 测试、Replay（回放）测试和 Sticky Canaries（金丝雀发布）。</p><p>&nbsp;</p><p></p><h2>迁移细节</h2><p></p><p>&nbsp;</p><p>在深入讨论这些技术之前，让我们简要了解一下迁移计划。</p><p>&nbsp;</p><p>在使用 GraphQL 之前：API 团队实施和维护的单体式 Falcor API：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e1e80d851d72e9ae2f01f72323829f6.jpeg\" /></p><p></p><p>&nbsp;</p><p>在迁移到 GraphQL 之前，我们的 API 层由使用 <a href=\"https://netflix.github.io/falcor/\">Falcor</a>\" 构建的单体服务器组成。一个 API 团队同时维护 Falcor 框架的 Java 实现和 API 服务器。</p><p>&nbsp;</p><p></p><h2>阶段 1</h2><p></p><p>&nbsp;</p><p>在我们现有的单体 Falcor API 之上创建了一个 GraphQL Shim 服务。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/2463d987b053f980136dc3754d72f43b.jpeg\" /></p><p></p><p>&nbsp;</p><p>到 2020 年夏季，许多 UI 工程师已准备好开始使用 GraphQL。我们没有选择从头到尾进行完整的迁移，而是在现有的 Falcor API 之上创建了一个 GraphQL shim。GraphQL shim 使得客户端工程师能够快速切换到 GraphQL，解决客户端方面的问题，如缓存规范化，尝试不同的 GraphQL 客户端，并在不受服务器端迁移的阻碍下研究客户端性能。为了安全地启动第一阶段，我们使用了 AB 测试。</p><p>&nbsp;</p><p></p><h2>阶段 2</h2><p></p><p>&nbsp;</p><p>废弃 GraphQL Shim 服务和传统 API 单体，转而采用由领域团队拥有的 GraphQL 服务。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/2307f6cfae26c1113b4aec326f861b1e.jpeg\" /></p><p></p><p>&nbsp;</p><p>我们不希望传统的 Falcor API 永远存在，因此我们采用了 Federated GraphQL 来支持一个具有多个 GraphQL 服务器的单一 GraphQL API。</p><p>&nbsp;</p><p>我们还可以通过联合指令将 GraphQL Shim 的字段实现与 Video API 进行交换。为了安全地启动第二阶段，我们使用了 Replay 测试和 Sticky Canaries。</p><p>&nbsp;</p><p></p><h2>测试策略：概述</h2><p></p><p>&nbsp;</p><p>我们的测试策略受到两个关键因素的影响：</p><p>&nbsp;</p><p>功能需求与非功能需求幂等性</p><p>&nbsp;</p><p>如果我们正在测试数据准确性等功能需求，并且请求是幂等的，我们会依靠 Replay 测试。我们知道我们可以使用相同的查询和相同的输入进行测试，并始终期望得到相同的结果。</p><p>&nbsp;</p><p>对于请求非幂等字段的 GraphQL 查询或变更，我们无法进行 Replay 测试。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/6999ce731507e78148627c1f3ddd9a89.jpeg\" /></p><p></p><p>&nbsp;</p><p>在非功能需求（如缓存和记录用户交互）的情况下，我们肯定无法进行 Replay 测试。在这种情况下，我们并不测试响应数据，而是整体行为。因此，我们依赖于基于更高层次指标的测试：AB 测试和 Sticky Canaries。</p><p>&nbsp;</p><p>让我们详细讨论一下这三种测试策略。</p><p>&nbsp;</p><p></p><h2>工具：AB 测试</h2><p></p><p>&nbsp;</p><p>Netflix 传统上使用 AB 测试来评估新产品功能是否符合客户的需求。在第一阶段，我们利用 AB 测试框架将一个用户群体分为两组，总共 100 万用户。对照组的流量使用传统的 Falcor 堆栈，而实验组利用新的 GraphQL 客户端，并指向 GraphQL Shim。为了确定对客户的影响，我们可以比较各种指标，如错误率、延迟和渲染时间。</p><p>&nbsp;</p><p>我们设置了一个客户端 AB 实验，测试 Falcor 与 GraphQL，并报告了粗粒度的用户体验指标（quality of experience metrics，QoE）。AB 实验结果表明，GraphQL 的正确性达不到遗留系统的水平。在</p><p>&nbsp;</p><p>接下来的几个月，我们深入研究了这些高级指标，并修复了缓存 TTL、有缺陷的客户端假设等问题。</p><p>&nbsp;</p><p></p><h3>优势</h3><p></p><p>&nbsp;</p><p>高级健康指标：AB 测试为我们的整体客户端 GraphQL 实现提供了所需的保证。这帮助我们在 6 个月内成功将移动首页画布上 100% 的流量迁移到 GraphQL。</p><p>&nbsp;</p><p></p><h3>注意事项</h3><p></p><p>&nbsp;</p><p>错误诊断：通过 AB 测试，我们可以看到粗粒度的指标，指出潜在的问题，但很难诊断出具体问题。</p><p>&nbsp;</p><p></p><h2>工具： Replay 测试 - 大规模验证！</h2><p></p><p>&nbsp;</p><p>迁移的下一个阶段是在一个以 GraphQL 为主的服务器（Video API Service）中重新实现我们现有的 Falcor API。Falcor API 已经成为一个逻辑复杂的单体，积累了十多年的技术债务。因此，我们必须确保重新实现的 Video API 服务器没有错误，并且与已经产品化的 Shim 服务完全相同。</p><p>&nbsp;</p><p>我们开发了一个 Replay 测试工具，以验证幂等的 API 是否从 GraphQL Shim 正确迁移到 Video API 服务中。</p><p>&nbsp;</p><p></p><h2>它是如何工作的？</h2><p></p><p>&nbsp;</p><p>Replay 测试框架利用 GraphQL 联合中提供的 @override 指令。该指令告诉 GraphQL 网关将请求路由到一个 GraphQL 服务器而不是另一个。例如，以下是 Shim 服务和 Video 服务定义的两个 GraphQL 模式：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a2b5298646b145c7c6355bb08a53194f.jpeg\" /></p><p></p><p>&nbsp;</p><p>在第一阶段，GraphQL Shim 首先定义了 certificationRating 字段（例如 Rated R 或 PG-13）。在第二阶段，我们建立了 VideoService，并定义了带有 @override 指令的相同 certificationRating 字段。具有 @override 指令的相同字段的存在告知 GraphQL 网关将此字段的解析路由到新的 Video Service，而不是旧的 Shim Service。</p><p>&nbsp;</p><p>Replay 测试工具从 <a href=\"https://netflix.github.io/mantis/\">Mantis</a>\" 中抽样原始流量。通过这些抽样事件，该工具可以捕获来自生产环境的实时请求，并对 GraphQL Shim 和新的 Video API 服务同时运行相同的 GraphQL 查询。然后，该工具比较结果并输出响应负载中的任何差异。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/61b35100b008fd71d3b8b45a8cac286e.jpeg\" /></p><p></p><p>&nbsp;</p><p>注意：我们不会对个人身份信息进行 Replay 测试。它仅用于 Netflix 用户界面上的非敏感产品功能。</p><p>&nbsp;</p><p>测试完成后，工程师可以查看以展开的 JSON 节点形式显示的差异。你可以在逗号左侧的括号中看到对照值，而在右侧则是实验值。</p><p>&nbsp;</p><p><code lang=\"null\">/data/videos/0/tags/3/id: (81496962, null)\n/data/videos/0/tags/5/displayName: (Série, value: “S\\303\\251rie”)</code></p><p>&nbsp;</p><p>我们在上述情况中捕获了两个差异，第一个差异是实验中缺少了一个 ID 字段的数据，第二个差异是编码不同。我们还注意到了本地化、日期精度和浮点数精度的差异。这让我们对复制的业务逻辑充满信心，其中订阅计划和用户地理位置确定了客户的目录可用性。</p><p>&nbsp;</p><p></p><h3>优势</h3><p></p><p>&nbsp;</p><p>对两种 GraphQL 实现之间的一致性充满信心。在数据由于过于急切的超时而丢失的情况下，能够进行调优配置。测试了需要许多（未知）输入和难以直观判断正确性的业务逻辑。</p><p>&nbsp;</p><p></p><h3>注意事项</h3><p></p><p>&nbsp;</p><p>不应使用 Replay 测试来测试包含个人身份信息（PII）和非幂等 API，因此有必要有一种机制来防止这种情况发生。手动构建的查询只能测试开发人员记得测试的功能。由于遗忘，我们最终会有一些未经测试的字段。正确性：正确性的概念也可能令人困惑。例如，对于一个数组来说，空数组更正确还是 null 更正确，或者只是噪音？最终，我们尽可能与现有行为保持一致，因为验证客户端错误处理的鲁棒性很困难。</p><p>&nbsp;</p><p>尽管存在这些缺点， Replay 测试仍然是我们实现大多数幂等查询的功能正确性的关键指标。</p><p>&nbsp;</p><p></p><h2>工具：Sticky Canary</h2><p></p><p>&nbsp;</p><p>虽然 Replay 测试验证了新的 GraphQL API 的功能正确性，但它并不提供性能或业务指标的洞察，例如用户交互的整体感知健康状况。用户的点击率是否相同？加载是否及时，以免用户失去兴趣？ Replay 测试也不能用于非幂等 API 的验证。为了增加信心，我们使用了 Netflix 的一种工具，称为 Sticky Canary。</p><p>&nbsp;</p><p>Sticky Canary 是一种基础设施实验，其中在整个实验期间，客户分配给金丝雀或基线主机。所有传入的流量根据设备和配置文件分配给实验或基线主机，类似于桶哈希。实验主机部署为分配给实验的所有客户提供服务。您可以观看我们在亚马逊云科技 Reinvent 的<a href=\"https://www.youtube.com/watch?v=Xbn65E-BQhA\">混沌工程</a>\"演讲，了解更多关于 Sticky Canary 的信息。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85f65d2f9181dd641855acf7689004b1.jpeg\" /></p><p></p><p>&nbsp;</p><p>在我们的 GraphQL API 案例中，我们使用了 Sticky Canary 实验来运行两个 GraphQL 网关实例。基线网关使用现有的模式，将所有流量路由到 GraphQL Shim。实验网关使用新的提议模式，将流量路由到最新的 Video API 服务。我们的主要边缘网关 Zuul 根据实验参数将流量分配给两个集群之一。</p><p>&nbsp;</p><p>然后，我们收集并分析两个集群的性能。我们密切监测的一些关键绩效指标包括：</p><p>&nbsp;</p><p>中位数和尾部延迟错误率日志资源利用率 - CPU、网络流量、内存、磁盘设备 QoE（用户体验质量）指标流媒体健康度指标</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/35fc90898de71ea6d0126cb7c09a466b.jpeg\" /></p><p></p><p>&nbsp;</p><p>我们从小规模开始，将客户的分配量控制在一个小时的实验范围内。在验证性能后，我们逐渐扩大范围。我们增加了客户分配的百分比，引入了多区域测试，并最终进行了长达 12 小时甚至整天的实验。在整个过程中进行验证非常重要，因为 Sticky Canary 会对实际生产流量产生影响，并持续地分配给特定客户。</p><p>&nbsp;</p><p>经过多次 Sticky Canary 实验，我们确信第二阶段的迁移改进了所有核心指标，可以放心地在全球范围内推广 GraphQL。</p><p>&nbsp;</p><p></p><h3>优势</h3><p></p><p>&nbsp;</p><p>Sticky Canary 对于建立对我们新的 GraphQL 服务的信心至关重要。</p><p>&nbsp;</p><p>非幂等 API：这些测试与具有变异或非幂等特性的 API 兼容。业务指标：Sticky Canary 验证了我们在迁移后的核心 Netflix 业务指标的改善。系统性能：对延迟和资源使用情况的了解帮助我们理解迁移后扩展配置的变化。</p><p>&nbsp;</p><p></p><h3>注意事项</h3><p></p><p>&nbsp;</p><p>对客户造成负面影响：Sticky Canary 可能对真实用户产生影响。在将某些客户持续路由到新服务之前，我们需要对新服务有足够的信心。这在一定程度上通过实时影响检测得到缓解，该检测将自动取消实验。短暂的：Sticky Canary 适用于短暂的实验。对于长期的测试，应使用全面的 AB 测试。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>技术不断变化，作为工程师，我们在职业生涯中的大部分时间都在进行迁移。问题不在于我们是否进行迁移，而在于我们是否能够安全、无停机时间地及时进行迁移。</p><p>&nbsp;</p><p>在 Netflix，我们开发了一些工具，确保对每个特定的测试用例进行迁移的信心。我们介绍了 AB 测试、Replay 测试和 Sticky Canary 这三个用于 GraphQL 迁移的工具。</p><p>&nbsp;</p><p>作者介绍：</p><p>&nbsp;</p><p>Jennifer Shin、Tejas Shikhare、Will Emmanuel 均为 Netflix 高级软件工程师。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/k8XQcQM2JveNMv9Djsa5\">GraphQL vs REST API 架构，谁更胜一筹？</a>\"</p><p><a href=\"https://www.infoq.cn/article/eXqi29lbAtgrSHLuBQi7\">GraphQL：API 的未来</a>\"</p><p><a href=\"https://www.infoq.cn/article/0fyZC6Yqvj6Ddr4XEHER\">为什么你的下一个 API 应该是 GraphQL 而不是 REST</a>\"</p>",
    "publish_time": "2023-07-04 16:23:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "突发！美国将限制中国企业使用亚马逊、微软等云服务",
    "url": "https://www.infoq.cn/article/px8H2Elp4mR9lUpWteGD",
    "summary": "<p>据<a href=\"https://www.reuters.com/technology/us-set-restrict-chinas-access-cloud-computing-wsj-2023-07-04/\">《华尔街日报》</a>\"援引知情人士消息称，拜登政府正准备限制中国企业使用美国云计算服务。</p><p>&nbsp;</p><p>据悉，如果这项新规定被采纳，可能会要求<a href=\"https://xie.infoq.cn/article/7a13971bbd3bcae4bb5915840\">亚马逊</a>\"(Amazon)和<a href=\"https://www.infoq.cn/article/9wzrGiMTl8XClRgSfL9A\">微软</a>\"(Microsoft)等美国云服务提供商在向中国客户提供使用先进人工智能芯片的云计算服务之前，必须获得美国政府的许可。</p><p>&nbsp;</p><p>美国商务部预计将在未来几周内实施该限制政策，该政策是作为 10 月份推出的半导体出口管制政策扩展的一部分。此前，美国要求禁止英伟达A100等高端人工智能芯片出口至中国，但是企业仍然可以通过云计算的方式在AWS、微软Azure等平台上获取A100等人工智能芯片提供的算力。</p><p>&nbsp;</p><p>最近，还有国外媒体报道称，美国和荷兰今夏将进一步限制芯片制造设备销售，荷兰正计划对该国最大企业阿斯麦(ASML)和其他公司的某些设备施加限制。</p><p>&nbsp;</p><p>昨天，商务部、海关总署宣布，对镓、锗相关物项实施出口管制。由于这些金属材料广泛用于半导体制造，并且中国是市场上主要的供应商，因此此举也被解读为中国对欧美半导体管制政策的反击。</p><p>&nbsp;</p><p>美国商务部、微软和亚马逊没有在工作时间之外立即回应置评请求。</p><p></p><p>参考链接：</p><p><a href=\"https://www.reuters.com/technology/us-set-restrict-chinas-access-cloud-computing-wsj-2023-07-04/\">https://www.reuters.com/technology/us-set-restrict-chinas-access-cloud-computing-wsj-2023-07-04/</a>\"</p>",
    "publish_time": "2023-07-04 18:15:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "进击的云数仓 3.0 ：探秘华为云 GaussDB(DWS) 厚积薄发之路",
    "url": "https://www.infoq.cn/article/RRdAHG6OAomzetSME9dq",
    "summary": "<p>数字时代，如何更进一步释放数据价值、推动业务增长，已成为各行业共同面临的挑战。作为数据分析、处理的一项关键技术——「数据仓库」逐渐成为了驱动行业发展的重要引擎。本期华为云和 InfoQ 联合出品的《探秘云新知丨GaussDB(DWS) 》，将带你探访华为云 GaussDB(DWS) ，了解最前沿的数仓技术与场景实践！</p>",
    "publish_time": "2023-07-04 18:30:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]