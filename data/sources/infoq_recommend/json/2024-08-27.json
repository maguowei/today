[
  {
    "title": "融到2.2 亿美元才3个月就“闹崩”！5个创始人走了3个，这家 DeepMind 系创企一款AI产品都还没发！",
    "url": "https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0",
    "summary": "<p>整理 | 华卫</p><p></p><p>日前，法国人工智能 （AI） 初创公司 H 表示，由于“运营分歧”，其三名联合创始人 Daan Wierstra、Karl Tuyls 和 Julien Perolat 将离开公司。在 5 月 21 日宣布品牌重塑之前，H 被称为 Holistic AI。</p><p></p><p>H 在 LinkedIn 上的一篇帖子中表示，“公司将由首席执行官 Charles A. Kantor 和首席技术官 Laurent Sifre 领导。虽然这对所有相关方来说都是一个艰难的决定，但大家都一致认为，这将使公司在未来取得最大的成功。H 将继续得到投资者和战略合作伙伴的全力支持。”</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ae/ae7a78a1f304efa0b824ad3475aebba8.png\" /></p><p></p><p>据悉，这三位联合创始人离开之际， H 筹集了 2.2 亿美元的种子轮融资后仅三个月，还未发布过任何产品。而 H 是在今年早些时候成立，计划在今年年底前发布一系列模型和产品。</p><p></p><p>“当 H 成立时，团队着手通过新一代动作模型将 GenAI 的力量推向全球人民和企业。”该公司在帖子中表示。“今天，H 的近 40 名工程师和研究人员团队仍然致力于这一愿景，开发尖端的动作能力，以提高工人的生产力并推动 AI 研究和工程的前沿。”</p><p></p><p>五人创始团队悄然“分家”，</p><p></p><p>无一人对此回应</p><p></p><p>根据该公司的 LinkedIn 帖子，H 现在拥有一支由 40 名工程师和研究人员组成的团队。相比之下，另一家资金雄厚的人工智能公司 Mistral AI 在招聘方面要保守得多。初成立之时， Mistral 除三位联合创始人外只有 3 名成员，团队总人数不到 10 人。</p><p></p><p>创立之初， H 有五位联合创始人，其中一位联合创始人兼该公司现任首席执行官 Charles A. &nbsp;Kantor 是斯坦福大学的计算数学研究员，而其他四位联合创始人都是谷歌旗下的人工智能公司 DeepMind 的资深科学家出身。</p><p></p><p>Karl Tuyls 是多智能体系统社区的著名科学家，自 2017 年起领导 DeepMind 的博弈论和多智能体团队，发起并领导了 DeepNash（一款在 Stratego 上击败人类专家玩家的自主智能体）和 TacticAI（一款角球自动助理足球教练）等多个著名项目，这些项目均发表在《科学》和《自然》杂志上。</p><p></p><p>Laurent Sifre 曾是 DeepMind 的首席科学家，在 DeepMind 工作了十年，为 AlphaGo、AlphaFold 和 AlphaStar、Chinchilla、Gemini 和 Gemma 等 GenAI 和深度神经网络的关键研究项目做出了贡献 。</p><p></p><p>Daan Wierstra 是 DeepMind 的一名高级计算机科学家，在 DeepMind 被谷歌收购之前就加入了该公司。在 DeepMind，Daan 曾领导了一支 100 多人的团队多年，并在 Deepmind 确立最初的研究方向方面发挥了关键作用。</p><p></p><p>Julien 从事博弈论和多智能体研究，共同领导了 DeepMind 在 Stratego 游戏（DeepNash）方面的科学和技术开发，以及在平均场博弈和基于人群的学习等主题上的许多其他基础性工作。</p><p></p><p>现在，从谷歌 DeepMind 转投 H 的四位联合创始人中的三位都将离开该公司。自人工智能爆火以来，业内闹过“分家”的知名 AI 企业不在少数。最近令不少人都仍记忆犹新的一家便是 OpenAI 了，11 人创始团队分崩离析至仅剩两人，分裂过程中内部发生多起“政变”。从去年 11 月首席执行官山姆·奥特曼（Sam Altman）被罢免以来，OpenAI 已经陷入大半年的“人事斗争”。</p><p></p><p>但 H 公司的“分家”却不同于此，除官方在 LinkedIn 上发布的公告帖以外，此前并未有任何公开的讨论和发言，连三位处于风波中心、将离开的联合创始人也未曾有过任何相关回应。</p><p></p><p>Tuyls 最近提及 H 的社交内容更新停留在 5 月 26 日，从其当时的状态看，他本人还沉浸在 H 公司成立的喜悦中，之后发布的帖子也未透露出要离开 H 或产生公司业务分歧的迹象。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8b/8b00b4b830fc8e4500355044c08e40e8.png\" /></p><p></p><p>唯一有所异常的是名为 @Daan Wierstra 的账号对外关闭了其社交内容页的展示，但尚无法确定是否是他本人。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/71/71e373711eb778a9d112fca99b5526b0.png\" /></p><p></p><p>众多亿万富翁投资，</p><p></p><p>H 公司是做什么的？</p><p></p><p>由于其创始人的背景和筹集到的资金数额，H 一直是法国最热门的 AI 初创公司之一，与 Mistral 和 Poolside 等公司并驾齐驱。毕竟，很少能听到超过 1000 万美元的种子轮融资。</p><p></p><p>就在三个月前，H 筹集了法国有史以来最大的种子轮融资之一，从包括全球风险投资公司 Accel 在内的投资者那里获得了 2.2 亿美元。该公司没有透露其中有多少是股权投资，多少是债务投资，但根据之前的报道，这一轮投资由多达 1.2 亿美元的可转换票据组成。这些是债务融资的一种形式，在满足特定条件后可以转换为股权。</p><p></p><p>今年 6 月，海外机构 PYMNTS 报告称，H 是引起投资者注意并吸引投资的人工智能代理初创公司之一，其估值领先于其业务基本面。</p><p></p><p>据介绍，这家初创公司的投资者包括众多亿万富翁（或其家族办公室）、一些知名风险投资基金和战略支持者。其中，亿万富翁名单里有前谷歌老板 Eric Schmidt、Courrier international 所属的 Le Monde 集团的个人股东 Xavier Niel、硅谷领先的风险投资家之一 Yuri Milner、Bernard Arnault（通过他的 Aglaé Ventures 基金）和 Motier Ventures（老佛爷百货集团所有者的家族办公室）等知名人士。</p><p></p><p>在风险投资名单上，投资者包括 Accel、法国巴黎银行的大型风险基金、Creandum、Elaia Partners、Eurazeo、FirstMark Capital 和 Visionaries Club。此外，还有一些产业投资者，包括亚马逊和三星。</p><p></p><p>有趣的是，总部位于纽约的机器人自动化软件公司 UiPath 也是 H 公司的投资者，这家欧洲机器人独角兽公司将在商业化和合作伙伴关系方面为 H 提供帮助。</p><p></p><p>据了解，H 建立的基础模型被称为 “代理”（agentic），这是一种旨在将任务分解为多个步骤并执行这些子任务，而不仅仅是一次一次地响应提示的人工智能。这家初创公司表示，其模型将比竞争对手的模型更有能力进行推理、规划和协作，致力于为商业和消费者垂直领域提供服务。</p><p></p><p>Kantor 曾表示，H 公司正在努力实现完全的人工通用智能（full-AGI），即与人类能力相当或超过人类能力的 AI 水平，能够完成各种任务。但老实说，这只是一个营销承诺，因为没人知道 AGI 是否或何时会实现。现实是，H 还需要筹集大量资金来支付计算能力和数据集的费用。</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://www.pymnts.com/personnel/2024/3-co-founders-leave-french-ai-startup-h-amid-operational-differences/\">https://www.pymnts.com/personnel/2024/3-co-founders-leave-french-ai-startup-h-amid-operational-differences/</a>\"</p><p></p><p><a href=\"https://techcrunch.com/2024/05/21/french-ai-startup-h-raises-220-million-seed-round/\">https://techcrunch.com/2024/05/21/french-ai-startup-h-raises-220-million-seed-round/</a>\"</p><p></p><p><a href=\"https://sifted.eu/articles/three-cofounders-leave-h-news\">https://sifted.eu/articles/three-cofounders-leave-h-news</a>\"</p><p></p><p><a href=\"https://www.accel.com/noteworthy/building-foundational-models-to-generate-actions-our-partnership-with-the-h-company\">https://www.accel.com/noteworthy/building-foundational-models-to-generate-actions-our-partnership-with-the-h-company</a>\"</p><p></p><p>内容推荐</p><p></p><p>2024年8月18-19日，AICon 全球人工智能开发与应用大会·上海站成功举办，汇聚超过60位大模型行业先锋，全方位剖析大模型训练与推理机制、多模态融合、智能体Agent前沿进展、检索增强（RAG）生成策略、端侧模型优化与应用等热点内容。经过嘉宾授权，「AI前线」为你独家整理了一份演讲PPT合集，不容错过。关注「AI前线」，回复关键词「PPT」免费获取。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/81/814c1f9a6b667134f3520e04d6d8dfc7.png\" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/79/791c6d47a29abdea4f3ba09bea3b176a.png\" /></p><p></p><p>今日荐文</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622338&amp;idx=1&amp;sn=4c17f35ca45df4107a8830186480c690&amp;chksm=fbeba70dcc9c2e1b4312a0642eea90bc67da383b4f3fd4e94b81bdd3eb9f29b9c76dae5b5de4&amp;scene=21#wechat_redirect\">《黑神话：悟空》被指抄袭，原作者开撕；IBM中国被曝数千研发权限突然被关；曝360儿童手表智能回答毁三观，周鸿祎道歉 | AI周报</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622313&amp;idx=1&amp;sn=1fbaa02128849c257d476d2f64fd5683&amp;chksm=fbeba766cc9c2e7067f35b00df38c29cdd8e813e7fbb10eb1c843f6e4274ba5f8f874e088ef0&amp;scene=21#wechat_redirect\">《黑神话：悟空》开发者遭猎头疯抢，联创发声求放过：你们不缺人才，别搞我们</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622096&amp;idx=1&amp;sn=a6d564c645e7644023d4142365fc19b5&amp;chksm=fbeba41fcc9c2d093abdaa2dbe6cac44301d4070a5f580572925ad46b28aa9bb0574062de232&amp;scene=21#wechat_redirect\">《黑神话：悟空》的第二个受害者出现了，竟是AI搜索惹的祸！</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=1&amp;sn=5e58c5a72a2d7fae816471954959b349&amp;chksm=fbeba49ccc9c2d8a35501b45bea911c9b684944634522011a859022dad48c9fda4b3a4e3b8fc&amp;scene=21#wechat_redirect\">《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621831&amp;idx=1&amp;sn=5ff4ba1979a3e77a914e8b6c5d390db7&amp;chksm=fbeba508cc9c2c1e83c061d1dbd107dca94d93ccda4172996c67d7920e2846d39123b77ee22b&amp;scene=21#wechat_redirect\">“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif\" /></p><p></p><p>******你也「在看」吗？******👇</p><p></p>",
    "publish_time": "2024-08-27 09:50:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Netflix 引入虚拟线程：性能和缺陷案例研究",
    "url": "https://www.infoq.cn/article/4Js5OeofgZONv1UBykUT",
    "summary": "<p>Netflix 是 Java 的长期采用者，最近他们升级到了 Java 21。他们现在正在利用 JEP 439 中引入的分代 ZGC 和 JEP 444 中引入的虚拟线程等新特性来提升其庞大的微服务组合的性能。虽然为高吞吐量并发应用程序设计的虚拟线程在早期展示出了优势，但它们在现实场景中也带来了独特的挑战。</p><p></p><p>在 Netflix Tech Blog 上最近的一篇文章中，其 JVM 生态系统团队分享了他们使用虚拟线程的经验，特别是服务遇到超时和挂起实例的问题。该问题与虚拟线程和阻塞操作和 OS 线程可用性的交互有关，导致其基于 SpringBoot 的应用程序出现类似死锁的情况。</p><p></p><p>Netflix 工程师在运行 Java 21 并使用 SpringBoot 3 和嵌入式 Tomcat 的服务中观察到了间歇性超时和无响应实例。尽管 JVM 实例保持活动状态，但它们停止提供流量，其特点是卡在 closeWait 状态的套接字显著增加。当远程端关闭 TCP 连接，但本地端尚未关闭其连接端，使套接字处于等待状态时，就会出现此状态。有关此问题的更多信息，请参阅术语部分的 RFC 793。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/23/231af9145d48696f813e3b72d42cb494.png\" /></p><p></p><p>初步诊断表明虚拟线程与此问题有关，尽管它们并未出现在传统的线程转储中。团队使用 jcmd Thread.dump_to_file 发现了数千个“空白”虚拟线程，表示这些线程已创建但尚未运行。问题追溯到 Tomcat 的请求处理，其中创建了新的虚拟线程，但由于操作系统线程不可用而无法安排。</p><p></p><p><code lang=\"shell\">#119821 \"\" virtual\n#119820 \"\" virtual\n#119823 \"\" virtual\n#120847 \"\" virtual\n#119822 \"\" virtual\n...\n</code></p><p></p><p>分析显示，Tomcat 的虚拟线程执行器正在为每个请求创建线程，但这些线程因等待锁定而停滞。具体而言，由于同步块内的阻塞操作，线程被固定到 OS 线程，而 ForkJoinPool 中可用的 OS 线程数量有限，让情况更加恶化。</p><p></p><p>该问题源于 一个经典的死锁场景，其中虚拟线程无法继续，因为所需的锁被其他虚拟线程持有，这些虚拟线程被固定到所有可用的 OS 线程。这阻止了新的虚拟线程的调度，结果阻塞了应用程序。</p><p></p><p>为了解决该问题，Netflix 的 JVM 生态系统团队使用一个堆转储来检查锁的状态，并确认没有线程拥有它，但等待它的线程无法继续。这是一个本应解决的瞬态，但却导致了类似死锁的情况。</p><p></p><p>团队确定了根本原因，并开发了一个可重现的测试用例，以防止将来出现类似问题。虽然 Java 21 中的虚拟线程已显示出通过减少开销来提高性能的潜力，但此案例意味着了解它们与现有线程模型和锁定机制的交互方式是很重要的。</p><p></p><p>除了 Netflix 的发现之外，InfoQ 上最近的一项案例研究还深入探讨了虚拟线程的实际挑战和优势，特别是在涉及大量并发负载的场景中。这项研究强调了在将虚拟线程集成到生产系统中时需要仔细考虑和测试，因为即使是很小的架构细节也会导致严重的性能影响。</p><p></p><p>除了虚拟线程之外，Netflix 采用的分代 ZGC 也在优化其系统时发挥了关键作用，正如最近的一篇文章中提到的那样。ZGC 能够在堆大小增加时保持较低的暂停时间，通过减少垃圾收集开销和增强响应能力，显著提高了 Netflix 的应用程序性能。有关分代 ZGC 的更多信息，请参阅这篇 InfoQ 新闻。</p><p></p><p>Netflix 还有一个强大的警报系统，基于其 Atlas Streaming Eval 平台，对帮助他们识别和诊断这些问题是很重要的。该系统旨在改进实时监控和警报，使团队能够捕获处于问题状态的实例并提供关键数据进行追溯分析。</p><p></p><p>尽管面临各种挑战，Netflix 对虚拟线程的未来仍持乐观态度，并预计即将发布的 Java 版本将取得进一步改进，特别是在解决锁定原语的集成挑战方面。此案例研究对于性能工程师和开发人员在其应用程序中探索虚拟线程时是一个有价值的示例。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/08/netflix-performance-case-study/\">https://www.infoq.com/news/2024/08/netflix-performance-case-study/</a>\"</p>",
    "publish_time": "2024-08-27 09:56:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "InfoQ 2024年趋势报告：AI 智能体发展不及预期，RAG 或成最大赢家",
    "url": "https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe",
    "summary": "<p>作为InfoQ的一大特色，趋势报告系列专注于软件开发的各个关键领域。这些报告旨在为InfoQ的读者和听众提供今年值得关注的技术发展趋势概览。</p><p></p><p>InfoQ的人工智能、机器学习和数据工程编辑团队邀请了业界专家，共同探讨了人工智能和机器学习领域的未来趋势，以及接下来12个月中值得关注的动态。本期播客记录了这次讨论的内容，小组成员们分享了他们对创新人工智能技术如何重塑行业格局的见解。</p><p></p><p></p><h1>关键要点：</h1><p></p><p>人工智能的未来是开放的。我们正处于大语言模型和基础模型的时代。尽管目前大部分模型是闭源的，但像Meta这样的公司正试图引领开源模型的趋势。检索增强生成（RAG）的重要性将日益凸显，特别是在大规模部署LLM的场景中。随着人工智能赋能的GPU基础设施和人工智能驱动的个人电脑的出现，AI驱动的硬件将获得更多关注。由于受基础设施设置和管理成本方面的限制，小语言模型（SLM）将得到更多的探索和采用。小语言模型也是边缘计算相关用例的一个很好的选择，它们可以在小型设备上运行。AI代理，如编码助手，将在企业应用开发环境中得到更多的采用。在语言模型的生命周期管理中，AI的安全性和隐私保护将持续占据重要地位。自托管模型和开源LLM解决方案将有助于加强AI的安全防护。LangOps或LLMOps将成为LLM生命周期的关键环节，它们在大模型生产环境部署的持续支持中发挥着重要作用。我们对未来12个月的AI发展做出了一些预测：机器人AI，即具身AI，将成为新的趋势；从AI寒冬过渡到更多具体的应用场景，涉及更多自动化工作流和智能体工作流，然后扩散到更多的边缘设备，如笔记本电脑和手机。</p><p></p><h1>简介</h1><p></p><p></p><p>Srini Penchikala：大家好，欢迎收听2024年人工智能与机器学习趋势报告播客。这个播客是我们年度报告的一部分，目的是与听众分享人工智能和机器学习领域的最新动态。我是Srini Penchikala，InfoQ人工智能、机器学习和数据工程社区的主编。我们有幸邀请到了一群杰出的专家和实践者，他们来自人工智能和机器学习的不同领域。</p><p></p><p>感谢大家来到这里。我非常期待与大家共同探讨人工智能和机器学习领域的最新动态，包括我们目前的发展阶段，更重要的是我们未来的发展方向。特别是自去年我们讨论趋势报告以来，人工智能技术的创新速度之快令人目眩。在开始深入播客主题之前，我想先向听众朋友们提供一些必要的信息。我们的年度报告包含两个主要部分。首先是这个播客，它提供了一个平台，让听众能够听到来自专家实践者们对创新人工智能技术如何颠覆行业的见解。其次是一份书面文章，将在InfoQ网站上发布，其中将包含技术采用不同阶段的趋势图，并详细介绍自去年趋势报告以来新增或更新的个别技术。</p><p></p><p>我强烈推荐大家在本月底文章发布时去阅读它。现在，让我们回到播客的讨论上来。自ChatGPT发布以来，生成式人工智能和大型语言模型技术的发展速度似乎达到了顶峰，而且这种快速的创新势头似乎不会很快放缓。技术领域的主要参与者都一直在忙着发布他们的人工智能产品。今年早些时候，谷歌在I/O大会上发布了几项新的更新，包括Gemini更新和生成式人工智能在搜索中的应用。同时，OpenAI也发布了GPT-4o，这是一个能够实时处理音频、视觉和文本的全能模型，提供了一种多模态解决方案。</p><p></p><p>紧接着，Meta也发布了Llama 3，并很快推出了基于4050亿参数的Llama 3.1版本。这些参数的数量级是亿，而且它们还在不断增加。像Ollama这样的开源解决方案也受到了越来越多的关注。看来这个领域一直在加速发展。生成式人工智能技术的基础是大语言模型，它们经过大量数据的训练，能够理解和生成自然语言及其他类型的内容，还能执行丰富多样的任务。因此，LLM可以作为我们今年趋势报告讨论内容的切入点。</p><p></p><p>Anthony，你一直在密切关注LLM模型及其发展。你能谈谈生成式人工智能和LLM模型的当前发展状态、最近的一些主要进展，以及我们的听众应该关注哪些方面吗？</p><p></p><p></p><h1>AI的未来是开放的</h1><p></p><p></p><p>Anthony Alford：如果要用一个词来概括LLM，我会选择“更多”，或者可能是“规模”。我们正处在LLM和基础模型的黄金时代。OpenAI可能是最显眼的领导者，当然，还有其他大玩家，比如谷歌，还有Anthropic推出的Claude。这些模型大多是封闭的，即便是OpenAI，他们的旗舰产品也只能通过API访问。然而，Meta在这方面是一个异类。实际上，我认为他们正试图引领趋势朝着更开放的方向发展。我记得扎克伯格最近说过，“人工智能的未来是开放的。”因此，他们开放了一些模型的权重。至于OpenAI，尽管他们没有公开模型权重，但也会发布一些模型的技术细节。例如，我们知道GPT-3的第一个版本有1750亿个参数，但对于GPT-4，虽然他们没有明确说明，但趋势表明它肯定拥有更多的参数，数据集更大，计算预算也更大。</p><p></p><p>我认为我们还将继续见证的另一个趋势是模型的预训练，也就是GPT中的“P”。这些模型在庞大的数据集上进行预训练，基本上是整个互联网的内容。然后，他们会进行微调，这是ChatGPT的关键创新之一。因此，这种指令微调现在变得极其普遍，我相信我们将继续看到这一趋势。接下来，让我们转到上下文长度这个话题，因为它代表了另一个发展趋势。上下文长度，即你可以输入模型的数据量，这个量正在增加。我们可以讨论这与新的SSM（State Space Model，状态空间模型，如Mamba）之间的区别，因为SSM是没有上下文长度限制的。Mandy，你对这个话题有什么看法？</p><p></p><p>Mandy Gu：我认为这绝对是我们正在见证的一个明显趋势，那就是更长的上下文窗口。当初ChatGPT等大语言模型开始普及时，这是人们普遍指出的一个不足之处。今年早些时候，Gemini、Google基金会以及GCP的基础模型引入了高达一百万个Token的上下文窗口长度，这无疑是一个改变游戏规则之举，因为之前我们从未有过如此长的上下文处理能力。我认为这引领了一种趋势，其他供应商也在尝试提供同样长甚至更长的上下文窗口。由此产生的一个二级效应是提升了可访问性，它使得像信息检索这样的复杂任务变得更加简单。在过去，我们可能需要进行多阶段的检索，例如RAG，但现在，我们可以将所有上下文信息直接输入到这一百万Token的上下文窗口中，虽然不一定意味着更好，但无疑简化了过程。这是过去几个月的一个非常有趣的进展。</p><p></p><p>Anthony Alford：Namee，你还有什么要补充的吗？</p><p></p><p>Namee Oberst：我们专注于小语言模型的应用。较长的上下文长度窗口有它的价值，但根据我们内部的研究以及YouTube上一些知名人士的实验，即便你只传了2000个Token的段落给大模型，它们在处理段落中间信息丢失的问题上表现并不出色。因此，如果你想要进行精确的信息检索，有时候较长的上下文窗口反而会误导用户，让用户误以为可以随意输入大量信息并精确地找到所需内容。我认为目前情况并非如此。我认为精心设计的信息检索工作流，如RAG，仍然是解决问题的关键。</p><p></p><p>基本上，无论上下文Token数量达到百万级别，甚至更长，如果考虑到企业在实际使用场景中所处理的文档数量，这样的上下文长度可能仍然不足以带来实质性的改变。但在消费者使用场景中，更长的上下文窗口确实能够显著提升信息检索的效率。</p><p></p><p>Anthony Alford：所以说回报是递减的，对吗？</p><p></p><p>Namee Oberst：确实存在回报递减的效应。这在很大程度上取决于具体的应用场景。设想一下，如果有人需要浏览上万份文档，那么增加上下文窗口的大小实际上帮助有限。大量研究表明，大语言模型并不适合作为搜索引擎使用，它们在精确检索信息方面表现并不好。因此，我个人不太推荐依赖长上下文的LLM，而更倾向于使用RAG。话虽如此，我认为在某些情况下，长上下文窗口确实非常有用。例如，当你需要传一篇很长的论文给大模型，然后要求模型对其进行重写，但这篇论文的长度超出了传统上下文窗口的处理能力……我特别喜欢用LLM来转换文档，比如将一篇Medium长文章转换成白皮书，这在以前是超出了常规上下文窗口的处理能力的。我认为这是一个非常好的应用场景。</p><p></p><p>Anthony Alford：你提到了RAG，也就是检索增强型生成技术。我们不如就来深入讨论一下这个主题。它似乎首先能够解决上下文长度的问题。此外，这看起来是一个相当普遍的应用场景。或许你可以就此发表一些看法，特别是对于小型的开放模型。现在，人们可以在本地或者自己的硬件、云平台上运行这些模型，利用RAG来解决问题，这样他们就不需要依赖那些大型的封闭模型了。Namee，你对这个问题有什么见解吗？</p><p></p><p>Namee Oberst：我非常支持这一理念。如果你看一下Hugging Face上可用的模型类型以及它们的性能基准测试，我认为这非常令人印象深刻。此外，这些开源模型的创新速度和节奏也同样令人赞叹。尽管如此，当你看着GPT-4o的推理速度和能力，以及它能够为亿万用户提供数百万种服务，你仍然会感到万分惊奇。</p><p></p><p>然而，如果你正在面对一个企业级的应用场景，你拥有明确的工作流，并且希望解决一个非常具体的问题，例如自动化特定的工作流，以自动化生成报告为例，或者是在这些预定义的10000份文档中进行RAG来实现深入的信息检索。我相信，你可以利用开源模型来解决这些问题，或者选择一个现有的较小规模的语言模型，对其进行微调，投入资源，然后基本上可以在企业私有云环境中运行这些模型，并且还可以逐渐将它们部署到边缘设备上。因此，我非常看好使用较小的模型来执行针对性任务。</p><p></p><p>Srini Penchikala：确实，几个月前我尝试用Ollama来处理一个特定的用例，我非常看好像Ollama这样的开源解决方案。你可以自行托管服务，这样你就无需将所有数据上传到云端，也不必担心数据的去向。利用这些自行托管的模型，并结合RAG技术，可以构建专有的信息知识库。我认为这种方式在企业界正获得越来越多的关注。企业希望保留数据的控制权，同时又能充分利用这项强大技术。</p><p></p><p>Roland Meertens：目前大多数企业都是以OpenAI作为起点来验证自身的商业价值，在证明存在商业价值以后，他们才可以开始思考，“我们如何将这项技术真正融入我们的应用程序？”我认为这非常棒，因为你可以很容易地开始使用这项技术，随后再构建自己的基础设施来支持应用程序的后续发展。</p><p></p><p>Srini Penchikala：是为了扩大规模，对吧，Roland？你可以评估出哪种模型最适合你的需求，对吧？</p><p></p><p>Roland Meertens：是的。</p><p></p><p>Srini Penchikala：让我们继续回到大语言模型的讨论上来。另一个值得关注的领域是多模态模型，例如GPT-4o，也就是所谓的全能模型。我认为这确实将LLM推向了一个新的高度。它不再局限于文本，我们还可以利用音频、视频或其他各种格式。那么，大家对GPT-4o或者多模态模型有什么见解吗？</p><p></p><p>Namee Oberst：为了参与这期播客，我实际上做了一项实验。我订阅了GPT-4o的服务，今天早上我出于好奇输入了几个提示词。由于我们的主要工作是基于文本的，所以并不经常使用这个功能。我要求它为LLMware生成一个新的标志，但它失败了三次，每次都无法正确处理“LLMware”这个词。尽管如此，我知道它非常令人印象深刻，并且我认为他们正在迅速取得进展。但我想看看它们目前的水平如何，今天早上对我来说体验并不佳。当然，我也知道它们可能仍然比市场上其他任何产品都要好。我先声明这一点，以免有人来找我麻烦。</p><p></p><p>Roland Meertens：在图像生成领域，我不得不说，去年我对Midjourney的表现感到非常惊讶。他们的进步速度令人惊叹，尤其是考虑到它还是一家小型公司。一家小型企业能够凭借更优秀的模型超越大型竞争者，这一现象确实令人感到惊叹。</p><p></p><p>Mandy Gu：大型公司，如OpenAI，有出色的泛化能力，并且非常擅长吸引新人才进入这一领域。然而，随着你更深入地探索，你会意识到，正如我们在人工智能和机器学习领域常说的，天下没有免费的午餐。你探索、测试、学习，然后找到适合你的方法，但并不总是那些大玩家才能做到。对我们来说，我们从多模态模型中受益最多的不是图像生成，而是OCR能力。一个非常典型的应用场景是，我们上传图像或文件，然后与大语言模型对话，尤其是针对图像内容。这已经成为我们最大的价值主张，并且深受我们开发者的喜爱。因为在很多时候，当我们在帮助最终用户或内部团队进行故障排查时，他们会发给我们堆栈信息跟踪或问题截图。能够直接将这些截图输入给模型中，而不是去解读它们，极大地节省了我们的时间。</p><p></p><p>因此，我们的价值并不仅仅来自图像生成，而是更多地来自于OCR技术的应用，它为我们带来了巨大的价值。</p><p></p><p>Srini Penchikala：这很有道理。当你采用这些技术，无论是OpenAI还是其他公司，你就会发现，在将这些技术应用到公司的具体用例时，并没有通用的解决方案。因此，每个公司都有其独特的应用场景和需求。</p><p></p><p>Daniel Dominguez：我觉得很有意思的是，现在我们看到Hugging Face上有超过80万个模型，那么明年会有多少新模型问世，这绝对是一个很有意思的话题。目前流行的趋势包括Llama、Gemma、Mistral和Stability。一年之内，不仅在文本领域，图像和视频领域也将涌现出多少新模型，这无疑是一个值得关注的点。回看过去一年的模型数量是件有趣的事情，但更令人兴奋的是，预测明年这个领域将出现的新模型数量，可能会是一个更加令人瞩目的数字。</p><p></p><p></p><h1>RAG在大规模LLM中的应用</h1><p></p><p></p><p>Srini Penchikala：没错，Daniel，你提出了一个好观点。我认为这就像20年前的应用服务器市场一样，几乎每周都有新产品问世。我认为这些产品有许多将逐渐融合，只有少数几个能够脱颖而出，并持续较长时间。说到RAG，我认为这是企业真正能够获得价值的地方，输入信息——无论是在本地还是云端——并通过大语言模型进行分析，从而获得深刻洞见。你认为有哪些RAG的实际应用案例可能会引起我们听众的兴趣？</p><p></p><p>Mandy Gu：我认为RAG是大语言模型规模化应用中最具有潜力的方向之一，其应用形态可以根据检索系统的设计而灵活变化，可以适应多样化的用例需求。在我们公司，RAG已被广泛应用于内部流程。我们开发了一个工具，它将我们的自托管大语言模型与公司所有知识库相连接。我们的文档存储在Notion中，代码托管在GitHub上，同时，我们还整合了来自帮助中心网站以及其他平台的公开资料。</p><p></p><p>我们实质上是在这些知识库之上构建了一个检索增强型生成系统。我们的设计思路是：每晚运行后台作业，从我们的知识源中抽取信息，并将它们存入我们的向量数据库。我们为员工提供了一个Web应用程序，他们可以针对这些信息提出问题或给出指令。在内部进行基准测试时，我们也发现，这种方法在相关性和准确性方面，明显优于将所有上下文信息直接输入给像Gemini 1.5这样的模型。但回到问题的核心，作为提升员工生产力的手段，RAG已经为我们带来了许多真正优秀的应用案例。</p><p></p><p>Namee Oberst：Mandy，你所分享的案例堪称经典，而且执行得非常到位，完美契合了你们的需求。这正是大语言模型强大能力的最佳体现。你还提到了一些非常有趣的内容。你说你们自托管了LLM，我想知道，你们是否采用了某个开源的LLM，或者你是否愿意分享一些这方面的信息？当然，你无需透露太多细节。不管怎样，这无疑是通用人工智能应用的一个杰出范例。</p><p></p><p>Mandy Gu：实际上，我们使用的都是开源模型，很多都是从Hugging Face获取的。我们在构建LLM平台之初，就旨在为员工提供一种安全且易于访问的方式来探索这项前沿技术。和其他许多公司一样，我们最初选择了OpenAI的服务，但为了保护敏感数据，我们在它前面加了一个个人信息保护层。然而，我们从内部用户那里得到的反馈是，这个个人信息保护层实际上限制了生成式AI最高效的用例，因为在日常工作中，员工需要处理的不仅仅是个人信息，还有大量其他类型的敏感信息。这个反馈促使我们转变了思路：从防止员工与外部供应商共享敏感信息到如何确保员工可以安全地与LLM共享这些信息。因此我们从依赖OpenAI的服务转向了自托管大语言模型。</p><p></p><p>Namee Oberst：我简直被你所做的事情震撼到了。我认为这正是我们在LLMware所追求的。实际上，这正是我们希望借助在后端串联小型语言模型进行推理所能提供的那种解决方案。你多次提到了Ollama，但我们基本上已经将Llama.cpp集成到我们的平台中，这样你就可以基于量化模型轻松、安全地进行推理。我坚信，你为你们企业设计的工作流非常出色。但同时，我也预见到其他工作流自动化的用例将会被简化，以便在笔记本电脑上运行。我几乎可以预见在非常近的未来，所有东西都将被微型化，这些大语言模型将变得更小巧，几乎成为软件的一部分，我们所有人都将能够轻松、精确且安全地在笔记本电脑上部署它们，当然，还有私有云。</p><p></p><p>Mandy Gu：你提到了Llama.cpp，我觉得这非常有趣，因为可能并不是每个人都能意识到量化模型和小模型能带来如此多的边际优势。目前，我们仍处于快速实验阶段，速度是关键。采用量化模型可能会在精度上略有损失，但我们从降低延迟和提高行动速度方面获得了回报，这对我们来说是非常值得的。我认为Llama.cpp本身就是一个巨大的成功案例，这个由个人或小团队所创造的框架，能够得到如此大规模的执行。</p><p></p><p></p><h1>AI驱动的硬件</h1><p></p><p></p><p>Namee Oberst：Llama.cpp是Georgi Gerganov开发的，他在开源领域做出了令人惊叹的贡献。Llama.cpp为Mac Metal进行了优化，但在NVIDIA CUDA上也表现出色。我们正在做的工作是，让数据科学家和机器学习团队不仅能在Mac Metal上实现解决方案，还能跨越所有AI PC平台。我们利用了Intel OpenVINO和Microsoft ONNX技术，这样数据科学家们就可以在他们喜欢的Mac上工作，然后也能轻松无缝地在其他AI PC上部署他们的模型，因为MacOS只占操作系统份额的大约15%，剩下的85%实际上是非MacOS系统。想象一下，当我们能够跨多个操作系统部署，并充分利用所有这些AI PC的GPU能力时，未来的发展将会多么激动人心。我认为，这将是未来趋势中一个非常令人期待的方向。</p><p></p><p></p><h1>小模型和边缘计算</h1><p></p><p></p><p>Srini Penchikala：你们都提到了小语言模型和边缘计算，我们或许可以就此话题展开讨论。我知道关于大语言模型，我们可以讨论很长时间，但我更想听听你们对其他主题的看法。关于小模型，Namee，你在LLMWare对SLM做了一些研究，还特别提到了一个为SLM量身定制的RAG框架。你能否更深入地谈谈这个领域？微软也在研究他们所谓的Phi-3模型。能否分享一些这方面的信息？这些模型之间有何不同？我们的听众如何能够快速了解并跟上SLM的最新发展？</p><p></p><p>Namee Oberst：实际上，我们是小模型领域的探索先锋。我们专注于小模型的研究已经有一年多，可以说相当早就开始了。实际上，RAG在过去三四年已经在数据科学和机器学习领域得到了应用。我们在公司成立初期就对RAG进行实验，并对我们的小型参数模型进行了一些非常早期的调整，我们发现可以让这些模型执行非常强大的任务，并且从中获得了性能上的显著提升。同时，我们也确保了数据的安全性和保障。这些因素始终是我考虑的重点，因为我有法律专业的背景，我最初是在一家大型律师事务所担任公司律师，后来还担任了一家公共保险经纪公司的总法律顾问。</p><p></p><p>数据安全和隐私保护一直是我们最为关注的重点。对于那些受到严格监管的行业来说，选择使用小模型或其他较小规模的模型，是一个显而易见的决定。Mandy已经详细阐述了许多原因，但成本效益同样不容忽视。实际上，成本是一个巨大的考量因素。因此，当你能够显著减少模型的资源占用并大幅降低成本时，就没有理由去部署那些庞大的模型。更令人振奋的是，越来越多的人开始认识到这一点，与此同时，小模型性能取得了显著进步。微软推出的Phi-3模型，以及我们针对RAG进行微调的模型，还有Hugging Face专为RAG设计的模型，都显示出了卓越的性能。我们使用专有数据集对这些模型进行微调，以相同的方式和数据集微调了20个模型，确保了我们可以进行公平的比较。Phi-3模型在我们的测试中表现卓越，超越了我们测试过的其他模型，包括那些拥有80亿参数的模型，成为了表现最佳的模型。</p><p></p><p>我们的模型涵盖了从10亿参数到高达80亿参数的范围，并且在精确度方面达到了前所未有的高度，这真的让我感到非常惊讶。Hugging Face上那些向全世界免费开发的小模型，正在变得越来越好，而且进步速度非常快。我认为这是一个非常激动人心的世界。正如我之前所断言的，按照这样的创新速度，这些模型将会变得越来越小，小到它们所占用的资源跟软件相当。在不久的将来，我们将会在边缘设备上部署大量这样的模型。</p><p></p><p>Srini Penchikala：确实，许多应用场景涉及线下大模型处理和线上边缘设备实时分析的组合。这正是小型语言模型能够发挥其优势的地方。Roland、Daniel或者Anthony，你们对小型语言模型有何看法？在这个领域，你们观察到了哪些趋势或发展？</p><p></p><p>Anthony Alford：确实如此。微软的Phi系列模型无疑已经成为了焦点。此外，我们也有这个议题，Namee，你提到这些模型正在变得更好。问题是，我们怎么知道它们有多好？什么样的表现才算足够好？目前有许多基准测试，比如MMLU、HELM、Chatbot Arena等，还有很多排行榜和指标。我不想说人们在操纵这些指标，但这有点像是p-hacking，不是吗？你发了一篇论文，宣称在某个特定指标上超越了其他基线，但这并不总能直接转化为实际的商业价值。因此，我认为这仍然是一个需要解决的问题。</p><p></p><p>Namee Oberst：实际上，我们做了一套内部基准测试，专注于评估模型回答一些基于常识的商业和法律问题的能力，这些问题都是基于事实的。我们的平台主要是面向企业用户，因此在这个场景下，我们更关注模型对事实性问题、基本逻辑和数学问题的回答能力，而不是创造力。我们甚至创建了自己的基准测试方法，Phi-3模型的结果就是基于这些测试得出的。我对一些公布的结果持怀疑态度，你真的看过HellaSwag上的一些问题吗？有时候我甚至不知道正确或错误的答案是什么。因此，我们决定开发自己的测试标准，而我们讨论的Phi-3模型的表现正是基于这些我们自己制定的标准。顺便说一句，微软并没有赞助我们，尽管我希望他们能。</p><p></p><p>Srini Penchikala：我们很快会开始讨论大模型的评估，在这之前，你们对语言模型还有什么看法吗？</p><p></p><p>Roland Meertens：Phi让我印象深刻的一个点是，它在训练过程中不仅使用了高质量的数据，还通过自主生成数据来提升学习效果。例如，在编程方面，他们让Phi为学生编写指导手册，然后利用这些手册作为训练数据。这让我深刻体会到，如果你拥有更优质的数据，并且能够精心挑选这些数据，将能够训练出更为出色的模型。</p><p></p><p>Anthony Alford：你是说”Textbooks Are All You Need“吗？</p><p></p><p>Roland Meertens：除此之外，Hugging Face的团队成员也发表了多篇相关论文。目前，对于如何选择合适的数据来训练这些模型，人们表现出了极大的兴趣。在我看来，数据选择在机器学习领域仍然是一项被低估且值得深入探讨的课题。</p><p></p><p>Srini Penchikala：除了Phi，Daniel，你之前提到了TinyLlama。关于这些小模型，你有何见解或要评价的？</p><p></p><p>Daniel Dominguez：确实，正如Namee所言，目前在Hugging Face平台上的很多语言模型还有许多未知领域值得我们去探索。此外，Hugging Face的一个吸引人之处在于他们对不同性能级别的GPU进行了分类，你可能已经注意到了他们在排行榜上的目标设定。根据你的硬件配置，可能会被归类为”富GPU“用户或”穷GPU“用户，但不论哪种情况，你都能够运行这些语言模型。同时，我们也要感谢目前行业所提供的芯片技术，例如NVIDIA的芯片，它们不仅能够在云端运行这些小模型，也能够在低端个人计算机GPU和系统上运行。</p><p></p><p>得益于NVIDIA等公司提供的高性能GPU，这些小模型得以顺利运行。在Hugging Face平台上，当你看着这些模拟演示时，你会发现无需依赖庞大的计算资源即可在自己的设备上运行这些模型，这无疑是一个令人兴奋的发现。</p><p></p><p>Srini Penchikala：还有很多其他的AI创新正在发生，在结束语言模型讨论之前，我们快速再聊一下评估问题。除了基准测试指标，这些我们可能需要谨慎对待的东西，我想知道在现实世界中的最佳实践是怎样的？正如你提到的，Daniel，面对众多的模型，一个新入行者如何评估并比较这些模型，排除那些可能不适合他们的，并选择适合他们的？你有没有注意到在这个领域有哪些行业实践或标准？</p><p></p><p>Mandy Gu：我认为Anthony提到的商业价值是一个值得我们在评估过程中考虑的要点。尽管我对那些通用的基准测试持保留态度，但我认为我们真正需要做的是全面评估大型语言模型，不仅包括基础模型本身，还涉及到使用的技术以及我们如何针对特定任务来协调整个系统。例如，如果我的目标是总结一篇研究论文并提炼其语言，我就应该针对这一特定任务来评估LLM的能力。毕竟，没有一套模型或技术能够适用于所有任务。通过这个实验过程，我可以更有信心地找到最适合的模型组合。归根结底，如何更准确地量化评估结果，应该基于对当前任务的评估和我们期望看到的成果。</p><p></p><p></p><h1>AI智能体</h1><p></p><p></p><p>Srini Penchikala：接下来我们聊聊AI智能体。据我所知，这一领域已经取得了显著进展，特别是在AI驱动的编程助手方面。Roland，你对此有何见解？我知道你已经对Copilot等工具进行了深入研究。</p><p></p><p>Roland Meertens：去年你问我对未来一年的趋势有何看法，我预测是AI智能体。但现在看来，我说的可能并不完全准确。我们看到智能体技术确实有所发展。OpenAI之前推出了GPT Store，允许用户自行创建个性化的智能体。然而，坦白地说，我还没有听到有人向我强烈推荐某个智能体，说它非常出色。所以，从这个角度来看，我认为目前的进步还是有限的。不过，我们确实看到了一些有趣的应用，例如Devin，一个AI软件工程师智能体，它有一个终端、代码编辑器和浏览器，你可以给它分配任务，比如：“嘿，试着解决这个问题。”它会尝试独立完成所有工作。目前，Devin的成功率大约是20%，但考虑到它是免费的，这个成功率对于一个免费的”软件工程师“来说已经相当令人满意了。</p><p></p><p>此外，还有一些像AgentGPT这样的平台，我让它为AI趋势博客创建一个大纲，它提出了一些话题，比如：“我们可以讨论CNN和RNN等趋势。”我不认为这些还是趋势，但它对这些话题仍然充满热情，这是件好事。但总的来说，我认为智能体仍然有巨大的潜力。如果你想完成某项任务，完全可以进行自动化，而不是我自己去决定使用ChatGPT发送哪封电子邮件，然后发送它，接着等待对方回复并用ChatGPT总结，再写回复。</p><p></p><p>Anthony Alford：我的疑问在于，究竟是什么定义了“智能体”？</p><p></p><p>Roland Meertens：这是个好问题。所以我认为，就我目前所看到的，智能体是一种能够整合并执行多种任务的东西。</p><p></p><p>Anthony Alford：在念研究生时，我的研究领域是智能代理。我们所谈论的智能体主要是关于自主性。所以我认为，AI安全领域的专家们所担忧的，可能就是赋予这些系统自主性。不管你对AI的未来发展持何种看法，关注自主性问题都是非常合理的。目前来看，ChatGPT可能还没有达到实现完全自主性的水平。</p><p></p><p>Roland Meertens：这取决于你想做什么，以及你愿意在多大程度上让渡自己的控制权。就我个人而言，我还不太愿意在工作中部署一个完全自主的“Roland智能体”。我觉得它可能不会表现得特别智能。但我看到有人在约会应用上这么做了，显然，他们愿意冒这个险。</p><p></p><p>Daniel Dominguez：正如Roland所说的，智能体还没有真正掀起大浪，但可以肯定的是，它们在未来一定会发生些什么。比如，扎克伯格最近提到，他们正在为小型企业开发新的Meta AI智能体，这些智能体将帮助小企业主在自己的业务领域实现自动化。Hugging Face也有许多AI智能体，用于日常的工作流。Slack也集成了许多AI智能体，用于帮助用户总结对话内容、任务以及日常的工作流等。</p><p></p><p>我认为，随着我们在这一领域不断进步，AI智能体在日常工作和小型企业中的应用将变得更加自然。因为它们将极大地帮助我们完成许多日常任务，越来越多的公司也将开始在自己的平台上推出各式各样的智能体服务。例如，据我所知，谷歌即将推出用于Gmail等任务的AI智能体服务。因此，这可能是在接下里的一年加速发展的一个趋势。</p><p></p><p>Roland Meertens：确实，特别是你可以借助Langchain，让事情变得相当容易：”我有这些API可以调用，我想要实现这样的工作流程。如果你能够实现，就执行相应的操作。如果无法实现，就使用另一个API。“将工具箱中的所有工具进行组合并实现自动化，这种能力是非常强大的。</p><p></p><p>Mandy Gu：你说到点上了。以Gmail为例，有一个嵌入式助手可以帮你管理电子邮件，你就不需要去ChatGPT那里问如何增强邮件，或者做你想做的任何其他事情。从行为学角度来看，让信息在不同平台之间流转是一个巨大的工作负担，如果我们能够减少用户完成他们的工作所需要打开的标签页或需要访问的系统，这将是一个巨大的进步。而真正推动智能体采用的，就是这些因素。</p><p></p><p>Srini Penchikala：如果这些智能体能帮助我们决定何时发送电子邮件，何时不发送而是改为打电话，那就很厉害了。我的意思是，那样可能会更有效率，对吧？</p><p></p><p>Roland Meertens：我在思考趋势的问题。在去年，每一家公司都宣称：“我们现在是一家AI公司。我们将拥有自己的聊天机器人。”我甚至看到一些同事说：“我想证明这个论点，我让ChatGPT为我生成了三页的论点，看起来不错。”但我现在不想关心你的论点是什么，我不想和聊天机器人聊天，我只想浏览网站。所以我也好奇，最终会出现什么样的结果？每一家公司、每一个网站都会变成一个聊天机器人吗？或者我们是否也可以直接查找一本书的价格，而不是必须要求智能体为我们订购它？</p><p></p><p>Srini Penchikala：我们不应该过度智能体化我们的应用程序，对吧？</p><p></p><p>Roland Meertens：我的建议是，不要让你的生活变得过度智能体化。</p><p></p><p></p><h1>Ai安全</h1><p></p><p></p><p>Srini Penchikala：Anthony，你之前提到了人工智能的安全性问题，接下来就让我们深入探讨一下安全性。Namee和Mandy，你们都在多个实际项目中有所涉猎。你们如何看待安全与创新之间的关系？我们怎样才能确保这些开创性的技术在保持隐私和消费者数据安全的同时给我们带来价值？</p><p></p><p>Mandy Gu：生成式人工智能确实在安全领域引发了一系列连锁反应，例如第四方数据共享和数据隐私问题，这些问题日益严重。我们与许多SaaS供应商合作，这些供应商也是许多公司的选择。他们通常会集成人工智能技术，但并不总是会明确告知，实际上很多时候，他们会将用户数据发给OpenAI。根据数据的敏感程度，这可能是用户希望避免的。因此，我认为我们需要关注两点。首先，我们需要全面了解和追踪我们的数据流向。随着人工智能集成的普及，这项工作变得更加复杂，我们必须牢记这一点。其次，如果我们希望员工遵循正确的数据隐私安全实践，就必须让他们选择最简单、最安全的路径。</p><p></p><p>回到我之前提到的例子，如果我们在与OpenAI和其他供应商的所有对话中都叠加一个极其严格的个人身份信息（PII）审查机制，这可能会让使用者感到挫败，他们可能会直接去使用ChatGPT。但如果我们能够为他们提供替代方案，并通过激励措施使这些替代方案更加易于使用，或者增加他们需要的其他功能，同时确保安全选项是最容易实施的路径，这样就能吸引他们，并逐步建立起一种积极、注重数据隐私的良好文化。</p><p></p><p>Namee Oberst：是的，Mandy，你描述的工作流实际上凸显了我在讨论数据安全时经常强调的一个观点：在企业当中，生成式人工智能工作流的设计对所有的敏感数据安全性都有重大影响。是否有供应商可能会无意中将我们的敏感数据发送给一个我们不信任的供应商，例如OpenAI，这只是一个例子。我们需要审视这些问题，需要审视数据的来源，需要确保工作流具备可审计性，这样就可以追溯所有推理之间发生的交互。人工智能的可解释性如何发挥作用？我设计的工作流是否存在潜在的攻击面？如何处理提示词注入问题？</p><p></p><p>顺便提一个有趣的事实，由于经常处理小规模任务，小模型能够很好地泛化，因此不太容易受提示词注入的影响。但我们仍然需要关注提示词注入、数据投毒等问题。所以我认为，企业在部署人工智能时需要考虑诸多因素。Mandy，你刚才提出的观点非常中肯。</p><p></p><p>Mandy Gu：你提到的攻击面问题，我非常认同，因为这确实是一个可能迅速失控的方面。有人将生成式人工智能及其集成比作有线电视与流媒体服务，因为众多公司都在推出自己的人工智能集成服务，购买所有这些服务就像同时订阅Netflix、Hulu以及其他所有流媒体服务，不仅成本不划算，而且确实增加了潜在的攻击面。我认为，这正是我们在权衡自行构建与购买时需要考虑的，并且对我们所支付的费用以及数据的去向要有清晰的认识和审慎的决策。</p><p></p><p>我注意到人们对于这些问题的普遍认识正在逐步提高。供应商，尤其是SaaS提供商，正在积极回应这些关切。越来越多的服务提供商开始提供这样的选项：“我们可以将服务托管在你的虚拟私有云（VPC）中。无论是在AWS还是GCP上，都可以运行Gemini，确保你的数据仍然保留在你的云租户内。”我认为这正是在安全意识方面所展现的一个积极趋势。</p><p></p><p></p><h1>LangOps或LLMOps</h1><p></p><p></p><p>Srini Penchikala：除了安全性之外，我们需要关注的另一个重要问题是如何在生产环境中管理这些大语言模型和人工智能技术？所有，让我们迅速进入LangOps或LLMOps这个话题。这一领域有几种不同的术语并存。Mandy，或许你可以先分享一下你的观点。你如何看待当前LLM在生产环境中的支持情况，以及有哪些宝贵的经验？</p><p></p><p>Mandy Gu：在WealthSimple，我们把LLM的工作分为三个明显不同的领域。首先是提升员工的工作效率，其次是优化客户业务流程，第三是基础的LLMOps，我们更愿意称之为LLM平台工作，它为前两个领域提供支持。我们在这方面积累了许多经验，对我们来说行之有效的是我们的赋能理念。我们的工作以安全性、可访问性和选择性为中心。我们的目标是为用户提供可选择性，让每个人都能为手头的任务选择最合适的技术和基础模型，帮助我们避免了这个领域常见的一个问题，即人们将LLM视为寻找问题的解决方案（拿着锤子找钉子）。通过提供这些可复用的平台组件，生成式AI的采纳变得更加普遍。</p><p></p><p>这是一个我们逐渐才领悟到的教训。在我们刚开始踏上LLM之旅时，我们构建了一个LLM网关，它有审计跟踪功能，让人们能够安全地使用OpenAI和其他供应商的服务。我们收到的反馈是，审计跟踪功能在很多实际应用场景中对他们造成了限制。因此，我们开始自托管模型，这样我们就可以轻松地加入开源模型，进行微调，然后将其集成到我们的平台中，并通过LLM网关为我们的系统和最终用户提供推理服务。然后我们开始构建检索功能作为可复用的API，并围绕向量数据库构建框架，增强可访问性。随着我们逐渐将这些组件平台化，我们的最终用户——包括科学家、开发者以及业务人员——开始尝试并发现：“这个工作流实际上可以通过LLM得到显著改进。”这时，我们就会介入，帮助他们将这些想法产品化，并实现大规模的产品部署。</p><p></p><p></p><h1>AI发展趋势预测</h1><p></p><p></p><p>Srini Penchikala：我们即将结束这次讨论，这是一次非常精彩的讨论。在结束之前，我想向在座的各位提出一个问题：你们对人工智能领域在未来12个月内可能发生的事情有怎样的预测？当我们明年再次聚在一起讨论时，可以回顾并讨论这些预测的实现情况。</p><p></p><p>Mandy Gu：我认为，围绕大模型的许多炒作将会逐渐平息。我们在过去一年半的时间里目睹了它们惊人的增长。对于许多企业和行业来说，LLM仍然是一个他们愿意持续投入的赌注。</p><p>然而，我认为在未来的12个月里，这种情况将会有所改变，我们将开始对这项技术设定更为现实的预期，并在期望获得具体成果之前，更加审慎地评估我们的探索深度。因此，我预测从现在开始的12个月内，LLM炒作将会减少，那些继续采用这项技术的公司将会找到切实可行的方法，将其无缝集成到他们的工作流或产品中。</p><p></p><p>Daniel Dominguez：我预测，随着人工智能不断产生海量数据，它将与区块链等技术有某种形式的融合。我已经注意到许多区块链项目已经开始探索与人工智能的数据整合。虽然区块链和人工智能的融合目前还处于早期阶段，但在未来将会取得显著进展，尤其是在数据管理方面。因此，我认为人工智能与区块链的结合将是未来技术发展的一个重要趋势。</p><p></p><p>Roland Meertens：我仍然对机器人技术抱有期待，不过现在我们更倾向于称之为具身人工智能。这是去年逐渐流行起来的一个新术语。我不确定什么时候会发生，智能体已经能为我们执行计算机任务，如果我们把它们放到机器人的身体里，它们还会帮我们干活。具身人工智能无疑将成为下一个重要的大事。</p><p></p><p>Srini Penchikala：看来这些机器人将成为你的付费程序员，对吗？</p><p></p><p>Roland Meertens：不是这样。智能体将成为你的编程伙伴，而机器人则会在日常生活中为你提供帮助。我好奇的是，现在的公司拥有大量的数据，他们是否会利用这些数据来微调自己的模型并将其商业化？或者继续使用RAG？设想一下，如果你是一个园艺师，多年来一直在拍摄花园的照片，并提供如何改善花园的建议。肯定有很多小型企业拥有这样的数据，他们将如何从这些数据中获取价值？我非常好奇这些小型企业将如何利用他们的数据，以及如何构建自己的智能体、聊天机器人或AI自动化解决方案。</p><p></p><p>Anthony Alford：人工智能寒冬，Mandy已经提到了，不是吗？她说“我们可能会看到炒作的热度逐渐降低”，这是“温和”版本的寒冬。而“强烈”版本的寒冬，或许你已经看到过这样的标题，我记得是《自然》杂志上的一篇论文，它指出：“如果你用生成式AI生成的内容来训练生成式AI，结果可能会变得更糟。”我认为人们已经开始思考互联网是否正在被这些生成式内容污染。让我们拭目以待。我真心希望我的担忧是多余的，我真心不希望这个预测会成为现实。</p><p></p><p>Srini Penchikala：这是非常可能的，对吧？Namee，你对接下来的12个月有怎样的预测？</p><p></p><p>Namee Oberst：我预测我们将会经历一些Anthony和Mandy所描述的情况，但很快会过渡到更有价值、更加现实和具体的应用场景上，包括更自动化的工作流、智能体工作流，以及进一步扩展到边缘设备，比如笔记本电脑和智能手机。这就是我的预测，这将会很有趣。</p><p></p><p>Srini Penchikala：是的，这将会很有趣，这也是我所预测的。我相信我们将看到更多融合、端到端、全面的人工智能解决方案，它们结合了小模型、RAG技术和人工智能硬件。我认为许多积极的变化正在发生。我希望所谓的人工智能寒冬不会持续太久。</p><p></p><p></p><h1>相关资源</h1><p></p><p>论文“<a href=\"https://arxiv.org/abs/2306.11644?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA\">Textbooks Are All You Need</a>\"”<a href=\"https://arxiv.org/abs/2301.03988?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA\">SantaCoder: don't reach for the stars!</a>\"</p><p></p><h1>嘉宾简介</h1><p></p><p>Mandy Gu</p><p>Mandy Gu是Wealthsimple的高级软件开发经理，负责领导机器学习和数据工程团队。此前，她拥有丰富的自然语言处理（NLP）和数据科学方面的工作经验。</p><p></p><p>Namee Oberst</p><p>Namee Oberst是一家专注于生成式和开源人工智能解决方案的初创公司的创始人。</p><p></p><p>Srini Penchikala</p><p>Srini Penchikala是一位资深的软件架构师，并担任InfoQ人工智能、机器学习与数据工程板块的主编。著有《Apache Spark大数据处理》和《Spring Roo实战》（合著者）。</p><p></p><p>Roland Meertens</p><p>Roland是一位机器学习工程师，在自动驾驶汽车领域深耕计算机视觉技术。此前，他曾在社交媒体平台、深度学习自然语言处理、社交机器人以及无人机领域从事计算机视觉方面的工作。</p><p></p><p>Anthony Alford</p><p>Anthony是Genesys高级开发总监，在设计和构建大规模软件方面拥有超过20年的经验。</p><p></p><p>Daniel Dominguez</p><p>Daniel是华盛顿大学机器学习专业的工程师，拥有超过12年的软件产品开发经验。</p><p></p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p>查看英文原文：<a href=\"https://www.infoq.com/podcasts/ai-ml-data-engineering-trends-2024/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA\">https://www.infoq.com/podcasts/ai-ml-data-engineering-trends-2024/</a>\"</p>",
    "publish_time": "2024-08-27 10:41:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "十大揭秘：用友BIP 3 R6核心技术",
    "url": "https://www.infoq.cn/article/S3tF9UXB8zszcCQU0fdd",
    "summary": "<p>“这个时代不缺技术，缺的是方法体系！”在充满挑战与机遇的时代，企业需要通过积极构建自身的AI竞争力，以实现创新发展。</p>\n<p>8月10日，用友在“2024全球商业创新大会”上发布用友BIP 3 R6，这代表着用友BIP智能和数据服务能力再升级，同时实现更强数智能力、更高运行性能、更低资源消耗以及更加安全可靠。</p>\n<p>为了揭开用友BIP 3 R6的神秘面纱，全面了解它背后强大的技术力量，作为前沿技术传播者的科技媒体InfoQ，全程参与此次大会，并对用友BIP产品的核心技术团队进行了一对一访谈，对用友BIP 3 R6进行技术全揭秘！</p>",
    "publish_time": "2024-08-27 16:25:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "人人创造，一起热AI ｜火山引擎首届AI创造者大赛来啦！",
    "url": "https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc",
    "summary": "<p>AI正以前所未有的速度融入千行百业，成为各行业智能化转型的加速器。火山引擎正式发起「AI创造者大赛」，大赛首场为汽车行业专场，由火山引擎携手领克汽车与英特尔联合主办、吉利汽车研究院协办，旨在携手汽车行业领军品牌，鼓励开发者利用豆包大模型和扣子专业版，针对领克汽车的真实业务场景，开发出具有实际应用价值的智能体解决方案。</p><p></p><p>本次大赛共设置三大赛道——AI 座舱赛道、AI 营销赛道、AI 售后赛道。目前大赛报名通道已开启，可登录火山引擎官网查看更多赛事详情，报名参赛即有机会赢取领克汽车Z10全年使用权，更有丰厚奖金与礼品等你带回家！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc8ca60bc6ac31816dc7296433f1232d.jpeg\" /></p><p></p>",
    "publish_time": "2024-08-27 17:44:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "夸克发布全新 PC 端，系统级全场景 AI 能力升级 AI 电脑",
    "url": "https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks",
    "summary": "<p>将一台电脑升级为 AI 电脑需要几步？今后只需安装一个夸克就够了！8 月 27 日，阿里智能信息事业群旗下夸克发布全新 PC 端，全面升级 AI 搜索、AI 写作、AI&nbsp;PPT、AI 文件总结等一系列功能。凭借“系统级全场景 AI”能力，夸克为你升级AI电脑，一站式完成信息的检索、创作和总结。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/48/32/4882ff0e6bc3b6c0c513d5eaaffb4932.png\" /></p><p></p><p>“始终面向用户、面向 AI、面向未来，夸克在人工智能领域持续探索突破性的用户体验。”阿里智能信息事业群总裁吴嘉表示，全新的夸克开启了创造革新性搜索产品的无限可能，也为阿里巴巴人工智能战略布局增添了强有力的路径与动能。</p><p>&nbsp;</p><p></p><h2>一、夸克 PC 端功能上新，为你升级一台 AI 电脑</h2><p></p><p></p><p>数字时代，PC 成为生产力的代名词，随着用户需求迭代，以及生成式 AI 技术跃迁，PC 的智能化改造成为必然。全新夸克 PC 端升级多项能力，让你的电脑秒变 AI 电脑，辅助你完成复杂、重复的任务，让效率再翻倍。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/3b/94/3bba3758ce5e89231a57bd7961c54f94.jpg\" /></p><p></p><p>一个月前夸克在 App 端推出的全新 AI 搜索，此次一并在 PC 端发布，并升级了更强的模型能力，提升到更快的交互速度。夸克AI回答的首字出现速度和吐字速度大幅领先行业，瞬时就能给你精准答案。三栏式的界面设计能更清晰地展现图文、视频等生成式回答和网页，让你一眼就能得到核心信息。</p><p></p><p>写作无疑是 PC 用户的高频需求，针对大学生、白领等重度用户，夸克就是你的“笔杆子”，任何体裁文章都能写得出色。当你输入主题和字数等要求后，夸克能撰写近 200 种类型的文稿，半分钟就能产出一篇高质量文章。夸克还提供多种方式撰写 PPT，比如输入主题智能生成大纲，或筛选模板再编辑内容，还支持 Word 一键转成 PPT。夸克帮你化繁为简，让你更专注创作本身。</p><p></p><p>此外，当你在 PC 上阅读大量的专业文档和网站内容时，想秒懂里边的内容，更需要一个会思考、能理解、会表达的小助理。夸克 AI 文件总结不惧几十万字的长文，能快速提取 PDF、Word、PPT 等文档中的核心内容，并通过持续提问、生成脑图等方式，更好地帮助用户理解关键信息。</p><p></p><p>夸克产品负责人郑嗣寿表示：“用户的需求在哪里，夸克就在哪里。夸克 PC 端给用户的信息检索、信息生成和信息处理带来了更快的速度和更强的效果，这是我们利用 AI 技术面向用户创造的新价值。”</p><p>&nbsp;</p><p></p><h2>二、系统级全场景 AI，随时随地帮你解决实际问题</h2><p></p><p></p><p>在 PC 中，用户会在桌面、文档、网页等多场景中进行操作，反复切换也练就了“黄金指”。夸克让电脑秒变AI电脑后，具备“系统级全场景 AI” 能力，在 Windows 电脑按下 Alt+Space 或苹果电脑的Option+Space，可以随时随地使用 AI 回答、AI 写作、AI&nbsp;PPT、AI 文件总结等功能。只要你有问题和需求，夸克的AI能力无处不在、触手可及。</p><p></p><p>比如在查网页、看文档等场景中，夸克能通过划词、截屏等方式，更加丝滑地进行搜索、解读、翻译和润色，无需再单独开启其他应用。就连辅导孩子作业，夸克也可以提供截屏搜索，依托海量学习题库和学习专属大模型，提供解题思路和答案，让自学和辅导的效率全面升级。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/34/c6/343e287e22353869be08e3588796c7c6.png\" /></p><p></p><p>“系统级全场景 AI”能力还会深入到电脑的每个场景中。右键点击文档，夸克能帮你快速总结Word、PDF、TXT等常用文件中的关键信息，还能一键帮你转换文档格式。夸克还提供官方插件，让你的浏览器变成AI浏览器，同样能使用 AI 回答、AI 写作、解读、翻译以及网页总结等功能。</p><p></p><p>在哪都能用，随你怎么用！夸克 PC 端不仅大幅提升了用户使用搜索、写作等功能的效率，也让 AI 电脑成为每个人的标配。随着用户需求的不断迭代，夸克的产品创新也会持续演进，让更多 AI 能力落地到不同设备的不同场景中。</p><p>&nbsp;</p><p></p><h2>三、突破性用户体验，创新践行“AI 驱动”战略</h2><p></p><p></p><p>夸克从诞生以来，以 AI 技术为业务发展引擎，面向用户探索下一代智能信息产品，短短数年就成长为用户过亿、增长强劲的新锐产品，尤其是获得了年轻人群的青睐。</p><p></p><p>进入 AI 时代，阿里集团将 AI 作为改变和加速业务增长的最强大变量，所有业务场景都可以通过人工智能创造更大的价值。夸克凭借多年积累沉淀的大模型技术、多应用场景、年轻用户群体等优势，大力革新搜索产品体验。自升级AI搜索以来，全新的夸克在用户规模与产品口碑方面均有不错的市场表现。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/70/32/70dcdeee01e1ca334695d46b2d75f532.png\" /></p><p></p><p>数据显示，6 月高考季，夸克高考 AI 搜索使用量超过 1 亿次。7 月，夸克升级“超级搜索框”，推出以 AI 搜索为中心的一站式 AI 服务，持续霸榜苹果应用商店免费榜。在《 2024 年第二季度 iOS 实力 AI 产品排行榜》，夸克作为 AI 搜索产品新兴势力，以 99.71 的高分在一众AI应用中位居榜首。</p><p></p><p>“生成式 AI 技术的突飞猛进，让夸克的目标和愿景更有机会得以落实和推进，加速了夸克的能力跃迁和产品迭代。”郑嗣寿透露，接下来，夸克会继续保持极快的迭代速度，在 AI 产品体验上迅猛推进，为用户创新一站式、多端一体的 AI 服务。</p>",
    "publish_time": "2024-08-27 18:18:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]