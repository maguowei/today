[
  {
    "title": "亚马逊云科技开源Mountpoint for Amazon S3，通过挂载点技术简化 Amazon S3 对象存储的使用",
    "url": "https://www.infoq.cn/article/OzPQdde0UPv9O5Cs9xAu",
    "summary": "<p>在<a href=\"https://aws.amazon.com/blogs/aws/celebrate-amazon-s3s-17th-birthday-at-aws-pi-day-2023/\">Pi Day</a>\"活动期间，AWS发布了<a href=\"https://aws.amazon.com/about-aws/whats-new/2023/03/mountpoint-amazon-s3/\">Mountpoint for Amazon S3</a>\"，这是一个开源文件客户端，可以提供Amazon S3的高吞吐量访问。该工具目前处于Alpha阶段，其本地挂载点提供了很高的单实例传输速率，主要用于数据湖应用程序。</p><p>&nbsp;</p><p>Mountpoint for Amazon S3将本地文件系统API调用转换为S3对象API调用，如GET和LIST。它支持对文件进行随机和顺序读操作，支持列出文件和目录。但该Alpha版本不支持写入操作（PUT），预计将来只支持新对象的顺序写入。</p><p>&nbsp;</p><p>AWS学者、德克萨斯大学助理教授<a href=\"https://www.amazon.science/author/james-bornholt\">James Bornholt</a>\"、AWS高级产品经理<a href=\"https://www.linkedin.com/in/devabrat-kumar/\">Devabrat Kumar</a>\"和AWS杰出工程师<a href=\"https://www.linkedin.com/in/andywarfield/\">Andy Warfield</a>\"承认，该客户端不是一个通用的网络文件系统，并且在文件操作上有一些限制，他们<a href=\"https://aws.amazon.com/blogs/storage/the-inside-story-on-mountpoint-for-amazon-s3-a-high-performance-open-source-file-client/\">写道</a>\"：</p><p>&nbsp;</p><p></p><blockquote>Mountpoint是为大规模分析型应用程序而设计的，它们可以并行读取和生成大量的S3数据，但不需要向现有对象中间写入数据。Mountpoint允许你将S3存储桶或前缀映射到实例的文件系统命名空间，遍历存储桶的内容，就像它们是本地文件一样，并实现对对象的高吞吐量访问。</blockquote><p></p><p>&nbsp;</p><p>这个开源客户端没有模拟目录重命名等操作，因为那会产生S3 API调用，或者说需要S3 API不支持的POSIX文件系统特性。</p><p>&nbsp;</p><p>Mountpoint for S3并不是第一个将S3作为文件系统呈现的客户端，我们也可以使用<a href=\"https://github.com/kahing/goofys\">Goofys</a>\"和<a href=\"https://github.com/s3fs-fuse/s3fs-fuse\">s3fs</a>\"这些流行的开源选项通过FUSE挂载桶。在Reddit上，有一些开发人员质疑<a href=\"https://www.reddit.com/r/aws/comments/11rdbhp/introducing_mountpoint_for_amazon_s3_a_file/\">新客户端的必要性</a>\"，并担心它会被用在<a href=\"https://www.reddit.com/r/aws/comments/11rdbhp/comment/jc80y8e/?utm_source=share&amp;utm_medium=web2x&amp;context=3\">数据湖之外的领域</a>\"，Bornholt、Kumar和Warfield写道：</p><p>&nbsp;</p><p></p><blockquote>Mountpoint并不是第一个用于访问S3的文件客户端——有几个开源的文件客户端，我们有客户使用过。然而，我们从这些客户那里都听到过的一个话题是，他们希望这些客户端能够提供与S3 REST API和AWS SDK相同的稳定性、性能和技术支持。</blockquote><p></p><p>&nbsp;</p><p>新客户端使用Rust在大多数AWS SDK都使用的<a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html\">公共运行时（CRT）</a>\"上构建，它依赖于<a href=\"https://aws.amazon.com/blogs/storage/how-automated-reasoning-helps-us-innovate-at-s3-scale/\">自动推理</a>\"来验证文件系统语义。The Duckbill Group首席云经济学家<a href=\"https://twitter.com/QuinnyPig/status/1635703963269107714\">Corey Quinn在推特上写道</a>\"：</p><p>&nbsp;</p><p></p><blockquote>哦不，AWS做了什么？15年来，我之所以没有呼吁人们不要将S3用作文件系统，就是希望S3团队自己来做这件事！</blockquote><p></p><p>&nbsp;</p><p>云专家和AWS无服务器英雄<a href=\"https://twitter.com/ben11kehoe/status/1635704924884303888\">Ben Kehoe提醒说</a>\"：</p><p>&nbsp;</p><p></p><blockquote>使用文件概念来考虑S3会使你对API的语义产生误解，最终做出错误的假设，当系统总是因为这些假设不成立而出现轻微的故障时，你就要难过了。</blockquote><p></p><p>&nbsp;</p><p>Mountpoint遵循Apache License 2.0许可，尚不能应用于生产工作负载。GitHub上提供了<a href=\"https://github.com/awslabs/mountpoint-s3\">初始Alpha版本</a>\"和<a href=\"https://github.com/orgs/awslabs/projects/84/views/1\">公共路线图</a>\"。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/mountpoint-amazon-s3/\">https://www.infoq.com/news/2023/03/mountpoint-amazon-s3/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://xie.infoq.cn/article/929a761df9243e16ee2317b5c\">通过 Amazon Managed Microsoft Active Directory 运行混合 Active Directory 服务</a>\"</p><p><a href=\"https://www.infoq.cn/article/LW5JeoLR0Jcl1ktRNV0o\">亚马逊云科技为蓝绿及金丝雀策略引入 CloudFront 持续部署</a>\"</p>",
    "publish_time": "2023-04-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "火山引擎发力大模型训练云服务：与字节国内业务“并池”，为大模型企业提供底层支持",
    "url": "https://www.infoq.cn/article/WKwdgBY1f4wdpjgoSDPf",
    "summary": "<p>4月18日，<a href=\"https://www.infoq.cn/video/xUq5Q66QSwsTU4G3sLQx\">火山引擎</a>\"在其举办的“原动力大会”上发布了自研DPU、分布式云原生平台、多云安全、多云<a href=\"https://xie.infoq.cn/article/fda69a1075f814cb8bd36d8b0\">CDN</a>\"、veStack混合云平台等等系列云产品，并推出新版机器学习平台：支持万卡级大模型训练、微秒级延迟网络，让大模型训练更稳更快。</p><p>&nbsp;</p><p>“国内有数十家做大模型的企业，大多已经在<a href=\"https://www.infoq.cn/article/yaV5V1R0iuUvpbrBIJYq\">火山引擎云</a>\"上”，火山引擎总裁谭待认为，大模型不会一家独大。与其他云厂商力推自家大模型不同的是，火山引擎将接入多家大模型深度合作，为企业和消费者提供更丰富的AI应用。</p><p>&nbsp;</p><p>另外在会上，火山引擎宣布与字节跳动国内业务并池。基于内外统一的云原生基础架构，抖音等业务的空闲计算资源可极速调度给火山引擎客户使用，离线业务资源分钟级调度10万核CPU，在线业务资源也可潮汐复用，弹性计算抢占式实例的价格最高可优惠80%以上。</p><p>&nbsp;</p><p></p><h2>上云，缓解成本焦虑</h2><p></p><p>&nbsp;</p><p>云的弹性，不仅能够帮助企业降低成本，也能大幅降低创新所需要的等待时间。尤其是大模型时代，训练和托管大模型都会带来大量的成本开销，继而带来巨大的价值不确定性。谭待认为，选择上云，是这个问题的最优解。火山引擎在这个方面的优势在于：</p><p>&nbsp;</p><p>资源复用，目前字节国内业务拥有过亿核CPU资源池、数十EB的企业存储。以此为基础，火山通过充分和抖音进行并池和混部，分钟级可调度十万级核CPU，实现更极致的弹性和性价比。全面云原生化，字节内部计算体系已经实现超过95%的容器化，并且基于云原生实现了超大规模存储池化。这些技术能帮助客户通过云原生进一步用好云的弹性，并且通过业务混布提升资源利用率。坚持全栈自研，包括自研服务器、自研OS、自研虚拟网络、自研mGPU技术等，将部署密度提升超过500%，为上层应用带来更高资源利用率。</p><p>&nbsp;</p><p>本次大会上，火山引擎还公布了全栈自研的新核心组件：火山引擎DPU。谭待表示，云计算的本质是资源池化和软件定义，但随着云基础设施规模越来越大，计算、存储、网络的虚拟化损耗始终占据10-20%的额外开销。要想提供更便宜的云服务，必须解决好这部分额外开销，把CPU和GPU释放到更关键的业务负载里。这就是火山要做DPU的原因。</p><p>&nbsp;</p><p>据悉，火山引擎DPU 整体网络性能升级到5000万pps转发能力，20us延迟。目前，字节内部已经实现上万台DPU的部署，并且将持续提升渗透率。基于自研DPU的各类计算实例性能也有显著提升，例如适用于大模型分布式并行训练场景的GPU实例，相较上一代实例集群性能最高提升3倍以上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2b55385f7663f003eab4e3651f1a05e.png\" /></p><p></p><p>谭待认为，未来3年内，大规模的算力中心都将形成“CPU+GPU+DPU”的混合算力结构：CPU负责通用计算，GPU负责AI计算，DPU负责资源卸载、加速和隔离，提升资源效率。</p><p></p><h4>应对多云挑战</h4><p></p><p>&nbsp;</p><p>会上，谭待分享了一组数据：2022年，火山引擎对超过4500个云消耗大于100万的企业进行调研，使用多云架构的企业占比达到88%，达到历史新高；根据麦肯锡的报告，到2025年依然会有42%的企业保留有私有云；根据IDC报告，25年超过30%的数据需要边缘实时处理。</p><p>&nbsp;</p><p>谭待表示，造成这些现象背后的原因是复杂的，既有业务形态和成本管控的原因，也有数据安全和监管要求的原因。对于企业来说，必须要有能力来解决好这一分布式多云体系带来的挑战。</p><p>&nbsp;</p><p>那么，企业如何建设好分布式云？火山引擎认为需要解决好三个问题：一是从单一公共云架构向多公共云架构升级，实现降本增效；二是从传统私有云架构向混合云架构升级，既能保障数据安全也能享受云的弹性；三是基于“算力靠近数据”的理念，形成覆盖1-40ms不同延时的架构方案，包括现场边缘、近场边缘和云边缘的体系化架构。</p><p>&nbsp;</p><p>谭待表示，火山引擎是中国最懂多云和分布式云的云计算公司。因为字节跳动成立11年以来，内部使用过全球几乎每一朵公有云服务和边缘云服务，形成了一整套完整的分布式云管理体系和架构实践。</p><p>&nbsp;</p><p>会上，火山引擎正式发布分布式云原生平台，能够支持超过20万节点，千万级核的使用场景，可以实现火山引擎云、第三方云、IDC 私有云和边缘云上集群资源与权限的统一管理，实施应用跨集群分发和故障迁移。面向边缘场景，火山引擎正式发布多云CDN管理平台。该平台基于字节数百TB带宽，十多家CDN厂商实操经验构建而成。安全方面，火山引擎发布了全栈多云安全平台。</p><p>&nbsp;</p><p>最后，面向公有云和私有云长期并存的场景，火山引擎正式推出混合云平台：veStack。据悉，veStack采取和火山公有云完全同源的架构设计，搭配火山DPU后可以让裸金属资源利用率达到100%。此外，veStack也支持多种异构存储协议，支持海光、鲲鹏等国产芯片服务器等。</p><p>&nbsp;</p><p></p><h2>云上如何创新</h2><p></p><p>&nbsp;</p><p>谭待分享了两个现象：一是多模态基础大模型的风起云涌。随着ChatGPT 、GPT-4的推出，国内的大模型创新公司也在奋力追赶；二是大模型在垂直行业的加速应用。</p><p>&nbsp;</p><p>在ChatGPT带动下，大模型成为新的技术热点。有人认为，大模型将成为云厂商弯道超车的机会。对此，谭待表示，大模型还在发展初期，数据安全、内容安全、隐私保护、版权保护等许多问题还需要解决。但可以预见，大模型将带动云上AI算力急剧增长，AI算力的工作负载与通用算力的差距会越来越小，这会为各家云厂商带来新的机会，同时也会对数据中心、软硬件栈、PaaS平台带来新的挑战。</p><p>&nbsp;</p><p>在大模型及下游应用发展推动下，无论传统企业还是初创企业，对AI算力都有迫切需求，企业使用多家云服务将成为常态。同时，各行业有自己高质量的私有语料，大模型不会一家独大，而是会在千行百业生长，形成多模型共生甚至协作的生态。谭待表示，未来将是“多云多模型”的时代。</p><p>&nbsp;</p><p>为此，火山引擎发布了面向推荐场景的高速训练引擎，支持100GB-10TB+超大模型的高效训练。该引擎实现高可用PS-Worker架构，支持 PS、Chief、Worker 全方位容错。另外还支持多种模型瘦身技术、多种GPU模式，训练加速比10倍以上，综合成本可降低25%~67%。</p><p>&nbsp;</p><p>同时，火山引擎对去年发布的机器学习平台进行了升级：一是全新的实验管理-模型效果对比功能：二是支持弹性实例，资源灵活调度；三是针对大模型训练场景进行了全面优化，目前可以支持万卡级别大模型训练场景，微秒级超低延时网络。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/99b2ef4cf764e3e20ac7b391288b66ec.png\" /></p><p></p><p>需要明确的是，火山引擎自己并不做大模型，主要是服务做大模型的公司，比如MiniMax、智谱AI等。</p><p>&nbsp;</p><p>谭待表示，国内很多科技公司投入到大模型建设中，他们有优秀的技术团队，也有丰富的行业知识和创新想法，但往往缺乏经过大规模场景实践的系统工程能力。火山引擎要做的就是为大模型客户提供高稳定性和高性价比的AI基础设施。</p><p>&nbsp;</p><p>谭待介绍，大模型需要大算力才能做出来，此外还需要很好的工程能力，去解决千卡甚至万卡并行的训练问题、网络问题，以及训练过程中如何更早地进行人为干预、观测。这些非常依赖机器学习平台。</p><p>&nbsp;</p><p>“MiniMax的训练场景都跑在火山引擎上，我们吸引它的点，除了算力供给外，最重要的就是通过机器学习帮它解决了很多工程上的问题，让它的资源更好地集中在业务上，即模型训练、分析和提效上等。”谭待表示，“所以对于火山引擎来说，除了算力供给，另外一方面就是云原生机器学习平台。”</p><p>&nbsp;</p><p>MiniMax是目前国内少数已经推出自研大模型产品的AI技术公司，拥有文本、视觉、声音三种通用大模型引擎能力。据MiniMax联合创始人杨斌介绍，MiniMax与火山引擎合作建立了超大规模实验平台，实现千卡级常态化训练；超大规模推理平台有万卡级算力池，支撑单日过亿次调用。在火山引擎的云上，MiniMax大模型业务实现快速突破。</p><p>&nbsp;</p><p>谭待透露，国内大模型领域，七成以上已是火山引擎客户。</p><p>&nbsp;</p><p></p><h4>机器学习做智能化</h4><p></p><p>&nbsp;</p><p>在杨震原看来，机器学习很重要的一点，是把问题数字化。数字化可以让这个问题可以定量评估。当问题可以定量评估的时候，接下来就可以智能化，进一步用机器学习的方法来优化。</p><p>&nbsp;</p><p>但用机器学习做智能化会主要面临两个问题：复杂和贵。复杂是因为机器学习软件栈很深，有PyTorch、TensorFlow等很多平台，也涉及到框架、操作系统，还有底层的硬件等，每个环节都要做对、做好。贵则体现在人力、数据、硬件等的昂贵上。</p><p>&nbsp;</p><p>字节跳动副总裁杨震原认为，机器学习做得好不好可以用下面的三角形来表示，其中最重要的是算法。算法在效果上做到领先就能对业务产生很大的价值。而支撑算法效果需求的有两件事：一是硬件ROI，另一个是人力ROI。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/13/13dc3503bff79a07b8dfe2d5fdd03059.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>硬件ROI指的是单位模型的成本，人力ROI 是指招一个厉害的算法工程师进来，他能否发挥最大潜能，主要看系统能否支持他足够容易、足够敏捷地去尝试新的想法。“业务创新需要试错，试错要大胆、敏捷，但试错也一定要控制成本。”杨震原表示，火山引擎通过潮汐、混部等方式，来实现资源的高利用率和极低成本。</p><p>&nbsp;</p><p>以抖音推荐系统为例，工程师用15个月的样本训练某个模型，5小时就能完成训练，成本只有5000元。据悉，火爆全网的抖音“AI绘画”特效，从启动到上线只用一周多时间，模型由一名算法工程师完成训练。</p><p>&nbsp;</p><p>杨震原表示，火山引擎的机器学习平台是内外统一的，火山引擎客户和抖音会用使用同样的平台。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-04-20 11:41:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]