[
  {
    "title": "腾讯汤道生：行业大模型已经过了“尝鲜期”",
    "url": "https://www.infoq.cn/article/uqKNBbQD7mcJS40u66WA",
    "summary": "<p></p><p>9月7日，在2023腾讯全球数字生态大会上，腾讯集团高级执行副总裁、云与智慧产业事业群CEO汤道生发表主题演讲，重点分享了AI大模型等技术产品对于产业发展的深远影响，以及腾讯云行业大模型的最新思考。</p><p>&nbsp;</p><p>“大语言模型的发展第一次让我们看到了AI在产业中大规模落地的可能。六个月前，很多企业惊叹于通用大模型的生成能力，迫不及待地尝试与业务结合。但很快发现，通用大模型在实际应用中，面临专业度、准确度、数据安全、成本等很多挑战。”汤道生说</p><p>&nbsp;</p><p>汤道生提到，行业大模型已经过了“尝鲜期”，目前正深入各类业务场景，推动企业全链条智能化。“六个月前，客户来谈大模型，能想到的应用场景基本只是文字客服。但今天，应用场景已经快速扩展到各个领域，比如在金融行业，大模型已经应用在开户、业务处理、风控等多个场景。”为此，腾讯尝试用大模型打造全栈产品能力，用于业务不同环节，助力企业全链条提质增效。</p><p>&nbsp;</p><p>与此同时，汤道生提到表示，在大模型产业化的落地过程中，客户关注的不仅是模型的大小、功能，而是更为关注如何选用并定制适合自身业务发展的模型产品。基于这些实际需求，腾讯云不断升级大模型精选商店，包括自研的通用大模型“混元”、20多种主流开源模型和更多行业大模型。企业可以根据自身需求选择合适的模型产品，并进行训练和精调，以满足个性化需求。</p><p>&nbsp;</p><p>企业专属模型的生成，涉及到数字资产资源管理、数据标注、训练、评估、测试和部署等很多环节。同时，根据业务发展，企业模型需要不断地调优、迭代，数据处理的整个过程，也要不断地重复。汤道生认为，模型的热潮导致各项成本攀升，拥抱大模型不能只是一时冲动，还要理性考虑落地成本，训练、推理的效率。</p><p>&nbsp;</p><p>针对此，腾讯云也提供了从数据处理、多机多卡训练到硬件优化的一站式解决方案，以帮助企业高效、低成本地创建和使用大模型。同时，腾讯云TI平台还进行了全新升级，有效提升了大模型的训练速度和推理效率。</p><p>&nbsp;</p><p>“AI大模型等产品的发展及落地将对千行百业产生‘质’的影响，我们也将持续开放产品能力、不断探索技术应用，助力产业实现更高质量的发展。”汤道生表示。</p><p>&nbsp;</p>",
    "publish_time": "2023-09-10 10:58:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "与x86、ARM三分天下，全球“开花”的RISC-V如何成为中国最受欢迎芯片架构？",
    "url": "https://www.infoq.cn/article/B5Z8aNCuJRkR7mcEdjtU",
    "summary": "<p>2010 年，加州大学伯克利分校的研究团队开始了一项新的探索项目：希望创建一个全新的开源指令集架构，这个架构具有易于理解的简洁指令集，并且能够提供高性能和低功耗的性能。这个项目就是 RISC-V。与大多数指令集相比，RISC-V 指令集可以自由地用于任何目的，允许任何人设计、制造和销售 RISC-V 芯片和软件。</p><p>&nbsp;</p><p>经过十余年的技术发展和社区的不断努力，如今 RISC-V 已经成为和 x86、ARM 并列的三大主流芯片架构。目前，RISC-V 被广泛应用于物联网、智能家居、AI、高性能计算等多个领域。RISC-V 国际基金会去年年底公布的数据显示，RISC-V 国际基金会的会员数量同比增长超过 26%，在 70 个国家/地区拥有超过 3180 名会员。如今，市场上有超过 100 亿个 RISC-V 核心，全球有数万名工程师致力于 RISC-V 计划。</p><p>&nbsp;</p><p>那么，RISC-V 是如何在一众指令集架构中脱颖而出的？如何更好地推进 RISC-V 技术创新和生态建设？如何才能进一步推动 RISC-V 的产业化落地？</p><p>&nbsp;</p><p>日前，在“第三届 2023 RISC-V 中国峰会”上，RISC-V 国际基金会 CEO Calista Redmond，本届峰会主席、中国科学院软件研究所总工程师武延军，开源芯片研究院首席科学家、中国科学院计算技术研究所副所长包云岗，平头哥半导体生态副总裁杨静，奕斯伟计算高级副总裁、首席技术官何宁，沁恒微电子技术总监、瑞斯科研究院院长杨勇，芯来科技市场战略副总裁李珏，算能科技产品总监陆吉年，Andes 晶心科技董事长暨执行长林志明，赛昉科技董事长兼 CEO 徐滔，SiFive 产品营销高级经理林宗民，Semidynamics 创始人兼 CEO Roger Espasa 接受了 InfoQ 在内的媒体采访，进一步分享大家对于&nbsp;RISC-V 技术发展和生态建设的理解与实践经验。</p><p></p><h2>RISC-V 缘何杀出重围？</h2><p></p><p>&nbsp;</p><p>基于开源开放的特点，RISC-V 架构在过去十余年中飞速发展，一跃成为 x86、ARM 最强有力的挑战者。有数据指出，ARM 架构用了 17 年完成 100 亿内核出货量，而 RISC-V 只用了 12 年。此外有预测数据显示，到 2025 年 RISC-V 架构的处理器核的出货量将突破 800 亿颗。</p><p>&nbsp;</p><p>回顾历史，RISC-V 并不是唯一开源的指令集架构，此前不少指令集架构有过短暂的辉煌，但终究没能稳坐浪潮之巅，RISC-V 缘何杀出重围？</p><p>&nbsp;</p><p>Calista Redmond 在接受采访时表示，一种架构能够被多方所拥抱、采纳，历来是以波浪式的方式所进行的。有的公司愿意做“第一个吃螃蟹的人”，会最早采纳新的架构；有的公司是“等待者”，他们看有更多同类型的公司采纳新架构，才会有拥抱意愿；有的公司是“后来者”，他们要等到整个行业都采用这个新架构，整个生态系统也建立起来了，或者他们的合作方、合作伙伴有采用 RISC-V 架构的要求，才会加入进来。“市场的影响力是巨大的。对于多数公司来说，被客户推动是他们选择新架构最大的因素，比如授权证书方面的条款，以及客户对兼容性提出的要求。此外，未来公司的机会在什么地方，也是他们选择新架构的驱动因素。”</p><p>&nbsp;</p><p>Calista Redmond 认为，过去的 50 多年里确实曾经出现过多个指令集架构，但只有 x86、ARM 这两个指令集架构与众不同，并且站稳了脚跟。其背后的原因就是&nbsp;x86 和 ARM&nbsp;在工作负载、规模以及纵向的整合能力方面有比较突出的表现。</p><p>&nbsp;</p><p>“现在移动终端的计算和过去有所不同，我们相信，在这个新的时代，一个指令集架构是否能够变得成熟丰富，一方面取决于生态的建立以及商业模式，另一方面取决于架构本身，这都构成了指令集架构的成功变量。这里面要特别强调的是开放性和合作，具体包括和不同的合作伙伴以及客户的合作，这些非常重要。此外，在这个时代，指令集架构能够给予设计者以及设计过程更大的自由度，避免和一个公司深度捆绑带来的一种局限。”Calista Redmond 说道。</p><p>&nbsp;</p><p>在技术特性方面，RISC-V 与其他指令集架构相比有很多相似点，区别在于 RISC-V 在技术设计上更有弹性，更有针对未来的扩充性，在商业模式上也能提供更大的灵活度。“这个灵活度是大家检视自己的应用，或者往新应用发展的时候，不依赖单一的供应商，这应该是大家真正喜欢用 RISC-V 做未来新应用的原因。”林宗民表示，过去几年国内拥抱 RISC-V 的热情高涨，在软硬件生态上持续投入，在全世界都是有目共睹的。</p><p>&nbsp;</p><p>此外，不少受访专家提到，过去几年 RISC-V 最明显的变化就是逐渐走向高端。</p><p>&nbsp;</p><p>何宁表示，过去，RISC-V 架构主要应用于物联网领域。在最近两年，大家能明显感受到 RISC-V 已经开始在一些中强生态的场景中得到应用，如 AI 计算、车载、多媒体计算等等。“比如在多媒体领域，Android TV 带有一定的生态，但 APP 或软件数量不如平板和手机多，相对比较可控。此外，一些电视主机厂也在开发自己的操作系统和应用程序。在这些场景中，客户有时会询问我们 RISC-V 架构的芯片何时能够落地，或者能否在这方面合作。未来，这些领域都可能是 RISC-V 非常好的应用场景。另外还有一些对算力有要求但不涉及生态软件的领域，例如 DPU，也可能是 RISC-V 非常好的落地场景。总的来说，在过去的两三年里，RISC-V 可以发力的地方非常多。”</p><p>&nbsp;</p><p>在徐滔看来，最近两年，RISC-V 走向高端的趋势已经越来越明显。随着 RISC-V 核规格的不断升级，从 A5 提升到 A7，甚至更高，其性能和功能也在不断提升。与此同时，超过 10 核的 RISC-V 版本也越来越多。“基于 RISC-V 的高性能芯片已经用于数据中心和 AI 领域。在美国，不少 Startup 公司都专注于数据中心的高性能计算。从投资的角度看，去年以来，在 RISC-V 领域涌现出了一批专注于高性能计算的 Startup 公司，并且拿到了投资。虽然在高端芯片市场还有很多其他好产品，但是 RISC-V 的变化和发展是值得注意的。”</p><p>&nbsp;</p><p>这种高端化的应用趋势能够推动 RISC-V 进一步发展，并促进 RISC-V 生态系统不断壮大，为更多的应用领域提供更好的支持和解决方案。</p><p></p><h2>RISC-V 生态建设进入加速期</h2><p></p><p>&nbsp;</p><p>对于 RISC-V 而言，能否在市场上取得成功，很大程度取决于生态。然而在早期的发展过程中，RISC-V 处理器主要集中在微控制器、微控制单元（MCU）这一层次，对于 MCU 以及嵌入式领域的一些应用来说，其生态短板明显。随着处理器能力不断升级，RISC-V 成功登陆 PC、笔记本和服务器，有越来越多的软件可以跑在上面。这时，生态才真正显现出来。</p><p>&nbsp;</p><p>Calista Redmond 表示，RISC-V 国际基金会非常看重生态的建立，在生态建设上会采取非常广泛的视野，同时也强调全球方面的合作，比如和其他基金会合作，以加速这种生态的建成。</p><p>&nbsp;</p><p>“对我们来说，生态系统中最重要的是利益相关方，包括工程师、学生、中小企业、跨国企业，还有硬件、软件生产厂商，以及各种工具、资源等等。谈到生态，我们相信它既是技术的问题，也是市场营销、商务的问题。我们强调不同技术团队来应对不同行业的需求，比如汽车制造业、数据中心，同时，我们也强调实施的重要性。在市场发展上，我们会一步步地关注汽车产业、数据中心、云部署、通讯业、移动终端等。在中国，我们强调与不同的协会进行合作，并与中国的学术界和行业合作，以帮助这个生态系统变得越来越成熟。我们还重视与硬件厂商的合作，相信这个生态系统能够使各种应用无缝链接移植到 RISC-V 技术架构上。因此，对我们来说，生态建设和合作具有非常重要的意义。任何玩家肯定都会强调多架构的部署，同时也要强调持续的优化。”</p><p>&nbsp;</p><p>在工具链层面上，从 2020 年开始，RISC-V 的软件优先级重要性提升，发展模式也变成了以软件驱动为主。要想支持 RISC-V，首先需要建立工具链，对于芯片来说，必须有编译工具链才能发挥其指令集特性。</p><p>&nbsp;</p><p>据武延军介绍，工具链一般有两种方式：一种是厂家自己开发，提供私有的工具链，每推出一款开发平台就有一套与之匹配的工具链；另一种是大家共同打造一个公共的工具链，对于私有的部分，可以以 binary 的方式提供，但其中 90% 都是公共打造的。对于 RISC-V 来说，第二种方式更为合适。</p><p>&nbsp;</p><p>“在几年前中国科学院先导专项支持下，我们开始开发 RISC-V 原生操作系统，其中很核心的一点就是做相应的编译工具链。我们在 GCC 和 ALVM（开源的架构编译器框架）上大量做指令集相关后端的实现，同时跟随 RISC-V 基金会标准的及时进程提供相应的参考实现。”武延军表示，现在的工具链参与方越来越多，功能也越来越完善。随着 RISC-V 指令集不断发展，指令集标准规范逐步完善，工具链也将逐渐成熟起来。“在过去的一年多时间里，我们陆续看到了很多知名开源社区、开源操作系统发行版都在支持 RISC-V。RISC-V 已经在软件生态建设的路上，走得越来越快。”</p><p>&nbsp;</p><p>经过十余年发展，RISC-V&nbsp;生态不断壮大，但仍有大量问题亟待解决。包云岗在接受采访时表示，要想让 RISC-V 在更多领域里发挥作用，软硬件仍很多工作要做。</p><p>&nbsp;</p><p>具体来说，在硬件层面，除了&nbsp;RISC-V CPU Core，还有外围互联的芯片，怎么把这些 Core 更好地连接起来仍有很多工作要做。目前在 RISC-V 硬件生态领域里缺少高质量的 IP，这可能会制约数据中心服务器级芯片的进一步提升。</p><p>&nbsp;</p><p>软件生态方面，当前软件生态最大的复杂度是大量的软件包需要移植到 RISC-V 整个架构上，这些软件包又分为系统软件和应用软件。系统软件包方面，像 RadHat 有很多发行版，每个发行版都有两三万个软件包，这些软件包都需要移植过来；应用软件包方面，以 Android 软件为例，加起来大概有 900 万个 App，如果用 RISC-V 来做手机芯片，需要考虑如何将这些应用移植过来。这些都是存量生态里需要解决的问题。</p><p>&nbsp;</p><p>包云岗认为，最好的方式是找到增量的生态和增量应用场景。无论是 x86 还是 ARM，现在还没有形成主流，RISC-V 正与它们处于同一条起跑线上。在这种情况下，RISC-V 的优势就可以发挥出来。它可以通过开放和开源的方式，联合全世界更多的开发人员，快速构建和发展这个生态。“大家都在关注这个新市场，很多人认为移动汽车是未来有前景的领域，而 RISC-V 也可以在其中发挥作用。我们期待在未来，比如从汽车领域的自动驾驶领域切入，形成新的生态，并逐步影响其他传统的生态。”</p><p>&nbsp;</p><p>当前，越来越多的中国企业积极参与到 RISC-V 国际生态建设中，对于那些还未参与到 RISC-V 生态的中国企业，武延军认为首先需要在认识层面上进一步提升。比如，有时为 RISC-V 软件生态做贡献可能没有直接的收益，但如果代码能进入主流系统代码，如 Linux 内核、GCC 编译器支持、LLVM 编译器支持，意味着能够在更大的舞台展示自身实力，并提高在软件生态中的发言权。</p><p>&nbsp;</p><p>“如果能够真正参与并贡献到主流的上游软件，将获得双重收益：一是品牌效应，二是对运营成本的降低。中科院软件所的目标是将基础软件做成 RISC-V 软件中的数字公共品，我们很早就在 RISC-V 社区中组建了 PLCT 实验室，专门为 RISC-V 生态做软件贡献，并在过去几年得到了国际社区的认可。现在，中科院也有新的先导专项，专项名称是 RISC-V 基础软件，以支持继续打造 RISC-V 基础软件生态。在这个过程中，我们将联合国内企业共同为 RISC-V 生态做贡献。”武延军说道。</p><p></p><h2>如何推动RISC-V产业化落地？</h2><p></p><p>&nbsp;</p><p>目前来看，RISC-V 已经进入了从 1 到 N 的阶段，并被广泛应用于物联网、智能家居、AI、高性能计算等多个领域，也有越来越多的企业开始拥抱 RISC-V。</p><p>&nbsp;</p><p>杨勇表示，基于新指令集架构设计内核和芯片一直存在两个问题，即生态和自主。但随着RISC-V的逐步流行，生态和自主不能兼顾的情况有所转变。“我们早先选择RISC-V设计芯片 时就意识到生态的必要性，所以我们推动了免费IDE MRS全面支持了青稞RISC-V全系列芯片。我们一方面自主研发RISC-V处理器及芯片，另一方面提供USB/蓝牙/以太网相关的应用解决方案，两个方面齐头并进，双轮驱动助力RISC-V尽早遍地开花。”</p><p>&nbsp;</p><p>目前 RISC-V 已经进入到一个主频在 2GHz 以上的高性能阶段，很多应用都可以使用 RISC-V 来实现。未来，从边缘侧的 AP 迭代到工业界的智能化升级，甚至到车载的 ADAS 系统或数据中心对高级算力的需求，都会有 RISC-V 发挥潜能的地方。</p><p>&nbsp;</p><p>“我想越来越多的人会看到，高性能的核和更丰富的软件，这两条路一定要一起走。”杨静认为，除了要关注 RISC-V 在 CPU 性能上的提高，还需要关注生态中的两个重要组成部分：</p><p>&nbsp;</p><p>首先，在高性能之上，需要更丰富、更好用的软件配套。从应用落到硬件，中间的过程非常漫长，需要所有合作伙伴共同努力，将软件能力补齐，才能真正帮助RISC-V 应用于各个领域。</p><p>其次，任何一个新的架构都需要找准自己的生长点，与“杀手级应用”协同爆发。比如，当年 ARM 的崛起，就是依托于智能手机市场的爆发。而如今，AI浪潮袭来，也让AI原生的 RISC-V架构有更多的“出圈机会”。依托于RISC-V本身具有的灵活和可扩展性等特点，通过平头哥 Vector、Matrix 扩展指令集以及 AI 部署工具HHB等，RISC-V完全有能力在AI领域取得新突破。</p><p>&nbsp;</p><p>“如果今天能够将RISC-V架构原生的AI潜力充分释放，使 CPU 的能力与RISC-V的扩展相结合，那么RISC-V 就一定能够打入 AI 领域。现在 AI 已经渗透到各行各业，RISC-V 除了需要继续提高计算能力，也需要加强 AI 能力，这也是我们平头哥接下来会持续投入的方向。”杨静说。</p><p>&nbsp;</p><p>那么，如何才能进一步推动 RISC-V 的产业化落地？</p><p>&nbsp;</p><p>何宁认为关键在于产品。目前，奕斯伟计算构建了数十款 RISC-V 架构的产品，在推动 RISC-V 产业化落地方面，何宁总结了三点心得体会：</p><p>&nbsp;</p><p>首先，需要拥有强大的内核研发能力。从内核研发到产业化落地，在同一个公司内部周期非常短，产品团队可以直接向内核团队提出需求，并快速调整，以适应市场需求。这种适配能力可以为产品带来强大的竞争优势。其次，产品本身要有行业影响力。一个行业的头部产品能够为其他产品提供示范效应，能够推动 RISC-V 架构的落地。最后，要实现生态的规模化推广，需要确保有足够的应用产品数量。奕斯伟计算的经验是，只要能够采用 RISC-V 架构，产品都会采用，每年都有许多颗 RISC-V 芯片同时实现产业化落地，采用 RISC-V 架构的比例越来越高。这种规模化效应非常强大，能够推动 RISC-V 生态的快速发展。</p><p>&nbsp;</p><p>同时，RISC-V 架构的出现也为车规应用带来了新机遇，不少受访专家提到，车辆是 RISC-V 新的爆发点。</p><p>&nbsp;</p><p>在过去，车载芯片要么采用 ARM 架构，要么采用欧洲芯片公司的私有架构，这些架构都需要在欧洲严格的车辆场景下通过认证。中国公司在开发车载芯片时，要想满足相关标准或是出口到欧洲，必须按照车规标准来打造芯片和系统。对于国内芯片厂商来说，在 IP 层面的选择非常少。</p><p>&nbsp;</p><p>RISC-V 开放架构的出现，使得基于该架构的车规级 IP 得以实现，这将极大地拓宽应用领域的范畴。此外，RISC-V 具有许多可以定制的优势，在这方面采取了与 ARM 不同的策略：ARM 先有通用的内核，然后在移动端有 Android 生态和一些特殊的 iOS 系统需求。但在汽车领域，目前还没有一个统一的操作系统或统一的 AUTOSAR 架构（AUTomotive Open System Architecture，汽车开放系统架构）。</p><p>&nbsp;</p><p>李珏认为，当前汽车软件和工具链仍比较碎片化，“如果使用 ARM 一个架构适用于所有情况，那么从上层软件厂商和操作系统到应用程序厂商都很难找到通用性。现在购买的电动车或新能源车的 APP 也不是通用的。包括地图，每家都在进行适配，就像以前的的山寨机一样，一款 APP 需要适配各家的机型，现在车辆上还是这种情况。因此，如果从底层的指令集上支持这些应用是一个标准的话，就可以支持多家核、多家标准、多家处理器核和多家芯片，这样上层开发软件就有可能利用 RISC-V 的标准场景做统一。这样，RISC-V 在汽车电子领域将有巨大的应用市场。”</p><p>&nbsp;</p><p>李珏提到，两年前，芯来开始在国内做车规级芯片的布局。大约半年后，有客户开始导入芯来的 IP，使用芯来的 IP 进行车规芯片的开发。“我们获得了车规认证后，他的 IP 和 CPU 部分就不需要再证明功能安全的等级特性，可以将 CPU 视为一个黑盒使用，只需要证明他的外围设备、工具链以及最终的软件是车规级的即可。这将有助于加快国内一些公司开发车规级芯片的流程，缩短时间。”</p>",
    "publish_time": "2023-09-10 15:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型之战，腾讯来了",
    "url": "https://www.infoq.cn/article/FksReogLLESJpqOR3ZxD",
    "summary": "<p>9月7日，腾讯在数字生态大会上正式推出混元大模型。据介绍，腾讯混元大模型是由腾讯全链路自研的通用大语言模型，拥有超千亿参数规模，预训练语料超2万亿tokens，具备强大的中文创作能力，复杂语境下的逻辑推理能力，以及可靠的任务执行能力。</p><p>&nbsp;</p><p>目前，腾讯云、腾讯广告、腾讯游戏、腾讯金融科技、腾讯会议、腾讯文档、微信搜一搜、QQ浏览器等超过50个腾讯业务和产品，已经接入腾讯混元大模型测试。同时，腾讯混元大模型将作为腾讯云MaaS服务的底座，客户不仅可以直接通过API调用，也可以将混元大模型作为基底模型，为不同产业场景构建专属应用。</p><p>&nbsp;</p><p>在这一波大模型浪潮中，云服务被认为是最理想的大模型承载平台，而大模型也将引领下一代云服务的演进。通过云厂商所提供的基础设施、模型服务、训练加速框架等支持，大模型的能力将很快渗透到各行各业中。</p><p>&nbsp;</p><p>与此同时，算力紧缺是摆在眼前的困境。如何在有限的卡上，构建稳定可靠的算力集群，如何让有限的算力资源发挥出最大的价值，也成为各家云厂商和模型公司最重要的课题。</p><p>&nbsp;</p><p></p><h2>大模型时代的赢家</h2><p></p><p>&nbsp;</p><p>如今我们正处于“智能涌现”的风口浪尖，人工智能只需访问每天产生的 2.5 万亿字节数据中的一小部分，就能创造出人类智力无法比拟的奇迹。不久前，由 Google DeepMind 构建的AlphaDev，结合了计算机推理和直觉，帮助我们人类发现一些我们原本不知道的东西：一种全新且更快的排序算法，排序速度上取得的突破超越了科学家们几十年来的研究。</p><p>&nbsp;</p><p>人类可能需要 20 年的时间才能成为领域专家，然后将这种思维应用于解决实际问题。如今，人工智能可以在几分钟或几秒钟内实现这种专业化。</p><p>&nbsp;</p><p>腾讯集团副总裁、云与智慧产业事业群COO、腾讯云总裁邱跃鹏将今年这一特殊的年份描述为“大模型的元年”，他说：“今天有点像移动互联网刚到来的时候，那一年我们往后看，可能没有太多人想到十几年之后移动互联网带来的深刻改变。”</p><p>&nbsp;</p><p>如今，在技术创新带来的这股潮流趋势下，全球的科技企业都开始了GPT“军备”竞赛，希望通过研发更强大的GPT模型来增强他们的产品和服务。以微软纳德拉为首的一众大佬都声称要将“所有产品上搭载AI，彻底改造”。</p><p>&nbsp;</p><p>然而，人工智能的高成本构成了一个不容忽视的难题，这也成为了许多企业参与的障碍，大企业反而比初创企业更能占据有利的地位。<a href=\"https://www.semianalysis.com/p/the-inference-cost-of-search-disruption\">Semianalysis&nbsp;</a>\"估计，截至 2023 年 2 月，OpenAI 使用超过 3,600 台 Nvidia HGX A100 服务器来为 ChatGPT 提供服务。这些 HGX 服务器均包含 8 个 A100 GPU，每台成本为&nbsp;$10,000美元。这大约相当于为 ChatGPT 提供服务的硬件成本高达 2.88 亿美元，显然这些基础设施只有大型云服务提供商才能提供。</p><p>&nbsp;</p><p>另一方面，训练和“推理”（实际运行）大语言模型的高昂成本是一种结构性成本，与之前的计算热潮不同。即使软件被构建或训练，它仍然需要大量的计算资源来运行大语言模型，因为每次生成提示响应时都需要进行数十亿次计算。相比之下，我们日常使用的应用程序所需的计算要少得多。也就是说，训练模型的成本只是冰山一角，隐藏在水面之下还有巨大的“推理成本”，即每次调用模型输出时产生的成本。<a href=\"https://www.semianalysis.com/p/the-inference-cost-of-search-disruption\">Semianalysis</a>\"表示，“以任何合理规模部署模型时，推理成本远远超过训练成本。事实上，ChatGPT 推理的成本超过了每周的训练成本。”</p><p>&nbsp;</p><p>因此，以最低的综合成本获取算力资源已经成为人工智能公司成功的关键因素，而在云计算行业中，那些提供这种“铲子”的企业价值不可小觑。</p><p>&nbsp;</p><p>“针对大模型所有相关能力的投入和研究是必需的，所有云厂商都必须做好对于AGI的支持，这是一个必答题，没有人能不答这道题。”邱跃鹏表示。</p><p>&nbsp;</p><p>“目前投资规模最大的是在训练，没有几十亿的资本投入，很难下场去持续做大模型。另一方面，真的下场做大模型的企业不多，未来真正的应用场景还是在下游。结合场景看，未来，推理是比训练更大的市场。今天我们看到很多场景用了大模型之后，带来的效果提升确实非常明显，这说明大模型的商业模式相对来说更清晰。虽然目前还很难讲具体的算力需求究竟有多少，但一定是非常长期的机会。”</p><p>&nbsp;</p><p></p><h2>传统云服务能满足需求吗？</h2><p></p><p>&nbsp;</p><p>传统的云基础设施并不是为支持大规模人工智能而设计的，随着AI普及度和复杂度越来越高，云厂商也面临了一些全新的挑战，计算、存储以及IT架构等层面都发生着翻天覆地的变化。</p><p>&nbsp;</p><p>传统云服务大部分由通用 CPU 的服务器组成，而更适合运行 AI 工作负载的 GPU 集群只占基础设施的一小部分。根据英伟达在2023 年台北电脑展会上的演讲，如果使用GPU进行训练，相比 CPU 服务器，客户可以以 4% 的成本和 1.2% 的电力消耗来训练一个 LLM。因此，该公司表示CPU 已落伍，用GPU 才是训练大语言模型的首选。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4e/4ee9da3fe81cd3493ab87d6659d7486d.png\" /></p><p></p><p>&nbsp;</p><p>传统服务器和AI服务器对GPU的依赖对比，来自摩根大通的估计。</p><p>&nbsp;</p><p>&nbsp;</p><p>但高密度算力需求，也会给云服务带来影响，比如IDC（数据中心）的规划对于高密集算力而言，仍然有许多瓶颈，例如需要高功率供电的机架。高功率的电力供给基础设施都很早期，高功率供电的机架很稀缺。</p><p>&nbsp;</p><p>邱跃鹏提到，“IDC是非常长周期的投资，面向未来，算力基础设施也需要相应地升级，来更好地承接大模型带来的新需求。”</p><p>&nbsp;</p><p>而从存储方面来说，在过去几年中，大语言模型的尺寸每年平均增长了10倍，参数数量从数百万个到万亿不等，大语言模型（LLM）也会面临存储容量的挑战，单个存储设备是不可能满足存储要求的。例如，OpenAI的GPT-3模型拥有1750亿个参数，仅其参数就需要超过300GB的存储空间。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4f0e4a71e6de2726b9b74c1cd9f55bd6.png\" /></p><p></p><p>&nbsp;</p><p>正如 OpenAI 论文“Language Models are Few-Shot Learners”中表示的那样，较大的模型往往表现更好，然而，参数数量增加也提出了更高的存储容量需求。如果在读取数据时机器出现问题，或者导致SSD硬件故障，都是不能容忍的。</p><p>&nbsp;</p><p>最后一个是大模型“解锁”了各行各业的数据。以前是个别企业带着大家通过AI技术逐个攻克单一领域的问题，模型参数量和算力需求在多年来其实也一直是一个比较稳定的水平，据估算，海量数据因为依赖结构化也只用起来了20%。而现在，随着大模型通用能力的加持，各行各业的万千企业都已经参与到了这里面来，Embeding（向量化）技术让非结构化数据的应用也有了更多的想象空间，模型训练的所需算力也变成了千卡规模，数据检索也会面临更大的压力。</p><p>&nbsp;</p><p>基于以上种种挑战，腾讯云认为，在新的大模型时代，需要更高效的云技术。如果把之前的云称为AI 1.0时代，那么在1.0时代里，行业着重关注的是单机单卡的性能、标量数据的结构化精确检索，以及云原生带来的自动调度。而现在，当行业进入到了AI 2.0时代，在这个背景下，由于服务重点发生了转变，现在更多关注AI企业和大模型，云厂商需要将重点转向集群性能的提升、向量数据的存储与检索等方向上。</p><p>&nbsp;</p><p></p><h2>腾讯云是如何解决当前挑战的</h2><p></p><p>&nbsp;</p><p>传统云计算已经无法满足AI企业及大模型诉求，云智算已成为了关键支撑，腾讯面向AI场景专用，打造了腾讯云AI超级底座。</p><p>&nbsp;</p><p>在这次大会上，我们关注到了腾讯云AI超级底座的几个关键产品：</p><p>&nbsp;</p><p></p><h3>高性能算力集群</h3><p></p><p>&nbsp;</p><p></p><blockquote>对于想要自己打造大模型的企业而言，需要考虑的头一件事，就是算力够不够。</blockquote><p></p><p>&nbsp;</p><p>基于当下大模型大规模、高效率的训练需求，腾讯云上线了全新一代高性能计算集群HCC，相比上一代整体提升了3倍。在实际业务测试当中，业内传统的集群方案训练一次需要50天的时间，而腾讯云只需要4天。</p><p>&nbsp;</p><p>HCC不止是一个单纯的算力资源，还包含了目前腾讯最先进的存储和3.2T RDMA网络能力，以及上层最前沿的软件定义技术、云原生编排技术和加速框架。</p><p>&nbsp;</p><p>存储层面，训练场景下，几千台计算节点会同时读取一批数据集，需要尽可能缩短数据集的加载时长。腾讯云提供了COS+GooseFS 对象存储方案，提升端到端的数据读取性能，以及CFS Turbo 高性能并行文件存储方案，解決大模型场景大数据量、高带宽、低延时的诉求。</p><p>&nbsp;</p><p>腾讯称，目前HCC已成为国内性能最强的训练集群，算力能够实现无损释放。</p><p>&nbsp;</p><p>他们在硬件方面进行了大量的优化和升级，不断提高了集群的计算能力和稳定性，基于腾讯自研星星海服务器，搭载了目前业内领先的GPU芯片，支持单卡在FP16精度下输出989TFlops的算力（如果在精度差一点的推理场景下，可以接近2000TFlops）。</p><p>&nbsp;</p><p>另外，运行成本是开发者普遍非常关心的问题，大模型兴起初期，有开发者抱怨说：“跑一把大模型 ，体验就感觉就像坐在一辆被困在路上的出租车里：要么下车，要么盯着计价器跳动”。</p><p>&nbsp;</p><p>虽然云计算工程师们已经能把故障率降得很低很低，但由于大模型任务和环境的复杂，仍然有可能出现偶发的中断。训练中断也是当前整个行业高度关注的话题。</p><p>&nbsp;</p><p>腾讯云基于云原生能力，能够支持集群的监控和断点续算能力。为此腾讯提供了7*24小时的全局监控视角，支持编排、框架、实例等多层级的指标监控。一旦触发故障，系统能够在5分钟内恢复任务，10分钟内恢复基础设施，并且无需人工干预，就能够自动最大化保障任务的连续进行。一个粗略的计算是，每减少一小时异常，干卡规模可节省数十万元成本。</p><p>&nbsp;</p><p></p><h3>高性能数据处理</h3><p></p><p>&nbsp;</p><p></p><blockquote>在 AI 2.0时代，所有的数据最终都将向量化。</blockquote><p></p><p>&nbsp;</p><p>为了满足企业在这个新时代对数据检索的需求，腾讯推出了一款专为 AI 场景打造的数据库：腾讯云向量数据库。</p><p>&nbsp;</p><p>这是一款企业级的分布式向量数据库，相较于传统的单机插件式向量数据库方案，腾讯提供了 10 倍的单索引规模，支持高达 10 亿级行数，助力企业应对海量非结构化数据检索的挑战。</p><p>&nbsp;</p><p>许多企业在 AI 接入过程中，Embedding 工程成为了非常大的瓶颈。而腾讯向量数据库集成 Embedding 能力，可以使得企业数据接入 AI 的工期从 30 天缩短到了 3 天，效率提升10倍。</p><p>&nbsp;</p><p></p><blockquote>大模型不仅需要大算力，还需要海量的高质量数据。</blockquote><p></p><p>&nbsp;</p><p>大模型的成功依赖于三个要素：模型、算力和数据，拥有高质量的数据能够使模型更加精准。</p><p>&nbsp;</p><p>腾讯在云上打造了云原生数据湖仓、向量数据库。它们就像“过滤器”，能够对大量的原始数据进行清洗、分类。</p><p>&nbsp;</p><p>云原生数据湖仓，目前能支持每秒百万级数据更新入湖、TB级海量吞吐能力。配合刚刚发布的腾讯云向量数据库，能够实现10亿级向量检索规模、100亿级的离线数据清洗规模，并将延迟控制在毫秒级。</p><p>&nbsp;</p><p>实测效果显示，对比传统方式，腾讯云的高性能数据处理引擎，让原始数据清洗性能提升了40%+，企业运行综合成本降低50% 。</p><p>&nbsp;</p><p>通过对各类数据的收集、分类、去重、清洗、管理，能够给大模型提供纯度极高的数据“燃料”，大模型也能基于这些高质量数据，全面提升训练和推理效率。</p><p>&nbsp;</p><p></p><h3>TI平台提供精调部署能力</h3><p></p><p></p><blockquote>不是所有的企业都需要从头开始训练大模型</blockquote><p></p><p>&nbsp;</p><p>很多企业需要在通用大模型的基础上，灌入自己的数据做训练或精调，从而打造出适合自身业务的行业大模型。腾讯云也结合开发者和企业的使用需求，进一步完善了训练推理框架和TI平台工具链。</p><p>&nbsp;</p><p>在训练推理层面，腾讯借助自研的机器学习框架Angel，完成了混元的训练。Angel训练速度相比业界主流框架提升1 倍，推理速度比业界主流框架提升1.3倍。目前，企业和开发者可以通过TI平台直接使用该框架。</p><p>&nbsp;</p><p>TI平台提供的工具覆盖环境准备、代码调试、性能评估和部署全链路，具备高性能的大模型精调与部署能力，覆盖大模型开发、应用全生命周期，让客户只需加入自己独有的场景数据，即可在短时间内精调出专属的模型方案。</p><p>&nbsp;</p><p></p><h2>写在最后</h2><p></p><p>&nbsp;</p><p>大模型已经成为当前云服务提供商不可或缺的机遇。</p><p>&nbsp;</p><p>事实上，云厂商之间正在围绕大模型展开竞争，重新争夺公有云市场，因为AI并不只是带来算力投入的增长，而是会带来整个业务的增长——当大模型的能力不断进化，企业在云上使用模型服务的需求也会增加；而被大模型改造的应用和产品，也带来了新的市场空间。基础设施、模型和应用，三者相辅相成，已经成为云厂商必争的空间。</p><p>&nbsp;</p><p>面对全新的发展机遇，腾讯集团高级执行副总裁、云与智慧产业事业群CEO汤道生表示：“以大模型生成技术为核心，人工智能正在成为下一轮数字化发展的关键动力，也为解决产业痛点，带来了全新的思路。腾讯产业互联网将通过智能增强、数据增强、连接增强，持续助力产业增强。”</p><p>&nbsp;</p><p>截至目前，国内主要大厂的大模型均已上线，而真正的竞争才刚刚开始，能否落地，能否真正产生价值，才是制胜关键。</p><p>&nbsp;</p><p></p><h2>&nbsp;</h2><p></p>",
    "publish_time": "2023-09-10 15:00:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "小米一开源项目被批“三无”，项目导师回应；Ruby on Rails之父将TypeScript从Turbo框架中移除，引起社区不满 | Q资讯",
    "url": "https://www.infoq.cn/article/guCLBuhyqTlq1NPppRSo",
    "summary": "<p></p><blockquote>苹果每天花费数百万美元开发对话式 AI；百川大模型开源之后在社区总下载量已突破 500 万；OpenAI官宣将举行首届开发者大会，将公布最新成果；谷歌成立25周年，CEO谈发展：搜索仍是核心，AI 是最大变革；小米一开源项目被批“三无”，项目导师回应；Ruby on Rails之父将TypeScript从Turbo框架中移除，引起社区不满......</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>苹果每天花费数百万美元开发对话式 AI</h4><p></p><p>&nbsp;</p><p>据 The Information 报道，苹果负责对话式人工智能的\"基础模型\"团队只有 16 人，但苹果每天都要花费数百万美元来训练语言模型。训练大型语言模型需要大量硬件，OpenAI 山姆-阿尔特曼（Sam Altman）举例说，公司为 GPT-4 花了 1 亿多美元。</p><p>&nbsp;</p><p>据知情人士透露，苹果的目标之一是开发一些功能，如用户仅需使用简单的语音命令，就可让 iPhone 自动执行涉及多个步骤的任务。例如，你可以告诉 Siri，“使用最近拍摄的 5 张照片创建一个 GIF，并将其发送给朋友”。</p><p>&nbsp;</p><p></p><h4>王小川：百川大模型开源之后在社区总下载量已突破 500 万</h4><p></p><p>&nbsp;</p><p>据巴比特资讯，9 月 6 日，王小川创立的大模型公司百川智能召开发布会，介绍百川大模型在开源方面的最新进展。王小川表示，Baichuan-7B、Baichuan-13B 两款开源大模型在多个权威评测榜单均名列前茅，目前下载量超过 500 万次。</p><p>&nbsp;</p><p>他认为，百川智能作为有过搜索经验的创业团队，知道如何基于万亿互联网数据精选、筛选优质垂直行业数据，进行去重、反垃圾训练，也因此，百川在训练语料上面的能力，帮助 Baichuan-7B 开源后在中英文主流任务上的表现全面超越了 LlaMA2-13B。</p><p>&nbsp;</p><p></p><h4>OpenAI官宣将举行首届开发者大会，将公布最新成果</h4><p></p><p>&nbsp;</p><p>美国人工智能公司OpenAI在官网宣布，其将于2023年11月6日在旧金山举行公司的首届开发者大会“OpenAI DevDay”。OpenAI在官网称，为期一天的活动将邀请来自世界各地的数百名开发者与OpenAI团队一起预览新工具并交流想法。现场与会者还可以参加由OpenAI技术人员领导的分组会议。</p><p>&nbsp;</p><p>OpenAI称，自2020年推出API以来，他们就一直在不断更新，以包括他们最先进的模型，使开发人员比以往任何时候都更容易通过简单的API调用将尖端人工智能进行集成。如今，超过200万开发人员正在使用GPT-4、GPT-3.5、DALL·E和Whisper，从将智能助理集成到现有应用程序，到构建以前不可能的全新应用程序和服务。OpenAI首席执行官Sam Altman表示：“我们期待展示我们的最新成果，帮助开发人员构建新事物。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>谷歌成立25周年，CEO谈发展：搜索仍是核心，AI 是最大变革</h4><p></p><p>&nbsp;</p><p>谷歌现任 CEO 桑达尔·皮查伊在公司官方博客撰文，庆祝谷歌 25 岁生日。同时，皮查伊在文中指出，搜索依然是谷歌的核心“登月计划”，但是 AI，则是帮助谷歌在未来十年实现其使命的关键。</p><p>&nbsp;</p><p>皮查伊认为人工智能将成为我们见到的最大的技术变革，比互联网本身还要重要。皮查伊展望了未来，希望人工智能可以帮助改善人们的生活，并强调负责任地部署和探索人工智能的重要性。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>金山办公WPS AI正式面向社会开放</h4><p></p><p>&nbsp;</p><p>9月5日，金山办公官方宣布，WPS AI正式面向社会开放，AI能力率先应用在WPS智能文档，更多WPS AI能力将在其他组件开放。用户可在最新版WPS客户端/APP、金山文档小程序/官网体验WPS智能文档。基于WPS AI的WPS智能文档，是金山办公旗下新一代在线内容协作编辑产品，支持内容生成、表达优化、文档理解及处理等功能。</p><p>&nbsp;</p><p>金山办公CEO章庆元表示，“在文心一言等大模型的加持下，办公软件的颠覆性变革已经开始。未来，我们只需要告诉WPS想要做什么，功能就直接实现了。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>ASML CEO警告：出口管制不是可行做法，不要“逼迫中国大陆创新”</h4><p></p><p>&nbsp;</p><p>据电子时报消息，荷兰半导体设备公司ASML正看到美国对华遏制政策的负面影响。阿斯麦（ASML）CEO彼得·温宁克在一档电视节目中分享了他对中国大陆问题以及该公司面临的出口管制和保护主义的看法。彼得曾在多个场合表达了他对出口管制以及中荷经济关系的担忧。</p><p>&nbsp;</p><p>彼得在接受采访时强调，通过出口管制完全孤立中国大陆并不是一个可行的做法。这间接表明，华为Mate 60 Pro中的芯片实现了突破，因为这些限制实际上正在推动中国大陆加倍努力创新。他表示，如果欧洲和美国不愿意分享技术，中国大陆就会自己来研究。他们正在考虑西方企业尚未考虑过的解决方案。西方的限制性政策迫使中国大陆变得高度创新。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>Meta 公司因争抢计算资源发生内斗</h4><p></p><p>&nbsp;</p><p>Meta Platforms 的大模型 Llama 和 Llama 2 作为 OpenAI 和 Anthropic 私有模型的开源替代而备受好评，但对大部分参与该项目的科学家和工程师来说，称赞来得太迟了。知情人士透露，参与 Llama 项目的研究人员很多已经离职了，原因是与公司另一个研究团队在计算资源上的内斗。另一个相竞争的模型已被 Meta 放弃。</p><p>&nbsp;</p><p>生成式 AI 需要大量的计算资源，需要专用芯片，而芯片是不会免费提高的。科技巨头通常比其它公司拥有更多的计算资源。但即便如此，它们的计算资源也是有限制的。人才外流凸显了大型科技公司在留住 AI 研究人员上面临的挑战。知情人士称，2 月发表的 Llama 论文的 14 名作者中，一半以上已经离开公司，其中几位加入了 AI 创业公司或其它大公司。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT业界</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>小米一开源项目被批“三无”，项目导师回应</h4><p></p><p>&nbsp;</p><p>9月7日，Apache 软件基金会 (ASF) 原董事吴晟在X（原名Twitter）上发表了一则内容，称小米一开源项目存在一些问题，提及该项目全篇只有一篇中文文档，属于“无用户、无文档、无开发者”的三无初始状态，并直言该项目捐赠ASF是一场“闹剧”。吴晟 (@wu-sheng)是Apache SkyWalking 作者，也是首位进入 ASF 董事会的中国人。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43807d917ae113de7c4a61c284b1d457.jpeg\" /></p><p></p><p>&nbsp;</p><p>据悉，该项目名为OzHera，是一个应用程序性能监控平台，属于小米研发效能平台的一部分。其实在ASF官方页面上相关争议和讨论自9月5日就已经开始。项目负责人Zhiyong Zhang当天曾发表陈述，表示整个项目https://github.com/XiaoMi/mone已经通过了小米内部审核，OzHera是其中一部分。他们也正在为该开源项目准备英文文档，且正在逐步招募小米以外的合作伙伴，希望OzHera能够在云原生可观测性方面发挥其价值。</p><p>&nbsp;</p><p>9月8日，吴晟再次发布了一条推文，表示该项目的Proposal被Mentor暂时撤销。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2e16b4fb1ce3080c30169931b3386a7d.jpeg\" /></p><p></p><p>&nbsp;</p><p>张铎作为该开源项目的Mentor回应称，他曾于2016年到2021年间任职于小米，所以他了解该项目的开发者们，“他们有为开源做出贡献的热情，尽管他们对开源不太熟悉”。并表示“OzHera项目还处于非常早期的开源阶段，所以我们一致认为，在进入孵化器之前最好先打磨一段时间。”</p><p>&nbsp;</p><p>后面他们将拆分mono repo，给OzHera一个单独的repo，明确捐赠范围，同时也确保OzHera可以单独构建和运行。最后他感谢了所有在这个帖子中指出问题的人。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fe/fe001bec61761fe8c4091e29a95b5999.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Twitter链接：<a href=\"https://twitter.com/wusheng1108/status/1699593021535445427\">https://twitter.com/wusheng1108/status/1699593021535445427</a>\"</p><p>Apache页面：<a href=\"https://lists.apache.org/thread/zpc40l2n2qyfd3639hdj1mjf0vwcl63o\">https://lists.apache.org/thread/zpc40l2n2qyfd3639hdj1mjf0vwcl63o</a>\"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>微软承诺为 Copilot AI 用户的侵权诉讼承担法律责任</h4><p></p><p>&nbsp;</p><p>微软首席法务官布拉德·史密斯在一篇名为《Microsoft announces new Copilot Copyright Commitment for customers》倡议的博文中表示，微软宣布推出 Copilot 版权承诺，解决 AI 知识产权问题。微软将为商业客户使用 Copilots 及其生成内容的版权侵权问题承担法律责任。</p><p>&nbsp;</p><p>这一承诺是微软对客户的支持，同时也表明他们重视保护作者权益和推动 AI 技术发展。微软已引入多项防护措施以减少侵权可能性。该承诺适用于 Microsoft 365 Copilot 和 GitHub Copilot 等商业服务，有望增强客户信心，促进知识产权与创新的和谐共存。</p><p>&nbsp;</p><p></p><h4>Ruby on Rails之父将TypeScript从Turbo框架中移除，引起社区不满</h4><p></p><p>&nbsp;</p><p>Ruby on Rails 的创建者 David Heinemeier Hansson（DHH） 从即将发布的 Turbo 框架第 8 版中删除了 TypeScript，并声称从未是它的粉丝。许多 Turbo 用户抗议说决定太仓促，不欢迎这种变化。</p><p>&nbsp;</p><p>在移除 TypeScript 的 GitHub <a href=\"https://github.com/hotwired/turbo/pull/971\">pull request</a>\" 上有一条评论认为，这个举措“对于库的用户和贡献者都是一种倒退”。截止目前，这条评论已经有357个赞，仅8个踩，显示了广泛的支持。</p><p>&nbsp;</p><p><a href=\"https://turbo.hotwired.dev/\">Turbo</a>\" 是一个用于传递 HTML 页面的框架，旨在“显著减少自定义 JavaScript 的数量”，并由 Hannson 的公司 37signals 赞助，其产品包括 Basecamp 项目管理平台和 Hey 消息系统。Turbo 是 Hotwire 的引擎，Hotwire 是“HTML over the wire”的缩写，因为它更喜欢发送 HTML 本身而不是 JSON 数据和 JavaScript 代码。</p><p>&nbsp;</p><p>尽管 Turbo 并不属于那批最受欢迎的框架，但 Ruby on Rails 很有名，像 GitHub 和 Shopify 这样的主要网站都在使用它。</p><p>&nbsp;</p><p>Hansson <a href=\"https://world.hey.com/dhh/turbo-8-is-dropping-typescript-70165c01\">发文</a>\"称 TypeScript “通过添加微不足道的类型技巧，让我的开发体验变得更加糟糕，而且频繁引发很多困扰。本应简单的事情反而变得很困难。”</p><p>围绕着 Turbo 开源项目的社区大多感到困惑和失望，不仅是因为变更本身，还因为变更的方式。</p><p>&nbsp;</p><p>\"回到 JS 意味着许多 Hotwire 生态系统的包都会受到影响。当前的所有开放 PR 都已完全过时。从我的角度来看，其中一些是非常好的候选项。IDE 不再提供与以前一样的自动补全功能，\" <a href=\"https://github.com/hotwired/turbo/pull/971#issuecomment-1708302536\">一位用户表示</a>\"。</p><p>&nbsp;</p><p>另一位用户<a href=\"https://github.com/hotwired/turbo/pull/971#pullrequestreview-1613489798\">抱怨</a>\"说：“匆忙进行这个重要的更改，忽视了所有（我是说所有）的 PR 评论...这会开一个坏头。Ruby on Rails 也会像这样来开发吗？取决于一个人的心血来潮？”</p><p>&nbsp;</p><p>Hansson <a href=\"https://github.com/hotwired/turbo/pull/971#issuecomment-1708430006\">回应</a>\"道：“非常感谢那些更喜欢 TypeScript 的贡献者。这只是争论之一，其中的论点不太可能改变任何人的根本立场，所以我不会尝试这样做。”</p><p>&nbsp;</p><p>他补充说：“现在，我们在 37signals 写的所有客户端代码都是纯 JavaScript，内部库也是如此。这次变更意味着保持一致。”</p><p>&nbsp;</p><p>微软的 Anders Hejlsberg 出于他的信念发明了 TypeScript，即如果使用强类型语言编写复杂应用程序，它们将更加健壮且易于维护。TypeScript 在编程社区的普及，表明了许多人持相同观点，而且一些来自 TypeScript 的概念，包括类型注解，也正在逐渐融入 ECMAScript，即 JavaScript 官方标准。无论开发者的选择如何，TypeScript 都会编译成 JavaScript，最终在浏览器或 Node.js 等环境中执行。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-09-10 15:04:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]