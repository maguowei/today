[
  {
    "title": "微软正式发布Stream Analytics无代码编辑器",
    "url": "https://www.infoq.cn/article/gmX1XbFdgdNR1x2S8ABx",
    "summary": "<p>在<a href=\"https://ignite.microsoft.com/en-US/home\">Ignite大会</a>\"上，微软发布了Azure Stream Analytics<a href=\"https://learn.microsoft.com/en-us/azure/stream-analytics/no-code-stream-processing\">无代码编辑器</a>\"，这是一个支持拖放的画布，可用于开发流处理场景下的作业，如流处理ETL、数据摄入、物化数据并公开发布到Azure Cosmos DB。该无代码编辑器托管在微软的大数据流平台和事件摄入服务<a href=\"https://azure.microsoft.com/en-us/products/event-hubs/\">Azure Event Hubs</a>\"中。</p><p>&nbsp;</p><p><a href=\"https://azure.microsoft.com/en-us/products/stream-analytics/#overview\">Azure Stream Analytics</a>\"是一个托管的实时分析服务。它提供的无代码编辑器让用户可以开发Stream Analytics作业而不用编写一行代码。今年早些时候，该公司<a href=\"https://azure.microsoft.com/en-us/updates/asanocodeeditor/\">发布</a>\"了一个公开预览版，现在发布的正式版包含了几项新功能，比如：</p><p></p><p>支持两个新的输出：Event Hubs和<a href=\"https://azure.microsoft.com/en-us/products/data-explorer/\">Azure Data Explorer</a>\"。在“管理字段”中支持三种数据操作的内置函数：日期时间函数、字符串函数和数学函数。在“Event Hubs – Process Data”下新增三个场景模板。</p><p>&nbsp;</p><p>Stream Analytics作业由三个主要组件组成：流输入、转换和输出。根据用户需要，作业可以包含任意数量的组件，包括多个输入、具有各种转换的并行分支和多个输出。要创建作业，用户可以打开Event Hubs实例、选择Process Data并选择任何可用的模板。</p><p></p><p><img src=\"https://uploader.shimo.im/f/8OKCzyOtWZDSXVkT.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc1MzU4MDYsImZpbGVHVUlEIjoib3ZDRW9oQkNYN3dCS0tkbSIsImlhdCI6MTY2NzUzNTUwNiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.DpGxA4jbhmeytY_YyPKaB1wL5_gpBQPMa5ozcRN43H0\" /></p><p></p><p>用户可以选择一个事件中心（模板中的第一步）作为作业的输入，并配置一个到事件中心实例的连接。接下来，用户还必须完成其他步骤，如分组、管理字段和输出（如Cosmos DB、Event Hub、Synapse和Azure Data Explorer）。</p><p></p><p><img src=\"https://uploader.shimo.im/f/85XemgAZRqKYvlow.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc1MzU4MDYsImZpbGVHVUlEIjoib3ZDRW9oQkNYN3dCS0tkbSIsImlhdCI6MTY2NzUzNTUwNiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.DpGxA4jbhmeytY_YyPKaB1wL5_gpBQPMa5ozcRN43H0\" /></p><p></p><p>在A Cloud Guru的一篇<a href=\"https://acloudguru.com/blog/engineering/azure-stream-analytics-no-code-editor\">博文</a>\"中，技术编辑团队说明了无代码编辑器的好处：</p><p></p><p></p><blockquote>本质上，这个新服务为你提供了一个画布，让你可以查看所有传入的数据流，然后根据需要对它们进行转换，并写入你选择的目标——所有这些都是以无代码的方式进行的。你可以利用Azure数据专家多年来积累的深厚知识，把时间用在思考数据整形（shape your data）的最佳方法上，而不是陷入设计数据查询和转换操作的语法中。</blockquote><p></p><p>&nbsp;</p><p>此外，微软Messaging and Eventing首席架构师<a href=\"https://mobile.twitter.com/clemensv\">Clemens Vasters</a>\"在推特上<a href=\"https://mobile.twitter.com/clemensv/status/1580825326778974208\">写道</a>\"：</p><p></p><p></p><blockquote>它不仅为构建分析作业提供了一种超级灵活的方式，而且还可以将事件数据发送到各种数据库存储和数据湖中。内置于Event Hubs门户体验中。</blockquote><p></p><p>&nbsp;</p><p>通过无代码编辑器，微软为其客户带来了类似于<a href=\"https://www.infoq.com/news/2022/10/confluent-stream-designer/\">Stream Designer</a>\"（由Confluent最近发布）的解决方案，那是一个可以简化数据流管道的点选式可视化构建器。</p><p>&nbsp;</p><p>要了解关于Stream Analytics的更多信息，请查阅<a href=\"https://learn.microsoft.com/en-us/azure/stream-analytics/\">官方文档</a>\"，其中包含无代码编辑器的使用教程。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/10/azure-stream-analytics-nocode/\">https://www.infoq.com/news/2022/10/azure-stream-analytics-nocode/</a>\"</p>",
    "publish_time": "2022-11-05 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里巴巴开源下一代云原生网关Higress：基于Envoy，支持Nginx Ingress零成本快速迁移",
    "url": "https://www.infoq.cn/article/eDP9ttYCRgbWETL6dT75",
    "summary": "<p></p><p></p><blockquote>近日，阿里巴巴正式<a href=\"https://github.com/alibaba/higress\">开源云原生网关Higress</a>\"。Higress是基于阿里内部两年多的Envoy Gateway实践沉淀、以开源Istio + Envoy为核心构建的下一代云原生网关，在标准上全面支持Ingress与Gateway API，积极拥抱云原生下的标准API规范。此外，Higress Controller支持Nginx Ingress平滑迁移，用户可以几乎零成本快速迁移到Higress。&nbsp;</blockquote><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb5a3e5a38e3f58fd3f57b9b06817b86.png\" /></p><p></p><p>&nbsp;</p><p>在容器化的云原生大背景下，Kubernetes已经成为了基础设施与上层应用的标准接口，而Kubernetes Ingress又标准化了入口网关，同时社区也在积极推进Gateway API的标准定义来解决目前Ingress标准存在的一些不足，如路由能力偏弱、配置不够灵活等问题。</p><p>&nbsp;</p><p>大家想到网关可能第一直觉会联想到API网关，这里有一个挺有意思的问题：在网关或者Web流量场景下什么是API呢？</p><p>&nbsp;</p><p>回归到原始概念下，它代指一组具备某些特性的流量，用于描述该特征流量的规则称为路由规则，在API管理领域称为API，在路由规则或者API之上可以附加很多策略，如熔断、限流、降级、认证与鉴权等等。在Kubernetes生态，Ingress与Gateway API是用于定义路由规则的，因此Ingress与Gateway API事实上已经成为云原生下的标准API规范。</p><p>&nbsp;</p><p>阿里巴巴从2017-2018年开始探索Service Mesh，基于这些探索沉淀了大量Istio和Envoy的实践经验，不过早期这些实践并不太涉及网关场景。</p><p>&nbsp;</p><p>2020年，在代号为“本地生活战役”的项目背景下，阿里集团和蚂蚁集团两侧有一些业务上互相访问的诉求（比如支付宝首页需要展示跟本地生活相关的推送、商品信息，而这些信息往往存储在阿里侧），出于安全性和业务快速迭代的考虑，双方技术团队希望打造一个新的互通网关，通过走内部RPC调用的方式实现互通，替代掉原来走公网的方式。同时，大家希望借这个机会把两边的RPC协议定义成一个统一规范。以此为契机，阿里开始了基于Envoy打造下一代云原生网关的探索。</p><p>&nbsp;</p><p>本文出自InfoQ特别策划的专题《Envoy当道？云原生时代的下一代网关选型和实践》。我们专访了阿里云研发工程师耿蕾蕾（如葑），深入挖掘阿里巴巴针对云原生网关的技术选型思考和实践路径，希望能为想要进一步了解 Envoy 和打造下一代网关的技术团队提供参考。</p><p>&nbsp;</p><p></p><h2>为什么基于Envoy？</h2><p></p><p>&nbsp;</p><p>阿里内部一直使用基于Nginx演进而来的Tengine作为统一接入网关。因此最初做新网关技术选型时，其实有两种演进思路，一种是基于 Tengine 优化，一种是基于 Envoy 内核来扩展网关场景。当时，如葑所在团队本身就负责Tengine的维护工作，同时也参与探索阿里内部Service Mesh的落地，因此团队对Tengine和Istio+Envoy这两套技术栈都比较熟悉，技术储备上都不存在障碍。选型的关键在于，哪种技术方案可以更简单地支持前文所述的统一RPC协议（这个由两方团队合作定义出的统一RPC协议，即后来Dubbo 3.0中支持的Triple协议）。</p><p>&nbsp;</p><p>据介绍，Triple协议是一种类gRPC协议，它的大部分能力复用了gRPC，比如序列化和通讯协议等。因此在选型网关时，首先需要新网关对gRPC支持友好，这样后续支持Triple协议的研发工作难度会小很多。恰恰当时Tengine上游对gPRC的支持并不完善，而Envoy诞生在Tengine之后，对gRPC的支持相对完善很多。如果选择Tengine的技术路线，团队前期需要投入很大的人力把Tengine上游支持gRPC的能力补齐，如果选择Envoy则不需要在这方面投入额外人力了。当时基于这一点考虑，团队几乎就要把Tengine这个思路Pass掉了。</p><p>&nbsp;</p><p>此外，Tengine（和Nginx）还存在Reload 访问有损的问题。由于不支持配置热更新机制，Tengine在做配置变更的时候需要Reload，而Reload对于长连接会引起抖动，导致流量短暂中断，但恰恰在RPC请求场景下大部分连接都是长连接。这意味着，如果要让互通网关做到接入服务快速生效，就要频繁做配置变更，Tengine就要频繁地Reload，长连接会频繁抖动，导致流量严重抖动。如果要控制Reload影响则只能减少Reload次数，但这又会导致接入服务生效变慢。相比之下，Envoy原生通过xDS支持配置热更新，就不存在这个问题。这也是团队在做技术选型时重点考量的一点。</p><p>&nbsp;</p><p>当然，也有团队成员对选型Envoy心存疑虑。毕竟Tengine脱胎于Nginx，而Nginx作为老牌网关已经在行业内应用多年，不管是业界口碑还是性能表现都很受推崇，相比之下Envoy只能算是初入江湖的“萌新”选手。被挑战最多的一点是，Envoy的性能是不是能够达到Nginx同等水平？或者说，至少不能出现量级的差异。针对这一点，团队找了一些网关场景，针对gRPC协议专门做了压测。压测结果表明，未作任何优化的情况下，Envoy的性能确实不能完全跟Nginx或Tengine持平，但还在同一个量级上，并没有出现跨量级的性能损失，而这种同一量级的性能差异是可以通过一定的性能优化工作补齐的。</p><p>&nbsp;</p><p>并且，在打造互通网关的项目中，原本就必须做一些整体的性能优化工作。</p><p>&nbsp;</p><p>还有另外一个因素坚定了团队选择Envoy。当时阿里内部已经在重点推进Service Mesh落地，而Service Mesh架构本身就包括东西向使用Sidecar做服务治理，同时还有一个基于Istio+Envoy的Gateway负责做跨集群之间的流量互通。如果选择Envoy作为网关，后续就有可能跟Service Mesh整合成一个大的流量调度方案，在阿里内部实现以一套技术架构同时调度南北向外部流量和东西向内部流量。从长远来看，这是更有利于未来向统一应用架构技术栈演进的选择。</p><p>&nbsp;</p><p></p><h2>阿里巴巴Envoy Gateway演进的三个阶段</h2><p></p><p>&nbsp;</p><p>由于业务体量庞大，阿里对Envoy Gateway的探索主要通过对单点业务场景改造的方式来推进。阿里和蚂蚁的互通网关是第一个落地场景，如葑将其称为<a href=\"https://mp.weixin.qq.com/s/ReaKB65cAw-QznUYfnSc2g\">Envoy Gateway 1.0</a>\"阶段。</p><p>&nbsp;</p><p>这一阶段Envoy Gateway要应用于东西向流量的 RPC 互通，其架构部署如下图：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9dea2c8648fa3e3a9e3ca65c8c7677fa.png\" /></p><p></p><p>&nbsp;</p><p>上图主要展示的是集团侧的架构，最终采用了Istio+Envoy的方案，在部署的时候又分成了出口集群和入口集群。之所以拆成两个集群，一方面是当时两边互访，蚂蚁调集团的流量要远远大于集团调蚂蚁的流量，上下行特别不均等；另一方面是分开之后两个集群可以各自维护，稳定性会更好。</p><p>&nbsp;</p><p>Envoy Gateway 1.0从开始立项到完成第一期研发，网关改造的核心工作差不多两个人投入了一个半月左右，其中还涉及到大量网络、安全等协调部门的工作。1.0架构并没有完全按照社区方案来设计，社区版本中配置变更和服务发现使用的是K8s，在阿里内部庞大的服务规模及配置量下社区原生方案不管在稳定性及性能上都无法满足要求，因此阿里这套方案重点对服务发现、配置存储组件做了替换，及优化xDS推送性能。</p><p>&nbsp;</p><p>这一阶段，团队基于 Envoy演进了网关的服务管理能力，支撑了当年双十一本地生活战役数十万 TPS 的流量洪峰。基于Envoy Gateway的互通网关给业务带来了实打实的收益。一方面，基于RPC调用方式，互访请求的延时远远好于原来走公网的方式，并且对此专门做过压测，整体延时降低了50%-60%，同时在网关侧的延时消耗几乎可以忽略不计，对用户体验的提升非常明显。另一方面，业务上线迭代速度也加快了，原来蚂蚁和阿里要调用对方一个业务，首先要走完一套安全合规的审批流程，然后配置变更走以前的统一接入网关Tengine，至少以天为单位，改造后走新的互通网关，业务上线时间可以缩短到小时级别，具体耗时主要取决于审批流程的快慢，网关侧支持热更新，一旦发出配置变更几乎是秒级生效。</p><p>&nbsp;</p><p>随着阿里巴巴上云战役的推进，越来越多的场景找到如葑的团队。比如云上云下业务互通，由于 Tengine 服务管理弱导致阿里内部大量二层微服务网关需要收敛，这就需要从业务上做 Tengine+Envoy 两层网关的演进，承担南北向网关流量。在 2020 年 12 月份，团队开始了向Envoy Gateway 2.0 架构的演进，以优酷场景为例的演进过程如下图：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/12a1976fccbd9dc9756de5cc4d38d5fa.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Envoy Gateway 2.0 南北向的架构图如下：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/83c575251bceee2dcff59d738d3a5183.png\" /></p><p></p><p>&nbsp;</p><p>在两层架构中，Envoy 网关更多承担了微服务网关和微服务治理的需求，和 Tengine 流量网关完成了整合。在这个过程里，团队支撑优酷内部多个二层微服务网关统一的工作，大幅提升了性能和运维效率。</p><p>&nbsp;</p><p>在这一阶段，Envoy Gateway 实现了东西向、南北向全域流量的调度分发，东西向上不仅支持跨业务域的蚂蚁 RPC 互通，也扩展到了混合云的云上云下 RPC 互通场景，覆盖钉钉文档、阿里视频云、达摩院的店小蜜、智慧数字人等。2.0 阶段的业务大图如下（云上云下互通场景，以钉钉为例说明）：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6971e8ef04db271f43986e5c775cfcf.png\" /></p><p></p><p>&nbsp;</p><p>随着 Envoy Gateway 覆盖的业务场景越来多，在跟优酷持续合作的过程中，双方团队不约而同提出了一个设想：Tengine Gateway（承担流量网关角色） + Envoy Gateway（承担微服务网关角色）的两层网关是否可以合并为一层 Envoy Gateway？</p><p>&nbsp;</p><p>如葑和团队对这一想法做了调研，答案是肯定的，并且当时大家也合作设计了新的架构方案，如下图：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0dc5ebf27fef0fcec41b4cfa8d8cb206.png\" /></p><p></p><p>&nbsp;</p><p>虽然由于各种各样的原因，这个方案最终没有跟优酷继续往下推进。但这个演进方向让如葑和团队明确了网关新的发展趋势：在以 K8s 主导的容器化背景下，由于K8s 集群内外网络的天然隔离性，用户需要一款兼顾高性能与安全性，以及强大服务治理能力的入口网关。这也为后续团队将技术沉淀变成云产品、推进Higress的诞生打下了基础。</p><p>&nbsp;</p><p>2021 年，阿里巴巴开启了中间件三位一体战役，目标是用云产品支撑集团业务。如葑和团队开始将孵化成熟的技术沉淀为云产品，即目前阿里云上提供的 MSE 云原生网关，一方面面向广大的公有云用户提供托管的网关服务，另一方面也对内服务集团。</p><p>&nbsp;</p><p>目前MSE 云原生网关已经支持通过 Envoy 将流量网关 + 微服务网关合二为一的技术方案。同时，通过硬件加速、内核优化等手段，团队也在持续不断地优化网关性能和网关资源部署成本。在功能扩展性上，团队做了高可用流量防护组件Sentinel商业化版本的集成，并支持将K8s的Ingress资源自动转换成Enovy网关能够识别的配置，便于大家从Nginx迁移到Envoy网关。</p><p>&nbsp;</p><p>如今，这套经过内部实践沉淀下来的云原生网关方案Higress正式对外开源，以Kubernetes Ingress网关为契机带来了流量网关与微服务网关融合的可能性，结合阿里内部实践沉淀Higress实现了流量网关 + 微服务网关 + 安全网关三合一的高集成能力，同时深度集成了Dubbo、Nacos、Sentinel等，能够帮助用户极大的降低网关的部署及运维成本，而且能力不打折。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8e6383e7e4e7a404c359f2a5ae6105e2.png\" /></p><p></p><p>软件的扩展性是个持久的话题，Kubernetes的成功离不开其强大Controller扩展能力的支持。据如葑介绍，Higress提供多种形式的扩展机制，包括Wasm插件、Lua插件、进程外插件，通过丰富的插件扩展机制，用户就可以使用多语言编写扩展插件。这能有效降低插件编写门槛，满足用户自定义的扩展诉求。</p><p>&nbsp;</p><p></p><h2>下一代网关尚未形成事实标准，但Envoy是可能性之一</h2><p></p><p>&nbsp;</p><p>如何定义下一代云原生网关，大家可能看法各异。经过这几年在网关方向上的实践，如葑及团队也有一些思考。</p><p>&nbsp;</p><p>在他看来，下一代云原生网关首先一定要对Kubernetes友好，在云原生的大背景下，必须能原生地支持相应特性，而不是依靠插件化的方式来支持，这是至关重要的一点。</p><p>&nbsp;</p><p>其次，要具备丰富的可观测性，不管是初始化状态还是持续运行状态都要能提供丰富的指标数据，用于观测和后续的自动化运维/AI智能运维。网关作为总的流量入口，对安全性、稳定性和性能要求都很高，目前有一些可观测解决方案通过添加Sidecar或者注入代码来提取可观测数据，这种方法对网关来说并不可行，更多还是要依靠网关自身来提供可观测能力。</p><p>&nbsp;</p><p>再次，过去传统网关常常是流量网关+业务网关（或微服务网关）两层架构，不仅运维成本高，资源部署成本也比较高。在云原生时代，K8s已经成为事实上的运维底座，由于K8s集群网络天然是隔离的，就诞生了Ingress网关，来解决流量入口的问题，而这个Ingress网关完全可以将过去的两层网关架构合二为一，成为一个功能更强大的网关。</p><p>&nbsp;</p><p>生于云原生时代的“后浪”Envoy天然能满足以上三点需求。</p><p>&nbsp;</p><p>如葑表示，虽然国内目前使用Envoy作为网关的开源项目或商业化产品不多，但是国外使用Envoy作为网关的产品或案例并不少，比如<a href=\"https://www.tetrate.io/tetrate-service-bridge/?lang=zh-hans\">Tetrate</a>\"、<a href=\"https://www.solo.io/products/gloo-edge/\">Gloo Edge</a>\"、<a href=\"https://github.com/emissary-ingress/emissary\">Ambassador</a>\"（网关改名为Emissary-ingress）等，虽然有的使用Istio作为控制面，有的选择自建控制面板，但大家都不约而同地把Envoy作为未来演进路线的一种选择。知名企业如Twitter、Lyft等也都在使用Envoy作为网关或者把网关往Envoy上面迁移。</p><p>&nbsp;</p><p>这首先得益于Envoy原生支持配置热更新，在如葑看来，这是一个非常重要的特性。现在已经有很多主流应用选择采用长连接，现在的HTTP 1.1一般默认会使用Keep-Alive去保持长连接，后续HTTP 2以及HTTP 3也是如此，随着网络协议的发展，未来使用长连接会变得更加普遍。而配置热更新天然对长连接非常友好。</p><p>&nbsp;</p><p>其次也得益于Envoy的xDS能力，xDS作为一个通用的配置请求协议规范，基于它可以做灵活地扩展，对数据面+控制面的架构天然友好。</p><p>&nbsp;</p><p>反观Nginx，它更多推崇的是使用本地的指令配置来做配置变更，虽然作为代理程序来说很便捷高效，但用作网关则不尽然。另外，Nginx原生不支持配置热更新，常见的做法是使用Lua脚本来支持，但这会带来非常大的性能衰减，如果配置热更新、可观测性等全部使用Lua实现，整体性能损失相比原生Nginx最高可能达到70-80%，社区中针对这一问题还有一个专门的Issue：<a href=\"https://github.com/kubernetes/ingress-nginx/issues/5658\">P</a>\"<a href=\"https://github.com/kubernetes/ingress-nginx/issues/5658\">oor p</a>\"<a href=\"https://github.com/kubernetes/ingress-nginx/issues/5658\">er</a>\"<a href=\"https://github.com/kubernetes/ingress-nginx/issues/5658\">formance in benchmark</a>\"。</p><p>&nbsp;</p><p>当然，下一代云原生网关这个方向目前还没有形成一个事实上的标准，除了Envoy，业内还有很多企业在探索这个方向，大家其实都想要抢占先机。</p><p>&nbsp;</p><p>国外如<a href=\"https://github.com/traefik/traefik\">traefik</a>\"、<a href=\"https://github.com/Kong/kong\">Kong</a>\"，甚至包括Nginx背后的厂商F5，都在做相关尝试。如葑认为，每种产品可能会有不同的用户群体。</p><p>&nbsp;</p><p>比如<a href=\"https://github.com/traefik/traefik\">traefik</a>\"，因为它原生是用 Golang 开发的，包括扩展机制也支持用Golang编写，所以对于使用 Golang 的群体来说可能相对更友好，同理还有国内百度开源的BFE。有些客户确实会因为 Golang 的入门门槛比Nginx的C、或Envoy的C++低很多，而选择尝试基于Golang的网关方案。如葑认为这类方案可能比较适合的场景是，规模不是特别大，且对性能的要求没有那么高，同时又希望有一个能够快速上手的开源网关产品或者商业化产品。但一些对性能或延时要求比较高的场景，比如电商或游戏场景，可能还是选择基于C或者C++的网关产品更好。因为基于Golang的网关方案，包括现在很流行的Java体系的Spring Cloud Gateway，都避免不了GC抖动的问题。</p><p>&nbsp;</p><p>国内如APISIX也正在云原生网关方向做一些探索。APISIX基于OpenResty开发，主要采用的还是Nginx+Lua的方案，但对这套方案做了很多改造，使其能够更好地贴合目前云原生化的需求。如葑认为APISIX选择的切入点也很好，并且目前在国内确实算是做得比较成功的一个开源项目。</p><p>&nbsp;</p><p>如葑表示，大家其实选择了不同的路线，现在很难明确地说哪一条路线一定会胜出。当然他自己还是更看好Envoy，除了前文所述技术选型的考虑，和团队过去几年维护Tengine和Envoy的经验总结，还有一部分是出于对技术生态和技术影响力的考量。在他看来，目前Nginx生态已经成熟到有点固化，不管是社区活跃度还是增长其实都到了一定的瓶颈，这时候投入很多人力优化和扩展现有项目，对团队来说并不是一个特别好的选择，不如选择一个新方向。</p><p>&nbsp;</p><p>另一方面团队更加看重的是Envoy Gateway与Service Mesh统一技术架构带来的长远价值。原来对中间件团队来说，有一个很大的痛点是，中间件架构演进无法独立于应用演进，而一定要依赖业务帮忙做升级。Service Mesh架构提供了一个新的可能，可以把中间件所有的通用能力下沉到Sidecar，更方便地为业务提供增量特性，缩短新业务上线的时间。这对团队来说有更大的意义。</p><p></p><h2>Envoy Gateway：现有格局的冲击者？</h2><p></p><p>&nbsp;</p><p>今年5月份，<a href=\"https://github.com/envoyproxy/gateway\">新项目Envoy Gateway</a>\"正式开源，在业内引发了一波讨论。</p><p>&nbsp;</p><p>在如葑看来，Envoy Gateway的开源，相当于从社区官方角度明确认可了使用Envoy作为网关这件事。以前大家可能更熟悉Envoy用作Sidecar的场景，一提到Envoy，第一个想到的就是Service Mesh里的Sidecar。Envoy Gateway这个项目发起之后，大家可能慢慢会改变认知并逐步接受，Envoy也可以用作网关并且是网关场景一个比较好的选择。</p><p>&nbsp;</p><p>如葑补充道，新开源的Envoy Gateway也可能对现有的格局造成一些冲击。他认为，其实在Istio+Envoy这个组合里，话语权更大的是Envoy。现在国内外使用Envoy作为网关主要有两个分支，一个是使用Istio+Envoy这套基础架构，另一个是使用自建的控制面+Envoy组成一套架构。刚开源的Envoy Gateway背后两家创始企业都选择的是自建控制面+Envoy这种方式，并没有使用Istio。目前社区围绕新建一个能够完全贴合网关的控制面这个话题也在做一些讨论。随之而来还有更多问题引发社区讨论，比如：为什么Envoy Gateway不首选目前已经成型的Istio，而是要再新建一个控制面？新的控制面以后跟Istio之间的关系会是什么样的？后续是否会支持Istio一键迁移？接下来Envoy Gateway社区走向值得我们长期关注。</p><p>&nbsp;</p><p>回到Envoy本身，也有一些需要改进的地方。众所周知，Envoy并不是一个容易上手使用的软件，它的复杂性劝退了很多开发者。虽然如葑所在团队因为有了前期的技术沉淀，复杂性并没有给他们造成明显困扰，但他也坦言，对于从未接触过Envoy或者对C/C++不熟悉的开发者来说，Envoy确实上手门槛比较高。</p><p>&nbsp;</p><p>对于想要尝试Envoy的技术团队，团队中最好能有一定的Envoy技术储备，如果团队中完全没有熟悉Envoy或C++的人，想要很简单地把Envoy用起来，或者平稳地把Envoy放到生产环境上使用，会面临比较大的挑战。在易用性和上手门槛方面，Envoy确实做得没有Nginx好。</p><p>&nbsp;</p><p>好的一面是，Envoy的技术架构，包括多线程模型等，都设计得比较优雅和规范。如葑建议，开发者在上手之前可以先到Envoy官方博客上看一看它的技术架构和设计思想，了解完这些之后再去读代码，可能会觉得容易一些。另外，得益于Envoy的 Filter扩展机制，开发者其实不需要对Envoy里面所有的细节都了解得非常清楚，如果想快速上手，重点了解一下它的Filter扩展机制大概是什么样子，然后参照社区里的Example，就可以快速写出一个Demo的Filter并运行起来，这跟基于Tengine开发一个module的难易度是差不多的。</p><p>&nbsp;</p><p>当然，如葑还是希望Envoy社区能够做一些尝试，适当地降低入门门槛，从而更好地把Envoy推广出去，让更多人能够把Envoy应用到业务中。此外，随着Envoy社区活跃度越来越高，参与的人越来越多，如葑觉得Envoy整体架构的复杂度其实是在不断膨胀的，后续社区也需要考虑如何在控制架构复杂度和新增功能两个方向上取得一定平衡。</p><p>&nbsp;</p><p>采访嘉宾介绍：</p><p>&nbsp;</p><p>耿蕾蕾（如葑），阿里云研发工程师，从 2020 年 5 月负责 Envoy Gateway 的构建到推出 3.0，作为技术负责人主导了整个演进过程，在云原生网关领域有着丰富的实践。</p>",
    "publish_time": "2022-11-05 12:41:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "开源大数据热力报告2022",
    "url": "https://www.infoq.cn/article/bKbCdRfqi0X9AQkQBPGl",
    "summary": "<p>11月5日，由开放原子开源基金会、X-lab开放实验室和阿里巴巴开源委员会联合出品、InfoQ战略支持的《2022年开源大数据热力报告》重磅发布。报告基于公开数据研究最活跃的102个开源大数据项目，探寻出开源大数据技术发展背后的“摩尔定律”：每隔40个月，开源项目热力值就会翻一倍，技术完成一轮更新迭代。在过去8年里，发生了5次较大规模的技术热力跃迁，多元化、一体化、云原生成为当前开源大数据发展趋势的最显著特征。</p>",
    "publish_time": "2022-11-05 15:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“后Hadoop时代”技术热力跃迁：《2022开源大数据热力报告》重磅发布",
    "url": "https://www.infoq.cn/article/89vXVzIjD2DJLEMQNN83",
    "summary": "<p>11月5日，在2022云栖大会一体化大数据智能峰会上，由开放原子开源基金会、X-lab开放实验室和阿里巴巴开源委员会联合出品的<a href=\"https://www.infoq.cn/minibook/bKbCdRfqi0X9AQkQBPGl\">《2022年开源大数据热力报告》</a>\"重磅发布。</p><p>&nbsp;</p><p>开放原子开源基金会副秘书长刘京娟女士对报告进行了深度解读。报告基于公开数据研究最活跃的102个开源大数据项目，探寻出开源大数据技术发展背后的“摩尔定律”：每隔40个月，开源项目热力值就会翻一倍，技术完成一轮更新迭代。在过去8年里，发生了5次较大规模的技术热力跃迁，多元化、一体化、云原生成为当前开源大数据发展趋势的最显著特征。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7e/7efb30124586fcd429c2459287bf9d94.png\" /></p><p></p><p></p><h2>定量分析“后Hadoop时代”开源趋势</h2><p></p><p></p><p>Hadoop&nbsp;作为开源大数据技术的起源，兴起于2006年，至今已有16年历史。我们收集了从Hadoop&nbsp;发展第10年（即2015年）至今的相关公开数据，并进行了关联分析，定义了开源项目热力值研究模型，使用量化指标，来刻画开源项目的开发迭代活跃度和受开发者欢迎程度。</p><p>&nbsp;</p><p>报告所呈现的开源大数据热力图，从技术全景、技术栈分类以及项目维度对入围项目的热力表现进行洞察，将项目进程中的关键事件与热力表现关联分析，并访谈了开源基金会、知名开源项目等领域专家，尝试找到项目健康发展一般规律，并对有效提升项目影响力的方法论进行了归纳总结。</p><p>&nbsp;</p><p></p><h2>开源大数据技术的“摩尔定律”即将打破</h2><p></p><p></p><p>报告发现，每隔40个月，热力值会提升1倍，开源大数据完成一轮技术迭代升级，而且技术周期在加速缩短。在8年时间内，发生了多轮热力变迁，反映出背后技术的更新换代趋势。开发者对「数据查询与分析」保持了长期的开发热情，这一技术栈连续8年位于热力值榜首。2017年，「流处理」热力值超过「批处理」，大数据处理进入实时阶段。随着数据规模越来越大，数据结构更多样化，「数据集成」从2020年开始爆发式增长。</p><p></p><h2>三大热力趋势：多元化、一体化和云原生</h2><p></p><p></p><p>用户需求多样化推动技术多元化。「数据湖」以34%的热力值年均复合增长率高居热力值增速第一位，「交互式分析」、「DataOps」紧随其后，分列第二、三位&nbsp;。而原有Hadoop体系的产品迭代则趋于稳定，热力值年均复合增长率为1%。</p><p></p><p>从2015年开始，计算部分率先进入「一体化」演进历程，其中的典型代表「流批一体」在2019年出现第一个热力峰值。以数据湖存储为代表的存储一体化从2019年起进入了一个新的发展阶段，涌现了Delta&nbsp;Lake、&nbsp;Iceberg和Hudi等热点项目。</p><p></p><p>云原生大规模重构开源技术栈。诞生于云原生时代的开源项目如雨后春笋般破土成长。「数据集成」、「数据存储」、「数据开发与管理」等领域都发生了非常大的项目更迭，新项目热力值占比已经超过了80%。</p><p></p><h2>开源大数据热力榜单TOP30</h2><p></p><p></p><p>本报告从102个入围项目中，评选出了TOP30热力榜单。Kibana以989.40的热力值高居榜首。ClickHouse（数据查询与分析）、Airflow（数据调度与编排）、Flink（流处理）、Airbyte（数据集成）分别摘得各自细分领域的TOP1。Pulsar、Doris、StarRocks、DolphinScheduler、SeaTunnel等一众中国开源项目也表现出高热力趋势。把解决用户痛点作为核心竞争力，是这些优秀开源项目的共同特征，这一特征保证它们与时俱进，成为热力趋势中的“常青树”。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/75/75ab6a833399e28c5b257cfbad87bd85.png\" /></p><p></p><p></p><blockquote>致谢：感谢开源中国、InfoQ和阿里云开发者社区的战略支持，感谢对本报告内容产出做出重要贡献的32位专家和贡献者，感谢合作社区 CSDN、DataFun、Segmentfault思否、开源社等。</blockquote><p></p><p>&nbsp;</p><p>完整报告下载地址：<a href=\"https://www.infoq.cn/minibook/bKbCdRfqi0X9AQkQBPGl\">https://www.infoq.cn/minibook/bKbCdRfqi0X9AQkQBPGl</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2022-11-05 15:05:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]