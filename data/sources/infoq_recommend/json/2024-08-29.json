[
  {
    "title": "所有主流浏览器都支持新的JavaScript集合方法",
    "url": "https://www.infoq.cn/article/BXCwkECbShAAe1hGYwdj",
    "summary": "<p>随着Firefox 127的发布，现在所有主流浏览器引擎都全面支持新的<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set#Set_methods?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjQ3OTEsImZpbGVHVUlEIjoiZ08zb2RNWVlKWHNuT09xRCIsImlhdCI6MTcyNDcyNDQ5MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.b5qKNZ2C82AbROURfTZxso_SxxHGVUN-AWDuyBPm2zk\">JavaScript集合方法</a>\"，包括intersection()、union()、difference()、symmetricDifference()、isSubsetOf()、isSupersetOf()和isDisjointFrom()。这意味着开发者们不再需要依赖polyfill来确保这些方法在不同环境中的兼容性。这些新加入的特性提供了一套便捷的内置工具来操作和比较集合，不仅简化了开发，还提升了程序的性能。</p><p></p><p>JavaScript中的Set与Array类似，但它可以确保集合中的每个元素都是唯一的。这种自动除重的特性使得Set成为创建唯一元素集合的理想选择。下面是一个简单的例子，展示了如何创建一个Set并向其添加元素：</p><p></p><p><code lang=\"javascript\">const users = new Set();\n\nconst alice = { id: 1, name: \"Alice\" };\n\nusers.add(alice);\n\nusers.forEach(user =&gt; { console.log(user) });</code></p><p></p><p>在检查元素是否存在时， Set通常比Array更加高效，这一特性使得它对于性能要求较高的应用程序来说非常有价值。</p><p></p><p>union()方法返回一个新Set，包含原始Set和给定Set中的元素。这个方法在合并集合时极为有用，同时确保结果集中不包含重复项：</p><p></p><p><code lang=\"javascript\">const set1 = new Set([\"Alice\", \"Bob\", \"Charlie\"]);\n\nconst set2 = new Set([\"Bob\", \"Charlie\", \"David\"]);\n\nconst unionSet = set1.union(set2);\n\nunionSet.forEach(name =&gt; {\n\n&nbsp; console.log(name); // 输出: Alice, Bob, Charlie, David\n\n});</code></p><p></p><p>intersection()方法返回一个新Set，只包含两个Set共有的元素。这个方法在识别两个集合共同元素时非常有用：</p><p></p><p><code lang=\"javascript\">const intersectionSet = set1.intersection(set2);\n\nintersectionSet.forEach(name =&gt; {\n\n&nbsp; console.log(name); // 输出: Bob, Charlie\n\n});</code></p><p></p><p>symmetricDifference()方法返回一个新Set，包含只在其中一个Set中出现的元素，不包含两个Set共有的元素。这个方法在识别两个集合各自的不同元素时非常有用：</p><p></p><p><code lang=\"javascript\">const symmetricDifferenceSet = set1.symmetricDifference(set2);\n\nsymmetricDifferenceSet.forEach(name =&gt; {\n\n&nbsp; console.log(name); // 输出: Alice, David\n\n});</code></p><p></p><p>difference()方法返回一个新Set，包含了原始Set中有而给定Set中没有的元素。这在需要从集合中排除某些元素时非常有用：</p><p></p><p><code lang=\"javascript\">const set1Only = set1.difference(set2);\n\nset1Only.forEach(name =&gt; {\n\n&nbsp; console.log(name); // 输出: Alice\n\n});</code></p><p></p><p>isSubsetOf()和isSupersetOf()方法根据两个Set之间的包含关系返回一个布尔值。isSubsetOf()方法检查一个Set的所有元素是否都包含在另一个Set中，而isSupersetOf()方法检查一个Set是否包含了另一个Set的所有元素。</p><p><code lang=\"javascript\">const subset = new Set([\"Alice\", \"Bob\"]);\n\nconst superset = new Set([\"Alice\", \"Bob\", \"Charlie\"]);\n\nif (subset.isSubsetOf(superset)) {\n\n&nbsp; console.log(\"subset is a subset of superset\"); // 这将被打印出来，因为subset的所有元素也都在superset中。\n\n} else {\n\n&nbsp; console.log(\"subset is not a subset of superset\");\n\n}\n\nif (superset.isSupersetOf(subset)) {\n\n&nbsp; console.log(\"superset is a superset of subset\"); // 这将被打印出来，因为subset中的所有元素也都在superset中。\n\n} else {\n\n&nbsp; console.log(\"superset is not a superset of subset\");\n\n}\n</code></p><p></p><p>isDisjointFrom()方法检查两个Set是否有共同元素：</p><p><code lang=\"javascript\">const set3 = new Set([\"Eve\", \"Frank\", \"Gina\"]);\n\nif (set1.isDisjointFrom(set2)) {\n  console.log(\"Set1 and Set2 are disjoint\"); // 这将被打印出来，因为集合set1和集合set2没有共同元素\n} else {\n  console.log(\"Set1 and Set2 are not disjoint\");\n}\n\nif (set1.isDisjointFrom(set3)) {\n  console.log(\"Set1 and Set3 are disjoint\");\n} else {\n  console.log(\"Set1 and Set3 are not disjoint\"); // 这将被打印出来，因为集合set1和集合set3有一个共同的元素“Charlie”</code></p><p>社区对这些新方法反响热烈。在<a href=\"https://www.reddit.com/r/javascript/comments/1dzqmj6/new_javascript_set_methods?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjQ3OTEsImZpbGVHVUlEIjoiZ08zb2RNWVlKWHNuT09xRCIsImlhdCI6MTcyNDcyNDQ5MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.b5qKNZ2C82AbROURfTZxso_SxxHGVUN-AWDuyBPm2zk\">Reddit的一个讨论帖</a>\"中，用户peterlinddk表示：</p><p></p><p></p><blockquote>“太好了，我们终于可以用Set做更多的事情，不仅仅是‘重复项检测器’。我还希望有一种方法，允许对象在不必是完全相同的实例的情况下也能被认为是‘相等’的，有点像Java的.equals和.hashCode方法。”</blockquote><p></p><p></p><p>另一位用户Pelopida92对这些新方法在性能上带来的提升表示赞赏，并表示：</p><p></p><p></p><blockquote>“Set太棒了。我在一些处理大数据量的脚本中广泛使用了这些Set方法，因为它们不仅在性能上优于数组，使用起来也非常简便和直观。”</blockquote><p></p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/07/javascript-set-methods/\">https://www.infoq.com/news/2024/07/javascript-set-methods/</a>\"</p>",
    "publish_time": "2024-08-29 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI技术如何深入各行各业？Intel AI Summit专场全栈落地实践分享丨AICon",
    "url": "https://www.infoq.cn/article/40q0yogaX2i13pOKQQar",
    "summary": "<p>在当今时代，人工智能技术正以前所未有的速度迅猛发展，企业落地人工智能应用已成为不可逆转的趋势。然而，这一过程中也伴随着诸多挑战和问题。如何助力企业加速人工智能的落地进程，如何最大限度地提升 IT 系统的资源利用率，以及如何有效增强计算效能，并便捷、稳定地部署应用 AI，都已成为整个行业关注的焦点。</p><p></p><p>8 月 18 日至 8 月 19 日，在上海举办的 AICon 2024 全球人工智能开发与应用大会上，诸多讨论都聚焦于这些问题。其中的 Intel AI Summit 「AI 全栈解决方案及行业实践」专场，来自英特尔及其合作伙伴的四位行业专家就 AI 应用落地的全栈解决方案，以及医疗领域的应用案例进行了深入探讨。</p><p></p><p></p><h2>激发 AI 潜能：xFT 助力算力解锁，最大化提升计算效能</h2><p></p><p></p><p>尽管“AI 赋能”具有巨大吸引力，但在资源有限的条件下，企业必须确保每一项投入都能获得最大化的效益，这对应用落地的成本和利用率提出了严格的要求。特别是在计算效能提升方面，需要硬件、软件、算法等多个层面的协同优化。高效的计算能力能够加速数据处理、模型训练和推理速度，使企业更迅速地做出明智决策，推出创新产品和服务。</p><p></p><p>为了解决算力对 AI 落地的限制，英特尔一直走在行业前列，力求为企业推出实用可靠的计算资源方案，通过技术优化最大化提升算力，推动大模型应用的落地，充分释放 AI 潜能。在本次会议上，英特尔数据中心和 AI 事业部首席工程师何普江带来了主题为《xFT 解锁至强算力，释放 AI 潜能》的演讲。</p><p></p><p>何普江认为，AI 的未来将由算力的突破来定义，而第五代英特尔®️&nbsp;至强®️&nbsp;可扩展处理器及其内置的英特尔®️&nbsp;AMX 技术正是这一突破的关键。英特尔®️ AMX 通过深度优化矩阵运算，为算力释放提供了坚实的硬件基础。</p><p></p><p>在演讲中，何普江分享了 xFT（xFasterTransformer）技术的设计理念：这是一个专为 AMX 优化的开源项目，不仅支持广泛的 AI 模型和数据类型，更通过软硬件的深度融合，显著加速了 AI 大模型推理。何普江也在分享中提到，与传统方法相比，目前通过 xFT 技术，可以在第五代处理器上面用 48 核跑出高达 1300 的 CRGPU 吞吐量，这一数字远超行业标准，在处理大规模数据集和复杂运算时表现出色。</p><p></p><p>在算法层面，何普江深入分享了 xFT 技术的多项创新，包括对 oneDNN 库的优化使用，以及针对不同 token size 优化的 Slim attention 机制。这些创新不仅提升了 xFT 技术的性能，也为 AI 社区提供了宝贵的实践经验。</p><p></p><p>探讨大模型与小模型的未来发展时，何普江指出，两者各有优势，将共同推动 AI 技术进步。他强调了多模态和 RAG 技术的重要性，并预测开源与闭源模型间差距将缩小。何普江还提到了 KV Cache 的关键作用，以及它对未来 AI 系统设计的影响。</p><p></p><p>他认为，随着硬件和软件的不断进步，大语言模型的成本将大幅降低，推动 AI 技术的更广泛应用和深入发展。开源与闭源模型间的差距正在缩小，未来开源模型将在 AI 领域扮演更加重要的角色。</p><p></p><p></p><h2>GenAI 开放平台 OPEA：一站式助力大模型应用，企业 AI 落地加速器？</h2><p></p><p></p><p>除了底层算力效能提升之外，在目前企业的 AI 应用实践中，还存在着训推优化、基础设施扩展、数据传输安全、应用碎片化等诸多环节。企业需要一个能够全栈助力落地 AI 应用的方案与平台，一站式解决生成式 AI 的落地问题，在性能优化、可扩展性、安全等角度为企业保驾护航。</p><p></p><p>在本次会议上，英特尔 AI 首席工程师吴震华围绕 OPEA 开放平台进行了分享。作为人工智能建模、特征工程、效果分析以及推荐增强等领域的资深专家，他在演讲《基于检索增强的企业 GenAI 开放平台落地实践》中详细梳理了 AI 技术的发展历程，并深入解析了英特尔企业 GenAI 开放平台（OPEA）的架构与底层技术。</p><p></p><p>吴震华认为，尽管基于检索增强的 RAG 技术并非新生事物，但其在企业中的应用潜力正随着大语言模型的能力而日益凸显。在吴震华看来，企业 AI 落地面临的挑战与机遇并存，特别是在生成式 AI 技术，如 ChatGPT 引爆市场之后，行业关注的焦点已从模型预训练的竞争转向了具体的应用落地。</p><p></p><p>OPEA 开放平台是一个由英特尔推动、捐赠给 Linux 基金会的开源项目。OPEA 旨在构建一个开放的生态系统，使企业能够快速利用大语言模型和 AI 技术带来的创新优势。吴震华详细介绍了 OPEA 的全栈架构，从基础设施层到平台集成层，再到面向用户的服务层，展示了一个多层次、模块化的 AI 应用平台。</p><p></p><p>展望未来，吴震华预计到 2028 年，80% 以上的商用 PC 将被新形态的 AIPC 所替代。他将企业 AI 应用的发展分为三个阶段：今天，AI 助手如 CO-Pilot 和 RAG 正在提升数据检索和编程流程的效率；明天，智能体将拥有更大的自主权，利用 AI 的推理能力完成特定任务；未来，AI 将深入企业流程的每个环节，优化每个生产要素。</p><p></p><p>吴震华还提出了企业 AI 应用的四个关键方向：易用性、开放性、安全性、负责任的使用，以及平台的可扩展性和参考实践的提供。他希望通过这些方向的努力，使企业 AI 快速享受到生成式 AI 革命的技术成果。</p><p></p><p>在演讲的最后，吴震华通过一个应用 demo 展示了低代码的基于至强®️&nbsp;微服务实现生成 AI 服务功能，他期待通过不断的迭代和更新，OPEA 能够推动企业 AI 方案的发展，方便快速地帮助企业用户解决实际的问题，让企业真正享受到生成式 AI 技术带来的红利。</p><p></p><p></p><h2>AI+ 医疗：大模型在病历质控中的应用实践</h2><p></p><p></p><p>生成式 AI、大模型技术正在为各行各业带来革命性的变化，医疗领域也不例外。在医院、健康机构等场景下，AI 辅助诊疗、病历质控等应用将成为未来技术趋势。惠每科技致力于通过人工智能解决方案提升医疗质量，守卫患者安全，在智能化诊疗、病历质控等技术领域不断创新大模型技术应用，推动医疗行业的数智化发展。在 Intel AI Summit 专场上，惠每科技算法专家凌鸿顺以《破解病历质控难题：医疗大模型质控优化策略》为主题，分享了惠每科技在病历质控领域的成功实践。</p><p></p><p>病历质控作为医疗质量评估的核心，直接影响医疗服务水平和患者安全。面对病历书写的及时性、规范性和完整性问题，惠每科技采用了大模型技术，利用其强大的文本理解和知识推理能力，有效提升了病历质控的效率和准确性。大模型基于 Transformer 架构，通过持续预训练和任务对齐，以及直接偏好优化，显著提高了对病历中关键信息的提取和分析能力。</p><p></p><p>凌鸿顺还提到，在模型训练优化方面，惠每科技采取了基座模型优化和大模型 prompt 工程优化的策略。通过知识注入、指令跟随和直接偏好学习，模型能够更好地理解和执行医疗领域特定的任务。特别是在处理病历中的矛盾和不规范问题时，大模型展现了其跨字段理解和医疗知识对比的优势。</p><p></p><p>惠每科技还制定了自动化 Few-shot 示例的方案，通过初始化阶段的 badcase 识别和迭代优化，以及相似度计算和多样性 prompt 的加入，进一步提升了模型的预测效果和泛化能力。这一策略不仅减轻了筛选 Few-shot prompt 的工作量，也为不同医院的特殊 case 提供了快速修复的可能。</p><p></p><p>凌鸿顺还提到，在大模型部署推理的实践中，惠每科技与英特尔的合作成果显著。通过xFasterTransformer、BigDL 量化方案和&nbsp;OpenVINO™️&nbsp;非量化方案，实现了医疗模型私有化部署的优化，解决了大模型在硬件资源和计算效率上的挑战。特别是英特尔®️&nbsp;AMX 技术的应用，为大模型的推理性能带来了质的飞跃。</p><p></p><p>展望未来，凌鸿顺对医疗大模型的应用持乐观态度。模型蒸馏技术有望将大模型的效果转移到更小、更易于部署的模型上。自动化 Few-shot 的进一步优化，将实现更高效、更准确的病历质控。同时，惠每科技也将与英特尔展开持续合作，进一步推动医疗 AI 技术的创新和应用，为医疗行业带来更多的价值和可能性。</p><p></p><p></p><h2>医疗 AI 革新：大模型技术深度融合与应用实践</h2><p></p><p></p><p>人工智能技术的发展对医疗行业的信息化升级和数智化变革具有重大意义。以国内医疗场景为例，大量专业化数据和对信息化处理的精准度要求极高，这些都是信息化过程中需要解决的实际问题。垂直领域的大语言模型将成为新一代医疗信息化系统的有力助手，帮助解决医疗系统中的诸多问题。然而，如何让医疗大模型真正可用、易用，仍需解决模型构建、集成、系统结合和应用设计的一系列问题。</p><p></p><p>在 Intel AI Summit 专场上，卫宁健康研发总监刘鸣谦带来了题为《大语言模型在医疗场景的落地实践》的分享，深入探讨了大模型技术如何深刻影响医疗信息化的发展和临床应用。</p><p></p><p>刘鸣谦首先回顾了医疗系统的发展历程，从早期的专家系统、本体推理到现代基于 AI 的图像辅助诊断和自然语言处理。她认为，自 OpenAI GPT3.5 发布以来，基于 Transformer 的大模型已成为医疗领域开发和应用的新范式。大模型的文本生成能力、推理能力和交互能力，为医疗领域带来了前所未有的创新潜力。</p><p></p><p>在数据工程方面，刘鸣谦分享了卫宁健康如何通过高质量的数据集和场景化处理来优化大模型的训练效果。通过上下文学习、RAG（Retrieval-Augmented Generation）和 Agent 方式，进一步提升了模型的效果和适应性。此外，通过直接偏好优化（DPO）和提示工程，模型在医疗场景中的适用性得到了显著提升。</p><p></p><p>卫宁健康的大模型训练采用了多轮迭代，结合了开源数据和自身积累的医疗知识，形成了强大的模型能力。刘鸣谦提到，卫宁健康开源了多款垂直领域大模型，以促进社区的交流和发展，并与英特尔合作，优化了基于英特尔®️&nbsp;AMX 技术的本地化部署方案，有效降低了成本同时保证了高性能。</p><p></p><p>卫宁团队开发的 Copilot，作为信息化系统和 AI 模型之间的桥梁，通过 API 插件等多种形式，实现了不同应用场景下的模型管理和服务。Copilot 的应用，使得医务人员能够无缝地体验到 AI 带来的便利。</p><p></p><p>在医疗应用场景方面，刘鸣谦详细介绍了大模型在医技、临床和管理场景下的实际应用案例。例如，在影像科中，大模型辅助医生快速生成报告，提高了工作效率；在超声科中，实现了实时报告质控，提升了医疗质量；在临床辅助诊断中，通过增强型 CDSS 系统，提供了更加精准的辅助决策支持。</p><p></p><p>此外，大模型还在病历文书助手和智能语言查房助手中发挥了重要作用，通过语音识别和自然语言处理技术，实现了医生口述内容的自动结构化输出，极大地提高了医生的工作效率。</p><p></p><p></p><h2>AI 企业落地，从概念走向现实</h2><p></p><p></p><p>随着人工智能技术的不断成熟和创新，其在企业中的应用已不再是遥远的梦想，而是触手可及的现实。英特尔 AI 全栈解决方案的提出和实践，为企业智能化转型提供了一个清晰的路径：从底层硬件的优化到顶层应用的创新，从单一技术的突破到全栈生态的构建，每一步需要关注企业实实在在的效益，才能让 AI 发挥出真正的价值。</p><p></p><p>未来，随着技术的进一步发展和应用的不断深入，AI 必将成为推动企业创新和增长的关键力量，开启一个全新的智能化时代。让我们拭目以待，共同见证 AI 技术如何助力企业实现跨越式发展，引领行业变革。</p>",
    "publish_time": "2024-08-29 11:16:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "NVIDIA H20与计算领域的革命：深入解析算力评估与应用",
    "url": "https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6",
    "summary": "<p></p><blockquote>在当今快速发展的科技时代，计算能力的重要性毋庸置疑。无论是在人工智能、深度学习还是高性能计算领域，算力的强弱决定了创新的速度与效果。作为NVIDIA最新推出的顶级显卡，H20以其强大的硬件配置和卓越的实际表现，吸引了众多关注。本文将深入探讨算力的概念、评估方法，以及在现代计算任务中的应用，特别是如何利用NVIDIA H20显卡来最大化算力优势。我们将结合理论与实际数据，全面分析H20的独特价值与未来发展方向。</blockquote><p></p><p></p><p></p><h1>算力的概念与历史演进</h1><p></p><p>1.1 算力的定义与基本概念</p><p></p><p>算力，或计算能力，是指计算设备在单位时间内所能完成的计算量。通常情况下，算力以每秒浮点运算次数（FLOPS）来衡量，这是浮点运算能力的标准单位。FLOPS代表每秒能进行的浮点数运算的次数，因此FLOPS越高，设备的计算能力越强。</p><p></p><p>在实际应用中，计算任务的种类繁多，从科学计算到深度学习模型训练，从金融数据分析到自动驾驶系统，各类任务对算力的需求各不相同。计算能力不仅仅是一个硬件性能指标，更是决定技术可行性和应用效果的重要因素。随着科技的进步，计算任务变得越来越复杂，数据量也在不断增加，因此对高算力的需求变得日益迫切。</p><p></p><p>1.2算力的发展历程</p><p></p><p>计算能力的发展可以追溯到计算机的早期历史。从最初的机械计算机到电子计算机，再到现代的超级计算机，计算能力的提升伴随着硬件技术的飞跃。早期的计算设备如ENIAC，每秒只能完成几千次简单的加法运算，而今天的超级计算机每秒可以完成数千万亿次浮点运算。</p><p></p><p>随着时间的推移，计算设备从单一处理器发展到多核处理器，再到并行计算和分布式计算。尤其是在图形处理单元（GPU）领域，NVIDIA等公司通过不断优化硬件架构，显著提升了计算能力。现代GPU如H20显卡，不仅在图形处理上表现优异，在并行计算、深度学习和科学模拟等领域也展现了强大的算力。</p><p></p><p>1.3 计算能力的重要性</p><p></p><p>计算能力是现代科技发展的基础。从物理模拟到分子建模，从图像识别到自然语言处理，强大的计算能力使得这些复杂任务得以实现。特别是在人工智能领域，深度学习模型的训练依赖于海量的数据和复杂的计算，因此对算力的要求极高。</p><p></p><p>在金融领域，高速交易系统依赖于实时的数据分析和决策，这些操作需要在微秒级别内完成，因此需要极高的计算能力。同样，在自动驾驶领域，车辆需要在短时间内处理来自多个传感器的数据，并做出驾驶决策，这也需要强大的算力支持。可以说，算力不仅是硬件性能的体现，更是推动科技进步的重要引擎。</p><p></p><p></p><h1>算力的评估与衡量方法</h1><p></p><p></p><p>2.1 评估算力的标准与方法</p><p></p><p>评估算力涉及多个方面，包括理论计算能力、实际执行效率和任务特定的表现。以下是几种常用的评估标准：</p><p></p><p>#1.&nbsp;FLOPS（每秒浮点运算次数）</p><p></p><p>FLOPS是评估计算能力的最直接指标，它表示硬件在一秒钟内能够完成的浮点运算次数。计算能力越高，硬件处理数据和执行任务的速度就越快。FLOPS通常分为单精度（FP32）、双精度（FP64）和混合精度（FP16、BFLOAT16等）不同类型，根据任务的不同，使用的精度类型也会不同。</p><p></p><p>#2.&nbsp;带宽（Bandwidth）</p><p>带宽指的是在单位时间内能够传输的数据量。内存带宽是决定计算设备性能的关键因素之一，尤其是在需要处理大量数据的任务中。高带宽可以有效减少数据传输的瓶颈，从而提高整体计算效率。在GPU计算中，带宽不仅影响数据加载的速度，也直接影响到模型训练的速度。</p><p></p><p>#3.&nbsp;延迟（Latency）</p><p>延迟是指从输入数据到获得输出结果所需要的时间。低延迟有助于减少数据传输和处理过程中的等待时间，特别是在并行计算中，减少延迟可以显著提高计算效率。延迟通常是并行计算系统的瓶颈，尤其是在大规模数据处理或多GPU协同工作时。</p><p></p><p>#4.&nbsp;能效比（Efficiency Ratio）</p><p>能效比是单位功耗下的计算能力。高能效比意味着在相同的功耗下，硬件能够提供更高的计算能力，这对于数据中心和高性能计算集群尤为重要。在实际应用中，能效比不仅影响计算成本，还影响系统的冷却和维护需求。</p><p></p><p>2.2 模型训练和推理中的算力评估</p><p></p><p>在深度学习和机器学习中，算力的评估往往与具体的任务需求挂钩。以下是几种常见的评估标准：</p><p></p><p>#1.&nbsp;训练速度（Training Speed）</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：训练速度通常以每秒处理的样本数（Samples per Second, SPS）或每秒处理的tokens数（Tokens per Second, TPS）来衡量。&nbsp;计算方式&nbsp;：SPS和TPS的计算方式如下：</p><p>SPS = 处理的样本总数 / 训练时间（秒）</p><p>TPS = 处理的tokens总数 / 训练时间（秒）</p><p></p><p>在计算过程中，样本数指的是输入数据的批次（Batch Size），而tokens数通常用于自然语言处理（NLP）模型的训练，指的是输入文本被分割后的最小单位（如词语或子词）。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>训练速度是衡量计算设备在模型训练过程中效率的关键指标。更高的训练速度意味着模型可以在更短的时间内处理更多的数据，从而加速模型的整体训练进程。这对于处理大型数据集或复杂模型（如深度神经网络、卷积神经网络等）尤为重要。</p><p></p><p>在实际应用中，提升训练速度有助于：</p><p>缩短模型的开发周期。提高资源的利用率，减少计算成本。在相同时间内进行更多实验，从而优化模型效果。</p><p></p><p>特别是在深度学习领域，使用更大的批次处理数据可以显著提高SPS或TPS，而高效的硬件如NVIDIA H20显卡能够支持更大的批次大小和更快的数据处理，从而提升训练速度。</p><p></p><p>#2.&nbsp;模型收敛性（Convergence）</p><p># 评估单位与计算方式 #</p><p></p><p>&nbsp;单位&nbsp;：模型收敛性没有统一的度量单位，但通常以训练轮数（Epochs）、迭代次数（Iterations），或达到某个性能指标所需的时间来衡量。&nbsp;计算方式&nbsp;：</p><p>收敛速度 = 目标性能指标 / 训练时间（秒）</p><p>或者使用收敛的轮数来衡量，即训练到模型性能稳定为止所需的训练轮数或迭代次数。</p><p></p><p>例如，在一个深度学习任务中，收敛速度可以通过模型达到一定的准确率或损失函数值所需的时间来表示。更少的训练轮数或迭代次数意味着更快的收敛速度。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>收敛性是衡量模型在训练过程中逐步逼近最优解的能力。算力越强，通常收敛速度越快，因为高算力设备可以支持更大的批次大小、更复杂的优化算法和更快的数据处理速度。这对于研究和开发时间有限的项目至关重要，因为加快收敛速度可以更快地得到有效的模型。</p><p></p><p>在实际应用中，收敛性与以下因素密切相关：</p><p>优化算法：如Adam、SGD等优化算法的选择和调整，直接影响模型的收敛速度。批次大小：更大的批次大小通常会加快收敛速度，但需要足够的显存支持，这也是高算力设备的优势。学习率：调整学习率可以帮助模型更快地达到收敛状态，但需要精细的调试以避免过拟合或欠拟合。</p><p>使用像NVIDIA H20这样具备高算力和大显存的设备，可以在保证计算精度的同时，加快模型的收敛速度。</p><p></p><p>#3.&nbsp;推理速度（Inference Speed）</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：推理速度通常以每秒处理的样本数（Samples per Second, SPS）或每秒处理的tokens数（Tokens per Second, TPS）来衡量，类似于训练速度。&nbsp;计算方式&nbsp;：SPS和TPS的计算方式如下：</p><p></p><p>SPS = 处理的样本总数 / 推理时间（秒）</p><p>TPS = 处理的tokens总数 / 推理时间（秒）</p><p></p><p>推理速度评估的是模型在实际应用中的响应时间，特别是在实时或近实时的应用中（如自动驾驶、语音识别、在线推荐系统等）。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p>推理速度是决定模型在生产环境中表现的关键指标之一。特别是在需要实时处理和响应的应用中，推理速度直接影响系统的用户体验和效能。</p><p>推理速度越快，系统的响应时间就越短，这对于以下场景尤为重要：</p><p></p><p>自动驾驶：车辆必须在极短时间内处理传感器数据并作出驾驶决策。实时翻译与语音识别：需要在用户发出命令后迅速给出响应。在线推荐系统：实时分析用户行为并推荐个性化内容。</p><p></p><p>NVIDIA H20显卡在推理任务中的表现尤为出色，特别是在FP8低精度计算中，能够在保持高效能的同时，提供极快的推理速度。</p><p></p><p>#4.&nbsp;精度与效率的平衡</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：精度通常以百分比（%）或数值（如损失值、准确率等）来表示；效率则以处理速度或能效比（FLOPS/Watt）来衡量。&nbsp;计算方式&nbsp;：</p><p></p><p>精度 = 模型在测试数据集上的性能指标（如准确率、F1分数等）</p><p>效率 = 计算资源消耗 / 达到目标性能所需的时间或能量。</p><p></p><p>在深度学习中，精度和效率往往需要进行权衡。例如，高精度计算通常需要更多的计算资源和时间，而低精度计算则可以在速度和资源占用上实现更高的效率。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>在实际应用中，精度与效率的平衡是设计和部署AI系统时必须考虑的重要因素。虽然追求更高的精度是许多AI任务的目标，但在某些场景下，高精度并不是唯一的考量。例如：</p><p>边缘计算设备：受限于计算资源和能耗，可能需要在精度和效率之间做出妥协。实时应用：如语音助手或实时翻译，更快的响应速度可能比绝对精度更重要。低成本部署：在大规模部署中，能够以更低的成本达到“足够好”的精度，可能比追求极限精度更具现实意义。</p><p></p><p>NVIDIA H20显卡提供了多种浮点运算模式（如FP16、FP8），允许开发者根据任务需求选择合适的精度和效率组合。例如，在训练阶段使用FP16混合精度可以提高训练速度，而在推理阶段使用FP8可以进一步优化性能，同时保持足够的预测精度。</p><p></p><p></p><h1>Part 3 NVIDIA H20显卡的深入解析</h1><p></p><p></p><p>3.1 H20显卡的硬件架构与技术创新</p><p></p><p>NVIDIA H20显卡基于最新的Hopper架构，在图形计算和并行计算领域引领了新一轮的技术革命。与前几代基于Ampere架构的显卡相比，H20在多个方面进行了大幅升级。尤其是在FP32、FP16以及新增的FP8精度计算能力上，H20展现了其在各种复杂计算任务中的卓越性能。</p><p></p><p>根据提供的图表，H20在FP32单精度浮点运算中达到了44 TFLOPS，这远高于基于Ampere架构的前代产品的19.5 TFLOPS。这一提升对于需要高精度计算的任务，如媒体处理、物理模拟等，具有重大意义。</p><p>在FP16和FP8的Tensor Core性能上，H20也大幅领先于前代产品。在FP16运算中，H20达到了148 TFLOPS，而在FP8的8bit浮点数据类型运算中，H20的性能更是达到了296 TFLOPS。这使得H20在处理需要大量并行计算的任务时，如深度学习模型的训练和推理，具备了极大的优势。</p><p></p><p>3.2 显存与带宽的优越性</p><p></p><p>H20显卡配备了96GB的HBM3显存，这是当前显存配置的顶级标准。这种显存不仅在容量上远超前代产品（80GB HBM2e），在内存带宽上也达到了惊人的4 TB/s，是前代产品带宽的近两倍。如此高的内存带宽使得H20显卡在处理大规模数据集和高分辨率任务时，能够更快地进行数据传输，减少处理延迟。</p><p>对于大模型训练和深度学习应用来说，显存的大小和带宽直接决定了硬件能否有效载入和处理训练数据。H20显卡凭借其96GB的显存，可以轻松应对需要大批量数据的任务，同时其4TB/s的带宽也确保了这些数据能够快速传输到GPU进行处理，这对于需要高效处理数据的任务如自动驾驶、图像识别等尤为重要。</p><p></p><p>3.3 H20的计算能力与实际表现</p><p></p><p>通过分析H20的计算能力图表，我们可以看到它在FP8、FP16等精度下的强劲表现。特别是在处理需要高效浮点运算的任务时，H20的Tensor Core能够提供前所未有的计算性能。例如，在8bit浮点数数据类型的FP8精度运算中，H20的性能达到了296 TFLOPS，适合用于量化训练和模型推理等场景。</p><p>NVLink互联带宽方面，H20也进行了显著的提升。相比前代产品的600GB/s和400GB/s，H20的NVLink带宽高达900GB/s。这意味着多个H20显卡在多卡互联时可以通过更高效的方式进行数据交换，减少了多GPU协同工作的延迟，从而提高了整体计算效率。</p><p></p><p>3.4 浮点运算模式的选择与H20的应用场景</p><p></p><p>在NVIDIA H20显卡中，不同的浮点运算模式为各种计算任务提供了灵活的选择。H20显卡支持从双精度（FP64）到最新的FP8低精度运算模式，覆盖了从高精度科学计算到高效推理任务的广泛应用需求。</p><p>双精度运算模式（FP64）：通常用于需要极高精度的科学和工程计算，如流体力学模拟、气候预测等领域。单精度运算模式（FP32）：是深度学习领域的主力，特别是在训练大型AI模型时，FP32能够提供足够的精度和较高的计算效率。半精度运算模式（FP16）：近年来在深度学习加速方面获得了广泛应用，尤其是在卷积神经网络（CNN）等任务中，FP16能够显著提高训练速度并减少显存占用。低精度运算模式（FP8和INT8）：随着量化技术的发展，FP8和INT8在推理任务中的应用越来越多，H20显卡的296 TFLOPS FP8算力使其在大规模模型推理中占据了显著优势。</p><p></p><p>3.5不同GPU型号的选择:SXM、PCIe、NVLink</p><p></p><p>为了满足不同用户的需求，NVIDIA为H20显卡提供了多种型号，包括SXM、PCIe和NVLink。这些型号的区别在于其硬件架构和连接方式，进而决定了它们在不同应用场景中的适用性。</p><p>SXM版：通过SXM模块设计，可以实现8块GPU的紧密互联，主要应用于高密度GPU服务器集群，如NVIDIA的DGX系统。这种设计不依赖传统的PCIe接口，而是通过NVSwitch实现更高的带宽和更低的延迟，特别适合用于超大规模AI训练和科学模拟任务。PCIe版：沿用了传统的PCIe接口，提供了更为灵活的部署方式。它支持与主板和CPU之间的直接通信，适用于传统的GPU服务器和通用计算任务。每两块GPU通过NVLink Bridge进行连接，虽然在带宽上不如SXM版，但在扩展性和兼容性上具有一定的优势。NVLink版：专为需要超高带宽的数据密集型任务设计。它提供了高达7.8 TB/s的传输带宽，适用于需要实时处理大量数据的大规模语言模型（LLM）训练任务。通过NVLink接口，多个H20显卡可以实现高速数据交换，减少计算过程中数据传输的瓶颈，提升整体计算效率。</p><p></p><p>通过结合这些不同型号的特点和应用场景，用户可以根据自己的具体需求选择最适合的GPU类型，从而在不同的计算任务中最大化地发挥H20显卡的性能优势。</p><p></p><h1>Part 4 H20在模型训练与推理中的实际应用</h1><p></p><p></p><p>4.1 通过Llama2模型探讨H20的应用</p><p></p><p>为了更好地理解H20显卡在实际应用中的表现，我们可以借助Llama2-70B模型的训练和推理数据来分析其性能。根据提供的图表，H20显卡在不同精度（FP8和FP16）以及不同输入输出长度下展现了卓越的计算能力。</p><p></p><p>在图表中，HGX H20模块在处理LLAMA2_70B模型时，在FP8精度下，输入长度为2048、输出长度为128的配置中，H20的吞吐量达到了1.2244595*A tokens/秒。而在输入长度为128、输出长度为2048的配置中，FP8精度下的吞吐量更是高达2.0981547*B tokens/秒。这表明在高精度和复杂模型的训练和推理任务中，H20显卡能够提供极为高效的计算性能。</p><p></p><p>相比之下，HGX A1XX模块在相同配置下，使用FP16精度的表现明显不如H20。这进一步证实了H20显卡在处理大规模语言模型时的优势。特别是在需要处理复杂输入输出关系的推理任务中，H20的高带宽和Tensor Core的强大性能，使其能够在更短的时间内完成推理，提供更高的吞吐量。</p><p></p><p>4.2 H20在推理任务中的独特优势</p><p></p><p>推理任务中，吞吐量和响应速度是两个关键指标。H20显卡凭借其FP8精度的计算能力，在处理LLAMA2_70B等大规模模型时，能够提供更高的tokens处理速度。结合前面的数据分析，H20在推理任务中的表现不仅仅体现在其计算能力上，还得益于其大容量的显存和超高的内存带宽，这些因素共同作用，使得H20能够在处理复杂推理任务时，保持高效和准确的性能输出。</p><p></p><p>通过NVLink的高带宽支持，多个H20显卡在多卡集群中可以实现高效的数据交换和协同计算，这对于需要实时处理和分析数据的任务来说，至关重要。例如，在自动驾驶系统中，H20显卡可以通过快速处理传感器数据并作出决策，从而提高系统的安全性和反应速度。</p><p></p><p>4.3 H20的GEMM性能分析</p><p></p><p>在矩阵乘法（GEMM）任务中，浮点运算性能是评估GPU计算能力的重要指标之一。以下是从表格中筛选出的伊迪雅H20在不同浮点精度下的GEMM性能数据，并对其进行详细分析。</p><p><img src=\"https://static001.geekbang.org/infoq/a3/a3e16713905cdb6c63c8cc433b76ba02.png\" /></p><p>分析与解读</p><p># FP8精度 #</p><p>峰值性能：293 TFLOPS实测性能：267.33 TFLOPS峰值百分比：91.25%</p><p></p><p>FP8精度下，伊迪雅H20的实测性能达到了267.33 TFLOPS，占峰值性能的91.25%。这一结果表明在低精度浮点运算中，伊迪雅H20的表现非常接近其理论最大值，表明其硬件设计在FP8运算任务中的效率极高，适合用于大规模模型推理和量化训练等场景。</p><p></p><p># INT8精度 #</p><p>峰值性能：293 TFLOPS实测性能：188.30 TFLOPS峰值百分比：64.27%</p><p></p><p>在INT8精度下，伊迪雅H20的实测性能为188.30 TFLOPS，占峰值性能的64.27%。虽然相对于FP8的表现有所下降，但INT8仍然提供了高效的计算能力。INT8精度广泛应用于需要处理大量数据的推理任务，尤其在资源受限的环境下，可以在降低计算复杂度的同时，保持合理的精度。</p><p></p><p># FP16精度 #</p><p>峰值性能：147 TFLOPS实测性能：141.55 TFLOPS峰值百分比：96.31%</p><p></p><p>在FP16精度下，伊迪雅H20几乎达到了其峰值性能，实测值为141.55 TFLOPS，占峰值的96.31%。这表明伊迪雅H20在FP16运算中能够充分发挥其硬件潜力，非常适合用于深度学习训练任务，特别是卷积神经网络（CNN）和递归神经网络（RNN）等对计算速度要求较高的模型。</p><p></p><p># TF32精度 #</p><p>峰值性能：74 TFLOPS实测性能：69.47 TFLOPS峰值百分比：93.88%</p><p></p><p>TF32是一种介于FP16和FP32之间的浮点精度模式，旨在提供比FP32更高的计算效率，同时保留一定的计算精度。在这一模式下，伊迪雅H20的实测性能为69.47 TFLOPS，占峰值性能的93.88%。这一表现说明TF32是一个平衡精度和效率的理想选择，特别是在要求较高的科学计算和AI模型训练中，能够显著提升计算速度。</p><p></p><p># FP32精度 #</p><p>峰值性能：40 TFLOPS实测性能：31.41 TFLOPS峰值百分比：78.53%</p><p></p><p>在FP32精度下，伊迪雅H20的实测性能为31.41 TFLOPS，占峰值性能的78.53%。虽然相对其他精度模式的效率稍低，但FP32依然是许多AI模型和科学计算任务的首选精度模式，特别是在需要高精度结果的场景中，FP32的稳定表现非常重要。</p><p></p><p>H20在不同精度下的GEMM性能分析，我们可以看到其在多种运算模式中的强大表现。无论是在高效推理任务中的FP8和INT8，还是在深度学习训练中的FP16和TF32，伊迪雅H20都能够提供接近其理论峰值的实际性能。这表明H20显卡不仅具备出色的硬件设计，还能在实际应用中充分发挥其计算能力，适合于从AI模型训练到大规模推理等广泛应用场景。</p><p></p><h1>Part 5 NVIDIA H20与H100的深入对比</h1><p></p><p></p><p>5.1 H100显卡的优势与应用场景</p><p></p><p>NVIDIA H100显卡是目前市场上最强大的GPU之一，其高达1979 TFLOP的理论计算能力，使得H100在处理高精度计算任务时具备无可比拟的优势。H100显卡的性能密度高达19.4，远超H20显卡的2.9，这使得H100在单位面积内能够提供更高的计算能力，特别适用于空间受限但需要高性能的计算环境。</p><p>在实际应用中，H100显卡主要应用于高精度科学计算、复杂AI模型训练和大规模数据分析等领域。对于那些需要极致性能的用户，H100显卡无疑是最佳选择。例如，在气候模拟、分子动力学和高精度物理模拟等任务中，H100显卡可以显著加快计算速度，减少模拟时间。</p><p></p><p>5.2 H20显卡的核心价值与独特优势</p><p></p><p>尽管H20显卡在理论计算能力上不如H100，但其在实际应用中的表现依然出色。特别是在大规模低精度模型训练和推理中，H20凭借其高显存、大带宽和较低的成本，展现了极高的性价比。对于那些需要处理大量数据且对计算精度要求不高的任务，如自然语言处理、推荐系统、图像识别等，H20显卡是一个非常具有竞争力的选择。</p><p></p><p>H20显卡的核心价值在于其出色的内存管理和高效的计算能力，特别是在FP8精度下，H20显卡能够以更少的节点数量完成训练任务，从而降低整体计算成本。这使得H20显卡在一些预算敏感的项目中，成为了性价比最高的解决方案。</p><p></p><h1>Part 6 H20在工业领域的广泛应用</h1><p></p><p></p><p>除了在AI研究中的广泛应用外，H20显卡在工业领域也展现了巨大的应用潜力。无论是在自动驾驶、智能制造，还是金融科技和医疗健康，H20显卡都能够通过其强大的计算能力，为各类复杂的计算任务提供解决方案。</p><p></p><p>在自动驾驶领域，H20显卡的高带宽和低延迟使得其能够实时处理来自多个传感器的数据，并做出驾驶决策。智能制造领域的复杂工艺模拟和优化，同样可以通过H20显卡的高算力得到加速。而在金融科技领域，H20显卡的快速数据处理能力可以显著提升高频交易系统的响应速度，降低市场风险。</p>",
    "publish_time": "2024-08-29 15:40:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“懒”人请进—— 有洞察的 AI 帮你完成数据分析",
    "url": "https://www.infoq.cn/article/6oDad1wtR7tgRJSMIKas",
    "summary": "<p>【云上探索实验室】专为开发者设计，旨在创造最前沿技术的一站式实操体验。2.0 版本的云上探索实验室全面升级！我们将采用沉浸式直播与实验平台实操相结合的方式，带你沉浸式感受生成式 AI 时代的技术魅力。</p>\n<p>系列公开课直播第三期《“懒”人请进—— 有洞察的 AI 帮你完成数据分析》将带你实操演示如何使用 Amazon Q in QuickSight 来构建 BI 报表，以及利用 Amazon Q，通过自然语言的方式来快速创建报表视图，并且体验如何利用大语言模型来生成一个数据故事。</p>\n<p>戳 <a href=\"https://sourl.co/fpUE76\">https://sourl.co/fpUE76</a> 进入云上探索实验平台，边学边玩开启实验！</p>",
    "publish_time": "2024-08-29 16:14:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]