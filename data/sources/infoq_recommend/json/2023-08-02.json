[
  {
    "title": "复杂业务开发之数据存储 ：SQL、数据库组件",
    "url": "https://www.infoq.cn/article/1UOSrFtkpdLSYGWwSYfV",
    "summary": "<p>在现代企业开发中，初级开发者难以掌握开发企业级 Java 应用，中高级开发者则被繁琐任务所占据。为了让大家能快速上手可以解决这个难题的“SoFlu 软件机器人”，前蚂蚁金服技术专家杨彪设计了《2 小时学会 10 个组件“取代”传统 Java 开发》课程，本节为该系列课程的第三讲。</p>\n<p>本节将主要针对 SoFlu 提供的常用组件——SQL及数据库组件展开讲解，同时解决“如何减轻开发者负担，避免数据操作异常？”的问题。要知道，数据存储是每个业务系统中必不可少的部分，也是开发者日常最多的工作，所以这是每位Java开发者的必听课。</p>\n<p>大家在课后可以登录 Gitee 下载 SoFlu 软件机器人客户端进行实践：<a href=\"http://suo.im/8wROo\">点击下载</a></p>\n<p>大家可以扫码添加小助手，进学习群与专家一对一交流：<br />\n<img alt=\"\" src=\"https://static001.infoq.cn/resource/image/e8/c9/e8833a01ba0bc705acab14a572b57cc9.png\" /></p>",
    "publish_time": "2023-08-02 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "搜索与分析型数据库论坛｜2023可信数据库发展大会",
    "url": "https://www.infoq.cn/article/9QeZ2pR1mHtFbElOWReC",
    "summary": "<p>近年来，随着非结构化数据成为各类组织数据的增长主力，搜索与分析型数据库发展迅速，关键技术陆续突破，应用场景日益增多，数据规模逐年上升，已成为企业必不可少的核心基础设施。在国产化建设需求持续升级的当下，如何搭建更稳定、安全、高效、智能的搜索与分析型数据库？在数据价值愈发凸显的未来，如何革新技术以更快更好地赋能百行百业？2023年7月5日，搜索与分析型数据库分论坛将邀请业内专家、学者及优秀厂商代表，与业界同仁深入探讨，为行业持续释放、挖掘数据价值注入新动力。</p>",
    "publish_time": "2023-08-02 09:25:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "时序时空与图数据库论坛｜2023可信数据库发展大会",
    "url": "https://www.infoq.cn/article/KECYPfa1bgM5YNclFNxl",
    "summary": "<p>从蛰伏、追随到自主创新，时空数据库在短短十几年里取得了里程碑式的飞跃。随着移动互联网、物联网、5G的迅猛发展，时空数据被广泛应用于能源管理、医疗保健等行业，高歌迈入属于它的黄金时代，爆发出惊人的增长姿态。与此同时，时空数据也迎来了不同业务场景下的重重难题。2023年7月5日，时序时空与图数据库分论坛将邀请本领域的专家亲临现场，溯源时序数据库的历史脉络，厘清发展途中面临的技术挑战，勾勒未来创新研发的趋势路径。</p>",
    "publish_time": "2023-08-02 09:26:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "互联网行业数据库创新应用论坛｜2023可信数据库发展大会",
    "url": "https://www.infoq.cn/article/lJeYTLuiZlKHevNdelOx",
    "summary": "<p>在大数据与云计算狂飙发展的时代，繁杂巨量的数据信息与快速迭代的新兴业务，引发了互联网企业对数据库技术的持续重构与升级。数据库在企业数字化转型的过程中，究竟发挥着怎样的作用？数据库在架构演进、自研内核优化方面有何实践经验？作为AI时代的新基建，数据库又将迎来怎样的挑战与发展？2023年7月5日，互联网行业数据库创新应用分论坛邀请数位互联网领域的知名技术专家，与数据库从业者共同探讨数据库自研、变革、创新之路。</p>",
    "publish_time": "2023-08-02 09:26:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌语音人工智能AudioPaLM，语音传输瞬间翻译",
    "url": "https://www.infoq.cn/article/6xrUw7llCxkeoebxHek7",
    "summary": "<p><a href=\"https://research.google/\">谷歌</a>\"的研究人员发布了<a href=\"https://arxiv.org/abs/2306.12925\">AudioPaLM</a>\"，这是一个大语言模型（LLM），可以通过语音传输执行文本转语音（TTS）、自动语音识别（ASR）和语音到语音翻译（S2ST）。AudioPaLM是基于<a href=\"https://www.infoq.com/news/2023/06/google-palm2-bard/\">PaLM-2 LLM</a>\"的，在翻译基准测试上优于<a href=\"https://www.infoq.com/news/2022/10/openai-whisper-speech/\">OpenAI的Whisper</a>\"。</p><p></p><p>AudioPaLM是一个基于Transformer的纯解码器模型，它将文本和音频输入组合成单个嵌入表示。与使用离散ASR、机器翻译（MT）和TTS模型等级联的传统S2ST模型不同，AudioPaLM可以保留声学特征，例如说话者的声音。AudioPaLM在S2ST和ASR基准测试中取得了最先进的成绩，并且还展示了零样本能力，对训练数据中不存在的输入和目标组合执行ASR。在<a href=\"https://paperswithcode.com/dataset/fleurs\">FLEURS数据集</a>\"上进行评估时，AudioPaLM在ASR任务上“显著”优于OpenAI的Whisper。</p><p></p><p>InfoQ最近报道了其他几个多语言人工智能语音模型。2022年，<a href=\"https://www.infoq.com/news/2022/10/openai-whisper-speech/\">OpenAI发布了Whisper</a>\"，这是一个基于Transformer的编码器/解码器ASR模型，可以转录和翻译97种不同语言的语音音频。今年早些时候，<a href=\"https://www.infoq.com/news/2023/06/meta-mms-speech-ai/\">Meta发布了MMS</a>\"，这是一个基于wav2vec的模型，可以用1100多种语言进行ASR和TTS。</p><p></p><p>与这些相比，AudioPaLM是一个基于Transformer的纯解码器模型。它是基于预训练的PaLM-2的。然后，将模型的标记字典扩展为包括声学标记，声学标记表示音频波形的短片段。它们被映射到与原始模型中文本标记相同的嵌入空间中。然后，模型的输入可以包括音频和文本。文本输入包括任务的简短描述，例如“[ASR意大利语]”。当模型的输出被解码时，可以使用<a href=\"https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html\">AudioLM模型</a>\"将声学标记转换回音频波形。</p><p></p><p><img src=\"https://imgopt.infoq.com/news/2023/07/google-audiopalm/en/resources/1audiopalm-architecture-1688226252526.png\" /></p><p></p><p>AudioPaLM的架构图。图片来源：<a href=\"https://google-research.github.io/seanet/audiopalm/examples/\">https://google-research.github.io/seanet/audiopalm/examples/</a>\"</p><p></p><p>AudioPaLM接受了来自100多种语言的数千小时的音频数据训练。它在多个基准上进行了评估，包括<a href=\"https://github.com/facebookresearch/covost\">CoVoST2</a>\"（AST）、<a href=\"https://research.google/resources/datasets/speech-to-speech-translation-corpus/\">CVSS</a>\"（S2ST）和<a href=\"https://github.com/facebookresearch/voxpopuli\">VoxPopuli</a>\"（ASR）。它在AST和S2ST上的表现优于基线模型，在ASR上具有“竞争力”。在使用<a href=\"https://paperswithcode.com/dataset/fleurs\">FLEURS</a>\"基准的零样本AST中，AudioPaLM“显著”优于Whisper。它在ASR任务上也优于Whisper，Whisper接受过ASR任务所涉及的语言的训练，而AudioPaLM没有。</p><p></p><p>研究人员还评估了AudioPaLM的音频生成质量，特别是在S2ST期间保留原始说话者的声音方面。他们结合“客观指标和主观评估研究”将其性能与基线模型进行比较，发现它“显著”优于基线。在他们的论文中，谷歌团队指出，需要更好的基准来衡量音频生成的质量：</p><p></p><p></p><blockquote>与文本相比，生成文本/音频任务的既定基准集的丰富性还不够成熟。这项工作主要集中在语音识别和语音翻译，它们的基准比较成熟。为生成音频任务建立更多的基准和指标将有助于进一步加快该研究。</blockquote><p></p><p></p><p>一些用户在Hacker News的帖子中<a href=\"https://news.ycombinator.com/item?id=36443676\">讨论了AudioPaLM</a>\"。在回答关于LLM翻译准确性的问题时，鉴于其会“产生幻觉”的倾向，一位用户表示，对于像AudioPaLM这样最先进的模型，幻觉“几乎不存在”。关于AudioPaLM的翻译，另一位用户观察到：</p><p></p><p></p><blockquote>令人印象深刻的是，它将“Morgenstund hat Gold imMund”（早晨口中含金子）翻译成了相应的英语表达“早起的鸟儿有虫吃”，而不是直译。</blockquote><p></p><p></p><p><a href=\"https://google-research.github.io/seanet/audiopalm/examples/\">AudioPaLM输出的若干示例</a>\"可以在网上找到。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/google-audiopalm/\">https://www.infoq.com/news/2023/07/google-audiopalm/</a>\"</p><p></p>",
    "publish_time": "2023-08-02 10:36:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "超越GPT-4！加州大学伯克利分校与微软研究院合作开源大型语言模型Gorilla",
    "url": "https://www.infoq.cn/article/gPRaLM4Edvq8ZtEN7Ar6",
    "summary": "<p>来自<a href=\"https://cs.berkeley.edu/\">加州大学伯克利分校</a>\"和<a href=\"https://www.microsoft.com/en-us/research/\">微软研究院</a>\"的研究人员开源了<a href=\"https://gorilla.cs.berkeley.edu/\">Gorilla</a>\"，这是一个可以编写API调用代码的大型语言模型（LLM）。在度量代码生成准确性的实验中，Gorilla优于包括GPT-4在内的几个基线模型。</p><p>&nbsp;</p><p>Gorilla被描述为“LLM的API应用商店”。它基于开源大型语言模型<a href=\"https://www.infoq.com/news/2023/03/meta-ai-large-language-model/\">LLaMA</a>\"。这个LLM在APIBench上做了调优。APIBench是一个新的ML模型API描述数据集，托管在<a href=\"https://huggingface.co/docs/hub/index\">HuggingFace</a>\"、<a href=\"https://pytorch.org/hub/\">TorchHub</a>\"和<a href=\"https://www.tensorflow.org/hub\">TensorHub</a>\"上。Gorilla还可以调用API定义的外部文档数据库，让它在访问新的API时无需重新训练。借助Gorilla，开发人员可以创建问题的自然语言描述，例如“调用图像分类模型，参数个数不多于10M，但ImageNet准确性至少要达到70%。”然后，Gorilla将输出Python代码，调用具有适当选项的ML模型。按照作者的说法：</p><p></p><p></p><blockquote>在各个领域，LLM正迅速普及。我们重点关注的是那些可以提高LLM在特定任务中API识别准确性的技术——这是这项技术发展中一个重要但经常被忽视的方面。作为一种通用语言，API函数能够实现各种系统间的有效通信。正确使用API可以提高LLM与更广阔世界中的工具进行交互的能力。</blockquote><p></p><p>&nbsp;</p><p>像<a href=\"https://www.infoq.com/news/2023/04/openai-gpt4/\">GPT-4</a>\"这样的LLM在包括生成代码在内的各种任务上都有出色的表现。然而，它们的API知识在训练时被“固定”了，因此，无法生成代码来调用更新的API。此外，它们经常会产生幻觉——在代码生成时，它们输出的代码可能会调用不存在的API。InfoQ之前报道过人们近来为解决这些问题所做的努力，例如，<a href=\"https://www.infoq.com/news/2023/04/meta-toolformer/\">Meta的Toolformer</a>\"可以调用外部服务API，<a href=\"https://www.infoq.com/news/2023/05/chatgpt-retrieval-plugin/\">ChatGPT的插件系统</a>\"可以利用外部资源来增强LLM。</p><p>&nbsp;</p><p>不过，伯克利团队指出，那些方法是利用API调用的例子来提示LLM。相比之下，Gorilla的方法侧重于“系统化地评估并构建一个可供未来使用的管道”。首先，研究人员构建了APIBench数据集。他们从HuggingFace模型中心、PyTorch中心和TensorFlow中心收集了所有的模型卡。经过过滤之后，获得了一个包含1645个API调用的集合。对于其中的每一个调用，研究人员使用GPT-4生成了一个指令-API对数据集，用于对Gorilla进行调优。</p><p>&nbsp;</p><p>在对Gorilla的输出进行评价时，一个主要的挑战是识别幻觉。首先，团队将幻觉定义为模型输出调用了在API定义外部数据库中不存在的API。这与错误不同，错误是指模型输出错误地调用了“真实存在”的API。团队使用所生成代码的抽象语法树（AST）来匹配数据库中的API和用于评估的测试集。在零样本任务中使用AST准确性度量，Gorilla比GPT-4高了20.43%。</p><p>&nbsp;</p><p>Gorilla的主要作者<a href=\"https://news.ycombinator.com/item?id=36333290\">Shishir Patil参加了黑客新闻关于这项工作的讨论</a>\"，并回答了几个问题。当被问及该模型的许可是否允许商业使用时，Patil指出，Gorilla有三个版本，基于LLaMA的版本没有商业应用许可，但基于MPT-7 base和Falcon-7B的版本可以。还有一位用户问，Gorilla与<a href=\"https://docs.langchain.com/docs/\">LangChain</a>\"相比怎么样。Patil回答说：</p><p></p><p></p><blockquote>Langchain是一个很棒的项目，它试图教代理如何利用提示来使用工具。我们对此的看法是，如果你想在数以千计的API之间做出选择，那么提示不具有可扩展性。而Gorilla作为一个LLM，可以帮你挑选API并编写语义、语法正确的API调用！它可以方便地替代Langchain！</blockquote><p></p><p>&nbsp;</p><p>Gorilla的<a href=\"https://github.com/ShishirPatil/gorilla\">代码和模型文件</a>\"托管在GitHub上。<a href=\"https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing\">这里</a>\"还有一个在谷歌Colab笔记本中的模型演示。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/microsoft-gorilla/\">https://www.infoq.com/news/2023/07/microsoft-gorilla/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/IQrajVXjyLTw1QjFxHV5\">比Bing更早将LLM集成到搜索引擎中，这家由谷歌前高管创立的公司为什么还是失败了？</a>\"</p><p><a href=\"https://www.infoq.cn/article/H2ZgktSAypUhC2I3zP2B\">马斯克等人热捧：高薪缺人，但要懂全栈懂LLM，一个全新职业正在兴起！</a>\"</p>",
    "publish_time": "2023-08-02 10:52:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC 应用与风险控制，你们先探索，我们紧随其后 ｜ArchSummit闭门会",
    "url": "https://www.infoq.cn/article/XzDAFw05DSkvCkfvkoj9",
    "summary": "<p>随着ChatGPT的火爆，大模型逐渐走入企业的视野，但在实际应用中存在着各种挑战与困难。在&nbsp;7&nbsp;月&nbsp;21&nbsp;日&nbsp;<a href=\"https://archsummit.infoq.cn/202307/shenzhen/\">ArchSummit全球架构师峰会（深圳站）</a>\"上，我们就&nbsp;AIGC&nbsp;在各行业的应用与风险控制展开了深入的讨论。以下为脱敏版本的分享纪要，希望对您有所启发。</p><p></p><h4>精彩分享1</h4><p></p><p>我们正在研发类似数字人的虚拟主播，以用于金融领域，如财报智能解读等。我们当前的方案是利用ChatGPT语言模型，并配合GRM来实现目标。我们选择在本地部署ChatGLM-6B模型，配合使用Stable&nbsp;Diffusion处理图片。</p><p></p><p>在常规情况下，我们会先根据上下文生成评论，然后进行PDF解析和添加图片，最后生成一分钟以内的播报。</p><p></p><p>另一个项目是关于垂直领域的场景，我们正在探索如何结合风控原有数据，利用Chatgpt进行解读。其中包括使用上下文或langchain等方式。</p><p></p><p>目前，这方面的效果还在验证阶段，不过它的效果将依赖于我们系统中台的能力。我们正在努力将中台的所有功能和服务能力转变为函数和服务接口，并通过GPT的提问方式调用这些接口，进而生成文本。</p><p></p><p>最后，我们团队正在认真考虑 AI 相关的风险问题。但是我们选择保守一些，让其他竞争者先探索，我们主要关注一些较浅层次的应用。</p><p></p><h4>精彩分享2</h4><p></p><p></p><p>在产品探索上，我们已经应用了两个小点。首先是，我们公司的市场品牌宣传，图文问题已经解决，以至于原来做这个事情的同事已经可以去别的部门了。</p><p></p><p>原来可能要花三天打磨一个80分的文章，现在一天可以产出5~10&nbsp;篇70~75分的文章。我们产出的这些文章主要用在SEO上，例如敏捷迭代、瀑布管理、DEBOR等概念的介绍。这些内容并不需要质量非常高，只需要达到一定的程度。现在来看，这类型的内容可以由&nbsp;AI&nbsp;生成的。</p><p></p><p>另外是第二个应用——生成单元测试的用例。这个过程是建立在需求描述清楚，特别是函数的注释清楚的情况下，以前我们都会根据函数的参数类型和注释生成测试用例，现在这个过程更进一步，变得更方便了。</p><p></p><p>现在大约有60-70%的单元测试就是这样生成出来的。但至于这个提效有多少，说实话，还不一定。我们用Copilot辅助写代码，感觉上很好，但很难量化它最终的产出效果。不像写营销文，我可以用1/4的成本，提升3~5倍的效率。目前来说我们内部的形容，大模型是一个更好用的补全工具。</p><p></p><h4>精彩分享3</h4><p></p><p></p><p>在我们内部沟通时，我注意到一个重要问题。当培训员工时，我会告诉他们何时可以使用某种功能，何时不可以使用。这对于掌握这项技术非常关键，因为编辑这个功能的开启时机非常重要。</p><p></p><p>我曾经亲自尝试过，唯一能够真正快速掌握并充分利用这项技术的方法是，将需求拆分成函数级别，确保函数的通用性。这样，我就可以解决问题并生成相应的代码段。</p><p></p><p>然而，如果函数与业务过于相关，复制粘贴可能会解决当前问题，但长远来看是行不通的，因为系统接口并不会理解你的需求。因此，在内部应用中，我们需要确保代码的通用性，以确保我们的系统能够高效运作。</p><p></p><h4>精彩分享4</h4><p></p><p></p><p>我想分享两方面&nbsp;Security&nbsp;for&nbsp;AI&nbsp;&amp;&nbsp;&nbsp;AI&nbsp;for&nbsp;Security。首先是&nbsp;Security&nbsp;for&nbsp;AI，对于千片卡以上的大型模型，很可能需要使用公有云来运行。尽管我们可以通过专线接入云服务，但是我们也意识到这样做可能存在较大的风险。</p><p></p><p>目前我们正在思考如何规避这些风险。例如，将大型模型运行在云上需要对接机器学习管理平台，这些平台提供模型算法和基础能力，并能直接调用底层资源配置。</p><p></p><p>在这个过程中，权限管理是一个头疼的问题。普通用户的权限管理相对简单，每类用户的权限基本上没有太多管控，他们都可以直接连接到平台进行操作、调参和模型调优。然而，一旦员工离职，可能花费了数月时间训练出的模型就可能被直接拷走。</p><p></p><p>为了保障数据的安全共享，我们目前正在内部采用TEE（可信执行环境）技术，用于不同部门间的数据共享。这样至少能确保数据是可信的，不会被窃取。但由于TEE技术受限于卡片性能，我们正在考虑如何提升性能。</p><p></p><p>目前，据说H800每百次可以支持一次计算性能的优化，然而目前性能上仍然相对较差。因此，我们正在研究是否有组合方案，例如将CPU的能力与A100和A800卡的性能结合，以实现隐私计算。</p><p></p><p>其次是AI&nbsp;for&nbsp;Security，安全领域一直面临着一个难解决的问题，即安全运维。无论是国外还是国内，数据安全需求不断增加，因为需要同时记录流量日志、事件和报警等信息，这对于每家企业来说数据量都非常庞大。</p><p></p><p>例如，当攻击流量涌现时，如何分析哪些数据是攻击的，AI&nbsp;在这方面能够提供很大的帮助。国外的一些公司，比如微软，已经将内部的安全运维交给了\"Copilot\"，通过对话形式，安全运维人员可以直接了解每天关注的事件及其解决方案。</p><p></p><p>而国内主要是在已有的数据上进行自动化统计，真正智能化生成的内容还相对较少。不过，这个方向对于安全领域来说是一个值得探索的方向。</p><p></p><h4>精彩分享5</h4><p></p><p></p><p>AI大型模型具有几个优势。首先，它拥有广泛的知识库，尽管随着链路的加深，有时知识会有些模糊，但它所知道的比单个人类要丰富；其次，它具备强大的推理能力，特别在复杂的领域如理赔案件判定和保险条件评估方面，可以代替人力进行基础性的决策，从而节省了理赔保全等事务处理所需的人工劳动力，保留了一些专家级别的人力资源，提升了效率。然而，在生产关系上，生产力的提升带来的是效率的提升，而岗位本身并不会消失。人工审核在某些领域仍然是必要的。</p><p></p><p>而大型模型不是简单的0和1的准确判断，而是通过自回归推断来完成，在某种程度上是一种更大概率的预测。在To&nbsp;B领域追求精准的情况下，很难完全取代某些工种，AI大型模型更多地作为副驾驶的存在，用于提高效率和生产力，但无法改变生产关系的基本需求，产品的需求仍然存在。</p><p></p><p>另一个方面是在风控场景中的应用。公司尝试使用智能助手辅助代理人使用复杂产品的APP。最后，大型模型在内容生成领域有着广泛的应用，如文本、音频和视频生成。结合数字人技术，可以用于培训行业或保险产品推荐。例如，保险计划书，未来可能由代理人的数字分身来向客户解释，</p><p></p><h4>精彩分享6</h4><p></p><p></p><p>我们的客户中有很多拥有大量文档，数量达到几千万份，这些文档的探索过程中发现了一些有意思的情况。在产品设计初期，由于搜索能力尚未完善，这些文档的潜在价值无法充分发挥，交互体验也不尽如人意。然而，随着搜索能力的提升，我们发现许多交互问题得到了解决。</p><p></p><p>然而，我们又发现搜索能力主要服务于普通员工，但存在一个场景尚未涵盖，即大部分政府单位，领导通常有秘书协助。他们需要整理相关文档内容。在政务领域等特定应用中，领导和局长需要查看与今年及前几年相关联的数据，秘书需要将这些相关内容整理并提供给领导。</p><p></p><p>这也是一个非常有市场需求的产品，目前，我们已经在开发阶段，通过模型来理解文档背后的逻辑，并将相关内容组织整理，模拟秘书的工作，然后将所需信息整理打包供查看和调阅。</p><p></p><h4>精彩分享7</h4><p></p><p></p><p>为了保证核电站的安全，我们需要投入大约500亿的资金，其中有2/3用于预防措施。核电发电成本本身非常低，但由于安全要求高，大部分投资都用于设备的维护和人员的配备与培训。核电站面临的主要问题是如何防止人员和设备出现错误。</p><p></p><p>关于设备出错，目前的解决方案效率较低，我们需要定期对设备进行检查和维修，通过设备运行的历史数据，我们可以判断出何时需要维修，然而，这项任务对人员来说并不容易实现。因此，我们将这项任务交由AI来执行，</p><p></p><p>全世界核电厂的数据都是公开透明的。每个核电站的设备信息、运行情况、维修经验等都在全球范围内共享，我们建立了一个经验反馈系统。我们利用大模型进行核电厂的经验反馈数据分析。通过将全球所有核电厂的数据输入给这个系统，它可以根据我们的设备信息来判断可能存在的风险和后果。通过这种方式，我们能够更好地保障核电站的安全和运行效率。</p><p></p><p>针对人员减少出错的问题，培训是一个非常重要的措施。然而，培训成本高昂且效率较低。在需求分析、过程实施、效果评估以及人员的孪生建模等方面，我们现在主要依赖人力来完成，通过AI，我们可以实现更精准的培训，它可以自动判断学员需要什么样的培训。</p><p></p><p>我自己是核电领域的第一个数字人，我用它来讲解核物理中的反应堆。不过我们希望进一步实现问答式的知识库。通过AI分析，我们可以直接提供学员所需的多模态教学内容，包括语音和视频教学。学员可以边学习边进行交互式评估。</p><p></p><p>我们正在开发一个名为\"数字教室\"的系统，将其作为教室的机器人，放置在教室中。通过这个系统，教员可以假装成学员，有意制造错误让机器人来分析是否需要相应的知识点。如果机器人能够说服教员，我们就可以投入使用。在人才培养领域，我们利用数字人技术代替了辅导员。</p><p></p><h4>精彩分享8</h4><p></p><p></p><p>风险控制这块，可以考虑让内部员工签署保密协议，以确保他们不会泄露关键信息。另一个方法是通过控制IDE上传的大小来限制代码的传递。这样可以确保大规模的代码不会全部上传，只传递输入和结果等关键部分。敏感词扫描也是一个有效的措施，可以检测和阻止包含敏感信息的内容上传。</p><p></p><p>例如，像清华大模型等大型模型可以限制上传的大小，只允许传递部分关键信息，这样可以降低泄露的风险。虽然你可以询问大规模的问题，但是返回结果可能会受到限制，以保护核心技术和数据。</p><p></p><p>另外，设置一层网关并进行关键词拦截是一个有效的措施。通过在网关上设置关键词拦截功能，可以阻止包含敏感信息或公司核心技术的数据传递到外部。</p>",
    "publish_time": "2023-08-02 11:44:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型竞争突然升级！亚马逊CEO亲自监督、组建新的核心技术团队，集中优势资源打造“最具野心”的大语言模型",
    "url": "https://www.infoq.cn/article/3utpk9247A6CtoyztLTB",
    "summary": "<p></p><blockquote>亚马逊“最具野心”的大语言模型，将会是什么样？</blockquote><p></p><p></p><h2>亚马逊被曝组建新团队研发大语言模型</h2><p></p><p>&nbsp;</p><p>据外媒 Insider 近日报道，亚马逊 CEO Andy Jassy 目前正直接领导一支新团队，负责开发该公司最具野心的 AI 项目。</p><p>&nbsp;</p><p>Jassy 告知亚马逊的 S-team（由 20 多名高管组成的亚马逊最高决策团队），他将提拔 Alexa 首席科学家兼高级副总裁 Rohit Prasad 作为自己的直接下属，向他直接汇报。目前 Jassy 共有 16 名直接下属，包括&nbsp;Devices and Services 高级副总裁 Dave Limp、零售业务负责人 Doug Herrington、云计算 CEO Adam Selipsky 以及 CFO Brian Olsavsky 等。</p><p>&nbsp;</p><p>根据 Insider 获得的一封内部邮件，Prasad 将在新位置上组织建立新的技术小组，致力于为亚马逊打造“最具野心”的大语言模型。</p><p>&nbsp;</p><p>大语言模型是 AI 工具中的底层技术，能够从巨大的训练数据集中学会生成与人类相似的响应结果。OpenAI、谷歌和 Meta 等企业都已建立起规模庞大、功能强劲的大语言模型，并在全球范围起掀起热潮。亚马逊之前也有相关布局，成果包括 Alexa Techer Model 和 Titan。</p><p>&nbsp;</p><p>在 6 月末发出的这封邮件中，Jassy 称 Prasad 将领导一支“中央小组”，负责构建亚马逊“最具泛用性”的大语言模型。</p><p>&nbsp;</p><p></p><blockquote>“简单说一下，Prasad 将调任新岗位，负责领导一支中央小组并构建我们最具泛用性的大语言模型。虽然我们已经在公司内部构建起多个大语言模型，还有另外几个项目也在推进，但这次我们将集中优势资源打造最具野心的大语言模型，并由 Prasad 领导这支队伍。在新岗位上，Prasad 将向我直接报告。”</blockquote><p></p><p>&nbsp;</p><p>资料显示，Prasad 统领 Alexa 团队已经超过十年，同时他也是 Dave Limp 领导的亚马逊 Devices and Services 团队中的一员，并将继续承担这方面工作。</p><p>&nbsp;</p><p>根据 Insider 看到的另一封邮件，就在 Jassy 于 6 月宣布上述决定后不久，Limp 在团队内部发出通告，称 Prasad 仍将是“Alexa 的关键合作伙伴与支持者，并将在我们的未来业务中发挥重要作用。”这封邮件称，亚马逊 Devices 副总裁 Daniel Rausch 将接掌 Alexa 产品与业务组织，包括各娱乐、合作伙伴参与及跨国团队。</p><p>&nbsp;</p><p>Limp 在邮件中强调，“我们在 Alexa 的使命和愿景上没有动摇，对于我们向客户交付新发明、新成果的能力，我也比以往任何时候都更加乐观。”有消息显示，亚马逊最近还启动另一个新项目，希望利用类似 ChatGPT 的技术让 Alexa 变得更智能、更富个性化。</p><p>&nbsp;</p><p>目前关于 Prasad 领导的新团队并未有更多消息流出，但可以看到，亚马逊在这场大模型竞赛中已经准备好了，至于未来能带来什么样的惊喜，还需要交给时间。</p><p></p><h2>亚马逊入局“大模型之战”</h2><p></p><p>&nbsp;</p><p>根据 Insider 之前的报道，亚马逊正急于应对生成式 AI 的迅速崛起。尽管亚马逊过去数十年间一直致力于 AI 技术研究，但微软、OpenAI 和谷歌等竞争对手明显取得了一定优势，Meta 最新的Llama 2 模型也在整个科技行业内掀起了波澜。为此，亚马逊决定在新计划中立足亚马逊云科技事业部组建新团队，专注于帮助客户使用生成式 AI 产品。</p><p></p><h3>亚马逊的 AI 产品布局</h3><p></p><p>&nbsp;</p><p>在这场大模型竞赛中，亚马逊云科技已经交出过不少答卷。今年 4 月，亚马逊推出了 Amazon Bedrock 服务、Amazon Titan 大语言模型，以及 AI 编码助手 Amazon CodeWhisperer。</p><p>&nbsp;</p><p>其中，Amazon Bedrock 既提供自研的大语言基础模型—— Amazon Titan Text 、Amazon Titan Embeddings，也与 AI21 Labs、Anthropic、Stability AI 等基础模型提供商广泛合作，助力企业轻松灵活构建生成式 AI 应用，降低所有开发者的使用门槛。</p><p>&nbsp;</p><p>Andy Jassy 在此前接受 CNBC 采访时表示，“大多数企业都想要用上大语言模型，但顶尖 AI 模型需要几十亿美元和长达数年的训练成本和周期，用户肯定不想亲自动手。因此，他们希望能在规模够大、性能更好的基础模型之上工作，再根据自身需求对其做出定制。而这，就是 Bedrock 的基本定位。”</p><p>&nbsp;</p><p>Amazon Titan 基础模型可以识别和删除客户提交给定制模型的数据中的有害内容，拒绝用户输入不当内容，过滤模型中不当内容的输出结果。Titan 系列模型分为两种，一种是用于内容生成的文本模型，另一种是可创建矢量嵌入的嵌入模型，用于创建高效搜索功能等。</p><p>&nbsp;</p><p>AI 模型经常会出现“一本正经地胡说八道”现象，尽管输出内容看似有理有据、令人信服，但实际上并没有相关训练数据可以支撑。针对 AI “幻觉”问题，亚马逊云科技副总裁 Bratin Saha 此前在接受外媒采访时表示，亚马逊非常关心准确性，并努力确保 Titan 模型能够生成高质量的响应结果。</p><p>&nbsp;</p><p>据外媒报道，十几年前起就一直在亚马逊工作的 Sivasubramanian 表示，亚马逊在 AI 领域已经持续投入二十多年，亚马逊云科技目前拥有超过 10 万家 AI 相关客户。他同时补充称，亚马逊也一直在使用 Titan 的微调版本交付主页上的搜索结果。</p><p>&nbsp;</p><p>Amazon CodeWhisperer 则是一款面向个人开发者免费使用的辅助代码编写工具，是一种人工智能代码生成扩展，目标是提高软件开发者的工作效率。CodeWhisperer 可以更快地完成更多工作，避免软件开发人员花费大量时间编写非常简单且无差别的代码，CodeWhisperer 作为 AI 编码伴侣，它能根据开发人员的自然语言评论和集成开发环境 ( IDE ) 中的先前代码实时生成代码建议，从根本上提高开发人员的工作效率。</p><p></p><h3>“大型语言模型和生成式AI对亚马逊意义重大”</h3><p></p><p>&nbsp;</p><p>在发布上述&nbsp;AI 服务/产品的同一天，亚马逊还发布了 2022 年度股东信，Jassy 在信中提到，公司正大力投资大型语言模型和生成式 AI。</p><p>&nbsp;</p><p>Jassy 表示，LLM 和生成式 AI 是能让“亚马逊未来几十年可以在每个业务领域都进行创新的核心”，将显著加速亚马逊已经深耕了 25 年的机器学习的应用，他称生成式 AI 具有变革性，对客户、股东和亚马逊来说都意义重大：</p><p>&nbsp;</p><p></p><blockquote>“亚马逊研发自己的 LLM 已有一段时间了，相信它将改变并改善几乎每一种客户体验，并将继续在所有我们的消费者、卖家、品牌和创作者体验中大量投资这些模型。&nbsp;与多年来亚马逊云科技的发展路径一样，我们正在推动各种规模的公司都可以利用生成式人工智能。亚马逊云科技提供了最具性价比的机器学习芯片Trainium和Inferentia，使得小型和大型公司都可以负担得起在生产中训练和运行他们的 LLMs。&nbsp;亚马逊的商业客户可以从各种LLMs中进行选择，并使用客户喜好的所有AWS安全、隐私和其他功能构建应用程序。此外，我们正在提供像AWS CodeWhisperer这样的应用程序，它通过实时生成代码建议来革命性地提高开发者的生产力。”</blockquote><p></p><p>&nbsp;</p><p>Jassy 最后提到，他本可以用生成式人工智能写一整封信，但他要把这封信留到未来。大型语言模型和生成式人工智能对客户、股东和亚马逊来说都将是一件大事。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.businessinsider.com/amazon-ceo-andy-jassy-oversees-group-most-ambitious-ai-models-2023-7\">https://www.businessinsider.com/amazon-ceo-andy-jassy-oversees-group-most-ambitious-ai-models-2023-7</a>\"</p><p><a href=\"https://www.infoq.cn/article/j3qbSPiG9Hmapal2exir\">https://www.infoq.cn/article/j3qbSPiG9Hmapal2exir</a>\"</p><p><a href=\"https://www.cnbc.com/2023/04/13/aws-launches-bedrock-generative-ai-service-titan-llms.html\">https://www.cnbc.com/2023/04/13/aws-launches-bedrock-generative-ai-service-titan-llms.html</a>\"</p>",
    "publish_time": "2023-08-02 15:05:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "打造中国版“Snowflake”，经济低迷时期技术创业型公司如何乘风破浪？",
    "url": "https://www.infoq.cn/article/1dwmueV39HfaatqC7MXq",
    "summary": "<p>在当下科技创业热潮中，多云及一体化数据平台提供商云器科技备受瞩目。成立于 2021 年的<a href=\"https://www.infoq.cn/article/uVkQaLTDVKLLOJwqNDhz\">云器科技</a>\"，在近期宣布完成连续两轮总计数亿元人民币的融资，并举办<a href=\"https://www.infoq.cn/video/22GhZyjOkqJEzbHD9o3I\">新产品发布会</a>\"的消息获得了广泛的关注。</p><p></p><p>这家初创公司选择在如此低迷的经济环境中乘风破浪，其背后秘诀何在？他们又是如何看待技术创新与商业成功的关系？在全球竞争激烈的市场上，云器科技如何保持核心竞争力？<a href=\"https://www.infoq.cn/article/Re4O8YyDbACbrsOh3R7l\">一体化趋势下</a>\"，云器自创的 Single-Engine 理念有何独特之处？近期，<a href=\"https://www.infoq.cn/video/AN33kAfP4y35xBeb6M22\">InfoQ【C 位面对面】</a>\"栏目特别邀请到云器科技联合创始人 &amp; CTO 关涛，来分享云器科技的创业之路、技术创新策略以及对中国数据平台发展的独到见解，</p><p></p><p>探讨主题如下：</p><p></p><p>中国版的 Snowflake 在哪里？在低迷的经济环境中，技术创业型公司如何乘风破浪？一体化趋势下，技术创新有哪些关键点？</p><p></p><p></p><h2>中国版的 Snowflake 在哪里？</h2><p></p><p></p><p>关涛：相比海外，中国的原创数据平台技术公司和产品还是太少。我们希望在技术侧推出自有的新技术理念，也希望构建中国的云上国之重器，打造中国原创的数据平台技术。</p><p></p><p>InfoQ：云器科技成立于 2021 年，彼时国内的数据平台正处于发展期向普惠期过渡的阶段。为什么选择这样一个时机创立云器科技？创立云器科技的初心是什么？</p><p></p><p>关涛：从整体发展的视角看，国内的数据平台市场有很大的空间和潜力。我个人来讲，我们希望在技术侧能推出有自己理念的新东西。我们之所以取名叫云器，也是希望构建中国的云上国之重器，打造中国原创的数据平台技术。</p><p></p><p>InfoQ：当时，国外市场上 Snowflake、Databricks 这样的产品已经经历了 8-10 年的发展，一直保持高速增长。云器是否考虑过复制他们的成功模式？云器科技设定的发展路线 / 策略是怎样的？</p><p></p><p>关涛：从商业模式来看 Snowflake 有几个特点。首先它是多云的商业模式，基础设施在云上，我们希望按这个模式来做。其次它是 SaaS 模式，用户无需关心技术细节，上手简单，我们也是一样的定位。所以我们与 Snowflake 的商业模式、产品理念基本一致，我们希望填补中国在这方面的空白，满足愿意用 SaaS 化的多云平台的企业需求。</p><p></p><p>云器与 Snowflake 的不同在于技术侧。多年前 Snowflake 凭借 shared-data 架构与云原生模式取得成功，但那时的先进技术现在已经成熟了。我们的技术理念有所不同，我们推出了自己的增量计算模式与 Single-Engine 一体化设计。</p><p></p><p>InfoQ：在数据平台领域，中美是否有差异？中国企业有哪些本土技术创新的机会？</p><p></p><p>关涛：从市场的差别看，首先美国绝大多数的企业都在云上，选择自建云平台的很少。而中国还有 30% 左右的企业在线下做自己的数据中心，云化和数字化还有潜力。第二点，美国绝大多数企业不自建数据平台，而是选择 Snowflake 等产品，而国内基于开源自建很普遍，这是市场侧的差异。技术视角的差异没那么大，但美国前五的数据平台都是原创技术，而国内企业更多是拿来主义，原创偏少。更多中国企业选择了自己修改开源方案，同质化明显。国内还没有 Snowflake 这样有规模和影响力的公司和产品，这也给国内注重原创技术的企业创造了很大的市场机会，这样的企业如果做得比较好就会有很大的回报。</p><p></p><p>InfoQ：最近网上热门的讨论主题在说“中国不需要 SaaS”，国内的大环境下 SaaS 企业很难生存，认为中国应该走印度 InfoSys（外包管理公司）这样的技术路线，不需要 SaaS，您怎么看待？</p><p></p><p>关涛：从我们自身情况看，云器从创业到现在，从云底座管控平台到 HR 系统、绩效系统、文档系统和周边的很多系统，几乎都选择了 SaaS 化服务。从这个层面上讲，我认为 SaaS 已经潜移默化地改变了非常多国内企业的形态。所以 SaaS 在中国已经发展起来了，并不是伪需求。</p><p></p><p>而很多企业之所以需要的是解决方案而非平台，是因为国内 IT 人员数量总体少、企业技术水平不高，没有足够的人基于 SaaS 做业务开发，需要其他公司来做整体的解决方案。所以国内的 SaaS 就要做得更加简单易用，而不是单纯抱怨市场。服务好客户仍然是国内 SaaS 的成功关键。</p><p></p><p>另外，国内企业更愿意选择 SaaS 加服务的模式。针对技术水平较高的企业可以提供标准化产品，同时针对技术薄弱的环节要提供定制化的服务做补充。</p><p></p><p>InfoQ：打造中国版的 Snowflake 是很多厂商的愿景，那么为什么在中国至今仍然没有出现 Snowflake 这样的巨头公司？如何看待多云数据平台与云厂商的竞争格局？</p><p></p><p>关涛：我们把 Snowflake 定义成多云独立、SaaS 化且有一定影响力和营收规模的平台，那么国内为什么没有这样的巨头？首先中国原创的数据技术还是太少了，而基础设施需要长周期、大投入，要有很多经验积累。而国内这一领域相对年轻，积累总体少。另一方面国内企业的付费意愿也不够高，SaaS 化的商业环境不够友好。但未来随着越来越多的企业意识到自建数据平台并非自己的核心竞争力，不应该在这方面投入重资产，国内迟早会有 Snowflake 这样的企业成长起来。</p><p></p><p>其实云器和国内云厂商的关系可以类比 Snowflake 和亚马逊云科技的关系。Snowflake 没有替代亚马逊云科技，云器也不会替代其他国内云厂商。云器 Lakehouse 平台是构建在云上，与其他厂商之间以合作关系为主。</p><p></p><p></p><h2>在低迷的经济环境中，技术创业型公司如何乘风破浪？</h2><p></p><p></p><p>关涛：经济低迷的环境中，有着深厚积累的技术平台企业有更多机遇。云器专注于细分领域，只要能做到最快、最简单、最低成本，就能在数据平台这样成熟的技术领域获得市场成功。</p><p></p><p>InfoQ：近日，云器科技宣布已完成连续两轮总计数亿元人民币的融资，且刚刚举办了新产品发布会。作为一家初创公司，在经济依旧低迷的当下，云器快速成长的秘诀是什么？</p><p></p><p>关涛：第一，最近一年半我们一直专注技术和产品本身，最近才开始正式接触客户，所以经济环境对我们影响不是很大。在经济低迷期，我们刚好做积累来构建产品。第二，经济低迷的环境让我们和客户都更容易想清楚定位。客户经过测试发现云器确实更简单，也能给他们降低成本。所以这样的经济环境对于平台技术企业并不完全是坏事。我们希望在这样的经济条件下提供给客户多一个选择，让轻资产的客户也可以有高性能、低成本的数据平台。</p><p></p><p>轻资产 / 重资产 IT 投入，数据平台是否自建，这些问题一直有争论。但我们认为，最终效率会成为判断的“黄金指标”。我们会更专注于细分领域，效率会是我们的核心价值。</p><p></p><p>InfoQ：您认为该如何判断一项技术是否能够在商业上取得成功？</p><p></p><p>关涛：我的理解，创业大概有两种模式。第一种创业模式叫 Searching，是说我有一套新技术、一套新想法，但不知道这套想法能不能最终转化成商业成就，不知道就去做 search ，类似在森林里找路的模式，找到金矿就赢，找不到就输。例如 LLM 大模型当前总体还属于这样的模式，大家都在寻找合适的商业化路径。还有一种叫 Mountain Climbing，也就是爬山模式。这种模式下商业场景是清楚的，大家是沿着不同路径向山顶爬，看谁爬得快爬得好，谁就能赢得市场。</p><p></p><p>数据平台领域其实是后者，数据平台的价值、应用场景都非常清楚，最后谁的技术最好、最简单、最低成本，谁就赢。</p><p></p><p>InfoQ：云器科技如何看待开源领域的发展？在竞争激烈的市场中保持核心竞争力的关键是什么？</p><p></p><p>关涛：云器还没有开源，短期开源战略也不是我们重点。我认为在一个技术领域的发展初期，开源会极大推动这个领域的发展。那时行业的头部企业和研发人员有着很好的技术水平，他们开源后回馈社区，形成技术迭代，推动行业快速成长。但随着技术发展逐渐成熟，很多传统行业的用户进入市场，他们就不那么看重开源，而只是想要简单、便宜、好用的产品，甚至不想知道内部的技术细节。我们认为现在数据平台已经发展到了这个阶段，这也是我们没有选择开源的一个原因。</p><p></p><p>另外，开源体系更像组装式的架构，每个开源的领域都很小，大家拼到一起形成一个大生态。但对于很多用户而言，比如 Snowflake 的客户，他希望你把所有细节都封装起来，展现给用户的是一个简单的接口，闭源模式更适合这个领域现在的状态。</p><p></p><p>InfoQ：在前不久的发布会上，云器科技发布了新一代数据平台云器 Lakehouse。能否聊聊云器 Lakehouse 的研发历程，在整个过程中团队遇到过哪些困难挑战，又是如何克服的？</p><p></p><p>关涛：我们创业两年来，目标一直是做下一代的数据平台，技术路线一开始就确定了，包括不开源、SaaS 模式、湖仓一体架构，Single-Engine 一体化这些大方向都没有变动过。执行阶段整体还是非常聚焦。原创数据平台技术门槛和投入高，我们产品 MVP 版本开发大约用了 1 年。之后是 PMF，经过多次迭代，听取客户反馈意见，不断加入新的需求，完善产品体验，最后有了现在的产品。但在细节上我们还是做了很多尝试，例如 AI for Data 这个方向我们就有很多尝试和调整。</p><p></p><p>技术架构选型上，就说一个细节，相比数仓来说，湖仓架构能兼顾数据的开放性、引擎的多样性，同时又兼具数仓的高性能。它是面向未来企业需求设计的架构。云器 Lakehouse 就是湖仓一体的架构，也是现在的主流设计。</p><p></p><p>InfoQ：有用户会担心，既然云器不开源，那么企业选择云器后如何避免被绑定呢？</p><p></p><p>关涛：这是好问题，我们在设计上也特别关注这个方面。首先，云器的多云架构保证了企业调整基础设施的灵活性，用户去哪个云都可以用这个平台；其次，我们在开放性上做了额外的工作，计算不锁定，存储开放。用户存储的数据可以开放成开源的标准格式，用户也有权限访问这些数据，用户的其它引擎都可以直接来读这些数据，所以并不会有绑定。</p><p></p><p></p><h2>一体化趋势下，技术创新的关键是什么？</h2><p></p><p></p><p>关涛：我们认为流、批和交互这三个计算范式都不能替代对方，需要一个更新的计算范式去覆盖，所以我们提出了第四种计算范式叫做增量计算。同时，我们选择了 Shared-Everything 系统架构替代 shared-data，我们认为这样的一套技术可以实现一体化的基础目标。</p><p></p><p>InfoQ：现如今，一体化或者 Converged infrastructure 成为业界的共同趋势。您如何理解“一体化”的概念？理想状态的“一体化”应该是怎样的？</p><p></p><p>关涛：我们谈一体化主要是从数据平台和数据分析的视角上来谈的。数据分析平台可以分成批处理、流处理和交互分析这三类场景，分别对应大批量数据计算、数据新鲜度较高的流计算和非常高性能的交互分析。对此国内主流解决方案是组装式的，用三个引擎分别满足这三个不同场景的需求。这样的好处是每个引擎都可以在特定场景上做到极致，但带来的挑战也有很多。比如三个引擎的接口不统一，有的引擎是带存储的，有的引擎带元数据，导致数据和存储都不统一，分散在三个引擎的不同位置，导致数据管理非常复杂。数据要在多个引擎间同步，数据版本都很难对齐。这种组装式的架构叫 Lambda 架构，</p><p>这个架构的问题和挑战，也业界也都清楚。业界为了解决 Lambda 架构的问题做了很多尝试，流批一体就是一个尝试。整体来说，技术是向着一体化的方向不断发展。云器的 Single-Engine 就实现了离线、实时、交互、分析的统一，希望尽可能同时覆盖三大场景，让用户轻松使用。</p><p></p><p>InfoQ：Single-Engine 方案的独特之处是什么？</p><p></p><p>关涛：Single-Engine 背后实际是增量计算的计算范式。之前我们经过分析，认为流、批和交互这三个计算范式都不能替代对方，需要一个更新的计算范式去覆盖，所以我们提出了第四种计算范式叫做增量计算。基于这个范式打造出来的引擎就叫做 Single-Engine。计算侧，我们是用增量计算的计算范式实现的；存储侧是增量的湖仓存储。整体的系统架构上我们选择了 Shared-Everything 架构，比 shared-data 又进一步。我们认为这样的一套技术可以实现一体化的技术目标。</p><p></p><p>InfoQ：在将产品推向市场的过程中，云器收到过哪些来自客户对于产品的反馈？</p><p></p><p>关涛：我们的技术理念和方向，还是得到普遍的认可。比如一家国内知名的汽车厂商，在他们的流水线中我们真正做到了低成本的全能增量化。客户原来只有 10% 的场景是实时化的，我们把他所有的场景都做到实时化，还帮他降了 50% 的成本，真正解决了客户的问题。</p><p></p><p>另外就是，客户试用、盲测，他们的第一印象就是我们的性能表现很不错。</p><p></p><p>客户 Onboard 是一个比较复杂的过程，因为涉及基础架构升级。第一阶段是客户的试用、盲测需求，他们的第一印象就是我们的性能表现很不错。经过初始的展示环节，客户会走 POC 的流程，选他自己的场景来做测试。之后如果觉得不错，会挑选多个业务场景中的一个来试用。比如说我们有一个客户选了实时化的线路，第一阶段是先把实时化的部分业务迁移上来，离线部分和 AI 的部分排在后面，这种渐进式的替代过程是比较典型的。</p><p></p><p>InfoQ：面向未来，您觉得数据领域的重要趋势是什么？云器科技未来有怎样的规划？</p><p></p><p>关涛：从技术视角看，在数据分析领域，一体化是大趋势。从客户视角看，只要能更好地解决客户问题、提升效率、降低成本就能赢得市场。一体化是提升效率、降低成本的一个好手段。</p><p></p><p>另外数据和 AI 的一体化也是一个趋势。AI 如何与数据平台结合还有很多值得探索的方向。未来 AI 一定很重要，但它的具体形态现在还不能完全确定。未来有更多物料数据进入湖仓平台，云器这样的平台可以通过 AI 能力提供扩展性，支持更丰富的数据类型。</p><p></p><p>我们今天身处很好的时代，这是一个技术加速迭代的时代。我们期待和国内的平台从业者、客户一起，共同推动中国的原创数据技术向前发展。我们一起在全球打出中国数据平台的技术品牌知名度和影响力，这需要客户的支持，也需要更多同业者的互相扶持。</p>",
    "publish_time": "2023-08-02 15:08:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动系统部项目管理负责人申建，确认担任QCon北京卓越项目管理专题出品人",
    "url": "https://www.infoq.cn/article/NwC3ACdG5jo374Mf14SX",
    "summary": "<p>9 月 3 日 - 5 日，在 <a href=\"https://qcon.infoq.cn/202309/beijing?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0802&amp;utm_content=shenjian\">QCon 全球软件开发大会（北京站）</a>\"，字节跳动系统部项目管理负责人申建将担任「卓越项目管理」的专题出品人。在此次专题中，你将了解到卓越的项目管理是实现成功项目的关键，以及它能给团队、客户、企业带来的益处。</p><p></p><p>申建，字节跳动系统部项目管理负责人、火山引擎边缘云项目管理负责人、中国软件行业协会项目管理专委会专家；MBA（博士在读）、PMP、PgMP。他担任多家互联网企业 PMO 友情咨询顾问，拥有二十年互联网产品研发和项目管理经验；曾在阿里和字节内部多 BU 组建和管理 PMO 组织，推行原生项目管理框架下多方法体系解决方案，不断探索适合互联网 / 云计算行业下项目经理和 PMO 组织的公认价值模型。</p><p></p><p>相信申建的到来，可以帮助提升此专题的质量，让你了解到卓越的项目管理能够使项目管理效率更高、项目成本更低，实现预算内完成更多项目的目标，进而提高团队和客户的满意度，并帮助企业保持竞争优势，在企业的良好发展中发挥着不可或缺的作用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff5317f4690587861f128f7a1888dd27.jpeg\" /></p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-02 15:13:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中台之上：业务架构设计",
    "url": "https://www.infoq.cn/article/1yc8IjNAFAOBy8Tw6xjk",
    "summary": "<p></p><h2>作者序</h2><p></p><p>有人说，将来的企业都是科技公司，虽然就目前来讲，这句话还为时尚早，但是，很多传统行业已经被科技大大改变了。大家都知道 BATJ 是科技公司，其实星巴克也已经是科技公司了，在美国的星巴克门店，有将近 16%的收入来自手机客户端；星巴克自己的 App 有将近 1300 万的活跃用户，星巴克内部已经将网页、手机、社交媒体、网络营销、StarbucksCard 、电子商务、Wi-Fi、星巴克数字网络和新兴的店内消费技术等等，统一作为数字业务战略。今年在风口浪尖上的滴滴，2016 年时就已经日产生数据超过 50TB（相当于 5 万部电影），每天规划 90 亿次路径；2017 年据称全年累计提供出行服务 74.3 亿次。美团公司也是人工智能的玩家，只不过他们没搞个钢铁侠之类的，而是更接地气的、服务快递小哥的语音助手，支持骑手全程通过语音和系统进行沟通、确认，避免了手动操作，提高了效率和安全性。以派单为例，语音系统会说：“派单，从哪里到哪里，收到回复”，骑手只需要说：“收到”，系统就可以确认派单。到了用户家附近的时候，骑手亦可以通过语音关键词回复，直接拨打电话，从而避免了掏出手机这个动作。在电量过低的时候，系统会提醒骑手，骑行速度过快的时候也会提醒骑手放慢速度，到达顾客附近时，自动提示顾客的地址。这个语音助手不仅方便了小哥，也让快递过程更加安全，减少事故发生。</p><p></p><p>倒退回 15 年前，恐怕没有多少人真的相信零售、餐饮、出租车、外卖这些行业会跟科技如此紧密相关，甚至直接成为了科技企业，而他们用的技术已经是大多数普通业务人员无法理解的，这不仅仅指技术原理无法理解，连应用方式都无法理解，这是一个真实的“数字鸿沟”。其实技术人员也被这道鸿沟困扰，成天喊着找场景、找场景，说到底，没场景要么是这项技术无用，要么就是没法让业务人员真正理解，导致无法与业务结合。</p><p></p><p>大家都清楚地认识到了科技的力量，心里都明白要应对技术推动的跨界竞争，但是，要怎么做呢？高薪聘请一些技术人员？买买大厂的科技产品？这些也是需要的，但正如交给你一把狙击步枪，不代表你已经成为了一名合格的狙击手，你还需要自身的转变。这种转变才是最终促成数字化转型的关键。</p><p></p><p>转变当然不是要大家都去学技术，都当技术小能手，而是转变思维方式，架起一道跨越“数字鸿沟”的桥梁，我认为，这就是业务架构的核心作用。业务架构可以帮助业务人员整体化、结构化、模块化地思考问题，从业务和系统的整体视角，附带一些对技术的基础了解，如分层理念去认识业务和技术；也能够帮助技术人员理解、归纳业务人员的想法和目标，从而让业务和技术能够在同一个语境下，使用同一种“语言”工作。过去那种业务不用管技术怎么实现、技术听懂需求就够了的时代已经过去，以后是深度融合的时代，深度融合就代表互相深入理解，而这种理解需要首先从思维方式的转变开始，通过建立业务架构，让双方都向对方迈出一步，当然，这一步对业务人员的挑战更大，但科技是这个时代的特征，在一个信息化的时代，就得具备这个时代的思维方式，这是任何人也无法回避的问题。在构建业务架构的过程中，业务人员需要技术人员的大力协助来共同掌握这个工具，这就不仅是一个通向理解的过程，更是一个达成信任的过程。此外，我们也无法忽视一点，如果业务本身不能被很好的结构化、模块化，我们也很难做出一个具有良好架构的系统来，就算你是中台的拥趸、“死粉”，也无法解决这个问题。所以，培养业务人员的逻辑思维、架构意识，对于系统开发而言，只有好处，没有坏处。</p><p></p><p>可能有些技术人员还会觉得应该让业务人员只专注业务就好，但是，不妨想一想，业务人员和技术人员在现实中的比例，你会发现要是业务人员也能对技术的思维方式有所了解，那将会对技术的合理应用乃至创新产生多大的推动力。打个不恰当的比方，技术人员就好比茶商，你可能想象不到，有多少现代人的喝茶习惯、茶叶知识都是拜茶商所赐，客户对茶叶了解的越多反而兴趣越浓，更愿意尝试不同的茶叶、茶具、技法，很多消费者最终在知识上远超越一般的茶商，这就是大家常说的培养客户、与客户共同成长吧。</p><p></p><h3>目录</h3><p></p><p>为什么业务架构存在 20 多年，技术人员还觉得它有点虚？</p><p></p><p>战略和组织结构，业务架构设计中不应被忽视的关键因素</p><p></p><p>面对复杂的流程和数据，我们总结出了一个分析套路</p><p></p><p>业务架构和中台的难点，都是需要反复锤炼出标准模型</p><p></p><p>如何为一个商业银行设计业务架构？</p><p></p><p>不神秘但很麻烦的业务架构落地过程</p><p></p><p>企业级业务架构的实现需要不断沟通和调整</p><p></p><p>业务架构设计“笨重”，它能跟敏捷沾边吗？</p><p></p><p>企业级业务架构设计的“五难”</p><p><img src=\"https://static001.geekbang.org/infoq/df/df109e0480d7f404a70d96df1a9e6a33.png\" /></p><p>扫/码/下/载</p>",
    "publish_time": "2023-08-02 15:33:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "矢量数据库：企业数据与大语言模型的链接器",
    "url": "https://www.infoq.cn/article/lahoGSbPsr9gyDIcqVAX",
    "summary": "<p>随着ChatGPT的推出，通用人工智能的时代缓缓拉开序幕。我们第一次看到市场在追求人工智能开发者，而不是以往的开发者寻找市场。每一个企业都有大量的数据：私有的用户数据、自己积累的行业数据、产品数据、生产线数据、市场数据等等。这些数据都不在基础大语言模型的记忆里，如何有效地将这些数据利用起来，是政府和企业在迈向通用人工智能的发展道路上面临的重要课题。</p><p></p><p>我们可以将私有数据作为微调语料来让大语言模型记住新知识，这种方法虽然可以让大模型更贴近企业应用场景、更高效使用私有数据，但往往难度较大，另外企业数据涵盖了文本、图像、视频、时序、知识库等模态，接入单纯的大语言模型学习效果较差。</p><p></p><p>我们今天来聊聊另一种更常见的方案，通过矢量数据库提取相关数据，注入到用户prompt context（提示语境）里，给大语言模型提供充分的背景知识进行有效推理。如图一所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/352b128176d2ee3d1fa9b18010a850b8.png\" /></p><p></p><p>矢量数据库允许任何对象以矢量的形式表达成一组固定维度的数字，可以是一段技术文档，也可以是一幅产品配图。当用户的提示包含了相似语义的信息，我们就可以将提示编码成同样维度的矢量，通过矢量数据库查寻K-NearestNeighbor（近邻搜索）来获得相关的对象。Approximate Nearest Neighbor（近似近邻搜索）作为矢量数据库的核心技术之一，在过去的十年里获得了长足进步。它可以通过损失一定的准确度在高维空间里快速搜索近邻矢量，比如NGT算法可以在接近一千维的矢量空间达到万次查询，而准确度不低于99%。如图二所示，不同的算法展现了不同的妥协效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2ccb4eb768eeb54e39d37bfa7b53ad6d.png\" /></p><p></p><p>这种语义搜索的方法起源于大语言模型时代之前，起初是为了降低企业搜索的工程复杂度，提升搜索结果的相关性，因为矢量本身和神经网络高度契合，也成为大语言模型应用的标准配置。甚至出现如Memorizing Transformer 和 KNN-LM这样的架构将近邻搜索算法和大语言模型结合来成功构造快速external memory（外部记忆）。</p><p></p><p>但是这样的架构依然存在一个重要的问题：从用户的提示生成矢量，通过近邻搜索找到有关数据，这两方面的矢量相似度高并不一定代表语义的相关性也高，因为两方的矢量可能并不在同一语义空间。如果企业数据的语义空间和大语言模型有比较大的区别，图一所示的架构就可能无法有效地关联重要数据而降低了可用性。</p><p></p><p>这种语义空间差别在处理多模态数据时尤其明显，比如从文本到图像的对齐（如图三），从文本到知识图谱的对齐（如图四）。同时，图像、视频、知识图谱、文档等等都蕴含大量的信息，压缩到单一矢量大大损失颗粒度，从而降低了近邻搜索的有效性。</p><p></p><p>如果将这些对象碎片化处理，再由大语言模型进行整合，除了复杂的碎片化工程，这种方法大大提高了提示语境的长度要求。尽管大量的研究工作已经从计算效率上解决了语境长度的瓶颈，比如Linear Transformer，Reformer，到最近的LongNet，理论上1B的Token已经是可行的，但实际的效果却显示当前的大语言模型并不能很好地利用长语境来获得相关信息（如图五）。归根结底，将大量背景信息有效高效地投射到文本语义空间从而让后端的大语言模型可以更好发挥依然是目前应用开发的一大难点。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/5d/5d98e6457d98d7458cecf317095d1b36.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/27042576e93199b46de37de4ee0055ba.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a63ee182000bd2eb497fc0ea698c90dc.png\" /></p><p></p><p>语义空间的投射可以看作是一个alignment（对齐）任务。在粗颗粒度上，单一矢量的空间对齐可以通过学习投射矩阵来实现（如图六所示）。这个投射空间小，可以用较少的标注数据训练，从而大大提升搜索结果的相关性，也已经成为业界广泛使用的技术。</p><p></p><p>而细粒度的对齐工作依然是目前技术突破的焦点，从Perceiver IO，CLIP到BLIP2，我们也渐渐看到交叉注意力机制的通用对齐能力（如图三、四），特别是大规模的无监督学习半监督学习大大提升了对齐的泛化能力。把这些对齐算法和矢量数据库结合起来提供快速高效的细粒度对齐将会极大提升大语言模型应用的用户体验，也是值得我们期待的方向。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19693318db86451fd5648748c315e91e.png\" /></p><p></p><p>总体而言，通过矢量数据库将企业内部数据和大语言模型结合起来拥有广泛的应用场景，但技术挑战也仍然很大，我们今天讨论的这些技术点仅仅是诸多挑战中的一两个环节，还有很多没有触碰，后面有机会和大家继续探讨。</p><p></p><p>参考资料：</p><p><a href=\"https://github.com/erikbern/ann-benchmarks\">https://github.com/erikbern/ann-benchmarks</a>\"</p><p><a href=\"https://arxiv.org/pdf/1911.00172.pdf\">https://arxiv.org/pdf/1911.00172.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2307.02486.pdf\">https://arxiv.org/pdf/2307.02486.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2006.16236.pdf\">https://arxiv.org/pdf/2006.16236.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2307.02486.pdf\">https://arxiv.org/pdf/2307.02486.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2301.12597.pdf\">https://arxiv.org/pdf/2301.12597.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2307.03172.pdf\">https://arxiv.org/pdf/2307.03172.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2307.03172.pdf\">https://arxiv.org/pdf/2307.03172.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2307.03172.pdf\">https://arxiv.org/pdf/2307.03172.pdf</a>\"</p><p><a href=\"https://finetunerplus.jina.ai/%25E3%2580%2591%25E3%2580%2582\">https://finetunerplus.jina.ai/</a>\"</p><p><a href=\"https://github.com/krasserm/perceiver-io\">https://github.com/krasserm/perceiver-io</a>\"</p><p><a href=\"https://arxiv.org/pdf/2103.00020.pdf\">https://arxiv.org/pdf/2103.00020.pdf</a>\"</p><p><a href=\"https://arxiv.org/pdf/2301.12597.pdf\">https://arxiv.org/pdf/2301.12597.pdf</a>\"</p><p></p><h4>作者介绍</h4><p></p><p></p><p>缪旭，九章云极DataCanvas公司首席AI科学家。二十余年人工智能研究和管理经验，深耕人工智能的技术实现和应用，发表多篇学术文章，并拥有多项授权发明，专注将可推理可解释的人工智能、大模型、大规模实时机器学习、知识图谱等前沿AI技术加速应用于各行各业。</p>",
    "publish_time": "2023-08-02 17:00:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI已为GPT-5申请商标，这家AI巨头的下一步是通用人工智能？",
    "url": "https://www.infoq.cn/article/SsGgljWxPa8VsAIElksJ",
    "summary": "<p>&nbsp;8月1日，有外媒报道称，OpenAI已经为GPT-5申请了商标。商标律师 Josh Gerben于7月31日在Twitter上分享了这一消息。该<a href=\"https://uspto.report/TM/98089548\">申请</a>\"于2023年7月18日提交，目前正在处理中。</p><p>&nbsp;</p><p>多年以来，微软支持的OpenAI已先后发布多种语言模型系统，包括GPT-4（一种可支持文本与图像输入的多模态大模型）、DALL-E（可生成和编辑图像的AI模型）、Whisper（音频到文本模型）、Embedding、Moderation等等。</p><p></p><h2>OpenAI为GPT-5 申请商标</h2><p></p><p>&nbsp;</p><p>而根据这份近期刚提交的美国商标申请可以看出，OpenAI可能计划推出另一种大语言模型“GPT-5”。在这份新商标申请中，OpenAI将“GPT-5”描述为一种“用于使用语言模型的可下载计算机软件”。</p><p>&nbsp;</p><p>继 GPT-4 发布之后，它预计将成为 OpenAI 生成式聊天机器人的下一个强大版本。</p><p>&nbsp;</p><p>OpenAI此前曾在前几代模型（例如GPT-4和GPT-3.5）的商标申请中，使用过同样的“用于使用语言模型的可下载计算机软件”这一描述。但很遗憾，最新申请文件中透露的唯一关键细节就只有“GPT-5”字样，并不代表OpenAI会在今年年内发布新版本。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c8/c8af461838d83082bc430900e220d492.png\" /></p><p></p><p>&nbsp;美国专利商标局 (USPTO) 的屏幕截图</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e6de24d5ca52640517b3544a90a9f62.png\" /></p><p></p><p>&nbsp;我们还深入研究了OpenAI所说的GPT-5可能是什么。在申请文件中，OpenAI既提到“可下载的计算机程序和使用语言模型的软件”，也包括“人工生成人类语音和文本”的说法。</p><p>&nbsp;</p><p>文件强调其具有&nbsp;“自然语言处理、生成、理解和分析”等潜在功能，其他功能则包括基于机器学习的语言和语音处理、从一种语言到另一语言的文本及语音翻译，以及用于机器学习和预测分析共享数据集的软件。</p><p>&nbsp;</p><p>此外，它还囊括语音和语言识别软件、生成文本、开发与实现人工神经网络。这款软件似乎还偏重于能够对数据进行学习、分析、分类和响应的算法。</p><p>&nbsp;</p><p>虽然GPT-5的具体细节和改进尚未得到OpenAI的证实，但外界推测它是自然语言处理和人工智能能力的重大进步。</p><p>&nbsp;</p><p>一位名为Siqi Chen的开发者在Twitter上发文称，GPT-5 预计将在今年年底完成训练，并有可能实现通用人工智能（AGI）。AGI 旨在创建无需显式编程即可执行任何智力任务的智能系统。如果 GPT-5 实现 AGI，它可以显著提高生产力并自动执行复杂的认知任务。</p><p>&nbsp;</p><p>然而，对于通过 GPT 目前的方法实现 AGI 的可行性存在担忧和不同的看法。OpenAI 首席执行官 Sam Altman 此前表示，该公司目前没有训练 GPT-5，以解决技术专家对快速开发强大 AI 系统相关风险的担忧。</p><p>&nbsp;</p><p>尽管已申请商标，但尚未确认 GPT-5 会立即开发。虽然 OpenAI 很可能在未来制定高级语言模型的计划，但商标申请的主要目的可能是保护“GPT-5”名称并防止他人未经授权使用。</p><p>&nbsp;</p><p>OpenAI 有关 GPT-5 开发的进一步更新尚未得到确认。</p><p></p><h2>ChatGPT-5可能不会很快出现</h2><p></p><p>&nbsp;</p><p>在GPT-4爆火后，GPT-5成为了万众期待的下一个版本。</p><p>&nbsp;</p><p>然而，在埃隆·马斯克 (Elon Musk) 和史蒂夫·沃兹尼亚克 (Steve Wozniak) 等人发出暂停先进AI研发的联名信后，OpenAI CEO 山姆·奥尔特曼 (Sam Altman) 决定“一段时间内”不再培训 GPT-4 的下一个版本，他表示公司在开始构建新的大模型之前还有很多工作要做。</p><p>&nbsp;</p><p>Sam Altman的上述言论还是4月份的事。</p><p>&nbsp;</p><p>时间转到6月，Altman再次在公开场合表示OpenAI 尚未开始训练 GPT-5，只会专注于构建新想法。因此，GPT-5的具体功能和增强功能OpenAI官方并未对外公布。</p><p>&nbsp;</p><p>此外，OpenAI还于2022年12月向美国专利商标局申请了“GPT”商标。OpenAI于4月向美国专利商标局请愿，要求加快商标审批进程，因为大量以GPT命名的应用程序如雨后春笋般涌现。&nbsp;</p><p>&nbsp;</p><p>Carr &amp; Ferrell 知识产权团队合伙人 Jefferson Scher 向媒体透露，GPT-5商标的申请仍在审理中，可能还需要 4-5 个月才能获得批准。</p><p>&nbsp;</p><p>值得注意的是，OpenAI的ChatGPT已经在Bing Chat等基于语言模型的聊天机器人提供支持，从短期来看GPT-5大模型不会快速亮相。根据外媒的了解，OpenAI计划更多关注GPT-4模型，尝试通过插件、自定义指令和函数等工具进一步增强其功能。</p><p>&nbsp;</p><p>因此，此次商标申请也不足以确认相关产品就一定存在。公司通常都会为尚未实际开发的概念申请商标或归属关系，借此保持竞争领先或保护自身知识产权。或许，GPT-5只能代表GPT-4的改进或增强版本。</p><p>&nbsp;</p><p>在OpenAI发布关于该模型功能和技术细节的官方声明之前，我们无法断言申请文件中的GPT-5会是个什么样子。OpenAI和微软甚至很可能会在GPT-5或6上放弃做模型本体的换代，而更多通过改进和插件扩展现有模型的功能。</p><p></p><h2>OpenAI的目标是在2030年之前实现超级智能</h2><p></p><p>&nbsp;</p><p>其实，对于GPT-4下一代版本是什么样子外界多有猜测，其中呼声较高的一个观点是认为OpenAI的下一代大模型产品可能会实现通用人工智能。</p><p>&nbsp;</p><p>OpenAI 在最近的博客文章中提到，它正在组建一个由熟练的机器学习研究人员和工程师组成的团队，以应对调整超级人工智能的基础技术挑战，并在未来四年为此分配 20% 的计算资源。</p><p>&nbsp;</p><p>“虽然超级智能系统现在看起来还很遥远，但我们相信它可能在十年内实现，”博客中写道。</p><p>&nbsp;</p><p>该部门由OpenAI 联合创始人兼首席科学家Ilya Sutskever和联盟负责人Jan Leike共同领导，主要重点是在四年的时间内解决超级智能系统中的基本技术问题。该团队由之前的大模型校准团队的研究人员和工程师以及公司其他部门的专家组成。</p><p>&nbsp;</p><p>OpenAI打算广泛分享其工作成果，并将促进非 OpenAI 模型的一致性和安全性作为其使命之一。新团队的工作是持续关注并增强ChatGPT 等现有模型的安全性，并解决其他与人工智能相关的风险，包括滥用、经济破坏、虚假信息、偏见和歧视、成瘾和过度依赖等。虽然新团队的重点是与超级智能人工智能系统和认知智能相关的机器学习挑战，但他们积极与跨学科专家合作，以确保他们的技术解决方案涵盖更广泛的人类和社会问题。</p><p>&nbsp;</p><p>超级智能可以应对全球挑战，但也带来人类丧失权力或灭绝的风险。但遗憾的是，OpenAI表示，目前我们对于大模型安全性的控制方法还不够。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.windowslatest.com/2023/08/01/microsoft-backed-openai-files-trademark-for-chatgpt-5-but-it-doesnt-mean-anything/\">https://www.windowslatest.com/2023/08/01/microsoft-backed-openai-files-trademark-for-chatgpt-5-but-it-doesnt-mean-anything/</a>\"</p><p><a href=\"https://analyticsindiamag.com/openai-files-trademark-for-gpt-5/\">https://analyticsindiamag.com/openai-files-trademark-for-gpt-5/</a>\"</p><p><a href=\"https://analyticsindiamag.com/openai-aims-to-achieve-superintelligence-before-2030/\">https://analyticsindiamag.com/openai-aims-to-achieve-superintelligence-before-2030/</a>\"</p><p><a href=\"https://openai.com/blog/introducing-superalignment#JanLeike\">https://openai.com/blog/introducing-superalignment#JanLeike</a>\"</p>",
    "publish_time": "2023-08-02 17:07:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC如何掀起智能客服“新革命”？ | InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/4DmRyoNqdGymWtDRqUvF",
    "summary": "<p>ChatGPT 的诞生打响了现代 AI 军备竞赛的第一枪。以 GPT-4、ChatGTP、Bard 等为代表的大语言模型在全球各界引起了广泛关注。结合 ChatGPT 的底层技术逻辑，未来中短期内 ChatGPT 产业化的方向大致有四类：即智能客服、文字模态的 AIGC 应用、代码开发相关工作以及图像生成。其中，最适合直接落地的项目就是智能客服类的工作。</p><p>&nbsp;</p><p>基于大模型技术所构建的智能客服正在从根本上改变传统的人机交互过程，大模型自动生成对话流程让运营智能客服更高效，可以提升复杂缠绕问题解决率、人机交互感知程度，以及意图理解、流程构建、知识生成等运营内容的效率。</p><p>&nbsp;</p><p>如果单从产品渗透率层面来看，智能客服早在过去的七八年里就已经在电商、金融等等领域慢慢普及开来了。大模型带来的两个核心改变，一个是开发智能客服产品的成本大幅度下降，另一个就是用户体验的提升。</p><p>&nbsp;&nbsp;</p><p>那么，想要将 LLM 大语言模型与智能客服产品进行结合，或者将前者落地于 ToB SaaS 应用软件领域，该如何着手搭建技术栈？大模型产品将如何赋能智能客服产品？本期《极客有约》我们特别邀请了bothub 创始人，布奇托网络科技创始人兼 CTO 徐文浩担任主持人，与华院计算技术总监兼数字人事业部联合负责人贾皓文、中关村科金智能交互研发总监、中关村科金智能客服技术团队负责人王素文，京东云言犀KA产品负责人王超一同探讨 AIGC 在智能客服产品中的落地及未来发展趋势。</p><p>&nbsp;</p><p>以下为访谈实录整理。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：观众朋友们，大家好！欢迎来到 InfoQ《极客有约》。今天的主题是《天工刊物 AIGC》 特别策划。我们希望通过这个策划，让大家全面了解 AIGC 在智能客服领域的方方面面，深入感知这场变革。&nbsp;在本期节目中，我们邀请了三位嘉宾与大家讨论 AIGC 在智能客服领域的应用。整体上，我们将分为三个部分进行讨论。首先是 AIGC 大模型在智能客服产品中的落地应用；其次是智能客服中 AIGC 架构的部署和工具应用的设计与选择；最后是构建高质量对话系统的方法。&nbsp;今天的三位嘉宾都是在智能客服和智能交互产品领域有着丰富经验的专家。第一位嘉宾是京东云言犀KA产品负责人王超老师。第二位嘉宾是中关村科金智能交互研发总监王素文老师。第三位嘉宾是华院计算技术总监兼数字人事业部联合负责人贾皓文老师。&nbsp;我们先从第一个问题开始，我非常好奇：AIGC 的出现对智能客服带来了哪些变化？我想先请京东云的王老师来分享一下您的看法。在您的观察中，AIGC 的出现给智能客服带来了哪些革新？</blockquote><p></p><p>&nbsp;</p><p>王超：AIGC的出现引起了整个智能客服领域的广泛关注，并促使相关同行进行了大量的探索。对于智能客服的认知和未来的改变，这些认知变化在日新月异。</p><p>&nbsp;</p><p>京东云言犀团队一直密切关注国内外智能客服应用的进展。另外，我们正在研发的言犀大模型将于7月发布，同时我们也在持续进行客服业务中的各种大模型实验。近几个月以来，我个人对AIGC的理解和3个月前已经完全不同，所以今天我想分享的观点更多代表个人意见和当前的看法。</p><p>&nbsp;</p><p>AIGC对智能客服带来的影响可以从两个层面来看。首先，从我们行业常见的管理问题和技术难题的角度来看，AIGC具有解决的潜力。我们都知道，大模型对于智能客服的应答水平、拟人度和服务体验等方面都会带来巨大的提升，并且能够大幅降低运营成本。</p><p>&nbsp;</p><p>在机器人方面，我认为不需要过多展开讨论，因为我们已经将很多注意力放在机器人上了。我想说的是，大模型在广泛的智能客服领域中，特别是客服管理智能化方向上的验证信息。例如，我们在智能辅助方面的实践，以往的一些技术在一些关键点上的推荐和会话中关键信息的提取等问题，尽管有解决方法，但成本和效果通常难以取得很好的平衡。然而，通过大模型的验证，我们发现它在处理这些问题上有很好的解决能力和潜力。另外，对于质检工作来说，行业中普遍使用的关键词正则等方法或者智能质检方法，虽然有一定效果，但准确率往往较低，工作量也很大，提升准确率的周期较长。</p><p>&nbsp;</p><p>然而，通过大模型的实践，我们发现它在理解抽象质检标准和执行质检工作方面效果很好。此外，在员工培训方面，我们已经看到一些头部银行引入对话机器人进行培训的例子。除了以上所述，我们还在客服中心进行经营分析，需要总结客户咨询中的需求、客户画像以及风险等方面的信息，而在实验中，大模型的效果也非常好。我认为在泛智能客服领域中，大模型的应用潜力是巨大的。我们相信，大约在半年左右的时间内，市面上的主流产品将迎来一次重要的升级。</p><p>&nbsp;</p><p>基于目前对该行业和领域的理解，我们可以探讨更广泛的领域是否会发生变化。例如，它能给客服和客户服务带来哪些变化？我认为这个问题可能更具挑战性，而且在目前的阶段，没有人能够确定具体的变化。但我们相信，至少在某些方向上，例如主动服务方面，它将带来巨大的变化。举个例子，电商经常进行各种活动，而活动的宣传和解释工作通常不会落在客服中心。因为如果要用人工或传统的机器方式进行这种广泛的活动承接，都是非常困难的。但我们可以想象一个未来，通过引入大模型，通过基础信息的输入，我们的客服可以很好地解释许多活动，这代表着未来客户服务在主动转型和升级方面可能存在的潜力。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：我总结一下，实际上，AIGC的出现将智能客服领域中可以应用智能部分的范围扩大了。不仅仅是回答售后问题这样的传统智能领域，而是整个客服环节的各个方面都可以应用大模型，甚至可以延伸到营销领域。王素文老师，从您的视角看到了怎样的现象？</blockquote><p></p><p>&nbsp;</p><p>王素文：正如刚才提到的，智能客服领域的范围非常广泛。智能客服和机器人等技术实际上可以改变传统的人机交互过程。通过大模型的运用，特别是利用自动生成对话流程，可以使传统智能客服的运营更高效。在传统方式中，我们通常需要通过人工手动配置知识库等方式，但效果并不明显。然而，通过大模型的自动生成对话流程，可以直接提高解决复杂问题的能力和问题的直接回答率，这是一个显著的颠覆性影响。</p><p>&nbsp;</p><p>第二点是，大模型还能实现降低成本、提高效率的目标。从智能化的角度来看，人工成本一直相对较高，因为它需要人工辅助机器。通过大模型的应用，可以辅助提升知识库建设和运营的效率，从而实现显著的降本增效效果。</p><p>&nbsp;</p><p>第三点是，关于机器人的拟人度和用户体验。传统的机器人在这些方面常常不尽如人意。然而，大模型的出现使得对话更流畅，拟人化程度更高，更像人与人之间的交流。这是一个非常强大的颠覆性影响。</p><p>&nbsp;</p><p>当然，对于泛客服而言，包括质检、助手和陪练等方面，大模型也会带来相应的颠覆性影响，不同的产品会产生不同的影响。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：我注意到贾老师是数字人事业部担任联合负责人，从您的角度来看，如果将智能客服与数字人结合，是否会带来一些新的革新呢？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：在回答这个问题之前，我想先回到智能客服领域。智能客服的发展本质上可以追溯到人工智能的历史，它建立在语言的积累基础上。我还记得大约十年前，有几层楼都是客服人员，他们的大部分工作时间都耗费在用户问题整理上。当时，甚至连现在两位老师提到的知识库等工具也不是很完备。</p><p>&nbsp;</p><p>现在回顾一下，支付宝等客服产品从最初的知识库到后来的高级助手，逐步扩展，衍生出像Rasa框架这样的用于模拟多轮对话场景的工具。然而，与刚才两位老师所说的一样，所有这些对话过程在人格拟人化方面仍然有所欠缺。以前的所有客服类工具本质上都无法通过计算机行业标准测试。但是，当大模型出现后，尤其是像ChatGPT或小羊驼（Vicuna）等，它们具有一定程度的人格特质，尽管可能是10岁或11岁孩子的水平。在某种程度上，它们能够通过图灵测试。</p><p>&nbsp;</p><p>现在回到您之前提出的问题，将智能客服与数字人结合，会带来一些新的革新。在数字人领域的起初阶段，基于知识库和大模型的方式进行与人类的拟人化问答是无法实时完成的。然而，随着技术的扩展，我们已经能够实现拟人形象，并结合大型模型来模拟真实的场景。当我们将其应用于知识库、客服以及数字员工等场景时，数字人领域可能面临两个挑战。首先是数据的完备性，尽管ChatGPT等看起来很酷，但它们本质上是基于过去的历史数据生成的，类似于完形填空的生成。虽然这种方法能够提高效率并降低脚本撰写的成本，但生成内容的质量实际上是无法控制的。在数字人和智能客服领域等综合领域的产出内容中，我们无法完全确信生成的内容。因此，我们可能需要引入不同的验证和保障措施，以确保数字人或ChatGPT等所说的话更像是人类在说话，而不是胡言乱语。</p><p>&nbsp;</p><p>总结下，虽然现在大模型可能比较火，未来的前景也很广阔，但当下阶段它还是一个从 0 到1的状态。未来我们可能期望它长成参天大树，但是现在不管是对整个行业来说，还是大家对它的期望来说，还是要让它在一个比较好的土壤里面逐步成长，真正的能够给智能客服、给数字人等领域来带来效率的提升。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：我想深入探讨一下这个问题。我想问一下贾老师，根据您的观点，如果我们现在在智能客服领域引入大模型，是否会带来收益？我指的是就当前情况而言，不考虑两年或三年后的发展。就现在的角度来看，从您的客户或内部产品的角度来看，引入大模型是否会带来收益？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：是否引入大模型取决于具体的业务场景和用户需求。对于大型公司如蚂蚁金服或京东等拥有庞大而成熟的系统的情况，盲目引入大模型可能会增加额外成本。我们需要采取一些兼容性措施来规避AI生成内容带来的不确定性。 对于规模较小的公司来说，将大模型作为知识库的补充，并辅助一定程度的人工审核，可能是一种提高效率的好方法。它可以帮助智能客服更好地理解用户的语义，提升知识库的质量，甚至改善用户体验。 此外，对于认知智能等更高层次的应用，引入大模型可能有助于更好地理解和认知用户。在互联网行业中，有一个重要的概念叫做\"千人千面\"，这意味着每个客服都能够提供个性化的服务。因此，我们需要在辩证的角度来看待是否引入大模型，结合具体情况做出决策。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：贾老师的观点是对于大型公司需要小心谨慎地引入大模型，引入大模型可能会增加成本，增加各种风险，需要依靠原有的方法或模型作为兜底措施。那我想问下王超老师，京东云客服引入AIGC了吗？引入后会的收益如何？</blockquote><p></p><p>&nbsp;</p><p>王超：对于大型公司在C端产品上谨慎应用大模型是出于保障顾客体验和服务安全性的考虑。在面向消费者的业务中，保持谨慎是至关重要的。在这方面，验证和实验都是以非常谨慎的态度推进的。</p><p>&nbsp;</p><p>在面向B端或面向运营的领域，您们在实验和验证更加“勇敢”。我们在帮助运营搭建文案和脚本等方面引入大模型，这为运营解决创意和效率问题提供了很好的支持。此外，对于质检、辅助和培训等面向员工管理的方向也非常积极地投入。</p><p>&nbsp;</p><p></p><h2>大模型是“刚需”还是“跟风”？</h2><p></p><p>&nbsp;</p><p></p><blockquote>徐文浩：这个大概能够把普通员工的效率提升百分之多少？有没有测算过，或者有一些具体的数据。</blockquote><p></p><p>&nbsp;</p><p>王超：我们目前还没有对这个问题进行具体的测算，因为大模型的应用在不同的工作项目中表现各异。例如，在生成脚本和文案方面，它的速度可能会提升几倍。然而，在日常分析和质检搭建等方面，效果可能因情况而异。因此，在当前阶段，我们很难量化并得出一个准确的结论，但是我们确信，大模型的应用确实提高了效率。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：王素文老师，您这边有客户或产品上引入了 AIGC 吗？能看到具体的收益吗？</blockquote><p></p><p>&nbsp;</p><p>王素文：在我们的业务领域，主要面向ToB市场，我们服务各种不同领域的客户，例如金融和保险行业等。这些客户在创新方面有一些需求，包括降低成本和提高效率的动机。我们与客户进行了一些共创合作，并进行了验证。例如，我们为某个客户创建了营销助手，可以自动生成一些文案。传统上，每个员工的水平参差不齐，包括营销话术方面也缺乏标准化和统一性。通过我们的营销助手，首先可以帮助他们生成统一的文案，根据历史上的优秀经验进行复制。其次是降低成本和提高效率，他们不再需要花费太多时间进行培训、学习、记忆等工作。我们的大模型在行业中得到了广泛应用。</p><p>&nbsp;</p><p>我们还开发了电销机器人，可以直接回答一些问题。当然，我们必须考虑到合规性问题，包括遵守相关法律和保护数据安全。我们正在与信创院合作，致力于解决这些标准化问题。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：大家确实在努力尝试发挥大模型的能力。然而，目前仍然有许多具体问题需要解决，尤其是可控性和安全性方面的挑战。从智能客服的角度来看，电商行业是早期引入智能客服的行业，因为有着“618”和“双十一”等大型促销活动，这些活动期间的流量峰值非常巨大。&nbsp;在过去几年中，国内的服务提供商也为各种金融机构（如银行、保险和理财机构）开发了各种智能客服产品。在这种情况下，如果金融机构不使用智能客服，似乎就会落后。但是，银行客服等机构并没有像“618”和“双十一”那样的高峰期，那这些机构引入智能客服是出于刚需，还是出于“跟风”或危机感？如果是刚需，那么这种需求最初是从哪里产生的呢？</blockquote><p></p><p>&nbsp;</p><p>王超：在数字化浪潮的推动下，金融机构更容易实现智能客服的落地。在金融领域，智能客服已经相对广泛应用，并成为金融机构比较常见的产品之一。对于金融机构而言，智能客服的核心需求主要是降低成本和提高效率，它能够将人力资源从繁琐的工作中解放出来。特别是在一些高频问题的自动问答、通知和回访等业务场景中，智能客服可以节省人力资源，使其能够将精力集中在更专业、有创造性的工作上，而不是重复、频繁且低价值的工作上。让他们可以有资源投入到开发和维护高净值客户方面，更专注于执行更有价值的任务。此外，随着新一代的基础突破，如大模型的出现，智能客服也能够获得强大的自然语言生成能力，使其变得更加智能和高效。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：所以本身是个刚需，因为有大量的重复劳动要去降本增效。贾老师，你这边的金融领域客户或者其他领域是否在关注智能客服呢？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：除了金融领域，像保险、法律和健康等垂直领域也存在对大模型的需求。特别是在公司的视角下，对于生成和提取知识的需求越来越明显。当我们与银行或保险公司交谈或与法律工作者讨论问题时，他们提供的信息本质上都是事实性的内容。通过大模型，我们可以更好地识别用户问题的意图，并进行聚类或分流处理。</p><p>&nbsp;</p><p>如王素老师所说，金融、保险和法律等行业的效率提升潜力非常大，这是一个降低成本和提高效率的过程。从我个人的角度来看，这是一个刚需，特别是在效率方面。然而，这些行业可能不会采取过于激进的方式，因为与金融、保险和法律相关的业务都是敏感性很高的，需要具备很强的专业性。在这方面，又引出了另一个问题，即对大模型生成结果的成熟度评估。虽然像 GPT-4 等大模型在美国的一些专业考试中表现良好，但在中国，特别是在中文这样庞大而复杂的语义背景下，它是否能通过相应的考试仍然是个问题，这可能需要进一步的研究。在这方面，我们公司计划在7月份与浙江大学合作发布一个法律垂直领域的大模型，为这个特定领域提供更好的解决方案。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：法律领域的大模型和金融领域类似，对于生成的质量要求非常严格。在法律领域，一个微小的错误可能会对消费者的体验产生负面影响，甚至对商家（B端）造成损失，这对生成结果的准确性和可靠性要求更高。就这个问题，王超老师怎么看？</blockquote><p></p><p>&nbsp;</p><p>王超：我认为无论是电商还是金融行业，对智能客服的要求都非常严格。另外，对于电商智能客服来说，大促销期间和日常的咨询都非常可观。我们团队在京东的自营业务中，日常期咨询量占70%，高峰期90%。这70%的咨询量已经具有巨大的价值。我相信这种逻辑也适用银行等各种机构。我们也注意到一些银行的智能客服服务能力仍有不足。因此，我们需要思考如何提升智能客服的成熟度，并且它需要与企业或银行的发展阶段相适应。目前，许多银行正处于智能客服的初级阶段，主要集中在FAQ和简单的多轮对话构建上，而服务能力和用户体验可能还无法达到令人满意的水平。</p><p>&nbsp;</p><p>基于我们在电商领域的经验，我们认为银行智能客服需要进一步发展，特别是在运营体系方面需要大幅升级。例如，我们要求基层客服在服务标准和技巧方面接近人工水平，这对我们的运营体系变革具有重大影响。然而，在与银行合作的过程中，我们发现许多银行在智能客服建设中面临挑战。技术供应商与银行之间的合作更类似于乙方和甲方的关系，与我们的合作模式有所不同。银行在组织架构和人才培养方面与我们的要求仍存在差距。我们与许多银行合作伙伴进行了交流，发现一种普遍现象，即他们认同我们的发展方法论，但也感到困难。推动内部改革对于银行来说是复杂的，需要更多的努力。我相信大模型是一个机会，因为它可以降低智能客服的运营难度，简化组织架构，我对此充满期待。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：我相信大模型对于在座的各位以及从事智能客服和自然语言处理领域的大部分人来说都是一个巨大的机会。我们的观点相似，大家都认为大模型可以帮助我们实现更加拟人化和个性化的对话体验。然而，当面对向C端用户提供服务时，无论是电商还是银行，大家都会更加谨慎。我们都希望确保最终的输出结果是可控的，无论是涉及1万元存单还是1000元订单，这是一件需要认真对待的事情。&nbsp;刚才我们也谈到了另一个重要的话题，即用户体验。我们可以观察到上一代的智能客服和对话机器人，它们的回答都是预先编写好的模板，例如关于送货地址的回答通常是固定的，只是稍作改动。这种固定模板的回答与真实人工客服相比，存在一定的差距。现在大家都在努力提升对话机器人的拟人程度，让背后的智能客服更具情感，更能理解用户情感，并进行多模态的计算。在这方面，大家是否已经投入研发了呢？</blockquote><p></p><p>&nbsp;</p><p>王素文：我认为可以从三个方面来讨论：拟人化、人性化和个性化。</p><p>&nbsp;</p><p>首先是拟人化。在智能客服领域，拟人化一直是一个痛点，因为传统的机器人在这方面表现还有待改进。我们一直在探索如何构建拟人化的对话交互。这涉及到如何设计情景化的对话，如何拆解问题，如何继承上下文以及如何理解多轮对话。总体而言，我们希望机器人能够提供更加贴切、自然的对话和交互模式。</p><p>&nbsp;</p><p>其次是人性化的服务。在精准识别场景或意图的基础上，我们还需要在拟人化的基础上进一步提升服务。我认为多模态情感计算是实现这一目标的有效方法。例如，我们公司开发了虚拟数字人客服，它可以进行人机交互对话，并结合情感计算，通过视频、语音和文本等多媒体方式识别用户情感表达。这样一来，智能客服可以对用户做出相应的情感反馈，打造出具有情感理解和温度的人机交互，实现更人性化的服务。在情感计算方面，传统的方法有规则和机器学习两种模式。通过机器学习，我们可以训练模型自动学习情感状态，并达到分类的标准，从而更好地适应不同领域和语境。这样可以获得更优秀的情感表达效果，提供更人性化的服务。</p><p>&nbsp;</p><p>最后是个性化服务。我们需要根据用户的画像实现个性化服务，以实现“千人千面”的效果。例如，我们开发了用户洞察平台，通过用户的基本画像信息和历史对话过程中的洞察分析，可以对用户进行标记和画像积累。在后续对话中，我们可以根据用户的画像为其提供不同的对话流程、回复方式和推荐，从而实现更精准的个性化服务，提升用户和企业的满意度。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：王素文老师提供了许多宝贵的经验分享，对于从事智能客服工作的人来说，可以借鉴和模仿。京东作为一个用户量和商品量都非常大的平台，大家都希望在使用智能客服时有一种背后是真人的感觉。我想问一下王超老师，京东在实现“千人千面”的能力上，在研发和产品方面都做了哪些工作？是否有什么经验可以分享给大家？</blockquote><p></p><p>&nbsp;</p><p>王超：我们在提升体验和个性化服务方面做了很多投入。言犀团队开发的情感智能客服是业界首个大规模商用的情感智能客服。自2018年开始，我们在机器人应答能力中引入了情感识别和应答的能力。这项技术不仅应用于客服领域，还应用于质检和人员管理服务。</p><p>&nbsp;</p><p>回答这个问题涉及两个方面。第一个方向是技术方面，例如多轮对话等前沿技术，这些是当前智能客服技术的主要发展方向。另一个重要方向是运营，即如何通过与人工客服进行对标，进行精细场景拆解，分析人机差异，并通过监控和工具体系实现自动化的问题发现和人机服务差异对比。通过这样的方式，我们从整体体验和人群服务体验的大面差异分析，逐步实现精细化的人群服务体验分析。有了这样的体系，我们才能够持续优化整体服务体验，并最终实现像京东目前日常机器接待量达到70%、大促期间达到90%的机器服务覆盖。</p><p>&nbsp;</p><p>贾皓文：今晚的直播主题是关于数字人客服领域和大模型的理解。我们对传统客服的理解，无论是人工客服还是FAQ，都可以被视为低端智能客服。作为用户，我们期望客服能够胜任各种问题，并能够提供排忧解难的帮助。同时，如果客服能够展现拟人化的特点和提供个性化服务，那对于用户来说体验会更好。</p><p>&nbsp;</p><p>在大模型领域，拟人化和人格化非常重要。我们公司更偏向于认知智能和心理学的研究方向。我们关注如何快速获取用户的心理标签，并通过心理学的角度对用户进行判断，从而提供更好的服务。我们可以设想一个场景，例如漫威电影《钢铁侠》中的贾维斯。如果未来的大模型能够像一个助手一样，可以根据我们当天的心情和喜好给出最合适的答案和推荐，那将是一个理想的状态。</p><p>&nbsp;</p><p>目前，在大模型和数字人结合的研究中，我们还处于初步阶段。但随着学界在多模态领域的研究成果的出现，可能会有一些更好、成本更低的体验产品涌现出来。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：大家都提到了对情感性的追求，似乎大家都在朝着更接近真人的方向发展。在直播间的大部分观众都是从事技术工作的同学，我们希望能分享一些经验，关于如何构建一个高质量的对话系统，无论是智能客服还是售后服务或售前导购方面。在进入这个领域时应该从哪个方面开始着手呢？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：如果我们要构建一个高质量的对话系统，实际上涉及到了传统的互联网项目立项的问题。在这个过程中，我们需要考虑业务产品架构、技术架构的选型，以及产品的实际落地形态。同时，我们还需要考虑到许多大型企业或中型企业已经拥有许多现有的客服产品。如果我们想通过大模型提升这些现有客服产品的能力，可能需要采取比较保守的方式。例如，大模型可用作一个外部知识库的工具，用于提供知识输入。 回到刚才提到的架构方面，产品架构和工具选择都是重要的考虑因素。对于初始的切入点，可能涉及到关键字的标注系统和传统的正则表达式等工具。然而，对于这种范式的具体选择并没有一个通用的标准范例，因为它与每个业务的特点相关。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：从业者的角度来看，无论是智能客服公司还是智能客服的SaaS或云平台，都可以思考如何进一步改善对话系统，以提供更高质量的服务。尽管我们今天讨论了很多关于大模型的话题，但实际上当涉及到传统的智能客服时，无论是在银行还是电商等领域，消费者多多少少都会感觉到背后没有一个真人在提供服务。我们需要考虑如何提升用户体验，并投入更多努力来改善现状。</blockquote><p></p><p>&nbsp;</p><p>贾皓文：如果我们将问题范围缩小，关注于提供更高效和高质量的智能客服内容输出，那我们可以将大模型视为一种增强型对话服务。在智能客服产品中，用户期望遇到的是一个智能、善解人意、善于交流的机器人，同时希望回答的内容能够聚焦于特定业务范畴，例如客服营销等场景。 在这种情况下，我们可以通过将大模型的意图识别、对话流程和多轮对话能力与传统的FAQ等外部数据源结合起来。这意味着我们需要收敛整个语言处理过程，例如对访客的问题数据进行归纳和与用户问题的对比，甚至在用户提问的同时输入大量的私有化数据，以补充传统的智能机器人、语音机器人和内外部知识库的能力。通过这种结合，可以在短时间内显著提高用户体验的效率。 进一步地，我们可以考虑拟人化能力的提升，但对于那些希望升级对话系统能力的公司来说，挑战可能较高，因此建议慎重引入。作为一个切入点，将大模型视为外部知识输入的一部分可能是一种成本较小且快速切入的方式。</p><p>&nbsp;</p><p></p><h2>如何提高大模型的对话质量</h2><p></p><p>&nbsp;</p><p></p><blockquote>徐文浩：王素文老师，如果去做一个高质量的对话系统来改善现有情况，应该在哪些方面做研发投入呢？</blockquote><p></p><p>&nbsp;</p><p>王素文：我们可以考虑以下几点来提升对话系统的质量。</p><p>&nbsp;</p><p>1. 数据标注成本的降低：传统的数据标注方法需要大量的人工标注，这会带来时间和资源的成本。为了降低这种依赖性，我们可以研究如何利用大量的无标注数据进行无监督训练，从而减少对人工标注数据的需求。这样的方法可以提高数据获取的效率并降低成本。</p><p>&nbsp;</p><p>2. 泛化能力的提升：仅仅回答单一问题是不够的，对话系统需要具备一定的泛化能力，以适应不同的场景和用户需求。通过学习语言的多样性和规律，我们可以提升模型的泛化能力，使其能够应对更多的问题和情境。</p><p>&nbsp;</p><p>3. 对话模型的构建和选择：在选择对话模型时，我们需要考虑不同场景下的模型适用性。当前已有许多大模型可供选择，因此我们需要根据具体需求选择适合的模型，以达到更高的准确性和效果。</p><p>&nbsp;</p><p>4. 持续学习和优化：对话系统需要进行持续学习和优化，因为初始上线的模型效果并不完善。系统应具备自我迭代和自我优化的能力，通过不断使用和反馈，逐渐提高效果和性能。这种持续学习和优化的过程可以满足客户的需求，并使系统变得越来越智能和高效。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：王超老师，您对之前的讨论有什么观点补充吗？</blockquote><p></p><p>&nbsp;</p><p>王超：我觉得这个问题的关键在于从客户和业务方的角度以及我们作为平台产品开发设计者的角度来看，其实都指向了相同的目标。无论是从哪个角度来看，我们都需要关注业务的核心需求，以及智能客服系统在提供服务方面的期望。在不同的服务形态和模式下，产品的技术架构和引入的技术能力可能会有所不同，但整体而言并没有太大的差异。</p><p>&nbsp;</p><p>举个例子来说，对于一些业务方来说，他们可能希望机器人能够提供基本的问答和信息查询能力，这时我们可能只需要提供一些FAQ和一些简单的对话工具和算法模型就能解决需求。而对有些业务方来说，他们希望机器人具备代为办理业务和跟进业务的能力，甚至提供情景化的对话服务和全程护航。针对不同层次的客户需求，我们需要相应地设计产品和构建技术架构，引入相应的能力。 因此，我认为跟进业务方、帮助业务进行咨询、深入了解他们的业务是非常重要的。根据不同的客户需求，进行产品设计和技术架构的搭建，并引入相应的能力，以满足他们的需求。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：大家试下来哪个模型效果比较好？需要哪些必备的工具、应用，架构如何选型等？</blockquote><p></p><p>&nbsp;</p><p>王素文：在使用ChatGPT或类似的大模型时，可以按以下步骤进行应用和部署。</p><p>&nbsp;</p><p>1. 模型训练和调优：选择可商用的开源大模型，如智普ChatGLM、百川大模型等。根据自己的需求和业务，验证和测试模型的性能。收集领域相关的数据，并使用这些数据对开源大模型进行领域训练，也可以进行指令集合的半自动化生成。通过微调和筛选多轮对话数据，增强领域大模型的对话能力。确保模型在安全性方面满足要求，根据规范和价值观进行微调和后处理。</p><p>&nbsp;</p><p>2. 模型工程化和性能优化：针对生成式模型，考虑模型推理的速度、容量和压缩问题。如果模型太大，单卡无法容纳，可以考虑单机多卡或多机多卡的并行推理。对模型进行性能优化，包括压速、压缩和加速，以实现更好的性能。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：王素文老师，您有推荐的中文基础商用模型吗？</blockquote><p></p><p>&nbsp;</p><p>王素文：我相信每个人在选择模型时都会根据自己的需求和标准进行权衡。每个模型都有自己的特点和优势。在我们的业务需求中，我们测试了多个模型，最终选择了智普和百川这两个大模型，因为智普大模型在商业化方面已经有了一定的成熟度。他们最近发布了新的模型，这也说明他们在不断地优化和迭代。我相信随着这些模型的不断改进，基于这些大模型再进行领域模型的开发，将会带来更好的效果提升。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：在测试大模型方面，贾老师有什么推荐的工具架构，或特别关键的应用吗？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：对于中小型公司来说，从零开始自己开发或在现有模型上进行指令集调优可能会比较困难。模型通常很大，甚至在单个显卡或单台机器上都无法容纳。此外，收集结构化数据，特别是与特定业务领域相关的数据，也是非常关键的。因为在ChatGPT的原始训练过程中，做了大量的数据收集和整理，这就需要在指令集调优的过程中使用自己领域的数据来微调模型。这涉及到一些多机多卡的并行计算，可能需要算法和模型训练人员具备高水平的知识，例如张量加速措施和梯度累加措施等。</p><p>&nbsp;</p><p>在部署和运维模型的环节中，可能需要考虑模型训练的网速、硬盘选型（如Zata或SSD）、存储器以及数据传输加速工具等方面，对运维环境有较高要求。总的来说，当前大模型的训练过程可能会相对较难，但单纯的部署和推理过程来说，基于6B或13B这样的大模型，在V100上进行部署，基本上是可行的。</p><p>&nbsp;</p><p>如果模型调优训练完成后，将其部署到线上系统中，我们通常会考虑整个架构的升级。目前业界比较热门的是Milvus向量数据库，它可以通过向量检索将生成的结果进行中间缓存，类似我们平时使用的Redis缓存。因为完形填空生成机制的特性，虽然每次生成的具体样式可能会不同，但大致意思是相同的。为了降低线上的成本，我们可以采用这样的机制。同时，对于生成内容的审核系统、训练数据准备系统和标注系统等，也需要有一套完整的解决方案。</p><p>&nbsp;</p><p>总的来说，训练大模型并不一定可怕，但可能会对我们之前的技术栈要求有所提升，但这种提升也是可以跨越的，只是可能稍微有一点难度，但我们完全可以通过学习和实践来应对这些挑战。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：模型训练本身只是整个过程的一部分，周边配套措施也是至关重要的。例如向量数据库、缓存系统、标注系统等，这些配套工具和系统对于产品的持续迭代和发展至关重要。在研发过程中，我们需要一个完整的工具链和解决方案，以支持数据的收集、预处理、标注，以及模型的训练、优化和部署。王超老师，您是否有补充的内容？</blockquote><p></p><p>&nbsp;</p><p>王超：在这个问题上，我可以分享一些关于正在开发的大型模型的信息。我们正在开发自己的产业大模型，并期待与企业和同行们进行合作。在7月份之后会公布更多关于合作机会的信息。另外，提到如何验证哪些大型模型更好的问题。在这方面，我们更关注的是如何在平台上成功应用已经验证过的优秀大型模型，比如百川等模型，鼓励大家关注和了解它们。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：在研发效能方面，在开发和部署大型模型的团队规模、算力投入以及时间预估等方面大家有什么经验分享？</blockquote><p></p><p>&nbsp;</p><p>王素文：整个开发和部署大型模型的过程确实需要花费一定的时间。特别是在模型的压缩、加速和优化方面，需要进行反复的实验和调整，这可能是非常耗时的。举个例子，我们曾经开发了一个模型，它的规模达到了7B，经过优化后，在A800服务器上的4卡b型配置上，推理速度从之前的28毫秒降低到了大约5毫秒。总的人力投入取决于你所做的工作。</p><p>&nbsp;</p><p>首先，需要构建整个基础框架，并对模型进行压缩和量化处理，包括算子的优化。我们基于英伟达的FastarTransformer进行了优化，因此需要自定义优化算子，选择适合需求的推理引擎，比如英伟达的Triton，根据不同的后端提供服务。最后，还需要进行整体的性能测试，以确定模型在不同设备上的最佳性能，并进行最终的部署。根据我们的经验，整个适配过程至少需要一个月的时间。此外，调整指令任务的优化也需要一定的时间，这取决于具体业务需求和指令数量。根据不同的业务类型，通常需要一个十几个人的团队来做。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：听起来大概需要十来人的团队，一两个月才能把这些模型的推理优化和训练过程走完。这还不是我们说的那种大的几百、几千、上亿参数的大模型。</blockquote><p></p><p>&nbsp;</p><p>王素文：是的，在特定行业训练好并优化好的模型可以快速将其部署并复制给该行业的客户。通过这种方式，我们能够利用之前的工作成果，为客户提供定制化的解决方案。举例来说，我们在金融领域、保险、财富管理和零售等行业已经完成了模型的优化，因此我们可以将这些优化的模型快速复制，并迅速落地为客户提供服务。这种复用的能力可以极大地提高效率，加快解决方案的交付速度。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：明白，其实目前它还只是个产品研发的过程，不是个项目落地的过程，研发了一个产品，可以给很多很多客户去用。贾老师对这个问题有什么看法？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：对于领域大模型开发，数据收集对于特定领域的模型是必不可少的。对于高度专业化的领域（如法律），数据收集可能需要投入较长的时间，可能需要半个月甚至一个月。完成数据收集并进行结构化处理后才能开始后面环节，如指令集的调优和多级多卡的训练过程等。完成这些环节之后，通常会进行多轮的模型效果评估，因为通过Transformer机制生成的结果可能不够可靠，需要进行大量的效果测试来确保模型的可靠性。当模型训练基本完成，可以初步商用时，我们可能会根据客户的需求进行进一步的产品化工作，将其打包成一个完整的产品，为用户提供全面的服务。</p><p>&nbsp;</p><p>从成本的角度来看，数据整理可能需要半个月至一个月的时间，训练一个规模为6B或7B的相对较小的模型。然而，这还需要一个重要的前提条件，即负责模型训练的团队必须熟悉多机多卡的训练方式和方法，并熟悉各种数据加速策略和内存加速策略等。此外，环境的准备也很关键。对于一些小型公司，如果要进行大模型的训练，可能需要在阿里云或腾讯云等平台上租赁机器并自行搭建环境。这额外的成本也需要考虑进去。</p><p>&nbsp;</p><p></p><h2>开发一款大模型，投入产出比如何？</h2><p></p><p>&nbsp;</p><p></p><blockquote>徐文浩：综合两位老师的观点，如果团队条件成熟，开发一个7B规模的微缩版大模型可能至少需要一个十几人团队，开发三个月左右。这样来看，即使是开发一个小模型，投入也是相当大的。那开发大模型的投入产出比大致是怎样的？监管风险和安全性问题怎么来解决呢？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：要评估投入产出比，需要考虑具体的业务场景和需求。对于创意类脚本生成等传统文案、广告和影视行业，AIGC等大模型能够快速生成大量的素材，虽然可靠性可能有所欠缺，但可以显著提高生产效率，对于这些创意生产工作来说，投入产出比可能非常划算。然而，对于其他领域如法律知识生成、案例剖析、保单分析等，因为产出结果可能并非完美，需要投入大量人力资源，并经过多轮模型调优，才能达到较为理想的产出。因此，投入产出比可能会较高。</p><p>&nbsp;</p><p>至于监管方面，像最近出台的深度生成相关的监管政策，也需要我们考虑。主要涉及几个方面。首先，需要关注是否会侵犯作曲家、作家、画家等知识产权或版权，以及是否容易产生虚假信息。在生成结果的监管方面，我们需要确保配套的内容审查和管理机制，以确保生成的文本和图像不侵犯知识产权。同时，对于传统行业，合规和风险控制也是重要的考虑因素。随着实验的发展，大模型产业链可能会形成，其中一些人致力于生成大模型的生产资料，而其他人则致力于防止大模型生成失控的措施。这种对应关系的建立将经过时间的迭代，以便在法律法规和道德规范的框架下实现大模型的生成与监管的良好平衡。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：我们不只会有 AI 公司，还会有专门的 AI 安全公司，就跟互联网上有很多专门做安全公司一样。我想请教下王素文老师对AI安全和监管问题上的看法。</blockquote><p></p><p>&nbsp;</p><p>王素文：首先，我们在进行领域模型或大模型的训练时，数据的合规性和合法性至关重要，应该通过正规的渠道获取数据，并确保数据的安全和保密性。</p><p>&nbsp;</p><p>其次，当我们为客户进行领域模型训练时，我们必须确保企业内部数据的安全合规性，并且不同企业之间的数据应该进行隔离，不能随意复用或在训练中使用。此外，在为客户训练领域模型时，我们还需要进行微调和对齐，以确保输出的模型符合合规要求。</p><p>&nbsp;</p><p>从监管层面来看，政府部门在制定监管框架时需要借助跨学科和跨领域的专家知识。我们与信科院合作，共同建立健全的监管框架，并与其进行深度合作。我们也在与信科院进行安全认证，以确保我们的大模型经过了严格的测试和验证。</p><p>&nbsp;</p><p>只有通过政府、专家和企业的合作，我们才能推动AIGC的健康发展，满足用户需求，并确保大模型的安全使用。</p><p>&nbsp;</p><p></p><h2>在智能客服领域，企业的核心壁垒是什么？</h2><p></p><p>&nbsp;</p><p></p><blockquote>徐文浩：对于智能客服领域，大家都在做多轮对话，情感分析，那企业的核心壁垒是什么呢？</blockquote><p></p><p>&nbsp;</p><p>王超：对于智能客服行业而言，同质化问题与企业采购智能客服建设目标和预期的 ROI 密切相关。在京东，企业在建设智能客服时应将其发展目标分为初阶、中阶和高阶（或成熟）三个阶段。如果客户的目标仍处于初阶阶段，即仅需简单的问答和查询功能，那么采购智能客服的需求可能相似。在这种情况下，同质化程度可能较高。但如果客户的需求定位在更高级的目标，例如提供主动服务、全程跟进和全情景化服务等，那么就需要考虑智能客服供应商是否具备与之匹配的运营方法论和相关的完整运营工具体系。基于这个逻辑，我认为当前一代智能客服产品的核心壁垒之一是供应商是否具备复杂成熟的机器人项目经验和丰富的运营经验。</p><p>&nbsp;</p><p>另一个关键点是，智能客服公司除了提供相关产品和技术外，还能否提供长期规划和指导意见的运营方法论，以及相关的配套运营工具体系。同时，我们是否能够帮助客户建立人才梯队，并提供培训服务。在当前一代智能客服中，这些因素非常重要。</p><p>&nbsp;</p><p>至于未来，大模型将成为一个重要壁垒。对于你所提及的产品，能否将不同类型的大模型与原有产品能力有效融合，将是一个关键因素。同时，具备大模型开发能力也将成为一个独特的竞争优势。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：我相信每家公司都会觉得自己在智能客服或者类似的这个产品上有独到的优势。贾老师，就您公司产品来讲，它的壁垒体现在哪？</blockquote><p></p><p>&nbsp;</p><p>贾皓文：从传统客服到大模型客服，整个过程可以看作是一种竞争壁垒。虽然在算力、数据量方面，我们可能无法与大厂竞争，但在交叉学科领域，如心理学知识在大模型训练中的应用，以及对认知智能与大模型应用的交叉领域，我们可能具备先发优势。对于其他公司而言，也可以结合自身特点，在日益同质化的大模型服务和训练过程中脱颖而出。</p><p>&nbsp;</p><p>王素文：这个问题实际上归结到两个核心点：智能客服公司需要考虑如何盈利和提高毛利率。为了实现这一点，首先要关注两个方面。第一，你需要提供优质的智能客服服务，让客户满意，这样你的企业才能长久发展。因此，关注产品的效果至关重要，包括产品力和智能化效果的提升，以提高用户体验和满意度。第二，注重提高效率，考虑投入产出比问题，降低成本并提高项目的毛利率。项目交付和运营效率的提升是关键，要考虑产品满意度、部署实施的效率以及与客户业务系统的快速集成和运营内容的对接。你需要拥有完整的交付方法论和运营工具来提高项目的毛利率，以实现盈利并保持持续发展。</p><p>&nbsp;</p><p>智能客服公司可以分为两类，一类是垂直领域的专业厂商，另一类是通用型厂商。垂直领域的智能客服厂商专注于特定领域，例如电商或保险，他们的优势和壁垒在于行业聚焦，持续优化行业知识图谱和数据，提供特殊的解决方案和核心竞争力。中关村科金作为对话式AI解决方案提供商，我们专注于金融、政务、零售等多个行业，已为900多家行业领军企业提供服务，并积累了丰富的行业知识。我们还计划推出通用和领域大模型，通过整合对话引擎，升级智能客服、外呼机器人、陪练和质检助手等产品，以提升我们在行业中的竞争力。</p><p>&nbsp;</p><p>其次，提高交付和运营效率也是关键。产品满意度对于降低项目交付成本至关重要，而高效的部署和实施，以及与客户业务系统的快速集成和运营内容对接，将提高运营效率。你需要拥有一整套交付方法论和运营工具，确保项目的毛利率最大化。这样做将使你能够盈利并保持长期可持续发展。</p><p>&nbsp;</p><p></p><h2>AIGC 会完全替代传统的客服人员吗？</h2><p></p><p>&nbsp;</p><p></p><blockquote>徐文浩：三位老师提到了3个核心壁垒：首先是聚焦垂直领域，其次是在产品层面寻求差异化，第三是跨学科的设计。这些措施将帮助企业在竞争激烈的市场中脱颖而出，为客户提供独特的价值。那今天最后一个问题，请用简单的语言畅想下未来AIGC 在这个领域的发展， AIGC 会不会完全替代掉传统的客服人员？</blockquote><p></p><p>&nbsp;</p><p>王超：作为一个从业者，我对AIGC的前景持乐观态度，而取代的问题涉及不同的视角。一种观点是以存量市场的视角看待，认为客服行业的市场空间有限，因此AIGC可能会取代传统人力。然而，我更倾向于从增量的角度来考虑。</p><p>&nbsp;</p><p>首先，智能客服仍然需要人类运营支持，在从传统客服到智能客服的转变过程中，运营人员仍然扮演着重要的角色。其次，未来的运营模式可能会发生变化，智能客服与人工运营人员配合，形成一种新的作战模式。这种模式下，少数运营人员可以携带智能客服机器人，提供高质量的24小时服务，而且成本较低，这可以让更多的小微企业以新的方式提供客户服务，并扩大市场规模。总之，从增量的角度来看，智能客服不会完全取代传统客服，而是与其相辅相成，为市场带来新的机会和发展空间。</p><p>&nbsp;</p><p>王素文：在可预见的未来，人工客服不会被完全取代，因为它们在处理复杂问题、思考性问题以及情感问题上具有独特的优势。特别是在处理高价值客户、潜在客户以及提高客户转化率方面，人工客服仍然扮演着重要角色。由于获取客户的成本较高，通过人工客服进行高效跟进并确保成交仍然是许多企业所希望的。因此，人工客服和智能客服之间更多是一种相互结合的合作模式。企业需要根据自身情况考虑人工客服和智能客服的优势，并制定最佳的客户服务模式。</p><p>&nbsp;</p><p>在整体上，我认为AIGC未来的发展空间是广阔的，并且整个行业也已经看到了这一点。在未来的两三年中，AIGC和类似ChatGPT的技术将以高速发展，并推动整个企服产业的升级。互联网和企服行业都将经历大规模的升级和变革，包括配套设施的改进。目前AIGC仍存在一些问题，如内容质量、投资成本、数据安全和版权等方面。因此，我们仍然需要更长远的发展，包括探索更精密、更有效的建模方式，以改进这些问题。我相信随着技术的进步，大模型的发展空间将是无限广阔的。</p><p>&nbsp;</p><p>贾皓文：确实，我们不应过于强调替代性，而是关注未来将带来的工作模式和业务模式的变革。在推进业务的过程中，我们需要衡量投入产出比，特别是在客服推进中需要考虑用户数据隐私的安全性、法律法规的遵守以及跨场景、跨行业的拟人化服务。大模型能够为传统客服人员带来很高的价值，它们会带来质的变革，但并不意味着取代人工客服。总的来说，尽管大模型目前存在一些问题，但它在未来的前景非常广阔。用一句比较文艺的话来说，未来不久大模型的发展将从梦境变成现实，我们很快就能亲身体验到。</p><p>&nbsp;</p><p></p><blockquote>徐文浩：感谢三位老师的精彩分享，我们今天的直播就到这里了，感谢观众们的陪伴，我们下期再见。</blockquote><p></p><p>&nbsp;</p><p></p>",
    "publish_time": "2023-08-02 17:57:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]