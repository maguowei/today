[
  {
    "title": "Hugging Face 大语言模型优化技术",
    "url": "https://www.infoq.cn/article/dAjSEe0AZw1GHuXZDROZ",
    "summary": "<p>大语言模型的生产部署存在两个主要的挑战，一个是需要大量的参数，一个是需要处理非常长的用于表示上下文信息的输入序列。Hugging Face基于他们提供大模型服务的经验<a href=\"https://huggingface.co/blog/optimize-llm?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">分享了一些克服这些障碍的技术</a>\"。</p><p></p><p>Patrick von Platen在文中介绍的Hugging Face研究的三种技术是降低数值精度、使用一种叫作Flash Attention的注意力算法，以及使用专门的推理架构。</p><p></p><p>大语言模型需要大量的VRAM来加载，从几十(bigcode/starcoder)到数百GB (Llama、Bloom、GPT3)。第一个优化手段是从float32切换到bfloat16精度：</p><p></p><p></p><blockquote>现在几乎所有的模型都是基于bfloat16训练的，如果你的GPU支持bfloat16，就没有理由基于全float32精度运行模型。float32不会给出比训练模型所使用的精度更好的推理结果。</blockquote><p></p><p></p><p>这可以使总体内存消耗减少一半，但可惜的是，在许多情况下仍然需要很大的内存。一种更激进的方法是将模型权重量化为8位或4位，这<a href=\"https://arxiv.org/abs/2208.07339?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">已经被证明不会导致显著的性能下降</a>\"。</p><p></p><p></p><blockquote>量化对于文本生成来说特别有效，因为我们所关心的是选择最有可能的下一个标记集合，而不是下一个标记Logit分布的确切值。</blockquote><p></p><p></p><p>这将进一步减少所需的内存，使得在只有16GB VRAM的GPU上运行较小的模型成为可能，尽管代价是推理时间稍长。</p><p></p><p>von Platen写道，使用<a href=\"https://arxiv.org/abs/2205.14135?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">Flash Attention</a>\"是另一相关键的优化，它是大语言模型用来理解输入标记上下文关系的自注意力层的一种算法，有可能打破输入标记数量的二次增长。</p><p></p><p>因为该算法太过复杂，无法在这里描述，但可以这么说，它利用了softmax规范化统计数据和一些数学手段，在只需要随输入标记线性增长的内存的情况下提供相同的输出。推理性能也得益于算法使用了更快的SRAM而不是更慢的GPU VRAM。</p><p></p><p></p><blockquote>在实践中，目前绝对没有理由不使用Flash Attention。该算法在数学层面给出了相同的输出，并且速度更快，内存效率更高。</blockquote><p></p><p></p><p>Here recent research can help to make the right choice with two components that quickly become bottlenecks, says von Platen, _positional embeddings_ and the _key-value cache_.</p><p></p><p>在生产环境中部署大语言模型的第三项优化措施是选择正确的架构，让它们能够有效地处理长文本输入。von Platen写道，最近的研究有助于我们如何对两个很快成为瓶颈的组件做出选择——一个是_位置嵌入(positional embeddings)_，一个是_键值缓存_。</p><p></p><p>位置嵌入通过将每个标记的位置编码为数字表示来帮助语言大模型理解序列顺序。对于需要处理大型文本输入任务的大语言模型，应该使用<a href=\"https://arxiv.org/abs/2104.09864?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">RoPE</a>\"和<a href=\"https://arxiv.org/abs/2108.12409?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">ALiBi</a>\"等相对位置嵌入技术进行训练。</p><p></p><p></p><blockquote>RoPE和ALiBi位置编码都可以外推到训练期间未遇到过的输入长度，而事实证明，与RoPE相比，外推对于开箱即用的ALiBi的效果要好得多。</blockquote><p></p><p></p><p>目前的许多大语言模型中已经在使用这两种算法。</p><p></p><p>键值缓存可以作为对对话上下文进行编码的一种方法。键值缓存在发生每个新交互时增加一个元素，这比为每个请求编码/解码上下文的方法要有效得多。von Platen详细介绍了两类键值缓存，即<a href=\"https://arxiv.org/abs/1911.02150?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">Multi-Query-Attention (MQA)</a>\"和<a href=\"https://arxiv.org/abs/2305.13245?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">Grouped-Query-Attention(GQA)</a>\" 。</p><p></p><p>von Platen的文章所涵盖的内容不只有本文所概述的这些，他的文章中还提供了实际的例子来证明他的观点，所以请不要错过他的文章。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/09/hugging-face-optimizing-llms/\">https://www.infoq.com/news/2023/09/hugging-face-optimizing-llms/</a>\"</p>",
    "publish_time": "2023-10-07 10:22:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一小时12元，我在北欧监狱里训练AI",
    "url": "https://www.infoq.cn/article/4v7RrYDw8M0ECaa6TBc2",
    "summary": "<p>芬兰工资水平普遍较高，并且很少有人从事互联网行业。外媒&nbsp;wired 实地走访发现，一家名为 Metroc 的大模型创业公司发现了一种新型劳动力——囚犯。</p><p></p><h2>芬兰囚犯的新工作：帮创业公司训练大模型</h2><p></p><p>&nbsp;</p><p>在一个没有窗户的房间里，隔着一张消过毒的白色桌子，我被介绍给了一位四十多岁的女性，她有着方形下巴，用一个淡蓝色的发带把金色的头发扎成了马尾。她说：“大家都叫我果酱”，让我也这么称呼她。</p><p>&nbsp;</p><p>一个星期三的早晨，在这座芬兰的监狱里，果酱给我们演示了一种新型的监狱劳动形式。</p><p>&nbsp;</p><p>桌子上只有一小塑料瓶水和一台 HP 笔记本电脑。她们每三小时轮班一次，每小时可以获得 1.54 欧元（约合 12 元人民币）的报酬。这台笔记本电脑用来向果酱展示关于房地产的短文，并就她刚刚读到的内容问她是或否的问题。其中一个问题是：“上面这段话说的是房地产决策而不是申请，对吗？”</p><p>&nbsp;</p><p>“有点无聊，”果酱耸了耸肩，她也不太清楚这项任务的目的。她认为，\"也许她正在帮助创建一个客服聊天机器人\"。</p><p>&nbsp;</p><p>事实上，她正在训练一款由芬兰创业公司 Metroc 开发的大型语言模型。该公司创建了一个搜索引擎，旨在帮助建筑公司找到新批准的建设项目。为了做到这一点，Metroc 需要标注员帮助其模型理解新闻和市政文件中关于即将开展的建设项目的线索。例如，人工智能必须能够区分已经委托给建筑师或正在安装窗户的医院项目和可能仍在招人的项目。</p><p>&nbsp;</p><p>在全球范围内，有数百万所谓的“网络工作者”在训练人工智能模型，教机器区分行人和棕榈树，或者描述暴力或性侵害的词语组合。通常，这类工作人员来自南半球，因为那里的工资比较低。例如，OpenAI 就用了一家外包公司，该公司在肯尼亚、乌干达和印度招聘了网络工作者。这种安排非常适合美国公司，因为它们使用全球使用最广泛的语言英语，但在南半球很难找到讲芬兰语的人。</p><p>&nbsp;</p><p>这就是为什么 Metroc 转向了监狱劳动力。该公司获得了廉价的、会讲芬兰语的工人，而监狱系统则可以为囚犯提供就业机会，也为他们出狱后进入数字化领域工作做好准备。利用囚犯来训练人工智似乎有点像科技领域下游经常存在的对廉价劳动力的剥削。但在芬兰，这个项目得到了广泛的支持。</p><p>&nbsp;</p><p>“数据劳动力是一个全球性的概念。但如果你仔细观察一下就会发现，芬兰的情况截然不同。”来自赫尔辛基大学的研究员图卡·莱赫蒂尼米（Tuukka Lehtiniemi）说，他一直在研究芬兰监狱中的数据劳动力。</p><p>&nbsp;</p><p>果酱在哈米纳林纳监狱已经呆了四个月。这座现代化的建筑有着很大的窗户。空旷的走廊上，色彩丰富的艺术品正努力营造出愉快的氛围。要不是因为厚重的灰色安全门挡住了每个进出口，你很容易就会以为，这些房间属于一所毫无灵魂的大学。</p><p>&nbsp;</p><p>芬兰监狱的开放性是出了名的，囚犯可以在附近的城镇工作或学习，但哈米纳林纳监狱不属于这一类。相反，哈米纳林纳监狱是芬兰安全级别最高的监狱，只收容女性囚犯。果酱被判了六年。根据监狱的隐私规定，wired&nbsp;不能发布她的真实姓名、确切年龄或其他任何可能让人识别出她身份的信息。在这个无期徒刑囚犯服刑 12 年后就可以申请刑满释放的国家里，六年是重刑。和其他 100 名住在这里的囚犯一样，她也不被允许离开监狱。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04e874cfd11a928cf1522b07438b7a59.png\" /></p><p>&nbsp;</p><p>当果酱第一次来到监狱的时候，她会看着其他女囚每天早上起床去工作：她们可以自愿做清洁、洗衣或缝纫。每六小时轮班一次，她们可以获得大约 6 欧元（约合 46.6 元人民币）的报酬。但果酱无法忍受这些工作。“我会觉得非常累，”她说。为此，有很长一段时间，她就呆在牢房里，直到有一位监狱辅导员建议她尝试“人工智能工作”。三小时一轮班吸引了她，至于报酬，有总比没有强。“虽然不多，但比呆在牢房里强，”她说。截至目前，她只轮过三次班，但已经获得了成就感。</p><p>&nbsp;</p><p>这所监狱允许囚犯通过数据工作赚钱。在芬兰，这样的监狱只有三所。每所监狱都备有三台笔记本电脑，供囚犯参与这项人工智能工作时使用。这项工作没有具体的目标，囚犯按小时取酬，而不是按工作速度或质量。</p><p>&nbsp;</p><p>在哈米纳林纳监狱，大约有 20 名囚犯尝试过这项工作。监狱工作导师米娜·英基宁（Minna Inkinen）留着红色的短发，她坐在果酱旁边和我们交谈。她说：“有些人确实比其他人更喜欢人工智能工作。”当我在一个星期三的早晨到到达这所监狱时，缝纫室已经忙碌了起来。囚犯们或忙着操作缝纫机，或在织物旁商量事情。但在果酱到达之前，开展人工智能工作的小房间里空无一人。英基宁解释说：”总共只有三名囚犯自愿定期参加人工智能工作，而另外两人目前正在上法庭。“果酱补充说：“我更喜欢在一个团队中做事。”她房间的门一直敞开着，这样她就可以在回答问题的间隙，与隔壁正在缝纫的狱友聊天。</p><p>&nbsp;</p><p>那些问题是我在监狱以南 100 公里外的赫尔辛基的一家现代化共享办公室内手写的。在那里，我见到了个子高挑、少年感十足的 Metroc 创始人兼首席执行官尤西·维尔纳拉（Jussi Virnala）。他带着我路过一排室内秋千、一张台球桌和一群西装革履的男士，来到一个异常闷热的电话间。他解释说，这一周真让人兴奋，公司刚刚完成了一轮 200 万欧元（约合 1554 万元人民币）的融资，他计划用这笔钱来扩展北欧市场，投资者对公司与芬兰监狱的关系很感兴趣。他说：“每个人都激动不已，对这种创新方式很感兴趣，我认为从产品方面来看，这非常有价值。”</p><p></p><h2>数据标注是个好工作吗？</h2><p></p><p>&nbsp;</p><p>将囚犯发展为劳动力的想法是维尔纳拉提出的。他们公司需要母语为芬兰语的人来帮助他们改进其大型语言模型理解建筑行业特有的语言。但在像芬兰这样的高薪经济体中，很难找到这样的数据劳动力。芬兰的福利体系可以提供可观的失业救济金，这就意味着很少有芬兰人会主动在类似亚马逊网络交易平台这样的网络工作平台上注册。“上面没有多少芬兰语工作人员，”维尔纳拉说，同时他还补充道，“自动翻译工具仍然不能很好地处理芬兰语，毕竟以芬兰语为母语的人总共也才 500 万。”</p><p>&nbsp;</p><p>当维尔纳拉向芬兰监狱和青少年教养所的智能监狱项目负责人皮娅·普拉卡（Pia Puolakka）提出他的想法时，她立刻表现出了浓厚的兴趣。她说，在人工智能火起来之前，另一家名为 Vainu 的芬兰科技公司曾经也试过用囚犯做数据劳动力，但其联合创始人之间的分歧导致项目负责人图奥马斯·拉西拉（Tuomas Rasila）离开了公司，Vainu 也就退出了这个项目。</p><p>&nbsp;</p><p>到 2022 年维尔纳拉提出他的提议时，普拉卡非常想恢复人工智能工作。她的工作是设法加强芬兰监狱与互联网之间的联系，使监狱更接近日益数字化的外部世界。到目前为止，监狱的独立牢房一直都配有笔记本电脑，以便囚犯可以浏览有限的网站并申请视频通话许可。她认为，数据劳动力也是这项任务的一部分。</p><p>&nbsp;</p><p>这项工作的目的不是为了取代传统的监狱劳动力，比如制作道路标志或园艺工作，它的目标是为囚犯提供更多的工作类型。数据标注员三小时就轮一次班。“如果一天八小时都只做这种工作，可能会让人觉得很累，”她补充说，如果囚犯可以将数据标注与其他类型的监狱工作并行开展，那就更好了。她说，“这项工作是面向未来的，如果要为囚犯出狱后的生活做准备，那么这些技能至少与监狱提供的传统工作类型一样重要”。</p><p>&nbsp;</p><p>然而，数据标注可以为囚犯提供多少可用于出狱后的工作技能还不清楚。作为 Vainu 公司联合创始人之一的图奥马斯·拉西拉（Tuomas Rasila）曾在那里管理了一年的监狱项目，他承认自己没有这方面的证据。他说，这个项目的运行时间还不足以收集证据，“我认为，让可能与社会脱节的人去学习现代社会最先进的技术是一个不错的赋能理念。”</p><p>&nbsp;</p><p>其他人认为，这种新形式的监狱劳动力可能会加剧人工智能革命所带来的廉价劳动力问题。“我们正朝着一个更便捷高效的全自动化社会发展，但这往往掩盖了这样一个事实，即许多系统实际上都是依赖于人的”，来自人权观察的人工智能高级研究员阿莫斯·陶（Amos Toh）如是说。</p><p>&nbsp;</p><p>在陶看来，对于网络工作者需求的增加已经引发了一种趋势，即公司更多地转向了那些几乎没有其他选择的人群：难民、国家陷入经济危机的人，现在是囚犯。</p><p>&nbsp;</p><p>“这种情况很常见，”陶说，“我们这里看到的只是一个更广泛的现象的一部分，即企业正在将技术开发背后的工作外包给可能在剥削性工作条件下劳动的工人。”</p><p>&nbsp;</p><p>对于数据工作是否能帮助囚犯培养数字技能，陶还也是持怀疑态度。“在监狱里，囚犯有很多提升自己的方式，比如考取证书和参加高等教育，”他说，“但我觉得，以每小时一欧元的价格为一家公司标注数据未必能帮他们取得有意义的进步。”哈米纳林纳监狱确实为囚犯提供了人工智能在线课程，但当工作人员试图解释其好处的时候，果酱坐在那里，面无表情。</p><p>&nbsp;</p><p>在我与来自赫尔辛基大学的研究员莱赫蒂尼米见面后，我对于监狱项目的优点有些不那么确定了。从监狱来到 Metroc 的办公室，监狱里的女性干着每小时 1.54 欧元的工作，而公司正在庆祝 200 万欧元的融资轮，这感觉非常不协调。在赫尔辛基大教堂对面的一家咖啡馆里，莱赫蒂尼米耐心地听我描述了这种感觉。</p><p>&nbsp;</p><p>但对囚犯的采访让莱赫蒂尼米有了不同的看法——他对这个项目总的来说是持积极态度的。至于薪酬差距，他认为，这些人是在监狱里，并不是主流社会中的普通劳动力。“将我作为研究员所获得的报酬与囚犯在监狱里劳动所获得的报酬进行比较，是没有意义的，”他说，“我唯一听到的负面意见是这样的工作不够多，只有很少的人可以做。”他提到了每所监狱只有三台笔记本电脑这个限制。</p><p>&nbsp;</p><p>“当我们提起数据劳动力时，我们往往会想到网络交易平台，全球南部或美国农村的人，”他说。但对他来说，这是数据劳工的一个独特的本地版本，它带来了有益于社会的转变。与其他监狱劳动力相比，它为囚犯提供了认知刺激的工作，同时也代表了芬兰语言在人工智能革命中的地位。</p><p>&nbsp;</p><p>莱赫蒂尼米担心，如果没有这种主动性，英语之外的语言将被下一代技术所淘汰，智能音箱仍然难以理解芬兰语。“并非所有芬兰人都能说一口流利的英语，所以在当地进行的数据标注还是有必要的，”莱赫蒂尼米说。Metroc 并不是唯一一家被迫寻找芬兰数据劳动力的公司。2011 年，国家图书馆发明了一款游戏，以激励志愿者帮助他们数字化其归档资料。2020 年，广播公司 YLE 与赫尔辛基大学及国家发展公司 VAKE 合作，请求志愿者捐赠他们的芬兰语录音。</p><p>&nbsp;</p><p>在某种意义上，芬兰的监狱项目只是一个开始。有些人担心，这可能会开创一个先例：在监狱中引入更具争议的数据标签类型，比如弱化暴力内容。“即使目前在芬兰进行的数据标注没有争议，我们也必须考虑它所开创的先例，”陶说，“有什么能防止公司将有创伤性和不雅内容的数据标注外包给监狱中的人，尤其是如果他们认为那是一个待开发的劳动力资源？”</p><p>&nbsp;</p><p>芬兰的监狱以帮助犯人改过自新而闻名，不知道芬兰监狱里的劳动条件在其他司法没那么先进的国家是否同样适用。根据公民权利团体美国公民自由联盟（ACLU）的数据，76% 的囚犯说监狱劳动是强制性的。拉西拉说，“美国的监狱系统与芬兰或北欧国家有很大的不同，理念完全不同。在芬兰，人们会积极推动这个项目，因为每个人都知道这是自愿的。”</p><p>&nbsp;</p><p>人工智能公司需要的数据劳动力只会越来越多，为了跟上发展的步伐，它们就不得不寻找非同寻常的劳动力。随着 Metroc 规划扩展到北欧以及芬兰以外的语言，维尔纳拉正在考虑是否将监狱劳动力项目扩展到其他国家，她说“这是我们需要探索的事情”。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.wired.com/story/prisoners-training-ai-finland\">https://www.wired.com/story/prisoners-training-ai-finland</a>\"</p>",
    "publish_time": "2023-10-07 10:26:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阳光保险集团人工智能部大模型首席专家张晗确认出席 FCon ，分享大模型技术在保险行业的创新应用与未来发展",
    "url": "https://www.infoq.cn/article/1AT3vxwwWpMKeZt8pXNb",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。阳光保险集团人工智能部大模型首席专家张晗将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5560?utm_source=infoqweb&amp;utm_medium=article\">大模型技术在保险行业的创新应用与未来发展</a>\"》主题分享，介绍大模型技术以及它在保险行业中的具体应用、通用能力全员应用的发展和应用范围，并分析保险领域专业大模型的关键突破，以及智能理赔机器人在人伤赔偿模式上的革新应用。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5560?utm_source=infoqweb&amp;utm_medium=article\">张晗</a>\"，现任阳光保险集团人工智能部大模型首席专家，毕业于北京理工大学，曾就职于腾讯、美团等互联网公司，长期从事搜索推荐算法相关工作，2021 年加入阳光保险集团人工智能部，负责“知周”智能对话平台、正言大模型开放平台的研发建设。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型技术在保险行业的创新应用与未来发展</p><p></p><p>大模型技术正蓬勃发展，渗透至各行各业中，阳光保险也紧跟潮流，展开了一系列的创新探索和实践，并已取得了显著的效果提升。在本次演讲中，我将向大家详细介绍大模型技术以及它在保险行业中的具体应用，重点探讨阳光正言 GPT 战略工程与保险领域的紧密结合与其重要性，并分享通用能力全员应用的发展和应用范围。最终，我将深入分析保险领域专业大模型的关键突破，以及智能理赔机器人在人伤赔偿模式上的革新应用。</p><p></p><p>演讲提纲：</p><p></p><p>大模型技术带来的新机遇</p><p>○ 大模型技术简介 </p><p>○ 大模型技术在保险业的应用形势</p><p>阳光正言 GPT 战略工程与保险领域结合的重要性</p><p>○ 阳光正言 GPT 战略工程的重点规划 </p><p>○ 大模型底座的建设与重构科技能力 </p><p>○ 全面赋能保险业务的阳光 GPT 工程底座整体架构</p><p>通用能力全员应用的推进与应用范围</p><p>○ 文本生成的通用能力应用 </p><p>○ 文生图与寿险营销的应用实践 </p><p>○ 常青藤编程的全员应用探索</p><p>保险领域专业大模型的重点突破</p><p>○ 保险领域专业大模型的打造机器人产品生态 </p><p>○ 机器人独立完成寿险销售新范式 </p><p>○ AI 全流程独立销售车险产品的新模式 </p><p>○ 改写人伤赔偿模式的智能理赔机器人</p><p></p><p>你将获得：</p><p></p><p>○ 了解阳光正言大模型在保险领域的实践</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-10-07 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "高效能不等于开发快，大模型时代如何正确提升研发效能？",
    "url": "https://www.infoq.cn/article/pzC0XXGzSWJwPkIOwSlz",
    "summary": "<p>从最初的敏捷软件开发方法到DevOps成熟度模型，研发效能的发展历程经过多个阶段。如今，基于大模型的AIGC技术正在催生软件工程的新范式，为研发效能的提升带来新的可能性。目前，越来越多的企业开始在实际的研发工作中，结合大模型增强软件开发在设计、需求、测试、发布和运维等各个环节中的能力，提高质量和效率。</p><p>&nbsp;</p><p>在今年 9 月 3-5 日举办的&nbsp;<a href=\"https://qcon.infoq.cn/202309/beijing/\">QCon 全球软件开发大会·北京站</a>\"中，Thoughtworks 中国区总经理肖然担任《<a href=\"https://qcon.infoq.cn/202309/beijing/track/1567\">AIGC 浪潮下的研发效能提升</a>\"》专题出品人，该专题探讨了 AIGC 浪潮下，大模型对软件研发工作流的改变，以及大模型是如何提升研发效能和质量的。以下为 InfoQ 与肖然的对话实录，经编辑。</p><p></p><h2>大模型时代下的研发效能提升</h2><p></p><p>&nbsp;</p><p>InfoQ：您作为 2023 QCon 全球软件开发大会·北京站《AIGC 浪潮下的研发效能提升》专题出品人，能分享下您对这个主题的理解吗？对于这波大模型结合软件开发应用热潮，您观察到哪些有趣的趋势？</p><p>&nbsp;</p><p>肖然：ChatGPT一经推出，全球最活跃的是很多技术自媒体，给大家展示自动化的代码生成，一些实例甚至直接生成可运行的小应用和游戏。目前最成功的GPT应用是GitHub为开发人员推出的Copilot，甚至于这个单词成了系列AI应用的代名词。所以说，大模型在软件研发中的应用实际上已经开始了。</p><p>&nbsp;</p><p>软件工程领域有一个大家比较认可的定义，即软件开发是人类历史上最复杂的脑力协作。“脑力”给我们带来了工作量度量的麻烦，我们没法像控制肢体运动一样控制思考；“协作”当然也不是免费的，要让另外一个人按照你的想法来做事情，沟通和理解的成本很高。由此也造成长久以来软件研发效能管理上的巨大黑洞。</p><p>&nbsp;</p><p>这两年由于数字化的深化，整个社会全产业对于软件的依赖性提升很快，客观上就推动了软件研发团队和组织的快速扩大。人多了自然效能管理就更重要了，但软件本身的“人月神话”等悖论，明确告诉我们用传统方式一定是越管越慢。虽然类似DevOps、CloudNative这些运动在向着正确的方向推动我们对效能度量和效能管理的认知，但实际上我们还是缺乏一些本质上的治理手段。</p><p>&nbsp;</p><p>所以当ChatGPT出现后，在不同的技术社区就开始发酵，大家看到了这一波基于大模型的AIGC技术带来的可能性。我们以前重来没有思考过的效能提升视角也逐渐浮现出来，比如研发团队不同专业之间的知识管理问题，之前我们还是在不停地鼓励和训练大家换位思考、高效沟通，现在出现了一个可以包容各类专业知识的大模型，这个“超级队员”之前是不存在的。而这个超级队员的出现，必然会给我们带来新的效能提升思路和方式。就《AIGC 浪潮下的研发效能提升》这个专题，是值得我们接下来几年持续研讨的，也会是研发效能治理领域最热门的一个赛道。</p><p>&nbsp;</p><p>InfoQ：有观点认为研发效能已经成为一家科技公司的核心竞争力，您是否认同？根据您的行业观察，这些年来企业的研发效能发生了哪些变化？</p><p>&nbsp;</p><p>肖然：首先我们还是要明确研发效能的定义，目前也不是完全统一。比较好的定义可以参考类似DORA这样的全球报告，国内也有一些专家小组做了比较好的定义。总体上我们应该避免“高效能就是开发快”这样一个认知误区。当然，这些年在效能领域的一个显著变化是大家认知更透彻了，很多企业还结合自身的业务特点在看待研发效能，比如银行业监管机构都提出了“双模”，即两种研发节奏，明确不是所有系统开发都追求快，要适配业务模式。</p><p>&nbsp;</p><p>在正确的效能定义的前提下，确实研发效能高是一家科技公司的核心竞争力。本质上未来的很多公司都是科技公司，因为业务在大面积的数字化，由此也带来了很多公司不断提升自身的科技人员占比。从这一点出发，这些年来企业在研发效能治理上投入是逐年增加的。很多企业抓住DevOps这个切入点，开始系统性的看待研发效能问题，从端到端的价值流视角来建立分析和改进体系。这点从行业角度看是可喜的。</p><p>&nbsp;</p><p>当然我们也存在比较大的管理效能指标的问题。很多企业管理着希望能够“看见”，所以开始建立效能方向的指标体系。但这种通过指标体系来管理的方式也容易走上治标不治本的道路，软件开发过程中处理的复杂度很难通过指标来说明问题。本质上研发团队和专业人员的能力提升才是核心，不要因为建立了指标反而忽视了效能治理的关键命题。目前看大模型的出现也并不能替代研发专业人员，而未来的应用和系统因为大模型的加入会变得更加复杂。</p><p>&nbsp;</p><p>InfoQ：过去大家提到研发效能一个比较头疼的点是，如何正确、有效的度量，结合 AI 技术，研发效能度量发生了哪些变化？AI 大模型在研发效能提升方面还有哪些独特的优势和潜力？</p><p>&nbsp;</p><p>肖然：度量在过去几年有比较大突破，特别是DORA经过研究发布了DevOps领域的4KM（四个关键指标）。软件研发的度量关键是尽量面向端到端的价值流，设计指标时关注协同效率，而不是单兵的生产效率。</p><p>&nbsp;</p><p>大模型这个“超级队员”的加入，实际上让我们更加容易进行端到端的度量，很多协同工作是可以通过大模型来自动完成的，自然就形成了更为完整的过程数据。目前应用大模型上比较火热的是AI Agent，我们可以预期未来针对效能度量和分析都会有相关的Agent出现。</p><p></p><h2>Thoughtworks是如何采用大模型技术的？</h2><p></p><p>&nbsp;</p><p>InfoQ：Thoughtworks 围绕大语言模型结合软件开发有哪些探索？您能分享 1-2 个具体的案例，以及你们在其中的思考/踩坑经验吗？</p><p>&nbsp;</p><p>肖然：首先肯定是类似Copilot这样工具在开发过程中的应用。由于TW崇尚结对编程的实践，所以Copliot的接受度很高。这种模式其他研发角色如BA和QA都会用自己的工具尝试，总体上我们认为是现有专业工作的增强，效果是明显的，比如QA在准备测试用例时采用大模型来自动准备，仅测试用例的数据准备就能够从半天缩短到十几分钟。</p><p>&nbsp;</p><p>另外一个类型的尝试就是关注专业角色的协同，比如我们开源的Boba平台（<a href=\"https://martinfowler.com/articles/building-boba.html%25EF%25BC%2589%25EF%25BC%258C%25E5%25B0%25B1%25E6%2598%25AF%25E6%2595%25B4%25E5%2590%2588%25E4%25BA%2586%25E5%25A4%259A%25E4%25B8%25AA%25E7%259B%25B8%25E5%2585%25B3%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BA%2594%25E7%2594%25A8%25EF%25BC%258C%25E5%25AE%258C%25E6%2588%2590%25E4%25BB%258E%25E5%25B8%2582%25E5%259C%25BA%25E7%25A0%2594%25E7%25A9%25B6%25E5%2588%25B0%25E9%259C%2580%25E6%25B1%2582%25E6%258B%2586%25E5%2588%2586%25E7%259A%2584%25E4%25BA%25A7%25E5%2593%2581%25E8%25AE%25BE%25E8%25AE%25A1%25E5%2585%25A8%25E8%25BF%2587%25E7%25A8%258B%25E3%2580%2582\">https://martinfowler.com/articles/building-boba.html），就是整合了多个相关大模型应用，完成从市场研究到需求拆分的产品设计全过程。</a>\"这种尝试目前还没有专业增强那么成熟，更多起到了“help me to learn”的效果。但我们认为这个方向在研发领域是潜力无限的。</p><p>&nbsp;</p><p>踩坑主要还是数据和信息安全问题，为了得到更为准确的生成结果，不可避免我们需要提供更多的上下文给大模型。目前类似OpenAI这样的大模型厂商并不能提供企业级数据和信息安全的保障，所以往往一些核心业务系统仍然难于使用。随着越来越多的开源模型发布，我们也帮助不少企业开始部署和微调私有的大模型。私有大模型一般会采用企业自己的系统作为语料去微调模型，在采用了类似Llama 2这样的基础模型之后，生成代码的可用度已经能够达到50%以上。</p><p>&nbsp;</p><p>也有企业从成本和风险角度考虑，希望仍然采用公有大模型，这个时候我们就需要针对企业数据进行脱敏处理，并且建立相关的矢量存储来作为企业私有知识的管理。目前相关的架构在逐步稳定，开源的工具也越来越多。</p><p>&nbsp;</p><p>InfoQ：当前 AI 研发效能提升的技术瓶颈和挑战是什么？如何评估 AI 研发效能提升技术的性能和效果？</p><p>&nbsp;</p><p>肖然：目前最大的挑战实际不在于大模型本身，而在于人员能力的提升。大部分研发人员开始时都是单一问题的0-shot prompt，不能够很好的和模型互动，由此得到的生成结果也不尽如人意。</p><p>&nbsp;</p><p>另外一个挑战就是很多企业认为只要有了大模型，每个人跟模型提问互动就是应用了。曾经在Hadoop兴起的大数据时代，很多企业也认为只要部署了Hadoop，就是拥有了大数据能力。显然如果希望大模型在企业里变得普适可用，有很多工程化和平台化的基础工作是要预先设计和部署的。</p><p>&nbsp;</p><p>目前其实还不适合去做评估，可以进行一些数据的采集，反应大家真实的使用感受。评估很可能带来的副作用是大家不愿意真实的反应实际情况。比如一个季度前，我在内部和BA社区开会研讨，很奇怪为什么使用反馈很少。线下找到老同事了解，才知道原来大家担心公司管理层知道了使用大模型提升效率，造成后续更多派活或直接减员。显然这个担心是多余的，但评估可能传递这样的错误信号。</p><p>&nbsp;</p><p>就当前阶段，我的建议还是在条件允许的情况下，鼓励大家多使用、多尝试，欢迎大家提炼总结新问题和新方法。</p><p>&nbsp;</p><p>InfoQ：目前市面上存在很多结合大模型的研发效能工具，但在一些企业的端到端落地过程中并不理想，也没有实现提效的突破，这背后存在哪些挑战？在不同场景下，如何选择和调整 AI 研发效能提升技术来满足不同的需求？</p><p>&nbsp;</p><p>肖然：首先还是需要明确采纳的方向和目的，如分享TW采用大模型经验，我们会从“专业增强”和“协同增效”两个主要方向去考虑大模型的应用：</p><p>&nbsp;</p><p>专业增强实际上在开发、测试和UI等领域已经有比较成熟的工具。值得关注的是需求方面的工具，潜力是大家共识的，但工具方面还有待创新。协同增效方面类似LangChain这样的工具已经被不少研发组织采用，当然大模型本身的生成内容准确度还是决定性因素。生成质量不高的内容很可能适得其反，提高了协同成本。当然LangChain仅仅是一个开始，目前的很多Agent已经在自动化地完成一些跨职能的协同工作。</p><p>&nbsp;</p><p>基于这两个方向，可以考虑不同的具体场景，场景选择上要结合研发组织自身的特点。比较好的方式是举办内部的创新应用比赛/黑客松，利用这样的形式让更多的人来一起想、一起实验。由于大模型技术本身仍然在快速迭代，依靠自上而下地规划反而容易造成应用不接地气，难有真正成果。</p><p></p><h2>大模型最大的价值是知识管理</h2><p></p><p>&nbsp;</p><p>InfoQ：您认为大模型在软件研发工作流中最大的价值是什么？大模型对软件研发工作流的改变，将会如何影响软件开发行业的未来发展趋势？</p><p>&nbsp;</p><p>肖然：软件研发本身是隐式知识的显式化过程，通俗讲就是用户开始说不清楚要什么，之后通过产品一轮又一轮的迭代慢慢清晰。从这点出发，我认为大模型在软件研发过程中最大价值是知识管理，因为这个“超级队员”的知识存储能力超过了任何人类个体和团队。</p><p>&nbsp;</p><p>一旦大模型真正有效成为了知识的管理员，我们软件研发的专业分工就要发生变化。这种变化还不仅仅是我们现在可以看到的“全栈工程师的复兴”，而是真正意义上的角色重新定义。当然这并不意味着我们专业人员变少了，相反新的专业分工可能出现，比如维护大模型的工程师、测评大模型的分析师等等。</p><p>&nbsp;</p><p>我们已经无法预测未来的发展趋势，但我想在开放的心态下，我们应该躬身入局，建立自己的感知网络，从而能够持续进化。</p><p>&nbsp;</p><p>InfoQ：大模型会对程序员带来哪些冲击？程序员如何和大模型更好地共生？</p><p>&nbsp;</p><p>肖然：程序员需要更加关注原则和设计。大模型自动生成代码和应用只会日趋完善，但生成的质量仍然是需要程序员来判断，一些关键问题，如性能和安全，更是需要程序员来负责。所以程序员需要更多思考一些原则和本质的东西，这样才能支持有效的判断。</p><p>&nbsp;</p><p>大模型是生成式的AI，生成内容的质量很大程度取决于问题的质量，也就是我们现在经常谈的prompt engineering（提示语工程）。目前很多模式正在被提炼和总结出来，每个程序员都应该持续学习。但即使有了问题的模式，问题的内容仍然是程序员个体决定的。这就像使用TDD的高手，面对复杂需求总能够庖丁解牛一般找到合理的任务拆分。同理，能够通过prompt一步步引导大模型生成高质量的内容本身就是一项关键能力，而这个能力跟程序员的设计思考是密切相关的。只有善于设计的人，才能够和大模型进行有效的互动。</p><p>&nbsp;</p><p>InfoQ：AIGC 的未来发展和趋势是什么？您认为未来 AIGC 技术会对研发效能提升带来哪些新的机遇和挑战？</p><p>&nbsp;</p><p>肖然：在软件研发上下文下，我觉得AIGC最重要的发展趋势是多模态MultiModal，即听说读写样样精通。结合前面提到的知识管理，研发效能的提升将很快突破单个专业的提效，产生整体质的飞跃。想象未来客户描绘了一个场景，大模型帮助下快速转换为视频故事，在做产品前就能够让客户有身临其境的感觉，同时也可以通过高度的可视化让团队快速共识理解。</p><p>&nbsp;</p><p>这样的可能性在即将到来的多模态时代应该说潜力无限。软件研发对于每个从业者来说最重要的还是持续提供可学习的知识。而通过多模态，我们专业个体的学习能力也会被千百倍的放大。作为一个专业研发人员，我也很期待将大模型多模态的能力应用到我们的研发过程中去。</p><p></p><h4>采访嘉宾</h4><p></p><p></p><p>肖然，Thoughtworks 中国区总经理，中关村智联创新联盟秘书⻓。在过去 10 年时间里，肖然带队先后为金融、保险、通信、物流、零售等核心产业的头 部企业提供了⻓期的从战略执行到组织运营各个方面的咨询服务，以务实的工作作⻛得到了行业内的广泛认可，也成为了中行、招行、华为等头部企业的高管参谋，为企业的⻓期发展出谋划策。</p>",
    "publish_time": "2023-10-07 11:57:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "微软裁员内幕",
    "url": "https://www.infoq.cn/article/m20ESsvlMYRlTsuwsHyi",
    "summary": "<p></p><p></p><p>编译 | 核子可乐、Tina</p><p></p><p>微软曾被称为“养老大厂”，但就是这样的大厂，也没有躲过硅谷的裁员寒潮。今年 1 月和 7 月，微软总共进行了两次大规模裁员，总计估计约 2 万人。</p><p></p><p>微软近十年来的发展历史中，这样的规模是前所未有的。微软首席执行官萨蒂亚·纳德拉将裁员归结为“考虑到可能出现的经济衰退风险，因此公司需要优化支出”。然而，9 月 16 日，一位经证实的微软员工在 Blind 上分享了一篇长文，文中详细阐述了这次裁员的主要原因，包括管理层错误的判断导致过度招聘、对 GPT 的投资，以及收入下降和未达预期。总之，他认为微软的领导层更应该为此负责。</p><p></p><p>他在文章中爆料称，微软裁员的决定早在 2022 年 8 月就做好了。并且，他曾在微软（和其他大型科技公司）即将进行裁员之前就发出了警告：在 2023 年 1 月大规模裁员前一周，他就在 Blind 上发布了裁员的确切日期和数量。这位员工多次准确地预测裁员事件，因此在社区中被认为是一位\"传奇人物\"。因为越来越多的人要求他分享更多细节，所以才有了这篇文章。</p><p></p><p>然而，微软肯定不希望看到这种分享，一位微软员工评论说：“我们的公关部门肯定要疯了。”也有人担心他因此而被开除，他回复说：“那正好可以休息一段时间。”另外，他也表示目前的版本已经在他律师的指导下进行了一些删减，“删减并保留了一些证据，以防不时之需”。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4a/4afeeb1afdd33734a2f4dca1cf2890ab.png\" /></p><p></p><p>这可能是关于科技行业内部持续腐败系统的最详细的帖子。而很多人仍然因这场危机而受苦，大概只有失业的人才能理解这种艰难。我们将这篇文章翻译出来，也希望能为科技行业的企业提供一些发展中的警示：</p><p></p><p></p><h2>我们是如何走到这一步的</h2><p></p><p></p><p>很多人要求我聊聊 2023 年科技大厂裁员的事情。在这里，我想跟大家分享一点关于微软内部、高管团队还有董事会那边的情况。闲言少叙，咱们马上进入正题。</p><p></p><p>这个故事分为三个阶段。</p><p></p><p>早期疫情期间，公司内部的讨论情况。为什么要在 2021 年和 2022 年初大规模扩招。大规模裁员计算公布后激发的讨论。</p><p></p><p></p><h3>早期疫情期间，公司内部的讨论情况</h3><p></p><p></p><p>大家应该还记得，2020 年 3 月疫情刚刚爆发时微软要求员工们在家里先办公两个礼拜。当时高管团队和各部门领导都陷入了恐慌，没人知道这次突发事件会给生产力、产品发布进度和士气造成怎样的影响。</p><p></p><p>微软指派了一名联络员，负责直接跟华盛顿州长 Jay Inslee 和州卫生部联系。对方的意见成为我们早期反应的依据。世界各国也在组建自己的工作组，为当地疫情局势提供政策指导。</p><p></p><p>微软任命 Kurt DelBene 负责推动公司在全球范围内的协调和响应。Kurt 对整个局势的早期把握，再加上从州联络处获得的帮助与指导，让微软从容度过了疫情爆发之初的恐慌期。微软成为第一家延长居家办公的巨头。（Kurt 的工作真的非常出色，比他的继任者好太多了。但为了保护隐私，这里不便透露后面这位负责人的姓名。）</p><p></p><p>在最初两个礼拜的居家办公协议中，华盛顿州卫生部明确强调疫情至少还要再持续几个月。但担心对心理健康产生影响，我们没有向员工透露这条消息。领导层当时主要在考虑三个问题。疫情会对生产力造成怎样的影响？我们要如何快速调动资源来应对激增的软件和服务需求，包括对 PC 和 Teams 软件的旺盛需求？我们要如何保持员工始终士气高昂？毕竟居家办公、脱离社交接触、无法与同事当面互动等现实状况，都可能给心理健康产生巨大影响。</p><p></p><p>我们很快制定一项策略，为员工提供家具和资源，保证他们在这明里也能高效工作。微软还鼓励各部门领导者频繁召开全体会议，重点关注士气和心理健康。（比如「你的工作处理得怎么样？」之类。）在远程办公的第一年，最让人头痛的问题反而出在远程实习这边。</p><p></p><p>时间快进几周，数据显示我们的生产力实际上每周提高了整整 8 小时。面对市场对于服务需求的大幅增长，我们把大量预算投入到新的招聘中来。我们还建立起强大的疫情应对团队，并与世界各地的卫生部门保持着良好沟通。</p><p></p><p>一切已经到位，微软公司成功度过了这段充满挑战的时期。我们还发现设施和运营成本实现了可观的节约，并考虑把其中一部分以一次性资金的形式发放给员工。</p><p></p><p></p><h3>为什么要在 2021 年和 2022 年初大规模扩招</h3><p></p><p></p><p>跟整个行业的大趋势一样，我们的产品和服务销售额也在疫情期间迅猛增长。</p><p></p><p>部门领导和财务负责人都对增长做出了乐观估计，这也很快成为全行业的基本共识。每个人都觉得疫情之下生意反而更好做了，某些企业和组织甚至认为未来几年内业务增长有望达到 30% 到 40%。</p><p></p><p>这绝对是个关键时刻。有些领导者更有先见之明，意识到这种增长其实不可持续。但他们最多也就是觉得业务本身确实在缓慢增长，只是疫情把需求提前了，最终还是会归于平稳。遗憾的是，当时很少有表达怀疑的声音，即使有也被迅速淹没在部门领导者和高管团队成员对股权奖励的无尽渴求当中。</p><p></p><p>除了苹果以外，行业内的每家厂商都在扩招，大家都觉得别人在做、我也得跟上。当时举债融资的成本也很低（特别是对微软这样一家债券评级特别优秀的企业），所以科技行业就出现了像抓宠物小精灵一样疯狂雇人的现象。</p><p></p><p>高管团队和董事会确实也讨论过业务增长达不到预期的可能性。但最终的主体共识是，如果不做好充分准备和资源来抓住这个机会，那么一旦增长成真，微软将蒙受巨大的损失。他们认为需求会持续更长时间、吞掉未来的潜在市场空间，所以把握住当下是第一要务。这也成为当时多数科技大厂的基本判断。</p><p></p><p>但现在回头来看，当时的乐观预测明显是错误的。贪婪的领导者梦想拿到可观的股权估值，并用一场集体大合唱掩盖掉了真正能反映现实的论调。</p><p></p><p></p><h3>大规模裁员</h3><p></p><p></p><p>后来的三个关键事件，最终促成了裁员这个艰难的决定。</p><p></p><p>招聘与留存成本上升AI 和 ChatGPT 的迅猛爆发部分业务的收入突然下降，且开始低于预期。</p><p></p><p>2021 年底到 2022 年初，招聘市场可谓一片兴旺。受限股权和签约奖金就跟不要钱一样狂撒。疯狂之举开始令华尔街感到不安，认为靠滥发股权来吸引人才绝对不可持续。为此，华尔街的基金经理们多次对稀释股权来维持招聘和留存表达了批评。</p><p></p><p>为此，华尔街主要基金经理和股东还开始制定计划，想办法打压这波病态的招人热潮。Reddit 上还出现多起泄密事件（我也是从该网站上探听到这波全行业裁员的最初风声），有对冲基金员工在讨论科技行业一年之内就得解雇 30 到 50 万员工，否则一定会被沉重的薪酬负担给压垮。</p><p></p><p>如果 CEO 和 CFO 不相信这种判断，他们还会继续向董事会施压，毕竟不少主要股东也在其中任职。总之，掌控一切的金融力量已经下定决心，必须遏制这场风波。</p><p></p><p>但我听说这一切，是在 2022 年 8 月的杰克逊霍尔经济研讨会上。当时各位 CEO、董事会成员、大型科技投资者和基金经理已经在对之前这波对抗进行复盘。</p><p></p><p></p><h4>AI 演示</h4><p></p><p></p><p>OpenAI 一直在大量使用 Azure 云资源，也有考虑转用 GCP 来节约成本。但考虑到呈指数级增长的计算需求，Azure 当然不想失去这位大客户。纳德拉派 Kevin Scott 前往 OpenAI，讨论入股这家公司有没有可行性，能否坚定他们继续使用 Azure 的决心。</p><p></p><p>Kevin Scott 一去之下震惊万分，回来告诉纳德拉一定要关注 OpenAI 拿出来的 AI 演示。毕竟多年以来，微软也一直在打造自己的 AI 团队。纳德拉也毫不掩饰自己的两大抱负——AI 和量子计算。总之，OpenAI 将对微软内部的 AI 投资产生巨大冲击。</p><p></p><p>纳德拉很快拿到了 OpenAI 的技术力演示，并很快意识到 GPT-4 将给开发者的生产力带来巨大提升。2022 年末，纳德拉开始考虑科技企业因 AI 发展而导致整体就业率下降的问题。他曾说过“科技行业的就业率会整体增长，但科技企业的岗位数量将有所下降。”</p><p></p><p>微软也很快调整了 AI 投资策略，第一次决定把资源投向外部实体。微软负责投资基础设施以供 OpenAI 开发产品，但加大 AI 基础设施建设也意味着削减其他领域的拨款。用纳德拉本人的话说，也就是压榨效能。</p><p></p><p></p><h4>PC 销量下降，收入预测未达预期</h4><p></p><p></p><p>压死骆驼的最后一根稻草，就是 PC 和 Xbox 销量开始以惊人的速度下降。其他企业级收入的预测结果也被证明完全错误。到 2022 年 8 月，整体趋势已经开始明朗。9 月下旬，高管团队批准了裁员计划，我们开始招聘人力资源专员，逐步将大规模裁员计划落实到位。很明显，面对新技术的突然爆发，微软的人力已经严重过剩。</p><p></p><p>预算紧缩已成定局，接下来就是对具体削减幅度进行核算。高管团队和各部门领导不再拿“我们都是一家人”的企业文化说事，反而以令人难以接受的冷漠和粗暴风格行事。这时候他们唯一关心的，就是明确裁员数量、然后落地执行。</p><p></p><p>到 2022 年 10 月，大多数部门的高层领导者都意识到这波裁员的广度和深度。有些部门甚至放宽了预算限制，提供额外的假期，让大家在这段最后的时光再聚一聚。但因为担心这会令员工们起疑，有些部门领导甚至邀请员工参加假期聚会。我也参加过其中一场，神奇的是在场的所有人都知道这是“最后的晚餐”，只有部门领导还美滋滋地认为自己干得很漂亮。</p><p></p><p>2022 年 10 月，我第一次开始在论坛的微软子频道上发出裁员警告。后来警告一份接一份，我还在 12 月把通知转到了整个技术频道，想要给大家提个醒。但当时返回的大多是怀疑和愤怒的态度，其中谷歌员工的反应最激烈。总之，我根据可靠的资源和数据先后透露过亚马逊、谷歌、微软的裁员计划。</p><p></p><p></p><h4>企业家在想什么？</h4><p></p><p></p><p>最后这部分，就是我前面提到的删减最多的地方。我很想告诉大家那帮高管人士有多么不关心自己的员工，甚至打算公布几张聚会照片。直到今天，每当回想起当时的情景，我都不禁会捏紧拳头。</p><p></p><p>这也是我第三次对纳德拉这位掌门人失去敬意。第一次是他消极处理微软内部消灭的女性受骚扰投诉。尽管证据确凿，但调查进展却相当缓慢。他们庇护多名高管，甚至愿意为其承担律师费。而抱怨性骚扰和批评工作环境的普通员工，却会被系统性地针对甚至辞退。第二次是他处理微软在全球范围内面临的多起关于系统性腐败和合同贿赂的投诉（同样有明确证据）。现在是第三次，他处理裁员计划的态度再次让人失望。很明显，他压根不在乎自己的决定会对多少人的生活产生巨大冲击，更不用说对此承担责任了。可恨的是，在此期间他居然要求董事会给自己加薪。这些问题目前仍然存在，而且交给了更强大的公关和人力部门去处理。可耻，真的非常可耻！</p><p></p><p>当然，对那帮家伙来说裁员就只是个数字。等到市场逐渐好转之后，他们又会笑脸相应、把大家再骗回去。</p><p></p><p>至于同理心，不存在的。</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://www.teamblind.com/post/How-we-got-here-Some-inside-scoops-from-Microsoft-on-handling-early-days-of-pandemic-to-cutting-over-20K-folks-in-2023-7ndQwLAU\">https://www.teamblind.com/post/How-we-got-here-Some-inside-scoops-from-Microsoft-on-handling-early-days-of-pandemic-to-cutting-over-20K-folks-in-2023-7ndQwLAU</a>\"</p><p></p><p><a href=\"https://twitter.com/TeamBlind/status/1706266044871086271\">https://twitter.com/TeamBlind/status/1706266044871086271</a>\"</p><p></p><p><a href=\"https://news.ycombinator.com/item?id=37643608\">https://news.ycombinator.com/item?id=37643608</a>\"</p><p></p><p>声明：本文为 InfoQ 翻译整理，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651183025&amp;idx=1&amp;sn=0d20db4a0fc20154c144aa8561b289d6&amp;chksm=bdb82fe28acfa6f4809cfa76dd124afff508142700efbdc273c89d26856fdf488fd86b9b2cfa&amp;scene=21#wechat_redirect\">Angular 重磅回归</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651183022&amp;idx=1&amp;sn=f2e732df3422a4f724b1056ae03dbc77&amp;chksm=bdb82ffd8acfa6ebc9ba809377f6d989ddc50816e971316a54389a9a62d6ff2cc942073b6a60&amp;scene=21#wechat_redirect\">安息吧，元宇宙</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651183021&amp;idx=1&amp;sn=8b75159e79851b0d68a82d1265a27bdc&amp;chksm=bdb82ffe8acfa6e834d01b5fc2778a893f7a292a91ee081fd8d01e76349b4070deafadac1367&amp;scene=21#wechat_redirect\">裁错了还是变相降薪？大厂粗暴裁员后又求员工回来，网友：拿什么再爱你？</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651183019&amp;idx=1&amp;sn=0f03c320ae1967d5e6fa230988f60787&amp;chksm=bdb82ff88acfa6ee9267ad4cb830c9fc293d811db0b9d6a281125fd7c2451c45199ed1f904f5&amp;scene=21#wechat_redirect\">一小时 12 元，我在北欧监狱里训练 AI</a>\"</p><p></p><p></p><p></p><p></p><p></p>",
    "publish_time": "2023-10-07 14:05:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Angular 重磅回归",
    "url": "https://www.infoq.cn/article/hxMfjE9msAMmSJlfAApl",
    "summary": "<p>作者 ｜ Loraine Lawson</p><p>译者 ｜ 张乐</p><p>策划 ｜ Tina</p><p>&nbsp;</p><p></p><blockquote>Angular正在复兴。</blockquote><p></p><p>&nbsp;</p><p>Angular是一个由 Google 和社区共同领导的开源 Web 应用框架。在设计上，<a href=\"https://www.infoq.cn/article/2018/02/roadmap-angular\">Angular 是 AngularJS</a>\" 的完全重写，由AngularJS 的同一个开发团队负责。</p><p>&nbsp;</p><p>去年1 月，Angular 团队宣布对 <a href=\"https://www.infoq.cn/article/iQEgJzZTY7flzXkE2kjN\">AngularJS 的长期支持正式停止</a>\"，团队已经将全部精力放到了开发新的功能上。该团队于今年5月正式发布了Angular v16 ，并打算在今年11月发布 Angular v17版本。目前他们推出了将要在 v17 中包含的一系列功能，比如控制流。</p><p>&nbsp;</p><p>关注Angular的前端开发者认为，Angular正在复兴。Progress高级开发大使Alyssa Nicoll也想向大家传达这样一个信息：是时候重新审视这个JavaScript框架了。</p><p>&nbsp;</p><p>Nicoll是<a href=\"https://angularair.com/\">Angular Air播客</a>\"的主持人，她的职责包括与<a href=\"https://thenewstack.io/google-engineer-outlines-whats-next-for-angular/\">Google Angular</a>\"团队共事。Nicoll表示，自2015年以来，基于TypeScript的框架发生了显著的变化，特性不断扩展。就在那个时候，Google重写了AngularJS，创建了一个全新的框架Angular 2+。然后在很长的一段时间里，Angular团队都在重写名为Ivy的基础视图引擎。她说，现在lvy已完成，Angular团队发布了之前推迟的升级和新特性。因此，Angular变得对所有用户都更加友好。</p><p>&nbsp;</p><p>Nicoll说：“这就是我将它与文艺复兴联系起来的原因，因为它不仅带来了活力、新鲜感和创造力，而且还大幅优化了开发体验。人文主义是文艺复兴运动的一个重要组成部分，涉及人类成长和人类潜力。围绕Angular的这一运动关注的也是开发体验以及人们使用Angular的潜力，并使其对所有用户都更加友好。”</p><p>&nbsp;</p><p>这可能不够有说服力。<a href=\"https://survey.stackoverflow.co/2023/\">Stack Overflow 2023年的调查数据</a>\"显示，<a href=\"https://thenewstack.io/dev-news-react-still-king-vercel-ai-tools-netlify-connect/\">框架使用率总体上呈下降趋势</a>\"，特别是过去一年，Angular的使用量下降了24%。同时，Svelte和Deno的使用率则分别增长了约62%和61%。Angular控制了约18%的框架“市场”，而React仍然以接近41%的使用率领先。</p><p>&nbsp;</p><p>之所以出现上述变化，其中一部分原因就是使用率下降。</p><p>&nbsp;</p><p>Nicoll说：“Angular团队……非常关注开发体验，因为我们正在设法吸引新的开发人员，因为如果没有大量新的开发人员采用，我们的社区将会慢慢萎缩。”</p><p>&nbsp;</p><p>Nicoll说，该框架的改进主要体现在以下三个方面。我们有必要重新对它进行审视。</p><p>&nbsp;</p><p></p><h3>1. 移除模块</h3><p></p><p>&nbsp;</p><p>在Angular中，最小的代码块不是组件，而是模块。在众多JavaScript框架中，只有它是这样的。Nicoll解释说，模块是封装器，其中包含依赖关系、共享功能甚至路由等内容。</p><p>&nbsp;</p><p><a href=\"https://javascript.plainenglish.io/getting-rid-of-ngmodule-in-angular-javascript-43fd510779bc\">移除模块</a>\"可能会让习惯了模块的“Angularites”感到困扰，但这将使其他开发人员更容易理解框架。</p><p>&nbsp;</p><p>她说：“即使是长期使用Angular的人，一旦停止使用模块，也会看到框架未来的潜力。这使得其他JavaScript开发人员使用我们的框架变得更容易。如果需要快速加入Angular项目——因为有很多团队有Angular项目、React项目或Vue项目，人们就更容易根据现有的经验来理解我们的框架，因为基本部件看起来一样。”</p><p>&nbsp;</p><p>对于经验丰富的Angular开发人员，Nicoll建议不要在生产应用中采用“淘汰和替换”的方法。</p><p>&nbsp;</p><p>她说：“你可以删除应用程序模块，这是启动整个应用程序的基础模块。但我不推荐这样做，因为我认为社区本身还不支持这一点。如果你这样做，你的许多依赖项都可能会出问题，因为它们会找不到应用程序，这是因为它们依赖这个基础模块来获取应用程序的信息，以及如何与之协同。”</p><p>&nbsp;</p><p>她补充说，支持Angular应用程序基础结构的工具和依赖项仍在发展，只是还没有达到这种程度。</p><p>&nbsp;</p><p>“我想说，一定要从组件中删除模块，或者在开发新组件或管道时不再使用它们。但是，在生产环境中，除非你非常确定所有的依赖项以及它们与应用程序的集成方式，否则就先等等，暂时保留基础模块。”</p><p>&nbsp;</p><p></p><h3>2. 添加信号</h3><p></p><p>&nbsp;</p><p>Nicoll表示，Angular正在添加信号，这为它的“内置响应性原语”。信号将使开发人员能够轻松管理和响应应用程序中的更改。她认为，这有可能彻底改变开发人员对响应式编程的处理方式，使其更容易被更广泛的开发人员所接受。</p><p>&nbsp;</p><p>“React及其他许多框架，甚至.Net都有信号的概念。”Nicoll说，“在某种程度上，是Angular正在追赶，使自己变得更好。”</p><p>&nbsp;</p><p>信号是一个对象，它有值，而且我们可以观察其变化。它们类似于React的状态，但是根据Google Bard的说法，信号主要有以下几个优势：</p><p>信号可以在组件之间共享，而不必将它们作为props向下传递。信号仅在需要时更新，这可以提高大型应用程序的性能。信号可用于创建复杂的状态管理模式，例如Redux和MobX。</p><p>&nbsp;</p><p>目前，Angular提供了可观察对象以实现响应性，并将其与OnPush相结合。虽然有效，但也有代价。</p><p>&nbsp;</p><p>她说：“使用可观察对象和OnPush的代价可归结为zone.js——很多Angular开发者提到它时都会做出呕吐的表情或者胸前画十字——和变化检测。像信号这样内置的反应原语就没有这种代价。”</p><p>&nbsp;</p><p>她补充说，就目前而言，未来在Angular中不会出现这种变化检测，这将缩短加载时间，提升应用程序性能，甚至提升开发速度。</p><p>&nbsp;</p><p></p><h3>3. 控制流</h3><p></p><p>&nbsp;</p><p>在外媒分享这个主题时，Nicoll 解释说，<a href=\"https://www.reddit.com/r/Angular2/comments/149bdkw/angular_is_bringing_new_builtin_control_flow/\">新提议的控制流</a>\"语法“很大程度上受到Svelte的控制流以及Mustache模板语言的启发”。她说：“想想内联的if、else、switch和defer。”</p><p>&nbsp;</p><p>控制流允许在模板中使用if和else语句，方便开发人员加载东西，甚或是延迟加载（例如图像），直到用户需要或执行到这块时。</p><p>&nbsp;</p><p>她说：“所有这些都可以改善Angular应用程序的用户体验。所有这一切，我提到的每一件事，都是可选的；它们不会强迫你改变使用Angular的方式，不会带来任何破坏性。我认为，这是他们会继续遵守的一项对于Angular社区的重要承诺。”</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://blog.angular.io/meet-angulars-new-control-flow-a02c6eee7843\">https://blog.angular.io/meet-angulars-new-control-flow-a02c6eee7843</a>\"</p><p><a href=\"https://blog.angular.io/angular-v16-is-here-4d7a28ec680d\">https://blog.angular.io/angular-v16-is-here-4d7a28ec680d</a>\"</p><p><a href=\"https://thenewstack.io/the-angular-renaissance-why-frontend-devs-should-revisit-it/\">https://thenewstack.io/the-angular-renaissance-why-frontend-devs-should-revisit-it/</a>\"</p>",
    "publish_time": "2023-10-07 14:13:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "软件打包，有没有更好的方法？！",
    "url": "https://www.infoq.cn/article/JINUB0hhCyxzDi2VkgLO",
    "summary": "<p>最近，一位朋友分享了<a href=\"https://gist.github.com/terabyte/15a2d3d407285b8b5a0a7964dd6283b0\">关于亚马逊内部构建系统的设计要点</a>\"，这也让我对于软件打包这事有了新的认识。</p><p>&nbsp;</p><p>根据推测，亚马逊的构建系统“Brazil”在原理上有点类似Nix/NixPkgs，也就是基于几乎一切现有包的声明、具备完全的可重现能力。但是，大家不仅可以选择为软件包的各个版本创建独立的快照，还能指定一组软件包semver（语义版本），在创建新的不可变build时通过单元测试强制保证其彼此兼容，这样得到了能够放心使用的最终更新。亚马逊，真有你的！跟Nix类似，Brazil还具备以下特性：</p><p>同时在系统上安装两个软件包版本，根据实际环境选择需要的版本。针对开发/调试环境对软件包做本地覆盖。提供二进制版本，确保一切均可复现。</p><p>&nbsp;</p><p>而且这些“都能实现”！我这位在亚马逊工作的朋友对此高度评价，觉得软件构建从未如此简单。其实这真的很难相信：</p><p>主build驱动会用Perl脚本生成大量Makefiles。整个构建系统只由最小Perl脚本引导，而此脚本会假设环境中仅包含最基本的Perl deps和GCC，然后下载所有其他依赖项。</p><p>&nbsp;</p><p>……但人家说能实现，那就是能实现喽！</p><p>&nbsp;</p><p></p><h1>大多数软件并非如此</h1><p></p><p>&nbsp;</p><p>在开始讨论之前，我们先明确解释几个要用到的术语：</p><p>软件包：软件的原子单元，包括库、应用程序等等。每个软件包又包含：接口版本：这些标识符用于让其他软件了解某个软件包是否支持某些功能。理想状态下会以semver兼容的方式存在，但实际操作中往往不一定。添加额外的调试记录或修复安装bug之类不会影响到消费者使用的操作，不会改变接口版本。Build版本：这些标识符与软件包生成的二进制文件中的差异一一对应，用于区分“我添加过额外调试记录或修复安装bug的库”和“还没调试/修复过的库”。也能反映不同build版本之间依赖项方面的差异。依赖项：软件包在构建及/或运行时所依赖的另一软件包。通常使用接口版本来指定，但也可以在build版本中指定。版本集：由已知能够良好协同运行的各软件包build版本所建立的集合。它的意义在于证明各build版本间能够良好协同，之所以不指向接口版本，是为了避免搞乱semver。环境：指当我们想要使用某个软件包时，所有能够对其产生影响的其他软件包的总合。</p><p>&nbsp;</p><p>据我所知，目前有两种常见方法来分发软件包并创建运行环境。除此之外当然还有其他，而且很多方法难以准确分类。这里我们就先讨论最典型的情况。</p><p></p><h2>共享一切</h2><p></p><p>有一个中央版本集，其中包含所有软件包，通常需要测试各软件包间能否良好协作。在任意给定时间，每个包只能安装一个build版本。如果想要同时拥有不同的build版本，则需要创建不同的包或为包指定别名。</p><p>&nbsp;</p><p>这就是软件环境的典型模型。Arch Linux、RHEL、pip、npm、Homebrew、Forge等等，但凡是包管理器，使用的就很可能是这种模型。虽然它们在更新频率、semver固定原理和所负责的工作方面各有差异，但我列出的所有示例都具有上述共通特征。</p><p>&nbsp;</p><p>现在，我要坦率地讲，这套模型相当差劲。不是我要尬黑，但能够正式安装的软件包只能有一个版本确实太少。如果想在中央版本集之外保留一个包含某个依赖项的build版本，那只有以下三种办法：</p><p>重新命名这个依赖项，再进行全局安装。在包管理器的控制范围之外“安装”这个依赖项。直接放弃。</p><p>&nbsp;</p><p>第一个选项太蠢了，因为这意味着我们得自己把接口/build版本指定为包名称，而这类版本区分的工作本来是该由包管理器负责的。选项二也很蠢，代表我们虽然有了好用的包管理器，但还是得使用CMakeLists.txt 和 shell 脚本对它做滚动更新。选项三更不行，毕竟咱搞开发的不能轻言放弃😤</p><p>&nbsp;</p><p>有时候，我们可以允许软件包拥有自己的依赖项范围，毕竟不是所有东西都得全局化。坦率地讲，目前这种糟糕的本地安装支持实在让人无法接受。所以下面，咱们再来看看事情的另一个极端：</p><p></p><h2>完全不共享</h2><p></p><p>&nbsp;</p><p>如果某个包有依赖项，可以用这种方式以自包含的形式将这些依赖项放进环境当中。目前有多种办法可以让单独安装的软件包融入同一环境。但如果没有包管理器的支持，这些办法要么缺乏可扩展性（这还是最好的情况），要么就是引发令人恼火的错误。奇怪的是，Windows和MacOS等消费级操作系统居然将此作为默认方法。更奇怪的是，最近Docker、Snap、Flatpak等容器化技术的普及也使得Linux软件开始以这种模式进行分发。为什么会这样？</p><p>&nbsp;</p><p>我猜测这种模式之所以流行开来，是因为它更利于产出比较一致的软件。Linux发行版长期面临的头号难题，就是“在我的机器上明明能跑啊”和“在我的发行版上明明能跑啊”这种不一致冲突。如果共享一切，那么只要在全局版本集之外进行尝试，甚至是在随时间推移而开展的同一发行版之间，软件包的构建都可能出现令人沮丧的意外。正因为如此，具有虚拟环境的特定语言包管理器都会选择完全不共享的方式，Docker大受欢迎的原因也在于此。全局环境不可避免存在“幽灵”，这些无形的依赖项会随时侵扰构建过程，因此隔离一切并驱散“幽灵”是实现可复现性的前提。</p><p>&nbsp;</p><p>当然这里也要强调，“不共享”方法也有自己的缺点。要求软件包把所有依赖项都捆绑进来、建立起内部的“共享一切”小环境会导致体积快速膨胀。反正我自己是不太想在机器上重复安装5个Tensorflow或者PyTorch副本的，但我又不想把所有一次性AI项目都塞进同一个全局Python环境，所以情况就很尴尬了。</p><p>&nbsp;</p><p></p><h2>有没有更好的方法？</h2><p></p><p>下面咱们捋一援理想构建系统的基本要求：</p><p>可稳定复现的构建：如果远程系统能够成功构建，那我们的本地系统也应该可以。本地覆盖：不仅可以在本地构建软件包，还能根据需求对包内容进行随意替换。远程托管的二进制版本：这样就不必每次想要安装软件时，都劳烦自己本地的CPU和硬盘。不设全局版本集：允许在系统上安装同一软件包的多个版本（包括主要版本、次要版本、不同补丁），而且均采用可稳定复现的构建基础。Semver和哈希固定：启用依赖项共享（如果支持），并在必要时提供精确的复现性。</p><p>&nbsp;</p><p>很明显，前面介绍的两种常见方法都满足不了要求，甚至可以说还差得远！也就是说，目前的软件包分发机制存在根本缺陷，导致我们身陷困境。</p><p></p><h1>迎难而上</h1><p></p><p>在对“共享一切”和“完全不共享”有了深入了解之后，现在我们就能体会到亚马逊Brazil的妙处了。它不仅允许隔离各软件包并分别指定其依赖项，而且一切都能稳定复现，甚至能够让各包共享具有相同接口版本的依赖项！这也太棒了，但亚马逊到底是怎么做到的？</p><p></p><h2>技术挑战</h2><p></p><p>这里我们不打算太过深入，但其实没有现成方案的原因并不是做不到。各种主流操作系统已经能把不同层级的环境妥善隔离开来，为什么软件包这边就不行？</p><p></p><h2>社会挑战</h2><p></p><p>所以最大的问题可能跟技术无关，而更多来自人们的漠不关心。开发者、发行版贡献者大都觉得“我为什么要改变自己构建软件的方式？目前的方案对我的用例来说已经足够了！”</p><p>&nbsp;</p><p>就个人而言，我也曾经在跟预期环境略有区别的环境中构建过不少软件，而且深受其害。每个包各不相同，拥有自己的脚本、命令行标志、环境变量和build目录，而这一切都让工作充满了不确定性。正如Brazil项目下一位评论者的留言：</p><p>根据个人经验，Brazil的打包概念之所以没能普及，就是因为之前的问题还没严重到改变的临界点。亚马逊有Brazil，可以用它轻松搞定Gem、NPM包、*.so或者JAR等依赖项。所以哪怕要经历一番痛苦（特别是在导入新的构建系统时），问题也总能得到解决。而且在打包完成后，这事就过去了。</p><p>&nbsp;</p><p>只有那帮闲着没事干的书呆子才愿意为此专门构建生态系统。Gentoo、NixPkgs、Guix、AUR的软件包维护者们各自举起自己的神器，想让整个软件世界臣服在自己脚下。于是乎，在同一系统之内“一切都正常运作”，但对我们这些不幸要在系统之间往来跨越的软件开发者来说，迎来的就是一场无休止的噩梦。啥都可能出问题，啥都没法顺利实现，而且没人愿意真的拿出时间和精力搞一套整体解决方案。又不是不能解决，忍着得了……</p><p></p><h2>亚马逊是怎么做的</h2><p></p><p>&nbsp;</p><p>简而言之，他们选择花钱解决问题。这笔钱，来自在包构建时浪费在每个依赖项传递、浪费在确保接口版本符合semver标准上的计算成本。也来自浪费在托管软件完整历史记录（源代码加二进制文件）以防止旧有build版本丢失的存储成本上。最重要的是，亚马逊愿意支持开发人员把自己想用的所有软件都移植进这个构建系统。</p><p>&nbsp;</p><p>所以，这种方法只适用于像亚马逊这样的科技巨头，毕竟对他们来说这点投入绝对物有所值。但我们其他人呢？</p><p></p><h2>我们能不能学两招？</h2><p></p><p>&nbsp;</p><p>老实说，我也不知道。也许NixPkgs和Guix都比较接近我想要的效果，能在一定程度上满足我对理想构建系统的要求（当然，semver固定这类没钱就不可能实现的要求除外）。我用得不多，所以还没有资格评价二者的使用体验。但一方面我听说过关于NixPkgs的抱怨，另一方面我几乎没听人提起过Guix，这两种情况似乎都不太妙。</p><p>&nbsp;</p><p>作为个人，我也没那个能力去迎难而上。我已经习惯了生活在噩梦的阴影下，用修修补补的方式把自己的Windows开发环境维持起来，这种情况在短时间内也不太可能改变。但我觉得，应该有一整个技术社区去迎难而上，这样即使我手头的Arch安装还是问题多多，但下一次Linux安装就能拥有稳定的可复现性。希望更多人能和我有同样的期待。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://cohost.org/PolyWolf/post/2613009-software-packaging-a\">https://cohost.org/PolyWolf/post/2613009-software-packaging-a</a>\"</p>",
    "publish_time": "2023-10-07 14:22:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "苹果中国App Store将不允许未备案应用上架；iPhone 15发热严重，问题源于第三方软件？Meta又要裁员了 | Q资讯",
    "url": "https://www.infoq.cn/article/DjaZgaMORWgwZduSeca1",
    "summary": "<p></p><blockquote>字节一季度财报出炉，营收达245亿美元，规模接近Meta；iPhone 15被投诉发热严重，用户被烫伤；全国首个大模型生态社区在沪揭牌；涉嫌非国家工作人员受贿罪，商汤科技知产总监被立案侦查、采取强制措施；苹果中国App Store将不允许未备案应用上架；微软已在Bing搜索引擎上投入了大约1000亿美元；Android 14发布，源代码登陆AOSP......</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>字节一季度财报出炉，营收达245亿美元，规模接近Meta</h4><p></p><p>&nbsp;</p><p>据<a href=\"https://ishare.ifeng.com/c/s/v002qFPShHF5--1Q1c0NLLjdbulM8gp8MDhSYTsYaV28OXL0__\">媒体报道</a>\"，北京时间10月3日，字节跳动公司在周一向员工分享了一份财报报告，提供了2021年、2022年以及今年第一季度的详细财务数据。自2021年营业亏损70亿美元（约合511.1亿元人民币）以来，字节跳动一直在采取措施扭转公司亏损。报告显示，字节跳动在营收迅速增长的同时，大幅削减了营销、管理和研发费用。2022年，字节跳动营收继续增长，同比增幅超过38%达到852亿美元。2022年，字节跳动销售和营销支出为148亿美元，低于2021年的192亿美元；研发支出为87亿美元，低于2021年的146亿美元；一般及行政支出为45亿美元，低于2021年的83亿美元。</p><p>&nbsp;</p><p>2023年第一季度，字节跳动营收接近245亿美元，同比增长近34%；营业利润接近60亿美元，几乎是去年同期的两倍。就营收而言，字节越来越接近Meta的规模，Meta在第一季度实现了72亿美元的自由现金流，第一季度的营收达286亿美元。</p><p>&nbsp;</p><p></p><h4>iPhone 15被投诉发热严重，用户被烫伤</h4><p></p><p>&nbsp;</p><p>据雷峰网消息，iPhone 15 Pro系列用上了全球唯一一颗3nm工艺芯片A17 Pro，却疑似在高压力下不堪重负，能效极低，导致iPhone 15 Pro系列在日常使用的时候也频频过热发烫。目前已有多位用户喊话称，自己被苹果15烫伤。</p><p>&nbsp;</p><p>据官方最新发布的信息显示，苹果否认了关于发烫问题与iPhone 15 Pro系列的硬件有关的传闻，称与之前的不锈钢手机相比，新设计改善了散热。并表示其烫手的问题是由于软件和应用程序相关的漏洞所致，Instagram、Uber Technologies Inc．的应用程序，以及游戏Asphalt 9导致了设备运行温度高于正常水平，将会很快为iPhone 15 Pro系列推送iOS 17.0.3。对于这样的回应，国内用户纷纷表示非常不满，因为上述借口对国内用户的发热根本没有任何指引性，毕竟国行版机型并没有安装这些应用程序。</p><p>&nbsp;</p><p></p><h4>全国首个大模型生态社区在沪揭牌</h4><p></p><p>&nbsp;</p><p>据上海经信委微信公众号发文，9月28日，上海“模速空间”创新生态社区暨人工智能大模型产业生态集聚区揭牌仪式在徐汇西岸举行。模型语料数据联盟服务基地、大模型测试验证与协同创新中心、上海大模型合规指导服务中心、上海大模型生态发展有限公司以及16家大模型企业率先入驻“模速空间”。9家单位代表共同启动上海智能算力加速计划，近30家创投机构共同启动上海大模型投融资合作伙伴计划。</p><p>&nbsp;</p><p></p><h4>涉嫌非国家工作人员受贿罪，商汤科技知产总监被立案侦查、采取强制措施</h4><p></p><p>&nbsp;</p><p>近日，据21世纪经济报道，商汤科技知识产权总监高某涉嫌非国家工作人员受贿罪被立案侦查、采取强制措施的消息，引发业内关注。经公安机关查明，该负责人利用职务上的便利，非法收受供应商贿赂，金额巨大，北京市公安局海淀分局对涉嫌受贿罪的知识产权总监立案件侦查并采取刑事强制措施，同时还对涉嫌对非国家工作人员行贿罪的供应商相关人员立案并采取刑事强制措施。</p><p>&nbsp;</p><p>公开简历介绍显示，高某毕业于清华大学，拥有丰富的知识产权职业经验，在国知局专利审查协作中心和北京某律所工作六年，后投身多家知名企业的知识产权管理。自2017年10月加入商汤后，“带领团队建立了比较完善的知识产权战略体系和制度框架，全面提升知识产权工作水平”。</p><p>&nbsp;</p><p></p><h4>Meta又要裁员了？</h4><p></p><p>&nbsp;</p><p>据路透社报道，两位知情人士周二透露，Meta计划于本周解雇其面向元宇宙的现实实验室部门（ Facebook Agile Silicon Team，简称 FAST）的员工，该部门专注于制造定制芯片。Meta 内部论坛 Workplace 上的一篇帖子向员工通报了裁员消息。路透社无法确定该部门的裁员程度，目前该部门约有 600 名员工。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e6eb587ccbebcf95e9f29138692028fb.png\" /></p><p></p><p>&nbsp;</p><p>另外，天风证券分析师郭明錤发文表示，Meta的头戴装置 (元宇宙) 硬件事业因需求疲软造成的亏损可能高于市场共识。郭明錤最新调查指出，Meta 公司的头显（元宇宙硬件）出货量持续显著衰退，而缩减头显（元宇宙）业务对改善 Meta 公司的亏损帮助也相对有限。“Quest 3 头显最初的出货预估在 2H23 达到 700 万部以上，但因预期需求疲软，今年下半年相关头显的出货预估为 200–250 万部，2024 年出货量则约 100 万部。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT业界</h2><p></p><p>&nbsp;</p><p></p><h4>苹果中国App Store将不允许未备案应用上架</h4><p></p><p>&nbsp;</p><p>近日，苹果更新了 “App 信息” 中 “在中国大陆的供应情况”，要求 App 有备案号才能在中国大陆的 App Store 中上架。这意味着大部分外国应用将无法通过 App Store 在中国区提供下载。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/8530b78b2e4946e35ffac099885f8d62.jpeg\" /></p><p></p><p><a href=\"https://developer.apple.com/cn/help/app-store-connect/reference/app-information/\">https://developer.apple.com/cn/help/app-store-connect/reference/app-information/</a>\"</p><p>&nbsp;</p><p></p><h4>微软CEO纳德拉：已在Bing搜索引擎上投入了大约1000亿美元</h4><p></p><p>&nbsp;</p><p>10月3日消息，微软CEO萨蒂亚·纳德拉提到微软已经花费了大约1000亿美元（备注：当前约 7310 亿元人民币）来构建和开发其Bing搜索引擎。他还指出，尽管微软在市场份额方面落后于谷歌，但它相信自己可以为互联网搜索行业做出贡献。</p><p>&nbsp;</p><p></p><h4>Android 14发布，源代码登陆AOSP&nbsp;</h4><p></p><p>&nbsp;</p><p>美国当地时间 10 月 4 日上午 10 点，谷歌在纽约举行了“Made by Google”活动。在这次活动上，谷歌正式发布了适用于 Google Pixel 手机等设备的 Android 14，并将源代码推送到 AOSP（Android 开源项目）。Android 14 的大部分更改是在 2023 年 2 月发布的首个Android 14 开发者预览版中引入的，其中包括性能改进、更好的隐私和安全性以及额外的用户自定义选项。</p><p>&nbsp;</p><p>另外，谷歌还发布了Pixel 8及Pixel 8 Pro手机，搭载了谷歌自研的Tensor G3 处理器。Pixel 8系列有更强的AI功能，能帮助用户拍照和录像。例如新增的最佳拍摄功能可以从一系列照片中选出最好照片；音频魔术橡皮擦可自动降低视频噪音等。这两款手机售价分别为699 美元和 999 美元，比苹果和华为最新的旗舰机要便宜不少。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-10-07 14:28:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "打破英伟达芯片短缺制约，OpenAI决定自研AI芯片：正物色收购目标",
    "url": "https://www.infoq.cn/article/dTZ14uTdUF7NW03C0qJR",
    "summary": "<p></p><h2>OpenAI正在探索自研AI芯片</h2><p></p><p>&nbsp;</p><p>据路透社 10 月 6 日报道，有知情人士透露，打造出 AI 超级明星 ChatGPT 的 OpenAI 公司目前正探索制造原研 AI 芯片，而且正在评估一家潜在的收购目标。</p><p>&nbsp;</p><p>据路透社在内部讨论中得到的消息，OpenAI 公司尚未决定是否继续推进。但知情人士透露称，至少自去年开始，OpenAI 就已经在讨论各种方案、希望解决因供应短缺而愈发昂贵的 AI 芯片问题。相关选项包括打造原研 AI 芯片、与包括英伟达在内的其他芯片制造商开展密切合作，以及在英伟达之外拓展更加多元的供应来源。</p><p>&nbsp;</p><p>对此，OpenAI 公司拒绝发表置评。</p><p>&nbsp;</p><p>目前还不清楚 OpenAI 到底会不会迈出定制芯片这关键性的一步。业内资深人士表示，此举将成为一项重大战略措施，也对应着可观的投资数额，其年均成本也许将高达数亿美元。而且即使 OpenAI 为此投入资源，也无法保证必然获得成功。</p><p>&nbsp;</p><p>如果能收购一家芯片企业，则可以加快 OpenAI 原研自有芯片的进程。比如，亚马逊曾在 2015 年收购 Annapurna Labs。</p><p>&nbsp;</p><p>据一位知情人士透露，OpenAI 已经在考虑对一家潜在收购目标开展尽职调查。但 OpenAI 计划审查和收购的这家公司是谁，目前仍然成谜。</p><p>&nbsp;</p><p>即使 OpenAI 继续推进定制芯片计划（包括实施收购），整个工作也可能要耗时数年，也就是说，该公司在相当长的时期内仍须调蓄依赖英伟达和 AMD 等商业供应商。</p><p></p><h2>芯片短缺是导火索</h2><p></p><p>&nbsp;</p><p>今年 6 月，OpenAI 创始人 Sam Altman 与 Humanloop CEO Raza Habib 以及其他 20 位开发者面对面进行了一场闭门交流。Altman 表示，目前 OpenAI 正受到 GPU 资源的严重限制，导致不少短期计划已经被迫推迟。</p><p>&nbsp;</p><p>比如，微调 API 受到 GPU 资源的限制。因为还没用上 Adapters 或 LoRa 等高效微调方法，所以 OpenAI 的微调运行和管理仍须占用大量算力。未来微调的支持效果会更好，OpenAI 甚至可能为社区贡献模型设立专门的市场。</p><p>&nbsp;</p><p>在这次闭门会上，几家大客户还抱怨了 API 的可靠性和速度表现。Altman 认同这些意见，并解释称主要问题源自 GPU 供应不足。</p><p>&nbsp;</p><p>此外，Altman 还曾公开抱怨图形处理单元供应不足，目前该市场由英伟达所主导，其在全球范围内控制着 AI 应用类处理芯片超 80% 的市场份额。</p><p>&nbsp;</p><p>Altman 强调，之所以要努力扩大芯片来源，主要基于两个现实问题：为 OpenAI 软件提供支持的先进处理器严重不足，且现有工作及产品所依赖的底层硬件所造成的运行成本“令人眼花缭乱”。</p><p>&nbsp;</p><p>在大语言模型和 AIGC 大爆发后，各 AI 企业对于 GPU 的需求比以往任何事时候都要紧迫。英伟达的高端 GPU 芯片价格已经达到了每片数万美元，AI 基础设施公司正在以数万台的价格购买它们。</p><p>&nbsp;</p><p>马斯克也曾表示他已经为他的新 AI 初创公司 X.AI 购买了 3 万多块英伟达顶级的 H100 GPU 芯片，每个价格超过 3 万美元。此外，Meta 和微软已经是今年英伟达GPU 的最大买家之一（Meta 可能排名第一，因为Facebook、Instagram、WhatsApp 和 Messenger 应用程序中有很多 AI 增强的东西要用到 GPU）。</p><p>&nbsp;</p><p>这就是为什么从 Altman 会表示 OpenAI 也很缺 GPU 的原因。Sam Altman 也曾在媒体采访中公开强调过 GPU 的可用性如何影响 OpenAI 今年及以后的计划。</p><p>&nbsp;</p><p>自 2020 年以来，OpenAI 在就一直在其最大支持者之一微软提供的大型计算系统之上开发生成式 AI 技术。这套计算系统搭载有 1 万个英伟达图形处理单元（GPU）。</p><p>&nbsp;</p><p>对于任何企业来说，ChatGPT 的运行成本都绝不是一个小数目。根据 Bernstein 分析师 Stacy Rasgon 的推测，ChatGPT 的单次查询成本约为 4 美分。如果 ChatGPT 查询最终能够增长到谷歌搜索规模的十分之一，则启动阶段就需要价值约 481 亿美元的 GPU，后续每年还需要价值约 160 亿美元的芯片才能保持服务运行。</p><p></p><h2>大厂集体迈入自研芯片时代？</h2><p></p><p>&nbsp;</p><p>在芯片短缺背景下，不少大型科技企业都开始自研芯片，但成果却相当有限。</p><p>&nbsp;</p><p>据路透社报道，Meta 的定制芯片研发就一直进展不顺，导致该公司最终废弃了部分 AI 芯片项目。作为 Facebook 的母公司，Meta 目前正开发一款新型芯片，希望能涵盖所有 AI 类型。</p><p>&nbsp;</p><p>另据技术外媒 The&nbsp;Information 报道，OpenAI 的主要支持者微软也在开发定制 AI 芯片，并交由 OpenAI 进行测试。OpenAI 自研 AI 芯片的消息可能标志着两家公司将由此分道扬镳、各自安好。</p><p>&nbsp;</p><p>自去年 ChatGPT 发布以来，全球市场对于专用 AI 芯片的需求可谓一路狂飙。最新生成式 AI 技术的训练和运行都需要特定芯片、或者说AI加速器的支持，而英伟达则是少数几家能够生产实用型 AI 芯片并在市场上占据主导地位的芯片制造商之一。</p><p>&nbsp;</p><p>如果真能开发自己的 AI 芯片，则意味着 OpenAI 将成功跻身少数科技巨头之列。对于 OpenAI 的自研芯片前景，你是否看好呢？</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.reuters.com/technology/chatgpt-owner-openai-is-exploring-making-its-own-ai-chips-sources-2023-10-06/\">https://www.reuters.com/technology/chatgpt-owner-openai-is-exploring-making-its-own-ai-chips-sources-2023-10-06/</a>\"</p><p><a href=\"https://web.archive.org/web/20230601000258/https://website-nm4keew22-humanloopml.vercel.app/blog/openai-plans\">https://web.archive.org/web/20230601000258/https://website-nm4keew22-humanloopml.vercel.app/blog/openai-plans</a>\"</p><p><a href=\"https://www.infoq.cn/article/xZaNyw2QsZcxmNXUvkZv\">https://www.infoq.cn/article/xZaNyw2QsZcxmNXUvkZv</a>\"</p>",
    "publish_time": "2023-10-07 14:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从服务网格看，如何做好通用的网络性能优化？",
    "url": "https://www.infoq.cn/article/MtxlsvGu8pDCFZhQEjhB",
    "summary": "<p></p><p>&nbsp;</p><p></p><blockquote>本文作者网易数帆云网络数据面负责人汪翰林，在工作中从事服务网格的网络数据面性能优化，发现其中的网络性能优化的原理具有相通性。本文对通用的网络性能优化方法做出了总结，包括服务网格及网络性能优化分析、网络性能优化技术介绍、网络性能优化思路三个方面，并列举了实际案例进行进一步诠释，供大家在实际做性能优化时参考。</blockquote><p></p><p>&nbsp;</p><p>前段时间，团队一直在做服务网格的网络数据面性能优化，发现其中的网络性能优化的原理是相通的，所以就想着总结一些通用的网络性能优化方法，供大家在实际做性能优化时参考。</p><p></p><h2>服务网格及网络性能优化分析</h2><p></p><p>&nbsp;</p><p>业务微服务化之后，为了提升微服务的治理能力，通常会引入一个业务无侵入的<a href=\"https://www.infoq.cn/article/hh12UJcPhkRmKoQLKyK7\">sidecar代理</a>\"来提供微服务的流控、熔断、升级等服务治理能力。sidecar会转发所有微服务的出口和入口流量，但这样会导致网络路径增加两跳，整个路径拉长，相应地时延也会增加。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b1/b14070f6081774981de62ec253a87eef.png\" /></p><p></p><p>&nbsp;</p><p>引入的时延主要分为sidecar业务处理时延以及网络收发时延。广义上的网络性能优化应该是端到端的时延优化，既包括处理时延优化，也包括网络收发时延优化。我们这里主要讨论狭义的网络性能优化，仅包括网络收发的时延优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a0905749feef799a483a6e1de8bcf33.png\" /></p><p></p><p>如果打开网络传输的链路，sidecar额外引入了Service到sidecar以及sidecar到sidecar的链路，网络时延的增加主要是因为链路中多经过的四次内核态TCP/IP协议栈的开销。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ba/baa582adab9f819fddb931c935893676.png\" /></p><p></p><p>我们通过perf火焰图分析了sidecar应用Envoy的CPU占用，发现内核态协议栈的CPU占比近50%，而剩下的50% CPU主要消耗在Envoy本身处理逻辑上。可见，网络时延的优化的重点也就是对内核态协议栈的优化。</p><p>&nbsp;</p><p></p><h2>网络性能优化技术</h2><p></p><p>&nbsp;</p><p>我们先来看一下针对内核态协议栈的网络性能优化技术，注意这里特指针对内核态协议栈的优化，因为网络性能优化的思路还有很多，我们会在后面章节提到。</p><p>&nbsp;</p><p>针对内核态协议栈的优化主要思路是kernel bypass，即让报文不经过内核态协议栈或者不经过完整的内核态协议栈，通过缩短网络路径来达到性能优化的目的。目前主要用到的技术包括eBPF、用户态协议栈以及RDMA。</p><p></p><h3>eBPF</h3><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/R7bTrXMAvI6JsmVZ6QaY\">eBPF（extended Berkeley Packet Filter）</a>\"是一个内核技术，可以在不需要重新编译内核的情况下，通过一种安全的、可编程的方式扩展内核的功能。eBPF的主要功能是在内核层面进行安全、高效的网络数据包过滤和处理，以及在运行时对系统进行监控和调试。eBPF可以通过独立的用户空间程序来编写和加载，这些程序可以安全地与内核交互，而不会影响内核的稳定性和安全性。</p><p>&nbsp;</p><p>简单来说，目前的内核协议栈处理流程的代码中预留了很多hook点，用户态编写的程序编译成eBPF字节码之后可以注册到这些hook点，之后内核处理报文到hook点时就会调用这些预编译好的字节码程序完成特定的功能。这样我们就可以控制内核针对报文的处理流程，结合eBPF的redirect能力可以将报文直接转发到对应的网络设备或者socket，从而做到kernel bypass。</p><p>&nbsp;</p><p>这里不得不提两类hook点：XDP和socket。XDP hook是网卡收到报文后最早的hook点，可以实现在网络驱动程序中拦截和处理报文。而socket hook可以拦截连接建立、收发的报文，是应用层处理报文最早的hook点。</p><p>&nbsp;</p><p>不过，eBPF程序终究还是运行在内核态，应用层进行网络报文传输时还是避免不了内核态和用户态的上下文切换开销。对于可以完全运行在内核态的应用，如安全检测、负载均衡等场景则可以避免这个开销。</p><p></p><h3>用户态协议栈</h3><p></p><p>&nbsp;</p><p>顾名思义，用户态协议栈是将协议栈的实现卸载到用户态来实现。卸载到用户态实现会带来几个好处：一是避免用户态和内核态的上下文切换；二是根据需要定制和精简协议栈的实现；三是调测和运维也会更加方便。</p><p>&nbsp;</p><p>报文卸载到用户态通常采用DPDK或者AF_XDP的方法。DPDK是Intel主导的开源项目，目前主流网卡都支持，成熟且性能高，不过对于虚拟口（如veth口）的支持不够好，在容器化场景受限。AF_XDP是基于eBPF的报文卸载技术，可以实现在网卡驱动层针对报文拦截并卸载到用户态，对于物理网口或者虚拟口的兼容性好，性能相比于DPDK稍差。</p><p>&nbsp;</p><p>用户态协议栈通常需要开发适配层来兼容socket接口，拦截socket调用并分发到用户态协议栈处理，这样应用层可以做到无需修改代码，完全无感知。</p><p></p><h3>RDMA</h3><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/o3rNxl2trb1GxEMMXDoJ\">RDMA (Remote Direct Memory Access)</a>\"是一种高性能、低延迟的网络通信技术，它能够让计算机之间直接访问彼此内存中的数据，从而避免了传统网络通信中的数据拷贝和CPU参与的过程。所以，RDMA是一项软硬结合的网络性能优化技术，将网络传输卸载到RDMA硬件网卡中实现，提升性能的同时可以将CPU节省出来给应用使用。</p><p>&nbsp;</p><p>RDMA的实现技术分为InfiniBand、RoCE和iWARP。InfiniBand是最早的实现方式，需要专门的硬件网卡和InfiniBand交换机配合组网，和以太网不兼容，成本高但性能最好。RoCE基于以太网传输RDMA报文，使用UDP封装报文，通常需要组建基于PFC/ECN的无损网络，否则丢包会导致性能劣化严重，总体性能较好，RoCE的主要代表是英伟达的Mellanox网卡。iWARP基于TCP，网络适应性好，但性能最差，代表是Intel网卡。</p><p>&nbsp;</p><p>RDMA对外提供ibverbs编程接口，和socket接口还是有较大区别，不过直接基于此进行应用代码改造理论上性能收益最大。Mellanox网卡在此基础上封装了UCX接口，还是不能做到完全兼容socket接口。由Intel和阿里主导的SMC-R可以实现socket接口的完全兼容，由于封装后只能支持报文接收的中断模式而不支持polling模式，会导致性能有损失，底层只能适配RoCE，无法兼容iWARP。</p><p>&nbsp;</p><p></p><h2>网络性能优化思路</h2><p></p><p>&nbsp;</p><p>性能优化是一个端到端的系统性工程，遵循二八原则，即80%的性能损耗在20%的处理流程上。所以优化前还是需要分析下导致80%性能损耗的20%的处理流程到底在哪儿，是应用本身的处理逻辑还是网络传输层面。通常我们可以先使用性能分析工具（如perf）做一下分析，确定瓶颈点。即使确定是网络传输的瓶颈，我们也可以先从硬件配置和组网层面进行分析：</p><p>&nbsp;</p><p>1. 网卡带宽是否受限？如10G更换25G或者100G网卡</p><p>2. 是否跨机房访问？</p><p>3. 是否经过网关？网关是否有瓶颈？</p><p>4. 物理网络设备是否有瓶颈？</p><p>5. 是否经过虚拟网络？虚拟网络是否有瓶颈？</p><p>&nbsp;</p><p>另外，我们也可以检查下内核参数配置，如网卡多队列配置，收发队列大小配置，连接跟踪配置等。</p><p>&nbsp;</p><p>如果以上检查后发现还是满足不了要求，网络传输依旧是瓶颈，就可以考虑使用针对内核态协议栈的性能优化技术。那么eBPF/用户态协议栈/RDMA技术我们如何来选择呢？</p><p>&nbsp;</p><p>从性能上来说，RDMA&gt;用户态协议栈&gt;eBPF。</p><p>&nbsp;</p><p>RDMA综合成本和性能，RoCE用的最多，不过RoCE目前受限于无损网络，组网会有限制，通常限制在一个机房甚至一个ToR下。</p><p>&nbsp;</p><p>用户态协议栈因为协议栈的定制实现，没有内核态协议栈的功能全面，所以应用的兼容性需要case by case的去做验证测试，比较适合于针对服务端的已知应用（如Redis/Nginx/Envoy等）的性能加速，而不太适合于作为一个通用的协议栈提供给所有应用使用。</p><p>&nbsp;</p><p>eBPF类似于给内核打补丁，哪里性能不好就可以打上一个补丁，适合于小的性能优化点的修修补补。</p><p></p><h2>实际案例</h2><p></p><p></p><h3>服务网格</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/20d9d9339784356ff01a7dbba9253be0.png\" /></p><p></p><p>&nbsp;</p><p>服务网格实际优化时结合了eBPF Sockops和用户态协议栈技术。</p><p>&nbsp;</p><p>eBPF Sockops注册了连接建立的hook点，将五元组信息保存到sockmap表中，后续报文发送可以直接查询sockmap表后redirect到对端socket，这样就bypass了内核协议栈。eBPF Sockops适用于Service和sidecar之间链路的加速，因为Service端应用的多样性不太适合使用用户态协议栈加速，而服务器内部进程间通信也不太适合RDMA加速。</p><p>&nbsp;</p><p>用户态协议栈用来加速sidecar和sidecar之间的通信链路，可以bypass掉sidecar之间通信的内核协议栈。因为sidecar之间通信的链路不一定能组建无损网络，RDMA加速不太合适。</p><p>&nbsp;</p><p>当前用户态协议栈我们支持性能优先模式和兼容优先模式。性能优先模式中，底层通过DPDK进行报文卸载，适用于物理口或者虚拟VF口的场景，容器化场景也可以对接SRIOV的容器网络。兼容优先模式中，底层通过AF_XDP进行报文卸载，适用于虚拟veth口的场景，可以支持目前主流的容器网络场景。</p><p>&nbsp;</p><p>另外，用户态协议栈在实际使用过程中可能会面临双栈的需求，即需要同时支持用户态协议栈和内核态协议栈。服务网格加速场景中sidecar和Service之间通信虽然通过eBPF进行了加速，但本质上还是走内核。所以用户态协议栈也需要支持根据路由配置来确定报文走内核态还是用户态协议栈。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f6/f635206289c9271d951a61acb77ecb5b.png\" /></p><p></p><p>&nbsp;图中是兼容优先模式最终加速的效果。时延有35-40%的降低，其中用户态协议栈贡献30%左右，eBPF贡献8%左右。</p><p></p><h3>存储</h3><p></p><p>&nbsp;</p><p>Curve存储是网易自研开源的分布式存储服务。在Curve块存储的开发及优化过程中，我们发现作为分布式系统，客户端需要通过网络和存储节点通信，存储节点之间也需要网络通信来完成多副本之间的一致性协商过程。在传统的TCP/IP网络协议栈中，网络数据包需要在用户态/内核态之间进行拷贝、TCP/IP协议的层层解析和处理，消耗了大量的CPU和内存带宽。所以我们引入RDMA的RoCE v2技术来优化这部分的网络性能，存储组网相对可控，交换机进行PFC/ECN的无损网络配置，结合网卡的DCQCN拥塞控制算法可以实现基于无损网络的RDMA组网。</p><p>&nbsp;</p><p>在实际的适配过程中，使用UCX接口来加速客户端和存储节点以及存储节点之间的的RPC通信，这样可以降低整体的开发难度及对业务代码的侵入修改。为了保证可靠且高效的 RDMA 报文传输，一方面，我们与交换机厂商进行密切沟通，分析并摸索建立了一套适合于系统的ECN+PFC配置方式；另一方面我们与网卡厂商共同分析网卡DCQCN拥塞控制参数对RDMA流量传输的影响，最终得出最适宜的网卡参数配置。</p><p>&nbsp;</p><p>加速的效果：</p><p>&nbsp;</p><p>1. 在单深度4K随机写场景下，平均延迟降低 35%；4K随机读，平均延迟降低 57%；</p><p>2. 与杭研云原生数据BosonDB完成对接测试和上线。在数据库比较关注的16K随机/顺序写中，IOPS提升至少25%。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>随着云计算，大数据和AI等技术的不断发展，大量的计算资源之间的交互越来越频繁，对网络的要求也越来越高，臃肿的内核协议栈处理逐渐成为网络传输的瓶颈所在。本文介绍几种内核协议栈的优化技术、相关的应用场景以及实际的优化案例，供大家参考。</p><p>&nbsp;</p><p>作者简介：</p><p>&nbsp;</p><p>汪翰林，网易数帆高级技术专家，云网络数据面负责人。近20年软件架构、设计和研发经验，曾就职于华三和华为，主持多个安全、视频监控、大数据和云计算等技术领域的产品设计和研发工作。目前在网易数帆专注于高性能网络技术预研和产品化落地，带领团队完成VPC网络/容器网络/服务网格/存储/P4网关等多个场景下的高性能网络套件在网易集团内部落地以及商业化输出工作。</p>",
    "publish_time": "2023-10-07 17:05:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]