[
  {
    "title": "Hugging Face 大语言模型优化技术",
    "url": "https://www.infoq.cn/article/dAjSEe0AZw1GHuXZDROZ",
    "summary": "<p>大语言模型的生产部署存在两个主要的挑战，一个是需要大量的参数，一个是需要处理非常长的用于表示上下文信息的输入序列。Hugging Face基于他们提供大模型服务的经验<a href=\"https://huggingface.co/blog/optimize-llm?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">分享了一些克服这些障碍的技术</a>\"。</p><p></p><p>Patrick von Platen在文中介绍的Hugging Face研究的三种技术是降低数值精度、使用一种叫作Flash Attention的注意力算法，以及使用专门的推理架构。</p><p></p><p>大语言模型需要大量的VRAM来加载，从几十(bigcode/starcoder)到数百GB (Llama、Bloom、GPT3)。第一个优化手段是从float32切换到bfloat16精度：</p><p></p><p></p><blockquote>现在几乎所有的模型都是基于bfloat16训练的，如果你的GPU支持bfloat16，就没有理由基于全float32精度运行模型。float32不会给出比训练模型所使用的精度更好的推理结果。</blockquote><p></p><p></p><p>这可以使总体内存消耗减少一半，但可惜的是，在许多情况下仍然需要很大的内存。一种更激进的方法是将模型权重量化为8位或4位，这<a href=\"https://arxiv.org/abs/2208.07339?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">已经被证明不会导致显著的性能下降</a>\"。</p><p></p><p></p><blockquote>量化对于文本生成来说特别有效，因为我们所关心的是选择最有可能的下一个标记集合，而不是下一个标记Logit分布的确切值。</blockquote><p></p><p></p><p>这将进一步减少所需的内存，使得在只有16GB VRAM的GPU上运行较小的模型成为可能，尽管代价是推理时间稍长。</p><p></p><p>von Platen写道，使用<a href=\"https://arxiv.org/abs/2205.14135?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">Flash Attention</a>\"是另一相关键的优化，它是大语言模型用来理解输入标记上下文关系的自注意力层的一种算法，有可能打破输入标记数量的二次增长。</p><p></p><p>因为该算法太过复杂，无法在这里描述，但可以这么说，它利用了softmax规范化统计数据和一些数学手段，在只需要随输入标记线性增长的内存的情况下提供相同的输出。推理性能也得益于算法使用了更快的SRAM而不是更慢的GPU VRAM。</p><p></p><p></p><blockquote>在实践中，目前绝对没有理由不使用Flash Attention。该算法在数学层面给出了相同的输出，并且速度更快，内存效率更高。</blockquote><p></p><p></p><p>Here recent research can help to make the right choice with two components that quickly become bottlenecks, says von Platen, _positional embeddings_ and the _key-value cache_.</p><p></p><p>在生产环境中部署大语言模型的第三项优化措施是选择正确的架构，让它们能够有效地处理长文本输入。von Platen写道，最近的研究有助于我们如何对两个很快成为瓶颈的组件做出选择——一个是_位置嵌入(positional embeddings)_，一个是_键值缓存_。</p><p></p><p>位置嵌入通过将每个标记的位置编码为数字表示来帮助语言大模型理解序列顺序。对于需要处理大型文本输入任务的大语言模型，应该使用<a href=\"https://arxiv.org/abs/2104.09864?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">RoPE</a>\"和<a href=\"https://arxiv.org/abs/2108.12409?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">ALiBi</a>\"等相对位置嵌入技术进行训练。</p><p></p><p></p><blockquote>RoPE和ALiBi位置编码都可以外推到训练期间未遇到过的输入长度，而事实证明，与RoPE相比，外推对于开箱即用的ALiBi的效果要好得多。</blockquote><p></p><p></p><p>目前的许多大语言模型中已经在使用这两种算法。</p><p></p><p>键值缓存可以作为对对话上下文进行编码的一种方法。键值缓存在发生每个新交互时增加一个元素，这比为每个请求编码/解码上下文的方法要有效得多。von Platen详细介绍了两类键值缓存，即<a href=\"https://arxiv.org/abs/1911.02150?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">Multi-Query-Attention (MQA)</a>\"和<a href=\"https://arxiv.org/abs/2305.13245?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY2NDU0NTYsImZpbGVHVUlEIjoiVk1BUExhOXp4UlRRWDRBZyIsImlhdCI6MTY5NjY0NTE1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.nS1BUkwmY8dZaPmT8rfOJsLYT8fQGvZdsjLh1n39W-s\">Grouped-Query-Attention(GQA)</a>\" 。</p><p></p><p>von Platen的文章所涵盖的内容不只有本文所概述的这些，他的文章中还提供了实际的例子来证明他的观点，所以请不要错过他的文章。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/09/hugging-face-optimizing-llms/\">https://www.infoq.com/news/2023/09/hugging-face-optimizing-llms/</a>\"</p>",
    "publish_time": "2023-10-07 10:22:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一小时12元，我在北欧监狱里训练AI",
    "url": "https://www.infoq.cn/article/4v7RrYDw8M0ECaa6TBc2",
    "summary": "<p>芬兰工资水平普遍较高，并且很少有人从事互联网行业。外媒&nbsp;wired 实地走访发现，一家名为 Metroc 的大模型创业公司发现了一种新型劳动力——囚犯。</p><p></p><h2>芬兰囚犯的新工作：帮创业公司训练大模型</h2><p></p><p>&nbsp;</p><p>在一个没有窗户的房间里，隔着一张消过毒的白色桌子，我被介绍给了一位四十多岁的女性，她有着方形下巴，用一个淡蓝色的发带把金色的头发扎成了马尾。她说：“大家都叫我果酱”，让我也这么称呼她。</p><p>&nbsp;</p><p>一个星期三的早晨，在这座芬兰的监狱里，果酱给我们演示了一种新型的监狱劳动形式。</p><p>&nbsp;</p><p>桌子上只有一小塑料瓶水和一台 HP 笔记本电脑。她们每三小时轮班一次，每小时可以获得 1.54 欧元（约合 12 元人民币）的报酬。这台笔记本电脑用来向果酱展示关于房地产的短文，并就她刚刚读到的内容问她是或否的问题。其中一个问题是：“上面这段话说的是房地产决策而不是申请，对吗？”</p><p>&nbsp;</p><p>“有点无聊，”果酱耸了耸肩，她也不太清楚这项任务的目的。她认为，\"也许她正在帮助创建一个客服聊天机器人\"。</p><p>&nbsp;</p><p>事实上，她正在训练一款由芬兰创业公司 Metroc 开发的大型语言模型。该公司创建了一个搜索引擎，旨在帮助建筑公司找到新批准的建设项目。为了做到这一点，Metroc 需要标注员帮助其模型理解新闻和市政文件中关于即将开展的建设项目的线索。例如，人工智能必须能够区分已经委托给建筑师或正在安装窗户的医院项目和可能仍在招人的项目。</p><p>&nbsp;</p><p>在全球范围内，有数百万所谓的“网络工作者”在训练人工智能模型，教机器区分行人和棕榈树，或者描述暴力或性侵害的词语组合。通常，这类工作人员来自南半球，因为那里的工资比较低。例如，OpenAI 就用了一家外包公司，该公司在肯尼亚、乌干达和印度招聘了网络工作者。这种安排非常适合美国公司，因为它们使用全球使用最广泛的语言英语，但在南半球很难找到讲芬兰语的人。</p><p>&nbsp;</p><p>这就是为什么 Metroc 转向了监狱劳动力。该公司获得了廉价的、会讲芬兰语的工人，而监狱系统则可以为囚犯提供就业机会，也为他们出狱后进入数字化领域工作做好准备。利用囚犯来训练人工智似乎有点像科技领域下游经常存在的对廉价劳动力的剥削。但在芬兰，这个项目得到了广泛的支持。</p><p>&nbsp;</p><p>“数据劳动力是一个全球性的概念。但如果你仔细观察一下就会发现，芬兰的情况截然不同。”来自赫尔辛基大学的研究员图卡·莱赫蒂尼米（Tuukka Lehtiniemi）说，他一直在研究芬兰监狱中的数据劳动力。</p><p>&nbsp;</p><p>果酱在哈米纳林纳监狱已经呆了四个月。这座现代化的建筑有着很大的窗户。空旷的走廊上，色彩丰富的艺术品正努力营造出愉快的氛围。要不是因为厚重的灰色安全门挡住了每个进出口，你很容易就会以为，这些房间属于一所毫无灵魂的大学。</p><p>&nbsp;</p><p>芬兰监狱的开放性是出了名的，囚犯可以在附近的城镇工作或学习，但哈米纳林纳监狱不属于这一类。相反，哈米纳林纳监狱是芬兰安全级别最高的监狱，只收容女性囚犯。果酱被判了六年。根据监狱的隐私规定，wired&nbsp;不能发布她的真实姓名、确切年龄或其他任何可能让人识别出她身份的信息。在这个无期徒刑囚犯服刑 12 年后就可以申请刑满释放的国家里，六年是重刑。和其他 100 名住在这里的囚犯一样，她也不被允许离开监狱。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04e874cfd11a928cf1522b07438b7a59.png\" /></p><p>&nbsp;</p><p>当果酱第一次来到监狱的时候，她会看着其他女囚每天早上起床去工作：她们可以自愿做清洁、洗衣或缝纫。每六小时轮班一次，她们可以获得大约 6 欧元（约合 46.6 元人民币）的报酬。但果酱无法忍受这些工作。“我会觉得非常累，”她说。为此，有很长一段时间，她就呆在牢房里，直到有一位监狱辅导员建议她尝试“人工智能工作”。三小时一轮班吸引了她，至于报酬，有总比没有强。“虽然不多，但比呆在牢房里强，”她说。截至目前，她只轮过三次班，但已经获得了成就感。</p><p>&nbsp;</p><p>这所监狱允许囚犯通过数据工作赚钱。在芬兰，这样的监狱只有三所。每所监狱都备有三台笔记本电脑，供囚犯参与这项人工智能工作时使用。这项工作没有具体的目标，囚犯按小时取酬，而不是按工作速度或质量。</p><p>&nbsp;</p><p>在哈米纳林纳监狱，大约有 20 名囚犯尝试过这项工作。监狱工作导师米娜·英基宁（Minna Inkinen）留着红色的短发，她坐在果酱旁边和我们交谈。她说：“有些人确实比其他人更喜欢人工智能工作。”当我在一个星期三的早晨到到达这所监狱时，缝纫室已经忙碌了起来。囚犯们或忙着操作缝纫机，或在织物旁商量事情。但在果酱到达之前，开展人工智能工作的小房间里空无一人。英基宁解释说：”总共只有三名囚犯自愿定期参加人工智能工作，而另外两人目前正在上法庭。“果酱补充说：“我更喜欢在一个团队中做事。”她房间的门一直敞开着，这样她就可以在回答问题的间隙，与隔壁正在缝纫的狱友聊天。</p><p>&nbsp;</p><p>那些问题是我在监狱以南 100 公里外的赫尔辛基的一家现代化共享办公室内手写的。在那里，我见到了个子高挑、少年感十足的 Metroc 创始人兼首席执行官尤西·维尔纳拉（Jussi Virnala）。他带着我路过一排室内秋千、一张台球桌和一群西装革履的男士，来到一个异常闷热的电话间。他解释说，这一周真让人兴奋，公司刚刚完成了一轮 200 万欧元（约合 1554 万元人民币）的融资，他计划用这笔钱来扩展北欧市场，投资者对公司与芬兰监狱的关系很感兴趣。他说：“每个人都激动不已，对这种创新方式很感兴趣，我认为从产品方面来看，这非常有价值。”</p><p></p><h2>数据标注是个好工作吗？</h2><p></p><p>&nbsp;</p><p>将囚犯发展为劳动力的想法是维尔纳拉提出的。他们公司需要母语为芬兰语的人来帮助他们改进其大型语言模型理解建筑行业特有的语言。但在像芬兰这样的高薪经济体中，很难找到这样的数据劳动力。芬兰的福利体系可以提供可观的失业救济金，这就意味着很少有芬兰人会主动在类似亚马逊网络交易平台这样的网络工作平台上注册。“上面没有多少芬兰语工作人员，”维尔纳拉说，同时他还补充道，“自动翻译工具仍然不能很好地处理芬兰语，毕竟以芬兰语为母语的人总共也才 500 万。”</p><p>&nbsp;</p><p>当维尔纳拉向芬兰监狱和青少年教养所的智能监狱项目负责人皮娅·普拉卡（Pia Puolakka）提出他的想法时，她立刻表现出了浓厚的兴趣。她说，在人工智能火起来之前，另一家名为 Vainu 的芬兰科技公司曾经也试过用囚犯做数据劳动力，但其联合创始人之间的分歧导致项目负责人图奥马斯·拉西拉（Tuomas Rasila）离开了公司，Vainu 也就退出了这个项目。</p><p>&nbsp;</p><p>到 2022 年维尔纳拉提出他的提议时，普拉卡非常想恢复人工智能工作。她的工作是设法加强芬兰监狱与互联网之间的联系，使监狱更接近日益数字化的外部世界。到目前为止，监狱的独立牢房一直都配有笔记本电脑，以便囚犯可以浏览有限的网站并申请视频通话许可。她认为，数据劳动力也是这项任务的一部分。</p><p>&nbsp;</p><p>这项工作的目的不是为了取代传统的监狱劳动力，比如制作道路标志或园艺工作，它的目标是为囚犯提供更多的工作类型。数据标注员三小时就轮一次班。“如果一天八小时都只做这种工作，可能会让人觉得很累，”她补充说，如果囚犯可以将数据标注与其他类型的监狱工作并行开展，那就更好了。她说，“这项工作是面向未来的，如果要为囚犯出狱后的生活做准备，那么这些技能至少与监狱提供的传统工作类型一样重要”。</p><p>&nbsp;</p><p>然而，数据标注可以为囚犯提供多少可用于出狱后的工作技能还不清楚。作为 Vainu 公司联合创始人之一的图奥马斯·拉西拉（Tuomas Rasila）曾在那里管理了一年的监狱项目，他承认自己没有这方面的证据。他说，这个项目的运行时间还不足以收集证据，“我认为，让可能与社会脱节的人去学习现代社会最先进的技术是一个不错的赋能理念。”</p><p>&nbsp;</p><p>其他人认为，这种新形式的监狱劳动力可能会加剧人工智能革命所带来的廉价劳动力问题。“我们正朝着一个更便捷高效的全自动化社会发展，但这往往掩盖了这样一个事实，即许多系统实际上都是依赖于人的”，来自人权观察的人工智能高级研究员阿莫斯·陶（Amos Toh）如是说。</p><p>&nbsp;</p><p>在陶看来，对于网络工作者需求的增加已经引发了一种趋势，即公司更多地转向了那些几乎没有其他选择的人群：难民、国家陷入经济危机的人，现在是囚犯。</p><p>&nbsp;</p><p>“这种情况很常见，”陶说，“我们这里看到的只是一个更广泛的现象的一部分，即企业正在将技术开发背后的工作外包给可能在剥削性工作条件下劳动的工人。”</p><p>&nbsp;</p><p>对于数据工作是否能帮助囚犯培养数字技能，陶还也是持怀疑态度。“在监狱里，囚犯有很多提升自己的方式，比如考取证书和参加高等教育，”他说，“但我觉得，以每小时一欧元的价格为一家公司标注数据未必能帮他们取得有意义的进步。”哈米纳林纳监狱确实为囚犯提供了人工智能在线课程，但当工作人员试图解释其好处的时候，果酱坐在那里，面无表情。</p><p>&nbsp;</p><p>在我与来自赫尔辛基大学的研究员莱赫蒂尼米见面后，我对于监狱项目的优点有些不那么确定了。从监狱来到 Metroc 的办公室，监狱里的女性干着每小时 1.54 欧元的工作，而公司正在庆祝 200 万欧元的融资轮，这感觉非常不协调。在赫尔辛基大教堂对面的一家咖啡馆里，莱赫蒂尼米耐心地听我描述了这种感觉。</p><p>&nbsp;</p><p>但对囚犯的采访让莱赫蒂尼米有了不同的看法——他对这个项目总的来说是持积极态度的。至于薪酬差距，他认为，这些人是在监狱里，并不是主流社会中的普通劳动力。“将我作为研究员所获得的报酬与囚犯在监狱里劳动所获得的报酬进行比较，是没有意义的，”他说，“我唯一听到的负面意见是这样的工作不够多，只有很少的人可以做。”他提到了每所监狱只有三台笔记本电脑这个限制。</p><p>&nbsp;</p><p>“当我们提起数据劳动力时，我们往往会想到网络交易平台，全球南部或美国农村的人，”他说。但对他来说，这是数据劳工的一个独特的本地版本，它带来了有益于社会的转变。与其他监狱劳动力相比，它为囚犯提供了认知刺激的工作，同时也代表了芬兰语言在人工智能革命中的地位。</p><p>&nbsp;</p><p>莱赫蒂尼米担心，如果没有这种主动性，英语之外的语言将被下一代技术所淘汰，智能音箱仍然难以理解芬兰语。“并非所有芬兰人都能说一口流利的英语，所以在当地进行的数据标注还是有必要的，”莱赫蒂尼米说。Metroc 并不是唯一一家被迫寻找芬兰数据劳动力的公司。2011 年，国家图书馆发明了一款游戏，以激励志愿者帮助他们数字化其归档资料。2020 年，广播公司 YLE 与赫尔辛基大学及国家发展公司 VAKE 合作，请求志愿者捐赠他们的芬兰语录音。</p><p>&nbsp;</p><p>在某种意义上，芬兰的监狱项目只是一个开始。有些人担心，这可能会开创一个先例：在监狱中引入更具争议的数据标签类型，比如弱化暴力内容。“即使目前在芬兰进行的数据标注没有争议，我们也必须考虑它所开创的先例，”陶说，“有什么能防止公司将有创伤性和不雅内容的数据标注外包给监狱中的人，尤其是如果他们认为那是一个待开发的劳动力资源？”</p><p>&nbsp;</p><p>芬兰的监狱以帮助犯人改过自新而闻名，不知道芬兰监狱里的劳动条件在其他司法没那么先进的国家是否同样适用。根据公民权利团体美国公民自由联盟（ACLU）的数据，76% 的囚犯说监狱劳动是强制性的。拉西拉说，“美国的监狱系统与芬兰或北欧国家有很大的不同，理念完全不同。在芬兰，人们会积极推动这个项目，因为每个人都知道这是自愿的。”</p><p>&nbsp;</p><p>人工智能公司需要的数据劳动力只会越来越多，为了跟上发展的步伐，它们就不得不寻找非同寻常的劳动力。随着 Metroc 规划扩展到北欧以及芬兰以外的语言，维尔纳拉正在考虑是否将监狱劳动力项目扩展到其他国家，她说“这是我们需要探索的事情”。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.wired.com/story/prisoners-training-ai-finland\">https://www.wired.com/story/prisoners-training-ai-finland</a>\"</p>",
    "publish_time": "2023-10-07 10:26:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阳光保险集团人工智能部大模型首席专家张晗确认出席 FCon ，分享大模型技术在保险行业的创新应用与未来发展",
    "url": "https://www.infoq.cn/article/1AT3vxwwWpMKeZt8pXNb",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。阳光保险集团人工智能部大模型首席专家张晗将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5560?utm_source=infoqweb&amp;utm_medium=article\">大模型技术在保险行业的创新应用与未来发展</a>\"》主题分享，介绍大模型技术以及它在保险行业中的具体应用、通用能力全员应用的发展和应用范围，并分析保险领域专业大模型的关键突破，以及智能理赔机器人在人伤赔偿模式上的革新应用。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5560?utm_source=infoqweb&amp;utm_medium=article\">张晗</a>\"，现任阳光保险集团人工智能部大模型首席专家，毕业于北京理工大学，曾就职于腾讯、美团等互联网公司，长期从事搜索推荐算法相关工作，2021 年加入阳光保险集团人工智能部，负责“知周”智能对话平台、正言大模型开放平台的研发建设。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型技术在保险行业的创新应用与未来发展</p><p></p><p>大模型技术正蓬勃发展，渗透至各行各业中，阳光保险也紧跟潮流，展开了一系列的创新探索和实践，并已取得了显著的效果提升。在本次演讲中，我将向大家详细介绍大模型技术以及它在保险行业中的具体应用，重点探讨阳光正言 GPT 战略工程与保险领域的紧密结合与其重要性，并分享通用能力全员应用的发展和应用范围。最终，我将深入分析保险领域专业大模型的关键突破，以及智能理赔机器人在人伤赔偿模式上的革新应用。</p><p></p><p>演讲提纲：</p><p></p><p>大模型技术带来的新机遇</p><p>○ 大模型技术简介 </p><p>○ 大模型技术在保险业的应用形势</p><p>阳光正言 GPT 战略工程与保险领域结合的重要性</p><p>○ 阳光正言 GPT 战略工程的重点规划 </p><p>○ 大模型底座的建设与重构科技能力 </p><p>○ 全面赋能保险业务的阳光 GPT 工程底座整体架构</p><p>通用能力全员应用的推进与应用范围</p><p>○ 文本生成的通用能力应用 </p><p>○ 文生图与寿险营销的应用实践 </p><p>○ 常青藤编程的全员应用探索</p><p>保险领域专业大模型的重点突破</p><p>○ 保险领域专业大模型的打造机器人产品生态 </p><p>○ 机器人独立完成寿险销售新范式 </p><p>○ AI 全流程独立销售车险产品的新模式 </p><p>○ 改写人伤赔偿模式的智能理赔机器人</p><p></p><p>你将获得：</p><p></p><p>○ 了解阳光正言大模型在保险领域的实践</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-10-07 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "高效能不等于开发快，大模型时代如何正确提升研发效能？",
    "url": "https://www.infoq.cn/article/pzC0XXGzSWJwPkIOwSlz",
    "summary": "<p>从最初的敏捷软件开发方法到DevOps成熟度模型，研发效能的发展历程经过多个阶段。如今，基于大模型的AIGC技术正在催生软件工程的新范式，为研发效能的提升带来新的可能性。目前，越来越多的企业开始在实际的研发工作中，结合大模型增强软件开发在设计、需求、测试、发布和运维等各个环节中的能力，提高质量和效率。</p><p>&nbsp;</p><p>在今年 9 月 3-5 日举办的&nbsp;<a href=\"https://qcon.infoq.cn/202309/beijing/\">QCon 全球软件开发大会·北京站</a>\"中，Thoughtworks 中国区总经理肖然担任《<a href=\"https://qcon.infoq.cn/202309/beijing/track/1567\">AIGC 浪潮下的研发效能提升</a>\"》专题出品人，该专题探讨了 AIGC 浪潮下，大模型对软件研发工作流的改变，以及大模型是如何提升研发效能和质量的。以下为 InfoQ 与肖然的对话实录，经编辑。</p><p></p><h2>大模型时代下的研发效能提升</h2><p></p><p>&nbsp;</p><p>InfoQ：您作为 2023 QCon 全球软件开发大会·北京站《AIGC 浪潮下的研发效能提升》专题出品人，能分享下您对这个主题的理解吗？对于这波大模型结合软件开发应用热潮，您观察到哪些有趣的趋势？</p><p>&nbsp;</p><p>肖然：ChatGPT一经推出，全球最活跃的是很多技术自媒体，给大家展示自动化的代码生成，一些实例甚至直接生成可运行的小应用和游戏。目前最成功的GPT应用是GitHub为开发人员推出的Copilot，甚至于这个单词成了系列AI应用的代名词。所以说，大模型在软件研发中的应用实际上已经开始了。</p><p>&nbsp;</p><p>软件工程领域有一个大家比较认可的定义，即软件开发是人类历史上最复杂的脑力协作。“脑力”给我们带来了工作量度量的麻烦，我们没法像控制肢体运动一样控制思考；“协作”当然也不是免费的，要让另外一个人按照你的想法来做事情，沟通和理解的成本很高。由此也造成长久以来软件研发效能管理上的巨大黑洞。</p><p>&nbsp;</p><p>这两年由于数字化的深化，整个社会全产业对于软件的依赖性提升很快，客观上就推动了软件研发团队和组织的快速扩大。人多了自然效能管理就更重要了，但软件本身的“人月神话”等悖论，明确告诉我们用传统方式一定是越管越慢。虽然类似DevOps、CloudNative这些运动在向着正确的方向推动我们对效能度量和效能管理的认知，但实际上我们还是缺乏一些本质上的治理手段。</p><p>&nbsp;</p><p>所以当ChatGPT出现后，在不同的技术社区就开始发酵，大家看到了这一波基于大模型的AIGC技术带来的可能性。我们以前重来没有思考过的效能提升视角也逐渐浮现出来，比如研发团队不同专业之间的知识管理问题，之前我们还是在不停地鼓励和训练大家换位思考、高效沟通，现在出现了一个可以包容各类专业知识的大模型，这个“超级队员”之前是不存在的。而这个超级队员的出现，必然会给我们带来新的效能提升思路和方式。就《AIGC 浪潮下的研发效能提升》这个专题，是值得我们接下来几年持续研讨的，也会是研发效能治理领域最热门的一个赛道。</p><p>&nbsp;</p><p>InfoQ：有观点认为研发效能已经成为一家科技公司的核心竞争力，您是否认同？根据您的行业观察，这些年来企业的研发效能发生了哪些变化？</p><p>&nbsp;</p><p>肖然：首先我们还是要明确研发效能的定义，目前也不是完全统一。比较好的定义可以参考类似DORA这样的全球报告，国内也有一些专家小组做了比较好的定义。总体上我们应该避免“高效能就是开发快”这样一个认知误区。当然，这些年在效能领域的一个显著变化是大家认知更透彻了，很多企业还结合自身的业务特点在看待研发效能，比如银行业监管机构都提出了“双模”，即两种研发节奏，明确不是所有系统开发都追求快，要适配业务模式。</p><p>&nbsp;</p><p>在正确的效能定义的前提下，确实研发效能高是一家科技公司的核心竞争力。本质上未来的很多公司都是科技公司，因为业务在大面积的数字化，由此也带来了很多公司不断提升自身的科技人员占比。从这一点出发，这些年来企业在研发效能治理上投入是逐年增加的。很多企业抓住DevOps这个切入点，开始系统性的看待研发效能问题，从端到端的价值流视角来建立分析和改进体系。这点从行业角度看是可喜的。</p><p>&nbsp;</p><p>当然我们也存在比较大的管理效能指标的问题。很多企业管理着希望能够“看见”，所以开始建立效能方向的指标体系。但这种通过指标体系来管理的方式也容易走上治标不治本的道路，软件开发过程中处理的复杂度很难通过指标来说明问题。本质上研发团队和专业人员的能力提升才是核心，不要因为建立了指标反而忽视了效能治理的关键命题。目前看大模型的出现也并不能替代研发专业人员，而未来的应用和系统因为大模型的加入会变得更加复杂。</p><p>&nbsp;</p><p>InfoQ：过去大家提到研发效能一个比较头疼的点是，如何正确、有效的度量，结合 AI 技术，研发效能度量发生了哪些变化？AI 大模型在研发效能提升方面还有哪些独特的优势和潜力？</p><p>&nbsp;</p><p>肖然：度量在过去几年有比较大突破，特别是DORA经过研究发布了DevOps领域的4KM（四个关键指标）。软件研发的度量关键是尽量面向端到端的价值流，设计指标时关注协同效率，而不是单兵的生产效率。</p><p>&nbsp;</p><p>大模型这个“超级队员”的加入，实际上让我们更加容易进行端到端的度量，很多协同工作是可以通过大模型来自动完成的，自然就形成了更为完整的过程数据。目前应用大模型上比较火热的是AI Agent，我们可以预期未来针对效能度量和分析都会有相关的Agent出现。</p><p></p><h2>Thoughtworks是如何采用大模型技术的？</h2><p></p><p>&nbsp;</p><p>InfoQ：Thoughtworks 围绕大语言模型结合软件开发有哪些探索？您能分享 1-2 个具体的案例，以及你们在其中的思考/踩坑经验吗？</p><p>&nbsp;</p><p>肖然：首先肯定是类似Copilot这样工具在开发过程中的应用。由于TW崇尚结对编程的实践，所以Copliot的接受度很高。这种模式其他研发角色如BA和QA都会用自己的工具尝试，总体上我们认为是现有专业工作的增强，效果是明显的，比如QA在准备测试用例时采用大模型来自动准备，仅测试用例的数据准备就能够从半天缩短到十几分钟。</p><p>&nbsp;</p><p>另外一个类型的尝试就是关注专业角色的协同，比如我们开源的Boba平台（<a href=\"https://martinfowler.com/articles/building-boba.html%25EF%25BC%2589%25EF%25BC%258C%25E5%25B0%25B1%25E6%2598%25AF%25E6%2595%25B4%25E5%2590%2588%25E4%25BA%2586%25E5%25A4%259A%25E4%25B8%25AA%25E7%259B%25B8%25E5%2585%25B3%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E5%25BA%2594%25E7%2594%25A8%25EF%25BC%258C%25E5%25AE%258C%25E6%2588%2590%25E4%25BB%258E%25E5%25B8%2582%25E5%259C%25BA%25E7%25A0%2594%25E7%25A9%25B6%25E5%2588%25B0%25E9%259C%2580%25E6%25B1%2582%25E6%258B%2586%25E5%2588%2586%25E7%259A%2584%25E4%25BA%25A7%25E5%2593%2581%25E8%25AE%25BE%25E8%25AE%25A1%25E5%2585%25A8%25E8%25BF%2587%25E7%25A8%258B%25E3%2580%2582\">https://martinfowler.com/articles/building-boba.html），就是整合了多个相关大模型应用，完成从市场研究到需求拆分的产品设计全过程。</a>\"这种尝试目前还没有专业增强那么成熟，更多起到了“help me to learn”的效果。但我们认为这个方向在研发领域是潜力无限的。</p><p>&nbsp;</p><p>踩坑主要还是数据和信息安全问题，为了得到更为准确的生成结果，不可避免我们需要提供更多的上下文给大模型。目前类似OpenAI这样的大模型厂商并不能提供企业级数据和信息安全的保障，所以往往一些核心业务系统仍然难于使用。随着越来越多的开源模型发布，我们也帮助不少企业开始部署和微调私有的大模型。私有大模型一般会采用企业自己的系统作为语料去微调模型，在采用了类似Llama 2这样的基础模型之后，生成代码的可用度已经能够达到50%以上。</p><p>&nbsp;</p><p>也有企业从成本和风险角度考虑，希望仍然采用公有大模型，这个时候我们就需要针对企业数据进行脱敏处理，并且建立相关的矢量存储来作为企业私有知识的管理。目前相关的架构在逐步稳定，开源的工具也越来越多。</p><p>&nbsp;</p><p>InfoQ：当前 AI 研发效能提升的技术瓶颈和挑战是什么？如何评估 AI 研发效能提升技术的性能和效果？</p><p>&nbsp;</p><p>肖然：目前最大的挑战实际不在于大模型本身，而在于人员能力的提升。大部分研发人员开始时都是单一问题的0-shot prompt，不能够很好的和模型互动，由此得到的生成结果也不尽如人意。</p><p>&nbsp;</p><p>另外一个挑战就是很多企业认为只要有了大模型，每个人跟模型提问互动就是应用了。曾经在Hadoop兴起的大数据时代，很多企业也认为只要部署了Hadoop，就是拥有了大数据能力。显然如果希望大模型在企业里变得普适可用，有很多工程化和平台化的基础工作是要预先设计和部署的。</p><p>&nbsp;</p><p>目前其实还不适合去做评估，可以进行一些数据的采集，反应大家真实的使用感受。评估很可能带来的副作用是大家不愿意真实的反应实际情况。比如一个季度前，我在内部和BA社区开会研讨，很奇怪为什么使用反馈很少。线下找到老同事了解，才知道原来大家担心公司管理层知道了使用大模型提升效率，造成后续更多派活或直接减员。显然这个担心是多余的，但评估可能传递这样的错误信号。</p><p>&nbsp;</p><p>就当前阶段，我的建议还是在条件允许的情况下，鼓励大家多使用、多尝试，欢迎大家提炼总结新问题和新方法。</p><p>&nbsp;</p><p>InfoQ：目前市面上存在很多结合大模型的研发效能工具，但在一些企业的端到端落地过程中并不理想，也没有实现提效的突破，这背后存在哪些挑战？在不同场景下，如何选择和调整 AI 研发效能提升技术来满足不同的需求？</p><p>&nbsp;</p><p>肖然：首先还是需要明确采纳的方向和目的，如分享TW采用大模型经验，我们会从“专业增强”和“协同增效”两个主要方向去考虑大模型的应用：</p><p>&nbsp;</p><p>专业增强实际上在开发、测试和UI等领域已经有比较成熟的工具。值得关注的是需求方面的工具，潜力是大家共识的，但工具方面还有待创新。协同增效方面类似LangChain这样的工具已经被不少研发组织采用，当然大模型本身的生成内容准确度还是决定性因素。生成质量不高的内容很可能适得其反，提高了协同成本。当然LangChain仅仅是一个开始，目前的很多Agent已经在自动化地完成一些跨职能的协同工作。</p><p>&nbsp;</p><p>基于这两个方向，可以考虑不同的具体场景，场景选择上要结合研发组织自身的特点。比较好的方式是举办内部的创新应用比赛/黑客松，利用这样的形式让更多的人来一起想、一起实验。由于大模型技术本身仍然在快速迭代，依靠自上而下地规划反而容易造成应用不接地气，难有真正成果。</p><p></p><h2>大模型最大的价值是知识管理</h2><p></p><p>&nbsp;</p><p>InfoQ：您认为大模型在软件研发工作流中最大的价值是什么？大模型对软件研发工作流的改变，将会如何影响软件开发行业的未来发展趋势？</p><p>&nbsp;</p><p>肖然：软件研发本身是隐式知识的显式化过程，通俗讲就是用户开始说不清楚要什么，之后通过产品一轮又一轮的迭代慢慢清晰。从这点出发，我认为大模型在软件研发过程中最大价值是知识管理，因为这个“超级队员”的知识存储能力超过了任何人类个体和团队。</p><p>&nbsp;</p><p>一旦大模型真正有效成为了知识的管理员，我们软件研发的专业分工就要发生变化。这种变化还不仅仅是我们现在可以看到的“全栈工程师的复兴”，而是真正意义上的角色重新定义。当然这并不意味着我们专业人员变少了，相反新的专业分工可能出现，比如维护大模型的工程师、测评大模型的分析师等等。</p><p>&nbsp;</p><p>我们已经无法预测未来的发展趋势，但我想在开放的心态下，我们应该躬身入局，建立自己的感知网络，从而能够持续进化。</p><p>&nbsp;</p><p>InfoQ：大模型会对程序员带来哪些冲击？程序员如何和大模型更好地共生？</p><p>&nbsp;</p><p>肖然：程序员需要更加关注原则和设计。大模型自动生成代码和应用只会日趋完善，但生成的质量仍然是需要程序员来判断，一些关键问题，如性能和安全，更是需要程序员来负责。所以程序员需要更多思考一些原则和本质的东西，这样才能支持有效的判断。</p><p>&nbsp;</p><p>大模型是生成式的AI，生成内容的质量很大程度取决于问题的质量，也就是我们现在经常谈的prompt engineering（提示语工程）。目前很多模式正在被提炼和总结出来，每个程序员都应该持续学习。但即使有了问题的模式，问题的内容仍然是程序员个体决定的。这就像使用TDD的高手，面对复杂需求总能够庖丁解牛一般找到合理的任务拆分。同理，能够通过prompt一步步引导大模型生成高质量的内容本身就是一项关键能力，而这个能力跟程序员的设计思考是密切相关的。只有善于设计的人，才能够和大模型进行有效的互动。</p><p>&nbsp;</p><p>InfoQ：AIGC 的未来发展和趋势是什么？您认为未来 AIGC 技术会对研发效能提升带来哪些新的机遇和挑战？</p><p>&nbsp;</p><p>肖然：在软件研发上下文下，我觉得AIGC最重要的发展趋势是多模态MultiModal，即听说读写样样精通。结合前面提到的知识管理，研发效能的提升将很快突破单个专业的提效，产生整体质的飞跃。想象未来客户描绘了一个场景，大模型帮助下快速转换为视频故事，在做产品前就能够让客户有身临其境的感觉，同时也可以通过高度的可视化让团队快速共识理解。</p><p>&nbsp;</p><p>这样的可能性在即将到来的多模态时代应该说潜力无限。软件研发对于每个从业者来说最重要的还是持续提供可学习的知识。而通过多模态，我们专业个体的学习能力也会被千百倍的放大。作为一个专业研发人员，我也很期待将大模型多模态的能力应用到我们的研发过程中去。</p><p></p><h4>采访嘉宾</h4><p></p><p></p><p>肖然，Thoughtworks 中国区总经理，中关村智联创新联盟秘书⻓。在过去 10 年时间里，肖然带队先后为金融、保险、通信、物流、零售等核心产业的头 部企业提供了⻓期的从战略执行到组织运营各个方面的咨询服务，以务实的工作作⻛得到了行业内的广泛认可，也成为了中行、招行、华为等头部企业的高管参谋，为企业的⻓期发展出谋划策。</p>",
    "publish_time": "2023-10-07 11:57:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]