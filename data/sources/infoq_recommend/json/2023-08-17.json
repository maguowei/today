[
  {
    "title": "基于 NNI 的 Transformer 系列模型压缩实践",
    "url": "https://www.infoq.cn/article/6mA1gDVFWU1oj1ZdQyD2",
    "summary": "<p>本文整理自微软亚洲研究院研发工程师张鑫在 <a href=\"https://qcon.infoq.cn/2023/beijing/\">QCon 2023</a>\" 的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2023/beijing/presentation/5161\">基于 NNI 的 Transformer 系列模型压缩实践</a>\"”。</p><p>&nbsp;</p><p>分享从四个方面展开。第一部分介绍一站式 AutoML 工具 NNI，第二部分介绍模型压缩模块，第三部分介绍基于 Transformer 系列模型的压缩实践，最后是结论。</p><p>&nbsp;</p><p></p><h2>前言</h2><p></p><p>大家上午好，我叫张鑫，是微软亚洲研究院的一名研发工程师。今天非常荣幸能够站在这里和大家分享我们的工作，今天分享的大主题是机器学习模型效率与易用性，下面我将基于 NNI 的Transformer 系列模型压缩实践为大家进行介绍，我将从以下 4 个方面为大家进行介绍。</p><p>&nbsp;</p><p>首先我将向大家介绍 NNI 这个一站式的自动机器学习工具，接下来我将和大家一起分析一下大模型的发展现状，并向大家介绍一下 NNI 中模型压缩模块的 Pipeline 及各方法的运行原理。随后我们将使用 NNI 中的模型压缩模块，在 Transformer 系列模型上进行一系列的压缩实践。在这个板块中，我们将首先和大家一起回顾一下 Transformer 的模型结构，并对Transformer 系列的模型进行分析。接下来我们会给出详细的压缩流程和具体的实验结果，最后进行总结。</p><p>&nbsp;</p><p></p><h2>NNI：一站式 AutoML 工具</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e89f5b6a7e901375a9e954961b32e41e.png\" /></p><p></p><p>&nbsp;</p><p>NNI 是一个开源的自动机器学习工具，它非常容易安装，且与现有的工具包环境兼容，用户在使用 NNI 时无需修改现有的 Python 代码即可轻松使用。同时 NNI 还为用户提供了简单的命令行和可视化的工具支持。NNI 中包含了多种主流算法，其能快速适应最先进的算法，具有极强的可扩展性，能够轻松集成自主开发的算法，我们可以把开发的算法部署到不同的平台上，尤其是云端。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cdabe4b8e4152a909b63ecf43778097.png\" /></p><p></p><p>&nbsp;</p><p>下面我们看一下 NNI 的具体模块构成。如图所示可以看到 NNI 中主要包含 4 个主要模块，第一个是神经网络架构搜索，也就是现在说的 NAS，其致力于为给定的任务寻找最优的网络结构。</p><p>&nbsp;</p><p>第二个是超参调优模块，也就是这里的 HPO 模块，即致力于用较少次数的尝试帮助用户自动化调优参数，以提升模型的性能。</p><p>&nbsp;</p><p>第三个是模型压缩模块，其致力于在尽可能少的降低模型精度的情况下，尽量的减少模型的参数和计算复杂度，以获得较低的计算延时。</p><p>&nbsp;</p><p>最后一个是特征工程模块，其致力于为机器学习算法抽取重要的特征，同时 NNI 也为用户提供了命令行工具和可视化的 Web 界面，以帮助用户批量化管理实验。</p><p>&nbsp;</p><p>此外 NNI 还提供了对多种计算资源的支持，如远程子服务组训练平台等，最大可能地方便用户的使用。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc3b882394ba7e01b9b4b3effa7e9751.png\" /></p><p></p><p>&nbsp;</p><p>本次重点向大家介绍我们的模型压缩模块，自 2018 年以来 NNI 已发布了多个版本，其凭借着强大的功能和易用性，在 GitHub 上已收获了 12.4k 的 Star，合约 1.7k 的 Folk，在即将发布的 v3.0 版本中，我们的大部分模块也将迎来重大更新，特别是我们 NNI 中的模型压缩模块，其在现有的量化剪枝模块的基础上，进一步引入了蒸馏模块，实现了蒸馏、剪枝、量化的一体化。</p><p>&nbsp;</p><p></p><h2>模型压缩模块</h2><p></p><p></p><h3>大模型发展现状</h3><p></p><p>在正式介绍模型压缩模块的工作原理前，让我们先一起来看一下大模型的发展现状，大规模预训练模型在人工智能领域扮演着重要的角色，其极大地推动了各领域的发展，然而随着模型规模的爆炸式增长，现有的计算资源已经越来越难以负担大规模模型的学习。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/db08174d2820e9e56aa265d58707e188.png\" /></p><p></p><p>图片来源：<a href=\"https://arxiv.org/pdf/2202.01306.pdf\">https://arxiv.org/pdf/2202.01306.pdf</a>\"</p><p>&nbsp;</p><p>如图所示，我们可以看到从 BERT 到 GPT3 再到 GShard，其模型参数从几亿规模逐渐爆炸式增长到了千亿、万亿级别。</p><p>&nbsp;</p><p>同时我们可以发现在实际的部署应用中，其庞大的参数量和运算量对服务端的软硬件资源带来了极大的挑战，因此模型压缩成为了模型部署前不可缺少的重要环节。</p><p>&nbsp;</p><p>然而由于模型压缩的技术和方法的多样性，且涉及模型推理延迟的编译优化，使得模型压缩过程变得繁琐而复杂，因此为了打通从模型压缩到模型加速的整个流程，我们开发了 NNI 的模型压缩模块，即包含了当前主流的模型压缩算法，剪枝量化和蒸馏，下面分别简单介绍一下这三种方法。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b0c72c247d8757849d82d91b7c82c7e.png\" /></p><p></p><p>&nbsp;</p><p>量化方法是对模型中的权重激活从高精度转化为低精度的过程，比如说将模型中的权重从float32 转化为整型 int8，那么考虑到模型中存在大量的冗余参数，即使去除这部分的冗余参数，也不会影响模型的拟合能力。</p><p>&nbsp;</p><p>因此可以使用剪枝的方法去除模型中的冗余参数，从而降低模型的参数量。</p><p>&nbsp;</p><p>而蒸馏的方法则是让小模型，也就是这里的学生模型去学习大模型，也就是教师模型中的有效知识，从而提升自身的泛化能力。</p><p></p><h3>模型压缩模块的 Pipeline 及各方法的运行原理</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/39f155f7c4b94bc83808b37af53b06d5.png\" /></p><p></p><p>下面让我们来详细看一下模型压缩模块是怎么工作的。对于用户而言，使用 NNI 的模型压缩模块是非常容易的，只需要三步，如图所示，这里我们统一将量化、蒸馏、剪枝方法称为compressor。对于用户而言，首先只需要根据自己具体的任务选择构建合适的 compressor 实例，接下来只需要调用 compressor.compress() 函数即可执行模型的整个模拟压缩过程。</p><p>&nbsp;</p><p>最后我们在使用 NNI 中的 SpeedUp 模块对模型进行压缩，实现真正意义上减少模型的参数量，下面对这三块进行讲解。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/442e9327f73c69549c1337fb081563ec.png\" /></p><p></p><p>&nbsp;</p><p>在 NNI 中构建一个 compressor 实例对用户来说是非常容易的，在 NNI 的剪枝量化模块，我们已经集成了大量的前沿算法，比如说在剪枝中，我们为用户提供了 ADMM、Slim、Taylor、Movement 等多种前沿剪枝算法，又如在量化中，我们也为用户提供了诸如 QAT、LSQ、PTQ 等多种前沿量化算法，对于用户而言，只需要根据自己的具体任务，选择相应的方法进行实例化即可。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b684552ee053b5345c13301d02139d8e.png\" /></p><p></p><p>&nbsp;</p><p>那么在创建了相应的 compressor 实例之后，我们只需要调用 compressor.compress() 函数，即可执行模型的模拟压缩过程。那么它具体是怎么工作的？大家可以看一下上图，对于一个原始模型，NNI 中的模型压缩模块首先会使用模型封装器对原始模型进行封装，并将封装后的模型传入模型评估器中，并和相应方法的模块进行交互，重复上述过程并返回一个模拟后的压缩模型，这就是我们整个模型的模拟压缩过程。</p><p>&nbsp;</p><p>这里不同方法对应着不同的交互模块，比如说从图上可以看到，在剪枝方法中它对应的就是掩码模块和剪枝算法，在量化中它对应的就是伪量化模块和量化算法，而在蒸馏中对应的就是蒸馏模块和蒸馏算法，下面以剪枝来说明整个模型的模拟压缩过程。</p><p>&nbsp;</p><p>首先对于封装后的模型将其传入模型评估器中，我们使用相应的掩码模块，收集模型目标值的有效信息，并将该有效信息传递给剪枝算法，剪枝算法根据具体的算法流程会对掩码进行计算和更新，并将更新后的值回传给掩码模块，在掩码模块这里，对目标直径用掩码进行更新，并将更新后的掩码回传给模型评估器，参与计算过程。</p><p>&nbsp;</p><p>重复上述过程就是我们整个模型的模拟压缩过程，这是从整体结构上来看，下面我们从一个更细腻度的角度来看这个问题。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a5e6c8576f39ca11dd090edc675802d.png\" /></p><p></p><p>&nbsp;</p><p>我们现在着眼于模型中的一个子 Module，来看一下当我们使用具体的压缩方法后，它的推理数据流具体是怎么做的。</p><p>&nbsp;</p><p>首先我们可以看一下图中最上面部分，先看一下原始模型中的推理数据流是怎么做的。首先从上一层中传入一个激活张量到当前 Module 中，当前 Module 会使用传入的激活张量和参数进行运算，并输出一个激活张量到下一层。那么经过 NNI 修改后，模型中的推理数据流又是怎么做的？如图所示，首先对于从上一层中传入的激活张量，我们需要使用相应的压缩方法对它进行更新，比如说如果我们使用的是量化方法，那么就用这里图上的伪量化模块对激活张量进行更新，如果我们使用的是剪枝方法，就直接用掩码剪枝模块对激活张量进行更新，如果我们用的是蒸馏方法，那么我们只需将激活张量记录下来即可，将更新后的激活张量传入到 Module 当中，并对 Module 中的参数，也就是图中的原始权重，采用类似的方法进行更新，得到更新后的参数，将更新后的参数和刚刚更新后的激活张量进行一个运算，并输出一个激活张量，并对输出的激活张量进行类似的操作，并将更新后的激活张量传入到下一层。</p><p>&nbsp;</p><p>我们对比上下两个图可以发现，当我们使用压缩方法后，和之前原始的推理数据流的差异就在于，我们对输入输出的激活张量以及模型的参数，使用相应的压缩方法进行了更新，而不同的压缩方法所使用的更新方式是有所不同的，这里对应的就是图上的伪量化、掩码、剪枝和记录，那么他们具体是怎么工作的？下面让我们具体来看一下。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/329fb160e5795e96d8279edc68b87aaa.png\" /></p><p></p><p>&nbsp;</p><p>首先向大家介绍一下掩码剪枝的工作原理。在前面我们已经提到，通过具体的算法，我们会使用具体的剪枝算法更新相应的掩码，更新后的掩码就对应图上的 mask，并将更新后的掩码和我们的目标值，也就是图上的 weight 进行运算，得到更新后的目标值，也就是这里的 weight'，这里的更新方式做的运算有两种，一种是乘法，一种是加法，具体用乘法还是加法取决于具体的 OP。我们会将所有目标值的掩码收集起来，作为整个模型模拟压缩过程的输出，以用于后续的 SpeedUp 模块压缩模型。</p><p>&nbsp;</p><p>接下来向大家介绍我们的伪量化模块，大家可以看一下右边的图，首先我们传入一个 32 Bit的目标值，在伪量化模块中，我们先会对它进行信息收集，比如说我们会收集目标值的最大最小值，包括它要量化的比特数等信息，并根据这些信息去计算它的 zero_point 和 scale 等信息，并利用这这些信息对目标值进行量化，以得到一个目标值的范围在 8 比特，目标数据类型仍然为 32 比特的值，也就是图上的 Fake 8 bits，我们接下来会对其再反量化，就又重新得到了一个 32 比特的值，这就是伪量化模块的工作流程。</p><p>&nbsp;</p><p>在这个过程中，我们会将伪量化模块收集到的部分信息存储在Calibration_config 中，并将其作为整个模型模拟压缩过程的输出，以用于后续 SpeedUp 模块去压缩模型。</p><p>&nbsp;</p><p>下面来看一下蒸馏的具体工作方式，因为蒸馏方法它其实和量化还有剪枝有所不同，蒸馏的目标主要是让小模型学习大模型中的有效知识，因此在记录模块中，只需要将目标值记录存储下来即可。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/976f481111ee094538d7d06fc2b7127b.png\" /></p><p></p><p>&nbsp;</p><p>可以看一下这个图，比如说第一个虚线框里，我们需要分别把学生模型和教师模型中的有效值和目标值收集下来，并根据存储的信息计算相应的蒸馏 loss，将所有的蒸馏 loss 和原始的 loss 进行加权求和，就可以得到总 loss。</p><p>&nbsp;</p><p>在介绍了模型的模拟压缩过程后，接下来看一下最后一步，使用 SpeedUp 模块压缩模型， SpeedUp 模块是用于剪枝或者量化后对模型的实际压缩过程。这里举例说明一下，比如说在我们常用的剪枝算法中，常规做法是直接将冗余参数的值置为 0，此时参数的维度没有改变，参数量也并没有改变，但是如果我们调用 SpeedUp 模块的话，它能够将冗余参数从参数中移除，从而实现真正意义上的减小模型的参数量，减小模型参数的一个实际维度。那么它具体是怎么工作的呢？下面我们用剪枝进行举例，看一下在剪枝过程中 SpeedUp 的具体流程。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/dab2575b6171e2cb8a7a1bfb18d1d7d9.png\" /></p><p></p><p>&nbsp;</p><p>大家可以看一下右下方这个图，首先我们的 SpeedUp 模块它在 torch.fx 的基础上引入了 concrete trace 的功能，因此我们可以使用实际的 tensor 以得到具体的模型图结构。接下来我们会根据得到的具体的模型图结构和在左边这个图第二步的模拟压缩过程所拿到的掩码，为依赖层中的参数生成相应的掩码，这里说明一下，因为在一个模型中，参数的维度在上下层之间是存在依赖关系的，因此如果我们对当前模型参数的维度进行修剪的话，那么我们就需要把上下间存在依赖关系的模型的相应维度也进行修剪，这样才能保证剪枝后的模型仍然能够正常使用。</p><p>&nbsp;</p><p>最后我们会根据最终的掩码修剪冗余的参数，以减小模型参数的大小。这里我们 NNI 里目前已支持 PyTorch 中绝大部分运算，对于尚未支持的运算，用户只需自己定义一个替换函数就可以正常使用。</p><p>&nbsp;</p><p></p><h2>基于 Transformer 系列模型的压缩实践</h2><p></p><p>&nbsp;</p><p>在介绍了我们模型压缩模块的工作原理后，下面让我们在 Transformer 系列模型上进行一些压缩实践。</p><p>&nbsp;</p><p></p><h3>Transformer模型结构</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7c607fd6070f6daffcbe7e1a29c4c786.png\" /></p><p></p><p>图片来源：<a href=\"https://arxiv.org/pdf/2202.01306.pdf\">https://arxiv.org/pdf/2202.01306.pdf</a>\"</p><p>&nbsp;</p><p>在正式介绍我们的压缩实践流程前，让我们先一起来回顾一下 Transformer 的模型结构。可以看到左边这个图，如图所示是 Transformer 主要由嵌入层、编码器和解码器构成，我们这里重点说一下它的编码器和解码器结构。先看左边图的左半部分，可以看到编码器是由 n 个 block 堆叠而成，每个 block 中主要由一个多头自注意力机制和前馈神经网络单元所构成，那么它的多头自注意力机制其实是 Transformer 里比较核心的部分，因为它能够让 Transformer 在训练时实现并行计算，那么它具体是怎么工作的？首先我们从名字拆解一下，多头自注意力机制说明是自己对自己的关注，因此它的 Query、Key、Value，也就是在右下图的 Q、K、V，它们都是自身，然后另外一个多头则是为了捕获多个粒度的信息，以增强隐藏层的状态表示。</p><p>&nbsp;</p><p>可以看一下右下方这个图，它的大概流程是比如说对 Query、Key、Value，也就是在右下图的 Q、K、V，分别经过线性层做一个运算，接下来会对 Q 和 K 做一个运算，以得到当前位置对上下文中每个位置的一个关注程度的分数，我们会基于该分数和 Value 进行加权求和，从而实现用上下文中的隐藏层状态表示，去增强当前位置的隐藏层状态表示。</p><p>&nbsp;</p><p>那么我们回到 Transformer 的结构图，可以发现右边这部分解码器的结构和编码器的结构是非常类似的，它也是由 n 个 block 堆叠而成，但它在编码器的基础上进一步还引入了一个交叉注意力机制模块，这个模块和自注意力模块的差别就在于此时它的 Query 是解码器的隐藏层状态表示，而它的 Key 和 Value 则是编码器的隐藏层状态表示。这样做的好处是我们能够用编码器的隐藏层状态表示去增强当前解码器的隐藏层状态表示。</p><p>&nbsp;</p><p></p><h3>Transformer系列模型分析</h3><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e7232845b394b494b30ca29a78e39cda.png\" /></p><p></p><p>图片来源：<a href=\"https://arxiv.org/pdf/2002.11985.pdf\">https://arxiv.org/pdf/2002.11985.pdf</a>\"</p><p>&nbsp;</p><p>从上面的分析可以发现 Transformer 的结构稍微复杂一些，由多个部分组成，比如说嵌入层、自注意力层和前馈神经网络层，那么我们进一步对 Transformer 系列模型 BERT 进行分析，大家可以看一下左边这个图，有研究者对 Bert-base 在长度为 256 的句子上进行了推理，以比较子模型大小及模型不同部分的理论要求，其以数百万的 FLOPs 为单位，底部的两个图表则跟踪了模型运行时的 memory 消耗和两个具有代表性硬件设置上的推理延迟。</p><p>&nbsp;</p><p>我们分析这个图可以发现消耗最多 memory 和执行最多 FLOPs 的子模型是 FFN 单元，这里的 FFN 就是我们说的前馈神经网络，而嵌入层是模型大小的重要组成部分，其 FLOPs 虽然为 0，因为它是一个查找表过程，在推理时不涉及任何算术计算，因此嵌入层的执行时间在很大程度上与执行模型的硬件无关。</p><p>&nbsp;</p><p>最后我们会发现自注意力层周围的线性层会产生额外的 memory 和计算开销，且之前的线性层的消耗大约是之后线性层消耗的三倍。</p><p>&nbsp;</p><p>大家可以从第二个图前面有一个 Linear before Attn 和 Linear after Attn 看出来，该研究者进一步指出，由于 BERT 的复杂架构，没有现有的压缩方法专注于模型的每一个方面，比如说自注意力层、线性层、嵌入层、模型深度等，相反每种压缩方法都适用于模型的某些组件，这就为 Transformer 系列模型的压缩带来了一定的困难性和繁琐性。</p><p>&nbsp;</p><p></p><h3>NNI 对于 Transformer 系列模型压缩的优势</h3><p></p><p>&nbsp;</p><p>然而幸运的是我们的 NNI 模块它是天然的适用于 Transformer 系列模型压缩的，NNI 中的模型压缩模块，它具有易用的算法框架，并且它集量化、剪枝及蒸馏于一体，三种方法都能够联合使用，同时它拥有大量的前沿算法，能够非常方便地针对模型的不同部分使用不同的算法。另外NNI 还为用户提供了 SpeedUp 模块，其能够实现真正意义上的减少模型的参数量，打通了从模型压缩到模型加速的整个流程。NNI 还为用户提供了详细的 tutorial 和文档教程，帮助用户快速上手使用。</p><p>&nbsp;</p><p></p><h3>压缩流程实践</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7d/7d735f09bb7fa6e08fb905f4b3fa03b4.png\" /></p><p></p><p>&nbsp;</p><p>下面让我们来看一下我们具体的实践流程，从之前的分析我们可以发现，前馈神经网络层是消耗最多内存和执行最多 FLOPs 的子单元，因此我们需要对前馈神经网络进行压缩。</p><p>&nbsp;</p><p>另外我们还考虑对多头自注意力模块和嵌入层进行压缩，在本次实践中我们将主要采用剪枝的方法对模型进行压缩，同时为了避免在模型压缩后损失过多的精度，我们将对压缩后的模型再训练。</p><p>&nbsp;</p><p>这里的模型再训练有两种方式，第一种是直接对模型进行 fine-tune，另外一种是用动态蒸馏机制对模型进行再训练。</p><p>&nbsp;</p><p>下面我们统一用动态蒸馏机制进行讲解，总的实践流程可分为 4 步，如图所示，首先我们需要准备数据模型等，接下来需要针对多头自注意力模块、嵌入层及前馈神经网络分别进行剪枝和再训练。这里说明一下这里的多头自注意力模块的剪枝和模型再训练，在后续中也可以指代对Transformer 中Decoder的交叉注意力模块的剪枝和模型再训练。</p><p>&nbsp;</p><p>先看一下模型数据的准备过程。如图所示，首先我们需要加载或者创建一个模型，接下来选择合适的数据集，并创建相应的 Dataloader，接下来构建相应的训练评估函数，并对模型进行Finetune。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d4ffad53f228e79d82b1c9532811815.png\" /></p><p></p><p>&nbsp;</p><p>下面来看一下多头自注意力模块的剪枝和再训练过程。该过程可分为三步，如图所示，首先需要构建相应的 Pruner 实例，也就是刚刚提到的 compressor 实例，接下来需要对自注意力模块进行剪枝，最后是使用动态蒸馏机制再训练模型，下面进行一一说明。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8b39de56ed55e528b68eebc8cf2b13e.png\" /></p><p></p><p>&nbsp;</p><p>首先在对多头自注意力模块进行剪枝中，使用了 Movement 剪枝算法，因此我们将需要剪枝的子模型名、模型类别及稀疏度，以及 Movement 算法的一些参数传入，以实例化一个 Movement Pruner。这里说明一下在 Movement 算法中有两种模式，一种是 soft 模式，一种是 hard 模式，在 soft 模式下我们根据 threshold 对模型进行修剪，因此我们无法精确控制剪枝后模型的稀疏度，而在 hard 模式下，剪枝后模型的稀疏度就是我们的目标稀疏度。</p><p>&nbsp;</p><p>接下来我们会调用 pruner.compress() 以执行剪枝过程，得到剪枝后的 mask 将这个掩码 mask 传入 SpeedUp 模块用于后续的模型压缩过程。</p><p>&nbsp;</p><p>最后我们以原模型 fine-tune 后的模型为教师模型，以剪枝后的模型为学生模型，使用动态蒸馏机制，对模型进行蒸馏，尽可能的恢复模型的性能。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/361c94a5552901f1c6eee03169856232.png\" /></p><p></p><p>&nbsp;</p><p>下面再让我们来看一下嵌入层的剪枝和模型再训练过程。该过程可分为两步，第一步算是迭代剪枝加动态蒸馏，要重复 n 次。第二个是动态蒸馏过程。先来看第一步，如左下角的图所示，在对嵌入层的剪枝过程中，我们使用的是Taylor剪枝方法，因此我们首先先创建一个Taylor Pruner 的实例，接下来调用一个 compress() 函数执行模型的模拟压缩过程，并将模拟压缩过程所生成的掩码传入到 SpeedUp 模块中。接下来会使用一个蒸馏函数，以计算一个蒸馏 loss，并将蒸馏 loss 和原始 loss 进行加权求和，得到总 loss，考虑到前馈神经网络的剪枝方法和再训练方法比较类似，这里不再赘述。</p><p>&nbsp;</p><p></p><h3>实验结果</h3><p></p><p>&nbsp;</p><p>下面来看一些实验结果。首先这次实验主要在 BERT、T5、和 ViT 模型上进行了实验，其中 BERT 使用了完整的流程，而 ViT 则没有对嵌入层进行剪枝，T5 则在 ViT 的基础上直接对模型进行 fine-tune，执行模型再训练过程，下面对实验结果进行一一分析和讲解。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6dbbd17189fe67476993df4ac3214012.png\" /></p><p></p><p>&nbsp;</p><p>首先大家可以看一下，这是 BERT-base 在 GLUE-MNLI 数据集的实验结果，在对 BERT 进行实验时，我们使用了一张 A100 进行实验，并将 batch_size 设置为 32。先看一下这个表格，它的第一列说的是自注意力模块的剪枝方法，这里主要用的是 movement Pruner 剪枝方法。</p><p>&nbsp;</p><p>这里通过控制 movement 算法中的 regular_scale 参数去控制对多头自注意力模块的剪枝程度，regular_scale 参数越大，多头自注意力模块的稀疏度就越高。第二个是我们对嵌入层的剪枝方法，这里我们使用Taylor剪枝方法对嵌入层进行剪枝，并且迭代剪枝三次。对于前馈神经网络，我们仍然使用 Taylor 方法进行剪枝，并且迭代剪枝 19 次或者 24 次，倒数第二列是总稀疏度的百分比。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f4126678de10f12d4d334442e91299bf.png\" /></p><p></p><p>图片来源：<a href=\"https://github.com/huggingface/nn_pruning\">https://github.com/huggingface/nn_pruning</a>\"</p><p><a href=\"https://github.com/princeton-nlp/CoFiPruning\">https://github.com/princeton-nlp/CoFiPruning</a>\"</p><p>&nbsp;</p><p>大家可以看一下上图这个计算公式，它是用原模型的大小减去压缩后模型的大小，并将它们的差值比上原模型的大小。最后一列是准确率的百分比，从实验结果可以发现当 regular_scale 大于等于 10 时，模型总的稀疏度能够超过 69%，性能损失超过 1%，随着模型总稀疏度的增加，模型的性能是逐渐下降的，且下降程度是逐渐增大的。</p><p>&nbsp;</p><p>当我们对嵌入层剪枝三次时，能够将模型的嵌入层维度从 768 减小至 561，这在一定程度上也减小了模型的参数量。</p><p>&nbsp;</p><p>进一步和其他两个平台的实验结果进行对比，也就是 CoFi_Pruning 和 nn_pruning，nn_pruning 是 huggingface 旗下的，CoFi_Pruning 是陈丹琪 ACL'22 的一篇论文里的一个结果。这两个都是目前用的比较多的，我们可以对比看一下左边这个图，它的横坐标是总的稀疏度的百分比，纵坐标是准确率的百分比，其中最上面那根线是直接 BERT base 在不剪枝的情况下，直接在 MNLI 数据集上 fine-tune 后的性能。</p><p>&nbsp;</p><p>最左边这根线是 nn_pruning 的实验结果，这条比较浅的线是 CoFi_Pruning 的实验结果，橙色那根线是 NNI 的实验结果，我们分别对比看看，首先看 nn_pruning 的实验结果和 NNI 的实验结果对比，可以发现使用 NNI 对 BERT 在 MNLI 数据集上剪枝后的性能好于使用 nn_pruning 框架剪枝后的性能。</p><p>&nbsp;</p><p>如果对比 CoFi_Pruning 和 NNI 的话，会发现当模型的总稀疏度低于 65% 时，NNI 和 CoFi_Pruning 对 BERT 在 MNLI 数据集上的性能差异是比较小的，也就是在这个点的分界线之前，它们的差异是很小的，但是当模型总稀疏度大于 65% 时，使用 NNI 对 BERT 在 MNLI 数据集上剪枝后的性能是好于 CoFi_Pruning 的。同时我们只看 NNI 这条线，会发现当模型总的稀疏度约为 79% 时，性能下降约为 4% 左右。这里再额外补充一句，因为我们用的是总稀疏度，对应一般论文里编码器里的稀疏度，应该已经超过了 90%。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/72881ad2c80349a918ac19f030263007.png\" /></p><p></p><p>&nbsp;</p><p>下面再来看一下 T5 和 ViT上的实验结果。首先看 T5 的实验结果，我们使用的是 T5-small 模型，并且在一个翻译任务 WMT16(en-ro) ，英文翻俄文的数据集上进行评估，实验结果如上面的表所示，在这个过程中我们对它的编码器解码器都进行了剪枝，第二列是它的 BLEU 值，最后一列是它的编码器和解码器的稀疏度。</p><p>&nbsp;</p><p>看左下角的公式，大概说一下编码器解码器的稀疏度，主要是用原始模型编码器和解码器的稀疏度之和，减去压缩后模型编码器和解码器的稀疏度之和，用它们两个的差值比上原始模型编码器和解码器的稀疏度之和。</p><p>&nbsp;</p><p>可以看到表格第一行是 T5-small 的结果，同样的，这一行的结果也对应着如果直接用 T5-small 在这个数据集上 fine-tune 它的一个实验结果，由实验结果可以发现当 T5 在编码器解码器上的稀疏度小于 50% 时，模型的 BLEU 值下降比较少，低于一个点，而当 T5 在编码器解码器上的稀疏度超过 75% 时，模型的性能下降约为三个点。</p><p>&nbsp;</p><p>再看一下 ViT 的实验结果，我们主要使用 vit-base-patch16-224 模型，主要使用 Cifar10 数据集进行评估，整个实验结果可以看左下方这个表格，同样它的第二列是准确率的百分比，最后一列是编码器的稀疏度百分比，计算公式和之前的计算公式是类似的，编码器的稀疏度就等于原始模型编码器的稀疏度减去压缩后模型编码器的稀疏度，用它们的差值比上原始模型编码器的大小。</p><p>&nbsp;</p><p>可以看一下第一行仍然是在这个模型不做压缩的情况下，直接 fine-tune 它的一个效果。第一列当中有一个 regular_scale 参数，这个和之前的参数是一样的，都是用来控制多头自注意力模块的稀疏度的。从实验结果可以发现，当 ViT 在编码器上的稀疏度小于 55% 时，模型的准确率下降比较少，但是当 ViT 在编码器上的稀疏度超过 75% 时，模型性能下降约为 3% 左右。</p><p>&nbsp;</p><p></p><h3>平台对比</h3><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be0b589e2d71e4cf33d59794d3298d24.png\" /></p><p></p><p>&nbsp;</p><p>我们把模型压缩模块中的剪枝模块和现在主流的两个平台，nn_pruning 和 CoFi_Pruning 进行了对比，主要考察以下几个指标，第一个是它们是否具有详实的实验表现，第二个是它们是否有像 SpeedUp 模块一样的工具，能够真正意义上的压缩模型。第三个是它们是否有详实的教程实例，以及它们支持模型的类别，包括还有它们所包含的剪枝算法的种类。</p><p>&nbsp;</p><p>大家可以看一下这个表，从表里可以发现 NNI 具有完整的教程实例，有详实的实验表现，和前面两个平台最大的差异在于它有一个 SpeedUp 模块，能够真正意义上减小模型的参数量，打通从模型压缩到模型加速的整个流程。同时 NNI 现在支持 BERT、RoBerta、GPT、BART、T5、 ViT 等多种主流模型，而反观其它两个平台，所支持的模型是非常有限的。最后 NNI 还为用户提供了 16 种前沿剪枝算法，它具有较强的通用性。</p><p>&nbsp;</p><p></p><h2>结论</h2><p></p><p>从上面的分析可以发现，NNI 的压缩模块，它通过统一的框架融合了模型压缩的多种技术与方法，打通了从模型压缩到模型加速的整个流程，NNI 的模型压缩模块能够联合使用量化、蒸馏、及剪枝方法，并能够非常方便的针对模型的不同部分使用不同的压缩算法，其具有非常强的通用性。</p><p>&nbsp;</p><p>通过对 Transformer 系列模型的压缩，我们发现了既能降低模型参数量和计算量，又能保持模型比较高精度的剪枝步骤和算法组合，在 BERT-MNLI 上获得了超越 SOTA 的模型剪枝结果，在 ViT-Cifar10 和 T5-WMT16(en-ro) 上也取得了较好的效果。</p><p>&nbsp;</p><p>三年多以来，NNI 一直在不断探索，不断更新，以更好的适应技术的发展，提升用户体验。在即将发布的 v3.0 版本中，NNI 的模型压缩模块将引入蒸馏模块，支持剪枝、量化和蒸馏的一体化，同时面向延迟敏感类模型的 NAS 也将支持更多的应用场景，更精简更友好的编程接口和调试体验，更强劲的算法性能。除此之外，还有许多其它有趣的更新，欢迎大家持续关注，也欢迎大家为我们 NNI 社区贡献一份自己的力量，谢谢大家。</p>",
    "publish_time": "2023-08-17 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数跨新阶，原生新纪 | 2023 数字化转型发展大会蓄力启航",
    "url": "https://www.infoq.cn/article/6ae177d7b5196c39b08e26ae6",
    "summary": "<p></p><h3>大会背景</h3><p></p><p>党的二十大报告提出加快发展数字经济，促进数字经济和实体经济深度融合。《数字中国建设整体布局规划》进一步明确做强做优做大数字经济。数字化转型对数字经济的发展具有支持和促进作用，顺应新一轮科技革命和产业变革趋势；与此同时，在数字经济浪潮下，一批围绕互联网、云原生、大数据、人工智能等新一代数字技术而建立的“数字原生”企业正在崛起。</p><p></p><p>为进一步推动各行业数字化转型发展，加快数实融合，以数字化转型驱动生产生活和治理方式变革，进而加快中国式现代化发展进程。同时也为了继续探寻数字原生企业发展之路，推进数字原生理念的发展和应用，持续推动产业革命。由中国信息通信研究院、中国通信标准化协会联合主办的2023数字化转型发展大会暨首届数字原生大会将于2023年9月13日-14日在北京国宾酒店举办。</p><p></p><p>本届大会以“数跨新阶，原生新纪”为主题，大会为期两天，设置1个主论坛和11个分论坛，围绕数字化转型、数字原生、数实融合、央国企数字化转型及云上数字政府等核心议题展开讨论，并发布多项研究成果。同时邀请行业专家学者、知名企业代表、政府机构等百余位专家同台论道，围绕产业发展、技术创新、最佳实践等角度展开交流与讨论，为参会者奉上一场数字化转型发展的思想和实践盛宴。</p><p></p><h3>大会亮点</h3><p></p><p></p><h4>亮点一、多项行业研究报告和成果发布</h4><p></p><p>在本次会上，将重磅发布2023数字化转型发展的多项成果，涉及趋势解读与最新评估结果、启动仪式与报告发布及数字化转型发展先锋人物名单公布。</p><p></p><p>在趋势解读与最新评估结果部分，中国信通院将发布《企业数字化转型发展双象限（2023）》，深入解析产业数字化转型发展的最新趋势。同时将发布上半年政企数字化转型成熟度模型IOMM最新评估结果。</p><p></p><p>在启动仪式与报告发布环节，将发布多项企业数字化转型成果、数字政府成果以及政企信创相关成果，包括“央国企高质量数字化发展赋能计划”、《流程挖掘行业发展报告（2023）》、《低代码无代码产业图谱》、《组装式发展白皮书（2023）》、《数字政府建设与发展研究报告（2023年）》、《一城一云白皮书》、《政务数据云发展与应用白皮书》、《政务大模型建设路径及评价体系》、《数字化供应链赋能产业链韧性协同发展（2023）》 、“信创ERP质效提升专项计划”启动仪式等。</p><p></p><p>在数字化转型发展先锋人物评选结果公布环节，“企业数字化发展共建共享平台（EDCC）先锋人物”，将公布在数字化转型中做出杰出贡献的专家和产业精英名单；“数字政府建设赋能计划先锋人物”，将公布推动数字政府建设的行业专家名单。</p><p></p><h4>亮点二、第二届“鼎新杯”总决赛结果重磅揭晓</h4><p></p><p>第二届“鼎新杯”数字化转型应用大赛，自启动以来受到了业界的广泛关注和积极参与，大赛设置了6大方向16个专题赛道，经历了案例征集、全国入围赛、公示和票选最佳人气案例等阶段，并在8月底进行全国总决赛。</p><p></p><p>大赛在案例征集阶段共收到近2000个参赛案例，经过大赛组委会形式审查及业界专家评选，共选出16个赛道的371个案例入围；在公示及票选最佳人气案例阶段，投票系统总访问量达2379万人次，有效投票达1784万票，并在16个赛道上分别诞生了最佳人气案例，其中人气最高的案例投票量高达221万票。</p><p></p><p>在接下来的全国总决赛中，将迎来更加激烈的比拼，最终16个专题赛道的一二三等奖将花落谁家？哪些案例将会成为6大方向标杆奖得主？哪些赛道组织者将获得好评？最终结果都将在本次大会上重磅揭晓。</p><p></p><h4>亮点三、数字原生从理念到落地实践，掀起数字化发展新浪潮</h4><p></p><p>数字经济浪潮下，一批践行“数字原生”理念的企业和组织在各行业中涌现，他们将数字化理念和技术与企业文化、商业模式、决策机制深度融合，形成创新性的产品与业务模式，实现远超传统企业的生产效率和企业价值，不断重塑行业传统格局和发展范式。</p><p></p><p>在本次大会上，将正式成立“数字原生新实体论坛（Digital&nbsp;Native&nbsp;Forum，DNF）”，汇聚数字原生特征明显的企业和组织，搭建数字原生新技术、新思想、新理念交流平台，促进各行业企业充分理解实体业务和数字技术融合的本质，营造“数字原生新实体”良好发展环境。</p><p></p><p>中国信通院云计算与大数据研究所所长何宝宏博士的新书《数字原生》将在本次会上重磅发布。该书以简洁幽默的风格，梳理了从数字革命到数字原生新阶段的发展历史、重要领域和基本规律，引发业界对数字化发展方向的深度思考，在会上何宝宏博士也将对数字原生的最新发展趋势进行解读。</p><p></p><p>此外，本次大会将发布数字原生系列标准和首批评估结果，中国信通院的专家将对标准进行详细解读。</p><p></p><h4>亮点四、深入行业实践，业内专家分享最佳实践成果</h4><p></p><p>数字化转型发展俨然已进入深水区，随着技术与业务的深度融合，不同行业、不同规模的企业在战略、管理、生产、运营、生态发展等方面将面临新的挑战和机遇。本次大会汇聚了各行业、各领域的专家，从技术趋势、业务应用、平台建设、流程贯通等场景，深入解析数字化转型在不同行业中的应用和实践，议题覆盖金融、电信、制造、零售、政务、医疗等各领域。</p><p></p><p>在本次会上，与会专家将结合技术最新趋势，分享AIGC、低/无代码、组装式、信创、中台等领域的最新技术与企业数字化转型发展结合的实践案例，如金融行业数字化风控系统、智能制造流程、数字化供应链体系建设、数字化安全生产、智慧中台、智慧大脑等，同时，第二届“鼎新杯”获奖案例企业将分享案例成果，为参会者解决数字化转型深水区的各类问题和挑战提供借鉴和参考。</p><p></p><h4>亮点五、多主题论坛，全方位诠释数字化转型发展内涵</h4><p></p><p>在延续以往数字化转型发展核心议题基础上，今年大会围绕数字化转型发展多个热点议题和前沿领域展开深入讨论，设立11个主题分论坛，包括：数字原生、云上数字政府、央国企信创与信创产业发展、大模型时代智慧政务发展新机遇、政务信创、经营管理数字化、流程数字化、智慧中台与智慧运营创新、数字化安全生产与信创运维实践、低无代码与组装式技术应用、云边端一体化等，力求从不同角度、不同应用方向、不同技术特点为参会者呈现数字化转型发展相关的前沿思维、应用实践与技术创新情况。</p><p></p><p>目前，大会已正式进入启动阶段，诚邀业内专家、产业精英莅临指导，共同推进数字化转型发展。</p><p><img src=\"https://static001.geekbang.org/infoq/61/61627e8d629987e1f647cef42d72047e.png\" /></p><p>扫描二维码报名参会</p><p></p><h4>联系人</h4><p></p><p>商务合作：董恩然 18601280900（微信同号） dongenran@caict.ac.cn郝明路 15810972078演讲人招募：车昕 18611139904（微信同号） chexin@caict.ac.cn媒体对接：董伟 13810413143（微信同号） dongwei1@caict.ac.cn</p><p></p><p></p>",
    "publish_time": "2023-08-17 10:17:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于Apache Kafka的Serverless架构演进",
    "url": "https://www.infoq.cn/article/E34DbaF3BpCvxO2ADWA9",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p>随着大数据、互联网技术的发展，数据量爆发式增长。<a href=\"https://www.infoq.cn/article/RTTzLOMBPOx2TsL7dM9T\">Apache Kafka</a>\"通过架构创新和生态完善，具备高吞吐、低延迟和横向扩展能力，完美匹配了大数据的各种需求，成为消息、事件流领域的事实标准。</p><p>&nbsp;</p><p>随着云计算的普及，近几年<a href=\"https://www.infoq.cn/topic/cloud-computing\">云原生</a>\"成为最热门的技术趋势。云原生意味着云计算原生，面向云计算的特点来重新设计软件，释放云计算的红利。对于业务应用而言，云原生意味着摒弃传统的运维开发框架，通过微服务、容器化、DevOps和持续交付，实现应用敏捷开发、弹性伸缩、自动化部署。对于基础软件而言，则是充分和云计算的基础设施层结合，重新设计，进一步提升关键技术指标。如这几年讨论得比较多的存算分离技术，存储计算一体的架构逐渐被云原生存储（存储托管化）+ 存储计算分离的云原生架构取代。在存算分离架构中，状态存储不再依赖于本地磁盘，摆脱了本地存储规格的限制，这使得计算资源与存储资源可以灵活配比，支持快速的扩缩容。同时状态下移到存储层，降低了基础软件的运维复杂度，提升运维效率。</p><p></p><p>云原生发展到今天，已经全面往Serverless的方向演进。因此我们希望能基于最新的云计算技术，对Apache Kafka存储引擎进行深度重构，使其能够实现真正的弹性Serverless架构形态。在云原生Serverless架构的基础上建设Kafka云服务，为客户提供按需弹性、按量计费的Serverless产品体验，把降本增效红利真正释放到客户侧。</p><p></p><h2>设计</h2><p></p><p></p><h4>目标</h4><p></p><p>&nbsp;</p><p>Kafka等消息队列是典型的存算不分离架构，基于本地文件系统和本地盘自建，这种架构主要有以下几个问题：</p><p>&nbsp;</p><p>单盘热点吞吐能力和容量受限；存算比例绑定，不能灵活调整适配，容易导致资源利用率低；节点有状态，导致扩缩容涉及数据迁移，但迁移速度受到原始节点负载、数据量和磁盘吞吐等因素影响，一般TB级数据需要小时级迁移时长。因此整体风险高，运维压力大。</p><p>&nbsp;</p><p>Kafka社区的二级存储是典型的共享冷数据架构。由于对象存储无法匹配本地盘的低延时和高吞吐，仍然需要本地盘存储热数据和S3、OSS等对象存储来存储冷数据。这种架构主要有以下几个问题：</p><p>&nbsp;</p><p>本地存储不能保证数据可靠，仍然需要多副本协议保证本地热数据不丢不错，而各计算节点之间同步副本会消耗网络带宽资源；需要实现本地存储文件和二级存储文件逻辑映射机制，增加系统复杂度。</p><p>&nbsp;</p><p>我们期望架构可以共享所有数据，且实现计算无状态化、计算容器化和存储托管化，从而充分发挥弹性和调度优势。故我们需要高性能分布式存储系统，提供对标本地盘的低延迟和高吞吐能力，同时支持冷热分层以提高存储性价比。</p><p>&nbsp;</p><p>盘古是阿里云自研的高性能分布式文件系统，解决了超大规模下数据不丢不错和高可用的难题，兼顾更加稳定可靠的存储能力、更大的容量和更高的性能等优点，广泛部署在全球数十个大型数据中心，服务阿里云上数百万的客户，覆盖互联网、政企、金融、制造等全行业。盘古是阿里云关键的创新技术之一，满足数字经济对海量存储、快速存储和稳定存储的需求，并入选世界互联网领先科技成果。阿里云盘古DFS是构建在阿里云盘古分布式文件系统之上的存储产品，会在云上提供大数据文件存储服务，满足云上客户的高性能存储需求。</p><p>&nbsp;</p><p>Kafka是面向日志或流的架构，且本身也是Append Only语义，这些特点和盘古模型非常贴合。基于调研和性能验证，最终我们选择基于阿里云的盘古DFS分布式文件存储产品实现Kafka云原生化，通过把Kafka的存储全卸载到云原生存储，将实现公共云基础设施级的资源并池，也就是存储池化，我们将具备无限存储空间，同时也解决社区版Kafka饱受诟病的数据倾斜问题。横向对比业界某些消息系统的存算分离架构，其存储并非是使用池化的云存储资源，依然要自己运维分布式存储系统，自己预先承担存储成本，因此其不是真正的按量付费。</p><p><img src=\"https://static001.infoq.cn/resource/image/4a/b8/4abf61fff553feb84b0e12f0a39f59b8.png\" /></p><p>图1 Serverless Kafka架构图</p><p>&nbsp;</p><p></p><h4>数据高可靠</h4><p></p><p>&nbsp;</p><p>盘古DFS使用多副本以及EC等策略来保证数据极高的可靠性，不要求磁盘本身高可用，因此完全可以采用廉价的磁盘来实现数据的安全存储。将数据打散到不同的rack上，当某一台廉价的服务器发生故障时，仍然能够快速地恢复出数据的副本以保证数据安全，这种可靠性保证为有把握对上层承诺数据不丢失提供了稳固的基石，使得Kafka更加可靠，数据更加安全。盘古DFS支持跨数据中心的容灾策略，百微秒级平均延迟、毫秒级长尾延迟以及单存储节点打满200Gbps网络的IOPS处理能力，同时数据可靠性达到12个9，可用性高达5个9。</p><p>&nbsp;</p><p></p><h4>服务高可用</h4><p></p><p>&nbsp;</p><p>开源Kafka基于ISR机制实现服务高可用，ISR（in-sync replica）为某个分区维护的一组同步集合，处于 ISR 集合中的副本意味着 follower 副本与 leader 副本保持同步状态，只有处于 ISR 集合中的副本才有资格被选举为 leader。一条 Kafka 消息，只有被 ISR 中的副本都接收到，才被视为“已同步”状态，节点故障或者网络断连等会触发 ISR 选举，选出新 leader 并对外提供服务。ISR机制复杂度高，问题调查难度大。存算分离后各计算节点无状态且共享存储，再配合我们设计的一套轻量级的故障切换机制，大大降低系统复杂度的同时还提升了可运维性。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a1/e0/a1cd84982ab8fb289f23135ff8c17ce0.png\" /></p><p>图2示例 可用区-C宕机不影响可用性</p><p>&nbsp;</p><p></p><h4>降低成本</h4><p></p><p>&nbsp;</p><p>计算成本。由于数据直接写入高可靠的盘古DFS，计算层无流量复制，极大的降低了计算节点的CPU和网络带宽消耗，相同成本下拥有更高吞吐上限。Kafka也在适配倚天ARM架构，通过软硬件协同进行全栈的深度优化，释放巨大技术红利，降低计算成本。</p><p>&nbsp;</p><p>存储成本。依赖盘古DFS实现高可靠的数据存储，盘古DFS通过纠删码、冷热分层、基于CIPU软硬件协同优化等技术实现存储成本的降低，这种技术红利也会持续降低Kafka的存储成本。针对“把Kafka作为长期存储”的场景，可以极大的降低存储成本。</p><p></p><h4>降低延迟</h4><p></p><p>&nbsp;</p><p>网络延迟。我们可能会质疑分布式文件系统性能不及本地文件系统，在上一代分布式文件系统中，这是一个比较明显的问题，但是随着用户态协议栈、高性能RDMA网络、NVMe SSD和新型软硬融合存储技术的持续发展，分布式文件存储进入微秒延迟时代。</p><p>&nbsp;</p><p>计算延迟。针对平均延迟，计算层无复制流量可以充分降低网络吞吐以避免拥塞。针对长尾延迟，开源Redpanda用c++重写kafka避免gc延迟，它带来的挑战是如何做到100%兼容开源Kafka并能保持很高的迭代效率。我们使用主流编程语言领域最顶尖的内存管理技术，即新一代分代无暂停GC（generational pauseless GC），仅设置堆大小和并发线程数就使得GC停顿时间小于2ms，大大降低了系统长尾延时。</p><p>&nbsp;</p><p>存储延迟。通过2-3异步写、Backup读、精细QoS控制和慢盘规避等技术保障了性能的稳定性，极大减少了性能的抖动，为客户提供了高质量可预期的平滑服务保障。</p><p></p><h4>弹性伸缩</h4><p></p><p>&nbsp;</p><p>阿里云容器服务通过硬件结构体系、操作系统、分布式调度配合，实现了面向SLO的资源精细化管理和弹性调度：VPA，弹性，超卖等调度技术，提升了资源弹性能力和资源的利用率。节点资源自动弹性结合调度能力提供了丰富的资源弹性能力：块资源弹性，resource limit阈值弹性，定时弹性等。通过调度和节点弹性技术大幅度提升了容器部署密度和部署效率。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/9a/5a/9acf339295cfc4d9de1629a9d5debd5a.png\" /></p><p>图3 弹性扩容Broker-4</p><p></p><h4>支持RDMA网络访问加速</h4><p></p><p>&nbsp;</p><p>传统的TCP/IP一直是业界主流的网络通信协议，众多应用都是基于TCP/IP构建的，但随着数据中心相关的业务蓬勃发展，应用对于网络的性能需求（如高并发、低延迟、高吞吐）越来越高。在Kafka中网络传输是一个关键的性能瓶颈，传统的TCP/IP协议会对网络传输带来一定的开销和延迟，尤其是在高并发、低延迟、高吞吐的数据传输场景下，TCP协议的开销和延迟会更加明显。</p><p>&nbsp;</p><p>从图4 上半部分可以看出在传统TCP/IP网络协议栈的通信过程中，存在若干次的数据拷贝过程，甚至在部分场景下，数据拷贝的CPU开销占比可达50%以上。而且对于一次通信的过程，发送端和接收端均需要经历上述的数据拷贝过程。随着云计算时代对数据中心算力的需求逐渐提高，TCP/IP协议栈逐渐遇到瓶颈，不再能够满足数据中心对于网络的性能需求。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/da/6c/da2d537118c25e10f39ba0dcedbf216c.png\" /></p><p>图4 传统TCP/IP协议栈与RDMA协议栈</p><p>&nbsp;</p><p>&nbsp;为了解决TCP/IP遇到的性能问题，一种高性能网络通信技术——RDMA（Remote Direct Memory Access）逐渐成为云计算时代的秘密武器，并逐渐在数据中心的业务中被广泛应用。简单来说，RDMA是一种基于硬件加速的网络传输技术，可以实现无CPU参与的数据传输，从而提高传输效率和性能。从图4 下半部分可以看出RDMA技术通过在网络适配器和内存之间建立直接的数据传输通道，绕过了操作系统内核和CPU的介入，直接在内存中进行数据传输。另外RDMA在远端节点CPU不参与通信的情况下对内存进行读写，以实现CPU卸载，再配合零拷贝技术，使得RDMA具有低延迟、高吞吐和低CPU开销的特点。</p><p>&nbsp;</p><p>我们希望借助RDMA打造云产品差异化竞争力同时具备可扩展性。最终需要解决以下问题：实现真正的零拷贝通信，避免中间缓冲；RDMA 和 TCP 数据路径在 Kafka 中共存而不妨碍其可用性和性能；共享内存避免直接分配大块内存；使用 RDMA FAA 加速偏移请求的提交。</p><p></p><h2>实现</h2><p></p><p></p><h4>整体架构</h4><p></p><p><img src=\"https://static001.infoq.cn/resource/image/65/4b/65a73e8b9c46d9a37b1516322a5ce84b.png\" /></p><p>图5 Serverless Kafka整体大图</p><p></p><h4>核心存储</h4><p></p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/36/8a/36252f5159f60aa2cc022e974916148a.png\" /></p><p>图6 单机存储</p><p>&nbsp;</p><p>Apache Kafka 生产者发送的消息首先写入PageCache，操作系统异步持久化到存储介质，通过在broker层面实现副本机制保证数据可靠性。云原生Kafka采用盘古DFS作为存储底座，消息数据（CheckPoint和index文件依然写入分布式文件系统）写入盘古DFS便可保证可靠性。然而也面临新挑战：盘古DFS与本地文件系统有本质区别，无法直接利用操作系统的PageCache机制，如果数据直接写穿盘古DFS，可能导致写延迟增加和吞吐量下降。</p><p>&nbsp;</p><p>为解决这一问题，我们基于堆外内存实现了用户态HotCache。生产者发送的消息首先写入HotCache，持久化到盘古DFS便可保证数据可靠性，然后响应生产者发送成功。针对数据持久化到盘古DFS的过程，我们进行了以下优化：首先在HotCache中批量聚合数据，然后用时间、空间和频率等多种策略触发持久化至盘古DFS，减少网络抖动毛刺同时更快地推进HighWaterMarker，使消费者及时读取消息，降低端到端延迟；针对大IO的业务场景，我们采用了I/O切片处理方式，支持更大并发操作同时提升吞吐上限。</p><p>&nbsp;</p><p>Apache Kafka 在Catch-up Reads或Streaming场景下可能出现大量冷读，这些数据往往不在PageCache中，需要从磁盘读取并加载到PageCache，当冷读较多时PageCache竞争激烈，频繁的换入换出导致缓存污染，严重影响写入性能。此外，Apache Kafka 处理生产和消费请求的同一线程池可能会受到冷读阻塞的影响，导致所有请求无法及时处理出现故障。因此，冷读会严重影响数据的写入和Tailing Reads。为缓解此问题，我们通过自研\"冷热数据隔离\"的堆外缓存、冷热线程（协程）分离和预加载等策略进行优化。</p><p></p><h4>高可用&nbsp;</h4><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c9/c5/c9c2f983017ffac071b0yy56e6e859c5.png\" /></p><p>图7 Serverless Kafka高可用</p><p>&nbsp;</p><p>存算分离架构下，计算层不再需要 ISR 这样重量级的副本复制协议，因此我们设计了一种更加轻量的 HA 方案优化了元数据管理机制、降低了系统复杂度。Follower Replica 仅作为计算资源的热备存在，只保有少量必要的元数据，并仅需要处理少量的元数据变化请求，进一步提高计算层的处理效率。这种架构下，新节点能够快速接管数据并提供服务，为弹性的极致体验打下了基础。</p><p></p><p>我们基于存算分离实现的HA 的新方案，相比于开源拥有如下优势：</p><p>&nbsp;</p><p>使用分布式锁解决了主从切换可能带来的双写问题（阻止旧进程对已打开文件继续写入；阻止旧进程新建文件，打开新文件继续写入）确保了 HA 过程的正确性，同时经过一系列的细节设计保证了 HA 过程的高效率；更加轻量化和安全的设计，开源的 ISR 方案是一种较为重量级的副本复制协议，给运维和安全都带来了挑战。在 HA 新方案下，我们优化了元数据管理的层次和复杂度，设计了 HA 事件标记机制，提供高安全性的同时，极大降低了运维的成本；面向极致弹性的设计，开源的 ISR 方案在节点扩容时通过 Reassignment 进行数据和流量的再平衡，这个过程需要等待新副本追赶上 leader 的数据才能提供服务，往往需要相当长的时间。在 HA 新模型下，我们的计算与数据是解耦的，扩容时新副本可以秒级接管数据并提供服务，为用户带来了更加极致，安全的弹性体验。</p><p>&nbsp;</p><p></p><h2>性能</h2><p></p><p>&nbsp;</p><p>我们使用<a href=\"http://openmessaging.cloud/docs/benchmarks/\">OpenMessaging Benchmark Framework</a>\"&nbsp;对阿里云云原生Kafka和Apache Kafka 3.3 进行对比。我们首先对比了在一定的延迟下，达到同样吞吐两者所需的成本；随后对比了聚批发送与碎片化发送下各自的吞吐延迟。</p><p></p><h4>成本对比</h4><p></p><p>&nbsp;</p><p>在一定的、可接受的延迟下达到同样的吞吐，对比云原生Kafka和Apache Kafka各自需要的资源与成本。测试采用单Topic 100分区进行测试，发送消息体大小为 1K，BatchSize为 1M，Acks为 -1，lingms为 1ms，吞吐为 1GB/s。开源用户自建常选择本地SSD机型搭建三节点以上集群，保证服务可用性和数据可靠性。考虑搭建吞吐达到 1GB/s的集群，且本地SSD机型的CPU、内存、带宽、存储容量为固定比例，我们选择阿里云本地SSD机型 ecs.i4.2xlarge（8C 64G，基础带宽 6 Gbit/s，突发带宽为 15Gbit/s；本地SSD 1788GB，读写宽带分别为 3GB/s和 1.5GB/s）机型为例，计算用户自建三节点成本。尽量达到同样的 1GB/s，并且TP999延迟控制在 50ms左右的场景下，两者的成本对比如下：</p><p></p><p></p><p>&nbsp;</p><p>自建三节点Apache Kafka之间存在大量数据复制，ECS带宽很容易达到瓶颈，在吞吐1G/s时，Apache Kafka的TP999 发送延迟开始快速恶化到 56ms，而云原生Kafka的TP999 发送延迟仅有 45ms。同时云原生Kafka所使用的计算资源更少，只需3 x 3C的计算资源。成本上，Apache Kafka 所使用的三台ecs.i4.2xlarge总价为 5514元，提供了1GB/s的吞吐和1788G的存储能力。而云原生Kafka所使用的集群为存储计算分离架构，我们将计算存储分开计算。3 x 3C的计算资源云原生Kafka所需成本为1000元，同时盘古DFS提供1788G的存储（盘古DFS为按量付费，这里按照最大使用量计算）按照1元/G/月计算，为1788元。因此云原生Kafka提供同样的1GB/s的吞吐和1788G的存储能力只需要2788元，对比自建Apache Kafka拥有极大的成本优势。</p><p></p><h4>吞吐延迟对比</h4><p></p><p>&nbsp;</p><p>我们限制计算资源为单台机器4C16G，并搭建三节点集群对比云原生Kafka和Apache Kafka的吞吐、发送延迟（包括了消息到达Broker的时间、Commit成功时间、Broker返回生产确认到生产者的时间）和端到端延迟（包括了消息到达Broker的时间、Commit成功时间、消息到达消费者的时间。消息在Broker Commit后，会优先wakeUp消费者线程。当数据包较小时，消息到达消费者的时间会比Broker返回生产确认到生产者的时间短，因此当流量较小时，会存在端到端延比发送延迟低的情况）。对比聚批发送与碎片化发送两种场景下的吞吐和延迟，两种场景下的生产者负载参数分别如下表： &nbsp;</p><p></p><p></p><p></p><p>聚批发送一般能够达到较高的吞吐，我们不断增加流量，对比不同吞吐下两者的TP999发送延迟与TP999 端到端延迟。结果如下：</p><p></p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2f/4a/2f113600ecc03006fca3cae235b6904a.png\" /></p><p>图8 攒批发送，不同吞吐下TP999 发送延迟对比</p><p>&nbsp;</p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/55/5e/55a09fabdcd9e924d9f418d4f164875e.png\" /></p><p>图9 攒批发送，不同吞吐下TP999 端到端延迟对比</p><p>&nbsp;</p><p>从结果中可以看出，云原生Kafka在TP999的延迟表现整体要优与Apache Kafka，并且随着吞吐的变大，这种优势越发明显。在吞吐达到1000MB/s 时，Apache Kafka TP999发送延迟达到了103.1ms，TP999端到端延迟达到了180ms。而云原生Kafka在同样的吞吐下，相应的延迟分别只有36.9ms和103.5ms。</p><p>&nbsp;</p><p>碎片化发送也是Kafka常见的工作负载之一，和攒批发送一样，我们同样对比了不同吞吐对延迟的影响，同时我们还比较了CPU资源的消耗。对比结果如下：</p><p></p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/09/0b/0910b65fe7960e119993c7b22bb4430b.png\" /></p><p>图10 碎片化发送，不同吞吐下TP999 发送延迟对比</p><p>&nbsp;</p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/af/54/af68c09cc7773d516007d07571069854.png\" /></p><p>图11 碎片化发送，不同吞吐下TP999 端到端延迟对比</p><p>&nbsp;</p><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a4/37/a4665df055f52f3f02814f694776b737.png\" /></p><p>图12 碎片化发送，不同吞吐下单节点平均CPU负载</p><p>&nbsp;</p><p>从图中可以发现，在碎片化发送场景下，云原生Kafka同样整体优于Apache Kafka。并且随着吞吐的增加，云原生Kafka的TP999发送延迟和TP999端到端延迟的优势越大。在吞吐达到200M/s时，Apache Kafka的TP999延迟达到了163.9ms，TP999端到端延迟达到了179.9ms，服务质量严重下降，因此我们停止继续增加流量。但此时的云原生Kafka 的相应延迟只有12.6ms和12.8ms，在可接受延迟下还远没有达到自身的吞吐上限。同时，我们对比CPU资源的消耗，云原生Kafka的平均单节点的CPU负载都远小于Apache Kafka。</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>可以看到，深度结合云原生技术的Kafka&nbsp;Serverless架构，在成本、延迟，弹性等多项核心技术指标上面都超越了社区版的Kafka。未来我们将持续打磨稳定性，补齐产品功能和提升体验，核心技术指标也会持续优化，如以下几个方向：</p><p>&nbsp;</p><p>吞吐：</p><p>操作系统参数调优。大页内存、64k memory page；适配JDK21的Loom虚拟线程。减少上下文切换，提高并发处理能力；探索一写多读，增加RO节点支撑更多的读流量。</p><p>&nbsp;</p><p>延迟：</p><p>适配RDMA网络、TCP BBR拥塞控制算法等降低端到端延迟；JDK21 的generational pauseless GC 优化长尾延迟；基于本地盘的就近访问加速和故障场景快速恢复。</p><p>&nbsp;</p><p>成本：</p><p>持续的软硬协同优化，适配倚天ARM架构，性价比全面超越x86实例；适配新型存储介质，减少读写放大、增加磁盘寿命；根据数据冷热度进行自动分层管理和存储。</p><p>&nbsp;</p><p></p><p>作者简介：</p><p></p><p>林清山，阿里云消息产品线负责人</p><p>张美平，阿里云Kafka负责人</p><p></p><p>参考链接：</p><p><a href=\"https://mp.weixin.qq.com/s/I0hHMOH5-nlfRCBNblg62Q\">https://mp.weixin.qq.com/s/I0hHMOH5-nlfRCBNblg62Q</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s/XWP_6o24o3vNWsd0ILEwEg\">https://mp.weixin.qq.com/s/XWP_6o24o3vNWsd0ILEwEg</a>\"</p><p><a href=\"https://openmessaging.cloud/docs/benchmarks/\">https://openmessaging.cloud/docs/benchmarks/</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-08-17 10:53:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动的多云云原生实践之路",
    "url": "https://www.infoq.cn/article/sK3v54WC6zKyr7HaWFCH",
    "summary": "<p>2022 年，火山引擎联合咨询机构 IDC 对超过 4500 个云消耗大于 100 万的企业进行调研，发现使用多云架构的企业占比达到 88%，达到历史新高。另据麦肯锡的报告，到 2025 年，依然会有 42% 的企业保留有私有云。在负载分布层面，边缘云占比在逐步上升，根据 IDC 报告，25 年超过 30% 的数据需要边缘实时处理。</p><p></p><p>造成这些现象背后的原因是复杂的，既有业务形态和成本管控的原因，也有数据安全和监管要求的考虑。对于企业来说，随着云上迁移的业务变多、复杂度变高，分布式云也成为各类组织必须迎接的挑战。如何做好多云策略，如何平衡好负载，如何保障安全，只有构建好适合自身的分布式云架构，才能真正做到“用好云”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f51e542298901d4faa59f75f39e16045.jpeg\" /></p><p></p><p>在 7 月举办的 ArchSummit 全球架构师峰会上，<a href=\"https://www.infoq.cn/article/qY1fhgRyzNzVVsbwLs0O\">火山引擎云原生平台负责人沈健</a>\"围绕“字节跳动的多云实践之路”为主题进行了分享，介绍了字节跳动实行多云云原生战略的原因、过程和最终成果。</p><p></p><h2>一、业务需求驱动多云架构建设</h2><p></p><p></p><p>云服务经过十几年的演进，如今在企业的应用已经发展出了多云、混合云、分布式云、边缘云、行业云等多种形态。面对业界层出不穷的新概念，很多人会困扰：它们的区别是什么？</p><p></p><p>在云服务商眼中，按照中国信通院发布的定义，所谓分布式云，是一种将云服务按需部署到不同地理位置，提供统一管理能力的云计算模式。它摒弃了公有云、私有云、混合云、多云等分类，首次将地理位置作为考量因素，为用户提供不同位置的云资源统一管理平面，能够增强混合多云一致性管理、拓展边缘计算服务能力、实现云服务统一托管治理。</p><p></p><p>但对于真正意义上需要用云的企业，不同云形态的含义则更加场景化：业务本身需要什么样的云，开发团队有能力用好什么形态的云，企业运维团队的云管理能力成熟度发展到了什么阶段……虽然大家都在谈云，但关注点是全然不同的。</p><p></p><p>字节跳动在发展过程中，也慢慢发展成了多云的状态：无论是中心云、私有云、边缘云，它们都是多云的一种形态，分布式云则是多云之上更高层次的一个形态。这种变化是和业务发展密切相关的：</p><p></p><p></p><blockquote>2017-2018 年，抖音经历快速发展，DAU 增长破亿。在这种场景下，由于单朵公有云、私有云的资源供给都存在时间周期，技术团队很难预估全年具体需要多少资源量，灵活从其他云厂商补充云资源成了一个必要的解决方案。视频直播业务盛行期间，为了更好地保障直播效果，技术团队需要采购对直播网络较友好的云资源——它们往往是地域性的、边缘性的，在业务驱动下，区域云、边缘云也进入了字节跳动的云计算资源池。早期业务出海期间，建设自主数据中心会给新业务带来巨大的成本压力，再加上各国不同的数据安全合规要求，在拓展海外业务的时候，我们也基本上都使用了海外的云资源。随着业务持续增长，出于成本、安全、信创的考虑，避免厂商绑定的重要性也日益凸显。长期使用单一供应商会存在云产品涨价、服务质量下降、技术架构不够灵活等风险，考虑到没有一朵云是 100% 无故障的，技术团队也更愿意选用更多的云供应商提供服务。</blockquote><p></p><p></p><p>由于上述问题的存在，字节跳动的技术团队坚定地选择了多云作为基础架构发展的主要路径。当然，这也带来了一些实践层面的挑战：</p><p></p><p>部署 / 运维复杂度：应用 / 服务多云部署方式，容器、主机、云上服务等不同类型的部署方式都额外增加了部署和运维的难度打通 / 互操作性：网络打通、身份 / 权限打通、运维打通、数据访问打通、流量管理数据管理 / 合规难度：数据离散分布之后数据资产的管理难度加大，数据合规挑战加大、数据泄漏风险和追踪难度加大成本控制复杂度：业务、成本、资产的管理难度</p><p></p><p></p><h2>二、字节跳动的多云实践</h2><p></p><p></p><p>在业务发展驱动下，<a href=\"https://www.infoq.cn/article/r3J9F7CtFJyljwQWwl79\">字节跳动</a>\"的多云实践在不同时期有不同的侧重点，驱动着云原生架构的逐步发展：</p><p></p><p>2016 年，今日头条等业务快速发展，字节跳动基础架构团队启动 TCE（Toutiao Cloud Engine）平台建设，用一个统一的云平台管理之前业务中台各自维护的资源池，解决了应用的快速部署问题和管理问题。2017 年，随着外部竞争态势的复杂化，快速迭代、快速推出新功能变得迫切，我们开始引入微服务架构，通过微服务的灵活性和服务网格的统一治理能力，提供多样性适配，让每个技术人员都能快速投入到业务发展中去。2019 年，抖音、今日头条等业务达到较大规模，频繁的营销活动要求底层有海量云资源供应，在这一阶段，基础架构大力推进了“推广搜”的云原生化，把物理机服务与在线服务进行全面融合，实现统一容器化调度。2020 年，为进一步控制资源使用成本，技术团队实现了常态化在离线混部，在面对高峰流量时能够快速进行资源出让，保障业务稳定性。同时，数据库、缓存等存储系统也开始进行云原生化改造，加速了更大范围资源池的统管和融合。</p><p></p><p>从上述演进不难看出，云原生架构这些年要解决的难题之一就是巨大的资源缺口。大量资源短缺会不可避免地导致“集群建设 — 应用搬迁 — 腾挪资源”，进而带来不小的运维成本和稳定性问题。</p><p></p><p>为了解决这一问题，早在 2019 年，我们就开始进行集群联邦建设，通过解耦应用和集群的绑定关系，将各个业务线的资源并池，以应对分布式云带来的挑战。到 2021 年，字节跳动正式实现了全场景应用编排和资源管理的标准化和统一化，目前联邦集群已管理近 50 万节点，即便面对超过 10 万的微服务数、每天 3 万多次的变更数，也能为业务提供持续、稳定的保障。</p><p></p><h4>多云下的海量算力实践</h4><p></p><p></p><p>如今再看字节跳动的底层算力平台，它可以被分为分布式云原生平台和计算平台体系两部分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19e202a6e434df3903b854dc06f66140.png\" /></p><p></p><p>其中分布式云原生平台汇集所有公有云集群、IDC 集群和汇聚集群（区域性 / 边缘集群），由 开源编排引擎 KubeAdmiral 统一管理。通过分布式的集群编排，在不采取任何其他措施的情况下，字节跳动的常态运维水位可以从 85%-90% 提高到 95%，资源利用率提升非常显著。</p><p></p><p>为了缓解运维复杂度问题，技术团队也开发了一个基于分布式编排引擎的统一调度器 Godel。这是一个融合调度器，能管理在离线资源，调度在离线任务，同时它也针对大规模场景进行了很多性能上的优化。</p><p>资源管控系统 Katalyst 采用 Kubernetes Native 的方式进行重构，能提供更强的资源管理能力、调度能力、抽象能力和数据能力。通过这些能力，技术团队可以更好地按级划分应用使用的资源，实施精细化的资源出让策略、多维度的资源隔离能力、多层级的负载驱逐策略，让整体混部变得更健壮。</p><p></p><p>在这些核心中间件之上，是持续交付、服务网格、应用引擎等服务，这些服务可以识别资源在哪个部门、哪条业务线使用，再通过流量分发引擎调度，实现全局性的资源和流量管理。</p><p></p><p>计算平台体系则是针对字节跳动内部存在的海量离线业务，这类业务存在资源离散的问题：各个云上的存储、各个机房的 HDFS、各个机器学习任务使用的 NAS……为了进行统一管理和使用，技术团队推出了大数据文件存储 CloudFS，提供对接多云对象存储能力，无论用户在哪里、用户想访问的数据在哪里，它都能提供本地缓存加速。</p><p></p><p>离线业务存在的第二个问题是大数据作业无法享受云原生的好处：传统大数据引擎不是针对云原生设计，难以直接云原生部署，各计算引擎和任务需要进行深度改造才能支持原先在 YARN 上的各种特性，改造成本巨大。基于此背景，字节跳动推出了基于云原生的 YARN 解决方案 —— Serverless YARN，它 100% 兼容 Hadoop YARN 协议， Hadoop 生态下的大数据作业无需修改即可透明迁移到云原生系统上，在线资源和离线资源间可以高效灵活转换、分时复用，集群整体资源利用率得到显著提升。</p><p></p><p>在这些系统之上，我们又建设了一个关键模块——多数据中心离线统一资源湖 ResLake。它作为一个融合了计算 + 存储 + 网络的巨大离线算力湖，方便批计算、流计算、AI 训练等任务接入，让技术团队可以进一步加强跨机房资源管控、加强热点数据治理、提升多集群多队列用户体验、提升多机房资源利用率。按照最新数据，在 ResLake 的作用下，技术团队实现了超过 1.4 的作业加速比，队列跨机房流量优化也超过 30%。</p><p></p><h4>降低运维部署复杂度</h4><p></p><p></p><p>对于在线业务，分布式云原生平台就变得至关重要了。举个例子，直播业务之前在各种云上都开了 Kubernetes 资源，在分布式云原生平台上线后，新平台如果需要对这些一开始就游离在外的资源进行纳管，就必须具备对存量应用的无缝接管特性：不仅需要无改造、无运行影响地转移应用，也要能连接多基础设施 Kubernetes 集群，方便集群接入。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c9f4f46184788f7239963d70b2829bf.png\" /></p><p></p><p>除了资源统一，在应用管理方面，分布式云原生平台也提供灵活的跨云分发策略，包含集群名称、标签、污点容忍调度，以及依赖资源的跟随分发。技术团队也着重锤炼和打磨了平台的开源兼容性，使其能完全兼容 Kubernetes 生态，支持原生 &nbsp;Kubernetes 及 CRD 资源、Helm 等应用定义。</p><p></p><p>在日常运维管理方面，字节跳动内部有一套统一的可观测体系，提供在离线应用的监控能力。如前文所述，我们的在离线业务是通过各种各样的中间件被混合在一起的，在这种情况下，我们可以轻松做到统一可观测，帮助业务团队快速定位问题、解决问题。</p><p></p><p>除此之外，字节跳动的分布式云原生平台也提供统一的应用治理。业务应用的实例可以多云多活的部署在不同云上的 Kubernetes 容器服务中，通过多集群的应用、流量、存储等的统一治理，实现高可用容灾，提升整个业务系统的故障弹性和可靠性标准。</p><p></p><h4>降低成本之资源利用率</h4><p></p><p></p><p>在统一资源底座后，技术团队接下来要面对的就是如何长期地提高资源利用率。我们把业务负载按时延容忍度和可重入性进行划分，在下图的两个象限中进行合理分布：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/75d021da464fcd552ca8dd9fe157c32b.png\" /></p><p></p><p>依据这样的分级分类，我们就能判断各个应用对哪些资源相对更敏感，在遇到一些特殊情况时，能够根据不同业务的优先级进行有梯度的分级去除，确保高优先级、高时延敏感任务的稳定运行。</p><p></p><p>此外，隔离能力也是非常重要的一个因素。因为计算机系统本身是一个分布式系统，它包含 CPU、硬盘、存储和网络，字节跳动内部也针对这些不同的算力资源采用了一些隔离机制，比如 CPU 会有一些 cache 隔离、系统级的唤醒能力，硬盘方面则实现了 cgroup 级别的内存回收，以及通过用户态的 advisor 机制实现兜底强杀。</p><p></p><p>技术团队也有尝试借助一些机器学习的能力，使得不同算力能按照不同要求，更精准有效地去匹配这些隔离机制，从而减轻各业务间的干扰影响。</p><p></p><p>目前，通过这些机制，<a href=\"https://xie.infoq.cn/article/dc0f4504a38a99e219f4c468b\">字节跳动</a>\"的混部方案已覆盖数十万机器，天极平均利用率高达 63%，部分核心业务集群也实现了整机天级利用率从 23% 到 60% 的提升。</p><p></p><h2>三、分布式云的下一阶段</h2><p></p><p></p><p>回到落地多云给企业带来的实践层面挑战，除了部署 / 运维复杂度、打通 / 互操作性和成本控制复杂度，最后一点就是数据管理 / 合规难度。随着国际格局愈发复杂，多云 / 分布式云也出现了一些亟待解决的下一阶段发展问题。</p><p></p><p>一方面，近年来 AI 兴起，以 GPU、FPGA、ASIC 为代表的 AI 芯片被广泛应用，并与 CPU 组合来满足高吞吐量、高并发和并发互联的需求。各式各样专有芯片的产生，对算力造成了巨大挑战：如何更好地匹配算力、如何更好地感知不同的算力、如何结合效率 / 成本 / 用户体验做出更加智能精准的判断、如何实现对应的调度……这是分布式云下一阶段在算力调度侧要解决的重要问题之一。</p><p></p><p>另一方面，近年来各个企业也开始越来越重视数据合规，如何对联通的数据进行隐私保护也成了一个重要课题。当前比较流行的方案是隐私增强计算（Privacy-enhancing Computation），包含三个主要流派：</p><p>联邦学习：一种分布式机器学习算法，在不交换原始数据的前提下，完成共享模型训练。联邦学习可以帮助多个参与方共享数据价值，实现数据可用但不可见；可信执行环境：基于硬件的安全机制，将参与计算的代码和数据加载至一个受 CPU 保护的可信环境中，在机密性和完整性上提供保护；多方安全计算：在运行时，多个参与方各自拥有私有数据，他们通过非明文的数据交互，来实现约定的对整体数据全集的某种计算（如联合查询、联合建模等）。</p><p></p><p>上述变化都对企业级云平台的管理能力提出了更高的要求：一是要 有能力解决应用的研发和管理问题，为用户提供一致的云原生体验，包括开发框架的跨云能力、整体效率问题和底层成本问题；二是 需要具备一定的开放接入能力，这是一个面向应用、面向开发者、面向企业的真正意义上友好的多元化增强平台所需要解决的问题。</p><p></p><p>这些问题都会伴随底层问题的破解被一一解决，并走向持续发展。</p>",
    "publish_time": "2023-08-17 13:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "马斯克又出昏招、最疯狂的举动之一！给不喜欢的网站增加5秒延迟",
    "url": "https://www.infoq.cn/article/FaYrWzSqvhhFLYatb6dW",
    "summary": "<p>马斯克正在限制他不喜欢的新闻网站和竞争对手的流量。在X（原Twitter）上点击纽约时报、路透社、Facebook、Instagram、Threads、Bluesky 和 ​​Substack 的链接，X故意增加5秒钟的开启延迟。</p><p>&nbsp;</p><p></p><h2>5秒延迟，新的降权举措？</h2><p></p><p>&nbsp;</p><p>在伊隆·马斯克将Twitter更名为X之外，原本的标识字母已经被从旧金山公司总部移除。</p><p>如今已被更名为X的原Twitter公司，正在刻意放慢用户访问《纽约时报》、Facebook等各新闻机构及X在线竞争对手的链接速度。而此举所影响的，似乎是那些招致马斯克不满情绪的公司。</p><p>&nbsp;</p><p>《华盛顿邮报》于本周二测试了这些延迟，点击X网站上的目标链接之后，X就将其发送到另一个网站，该中间链接将打开一个持续几秒钟的空白屏幕，从而形成一个“延迟打开目标链接”的效果。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3af0d60f8b3d8a2cda43a4a88e54b3bd.png\" /></p><p></p><p>&nbsp;</p><p>遭到延迟启动的网站包括X的竞争对手Facebook、Instagram、Bluesky还有Substack，外加路透社和《纽约时报》。他们之前都曾被马斯克点名嘲笑或攻击。无法确定延迟从何时开始，但Hacker News上的一位用户（由于未获公开发言授权而要求匿名）表示，他首次发现《纽约时报》报道的链接延迟问题是在8月4日。</p><p>&nbsp;</p><p>此番延迟还影响到了X在社交媒体领域的最大竞争对手。Facebook、Instagram和Threads的链接均被限流——这三者全部归Meta所有，其创始人兼CEO马克·扎克伯格跟马斯克更是水火不容。两人甚至打算在综合格斗擂台上一决高下。</p><p>&nbsp;</p><p>其中Threads的5秒延迟据说已经持续了一个多月。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a36f45b622f9a13e2aa4fec9f88168f7.jpeg\" /></p><p></p><p>&nbsp;</p><p>被X限流的还有Bluesky，后者在Twitter前CEO Jack Dorsey的帮助下创立而成，并成为批评马斯克领导能力的重要战场。Substack同样遭到限流，这是一个运行自有短文本服务Substack Notes的邮件通讯平台。</p><p>&nbsp;</p><p>周二早些时候，一名用户率先在技术论坛Hacker News上提出了延迟问题。大家也就这个问题进行了大量的讨论和分析。最早是因为有网友发现 t.co 返回一个空的正文及其 301 响应，以及在点击“纽约时报”的链接的时候，显示五秒重定向延迟：“wget <a href=\"https://t.co/4fs609qwWt\">https://t.co/4fs609qwWt</a>\" ”，它立即重定向到 gov.uk：“wget <a href=\"https://t.co/iigzas6QBx\">https://t.co/iigzas6QBx</a>\" ”。</p><p>&nbsp;</p><p>但也有人怀疑是X（原Twitter）的基础设施“腐坏”了，导致程序中某些“读取”逻辑出现了问题。</p><p>&nbsp;</p><p>然而，周二下午，《华盛顿邮报》发布了首篇报道，几小时之后，大家发现X快速撤销了对某些网站的限制，将延迟时间重归为零了。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a7d10e43b777906e49ccd695bd97ea9.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>“我认为 Twitter，呃，X，刚刚关闭了延迟，因为这个行为引起了媒体的广泛关注。我早些时候可以一遍又一遍地复现它，但现在我不能了。”</blockquote><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/70dd8981d44c23ab1c568aa0e03aecec.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>此次延迟影响的是t.co域，即X用于将各网站链接转换为短链接形式的服务。流量在该域上进行路由，X可以借此跟踪（或者说限制）目标网站的活动，拉低马斯克个人不喜欢的企业的流量与广告收入。</p><p>&nbsp;</p><p></p><h2>这是马斯克最疯狂的举动之一</h2><p></p><p>&nbsp;</p><p>《华盛顿邮报》在分析中发现，大多数其他网站的链接并未受到影响，具体包括《华盛顿邮报》、福布斯新闻、Mastodon以及YouTube等社交媒体服务链接。对于这些网站，短链接都能在1秒以内顺利路由至最终站点。</p><p>&nbsp;</p><p>马斯克没有回应置评请求，X官方也未做评论。在《华盛顿邮报》于周二主动联系时，部分受影响企业表示正就此事开展审查。</p><p>&nbsp;</p><p>《纽约时报》发言人Charlie Stadtlander在一份声明中强调，该新闻媒体“也观察到了类似的”系统性延迟，但“没有收到X平台对此做出的任何解释”。</p><p>&nbsp;</p><p>“虽然我们不清楚引发延迟的具体理由，但我们确实担心新闻机构因为某些不明原因而受到特殊针对。”</p><p>&nbsp;</p><p>Substack网站联合创始人Chris Best、Hamish McKenzie和Jairaj&nbsp;Sethi在发给《华盛顿邮报》的声明中则表示，他们正敦促X撤销对Substack链接的限流决定。</p><p>&nbsp;</p><p>“Substack的建立就是为了直接回应社交媒体企业的这种行为。如果作者与读者之间的联系要依赖于那些没有可靠性可言的平台，特别是那些给用户带来负面影响和改变的平台，那其根本不可能建立起可持续的业务体系。”</p><p>&nbsp;</p><p>在线企业投入了数百万美元来确保自己的网站能尽快显现。因为大家都知道，哪怕是一点点微波的延迟，也会导致网站流量急剧下降。如今的用户往往耐心极差，稍不如意就会关掉页面、前往别处。谷歌2016年对移动流量的一项研究发现，如果网站加载时间超过3秒，那么53%的用户都会直接放弃该网站。一位熟悉《纽约时报》运营情况的人士表示，自延迟问题出现以来，该机构发现来自X的流量确实在下降。</p><p>&nbsp;</p><p>Twitter前信任与安全主管Yoel Roth周二也在Bluesky上发帖，表示此番延迟事件“简直疯狂得令人难以置信，即使是对几乎已经没有下限的Twitter来说也太夸张了。”但他确实在自己的网站上重现了这个问题。经过亲身测试，他表示“延迟确实非常恼人，在潜意识里逼着人放弃访问”。</p><p>&nbsp;</p><p>“这是马斯克迄今为止<a href=\"https://www.linkedin.com/news/story/x-throttles-some-link-traffic-wapo-5734796/\">最疯狂的举动之一。</a>\"”品牌营销人<a href=\"https://www.linkedin.com/in/bl-ochman/\">B.L. Ochman</a>\"也如此评论。</p><p>&nbsp;</p><p>马斯克显然是不惮于利用X的技术工具解决私人恩怨。去年12月在马斯克正式接管之后，Twitter封禁了一个名叫ElonJet的账户，该账户在持续跟踪马斯克私人飞机的飞行情况。除了ElonJet之外，马斯克还封禁了报道此事的记者的账户，甚至连在推文中提及此事的Mastodon官方账户也一并冻结了。</p><p>&nbsp;</p><p>此外，Twitter网站还开始使用各种技术障碍提高用户访问Mastodon的难度，包括将该网站标记为“不安全”并阻止用户在个人资料中添加Mastodon链接。受形势所迫，ElonJet目前只能在Threads、Mastodon和Bluesky上坚持发帖。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/\">https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=37130060\">https://news.ycombinator.com/item?id=37130060</a>\"</p>",
    "publish_time": "2023-08-17 14:12:53",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]