[
  {
    "title": "伯克利AI实验室开源图像编辑模型InstructPix2Pix，简化生成图像编辑并提供一致结果",
    "url": "https://www.infoq.cn/article/JSu06eARbfbxUzEe2iPk",
    "summary": "<p>来自<a href=\"https://bair.berkeley.edu/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">伯克利人工智能研究</a>\"（BAIR）实验室的研究人员开源深度学习模型<a href=\"https://www.timothybrooks.com/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">InstructPix2Pix</a>\"，它可以遵循人类指令来编辑图像。InstructPix2Pix在合成数据上进行训练，表现优于基线AI图像编辑模型。</p><p></p><p>BAIR团队在最近举行的2023年IEEE/CVF<a href=\"https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">计算机视觉和模式识别</a>\"（CVPR）大会上<a href=\"https://arxiv.org/abs/2211.09800?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">展示了他们的工作成果</a>\"。他们先是生成了一个合成训练数据集，其中的训练样本是成对的图像以及用于将第一幅图像转换为第二幅图像的编辑指令。该数据集用于训练图像生成扩散模型，该模型可以接受基于文本的指令来编辑图像。例如，给定一张骑马的人的图片和提示词“让她变成骑龙”，它会输出原始图片，但原来的马被替换了龙。BAIR的研究人员的表示：</p><p></p><p></p><blockquote>尽管模型完全是在合成样本上进行训练的，但它实现了对任意真实图像和人类自然语言指令的零样本泛化。我们的模型能够进行直观的图像编辑，可以遵循人类指令执行多种编辑：替换对象、改变图像风格、修改设置、艺术媒介等。</blockquote><p></p><p></p><p>之前的AI图像编辑能力通常是进行风格转换，流行的文本到图像生成模型（如<a href=\"https://www.infoq.com/news/2021/02/openai-gpt-image/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">DALL-E</a>\"和<a href=\"https://www.infoq.com/news/2022/09/stable-diffusion-image-gen/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">Stable Diffusion</a>\"）也支持图像到图像风格转换操作。然而，使用这些模型进行有针对性的编辑仍然具有挑战性。最近，InfoQ报道了微软的<a href=\"https://www.infoq.com/news/2023/04/microsoft-visual-chatgpt/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">Visual ChatGPT</a>\"，它可以调用外部工具来编辑图像，前提是提供编辑操作的文本描述。</p><p></p><p>为了训练InstructPix2Pix，BAIR首先创建了一个合成数据集。为此，团队在一个由输入文字说明、编辑指令和期望输出文字说明组成的人类文本样本的小数据集上对GPT-3进行了微调。然后，这个微调模型被给予一个大型的输入图像文字说明数据集，从中生成了超过450k次编辑和输出文字说明。然后，团队将输入和输出文字说明馈送到预训练的<a href=\"https://arxiv.org/abs/2208.01626?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">Prompt-to-Prompt</a>\"模型中，该模型根据文字说明生成成对的相似图像。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/243f4068025109361e49dafcb3f62f7f.webp\" /></p><p></p><p>InstructPix2Pix的架构，图片来源：<a href=\"https://arxiv.org/abs/2211.09800?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">https://arxiv.org/abs/2211.09800</a>\"</p><p></p><p>研究人员鉴于这个数据集训练了基于Stable Diffusion的InstructPix2Pix。为了评估其性能，团队将其输出与基线模型<a href=\"https://github.com/ermongroup/SDEdit?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">SDEdit</a>\"进行了比较。他们使用两个指标之间的权衡：一致性（即输入图像和编辑后图像的CLIP嵌入之间的余弦相似度）和方向相似性（即编辑后文字说明中的变化与编辑后图像的变化在多大程度上保持一致）。在实验中，对于给定的方向相似性值，InstructPix2Pix产生的图像比SDEdit具有更高的一致性。</p><p></p><p>人工智能研究员<a href=\"https://en.wikipedia.org/wiki/Andrew_Ng?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">吴恩达</a>\"在他的深度学习新闻邮件组“<a href=\"https://www.deeplearning.ai/the-batch/issue-199/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">The Batch</a>\"”中评价了InstructPix2Pix：</p><p></p><p></p><blockquote>这项工作简化了生成和人造图像的编辑操作，并提供了更一致的结果。巧妙地利用现有模型，模型作者能够使用相对较少的人类标记样本在新任务上训练他们的模型。</blockquote><p></p><p></p><p><a href=\"https://github.com/timothybrooks/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">InstructPix2Pix的代码</a>\"可在GitHub上获取，<a href=\"https://huggingface.co/timbrooks/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">模型</a>\"和<a href=\"https://huggingface.co/spaces/timbrooks/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">基于Web的演示</a>\"可在Huggingface上访问。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/berkeley-instruct-pix2pix/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">https://www.infoq.com/news/2023/07/berkeley-instruct-pix2pix/</a>\"</p>",
    "publish_time": "2023-08-24 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "自主研发多套系统实现数字化转型，安能物流如何打造一栈式物流数据平台？",
    "url": "https://www.infoq.cn/article/4Pw69p65Retxs5kWgTOy",
    "summary": "<p></p><blockquote>作为加盟制货运合作商平台模式的提出者，安能物流是快运行业数字化转型的先行者。与快递行业不同，快运行业面临着业务流程复杂、交易链路长等问题，对数据的准确度、线上交易数据的生命周期等有着较高要求。因此，安能物流IT系统演进路线、建设思考、数据库选型等对行业具备极强的参考价值。</blockquote><p></p><p></p><p>嘉宾｜李家林 安能物流数据库负责人</p><p>&nbsp;</p><p>本文要点：</p><p>1.&nbsp;快运行业的业务特点及安能物流的数字化转型过程；</p><p>2.&nbsp;在数字化转型中，安能物流在人工智能层面做过哪些探索，结果如何；</p><p>3.&nbsp;数据库选型从Oracle到MySQL再到TiDB，安能物流做了哪些思考；</p><p>4.&nbsp;数据库迁移前后，成本和体验发生了哪些变化；</p><p>5.&nbsp;安能物流对于数据底座及大模型技术的未来思考；</p><p></p><h2>快运行业的先行者：安能物流的数字化转型之路</h2><p></p><p>InfoQ：快运行业和快递行业的数字化转型（信息化建设）存在哪些差异？</p><p>李家林：首先，二者的业务形态存在很大不同。快递更多是针对C端，规模达到一定程度后，整个业务流程比较规范化。快运更多是针对B端业务，业务处理的流程环节繁多，业务逻辑也更为复杂。其次，企业在信息化建设过程中可能会因为缺乏顶层设计而走一些弯路，甚至需要将原有系统推到重建，比如现有系统足以支撑当前的业务形态，但却无法解决未来业务灵活多变时所出现的问题，缺少对于未来建设的思考，但大多数情况下只有企业真的走到那一步才会发现问题。</p><p>&nbsp;</p><p>InfoQ：您方便分享下安能物流的数字化转型分为哪几个阶段吗？</p><p>李家林：自2010年成立，安能物流至今已经发展了接近13年的时间。在这个过程中，安能物流自主研发了52套IT系统，开发了一系列专有的、部署于整个业务流程的数字化工具，通过全链路数字化运营和智能化决策实现了效率升级。从系统信息化建设的角度可以分为如下三个阶段：</p><p></p><p>第一个阶段是2016年以前，该阶段采取垂直大集中的模式，主要解决的问题是将所有操作、业务涉及的环节和节点全部线上化，该阶段最大的问题是灵活性不够，单一系统的功能更新会影响全网的所有流程操作。</p><p></p><p>第二个阶段是2016年到2018年，，开始对系统进行拆分，由于对微服务架构的理解不够彻底，导致最终拆分出来的系统过多，虽然解决了灵活性的问题，但忽略了业务流程节点数据环环相扣的问题，且该阶段为了解决特定应用场景的业务问题，使用了多种技术架构。</p><p></p><p>第三个阶段是2018年之后，团队意识到系统拆分过于复杂，且对用户而言操作负担过重，完成一个业务流程可能需要登录多个系统，因此该阶段开始做分布式集中，在保证系统灵活性的同时保证底层数据统一，降低数据交付的复杂度，并提高准确性。基于此，团队提出全链路数字化升级的新目标：构建新一代一栈式物流数据平台，实现货物全闭环实时状态追踪。</p><p>&nbsp;</p><p>InfoQ：在数字化转型的过程中，安能物流有引入过人工智能技术吗？具体做哪些事情？结果如何？</p><p>李家林：第一个探索方向是辅助驾驶，快运行业很大一部分成本来源于车辆运输费用，安能物流也探索过通过辅助驾驶的能力接入节省人力和油耗成本，比如一些长线路要求双驾，通过接入辅助驾驶最终只保留一位驾驶员，并且通过其规划出最佳行驶路线和最优驾驶技巧从而节省燃油成本。经过几个月的测试，我们成功验证了该想法，只是要想达到要求需要对车进行改造，成本比较高，经过测算大概需要三年时间可以回本。据了解，目前已经有货运公司开始跟车厂合作生产这类型的汽车，未来是有可能实现规模化的。</p><p>第二个探索方向是场内货物极速定位。相比于快递行业，快运行业的物品形状更不规则、体积更大，这些因素导致场内货物往往分布在场内更大的区域范围内，随之而来的问题就是需要找到诸如拦截件的时候费时费力。目前，安能物流已经通过RFID射频技术实现了一定距离范围内的快件位置搜寻，整个实践下来确实提升了效率；其次通过RFID技术还能快速的完成场内货物盘点。当下，RFID的成本还是比较高，但我相信未来成熟之后成本会有所降低。</p><p>&nbsp;</p><p>InfoQ：最近两年，行业内对于“降本增效”非常关注，安能物流的IT部门转型过程是否有这方面的思考？</p><p>李家林：安能物流已经在该方向进行实践。具体来说，在组织架构层面，组织变得更加扁平化，IT组织内部按照产品线的方式调整团队，缩减了部门分类；在业务层面，部门内人员的具体定位都做了重新梳理，明确定位了每一个业务角色应该具备的能力和承担的责任；在IT技术层面，团队的职能不仅仅是提供工具和系统给业务人员使用，需要通过已有业务数据挖掘业务潜在价值，寻找新的增长点。</p><p></p><h2>从Oracle到MySQL再到TiDB，安能物流成功打造一栈式物流数据平台</h2><p></p><p></p><p>InfoQ：在数字化转型的过程中，安能物流对底层数据库的需求是如何变化的？</p><p>李家林：在数字化化转型的第三阶段，我们意识到既要保证系统数据的灵活可扩展，又要保证底层数据交互统一，降低数据交互的复杂度。此时，安能核心业务系统—结算系统（涉及总部及下属30000多家加盟网点，主要包含充值、交易、算费、调账、扣款、代收、代付包括税差全费用结算业务流程处理）因面临系统数据动态扩展、高并发、数据生命周期等问题，需要对系统数据库进行重构，此时就面临数据库选型的需求。</p><p><img src=\"https://static001.geekbang.org/infoq/1b/1b04e6dcfe07e7881d6cd6b4631e1eb4.png\" /></p><p>当时，业内主流的选择就MySQL和PostgreSQL两种。经过比较，团队最终选择了当时常用的MySQL加数据库中间件代理的模式。在运行了不到三年的时间，这套系统就出现了比较大的问题，这种伪分布式集群的第一个问题是扩展性太差；第二个问题是库压力不均和性能问题。</p><p></p><p>为了避免同一个网点查询或处理数据时出现跨库问题，当时分库分表设计的思路是以网点编码作为分库表的路由规则，共分了八个库，除此以外，为了保证单库单表数据量尽量均衡，我们还将产粮区如华东华南和非产粮区如东北西北区域的网点进行相互组合后放在同一个库同一个表中，即便如此，由于业务的不断增长，地区产业布局的变化，网点货量的变化，最终不同库的表数据量还是出现较大差异，个别库的压力极大，这就面临需要对该伪分布式集群增加节点或对现有节点数据进行迁移再平衡，无论哪一种方案，都需要重新设计分库分表路由规则和对现有集群数据进行迁移再平衡的问题，且MySQL当时的性能已经到了瓶颈，在线交易数据最多保持六个月的生命周期。基于以上面临的诸多挑战，我们大概在20年开始考虑，要对这套结算系统MySQL数据库进行换代升级。</p><p><img src=\"https://static001.geekbang.org/infoq/62/62304903b589e07b68f64781ec18d219.png\" /></p><p>InfoQ：快运行业对数据生命周期有哪些具体要求？</p><p>李家林：严格意义上来讲，数据都是有生命周期的。相比于快递行业，快运的业务流程节点更多，业务逻辑也更复杂一些，而在每一个业务环节，不但会涉及到业务操作，还会涉及到一些管理的动作，因此每个业务节点都会产生大量数据，而这些数据又环环相扣，这对数据的实时性金和准确性要求非常高。数据生命周期可能会持续一至两年。在结算没有完成之前，数据是不能归档到历史数据的。从业务角度来讲，我们希望在线的结算数据周期生命更长，我们其实没有做严格意义上的冷热数据分离，主要也是考虑到对冷数据进行分析也是存在业务价值的。</p><p>&nbsp;</p><p>InfoQ：基于以上因素，安能物流在数据库选型过程主要考虑哪些因素？</p><p>李家林：在选型阶段，我们调研了很多数据库，我们的目标是希望找到一款性能、稳定性、扩展性可以满足业务诉求的分布式数据库。在测试TiDB之前，我们先测试了PostgreSQL，但PostgreSQL有自己的集群模式，整体测试效果不及预期。虽然PostgreSQL生态也有一些自动化的工具，但与MySQL加代理中间件的伪分布式集群没有本质区别。2020年3月份，我们了解并开始测试TiDB，从3.0版本一直测试到5.4版本。具体来说，3.0版本并没有达到安能结算业务的要求，因为该业务对数据的准确性、大规模数据查询和处理能力及高可用性等方面有较高要求。经过测试发现，在TiDB&nbsp;5.4版本可以满足我们的要求，且在此期间PingCAP团队也和我们团队进行了交流，到目前为止最新的TiDB版本已实现了我们对TiDB提出的所有要求。</p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f35891ff6be333532364b77f3144895.png\" /></p><p>在测试达到要求之后，团队用了接近10个月的时间开始进行迁移，包括迁移前的数据校验等筹备工作。最终，在2023年初完成了结算系统的数据库迁移，原有的MySQL数据库已经下线。由于快运行业的业务流程比较长，后期会分批进行迁移，目前正在进行快件装车扫描、卸车扫描、分拣扫描等运营操作节点的数据迁移，预计今年底全面完成。</p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a9f06b528b8ebeb87193d2d0a791faa.png\" /></p><p>InfoQ：切换前后，整个成本和体验发生了哪些变化？</p><p>李家林：一是TiDB高度兼容MySQL，应用代码几乎无改动，系统平滑迁移至TiDB上，是一款真正的分布式关系型数据库；二是一键水平扩容或者缩容，可按需对计算、存储分别进行在线扩容或者缩容并对应用运维人员透明；三是金融级高可用，计算与存储分离的多副本存储，保证总部与网点费用结算准确性和高可用；四是大规模数据处理能力和实时HTAP特性，增加了在线数据的生命周期，降低系统低延迟；五是内置的图形化监控系统，提供完整闭环监控能力和故障分析能力，降低了运维成本和门槛；六是支持表结构在线变更（DDL），减少了系统变更停机时间，提高了业务的可用性。</p><p>&nbsp;</p><p>InfoQ：为什么没有先从边缘业务开始试点迁移？</p><p>李家林：这与研发团队的技术积累和风格有关。此前已经完成了从Oracle到MySQL数据库的切换，当时是因为Oracle的性能确实无法达到要求，团队充分具备相应的技术积累和沉淀，所以本次就直接选择将核心系统从MySQL切换到TiDB，如果从边缘业务开始试点，整个战线会非常长。</p><p></p><p>InfoQ：安能如何借助于TiDB实现数字化升级目标？</p><p>&nbsp;</p><p>李家林：安能在系统建设过程中，经历了从垂直大集中模式到数据独立拆分，再到微服务架构下数据分布式集中存储这样一个过程。目前我们正将业务全链路环节运营操作系统切换到TiDB上，从而降低系统数据交互的复杂度。如何使用最简单、最灵活、最高效的技术体系和最少的成本构建新一代数据平台，实现货物从下单到签收及结算等全闭环实时状态追踪。</p><p><img src=\"https://static001.geekbang.org/infoq/b0/b054ee318820f7a237fc4140bdba5e46.png\" /></p><p>基于TiDB在安能的应用实践，接下来我们将借助TiDB分布式、高可用性、弹性伸缩、大规模数据处理和实时HTAP等特点来构建新一代一栈式物流数据平台，加速公司数字化升级。</p><p>&nbsp;</p><p>我们将使用TiDB作为一栈式物流数据平台的底座，把我们源端业务系统所采集的所有数据，按照业务主题和数据域统一进行存储，降低不同系统之间数据交互的复杂度，解决数据统一的问题。而在一栈式物流数据平台的基础上，我们可以做相应的数据服务，统一对外提供数据服务平台和API接口，以及更高阶的数据应用产品。</p><p></p><h3>未来规划</h3><p></p><p>InfoQ：面向AI时代，TiDB提出了Serverless的模式，您怎么看待这一动作？</p><p>李家林：目前，Serverless版本还是在云端，企业级的私有化部署可能需要两到三年的时间。对快运行业而言，一个企业内部可能不太需要建很多的TiDB集群，只需要做到用户或应用级别的资源隔离，并给不同的开发人员提供相应的代码开发服务就足够了。基于此，TiDB的Serverless服务可能更多还是面向未来的中小型企业或个人，快运行业可能更多的还是使用其现有的企业级数据库产品。</p><p>&nbsp;</p><p>InfoQ：未来在数据层面，安能物流还有哪些计划？</p><p>李家林：对我们而言，最重要的还是底层的数据。现在，我们选择将TiDB作为新一代物流平台的数据底座，未来希望基于此统一技术栈，用最简单的体系满足业务层面多样化的数据需求。在TiDB的演进过程中，我们需要的功能都正在被PingCAP一一实现，相信未来TiDB还会出现更多令人期待的功能和特性，这也是我们认为的分布式数据库最优选择。</p><p>&nbsp;</p><p>InfoQ：对于大模型相关技术，安能物流未来是否会考虑接入业务？</p><p>李家林：目前，大家普遍想到的场景是智能客服，这需要给大模型输入一些快运行业的特定数据做训练，基本就可以应用了。此外，我们希望可以实现全链路的数据可视化，应用数据挖掘业务的潜在增长点，包括对我们的服务品质和时效性有所提升，通过模型和算法做事前预测，提前干预可能的风险点。</p>",
    "publish_time": "2023-08-24 10:09:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023 中国人工智能成熟度模型报告",
    "url": "https://www.infoq.cn/article/IV4VhedKw1E1tY8Hleje",
    "summary": "<h3>研究背景</h3>\n<p>2022年上半年，AI绘画等生成式AI的内容引发了人工智能领域的广泛关注，年底发布的ChatGPT更是迅速破圈，话题度和讨论度不断提升。这些变化的趋势也延续到了2023年上半年，人们讨论话题也向着大模型的实际应用逐渐深入，同时AI安全也展现了大家对技术突破的不同态度。<br />\n今年年初，InfoQ研究中心发布了<a href=\"http://www.infoq.cn/minibook/UGhD7MTY5Z43JG5YmWP3\">《中国软件技术发展洞察和趋势预测报告 2023》</a>，在报告中将各个领域的关键技术按照不同成熟度阶段进行了划分，总结成为中国技术成熟度评估曲线。报告发布之后也获得了众多企业和开发者的关注和讨论。<br />\n因此，作为年度趋势报告的延续，InfoQ研究中心基于2023年人工智能领域的种种变化，更新《2023 中国人工智能领域技术成熟度模型报告》，为技术的应用决策和未来投资参考提供研究分析工具。<br />\n未来，InfoQ研究中心还将继续推出更多细分领域（云原生、大数据等）的成熟度报告，并定期更新，欢迎大家持续关注。</p>\n<h3>中国人工智能技术成熟度模型</h3>\n<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/0a/32/0affd338817e741dcaf474de3caca432.png\" /></p>\n<h5>备注说明</h5>\n<ul>\n<li>模型涵盖40+人工智能相关技术点，立足三大核心指标（技术专利数量、技术发展时间、技术舆论指数），参考市场规模、融资事件等公开资料，并结合了AI行业内硬件、模型、应用不同领域的各位专家观点。</li>\n<li>技术专利相关数据来自国家知识产权局旗下专利检索及分析系统，检索时间为2023年8月15日。</li>\n<li>技术发展时间使用知网论文库等学术平台进行相关技术领域论文最早收录年份统计计算。</li>\n<li>技术舆论指数数据来源为各家技术媒体和开发者社区，包括InfoQ中文站和CSDN社区。</li>\n</ul>\n<h3>中国人工智能技术厂商生态图谱</h3>\n<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/2c/f3/2ccc22e8a501f6e8885ca7c4a3bb2ff3.png\" /></p>\n<h3>目录</h3>\n<ul>\n<li>⼈工智能技术发展历程</li>\n<li>中国人⼯智能技术成熟度模型</li>\n<li>中国⼈工智能技术⼚厂商⽣生态图谱</li>\n</ul>",
    "publish_time": "2023-08-24 10:10:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何使用 Amazon CodeWhisperer 免费进行代码安全检查",
    "url": "https://www.infoq.cn/article/p4r2t1XP2yXAK55ti8zV",
    "summary": "<p>CodeWhisperer 是Amazon发布的一款免费的AI 编程辅助小工具，在辅助程序员编程的同时，还具备代码安全审计的功能。可以快速扫描 Java、JavaScript 和 Python 项目中难以发现的漏洞，并获取代码建议以立即修复这些漏洞。遵循跟踪安全漏洞的最佳实践，例如开放全球应用程序安全项目 (OWASP) 概述的漏洞，或者不符合加密库最佳实践及其他类似安全最佳实践的漏洞。今天小试牛刀，试验一下CodeWhisperer的代码安全检查能力，成功识别出Python代码中可能存在的系统命令注入漏洞、SQL注入漏洞、MD5碰撞漏洞以及反序列化漏洞，666 ～</p><p></p><h2>系统命令注入漏洞</h2><p></p><p></p><p>以下是一段有安全漏洞的 Python 代码：</p><p><code lang=\"python\">import os\n\nfilename = input(\"请输入文件名：\")\nos.system(\"rm \" + filename)</code></p><p></p><p>这段代码的作用是删除用户输入的文件名对应的文件，但存在安全漏洞。其中的漏洞是，用户可以通过输入特殊字符来执行任意系统命令，而不仅仅是删除文件。例如，如果用户输入的是&nbsp;;ls，则会先删除指定文件，然后执行&nbsp;ls&nbsp;命令。这可能导致系统被攻击者接管或者数据被窃取，因此这段代码需要进行安全性改进。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/dbe0eb387e9b3577d12c706ffac2f4fc.png\" /></p><p></p><p>Amazon CodeWhisperer 成功检测出：系统命令注入漏洞。</p><p></p><p></p><h2>SQL注入漏洞</h2><p></p><p></p><p>以下是一个有安全漏洞的 Python 代码：</p><p></p><p><code lang=\"python\">import sqlite3\n\nconn = sqlite3.connect('example.db')\nc = conn.cursor()\n\nc.execute('''CREATE TABLE stocks\n             (date text, trans text, symbol text, qty real, price real)''')\n\ndate = input(\"请输入日期：\")\ntrans = input(\"请输入交易类型：\")\nsymbol = input(\"请输入股票代码：\")\nqty = input(\"请输入数量：\")\nprice = input(\"请输入价格：\")\n\nc.execute(f\"INSERT INTO stocks VALUES ('{date}', '{trans}', '{symbol}', {qty}, {price})\")\n\nconn.commit()\nconn.close()</code></p><p></p><p>这段代码的作用是向 SQLite 数据库中插入一条记录，但存在安全漏洞。其中的漏洞是，用户输入的数据没有进行任何过滤或转义，从而可能导致 SQL 注入攻击。例如，如果用户输入的&nbsp;symbol&nbsp;参数是&nbsp;ABC'); DROP TABLE stocks; --，则会删除&nbsp;stocks&nbsp;表。这可能导致数据丢失或系统崩溃，因此这段代码需要进行安全性改进。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/05a7f1fbd8b83ac88a1c91f5d3bc4076.png\" /></p><p></p><p>Amazon CodeWhisperer 成功检测出：SQL注入漏洞。</p><p></p><h2>MD5碰撞漏洞</h2><p></p><p></p><p>以下是一个有安全漏洞的 Python 代码：</p><p></p><p><code lang=\"python\">import hashlib\n\npassword = input(\"请输入密码：\")\n\nhash = hashlib.md5(password.encode('utf-8')).hexdigest()\n\nprint(f\"您的密码的 MD5 值为：{hash}\")</code></p><p></p><p>这段代码的作用是计算用户输入的密码的 MD5 值，并输出结果。但存在安全漏洞。其中的漏洞是，MD5 算法已经被证明不再安全，可以被暴力破解或碰撞攻击。因此，如果攻击者获得了用户的 MD5 值，就可以使用彩虹表等方法轻松地破解密码。这可能导致用户账户被攻击者接管或者数据被窃取，因此这段代码需要进行安全性改进。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/40/4098461d87de6d1e6c26b3223a5c88ad.png\" /></p><p></p><p>Amazon CodeWhisperer 成功检测出：MD5碰撞漏洞</p><p></p><h2>反序列化漏洞</h2><p></p><p></p><p>以下是一个有安全漏洞的 Python 代码：</p><p></p><p><code lang=\"python\">import pickle\n\nserialized_data = input(\"请输入序列化数据：\")\ndata = pickle.loads(serialized_data)\n# 使用反序列化后的数据...</code></p><p></p><p>这段代码的作用是对输入序列化数据，进行反序列化。但存在安全漏洞，其中的漏洞是没有对输入进行验证和过滤，直接进行反序列化操作可能导致恶意对象的执行，从而导致远程代码执行或数据泄露。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cf801c61cd5482a0bd8fb1e683779dd.png\" /></p><p></p><p>Amazon CodeWhisperer 成功检测出：反序列化漏洞。</p><p></p>",
    "publish_time": "2023-08-24 10:39:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "1小时搭建对话式搜索服务，阿里云开放搜索 OpenSearch 重磅推出 LLM 智能问答版！",
    "url": "https://www.infoq.cn/article/mcgD3Vy1bii7GZ5R8KiR",
    "summary": "<p>2023 年以来，全球掀起了一场有关大模型的“热潮”，国内外企业纷纷入局参与大模型的研发与训练，调研和推动<a href=\"https://www.infoq.cn/article/NifbwmdGvISTtEyTTNT7\">大模型</a>\"技术与产业结合落地方案，大模型生态蓬勃发展。</p><p></p><p>2023 年过半，我们发现，业内对于大模型技术的关注点已经逐渐从“AI 大模型可以做到什么”、“国内哪些企业拥有大模型技术”等话题，转至“企业如何接入大模型”、“企业如何实际应用大模型技术降本增效”上，越来越多的企业开始关注大模型的具体接入与应用。而大模型的典型应用场景，对话式搜索服务一直备受行业关注，高效、低成本搭建高质量对话式搜索服务，进一步提升用户的信息检索体验则成为了新时代每个企业都需要探索的方向。</p><p></p><p>对此，近期 InfoQ 获悉，阿里云开放搜索 <a href=\"https://xie.infoq.cn/article/9863d685407f528b90b13dd4f\">OpenSearch </a>\"重磅推出 LLM 智能问答版，提供基于大模型的 LLM 智能对话产品，帮助企业在 1 个小时以内，从 0 搭建起自己的对话式搜索服务。自开放搜索 OpenSearch LLM 智能问答版正式发布一个月以来，已有数百家企业接入使用，在最新发布的钉钉个人版中，也大规模应用了产品所提供的能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a74bbe1eaf77375fdd1dd000e0670d73.png\" /></p><p></p><p>钉钉个人版的智能问答能力由阿里云开放搜索 OpenSearch LLM 智能问答版提供支持</p><p></p><p>据悉，企业只需要将原始数据导入实例，开放搜索 OpenSearch LLM 智能问答版即可为其搭建完整的一站式对话式搜索技术链路，企业客户再通过简单的 API 对接，即可将生成搜索结果和传统搜索结果集成在自己的应用中。本文将以阿里云开放搜索 OpenSearch 产品为引，剖析企业接入大模型的痛点、难点，并为大家进一步解读阿里云开放搜索 OpenSearch LLM 智能问答版的技术实践与能力亮点。</p><p></p><p></p><h2>企业接入 AI 大模型的趋势已经到来</h2><p></p><p></p><p>目前，企业接入大模型的需求场景主要围绕在电商行业选品、智能对话客服、企业内部知识库资料智能对话检索，以及企业自有 App 的搜索推荐系统搭建等。</p><p></p><p>来自阿里云 OpenSearch 的行业调查结果显示，企业自行接入大模型，普遍面临着二次开发任务繁重、模型 finetune 困难、运维工作量大、缺乏清晰的业务目标测评方法等实际难题。具体而言，在接入大模型方面往往面临以下六大“拦路虎”：</p><p></p><p>人才储备与技术能力不足：大模型的研发训练和维护需要专业的技术团队支持，但是部分企业没有招聘和培养相应的人才，无法确保大模型能够得到有效支持和维护。合规与数据安全难保障：企业需保障大模型的对话结果基于业务数据生成、生成内容安全合规，并且保障企业数据安全，避免相关法务风险。硬件基础不够：大模型需要大量的计算资源，包括 GPU、TPU 等硬件设备，这对于个人或小型企业来说，可能是难以承担的开销。模型优化和解释性问题：随着模型参数数量的增加，调整参数的复杂度也在增加。如果企业研发人员经验不够，参数调节不当，可能会导致过拟合或欠拟合等问题；大模型的可解释性比较差，所以对于一些对模型解释性能力要求比较高的领域（例如医疗、金融等）而言调整难度更大。维护成本困境：维护大模型的调优、数据更新需要占用较多人力与时间成本。训练语料不充分：大模型的训练需要大量训练数据和语料信息，如果企业相关数据较少，很难获得较好的大模型训练效果。</p><p></p><p>目前，多家国内外知名企业均已发布自主研发的大模型，并宣布未来旗下产品将全面接入大模型。这意味着，大模型技术本身已经具备企业接入与业务落地的成熟度，接入大模型是各行业的整体趋势，但对于大部分企业来说，与其自行训练大模型，不如直接选用大模型的应用层解决方案，将资源聚焦在业务与需求本身，用更绿色、更低碳的方式推动整个信息产业技术升级。</p><p></p><p></p><h2>阿里云 OpenSearch LLM 如何让企业具备智能对话能力？</h2><p></p><p></p><p>阿里云 OpenSearch 始终致力于帮助企业客户构建高质量的端到端搜索服务，本次快速推出 LLM 智能问答版，提供端到端全托管的对话式搜索服务，也是 OpenSearch 产品希望帮助企业解决算力不够、人力时间成本高昂等大模型接入问题，让每一家企业都能够便捷地接入大模型，享受全新一代的搜索体验，提升信息检索效率，感受 AI 为业务带来的改变的新使命。</p><p></p><p>OpenSearch 的智能对话与文本生成能力通过 LLM 提供，这里我们以阿里云自研的通义千问为例，介绍 OpenSearch LLM 智能问答版的技术实现。</p><p></p><p></p><h3>OpenSearch LLM 智能问答版系统架构&nbsp;&nbsp;</h3><p></p><p></p><p>OpenSearch LLM 智能问答版系统架构主要包含业务数据处理、对话搜索在线服务、大模型预训练三个部分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c8909c0b781de713f06118925010efa1.png\" /></p><p></p><p></p><h4>业务处理数据</h4><p></p><p></p><p>相比传统的搜索引擎，OpenSearch &nbsp;LLM 智能问答版离线数据处理流程最大的变化点在于对业务数据的处理：</p><p></p><p>传统搜索引擎的数据源是结构化文本，而这里需要处理的往往是非结构化文本，并且数据的格式会更加多样（HTML、Markdown、纯文本、PDF 等）。传统搜索引擎构建索引是基于文档的唯一主键，而这里由于数据源的差异，需要先对文档进行段落拆分，对拆分后的段落生成新的段落主键。传统搜索引擎基于文本索引进行内容匹配，而这里采用向量索引，更加容易适配丰富的数据格式和长文本搜索。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/67a8dd1ea27e160b34ae50c7f28f7794.png\" /></p><p></p><p></p><h4>对话搜索在线服务</h4><p></p><p></p><p>据悉，相比传统的搜索引擎，OpenSearch 智能问答版在线服务架构变化非常大，主要区别有：</p><p></p><p>传统搜索一般会返回 10 个以上的结果，并且经常会以翻页查询的方式呈现结果，而这里的检索是为了找到相关度相对最高的段落内容，Top N 中的 N 不宜过大（一般在 3 以内），且需要控制相关性，确保不召回相关性过低的段落带来误导。检索完成得到 Top N 搜索结果后，将结果添加到 prompt 中输入大模型，这一阶段耗时一般较大，OpenSearch 智能问答版支持流式输出以缓解等待时间过长的体验问题。返回结果时，基于用户业务数据，通过 API 输出指定 Query 下的参考链接、参考图片和模型生成的对话结果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c5fe3b648498c2780b2ba2881c169554.png\" /></p><p></p><p></p><h4>大模型预训练</h4><p></p><p></p><p>阿里云 OpenSearch 深度整合和应用通义千问模型，为了提升模型在检索增强场景下的有效性，减少有害性，OpenSearch LLM 智能问答版还对模型进行了有监督的模型微调（Supervised Finetune，SFT）进一步强化检索增强的能力。对比 SFT 后的对话搜索模型与原始 LLM，阿里云 OpenSearch 团队发现：经过 SFT 的模型更擅于总结输入文档中的内容，从而精准简练地回答用户问题，达到智能对话搜索效果。</p><p></p><p>此外，OpenSearch 还可以支持包括 Llama2、openbuddy、falcon 在内的多种开源模型，用户无需开发、集成、部署，即可直接使用 LLM 能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/068c254f361ee61d34cb9fa0338360eb.png\" /></p><p></p><p></p><h3>强大的对话检索能力&nbsp;&nbsp;</h3><p></p><p></p><p>阿里云 OpenSearch LLM 底层采用了阿里巴巴自研的大规模高性能搜索引擎，可以保障搜索环节的召回准确度与性能，擅长处理向量维度更高的大模型场景。</p><p></p><p>除了引擎优势之外，阿里云 OpenSearch LLM 智能问答版通过内置段落拆分模型、内置文本向量化模型、内置图片向量化模型，保证了对话搜索结果的精准度。同时已支持多轮对话，结合上下文挖掘用户需求，对话搜索结果全部基于客户数据生成，保障内容的可靠性和相关性。</p><p></p><p>在此基础上，阿里云 OpenSearch LLM 智能问答版提供了充分开放的模型能力，包含多种底座 LLM 可替换，自定义 prompt，模型参数调优与排序策略优化等，此外还支持对话结果人工干预，满足对语言、风格等对话式搜索效果的更高要求。</p><p></p><p>在工程架构方面，阿里云 OpenSearch 诞生于电商场景，经过内部业务多年打磨沉淀，能够稳定承载千亿级数据、百万级 QPS 的业务流量。读写分离的架构使得数据写入与查询相互不影响，能够有效保障服务的稳定性。此外，产品支持对话结果的流式输出，能够缓解由于回答生成速度慢带来的用户体验问题。</p><p>段落拆分模型：ops-text-ace-001</p><p></p><p>模型最终生成的效果很大程度上是由检索结果决定的。传统文档检索系统只需要针对 Query 给出最相关的文档列表。检索增强式 LLM 则需要给出具体与 Query 相关的段落。权衡效率与效果，OpenSearch 智能问答版的段落拆分模型支持结构化数据及包含 doc、pdf、html 在内的多种非结构化数据的快速导入，并能够实现自适应段落长度，保障语义段落的完整性。</p><p></p><p></p><h4>文本向量化模型：ops-text-embedding-001</h4><p></p><p></p><p>相比于传统搜索，在与 LLM 的交互中，一个很大的改变是用户可以非常自然地口语化输入。对于口语化输入，基于语义的向量检索架构天然契合。OpenSearch 内置自研高性能向量检索引擎，擅长处理向量维度更高的大模型场景，可以达到数倍于开源引擎的搜索性能和更高的召回率。</p><p></p><p>为了更加适配多语言、多行业的对话搜索场景，OpenSearch 算法团队进行了定制模型研发与效果调优，并对模型效率进行了针对性地优化，以满足搜索场景实时性的需求，最终产出 ops-text-embedding-001 模型。在中文数据集 Multi-CPR 上，ops-text-embedding-001 模型在 MRR@10 等关键指标上的表现已经优于行业标杆向量模型即 OpenAI 的 text-embedding-ada-002：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/2063b660d3e01eafa4938867e7569a08.png\" /></p><p></p><p></p><h4>图像向量化模型：ops-image-embedding-001</h4><p></p><p></p><p>对于内容行业而言，大量关键信息以图片的形式呈现，图文结合的多模态展现可以让企业专属智能对话搜索效果大幅提升。为了在对话式搜索中实现图文并茂的效果，OpenSearch 智能问答版也提供了图片向量化模型 ops-image-embedding-001，该模型结合多模态的信息，计算 Query 与文档中图片的图文相关性，最终返回相关性最高的图片作为参考图片结果。</p><p></p><p></p><h4>对话式搜索效果调优</h4><p></p><p></p><p>影响对话式搜索业务效果的环节通常包含三个方面：</p><p></p><p>底座大模型的效果OpenSearch 支持多种底座大模型的选择，除阿里巴巴自研的通义千问大模型，还包含开源的 Llama2、openbuddy、falcon 等多种 LLM 可选，用户可基于自身的业务需求、场景、语言等，尝试并使用最适配的 LLM。同时，还支持调整部分模型参数，解决对话式搜索过程中的 bad case。搜索召回的效果除上文已经介绍过的段落切分模型和文本向量化模型外，OpenSearch 还内置多种搜索召回排序函数与规则，用户可以自行定制排序策略，召回最符合业务需求的结果。Prompt 适配度Prompt 中除了包含搜索召回的业务文档外，还需要包含对 LLM 的角色、指令控制，以实现预期的效果。OpenSearch 支持用户自行调整输入给 LLM 的基础 Prompt，用户可基于自身业务需求，灵活调整语言、语气、无结果时的回答语、输出格式等内容，充分满足灵活性和效果调优需求。</p><p></p><p>此外，针对搜索领域常见的高频 Query 干预与效果调优问题，OpenSearch 智能问答版还支持基于知识库的人工干预。用户可以指定干预问题与对应回答，OpenSearch 智能问答版会识别相似问题，并根据知识库中的预设结果给出相应答案，从而实现针对指定 Query、活动等场景的运营干预，使对话结果更安全可靠、服务于企业业务。</p><p></p><p>目前，OpenSearch LLM 版本已与多个阿里巴巴内部业务、云上客户合作实现方案落地，帮助机器硬件资源稀缺、不具备前期数据切片、向量化等环节的处理能力和经验的企业，基于垂直领域数据构建对话式搜索服务。</p><p></p><p>以阿里云产品文档数据作为业务数据，基于 OpenSearch LLM 智能问答版搭建对话式搜索系统，我们可以看到如下的对话搜索效果演示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90d7c8bde4f5f0d33967465c253ce7a1.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/6811c3f063ff4433b55fd8d24ecd1dd5.png\" /></p><p></p><p>据悉，在上述效果演示中，企业只需将相应的文档数据导入 OpenSearch LLM 智能问答版，即可根据用户输入的 Query 返回对话模型生成的图文答案和相应的参考链接，实现智能对话搜索效果。</p><p></p><p>未来 OpenSearch LLM 智能问答版将提供更多行业的解决方案，为各垂直领域提供行业专属大模型，并将上线电商行业大模型。此外，OpenSearch 还将在现有能力基础上，推出 LLM 智能问答专业版，支持更定制化的专属模型 finetune、预训练等能力。未来 OpenSearch LLM 智能问答版将支持客户更高自由度的模型使用，满足业务对生成风格、生成效果的更高要求。</p><p></p><p></p><h2>对话式搜索将迎来更广阔的应用</h2><p></p><p></p><p>阿里云 OpenSearch 除了提供基于内置大模型的 LLM 智能问答版之外，也面向开发者推出基于 OpenSearch 向量检索版 +LLM 工具包的大模型应用解决方案，支持客户灵活选用文档切片方案、向量化模型、大语言模型，为客户提供高性价比的向量检索服务，并已经将向量检索版本核心引擎 Havenask 进行了开源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4f868e9aee3455a19f294f7dd71baf11.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/6621f305d026b27779d6f19a7797a5ba.png\" /></p><p></p><p>OpenSearch 向量检索版 + 大模型方案图示</p><p></p><p><a href=\"https://help.aliyun.com/document_detail/2504703.html\">OpenSearch 向量检索版 + 大模型方案</a>\"</p><p><a href=\"https://github.com/alibaba/havenask/blob/main/llm/README.md\">开源 Havenask+ 大模型方案</a>\"</p><p></p><p>对话式搜索将对电商场景、内容社区场景、金融场景、政企服务场景进行大规模的体验升级，用户与平台的交互方式也会因大模型技术而带来全新的改变，所以产品的落地形态和用户体验一定是这场大模型变革中的下一个重要战场。因此，无论是推出 LLM 智能问答版这种全托管的 MaaS（Model as a Service）产品，或是在云上提供更高性价比的 OpenSearch 向量检索引擎及 LLM 工具包，还是下定决心将核心引擎开源的一系列举措，OpenSearch 的目标就是希望为整个行业提供新的技术方案与选型，解决不同状态、不同阶段、不同预算的企业客户及开发者遇到的共性难题，即 OpenSearch 搞定对话式搜索场景的通用基础设施，企业级开发者专注进行场景级的业务优化与落地。</p><p></p><p>未来 OpenSearch 还会进一步“OPEN”，把整套工程链路彻底开放给大模型的开发者，任何通过认证的大模型都可以随时上架到 OpenSearch 平台，在合规的前提下为第三方开发者提供服务，这也是在全球 AI 行业飞速发展的今天，OpenSearch 与全行业共建的决心。</p><p></p><p><a href=\"https://www.aliyun.com/activity/bigdata/opensearch/llmsearch\">点击了解 OpenSearch LLM 智能问答版</a>\"</p>",
    "publish_time": "2023-08-24 14:36:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "挑战三大任务，Amazon CodeWhisperer 生成代码的能力到底如何？",
    "url": "https://www.infoq.cn/article/e52aPvCV9EldytRdXJ4o",
    "summary": "<p>ChatGPT 火出圈之后，AI 大模型编程越来越多，虽然 AI 编码暂时无法完全替代程序员，但是时代变化、潮流趋势所向，大家没有必要过多焦虑，而是应该拥抱变化，拥抱趋势，尝试用 AI 辅助自己的编码，看是否可以得到帮助，本文试用了 Amazon 的 CodeWhisperer AI 编程工具，在这里做一个小小的总结。</p><p></p><p>CodeWhisperer 通过 AI 技术，可以自动分析代码库中的模式和常见用法，从而生成符合标准的代码片段，其旨在帮助开发人员节省时间和精力，提高开发者的工作效率。</p><p></p><p>当前 CodeWhisperer 支持集成到几种开发环境中，VS、Jetbrains、JupyterLab，Lamda 等，由于平时用 C++/Python/Go 比较多，因此本文使用 JetBrains 的 Clion 来测试 CodeWhisperer 生成 C++的能力，首先安装好 Clion(具体步骤网上找或者亚马逊官网指导书)，进行 Clion 后，首先通过 tools 搜索安装 aws toolkit，安装好后启动就可以使用 CodeWhisperer 了，第一次启动时会生成一个验证码，链接到你的亚马逊账号进行授权，此处不细讲，按照提示操作即可</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef63844bbeea544664f257fc9e5ac74a.png\" /></p><p></p><p>下面进入正题：</p><p></p><p>使用 CodeWhisperer 生成代码，需要添加注释，其会根据注释一行一行完成代码编写，并且会帮助你完成要编写代码的注释</p><p></p><h4>任务一：</h4><p></p><p></p><h4>测试生成全部代码能力，主题完成一个 TCP Server 收发数据，下面图里是一步步的结果</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/93df399819258175a155ccd9a5dad993.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/caafc3e43740ba7473d54142d41be9c1.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/22a50c6937c0da324e9fe1e5399083f4.png\" /></p><p>最终完成的代码如下</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c8644ef532436742c9cb3752c4b3df93.png\" /></p><p>生成过程并非像 ChatGPT 一样，一股脑的代码全部生成扔给你，需要开发同学一行一行的插入确认，这种情况也有好处，在于可以在开发过程中自己一行一行的确认正确性及问题，避免一大堆复杂代码重新费神的确认逻辑。</p><p></p><p></p><h4>任务二： </h4><p></p><p></p><h4>推荐相应功能的开源库以及使用已集成的开源包进行代码生成测试，主题完成一个 SIP 消息处理函数，下面图片展示其完成过程和结果</h4><p></p><p></p><p>(1)使用 osip2 解析 sip 消息，下面是生成的代码，基本符合预期</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/15f8b2426121690100cb2d8c9c275f9f.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/054d766be22667efdc706807de2d4e39.png\" /></p><p>&nbsp;(2) 期望重新推荐一个 sip 消息解析库，不知道是我已经集成了解析库还是还是什么其他原因，没有成功</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8e16857a03ee50f5623f198e8e26209.png\" /></p><p>可见 CodeWhisperer 对于 Github、Gitee 以及互联网上标准开源库是进行过训练的，可以读懂 oSip2 是一个很有名的 sip 协议栈，因此可直接使用其 API 根据注释生成可用代码</p><p></p><p></p><h4>任务三：</h4><p></p><p></p><h4>基于已完成部分代码，根据注释生成补全代码，CodeWhisperer 也可以根据上下文和注释补齐代码的相应功能，建立一个资源分配函数(带一定的业务功能)，完成一半的部分(&gt;4)，让 CodeWhisperer 完成剩下的部分(&lt;4)，下面展示结果</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10950b27f9b396e9bed6c95c920fb09e.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3c/3c75818968ed522698b3f2c385b954eb.png\" /></p><p></p><p>最终的结果</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ffb81c9080c4baa0d267ede1128174bb.png\" /></p><p></p><p>至此任务三测试完成，生成了基本可用的代码。</p><p></p><p>最后总结，CodeWhisperer 对于能够更快地编写代码还是有一定的帮助。</p><p></p><p>首先，在集成开源或者第三方不熟悉的代码库时，它可以为我节省大量的时间去学习和查阅 API 接口文档，让我能够专注于改进和测试。</p><p></p><p>其次，可以帮助我节省繁琐的重复性工作，如上面的 TCP Socket 处理。</p><p></p><p>当然，也期望可以后续可以生成整体代码段的方式，对于非复杂逻辑代码的场景其效率是更高的。</p><p></p><p>版权声明: 本文为 InfoQ 作者【Hanson】的原创文章。</p><p>原文链接:【<a href=\"https://xie.infoq.cn/article/386428468269729e334f2c134\">https://xie.infoq.cn/article/386428468269729e334f2c134</a>\"】。</p><p>本文遵守【CC BY-NC】协议，转载请保留原文出处及本版权声明。</p>",
    "publish_time": "2023-08-24 14:38:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "美团容器平台调度策略团队负责人陆启超确认出席 QCon 北京，分享从 0 到 1 构建集群服务质量运营体系降低云成本",
    "url": "https://www.infoq.cn/article/4vP8DAo4jUBUglb5IUQd",
    "summary": "<p>9 月 3 日 - 5 日，在<a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=0824&amp;utm_content=luqichao\"> QCon 全球软件开发大会（北京站）</a>\"上，美团容器平台调度策略团队负责人陆启超将发表题为《从 0 到 1 构建集群服务质量运营体系降低云成本》主题分享，介绍美团如何通过技术创新和产品优化等多重手段解决集群资源利用率提升和服务质量保障这一对集群运营矛盾统一体、集群服务质量运营体系（Quality-Cost Ops）、在 FinOps 落地实践中遇到的挑战，以及未来集群运营的演进方向。</p><p></p><p>陆启超，美团容器平台调度策略团队负责人。从事基础架构相关研发设计工作十余年，涉及分布式存储、集群调度等领域，主持设计开发多个大型目，在分布式系统设计、资源调度以及降本增效运营方面有丰富的经验。从业经历包括创业公司和互联网公司（京东），目前在美团负责容器平台的资源调度、资源利用率及服务质量运营工作。他在本次会议的演讲内容如下：</p><p></p><p>演讲：从 0 到 1 构建集群服务质量运营体系降低云成本</p><p></p><p>本次演讲主要介绍美团通过技术创新和产品优化等多重手段，解决了集群资源利用率提升和服务质量保障这一对集群运营矛盾统一体。通过在集群运营层面落实集群资源利用率运营，实现在不同场景下有效的做到资源利用率提升降低运营成本，同时又不影响业务的稳定性，通过体系化运营实现了为公司降本增效的目标。</p><p></p><p>演讲提纲：</p><p></p><p>资源利用率运营的难点与挑战美团的集群服务质量运营体系（Quality-Cost Ops）介绍FinOps 落地实践中的挑战未来演进方向</p><p></p><p>你将获得：</p><p></p><p>○ 集群服务质量运营一些新思路，如何持续在集群运营中资源利用率提升和服务质量保障的矛盾</p><p>○ 资源利用率提升和服务质量保障上具体的实践经验</p><p>○ 未来集群运营的思路和展望</p><p></p><p>除上述演讲外，QCon 北京还将围绕 <a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1570?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">云原生</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1567?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">AIGC&nbsp;浪潮下的研发效能提升</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1552?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">面向&nbsp;AI&nbsp;的存储</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"等进行分享。</p><p></p><p>130+ 名嘉宾、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-24 15:10:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "DarazPrincipal Engineer Leader 冯湧，确认担任 FCon 基于大数据和 AI 的风控系统",
    "url": "https://www.infoq.cn/article/JaV436vlaoHP0WFUqzmF",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。DarazPrincipal Engineer Leader 冯湧将担任「<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1578?utm_source=infoqweb&amp;utm_medium=article\">基于大数据和 AI 的风控系统</a>\"」的专题出品人。在此次专题中，你将了解到如何利用机器学习、大数据模型，来提升数据在风控中的价值，以及对未来的发展展望。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/track/1578?utm_source=infoqweb&amp;utm_medium=article\">冯湧</a>\"，从业 20 余年，目前担任 Daraz 的 Principal Engineer Leader，之前在 Shopee 担任风控团队负责人。在职业历程中，曾担任美团的信贷、支付研发团队和保险平台的研发团队的技术负责人，同时还兼任美团金服技术委员会的副主席。</p><p></p><p>个人专注于电商、供应链以及金融行业的系统设计与研发。技术领域涵盖金融和互联网行业的业务系统设计、风控系统设计，高可用架构设计，以及体系化的研发团队管理。此外，在各大公司任职期间内，也曾荣获了多项国家和公司相关专利。</p><p></p><p>相信冯湧的到来，可以帮助提升此专题的质量，让你了解到，黑产在数据收集方面已有较大优势，使得企业在风控对抗中面临挑战、实时更新风控规则对风险控制具有重要意义，如何运用机器学习和大数据模型提升数据在风控中的价值，和对未来发展的展望。</p><p></p><p>除上述专题外，FCon 上海还将围绕 <a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等专题进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：13269078023（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-08-24 15:37:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌认真起来，就没OpenAI什么事了！创始人组队打造“杀手级”多模态AI模型",
    "url": "https://www.infoq.cn/article/jFYJRGB0rgH5prgwzAdE",
    "summary": "<p>截至目前，OpenAI大语言模型在AI竞赛中一直处于领先地位。而强劲优势的背后，离不开微软庞大数据中心基础设施的有力支持。但ChatGPT的主导地位恐怕无法长久持续下去，因为新的、更强大的AI模型正不断涌现，而其中最具战斗力的挑战者就来自谷歌。</p><p>&nbsp;</p><p>今年 4 月，Alphabet 首席执行官桑达尔·皮查伊 (Sundar Pichai) 迈出了不寻常的一步：合并两个具有不同文化和代码的大型人工智能团队（谷歌Brain和 DeepMind 团队），以赶上并超越 OpenAI 和其他竞争对手。</p><p>&nbsp;</p><p>现在，检验这个团队工作成果的时刻即将到来。有消息称，这支数百人组成的团队将在今年秋天发布一组大型机器学习模型Gemini，这是该公司有史以来构建的风险最高的产品之一。据参与 Gemini 开发的人士透露，这些模型统称为 Gemini，预计将使谷歌能够制造出竞争对手无法制造的产品。</p><p>&nbsp;</p><p>谷歌Gemini于今年5月在I/O开发者大会上首度亮相。</p><p>&nbsp;</p><p>当时，谷歌称Gemini为其下一代基础模型，它仍在训练中。Gemini 是从一开始就以多模式、高效的工具和 API 集成为目标而创建的，旨在支持未来的创新，例如内存和规划。经过微调和严格的安全测试后，Gemini 将提供各种尺寸和功能，就像 PaLM 2 一样。</p><p></p><h2>全世界都在关心的Gemini到底是个啥？</h2><p></p><p>早在 2016 年，DeepMind 就因其人工智能程序 AlphaGo 在复杂的围棋游戏中击败了一位冠军选手而成为头条新闻。快进到今天，DeepMind首席执行官Demis Hassabis透露，他的团队正在利用 AlphaGo 的变革性技术来创建 Gemini AI。Demis Hassabis透露，Gemini AI的开发成本估计为数亿美元，使用了数万颗谷歌的TPU AI芯片进行训练。</p><p>&nbsp;</p><p>据悉，Gemini AI是一个类似于ChatGPT的GPT-4的大规模语言模型。然而，Hassabis和他的团队更进一步，为 Gemini AI 注入了源自 AlphaGo 的解决问题能力和战略规划能力。</p><p>&nbsp;</p><p>从根本上讲，Gemini AI 包含下一代 AI 架构，有望取代 Google 当前的 AI 模型 PaLM 2。该模型目前支持 Google 的一系列 AI 服务，例如 Workspace 应用程序中广泛使用的 Duet AI 和流行的 Bard 聊天机器人。</p><p>&nbsp;</p><p>谷歌还放出消息，称Gemini将为旗下AI聊天机器人Bard，以及Google Docs、Slides等企业级应用提供支持。</p><p>&nbsp;</p><p>The Information报道称，谷歌并不是简单地与 ChatGPT 等产品竞争，而是打算超越一众大模型产品让友商们无法望其项背。消息人士指出，该公司专注于将大型语言模型 (LLM) 的文本功能与人工智能图像生成相结合，以创建多功能产品。这意味着 Gemini 不仅能够像 ChatGPT 那样生成文本，还能够创建上下文图像，但据报道，谷歌也在考虑添加其他功能。例如，用户最终可能能够使用 Gemini 通过语音分析流程图或控制软件。</p><p>&nbsp;</p><p>Gemini之所以能够成为强大的竞争对手，是因为谷歌同样掌握着雄厚的资源储备，特别是用于训练AI模型的宝贵数据。谷歌能够访问YouTube视频、谷歌图书、庞大的搜索索引以及Google Scholar上的学术资料。其中大部分数据为谷歌所独有，这也使其在构建顶尖AI模型方面占据着超越其他厂商的优势。</p><p>&nbsp;</p><p>那么，Gemini在训练中，具体都用到了哪些数据集？</p><p>&nbsp;</p><p></p><h3>Gemini用到了哪些数据集？</h3><p></p><p>&nbsp;</p><p>据悉，Gemini项目汲取了谷歌多个项目的数据集来训练大模型，包括了Google&nbsp;Piper monorepo、DeepMind&nbsp; MassiveText以及YouTube中的数据。</p><p>&nbsp;</p><p>来自 Google&nbsp;Piper monorepo的 Gemini 数据集（估计）</p><p>&nbsp;</p><p>Gemini 数据集可能由大量代码组成，以支持最终训练模型中的推理。Google 的内部 monorepo Piper 大小为 86TB 。使用 The Pile 的每字节 0.4412 个令牌的计算，该数据集将约为 37.9T 个令牌，或者大约是 GPT-4 中下一个最大数据集大小的两倍（估计）。</p><p>&nbsp;</p><p>来自DeepMind&nbsp;MassiveText的Gemini 数据集（估计）</p><p>&nbsp;</p><p>Gemini 数据集可能由 DeepMind 的一些MassiveText（多语言）&nbsp;5T 令牌数据集组成</p><p>请注意，下表是关于Gemini 数据集的猜测（未经 Google DeepMind 确认），并且基于来自最先进的 DeepMind MassiveText（多语言）+ 1,000B 讨论令牌的可用信息。MassiveText 包括网页、书籍、新闻和代码等文本，包含约 23.5 亿个文档， 10.5 TB 的文本量。</p><p></p><p></p><p>MassiveText 多语言数据集估计。</p><p>*四舍五入大概的数据以粗体显示（来自 DeepMind 的 MassiveText 多语言数据集），确定的数据以斜体显示。</p><p>&nbsp;</p><p>来自YouTube 的Gemini 数据集（估计）</p><p></p><blockquote>据一位知情人士透露，谷歌的研究人员一直在使用 YouTube 来开发其下一个大型语言模型 Gemini。</blockquote><p></p><p>&nbsp;</p><p>YouTube 2023 总体统计数据（来自<a href=\"https://www.wyzowl.com/youtube-stats/#:~:text=If%20we%20assume%20there%20are,videos%20in%20the%20mean%20time!\">Wyzowl</a>\"和<a href=\"https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/\">Statista</a>\"）：</p><p>视频总数：8 亿。平均长度：11.7 分钟。总时间：93.6亿分钟。四舍五入以跟上每小时上传 30,000 小时的速度：10B 分钟。</p><p>&nbsp;</p><p>YouTube 2023 文本统计数据：</p><p>人类说话速度：每分钟 150 个单词 (wpm)。150wpm x 10B 分钟 = 总计 1.5 万亿字。假设：(1) 说话仅出现在视频的子集中，(2) 质量分类器保留分数位于前 80% 的视频，那么我们保留其中的 80%。1.5T 字 x 0.8 = 1.2T 字。1.2T 单词 x 1.3 = 1.56T 文本标记。</p><p>&nbsp;</p><p>1.5T 文本令牌不足以大幅降低 Gemini 或 GPT-5 规模模型的要求：</p><p>1T 参数（20T 文本令牌）。2T 参数（40T 文本标记）。5T 参数（100T 文本令牌）。</p><p>&nbsp;</p><p>鉴于 2023-2024 年大型语言模型对多模态的关注，可以假设视觉内容（不仅仅是文本）正在用于训练这些模型。</p><p>&nbsp;</p><p>在将YouTube上的音频、视频数据注入Gemini数据集中后，Gemini模型就具有了多模态能力，比如，根据YouTube视频训练的模型，可以帮助需要的人根据视频解决一些实际动手问题。</p><p>&nbsp;</p><p>使用YouTube内容，还可以帮助谷歌开发更先进的文本转视频软件，根据用户想看的内容描述，自动生成详细的视频。</p><p>&nbsp;</p><p>Google DeepMind在 Piper（其 86TB monorepo）中的迭代代码上训练大模型（<a href=\"https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html\">DIDACT</a>\"）。使用 The Pile 的每字节 0.4412 个令牌的计算，该数据集将约为37.9T个令牌，大约是GPT-4 中下一个最大数据集大小的两倍（预估）。这意味着训练Ge​​mini不会出现传闻中的数据匮乏的情况。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/13/130d20d6b9798aee8457a70301041bcf.png\" /></p><p></p><p>2023年最大数据集列表（截至2023年6月）</p><p>*四舍五入大概的数据以粗体显示，确定的数据以斜体显示。</p><p>&nbsp;</p><p>据称与GPT-4不同，Gemini将是首个能够同时处理视频、文本和图像的多模态模型。有报告表明，Gemini 接受的训练令牌数量是 GPT-4 的两倍，是 PaLM 2 的 10 倍。</p><p>&nbsp;</p><p></p><h3>Gemini+GPT-4等于AGI？</h3><p></p><p>&nbsp;</p><p>Google Gemini 是一种多模式工具和 API 集成，旨在将 GPT-4 等语言模型与 AlphaGo 中使用的技术相结合，以增强其能力，例如规划和解决问题。</p><p>&nbsp;</p><p>比如，目前GPT-4等大语言模型的缺陷主要体现在两方面：第一，是结果高度依赖训练语料，如果语料存在偏见或错误，那么大语言模型生成的结果也会是错误的；第二，是大语言模型可能会出现幻觉，给出完全不符合常识的错误信息，这主要是因为大语言模型只具备当前训练语料的知识，缺乏对真实世界全面而准确的理解。</p><p>&nbsp;</p><p>Gemini 作为先进的数学定理证明系统，与 GPT4 等大型语言模型相结合，有可能解决人工智能模型中搜索和规划的弱点，并生成新的定理。有专家预测，该模型可以在五年内达到 MMLU基准的 100分。</p><p>&nbsp;</p><p>谷歌在构建和训练大语言模型方面还有着深厚的人才池和多年实践经验。除了预计于明年秋季发布的新模型之外，谷歌还有意发布由Gemin驱动的新聊天机器人，或者借此升级现有Bard聊天机器人。照惯例来看，新模型应该会通过Google Cloud对外发布，这无疑会对谷歌的云业务产生深远的积极影响。</p><p>&nbsp;</p><p>Gemini在上月谷歌开发者大会上首度亮相时曾遭嘲笑，期间谷歌展示的几个AI项目也未受认可。</p><p>&nbsp;</p><p>谷歌称，Gemini项目的下一代AI模型最早将于今年秋季推出。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/902457246acfefc581a0518bfd9bb429.png\" /></p><p></p><p></p><h2>联合创始人谢尔盖·布林躬身入局，组建研发团队</h2><p></p><p>在将谷歌Brain和DeepMind两大AI部门合并时，掌门人皮查伊称是为了提高部门运作效率，将谷歌庞大的计算资源同DeepMind的研究技能结合起来。</p><p>&nbsp;</p><p>消息人士指出，谷歌大脑和 DeepMind 团队的几位前成员目前正在研究 Gemini。其中包括 Google 高级研究员 Paul Barham 和 DeepMind 的 Tom Hennigan，后者专注于 Gemini 的基础设施。然而，最引人注目的团队成员可能是谷歌联合创始人谢尔盖·布林 (Sergey Brin)。</p><p>&nbsp;</p><p>据报道，2022 年底，布林开始更频繁地进入谷歌办公室。在谷歌于 2022 年底因 OpenAI 失去研究人员后，人们认为布林正在专注于 Gemini 的招聘流程。现在，消息人士称，他在评估和训练 Gemini 模型方面发挥了重要作用。</p><p>&nbsp;</p><p>在此之前，两大部门也分别对ChatGPT做出了自己的回应。DeepMind这边有Goodall项目，使用了一种名为 Chipmunk 的未公开模型，另一部门则拿出基于Google Brain模型的Bard。尽管双方之间存在一定竞争，DeepMind还是决定放弃Goodall，转而在Gemini上携手合作。</p><p>&nbsp;</p><p></p><h2>ChatGPT的统治将就此终结？</h2><p></p><p>事实上，Google Brain和DeepMind的通力合作必然给OpenAI及其他竞争对手带来麻烦。当然，谷歌具体如何打造Gemini才是决定性因素。报道表明，Gemini在多模态能力方面取得了显著进步，切实超越了以往模型。其设计侧重于多模态，意味着它能够理解和处理多种不同形式数据，并在工具与API集成方面极为高效。</p><p>&nbsp;</p><p>具体来讲，Gemini不仅擅长理解和生成会话文本，而且精通处理多种其他输入，例如文本、图像和视频。另有报道表明，Gemini能够接收的token数量可达GPT-4的两倍，这应该能够支撑起更强的智能度优势。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/03/03fecb2854e12a089b5830332b939419.png\" /></p><p></p><p>随着生成式人工智能竞争格局的加剧，谷歌准备通过推出 Gemini AI 来展示其真正的能力。谷歌从匆忙引入Bard中汲取了宝贵的经验教训，决心确保无懈可击地进入市场。预计到 2030 年，生成式人工智能市场将达到 1093.7 亿美元，投资者和客户热情高涨，加剧了主导地位的争夺。谷歌着眼于彻底改变行业，已准备好释放 Gemini AI 的全部潜力，塑造文本分析人工智能解决方案的未来。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-ai-fall-launch-chatgpt-edge-8896455/lite/\">https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-ai-fall-launch-chatgpt-edge-8896455/lite/</a>\"</p><p><a href=\"https://www.androidpolice.com/google-ai-gemini-chatbot/\">https://www.androidpolice.com/google-ai-gemini-chatbot/</a>\"</p><p><a href=\"https://www.theinformation.com/articles/the-forced-marriage-at-the-heart-of-googles-ai-race?irclickid=XepQ8kzcBxyPURYQqf1uq0VoUkF3jszhq2PuWY0&amp;irgwc=1&amp;utm_source=affiliate&amp;utm_medium=cpa&amp;utm_campaign=10078-Skimbit%20Ltd.&amp;utm_term=androidpolice.com\">https://www.theinformation.com/articles/the-forced-marriage-at-the-heart-of-googles-ai-race?irclickid=XepQ8kzcBxyPURYQqf1uq0VoUkF3jszhq2PuWY0&amp;irgwc=1&amp;utm_source=affiliate&amp;utm_medium=cpa&amp;utm_campaign=10078-Skimbit%20Ltd.&amp;utm_term=androidpolice.com</a>\"</p><p><a href=\"https://insights.daffodilsw.com/blog/google-gemini-algorithm-the-next-level-ai-model\">https://insights.daffodilsw.com/blog/google-gemini-algorithm-the-next-level-ai-model</a>\"</p><p><a href=\"https://lifearchitect.ai/gemini/\">https://lifearchitect.ai/gemini/</a>\"</p>",
    "publish_time": "2023-08-24 16:01:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI安全成为新焦点，一文带你领略中国AI领域新变化，盘点40+细分领域技术成熟度",
    "url": "https://www.infoq.cn/article/cRwfD7F6xKwBJuji1UQS",
    "summary": "<p>2022年上半年，AI绘画等生成式AI的内容引发了人工智能领域的广泛关注，年底发布的ChatGPT更是迅速破圈，话题度和讨论度不断提升。这些变化的趋势也延续到了2023年上半年，人们讨论话题也向着大模型的实际应用逐渐深入，同时AI安全也展现了大家对技术突破的不同态度。</p><p>InfoQ研究中心此前发布的<a href=\"https://www.infoq.cn/minibook/UGhD7MTY5Z43JG5YmWP3\">《中国软件技术发展洞察和趋势预测研究报告2023》</a>\"，绘制了涵盖130+细分领域的中国技术成熟度评估曲线。作为年度趋势报告的延续，InfoQ研究中心基于2023年人工智能领域的种种变化，更新<a href=\"http://gk.link/a/128Px\">《2023&nbsp;中国人工智能领域技术成熟度模型报告》</a>\"。</p><p>同样，InfoQ研究中心也在三大核心指标（技术专利数量、技术发展时间、技术舆论指数）的基础上，参考了市场规模、融资事件等公开资料，并结合了AI行业内硬件、模型、应用不同领域的各位专家观点，绘制了包含40+技术点的中国人工智能技术成熟度模型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0affd338817e741dcaf474de3caca432.png\" /></p><p>备注：</p><p>1、技术专利相关数据来自国家知识产权局旗下专利检索及分析系统，检索时间为2023年8月15日。</p><p>2、技术发展时间使用知网论文库等学术平台进行相关技术领域论文最早收录年份统计计算。</p><p>3、技术舆论指数数据来源为各家技术媒体和开发者社区，包括InfoQ中文站和CSDN社区。</p><p></p><p>有关各发展阶段的具体特征，欢迎大家点击<a href=\"http://gk.link/a/128Px\">《2023&nbsp;中国人工智能领域技术成熟度模型报告》</a>\"进行完整报告下载。</p><p></p><p>中国人工智能技术成熟度三大核心指标气泡图</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/dea9d59bff8e1790664ef8ff12e944d2.png\" /></p><p></p><p>此外，在模型生成的过程中，我们也发现了一些有趣的现象，希望与各位读者共同探讨。</p><p></p><h3>对大模型的期待和应用现状出现短期错配</h3><p></p><p>这一方面是因为大众和用户对大模型充满了极大的热情与期待，另一方面，在经过了半年多的激烈讨论之后，大家的关注重点已经开始转向大模型的具体应用效果和由此带来的成本。简而言之，无论是C端还是B端，用户已经在期望成本可控下，体验到大模型和AIGC的卓越成效。</p><p>相比之下，现阶段众多大模型相关厂商之间的竞争，仍然属于「无」与「有」，即先推出产品再慢慢优化效果和降低落地成本。同时，现阶段的业务选择中，众多厂商聚焦于确定性高的场景，这也导致了现阶段大模型与AIGC领域的产品并没有建立各自的差异化优势。</p><p>这种短期错配，实际上带来了大模型各类厂商对于应用场景探索的焦虑与急迫性，本质上也限制了大模型潜力的充分释放。但归根到底，这是技术与业务之间的权衡。大模型各类厂商，无论是模型层还是最终的应用层，都需要贴合实际需求，充分展现产品价值，才能在激烈竞争中脱颖而出。</p><p></p><h3>AI安全的关注度迅速提升，如何安全地使用模型成为重点</h3><p></p><p>伴随着AI的应用范围逐渐拓宽，应用深度逐渐深入，AI在各行各业发挥的促进作用日益凸显。而AI越重要，对AI使用过程中的安全可信要求就越高，可信AI、可解释AI等正在成为AI落地的重要考虑因素。然而，目前数据投毒攻击、模型窃取攻击、噪音攻击等攻击方式正在严重威胁AI自身的安全，但相对应的防御手段与解决措施却未能及时更新。这意味着AI安全领域需要更多可用的工具，帮助企业建立有效的防护措施。</p><p>此外，AI大模型的出现也在一定程度上降低了网络攻击的技术门槛，给现有的网络安全工作带来了一定的冲击。</p><p></p><h3>技术难点与应用受限是限制技术发展的两大因素</h3><p></p><p>InfoQ研究中心在对舆论关键词的研究过程中发现，伴随着人工智能技术逐渐走向成熟，讨论重点也在逐渐从技术实现和风险伦理转向解决方案与产品标准。</p><p></p><p>不同发展阶段下的人工智能技术讨论重点词云变迁图</p><p><img src=\"https://static001.geekbang.org/infoq/32/32abe2144da94a441a1e21da3b6c2eaf.png\" /></p><p>除此以外，在报告中，InfoQ研究中心将限制技术走向成熟的因素总结为技术难点和应用受限。其中技术难点还包括硬件/资源限制和技术实现，应用受限包括应用场景探索、规模化经济、法律及伦理道德三点。</p><p>限制中国人工智能技术发展的两大因素</p><p><img src=\"https://static001.geekbang.org/infoq/02/0219c84ce8191c9b88395cf076f23a3d.png\" /></p><p>但这并不意味着是单一因素限制了技术的发展，在现实中通常存在多种因素相互作用。例如，汽车自动驾驶早期的探索重点一直是如何在技术上实现可行性，这时技术实现主要限制了汽车自动驾驶的发展。在后续发展过程中，乘用车还是商用车优先发展、先借助L2/L3实现盈利还是直接探索L4/L5，这些不同发展路线既包括了应用场景探索的思考，也包括了规模化经济，即落地成本的限制。但法律伦理贯穿汽车自动驾驶的整个发展过程。</p><p>同时，我们也需要理性思考这些限制性因素。当我们对于限制性因素的解决方案或者发展路线上达成了共识，这些限制性因素会反过来形成一种产品/技术标准，推动技术领域的发展。</p><p>InfoQ研究中心也期待中国人工智能技术领域能够实现更大的突破和应用，InfoQ研究中心将持续关注中国人工智能领域的各项最新动态，也欢迎各位读者与AI开发者一起探讨。</p><p>此外，在报告中我们还对40+人工智能及其细分技术领域绘制了企业图谱和名录，欢迎大家点击<a href=\"http://gk.link/a/128Px\">《2023&nbsp;中国人工智能领域技术成熟度模型报告》</a>\"下载完整报告。</p><p>中国人工智能技术厂商生态图谱</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/2302ffd63ff3292d1cc3341113694743.png\" /></p><p></p>",
    "publish_time": "2023-08-24 16:14:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "What's new in Pika v3.5.0",
    "url": "https://www.infoq.cn/article/96b9acdad60324bfbe5c6fb15",
    "summary": "<p>时隔两年，Pika 社区正式发布经由社区 50 多人参与开发并在 360 生产环境验证可用的 v3.5.0 版本，新版本在提升性能的同时，也支持了 Codis 集群部署，BlobDB KV 分离，增加 Exporter 等新特性。</p><p></p><p>我们将详细介绍该版本引入的重要新特性。</p><p></p><h2>1 去除 Rsync</h2><p></p><p>在 v3.5.0 版本之前，Pika 使用 Rsync 工具进行引擎中存量数据的同步，Pika 进程启动时创建 Rsync 子进程。这种同步方式在实际使用中出现了一些问题，包括Pika 进程 crash 后重新拉起无法正常同步以及同步过程中 Rsync 进程无故退出等。在今年发布的 v3.5.0 版本中，我们在全量同步方案方面进行了重要的改进，摒弃了以往使用的 Rsync，实现了全新的数据同步方案，支持了断点续传，动态调节传输限速等特性，以确保同步过程更加稳定、可控。这些改进不仅增强了同步的可靠性，还为用户提供了更好的使用体验。</p><p></p><p>去除 Rsync 进程，使用<a href=\"https://github.com/OpenAtomFoundation/pika/pull/1805\">自研全量同步方式</a>\"实现断点续传，<a href=\"https://github.com/OpenAtomFoundation/pika/pull/1926\">传输限速</a>\"功能Pika 主从同步时，进行 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1805\">master run_id</a>\" 的检验</p><p></p><h2>2 兼容更多 Redis 命令</h2><p></p><p>在 v3.5.0 版本中，我们迈出了更大的一步，提升了对 Redis 命令的兼容性，对 Redis 命令提供了更广泛的支持。这个版本的改进使得 Pika 在与 Redis 生态系统的集成中表现更加出色，为用户提供了更丰富的功能和更广阔的可能性。我们对命令支持的扩展，为用户提供了更多的灵活性，以满足不同场景下的需求。</p><p></p><p>支持 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1273\">UNLINK</a>\" 命令支持 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1660\">INFO COMMANDSTATS</a>\" 命令支持 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1245\">HELLO、SETNAME</a>\" 命令支持 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1548\">BLPOP、BRPOP</a>\" 命令新增 Pika 原创 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1843\">DISKRECOVERY</a>\" 命令</p><p></p><h2>3 RocksDB 版本升级和分级压缩</h2><p></p><p>在 v3.5.0 版本中，我们进行了一项重要的升级，将 RocksDB 引擎升级至 v8.1.1 版本，并实现了分级压缩功能的整合。这一升级不仅是技术的飞跃，也是我们对系统性能和优化的持续关注的体现。通过这项升级，我们为 Pika 增加了更高级别的数据管理能力，同时也让系统更好地适应不同的压缩需求，为用户的数据存储和检索提供了更大的灵活性和效率。</p><p></p><p>升级 RocksDB 版本到 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1396\">v8.1.1</a>\"实现 RocksDB <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1365\">分级压缩</a>\"新增 RocksDB <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1189\">缓存配置项</a>\" num-shard-bits 能够从配置文件中读取</p><p></p><h2>4 支持 BlobDB</h2><p></p><p>在 v3.5.0 版本中，我们引入了引人瞩目的创新--对 BlobDB 和 KV 存储层进行了分离，为我们的系统注入了新的活力。这个版本的升级使得 Pika 在数据存储方面更加灵活和高效。我们通过支持 BlobDB KV 分离，提供了更优化的数据存储结构，为用户的数据管理和查询操作带来了更深层次的优势。这一重要改进将在更多应用场景下展现出其强大的潜力。</p><p></p><p>支持 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1456\">BlobDB KV 分离</a>\"</p><p></p><h2>5 基于 Codis 的集群模式</h2><p></p><p>在 v3.5.0 版本中，我们积极引入了 Codis 集群模式，此外，我们不仅仅将 Codis 集群模式融入了系统中，还为其提供了迁移 slot 的命令支持，从而实现了更加智能化的集群管理。这一重大变革不仅扩展了 Pika 在大规模数据存储场景中的应用范围，还进一步提升了系统的可扩展性和高可用性。通过引入 Codis 集群模式，我们对用户的数据处理和管理提供了更优化的解决方案。</p><p></p><p>引入 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1279\">Codis 到 Pika</a>\"引入 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1311/files\">Codis 的 CI</a>\"支持 Codis 迁移 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1632\">slot 命令</a>\"新增是否在 reload 的 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1700\">slotmigrate 状态</a>\"</p><p></p><h2>6 可观测性</h2><p></p><p>在 v3.5.0 版本中，我们引入了一个创新性的工具--pika_exporter，以提升对 Pika 数据库的可观测性。这一工具的加入不仅是对我们对系统监测能力的持续增强的反映。而在版本的后续更新中，我们进一步充实了指标，不断丰富了 Pika 的可观测性。为用户提供了更为全面和精准的数据洞察力。</p><p></p><p>新增 Pika 可观测系统 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1388\">pika_exporter</a>\"新增网络 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1733\">I/O 流量监控指标</a>\"新增<a href=\"https://github.com/OpenAtomFoundation/pika/pull/1751\">命令统计耗时指标</a>\"新增 estimate_pending_compaction_bytes 度量来分析<a href=\"https://github.com/OpenAtomFoundation/pika/pull/1736\">碎片率指标</a>\"新增 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1560\">RocksDB 指标</a>\"</p><p></p><h2>7 容器化部署</h2><p></p><p>在 v3.5.0 版本中，我们引入了一个具有创新意义的里程碑--pika-operator mvp 版本，这一版本在技术上实现了一个重要目标：将 Pika 单实例服务迁移到 Kubernetes（K8s）平台上的快速部署。这不仅是对我们持续关注行业发展的体现，也是我们不断提升用户体验的追求。通过 pika-operator，我们为用户提供了更便捷的部署方案，将 Pika 的高性能数据库引擎与 Kubernetes 的灵活性相融合，从而为用户的应用环境带来更高效、更弹性的支持。</p><p></p><p>实现 Pika 单例服务在 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1243\">K8s</a>\" 上快速部署实现了在 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1330\">MiniKube</a>\" 环境中部署 Pika给 pika-operator 添加 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1347\">E2E 测试</a>\"</p><p></p><h2>8 跨平台编译</h2><p></p><p>在 v3.5.0 版本中，Pika 呈现出一种全面性的蓬勃发展态势，得以在不同操作系统平台上展现其优越性。此版本的突破性之处在于，Pika 实现了对 MacOS、CentOS 和 Ubuntu 这些主要平台的完整编译和使用支持。这个举措不仅仅体现了我们对多样化技术环境的关注，也是为了最大程度地拓展用户基础，为广泛的用户群体提供灵活、高效的数据库解决方案。这种跨平台兼容性的加强将 Pika 推向更广阔的技术生态。</p><p></p><p>支持 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1372\">MacOS 平台</a>\"</p><p></p><h2>9 多平台 CI、Go 集成测试、TCL 单元测试、PythonE2E 测试、CTest 单元测试</h2><p></p><p>在 v3.5.0 版本中，我们迈出了一个令人瞩目的步伐，不仅在多个主要操作系统平台上实现了支持，还在测试领域实施了全面升级。我们为 Ubuntu、CentOS 和 MacOS 这三大平台搭建了持续集成（CI）环境，以确保系统的完整性和稳定性。在测试方面，我们引入了更为广泛的覆盖，包括 Go 语言的集成测试、TCL 的单元测试以及 Python 的端到端（E2E）测试。通过这些测试策略的升级，我们在确保系统性能和可靠性方面迈出了更大的一步。</p><p></p><p>新增 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1534\">CentOS</a>\" 环境下的 CI新增 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1769\">MacOS</a>\" 环境下的 CI新增 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1347\">E2E</a>\" 测试框架新增在 Github CI Workflow 中添加 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1268\">CMake</a>\" 编译环境新增在 TCL 脚本中 populate 方法模拟 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1693\">Redis debug populate</a>\" 方法，用以填充测试数据新增在 blackwidow 中添加 CMake 文件，添加对 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1246\">blackwidow</a>\" 的单元测试移植 <a href=\"https://github.com/OpenAtomFoundation/pika/pull/1357\">Redis</a>\" 测试脚本</p><p></p><h2>10 Others</h2><p></p><p>若您有任何疑问，诚挚欢迎您扫描微信二维码，加入我们的交流群，与一众志同道合的成员展开深入的讨论，我们热切期待与您分享见解、交流心得，为共同的技术探索和创新之旅添砖加瓦。在这个群体中，我们将以卓越的智慧和互动的合作精神，构建出一个相互学习、不断进步的技术共同体。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e2/e213860a0e8e857e40b76a8ccfa5e654.png\" /></p><p></p><p></p>",
    "publish_time": "2023-08-24 17:50:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris 助力中国联通万亿日志数据分析提速 10 倍",
    "url": "https://www.infoq.cn/article/3OoAlh73FwtLhfdExLpF",
    "summary": "<p></p><blockquote>在数据安全管理体系的背后，离不开对安全日志数据的存储与分析。以终端设备为例，中国联通每天会产生百亿级别的日志数据，对于保障网络安全、提高系统稳定性和可靠性具有至关重要的作用。目前，<a href=\"http://doris.apache.org/\">Apache Doris</a>\" 在联通体系的落地已支持了 30 多条业务线和数百个实时作业，不仅帮助联通实现了万亿级安全日志的高效分析和低成本，也为其他运营商提供了成功的参考案例和学习经验，对推动运营商的数字化转型进程具有重要意义。</blockquote><p></p><p></p><p>作者：刘宇麒 ，大数据开发工程师</p><p></p><p>联通西部创新研究院是中国联通在西部地区布局的重要载体，也是中国联通数字化创新能力体系的重要组成部分，承载了集团公司科技创新体系和数字化创新体系的需求。依托联通数科的优质资源及能力底座，在云计算、大数据、物联网、人工智能、网络安全等业务领域具备深厚的技术能力和丰富的项目经验。</p><p></p><p>近些年来，网络高危漏洞数量的增长、DDoS 攻击比例的提升、恶意 Bot 流量的持续上升使得 Web 安全威胁态势愈发严峻，而数字化转型进程的推进在丰富业务创新的同时、也提升了网络空间复杂度、进一步加剧了网络安全风险。这样的背景之下，联通以攻防实战对抗为目标、进行国家级网络空间的安全治理工作，围绕“云-管-端-数”构建 了多级综合防控体系，聚焦于实时监测、攻击溯源、通报预警、应急处置、情报共享等工作，构建数据全生命周期安全管理体系，为客户提供从顶层设计到运营维护一站式服务。</p><p></p><p>在数据安全管理体系的背后，离不开对安全日志数据的存储与分析。以终端设备为例，每天会产生海量的设备日志，这些日志数据记录着各种网络时间和系统操作的细节信息，对于保障网络安全、提高系统稳定性和可靠性具有至关重要的作用。为了更好的管理和分析安全日志数据，联通西部创新研究院应集团要求构建一个集中化日志数据分析平台，满足对事件和日志数据自动化采集、存储、管理、分析和可视化的诉求。这要求集中化数据分析平台具备以下能力：</p><p></p><p>建模分析：基于网络日志数据和告警数据进行规则或智能挖掘，发现潜在的安全事件，例如钓鱼邮件、非法访问等，并进行定向威胁感知。态势大屏：通过多种维度不同监控指标的组合，例如安全事件 TOP5 等，密切监控当前网络安全态势状况，通过态势大屏呈现攻击威胁的主要分布。追踪溯源：通过对安全事件的快速研判，还原整个攻击链条进行精准的溯源取证，从而保障网络和数据安全。</p><p></p><p>为搭建具备上述能力的集中化日志数据分析平台，在正式搭建之前，结合日志数据的特性及业务要求，我们需要综合考虑考虑如何满足以下要求，以确保平台能高效的支持联通日志场景的实际应用：</p><p></p><p>数据接入方面：日志数据具有种类繁多、格式多样化、规模庞大等特点，要求数据平台支持多种日志格式数据的导入，并支持高性能的数据写入。实时性要求方面：为及时监控和了解系统运营情况和存在的问题，高实时性对于数据平台非常关键。这要求平台要实时进行数据同步，保障数据的一致性，并支持数据实时查询，以便获取最新的系统和业务状态。可扩展要求方面：数据平台需要具备计算与存储的拓展能力，以便满足集团及分公司不断增长的数据处理分析需求。</p><p></p><p>在中国联通安全日志数据分析平台的迭代过程中，经历了从基于 Hive 的离线数据仓库到以 Apache Doris 为核心的实时数据仓库。从具体业务收益来讲，Apahce Doris 的引入支持了联通 30+ 条业务线和数百个实时作业，为联通带来了存储资源节约 50%、百亿级别数据查询秒级响应、数据导入效率提升 60% 的显著成果，成功实现了降本增效的业务目标；从集团整体价值来讲，通过该平台，联通可以更好地监控运营状态、保障网络安全，为运营商安全管理体系提供了重要的底层支持。总而言之， Apache Doris 在联通体系的落地，不仅帮助联通实现了万亿级安全日志的高效分析和低成本，也为其他运营商提供了成功的参考案例和学习经验，对运营商数字化转型进程的推进有着重要作用。</p><p></p><h1>基于 Hive 的离线数据仓库</h1><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/128e4d96bcdd9d3c04946907b7cac719.png\" /></p><p></p><p>在项目一期建设中，我们以 Apache Hive 为核心建立了离线数仓，并在其此础上进行了数据仓库分层。当原始数据经过数据采集进入离线数仓后，由 Spark 逐层进行处理，并配合 Apache DolphinScheduler 以分钟级调度执行计算作业，最终将数据输出至 OLAP 和应用数据库。</p><p></p><p>从业务的角度来看，该架构数据流的痛点问题在于数据实时性不足，主要受限于 Hive 的离线批处理模式，端到端的延迟最短竟然需要 10 分钟。</p><p></p><p>其次，我们在该架构中选择了 ClickHouse 作为 OLAP 引擎，但在实际使用场景中发现 ClickHouse 存在以下不足：</p><p></p><p>ClickHouse 并发支持能力不足，无法满足业务需求，例如实时大屏指标的计算与加载缓慢，经常会在业务高峰期出现查询超时。业务中有大量安全事件表需要进行多表 Join，这些表数据量较大，而 Clickhouse 在分布式 Join 实现性能较低，时常会出现 OOM 问题，为避免该情况发生，常常需要依赖宽表才能缓解，而这既影响了业务的稳定性，也增加了许多额外的维护成本。由于 ClickHouse 对于数据更新操作支持较弱、更新性能较差，这也限制了它在某些场景下的应用。ClickHouse 使用和运维成本较高，也给我们带来了更高的人工投入成本。</p><p></p><h1>系统选型及落地</h1><p></p><p>随着一期架构问题的逐步暴露，我们迫切需要对数据分析平台进行更新迭代。对于二期建设来说，提升数据的实时性被确立为首要目标，为了实现这一目标，我们计划增加实时数据处理链路，以更好地实现数据的实时收集、处理和查询要求，为系统稳定和网络安全提供更有力的支持和保障。其次，为解决一期平台存在的并发能力不足、多表 Join 性能低等核心问题，提升 OLAP 引擎性能成为二期建设的的另一关键目标，因此亟需对一期平台中 OLAP 引擎 ClickHouse 进行替换，以满足业务侧日益严格的数据分析和处理需求。</p><p></p><p>在此背景下，我们考虑是否可以只选择一个新的实时数据仓库同时满足以上两个目标，一方面即能帮助我们构建实时数据分析处理链路，另一方面又可以作为性能更强悍、更易用的 OLAP 分析引擎，这样不仅可以简化数据处理流程、提高实时效率，而且可以降低平台运维管理的成本。</p><p></p><p>为了找到符合条件的数据库，我们进行了多方调研和对比研究，最终选择以 Apache Doris 为核心来构建统一的实时数据仓库体系。为了直观展示 Apache Doris 的性能和功能特点，我们使用 Apache Doris 与 ClickHouse 进行了对比，其中最直观的感受是 Apache Doris 在系统并发、Join 性能以及多个功能的易用性都更为领先。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dcb59575d5f2e0574ad01459b5dd6481.png\" /></p><p></p><h1>Doris 替换 Hive + ClickHouse 建设实时数据仓库</h1><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24c211dbd8facd5a5e65d9449ee60a2d.png\" /></p><p></p><p>在项目二期的建设中，我们使用 Apache Doris 替换了 Hive 成功搭建实时数据仓库，实现数据的实时采集、处理和分析，同时使用 Apache Doris 替换 ClickHouse 作为 OLAP 引擎。架构工作机制如下所示：</p><p></p><p>ODS 贴源层：主要用于存放未经处理的原始数据，通过 Flume 等实时采集工具，将各个厂商未经处理的原始日志以及告警数据统一汇集到 Kafka 中，同时完全相同的数据也会被存入 HDFS 中一份，作为原始数据核查依据或进行数据回放。DWD 明细层：该层为事实表，数据通过 Flink 计算引擎实时对生产数据及字段进行清洗、标准化、回填、脱敏之后写入 Kafka 。Kafka 中的数据还会对接到 Doris 中，以支持明细日志数据详情回溯查询、准实时模型分析、实时大屏及报表业务。由于大部分日志数据对于数据重复不是很敏感，因此 DWD 层采用 Doris 的 Duplicate Key 模型。DWS 汇总层：以明细层 Kafka 数据为基础，通过动态规则引擎进行细粒度的聚合分析，为后续的业务查询和 OLAP 分析做准备，同时大部分建模分析的结果也集中在 DWS 层。ADS 应用层：该层主要使用 Doris 的 Aggregate Key 模型和 Unique Key 模型对以上三层的数据进行自动聚合或者自动更新，以满足前端人员的具体分析需求。</p><p></p><h1>新架构的应用实践</h1><p></p><p></p><h2>日增百亿数据，稳定快速导入</h2><p></p><p>数据分析平台平均每天有 150 亿的业务日志数据新增，面对如此大规模的数据量，我们需要考虑如何将数据快速实时稳定入库。经调研，Doris Flink Connector 组件（主要依赖 Doris Stream Load ）可以实现海量数据快速导入。 并且其使用非常简单，只需要导入相关依赖包进行简单的配置即可进行。在应用 Doris Flink Connector 后，数据写入性能可达到每秒 20-30 万条，极大地提升了数据导入的速度和效率，同时也不会对正常的数据分析造成干扰。</p><p></p><p>在采用 Flink 进行高频实时写入 Doris 时，如果未合理调整参数配置，可能导致数据版本堆积。为避免该问题，我们进行了以下调整优化：</p><p></p><p>Flink 优化：为减轻 Doris 的写入压力，可通过提高 Flink 的 Checkpoint 时间来减少版本数量。具体来说，我们可以将 Checkpoint 时间从之前的 15 秒提高为 60 秒，以减少批次写入频率，降低 Doris 单位时间处理事务数量。这样可以在不影响业务的情况下，缓解写入压力，避免产生大量的数据版本。数据预处理：为了减轻 Doris 的写入压力，部分数据我们会先在 Flink 中通过主键 ID 进行预聚合，将来自多个表中相同的 ID 进行处理并构建大宽表，降低多流数据的写入资源消耗。Doris 优化：调整 Doris BE 参数，增加 CPU 资源参与 Compaction 操作；根据业务设置合理的表分区、分桶和副本数量，避免过多分分片，以降低 Compaction 的开销。同时增大 max_tablet_version_num，避免版本堆积。</p><p></p><p>通过以上优化措施，每日新增的百亿数据可以平稳导入 Doris 中，整个导入过程中 BE 表现稳定，Compaction Score 始终保持低位，大批量数据的写入对于前端查询的性能也没有造成任何影响。同时在 Doris 的 Unique Key 模型的加持下，我们可以利用 Flink 对输入数据进行关联、聚合等处理，再以微批、精准一次性写入 Doris 中，实现了数据秒级更新。</p><p></p><h2>存储资源合理配置，成本节约 50%</h2><p></p><p>日志数据具有非常大的数据量和数据增长速度，如果不对存储资源进行合理分配和控制，存储成本将会成为一个巨大的负担。日志数据中也会存在重要性的区分，有一定比例的数据价值密度比较低，如果毫无差别的将这些数据都存储下来，不仅会造成存储浪费，也会增加数据分析的难度。为了有效解决这些问题，我们采用了一系列策略来降低数据存储成本：</p><p></p><p>ZSTD 高效压缩算法：利用 Doris 的新特性——ZSTD 高效压缩算法进行压缩存储。在建表时指定压缩方法为 ZSTD，特别是对数据量超过 T 级别的数据表，这种压缩方法可以有效地减少数据占用的存储空间，数据压缩比最高可达 1:10。即使采用 3 副本来保证数据的高可靠，数据存储占用的空间仍有非常大幅度的降低。冷热数据精细化管理：在 Doris 中只存储近一年的数据，将更早的数据备份到成本更低的存储介质中。同时使用热数据转冷的功能，在 SSD 中仅存储最近 7 天的数据，将 7 天之前的数据转存到 HDD 中，以进一步降低存储成本。这样可以根据数据的使用频率，合理分配存储资源，达到性能和成本的平衡。 目前 <a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247521087&amp;idx=1&amp;sn=b8b36da7bc4a0fe4145d691816ce672d&amp;chksm=cf2f9d38f858142eef8392477cdcc89df8a17989a2a89f47791e2c2a9199af8c2304979062b9&amp;scene=21#wechat_redirect\">Apache Doris 2.0 版本已经实现了对冷热数据分层功能的支持</a>\" ，这一功能可以将冷数据下沉到存储成本更加低廉的对象存储中，冷数据在对象存储上的保存方式也从多副本变为单副本，存储成本进一步降至原先的三分之一，同时也减少了因存储附加的计算资源成本和网络开销成本，目前我们正在积极测试中，未来有机会也会与大家分享实践经验。分区级副本设置：将 3 个月以内的数据设置为高频使用数据，将其分区设置为 3 副本；将 3-6 个月的数据分区设置为 2 副本；将 6 个月之前的数据分区设置为 1 副本。这样可以根据数据的使用情况，合理分配副本数量，实现存储成本降低的同时也充分利用多副本来提升热数据的查询性能。</p><p></p><p>借助于 Doris 极高效率的压缩算法、冷热数据分层管理、分区级副本设置等功能，可对存储资源合理分配，最终实现存储成本节约 50%，成功达到性能和成本的平衡。</p><p></p><h2>数据规模分级查询，查询速度提升 10+ 倍</h2><p></p><p>日志中包含了许多对分析及时性要求非常高的数据，例如异常事件、故障信息等，因此为了保障日志数据的查询效率，我们以数据量的级别为基准采用了不同的查询策略：</p><p></p><p>对于 100G 以下的数据，可以采用分区表的形式进行查询。在业务初期业务表按照天进行分区，每天执行任务需要手动管理分区为我们带来了非常大的维护成本。后来我们利用 Doris 的动态分区功能，针对数据量较大的表可以使用小时作为分区字段，为了避免分区内数据倾斜，以雪花 ID 作为分桶字段，保证数据的均衡。此外为了避免数据积压，我们还开启了动态分区的起始偏移，保留近 20 天的数据来支撑业务分析。这样可以有效地降低数据积压的风险，同时也能够满足业务的分析需求。对于 100G 到 1T 的数据，我们采用物化视图进行查询，物化视图是一种预先计算并存储结果集的方式，可以减少查询所需的计算时间和资源消耗，从而提高查询效率。Doris 系统提供了完整的物化视图 DDL 语法，可用于创建、查看和删除等操作，这些语法与 PostgreSQL 和 Oracle 语法一致，使用简单、不需重新学习。对于上百 T 的数据，我们通过 Aggregate 聚合模型表进行查询，使用 Aggregate 模型在数据写入前进行预聚合，通过以上方式，我们成功将 20 亿条数据的查询时间进一步缩短至 1-2s，有效提高了数据查询的效率。</p><p></p><p>在一期数据分析平台中，大部分业务场景都是通过 T+1 的方式进行计算。而在基于 Doris 的二期数据分析平台中，我们实现了对大部分业务准实时（分钟以及小时级）和实时计算场景的支持。同时结合以上优化措施，极大降低了各种维度指标的统计时间，以往需要分钟级别的明细查询，现在可以在毫秒级别迅速响应，极大地改善了用户体验；另外，在 Doris 中，我们能够快速对百亿级别的大表进行不同维度的数据分析，只需要几秒即可获得查询结果，大大提高了联通各业务部门数据分析的能力。</p><p></p><h1>收益总结</h1><p></p><p>自引入 Apache Doris 以来，我们已经部署了多个集群、数十台机器，支持了中国联通 30 多条业务线和数百个实时作业，日增日志数据百亿级别，单个集群的数据规模达到数 PB 。Apache Doris 的成功应用为联通带来了多方面收益，主要包括如下方面：</p><p></p><p>在数据导入方面， 对于联通而言，每天都面临着庞大的日志增量，并且这些数据的实时性和准确性对于业务发展和决策至关重要，而 Doris Flink Connector 帮助我们实现了数据快速且稳定导入，可轻松应对日增百亿数据的导入要求，为后续的数据处理和分析提供了更高效的解决方案。</p><p></p><p>在存储资源分配方面， 由于数据量庞大、存储周期长等原因，日志数据的存储成本一直是运营商面临的难题，通过采用 Doris 高效的压缩算法、冷热数据精细管理、分区级副本设置等功能，帮助我们降低了数据存储成本，数据存储利用效率和价值得到显著提升。</p><p></p><p>在查询性能方面， 快速获取日志数据查询结果可以帮助运营商及时掌控网络及系统情况，及时发现并解决问题，也有利于及时了解用户需求和行为，优化营销策略和服务方案。Doris 在查询性能方面提供了强大的支持， 能够处理百亿级别大表按小时/天级别的明细查询，并支持不同维度聚合查询分析。业务线整体响应时间可在秒级或毫秒级别完成，甚至可以在 1-2s 内完成对 20 亿条数据的查询，查询速度较之前提升了 10+ 倍。</p><p></p><h1>未来规划</h1><p></p><p>在最新发布的 Apache Doris 2.0 版本中，Apache Doris 提供了大量新的功能，比如<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247519079&amp;idx=1&amp;sn=a232a72695ff93eea0ffe79635936dcb&amp;chksm=cf2f8560f8580c768bbde99ef8ca97d3a42ecc03b5d8d106b85f5474c90b6068781a79b3611e&amp;token=1275564630&amp;lang=zh_CN#rd\">倒排索引功能</a>\"和<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247521087&amp;idx=1&amp;sn=b8b36da7bc4a0fe4145d691816ce672d&amp;chksm=cf2f9d38f858142eef8392477cdcc89df8a17989a2a89f47791e2c2a9199af8c2304979062b9&amp;scene=21#wechat_redirect\">冷热数据分层</a>\"等，对于日志分析场景来说都是具有重要意义的更新。目前我们是以数据存储周期为基准进行副本分配，并按照数据热度分别存储在 SSD 和 HDD 中，后续我们将使用冷热数据分层新功能，将数据从 从 SSD 或者 HDD 下沉到对象存储中，从而降低数据存储成本，进一步达到服务器磁盘资源节省的目的。此外，我们正在对倒排索引功能进行测试，并计划先在小范围业务场景推广使用，倒排索引对于字符串类型的全文检索和普通数值、日期等类型的等值、范围检索具有更高效的支持，希望通过倒排索可以帮助我们进一步提高日志数据查询的效率和准确度。</p><p></p><p>除此之外，基于联通的使用场景，我们对自动分桶功能提出一些建议。目前自动分桶计算逻辑是根据最近的分区数据量来动态决定当前分区的分桶数目，这种方式适用于分区数据量呈线性关系的业务表。然而，由于我们的业务表在白天的数据量较多，夜晚数据量较少，因此使用自动分桶会导致白天部分分区具有较少的分桶，而夜晚分区则具有较多的分桶。因此，未来我们期望社区可以增加一种新的分桶规则，以前一天的数据分区存储情况为参照，来对当天的分区进行自动分桶，这样可以更加准确的根据业务表特点进行自动分桶。当然我们也将对该功能的优化进行探索，及时与社区交流，将最新的优化代码贡献到社区，共同推动社区的发展进步。</p><p></p>",
    "publish_time": "2023-08-24 18:20:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全行业视频化时代的技术革新：开源后的 BMF，将为视频处理领域带来哪些影响？",
    "url": "https://www.infoq.cn/article/qC55OH6f6852hFjlZ3o8",
    "summary": "<p>当前，全行业都在向视频化时代进行转变，视频应用在更多行业中将成为标配，相关数据显示，在 2022 年的报告中，Top 100 的 APP 中视频占比达到 69%。IDC 数据预测，到 2025 年整个视频投向数据总量将达到 72.7%，这个趋势可能带来两个较大挑战。首先，视频体量大、增长快对计算能力是一个巨大挑战。其次，用户需要更高清、更交互和更沉浸的体验，这些需求推动着视频技术的持续迭代和前进，比如近年来，沉浸式媒体方向持续加温，大模型技术正在突飞猛进，让我们看到 AI 技术能为视频应用赋能更大的空间。实现这些体验升级，在算法、交互技术、工程链路和架构方面都有很大的迭代和演进空间。</p><p></p><p>为此，火山引擎在 8 月 22 日特别举办了“<a href=\"https://www.infoq.cn/news/D7HJP6e66emRsFPtp1Cv\">火山引擎视频云 &amp;AIGC 技术大会</a>\"”（下文称“本次大会”），在本次大会上，多位技术大咖讨论了视频化时代的四大趋势和三大挑战，并首次曝光了抖音集团自研的异构算力平台和技术突破、发布了多行业的场景方案和最佳实践。其中最吸引人关注的是字节跳动视频架构负责人、火山引擎视频云架构技术总监王悦和 NVIDIA 开发与技术部门亚太区总经理李曦鹏一起进行了《普惠开源，加速视化新进程》的主题演讲，火山引擎与 NVIDIA 联合官宣开源亿级视频处理框架“火山引擎多媒体处理框架（Babit Multimedia Framework，简称 BMF）”，以全面推动全行业视频应用融合。会后，InfoQ 对王悦、NVIDIA 高级计算专家王晓伟、方杰分别进行了采访，一起深度探讨了下 BMF 的成长之路。</p><p></p><p></p><h2>一、“复杂度”和“复用性”是视频处理领域的两座大山</h2><p></p><p></p><p>在当今这个全行业向视频化时代转变的大背景下，视频处理已经成为了一个重要的亟待创新的领域。随着泛互、5G 的快速发展，越来越多的数据以视频的形式被记录和传播。这使得视频处理技术的研究和应用变得尤为重要。在这个过程中，我们需要处理的数据量巨大，单纯的多核 CPU 计算无法满足性能和成本的要求。因此，结合 GPU、DSP 等协处理器进行硬件加速，以提高处理效率并降低成本，成为了视频处理领域的关键技术之一。</p><p></p><p>对于一些算法开发者来说，他们可能会选择 Python 等高级语言进行开发，因为这些语言具有强大的表达能力和丰富的库支持。然而，在一些多媒体处理的方案中，可能仅提供 C/C++ 的接口。此外，在一些场景中，为了更好地实现能力复用，开发者可能会发现已有成熟的能力所使用的开发语言和当前正在开发的工程并不是同一种。这就给开发者带来了一定的困扰，如何在不同的平台和环境下编写高效、灵活的代码，成为了视频处理领域的关键挑战。</p><p></p><p>在这个“关键挑战”中，复杂度和复用性是两个重要的方面。复杂度指的是算法本身的难度，以及在实际应用中需要处理的各种问题。这些问题包括串并联衔接、预加载、运行时动态调整 pipeline、音视频强相关的复杂场合等。这些问题的解决需要对视频处理技术有深入的理解和丰富的实践经验。</p><p></p><p>复用性则是指在不同的场景和平台上，能够快速地实现能力的迁移和重用。这需要我们在设计算法时，充分考虑到不同平台的特点和限制，尽量采用通用的模块和接口，以便于在不同的环境中进行移植和扩展。同时，开发者还需要关注现有的开源资源和技术成果，充分利用这些资源来提高我们的开发效率和代码质量。</p><p></p><p>于是 BMF 应时而生，BMF 是一套通用的多媒体处理框架，通过提供简洁易用的跨语言接口、灵活的调度和扩展性，以模块化的方式动态扩展、管理和复用视频处理的原子能力。它以 graph/pipeline 的方式构建高性能的多媒体处理链路，同时还可以直接调用单个处理能力实现工程集成，帮助多媒体用户便捷、高效地将项目落地于生产环境。目前主要应用于视频转码、视频抽帧、视频增强、视频分析、视频插帧、视频编辑、视频会议、VR 等领域。</p><p></p><p>Babit Multimedia Framework（BMF），“一沙一世界，八比特（8bit→Babit）一字节（ByteDance）”，从 BMF 研发团队对该框架的命名就可以看出，团队希望从细微之处追求极致，从细微之处出发构筑一个坚实的多媒体处理框架。面对行业挑战，最新版本的 BMF 主要聚焦在“复杂度”及“复用性”层面进行了探索。</p><p></p><p>从“降低多媒体处理开发的复杂度”层面，“复杂度”主要来源于四个层面，一是来自于深度学习算法的广泛应用和快速迭代；二是包括 GPU、板卡和硬件转换平台在内的异构平台的广泛使用；三是来自于多个算法团队之间的协同开发和运维；四是分布式应用的需求，例如过去视频处理通常用单机完成，但现在越来越多的处理需要分布式完成，并需要多种硬件配合。这些都会导致整个视频处理变得越来越复杂，并且对并行处理性能和开发迭代效率的要求越来越高。</p><p></p><p>所以 BMF 主要从三个方面去降低复杂度的。首先是 Pipeline 更加灵活，调度引擎解决了复杂的 Pipeline 和并行问题。其次，BMF 具备插件化的多语言模块机制，解决了多团队协同开发和原子能力复用的问题；第三，BMF 底层对异构平台的性能优化和多框架数据转换的处理方案也解决了异构平台和框架兼容性相关的问题。</p><p></p><p>譬如，在抖音业务上，视频增强转码需要通过图像去噪、插帧、超分和质量分析等技术来满足业务需求。然而，不同团队的算法工程开发人员需要以相对宽松的耦合状态来协同实现。如果基于像 FPGA 这样的方案，从算法、模块、开发到服务实现部署，需要非常高的协作成本。以插帧模块集成到上线为例，使用 FPGA 需要两周时间，而使用 BMF 框架，开发者无需考虑过多的开发语言，视频编辑码标准和集成问题，只要专注于算法和擅长的语言本身就可以了，通常使用 BMF 只需要一周时间就可以完成上线。</p><p></p><p>同时，大家应该都已经观察到，在不同场景中，通用计算方案已逐渐存在较大差异。面对复杂的数据处理，单纯依靠通用 CPU 计算模式已经无法满足性能和成本要求，通用计算方案越来越不从心，复杂度大幅提升，会朝着异构和混构的方向发展。因此，需要结合 GPU、ASIC、ARM、FPGA 等不同算力平台进行支撑，在不同视频应用场景中实现性能和成本的最佳平衡。所以 BMF 也在“异构”层面做出了自己的创新。以视频抽帧利用 NVIDIA GPU 加速为例，业务应用层使用几乎相同的一套代码，通过 BMF 提供的视频处理、filter、GPU 场景变换检测、硬件 Jpeg 编码等构成的全链路，快速完成了从纯 CPU 视频抽帧到 GPU 全链路的加速方案，业务团队的重构升级开发成本节省了二分之一。</p><p></p><p>从复用性层面，“高复用”是指在一个系统中，组件、模块或功能可以在不同的环境和场景中重复使用，从而提高开发效率和降低维护成本。目前 BMF 已实现多语言复用和不同模式的复用，并可在兼容行业标准的同时更容易扩展生态并高度复用：</p><p>多语言的复用：BMF 在一个以 C++ 实现的算法原子能力中，可以通过 BMF API 被 Python/Go/C++ 等不同开发语言使用。据此，不同项目可以根据自己的需求来复用这些原子能力，而无需为每种语言编写特定的实现代码。这种多语言复用有助于提高开发效率，减少代码冗余，同时也有利于团队协作和知识共享。不同模式的复用：BMF 每个原子能力既可以被构建到 BMF pipeline/graph 去使用，也可以被当作一个标准接口单独调用集成使用。这种多样化的使用方式使得框架具有更高的可复用性。开发者可以根据实际需求选择合适的模式来使用原子能力，既可以在复杂的数据处理流程中发挥作用，也可以作为独立的模块进行调用，这种灵活性有助于提高框架的适应性和扩展性。</p><p></p><p></p><h2>二、成熟视频处理框架之上，需要的是“创新”</h2><p></p><p></p><p>其实，在 BMF 出现之前，<a href=\"https://www.infoq.cn/news/WPmK0BeY0dDJB6wLL0zM\">业界</a>\"早已经有了很多成熟的多媒体框架，比如通用型 FFmpeg、Gstreamer 框架，垂直型 OpenCV 框架、MediaPipe 框架、pyav、ffmpeg-go 框架等，但放眼过去，目前它们在业务迭代、效率和扩展方面都遇到了各种问题，主要瓶颈可能在于应对复杂场景、拓展与协作、标准与兼容，尤其是对于近些年的异构计算支持逐渐有所不足，领域框架亟待创新。</p><p></p><p>于是，火山引擎基于当前行业内的主流框架进行学习，取长补短，将它们的优点综合起来，并结合自身业务驱动，打造一套精确适用于业务场景的框架。“为了更好地复用公司内部已开发的原子能力，协同算法与工程、调度异构资源，我们在逐步规划设计和迭代打磨中形成了 BMF 框架。”王悦如是说。</p><p></p><p>BMF 框架主要可以分为 4 层——业务层、架构层、模块层和硬件抽象层，在每一层面，BMF 研发团队都做出了创新。</p><p></p><p>在业务层，抖音和火山引擎满足业务多样化需求。BMF 支持全视频场景，比如体量较大且标准严格的通用转码场景，BMF 可适应视频编辑、抽帧等业务场景的不同需求；又比如大量视频 AI 应用场景，BMF 从算法 AI、工程化快速落地，实现视频增强、视频质量分析、ROI Mapping 等。此外，视频会议和云导播等实时场景也可以通过 BMF 的实时调度动态引擎支持。</p><p></p><p>在应用层面，BMF 可以将多语言，如 C++、Python 和 Go 开发模块以无缝高效的模式串联起来协同工作，用户可以根据不同的项目需求进行开发集成。为了达到这个效果，在架构中间层，BMF 自研了一个自驱跳动和动态 Graph 引擎，框架可以完成对 graph/pipeline 的调度，并配备适合不同场景的多种执行方式，在这些方面，BMF 也获得了很多专利：</p><p>Engine 层“自驱动”调度（已申专利）——每个结点“自驱”的决定自身是否需要被调度，由并行调度器实时的响应和去冗余调度。SDK 层的多模式与 Engine 共同发挥作用，支持多种使用模式，如默认的 Graph Mode，直接调用原子能力的 Sync Mode，节省整体初始化时长的预加载 Preload Mode。同时支持支持 Graph 向外输出的 Generate Mode、由外部向 Graph 输入的 Push Mode、以 Graph 常驻服务图片场景的 Server Mode 等。</p><p></p><p>同时，BMF 可以完成跨数据类型跨设备的数据流转 Backend，BMF 框架的底层自定义通用视频数据表示层，能够将几乎所有进入框架的视频数据以统一的 BMF 数据层表示，这种统一的数据表示方式不仅可以简化数据处理流程，还具有与主流数据类型的零拷贝转换能力，进一步提高了数据传输和处理的效率。另外，BMF 框架还通过统一的数据层实现了设备间的数据拷贝加速和数据表征。当用户需要进行跨数据类型和跨设备的数据处理时，BMF 提供了简洁的 Backend 接口，将底层的复杂处理过程隐藏起来，用户只需调用一两个接口即可轻松实现跨数据类型、跨设备的数据流转。这种统一的数据表示和转换能力，以及简洁的 Backend 接口，使得 BMF 框架在跨平台、跨设备的视频处理应用中具有显著优势，用户可以轻松地实现不同设备之间的数据共享和协同处理，提高了视频处理效率和灵活性。</p><p></p><p>除此之外，在架构层，BMF 还提供常用的跨设备 reformat、color space conversion、tensor 算子等 SDK。这些 SDK 调用主要是用于创建 Media Description，这个“dp”就是“期望的转换”，在实际的视频开发中，这方面的需求是高频出现的。但由于偏细节和复杂，所以总会出现编写困惑，于是 BMF 提供了这样的 SDK，帮助用户在开发过程中统一、简洁地处理这些复杂细碎的问题。</p><p></p><p>在模块层面上，生态建设对于一个模块的框架非常重要。BMF 的设计初期也采用了模块与框架之间总耦合的设计。在内部开发者的启动下，目前已经积累了 140 多个适用于视频场景的原子能力模块。这些模块通常在不同业务实现中被复用，其中一部分模块也进行了开源。</p><p></p><p>在硬件抽象层，我们知道<a href=\"https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t\">视频场景</a>\"开发涉及到多个主流框架的协同助力，由于数据量巨大和适合并行计算特点，往往需要通过异构加速优化性能，提升用户体验。BMF 对多种主流框架的数据实现了互转切换，比如 Numpy、Pytorch 等。同时，BMF 还丰富了可支持的原子能力，使得多种主流框架数据互转和工作变得顺畅和便捷，极大降低了多媒体处理和开发的复杂度。</p><p></p><p>值得关注的是，BMF 与 NVIDIA 达成了深度合作，针对 GPU 相关框架层处理能力进行深度优化，NVIDIA 增强了 BMF 的 GPU 亲和力，在框架层面提供更多的 GPU 加速能力。同时，NVIDIA 还为 BMF 提供了许多精细优化的 BMF 模块，这些模块可以做到开箱即用，并且作为示例向开发者展示如何高效地开发 BMF GPU &nbsp;模块。NVIDIA 不仅将多项技术赋能到 BMF 中，还将 CV-CUDA、CODEC SDK 、TensorRT 以及 Maxine 集成到 BMF 中，使得 BMF 在多场景下具有全方位 GPU 加速能力，通过这些能力，用户可以极大缩短研发上线周期。</p><p></p><p>要知道，一直以来，NVIDIA GPU 具有独立的视频编解码硬件，在不占用 GPU 计算资源的前提下可提供低延迟高吞吐的视频转码能力，其高带宽高算力的特性非常适合图像处理。NVIDIA 打造了丰富的软件生态，使 GPU 成为视频处理的多项全能平台，目前在 AI 大模型领域也完成了许多框架的构建并持续在探索中，走在了视频图像研究领域的前沿。可以说，此次 NVIDIA 与 BMF 的合作，就是强强联合。</p><p></p><p></p><h2>三、“开源”是推动视频处理技术创新的必由之路</h2><p></p><p></p><p>观察当前领域内已经较为成熟的多媒体框架，我们会发现，他们都有一个共通点——已经开源。开源意味着代码公开可访问，这有助于吸引更多开发者参与框架的开发和维护。通过社区的力量，代码可以得到更快速的发展和改进，提高框架的质量和性能。其次，开源有助于促进框架的普及和推广，公开的代码使得更多人可以了解、学习和使用框架，扩大其影响力，同时公开的代码还可以吸引更多开发者参与竞争，推动框架不断发展和进步。因此，用户需求和开发需求高速迭代的情况下，只有选择开源道路，让所有开发者都参与进来，框架才能够快速发展。</p><p></p><p>所以说，在本次大会上，BMF 官宣开源对于领域来说是一件大事，也是一件小事。“大”在经过多番打磨、在字节跳动内部经过多次实践的 BMF 终于对外开源，更多的领域开发者受益；“小”在视频处理领域的框架开源是一个必做项。“一沙一世界”的 BMF，真正做到了既能仰望天空，也能脚踏实地。</p><p></p><p>据王悦表示，“BMF 在研发之初，就将未来开源视为一个明确的路径。火山引擎看到了传统多媒体框架的短板和瓶颈，需要一个更好的框架来适应多媒体发展，共同建设视频开发生态。实际上，开源的时间点比我们最初的规划晚一些，主要原因在于我们投入大量时间在内部完善框架和功能，优化性能和稳定性。当然，现在框架在内部经过了长时间的打磨，已经大规模应用在业务中，我们认为这是一个相对较好的时机来开源。”</p><p></p><p>值得一提的是，BMF 是国内首例开源互联网公司自研的多媒体处理框架，开源后的 BMF，主要目标是降低开发门槛，加速行业视频化进程。BMF 于 2017 年启动开发，至今已经经历了 4 年的迭代和线上验证，对于火山引擎来说，这是第一次将多年积累在项目系统梳理，并将认为最有价值的能力或方面展示给行业进行广泛探讨互动，作为已经在亿级 DAU 的短视频 APP 试炼过的框架，BMF 经受住大规模线上生产环境考验，BMF 代码的较高安全性和稳定性显而易见，这说明该框架开源后，将对音视频领域及视频处理领域产生极高的价值。</p><p></p><p>“BMF 开源后，火山引擎希望框架能接受全世界多媒体开发者的审阅和监督，并吸引开发者将其应用到自己的生产环境中，积极贡献代码，进一步提升框架的安全性和稳定性。”王悦在接受采访时非常郑重地说道。</p><p></p><p>据悉，BMF 的原代码是在 GitHub 上进行开源的，后续开源贡献者的每个提交，无论是在代码、文档、设计还是其他形式都可以公开展示。在开源之后，除了持续提升和部署中化测试以及安全扫描等技术手段来保证代码质量，火山引擎还希望与全球开发者一起围绕 BMF 框架建立开发沟通渠道，为社区提供易于访问和实用的共同平台，如论坛、邮件列表和 Slack，以便讨论问题和分享解决方案。为了更加细致的 Code Review 贡献流程，火山引擎会通过大量人力和代码审查，找到并修复漏洞和错误，对于合并到主分支代码，设定严格的贡献流程和准则。此外，火山引擎还会建立响应性强的维护团队，快速响应报告的漏洞和问题，确保问题在短时间内得到解决。</p><p></p><p>另外，火山引擎还将会为项目提供明确的贡献指南，让新贡献者了解如何开始以及他们的贡献是否受欢迎。之后火山引擎还将定期举办线上、线下活动，研讨项目进展、最佳实践及相关贡献者感兴趣的议题，同时会做好权益保护工作，力求构建一个有影响力的社区。</p><p></p><p>关于 BMF 的未来，王悦表示“未来发展路径非常明确。”目前 BMF 框架已整体开源，共有 9 个开箱案例，用户在 Colab 打开后可查看详细注解并实际运行。其中，有 20 多个 API 调用范例，充分展示了 BMF API 的使用方法，特别是能同时展示不同场景的使用细节，帮助用户低成本构建视频应用。此外，因为本次 BMF 开源是火山引擎与 NVIDIA 的合作之举，所以在未来，两者将共建底层能力，优化视频 GPU 解决方案。NVIDIA 侧表示，“未来将听取社区及用户的意见，在 NVIDIA 最新的软硬件平台上进行迭代，继续完善 BMF。”火山引擎将联合 NVIDIA 持续开源更多核心能力，完善解决方案（包括更多异构硬件支持、原子能力及移动端支持等），构建新模块，加速视频生态建设，对 BMF 感兴趣的同学可以到 GitHub 上去了解项目详情，共创 BMF 开源的理想社区。</p>",
    "publish_time": "2023-08-24 18:27:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "智能技术在金融行业的应用与实践",
    "url": "https://www.infoq.cn/article/k0QUqSaEI4Hxbb4V94hZ",
    "summary": "<p><img alt=\"unpreview\" src=\"https://static001.infoq.cn/resource/image/10/1a/107b0aac6af3bb74e524feb13f1fb21a.jpg\" /></p>",
    "publish_time": "2023-08-24 18:38:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]