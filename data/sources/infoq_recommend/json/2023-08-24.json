[
  {
    "title": "伯克利AI实验室开源图像编辑模型InstructPix2Pix，简化生成图像编辑并提供一致结果",
    "url": "https://www.infoq.cn/article/JSu06eARbfbxUzEe2iPk",
    "summary": "<p>来自<a href=\"https://bair.berkeley.edu/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">伯克利人工智能研究</a>\"（BAIR）实验室的研究人员开源深度学习模型<a href=\"https://www.timothybrooks.com/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">InstructPix2Pix</a>\"，它可以遵循人类指令来编辑图像。InstructPix2Pix在合成数据上进行训练，表现优于基线AI图像编辑模型。</p><p></p><p>BAIR团队在最近举行的2023年IEEE/CVF<a href=\"https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">计算机视觉和模式识别</a>\"（CVPR）大会上<a href=\"https://arxiv.org/abs/2211.09800?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">展示了他们的工作成果</a>\"。他们先是生成了一个合成训练数据集，其中的训练样本是成对的图像以及用于将第一幅图像转换为第二幅图像的编辑指令。该数据集用于训练图像生成扩散模型，该模型可以接受基于文本的指令来编辑图像。例如，给定一张骑马的人的图片和提示词“让她变成骑龙”，它会输出原始图片，但原来的马被替换了龙。BAIR的研究人员的表示：</p><p></p><p></p><blockquote>尽管模型完全是在合成样本上进行训练的，但它实现了对任意真实图像和人类自然语言指令的零样本泛化。我们的模型能够进行直观的图像编辑，可以遵循人类指令执行多种编辑：替换对象、改变图像风格、修改设置、艺术媒介等。</blockquote><p></p><p></p><p>之前的AI图像编辑能力通常是进行风格转换，流行的文本到图像生成模型（如<a href=\"https://www.infoq.com/news/2021/02/openai-gpt-image/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">DALL-E</a>\"和<a href=\"https://www.infoq.com/news/2022/09/stable-diffusion-image-gen/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">Stable Diffusion</a>\"）也支持图像到图像风格转换操作。然而，使用这些模型进行有针对性的编辑仍然具有挑战性。最近，InfoQ报道了微软的<a href=\"https://www.infoq.com/news/2023/04/microsoft-visual-chatgpt/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">Visual ChatGPT</a>\"，它可以调用外部工具来编辑图像，前提是提供编辑操作的文本描述。</p><p></p><p>为了训练InstructPix2Pix，BAIR首先创建了一个合成数据集。为此，团队在一个由输入文字说明、编辑指令和期望输出文字说明组成的人类文本样本的小数据集上对GPT-3进行了微调。然后，这个微调模型被给予一个大型的输入图像文字说明数据集，从中生成了超过450k次编辑和输出文字说明。然后，团队将输入和输出文字说明馈送到预训练的<a href=\"https://arxiv.org/abs/2208.01626?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">Prompt-to-Prompt</a>\"模型中，该模型根据文字说明生成成对的相似图像。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/243f4068025109361e49dafcb3f62f7f.webp\" /></p><p></p><p>InstructPix2Pix的架构，图片来源：<a href=\"https://arxiv.org/abs/2211.09800?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">https://arxiv.org/abs/2211.09800</a>\"</p><p></p><p>研究人员鉴于这个数据集训练了基于Stable Diffusion的InstructPix2Pix。为了评估其性能，团队将其输出与基线模型<a href=\"https://github.com/ermongroup/SDEdit?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">SDEdit</a>\"进行了比较。他们使用两个指标之间的权衡：一致性（即输入图像和编辑后图像的CLIP嵌入之间的余弦相似度）和方向相似性（即编辑后文字说明中的变化与编辑后图像的变化在多大程度上保持一致）。在实验中，对于给定的方向相似性值，InstructPix2Pix产生的图像比SDEdit具有更高的一致性。</p><p></p><p>人工智能研究员<a href=\"https://en.wikipedia.org/wiki/Andrew_Ng?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">吴恩达</a>\"在他的深度学习新闻邮件组“<a href=\"https://www.deeplearning.ai/the-batch/issue-199/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">The Batch</a>\"”中评价了InstructPix2Pix：</p><p></p><p></p><blockquote>这项工作简化了生成和人造图像的编辑操作，并提供了更一致的结果。巧妙地利用现有模型，模型作者能够使用相对较少的人类标记样本在新任务上训练他们的模型。</blockquote><p></p><p></p><p><a href=\"https://github.com/timothybrooks/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">InstructPix2Pix的代码</a>\"可在GitHub上获取，<a href=\"https://huggingface.co/timbrooks/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">模型</a>\"和<a href=\"https://huggingface.co/spaces/timbrooks/instruct-pix2pix?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">基于Web的演示</a>\"可在Huggingface上访问。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/berkeley-instruct-pix2pix/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTIzMzgwOTYsImZpbGVHVUlEIjoiczFUaFlGRFNnWm9laVJkSyIsImlhdCI6MTY5MjMzNzc5NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.pRHWBdOIPJ034Vs28K3HLWFeLf-d1eewbwAGKyHBAUY\">https://www.infoq.com/news/2023/07/berkeley-instruct-pix2pix/</a>\"</p>",
    "publish_time": "2023-08-24 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]