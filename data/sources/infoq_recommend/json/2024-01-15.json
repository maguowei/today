[
  {
    "title": "Quarkus 开发基于 LangChain4j 的扩展，方便将 LLM 集成到 Quarkus 应用程序中",
    "url": "https://www.infoq.cn/article/TiriYas2wqBLwcYt4S5J",
    "summary": "<p>LangChain for Java（LangChain4J）战略顾问和布道师 Lize Raes 在 2023 年比利时 Devoxx 大会上做了“Java Meets AI”的演讲，受此启发，Quarkus 团队开始开发基于 LangChain4J 库的扩展，这是 LangChain 库的 Java 重新实现版本（最初用 Python 或 JavaScript 实现）。这将允许开发人员将大语言模型（LLM）集成到他们的 Quarkus 应用程序中。Quarkus LangChain4J 第一个公开版本，即 0.1 版本，在 2023 年 11 月中旬发布。这个扩展几乎每周都会发布一次，最新版本是 0.5.1。</p><p></p><p>InfoQ 采访了 Quarkus 项目联合负责人 Max Rydahl Andersen，分享了他对该扩展未来发展以及是否适合投产的看法。他说：</p><p></p><p></p><blockquote>在真实项目中使用它？可以尝试，但 langchain4j 的 API 仍在变化，所以我们还处于实验阶段。我们将继续跟进 langchain4j 并不断扩展它。我们十分关注扩展对“开放”模型的支持，特别是那些可以在云端或本地基础架构上运行的模型。</blockquote><p></p><p></p><p>Andersen 认为 LLM 可以被用在现有和未来的许多企业项目中，他认为新兴的编程模型与 Quarkus 现有的功能集相契合。该扩展允许声明性地定义 LLM 集成点，类似于 Quarkus REST Client：用 @RegisterAiService 注解接口，然后通过在应用程序的任意位置注入服务来使用 LLM。这种方法具有以下优点：</p><p></p><p>可测试性，可以通过伪接口实现来模拟服务 ;可观测性，开发人员可以用指标注释来监控方法 ;弹性，开发人员可以通过容错注释来处理故障、超时和其他临时问题。</p><p></p><p><code lang=\"typescript\">@RegisterAiService\npublic interface TriageService {\n  // methods.\n}\n</code></p><p></p><p>在使用像 ChatGPT 这样的 LLM 时，大多数交互是通过自然语言提示进行的，而在传统应用程序中，交互是通过编程语言进行的。与传统代码不同，quarkus-langchain扩展保留了与 LLM 交互的方式，允许开发人员通过自然语言定义范围和任务。LLM 的范围可以通过 @SystemMessage(String) 注解来定义，任务可以通过 @UserMessage(String) 注解来定义。</p><p></p><p><code lang=\"properties\">@RegisterAiService\npublic interface TriageService {\n    @SystemMessage(\"\"\"\n        You are working for a bank, processing reviews about\n        financial products. Triage reviews into positive and\n        negative ones, responding with a JSON document.\n        \"\"\"\n    )\n    @UserMessage(\"\"\"\n        Your task is to process the review delimited by ---.\n        Apply sentiment analysis to the review to determine\n        if it is positive or negative, considering various languages.\n\n        For example:\n        - `I love your bank, you are the best!` is a 'POSITIVE' review\n        - `J'adore votre banque` is a 'POSITIVE' review\n        - `I hate your bank, you are the worst!` is a 'NEGATIVE' review\n\n        Respond with a JSON document containing:\n        - the 'evaluation' key set to 'POSITIVE' if the review is\n        positive, 'NEGATIVE' otherwise\n        - the 'message' key set to a message thanking or apologizing\n        to the customer. These messages must be polite and match the\n        review's language.\n\n        ---\n        {review}\n        ---\n    \"\"\")\n    TriagedReview triage(String review);\n}\n</code></p><p></p><p>由于大型语言模型的知识收到训练集数据的限制，Quarkus LangChain4j 扩展提供了两种机制来扩展知识：工具和文档存储。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cc/cc6b754e71f009789126c128be099cdc.png\" /></p><p></p><p>工具允许 LLM 与父应用程序发生交互，它通过调用 REST 端点或执行数据库查询来实现交互。LLM 决定要使用的参数以及如何处理结果。要声明一个工具，只需在&nbsp;bean&nbsp;方法上使用@Tool注解：</p><p></p><p><code lang=\"java\">@ApplicationScoped\npublic class CustomerRepository implements PanacheRepository {\n\n    @Tool(\"get the customer name for the given customerId\")\n    public String getCustomerName(long id) {\n        return find(\"id\", id).firstResult().name;\n    }\n}\n</code></p><p></p><p>文档存储是 Quarkus 的检索增强生成（RAG）实现，这是一种用与感兴趣主题（用户手册、内部文档等）有关的文档来扩展 LLM 上下文的机制。从文档中获取信息包括两个步骤：</p><p></p><p>摄入过程——解析文档并计算其向量表示，然后存储在文档存储库中。Quarkus 提供了一个 Ingestor 来简化信息的摄入。</p><p></p><p><code lang=\"java\">@Inject\n    EmbeddingModel embeddingModel;\n\n    public void ingest(List documents) {\n        var ingestor = EmbeddingStoreIngestor.builder()\n                .embeddingStore(store)\n                .embeddingModel(embeddingModel)\n                .documentSplitter(recursive(500, 0))\n                .build();\n        ingestor.ingest(documents);\n    }\n}\n</code></p><p></p><p>RAG 过程——在调用 LLM 之前，查询文档存储并丰富上下文。Quarkus 在这里使用的是 Retriever。</p><p></p><p><code lang=\"java\">@ApplicationScoped\npublic class RetrieverExample implements Retriever {\n\n    private final EmbeddingStoreRetriever retriever;\n\n    RetrieverExample(RedisEmbeddingStore store, EmbeddingModel model) {\n        retriever = EmbeddingStoreRetriever.from(store, model, 20);\n    }\n\n    @Override\n    public List findRelevant(String s) {\n        return retriever.findRelevant(s);\n    }\n}\n</code></p><p></p><p>目前，该扩展支持 Redis Store、Chroma Store、Pinecone Store、PgVector（PostgreSQL）Store、进程内 Embedding 或加载 CSV 文件以及与商业（例如 OpenAI）和开源模型（例如 Hugging Face 或 Ollama）进行交互的能力。</p><p></p><p>Quarkus 紧跟 Spring Framework 的脚步，加入了嵌入 AI 能力的行列。该实现基于 LangChain4j，并得到了 LangChain4j 作者 Dmytro Liubarskyi 及其团队的支持。因为发展迅速，团队正在寻求反馈和想法来改进这些集成。Andersen 表示，LLM 扩展是对其他现有集成非常好的补充：可以集成各种数据摄入系统（例如，通过 Apache Camel 集成），而 Quarkus 的云原生 DNA 可以实现轻松高效的部署。</p><p></p><p>查看英文原文：</p><p></p><p><a href=\"https://www.infoq.com/news/2023/12/quarkus-langchain-llm-integratio/\">https://www.infoq.com/news/2023/12/quarkus-langchain-llm-integratio/</a>\"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651192572&amp;idx=1&amp;sn=4567a055013eabe0d3d443651faaa728&amp;scene=21#wechat_redirect\">雷军为澎湃 OS 团队颁发百万美金技术大奖；阿里 Lazada 在东南亚六国大裁员；华为员工 2023 年出差134万次｜Q资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651192432&amp;idx=1&amp;sn=3078ba85073a8de315fcc2da43cddc3e&amp;chksm=bdb804238acf8d355d9ad35e01d701c9dc0781164f6925bc628f75a2230a23a3225d9c9f6a73&amp;scene=21#wechat_redirect\">颠覆软件工程、“杀死”开发者？回溯大模型落地应用这一年 | 盘点</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651192268&amp;idx=1&amp;sn=8f97901c0e588bc432f55d50afe0bd64&amp;chksm=bdb8039f8acf8a89750411bc9bbe7b3e8fbcdf5f3e8b8b64044947b52f92051a8fd27a8e853d&amp;scene=21#wechat_redirect\">今年向量数据库“杀疯了”，但纯向量数据库“凉”了？| 盘点</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651192148&amp;idx=1&amp;sn=cc4a050e0199a9d52c4195a95a2e7b8e&amp;chksm=bdb803078acf8a11fdc6ed87ab25e6324f536e98108492a39126a80466cbf0f59517705cbb48&amp;scene=21#wechat_redirect\">金融业采用大模型，是“用大炮轰蚊子”吗？| 盘点</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/93/93a6e30a75e8e663d639c54513765ef5.gif\" /></p><p></p>",
    "publish_time": "2024-01-15 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]