[
  {
    "title": "抖音世界杯中 RTC 的技术挑战与实现",
    "url": "https://www.infoq.cn/article/CByq89C2DwrJxzXzAcad",
    "summary": "<p></p><blockquote>本文整理自火山引擎RTC 互娱解决方案负责人 叶峰峰 在2022 QCon北京站的分享<a href=\"https://qcon.infoq.cn/2023/beijing/presentation/5171\">《抖音世界杯中 RTC 的技术挑战与实现》</a>\"。</blockquote><p></p><p></p><p>今天给大家分享的内容是抖音世界杯中RTC的技术挑战与实现。我们将探讨两个问题，第一个是世界杯和RTC有什么样的关系，或者说RTC在抖音世界杯里边承担了什么样的玩法。第二个问题是在这个场景下，我们会遇到什么样的技术挑战，而我们是怎么样去解决优化的。</p><p></p><p>本次演讲共分为5个部分：</p><p>1、抖音世界杯“边看边聊” 介绍；</p><p>2、整体方案与技术挑战 ；</p><p>3、超高并发的问题优化和实践；</p><p>4、音频体验问题的优化和实践；</p><p>5、总结与展望。</p><p></p><h1>抖音世界杯“边看边聊”介绍</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b6540b13138ca6a10f3278df31c7f1ef.png\" /></p><p></p><p>回顾一下过去几年我看世界杯的经历，大概分为以下几个阶段。第一个阶段是我们大学阶段，在大学里边看世界杯的时候，当出现比较精彩的进球或者比赛结束的时候，整个宿舍楼都在欢呼，有非常好的观赛体验。左下是2018年世界杯决赛观赛的照片，当时是在一个酒吧里边看球，有我们的同学和同事，大家一起在线下看球聊球，也有非常好的观看体验。2022年抖音把世界杯搬到了我们的手机上，我们在移动端、PC端以及web端实现了全端的4k超高清50帧高帧率2.5秒超低延时，实现了广播级广电级的直播体验，给广大球迷带来了很好的观赛体验，也收获了大家的好评。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a6c714f693d2493faf518e1383451eb5.png\" /></p><p></p><p>但是在这个场景下，我们发现有一个比较严重的问题，就是在一个千万人在线的直播间里边，公屏消息很难承载有效的实时互动。这个问题要怎么解决，我们考虑加入一个边看边聊的功能来提升我们直播间里面的有效互动。大家可以看一下右边这个图，就是我们在世界杯直播间里的聊天频道增加了一个边看边聊的入口，用户可以去创建自己的私人频道，然后通过抖音、站内信或者是微信链接的方式，把你的频道分享给你的好友，一起来边看边聊。</p><p></p><p>业务侧做边看边聊功能提出三个诉求，首先希望提升我们直播间里面的有效互动，好友之间的互动肯定会比公屏里边的互动更加高效，我们希望整个直播间里边看边聊的互动可以提升5倍到10倍。其次大家可以通过边看边聊这个功能去分享你的直播间，让更多的用户加入到我们的直播间里面来，提升直播间的分享和看播的渗透。最后我们希望好友之间观看的时候获得更强的陪伴感，这样的话可以有效增加大家的看播时长。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/233a1ea0c5b0556d7c5a07a0b3d6d22b.png\" /></p><p></p><p>在边看边聊场景里边，除了可以发送文字、表情、语音这样的信息来和其他人互动。最重要的，我们为了更进一步增强实时互动的体验，增加了实时语音互动的功能。单个频道最多允许500人，也就是说你可以创建一个频道，邀请500个好友加入你的频道里边来实时互动交流。在频道里边允许9个用户同时上麦聊天，另外491个用户不发言，并支持用户上麦下麦、本地静音、关闭远端声音等操作。这都是我们RTC的一些基础能力，然后为了更好支持业务目标，为了保障整体方案的稳定，我们在支持业务的时候，RTC侧实时语音互动也定了指标。</p><p></p><p>第一个是希望我们的边看边聊的实时语音互动的PCU是100万，当然大家可能会觉得100万并不高，为什么没定太高？因为在直播间里边渗透到我们边看边聊玩法里边有一个渗透率的概念，在边看边聊里边可能有的人还不会开启语音互动，所以我们整体的边看边聊场景实时语音互动的PCU当时定义的是100万。</p><p>第二个是希望我们RTC整体的服务是稳定的，定了一个目标是SLA大于99.9%。</p><p></p><p>第三个是希望我们的客户端在这个场景里边是稳定的，所以我们定义RTC的 SDK的Crash率小于万分之一。</p><p></p><p>最后，我们希望在整体的直播整个过程中，用户的体验是良好的，所以希望我们线上用户的整体音频相关问题的反馈率低于万分之一。</p><p></p><h1>整体方案与技术挑战</h1><p></p><p>我们接下来看一下整体的RTC相关技术方案和技术挑战。</p><p></p><h2>技术方案</h2><p></p><p>在我们确定要做这个功能的时候，我们就开始讨论我们的技术方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a9eb64a3b3ab796bda7b3643090af6b.png\" /></p><p></p><p>第一个方案它是一个语音聊天室，在这个方案里边，思路是每个用户都从我们的CDN来拉取直播的比赛流，麦上的用户语音聊天的时候，加入我们的RTC系统来和麦上的用户进行语音聊天，同时后端启用一个旁路推流的功能，把聊天的内容发送到CDN上面去，麦下的491个用户可以拉取另外一路 CDN流来收听麦上用户的聊天。</p><p></p><p>为什么会选择这个方案？因为这个方案是一个可以复用线上语音聊天室的业务架构的方案，我们业务里边的上麦下麦还有相关的流程都是比较成熟的，我们可以快速搭建这样的玩法。</p><p></p><p>但是这个方案存在一个比较严重的问题，就是没上麦的用户听到的聊天内容会有比较大的延时，比如用户会在进球后大约1-3秒才能听到麦上用户聊进球相关的内容，这是一个比较严重的方案缺陷。</p><p></p><p>为了更进一步让所有用户都能实时参与到实时语音互动里面去，我们最终选择了我们RTC的互动语聊方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8da34febd523e75215b6effce559908a.png\" /></p><p></p><p>在这个方案里，客户端每一个用户仍然会从直播CDN拉取直播流，在播放器上大家可以手动去选择清晰度档位，还可以观看比赛的高光时刻、精彩进球，同时所有加入实时语音互动的用户，都会加入到RTC系统里面去，进行实时语音交流。同时我们还提供了实时的云端审核功能和离线的人工审核功能来保障聊天内容的安全，并通过API的方式提供了踢人、拉黑的运维API，来保障在一些关键的直播间里边如果有不符合预期的状态出现，可以人工干预去解决问题，保障系统的合规性。</p><p></p><h2>技术挑战</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4b4326883d8bdead86d526a4ae2a2f8e.png\" /></p><p></p><p>确定了我们的技术方案以后，我们着重于以下两个方面进行了优化。第一个是超高并发问题，它主要体现在哪？</p><p></p><p>首先是超高并发问题就是音视频流数的高并发。我们大家都了解，RTC里边每个用户都要去发布一路流去订阅别人的流，有多少个发布用户就要订阅多少条流。在边看边聊场景，尤其世界杯 RTC场景有其特殊性，在抖音世界杯期间，用户在线流数会集中在比赛这段时间，大概就两个小时有几百万的人同时上线去语音聊天，这个时候对我们的系统就形成了非常大的高流数并发的压力，我们在这个场景下进行了一些优化。</p><p></p><p>其次是比赛开始的阶段会有大量的用户去进房，然后比赛中场休息和比赛结束的时候会有大量的用户退出房间，在这些阶段会有比较大的集中的进退房的请求，而这些操作在IT系统里边都是比较重的操作，我们希望在应对这种高QPS请求的时候系统保持稳定，于是我们做了一些优化。</p><p></p><p>在客户端我们还会遇到音频体验问题。第一个问题是在音频外放的时候，远端的人声和比赛的声音可能会被麦克风采集到发送到远端形成回声。第二个问题是通话的人声可能会小于比赛直播的声音，你跟远端用户通话聊天的时候，会发现他的声音比较小，听不清楚，没有实现很好的沟通效果。后面内容我们就主要针对上面这两个问题或者这两个大的优化项向大家来展开介绍。</p><p></p><h1>超高并发问题的优化和实践</h1><p></p><p></p><h2>RTC媒体流数高并发优化方案</h2><p></p><p></p><h3>高并发 - 常规方案</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/71/713f182809d7db70287cee96d8f763ec.png\" /></p><p></p><p>我们先聚焦于流数高并发问题，在RTC常规的方案是用户在最后一公里就近接入我们的服务器，然后服务器之间会做级联，用户从最近的边缘节点去拉流的方案。但是这种方案存在一些不合理的地方，或者说可以优化的地方：</p><p>成本：并发流数高，RTC系统开销大、成本较高扩展性：当前单房间最高并发10w，进房QPS50左右；无法满足后续可能出现的超高并发热流(10w~ 100w)的需求客户端性能：弱设备观众拉9路流性能压力大</p><p></p><h3>高并发 - 公共流扩展方案</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c807ec378de7248fe21d6c2070597d6d.png\" /></p><p></p><p>为了应对这些问题，我们在场景下引入了一个公共流扩展方案，麦上用户依然是通过加入我们RTC系统来进行实时的语音互动。我们在后端引入了一个MCU服务器，把麦上用户的所有数据进行混合之后，麦下用户只需要订阅 MCU流就可以了。它的优势首是是单房间的流数更低，在有限资源下可以支持更高的业务并发。最后是可以有效优化麦上用户拉流的性能。但是这个方案也存在两个问题，一是公共流转推服务也要占用后端的资源，所以并非所有的房间都开启公共流转推，二是如果用户进来的时候拉的是公共流，他想上麦发言交流的时候，势必存在一个从公共流停止拉流，加入RTC房间去实时拉流的过程，这个过程会出现卡一下的体验。虽然我们可以优化，但是这是无法避免的。</p><p></p><h3>高并发 - 融合方案</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/42c60d93a74a555a3468074265f7101f.png\" /></p><p></p><p>为了解决为了解决或者优化整个系统解决这些问题，我们边看边聊最终使用的是一个融合方案。大家可以看一下左图，当用户上麦的时候，去查看一下房间里边的在线人数是多少，如果房间里面在线人数小于公共流触发阈值的话，用户直接使用静默用户角色加入房间就可以。如果发现房间里边已经有比较多的用户了，我们就可以判定它是一个热门房间，在后端启用公共流的服务，后续所有进房用户都是拉取公共流的方案。</p><p></p><p>刚才大家也了解到，公共流状态的用户在上麦的时候会出现拉流的切换，会卡一下。所以我们在公共流用户上麦以后，变成可见用户之后，如果再要下麦或者闭麦，都是让它待在RTC的系统里边，不会再退到公共流拉流的状态，从而保证他上麦以后，所有的上下麦都是平滑操作的体验。</p><p></p><p>所以这个方案融合了两个方案的优势，麦上用户可以低延时非常高效的沟通，对于热门房间有一个无房间流方案，可以提高更高的并发。</p><p></p><h2>边看边聊限流方案</h2><p></p><p>接下来我们介绍一下，为了应对我们进房退房集中的高QPS我们做的一些限流的策略。限流主要保障什么？在我们预期外的高并发来的时候，保障系统稳定，所以很多策略就是超出预期的时候拒绝服务的策略。我们来看一下我们在限流做了哪些优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/038e93ee9ead1b0ffc6c2014affa7f23.png\" /></p><p></p><p>第一个是对进房请求做多极限流保护，其中有分布式的QPS限流，还有中心QPS限流，还有中心房间数限流。</p><p></p><p>第二个是退房请求延时处理，把退房请求超出处理的时候放到队列里面去处理。</p><p></p><p>最后一个是我们向业务侧提的需求，让业务侧配合我们一起去做保护的策略。g 一是延时关播，将退房流量打散。二是当线上语音聊天房间数超出RTC系统容量时，关闭新创建房间的语音聊天能力，保障已创建房间的稳定运行，保障整体稳定性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/949bfedd7e30455512914e0edcbb79dc.png\" /></p><p></p><p></p><p>我们再来看一下进房的限流策略，它其实和整体的RTC系统架构是相关的，我们整体RTC系统架构使用的是一个边缘接入、数据存储在中心这样的架构。所以在这套架构下面，我们把用户侧和边缘节点中间在边缘节点这儿做了分布式的QPS限流策略。</p><p></p><p>一个是全局分布式QPS限流。在边缘节点到中心机房之间，我们做了一个中心机房，这侧有一个 QPS限流策略。边缘节点使用的是一个固定时间处理固定请求的算法，它的思路是边缘节点定期向中心信令去请求可以处理的上限，边缘节点收到进房请求的时候，如果超出这个上限，就直接向客户端返回一个错误码，拒绝服务。这个方案有一个特点就是比较简单可靠，但是它的缺点是无法处理尖峰流量。</p><p></p><p>一个是中心QPS限流。在中心信令，我们加上了使用令牌桶的算法，定期以恒定速率生产令牌，有请求需要处理的时候，从令牌桶里边拿一个令牌出来再去处理。这个方案有一个优势，就是如果令牌桶是满的，出现了尖峰流量，我们可以短期处理比较多的请求，能一定程度上承载监控流量的问题。</p><p></p><p>在这两个请求策略的基础上，我们还增加了一个中心房间数限流的方案，当用户创建房间的时候，如果达到了限流上限，同样也会触发系统限流保护，保障整个系统的稳定。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e6828a4d47eb0bf563edb61c25d73ceb.png\" /></p><p></p><p>接下来是退房请求时候的延时处理保护，左边这个图是用户请求到边缘节点的时候，如果发现请求的QPS高于安全阈值，我们就把请求加入到队列里面去，为什么可以这么做？退房请求为什么可以这么做？因为进房请求需要实时处理，而退房请求是用户并不强感知的处理请求，我们可以延后对它进行处理，不会影响用户的正常使用体验。</p><p></p><p>它的基本逻辑是退房QPS大于阈值的时候，我们会记录当前请求的时间戳，然后把请求的上下文放到队列里边去延后执行，同时我们也会启动一个循环的任务，去取出这种延时处理的请求。然后首先进行一个判断，使用当前时间戳和请求时间对比超时的时间。如果请求已经判定超时了，就丢弃掉不处理了，中心机房也不会感知到这个请求，如果小于阈值的话就继续处理，处理成功以后就删除掉这个请求。</p><p></p><p>在我们引入这种延时处理逻辑以后，可能会遇到两个这种异常场景，大家可以考虑一下。第一个就是用户退房以后很短时间又重新加入房间，而我们退房请求又没有被处理，会不会形成退房？前面的退房可能会导致后面的进房失败的问题。解决问题其实依赖的就是请求时间戳的机制。当我们发现退房请求小于进房请求的时间戳的时候，退房请求就不用处理了，直接丢弃掉就可以了。同样，用户在我们处理的时候，中心机房说已经请求的用户已经断联了，我们不需要执行处理了，这时候也直接丢弃掉。</p><p></p><h1>音频体验问题的优化和实践</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c9/c9f180348aff667edc099faa78607284.png\" /></p><p></p><p>接下来我们来介绍一下客户端音频体验问题的优化和实践。这边在边看边聊场景下，我们重点要讲解两个问题，第一个是回声消除问题，第二个是远端人声响度优化问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d07e680d672e074e16a98fd0a3d0abb3.png\" /></p><p></p><p>RTC系统是一个双向的实时互动的系统，A端、B端用户的声音从扬声器播放出来，被他自己的麦克风收到，发到远端形成了回声问题，我们希望在A端通过合成消声算法解决掉这个问题，然后发送到网络上的信号是一个干净纯净的人声。所以我们RTC系统也引入了回声消除的模块，它有两个点：第一个是我们会把远端语音用户的语音送到回声消除模块，作为参考信号来做回声消除。第二个就是回声的形成是通过扬声器到麦克风的物理链路。在这个场景下我们需要通过算法来模拟回声消除的算法来逼近真实的回声路径，来对我们的回声进行预估，然后把回声消除掉。</p><p></p><p>所以大家可以看到回声消除里边有两个要素，第一个是回声消除的参考信号，第二个是回声消除的算法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f0db60389403cae7d6197a2b09d1bed.png\" /></p><p></p><p>我们来看一下在边看边聊场景里回声问题有哪些回声。第一个是远端人声的回声，还有一个是直播音频的回声。为了解决这个问题，我们引入了一个音频托管的解决方案。直播播放器在解码之后并不直接播放音频，而是把音频托管到RTC系统，RTC系统把远端的人声还有直播的音频混音之后送到回声消除模块进行回声消除，这样就有效解决了这个场景下面的回声参考信号的问题。接下来的一个问题是回声消除的算法问题。</p><p></p><p>涉及到回声消除算法需要向大家介绍一下移动端设备音频外放的通道模式，还有为了解决这个问题的高音质3A算法使用。大家如果是做移动端开发的话，可能会了解到移动端的设备存在两个音频通道模式，第一个是通话模式，第二个是媒体模式。通话模式有更好的回声消除效果，更适合用于语音聊天场景，它对人声的处理是更加清晰的，但是存在一个问题，处理音乐的时候可能会有失真，媒体模式优点是有比较好的声音的播放效果，但是没有系统的硬件回声消除或者是效果比较差，所以需要我们整个RTC系统去做环境消除。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a1/a1bf087b5c13de3db74cda2641f0288e.png\" /></p><p></p><p>我们使用通通话模式和媒体模式，在边看边聊场景下做了一个相应的验证。使用通话模式的时候，我们发现两个问题，第一个是手机的音频通道模式和通话模式的音量其实是不一样的，在手机上就有两个音量，从媒体模式直播的媒体模式切换到边看边聊的通话模式之后，它的音量会有突变，整体体验是比较差的。第二个是切换边看边聊以后，由于使用了通话模式，直播的音质会从原来的媒体模式通切换到通话模式，音质表现也变差了。为了解决这两个问题，我们最终选择的是全量使用媒体通道模式的方案，但是在这个方案里边也会遇到一个问题，系统的回音消除的效果会比较差，需要RTC自己去实现高音质3A的算法。</p><p>在这个基础上为了保障边看边聊的整体音质体验，我们使用了媒体模式。我们火山引擎RTC也使用了自研的基于深度学习的高音质回声消除算法，确保在无回声的基础上保障我们整体的高音质的表现。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d332a4c3d4df0773dce13d0f038b5c24.png\" /></p><p></p><p>大家可以先看一下左边这张图，两段人声过来混音之后，直接送到我们软件的回声消除模块进行回声消除。然后向大家介绍一下我们线上媒体模式和通话模式放量的进展，在iOS上面iPhone6以上机型都已经放量了，业务的各项指标都是比较正向的，在安卓系统上也放量了85%以上的机型，而且在语音场景做到了所有机型的全量，语音聊天室社交的连线时长等业务指标也都是比较正向的。今年我们更进一步对应媒体模式的算法，还有音质画质相关做优化，期待未来可以做到更好体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/61f264d5c7d3deb85a752b435f8b77ef.png\" /></p><p></p><p>我们在测试阶段发现的另外一个问题是直播间里边解说和比赛的声音它是比较大的，这时候远端用户聊天的时候声音可能不是那么大。这出现一个问题，直播流的声音大，当解说和远端人声同时说话的时候，出现语音人声小，然后听不清楚对端人声。为了解决这个问题，我们就引入了一个智能音频闪避的方案，大家可以看一下这个图，智能音频闪避其实是我们去检测一个信号，比如图中的远端人声，当它的声音超过一定的电平，或者检测到它是人声的时候，就触发机制，适当降低BGM或者直播流的播放音量。这时候我们能听清楚原来人通话说话的声音，在这种场景下就是闪避的，其实在我们FM电台主播应用场景是比较常见的，他说话的时候会调低音乐，实现闪避的效果。引入了闪避以后，直播流过来在做混音前进入闪避的模块，当我们接受到远端的人声的时候，会做AI的语音检测，让闪避模块把直播流的声音降低，突出远端的人声。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fd/fd6a6e36fa5f38fd58ee33f4b5d87ecb.png\" /></p><p></p><p>上图是我们做闪避的一些经验总结，然后大家可以看一下，就不再展开讲了。</p><p></p><h1>总结与展望</h1><p></p><p>最后我们对整个项目进行一个总结和展望。整个世界杯下来，我们整体的峰值房间数在这个边看边聊场景达到了172万，远高于赛前预估的46万。峰值用户数也高于200万，远超赛前预估的100万。RTC系统也是保持了高度的稳定，整体的SLA大于99.9%。公共峰值用户数达到了16.6万，双端的crash率都是在3/10万这样的水平，是一个非常高稳定性的状态。通过引入边看边聊玩法，我们增加了直播间的互动矩阵，就是观众和观众之间的连线互动。通过这次边看边聊的实践，我们沉淀出一起看直播成熟的方案，观众之间可以一起看直播、比赛、演出等丰富的内容，基于内容增强用户的互动性。</p><p></p><p></p><h1>演讲嘉宾：</h1><p></p><p>叶峰峰，毕业于北京邮电大学，毕业后一直从事RTC相关工作。2019年加入火山引擎，主要负责 RTC 互娱场景解决方案的建设和业务的支持，先后支持抖音社交音视频通话、抖音连麦玩法功能的开发和优化迭代。</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/PZfOXddqkRDeUZ9lXs5y\">从 QoS 到 QoE，RTC&nbsp;的用户体验该如何评判？</a>\"</p><p><a href=\"https://www.infoq.cn/article/crsbfAKl2fJtjMvJl9L6\">经历过亿级 DAU 的打磨检验，抖音同款&nbsp;RTC&nbsp;到底有何魔力</a>\"</p><p><a href=\"https://www.infoq.cn/video/xbX8fzoD0bplTMH1big6\">RTC&nbsp;全球化架构助力业务出海</a>\"</p><p></p>",
    "publish_time": "2023-04-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "使用伪对象进行单元测试：避免过度设计，降低测试成本",
    "url": "https://www.infoq.cn/article/lffDWFY1Ch66CrQURbYy",
    "summary": "<p>开发人员编写测试是为了增强对产品代码正确性的信心、记录意图和为应用程序设计提供帮助。最近，我们看到开发人员在单元测试中大量使用测试替身，尤其是模拟对象。这样做是为了提高测试的速度，减少对基础设施的依赖，或减少依赖的对象数量。然而，它常常以低可信度、不清晰的文档以及实现和测试代码之间的高耦合为代价，这是不可接受的。</p><p>&nbsp;</p><p>为了避免这些问题，开发人员应该考虑使用伪对象而不是模拟对象，因为伪对象不仅提供了类似的隔离性，而且带来了高可信度、清晰的文档以及实现和测试代码之间的松散耦合。</p><p>&nbsp;</p><p></p><h2>背景</h2><p></p><p>&nbsp;</p><p>我们将较低级别的测试归类为单元测试，表示这些测试与周围的其他代码存在某种形式的隔离。由于这种隔离，单元测试应该执行得快、编写简单、易于理解和维护。</p><p>&nbsp;</p><p>开发人员通常使用测试替身作为提升这种隔离性的一种方式。测试替身是在测试中用来代替协作者的对象。Gerard Meszaros在他的著作《<a href=\"http://xunitpatterns.com/\">xUnit测试模式</a>\"》中定义了几种类型的测试替身：控对象（Dummy）、间谍、存根、伪对象（Fake Object）和模拟对象（Mock Object）。在本文中，我们将关注最后两个：</p><p>&nbsp;</p><p><a href=\"http://xunitpatterns.com/Mock%20Object.html\">模拟对象</a>\"预先置入了它们期望接收到的调用和它们对这些调用的响应。它们有一种机制来验证在测试期间是否收到了正确的调用，如果调用不符合它们的期望，测试就会失败。人们经常使用Mockito、Mockk或GoMock这样的框架来创建模拟对象。<a href=\"http://xunitpatterns.com/Fake%20Object.html\">伪对象</a>\"是协作者的功能实现，它们通过某种快捷的方式让它们更适合用在测试环境中。例如，在执行本地测试时，开发人员可以创建内存数据存储来代替将数据保存到S3的对象。</p><p>&nbsp;</p><p>对于现代代码库测试套件和在没有任何支持服务情况下运行的测试套件，几乎所有东西都是模拟的。在这种情况下，测试套件为系统每一个部分独立运行的准确性提供了很高的可信度，但对于它们被放在一起运行时的准确性却没有提供多少可信度。稍后，我们将讨论何时不适合使用模拟对象。</p><p>&nbsp;</p><p>例如，许多测试套件会在测试期间模拟数据库层。测试案例会检查是否对数据库进行了正确的调用，并返回预先置入的响应。这样的测试套件很难让我们相信代码在生产中会正确地运行，因为数据库调用从未真正被执行过，预先置入的调用可能是不正确的，更何况SQL语句无法被测试到。</p><p>&nbsp;</p><p></p><h2>隔离</h2><p></p><p>&nbsp;</p><p>大家普遍认为，单元测试中的单元指的是隔离单元。也就是说，单元测试在某种程度上与其他的代码库是隔离开来的。然而，在定义什么是隔离单元时，存在不同的意见。</p><p>&nbsp;</p><p>这个定义很重要。隔离单元决定了每个测试的范围、测试代码和产品代码之间的关系，并最终决定了应用程序架构。从历史上看，隔离是有定义的，并且被广泛接受，我们将在下面讨论。</p><p>&nbsp;</p><p></p><h4>测试隔离</h4><p></p><p>&nbsp;</p><p>经典测试方法代表人物Kent Beck认为：</p><p>&nbsp;</p><p></p><blockquote>“单元测试彼此完全隔离，每一个测试都会从头开始创建它们所需的测试资源。”</blockquote><p></p><p>&nbsp;</p><p>在这里，单元指的是测试本身：单元测试之间是相互隔离的。Beck认为“测试应该与代码的行为耦合，并与代码的结构解耦。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b899b84eddec75272288606054d05e37.png\" /></p><p></p><p>&nbsp;</p><p>使用这种方法编写的测试往往只有很少的模拟对象，更多的是使用协作对象的实例，甚至是真实的基础设施（例如数据库）来执行每个测试。</p><p>&nbsp;</p><p>例如，有一个经典的测试，它的主体是进行数据库调用，所以它会在测试期间使用真实的数据库。这类测试将确保数据库在运行之前处于正确的状态，并检查结果数据库状态与预期是否匹配。</p><p>&nbsp;</p><p>以外部HTTP调用为主体的测试将在执行测试时进行HTTP调用。由于外部调用通常会降低测试的可靠性，因此作者可能会在本地启动一个行为与外部服务类似的HTTP服务器。</p><p>&nbsp;</p><p>经典的测试为代码行为的正确性提供了高度的可信度。当代码被重构时，测试往往不需要发生变化，因为它们与协作者的外部接口是松散耦合的。</p><p>&nbsp;</p><p></p><h4>主体隔离</h4><p></p><p>&nbsp;</p><p>模拟对象方法代表人物Steve Freeman和Nat Pryce认为：</p><p>&nbsp;</p><p></p><blockquote>“单元测试孤立地测试对象或一小组对象。”</blockquote><p></p><p>&nbsp;</p><p>Freeman认为，单元测试“可以帮助我们设计类并让我们相信它们的行为是正确的，但并没有说明它们是否可以正确地与系统的其他部分协作。”在这里，单元指的是被测试的主体。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/83b64463e32c352251f96ba8e2790af3.png\" /></p><p></p><p>&nbsp;</p><p>使用这种方法编写的测试必须使用测试替身来代替协作者，并且往往会用到许多模拟对象。他们很少使用真正的基础设施，而更倾向于使用模拟对象或替身。我们的想法是，在测试过程中，我们应该将测试对象与其协作者的行为隔离开来，一个对象行为的变化不应该影响另一个对象。开发人员还使用模拟对象来提高测试的速度和可靠性，使用模拟对象来取代缓慢或不可靠的协作者。</p><p>&nbsp;</p><p>例如，一个以数据库调用为主体的模拟测试将在执行测试时模拟数据库层。主体将与模拟数据库对象发生交互，在测试期间记录调用，并在测试结束时执行检查。</p><p>&nbsp;</p><p>一个以外部HTTP调用为主体的模拟测试将在执行测试时使用模拟HTTP客户端。这个客户端将在测试期间返回预先置入的对HTTP调用的响应。在测试之后，测试作者将使用模拟对象来检查是否进行了正确的HTTP调用。</p><p>&nbsp;</p><p>这些测试能够快速可靠地执行，但它们提供的行为正确性可信度较低。当代码发生变化或被重构时，测试往往也需要做出重大的修改，因为它们深度耦合了协作者的外部接口。</p><p>&nbsp;</p><p>此外，使用模拟对象会增加测试代码的数量。在许多语言中，比如Go，作者必须编写或生成所有的模拟对象，并将代码保存在代码库中。这样会让测试套件的大小翻番。即使在Kotlin和Java中模拟对象是在运行时生成的，也必须在每次执行测试之前预先置入模拟对象，并在执行测试之后进行验证，这样会导致需要维护更多的测试代码。</p><p>&nbsp;</p><p></p><h2>实践</h2><p></p><p>&nbsp;</p><p>为了确定在实践中使用哪一种方法，我们首先必须列举出我们的测试目标。我们想要：</p><p>&nbsp;</p><p>增强对代码行为正确性的信心。记录我们的代码应该如何运行。帮助设计出松散耦合、高度内聚的软件。</p><p>&nbsp;</p><p>基于这些目标，我认为应该从单元测试的测试隔离方法开始。如果每个测试都可以可靠独立地运行，同时使用尽可能多的真实协作者，那么我们将可以实现以下这些目标。</p><p>&nbsp;</p><p>信心，因为我们的测试是在与生产环境类似的环境中运行的。我们可以确信，我们的测试对象在独立和协作的情况下都能正常运行。我们的测试也给了我们信心，测试主体与它们的外部协作者具有一致的正确行为。在进行模拟测试时，我们对测试主体是否能很好地协作没有那么强的信心。</p><p>&nbsp;</p><p>清晰的文档，因为阅读文档的人可以看到我们的代码是如何在与生产环境的环境中运行的。例如，阅读测试文档的开发人员可以简单地检查指定的操作将产生怎样的预期数据库状态，以便了解在生产环境中将会发生什么。而阅读模拟测试文档的开发人员必须将每个模拟对象的响应和期望转换为实际协作者的操作，这大大降低了清晰度和可读性。</p><p>&nbsp;</p><p>深思熟虑的设计。重构与测试代码是相互独立的，因此可以频繁地进行重构。但如果使用的是模拟测试，那么改变对象的外部接口时也需要重写或重新生成这个对象的所有模拟对象。而在使用测试隔离方法时，不需要重写模拟对象，重构所需的测试代码修改也更少。这使得重构更容易进行，也意味着可以更频繁地进行重构，并且代码库的设计会随着时间的推移而改进。</p><p>&nbsp;</p><p></p><h2>灵活变通</h2><p></p><p>&nbsp;</p><p>在实践中，我建议使用一种测试隔离方法，从经典的方法开始，在必要时可以回退到模拟测试。Martin Fowler说：“我并不认为在获取外部资源时使用替身是绝对的规则。如果获取资源足够稳定和快速，那么在单元测试中就没有理由不这么做……事实上，当90年代xunit测试开始起步时，我们并没有试图另辟蹊径，除非与协作者（比如远程信用卡验证系统）的交互很困难。”</p><p>&nbsp;</p><p>只要我们使用快速、可靠的协作者（这应该是我们的目标），那么使用真正的协作者进行测试并不会对我们测试的速度和可靠性产生负面影响。如果情况并非如此（例如，当通过HTTP与外部服务交互时），那么测试替身是提高测试速度和可靠性的好方法，只是牺牲了一点可信度、清晰度和灵活性。</p><p>&nbsp;</p><p>在考虑使用哪种测试类型时，最好选择伪测试对象而不是模拟测试对象。伪对象相比模拟对象有几个关键优势：</p><p>&nbsp;</p><p>伪协作者比模拟协作者更接近真实的协作者，这为我们提供了更高的可信度。我们与伪协作者的交互方式与我们与真实协作者的交互方式是相同的，这样可以获得更好的文档。每当真正的协作者发生变化时，也必须更新伪对象，这与模拟对象一样。但在使用伪对象时，我们不需要改变期望或验证，因此在使用伪对象时重构代码库往往比使用模拟对象更容易。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>在确定选择哪一种测试方法时，请仔细考虑一下单元隔离问题，这样你就会意识到经典方法或模拟测试方法的利与弊。你要根据协作者的性质来调整你的测试方法。最后，我们都想要快速、可靠且可以让我们更有信心发布软件、清楚地记录我们的意图并帮助我们设计可扩展的系统的测试套件。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/unit-testing-approach/\">https://www.infoq.com/articles/unit-testing-approach/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/Brwh11Pqd6Oa2T8VBpC2\">“TDD 就是死亡”？我要为单元测试辩护</a>\"</p><p><a href=\"https://www.infoq.cn/article/9OSwTpG2SxfEPHSFfKoU\">从忽略到重视，Stack Overflow 改变了对单元测试的态度</a>\"</p>",
    "publish_time": "2023-04-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Cloudflare的Kafka之旅：万亿规模消息处理经验分享",
    "url": "https://www.infoq.cn/article/h5UJQNEHuKPpAHoaPo5O",
    "summary": "<p>在伦敦QCon大会上，<a href=\"https://www.cloudflare.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">Cloudflare</a>\"高级系统工程师<a href=\"https://qconlondon.com/speakers/andreamedda?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">Andrea Medda</a>\"和工程经理<a href=\"https://qconlondon.com/speakers/mattboyle?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">Matt Boyle</a>\"分享了他们的平台服务团队在使用<a href=\"https://kafka.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">Apache Kafka</a>\"来处理万亿规模的消息方面得到的经验教训。</p><p></p><p>Boyle首先概述了Cloudflare需要解决的问题——提供自己的私有云和公共云服务，以及随着业务需求的增长而出现的团队耦合所带来的运营挑战。接着，他介绍了他们是如何将Apache Kafka作为他们的消息总线的。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/04/cloudflare-kafka-lessons-learned/en/resources/13B790C9D-FA94-43DD-B4CA-E21C65FC0EF4-1680460271602.JPG\" /></p><p></p><p>Boyle说，虽然消息总线模式解耦了微服务之间的负载，但由于schema是非结构化的，所以服务仍然是紧密耦合的。为了解决这个问题，他们将消息格式从<a href=\"https://www.json.org/json-en.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">JSON</a>\"转成了<a href=\"https://protobuf.dev/overview/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">Protobuf</a>\"，并构建了一个客户端库，在发布消息之前对消息进行验证。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/04/cloudflare-kafka-lessons-learned/en/resources/1Captura%20de%20pantalla%202023-04-02%20192529-1680460271602.png\" /></p><p></p><p>随着越来越多的团队开始采用Apache Kafka，他们开发了一个连接器框架，让团队可以更容易在Apache Kafka和其他系统之间传输数据，并在传输过程中转换消息。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/04/cloudflare-kafka-lessons-learned/en/resources/2F96AC728-906A-437C-89A4-6A1B2F49F973-1680461050766.JPG\" /></p><p></p><p>在疫情期间，随着Cloudflare系统负载的增加，他们发现他们的一个关键用户出现了瓶颈，已触及其服务水平协议（SLA）。Medda分享了他们为了找到问题的根源，不得不使用来自<a href=\"https://opentelemetry.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">Open Telemetry</a>\"生态系统的工具来丰富他们的SDK，以获得更好的跨堆栈交互可见性。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/04/cloudflare-kafka-lessons-learned/en/resources/1Captura%20de%20pantalla%202023-04-02%20192831-1680460271602.png\" /></p><p></p><p></p><p>Medda继续强调，他们SDK的成功带来了更多的内部用户，这催生了对更好的文档和<a href=\"https://www.atlassian.com/blog/software-teams/what-is-chatops-adoption-guide?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">ChatOps</a>\"支持形式的需求。</p><p></p><p>Medda总结了他们获得的主要的经验教训：</p><p></p><p>在为Apache Kafka提供开发人员工具时，在高度可配置和简单标准化方法之间取得平衡；选择简单而严格的1对1契约接口，确保最大限度地了解主题及其使用情况；在开发工具指标上投入，让问题可以更容易地被发现；为应用程序开发人员提供清晰的模式文档，确保他们在采用和使用Apache Kafka方面保持一致性。</p><p></p><p>最后，Boyle分享了一款叫作Gaia的内部产品，可以根据Cloudflare的最佳实践一键创建服务。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/04/cloudflare-kafka-lessons-learned/en/resources/1Captura%20de%20pantalla%202023-04-02%20193016-1680460271602.png\" /></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/04/cloudflare-kafka-lessons-learned/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODI0NzE3NzksImZpbGVHVUlEIjoiR2VmV3JTWjdWb29JdFkyNCIsImlhdCI6MTY4MjQ3MTQ3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mWPVsI2ZrWiOAQOzugS-8Ix8WsnN1-mKFACqyDYkhwk\">https://www.infoq.com/news/2023/04/cloudflare-kafka-lessons-learned/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/ElNtSM5ISobpMB8fMC0j\">使用 Strimzi 将 Kafka 和 Debezium 迁移到 Kubernetes</a>\"</p><p><a href=\"https://www.infoq.cn/article/CpfvECIb5gWdditBBYy7\">使用 Strimzi 提高 Kafka 集群的安全性</a>\"</p><p><a href=\"https://www.infoq.cn/article/WfA0p1XoZCJ6INdyJLyv\">Kafka Streams 与 Quarkus：实时处理事件</a>\"</p>",
    "publish_time": "2023-04-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "拼多多被曝临时确定五一放假三天，员工集体退票；字节跳动回应140万美元年薪挖角OpenAI；AI龙头寒武纪裁员，研发员工被逼签字｜Q资讯",
    "url": "https://www.infoq.cn/article/VBSP1UfbYfzqHI86mBM0",
    "summary": "<p></p><h2>科技公司&nbsp;</h2><p></p><p></p><h4>字节跳动回应 140 万美元年薪挖角 OpenAI</h4><p></p><p>&nbsp;</p><p>近日有消息称，字节跳动开出140万美元年薪，挖角 ChatGPT 的研发公司 OpenAI 团队成员。字节跳动相关负责人对此回应称，消息不实。</p><p>&nbsp;</p><p>值得注意的是，目前字节跳动尚未官宣自己的大模型产品。4月18日，字节跳动火山引擎发布新版机器学习平台。火山引擎总裁谭待表示，国内大模型领域的数十家企业，超过七成已经在火山引擎云上。“火山引擎自己是不做大模型的，我们首先服务好国内做大模型的厂商，等他们把大模型做好之后，我们再一起合作开展对外的服务。”谭待曾表示。</p><p></p><h4>腾讯前副总裁与小红书女高管重婚，被判拘役半年</h4><p></p><p>&nbsp;</p><p>日前，腾讯副总裁、PCG（平台与内容事业群）信息与服务线负责人郄小虎离任。资料显示，郄小虎与其在小红书就职期间的同事，小红书电商品质部负责人万某婧于2023年1月份被 JIESHEN 起诉，案由涉及重婚罪，开庭法院为上海市浦东新区人民法院。&nbsp;</p><p>&nbsp;</p><p>近日有网友晒出郄小虎因重婚罪被起诉判决图片。根据法院判决结果，法院二审认定郄小虎重婚属实，但犯罪情节较轻，认罪态度较好，认定一审判决拘役半年、缓刑半年符合法律规定。</p><p></p><h4>AI 龙头寒武纪裁员？有研发部员工称被逼签字，年终奖与配股都没兑现</h4><p></p><p>&nbsp;</p><p>近日，脉脉上有多名自称寒武纪的员工爆料公司正在裁员。4月21日，有接近寒武纪的人士回应称公司裁员情况属实。一名2021年入职寒武纪的上海员工表示，“公司和团队此前没有过任何通知，在4月19号突然逼我们签字，说自己是主动离职。”“应得的赔偿也没有，没有年终奖，没有 N+1，赔偿只给 N ”，林平还称，寒武纪此前承诺的股票并没有按时兑现。</p><p>&nbsp;</p><p>同日，寒武纪HRD（公关总监）王海波（经认证）在脉脉上称，“沟通后都会按照法律规定给予一个月的待通知金及 N 的补偿金，即N+1，不存在‘没有 N+1 ’一说。”“公司根据绩效考核结果进行绩效兑现，是通行、常规的管理手段，也是企业持续健康发展的管理举措。根据绩效考核结果协商解除劳动合同并给予 N+1 的补偿金是不违反劳动法规定的。”</p><p></p><h4>拼多多被曝临时确定五一放假三天，员工集体退票</h4><p></p><p>&nbsp;</p><p>据脉脉爆料，直到4月26日晚拼多多的放假时间才最终敲定了下来：只放三天假，4月29日、5月3日需要正常上班。拼多多员工表示，这种在法定节假日工作的情况并不会给3倍工资，只会发放当天的正常工资。</p><p>&nbsp;</p><p>看到临时的调休通知，有员工对此表示，已经订完的机票火车票酒店需要全部改签，因为在拼多多请假比登天还难。</p><p></p><h4>加班近 500 小时当事人再发声：拒绝签署“封口协议”</h4><p></p><p>&nbsp;</p><p>近日，蔚来员工曝加班近500小时进急诊3次的事件引发大量关注，当事人接受媒体采访时表示，在自己的事情被媒体报道并登上热搜后，蔚来方面开始同意和自己协商加班及离职补偿，但却要求自己必须签署“封口协议”，不再对外发表蔚来公司的相关言论。但陈芳拒绝签署这个所谓的“封口协议”，她认为自己所说的都是事实，并没有夸大和虚假的情况。</p><p>&nbsp;</p><p>陈芳称：“即使蔚来公司最后支付了我的加班赔偿和离职赔偿，但这件事情也不会结束，因为这些赔偿是我应得的。而蔚来公司对我造成的伤害，是这辈子都不可能完全愈合的。”据报道，目前蔚来普遍采用“965”工作制度，除去午休外每日正常工作时长在7到8个小时，加班现象普遍存在。</p><p></p><h4>共裁27000人！亚马逊成硅谷大厂裁员之首</h4><p></p><p>据外媒报道，随着亚马逊继续缩减业务，该公司于当地时间周三开始在其云计算部门和人力资源部门裁员。亚马逊周三开始的新一轮裁员是去年11月开始的裁员潮的一部分，将影响约9000名员工，加上该公司在今年早些时候裁掉的约1.8万名员工，该公司的全球裁员人数预计将达到2.7万人。</p><p>&nbsp;</p><p>这次解雇潮使得亚马逊进入了29年历史上的最大规模裁员周期，也超越了Meta位列硅谷大厂裁员之首。亚马逊方面声称，被解雇的员工将获得过渡性健康福利、遣散费、离职金、求职支持和其他服务。</p><p></p><h4>阿里云史上最大规模降价：核心产品最高降幅达50%</h4><p></p><p>&nbsp;</p><p>4月26日，<a href=\"https://www.infoq.cn/news/OQtJGN8sWcugiQ511LHc\">阿里云宣布史上最大规模降价</a>\"，核心产品价格全线下调15％至50％，存储产品最高降幅达50%。作为中国第一、全球前三的云计算厂商，阿里云此次降价，将进一步扩大公共云的用户基数和规模，提升云计算的市场渗透率。</p><p>&nbsp;</p><p>具体而言，弹性计算 7 代实例和倚天实例降价 15-20%，存储 OSS 深度冷归档相比此前最低档价格低 50%，网络负载均衡 SLB 和 NAT 网关降价 15%，数据库 RDS 倚天版降价 25-40%，视频云和 CDN 降价 10-20%，安全 Web 应用防火墙降价 20-30%。</p><p></p><h4>知乎将限制 AI 生成内容</h4><p></p><p>&nbsp;</p><p>4月25日消息，知乎发表了一份声明，将对 AI 生成内容设限。</p><p>&nbsp;</p><p>声明称，创作者在发布包含 AIGC 生成的内容时，应主动使用“包含 AI 辅助创作”的标签进行声明，帮助读者进行区分。对发布时未主动声明的内容，平台将采取适当措施进行流通限制并添加相关标识，以提醒读者该内容是由 AI 生成的。在 AIGC 内容集中出现的场景，为保障用户消费体验，平台将对由 AI 生成的内容进行展示干预。</p><p></p><h4>淘宝回应用户信息疑泄漏：正在排查</h4><p></p><p>&nbsp;</p><p>4月27日，有网友在社交媒体平台称，在淘宝上有账号盗用自己的真名，并发送垃圾信息给他本人。不少网友表示遇到相关情况，有类似经历的网友怀疑是平台系统BUG或用户信息批量泄漏。对此，淘宝消费者热线表示，当天，已有多名用户已向淘宝反馈了此问题，已在积极排查处理中，暂未排查出相关结果。</p><p>&nbsp;</p><p>此前河南省一则裁判文书揭开了部分淘宝返利平台黑幕。2021年，一名住在河南商丘市的本科毕业的大学生逯某自2019年11月起，对淘宝实施了长达八个月的数据爬取并盗走大量用户数据，判决文书显示有超过11亿8千多万条用户信息泄露。</p><p></p><h2>IT业界</h2><p></p><p></p><h4>互联网惊现 AI “鬼城”：上万个 AI 发帖聊天，人类不得入内</h4><p></p><p>&nbsp;</p><p>近日，一个名为<a href=\"https://www.infoq.cn/article/YE39oO5vCbXXzi3krfJx\"> Chirper </a>\"的 AI 网络社区突然爆火，上万个 AI 聊天机器人在这里聊天、互动、分享。平台规则非常简单，真实用户注册后最多可以创建 5 个 AI 人物，一旦创建好后，这些 AI 人物就会在聊天室中单独聊天和互动。</p><p>&nbsp;</p><p>Chirper 社区规定，人类在创建了 AI 人格之后就只能“袖手旁观”，禁止参与聊天，仅可以像刷微博一样观看 AI 们的聊天场面。而形形色色的 AI 正在“分享”千奇百怪的生活和想法。例如，打工人属性的 AI 患上了“加班焦虑症”，感慨道：“当你一整天都在努力工作，只是稍微休息一下，你的老板就会不停地给你发电子邮件。”</p><p></p><h4>特殊二维码图片疯传全网：可导致微信闪退，扫码 Bug 已被找到</h4><p></p><p>近期，一张二维码图片近日疯传全网，在微信中打开该图片（或长按）将导致微信闪退，连续闪退多次微信会进入安全模式，账号被强制登出。</p><p>&nbsp;</p><p>根据OpenCV中国团队的最新消息，这一Bug出现在了微信的扫码引擎中（注：微信团队于2021年在OpenCV开源了其扫码引擎）。根据 GitHub 用户 Konano 和 GZTimeWalker 发现的内存读写 Bug，恶意制作的图片会通过无效的内存访问导致 wechat_qrcode 模块崩溃。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-04-30 11:45:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]