[
  {
    "title": "领导者如何告别责备文化，营造容纳失误、鼓励创新的组织文化",
    "url": "https://www.infoq.cn/article/PrK9S9oUZfeRccQ8yfo9",
    "summary": "<p>Diana Larsen 认为，责备文化是对人类潜力的浪费。当人们的精力都用在避免羞耻和责备上时，就无法完成最好、最有创造性的工作。她认为，要做到无责备领导，需要向学习和保持好奇心的方向转变。这需要建立或恢复与人们之间的信任和可信赖关系。</p><p></p><p>Diana Larsen 在 ScanAgile 2023 大会上发表了关于无责备领导的主题演讲。</p><p></p><p>Larsen 表示，企业在招聘最优秀的人才上投入了大量的时间、精力和金钱，但从实际的角度来看，将这些人带入一个责备盛行的文化中意味着这些投资的浪费：</p><p></p><p></p><blockquote>如果一个人总是担心自己会成为责备的对象，那么他就不可能达到自己的最佳状态。这会分散注意力，让人伤心，是对人类潜力的浪费。</blockquote><p></p><p></p><p>Larsen 提到，当人们感觉到责备即将降临在他们头上时，他们会尽其所能来避免。他们会回避、转移责任、隐藏错误。当责备来临时，他们会因为被认为不称职而感到羞耻。她说，当人们的精力用在这些反应上面时，就无法完成最佳或最有创造力的工作。</p><p></p><p>要想实现无责备领导，就需要转变思维方式，并深刻理解当指责盛行时，每个人都会受到伤害。Larsen 表示，责备的对立面是学习和好奇心。她建议，与其寻求指责，不如寻找导致意外、令人失望的事情的系统性根源，比如交付失误、编码错误或旷工等。</p><p></p><p>Larsen 提到，学习式领导可以表现为领导者在员工向其提出问题时承认自己并不了解所有答案。例如，你可以说：“我不知道该怎么做，让我们一起去找出解决办法！”当出现新的、意想不到的问题时，鼓励员工去探索、保持好奇心和学习。</p><p></p><p>无责备领导的第一步是建立或恢复与人们之间的信任和可信赖关系，正如 Larsen 所说的：</p><p></p><p></p><blockquote>我听到过一个挪威谚语：“他们的肩膀耷拉下来了。”我喜欢这句话。问问员工和团队成员，怎样才能让他们停止紧张地耸肩，以一种更顺畅、更放松、更投入的方式专注于工作。很多时候，我们可以很容易得到答案。</blockquote><p></p><p></p><p>Larsen 建议提出类似这样的问题：“怎样才能在把更多的时间投入到工作中？”、“你目前的工作环境缺少什么？”、“怎样才能让你学到完成团队工作所需的知识？”然后，采纳他们的建议，或解释你为什么不能（解释理由必须充分），并要求他们与你一起努力改进，这对你作为领导者以及他们作为团队成员来说都是更好的做法。</p><p></p><p>InfoQ 就无责备领导的话题对 Diana Larsen 进行了采访。</p><p></p><p>InfoQ：责备是如何成为领导方式中根深蒂固的一部分？</p><p></p><p>Diana Larsen：这是人们和人类系统很早就养成的习惯。回想一下，直到一二十年前，责备和打孩子仍然还是学校和家长公认的做法。现在，大多数人都对这种想法感到恐惧。人们把他们在家里和学校里学到和看到的习惯带到了工作场所。</p><p></p><p>甚至还有一些关于责备的传统格言，好像责备是一种预期行为。“让他们受点压力吧。”、“呆在你自己的泳道里！”、“你为什么不能像某某同事一样？”、“谁对这个错误负责？”还有很多其他的回应，比如“在这里，你必须想办法开脱罪责！”、“低调行事”，等等。员工、经理和企业高层领导都已经习惯了这种模式。</p><p></p><p>InfoQ: 领导者必须培养哪些技能才能实现学习式领导？</p><p></p><p>Larsen：如果他们在软件 /IT 行业工作，就需要培养识别、理解、影响和与复杂系统（技术和人文）协同工作的能力。其他的技能还包括学习领导团队而不是个体贡献者，了解动机的变化和团队环境中的重要因素。将领导者的注意力转移到为团队预期的工作性质创造最佳的工作环境上。</p><p></p><p>大多数软件开发团队在学习工作上花费的时间远远超过应用知识的时间。当工作环境充满不确定性、复杂性、模糊性和快速变化（VUCA）时，他们必须关注新的学习相关技能和了解信息的方法。</p><p></p><p>英文原文：</p><p></p><p><a href=\"https://www.infoq.com/news/2023/12/leading-without-blame/\">https://www.infoq.com/news/2023/12/leading-without-blame/</a>\"</p><p></p>",
    "publish_time": "2023-12-28 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI 的超级对齐团队是在做什么",
    "url": "https://www.infoq.cn/article/AhtiFVSLXkudHJq3XLNd",
    "summary": "<p>OpenAI 不久前宣布了该公司超级对齐团队的第一项成果。这个团队是该公司的一项内部计划的产物，致力于预防一种超级智能体（一种假象的未来计算机，可以比人类更聪明）走向失控。</p><p>&nbsp;</p><p>与该公司的许多公告不同的是，这次的公告并没有包含什么重大突破。在一篇低调的研究论文中，该团队描述了一种技术，可以让一个水平较低的大型语言模型监督一个能力更强大的语言模型，论文声称这可能是向着“弄清楚人类如何监督超人类水平的机器”这一目标迈出的一小步。</p><p>&nbsp;</p><p>此前 OpenAI 陷入了危机，首席执行官 Sam Altman 被监督委员会解雇（这显然是由首席科学家 Ilya Sutskever 领导的政变），三天后他又重新上任。这次的公告距离这桩风波不到一个月，而它传达的信息很明确：公司又回到了正轨，一切如常。</p><p>&nbsp;</p><p>然而 OpenAI 的业务并不寻常。许多研究人员仍然质疑机器是否能够媲美人类的智能水平，更不用说超越人类智能了，但 OpenAI 团队认为机器最终一定会取得优势。“过去几年中人工智能的进步非常快，”超级对齐团队的研究员 Leopold Aschenbrenner 说：“我们不断刷新所有基准测试纪录，而且这种势头有增无减。”</p><p>&nbsp;</p><p>对于 Aschenbrenner 和公司的其他人来说，行业出现具有接近人类能力水平的模型是指日可待的。 “但它不会就此止步，”他说：“我们将拥有超人模型，也就是比我们聪明得多的模型。这样的未来将会带来很多全新的、直击根本的技术挑战。”</p><p>&nbsp;</p><p>7 月，Sutskever 和 OpenAI 科学家 Jan Leike 成立了超级对齐团队来应对这些挑战。 “我这样做是为了我自己的利益，”Sutskever 在 9 月份告诉《麻省理工科技评论》：“我们得保证任何人构建的任何超级智能都不会失控，这一点显然非常重要。”</p><p>&nbsp;</p><p>人们猜测 Altman 因他在公司的人工智能安全策略方面的做法反复无常而被解雇，现在 Sutskever 的超级对齐团队又成了头条新闻。许多人都在期待着，想知道到底发生了什么。</p><p>&nbsp;</p><p></p><h2>该做什么和不该做什么</h2><p></p><p>&nbsp;</p><p>该团队想要回答的问题是如何控制或“调整”假想中的、比我们聪明得多的未来模型，即所谓的超人模型。对齐意味着让模型确保执行你希望它执行的操作，而不执行你不希望它执行的操作。超对齐的理念把这种思想应用到了超人模型上。</p><p>&nbsp;</p><p>用于调整现有模型的一项非常流行的技术称为“通过人类反馈的强化学习”。简而言之，人类测试人员会对模型的反应打分，对他们希望看到的行为投赞成票，对他们不希望看到的行为投反对票。然后这些反馈会被用于训练模型，使其仅产生人类测试人员喜欢的响应类型。这项技术是让 ChatGPT 变得如此吸引人的一个重要原因。</p><p>&nbsp;</p><p>问题在于，这种方法要求人类首先能够辨别什么是理想的行为、什么是不理想的行为。但超人模型这种情况下，模型可能会做出一些人类测试人员无法理解的事情，因此测试人员无法对它们评分。（Sutskever 告诉我们，它甚至可能试图向人类隐藏其真实行为。）</p><p><img src=\"https://static001.geekbang.org/infoq/8b/8bac53e88ab1172dec77d0990f410645.jpeg\" /></p><p>OpenAI 解决超对齐问题的方法</p><p>&nbsp;</p><p>研究人员指出，这个问题很难研究，因为超人机器目前并不存在，所以他们使用了一种替代方法。他们没有研究人类该如何监督超人类机器，而是研究了 OpenAI 五年前发布的模型 GPT-2 该如何监督 OpenAI 最新、最强大的模型 GPT-4。 “如果你能做到这一点，这也许就能证明你可以使用类似的技术来让人类监督超人类模型，”超级对齐团队的另一位研究员 Collin Burns 说。</p><p>&nbsp;</p><p>该团队引入 GPT-2，并训练它执行一些不同的任务，包括一组国际象棋谜题和 22 个评估推理、情感分析等常见自然语言处理测试。他们利用 GPT-2 对这些测试和谜题的反应来训练 GPT-4 执行相同的任务，这就好像让三年级学生教十二年级学生如何完成任务一样。诀窍是在不让 GPT-4 的性能受到太大影响的情况下做到这一点。</p><p>&nbsp;</p><p>结果好坏参半。该团队测量了根据 GPT-2 最佳猜测结果训练的 GPT-4 与根据正确答案训练的 GPT-4 之间的性能差距。他们发现，经过 GPT-2 训练的 GPT-4 在语言任务上比 GPT-2 表现好 20% 到 70%，但在国际象棋难题上表现较差。</p><p>&nbsp;</p><p>团队成员 Pavel Izmailov 表示，GPT-4 完全超越了它的老师，这一事实令人印象深刻：“这是一个非常令人惊讶和积极的结果。”但他说，它远远没有发挥出它自己的潜能。他们的结论是，这种方法很有前景，但还需要做更多的工作。</p><p>&nbsp;</p><p>“这是一个有趣的想法，”德国斯图加特大学从事对齐研究的人工智能研究员 Thilo Hagendorff 说道。但他认为 GPT-2 可能太笨了，无法成为一名好老师。 “GPT-2 往往会对任何稍微复杂或需要推理的任务给出无意义的响应，”他说。 Hagendorff 想知道如果改用 GPT-3 会发生什么事情。</p><p>&nbsp;</p><p>他还指出，这种方法并没有解决 Sutskever 所假设的一种场景，也就是超级智能会隐藏其真实行为，并假装和人类保持一致，虽然它实际上可能已经跑偏了。 “未来的超人模型可能会拥有研究人员也不了解的新兴能力，” Hagendorff 说：“在这些情况下，对齐方法该如何发挥作用呢？”</p><p>&nbsp;</p><p>但他说，指出缺点是很容易的事情。他很高兴看到 OpenAI 开始从猜想转向实验：“我对 OpenAI 的努力表示赞赏。”</p><p>&nbsp;</p><p>OpenAI 现在希望招募其他人加入他们的事业。除了这项研究成果更新之外，该公司还宣布了一项新的 1000 万美元资金计划，计划用于资助从事超级对齐工作的人员。它将向大学实验室、非营利组织和个人研究人员提供高达 200 万美元的赠款，并向研究生提供 15 万美元的一年期奖学金。 “我们对此感到非常兴奋，” Aschenbrenner 说：“我们的确认为新加入的研究人员可以做出很多贡献。”</p><p>&nbsp;</p><p>相关链接：</p><p><a href=\"https://www.technologyreview.com/2023/12/14/1085344/openai-super-alignment-rogue-agi-gpt-4\">https://www.technologyreview.com/2023/12/14/1085344/openai-super-alignment-rogue-agi-gpt-4</a>\"</p><p><a href=\"https://openai.com/blog/introducing-superalignment\">https://openai.com/blog/introducing-superalignment</a>\"</p>",
    "publish_time": "2023-12-28 09:39:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI Agent 在全球化背景下的机遇和挑战 ｜InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/WDwBxN02ZSHlpLZ5vpfF",
    "summary": "<p></p><blockquote>嘉宾｜杨晶生，字节跳动飞书技术Leader特约主持人｜吴少杰，InfoQ社区编辑，高级算法专家</blockquote><p></p><p></p><p>在全球化背景下，AI&nbsp;Agent的发展面临着前所未有的机遇和挑战。作为一种具有高度智能和自主性的技术，AI&nbsp;Agent正在改变着各行各业的运作方式，为人们的生活和工作带来巨大的便利。</p><p></p><p>AI&nbsp;Agent的应用使得跨国企业和全球协作变得更加高效和便捷。通过智能化的交互和自主性的决策，AI&nbsp;Agent能够快速地处理和分析全球范围内的数据和信息，帮助企业做出更加明智的决策，加速全球化进程。然而，随着技术的不断发展和普及，AI&nbsp;Agent也面临着越来越多的挑战和问题。本期《极客有约》，我们邀请到了字节跳动飞书技术Leader&nbsp;杨晶生老师，一同来探讨&nbsp;AI&nbsp;Agent&nbsp;面临的机遇与挑战及未来的发展趋势。</p><p></p><p>InfoQ：各位亲爱的InfoQ的新老朋友，大家晚上好，欢迎来到《极客有约》直播，我是今天的特邀主持人，也是InfoQ社区编辑的吴少杰。目前，我的工作主要涉及垂直行业，专注于大模型和推荐系统相关领域。之前，我在新浪微博和58同城从事推荐和NLP相关工作。</p><p></p><p>在本期直播中，我们有幸邀请到了字节跳动飞书技术Leader，杨晶生老师，为我们分享他在全球化背景下面临的挑战以及与AI&nbsp;Agent相关的经验。杨晶生老师目前负责字节跳动飞书AI方面的研发工作。接下来，请让我们欢迎杨晶生老师，请杨老师与直播间的朋友们打个招呼。</p><p></p><p>杨晶生：我非常荣幸能够参加InfoQ《极客有约》的活动。回顾我的职业发展，最初加入计算机行业时，与吴少杰有些相似，我也是从推荐领域入手的。随着NLP技术的不断成熟，我逐渐转向了NLP，并拓展了一些与语音相关的工作。</p><p></p><p>近年来，我主要在飞书工作，负责AI方面的研发工作。尤其是在今年以来，由于大模型和AI&nbsp;Agent等技术的演进，我们在这个领域做了很多工作。今天，我很高兴能够从行业和技术演变的角度与大家分享一些我个人的学习积累和感受。</p><p></p><p>InfoQ：我了解到杨老师您是在飞书负责AI相关的研发工作，您一直从事与AI相关的工作，想请问下您是如何看待最近两年AIGC引发的技术变革的？</p><p></p><p>杨晶生：最近两年，确切地说是最近一年，整个AI领域的发展迎来了一波新的高潮。回溯过去，上一波高潮可能是在自动驾驶兴起的时候。去年，自动驾驶领域似乎略显停滞，但在ChatGPT之前，我们也看到有一些与CV相关的技术进展，如Midjourney和Stable&nbsp;Diffusion。</p><p></p><p>现在，大家对于AIGC这个词可能有不同的概念，比如GAI、AGI，它们之间存在一些区别。在国外，通常更多地使用GAI这个术语，指的是生成式AI。原先AI主要执行判别式任务，如分类等，从去年开始，生成文字和图像的技术有了巨大的突破。而人工通用智能（AGI）其中G代表通用，特别是OpenAI认为，追求更加通用的人工智能是一个长期的目标。</p><p></p><p>在过去几年里，我一直在飞书从事与AI相关的工作，包括智能纠错等工作。过去，实现这些任务通常需要大量的数据积累和模型训练，每个任务都需要一个独立的模型，这导致了扩展性和可复用性的问题。然而，引入大模型后，最大的差异不仅体现在最终用户体验方面，更在于加速了探索和扩展业务的速度。</p><p></p><p>现在，很多人都在用提示词工程，只需输入几个词、几句话，即可帮助完成任务。与以前需要标注大量数据并进行长时间的训练相比速度快很多。虽然许多大模型在任务上的精度可能不如以前专门设计的模型高，但它们确实使我们能够以非常快的速度尝试许多事情，这也是为什么今年以来AI领域发展迅速的原因之一。</p><p></p><p>从大模型的工作中，我们发现可能有一些“涌现”出的AGI可能性，今天关于Agent这个概念，我们也认为AI&nbsp;Agent可能是AGI早期表现之一。</p><p></p><p>InfoQ：AIGC的火爆也带火了AI&nbsp;Agent，您能先跟我们聊聊到底什么是AI&nbsp;Agent吗？它的定义是什么？</p><p></p><p>杨晶生：关于Agent这个词，目前有很多看法。实际上，这个词的定义并没有一个特别精确的界定。首先，Agent这个概念已经存在很久了，根据一些论文，可以追溯到古希腊或古罗马时代。最初，它指的是能够接受人的指令并完成任务的实体。起初，这个角色主要由人类担任，但随着时间的推移，在蒸汽工业时代，机器也开始扮演这个角色，所以Agent这个概念并不新鲜。</p><p></p><p>然而，在过去的几年中，特别是在大约2001年之后，随着深度学习的引入，人们开始更加关注与人工智能相关的Agent，并将其与图灵测试等概念联系起来。因此，Agent这个词在基于人工智能的背景下引起了更多关注，现在很多人也称之为“智能体”。</p><p></p><p>目前，有很多对于Agent的定义，比如来自OpenAI应用人工智能安全研究负责人翁丽莲（Lilian&nbsp;Weng）的定义。她在自己的博客中提到，我们现在可以更好地使用Agent来实现更多任务。她认为，Agent应该具有记忆规划和工具的概念。在较早的时候，比如去年底，随着GPT模型出现，出现了LangChain的框架，LangChain也对Agent进行了定义。他们认为，如果只是按照必须的步骤使用模型来完成任务，那么这个东西还是机械的。如果让模型决定如何使用工具或API，让模型理解问题并决定如何调用工具，那么这个模块就被称为Agent。</p><p></p><p>最近，OpenAI在DevDay上提出了一个新功能叫做GPTs，提到GPTs能够通过一些配置来构建一个早期Agent。因此，虽然对于Agent有各种不同的定义，但整体概念相似，它首先要通过环境和人类给定的指令获取一些输入，然后通过自身的能力进行记忆、推理，最终使用工具完成给定的任务。在我们的生活中，这实际上在一定程度上能够以一种更加人类可理解的方式进行交流。</p><p></p><h2>AI&nbsp;Agent落地的瓶颈有哪些？</h2><p></p><p></p><p>InfoQ：AI&nbsp;Agent落地的瓶颈在哪里？您了解到的有哪些已经落地的AI&nbsp;Agent可以分享吗？</p><p></p><p>杨晶生：刚刚我们提到了AI&nbsp;Agent的几个方面，其中包括它如何理解指令输入、执行工具以及思考的过程。在谈论这些具体步骤之前，我想提及一个问题，那就是当前对AI&nbsp;Agent的评测非常困难，这也是它在实际应用中面临的一个重要问题。</p><p></p><p>我们目前看到许多新模型的出现，包括一些涉及AI&nbsp;Agent能力的榜单，人们会进行比较，但实际上，即使分数很高，当真正使用时，我们可能会发现它的理解仍然存在一些偏差。这主要是因为评测本身确实非常难以精确执行，目前仍然高度依赖人工评测，并且如何实现更全面的评测覆盖也是一个难题。我认为这可能是当前应用落地中最大的瓶颈之一。</p><p></p><p>关于AI&nbsp;Agent的实现方面的挑战，我认为有几个方面需要考虑。首先是观察，即它如何接受指令和环境输入。目前，很多工作都基于大语言模型，主要以文本为输入。但实际上，要处理图像、语音以及其他传感器数据等多样的输入，是相当困难的。然后执行工具方面，如何安全地执行，也是一个非常难以解决的问题。</p><p></p><p>另一个方面是AI&nbsp;Agent的推理能力，这是目前大语言模型中“涌现”出的一个惊喜。然而，因为它最初主要用于生成任务，现在虽然我们发现它具有一定的推理能力，但在这方面它可能显得有些笨拙。特别是在纠正错误方面，可能变得更加困难。例如，最近流行的AutoGPT框架在一个演示中展示了输入任务并要求制定一套股票购买策略，看起来很华丽，但实际应用时可能遇到一些困难，因为在推理过程中的把控难度较大。</p><p></p><p>综上所述，当前AI&nbsp;Agent在广义场景中的实际应用仍然面临着一些困难。当然，在某些相对狭窄的领域，由于其要求较低，一些具体场景的应用可能会相对顺利。</p><p></p><p>InfoQ：AI&nbsp;Agent能否从专用抵达通用？</p><p></p><p>杨晶生：关于AI&nbsp;Agent，根据我们刚才的讨论，它实际上是为了帮助解决任务。你给它一个任务，然后它会根据手上拥有的工具，自己想出如何使用这些工具来解决任务。因此，它本身具有相当强的领域定制性。</p><p></p><p>Agent的领域性与大语言模型相比，可能在概念上略有不同。大语言模型的领域性体现在其学习到的知识的范围。不同参数量的大模型所能学到的知识量级不同。另外，由于一些数据的隔离性，可能会有一些所谓的行业或领域的大模型。</p><p></p><p>而对于Agent来说，比如，某个Agent的工具可能是用于文档编辑，那么它自然就是文档编辑领域的Agent。如果它的工作是用于指挥机器人完成一些机器人的操作，那么它自然就是关于机器人的Agent。所以，这种Agent本身的领域性是很自然的，即使在虚拟领域，我们也发现给予不同的输入、输出定义，它也能够形成一个具有领域性的Agent。在这方面，一个很好的例子是国内最近启动的项目，称为MetaGPT。这个项目的理念是创建一个虚拟的软件公司，其中有一个虚拟的Agent，即老板。这个Agent的输入是用户的需求，然后它将任务分配给产品经理、研发和测试，然后它定义了不同的产品经理、研发和测试，他们会使用基于语言模型的虚拟工具完成不同的任务。由于定义了不同的工具，因此它可以自然地包含不同的领域模型，而在推理时，我们也可以通过召回不同的背景知识，从而形成具有领域性的Agent。</p><p></p><p>InfoQ：通用AI&nbsp;Agent实现的路径是什么？</p><p></p><p>杨晶生：这是一个极具挑战性的问题。我们刚才已经定义了AI&nbsp;Agent，它的能力取决于所定义的工具，使其自然地成为特定领域的Agent。如果我们反过来看所谓的通用Agent，它不需要特定定义一堆工具，而是可以从用户那里获得一些通用的能力，比如上网和文件编写，并自行完成任务，这实际上是一个极具挑战性的问题，从某种程度上来说，如果达到这个程度，它就已经是我们所谓的AGI，即通用人工智能的一种形式。</p><p></p><p>在回答这个问题时，我们可以稍微再谈一下AGI。前两天我看到DeepMind发表了一篇关于AGI级别的定义。他们的AGI定义与我们之前讨论的相似，但将其分为所谓的狭义AI和通用AI。</p><p></p><p>狭义AI指的是对某一领域非常精通的AI或者Agent，类似于自动驾驶，分为L0到L5，其中L0表示没有，L5表示完全超越人类。有趣的是，在狭义AI列中，L5实际上已经被填满，即在某些领域，例如AlphaFold在发现蛋白质结构方面超越了人类实验。然而，在通用AI的这条线上，他们将ChatGPT仅放在了L1，即“涌现”的阶段，甚至都没有说能够在某个固定的比例上超越人类，这表明了对于通用AI或通用AI&nbsp;Agent的认知。在目前来看，一旦我们涉及通用领域，问题就变得非常困难。</p><p></p><p>谈到实现路径，目前有很多探索，包括中国的智源研究院以及美国的一些机构，大家对于如何走向通用AI或AGI有很多考虑，众说纷纭，因为目前还没有确定的路径，很难说哪条路是正确的。然而，我个人认为MetaAI的负责人杨立昆（Yann&nbsp;LeCun）提出的三个点是比较有帮助的。首先是关于系统的反思能力，即如何让通用AI能够在执行过程中更灵活并及时纠正问题。其次是关于规划和目标的能力，即如何使通用AI能够像人类一样在长期和短期目标之间进行规划。最后一个难点是复杂能力的训练过程，即如何让通用AI在面对非常复杂的任务时能够更好地在训练中体现，而不是仅仅依靠涌现。这些是通用AI或AI&nbsp;Agent的难点，但从实际执行的角度来看，现在全世界都在探索，但还没有人敢说自己一定知道正确的路径。</p><p></p><p>InfoQ：您了解到的有哪些已经落地的AI&nbsp;Agent可以分享吗？</p><p></p><p>杨晶生：正如我们之前所讨论的，垂直领域的Agent由于专精于某个领域，因此它所面对的任务相对明确。目前，这方面已经在多个场景中尝试，并且已经在一些领域取得了显著的成果。其中一个概念是Copilot，由微软提出，在其Office和GitHub平台上得到了应用。Copilot可以看作是一种虚拟的协作工具，尤其是GitHub的Copilot。在编写代码的过程中，它根据上下文来预测接下来的代码，实时指出潜在问题，并辅助生成单元测试等。Copilot在实际应用中表现出色，尤其在在线协作场景下，为任务导向的工作提供了良好的支持。</p><p></p><p>这种趋势不仅体现在办公软件中，如飞书等，而且在设计软件（如Adobe的产品）和销售工具（如Salesforce）中也有所体现。虽然在实践中，人们对Agent在这些流程中到底能够提供多大的帮助还在研究中，但引入Agent的概念在这些环节中应该是有意义的。</p><p></p><p>另一个有趣的方向是Agent在与人类互动中也有一定的平等性。比如近期一些游戏中采用了大语言模型来加强与NPC的对话，使得互动更加丰富，甚至包括一些数字、唇动和动画生成，使得与机器之间的对话更加平等。</p><p></p><p>因此，总体来说，Agent在一些垂直领域中已经发挥了重要作用，即使在大语言模型出现之前也是如此。而随着大语言模型的应用，这一领域的发展速度显著提升。</p><p></p><p>InfoQ：随着大模型多模态能力的提升，您认为多模态会为Agent带来什么？</p><p></p><p>杨晶生：在多模态方面，我认为它非常重要。因为刚刚在讨论&nbsp;Agent&nbsp;面临的难题时，提到了一个关键点，即现实世界或虚拟世界中，并非所有信息都通过文字表达。因此，更好地理解图像和语音，使&nbsp;Agent&nbsp;能够执行更多任务，尤其是在机器人场景下，变得至关重要。</p><p></p><p>目前，机器人的驱动力背后采用的方式已经发生了变化。以前，大多数方法是通过规则或者某些硬件机械的方式来驱动机器人。然而，如今包括&nbsp;DeepMind&nbsp;在内的一些机构，以及国内的一些公司，正致力于利用大模型或&nbsp;Agent&nbsp;来驱动机器人。在这种情况下，仅仅通过文本输入显得力不从心。因此，我们认为在现实世界应用&nbsp;Agent&nbsp;的情境下，多模态处理变得至关重要。</p><p></p><p>InfoQ：在您看来，AI&nbsp;Agent的发展在国内和国外有什么差异吗？您认为什么样的公司能在这场技术变革中跑出来？有哪些机遇是我们可以把握住的？</p><p></p><p>杨晶生：国内和国外在当前全球化市场中的区别在逐渐模糊。在技术领域，国际交流频繁，许多公司也在海外拓展业务。因此，从地理角度看，国内和国外在做许多事情上存在较大的共性，尽管在不同行业和地区，由于用户需求的差异，实际落地时可能存在一些差异，形成了业务上的不同。</p><p></p><p>对于&nbsp;Agent&nbsp;技术而言，我认为差异更多地体现在业务层面，而不是技术路线上。在狭义上来说，尤其是考虑到目前许多&nbsp;Agent&nbsp;的基础是大语言模型，而使用这些模型在合规方面有明确要求。所以，由于在海外&nbsp;GPT&nbsp;是一个合规使用的基座，国外业务整体上可能处于领先地位，推动了该领域能力的发展。国外公司在&nbsp;GPT&nbsp;基础上进行应用的情况相当活跃，很多小公司可能并不涉足模型功能的开发，而是基于&nbsp;GPT&nbsp;进行应用开发。当然，近期随着GPTs和Assistant&nbsp;API的出现，出现了对创业空间压缩的担忧。确实，一些中间层公司在信息和语言模型基础设施方面受到了一些挤压，但我们也能看到许多应用型公司受益颇丰。</p><p></p><p>在国内，虽然可能相对于&nbsp;GPT&nbsp;来说，目前合规的大语言模型基座可能稍显早期，但我们可以看到国内在销售和营销等方面的实际应用上非常领先。特别是在多模态理解方面，国内公司在处理复杂数据理解上做得非常出色。在激烈竞争的市场中，很多公司在复杂数据理解方面的细致工作要比国外公司更为出色。这种对复杂格式数据的深刻理解，对&nbsp;Agent&nbsp;应用的提升有很大帮助。总体而言，无论是与机器人结合还是与办公软件结合，国内外在&nbsp;Agent&nbsp;技术应用方面的共性大于差异。</p><p></p><p>InfoQ：AI&nbsp;Agent的伦理和隐私问题如何解决？我们应该如何规范和引导AI&nbsp;Agent的发展？</p><p></p><p>杨晶生：这个话题涉及到一些宏观层面的问题。坦白说，我个人可能不是专业的伦理、隐私、法律方面的专家，我更愿意从技术和安全的角度谈一些观点。</p><p></p><p>首先，关于隐私问题，最近有一例涉及到&nbsp;GPTs&nbsp;的情况。有人上传了一个文件用于对话生成的资料，但发现其他人通过对话可以获取该文件的下载地址。虽然这引起了一些关注，但我认为这并不是一个特别严重的问题。即便不通过下载文件，通过对话，使用者仍然可以获取文件内容的很多信息。从理论上讲，这更像是一个安全问题，可能会暴露系统的实现，而不是一个侵犯隐私的问题。总的来说，许多隐私问题并不是由&nbsp;AI&nbsp;Agent&nbsp;或者对话式&nbsp;AI&nbsp;引入的，而是系统之前就存在的问题，只是由于引入了&nbsp;AI，使得这些问题更容易引发关注。</p><p></p><p>另一个例子涉及到&nbsp;API&nbsp;的鉴权问题。在一些场景中，用户可能通过对话请求获取其他用户的文档，这需要通过&nbsp;API&nbsp;进行身份验证。解决这类问题并不是仅仅依赖于&nbsp;AI&nbsp;Agent&nbsp;的技术设计，而更需要在&nbsp;API&nbsp;设计阶段做好权限管理，确保用户只能访问其有权限的内容。</p><p></p><p>关于伦理问题和规范引导，当&nbsp;AI&nbsp;Agent&nbsp;代替用户执行操作时，责任归属变得复杂。例如，在法律助手的场景中，如果&nbsp;AI&nbsp;理解错误导致合同出现问题，责任究竟是用户还是系统的提供方？这是一个复杂的问题，目前在法律专家中也存在一些争议。从我个人的看法来看，AI&nbsp;应该被看作是一种辅助工具，而在生成内容之后，我们需要进行一些确认环节来规避潜在的问题。</p><p></p><p>此外，有一些实验性的工作涉及到多个&nbsp;AI&nbsp;Agent&nbsp;之间的互动，例如斯坦福创建的虚拟小镇项目，小镇上25个Agent互相驱动，对话并产生故事。再比如在类似微博的平台上，所有发言都是由&nbsp;AI&nbsp;生成的。这引发了一些关于创作权和责任的问题。如果某人不知道发言者是&nbsp;AI，而将其当作真实的言论，责任归属于谁？这些问题仍然需要深入探讨。</p><p></p><p>总的来说，随着技术的发展，我们可能需要不断探索并适应，先进行实验性的应用，发现问题后再进行修正和规范。这也许是一个逐步认知的过程。</p><p></p><h2>AI&nbsp;Agent的发展趋势和前景</h2><p></p><p></p><p>InfoQ：未来，AI&nbsp;Agent的发展趋势和前景是什么？您看好AI&nbsp;Agent未来的发展吗？您认为多久我们会迎来AI&nbsp;Agent的大规模落地？</p><p></p><p>杨晶生：对于垂直领域，特别是随着大语言模型的不断改进和多模态语言模型的发展，我对未来感到比较乐观。随着这些技术的进步，我们可以期望它们能够更全面地理解知识，特别是在一些协作平台上充当助手的角色。尽管整个行业普遍认为这仍处于早期阶段，但我个人认为，作为助手或者在特定垂直领域的智能代理，在不久的将来，AI助手或代理可能会更多地采用对话式进行交互，而不是以前更倾向于依赖图形用户界面（GUI）的方式。</p><p></p><p>然而，对于所谓的通用Agent，目前还很难预测。尽管大家普遍认为它在加速发展，但我们回顾一下之前的自动驾驶技术，它也曾经经历了一段加速发展的阶段，但后来也遇到了一些瓶颈。</p><p></p><p>通用性意味着Agent能够非针对性的胜任各种任务，你让它针对性的精通100个任务甚至1万个任务，可能都不算通用，实现真正的通用性是非常困难的。尽管我们相信这个领域正在加速发展，但真正的通用人工智能何时实现，我们只能拭目以待。</p><p></p><p>InfoQ：对于想要进入这个领域的公司或个人来说，需要了解哪些相关知识？您有什么意见给到这些人吗？</p><p></p><p>杨晶生：关于这个问题，我的观点可能更倾向于开发者或者初创公司的视角。投资机构和孵化器在创业投资时不仅需要考虑技术和用户，更需要考虑商业层面的因素，我就不细讲了。</p><p></p><p>从个人和项目的角度出发，我将其分为两个层面。</p><p></p><p>第一个层面，当我们基于当前的基础设施，也就是大语言模型或多模态模型来进行应用开发的时候，我们必须对通过这些AI能够解决问题保持信心，这可能需要一些fine-tuning、提示词工程或者进行更底层的模型工作。比如像GPT&nbsp;3.5这样的模型，在最初接触时可能感觉它不那么“聪明”，但我们仍然需要相信通过大语言能够解决问题，而不是用太多的策略妥协。在这方面，我们需要相信这些模型能够从“婴儿状态”进化为“小学生状态”，能够理解并遵循指令，尽管可能需要通过思维链等方式来逐步引导。</p><p></p><p>第二个层面是关于做应用的算法工作者的思维方式。与做工程开发不同，算法工作者更加注重评测。评测在整个过程中至关重要，不仅仅是为了调通，更是为了指导优化和迭代。这种思维方式的改变可能是很大的，需要更深入地理解业务，拆解需求为算法任务并制定指标。最终，理解业务比纯粹的技术能力更为重要。</p><p></p><p>另外，对于那些转入AI应用领域的开发者来说，深入了解机器学习的基础知识和一些计算学的知识可也能是有益的。同样，对于算法研究出身的人，补充一些工程和业务理解的能力也是必要的。总体而言，尽管我们要解决的任务没有发生变化，但如何有效地利用技术来加速这一过程变得更为关键。</p><p></p><p>InfoQ：有观众提出一个问题，是否可以对Agent的Benchmark进行分级，对于这个思路的可行性，我想进行一些讨论。</p><p></p><p>杨晶生：我认为这个思路在理论上是合理的，就像自动驾驶技术一样，它最初也经历了从L1到L5的分级。这位同学提出的建议非常不错，分层评测在不同的阶段可能会更为有益。</p><p></p><p>然而，从实际评测的角度来看，特别是对于通用Agent的评测，目前存在一些挑战。首先，许多评测方法已经为大家所熟知，容易受到过拟合的影响。虽然大多数从业者可能不会真的将评测数据直接训练到模型中，但由于对评测问题的关注，模型在实际训练中可能会有所偏向。我认为评测的同时，我们也需要关注模型在实际业务中的表现，就像自动驾驶技术一样。我们需要观察模型在真实业务环境中的反馈指标，这同样重要。</p><p></p><p>InfoQ：下一位小伙伴可能在金融行业，他想了解下在金融垂直领域方面，开发工作需要涉及哪些方面？考虑到金融行业的竞争可能较激烈，你对于在这个领域从事算法开发有何建议？</p><p></p><p>杨晶生：在金融领域，尽管我个人没有深入从事相关工作，但通过我的理解，我认为金融领域也涉及协作系统。这一部分与其他领域相似，首先要充分利用当前的工具，如Copilot，以提高日常工作效率。对于程序员而言，这可能涉及到处理代码，而对于从事文案工作的人，则可以利用各种文案助手。</p><p></p><p>进一步而言，如果涉及到金融科技，例如量化领域或投资顾问方面，人工智能在这个领域一直备受关注。在金融科技领域，尤其是量化和投资方面，行业内也曾尝试过一些执行策略。这些方法可能是值得尝试的，但在涉及实际交易时，务必保持谨慎。</p><p></p><p></p>",
    "publish_time": "2023-12-28 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "领导者如何告别责备，营造容纳失误、鼓励创新的组织文化",
    "url": "https://www.infoq.cn/article/PrK9S9oUZfeRccQ8yfo9",
    "summary": "<p>Diana Larsen 认为，责备文化是对人类潜力的浪费。当人们的精力都用在避免羞耻和责备上时，就无法完成最好、最有创造性的工作。她认为，要做到无责备领导，需要向学习和保持好奇心的方向转变。这需要建立或恢复与人们之间的信任和可信赖关系。</p><p></p><p>Diana Larsen 在 ScanAgile 2023 大会上发表了关于无责备领导的主题演讲。</p><p></p><p>Larsen 表示，企业在招聘最优秀的人才上投入了大量的时间、精力和金钱，但从实际的角度来看，将这些人带入一个责备盛行的文化中意味着这些投资的浪费：</p><p></p><p></p><blockquote>如果一个人总是担心自己会成为责备的对象，那么他就不可能达到自己的最佳状态。这会分散注意力，让人伤心，是对人类潜力的浪费。</blockquote><p></p><p></p><p>Larsen 提到，当人们感觉到责备即将降临在他们头上时，他们会尽其所能来避免。他们会回避、转移责任、隐藏错误。当责备来临时，他们会因为被认为不称职而感到羞耻。她说，当人们的精力用在这些反应上面时，就无法完成最佳或最有创造力的工作。</p><p></p><p>要想实现无责备领导，就需要转变思维方式，并深刻理解当指责盛行时，每个人都会受到伤害。Larsen 表示，责备的对立面是学习和好奇心。她建议，与其寻求指责，不如寻找导致意外、令人失望的事情的系统性根源，比如交付失误、编码错误或旷工等。</p><p></p><p>Larsen 提到，学习式领导可以表现为领导者在员工向其提出问题时承认自己并不了解所有答案。例如，你可以说：“我不知道该怎么做，让我们一起去找出解决办法！”当出现新的、意想不到的问题时，鼓励员工去探索、保持好奇心和学习。</p><p></p><p>无责备领导的第一步是建立或恢复与人们之间的信任和可信赖关系，正如 Larsen 所说的：</p><p></p><p></p><blockquote>我听到过一个挪威谚语：“他们的肩膀耷拉下来了。”我喜欢这句话。问问员工和团队成员，怎样才能让他们停止紧张地耸肩，以一种更顺畅、更放松、更投入的方式专注于工作。很多时候，我们可以很容易得到答案。</blockquote><p></p><p></p><p>Larsen 建议提出类似这样的问题：“怎样才能在把更多的时间投入到工作中？”、“你目前的工作环境缺少什么？”、“怎样才能让你学到完成团队工作所需的知识？”然后，采纳他们的建议，或解释你为什么不能（解释理由必须充分），并要求他们与你一起努力改进，这对你作为领导者以及他们作为团队成员来说都是更好的做法。</p><p></p><p>InfoQ 就无责备领导的话题对 Diana Larsen 进行了采访。</p><p></p><p>InfoQ：责备是如何成为领导方式中根深蒂固的一部分？</p><p></p><p>Diana Larsen：这是人们和人类系统很早就养成的习惯。回想一下，直到一二十年前，责备和打孩子仍然还是学校和家长公认的做法。现在，大多数人都对这种想法感到恐惧。人们把他们在家里和学校里学到和看到的习惯带到了工作场所。</p><p></p><p>甚至还有一些关于责备的传统格言，好像责备是一种预期行为。“让他们受点压力吧。”、“呆在你自己的泳道里！”、“你为什么不能像某某同事一样？”、“谁对这个错误负责？”还有很多其他的回应，比如“在这里，你必须想办法开脱罪责！”、“低调行事”，等等。员工、经理和企业高层领导都已经习惯了这种模式。</p><p></p><p>InfoQ: 领导者必须培养哪些技能才能实现学习式领导？</p><p></p><p>Larsen：如果他们在软件 /IT 行业工作，就需要培养识别、理解、影响和与复杂系统（技术和人文）协同工作的能力。其他的技能还包括学习领导团队而不是个体贡献者，了解动机的变化和团队环境中的重要因素。将领导者的注意力转移到为团队预期的工作性质创造最佳的工作环境上。</p><p></p><p>大多数软件开发团队在学习工作上花费的时间远远超过应用知识的时间。当工作环境充满不确定性、复杂性、模糊性和快速变化（VUCA）时，他们必须关注新的学习相关技能和了解信息的方法。</p><p></p><p>英文原文：</p><p></p><p><a href=\"https://www.infoq.com/news/2023/12/leading-without-blame/\">https://www.infoq.com/news/2023/12/leading-without-blame/</a>\"</p><p></p>",
    "publish_time": "2023-12-28 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]