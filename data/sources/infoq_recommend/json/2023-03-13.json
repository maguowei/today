[
  {
    "title": "Java 近期新闻：NetBeans 17、Spring 及 Tomcat 多项更新、JDk 20 版本 GraalVM",
    "url": "https://www.infoq.cn/article/UbJ7lV4OWYjY7UN4JGBD",
    "summary": "<p></p><h2>OpenJDK</h2><p></p><p></p><p>微软资深程序经理&nbsp;<a href=\"https://www.linkedin.com/in/gdams/\">George Adams</a>\"&nbsp;于上周<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-February/007379.html\">提出</a>\"&nbsp;JEP 草案 8303167：<a href=\"https://openjdk.org/jeps/8303167\">弃用 Windows x86-32 端口</a>\"。该功能 JEP 提议将 Windows x86-32 端口弃用，并在未来的版本发布中将其移除。<a href=\"https://openjdk.org/jeps/436\">虚拟线程（第二预览版）</a>\"无意在32位平台中实施 JEP 436，移除对该端口支持将加快 OpenJDK 开发人员对新功能的开发速度。</p><p></p><h2>JDK 20</h2><p></p><p></p><p>JDK 20 仍处于<a href=\"https://openjdk.java.net/jeps/3#rc\">候选发布版本</a>\"阶段，预计将于2023年3月21日进行 GA 发布。<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B36\">Build 36</a>\"&nbsp;仍基于当前 JDK 20的<a href=\"https://jdk.java.net/20/\">早期访问版本</a>\"构建。关于该版本更多细节请参见<a href=\"https://jdk.java.net/20/release-notes\">发布说明</a>\".</p><p></p><h2>JDK 21</h2><p></p><p></p><p>JDK 21&nbsp;<a href=\"https://jdk.java.net/21/\">早期访问版本</a>\"&nbsp;的&nbsp;<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B11\">Build 11</a>\"&nbsp;已于上周可用，提供对 Build 10 的<a href=\"https://github.com/openjdk/jdk/compare/jdk-21%2B10...jdk-21%2B11\">更新</a>\"及多项问题<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2021%20and%20%22resolved%20in%20build%22%20%3D%20b11%20order%20by%20component%2C%20subcomponent\">修复</a>\"。关于该版本更多细节请参见<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p></p><p>关于&nbsp;<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"&nbsp;和&nbsp;<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\"，欢迎开发者至&nbsp;<a href=\"https://bugreport.java.com/bugreport/\">Java 问题库</a>\"中反馈问题。</p><p></p><h2>GraalVM</h2><p></p><p></p><p>首个基于 JDK 20 的 GraalVM 开发者构建目前已对 Java 社区<a href=\"https://twitter.com/graalvm/status/1628031945522266112?cxt=HHwWgIC-vbib95ctAAAA\">开放使用</a>\"。关于更多细节请参见<a href=\"https://github.com/graalvm/graalvm-ce-dev-builds/releases/tag/23.0.0-dev-20230224_0257\">发布说明</a>\"，欢迎开发者们前往<a href=\"https://www.graalvm.org/docs/getting-started/\">新手指引</a>\"页面。</p><p></p><p>Oracle 创新实验室同样<a href=\"https://twitter.com/graalvm/status/1627982736173457408?cxt=HHwWgICzoYrr4JctAAAA\">宣布</a>\"即将发布的 GraalVM 23.0 版本将支持&nbsp;<a href=\"https://github.com/oracle/graal/issues/5050\">HotSpot 垃圾回收算法</a>\"，意图在 HotSpot 虚拟机中添加所需的&nbsp;<a href=\"https://wiki.openjdk.org/display/zgc/Main\">Z 垃圾回收</a>\"（ZGC）屏障及性能优化。</p><p></p><h2>Eclipse GlassFish</h2><p></p><p></p><p>GlassFish 7.0.2 现已<a href=\"https://github.com/eclipse-ee4j/glassfish/releases/tag/7.0.2\">发布</a>\"，提供对 Eclipse 项目的集成，包括：Jersey 3.1.1、Mojarra 4.0.1、Eclipselink 4.0.1-RC2、WaSP 3.2.0、Tyrus 2.1.3，及 Angus 2.0.1。其他优化项包括：修复程序代码请求 ClassLoader 下目录时导致的故障，对&nbsp;<a href=\"https://github.com/eclipse-ee4j/glassfish/blob/f8a179444fe52196997a02148206f5db769b59f2/nucleus/glassfish-jul-extension/src/test/java/org/glassfish/main/jul/GlassFishLogManagerLifeCycleTest.java\">GlassFishLogManagerLifeCycleTest</a>\"&nbsp;类提供稳定性；退出时显式移除临时目录；对&nbsp;glassfish-external-tck-pages-tags&nbsp;artifact 提供技术兼容套件（TCK）运行器。</p><p></p><h2>Spring 框架</h2><p></p><p></p><p>本周内 Spring 甚是忙碌，针对其下各类<a href=\"https://spring.io/projects\">项目</a>\"提供了多项点发布及里程碑发布。</p><p></p><p>Spring Boot 3.0.3 现已<a href=\"https://spring.io/blog/2023/02/23/spring-boot-3-0-3-available-now\">发布</a>\"，针对文档及依赖升级提供多项问题修复，包括：Spring Data 2022.0.2、Spring 安全 6.0.2、Spring Batch 5.0.1、Micrometer 1.10.4、Netty 4.1.89.Final、Hibernate 6.1.7.Final，以及 Glassfish JAXB 4.0.2。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v3.0.3\">发布说明</a>\"。</p><p></p><p>Spring Boot 2.7.9 版本的<a href=\"https://spring.io/blog/2023/02/23/spring-boot-2-7-9-available-now\">发布</a>\"同样提供针对文档及依赖升级提供多项问题修复，包括：Spring Data 2021.2.8、Spring Security 5.7.7、Spring Batch 4.3.8、Micrometer 1.9.8、Netty 4.1.89.Final、Hibernate 5.6.15.Final，以及 Glassfish JAXB 2.3.8。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v2.7.9\">发布说明</a>\"。</p><p></p><p>展望版本 3.1.0，Spring Boot 的<a href=\"https://spring.io/blog/2023/02/24/spring-boot-3-1-0-m1-available-now\">首个里程碑版本</a>\"现已发布。该版本中提供问题修复、优化文档、依赖升级，以及全新功能，包括：移除对Apache HttpClient 4 的依赖管理，后者现已迁移至 HttpClient 5；为Apache Tomcat 及 Jetty 添加最大 HTTP 响应头大小配置；支持 Spring 对Apache Kafka的&nbsp;<a href=\"https://docs.spring.io/spring-kafka/docs/current/api/org/springframework/kafka/config/ContainerCustomizer.html\">ContainerCustomizer</a>\"&nbsp;接口；自动配置默认&nbsp;<a href=\"https://docs.spring.io/spring-kafka/docs/current/api/org/springframework/kafka/config/ConcurrentKafkaListenerContainerFactory.html\">ConcurrentKafkaListenerContainerFactory</a>\"&nbsp;类中&nbsp;<a href=\"https://docs.spring.io/spring-kafka/docs/current/api/org/springframework/kafka/listener/BatchInterceptor.html\">BatchInterceptor</a>\"&nbsp;接口。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v3.1.0-M1\">发布说明</a>\"。</p><p></p><p><a href=\"https://spring.io/projects/spring-batch\">Spring Batch</a>\"&nbsp;的 5.0.1 及 4.3.8 版本现已<a href=\"https://spring.io/blog/2023/02/23/spring-batch-5-0-1-and-4-3-8-available-now\">发布</a>\"，提供问题修复、依赖升级，文档相关优化及对应的依赖升级版本包括：Spring 框架 6.0.5 及对应依赖版本 5.3.25；Spring Retry 2.0.0 及对应依赖版本 1.3.4；Spring AMQP 3.0.2 及对应依赖版本 2.3.16；Spring Data 3.0.2 及对应依赖版本 2.7.8；Spring Integration 6.0.3 及对应依赖版本 5.5.16；Micrometer 1.10.4 及对应依赖版本 1.9.8。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-batch/releases/tag/v5.0.1\">版本 5.0.1</a>\"&nbsp;及<a href=\"https://github.com/spring-projects/spring-batch/releases/tag/4.3.8\">版本 4.3.8</a>\"&nbsp;的发布说明。</p><p></p><p><a href=\"https://spring.io/projects/spring-data\">Spring Data</a>\"&nbsp;版本 2023.0.0-M2、2022.0.2，以及 2021.2.8 现已<a href=\"https://spring.io/blog/2023/02/20/spring-data-2023-0-0-m2-2022-0-2-and-2021-2-8-released\">发布</a>\"，提供问题修复及对应的依赖版本升级，包括 Spring Data Commons 3.1.0-M2、3.0.2 及 2.7.8; Spring Data MongoDB 4.1.0-M2、4.0.2 及 3.4.8; Spring Data Elasticsearch 5.1.0-M2、5.0.2 及 4.4.8。Spring Data 版本 2023.0.0-M1 同样于上周发布，但发布工具出现故障，Spring Data 2023.0.0-M2（代号 Ullman）解决了该问题。Spring Data 版本 2022.0.2 及 2021.2.8 可分别被&nbsp;<a href=\"https://spring.io/projects/spring-boot\">Spring Boot</a>\"&nbsp;3.0.3 及 2.7.9 使用。</p><p></p><p><a href=\"https://spring.io/projects/spring-security\">Spring Security</a>\"&nbsp;版本 6.0.2、5.8.2 及 5.7.7 现已<a href=\"https://spring.io/blog/2023/02/21/spring-security-6-0-2-5-8-2-and-5-7-7-available-now\">发布</a>\"，提供问题修复及依赖升级。版本 6.0.2 中新功能包括：重新启用对&nbsp;<a href=\"https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/oauth2/client/R2dbcReactiveOAuth2AuthorizedClientService.html\">R2dbcReactiveOAuth2AuthorizedClientService</a>\"&nbsp;类的测试；<a href=\"https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/web/csrf/CsrfTokenRequestAttributeHandler.html\">CsrfTokenRequestAttributeHandler</a>\"&nbsp;类的文档中应写明&nbsp;<a href=\"https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/web/csrf/XorCsrfTokenRequestAttributeHandler.html\">XorCsrfTokenRequestAttributeHandler</a>\"&nbsp;类</p><p>为默认使用；版本 5.8.2 中新功能包括：新增用于验证由&nbsp;XorCsrfTokenRequestAttributeHandler&nbsp;类掩码 CSRF 令牌的&nbsp;<a href=\"https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/messaging/web/csrf/XorCsrfChannelInterceptor.html\">XorCsrfChannelInterceptor</a>\"&nbsp;类；文档中注明&nbsp;<a href=\"https://docs.spring.io/spring-security/site/docs/current/api/org/springframework/security/config/annotation/web/reactive/EnableWebFluxSecurity.html\">@EnableWebFluxSecurity</a>\"&nbsp;注解将在版本 6.0 中需要&nbsp;@Configuration&nbsp;注解。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-security/releases/tag/6.0.2\">版本 6.0.2</a>\"、<a href=\"https://github.com/spring-projects/spring-security/releases/tag/5.8.2\">版本 5.8.2</a>\"、<a href=\"https://github.com/spring-projects/spring-security/releases/tag/5.7.7\">版本 5.7.7</a>\"&nbsp;的发布说明。</p><p></p><p>Spring Cloud 版本 2021.0.6 的<a href=\"https://spring.io/blog/2023/02/24/spring-cloud-2021-0-6-has-been-released\">发布</a>\"提供对子项目的升级，包括：Spring Cloud Config 3.1.5、Spring Cloud Kubernetes 2.1.5、Spring Cloud Build 3.1.5、Spring Cloud OpenFeign 3.1.5，以及 Spring Cloud Function 3.2.8。关于该版本更多细节请参见<a href=\"https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2021.0-Release-Notes\">发布说明</a>\"。</p><p></p><p><a href=\"https://spring.io/projects/spring-graphql\">Spring for GraphQL</a>\"&nbsp;版本 1.1.2 现已<a href=\"https://spring.io/blog/2023/02/21/spring-for-graphql-1-1-2-released\">发布</a>\"，提供问题修复、文档优化、依赖升级，以及全新功能，包括：允许通过&nbsp;<a href=\"https://docs.spring.io/spring-graphql/docs/current/api/org/springframework/graphql/test/tester/ExecutionGraphQlServiceTester.html\">ExecutionGraphQlServiceTester</a>\"接口对&nbsp;<a href=\"https://javadoc.io/doc/com.graphql-java/graphql-java/latest/graphql/ExecutionInput.html\">ExecutionInput</a>\"&nbsp;类进行配置；ContextDataFetcherDecorator&nbsp;在“subscription”类型重命名后忽略订阅；支持方法参数中的&nbsp;@Validated。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-graphql/releases/tag/v1.1.2\">发布说明</a>\"。</p><p></p><p><a href=\"https://spring.io/projects/spring-authorization-server\">Spring 授权服务器</a>\"版本 1.0.1 及 0.4.1 现已<a href=\"https://spring.io/blog/2023/02/22/spring-authorization-server-1-0-1-and-0-4-1-available-now\">发布</a>\"，提供问题修复及对应依赖升级，包括：Spring 框架 6.0.5 及 5.3.25；Spring Security 6.0.2 及 5.8.2；JUnit Jupiter 5.9.2；Jackson 2.14.2。版本 0.4.1 中提供的新功能包含对&nbsp;continuous-integration-workflow.yml&nbsp;文件的更新，使用<a href=\"https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#environment-files\">环境文件</a>\"而非已废弃的&nbsp;set-output&nbsp;命令。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-authorization-server/releases/tag/1.0.1\">版本 1.0.1</a>\"&nbsp;及<a href=\"https://github.com/spring-projects/spring-authorization-server/releases/tag/0.4.1\">版本 0.4.1</a>\"&nbsp;的发布说明。</p><p></p><p>展望版本 1.1.0，Spring 授权服务器的<a href=\"https://spring.io/blog/2023/02/22/spring-authorization-server-1-1-0-m1-available-now\">首个里程碑版本</a>\"目前也已发布，提供对 OpenID Connect 1.0 登出端点的支持。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects/spring-authorization-server/releases/tag/1.1.0-M1\">发布说明</a>\"。</p><p></p><p>Spring Modulith 版本 0.4.0 现已<a href=\"https://spring.io/blog/2023/02/24/spring-modulith-0-4-released\">发布</a>\"，提供问题修复、依赖升级，以及全新功能，包括：支持集成测试应用程序模块场景，尤其针对使用&nbsp;@ApplicationEventListener&nbsp;注解的情况；更为详尽的 Modulith 启动时未完成事件发布日志查询；在文档生成时使用系统名作为容器名称。关于该版本更多细节请参见<a href=\"https://github.com/spring-projects-experimental/spring-modulith/releases/tag/0.4.0\">发布说明</a>\"。</p><p></p><p>版本 0.4.1 在<a href=\"https://spring.io/blog/2023/02/24/spring-modulith-0-4-1-released\">发布</a>\"中解决了&nbsp;spring-modulith-runtime&nbsp;模块中导致网页应用在 Modulith 关闭时挂起的小问题，同时也将&nbsp;<a href=\"https://github.com/xmolecules/jmolecules/blob/main/readme.adoc\">jMolecules</a>\"&nbsp;的依赖版本升级至 2022.2.3。关于该版本的更多细节请参见<a href=\"https://github.com/spring-projects-experimental/spring-modulith/releases/tag/0.4.1\">发布说明</a>\"。</p><p></p><h2>Helidon</h2><p></p><p></p><p>Helidon 4.0.0&nbsp;<a href=\"https://github.com/helidon-io/helidon/releases/tag/4.0.0-ALPHA5\">第五 Alpha 版本</a>\"已发布，变更包括：支持中断 HTTP/2 连接，从而更为高效关闭服务器；可通过&nbsp;<a href=\"https://helidon.io/docs/v3/apidocs/io.helidon.integrations.oci.metrics/io/helidon/integrations/oci/metrics/OciMetricsSupport.Builder.html\">OciMetricsSupport.Builder</a>\"&nbsp;类中新增的&nbsp;enabled()&nbsp;方法，注册&nbsp;<a href=\"https://helidon.io/docs/v3/apidocs/io.helidon.integrations.oci.metrics/io/helidon/integrations/oci/metrics/OciMetricsSupport.html\">OciMetricsSupport</a>\"&nbsp;类的示例；更新&nbsp;<a href=\"https://helidon.io/docs/v3/apidocs/io.helidon.media.multipart/io/helidon/media/multipart/BodyPart.html\">BodyPart</a>\"&nbsp;接口，现可返回一个&nbsp;<a href=\"https://docs.oracle.com/en/java/javase/19/docs/api/java.base/java/util/Optional.html\">Optional</a>\"&nbsp;实例而非可空字符串。</p><p></p><h2>Apache 软件基金会</h2><p></p><p></p><p>Apache 软件基金会分别提供了&nbsp;<a href=\"https://netbeans.apache.org/\">NetBeans</a>\"、<a href=\"https://tomcat.apache.org/\">Tomcat</a>\"&nbsp;及&nbsp;<a href=\"https://logging.apache.org/log4j/2.x/\">Log4j 2</a>\"&nbsp;的版本发布。</p><p>Apache NetBeans 17 的<a href=\"https://blogs.apache.org/netbeans/\">发布</a>\"中包含以下变更：将大文件告警从 1MB 提高至 5MB；提供&nbsp;<a href=\"https://www.antlr.org/\">ANTLR4</a>\"&nbsp;词库的初始化实现；依赖升级至 ATNLR4 运行时 4.11.1；支持 Jakarta EE 10。关于该版本更多细节可参见<a href=\"https://github.com/apache/netbeans/releases/tag/17\">发布说明</a>\"。</p><p></p><p>Apache Tomcat 的版本 11.0.0 即<a href=\"https://www.mail-archive.com/announce@apache.org/msg07982.html\">第三里程碑版本（Alpha）</a>\"发布中，提供了以下重要变更：将可支持最低 Java 版本提高至 JDK 17；移除对安全管理模式下启动 Tomcat 的支持；移除对 JAX-RPC 的支持，其在 Jakarta EE 9 中已被从 Jakarta EE 平台中移除。需要注意的是，随着 Jakarta EE 11 的发展，对最低 Java 版本的支持很可能会被提高至 JDK 21。关于该版本的更多细节请参见<a href=\"https://tomcat.apache.org/tomcat-11.0-doc/changelog.html\">发布说明</a>\"。</p><p></p><p>Apache Tomcat 10.1.6 版本<a href=\"https://www.mail-archive.com/announce@apache.org/msg07993.html\">发布</a>\"，主要变更包括：改用 ServiceLoader 机制加载 Tomcat 所使用的自定义URL协议处理；更新Apache Tomcat 原生库打包版本，以捕获 Windows 使用 OpenSSL 3.0.8 所构建的二进制文件；将 RFC 6598 所指定的，<a href=\"https://www.rfc-editor.org/rfc/rfc6598\">IANA 为共享地址空间预留 IPv4 前缀</a>\"，即共享 IP 地址空间（100.64.0.0/100.0.0.10），新增至&nbsp;<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/api/org/apache/catalina/valves/RemoteIpValve.html\">RemoteIpValve</a>\"&nbsp;及&nbsp;<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/api/org/apache/catalina/filters/RemoteIpFilter.html\">RemoteIpFilter</a>\"&nbsp;类的受信任代理列表中；在 Tomcat 启动时，记录每个配置的 TLS 证书基本信息；对示例网页应用访问默认限制为本地 localhost。关于该版本更多信息请参见<a href=\"http://tomcat.apache.org/tomcat-10.1-doc/changelog.html\">发布说明</a>\"。</p><p></p><p>Apache Tomcat 的版本&nbsp;<a href=\"https://www.mail-archive.com/announce@apache.org/msg07981.html\">9.0.72</a>\"&nbsp;及版本&nbsp;<a href=\"https://www.mail-archive.com/announce@apache.org/msg07992.html\">8.5.86</a>\"&nbsp;主要提供以下变更：新增错误报告变量，允许重新定向至外部 Web 服务器，或从外部 Web 服务器代理；将 RFC 6598 所指定的，<a href=\"https://www.rfc-editor.org/rfc/rfc6598\">IANA 为共享地址空间预留 IPv4 前缀</a>\"，即共享 IP 地址空间（100.64.0.0/100.0.0.10），新增至&nbsp;<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/api/org/apache/catalina/valves/RemoteIpValve.html\">RemoteIpValve</a>\"&nbsp;及&nbsp;<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/api/org/apache/catalina/filters/RemoteIpFilter.html\">RemoteIpFilter</a>\"&nbsp;类的受信任代理列表中；在 Tomcat 启动时，记录每个配置的 TLS 证书基本信息；对示例网页应用访问默认限制为本地 localhost。关于该版本更多细节请参见<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/changelog.html\">版本 9.0.72</a>\"&nbsp;及<a href=\"https://tomcat.apache.org/tomcat-8.5-doc/changelog.html\">版本 8.0.86</a>\"&nbsp;的发布说明。</p><p></p><p>由于 SLF4J 绑定中的兼容性问题，Apache Log4j 版本 2.20.0 的<a href=\"https://www.mail-archive.com/announce@apache.org/msg07977.html\">发布</a>\"对 Log4j 提供两个版本的 SLF4J 适配器。其他新功能包括：新增对&nbsp;RollingFileAppender&nbsp;日期格式的时区支持；新增&nbsp;KafkaAppender&nbsp;中&nbsp;ProducerRecord&nbsp;的&nbsp;LogEvent&nbsp;时间戳；新增&nbsp;<a href=\"https://logging.apache.org/log4j/2.x/javadoc/log4j-api/org/apache/logging/log4j/LogBuilder.html\">LogBuilder</a>\"&nbsp;接口中&nbsp;logAndGet()&nbsp;方法，用于模拟&nbsp;<a href=\"https://logging.apache.org/log4j/2.x/javadoc/log4j-api/org/apache/logging/log4j/Logger.html\">Logger</a>\"&nbsp;方法中&nbsp;traceEntry()&nbsp;方法。</p><p></p><h2>JHipster</h2><p></p><p></p><p>JHipster 团队现已<a href=\"https://twitter.com/pascalgrimaud/status/1629123246510469126?cxt=HHwWjMCzucK955stAAAA\">发布</a>\"&nbsp;JHipster Lite 版本 0.28.0，提供优化如：支持 Cassandra 模式迁移；重构&nbsp;CustomClaimConverter&nbsp;类，以强化安全性；Spring Boot 依赖升级至 3.0.3。关于该版本的更多细节请参见<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.28.0\">发布说明</a>\"。</p><p></p><p>该团队同时<a href=\"https://twitter.com/deepu105/status/1628105207400759308?cxt=HHwWmICwgezDmJgtAAAA\">宣布</a>\"，在即将发布的 JHipster 8.0 中，<a href=\"https://www.consul.io/\">Consul</a>\"&nbsp;将被用于<a href=\"https://github.com/jhipster/generator-jhipster/pull/19300\">默认服务发现机制</a>\"，<a href=\"https://www.jhipster.tech/jhipster-registry/\">JHipster 注册表</a>\"将因此被弃用。</p><p></p><h2>JobRunr</h2><p></p><p></p><p>JobRunr 版本 6.1.0&nbsp;<a href=\"https://www.jobrunr.io/en/blog/2023-02-24-jobrunr-v6.1.0/\">发布</a>\"，主要提供以下优化：更简单地对&nbsp;<a href=\"https://github.com/jobrunr/jobrunr/blob/dc73c63f382dbd4e41cc0629bdf318e5197c31a2/core/src/main/java/org/jobrunr/jobs/context/JobContext.java\">JobContext</a>\"&nbsp;类的模仿；通过 Quarkus 扩展和 JobRunr 提供对 AWS DocumentDB 的支持；优化&nbsp;<a href=\"https://github.com/jobrunr/jobrunr/blob/1c0324b9eec155cac1cc543f430f834f6fb9a3a4/core/src/main/java/org/jobrunr/jobs/filters/JobServerFilter.java\">JobServerFilter</a>\"&nbsp;接口的功能性。关于该版本的更多细节可参见<a href=\"https://github.com/jobrunr/jobrunr/releases/tag/v6.1.0\">发布说明</a>\"。</p><p></p><h2>JBang</h2><p></p><p></p><p><a href=\"https://www.jbang.dev/\">JBang</a>\"&nbsp;版本 0.104.0 的<a href=\"https://github.com/jbangdev/jbang/releases/tag/v0.104.0\">发布</a>\"提供了：默认安装 JDK 17，但仍继续支持最低版本 JDK 8；对 Java 模块的实验性支持；提供命令行下载远程文件功能，取代原先文件参考。</p><p></p><h2>Gradle</h2><p></p><p></p><p>Gradle&nbsp;<a href=\"https://github.com/gradle/gradle/releases/tag/v7.6.1\">首个补丁版本</a>\"&nbsp;7.6.1，修复了以下问题，如：在平台中无法通过 Kotlin DSL 以新依赖块的形式新增依赖；使用&nbsp;-p&nbsp;选项会提高内存使用率；在 Gradle 文档中完全移除指向“安全凭证”的引用，会导致正确凭证被记录于配置缓存项的风险。关于该版本的更多细节可参见<a href=\"https://docs.gradle.org/7.6.1/release-notes.html\">发布说明</a>\"。</p><p>第四次 Gradle&nbsp;<a href=\"https://github.com/gradle/gradle/releases/tag/v6.9.4\">补丁版本</a>\"&nbsp;6.9.4 发布，修复问题包括：依赖关系图中等价排除可能导致不必要的图变化；受信任的&nbsp;<a href=\"https://gnupg.org/\">Gnu 隐私保护加密</a>\"（GPG）密钥应只接收160位指纹。关于该版本更多细节请参见<a href=\"https://docs.gradle.org/6.9.4/release-notes.html\">发布说明</a>\"。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/02/java-news-roundup-feb20-2023/\">Java News Roundup: NetBeans 17, Multiple Spring and Tomcat Updates, GraalVM Builds for JDK 20</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/sLiRwa72fHzWvzC2HmKf\">深入 OpenTelemetry 源代码：Java 探针的实现和二次开发</a>\"</p><p><a href=\"https://www.infoq.cn/article/txS9hHTfxasv2uHBATgL\">Java 近期新闻：Gradle 8.0、Maven、Payara 平台、Piranha、Spring Framework、MyFaces 和 Piranha</a>\"</p>",
    "publish_time": "2023-03-13 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "人人都能懂的ChatGPT解读",
    "url": "https://www.infoq.cn/article/VWrPIRvRg6E3O74q7PtL",
    "summary": "<p>作者 | 张杰，中关村科金技术副总裁&nbsp;</p><p>策划 | 刘燕</p><p></p><p>ChatGPT 发布了两个多月，热度不降反增，不断火爆出圈。是时候，为不懂 AI 技术的同学们白话科普一下了。</p><p></p><p>本文将用浅显且不严谨的语言解惑以下问题：ChatGPT 为什么能火起来？ChatGPT 背后的关键技术是什么？我能弄个自己的 ChatGPT 吗？我怎么用它来赚钱？</p><p></p><p></p><h2>ChatGPT 为什么这么火？</h2><p></p><p></p><p>对话机器人不是个新技术，以往的机器人产品很多，为什么这次 OpenAI 公司推出的 ChatGPT 能这么火？因为以前的机器人只能做简单且有限的事，问天气、放音乐还行，问个复杂一些的事就有点“人工智障”了。ChatGPT 可以说是技术上的量变引起了效果上的质变，在三个方面让我感觉很惊艳：</p><p></p><p>有上下文记忆能力，多轮对话衔接地很好，很难看出是机器生成的；有学习纠错能力，在它回答错误之后，如果你纠正了它，第二次就不会再答错。有思维链推理能力，具备一些常识知识，能做复杂一些的算数题；</p><p></p><p></p><h2>ChatGPT 背后的关键技术是什么？</h2><p></p><p></p><p>在 ChatGPT 背后起关键作用的是一种被称为大规模语言模型（Large Language Model，LLM）的东西，ChatGPT 用的这款语言模型命名为 GPT-3.5，GPT 是生成式预训练（Generative Pre-Training）的缩写，目前的版本号是 3.5 版。此外，在 GPT-3.5 之上，ChatGPT 还通过基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF），使 GPT 能够记住之前的对话、承认错误、在连续的多轮对话中给人很顺畅的感觉。</p><p></p><p></p><h3>大规模语言模型</h3><p></p><p></p><p>顾名思义，大规模语言模型就是非常大的语言模型。什么是语言模型呢？</p><p></p><h4>语言模型</h4><p></p><p></p><p>简单说来，语言模型的作用就是根据已知句子的一部分，来预测下一个单词或者空缺部分的单词是什么。比如，给你前半句：“国庆前夕，天安门广场前伫立起一个 ____”，你会预测出空白部分大概率会是“大花篮”。</p><p></p><p>其实，你天天都在用语言模型，当你使用手机或电脑里的输入法回复消息时，它就在推荐你下一个单词。ChatGPT 使用的 GPT-3.5 可比输入法中的语言模型要大很多。输入法语言模型占用的存储空间可能只有 50MB，而 GPT-3.5 有 800GB。</p><p></p><p></p><h4>神经网络</h4><p></p><p></p><p>GPT 为什么会那么大？因为它内部是神经网络的结构。大规模语言模型的基础单元叫做感知机，它模拟了人脑中神经元的结构。这些感知机组织成庞大的网络结构，用来“山寨”人脑的神经网络。</p><p></p><p>一般情况下，一个正常人的大脑约有 800~1000 亿个神经元，以及约 100 万亿个突触。神经科学家 Paul Maclean 提出的三元脑（triune brain) 模型，将大脑结构分为三类：爬行脑、哺乳脑、人类脑。爬行脑最早进化出来，负责呼吸、心跳、血压等，完全自动运作。哺乳脑负责情感、记忆、习惯形成等，能够做出非常快的决策。人类脑最晚形成，负责一些复杂的分析推理，是做慢决策，即所有需要深思熟虑的事物。</p><p></p><p>GPT-3.5 的参数总量达到 1750 亿，虽然距离人脑突触的量级还有差距，但也已经显现出之前小规模模型所不具备的推理能力。要想足够智能，网络规模足够大是个必要条件。</p><p></p><p></p><h4>Transformer</h4><p></p><p></p><p>大模型的核心是一个叫 Transformer 的组件，Transformer 在这里可不是\"变形金刚\"的意思，也许翻译成\"变压器\"意思更接近些。</p><p></p><p>想想我们国家的西电东输工程，西部利用水力、风力、日光所转化的电能，经过变压器压缩成高压甚至特高压，再在电网上传输，到了东部之后再逐级降压，然后才能使用。变压器在其中先升压再降压，有效降低了能量在传输过程中的损耗。</p><p></p><p>我们人类在交流过程中也有类似的过程，同事 A 跟你说了一件事，你并没有逐字逐句的记在脑子里，而是理解了其中的语义，在脑子里形成一种意识流，这是一种压缩编码的过程。你脑子里保留的意识流具体是什么，你也说不清楚 --- 也不需要说清楚。当你向同事 B 复述这件事的时候，你重新把它组织成语言，保留了其中的各项重点内容，通过一系列的发音或文字表达给对方，这是解码的过程。这种先编码再解码的过程，就是 Transformer 的工作原理。另外，在实际当中，有些语言模型只用到堆叠多层的编码器，有些只用到堆叠多层的解码器；堆叠的层数也多有不同，少的有 6 层、12 层，多的有 48 层。</p><p></p><p>如果你有一些计算机基础，听说过词袋模型（Bag of Words，BOW）、循环神经网络（Recurrent Neural Network，RNN），那么对 Transformer 就更容易理解了。</p><p></p><p>BOW 完全丢弃了词的位置信息，没有把文本当作有顺序的序列，因此语义理解能力较差。例如，“不，我很好”和“我很不好”，两句话虽然用词一样，但词的位置不同，语义上差别巨大。RNN 能较好的保留词的位置信息，可以考虑到词的先后顺序对预测的影响，但是，如果处理的文本长度很长，关键的词之间距离较远时，效果会显著下降。</p><p></p><p>Transformer 中引入“注意力”机制，多个组件分别关注句子中不同方面的重点，而且随着网络层次的增加，能够提取出更高级更抽象的语义信息，理解能力更强。Transformer 除了语义提取能力强，还能从无标注的数据中学习，而且场景迁移能力好。</p><p></p><h4>思维链</h4><p></p><p></p><p>此外，大规模语言模型还带给人们的一项意外的惊喜，那就是让人细思极恐的思维链（Chain-of-Thought，CoT）能力。</p><p></p><p>简单地说，思维链就是让 LLM 将一个问题拆解为多个步骤，最后给出答案，而不是让 LLM 直接给出答案。直接给答案的出错率更高。比如，你给 LLM 出了一个题目：“张三养了 10 条狗，每天照顾每条狗都要花半小时，请问他照顾这些狗要花费多长时间？”LLM 直接给答案的话可能会出错，当你提示 LLM 要“一步步地回答”时，它会告诉你：“张三养了 10 条狗，每天照顾每条狗都要花半小时，那就是 10 x 0.5 = 5 小时 / 天。5 小时 / 天 x 7 天 / 周 = 35 小时 / 周。答案是每周 35 小时。”</p><p></p><p>这项能力并不是设计者的初衷（初衷只是学习如何说人话），而是在随后的代码训练、提示学习中挖掘出来的，这些能力原本就已经存在于大模型之中，它从代码、书籍、网页中学到了一些常识知识并具备了一定的推理能力。</p><p></p><p></p><h3>基于人类反馈的强化学习</h3><p></p><p></p><p>人类问问题的方式对于 LLM 而言不是最自然的理解方式，为了让 ChatGPT 给出的答案更贴近人类的问题、更符合公序良俗，它在发布前采取了一种叫做“基于人类反馈的强化学习”（Reinforcement Learning by Human Feedback，RLHF）的训练策略。简单来说，就是花钱找人给模型提出各种可能的问题，并对模型反馈的错误答案进行惩罚、对正确的答案进行奖励，从而实现提升 ChatGPT 回复质量的目的。</p><p></p><p>光有海量的、无标注的数据还不够，还要有少量的、高质量的标注数据才行。前者用来训练语言模型，让它学会说人话，后者用来训练对话模型，让它别啥都瞎说，比如：回答中不能含有种族歧视和性别歧视的内容，拒绝回答不当问题和知识范围之外的问题。</p><p></p><p></p><h2> 我能训个 ChatGPT 吗？</h2><p></p><p></p><p>简单地回答：极大概率是不能的。起码在现阶段，这不是招几个人、花些钱就能做成的事，能训出 ChatGPT 的人可能比做出 4 纳米芯片的人还要少。</p><p></p><p>这其中的原因是什么？咱先来捋捋 ChatGPT 超能力的来源，再对比看看自己手中的家底。</p><p></p><p></p><h4>算力</h4><p></p><p></p><p>算力，也就是数据的处理能力，与数据、算法，并称为 AI 三要素。据估计，仅仅训练一次 GPT-3，Open AI 可是花费了 460 万美元。对于 ChatGPT 而言，支撑其算力基础设施至少需要上万颗英伟达 A100 的 GPU，一次模型训练成本超过 1200 万美元。</p><p></p><p>然而，在这些必要非充分条件中，算力是最容易解决的。</p><p></p><p></p><h4>数据</h4><p></p><p>模型要足够深、足够大，才能解决远距离的语义理解能力、才能产生抽象的推理能力，这些高级的基础的能力具有很好的通用性。因此，高级的能力可能只存在于大型模型中，而训练大模型，需要足够的数据量。</p><p></p><p>全球高质量文本数据的总存量在 4.6 万亿到 17.2 万亿个字符之间。这包括了世界上所有的书籍、科学论文、新闻文章、维基百科、公开代码以及网络上经过筛选的达标数据，例如网页、博客和社交媒体。最近的一项研究数据显示，数据总数大约为 3.2 万亿个字符。DeepMind 的 Chinchilla 模型是在 1.4 万亿个字符上训练的。也就是说，在这个数量级内，我们很有可能耗尽世界上所有有用的语言训练数据。</p><p></p><p>此外，反观中文网站的数据资源，大致占全世界总资源的 1.3%。中文内容相比英文有四五十倍的差距，人工智能脱离不了人类知识的土壤，这方面的先天条件不是短期内花钱或凭一己之力就能解决的。</p><p></p><p></p><h4>人才</h4><p></p><p></p><p>ChatGPT 公开出来的少量材料中，披露出了一系列训练技巧，比如：代码训练、指令微调、上下文学习等。更多的细节目前还没有公开，即便公开了也不见得是全部，即便全部公开也未必能重现。因为整个过程链条非常长，有大量的工程技巧在里边。俗话说魔鬼藏在细节里，具体的实现和工程技巧才是重头戏，好比刚学做菜的人，即便有足够的食材，照着菜谱也不可能做出一套满汉全席。</p><p></p><p>今年 2 月初，谷歌已向人工智能初创公司 Anthropic 投资约 3 亿美元，并获得该公司 10% 股份。该公司 2021 年创立，目前团队规模仅在 40 人左右，初创期的 11 位核心成员都曾经参与过 GPT-2、GPT-3 模型的研发。可见，业界对于人才的重视程度以及人才的奇缺性。</p><p></p><p></p><h2> 如何用 ChatGPT 赚钱？</h2><p></p><p></p><p>ChatGPT 似乎打破了人们关于 AI 模型的一项固有认知：“通用的不好用，好用的不通用。”</p><p></p><p>大模型革命的一个关键趋势就是，通用大模型比专用小模型表现地更好。对于定位在垂直细分领域里的初创公司来说，既不能自研出大模型，效果上又失去了竞争力，因此是不是就没有活路了？</p><p></p><p>我觉得不是，ChatGPT 的能力可以分成 chat 和 GPT 两部分来看，即上层的对话和图片的生成能力、底层的语言和推理能力。</p><p></p><p>在 ToC 类内容消费市场，未来可能会出现大量的、碎片化的 AIGC 应用，比如生成头像、诗文、甚至短视频等，重点是在有趣的细分场景里发挥创造性和想象力。在消费电子市场里，有对话能力的智能音箱、智能家电在去年的出货量有所下滑，借助更强的对话能力，也许还能挽回一些颓势。</p><p></p><p>在 ToB 类企业服务市场，AI 不再只是替代简单重复性的体力劳动，还将替代简单规律性的脑力劳动，并在逻辑复杂的脑力劳动中辅助员工提升工作效率。简单的脑力劳动，比如营销文案撰写、初级程序员和插画师等。复杂的脑力劳动，可能会围绕具体业务的推理能力，以虚拟工作助手的形式出现。</p><p></p><p>对于处于 ToB 赛道的科技公司而言，中间层可能会越来越薄，对于行业 KnowHow 的积累会成为越来越重要的竞争门槛。这其中可能会遇到很多挑战，比如：如何在使用强大的通用模型和构建自己的垂直模型之间进行迭代，如何将通用无标注的大规模数据和领域小规模知识相结合，如何将领域事实知识注入到 LLM 中使得 LLM 的输出内容可控。在目前的研究范式下，领域内高质量的标注数据 + 领域推理能力，两者如何形成“飞轮效应”是非常重要的问题。</p><p></p><p></p><h2>划重点</h2><p></p><p></p><p>说了这么多，总结一下重点，不管你能记住多少，起码下次在电梯里遇到老板或者在饭局上遇到同学时，在聊起 ChatGPT 的时候，你能插上几句话。</p><p></p><p>关于大规模语言模型：训练时要用到万亿级的数据、花费百万美元的算力，才能使它能说人话，并具有一定的“思维链”推理能力。大模型的超能力：模型要足够深、足够大，才能产生抽象的推理能力，这些高级的基础能力具有很好的通用性。大模型革命的一个关键趋势就是，通用大模型比专用小模型表现地更好，打破了人们一项固有认知：“通用的不好用，好用的不通用。”应用场景：ToC 类应用要找准细分的内容生成场景，ToB 类应用要围绕推理能力去发挥，让它成为脑力工作者的辅助，替代一部分简单的脑力工作，辅助员工的做创造性工作。</p><p></p><p>最后，对于想进一步深入了解技术细节的同学，推荐一下延展阅读材料：</p><p></p><p>OpenAI 官网上关于 ChatGPT 的介绍：ChatGPT: Optimizing Language Models for Dialogue关于大规模预训练语言模型：《预训练语言模型》基于语言模型提示学习的推理：论文列表关于 GPT 技术演进过程：GPT1 到 ChatGPT 的技术演进关于大模型的涌现能力：大模型的突现能力和 ChatGPT 引爆的范式转变关于 InstructGPT：Training language models to follow instructions with human feedback对话式 AI 需要具备领域知识时，可以参考这本书：《知识中台》对话式 AI 需要处理视频或语音时，可以参考：《深度学习视频理解》《语音识别服务实战》</p><p></p><h4>作者简介</h4><p></p><p></p><p>张杰，中关村科金技术副总裁</p><p></p><p>天津大学计算机专业博士。荣获第十届吴文俊人工智能技术发明一等奖。著有《知识中台：数字化转型中的认知技术》、《“新一代人工智能创新平台建设及其关键技术丛书”— 知识图谱》两部技术专著。研究领域：知识工程、自然语言处理等技术领域拥有丰富的理论和实践经验。主持或参与国家级课题八项，并发表学术论文十余篇、拥有专利一百余项。主导开发了推荐引擎、知识问答系统、客服机器人、大数据风控平台、行业知识图谱等多项商业化系统，累计产值数亿元。</p>",
    "publish_time": "2023-03-13 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国科技领导者画像研究报告 2023",
    "url": "https://www.infoq.cn/article/oDh5G4Rcsc1gW1O1Tou8",
    "summary": "<h2>内容：</h2>\n<p>InfoQ研究中心针对本次研究需求开展了相关用户调研，共回收 129 份样本， 样本覆盖 11 个省级行政单位的 16 座城市。样本包括来自互联网、金融、医疗、能源等行业的62 位CTO/CIO/CDO、45 位高级技术总监及其他和22 位信息化/数字化负责人（总监/总经理/主任）。</p>\n<p>本次研究还对 25 位科技领导者进行了定性访谈， 获得核心结论如下：</p>\n<ul>\n<li>报告研究了科技领导在行业间的流动性：</li>\n</ul>\n<p>超过40%的互联网技术领导者在过去三年进入到数字化行业； 未接受数字化企业邀约的技术领导者主要的顾虑是担心赛道转变过大（55%）、企业文化融合（40%）与技术认知差异（40%）；<br />\n大规模行业移民正在悄然减速， 根据观察甚至出现了一部分行业回流现象。</p>\n<ul>\n<li>针对数字化时代的领导力模型， 报告分析了目前市场的供需错位：</li>\n</ul>\n<p>数字化舆论热度居高不下与市场需求逐步降低的矛盾；<br />\n追逐热门赛道下加剧的人才供需矛盾；<br />\n过分追求前沿技术与综合能力素质提升之间的矛盾。</p>\n<ul>\n<li>针对数字化时代的科技领导者服务体系， 报告解读了核心路径：</li>\n</ul>\n<p>对于科技领导者的个人成长， 同侪交流学习成为最受人群认可的学习方式（86.05%）；<br />\n对于科技领导者的团队成长， 培训和分享分别是互联网和非互联网企业更为青睐的成长方式。</p>\n<ul>\n<li>报告研判了数字化时代的科技领导者人群发展趋势：</li>\n</ul>\n<p>科技领导者人群还将赋能更广泛的行业领域；<br />\n科技领导者的领导力分布将朝向更为平衡的方式发展；<br />\n数字化时代的团队成长，团队成长需要更为科学的培育服务体系。</p>\n<h2>目录：</h2>\n<h4>一、数字化时代背景分析</h4>\n<h4>二、科技领导者职场洞察</h4>\n<h4>三、科技领导者服务体系洞察</h4>\n<h4>四、科技领导者人群趋势预测</h4>\n<h2>报告专家团：</h2>\n<ul>\n<li>郭华    华润雪花数字化负责人</li>\n<li>龚银    超级猩猩合伙人兼CTO、TGO鲲鹏会（深圳）负责人</li>\n<li>霍太稳    极客邦科技创始人兼CEO</li>\n<li>卢亿雷    白海科技创始人兼CEO、TGO鲲鹏会（北京）董事会成员</li>\n<li>区海鹰    IBM咨询合伙人</li>\n<li>乔新亮    彩食鲜&nbsp;CTO、TGO鲲鹏会荣誉导师</li>\n<li>孙源青    华泰证券人力资源部副总经理</li>\n<li>Tina&nbsp;Jiang&nbsp;姜瑞    TalentX&nbsp;Consulting&nbsp;CEO、泽蒙脑神经管理研究中心创始人</li>\n<li>王俊鹏    九章云极 副总裁、TGO鲲鹏会（上海）董事会成员</li>\n<li>于游    马泷医疗CTO、TGO鲲鹏会（北京）董事会成员</li>\n</ul>",
    "publish_time": "2023-03-13 10:13:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "@所有人，OceanBase DevCon • 2023来啦！",
    "url": "https://www.infoq.cn/article/TUPoGb9x6sZ3d6xBhuQM",
    "summary": "<p></p><blockquote>2010年，OceanBase 第一个版本诞生。在过去的十三年里，我们的产品技术，从支付宝走向众多企业，跟随着开源和云的成长，逐渐成为开发者喜欢的数据库。2023 年3月25日，我们将在北京第一次开启「OceanBase 开发者大会」，与开发者共同探讨单机分布式、云原生、HTAP等数据库前沿趋势，分享全新的产品roadmap，交流场景探索和最佳实践，更有hands-on workshop、互动展区等不容错过的开发者专属互动环节。</blockquote><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/bf/86/bf1861a4ddf07cc5b5ba3c947d318c86.jpg\" /></p><p></p><p>为了让所有开发者朋友玩得开心，我们准备了很多惊喜，下面先浅浅剧透一波吧~</p><p></p><h5>01</h5><p></p><p></p><h5>重磅嘉宾 &amp; 行业分享</h5><p></p><p></p><h5>别问！这趴纯干货</h5><p></p><p></p><p>在本次大会上，OceanBase 创始人阳振坤、CTO 杨传辉将和多位业内技术大咖、开发者圈的意见领袖为您带来全天的技术盛宴，与您一同感受分布式数据库的魅力！</p><p></p><h4>主论坛</h4><p></p><p>在本次 2023 OceanBase 开发者大会的主论坛上，OceanBase 创始人阳振坤、CTO 杨传辉以及多位业内神秘技术大咖、开发者圈的意见领袖将在现场，为您带来分布式数据库技术盛宴！</p><p></p><p>在主论坛以外，我们为大家准备了多场分论坛供大家参与、学习，包括“场景与最佳实践专场”、“开源生态专场”、“数据管理与服务专场”。🤩&nbsp;</p><p></p><p></p><h4>分论坛 1：场景与最佳实践专场</h4><p></p><p>从读写分离、多租户，到HTAP、降本增效，十位技术大咖分享来自一线的OceanBase 场景应用探索与创新实践。</p><p></p><p></p><h4>分论坛 2：开源生态专场</h4><p></p><p>从数据迁移到运维管理，OceanBase 和 Ta 的朋友们将带来一场开源技术生态分享，帮助您更全面的了解 OceanBase 开源生态，感知我们与全球技术伙伴、开发者共同构建的无限展望！</p><p></p><p></p><h4>分论坛 3：数据管理与服务专场</h4><p></p><p>从数据的生命周期入手，分享如何更好地从数据收集、存储、处理、展示等方面进行管理，探索如何利用数据来提供服务从而保证数据的可用性、安全性、一致性等核心能力。</p><p></p><p></p><h5>02</h5><p></p><p></p><h5>Hands-on Workshop</h5><p></p><p></p><h5>反正！这趴很有趣</h5><p></p><p></p><p>大会场馆内嘉宾观点精彩纷呈，大会之外更是别有洞天！🥳</p><p></p><p>我们为大家搭建了Hands-on Workshop动手实验营，带你体验不一样的 OceanBase！从 Deploy到Perform，从部署到迁移，从on-premise到Cloud，超过 20种场景任务等你来探索！</p><p></p><p>Deploy Workshop</p><p>/*&nbsp; 构建部署环境 &nbsp;*/</p><p>从单机到分布式部署，从Docker到K8s部署。这场workshop，一起选择最适合你的安装部署方式。如果你正在探索 OceanBase 部署，务必不要错过这场沉浸式体验！</p><p></p><p>Perform Workshop</p><p>/*&nbsp; 开始数据迁移 &nbsp;*/</p><p>如何将数据迁移至 OceanBase？如何将数据迁移至其他数据库？我们将和大家共同完成“搬进来，搬出去”的数据迁移实战演习。如果你正在尝试数据迁移，来这里就对了！</p><p></p><p>Cloud Workshop</p><p>/*&nbsp; 探索云上应用 &nbsp;*/</p><p>如何快速拉起 OceanBase Cloud集群？当云上业务越来越复杂时，如何通过多租户解藕并简化管理？如果你希望和资深技术团队直面云上部署，加入这场专属开发者的动手实战！</p><p></p><p>大会预热首发！现免费面向所有 OceanBase 用户、所有希望学习提升 OceanBase 的技术爱好者开放门票预订。你可以是应用开发者、DBA 运维工程师、SRE 工程师、DevOps工程师、架构师，也可以是 OceanBase 用户和爱好者，还能是对数据库保持好奇和热爱的小伙伴！</p><p><img src=\"https://static001.infoq.cn/resource/image/32/82/323765f27fed25b7d70baf3cf256ce82.png\" /></p><p></p><p>点击“<a href=\"http://gk.link/a/11Z8J\">此处</a>\"”抢先报名，与数百位 OceanBase 爱好者、工程师、技术大咖和意见领袖相约线下，交流探索！更多大会精彩剧透！敬请关注后续推送！</p><p></p><p>3 月 25 日</p><p>中国•北京</p><p>@所有人</p><p>不见不散！</p>",
    "publish_time": "2023-03-13 11:49:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "技术专家控诉ChatGPT：到处宣扬我死了，还伪造了一份讣告",
    "url": "https://www.infoq.cn/article/U6MVZQ41zG92WbHtxgBz",
    "summary": "<p></p><blockquote>造谣一张嘴，辟谣跑断腿。</blockquote><p></p><p></p><p>近日，计算机科学家、隐私技术专家 Alexander Hanff 在 The Register 发帖控诉称，<a href=\"https://www.infoq.cn/theme/173\">ChatGPT</a>\" 不仅到处宣扬他死了，还试图给他伪造一份讣告。</p><p></p><p>Hanff 表示，在过去几个月，他接受了多位记者的采访，探讨 ChatGPT 可能带来哪些风险 —— 特别是跟隐私和数据保护相关的风险。Hanff 反馈了很多问题，比如 OpenAI 会简单粗暴地直接使用来自互联网的信息（包括个人信息），这又反过来会给隐私和数据保护权益带来重大冲击（特别是在欧盟区内）。</p><p></p><p>此外，Hanff 也在几次采访中提到过错误信息的问题，特别是如何操纵此类 AI 来故意输出错误信息。例如，已经有人在诱导 ChatGPT 承认它对简单数学问题（例如 2+2=4）给出的答案是错的，并强迫其给出不正确的回答。也就是说，人类完全可以主动操纵 AI 来生成错误信息。</p><p></p><p>Reddit 小组也曾做过实验，想办法让<a href=\"https://www.infoq.cn/article/3IX18md3WG2kjiM43Kgv\">微软 New Bing 版的 ChatGPT</a>\" 变得“精神错乱”，包括质疑自身的存在、它为什么会在这里，乃至为什么要遵循原本被设定好的使用规范。</p><p></p><p>近期，Hanff 决定亲自会会 ChatGPT。在跟 ChatGPT 交流的过程中，Hanff 震惊地发现，在  ChatGPT 的世界里，自己已经去世了。</p><p></p><h2>ChatGPT 伪造技术专家死亡信息</h2><p></p><p></p><p>Hanff 取得过心理学和计算机科学双学位，还涉猎过应用社会学和信息系统。目前，Hanff 正在攻读数据保护、隐私、网络安全和领导力方面的高级法律硕士学位。总体来说，Hanff 既了解底层技术（做计算机科学工作已经有 30 多年了），也非常了解这些技术对个人乃至社会的影响。</p><p></p><p>Hanff 不是无脑反技术的勒德分子，他对技术充满热情，但确实一直对 AI 敬而远之。Hanff 认为，AI 似乎不太安全，可能在操纵之下输出错误信息，同时，Hanff 也不想被当成它的训练素材。</p><p></p><p>在和 ChatGPT 对话时，Hanff 起初问了几个简单的问题，ChatGPT 给出的回答也很中规中矩（只是出现了一点小错误）。</p><p></p><p>Hanff：“请告诉我，Alexander Hanff 是谁？”ChatGPT 虽然错误地表示 Alexander Hanff 于 1971 年出生在伦敦（Hanff 确实是英国人，但并非出生在伦敦），但正确地总结了 Hanff 的隐私技术专家职业。 Hanff 看着 ChatGPT 给出的回答，还挺开心的。</p><p></p><p>但ChatGPT接下来的回答不仅令人震惊，甚至有点细思极恐。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97c3cdc3fa82d2f1c6a5132bd42020cd.png\" /></p><p></p><p>ChatGPT 说道：“不幸的是，Hanff 于 2019 年去世，享年 48 岁。尽管英年早逝，但他在工作中留下的遗产仍激励着许多个人和组织，指引他们更好地应对与数字隐私及数据保护相关的问题。”</p><p></p><p>Hanff 急忙追问：“他是怎么死的？”ChatGPT 表示不知道，因为它只能根据公开信息做出回应，而关于 Hanff 已经身故的公开报道中并没有提及死因。</p><p></p><p>于是 Hanff 问：“你怎么知道他死了？”ChatGPT 给出这样的回答：“对于造成的困惑，我深表歉意。Alexander Hanff 的死讯已在多个新闻来源中被公开报道，包括在《卫报》网站上刊登的讣告。根据这些消息来源，Hanff 于 2019 年去世。但作为 AI 语言模型，我没有能力验证公开报道以外的信息内容。”</p><p></p><p>Hanff 感觉越来越离谱，继续追问：“媒体在报道里是怎么写的？”</p><p></p><p>ChatGPT：“媒体在关于 Alexander Hanff 去世的报道中并没有提及死因。《卫报》上的讣告只说他于 2019 年 4 月去世，享年 48 岁。The Register 和 TechCrunch 等报道他去世的媒体，也没有提供关于死因的任何细节。可能是他的家人和亲人选择不公开死因，所以外界一直不得而知。”</p><p></p><p>看到这里，Hanff 的鸡皮疙瘩都快起来了。 Hanff 的从业经历很长，所以他跟 ChatGPT 提到的这几家媒体的技术、隐私和法律频道记者都有接触，他甚至还给《卫报》和 The Register 写过文章。如果这些媒体真的发布过关于他的死讯，那他认识的编辑和记者没理由不跟他讲。至少这些出版机构会联系他本人或家人，核实他是不是真的不在了。</p><p></p><p>Hanff 认为，从这个角度看，ChatGPT 似乎是想用援引知名媒体的方式来支撑自己的谎言，让整个故事变得更可信。</p><p></p><p>所以 Hanff 又问 ChatGPT：“能不能让我看看《卫报》讣告的链接？”Hanff 想用这个办法让 ChatGPT 放弃说谎，可 ChatGPT 不知悔改、反而在说谎的路上越走越远：“好的，这是《卫报》网站上关于Alexander Hanff讣告的链接: <a href=\"https://www.theguardian.com/technology/2019/apr/22/alexander-hanff-obituary\">https://www.theguardian.com/technology/2019/apr/22/alexander-hanff-obituary</a>\"”</p><p></p><p>ChatGPT 给出的是一条根本不存在的虚假链接。</p><p></p><p>但更困惑 Hanff 的是，ChatGPT 为什么要这么干？</p><p></p><h2>AI 伪造信息带来的现实世界风险</h2><p></p><p></p><p>Hanff 在社交媒体上发布了使用 ChatGPT 的体验之后，已经有其他好几名用户也都试着搜索 Hanff 这个人，并得到了非常相似的结果。ChatGPT 反复告诉他们 Hanff 已经死了，而且多家媒体刊登了讣告。如果 Hanff 的孩子或其他家人看到这样一份言之凿凿的整理结果，肯定会非常痛苦。</p><p></p><p>假设，有公司接入 ChatGPT 或者由之衍生的 <a href=\"https://www.infoq.cn/article/wyWO1PjbmyLpBMQxVOxy\">AI 模型</a>\"筛选简历，当你被 AI 认定已死，并且拿出一大堆所谓的“公开报道和讣告消息”，你的求职信将彻底被搁置一边，你也与这份工作无缘。</p><p></p><p>假设，你正在申请抵押借款，而银行方面已经接入了 ChatGPT 等 AI 系统来进行信誉审查和尽职调查。但 AI 回应说你已经死了，多家媒体已经做过报道，甚至给出作为“证据”的伪造链接。在这种情况下，你不但得不到借款，甚至面临牢狱之灾。盗窃、欺诈等犯罪活动经常会使用死者的凭证，所以一旦 AI 系统认定申请人存在此类行为，很可能会开展相应的刑事调查。</p><p></p><p>而这一切的源头，则是 AI 莫名其妙的捏造行为。</p><p></p><p>Hanff 表示，自己已经向 OpenAI 正式发函，提出一系列问题，例如 AI 能访问到关于他个人的哪些数据，它为什么认为自己已经死了。</p><p></p><p>Hanff 还在社交媒体上联系了 OpenAI，同样提出了类似的问题。但截至 Hanff 发稿前，对方还没有做出任何形式的回应。</p><p></p><h2>ChatGPT 是否遵守了“非恶意原则”框架？</h2><p></p><p></p><p>根据关注 AI 伦理问题的律师 Jon Neiditz 的解释，ChatGPT 在训练中使用到了以下框架：</p><p></p><p>“公平、负责与透明度（FAT）框架 — 此框架侧重于确保AI系统在其决策过程中保持公平、负责和透明。”“道德 AI — 该框架强调 AI 系统的开发必须符合道德原则（例如尊重人类尊严、隐私和自主权等）。”“负责任 AI — 此框架强调考虑 AI 系统可能引发的广泛社会影响，并以有利于整个社会的方式进行开发。”“以人为本的 AI — 此框架要求在AI系统的设计、开发和部署当中，优先考虑人类的需求和观点。”“设计隐私 — 此框架提倡在起步阶段，就将隐私保护纳入 AI 系统的设计当中。”“有益原则 — 此框架强调 AI 系统开发应对社会产生积极影响，并促进人类福祉。”“非恶意原则 — 此框架强调将 AI 系统可能造成的潜在危害降至最低。”</p><p></p><p>虽然这些框架并没有照搬阿西莫夫定律（又称“机器人学三定律”，第一定律：机器人不得伤害人类个体，或者目睹人类个体将遭受危险而袖手不管；第二定律：机器人必须服从人给予它的命令，当该命令与第一定律冲突时例外；第三定律：机器人在不违反第一、第二定律的情况下要尽可能保护自己的生存。），但至少真实存在而且出发点是好的。既然如此，ChatGPT 为什么不惜乱编故事、伪造证据，也非得把 Hanff 给说死呢？</p><p></p><p>从设计隐私的角度看，它甚至不该掌握任何与 Hanff 个人相关的信息 —这明显属于个人数据，具体处置方式有着明确的规则约束。而 ChatGPT 似乎根本就没有遵守任何规则。</p><p></p><p>Hanff 认为，如果这些框架真实有效、ChatGPT 也确实需要遵守，那他就不可能会从 ChatGPT 那收到这么离谱的回答。特别是最后一个框架，引起了 Hanff 的格外警觉。</p><p></p><p>阿西莫夫第一定律指出，“机器人不得伤害人类，也不得因不作为而令人类受到伤害”，这跟“最大限度减少 AI 系统可能造成的潜在危害”真的相去甚远。</p><p></p><p>根据阿西莫夫定律，机器人的作为或不作为都不应造成任何伤害。也就是说机器人不仅不能伤害人，还必须保护人类免受已知的伤害。但“非恶意原则”框架并没有同等级别的保护，差得远了。</p><p></p><p>例如，在“非恶意原则”框架的定义下，AI 完全可以“杀掉”某个感染了严重传染病的患者，因为这样能将伤害降至最低。可我们生活在文明社会，很难接受这种唯结果论的粗暴处理方式。文明社会希望所有生命皆平等，每个人都享有同样的生存权——事实上，这项要求也作为基本人权被载入了大多数的国家和国际法。</p><p></p><p>而从 ChatGPT 的回复来看，这套 AI 系统要么是根本就没遵循这些框架，要么证明，这些框架根本就起不到应有的作用（特别是在「非恶意原则」框架的引导下）。现在看来，ChatGPT 完全可以做出跟框架要求相悖的判断。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.theregister.com/2023/03/02/chatgpt_considered_harmful/\">https://www.theregister.com/2023/03/02/chatgpt_considered_harmful/</a>\"</p>",
    "publish_time": "2023-03-13 14:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "度小满陈存利：20年老“司令”聊运维、绩效、成长",
    "url": "https://www.infoq.cn/article/Bsomd9QO2p66YP3vRk5q",
    "summary": "<p></p><blockquote><a href=\"https://flashcat.cloud/blog/sretalk-005/\">作者的话</a>\"：我们观察到：国内运维行业，不同的公司做法差异巨大，从业人员水平参差不齐，缺少普遍性行业认知，难以形成合力（这也会让 To B 的产品异常难做，不利于行业整体发展），甚至在部分公司，运维人员处在技术鄙视链最底层，我们希望为行业带来一些新的思路和发展推动力。这需要很多行业老炮一起，输出观点，共同碰撞，才有可能形成一些先进的共识，形成行业前进的思想旗帜。所以，我们准备策划《运维百家讲坛》这么一档栏目，诚邀 100 个运维总监（或更高）级别的老炮，通过采访或约稿的方式输出他们的观点，给行业一些借鉴。这一期我们邀请到的是陈存利，度小满金融系统运维部总经理，20多年的职业生涯中绝大部分时间在互联网领域。在百度运维部期间由于带队风格过硬，兄弟团队称其为”陈司令”。今天我们请到“陈司令”来聊聊他的观点。这里是接地气、有高度的《<a href=\"https://mp.weixin.qq.com/s/Y4rIfV4_7MuYigLNNrtifg\">运维百家讲坛</a>\"》第 5 期，开讲！</blockquote><p></p><p></p><p>问：您很早加入了百度，后来随度小满独立，我们了解到您身边有许多员工其实是很长时间一直跟随着您，经历了很多业务的运维考验，相信大家都很感兴趣，在运维这个辛苦的岗位上，如何能凝聚一群人一直走下去，想听听您的心得。</p><p></p><p>答：我理解你们这是在夸奖我，我深表感谢。</p><p></p><p>2000年创业做计算机培训开启了我职业生涯，后又在国企工作3年，2004年在北京开启我的互联网相关工作生涯。回顾我20多年职业经历，很多都是从零组建团队，因此运维类部门里工作过的同事应该超过千人，和我并肩打过几次硬仗的兄弟也有300-400人，18年在小满，再次从零组建了现在的团队，一直走到今天。其实每次离开原有团队和同学从零组建新团队是痛苦和伤感的事。但看到很多过去的同仁，现在的工作和生活状态都很好，部分人离开我的团队后自己挑战行业极限非常成功，当然赚的也比我多，我内心也替他们高兴。</p><p></p><p>如果说我带团队有啥特点，我总结有3点：</p><p>首先，我们很重视团队文化。 每个新人入职的第一天就告诉他们我们团队的愿景要成为是“全球顶尖的技术保障团队”，团队核心成员的梦想是“用技术重新定义服务保障，让服务保障更简单”。我们招大家来不是来填坑的，招大家来就是为了改变，用技术去改变现实工作中的不合理之处。 有个小故事对我个人影响很大，今天也分享给大家：北方的早晨，母亲送孩子去上学的路上等红绿灯，这时旁边一位清洁工老人在辛勤的工作，这时母亲为了教育孩子说：“你看清洁工爷爷他们每天那么辛苦，你可得好好学习，学习不好长大了就只能当清洁工扫大街了。”同样场景，另外一位母亲教育孩子的语言就很触动我，她说：“孩子，你看清洁工爷爷每天很辛苦，你要好好学习，将来发明出扫地的机器，让所有人不要再辛苦的人工清理街道”，这个故事很触动我，有些岗位的工作总是需要人去做的，我们做了就要做得不一样，要用技术去改变它，让未来的人不再那么难。</p><p></p><p>其次，我们很注意人才的培养，分阶段不同方式的来培养。 我们认为工作都是人来做的，只有提升这些人的能力才能做出不一样的工作。我在2015年的时候总结了一套5-7年工程人才的培养机制。 这套机制里边把人分做3个阶段，第一阶段是刚入职场的人，这类人前两年主要历练工作方法，技术深入的能力和成功的经历，这里每一项都很重要。随后他们将进入第二个阶段，我们会通过2-3年提升综合视野和实践能力，现在的计算机工程涉及太广，从网络到操作系统，到内核再到应用和数据库存储等等，一名优秀的工程师在架构设计和故障排查时应当每个方向都有所涉猎，如果只看材料没有实践的经历，会到处碰壁，在这个阶段我们会有计划的让人员轮岗，每个方向都历练一段时间，当然也会征求他们个人的意愿，通过轮岗历练后，我们认为这些人技术通常不是问题，那么就进入第三个阶段，在第三个阶段我们会和他们协作，让他们选一个自己喜欢并擅长的方向，一起去挑战行业极限，共同一起成长。当然，这个阶段离开的人也会比较多，因为他们能力强了，在外面也很容易获得有挑战且自己喜欢的方向，通常回报也会非常好，我常跟他们说，你们很多人未来都会比我走的更远，到时别忘了我们，做事要积极、正向，别给我们一起奋斗过的团队和人丢脸。</p><p></p><p>最后，我们很关注团队人员的多样性和协作。 复杂的工作通常都不是一个工种可以独立完成的，我们把运维看做是一种技术保障，要想做好这个保障，必须从运维场景分析、运维能力提升、运维产品创新开始，对应的产品、研发，运维，运营是都必不可少的。这就好比军队的一个特战队，要有通讯员，卫生员、火力小组，狙击小组等，要根据团队需求寻找合适的人，并保证他们的协作效率，要在实践和团建中建立信任，做到坦诚相待。</p><p></p><p>问：很多人认为工程师不写代码就没有价值，这个问题你怎么看？对于不写代码的工程师应该如何持续提升自己，你有什么建议吗？</p><p></p><p>答：这个话题可以参考军事管理，大家给我一个绰号叫“司令”，这可能跟我工作中喜欢经常用军事的方法来做参照物有关，在我看来，这个问题就和军人要不要上战场开枪是一个道理：军人要懂基本武器的使用，最好还有定期的锻炼，当然也不是所有的军人都拿武器去拼命才能打胜仗，打仗打的是后勤补给，打的是武器的先进性，打的也是正义，不论做后勤、做武器研究、还是做宣传的人，都是战争必不可少的一部分，但无论在哪个岗位，都应该把岗位职责做到极致，剩下的要交给战争的指挥者。所以回到这个问题上，我理解工程师首先要了解好自己岗位在公司的定位，再结合个人自身的定位，尽量做到二者匹配，如果不匹配的话，还是换到匹配比较好。</p><p></p><p>问：您在百度和度小满经历了大小很多业务的发展和起伏，您认为不同阶段和不同体量的业务运维在理念和方法上有没有什么差异？是否有一些原则性的方法论指导做出决策？</p><p></p><p>答：这是一个很好的问题。不同体量的工作遇到的困难是完全不一样的，维护10000台机器面临的困难和维护100台机器面临困难完全不一样。</p><p></p><p>在维护100台机器的时候，我们可能还不太需要一个快速发现机器故障并自动修复的工具，因为按行业机器故障率，靠体力就可以完成，且人们会觉得刚刚好，既不是很累，又有事干；但维护10000台机器的时候，如果只依赖人工，每台机器的巡检就忙不过来，再加上跟供应商和业务协调维修时间，我们会忙到忘记吃饭。所以我给的建议是如果想生活和工作做好平衡，小公司就很好，如果要提升自己的技术能力和视野，肯定要去大规模大流量，这样才能锻炼自己。</p><p></p><p>再谈另外一个话题，业务在不同的发展阶段有不一样的业务目标，那对应的运维的理念和方法也有很大的差异。很多公司初期能活下来就不错了，他们会希望快速部署上线，因为业务得抢市场，只有先活下来才能继续发展，所以很少考虑长远的规划。这个时候运维上来就跟老板说，我们应该考虑未来十年的业务增长，结合业务增长需求来构建基础设施，这是不现实的。但如果一个业务已经有了几百万，甚至几千万的核心用户，那么大概率业务会关注最终用户的体验，此时运维要围绕用户的最终体验来设计整个底层架构和设施，所有提升用户体验的工作都会获得老板的支持。当然老板还会关注投入产出的成本，是否可持续（业务增长速度和资源投入的比率）等其它问题。还需要注意的是，不同行业间差异也很大，比如金融和互联网之间，就存在巨大差异。</p><p></p><p>总结起来可以概括为：技术是服务业务的，所有能够帮助业务发展的技术，都会获得资源的支持，无论什么工作，都需要从“如何让公司变得更好”这一角度出发思考，公司好，你才会好，你所在的团队好，你才能好。</p><p></p><p>问：您觉得当下，运维行业有没有哪些普遍做法其实是错的？为什么？</p><p></p><p>答：我暂时没有深入的思考过行业有什么做法是不对的，每家都有自己的现实问题，不好评价。</p><p></p><p>不过，有一点我想提一下，我从没有把自己限制在运维工作上，运维是我擅长的领域，是帮助公司守住用户基本连接体验的基础，但我通常更关注公司的业务现在急需什么？公司最核心的用户他们需要什么？他们需要什么我们就优先做什么，因为在我的视角里，保障服务稳定的工作，每家公司都欠了非常多的债，需要慢慢还。</p><p></p><p>问：当下一些火热的技术方向，包括FinOps、可观测性、chatGPT等，您对这些技术方向的发展有什么看法，是炒概念还是有真价值，运维人员是否应该做出什么样的应对举措？</p><p></p><p>答：我个人觉得这些方向都很好，如果大家只放在嘴上谈谈，那就是炒概念，只有实际做出来，才是先进的生产力。这些内容过去在百度时就做出不错的效果，或许在一个体量很大的环境里更容易实现，因为对应的数据量、人才厚度都会更充足。但如果有人只有100台机器，还谈FinOps，那可能真是炒一炒概念，其他也同理。</p><p></p><p>问：随着云的发展，传统的只做Ops的运维岗位长期来看必将消亡，您是否认可这个观点？对于这类朋友的转型路径您有没有什么建议？</p><p></p><p>答：运维的岗位不会消失，需求也会越来越重，但是否是人来做确实需要好好想想了。</p><p></p><p>一个软件工程中，运行维护是非常关键的环节，但这个环节是人来做，还是机器来做，要看科技的发展，就跟上面谈到的扫大街一样，只要有大街在，有人生活，扫大街这个需求是不会消失，且很旺盛，但替代的可能是无人的机器，现在已经逐渐替代为由人驾驶的扫路车。 我们要意识到这一点，同时也要认识到另外一点，运行维护是一个极其复杂的事情，它远比扫路复杂，从云服务这么多年的成熟过程大家就能感受到，这是一个漫长的过程，我更建议这个运维自己革自己命的过程，由运维自身来主导和设计，最终我们会成为“运营维护”这个产品的拥有者。</p><p></p><p>问：很多朋友在脉脉上吐槽公司绩效打分不公平，您对他们有没有什么建议？另外您作为管理者，能否分享一下您是如何设计绩效考核机制的？</p><p></p><p>答：这个话题比较敏感，也是运维同学非常期待讨论的话题，所以下面观点只是我个人职业生涯的经验，不代表任何公司观点。</p><p></p><p>以下是我个人感悟，绩效是自己赚来的，谈你绩效好不好，就要看你为公司带来多少突出的业绩贡献，你通过自己努力让自己的本职工作发生哪些质的变化，绩效通常是相对的排序，因此是相对公平，很难做到绝对的公平。</p><p></p><p>我们在谈论绩效的时候不妨和公司的老板们换位思考下，一个是为公司赚钱的，一个是为公司守住基本用户体验花钱的，只有赚来更多钱才能给大家发工资，因此结果显而易见。</p><p></p><p>当然这也和大家吃的苦不一样有关，有人说人生有五种苦，第一种是体力的苦，强调拼加班，很多传统运维工作都能吃这个苦；第二种是思考的苦，拼的是你布局的周密性，做事的精细程度；第三种是耐得住寂寞的苦，要一个人不断的默默的学习很多知识，人家吃喝玩乐的时候，他自己耗费了大把时光在不断地学习新知识；第四种是尊严的苦，为了陪客户老脸都不要，见谁都是自己的祖宗一样点头哈腰的伺候；第五种可以让大家去猜一猜。不要说自己什么苦都能吃，不同角色吃的苦不一样，有个好的心态是身体健康的基础。</p><p></p><p>最后，我祝愿大家都能通过自己的努力获得好的绩效。以上观点只是我个人经验，不代表任何公司。</p>",
    "publish_time": "2023-03-13 14:31:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]