[
  {
    "title": "消除了“泡沫”的数据中台如今怎么样了？《2023数据中台调研报告》问卷启动",
    "url": "https://www.infoq.cn/article/eQ89yuYWOTwOC0kVCEwu",
    "summary": "<p>自2020年<a href=\"https://www.infoq.cn/article/4PXxXJ*ZOmPVlAtB2Ttb\">《数据中台已成下一风口，它会颠覆数据工程师的工作吗？》</a>\"文章发布以来，数据中台的热度从火速蹿红到稳定落地，正处于全面开花阶段，正如两年之前作者预测的那样，数据中台的生命力和商业化能力远高于业务中台。</p><p></p><p>两年过去，众多企业都已经建设了数据中台，数据中台成为了一个新的本土细分市场。虽然业界对数据中台的争议声也接连不断。但是，仍然有不少企业把数据中台始终作为数字化能力建设的关键部分。那么，到底数据中台是否产生了业务价值？企业数据中台的建设究竟处于什么样的进展？企业对此是否有了更进一步的需求和洞见?</p><p></p><p>为了为深入了解中国企业对数据中台的最新认知和需求，以及数据中台在企业数字化转型中的现实价值，现发放《2023数据中台的数据调研》，调研收集的数据将被统计分析写入《2023数据中台调研报告：让数据的价值被看见》。扫码参与问卷填写，即可获得报告全文。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/de/26/dea9d848cc7ef3ebf5b6d5ece3dc8126.png\" /></p><p></p>",
    "publish_time": "2023-01-03 09:21:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "下一代前端语言之争，JavaScript 要被新语言反超？",
    "url": "https://www.infoq.cn/article/4Et0wIAxWFbVez5wbMCC",
    "summary": "<p></p><p>假如大家正在编写<a href=\"https://www.infoq.cn/topic/Front-end\">前端</a>\"代码，那么会选择哪种编程语言？目前来看，最有希望的选手主要有三个：首先是最常规的 <a href=\"https://xie.infoq.cn/article/9c44692def1e3a908c9121c8f\">JavaScript</a>\"，然后是能编译为 <a href=\"https://www.infoq.cn/article/Q02piNCyvfAM0YJobfVq\">WebAssembly</a>\"（Wasm）的语言，最后则是能编译成 JavaScript 的语言。</p><p></p><p>常规 JavaScript 需要的配套工具最少，但代价是调试起来相当麻烦，代码可读性也差。虽然选择 JS 确实门槛较低，不过除了一味痴迷“极简主义”的铁粉以外，我个人觉得这个选项只能说一般。</p><p></p><p>能编译为 Wasm 的语言虽然越来越多，但总体上还是新生事物。这些语言往往带有大量的二进制文件，因为其中大多需要配合额外的运行时。Interop 距离发展成熟还差得远。另外，即使两种语言都能编译成 Wasm，也不代表它们之间就能良好实现互操作。再有，这个阵营的生态储备还远远比不上积累了几十年的 JavaScript DOM 库。在 Wasm 这边，React 和 Svelte 应该是最好的选项了。大家千万别误会，我可不是在唱衰 Wasm。它已经拥有专属于自己的表现舞台，如果大家想要在浏览器中运行高计算量原生代码，但 Wasm 就是最完美的选项。可如果不是这种情况，我个人不太推荐用它进行日常前端开发。</p><p></p><p>最后剩下的就是能编译成 JavaScript 的语言了。但这个阵营形成了一家独大的局面，其中的老大我们稍后会具体讨论。相比之下，ClojureScript、Elm、ReScript、Dart 等语言都形成了颇具体量的社区，但未来市场份额还能不能进一步扩大尚未可知。这就很尴尬了，毕竟能编译成 JavaScript 的语言代表的基本就是浏览器上的最佳编程体验。在它们的支持下，我们既能享受 JS 所不具备的良好功能，比如静态类型、强类型、不变性、宏等，同时也能通过 bindings 支持 JS 及其广泛的生态系统。而且，它们还不需要笨拙的大型运行时。</p><p></p><p>由于 Wasm 的存在，我怀疑 JS 编译阵营会有所保留，毕竟很多人觉得前者才是浏览器上的最佳编译目标。我其实并不同意这种观点，能编译成 JavaScript 的语言还是越多越好。总之，我想借这篇文章跟大家聊聊现有及未来可能出现的前端语言，应该朝着哪个方向发展。</p><p></p><h4>TypeScript 还行吗？</h4><p></p><p></p><p>这就是我前文提到的 JS 编译阵营中的“老大”——TypeScript。TypeScript 是种很棒的语言，显著改善了开发者体验。它还新增了安全层，促进工具质量提升，并大大降低了使用门槛。考虑到生态系统的繁荣现状以及对 JS 类型检查难题的妥善解决，TypeScript 确实取得了非凡的成就。</p><p></p><p>当然，也有不少针对 TypeScript 的非议值得关注。首先就是这门语言的性能和健全性问题。需要注意的是，TypeScript 团队其实很清楚这两大顽疾，而其根源是开发团队在项目之初做出的明确权衡。在我看来，这些权衡是当时为了提高执行效率而做出的正确选择。</p><p></p><p>话虽如此，但性能确实是 TypeScript 最受诟病的问题。TypeScript 是自实现的，而且这种实现非常复杂。它的类型系统本身可以算是种迷你编程语言，这导致类型检查的速度极其缓慢。</p><p></p><p>第二个问题就是健全性。这事的讨论热度没那么高，但在编程爱好者群体内部还挺受关注。概括来讲，TypeScript 一身都是“缺陷”——allwJs 配置选项、any 类型和 intersection 类型，其类型系统根本无法保证代码的类型安全。换言之，我们编写的 TypeScript 很可能会触发运行时 bug。另外，除了极其简单的场景之外，TypeScript 还缺乏可靠的类型推断，所以开发者在很多地方都得明确标出类型注释。</p><p></p><p>但同样的，这两点也是项目权衡的结果。</p><p></p><p>引导编译器的存在对于 TypeScript 的内部测试至关重要，这能帮助项目开发者理解 TypeScript 这种语言用起来的真实感受。具体来讲，项目团队要体验如何编写大型 JS 代码库，再逐步采用代码库中的类型。在健全性方面放松一点，开发者才能在现有 JS 代码库中逐步引入 TypeScript，也能轻松使用 any 类型来直接摆脱类型系统的束缚。</p><p></p><p>光是这部分就够单独写篇文章了。在我看来，TypeScript 可能是第一种更多关注开发者体验、而非自身语义的编程语言。它并没有添加任何运行时结构、不插手性能，而是添加了一套类型系统，并让整个语言社区接纳了这种不用类型也行、没高质量工具也行，还不强调正确性的生态氛围。这简直是个不可思议的壮举。</p><p></p><h4>下一代前端语言是什么样？</h4><p></p><p></p><p>所有这一切都表明，TypeScript 早在十年前就做出了一众对自身产生巨大影响的权衡。而随着时间推移，我觉得是时候通过新语言再做一轮权衡了。确切来讲，我们需要一种具备健全性、类型推断和更快编译速度的语言。</p><p></p><p>要求明确了，但我们该拿什么来换？</p><p></p><h5>健全性</h5><p></p><p></p><p>先从健全性说起。下一代语言不再努力对各种 JS 模式进行类型检查，而是以独立语言的形态通过更简单的类型系统将代码编译成 JS。它会将现有 JS 代码视频外部互操作对象，对 JS 代码执行显式运行时类型检查，而且依靠不同的原生语言来实现。</p><p></p><p>为什么要这样？首先，我个人特别喜欢具备既健全、又相对简单的类型系统的语言。我希望这种语言能够在浏览器中运行良好，而且能顺畅适配现有 Web 生态系统。那些能编译成 Wasm 的语言经常忽略 Web 生态系统中的其余部分，总想在浏览器中建立起基于像素的原生 UI。我觉得这个想法不错，只是跟我的观念相悖。我只想用下一代语言开发常规网站；我不想要纯函数式语言，而更倾向于跟 C 的老派风格相似的语言（对不起了，Elm！）；我希望这种语言能体现出我在工具设计上的想法。</p><p></p><p>那为什么下一代前端语言应该诞生在现在这个时间点？俗话说得好，种一棵树最好的时机是十年前，其次是现在。这十年来，JS 社区已经发生了很大变化。人们开始学习 TypeScript，也习惯于关注编译器并通过类型进行数据建模。现在，很多开发者开始使用 Rust、Swift 和 Kotlin 等语言，也意识到高质量工具的重要性。我不是说十年前的人们会抵抗强调类型安全的语言，但那时候的普及难度确实更高。</p><p></p><p>明确表达了需求，有些朋友可能觉得这说的不就是 ReScript/ReasonML 吗？没错，确实有几分相像。但在理想情况下，我期待的下一代语言应该能对 JS 代码和特性进行显式运行时类型检查。运行时类型检查是达成良好互操作性的前提，这样我们就能更轻松地随意使用 JS 库。</p><p></p><p>同样地，我觉得 traits 对用户来说也很重要，它们可以跟其他语言特性映射起来，比如 Java 接口和 C++ 概念。这可太方便了，比如轻松通过 Display trait 输出任意类型。这类需求听起来简单，但确实能大大提升语言的可用性，消除“我该怎么输出这个？”或者“为什么 + 代表整数加法，而 +. 代表浮点加法？”之类特别劝退的问题。再有，我还想去掉一些没用的东西，比如对象、链表、多态变体等。这些都是 ReScript/ReasonML 做不到的，而且我上次试用的时候，ReScript 的开发体验和错误消息也没给我留下深刻印象。</p><p></p><p>也就是说，我不排除 ReScript 代表着正确方向的可能性。毕竟上次尝试已经是几年之前了，也许是我记错了、也许它已经变得更好了。而且随着同 OCaml 的剥离，ReScript 确实成了很好的前端语言选项，我有必要再确认一下。</p><p></p><h5>类型安全</h5><p></p><p></p><p>对于下一代前端语言，我希望能用一种更系统的方法实现类型安全。具体来说，我觉得用 Rust 处理非安全代码块的方式实现 JS 互操作性的好办法。基本上，在调用 JS 的过程中，我们需要将代码打包在一个非安全代码块中。这会是个明确的标志，提醒开发者要认真阅读这段代码。接下来的目标，就是在这些指向 JS 库的非安全代码块上实现 bindings。起初这个过程需要手动完成，但后续应该会有类似 bindgen 和 cxx 的工具出现。</p><p></p><p>在 JS 中使用非安全代码块好像有点反直觉，毕竟 JS 的安全性又不像 C 那么糟糕。但很多人似乎没意识到，安全的意义并不仅限于安全本身。所谓安全，是指可以任意使用一个值、而不必担心其是否为 null 的保障能力。所谓安全，是在不致引入 Bug 或混乱的前提下保证可变性的能力。Rust 的非安全块概念允许用户既维护自己的安全区，又能与大量非安全代码交互。下一代浏览器语言也该做到这一点。</p><p></p><p>至于运行时检查，我觉得它仍然物有所值。我们已经在 JS 当中进行过大量模式验证，只是以往只能通过 zod 这类临时性机制完成。在下一代前端语言中，这类功能也许是在运行时出错时对语言类型执行自动转换，也许能对 JS 值进行模式匹配。</p><p></p><p>对于 WebAssembly，我还是很看好它的发展前景的。但要说它一定能成为浏览器的通用运行时，我个人还是持怀疑态度。也许未来我的态度会有转变，但目前我更多是将 Wasm 看作一种硬件加速器。</p><p></p><p>当用户的高强度计算任务要求调用固定宽度整数和静态函数时，大家就会使用 Wasm；这就像在需要执行并行计算时，大家会选择 GPU 一样。在这样的模型中，我看到了支持异构编译的潜力——其中部分代码可以被编译成 JS，另一部分代码则可编译为 Wasm。这项工作可以由用户显式完成，由分析自动完成，甚至可以即时完成。通过对 JS 和 Wasm 代码的同时控制，编译器就能最大限度减少跨越语言边界的次数，从而提高性能水平。我觉得未来甚至可以有某种机制将部分代码发送给 WebGPU。</p><p></p><p>在这样的模型之上，也许我们可以更轻松地编写计算密集型程序，比如机器学习模型、电子游戏和渲染软件。</p><p></p><p>这种对 Wasm 和 JS 进行分别编译的概念，可以在下一代前端语言中体现出来。我希望其中能有显式整数和浮点类型，最好还能有 Rust 中 usize 那样的显式索引类型。这样如果需要把代码编译成 Wasm，新语言就能利用 Wasm 的固定宽度整数。</p><p></p><p>还有另一种可能性，就是为语言创建一个子集，在这里整合闭包、垃圾收集等动态特性以提升 Wasm 编译质量。要跟这个子集交互，开发者需要使用 unsafe 代码块，比如 strict 块，或者让该子集通过 dynamic 块跟外部代码交互。这些都是假设，但我觉得其中确有探究的价值。</p><p></p><h5>具体实现</h5><p></p><p></p><p>这种新语言可能会用 Rust 来实现。毕竟我个人是 Rust 的粉丝，而且相信代数数据类型、相对更高的代码性能、受限但可用的可变性，以及比较丰富的库组合足以支撑起一套优秀的编译器。</p><p></p><p>如果 Wasm 后续发展得够好、性能几乎逼近原生水平，那我也会考虑使用由编译为高速 Wasm 代码的语言子集来引导编译器。但这应该不着急，毕竟一个 Rust 编译器应该就够用好多年了。</p><p></p><h4>总结</h4><p></p><p></p><p>大家可能已经注意到，类型安全和 Wasm 部分其实就是在从系统语言（例如非安全概念和硬件加速）中汲取灵感，再把它们应用到基于浏览器的语言当中。这是设计使然，毕竟不少最有趣的编程语言都是从系统层面衍生出来的。我只希望这些好点子也能在浏览器上有所体现。</p><p></p><p>这里我要澄清一下，我所指的下一代前端语言绝不是单一语言，我希望能有多种语言齐头并进、朝着前面提到的方向共同探索。我想激励更多朋友在浏览器语言领域不断创新。当然，我个人也会参与其中，目前正在研究的是名叫 vicuna 的实现方案，但还处于非常早期的阶段。</p><p></p><p>原文链接：</p><p><a href=\"https://uptointerpretation.com/posts/the-next-browser-language/\">https://uptointerpretation.com/posts/the-next-browser-language/</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-01-03 09:46:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "了解下 Rust 的所有权机制",
    "url": "https://www.infoq.cn/article/9f588494f0f8634aaabb48cdc",
    "summary": "<p></p><h2>what's ownership?</h2><p></p><p>常见的高级语言都有自己的 Garbage Collection（GC）机制来管理程序运行的内存，例如 Java、Go 等。而 Rust 引入了一种全新的内存管理机制，就是 ownership（所有权）。它在编译时就能够保证内存安全，而不需要 GC 来进行运行时的内存回收。</p><p></p><p>在 Rust 中 ownership 有以下几个规则：</p><p></p><p>每个值都有一个 woner（所有者）在同一时间，每个值只能有一个 owner当 owner 离开作用域，这个值就会被丢弃</p><p></p><h2>Scope (作用域)</h2><p></p><p>通过作用域来划分 owner 的生命周期，作用域是一段代码的范围，例如函数体、代码块、if 语句等。当 owner 离开作用域，这个值就会被丢弃。</p><p></p><p>example:</p><p></p><p><code lang=\"rust\">fn main() {\n    let s = String::from(\"hello\"); // 变量 s 进入作用域，分配内存\n\n    // s 在这里可用\n\n} // 函数体结束，变量 s 离开作用域，s 被丢弃，内存被回收\n</code></p><p></p><h2>ownership transfer（所有权转移）</h2><p></p><p>和大多数语言一样，Rust 在栈上分配基本类型的值，例如整型、浮点型、布尔型等。而在堆上分配复杂类型的值，例如 String、Vec 等。所以，这里就引入了两个概念，move 和 clone。</p><p></p><h3>move</h3><p></p><p>move 操作会将变量的所有权转移给另一个变量，这样原来的变量就不能再使用了。这里需要注意的是，move 操作只会发生在栈上的值，因为在堆上的值是不可复制的，所以只能通过 clone 操作来复制。</p><p></p><p>example:</p><p></p><p><code lang=\"rust\">fn main(){\n    let s1 = String::from(\"hello\");\n    let s2 = s1;\n    print!(\"s1 = {}, s2 = {}\", s1, s2);\n}\n</code></p><p></p><p>在上面的代码例子中，如果你执行就会在编译时报错：</p><p></p><p><code lang=\"bash\">  --&gt; src/main.rs:11:32\n   |\n9  |     let s1 = String::from(\"hello\");\n   |         -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait\n10 |     let s2 = s1;\n   |              -- value moved here\n11 |     print!(\"s1 = {}, s2 = {}\", s1, s2);\n   |                                ^^ value borrowed here after move\n</code></p><p></p><p>编译器提示我们，s1 在赋值给 s2 时发生了 move 的操作，它把字符串 hello 的所有权移交给了 s2，此时 s1 的作用域到这里就结束了，所以后面再使用 s1 就会报错。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f9b3b9f97745f9f3cd430cc33d645e7.jpeg\" /></p><p></p><h3>clone</h3><p></p><p>clone 操作会将变量的值复制一份，这样原来的变量和新的变量就都可以使用了。这里需要注意的是，clone 操作只会发生在堆上的值，因为在栈上的值是可复制的，所以只能通过 move 操作来转移所有权。</p><p></p><p>example:</p><p></p><p><code lang=\"rust\">fn main(){\n    let s1 = String::from(\"hello\");\n    let s2 = s1.clone();\n    print!(\"s1 = {}, s2 = {}\", s1, s2);\n}\n</code></p><p></p><p>我们对 s1 进行 clone 操作，这样 s1 和 s2 都可以使用了，而且 s1 的所有权也没有被转移，所以后面还可以继续使用 s1。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/acca66b7e0e92473e94ddfdda665d3d6.jpeg\" /></p><p></p><h3>copy</h3><p></p><p>如果一个类型实现了 copy 这个 trait，使用它的变量不会移动，而是被简单地复制，使它们在分配给另一个变量后仍然有效。</p><p></p><p>example:</p><p></p><p><code lang=\"rust\">fn main() {\n    let x = 5;\n    let y = x;\n    print!(\"x = {}, y = {}\", x, y);\n}\n</code></p><p></p><p>当 x 赋值给 y 后，x 和 y 都可以使用，而且 x 的所有权也没有被转移，所以后面还可以继续使用 x。这是因为 i32 这个类型实现了 copy 这个 trait，所以 x 的值被复制了一份，所以 x 和 y 都可以使用。</p><p></p><p>以下这些数据类型实现了 copy 这个 trait：</p><p></p><p>所有的整数类型，例如：u32、i32。布尔类型，bool，有 true 和 false 两个值。所有的浮点数类型，例如：f64、f32。字符类型，char。元组，当且仅当它们的元素类型都实现了 copy 这个 trait。例如，(i32, i32) 实现了 copy，但是 (i32, String) 就没有实现。</p><p></p><h2>References and Borrowing（引用和借用）</h2><p></p><p>我们将创建引用的动作称为借用。就像在现实生活中一样，如果一个人拥有某样东西，你可以向他们借用。完成后，您必须将其归还。你不拥有它。引用有以下几个规则：</p><p></p><p>在任何给定时间，你可以拥有任意数量的引用，但是只能拥有一个可变引用。引用必须总是有效的。</p><p></p><p>example1:</p><p></p><p><code lang=\"rust\">fn main() {\n    let s1 = String::from(\"hello\");\n    let len = calculate_length(&amp;s1);\n    println!(\"The length of '{}' is {}.\", s1, len);\n}\n\nfn calculate_length(s: &amp;String) -&gt; usize {\n    s.len()\n} // s 作用域失效，但是由于 s 是一个引用，没有所有权，所以不会发生任何事情\n</code></p><p></p><p>上面代码中，我们使用符号 &amp; 来创造一个变量的引用。这里我们使用 &amp;s1 来把这个引用指向 s1。函数 calculate_length 的参数 s 的类型是 &amp;String，这意味着它是一个指向 String 类型的引用，然后在函数体内获取 s 的长度并返回给调用者。</p><p></p><p>example2:</p><p></p><p><code lang=\"rust\">fn main(){\n    // 同一时间可以拥有多个不可变引用\n    let s1 = String::from(\"hello\");\n    let s2 = &amp;s1;\n    let s3 = &amp;s1;\n    println!(\"s1 = {}, s2 = {}, s3 = {}\", s1, s2, s3);\n\n}\n</code></p><p></p><h3>Mutable References（可变引用）</h3><p></p><p>可变引用指的是可以改变引用值的引用。在同一作用域中，同一时间只能有一个可变引用。</p><p></p><p>example:</p><p></p><p><code lang=\"rust\">fn main(){\n    let mut s = String::from(\"hello\");\n    change(&amp;mut s);\n    println!(\"{}\", s);\n}\n\nfn change(some_string: &amp;mut String) {\n    some_string.push_str(\", world\");\n}\n</code></p><p></p><p>上面代码中，我们用 mut 先创建了一个可变变量 s,然后使用 &amp;mut s 创建了一个指向 s 的可变引用。函数 change 的入参也是一个指向 String 类型的可变引用，这样我们就可以在函数 change 中改变 s 的值了。</p><p></p><p>example2:</p><p></p><p><code lang=\"rust\">fn main() {\n    let mut s = String::from(\"hello\");\n    let r1 = &amp;mut s;\n    let r2 = &amp;mut s;  // 在这里。编译器会报错，因为在同一作用域中，同一时间只能有一个可变引用。\n\n    println!(\"{}, {}\", r1, r2);\n}\n</code></p><p></p><p><code lang=\"bash\">  --&gt; src/main.rs:41:14\n   |\n40 |     let r1 = &amp;mut s;\n   |              ------ first mutable borrow occurs here\n41 |     let r2 = &amp;mut s;\n   |              ^^^^^^ second mutable borrow occurs here\n42 |\n43 |     println!(\"{}, {}\", r1, r2);\n   |                        -- first borrow later used here\n</code></p><p></p><h3>Dangling References（悬垂引用）</h3><p></p><p>悬垂引用是指引用一个不存在的值。在 Rust 中，这是不可能的，因为编译器会在编译时就检查这种情况。下面是一个例子：</p><p></p><p><code lang=\"rust\">fn main() {\n    let reference_to_nothing = dangle(); // 获得一个指向不存在值的引用\n}\n\nfn dangle() -&gt; &amp;String {\n    let s = String::from(\"hello\"); // s 进入作用域\n\n    &amp;s // 返回 s 的引用\n} // s 作用域结束，s 被丢弃，内存被释放\n</code></p><p></p><p><code lang=\"bash\">  --&gt; src/main.rs:51:16\n   |\n51 | fn dangle() -&gt; &amp;String {\n   |                ^ expected named lifetime parameter\n</code></p><p></p><p>因为变量 s 的作用域只在 dangle 函数内，当 dangle 函数返回 s 的引用时，s 已经被释放了，所以这个引用就是悬垂引用了。解决这个的方法是返回一个 String 而不是一个引用，这样 s 就不会被释放，而是把 s 的所有权转移给了调用者，也就不存在悬垂引用了。</p><p></p><p><code lang=\"rust\">fn dangle() -&gt; String {\n    let s = String::from(\"hello\");\n    s\n}\n</code></p><p></p>",
    "publish_time": "2023-01-03 10:28:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "反Twitter平台用户激增250万，这名29岁程序员如何凭一己之力扛住超8倍流量增长？",
    "url": "https://www.infoq.cn/article/j56P0ocglkyiKSBG6OGD",
    "summary": "<p></p><blockquote>作为 Twitter 的替代品，Mastodon 越来越受欢迎，但这个平台背后，只有一名全职员工。面对激增而来的用户，凭一己之力运营Mastodon是个什么感受？</blockquote><p></p><p>&nbsp;</p><p>Mastodon明显需要感谢Twitter的CEO马斯克。马斯克于10 月下旬正式接管 Twitter，Mastodon 用户数量就开始激增。仅在马斯克接掌Twitter的几天之后，Mastodon宣称其用户已从30万涨到了65万5千人。这段时间，马斯克削减开支、大幅裁员和对平台进行混乱更改，在这些神操作下，越来越多用户涌向了Mastodon平台，终于在12月底，每月活跃用户增加到了250万。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/48a75ba0d60999e8404b116ac2ee87fb.png\" /></p><p></p><p>除了普通用户，还有些组织也宣布入驻 Mastodon。12月20日， Firefox 浏览器的开发商<a href=\"https://blog.mozilla.org/en/mozilla/mozilla-launch-fediverse-instance-social-media-alternative/\">Mozilla宣布将开始运行Mastodon“实例”</a>\"，并计划于明年初进行公开测试，正式开启了在该平台上建立业务的热潮。</p><p>&nbsp;</p><p>资金也希望能够流入 Mastodon，据<a href=\"https://www.ft.com/content/de808736-2e05-4c3b-a53c-55b170ae9efd\">《金融时报》消息</a>\"，来自硅谷风险投资公司对 Mastodon充满了兴趣，但Mastodon创始人为了保持这个社交媒体平台的非营利地位，在近几个月里数次拒绝了他们的投资提议。</p><p>&nbsp;</p><p>Mastodon的名气不断上升，马斯克不得不承认链接到 Twitter 竞争对手是一个错误，他一度禁止掉了所有指向竞争对手服务的链接，还封禁了不少相关用户账户。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/63/634e4b4cac64d34955fe6e6b1562df06.png\" /></p><p></p><p></p><h2>用户多了，创始人的压力也更大了</h2><p></p><p>&nbsp;</p><p>Mastodon由一位名叫Eugen Rochko的德国程序员创立。2016 年，Twitter 平台将信息流改为采用算法驱动的乱序默认时间线，当时，Rochko刚从一所德国大学里获得计算机科学学位，因为对Twitter平台的不满，他决定实现一个自己的社交媒体。</p><p>&nbsp;</p><p>Mastodon被设计成为了一个开源的社交媒体平台，建立在 Fediverse 去中心化网络上，这是由数千个独立服务器连接的应用程序和网站构成的在线世界。目前 Fediverse 上大约有 5,700 台 Mastodon 服务器。该软件<a href=\"https://www.w3.org/TR/activitypub/\">使用名为ActivityPub</a>\"的协议，由 Ruby on Rails 后端、JavaScript 前端、Sidekiq 作业管理和 PostgreSQL 关系数据库组成。作为反Twitter平台，Mastodon 没有使用推荐算法，也不会尝试向用户推荐任何东西，也没有统一的审核机制，各分支可以按照自己的规则来决定哪些人可以加入、哪些内容可以发布。</p><p>&nbsp;</p><p>2017 年，Rochko发布了第一版代码，但用户增长一直十分缓慢，最开始Mastodon的追随者都是技术作家和黑客，这也是创建时的目标人群，而且对“权力下放”和“开源”的强调只会让最精通技术的人感到鼓舞和受到吸引。</p><p>&nbsp;</p><p>2021 年，美国前总统特朗普团队使用了 Mastodon 的源代码，开发了他们的新社交媒体平台，即所谓的 Truth Social，并获得了上百万的独立访问用户，这才让 Mastodon 名声大噪。特朗普团队最初还隐瞒了使用了 Mastodon 代码这一事实，但该行为违反了 GNU 开源协议。后来这一事实被 Mastodon 团队发现了，他们给 Truth Social 的开发人员发了律师函，特朗普团队才不得不公布了源代码。Truth Social 也是最著名的 Mastodon 实例。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2999d4126331441b397282c0f21394a.png\" /></p><p></p><p>作为该项目唯一一个全职员工，Eugen Rochko 曾说：“我们没有办法控制成功何时才能到来，只能不断为它奠定基础。”现在，因为马斯克的神操作，Mastodon用户激增，这位现年29岁的德国程序员也在几周之内突然再次闻名全球，同时也让他压力倍增。</p><p>&nbsp;</p><p>新增的上百万 Mastodon 帐户，使流行的服务器不堪重负，新手介绍、问题和投诉淹没了现有用户的时间线。自从马斯克接管 Twitter 以来，Rochko 一直在长时间工作以保持自己的服务器<a href=\"https://mastodon.social/\">Mastodon.Social</a>\"正常运行，同时还准备对 Mastodon 进行重大升级。</p><p>&nbsp;</p><p>今年11月，Rochko在接受 Wired 采访时表示，自从该网络上个月人气暴涨以来，他每天需要工作 14 个小时。</p><p>&nbsp;</p><p>与巨头竞争对手Twitter相比，Mastodon 的规模和结构看起来非常不同。在公司 2021 年的年度报告中，罗奇科是唯一的固定雇员，他在在德国东部一个小镇的家中编程，每月收入 2,400 欧元，团队中还有一些合同工。去年社区向运行 Mastodon 的非营利性组织<a href=\"https://www.patreon.com/posts/mastodon-annual-61911077\">捐款总计 55,000 欧元</a>\"，但该团队只花了 23,000 欧元。</p><p>&nbsp;</p><p>随着服务变得越来越流行， Mastodon 实例的托管成本也显着增加，Mastodon不得不扩大规模并为服务器支付更多费用。“但是 Mastodon和 Fediverse网络提供了将负载分散到多个不同参与者的能力。我可以在我的服务器上关闭注册，其他服务器和网络将立即接收正在尝试注册的人。”</p><p>&nbsp;</p><p>值得强调的是，跟 Twitter 是单一的一个网站不同，Mastodon 并不是一个单独的平台。它也不是一个实实在在的“东西”，不是由某个人或某个公司拥有。Mastodon 的核心是 Rochko 提供的开源软件系统，任何人都可以在自己的服务器上部署这样一个系统，运作自己的 Mastodon 服务，每个服务也可以称为“实例”(instance)。每一个 Mastodon 服务器，都是一个自给自足的“微型”社交网络，可以有它自己的主题，比如金融、数码、生活、职场等等。用户在注册 Mastodon 的时候，必须选择其中一个。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a3a0b8fecfed2f725d62d53da816f25.png\" /></p><p></p><p>当所有Mastodon服务器连接在一起，才形成了一个非集中性的网络。这也是去中心化平台的粉丝们所喜欢的一个特点，因为这种平台不会像Twitter那样容易被控制。</p><p>&nbsp;</p><p>虽然作为一个全球分布式现代应用程序，但激增的用户也会给维护这些服务器的人员和团队带来挑战。就像Rochko所说的，当压力增大时，有的服务器可以在不堪重负的情况下选择关闭注册，将压力分摊给其他服务器或网络。</p><p></p><h2>在压力下，运行Mastodon服务器的技术人</h2><p></p><p>&nbsp;</p><p>Jaz-Michael King就是其中一位运营Mastodon服务器中的人。他是一名威尔士人，经营一家美国互联网公司，业余时间运营着一个叫作Toot Wales的Mastodon服务器。与其他Mastodon服务器一样，在马斯克接管Twitter后，Toot Wales用户活跃度呈指数级增长。</p><p>&nbsp;</p><p>“在马斯克抱着水槽走进Twitter办公室之前，我们每天大概处理15万条消息……现在每天要处理300万到400万条消息。我们的注册用户是之前的3倍，活跃度也大大增加了。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0275530a5ea37df7075347bdb17cf37.png\" /></p><p></p><p>Toot Wales</p><p>&nbsp;</p><p>为了运营好Toot Wales，King选择了一家知名的托管公司mastohost。“我对技术了解得足够多，可以要求他们做出任何我想要的调整，但我还是尽量不去这么做。我的另一个原则是我们只运行主分支，不使用任何试验性的东西，也不使用不稳定的软件更新。我想要稳定的、功能性的、经过实战测试的代码。我在边缘部分做了一些改动——我以前会在前端做很多改动，最近升级到Mastodon 4.0后，我甚至把前端大部分东西都去掉了，所以它非常简洁。我想要知道我运营的是什么，想要知道什么是有效的，但我尽量不去动它。”</p><p>&nbsp;</p><p>他还对设计做了一些修改。主要是为了让非技术用户不会对Mastodon的用户界面感到厌烦——即使是最狂热的信徒也应该承认，与Twitter的用户体验相比，Mastodon的用户界面略显逊色。Mastodon的布局看起来有点像TweetDeck（一款Twitter客户端），有多个列。虽然这对于关注列表或标签搜索很有用，但它的实现不如TweetDeck的好。他说：“我所能做的是改进用户体验。我坚信我们不需要多列视图，我们是一个提供单列试图的服务器。”</p><p>&nbsp;</p><p>尽管他尽量不插手托管方技术上的事情，但自从发生Twitter收购事件以来，King一直忙于处理社区和内容方面的事情。“内容和社区管理，包括内容审核，一直是一个沉重的负担。我们需要巧妙地向人们描述我们为什么存在，以及为什么我们并非另一个特推——很多人以为Mastodon是在不久前创建的，目的是为了替代Twitter。因此，我们需要向人们解释他们使用的其实是几年前就有的东西，然后管理现有的和即将到来的社区期望，并找到中间地带。”</p><p>&nbsp;</p><p>King希望Mastodon在社区管理功能方面做出改进。首先，他希望“在产品中加入比现在更多的联邦和去联邦服务。”他指的是与其他服务器通信的方法，比如阻止清单。他想要一种在服务器之间“谨慎、合乎道德、谨慎地共享”信息的方法。</p><p>&nbsp;</p><p>作为一个开源软件项目，Mastodon的一个设计是不鼓励用户使用关键字搜索。默认情况下，你可以搜索话题标签，但（与Twitter不同）不能输入搜索关键字——比如“马斯克”——然后等待结果。这在不同的服务器之间的实现是不同的，有些服务器可以搜索到自己发的或参与的帖子，但从更大的范围来看，并没有结果。</p><p>&nbsp;</p><p>Mastodon的GitHub主页有一个关于搜索限制的争议。King对此表示，从Twitter迁移过来的用户期望他们能够搜索关键字，因为他们在Twitter上已经习惯了，但他仍然对此持抵制态度。Toot Wales的用户甚至不能搜索自己的帖子。King考虑过开启这个选项，但他担心这会对他的服务器造成影响。“我们可以开启搜索功能的增强特性，让用户可以搜索自己的内容。说实话，我认为它不值得浪费CPU资源。”</p><p>&nbsp;</p><p>撇开用户喜好不谈，King通过Toot Wales为威尔士在线社区提供了很好的服务。他没有从中赚到钱，这个网站的运营主体实际上是King创立的一个美国慈善机构。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://joinmastodon.org/zh/servers\">https://joinmastodon.org/zh/servers</a>\"</p><p><a href=\"https://github.com/mastodon/mastodon\">https://github.com/mastodon/mastodon</a>\"</p><p><a href=\"https://www.section.io/blog/mastodon/\">https://www.section.io/blog/mastodon/</a>\"</p><p><a href=\"https://www.wired.com/story/the-man-behind-mastodon-eugen-rochko-built-it-for-this-moment/\">https://www.wired.com/story/the-man-behind-mastodon-eugen-rochko-built-it-for-this-moment/</a>\"</p><p><a href=\"https://www.lemonde.fr/en/m-le-mag/article/2022/11/21/who-is-eugen-rochko-the-young-creator-behind-mastodon_6005052_117.html\">https://www.lemonde.fr/en/m-le-mag/article/2022/11/21/who-is-eugen-rochko-the-young-creator-behind-mastodon_6005052_117.html</a>\"</p><p><a href=\"https://www.euronews.com/next/2022/11/25/the-anti-elon-musk-meet-eugen-rochko-the-man-behind-twitter-rival-mastodon\">https://www.euronews.com/next/2022/11/25/the-anti-elon-musk-meet-eugen-rochko-the-man-behind-twitter-rival-mastodon</a>\"</p><p><a href=\"https://thenewstack.io/what-its-like-to-run-a-mastodon-server-in-the-musk-twitter-era\">https://thenewstack.io/what-its-like-to-run-a-mastodon-server-in-the-musk-twitter-era</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-01-03 11:09:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "平台工程中认知负荷的挑战",
    "url": "https://www.infoq.cn/article/doz4HmWsaZwH4hi6FqXX",
    "summary": "<p>在<a href=\"https://platformengineering.org/blog/cognitive-load\">最近的一篇文章中</a>\"，Syntasso首席运营官<a href=\"https://www.linkedin.com/in/paulalkennedy/\">Paula Kennedy</a>\"分享了她对开发团队所承受的不断增加的认知负荷的看法。她解释说，平台工程方式试图减轻开发团队的一些认知负荷，但这可能是以将认知负荷转移给平台团队为代价的。</p><p>&nbsp;</p><p><a href=\"https://en.wikipedia.org/wiki/Cognitive_load\">认知负荷理论（CLT）</a>\"由John Sweller于1988年首次提出，它认为人类的认知是工作记忆和长期记忆的结合。工作记忆的容量有限，由负责引导注意力和协调认知过程的多个组成部分组成。另一方面，长期记忆具有无限的存储容量，可以根据需要与工作记忆一起检索信息。</p><p>&nbsp;</p><p>CLT最初被设计为一种改善课堂教学的手段，但也有应用于软件开发。Sweller确定了三种类型的认知负荷：内在的、外在的和增生的。内在认知负荷是与手头任务相关的努力。在数学课上，这可能是2+2或尝试化简一个多项式方程。用教学术语来说，这通常被认为是不可变的。</p><p>&nbsp;</p><p>外在认知负荷产生于强加给执行任务的个人的任务本身及任务之外的要求。它可能包括分散注意力的信息、糟糕的指令或以不理想的方式传达的信息。例如，口头向某人提供配置负载平衡器的步骤，而不是以书面文档的形式提供。</p><p>&nbsp;</p><p>正如Psychologist World的<a href=\"https://www.psychologistworld.com/memory/cognitive-load-theory\">一篇文章</a>\"所述，增生认知负荷是：</p><p>&nbsp;</p><p></p><blockquote>通过构建模式生成，被认为是可取的，因为它有助于学习新技能和其他信息。</blockquote><p></p><p>&nbsp;</p><p>在认知科学中，模式是一种先入为主观念的心理构造，就像是一个代表世界各方面的框架。虽然内在认知负荷被认为是不可变的，但我们希望最小化外在认知负荷，并尽量最大化增生认知负荷。</p><p>&nbsp;</p><p>对平台工程中开发人员体验的关注是对负责整个产品开发生命周期的团队所承受的沉重认知负荷的一种回应。Puppet的区域首席技术官<a href=\"https://twitter.com/nigelkersten\">Nigel Kersten</a>\"<a href=\"https://www.youtube.com/watch?v=dFqp-p1-1ZI&amp;ab_channel=PlatformEngineering\">解释说</a>\"，实施完全自主DevOps团队的组织，尤其是大型企业，可能会对局部进行优化，但会以牺牲其他团队为代价：</p><p>&nbsp;</p><p></p><blockquote>这可以很好地为特定的价值流团队或应用程序进行局部优化。但它并不是针对整个组织进行优化，而是为审计人员、IT资产管理人员以及所有围绕成本控制和安全的治理问题创造了认知负荷。你如何从一个团队切换到另一个团队？所有这些都会变得非常非常复杂。</blockquote><p></p><p>&nbsp;</p><p>随着对开发团队需求的增加，我们开始触及可处理信息量的极限。沉重的认知负荷会对有效完成任务的能力产生负面影响。Kennedy指出：</p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>对于任何试图驾驭复杂技术环境的人来说，这是一场持续的斗争。每天都会有新工具发布，跟上新功能、评估工具、为工作选择合适的工具，更不用说理解这些工具如何相互交互以及它们如何适用于你的技术堆栈，这都是一项艰巨的任务。</blockquote><p></p><p>&nbsp;</p><p>这些活动与分配给业务流（stream-aligned）团队的关键任务无关，因此会干扰实现业务基本优先级的快速流动。对于业务流开发团队来说，交付业务价值是团队应该花费大量时间和精力的任务。对于大多数公司来说，像更新新的CI/CD工具或最新的安全威胁之类的任务对其所销售的产品来说并不是直接至关重要的。</p><p>&nbsp;</p><p>MYOB架构主管<a href=\"https://www.linkedin.com/in/evanbottcher/?originalSubdomain=au\">Evan Bottcher</a>\"将平台<a href=\"https://martinfowler.com/articles/talk-about-platforms.html\">描述</a>\"为：</p><p>&nbsp;</p><p>数字平台是自助服务API、工具、服务、知识和支持的基础，它们被分配为一个引人注目的内部产品。自主交付团队可以利用该平台以更快的速度交付产品功能，同时减少协作。</p><p>&nbsp;</p><p>这就是为什么开发人员的经验对于一个设计良好的平台来说是如此重要的原因。一个引入了额外外部认知负荷的平台，或者不是一个促进健康增生认知负载的模式，实际上会增加使用它的开发人员的认知负荷。一个不以最终用户为中心、不欣赏用户体验的平台将无法成功地改善它们的交付。</p><p>&nbsp;</p><p>正如Amenitiz的高级工程师<a href=\"https://www.linkedin.com/in/cristobalgarcia/\">Cristóbal García García</a>\"和Thoughtworks的技术主管<a href=\"https://www.linkedin.com/in/ctford/\">Chris Ford</a>\"<a href=\"https://martinfowler.com/articles/platform-prerequisites.html#ProductThinking\">所言</a>\"：</p><p>&nbsp;</p><p>你永远不要忘记，你正在开发的产品是为了取悦它们的客户——你的产品开发团队。任何阻碍开发人员顺利使用你的平台的因素，无论是API可用性方面的缺陷还是文档方面的差距，都会威胁到平台商业价值的成功实现。</p><p>&nbsp;</p><p>从认知负荷理论的角度看，愉悦感成为了一种限定平台为开发团队及其在完成任务的工作中所带来的认知负担的方式。正如Kennedy所描述的那样，平台团队的主要关注点是“提供‘开发人员愉悦”，同时避免技术膨胀，避免陷入构建不满足开发人员需求且未被采用的平台的陷阱。”</p><p>&nbsp;</p><p>她接着指出了铺砌路径（也称为黄金路径）的重要性：</p><p>&nbsp;</p><p>通过向开发人员提供黄金路径，平台团队可以鼓励他们使用业务首选的服务和工具。这有助于简化提供的工具数量，减少过多选项的认知负荷，并减少平台的技术膨胀。</p><p>&nbsp;</p><p>应用认知负荷理论，铺砌路径是一种增加增生认知负荷的方法，它为开发团队提供了一种模式，以更好地理解他们所处的问题空间。</p><p>&nbsp;</p><p>在<a href=\"https://twitter.com/matthewpskelton/status/1581191123300913152\">最近的一条推文</a>\"中，《团队拓扑》的合著者<a href=\"https://www.linkedin.com/in/matthewskelton/\">Matthew Skelton</a>\"回应了这种观点以及遵循良好模式所能带来的价值：</p><p>&nbsp;</p><p>如果“平台工程”中最重要的部分是维护一个高质量的wiki，它具有经过验证的共情模式，以供业务流团队遵循，那该怎么办？</p><p>&nbsp;</p><p>在教育学领域，有大量的研究表明，提供实际的例子有助于提高学习。正如Dan Williams<a href=\"https://set.et-foundation.co.uk/resources/the-importance-of-cognitive-load-theory\">所指出</a>\"的那样，</p><p>&nbsp;</p><p>这些步骤为学习者提供了指导和支持，帮助他们创建如何解决问题/任务或“好”的心理模型。另一方面，发现或基于问题的学习会给工作记忆带来负担，因为学习者没有足够的先验知识来支持他们的学习。</p><p>&nbsp;</p><p>铺砌路径和经过验证的模式有助于为开发团队提供从包含合规性和治理等领域的整体问题空间中获得良好的外观。由于大多数开发工作都是模仿基于问题的学习，因此开发团队的认知负荷已经非常高了。铺砌路径，以及相关的平台工具，可以简化问题空间，减少外在认知负荷。</p><p>&nbsp;</p><p>Kennedy确实担心，在给团队分配建立这个平台的任务时，我们不仅仅是均匀地分散认知负荷，而是把它推给了平台团队。她注意到</p><p>&nbsp;</p><p>这些团队已经开始负责提供开发人员体验了，但由于需要整合许多工具，以及合规性和治理等其他问题，他们面临着巨大的认知负荷。这通常是一个投资不足的团队，但他们还要负责提供支持客户价值交付的平台。</p><p>&nbsp;</p><p>Kennedy想知道，除了当前对开发人员体验的关注之外，我们是否还应该讨论并改善平台工程师的体验。</p><p>&nbsp;</p><p>InfoQ与Kennedy进行了坐谈，更详细地讨论了这篇文章。</p><p>&nbsp;</p><p>InfoQ：通过将负担转移到平台工程师身上，你表明我们实际上是把认知负荷往下推，而不是把它均匀地分摊出去。这感觉就像是我们在试图解决应用程序工程师当前的困境时，引入了一个新的问题。你建议我们如何确保平台团队不会超载呢？</p><p>&nbsp;</p><p></p><blockquote>Paula Kennedy：在过去几年中，有很多关于如何改善开发人员体验或“DevEx”的讨论和关注，以使开发人员更容易交付并减轻他们的认知负荷。不幸的是，这种认知负荷并没有消失，而且通常只是转移给其他人来管理了，比如平台工程师。&nbsp;为了帮助减轻这一负荷，我很乐意看到有人谈论平台团队体验以及我们如何改进它。无论平台团队是管理云提供商、运行现成的PaaS，还是在Kubernetes之上构建了自己的平台，我认为我们都需要为这些平台团队提供更多的资源和工具，以使它们能更容易地规划支持其组织的平台。随着对平台工程主题的讨论越来越多，我很高兴看到出现了帮助解决这一挑战的模式和工具。</blockquote><p></p><p>&nbsp;</p><p>InfoQ：你注意到，平台团队“负责提供支持客户价值交付的平台”，但通常投资不足。你认为这是为什么？平台团队可以做些什么来纠正这一问题？</p><p>&nbsp;</p><p></p><blockquote>Kennedy：根据我的经验，平台工程通常是有机发展，而不是刻意组建的平台团队，我们至少在三个方面看出这一点。&nbsp;首先，当组织接受DevOps文化并使团队能够自主地将其软件交付到生产中时，这些DevOps团队最终会在无需任何额外资源的情况下管理平台级的关注点。&nbsp;其次，一些组织认为任何内部平台都是基础设施团队的责任，被视为另一个需要最小化的基础设施成本。&nbsp;最后，我看到一些组织引入了一个由供应商支持的平台即服务，希望它能够解决所有的内部平台挑战，并且只需要最少的维护，因为一切都“开箱即用”的——但事实并非如此。&nbsp;在所有这些情况下，都没有理解管理内部平台所需的技能和资源，从而导致投资不足。&nbsp;值得庆幸的是，我们看到越来越多分享平台工程和平台团队可以带来的好处的资源和经验。《团队拓扑》是一本我经常推荐的书，因为它提供了一个词汇表来描述如何通过明确的团队职责和减少团队之间的摩擦来实现价值在整个组织中的流动，并最终流入客户手中。&nbsp;在《团队拓扑》中，作者主张拥有一个支持多个业务流团队的平台团队，这些团队应该协作来理解彼此的需求并建立同理心，同时推动X-As-a-Service模式，在这个模型中，平台团队确保业务流团队能够自助提供所需的工具和服务。随着越来越多关于平台团队为软件交付生命周期所带来的价值示例被公开分享，公司开始认识到投资于该团队的重要性，以确保他们拥有正确的技能和工具来支持更广泛的组织。&nbsp;平台团队还可以采取措施在内部展示其价值，方法是通过考虑对其组织非常重要的指标或关键绩效指标（KPI），以及他们的工作是如何有助于改进这些指标的。这可能看起来像是运行价值流映射练习，确定哪里存在浪费或重复，并演示平台团队如何提供集中服务来改善这一点。&nbsp;如果合规性是一个组织的关键问题，那么平台团队可以推动与合规团队和应用程序团队之间的密切合作，通过内置的合规步骤来创建无摩擦的生产路径，确保更快、更合规地交付软件。内部指标或KPI在很大程度上与具体环境密切相关，但通过旨在度量对业务重要的内容，平台团队可以随着时间的推移证明其价值及其所做的改进。</blockquote><p></p><p>&nbsp;</p><p>InfoQ：你提到DevOps在试图纠正因Dev和Ops分离而产生的问题时，导致开发人员背负了更多的认知负荷。平台工程范式正试图通过将部分负荷转移到一个自助平台上来解决这一问题，该平台可以处理将代码投入生产并处理支持代码的大部分负荷。我们是否正面临着重新引入DevOps试图打破的孤岛风险呢？</p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>Kennedy：“DevOps”这个术语对不同的人来说有着不同的含义，尽管这个术语已经出现了十多年，但它仍然经常引起混淆。就我个人而言，我喜欢引用Patrick Debois在2010年的定义：&nbsp;Devops运动是周围着这样一群人建立的，他们相信适当的技术和态度的结合应用可以彻底改变软件开发和交付的世界。&nbsp;DevOps运动的核心绝对是减少孤岛，增加团队之间的沟通和同理心，以及提高自动化，并努力实现持续交付。对我来说，拥有一个内部平台团队只是DevOps的自然演进，尤其是规模上的DevOps。&nbsp;平台团队的成员负责他们的内部平台产品，他们既需要开发该平台以满足其用户（应用程序团队）的需求，也需要日常运维这个平台。&nbsp;对于专注于向最终客户提供功能的软件开发人员，他们负责开发其软件，并使用平台团队支持的自助服务工具进行操作。在这个模型中，每个人都在做“DevOps”，即每个人都在开发和运维自己的软件。但要想真正发挥作用，避免出现更多的孤岛，还需要考虑一个重要的文化因素，这就是平台团队将其平台视为产品的思维方式转变。&nbsp;在过去的几年里，我曾多次谈到这一问题，但这取决于平台团队在组织中的演变，这可能会带来巨大的挑战。当将内部平台视为产品时，这包括理解用户需求（应用程序团队）、分批交付以寻求反馈、提供高质量的用户体验以取悦开发人员、内部营销和平台宣传等等。只要平台团队接受这种思维方式，我们就能看到显著的好处，如Puppet 2021DevOps状态报告（Puppet State of DevOps Report 2021）等行业研究所证明的那样：“并不是每个平台团队都会自动获得成功，但成功的团队会将它们的平台视为一种产品。”</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/cognitive-load-platform-engineering/\">https://www.infoq.com/articles/cognitive-load-platform-engineering/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/7porVp7qVF03BVc2tDd6\">DevOps 已死，平台工程才是未来</a>\"</p><p><a href=\"https://www.infoq.cn/article/HQDbVRQq70wp0XLdSmuG\">滴滴工程效能平台建设之路</a>\"</p>",
    "publish_time": "2023-01-03 11:50:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "第一批AI绘画公司已经倒闭了",
    "url": "https://www.infoq.cn/article/7JW5mXSgSzUD33WLAJC6",
    "summary": "<p></p><blockquote>AI 绘画，路在何方？</blockquote><p></p><p></p><p>AIGC（<a href=\"https://www.infoq.cn/article/k1EU1cHr1FflCxRuATtv\">生成式 AI</a>\"）作为 2022 年最火热的概念之一，已然成功掀起了内容生产力变革。其中，AI 绘画更是在社交媒体上获得了大量的关注，多款应用火爆出圈。这一赛道的入局者不仅包括谷歌、Meta 这样的互联网科技巨头，还不乏不少初创公司。</p><p></p><p>资料显示，2022 年，AI 绘画捧红了多个初创公司。不过，并不是所有的 AI 绘画公司都能成功站在风口起飞，在激烈的竞争下，已有公司出局。</p><p></p><h2>AI 绘画公司 StockAI 倒闭</h2><p></p><p></p><p>2022 年 12 月 28 日，AI 绘画公司 StockAI 在 Twitter 上发帖称，目前的 StockAI 平台将正式关闭，用户订阅计划将被取消，并根据账户剩余时间进行退款。在 2023 年 1 月 15 日之前，用户仍可访问账户、发票、购买历史和收藏夹。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b1305c878b2f49d0a0f9cc97e58e3026.jpeg\" /></p><p></p><p>StockAI 创始人 Danny Postma 表示，运营一家像 StockAl 这样由人工智能驱动的初创公司成本很高，当前的付费用户基础无法支付这笔费用，所以公司不得不做出改变。</p><p></p><p>据了解，StockAI 网站内包含数千张由 AI 生成的照片，用户可以通过描述自行创建照片。所有照片都可以免费下载使用，可以用于商业及非商业用途，无需注明来源，但禁止用户将平台生成的图片用于二次销售。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dd/dd9f73db38f68e50aceaa85d8004a634.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90efce76c4a2588de4bcac723986a0ec.png\" /></p><p></p><p>虽然 StockAI 上的部分照片仔细观看会稍显违和，但整体来看风景、城市、建筑类型的照片质量还是比较高的，画面细腻显眼。此外，在“自定义照片”中，用户也可以自行设定照片的各种修改，包括：照片尺寸比例、镜头和光圈、位置场景、距离特写、快门速度、时间天气、照明等等，其中大部分功能都是免费的。</p><p></p><p>公开信息显示，StockAI 成立于 2022 年 9 月，从成立到关闭仅有 4 个月时间。当前，距离 StockAI 关闭仅剩下十余天，未来，StockAI 也许会以新的形式再和用户见面。</p><p></p><p>据 Danny Postma 介绍，StockAI 将在 2023 年一季度推出一个新平台，模式将从用户生成内容转向由平台自己策划的 AI 生成内容。有用户为 StockAI 送上祝福：“祝你生意兴隆。我下载了一些背景和一些非常有创意的图像与经典的 AI 错误，仍然有很多魅力。谢谢你的图片。”</p><p></p><h2>AI 绘画市场规模超 600 亿元，机遇与乱象齐飞</h2><p></p><p></p><p>2022 年，AI 绘画实火。不少初创公司迅速蹿红，强大的“吸金能力”使其声浪愈响。</p><p></p><p>2022 年 10 月 17 日，英国开源人工智能公司 Stability AI 宣布获得 1.01 亿美元融资，投后估值达 10 亿美元，而它的产品正是爆火的 AI 绘画模型——Stable Diffusion。10 月 18 日，AI 内容创作平台 Jasper 宣布获得 1.25 亿美元首轮融资，估值达 15 亿美元。该平台的核心产品 Jasper AI 除了能通过 AI 绘画，还能生成社交媒体内容、博客文章、邮件。</p><p></p><p>据国泰君安研报预计，未来 5 年，AI 绘画在图像内容生成领域的渗透率将达到 10％-30％，市场规模超 600 亿元的新兴市场将吸引更多企业和资本，创造更大的市场价值。</p><p></p><p>在享受极高热度的同时，AI 绘画赛道也面临<a href=\"https://www.infoq.cn/article/MYYhWiSNPaAQIGfZywa0\">争议与挑战</a>\"。当前，AI 绘画最大的争议就是抄袭与版权问题，对此，甚至有艺术家发起了“抵制 AI”的活动。</p><p></p><p>在世界著名艺术家网站 Artstation，不少艺术家要求删除站内所有 AI 生成的绘画作品，以此来呼吁原创、抵制抄袭。连美漫著名画师 Alex Ross、《地狱男爵》漫画原作者 Mike Mignola 都站出来抵制 AI。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/692dcee5238db95697b2fd9e5c7a08ba.png\" /></p><p></p><p>Twitter 用户 Zakuga Mignon Art 表示，AI 创造的“艺术”都来自被剥削的艺术家。AI “艺术”目前正在网络上搜索艺术，并将其用于数据集。没有艺术家同意别人使用他们的作品，并且艺术家们也没有得到补偿。</p><p></p><p>也有人认为 Stable Diffusion 这种模仿在世艺术家风格和美学的行为不值得提倡：这不仅可能违反版权条例，更不符合道德规范。一条批评该软件的推文还详尽列出了该模型模仿过的部分在世艺术家。</p><p></p><p>争议之外，<a href=\"https://www.infoq.cn/article/rggHjzaBfCVPV5hxTF7H\">AI 绘画</a>\"在技术上也存在一定的挑战，依然有很大的优化空间。在技术角度，需要提高的主要是两个方面，包括生成的可控性和细节描述能力。</p><p></p><p>AI 绘画的可控性有待提升，对于数量、逻辑、关系、多图关联等问题暂无有效的解决方案。比如说要求生成 2 个苹果，左边是红色，右边是绿色。虽然这里边的关系并不复杂，但模型很多时候，并不能稳定地生成正确的结果。细节描述能力有待提升，对于更加复杂的、有规律性的细节的描述能力有待提升。比如对于一栋居民楼的图片，窗户应该是有多种不同描绘，有开的、有关的、有晾衣服的，同时很多窗户应该对齐且规格统一。</p><p></p><p>2023 年，AI 绘画将继续在争议中前行。对于 AI 绘画公司来说，如何在激烈的竞争中杀出重围，站上风口，是一件值得思考的事情。</p>",
    "publish_time": "2023-01-03 14:17:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "南京银行：国内首个商业银行互联网金融核心分布式升级实践",
    "url": "https://www.infoq.cn/article/DMrM4aL54l08PxyKZO5L",
    "summary": "<p></p><blockquote>随着社会各行业数字化转型逐渐走入深水区，银行业的数字化转型正经历颠覆性嬗变。未来五到十年，银行业内的并购、重组、洗牌将成为行业发展的新常态。银行自身想要发展，就要摆脱原有的惯性思维，在传统业务发展遇到瓶颈时，积极寻求“第二增长曲线”。</blockquote><p></p><p></p><p>“工欲善其事，必先利其器”，南京银行一直以“做强做精做出特色”的运营理念为“利器”，走在科技创新前沿。成立26年来，南京银行不断加大信息科技投入，推动数字化转型，强化数字赋能和场景应用，成为国内首家互联网金融核心系统分布式升级的商业银行，最终跻身“万亿俱乐部”。</p><p></p><p>南京银行成立于1996年，总部位于江苏南京，2007年7月成功在上海证券交易所上市，成为全国第一家登陆上交所的城市商行。截至2022年三季度末，公司股本103亿元，资产规模超1.92万亿元，位列英国《银行家》杂志公布的全球1000家大银行第101位、全球银行品牌500强排行榜第98位、中国银行业协会评选的2022年度“中国银行业100强”榜单第21位。</p><p></p><h2>破局：业务规模激增十倍，传统IT架构难堪重负</h2><p></p><p></p><p>随着云计算、大数据、移动支付、人工智能等互联网技术的迅猛崛起，银行业也在澎湃的发展浪潮中享受着前所未有的技术红利。互联网的发展和智能移动终端的普及把客户行为从线下转变为线上与线下并重，在平台和场景中获取的金融服务已经成为主流。</p><p>&nbsp;</p><p>与金融科技蓬勃发展的势头相伴，近几年南京银行的消费金融业务增速不断加快，线上一年的发展规模达到了线下十年的业务发展规模，实现单日万级进件、万级获客、亿级投放。</p><p></p><p>南京银行上一代核心系统早在2003年就投入使用，与许多传统银行一样基于“IOE”架构运作。随着银行资产证券化、贷款等新业务的快速增长，上一代核心系统已无法支撑新业务的正常运行。一方面，传统架构的开发周期较长，导致需求响应较慢，无法适应互联网业务的快速发展，并且系统升级需要停机，无法保障7×24小时运行，且不支持灰度发布，导致几乎没有试错空间；另一方面，传统“IOE“架构成本、应用耦合度、系统研发成本都过高，生产问题的频发导致数据治理难以落地，影响数据的应用发展。</p><p></p><p>2015年1月，南京银行启动第二代核心系统建设，并于2016年中上线。由于历史原因，第二代核心系统仍然延用集中式架构。</p><p>&nbsp;</p><p>然而，市场规模和新业务增速远超南京银行预期，仅网络贷款这一项业务需求就在六个月内增长了近十倍。对于银行系统来说，每笔交易金额无论是1元还是10万元，它们的计算量都是相同的，所以在电商、互联网金融出现后，小额高频交易让传统系统不堪重负，业务并发增速远非系统优化能力所及。若新一代核心系统不进行分布式升级，南京银行将面临——要么放弃网络贷款业务，要么放弃系统稳定性。</p><p>&nbsp;</p><p>紧接着，南京银行开启探访包括网商银行在内的多家互联网银行典型代表，了解其互联网金融核心系统和产品系统架构。此时，恰逢OceanBase迈出商用的一小步，经过综合考查与多轮选型，南京银行最终决定将整个互联网核心业务系统全部“架”在OceanBase之上。</p><p></p><h2>蓄势：确定“双模运行”架构，落地三副本集群</h2><p></p><p>&nbsp;</p><p>2017年初，南京银行总结近年来行业内类似项目的成败经验，确定“双模运行”原则，即保留银行可靠稳定的传统核心系统，同时采用业界成熟可信的分布式架构技术搭建互金平台，建立一个更开放、灵活、松耦合、高性能、易扩展的第二核心。</p><p>&nbsp;</p><p>2017年7月，蚂蚁集团多个核心架构技术团队入驻南京银行：与咨询合作伙伴以及开发商一道就应用架构、数据架构以及技术架构等层面的技术规范和最佳实践进行充分讨论和设计指导，避免走弯路。</p><p>&nbsp;</p><p>2017年8月，蚂蚁集团交付团队入驻南京银行：两周内完成软硬件安装部署，及时支持应用部署至生产环境，并开展联调、压力测试等工作。交付团队每日守在现场为南京银行及ISV提供架构咨询和技术支持，确保开发进度。</p><p>&nbsp;</p><p>2017年9月，南京银行、阿里巴巴及蚂蚁集团签署战略合作协议，并于10月发布南京银行“鑫云+”互金开放平台。“鑫云+”互金开放平台开创“1+2+3N”的互联网合作新模式，“1”代表南京银行；“2”代表阿里云和蚂蚁金融云；“3N”分别代表医、食、住、教、产、销等N个场景，旅游、电商、快递、等N个行业平台，以及N家中小银行，致力于做中小银行和行业平台的连接者，打通中小银行和行业平台两个生态圈。</p><p>&nbsp;</p><p>经历5个月的架构咨询、开发测试、技术支持、培训保障后，2017年11月18日，南京银行“鑫云+”互金开放平台正式上线，并逐步承担来自旧系统的互联网金融业务流量。“鑫云+”互金开放平台在数据库层面通过OceanBase实现三副本集群，每个副本有完整的数据，其中一个集群作为主库对外提供读写服务，其余集群作为备库，接收主库的事务日志和回放日志。当主库故障时，剩下的集群会立刻自动发起投票选举，选出新的主库，新主库从其他集群获得可能存在的最新事务日志并回放，完成后对外提供服务。这个故障切换过程在秒级就可以完成，意味着传统机房容灾最难解决的数据容灾切换降低到秒级，极大降低机房容灾的复杂度、保证业务连续性。</p><p></p><h2>嬗变：“亮剑”三大核心能力，力促集中式向分布式演进升级</h2><p></p><p></p><p>由于历史原因，集中式架构多用于传统银行、电信等行业，南京银行也不例外。但近年来，随着包括谷歌、亚马逊、阿里巴巴等在内的互联网巨头纷纷“转战”分布式架构以来，分布式的概念甚嚣尘上。客观来讲，面对金融系统需要处理的海量交易和数据时，集中式架构虽然在一致性、可靠性方面占有一定优势，但从成本、自主研发、灵活兼容、伸缩扩展等方面都逊于分布式架构，由集中式架构向分布式架构演进成为了南京银行的必然选择。</p><p>&nbsp;</p><p>为了帮助南京银行顺利完成从集中式架构向分布式架构的升级，OceanBase亮出了三大核心技术能力：</p><p>&nbsp;</p><p>一、平滑的横向扩展能力：过去，南京银行使用集中式独立数据库，存储空间有限。随着业务的发展，存储空间经常被用完。扩展传统集中式数据库的唯一方法是升级硬件，但硬件资源总有用尽之时，而以OceanBase为代表的新一代原生分布式数据库不存在单点瓶颈，弹性扩缩容能力破解业务扩展难题。</p><p>&nbsp;</p><p>二、强大的混合负载能力：南京银行内部有很多复杂的场景，需要同时具备在线事务处理（OLTP）和在线分析处理（OLAP）两种能力。目前，只有Oracle、DB2等大型商业数据库支持对同一条数据进行实时计算。一般情况下，为了同时具备以上两种能力，常见的做法是将数据导入数据仓库进行离线计算，但这种做法就造成了资源的浪费。OceanBase在蚂蚁集团内部的金融场景中经过了长期的考验和完善，其自研的SQL引擎和分布式并行计算框架能够同时支持OLTP和OLAP，以更高的效率和更低的成本满足南京银行的多样化需求。</p><p>&nbsp;</p><p>三、业务的无损迁移能力：在数据库迁移过程中，往往银行最关心的是尽量减少数据迁移的时间和成本，同时保证业务不中断，不丢失数据。基于蚂蚁集团十年数据库架构升级的先进经验，OMS 支持秒级数据校验和回滚，高度兼容Oracle和MySQL等多种数据库类型，便捷设置数据迁移通道和回滚机制，降低南京银行的业务迁移成本。&nbsp;</p><p></p><h2>向新：积极求变，开辟“互联网+”新金融之路</h2><p></p><p></p><p>面对巨变的金融形势与市场环境，南京银行主动求变，积极寻找传统银行转型的密钥，最终走出了一条“云融开放，智启未来”的新金融之路。</p><p>&nbsp;</p><p>目前，“鑫云+”互金开放平台已服务多家银行的线上百万用户，平均每个客户的放款时间只需1秒，日处理订单量设计容量从原来的10万笔/日以下，增加到了100万笔/日以上，处理效率是原来的10倍；银行单账户的维护成本也从过去的30~50元/账户，降低到4元/账户，节省管理成本约80%~90%；此外，“鑫云+\"平台的人工维护成本也大大降低，维护专员数量约为传统银行系统的1/5。</p><p>&nbsp;</p><p>通过OceanBase及蚂蚁分布式金融交易处理架构搭建的高效输出、快速迭代、无限扩容的“鑫云+”互金开放平台，从系统建设、研发模式、运维管控等方面，全面引领南京银行的信息科技架构实现数字化转型，为平台容量的线性扩展和高可用部署提供了有力支撑，为金融机构从传统架构演进到分布式异地多活架构提供首个参考案例，提出完善的方法论与路线图：从单机房无单点，到双机房双活，再到两地三中心，最后到可无限扩展的异地多活架构。</p><p>&nbsp;</p><p>长期以来，银行信息科技团队将确保客户和银行的资金安全作为恪守铁律，使用最成熟、稳定的技术，保证系统交易的实时一致性，是银行信息技术部门的基本工作原则。因此，银行架构无一例外地选择使用以“IOE”为代表的成熟、稳定、具备良好技术支持保障体系的“经典”商用软件和体系架构，并培养了大批熟悉应用相关技术的优秀技术人员。分布式架构与传统架构相比，在架构、设计、开发、运维、管理上需要有不同的思维和技术能力，会对现有技术团队的思想观念和思维模式造成巨大冲击，需要有一个“脱胎换骨”的转变过程。</p><p>&nbsp;</p><p>此次南京银行积极探索创新，意味着我国传统金融机构完全能够掌握全球领先、拥有完全自主知识产权、安全可控的金融科技能力，在普惠金融业务上不再受限于技术瓶颈，使金融机构能更专注于业务创新，加快“互联网+”的转型与实践。</p><p>&nbsp;</p>",
    "publish_time": "2023-01-03 15:11:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "服务器操作系统阵营再添一军！Inspur KOS横空出世，通过软硬件协同释放多元算力",
    "url": "https://www.infoq.cn/article/eHvCkzJylPgZYwrvmWiH",
    "summary": "<p></p><p>近日，InfoQ 获悉，浪潮信息发布了其自研的服务器操作系统Inspur KOS。</p><p></p><h3>浪潮信息发布自研服务器操作系统 Inspur KOS</h3><p></p><p>&nbsp;</p><p>浪潮信息副总裁张东表示，在<a href=\"https://www.infoq.cn/article/9NeXm0AYESkRAddGjVhi\">算力</a>\"大发展和国家推动软硬件生态的大背景下，浪潮信息发布了自研服务器操作系统Inspur KOS。</p><p>&nbsp;</p><p>Inspur KOS是浪潮信息基于Linux Kernel、OpenAnolis等开源技术自主研发的一款服务器操作系统，支持x86、ARM等主流架构处理器，性能稳定性领先，可满足云计算、大数据、分布式存储、人工智能、边缘计算等应用场景需求。</p><p>&nbsp;</p><p>张东表示，Inspur KOS将成为浪潮服务器适配最优的操作系统，引领浪潮信息在软硬件协同方面的技术创新，推动整机系统生态繁荣，为用户构建智算中心基础设施提供更优选择，用户无论使用什么样的服务器，Inspur&nbsp;KOS都能为用户提供软硬件一体化的、协同设计的系统。</p><p>&nbsp;</p><p>张东详细介绍了Inspur&nbsp;KOS操作系统的研发背景，以及在技术、生态等方面的创新和实践。</p><p>&nbsp;</p><p></p><h3>数据中心面临挑战</h3><p></p><p>&nbsp;</p><p>在智算时代，<a href=\"https://www.infoq.cn/article/XheatwzU9gQDUN0rUAJ0\">数据中心</a>\"面临诸多挑战。</p><p>&nbsp;</p><p>首先是协同上的挑战。</p><p>&nbsp;</p><p>进入大数据、云计算、人工智能等技术蓬勃发展的智慧时代后，各种运算场景越来越丰富，软硬件协同成为一种流行的解决方案。</p><p>&nbsp;</p><p>现在是体系结构的新黄金时代，以前完全靠芯片，靠摩尔定律的快速增长，提升芯片性能来满足所有场景下不同应用的需求。现在，这条路慢慢走不通了。因此，体系结构产生了很大的变化，从而产生了众多不同的芯片。</p><p>&nbsp;</p><p>在通用场景下，通用处理器仍是现在用量最大的计算单元。从算力上看，通用处理器提供的算力已慢慢让位于各种通用的加速处理器。在存储单元、网络单元和传输模块里也出现了很多新式的介质，各种各样新的连接方式和新的传输模式。如此多的复杂芯片出现后，要用到一个完整的系统里，给软件带来了很大的协同上的挑战，协同做不好，将有损芯片性能，甚至其在综合应用中体现出的性能还不如用通用计算。这是现在很多做加速计算的厂商面临的很大问题，芯片的性能表面上看很好，但却发挥不出来。</p><p>&nbsp;</p><p>在能耗、扩展性方面也存在很大问题。不同芯片间需要做数据搬移，需要打破互相之间的内存墙、传输墙。这些问题都需要在软件和硬件上进行协同设计，共同做好资源调度，从而使得软硬件协同后能根据各种业务特征调度到最适用的计算单元上去，这对软件的挑战非常大。</p><p>&nbsp;</p><p>第二是运维方面的挑战。</p><p>&nbsp;</p><p>现在数据中心越来越大，普遍都在万台设备以上，设备规模的增长加大了运维的难度。</p><p></p><p>现在业内普遍讲自动化运维、AIOps，这都有赖于最基础的软硬件提供最基本的能力，如监控数据。上层的AIOps算法写得再好，没有底层的监控数据，算法就发挥不了作用。因此，对于最基础的硬件和软件，一定要能提供完整的监控数据，但现在的数据中心里，很多新品和器部件、基础软件在监控方面做的还不够全、不够丰富，很多数据无法抓到。</p><p>&nbsp;</p><p>因为部件种类多，当各种故障出现时，反向跟踪定位非常困难。浪潮信息为很多大型数据中心提供设备，实际上，真正在反向定位时可能有超过百分之四五十的故障无法找到根本原因，只能依靠换机器、主板等解决问题，依赖人工经验，但有时人工经验也没法真正处理这些问题。</p><p>&nbsp;</p><p>自动化程度也不够，尽管有自动化运维和AIOps，但实际上现在很多运维软件还处在手工处理的阶段。此外是被动响应的问题，当天发生了故障，事后才处理，无法预知故障，会给业务造成一定损失。</p><p>&nbsp;</p><p>第三是生态方面的挑战。</p><p>&nbsp;</p><p>尤其在算力多样化的背景下，这一挑战尤为凸显。基础算力的供给端产生了很大变化，原来某些芯片一统天下的格局在这两年逐步被打破。在这种情况下，芯片供应商变多了，上层软件的供应商也在逐步增多，整个生态面临很大挑战。原来芯片厂商、整机厂商、<a href=\"https://www.infoq.cn/article/4lDxHRoqXbSw6zl29AEo\">操作系统</a>\"厂商、中间件、数据库等完全分层的模式产生了很大变化。</p><p>&nbsp;</p><p>上层的各种应用同样面临这些问题。例如，这些年新出现的AI应用、中间件应用都出现了分散化的问题。另外，在多处理器并存的情况下，不同的系统要跑在不同的处理器之上，这么多复杂的环境如何结合在一起，软件层很重要。</p><p>&nbsp;</p><p>针对上述种种复杂的情况，作为一个整机厂商，浪潮信息提出了以系统设计为中心的技术路线。所谓以系统设计为中心，就是以软硬件一体化的系统设计为中心，为不同的场景下的应用构建多元异构的算力融合、软硬件协同的系统，通过标准的接口规范形成一些规格，使得用户从应用场景出发选择系统时，无需太多关心底下的架构。</p><p>&nbsp;</p><p></p><h3>Inspur KOS核心特征</h3><p></p><p>&nbsp;</p><p>Inspur KOS具有多个核心特征。</p><p>&nbsp;</p><p>稳定可靠</p><p>&nbsp;</p><p>Inspur&nbsp;KOS提供RAS增强、应用高可用等能力，保障了业务的连续性、可靠性。其中，Inspur&nbsp;KOS具备关键数据冗余机制，可对核心数据内存进行镜像保障业务关键数据可靠，而增强性容错能力则可将引起系统宕机的UCE进行降级容错处理，大幅降低了系统宕机率。</p><p></p><p>此外，Inspur&nbsp;KOS支持CPU、内存等核心部件的热替换，能够有效提升系统可维护性，支持内核、应用软件的热升级，能够有效保障用户业务连续性。在安全可信方面实现全栈可信链，覆盖硬件启动、内核启动、驱动加载及应用执行等。</p><p>&nbsp;</p><p>高效协同</p><p>&nbsp;&nbsp;</p><p>Inspur&nbsp;KOS提供高效算力调度、统一编程模型等能力，优化了对虚拟化、云原生、人工智能等场景支持。Inspur&nbsp;KOS设计了应用、算力、芯片跨层次的资源调度机制，通过深度感知应用的算力需求特征、负载特征，以及算力设备的能耗特征，实现业务与算力的最佳匹配和弹性伸缩。</p><p></p><p>同时，Inspur&nbsp;KOS实现了对虚拟化、云原生、人工智能等场景的优化增强，如Inspur KOS大幅提升了AI应用的开发和运行效率，支持基于DPU的高性能虚拟化网络、存储，在容器密度、网络性能方面的优化，也为云原生场景带来了增强。</p><p>&nbsp;</p><p>全天候运维</p><p>&nbsp;</p><p>Inspur&nbsp;KOS提供深度监控、专家诊断规则、云端运维服务接入等能力，显著提升了运维效率。</p><p></p><p>Inspur&nbsp;KOS支持超过700余种深度监控数据，能够全面展示从底层芯片、部件到整机系统、软件的运行情况，支持基于专家规则的自动化运维和诊断，可以方便地进行故障定位、性能分析、性能优化。</p><p></p><p>同时，Inspur&nbsp;KOS可以无缝接入到浪潮InService云端运维服务，实现运维远程托管，提供故障预测、实时告警、一键报修、资源扩容、技术支持等全场景全天候运维能力。</p><p>&nbsp;</p><p>广泛兼容</p><p>&nbsp;</p><p>浪潮信息通过与上下游伙伴广泛合作，完成了大量的兼容认证，覆盖主流芯片、板卡、数据库、中间件。</p><p>&nbsp;</p><p></p><h3>应用：支撑内部软件产品</h3><p></p><p>&nbsp;</p><p>Inspur KOS虽是首次对外发布，但在此之前已支撑了浪潮内部的云海OS、AS13000、AIStation等软件产品。目前，政务、金融、能源、交通、医疗、企业、教育等关键行业的客户都使用了Inspur KOS操作系统，累计装机量超20万台。</p><p>&nbsp;</p><p>例如金融领域，Inspur KOS有效支撑某大型银行，承载70多个核心业务，包括风控、国际结算、第三方结算、手机银行、渠道业务等。在其渠道二期项目中，KOS稳定支撑双11当天超1.5亿笔交易，顺利满足业务峰值需求。</p><p>&nbsp;</p><p>通信领域，Inspur KOS支撑某运营商业务支撑云平台，助力百万级应用并发接入平台，实现500+节点PB级数据的双中心容灾，有效保障业务数据跨中心级高可用。</p><p>&nbsp;</p><p>政务领域，Inspur KOS支撑建立了2000+节点省级一云多芯政务云平台，涵盖多种架构服务器，稳定承载100+业务系统运行至今。</p><p>&nbsp;&nbsp;</p><p></p><h3>版本长期维护计划</h3><p></p><p>&nbsp;</p><p>据悉，浪潮服务器出厂全面预装Inspur&nbsp;KOS，并面向个人和企业用户提供1年产品试用授权，用户可以自由下载、自由安装，而且提供在线升级能力。Inspur KOS推出后，会和其他硬件、软件形成一站式的方案，为用户提供更好的体验。</p><p>&nbsp;</p><p>此外，浪潮信息还为使用了CentOS的用户，量身打造了迁移的整体方案C2X，无缝地让原有的应用从CentOS上面迁移过来，保证业务的连续性。同时Inspur KOS也和服务器、存储、云海OS、AIStation等产品进行深度的优化，保证系统的整体体验是最优的。</p><p>&nbsp;</p><p>操作系统的长期稳定非常重要，浪潮信息坚持“以客户为中心”，为用户提供长达10年的操作系统维护支持</p><p>&nbsp;</p><p>只要用户不换设备，长稳版本会提供十年的支持维护。即便换了设备也可以继续使用，并从浪潮信息继续得到补丁升级、漏洞修复等支持服务。同时为了满足一些用户对最新特性的要求，如对处理器里最新特性的支持，会在每6个月发布一次创新版本。当这些特性在创新版本里得到验证之后，再逐步进入长稳版本。形成两年一个长稳版本、半年一个创新版本的发版节奏。</p><p>&nbsp; &nbsp;</p><p></p><h3>和开源社区共建生态</h3><p></p><p>&nbsp;</p><p>操作系统的发展投入巨大，需要更多的厂商加入进来，才能保证操作系统广泛的兼容。接下来，浪潮信息会和开源社区一起共建操作系统生态。</p><p>&nbsp;</p><p>在软件方面，浪潮信息会和龙蜥社区展开合作。加入社区之后，浪潮信息将和社区共同推动操作系统的软硬件适配，面向更多最新的场景做协同创新，对一些关键问题做联合攻关，使操作系统的版本能更加适用于行业用户的需求。</p><p>&nbsp;</p><p>浪潮信息还和龙蜥成立了联合实验室，希望通过这个实验室，联合上下游的合作伙伴，包括芯片、服务器整机、存储整机、数据库、中间件等厂商，共同在实验室里把操作系统做好，相关的成果会同步反馈给社区。</p><p>&nbsp;</p><p>同时，浪潮信息也会和硬件的开放社区保持合作，浪潮信息也主导发起了国内的开放计算社区、开放计算标准委员会，在其中会展开一些从芯片、板卡到固件层面的创新，这些创新也将和在操作系统层面的创新结合起来，使得软硬件协同这条路走得更加顺畅。</p><p></p>",
    "publish_time": "2023-01-03 15:52:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022云智峰会｜智算峰会：AI 创新力专题论坛",
    "url": "https://www.infoq.cn/article/m9xAYTXHgy3hT7KjVLkp",
    "summary": "<p>2022 年 12 月 27 日，百度智能云与中国电子技术标准化研究院将共同在北京（线上）举办“2022 百度云智峰会·智算峰会”，将围绕百度智能云“云智一体，深入产业”的全新战略，为各位带来深入洞察。</p>",
    "publish_time": "2023-01-03 15:58:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022 云智峰会｜智算峰会：生命科学专题论坛",
    "url": "https://www.infoq.cn/article/iWZcO2WuqylS0y1SfjBr",
    "summary": "<p>2022 年 12 月 27 日，百度智能云与中国电子技术标准化研究院将共同在北京（线上）举办“2022 百度云智峰会·智算峰会”，将围绕百度智能云“云智一体，深入产业”的全新战略，为各位带来深入洞察。</p>",
    "publish_time": "2023-01-03 16:04:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022 云智峰会｜智算峰会：自动驾驶专题论坛",
    "url": "https://www.infoq.cn/article/dl80E267wJoMfQwVszEN",
    "summary": "<p>2022 年 12 月 27 日，百度智能云与中国电子技术标准化研究院将共同在北京（线上）举办“2022 百度云智峰会·智算峰会”，将围绕百度智能云“云智一体，深入产业”的全新战略，为各位带来深入洞察。</p>",
    "publish_time": "2023-01-03 16:11:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022 云智峰会｜智算峰会：智能芯力量专题论坛",
    "url": "https://www.infoq.cn/article/LPJUUY496ojVdvpZaJyE",
    "summary": "<p>2022 年 12 月 27 日，百度智能云与中国电子技术标准化研究院将共同在北京（线上）举办“2022 百度云智峰会·智算峰会”，将围绕百度智能云“云智一体，深入产业”的全新战略，为各位带来深入洞察。</p>",
    "publish_time": "2023-01-03 16:19:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022 云智峰会｜智算峰会：AI安全专题论坛",
    "url": "https://www.infoq.cn/article/PTAwHBADEZdVCY5IOm64",
    "summary": "<p>2022 年 12 月 27 日，百度智能云与中国电子技术标准化研究院将共同在北京（线上）举办“2022 百度云智峰会·智算峰会”，将围绕百度智能云“云智一体，深入产业”的全新战略，为各位带来深入洞察。</p>",
    "publish_time": "2023-01-03 16:27:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022 云智峰会｜智算峰会：智算技术专题论坛",
    "url": "https://www.infoq.cn/article/DcXdNaISWZQUZNgWsDqV",
    "summary": "<p>2022 年 12 月 27 日，百度智能云与中国电子技术标准化研究院将共同在北京（线上）举办“2022 百度云智峰会·智算峰会”，将围绕百度智能云“云智一体，深入产业”的全新战略，为各位带来深入洞察。</p>",
    "publish_time": "2023-01-03 16:39:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023年科技、营销、个人应该往哪个方向前进？| InfoQ 合作伙伴年度盛典",
    "url": "https://www.infoq.cn/article/sE9kfwRtOjruFiWYjgr5",
    "summary": "<p>在“不确定”中，我们度过了 2022 年。</p><p></p><p>好消息是，2023 年到来了。</p><p></p><p>坏消息是，未来一年乃至五年依旧充满着“不确定”。</p><p></p><p>我们需要做的则是从“不确定”中找到大致正确的前进方向。基于此，InfoQ 极客传媒合作伙伴年会将于 1 月 5 日 16:00 正式开启。本届年会以“微光成炬·破晓而出”为主题，共分为“Memory / 回溯”、“Manner / 瞻望”、“Moment / 灵感”三个环节，与大家一起探讨科技、营销以及个人未来一年可能的前进方向。</p><p></p><h2>Memory / 回溯：科技热词背后的未来世界</h2><p></p><p></p><p>2022 科技人的年度记忆中总有一些趋势是可以脱口而出的，比如元宇宙、数字人、AIGC…</p><p></p><h3>元宇宙有了基金会</h3><p></p><p></p><p>12 月 15 日，Linux 基金会宣布拟成立开放元宇宙基金会（Open Metaverse Foundation，简称，OMF）。开放元宇宙基金会总经理 Royal Obrien 如此解释开放元宇宙基金会成立的初衷：</p><p></p><p>“开放元宇宙的潜力是巨大的。它可以像万维网一样具有影响力，但前提是我们必须共同努力。最好的情况是，它可以是一种不断变化但持久的沉浸式体验。它既存在于数字世界和现实世界之间，也存在于数字世界和现实世界之中。为了让元宇宙真正像互联网一样蓬勃发展，它需要成为一个面向所有人的开放的元宇宙”。</p><p></p><p>当然，这仅仅是个开始。我们仍处于元宇宙的早期阶段。元宇宙的最终实现是一个迭代的谜题，如果想要获得成功，就必须将不同的部分组合在一起并进行互操作。在这个过程中，要做的工作太多了，2023 年会走到哪一步是值得期待和向往的。</p><p></p><h3>数字人越来越常见</h3><p></p><p></p><p>整个市场的流量红利正在逐渐消减，交互正在越来越深化。当你每天在 N 多个 APP 上交互，和更多的人进行交互……我们就会越来越难以承载这样的交互。因此，很多场景需要用人工智能来代替人做出社交决定。</p><p></p><p>此外，社交网络的深化产生了一些新问题，如明星塌房事件频发，有明星因私生活有问题影响到了演艺生活。理论上互不影响，但由于是同一主体，人类在社交中有很多侧面，可控性较差。相对而言，人工智能的可控性就很高。数字人本质是一条被技术驱动的赛道。由于技术发展到了可以向场景中应用的阶段，再加上客户以及用户的旺盛需求，他们“握手”到一起，促成了这一波数字人的行业热潮。</p><p></p><p>2022 年我们已经见识到了小冰、谷小雨等众多形象出现在晚会现场等多种场合。2023 年，这些数字人又将对我们的生活带来哪些影响，这是值得探讨的。</p><p></p><h3>AIGC 对就业带来冲击？</h3><p></p><p></p><p>文生图模型 Stable Diffusion 的开源、 ChatGPT 的爆火出圈，都让我们不得不正视 AI 可能带来的影响力。ChatGPT 展示出的强大能力和无限可能，让人们看到通过 ChatGPT 这样的技术方案解决很多任务的潜力。大家感到惊奇的是，在一个模型里面就可以完成各种任务，而且是很难的任务。在过去一些看似比较困难的任务（比如伦理道德方面），ChatGPT 也能解决得很好。</p><p></p><p>这让我们不得不思考 AICG 是否会在一些场景取代某些服务、取代某些人力呢？</p><p></p><p>我们将邀请业内专家在合作伙伴年会现场就上述问题与大家一起交流自己的想法。</p><p></p><h2>Manner / 瞻望：未来一年的大方向</h2><p></p><p></p><p>2019-2022，过去的三年，让行业技术演进的主线略显单调，比如远程协作。远程协作相关的技术、产品、解决方案，在疫情之初迅速成为各团队关注的焦点，甚至催生了一批创业公司。表面上看，远程协作相关工具，是为了解决异地办公的场景问题，实际上，还是为了提升公司人效、降低成本。所谓远程协作，除了对基础设施性能指标（低延时网络）的要求新颖，并无太多新奇之处。</p><p></p><p>这种对人效的追求，一直延伸到 2022，于年末达到极致。以元宇宙为首的新概念、新机会，在 2022 都受到了来自全球经济下行的冲击。在技术上，今年似乎只有一个主线关键词：成就业务，其余的都没那么重要。除了人力成本，对技术类采购的成本评估也成为重点。云计算诞生十余年，对云成本的讨论、质疑、评估、优化又一次达到高峰，能否将云成本降至三分之一，是今年最火的话题之一。说起来有点无聊，如果非要给疫情三年总结一个核心主题，大概就是“省钱”吧。</p><p></p><p>不过，在 2022 -2023 ，这一切很可能出现变化，而变化的源头在于市场。对于企业而言，如果市场只剩存量竞争，那么降本增效必定是唯一的主旋律。至于增量机会在哪里，需要我们走到线下，面对面的去发现，去摸索。过去三年，这不可能；未来三年，这将成为常态。浙江商人包机去欧洲，就是一个明确的信号。企业的探索方向，决定了“打工人”的工作方向，我们相信，这一切都将酝酿出疫情三年来最大的行业变革 —— 数字经济高速发展，虚实融合初步完成。</p><p></p><p>人们重新回到跨地区、文化的社交状态中；在面对面的交流中，发现新的数字化机会；企业跟进新机会，数字人才培养升级，行业能够消化新机会。我们相信，这可能是未来几年的创新源泉。</p><p></p><p>InfoQ 基于对开发者、技术专家、行业大咖以及企业的长期追踪和深入访谈，我们将在合作伙伴年会现场分享对于 2023 年的技术趋势思考。此外，趋势变革将会带来市场营销方案的变化，我们将邀请相关人士与大家交流应该如何更好地面对 2023 年的市场变化。</p><p></p><h2>Moment / 灵感：跨界让灵感迸发</h2><p></p><p></p><p>广告大师詹姆斯·韦伯·杨曾说过：“创意就是旧元素的新组合”。在愈发内卷的当下，跨界营销俨然已经成为常见的营销手段。</p><p></p><p>2022 年，中国邮政跨界开卖咖啡、喜茶 x 藤原浩跨界联名、瑞幸咖啡×椰树椰汁跨界联名、茅台上线冰淇淋… 谁又说得好科技企业的下一个跨界对象会是谁呢？科技企业为什么不能从运动项目、传统文化中汲取灵感呢？</p><p></p><p>在这一环节，我们邀请到了业内众多玩咖为大家分享各自的“灵感迸发地”。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d1/d19b75237fd92ccd8525dcead8b1f87e.jpeg\" /></p><p></p><p>2022 年，我们沮丧过、焦虑过，也曾欣喜过、感动过。感恩过去一年与 InfoQ 携手前行的每一位伙伴。正如蜿蜒才是河流应有的走向，所幸在不断变化的环境下，我们团结一致练就了摆平失序的能力与本领。</p><p></p><p>守得云开见月明，我们相信 2022 年的沉淀就像一束束微光，终将在 2023 年汇聚成光芒。</p><p></p>",
    "publish_time": "2023-01-03 16:49:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Stable Diffusion、DreamFusion、Make-A-Video、Imagen Video 和下一步",
    "url": "https://www.infoq.cn/article/BKPTWSyeidEDNaoBvEKv",
    "summary": "<p>生成式人工智能才刚刚起步，但是正在以指数级的速度发展着。自 OpenAI 第一次发布 GPT-3 和 DALL-E 之后，就开始在人工智能领域大放异彩。</p><p></p><p>2022 年是文本到内容的生成年（又称 AIGC）。在 2022 年 4 月，OpenAI 发布了 DALL-E 2，在关于 CLIP 和扩散模型的<a href=\"https://arxiv.org/abs/2204.06125\">论文</a>\"中有所描述。这是第一次从自然语言的文本描述中创建逼真的图像和艺术。</p><p></p><p>四个月之后，初创公司 StabilityAI <a href=\"https://stability.ai/blog/stable-diffusion-public-release\">宣布</a>\"发布 Stable Dispossion，这是一个开源的文本到图像生成器，它能在几秒钟内创造出令人惊叹的艺术。它可以在消费级 GPU 上运行，在速度和质量上都有突破性进展。它的热度如此之高，以至于在 2022 年 10 月 17 日的种子轮中成为了独角兽。</p><p></p><p>2022 年 9 月 29 日，谷歌发布了 <a href=\"https://arxiv.org/abs/2209.14988\">DreamFusion</a>\"，用于使用 2D 扩散实现文本到 3D 的生成。同一天，Meta 发布了 Make-A-Video，它不需要文本和视频的数据，就可以进行文本到视频的生成。</p><p></p><p>不到一周，谷歌似乎回应了 Meta 的 Make-A-Video，首次推出了 Imaged Video，用于文本到视频的生成。</p><p></p><p>在过去半年的这一激动人心的旅程中，Midjourney 和 CogVideo 的重要性不容忽视。Midjourney 是一家独立的研究实验室，提供 Midjourney Bot，从文本中生成图像。CogVideo 是第一个开源的、具有 94 亿个参数的大规模预训练文本到视频模型。</p><p></p><p>在本文中，我将描述他们如何为 Stable Dispossion、文本到 3D 和文本到视频工作。另外，让我们体验一下无需编码即可实现令人惊叹的文本到图像的功能，看看接下来会发生什么。</p><p></p><h3>Stable Diffusion 与 Unstable Diffusion</h3><p></p><p></p><p>Stable Diffusion 引入了条件潜在扩散模型（LDM），以实现图像修补和类条件图像合成的最新分数，并在各种任务（包括文本到图像合成、无条件图像生成和超分辨率）上具有高度竞争力的性能，同时与基于像素的潜在扩散模型相比，显著降低了计算需求。这种方法可以显着改善降低扩散模型的训练和采样效率，而不会降低质量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d3fec35a5a86e8c92de6e5718746340.png\" /></p><p></p><p>条件潜在扩散模型通过串联或更一般的交叉注意机制来解释（资料来源：潜在扩散模型）</p><p></p><p>在发布 Stable Diffusion 的同时，StabilityAI 开发了一个默认启用的基于人工智能的安全分类器。它在代中理解概念和其他因素，为用户删除不需要的输出。但它的参数可以随时调整，以实现强大的图像生成模型。</p><p></p><p>基于 Stable Diffusion，Mage 显示在浏览器中生成 NSFW 内容。它很简单，可以在没有 NSFW 过滤的情况下免费使用。</p><p></p><p>不要被混淆了。Unstable Diffusion 是一个支持使用 Stable Diffusion 的人工智能生成的 NSFW 内容的社区。毫无疑问，这些模型可以在 Patreon 和 Hugging Face 上找到。</p><p></p><p></p><h3>用于文本到 3D 的 DreamFusion</h3><p></p><p></p><p>谷歌和 UCB 共同推出了 DreamFusion，用于使用二维扩散的文本到 3D 的生成。</p><p></p><p>DreamFusion 的工作原理是通过新颖的 SDS（Score Distillation Sampling，得分蒸馏采样）方法和新颖的 NeRF（Neural Radiance Field，神经辐射场）类似的渲染引擎将可扩展的、高质量的 2D 图像扩散模型转移到 3D 域。DreamFusion 不需要 3D 或多视图训练数据，仅使用预训练 2D 扩散模型（仅在 2D 图像上训练）来进行 3D 合成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3a20f6a37127740f8d01a29fb8604b52.png\" /></p><p>DreamFusion 演示如何从自然语言标题中生成 3D 对象（来源：DreamFusion）</p><p></p><p>如上图所示，一个场景由一个随机初始化的 NeRF 表示，并为每个标题从头开始训练。NeRF 用 MLP 对体积密度和反照率（颜色）进行参数化。DreamFusion 从一个随机的摄像头渲染 NeRF，使用从密度梯度计算出的法线，以随机的照明方向对场景进行着色。阴影揭示了从单一视角看是不明确的几何细节。为了计算参数更新，DreamFusion 对渲染进行了扩散，并用一个（冻结的）有条件的 Imagen 模型对其进行重建，以预测注入的噪声。</p><p></p><p>尽管 DreamFusion 产生了令人信服的结果，但它仍然处于文本到 3D 的早期阶段。SDS 在应用于图像采样时不是一个完美的损失函数。因此，在 NeRF 的环境下，它经常产生过度饱和和过度平滑的结果，并且缺乏多样性。此外，DreamFusion 使用 64×64 Imagen 模型来平衡质量和速度。</p><p></p><p></p><h3>用于文本到视频的 Make-A-Video</h3><p></p><p></p><p>Meta（又名 Facebook） 从来没有在人工智能的发展上落后过。在 DreamFusion 发布的当天，Meta 推出了 Make-A-Video，用于文本到视频的生成。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/9953dc80097584120a6c90bbedfad992.png\" /></p><p>Meta Make-A-Video 高级架构（来源：Make-A-Video）</p><p></p><p>按照以上的高级架构，Make-A-Video 可以分为三个主要层：1) 一种基于文本 - 图像对的基本 T2I （文本到图像）模型；2) 时空卷积和注意力层，将网络的构建模块扩展到时间维度；3) 由时空层和 T2V 生成所需的另一个关键要素组成的时空网络ーー一种用于高帧速率生成的帧内插网络。</p><p></p><p>因此，Make-A-Video 是建立在 T2I 模型的基础上，具有新颖实用的时空模块。它加速了 T2V 模型的训练，而不需要从头学习可视化和多模态表示。它不需要成对的文本 - 视频数据。而生成的视频则继承了这种广阔性（美学上的多样性、奇异的描绘等等）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af7c8e8d9a51d645f37c94aa6580d032.png\" /></p><p></p><p>谷歌的 Imaged Video</p><p></p><p>Google 的 Imagen Video 是一个基于视频扩散模型级联的文本条件视频生成系统。</p><p></p><p>给定一个文本提示，Imagen Video 使用基本视频生成模型和一系列交错的空间和时间视频超分辨率模型生成高清视频。</p><p></p><p>它由七个子模型组成，分别执行文本条件视频生成、空间超分辨率和时间超分辨率。整个级联生成 1280×768（宽×高）的高清视频，每秒 24 帧，持续 128 帧 （~5.3 秒），大约 1.26 亿像素。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af7c8e8d9a51d645f37c94aa6580d032.png\" /></p><p></p><p>Imagen Video 示例：“一束秋天的树叶落在平静的湖面上，形成文本‘Imagen Vide’。平滑。”生成的视频分辨率为 1280×768，持续时间为 5.3 秒，每秒 24 帧（来源：Imaged Video）</p><p></p><p></p><h2>用于 Stable Diffusion 的无代码人工智能</h2><p></p><p></p><p>如上所述，我们可以看到扩散模型是文本到图像、文本到 3D 和文本到视频的基础。让我们用 Stable Diffusion 来体验一下。</p><p></p><p>使用建议文本：亚马孙雨林中的高科技太阳朋克乌托邦</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/50ea3361f2c4d4b5b8dcfb03a7bf8dc3.png\" /></p><p></p><p></p><p>文本：动漫女机器人头上长满了鲜花</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1b422b064e818c5d62f89c2c80ea7ad0.png\" /></p><p></p><p>从空中近距离观察雷尼尔山</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7c5e14d8f6c4ffe17527365bd0a77c91.png\" /></p><p></p><p>你可能等不及了，下面是许多无需任何代码就可以尝试的内容。</p><p>1.StabilityAI’s Stable Diffusion hosted on Hugging Face</p><p>2.Stable Diffusion Online</p><p>3.StabilityAI’s DreamStudio</p><p>4.Mage enabled with NSFW</p><p>5.Playground AI</p><p>6.Text-to-Image (Beta) on Canva)</p><p>7.Wombo Art</p><p></p><h3>下一步是什么</h3><p></p><p></p><p>生成式人工智能令人惊叹，并且正在快速地发展。虽然我们仍沉浸于文本到图像的真实感图像和艺术之中，但我们现在正在进入下一个前沿领域：文本到视频和文本到 3D。</p><p></p><p>但它还处于萌芽阶段，在相关性、高保真度、广泛性和效率等方面都面临着诸多挑战。</p><p></p><p>相关性：我们注意到，在相同的输入文本下，这些模型产生不同（甚至显著不同）的结果，以及一些不相关的结果。在创作艺术时，如何用自然语言描述输入似乎成了一门艺术。高逼真度：DALL-E2 和 Stable Diffusion 中的许多逼真图像给我们留下了深刻的印象，但它们仍然有很大的高保真空间。广泛性：广泛性是指美关于美学、奇幻描绘等方面的多样性。它可以为广泛的输入提供丰富的结果。效率：生成图像需要几秒钟和几分钟的时间。对于 3D 和视频，需要更长的时间。例如，DreamFusion 使用较小的 64×64 Imagen 模型，通过牺牲质量来加快速度。</p><p></p><p>好消息是，它开辟了许多令人兴奋的机会。人工智能工程、基础模型和生成式人工智能应用。</p><p></p><p>人工智能工程：人工智能工程对于自动化 MLOps、提高数据质量、增强人工智能可观察性和自我生成应用内容至关重要。基础模型：独立训练和操作许多大规模的模型是昂贵的，而且变得不现实。最终，它将统一或整合成几个基础模型。这些大规模模型在云端中运行，以服务于上述不同的领域和应用。生成式人工智能应用：有了人工智能工程和基础模型，它是一个巨大的应用机会，包括元宇宙和 NFT 领域的数字内容。例如，创业公司 Rosebud 专注于多样化的照片生成。</p><p></p><p>到 2025 年，生成式人工智能模型将产生所有数据的 10%。我们可以预期，在未来几年中，随着人工智能生成进化的步伐，将会发生显著的变化。</p><p></p><p>&nbsp;作者简介：</p><p></p><p>Luhui Hu，@Aurorain 创始人，VC 投资者。曾供职于亚马逊、微软、Meta。在机器学习和数据云领域拥有 30 多项专利。</p><p></p><p>原文链接：</p><p></p><p>https://towardsdatascience.com/generative-ai-878909fb7868</p>",
    "publish_time": "2023-01-03 17:51:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "受不了Rust 这些问题，我将后端切换到了 Go",
    "url": "https://www.infoq.cn/article/ZktGy1loVGIWJ25QcYVd",
    "summary": "<p>本文最初发布于 Level Up Coding 博客。</p><p></p><p>别激动！我能感受到你点击这篇文章时怀有的愤怒。我并不讨厌 <a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247579834&amp;idx=2&amp;sn=fab0385664281e72d8266e5cdfd5e509&amp;chksm=fbeb0175cc9c88639a2c22b35320595b61abbcf672cf7e80181370c50aae2a4ff035d75b7310&amp;scene=27#wechat_redirect\">Rust</a>\"——在许多场景中，我都倾向于使用它。所有编程语言都是达成目的的手段。然而，就我要处理的场景而言，Rust 并不是很适合，我不得不把这个项目推倒重来，用 Golang 重写。</p><p></p><p>该项目是 Hasura 的一个简单的后端 webhook 服务。你可能不了解 Hasura，那是一个 Postgres 数据库封装器，可以即时提供 GraphQL API。对于像我这样独自开发个人兴趣项目的人来说，这非常方便：每个 REST 端点或 GQL 解析器都要编写的话会耗费大量的时间，而且每个模型的 CRUD 操作基本相同。当需要一些比较复杂的逻辑时，它就不那么有效了——为此，Hasura 允许你将 GQL 请求映射到自定义 webhook。举例来说，我就是用这种方法进行 S3 文件上传或身份验证。</p><p></p><p>问题 1：依赖注入难</p><p></p><p>Rust 依赖注入是一个有趣的问题。如果你需要一个具体的类型，如：</p><p><code lang=\"null\">fn do_stuff(db: &amp;Database) {    db.create(Stuff);  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;db.read(Stuff);}</code></p><p></p><p>你必须给 do_stuff 传递一个 Database 实例；概莫能外！你不能“子类化”Database （Rust 没有子类的概念）。所以，如果你是一个不自己测试代码的程序员，那么这完全没问题；实际上，你只会有一个 Database 的实现，因此也就没有理由让这个函数接受 Database 以外的任何东西。</p><p></p><p>那我们测试人员呢？我们必须重写函数签名。Database 需要是 trait 类型的，然后我们把那个它在 mock 对象上实现。好吧，还不算太坏。事实上，在 Golang 中，我做的事情基本相同；那到底是从哪里开始有问题的呢？</p><p></p><p>问题 2：异步 Trait</p><p></p><p>在 Rust 中，异步很简单，trait 也很简单，但异步 trait 却有些困难。我在 Rust 中找到的大多数异步 trait 示例都用了 async_trait 宏。这很有帮助，我正在用它，体验还不错。</p><p></p><p>以下是我到目前为止对这个过程的一个总结：</p><p></p><p>编写一个结构；开心。编写一个测试；意识到无法依赖注入。难过。将结构转换为 trait；开心。心满意足地依赖注入。使用 mockall crate 自动生成 mock。非常非常开心！做一个异步 http 调用。需要用一个特殊的宏实现异步 trait。意识到这个宏无法很好地与 Mockall 一起工作。难过。</p><p></p><p>事后来看，这个问题<a href=\"https://shimo.im/outlink/gray?url=https%3A%2F%2Fdocs.rs%2Fmockall%2Flatest%2Fmockall%2F%23async-traits%3FaccessToken%3DeyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NzI3Mzk4NjAsImZpbGVHVUlEIjoiZ3dnRGtRb3N5YWc1b0ZScCIsImlhdCI6MTY3MjczOTU2MCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyMDQxOTA5MH0.6raJxRNSDrtgQr5ZWNiRtG8oaW1-XWaCtgMgOM7bm28\">是有办法解决</a>\"的。也许在切换到 Go 之前我应该再试一次，但那时，下面这一点已经让我有点沮丧了……</p><p></p><p>问题 3：编译慢（致命一击）</p><p></p><p>Rust 的编译时间很糟糕。我们已经听过无数次了；不可能有一种无所不能却没有缺点的语言。那是不可能的——Rust 的缺点是难以理解的生命周期以及糟糕的编译时间。</p><p></p><p>我有一台漂亮耐用的笔记本电脑 M1 Mac，那可是一头老黄牛。在我的 Mac 上编译 Rust 绝对没有问题。通常，在编写服务器时，我会在本地开发，并且要保证每次有修改时，本地服务器会重新加载，让我可以在提交真正的单元测试之前非常快速地测试功能。两次试验之间需要进行大量的编译；可以接受！还是说，在 Mac 上编译 Rust 没有问题。</p><p></p><p>在容器里吗？还是算了吧。</p><p></p><p>对我来说，要编排许多本地服务而又不用费事在每个服务（Hasura、Web 钩子、mock s3、mock oauth 服务器……）中运行 npm run ，最简单的方法是有一个 docker-compose.yaml 文件可以运行所有这些东西。通常就是一个 docker-composition.dev.yaml 文件，因为我实际上并不使用 docker compose 进行部署。只在本地进行开发。然而，这有一个副作用，就是我的 Rust 代码需要在容器中编译，因为：必</p><p></p><p>须自动热重载。必须在容器里开发。</p><p></p><p>我有两个选择：要么启动一个可能会跑死整台计算的巨大镜像，以便在其中编译 Rust 代码，要么就要与 3 分多钟的编译时间周旋。开发周期陷入停滞，让我感觉非常低效。我试着改变工作流程，在手动测试之前编写代码和测试，或者不使用自动热加载，但糟糕的是，我就是没能做到。</p><p></p><p>最后，我咬紧牙关，换成了 <a href=\"https://s.geekbang.org/search/c=1/k=GO/t=\">Go</a>\"。让人怀念的 Rust：我非常喜欢编写 Rust 代码。我觉得它漂亮而富有表现力，实用而优雅。</p><p></p><p>如果我正在编写本地辅助库、性能敏感代码、任何不需要在容器中运行的后端服务……那么，Rust 会是我的第一选择。特别是如果我不需要说服其他任何人使用它。</p><p></p><p>对于我提到的问题，特别是最后一个问题，如果你有任何解决方法，请务必告诉我。我想让 Rust 回到项目中，我愿意回到旧版本，并将其提升到同等水平。</p><p></p><p>原文链接：https://levelup.gitconnected.com/why-i-switched-from-rust-to-go-on-the-backend-28bda21dbee9</p>",
    "publish_time": "2023-01-03 18:01:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Julia 快到离谱？不，它并没有比Python快340000,000,000倍",
    "url": "https://www.infoq.cn/article/roz4HjDsgNOizcWq4pOy",
    "summary": "<p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247495522&amp;idx=1&amp;sn=cb83be3fca851e8643f53ab8d75b7613&amp;chksm=fbea56adcc9ddfbbfc73471df429b0d0914230428162370563ceec21c024e4a7cb7f2269da5a&amp;scene=27#wechat_redirect\">Julia </a>\"运行速度很快，但从性能表现上看，也没快的那么离谱。</p><p></p><p>几周前，当我在 YouTube 上刷编程趣闻时，无意中看到一个视频，它展示了 C++ 和 Python 从 0 加到 10 亿时的性能差异。不出所料，Python 在执行此操作过程中不是非常快，耗时 1m52s，C++ 耗时 2.4s，但我很想看看 Julia 执行效果是什么样子。</p><p></p><p>接着，我开始写一些简单的 Julia 代码，来运行这个基准测试，以此看看 Julia 是否比 C++ 还快，是否能碾压 Python 很多（虽然这不是一个专业性的对比实验，但仍然可以作为一个有趣的参考指标）。</p><p>我使用的 Python 代码跟 YouTube 视频中的几乎一样，把它运行起来也比较简单：</p><p></p><p><code lang=\"null\">\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; def count():\n...     start = time.perf_counter()\n...     n = 0\n...     while n &lt; 1000000000:\n...             n += 1\n...     print(f\"Completed Execution in {time.perf_counter() - start} seconds\")\n... \n&gt;&gt;&gt; count()\nCompleted Execution in 44.67635616599998 seconds\n\n</code></p><p></p><p>如您所见，我使用 Python 代码，整个计算过程花了惊人的 44.67 秒，这个结果比视频中的运行时间快了很多，但这可能是由于很多其它原因导致的，比如我电脑 CPU 等硬件差异。</p><p></p><p>实现相同功能的 Julia 代码，与 Python 的代码比较相似，只需做一些小的改动。</p><p><code lang=\"null\">\njulia&gt; function count()\n           n = 0\n           while n &lt; 1000000000\n               n +=1\n           end\n       end\njulia&gt; using BenchmarkTools\njulia&gt; @benchmark count()\nBenchmarkTools.Trial: 10000 samples with 1000 evaluations.\n Range (min … max):  1.167 ns … 10.584 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     1.250 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   1.261 ns ±  0.222 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n</code></p><p></p><p>我相信，你肯定也跟我一样，刚开始也为这个结果大吃了一惊。这段 Julia 代码只花了 1.25 纳秒。这样的结果好的令人难以置信，它比 Python 代码快了近 34,000,000,000 倍。</p><p></p><p>一个小小的区别是，这个函数目前还没有返回 n 的值。但是即便我们加上返回值，整体运行时间也基本维持在 2.0 纳秒左右。</p><p></p><p>另外，如果在 print 语句中打印出结果，这个时间花费接近~ 33.084 微秒，这明显影响很小。</p><p></p><p><code lang=\"null\">\n1 evaluation.\n Range (min … max):  25.333 μs … 79.291 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     33.084 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   37.645 μs ±  9.828 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n</code></p><p>我知道这样难以置信的效果应该不是真实的，之后我没有去 Julia slack 而是直接到 Julia 社区寻求帮助，很快我便得到了一些有效反馈，<a href=\"https://twitter.com/MoseGiordano\">Mosè Giordano</a>\" 建议使用 @code_llvm 来分析下 LLVM，看看在底层创建了什么（LLVM 是 Julia 的编译器）。</p><p>确实，之后 Julia 编译器在这个例子中发挥了关键性的作用：</p><p><code lang=\"null\">\njulia&gt; @code_llvm count()\n;  @ REPL[7]:1 within `count`\ndefine i64 @julia_count_868() #0 {\ntop:\n;  @ REPL[7]:6 within `count`\n  ret i64 1000000000\n}\n\n</code></p><p></p><p>如您所见，编译器完全移除了循环，并选择立即返回 10 亿的值。作为一个对编译器基本一无所知的人（我也期望我能有更多的了解），这种操作着实让我大吃一惊。我写这篇文章的目的也是为了防止其他人在自己的代码中发现类似的误导性基准。</p><p></p><p>在这种情况下，该函数没有足够的计算复杂度，无法与 Python 版本进行充分比较。</p><p></p><p>如果想了解 Julia 的真实速度性能，Mosè有一个不错的帖子，它对 Julia 的速度神话提出了挑战，我强烈建议你去看看：</p><p></p><p></p><h2>Julia 如何做基准测试</h2><p></p><p></p><p>在 Julia 社区，基准测试是个热门话题，因此有相当多的文档资源。我强烈建议您通读 BenchmarkTools.j1 文档，有很多实践案例可以查看：</p><p></p><p>https://juliaci.github.io/BenchmarkTools.jl/stable/manual/</p><p></p><p>最简单的，你可以使用 @benchmark 宏对任何函数进行基准测试（提示——基准测试会“打印”代码，如下所示，可能会导致很多东西打印在你的屏幕上）：</p><p></p><p><code lang=\"null\">\njulia&gt; @benchmark print(\"Hello world\")\n Range (min … max):  25.125 μs … 87.625 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     32.792 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   37.638 μs ± 10.026 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n     ▁▃▆██▆▄▂                                 ▁                \n  ▂▅▇█████████▇▆▅▅▅▄▄▃▃▂▂▂▂▂▂▃▃▃▅▆▄▄▅▄▃▂▂▂▃▃▄▆█▇▆▅▃▃▃▄▃▂▂▂▁▁▁ ▃\n  25.1 μs         Histogram: frequency by time        59.6 μs &lt;\n Memory estimate: 48 bytes, allocs estimate: 2.\n\n</code></p><p></p><p>你也许会对 Valentin Churavy 写的关于 Julia 性能的 notebook 程序感兴趣，可以通过它更好的理解 Julia 性能。Valentin 是 Julia 的长期贡献者，在这个领域有很大的权威性。</p><p></p><p>Valentin 是一个有智慧的人，这里分享下他在 Julia slack 说过的一段话：</p><p></p><p></p><blockquote>基准测试是困难的，你首先需要确保度量的是真实的东西™</blockquote><p></p><p></p><p></p><h2>使用 Julia 可以带来哪些方面的性能改善呢？</h2><p></p><p></p><p>虽然关于 Julia 的性能有很多夸大和错误引导的文章，但使用 Julia，比起 Python 和 Matlab 等语言，在多数情况下我们还是能获得比较大的性能提升。</p><p></p><p>2022 年 4 月，<a href=\"https://sciml.ai/\">SciML</a>\" 团队在 Julia 博客上发表了一篇关于在 Julia 中使用小型网络进行科学机器学习的文章。他们将其性能与 PyTorch 的等效性能进行了比较，并在此基础上获得了 5 倍的速度提高。虽然 PyTorch 通常是同类中最好的工具，但该文章强调，当您想将科学计算结合到您的深度学习（DL）工作流程中时，Julia 在深度学习（DL）领域确实具有较好的效果。你可以在这里阅读全文：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/872fa6b7284f07ce7faade0fd1c1758c.png\" /></p><p></p><p></p><p>我们来看几个其他的例子：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8fc184822243ed4a61621aabba633724.png\" /></p><p></p><p>图片来自 julialang.org</p><p>上图标识了几种基本操作及其在每种语言中的速度。该基准测试位于于：</p><p></p><p>https://julialang.org/benchmarks/，你可以去了解更多的细节。</p><p></p><p>Julia 提高速度性能的另一个地方是读取 CSV（大多数数据科学家应该都不愿意承认他们要频繁做这个操作）。下面这篇文章写的非常好，它描述了 Julia 和 CSV.jl 是如何做到比 Python 和 R 快 10-20 倍的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/042a13a339012d07a3cc487c9c427448.png\" /></p><p></p><p>另一个案例来源于 Pfizer(辉瑞) 公司的团队，他们使用 Julia 将他们的一些模拟速度提高了 175 倍:</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/efb38754b978c680da5ff3ac07a3be9c.png\" /></p><p></p><p>为了避免重复造轮子，我想向大家推荐最后一篇关于 Julia 速度性能的文章。在下面这篇文章中，作者介绍了用 Julia 编写的一些基本算法，并将它们的性能与其他语言进行了比较:</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f24bdf728eb98e015b94925dd381a84a.png\" /></p><p></p><p>如果您有 Julia 代码方面的疑问想要获得帮助，可以到 https://discourse.julialang.org的“General Usage”主题、“Performance”子类别下发帖：https://discourse.julialang.org/c/usage/perf。</p><p></p><p>英文原文地址：https://juliazoid.com/no-julia-is-not-34-000-000-000-times-faster-than-python-f63e956313d7</p>",
    "publish_time": "2023-01-03 18:54:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]