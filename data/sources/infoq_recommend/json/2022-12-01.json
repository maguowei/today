[
  {
    "title": "腾讯云技术实践精选集 2022",
    "url": "https://www.infoq.cn/article/RuveM045ptfTNKwfeq6c",
    "summary": "<p>2022 年，数字化转型成为了全行业关注的焦点，数字经济的价值进一步凸显。云计算作为数字时代的关键要素之一，正从单一云向多云、混合云战略过渡；分布式云服务也进入了高速发展的黄金期；随着 Docker、K8s 等容器技术的流行，云原生正成为云计算的新赛场，也成为企业数字化转型发展的“默认选项”、新底座；除此之外，人工智能、边缘计算等技术的普及进一步加速了云计算技术的变革。无论是数字原生行业还是非数字原生行业，云计算在行业数字化解决方案、产业链数据流转、资源动态配置、业务创新等方面正产生着难以估量的价值。</p>\n<p>对于腾讯的技术团队来讲，2022年也是一个重要的技术里程碑之年。<span class=\"orange\"><strong>历经三年，包括 QQ、微信、王者荣耀、腾讯会议等亿级用户规模的腾讯自研业务已全面上云，集群规模突破 5000 万核，累计节省成本超 30 亿，这使得腾讯打造出国内最大规模的云原生实践。</strong></span></p>\n<p>如何把这些亿级业务全部搬到云上并实现云原生改造？腾讯云做了大量的技术优化和革新，比如：</p>\n<ul>\n<li>在容器调度领域 ，通过混部技术将资源利用率提升到 65%；</li>\n<li>在数据库领域，通过存算分离技术，打造了国内第一款云原生 Serverless 数据库；</li>\n<li>在安全领域，借助云原生技术本身的可观测性手段，创新地与安全结合，打造了更贴合云原生技术的专业安全防护能力等等。</li>\n</ul>\n<p>为此也沉淀了一份 6 万多字的《腾讯大规模云原生技术实践案例集》，包括 10 多个国民级应用的上云实践，可扫描封底二维码下载阅读。</p>\n<p>除了赋能自研业务外，腾讯云还将上述诸多产品或服务以及配套的基础软件、底层技术等开放给百万级的外部客户，全部基于公有云模式开发运营，赋能千行百业，也造就了一大批金融、游戏、企业服务、智能制造、教育等场景下的最佳实践。</p>\n<p>此外，为了解决客户上云、用云的成本之忧，腾讯云基于内外云原生成本管理最佳实践，并结合行业优秀案例，提出了一套体系化的云原生上云成本优化方法论和最佳实践路径，发布了两个业界“标准”：《云原生最佳实践路线图》和《降本之源 · 云原生成本管理白皮书》，旨在帮助企业改善用云成本，充分发挥云原生的效能和价值。</p>\n<p>2022 年，是不平凡的一年，感恩来自行业、伙伴、团队的力量推动着我们勇往直前。在今年，我们参与了 DIVE 2022 北京站、ArchSummit 2022 深圳站、QCon 2022 广州站、ArchSummit 2022 北京站、ArchSummit 2022 杭州站等多场大会，与 1000+ 位技术人邂逅并分享心得。</p>\n<p>此外，这也是腾讯云连续两年推出《腾讯云技术实践精选集》，去年 2021 版精选集共 4 万多字，全网带来 7000 多次下载。<span class=\"orange\"><strong>2022 版的精选集总字数近 10 万，尤其首次收录了“腾讯自研业务大规模云原生实践”系列内容，全面解密腾讯如何锤炼腾讯云。</strong></span></p>\n<p>每一次相遇，都难能可贵，每一场交流，都价值满满，遂整理成文，共享丰沃。</p>\n<p>展望 2023 ，愿与诸君携手同行，共攀技术新峰！</p>\n<h1>目录</h1>\n<h2>第一部分  腾讯自研业务大规模云原生实践</h2>\n<ul>\n<li>如何管理超千万核资源的容器规模</li>\n<li>50W+ 小程序开发者背后的数据库降本增效实践</li>\n<li>拥抱云原生，数十万规模 GPU 卡的利用率极致优化之路</li>\n<li>TDSQL-PG 数据库在微信支付的应用实践</li>\n<li>将云原生进行到底：腾讯百万级别容器云平台实践揭秘</li>\n<li>云原生安全可观测性探索与实践</li>\n<li>大规模代码中引入供应链漏洞的分析技术前瞻</li>\n</ul>\n<h2>第二部分  大数据与云数据库技术探索及实践</h2>\n<ul>\n<li>腾讯云大数据 TBDS 在私有化场景万节点集群的实践</li>\n<li>PB 级数据秒级分析，腾讯云原生湖仓 DLC 架构揭秘</li>\n<li>CDW PG 大规模在线数仓技术构架分享</li>\n<li>云原生数据库管控探索和实践</li>\n<li>腾讯云原生数据库 TDSQL-C 架构探索和实践</li>\n<li>金融级分布式数据库 TDSQL 升级版引擎架构和关键技术介绍</li>\n<li>国产金融级分布式数据库在金融核心场景的探索实践</li>\n<li>腾讯云 MongoDB 智能诊断及性能优化实践</li>\n<li>腾讯云数据库云上 SaaS 生态演进</li>\n</ul>\n<h2>第三部分  云成本优化与研发提效</h2>\n<ul>\n<li>企业上云，云上资源整体成本优化管理如何做？</li>\n<li>企业如何利用云厂商能力构建自己的分布式云？</li>\n<li>从混部到 Serverless 化，腾讯自研业务的稳定性及云原生成本优化实践</li>\n<li>Serverless 时代下，企业微服务的降本思考与实践</li>\n<li>腾讯课堂面向协作的 DevOps 流程设计与实践</li>\n</ul>\n<h2>第四部分  中间件与基础设施</h2>\n<ul>\n<li>Kafka Stream - 的进化探索：流式 Serverless 计算</li>\n<li>JVMTI Agent在中间件领域的应用</li>\n<li>区块链如何支撑 Web 3.0</li>\n<li>腾讯操作系统的创新之路</li>\n<li>腾讯明眸媒体处理实践</li>\n</ul>",
    "publish_time": "2022-12-01 01:55:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何用Go语言构建、测试和部署可扩展的REST API",
    "url": "https://www.infoq.cn/article/LdimuNAoab0ZHpmVDKOf",
    "summary": "<p></p><h2>引言</h2><p></p><p>在本文中，我们将了解如何使用<a href=\"https://gin-gonic.com/\">gin</a>\"框架创建一个简单的Golang应用程序。我们还将学习如何使用持续部署工具<a href=\"https://circleci.com/\">CircleCI</a>\"实现自动化测试和部署。</p><p>&nbsp;</p><p>Go是一种静态类型的开源编程语言，由谷歌工程师创建，其唯一的目的是简化复杂的软件开发过程和架构。它的主要特性包括：高性能网络、并发性和易用性。Go中广泛使用了Goroutine。Goroutine是一个在程序中与其他Goroutine并行运行的函数。当需要同时做几件事时，Goroutine会很有用。举例来说，谷歌、Cloudflare、MongoDB、Netflix和Uber几家公司都使用了Go。</p><p>&nbsp;</p><p><a href=\"https://gin-gonic.com/\">Gin</a>\"是一个基于Go的高性能HTTP Web框架，可以用来构建微服务和Web应用程序。Gin的主要优势在于，它使得开发人员可以创建高效、可扩展的应用程序，而且不必编写大量的样板代码。它生成的代码简洁、易于阅读。它还内置了路由、用于处理功能的中间件、日志记录器和Web服务器。</p><p></p><h2>构建一个简单的CRUD API</h2><p></p><p>我们将为学院俱乐部的学生管理工具创建一个简单的API。完成后，俱乐部主席将能够新增学生及检索所有学生。如果想完全按照本教程来操作，则需要做好以下准备：</p><p>安装Go，了解该语言的基本知识；了解测试，知道如何编写测试；一个GitHub账号；一个CircleCI账号；一个Heroku账号。</p><p>&nbsp;</p><p>请注意，如果你想使用Heroku的免费帐户进行试验，那么Heroku很快就会将其停用。不过，这里描述的过程可以很容易地应用于其他大多数云托管平台。</p><p></p><h3>构建一个CRUD应用程序</h3><p></p><p>这个简单的学院俱乐部API只有两个功能：将学生添加为会员和查看所有会员；没有什么复杂的东西！我们将用到POST和GET请求。我们不会连接任何数据库，如MongoDB或MySQL。但是，我们将使用本地存储并默认在数据库中创建一个学生。每当服务器重启时，就会自动添加这个学生。</p><p>&nbsp;</p><p>让我们开始吧。首先，我们将创建一个项目文件夹，并命名为stup -api。在这个文件夹中，我们将初始化Golang程序并安装所需的所有依赖。</p><p><code lang=\"go\">mkdir stud-api\ncd stud-api</code></p><p>接下来，我们将初始化go.mod文件，并安装所需的所有依赖：</p><p><code lang=\"go\">go mod init stud-api\ncd stud-api\ngo get -u github.com/gin-gonic/gin github.com/rs/xid github.com/stretchr/testify </code></p><p><a href=\"http://github.com/rs/xid\">Github.com/rs/xid</a>\"是一个用于创建惟一标识的库。在这个项目中，我们将用它自动为每个新学生生成一个ID。我们将用<a href=\"http://github.com/stretchr/testify\">github.com/stretchr/testify</a>\"包测试各个端点。</p><p>&nbsp;</p><p>下面开始讨论API。简单起见，我们只创建一个名为main.go的文件。这个文件将包含struct 、API控制器、服务和路由。我们将创建三个端点：</p><p>一个发送欢迎消息的欢迎函数；一个将学生添加到数据库的CreateStudent()&nbsp;函数；一个返回数据库中所有已注册学生的GetStudents()函数。</p><p>&nbsp;</p><p>下面在新创建的main.go文件中导入三个包：HTTP包、xID包和gin包。接下来，编写一个main()函数，其中将包含所有的API路由。然后，另外创建一个函数WelcomeMessage()，在调用相关的路由时，它会打印一条简单的消息。</p><p><code lang=\"go\">package main\nimport (\n\"net/http\"\n\"github.com/gin-gonic/gin\"\n\"github.com/rs/xid\"\n)\n\n\nfunc main() {\n//设置路由\nrouter := gin.Default()\nrouter.GET(\"/\", WelcomeMessage)\nrouter.Run()\n}\n//欢迎消息\nfunc WelcomeMessage(c *gin.Context) {\nc.JSON(http.StatusOK, gin.H{\"message\": \"Hey boss!\"})\n}</code></p><p>现在，可以使用下面的命令来启动服务器，看看到目前为止我们都做了什么：</p><p><code lang=\"go\">go run main.go</code></p><p>如果运行成功，则CLI将显示“Hey boss!”。这个简单的函数就创建完成了。现在我们将继续讨论数据库和struct 。</p><p>&nbsp;</p><p>我们将构建一个简单的Student struct ，它接受三个参数：学生姓名、学院和年级，并在用户成功添加到数据库时为其生成一个ID。</p><p><code lang=\"go\">//定义学生结构\ntype Student struct {\nID         string `json:\"id\"`\nName       string `json:\"name\"`\nDepartment string `json:\"department\"`\nLevel      string `json:\"level\"`\n}</code></p><p>现在，我们创建下本地数据库，它将存储我们传递给服务器的三个值以及生成的ID。我们将数据库命名为Students，其中会包含一个学生的默认数据，而新创建的任何学生都会添加到这里。</p><p><code lang=\"go\">//学生数据库\nvar students = []Student{\n{\nID:         \"10000xbcd3\",\nName:       \"Alicia Winds\",\nDepartment: \"Political Science\",\nLevel:      \"Year 3\",\n},\n}</code></p><p>好了，数据库设计就完成了，现在我们编写下CreateStudent()函数以及与其交互的路由。</p><p><code lang=\"go\">//新建一个学生账号\nfunc CreateStudent() gin.HandlerFunc {\nreturn func(c *gin.Context) {\nvar newStudent Student\nif err := c.BindJSON(&amp;newStudent); err != nil {\nc.JSON(http.StatusBadRequest, gin.H{\n\"Status\":  http.StatusBadRequest,\n\"Message\": \"error\",\n\"Data\":    map[string]interface{}{\"data\": err.Error()}})\nreturn\n}\n//生成一个学生ID\nnewStudent.ID = xid.New().String()\nstudents = append(students, newStudent)\nc.JSON(http.StatusCreated, newStudent)\n}\n\n\n}</code></p><p>现在将与该函数交互所需的路由添加到main()函数。</p><p><code lang=\"go\">func main() {\n-------------\nrouter.POST(\"/createStudent\", CreateStudent())\n-------------\n}</code></p><p>要测试到目前为止所做的工作，请启动服务器，并在Postman或任何其他环境中测试端点（localhost:8080/createStudent）。在消息体中传递姓名、学院和年级，就会自动生成一个具有惟一ID的新用户。请注意，这是一个非持久化数据库。</p><p>&nbsp;</p><p>现在，让我们创建最后一个函数。我们将使用它来获取俱乐部数据库中的所有学生。这个请求是一个简单的GET函数，它将搜索学生数据库并返回其中的所有内容。</p><p><code lang=\"go\">func GetStudents() gin.HandlerFunc {\nreturn func(c *gin.Context) {\n//获取数据库中的所有学生\nc.JSON(http.StatusOK, students)\n}\n}</code></p><p>最后，我们将创建与新建函数进行交互的路由。我们将把它加入主函数，和其他路由放在一起。</p><p><code lang=\"go\">func main() {\n------------------\nrouter.GET(\"/students\", GetStudents())\nrouter.Run()\n}</code></p><p>也使用Postman测试一下！为此，我们需要启动服务器并访问端点localhost:8080/students。我们所需要做的就是使用HTTP谓词GET，不需要包含任何消息体或查询参数。运行成功后，它将返回数据库中的所有学生。这样，这个简单的CRUD API就完成了！</p><p></p><h2>编写简单的本地测试</h2><p></p><p>在这一节中，我们将对已创建的端点进行单元测试。目标是确保每个函数的行为都符合预期。为了测试这些函数，我们将使用<a href=\"https://github.com/stretchr/testify\">testify</a>\"包。此外，我们必须新建一个文件new_test.go。我们将要编写的各种测试都将放在这个文件中。在主目录的根目录中创建完新文件后，我们需要导入几个包。</p><p><code lang=\"go\">func main() {\n------------------\nrouter.GET(\"/students\", GetStudents())\nrouter.Run()\n}</code></p><p>在testify中，执行简单的断言和模拟都很容易。在Go中，testing.T对象作为assert函数的第一个参数传入。然后，assert函数会返回一个bool值，说明断言是否成功。<a href=\"https://github.com/stretchr/testify\">testify mock</a>\"包提供了一种快速创建模拟对象的方法，在编写测试代码时可以用它代替实际的对象。</p><p>&nbsp;</p><p>现在，我们将设置一个路由，并为欢迎消息编写一个简单的测试。如下所示，在这个测试中，assert函数将使用变量的相等比较来确定测试参数是否与模拟响应相匹配。</p><p><code lang=\"go\">func SetRouter() *gin.Engine {\nrouter := gin.Default()\nreturn router\n}\n\n\nfunc TestWelcomeMessage(t *testing.T) {\nmockResponse := `{\"message\":\"Hey boss!\"}`\nr := SetRouter()\nr.GET(\"/\", WelcomeMessage)\nreq, _ := http.NewRequest(\"GET\", \"/\", nil)\nw := httptest.NewRecorder()\nr.ServeHTTP(w, req)\nresponseData, _ := ioutil.ReadAll(w.Body)\nassert.Equal(t, mockResponse, string(responseData))\nassert.Equal(t, http.StatusOK, w.Code)\n}</code></p><p>接下来，我们将使用模拟数据为createStudent()函数编写一个简单的测试。还是使用xID包来生成Student ID，我们会收到一个说明测试是否成功的bool值。</p><p><code lang=\"go\">func TestCreateStudent(t *testing.T) {\nr := SetRouter()\nr.POST(\"/createStudent\", CreateStudent())\nstudentId := xid.New().String()\nstudent := Student{\nID:         studentId,\nName:       \"Greg Winds\",\nDepartment: \"Political Science\",\nLevel:      \"Year 4\",}\njsonValue, _ := json.Marshal(student)\nreq, _ := http.NewRequest(\"POST\", \"/createStudent\", bytes.NewBuffer(jsonValue))\nw := httptest.NewRecorder()\nr.ServeHTTP(w, req)\nassert.Equal(t, http.StatusCreated, w.Code)}</code></p><p>最后，我们将针对GetStudents()函数编写最后一个测试。</p><p><code lang=\"go\">func TestGetStudents(t *testing.T) {\nr := SetRouter()\nr.GET(\"/students\", GetStudents())\nreq, _ := http.NewRequest(\"GET\", \"/students\", nil)\nw := httptest.NewRecorder()\nr.ServeHTTP(w, req)\nvar students []Student\njson.Unmarshal(w.Body.Bytes(), &amp;students)\nassert.Equal(t, http.StatusOK, w.Code)\nassert.NotEmpty(t, students)\n}</code></p><p>我们已经完成了所有的测试，现在可以在本地运行了。这很简单，只需执行下面这行命令：</p><p><code lang=\"go\">GIN_MODE=release go test -v</code></p><p>下面是最终结果：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c89269c1ab1abe45d5b1c8055955e19b.png\" /></p><p></p><p></p><h2>利用持续开发实现测试自动化</h2><p></p><p><a href=\"https://circleci.com/\">CircleCI</a>\"是一个用于持续集成和交付的平台，可用于DevOps实践。在本文中，我们将使用这个CI/CD工具实现测试自动化并将代码部署到服务器上。我们先从使用CircleCI自动化测试开始说起。</p><p>&nbsp;</p><p>确保你有一个CircleCI帐户（正如准备工作部分所介绍的那样），并且已经成功地将代码推送到GitHub。检查CircleCI仪表板，确保项目存储库是可见的。</p><p>&nbsp;</p><p>现在，在项目目录中，创建文件夹.circleci和配置文件config.yml，该文件将包含自动化测试所需的命令。</p><p></p><h2>配置config.yaml</h2><p></p><p>该文件包含自动化Heroku部署和测试所需的所有配置。我们暂时不关注Heroku部分，因为我们更感兴趣的是帮助实现自动化测试的代码。该文件包含检出并运行测试的Go orb和作业。在将下面的代码添加到配置文件后，我们需要将其重新推送到GitHub。</p><p><code lang=\"go\">workflows:\n  heroku_deploy:\n    jobs:\n      - build\n      - heroku/deploy-via-git:  \n          requires:\n            - build\n          filters:\n            branches:\n              only: main\njobs:\n  build:\n    working_directory: ~/repo\n    docker:\n      - image: cimg/go:1.17.10\n    steps:\n      - checkout\n      - restore_cache:\n          keys:\n            - go-mod-v4-{{ checksum \"go.sum\" }}\n      - run:\n          name: Install Dependencies\n          command: go get ./...\n      - save_cache:\n          key: go-mod-v4-{{ checksum \"go.sum\" }}\n          paths:\n            - \"/go/pkg/mod\"\n      - run:\n          name: Run tests\n          command: go test -v</code></p><p>完成这一步之后，返回CircleCI仪表板并选择我们的项目。然后，单击它旁边的Setup按钮，并选择我们正在使用的分支。当我们点击Setup按钮时，程序将开始运行。构建成功的话应该可以看到如下所示的信息（向下滚动到运行测试的部分）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dcd8ecfbdd26422690454f942590e501.png\" /></p><p></p><p>就是这样！我们成功地构建了一个简单的API，创建了本地测试，并实现了测试过程自动化。这个自动化过程意味着，每次向GitHub存储库上的分支推送时，管道都会尝试运行测试。</p><p></p><h2>使用CircleCI自动部署到Heroku</h2><p></p><p>首先是配置Heroku。如果你还没有Heroku帐户，就需要创建一个。为了方便部署和自动化，你还需要将GitHub配置文件连接到Heroku帐户。上述工作完成之后，需要在项目文件夹中创建一个Procfile（是的，没有扩展名），并向其中添加以下内容：</p><p><code lang=\"go\">web: app</code></p><p>之后，推送到GitHub。现在，快速看一下之前创建的config.yaml文件，分析下第一部分。可以看到，我们导入了Heroku orb，其中还有一个工作流，里面是一个在主存储库中构建和部署代码的作业。</p><p>&nbsp;</p><p>回到Heroku仪表板，我们必须首先在Heroku上创建一个项目，并获取API密钥（可以从帐户设置中找）。我们需要把这个密钥添加到我们的CircleCI项目。为此，在CircleCI上导航到现有项目并选择项目设置。然后转到环境变量部分，添加下面这两个东西：</p><p>HEROKU_APP_NAME，值为stud-api&nbsp;（应用程序名称）；HEROKU_API_KEY&nbsp;，值为我们刚刚从Heroku获取的密钥。</p><p>&nbsp;</p><p>我们已经成功地配置了我们的CircleCI项目，可以向Heroku持续部署了。如果没什么问题，在CircleCI仪表板上，我们应该可以看到下面这样一条说明构建已经成功的消息：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/94ab3fc91daeedd7c2f24286909ac969.png\" /></p><p></p><p>返回Heroku仪表板并检索项目URL，看看我们都做了什么。这里，URL是：<a href=\"https://stud-app-api.herokuapp.com/\">https://stud-app-api.herokuapp.com/</a>\"。你可以将想要测试的路由附加到URL末尾来测试所有的功能。例如，测试获取所有学生的端点：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4e/4eb298655ea8ae784774f6ec8fd9287a.png\" /></p><p></p><p></p><h2>小结</h2><p></p><p>持续开发使开发人员能够更快地创建更好的产品。持续集成和开发工具通过自动化操作简化了整个过程，减少了所需的时间或专业知识。CI/CD工具通过自动化从测试到应用程序快速部署之间的所有事情，帮助我们逐步提高产品质量。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/build-deploy-scalable-golang-api/\">https://www.infoq.com/articles/build-deploy-scalable-golang-api/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/jfkZ7LHF1HbONN2sPOyw\">REST 如何站到了自己的对立面？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/abaa53f80114223d2940f439d\">什么是 RESTful，REST api 设计时应该遵守什么样的规则？</a>\"</p>",
    "publish_time": "2022-12-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一年覆盖九种语言上千服务，作业帮 Service Mesh 如何大规模落地？",
    "url": "https://www.infoq.cn/article/98SQPazzQUxMglZx28Yq",
    "summary": "<p>2019 年底，<a href=\"https://www.infoq.cn/article/gNzSTfhgs8xzRlYgciEv\">作业帮</a>\"技术栈比较多元且业务复杂度较高：使用最多的语言为 PHP 和 Golang，约占到总模块数量的 60% 左右；此外还有大量的系统使用 NodeJs、Java、C++、lua、python 编写等，即使是同样的技术栈，也会因为业务特点、团队特点，技术栈上呈现出较大差异；工具型产品侧重客户端，服务端技术偏保守。产业互联网业务，领域驱动，大量使用微服务架构，但由于没有统一的标准，各自团队也在自研服务治理体系的基础设施。</p><p></p><p>语言栈多元后，业务间的沟通协作就变得困难。跨语言的微服务框架难以落地统一的服务治理标准，致使业务服务治理参差不齐，大大限制了业务的快速迭代和稳定性。</p><p></p><p>同时作业帮的服务调用方式比较多样化，有 HTTP、gRPC、自研协议等，治理难度非常大，不同的 RPC 协议也会导致服务间通信困难。作业帮当时已有数千个服务，且服务间请求链路较长，一次请求的深度可能就超过一百多、甚至数百。</p><p></p><p>此时，已经完成容器化进程的作业帮开始着手调研服务治理技术，希望借助 <a href=\"https://xie.infoq.cn/article/7cb4393bd2828d5300edb31ec\">Service Mesh</a>\" 技术，来解决当时复杂的服务治理问题。</p><p></p><p>同年，为了适应云原生发展，作业帮进行了组织架构调整，将运维、安全、数据库、架构研发和部分通用中台等统一归纳到了基础架构团队。Service Mesh 的工作也落到了基础架构团队头上。</p><p></p><p>解决复杂治理问题最大的“拦路虎”是：业务既渴望使用新技术，但又担忧会带来高昂的使用成本，因此宁愿原地停留也不进行升级。那么，作业帮基础架构团队是如何解决这一问题的呢？这次，InfoQ 有幸采访了基础架构团队负责人董晓聪和架构研发团队负责人吕亚霖，为我们详细阐述了作业帮的实践思路和效果。</p><p></p><h3>全自研 Mesh</h3><p></p><p></p><p>“基础架构团队在进行服务治理时，既要保证稳定性，还要能够帮助业务提升效率。”董晓聪总结道。具体来说，团队的核心目标就是让 Service Mesh 接管服务治理里大量的非功能逻辑，实现服务的流量控制、可观测性和安全韧性，并且通过能力下沉，让业务透明、无感地接入。</p><p></p><p>但面对当前市面上各种开源产品，基础架构团队在前期充分调研后，认为这些产品都不太能满足作业帮的需求，因此选择了一条全自研的路。</p><p></p><h4>数据面，更看重性能</h4><p></p><p></p><p>数据面上，作业帮更倾向追求极致的性能，而对扩展性的需求并不迫切。这是由于作业帮内部的链路较长（上百 span），对业务来说哪怕是单次请求毫秒级的损失，整体上都会带来较大的影响。</p><p></p><p>对于主流产品<a href=\"https://www.infoq.cn/article/aulwXCUFJctf5FK5zYzu\"> Envoy</a>\"，作业帮基础架构团队不太认同它的插件模式。“插件机制引入主要是为了降低开发难度，虽然能有效降低研发成本，但是性能较差，同时提供的隔离也不彻底。”作业帮架构研发团队负责人吕亚霖说道。</p><p></p><p>因此，基础架构团队用 C++ 自己实现数据面，初期按需求紧迫程度支持了三类协议：第一类是 RPC 数据面，负责服务通信；第二类是对象存储数据面，负责对象存储资源的鉴权管理、流控、分发及性能提升；第三类是加解密数据面，负责提供安全和数据加密的能力。这三种数据面的协议差异性很大，分别对应了 RPC 通信协议、文件读写流协议、加解密协议，且对性能要求极为苛刻。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/51/518b0f6597c8c67dccb482683d2a941a.png\" /></p><p></p><p>整体上，基础架构团队主要针对 RPC 协议，采用了 Mesh-proxy 代理方式。对于入流量，直接通过 UDS 转发给了 Server，Server 需要支持 UDS 监听；对于出流量，则与 Istio 类似，用 iptable 拦截，不过团队另用 eBPF 做了优化。</p><p></p><p>吕亚霖表示，iptable 出流量拦截过程复杂，带来了不少的性能损失，因此团队使用 eBPF 优化内核网络劫持，在 Sock Map 进行映射、省掉中间环节，就可以极大提升劫持性能。eBPF 优化的前提是系统内核是 5.X 版本以上，作业帮使用的是 5.10 版本，主要是 5.10 版本解决了 CO-RE 有利于后期的升级维护，而 Istio 为了适应大部分客户内核低版本情况并没有支持。</p><p></p><p>根据作业帮此前的测试，以 Envoy 官方数据为例，在 QPS 为 1000 下，Envoy 在模式的 v2-stats-wasm_both 下 P90 响应时间是 2.25 毫秒，相比于 baseline 测试结果，添加了 Envoy 边车之后 P90 时延增加了 1.3 毫秒，且随着并发加大逐步恶化。作业帮自研数据面实测添加了 mesh 边车之后 P90 时延增长比这个数据降低了 0.38 毫秒，随着并发加大比较稳定。当 QPS 从 1000 加到 10000 时，平均响应时间减少 0.4 毫秒。另外在 CPU 耗损上，在 QPS 为 1000 的情况下，Envoy 耗损 0.54 核，作业帮的数据面损耗 0.09 核。这项测试也验证了作业帮数据面的可用性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/16/1688eb90c29aa220e469724c0543cd7e.png\" /></p><p></p><p>事实上，从立项到数据面研发完成，基础架构团队只用了两个月左右的时间。“我们是把问题进行了拆解和聚焦，使用了云原生 Kubernetes 的基座，初期只对 RPC 类流量做了 mesh 劫持。同时聚焦数据面性能本身。”作业帮基础架构负责人董晓聪说道。</p><p></p><h4>控制面，纳入统一管控平台</h4><p></p><p></p><p>控制面上，作业帮本身有完整的运维管控体系，即包含服务注册发现相关，还有服务感知的一系列能力。所以作业帮并没有单独做一个 Service Mesh 的控制面，而是基于已有体系扩展，直接实现进行流量管控、通信协议、安全等。</p><p></p><p>对于作业帮来说，管控面是 DevOps 理念的落地，能更好的贴合自身研发流程和组织管理，比如：统一鉴权、工单体系、审计体系等。</p><p></p><p>对于 Service Mesh 的控制面，作业帮并没有引入类 Istio 在 Service Mesh 的实现服务注册发现，而是使用原生的 Kubernetes 的注册发现。作业帮基础架构团队认为，在大型的复杂工程中，架构设计的核心目标是复杂度降维，而将服务注册发现引入到 Service Mesh 却是在将复杂度升维，Kubernetes 的服务发现是 node 级别，而 Istio 的服务发现是 pod 级别，服务发现数据是数量级的提升。而对于升维带来的更强计算力和内存需求等问题也没有给出很好的解决方案。比如 Istio 的 xDS 使用全量下发策略，会把整个网格内所有服务发现数据同步给 proxy 边车，这会带来一系列的问题。</p><p></p><p>另外根据团队观察，很多公司在接入的时候，精力被耗在了 xDS 对接上，从原有协议向 xDS 转换过程中也充斥着各种各样的问题。</p><p></p><p>基于以上考虑，作业帮并没有使用 xDS 来提供发现机制，而是沿用了 Kubernetes 里注册发现方式，通过 eBPF 来优化 service，也具备路由决策等能力。在基础架构团队看来，这种方式更加轻量级，也与 Kubernetes 社区的结合更为紧密。</p><p></p><p>另外在安全性方面，团队进行了认证授权、网络拦截等；观测性上，全量埋下了分布式追踪，支持日志的统一观测和监控等。</p><p></p><p>在作业帮基础架构团队看来，用不用某个协议只是看其能否满足实际需求或者是否认可它的模式。用 xDS 的好处是会有统一标准，但 xDS 能否成为事实标准，还有待时间的检验。</p><p></p><p>吕亚霖表示，目前将 xDS 当作标准还有一些风险。“xDS 成为标准的前提是升维带来的复杂度被其他方面拆解掉，但它现在的降维手段并不是一个通用方案，我们在看到新方案出来之前是不会跟进的。”不过团队也会继续观望，关键还是要看其能否在大型企业的大规模复杂场景下落地。</p><p></p><h3>超预期的推广速度</h3><p></p><p></p><p>研发完成后，基础架构团队开始了“边放量、边灰度、边优化”的循环。对作业帮来说，基础架构团队涵盖了不同方向的小组，团队内就可以完成闭环测试，然后再向业务推广。</p><p></p><p>实际上，Mesh 在作业帮内部的推广速度远超基础架构团队的预期。据悉，目前作业帮 Mesh 覆盖率已经超过 80%，涵盖了 C++、Python、PHP、Go、Java 等语言栈。董晓聪表示背后的核心原因有多个，一是流量管控和观测，真正帮助到了业务，二是 Mesh 是相对无感接入，三是 Mesh 落地提升了研发效率。</p><p></p><p>流量管控和观测方面：主要实现了流量管控、安全（认证授权）、可观测性（日志统计、全量分布式追踪）。流量管控上实现了很多业务期待的功能，比如自适应限流。很多情况下业务需要熔断限流，但是不太可能每个服务都进行压测配置限流阈值，同时业内的自适应限流算法与流量以及自身资源相关，也无法实现因下游服务容量问题时主动限流。作业帮将机器学习应用于该领域，实现了智能的流控，当通用模型不适合某些特定流量场景时，也提供基于过去半年的数据，学习和训练特有模型。</p><p></p><p>接入相对无感：业务几乎不需要改造，只需要兼容 listen uds，其他由基础架构直接升级，然后进行放量观察即可。改造成本极低。</p><p></p><p>研发效率方面：在原来的微服务开发模式下，模块非常多，业务进行联调和测试的效率非常低。比如联调要经过 CI/CD、发布到测试环境、验证等一系列复杂的过程，而微服务里改动一个需求可能就要改动几十个模块。一个研发每次改动就要重复一遍上述过程。而 Service Mesh 在研发阶段的作用就是使研发可以直接在本地启动服务，并且这个本地服务可以无缝的和远程的 Kubernetes 测试集群中的各个其它服务实现互相调用，免去了复杂的 CI/CD 流程。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/73/733f51abdd4227ad9a6c338455149b6b.png\" /></p><p></p><p>“一个业务线里面有一两个服务用了发现效率提升非常明显后，会形成很好的口碑效应，之后该业务其他研发会主动找过来要求升级。”吕亚霖表示。据悉，作业帮内部研发主动升级的比例在 50% 左右。</p><p></p><p>不过在推广过程中，基础架构团队也踩过坑。一次灰度放量时，某业务线使用了不规范的 HTTP 协议，但 Mesh 拦截 RPC 流量是按标准协议处理的，所以就产生了冲突，该问题影响了服务间的通信。</p><p></p><p>“这也让我们认识到，多语言栈下的协议确实没有那么规范，而这种不规范更多是因为各个语言，在协议实现层面上的差异导致。”董晓聪说道，“同时也展现了 Service Mesh 这项技术的意义，即接管服务治理里大量的非功能逻辑，实现标准化。”</p><p></p><p>除了这次事故，其他方面都远远超过了团队的预期，甚至由于需求变多，团队不得不投入更多的人力。</p><p></p><p>同时这次 Mesh 落地实践也让基础架构团队深刻体验到了内核研发能力的重要性。</p><p></p><p>“服务的各种小问题会特别容易摧毁业务线对你的信任。问题多了，哪怕是他业务的问题也会怀疑是 Service Mesh 的问题。”吕亚霖表说道，“这些问题都要及时解决，但在这个过程中，团队发现最后这些问题大部分需要在内核层面定位和配合解决。比如业务反馈平响多了几毫秒或者毛刺率变高，都需要在内核底层进行追踪定位，需要基础架构团队有一套分析定位内核的工具，同时内核追踪需要和业务的分布式追踪关联，才能快速定位解决问题。</p><p></p><p>总体来看，作业帮 Service Mesh 的落地收益比还是很不错的。作业帮基础架构团队只投入了两个专职人力，进行 Service Mesh 的研发和迭代。推广上持续了一年。通过 Mesh 的协议升级，带来了性能和稳定性的提升，比如 PHP、python 很多都是 HTTP1.0 的短链，团队通过 mesh 层进行协议协商升级，在业务无感的情况下将 HTTP 升级到了 2.0，减少网络建连、提升了网络传输速率。同时 Service Mesh 的落地带来了治理能力的大幅提升，全量的分布式追踪、完善的监控报警，以及在协议实现上的标准和统一。</p><p></p><h3>结束语</h3><p></p><p></p><p>虽然如此，吕亚霖提醒道，业务体量不大的时候跟风上 Service Mesh 并不会带来太多收益，甚至可能使整体产品的迭代变慢。多语言栈、链路比较长、业务要求比较高的情况更适合接入 Mesh。</p><p></p><p>“真正落地的时候你就会知道适不适合了，真正引入 Mesh 后发现推广不下去，本质上就说明了不适合。一项技术非常匹配时，推广便是一件水到渠成的事情。”吕亚霖说道。</p><p></p><p>另外，企业真的要接入 Mesh，也没有必要自研。自研对企业的研发能力要求会比较高。首先，Mesh 自研对 C++ 工程师的研发能力要求很高，但如今很多公司大多是 Java 工程师做基础架构。其次，根据作业帮基础架构团队的经验，团队要有内核研发和定位的能力，对于一些小公司来说可能并不具备这样的条件。</p><p></p><p>对于当前自研的选择，吕亚霖表示，这是有很多前提条件的。“比如 97% 的容器化、整体内核升到了 5.10、多语言栈、业务链条复杂、对时延敏感等，我们会觉得自研会更有优势。一旦这些前提不在了，自研就不见得比选择现有产品更好。”</p><p></p><p>嘉宾介绍：</p><p></p><p>董晓聪，作业帮基础架构负责人</p><p>吕亚霖，作业帮基础架构 - 架构研发团队负责人</p><p></p><p></p>",
    "publish_time": "2022-12-01 09:36:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "火山引擎公开课",
    "url": "https://www.infoq.cn/article/xUq5Q66QSwsTU4G3sLQx",
    "summary": "<p>目前，大数据、“云”相关技术逐渐成熟，处于大规模落地应用实践的喷发期，有人称现在这两个技术都要被提“烂”了，几乎每家都在做，无论是传统企业寻求数字化转型之路，还是互联网行业寻找新的业务增长点，都想要在这两者身上找到突破口。企业想要通过“云”完成降本增效和技术创新，通过大数据技术完成数据治理、数据分析等可以促进业务侧增长的任务。</p>\n<p>然而，当企业在应用了云和大数据等热门技术后会发现，企业规模扩大到一定程度后，不仅要考虑短时间内业务增长问题，而且还要能够找到持续实现业务增长的方式，“从 0 到 1 容易，从 1 到 100 不容易”。本视频便从“云”和“数据”两个方面给出了增长解决方案。</p>\n<h2><strong>视频大纲</strong></h2>\n<h4>《低成本也能稳增长，三招实现“数据驱动”的业务增长》</h4>\n<p>1 什么是低成本稳增长<br />\n2 如何实现低成本稳增长<br />\n2.1 构建用户画像，实现精细化运营<br />\n2.2 数据分析，驱动科学决策<br />\n2.3 A/B实验，用户增长的秘密武器<br />\n3 数据驱动业务增长背后的故事<br />\n3.1 火山引擎VeDI数智平台<br />\n3.2 A/B实验平台<br />\n3.3 智能数据洞察平台<br />\n3.4 客户数据平台</p>\n<h4>《亿级流量场景下的火山引擎边缘云应用实践》</h4>\n<p>1 超大规模流量场景面临的挑战</p>\n<p>2 火山引擎边缘云流量场景全链路解决方案<br />\n2.1 便捷的流量接入<br />\n2.2 高效的内容分发网络<br />\n2.3强大的边缘云基础设施<br />\n3 最佳实践</p>",
    "publish_time": "2022-12-01 10:39:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022 亚马逊云科技 re:Invent：一图看尽 Day 2 重要发布",
    "url": "https://www.infoq.cn/article/h2KIBbX6lPI0b8Fj0Reb",
    "summary": "<p></p><p><img src=\"https://static001.infoq.cn/resource/image/d2/e3/d268yyea5d05981804e04878c70947e3.png\" /></p><p></p>",
    "publish_time": "2022-12-01 11:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "聊聊云计算和 re:Invent 的现在和未来",
    "url": "https://www.infoq.cn/article/gYjPO4eITE4OIEteRFSC",
    "summary": "<p>云计算的出现给不同国家的开发者带来了哪些不一样的改变？对于re:Invent的未来，开发者们有着怎样的期待？日本开发者表示：希望现场能多点座位，因为那些高人气的会议会迅速爆满，手慢根本抢不到座位。的确，re:Invent 的人气真的太旺了！你对这场云计算盛会的未来有着怎样的期待呢？</p>",
    "publish_time": "2022-12-01 12:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "郭华：每一个人都不简单，每一瓶酒才放光彩——华润雪花的数字化人才观 ｜ 数字人才蓄能季高端论坛",
    "url": "https://www.infoq.cn/article/sCrEs13sPIU5xUTwVNkT",
    "summary": "<p>企业数字化转型过程中，IT 团队想要胜任领头羊角色，首先需要完成自我变革。重塑观念、行为和协作方式，与业务深度链接，才能发挥出技术的核心价值。IT 团队要如何助力企业实现数据驱动、科技驱动的业务增长？应该具备怎样的数字化能力？如何适应数字化组织的变革？您可以从「数字人才蓄能季高端论坛」的分享中探寻答案。</p>\n<p>极客时间企业版和培训杂志共同举办「数字人才蓄能季高端论坛」，吸引了上万名观众线上参会。论坛上华润雪花的信息化、数字化负责人 郭华分享了雪花转型升级过程中，公司人员规模缩减 50% 的挑战下，如何通过“补短板、提质量、增效益”，打造专业分工明确的数字化人才梯队，构建作战型团队构成的、灵活的、充分数字化的组织，成就雪花啤酒华丽蜕变。极客邦科技 COO 司巧蕾发布了“极客时间专项 IT 核心人才发展计划”，提供研测学考评一体化培训体验，助力企业打造卓越 IT 团队。极客时间内容总监李佳也带来了技术人才岗位能力模型搭建方法论和案例分享。</p>",
    "publish_time": "2022-12-01 12:03:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何破解Web3的「存力」难题？",
    "url": "https://www.infoq.cn/article/1pIQYNvrZlyCILUsWbMf",
    "summary": "<p>作者 | 蚂蚁链 LETUS 技术负责人 田世坤</p><p></p><h2>写在前面</h2><p></p><p></p><p>文字产生以前，结绳记事是人类用来存储知识和信息的主要方式。此后，从竹简、纸张的发明，到工业时代的磁盘存储，再到信息时代的数据库，存储方式不断革新，“存力”不断提高。</p><p></p><p>11 月 3 日，在 2022 云栖大会上，蚂蚁链历经 4 年技术攻关与测试验证的区块链存储引擎 LETUS（Log-structured Efficient Trusted Universal Storage）正式发布。</p><p></p><p>这一款面向区块链可信数据存储的技术产品，不仅用来解决当前蚂蚁链及区块链产业的规模化发展问题，也面向 Web3 时代提供“可信存力”支撑。</p><p></p><p>我们认为，随着大量的数据和数字资产在数字化世界里流转，可信数据的“存力”将如同电力网络的承载力一样重要。</p><p></p><p>本文希望通过对 LETUS 的深入技术解读，回答读者们普遍关心的关键问题：LETUS 是什么？主要解决哪些问题？为什么坚持用“可验证结构”？为什么要自研？以及未来要走向何处？</p><p></p><h3>背景是什么？</h3><p></p><p></p><p>从 2009 年序号为 0 的创世块诞生至今已过去十多年，“中本聪”依然神秘，但区块链技术的发展却因为公链、token、开源的推动，没有丝毫神秘感。</p><p></p><p>经过几代技术演进，在比特币的 UTXO 模型基础上诞生了应用更为广泛、支持可编程智能合约的区块链技术：通过密码学、共识算法、虚拟机、可信存储等技术，多个参与方执行相同的“指令”，来完成同一个业务逻辑，如账户转账，或者合约调用，维护不可篡改和不可伪造的业务数据。</p><p></p><p>简单讲，可将这类账本数据库，看作一个去中心化防作恶、防篡改的复制状态机，所执行的是智能合约描述的业务逻辑，而状态机通过日志 (区块数据）产生新的状态（状态数据）：</p><p></p><p>区块数据：包括交易、回执、世界状态 Root Hash 等信息，和数据库系统中的日志类似，但是块之间由 Hash 锚定防篡改，并且不会删除。（区块数据记录的是区块链上发生的每一笔交易，如：Alice 向 Bob 转账 xx）</p><p></p><p>状态数据：记录账户、资产、业务合约数据等状态信息，和数据库系统中表数据类似，需要实现可验证可追溯。（状态数据记录的是区块链上每个账户或智能合约的当前状态，如：Bob 账户剩余 xx）</p><p></p><p>链上数据的特点可以总结为以下三个：</p><p></p><p>持续增长：从创世块开始，账本数据随交易持续增长，保留周期长；</p><p></p><p>多版本：交易修改状态数据产生新版本，系统提供历史版本查询和验证功能；</p><p></p><p>可验证：交易和账户状态通过 Merkle 根哈希（Merkle Root Hash）锚定在区块头，通过 SPV（simple payment verification，简单支付证明）提供存在性证明；</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ac/aca3339ae8b6b7e66ce54267b795e30e\" /></p><p></p><p>区块链应用通过可验证数据结构（Authenticated Data Structure，如 Merkle tree）实现可验证和可追溯。我们认为，Web3“存力”一个非常重要的要素是可验证，而今天我们看到的区块链存储瓶颈大多来源于可验证结构 ADS（如 Merkle tree）的低效存取和查询，这正是蚂蚁链 LETUS 重点攻克的难题。</p><p></p><h2>我们要什么？</h2><p></p><p></p><p>随着时间推移和链上交易的增加，对存储容量的要求也不断增长，随之而来的是区块数据存储成本的大幅提升；与此同时，链上状态数据规模也持续增加，可验证数据结构持续膨胀，导致交易性能随账户规模提升和历史状态数据增加而持续下降。</p><p></p><p>2019 年，蚂蚁链上线了一个供应链金融业务，大家特别兴奋。但是，这种兴奋并没有维持多久，随着程序跑的时间越来越长，问题慢慢暴露出来。</p><p></p><p>供应链金融是面向 ToB 的，不像 ToC 端随时都有数据，可能会在某个时刻（比如每天晚上）有一笔状态数据非常大的交易进来，跑了一个星期后发现性能越来越慢。</p><p></p><p>链平台 TPS 的衰减和存储直接相关，而与共识、虚拟机都无关，随着业务合约持续写入数据，存储性能大幅衰减。</p><p></p><p>如果要在技术上长时间支持亿级账户规模、每天能稳定支撑亿级交易量，存储的规模和性能问题必须要攻克。</p><p></p><p>期间，团队也曾试过各种技术方法对他进行优化，得到一些缓解。但多次尝试之后发现，随着数量增加而出现的性能衰减，是一个绕不开的瓶颈，需要从本质上解决。</p><p></p><p>我们需要从问题表象分析背后的原因。</p><p></p><p>区块链应用通过可验证数据结构实现可验证和可追溯，但是可验证数据结构会带来读写放大（问题 1）和数据局部性（问题 2）。</p><p></p><p>而存储系统为了实现数据管理，需要对数据分页 / 分层、排序，如 KV 数据库基于 LSM-tree 将数据分层有序存储，而 MySQL 之类的数据库将数据分页，也会基于 B-tree 数据结构来排序索引。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ed/ed3ba1385d8e84b7e38fb857308689d4\" /></p><p></p><p>业界现有的实现方式，大多采用基于 LSM 架构的通用 Key-Value 数据库，在数据库之上运行一个独立 Merkle 树来实现可验证，如：</p><p></p><p>以太坊：MPT(Merkle Patricia &nbsp;Tree)+LevelDB</p><p></p><p>Diem：JMT(Jellyfish Merkle Tree)+RocksDB</p><p></p><p>背后的核心矛盾为：</p><p></p><p>Merkle 树每次状态数据修改，即使只改一个 KV，也需要从叶子节点到根节点，每一层节点都重新编码后，写到 KV 数据库，例如上图中 Alice 给 Bob 转账，需要写入 Merkle 树的 2 个叶子节点和 3 个中间节点，最坏情况需要写入数十个中间节点；</p><p></p><p>Merkle 树的节点的 key 完全随机 (如对内容算 hash，再以 hash 为 key)，数据局部性（data locality）非常不友好，如 RocksDB 里为了让 Level 内 sst 文件有序，即使没有垃圾依然需要层层进行数据压实（compaction），从而消耗了大部分的磁盘读写带宽；</p><p></p><p>数据规模越大，Merkle 树本身的层数越多，需要额外写入的 key-value 越多，DB 里的数据量越多，后台数据管理的代价越大（如 compaction 流量），消耗大量的磁盘和 CPU 资源。</p><p></p><p>除此之外，吞吐、延时等存储性能（问题 3）、持续增长下的存储成本（问题 4）、单机存储下的规模瓶颈（问题 5）也都是需要解决的问题。</p><p></p><h2>面临什么挑战？</h2><p></p><p></p><p>在过去几年的快速发展中，区块链的业务场景对交易吞吐量和响应时间要求越来越高，很多技术也被推动迭代发展，如 PBFT、HoneyBadger、MyTumbler 等高性能共识算法，BTN 等网络基础设施，JIT 加持的 WASM 虚拟机、以及高效的并行执行技术。</p><p></p><p>但比较而言，存储的性能对区块链平台整体性能影响非常大。对面向 2C 场景的数字藏品类业务（如鲸探，需支持秒杀），交易 TPS 与延时要求极为苛刻；而对需要在链上保存大量数据的存证类业务，大容量存储带来的成本又十分可观。</p><p></p><p>要支撑业务的长期可持续发展，我们归纳出区块链存储面临的核心挑战：</p><p></p><p>规模：业务账户规模可达数 10 亿，状态数据和历史版本规模分别需要支撑到十亿、千亿级；</p><p></p><p>性能：转账交易需求可达十万级 TPS、百毫秒级延时，要求性能不能受制于单机瓶颈，数据规模持续增长下性能不衰减；</p><p></p><p>成本：随着交易增长，存储容量持续增加，存储空间占用、节点间带宽占用居高不下。业务持续增长要求低成本存储。</p><p></p><p>这些问题在行业内很普遍。业界技术路线主要分三条：</p><p></p><p>路线 A：弱化可验证可追溯，如 HyperLedger Fabric 1.0 开始不支持可验证和多版本，保存读写集、只持久化最新版本状态数据；</p><p></p><p>路线 B：优化 KV 数据库存储，如实现键值分离、hash 索引的 KV 数据库等 (BadgerDB、ParityDB)，接入通用分布式数据库 (MySQL) 等；</p><p></p><p>路线 C：优化 Merkle 树，交易 ID 作为版本、树结构稀疏化，如 Diem JMT。</p><p></p><p>根据公开信息，目前区块链产品中主流的 MPT + LevelDB、JMT + RocksDB、MySQL 等存储架构，没有能全部解决上述 5 个问题的方案，难以在支持多版本和可验证的同时，满足 10 亿级账户规模下的高性能、易扩展、低成本的业务要求。</p><p></p><h2>我们做到了什么？</h2><p></p><p></p><p>我们自研了一套区块链存储引擎 LETUS(Log-structured Efficient Trusted Universal Storage)，保证完整的可验证、多版本能力，既满足区块数据不可篡改、可追溯、可验证等要求，也提供对合约数据友好访问、存储规模可分片扩展，高性能低成本等特性。同时也满足通用性，统一管理区块数据、状态数据。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/16/16f4996a2c66203f6eae5c2c61a74f22\" /></p><p></p><p>4 年前不敢想象的能力现在具备了（以下数据为统一环境下的测试结果）</p><p></p><p>大规模：通过存储集群扩展支持十亿账户规模，TPS 超过 12 万，交易平均时延低于 150ms；高性能：存储层 IO 吞吐相比以太坊 MPT + LevelDB 等架构提升 10~20 倍，IO 延迟降低 90% 以上。链平台在 7x24 高压力压测中，端到端 TPS 不随数据量增加而衰减；低成本：相比 MPT + LevelDB 架构，磁盘带宽减少 95%、空间占用减少 60%；相比于 Diem JMT + RocksDB 架构，磁盘带宽减少约 60%、空间占用降低约 40%；进一步降成本方案，供用户选用：</p><p>a. 针对区块数据容量与成本持续增长，提供智能控温分层存储能力，并应用于存证等业务降低约 70% 存储成本，同时也降低运维成本。</p><p>b. 针对状态数据的历史版本容量与成本持续增长，提供范围扫描的批量裁剪能力，实现历史版本状态数据的裁剪和后台空间回收，在十亿账户规模时，使用链原生存储可以减少近 90% 状态存储空间。</p><p></p><p>但这背后是一个技术架构的跨越，从下图左边的可验证数据结构 +KV 数据库架构，升级为现在的 LETUS 存储引擎，架构更简洁，系统更高效。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d6/d6f36bc6f97a77af1727cc47c80c8e99\" /></p><p></p><p>如 Alice 给 Bob 转账，只需要写增量数据，不需要写入 7 个 Merkle 树节点，数据局部性更友好，如 Alice 和 Bob 的账户数据，按区块号有序，不再 hash 随机。</p><p></p><h2>怎么做到的？</h2><p></p><p></p><p>图片回顾这四年，主要经历的三个大的阶段。</p><p></p><h3>阶段一：开源思路优化</h3><p></p><p></p><p>第一年里，为了满足业务急迫诉求，我们需要在有限时间内，实现亿级账户规模和交易 TPS。先从已有系统入手，深度优化了状态树，基于开源 MPT 到自研 FDMT，同时调优 RocksDB 数据库、增加并发、提升介质性能。</p><p></p><p>一系列优化措施缓解了问题，但依然无法根本解决，例如数据规模增加后，写放大依然有几十倍，数据在底层存储里依然随机分布。</p><p></p><h3>阶段二：自研存储引擎</h3><p></p><p></p><p>为了能彻底解决上述所有问题，我们不得不重新思考存储引擎的设计。</p><p></p><h4>核心设计</h4><p></p><p></p><p>针对读写放大（问题 1）、数据局部性（问题 2）和性能（问题 3），我们结合区块链特征，如可验证数据结构的读写行为、链上数据的多版本诉求、只追加和不可篡改等，重新设计存储引擎的架构分层、关键组件、索引数据结构：</p><p></p><p>根据区块链特征，我们根据可验证数据结构的读写行为、链上数据的多版本诉求，重新设计存储引擎的架构分层、关键组件、索引数据结构：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/09/09d0649ac73a74f4ff866fc3616eaf7e\" /></p><p></p><p>将可验证特性下推到存储引擎内部，由内置的 Version-based（区块号）多版本 Merkle 树提供可验证可追溯，并且直接操作文件，从而缩短 IO 路径；</p><p></p><p>将可验证特性下推到存储引擎内部，由内置的 Version-based（区块号）多版本 Merkle 树提供可验证可追溯，并且直接操作文件，从而缩短 IO 路径；</p><p></p><p>多版本 Merkle 树的 Node 聚合为 page，提升磁盘友好性，page 存储采用 Delta-encoding 思想避免 in-place 更新（结合 Bw-tree 思路），状态数据修改时主要保存增量，定期保存基线，从而减少写放大，也减少了空间占用；</p><p></p><p>为 page 存储实现 Version-based 的存储与检索，索引 page 都按区块号有序写入、在索引文件里有序总局，核心数据结构为 B 树变种，从而实现有序数据 locality；</p><p></p><p>利用区块链场景数据的追加写、Immutable 特点，架构上采用 Log-Structured 思想，通过日志文件来组织数据；</p><p></p><p>数据与索引分离，数据按区块号有序写入数据文件，通过异步 IO、协程并发等提升系统并发度，索引多模，区块 &amp; 状态通用，除 Merkle 树支持状态数据，实现有序 B 树支持区块数据；</p><p></p><p>当前最新版本 Merkle 树优先在内存里缓存或者全部缓存，链上合约执行时，如果存在则直接读取，不需要访问 page 来重放，从而加速合约执行。</p><p></p><p>基于些核心设计，实现了成本降低的同时性能提升，链平台交易 TPS、延时等性能指标不会随着数据规模的提升而衰减。</p><p></p><p></p><h4>降成本</h4><p></p><p></p><p>虽然存储资源占用大幅降低后，但是链上数据依然面临持续增长带来的高成本问题（问题 4）。</p><p></p><p>基于 LETUS 架构的后台数据治理框架，我们能很方便的扩展实现数据迁移 / 压缩 / 垃圾回收等治理策略，基于这些策略，为用户提供进一步降成本能力，并针对自己的业务特点来选择使用：</p><p></p><p>（1）智能控温分层存储：存储介质按照性能、成本分层，通过智能控温调度数据在不同介质的分布量，将冷数据后台自动迁移到廉价介质（如 NAS），降低存储整体成本，并实现容量扩展，不受单盘空间限制。</p><p></p><p>（2）范围扫描的批量裁剪：对于历史版本 Merkle 树和状态对象，基于版本有序性与内置 Merkle 树，让用户可以指定目标区块号范围裁剪，通过 Page 边界扫描，批量索引与数据裁剪、垃圾回收实现存储空间释放，进一步降低状态数据成本。</p><p></p><h4>规模扩展</h4><p></p><p></p><p>针对问题 5，LETUS 采用分布式存储架构，实现单个共识参与方计算和存储分离，计算层和存储层可分别部署独立集群，通过高性能网络通讯框架进行数据读写访问。</p><p></p><p>为了对海量状态数据进行灵活的数据分片，并且保证各个区块链的参与方 hash 计算的一致性，将数据切片为 256 个最小存储单元（msu），并将一个或者多个 msu 构成一个状态数据分片（partition），将所有数据分片调度到多个物理机器。从而实现规模弹性扩展，解决了单机存储的容量瓶颈和带宽瓶颈。</p><p></p><h3>阶段三：生产落地</h3><p></p><p></p><p>为了全面落地铺开的同时让业务平稳运行，能够开着飞机换引擎，在这几年的研发过程里，我们充分准备、循序渐进的分阶段落地：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/27/27c805677fee7ad148ef5b842fc7467d\" /></p><p></p><p>2021 年 5 月，基于 LETUS 存储引擎的区块数据冷热分层，在版权存证业务灰度上线，存储成本降低 71%，解决容量瓶颈并降低运维成本。</p><p></p><p>2021 年 8 月，基于 LETUS 存储引擎的状态数据，在数字藏品平台“鲸探”双写灰度上线，并成功支撑秒杀场景；</p><p></p><p>2022 年 2-6 月，LETUS 引擎的历史状态数据裁剪、存储服务架构升级等生产 ready，在数字藏品和版权存证等业务全面落地，并从灰度双写切为单写；LETUS 单写意味着对硬件资源要求大幅下降，我们将“鲸探”生产环境的云资源全面降配，降配后链平台性能水位提升 200%，同时存储成本下降 75%。</p><p></p><h2>总结与展望</h2><p></p><p></p><p>蚂蚁一直坚持“成熟一个开放一个”的技术战略。同样的，LETUS 不只为蚂蚁链定制，也同样给其他联盟链、公链提供高性能、低成本的支持。</p><p></p><p>蚂蚁链坚持技术自研，确保在共识协议、智能合约、网络传输、存储引擎、跨链技术、区块链隐私计算等领域处于全球领先水平。我们始终认为，坚持技术自主研发是建立长期可持续竞争力的关键。</p><p></p><p>在“可信存力”这条赛道上，我们也需要为进一步的技术壁垒提前布局，如合约结构化查询语言，为链上合约实现结构化 + 可验证的查询能力, 提升开发者体验；Fast-Sync 与节点多形态，提升组网效率和节点成本灵活性；以及 Web3 等潜在的技术生态。</p><p></p><p>技术创新永远在路上。接下来，继续沿着硬核技术方向突破，啃一些硬骨头，持续为整个价值互联网提供可靠的、可持续的存力。</p>",
    "publish_time": "2022-12-01 12:36:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenStack已死？最新报告显示OpenStack部署呈爆发式增长，整体规模超4000万",
    "url": "https://www.infoq.cn/article/xb600VAYhlzQcjuvAKeN",
    "summary": "<p></p><p>&nbsp;在业内人士质疑 <a href=\"https://www.infoq.cn/article/Z8p-Se31uh0NrLNHt5BT\">OpenStack</a>\"（世界第四大开源项目）是否已死之际，OpenInfra 基金会近日<a href=\"https://www.openstack.org/user-survey/2022-user-survey-report\">测得的数据</a>\"显示，生产中的 OpenStack 内核数量达到前所未有的 4000 万，同比 2021 年增长了 60%，自 2020 年以来增长了 166%。官方认为，该增长是对混合云环境和Kubernetes集成支持的依赖程度增加带来的。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b5/b5a134634d339663547e432f0719e828.png\" /></p><p></p><p>&nbsp;OpenStack表示，各种规模的组织都在进行扩展来满足最终用户的需求。如LINE是日本的一款即时通讯服务，在日本每月有1.76亿活跃用户。他们的云由OpenStack提供支持，其内核数量增加到了400万个，比2021年增加了150%。Workday是一家基于云计算的财务管理、人力资本管理和学生信息系统软件供应商，今年其OpenStack内核数量再次翻了一番，达到284万个。据悉，全球超过300个数据中心由OpenStack 驱动的公有云支持。</p><p>&nbsp;</p><p>另外，作为 OpenInfra 标准，Linux OpenStack Kubernetes Infrastructure (LOKI) 正在以越来越快的速度在生产中实现。Kubernetes 现在集成在超过 85% 的 OpenStack 部署中：73% 通过 vanilla Kubernetes，另外 12% 通过 Red Hat 的Kubernetes 发行版<a href=\"https://xie.infoq.cn/article/d6672373174b4a0ee3288127a\">OpenShift</a>\"。</p><p>&nbsp;</p><p>Red Hat 也是 OpenStack 的支持者。Red Hat 产品管理经理 Maria Bracho 表示：“在过去的几年里，越来越多的客户在不同的部署模型中同时使用 OpenStack 和 OpenShift。在 Red Hat，我们做了大量的工作来确保这些平台可以一起使用，用户不需要在一个平台或另一个平台之间做出选择，而是可以自由、坚定地选择最适合当前和未来工作负载的配置。”&nbsp;</p><p>&nbsp;</p><p>OpenStack 和 Kubernetes 生产集成的增长进一步体现在，使用 Magnum（用于容器编排的 OpenStack 服务）运行生产工作负载的用户增加到 21%（去年仅为 16%）。</p><p>&nbsp;</p><p>OpenStack 通常用于混合云。根据 Flexera 报告，80% 的受访者正在采取公有云和私有云同时使用的混合云策略。这一趋势也反映在了 OpenStack 的用户调查中：运行混合云环境和 OpenStack 部署的受访者从 77% 上升到 80%。</p><p>&nbsp;</p><p>Octavia的采用率增加，为越来越多部署了OpenStack的混合云环境提供支持。为了实现工作负载在不同云环境之间的平稳过渡，越来越多的运营商转向使用开源的负载均衡方案 Octavia， Octavia 可以与 OpenStack 配合使用。几乎一半的生产部署都在使用 Octavia，比去年增加了 11%。</p><p>&nbsp;</p><p>OpenInfra 基金会总经理 Thierry Carrez&nbsp;表示，“随着OpenStack部署以惊人的数量持续增长，OpenStack社区正在证明它不仅活得很好，还为组织提供了无可争辩的价值。”</p>",
    "publish_time": "2022-12-01 12:42:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云基础设施性能再提升！re:Invent Day1 重大发布一览",
    "url": "https://www.infoq.cn/article/CLHBFwbSNzSYG4PhYeZH",
    "summary": "<p>re:Invent Day1 ，亚马逊云科技高级副总裁 Peter DeSantis 带来以云性能为核心的多项重要发布！一场开发者们的聚会还在继续～</p>",
    "publish_time": "2022-12-01 12:42:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "1年巨亏100亿美元，亚马逊Alexa成烧钱“无底洞”，语音助手为什么不赚钱？",
    "url": "https://www.infoq.cn/article/QNcQ16QQRGK7tQhOqeU9",
    "summary": "<p></p><blockquote>从亚马逊Alexa聊起，语音助手为什么就不赚钱？留给大型科技公司语音助手的时间是不是不多了？</blockquote><p></p><p></p><p>近日，有报道称亚马逊的<a href=\"https://www.infoq.cn/article/gsEel0fax469NrUARbjp\">Alexa语音助手</a>\"和智能音箱业务总亏损已达100亿美元。值此消息发布之际，Alexa的几位竞争对手也纷纷表示日子难过，需要找到靠谱的办法让自家产品开始盈利。</p><p></p><p>从Alexa和其他几款语音助手的现状来看，用技术让人眼前一亮简单，但把这种技术转化为能赚钱的业务却是无限艰难。如今科技企业正忙于裁员、为即将到来的经济大衰退做准备，这些看着酷炫但却无助于进项的产品必须尽快找到价值定位，否则必然被无情抛弃，以省下资源来抵御寒冬。</p><p></p><h2>处于危机中的 Alexa</h2><p></p><p></p><p>当前，亚马逊正在经历着公司历史上最大规模的裁员，计划削减约 1 万个工作岗位。受冲击最严重的部门之一是亚马逊的 Alexa 语音助手。显然，该部门在这家电子商务巨头已经失宠。</p><p></p><p>Business Insider 在一篇报道中详细描述了“语音助手和亚马逊大型硬件部门的迅速衰落”。</p><p></p><p><a href=\"https://www.infoq.cn/article/2017/12/Alexa-amazon-bussiness\">Alexa </a>\"是一个开创性的语音助手，诞生已有近 10 年，谷歌和苹果都在很大程度上模仿了它。然而，Alexa 从未设法创造出可持续的收入流，所以 Alexa 并没有真正赚到钱。</p><p></p><p>Alexa 部门与亚马逊 Prime 视频同属于“全球数字”集团。按照 Business Insider 的说法，该部门仅在 2022 年第一季度就亏损了 30 亿美元，其中“绝大多数”亏损都是因为 Alexa。显然，这是其他部门亏损总和的两倍。该报道称，硬件团队今年预计将亏损 100 亿美元。听起来，亚马逊已经厌倦了烧钱。</p><p></p><p>Business Insider 在报道前采访了该公司硬件团队的 12 名现任和前任员工，他们将 Alexa 描述为“一个处于危机中的部门”。几乎所有利用 Alexa 变现的计划都失败了，其中一名前员工称，Alexa 是“想象力的巨大失败”和“一个浪费掉的机会”。11月的裁员说明他们多年来试图扭转局面的努力并未取得实质性的成果。</p><p></p><p>据报道，在亚马逊，Alexa 是前首席执行官杰夫·贝佐斯的“宠物项目”，当时它获得了巨大的发展空间。2019 年，他们召开了一次全员危机会议，试图扭转亏损问题，但无果而终。到 2019 年底，Alexa 的招聘工作冻结，从 2020 年前后开始，贝佐斯对该项目失去了兴趣。当然，亚马逊现在有了一位全新的首席执行官安迪·贾西，显然，他对保护 Alexa 不感兴趣。</p><p></p><p>报道称，尽管 Alexa Echo 系列是“亚马逊上最畅销的产品之一，但大多数设备都是按成本价出售的。”对于其商业模式，一份内部文件是这样描述的：“我们希望在人们使用我们的设备时赚钱，而不是在他们购买我们的设备时赚钱。”</p><p></p><p>不过，这个计划从未真正实现过。</p><p></p><p>他们希望在用户使用 Alexa 的间隙插播广告，通过这种方式引导人们通过语音在亚马逊上购物。但是，没有多少人愿意信任一个 AI 产品，不看图片或阅读评论就花钱或购买商品。报道称，到 Alexa 实验的第四年，“Alexa 每周收到 10 亿次交互，但其中大多数对话都是播放音乐或询问天气之类的简单命令。”这些简单对话是无法盈利的。</p><p></p><p>亚马逊还试图围绕 Alexa 技术与一些公司开展合作，让用户通过一个语音指令就可以购买达美乐的披萨或叫辆优步，而在这个过程中，亚马逊可以获取佣金。报道称：“由于使用不多，到 2020 年，该团队已停止发布销售目标。”该团队还试图将 Alexa 描绘成一个光环产品，吸引那些更有可能在亚马逊购物的用户，即使他们不通过语音购物，但对这一理论的研究发现，这些用户的经济贡献往往达不到预期。</p><p></p><p>在给员工的一份公开声明中，贾西表示，公司对 Alexa 的发展仍然“有信心”，但这是在大幅削减 Alexa 团队之后。一名员工告诉 Business Insider，目前，“对于这款硬件设备的未来，尚没有明确的指示”，而且由于硬件不赚钱，也没有明确的动机来不断迭代这个深受大众喜爱的产品。这种方向性的缺失给了在亚马逊内部颇受争议的 Astro 机器人 机会。实际上，这个价值 1000 美元的机器人就是一个带轮子的 Alexa。根据 Business Insider 的追踪调查，Alexa 目前在美国语音助手大战中排名第三，谷歌的 Assistant 有 8150 万用户，苹果的 Siri 有 7760 万用户，Alexa 有 7160 万用户。</p><p></p><p>下面，我们来看看已有8年历史的亚马逊Alexa到底做对了什么，又有哪些遗憾。</p><p></p><h2>技术不错，但还不够好</h2><p></p><p></p><p>一系列技术创新，使得亚马逊Alexa这样的语音助手能够实现10到15年前根本无法想象的理解能力。</p><p></p><p>自动语音识别持续发展，助手可以在各种背景噪音下准确采集指令、解析用户口音。由深度神经网络（包括Trasnformers、RNN和LSTM等）驱动的自动语言处理系统，则让语音助手能够将语音内的不同细微差别映射至对应的指令，允许用户用灵活多变的方式下达类似的要求。另外，各种应用平台、API等已经能让语音助手遍历网络上的大量信息，再把指令跟应用程序功能对接起来。</p><p></p><p>这一切听起来都很美，但如今的语音助手仍有着明确的功能边界。在大多数情况下，亚马逊Alexa只能完成简单的任务，例如设置闹钟、播放音乐、播报天气和在网上搜索简单信息等。这些功能要么指向性很强，根本就没多少犯错的空间，要么就是敏感性很低，哪怕做错了也没多大影响。</p><p></p><p>但如果我们想要执行某些敏感、需要多次交互或者具有多模实质的任务，语音助手的可靠性就会急剧下降。</p><p></p><p>例如，当我们想要网购（这也是亚马逊最初为Alexa规划的重要用例之一）时，就属于典型的敏感任务，因为其中涉及金钱，用户不希望出错。另外，网购操作比较复杂，其中往往涵盖多个步骤，用户希望看到自己买了什么，参考其他建议和类似的选项。这一切在纯语音界面上显然很难实现。出于类似的理由，日程安排和会议规划等预期用途在语音助手上也基本没能铺开。</p><p></p><h2>语音助手无法创造可盈利的商业模式</h2><p></p><p></p><p>好了，现在我们有了一款很酷的语音助手，能够非常准确地执行某些特定任务，而在其他场景下虽然也做尝试，但表现平平。在这样的前提下，我们要怎么靠它赚钱？</p><p></p><p>具体选项无非以下几种。</p><p></p><p>首先就是销售硬件，例如亚马逊Echo、苹果HomePod或者谷歌Nest之类的智能音箱。</p><p></p><p>在这种情况下，语音助手技术的商业价值就直接取决于设备价格、所能售出的设备数量以及客户更换这类设备的频率。换言之，这种商业模式跟智能手机很像，人们每隔几年就会掏几大千去买下一代iPhone或者Pixel手机。</p><p></p><p>但那可是手机，智能音箱缺少催人升级换代的动力。第一，人们不想为这些音箱支付很高的溢价，毕竟本身使用频率就不高；第二，智能音箱的升级空间也不大，一个麦克风、一个喇叭、再加个显示屏，也就差不多了；最后，用于支持语音助手的云服务也有升级和维护成本。所以综合来讲，用户对智能音箱的使用会给厂商带来持续存在的成本，并在日积月累后逐步超过当初销售音箱所创造的利润。</p><p></p><p>第二种思路就是出售服务。在这种情况下，用户需要按月或按年付费才能使用手机或智能音箱上的语音助手。但让人单独付费真的很难，产品必须具有明确的价值才能说服用户为此买单。为了让这种商业模式取得成功，产品必须能够解决某些业界尚未解决的难题，或者创造足够直观的附加价值，进而与产品/市场相契合。遗憾的是，亚马逊Alexa和其他语音助手都达不到这样的高度，单独收费更是痴心妄想。</p><p></p><p>最后，有些朋友可能还指望着将亚马逊Alexa当成吸引用户购买其他产品的渠道，比如前面提到的网购操作界面。但语音助手天然就不适合这类操作，所以Alexa没法提供理想的购物体验。用户显然更愿意在手机或电脑上操作购物软件，那样更直观也更顺畅。</p><p></p><p>总而言之，从科学和工程的角度看，亚马逊Alexa确实令人印象深刻&nbsp;。但从产品和业务的角度看，它也确实不具备变现的条件。</p><p></p><h2>下一代语音助手会是什么样子？</h2><p></p><p></p><p>第一代语音助手的思路还是不错的，即将声音作为计算机操作界面，但却无法创造可盈利的商业模式。类似的情况在1990年代的VR头显（价格太高、体验太差）和2010年代的AR眼镜（同样是价格太高，在功能上支撑不起这样的成本）也曾出现过。</p><p></p><p>如今Alexa和Siri之所以还存在，就是因为其背后是两家财力极为雄厚的企业。双方可以不断砸钱进去，在承受亏损的同时慢慢摸索商业模式和技术可能性。</p><p></p><p>那么，下一代语音助手又将朝哪个方向前进？也许有以下几种可能。</p><p></p><p>首先，等待<a href=\"https://www.infoq.cn/article/6J7vxg0J25cMzWRoMQkU\">AI技术</a>\"的发展让语音助手愈发强大，从而为更多应用场景提供支持（例如主动式语音助手，由它在必要时主动询问意见，而非靠指令被动激活）。</p><p></p><p>另一种方案就是将现有通用型语音助手转化为面向特定场景的专业助手。如此一来，我们就能把语音助手集成到应用场景的上下文和工作流中，让它们有能力处理步骤更多、复杂度更高的任务。这种形式有望建立起B2B商业模式，特别是在那些需要大量手动操作的行业（例如制造业、饭店和酒店等）中，可以尝试用语音助手降本增效。正如第二代谷歌眼镜在手工领域获得了不错的市场认可，这类业务场景的附加价值也将远超普通消费级市场。</p><p></p><p>目前还很难确定未来的智能音箱到底还需不需要显示屏。毕竟我们的大部分日常事务都涉及视觉元素，而纯语音助手在应用上还有很多局限。技术发展应该会给出答案。</p><p></p><p>总之，目前亚马逊还没有放弃Alexa的打算。但无论如何，现有语音助手的能力极限已经明确，是时候朝着下一代技术方案进军了。</p><p></p><p>原文链接：</p><p><a href=\"https://arstechnica.com/gadgets/2022/11/amazon-alexa-is-a-colossal-failure-on-pace-to-lose-10-billion-this-year/\">https://arstechnica.com/gadgets/2022/11/amazon-alexa-is-a-colossal-failure-on-pace-to-lose-10-billion-this-year/</a>\"</p><p><a href=\"https://bdtechtalks.com/2022/11/28/amazon-alexa-revenue/\">https://bdtechtalks.com/2022/11/28/amazon-alexa-revenue/</a>\"</p>",
    "publish_time": "2022-12-01 13:40:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "亚马逊云科技 CEO 倾情分享，系列重磅产品发布！",
    "url": "https://www.infoq.cn/article/Db2Bb03cBij3IAzjT81g",
    "summary": "<p>亚马逊云科技 CEO 现场阐述云战略，系列重磅产品发布！</p>",
    "publish_time": "2022-12-01 15:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全网都是数字人，鹅厂的数智人有何不同？",
    "url": "https://www.infoq.cn/article/zZ5AV78D35mL34eaG44O",
    "summary": "<p></p><p></p><blockquote>一个智慧与颜值并存的数智人是如何“养成”的？</blockquote><p></p><p></p><p>在元宇宙风潮之下，数字人先火了。近两年，国内数字人项目呈现井喷态势。IDC预计，到2026年中国AI数字人市场规模将达到102.4亿元。作为时下最热的技术话题，我们判断，开发者有必要对数字人技术有完整的认知和理解。</p><p></p><p>在此背景下，InfoQ 特别策划了《数字人基础技术解析》专题。本专题将首先对数字人做概要介绍，紧接着围绕数字人的技术、应用落地等维度分别做解读。我们将收集来自国内业界一流团队的最佳实践，供读者参考。</p><p></p><p>本文是本专题的技术&amp;实践篇。近日，腾讯云智能数智人产品总经理陈磊接受了InfoQ专访，详细介绍了腾讯云智能在数智人上的技术探索和应用落地实践。</p><p></p><h2>“数智人”和“数字人”</h2><p></p><p></p><h3>市场升温</h3><p></p><p>近两年，尤其是在今年，数字人火出了天际。陈磊认为，这种火热主要基于两方面因素助推。</p><p></p><p>一方面，得益于多项技术的发展，云计算、5G、音视频技术、人工智能技术、渲染技术等都在不断地发展和突破，让虚拟人在制作和应用环节大大提效、简化；在虚拟人的落地和行业扩展上，不再只是从前传统的“中之人”方式，只面向影视、娱乐行业输出，而是在各种各样的行业加速落地。</p><p></p><p>另一方面，从行业的维度看，全球科技巨头纷纷对元宇宙加大投入，例如Facebook改名Meta，宣布All In 元宇宙；英伟达推出元宇宙基础模拟和协作平台Omniverse；EPIC的MetaHuman等加速元宇宙基建进程... 这些都是强烈的市场信号。而被广泛认为是人机交互新入口的数字人乘着这股东风率先起势，从技术发展、市场应用双轨运行的发展情况来看，虚拟数字人行业进入了快速发展的轨道。</p><p></p><h3>腾讯布局“数智人”</h3><p></p><p>嗅到这样的产业趋势后，2021年，腾讯云智能围绕对话式AI，升级研发推进“数智人”业务，陈磊主要负责该业务团队的产品和研发。</p><p></p><p>在去年的腾讯数字生态大会上，腾讯首次公布了云智能的战略架构，整体面向管理者、生产者、开发者、用户四大人群，提供决策、协作、创新和服务四大核心能力。陈磊表示，数智人是腾讯云智能的一个重要的组成部分。数智人以服务于人作为价值理念，实现智慧与颜值兼备，为企业创造价值，为用户提供有温度的服务。</p><p></p><p>与市面上流行的“数字虚拟人”，“AI数字人”、“数字人”等概念不同，腾讯云智能对数字人的命名倾向于“数智人”。</p><p></p><p>“腾讯的理解是，通常说的数字虚拟人，AI数字人等概念，更侧重好看的皮囊，但数智人本身面向企业服务时需要两个维度，既要有好看的皮囊，还要有有趣的灵魂。腾讯把数智人定位在“交互智能入口级”相关的应用，技术上包括感知、驱动、决策等链路，应用上链接了整个腾讯丰富的内容与服务生态。依赖这种生态，我们打磨了平台能力。在具体场景应用时我们也可以结合行业里的知识图谱，从智能维度做升维，让客户打造出具有自主IP、智能决策和生动交互的AI数字人，实现数字人到数智人的进化”陈磊解释。</p><p></p><p>在12月1日举办的腾讯数字生态大会上，《数字人产业发展趋势报告2023》发布并指出，AI驱动的数字人。通过 AI 建立人与大数据的连接，提高效率并满足人情感交流需求 ，提升用户体验，将成为人机交互新入口。</p><p></p><p>“目前使用文字或语音交互的场景都可用 AI 数字人软着陆的方式替代，不需要改变原有的业务逻辑和商业模式 ，大众接受成本较低。与此同时，数字人作为企业的数字资产 ，是对员工工作的增强，具有生产力的属性，可以进一步释放生产力，同时降本增效。未来数字人将根据不同行业的业务特点和应用场景进行更深度结合， 孵化千行千面的数字员工 ，提供智能化服务。”</p><p></p><h2>如何打造“智慧”与“颜值”兼备的数智人？</h2><p></p><p></p><h3>多项技术作支撑，AI技术是核心</h3><p></p><p>去年11月，腾讯云智能发布了数智人产品矩阵，包括3D写实、3D半写实、3D卡通、2D真人、2D卡通五种风格的数智人产品。</p><p></p><p>这些多样风格的产品主要基于形象的写实度和应用场景维度划分，可以定制化不同的角色，满足各类场景服务需求，可承担资讯播报、文旅导览、座席客服、多语种主播、手语主播等角色。</p><p></p><p>这些全能的功能背后，由一系列技术在支撑。3D写实数字人提供交互服务时，需要很强的渲染技术，包括端渲染技术、云端渲染技术等。形象表现力层面，需要很强的算力和渲染支撑。在应用维度，音视频、5G等更快的网络技术可以帮助数智人加速应用落地。</p><p></p><p>陈磊介绍，数智人是AI综合类的应用，从形象生成到交互、决策、驱动、服务等环节都会应用到AI技术。</p><p></p><p>腾讯云智能数智人集合了腾讯公司内部多个团队的优势AI能力，例如在数字人生产环节，在人像建模方面，通过优图实验室的相关AI技术，可以做到凭借几张照片就快速生成写实级的人脸；在动作绑定环节，例如将动作从A数智人迁移到B数智人，可以利用IEG的NExT Studios团队的动作和表情迁移工具，快速绑定，相比于传统的需要用动捕设备从零开始采集动作大大提升了生产效能；对话式AI，由云小微语音AI技术支撑；多情感驱动能力，由AI Lab的相关支持等等。此外，如果面向多维终端，终端渲染性能不够的时候，云渲染或云游戏相关的技术可以实现，即使在云端渲染，但能实现实时交互性无感知，延迟性很低....</p><p></p><p>虚拟数字人的发展需要一个全域的技术支撑。腾讯的优势在于，每一个单点的技术维度，都有团队在多种应用场景中，做持续的深耕跟突破，因此在数智人构建的每一个维度都已经建立起很强的技术壁垒，这也是腾讯做数智人的优势所在。与此同时，这些技术都在腾讯云的技术架构下，通过云智能数智人整体对外输出，助力行业升级。</p><p></p><h3>多模态交互</h3><p></p><p>多模态交互是数智人背后的核心技术支撑，各家都在强调这项技术。</p><p></p><p>陈磊介绍，与业内同类产品相比，腾讯云智能数智人在该项技术上拥有多项差异化优势。</p><p></p><p>具体而言，腾讯云智能数智人融合了ASR、TTS、NLP、计算机视觉、知识图谱等全栈式的AI底层能力。相对来说，在中小企业里面，具备全栈式能力的还是少数。而且，如何更好地将这些全栈型能力整合在一起更富有挑战。将这些能力融合在一起，才能让数智人拥有强大的表现力、识别力和感知理解能力。</p><p></p><p>此外，腾讯云智能数智人目前支持34个语种，包括各种方言，翻译超过11个语种。在多个垂直行业，有46万个垂直行业的不同场景的热词库，在多业务场景里能让数智人做到“听得清、听得懂、会表达”。</p><p></p><p>在实时交互上，与行业相比，腾讯云智能数智人图像生成首帧延时小于600ms, 行业同类产品图像生成首帧延时大于1s、1.5s。</p><p></p><p>陈磊介绍，低延迟这这一效果的实现，依赖全链路的优化。一般分三步，第一步是图像生成：生成每帧视频内容；第二步是链路传输：把视频通过音视频技术传输；第三步，设备终端播放。其中在第一步，团队采用了模型裁剪、蒸馏技术，加速推理方式，快速提升了生成数智人的图像生成能力；第二步中应用到的音视频传输链路技术素来是腾讯的强项，公司音视频实验室为此提供了很多技术能力，将所有核心环节的能力打通串联后，才实现了600毫秒以内的延迟效果；第三步，在多设备的终端播放中，腾讯也有配套的编解码播放器。</p><p></p><h3>音视频技术</h3><p></p><p>当涉及到一些写实数智人时，对算力的要求会比较高。如果普通大众使用，很可能在手机上根本跑不起来。</p><p></p><p>但手机端又是数智人的一个很重要的应用端口，有一些客户希望自己的数智人与用户进行互动或服务。这个过程会涉及到云端渲染的能力以及音视频传输的能力。用户手机中的网络环境、带宽场景等各不相同，如何保证实时流畅的、低延迟的交互，就需要应用到音视频技术。</p><p></p><h3>小模型训练方案：以小胜大</h3><p></p><p>与业内一些数字人产品多采用大模型方案不同。在训练方案上，腾讯云智能数智人更偏爱小模型，采用了 5亿级别的小参数量的多语言预训练模型“神农MShenNonG”。</p><p></p><p>陈磊介绍，腾讯云智能在一些场景下也构建了百亿或千亿级的超大模型，但在实际应用中发现，考虑到在应用过程中部署的便利性和部署成本因素，仍需要对模型参数量进行控制。以千亿级别的超大模型为例，训练时间长，从训练到落地需要很长的周期，而且如果当客户偶然有小范围的数据变化时，需要不断滚动模型。总而言之，周期、成本等对应用上线带来了较大挑战。</p><p></p><p>针对亿级别这种参数小规模的多语言训练模型，也需要做数据量控制。对此，腾讯云智能数智人团队综合运用了混合编码的数据增强，基于多尺度的多语言信息融合，将语种和语义做对比等策略性尝试。</p><p></p><p>模型虽小但智能化程度一点不差。数智人构建采用小模型的训练方案具有诸多优点。在数据层面，腾讯云智能研究团队进行了混合编码数据的构造方式，利用双语对齐的词典和句子检索工具，能构造出大量多语言混合的训练数据。此外，在模型层面，团队还做了一些特殊工作，如可插拔，基于多尺度的多语言信息的融合技术，对低频词汇建模等。在低资源语种方面，小数据、小模型能解决很多问题。在训练层面，相比于传统的市场上一般需要一个月以上的模型迭代周期，神农MShenNonG只需十天左右就能快速达成模型的迭代。</p><p></p><p>陈磊认为，未来，在数智人的模型训练方案上，将逐渐呈现融合趋势。如果企业具有足够大量的数据或足够大的场景，大模型方案是优选。但现实是，在实际应用场景下，很多时候没有那么多数据，也缺乏算力资源，在迭代周期上也不允许时间战线拉得很长。因此，未来会是大模型与小模型融合的趋势。</p><p></p><h2>数智人走进现实</h2><p></p><p></p><h3>数智人形象生产平台</h3><p></p><p>过去一年，云智能数智人团队发现，数智人在行业应用时，落地场景和行业非常广泛，实现规模化的批量生产显得越来越重要。在行业场景越来越多的情况下，如何持续、快速地实现行业落地，比较大的挑战是怎么样能快速地生产出数智人，且用低成本高效率的服务把它送到客户的场景里去。</p><p></p><p>因此，在数智人生产维度上，团队重点打造了数智人形象生产平台，通过自动化的生产管线，提升面向行业的数智人的供给效能，降低生产周期。</p><p></p><p>关于数智人形象生产平台的具体运作流程。以面向传媒落地的2D交互数智人为例，传统的数智人的生产方式是，找主播在录影棚录制形象视频，且线下会对视频数据做很多手工处理，整个构建流程需要花1-2个月时间。有了管线后，可以通过AI技术来处理视频数据，如自动做AI人像分割，人与背景分离，自动拆帧，以提升交互表现力，在拆解后还可以对形象做美颜，进行各种数据增强，如美白、去痣、眼神对齐等影视级交互技术。</p><p></p><p>一般而言，客户定制一个3D写实数智人的流程包括人物设定，原画设计，建模，绑定，服装、发饰、渲染，再加上驱动和多模态交互等环节。</p><p></p><p>AI技术可以对上面各个环节实现降本增效。如在建模环节，比传统CG建模时间节省不少， 以前需要月级别时间，现在通过人像生成技术，周级别就能搞定。在建模成本上也有了很大降低，写实类的数字人，普通公司建模成本仍很高，腾讯云智能数智人的建模成本已显著降低，例如在建模维度上已基本不需要过多成本，只需要配一些头发加衣服，成本可以缩减一半以上。</p><p></p><p>但值得注意的是，目前在数智人建模技术上仍存在挑战。对于超写实数字人这个类别，通用建模的能力和标准还无法做到特别完美，当前技术上还达不到超写实的自动生成技术，主要还是采用传统方式，如在头发制作环节，多数依赖传统CG公司手工制作。现在AI在头发生成技术上也在做探索，但目前还没达到可落地应用的阶段。比如，基于同一个衣服作为基底的模版，在上面换纹理，可以降低成本，但如果是一件全新的衣服，且是柔性材质，也还依赖外部能力供给。</p><p></p><p>但对于2D真人数字人，通过这套生产管线，即便是一个不懂AI技术的员工也可以自主跑通这套流程，大大降低了制作门槛。构建时间压缩到了天级别，且管线在并发维度不存在卡点，比之前大大提速。</p><p></p><h3>应用在金融、传媒等多个领域</h3><p></p><p>据介绍，在应用环节，腾讯云智能数智人团队最初重点关注金融、传媒等典型场景的典型应用，之后再做单点切入。今年，团队一方面在行业服务或企业服务维度上做更多垂直或场景的提升。另一方面关注交互智能入口维度。数智人定位在交互智能入口级的应用。如常常能看到数智人在银行迎宾、理财知识讲解等场景下应用，其背后的逻辑主要是让服务的“体感温度”得到较大提升。</p><p></p><p>目前腾讯云智能数智人已在金融、政务、传媒、文旅、交通等多行业广泛落地。</p><p></p><p>如在应用服务上，在中信建投应用里，腾讯云智能数智人是在证券行业首个落地的交互型数智人；在手语维度上，3D手语数智人聆语担任冰雪赛事手语解说翻译官；在文旅领域，打造了国博的虚拟形象代言人艾雯雯；在汽车领域，将交互型的数智人做了体感升级；在与一汽大众的合作中，将虚拟人与虚拟空间结合做更新形式的体验式服务的改进，大幅提升用户体验。</p><p></p><p>陈磊介绍，数智人的落地行业适配，一般分为两个阶段，第一是基于既有行业的数据积累，构建垂直领域的预训练模型；第二，针对一些特殊项目，数智人具备可以提供第三方知识的能力，或基于客户的小样本数据再训练的能力，将这两点结合起来就能把整体的数智人企业服务做得更好。</p><p></p><p>比如，面向行业做深度的企业服务，融入对话式AI，从前台疑问解答、产品推荐到售后客户服务，再结合智能对话能力做会话洞察，最终可以横跨整个生命周期。这要求数智人服务同时具备行业深度和场景深度，结合对话式AI模型的训练能力做行业增值与提效。</p><p></p><p>例如在金融场景，腾讯已在某金融机构落地了3000+以上的数智人客户服务，辅助传统人工客服工作，由数智人+对话式AI帮助解答用户问题。</p><p></p><p>腾讯云智能数智人在不同的行业实际应用时，对领域知识的要求也较高，在不同的行业构建行业知识图谱。通过神农MShenNonG对话模型快速迁移、进驻到一个行业，在进驻行业后又帮客户快速打造出面向自己行业的模型。</p><p></p><h3>应用难点</h3><p></p><p>但整体上来看，数智人在行业落地还存在不少难点。如行业知识沉淀储备不够，或者内容不够规范完备。对此，腾讯云智能数智人团队在构建知识生产的工具，提升知识生产的效率与效应。此外，在行业拓展时，腾讯云智能数智人团队走的一个方向是通过NLP的能力，快速训练模型的能力，让数智人掌握住更好的领域技能。</p><p></p><p>另一个难点是算力。虚拟人对算力有较强的要求，随着算力提升，数智人整体的表现力将直线上升。表现力包括形象渲染的逼真度，表情的逼真度，驱动技术、感知技术、决策智能的技术的呈现效果。在数智人的一些内容生产环节，以及数智人与虚拟空间结合的一些技术，对算力的要求都很强。如腾讯数智人跟虚拟空间结合时，一个虚拟空间会有几十G的渲染资源要做加载。3D超写实的数智人一般会有20万 ～ 30万面片的mesh，这些都需要很强的算力。</p><p></p><p>此外，腾讯云智能数智人希望通过AI驱动技术，把数智人放到企业服务场景下为企业做增效服务，这些都对算力有一定要求。当企业的写实类数智人面向用户做规模化服务时，有时需要大规模并发，因为客户或客群整体规模大，也需要很高的弹性或云计算的空间。</p><p></p><p>在这层，腾讯云智能团队做了很多技术优化，首先在渲染维度上，通过云游戏的技术或音视频的链路实现成本降低。还结合云端混合渲染，与客服做深度绑定，结合云端的混合渲染的模式降低服务成本。</p><p></p><h3>商业模式</h3><p></p><p>尽管虚拟人的发展仍在早期，但业界关于其商业模式的讨论已提上日程，腾讯云智能数智人目前在商业模式上也进行了一些探索。</p><p></p><p>当前腾讯云智能数智人主要服务于B端企业用户，且通过服务B端的客户连接服务到C端用户（B2C），通过数智人提升企业服务的质量，企业也可以通过数智人做用户运营，提升服务用户的体验。</p><p></p><p>据陈磊观察，从数智人发展趋势上看，面向企业的服务以及周期分为以下几种。第一帮助企业做形象代言人打造。第二，当企业里有形象代言人时，在一些营销场景，如广告落地转化、应用，数智人会带来很强的吸睛效应。此外，腾讯云智能在与一汽大众的合作中，探索了虚拟数智人与虚拟空间结合的形式，通过一种新的方式助力客户提升留资。还有的企业用数字人做日常用户运营，如偏直播方向，有真人驱动或AI驱动的方式，面向自身客户运营做直播，这些工具能扩大私域运营的手段方式。此外，数智人还可以围绕用户服务生命周期做精细化的企业服务，如客服场景、对话场景等，当前在每一个维度都有一些比较好的实践落地。</p><p></p><h2>数智人和全真互联</h2><p></p><p></p><h3>人机交互的下一个范式</h3><p></p><p>陈磊表示，腾讯将数智人定位成交互智能的重要入口。今年7月，腾讯云智能与腾讯研究院、创业黑马联合发布的“数智人十大趋势”中提到，无论是身份型的数智人，还是融合了语言理解表达或学习交互能力的服务型数智人，从在线服务到场景体验，数智人都是人机交互相对典型的一个好的范式和入口。</p><p></p><p>而且，从整体的内外部趋势看，数智人会进入大规模的应用期，加速转化为现实生产力，无论是消费级还是企业服务赛道都能创造很大的商业价值。</p><p></p><h3>全真互联世界的数智人</h3><p></p><p>数字虚拟人的话题度很大程度上与元宇宙分不开。元宇宙是近两年的顶流概念。一种说法认为，腾讯所提的全真互联是对元宇宙的另一种表述。</p><p></p><p>陈磊表示，腾讯希望用IOT、AI、云计算等技术，将真实世界和虚拟世界做全面的感知、链接，用数智融合的创新技术满足各行业升级转型的需求。</p><p></p><p>数智人是AI技术具像化的一种展现形式。一个AI综合的应用需要不断整合各种创新AI，包括背后的决策智能的大数据，云计算的技术、音视频技术等，本质上，数智人是技术层面对全真互联的探索。</p><p></p><p>陈磊列举了一些数智人未来在全真互联的应用场景。</p><p></p><p>例如，去银行办理业务，现在用户要去线下的物理银行的窗口办理，但如果是全真互联的形式，用户坐在家里，用电脑或AR就可以通过自己的身份进入场景中，进入银行展厅后可以获得营销服务，这些服务就由企业的数智人提供，包括营销推荐、推荐后实时互动，银行开卡业务等。将交互服务做了升级，节约了用户线下去实体网点办理的时间，提升效率。在类似这样的交互入口中，数智人会起到很大作用。</p><p></p><p>值得注意的是，在面向企业服务的场景，如果企业数智人自身在一些智能化场景里有足够的行业积累和知识图谱，在AI技术的加持下，数智人就可以通过自适应、自学习，不断提升服务能力，实现在全真互联中为用户提供更好的服务。</p><p></p><p>采访嘉宾介绍：</p><p></p><p>陈磊，腾讯云智能数智人产品总经理。先后参与负责QQ浏览器、腾讯翻译君、腾讯同传、腾讯云智能数智人等产品的研发工作。</p><p></p>",
    "publish_time": "2022-12-01 15:01:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "初创公司想要聊天机器人，但80%缺乏关于对话式人工智能的知识",
    "url": "https://www.infoq.cn/article/sQH37LsxTC3YbJrzwXPW",
    "summary": "<p>本文最初发布于readwrite博客。</p><p></p><p><a href=\"https://readwrite.com/the-ultimate-guide-to-conversational-ai-in-2022/\">对话式人工智能</a>\"市场正在急剧增长，训练新的大型生成模型，扩大技术栈，并为市场带来更多先进的产品。到2030年，其市场规模<a href=\"https://www.grandviewresearch.com/industry-analysis/conversational-ai-market-report\">预计将达</a>\"420亿美元左右。对话式AI产品背后的技术也变得越来越复杂。</p><p></p><p>这并不奇怪——科技巨头在研发上投入了大量资金。例如，OpenAI项目（由埃隆·马斯克及其他科技明星在2015年创建）获得了10亿美元的捐赠，在接下来的三年又额外从微软<a href=\"https://techcrunch.com/2019/07/22/microsoft-invests-1-billion-in-openai-in-new-multiyear-partnership/\">筹集</a>\"了10亿美元。</p><p></p><p>与此同时，除了GPT-3（Generative Pre-Trained Transformer 3）等大型生成模型外，现代而又先进的ConvAI应用范围还很有限。尽管有许多“玩具”项目使用了GPT-3，也有一些聊天产品使用了GPT-3及类似模型，但其他方面的进展应用还没有那么广泛。如今，小公司该如何从技术发展中受益并实现自己的人工智能助手？</p><p></p><h2>期望与现实</h2><p></p><p>DeepPavlov.ai团队开发的开源对话式AI技术栈旨在加速和简化聊天机器人和人工智能助手的开发。2022年6月，我们向在其业务中使用会话式AI的初创公司了解了他们使用这项技术的预期结果。</p><p></p><p>我们采访了20位初创公司的创始人和首席技术官，他们来自教育科技、金融科技、自动化和咨询市场。所有人都提到，聊天机器人为其细分市场带来了无可争辩的好处，包括让业务更加顺畅。</p><p></p><p>结果显示，初创公司希望<a href=\"https://readwrite.com/the-ultimate-guide-to-conversational-ai-in-2022/\">ConvAI</a>\"能带来实质性的结果，其中包括提高客户接触的遏制率，提高呼叫中心的人员效率，以及借机削减运营成本。但与此同时，近80%的受访者提到，在实施和开发聊天机器人时遇到了困难。</p><p></p><p>早些时候，RASA也研究了<a href=\"https://readwrite.com/increase-your-website-conversions-by-3x-with-conversational-ai-chatbot/\">会话式AI在客户服务领域的应用</a>\"现状。他们指出，这项技术主要带来了两方面的业务优势：自动与客户进行双向自然语言对话的能力，以及通过分析对话了解客户需求的能力。</p><p></p><p>这些结论与Gartner的全球研究结果一致。Gartner表示，小型企业可以通过<a href=\"https://readwrite.com/artificial-intelligence-in-customer-support-what-ai-means-for-your-chatbots/\">使用聊天机器人取代人工</a>\"来节省工资和培训费用。</p><p></p><p>投资聊天机器人的主要成果包括通过虚拟助手增加客户联系次数，改善客户体验，同时提高销售经理的效率，获得额外的商业机会。</p><p></p><h2>采用障碍</h2><p></p><p>现代ConvAI采用的瓶颈是初创公司的创始人对技术机遇缺乏清晰、深刻的理解。他们不是很清楚自己需要聊天机器人（特别是NLP分类器）提供什么功能。这一点，他们在回答我们的提问时提到了。</p><p></p><p>一半的受访者表示，聊天机器人可以在与客户对话时帮助他们提取人类语音中的意图和特征。另一个普遍的需求，大约占45%，是提取命名实体。而10%的人则表示，他们需要<a href=\"https://readwrite.com/how-ecommerce-businesses-can-take-advantage-of-ai/\">对人类语音的情感特征进行分类</a>\"。</p><p></p><p>由此可以看出，对话式AI创建者的设想通常是多么模糊。大多数人（80%）不理解先进的对话式AI和聊天机器人对他们的业务都意味着什么。因此，这限制了他们实施此类技术。</p><p></p><p>此外，对话式AI平台支持的领域、模式和行业越多就越复杂，就越难为普通的初创公司市场参与者所使用。</p><p></p><p>因此，无论是技术的创造，还是让它变得可理解，都对未来对话式AI的蓬勃发展至关重要。在初创公司开展会话式AI培训可以帮助他们填补知识空白。</p><p></p><h2>让初创公司可以从对话式AI受益</h2><p></p><p>只有科技巨头、实验室、开发人员和初创公司之间展开对话，聊天机器人及其可用性才会有一个更好的未来。</p><p></p><p>Alexa Prize挑战赛就是一个很好的例子。大赛为团队提供了一个在安全空间里测试技术并进行数千次对话的机会，让他们可以获得独特的发现。</p><p></p><p>例如，去年，其中就有一个团队<a href=\"https://hai.stanford.edu/news/how-create-better-chatbot-conversations\">了解到</a>\"，有10%的用户与机器人交谈超过10分钟，并试图通过提出个性化问题来建立关联性联系，甚至容忍了机器人的古怪行为。</p><p></p><p>毫无疑问，亚马逊在鼓励开发人员增加Alexa技能或以它们为基础构建ConvAI解决方案方面做得很好。但由于其核心技术是闭源的，其进一步应用受到了限制。</p><p></p><h3>另一种方法是借助开源ConvAI解决方案提供的强大功能。</h3><p></p><p>RASA正在用它的开源框架支持面向任务的聊天机器人开发。但是，使用该技术开发多功能AI助手仍然是一个挑战。作为一个诞生于学术界的项目，DeepPavlov的团队希望可以促进其开源应用。</p><p></p><p>我们的目标是帮助目标用户（包括中小型企业）简化复杂产品的开发，提升他们的开发速度。</p><p></p><h3>人工智能可以为初创公司带来巨大收益</h3><p></p><p>不过，有一件很重要的事是先开展相当数量的培训。为了最大化<a href=\"https://readwrite.com/are-customer-service-chatbots-worth-the-hype/\">聊天机器人和虚拟助手的收益</a>\"，初创公司应该知道预期交付成果是什么，以及如何针对他们特有的情况开发这项技术。</p><p></p><p>市场玩家要意识到，技术的进步应易于理解和使用。</p><p></p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p></p><p>原文链接：<a href=\"https://readwrite.com/startups-want-chatbots-but-80-lack-knowledge-about-conversational-ai\">https://readwrite.com/startups-want-chatbots-but-80-lack-knowledge-about-conversational-ai</a>\"</p>",
    "publish_time": "2022-12-01 15:30:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]