[
  {
    "title": "AirBnb开源动画引擎Lottie：采用Core Animation提高性能",
    "url": "https://www.infoq.cn/article/hPFqXYhRPqjOh6CdHuYR",
    "summary": "<p>AirBnb<a href=\"https://medium.com/airbnb-engineering/announcing-lottie-4-0-for-ios-d4d226862a54\">宣布了</a>\"其开源矢量动画引擎Lottie的第四次主要迭代。AirBnb iOS工程师Cal Stephens表示，由于采用了核心动画（Core Animation），Lottie 4.0提供了显著的性能改进，并降低了CPU负载。</p><p>&nbsp;</p><p>Lottie是一个用于iOS和Android的库，可以实时渲染After Effects动画。Lottie中的动画是通过JSON文件描述的，可以使用<a href=\"https://github.com/bodymovin/bodymovin\">Bodymovin</a>\"插件从After Effects中导出。Lottie将对JSON进行解码，并访问渲染动画所需的所有资源，就像它只是应用程序中的另一个静态资源一样。</p><p>&nbsp;</p><p>在Lottie 4.0中，AirBnb放弃了使用<a href=\"https://developer.apple.com/documentation/quartzcore/cadisplaylink\">CADisplayLink</a>\"在主CPU线程上制作图形动画的原始方法：</p><p>&nbsp;</p><p></p><blockquote>每帧一次，Lottie将在主线程上执行代码，以推进动画的进度并重新渲染其内容。这意味着动画在播放时将消耗5–20%以上的CPU，从而减少了用于应用程序其余部分的可用CPU周期。</blockquote><p></p><p>&nbsp;</p><p>根据Stephens的说法，这使得Lottie在更复杂的情况下无法维持所需的帧速并且开始丢弃帧会变得相对常见。此外，当主线程忙于一些昂贵的任务时，动画可能会出现一些抖动。</p><p>&nbsp;</p><p>切换到核心动画（Core Animation）意味着动画被卸载到GPU上，这有三个好处：利用硬件加速，减少CPU负载，并在CPU繁忙时提高帧速。</p><p>&nbsp;</p><p></p><blockquote>例如，Airbnb应用程序在首次启动时显示Lottie动画。我们在这里进行了一项实验，发现切换到新的渲染引擎可以缩短应用程序的总启动时间，同时还可以提高启动动画的帧速和用户体验。</blockquote><p></p><p>&nbsp;</p><p>除了新的基于核心动画（Core Animation）的渲染引擎外，Lottie 4.0还带来了一种<a href=\"https://dotlottie.io/\">新的文件格式</a>\"，它使用压缩将一个或多个Lottie JSON文件及其相关资源聚合到一个文件中。JSON解码管道已被重写，速度提高了约2倍。</p><p>&nbsp;</p><p>Lottie最初是用Objective-C编写的，两年前在Swift中被完全重写，最终发布了第3版。你可以从<a href=\"https://github.com/airbnb/lottie-ios\">GitHub repo</a>\"或使用包管理器（如CocoaPods、Carthage或Swift包管理器）安装它。</p><p>&nbsp;</p><p>作者简介：</p><p>Sergio De Simone是一名软件工程师。Sergio已经在一系列不同的项目和公司担任软件工程师超过15年，其中包括西门子、惠普和小型创业公司等不同的工作环境。在过去的几年里，他一直专注于移动平台和相关技术的开发。他目前就职于BigML，Inc.，负责iOS和OS X的开发。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/12/airbnb-lottie-4/\">https://www.infoq.com/news/2022/12/airbnb-lottie-4/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://xie.infoq.cn/article/c69eeb94f0023cde7bc0bf80a\">React 源码解读之 React Fiber</a>\"</p><p><a href=\"https://www.infoq.cn/article/jEs0tEhfmkjbQ1LGPkbI\">虚拟角色赛道的新“闯入者”：3D 引擎 Cocos 和它的新故事</a>\"</p>",
    "publish_time": "2022-12-29 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "突发：阿里云高层大调整，程立并未离职，张勇兼任阿里云总裁",
    "url": "https://www.infoq.cn/article/bdVx72jsR4fzEmiPNNH3",
    "summary": "<p>12 月 29 日，据钛媒体 APP 报道称：阿里云高层今晨大调整，阿里巴巴集团 CEO 张勇（花名：逍遥子）将兼任阿里云智能总裁，原阿里巴巴 CTO 程立离职；张建锋（花名：行癫）不再担任阿里云智能总裁，继续担任达摩院院长，专注前沿科技探索；阿里云智能周靖人（花名：靖人）将担任阿里云智能 CTO，并同时继续兼任达摩院副院长。</p><p></p><p>根据 InfoQ 多方求证，程立并未离职，而是卸任。他会继续担任 CEO 的技术顾问。程立卸任的一个主要的原因是，向年轻的阿里技术 Leader 交棒传承，这也是阿里一直以来的传统。</p><p></p><p>另外，据悉张勇也在今天上午发出了全员邮件，宣布了阿里云的组织变化。具体调整包括：</p><p></p><p>张建锋不再担任阿里云智能总裁，张勇将兼任阿里云智能总裁。张建锋继续担任达摩院院长，全力带领达摩院进行科技创新突破与前沿科技研究。张勇在信中表示，希望张建锋带领达摩院在科技领域继续勇攀高峰。</p><p></p><p>张勇同时任命了阿里巴巴的新首席人力官（CPO）和新首席技术官（CTO）：程立将不再担任阿里巴巴 CTO，由吴泽明接任，这是阿里巴巴的首位 80 后 CTO。明年 4 月 1 日起，童文红不再担任阿里巴巴 CPO，由蒋芳接任。</p><p></p><p>针对内部信中的调整，阿里云公关向 InfoQ 回应称，消息属实。</p><p></p><p>&nbsp;附张勇内部信全文：</p><p></p><p>以奋进之心 迎接全新一年</p><p>&nbsp;</p><p>各位阿里人，</p><p>&nbsp;</p><p>过去一年非常不易，新的一年即将开启。特别在当前疫情快速蔓延的阶段，很多很多同学在和病毒抗争的过程中，还心系工作，努力保障着集团各项业务稳定运行，用各种可行的方式继续着对客户的服务。在这里，我衷心地感谢大家，也非常地牵挂大家，希望我们都能平稳渡过，顺利进入到一个工作、生活的崭新阶段。</p><p>&nbsp;</p><p>在今年年初做全年展望的时候，我写下的关键字是‘定’；在2023年即将开始的时候，我希望新的一年中这个关键字是‘进’。随着社会进入到和疫情共处的新阶段，随着国家对平台经济指明了未来发展方向，我们更加确信，唯有发展才是硬道理，才能解决今天碰到的很多问题。打铁还需自身硬，阿里发展的核心是做‘好’自己，做‘好’企业。只有在一点一滴的行动上保障好客户利益，创造好客户价值，才能承担好引领发展、创造就业、参与国际竞争的大使命。</p><p>&nbsp;</p><p>我希望在新年即将开始之际，和大家分享关于集团组织变化的系列决定：</p><p>&nbsp;</p><p>张建锋（行癫）不再担任阿里云智能总裁，继续担任达摩院院长，专注前沿科技探索，并将继续分管平头哥和智能互联。我将acting阿里云智能总裁，并直接分管钉钉。周靖人（靖人）将担任阿里云智能CTO，并同时继续兼任达摩院副院长。</p><p>&nbsp;</p><p>过去四年，行癫带领全体阿里云小二在技术创新和行业影响上取得了显著成绩，对阿里云的发展做出了重要贡献。希望行癫带领达摩院在科技领域继续勇攀高峰。对云计算而言，稳定和安全是对客户最基本的责任，我们要始终秉持敬畏之心，不辜负客户的信任和依托。</p><p>&nbsp;</p><p>程立（鲁肃）将不再担任集团CTO和达摩院常务副院长，由吴泽明（范禹）担任集团CTO和达摩院副院长。进入阿里巴巴几年间，鲁肃为阿里技术发展特别是技术人才培养做出重要努力和贡献，未来他将以技术顾问的身份继续协助我的工作。吴泽明（范禹）同时继续兼任本地生活业务的CTO。</p><p>&nbsp;</p><p>上述任命即日起生效。</p><p>&nbsp;</p><p>童文红（Judy）将于2023年4月1日起不再担任阿里巴巴集团CPO。童文红从初创时期就进入公司，伴随公司成长，历经多个重要岗位，建设了我们美丽的西溪园区，参与创建了菜鸟物流，在担任集团CPO期间，全面推动了集团的组织体系建设。未来她将继续以阿里巴巴合伙人的身份参与集团文化运营。集团CPO的岗位将由蒋芳（蒋方）接任。</p><p>再次特别感谢行癫、鲁肃和童文红三位，他们把最好的时光给了阿里，多年来承担了重大责任，为阿里的成长做出了巨大贡献。</p><p>&nbsp;</p><p>2023年马上就要到来了，对新的一年我们都有很多憧憬。2023年对阿里巴巴是非常关键的一年，希望所有阿里人一起用进取之心、集奋进之力，来迎接这全新一年。我们携手并进！</p><p>&nbsp;</p><p>阿里巴巴集团</p><p>董事会主席兼CEO</p><p>张勇（逍遥子）</p><p>2022.12.29</p><p>&nbsp;</p>",
    "publish_time": "2022-12-29 13:09:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Twitter 终究还是挂了",
    "url": "https://www.infoq.cn/article/SEMJIMViDzxqUQ4gebBT",
    "summary": "<p></p><blockquote>这次大规模崩溃发生在马斯克以 440 亿美元收购 Twitter 的两个月后，不确定这次宕机是否预告着一次即将发生的“完全崩溃”。</blockquote><p></p><p>&nbsp;</p><p>Twitter 用户报告了这次重大的全球中断，全球数万用户无法访问该网站或使用其主要功能。据流量跟踪网站<a href=\"https://downdetector.com/status/twitter/\">downdetector.com 称</a>\"，该网站变更为“不可用”发生在格林威治标准时间午夜。在一个小时内，该网站记录了 10,000 多份用户访问失败的报告，其中包括美国、日本、英国等多个国家的用户。</p><p>&nbsp;</p><p>总部位于伦敦的互联网监控器<a href=\"https://twitter.com/netblocks/status/1608264345498312705\">NetBlocks</a>\"表示，“Twitter 正在经历全球性中断，影响包括移动应用程序、功能（含通知）”。</p><p>&nbsp;</p><p>NetBlocks 还补充说，这次属于“广泛性”事件，与“国家级互联网中断或过滤”无关。Twitter<a href=\"https://api.twitterstat.us/\">尚未承认中断</a>\"，其<a href=\"https://api.twitterstat.us/\">状态页面</a>\"显示所有系统都在运行。</p><p>&nbsp;</p><p>一些用户反馈Twitter出现很多奇怪的错误消息，比如看到的是空白页面，或无法回复推文或关注热门话题，而另一些人是退出服务。Twitter 还向一些用户显示“速率超出限制”，这表明其服务器无法处理传入的请求。话题标签#TwitterDown 正在该平台上流行。</p><p>&nbsp;</p><p>Twitter首席执行官埃隆·马斯克表示，他仍然可以使用这项服务。“对我有用，”马斯克回应一位询问 Twitter 是否崩溃的用户时说。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b7df1cdfa815bdc06a6a74690f21d681.png\" /></p><p></p><p>12 月24日，马斯克还在炫耀Twitter的稳定性，“即使在我断开了其中一个更敏感的服务器机架的连接后，该服务仍在运行。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0acfbbf4fee5e3c5a930dc00265676f1.jpeg\" /></p><p></p><p></p><p>参考链接：</p><p>https://techcrunch.com/2022/12/28/twitter-down-outage/</p><p>https://www.reuters.com/technology/twitter-down-thousands-users-downdetectorcom-2022-12-29/</p><p>https://www.theguardian.com/technology/2022/dec/29/twitter-users-report-global-outage-with-many-unable-to-log-into-website-or-app</p><p>https://www.theguardian.com/technology/2022/dec/29/twitter-users-report-global-outage-with-many-unable-to-log-into-website-or-app</p><p></p>",
    "publish_time": "2022-12-29 13:35:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "历时一年半，Apache Kyuubi正式毕业，成为 ASF 顶级项目",
    "url": "https://www.infoq.cn/article/Tw95p8r6dzNJhzOjK0DC",
    "summary": "<p></p><h2>Apache Kyuubi正式毕业</h2><p></p><p>&nbsp;</p><p>2022年12月22日，Apache 软件基金会（ASF）官方宣布<a href=\"https://www.infoq.cn/article/rlVLvAVVxJ7ym4ytgFEk\"> Apache Kyuubi</a>\" 正式毕业，成为顶级项目（TLP）。</p><p>&nbsp;</p><p>Apache Kyuubi 是一个分布式和多租户网关，用于在数据仓库和湖仓上提供无服务器SQL。项目最初由网易数帆开发并于2018年开源，2021年6月捐赠 Apache基金会，经过1年多的孵化于2022年11月通过投票，在12月顺利毕业，成为 Apache 基金会顶级开源项目。</p><p>&nbsp;</p><p>据介绍，Apache Kyuubi 在孵化过程中，<a href=\"https://mp.weixin.qq.com/s/Rm_TO-CjC0-OEoGpIe6KWw\">社区</a>\"迎来了一百多个贡献，有数千个提交，总计成功发布了九个版本。来自不同公司和国家的开发者和其他类型的贡献者在社区中度过了一段快乐的旅程。</p><p>&nbsp;</p><p>Kyuubi 目前已被全球数百家企业<a href=\"https://xie.infoq.cn/article/9e6ddbde30c7de8847ad5115b\">采用</a>\"，涉及多个行业，如云基础设施、互联网、金融、医疗、打车服务、物流、游戏和ACG，等等。像阿里巴巴、Bilibili、中国移动、携程、丁香园、eBay、爱奇艺、广发证券、kt NexR、网易、腾讯、T3、Womply、小米、雪球和知乎等公司都在使用 Apache Kyuubi。</p><p>&nbsp;</p><p>网易副总裁、网易杭州研究院执行院长汪源表示：“Kyuubi 是一个广泛使用的 SQL 门户，其无服务器设计，支持常见的 SQL 驱动，大大提升了大数据技术的使用便利性。作为客户端和底层技术之间的门户和网关，Kyuubi 也有很大的想象空间进行功能扩展，如 SQL 审计、检查、基于查询历史的优化等等。这是一个具有巨大潜力的项目。”</p><p>&nbsp;</p><p>Apache Kyuubi VP 姚琴表示：“看到社区成长并从孵化器毕业，这意味着我们已经成功地采用了 ‘Apache Way’并能够自我管理，这很有趣也很令人兴奋。作为一个 Apache 顶级项目，Kyuubi 是由全球各地的人们和组织开发和使用的。Kyuubi 帮助他们实现数据民主化，使具有任何技能水平的人都能舒适地处理数据，而不管他们的技术知识如何。”</p><p></p><h2>“九尾狐”Kyuubi</h2><p></p><p>&nbsp;</p><p>Kyuubi 的命名源自中国神话《山海经》，意为“九尾狐”。狐会喷火，象征 Spark；狐有九尾，类比多租户。这个命名也体现出了 Kyuubi 系统设计之初的主要目的——在 Spark 上实现多租户。</p><p>&nbsp;</p><p>事实上，在 Kyuubi 之前，市面上已存在具备类似能力的产品，比如 Spark ThriftServer（简称 STS）。这是 Spark 社区现有的、基于 HiveServer2 实现的 Thrift 服务，旨在无缝兼容 HiveServer2。</p><p>&nbsp;</p><p>虽然 STS 的性能极佳，但当前并不完善，尤其在企业场景下存在较多短板。比如，单 Spark 应用实现的 STS 并不能完整支持多租户，因为 STS 本质上是一个 Spark Application，整个 Application 只有全局唯一的用户名，并同时包括 Driver 端和 Executor 端。而对于像网易这样有多条产品线的互联网公司来说，每条产品线的数据在一定程度上是隔离的。因此，只有支持多租户才能满足公司对于数据安全、资源隔离、高可用以及高并发的要求。</p><p>&nbsp;</p><p>这也就促使网易内部开发了 Kyuubi。Kyuubi 在统一接口基础上，拓展了 STS 在多租户模式下的使用场景，并依托多租户概念获得了完善的资源隔离共享能力和数据安全隔离的能力。</p><p>&nbsp;</p><p>Apache Kyuubi 在各种现代计算框架之上建立了分布式SQL查询引擎，例如 Apache Spark™、Apache Flink™、Apache Doris™、Apache Hive™和Trino等，以查询分布在异质数据源的机器群上的大规模数据集。</p><p>&nbsp;</p><p>统一网关：通过一个入口点实现对任何集群资源的简化、安全访问，为终端用户部署不同的工作负载</p><p>&nbsp;</p><p>应用编程接口：支持各种API，包括Apache Thrift™、JDBC、ODBC、REST等，便于访问。多租户：支持端到端的多租户，这对集群的安全性和并发性都有利。高可用性：确保其在指定时间内连续无障碍运行，以满足商定的运行性能水平。</p><p>&nbsp;</p><p>无服务器SQL及更多：使最终用户更容易从数据宇宙中获得洞察力，并优化数据管道，无论他们的技术知识如何。它能够使用熟悉的SQL为各种工作负载提供与RDBMS相同的用户体验，在不同的数据源上提供广泛和安全的数据访问能力，并通过可扩展的计算资源为大量数据提供高性能。</p><p>&nbsp;</p><p>易用性：终端用户可以有一个优化的体验，以无服务器的方式探索他们的数据宇宙。相应的引擎，如 Spark 和Flink 的 \"超能力 \"不再是必要的。在任何地方以任何规模运行：所有的预编程引擎都有分布式后端，可以在单节点机器上或跨集群安排任务。高性能：最先进的查询引擎、服务器端的全局和持续优化等保证了整个集群的性能提升。</p><p></p>",
    "publish_time": "2022-12-29 14:40:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里妈妈内容风控模型预估引擎的探索和建设",
    "url": "https://www.infoq.cn/article/2f9827a4384d7ce6b66987107",
    "summary": "<p>​作者：徐雄飞、金禄旸、滑庆波、李治</p><p></p><p></p><blockquote>内容作为营销的重要载体，能够促进信息的交流和传播。在营销场景中，广告高曝光的特性放大了风险外漏带来的一系列问题，因此对内容的风控审核就显得至关重要。本文将为大家分享阿里妈妈内容风控模型预估引擎的探索和建设。</blockquote><p></p><p></p><p></p><h1>一、业务背景及问题</h1><p></p><p></p><p></p><h2>1.1 业务背景</h2><p></p><p></p><p>内容作为营销的重要载体，能够促进信息的交流和传播。在营销场景，广告内容需要满足广告法要求，常见的风险类型有两类：一类是面底线的A类风险，如政治等，一类是面向平台调性，广告法要求的B类风险，如低俗不雅、公序良俗等。一旦风险外漏，由于广告高曝光的特性，会导致舆论压力，甚至有业务关停的风险。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/5699241aece0edfa952d40eca8618aeb.png\" /></p><p></p><p>阿里妈妈内容风控核心由机审、人审两部分组成，其中机审防控包括增量和全量防控（链路如下图所示），机审与人审环节都依赖模型服务，由于广告数据和主站存在差异，且风控的强对抗性、模型都自研实现。随着模型不断增多，模型的线上服务部署和管理变的越来越臃肿，出现诸多问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/bec80eaf7cebbed9dee8984e38811480.png\" /></p><p></p><p></p><h2>1.2 现存问题</h2><p></p><p></p><p>现有的模型预估引擎（Inference-kgb）由于历史业务演进、临时性支持妥协，随着模型数量不断增加问题逐渐暴露出来，下面从能力、效率、质量、成本四个方面详细分析现有的问题，为我们重构新的引擎打下基础。</p><p></p><p></p><h3>1.2.1&nbsp;能力问题</h3><p></p><p></p><p>缺乏统一的加速方案：仅部分模型通过GPU进行加速，大部分模型性能较差；异构混部模型缺乏通用的接入、导流、蓄洪和限流等服务特性和运维特性。</p><p>﻿</p><p>一致性问题：离线和在线模型结果一致性难以保障，没有形成通用方案；跨平台模型Case by Case上线，没有统一的服务API；运维干预纯靠人工经验，无可靠的管控API。</p><p></p><p></p><h3>1.2.2 效率问题</h3><p></p><p></p><p>研发周期长：Inference-kgb框架底层采用C++实现，算法同学上线前需要将训练的Python代码翻译成C++代码，开发周期长且部分库无法直接翻译需要手写，会导致实验结果和线上不一致。</p><p></p><p>部署上线流程复杂：每个模型上线都需要代码开发，像分类模型代码及配置都相同只更换模型文件，由于框架限制需要走完开发、测试、部署、上线全流程。过去模型部署链路在/近/离线是分别管理的，模型上线过程中需要跨多个平台进行配置，操作繁琐。每次模型上线需要通过邮件申请Metaq（阿里内部消息中间件）消息，等Metaq同学回复邮件后才能够继续发布流程；研发流程无法保障多人协作安全、高效。</p><p></p><p></p><h3>1.2.3&nbsp;质量问题</h3><p></p><p></p><p>缺乏统一的接入标准：早期整个框架是完全自研实现，可参考的方案较少，不断演进之后与业界方案存在差异。如缺乏统一的Tensor in Tensor out模型层面无业务语义的接口，用户接口设计不合理，接口核心字段为imageUrl和textString导致很多字段都通过拼接的方式传入textString中；输入非结构化存在诸多问题，如离线冷启动执行模型时，需要手动按照线上逻辑进行字段拼接。</p><p>﻿</p><p>子模型问题：将多个通用分类、检测模型部署在一个服务中，然而服务及运维属性都是按照模型粒度设置的，无法根据子模型定制：如Cache、计算资源等是针对整个服务维度生效的，无法根据子模型进行精细化控制。</p><p>﻿</p><p>同质代码缺乏复用：如检索类模型提取向量特征之后需要进行检索库匹配操作，过去的服务框架将特征提取和检索融合在一起，由于检索库的差异会导致部署模型数量膨胀。</p><p></p><p></p><h3>1.2.4 成本问题</h3><p></p><p></p><p>GPU利用率低下：部分大流量的模型之前没有GPU版本，GPU模型普遍使用率比较低。离线无法充分利用资源提取特征，由于模型缺乏GPU镜像，导致离线提取特征是无法充分利用智能引擎(MDL)资源池、主搜(Hippo资源调度)资源池的离线GPU资源。</p><p></p><p></p><h1>二、业界对比选型</h1><p></p><p></p><p>基于以上问题，我们考虑对现有模型预估系统（Inference-kgb）进行重构，优先考虑复用集团或开源现有的能力，因此我们对集团及开源方案进行了调研。</p><p></p><p></p><h2>2.1 业界方案对比</h2><p></p><p></p><p>分别调研阿里云EAS、达摩院Aquila、CRO灵境、阿里妈妈HighService、开源TritonServer。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6e21df11912dc4c5f46b378c7c8ffe94.png\" /></p><p></p><p></p><h2>2.2 业务诉求</h2><p></p><p></p><p>以上平台/框架，有些包含完整的模型部署、运维功能，有些只包含模型加速功能。我们尝试推演将模型整体部署到平台上，遇到以下几个问题：</p><p></p><p>资源互通问题：以上平台（如EAS），需要在ASI（Alibaba Serverless infrastructure）上统一申请资源，现有的Hippo资源无法直接迁移过去使用。</p><p></p><p>离线计算问题：目前已有平台解决的基本都是在线服务部署的问题，由于风控业务特性，我们遇到风险事件时需要在离线环境进行大批量的特征提取及全量数据的风险防控，在指定时效内完成风险清理，这部分没有现有能力支持。</p><p></p><p>缺乏偏业务语义的功能：针对偏业务语义的功能，平台没有很好的支持，如模型对应的风险管理，版本迭代后的业务效果，模型结果复用等。</p><p>﻿</p><p>考虑到业务诉求及已有能力最大程度复用，核心构建上层偏业务的服务能力，底层能够接入EAS、HighService、Aquila 等多种Backends，进行模型服务和加速。</p><p></p><p></p><h1>三、模型服务框架</h1><p></p><p></p><p>风控业务场景，模型服务核心解决机审风险识别及人审工单拦截等相关问题。模型分别服务于在/近/离线业务，引擎内核复用同一套逻辑，确保在/近/离线数据一致，底层接入多种Backends，对模型进行部署、加速。可以在各个调度器及资源池中进行计算。重构后新的风险预估引擎RD（RiskDetection）层次结构如下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e88a14822184d49bea5ffcd6c2c8a41f.png\" /></p><p></p><p>RD内核部分，参考NIVIDIA Triton Server实现（NVIDIA开源推理框架），我们定义了一套标准业务接口，支持多Backends对接的在线服务。其中Standard API支持Tensor in Tensor out标准模型协议，由Model、Version、Tensor标准三元组构成，模型内部支持动态Batching特性。通过对接集团及开源社区已有的Backends低成本的实现模型预估能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b40c2c343c92c66405662d7639c099a6.png\" /></p><p></p><p></p><h2>3.1 标准化接口</h2><p></p><p></p><p>接口定义分为面向业务及面向数据两个维度，面向业务包含图片、文本、视频等内容识别输入输出接口；面向数据面，我们与主流的KServe（开源云原生模型推理框架）对齐，采用Tensor in Tensor out进行模型预估逻辑。</p><p></p><p></p><h2>3.1.1 业务接口</h2><p></p><p></p><p>业务侧支持图片、文本、视频等素材类型，包含检测、识别、分类、检索等模型服务。过去的系统（Inference-kgb）所有的模型都共用一个接口，输入是图片、文字，输出是Map对应的K,V。系统运行一段时间之后，发现输入、输出变的越来越复杂，于是在字段上进行各种拼凑，导致输入、输出变的越来越复杂。吸取Inference-kgb的经验教训，也参考阿里云等开源接口设计，我们针对不同类别的模型进行独立接口设计。</p><p></p><p></p><h3>3.1.2 数据面接口</h3><p></p><p></p><p>在模型内部，都是以Tensor数据格式进行传输，我们支持调用方通过Tensor接口调用模型服务，通过TensorFlow、KServe以及内部EAS对TensorFlow/PyTorch模型的服务接口抽象，定义我们自己需要的TensorFlow/PyTorch模型服务标准API如下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f1/f10f21e2368ea14db7b04a545eef476c.png\" /></p><p></p><p></p><p></p><h4>3.1.2.1 Predict接口</h4><p></p><p></p><p>1）TensorDataType：TensorDataType用于标准化指定Tensor的数据类型。</p><p></p><p>2）TensorShape：用于标准化指定Tensor的维度。</p><p></p><p>3）TensorDataContent：内部的EAS和外部开源的TensorFlow其实都没有TensorDataContent这个抽象，都使用的完全平铺的方式定义Tensor内容，KServe最新版本对TensorData做了抽象，是能够包含EAS和TensorFlow的定义的，因而采用。</p><p></p><p>4）InferParameter：InferParameter这个数据结构，内部的EAS和开源的TensorFlow都没有定义，但是KServe最新版本的API定义里预留出来了，目的是未来能够基于标准化的Tensor定义支持跨多个不同的非标准化推理服务平台后端，通过拓展参数增强跨平台的可移植性。考虑到内容风控未来线上模型也会对接多个非标准化Backend，所以考虑在前面统一的API Protocol标准化Tensor结构定义的基础上，也引入InferParameter以实现跨平台的对接能力。</p><p></p><p>5）TensorEntity：把TensorDataType、TensorShape、TensorDataContent和InferParameter组合，得到一个标准化定义的TensorEntity数据结构。</p><p></p><p>6）PredictRequest：如前面所述，我们把原子的Tensor in Tensor out的推理服务命名为Predictor，PredictRequest则是每个Predictor角色的标准化输入接口入参，这里提取EAS、TensorFlow和KServe各自有用的部分组合而成。</p><p></p><p>7）PredictResponse：PredictResponse核心返回数据结构是TensorEntity，同时把PredictRequest原始请求的模型信息、拓展参数信息也带回来（参考KServe设计，原生TensorFlow和EAS没有）。</p><p></p><p></p><h4>3.1.2.2 Metadata接口</h4><p></p><p></p><p>1）TensorMetadata：Tensor的元数据由Tensor名、Tensor数据类型和Tensor维度组成的三元组表示。</p><p></p><p>2）ModelPlatform：后端支持的模型框架平台类型，目前支持原生的Tensorflow、PyTorch、TensorRT三类，以及集团内部HighService、EAS、Aquail等。</p><p></p><p>3）ModelMetadataRequest：ModelMetadataRequest是用于请求模型推理服务元信息的标准化抽象接口，由模型名(name) + 模型版本(version)构成二元组。</p><p></p><p>4）ModelMetadataResponse：根据ModelMetadataRequest指定的模型名+模型版本二元组获取详细的模型推理服务信息，主要由传入的模型名、模型版本、输入Tensor元数据和输入Tensor元数据这四部分组成。</p><p></p><p></p><h2>3.2 RD内核技术方案</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/012694924161a8151eb8d610b0a7f7da.png\" /></p><p></p><p>RiskDetection：提供标准的模型用户接口，提供模型服务特性和运维特性，串联模型前处理、模型预估、模型后处理阶段。</p><p>﻿</p><p>Predictor：提供模型标准数据面Tensor in Tensor out接口，兼容多种RTP协议的Backends。Predictor逻辑上是拆分成独立的服务，物理实现为了减少I/O请求，可以考虑和前后处理部署在同一台机器上。</p><p>﻿</p><p>Transformer：Transformer用于模型服务的前后置处理，它和Predictor共同完成一个完整的模型推理请求。Transformer一方面可以通过拓展前处理逻辑把外部输入转换成Predictor接收的标准输入格式，另一方面可通过拓展后处理逻辑把Predictor返回的结果做进一步转换供给业务方应用。</p><p>﻿</p><p>Backends：底层具体采用的模型服务框架，包含集团及开源解决方案。</p><p></p><p></p><h2>3.3 数据一致性保障</h2><p></p><p></p><p>风控业务包括了在/近/离线场景，不同场景对资源、时效、吞吐等要求不一样，各场景提取出来的特征需要保障最终一致。对一致性保障可以划分成两个层级：特征完全一致、业务效果一致。</p><p>﻿</p><p>1）特征完全一致：高保障业务场景，对风险判断要求更严格，我们需要确保模型输出Tesnor保持一致，因此在/近/离线需要保证镜像、模型文件版本、计算资源完全一致。</p><p>﻿</p><p>2）业务效果一致：在一些保证级别要求不高，且计算量大的场景，我们允许在/近/离线计算资源不一致，如在线采用GPU，离线采用CPU（GPU通过TensorRT 半精度加速后结果有损），只需要在业务效果上保障一致即可。</p><p></p><p></p><h1>四、模型推理加速</h1><p></p><p></p><p>过去的Inference-kgb基于原生TensorRT进行模型加速，随着模型迭代更新越来越频繁，每次算法同学训练模型到上线都需要将Python代码用C++重新实现一遍。通过对集团及开源方案调研，现有的框架很多都支持Python直接进行模型部署、加速，能够解决Python语言带来的GIL锁冲突问题，新的框架我们采用Python进行模型预估开发，并且能够支持多种模型加速Backends(如HighService,EAS,Aquila等)，可以直接复用已有能力，无需从0开始重新搭建模型推理加速功能。由于整体框架初见雏形，我们先接入了HighService和EAS两个Backends，支持我们核心模型的落地。</p><p></p><p></p><h2>4.1 HighService Backend接入</h2><p></p><p></p><p>HighService是阿里妈妈异构计算团队内部开发提供的，基于Python多并发异构计算服务框架。HighService框架通过解耦GPU及CPU计算，使用多进程进行CPU计算，充分利用多核CPU资源，在多个CPU进程与单个GPU进程之间通信，避免GPU进程饥饿，同时CPU进程数不受GPU大小内存限制。内部集成了TensorRT加速功能，成倍地增加了PyTorch模型的计算能力。使用HighService框架服务的动机，除了它的高性能以外，还有一个重要原因，就是利用Python模型易于迭代、维护、定位问题的优势，可以更好地服务业务，解决了业务方研发周期长的痛点。相比于过去的Inference-kgb框架，利用HighService的RD框架上线一个GPU模型的研发周期得到大幅度提升。</p><p></p><p></p><h2>4.2 EAS Backend接入</h2><p></p><p></p><p>EAS广泛在集团使用，它与PAI平台（模型训练平台）无缝衔接，并且底层拥有完善的Blade加速方案，服务和运维特性非常全面。在与EAS同学深入沟通后，发现由于计算资源池及离线业务特殊性等原因，无法直接将服务完全托管。但EAS提供的SDK能够给支持模型的部署、加速能力，这部分能力可以直接接入RD作为Backend，接入后基于镜像部署可以同时支持在/近/离线的业务。</p><p>﻿</p><p>EAS主要的接入方式有两种：Processor方式接入和Mediaflow方式接入，Processor方式需要将模型部署在EAS平台上，能够方便的部署模型，并且提供标准化的Tensor接口；由于我们无法直接将模型部署到EAS平台，因此我们采用Mediaflow的方式接入。具体实现demo如下：</p><p></p><p><code lang=\"null\"># pipeline的方式进行构建with graph.as_default():    mediaflow.MediaData() \\        .map(tensorflow_op.tensorflow, args=cfg) \\        .output(\"output\")#调用方式results = engine.run(data_frame, ctx, graph)</code></p><p></p><p>Mediaflow方式是EAS团队今年新研发的方式，可以将整个模型流程采用DAG进行串联（也可以是单个节点），能够轻量级接入SDK，并且享受到底层Blade加速效果。</p><p></p><p></p><h1>五、业务支持及效果</h1><p></p><p></p><p>完成RD系统重构，我们进行压测及效果验证达标后，开始对接业务。目前RD主要服务于阿里妈妈内容风控在/近/离线模型计算，为内容风控机审风险识别和人审提效服务。在/近线都是常驻型服务，拉起后提供HSF服务接口，离线是通过批量任务的方式启动。</p><p></p><p></p><h2>5.1 在/近线业务支持</h2><p></p><p></p><p>对比过去的Inference-kgb服务拉起配置较为繁琐，需考虑了多个子模型DAG之间的串联，采用OP算子进行关联。每个模型上线都需要开发OP，哪怕仅更换模型文件拉起不同的服务。我们将子模型组装DAG前置到业务编排组件，RD就是纯粹的进行特征提取、模型计算，因此在服务拉起及服务运维方面有较大提升。</p><p></p><p></p><h3>5.1.1 快速服务拉起</h3><p></p><p></p><p>拉起服务主要依赖 镜像+模型文件+模型配置 三元组，三元组唯一确定一个模型服务，因此快速拉起服务的首要任务就是管理服务依赖的三元组。</p><p>﻿</p><p>镜像：RD所有支持的模型都共用相同的镜像，同时支持CPU、GPU模型运行，这样方便整体的管理和运维。统一镜像也会带来一些负面作用，如修改A模型可能会影响B模型，这个需要我们在开发过程中，修改到公共部分时特别小。需要借助单元测试、集成测试等标准化流程规避此类问题。</p><p></p><p>模型文件：统一管理在OSS上，由模型名称+版本号唯一确定一个模型文件。可以根据模型文件恢复历史任意的模型服务。该功能为后续做模型效果对比等功能提供底层能力支持。</p><p></p><p>模型配置：吸取过去的Inference-kgb复杂模型配置教训，RD极大的简化配置内容，将配置分为静态和动态两部分。静态模型配置：如模型代码路径，模型Backend配置，模型是否需要Cache等，这些配置与部署的模型实例无关。如部署GPU、CPU版Face，以上配置都是相同的。动态模型配置：如服务接口、模型版本、资源类型等，每个部署实例有不同的配置。动态配置能够方便的启动相同模型的不同服务版本，能够支持业务进行效果对比或者CPU、GPU混部等能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/63/63d9d445e6322887d51fb63b41bbc640.png\" /></p><p></p><p>如上图，通过以上设计我们能够更方便的针对同一个模型启动各种版本，使用不同类型和保障级别的资源，支持各种隔离级别的业务。</p><p></p><p></p><h3>5.1.2 模型服务/运维特性</h3><p></p><p></p><p>参考集团及开源方案，核心服务/运维特性主要包括Caching、Batching、Scaling，支持以配置化地方式拓展服务/运维特性从而拓展系统处理能力，主要特性描述如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/28/28e85e81a88dc5197266c53ded1cb6b8.png\" /></p><p></p><p></p><h2>5.2 离线业务支持</h2><p></p><p></p><p>通过在线防控，我们能够解决增量风险问题，然而如果有风险外漏，或者监管调整，我们都需要对全量数据进行风险防控。同时算法同学需要在离线进行大量实验、评测，这些都依赖于离线具备大规模模型计算能力，因此RD需要具备离线计算的能力。</p><p></p><p></p><h3>5.2.1 离线任务对接</h3><p></p><p></p><p>Starling是阿里妈妈风控自研的基于主搜Drogo（资源调度管理平台）之上进行业务调度平台，底层使用主搜Hippo资源池。风控场景在离线需要提取百亿级图片、文本特征进行实验或者风险防控，模型计算需要大量的计算资源，Hippo资源池拥有海量非保障（离线）资源，Starling的初衷是可以削峰填谷地使用这些资源支持业务特性。</p><p></p><p>RD对接上Starling之后，便拥有了离线调度计算的能力，能够灵活的拉起Hippo资源，并与ODPS（对外产品为MaxCompute）打通，对ODPS数据进行读写。同时能够通过ODPS进行任务调度，进行定时增量数据特征提取。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a153171431e2876e09b64e544c8aef9.png\" /></p><p></p><p>如上图为RD与Starling对接的架构图，Starling的Master节点管理和调度RD进行模型计算，和模型配置文件分发，Starling SDK实现ODPS对接及Slave节点状态上报。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f86bfce5b62bbaa1ad68099b5389fef0.jpeg\" /></p><p></p><p>上图为通过ODPS配置Starling任务节点进行调度，实现定时特征提取功能。</p><p></p><p></p><h3>5.2.2 单容器多实例部署</h3><p></p><p></p><p>对于模型计算而言，离线与在线存在的核心差异是数据源及消费方式，在线是流式源源不断输入，离线则是固定数据集运行特定的模型。一个任务计算量是亿甚至百亿级别，内容场景有很多图片模型。考虑到拉图成本及多模型任务依赖的管理成本，我们通过单容器支持多个模型运行的方式，节约拉图60%左右，以及简化任务配置实例。</p><p>﻿</p><p>支持单模型多实例部署的前提是模型之间没有相互冲突，所以CPU资源上更容易实现，GPU资源由于16G显存的限制，很容易OOM。CPU也需要根据模型大小、内存申请等控制实例部署个数，例如30G内存部署3个模型。以下是合并前和合并后任务节点的配置，可以发现合并之后节点明显简洁得多。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/42b8cd5cb9f9a267889f8f61bf176a0a.png\" /></p><p></p><p></p><h2>5.3 业务效果</h2><p></p><p></p><p>RD上线在/近线迎来首个双十一考验，离线需解决大规模特征提取问题，无论是性能、效率都需要有显著提升，能够给业务带来可感知变化，接下来分别介绍在/近/离线因为RD上线带来的相关改变。</p><p></p><p></p><h3>5.3.1 离线清理支持</h3><p></p><p></p><p>离线部署6个核心模型支持全量及增量数据提取任务，通过Starling使用非保障资源批量提取增量百万级图片特征可在20分钟内完成。</p><p></p><p></p><h3>5.3.2 双十一业务支持</h3><p></p><p></p><p>大促结束后平台要求广告主在规定时间内下线双十一相关内容，因此相当于整体的广告内容这个时刻开始全量更新一次。</p><p>﻿</p><p>RD目前对外提供的接口是同步请求方式，QPS超过模型服务单机承载能力（CPU/GPU Load打满）时，会造成模型服务RT成倍增长影响业务稳定，因此需要对每个模型进行精确限流。内容风控模型服务众多，从模型部署角度上讲：同一个模型可能跑在GPU或者CPU上，也有可能部署在不同机房（机房拉图带宽有上限，需要多机房部署），不同集群之间有Backup的关系（比如GPU集群被限流则自动将流量导入到CPU集群），从业务角度上讲：对于高优先级场景中的底线类风险需要百毫秒级以内得到模型结果，而一些低优先级的体验类风险则可以使用一些便宜的机器进行计算，能够承受一定RT延迟。仅依赖HSF（阿里RPC框架）的限流配置或Sentinal限流（阿里分布式限流框架）组件远不能满足这种异构部署和业务上多样化的导流需求，因此我们自建了InferenceProxy服务来支持基于业务标签的QoS和导流能力，在保护下游RD服务能力的同时，对物理层面的调用关系进行配置化的编排。</p><p></p><p>导流表其中几项配置如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2eb37e9776774e6c67feacc28ef85a95.png\" /></p><p></p><p>由用户自定义的一段逻辑从请求原始Payload提取信息并打上标签，例如上面这个请求会打上label=1125这个标签。执行时会首先查出符合条件的所有导流项：1、2。按照同优先级+权重的方式分配流量，详细来说：其中1和4的priority都是0，1的weight是10, 4的weight是50，则这个请求以1/6的概率打在服务1上，以5/6的概率打在服务4上。</p><p>﻿</p><p>如果请求priority=0的服务失败或超时，则会打到priority等于1的第二项服务2上，仍旧超时或失败，则该请求会进入离线兜底队列。参考导流表设计，一个请求到来时，会筛选出符合条件的几个导流策略，按照priority进行排序分层，各priority之间使用责任链模式串联，如果请求失败或超时，则进入责任链的下一个策略priority进行执行。</p><p></p><p>核心类图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/99a0887d3ea625dd21ae6078fb171d65.png\" /></p><p></p><p>过去的Inference-kgb由于性能问题，TOP3请求的模型存在严重的性能瓶颈。RD上线后，我们可以同时支持GPU版和CPU版在线上运行，性能相比线上CPU版有了几十倍的提升，通过InferenceProxy进行模型服务接入，能够精准的控制机器和流量配比。</p><p></p><p>通过InferenceProxy切流方案，我们能够将新的RD安全稳定的接入到在线服务中，承载TOP3模型流量，优雅解决了双十一大促过程中模型积压的问题。切流链路如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/0674032b8a8187b042ecc54333b0c0d6.png\" /></p><p></p><p></p><h1>六、未来展望</h1><p></p><p></p><p></p><h2>6.1 能力方面</h2><p></p><p></p><p>模型推理加速：完成Inference-kgb迁移RiskDetection GPU加速，并且持续打磨HighService及接入更多Backends，让模型接入更加简单方便，形成标准化接入流程；新增CPU通用加速流程，形成CPU标准化加速方案，并在ID模型上落地；调研PPU加速方案，尝试采用PPU解决在/近/离线资源瓶颈问题。</p><p>﻿</p><p>服务运维特性：完善核心三大服务及运维特性：Caching，Batching、Scaling；其中Batching、Scaling依赖Backends提供相应的能力，ID实现通用配置化管理。</p><p></p><p></p><h2>6.2 效率方面</h2><p></p><p></p><p>模型源数据管理：模型文件、镜像、配置进行标准化管理，在/近/离线复用同一套管理标准。</p><p>﻿</p><p>简化模型发布流程：借助已有平台标准化模型效果验证、配置、部署、回滚、灰度发布流程。</p><p></p><p></p><h2>6.3 质量方面</h2><p></p><p></p><p>代码质量：由于Drogo多Role部署不能直接按照Aone发布流程部署，需要手动在Drogo上部署，导致Aone上设置的CodeReview、单元测试等卡点无法直接生效。因此我们需要制定发布标准，并严格按照标准执行。</p><p>﻿</p><p>服务质量：标准化开发、测试流程，对模型服务质量、代码质量、服务质量提出明确要求，并遵守标准规范，不合规范不允许上线。完善系统监控，报警信息。</p><p></p><p></p><h2>6.4 成本方面</h2><p></p><p></p><p>提升资源利用率：通过CPU、GPU加速、Batching等功能提升系统性能，从而达到减低计算成本的作用。通过弹性缩扩容落地，更加合理使用资源提升整体资源使用效率。</p><p>﻿</p><p>对模型类型约束：相同类别的模型尽量复用同一套代码（如分类模型）限制模型类别，不过度限制模型数量。类别固定有利于模型更方便的复用，减低开发、运维成本。</p><p>﻿</p><p>降本增效：持续度量模型ROI，替换低ROI模型及策略。</p><p></p><p></p><h2>相关阅读</h2><p></p><p></p><p>[01] 云原生模型推理服务框架-KServe</p><p>https://www.kubeflow.org/docs/external-add-ons/kserve/kserve/</p><p></p><p>[02] NIVDIA模型推理服务框架-TritonServer</p><p>https://developer.nvidia.com/nvidia-triton-inference-server</p><p></p><p>[03] 机器学习平台PAI-EAS</p><p>https://www.aliyun.com/product/bigdata/learn?utm_content=se_1012296429</p><p></p><p>[04] 达摩院图片文字识别服务-读光</p><p>https://duguang.aliyun.com/experience</p><p></p><p>[05] NIVIDA GPU加速-Multi Process Service</p><p>https://docs.nvidia.com/deploy/mps/index.html</p><p></p><p>[06] 云原生大数据计算服务-MaxCompute</p><p>https://help.aliyun.com/product/27797.html</p><p></p><p>[07] 阿里中间件Metaq介绍</p><p>https://www.jianshu.com/p/9860a09153f1</p><p></p><p>[08] 阿里妈妈AI技术路线简介</p><p>https://baijiahao.baidu.com/s?id=1702157600438083362</p><p></p><p>[09] 阿里搜索资源池调度平台-Hippo</p><p>https://blog.csdn.net/hahachenchen789/article/details/80848925</p><p></p><p>[10] 阿里搜索调度管理平台-Drogo</p><p>https://www.pianshen.com/article/6481701214/</p><p></p><p>[11] 揭开阿里巴巴复杂任务资源混合调度技术面纱-ASI</p><p>http://gxsns.yfsoft.com.cn/5200.html</p><p></p><p>[12] 阿里巴巴开源分布式限流框架-Sentinel</p><p>https://sentinelguard.io/zh-cn/docs/introduction.html</p><p></p><p>[13] Dubbo 和 HSF 在阿里巴巴的实践</p><p>https://xie.infoq.cn/article/4d94b3eb548b2aed1962dd0ad</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/88/888dfa7a4ba608fe3cfb616c01e83244.png\" /></p><p></p>",
    "publish_time": "2022-12-29 15:07:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动Serverless 高密度部署与 Web-interoperable Runtime实践",
    "url": "https://www.infoq.cn/article/EjNwWZmWeblfp4epEszT",
    "summary": "<p>在 Serverless 的浪潮下，用户的运维成本越来越低，但调度的粒度还是不够细，高密度部署解决方案将函数部署粒度进一步拆分，直达容器内部，通过调度策略能够充分压榨服务器资源（CPU、内存等）。在此基础上搭配业界标准的 Web-interoperable Runtime，让用户的开发成本、运维成本进一步降低，同时提高资源利用率。</p><p></p><p>本文整理自字节跳动架构师朱凯迪（死月）在 <a href=\"https://archsummit.infoq.cn/2022/shenzhen/\">ArchSummit 全球架构师峰会（深圳站）</a>\"上的演讲分享，主题为“<a href=\"https://archsummit.infoq.cn/2022/shenzhen/presentation/4762\">Serverless 高密度部署与 Web-interoperable Runtime 在字节跳动的实践</a>\"”。分享主要分为四部分：1、Serverless ⾼密度部署；2、Web-interoperable Runtime；3、高密度部署在字节跳动的实践；4、未来展望。</p><p></p><p></p><h2>1 Serverless ⾼密度部署</h2><p></p><p></p><p>首先，我们先讲一下 Serverless 高密度部署。传统的 Serverless 有这么几个方向，一个是低门槛，然后有 Hostless、Stateless、弹性、分布式，以及事件驱动。</p><p></p><p></p><h3>传统 Serverless</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/39/39bc82d7aa6ecbed879c7d9307e2d494.png\" /></p><p></p><p>传统 Serverless 由于是事件驱动的，它的入口可以是 HTTP、消息队列或者定时任务，通过不同的 Triger 来触发函数，如果是 HTTP，则通过网关，如果是消息队列，则通过 MQ Consumer，最后分发到对应的函数 Instance。通常一个实例就是一个 Pod，Pod 里面通常会监听一个端口，传统 Serverless 的模型差不多就是这样的。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/47/47463901bb8e07ebc0ab0d3cfd036243.png\" /></p><p></p><p></p><h4>分层模型</h4><p></p><p></p><p>我们再把它变成一个分层的模式来看，整个流量大概就是这么一个心型的脉络。流量首先进入一个统一的网关，再由第二层的 FaaS 网关去负载均衡到不同的 Pod，也就是刚才提到的函数实例，我们把这种方式称为一种二层调度，分别是机房调度或者说是 IaaS 层的调度，然后是 K8s 层的调度，相当于容器这一层的调度。在 2 层调度中，每一层都有它自己对应的交付时间，或者说弹性的时间。它们对应的交付时间分别是天级和分钟级或者秒级的交付。</p><p></p><p>交付时间会影响到 Serverless 的冷启动时间，通常在我们的实践过程中，函数的冷启动时间是不确定的，可以是几十秒，或者几秒，或者几百毫秒，这个跟不同的应用场景、不同的业务、不同的基础服务相关。其中有很多影响因素，比如设备性能、函数运行时、网络延迟、代码包大小，或者镜像大小等。总结起来，就是约等于一个容器的冷启动时间，它又影响着 Serverless 特性中非常重要的几点：一个是 Hostless，就是我们不需要管服务器；一个是 Stateless，就是无状态；以及弹性，一个函数用完，我们可以直接装垃圾筒扔掉，随起随停，用完就丢。这是我们现在 Serverless 很多情况下的一种使用方式，或者说 Serverless 本意就是想让大家这么做。流量来了马上启动起来，弹起实例，实例化出相应的函数；流量处理完了就马上删掉。这个做法对函数的冷启动有非常高的要求，冷启动的优化是在 Serverless 中大家需要去探索的一个课题。</p><p></p><p></p><h3>高密度部署</h3><p></p><p></p><h4>冷启动优化</h4><p></p><p></p><p>冷启动的优化方式有很多。对于供应商来说，可以更换设备提高一些性能，或者提升它的整个网络架构。对于业务方来说，可以用更轻量级的运行时。比如，如果它原来用 Java 之类的，可以换成 Node，也可以去合理组织函数代码，让函数更轻量，类似于微服务的方式。但是业务方不管怎么优化，最终启动时间还是约等于容器的冷启动时间。这个时候就有一个问题了，既然冷启动速度约等于容器时间，有什么东西可以比容器更快的？答案是有的，比如说弹起一个进程的时间肯定要比弹起一个容器的时间快，当然也有可能还有其它的方式，比如进程直接起好了，我们用线程去跑。所以我们在高密度部署里面，就有了两种方案，分别是进程和线程。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/12/125a506485158e99d2dd5baceb179f64.png\" /></p><p></p><p>我们分别从四个维度对这两种方案做了比较，发现它们不同方面各有优劣。但我既要启动速度又要安全性，我们在进程的模式上加了一些黑魔法，让它的启动速度变得非常快，可以达到亚毫秒级别（≤ 1 毫秒）的启动。</p><p></p><p></p><h4>三层调度</h4><p></p><p></p><p>我们最终选了进程做高密度部署，它是一个三层调度，比之前的二层调度多了一层，就是在容器内部多了一个进程或者线程级别的调度，这个调度的交付时间可以做到非常快，到毫秒级甚至是亚毫秒级。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/45/45061a24755d5880534a1071fa2910af.png\" /></p><p></p><p>由于多了一层调度，之前的那个心型的脉络就变成了一个方块的脉络，流量从统一网关进到 FaaS 网关，再到 Pod，到了 Pod 之后，又多了一层节点，就是在容器内部又多了一个负载网关或者调度器，再流到不同的函数实例上去，这就是我们整个高密度部署的三层调度模式。</p><p></p><h3>Executable as an Image 与 Process as a Container</h3><p></p><p></p><p>业务方一般不想做运维的事，他们会觉得评估资源很烦，于是就怎么大怎么来，导致产生非常大的资源浪费。为什么业务方往往不愿意做这个评估？因为对于他们来说，估少了流量机器扛不住，如果要扩容，需要时间比较长，这个时候万一系统挂了就不好了，哪怕有熔断限流这些机制，部分用户不可用也是不好的。而高密度部署就可以帮他们解决问题，反正弹的快，可能一毫秒都不用就把函数弹起来了，流量来了马上弹起来，不用估直接用就好。这样就可以提高资源利用率。</p><p></p><p>弹性变高了，运维降低了，资源利用率也提高了，还会有什么问题呢？比如一个容器里面部署了多个函数，如果它们乱搞，最后一下子炸了，但没有做一些相应的资源限制和资源隔离，各函数之间可能就会相互影响。我们参考了云原生的 OCI 标准，提出 EaaI(Executable as an Image) 和 PaaC(Process as a Container)，相当于跟 Docker 镜像和 Docker 容器对标，把可执行文件看成是镜像，把进程看成是容器。在启动的时候，可以理解为原来部署一个 Docker 容器，现在变成部署一个高密度的容器，高密度容器就是 PaaC，对应使用的二进制就是一个镜像，就是 EaaI。本质上它们就是一个可执行文件和进程，只不过我们在这中间加上了 OCI 的规范。开源社区有一个叫 RunC 的容器运行时，在 K8s 体系中容器可以通过 RunC 去启动，但是 RunC 只是用于启动容器的，并不能启动高密度容器。我们最后自研了一个叫 iku 的容器运行时，它是遵循 OCI 规范的容器运行时，用于启动高密度容器。而且我们的高密度容器也通过它做了一些资源的限制以及资源的隔离，就是规定一个容器只能用多少 CPU、多少内存，这个跟容器的限制是一样的。</p><p></p><h2>2 Web-interoperable Runtime</h2><p></p><p></p><p>Web-interoperable Runtime（Winter）是我们用于搭配高密度部署而研发的新函数运行时。首先解释一下，什么叫 Web-interoperable Runtime，它在以前有一个更糙且不严谨的名字，叫做 V8 Worker。因为好多厂商都基于 V8 做了 JavaScript 的运行时，但是后来经过标准化规范化之后，国际上的几家厂商就一起给它起了一个新的名字，并且开始做一些标准化的事情，组建了一个组织叫做 WinterCG（Web-interoperable Runtimes Community Group），它是由几家国际公司联合起来搞的 W3C 下的一个社区组，致力于做 Web-interoperable Runtime 标准化。</p><p></p><p>这里有一个核心的单词叫 interoperable，就是互通性，什么是互通性？就是运行时之间是可以互相替代、互相兼容的。各种浏览器之间就是 interoperable，经过标准化之后，大家 API 长的都一样，这就是所谓的互通性。Node.js 几个大版本也是开始往 Winter 上靠，市面上的 Winter 除了 Node.js 之外还有 Deno、CloudFlare Workers、Oxygen + Hydrogen 等等。我们自己也有一套自研的 Winter 运行时，叫 Hourai.js，它是用于我们的高密度部署的。</p><p></p><p>Winter 相较于 Node.js 来说，有一些很好的优势，比如低门槛，因为写 JS 的前端开发者们更熟悉浏览器 API。选择 Node.js 你要自己实现一个服务器，你要监听端口，自己去实现整个 HTTP 服务器，除此之外，你还要搞它的 PM2、运维、部署，等等。如果上了 Winter 就简单了，我们不需要监听端口，只需要监听 Fetch 事件，之后直接把它上到高密度部署，其他什么事都不需要管，它直接会触发事件，我们只要写里面的逻辑就可以了。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c5/c59de3c208c989f709dd97c0009d7d8e.png\" /></p><p></p><p>回顾一下，高密度部署是希望可以解决随起随停、用完就丢的问题，主打的是轻量以及启动速度快。Node 冷启动大概需要两百毫秒，那么 Serverless 函数至少要两百毫秒往上了，我们当然也可以通过池化，或者 snapshot 之类的来进行优化，但是为什么这么麻烦，为什么不直接一点？我们 iku 里面就提供了一个 ASSS 能力，即 Active Safety Strip Snapshot，专攻启动速度，最终达到了小于一毫秒。</p><p></p><p></p><h2>3 高密度部署在字节跳动的实践</h2><p></p><p></p><p>我们有一组实践数据，某服务迁移到线程级高密度部署，CPU 核数从原来 287 降低到 24，内存从原来 574G 降低到 39G。所以，高密度部署就约等于更高的资源利用率 + 更快的调度速度 + 更低的运维成本。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/48/48567b92131a64cd500bc8bd457c4acd.png\" /></p><p></p><p>我们也做了其它的一些事情，比如往云原生上去靠，实现了 OpenTelemetry 这一套 API，Hourai.js 通过可观测性相关 API 传到它的 Agent，再到 Collector，最后再传到我们内部的一些系统，这就是整个 API 链路。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4e/4eb117b6754a425f52167416d7692a98.png\" /></p><p></p><p>在多运行时调用上面，我们也有一套实践，就是遵循了 Dapr，在拿到 Dapr 的实例之后，直接 Invoke，把对应的参数传进去，不需要管多运行时的那些 SDK，这个就是云原生给我们带来的好处，大家既然上了 Serverless 干脆就全套上云原生。</p><p></p><p>我们还针对不同的场景做了抽象，它可以根据不同的业务需求来进行扩展，对上可以去接不同的网关层，对下可以接到不同的 IaaS 层，比如说私有化部署，它可能没有字节内部的 FaaS 环境，但很方便就能把我们的高密度部署部到类似于 ECS 的环境。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/79/792a318ee9c3aa7c167ec2fab58e2d36.png\" /></p><p></p><p>Serverless 还有一个非常合适的场景，就是流程编排。Serverless 流程编排是流程里面不同的 Function，一个一个节点流过去，它的高密度部署相较于传统的来说更适合这个方式，就是一个函数流过去，马上就启动这个函数，用完马上就丢，非常适合流程编排。因为高密度部署就是更高的资源利用率，更快的调用速度，以及更低的运维成本。因为它弹的更快，流程编排内容非常细粒度的东西就非常好上；然后更低运维成本对于编排的用户来说，关心的就是只要写下一小段代码片段就好了；更高的资源利用率也是毋庸置疑的。</p><p></p><p>关于 APP 网站，首屏速度非常重要，因为首屏时长的长短等同于一个用户的转化率，如何优化首屏时间也是一个亘古不变的课题。我们目前正在跟字节跳动的前端框架团队合作，就是用 Modern.JS，加上边缘机房，加上高密度部署以及 Winter，整理出了一套边缘上的高密度部署渲染方案，以此来提高首屏渲染的速度。</p><p></p><h2>4 展望未来</h2><p></p><p></p><p>未来在高密度部署方面，我们可以让 K8s 直接穿透我们的一个调度，穿透到容器内部来。因为我们也是遵循 OCI 标准的，理论上 K8s 可以直接调进来，不用先调到容器，容器再调进来，它可以直接穿透进来，启动我们的高密度容器，甚至，它可以抛弃容器，直接部署到物理机上，作为高密度容器让 K8s 直接调度，甚至可以替换掉 K8s，直接由我们的 iku 做整个调度。</p><p></p><p>在 Winter 这一方面，我们要做一个更极致的 ASSS 能力，以及分布式的极速启动能力，我们现在的 ASSS 能力都是以单机为单位的。另外，我们可以与社区一起去推进 WinterCG 的发展，往后会对外进行 ToB，或者之后有可能会去做一些开源的事情。</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/8f68c7fabb67aa4ba8163c561\">如何用 7 分钟击破 Serverless 落地难点？</a>\"</p><p></p><p><a href=\"https://xie.infoq.cn/article/d6b5fb50ecfc4a41d8e1a6bc5\">Serverless Devs 重大更新，基于 Serverless 架构的 CI/CD 框架：Serverless-cd</a>\"</p><p></p><p><a href=\"https://xie.infoq.cn/article/5724f1c2c5f448b15339eade2\">应用 Serverless 化，让业务开发心无旁骛</a>\"</p><p></p><p><a href=\"https://xie.infoq.cn/article/5c76ea183f1b079f5077ba46d\">Serverless 的前世今生</a>\"</p>",
    "publish_time": "2022-12-29 15:31:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "openGauss正式推出资源池化架构，全新升级社区工具DataKit",
    "url": "https://www.infoq.cn/article/LVCDELhUubqzTX0Sz9Mv",
    "summary": "<p>12月29日，以“汇聚数据库创新力量，加速企业数字化转型”为主题的openGauss Summit 2022在线上举行。会上，<a href=\"https://www.infoq.cn/article/Dpry0A2vaOQiSRxh98E5\">openGauss社区</a>\"理事会理事长江大勇对外公布了最近社区及生态进展，并宣布推出资源池化架构，实现软硬融合，产业链全栈创新；openGauss社区全新升级工具DataKit，帮助用户从使用数据库到实现数据全生命周期的管理；基于openGauss的中国移动磐维数据库正式发布；openGauss社区理事会升级等重磅内容。</p><p>&nbsp;</p><p>据介绍，2022年，openGauss社区用户数首次突破100万，社区贡献者超4000人，共有220家企业、组织加入社区。在行业应用方面，联合伙伴累计完成500+个行业解决方案适配，覆盖政府、金融、运营商等10余个行业，在中国非云集中式数据库市场中占有率突破10%，累计使用超过3万套。</p><p>&nbsp;</p><p>当前，随着数据规模爆炸性增长、数据应用快速深化，数字技术正加速落地行业应用，产业数字化加速产生了海量的数据，图、流、时序和地理空间等多种数据类型。支持多样性计算，同时高性能处理、多模融合成为数字时代数据库架构发展的重要方向。为此，openGauss推出资源池化架构，通过一体化平台实现智能运维、集群管理、资源编排等能力，实现数据库在高负载下的稳定运行和弹性伸缩。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/jtvpKsViOAlakMmrU6DB\">openGauss</a>\"资源池化架构由3层池化（存储池化、内存池化和计算池化）、1个平台和1个标准组成。</p><p></p><p>存储池化支持多种存储，实现一份数据服务于多种计算并通过SQL算子卸载的NDP技术，大幅提升SQL处理效率、消减网络I/O流量。内存池化实现计算节点间内存的互联，通过同步事务信息和数据库缓存，实现多节点下的多版本快照一致性读能力。计算池化支持多样性算力，基于鲲鹏、昇腾等算力，为应用提供从TP行存加速、AP列存加速、AI训练推理等全方位的数据服务。</p><p>&nbsp;</p><p>通过灵活的架构与底层根技术突破，openGauss真正实现软硬协同，全栈融合、敏捷创新。</p><p>&nbsp;</p><p>针对行业应用过程中的业务建模、开发、管理、安装部署和迁移运维openGauss社区联合伙伴开发了大量工具，帮助用户从使用数据库到实现数据全生命周期的管理。会上，openGauss社区全新升级社区工具DataKit，其集成目前openGauss所有相关工具，形成工具超市，开发和运维人员可快速按需找到所需工具，方便快捷。</p><p>&nbsp;</p><p>会上还正式发布了基于openGauss的中国移动磐维数据库。中国移动信息科技公司联合openGauss社区、北京分公司、浙江分公司和广东分公司成功推出中国移动磐维数据库，其是中移信息首个基于中国本土开源<a href=\"https://www.infoq.cn/article/2EXtpsq5tlde6IvNEMYp\">数据库</a>\"打造的面向ICT基础设施的自有数据库产品。目前己在北京移动、河北移动和浙江移动的基础通信网络中部署应用，确保5G高质量通信服务。</p>",
    "publish_time": "2022-12-29 16:28:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "StackOverflow 最新调查：使用 Linux的开发者多于 Mac、JavaScript 依然最流行",
    "url": "https://www.infoq.cn/article/RQ0eA1QYi5GgfEkScYH0",
    "summary": "<p><a href=\"https://survey.stackoverflow.co/2022/#technology\">2022 年 StackOverflow 开发者调查</a>\"显示，使用 Linux的开发者多于 Mac。尽管 Windows 仍然是开发人员最常用的平台，但总体上并没有大家想象得那么多。Microsoft Windows 是参与调查的开发人员中使用最广泛的操作系统。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/43/4301edaae107c91244688dacd6084d2e.png\" /></p><p></p><p></p><h4>最流行的10个编程语言：</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/65/655ac525acd73b2885619a8ac2281042.png\" /></p><p></p><p>这是 JavaScript 连续第十年成为最流行的编程语言。此外，<a href=\"https://www.infoq.cn/article/1g3Yeu0kX7UXhV3R9E0P\">Rust</a>\" 被加冕为最受喜爱的编程语言，而 MATLAB 则成了最令人恐惧的语言。此外，Rust 还在开发人员明年最想使用的语言排行榜中名列前茅。</p><p></p><h4>最流行的10个云平台：</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/1664ec3ef2c6c7e2ddcb1fee0d8ec1e5.png\" /></p><p></p><h4>最流行的10个数据库：</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4318154c4d733d92931bf6d84bebb28.png\" /></p><p></p><p></p><h4>&nbsp;最流行的10个Web框架：</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4be70d4621222d3aa3a602ed1069f9e5.png\" /></p><p></p><p></p><h4>&nbsp;最流行的10个IDE：</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d95e733c9c99b7d3913dca715370bab2.png\" /></p><p></p><p>&nbsp;</p><p>查看更多信息：</p><p><a href=\"https://survey.stackoverflow.co/2022/#most-popular-technologies-language-learn\">https://survey.stackoverflow.co/2022/#most-popular-technologies-language-learn</a>\"</p><p></p><p></p><p></p><p></p><p></p>",
    "publish_time": "2022-12-29 17:53:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]