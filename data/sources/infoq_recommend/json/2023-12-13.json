[
  {
    "title": "一站式数据库上云迁移、同步与集成平台 DTS 的设计实践",
    "url": "https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG",
    "summary": "<p></p><h2>一、数据库迁移面临的挑战</h2><p></p><p></p><p>根据大数据技术标准推进委员会今年 7 月发布的《数据库发展研究报告（2023 年）》，我们可以看到，去年国内的数据库市场规模约为 400 亿，今年预计可以达到 540 亿，预计到 2027 年，国内数据库市场规模可达 1280 亿。未来几年复合增长率预期可以达到 26% ，市场潜力非常大。</p><p></p><p>进一步看市场结构，我们可以发现一个明显的趋势：公有云在国内数据库市场中开始逐渐占据主导地位。近三年，国内公有云数据库市场规模的增速在 50% 左右，远高于本地部署市场的 15%。预计今年公有云数据库整体占比可达到 60%。</p><p></p><p>相较于本地部署的方式，公有云数据库通过全托管服务、云原生数据库等形态，在弹性、成本、易用性上更加具有优势。所以在过去十年，越来越多的企业客户选择数据库上云。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d6d97f6ade3ca5770a0aaf8cb7d0ed9.png\" /></p><p></p><p>数据库上云面临的技术挑战非常多，主要体现在以下四个方面：</p><p></p><p>第一，数据库上云的选型。企业需要考虑在云上数据库使用的引擎、选用单机还是分布式的架构、以及选用何种套餐，同时还要结合业务特点对数据库做针对性调优。如果遇到异构迁移或者跨版本迁移，还需要评估结构对象的兼容性，给出业务 SQL 的改造方案。</p><p></p><p>第二，云上迁移流程较长。</p><p></p><p>首先，我们需要打通云上云下的网络，把数据库的账号/角色、结构、存量、增量数据完整地搬迁上云。然后，对两端的数据一致性做校验。在一致性校验通过后，业务方把流量从云下割接到云上。最后，需要把云上数据库的增量写入内容反向同步回云下环境，以保留云下环境用于迁移后的灾备。</p><p></p><p>第三，迁移过程中的效率和容灾。迁移过程对业务的影响要尽可能少，迁移链路本身也应当具备容灾能力。</p><p></p><p>第四，迁移的数据一致性保障。数据库承接的往往是在线服务，少一条数据都可能给业务带来严重影响。因此，迁移链路自身要保证两端数据的最终一致性，同时也要提供校验工具用于检查确认两端实例的数据一致性。</p><p></p><p>百度智能云在多年的数据库上云迁移实践中积累了丰富的经验。我们认为，在数据库上云迁移中，平滑和可靠是客户的核心诉求，也是对迁移服务的必然要求。</p><p></p><p>平滑主要体现在易用性、兼容性和业务影响上。</p><p></p><p>易用性：迁移服务要开箱即用，能够托管迁移全流程。兼容性：能够兼容源和目标不同引擎、不同架构、不同版本和不同网络环境，尽可能地降低业务改造成本。业务影响：支持账号、结构、存量、增量的不停服迁移上云，将业务影响降到最小。</p><p></p><p>可靠主要体现在一致性、可回滚和高可用上。</p><p></p><p>一致性：保证源和目标的数据一致性，并提供校验能力。可回滚：支持割接后的反向回滚同步，保留云下环境用于灾备回滚。高可用：迁移和回滚链路要具备故障恢复能力，尤其是当上下游数据库发生主从切换后，迁移链路要具备自愈能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d0ab9d6458db6ad5ca32d004408d9c1.png\" /></p><p></p><p>在数据库上云后，我们看到客户在使用数据库时仍然有很多数据传输的新场景，其中有三个典型场景：</p><p></p><p>异地多活：当客户的服务部署在全球多个机房时，机房间的通信延迟最长可达秒级。如果数据库架构依然是单点写入，请求耗时就会变得非常高。但如果拆分数据库，则会牺牲数据的全局一致性，不满足业务需求。较为理想的架构应当是每个地域的数据库本地读写，然后通过数据同步工具同步至异地节点，最终实现数据全局一致。多云灾备：近几年来，基于服务可用性的考量，越来越多的客户选择多云部署。在生产云出现故障时，客户可以将流量切到灾备云上，以保证服务无损。数据库是有状态服务，因此需要数据同步工具将生产云的实时增量写入同步到灾备云上，以保证两端数据一致，满足灾备需求。数据集成：对用户行为的学习、分析和推理可以帮助企业快速决策，并进一步为用户提供个性化服务。通过数据集成工具，企业可以将数据从生产域实时准确地集成到分析域，从而实现数据深层价值的挖掘。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/998f41c889a3a701908cc51723f629ef.png\" /></p><p></p><p>基于上述的客户需求，百度智能云推出了一站式数据库上云迁移、同步与集成平台 DTS。</p><p></p><p>在上云迁移方向，<a href=\"https://www.infoq.cn/video/Vi9e7DGG3wBMVJQF3wBW?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">DTS</a>\" 基于百度多年实践总结出一套成熟的数据库上云迁移方案，围绕该方案提供一站式上云迁移体验。</p><p></p><p>在同步/集成方向，DTS 聚焦业务场景，基于场景需求的关键特性打磨产品，提供易用稳定的一站式同步/集成服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f9bfad0f6907f5ade3347eec251bb357.png\" /></p><p></p><p></p><h2>二、上云迁移方案</h2><p></p><p></p><p>数据库上云迁移是一个复杂的系统性工程，需要客户和云服务商共同配合完成。</p><p></p><p>我们将上云迁移分为三个阶段：迁移前、迁移中、迁移后。</p><p></p><p>迁移前的工作主要是做数据库选型和迁移可行性的评估。数据库选型的维度包括：产品、套餐、架构和存储介质等。选型的过程往往需要对相关的云上数据库产品做功能和性能测试以验证是否满足业务需求。迁移评估则是检查待执行迁移的源端和目标端数据库实例及其宿主和网络环境，得出迁移的可行性结论。在完成了迁移前的选型和评估工作后，就是数据库上云迁移过程。DTS 支持将源端的账号/角色、结构对象、存量数据、增量写入迁移至目标端，迁移过程中无需客户停服。在增量延迟追平后，DTS 支持对两端数据一致性做校验。通过一致性校验后，客户可以将业务流量割接到云上。在验证业务的同时，客户可以使用 DTS 的一键反向功能，快速拉起云上同步回云下的反向回滚链路，将云上的流量反向同步回云下，保留云下环境用于迁移后的灾备。数据库迁移到云上后，<a href=\"https://www.infoq.cn/article/SGPHdt4a0GyPUVyOotSm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">百度智能云</a>\"也提供了<a href=\"https://xie.infoq.cn/article/28475b63dde77ae8aace1af29?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">数据库智能驾驶舱（DSC）</a>\"帮助客户管理、审计和调优云上数据库。</p><p></p><p>下方全景图中标蓝的步骤由 DTS 提供支持，我们可以简单总结为四步：评估、迁移、校验、回滚，下面我将详细介绍每个步骤的方案设计。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/becbd89b490e58c3acdeee360ae0d928.png\" /></p><p></p><p></p><h4>2.1&nbsp; &nbsp; 迁移评估与网络接入</h4><p></p><p></p><p>迁移前的第一个准备工作是迁移评估。</p><p></p><p>DTS 迁移任务在启动迁移前，会先执行前置检查，包括检查迁移对象、数据兼容性、两端数据库配置等，最终输出迁移可行性评估结论。对于不通过的检查项会给出修复建议。</p><p></p><p>此外，DTS 还提供了本地迁移评估工具，支持在本地环境执行，支持对多个实例执行批量评估。</p><p></p><p>迁移前的另一个准备工作是网络接入。</p><p></p><p>当前，DTS 支持通过公网、专线、VPN、云自建、云服务等方式接入，通过控制台或 OPENAPI 一键完成网络接入，无需人工部署。</p><p></p><p>此外，对于上云迁移高频依赖的专线或 VPN 接入，DTS 进一步优化了网络接入方式，支持客户将数据库内网域名作为任务端点，保证迁移链路具备切换自愈能力。当云下数据库实例发生主从切换时，只要实例域名保持不变，DTS 任务就可以快速自愈恢复。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8ffcb367295511dea50186ace04e102d.png\" /></p><p></p><p></p><h4>2.2&nbsp; &nbsp; 数据迁移原理</h4><p></p><p></p><p>下面介绍 DTS 数据面的数据迁移原理。</p><p></p><p>数据面整体遵循 ETL 插件式设计，支持不同插件的自由组合，以满足不同数据流的迁移需求。</p><p></p><p>数据抽取（E）：通过不同的数据抽取插件，DTS 数据面可以支持采集账号/结构、全量、增量等数据。其中，全量数据抽取由于数据量较大，因此我们通过并发抽取、大表分片等优化手段进一步提升整体吞吐。增量数据则基于 CDC 异步捕获变更，可以让源端负载更少，同时传输实时性更好。性能的相关优化我们在后面会具体介绍。数据转化（T）：完成数据抽取后，源端的原始数据会被归一化为统一的抽象数据结构，这样就实现了异构数据源上下游解耦和端到端自由组合；然后再由数据转化插件对抽象数据结构做数据加工（如：库/表/行/列的过滤和映射）；最后再将数据格式改写为目标端数据源支持的协议。数据加载（L）：数据加载插件将完成协议转换的数据并行批量加载到目标端数据源中。为了进一步提升加载性能，数据面往往通过多线程并行加载数据。但增量同步需保证数据加载的时序与源端严格一致。因此，DTS 支持按表或主键粒度并行分发，属于同一个表或主键的数据会被分配到同一个加载线程串行执行，不同表或主键的数据则可能分配到不同线程上，在保证时序的前提下进一步提升了性能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b33b7c85d71aa44bdd070a94ceda3c4c.png\" /></p><p></p><p></p><h4>2.3&nbsp; &nbsp; 数据一致性校验</h4><p></p><p></p><p>整体的数据迁移流程遵循先结构、再全量、后增量的顺序。</p><p></p><p>考虑到各类数据库中的 CDC 日志通常是易失的，如：MySQL 的 binlog、MongoDB 的 oplog、Redis 的 backlog 等。数据库往往通过固定缓冲区、定时或限制容量清理等方式限制 CDC 日志的存储用量。</p><p></p><p>DTS 针对该问题优化了迁移流程编排。在进入全量迁移过程后，DTS 除了导出和加载全量数据外，还同时导出增量数据，将其缓存到 DTS 的内部存储中，以避免尚未迁移的增量数据被源端数据库清理。待进入增量迁移阶段后，再将缓存的增量数据加载到目标端。</p><p></p><p>增量同步延迟追平后，即可执行数据校验检查两端数据一致性。由于 DTS 增量同步是异步加载，因此源端和目标端的数据版本实际上存在毫秒级的延迟。因此数据校验可能会因为同步延迟出现误报。我们引入了 X Round Recheck 的方案进一步降低了误报概率。在后面的内容里会专门介绍数据校验的原理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b27c290770883a240527852a41e692be.png\" /></p><p></p><p>DTS 以数据不丢为基础，进一步实现了数据的不丢不重，以保障数据的一致性。</p><p></p><p>数据不丢（At-Least-Once）：依赖于 DTS 数据面的低水位进度管理机制。如下图所示，每一条数据都会关联一个单调递增的版本号，这些版本号组成了一个单调递增的进度序列。当某条数据写入下游并收到了确认写入成功的响应后，该数据对应的版本号会被标记并在进度序列中更新状态。此时，进度管理线程会检查进度序列的水位。</p><p></p><p>在下图中 1、2、3、4 都已经被标记，但 5 尚未标记，因此版本号 4 是低水位里的最大版本号。所以将 4 作为最新进度保存到外部存储中。一旦此时任务容灾恢复，恢复后的任务将以外部存储中记录的最新进度 4 作为断点重新执行迁移。</p><p></p><p>At-Least-Once 机制可以保证数据不丢，但无法保证数据不重。我们在下图中可以看到，6、7 此时都已写入下游，但并未记录到最新进度中，一旦任务以 4 作为断点重新执行，则 6、7 对应的数据会重复迁移。</p><p></p><p>为了实现数据不丢不重（Exactly-Once），DTS 的思路是基于目标端数据库特性去重。对于关系型/文档数据库、数仓来说，表中主键列或唯一键列具有唯一性，因此，我们可以改造 SQL。利用唯一约束实现幂等写入。</p><p></p><p>而对于 Schema-less 的消息队列、分布式文件系统等，DTS 会在投递的消息中加入 UUID，目标端消费方可以基于 UUID 自行去重。</p><p></p><p>最后对于键值数据库（如 Redis），DTS 的思路是将不满足幂等写入的命令改写为满足幂等性，比如累加、累减、插入队列等改为覆盖写。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc31ca309acf6adbab7374c6ecc0f593.png\" /></p><p></p><p>除了迁移系统本身的一致性保证外，DTS 还提供了独立于迁移系统的数据校验功能。数据校验与数据迁移由不同的任务独立执行，以保证校验结果的可信。</p><p></p><p>数据校验的流程可以拆解为：抽取、转化和校验。</p><p></p><p>其中，抽取与转化的实现原理与数据迁移的对应模块实现类似，这里不再赘述。下面重点介绍一下数据校验插件的原理。</p><p></p><p>首先，校验插件在收到待校验数据后会根据主键或唯一键实时查询源端和目标端最新的数据。然后，根据数据加工规则对源端和目标端的数据做归一化，对齐数据元信息。最后，根据不同的任务配置，比对规则校验数据一致性，并将校验结果保存到外部存储中。</p><p></p><p>这套流程在实践中存在小概率误报，原因是在执行数据校验的同时，源端还在持续写入。因此，源端与目标端的数据版本会存在毫秒级的同步延迟，正是这一延迟导致了少量数据的校验误报。</p><p></p><p>因此，DTS 引入了 X Round Recheck 机制。在数据不一致时，会等待一段时间后进入下一轮比对，重新读源端和目标端的数据后再次比对。只有当多次重复比对均不一致的数据会被记录为不一致数据上报。</p><p></p><p>X Round Recheck 大大降低了数据校验的误报频率。经测试，校验误报频率从约千分之一下降到小于百万分之一。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a0c5e65544c682c90c921ee21696f29e.png\" /></p><p></p><p></p><h4>2.4&nbsp; &nbsp; 反向回滚</h4><p></p><p></p><p>在完成数据一致性校验后，客户就可以把流量割接到云上了。</p><p></p><p>完成切流后，我们观察到客户往往需要保留云下环境用于容灾回滚。当云上生产环境不可用时，可以将流量快速切回云下灾备环境，服务快速恢复。针对这一痛点，DTS 推出了一键反向功能，支持在流量割接后快速拉起反向回滚任务，将云上流量反向同步回云下。</p><p></p><p>如下图所示，客户在 T1 时刻执行流量割接，此时正向迁移任务运行中，而反向回滚任务处于挂起状态。在 T2 时刻，客户完成割接，此时业务流量已切到目标端数据库，此时执行一键反向，DTS 会将正向迁移任务挂起，反向回滚任务启动，从客户指定的 T1 时刻开始将目标端的业务写入实时同步回源端。从而完成正向迁移到反向回滚的切换。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b36fcc5a27470d0d1987df2f43d4cfae.png\" /></p><p></p><p></p><h2>三、同步/集成方案设计</h2><p></p><p></p><p>我们在前面介绍了同步/集成方向的三个典型业务场景：异地多活、多云灾备和数据集成。其中，高可用和高性能是这些场景共同需要的关键能力。</p><p></p><p>异地多活和多云灾备属于在线数据库的实时同步需求。因此支持数据库多主架构和数据一致性的有保证/可校验能力是同步场景的核心痛点。数据集成场景的痛点在于，传统的集成方式中，流批架构不统一、不同数据源使用的集成工具比较繁杂，ELT + ODS 的数据预处理方案实时性差/复杂度高等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/ca50070612eff19bdbb3383b49c373f2.png\" /></p><p></p><p></p><h4>3.1&nbsp; &nbsp;&nbsp;DTS 的高可用和高性能</h4><p></p><p></p><p>接下来，我们分别介绍 DTS 针对上述痛点的方案设计。</p><p></p><p>首先介绍高可用能力。</p><p></p><p>DTS 的高可用设计目标是满足长期不间断的生产级数据同步需求。方案设计可分为三个方面：断点续传、实时容灾和数据库切换自愈。</p><p></p><p>断点续传。DTS 会将传输进度做定期 checkpoint 并持久化到外部存储中。目前 DTS 大部分数据流的全量和增量迁移都支持断点续传。同时，DTS 基于之前介绍的低水位进度管理机制，可以保证断点续传前后数据不丢失，基于目标端的唯一约束可以实现数据不丢不重。实时容灾。DTS 数据面模块支持故障秒级接管和恢复。并且支持对极端脑裂场景的自动检测和恢复，保证脑裂恢复前后的数据最终一致性。切换自愈。DTS&nbsp;支持在源端、目标端数据库实例发生故障切换时，自动发现新的可用节点。当不同节点的传输位点发生变化时，DTS 可以基于时间自动定位新节点的传输位点，保证切换后数据不丢。</p><p></p><p>经过百度智能云内外部数据传输场景的长期实践打磨，DTS 承诺的任务可用性为 4 个 9。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/702ab336cc15fa05036c699b186247ae.png\" /></p><p></p><p>接下来让我们再看高性能。DTS 在性能方向的设计目标是追求高吞吐和低延迟。</p><p></p><p>首先是高吞吐，DTS 具有如下的特点：</p><p></p><p>DTS 支持预读取全量数据。当遇到大表时，DTS 支持将大表分片并行读取，解决大表长尾的问题。DTS 支持按照表、主键粒度并行转换和回放。DTS 对数据回放的单线程写入性能做了优化。比如当写入 1000 条数据时， DTS 将 1000 条 INSERT 合并为一条 INSERT，提升了目标端数据库的 SQL 写入效率。写入语句批量执行，网络延迟均摊。</p><p></p><p>其次是低延迟，DTS 提供了如下的能力：</p><p></p><p>DTS 自身基于 CDC 实现增量数据捕获，无需扫表，实时性更好。DTS 采用了数据流式传输模型，全程流水线作业。DTS 选用消息队列缓存和回放增量数据，端到端的同步延迟更低。DTS 针对热点数据，支持基于逻辑的事务合并。在保证最终一致性的前提下，压缩了同步的数据量。</p><p></p><p>以 MySQL 同步为例，全量吞吐峰值约为 20W 行/s，增量吞吐峰值约为 1W 行/s，延迟可以达到毫秒级别。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8765256df2e3f29e51c56a51c9165ef.png\" /></p><p></p><p></p><h4>3.2 &nbsp;&nbsp;异地多活场景</h4><p></p><p></p><p>异地多活场景的核心特性是双向同步，可以支持两端数据库写入相互同步，从而实现数据库多主架构。</p><p></p><p>双向同步由正向和反向两个 DTS 同步任务实现，每个任务在同步数据时会通过加入特定的 DML 将该数据所在的事务染色；而另一方向的同步任务在读到染色事务时会直接过滤，从而避免了数据的同步回环。此外，双向同步支持级联，客户可以通过搭建多条双向同步链路实现 N 个地域的数据库多主架构。</p><p></p><p>不过双向同步的使用也有一定限制：</p><p></p><p>业务需避免在两端同时变更主键/唯一键相同的行记录，尤其是避免同时执行 UPDATE，否则可能产生冲突，造成数据不一致。因此，我们推荐业务层面支持流量单元化。表中不能使用自增主键，这是为了避免主键冲突导致的数据不一致。仅有正向同步任务支持同步 DDL，反向同步任务仅同步 DML，因此若需执行 DDL 建议在正向同步任务的源端执行。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d321d9451ff5a40360143c3d278be810.png\" /></p><p></p><p></p><h4>3.3&nbsp; &nbsp;数据集成场景</h4><p></p><p></p><p>经典的 Lambda 架构包含定时和实时两套架构，分别处理流和批两种不同的数据。架构不统一会带来运维迭代成本高、流批产出数据不一致等问题，所以现在业界都在逐渐转向流批一体。</p><p></p><p>DTS 的架构天然支持流批一体，源端无论是有界数据（数据库快照，指定区间的增量）还是无界数据（持续的数据库流量），都会通过数据切片的方式切分为无数个 Micro-Slice，通过流水线作业最终同步到目标端的仓、湖或流式计算框架。</p><p></p><p>目前 DTS 目标端支持 Doris、Elasticsearch 等数仓，以及百度智能云数据湖 EDAP，支持通过消息队列将数据推送到 Flink 等流式计算框架中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5b0dd52a21c254741c04835648a0817.png\" /></p><p></p><p>在面对上下游异构数据源时，解耦上下游的架构设计能够为系统提供更高的灵活性和可扩展性。这种设计使得 DTS 能够支持端到端的任意组合，组合复杂度从 M*N 降低到 M+N，并能够快速扩展支持新的数据源。</p><p></p><p>DTS 定义了抽象数据格式 DTS Record，可以将源端各类数据库的数据转换为标准的 DTS Record（Any To One），然后再将 DTS Record 转换为目标端数据源接受的数据格式（One To Any）。</p><p></p><p>当 DTS 需要接入新的数据源时，只需要定义新数据源到 DTS Record 的转换规则，即可快速支持现有全部数据源到新数据源的异构数据传输。</p><p></p><p>当然，在异构字段映射的过程中，部分数据可能会因为浮点数精度/字符集不同造成数据精度损失。因此，DTS 优化了同构字段映射规则。当上下游数据源同构时，源端数据能够无损映射到目标端。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac26b104b1e719ebc98991747eb8865b.png\" /></p><p></p><p>接下来我们看下数据集成的预处理环节。</p><p></p><p>当前业界的主流集成架构是 ELT+ODS。即将数据通过 Sqoop、Spark 等工具，几乎不做 join 或 group 等复杂转化，直接抽取到数据仓库里的贴源层（ODS），再在数据仓库中通过 SQL/H-SQL，将数据从贴源层（ODS）加载到数据明细层（DWD），最终汇总到数据汇总层（DWS）和数据集市（DM）。</p><p></p><p>ELT+ODS 架构的思路是利用数仓的 MPP 高性能计算做 ODS 到 DWD 的大数据预处理。但这仅适用于数据源模式比较简单的情况。当 ODS 到 DWD 规范化复杂度比较高时，往往需要引入 Spark/MapReduce 等框架专门处理。</p><p></p><p>另外，ELT+ODS 架构的实时性较差，难以满足实时分析场景和即时查询的需求。ODS 与 DWD 的数据重复率也比较高，需要付出额外的存储成本。</p><p></p><p>DTS 基于业界最新提出的 EtLT 架构推出了支持实时数据加工的集成方案。它可以将数据从在线域直接集成到 DWD，在集成阶段即可完成实时的数据规范化，无需维护额外的 ODS 层。EtLT 架构的实时性要优于 ELT+ODS 架构，可以支持实时的数据分析和查询统计，让复杂的数据抽取、规范化和加载的过程对数据分析过程透明，帮助其更加聚焦业务。</p><p></p><p>DTS 支持实时数据加工的集成方案预计会在 2024 H1 开放公测，大家可以期待一下。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3bfb46f6ff54c44e7d6fac3968914c71.png\" /></p><p></p><p></p><h2>四、DTS 落地实践案例</h2><p></p><p></p><p>第一个案例是某国内大型在线视频服务公司，DTS 支持了该客户的数据库上云迁移和多活同步的需求。</p><p></p><p>该客户的业务痛点主要包括三个方面：</p><p></p><p>迁移规模大：在线服务数据库（MySQL/Redis/MongoDB）中，涉及到上百条业务线的 1.5W+ 集群迁移上云，过程管理难度大。高可用需求高：需要支持数据库不停服迁移、支持切换自愈、支持反向回滚、支持监控指标推送。异地多活：需要支持跨地域多活同步（单元化）、低延迟（&lt;100ms）。</p><p></p><p>针对该客户的三个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>客户业务使用自助上云平台完成上云迁移：平台集成 DTS 服务。DTS 迁移全流程（评估、迁移、校验、回滚）100% 接口化（支持控制台操作），无人工干预。客户 IDC 自建实例切换自愈：DTS 支持专线/ VPN 域名接入，数据库故障切换断点自愈恢复。跨地域多活同步：DTS 支持 MySQL/GaiaDB 双向同步。</p><p></p><p>最终，百度智能云帮助该客户跑通了数据库上云迁移的全流程，并支持客户自助上云迁移，目前已经支持 2000+ 集群的迁移，单集群的迁移周期缩短到 3-4 天。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b25063cab42fe1fd84350a9f48abb3a5.png\" /></p><p></p><p>第二个案例是某国有控股大型商业银行，DTS 支持了行方的实时数据分析和跨机房容灾的需求。</p><p>该客户的业务痛点主要包括两个方面：</p><p></p><p>高吞吐：行方核心业务（存/取款明细）涉及到 64 分片集群每月集中跑批， TPS 峰值达到 50W+，要求数据同步延迟分钟级。高可用：数据库及生态产品（DTS）整体具备跨机房容灾能力，故障恢复要求为 RPO = 0，RTO &lt; 1min。</p><p></p><p>针对该客户的这两个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>端到端吞吐优化：CDC 异步拉取/并行解析，表/主键粒度并行转换/加载，数据打包写入。跨机房容灾：基于 load checkpoint 的段点续传（RPO）和数据库拖段服务切换自愈（RTO）实现了任务实时容灾恢复。</p><p></p><p>最终，百度智能云帮助该行落地了实时风控、监控大屏、收支分析等业务场景，线上长期同步任务 900+；同时，支撑行方核心业务（存/取款明细）集群数据同步需求，同步延迟秒级。此外，DTS 服务支持同城双活，机房级故障恢复实现了&nbsp;RPO = 0，RTO &lt; 30s。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/932f451df069b833f97ea0e57ed87be8.png\" /></p><p></p><p>第三个案例是某国内大语言模型服务，DTS 支持了业务方的事实数据分析和检索的需求。</p><p></p><p>该客户的业务痛点主要包括两个方面：</p><p></p><p>同步性能：包括大语言模型对话数据、模型 Trace日志实时分析等，部分业务场景要求数据同步延迟达到秒级。快速迭代：&nbsp;大语言模型功能迭代速度快，在线数据库表结构更新较为频繁。</p><p></p><p>针对该客户的这两个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>低延迟调优：自适应写入性能调优，数据打包窗口动态调整。整库同步：DTS 支持库级别同步，目标端 Doris 支持增量同步 DDL，支持增量阶段新增/删除同步对象。数据规范化：支持库表行列过滤、库表列名映射等功能。</p><p></p><p>百度智能云支撑了该大语言模型中服务日志实时分析与实时报表的需求，同时也满足了业务快速迭代带来的整库同步、表结构更新同步等需求，最终实现了长期同步任务 470+，同步延迟最低达到秒级的业务效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/59c5196f7db586c10a37ac61eb6bd84e.png\" /></p><p></p><p>我们对 DTS 的所能支持的各类业务需求归纳为 8 种典型的场景，供大家参考。分别是：不停服迁移上云、异地多活、多云灾备、业务事件驱动、信创迁移、缓存更新、实时分析、实时入湖/仓。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a7dd5d322f241190f6c9e591ed7a1e75.png\" /></p><p></p><p>最后，我们对今天的分享做个总结，我们分享了百度智能云在数据库上云迁移和数据同步/集成方向的设计思路和落地实践。</p><p></p><p>在上云迁移部分，百度智能云提供了平滑可靠的一站式上云迁移服务。</p><p></p><p>迁移服务开箱即用，支持评估、迁移、校验、回滚的全流程托管。兼容源和目标不同引擎、架构、版本和网络环境，业务改造成本低。上云迁移无需停服，业务影响小，迁移和回滚链路支持端到端的故障恢复</p><p></p><p>在同步/集成部分，DTS 可以提供易用稳定的数据同步/集成服务：</p><p></p><p>在异地多活场景中，基于 DTS 双向同步可以支持数据库多主架构，实现全球化服务访问加速，保证数据全局一致。在多云灾备场景中，DTS 支持数据库跨云长期同步，支持端到端容灾自愈，数据一致性有保证可校验。在数据集成场景中，DTS 立足流批一体设计，支持端到端的自由组合，可以实现秒级实时同步，未来计划支持实时数据加工。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5d4ec5fc7ab0f57eb95a01f2e0a38f7e.png\" /></p><p></p>",
    "publish_time": "2023-12-13 09:51:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "银行如何以“价值链视角，数字化手段”思考规划落实中央金融工作会议精神",
    "url": "https://www.infoq.cn/article/T7cUzo9pjuiXXVsdSRjc",
    "summary": "<p>中央金融工作会议精神思想深邃、立意高远，需要整体把握、综合实践，因此也需要以整体视角进行学习理解、规划落实，本文拟尝试在之前文章的学习基础上，运用价值链视角，结合数字化转型进行落实层面的分析，供从业者借鉴，不当之处，请多指正。</p><p></p><h3>一、 分析工具介绍</h3><p></p><p></p><p>价值链视角是一种整体分析视角，从企业价值创过程出发，对企业能力的分布和关系进行研究，有利于企业做“一盘棋”式的战略规划设计。该方法在国内银行业过去十年的数字化转型实践中，多次被运用在整体设计和实施指导上，经过了实践检验和国内创新，在实现总体概念的统一、传导方面，具有良好效果，笔者在《银行数字化转型》一书、《说透数字化转型》课程中也充分运用了该工具进行数字化规划分析和解释。对于学习改革目标如此之深、之广的中央金融工作会议精神、对于落实“八个坚持”、做好银行的“五篇大文章”很有运用价值。</p><p></p><p>笔者在《银行数字化转型》一书提出的数字化银行价值链如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1a/1a444b936c092f0e0cbcd1a1f64f3e3b.jpeg\" /></p><p></p><p>该价值链涵盖了<a href=\"https://www.infoq.cn/article/BpAYeYkzIHtJaPlHC5TR\">银行</a>\"内部数字化改革和外部数字化支持，本文将在该价值链的基础上，以中央金融工作会议精神为指引、结合数字化手段的提升作用对其进行演进分析。</p><p></p><h3>二、 从“八个坚持”的视角看银行价值链的重塑</h3><p></p><p></p><p>（一）坚持党中央对金融工作的集中统一领导。落实在各家银行身上，首先是要充分学习党的理论，对党员开展深入的党性教育，只有充分理解了“中国特色金融发展之路”，保持金融队伍的“纯洁性”，才能将党中央对金融工作的集中统一领导深入落实到各个业务品类、各个业务环节上。因此，应当首先在“决策管理”环节加入对“中国特色金融发展之路”的研究，将“八个坚持”、“战略管理”能力体现在这一环节中，并在“组织与人力”环节加入“党性教育”。</p><p></p><p>（二）坚持以人民为中心的价值取向，坚持把金融服务实体经济作为根本宗旨。这两点在原有的价值链设计中已经有所体现，该价值链是以客户为中心、由与客户的沟通交流驱动的金融服务交付过程，客户在金融产品的设计和实施中具有明显的“中心性”，以客户为中心也自然会做到以服务实体经济为根本宗旨。</p><p></p><p>此外，这一点还体现在“环境研究”环节中，“环境研究”中包括的“整体研究”、“亚群体研究”正是对完整客户群和分类客户群的研究，银行以往在金融服务过程中，对客户份额争抢的关注大于对客户群体的深入研究，以人民为中心的价值取向应该是关注人民需要的金融服务，而不是过度的份额竞争，事实上，对于诉求的分析本身是有利于扩大市场的，但是竞争的同质化、客户群体的高度重叠，使竞争手段日趋白热化、简单化，客户画像等手段的引入虽然提升了银行的客群经营能力，但是市场压力还是难以让银行有充分的时间考虑服务质量和内涵的提升。</p><p></p><p>真正做到以人民为中心的价值取向不仅银行自身要调整，市场竞争秩序也需要规范和调整，以支持银行的服务动作不会“变形”。原有价值链中的客户洞察、产品改进是体现“以人民为中心的价值取向”中的重点环节，也是数字化手段提升银行服务能力的重点领域，银行现有的数字化能力在产品设计和产品实施方面是有一定成绩的，但是还很难做到将以“智慧交流”为核心的客户触达和服务能力大幅度提升，毕竟客户群体太大，技术也有待完善。在“客户洞察”环节需要补充的是“客群规划”能力，要结合会议精神和今后的各类改革要求不断灵活调整客群定义的能力，没有客群的灵活定义，也很难统计改革的落实情况；在“产品改进”环节中增加“综合演进”，能够将客户对金融服务的意见分客群、分产品进行汇总，综合改进，持续跟踪，以使银行更加了解、更深分析人民需要的金融产品。</p><p></p><p>（三）坚持把防控风险作为金融工作的永恒主题。站在银行的视角，风控可以分为企业级和产品级两层，前者是站在银行整体视角衡量总体风险分布、衡量单个客户综合所有业务之后的总体风险水平，后者则是站在单个客户单笔业务、单个产品的视角衡量相对微观的风险，产品级风险防控会贯穿在单次业务办理过程的始终，而企业级风险则时刻从总量视角为单笔业务的办理做好总体控制，这是银行风险管理的基本要求。</p><p></p><p>在风控业务中，数字化的加持主要在于风控模型的运用，外部风险信息的及时获取，整体风险水平的及时、动态测算，以及对压力测试的持续模拟等。根据中央金融工作会议精神以及之前的各项政策要求，风控模型的有效运用和适度修订是必然要考虑的问题，在企业级风控中，修订客群的风险评价模型，灵活调控客户准入规则；在产品级风控中增加对政策要求向风控参数的转化能力，灵活调控产品准入、单次准入规则；通过总量测算控制总体风险水平。在面向中小微客户时，风控模型可以考虑宽准入、控规模、容多头的原则，以扩大普惠群体，但根据银行体量控制好单户规模，通过多头方式提升信贷资源面向单个客户的供给总量，并保持风险分散。但是这种操作方式的实现，需要行业信用体系的良好运转、信息便捷共享，也需要更多来自外部核心企业提供的供应链平台信息，只有企业的运作信息、信用信息公开透明易获得，才能推动风控工作总体水平的提升。</p><p></p><p>因此，风控工作需要在银行价值链的外部环节中提供更大支持，推动数据要素的共享、交易，通过数据流动改善风控环境。中小金融机构的改革也是降低银行整体风险水平、提升风险应对能力的重要部分，大型银行是主力军、压仓石，但是中小银行的问题也必须关注。</p><p></p><p>（四）坚持在市场化法治化轨道上推进金融创新发展。这一点在近年数据保护工作的推动中表现非常明显，个人信息保护相关法律的实施也让银行的经营行为、科技开发工作出现了调整，数据要素相关法律未来会深刻影响银行业务，所以，对于法律的宣贯应在银行价值链中有明显的一席之地，在“组织与人力”中增加“法律教育”，并将对法律的理解运用到完整的客户服务过程中。</p><p></p><p>（五）坚持深化金融供给侧结构性改革。银行必须紧跟政策脚步、紧跟环境变化，及时动态调整经营策略、调整产品供给、调整服务能力，这需要银行始终对自身能力、自身结构“洞若观火”，只有了解自己的结构，才能制定合理的调整方案，才能将业务变化快速、准确地传导给技术侧，实现业务和系统、实体和数字的联合变动。</p><p></p><p>供给侧的结构性改革，需要结构化的分析能力、结构化的实现能力，结构化思维，也就是系统思维是数字时代的基本思维方式，是业务侧和技术侧共同需要掌握的思维模式、沟通方式，也是数字时代必须要补充的管理思维，企业架构正是系统思维的具体实现方式之一，也是国内银行业之前数字化转型过程中多有采用的一种工程思维，它已经过了国内实践的脱胎换骨，可以从技术思维变成业务思维、管理思维了，这也是笔者之前就在数字化银行价值链中植入企业架构能力的原因，企业架构能力不仅体现在“产品设计”环节，更是整个银行数字化转型方法论设计和落实的基础，是为“一盘棋”准备的理想实施工具，从方法论融合和演进角度看，在“基础研究”环节还应加入“知识管理”，为金融理论、管理理论、架构理论的探索与发展留有空间。</p><p></p><p>（六）坚持统筹金融开放和安全。这一点需要监管层面多提供行为指导，在商业银行层面，更多涉及的是金融环境研究和全面风险管理，在“环境研究”和“企业级风控”中需要适当做些内容调整。</p><p></p><p>（七）坚持稳中求进工作总基调。这一点体现在价值链重塑的整体设计和落地推进过程需要结合银行自身情况、深入领会会议精神、稳扎稳打逐步推进，不在排除风险的过程中又增添额外风险，数字化本身也是有风险的，对此，监管机构也曾要求将数字化风险纳入全风体系进行管理。</p><p></p><h3>三、从“五篇大文章”的视角看银行价值链的重塑</h3><p></p><p></p><p>（一）科技金融。科技金融对于大多数银行而言，其知识壁垒、风投特性是从事该项业务较为困难之处，科技金融中涉及到科创类企业的，银行往往难以做出合适的评估，加之该类企业对长期股权类资金的需求也很强烈，所以，也有供需不匹配的情况出现。</p><p></p><p>对此，银行从事该类业务，首先在“客户洞察”环节必须有相适应的科技企业评估模型，这方面大型银行有些探索，中小银行结合当地特色产业也有一定的探索，但是来自行业的基础性评估模型的总结、提升仍有待加强，行业指引也需要具有更加丰富、详实、视角更宽的数据信息，而银行自己则需要专注于某些赛道，长期培养相对专业的客户经理、风险经理，在“环境研究”环节需要有自己专注的行业研究，这就决定了银行开展科技金融业务需要一定的“错位部署”，尤其是对中小银行而言，只能结合当地特点开展特定领域的研究。对于大型银行而言，则应思考利用一级分行的区位优势，分开部署，对于有交叉的领域，建立一级分行的业务小圈子，“一行一策”有特点地做。对于一些特殊性国家重点领域，则应展开跨行合作，联合进行。</p><p></p><p>（二）绿色金融。绿色金融业务具有广阔的业务前景，从最新公布的数据看，大型银行的绿色金融业务也达到了一定的规模，但是其进一步发展仍然有一些问题需要解决，比如统一的行业标准和管理要求，面向客户和银行两端的法律法规、配套政策、约束激励机制等，都需要持续完善，绿色金融是长期性金融业务，很难基于现有的业绩考核标准推广，在价值衡量上需要有统一的行业指引并落实到银行内部，“价值核算”环节应突出“更有政策导向的业务核算”能力。</p><p></p><p>（三）普惠金融。普惠业务是一篇范围足够大的文章，而且是典型的“长尾业务”，也正因为如此，它也是世界级难题。随着大型银行在普惠市场中的下沉服务，这一领域的金融供给在逐步增加，普惠业务也成为全行业关注的焦点领域。</p><p></p><p>1、在客户洞察环节，上文提到要“宽准入、控规模、容多头”，也就是准入要放宽，不然不足以体现普惠，但是单户的业务量要适度控制，并且鼓励多头业务，以保证供给、分散风险，这就要求客户洞察阶段的客户画像要完善、更新要及时，所以，外部数据的供给对普惠业务是非常重要的，仅凭银行自身的数据，难以及时响应普惠金融的业务需求。行业内的信息共享也应适当考虑，普惠业务可能需要的是客户共享而非客户竞争。</p><p></p><p>2、在产品设计环节，即针对普惠客户提供金融产品时，由于普惠客户的特点，往往在产品设计上强调对担保形式的灵活处置，但除了担保之外，定价也是产品设计中的一个重点，定价方面除了传统的银行内部资金转移价格加风险加成的定价外，可以再考虑一种转移定价方式，也即将大型企业客户身上获取的收益转移到降低普惠产品价格方面，这并非单纯的“让利”，而是具有一定的“转移支付”功能。这种操作方式在中小银行和大型银行的一级分行层面具有一定的可尝试性，采用这种计算方式的优点在于，有利于量“入”为“出”，以“转移价格”衡量基础性的让利空间，更有利于相对“安全”的分配普惠让利任务；普惠业务本身仍可以按照名义价格和让后价格核算，原始业务成效和让利工作都可以清楚统计；对于让利空间不足的地区，总行可以基于总账核算层面通过总盘子给予支持，能够体现总行的工作成果，也能够让总行对地区间的不平衡状况、普惠工作带来的时间性改变有更清楚的了解。</p><p></p><p>但这种方式的执行也有一个需要支持的点，这一点可能在思考如何逆转“二八定律”去做好“普惠金融”时需要进行一定的探讨，也就是银行如何能够对大型客户取回一定的话语权，站在商业银行经营的视角，资金安全、业务量大的大型客户是“兵家必争之地”，但是银行对大型客户的话语权很低，往往定价不高，好在体量大，具有一定回报，但是服务大型客户也低效地消耗了一定的金融资源，而这些大型客户常常也是“不差钱”的，所以，从行业视角规范大型客户的服务秩序，制定合理的行业限价，降低对大型客户的竞争性服务程度，可能是银行能够更“安全”、更大范围、投入更多资源开展普惠业务的一个辅助性条件。从大型企业客户身上转移的“价格”也可以作为大型企业客户的社会责任加以“记账”，从而形成新的客户评价机制，补充到“客户洞察”环节，可以提升客户的社会价值。</p><p></p><p>此外，中小银行在进行普惠业务时也会存在资金不足、资金成本过高的问题，那么，站在同业业务的视角，可以在银行间市场中建立更明确的普惠规则、普惠板块，符合普惠条件的同业业务，由需求方挂牌，供给方选择进行低价资金供应，成交的业务可以折价记入供给方的普惠业务指标统计，毕竟资金最终是用到了普惠业务上。在上述空间以外需要继续投入的普惠支持，可能也需要银行开拓更多的资金来源进行供给了。以上几点思考都是与价格相关的，需要银行自身探索，也需要更强大的行业力量予以支持和研究。</p><p></p><p>3、在产品设计环节还要增强企业级的架构设计。普惠业务面向的多是人员少、时间紧的中小微企业，所以金融产品设计应更加简洁，但这种简洁需要产品设计的抽象能力，并且，并非业务简洁涉及的数字化系统就会少，实际上表面上的简洁很多时候是系统有效地“负重”了，所以，普惠产品的发展也很需要架构理念的支持，做好业务抽象，管理好客户体验，充分调动现有系统的数字化能力，提供综合性服务。</p><p></p><p>4、加强连接。做普惠金融也不仅仅是一户一户“啃”的零碎业务，也需要通过商业生态连接来加强对行业动态的充分了解，考虑到数字化转型的深入，如何将银行的数字化与客户的数字化结合，并将商业联系转化为数字联系，这是时代命题，而非单纯的发展意愿问题。</p><p></p><p>（四）养老金融。养老金融不仅有之前银行都在做的“适老化”改造，这主要体现在客户洞察和产品设计环节，尊重老年人的习惯是文明的体现。此外，养老金融逐渐已经发展到了存款、基金、理财、财富管理、支付等诸多领域，引导合理的理财观念逐渐成为养老金融的一大话题。养老金融形成的资金来源具有一定的长期性特征，对于银行而言，可以成为一些长期标的的资金源，但是养老金融业务的广泛开展也是需要跨行业信息获取和跨行业场景融入的，这些能力诉求体现在“产品设计”、“产品实施”、“生态管理”等环节中。</p><p></p><p>（五）数字金融。按照笔者之前文章介绍的<a href=\"https://www.infoq.cn/article/ROrh4hSJPu1UkXzx5Uhc\">对数字金融的两层含义</a>\"的理解，数字金融是涉及到银行价值链所有环节的，毕竟金融产品的交付形态大多都已经转化成了数字形态，对客户的理解、对产品的设计、对风险的防控、对价值的衡量、对未来方向的把握，都离不开对数字化的理解和实践，数字化是银行高质量发展的必由之路，笔者以前的文章大多是谈数字化转型的，所以在此只介绍几个重点：</p><p></p><p>1、对技术方向的关注。银行虽然不是纯粹的科技企业，但是技术的发展对银行的影响却毋庸置疑，尤其是渠道端，随着人工智能技术的发展，渠道正在从最近十年的移动端向体验更好、能力更强的智能渠道加速发展，渠道变革带来的影响绝不能忽视，它会深刻改变银行的业务形态，就像从柜台到手机一样，业务模式、系统布局都进行了大幅度调整。</p><p></p><p>2、对服务能力的关注。无论是普惠金融还是养老金融，都需要更强大、更广泛的具有“一对一”体验的“一对多”服务能力，无论是移动端、远程银行还是任何可以打破空间限制提供大范围服务能力的技术都值得关注。</p><p></p><p>3、对数据能力的关注。数字化的核心还是对数据要素的挖掘，无论做了多少系统，数据的价值发挥不出来，系统就只是个记录系统而已，留下了业务痕迹，但是没有发挥出更大的数据价值，无法向今天所谈的数字化方向靠拢。但是发挥出数据价值是需要人的能力和系统能力互相配合的，所以，面向数字的基础技能，比如结构化思维等，是银行必须要增加的从业人员技能，没有结构化思维，就很难理解数字化，也很难有效定义、采集、运用、评估数据资产。</p><p></p><p>（六）补充调整。之前的文章讨论也曾提到，做好这“五篇大文章”需要银行进行价值取向的调整，而这必然涉及到考核方式的变化，金融行业当前的任务是做优做强，金融理论要突出政治性、人民性，考核体系的设计也应思考从经营 KPI 的设置转向社会责任性 KPI 的设置，规模、盈利还是不是首当其冲的考核要求，如果要适度调整银行的逐利性，则必须从行业引导开始，价值导向、秩序导向、风险控制都需要超越行业级别的指引，否则银行在商业博弈环境中很难自动转向；有大级别的指引，也需在银行内部考核上落实指标转向，银行可以借此机会思考如何从激烈的同业竞争中适度解脱出来，将资源更多用于服务而非市场争夺。</p><p></p><h3>四、对银行价值链重塑的归纳总结</h3><p></p><p></p><p>面对<a href=\"https://www.infoq.cn/theme/212\">新的金融发展方向</a>\"，重塑银行价值链是一定要思考的问题，对以上讨论进行归纳总结后，数字化银行的价值链可以做如下演变：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/68b401679f077eab18de146d963dffc2.jpeg\" /></p><p>上述分析仅是基于价值链工具，融合数字化手段之后，对中央金融工作会议精神进行学习的尝试，在实际操作中，为了便于价值链分析结果更有利于落地，通常会对价值链进行压缩，调整成一维的价值链，比如可以按照如下方式压缩：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/81/819fb62b1dbfcacc9f49ce5b7bef09e7.jpeg\" /></p><p></p><p>再将提炼的能力重新归置，然后结合业务领域的定义进行能力布局分析，比如将“五篇大文章”作为行，将价值链各环节作为列，行列交叉后形成分析棋盘，将能力进行对比分析，识别公用和非公用能力，就能够成为落实“五篇大文章”所需要的能力结构分析，最后转化到系统实现上，成为落实会议精神、体现业技融合的全局性大文章。这些已经属于企业架构的应用范畴了，本文仅是借用这个工具来加强对中央金融工作会议精神的学习。</p><p></p><p>2024 年可能是各银行充分学习领会中央金融工作会议精神、思考“中国特色金融发展之路”指引下的商业银行发展方向、行为模式的内省之年，多运用系统思维、架构方法，深入地、结构化地学透、贯彻会议精神，是新年工作的重中之重，开门红应该红在哪里，也是需要思考的。</p><p></p><h4>作者介绍</h4><p></p><p>付晓岩，《企业级业务架构设计：方法论与实践》、《银行数字化转型》和《聚合架构：面向数字生态的构件化企业架构》三书作者。北京天润聚粮咨询服务有限公司执行董事总经理，数孪模型科技公司高级副总裁；工业和信息化重点领域数字化转型与人工智能产业人才基地专家委员会副主任；中国计算机学会软件工程专委会委员；信通院企业架构推进中心、组装式推进中心技术专家；中华全国数字化人才培育联盟专家委员会特聘专家；工信部中小企业发展促进中心产教融合产业实践教授；国家工程实验室金融大数据应用与安全研究中心研究员；CIC 金融科技与数字经济发展专家委员会成员；国家互联网数据中心产业技术创新战略联盟专家委员会副主任专家委员。</p><p></p><p></p><h4>内容推荐</h4><p></p><p>11 月 19 日 -20 日在上海成功举办的首届 FCon 全球金融科技大会，以「科技 + 金融，激发创新力量」为主题，汇聚了来自金融龙头企业的数百名技术高管，掀起一场探讨新时代金融科技未来的高潮。经征得大会分享嘉宾同意，InfoQ 数字化经纬为您奉上精彩演讲 PPT！关注「InfoQ 数字化经纬」，回复「金融创新」即可获取 PPT，深度洞悉科技趋势，助您引领金融创新未来！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/afc55058ba66a0cd61acf97f02db59a1.png\" /></p><p></p>",
    "publish_time": "2023-12-13 09:54:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云原生数据库 GaiaDB 架构设计解析：高性能、多级高可用",
    "url": "https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz",
    "summary": "<p><a href=\"https://www.infoq.cn/article/SGPHdt4a0GyPUVyOotSm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">百度智能云</a>\"团队在今年 11-12 月特别推出了四期《百度智能云数据库》系列云智公开课，为大家全面地介绍了以云原生数据库 GaiaDB 和分布式数据库<a href=\"https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\"> GaiaDB-X</a>\" 为代表的百度智能云数据库系列产品。</p><p></p><p>在《百度智能云数据库》系列云智公开课的第二期内容中，百度智能云数据库高级架构师邱学达为我们介绍了云原生数据库的不同技术路线及能力对比，并对比传统单体数据库介绍了云原生数据库的技术差异和挑战，同时深入浅出地解析了 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p></p><p>下文为他的演讲内容整理：&nbsp; &nbsp;&nbsp;</p><p></p><h2>云原生数据库和 GaiaDB</h2><p></p><p></p><p>目前，<a href=\"https://xie.infoq.cn/article/be269a4dac007392339e5f63b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">云原生数据库</a>\"已经被各行各业大规模投入到实际生产中，最终的目标都是「单机 + 分布式一体化」。但在演进路线上，当前主要有两个略有不同的路径。</p><p></p><p>一种是各大公有云厂商选择的优先保证上云兼容性的路线。它基于存算分离架构，对传统数据库进行改造，典型产品有 AWS Aurora、阿里云 PolarDB、腾讯云 TDSQL-C、百度智能云 GaiaDB。</p><p></p><p>数据库作为公有云上的核心基础设施，第一要务是实现用户上云的平滑性。目前像云网络、云主机，云盘都实现了完全透明兼容。云原生数据库也必须实现从语法、使用习惯、再到生态上的全面兼容。因此，基于现有生态做分布式化改造成为了一条首选的演进路线。使用存算分离路线的云原生数据库可以完美兼容传统的使用习惯，为交易类场景提供低延迟的写事务能力，同时读扩展性与存储扩展性借助了分布式存储的池化能力，也得到了很大增强。</p><p></p><p>另外一种路径是先搭建一套分布式框架，然后在其中填充数据库逻辑。OceanBase 和 TiDB 就是其中两个比较典型的产品。它们将事务的子系统和锁的子系统拆分为单独的模块。计算层通过与这些模块交互，可让多个节点均支持写请求。然后由统一的新事务 + 锁中心节点来进行仲裁。这样，对需要较多计算资源的写负载场景会有较好的提升。由于事务和锁都需要跨网络进行交互，因此事务延迟相对较高，在锁负载较重的情况下会成为一定的瓶颈。</p><p></p><p>目前这两个路线并不是泾渭分明，独立发展的，大家都在向着统一的目标演进。因此我们可以看到，存算分离路线在逐渐增强 SQL 的多级并行能力，同时也在探索和支持多个写节点的库表级 / 行级的多写能力。同时分布式事务路线也在积极探索在小数据规模下的单机部署架构。</p><p></p><p>所以在未来，这两个路线会不断融合。业务的数据规模不管多大，都可以平稳快速地运行在数据库系统上，而不需要用户去过分关注分区、索引、事务模型等信息。就像十年前如何在机器之间存储海量小文件还是一个后端研发工程师的必修课，而随着 S3 存储的出现，用户再也不需要考虑如何通过哈希等方式来保证单个文件夹不会保存太多文件一样。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/494bca1fca9378e1fa0da766c469e03a.png\" /></p><p></p><p>GaiaDB 是从百度智能云多年数据库研发经验积累中逐渐迭代而来。GaiaDB 于 2020 年发布首个版本，首次实现了基于存算分离的大容量存储和快速弹性能力，解决了百度内部的历史库、归档库等大容量存储需求。</p><p></p><p>紧接着，为了满足集团内大部分核心业务的跨地域热活准入门槛和就近读性能需求，GaiaDB 于 2021 年发布了地域级热活功能。跨地域热活仍然使用存储层同步的方案，同步延迟与吞吐都相较逻辑同步有很大提升，从地域可以实现与主地域接近相同的同步能力，不会成为拖慢整体系统的短板，也不会像逻辑同步那样在大事务等场景下出现延迟飙升的问题。</p><p></p><p>所以 2.0 版本上线后，GaiaDB 逐渐接入了手百、贴吧、文库等多个核心产品线，解决了业务在跨地域场景下的延迟与性能痛点。</p><p></p><p>随着业务的逐渐上云，多可用区高可用的需求慢慢凸显，如何实现单机房故障不影响服务成为了很多业务上云的关注点。为此 GaiaDB 打造了可支持跨可用区热活的 3.0 版本，每个可用区都可以实时提供服务并且不增加额外的存储成本。而在今年， GaiaDB 推出了更加智能化的 4.0 架构，性能进一步提升，功能完整度也在持续完成覆盖。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/729c24394d30b36427010d70f44034d2.png\" /></p><p></p><p>接下来整体介绍一下 GaiaDB。目前 GaiaDB 已经实现了线上全行业场景覆盖，最大实例达到了数百 TB，不仅兼容开源生态，还实现了 RPO=0 的高可靠能力。在成本方面，由于在架构设计上采用了融合的技术理念，GaiaDB 不依赖特殊硬件和网络环境也可以保证性能，实现云上云下一套架构。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d33cdaf891b3d0f762e21079e50e16c.png\" /></p><p></p><p></p><h2>GaiaDB 的高性能 &amp; 多级高可用设计</h2><p></p><p></p><p>接下来我来分享一下 GaiaDB 的性能核心设计理念——通过融合和裁剪，将数据库和分布式存储进行深度融合，为全链路的同步转异步化提供条件，从而实现极致的性能与通用性。</p><p></p><p>我们可以看到，如果数据库简单使用通用分布式协议和单机存储引擎，如左图所示，那么数据库需要处理主从同步，需要有 CrashSafe 所需要的物理日志。同时，一致性协议也要有主从同步，要写自己的 WAL 以及持久化快照。而单机引擎同样需要 CrashSafe 以及一套日志系统和数据存储逻辑。</p><p></p><p>我们发现，多层日志的嵌套带来了层层延迟与写放大。更复杂的是，数据流中嵌套多层逻辑后，也给系统整体数据安全带来了一定挑战。同时由于多层之间需要串行等待，所以在加入了网络延迟后会给数据库带来很大的性能下降。虽然可以使用定制化硬件与网络来缩短网络和磁盘落盘的延迟以降低链路耗时，但这又引入了新的不确定性并导致了更高的成本。</p><p></p><p>GaiaDB 的解决思路是将事务和主从同步逻辑、日志逻辑、快照和存储持久化逻辑重新组合和排布。</p><p></p><p>首先是将分布式协议的主从同步逻辑融合进数据库计算节点中。由于计算层本身就需要处理主从同步、事务和一致性问题，相关的工作量增加并不大。这样一来，最直接的收益就是将两跳网络和 I/O 精简为一跳，直接降低了链路延迟。</p><p></p><p>其次 GaiaDB 将多层增量日志统一改为使用数据库 Redo 物理日志，由 &nbsp;LogService 日志服务统一负责其可用性与可靠性。</p><p></p><p>除此之外，GaiaDB 也将持久化、快照和数据库回放功能融合入存储节点。由于存储层支持了数据库回放能力，可以很轻松实现数据页级别的 MVCC。这样全链路只剩下了数据库语义，数据流简单可靠，逻辑大大简化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/868f47ab0c9ef26160ad0bb62bc6da58.png\" /></p><p></p><p>下面我们一起来看下共识模型上的改变。</p><p></p><p>像 Raft 协议是需要两跳网络才能实现一次提交确认的，右上角就是 Raft 的数据流架构：CN 节点将写发送给 Leader 后，需要等待 Leader 发送给 Follower 并至少收到一个返回后才能成功。</p><p></p><p>这里就带来了两跳网络和 I/O 的同步等待问题。而 GaiaDB 则是计算节点直接发送给多个 Log 服务并等待多数派返回，这样不依赖任何特殊硬件与网络就降低了延迟。这样系统里不管是事务的一致性还是多副本一致性，统一由计算节点统筹维护，所有的增量日志也统一为数据库物理日志，整体数据流简单可控。</p><p></p><p>对于数据风险最高的 Crash Recovery 场景，由于统一使用了数据库语义，整体流程更加健壮，数据可靠性更高，降低了数据在多种日志逻辑之间转换和同步带来的复杂度风险。而在性能方面，由于存储层自身具备回放能力，可以充分利用 LogService 层的日志缓存能力。对于写操作来说，不需要每次更改都刷盘，可以批次回放刷盘，大大节省了磁盘吞吐与 I/O。</p><p></p><p>经过以上改造，线上吞吐性能可以提升 40% 。同时由于链路简化，也大大优化了长尾延迟。像之前计算节点与分布式主节点之间发生网络抖动的场景，就会被多数派的返回特性来优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/88/880c11b4c3596b9934ec882ae9281276.png\" /></p><p></p><p>分享完一致性协议层优化，接下来我们来探讨一下链路层优化。</p><p></p><p>我们知道，总吞吐与并发度成正比，与延迟成反比。一致性协议层改造并缩短了数据链路，可以通过降低延迟来增加吞吐。那么有没有办法通过提升数据流的并发度来提升吞吐呢？答案是可以。由于数据库的物理日志自带版本号与数据长度，所以不需要像通用存储一样实现块级别串行提交。之所以使用通用存储需要串行提交，是因为存储端只能根据请求到达的先后确定数据版本，如果乱序到达，最后生效的版本是不可知的。</p><p></p><p>而对于 GaiaDB 来说，由于 LogService 具备数据库语义的识别功能，所以计算节点只需要异步进行写入，日志服务就会自动根据数据版本选取最新数据，然后根据写入情况批量返回成功，这样链路就可以实现延迟与吞吐的解耦。</p><p></p><p>当然计算层依然会等待日志层批量返回的最新落盘版本后再返回事务提交成功，所以依然可以满足提交成功的事务一致性、持久化的要求。</p><p></p><p>另外针对高负载下 I/O 请求与数据库业务请求争抢 CPU 的问题，我们使用了 I/O 线程隔离技术，通过资源隔离的方式，将 I/O 线程与数据库业务线程进行隔离。这样即使在复杂负载场景下，I/O 延迟仍可以保持在较低水平。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc00033498ca3707091d9bbfb5dadcbc.png\" /></p><p></p><p>在分析完前面两部分之后，可能会有同学有疑问：既然日志层到存储层不是同步写，是不是最终系统的一致性降低了？有没有可能发生数据丢失或不一致的问题呢？答案是不会。因为 GaiaDB 的存储是一套支持 MVCC 的多版本系统。所以即使回放实现上是异步，但是由于请求方会提供所需要的数据版本，存储层可以提供对应版本的强一致数据视图。</p><p></p><p>GaiaDB 的存储节点支持数据页的回放功能，可以动态回放至任意目标版本后再返回，在之前的版本里，假如由于异步的因素还没有获取到这部分增量日志，存储节点也会启用优先拉取的策略实时拉取一次日志后再回放，以此来提供较好的时效性。而在最新的 GaiaDB 版本中，我们也在计算层添加了同样的回放能力，存储节点尽力回放后仍不满足需求的，由计算节点进行剩余任务。</p><p></p><p>这样对于存储慢节点的兼容能力就大大增强了，同时由于存储节点会尽力回放，所以也可以最大化利用存储层的算力资源。对于刷脏逻辑目前也完全下沉到了存储层，存储节点可以自主控制刷盘策略和时机，尽量合并多次写后再进行落盘，大大节省了磁盘 I/O 负载，平均 I/O 延迟降低了 50%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a5fdd7d1fa003f9f635a729ca00a5f72.png\" /></p><p></p><p>下图中我们可以看到，在综合了多项优化后，读写性能实现了最高 89% 的提升，其中写链路线路提升尤其明显。这些都是在使用普通存储介质和网络环境的情况下测试得出的，主要得益于数据链路的缩短与同步转异步的自适应高吞吐能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9eb5e4d3c677acd21e5b83096f16ceb3.png\" /></p><p></p><p>在讨论完性能后，再分享一下 GaiaDB 在高可用方面的思考和设计理念。</p><p></p><p>数据库作为底层数据存储环节，其可用性与可靠性直接影响系统整体。而线上情况是复杂多变的，机房里时时刻刻都可能有异常情况发生，小到单路电源故障，大到机房级网络异常，无时无刻不在给数据造成可用性隐患。</p><p></p><p>作为商业数据库，具备多级高可用能力是最核心的必备能力。这样才能抵御不同级别的异常情况，有力保障客户业务的平稳运行。GaiaDB 支持多副本、跨可用区、跨地域三级别高可用，创新性地实现了多可用区热活高可用、单个实例支持跨可用区部署。在不增加成本的情况下，每个可用区均可提供在线服务，任何可用区故障都不会打破存储一致性。下面我们来分别看一下每个级别高可用能力的实现。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d1f9701867ba0e5edfacb8cc3d4de9be.png\" /></p><p></p><p>首先是实例的多副本高可用能力。</p><p></p><p>GaiaDB 对整体的分布式架构进行了重新设计，系统共分为三层，即计算层、日志层、存储层。其中计算层本身无状态，仅负责事务处理与一致性维护，所以获得了很强的弹性能力，实现了秒级切换、多节点容灾，同时扩缩容只需要内存启动即可。</p><p></p><p>日志层负责系统增量日志部分的持久化，实现了多数派高可用。同时由于一致性协调角色上移到了计算层，所以该层全对称，任意节点故障不需要进行等待选主，也不会有重新选主带来的风暴和业务中断问题。</p><p></p><p>再往下是存储层，负责数据页本身持久化与更新。由于上层保留了增量日志，所以存储层可以容忍 n-1 副本故障。简单来说就是只要有一个副本完好，加上上层提供的增量日志，即可回放出所有版本的完整数据，实现了相比传统多数派协议更高的可靠性能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/38691a2f2817476cc4e38ca54e83b504.png\" /></p><p></p><p>其次是跨可用区与跨地域的高可用能力。</p><p></p><p>GaiaDB 的多级高可用都是基于存储层物理日志的直接复制。相比逻辑复制，数据链路大大缩短，同步延迟也不再受上层大事务或者 DDL 等操作影响，在主从同步延迟上具有很大优势。</p><p></p><p>对于跨可用区高可用来说，由于 GaiaDB 具有对称部署架构，所以可以很方便地进行跨可用区部署。这样可以在不增加存储成本的情况下实现多可用区热活，任一可用区故障都不影响数据可靠性。</p><p></p><p>写数据流可以自适应只跨一跳最短的机房间网络，不需要担心分布式主节点不在同机房带来的两跳跨机房网络和跨远端机房问题，而读依然是就近读取，提供与单机房部署接近的延迟体验。由于跨机房传输的网络环境更为复杂，GaiaDB 添加了数据流的链式自校验机制，使数据错误可以主动被发现，保障了复杂网络环境下的数据可靠性。</p><p></p><p>对于跨地域高可用来说，由于同样使用了异步并行加速的物理同步，及时在长距离传输上，吞吐依然可以追齐主集群，不会成为吞吐瓶颈，在计入网络延迟的情况下，国内可以实现数十毫秒的同步延迟，这是因为跨地域同样可以使用异步并行写加速，自动适应延迟和吞吐之间的关系。同时地域之间还可以实现主动快速切换和默认就近读取。</p><p></p><p>所以在使用了 GaiaDB 的情况下，业务可以不做复杂的数据同步逻辑就可以实现低成本的跨可用区与跨地域高可用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb38aea27c6a8d0956fe7232108ba038.png\" /></p><p></p><p>介绍完高性能和高可用两部分的设计理念后，接下来再介绍一下我们正在内部灰度中的新功能：</p><p></p><p>并行查询：并行查询从并发度上进行加速的并行查询能力，这对大数据规模下的多行查询有非常好的加速作用，可以充分利用计算节点的 CPU 和内存资源和分布式存储层的并行 I/O 能力。分析型从库（HTAP）：分析型从库具备多种行列加速能力，既有支持百 TB 级别数据计算的分析型节点解决方案，也有支持百万行以上检索加速的列式索引引擎。其中列式索引引擎同样采用物理日志同步，不需要业务维护数据一致性，可以和当前交易类负载的事务隔离级别兼容。Serverless：我们也在探索充分利用内部潮汐算力的资源优化调度方案，在白天业务高峰期，将资源向实时性更强的交易类业务倾斜，在低峰期自动缩容，将资源复用投入到离线计算类业务中，不但客户节省了运维成本与资源成本，也避免了资源闲置和浪费，实现了更高的资源利用率。</p><p></p><p>以上功能预计都会在近期开放灰度试用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/ad6014a894d8b5543a024011f1729a80.png\" /></p><p></p><p></p><h2>写在最后</h2><p></p><p></p><p>自 11 月 15 日起，百度智能云团队每周三都会上线一节《百度智能云数据库》系列云智公开课。在前 4 期的课程中，专家们围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开了分享。</p><p></p><p>每节直播课的视频我们都进行了录制留存，都整理进了课程专题页中，课程持续更新中，大家立即点击<a href=\"https://www.infoq.cn/theme/222\">【此处链接】</a>\"进行观看吧~</p>",
    "publish_time": "2023-12-13 10:08:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "英伟达成为人工智能公司主要投资者：条件是必须使用英伟达产品",
    "url": "https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi",
    "summary": "<p>据英国《<a href=\"https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a\">金融时报》</a>\"报道，英伟达今年已投资了“二十多家”公司，包括从价值数十亿美元的大型新人工智能平台到将人工智能应用于医疗保健或能源等行业的小型初创企业。</p><p>&nbsp;</p><p>根据跟踪风险投资机构 Dealroom 的估计，英伟达在 2023 年参与了 35 笔交易，几乎是去年的六倍。Dealroom 表示，这是英伟达人工智能领域交易最活跃的一年，超过了 Andreessen Horowitz 和红杉等硅谷大型风险投资公司（不包括 Y Combinator 等小型加速器基金）。</p><p>&nbsp;</p><p>英伟达专门风险投资部门 NVentures 的负责人Mohamed Siddeek 表示：“总体而言，对于 Nvidia 来说，（进行初创企业投资）的首要标准是相关性。”&nbsp;Siddeek 解释道，“使用我们的技术、依赖我们的技术、在我们的技术上建立业务的公司……我无法想象我们会投资一家不使用 Nvidia 产品的公司。”</p><p>&nbsp;</p><p>据报道，英伟达的总体投资组合包括OpenAI的两大竞争对手Inflection AI和Cohere。这些公司还是英伟达的现有客户，只要公司继续成长和发展，对双方来说都会是一件好事。</p><p>&nbsp;</p><p>英伟达的另一项投资是<a href=\"https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3\">Mistral</a>\"，这是一家总部位于巴黎的人工智能初创企业，本月早些时候获得了20亿欧元的估值。另外两个是Hugging Face和CoreWeave，它们都是Nvidia GPU芯片或软件的用户。</p><p>&nbsp;</p><p>对于“接受投资的人也得到了优惠条件”的说法，Siddeek回应称，“我们不帮助任何人插队。”他反驳道，在任何投资中都有使用英伟达产品的条件，但他补充说，“我们会尽量对投资者友好。”</p><p>&nbsp;</p><p>据悉，英伟达的H100 GPU芯片最近已经成为硅谷最受欢迎的产品之一。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a\">https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a</a>\"</p><p><a href=\"https://readwrite.com/nvidia-emerges-as-leading-investor-in-ai-companies/\">https://readwrite.com/nvidia-emerges-as-leading-investor-in-ai-companies/</a>\"</p>",
    "publish_time": "2023-12-13 10:25:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]