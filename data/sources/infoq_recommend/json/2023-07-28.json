[
  {
    "title": "新手入门：探索eBPF的可观测性与安全性工作流",
    "url": "https://www.infoq.cn/article/085bCgNVTETVFvAr62i4",
    "summary": "<p>本文分享了学习eBPF的经验，eBPF是一种新的云原生技术，其目标是改善可观测性和安全性工作流。我们可能感觉它的入门门槛很高，通过eBPF工具来辅助生产环境调试的步骤会很多。本文将会介绍如何使用相关的工具并将其应用到自己的开发中，请逐步迭代自己的知识，并将其用到更高级的使用场景中。最后，我们会讨论如何在CI/CD中实现自动化开发及其面临的挑战。</p><p></p><h2>如何开始入门eBPF？</h2><p></p><p></p><p>我第一次听说eBPF是在2021年，当时它是与可观测性相关的主题一起出现的，起初我并不能真正理解它的含义。描述中声称这是一种收集事件数据的新方法，有助于提升可观测性，也有助于实现安全的可观测性和实际执行。</p><p></p><p>实际上，我后来才知道，Falco使用eBPF来探查Kubernetes中容器的活动。我的学习历程是将Falco视为云原生的安全工具，而没有去质疑其底层的技术。<a href=\"https://www.oreilly.com/library/view/hacking-kubernetes/9781492081722/\">“Hacking Kubernetes”</a>\"一书帮助我完善了对容器运行时、eBPF和安全执行的学习。</p><p></p><p><a href=\"https://opsindev.news/archive/2022-06-13/#kubecon-eu\">KubeCon EU 2022上的eBPF日</a>\"，以及后续的<a href=\"https://opsindev.news/archive/2022-10-15/#ebpf-summit\">eBPF峰会活动</a>\"，都有助于说明这一点。eBPF的学习策略与技术领域的其他知识类似，也就是倾听、做笔记，但你依然无法理解它的所有内容。</p><p></p><p>参加讲座和阅读文章时，我们经常会遇到一些需要认识的术语模式，比如，我立即记住的术语包括eBPF、BPF、bcc、bpftrace和iovisor。<a href=\"https://www.brendangregg.com/index.html\">Brendan Gregg的博客</a>\"也经常被提及。</p><p></p><p>在一个社区聚会上，通过自由讨论营（barcamp）式的演讲，我问到，“如何开始使用eBPF？”。随后，我们使用三张幻灯片拉开了关于它如何运行的讨论，一起验证了相关的知识，并思考了其使用场景。在eBPF峰会上，有一个名为 <a href=\"https://github.com/isovalent/eBPF-Summit-2022-CTF\">Capture-the-Flag的环境</a>\"可以进行学习，这吸引我停下脚步并亲自进行探索挑战。随后，我决定在自己的公共学习平台o11y.love上收集<a href=\"https://o11y.love/topics/ebpf/\">所有的eBPF资源</a>\"，并决定以公开的方式进行学习，记录在这个过程中遇到的所有错误、误解和问题。</p><p></p><p>内核开发听起来很难，而且理解和入门eBPF可能存在一定的障碍。对于利用eBPF的工具和库，改变使用它们的方法，并配合生产环境的用例（例如在生产中进行调试），这极大地帮助了我的学习和迭代。对Linux操作系统、资源处理和故障排除的一般理解也很有助益。</p><p></p><p>更高层次的阐述和ebpf.io上的描述图片有助于对eBPF架构的一般理解。我非常喜欢来自Brendan Gregg的<a href=\"https://www.brendangregg.com/blog/2019-01-01/learn-ebpf-tracing.html\">解释</a>\"：</p><p></p><p></p><blockquote>“eBPF对Linux的作用就像JavaScript对HTML的作用。（某种程度上，可以这么说。）因此，JavaScript可以让我们定义在点击鼠标等事件中运行的小型程序，而不再是静态的HTML站点，这些程序会在浏览器的安全虚拟机中运行。有了eBPF之后，我们不再是一个固定的内核，而是可以编写在磁盘I/O等事件上运行的小型程序，这些程序会在内核的安全虚拟机中运行。实际上，eBPF更像是运行JavaScript的v8虚拟机，而不是JavaScript本身。eBPF是Linux内核的一部分。”</blockquote><p></p><p></p><p>eBPF被添加到Linux内核中，以实现小型的沙箱程序。这兼顾了稳定的内核需求和少量的创新可能性，而eBPF程序能够有助于扩展和驱动创新，而不会阻碍内核的发展。</p><p></p><p>eBPF的用例包括高性能网络和负载均衡、应用程序的追踪和性能问题的排查。此外，细粒度的安全可观测性和应用/容器的运行时安全执行也是我能想到的场景。</p><p></p><p>编写eBPF程序是很难的，内核期望的是字节码，但是它手动编写的效率并不高。因此，需要有一个抽象层，包括从更高级的编程语言生成字节码的编译器。在这种情况下，经常涉及到的工具是Cilium、bcc和bpftrace。eBPF程序的校验发生在从字节码向机器特定指令集的即时编译过程中。这使得在CI/CD工作流中进行静态校验更加困难。稍后，我们会看到更多这方面的内容。</p><p></p><p>在了解了需求之后，真正的问题在于，我们有什么实际的例子可以尝试和学习，然后深入研究实际的源码？</p><p></p><h2>正式开始：游乐场</h2><p></p><p></p><p>Brendan Gregg的<a href=\"https://www.brendangregg.com/blog/2019-01-01/learn-ebpf-tracing.html\">学习eBPF跟踪：教程和样例</a>\"博文是一个很好的起点。不同的尝试和路线最终都会回到这里进行自学。在深入研究库和eBPF程序如何构建之前，在命令行上尝试不同的工具并测试它们的效果，这是一个很好的策略。</p><p></p><p>注意：Liz Rice的<a href=\"https://www.oreilly.com/library/view/learning-ebpf/9781098135119/\">“Learning eBPF”</a>\"一书能够有助于进一步降低入门门槛，该书于2023年3月出版。</p><p></p><p>推荐的入门方式是选择具有最新内核（大约4.17版本）的Linux发行版，如Ubuntu 22.04 LTS。请使用本地虚拟化方法，或在你喜欢的云厂商上生成一个虚拟机。下面的样例使用Hetzner Cloud CLI来生成一个新的Ubuntu虚拟机：</p><p></p><p><code lang=\"text\">$ hcloud server create --image ubuntu-22.04 --type cx21 --name ebpf-chaos\n</code></p><p></p><p>请根据你的需要重新创建设置过程，可以考虑编写Ansible playbooks或脚本来重复安装步骤。这对跟团队成员分享具体学习环境中使用的工具和库会很有帮助。本文讨论的工具和想法在<a href=\"https://go.gitlab.com/xtBYnL\">GitLab上有基于Ansible的样例</a>\"。有些默认的工具需要安装（git、wget、curl、htop和docker），还有eBPF、混沌实验和可观测性等更具体的用例。</p><p></p><p>接下来的章节将讨论eBPF工具的样例。要构建和安装它们，需要Linux内核头文件和额外的依赖。在Ubuntu 22 LTS上还有一个额外的步骤就是启用<a href=\"https://wiki.ubuntu.com/Debug%20Symbol%20Packages\">DDebs仓库</a>\"，以访问调试符号（debug symbol），接下来是一个完整的编译器工具链。<a href=\"https://go.gitlab.com/PPwfc5\">该针对eBPF的Ansible配置</a>\"详细描述了安装步骤。你可以查看Git的历史记录，了解学习的步骤以及这个过程中的错误。下面的几节主要是运行这些工具，并阐述它们的使用场景。</p><p></p><h3>跟踪系统调用</h3><p></p><p></p><p>你可能已经使用过strace命令来跟踪运行中的二进制文件的系统调用，查看是否有文件被打开和权限错误等。Brendan Gregg的教程博客建议从提供execsnoop命令的<a href=\"https://github.com/iovisor/bcc\">bcc toolchain</a>\"开始。它可以跟踪exec()系统调用。一个很容易的测试方法是打开SSH连接，或者在另外一个终端上执行curl opsindev.news命令。</p><p></p><p><code lang=\"text\">$ execsnoop -t\n\n115.816 curl             879320 879305   0 /usr/bin/curl opsindev.news\n118.481 sshd             879322 67197    0 /usr/sbin/sshd -D -R\n124.287 sshd             879324 67197    0 /usr/sbin/sshd -D -R\n</code></p><p></p><p>我们已经学习了一种跟踪系统调用的新方法。bcc工具链提供了更多实用的工具和用例。从学习的角度来讲，还有哪些工具可以用来深入研究eBPF呢？</p><p></p><h3>bpftrace：高级的跟踪语言</h3><p></p><p></p><p><a href=\"https://github.com/iovisor/bpftrace\">Bpftrace</a>\"提供了自己的高层级跟踪语言，类似于DTrace这样的调试框架。乍看上去，在线样例可能会让人无所适从，但由于我们使用的是测试虚拟机，所以可以运行这些样例，以后再分析语言。Bpftrace允许我们跟踪更多的系统调用，例如open()。这个方法可以用来打开文件、套接字等，更通用地来讲，是进程可以打开的所有内容，不管是善意还是恶意的。它可以视为strace命令的一种更为现代的方式。</p><p></p><p>为了使用可预测的样例来测试bpftrace，我们可以使用这个最小化的C程序，它打开一个文件句柄来创建新文件（<a href=\"https://go.gitlab.com/0mtuqx\">源码</a>\"）：</p><p></p><p><code lang=\"text\">#include \n#include \n#include \n#include \n#include \n#include \n\nint main()\n{\n  int fd;\n\n  if ((fd=open(\"ebpf-chaos.txt\", O_WRONLY | O_CREAT, 0660)) == -1)\n  {\n    printf(\"Cannot open file.\");\n    exit(1);\n  }\n\n  close(fd);\n}\n</code></p><p></p><p>使用gcc编译器编译C程序，并在启动bpftrace命令后运行它。如果opensnoop.bt命令在Ubuntu 22 LTS上运行失败的话，请从DDeb仓库安装调试符号。</p><p></p><p><code lang=\"text\">$ gcc sim-open-file.c -o sim-open-file\n$ chmod +x sim-open-file\n$ ./sim-open-file\n</code></p><p></p><p><code lang=\"text\">$ bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(\"%s %s\\n\", comm, str(args-&gt;filename)); }'\n\nAttaching 1 probe...\nsim-open-call /etc/ld.so.cache\nsim-open-call /lib/x86_64-linux-gnu/libc.so.6\nsim-open-call\n</code></p><p></p><p>跟踪语言允许挂钩进入特定的系统调用。要找到正确的系统调用名称，需要慢慢试验，可能还会遇到错误。我不得不将sys_enter_open改为sys_enter_openat来触发C程序中的打开文件的调用。bpftrace -l可以列出所有可跟踪的系统调用。</p><p></p><p><code lang=\"text\">$ bpftrace -l 'tracepoint:syscalls:sys_enter_open*'\ntracepoint:syscalls:sys_enter_open\ntracepoint:syscalls:sys_enter_open_by_handle_at\ntracepoint:syscalls:sys_enter_open_tree\ntracepoint:syscalls:sys_enter_openat\ntracepoint:syscalls:sys_enter_openat2\n</code></p><p></p><p>上述代码会将命令和文件名的路径打印到终端上。访问要打印的文件名需要阅读C结构的代码，以了解在这种情况下，哪些属性是可用的。</p><p></p><p>学习曲线的“顿悟时刻（aha moment）”不仅仅会看到文件打开和写入调用，而且还会加载库的依赖关系（stdlib需要libc）。bfptrace工具对于验证二进制文件是否真的加载了某些库是非常有用的，其次是使用ldd和nm来窥探依赖关系和调试符号。</p><p></p><p><code lang=\"text\">$ ldd sim-open-call\n    linux-vdso.so.1 (0x00007ffe42c78000)\n    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f24c7247000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f24c747d000)\n\n$ nm sim-open-call | grep open\n                 U open@GLIBC_2.2.5\n</code></p><p></p><h3>深入研究源码和eBPF程序</h3><p></p><p></p><p><a href=\"https://github.com/iovisor/bcc\">BPF编译器集合（BPF Compiler Collection，BCC）</a>\"提供了一些样例来学习内核和用户空间之间的数据传输和交互。以前的样例只是挂钩系统调用并立即返回。BCC在C代码中提供了内核插装，并允许使用Python或Lua编写前端用户空间的应用。按照描述，使用场景包括性能分析和网络流量控制，这都是很好的洞察点，并为以后的知识验证增加了学习难度。Python和C语言知识有助于更容易地深入研究这些样例。</p><p></p><p>另外，基于我的研究过程，推荐<a href=\"https://github.com/libbpf/libbpf\">libbpf</a>\"库，因为它的<a href=\"https://github.com/libbpf/libbpf-bootstrap\">bootstrap项目</a>\"提供了更多的演示应用。它们提供了真实的程序，可以用来实现自己的第一个eBPF程序。其中有一个样例是使用Rust编写的，允许我们按照<a href=\"https://en.wikipedia.org/wiki/Express_Data_Path\">XDP规范</a>\"检查网络流量以及数据包的大小。eXpress Data Path（XDP）允许在大规模网络调用时挂钩发送/接收的网络数据包，这会发生在中断之后和内存分配之前。例如，这可以用来悄悄地丢弃数据包（请注意后面高级的eBPF程序开发用例）。</p><p></p><p>用户需要指定端口号，这会导致再一轮的试验和错误排查。使用eth0作为接口名称无法成功运行。这个样例的输出源自同一台主机上运行的Prometheus服务器实例，产生的网络流量来自以HTTP端点探查监控目标的输出。</p><p></p><p><code lang=\"text\">$ apt install rustc cargo clang rustfmt\n\n$ git clone https://github.com/libbpf/libbpf-bootstrap\n$ cd libbpf-bootstrap/examples/rust\n$ cargo build\n$ cargo run\n\n$ sudo ./target/debug/xdp 1 #if number\n\n$ sudo tail -f /sys/kernel/debug/tracing/trace_pipe  \n\nprometheus-660     [001] d.s11 295903.782373: bpf_trace_printk: packet size: 74\nprometheus-659     [000] d.s11 295903.782735: bpf_trace_printk: packet size: 74\nprometheus-659     [000] d.s11 295903.782762: bpf_trace_printk: packet size: 54\nprometheus-671     [001] d.s11 295908.509751: bpf_trace_printk: packet size: 352\nprometheus-671     [001] d.s11 295908.513184: bpf_trace_printk: packet size: 4162\nprometheus-671     [001] d.s11 295908.513218: bpf_trace_printk: packet size: 66\nprometheus-671     [001] d.s11 295908.513295: bpf_trace_printk: packet size: 4162\nprometheus-671     [001] d.s11 295908.513307: bpf_trace_printk: packet size: 66\nprometheus-671     [001] d.s11 295908.513368: bpf_trace_printk: packet size: 1630\n</code></p><p></p><p>在构建和运行更多的样例后，我们并不完全清楚复制或修改源码是否为一个好的策略。如何将XDP样例缩减至最小的尺寸？也许有更好的方法来逐步入手编写eBPF程序代码，并增加学习过程中获得的经验。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/learning-ebpf-observability/en/resources/2figure-1-large-1684250535157.jpg\" /></p><p></p><h2>eBPF程序开发，学习更多用例</h2><p></p><p></p><p>在深入研究如何开发自己的程序之前，了解BPF和eBPF的基础知识是很重要的。eBPF是Berkley Packet Filter（BPF）的一个扩展版本，它提供了一个运行在Linux内核中的抽象虚拟机，在受控的环境中运行eBPF程序。从根本上说，Linux内核中的“老”BPF标准可以被<a href=\"https://ebpf.io/what-is-ebpf/#what-do-ebpf-and-bpf-stand-for\">称为“经典BPF”，以便于和eBPF进行区分</a>\"。</p><p></p><p>我们可以从尝试bcc工具开始，运行bpftrace并识别在日常业务和事件中有助于SRE和DevOps工程师的用例。这可能包括跟踪程序的启动/退出、查看控制组（cgroups）、观察TCP连接、检查网络接口等等。建议尽可能保持用例的简单性，以确保稳定的学习曲线。</p><p></p><p>在验证了关于eBPF的基础知识并定义了用例之后，请以<a href=\"https://ebpf.io/infrastructure/\">库和工具链</a>\"的形式探寻抽象的概念。现代编译器和库可用于Go、Rust和C/C++。在决定编写eBPF程序之前，建议先学习基本的编程语言。根据我自己的经验，在具有C++或Python知识之后，学习Rust是一条可行的发展道路。这有助于避免内存处理相关的运行时错误，与C/C++ eBPF程序相比，可以说这是一种更安全的方法。</p><p></p><p>Cillium在一个<a href=\"https://github.com/cilium/ebpf\">Golang的开源库中</a>\"实现了它的eBPF功能。除了学习编写自己的eBPF程序外，该库还提供了如下用例：将程序附加到入口/出口、计算egress流量包，以及探查网络接口（请注意XDP术语，以供后续学习）。XDP程序可以用Go编译器工具链进行构建，并接受接口名称作为命令行参数。它使用map来持久化特定IP地址的网络包的数量；对于Kubernetes节点上的任意类型的网络接口，探查容器流量或跟踪嵌入式硬件的流量都是很好的使用场景。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/learning-ebpf-observability/en/resources/2figure-2-large-1684250535158.jpg\" /></p><p></p><p>如果你觉得编写Rust代码更舒服的话，aya-rs的维护者提供了一个<a href=\"https://github.com/aya-rs/aya\">Rust开发人员工具链</a>\"，包含一本带有教程的<a href=\"https://aya-rs.dev/book/\">图书</a>\"。书中的样例实现了一个类似的XDP网络流量场景，可以直接从Cargo构建链中运行，使开发过程更加高效。</p><p></p><p><code lang=\"text\">$ git clone https://github.com/aya-rs/book aya-rs-book\n$ cd examples/xdp-hello\n$ cargo install bpf_linker\n$ cargo xtask build-ebpf\n$ cargo build\n\n$ RUST_LOG=info cargo xtask run\n</code></p><p></p><p>样例程序没有跟踪IP地址及其数据包的数量，但是这可以作为一个很好的练习，模仿Go库样例中的行为。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/learning-ebpf-observability/en/resources/2figure-3-large-1684250535158.jpg\" /></p><p></p><p>aya-rs的其他实际用例是持续剖析（profiling），Polar Signals的开发人员将Rust库用到了Parca代理中，用于自动的函数调用栈分析和更好的内存安全性（<a href=\"https://static.sched.com/hosted_files/cloudnativeebpfdayeu22/7f/eBPF%20Day%202022_KubeCon%20EU_%20eBPF%3F%20Safety%20First%21.pdf\">来自KubeCon EU 2022 eBPF日上的幻灯片</a>\"和<a href=\"https://github.com/parca-dev/parca-agent/pull/377\">Pull Request</a>\"）。</p><p></p><p>有不同的方式来着手开发eBPF程序。请记住，该架构遵循将字节码编译的eBPF程序加载到内核，并需要一个用户空间的“收集器（collector）”或“打印器（printer）”。通信是通过套接字或文件句柄进行的。</p><p></p><h2>测试和校验eBPF程序</h2><p></p><p></p><p>在CI/CD流水线中自动化测试eBPF程序是很棘手的事情，因为内核会在加载时验证eBPF程序并拒绝潜在的不安全程序。测试将会需要一个新的虚拟机沙箱，加载eBPF程序，并模拟内核和eBPF程序相关的行为。需求包括触发事件，再次触发eBPF程序代码所订阅的钩子。根据不同的目的，这会涉及到不同的内核接口和系统调用（网络、文件访问等）。创建一个独立的单元测试mock是很难的，需要开发人员模拟一个运行中的内核。</p><p></p><p>有人<a href=\"https://blog.trailofbits.com/2023/01/19/ebpf-verifier-harness/\">试图将eBPF验证器转移到内核之外</a>\"，并允许在CI/CD中测试eBPF程序。同时，在CI/CD中加载eBPF程序需要一个运行中的Linux虚拟机，其CI/CD的runner/executor要具有较高的权限。在Ubuntu 22 LTS中，加载非特权程序默认已被禁用，可能需要通过运行sudo sysctl kernel.unprivileged_bpf_disabled=0来启用。</p><p></p><h3>CI/CD中的持续测试</h3><p></p><p></p><p>为了提供持续测试的CI/CD runner环境，建议使用Ansible/Terraform生成一个Linux虚拟机，安装CI/CD runner，将其注册到CI/CD服务器上，并准备好加载和运行eBPF程序的需求。对于不同的供应商来说，这是一个通用的模式。下面的样例使用Ansible安装并注册GitLab Runner到GitLab.com项目中，然后使用它来构建和运行eBPF程序。GitLab Runner注册了标签ebpf，它将只会执行使用了该标签的CI/CD job。</p><p></p><p><code lang=\"text\">---\n\n- name: GitLab Runner for eBPF\nhosts: all\nvars:\nansible_python_interpreter: /usr/bin/python3\ntasks:\n- name: Get GitLab repository installation script\nget_url:\nurl: \"https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh\"\ndest: /tmp/gitlab-runner.script.deb.sh\nmode: 0744\n- name: Install GitLab repository\ncommand: bash /tmp/gitlab-runner.script.deb.sh\nargs:\ncreates: \"/etc/apt/sources.list.d/runner_gitlab-runner.list\"\nbecome: true\n- name: Install GitLab Runner\napt:\nname: gitlab-runner\nstate: present\nallow_downgrade: true\nbecome: true\nenvironment:\nGITLAB_RUNNER_DISABLE_SKEL: \"true\"\n\n- name: Allow the gitlab-runner user to run any commands as root with sudo -u root\ncommunity.general.sudoers:\nname: gitlab-runner sudo\nstate: present\nuser: gitlab-runner\nrunas: root\ncommands: ALL # Review this for production usage. For demos, it is enabled, and forked MR CI/CD builds won't run.\n</code></p><p></p><p><a href=\"https://docs.gitlab.com/runner/register/\">注册</a>\"需要gl_runner_registration_token变量，该变量来自GitLab项目中针对CI/CD Runners的配置。</p><p></p><p><code lang=\"text\">---\n- name: GitLab Runner for eBPF - register once\nhosts: all\nvars:\nansible_python_interpreter: /usr/bin/python3\ntasks:\n- name: \"Configure GitLab Runner (running to populate config.toml)\"\ncommand: &gt;\ngitlab-runner register\n--non-interactive\n--url \"https://gitlab.com/\"\n--executor \"shell\"\n--tag-list ebpf\n--registration-token=\"{{ gl_runner_registration_token }}\"\n</code></p><p></p><p>GitLab runner可以在项目设置的CI/CD &gt; Runners中看到。</p><p></p><h3>在CI/CD中测试基于Rust的eBPF程序</h3><p></p><p></p><p>我们使用一个实际的eBPF程序来尝试一下CI/CD工作流，这里使用aya-rs Rust库模板作为演示样例。首先，在Linux虚拟机上本地安装Rust和所需的eBPF，以验证一切均能正常运行。</p><p></p><p><code lang=\"text\">curl https://sh.rustup.rs -sSf | sh\nsource \"$HOME/.cargo/env\"\n\nrustup install stable\nrustup install nightly\n\nrustup default stable\nrustup toolchain add nightly\nrustup component add rust-src --toolchain nightly\n\n# required for cargo-generate\napt -y install libssl-dev\n\ncargo install cargo-generate\ncargo install bpf-linker\ncargo install bindgen-cli\n</code></p><p></p><p>接下来，生成一个模板骨架树，用于使用XDP（eXpress Data Path）类型创建一个演示程序。探查ebpf-chaos-demo-xdp/src/main.rs中的代码，并更新网络接口名。然后，构建并运行程序，将日志级别设置为info（或debug）。</p><p></p><p><code lang=\"text\">cargo generate --name ebpf-chaos-demo-xdp -d program_type=xdp https://github.com/aya-rs/aya-template.git\n\nRUST_LOG=info cargo xtask run\n</code></p><p></p><p>示例代码由两部分组成：ebpf-chaos-demo-xdp-ebpf/src/main.rs中的内核空间eBPF程序和ebpf-chaos-demo-xdp/src/main.rs中的用户空间程序，后者会加载eBPF程序并将其附加至内核跟踪点。为了只构建eBPF程序，我们可以调用build-ebpf xtask并使用llvm-objdump命令检查字节码：</p><p></p><p><code lang=\"text\">cargo xtask build-ebpf\n\nllvm-objdump -S target/bpfel-unknown-none/debug/ebpf-chaos-demo-xdp\n</code></p><p></p><p>完整的源代码位于<a href=\"https://go.gitlab.com/QfjbTn\">该GitLab项目</a>\"中，可以使用GitLab CI/CD流水线进行测试。注意，它需要在runner环境中安装Rust工具链。随后的流水线运行将会使用配置好的缓存。该流水线有三个job：</p><p></p><p>install-deps准备Rust环境，这需要将CARGO_HOME变量指定为runner的项目目录。aya-rs-xdp-build-ebpf构建核心eBPF程序，并运行llvm-objdump命令。aya-rs-xdp-run运行用户空间程序，这需要sudo权限。它会将命令放到后台，捕获stdout，睡眠60秒，然后使用pkill来杀死xtask命令，最后打印捕获到的输出。</p><p></p><p>对输出分析进行增强以及思考运行eBPF程序的更多测试报告是留给读者的练习。</p><p></p><p><code lang=\"text\"># eBPF GitLab Runner required for this project\n# Note: Various commands need sudo/root access on the Linux host, see ansible-config/.\n# By default, for security reasons, CI/CD pipelines are not run from forks in the parent project.\n# See https://docs.gitlab.com/ee/ci/pipelines/merge_request_pipelines.html#use-with-forked-projects  \ndefault:\n  tags:\n    - ebpf\n\nstages:\n  - pre\n  - build\n  - run\n\nvariables:\n  RUST_LOG: \"info\"\n  RUNTIME: 300 # set to &gt;= 5*60 = 300s because cargo xtask run also compiles the binary first\n                              \n\n# These steps should not take long after subsquent runs on the Linux VM\ninstall-deps:\n  stage: pre\n  script:\n    - sudo apt install libssl-dev # required for cargo-generate on Ubuntu 22 LTS\n    - curl https://sh.rustup.rs -sSf -o rustup.sh\n    - sh rustup.sh -y --profile default\n    - source \"$HOME/.cargo/env\"\n    - rustup install stable\n    - rustup install nightly\n    - rustup default stable\n    - rustup toolchain add nightly\n    - rustup component add rust-src --toolchain nightly\n    # 'cargo install' is not idempotent. --force takes too long. Treat an error as 'ok, installed' here.\n    - cargo install cargo-generate bpf-linker bindgen-cli || true\n\naya-rs-xdp-build-ebpf:\n  stage: build\n  script:\n    - cd examples/ebpf-chaos-demo-xdp\n    - source \"$HOME/.cargo/env\"\n    - cargo xtask build-ebpf\n    - llvm-objdump -S target/bpfel-unknown-none/debug/ebpf-chaos-demo-xdp\n\naya-rs-xdp-run:\n  stage: run\n  # We need to send the cargo xtask run command into the background, capture stdout, kill it after a defined interval, and generate a test report for CI/CD\n  script:\n    - cd examples/ebpf-chaos-demo-xdp\n    - source \"$HOME/.cargo/env\"\n    - rm ${CI_PROJECT_DIR}/run.pid\n    - nohup cargo xtask run &gt; ${CI_PROJECT_DIR}/nohup.out 2&gt;&amp;1 &amp; echo $! &gt; ${CI_PROJECT_DIR}/run.pid\n    - sleep $RUNTIME\n    - kill -s TERM `cat ${CI_PROJECT_DIR}/run.pid` || true\n    - rm ${CI_PROJECT_DIR}/run.pid  \n    - cat \"${CI_PROJECT_DIR}/nohup.out\"\n    - echo \"Finished running eBPF program. TODO - analyze the output more.\"\n  artifacts:\n    expire_in: 30 days\n    paths:\n      - ${CI_PROJECT_DIR}/nohup.out\n</code></p><p></p><p>该截屏显示了运行eBPF程序的job，以及捕获网络数据包的日志输出。根据对源代码的修改，输出会发生变化并且可以进行测试。一个思路是以机器可读的格式总结捕获到的数据包，并在终止时创建一个汇总表。在CI/CD以及命令行中，这种方式更易于消费和理解。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/learning-ebpf-observability/en/resources/2figure-4-large-1684251660936.jpg\" /></p><p></p><p>将进程放入后台的方法可能无法正确地唤醒它，这可能需要更好的信号处理实现。它远远谈不上完美，你可以在这个<a href=\"https://go.gitlab.com/7xvkFu\">合并请求中看到我的学习历史</a>\"。可能有更好的方式来构建要发布的二进制文件，并通过supervisorctl或systemd命令来启动它，这是下一个学习步骤。终止和卸载过程的实现比较棘手。下面的代码片段实现了正确的信号处理，但是无法始终从运行中的内核卸载已注册的XDP链接。另一种方法是为每次的CI/CD运行生成一个新的Linux虚拟机，以避免这些可重复性相关的失败。但是，其缺点是我们需要一个Rust构建的远程缓存，以避免较长的CI/CD构建运行时间。</p><p></p><p><code lang=\"text\">// Implement signal handling for CTRL+C and SIGTERM\nuse tokio::signal::unix::{signal, SignalKind};\n\n…\n\n    let program: &amp;mut Xdp = bpf.program_mut(\"ebpf_chaos_demo_xdp\").unwrap().try_into()?;\n    program.load()?;\n    program.attach(&amp;opt.iface, XdpFlags::default())\n        .context(\"failed to attach the XDP program with default flags - try changing XdpFlags::default() to XdpFlags::SKB_MODE\")?;\n\n\n    // Implement signal handling for CTRL+C (SIGINT) and SIGTERM\n    // CTRL+C can be used for terminal tests\n    // SIGTERM will be sent from CI/CD jobs to the background process\n    let mut sigterm = signal(SignalKind::terminate())?;\n    let mut sigint = signal(SignalKind::interrupt())?;\n\n    tokio::select! {\n        _ = sigterm.recv() =&gt; { println!(\"SIGTERM shutting down\") }\n        _ = sigint.recv() =&gt; { println!(\"SIGINT shutting down\") }\n    }\n\n    Ok(())\n    // Destroying the bpf object will detach and cleanup the loaded program.\n    // Debug with 'bpftool link show'\n}\n</code></p><p></p><h3>CI/CD和DevSecOps工作流的额外待办事项</h3><p></p><p>剩下的挑战就是扩展eBPF程序以生成测试报告，并创建运行时测试环境，即通过用curl命令运行网络流量测试周期，并验证输出包的确切大小。另外，架构也很重要，要么eBPF程序被加载到内核中，并且有一个用户空间应用来读取其结果，要么eBPF程序是一个单一的二进制文件，直接附加其探针。后者需要在CI/CD job中将程序发送至后台，捕获它的输出，执行测试，然后合并测试报告。对于DevSecOps工作流来说，这个过程还有许多需要改进的地方，但我相信在不久的将来我们会达到最终的目的。</p><p></p><p>代码覆盖是测试eBPF程序的另一个新领域。目前并没有太多的工具帮助开发人员理解代码在Linux内核中运行时的路径，哪些代码区域会受到影响，哪些代码没有被覆盖到。<a href=\"https://www.elastic.co/blog/code-coverage-for-ebpf-programs\">bpfcov是由Elastic的工程师创建的</a>\"，以帮助解决这个问题，让开发人员了解eBPF程序的代码执行路径。在CI/CD中运行自动化的代码质量和安全扫描也是一项挑战：如何确定一个有可能拖慢内核操作的编程错误呢？比较有意思的是，我们可以看一下eBPF程序的持续剖析（continuous profiling）是否可以实现（本身就是使用eBPF的，如<a href=\"https://www.parca.dev/\">Parca项目</a>\"）。还有一些编程模式会规避内核验证器，并造成对软件供应链的安全攻击，通过贡献的拉取和合并请求，将恶意代码注入到已发布的eBPF程序中。这需要<a href=\"https://go.gitlab.com/vvAGaM\">DevSecOps工作流</a>\"来确保安全措施行之有效。AI可能也会提供一些帮助。</p><p></p><h2>结论</h2><p></p><p></p><p>eBPF是一种收集可观测性数据的新方法，它有助于实现网络洞察力，以及安全的可观测性和执行。为了获得最好的库、工具和框架，我们需要一起公开学习，以降低知识的壁垒，并使每个人都能做出贡献。从测试现有的工具到编写eBPF程序的详细教程，我们还有很长的路要走。在CI/CD中进行eBPF程序测试和验证是一项重要的工作，接下来就是将所有的想法带到上游，降低使用和贡献eBPF开源项目的入门门槛。</p><p></p><p>要想开始相关的工作，需要启动一个Linux虚拟机，使用脚本/Ansible进行可重复的设置，并进行测试和开发。当接口名称和内核技术阻碍学习的进度时，那就回退一步，你并没有必要完全理解eBPF的全部内容。当遇到生产环境中断时，对数据收集有一般化的了解能够提供一定的帮助。最后，但同样重要的是，这里有个提示，那就是当调试eBPF程序时，考虑在多个发行版上进行测试，避免遇到内核相关的缺陷。</p><p></p><p>作者简介：</p><p>Michael Friedrich是GitLab的高级开发人员布道者，专注于可观测性、DevSecOps和AI。Michael创建了o11y.love作为可观测性学习平台，并在他的opsindev.news通讯中分享技术趋势以及对day-2ops、eBPF、AI/MLOps的见解。在没有旅行和远程工作的时候，他喜欢搭建乐高模型。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/learning-ebpf-observability/\">Learning eBPF for Better Observability</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/5xvC1Ic6BdLQYju6YWV0\">颠覆传统、应用大爆发，eBPF&nbsp;何以改变 Linux？</a>\"</p>",
    "publish_time": "2023-07-28 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "使用Cloud Studio&Flutter完成全平台博客网站的搭建",
    "url": "https://www.infoq.cn/article/66c2a20169b6cb5906f9db64b",
    "summary": "<p></p><h1>使用Cloud Studio&amp;Flutter完成全平台博客网站的搭建</h1><p></p><p></p><p></p><h1>前言</h1><p></p><p>本文我将使用Cloud Studio 以及Flutter完成自己的一个博客平台的搭建。并且会将该项目作为模版，供大家使用。</p><p></p><p>先来看一下效果</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/511c76a4911f18553d552dbb600f0ff4.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed8211dafd8de7d376e3c1abcab1c46f.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8ae60b9434d140c6daaaf9b84cb472c7.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a5e191b5f571848a34747ce2714e196a.png\" /></p><p></p><h1>一.Cloud Studio</h1><p></p><p>Cloud Studio 是基于浏览器的集成式开发环境(IDE)，为开发者提供了一个永不间断的云端工作站。用户在使用CloudStudio 时无需安装，随时随地打开浏览器就能在线编程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b247a08e076eeb1470247b714e709e3e.png\" /></p><p></p><p>大家也看到了，很多模版以及环境都有提供，大家也都知道我以前是搞Flutter的，于是就先尝试了一下Flutter模版，然后刚开始，可能确实不太会，但熟悉了一会，就发现他的好处了。</p><p></p><p>Cloud Studio 作为在线IDE，包含代码高亮、自动补全、Git集成、终端等IDE的基础功能，同时支持实时调试、插件扩展等，可以帮助开发者快速完成各种应用的开发、编译与部署工作。我将这次的这个博客网站使用Cloud Studio推送到了Gitee，<a href=\"https://gitee.com/jianguo888/flutter_bloc_super\">大家可以访问。</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85f2a1b3b84e59890d93f0dc39af3747.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ae/ae50de82c3f2f13c51a7d46e99800709.png\" /></p><p></p><h1>二.应用场景</h1><p></p><p>Cloud Studio 在线编程工具适用于以下几个场景：</p><p></p><h2>2.1快速启动项目</h2><p></p><p>使用 Cloud Studio 的预置环境，您可以直接创建对应类型的工作空间，快速启动项目进入开发状态，无需进行繁琐的环境配置。</p><p></p><p>下面就是我的工作空间，大家可以下次使用的时候，进入对应的工作空间，就可以继续编写代码，很是方便。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b0040e32a5c689bf98bf92eaf96c58d9.png\" /></p><p></p><h2>2.2实时调试网页</h2><p></p><p>Cloud Studio 内置预览插件，可以实时显示网页应用。当您的代码发生改变之后，预览窗口会自动刷新，这样您就可以在 Cloud Studio 内实时开发调试网页了。</p><p></p><p>下面这个就是我创建的第一个模版项目，你会发现很是方便。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5d27bf7cfaa9f6912300fa558d66a9a2.png\" /></p><p></p><h2>2.3远程访问云服务器</h2><p></p><p>Cloud Studio 支持您连接自己的云服务器，这样就可以在编辑器中查看云服务器上的文件，进行在线编程和部署工作。</p><p></p><p>只有有自己的云服务器，那么你就可以在这里通过配置，很方便的入手开发。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/de863fef3ecba375232895919c03e449.png\" /></p><p></p><h1>三.登录注册</h1><p></p><p>Cloud Studio 在线编程平台支持使用 <a href=\"https://coding.net/\">CODING (opens new window)</a>\"账号和 GitHub 账号，以及微信登录，可以在<a href=\"https://codingcorp.cloudstudio.net/api/public/login\">登录 (opens new window)</a>\"界面输入相应的账号登录前往 Web IDE，这里我用的是微信登录。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6aa9b20bbbcf929b84e3446d7b2f99c3.png\" /></p><p></p><h1>四.工作空间的创建与使用</h1><p></p><p>一个工作空间是一个虚拟计算单元，它包含独立的存储、计算资源以及开发环境。Cloud Studio 是以工作空间来组织的，本文为您介绍如何创建工作空间。</p><p></p><h2>4.1创建工作空间</h2><p></p><p>进入 Cloud Studio 云端 IDE，可以通过两种方式创建工作空间，第一种方式：点击模板直接创建工作空间，第二种方式：单击【新建工作空间】，进入工作空间创建页面</p><p></p><h3>4.1.1填写工作空间信息</h3><p></p><p>第一种方式点击模板创建工作空间，可自动生成工作空间名称，并运行模板的预置环境及样本代码。这里我用的是Flutter。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c8704f10d1e27ed91ea968e7155ba8c.png\" /></p><p></p><p>第二种方式，选择创建工作空间，然后选择预置环境，填写工作空间名、描述，并选择运行环境和代码来源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f308a840eec389da75501c2df70c2869.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/043f6130fff31544bdd1b464cccce74a.png\" /></p><p></p><p>工作空间名：您的工作空间的唯一标识，只能由字母、数字、下划线（_）、中划线（-）、点（.）组成，不能包含空格或其它字符。描述：对该工作空间作用的描述。运行环境：工作空间内代码运行的环境，您可以选择预置环境，包含 Ubuntu、Python、Java 和 Node.js 四种；也可以选择将其连接到自己的云服务器上，代码来源：工作空间内的代码来源，此处我们选择“空”，即不添加任何代码。</p><p></p><p>单击【创建】按钮，即可完成工作空间的创建。您还可以创建代码来自于 Git 仓库的工作空间，代码会被自动克隆到工作空间</p><p></p><h2>4.2工作空间的使用</h2><p></p><p>您可以在 Cloud Studio 云端 IDE 的工作空间内存放自己的项目代码，安装所需要的软件环境，运行或编译项目，本文为您介绍如何使用工作空间。</p><p></p><p>注意：</p><p></p><p>数量限制：目前每个用户最多可以创建 10 个工作空间，并且只能同时运行一个工作空间，如果您需要打开另一个工作空间需要先关闭当前运行中的工作空间。时间限制：每个用户每月可以免费使用工作空间共 3000 分钟，超出时间将产生扣费（连接云主机的工作空间无此限制）。</p><p></p><h3>4.2.1工作空间界面简介</h3><p></p><p>工作空间是我们主要的工作区域，主要由顶部菜单栏、左侧操作面板、右侧代码编辑区和底部状态栏组成。</p><p></p><p>您可以根据自己的习惯设置界面外观、偏好，安装自己需要的插件。</p><p></p><p>需要注意的是，您的偏好设置和插件在每个工作空间中是互相隔离的，也就是说您可以给不同的工作空间设置不同的偏好，安装不同的插件。这里面大部分和你在本地使用vscode是一样的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85f2a1b3b84e59890d93f0dc39af3747.png\" /></p><p></p><p>我们可以通过终端来进行这些操作，点击菜单栏--终端--新终端，会在底部打开一个面板，点击【终端】切换到终端。</p><p></p><h3>4.2.2管理工作空间</h3><p></p><p>在 Cloud Studio 云端 IDE 的工作空间列表页面，您可以运行、停止、删除和恢复工作空间。</p><p></p><h4>运行</h4><p></p><p>单击对应的工作空间卡片，就会在新的页面打开并运行该空间，此时该工作空间卡片上会显示“运行中”状态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90543a9c03994794b43a378679dccad2.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/4858759ef857ce8e1b795a5243434928.png\" /></p><p></p><h4>停止</h4><p></p><p>对于处在“运行中”状态的工作空间，单击卡片右边的【停止】，就可以停止运行该工作空间。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc7b1dd20510c97b5ab8235d14e954f3.png\" /></p><p></p><h4>删除</h4><p></p><p>您可以删除未运行的工作空间，单击工作空间卡片右下角的【删除】即可删除。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc00a0322626716ae455d02c8ea4019b.png\" /></p><p></p><h4>恢复</h4><p></p><p>为了防止误删除，已删除的工作空间会展示在下方“已删除的工作空间”列表中，保留24小时。在此之前您可以随时单击【恢复】，还原您的工作空间，超过 24 小时未恢复的工作空间将被永远销毁。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f90218cbd97182a1085b5511851001a.png\" /></p><p></p><h1>五.使用 Git 进行版本控制</h1><p></p><p>Cloud Studio 云端 IDE 的工作空间支持从代码仓库创建，不过在此之前您需要将工作空间的 SSH Key 添加至对应代码托管平台的个人公钥列表。</p><p></p><h2>5.1Cloud Studio 查看SSH公钥</h2><p></p><p>这里我们点击个人头像，打开系统设置，里面有SSH公钥，然后我们把密钥复制，添加到Gitee</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8b3dbb779b8c7ad5b17ceabc83aa6d66.png\" /></p><p></p><h2>5.2Gitee添加SSH公钥</h2><p></p><p>在下图，添加SSH公钥，补充标题和公钥</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/066b76745a7e72c555fb3d20f9f03e5a.png\" /></p><p></p><h2>5.3Gitee上新建一个仓库</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/fb/fbea260cb81fb12c4220c91fdac51aea.png\" /></p><p></p><p>在我们的云IDE的工作空间里，打开终端。</p><p></p><h2>5.4Cloud Studio配置邮箱和密码</h2><p></p><p><code lang=\"text\">git config --global user.name \"坚果\"    \n\ngit config --global user.email \"852851198@qq.com\"                                                            \n</code></p><p></p><h2>5.5Cloud Studio提交代码</h2><p></p><p>然后初始化仓库，提交修改，添加commit信息，然后推送</p><p></p><p><code lang=\"text\">git init\ngit remote add origin git@gitee.com:jianguo888/flutter_bloc_super.git\ngit add .\ngit commit -s -m '初始化'\ngit push origin master\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8ed19208fb1f57b2351bebee4e33a44.png\" /></p><p></p><h1>六.Flutter博客网站的开发</h1><p></p><p>Flutter 是谷歌的移动UI框架，Flutter 最近发布了 Flutter V3.10.6，可以快速在 iOS、Android、Web 等多平台上构建高质量的原生用户界面。 Flutter 可以与现有的代码一起工作。在全世界，Flutter 正在被越来越多的开发者和组织使用，并且 Flutter 是完全免费、开源的。 目前 Cloud Studio 云端 IDE 支持 Flutter Web 应用开发。这就是为什么今天我们使用在 Web、macOS 应用、Android 和 iOS 应用上运行的 flutter 创建响应式博客主题。</p><p></p><h2>6.1创建项目</h2><p></p><p>打开云IDE之后，创建一个Flutter项目，当前，我使用的是 Flutter 3.0.1</p><p></p><p>创建完成之后，我们就可以编写代码</p><p></p><p>首先打开云IDE，选择创建项目</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d317bed09fff4e71284a945099afd22e.png\" /></p><p></p><p>然后这里我们给自己的项目命名</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9c9ed84c330c6420691ea4f162437a3.png\" /></p><p></p><p>等待项目加载完成</p><p></p><p>然后运行下面的这行命令</p><p></p><p><code lang=\"text\">cd ./ &amp;&amp; flutter pub get &amp;&amp; flutter run -d web-server --web-port 9000  --web-hostname 0.0.0.0 &amp;&amp; echo success\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1fcd65cb5f8ec9ee7af6e4595aaeaba.png\" /></p><p></p><p>我们可以选择打开内置浏览器或者浏览器</p><p></p><p>这里我选择打来浏览器，大家可以看到这个项目运行成功。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/02bcba9297a2e885e9c965e7c7532af4.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2642d840b66c86e8355f0b38dfc2c13.png\" /></p><p></p><p>这个时候，说明我们的环境是ok的。我们可以后面的工作了</p><p></p><h2>6.2.打开端口面板实时预览调试</h2><p></p><p>点击最右边的按钮弹出预览页面。</p><p></p><p>看到这些红色的文字 To hot restart changes while running, press \"r\" or \"R\". 说明项目编译好了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7e/7e9ea61773e26b2776df3f8db4abc079.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c4604a77560e3237560bdde9916e0ec.png\" /></p><p></p><p>修改代码重新编译</p><p></p><p>点击终端， 按 r 键即可重新编译， 再按预览页面的刷新按钮即可看到实时修改后的效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4ac845d266d5f4a60b5b1393d6be95e4.png\" /></p><p></p><p>目前 Flutter Web 应用不支持热更新，需要手动刷新页面。要项目编译完成才能代码预览页面， 否则会一直卡在 Loading 界面。一直卡在 Loading 界面可尝试刷新预览界面。</p><p></p><h2>6.3发布web版</h2><p></p><p>我们希望你完成迁移后尽快将其发布，可以作为预览版：</p><p></p><p>参考文章：https://dart.cn/null-safety/migration-guide</p><p></p><p>细心的小伙伴可能会发现,安卓有android文件夹, iOS 有ios的文件夹,但目前目录结构是没有web文件夹的,</p><p></p><h3>6.3.1. 创建web文件夹</h3><p></p><p>输入下面的命令创建web文件</p><p></p><p><code lang=\"undefined\">flutter create .\n</code></p><p></p><p>然后就会创建一系列web相关的文件 ,如下图, 目录结构也会多一个web的文件夹. 如下图</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d3507c2894f6d43299fe9f103665873a.png\" /></p><p></p><h3>6.3.2. 打包web版本</h3><p></p><p>我们知道要给android手机用,需要打包apk出来, 要给iPhone手机用,需要打包ipa出来;同样的道理要给浏览器用,也需要打包web相关代码.</p><p></p><p><code lang=\"undefined\">flutter build web --web-renderer html\n\nflutter build web \n\nflutter build web --web-renderer canvaskit\n</code></p><p></p><p>这将生成包括资源的应用程序，并将文件放入项目的 /build/web 目录中。</p><p></p><p>一般的应用程序的 release 版本具有以下结构：</p><p></p><p>content_copy</p><p></p><p><code lang=\"none\">/build/web\n  assets\n    AssetManifest.json\n    FontManifest.json\n    NOTICES\n    fonts\n      MaterialIcons-Regular.ttf\n      \n    <img />\n  index.html\n  main.dart.js\n  main.dart.js.map\n</code></p><p></p><p>启动 Web 服务器（例如，python -m SimpleHTTPServer 8000，或使用 <a href=\"https://pub.flutter-io.cn/packages/dhttpd\">dhttpd</a>\" package），然后打开 /build/web 目录。在浏览器中访问 localhost:8000（前文用 Python 启动的服务器）以查看应用程序的 release 版本。</p><p></p><p>经过测试,上面三种方式都可以打包web版本, 其中第一种是针对移动端的打包方式, 第二种是一般的打包方式, 第三种是针对pc端的打包方式.</p><p></p><p>那这3种方式打包出来,运行起来有什么不同呢</p><p></p><p>flutter build web --web-renderer html 打开速度最快,兼容性好(是指ie,chrome,safari等浏览器兼容)</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d44130dda9b176797ada90b434bc682e.png\" /></p><p></p><p>flutter build web 打开速度一般,兼容性好</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3a8ea748f8bfe5a34160bdf6640613ae.png\" /></p><p></p><p>flutter build web --web-renderer canvaskit 打开速度最慢,兼容性好</p><p></p><h2>6.3.3结论</h2><p></p><p>就是使用第一种打包方式会比较好</p><p></p><p><code lang=\"undefined\">flutter build web --web-renderer html\n</code></p><p></p><h2>6.4常见问题</h2><p></p><p></p><h3>坑1:  找到了index.html,用浏览器打开一片空白</h3><p></p><p>这个属于正常的, 这个不像前端web ,html css js那套,点击index.html就能访问的.  在flutter里面是不能直接访问的,一定要放到容器里面去才能访问,如:tomcat等</p><p></p><h3>坑2:  已经用nginx代理,用浏览器打开还是一片空白</h3><p></p><p>那是因为文件路径引用不对.解决办法有2种方法1:用编辑器打开index.html,能看到源文件,把,改成</p><p></p><p>方法2:用编辑器打开index.html,能看到源文件,把,改成你服务器的路径比喻说:</p><p></p><p>然后nginx代理</p><p></p><p><code lang=\"dart\">  #flutter\n    server {\n       listen       251 ;\n       server_name  flutterblog;\n       location / {\n           root   /root/study/flutter/web/;\n           index  index.html index.htm;\n        #    proxy_pass   http://127.0.0.1:12345;\n        #    access_log  /usr/local/nginx/logs/go.101.log ;\n\n       }\n    }\n</code></p><p></p><p>撒花</p><p></p><h1>七.自定义模板</h1><p></p><p>自定义模板是 Cloud Studio 云端 IDE 推出的面向团队模板能力的功能。该功能支持将当前项目作为自定义模板，能够覆盖到 Git 仓库的项目、普通项目、示例项目等，很大程度上提高了团队标准化代码开发环境的一大诉求。</p><p></p><h2>7.1自定义模板功能简介</h2><p></p><p>当前自定义模板实现的功能主要有四个方面，创建、发布、分享和管理自定义模板。</p><p></p><h2>7.2创建自定义模板</h2><p></p><p>当您处在当前项目 IDE 中，您可以创建自定义模板：</p><p></p><p>这里我把我的Flutter 博客网站发布成模版。</p><p></p><p>（1）点击功能栏中的“文件”，在下拉选项中选择“发布自定义模板”；</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f1/f1f1359c495b50f4e438c8fb3b0652af.png\" /></p><p></p><p>（2）右侧布局窗口中会自动打开新标签页，可以选择您心仪的图标和标签，以及填写您模板的描述</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/7218f66b9cde2919168ca0e49953c80c.png\" /></p><p></p><p>点击完成</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c6/c6e13b81d07edc9d67ad35b71c6b1842.png\" /></p><p></p><h2>7.3发布自定义模板</h2><p></p><p>当您成功填写完自定义模板信息后，您可以进行自定义模板发布：</p><p></p><p>（1）点击“完成”即可发布您的自定义模板；</p><p></p><p>（2）在分享前点击“再次发布”，可以修改您的发布信息后再次分享，分享链接无变化，根据原模板已创建的 IDE 实例不受影响；</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/064ba2459af6cb6d91771b6644c3ce93.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/1696749275196cf513b7a439dd19fdeb.png\" /></p><p></p><h2>7.4分享自定义模板</h2><p></p><p>当您的模板发布成功后，您可以有两种方式分享自定义模板：</p><p></p><p>（1）进入分享页，复制您的自定义模板链接，分享给您的伙伴；</p><p></p><p>坚果（个人）分享了「Flutter Blog」模板 https://cloudstudio.net/templates/r9IAX1JuTF2</p><p></p><p>（2）通过嵌入 Markdown 徽章进行分享，将模板徽章嵌入 README 文件或者博客中，您的伙伴点击徽章即可获取模板；</p><p></p><p><img src=\"https://static001.infoq.cn/static/write/img/img-copy-disabled.4f2g7h.png\" /></p><p></p><p>至此，我们的一整个流程就完成了。</p><p></p><h1>总结</h1><p></p><p>通过这一次的一个体验过程，我总结了一下几个优势：</p><p></p><p>Cloud Studio 作为 Web IDE/在线 IDE/Cloud IDE，和本地 IDE 相比具有以下优势：</p><p></p><p>无需安装，跨平台：只要有浏览器就可以使用；预置常用环境，无需手动安装；支持创建网页预览，在线开发调试。全功能：无需下载安装，随时随地开发编码，拥有媲美本地 IDE 的流畅编码体验。多环境：内置 Node.js、Java、Python 等常见环境，也可以连接到云服务器进行资源管理。兼容 VS Code 插件：若默认的配置无法满足需求，可以在线安装 VS Code 插件来增强使用体验。持久化和快速加载：随开随写，随时保存，再也无需担心断电未保存，不浪费您的每一份灵感。</p><p></p><p>在我的体验下，概括来说就是Cloud Studio 是用来开发中小型项目，在线修改代码，或者连接云服务器进行部署工作的不二之选。真正的达到了一键秒开、全持久化、预置环境及内置开发工具，跨团队无缝复制和共享，让开发化繁为简。</p>",
    "publish_time": "2023-07-28 09:01:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "科技领导者的崭新征途：聚焦业务协作，多维能力支撑",
    "url": "https://www.infoq.cn/article/fI93WehedVlcYx4BCBq0",
    "summary": "<p>随着数字经济的高速发展，中国科技领导者在经济高质量发展中扮演着至关重要的角色。在不确定性快速上升的时代和企业期待的转变中，他们作为中国数字经济发展的推动者和护航者，正面临着一场崭新的征途。在此背景下，InfoQ研究中心与TGO鲲鹏会共同组织发起了针对科技领导者的调研工作，制作并发布了《中国科技领导者画像研究报告》，在报告中详细拆解了科技领导者面临的时代和企业期望的变化，以及科技领导者自身的成长路径。</p><p></p><p>VUCA时代下，科技创新成为企业发展的核心动力</p><p>在复杂且不确定的VUCA时代中，科技的力量超出了传统的经济和商业的期待。越来越多的企业意识到科技赋能业务的重要性，需要科技创新提供更多解决方案，并带来新的希望和机遇。为了应对这种变化，数据驱动与敏捷迭代两方面的能力成为科技领导者应对时代挑战并推动企业发展的必备能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b3c8c90d56162e69ce7a9b85513e490.png\" /></p><p></p><p>企业期待科技领导者同时具有技术能力与领导能力</p><p>科技领导者们的企业内在价值在不断提升，企业也越来越认可科技领导者所带来的技术价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d7fa04c388fcd144afd090876632ba05.png\" /></p><p></p><p>出于这份认可，互联网行业科技领导者价值被逐渐推高并进入追捧期。在经历过市场追捧期后，非互联网行业的企业更加理性地评估CTO、CIO等职位的价值、设置与人选。企业不仅仅期望科技领导者的加入能够带来技术升级，也期望能够影响并带动生产、运营等其他团队。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97b4ff91c44081197f5af0b49ff4b47c.png\" /></p><p></p><p>科技领导者不但要有相匹配的技术能力，还要具备相应的管理意识与业务认知，能够让企业的需求切实落地。</p><p>InfoQ研究中心发现，数字化程度越高的企业越是需要科技领导者具有技术战略高度视野、前沿技术理解力、技术赋能力、深度技术落地理解力。内部业务协同多样、外部服务团队多样、内部团队规模大的企业对科技领导者的团队领导能力要求越高。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e2/e2ddbcd1e1b25195f72cc1fafea638f6.png\" /></p><p></p><p>双重变化下，科技领导者画像升级</p><p>通过对时代背景和企业期望的分析，InfoQ研究中心将科技领导者人群画像升级为四大能力要求。</p><p>一是数据驱动。他们需要具备业务决策能力和实践领导力，应用数据的采集和业务流程分析，通过合适的技术、演进方案、人员成本来解决企业的问题。</p><p>二是敏捷迭代。这对科技领导者在团队内部的领导力提出了更高的要求。科技领导者需要向团队澄清目标并辅助团队成长。在团队软性文化打造上发力，为团队带来心理安全感与工作责任感，并且通过精准的个体辅导增强团队凝聚力。</p><p>三是边际融合。在各部门之间，科技领导者需要以更扎实的业务成长方法论与业务领导力，聚焦业务需求、帮助业务创新、辅助业务部门决策并协调团队之间的合作。</p><p>四是协同共生。这意味着科技领导者需要具备协作领导力。通过与内部、外部合作伙伴，如供应商、客户、合作伙伴等，建立良好的合作关系并让合作伙伴清晰地理解正在进行的工作，指导企业制定相应的技术战略和业务发展计划。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a3906168210f0ce9a3f21f7149a54aa3.png\" /></p><p></p><p>在快速变化的局势和企业的多维度期待下，科技领导者必须让业务领导力、团队领导力、个人领导力得到均衡地成长，并同时沿着技术成长、业务成长、管理成长的路径进行发展，聚焦于通用能力。不断增长的视野与能力的提升将成为他们应对时代湍流的强大武器，助力他们引领企业迈向更加光明的未来。</p><p></p><p>中国科技领导者画像正在经历着非常微妙的变化，想要了解更多中国科技领导者的人群画像、成长路径等内容，欢迎点击链接或扫描二维码添加小助手下载完整报告：<a href=\"https://www.infoq.cn/article/3amBoEYUc5mjsEP3VAVG\">https://www.infoq.cn/article/3amBoEYUc5mjsEP3VAVG</a>\"</p><p><img src=\"https://static001.geekbang.org/infoq/70/70d97552604ebf82671669e143cb27e1.png\" /></p><p></p>",
    "publish_time": "2023-07-28 10:47:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里巴巴前副总裁胡臣杰加入开源中国，推动中国开源文化蓬勃发展",
    "url": "https://www.infoq.cn/article/R6PDcN25QOuVEFYhVmFR",
    "summary": "<p>中国知名开源技术社区开源中国日前宣布，前阿里巴巴副总裁胡臣杰加入该公司，担任首席战略官一职。胡臣杰作为“国千”专家，拥有丰富的数字化工作经验和广泛的行业影响力，将在开源中国的发展战略规划及进一步推动开源文化的发展方面发挥重要作用。</p><p></p><p>在过去的职业经历中，胡臣杰曾担任阿里巴巴集团副总裁、南方航空公司首席信息官等职务。在阿里巴巴任职期间，他先后负责阿里云新零售行业线、阿里云企业战略合作事务和飞猪大交通业务等重要领域，并创办了阿里CIO学院，致力于推动开源文化在大中型企业的普及与应用。</p><p></p><p>胡臣杰在南方航空公司担任首席信息官时，深刻认识到开源技术在企业信息化建设中的巨大潜力。他坚定地采用开源软件，不仅降低了成本，还提升了团队能力，为公司节省了数千万的IT投资。此外，他曾推动实施了中国第一张电子客票、电子登机牌和电子货单等创新项目，引领了中国民航行业的数字化创新。</p><p></p><p>他表示，开源技术对于整体团队能力的提升有巨大的好处，开放的源代码让年轻人有机会深入学习和钻研。例如，南方航空曾经有一名工程师通过深入研究开源软件负载均衡方案，成功替代了昂贵的商业设备，既节约了投资，又增强了团队的技术实力。</p><p></p><p>加入开源中国后，胡臣杰将致力于推广开源技术在大中型企业和政府机构的应用，并负责开源中国与资本市场的资源对接与整合工作。同时，他还希望推动开源文化在各个领域的普及，促进开源技术与数字化转型的有机结合。</p><p></p><p>开源中国作为中国具影响力的开源软件社区平台，近日宣布完成B+轮战略融资。该社区于2008年创立，收录全球知名开源项目近10万款，并于2022年发布中国开源社区Landscape，收录200+开源社区；同时还收购了日本老牌开源社区OSDN。</p><p></p><p>开源中国旗下的代码托管平台Gitee是国内领先的代码托管服务平台，服务1000万开发者用户、26万家企业，以及2000多家高等院校。</p><p></p><p>通过胡臣杰的加入，开源中国期待进一步推动开源文化的传播与应用，为中国企业的数字化转型提供更多支持与引领。</p>",
    "publish_time": "2023-07-28 11:01:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "比Bing更早将LLM集成到搜索引擎中，这家由谷歌前高管创立的公司为什么还是失败了？",
    "url": "https://www.infoq.cn/article/IQrajVXjyLTw1QjFxHV5",
    "summary": "<p></p><blockquote>Neeva 更快、更简单且无广告。但做出比谷歌更好的东西，却并不足以击败谷歌。</blockquote><p></p><p>&nbsp;</p><p>在出走谷歌，创立 Neeva 以前，Sridhar Ramaswamy 曾在谷歌工作过 15 年，担任广告业务高级副总裁一职；Vivek Raghunathan 曾担任 YouTube 货币化副总裁。Ramaswamy 认为，过度依赖广告会损害搜索结果的质量，于是在 2019 年，Ramaswamy 和 Raghunathan 联合创立了搜索引擎&nbsp;Neeva。Neeva 的使命是让搜索服务回归用户，旨在通过提供优先考虑用户隐私并提供无广告体验的替代方案来颠覆搜索引擎市场。</p><p>&nbsp;</p><p>Neeva 从头开始构建了一个搜索堆栈，并组建了一个 50 人的小团队。Neeva 曾在短期内迅速吸引了大量用户，在推出后四个月内月活跃用户增长到 50 万。到 2022 年初，Neeva 已经将大语言模型集成到其搜索堆栈中，成为第一个为大多数查询提供引用的实时人工智能答案的搜索引擎。有人曾将 Neeva 看作是谷歌最强有力的竞争者。Neeva 在某些方面的确远远领先于谷歌，比如将 10 个蓝色链接替换为更直观的页面，并强调人工创建的信息。</p><p>&nbsp;</p><p>但构建搜索引擎实际上是最容易的部分。Ramaswamy 和 Raghunathan 在宣布 Neeva 关闭的博客文章中写道：“在整个旅程中，我们发现构建搜索引擎是一回事，说服普通用转向更好的选择则完全是另一回事”。Ramaswamy 和 Raghunathan 称，他们在吸引新用户方面面临着重大挑战，再加上所有公司目前面临的困难经济环境，意味着继续目前的路线已不再可行。 他们写道：“在消费者搜索领域创建可持续业务已不再可行。” “因此，在接下来的几周内，我们将关闭 neeva.com 和我们的消费者搜索产品，并转移到新的重点领域。”</p><p>&nbsp;</p><p>仅仅四年，Neeva 就停止运营了。2023 年 5 月，云数据库公司&nbsp;Snowflake 以约 1.5 亿美元收购了&nbsp;Neeva，未来将帮助服务企业客户利用 AI 去快速搜索和分析数据点、数据资产，获得数据洞察的能力。</p><p></p><h2>Neeva：构建自有搜索索引，首批集成AI功能</h2><p></p><p>&nbsp;</p><p>Neeva的计划来自一个简单的想法：谷歌的商业模式已经在拖累搜索引擎的进步。Ramaswamy认为，从长远来看，这种以广告为基础的模式必然导致搜索结果劣化。要想打造更好的搜索引擎，首先需要改变激励措施。这种改变意味着不再以展示广告为诉求，而是始终把用户体验放在第一位。这种新模式不需要让用户输入查询，也不需要帮广告商收集用户数据。其目标就是帮助人们找到自己想要的页面，并避开途中的一切障碍。</p><p>&nbsp;</p><p>David Pierce&nbsp;在一篇分析文章中指出：“从零开始构建搜索引擎既困难又昂贵，因此很多人对此根本不感兴趣。他们选择以10到25美元的价格购买 Bing 提供的 1000 条数据搜索许可，再以此为基础添加自己的功能和界面。”而 Neeva 对于彻底改革搜索技术有很多自己的想法，因此最终决定要控制底层数据。</p><p>&nbsp;</p><p>Raghunathan 表示，“我们想要加快搜索速度、充实预览内容、提供首选网站、开放个人搜索选项，但这一切都遇到了困难。”Bing API 提供的链接并不支持这些额外功能，所以Neeva的思路就成了空想。如果真想打造一套更好的搜索引擎，那Neeva就必须得亲自动手、从零起步。</p><p>&nbsp;</p><p>经过两年的构建、训练、完善、再训练和再完善，Neeva搜索引擎终于建立起完全自主的技术基础。构建自有搜索索引的一大优势就是，能给大语言模型收集到一组非常实用的训练数据。Neeva还是首批推出AI搜索助手（名为NeevaAI）的公司之一，它能总结搜索结果，有时甚至直接在页面顶端回答用户的问题。</p><p>&nbsp;</p><p>Neeva团队还建立起了带有更大图像和比较信息的购物页面，这里优先参考了Reddit和Quora等平台的结果。体育搜索也变成了漂亮的全屏记分牌。之所以这样做，就是希望大家在搜索“布拉德·皮特IMDb”或“WhatsApp Web版”时，Neeva的自动补全功能会直接将用户带入网站，压根不需要中间的结果页面。Neeva干净、简单，早期用户纷纷表示这种不骗人看广告的搜索引擎才是好引擎。</p><p>&nbsp;</p><p>但打造一款好产品和让用户喜欢上它完全是两码事。毕竟Neeva的使用体验太不同了，用户得放弃自己上网时最简单、也最根深蒂固的习惯，才能适应这种全新设计。</p><p>&nbsp;</p><p>科技行业一直有个原则，即人们不会愿意改变自己的使用习惯。Ramaswamy在采访中坦言，“我们面临的最大障碍之一，确实就是扭转用户的固有习惯。人们忘记了谷歌的成功不仅仅是开发出了更好的产品。为了实现目标，我们必须做出一系列精准的分发决策。”</p><p>&nbsp;</p><p>据报道，谷歌每年向苹果支付高达150亿美元，为的就是能在各类苹果设备的Safari浏览器中成为默认搜索引擎。谷歌同时也向Mozilla支付费用，借此成为Firefox浏览器中的首选搜索引擎。而这笔费用高达每年4.5亿美元。谷歌还跟其他设备制造商和浏览器开发商有合作，甚至跟电信运营商也有类似的交易。据《华尔街日报》报道，三星曾在2023年短暂考虑结束与谷歌的交易，但由于各种原因而最终放弃，其中包括“可能对与谷歌间的广泛业务关系产生影响”。</p><p>&nbsp;</p><p>谷歌的真正优势在于旗下的其它产品。Android是目前全球最受欢迎的移动操作系统，市场份额约占78%。Chrome则是最受欢迎的网络浏览器，市场占比约62%。在这两大平台上，谷歌自然也成为不可撼动的默认搜索引擎。</p><p></p><h2>做搜索引擎，既复杂，又简单</h2><p></p><p>&nbsp;</p><p>搜索引擎是种神奇的事物——既复杂无比，又简单纯粹。</p><p>&nbsp;</p><p>实际上，搜索引擎所做的就是编译网页数据库（即「搜索索引」），之后在每次收到查询时浏览该数据库，从中提取并交付质量最高、相关度最强的一组页面。但这过程中的每一步，都涉及着巨大的复杂性，需要做出一连串权衡。而权衡的核心有二：时间与金钱。</p><p>&nbsp;</p><p>即使创业者能建立一套不断更新的数据库，囊括互联网上的数千亿个页面，但光是它产生的存储和带宽成本就足以让地球上任何一家巨头企业破产。这还不包括每天对数据库执行无数次检索的成本。另外，搜索响应中的每一毫秒都非常重要——谷歌会在结果上方显示每次查询耗费的时间。总而言之，创业者恐怕没有足够的时间逐个查看整个数据库。</p><p>&nbsp;</p><p>此外，搜索引擎的构建还要从一个基本哲学问题开始：什么叫高质量网页？创业者必须决定哪些分歧是合理的，而哪些信息属于纯粹的胡说八道，必须搞清广告占比到多少才不会过度。那些由 AI 编写且充斥着 SEO 垃圾的网站当然不好，但个人认真撰写、且同样充斥 SEO 垃圾的美食博客则还不错。</p><p>&nbsp;</p><p>一旦完成了上述讨论并设定出明确的边界，那搜索引擎中就基本确定了需要保留的几千个域名。其中包括 CNN 和 Breitbart 等新闻网站，Reddit、Stack Overflow 和 Twitter 的热门讨论板，维基百科和 Craigslist 等工具服务，YouTube 和 Amazon 等服务平台，还有各类最顶级的食谱/体育/购物网络。有时候，创业者可以跟这些网站洽谈合作，以结构化方式直接获取数据，不再单独浏览各个页面。值得一提的是很多大平台都有专门的团队，有时甚至愿意免费配合。</p><p>&nbsp;</p><p>之后就该放出爬虫了。这些机器人能爬取给定网页上的内容，之后查找并跟踪页面上的各个链接、索引全部页面内容，就这样完成链接、索引的查找与跟踪循环。而每次爬虫访问一个页面时，都会根据之前设定的高质量网页标准对其做评估。被认定为高质量的内容将被下载至某台服务器上，于是搜索索引开始迅速膨胀。</p><p>&nbsp;</p><p>当然，爬虫也不是在哪里都受欢迎。爬虫每次打开网页，都会给内容提供商带来带宽成本。现在想象一下，一套搜索引擎每秒都会对网站上的各个页面进行加载和保存，这样的更新成本将很快超出提供商的承受能力。</p><p>&nbsp;</p><p>因此，大多数网站都设置一个名为 robots.txt 的文件，用于定义哪些爬虫可以访问其内容、哪些爬虫不行，以及允许爬虫爬取哪些 URL。从技术上讲，搜索引擎完全可以不理会 robots.txt 上的规则，但这是 Web 结构和文化中的一部分。几乎所有网站都愿意接纳谷歌和 Bing，因为它们带来的可发现性已经超过了带宽成本。也有很多人会阻止特定的服务商，例如不希望亚马逊爬取并分析他们的购物网站。其他人则制定一揽子规则：除了谷歌和 Bing 外，其余爬虫概不接待。</p><p>&nbsp;</p><p>很快，爬虫就会带回相当广泛的互联网快照。接下来的工作就是针对搜索引擎可能收到的每条查询，按顺序对全部页面做排名。大家可以按主题对页面做排序，这样就能划分成更小、更易于搜索的索引，而不是包罗万象的庞然大物。简单来讲，就是本地结果与本地结果匹配，购物与购物匹配，新闻与新闻匹配。我们需要使用大量机器学习技术来收集特定页面的主题和内容，同时也离不开人工协助。</p><p>&nbsp;</p><p>此外，还会引入评分团队，向他们展示查询和结果，并要求他们从0到10为结果的真实性打分。有时候问题很明显，如果有人搜索「Facebook」，但响应结果的第一条居然不是facebook.com，那肯定不能接受。但大多数情况下，我们会合并来自大量输入的评分，并将其馈送到索引和主题模型当中，之后不断重复这个过程。</p><p>&nbsp;</p><p>到这里，问题才刚刚解决了一半。我们还得提高所谓“查询理解”能力，也就是意识到搜索“巨石强森”和搜索“道恩·约翰逊”的人其实是想找同样的信息。最终，我们将积累起一个庞大的同义词和相似性库，并据此重写查询以降低搜索难度。而且如谷歌所说，每天他们的引擎中都有15%的全新搜索，所以这场理解人们真实需求和扩充新知识的赛跑将永远没有终点。</p><p>&nbsp;</p><p>一段时间之后，搜索引擎正式上线了，开始获得更多人的关注、点击和偏好。这里还有一项黄金标准：如果用户在点击链接后，不再立即搜索和点击其他链接，就代表当前结果的质量令人满意。而另一方面，用户们的点击量越大，就越能了解他们真正想要的是什么。</p><p>&nbsp;</p><p>此外，运行搜索引擎还需要不断在速度、成本和质量三者中取得平衡。比如，当有人输入“YouTube”并按下回车时，如果搜索整个数据库会耗费太长时间、造成不必要的带宽和存储成本；如果保留一个容纳整个互联网的数据库，不但存储成本高昂，搜索速度也会太过缓慢；如果设定只显示网络上最受欢迎的100个网站，就能保证速度和成本，但会存在内容不全面、质量不可靠的情况。同时，各个网站本身也在不断变化，搜索引擎的爬虫和排名系统也要持续跟进。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theverge.com/23802382/search-engine-google-neeva-android\">https://www.theverge.com/23802382/search-engine-</a>\"<a href=\"https://www.theverge.com/23802382/search-engine-google-neeva-android\">google</a>\"<a href=\"https://www.theverge.com/23802382/search-engine-google-neeva-android\">-neeva-android</a>\"</p><p><a href=\"https://www.snowflake.com/blog/snowflake-acquires-neeva-to-accelerate-search-in-the-data-cloud-through-generative-ai/\">https://www.snowflake.com/blog/snowflake-acquires-neeva-to-accelerate-search-in-the-data-cloud-through-generative-ai/</a>\"</p><p><a href=\"https://techcrunch.com/2023/05/21/google-challenger-neeva-gives-up-on-consumer-search-goes-all-in-on-ai-and-the-enterprise/\">https://techcrunch.com/2023/05/21/</a>\"<a href=\"https://techcrunch.com/2023/05/21/google-challenger-neeva-gives-up-on-consumer-search-goes-all-in-on-ai-and-the-enterprise/\">google</a>\"<a href=\"https://techcrunch.com/2023/05/21/google-challenger-neeva-gives-up-on-consumer-search-goes-all-in-on-ai-and-the-enterprise/\">-challenger-neeva-gives-up-on-consumer-search-goes-all-in-on-ai-and-the-enterprise/</a>\"</p>",
    "publish_time": "2023-07-28 14:11:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“Twitter如今就像疯人院！”睡地板仍被裁女高管爆料：马斯克带来“恐惧文化”，被裁是最大解脱",
    "url": "https://www.infoq.cn/article/2P0V9B0WIGhZFAM9PvPp",
    "summary": "<p>&nbsp;</p><p></p><blockquote>大家还记得马斯克刚掌管Twitter时，那名因睡在办公室地板上的睡袋里而走红的女高管Esther Crawford 吗？当大家都以为她获得马斯克信任时，她还是被解雇了。网友有人说Crawford“阿谀奉承”，也有人讽刺她，“在办公室睡觉还不够，这真是令人震惊”。&nbsp;对此。Crawford 也在推特中回应道，“看到我在 Twitter 2.0 上全力以赴，你可能会认为我的乐观或努力工作是一个错误。那些嘲笑和嘲笑的人一定是旁观者，而不是竞技场上的人。我为团队在如此多的噪音和混乱中进行建设感到非常自豪。”&nbsp;离职后，Crawford 在社交平台上写下了一篇“我在Twitter的一份工作总结”的文章，这也让我们看到了之前的Twitter 内部管理的混乱和马斯克入主Twitter 后的随性和喜怒无常。我们翻译并整理了Crawford 的帖子，以飨读者。</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d3/d39cd3510ef14ea7a9d67d36c131d1e6.png\" /></p><p></p><p></p><h2>马斯克之前的Twitter：管理层都“躺平”了</h2><p></p><p>&nbsp;</p><p>跟用过Twitter的朋友们一样，我对这个经典厂牌被改成“X”也有不少看法。而当下，可能正是聊聊自己在这家公司这段工作经历的最好时机。</p><p>&nbsp;</p><p>其实我并不算是老员工，只是像大家一样热爱Twitter、使用Twitter。直到2020年我建立的初创公司被Twitter收购之后，我才开始以内部人士的角度审视一切。观察之下，我发现它既令人惊奇又有着种种诡异问题……当然，现实生活中的人和事又何尝不是如此呢。</p><p>&nbsp;</p><p>我是个内心深处喜欢给自己加压、充满紧迫感的人，而Twitter的工作氛围却孤独且官僚主义横行。出于自我意识的愚蠢权力斗争、重组和团队名称变更时常出现，粗暴打断员工们的正常安排。</p><p>&nbsp;</p><p>光是能劳动还不够，你还得懂政治。</p><p>&nbsp;</p><p>我对Twitter基础设施的陈旧和非标而感到震惊。但除了季度收益电话会议之外，几乎没有哪位高管愿意认真想想这件事，毕竟只要收益到位、还有什么不能接受的呢？员工们总感觉公司里的一切都是拿胶带缠起来的，只能算是勉强能运行。另外，大家需要一段时间才能适应一个小小的产品变更，也要拖上几个月、甚至几个季度才能完成的事实。</p><p>&nbsp;</p><p>为了适应职业发展，管理层变得愈发臃肿，企业文化也相当软弱、不太适合我自己的脾气。这里缺少良性辩论和批评，反而默认用“不好意思，做不到”和“那是其他团队的事情，别越界”来搪塞。</p><p>&nbsp;</p><p>团队可能会耗费几个月来构建一项功能，并在最后一刻再因风险太大而被毙掉。</p><p>&nbsp;</p><p>与客户的直接交流，很可能会演变成一场地盘争夺战，并在各职能部门间制造僵局。</p><p>&nbsp;</p><p>我还记得这么件事：有位同事花了一个月时间，想申请接触几位创作者。他先后接触了3位高管和6个不同的职能团队，最终有4名高管参与审批。这也太离谱了……而且遗憾的是，我亲眼看到很多极富才能的人在经历这样的流程之后，筋疲力尽、士气低落。</p><p>&nbsp;</p><p>大多数人都很擅长自己手头的工作，但在Twitter我们几乎不可能裁掉那帮人浮于事的家伙。他们只会被调到其他团队，因为经理们很少有意愿或者能力弄清要怎么把低效人士扫地出门。</p><p>&nbsp;</p><p>高绩效文化会让人们不断保持进步，而躺平文化则会让人们感到沮丧。Twitter就是这样一个地方，总让人感觉自己在浪费自己的潜力。在我任职期间，最擅长打破废话、激发愿景的人是Kayvon Beykpour，但因为他并不是CEO，所以并没有全面管理公司运营的权力。</p><p>&nbsp;</p><p>尽管存在这些实际问题，但我还是很幸运能参与到Twitter的产品、设计、工程、研究、法律、商务拓展、信任与安全、营销、公关等领域，跟最有才华的人们一起工作。通常，由具有内驱力的人们组成的小型跨职能团队往往能挑战某些关键假设，从而带来深远的业务影响。参与这些团队非常有趣，可这种感受却总是种例外、而非规则内的产物。</p><p>&nbsp;</p><p>等待2022年收购交易的那几个月尤其漫长且痛苦，感觉就像领导层集体消失了，躲在律师和法务身后。无论提出什么关于公司未来的问题，他们给出的答案都是“信托责任”那种陈词滥调。同事们开始公开讨论Twitter收购案，认为主要原因是领导层对自己规划和解决长期问题的能力缺乏信心。</p><p>&nbsp;</p><p></p><h2>马斯克带来了“恐惧文化”</h2><p></p><p>&nbsp;</p><p>虽然我对马斯克了解不多，但当时还是持谨慎乐观的态度——毕竟他建立了特斯拉和SpaceX等令人难以置信的卓越企业，还一直保持着稳定运营。也许他的介入能够改变现状，为公司注入新的活力。</p><p>&nbsp;</p><p>而之后的真实经历，用一个个具体事例给我好好上了一课。</p><p>&nbsp;</p><p>当人们问我为什么要留下时，我的答案很简单：乐观、好奇、个人成长，还有丰厚的报酬。</p><p>从一开始，我就意识到马斯克要推动的改革有些很聪明、也有些很愚蠢。但作为团队的一员，我总是秉持着“公开表扬、私下批评”的理念。我不是那种沉默寡言、埋头干活的类型，无论是在收购前还是收购后，我都不惮于公开分享自己的观点和反驳意见。</p><p>&nbsp;</p><p>而最终，我不得不接受这样一个事实：我在Twitter 2.0里找不到心理安全感，就是说我随时可能被解雇，而且连个严肃理由都没有。我已经一次次见证这样的情况，看到这对团队士气产生了怎样的负面影响。尽管我改变不了现状，但还是会尽最大努力激励那些负责重要工作的同事，也为正努力适应这种野蛮、硬核企业文化的人们提供一点情感支持。</p><p>&nbsp;</p><p>马斯克这人有一种奇特的魅力，而且他真的很有趣。他的性格里不乏各种怪癖，比如一遍遍重复同样的故事和笑话。但最大的问题是，他的情绪可能瞬间从兴奋变成愤怒，所以人们很难预判他的心情和对事情的反应。于是乎，大家越来越害怕参加会议、或者向他传达负面消息。</p><p>&nbsp;</p><p>有时候，我感觉核心团队对马斯克观点的支持有点过度积极、甚至到了狂热的地步。每当有人提醒我要小心发言时，我会礼貌地感谢他们，但也强调我就是这样的性格。我不想被笼罩在马斯克制造的恐惧文化下。他要么尊重我的特性，要么让我卷铺盖走人。无论是哪种结果，我都愿意坦然接受。</p><p>&nbsp;</p><p>我很快意识到，马斯克治下的产品和业务决策都是他直觉判断的结果，而且他似乎并不愿意寻找和依赖大量数据及专业知识来充实自己的决策过程。这对我来说真的很受打击，因为我坚信但凡能多搜集一点信息和知识，他的决策质量都能上一个台阶。相反，他会在Twitter上开展民意调查、询问朋友，甚至向他的传记作者寻求产品建议。有时候，他似乎比那些一辈子做研判的专家更相信随机反馈。我一直不明白他为什么会这样，甚至到现在也没理解。</p><p>&nbsp;</p><p>我觉得情况并不一定非得走到如今的地步，但我也不是说马斯克的到来就完全是坏事、或者应该想办法把他踢出局。他是个聪明人，有足够的钱去犯错误，并在过程当中纠正问题、找到方向。身为Twitter最大的股东，他可以在短期之内拉低公司价值，只要最终能扭转局面就行。</p><p>&nbsp;</p><p>他对速度的关注已经到了难以理解的地步，而且他明显并不害怕砸碎一个旧世界。但现在真正的问题，是他要怎么建设起一个新世界，特别是还有没有足够多的用户愿意接受他构想中的那个新世界。</p><p>&nbsp;</p><p>通过近距离马斯克，我学到了很多东西——有好的、有坏的，也有一言难尽的。他永远大胆、热情、懂得如何讲个令人振奋的故事，但同时他思维跳脱、缺乏同理心，这给Twitter员工造成了巨大的痛苦。</p><p>&nbsp;</p><p>马斯克在解决物理难题方面有着非凡的天赋，但要想打造出能促进人与人之间联系和沟通的产品，需要的则是完全不同的社交能力和情商。在这方面，他明显做得不够好。</p><p>&nbsp;</p><p>社交网络虽然很难彻底“咽气”，但仍会在一圈圈的螺旋下降中走向衰败。只有时间会告诉我们结果如何，而我衷心希望X能够站稳脚跟，毕竟良好的竞争对消费者最为有利。</p><p>&nbsp;</p><p></p><h2>“如今的Twitter简直像个疯人院”</h2><p></p><p>&nbsp;</p><p>与此同时，我对各位在幕后不知疲倦工作的员工、想要稳定平台销售产品的广告商，还有经历过混乱更新的客户们深表同情。我承认，如今的Twitter简直像个疯人院。</p><p>&nbsp;</p><p>Twitter曾经拥有迅猛的发展速度，但也饱受官僚主义之苦。而如今的X则由一位善变的领导者掌管，平台上最大的声音来自他的本能和直觉，这艘巨轮就在这样怪异的状态下继续向前行驶。</p><p>很多朋友听说我，可能是从所谓“睡袋事件”来的。当时我晚上不回家，就睡在会议室的地板上。那我就来聊聊这事好了。</p><p>&nbsp;</p><p>亲身经历这样一场病毒式的新闻传播是种有趣的经历。我被左翼人士攻击，被称为亿万富豪身边的马屁精；同时也被右翼人士攻击，因为我同时是名母亲，而他们把我妖魔化成了女性为了选择事业而放弃家庭的反面典型。</p><p>&nbsp;</p><p>值得庆幸的是，我自己没什么心理包袱，也不会对键盘侠们的说辞太过较真。毕竟要想让自己的言论在网上激起哪怕一小会热度，厚脸皮和强烈的自我意识都是必要的因素。</p><p>&nbsp;</p><p>真实故事其实非常简单。马斯克上任后，马上给我安排了一个几乎实现不了的项目期限。作为产品负责人，我永远不会要求员工做我自己做不到的事情。所以我跟一支跨多个时区的出色团队昼夜不停地赶进度，我们最终克服了困难、按时完成了交付。这段过程很辛苦，但也很有趣。</p><p>最初那几个月确实很疯狂，但这是我自己的选择，所以我从不后悔。</p><p>&nbsp;</p><p>其实大多数情况下，这种全力以赴的状态很难找到，甚至值得为之庆祝。但很明显，没人能永远以这样的速度赶工，只有在出现紧急任务时爆发一下。无论是职业生涯还是当初的学生时代，我都时不时会为重要的事情熬个夜。我并不后悔投入了这么多时间和精力，也为自己一路走来取得的进步和成果感到自豪。这是种职业道德，我愿意接受伴生而来的压力和辛劳。</p><p>&nbsp;</p><p>我觉得生活就像一场游戏，而Twitter易主之后我们瞬间进入了困难模式。当然，我喜欢接受艰难的挑战，因为这样既有趣又有益，是我们快速成长和学习的底层动力。</p><p>&nbsp;</p><p>我意识到，如今的社会正走向两极分化。但对于Twitter这款应用、它的所有者和未来前景时，我既不是忠诚的铁粉、也不是充满恨意的批评者——而是个乐观的务实者。</p><p>&nbsp;</p><p>但所谓“忠诚不绝对，就是绝对不忠诚”，我这样的态度在互联网上很不受待见，毕竟人们不能简单把我归结成支持或者反对派。可我觉得自己能够摆脱原教旨主义观念、自由看待事物，其实是件很幸福的事。任何一个人都可以被描述成英雄或者恶棍，具体要看是谁从哪个角度来讲这个故事。马斯克也一样，不能简单用认可和贬低来定义。他是个复杂的人，掌握着深不可测的金融和地缘政治力量，因此整个社会才需要他站在善良的一边，而绝不可滑向政治分歧与割裂那一边。</p><p>&nbsp;</p><p>我对他的很多决定也抱有异议，对他毫不留情毁掉那么多事物感到惊讶。但只要继续投入充足的资金和时间，更多创新事物可能也会在废墟中开花结果。</p><p>&nbsp;</p><p>至少我希望是这样。</p><p>&nbsp;</p><p></p><h2>被解雇是“最大的解脱”</h2><p></p><p>&nbsp;</p><p>有时候，人们会问我被解雇后感觉如何，其实这对我来说是这辈子最大的解脱。虽然网上怎么评价这件事的声音都有，但我可是身经百战了，并不会受什么影响。我知道我自己是怎么工作的、为什么要这么拼命，所以不会轻易被舆论所动摇。我管理的产品团队被解散也很正常，毕竟几乎所有其他项目经理都被辞退了，我又有什么特殊的呢？</p><p>&nbsp;</p><p>之后，我去好好休了个假，终于找回了休息和放松的感觉。我是个富有创造力的人，也是个勤勤恳恳的构建者，所以我尽早会回到一家高强度工作的企业。但我很感激这段能尽情思考、阅读、旅行和陪伴爱人的闲暇时光。</p><p>&nbsp;</p><p>在认真反思之后，我比以往任何时候都更加坚信，最好的结果必然来自能将头脑与心灵完美结合的伟大领导力。</p><p>&nbsp;</p><p>而且我还总结出一个对任何成功者都适用的警示故事，也是我自己没有做好的地方——我们所在的位置越高，自己的世界就会变得越小。这是个奇怪的导论，但最富有、最具权势的人，往往也是最隔绝于世界的人。</p><p>&nbsp;</p><p>我之前经常观察马斯克，他看起来非常孤独，因为他把全部时间和精力都投入到了工作上。这绝不是我想要的生活模式。</p><p>&nbsp;</p><p>金钱和名誉会造起一座心灵监狱，导致心理健康严重恶化。大家肯定都听过类似的名人轶事，他们最终都独享抑郁、偏执、自大、妄想、狂躁和行为古怪的泥潭。</p><p>&nbsp;</p><p>生活在这样一间“回声室”里相当危险，因为身处高位的人们往往会被唯唯诺诺者所包围。这帮家伙需要以此谋生，从你的成功当中分一杯羹。所以一定要分得清谁是家人、谁是朋友、谁是战友，留住我们身边的这一丝美好。只有这样，我们才能始终保持正轨，并经受得起风浪与颠沛的洗礼。每个人有时都需要直面残酷的现实，如果解雇掉所有敢于发声的人，那么现实就可能被扭曲成一个诡异变形的漩涡。</p><p>&nbsp;</p><p>我之所以关注Twitter，就是因为我痴迷于这种孤独感和在人与人间建立联系的问题。让人倍感不安的是，尽管我们建立起了一个更安全、更富裕的世界，但人类却变得越来越孤独。我不觉得这是什么必然的取舍，所以我才会在个人经历和职业生涯中一次次挑战这样的现实。</p><p>&nbsp;</p><p>总而言之，Twitter是互联网上一个奇异且特别的地方。我很庆幸自己能在它的发展历程中，扮演一个微小的角色、做出一点微小的贡献。</p><p>&nbsp;</p><p>无论接下来会发生什么，我都会继续坚持使用Twitter。这里的社交活动仍旧活跃，我也会继续关注、参与和分享热门话题。因为我不想，也绝不可能自绝于这片网络天地。</p><p>&nbsp;</p><p>也许X会取得巨大成功，但也不排除它最终会彻底失败。</p><p>&nbsp;</p><p>总之，这应该会是一段有趣的旅程，我也将对未来拭目以待。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://esthercrawford.medium.com/an-epilogue-to-my-time-working-at-twitter-24a126098246\">https://esthercrawford.medium.com/an-epilogue-to-my-time-working-at-twitter-24a126098246</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-07-28 14:44:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "聆心智能Open Day发布CharacterGLM超拟人大模型，打造“走心”的AI",
    "url": "https://www.infoq.cn/article/1euYj7ccJfN5VLC8DaEG",
    "summary": "<p>7月25日，北京聆心智能科技有限公司（以下简称“聆心智能”）举办以AI之名，从“心”出发——聆心智能Open Day活动，发布“超拟人大模型”最新进展——CharacterGLM超拟人大模型。现场，学界和业界嘉宾围绕“超拟人大模型”商业化进行了深度思考与分享，从技术、产品、应用到商业模式，全面探索“超拟人大模型”行业新趋势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/7244d6c8880e971e568a3eced02483be.png\" /></p><p></p><p>聆心智能Open Day活动现场</p><p></p><h2>CharacterGLM 超拟人大模型：打造“走心”的AI</h2><p></p><p></p><p>聆心智能创始人、首席科学家黄民烈教授带领团队在开放日活动展示了聆心智能最新的技术进展和产品，与参会嘉宾分享和交流了大模型产业的趋势、挑战和机会。</p><p></p><p>活动现场，聆心智能CEO张逸嘉首先围绕“AI Companion：从过去走向未来”的主题，分享了AI聊天机器人的“前世今生”。自1950年艾伦·图灵推出了著名的图灵测试到2022年ChatGPT的火爆，人类从未停止对于聊天机器人的探索，功能型AI与拟人型AI在大模型时代均迎来革命性突破。如今，ChatGPT增长已达瓶颈，人们对“数字生命”的需求远超预期，为此聆心智能依托自研的超拟人大规模预训练模型，打造“数字生命大脑”，并在此基础上构建AI角色生成引擎，应用于社交、娱乐、教育、文旅、健康等多场景中，发挥“拟人”优势，打破AI聊天机器人的应用瓶颈。</p><p>&nbsp;</p><p>此外，聆心智能联合创始人郑叔亮分享了CharacterGLM超拟人大模型的最新进展。聆心智能与AI大模型公司智谱AI深度合作，基于于GLM基座深度优化升级，打造CharacterGLM 超拟人大模型，具备“六边形能力”——人格、知识、能力、社会化、成长性、价值观。</p><p></p><p>CharacterGLM超拟人大模型能够使AI拥有自己的“个性”和“情感”，在交流过程中呈现出丰富的立体化“人格”，不局限表面上“机械性”话术，具有更符合人类逻辑的思考能力、动作表情等非语言信息表达能力、角色的延续性记忆，根据用户实时情绪和性格人设给予富有个性但不失温度的回答，改善“枯燥无味”的聊天感受，同时更符合人类伦理道德。</p><p></p><p>除了能力超群之外，CharacterGLM超拟人大模型还具备行业领先的技术优势，包括更强大的性能、更高效的推理、更长的上下文和更低成本的模型定制，能够允许更详细的角色设定、更多轮次的对话和更远的记忆深度，同时实现小数据多风格迁移的强大能力，兼顾灵活与高效。</p><p>&nbsp;</p><p>基于CharacterGLM，聆心智能研发了全新产品AiU，目前正在内测中。AiU是一个连接人与 AI的兴趣互动社区。在社区内，用户可以根据个人偏好创造不同性格与人设的 AI 角色，实现互动聊天、分享美好瞬间。通过互动，AI角色能够记住用户的喜好和信息，实现用户对“角色”的不断“塑造”，同时，这些AI角色也在时时刻刻“学习”和“感受”外部世界，并发展成自己独特的成长轨迹。</p><p></p><p>在AiU社区内，用户能够实现“Ai by U，Ai for U，Ai with U（AI由你创造，AI因你而在，AI伴你同行）”，不仅能获得工具性支持，更能感受精神上的陪伴，共同创造有爱的“乌托邦”。</p><p></p><h2>有情更有“用”：社交、情感、娱乐多领域探索AI商业化道路</h2><p></p><p></p><p>CharacterGLM超拟人大模型已经开启了商业化探索之路，聆心智能联合创始人郑叔亮以《商业模式的初探：用“心”创造商业价值》为题进行了分享。聆心智能以开放API、云端私有化、本地私有化为底层技术支撑，构建了多样化的服务模式，包括创新型应用开发合作伙伴陪伴成长计划、领域代表性企业赋能&amp;科研课题合作、平台企业&amp;大型业务&amp;规模化系统集成。</p><p></p><p>目前，聆心智能在社交、教育、文旅、娱乐、营销、健康、直播、客服等领域布局超拟人大模型服务，例如提供专业的心理陪伴和咨询；快速配置上线适应多场景的“数字人”；生产游戏剧情和辅助策划；等等。</p><p>&nbsp;</p><p>活动现场，数字栩生CMO郭学赟分享了大模型在数字人领域的应用。数字栩生是一家数字人底层技术基础设施服务商，曾经打造了字节跳动游戏主角“李星澜”、高逼真数字演员“春草”等专属数字人IP， 并参与了“梅兰芳”大师复现项目，成功打造了全国首个高精度京剧数字名人。郭学赟认为，大模型是驱动数字人资产的重要底层，事实上当前还有很多数字人停留在基于语音、文本或预设触发的基础层级上，基于数字栩生和聆心智能双方长久以来的合作，将聆心智能CharacterGLM超拟人大模型与数字栩生自研的AIGC数字人智能交互系统相结合，让数字人具备更好的情绪价值，提供更多的情感交流，真正成为人类的贴心朋友。</p><p>&nbsp;</p><p>洪恩AI产品负责人聂靖骐分享了基于CharacterGLM拟人大模型打造的洪恩AI问答2.0。该项目有效利用聆心智能在大模型领域的“技术优势”和洪恩高度契合用户需求的“场景优势”，2023年第一季度已上线，以独特的IP角色人设实现“有温度的交流”和“准确的知识引导”，帮助小朋友收获德与智的双面成长。</p><p>&nbsp;</p><p>聆心智能CharacterGLM超拟人大模型的发布，意味着“超拟人大模型”在“情感”方面获得了显著提升，将在个性化表达、共情能力、伦理道德等方面推动AI发展，让AI更“有心”“有爱”。此外，聆心智能与合作伙伴在商业化方面的探索，也将助力大模型的应用普及，共创未来美好人类社会。</p>",
    "publish_time": "2023-07-28 17:00:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Oracle正式发布MySQL HeatWave Lakehouse，数据查询速度更快、扩展性更强",
    "url": "https://www.infoq.cn/article/6pqTRMIM0tEQVCkJJGJ2",
    "summary": "<p>近日，Oracle宣布推出<a href=\"https://www.infoq.cn/article/6pqTRMIM0tEQVCkJJGJ2#lakehouse\">MySQL HeatWave Lakehouse</a>\"，让客户能够像在数据库内查询数据一样快地查询对象存储中的数据。</p><p></p><p>MySQL HeatWave Lakehouse支持各种文件格式（例如CSV、Parquet）和从其他数据库导出的文件，并且可以在同一查询中将对象存储文件数据与MySQL数据库中的数据相结合。对象存储中的文件由HeatWave直接查询，而无需将数据复制到MySQL数据库中。因此，在对象存储中查询数据方面，MySQL HeatWave Lakehouse为查询处理、数据加载速度、集群预配时间和自动化奠定了新的可扩展性和性能标准。</p><p>&nbsp;</p><p>甲骨文公司首席企业架构师Edward Screven表示：“超过80%的数据存储在文件系统中，该数字还在不断增长。客户希望将各种外部数据与内部事务处理数据集成和分析，但处理起来往往过于复杂或成本太高。MySQL HeatWave Lakehouse能够帮助客户将对象存储中的数据与数据库数据结合在一起，让客户能够轻松获得宝贵的实时洞察，同时显著提高查询性能并降低数据加载速度。”</p><p></p><h2>对象存储中的数据查询速度，与数据库中的数据查询速度一样快</h2><p></p><p>&nbsp;</p><p>如<a href=\"https://github.com/oracle/heatwave-tpch\">10 TB TPC-H*基准测试</a>\"所示，使用MySQL HeatWave Lakehouse以常用文件格式查询对象存储中的数据的速度，与在MySQL数据库中查询数据的速度一样快。这是因为MySQL HeatWave的内置功能<a href=\"https://www.infoq.cn/article/6pqTRMIM0tEQVCkJJGJ2#autopilot\">MySQL Autopilot</a>\"&nbsp;提供了基于机器学习的自动化，可以从查询执行中学习，并改进未来查询的执行计划。MySQL Autopilot是MySQL HeatWave一项特别的创新功能。基于Oracle云基础设施远程软件服务（Oracle Cloud Infrastructure, OCI）的MySQL HeatWave采用AMD EPYC™处理器。</p><p>&nbsp;</p><p>AMD数据中心解决方案业务小组执行副总裁兼总经理Forrest Norrod表示：“AMD和MySQL HeatWave工程团队正在密切合作，合力优化AMD EPYC处理器与MySQL HeatWave的性能，以利用新的处理器功能。得益于本次合作，在AMD EPYC CPU驱动的OCI实例上运行MySQL HeatWave的MySQL客户可在关键业务工作负载方面获得较为突出的性价比，其中包括针对海量对象存储数据的实时分析。”</p><p>&nbsp;</p><p>MySQL HeatWave的性能源自于其横向扩展架构，该架构支持通过大规模并行来配置集群、加载数据和处理高达512个节点的查询。此外，MySQL Autopilot的增强功能可自动为对象文件创建元数据，并动态适应底层对象存储的性能，确保在OCI区域中都能提供出色的性能。</p><p>&nbsp;</p><p>MySQL HeatWave是重要的云端服务，可在单一的MySQL数据库服务中提供事务处理、实时分析、机器学习、数据池查询和基于机器学习的自动化功能。作为Oracle Distributed Cloud&nbsp;策略的核心，MySQL HeatWave在OCI中提供，在Amazon Web Services中原生提供，作为Oracle Database Service for Azure的一部分提供，以及通过OCI Dedicated Region在客户数据中心内提供。</p><p></p><h2>客户、合作伙伴和分析机构如何评价？</h2><p></p><p>&nbsp;</p><p>Natura&amp;Co解决方案架构师Fabricio Rucci表示：“数据呈指数级增长，我们在数据湖中存储的数据量也是如此。能够使用标准MySQL语法，在数据库和对象存储中查询数据获得实时洞察，这一点对Natura而言非常重要。这为我们提供了新的机会，如果我们可以比竞争对手更快地分析所有的数据，就可能获得新的竞争优势。”</p><p>&nbsp;</p><p>德勤咨询公司(Deloitte Consulting)云基础设施与工程团队负责人Henry Tullis表示：“无论是从对象存储加载数据，还是在对象存储上运行查询，HeatWave Lakehouse都展示了良好的可扩展性。随着数据量以及HeatWave集群大小的增加，加载时间和查询时间几乎保持不变。HeatWave Lakehouse在数据管理方面的这种横向扩展特性是高效处理大量数据的关键。”</p><p></p><p>Constellation Research副总裁兼首席分析师Holger Mueller表示：“自大数据出现以来，大数据/湖仓一体查询的速度大大低于事务查询速度。MySQL HeatWave打破了这一局面，证明了Lakehouse可以实现与事务查询同等的性能，这是很多人未曾听过、未曾想过的。通过查询性能奇偶校验，HeatWave使CxOs无需再担心数据需要存储在哪里、如何查询数据。其中的秘诀在于能够优化查询的HeatWave的Autopilot。HeatWave团队再一次取得了业内突破性成果。”</p><p></p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-07-28 17:18:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何基于 Apache Doris 构建简易高效的用户行为分析平台？",
    "url": "https://www.infoq.cn/article/SoCIclCLD8f4vSzLB4dX",
    "summary": "<p>用户行为分析是企业了解用户的重要方式之一，可以从点击、登录、观看、跳出、下单购买等多维角度还原用户动态使用场景和用户体验，通过对用户行为埋点数据进行分析，可以详细、清楚地了解用户的行为习惯，从中发现用户使用产品的规律，以用于精确营销、产品优化，从而驱动业务实现增长。</p><p></p><p>随着数字化转型进程的不断推进，用户行为分析平台在企业内部扮演的角色愈发重要，如何进一步挖掘用户行为数据价值，也成为了当下各企业不断努力探索的方向。而系统平台建设过程中所遭遇的挑战，也成了制约企业实现精细化运营过程中的重要因素。因此本文将从某社交 APP 的实际业务场景出发，与大家分享 <a href=\"http://doris.apache.org/\">Apache Doris</a>\" 如何助力企业构建高效的用户行为分析平台，实现数据驱动业务发展。</p><p></p><h2>从一个业务场景说起</h2><p></p><p>在此以某社交 APP 为例，如果想要更好地提升用户使用体验并进一步实现转化率的增长，基于用户行为数据进行分析并调整业务相应策略是其中的关键，而各个业务团队对用户行为数据往往诉求存在一定差异：</p><p></p><p>算法团队想知道该 APP 最近一段时间的用户活跃数据，来判断是否需要调整推荐算法；商业部门想知道多少人观看广告后进行了点击，以分析广告带来的用户体验如何；运营部门想知道多少人通过落地页参与活动以及其转化率，以判断活动 ROI；产品部门想知道不同功能用户访问数据的差异，通过 A/B 实验指导正确的产品优化路径；......</p><p></p><p>为了承接以上需求，过去该公司使用了基于 Hive 的离线数据仓库，整体数据平台架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/06c50c0cbfa37ff5d05bbdd2bfd98f90.png\" /></p><p></p><p>原始数据主要来自关系型数据库 MySQL 、消息队列 Kafka 以及采集到的日志数据；利用 Sqoop 和 DataX 进行数据同步，通过 Flink 和 Spark 进行 ETL 以及 Yarn 和 Airflow 进行作业和任务调度；处理完成的数据落入 Hive ，Impala 作为分析引擎，为上层自研的 BI 产品提供交互式分析服务；</p><p></p><p>在这样的平台架构下，留存着一系列挑战有待解决：</p><p></p><p>数据时效性较差：原有架构数据链路比较长，数据时效性差，T+1 的数据生产模式严重影响业务分析的效率；运维成本高：数据链路较长，维护数据流转的成本高，一旦出现问题则需要排查上下游多个系统；且 Impala 本身不具备存储数据的能力，不得不引入 Hadoop 体系，而组件的繁多冗杂也大幅提升了企业运维的成本投入；数据分析难度高：对于数据分析人员来说，没有合适的分析函数将会带来很多额外的工作量，比如编写 SQL 逻辑冗长、执行 SQL 耗时耗力等，严重影响数据分析的效率。</p><p></p><p>以该公司数据为例，我们将 APP 数据简化抽象出来，以一个常见需求来看数据分析的成本：</p><p></p><p><code lang=\"sql\"> -- APP用户表\n CREATE TABLE app (\n     id int,   -- 用户id\n     a_time datetime,   -- 动作的时间\n     act varchar(20)  -- 动作（登录、观看、点击等等）\n ) \n unique key (id, a_time, act) \n COMMENT 'OLAP' DISTRIBUTED BY HASH(`id`) BUCKETS 8  \n</code></p><p></p><p>以背景介绍中的需求为例，算法部门想知道该 APP 最近一段时间的用户活跃及留存数据，来判断是否需要进行推荐算法和展示页的调整。上述其实就是一个求留存率的需求，实现逻辑并不复杂，我们可以很容易写出如下SQL：</p><p></p><p><code lang=\"sql\">select dt, activ_2 / activ_1 as retention\nfrom\n(\n    select to_date(aa.o_time) as dt, count(distinct a.id) as activ_1,\n    count(distinct b.id) as  activ_2\n    from app a\n    left join app b\n    on a.id = b.id and to_date(a.a_time) = days_add(to_date(b.a_time), 1)\n    where to_date(a.a_time) = 'xxxx'\n    group by to_date(aa.a_time)\n) as aa\n</code></p><p></p><p>但其中不能忽视的问题出现了，我们每查询一个留存比例就需要 Join APP 表自身一次，查询多个比例则需要 Join 该表自身多次，SQL 语句变得无比冗长；同时当执行该 SQL 时，多表 Join 带来的耗时也会变得很长。由此可知，没有合适的行为分析函数，会降低分析过程的效率。</p><p></p><h2>全新的用户行为分析平台</h2><p></p><p>经过慎重选型和对比，该公司决定使用 Apache Doris 来作为分析和计算引擎，主要考虑到如下优势：</p><p></p><p>数据集成简易：提供无缝接入 Kafka 和 MySQL 的能力，可复用已有架构并减少对接工作量；架构简单：只有 FE 和 BE 两种角色，无需引入第三方组件，维护成本极低；性能优异：列式存储引擎、MPP 查询框架、全向量化执行，在实际测试中性能表现突出；功能丰富：支持丰富的用户分析函数，分析结果即查即出；....</p><p></p><p>在引入 Apahce Doris 后，整体数据架构得到简化，数据处理链路得到大幅缩短，以下是新的架构：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3d7e03a63525f21746b423782f48a7c.png\" /></p><p></p><h3>数据导入更便捷</h3><p></p><p>首先，Apache Doris 数据生态丰富，提供了多种数据导入方式，与已有数据源无缝对接：</p><p></p><p>通过 Routine Load 可以直接订阅 Kafka 数据；通过 INSERT INTO SELECT可以导入外部表的数据，目前已支持 MySQL、Oracle、PostgreSQL、SQL Server 等多个数据源；通过 Stream Load 可以直接导入本地数据文件；......</p><p></p><p>用户可以针对不同的数据源选择不同的数据导入方式，以快速集成来自不同数据源的数据。文档参考：https://doris.apache.org/zh-CN/docs/dev/data-operate/import/load-manual</p><p></p><p>其次，Apache Doris 1.2 版本中增加了 Multi-Catalog 功能，可实现无缝对接外部异构数据源，用户无需进行数据导入，即可直接通过创建 CREATE CATALOG  来查询底层数据。相对外部表， Multi-Catalog 无需创建表与表之间的映射关系，可以实现元数据层的对接，进一步加强联邦数据分析能力。</p><p></p><p><code lang=\"sql\">-- 我们以mysql为例，来详细讲解读取和写入的具体实现\n\n-- 创建catalog\nCREATE CATALOG jdbc PROPERTIES (\n    \"type\"=\"jdbc\",\n    \"jdbc.user\"=\"root\",\n    \"jdbc.password\"=\"123456\",\n    \"jdbc.jdbc_url\" = \"jdbc:mysql://127.0.0.1:13396/demo\",\n    \"jdbc.driver_url\" = \"file:/path/to/mysql-connector-java-5.1.47.jar\",\n    \"jdbc.driver_class\" = \"com.mysql.jdbc.Driver\"\n);\n\n其中jdbc.driver_url可以是远程jar包：\n\nCREATE CATALOG jdbc PROPERTIES (\n    \"type\"=\"jdbc\",\n    \"jdbc.user\"=\"root\",\n    \"jdbc.password\"=\"123456\",\n    \"jdbc.jdbc_url\" = \"jdbc:mysql://127.0.0.1:13396/demo\",\n    \"jdbc.driver_url\" = \"https://path/jdbc_driver/mysql-connector-java-8.0.25.jar\",\n    \"jdbc.driver_class\" = \"com.mysql.cj.jdbc.Driver\"\n);\n\n-- 创建catalog后，可以通过 SHOW CATALOGS 命令查看 catalog：\nMySQL [(none)]&gt; show catalogs;\n+-----------+-------------+----------+\n| CatalogId | CatalogName | Type     |\n+-----------+-------------+----------+\n|         0 | internal    | internal |\n|     10480 | jdbc        | jdbc     |\n+-----------+-------------+----------+\n\n-- 通过 SWITCH 命令切换到 jdbc catalog，并查看其中的数据库\nMySQL [(none)]&gt; switch jdbc;\nQuery OK, 0 rows affected (0.02 sec)\n\nMySQL [(none)]&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| __db1              |\n| _db1               |\n| db1                |\n| demo               |\n| information_schema |\n| mysql              |\n| mysql_db_test      |\n| performance_schema |\n| sys                |\n+--------------------+\n\nMySQL [demo]&gt; use db1;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\nDatabase changed\n\nMySQL [db1]&gt; show tables;\n+---------------+\n| Tables_in_db1 |\n+---------------+\n| tbl1          |\n+---------------+\n\n-- 使用catalog查询外部数据源\nMySQL [db1]&gt; select * from tbl1;\n+------+\n| k1   |\n+------+\n|    1 |\n|    2 |\n|    3 |\n|    4 |\n+------+\n\n-- 创建doris表，注意schema与mysql一致\nCREATE TABLE IF NOT EXISTS test.test (\n    k1 int\n)\nDUPLICATE KEY(`col1`)\nDISTRIBUTED BY HASH(col1) BUCKETS 1\nproperties(\n\"replication_num\"=\"1\"\n);\n\n-- 使用catalog直接将mysql中的数据导入doris\n-- 我们只用三级元数据层级，catalog.db.table的方式\ninsert into internal.test.test select k1 from jdbc.db1.tbl1;\n</code></p><p></p><h3>数据时效性提升</h3><p></p><p>数据架构简洁有力，引入 Apache Doris 后，数据架构缩减到 3 层，有效避免了过长数据处理链路带来的时延，整体数据时效性从天级降至分钟级；</p><p></p><p>用户查询耗时更低，SQL 查询耗时从过去的分钟降低至秒级甚至毫秒级，极大提升了业务分析人员的分析效率。</p><p></p><h3>数据分析效率进一步提升</h3><p></p><p>前文中有提到，没有合适的分析函数会使得分析工作变得艰难；而 Apache Doris 为用户行为分析提供了丰富的分析函数，使得数据分析难度大幅降低，这些函数包括但不限于：</p><p></p><p>intersect_countsequence_countsequence_matchretentionwindow_funnelArray 类函数......</p><p></p><h2>丰富的用户行为分析函数</h2><p></p><p></p><h3>数据准备</h3><p></p><p>在此以上述 APP 表为例，前期需要完成建表以及数据导入等准备工作：</p><p></p><p><code lang=\"sql\"> -- 建表\n CREATE TABLE app (\n     id int,   -- 用户id\n     a_time datetime,   -- 动作的时间\n     act varchar(20)  -- 动作（登录、观看、点击等等）\n ) \n unique key (id, a_time, act) \n COMMENT 'OLAP' DISTRIBUTED BY HASH(`id`) BUCKETS 8  \n PROPERTIES (\"replication_allocation\" = \"tag.location.default: 1\");\n \n \n -- 插入数据\ninsert into app values \n(111, '2022-01-01 10:00:00', 'login'), \n(111, '2022-01-01 10:01:00', 'view'),\n(111, '2022-01-01 10:02:00', 'click'), \n(111, '2022-01-02 10:00:00', 'login'), \n(111, '2022-01-02 10:01:00', 'view'), \n(222, '2022-01-01 11:00:00', 'login'),\n(222, '2022-01-01 11:01:00', 'view'), \n(333, '2022-01-01 12:00:00', 'login'),\n(333, '2022-01-01 12:01:00', 'view'),\n(444, '2022-01-01 13:00:00', 'login');\n \n -- 查看数据\nselect * from app order by a_time;\n+------+---------------------+-------+\n| id   | a_time              | act   |\n+------+---------------------+-------+\n|  111 | 2022-01-01 10:00:00 | login |\n|  111 | 2022-01-01 10:01:00 | view  |\n|  111 | 2022-01-01 10:02:00 | click |\n|  222 | 2022-01-01 11:00:00 | login |\n|  222 | 2022-01-01 11:01:00 | view  |\n|  333 | 2022-01-01 12:00:00 | login |\n|  333 | 2022-01-01 12:01:00 | view  |\n|  444 | 2022-01-01 13:00:00 | login |\n|  111 | 2022-01-02 10:00:00 | login |\n|  111 | 2022-01-02 10:01:00 | view  |\n+------+---------------------+-------+\n</code></p><p></p><h3>留存分析</h3><p></p><p>算法部门想知道该 APP 最近一段时间的用户活跃及留存数据，来判断是否需要进行推荐算法和展示页的调整。该需求可以理解为留存率，主要是指注册后在一定时间内或者一段时间后有登录行为且仍在继续使用该产品的留存用户，在当时总的新增用户中所占比例。该需求为前文提到的第一个需求，接下来我们看看使用 Doris 提供的分析函数如何实现呢？</p><p></p><p>正交 Bitmap 函数计算留存率</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a5077e69567ca4eb624c2bf1a1e5f4c0.png\" /></p><p></p><p><code lang=\"sql\">-- 求第N天登录的用户\nselect intersect_count(to_bitmap(id), to_date(a_time), '2022-01-01') as first from app;\n+-------+\n| first |\n+-------+\n|     4 |\n+-------+\n\n\n-- 求第N天和N+1天都登录的用户\nselect intersect_count(to_bitmap(id), to_date(a_time), '2022-01-01', '2022-01-02') as second from app;\n+-------+\n| second |\n+-------+\n|     1 |\n+-------+\n\n\n-- 二者的比例即为所求\nselect intersect_count(to_bitmap(id), to_date(a_time), '2022-01-01', '2022-01-02') / intersect_count(to_bitmap(id), to_date(a_time), '2022-01-01') as rate from app;\n+------+\n| rate |\n+------+\n| 0.25 |\n+------+\n</code></p><p></p><p>Retention 函数计算留存率</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1b7380af2a6f493337cc1767205edb9.png\" /></p><p></p><p>Retention 通常需要跟group by联合使用，以获取group by列匹配的条件。而输入的参数是可变长参数，Retention 会返回跟输入参数长度相等的数组。数组取值则要看匹配条件能否满足，如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a86e8bf515eebbe34b985076bb47eb5f.png\" /></p><p></p><p><code lang=\"sql\">-- 求第N天登录的用户\nselect id, retention(to_date(a_time)='2022-01-01') as first from app group by id;\n+------+-------+\n| id   | first |\n+------+-------+\n|  222 | [1]   |\n|  111 | [1]   |\n|  444 | [1]   |\n|  333 | [1]   |\n+------+-------+\n\n-- 求第N天和N+1天都登录的用户\nselect id, retention(to_date(a_time)='2022-01-01', to_date(a_time)='2022-01-02') as second from app group by id;\n+------+--------+\n| id   | second |\n+------+--------+\n|  222 | [1, 0] |\n|  111 | [1, 1] |\n|  444 | [1, 0] |\n|  333 | [1, 0] |\n+------+--------+\n\n-- 二者的比例即为所求\nselect sum(re[2])  / sum(re[1]) as rate  from (select id, retention(to_date(a_time)='2022-01-01', to_date(a_time)='2022-01-02') as re from app group by id) as a;\n+------+\n| rate |\n+------+\n| 0.25 |\n+------+\n</code></p><p></p><h3>路径分析</h3><p></p><p>商业部门想知道多少人观看广告后进行了点击，以分析广告带来的用户体验是否合适。该需求可以理解为行为分析中的路径分析，而路径分析是一种基于行为顺序、行为偏好、关键节点、转化效率的探索型模型。依据路径分析可以直观掌握用户行为扩展路线，以供优化节点内容、提升整体转化效率。</p><p></p><p>sequence_count 路径分析</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ba/ba458bd0303d89357bac32b228db3527.png\" /></p><p></p><p>sequence_count通常需要跟group by一起使用，以获取group by列匹配的条件。函数使用方法为：sequence_count((?1)(?t&lt;3600)(?2), date, col1=1, col3='a')，表明当前 col1=1 是第一个条件；此时如果有 col2=a，并且 col2 的时间与 col1 的时间在 3600 秒之内(col2 的时间减去 col1 的时间)，则sequence_count对这样一组匹配结果记一个数。其工作逻辑如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d87f9cb9d79a31c40a3a8863c5e8cc0.png\" /></p><p></p><p><code lang=\"sql\"> -- 哪些用户在登录后2分钟内观看了广告\nselect id, sequence_count('(?1)(?t&lt;=120)(?2)', a_time, act = 'login', act = 'view') as result from app group by id;\n+------+--------+\n| id   | result |\n+------+--------+\n|  111 |      2 |\n|  444 |      0 |\n|  222 |      1 |\n|  333 |      1 |\n+------+--------+\n-- 上述SQL可以看到，先观看广告再点击广告的用户有3人；而111这个用户有两次都是登录后观看了广告。pattern中的(?1)对应act = 'login'，(?2)对应act = 'view'，时间间隔为120秒\n\n\n-- 哪些用户在登录后2分钟内观看了广告，并在2分钟内点击了广告\nselect id, sequence_count('(?1)(?t&lt;=120)(?2)(?t&lt;=120)(?3)', a_time, act = 'login', act = 'view', act = 'click') as result from app group by id;\n+------+--------+\n| id   | result |\n+------+--------+\n|  333 |      0 |\n|  111 |      1 |\n|  444 |      0 |\n|  222 |      0 |\n+------+--------+\n</code></p><p></p><h3>漏斗分析</h3><p></p><p>运营部门想知道多少人通过落地页参与活动以及其转化率，以判断活动 ROI 以及确定策略带来的价值是否符合预期。该需求可以理解为转化率，而转化率主要是指是指用户进行了相应目标行动的次数与总次数的比率；转化率可以衡量一个产品用户需求强弱、评价产品设计好坏、对比流程和渠道权重等。</p><p></p><p>window_funnel 漏斗转化分析</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2e2ffb35087ae6e0a220e60ca60b07c1.png\" /></p><p></p><p>window_funnel也是有可变参数，并且需要指定时间窗口列和窗口大小。假设窗口大小为 3600 秒，并且datetime列为时间列，则窗口就是沿着datetime列滑动。首先匹配条件1 ，如果有一列满足则待返回值变成1；然后去匹配条件 2，如果条件 2 匹配，并且跟条件 1 相差时间在 3600 秒内，则待返回值加一；接着匹配条件3，过程与条件2相同。最后返回一个计数值，该值表明这些可变条件满足多少个。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d1558ccb7e064d17328e5b4c6a78a06f.png\" /></p><p></p><p><code lang=\"sql\">-- 哪些用户在登录后2分钟内观看了广告\nselect id, window_funnel(120, 'default', a_time, act = 'login', act = 'view') as w  from app  group by id;\n+------+------+\n| id   | w    |\n+------+------+\n|  333 |    2 |\n|  222 |    2 |\n|  111 |    2 |\n|  444 |    1 |\n+------+------+\n-- 从上述SQL可以看出，第一个参数120秒指定了滑动窗口的大小，并且窗口是按a_time这一列滑动的。act = 'login', act = 'view'是两个条件，显然用户111，222,333均满足这两个条件，即一小时内，先登录再观看广告；但是444这位用户就不满足先登录再观看广告，因为他只有登录操作，所以window_funnel对他返回了1，因为他只满足第一个条件\n\n-- 根据上述结果进行筛选，哪些用户在登录后2分钟内观看了广告\nselect id, window_funnel(120, 'default', a_time, act = 'login', act = 'view') as w  from app  group by id having w = 2;\n+------+------+\n| id   | w    |\n+------+------+\n|  111 |    2 |\n|  333 |    2 |\n|  222 |    2 |\n+------+------+\n\n\n-- 哪些用户在一小时内，登录、观看、点击都执行了\nselect id, window_funnel(3600, 'default', a_time, act = 'login', act = 'view', act = 'click') as w from app  group by id having w = 3;\n+------+------+\n| id   | w    |\n+------+------+\n|  111 |    3 |\n+------+------+\n</code></p><p></p><h3>其他</h3><p></p><p></p><h4>Array 类函数</h4><p></p><p>熟悉行为分析的同学都知道，固然丰富的分析函数有助于帮助我们提高分析效率，但是分析函数无法覆盖所有的场景，一些特殊的需求还是依赖特殊或者复杂的 SQL 来实现，而这些 SQL 很多都需要借助数组来实现。鉴于篇幅所限，该部分不会展示纷繁复杂的需求，而是会通过几个浅显的例子来展示 Apache Doris 丰富的数组类函数。</p><p></p><p><code lang=\"sql\">// split_by_string函数：指定分隔符切分字符串，得到切分后的数组：\nselect split_by_string('a#b#c#d','#');\n+---------------------------------+\n| split_by_string('a#b#c#d', '1') |\n+---------------------------------+\n| ['a', 'b', 'c', 'd']            |\n+---------------------------------+\n\n\n// array_sort函数：对数组进行升序排序。下表的k1是数组类型：\nselect k1, array_sort(k1) from test;\n+-----------------------------+-----------------------------+\n| k1                          | array_sort(`k1`)            |\n+------+-----------------------------+----------------------+\n| NULL                        | NULL                        |\n| [1, 2, 3, 4, 5, 4, 3, 2, 1] | [1, 1, 2, 2, 3, 3, 4, 4, 5] |\n+-----------------------------+-----------------------------+\n\n// array_size函数：获取数组大小。下表的k1是数组类型：\nselect k1,size(k1) from test;\n+-----------+------------+\n| k1        | size(`k1`) |\n+-----------+------------+\n| [1, 2, 3] |          3 |\n| []        |          0 |\n| NULL      |       NULL |\n+-----------+------------+\n\n// array_remove函数：返回移除所有的指定元素后的数组。下表k1是数组类型：\nselect k1, array_remove(k1, 1) from test;\n+--------------------+-----------------------+\n| k1                 | array_remove(k1, 1) |\n+--------------------+-----------------------+\n| [1, 2, 3]          | [2, 3]                |\n| [1, 3]             | [3]                   |\n| NULL               | NULL                  |\n| [1, 3]             | [3]                   |\n| [NULL, 1, NULL, 2] | [NULL, NULL, 2]       |\n+--------------------+-----------------------+\n\n\n// array_slice函数：返回一个子数组，包含所有从指定位置开始的指定长度的元素。下表k1是数组类型。从1开始从左至右计数；位置可以为负数，负数从-1开始从右到左开始计数\n\nselect k1, array_slice(k1, 2, 2) from array_type_table_nullable;\n+-----------------+-------------------------+\n| k1              | array_slice(`k1`, 2, 2) |\n+-----------------+-------------------------+\n| [1, 2, 3]       | [2, 3]                  |\n| [1, NULL, 3]    | [NULL, 3]               |\n| [2, 3]          | [3]                     |\n| NULL            | NULL                    |\n+-----------------+-------------------------+\nselect k1, array_slice(k1, -2, 1) from test;\n+-----------+--------------------------+\n| k1        | array_slice(`k1`, -2, 1) |\n+-----------+--------------------------+\n| [1, 2, 3] | [2]                      |\n| [1, 2, 3] | [2]                      |\n| [2, 3]    | [2]                      |\n| [2, 3]    | [2]                      |\n+-----------+--------------------------+\n\n\n// array_distinct函数：去除数组中的重复元素。下表k1是数组类型\nselect k1, array_distinct(k1) from test;\n+-----------------------------+---------------------------+\n| k1                          | array_distinct(k1)        |\n+-----------------------------+---------------------------+\n| [1, 2, 3, 4, 5]             | [1, 2, 3, 4, 5]           |\n| [6, 7, 8]                   | [6, 7, 8]                 |\n| []                          | []                        |\n| NULL                        | NULL                      |\n| [1, 2, 3, 4, 5, 4, 3, 2, 1] | [1, 2, 3, 4, 5]           |\n| [1, 2, 3, NULL]             | [1, 2, 3, NULL]           |\n| [1, 2, 3, NULL, NULL]       | [1, 2, 3, NULL]     |\n+-----------------------------+---------------------------+\n\n\n// 配合array使用较多的explode函数，可以轻松实现列转行：\nselect e1 from (select 1 k1) as t lateral view explode([1,2,3]) tmp1 as e1;\n+------+\n| e1   |\n+------+\n|    1 |\n|    2 |\n|    3 |\n+------+\n\n// explode_split函数：按分隔符分割字符串，并将结果打散，实现列转行：\nselect * from example1;\n+---------+\n| k1      |\n+---------+\n| a, b, c |\n+---------+\nselect e1 from example1 lateral view explode_split(k1, ',') tmp1 as e1;\n+------+\n| e1   |\n+------+\n|  b   |\n|  c   |\n|  a   |\n+------+\n</code></p><p></p><h2>总结</h2><p></p><p>通过 Apache Doris 系统自身的优异能力和丰富的行为分析函数，已经有越来越多的企业选择基于 Apache Doris 构建高效的用户行为分析平台，更多案例欢迎关注 SelectDB 公众号以及相关技术博客。后续我们仍会持续加强这方面的能力，包括提供更丰富的数据类型以及行为分析函数，如果您在搭建用户行为分析平台过程中遇到任何问题，欢迎联系社区进行支持。同时也欢迎加入 Apache Doris 社区，一起将 Apache Doris 建设地更加强大！</p><p></p><p>作者介绍：</p><p></p><p>李仕杨，SelectDB 生态研发工程师，Apache Doris Contributor</p><p></p><p></p>",
    "publish_time": "2023-07-28 17:30:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "SOFAServerless 应用架构助力业务10倍效率提升，探索微服务隔离与共享的新平衡｜QCon",
    "url": "https://www.infoq.cn/article/7zJ1h3rExZwlYRhsXDkw",
    "summary": "<p>当前<a href=\"https://qcon.infoq.cn/202309/beijing/track/1550?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">微服务</a>\"发展至今，仍然存在几个问题：</p><p></p><p>业务开发者需要感知复杂基础设施，启动慢（分钟级），研发效率低拆分微服务的成本高：拆分后每个子应用都包含公共部分（框架、中间件等），资源成本高，且需要长期维护拆分微服务的敏捷度与业务、组织发展的敏捷度不一致，如何合理的拆分微服务始终是个老大难的问题，拆得多造成资源和管理成本，拆的不够造成协作效率问题……</p><p></p><p>蚂蚁在研究了业务痛点后，采用分层思维模式，在传统微服务只是横向拆分基础上，改进成同时进行纵向和横向拆分，纵向拆分成基座和模块，使得模块不占用额外机器（节省容量），模块开发者只关注业务自身（认知负荷低）；横向拆分成多个模块，让模块开发者可以独立迭代互不干扰（协作效率高）。</p><p></p><p>当前蚂蚁内已经全部 BG 共 40W Core 应用使用该新型应用架构模式，构建产物从原来的 GB 下降到 MB, 业务启动降到秒级，平均迭代效率提升 10 倍。</p><p></p><p>即将于 9 月 3-5 日举办的<a href=\"https://qcon.infoq.cn/202309/beijing/track?utm_source=infoqweb&amp;utm_medium=presentation&amp;utm_campaign=9&amp;utm_term=0728&amp;utm_content=zhaozhenling\"> QCon 全球软件开发大会·北京站</a>\"，邀请到蚂蚁集团技术专家<a href=\"https://qcon.infoq.cn/202309/beijing/presentation/5407?utm_source=infoqweb&amp;utm_medium=presentation&amp;utm_campaign=9\">赵真灵</a>\"前来分享以上经验。他于 2018 年加入蚂蚁集团， 曾负责基于 K8s Deployment 的应用发布运维平台建设、K8s 集群的 Node / pod 多级弹性伸缩与产品建设。当前主要负责应用架构演进和 Serverless 相关工作，完成蚂蚁新应用架构研发框架与平台的设计落地，全面应用于蚂蚁集团内部业务线，为线上 40W core 提供秒级验证发布能力，获得 2022 年信通院云原生技术创新奖。同时，也是 SOFAArk 社区的开发和维护者以及 KNative 社区的贡献者。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9a/9a0dd3fcd5e8a646c49f71fb909353c3.png\" /></p><p></p><p>为什么我们推荐你来听这个分享，因为——</p><p></p><p>现有应用可以平滑接入使用该方案，改造成本低提升效果明显，从原来分钟级启动降至秒级甚至毫秒级，且可以多个业务并行迭代支持业务按需进行微服务的拆分，支持平滑回退或演进成独立微服务</p><p></p><p>但赵老师坦言，在实践过程中，该方案也存在一些痛点——</p><p></p><p>在 Java 技术栈里，现有技术体系大多都是基于单 ClassLoader 来实现的，而该模式是基于多 ClassLoader 的，存在一定的不一致情况多个模块合并部署在一个 JVM 内，有资源抢占</p><p></p><p>在本次分享中，他也会分享相关应对经验，欢迎各位前来交流。此外，据了解，该方案正在开源中，感兴趣的同学可以入群了解详情，钉钉群号：24970018417。</p><p></p><p>活动推荐</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！ 现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/247116a4574c4fc713fe9fb14a1286fe.png\" /></p><p></p>",
    "publish_time": "2023-07-28 17:30:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "初识 SoFlu 庐山真面目：与传统 Java 开发有何区别？",
    "url": "https://www.infoq.cn/article/RLw7o3FuPzHBysF01MpR",
    "summary": "<p>在现代企业开发中，初级开发者难以掌握开发企业级 Java 应用，中高级开发者则被繁琐任务所占据。为了让大家能快速上手可以解决这个难题的“SoFlu 软件机器人”，前蚂蚁金服技术专家杨彪设计了《2 小时学会 10 个组件“取代”传统 Java 开发》课程，本节为该系列课程的第一讲。</p>\n<p>在本节课程中，杨彪将整体介绍 SoFlu 的项目管理、配置中心、系统配置、组件列表、接口管理、资源管理、实体模型等功能，帮助 0 基础使用小白快速了解“ SoFlu 软件机器人”的功能。同时通过对比传统 Java 开发和使用“SoFlu 软件机器人”开发增删改查功能的区别，详细介绍“SoFlu 软件机器人”提供的接口管理、资源管理、实体管理、数据库设计和 SQL 管理等模块，帮助大家理解“ SoFlu 软件机器人”全新开发模式的过程，并进行优劣点分析。</p>\n<p>大家在课后可以登录 Gitee 下载 SoFlu 软件机器人客户端进行实践：<a href=\"http://suo.im/8wROo\">点击下载</a></p>\n<p>大家可以扫码添加小助手，进学习群与专家一对一交流：<br />\n<img alt=\"\" src=\"https://static001.infoq.cn/resource/image/e8/c9/e8833a01ba0bc705acab14a572b57cc9.png\" /></p>",
    "publish_time": "2023-07-28 17:54:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "查询提速400倍！河北幸福消费金融基于 Apache Doris 构建实时数仓技术实践",
    "url": "https://www.infoq.cn/article/aqSHzkvfUX6VglrT3u5d",
    "summary": "<p></p><blockquote>随着河北幸福消费金融的客户数量和放贷金额持续上升，如何依托大数据、数据分析等技术来提供更好决策支持、提高工作效率和用户体验，成为了当前亟需解决的问题。基于此，公司决定搭建数据中台，从基于 TDH 的离线数仓再到基于 Apache Doris 的实时数仓，最终统一了数据出口，提升了数据质量，并实现查询速度近 400 倍的提升。本文将详细为大家分享河北幸福消费金融数据中台搭建经验和应用实践，希望为其他企业带来一些有益的参考。</blockquote><p></p><p></p><p>作者｜河北幸福消费金融 信息科技部</p><p></p><p>河北幸福消费金融股份有限公司由张家口银行发起设立，是 2017 年 6 月正式开业的全国第 22 家、河北省首家消费金融公司，主要面向个人客户发放最高额不超过 20 万元的普惠、小额、信用消费贷款。目前公司服务区域覆盖全国 32 个省级单位，相继获评为国家科学技术部认定的全国高新技术企业、河北省科技型中小企业和石家庄市科技“小巨人”企业。</p><p></p><p>随着客户数量和放贷金额持续上升，如何依托大数据、数据分析等技术，为各业务线人员提供更好的决策支持，如何提高工作效率、为客户提供更佳的使用体验，成为了当前亟需解决的问题。具体需求如下：</p><p></p><p>高管看板类：搭建高管驾驶舱帮助高层快速了解公司当前的整体经营状况，驾驶舱集成全业务线数据，包括实时业务指标和离线业务指标，在该场景下我们希望查询结果可以在 毫秒内返回，便于管理层进行高效决策。实时变量类：为给风控决策提供实时支撑，需在 500ms 内返回全产品线查询结果，并可基于申请、授信、关联人、逾期以及还款等基础信息，计算产品级、客户级和借据级高维度衍生变量 。决策分析类：为给各业务部门提供业务分析和决策支撑，需在秒级返回年、季、月多维度的主题报表，特别是风险部门，需要从放款开始回溯全生命周期的运营情况；而计财部门，则需要通过以往经营数据表现，预测未来的盈利，数据量大且逻辑复杂。风险建模类：提供全量、明细级的数据，用于风险建模的变量跑批，满足对客户评分评级，参与审批、授信等核心业务的要求，以支持业务指标的观测、投入产出分析、变量筛选以及决策制定等业务需求。监管合规类：为确保业务符合相关法律法规、行业标准等规范，需要根据监管合规的要求进行合规子系统的定期上报，上报的数据分为指标汇总项数据和明细数据。</p><p></p><p>为了满足不同业务线对数据分析的需求，公司开始搭建数据中台并对进行优化。最初，公司基于商业化产品 TDH 搭建了离线数仓，以满足基本数据分析需求。然而，随着数据时效性的提高和实时分析需求增加，公司迫切需要搭建一款实时数据仓库。因此引入了 Apache Doris 并在此基础上搭建了实时数仓，最终建立了一个高效、稳定的数据中台。本文将详细为大家分享河北幸福消费金融数据中台搭建经验和应用实践，希望为其他企业带来一些有益的参考。</p><p></p><h2>基于 TDH 的离线数仓</h2><p></p><p>因早期主要解决的是离线分析需求，优先基于 TDH 集群建设离线数仓。通过 Sqoop、DataX 将上游数据采集到离线数仓，经过标准化数据清洗，完成数仓的日常跑批。离线数仓架构图如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0192ff6712b389cd216ce2e781fdaa98.png\" /></p><p></p><p>随着数据积累和业务人员对数据时效的要求越来越高，基于 TDH 离线平台的一些问题逐渐显现出来：</p><p></p><p>数据采集同步时效慢：离线数仓抽数工具依赖于 Sqoop、DataX 等组件，而受限于调度周期，此类工具采集的数据必然有滞后性。资源冲突大：离线数仓每日跑批时间跨度大，一般从凌晨到下午5点左右，这将导致跑批和即席查询之间发生资源冲突，影响业务人员的使用体验。查询分析慢：在使用离线平台进行自定义统计分析和数据探索时，查询分析响应速度慢、时效性难以保障，严重影响工作效率。T+1 延迟高：各业务线对实时数据处理的需求在逐渐增多，T+1 的数据已经无法适应数据快速获取和产生业务价值的诉求。报表定制周期长：报表定制化开发有固有的迭代周期，难以满足业务人员对数据的灵活多样的分析探索。烟囱效应：定期上报数据时，数据中台需要从多个业务系统中拉取数据。当业务系统发生变更后，会波及上述相关的报送子系统，形成烟囱效应。</p><p></p><h2>技术选型</h2><p></p><p>为了解决上述问题，我们迫切需要一款 MPP 引擎来构建实时数仓。对于新引擎我们有几个基本的要求：首先，需要简单易上手，以便团队快速掌握和使用；其次，需要具备强大的数据导入能力，以便快速高效地导入海量数据；同时，需要兼容离线数仓相关工具，以便与现有的数据处理工具和技术体系无缝衔接；此外，搭建和切换成本也需要低，以便快速部署使用和进行扩缩容；最后，它需要具备较好的并发能力和优异的查询性能，以便支持高并发、复杂查询等业务场景的需求。</p><p></p><p>在以上选型要求驱动下，我们对目前比较流行的 ClickHouse 与 Doris 进行了系统的调研，其中 Apache Doris 更符合我们的选型的要求，具体原因如下：</p><p></p><p>部署成本低：Doris 采用分布式技术架构，部署只需两个进程，不依赖其他系统，在线集群扩缩容，自动副本修复，部署及使用成本较低。快速上手使用：Doris 采用主流的分区分桶设计思路，索引结构与 MySQL 的思路类似，相关人员在使用 Doris 时无需学习大量的新知识。相比之下，ClickHouse 在建库建表需要分别指定类型，使用流程相对比较繁琐，上手难度也比较高。工具兼容：业务人员通常使用 TDH 的客户端工具 WaterDrop 进行离线数仓查询，Doris 通过标准协议链接可完美兼容 WaterDrop，而 ClickHouse 无法兼容。数据生态圈丰富：Doris 数据生态圈丰富，与 Flink、Kafka 等组件结合度较高，同时支持联邦查询，提供了丰富的数据导入和接入方式，可以满足多场景下的数据处理需求。高并发能力：我们对 Doris 进行了性能压力测试，在高并发和大数据量的情况下，Doris 表现出较好的性能和稳定性，能够满足不同业务场景的需求。社区活跃度高：Doris 社区非常活跃，有大量的开发者和用户参与其中，提供了丰富的技术支持和解决方案。同时 Doris 社区提供了全面的文档和资料，方便用户学习和使用 Doris。此外，SelectDB 为社区提供了一支全职专业的技术团队为社区用户提供服务与支持，任何问题均可得到快速响应。</p><p></p><h2>基于 Doris 的实时数据仓库</h2><p></p><p>在离线数仓的基础上，使用 Doris 结合 CDH 集群、Airflow 集群搭建了实时数仓，实时数仓的数据来源主要为离线数仓和 MySQL，使用 Flink CDC 结合 PyFlink（使用 Python 调用 Flink  的 API，简称 PyFlink）将 MySQL 中的数据实时地采集到核心计算引擎 Doris 中（后文将详细介绍），上层为 Airflow 分布式调度系统，可以将实时任务进行常规化的调度运维。我们对 Doris 引擎进行了基本数仓分层，数据经过各层处理后统一为各场景提供数据服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e2/e2b54a887c47bbc8ad22efebed38b20e.png\" /></p><p></p><p></p><p>基于 Doris 提供的丰富的导入方式，我们可以快速将离线数仓中的实时数据清洗整合接入到 Doris 集群中，实现数据的快速迁移。目前我们已经将基于 TDH 的查询分析和数据探索服务全部转移到 Doris 引擎上，借助 Doris 引擎快速计算的能力和优异的查询性能，可以更高效地进行数据处理和分析，业务处理速度和效率得到显著提升。</p><p></p><p>以某 SQL 为例，该 SQL 主要应用在信贷审批场景。我们对比了原有架构和新架构从十万、千万，亿级别的三个大表中的查询返回速度。结果显示，在过去 TDH 架构中执行查询需要 11 分 30 秒才可返回结果，而在基于 Doris 的新架构中仅需要 1.7 秒即可返回结果，速度提升近 400 倍！</p><p></p><p>数据规模：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3a8ae36dfc04c4d7152582bdd0c89ff4.png\" /></p><p></p><p>原有离线数仓：需要 11 分 30 秒才可返回结果</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8a35137c87c60c933d2a93b446b17b9f.png\" /></p><p></p><p>基于 Doris 的新数仓：优化查询后仅需要 1.7 秒即可返回结果，有时甚至可以 1 秒内返回。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/34/3402da06b480d347e9e8d7d07e456dcc.png\" /></p><p></p><h2>应用实践</h2><p></p><p></p><h3>实时数据归集</h3><p></p><p>公司的业务系统通常是按照产品进行库划分，各个产品表结构保持一致。而实时数仓核心功能就是依靠 Doris 丰富的导入能力，将分散的库对应的相同的逻辑表归集到 Doris 下的同一个逻辑表上，汇集后的数据也能在监管主题层面进行整体调整，避免烟囱效应的发生。汇集的实时数据进入数仓后，会主动触发衍生变量的自动计算，更新衍生变量的值。而衍生变量的汇总值在一个单独的表中，当进行查询时，可以实现毫秒级别的查询响应。</p><p></p><p>在进行实时数仓归集时，首先需要确定 FlinkCDC、Flink 、Flink on Yarn、Apache Doris 等核心组件的版本号，接着基于 PyFlink 进行产品化自动接入实时数仓的建设。具体操作如下：</p><p></p><p>在数据层面，将业务系统数据库按照水平和垂直进行切分，以提升读写性能并增加高可用性。在数仓层面，我们对业务表的数据进行了维度汇集，以便进行更好的统一汇总分析。在数据接入方面，我们需要高效地接入现有业务系统的存量数据，并持续稳定地接入增量数据。</p><p></p><p>此外，我们还提供了标准化的接入方案和接口，以满足不同业务场景的需求。</p><p></p><p>使用步骤：</p><p></p><p>1、接入配置表：配置归集的业务库表的相关信息</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1b1e7292afabc35aaf4b8926b9e11a36.png\" /></p><p></p><p>2、调度系统部署：通过调度系统部署实时归集的任务</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/957017277548344cc48aae86eb921189.png\" /></p><p></p><p>3、任务常规运维：我们对任务上线、启动、停止和异常恢复处理等功能进行了高度封装，并与分布式调度系统   Airflow 进行了深度集成和融合。使用人员不必关心底层细节，可以轻松地将 MySQL 表一键迁移到 Doris，实现存量和增量数据的自动化迁移。经沟通，社区目前已发布了 <a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247521271&amp;idx=1&amp;sn=4f1a52806a06b8062cb206f586398463&amp;chksm=cf2f9df0f85814e69563a494fc9ee5824a3d02e916651b66b9090aa1b33c109694d6e4571657&amp;token=1510467837&amp;lang=zh_CN#rd\">Doris-Flink-Connector 1.4.0 版本</a>\"，该版本集成了 Flink CDC，可以实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。</p><p></p><h3>数据质量监控</h3><p></p><p>离线数仓存在各种数据质量问题，这些问题通常在数据跑批时才会暴露出来，导致数据修复时间窗口急剧被压缩。为了解决这个问题，我们利用 Doris 建立了数据质量监控系统，同时将离线数仓的数据质量监控模型迁移到  Doris 。基于该系统可以实时监控业务指标和数据质量，并在发现问题时及时进行人工干预或报警，提高离线数仓跑批的稳定性和效率。另外当实时数仓获取归集后的数据后，可通过数据质量监控系统的校验规则第一时间对数据质量进行实时检查，保证数据归集的准确性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/de92951f615a5e1b94dc601bde282888.png\" /></p><p></p><p>目前我们已经将 30% 的数据监控指标和 35 个业务指标迁移到 Doris 实时集群上，每月可成功规避问题 3 次以上，有效提升了离线数仓跑批的数据质量。后续我们将继续将更多的数据监控指标和业务指标迁移到 Doris 集群中，以进一步提高数据处理的效率和质量。</p><p></p><h2>数据联邦查询</h2><p></p><p>各业务条线的核心数据存储在不同类型的数据库中，如 MySQL、Hive、ES 等。Apache Doris 1.2 版本提供的 Multi Catalog 功能可以统一数据查询出口、实现联邦查询，为数据分析提供了极大的便利。同时，借助 Doris 的持久化能力，可以通过外表的方式快速同步其他数据源数据，方便快捷。此外，通过 Apache Doris 聚合查询、向量化引擎等技术的加持，我们真正实现了高效的数据统一门户，提高了数据分析的效率。</p><p></p><h1>优化经验</h1><p></p><p></p><h2>负载均衡</h2><p></p><p>随着 Doris 接入的业务量不断增加，FE 的负载也在不断增长。为了实现 Doris 的高可用性，我们增加了 FE 节点数，在多个 FE 节点上部署负载均衡层。我们选择基于 Nginx TCP 反向代理的方式来构建 FE 的负载均衡，有效地实现了 FE 角色之间的负载均衡。具体配置方式如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/51af7203f797dc780291ae0b6dcf501b.png\" /></p><p></p><h2>查询优化</h2><p></p><p>当前审批系统的业务数据持久化在关系型数据库 MySQL 中，累计总进件量将近 2.8 亿。为了应对日益膨胀的的数据问题，业务系统的数据库采用了分表和数据归档的设计思路。但是在业务上，我们仍需要对全量数据进行业务查询，并且时效要求在 3 秒内返回结果。以下是查询需求的抽象分类：</p><p></p><p>以“申请编号”，“客户编号”，“身份证号”，“核心客户号”中的一个或多个作为查询条件进行查询以“申请日期”或“更新日期”中的一个条件，结合“姓名”、“申请类型”、“进件渠道”、“白名单渠道”、“决策阶段”、”审批类型”、”审批结果”等形成复核条件做查询以“申请日期”或“更新日期”中的任意一个为条件，对近一周的审批明细数据进行查询查询</p><p></p><p>为了满足以上查询场景的需求，我们将审批进件数据结合 Doris 引擎的分区分桶技术、布隆过滤器和位图索引进行合理的设计，最终整体实现了满足业务上 3 秒内的查询效率需求。</p><p></p><p>优化策略：</p><p></p><p>分区：apply_time</p><p></p><p>分桶：ID、database_name、table_name</p><p></p><p>布隆索引：id_number, bhb_customer_id, customer_name, customer_id, serial_no</p><p></p><p>位图索引：apply_source,white_channel,approval_result,approval_status,product_type,decision_stage</p><p></p><p>基于上述查询的压测指标效果如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1b8348acd3e3f59de9fc34a4b1de1bb7.png\" /></p><p></p><h1>运维管理</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f77846ed7f8039d3b22bb384ee686832.png\" /></p><p></p><p>通过 Doris 提供的 Prometheus 和 Grafana 可以快速获取 Doris 集群的整体健康状况以及各个角色的多方面指标值。同时我们还将监控平台与公司统一告警平台进行二次融合，告警平台可以通过 API 获取 Prometheus 的基础指标值与阈值进行比较，从而触发不同级别的报警或者达到服务自动重启。此外，我们在 FE 和 BE 服务级别上实现任务的自动运维，确保在服务异常时能够自动拉起，保证核心服务的可用性。</p><p></p><h1>总结收益</h1><p></p><p>Doris 已经在公司内部得到了广泛的应用，目前已搭建数十台集群规模，为公司带来了以下收益：</p><p></p><p>数据处理时效提升：数据处理的时效从 T+1 到实时，解决了离线数据延迟的问题。秒级查询响应：借助 Doris 分区分桶、物化视图、布隆索引等功能进行查询优化，即席查询的速度从原先的 20 分钟左右降低到分钟甚至秒级响应，相较之前有近 400 倍的速度提升。统一查询出口：依赖于 Doris 强大的导入能力和 Multi Catalog 功能成功将各业务库的数据整合汇总到 Doris 中，由 Doris 统一提供数据查询及分析服务，极大的提升了查询分析响应效率。提升数据质量：基于 Doris 建立了数据质量监控系统，目前我们已经将 30% 的数据监控指标和 35 个业务指标迁移到 Doris 实时集群上，有效提升了离线数仓跑批的数据质量。</p><p></p><p>综上所述，Doris 在公司内部的广泛应用，为我们带来了多方面的收益，助力企业提升数据分析效率、降低数据管理成本、实现统一、实时、高效的数据中台建设，为业务向好发展注入了新的动力。</p><p></p><p>未来我们将继续扩大 Doris 的使用范围，在实时、性能、时效要求更高的业务领域发力，其次我们还将尝试使用 Doris 更多的功能及新特性，一方面深化 Doris 在公司的使用，另一方面我们会将真实的使用体验反馈到社区，帮助 Doris 进一步迭代优化。</p><p></p>",
    "publish_time": "2023-07-28 18:03:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]