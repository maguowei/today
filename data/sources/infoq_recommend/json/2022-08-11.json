[
  {
    "title": "在优化设计过程中为什么DesignOps很重要",
    "url": "https://www.infoq.cn/article/QUbPttfgyPAucmpHxBri",
    "summary": "<p>在设计团队规模不断扩大，项目的不断发展由最新的用户界面（UI）和用户体验（UX）所主导的大环境下，它们的成功不仅取决于技能，还取决于关键的操作和用户体验指标。</p><p>&nbsp;</p><p>当设计过程有时比它们所需要的还要更复杂、分散和混乱时，必须将DesignOps作为一项战略职能。</p><p>&nbsp;</p><p>但是，DesignOps的概念已经发生了显著的变化，现在它的实际应用已经远远超出了优化自己的设计工作流的范围。当今的设计运维需要共同的文化转变和不同的思维方式：</p><p>&nbsp;</p><p>培养跨职能，包括设计师和开发人员在内，通过项目经理（PM）、利益相关方和营销人员，更好地传递想法和结果，提高最终产品的质量，并增加团队共同产生的影响。欢迎围绕已经建立的用户界面（UI）和用户体验（UX）实践使用工具、系统和服务来完善路线图，消除孤岛，减少重复性任务，帮助建立单一的事实源，并提高数字产品设计和研发的速度和质量。理解数据是一座金矿，设计过程中的每一个决策和步骤都必须基于从调查、用户、可用性测试以及用户评估中收集到的可衡量的统计数据。</p><p>&nbsp;</p><p>使用这三个概念——跨职能团队、工具和数据的使用，你可以构建更好的工作设计操作，并将其作为战略职能使用，从而产生更好的产品、更快的上市时间，甚至增加收入。</p><p>&nbsp;</p><p>尽管使用设计运维并不是一种新的实践，但直到最近才有很多公司了解它的价值。Airbnb是首批创建并确立将DesignOps战略作为其现代产品开发周期一部分的全球化公司之一。他们的目标是“通过能提高执行速度和质量的集中式工具、系统和服务来为整个产品组织提供敏捷性。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3d8022eaf8602a22814c8bad0c7180f.png\" /></p><p></p><p>为了建立更有效的流程，Airbnb将其DesignOps扩展到5个不同的部分，每个部分都专注于其数字产品设计和开发的特定方面。他们的设计流量增长总监<a href=\"https://airbnb.design/designops-airbnb/\">Adrian Cleave</a>\"解释说：“我们的职能包括设计项目管理、设计工具、本地化、制作设计和团队协调人员。我们与营销、产品、设计和工程部门密切合作，尽可能地创造最佳的用户体验。”</p><p></p><h2>整合的重要性和挑战</h2><p></p><p>&nbsp;</p><p>最初，Airbnb的案例听起来像是他们只需要将团队和任务结合在一起，但要将类似的战略职能应用到成功的DesignOps中，首先需要解决以下的挑战：</p><p>&nbsp;</p><p>用户体验创建者使用的特殊工具和流程会导致设计-开发的不一致性和运维功能障碍，这些问题会影响到整个数字产品链。有一种误解，即认为存在一种适用于一切流程的解决方案，而实际上每个项目都需要在微观层面上进行探索，根据其具体情况，需要对设计运维进行调整和更改。缺乏深入的沟通和定期的多学科会议，在这些会议上用户体验（UX）和用户界面（UI）设计、开发、产品管理团队和营销团队可以共同行动，清晰地交流设计理念、原型、愿景、目标和细节等。忽视了数字产品设计和开发工具的重要性，这些工具是操作事实的单一源。没有探索和测试新的想法、新的设计方法，因此落后于用户界面（UI）和用户体验（UX）设计的新趋势，这对于非常精通新时代技术的数字消费者来说是一种排斥。</p><p></p><h2>理解DesignOps的4Ps</h2><p></p><p>&nbsp;</p><p>当谈到DesignOps时，会涉及到四个关键部分或构建模块。 我称它们为4Ps：产品（Product）、流程（Process）、人员（People）和程序（Program）。 每个环节都能为设计师和数字产品开发团队带来了各自的好处。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/50609962ec1c2ed7406a135fc91c5032.png\" /></p><p></p><p>&nbsp;</p><p>DesignOps中以产品为中心（Product-focused）的部分由组织思想引导。它致力于：</p><p>&nbsp;</p><p>发现并消除瓶颈。阐明数字产品的目的，了解客户的需求，并设想它将为最终用户带来的价值。执行用户和可用性测试以获得更好的用户体验（UX）。创建一个重点突出的路线图，以交付从启动到测试再到交付的高级别设计项目。概述给定产品的工作分解结构，以便团队成员知道从哪里开始和结束。定义<a href=\"https://www.infragistics.com/community/blogs/b/jason_beres/posts/build-great-user-experiences-with-user-centered-design\">以用户为中心设计</a>\"的优劣通用标准和原则，特别是针对你正在开发的产品。定义、选择并调整设计质量指标，这些指标可以在团队中任何参与给定项目的人之间共享。构建情境和项目意识，使任何参与的相关人员都能保持一致，这样更容易跟踪项目进展。</p><p>&nbsp;</p><p>&nbsp;</p><p>流程（Process）的“P”围绕着这样的一个前提展开，即DesignOps关注的是组织的优先级和运作。这将业务和具体的设计项目结合在了一起，并专注于：</p><p>&nbsp;</p><p>在跨职能团队之间制定更好的工作流程。通过与来自不同部门（项目经理、开发人员等）的团队成员的定期会议来跟踪所有正在进行的事情。保留有关战略业务目标、工作范围和文档的信息作为指导。这样，你就可以更容易地跟上品牌要求和业务目标。确定最后期限，组织任务，并确定项目的优先顺序（如果有多个项目的话）。比如，这将会帮助你规划某些功能并确定它的优先级。</p><p>&nbsp;</p><p>人员（People）的“P”强调社交化，而不仅仅是分享更新。此处设计运维的重点是为沟通需求、技能、目的和设计团队能力提供更好的方法。主要的优先事项包括：</p><p>&nbsp;</p><p>消除沟通障碍和隔阂。明确用户界面/用户体验（UI/UX）设计部门中每个人的角色。确定设计师的具体需求，并尽可能确定技能差距。传达共同的愿景和共同的目标，以构建成功的产品和/或服务。参与设计冲刺，以改进你和你的团队如何看待和理解给定设计项目的范围、特性、目标和方向，避免设计偏差。了解设计团队的能力和工作量，以避免工作倦怠。</p><p>&nbsp;</p><p>DesignOps的第四个部分则专注于程序（Program），它致力于标准化的自动化DesignOps工具和技术。它需要：</p><p>&nbsp;</p><p>依靠相同的DesignOps工具、数字资产管理程序（DAMS）或其他数字产品开发平台来简化和支持数字产品设计工作流。开发用户研究数据库存储，使产品设计和开发过程中的每个人都能轻松访问。通过全方位的设计和沟通工具，在团队成员和部门之间建立跨职能协作。从参与设计项目的任何人那里获得设计构思、评估和建设性反馈。克服沟通不畅的障碍，并解决简单的日常运作问题，例如，开发人员不知道由哪个设计师负责创建某个特性。</p><p></p><h2>结合4Ps，改进设计工作流程和操作的技巧</h2><p></p><p>&nbsp;</p><p>采用单一的操作事实源。</p><p>&nbsp;</p><p>这将帮助你和整个数字产品团队（包括市场营销人员、项目经理和开发人员）管理估计、规划和跟踪孤岛，跟踪任务和进度，并清晰地理解设计和开发规范。Gartner建议在此过程中使用通用的企业敏捷规划（EAP）工具和敏捷实践来提供帮助。</p><p>&nbsp;</p><p>在他们的“<a href=\"https://www.gartner.com/doc/reprints?id=1-28D1TLEQ&amp;ct=211208&amp;st=sb\">DesignOps：快速组织、协作和创新产品用户体验</a>\"”一文中，作者指出：</p><p>&nbsp;</p><p>“DesignOps的一个基本方面是采用敏捷工作分解结构 (WBS) 来组织用户体验工作，从与大的战略目标保持一致，到单一EAP工具中的屏幕级细节。虽然这对大多数用户体验实践者来说一开始感觉很陌生，但敏捷WBS能很好地映射到用户体验工作中。这种方法的业务和运营效益是深远的，包括更准确的计划、估计、跟踪和报告。”</p><p>&nbsp;</p><p>作为DesignOps战略的一部分，通过为管理人员、设计师、开发人员甚至利益相关方提供一个单一的工作环境，每个人都可以轻松地调整他们的工作和任务，实时测试和评论原型，消除设计切换，减少成本高昂的迭代，跟踪进度并识别瓶颈。更重要的是，整个数字产品团队将受益于对最终用户、项目时间表、设计细节、特性和功能的即时情境感知。</p><p>&nbsp;</p><p>但是由于公司和用于EAP的自动化DesignOps工具的不同，结构可能会有所不同。</p><p>&nbsp;</p><p>培养跨职能团队之间的深度沟通，并使利益相关方参与其中。</p><p>&nbsp;</p><p>你可能会问“为什么是利益相关方？”当一个原型被丢弃，一切都必须从头开始时，这会导致时间、精力、金钱的浪费并会延长流程。为了避免这种情况的发生并构建出更高效的DesignOps，请确保参与的每个人（包括利益相关方）都能参与到协作创造力中。这有助于简化设计过程，并能极大地缩短设计开发时间。</p><p>&nbsp;</p><p>通常情况下，团队可以在流程的早期进行包括设计师和利益相关方在内的冲刺工作，以探索并测试不同的想法。通过这种方式，设计师可以直接洞察设计元素如何转化为网页、移动或桌面应用程序，而开发人员可以根据设计与代码沟通潜在的问题，利益相关方可以充当代理，测试产品的可用性，以提供新鲜且独立的视角。</p><p></p><p>了解有关公司、当前设计流程、项目管理和最终用户的所有信息。</p><p></p><p>这将帮助你和你的团队评估产生投资回报（ROI）的优势领域，消除最大的挫折，解决设计过程中的痛点，并从那里开始优化。召开公司和团队会议可以为你提供这些有价值的信息。</p><p>&nbsp;</p><p>必须要有专门的DesignOps人员。</p><p></p><p>不存在一个能够处理所有流程和任务的设计师，因为到最后，他们只能做实际的设计工作。数字产品设计是一项多层次的工作，需要在特定领域中有不同经验的单位。正如需要将用户体验（UX）和用户界面（UI）设计分离并由两个不同的专家分别处理一样，也需要一个专门的DesignOps人员。</p><p>&nbsp;</p><p>他或她的职责是：</p><p>预测并管理工作和资源帮助招聘具有合适技能的设计师沟通设计标准分配适当的DesignOps工具制定敏捷的工作分解结构，以优化设计师的日常工作流程识别既定流程和改进策略之间的关键差距帮助建立更具生产力的设计文化</p><p>&nbsp;</p><p>DesignOps专家的角色取决于特定公司的需求、优劣势，以及团队当前正在开发的数字产品/服务。</p><p>&nbsp;</p><p>使用并掌握数据来衡量设计标准，区分优劣实践和设计，并建立与最终用户的最佳共鸣。</p><p></p><p>没有什么比利用可操作的数据来构思和设计下一个数字产品有更大的价值优化和质量提升了。从这个意义上说，将数据分析作为DesignOps的一部分可以激发更好的设计和更好的用户体验，甚至可以帮助你紧跟当前的设计趋势，比如数据可视化和数据故事讲述等。</p><p>&nbsp;</p><p>总而言之，DesignOps是一种实践和思维方式的结合，可以构建繁荣的文化、改进设计工作流程、促进多部门流程（如设计师与开发人员的交接）、改进产品和服务的制作方式，并使项目能够以更快的速度发展。但要实现所以这一切并建立运作良好的DesignOps，你必须通过数据进行创新和设计，促进深度沟通和创造性协作，通过采用DesignOps工具适应数字化转型，了解你的公司及其业务实践，研究并探索最终用户，并与专门的DesignOps人员合作。</p><p>&nbsp;</p><p>作者介绍：&nbsp;</p><p>创新专家Jason Beres是全球领先的企业软件制造商Infragistics的开发工具高级副总裁。Jason曾为多家酒店撰写技术文章，在全国性会议上发表演讲，并撰写/合著了10本关于软件/开发的书籍。他在开发方面的专长进一步延伸，以确保数据和分析在现代网络和移动平台上能以创新和客户驱动的方式展示。Jason是软件测试过程、数据驱动团队、客户对产品设计、开源以及过去30年数据分析和商业智能变化等技术问题方面的专家。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/designops-improve-process/\">https://www.infoq.com/articles/designops-improve-process/</a>\"</p>",
    "publish_time": "2022-08-11 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "快手发布B端业务品牌StreamLake，专注视频化升级全链路解决方案",
    "url": "https://www.infoq.cn/article/ZHcVAsKIRY3yrjk8Z86j",
    "summary": "<p>8月10日，短视频与直播平台快手在北京召开发布会，发布视频云品牌StreamLake，正式进军B端业务领域。</p><p></p><p>快手首席技术官陈定佳，快手高级副总裁、StreamLake业务负责人于冰，快手技术副总裁、AI技术及主站技术负责人王仲远出席发布会，知乎合伙人兼CTO李大海、联通在线信息科技有限公司副总经理秦吉波、NVIDIA英伟达亚太区开发与技术总经理李曦鹏等合作伙伴代表和专家分享了视频化浪潮的趋势与需求。</p><p></p><p>当前，视频化升级正在席卷众多行业，激发出庞大的视频云需求。快手StreamLake专注于成为视频化升级助推器，将沉淀多年的音视频和AI等关键技术能力以产品化的形式对外开放，提供一站式音视频+AI解决方案。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/60/53/603138cab083979163b499b54154f653.jpg\" /></p><p>图：StreamLake品牌发布会启动仪式</p><p></p><p></p><h2>Streamlake要把“重工业”变成“轻工业”</h2><p></p><p></p><p>视频正在成为各行业连接客户最广泛的载体。来自Ericsson Mobility最新报告显示，未来六年移动数据流量增长4.38倍，其中视频流量占比将达到79%。</p><p></p><p>快手首席技术官陈定佳表示，对于下一代互联网的形态，目前还存在比较大的不确定性，但无论承载介质是什么，底层都离不开音视频和AI技术。</p><p></p><p>“我们经常思考，快手过去得到的经验，打磨过的技术，沉淀出的解决方案，能否为更多企业所用？最终，我们以内容汇聚与分发方面的大规模实践经验，将多年积累的技术能力，沉淀为一套可复用的产品体系与服务。”陈定佳表示。</p><p></p><p>在快手高级副总裁、StreamLake负责人于冰眼中，随着5G广泛部署，未来几年视频将像空气和水一样无处不在，但视频化升级面临体验痛点多，带宽成本高，创新和规模化效率低等难题，是名副其实的“重工业”。</p><p></p><p>“StreamLake要把‘重工业’变成‘轻工业’，把快手服务大规模用户所积累的基础设施、AI、音视频、算法等核心能力，通过模块化、标准化方式开放给其他企业，形成全链路服务。”</p><p></p><p>快手技术副总裁、AI技术及主站技术负责人王仲远表示，StreamLake将帮助客户加速业务创新，提升用户体验，支撑业务增长，同时降低运营成本。</p><p></p><h2>StreamLake：视频化转型操作系统</h2><p></p><p></p><p>随着网络和屏幕的迭代，用户对视频清晰度需求持续升级。卡顿率的上升、机型适配的难度增加、成本的大幅提升等成为普遍挑战。</p><p></p><p>于冰表示，解决这些挑战要从音视频的第一性原理出发思考，即更清晰、更流畅和更低成本，这三个问题相互影响，相互制约，但快手经过多年的实践，总结出一套成熟的方法论。</p><p></p><p>首先，视频压缩和增强技术能够同时兼顾三个方向，可做到同等清晰度下把视频文件压缩到更小，从而实现流畅播放，带宽占用更低，又实现在同等带宽用量条件下，显著提升视频清晰度。快手自研的KVC编解码标准，相比最新一代国际视频编码标准H.266/VVC功耗高、难以普及的困境，能将解码功耗优化到与上一代H.265/HEVC标准基本一致，同时比行业平均水平节约40%～50%带宽。</p><p></p><p>其次，为了平衡清晰和流畅的矛盾，快手提出基于大数据的QoE（Quality of Experience）优化方案，通过大规模AB实验来验证每个优化手段的效果。其中最有效手段之一是多码率自适应算法，可在网络速度和清晰度之间做最优平衡，上线后用户卡顿率大幅下降49%。这些能力都会包含在StreamLake产品中，为行业客户提供最佳实践。</p><p></p><p>最后，成本优化方面，快手提出了基于ROI（投资回报比）的视频生命周期管理方案，运用精准视频热度预测模型来提前触发视频压缩任务，从而最大化节约成本。</p><p></p><p>于冰表示，基于快手深厚的底层技术积累和沉淀，StreamLake形成了编解码算法、传输算法、AI特效、知识图谱等丰富的原子能力，以及行业领先的下一代产品矩阵，点播云、直播云、媒体处理与存储、虚拟人、内容审核、特效美化、内容理解、互动与创作、快手云剪、快手极目、音视频SDK、SoundMatrix智能会议音箱、6DoF自由视角视频等产品，辅以专业技术咨询服务，为行业客户提供场景化的解决方案。</p><p></p><p>“把所有技术细节封装成一个用于全行业视频化转型的操作系统，给用户提供交互简洁、容易接入的交付界面，又能够灵活的针对各行各业做到深度定制和优化，解决各行各业特定领域的问题。”于冰表示，StreamLake在商业模式设计上希望与合作伙伴保持同向，优化视频体验，同时让成本可控，实现业务可持续发展与客户共赢。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/fd/d5/fd4f6cfb0fab150b6a9ac506f03fedd5.png\" /></p><p>图：StreamLake OS全景图</p><p>&nbsp;</p><p></p><h2>视频AI，为业务赋能</h2><p></p><p></p><p>StreamLake的技术能力与模块中，视频AI是重要组成部分。快手技术副总裁、AI技术及主站技术负责人王仲远分享了智能视频创作、智能视频理解、数字人及XR等技术给客户带来的价值。</p><p></p><p>王仲远介绍，快手上每天有海量的视频内容诞生，每个视频在平台上需要经历内容创作、理解、分发三个环节，在帮助创作者更加便捷、智能化地进行视频内容创作，以及结构化管理海量视频内容上AI发挥着不可替代的作用，快手日均AI服务调用达到了4500亿次，总共涵盖了全球超过10万种的机型。</p><p></p><p>基于快手海量视频汇聚及分发过程沉淀的AI技术能力，面向行业合作伙伴，王仲远正式发布了视频AI的三大类解决方案：智能视频创作，智能视频理解，数字人及XR，以助力各行各业客户实现视频化、智能化升级。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e8/73/e843e1459dc45da19088157de47ce873.png\" /></p><p>图：StreamLake视频AI全景图</p><p></p><p>在智能视频创作解决方案中，包含了内容创作环节的人像美化、智能特效、图集成片、文案成片等能力，大幅降低视频创作的门槛，提升视频创作的品质；智能视频理解解决方案，依托快手多年沉淀的多模态技术，实现海量视频的结构化管理，像管理图书一样管理短视频，应用于内容生态分析、基于兴趣的个性化推荐、智能审核、原创保护等方向。</p><p></p><p>发布会现场，王仲远首次发布了“数字人及XR解决方案”，包括传播型数字人、服务型数字人、虚拟化身数字人，快手在三个方向均有布局和成功案例。目前，快手也已与联通在线达成战略合作，双方将共同研发数字人能力开放平台。</p><p></p><h2>坚持技术开放，专注视频云和解决方案</h2><p></p><p></p><p>据了解，快手StreamLake业务2020年开始酝酿，2021年起步探索，并在今年成为独立业务，StreamLake品牌发布成为快手技术toB业务的全新起点。</p><p></p><p>基于自身大规模C端业务的锤炼和极致优化经验，以及合作伙伴的成功合作验证，快手首席技术官陈定佳对StreamLake技术toB业务充满信心。“技术toB是长期坚定的方向，我们愿意和合作伙伴一起携手，赋能用视频技术探索商业世界的每一步，实现共生双赢。”</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b2/8d/b293fe546532a0591bff3faaa8840f8d.png\" /></p><p>图：快手音视频核心技术积累</p><p></p><p>据介绍，过去一年，快手已携手知乎、中国联通、央视频、小米等众多合作伙伴，在视频化、智能化领域展开探索。</p><p></p><p>陈定佳表示，视频化正在赋能千行百业，StreamLake将在创新、体验、规模、成本四个方面为客户提供价值，快手自身发展中所经历的快上线、提品质、扛规模、控成本四个阶段所积累的底层基础设施和技术团队将提供有力支撑。</p><p></p><p>此外，NVIDIA将与StreamLake共建生态，携手致力向客户和伙伴分享最新的硬件和生态发展，向更多的开发者、初创公司和合作伙伴提供服务与支持。</p>",
    "publish_time": "2022-08-11 10:02:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "将 50 万个文件放在一个Git存储库中，我们做了这些",
    "url": "https://www.infoq.cn/article/toMhTgpmuY4oqHpVf0w1",
    "summary": "<p></p><p>本文最初发布于Canva工程博客。</p><p>&nbsp;</p><p>在Canva，我们有意采用了monorepo模式，它有好处也有坏处。自2012年第一次提交以来，库的规模和流量都与产品一起迅速增长。在过去的10年里，代码库已经从几千行增长到2022年的将近6000万行。数百名工程师在50万个文件中工作，每周会产生近百万行的修改（包括生成的文件），数万个提交，数千个拉动请求的合并。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bcc4cdbb7d1bcd11b8c558fc3bc9c117.png\" /></p><p></p><p>从2012年到2022年10年间代码量的增长</p><p>&nbsp;</p><p>这种增长显著降低了本地Git的性能，因为像git status这样的命令运行时间越来越长。在新克隆的一个monorepo上，git status平均需要10秒，而git fetch可能需要15秒到1分钟，这取决于工程师合并了多少变更。由于Git在工程师的工作流程中非常重要，每天多次运行这些命令会缩短工程师每天的总生产时间。而且，时间太长的话，工程师可能会进行上下文切换，这进一步降低了他们的生产率。</p><p></p><h1>现有解决方案</h1><p></p><p></p><p>所幸，并不是只有我们在使用monorepo时遇到了这个问题。<a href=\"https://monorepo-book.github.io/\">Github</a>\"、<a href=\"https://dropbox.tech/application/speeding-up-a-git-monorepo-at-dropbox-with--200-lines-of-code\">Dropbox</a>\"之前的工作以及Git近期的改进提供了大量可以用来提高Git性能的资源。多年来，多位工程师发表了数篇关于如何“让Git更快”的内部博文。这些工作最终形成了一个自动化脚本，建议工程师们将这个脚本作为入职培训流程的一部分。</p><p>&nbsp;</p><p>首先，为了减少Git查找变更的开销，我们在Watchman中使用了<a href=\"https://git-scm.com/docs/git-update-index#_file_system_monitor\">fsmonitor</a>\"钩子，这样就能在变更发生时捕获它们，而不是在每次运行命令时扫描存储库中的所有文件。我们还启用了<a href=\"https://git-scm.com/docs/git-config#Documentation/git-config.txt-featuremanyFiles\">feature.manyFiles</a>\"，而它在后台启用了<a href=\"https://git-scm.com/docs/git-update-index#_untracked_cache\">untracked cache</a>\"，可以跳过没有变化的目录和文件。</p><p>&nbsp;</p><p>Git还内置了一个命令（<a href=\"https://git-scm.com/docs/git-maintenance\">maintenance</a>\"），用来优化存储库的数据，加速其他命令的执行并减少磁盘占用。它默认是不启用的，所以我们将它注册为每日调度和每小时调度。</p><p>&nbsp;</p><p></p><h1>我们的monorepo有什么独特之处？</h1><p></p><p>&nbsp;</p><p>深入研究monorepo中的文件类型，我们发现.xlf文件几乎占文件总数的70%。这些<a href=\"https://canvatechblog.com/how-to-design-in-every-language-at-once-f2dd66a2780f#bdd4\">.xlf文件</a>\"包含针对每种语言环境翻译好的字符串。即使它们位于一个monorepo中，工程师也从来不会手工编辑，因为它们是在字符串翻译时自动生成的。也就是说，Git在花费资源跟踪工程师永远不会修改的文件。然而，我们不能直接删除或忽略这些文件，因为翻译系统有它们才能顺利运行。次优解决方案是告诉Git不要使用稀疏签出（<a href=\"https://git-scm.com/docs/git-sparse-checkout\">sparse checkout</a>\"）在工作树中填充它们，局部性地忽略它们，同时仍然通过monorepo的其他源头跟踪它们的变化。实际上，为了使构建系统的其他部分可以正常工作，我们必须包含用于en-AU语言环境的字符串，但这点差异可以忽略不计。尽管忽略了这些翻译，但我们的本地环境仍然默认使用<a href=\"https://canvatechblog.com/how-to-design-in-every-language-at-once-f2dd66a2780f#a626\">en-psaccent</a>\"伪本地化语言环境运行，因此，工程师仍然要注意本地化需求。</p><p>&nbsp;</p><p>我们用一个单独克隆的monorepo做了以下实验，使用Git的默认设置，没做前面提到的优化。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/70/708282b4f69c9f05efdb6b4339b31544.png\" /></p><p></p><p>对于稀疏签出功能的使用，我们可以更进一步。通常，我们的工程师团队都是小团队，只会用到monorepo的一个更小的子集。例如，处理编辑器的前端工程师不太可能修改与搜索或计费服务相关的代码。如果一名工程师能告诉我们他们通常在做什么，我们就可以设计一个包含所有必需依赖项的签出模式，让他们可以在本地运行和测试代码，同时保持签出尽可能小。</p><p>&nbsp;</p><p>Canva的monorepo使用<a href=\"https://bazel.build/\">Bazel</a>\"作为构建系统，我们在其中定义构建目标（主页、搜索服务器、导出按钮等）和它们的依赖项。也就是说，给Bazel定一个目标，我们就可以准确地确定哪些文件需要构建、运行和测试，并通过查询Bazel依赖关系图排除其他文件。在实践中，这需要更多的启发式逻辑，因为不是所有的东西都连接到了Bazel构建图中，需要解析符号链接，但这都是些可以解决的边缘情况。下表展示了一些示例配置以及它们对签出的影响。</p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6c2f8e6388c50690f889d5c8d2ad8fc.png\" /></p><p></p><p>后端服务往往模块化程度更高，可以独立运行，前端页面则不那么独立。原因有很多，如TypeScript配置，代码分析工具的设置方式，以及在编写工具和网页时没有考虑稀疏签出，而是假设所有文件任何时候都存在。这些针对翻译文件的配置提供了每名工程师都可以使用的增量改进。</p><p>&nbsp;</p><p>稀疏签出也有缺点。首先，现在有跟踪文件没有物理地填充到磁盘上，所以不能搜索或交互。意外变更或错误的合并冲突可能会使这些文件处于错误的状态（例如，如果两个pull请求修改了同一个要翻译的字符串并产生冲突，现在就必须手动解决）。工程师在工作过程中必须注意这种情况。</p><p>&nbsp;</p><p>每个Git签出命令都有开销用于检查是否应该填充或忽略更新的文件。虽然对于忽略.xlf文件这样的简单模式来说，这种开销很小，但对于更复杂的模式，这种开销就变得非常明显了。</p><p></p><h1>小结</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/55/55b1ccbe2af305af09d17998912bbe4a.png\" /></p><p></p><p>该脚本根据推荐的Git配置设置Canva monorepo。</p><p>&nbsp;</p><p>结合现有的monorepo Git配置最佳实践，我们通过缓存将git status的时间缩短到3到2秒。虽然对于工程师来说，这比什么都不做要好得多，但随着monorepo越来越大，工程师越来越多，为了缩小这一数值，我们仍然有很长的路要走。</p><p>&nbsp;</p><p></p><h1>进行中的工作</h1><p></p><p>&nbsp;</p><p>这些优化为配置大型存储库设定了一条基线。但是，为了量化Git性能对工程师生产力的影响，并进一步分析问题和优化，我们需要能够收集Git操作的遥测数据。</p><p>&nbsp;</p><p>Git通过<a href=\"https://git-scm.com/docs/api-trace2/\">trace2</a>\"格式输出跟踪信息，我们可以捕获这些信息并将其发送到中央存储进行分析。为此，我们团队一直在开发一个名为Olly的内部工具，用于捕获操作时间、失败及其原因，跟踪新分支的创建，等等。这有助于我们在整个组织中监控Git的性能，将来可以在Git配置发生变化时通知我们，这是量化工程师开发周期的第一步。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24f5fb59c022c09df505155c114b4f8b.png\" /></p><p></p><p>常见Git操作的平均时长</p><p>&nbsp;</p><p>有了这些遥测信息，我们现在可以具体地量化Git性能对工程师的影响，关注痛点（上图中的git fetch），并进一步深入分析每个操作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7e/7e73390ddb57a0c307d3d7bc0a25d683.png\" /></p><p></p><p>git fetch跟踪信息</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/ad2261f7d9dc2b3a9d9d4a4c7d796777.png\" /></p><p></p><p>深入研究index-pack操作，我们可以得到正在传输的包、对象和字节的数量。</p><p>&nbsp;</p><p>此外，鉴于Git<a href=\"https://github.blog/2022-04-12-git-security-vulnerability-announced/\">最近暴出的漏洞</a>\"，我们正致力于向所有工程师提供一个明确的Git版本，默认提供正确的配置，确保每个人都能获得最新的安全补丁和性能改进。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://canvatechblog.com/we-put-half-a-million-files-in-one-git-repository-heres-what-we-learned-ec734a764181\">We Put Half a Million files in One git Repository, Here’s What We Learned</a>\"</p>",
    "publish_time": "2022-08-11 11:49:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache DolphinScheduler 正式发布3.0.0 版本",
    "url": "https://www.infoq.cn/article/GzIesGahvDH2WFSfbgeA",
    "summary": "<p>2022 年 8 月 10 日，在经过 3.0.0 alpha、3.0.0-beta-1、3.0.0-beta-2 不断验证之后，Apache DolphinScheduler 终于正式发布第三个大版本。3.0.0 正式版本发生了自发版以来的最大幅度变动，新增了众多全新功能和特性。</p><p></p><p>经过迭代的 3.0.0 正式版与此前 3.0.0 alpha 版本更新文中所描述的主要功能和特性更新、优化项和 Bug 修复大致一致，包括“更快、更强、更现代化、更易维护”这四个关键词总结。</p><p></p><p>更快：重构了 UI 界面，新 UI 不仅用户响应速度提高数十倍，开发者构建速度提高数百倍；更强：带来了许多振奋人心的新功能，如数据质量保证、自定义时区、新增多个任务支持和多个告警插件；更现代化：新 UI 除了更快外，大到页面布局，细到图标样式都更加现代化；更易维护：后端服务拆分更加符合容器化和微服务化的发展趋势，还能明确各个服务的职责，让维护更加简单。</p><p></p><p></p><h3>新功能和新特性包括：</h3><p></p><p></p><h4>全新UI</h4><p></p><p></p><p>3.0.0  最大的变化是引入了新的 UI，切换语言页面无需重新加载，并且新增了深色主题。新 UI 使用了 Vue3，TSX，Vite 相关技术栈。对比旧版 UI，新 UI 不仅更加现代化，操作也更加人性化，前端的鲁棒性也更强，使用户在编译时一旦发现代码中的问题，可以对接口参数进行校验，从而使前端代码更加健壮。</p><p></p><p>此外，新架构和新技术栈不仅能让用户在操作 Apache DolphinScheduler 时响应速度有数十倍的提升，同时开发者本地编译和启动 UI 的速度有了数百倍的提升，这将大大缩短开发者调试和打包代码所需的时间。</p><p></p><p></p><h4>AWS支持</h4><p></p><p></p><p>随着 Apache DolphinScheduler 用户群体越来越丰富，吸引了很多海外用户。但在海外业务场景下，用户在调研过程中发现有两个影响用户便捷体验 Apache DolphinScheduler 的点，一个是时区问题，另一个则是对海外云厂商，尤其是对 AWS 的支持不足。此版本中，我们决定对 AWS 较为重要的组件进行支持，目前已经涵盖 Amazon EMR 和 Amazon Redshift 两个 AWS 的任务类型，以及实现了资源中心支持 Amazon S3 存储。</p><p></p><p>针对 Amazon EMR，Apache DolphinScheduler 创建了一个新的任务类型，并提供了其 Run Job Flow 的功能，允许用户向 Amazon EMR 提交多个 steps 作业，并指定使用的资源数量。详情可见：https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/guide/task/emr.html</p><p></p><p>对于 Amazon Redshift，Apache DolphinScheduler 目前在 SQL 任务类型中扩展了对 Amazon Redshift 数据源的支持，现在用户可以在 SQL 任务中选择 Redshift 数据源来运行 Amazon Redshift 任务。</p><p></p><p>对于 Amazon S3，Apache DolphinScheduler 扩展了 Apache DolphinScheduler 的资源中心，使其不仅能支持本地资源、HDFS 资源存储，同时支持 Amazon S3 作为资源中心的储存。详情可见：https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/guide/resource.html 中的 `resource.storage.type`</p><p></p><h4>服务拆分</h4><p></p><p>全新的 UI 是 3.0.0 &nbsp;前端的最大变化，而后端最大的变化就是对服务进行拆分。考虑到容器和微服务的概念越来越火热，Apache DolphinScheduler 开发者做出了重大决定：对后端服务进行拆分。按照职能，Apache DolphinScheduler 将服务拆分成了以下几部分：</p><p>master-server: master服务worker-server: worker服务api-server: API服务alert-server: 告警服务standalone-server: standalone用于快速体验 dolphinscheduler 功能ui: UI资源bin: 快速启动脚本，主要是启动各个服务的脚本tools: 工具相关脚本，主要包含数据库创建，更新脚本</p><p></p><p>所有的服务都可以通过</p><p></p><p><code lang=\"null\"> `bin/dolphinscheduler-daemon.sh` </code></p><p>的方式进行启动或者停止。</p><p></p><p></p><h4>数据质量保证</h4><p></p><p></p><p>此版本中，用户们从 2.0.0 开始就期待已久的数据质量保证应用功能上线，解决了从源头同步的数据条数准确性，单表或多表周均、月均波动超过阈值告警等数据质量问题。Apache DolphinScheduler 此前版本解决了将任务以特定顺序和时间运行的问题，但数据运行完之后对数据的质量一直没有较为通用的衡量标准，用户需要付出额外的开发成本。</p><p></p><p>现在，3.0.0 &nbsp;已经实现了数据质量原生支持，用户可以直接通过配置的方式，轻松实现数据质量监控，在保证工作流运行的前提下，保证运行结果的准确性。</p><p></p><p></p><h4>任务组</h4><p></p><p></p><p>任务组主要用于控制任务实例并发并明确组内优先级。用户在新建任务定义时，可配置当前任务对应的任务组，并配置任务在任务组内运行的优先级。当任务配置了任务组后，任务的执行除了要满足上游任务全部成功外，还需要满足当前任务组正在运行的任务小于资源池的大小。当大于或者等于资源池大小时，任务会进入等待状态等待下一次检查。当任务组中多个任务同时进到待运行队列中时，会先运行优先级高的任务。</p><p></p><p>详见 链接：https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/user_doc/guide/resource.html</p><p></p><p></p><h4>自定义时区</h4><p></p><p></p><p>在 3.0.0 之前版本，Apache DolphinScheduler 默认的时间是 UTC+8 时区，但随着用户群体扩大，海外用户和在海外开展跨时区业务的用户在使用中经常被时区所困扰。3.0.0 &nbsp;支持时区切换后，失去问题迎刃而解，满足海外用户和出海业务伙伴的需求。例如，如当企业业务涉及的时区包含东八区和西五区，想要使用同一个 DolphinScheduler 集群时，可以分别创建多个用户，每个用户使用自己当地时区，对应 DolphinScheduler 对象显示的时间均会切换为对应时区的当地时间，更加符合当地开发者的使用习惯。</p><p></p><p>详见 链接：https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/user_doc/guide/howto/general-setting.html</p><p></p><p></p><h4>任务定义列表</h4><p></p><p></p><p>使用 Apache DolphinScheduler 3.0.0 &nbsp;此前版本，用户如果想要操作任务，需要先找到对应的工作流，并在工作流中定位到任务的位置之后才能编辑。然而，当工作流数量变多或单个工作流有较多的任务时，找到对应任务的过程将会变得非常痛苦，这不是 Apache DolphinScheduler 所追求的 easy to use 理念。所以，Apache DolphinScheduler 在 3.0.0 &nbsp;中增加了任务定义页面，让用户可以通过任务名称快速定位到任务，并对任务进行操作，轻松实现批量任务变更。</p><p></p><p>详见 链接：https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/user_doc/guide/project/task-instance.html</p><p></p><p></p><h4>新告警类型支持</h4><p></p><p></p><p>在 3.0.0 &nbsp;中，告警类型也进行了扩展，增加了对 Telegram、Webexteams 告警类型的支持。</p><p></p><p></p><h4>Python API&nbsp;新功能</h4><p></p><p></p><p>3.0.0 中，Python API 最大的变化是将对应的 PythonGatewayServer 集成到了 API-Server 服务, 并将其重命名 PythonGatewayService, 现在用户在启动 api-server 时会默认启动 PythonGatewayService；如果不想要启动 PythonGatewayService，可以将 application.yaml 中的 python-gateway.enabled 设置成 false。</p><p></p><p>此外, Python API 还增加了 CLI 和 configuration 模块。Configuration 模块允许用户修改 Python API 默认的配置, 如修改工作流默认的用户名、worker 分组等内容, 可以通过环境变量、直接修改文件、Python 动态修改来改变值。</p><p></p><p>目前 CLI 只有 version 和 config 两个子命令, 用于确认当前版本以及增删配置文件。后续，将引入更加多功能，方便用户通过命令行操作 DolphinScheduler。值得注意的是，Python API 还支持新增和上传资源中心文件功能，方便资源管理；支持同一个 project 不同 workflow 写入不同名称；增加集成测试，让测试更加便捷。&nbsp;</p><p></p><p></p><h3>此前版本未公布的功能和特性更新</h3><p></p><p></p><p></p><h4>支持 Flink 任务类型</h4><p></p><p></p><p>在该版本中，Apache DolphinScheduler扩展了 Flink 任务类型，使其支持运行 Flink SQL 任务，其使用 sql-client.sh 提交任务。在此前的版本中, 我们仅支持通过 flink cli 的方式提交任务, 这种方式需要结合资源中心, 将资源文件提交到资源中心, 然后在任务定义页面引用改资源, 对于版本化和用户透明都不是十分友好. 随着 flink sql 逐渐成为 flink 使用者的主流, 加之直接在编辑页面写 sql 更加用户透明, 我们采纳了向社区贡献的 flink sql 功能. 3.0.0 以后的版本用户可以更加方便的使用 flink 任务了。</p><p></p><p>更多详情查看：<a href=\"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sqlclient/\">flink sql client</a>\"</p><p></p><p>对应 PR：https://github.com/apache/dolphinscheduler/pull/9840</p><p></p><p></p><h4>新增 Zepplin 任务类型</h4><p></p><p></p><p>在该版本中，增加了 Zeppelin 任务类型，用于创建并执行 Zeppelin 类型任务。Worker 执行该任务时，会通过 Zeppelin Cient API 触发 Zeppelin Notebook 段落。</p><p></p><p>对应PR：https://github.com/apache/dolphinscheduler/pull/9810</p><p></p><p></p><h4>Bash 传参功能</h4><p></p><p></p><p>新版本还新增了通过 bash 传参的功能，如果你想在下游任务中使用 bash 变量而不是常量值 export 参数，你可以在通过 setValue 和 Bash 变量实现，它更加灵活，可以让你动态地获取现有的本地或 HTTP 资源获取设定变量。</p><p></p><p>可以使用类似的语法</p><p><code lang=\"null\">lines_num=$(wget https://raw.githubusercontent.com/apache/dolphinscheduler/dev/README.md -q -O - | wc -l | xargs)echo \"#{setValue(set_val_var=${lines_num})}\"</code></p><p></p><p></p><h4>允许用户上传没有后缀的文件</h4><p></p><p></p><p>之前资源中心只能上传有后缀的文件，3.0.0&nbsp;版本支持用户上传没有后缀的文件。</p><p></p><p></p><h4>其他功能增强</h4><p></p><p></p><p>除了上述功能新增外，3.0.0 版本还进行了很多细节功能增强，如重构任务插件、数据源插件模块，让扩展更简单；恢复了对 Spark SQL 的支持；E2E 测试已经完美兼容新 UI 等。</p><p></p><p></p><h3>主要优化项</h3><p></p><p>任务后端插件优化，新插件只需要修改插件自带的模块在工作流下提交/创建 cron 时验证结束时间和开始时间Dependent 添加依赖时可以选择全局项目AlertSender 优化及关闭优化，如 MasterServer增加 slot 条件查询数据库, 减少返回数据记录通过将 python gatewar 迁移到 apiserver 来精简 dist 包[python] 将 pythonGatewayServer 迁移到 api 服务器[python] 添加缺失的配置和连接远程服务器文档[Master/Worker] 将任务 ack 更改为运行回调[Master] 添加任务事件线程池</p><p></p><p></p><h3>Release note</h3><p></p><p></p><p>GitHub: https://github.com/apache/dolphinscheduler/releases/tag/3.0.0</p><p></p><p>下载：https://dolphinscheduler.apache.org/en-us/download/download.html</p>",
    "publish_time": "2022-08-11 12:10:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "微软进一步削减消费研发部门",
    "url": "https://www.infoq.cn/article/iR3te8f1Jl35SGKO6NVY",
    "summary": "<p></p><p>根据<a href=\"https://techcrunch.com/2022/08/09/microsoft-makes-further-cuts-focused-on-consumer-rd-group/\">外媒</a>\"消息，在 7 月份取消Office 和 Windows 部门的空缺职位并<a href=\"https://techcrunch.com/2022/07/12/microsoft-lays-off-a-portion-of-its-workforce-as-part-of-a-realignment/\">解雇</a>\"了部分员工后，微软本周进一步裁员。这一轮裁员集中在其现代生活体验 (MLX) 集团，该集团是该公司负责以客户为中心的项目研发集团之一。</p><p>&nbsp;</p><p>据悉，该部门员工需要在60天内在公司内部找到其他职位，否则将被解雇。有员工爆料称，公司将为这些员工提供遣散费。具体标准为，工作时间每满6个月就可以获得相当于一周薪资的遣散费，并且这些员工还将在9月获得年度奖金。这名员工估计，该团队的规模大约为200人。根据 LinkedIn 上的帖子，最近的裁员也影响了包括芝加哥在内的多个地点的合同招聘人员。</p><p>&nbsp;</p><p>微软发言人拒绝提供细节，但没有否认裁员已经发生。</p><p>&nbsp;</p><p>微软的 MLX 集团遍布温哥华和旧金山等城市。MLX团队最初作为初创企业Mobile Data Labs而存在，开发了备受好评的里程跟踪工具MileIQ。微软在2015年将该公司收购，该团队首先以MileIQ的名义重新命名，然后被赋予新的名称 Modern Life Experiences。该团队的职责范围也逐步扩大，与微软的家庭安全小组合作，为 iOS 和 Android构建了<a href=\"https://www.microsoft.com/en-us/microsoft-365/family-safety\">家庭安全应用程序的第一个版本</a>\"，还开发了个人财务工具Money in Excel。</p><p>&nbsp;</p><p>在2020年2月的一次高管洗牌中，Windows产品和教育部门首席副总裁埃兰·米吉多(Eran Megiddo)同时担任了MLX和微软教育部门的负责人。前移动数据实验室CTO Max Wheeler在收购后加入微软，现在仍然是工程总监。</p><p>&nbsp;</p><p>2020 年 6 月，MLX 集团推出了个人财务工具Money in Excel，该工具可以自动将银行、信用卡、投资和贷款账户自动连接到 Excel 上，并对这些数据进行分析。根据<a href=\"https://www.howtogeek.com/808679/microsoft-is-shutting-down-money-in-excel-and-more/\">外媒报道</a>\"，Money in Excel 计划于 2023 年 6 月 30 日关闭。此外，该团队还负责为 Windows、Xbox、移动设备和网络上的家庭孵化“创新的新客户体验”。</p><p>&nbsp;</p><p>在7月份，微软就被爆裁撤 180,000 名中不到 1% 比例的员工，该行为是为了适应业务调整需要。不过，微软表示，将继续投资业务，并在未来一年增加员工总数。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://techcrunch.com/2022/08/09/microsoft-makes-further-cuts-focused-on-consumer-rd-group/\">https://techcrunch.com/2022/08/09/microsoft-makes-further-cuts-focused-on-consumer-rd-group/</a>\"</p>",
    "publish_time": "2022-08-11 12:26:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌大张旗鼓向苹果施压：赶紧支持 RCS 标准",
    "url": "https://www.infoq.cn/article/B0KNtrg7ij8b46uk254p",
    "summary": "<p></p><blockquote>“蓝绿气泡之争”终于要画上句号了吗？</blockquote><p></p><p></p><h2>谷歌鼓励苹果接受 RCS</h2><p></p><p></p><p>据<a href=\"https://www.theverge.com/2022/8/9/23297951/google-apple-rcs-adoption-campaign-getthemessage-blue-green-messages\">外媒报道</a>\"，谷歌 Android 部门周二启动了一项新的宣传活动，专门上线了“Get The Message”网站，鼓励苹果采用新一代跨平台消息传递协议 RCS，以便在跨 iOS / Android 设备的消息传递服务上，实现 iMessage 可提供的大部分功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/69bfbd96a100dd293876f3d81c715bba.png\" /></p><p></p><p>RCS（Rich Communication Suite）是融合通信，指的是通信技术和信息技术的融合，是从2008年开始由GSMA（全球移动通信系统协会）主导研发的次世代手机消息技术。RCS 的诞生，正是为了取代老旧的短信（SMS）与彩信 (MMS) 标准。目前，RCS 协议已经得到了众多厂商的拥护。</p><p></p><p>谷歌也在 Get The Message 网站上，围绕 iOS 与 Android 设备该如何顺畅交换消息发布了不少有力论据，极力劝说苹果支持 RCS 新标准。熟悉互联网传播套路的谷歌还专门添加了#GetTheMessage 主题标签，打算在用户中掀起一波“逼宫”浪潮。</p><p></p><p>谷歌为何要如此大张旗鼓地向苹果施压，让其接受 RCS？这还得从今年年初的“<a href=\"https://it.sohu.com/a/575761746_121119002\">蓝绿气泡之争</a>\"”开始说起。</p><p></p><p>简单来说，当 Android 用户向 iOS 用户发送短信时，苹果会将短信降级到老式的短彩信（SMS / MMS），并且以绿色气泡进行显示。而苹果推出的免费通信服务 iMessage 则以蓝色气泡进行显示。</p><p></p><p>谷歌认为，苹果将短信降级到老式的短彩信（SMS / MMS），主要为纯文本时代设计。而它打破了人们已经开始依赖的许多现代信息功能，长期以来都有许多用户抱怨其带来糟糕的压缩视频画质、缺少已读回执，以及其他一些令人头痛的问题。</p><p></p><p>另一方面，虽然颜色本身是中性的，但却代表了两种截然不同的体验，让两类用户在短信交流时形成割裂之感。甚至有 Android 用户认为，iMessage 强行“割据一方”，消息收发就应该放之四海而皆准，建立起统一的行业生态。</p><p></p><p>为了解决这个问题，谷歌近几个月来一直在或明或暗地放消息，要求苹果支持 RCS。利用 RCS 协议，iOS 和 Android 之间足以实现 iMessage 内部的大多数（虽然还不是全部）功能。谷歌还表示，希望通过今年开发者大会和几个月来的推文宣传，让“每种移动操作系统都升级到 RCS”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bbf40096045ef4f5de016184bee1247.png\" /></p><p></p><h2>RCS 已在 Android 世界畅通无阻</h2><p></p><p></p><p>不得不承认的是，RCS 仍在发展中，甚至在过去一段时间，各家手机厂商对 RCS 的支持也相当迟缓、不成体系。</p><p></p><p>而 RCS 在发展过程中也面对诸多批评和质疑。RCS 国际特赦组织研究员 Joe Westby 批评 RCS 不允许端到端加密，因为它被视为运营商的服务，因此受到合法拦截。The Verge 批评美国对 RCS&nbsp; 的支持不一致，运营商没有在所有市场上支持 RCS，没有在所有手机上认证服务，或者还不支持 Universal Profile。Ars Technica 还批评了谷歌推出直接面向消费者的 RCS 服务的举动，认为这与 RCS 是运营商原生提供的功能让人想起消息应用程序相矛盾。</p><p></p><p>不过，RCS 也有诸多优势。​和传统的短信相比，RCS 可以发送高清图片、高清表情包，能进行语音甚至视频聊天，甚至还可以用卡片的形式提供类似“小程序”的交互界面。</p><p></p><p>早在 2015 年，谷歌方面就开始在 Android 阵营中力推 RCS 标准的普及。​自 2019 年谷歌亲自接手 RCS 标准以来，谷歌更是对 RCS 推崇备至。当前，RCS 基本在 Android 世界畅通无阻。今年，就连全球最大的 Android 手机厂商三星，也开始在其旗舰 Galaxy S22 系列中默认使用谷歌的 RCS 兼容消息应用。</p><p></p><p>目前，RCS 逐渐获得了与 iMessage 相同的加密功能。RCS 现在支持在一对一聊天中实现端到端加密（E2EE），群聊版 E2EE 也将在今年晚些时候推出。</p><p></p><p>但站在苹果视角来看，苹果还没有必须支持 RCS 的理由。无论是在功能、界面，还是使用方式上，苹果自身的 iMessage 都与 RCS 相似。​目前的割裂态势对苹果有好处，能帮助其保持对自家客户的锁定效应。iMessage 用户之间的无缝通信，和与 Android 用户间突兀的绿色气泡，时刻在营造“我们不一样”的氛围。正因为如此，苹果高管才在内部邮件中承认，将 iMessage 引入 Android “对苹果来说弊大于利”。</p><p></p><p>至于苹果是否会采用 RCS，按照苹果一贯的脾气来看，这种可能性着实不大。The verge 记者 Jon Porter 甚至评价称：“苹果采用 RCS， 感觉就像美国人集体放弃 iMessage 并转向 WhatsApp 或 Signal。”</p><p></p><p>不过，谷歌此番如此声势浩大地宣传 RCS，向苹果施压，接下来可以期待苹果方面的回击了。</p><p></p><p>而谷歌的 RCS 大梦，最终可能得靠中国移动运营商完成。</p><p></p><p>据<a href=\"https://baijiahao.baidu.com/s?id=1727276047793771245&amp;wfr=spider&amp;for=pc&amp;searchword=%E8%B0%B7%E6%AD%8CRCS%E6%A0%87%E5%87%86\">三益在线</a>\"报道，国内的移动运营商去年将国际标准、且本质上与 5G 没有任何关联性的 RCS，摇身一变包装成了“5G 信息”。不仅如此，根据网友在去年年底的测试结果表明，国内运营商为了让“5G 消息”名副其实，在技术层面限制了只有 5G 手机才能正常收发 RCS 信息，而 4G 手机收发 RCS 信息则会自动被转成短信或彩信。这个结果说明，运营商是直接将 RCS 技术与“推广 5G 手机”进行了强绑定。而追溯历史不难意识到，他们未来完全是有可能将“是否支持 RCS”，作为运营商渠道采购或补贴的硬性标准。</p>",
    "publish_time": "2022-08-11 14:13:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "NNAISENSE 正式发布开源进化算法库 EvoTorch",
    "url": "https://www.infoq.cn/article/pDBlPGFsDH9mx6uHrq3w",
    "summary": "<p></p><h2>首个开源进化算法库 EvoTorch</h2><p></p><p></p><p>据<a href=\"https://www.prnewswire.com/news-releases/nnaisense-launches-evotorch-the-worlds-most-advanced-evolutionary-algorithm-library-for-the-machine-learning-community-301602348.html\">外媒报道</a>\"，8 月 9 日，人工智能公司 NNAISENSE 宣布正式推出首个<a href=\"https://evotorch.ai/\">开源进化算法库 EvoTorch</a>\"，为行业提供进化算法。</p><p></p><p>在接受 VentureBeat 采访时，NNAISENSE 的联合创始人兼深度学习总监 Jonathan Masci 表示：“工业自动化的圣杯自古以来就是提高流程效率。” “进化计算基本上是一种强化学习的方式。”</p><p></p><p>据介绍，EvoTorch 建立在开源 PyTorch 机器学习库之上。NNAISENSE 的研究科学家 Timothy Atkinson 表示，EvoTorch 有几个组件，包括一系列进化算法和日志记录功能，因此数据科学家可以实时跟踪机器学习实验。“主要思想是，你可以使用 PyTorch 中构建的任何内容，并立即使用 EvoTorch 对其进行优化”。 </p><p></p><p>NNAISENSE 还将 EvoTorch 与用于扩展 Python 和 AI 应用程序的开源 Ray 框架集成。Atkinson 表示，如果数据科学家将问题构建为 PyTorch 函数以在 EvoTorch 上进行优化，则可以扩展到数千个 CPU 和数百个 GPU。</p><p></p><p>“我们在 Ray 库之上以一种非常明智的方式构建了 EvoTorch，这意味着它可以在您负担得起的范围内扩展，”Atkinson 说。</p><p></p><p>据悉，NNAISENSE 是“递归神经网络之父” Juergen Schmidhuber 的创业公司。Schmidhuber 在人工智能领域颇有声望，他曾在 1997 年与别人合著了一篇论文，为现代人工智能系统奠定了基础。文中介绍了一种基于人类大脑的记忆模型，帮助计算机根据语境和先前经验做出决定。这种观念支撑了大部分的现代人工智能网络。此后，他担任瑞士一家人工智能研究所的负责人，并培养除了许多顶尖专家。纽约时报曾称他“本应该是人工智能之父”。</p><p></p><p>目前，NNAISENSE 已经成了工业应用人工智能工程的领导者。公司的显著贡献之一是为奥迪创建了第一个基于 RL 的自主停车系统，并为 Festo 气动机器人“软”手训练控制器。NNAISENSE 的客户和投资者基础包括三星、奥迪、Trumpf GmbH + Co. KG、EOS GmbH 和 Schott AG 等。</p><p></p><h2>进化算法将制造更聪明的人工智能</h2><p></p><p></p><p>随着机器学习（尤其是深度学习模型）取得快速突破，人工智能的优势变得越来越明显，并且在许多垂直行业中发挥着重要作用，但人工智能的大规模实施仍然存在一些障碍。而进化算法正是重要的解决方案之一，它可以解决伴随自动化流程复杂性和规模增加而带来的级联挑战。</p><p></p><p>此外，大规模人工智能实施中最大的缺失部分是适应。建立在强大的机器学习模型上的人工智能系统应该能够适应不断变化的领域，因此需要更加依赖进化算法——模型可以根据需要转移和改变。这将有助于 AI 系统适应时代，并能够利用每个不同的环境，而无需花费时间和金钱来重新训练模型。</p><p></p><p>受生物进化的启发，研究人员使用了基于种群的进化技术，提出了超越人类设计的人工智能的设计。例如，在网站优化领域，人工智能支持的解决方案可以“进化”网站设计，并在类似于自然选择的过程中，结合性能最佳的元素和功能来生成最优化的网站以进行转换。</p><p></p><p>进化可以与神经网络配对，用网络权重的在线适应来代替缓慢而密集的反向传播训练过程。这意味着研究人员实时训练神经网络，而不是离线训练历史数据。对于许多在线问题，例如在线数字媒体的自动分段和超个性化，这种方法非常强大。强化学习系统在适应方面也取得了进展。强化学习是一种在线学习形式，它允许系统根据最直接的上下文来衡量其决策。</p><p></p><p>进化算法还有一大作用就是可以帮助建立一个持续学习的模型。在这方面，进化算法可以被认为类似于强化学习的概念。</p><p></p><p>Atkinson 表示，通过强化学习，算法在 AI 采取的行动空间中进行搜索，然后尝试使用在行动中发现的内容，以帮助在未来创造改进。进化算法会改变学习发生的位置，搜索单个网络，然后将更改传播到网络中。他还指出，进化算法不仅仅是学习，还可以用于工业优化和过程控制。“在强化学习和进化算法案例中，你都可以使用经过预训练的模型作为学习良好行为的基础，”Atkinson 说道。</p><p></p><p>另外，与标准的基于梯度的替代方案相比，进化算法不需要可微的成本函数，并且更适合现代硬件上的大规模并行化。这意味着，可以更高效地解决从学习机器人控制器到优化计划或产品设计的更广泛的问题。</p>",
    "publish_time": "2022-08-11 15:05:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据思维的关键，不只是要“善用数据”更要能“解决问题”",
    "url": "https://www.infoq.cn/article/YcvRvZaOoez9sKJ92jHR",
    "summary": "<p>写这篇文章的初衷，是因为最近和一些企业交流时，发现大家在<a href=\"https://www.infoq.cn/theme/144\">数字化转型</a>\"的过程中都或多或少遇到了同一个困惑——如何提升员工的“数据思维”，让每一个人都能理解数据的价值和规律，甚至都具备数据分析的能力。即便是对于金融这样走在数字化前排的行业来说，也在受类似问题的困扰。</p><p></p><p>和其它传统的实体行业不同的是，<a href=\"https://www.infoq.cn/article/1zd5MLr5iQOBzgAaGpOm\">金融</a>\"几乎就是一个基于数字的“游戏”。但是，拥有数据是一回事，能把数据价值释放出来又是另一回事。</p><p></p><p>不少金融企业表示，虽然行业整体在平台建设和数据整合方面取得了可观进展，然而对于如何提高数据的利用率，真正释放数据要素价值，还有很多问题亟待解决——比如，内部员工如果不具备数据思维，就不能在日常开展业务的过程中把数据的价值纳入考虑范围，即便企业坐拥海量数据，也可能形同虚设。</p><p></p><p>所以，InfoQ带着“如何提升企业员工的数据思维”这一问题，采访了曾在被奉为“数据驱动金融先驱”的Capital One（美国第一资本银行）任职多年的晋梅博士。</p><p></p><p>晋梅是中国科技大学数学系本科、美国乔治华盛顿大学统计学博士，毕业后就加入了Capital One；回国后，她先后在量化派、顶象技术、全量全速等公司任职，现在是<a href=\"https://www.infoq.cn/video/MGUUzAtk1mO69BXdKftY\">神州信息</a>\"资深金融科技专家。对于金融和互联网的业务经营、数据建模、运营分析和风险管理，她拥有着非常丰富的实战经验。</p><p></p><p>而针对我们的“迷思”，晋梅可以说是“一言惊醒梦中人”——她说——企业数字化做不好，大概率是因为，只看见了数据思维中的“数据”，但对“解决问题”鸵鸟式逃避。</p><p></p><h1>数据不是越多越好，模型不一定越复杂越好</h1><p></p><p></p><p>一提到“数据思维”，我们常常会听到这样的说法，一定要积累海量的数据、一定要用上复杂的算法和模型、一定要实现对业务的颠覆式创新。事实是这样吗？晋梅认为，数据思维的本质，不是追求数据的积累或是算法的复杂度，而是要清楚地知道，如何用好数据、更好地解决实际问题。</p><p></p><p>“比如说，有的企业的基础设施不够完善，业务线上化程度和数据积累也很有限。但他们在业务逻辑上想得很透、对业务流程的拆解很到位、对提升和增长所需要重点关注的环节抓得很准。在重要环节上，他们敢于提出问题和关键假设，然后有的放矢地去积累数据。他们及时复盘，基于分析结果一边优化、一边提出新的问题和假设——在这个过程中，即便他们只使用了几张不超过10M的Excel表格、只运用了小学生都能熟练掌握的四则运算，我们也不会认为这样的企业没有‘数据思维’、算不上‘<a href=\"https://xie.infoq.cn/article/450ece96e31f0b23ce976ec8c\">数据驱动</a>\"’。”晋梅强调，“能够踏实做到这一步的企业，就算是已经吃透‘数据思维’的内核了。”</p><p></p><p>她举了个例子：拿所有<a href=\"https://www.infoq.cn/article/i0CftJvEiF8mbisle4NJ\">银行</a>\"都在做的公众号来说，即便是在没有后台权限的情况下，运营分析师也能根据有限的公开数据——比如推文标题、位置和时间、内容和形式等信息，判断出某个分行或者业务部门在特定时间段内对公众号渠道的定位是什么、重点关注哪些客群、营销什么产品和服务、主推什么卖点和权益、覆盖广度和投入力度如何等等。如果进一步结合每个推文的阅读量、转发量、点赞量、评论关键词等数据，还可以做定性和定量的主题分析，围绕分行或业务部门经营目标有针对性地给出内容运营的优化思路和测试路径，甚至还能在用户运营和权益规划方面给到非常有价值的建议。</p><p></p><p>换言之，数据是否具备业务价值，不完全取决于规模或体量，而是看是否能解决需求或问题。即便数据资源有限，在特定场景下也可能创造价值。反之，数据体量大固然蕴藏着更大的潜力、更多的可能性，但如果没想清楚要解决什么问题、为什么利用数据就能提供更有效的解决思路、如何使用数据才能达到预期效果这些问题，就会既增加数据成本、更增加决策难度。</p><p></p><p>那么，越来越流行的<a href=\"https://xie.infoq.cn/article/dce85209a2a03378850538d06\">AI分析</a>\"工具以及复杂的模型或算法又是不是必选项呢？晋梅的答案依然是否定的。“和到底是要用铲子还是挖掘机、要用水果刀还是手术刀这类问题一样，我们要分析数据、挖掘洞察，应该选择什么方法、使用什么工具，依然需要具体情况具体分析，合理、有效、划算的才是好的。”</p><p></p><p>比如，当下非常火的短视频或直播，对于拥有海量用户和内容的平台来说，需要相对复杂和专业的算法来识别用户的偏好、内容类目，通过给用户和内容分别打上标签，进行及时和准确的推荐，以此增加用户的粘性、催生更多有需求的内容。</p><p></p><p>但是，在这个生态中，能利用数据去成就和放大业务价值的，除了专业的算法团队，还有运营团队。在直播的过程中，运营全程紧盯人气数据、带货数据，分析观众画像、流量来源、观众互动和商品转化，捕捉有可能影响交易量的潜在因素，通过持续测试和优化投放策略、调整直播间的“人-货-场”来创造价值。</p><p></p><p>“在这个过程中，他们常用的分析方法在绝大多数情况下并不涉及复杂算法和数学模型，很多运营指标也只是基本的加减乘除，但这完全不妨碍优秀的运营人员利用数据去解决转化的问题。”晋梅表示，“我合作过不少顶级的运营，他们不会写SQL、没听过逻辑回归、也弄不懂GBDT，但他们把坚持数据驱动作为信仰，是真正具备数据思维并从中获益的人。”</p><p></p><p>所以，在她看来，是否需要AI分析工具、使用什么复杂度的模型，先要明确业务的目标，基于这个目标去做细化拆解、梳理环节，提出关键问题和假设，再结合数据的实际情况，综合评估不同方案的可行性、投产比、优势短板后再做选择。</p><p></p><h1>数据是锦上添花，而不是救命稻草</h1><p></p><p></p><p>尤其是在数字化愈演愈烈的当下，市场上不乏打着“<a href=\"https://www.infoq.cn/video/9JW4XRrqLxzsP0UMrAiM\">数字化转型</a>\"”的旗号贩卖焦虑者，也不缺把“数字化”当作救命稻草，或者拿着锤子到处找钉子的企业。越是这样，企业越是要从做业务的初心和商业的底层逻辑出发，对自身的核心竞争力有清晰的定位，对市场需求和趋势发展有客观的判断，对数据科学的优势和局限性有必要的认知。否则，很容易陷入先把“数据思维”当作万金油，一顿操作猛如虎却没有感受到业务突飞猛进的尴尬境地。到了最后，只能“甩锅”给数据。</p><p></p><p>以这些年业界对<a href=\"https://www.infoq.cn/article/5ydIS3A81M4MdFC5GJTy\">Capital One</a>\"的解读风向变化为例：晋梅表示，因为短短几十年Capital One就跻身全美头部银行，很多人给它贴上了“大数据风控先驱”的标签，而后冒出来很多主打“大数据风控”的公司，号称他们的风控模型里用了成千上万个特征，智能到恨不得每分每秒都在自我迭代，甚至可以连模型带系数从A银行直接迁移到Ｂ银行去帮后者快速冷启动。但随着后期Capital One遇上股价波动、业务调整，业界同样着急下结论——表示“Capital One走下神坛”。</p><p></p><p>那么，事实果真如此吗？晋梅解释道，“成就Capital One的肯定不是所谓的‘大数据风控’，Capital One也没有什么秘密武器，只是一直遵循着‘提出问题、分析问题、解决问题’的结构性思维方式，让数据恰如其分地发挥价值罢了。”</p><p></p><p>据她介绍，在日常的业务推进过程中，Capital One的算法模型团队会频繁和业务反复沟通金融产品要素、目标客群、推广渠道、营销策略、市场环境和宏观趋势。双方不仅对业务的历史、现状、未来规划都有充分共识，还会对建模样本的选择策略、模型的框架、建模的方法、模型适用性、模型的验证、上线后的监控、可能会出现哪些问题、出现这些问题后的应对预案等信息去做充分的讨论——包括对入模的原始数据、衍生特征的业务价值和投入成本客观评估，以及对关键变量的系数从符号到数值是否合理、映射到业务上代表着什么等问题逐一明确。</p><p></p><p>“脱离了具体业务和场景的模型不但很难放大数据的价值、甚至可能带来毁灭性的灾难。”晋梅强调，“在Capital One工作这么多年给我的启示就是，永远不要在没梳理清楚‘产品-营销-运营’闭环的业务逻辑和关键问题之前，盲目扎入漫无边际的数据海洋；不预设问题、说不明白要验证什么的建模和分析，都是低效甚至无效的。“</p><p></p><p>所以，商业的本质，归根结底还是供给和需求的匹配，是用对的产品或服务、在对的时间和空间、以对的方式满足用户的需求。对于大多数企业来说，直接创造价值的不是数据本身，而是让数据助力实现供给和需求之间的极致匹配。</p><p></p><p>换句话说，企业自身如果没有好的商业模式、好的产品和服务、好的客户体验，那么即便是再先进的技术、再优秀的算法模型、再多的数据也无法帮助它扭转乾坤——比如风靡一时的共享单车，背后也有海量数据在驱动资源配置，但是商业模式变现慢、服务管理不完善等问题同样导致了它最终的失败。过度高估数据价值，指望数据创造奇迹，让本就不符合逻辑的商业模式翻身，那多半会失望而归。</p><p></p><h1>要运用数据，先忘掉“数据”</h1><p></p><p></p><p>“所以，我觉得‘数据思维’，归根结底是因为有了数据加持，从而更有底气和把握做出正确决策，继而更高效地解决问题的思维。我们不能拘泥于字面上的、狭义的‘数据’这个词本身。”晋梅强调，“甚至可以试着先忘掉‘数据’，捋一捋业务模式、搞清楚要解决的核心问题到底是什么；在此基础之上，再引入数据、考虑数据能帮到什么，进而刷新对业务模式和核心问题的理解。”</p><p></p><p>具体来说：第一，理解业务、提出问题；第二，拆解成多个子问题；第三，逐个分析和评估；第四，总结和决策。晋梅表示，这种结构化思维指导下的、解决问题的框架在如今的数字化背景下非常适用。</p><p></p><p>“首先，企业要明确目前业务上面临的核心问题是什么，大家充分探讨和论证、要达成共识；然后，是对问题进行拆解，可以根据业务流程、关键要素或者部门职能等维度细分成多个子命题。围绕每个子命题要敢于提出关键假设、圈定测试范围、排好优先级；第三步用包含数据分析在内的手段，对第二步拆解出来的问题和假设进一步量化、验证和评估。最后一步，基于前面的分析结果，总结和决策。在落地执行、业务迭代的过程中一定还会碰到新的挑战、出现新的问题，这时候再从第一步开始、螺旋式推进。”</p><p></p><p>以<a href=\"https://www.infoq.cn/article/HTEecx2A2UetVtvrZf86\">银行</a>\"的权益运营为例：</p><p></p><p>第一步，明确要解决的问题是，通过丰富的权益持续满足客户的需求，提升<a href=\"https://xie.infoq.cn/article/f58342937546d087dfd6f1385\">客户体验</a>\"、加强客户的粘性，继而提升客户的经营价值；</p><p></p><p>第二步，要做到在对的时间、把对的权益、以对的兑换积分金额和对的兑换方式去满足客户的需求。可以按照“人-货-场”的运营模型进行细分拆解，围绕着每一个维度，做好基础标签体系的建设，梳理交互环节，根据可提升空间和价值明确优先级；</p><p></p><p>第三步，对关键环节开展数据采集、积累、分析和建模并提炼洞察。比如，通过兑换数据，可以把同一个AUM层级的客户按照兑换品类的偏好进一步细分群，也可以评估兑换流程的转化效率、从而定位优化兑换体验的环节，还可以根据权益单品的兑换热度调整选品策略和组合策略；</p><p></p><p>最后，通过对这些信息的综合分析，业务团队和运营人员就可以更有据可依地开展分群运营、渠道优化、商品管理和供应商管理等工作。</p><p></p><p>晋梅告诉InfoQ记者，很多企业尤其是技术人员在推动数字化的过程中，经常把自己局限在第三步，没有考虑具体问题和具体场景的全貌，只是接受一个笼统的需求、确认下字段的口径就开始做分析和模型，最后往往不能提供业务用得上、切实辅助决策的输出。</p><p></p><p>比如，建模同学M接到需求说要给A产品做信用评分卡，他兢兢业业找出来A产品的历史数据、认认真真建模和回测，感觉都没问题了就交付给策略同学P使用。没过多久，策略同学P就抱怨M的评分卡不好使，时灵时不灵。</p><p></p><p>后来才发现，业务团队为了完成增长KPI，自行调整了A产品的受众群体，把过去只聚焦在优质客群的A产品推向基数更大但信用资质略差的客群。为此，他们新增了A产品的进件合作渠道，在展示坑位、流量费用等方面也都做了调整。但是，在推进这些尝试的过程中，他们只使用了过去优质客群的历史表现数据，自然，M同学原来建立的评分卡的有效性就非常有限。</p><p></p><p>所以，晋梅认为，解决问题的经典框架中的每一步如果没有放在整体逻辑中去考虑，很难有价值可言。上面这个例子的漏洞就出在没有对问题进行合理的拆分和定位。除此之外，还有的企业会在第一步明确问题的过程中，就开始出现目标上的偏差。</p><p></p><p>“比如，有些银行在做APP的时候要拼日活数据，为了达标玩命地在App里添加功能，俨然一副要和字节拼内容、和腾讯拼社交、和PDD拼电商的架势。且不说银行App在这类比拼中到底有什么优势，就说日活数据的提升如何与银行经营指标挂钩？或者流量真的来了，银行要用什么产品和服务去承接去变现？而这些接得住流量需求的产品和服务目前是什么状况、有没有需要优化的地方、优化的节奏又是什么？完整的商业模式、业务链路和实现节奏企业自己是要先想清楚的。否则就是盲目投入，很可能钱花了不少却总说不清产出在什么地方。”</p><p></p><p>也就是说，这背后要求企业对自身业务的走势有清晰认知和合理的规划，能够识别和解决到业务的“真问题”而不是“伪需求”。</p><p></p><h1>每个人都该有“业务体感”和“数据思维”</h1><p></p><p></p><p>那么，在一个企业当中，究竟谁应该具备这种结构性思维和解决问题的能力？在传统认知中，我们通常认为创造业务价值并且在这个过程中负责解决具体问题的，是冲在前线的业务部门。而技术部门扮演的是执行者的角色，只要在幕后负责接收业务需求，然后针对性地做开发、找技术、做模型。</p><p></p><p>但是，时过境迁，如今数字化转型的背后需要源源不断的技术内燃力，技术已经成为其中的关键角色，这意味着过去这一套协作模式无法奏效。</p><p></p><p>所以，晋梅的答案是——无论是一线的业务人员还是中后台的技术人员，所有人都应该具备上面所说的解决问题的思维和能力——更确切来说，每个人都要有“业务体感”和“数据思维”。</p><p></p><p>她讲了某区域银行的例子：“在诊断这家银行营销系统时发现，他们很多营销活动的响应率居然都能达到90%以上。即便是非常牛的互联网爆款产品，也很难达到这么高的响应。所以，当我们把这些数据拉出来看的时候，就发现了问题。这些高响应率活动的营销方式都是短信，而这个所谓的响应率其实是送达用户手机的比例，除了被拦截的短信，超过90%多都能送达。事实上，短信送达后用户并不一定会点击参与这些营销活动，而这一步的点击率是比短信触达率更具业务价值的指标。更让我震惊的是，这个数值高达90％、以‘响应率’自居的指标，在这家银行的营销系统里已经静静躺了几年了。”</p><p></p><p>在晋梅看来，这就是缺少“业务体感”，进而对数据的业务意义和价值、对数据所反映出来的业务漏洞也不敏感的一种表现。很多企业拼命花钱做项目、上技术，最后要么没效果、要么说不清楚效果，就宣告项目失败，其实背后可能不是技术水土不服，而是围绕业务闭环本身该做的思考和论证太过敷衍和草率。</p><p></p><p>但是，现实情况是，这种兼具“业务体感”和“数据思维“的<a href=\"https://www.infoq.cn/article/A7VCO3RePjBOPTLKS79B\">人才</a>\"非常稀缺，他的基本能力要求是既要懂业务又要懂技术，复合能力叠加，培养难度也加倍。晋梅指出，企业要解决的当务之急，需要业务人员与技术人员的“双向奔赴”。</p><p></p><p>“一方面，技术一定要去理解业务，明确业务目标、流程和痛点，你要清楚自己通过哪些技术、算法能产生什么样的影响，带来什么样的价值；另一方面，业务对技术的理解，不要只在大数据、人工智能这些热词表面，比如，当你的业务对技术、对数据的依赖越来越重，起码要知道关键技术的基础原理、构建逻辑、优势、短板和局限性等。”晋梅指出。</p><p></p><h1>业务技术双向奔赴，“说人话”比“造新词”更重要</h1><p></p><p></p><p>很多人认为，技术与业务人员之间的隔阂是天然存在的，由于工作内容的差异，双方关心的问题并不相同，无法在同一套话语体系下沟通是非常普遍的现象。晋梅表示，这个问题不是没有解，但也没有捷径。</p><p></p><p>“业务、技术、数据等核心部门首先都不要回避这个问题，其次要一起迎接这个挑战。技术不要认为业务背景的人肯定听不懂技术和数据，高段位技术的一个重要能力，就是让业务人员听懂技术的价值；同样，业务也不要认为技术人员弄不明白商业模式，高段位业务一定要具备的重要能力，就是简单、直接地讲清楚业务本质。”</p><p></p><p>据晋梅介绍，Capital One把这样的文化观念通过制度和流程的方式做了固化和沉淀。</p><p></p><p>比如，在人才招聘过程中，无论是业务、数据还是技术岗，面试时都会安排一轮案例分析。主导这一轮的面试官都是VP级别的高管、拥有一票否决权。而在整整一小时、和高管1V1的面试中，候选人会被提供一个具体的业务问题，围绕着这个问题会被给到业务背景、数据报表等信息，但这些信息不是一次性、一股脑给到候选人的（这也符合我们在实际工作中碰到的情况），而是由候选人和VP展开多轮互动，询问、确认、提出假设、分析解读、给出建议，然后再被提供更多信息后，进一步分析和优化建议。</p><p></p><p>也就是说，在整个面试过程中，候选人身上没有“业务岗”、“数据岗”、“技术岗”的标签，而是一个先快速吸收背景知识，然后提出问题、分析问题和解决问题的角色。</p><p></p><p>再比如，在部门协作过程中，Capital One模型团队在每一次对模型进行调整时，都会和业务充分沟通调整的原因和方式，调整前后的对比，新模型的优势、短板和局限性；反之，业务团队每一次做业务策略调整时也会把调整背景、调整方向、预判影响等信息同步给模型团队。并且，在整个流程中，双方不是对立的状态，也不是各自扛各自的指标，而是一起扛业务最终的收益。</p><p></p><p>此外，Capital One在培训体系中还有一个比较有意思的做法，是在内部培训中设立了一门叫做“COF Lessons Learned”的课程。这是一门不断积累案例的培训，积累的都是公司付出过代价、在践行数据驱动业务的道路上实实在在踩过的“坑”。而培训的老师，首选是那些曾亲身参与和经历过这些“坑”的同事、尤其是负责人。案例的内容很丰富，包括对这些项目的业务背景、入坑复盘，梳理当初的思路是什么、为什么会出现问题、是哪个点没有考虑清楚、或者沟通上不够顺畅，最后导致了什么样的业务结果等等。</p><p></p><p>并且，为了让每一位员工清醒认识数据中既有真相、也有盲区，模型会揭示规律、也会制造错觉，Capital One还有另一门叫做“Statistical Pitfalls”的内训课，专门讲述各类统计模型和<a href=\"https://xie.infoq.cn/article/cf6b6f3fd1e56d6daaf15e4bd\">数据分析</a>\"在支撑业务决策的应用过程中，可能存在的局限性和常见的误区。 </p><p></p><p>“任何技术、任何工具，只有对它的优势、短板、适用性、局限性等有客观的认识，才能把好钢用在刀刃上、真正发挥它的价值。而在这个过程中，前台不应该自诩为商业奇才、中后台也不应该以‘技术大神’自居，探讨问题要以表达清楚、阐述明白为目标，不刻意、不做作。”晋梅强调。</p><p></p><h1>总结</h1><p></p><p></p><p>2020年4月，《<a href=\"http://news.cnr.cn/native/gd/20200409/t20200409_525048284.shtml\">中共中央国务院关于构建更加完善的要素市场化配置的体制机制的意见</a>\"》发布。这是中央第一份关于要素市场化配置的文件，首次将“数据”与土地、劳动力、资本、技术等传统要素并列，明确数据是一种新型生产要素。</p><p></p><p>在这样的背景下，企业需要对数据有客观清醒的认知：首先，数据作为关键生产要素，的确能够给企业带来竞争力提升；但是，数据又不是万能的，它解决不了根本的业务模式问题；另一方面，数据驱动文化的构建、数据思维的培养并非一朝一夕，即便是像Capital One这样曾经被“封神”的公司，它的文化价值观也是在不断跌进去、爬出来，再跌进去、爬出来的过程中反复总结和打磨出来的。</p><p></p><p>所以，留给企业的课题是——在数字化转型过程中，如何尊重规律、尊重分析、讲究逻辑和依据——以业务为船舵，用数据做引擎。</p>",
    "publish_time": "2022-08-11 15:39:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "专属Region、计算巢加速器、无影云电脑：阿里云飞天技术峰会亮点一览",
    "url": "https://www.infoq.cn/article/a7ByD7lC4X6mh18xRfR1",
    "summary": "<p>8月11日，2022阿里云飞天技术峰会在深圳举行。会上阿里云推出了计算巢加速器，将阿里云核心能力开放给更多生态伙伴，同时发布了专属Region等一系列新品及智慧医疗云等解决方案。此外，基于自研飞天+CIPU的核心技术体系，阿里云进一步优化计算效率，释放技术红利，降低客户成本，即日起部分实例用户最多可省19%。</p><p></p><h2>聚焦云计算本质，释放技术红利</h2><p></p><p></p><p>阿里云弹性计算产品线负责人张献涛在会上表示，云计算已经进入高质量发展阶段，阿里云持续投入核心技术研发与基础架构的打造，聚焦云计算的本质，回归技术底层。</p><p>&nbsp;</p><p>今年6月，阿里云推出了云数据中心专用处理器CIPU，深度适配神龙计算平台、盘古存储平台和洛神网络平台，对其进行加速和效率优化，为云服务带来更高的性能、更可靠的稳定性，整体性价比大幅提升。</p><p>&nbsp;</p><p>计算方面，第七代阿里云ESC云服务器相比上一代单核性能提升 30%，整机算力提升 50% 以上；容器部署密度最大可提升 6 倍，是云原生的最佳载体；全量搭载安全芯片，支持可信计算+加密计算，提供金融级的“全方位立体化安全防护”的安全可信环境。</p><p>&nbsp;</p><p>存储也正从成本中心向价值中心升级。针对客户需求带来的改变，阿里云盘古存储推出了企业级数据湖满足大数据的场景需求；对于高性能计算场景，阿里云提供了文件存储CPFS，加速AI与大规模计算；面向企业传统应用负载，ESSD能帮助企业核心应用无缝上云；全球首款云定义存储CDS，打破传统存储物理边界，为企业提供数智未来更多的想象空间。</p><p>&nbsp;</p><p>网络方面，阿里云洛神云网络推出了面向车联网场景的云连接器和NLB，以及面向工业园区场景的5G云专网产品，同时对全球互联网络方案进行了全新升级，CEN、ALB、GA和EIP等产品的各项功能都得到了强化。</p><p>&nbsp;</p><p>为进一步释放技术红利，激活企业创新发展，阿里云在会上宣布，针对中国大陆地域的第六代和第七代云服务器ECS进行调价，用户最高可节省19%，具体规格族包括计算型实例c6&amp;c7、通用型实例规格族g6&amp;g7、内存型实例规格族r6&amp;r7等。</p><p></p><h2>“坚持伙伴优先”，对外开放核心能力</h2><p></p><p>&nbsp;</p><p>刚刚结束的伙伴大会上，阿里云明确了“保持敬畏之心，坚持伙伴优先”的生态策略，“让伙伴走在前面”，为合作伙伴体系的扩张提供充足的空间。上一财年，阿里云伙伴带来的业务规模已达185亿，四年间增长超7倍。</p><p>&nbsp;</p><p>以“计算巢”为例，阿里云在2021年对合作伙伴开放飞天技术底座，目前已有数百家各行各业的ISV入驻计算巢，覆盖大数据、企业管理和工业仿真平台等，使得企业运营成本下降50%，应用交付效率提升10倍。其中不乏各行各业的领头羊PingCAP、达梦、深势科技、泛云科技等。</p><p>&nbsp;</p><p>张献涛表示，为了让ISV上好云、用好云，实现业务提速，阿里云推出了计算巢加速器，针对所有入选加速器的企业，将提供专门的技术专家提供1对1的支持，通过集成阿里云底层技术能力实现技术加速，同时提供钉钉、云市场上架的绿色通道，提供行业客户对接，实现业务加速。</p><p>&nbsp;</p><p>会上，阿里云还与卫宁健康联合推出了WinCloud智慧医疗云联合解决方案，与金蝶云联合推出了苍穹PaaS联合解决方案，通过云盒实现SaaS服务、PaaS服务的本地化、轻量化输出，满足不同客户的灵活部署需求。</p><p></p><h2>专属Region、ACK ONE、无影云电脑，拓展计算边界</h2><p></p><p>&nbsp;</p><p>基于CIPU+飞天云操作系统的新一代云计算架构体系，阿里云具备了一云多形态的能力，用户在任何位置都能享用到云计算的便捷优势。</p><p>&nbsp;</p><p>阿里云在此次峰会上发布了全新的专属Region模式，用户可在其自有的IDC里部署阿里公共云Region级产品并独享使用，减少传统模式下一次性的投入。专属Region由阿里云统一进行底层运维，客户无需关心软硬件的维护和不同云服务之间的适配问题，计算资源更靠近业务本身，适用于大中型泛企业、交通、自动驾驶、科研等场景。</p><p>&nbsp;</p><p>同样，在企业任何需要容器等云原生的地方，基于“ACK Anywhere”理念，阿里云推出了企业级多地域/多集群容器管理平台ACK One，大幅简化了集群管理界面，提供一致的管理、交付、运维体验。无论是基于公共云、专有云、自有IDC还是边缘节点，用户都可以通过ACK One进行统一的容器集群管理、资源调度、数据容灾和应用交付。</p><p>&nbsp;</p><p>无影云电脑则将新一代云计算架构应用于各种终端场景，让生产力触手可及。在本次峰会上，无影针对“千人协同”场景推出四套解决方案，包括分支机构、教育实训、人力外包与安全办公。客户可以在几分钟创建数万台云电脑，让使用者拥有跨终端、强安全、强算力的数字工作环境。在这些场景中，传统个人电脑面临着资源错配、组织生产工具的灵活性与安全性难以均衡等问题。此外，大会上还发布了无影集成至乐播、MAXHUB等更多终端形态的计划。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2022-08-11 15:51:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "直播预告 | Authing 如何打造云原生 SaaS 产品架构？",
    "url": "https://www.infoq.cn/article/488d415de0373d90057123997",
    "summary": "<p>正在或者即将迈入 SaaS 赛道上的你，有没有遇到以下问题：</p><p></p><p>究竟要不要复制欧美模式，搞 PaaS、低代码那一套呢？云原生时代倡导 DevOps，可是自家产品的开发运维割裂怎么办？传统软件厂商向 SaaS 转型，正在经历由认知差异造成的转型阵痛期，该怎么破局？又如何在塑造 SaaS 产品时，发挥前期积累的品牌影响力、沉淀的客户资源和技术实力？企业上云之后，怎么让旧系统方便、快捷迁移到新系统，保证新旧系统的协同呢？</p><p></p><p>不急，8 月 16 日（下周二）15:00-16:00，亚马逊云科技联合 Authing 带来第二期《SaaS 转型之旅》，用最鲜活的理论和最佳的实践案例，为大家解答以上困惑。</p><p></p><p>本期「产品加速营」我们邀请了：亚马逊云科技解决方案架构师龚涛、Authing 产品生态负责人李姿颖共同探讨 《生于云端：Authing 如何打造高效的云原生 SaaS 产品架构》。</p><p></p><p>扫描下方海报二维码</p><p>即可免费报名参与直播</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc7c6b5a148b7db2a1ffd21170fc50c5.jpeg\" /></p><p></p><p>关于 Authing</p><p></p><p>Authing 是国内首款以开发者为中心的全场景身份云产品，为企业和开发者提供完善安全的用户认证和访问管理服务。作为云原生架构下的身份云产品，Authing 在产品创建初期，目标就是服务亿级的企业和个人开发者客户，轻量级、易部署、低消耗、技术栈成熟，运维易的云原生技术产品架构，成为了 Authing 的首选。</p><p></p>",
    "publish_time": "2022-08-11 16:00:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从10月19日起，GitLab将对所有免费用户强制实施存储限制",
    "url": "https://www.infoq.cn/article/LZpQ0SzJ31zLKH8K0723",
    "summary": "<p>从 2022 年 10 月 19 日开始，GitLab将对所有 GitLab SaaS版免费用户的命名空间实施存储限制，容量不超过5GB。有观点认为，该动作是继上周“删除免费用户的不活跃项目”计划泄漏风波后，GitLab为进一步节省成本而做出的决定。</p><p></p><h2>对所有免费用户实施存储限制</h2><p></p><p></p><p>在官网的“存储使用配额”页面上，GitLab 明确表示：GitLab SaaS 免费套餐的命名空间有 5 GB 的存储限制。</p><p></p><p>如果用户的总命名空间存储超过可用的存储配额，那么该命名空间将继续拥有读取权限，但将无法写入任何新数据，其所有项目都将被锁定，用户将不能更改推送到锁定的项目。</p><p></p><p>受影响的用户将会收到电子邮件通知，同时，应用内通知将于 8 月 22 日开始发布。“到目前为止，我们已经联系了3万名用户。”对于新的通知，GitLab工作人员近期在Hacker News论坛上进行了公开讨论和回应。</p><p></p><p>根据官方公布的时间表，针对命名空间存储限制，GitLab将先从 45000 GB 开始往下实施，逐步降低，并最终将其限制到 5 GB。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/7689c10ca0c386080884f2748fb82bd1.png\" /></p><p></p><p>以防超出命名空间存储配额，GitLab 给出的建议是：</p><p>购买更多存储空间。升级到付费等级。减少存储使用。</p><p></p><h2>删除不活跃项目的计划泄漏</h2><p></p><p></p><p>上述举动不免让人将其与上周GitLab陷入的舆论风波联想起来。</p><p></p><p>8月4日，外媒The Register报道揭露，GitLab计划自动删除免费用户中的一年内不活跃项目，并拟在2022年9月生效。</p><p></p><p>一份内部会议通知的议程中列出了删除不活跃代码仓的计划，具体描述如下：</p><p></p><p></p><blockquote>2022 年 9 月 22 日之后，我们将为免费用户推出数据保留政策。该子计划将对免费项目在我们自动删除它（和其中的数据）之前可以保持不活动状态的时间加以限制。据报道，这些项目占GitLab服务器托管成本的四分之一，而删掉这些项目可以为这个云端编码协作服务每年节省高达100万美元。因此，该政策被建议用来帮助保持GitLab的财务可持续性。</blockquote><p></p><p></p><p>知情人士透露，GitLab也意识到该计划可能会引起愤怒和反对，因此在正式删除之前，会给用户提前数周或数月发出警告提醒。此外，在一年12个月的周期内，只要该项目有新的评论、PR 提交或 issue，那这个项目就算得上有“活跃度”，可以继续保留。</p><p></p><p>据悉，关于删除不活跃项目的自动化代码早已在7月底写完。</p><p></p><p>消息一出，开源倡导者、参与.Net开源社区的Geoff Huntley将这一政策形容为“绝对疯狂”，并表示社区对此感到非常愤怒。Huntley进一步指出，源代码并不占据多少空间，删除所有代码将会破坏社区，损害其品牌和信誉。他指出，当软件写完了，某种程度上可以说是完美或者“够用”，不再需要额外更新，那这是否意味着不活跃？</p><p></p><p>另一方面，对于依赖于所谓的“不活跃项目”的其它项目，如果这些“不活跃项目”被删除，那么将会导致下游项目无法编译和运行。</p><p></p><p>“人们在那里托管他们的代码，因为有这样的想法，它将可供公众重复使用和结合，”Geoff Huntley补充道。“当然不能保证它会一直托管在那里，但开源中的不成文规则是你让代码可用并且不会删除它。”</p><p></p><h2>GitLab回应</h2><p></p><p></p><p>后来，GitLab要删除不活跃项目的消息进一步在Twitter和Reddit上发酵并引发了争议。在网络舆论压力之下，GitLab“取消”了该政策。</p><p></p><p>8月5日，GitLab 宣布不会删除免费用户的不活跃项目，其在推特上发出了以下声明：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b6fb7179f71ab55b8dd7b29994dc61fe.png\" /></p><p></p><p></p><blockquote>我们在内部讨论了如何处理不活跃的仓库。我们达成了一个决定，将未使用的代码仓转移到对象存储中。一旦实施，它们仍然可以被访问，但在长期闲置后需要更长的时间才能访问。“在这种情况下，不活跃的定义是什么？”有人在下面留言问，</blockquote><p></p><p></p><p>GitLab 联合创始人兼CEO Sid Sijbrandij对此回应道：“我们还不确定。可能所有的‘写入操作’都能让项目保持活跃，比如创建问题、合并请求、将更改推送到分支等等。只要人们在进行诸如克隆、fork等读取操作，我们也可能保持它处于活动状态。”</p><p></p><p>他还表示，放入对象存储中的项目仍然对所有公众可见，并不会加上只有所有者可见的限制。</p><p></p><p>总的来说，GitLab并未在这则声明中否认报道的真实性。</p><p></p><p>针对GitLab强调的5 GB限制，有网友拿GitHub做对比：</p><p></p><p></p><blockquote>建议仓库保持较小，理想情况下小于 1 GB，强烈建议小于 5 GB。较小的仓库克隆速度更快，使用和维护更容易。 如果您的仓库过度影响我们的基础架构，您可能会收到来自 GitHub 支持 的电子邮件，要求您采取纠正措施。 我们力求灵活，特别是对于拥有很多协作者的大型项目，并且尽可能与您一起找到解决方案。“虽然这有点含糊其辞，但听起来似乎有酌情处理/例外的空间，而不是硬性规定5GB的上限。”该网友表示。</blockquote><p></p><p></p><p>此外，也有观点对“GitLab 5GB上限”表示理解，毕竟“GitLab作为一家企业，前提是要先活下来”，而GitHub背靠微软，商业压力没那么大——“微软从 GitHub 获得的价值不是它的收入，而是它的影响力。”</p><p></p><p>参考链接：</p><p><a href=\"https://docs.gitlab.com/ee/user/usage_quotas.html#namespace-storage-limit-enforcement-schedule\">https://docs.gitlab.com/ee/user/usage_quotas.html#namespace-storage-limit-enforcement-schedule</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=32386323\">https://news.ycombinator.com/item?id=32386323</a>\"</p><p><a href=\"https://www.theregister.com/2022/08/05/gitlab_reverses_deletion_policy/\">https://www.theregister.com/2022/08/05/gitlab_reverses_deletion_policy/</a>\"</p><p><a href=\"https://www.theregister.com/2022/08/04/gitlab_data_retention_policy/\">https://www.theregister.com/2022/08/04/gitlab_data_retention_policy/</a>\"</p>",
    "publish_time": "2022-08-11 16:44:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "网易有道发布多款智能硬件：首创词典笔OS，学习机首次引进 IRT 自适应技术",
    "url": "https://www.infoq.cn/article/5LaZorFzj54vi6YV5xLf",
    "summary": "<p>8 月 9 日，网易有道举办 2022 智能硬件秋季新品发布会。发布会上，网易有道发布了有道词典笔 X5、有道 AI 学习机两款新品，并展示了包括有道词典笔系列、有道 AI 学习机、有道智能学习灯、有道听力宝在内的智能学习硬件产品矩阵及成绩单。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/79/bf/79f9df254b7bb993855bf730cd0edbbf.png\" /></p><p></p><h2>词典笔 X5 背后的 AI 黑科技</h2><p></p><p></p><p>据介绍，词典笔 X5 在查词功能上，查词速度再提升 15%；全面升级的词库量达到 1000万；实现了分学段学单词，给不同学段用户自动匹配适合的词典和查词结果，让单词学习更精准。</p><p></p><p>翻译方面，有道词典笔 X5 支持超过 100 种语言在线翻译，其中法语、西语、德语等 14 种高频使用外语搭载了有道自研的神经网络翻译系统（YNMT），翻译质量媲美真人。神经网络翻译系统（YNMT）上线于 2017 年，是一个以 NMT 系统为核心的混合翻译引擎，它能根据用户查询的内容，选择最优的算法给出高准确率的自动翻译结果。</p><p></p><p>针对听力练习，为提升学习体验和效果，有道词典笔 X5 在软硬两方面同时发力：内容方面内置 354 个听力专题，超 10000 条优质听力资源，可根据学段为用户智能匹配合适的听力内容；硬件方面，全新 AI Sound 超强声学配置带来极致音质体验，让用户能够随时随地精准训练听力。</p><p></p><p>此外，有道词典笔 X5 还搭载了分学段写作指导、SuperMemo 记忆法背单词、全国主流英文中小学教材同步等辅助学习功能，更好的满足多元化语言学习需要。</p><p></p><h2>首创词典笔 OS</h2><p></p><p></p><p>在发布会后接受媒体采访时，有道表示，随着用户规模增大，团队发现了更多用户的多元化需求，并且由于笔能提供更高效的交互方式，用户对词典笔也有着更多期待。于是，团队开始在操作系统上寻找答案，希望通过打造一个更加智能化的、用户可以定制化的操作系统，为用户提供更多应用。</p><p></p><p>在此背景下，网易有道首创词典笔 OS，让词典笔实现智能化定制，满足多元的用户需求。</p><p></p><p>在应用方面，有道词典笔 X5 搭载词典笔 OS，用户可根据个人需要自主下载学习应用，系统也会根据用户的学龄段智能推荐，让词典笔实现一笔多用。以搭载有道云笔记为例，词典笔不仅能通过语音实时转写文字，还支持笔端扫描摘录，让词典笔同时实现摘录笔、录音笔功能，在线文档可在词典笔、手机和电脑上多端同步查看。</p><p></p><p>“类比到手机的变化，功能机到智能机时代的变化，词典笔从传统的词典笔到拥有OS的词典笔，更核心的变化就是一笔多用，不仅仅是一个词典笔。”有道产品经理在接受采访时表示。</p><p></p><h2>有道 AI 学习机首次引进 IRT 自适应技术</h2><p></p><p></p><p>发布会上，有道还带来了新品 AI 学习机。据介绍，有道 AI 学习机首次引进 IRT 自适应技术，打造了 AI 精准学功能。这套技术和美国 SAT、GRE 等考试同源，它的特点是通过更少的题目，精准地定位到孩子的学习薄弱点，进而精准提升。</p><p></p><p>具体来说，第一步，通过线上做题找到薄弱点，也可以将作业、试卷一键拍照，系统会根据1.5亿题库精准检索，自动生成个性化知识图谱；第二步，根据知识图谱智能规划学习路径，推荐学习内容；第三步，练习，内置 7 万道高频考题；第四步，错题整理，制定个性化错题本，能自动收录错题，分类标记错因。</p><p></p><p>此外，有道 AI 学习机配置了包括英文作文批改、智能听写、口算检查、指尖查词查句、课本指读在内的多项作业小助手功能。</p><p></p><p>对于网易有道的教育智能硬件战略，网易有道 CEO 周枫在接受采访时表示，有道做教育智能硬件有三个优势：硬件、AI 算法、内容。“我们对自己的定位就是三个方面都要强，这性年来我们一直干的事就是不断地提升能力”。</p><p></p><p>“硬件方面，这一代非常大的提升就是让所有的绝大部分运算算法 NPU 化，芯片里面其实有 CPU 部分和 NPU 部分，这是做人工智能的计算。NPU 部分其实有机会大幅度提升性能和效果的，因为他是一个比较新的技术，所以大部分团队没有办法把 NPU 跑起来。我们现在 OCR 的识别也好，这些 NPU 我们完全优化的非常好了。AI 翻译能力不用讲，这几年我们语音方面也有进步。”周枫说道。</p>",
    "publish_time": "2022-08-11 17:07:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "【精通内核】计算机程序的本质、内存组成与ELF格式",
    "url": "https://www.infoq.cn/article/8f738c08e409b011e92acb03f",
    "summary": "<p></p><blockquote>前言📫作者简介：<a href=\"https://www.infoq.cn/u/xiaoming/publish\">小明java问道之路</a>\"，专注于研究计算机底层/Java/Liunx内核，就职于大型金融公司后端高级工程师，擅长交易领域的高安全/可用/并发/性能的架构设计📫&nbsp;🏆CSDN专家博主/Java领域优质创作者、阿里云专家博主、华为云享专家、51CTO专家博主🏆🔥如果此文还不错的话，还请👍关注、点赞、收藏三连支持👍一下博主~</blockquote><p></p><p></p><p></p><h1>本文导读</h1><p></p><p>精通真正的高并发编程，不仅仅是API的使用和原理！计算机最基础的程序是怎么组成的呢？这个都不知道，又如何能证明你的程序是高并发的？本文深入浅出，讲解程序的本质（编译的过程）、组成（程序所需的内存）与格式（ELF），希望读者可以构建计算机从写代码到编译到执行的链路的底层思维。</p><p></p><h1>一、计算机程序的组成</h1><p></p><p></p><h2>1、程序执行的本质</h2><p></p><p>我们先了解下我们平时写的程序都怎么写？怎么执行的？Java、Python、GO、C/C++等等语言，我们需要通过IDE（Integrated Development Environment&nbsp;）中编写，Java有JDK、各个语言都有自己的集成开发环境，然后在idea或者eclipse等等开发工具，通过集成开发环境IDE中的编译器或者内置编译器，变成CPU能工作执行的格式来执行。</p><p><img src=\"https://static001.geekbang.org/infoq/9b/9b40a9fe3f782bce55ba8c3c2c87098e.png\" /></p><p>举一个简单的C语言的小例子，printf('');&nbsp; 是操作硬件打印字符，由于只有OS才能操作硬件，所以这个函数调用，一定调用了系统调用的接口，由于系统调用的接口约定较为复杂，且每个OS都不一样，而C语言需要可移植性，所以推理得出 有个东西包装了系统调用过程，提供统一接口，给应用程序使用，那么其就会调用GLIBC 的函数库函数。&nbsp;</p><p><code lang=\"cpp\">#include \nint main()&nbsp; &nbsp;    // 封装了汇编的指令片段调用过程：保存返回地址+开辟栈帧+传递参数+返回值\n{&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 定义作用域：标识指令片段&nbsp; \n    printf('');&nbsp; // 封装 call 指令和参数传递过程（寄存器传递，栈传递）&nbsp; \n    return 0;&nbsp;   // 将返回值放入约定的地方\n}</code></p><p> 等价于 java（其他语言的）import ，将函数定义导入，为什么导入？因为编译器需要这些东西，虽然不知道具体的函数地址、变量地址在哪里，但是知道调用的什么东西，编译器才能对其记录，并且在某个时候将它所需要的东西给出。</p><p></p><h2>2、程序保存在哪里</h2><p></p><p>上面说的，这些编译好的数据保存在我们的电脑上，具体在我们的计算机哪里，我们需要了解计算机有哪些存储器，存储器系统（memory system）是一个需要考虑多元因素存储设备的层次结构，例如容量、成本和访问时间。</p><p><img src=\"https://1484576603-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MV9vJFv4kmvRLgEog6g%2F-MXeRbSaFFPrYHtrsLZd%2F-MXeSA_XumzA0W1miWSg%2F%E6%8D%95%E8%8E%B7.PNG?alt=media&amp;token=ded59e91-4761-4a88-b41f-95ae1702aaa3\" /></p><p>这些编译好的数据就保存在磁盘上（local disks），然后我们用过系统调用告诉操作系统（OS），由操作系统来获取编译好的数据并进行解析，将这些数据从磁盘加载到内存（DRAM）。</p><p>然后在内存中创建一个进程来代表你写的这个程序，最后CPU执行。在这个过程中最重要的两个方面，一方面是上一小结的编译原理，另一方面就是这些编译好的数据是如何存储的，以及他们的格式是什么样的，这个格式就是操作系统（下属说操作系统都是Linux）如何正确的解释汇编代码。</p><p></p><h2>3、计算机程序的组成</h2><p></p><p>计算机程序保存的位置我们知道了，程序在内存的细节就需要了解了，程序组成不是凭空而来的，我们在了解编译及编译器和计算机内存体系之后，看看程序应该由哪几部分组成</p><p>栈区（stack，用于存储函数的参数值、局部变量值等）编译器可以自动分配和释放。</p><p>堆区（heap）由程序员进行分配和释放（一些编译器中也会自动管理，例如JVM的GC），若程序员不释放，程序结束由OS进行回收。</p><p>​可执行程序包括BSS段、数据段、代码段：</p><p>数据段（data段），初始化的全局变量和静态变量的区域，程序结束由OS释放。</p><p>BSS段（Block Started bySymbol以符号开始的块，BSS是Linux 链接器产生的未初始化数据段），未初试化的全局变量和静态变量的区域，只记录需要的内存大小并不实际存放数据，同样结束由OS释放。</p><p>代码段（text 段）存储程序的二进制代码和场景等。</p><p><img src=\"https://static001.geekbang.org/infoq/d0/d0bf78e85bf5bcdcda086ac447014b2b.png\" /></p><p>一个进程在运行过程中，代码是根据方法依次执行的，当然跳转和递归有可能使代码执行多次，而数据一般都需要访问多次，因此需要单独开辟空间以方便访问和节约空间。</p><p>临时数据及需要再次使用的代码在运行时放入栈区中，生命周期短。全局数据和静态数据有可能在整个程序执行过程中都需要访问，因此单独存储管理。堆区由用户自由分配，以便管理。</p><p></p><h2>4、程序的格式（ELF）</h2><p></p><p>ELF（Executable and Linkable Format，可执行链接的格式），这里的格式说的就是程序的格式，程序的格式分为3类：</p><p>可执行文件，文件保存着一个用来执行的程序（如bash、gcc等）。</p><p>可重定向文件，文件保存着代码和适当的数据，用来和其他的目标文件一起来创建一个可执行文件或者是一个共享目标文件(目标文件或者静态库文件，即 Linux 通常后缀为 .a和 .o的文件)。</p><p>共享目标文件，共享库文件保存着代码和合适的数据，用来被下连接编辑器和动态链接器链接（Linux中后缀为 .so的文件）。</p><p>本节用一个简单的例子，用C语言生成一个可执行文件，然后根据这个可执行文件分析和理解ELF格式下的 可执行文件的组成</p><p><code lang=\"cpp\">#include \nint main()&nbsp; &nbsp;    // 封装了汇编的指令片段调用过程：保存返回地址+开辟栈帧+传递参数+返回值\n{&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 定义作用域：标识指令片段&nbsp; \n    printf('');&nbsp; // 封装call指令和参数传递过程（寄存器传递，栈传递）&nbsp; \n    return 0;&nbsp;   // 将返回值放入约定的地方\n}</code></p><p>用 gcc demo.c  命令执行后，将会得到一个 a.out 的文件，这个文件就是 ELF文件可执行文件，当我们运行 ./.a,out  文件后，将会在控制台打印。</p><p>现在就通过这个得到的 a.out 文件来分析 ELF 文件的格式。这段代码的第一行 #include ，引入它是为了调用&nbsp;printf 函数，底层通过调用 OS 提供的函数，向控制台打印，这不是我们自己实现的，这个步骤需要依赖C语言函数库 Glibc，而 stdio 就是含数据中包含的基本输入输出的定义。</p><p>但是我们编译后，可以看到并没有 Glibc-&gt;printf 的实现文件，那这个文件在哪里呢？</p><p>就在共享目标文件（共享链接库）中，并以 .so 结尾，在 Linux 中真正执行文件的输出操作的代码，已经在文件中了，接下来如何调用，这里面就涉及动态链接的知识 </p><p></p><h1>二、计算机程序的可执行文件</h1><p></p><p></p><h2>1、动态连接</h2><p></p><p>我们再看上面代码，代码中包含的 stdio.h 会告诉编译器，我们需要依赖这个函数定义的函数。因此编译器编译好的 a.out 文件中包含一个动态的链接符好 printf，当运行  ./.a,out  时，OS中动态链接器会发现该符不完整，还需要一个函数的地址，即 printf 的地址，这时链接器就会加载动态链接库，找到它的符号表，并发现文件中包含了 printf函数，于是就把原来调用 printf 的符号地址修改为动态链接库中的真实地址。</p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfc6beddb5780c3a935aa1fa8bd2987d.png\" /></p><p>通过上图可以看到，gcc demo.c  命令默认是使用动态链接的方式来生成可执行文件，这时的文件中不完整，因为它需要的 printf 函数是没有执行体的，只有一个符号，称之为符号地址，当它被 OS 加载到内存中后，会通过动态链接器将符号地址修改为指向动态链接库的 printf 函数地址。通过 ll -h 命令，我们可以看到 a.out 文件的大小为8KB。</p><p></p><h2>2、静态连接</h2><p></p><p>此时，读者肯定在想，为什么要在加载后由链接器找到真实的地址，而不是在编译生成aout文件时就将 动态链接库的代码包含其中呢？</p><p>相信这是大部分读者的想法。不过别着急，我们确实有办法让 a.out 在编译时就包含 printf 代码。这种方式被称为静态链接，可以通过可重定向文件 libc.a 实现，这个文件里同样包含 printf 的符号定义，但我们一般不这么做，我们看下静态编译的效果。</p><p><img src=\"https://static001.geekbang.org/infoq/37/371434277d097272767cdee74d24787f.png\" /></p><p>通过上图，描述了通过 gcc-static demo.c 命令生成可执行文件件，这时的文件就是完整的可执行程序，它不包含符号地址，并且都是指向真实函数执行地址，当 OS 将它加载进入内存后，可以直接执行，不需要通过动态链接器对它进行链接。</p><p>但是，我们却发现它的大小居然有841KB。这个大小远大于使用动态链接生成的可执行程序。</p><p>此时，读者就知道二者之间的区别和使用选择了，假设有 100个程序，都使用动态链接库函数，如果用动态链接，这100个线程可以共享这一个动态链接库。换而言之，内存里只会存在一份 printf 代码，100个线程共享。但如果我们选择静态链接呢?将会造成内存里存在100份相同的printf代码。这肯定不好，占用内存，还得不到什么好处。</p><p>动态链接这么好，我们是否可以在任何场景下都用它呢?肯定不可以。静态链接虽然有缺点，但存在即合理。当开发者不想让使用者提供动态链接库时，可以直接给它一个静态链接生成的应用程序，这样使用者就不用再去安装依赖的动态链接库，直接执行即可。同时，也可以避免使用者发现开发者依赖了哪些动态链接库，进而推敲代码的功能和特点。</p><p></p><h1>三、ELF原理解析</h1><p></p><p></p><h2>1、ELF格式及其原理</h2><p></p><p>在了解了程序执行本质、程序的内存区域和格式、ELF 文件的3种体现形式、动态链接和静态链接的概念后</p><p>我们看看ELF 文件格式到底是什么样子的，如下图所示。</p><p>ELF文件可分为两部分来看待，即链接节（linkable section）和执行段（exectable segment）。</p><p>从链接器的角度看，看到的是一堆 section，也称之为节。</p><p>从 CPU 调度执行的角度看，看到的是一堆 segment，也称之为段。还记得前面学习的段寄存器吗?那是将内存分段，这里是将一个可执行程序按功能分如代码段、数据段等。</p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d06daca9c41ac319594f8c204374c3f.png\" /></p><p>同Java 语言编译后的 class 文件一样，它也有自己的头部，上图中ELF header 头部的下方存在 program header table，用于告诉 OS 在加载这个可执行文件后，这些段在哪里。</p><p>文件末尾的 section header table 则由链接器识别使用，用寻找这个文件的用于链接的节处于文件的哪个位置。</p><p>这时，我们可以给出结论，在动态链接器执行时，将忽略掉 program header table，只使用 section header table；而当程序被执行时，将忽略掉 section header table，只使用 program header table，同时一个 segment 段可以由多个 section 节组成。下图描述了 ELF 文件的不同视图。</p><p><img src=\"https://static001.geekbang.org/infoq/e1/e131e4cb72e1e1eb6522d9b089b711bc.png\" /></p><p></p><h2>2、ELF文件内部实现</h2><p></p><p></p><h3>2.1、ELF header文件头分析</h3><p></p><p>ELF header 开始了解 ELF 文件格式，仍然用 hello world 的例子，用 gcc demo.c 生成使用动态连接的可执行文件。</p><p>使用 readelf -h a.out  指令输出 a.out 执行程序的 ELF 头部信息。同 Java 中的 .class 文件一样，ELF 也需要 magic 变量来表明它是一个 ELF 文件，且类别为 ELF64。此外，ELF 还需要使用版本信息和数据信息。这里，只需要关注以下几条信息。</p><p>MELF 类型为 EXEC，表明为可执行文件。</p><p>Entry point address 程序开始执行点为 0x400440。 </p><p>program headers 程序头部表在 64byte偏移处。 </p><p>section headers 节头部表处于 6480byte 偏移处。</p><p>此时，链接器或者 OS 就可以通过这些信息在内存中加载、链接、执行这个程序了。</p><p><img src=\"https://static001.geekbang.org/infoq/ab/abb4b34f3140147a1d316f0112b3a186.png\" /></p><p>当我们使用  gcc -static demo.c 、 readelf -h a.out  后可以看到，动态链接器的 INTERP 不见了，证明静态链接包含了所有需要的信息，所以不需要动态链接器的参与</p><p></p><h3>2.2、ELF文件header table头部表分析</h3><p></p><p>接下来，看看这个程序的 Program headers table 信息。</p><p>使用 readelf -l a.out  命令输出其 Program headers 信息，从下图可以看出以下信息。</p><p>PHDR，表明程序头部表的虚拟地址信息（偏移量为0x40，十进制为64byte，即Program headers</p><p>地址）。</p><p>INTERP，表明程序被&nbsp;OS&nbsp;加载到内存中后，必须调用的解释器，它通过链接其他库来满足未解析的符号引用，用于在虚拟地址空间中映射当前程序运行所需的动态链接库的函数，如程序中使用的&nbsp;printf 函数。</p><p>LOAD，表明当前程序文件映射到虚拟地址空间的段，其中保存了常量数据（如字符串）、程序目标代码等。另外，还可以通过后面跟着的权限位来判断是代码段还是数据段，其中RE表明为可读、可执行的段，则为代码段；RW&nbsp;表明可读、可写的段，则为数据段。&nbsp;</p><p>DYNAMIC，表明保存了由动态连接器，即 INTERP 段中指定的解释器使用的信息。</p><p>Section&nbsp;to&nbsp;Segment&nbsp;mapping，表明，这些段由 section&nbsp;节组成。</p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c7c2fc63ad94e04e8e342ed7ee9e958.png\" /></p><p></p><h3>2.3、ELF文件section节信息分析</h3><p></p><p>接下来，我们来分析程序的section节信息。通过 readelf -S a.out  命令，可得到 section节信息。包含了当前程序的节表信息，包括每个节的大小、类型、虚拟地址信息和偏移量。通过这些信息我们可以在动态链接时组合成相应的段信息。对于每个节的标志位，Key to Flags中已经给出，这里不再赘述。通常我们可以看到以下节信息。</p><p>.hash：符号哈希表。</p><p>.dynsym、.dynstr：动态链接符号表，动态链接字符串表。</p><p>.rel.dyn、.rel.plt：节区中包含了重定位信息。</p><p>.init：此节区包含了可执行指令，是进程初始化代码的一部分。当程序开始执行时，系统要在开始调用主程序入口之前（通常指 C 语言的 main 函数）执行这些代码。 </p><p>.plt：此节区包含过程链接表（procedure linkage table）。</p><p>.text、.fini：此节区包含程序的可执行指令。fini 是进程终不止代码的一部分，程序正常退出时，系统将安排执行这里的代码。</p><p>.rodata：这些节区包含只读数据，这些数据道通常参与进程映像的只读代码段。</p><p>.init_array、.fini_array：进程初始化、退出时时所运行的函数指针数组。 </p><p>.dynamic：此节区包含动态链接信息。</p><p>.got：此节区包含全局偏移表，其与 plt 一起协作完成符号的动态查找。</p><p>.data：节区包含初始化了的数据，将出现在程序的内存映像中。</p><p>.bss：包含将出现在程序的内存映像中的为初始化数据。根据定义，当开始执行程序时，系统将把这些数据初始化为 0。此节区不占用文件空间。</p><p>.comment、.debug_*：符号调试信息。</p><p></p><h3>2.4、ELF文件分析总结</h3><p></p><p>对于ELF 的文件描述和程的组成信息就描述到这里。这里读者只需要对程序组成有个基本印象，为后续的学习铺路，形成计算机思维。</p><p>读者可以看到，实际上程序分为3类，即共享目标文件、动态链接库和可执行文件。其中，可执行文件又分为可动态链接的执行文件和静态连接的执行文件。这里只分析了可动态链接的执行文件对比了静态连接的执行文件的程序头部表，看了看段信息，并没有去分析另外两类文件。这里给出了方法和相应的命令。</p><p>读者只需要通过本节了解如下信息即可，什么是 ELF，ELF header 的了解，什么是 section 节和 segment 段信息。</p><p></p><h1>本文总结</h1><p></p><p>本文深入浅出，讲解程序的本质（编译的过程），通过C语言代码的例子，分析程序从运行到CPU执行的整个过程和其中流转的原理。通过这个过程，我们开始了解程序的组成（程序所需的内存），程序保存在磁盘、内存，程序在内存中的由那几部分组成，堆区栈区代码段等等。最终通过这个保存，引申出程序的格式（ELF），分析了程序的动态连接、静态链接，什么是 ELF、ELF header 、section 节和 segment 段信息。相信这里读者对程序组成有个基本印象，为后续的学习铺路，形成完整的计算机思维。</p><p></p>",
    "publish_time": "2022-08-11 00:00:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Kubernetes 之父：他们现在在哪里？",
    "url": "https://www.infoq.cn/article/jtKJpfeGvX1hMpC83KvF",
    "summary": "<p>Kubernetes 已经席卷了软件开发领域，因高可扩展性、弹性和可靠性，被无数需要使用容器的企业采用。根据CNCF 2021 年年度调查，96% 的组织正在使用或评估 Kubernetes。Kubernetes 是容器编排和调度的事实上的选择。最初，Kubernetes由Joe Beda、Brendan Burns和Craig McLuckie创立，并由谷歌在2014年首次对外宣布。如今，新的领导人正在涌现，将火炬传递下去。</p><p>&nbsp;</p><p></p><h2>他们现在在哪里？</h2><p></p><p>&nbsp;</p><p>Kubernetes系统的开发和设计都深受谷歌的Borg系统的影响。“Kubernetes 在某种程度上是 Borg 的精神继承者，”Kubernetes 的联合创始人 Joe Beda 在 2021 年接受采访时这样描述。</p><p>&nbsp;</p><p>Kubernetes 的出现是由于正确的技术、正确的时机和正确的人员的结合。在此之前，谷歌员工正在研究 Google Compute Engine，即谷歌的 EC2 版本，他们也使用Docker，受到 Docker 工具和容器标准化潜力的启发，于是开始着手创建一个 “最小可行的编排器”，为容器化的应用提供资源调度、部署运行、服务发现和扩容缩容等丰富多样的功能。</p><p>&nbsp;</p><p>Kubernetes 于 2014 年开源，并于 2018 年捐赠给云原生计算基金会。该项目现在得到了广大机构和社区成员的支持，远远超出了谷歌的控制范围。Kubernetes联合创始人也继续做着非凡的事情——那么他们今天在哪里？</p><p>&nbsp;</p><p></p><h3>Joe Beda</h3><p></p><p><a href=\"https://www.linkedin.com/in/jbeda/\">Joe Beda</a>\"现在处于“半退休“状态，他最近的职位是在VMware担任首席工程师。</p><p>&nbsp;</p><p>Beda 拥有辉煌的职业生涯，曾在 VMware、Split Software、Heptio、Shippable、CoreOS 和 Microsoft 任职。作为在谷歌的高级软件工程师，Beda 是 Kubernetes 的联合创始人和技术负责人。在谷歌的 10 年里，他还为许多其他重要项目做出了贡献，例如 Google Hangouts 和 Google Compute Engine。</p><p>&nbsp;</p><p>根据他的GitHub数据显示，Beda 仍然是一名活跃的开源贡献者，最近为 Kubernetes、VMware Tanzu、ngrok-k8s 和其他项目做出了贡献。让 Beda 印象深刻的是 Kubernetes 采用率的大幅上升，Beda 在一次会议上说，“我们没预见到这一点。”</p><p>&nbsp;</p><p></p><h3>Brendan Burns</h3><p></p><p>Kubernetes 联合创始人<a href=\"https://www.linkedin.com/in/brendan-burns-487aa590/details/experience/\">Brendan Burns</a>\"现在是微软副总裁，负责 DevOps 相关的 Azure 项目，包括 Azure 上的 K8s。Burns 最近的工作重点是负责一个与 Kubernetes API 一起使用的客户端库。在Borg时代，Burns是谷歌的高级软件工程师。他在谷歌工作了八年。</p><p>&nbsp;</p><p>在 2017 年，Burns 曾在一次采访中说他加入微软是帮助微软，特别是混合多云环境中，更灵活地使用容器化。</p><p>&nbsp;</p><p></p><h3>Craig McLuckie</h3><p></p><p>&nbsp;</p><p>Kubernetes 联合创始人<a href=\"https://www.linkedin.com/in/craigmcluckie/details/experience/\"> Craig McLuckie</a>\"现在是一位自称自雇的全职爸爸（Stay-at-Home Dad），曾在谷歌担任首席产品经理和集团产品经理，还是 Google Compute Engine 的原始产品负责人。</p><p>&nbsp;</p><p>离开谷歌后，McLuckie 成为 Heptio 的创始人兼 CEO，2018 年 Vmware 收购 Heptio 后担任 VMware 研发副总裁。MckLuckie 也是云原生计算基金会诞生的主要支持者。McLuckie 在 2019 TechCrunch 的采访中说道：“我们让基础设施技术变得非常非常无聊。”</p><p>&nbsp;</p><p></p><h2>现在的接班人是谁？</h2><p></p><p>&nbsp;</p><p>这些当年的创新者退居幕后，那么谁会继续接替他们？我们查看了开源云原生世界中的一些KOL，以下是一些当今掌舵Kubernetes的关键人物。</p><p>&nbsp;</p><p></p><h3>Kelsey Hightower</h3><p></p><p>&nbsp;</p><p>熟悉企业软件架构趋势的人都知道Kelsey Hightower，目前受雇于谷歌的云计算部门，是谷歌的首席工程师，负责开发谷歌的云平台。 在谷歌，Hightower 帮助开发了谷歌的 Kubernetes Engine (GKE) 和 Cloud Functions。</p><p>&nbsp;</p><p>自 2014 年以来，Hightower一直是 Kubernetes 的布道者和持续贡献者，是Kubernetes 里最著名的演讲者，也是云计算领域的杰出人物。他于 2015 年创立了 KubeCon，甚至与 Kubernetes 的联合创始人 Beda 和 Burns 合作撰写了一本书，《Kubernetes: Up and Running》，由 O'Reilly 于 2017 年出版。他的背景特殊，是一位自学成才的程序员。</p><p>&nbsp;</p><p></p><h3>James Governor</h3><p></p><p>&nbsp;</p><p>James Governor， RedMonk 的分析师兼联合创始人。</p><p>&nbsp;</p><p>根据Governor的说法，现在的重点是发展社区，拓宽平台，加强事件驱动计算和无服务器的宣传。</p><p>&nbsp;</p><p></p><h3>Brian Behlendorf</h3><p></p><p>&nbsp;</p><p>Brian Behlendorf是Linux 基金会下Open Source Security Foundation&nbsp;负责人，也是 Linux 基金会主办的开源区块链 Hyperledger 的执行董事。</p><p>&nbsp;</p><p></p><h2>K8s 和云原生的未来</h2><p></p><p>Kubernetes 已成为企业软件开发的重要工具。毫无疑问，它是大规模管理大型容器集群的最佳方式之一。值得庆幸的是，Kubernetes 还受益于充满活力社区文化，在最新的KubeCon + CloudNativeCon Europe 2022上有超过 26,000,000 名线上线下参会者。除了上面提到的那些名字之外，无数贡献者都在推动这个平台向前发展。</p><p>&nbsp;</p><p>Craig McLuckie也看好云原生技术的边缘计算：“从未来的角度来看，一切都与边缘有关。这将是一个有巨大增长的领域，并且将会在未来几年内成为极具颠覆性的创新领域。”</p>",
    "publish_time": "2022-08-11 17:38:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OceanBase迈入4.0时代：首次实现单机分布式一体化架构，MySQL兼容能力全部开源",
    "url": "https://www.infoq.cn/article/0iiP1IGakozZGLVGEwo5",
    "summary": "<p>8月10日，<a href=\"https://www.infoq.cn/article/dsOJbl8uJv5bGWUBf4rR\">2022 OceanBase年度发布会</a>\"在京沪深三地同时召开。此次发布会上，<a href=\"https://www.infoq.cn/minibook/61tq7MPNErVJBTM4deZb\">OceanBase</a>\"面向全球用户发布了全新4.0产品—“小鱼”，此外还宣布了对产品、服务、生态、开发者应用进行全面升级。新策略下，OceanBase的产品性能有哪些突破？开源版本如何升级？在此次发布会上，OceanBase CEO杨冰就以上问题给出了答案。</p><p></p><h2>OceanBase迈入4.0时代，有哪些新特性值得关注？</h2><p></p><p>&nbsp;</p><p>新发布的OceanBase 4.0采用了业内首个单机分布式一体化架构，可以实现单机部署并兼顾分布式<a href=\"https://archsummit.infoq.cn/2022/beijing/track/1184\">架构</a>\"的扩展性与集中式架构的性能优势，实现了更低的部署成本和运维复杂度，灵活满足不同使用场景需求，极大降低了中小企业使用分布式数据库的门槛，打破了当前分布式数据库只适用于大客户的局限，让中小企业也可以低成本享受分布式数据库的高性能。</p><p>&nbsp;</p><p><a href=\"https://xie.infoq.cn/article/3e4eeb101ff98a47cd03d58c0\">杨冰</a>\"表示，自2020年正式商业化至今，<a href=\"https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247510671&amp;idx=2&amp;sn=2adc5fe0b8b59766f9cbdfaf5b91ca05&amp;chksm=e8d4554ddfa3dc5b106e0938756bcf8ec4a21a60d7b0ff53ea84ae60c6e1b95caddb46486c86&amp;scene=27#wechat_redirect\">OceanBase</a>\"经过了两年的商业化发展，目前已经服务了来自金融、政府、运营商、零售、互联网等多个行业的400多位客户。经过2年发展，OceanBase发生了非常微妙的变化。</p><p>&nbsp;</p><p>孵化于金融行业的OceanBase的金融与非金融收入比例已经从去年的65%：35%变为今年的五五开，越来越多非金融客户开始使用OceanBase；客户结构也从一开始的大客户占比35%：65%降至目前的三七开，越来越多的中小企业客户也开始使用OceanBase。“这些数据都表明，OceanBase已经从只适用于当年支付宝场景的金融级分布数据库走向了面向云时代的全场景通用数据库。”</p><p>&nbsp;</p><p>随着客户数量的增长，<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651116827&amp;idx=1&amp;sn=f5f363c5766b46a4735d239f1fda9e50&amp;chksm=bdb92d488acea45e9cef0c540e0e2f85a2ba39db9f025059d87cf9cae8eb1c6f18570742bdd7&amp;scene=27#wechat_redirect\">数据库</a>\"要应对来自于不同客户的挑战，在公有云行业有越来越多的中小企业对于整个运营成本非常敏感。于是OceanBase团队意识到，数据库等产品不仅要解决客户眼前的问题，更要解决长期潜在的问题，于是便有了4.0“小鱼”版本的诞生。</p><p>&nbsp;</p><p>发布会现场，杨冰还展示了4.0版本能在全球最小的电脑（树莓派）上运行，即使是普通的个人电脑也可以流畅运行单机分布式一体化数据库，让<a href=\"https://archsummit.infoq.cn/2022/beijing/track/1320\">分布式数据库</a>\"触手可及。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f0/f086362672b229c5bdaf9a6563048851.png\" /></p><p></p><p>此外，4.0版本在单机部署模式下的性能也较之前版本有了很大提升。该版本上可实现分布式部署的完整功能，包括Oracle/MySQL兼容性、TP事务处理能力、AP并行分析查询能力、租户资源隔离等。</p><p>&nbsp;</p><p>OceanBase 4.0<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651024527&amp;idx=2&amp;sn=7e5422dc08347aa22336e94842de5c2f&amp;chksm=bdbe94dc8ac91dcae4b1ecdba044e8cf15e803ee0481424ad9eae5369b98a1e3abb6c237dcac&amp;scene=27#wechat_redirect\"> MySQL</a>\"兼容能力全开源，这也意味着OceanBase社区版将享受企业版同等性能，企业可通过社区版免费享受到OceanBase企业版的全部功能，进一步降低了开发者的应用门槛。</p><p>&nbsp;</p><p>值得一提的是，杨冰还介绍，4.0版本能够把故障恢复时间（RTO），从30秒提升到8秒以内。当故障发生时，数据丢失率（RPO）为零。</p><p></p><h2>推出“珊瑚计划”， 建立以合作伙伴为中心的商业模式</h2><p></p><p>&nbsp;</p><p>杨冰表示，在生态领域，OceanBase要打造一个以合作伙伴为中心的商业模式，随着产品成熟和不断标准化，OceanBase团队将会不断提高在商业合作过程中伙伴对客签约的比例并且不断持续向生态伙伴提供更加完善更为专业的支撑体系。</p><p>&nbsp;</p><p>针对区域合作伙伴，OceanBase将推出“珊瑚计划”：未来3年，面向全国重点省会城市，培养60家核心经销商。通过政策支持，帮助核心经销商成长，三年实现合作伙伴收入份额占总销售份额60%以上。</p><p>&nbsp;</p><p>OceanBase 与伙伴的关系就像海洋里的鱼和珊瑚一样互为生态，共生共荣，希望打造生生不息的数据库的产业生态。</p><p>&nbsp;</p><p>杨冰表示，“OceanBase希望把这次的创新和一些突破分享给更多的伙伴，让更多用户能尝试和使用，我们也希望继续构建一个真正开放平等的开源生态，和大家一起把中国的开源数据库做大做强。最后，我想说，经过这么多年的发展和积累，中国的基础软件已经具备了有能力进行根创新的阶段，我们必须要有信心，同时要有战略定力，只要坚持长期的投入，<a href=\"https://archsummit.infoq.cn/2022/beijing/track/1197\">中国数据库</a>\"的未来一定会迎来大爆发。”</p>",
    "publish_time": "2022-08-11 18:03:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "优秀的数字化转型案例是什么样的？｜第二期完整版（下）",
    "url": "https://www.infoq.cn/article/zTp6wj6S4BmAtkRmh0II",
    "summary": "<p>《行知数字中国》第二期，极客邦科技创始人霍太稳对话前阿里巴巴、Oracle高管，现PingCAP 副总裁刘松，从如何善用云和开源的角度出发，探讨在数字化时代，企业该如何定位自身的 SaaS 需求，发掘可以发力的增长点，从而实现业务的突破。</p>\n<p>本视频为第二期完整版的（下）集。</p>",
    "publish_time": "2022-08-11 19:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]