[
  {
    "title": "使用Strimzi将Kafka和Debezium迁移到Kubernetes",
    "url": "https://www.infoq.cn/article/ElNtSM5ISobpMB8fMC0j",
    "summary": "<p>在本系列文章的<a href=\"https://www.infoq.cn/article/cFpvXRLmZzJBGbzAeFu5\">第1部分</a>\"和<a href=\"https://www.infoq.cn/article/WfA0p1XoZCJ6INdyJLyv\">第2部分</a>\"中，我们学习了<a href=\"https://kafka.apache.org/\">Apache Kafka</a>\"、Apache Kafka Streams和<a href=\"https://quarkus.io/\">Quarkus</a>\"之间的集成。我们开发了一个简单的应用程序，向Kafka主题生成事件，并使用<a href=\"https://kafka.apache.org/documentation/streams/\">Kafka Streams</a>\"实时消费和处理它们。</p><p></p><p>在那个例子中，我们模拟了一家电影流媒体公司。我们将电影信息保存在一个Kafka主题中，并在另一个Kafka主题中保存用户停止观看电影时的事件，并捕获影片播放的时间。我们实时对这些事件进行后期处理，计算电影播放超过10分钟的次数。</p><p></p><p>下图是这个应用程序的架构。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/strimzi-the-gitops-way/en/resources/11-1664224713829.jpeg\" /></p><p>然后，在<a href=\"https://www.infoq.cn/article/V4kqPgrQlqqPgWQuMmDP\">第3部分</a>\"中，我们介绍了发件箱模式和Debezium，用于避免在不同系统需要同步相同数据时发生的双写问题。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/strimzi-the-gitops-way/en/resources/12-1664224713829.jpeg\" /></p><p>在前面的三篇文章中，我们已经从开发人员的角度学习了所有这些技术，并最终在开发人员的本地机器上（以开发模式）部署应用程序。</p><p></p><p>在本文中，我们将探讨如何将所有东西部署到生产环境，更具体地说，部署到Kubernetes中。我们将学习：</p><p></p><p>在Kubernetes中安装和管理Apache Kafka集群。容器化Quarkus应用程序。配置一个带有生产参数的Quarkus应用程序。将Debezium Embedded迁移成Debezium Server。</p><p></p><h2>Kubernetes</h2><p></p><p></p><p>Kubernetes是一个开源的容器编配器，是部署微服务的事实上的平台。这些服务既可以在裸金属环境中运行，也可以在云环境中运行。</p><p></p><p>本文使用<a href=\"https://minikube.sigs.k8s.io/docs/\">minikube</a>\"作为Kubernetes集群，但同样的步骤应该适用于任何其他实现。</p><p></p><h4>启动集群</h4><p></p><p>在终端窗口中执行以下命令，在配备了8GB内存和2个vCPU的VirtualBox机器上启动集群。</p><p></p><p><code lang=\"plain\">minikube start -p strimzi --kubernetes-version='v1.22.12' --vm-driver='virtualbox' --memory=8096\n\n  [strimzi] minikube v1.24.0 on Darwin 12.5\n  minikube 1.26.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.26.1\n  To disable this notice, run: 'minikube config set WantUpdateNotification false'\n\n✨  Using the virtualbox driver based on user configuration\n  Starting control plane node strimzi in cluster strimzi\n  Creating virtualbox VM (CPUs=2, Memory=8096MB, Disk=20000MB) ...\n    &gt; kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n    &gt; kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n    &gt; kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n    &gt; kubeadm: 43.74 MiB / 43.74 MiB [-------------] 100.00% 13.98 MiB p/s 3.3s\n    &gt; kubectl: 44.77 MiB / 44.77 MiB [-------------] 100.00% 11.11 MiB p/s 4.2s\n    &gt; kubelet: 115.30 MiB / 115.30 MiB [-----------] 100.00% 20.16 MiB p/s 5.9s\n\n    ▪ Generating certificates and keys ...\n    ▪ Booting up control plane ...\n    ▪ Configuring RBAC rules ...\n    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n  Verifying Kubernetes components...\n  Enabled addons: storage-provisioner, default-storageclass\n\n❗  /usr/local/bin/kubectl is version 1.24.0, which may have incompatibilites with Kubernetes 1.22.12.\n    ▪ Want kubectl v1.22.12? Try 'minikube kubectl -- get pods -A'\n  Done! kubectl is now configured to use \"strimzi\" cluster and \"default\" namespace by default\n</code></p><p></p><p>在终端窗口执行下面的命令检查Kubernetes集群是否正常运行。</p><p></p><p><code lang=\"plain\">kubectl get nodes\n\nNAME      STATUS   ROLES                  AGE    VERSION\nstrimzi   Ready    control-plane,master   3m4s   v1.22.12\n\nkubectl get pods\nNo resources found in default namespace.\n</code></p><p></p><h2>Apache Kafka</h2><p></p><p></p><p>在之前的文章中，我们通过Quarkus的开发模式来启动运行应用程序所需的外部依赖项（Kafka集群和MySQL数据库）。从开发的角度来看，开发模式非常棒，但在部署到生产环境时，你会发现这些东西管理起来更加复杂。第一个障碍可能是在Kubernetes中安装和配置Kafka集群。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/strimzi-the-gitops-way/en/resources/13-1664224713829.jpeg\" /></p><p>你可能想知道以下这些问题的答案：</p><p></p><p>Kafka组件（Kafka、Zookeeper等）需要使用哪个容器镜像?如何在Kubernetes中轻松部署所有这些组件？如何在Kubernetes中创建用户、主题或HA？安全性如何？你可以尝试手动完成所有这些事情，例如编写很长的YAML文件和使用Kafka CI工具配置Kafka组件。然而，还有另一种Kubernetes原生的、完全自动化和可复制的（非常适合CI/CD）方法，就是使用Strimzi。</p><p></p><h4>Strimzi</h4><p></p><p><a href=\"https://strimzi.io/\">Strimzi</a>\"是一个<a href=\"https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator\">Kubernetes Operator</a>\"，通过控制器来创建、配置和保护Kafka集群，就像其他Kubernetes资源（如Pod、Deployment、ConfigMap等）一样。</p><p></p><p>Strimzi项目包含三个Operator——一个用于管理Kafka集群，一个用于管理主题，一个用于用户管理。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/strimzi-the-gitops-way/en/resources/14-1664224713829.jpeg\" /></p><p>在Kubernetes集群中安装了Strimzi Operator之后，你只需要使用下面的YAML文件就可以启动并运行一个Kafka集群，其中包含了一个Kafka副本和三个使用临时存储（没有挂载持久卷）的ZooKeeper副本。</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n name: my-cluster\nspec:\n kafka:\n   version: 3.2.0\n   replicas: 1\n   listeners:\n     - name: plain\n       port: 9092\n       type: internal\n       tls: false\n     - name: tls\n       port: 9093\n       type: internal\n       tls: true\n   config:\n     offsets.topic.replication.factor: 1\n     transaction.state.log.replication.factor: 1\n     transaction.state.log.min.isr: 1\n     default.replication.factor: 1\n     min.insync.replicas: 1\n     inter.broker.protocol.version: \"3.2\"\n   storage:\n     type: ephemeral\n zookeeper:\n   replicas: 3\n   storage:\n     type: ephemeral\n entityOperator:\n   topicOperator: {}\n   userOperator: {}\n</code></p><p></p><p>接下来，我们将在已经启动的集群中安装Strimzi。</p><p></p><h4>安装Strimzi</h4><p></p><p>首先是创建一个命名空间来安装Strimzi Operator。在本例中，我们使用了命名空间kafka。在终端窗口中执行如下命令：</p><p></p><p><code lang=\"plain\">kubectl create namespace kafka\nnamespace/kafka created\n</code></p><p></p><p>接下来，我们应用Strimzi安装文件，其中包括用于声明式管理Kafka集群、Kafka主题和用户的CRD（CustomerResourceDefinition）。</p><p></p><p><code lang=\"plain\">kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka\n</code></p><p></p><p>运行下面的命令验证Operator是否安装正确。</p><p></p><p><code lang=\"plain\">kubectl get pods -n kafka\n\nNAME                                        READY   STATUS    RESTARTS   AGE\nstrimzi-cluster-operator-597d67c7d6-ms987   1/1     Running   0          4m27s\n</code></p><p></p><p>现在，我们开始创建带有movies主题的Kafka集群。我们将在这个主题中保存所有电影的信息，稍后Kafka Streams将消费这个主题，正如我们在本系列文章的<a href=\"https://www.infoq.cn/article/WfA0p1XoZCJ6INdyJLyv\">第2部分</a>\"中所看到的那样。</p><p></p><h4>创建Kafka集群</h4><p></p><p>创建一个新的文件（即kafka.yaml）来安装一个带有一个副本的<a href=\"https://strimzi.io/docs/operators/latest/configuring.html#assembly-config-kafka-str\">Kafka集群</a>\"，不启用TLS，作为内部<a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Kubernetes服务</a>\"。</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n name: my-cluster\nspec:\n kafka:\n   version: 3.2.0\n   replicas: 1\n   listeners:\n     - name: plain\n       port: 9092\n       type: internal\n       tls: false\n   config:\n     offsets.topic.replication.factor: 1\n     transaction.state.log.replication.factor: 1\n     transaction.state.log.min.isr: 1\n     default.replication.factor: 1\n     min.insync.replicas: 1\n     inter.broker.protocol.version: \"3.2\"\n   storage:\n     type: ephemeral\n zookeeper:\n   replicas: 1\n   storage:\n     type: ephemeral\n entityOperator:\n   topicOperator: {}\n   userOperator: {}\n</code></p><p></p><p>然后在终端窗口中使用kubectl命令创建这个资源：</p><p></p><p><code lang=\"plain\">kubectl create -f kafka.yaml -n kafka\nkafka.kafka.strimzi.io/my-cluster created\n</code></p><p></p><p>此时，Strimzi开始在默认命名空间中安装Kafka集群。</p><p></p><p>现在，我们通过获取默认的名称空间Pod来检查集群的创建情况。</p><p></p><p><code lang=\"plain\">kubectl get pods -n kafka\n\nNAME                                          READY   STATUS    \nmy-cluster-entity-operator-755596449b-cw82g   3/3     Running   \nmy-cluster-kafka-0                            1/1     Running \nmy-cluster-zookeeper-0                        1/1     Running\n</code></p><p></p><p>Kafka集群已启动并运行。我们除了可以将Kafka作为Kubernetes资源安装之外，还可以查询和描述它。例如，在终端窗口中执行如下命令：</p><p></p><p><code lang=\"plain\">kubectl get kafka -n kafka\n\nNAME         DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS   READY   WARNINGS\nmy-cluster   1                        1                     True    True\n\n\n\nkubectl describe kafka my-cluster -n kafka\n\nName:         my-cluster\nNamespace:    default\nLabels:       \nAnnotations:  \nAPI Version:  kafka.strimzi.io/v1beta2\nKind:         Kafka\nMetadata:\n  Creation Timestamp:  2022-08-09T10:57:39Z\n…\n</code></p><p></p><p>当然，你也可以像删除其他Kubernetes资源一样删除它。此外，系统还创建了4个Kubernetes服务来访问Kafka集群：</p><p></p><p><code lang=\"plain\">kubectl get services -n kafka\n\nNAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\n               143m\nmy-cluster-kafka-bootstrap    ClusterIP   172.30.77.150           9091/TCP,9092/TCP            21m\nmy-cluster-kafka-brokers      ClusterIP   None                    9090/TCP,9091/TCP,9092/TCP   21m\nmy-cluster-zookeeper-client   ClusterIP   172.30.5.186            2181/TCP                     21m\nmy-cluster-zookeeper-nodes    ClusterIP   None                    2181/TCP,2888/TCP,3888/TCP   21m\n</code></p><p></p><p>应用程序用于访问集群的服务是my-cluster-kafka-bootstrap，它公开了Kafka的9092端口。</p><p></p><p>在进入到应用程序部分之前，我们需要使用另一个YAML文件来创建和配置movies主题。</p><p></p><h4>创建movies主题</h4><p></p><p>Strimzi有一个用于创建和管理主题的Operator。要创建一个新主题，我们需要创建一个<a href=\"https://strimzi.io/docs/operators/latest/configuring.html#using-the-topic-operator-str\">KafkaTopic</a>\"类型的Kubernetes资源文件，在strimzi.io/cluster中指定主题的名称和集群的名称（在我们的例子中是my-cluster）。我们使用下面的内容创建一个名为movies-topic.yaml的新文件。</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n name: movies\n labels:\n   strimzi.io/cluster: my-cluster\nspec:\n partitions: 1\n replicas: 1\n config:\n   retention.ms: 7200000\n   segment.bytes: 1073741824\n</code></p><p></p><p>并应用这个文件：</p><p></p><p><code lang=\"plain\">kubectl apply -f movies-topic.yaml -n kafka\nkafkatopic.kafka.strimzi.io/movies create\n</code></p><p></p><p>和其他Kubernetes资源一样，我们也可以查询和描述它。</p><p></p><p><code lang=\"plain\">kubectl get kafkatopic -n kafka\n\nNAME                                                                                               CLUSTER      PARTITIONS   REPLICATION FACTOR   READY\nconsumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a                                        my-cluster   50           1                    True\nmovies                                                                                             my-cluster   1            1                    True\nstrimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55                                     my-cluster   1            1                    True\nstrimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b   my-cluster   1            1                    True\n</code></p><p></p><p>描述已创建的主题：</p><p></p><p><code lang=\"plain\">kubectl port-forward -n kafka service/my-cluster-kafka-bootstrap 9092:9092\n\nForwarding from 127.0.0.1:9092 -&gt; 9092\nForwarding from [::1]:9092 -&gt; 9092\n</code></p><p></p><p>我们来检查一下创建的主题是否有<a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\">端口转发</a>\"。</p><p></p><p>在终端窗口执行如下命令：</p><p></p><p><code lang=\"plain\">kubectl port-forward -n kafka service/my-cluster-kafka-bootstrap 9092:9092\n\nForwarding from 127.0.0.1:9092 -&gt; 9092\nForwarding from [::1]:9092 -&gt; 9092\n</code></p><p></p><p>打开一个新的终端窗口，使用<a href=\"https://github.com/edenhill/kcat\">kcat</a>\"工具列出Kafka集群的元素。我们可以使用localhost作为主机名，就像在上一步中使用端口转发技巧一样。</p><p></p><p><code lang=\"plain\">kcat -b localhost:9092 -L\n\nMetadata for all topics (from broker -1: localhost:9092/bootstrap):\n 1 brokers:\n  broker 0 at my-cluster-kafka-0.my-cluster-kafka-brokers.default.svc:9092 (controller)\n 4 topics:\n  topic \"movies\" with 1 partitions:\n    partition 0, leader 0, replicas: 0, isrs: 0\n</code></p><p></p><p>最后，我们停止端口转发进程，对项目进行容器化，就像我们在本系列文章的<a href=\"https://www.infoq.cn/article/V4kqPgrQlqqPgWQuMmDP\">第3部分</a>\"中所做的那样，并进行一些相应的配置，以便连接到Kafka集群。</p><p></p><h2>生产者Debezium</h2><p></p><p></p><p>我们在系列文章的<a href=\"https://www.infoq.cn/article/V4kqPgrQlqqPgWQuMmDP\">第3部分</a>\"解决了双写问题，使用Debezium（具体来说是<a href=\"https://github.com/lordofthejars/movie-plays-kafka/blob/main/movie-plays-producer-debezium/src/main/java/org/acme/DebeziumListener.java\">Debezium Embedded</a>\"）修复了这个问题，具体方法是监听来自MySQL服务器的事务日志，并在每次插入新的电影播放信息时生成带有数据的Kafka事件。你可以在本地机器上运行这个示例，使用开发服务启动所需的服务（MySQL和Kafka），并自动配置应用程序来连接它们。</p><p></p><p>现在有点不一样了——服务必须运行在Kubernetes集群中，包括在前面步骤中创建的Kafka集群和MySQL数据库。要让它在Kubernetes中运行，需要做出三个改变。</p><p></p><p>使用新的Kafka和MySQL参数（主机名、端口、用户名和密码）来配置服务。将应用程序装入容器，并推送到容器注册表。创建Kubernetes资源文件，用于部署服务。</p><p></p><h2>配置服务</h2><p></p><p></p><p>首先要配置的是Kafka的主机名和端口，它们指向Strimzi创建的Kubernetes服务。打开src/main/resources/application.properties文件并添加下面的内容：</p><p></p><p><code lang=\"plain\">%prod.kafka.bootstrap.servers=my-cluster-kafka-bootstrap:9092\n</code></p><p></p><p>%prod前缀表示这个属性仅在应用程序以prod模式下运行时使用（而不是在dev或test模式下)。</p><p></p><p>其次时配置插入影片信息的数据库连接。在application.properties文件中添加下面的内容。</p><p></p><p><code lang=\"plain\">quarkus.hibernate-orm.database.generation=drop-and-create\n%prod.quarkus.datasource.username=alex\n%prod.quarkus.datasource.password=alex\n%prod.quarkus.datasource.jdbc.url=jdbc:mysql://mysql:3306/moviesdb\n</code></p><p></p><p>稍后，我们将使用这些参数部署一个MySQL实例。现在，我们假设配置参数是正确的。</p><p></p><h4>容器化</h4><p></p><p>Quarkus为创建容器提供了与<a href=\"https://github.com/GoogleContainerTools/jib\">Jib</a>\"项目的集成，让容器镜像的构建和推送简单得只需要执行一个Maven/Gradle任务。</p><p></p><p>打开pom.xml文件，在dependencies部分添加以下依赖项：</p><p></p><p><code lang=\"plain\">\n     io.quarkus\n     quarkus-container-image-jib\n\n</code></p><p></p><p>添加了<a href=\"https://quarkus.io/guides/container-image\">Jib依赖项</a>\"后，它将在打包时自动将应用程序装入容器。因为<a href=\"https://github.com/GoogleContainerTools/jib\">Jib</a>\"的一些默认配置选项可能不适用于所有情况，所以你可以在src/main/resources/application.properties中覆盖它们。对于本例，我们将覆盖生成的容器镜像的group和容器注册中心的主机。</p><p></p><p>打开application.properties文件，并添加下面的内容：</p><p></p><p><code lang=\"plain\"># Substitue the value with your account name\nquarkus.container-image.group=lordofthejars\n \n# Defaults to Docker.io, overriden to Quay.\nquarkus.container-image.registry=quay.io\n</code></p><p></p><p>你需要设置容器注册表的凭据，以便向注册表推送容器。你可以在执行构建之前运行docker的login命令。Maven将从那里读取凭据，或者你可以使用quarkus.container-image.username和quarkus.container-image.password属性。</p><p></p><p>在项目的根目录下运行下面的命令来构建应用程序，它将构建出一个容器并将其推到指定的容器注册表中。</p><p></p><p><code lang=\"plain\">./mvnw clean package -DskipTests -Dquarkus.container-image.push=true\n\n[INFO] Scanning for projects...\n[INFO]\n[INFO] ---------------&lt; org.acme:movie-plays-producer-debezium &gt;---------------\n[INFO] Building movie-plays-producer-debezium 1.0.0-SNAPSHOT\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ movie-plays-producer-debezium ---\n[INFO] Deleting /Users/asotobu/git/quarkus-integrating-kafka/strimzi/movie-plays-producer-debezium/target\n…\n[INFO] [io.quarkus.container.image.jib.deployment.JibProcessor] Using base image with digest: sha256:1a2fddacdcda67494168749c7ab49243d06d8fbed34abab90566d81b94f5e1a5\n[INFO] [io.quarkus.container.image.jib.deployment.JibProcessor] Container entrypoint set to [java, -Djava.util.logging.manager=org.jboss.logmanager.LogManager, -jar, quarkus-run.jar]\n[INFO] [io.quarkus.container.image.jib.deployment.JibProcessor] Pushed container image quay.io/lordofthejars/movie-plays-producer-debezium:1.0.0-SNAPSHOT (sha256:73dfe42d53f8d7e3c268dbebc2e5f866596de33b8fcaf82c27bdd414d28bdb8a)\n</code></p><p></p><p>从最后一行日志可以看到，容器被创建，并使用application.properties中指定的账号推送到注册中心。</p><p></p><h4>Kubernetes</h4><p></p><p>在将容器推送到注册表之后，我们准备将服务部署到Kubernetes中。我们可以手动创建Kubernetes资源文件，但没有必要这么做，因为Quarkus为我们提供了一个<a href=\"https://quarkus.io/guides/deploying-to-kubernetes\">Kubernetes扩展</a>\"。</p><p></p><p>打开pom.xml文件，并在dependencies部分添加下面的依赖项。</p><p></p><p><code lang=\"plain\">\n     io.quarkus\n     quarkus-kubernetes\n\n</code></p><p></p><p>每次Maven打包应用程序时都会注册Kubernetes扩展，并生成将应用程序部署到Kubernetes集群的kubernetes.yml文件。你可以通过application.properties来修改生成文件的内容。例如，我们将Kubernetes Service设置为LoadBalancer而不是ClusterIP，并将命名空间设置为kafka。</p><p></p><p>打开application.properties文件并添加下面的内容。</p><p></p><p><code lang=\"plain\">quarkus.kubernetes.service-type=load-balancer\nquarkus.kubernetes.namespace=kafka\n</code></p><p></p><p>修改好以后运行Maven package生成部署文件。</p><p></p><p><code lang=\"plain\">./mvnw clean package -DskipTests\n\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n</code></p><p></p><p>检查生成的文件target/kubernetes/kubernetes.yml：</p><p></p><p><code lang=\"plain\">cat target/kubernetes/kubernetes.yml\n</code></p><p></p><p>输出的内容应该类似于下面这样：</p><p></p><p><code lang=\"plain\">---\napiVersion: v1\nkind: Service\nmetadata:\n …\n name: movie-plays-producer-debezium\nspec:\n ports:\n   - name: http\n     port: 80\n     targetPort: 8080\n selector:\n   app.kubernetes.io/name: movie-plays-producer-debezium\n   app.kubernetes.io/version: 1.0.0-SNAPSHOT\n # Type is LoadBalancer as set in the application.properties file \n type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n …\n name: movie-plays-producer-debezium\nspec:\n replicas: 1\n selector:\n   matchLabels:\n     app.kubernetes.io/name: movie-plays-producer-debezium\n     app.kubernetes.io/version: 1.0.0-SNAPSHOT\n template:\n   metadata:\n     …\n   spec:\n     containers:\n       - env:\n           - name: KUBERNETES_NAMESPACE\n             valueFrom:\n               fieldRef:\n                 fieldPath: metadata.namespace\n         # The image is correctly set automatically\n         image: quay.io/lordofthejars/movie-plays-producer-debezium:1.0.0-SNAPSHOT\n         imagePullPolicy: Always\n         name: movie-plays-producer-debezium\n         ports:\n           - containerPort: 8080\n             name: http\n             protocol: TCP\n</code></p><p></p><p>在本例中，配置参数是硬编码在application.properties中的，但你可以将它们作为环境变量传递进去。要在Kubernetes Deployment对象中设置环境变量，比如覆盖Kafka的配置，可以添加下面的行：</p><p></p><p><code lang=\"plain\">quarkus.kubernetes.env.vars.kafka-bootstrap-servers=my-new-cluster:9092 \n</code></p><p></p><p>生成文件的env部分将包含这个新的环境变量：</p><p></p><p><code lang=\"plain\">containers:\n       - env:\n           - name: KAFKA_BOOTSTRAP_SERVERS\n             value: my-new-cluster:9092\n         image: quay.io/lordofthejars/movie-plays-producer-debezium:1.0.0-SNAPSHOT\n</code></p><p></p><h4>全部放到一起</h4><p></p><p>我们已经使用Strimzi在Kubernetes集群中部署了一个Kafka集群。我们将应用下面的文件（mysql-deployment.yaml）和application.properties中配置的参数部署MySQL实例。</p><p></p><p><code lang=\"plain\">apiVersion: v1\nkind: Service\nmetadata:\n name: mysql\n labels:\n   app: mysql\nspec:\n ports:\n   - port: 3306\n selector:\n   app: mysql\n clusterIP: None\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: mysql\n labels:\n   app: mysql\nspec:\n selector:\n   matchLabels:\n     app: mysql\n strategy:\n   type: Recreate\n template:\n   metadata:\n     labels:\n       app: mysql\n   spec:\n     containers:\n     - image: mysql:8.0.30\n       name: mysql\n       env:\n       - name: MYSQL_ROOT_PASSWORD\n         value: alex\n       - name: MYSQL_DATABASE\n         value: moviesdb\n       - name: MYSQL_USER\n         value: alex\n       - name: MYSQL_PASSWORD\n         value: alex\n       ports:\n       - containerPort: 3306\n         name: mysql\n</code></p><p></p><p>将MySQL实例部署到Kubernetes集群：</p><p></p><p><code lang=\"plain\">kubectl apply -f mysql-deployment.yaml -n kafka\n</code></p><p></p><p>最后要部署的是应用程序本身。我们有两个选择，第一个是直接应用资源：</p><p></p><p><code lang=\"plain\">kubectl apply -f target/kubernetes/kubernetes.yml -n kafka\n</code></p><p></p><p>第二个是将quarkus.kubernetes.deploy标志设置为true来打包应用程序。当这个标志设置为true时，Maven将：</p><p></p><p>创建应用程序JAR文件。构建容器镜像。将容器镜像推送到注册表中。自动应用kubernetes.yml资源文件到已连接的Kubernetes集群。为了验证所有的东西都能正确地运行，我们将发送一个插入新电影信息的请求，并验证在Kafka主题中插入的新事件。</p><p></p><p>在终端窗口中执行以下命令获取访问服务的IP和端口。</p><p></p><p>获取访问服务的IP：</p><p></p><p><code lang=\"plain\">minikube ip -p strimzi\n\n192.168.59.104\n</code></p><p></p><p>获取movie-plays-producer-debezium的公开端口，也就是第二个端口。</p><p></p><p><code lang=\"plain\">kubectl get services -n kafka\n\nmovie-plays-producer-debezium   LoadBalancer   10.100.117.203        80:30306/TCP                 67m\n</code></p><p></p><p>运行curl命令，插入一条新的电影信息记录。</p><p></p><p><code lang=\"plain\">curl -X 'POST' \\\n  'http://192.168.59.104:30306/movie' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"name\": \"Minions: The Rise of Gru\",\n  \"director\": \"Kyle Balda\",\n  \"genre\": \"Animation\"\n}'\n</code></p><p></p><p>检查Quarkus日志，查看数据库运行的SQL语句：</p><p></p><p><code lang=\"java\">kubectl get pods -n kafka\n\nNAME                                             READY   STATUS      RESTARTS   AGE\nmovie-plays-producer-debezium-56f644cb87-5cchk   1/1     Running     0          6m5s\nmy-cluster-entity-operator-755596449b-cw82g      3/3     Running     0          35h\nmy-cluster-kafka-0                               1/1     Running     0          35h\nmy-cluster-zookeeper-0                           1/1     Running     0          35h\n</code></p><p></p><p>打印movie-plays-producer-debezium的日志：</p><p></p><p><code lang=\"java\">kubectl logs movie-plays-producer-debezium-6b9b65bf4-9z524 -n kafka\n\n2022-08-11 07:44:25,658 INFO  [org.acm.MovieResource] (executor-thread-1) New Movie inserted Minions: The Rise of Gru\n:)\nHibernate:\n    select\n        next_val as id_val\n    from\n        hibernate_sequence for update\n\nHibernate:\n    update\n        hibernate_sequence\n    set\n        next_val= ?\n    where\n        next_val=?\nHibernate:\n    insert\n    into\n        Movie\n        (director, genre, name, id)\n    values\n        (?, ?, ?, ?)\nHibernate:\n    insert\n    into\n        OutboxEvent\n        (aggregatetype, aggregateid, type, timestamp, payload, tracingspancontext, id)\n    values\n        (?, ?, ?, ?, ?, ?, ?)\n\n# Debezium reacts to the change\n2022-08-11 07:44:25,867 INFO  [io.deb.con.com.BaseSourceTask] (executor-thread-0) 1 records sent during previous 00:20:44.297, last recorded offset: {transaction_id=null, ts_sec=1660203865, file=binlog.000002, pos=14795, row=1, server_id=1, event=4}\nMovie Created and Reacting\n</code></p><p></p><p>你还可以使用Kafka容器里的Kafka-console-consumer.sh脚本来检查Kafka中的内容。进入容器并运行下面的命令：</p><p></p><p><code lang=\"java\">kubectl exec -ti my-cluster-kafka-0 -n kafka /bin/bash\n\n./bin/kafka-console-consumer.sh --topic movies --from-beginning --bootstrap-server localhost:9092\n{\"id\":1,\"name\":\"Minions: The Rise of Gru\",\"director\":\"Kyle Balda\",\"genre\":\"Animation\"}\n</code></p><p></p><p>要返回本地终端窗口，请按Ctrl+C停止kafka-console-consumer进程，然后执行exit命令。</p><p></p><p>到目前为止，一切都很顺利。我们已经得到了与本系列文章第3部分中相同的应用程序，只是现在它运行在Kubernetes集群中。</p><p></p><p>到目前为止，我们使用的是Debezium Embedded，但其实我们可以使用Debezium Server。</p><p></p><h2>Debezium Server</h2><p></p><p></p><p><a href=\"https://debezium.io/documentation/reference/stable/operations/debezium-server.html\">Debezium Server</a>\"是一个可配置的、使用就绪的应用程序，它将事件从源数据库流到消息传递系统中，如Kafka。它可以被注册成一个<a href=\"https://kafka.apache.org/documentation/#connect\">Kafka Connect组件</a>\"，作为源连接器。</p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/strimzi-the-gitops-way/en/resources/15-1664224713829.jpeg\" /></p><p>虽然我们不能在所有的场景中都使用Debezium Server，但在我看来，使用这种方法有两个大的优点：</p><p></p><p>你可以获得Kafka连接器的所有好处（容错、可扩展、可重用等）。因为它是一个外部组件，所以不需要更改应用程序代码，也不需要Debezium Embedded相关的代码或依赖项。因此，任何应用程序都可以在不做出修改或重新部署的情况下开始使用Debezium。接下来，我们来看看如何从Debezium Embedded迁移到Debezium Server。</p><p></p><h4>移除Debezium Embedded</h4><p></p><p>首先要做的是删除Debezium Embedded相关的依赖项。</p><p></p><p>打开pom.xml文件，删除以下依赖项：</p><p></p><p><code lang=\"java\">\n     io.debezium\n     debezium-ddl-parser\n\n\n     io.debezium\n     debezium-embedded\n\n\n     io.debezium\n     debezium-connector-mysql\n\n</code></p><p></p><p>下一步是删除所有与Debezium Embedded配置和监听器相关的代码。删除这些类文件——DebeziumConfiguration.java、DebeziumListener.java和MySqlJdbcParser.java。</p><p></p><p>因为所有与Kafka的交互都是通过Kafka Connect组件进行的，不需要Kafka代码，所以最后一步是从pom.xml中移除以下依赖项：</p><p></p><p><code lang=\"go\">\n     io.quarkus\n     quarkus-smallrye-reactive-messaging-kafka\n\n</code></p><p></p><p>application.properties文件中的这一行不再需要：</p><p></p><p><code lang=\"java\">%prod.kafka.bootstrap.servers=my-cluster-kafka-bootstrap:9092\n</code></p><p></p><p>项目中已经没有了Kafka或Debezium Embedded依赖项。创建一个包含这些最新变更的容器镜像。</p><p></p><p>在终端窗口执行以下命令，删除之前的部署：</p><p></p><p><code lang=\"go\">kubectl delete deployment movie-plays-producer-debezium\nkubectl delete service movie-plays-producer-debezium\n</code></p><p></p><p>要保留带有Debezium Debezium的容器镜像，请将artifactId更改为movie-plays-producer-debezium-server。</p><p></p><p>然后将不带Debezium代码的新版本部署到Kubernetes集群中，如下所示：</p><p></p><p><code lang=\"plain\">./mvnw clean package -DskipTests -Dquarkus.kubernetes.deploy=true\n</code></p><p></p><p>运行以下命令验证新部署的服务：</p><p></p><p><code lang=\"java\">kubectl get pods -n kafkaa\n\nNAME                                                    READY   STATUS    RESTARTS   AGE\nmovie-plays-producer-debezium-server-59db564b74-vhdmf   1/1     Running   0          73m\n</code></p><p></p><h4>部署Debezium Kafka Connect</h4><p></p><p>首先，部署一个Kafka Connect组件与所需的MySQL连接器插件。你可以认为它跟我们在<a href=\"https://github.com/lordofthejars/movie-plays-kafka/blob/main/strimzi/movie-plays-producer-debezium/src/main/java/org/acme/DebeziumListener.java\">DebeziumListener</a>\"类中实现的逻辑差不多，只是被作为一个Kafka Connect元素，可以在项目中重用。我们必须为Kafka Connect和连接器插件创建一个容器镜像，因为Debezium没有为各种可能的Kafka与数据的组合提供“官方”镜像。对于本例，我们使用Kafka 3.2.0的MySQL连接器创建一个容器镜像。</p><p></p><p>本文中MySQL连接器的容器镜像可以在<a href=\"http://quay.io/lordofthejars/debezium-connector-mysql:1.9.4\">quay.io/lordofthejars/debezium-connector-mysql:1.9.4</a>\"找到，如果你对它的构建过程感到好奇，可以查看位于这个<a href=\"https://github.com/lordofthejars/movie-plays-kafka/blob/main/strimzi/Dockerfile\">GitHub</a>\"存储库中的Dockerfile文件。</p><p></p><p>为了部署Debezium Kafka Connect，我们将使用Strimzi提供的<a href=\"https://strimzi.io/docs/operators/latest/deploying.html#deploying-kafka-connect-str\">KafkaConnect</a>\"，因为它简化了整个过程。在这个Kubernetes资源文件中，我们指定了Kafka版本、Kafka集群的位置（my-cluster-kafka-bootstrap:9092）、容器镜像（quay.io/lordofthejars/debezin-connector-mysql:1.9.4），以及一些特定的配置参数。</p><p></p><p>创建一个名为debezium-kafka-connect.yaml的文件，内容如下：</p><p></p><p><code lang=\"java\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\n name: debezium-connect-cluster\n annotations:\n   strimzi.io/use-connector-resources: \"true\"\nspec:\n version: 3.2.0\n image: quay.io/lordofthejars/debezium-connector-mysql:1.9.4\n replicas: 1\n bootstrapServers: my-cluster-kafka-bootstrap:9092\n config:\n   group.id: connect-cluster\n   key.converter: org.apache.kafka.connect.json.JsonConverter\n   value.converter: org.apache.kafka.connect.json.JsonConverter\n   key.converter.schemas.enable: false\n   value.converter.schemas.enable: false\n   offset.storage.topic: connect-offsets\n   offset.storage.replication.factor: 1\n   config.storage.topic: connect-configs\n   config.storage.replication.factor: 1\n   status.storage.topic: connect-status\n   status.storage.replication.factor: 1\n</code></p><p></p><p>然后在终端窗口中应用这个资源：</p><p></p><p><code lang=\"java\">kubectl apply -f debezium-kafka-connect.yaml -n kafka\n</code></p><p></p><p>并通过运行以下命令验证它是否被正确部署：</p><p></p><p><code lang=\"java\">kubectl get pods -n kafka\n\ndebezium-connect-cluster-connect-546c8695c-lszn7        1/1     Running   0          91m\n</code></p><p></p><p>请记住，这个过程可能需要几分钟的准备时间。</p><p></p><p>Kafka Connect组件现在连接到了Kafka集群，最后一步是通过配置让它监听MySQL实例的数据变更。</p><p></p><p>为此，我们将使用Strimzi提供的KafkaConnector。这有点类似于我们在DebeziumConfiguration类中所做的那样，提供database.hostname或table.include.list之类的参数。此外，我们还要将strimzi.io/cluster的值设置为上一个YAML文件中指定的KafkaConnect名称（debezum-connect-cluster）。</p><p></p><p>创建一个名为debezium-kafka-connector.yaml的文件，内容如下：</p><p></p><p><code lang=\"java\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnector\nmetadata:\n name: debezium-connector-mysql\n labels:\n   strimzi.io/cluster: debezium-connect-cluster\nspec:\n class: io.debezium.connector.mysql.MySqlConnector\n tasksMax: 1\n config:\n   tasks.max: 1\n   database.hostname: mysql\n   database.port: 3306\n   database.user: root\n   database.password: alex\n   database.server.id: 184054\n   database.server.name: mysql\n   database.include.list: moviesdb\n   database.allowPublicKeyRetrieval: true\n   table.include.list: moviesdb.OutboxEvent\n   database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092\n   database.history.kafka.topic: schema-changes.movies\n</code></p><p></p><p>通过应用资源来配置Debezium Connector：</p><p></p><p><code lang=\"plain\">kubectl apply -f debezium-kafka-connector.yaml -n kafka\n</code></p><p></p><p>为了验证一切工作正常，我们添加一条新的电影数据记录，并验证将新记录插入数据库时Kafka主题中会产生一个新事件。</p><p></p><p>获取新服务的端口，IP仍然是相同的：</p><p></p><p><code lang=\"java\">kubectl get services -n kafka\n\nmovie-plays-producer-debezium-server   LoadBalancer   10.100.117.203        80:30307/TCP                 67m\n\ncurl -X 'POST' \\\n  'http://192.168.59.104:30307/movie' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"name\": \"Minions: The Rise of Gru\",\n  \"director\": \"Kyle Balda\",\n  \"genre\": \"Animation\"\n}'\n</code></p><p></p><p>使用kafka-console-consumer.sh脚本验证插入的数据：</p><p></p><p><code lang=\"java\">kubectl exec -ti my-cluster-kafka-0 -n kafka /bin/bash\n</code></p><p></p><p>然后在容器中运行脚本。注意，Debezium连接器将事件发送到一个Kafka主题，名称是这样的..，在这个示例中是mysql.moviesdb.OutboxEvent。<p></p><p></p><p><code lang=\"java\">./bin/kafka-console-consumer.sh --topic mysql.moviesdb.OutboxEvent --from-beginning --bootstrap-server localhost:9092\n\n{\"before\":null,\"after\":{\"id\":\"Yxk0o5WwTvi0+nwBr2Y36wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\"aggregatetype\":\"Movie\",\"aggregateid\":\"5\",\"type\":\"MovieCreated\",\"timestamp\":1660253420864918,\"payload\":\"{\\\"id\\\":5,\\\"name\\\":\\\"Minions: The Rise of Gru\\\",\\\"director\\\":\\\"Kyle Balda\\\",\\\"genre\\\":\\\"Animation\\\"}\",\"tracingspancontext\":null},\"source\":{\"version\":\"1.9.4.Final\",\"connector\":\"mysql\",\"name\":\"mysql\",\"ts_ms\":1660253420000,\"snapshot\":\"false\",\"db\":\"moviesdb\",\"sequence\":null,\"table\":\"OutboxEvent\",\"server_id\":1,\"gtid\":null,\"file\":\"binlog.000002\",\"pos\":8788,\"row\":0,\"thread\":41,\"query\":null},\"op\":\"c\",\"ts_ms\":1660253420878,\"transaction\":null}\n</code></p><p></p><p>before字段是空的，因为是插入操作，所以没有前值，但是在after字段中有电影信息数据。</p><p></p><h2>结论</h2><p></p><p></p><p>我们已经将应用程序从本地迁移到了Kubernetes集群中。</p><p></p><p>Strimzi为我们提供了在Kubernetes中部署和管理Apache Kafka集群的一个关键元素。我们可以使用Kubernetes资源文件安装和管理集群，采用GitOps的方式来管理Kafka。</p><p></p><p>Debezium Embedded适用于一些场景，比如在检测数据变更时使用的临时逻辑。不过，在其他项目中（特别是在遗留项目或需要高可伸缩性和容错性的项目），Debezium Server可能更合适。</p><p></p><p>有了Strimzi、Jib和Kubernetes Quarkus扩展，从本地转移到Kubernetes集群应该并不难。</p><p></p><p>本文的源代码可以在<a href=\"https://github.com/lordofthejars/movie-plays-kafka/tree/main/strimzi\">GitHub</a>\"上找到。</p><p></p><h2>作者简介</h2><p></p><p></p><p>Alex Soto是Red Hat的开发者体验总监。他对Java和软件自动化领域充满热情，并相信开源软件模型。Soto是《Testing Java Microservices》（Manning出版）和《Quarkus Cookbook》（O'Reilly出版）的合著者，也是多个开源项目的贡献者。自2017年以来，他获得Java Champion的称号，也是Salle URL大学的国际演讲师和教师。你可以在Twitter上关注他（Alex Soto），继续关注Kubernetes和Java世界的动态。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/strimzi-the-gitops-way/\">https://www.infoq.com/articles/strimzi-the-gitops-way/</a>\"</p><p></p><p>相关阅读：</p><p>本系列第一部分：<a href=\"https://www.infoq.cn/article/cFpvXRLmZzJBGbzAeFu5\">使用 Apache Kafka 实现 Quarkus 的反应式消息</a>\"</p><p>本系列第二部分：<a href=\"https://www.infoq.cn/article/WfA0p1XoZCJ6INdyJLyv\">Kafka Streams 与 Quarkus：实时处理事件</a>\"</p><p>本系列第三部分：<a href=\"https://www.infoq.cn/article/V4kqPgrQlqqPgWQuMmDP\">Debezium 和 Quarkus：通过 CDC 模式来避免双重写入</a>\"</p><table></table></p>",
    "publish_time": "2022-10-12 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "现代化工具链在大规模 C++ 项目中的技术实践",
    "url": "https://www.infoq.cn/article/KsctTt5cIpCCl5T2SmtJ",
    "summary": "<p></p><blockquote><a href=\"https://www.infoq.cn/article/Y9KJX5zaEXov90wK68JP\">C++ 语言</a>\"与<a href=\"https://xie.infoq.cn/article/f5c1ccfd528e8a169edbc3711\">编译器</a>\"一直都在持续演进，出现了许多令人振奋的新特性，同时还有许多新特性在孵化阶。除此之外，还有许多小更改以提高运行效率与编程效率。本文整理自全球 C++ 及系统软件技术大会上的精彩分享，作者将带我们了解&nbsp;C++ 项目的实践工作等具体内容。</blockquote><p></p><p></p><h2>介绍</h2><p></p><p></p><p>C++ 是一门有着长久历史并依然持续活跃的语言。C++ 最新标准已经到了 <a href=\"https://xie.infoq.cn/article/bb41da0e3eed0c7f42571c3d8\">C++23</a>\"。Clang/LLVM、GCC 与 MSVC 等三大编译器都保持着非常频繁的更新。除此之外的各个相关生态也都保持着持续更新与跟进。但遗憾的是，目前看到积极更近 C++新标准与 C++新工具链的都主要以国外项目为主。国内虽然对 C++ 新标准也非常关注，但大多以爱好者个人为主，缺乏真实项目的跟进与实践。</p><p></p><p>本文以现代化工具链作为线索，介绍我们实际工作中的大型 C++ 项目中现代化工具链的实践以及结果。</p><p>对于 C++ 项目，特别是大型的 C++项目而言，常常会有以下几个特点（或痛点）：</p><p></p><p>项目高度自治 – 自主决定编译器版本、语言标准高度业务导向 – 少关注、不关注编译器和语言标准先发劣势 – 丧失应用新技术、新特性的能力沉疴难起 – 编译器版本、语言标准、库依赖被锁死</p><p></p><p>许多 C++ 项目都是高度自治且业务导向的，这导致一个公司内部的 C++ 项目的编译器版本和语言标准五花八门，想统一非常困难。同时由于日常开发主要更关心业务，时间一长背上了技术债，再想用新标准与新工具链的成本就更高了。一来二去，编译器、语言标准与库依赖就被锁死了。</p><p></p><p>同时对于业务来说，切换编译器也会有很多问题与挑战：</p><p></p><p>修复更严格编译器检查的问题修复不同编译器行为差异的问题修复语言标准、编译器行为改变的问题 – 完善测试二进制依赖、ABI兼容问题 – 全源码编译/服务化性能压测、调优</p><p></p><p>这里的许多问题哪怕对于有许多年经验的 C++工程师而言可能都算是难题，因为这些问题其实本质上是比语言层更低一层的问题，属于工具链级别的问题。所以大家觉得棘手是很正常的，这个时候就需要专业的编译器团队了。</p><p></p><p>在我们的工作中，少数编译器造成的程序行为变化问题需要完善的测试集，极少数编译器切换造成的问题在产线上暴露出来 – 本质是业务/库代码的 bug，绝大多数问题在构建、运行、压测阶段暴露并得到修复。</p><p></p><p>这里我们简单介绍下我们在实际工作中遇到的案例：</p><p></p><p>业务1（规模5M）</p><p>业务本身10+仓库；三方依赖50+，其中大部分源代码依赖，部分二进制依赖。二进制依赖、ABI兼容问题 – 0.5人月；编译器切换、CI、CD – 1.5人月；性能分析调优 – 1人月。</p><p></p><p>业务2（规模7M）</p><p>二方/三方依赖 30+，二进制依赖。编译器切换改造 – 2 人月；性能压测调优 – 1 人月。</p><p></p><p>业务3（规模3M）</p><p>二方/三方依赖 100+，多为二进制依赖。二进制依赖、ABI 兼容问题 – 预估 2 人年。</p><p></p><p>在切换工具链之后，用户们能得到什么呢？</p><p>更短的编译时间更好的运行时性能更好的编译、静态、运行时检查更多优化技术 – ThinLTO、AutoFDO、Bolt 等更新的语言特性支持 – C++20 协程、C++20 Module 等持续性更新升级 – 良性循环</p><p></p><p>其中更短的编译时间本身就是 clang 的一个特性，从 gcc 切换到 clang 就会得到很不错的编译加速。同时运行时性能也一直是编译器的目标。而各种各样的静态与运行时检查也是编译器/工具链开发的一个长期主线。另外更新的工具链也会带来更多的优化技术与语言特性支持，这里我们后面会重点介绍。最后是我们可以得到一个长期持续性更新升级的良性循环，这一点也是非常重要和有价值的。</p><p></p><h2>优化技术简介</h2><p></p><p></p><h3>ThinLTO</h3><p></p><p></p><p>传统的编译流程如下图所示</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/49a0a0844ea30120bc669b0d58eb8ab2.png\" /></p><p></p><p>编译器在编译&nbsp;*.c&nbsp;文件时，只能通过&nbsp;*.c&nbsp;及其包含的文件中的信息做优化。</p><p></p><p>LTO （Linking Time Optimization）技术是在链接时使用程序中所有信息进行优化的技术。但 LTO 会将所有&nbsp;*.o&nbsp;文件加载到内存中，消耗非常多的资源。同时 LTO 串行化部分比较多。编译时间很长。落地对环境、技术要求比较高，目前只在 suse 等传统 Linux 厂商中得到应用。</p><p></p><p>为了解决这个问题，LLVM 实现了 ThinLTO 以降低 LTO 的开销。</p><p></p><p>GCC WHOPR 的整体架构如图所示。思路是在编译阶段为每个编译单元生成 Summary 信息，之后再根据 Summary 信息对每个编译单元进行优化。</p><p><img src=\"https://static001.geekbang.org/infoq/47/472f5387eac79dab985b7d1c891a3586.png\" /></p><p></p><p>ThinLTO 技术的整体架构如上图所示。都是在编译阶段为每个&nbsp;*.o&nbsp;文件生成 Summary 信息，之后在 thin link 阶段根据 Summary 信息对每个&nbsp;*.o&nbsp;文件进行优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6dcf4ec3f426527b9f01d7a15bc768a7.png\" /></p><p></p><p>使用 GCC LTO 的原因是 GCC 的 LTO 实现相对比较成熟。</p><p></p><p>从图上可以看出，在性能收益上 ThinLTO 与 &nbsp;LTO 的差距并不大。而 ThinLTO 与 LTO 相比最大的优势是占用的资源极小：</p><p><img src=\"https://static001.geekbang.org/infoq/84/8434fb161d7b6a7820dcb6f9026092ec.png\" /></p><p></p><p>如图为使用 LLVM ThinLTO、LLVM LTO 以及 GCC LTO 链接 Chromium 时的内存消耗走势图。</p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff6e9f042e2524c435e24b14807bb301.png\" /></p><p></p><p>所以使用 ThinLTO 可以使我们的业务在日常开发中以很小的代价拿到很大的提升。同时开启 ThinLTO 的难度很低，基本只要可以启用 clang 就可以使能 ThinLTO。在我们的实践中，一般开启 ThinLTO 可以拿到 10% 的性能提升。</p><p></p><h3>AutoFDO</h3><p></p><p></p><p>AutoFDO 是一个简化 FDO 的使用过程的系统。AutoFDO 会从生产环境收集反馈信息（perf 数据），然后将其应用在编译时。反馈信息是在生产环境的机器上使用 perf 工具对相应硬件事件进行采样得到的。总体来说，一次完整的 AutoFDO 过程如下图可分为 4 步：</p><p><img src=\"https://static001.geekbang.org/infoq/b7/b759f72f9bbfca37e9cd37dba7cb2c07.png\" /></p><p></p><p>1. 将编译好的 binary 部署到生产环境或者测试环境， 在正常工作的情况下使用 perf 对当前进程做周期性的采集。</p><p>2. 将 perf 数据转化成 llvm 可以识别的格式，并将其保存到数据库中。</p><p>3. 当用户再次编译的时候，数据库会将亲近性最强的profile文件返回给编译器并参与到当前构建中。</p><p>4. 将编译好的二进制进行归档和发布。</p><p></p><p>对于业务而言，AutoFDO 的接入有同步和异步两种接入方式：</p><p></p><p>同步接入：首先编译一个 AutoFDO 不参与的二进制版本。在 benchmark 环境下运行当前二进制并使用perf采集数据。使用 AutoFDO 再次构建一个二进制版本，此二进制为最终发布版本。异步接入：在客户线上机器进行周期性采集，将采集数据进行合并和保存。构建新版本的时候将对应的数据文件下载， 并参与当前版本的编译。</p><p>在实际中开启 AutoFDO 可以拿到 2%～5% 的性能提升。</p><p></p><h3>Bolt</h3><p></p><p></p><p>Bolt 基于 LLVM 框架的二进制 POST-LINK 优化技术，可以在 PGO/基础进一步优化。</p><p></p><p>Bolt 应用于其数据中心负载处理，即使数据中心已进行了 PGO(AutoFDO)和 LTO 优化后，BOLT 仍然能够提升其性能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/28/28b91f2f276b42ba9a56a043d3f148eb.png\" /></p><p></p><p>1. Function Discovery：通过 ELF 符号表查找所有函数名字与地址。</p><p>2. Read debug info：如果二进制编译时带有 Debug 信息，读取 Debug 信息。</p><p>3. Read Profile data：读取 Profile 数据，用于驱动 CFG 上优化。</p><p>4. Disassembly：基于LLVM将机器码翻译成保存在内存里的汇编指令。</p><p>5. CFG Construction：依据汇编指令构建控制流图（Control-Flow graph）。</p><p>6. Optimization pipeline：经过上述操作，汇编指令内部表示形式均含有Profile信息，就可以进行一系列的操作优化:</p><p>BasicBlock ReorderingFunction Reordering...</p><p>7. Emit and Link Functions：发射优化后代码，重定向函数地址；</p><p>8. Rewrite binary file：重写二进制文件。</p><p></p><p>Bolt 的接入类似 AutoFDO，也需要先收集到 Perf 数据同时使用该数据重新编译。在我们的实践中性能可以提升 8%。</p><p></p><h2>语言特性</h2><p></p><p></p><p>这里我们简单介绍下两个 C++ 语言的新特性 Coroutines &nbsp;与 Modules 来展示更新到现代化工具链后可以使用的 C++ 新特性。</p><p></p><h3>Coroutines</h3><p></p><p></p><p>首先可以先简单介绍一下&nbsp;Coroutines：</p><p>协程是一个可挂起的函数。支持以同步方式写异步代码。C++20 协程是无栈协程。在语义层面不保存调用上下文信息。对比有栈协程两个数量级的切换效率提升。更好的执行 &amp; 切换效率。对比 Callback更简洁的编程模式，避免 Callback hell。</p><p>接下来我们以一个简单的例子为例，介绍协程是如何支持以同步方式写异步代码。首先我们先看看同步代码的案例：</p><p></p><p><code lang=\"text\">uint64_t ReadSync(std::vector Inputs) {\n    uint64_t read_size = 0;\n    for (auto &amp;&amp;Input : Inputs)\n      read_size += ReadImplSync(Input);\n    return read_size;\n}</code></p><p>这是一个统计多个文件体积的同步代码，应该是非常简单。</p><p></p><p>接下来我们再看下对应的异步写法：</p><p><code lang=\"text\">template \nfuture do_for_each(Range, Lambda);                    // We need introduce another API.\nfuture ReadAsync(vector Inputs) {\n    auto read_size = std::make_shared(0);        // We need introduce shared_ptr.\n    return do_for_each(Inputs,                                           // Otherwise read_size would be\n                 [read_size] (auto &amp;&amp;Input){            // released after ReadAsync ends.\n                                    return ReadImplAsync(Input).then([read_size](auto &amp;&amp;size){\n                                             *read_size += size;\n                                             return make_ready_future();\n                                       });\n                                })\n      .then([read_size] { return make_ready_future(*read_size); });\n}</code></p><p></p><p>肉眼可见地，异步写法麻烦了非常多。同时这里还使用到了&nbsp;std::shared_ptr。但&nbsp;std::shared_ptr&nbsp;会有额外的开销。如果用户不想要这个开销的话需要自己实现一个非线程安全的&nbsp;shared_ptr，还是比较麻烦的。</p><p></p><p>最后再让我们来看下协程版的代码：</p><p></p><p><code lang=\"text\">Lazy ReadCoro(std::vector Inputs) {\n    uint64_t read_size = 0;\n    for (auto &amp;&amp;Input : Inputs)\n        read_size += co_await ReadImplCoro(Input);\n    co_return read_size;\n}</code></p><p></p><p>可以看到这个版本的代码与同步代码是非常像的，但这份代码本质上其实是异步代码的。所以我们说：</p><p>协程可以让我们用同步方式写异步代码；兼具开发效率和运行效率。</p><p></p><p>接下来来简单介绍下 C++20 协程的实现：</p><p></p><p>C++20 协程是无栈协程，需要编译器介入才能实现。判定协程并搜索相关组件。（Frontend Semantic Analysis）生成代码。（Frontend Code Generation）生成、优化、维护协程桢。（Middle-end）C++20 协程只设计了基本语法，并没有加入协程库。C++20 协程的目标用户是协程库作者。其他用户应通过协程库使用协程。</p><p></p><p>同时我们在 GCC 和 Clang 中做了以下工作：</p><p></p><p>GCC与社区合作进行协程的支持。GCC-10 是第一个支持 C++ 协程特性的 GCC 编译器。仅支持，无优化。Clang/LLVM与 Clang/LLVM 社区合作完善 C++ 协程。改善&amp;优化：对称变换、协程逃逸分析和CoroElide优化，协程帧优化（Frame reduction），完善协程调试能力、尾调用优化、Coro Return Value Optimization等。在 Clang/LLVM14 中，coroutine 移出了 experimental namespace。Maintaining</p><p></p><p>最后我们还实现并开源了一个经过双 11 验证的协程库 async_simple：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/2097c206f96f6fab1d6971ba1b795f6d.png\" /></p><p>async_simple设计借鉴了 folly 库协程模块。轻量级。包含有栈协程、无栈协程以及 Future/Promise 等异步组件。从真实需求出发。与调度器解藕，用户可以选择合适自己的调度器。经受了工业级 Workload 的考验。开源于：https://github.com/alibaba/async_simple</p><p></p><p>最后我们来看下我们应用协程后的效果：</p><p>业务1（1M Loc、35w core）原先为同步逻辑协程化后 Latency 下降 30%超时查询数量大幅下降甚至清零业务2（7M Loc）原先为异步逻辑协程化后 Latency 下降 8%业务3（100K Loc、2.7w core）原先为同步逻辑协程化后 qps 提升 10 倍以上性能</p><p></p><h3>Modules</h3><p></p><p></p><p>Modules 是 C++20 的四大重要特性（Coroutines、Ranges、Concepts 以及 Modules）之一。Modules 也是这四大特性中对现在 C++ 生态影响最大的特性。Modules 是 C++20 为复杂、难用、易错、缓慢以及古老的 C++ 项目组织形式提供的现代化解决方案。Modules 可以提供：</p><p></p><p>降低复杂度与出错的机会更好的封装性更快的编译速度</p><p></p><p>对于降低复杂度而言，我们来看下面这个例子：</p><p></p><p><code lang=\"text\">#include \"a.h\"\n#include \"b.h\"\n// another file\n#include \"b.h\n#include \"a.h\"</code></p><p></p><p>在传统的头文件结构中 a.h与 b.h 的 include 顺序可能会导致不同的行为，这一点是非常烦人且易错的。而这个问题在 Modules 中就自然得到解决了。例如下面两段代码是完全等价的：</p><p></p><p><code lang=\"text\">import a;\nimport b;</code></p><p></p><p>与</p><p></p><p><code lang=\"text\">import b;\nimport a;</code></p><p></p><p>对于封装性，我们以 asio 库中的&nbsp;asio::string_view&nbsp;为例进行说明。以下是&nbsp;asio::string_view&nbsp;的实现：</p><p></p><p><code lang=\"text\">namespace asio {\n\n#if defined(ASIO_HAS_STD_STRING_VIEW)\nusing std::basic_string_view;\nusing std::string_view;\n#elif defined(ASIO_HAS_STD_EXPERIMENTAL_STRING_VIEW)\nusing std::experimental::basic_string_view;\nusing std::experimental::string_view;\n#endif // defined(ASIO_HAS_STD_EXPERIMENTAL_STRING_VIEW)\n\n} // namespace asio\n\n# define ASIO_STRING_VIEW_PARAM asio::string_view\n#else // defined(ASIO_HAS_STRING_VIEW)\n# define ASIO_STRING_VIEW_PARAM const std::string&amp;\n#endif // defined(ASIO_HAS_STRING_VIEW)</code></p><p></p><p>该文件的位置是&nbsp;/asio/detail/string_view.hpp，位于 detail 目录下。同时我们从 asio 的官方文档（链接地址见文末）中也找不到 string_view 的痕迹。所以我们基本可以判断 asio::string_view这个组件在 asio 中是不对外提供的，只在库内部使用，作为在 C++ 标准不够高时的备选。然而使用者们确可能将&nbsp;asio::string_view作为一个组件单独使用（Examples），这违背了库作者的设计意图。从长远来看，类似的问题可能会导致库用户代码不稳定。因为库作者很可能不会对没有暴露的功能做兼容性保证。</p><p>这个问题的本质是头文件的机制根本无法保证封装。用户想拿什么就拿什么。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/3f/c3/3fd7d88f9d647365bcb8794a7134c5c3.png\" /></p><p></p><p>而 Modules 的机制可以保障用户无法使用我们不让他们使用的东西，极强地增强了封装性：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/67/e9/670994c8b42864f25cff5858bde4bce9.png\" /></p><p></p><p>最后是编译速度的提升，头文件导致编译速度慢的根本原因是每个头文件在每个包含该头文件的源文件中都会被编译一遍，会导致非常多冗余的编译。如果项目中有&nbsp;n&nbsp;个头文件和&nbsp;m&nbsp;个源文件，且每个头文件都会被每个源文件包含，那么这个项目的编译时间复杂度为&nbsp;&nbsp;O(n*m)。如果同样的项目由&nbsp;n&nbsp;个 Modules 和&nbsp;m&nbsp;个源文件，那么这个项目的编译时间复杂度将为&nbsp;O(n+m)。这会是一个复杂度级别的提升。</p><p></p><p>我们在&nbsp;https://github.com/alibaba/async_simple/tree/CXX20Modules&nbsp;中将 async_simple 库进行了完全 Modules 化，同时测了编译速度的提升：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8bf8381ba2633d29051bdbb9f913a5cf.png\" /></p><p></p><p>可以看到编译时间最多可以下降 74%，这意味着 4 倍的编译速度提升。需要主要 async_simple 是一个以模版为主的 header only 库，对于其他库而言编译加速应该更大才对。关于 Modules 对编译加速的分析我们在今年的 CppCon22 中也有介绍（链接地址见文末）。</p><p></p><p>最后关于 Modules 的进展为：</p><p></p><p>编译器初步开发完成支持 std modules优先内部应用已在 Clang15 中发布探索编译器与构建系统交互 (ing)</p><p></p><h2>总结</h2><p></p><p></p><p>最后我们再总结一下，使用现代化工具链带来的好处：</p><p></p><p>更短的编译时间更好的运行时性能更好的编译、静态、运行时检查更多优化技术 – ThinLTO、AutoFDO、Bolt 等更新的语言特性支持 – C++20 协程、C++20 Module 等持续性更新升级 – 良性循环</p><p></p><p>希望更多的项目可以使用更现代化的工具链。</p><p></p><p>相关链接：</p><p>asio官方文档链接地址：</p><p>https://think-async.com/Asio/asio-1.22.1/doc/asio/index.html</p><p>CppCon22 链接地址：</p><p>https://cppcon.digital-medium.co.uk/session/2022/how-much-compilation-speedup-we-will-get-from-c-modules/。</p>",
    "publish_time": "2022-10-12 10:47:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "达摩院损失AI“大将”，预训练大模型M6技术负责人杨红霞离职",
    "url": "https://www.infoq.cn/article/DXPyv0mf6q09hmWtNxEV",
    "summary": "<p>阿里达摩院损失AI“大将”。</p><p></p><p>日前，据 Tech 星球报道，阿里达摩院大模型带头人杨红霞已于9月初离职。InfoQ发现，杨红霞于不久前注销了钉钉账号。</p><p></p><h3>全球最大AI 预训练模型 M6 背后的技术负责人</h3><p></p><p></p><p><a href=\"https://www.infoq.cn/article/IhiliY5-iSW4H60ushSl\">杨红霞</a>\"博士是超大规模多模态预训练模型M6的技术负责人。</p><p></p><p>M6，英文全称是 MultiModality-to-MultiModality Multitask Mega-transformer，6 个 M，简称 M6。</p><p></p><p>顾名思义，M6 大模型主打多模态、多任务能力，其目标是打造全球领先的具有通用性的人工智能大模型。</p><p></p><p>2021年 3 月，达摩院发布了国内首个千亿参数多模态大模型 M6，引发海外关注。OpenAI 前政策主管 Jack Clark 公开点评道：“这个模型的规模和设计都非常惊人。这看起来像是众多中国的 AI 研究组织逐渐发展壮大的一种表现。”</p><p></p><p>2021年11月，阿里 M6 宣布<a href=\"https://www.infoq.cn/article/z40A8r0QeP32g0Jo4TK1\">升级</a>\"至万亿参数，并在全球范围内首次大幅降低了万亿参数超大模型训练能耗，更加符合业界对低碳、高效训练 AI 大模型的需求。</p><p></p><p>据悉，通过一系列突破性的技术创新，达摩院团队仅使用 480 卡 V100 32G GPU，即训练出了规模达人类神经元 10 倍的万亿参数多模态大模型 M6，与英伟达、谷歌等海外公司实现万亿参数规模相比，能耗降低超八成、效率提升约 11 倍。</p><p></p><p>这一技术突破将极大降低万亿模型训练门槛，让大模型研究和工业化落地进入更加普惠的时代。</p><p></p><p>针对此次升级，达摩院资深算法专家杨红霞曾表示，“接下来，M6 团队将继续把低碳 AI 做到极致，推进应用进一步落地，并探索对通用大模型的理论研究。”</p><p></p><p>以下为 M6 发展历程：</p><p></p><p>2021 年 1 月&nbsp;——&nbsp;M6 百亿参数模型达成，国内首个百亿规模多模态大模型2021 年 2 月&nbsp;——&nbsp;M6 千亿参数模型达成，国内首个千亿规模多模态大模型2021 年 5 月&nbsp;——&nbsp;M6 万亿参数模型达成，全球范围内首次大幅降低了万亿参数超大模型训练能耗，且成为国内首个实现商业化落地的多模态大模型</p><p></p><h2>AI项目落地难？</h2><p></p><p></p><p>据报道，杨红霞此次离职是因为个人家庭原因。</p><p></p><p>Tech 星球的报道中称，此番杨红霞离职，被认为是达摩院对一些难以落地的商业化项目进行调整。一位阿里云内部人士透露，“达摩院很多项目都是远看很牛，近看难以落地”，虽然二者都在云与科技，但是达摩院的项目与业务产研隔的较远，也很少和云服务一起对外售卖。所以达摩院每个项目的落地应用和商业化程度，很多是个谜。</p><p></p><p>去年5月，阿里宣布AI 大模型首次商用，M6 成为国内首个实现商业化落地的多模态大模型。经过一段时间的试用，M6 作为 AI 助理设计师正式上岗阿里新制造平台犀牛智造，通过结合潮流趋势进行快速设计、试穿效果模拟，有望大幅缩短快时尚新款服饰设计周期。M6 还已应用于支付宝、淘宝等平台，参与跨模态搜索、文案撰写、图片设计等工作。</p><p></p><p>此前，阿里一直强调，达摩院不用有盈利压力。但2022年，互联网企业普遍降本增效，达摩院也进行了诸多调整。在杨红霞之前，阿里集团副总裁、阿里云研究院副院长肖利华，达摩院副院长<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247544724&amp;idx=2&amp;sn=e89dbb5a3823608d0ae1ab7cebbd1ce7&amp;chksm=fbea965bcc9d1f4da26769446553fc0cb96fa8fadde2462163ca238320e6b4bd8b10b3c43a23&amp;scene=27#wechat_redirect\">金榕</a>\"等都已相继离开阿里。</p><p></p><p>杨红霞是AI领域杰出的人工智能科学家。资料显示，杨红霞&nbsp;2007 年本科毕业于南开大学，获统计学学士学位。其后她去往美国杜克大学统计科学系攻读博士学位，师从 David Dunson 教授。杨红霞拥有顶级论文 40 余篇。曾任 IBM Watson 研究员、Yahoo！主任数据科学家等职。她曾带领团队获2019世界人工智能大会最高奖卓越人工智能引领者（Super AI Leader，简称SAIL奖），曾获2022年福布斯中国科技女性50强的荣誉，获得2020年国家科学技术进步奖二等奖。</p>",
    "publish_time": "2022-10-12 12:07:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "十问“海外名声大噪”的现代数据堆栈：定义、架构特点及发展趋势",
    "url": "https://www.infoq.cn/article/ZX7RP0GEQOMrRaFtvMuO",
    "summary": "<p>嘉宾 ｜吴英骏、李栋、王宇飞</p><p>采访 ｜赵钰莹</p><p></p><p>数据堆栈是近几年在海外方兴未艾的概念，其中，最知名的当属&nbsp;dbt 的 CEO Tristan Handy 在 2020 年下半年发表的“The Modern Data Stack: Past, Present, and Future”(The Modern Data Stack: Past, Present, and Future)，在文章中，他将现代数据堆栈分成了寒武纪大爆发一期（2012-2016），部署（2016-2020），与寒武纪大爆发二期（2020-2025）三个时代。</p><p>&nbsp;</p><p>在国内，红点中国的合伙人刘岚也在今年初发表的一篇文章中提到，国内也出现了一些在现代数据栈方面布局的企业。那么，到底什么是现代数据堆栈？它是哪些问题的解决之道？如何区分现代数据堆栈和企业内部构建的数据平台、中台、湖仓一体等架构？</p><p>&nbsp;</p><p>本期<a href=\"https://www.infoq.cn/video/IQdSQUTqbmgjV5cel8DC\">《极客有约》</a>\"，我们邀请到了 RisingWave Labs 创始人 &amp;CEO 吴英骏、Kyligence 合伙人兼副总裁李栋、字节跳动数据平台-开发套件方向的负责人王宇飞共同探讨这一新晋技术概念，希望帮助国内开发者和感兴趣的朋友更好地了解<a href=\"https://archsummit.infoq.cn/2022/hangzhou/presentation/4901\">现代数据堆栈</a>\"。</p><p></p><p></p><p>本文将直播精华内容整理后予以刊布，以飨同好。</p><p>&nbsp;</p><p>InfoQ 第一问：“现代数据堆栈”（Modern Data Stack）这个概念具体指的是什么？</p><p>&nbsp;</p><p>吴英骏：我理解现代数据堆栈解决了从数据中提取“信息”的问题。以往主要采用结合&nbsp;Oracle、DB2、SQL Server 等数据库软件搭建一套平台的方式，现在随着云的兴起和各种商业化创业公司的兴起，主要通过“现代”的方式从原始数据中提取信息。</p><p>&nbsp;</p><p>王宇飞：现代数据堆栈的核心是企业数据上云，在此基础上，大数据从采集到应用的全链路被不同细分领域的 SaaS 服务支撑，并且这些服务是可选择和拼装的，这是堆栈的一个意义。我个人感觉，云数仓的演进进一步催化了各个细分数据领域里 SaaS 场景的发展，围绕云上数据全链路的建设过程，可以选择多个 SaaS 服务来解决。比如：第一，数据上云需要先选择一个云数仓；第二，数据落仓，需要对数据进行建设和加工；第三，需要具备元数据管理能力和数据治理能力来保证数据质量和成本；最后，需要反向 ETL 领域的公司推数据到营销系统或 CRM 系统之中。“现代数据堆栈”概念本身不重要，关键是要关注背后要解决的问题。但不得不说的是，国外能发展数据堆栈的核心是企业数据上云和 SaaS 发展比较成熟，国内的情况与国外相比还是差距比较大的。</p><p>&nbsp;</p><p>李栋：技术的出现往往是要解决特定的问题，想要了解 Modern Data Stack 需要先了解 Modern Data Challenge。首先，从用户的角度来讲，数据安全合规的要求、不同企业技术架构的选型和限制导致数据难以集中储存在一个平台或数据库中，最终形成数据孤岛；第二，从需求方面，整个市场趋势变化很快，需要对新信息有特别快的反应速度；第三，在人力成本方面，企业面临如何用更低的 IT 成本来满足更多业务需求的问题。我认为现代数据堆栈就是充分利用云、AI、大数据等技术简化 data pipeline 来帮助用户提升数据使用效率和数据治理效率。</p><p>&nbsp;</p><p>InfoQ 第二问：与现代数据堆栈相对应的还有传统数据堆栈，这二者之间有何区别？企业在从传统数据堆栈向现代数据堆栈转换的过程中，会不会存在一些中间地带？</p><p>&nbsp;</p><p>吴英骏：与 20 年前相比，当今时代数据变得越来越复杂，但数据栈越来越简化，处理的问题也越来越复杂。传统数据库时代，往往需要招不同的工程师去解决数据库管理、导入数据、提取数据、运维等问题。之后，随着云的出现，将管理、运维等事情“外包”出去，企业不再需要招各种工程师，唯一需要做的就是点几下鼠标，拖拽几下或者写几个 SQL 就能全部搞定，帮助企业降低了成本。</p><p>&nbsp;</p><p>王宇飞：现代数据堆栈，核心是基于云实现模块化，倾向于使用专精的组件构成。刚刚英骏老师也提到，传统数据堆栈，需要自己进行部署运维，维护成本较大，云帮助我们降低了成本。另外，随着云的发展和硬件存储成本的降低，数据处理模型也出现了从传统的 ETL 到 EL（T） 模式的转变，ELT 解耦了 EL 和 T，让数据集成系统专注于解决数据抽取问题，简化数据抽取链路的复杂度，使数据开发同学更专注于业务相关的数据转化部分。从字节的角度来看，数据中台的设计一开始就采用了 EL（T）的模式，近期数据集成组件也会开源出来，希望能帮助解决企业数字化转型过程中的数据集成问题。</p><p>&nbsp;</p><p>李栋：云的引入在成本方面实现了很大的优化和突破，除了资源和 IT 基础设施本身的成本优化外，还带来了企业内部数据使用过程中协作流程的转变和优化。传统的数据使用方式大多以 IT 为中心，相反 Modern&nbsp;Data&nbsp;Stack 简化了流程，并以业务为核心。举一个我们服务的国内头部股份制银行的例子，在过去，传统的 BI 使用方式是堆砌报表，业务运营人员需要依赖数据开发团队来分析客户画像，业务人员什么时候想看什么数据完全取决于数据开发团队的资源排期。后来他们搭建了统一指标中台（指标中台是 Modern Data Stack 的一部分），业务人员可以在平台上自助地创建、使用和管理指标，消除对开发团队的依赖，整体效率大幅提高，完成了从以 IT 为中心向以业务为中心的转变。</p><p>&nbsp;</p><p>InfoQ 第三问：刚刚聊到的云成本控制，不少公司过去一段时间都在抱怨收到的云账单费用太贵，这有解决方案吗？</p><p>&nbsp;</p><p>李栋：在我们公司早期做云产品和业务的性能测试时，也经常会遇到一些血泪教训。后来我们总结了一下，这块也是需要从管理上入手的，我们会用指标中台的实践来管理我们的云成本：定义一个指标体系，除了管理所有的账单数据之外，更多的是去管理云资源的用量数据，除了一些结果指标外还要寻找 CPU 资源利用率、资源闲置率、超期使用率等过程指标。从过程上抓管理，通过这种方式，云研发的 Leader 只需要每周或者每天去跟下属们抓这些云的过程指标，确保没有资源的浪费，或者是在资源到期之后及时停掉，通过这些过程日积月累节省云资源。</p><p>&nbsp;</p><p>王宇飞：我自己的理解是，成本可以从两方面来看，一方面是我们真正使用资源的成本，比如如果数据上云要用哪些 SaaS 服务以及一些实际使用云的资源，如果不上云，需要自己去私有化、部署、运维和开发，这个过程还是有开发成本的。最终要考虑这两部分哪个更划算；另一方面，其实海外也存在成本控制方面的痛点，比如有专门的公司提供云上成本控制相关的服务，把数据治理做得更好，在这个过程中帮助企业降本增效。</p><p>&nbsp;</p><p>吴英骏：尽管大家都觉得Redshift或者Snowflake卖的很贵，但从厂商角度一直觉得自己卖东西不贵。要理解这件事，实际上要从另外一个角度看问题，卖东西的时候不是只卖机器，也不是只卖数据库，它其实还“卖人”；假设我们去看 Snowflake 这家公司，我们在云上面购买了一台机器，或者说订阅了一个服务，我们不仅仅是订阅了这个服务，我们还订阅了这个服务背后的一些工程师，这个成本就高了，哪怕电费都是非常高的。假设我们不去买这些服务，只是在 AWS 上开一台 EC2 的机器，大家看账单的时候，可能会觉得开一年这个机器的钱似乎可以直接买一台机器了，如果这样想问题，考虑的可能还不够完善，因为要买一台机器，还要考虑找谁去搬这台机器，我还要付房租，而且机器可能过几年又要淘汰掉了等等，如果把这些成本都算上，我觉得云成本没有那么恐怖。</p><p>&nbsp;</p><p>当然了，我们作为用户来讲，肯定还是觉得云是非常贵的，因为最早的时候数据仓库如Redshift 还是按照机器的数量来付费的，机器开着哪怕不用，还是照样需要为这台机器付费的。所以现在出现了所谓的 Serverless 这种 pay as you go 的方式，使用了多少资源就付多少钱。但是如果使用次数较少，你会看到账单还是比较低的，如果长期用，你会发现还不如去订阅一台机器，这个时候也会更加高效得使用资源。</p><p></p><p>InfoQ 第四问：如果在企业中推进现代数据堆栈，可能谁会最先推进，最先解决的是谁的生产力？解决的是什么样的问题？</p><p>&nbsp;</p><p>吴英骏：推进现代数据堆栈，首先需要换掉数据仓库，再上其他的服务。因为数据仓库是现代数据堆栈的核心。而针对一些没有数仓需求的小公司，在 AWS 上开一个数据库也是可行的，只要服务在云上，就是整个生态中的一环，就可以连接生态里的其他组件，未来如果想换更加高端的数仓，迁移过程也非常简便。</p><p>&nbsp;</p><p>王宇飞：不管是之前的技术栈还是现在的技术栈，本质上解决的都是让数据高效发挥自身价值的问题。在我的理解中，现代数据堆栈强调的更多是以云上数据为中心的平台技术和产品能力，但让数据发挥价值，还需要符合自身行业模式的一些方法论和沉淀。</p><p>&nbsp;</p><p>推进现代数据堆栈，靠数据驱动、自身有较强研发能力的企业，一般通过自研搭建平台产品的方式解决问题；具有一定研发能力的企业，通常采用拼凑组合一些开源产品或者部分采用商业化产品的模式；一些传统或者初创企业可能会采用完全商业化产品。</p><p>&nbsp;</p><p>现代数据堆栈可以解决企业内使用数据同学的生产力，使得他们可以更专注于解决自身业务问题，而不是数据质量问题或数据研发效率问题等。</p><p>&nbsp;</p><p>李栋：在企业里推进现代数据堆栈，除了可以解决降本增效等问题外，还可以提升数据以及数据团队的价值。举一个电商平台的例子：在传统模式中，通过 ETL 开发各种宽表，每个宽表又支撑一些报表，久而久之就会出现宽表爆炸的情况，即便是存储在云上也会产生很大的成本。在数据湖上搭建指标中台，把数据以指标的方式进行统一管理，所以平台中维护的每个指标都是有业务意义的数据资产而不再是成本，一方面可以从业务价值角度更好地管理数据、提升数据管理 ROI，另外一方面还可以帮助数据/IT团队从管理成本变为管理资产，不断提升团队价值和影响力。</p><p>&nbsp;</p><p>InfoQ 第五问：这两年，国内的很多企业在解决数据问题上做了不少事情，比如中台、平台、湖仓一体等，这些事情和现代数据堆栈有什么区别和联系？</p><p>&nbsp;</p><p>李栋：不论是国内的湖仓一体、数据中台还是海外的<a href=\"https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng==&amp;mid=2247502757&amp;idx=3&amp;sn=24627a171ef8402303d0b7f42050fbe9&amp;chksm=eba7c838dcd0412e38fefb681b8ea8c4bda482d8480c893da2eb77d0af44f8acb341c21563a8&amp;scene=27#wechat_redirect\"> Data Mesh、Data Fabric&nbsp;</a>\"等理念，核心都是解决数据使用和数据管理中遇到的各类问题。Modern Data Stack 的优势在于丰富的技术生态，企业可以根据自身实际情况在生态中选型，比如想帮助企业建立一个指标体系的时候可以考虑指标中台的方式。</p><p>&nbsp;</p><p>王宇飞：数据中台、湖仓一体和现代数据堆栈应该不是一个维度和层面的概念。企业内部，数据中台应当具备平台能力、方法论和组织模式这三个比较核心的要素，三者结合在一起解决了企业内数据体系建设问题；湖仓一体本质上是从数据处理和存储的场景出发，降低不同场景需求下数据的移动和存储成本，在解决非结构化、半结构化和结构化的数据处理和管理的同时兼顾性能和成本；现代数据堆栈可以解决数据中台的平台产品能力方面的问题，现代数据堆栈中的中心存储和云数仓解决方案可以采用湖仓一体的技术方案。</p><p>&nbsp;</p><p>吴英骏：在小公司中，各个团队之间彼此知道对方在做什么，可以使用&nbsp;USB、Excel 等一些简单的方法传输数据，当发展成比较大的公司后，不同组之间传递数据变得复杂，需要中台这种统一的方式传输数据。在中台中使用的技术栈也可以是现代技术栈，这只是中台的一种实现方式。湖仓一体本质上说的是数据存放到哪里的问题，不管是数据湖还是数据仓库，它都是存储。而现代数据栈的核心是存储，只要存了数据之后，才有管理、分析和可视化数据的需求。现代数据栈的核心是“数据湖”，“湖”的周围是一些生态。</p><p>&nbsp;</p><p>InfoQ 第六问：国内外目前对这件事情的热度差别很大，具体是什么原因？</p><p>&nbsp;</p><p>王宇飞：本质上还是发展阶段不太一样。数据堆栈的核心是基于云，目前国内云的发展和普及度与海外相比还存在一定差距；从客户偏好上来说，国内的客户更喜欢用 All In One 的解决方案。这背后主要有几点原因，一是国内平均 IT 基础能力更差一些，让客户在一个场景的不同细分领域选择不同的产品和解决方案，对客户自身的 IT 要求是比较高的，另外一个原因是国内云 SaaS 产品之间的标准化程度没有海外那么成熟，这也导致目前不同 SaaS 组件之间的成本比较大。</p><p>&nbsp;</p><p>目前来看，整体 All IN ONE 的解决方案对国内企业数据上云和数据化转型过程来说，成本更低。除此之外，ALL IN ONE 还有组件内部联动性和组件之间数据共享方面的优势。举个例子，比如在使用&nbsp; DataLeap 开发平台的过程中，除了写 SQL、配置任务依赖、部署发布耦合运维管理之外还需要拿到元数据平台的信息去提效整个开发过程，在上线前加上治理平台提供的数据质量检测能力，整个操作链路对用户来说是更闭环而不是跨多个产品的方式，这是核心的操作链路体验一致性方面的优势。</p><p>&nbsp;</p><p>但这并不是说现代数据堆栈的这种多细分领域的 SaaS 模式做不到 ALL IN ONE 产品解决方案的优势，只是如果它做到相同程度，成本会高一些。</p><p>&nbsp;</p><p>现代数据堆栈的兴起一定程度上也验证了我们当初的一些判断：随着 SaaS 细分领域的成熟也会出现灵活的对接需求。比如，虽然目前DataLeap开发套件是以 ALL IN ONE 的形式对外，但从比较早期的设计来看，也考虑了各个场景的产品能力，具备一定的独立性，尽量做到了开放兼容的能力，也可以去对接一些其他产品。从最后的发展来说，两种形态可能会是一个长期共存的状态。</p><p>&nbsp;</p><p>最后想说的是，对于企业客户来说，不用过多关注现代数据堆栈、湖仓一体、流批一体等炒热的概念，我们需要看清它背后解决的问题以及给我们带来的价值，依据自身的发展阶段做合理的选择。</p><p>&nbsp;</p><p>吴英骏：我说一下我为什么要做现代数据堆栈，我一开始是不知道这个概念的，等到我创业的时候发现，不管是什么公司都要提这个概念，而这个现象的本质就是，在国外大家非常注重生态，而生态在于打通，我把负责的部分做到最好，其他的部分放心交给其他厂商去做。在所谓的生态里，我可以只做我自己的事情，但同时也希望给用户一个选择的权利，我跟其他厂商的关系不仅仅是单纯的竞争关系，我们两者之间承担一种共建生态的职责而不是内卷。现代数据堆栈就很好地体现了生态这个概念，在现代数据堆栈中，每个平台都会去做集成。而在国内，这相对来说是一件难以接受的事情，大家更习惯一种大一统的解决方案。</p><p>&nbsp;</p><p>李栋：英骏老师针对海外的一些观念和思路介绍的很详细，我多讲一下国内。举一个我们自己的例子，在过去的两年，我们帮国内的很多客户实践了指标中台。去年下半年的时候，我们观察到&nbsp;Metrics&nbsp;Store 在海外也在兴起并出现了一系列的创业公司。当时，虽然 Modern Data Stack 这样的概念在国内还没有兴起，但是 Modern Data Stack 当中的一些细分领域已在国内有很多沉淀，就像我们的客户在指标中台这个方面已经有了实践。</p><p>&nbsp;</p><p>但是，为什么国内就没有形成 Modern Data Stack 这样的生态或者说还正在起步过程当中？云是一方面，另一方面，国内基于细分领域的生态建设的成熟度还不够。但在过去的一两年，越来越多的云原生技术组件和公司都已发展起来，一些开源技术也都开始向云原生方面切入，开始形成这个生态，这些都是很好的现象。我对国内想形成自己的 Modern Data Stack 还是很有信心的，甚至因为不同国家地区的情况不一样，我们也许会形成一个叫“China Data Stack”的概念，更适合于中国本土的一些企业。</p><p>&nbsp;</p><p>InfoQ 第七问：李栋老师刚才提到很多客户开始部署指标中台，这里面是什么在驱动？</p><p>&nbsp;</p><p>李栋：在指标中台部分，核心要看解决的问题是什么。如前面谈到的，企业都需要把使用效率提升起来以及把数据管理做起来，最简单的一个问题在于数据口径的管理，可能大家都有数据仓库或者数据湖，也有客户在做湖仓一体的建设，也许不是云上，但是久而久之，大家都会遇到，也许就是两个部门的人在做数据分析时发现计算指标的口径是不一样的，比如一个零售的企业开会，可能华南区和华北区销售额的计算口径、计算逻辑各有不同，这样会带来背后的无论是数据开发还是业务决策的整个流程上的很多问题，所以就有了统一管理这些指标的需求，就产生了对指标中台的诉求。</p><p>&nbsp;</p><p>InfoQ 第八问：传统BI，自助BI，还有指标中台之间的联系？</p><p>&nbsp;</p><p>李栋：有两点最大的区别。一是一般企业想把BI用好，还是有一定的门槛的，比如各大BI厂商都有一些认证，想成为BI专家一般是需要专业认证的，这就带来了使用门槛上的区别，任何一个业务人员都有自己的KPI或OKR，也就是自己工作中最关心的指标，指标中台是以指标为核心，在指标中台中可以定义、管理、查看、分析这些指标，并开展业务决策，这里的门槛很低，更不需要考认证等；二是数据治理方面，在多数BI系统中，核心管理的都是可视化报表，但是不同报表中很容易存在指标重复或口径不一的问题。通过指标中台可以统一管理所有指标的定义，无论是原子指标还是衍生指标，所有的业务用户或者所有下游的应用都可以在这个平台中获取到最可靠的数据指标，确保所有口径的一致性。</p><p>&nbsp;</p><p>InfoQ第九问：字节跳动在数据治理上面的一些关键动作？</p><p>&nbsp;</p><p>王宇飞：数据治理对我们来说是一个比较看重的问题，这两年我们也在这方面投入了很大的精力，字节的数据治理还是有一些自身的挑战：字节的业务线比较多，组织比较扁平灵活，从上到下发布治理运动的模式在字节不见得行得通；另外由于业务发展快和复杂度高，我们的治理域问题涉及广，不只是成本治理。</p><p>&nbsp;</p><p>基于这些挑战，我们也沉淀了一个全域的数据治理平台，它把像SLA 治理、成本治理、报警治理等治理域全部都串联在一起，可以通过规划式路径一站式的解决多领域问题，提升治理效率。同时我们提倡“集中决策和分布自治”的理念，让不同的业务根据自身发展阶段，制定自己的治理目标。另外，平台也可以监控整个治理过程与最终效果，治理平台能力我们近期也会对外输出。</p><p>&nbsp;</p><p>InfoQ 第十问：未来三到五年，在数据架构的发展方向上，国内和国外的发展状态和方向大概是什么样子的？</p><p>&nbsp;</p><p>王宇飞：我个人理解，国内的数据架构随着云的发展会越来越加强整个生态的建设。现在看到，很多云厂商其实都是以All In One的形式去提供服务，本身也会有局部组件的能力变得越来越开放或局部组件也都在走向开源，像刚刚提到的字节跳动的数据集成工具即将开源，可能也会有一些其他组件开源出来，或者提供第三方对接能力，帮助完善国内整个数据生态的建设和发展。</p><p>&nbsp;</p><p>我个人觉得，如果走到国外这么成熟的SaaS服务生态体系，还需要一定时间。</p><p>&nbsp;</p><p>李栋：我觉得接下来技术的发展，云肯定是一个方面，另一方面是自动化或者智能化。因为无论是AIOps还是数据分析领域增强分析的概念，这些技术的背后都是更多地把AI的自动化技术应用到整个数据平台中。在这样的理念下，Modern Data Stack中的很多场景都可以通过自动化来简化人工的工作。过去很多需要人力的工作被软件和工具所替代，在替代过程中，就需要有更多的AI方面的技术集成进来去实现一些自动化。</p><p>&nbsp;</p><p>吴英骏：针对国外的情况，接下来可能会朝着几个方向发展。第一个方面，接下来会从Subscription Base全面转向Consumption Base；第二个方面 ，所谓天下大事合久必分，分久必合，从SaaS角度来讲，我相信还是会有小的领域互相吃掉，这种也是比较良性的；哪怕是在云这个方面，现在有三朵云AWS、GCP和Microsoft Azure，尽管这三朵云之间不会“互吃”，但我们也发现其实已经有一些工具统一了，比如伯克利大学最近搞的sky computing，把比较割裂的体验做中和；第三个方面，就是我们在做的realtime这个方向。现在的公司数据量特别大，大家会更希望看到实时的结果，所以大家会看到现在很多软件都在做实时推送，同步的频率会越来越高，而我们做的就是对实时流进来的数据做实时处理，同时我们也发现哪怕是可视化的部分也已经有实时的倾向了，就比如之前可能是一张静态报表，现在大家会希望看到一张动态的报表，以上这几方面我相信是比较大的趋势</p>",
    "publish_time": "2022-10-12 13:58:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "龙芯中科官宣：龙架构平台已初步支持OpenHarmony操作系统",
    "url": "https://www.infoq.cn/article/gJ8ClaBdEOqcy89jG2qT",
    "summary": "<p>近日，<a href=\"https://www.infoq.cn/article/6uhRvm2HsDxUwic8Xsa3\">龙芯中科</a>\"宣布，在龙芯中科与润和软件共同努力下，OpenHarmony操作系统与龙芯2K0500开发板完成适配验证，龙架构（LoongArch）平台对于<a href=\"https://www.infoq.cn/article/NE86B4oiDempVd5RV06g\">OpenHarmony</a>\"已形成初步支持，万物互联的生态体系与龙芯平台即将全面连接。</p><p></p><p>龙芯中科表示，当前基于龙芯2K0500平台，多个龙芯生态伙伴已形成面向工业物联网关、国产化BMC、教育开发板、IDE编程工具、自主打印机等一系列解决方案。在OpenHarmony的支持下，龙芯中科与更多合作伙伴将进一步扩大国产自主生态丛林，形成信息产业命运共同体，推动万物智联领域创新发展。</p><p></p><p>据了解，龙芯2K0500是一款基于64位LA264处理器核设计的高集成度处理器芯片，主要面向工控互联网应用、打印终端、BMC等应用场景，可实现ACPI、DVFS/DPM动态电源功耗管理等低功耗技术，支持多种电源级别和唤醒方式，并可根据具体应用场景对芯片部分功能和高速接口进行裁剪。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44d159310c3b0a607f7d7deed01f2b0d.png\" /></p><p></p><p>OpenHarmony是全球开发者共建的开源分布式操作系统，2020 年 12 月，博泰、华为、京东、润和、亿咖通、中科院软件所、中软国际等七家单位在开放原子开源基金会的组织下成立了 <a href=\"https://xie.infoq.cn/article/22108203d79c3809d4720bac8\">OpenHarmony 项目群工作委员会</a>\"，开始对 OpenHarmony 项目进行开源社区治理。当前，国内众多厂商已基于其形成多个跨终端全领域的发行版操作系统。</p><p></p><p>为推动龙架构芯片适配OpenHarmony系统，2022年4月，龙芯中科与润和软件、慧睿思通、龙芯俱乐部等发起成立OpenHarmony LoongArch SIG。</p><p></p><p>龙芯中科表示，下一步将与润和软件携手继续完成更多龙架构芯片与OpenHarmony的适配，共建基于龙架构平台的OpenHarmony国产自主生态及全栈式解决方案，推进产品化工作以及相关方案落地。同时，双方将就OpenHarmony产业人才教培展开深入合作，并会逐步将代码开源到OpenHarmony社区，为开源生态持续贡献力量。</p>",
    "publish_time": "2022-10-12 14:23:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]