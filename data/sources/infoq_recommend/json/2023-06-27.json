[
  {
    "title": "构筑成功之桥：从组织文化到SRE实施",
    "url": "https://www.infoq.cn/article/7XbV7urmZm9bAs4EwjUg",
    "summary": "<p></p><h2>引言</h2><p></p><p></p><p>西门子健康集团的teamplay数字健康平台和应用是一个大型的分布式组织，由25个团队组成，拥有医疗领域不同的众多数字服务。</p><p></p><p>该组织经历了一次SRE转型，这是一次深刻的社会性技术变革，改变了生产经营的技术、流程和文化。在本文中，我们将重点讨论：</p><p>在SRE转型之初，如何在生产经营方面评估组织文化？如何创建一个随着时间推移不断累积小规模文化变革的路线图？领导层如何推动必要的文化变革？</p><p></p><h2>评估组织文化的必要性</h2><p></p><p></p><p>在引入SRE时，很容易直接跳到变革的技术部分，并开始着手实施新的工具、基础设施和仪表盘。</p><p>毋庸置疑的是，这些制品本身不足以动摇一个组织的生产运营方式。SRE转型在很大程度上是一个社会性技术（sociotechnical）变革。</p><p></p><p>变革的“社会（socio）”部分需要从SRE转型的一开始就发挥同样的作用。</p><p></p><p>在这种情况下，从生产运营的角度评估组织目前的文化是很用的。这会带来如下的好处：</p><p>它能够让推动转型的SRE教练了解当前组织中对生产运营的态度。它能够揭示组织在信息共享、决策制定、协作、学习和其他方面有可能加速或阻碍SRE转型的运营方式，这些方式可能是非常微妙且难以察觉的。它能够激发组织转向SRE的热情，并且能够首次预测演进的速度。</p><p></p><p>既然存在这些收益，那么该如何从生产运营的角度来评估组织文化呢？这就是下一节的主题。</p><p></p><h2>如何评估组织文化？</h2><p></p><p></p><p>有种流行的组织文化拓扑学是由Ron Westrum提出的Westrum模型。该模型根据组织处理信息的方式，将文化分为病态型、官僚型和生机型：</p><p>病态型文化是以权力为导向的官僚型文化是以规则为导向的生机型文化则是以绩效为导向的</p><p></p><p>基于Westrum模型，谷歌的DevOps研究和评估（<a href=\"https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture\">DORA</a>\"）项目通过严格的研究发现，生机型文化会促成高效的软件交付。根据Westrum模型，生机型高效文化包含如下六个方面：</p><p>高度合作训练信使共担风险鼓励交流失败时追根溯源接纳新颖的想法</p><p></p><p>这六个方面可以用来评估一个组织的运营文化。要做到这一点，需要将这六个方面映射到SRE中，以了解文化的目标状态。基于我的图书“<a href=\"https://www.informit.com/store/establishing-sre-foundations-a-step-by-step-guide-to-9780137424658\">Establishing SRE Foundations</a>\"”，下表提供了如下的映射。</p><p></p><p></p><p></p><p>有了上表中定义的目标状态，SRE教练就可以分析他们的组织目前距离目标文化状态还有多远。</p><p></p><h2>随着时间推移积累小规模的文化变革</h2><p></p><p></p><p>SRE教练了解现状之后，就可以开始SRE转型的活动了。这些活动将会包括技术、流程和行为的改变。为了推进这项运动，SRE教练需要寻找小的行为变化、对此进行庆祝，并按照这样的方式缓步前行，随时间推移进行积累。</p><p></p><p>例如，如下所列的小变化可以逐步促成更大的行为变化，随着时间的推移，将组织的文化逐步推向上一节中所描述的目标状态。</p><p></p><p></p><p></p><p>上表中的文化变革是通过正式领导和非正式领导的相互作用来推动的。我们将会在下一节描述这种推动力。</p><p></p><h2>正式领导和非正式领导的相互作用</h2><p></p><p></p><p>在每一个层级化的组织中，都会有一些领导者，他们因为在组织结构图中的位置而获取相应的权力。如果这些领导者在更广阔的组织内得到信任，那么他们努力的成果就会成倍增加，因为在组织内，他们有大量的追随者。</p><p></p><p>同时，在很多层级化的组织中，都存在非正式的领导者，他们并没有正式的权力，因为在组织结构图中他们没有正式的位置。但是，他们已经赢得了整个组织的信任。这种信任也能使他们努力的成果得到成倍增加，因为组织中有大量的人自愿跟随他们。</p><p></p><p>在下表中，我们总结了正式领导和非正式领导类型。</p><p></p><p></p><p></p><p>最左边和最右边的列描述了一个关于领导力的良好组合，它能够提供必要的环境，有利于在组织内自上而下和自下而上地推动SRE。在SRE转型的动态环境中，它能够保持一致性、稳定性并坚定信心。团队会认为正式的领导者支持SRE，而非正式的领导者则能够帮助推动整个组织必要的心态、技术和流程变革。这能够最大化SRE转型成功的可能性。</p><p></p><h2>来自一线的经验</h2><p></p><p></p><p>上述的文化评估方法帮助西门子健康集团的数字健康平台组织成功地推动业务向SRE方向发展。在本节中，我们将会介绍一些从SRE转型一线获取的真正经验。</p><p></p><h3>经验1：从一开始就让产品所有者参与进来</h3><p></p><p></p><p>我们得到的最深刻的经验之一就是让产品所有者从一开始就参与到SRE转型中。对于产品所有者来说，SRE的价值在于减少客户因为数字服务没有达到预期效果而导致的问题升级。这些问题升级是令人讨厌的、耗时的，并且会导致管理层不必要的关注。这为产品所有者参加SRE会议提供了动力，在这种会议上会定义SLO并讨论相关的流程。</p><p></p><p>在SRE会议上，产品所有者需要：</p><p>从业务角度提供关于客户最重要流程的背景知识根据会议中讨论的成本，评估更高的可靠性所带来的商业价值通过从一开始就参与SRE讨论，进一步了解生产运营了解如何以数据驱动的方式考虑对可靠性和新特性进行投资的优先级</p><p></p><h3>经验2：让开发人员的注意力首先放到生产环境上</h3><p></p><p></p><p>对于刚接触软件即服务的组织来说，主要的问题在于，开发人员不习惯关注生产环境。相反，在传统方式中，他们的世界以特性描述为始，以特性实现为终。在生产环境中运行特性并不在他们的关注范围之内。这就是我们的组织在SRE转型之初的情况。</p><p></p><p>在这种情况下，SRE转型之初最具影响力的里程碑事件就是让开发人员将注意力转移到生产环境上。这是一个80/20类型的里程碑，20%的努力带来了80%的改善。</p><p></p><p>对于开发人员来说，定义完美的SLO和错误预算策略并没有那么重要。相反，重要的是为开发人员提供基本的工具和初始的动力，让他们将注意力转移到生产环境中。在获取运维软件的新习惯时，定期花费时间对生产环境进行分析就成功一半了。</p><p></p><p>做到这些之后，应用SRE方法的准确性就可以逐步实现了。</p><p></p><h3>经验3：不要害怕让团队一开始就快速失败</h3><p></p><p></p><p>按照我们的经验，在定义最初的SLO时，团队一开始会倾向于高估其服务的可靠性。他们倾向于设置比服务的平均水平更高的SLO。同样的，他们倾向于设置比服务可以达到更严格的延迟SLO。</p><p></p><p>在这个初始阶段，试图说服团队放松最初的SLO是徒劳的，即便是历史数据有时候也无法说服团队。我们发现，快速失败的方式实际上是最有效的。</p><p></p><p>我们按照团队的建议设置了SLO，没有进行过多的争论。毫不意外，团队被大量的SLO告警淹没了。下一次SRE会议的主题不可避免地变成了团队无法处理大量的告警。</p><p></p><p>这使团队充分理解了他们的SLO决定的后果。于是，重新定义SLO的过程开始了。这正是我们需要的：一个来自生产环境是否满足SLO的强大反馈循环，这促成了对SLO的重新评估。</p><p></p><h3>经验4：构建正式领导者和非正式领导者的联盟</h3><p></p><p></p><p>我们发现，由正式领导者和非正式领导者组成的联盟在组织中倡导SRE是非常有用的。非正式的领导自学了SRE，对将它引入到组织中充满了热情。为了实现这一点，他们需要正式领导的支持，以便于团队投入SRE相关的工作。</p><p></p><p>非正式领导者需要向正式领导推销SRE方案，承诺减少因为服务中断所导致的客户问题升级。这些对话一般会发生在研发主管和运营主管身上。反过来，这些领导者需要向整个领导团队推销SRE方案，以便于将该主题列入组织的重大行动清单中。</p><p></p><p>这样做之后，就会形成一个强大的组合，即有足够多的正式领导者支持SRE，SRE列入到了组织的重大行动清单中，还有一群充满活力的非正式领导准备在整个组织内推动SRE。</p><p></p><p>这种组织状态有利于使用SRE实现成功的生产运营！</p><p></p><h2>总结</h2><p></p><p></p><p>对于刚刚接触数字化服务运营的软件交付组织来说，SRE转型是一个巨大的社会性技术变革。变化的速度在很大程度上取决于目前的组织文化。人们对生产环境运营的态度和看法才是需要搬掉的最大绊脚石，而不是人们日常使用的工具和仪表盘。</p><p></p><p>因此，在开始SRE转型之前评估组织文化是一项非常有用的工作。它能够让推动转型的SRE教练了解组织目前在运营文化方面的情况。它进一步点燃了一个有价值的思考过程，也就是如何将文化发展到SRE。</p><p></p><h5>作者简介：</h5><p></p><p>Vladyslav Ukis博士毕业于德国Erlangen-Nuremberg大学的计算机科学专业，后来又毕业于英国曼彻斯特大学。他在每次毕业后都加入了西门子健康集团，并一直从事软件架构、企业级架构、创新管理、私有云和公共云、团队管理、工程管理、产品套件管理、合作伙伴管理以及数字化转型等方面的工作。他目前担任西门子健康集团teamplay数字健康平台的研发主管。2022年，他在Addison-Wesley出版的“Establishing SRE Foundations”一书中分享了他的<a href=\"https://www.informit.com/store/establishing-sre-foundations-a-step-by-step-guide-to-9780137424658\">DevOps知识</a>\"。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/assess-culture-sre-adoption/\">Assessing Organizational Culture to Drive SRE Adoption</a>\"</p>",
    "publish_time": "2023-06-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Cloudflare的Kafka之旅：万亿级消息处理实践",
    "url": "https://www.infoq.cn/article/ZHFZHsctlAhN8Ai9wdpo",
    "summary": "<p>在不到6年的时间里，Cloudflare已经向Kafka中生成了超过1万亿条用于服务间通信的消息。随着公司和应用程序服务团队的发展，他们必须不断调整他们的工具来支持持续的快速交付。</p><p>&nbsp;</p><p>我们将在本文中介绍我们早期的分布式团队工作，以及如何基于Kafka构建抽象来处理万亿条消息。</p><p>&nbsp;</p><p>我们还将介绍近年来可伸缩性给我们带来的挑战，以及我们为应对不断增长的需求而采用的一些方法和模式。</p><p>&nbsp;</p><p></p><h1>Cloudflare简介</h1><p></p><p>&nbsp;</p><p>Cloudflare为用户提供了一个全局网络，并帮助他们提升他们网站、API和网络流量的安全性。</p><p>&nbsp;</p><p>这个网络还可以保护企业网，让他们能够在边缘运行和部署整个应用程序。</p><p>&nbsp;</p><p>Cloudflare提供了一系列产品，包括CDN、Zero Trust和Cloudflare Workers，来实现这些目标，以及识别和阻止恶意活动，让用户能够把精力专注在他们的业务上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a21d31a11021e2e2d3de8d581ec6f930.png\" /></p><p></p><p>图1 Cloudflare的全局网络</p><p>&nbsp;</p><p>从工程角度来看，Cloudflare网络有两个主要组成部分：全局边缘网络和Cloudflare控制平面。</p><p>&nbsp;</p><p>这个网络的很大一部分是使用Cloudflare的产品来构建的，其中Workers被部署在边缘网络上。另一方面，控制平面是一组数据中心，在数据中心的机器上运行着Kubernetes、Kafka和数据库。Kafka生产者和消费者通常都部署在Kubernetes中，但也取决于工作负载和期望获得的结果。</p><p>&nbsp;</p><p>在本文中，我们将重点关注Cloudflare控制平面，并探索如何通过扩展服务间通信和工具来为运营提供支持。</p><p>&nbsp;</p><p></p><h1>Kafka</h1><p></p><p>&nbsp;</p><p><a href=\"https://kafka.apache.org/\">Apache Kafka</a>\"有集群的概念，集群由多个broker组成，每个集群都有一个指定的首领broker负责协调通信。在下图中，Broker 2就是首领。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da52df231d7d357bd1e51c05a89cdc9d.png\" /></p><p></p><p>图2 Kafka集群</p><p>&nbsp;</p><p>消息被分类到主题中，例如用户事件（创建用户或更新用户信息等）被放在同一个主题中。主题被分为分区，这样Kafka可以很好地进行横向伸缩。在图中，两个broker上都有主题A的分区，每个分区都有一个指定的首领来确定其“真相来源”。为了确保弹性，分区根据指定的复制系数进行复制，通常最小的复制系数为3。向Kafka发送消息的服务叫作生产者，从Kafka读取消息的服务叫作消费者。</p><p>&nbsp;</p><p></p><h1>Cloudflare的工程文化</h1><p></p><p>&nbsp;</p><p>在过去，Cloudflare是一个独立的PHP应用程序，随着公司的发展和多样化，这种方式暴露出了它的局限性和风险。</p><p>&nbsp;</p><p>现在，团队不再强制指定使用特定的工具或编程语言，他们可以构建和维护他们自己的服务，公司鼓励创新并倡导使用高效的工具和实践。应用程序服务团队是一个相对较新的团队，职责是通过提供遵循最佳实践的工具帮助其他团队更容易完成工作，这样开发团队就可以专注于交付业务价值。</p><p>&nbsp;</p><p></p><h1>紧密耦合</h1><p></p><p>&nbsp;</p><p>随着产品规模的增长，有必要找到更好的方法，让团队能够按照自己的节奏前进，并与其他团队解耦，工程团队也需要对回退请求和工作完成保证有更多的控制。</p><p>&nbsp;</p><p>由于我们已经在使用Kafka集群来处理大量的数据，所以我们决定花点时间创建一个通用的消息总线集群：很简单，只需要创建一个拉取请求，设置一个新主题所需的一切，包括复制策略、保留期限和ACL。下图说明了<a href=\"https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages/\">消息总线</a>\"集群是如何帮助解耦团队的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/672d78ae8c238f7fab2e2c3a4e7a6436.png\" /></p><p></p><p>图3 通用的消息总线集群</p><p>&nbsp;</p><p>上图中的三个团队可以发出审计日志系统感兴趣的消息，而不需要了解特定的服务。更少的耦合让工程团队可以更有效地完成工作。</p><p>&nbsp;</p><p></p><h1>非结构化的通信</h1><p></p><p>&nbsp;</p><p>对于事件驱动的系统来说，为了避免耦合，系统之间不应该相互感知。最初，我们没有强制要求使用怎样的消息格式，生产者团队可以自己决定如何生产消息。如果团队之间没有严格的契约，这可能会导致非结构化的通信，并带来挑战，无法被处理的消息数量会激增。</p><p>&nbsp;</p><p>为了避免出现非结构化的通信，应用程序服务团队在Kafka生态系统中寻找解决方案，并找到了两个可行的选择——<a href=\"https://avro.apache.org/\">Apache Avro</a>\"和<a href=\"https://protobuf.dev/\">protobuf</a>\"，并最终选择了后者。我们以前一直使用JSON，但发现很难实现兼容性，而且JSON消息比protobuf大。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/3997f3a64d7f02d9bef7fe6f1e85e109.png\" /></p><p></p><p>图4 protobuf消息示例</p><p>&nbsp;</p><p>protobuf提供了严格的消息类型和向前和向后兼容性，能够用多种语言生成代码也是它的一个主要优势。应用程序服务团队建议在protobuf消息中加入详细的注释，并使用Uber的开源工具<a href=\"https://github.com/uber/prototool\">Prototool</a>\"进行变更检测和强制执行风格规则检查。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a36d69451d9939724e3ed22b81d46e94.png\" /></p><p></p><p>图5 切换到Protobuf</p><p>&nbsp;</p><p>只有Protobuf是不够的：不同的团队仍然可以向同一个主题发送消息，由于格式不符合预期，消费者可能无法处理它们。此外，配置Kafka的消费者和生产者并不是一件容易的事情，需要对工作负载有充分的了解。由于大多数团队都在使用Go，我们决定使用Go构建一个“消息总线客户端库”，既可以遵循最佳实践，也可以让团队更快地完成他们的工作。</p><p>&nbsp;</p><p>为了避免团队向同一个主题发送不同的消息，我们做出了一个有争议的决定，即（在客户端）强制每个主题使用一种protobuf消息类型。这个决定很容易被采用，但它会导致创建太多的主题，复制太多的分区，因为分区的复制系数至少为3。</p><p>&nbsp;</p><p></p><h1>连接器</h1><p></p><p>&nbsp;</p><p>通过引入工具和抽象，应用程序服务团队在简化Kafka基础设施方面取得了重大进展，但我们发现，要遵循最佳实践，还有很多应用场景和模式需要处理，于是团队开发了连接器框架。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d26e3876111ebaaf27063326b44e9e0c.png\" /></p><p></p><p>图6 连接器框架</p><p>&nbsp;</p><p>这个框架基于Kafka连接器，工程师们可以用它创建从一个系统读取数据并将其推送到另一个系统（如Kafka或<a href=\"https://blog.cloudflare.com/introducing-quicksilver-configuration-distribution-at-internet-scale/\">Quicksilver</a>\"——Cloudflare的Edge数据库）的服务。为了简化使用过程，我们使用<a href=\"https://www.cookiecutter.io/\">Cookiecutter</a>\"来创建服务模板，工程师们只需要在命令行上输入一些参数。</p><p>&nbsp;</p><p>连接器的配置过程很简单，可以通过环境变量来设置，无需修改代码。</p><p>&nbsp;</p><p>下面的示例将从Kafka读取数据并将其写入Quicksilver。连接器被设置为从topic1和topic2读取数据，并应用pf_edge函数。完整的配置就是这些，其中还包括指标、警报和部署到生产环境所需的其他所有东西。工程团队可以选择注册自定义转换器，这些是他们唯一需要编写的代码。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/74/74f831340d6c937467aedaf532bc9940.png\" /></p><p></p><p>图7 连接器示例</p><p>&nbsp;</p><p>例如，我们在通信方式首选项服务中使用了连接器：如果用户希望在Cloudflare仪表盘中移除营销信息，他们可以通过与这个服务发生交互来完成这个操作。通信方式首选项变更事件被保存在数据库中，并向Kafka发送消息。为了确保变更可以在三个不同的源系统中生效，我们使用单独的连接器分别将变更同步到事务性电子邮件服务、客户管理系统和市场电子邮件系统。这种方法可以让系统最终保持一致，我们利用Kafka提供的保证机制来确保整个过程顺利进行。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a53d0b5393202fd08bc630fe41f2bf0a.png\" /></p><p></p><p>图8 连接器和通信方式首选项变更</p><p>&nbsp;</p><p></p><h1>可见性</h1><p></p><p>&nbsp;</p><p>在疫情期间，随着用户的迅速增长，吞吐量也在快速增长，我们创建的一些抽象的可伸缩性问题就凸显了出来。</p><p>&nbsp;</p><p>以我们为Kafka用户处理的审计日志为例：我们建立了一个系统来管理这些日志，生产者生成事件，我们监听它们，并将数据记录到数据库中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4e/4e29fe9f9f5b0533bb97da6a0f138b59.png\" /></p><p></p><p>图9 审计日志推送</p><p>&nbsp;</p><p>我们通过一个API和日志推送集成方式来暴露这些信息，我们通过日志推送将审计日志推送到不同的数据桶中，比如<a href=\"https://www.cloudflare.com/products/r2/\">Cloudflare R2</a>\"或Amazon S3。</p><p>&nbsp;</p><p>在疫情期间，新注册的审计日志比以往更多，用户开始使用我们的API来获取最新数据。由于这种方法不可伸缩，我们决定开发一个管道来解决这个问题。我们创建了一个小型服务来监听审计日志事件，并将其转换为适当的格式，然后直接存储到数据桶中，而不会让API过载。</p><p>&nbsp;</p><p>随着日志越积累越多，并且无法足够快地清除它们，我们遇到了更大的问题，导致出现延迟和违反SLA。我们无法确切知道导致延迟的原因，因为我们的SDK缺少诊断问题的工具：瓶颈在于从Kafka读取数据、转换数据还是将数据保存到数据库？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d27e6eccaf6fa17e6074e3c9c80e55a.png\" /></p><p></p><p>图10 瓶颈在哪里？</p><p>&nbsp;</p><p>我们决定使用Prometheus指标来解决这个问题。直方图指标可以知道处理消息的每个步骤需要花费多少时间，这有助于我们识别出较慢的步骤，但我们仍然无法判断哪个组件处理特定消息花费的时间更长。为了解决这个问题，我们对OpenTelemetry进行了调研，重点关注它的跟踪集成能力：Kafka方面并没有很多好的OpenTracing集成工具，而且跨多个服务传播跟踪事件也是件具有挑战性的事情。</p><p>&nbsp;</p><p>随着团队使用OpenTracing来增强SDK，我们发现将数据推送到数据桶和从Kafka读取数据都是瓶颈，并优先修复了这些问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5a/5a32c28aa299f540d88f48bf8284bc28.png\" /></p><p></p><p>图11 找出瓶颈所在</p><p>&nbsp;</p><p>将指标添加到SDK中，我们能够更好地了解集群和服务的健康状况。</p><p>&nbsp;</p><p></p><h1>通知噪音</h1><p></p><p>&nbsp;</p><p>我们遇到了一个问题：因为收集了大量指标，出现了通知噪音，其中有许多与不健康的应用程序和延迟问题相关的警报。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/54b5be34166d6997fbd1f598ce291ed3.png\" /></p><p></p><p>图12 警报管道</p><p>&nbsp;</p><p>基本的警报管道由Prometheus和AlertManager组成，它们会将警报转到PagerDuty。由于服务的重启或伸缩并不是很理想，所以我们决定使用Kubernetes，并实现服务的健康检查。</p><p>&nbsp;</p><p>Kubernetes中有三种类型的健康检查：活动、准备就绪和启动。对于Kafka来说，准备就绪探测没有用，因为通常情况下，HTTP服务器是不公开的。为了解决这个问题，我们实现了另一种方法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/097a38c1719342fdc00cd177d9f0d9a2.png\" /></p><p></p><p>图13 健康检查和Kafka</p><p>&nbsp;</p><p>在收到活动检查请求时，我们让一个broker执行一个基本操作，例如列出主题，如果响应成功，则检查通过。然而，在某些情况下，应用程序虽然在运行，但无法生产或消费消息，于是我们为消费者实现了更智能的健康状况检查。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4be53e0ddcad71c20e72145ac21a5a30.png\" /></p><p></p><p>图14 健康检查的实现</p><p>&nbsp;</p><p>Kafka的当前偏移量就是分区最后一个可用的偏移量，而提交的偏移量是消费者成功消费的最后一个偏移量。</p><p>&nbsp;</p><p>我们在进行健康状况检查时读取这些偏移量，这样就确定消费者运行是否正确：如果无法读取偏移量，则可能出现了问题，并且将消费者报告为不健康。如果可读取偏移量，我们将最后提交的偏移量与当前偏移量进行比较。如果它们相同，则没有追加的新消息，那么我们就认为消费者是健康的。如果最后一次提交的偏移量不同，我们就检查它是否与之前记录的最后一次提交的偏移量相同，这样可以知道消费者是否卡住了并需要重启。这个过程带来了更好的通知体验和用户体验。</p><p>&nbsp;</p><p></p><h1>消费延迟</h1><p></p><p>&nbsp;</p><p>我们有一个为电子邮件系统生成Kafka事件的系统。这些事件包含了一个模板，例如，一个“受攻击”模板，其中包含了受攻击网站的信息和攻击者的身份，以及元数据。</p><p>&nbsp;</p><p>我们监听这些事件，从注册中心获取电子邮件模板，对其进行填充，并将其发送给客户。然而，我们开始遇到负载问题：生产事件的速率出现了峰值，导致了消费延迟，影响到了重要的OTP和SLO。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/34/349c3756789c117c633e980357a96654.png\" /></p><p></p><p>图15 消费延迟</p><p>&nbsp;</p><p></p><h1>批次处理</h1><p></p><p>&nbsp;</p><p>我们开始研究通过不同的解决方案来解决这个问题，最初的解决方案是扩展分区和消费者的数量，但并没有获得显著的改进。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0c/0ced7dc886450bff799e2662c5719ff8.png\" /></p><p></p><p>图16 批次处理</p><p>&nbsp;</p><p>我们决定使用一种更简单但更有效的方法——批次处理，即一次处理一定数量的消息、执行转换和分发。这种方式被证明是有效的，让团队可以轻松地应对高生产速率。</p><p>&nbsp;</p><p></p><h1>文档化</h1><p></p><p>&nbsp;</p><p>在开发SDK时，我们发现许多开发人员在使用时会遇到问题。有些人发现有bug，有些人不确定如何实现某些功能或不知道一些错误是什么意思。为了解决这个问题，我们在Google Chat上创建了频道，让他们来问我们问题。我们有一个值班的人负责回复问题，并在我们的维基页上记录我们发现的问题和答案。这有助于改善SDK的整体用户体验。</p><p>&nbsp;</p><p></p><h1>结论</h1><p></p><p>&nbsp;</p><p>我们总结了四个教训：</p><p>&nbsp;</p><p>始终在灵活性和简单性之间找到恰当的平衡：可配置的设置步骤提供了更大的灵活性，而更简单的设置有助于进行跨不同管道的标准化。可见性：尽早将指标添加到SDK中，这样可以帮助团队了解系统的行为并做出更好的决策，特别是在发生故障时。契约：建立强大的、严格的契约，让人们能够很好地了解主题内部发生了什么，知道谁在写入和读取消息。将所做的事情记录下来，这样就不需要花时间答疑解惑或帮助人们调试问题。这可以通过Google Chat和维基页等方式来实现。</p><p>&nbsp;</p><p>我们遵循这些原则，改进了我们的系统，让我们的客户感到满意，即使是在面临高度压力的情况下。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/kafka-clusters-cloudflare/\">https://www.infoq.com/articles/kafka-clusters-cloudflare/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/9c32f07d324895a395c8b8fce\">Kafka&nbsp;往事——揭露&nbsp;Kafka&nbsp;推出&nbsp;Kafka&nbsp;Streams 背后原因</a>\"</p><p><a href=\"https://xie.infoq.cn/article/876c9c6bad7dde44e673736ba\">深度剖析：Kafka 请求是如何处理的? 看完这篇文章彻底懂了</a>\"</p><p><a href=\"https://xie.infoq.cn/article/d076332e1029d6ad453796587\">Kafka 原理——Kafka 为何如此之快？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/20714291d34ee83e7b3540970\">用好 kafka，你不得不知的那些工具</a>\"</p>",
    "publish_time": "2023-06-27 09:42:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "300毫秒分胜负：维基百科的总阻塞时间优化之道",
    "url": "https://www.infoq.cn/article/rEoBohpb2NMs8Qe0lwuT",
    "summary": "<p></p><p>大家有没有遇到过响应缓慢、卡顿崩溃的垃圾网站？遇上这类性能缺陷，脾气火爆的朋友往往果断选择：</p><p></p><p>狂点鼠标；退出走人，拉低客源转化率；搜索引擎排名因此下降。</p><p>&nbsp;</p><p>三年多来，维基百科的移动版网站也深受一段JavaScript代码的戕害。在低端手机上，这段JS代码的页面加载时间可能超过600毫秒，大大影响了用户的交互体验。</p><p>&nbsp;</p><p>在本文中，我们将一同了解如何通过几个简单步骤，让任务执行时间有效缩短约50%。</p><p>&nbsp;</p><p></p><h2>总阻塞时间：长任务的重要性</h2><p></p><p>&nbsp;</p><p>在JS执行方面，600毫秒似乎并不是什么无法接受的问题。但我们不妨想象这样的场景：当这段600毫秒的JS代码开始执行时，用户恰好想在加载过程中单击某个按钮。因为特定时段内浏览器的主线程只能处理一个任务，所以用户必须等待以下步骤完成后才能获得操作反馈：</p><p>JS任务花600毫秒执行完毕点击处理任务随后开始执行浏览器执行必要的渲染步骤，最终更新页面内容</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc4353141860b08acebd04f15d6c071a.png\" /></p><p></p><p>&nbsp;长任务可能导致视觉更新延迟，拖慢点击程序的处理速度</p><p>&nbsp;</p><p>每个步骤都需要消耗时间，而任何超过100毫秒的响应速度都会给用户带来非常明确的延迟感受。正因为如此，谷歌将一切耗时超过50毫秒的任务定性为“长任务”，认为其会影响页面对用户输入的响应。他们甚至专门为此制定了“总阻塞时间”（TBT）的指标。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f48cd6d15548e50fffe6c5acbd1c0ece.png\" /></p><p></p><p>这里有两个长任务（大于50毫秒）——分别耗时80毫秒和100毫秒</p><p>&nbsp;</p><p></p><h2>总阻塞时间是什么？</h2><p></p><p></p><p>所谓总阻塞时间，是指浏览器主线程上全部长任务在首屏内容绘制（FCP）和响应时间（TTI）之间的阻塞部分的总和。换言之，“阻塞部分”代表着每个长任务超出50毫秒之外的时间消耗。</p><p>&nbsp;</p><p>计算以下示例中的总阻塞时间：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc93882c7dd1fd6667b5ac649b9d8510.png\" /></p><p></p><p>80毫秒任务比50毫秒基准多出30毫秒，因此带来了30毫秒的总阻塞时间。30毫秒的任务不构成阻塞时间，因为其小于50毫秒且不属于长任务。100毫秒的任务比50毫秒基准多出50毫秒，因此带来了50毫秒的总阻塞时间。</p><p>&nbsp;</p><p>由于总阻塞时间对应每个长任务超过50毫秒部分的总和，所以示例中的最终结果是30毫秒+50毫秒=80毫秒。</p><p>&nbsp;</p><p>在常规移动硬件上进行测试时，谷歌建议站点的总阻塞时间应小于200毫秒。但维基百科上一项任务的执行就可能超过600毫秒——相当于总体上限的3倍。</p><p>&nbsp;</p><p>我们该如何改进性能？</p><p>&nbsp;</p><p></p><h2>如何降低总阻塞时间</h2><p></p><p></p><p>要降低这项指标，我们需要：</p><p>在首屏绘制和交互时间之间，减少主线程上的工作负载；在保证工作内容不变的情况下，将长任务拆分成多个不超过50毫秒的小任务。</p><p>&nbsp;</p><p>本文主要侧重第一种解决思路。</p><p>&nbsp;</p><p></p><h3>步骤1：删除不必要的JS代码</h3><p></p><p></p><p>HTML解析、绘制和垃圾收集都需要在主线程上运行，但引发总阻塞时间过长的罪魁祸首仍然是JS代码。毕竟有经验的前端开发者都知道，网站降速背后总有JS的身影。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/433672102478253df302a01f83c7b4fa.png\" /></p><p></p><p>&nbsp;在分析维基百科的移动站点时，我发现_enable方法占用了大部分执行时间。此方法负责对移动站点上的部分开展和折叠行为进行初始化。配置文件则显示，在_enable方法中，对jQuery .on(\"click\")方法的调用同样速度很慢。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d06cc7ae68dd43e5f267d54e144e8412.png\" /></p><p></p><p>&nbsp;这里的.on(\"click\")调用负责向内容中的几乎所有链接附加点击事件侦听器，这样如果点击的链接包含哈希片段，相应的部分就会被展开。对于链接较少的短文章，这部分性能影响几乎可以忽略不计。但对于像“美国”这类的长词条，其内容可能包含超过4000个链接，因此在低端设备上的执行时间会超过200毫秒。</p><p>&nbsp;</p><p>更糟糕的是，这种设计完全没有必要。侦听haschange事件的下游代码已经调用了与点击事件侦听器相同的方法。除非窗口位置已经指向链接目的地，否则点击链接会对checkHash方法调用两次——一次用于链接点击事件处理程序，另一次用于hashchange处理程序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/13/13942e921f869b9faff2c905b21df261.png\" /></p><p></p><p>这种情况下，最好的方法当然是直接删除这个JS代码块，在几乎不影响主线程功能的前提下直接省下近200毫秒。</p><p>&nbsp;</p><p>在分析过程中，请始终检查最耗时的部分，之后看看有没有能够优化或者删掉的代码。</p><p>&nbsp;</p><p>经验之谈：加快网站速度的首选方法，永远是删除JS代码。</p><p>&nbsp;</p><p></p><h3>步骤2：优化现有JS代码</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/362197fcbdd45cc08baa12d31434609a.png\" /></p><p></p><p>另一项性能审查显示，initMediaViewer方法需要约100毫秒的执行时间。此方法负责将点击事件侦听器附加到内容中的各个缩略图处，这样点击缩略图即可打开媒体查看器：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af6065373bd417b333e84ff2809189b7.png\" /></p><p></p><p>&nbsp;与步骤1中的链接示例类似，这种向页面上各个缩略图附加事件侦听器的方法不利于性能扩展。</p><p>&nbsp;</p><p>维基百科中的词条可能包含数千张相关图片。在这些多图页面上运行这段代码时，其执行时间可能超过100毫秒，必然增加页面的总阻塞时间。下面来看替代方法。</p><p>&nbsp;</p><p>答案就是事件委托。</p><p>&nbsp;</p><p>事件委托是一种强大的技术，允许我们将单个事件侦听器附加到单一元素，而该元素可以是大量其他元素的共同祖先。对于可以添加任意数量元素的用户生成内容，我们往往可以通过事件委托提高其执行效率。整个过程会用到事件冒泡，具体如下：</p><p>将事件侦听器附加至容器元素。在事件处理程序中使用event参数，检查event.target属性以查看事件源。可以选择使用event.target.closest(selector) API来检查祖先元素。如果该事件源就是我们所关注的元素或者其子元素，则进行处理。</p><p>&nbsp;</p><p>更新后的代码如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8aaf53465ba4a1eb5cc0bca10f02f9b8.png\" /></p><p>&nbsp;</p><p>我们修改了initMediaViewer方法，将一个点击事件侦听器附加到了包含所有图像的单一容器元素上。在onClickImage方法中，我使用ev.target.closest(selector) API来检查点击是否来自缩略图元素或者其子元素。如果都不是，代码会提前返回，因为这里只需要关注对缩略图的点击。如果是，则代码将处理该事件。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>我们分别通过两轮部署，将步骤1和步骤2中的优化发布到了生产环境。</p><p>&nbsp;</p><p>根据维基百科的综合性能测试数据，在Moto G（5）实机测试当中，首轮部署将总阻塞时间缩短了约200毫秒，第二轮部署进一步缩短了约80毫秒。总体而言，这两个步骤让Moto G（5）等移动设备访问长文章时的总阻塞时间缩短了约300毫秒。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/20/2056d8a871af7cecd3725420f4329660.png\" /></p><p></p><p>&nbsp;维基百科通过Moto G（5）在综合性能测试中访问“瑞典”词条</p><p>&nbsp;</p><p>虽然仍有进一步改进空间，且查询任务仍高于建议的低端设备延迟上限，但此次优化还是取得了显著成果。为了更大程度缩短总阻塞时间，后续可能有必要将任务拆分成更多小任务。</p><p>&nbsp;</p><p>此次试验表明，有针对性的小规模优化有望实现显著的性能改进。通过删除或优化特定代码片段，看似微小的更改也会对网站的整体性能产生重大影响。换言之，要想在所有设备上改善响应速度和浏览体验，并不一定要对代码库开展复杂且广泛的修改。有时候，小小一点调整就足以引发性能质变。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.nray.dev/blog/300ms-faster-reducing-wikipedias-total-blocking-time/\">https://www.nray.dev/blog/300ms-faster-reducing-wikipedias-total-blocking-time/</a>\"</p><p>&nbsp;</p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/09cb2a1d3c5cd5b93c0da4476\">维基百科技术架构</a>\"</p><p><a href=\"https://xie.infoq.cn/article/9919e3bc478edd1f8c977d629\">怎样搭建企业内部维基百科</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxODcyNjEzNQ==&amp;mid=2247541219&amp;idx=4&amp;sn=b0e51a1136ca72d6d71234141d774bb3&amp;chksm=9bd38c7baca4056dd2655d7bfb12204289ced324067453edbb85a9118ba1cd99c555a989a42d&amp;scene=27#wechat_redirect\">如何自己搞一个维基百科？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ca3d93fc3e4b05d5ecdbb19cd\">spark 实战之：分析维基百科网站统计数据 (java 版)</a>\"</p>",
    "publish_time": "2023-06-27 09:43:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面向AI大模型，腾讯云首次完整披露自研星脉高性能计算网络",
    "url": "https://www.infoq.cn/article/y3iGlmRCOgSvBqWoO03W",
    "summary": "<p></p><blockquote>&nbsp;AIGC的爆发除了带来算力上的挑战，对网络的要求也达到了前所未有的高度。</blockquote><p></p><p></p><h2>星脉高性能计算网络：最大带宽达3.2T，可支持10万卡超大算力集群</h2><p></p><p>&nbsp;</p><p>6月26日，<a href=\"https://www.infoq.cn/article/lUP7E2xx5OCMIUEMdhEr\">腾讯云</a>\"首次对外完整披露自研<a href=\"https://www.infoq.cn/article/N8Wcppi2vFxkh3C0VdAw\">星脉高性能计算网络</a>\"：星脉网络具备业界最高的3.2T通信带宽，能提升40%的GPU利用率，节省30%~60%的模型训练成本，为AI大模型带来10倍通信性能提升。基于腾讯云新一代算力集群HCC，可支持10万卡的超大计算规模。</p><p>&nbsp;</p><p>AIGC的火爆带来AI大模型参数量从亿级到万亿级的飙升。为支撑海量数据的大规模训练，大量服务器通过高速网络组成算力集群，互联互通，共同完成训练任务。大集群不等于大算力，相反，GPU集群越大，产生的额外通信损耗越多。大带宽、高利用率、信息无损，是AI大模型时代网络面临的核心挑战。</p><p>&nbsp;</p><p>千亿、万亿参数规模的大模型，训练过程中通信占比最大可达50%，传统低速网络的带宽远远无法支撑。同时，传统网络协议容易导致网络拥塞、高延时和丢包，而仅0.1%的网络丢包就可能导致50%的算力损失，最终造成算力资源的严重浪费。</p><p>&nbsp;</p><p>基于全面自研能力，腾讯云在交换机、通信协议、通信库以及运营系统等方面，进行了软硬一体的升级和创新，率先推出业界领先的大模型专属高性能网络——星脉网络。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/76/76f78675e9b823b413a7cdb208711259.png\" /></p><p></p><p>腾讯云副总裁王亚晨表示：“星脉网络是为大模型而生。它所提供的大带宽、高利用率以及零丢包的高性能网络服务，将助力算力瓶颈的突破，进一步释放AI潜能，全面提升企业大模型的训练效率，在云上加速大模型技术的迭代升级和落地应用。”</p><p>&nbsp;</p><p>在硬件方面，星脉网络基于腾讯的网络研发平台，采用全自研设备构建互联底座，实现自动化部署和配置。</p><p>&nbsp;</p><p>在软件方面，腾讯云自研的TiTa网络协议，采用先进的拥塞控制和管理技术，能够实时监测并调整网络拥塞，满足大量服务器节点之间的通信需求，确保数据交换流畅、延时低，实现高负载下的零丢包，使集群通信效率达90%以上。</p><p>&nbsp;</p><p>此外，腾讯云还为星脉网络设计了高性能集合通信库TCCL，融入定制化解决方案，使系统实现了微秒级感知网络质量。结合动态调度机制合理分配通信通道，可以避免因网络问题导致的训练中断等问题，让通信时延降低40%。</p><p>&nbsp;</p><p>网络的可用性，也决定了整个集群的计算稳定性。为确保星脉网络的高可用，腾讯云自研了端到端的全栈网络运营系统，通过端网立体化监控与智能定位系统，将端网问题自动定界分析，让整体故障的排查时间由天级降低至分钟级。同时，大模型训练系统的整体部署时间从19天缩减至4.5天，保证基础配置100%准确。</p><p></p><h2>腾讯数据中心网络技术演进历程</h2><p></p><p>&nbsp;</p><p>星脉网络全方位的升级背后，是腾讯数据中心网络历经三代技术演进的成果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/756c4522df28f945ad51839068cce57d.png\" /></p><p></p><p>在腾讯发展初期，数据中心网络流量主要由用户访问数据中心服务器的南北向流量构成，网络架构以接入、汇聚、出口为主。这一阶段主要使用了商用网络设备，搭建标准化数据中心网络，支撑QQ在线人数增长超过1亿，服务器规模增长超10万。</p><p>&nbsp;</p><p>随着大数据和云计算的兴起，服务器之间的东西向流量逐渐增多，云租户对网络产生了虚拟化和隔离的要求。数据中心网络架构逐渐演变为同时承载南北向和东西向流量的云网络架构，腾讯云构建了全自研网络设备与管理系统，打造超大规模数据中心网络，服务器规模近200万台。</p><p>&nbsp;</p><p>如今随着<a href=\"https://www.infoq.cn/article/gsdQ55Gf5BkbiWx0NuWT\">AI大模型</a>\"的出现，腾讯云在国内率先推出高性能计算网络，采用东西向、南北向流量的分离架构。构建了独立的超大带宽、符合AI训练流量特征的网络架构，并配合自研软硬件设施，实现整套系统的自主可控，满足超强算力对网络性能的新需求。</p><p>&nbsp;</p><p>日前，腾讯云发布的新一代HCC高性能计算集群，正是基于星脉高性能网络打造，可以实现3.2T超高互联带宽，算力性能较前代提升3倍，为AI大模型训练构筑可靠的高性能网络底座。</p><p>&nbsp;</p><p>未来，腾讯云还将持续投入基础技术的研发，为各行各业的数智化转型提供有力的技术支撑。</p>",
    "publish_time": "2023-06-27 10:04:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI 如何使能千行百业？探秘华为云盘古大模型",
    "url": "https://www.infoq.cn/article/t9SEK7arwsmEIMPA0zBB",
    "summary": "<p>人工智能新的爆发点会出现在哪里？预训练大模型究竟能做些什么？对我们的生产生活又会有哪些影响？华为云和 InfoQ 联合出品《探秘云新知》，带你探秘「华为云盘古大模型」，解构最前沿的人工智能技术与行业应用！</p>",
    "publish_time": "2023-06-27 10:18:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "王志龙确认出席 ArchSummit 深圳，将分享《计算密集型应用以 Service Mesh 为支点解决分布式问题的探索与实践》话题",
    "url": "https://www.infoq.cn/article/HTZpkKeTBD1KADAAJTMu",
    "summary": "<p>7&nbsp;月&nbsp;21&nbsp;日&nbsp;-&nbsp;22&nbsp;日，&nbsp;在&nbsp;<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">ArchSummit&nbsp;全球架构师峰会（深圳站）</a>\"，京东搜索与推荐部架构师王志龙，将于会上发表题为《计算密集型应用以&nbsp;Service&nbsp;Mesh&nbsp;为支点解决分布式问题的探索与实践》的演讲，分享和探讨京东以搜索业务为先行载体，落地&nbsp;Service&nbsp;Mesh&nbsp;的技术挑战和经验，以及在多个方面取得技术收益赋能业务的探索与实践。</p><p></p><p>王志龙拥有&nbsp;10&nbsp;年互联网一线开发经验，Kubernetes&nbsp;Contributor，Layotto&nbsp;Committer，专注云原生领域，擅长性能极限优化。曾工作于腾讯、阿里，参与过微信云平台从&nbsp;0&nbsp;到&nbsp;1&nbsp;建设，阿里 Serverless&nbsp;C++ 和&nbsp;Golang&nbsp;Runtime&nbsp;研发及落地。目前在京东负责搜索微服务治理和新一代&nbsp;Serverless&nbsp;云化平台研发工作。</p><p></p><p>相信通过王志龙的分享，你将收获搜推广等计算密集型应用落地&nbsp;Mesh&nbsp;的挑战和经验，了解到使用&nbsp;Mesh&nbsp;技术解决分布式问题提升性能的典型案例。</p><p></p><p>除上述议题外&nbsp;，ArchSummit&nbsp;深圳还将围绕<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">基础架构技术</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1532?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">DataOps、Data&nbsp;Fabric&nbsp;等高效数据开发与服务模式</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1534?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">Mesh&nbsp;技术实践案例</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1535?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">QUIC&nbsp;传输和架构优化</a>\"等进行分享。</p><p></p><p>数十位业界专家，上百个国内外一线大厂前沿技术案例，一定会给你带来很多全新的开发灵感。期待与你线下交流！&nbsp;现在购票，享&nbsp;9&nbsp;折特惠，立省&nbsp;¥880！咨询购票请联系&nbsp;18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/9d/aa/9d6a27547062ee2e089f91bdc4ba1eaa.png\" /></p><p></p>",
    "publish_time": "2023-06-27 11:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI模型只能部署在云端？高通白皮书发布：混合AI是AI的未来",
    "url": "https://www.infoq.cn/article/KqqfSwMXaXGqoL8YcQOa",
    "summary": "<p>近日，<a href=\"https://www.infoq.cn/article/NjuBjKw2daCjtHzqejcj\">高通</a>\"技术公司正式发布白皮书《混合AI是AI的未来》。高通在白皮书中提到，随着生成式 AI 正以前所未有的速度发展以及计算需求的日益增长，AI 处理必须分布在云端和终端进行，才能实现 AI 的规模化扩展并发挥其最大潜能——正如传统计算从大型主机和瘦客户端演变为当前云端和边缘终端相结合的模式。与仅在云端进行处理不同，混合 AI 架构在云端和边缘终端之间分配并协调 AI 工作负载。云端和边缘终端如智能手机、 汽车、个人电脑和物联网终端协同工作，能够实现更强大、更高效且高度优化的 AI。</p><p>&nbsp;</p><p>高通产品管理高级副总裁兼AI负责人Ziad Asghar在媒体沟通会上进一步分享了高通对于混合AI的愿景，以及如何结合自身的产品技术优势，让混合AI的愿景成为现实。</p><p>&nbsp;</p><p>Ziad Asghar表示，“我们正在引领混合AI愿景的实现。对隐私和安全要求比较高的终端侧工作负载，可以继续通过边缘云，完全在终端侧完成。对于其它的模型工作，我们也可以和云服务供应商合作完成。通过在云端和边缘侧终端分布工作负载，我们能够大幅度减少云端的处理量。混合AI的优势在于，即使不同终端处理能力不尽相同，但仍然能够提供相近的体验，同时带来包括成本、能耗、隐私与安全、个性化等优势；还能通过出色的5G连接技术确保信息在端到端之间进行高效传输。”</p><p></p><h2>混合 AI 对生成式 AI 规模化扩展至关重要</h2><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/FRcz5vjOvl3bM2d57opX\">ChatGPT</a>\" 的爆火掀起生成式AI热潮。自 2022 年 11 月推出后，ChatGPT仅用了短短两个月时间月活用户便达到 1 亿，成为有史以来增长速度最快的消费类应用和第一个杀手级的生成式 AI 应用。</p><p>&nbsp;</p><p>作为一项变革性的技术，生成式AI颠覆了原有的工作、娱乐方式，并拥有非常丰富的应用领域，应用数量也在不断激增。具体而言，生成式AI的应用主要包括搜索、内容生成、生产力、代码编写等等，能够在数秒之内通过大型基础模型创作内容。数据显示，AI 正迎来大爆发时期，目前已有超过 3000 个可用的生成式 AI 应用和特性。</p><p>&nbsp;</p><p>据初步估计显示，生成式 AI 市场规模将达到 1 万亿美元，广泛覆盖生态链的各个参与方。为把 握这一巨大机遇，并推动 AI 成为主流，计算架构需要不断演进并满足大规模生成式 AI 日益增长的处理和性能需求。</p><p>&nbsp;</p><p>拥有数十亿参数的众多生成式 AI 模型对计算基础设施提出了极高的需求。因此，无论是为 AI 模型优化参数的 AI 训练，还是执行该模型的 AI 推理，至今都一直受限于大型复杂模型而在云端部署。</p><p>&nbsp;</p><p>AI 推理的规模远高于 AI 训练。尽管训练单个模型会消耗大量资源，但大型生成式 AI 模型预计每年仅需训练几次。然而，这些模型的推理成本将随着日活用户数量及其使用频率的增加而增加。 在云端进行推理的成本极高，这将导致规模化扩展难以持续。</p><p>&nbsp;</p><p>高通认为，混合 AI 能够解决上述问题，正如传统计算从大型主机和瘦客户端演变为当前云端和 PC、智能手机等边缘终端相结合的模式。</p><p>&nbsp;</p><p>具体来说，混合 AI 指终端和云端协同工作，在适当的场景和时间下分配 AI 计算的工作负载，以提供更好的体验，并高效利用资源。在一些场景下，计算将主要以终端为中心，在必要时向云端分流任务。 而在以云为中心的场景下，终端将根据自身能力，在可能的情况下从云端分担一些 AI 工作负载。混合 AI 架构(或仅在终端侧运行 AI)，能够在全球范围带来成本、能耗、性能、隐私、安全和个性化优势。</p><p></p><h2>利用边缘侧终端规模化扩展生成式AI</h2><p></p><p>&nbsp;</p><p>Ziad Asghar表示，当前很多人将生成式AI和云端联系在一起，通过高通的技术，能够让这些出色的用例在边缘侧实现。“生成式AI对众多领域产生了广泛影响，目前有大量的新兴应用需要生成式AI能力，且已经拥有了庞大的用户规模，市场上也出现了众多非常庞大的模型。我们认为，要真正释放生成式AI的全部潜能，AI需要在边缘侧运行，这也是高通一直努力的方向，我们相信凭借我们的技术，我们能够带来遥遥领先的终端侧生成式AI体验。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6aee7603fbe8ae8b6a5c42a25217c5d9.png\" /></p><p>&nbsp;</p><p>据了解，目前生成式AI的应用能够在高通所推出的几乎所有主要产品线中使用。举例来说，手机作为高度个性化的设备，能够通过生成式AI成为消费者真正意义上的数字助手，它可以接受用户的所有需求，且甚至无需联网就能够完成任务，并完全通过大型基础模型（例如文本生成文本模型LLaMA）与用户交流。此外，生成式AI能够基于视频会议的语音转录内容，制定任务清单，并自动生成完整的演示文稿直接供用户使用，使生产力能够成倍增长。骁龙计算平台拥有专用的硬件单元，能够原生支持生成式AI在本地使用。</p><p>&nbsp;</p><p>在XR方面，生成式AI能够根据终端侧所提供的用户信息进行定制和优化，为用户带来完全不同的独特虚拟世界体验。Ziad Asghar表示，如果只在云端运行，则不具备终端侧的情境信息，因此利用终端能够带来更好的用户体验。</p><p>&nbsp;</p><p>汽车领域的用例也非常丰富。在座舱中使用对话式AI，能够帮助用户规划路线，在去餐厅的路上推荐用餐选项，或者在上班途中列出今日的工作事项。生成式AI还可以根据出发点和目的地信息，结合汽车的丰富传感器数据制定不同的路线规划，找到最佳路线。</p><p>&nbsp;</p><p>物联网领域，生成式AI能够助力打造面向专业领域的GPT类型模型，以及帮助用户完成不同任务的IoT助手。如果来到一个新的城市，生成式AI能够帮助提供旅行目的地推荐。此外它还适用于其他的垂直领域，如医疗、零售、酒店管理等等。</p><p>&nbsp;</p><p>随着强大的生成式AI模型不断缩小，以及终端侧处理能力的持续提升，混合AI的潜力将会进一步增长。参数超过10亿的AI模型已经能够在手机上运行，且性能和精度达到与云端相似的水平。不久的将来，拥有100亿或更高参数的模型将能够在终端上运行。</p><p></p><h4>全栈AI优化</h4><p></p><p>&nbsp;</p><p>Ziad Asghar表示，目前高通已经实现了全球首个Android手机上的Stable Diffusion终端侧演示。Stable Diffusion是一个参数超过10亿的超大神经网络基础模型，能够基于输入的文本提示生成图片。高通的这一终端侧演示是在飞行模式下进行的，通过高通的全栈AI优化，这一模型能够完全在终端侧运行，实现在15秒内完成20步推理，生成饱含细节的图像。</p><p>&nbsp;</p><p>高通面向Stable Diffusion进行了全栈AI优化。2022年6月，高通推出了专门面向边缘侧AI的领先软件栈产品——高通AI软件栈，能够从软件层面进行模型优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/47/478da8f0ce15cc4c8ee1782736f96990.png\" /></p><p></p><p>Ziad Asghar表示，在云端服务器上训练的模型一般采用32位浮点运算（FP32），这意味着完成模型推理需要大量的处理工作。“我们希望通过整数运算模式和量化技术进行AI推理，即时获取模型推理结果。针对Stable Diffusion，我们所采用的是8位整数运算（INT8）。去年年底在第二代骁龙8移动平台上，我们已经进一步支持了4位整数运算（INT4）能力。我们的硬件、软件以及工具设计也都考虑了如何充分利用这一关键优势。”</p><p>&nbsp;</p><p>目前高通能够支持<a href=\"https://www.infoq.cn/article/hzqWDTkJbUCLghFPXmel\">Stable Diffusion</a>\"这一超过10亿参数的模型在终端侧运行，但许多关键的生成式AI模型，比如文本生成图像、自然语言处理、编程、图像理解、图像创作等，模型规模一般在10亿到100亿参数之间。Ziad Asghar表示未来几个月内，高通将有望支持参数超过100亿的模型在终端侧运行。</p><p></p><h2>对话Ziad Asghar：AI大模型会在C端和B端同步落地</h2><p></p><p>&nbsp;</p><p>在媒体沟通会上，Ziad Asghar接受了 InfoQ 在内的部分媒体采访。以下为采访实录，经编辑。</p><p>&nbsp;</p><p>问：刚才说到，高通在几个月之后就可以实现在终端侧处理参数规模达100亿的模型，你们会用什么样的大语言模型？</p><p>&nbsp;</p><p>Ziad Asghar：我们看到目前大语言模型的模态非常丰富，并且已经出现了多模态模型，包括文本生成图片、文本生成文本、文本生成视频，甚至还有图片生成文本、图片生成视频等方式。这将揭开新的序幕，开启许多人们未曾想象过的全新用例。我们已经开始面向不同场景和用例需求的模型展开工作。</p><p>&nbsp;</p><p>问：对于文本生成文本模型，会不会考虑使用来自于Meta的开源LLaMA模型？</p><p>&nbsp;</p><p>Ziad Asghar：我们对模型的应用持有非常开放的态度。针对中国市场的模型，我们会专注于面向本地语言和使用场景的模型调优和训练，以让用户能够根据不同的需求，随时随地地使用模型。我们当前在关注不同的开源模型，同时我们也将与众多的中国合作伙伴携手，实现这些模型在本土市场的终端侧部署。</p><p>&nbsp;</p><p>问：智能手机端侧运行生成式AI会成为未来的大趋势吗？</p><p>&nbsp;</p><p>Ziad Asghar：我们认为这将是一个非常值得期待的重要趋势。所有不同的终端在生成式AI的助力之下，将为消费者带来更强大的吸引力。终端的可用性、娱乐性和生产力价值将远远超越当前的水平。</p><p>&nbsp;</p><p>问：您刚刚提到的Stable Diffusion成功在安卓手机上运行，不到15秒生成图片，这个是完全在终端侧上就能运行吗？我们大约什么时候能用上？以什么样的方式用上？</p><p>&nbsp;</p><p>Ziad Asghar：目前我们已经能够完全在终端侧运行Stable Diffusion，无需连接云端，即使是将手机调到飞行模式也可以。但是目前只有采用高通技术的终端能够实现。对于具体的用例，举例来说，我们可以将Stable Diffusion的能力集成到相机应用中，用户在任何一个地点拍摄照片，再提出需求，例如将照片背景改为夕阳之下的万里长城，Stable Diffusion就能够完成这一任务。此外还有其他的用例，比如数字助手、生产力应用等。我们相信通过与合作伙伴的共同努力，用户将能在今年体验到这些终端侧用例。</p><p>&nbsp;</p><p>问：高通实现终端侧运行AI大模型，在硬件、软件层面的核心技术优势是什么？未来基于其他移动芯片平台的产品是否也会跟进这一能力？</p><p>&nbsp;</p><p>Ziad Asghar：长期以来，高通致力于持续基于我们所打造的硬件、软件和工具资源，驱动生成式AI在终端侧的规模化扩展。首先在硬件方面，我们在既定功耗下的处理能力领先于市场上的其他竞争对手，这让我们能够在运行生成式AI用例时实现非常出色的表现。凭借我们的研究投入，我们能够在终端侧利用量化技术，在处理相同用例时大幅节省功耗和算力，同时完全不影响准确性，这是我们的竞争对手做不到的。另外一个优势在软件方面，我们提供高通AI引擎Direct以及Qualcomm AI Studio等软件工具，让这些模型能够完全在终端侧运行。</p><p>&nbsp;</p><p>问：以聊天机器人对代表的生成式AI应用要有好的使用体验，一个比较大的挑战是时延，每个指令（token）的时延需要在毫秒级别，如何才能将这类应用部署在终端侧，并且拥有不错的体验？</p><p>&nbsp;</p><p>Ziad Asghar：我们能够提供非常高效的token生成速率，完全不会因为时延影响到用户的体验。时延对于用户体验的确至关重要，而得益于我们的技术，我们的每秒token生成速率能够为用户提供流畅的体验。</p><p>&nbsp;</p><p>问：高通的AI硬件在处理AI应用时比CPU有明显优势，接下来是否会增加transformer核心让端侧生成式AI的体验更好？</p><p>&nbsp;</p><p>Ziad Asghar：高通AI引擎涵盖了CPU、GPU以及Hexagon处理器，从而能够在最合适的位置进行AI处理。谈到在高通AI硬件上进行AI处理的优势，除了我们的硬件引擎有着非常强大的处理能力外，我们也在去年推出了专门面向transformer处理的领先技术，能够大幅提升transformer处理效率。所以在硬件层面我们的技术已经完备，能够支持在终端侧获得大幅提升的transformer处理表现。</p><p>&nbsp;</p><p>问：我们注意到高通今天正式将自研AI技术的起步时间点定在了“十年前”，也就是曾经的Zeroth处理器。我们想知道，当年Zeroth的SNN网络架构在如今的骁龙移动平台上得到了多大程度的继承？</p><p>&nbsp;</p><p>Ziad Asghar：高通长期专注于脉冲神经网络（SNN）研究，骁龙820平台是我们最先应用这一技术的产品。我们的大量工作也得益于这一技术研究，我们在这一技术基础之上不断积累，覆盖了各个领域，包括技术、硬件增强和软件等等。我认为这也是我们今天能够在终端侧取得如此领先和丰富的AI能力的原因之一。</p><p>&nbsp;</p><p>问：您认为目前的AI大模型在C端和B端，哪侧会更快落地？</p><p>&nbsp;</p><p>Ziad Asghar：我认为应该会在C端和B端同步落地，同时高通也有能力来支持这些模型落地。无论是智能手机、VR、AR还是汽车等面向消费者的智能设备，亦或是企业级的搭载骁龙计算平台的PC、智能手机等设备。我们的产品和技术能够支持面向专业领域的GPT模型以及丰富的模型模态（比如文本生成图片等），这能够为C端和B端都带来巨大可能性，为所有人带来出色体验，无论是在家中还是在工作场所。甚至只要人们用手机，就可以感受到AI带来的优势。</p><p>&nbsp;</p><p>问：生成式AI在汽车座舱、智能驾驶上的应用进展如何？需要调用数据量和模型形式和手机端有哪些本质不同？边缘侧的低功耗、低时延，是结合5G座舱芯片或大算力芯片共同实现的吗？</p><p>&nbsp;</p><p>Ziad Asghar：第一个问题，关于生成式AI赋能的数字座舱体验，大家可以想象一下，用户可以体验到真正意义上的“和自己的车对话”。你可以告诉你的车：导航带我去机场，但是在去机场的路上，我要找个地方吃个汉堡，再找个地方喝某种口味的咖啡，顺便把我之前干洗的衣服取了。在数字座舱里，我们可以为用户提供真正意义上的虚拟助手。对于汽车应用的不同模态，其要求会更加严格，并且需要更高的准确性。因为与其他商用终端不同，在汽车里出现任何一个小错误都可能带来非常严重的后果。所以我们在确保提供最佳体验的同时，也要确保极高的准确性。</p><p>&nbsp;</p><p>第二个问题，在汽车领域我们需要将多模态相结合，同时结合雷达、激光雷达、以及摄像头等传感器数据，从而让我们在使用生成式AI规划路线时，获得最佳的效果。</p><p>&nbsp;</p><p>第三个问题，汽车需要非常强大的处理能力。一方面，汽车领域的生成式AI用例需要非常丰富的终端侧处理能力，同时，它还需要通过高速低时延的5G连接，在需要的情况下利用云端资源进行处理。与我们其他产品线的产品相比，我们的汽车产品通常能够提供更多的生成式AI处理能力。</p><p>&nbsp;</p><p>问：目前在PC以及其他平台上，NPU通常是一个独立于CPU、GPU的计算单元。但是在骁龙移动平台上，CPU、GPU、DSP、ISP、甚至调制解调器都具备一定的AI计算能力，这就意味着骁龙平台的AI计算架构实际上是一种分布式的设计。那么这是否会加大软件开发的难度？或者是否会出现某些应用不能完整调用全部AI计算单元的情况？</p><p>&nbsp;</p><p>Ziad Asghar：我们的平台采用的是异构计算架构，高通AI引擎包括Hexagon处理器、CPU、GPU以及ISP。我们相信AI是能够赋能整个平台的通用技术，无论是摄像头还是图像、调制解调器、视频、音频、语音等等都可以利用AI技术。同时，基于我们在软件方面进行的大量投入，无论要在终端侧运行何种应用，高通AI引擎都能提供充沛、强大的算力。</p><p>&nbsp;</p><p>问：终端设备上的AI模型是否对用户的个人数据进行处理？</p><p>&nbsp;</p><p>Ziad Asghar：针对用户所担心的个人隐私数据保护，终端侧处理恰恰能够解决这一问题。正如我刚刚所讲，无论是10亿参数的模型，还是100亿参数的模型，如果我们能够完全在终端侧来运行，比如用户发出一个查询，终端接收之后能够独立完成推理，那么所有相关的查询信息和数据都会留在终端上，不会离开终端，这也是边缘处理相对于云端处理的独特优势所在，因为如果要在云端进行查询，那么数据就要先发送到云端，处理完再从云端回到终端。</p>",
    "publish_time": "2023-06-27 11:55:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "倒计时3天！中国信通院2023云边协同大会详细议程公布 | Q推荐",
    "url": "https://www.infoq.cn/article/aYlF2xGAfiughkleXaNA",
    "summary": "<p>由中国信息通信研究院、中国通信标准化协会主办的第三届“云边协同大会”将在京举办。本届大会以“云智物联，边筑算新”为主题，聚焦<a href=\"https://xie.infoq.cn/article/f4892c13764e68aa8a3c918ae\">分布式云</a>\"、<a href=\"https://xie.infoq.cn/article/c574df54e396b1973376ba537\">边缘计算</a>\"、AIoT 平台等领域前沿焦点，搭建政产学研用交流对接平台，深化协同开放合作，推动产业创新发展。</p><p></p><p>本届大会将于 2023 年 6 月 30 日在北京歌华开元大酒店举办，共设置一场开幕式及主论坛，三场高质量发展论坛，邀请政产学研用行业内大咖齐聚一堂，深入探讨产业发展现状，洞察未来演进趋势。同时，中国信通院将在本届大会中发布白皮书及研究报告、技术创新与最佳实践案例、评估评测等最新成果，全面推动我国云边协同应用建设走深向实。</p><p></p><p>目前大会持续火热报名中，期待您的莅临！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/efb3707d68d98089a80074473c71f434.jpeg\" /></p><p></p><p>洞察前沿，透视未来。中国信通院2023云边协同大会，与行业“先行者”共同把握云边协同产业发展新机遇。如果你想现场倾听“算力时延圈、分布式云、边缘计算、AIoT平台”等前沿焦点的干货内容，想与行业大咖深入探讨产业发展现状、创新实践方案及未来发展趋势，赶紧扫描图中二维码或者<a href=\"http://www.idcquan.com/Special/CECS2023/\">点击进行报名</a>\"吧！</p>",
    "publish_time": "2023-06-27 11:57:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]