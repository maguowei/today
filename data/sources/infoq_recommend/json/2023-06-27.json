[
  {
    "title": "构筑成功之桥：从组织文化到SRE实施",
    "url": "https://www.infoq.cn/article/7XbV7urmZm9bAs4EwjUg",
    "summary": "<p></p><h2>引言</h2><p></p><p></p><p>西门子健康集团的teamplay数字健康平台和应用是一个大型的分布式组织，由25个团队组成，拥有医疗领域不同的众多数字服务。</p><p></p><p>该组织经历了一次SRE转型，这是一次深刻的社会性技术变革，改变了生产经营的技术、流程和文化。在本文中，我们将重点讨论：</p><p>在SRE转型之初，如何在生产经营方面评估组织文化？如何创建一个随着时间推移不断累积小规模文化变革的路线图？领导层如何推动必要的文化变革？</p><p></p><h2>评估组织文化的必要性</h2><p></p><p></p><p>在引入SRE时，很容易直接跳到变革的技术部分，并开始着手实施新的工具、基础设施和仪表盘。</p><p>毋庸置疑的是，这些制品本身不足以动摇一个组织的生产运营方式。SRE转型在很大程度上是一个社会性技术（sociotechnical）变革。</p><p></p><p>变革的“社会（socio）”部分需要从SRE转型的一开始就发挥同样的作用。</p><p></p><p>在这种情况下，从生产运营的角度评估组织目前的文化是很用的。这会带来如下的好处：</p><p>它能够让推动转型的SRE教练了解当前组织中对生产运营的态度。它能够揭示组织在信息共享、决策制定、协作、学习和其他方面有可能加速或阻碍SRE转型的运营方式，这些方式可能是非常微妙且难以察觉的。它能够激发组织转向SRE的热情，并且能够首次预测演进的速度。</p><p></p><p>既然存在这些收益，那么该如何从生产运营的角度来评估组织文化呢？这就是下一节的主题。</p><p></p><h2>如何评估组织文化？</h2><p></p><p></p><p>有种流行的组织文化拓扑学是由Ron Westrum提出的Westrum模型。该模型根据组织处理信息的方式，将文化分为病态型、官僚型和生机型：</p><p>病态型文化是以权力为导向的官僚型文化是以规则为导向的生机型文化则是以绩效为导向的</p><p></p><p>基于Westrum模型，谷歌的DevOps研究和评估（<a href=\"https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture\">DORA</a>\"）项目通过严格的研究发现，生机型文化会促成高效的软件交付。根据Westrum模型，生机型高效文化包含如下六个方面：</p><p>高度合作训练信使共担风险鼓励交流失败时追根溯源接纳新颖的想法</p><p></p><p>这六个方面可以用来评估一个组织的运营文化。要做到这一点，需要将这六个方面映射到SRE中，以了解文化的目标状态。基于我的图书“<a href=\"https://www.informit.com/store/establishing-sre-foundations-a-step-by-step-guide-to-9780137424658\">Establishing SRE Foundations</a>\"”，下表提供了如下的映射。</p><p></p><p></p><p></p><p>有了上表中定义的目标状态，SRE教练就可以分析他们的组织目前距离目标文化状态还有多远。</p><p></p><h2>随着时间推移积累小规模的文化变革</h2><p></p><p></p><p>SRE教练了解现状之后，就可以开始SRE转型的活动了。这些活动将会包括技术、流程和行为的改变。为了推进这项运动，SRE教练需要寻找小的行为变化、对此进行庆祝，并按照这样的方式缓步前行，随时间推移进行积累。</p><p></p><p>例如，如下所列的小变化可以逐步促成更大的行为变化，随着时间的推移，将组织的文化逐步推向上一节中所描述的目标状态。</p><p></p><p></p><p></p><p>上表中的文化变革是通过正式领导和非正式领导的相互作用来推动的。我们将会在下一节描述这种推动力。</p><p></p><h2>正式领导和非正式领导的相互作用</h2><p></p><p></p><p>在每一个层级化的组织中，都会有一些领导者，他们因为在组织结构图中的位置而获取相应的权力。如果这些领导者在更广阔的组织内得到信任，那么他们努力的成果就会成倍增加，因为在组织内，他们有大量的追随者。</p><p></p><p>同时，在很多层级化的组织中，都存在非正式的领导者，他们并没有正式的权力，因为在组织结构图中他们没有正式的位置。但是，他们已经赢得了整个组织的信任。这种信任也能使他们努力的成果得到成倍增加，因为组织中有大量的人自愿跟随他们。</p><p></p><p>在下表中，我们总结了正式领导和非正式领导类型。</p><p></p><p></p><p></p><p>最左边和最右边的列描述了一个关于领导力的良好组合，它能够提供必要的环境，有利于在组织内自上而下和自下而上地推动SRE。在SRE转型的动态环境中，它能够保持一致性、稳定性并坚定信心。团队会认为正式的领导者支持SRE，而非正式的领导者则能够帮助推动整个组织必要的心态、技术和流程变革。这能够最大化SRE转型成功的可能性。</p><p></p><h2>来自一线的经验</h2><p></p><p></p><p>上述的文化评估方法帮助西门子健康集团的数字健康平台组织成功地推动业务向SRE方向发展。在本节中，我们将会介绍一些从SRE转型一线获取的真正经验。</p><p></p><h3>经验1：从一开始就让产品所有者参与进来</h3><p></p><p></p><p>我们得到的最深刻的经验之一就是让产品所有者从一开始就参与到SRE转型中。对于产品所有者来说，SRE的价值在于减少客户因为数字服务没有达到预期效果而导致的问题升级。这些问题升级是令人讨厌的、耗时的，并且会导致管理层不必要的关注。这为产品所有者参加SRE会议提供了动力，在这种会议上会定义SLO并讨论相关的流程。</p><p></p><p>在SRE会议上，产品所有者需要：</p><p>从业务角度提供关于客户最重要流程的背景知识根据会议中讨论的成本，评估更高的可靠性所带来的商业价值通过从一开始就参与SRE讨论，进一步了解生产运营了解如何以数据驱动的方式考虑对可靠性和新特性进行投资的优先级</p><p></p><h3>经验2：让开发人员的注意力首先放到生产环境上</h3><p></p><p></p><p>对于刚接触软件即服务的组织来说，主要的问题在于，开发人员不习惯关注生产环境。相反，在传统方式中，他们的世界以特性描述为始，以特性实现为终。在生产环境中运行特性并不在他们的关注范围之内。这就是我们的组织在SRE转型之初的情况。</p><p></p><p>在这种情况下，SRE转型之初最具影响力的里程碑事件就是让开发人员将注意力转移到生产环境上。这是一个80/20类型的里程碑，20%的努力带来了80%的改善。</p><p></p><p>对于开发人员来说，定义完美的SLO和错误预算策略并没有那么重要。相反，重要的是为开发人员提供基本的工具和初始的动力，让他们将注意力转移到生产环境中。在获取运维软件的新习惯时，定期花费时间对生产环境进行分析就成功一半了。</p><p></p><p>做到这些之后，应用SRE方法的准确性就可以逐步实现了。</p><p></p><h3>经验3：不要害怕让团队一开始就快速失败</h3><p></p><p></p><p>按照我们的经验，在定义最初的SLO时，团队一开始会倾向于高估其服务的可靠性。他们倾向于设置比服务的平均水平更高的SLO。同样的，他们倾向于设置比服务可以达到更严格的延迟SLO。</p><p></p><p>在这个初始阶段，试图说服团队放松最初的SLO是徒劳的，即便是历史数据有时候也无法说服团队。我们发现，快速失败的方式实际上是最有效的。</p><p></p><p>我们按照团队的建议设置了SLO，没有进行过多的争论。毫不意外，团队被大量的SLO告警淹没了。下一次SRE会议的主题不可避免地变成了团队无法处理大量的告警。</p><p></p><p>这使团队充分理解了他们的SLO决定的后果。于是，重新定义SLO的过程开始了。这正是我们需要的：一个来自生产环境是否满足SLO的强大反馈循环，这促成了对SLO的重新评估。</p><p></p><h3>经验4：构建正式领导者和非正式领导者的联盟</h3><p></p><p></p><p>我们发现，由正式领导者和非正式领导者组成的联盟在组织中倡导SRE是非常有用的。非正式的领导自学了SRE，对将它引入到组织中充满了热情。为了实现这一点，他们需要正式领导的支持，以便于团队投入SRE相关的工作。</p><p></p><p>非正式领导者需要向正式领导推销SRE方案，承诺减少因为服务中断所导致的客户问题升级。这些对话一般会发生在研发主管和运营主管身上。反过来，这些领导者需要向整个领导团队推销SRE方案，以便于将该主题列入组织的重大行动清单中。</p><p></p><p>这样做之后，就会形成一个强大的组合，即有足够多的正式领导者支持SRE，SRE列入到了组织的重大行动清单中，还有一群充满活力的非正式领导准备在整个组织内推动SRE。</p><p></p><p>这种组织状态有利于使用SRE实现成功的生产运营！</p><p></p><h2>总结</h2><p></p><p></p><p>对于刚刚接触数字化服务运营的软件交付组织来说，SRE转型是一个巨大的社会性技术变革。变化的速度在很大程度上取决于目前的组织文化。人们对生产环境运营的态度和看法才是需要搬掉的最大绊脚石，而不是人们日常使用的工具和仪表盘。</p><p></p><p>因此，在开始SRE转型之前评估组织文化是一项非常有用的工作。它能够让推动转型的SRE教练了解组织目前在运营文化方面的情况。它进一步点燃了一个有价值的思考过程，也就是如何将文化发展到SRE。</p><p></p><h5>作者简介：</h5><p></p><p>Vladyslav Ukis博士毕业于德国Erlangen-Nuremberg大学的计算机科学专业，后来又毕业于英国曼彻斯特大学。他在每次毕业后都加入了西门子健康集团，并一直从事软件架构、企业级架构、创新管理、私有云和公共云、团队管理、工程管理、产品套件管理、合作伙伴管理以及数字化转型等方面的工作。他目前担任西门子健康集团teamplay数字健康平台的研发主管。2022年，他在Addison-Wesley出版的“Establishing SRE Foundations”一书中分享了他的<a href=\"https://www.informit.com/store/establishing-sre-foundations-a-step-by-step-guide-to-9780137424658\">DevOps知识</a>\"。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/assess-culture-sre-adoption/\">Assessing Organizational Culture to Drive SRE Adoption</a>\"</p>",
    "publish_time": "2023-06-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Cloudflare的Kafka之旅：万亿级消息处理实践",
    "url": "https://www.infoq.cn/article/ZHFZHsctlAhN8Ai9wdpo",
    "summary": "<p>在不到6年的时间里，Cloudflare已经向Kafka中生成了超过1万亿条用于服务间通信的消息。随着公司和应用程序服务团队的发展，他们必须不断调整他们的工具来支持持续的快速交付。</p><p>&nbsp;</p><p>我们将在本文中介绍我们早期的分布式团队工作，以及如何基于Kafka构建抽象来处理万亿条消息。</p><p>&nbsp;</p><p>我们还将介绍近年来可伸缩性给我们带来的挑战，以及我们为应对不断增长的需求而采用的一些方法和模式。</p><p>&nbsp;</p><p></p><h1>Cloudflare简介</h1><p></p><p>&nbsp;</p><p>Cloudflare为用户提供了一个全局网络，并帮助他们提升他们网站、API和网络流量的安全性。</p><p>&nbsp;</p><p>这个网络还可以保护企业网，让他们能够在边缘运行和部署整个应用程序。</p><p>&nbsp;</p><p>Cloudflare提供了一系列产品，包括CDN、Zero Trust和Cloudflare Workers，来实现这些目标，以及识别和阻止恶意活动，让用户能够把精力专注在他们的业务上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a21d31a11021e2e2d3de8d581ec6f930.png\" /></p><p></p><p>图1 Cloudflare的全局网络</p><p>&nbsp;</p><p>从工程角度来看，Cloudflare网络有两个主要组成部分：全局边缘网络和Cloudflare控制平面。</p><p>&nbsp;</p><p>这个网络的很大一部分是使用Cloudflare的产品来构建的，其中Workers被部署在边缘网络上。另一方面，控制平面是一组数据中心，在数据中心的机器上运行着Kubernetes、Kafka和数据库。Kafka生产者和消费者通常都部署在Kubernetes中，但也取决于工作负载和期望获得的结果。</p><p>&nbsp;</p><p>在本文中，我们将重点关注Cloudflare控制平面，并探索如何通过扩展服务间通信和工具来为运营提供支持。</p><p>&nbsp;</p><p></p><h1>Kafka</h1><p></p><p>&nbsp;</p><p><a href=\"https://kafka.apache.org/\">Apache Kafka</a>\"有集群的概念，集群由多个broker组成，每个集群都有一个指定的首领broker负责协调通信。在下图中，Broker 2就是首领。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da52df231d7d357bd1e51c05a89cdc9d.png\" /></p><p></p><p>图2 Kafka集群</p><p>&nbsp;</p><p>消息被分类到主题中，例如用户事件（创建用户或更新用户信息等）被放在同一个主题中。主题被分为分区，这样Kafka可以很好地进行横向伸缩。在图中，两个broker上都有主题A的分区，每个分区都有一个指定的首领来确定其“真相来源”。为了确保弹性，分区根据指定的复制系数进行复制，通常最小的复制系数为3。向Kafka发送消息的服务叫作生产者，从Kafka读取消息的服务叫作消费者。</p><p>&nbsp;</p><p></p><h1>Cloudflare的工程文化</h1><p></p><p>&nbsp;</p><p>在过去，Cloudflare是一个独立的PHP应用程序，随着公司的发展和多样化，这种方式暴露出了它的局限性和风险。</p><p>&nbsp;</p><p>现在，团队不再强制指定使用特定的工具或编程语言，他们可以构建和维护他们自己的服务，公司鼓励创新并倡导使用高效的工具和实践。应用程序服务团队是一个相对较新的团队，职责是通过提供遵循最佳实践的工具帮助其他团队更容易完成工作，这样开发团队就可以专注于交付业务价值。</p><p>&nbsp;</p><p></p><h1>紧密耦合</h1><p></p><p>&nbsp;</p><p>随着产品规模的增长，有必要找到更好的方法，让团队能够按照自己的节奏前进，并与其他团队解耦，工程团队也需要对回退请求和工作完成保证有更多的控制。</p><p>&nbsp;</p><p>由于我们已经在使用Kafka集群来处理大量的数据，所以我们决定花点时间创建一个通用的消息总线集群：很简单，只需要创建一个拉取请求，设置一个新主题所需的一切，包括复制策略、保留期限和ACL。下图说明了<a href=\"https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages/\">消息总线</a>\"集群是如何帮助解耦团队的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/672d78ae8c238f7fab2e2c3a4e7a6436.png\" /></p><p></p><p>图3 通用的消息总线集群</p><p>&nbsp;</p><p>上图中的三个团队可以发出审计日志系统感兴趣的消息，而不需要了解特定的服务。更少的耦合让工程团队可以更有效地完成工作。</p><p>&nbsp;</p><p></p><h1>非结构化的通信</h1><p></p><p>&nbsp;</p><p>对于事件驱动的系统来说，为了避免耦合，系统之间不应该相互感知。最初，我们没有强制要求使用怎样的消息格式，生产者团队可以自己决定如何生产消息。如果团队之间没有严格的契约，这可能会导致非结构化的通信，并带来挑战，无法被处理的消息数量会激增。</p><p>&nbsp;</p><p>为了避免出现非结构化的通信，应用程序服务团队在Kafka生态系统中寻找解决方案，并找到了两个可行的选择——<a href=\"https://avro.apache.org/\">Apache Avro</a>\"和<a href=\"https://protobuf.dev/\">protobuf</a>\"，并最终选择了后者。我们以前一直使用JSON，但发现很难实现兼容性，而且JSON消息比protobuf大。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/3997f3a64d7f02d9bef7fe6f1e85e109.png\" /></p><p></p><p>图4 protobuf消息示例</p><p>&nbsp;</p><p>protobuf提供了严格的消息类型和向前和向后兼容性，能够用多种语言生成代码也是它的一个主要优势。应用程序服务团队建议在protobuf消息中加入详细的注释，并使用Uber的开源工具<a href=\"https://github.com/uber/prototool\">Prototool</a>\"进行变更检测和强制执行风格规则检查。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a36d69451d9939724e3ed22b81d46e94.png\" /></p><p></p><p>图5 切换到Protobuf</p><p>&nbsp;</p><p>只有Protobuf是不够的：不同的团队仍然可以向同一个主题发送消息，由于格式不符合预期，消费者可能无法处理它们。此外，配置Kafka的消费者和生产者并不是一件容易的事情，需要对工作负载有充分的了解。由于大多数团队都在使用Go，我们决定使用Go构建一个“消息总线客户端库”，既可以遵循最佳实践，也可以让团队更快地完成他们的工作。</p><p>&nbsp;</p><p>为了避免团队向同一个主题发送不同的消息，我们做出了一个有争议的决定，即（在客户端）强制每个主题使用一种protobuf消息类型。这个决定很容易被采用，但它会导致创建太多的主题，复制太多的分区，因为分区的复制系数至少为3。</p><p>&nbsp;</p><p></p><h1>连接器</h1><p></p><p>&nbsp;</p><p>通过引入工具和抽象，应用程序服务团队在简化Kafka基础设施方面取得了重大进展，但我们发现，要遵循最佳实践，还有很多应用场景和模式需要处理，于是团队开发了连接器框架。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d26e3876111ebaaf27063326b44e9e0c.png\" /></p><p></p><p>图6 连接器框架</p><p>&nbsp;</p><p>这个框架基于Kafka连接器，工程师们可以用它创建从一个系统读取数据并将其推送到另一个系统（如Kafka或<a href=\"https://blog.cloudflare.com/introducing-quicksilver-configuration-distribution-at-internet-scale/\">Quicksilver</a>\"——Cloudflare的Edge数据库）的服务。为了简化使用过程，我们使用<a href=\"https://www.cookiecutter.io/\">Cookiecutter</a>\"来创建服务模板，工程师们只需要在命令行上输入一些参数。</p><p>&nbsp;</p><p>连接器的配置过程很简单，可以通过环境变量来设置，无需修改代码。</p><p>&nbsp;</p><p>下面的示例将从Kafka读取数据并将其写入Quicksilver。连接器被设置为从topic1和topic2读取数据，并应用pf_edge函数。完整的配置就是这些，其中还包括指标、警报和部署到生产环境所需的其他所有东西。工程团队可以选择注册自定义转换器，这些是他们唯一需要编写的代码。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/74/74f831340d6c937467aedaf532bc9940.png\" /></p><p></p><p>图7 连接器示例</p><p>&nbsp;</p><p>例如，我们在通信方式首选项服务中使用了连接器：如果用户希望在Cloudflare仪表盘中移除营销信息，他们可以通过与这个服务发生交互来完成这个操作。通信方式首选项变更事件被保存在数据库中，并向Kafka发送消息。为了确保变更可以在三个不同的源系统中生效，我们使用单独的连接器分别将变更同步到事务性电子邮件服务、客户管理系统和市场电子邮件系统。这种方法可以让系统最终保持一致，我们利用Kafka提供的保证机制来确保整个过程顺利进行。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a53d0b5393202fd08bc630fe41f2bf0a.png\" /></p><p></p><p>图8 连接器和通信方式首选项变更</p><p>&nbsp;</p><p></p><h1>可见性</h1><p></p><p>&nbsp;</p><p>在疫情期间，随着用户的迅速增长，吞吐量也在快速增长，我们创建的一些抽象的可伸缩性问题就凸显了出来。</p><p>&nbsp;</p><p>以我们为Kafka用户处理的审计日志为例：我们建立了一个系统来管理这些日志，生产者生成事件，我们监听它们，并将数据记录到数据库中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4e/4e29fe9f9f5b0533bb97da6a0f138b59.png\" /></p><p></p><p>图9 审计日志推送</p><p>&nbsp;</p><p>我们通过一个API和日志推送集成方式来暴露这些信息，我们通过日志推送将审计日志推送到不同的数据桶中，比如<a href=\"https://www.cloudflare.com/products/r2/\">Cloudflare R2</a>\"或Amazon S3。</p><p>&nbsp;</p><p>在疫情期间，新注册的审计日志比以往更多，用户开始使用我们的API来获取最新数据。由于这种方法不可伸缩，我们决定开发一个管道来解决这个问题。我们创建了一个小型服务来监听审计日志事件，并将其转换为适当的格式，然后直接存储到数据桶中，而不会让API过载。</p><p>&nbsp;</p><p>随着日志越积累越多，并且无法足够快地清除它们，我们遇到了更大的问题，导致出现延迟和违反SLA。我们无法确切知道导致延迟的原因，因为我们的SDK缺少诊断问题的工具：瓶颈在于从Kafka读取数据、转换数据还是将数据保存到数据库？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d27e6eccaf6fa17e6074e3c9c80e55a.png\" /></p><p></p><p>图10 瓶颈在哪里？</p><p>&nbsp;</p><p>我们决定使用Prometheus指标来解决这个问题。直方图指标可以知道处理消息的每个步骤需要花费多少时间，这有助于我们识别出较慢的步骤，但我们仍然无法判断哪个组件处理特定消息花费的时间更长。为了解决这个问题，我们对OpenTelemetry进行了调研，重点关注它的跟踪集成能力：Kafka方面并没有很多好的OpenTracing集成工具，而且跨多个服务传播跟踪事件也是件具有挑战性的事情。</p><p>&nbsp;</p><p>随着团队使用OpenTracing来增强SDK，我们发现将数据推送到数据桶和从Kafka读取数据都是瓶颈，并优先修复了这些问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5a/5a32c28aa299f540d88f48bf8284bc28.png\" /></p><p></p><p>图11 找出瓶颈所在</p><p>&nbsp;</p><p>将指标添加到SDK中，我们能够更好地了解集群和服务的健康状况。</p><p>&nbsp;</p><p></p><h1>通知噪音</h1><p></p><p>&nbsp;</p><p>我们遇到了一个问题：因为收集了大量指标，出现了通知噪音，其中有许多与不健康的应用程序和延迟问题相关的警报。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/54b5be34166d6997fbd1f598ce291ed3.png\" /></p><p></p><p>图12 警报管道</p><p>&nbsp;</p><p>基本的警报管道由Prometheus和AlertManager组成，它们会将警报转到PagerDuty。由于服务的重启或伸缩并不是很理想，所以我们决定使用Kubernetes，并实现服务的健康检查。</p><p>&nbsp;</p><p>Kubernetes中有三种类型的健康检查：活动、准备就绪和启动。对于Kafka来说，准备就绪探测没有用，因为通常情况下，HTTP服务器是不公开的。为了解决这个问题，我们实现了另一种方法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/097a38c1719342fdc00cd177d9f0d9a2.png\" /></p><p></p><p>图13 健康检查和Kafka</p><p>&nbsp;</p><p>在收到活动检查请求时，我们让一个broker执行一个基本操作，例如列出主题，如果响应成功，则检查通过。然而，在某些情况下，应用程序虽然在运行，但无法生产或消费消息，于是我们为消费者实现了更智能的健康状况检查。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4be53e0ddcad71c20e72145ac21a5a30.png\" /></p><p></p><p>图14 健康检查的实现</p><p>&nbsp;</p><p>Kafka的当前偏移量就是分区最后一个可用的偏移量，而提交的偏移量是消费者成功消费的最后一个偏移量。</p><p>&nbsp;</p><p>我们在进行健康状况检查时读取这些偏移量，这样就确定消费者运行是否正确：如果无法读取偏移量，则可能出现了问题，并且将消费者报告为不健康。如果可读取偏移量，我们将最后提交的偏移量与当前偏移量进行比较。如果它们相同，则没有追加的新消息，那么我们就认为消费者是健康的。如果最后一次提交的偏移量不同，我们就检查它是否与之前记录的最后一次提交的偏移量相同，这样可以知道消费者是否卡住了并需要重启。这个过程带来了更好的通知体验和用户体验。</p><p>&nbsp;</p><p></p><h1>消费延迟</h1><p></p><p>&nbsp;</p><p>我们有一个为电子邮件系统生成Kafka事件的系统。这些事件包含了一个模板，例如，一个“受攻击”模板，其中包含了受攻击网站的信息和攻击者的身份，以及元数据。</p><p>&nbsp;</p><p>我们监听这些事件，从注册中心获取电子邮件模板，对其进行填充，并将其发送给客户。然而，我们开始遇到负载问题：生产事件的速率出现了峰值，导致了消费延迟，影响到了重要的OTP和SLO。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/34/349c3756789c117c633e980357a96654.png\" /></p><p></p><p>图15 消费延迟</p><p>&nbsp;</p><p></p><h1>批次处理</h1><p></p><p>&nbsp;</p><p>我们开始研究通过不同的解决方案来解决这个问题，最初的解决方案是扩展分区和消费者的数量，但并没有获得显著的改进。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0c/0ced7dc886450bff799e2662c5719ff8.png\" /></p><p></p><p>图16 批次处理</p><p>&nbsp;</p><p>我们决定使用一种更简单但更有效的方法——批次处理，即一次处理一定数量的消息、执行转换和分发。这种方式被证明是有效的，让团队可以轻松地应对高生产速率。</p><p>&nbsp;</p><p></p><h1>文档化</h1><p></p><p>&nbsp;</p><p>在开发SDK时，我们发现许多开发人员在使用时会遇到问题。有些人发现有bug，有些人不确定如何实现某些功能或不知道一些错误是什么意思。为了解决这个问题，我们在Google Chat上创建了频道，让他们来问我们问题。我们有一个值班的人负责回复问题，并在我们的维基页上记录我们发现的问题和答案。这有助于改善SDK的整体用户体验。</p><p>&nbsp;</p><p></p><h1>结论</h1><p></p><p>&nbsp;</p><p>我们总结了四个教训：</p><p>&nbsp;</p><p>始终在灵活性和简单性之间找到恰当的平衡：可配置的设置步骤提供了更大的灵活性，而更简单的设置有助于进行跨不同管道的标准化。可见性：尽早将指标添加到SDK中，这样可以帮助团队了解系统的行为并做出更好的决策，特别是在发生故障时。契约：建立强大的、严格的契约，让人们能够很好地了解主题内部发生了什么，知道谁在写入和读取消息。将所做的事情记录下来，这样就不需要花时间答疑解惑或帮助人们调试问题。这可以通过Google Chat和维基页等方式来实现。</p><p>&nbsp;</p><p>我们遵循这些原则，改进了我们的系统，让我们的客户感到满意，即使是在面临高度压力的情况下。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/kafka-clusters-cloudflare/\">https://www.infoq.com/articles/kafka-clusters-cloudflare/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/9c32f07d324895a395c8b8fce\">Kafka&nbsp;往事——揭露&nbsp;Kafka&nbsp;推出&nbsp;Kafka&nbsp;Streams 背后原因</a>\"</p><p><a href=\"https://xie.infoq.cn/article/876c9c6bad7dde44e673736ba\">深度剖析：Kafka 请求是如何处理的? 看完这篇文章彻底懂了</a>\"</p><p><a href=\"https://xie.infoq.cn/article/d076332e1029d6ad453796587\">Kafka 原理——Kafka 为何如此之快？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/20714291d34ee83e7b3540970\">用好 kafka，你不得不知的那些工具</a>\"</p>",
    "publish_time": "2023-06-27 09:42:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "300毫秒分胜负：维基百科的总阻塞时间优化之道",
    "url": "https://www.infoq.cn/article/rEoBohpb2NMs8Qe0lwuT",
    "summary": "<p></p><p>大家有没有遇到过响应缓慢、卡顿崩溃的垃圾网站？遇上这类性能缺陷，脾气火爆的朋友往往果断选择：</p><p></p><p>狂点鼠标；退出走人，拉低客源转化率；搜索引擎排名因此下降。</p><p>&nbsp;</p><p>三年多来，维基百科的移动版网站也深受一段JavaScript代码的戕害。在低端手机上，这段JS代码的页面加载时间可能超过600毫秒，大大影响了用户的交互体验。</p><p>&nbsp;</p><p>在本文中，我们将一同了解如何通过几个简单步骤，让任务执行时间有效缩短约50%。</p><p>&nbsp;</p><p></p><h2>总阻塞时间：长任务的重要性</h2><p></p><p>&nbsp;</p><p>在JS执行方面，600毫秒似乎并不是什么无法接受的问题。但我们不妨想象这样的场景：当这段600毫秒的JS代码开始执行时，用户恰好想在加载过程中单击某个按钮。因为特定时段内浏览器的主线程只能处理一个任务，所以用户必须等待以下步骤完成后才能获得操作反馈：</p><p>JS任务花600毫秒执行完毕点击处理任务随后开始执行浏览器执行必要的渲染步骤，最终更新页面内容</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc4353141860b08acebd04f15d6c071a.png\" /></p><p></p><p>&nbsp;长任务可能导致视觉更新延迟，拖慢点击程序的处理速度</p><p>&nbsp;</p><p>每个步骤都需要消耗时间，而任何超过100毫秒的响应速度都会给用户带来非常明确的延迟感受。正因为如此，谷歌将一切耗时超过50毫秒的任务定性为“长任务”，认为其会影响页面对用户输入的响应。他们甚至专门为此制定了“总阻塞时间”（TBT）的指标。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f48cd6d15548e50fffe6c5acbd1c0ece.png\" /></p><p></p><p>这里有两个长任务（大于50毫秒）——分别耗时80毫秒和100毫秒</p><p>&nbsp;</p><p></p><h2>总阻塞时间是什么？</h2><p></p><p></p><p>所谓总阻塞时间，是指浏览器主线程上全部长任务在首屏内容绘制（FCP）和响应时间（TTI）之间的阻塞部分的总和。换言之，“阻塞部分”代表着每个长任务超出50毫秒之外的时间消耗。</p><p>&nbsp;</p><p>计算以下示例中的总阻塞时间：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc93882c7dd1fd6667b5ac649b9d8510.png\" /></p><p></p><p>80毫秒任务比50毫秒基准多出30毫秒，因此带来了30毫秒的总阻塞时间。30毫秒的任务不构成阻塞时间，因为其小于50毫秒且不属于长任务。100毫秒的任务比50毫秒基准多出50毫秒，因此带来了50毫秒的总阻塞时间。</p><p>&nbsp;</p><p>由于总阻塞时间对应每个长任务超过50毫秒部分的总和，所以示例中的最终结果是30毫秒+50毫秒=80毫秒。</p><p>&nbsp;</p><p>在常规移动硬件上进行测试时，谷歌建议站点的总阻塞时间应小于200毫秒。但维基百科上一项任务的执行就可能超过600毫秒——相当于总体上限的3倍。</p><p>&nbsp;</p><p>我们该如何改进性能？</p><p>&nbsp;</p><p></p><h2>如何降低总阻塞时间</h2><p></p><p></p><p>要降低这项指标，我们需要：</p><p>在首屏绘制和交互时间之间，减少主线程上的工作负载；在保证工作内容不变的情况下，将长任务拆分成多个不超过50毫秒的小任务。</p><p>&nbsp;</p><p>本文主要侧重第一种解决思路。</p><p>&nbsp;</p><p></p><h3>步骤1：删除不必要的JS代码</h3><p></p><p></p><p>HTML解析、绘制和垃圾收集都需要在主线程上运行，但引发总阻塞时间过长的罪魁祸首仍然是JS代码。毕竟有经验的前端开发者都知道，网站降速背后总有JS的身影。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/433672102478253df302a01f83c7b4fa.png\" /></p><p></p><p>&nbsp;在分析维基百科的移动站点时，我发现_enable方法占用了大部分执行时间。此方法负责对移动站点上的部分开展和折叠行为进行初始化。配置文件则显示，在_enable方法中，对jQuery .on(\"click\")方法的调用同样速度很慢。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d06cc7ae68dd43e5f267d54e144e8412.png\" /></p><p></p><p>&nbsp;这里的.on(\"click\")调用负责向内容中的几乎所有链接附加点击事件侦听器，这样如果点击的链接包含哈希片段，相应的部分就会被展开。对于链接较少的短文章，这部分性能影响几乎可以忽略不计。但对于像“美国”这类的长词条，其内容可能包含超过4000个链接，因此在低端设备上的执行时间会超过200毫秒。</p><p>&nbsp;</p><p>更糟糕的是，这种设计完全没有必要。侦听haschange事件的下游代码已经调用了与点击事件侦听器相同的方法。除非窗口位置已经指向链接目的地，否则点击链接会对checkHash方法调用两次——一次用于链接点击事件处理程序，另一次用于hashchange处理程序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/13/13942e921f869b9faff2c905b21df261.png\" /></p><p></p><p>这种情况下，最好的方法当然是直接删除这个JS代码块，在几乎不影响主线程功能的前提下直接省下近200毫秒。</p><p>&nbsp;</p><p>在分析过程中，请始终检查最耗时的部分，之后看看有没有能够优化或者删掉的代码。</p><p>&nbsp;</p><p>经验之谈：加快网站速度的首选方法，永远是删除JS代码。</p><p>&nbsp;</p><p></p><h3>步骤2：优化现有JS代码</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/362197fcbdd45cc08baa12d31434609a.png\" /></p><p></p><p>另一项性能审查显示，initMediaViewer方法需要约100毫秒的执行时间。此方法负责将点击事件侦听器附加到内容中的各个缩略图处，这样点击缩略图即可打开媒体查看器：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af6065373bd417b333e84ff2809189b7.png\" /></p><p></p><p>&nbsp;与步骤1中的链接示例类似，这种向页面上各个缩略图附加事件侦听器的方法不利于性能扩展。</p><p>&nbsp;</p><p>维基百科中的词条可能包含数千张相关图片。在这些多图页面上运行这段代码时，其执行时间可能超过100毫秒，必然增加页面的总阻塞时间。下面来看替代方法。</p><p>&nbsp;</p><p>答案就是事件委托。</p><p>&nbsp;</p><p>事件委托是一种强大的技术，允许我们将单个事件侦听器附加到单一元素，而该元素可以是大量其他元素的共同祖先。对于可以添加任意数量元素的用户生成内容，我们往往可以通过事件委托提高其执行效率。整个过程会用到事件冒泡，具体如下：</p><p>将事件侦听器附加至容器元素。在事件处理程序中使用event参数，检查event.target属性以查看事件源。可以选择使用event.target.closest(selector) API来检查祖先元素。如果该事件源就是我们所关注的元素或者其子元素，则进行处理。</p><p>&nbsp;</p><p>更新后的代码如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8a/8aaf53465ba4a1eb5cc0bca10f02f9b8.png\" /></p><p>&nbsp;</p><p>我们修改了initMediaViewer方法，将一个点击事件侦听器附加到了包含所有图像的单一容器元素上。在onClickImage方法中，我使用ev.target.closest(selector) API来检查点击是否来自缩略图元素或者其子元素。如果都不是，代码会提前返回，因为这里只需要关注对缩略图的点击。如果是，则代码将处理该事件。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>我们分别通过两轮部署，将步骤1和步骤2中的优化发布到了生产环境。</p><p>&nbsp;</p><p>根据维基百科的综合性能测试数据，在Moto G（5）实机测试当中，首轮部署将总阻塞时间缩短了约200毫秒，第二轮部署进一步缩短了约80毫秒。总体而言，这两个步骤让Moto G（5）等移动设备访问长文章时的总阻塞时间缩短了约300毫秒。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/20/2056d8a871af7cecd3725420f4329660.png\" /></p><p></p><p>&nbsp;维基百科通过Moto G（5）在综合性能测试中访问“瑞典”词条</p><p>&nbsp;</p><p>虽然仍有进一步改进空间，且查询任务仍高于建议的低端设备延迟上限，但此次优化还是取得了显著成果。为了更大程度缩短总阻塞时间，后续可能有必要将任务拆分成更多小任务。</p><p>&nbsp;</p><p>此次试验表明，有针对性的小规模优化有望实现显著的性能改进。通过删除或优化特定代码片段，看似微小的更改也会对网站的整体性能产生重大影响。换言之，要想在所有设备上改善响应速度和浏览体验，并不一定要对代码库开展复杂且广泛的修改。有时候，小小一点调整就足以引发性能质变。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.nray.dev/blog/300ms-faster-reducing-wikipedias-total-blocking-time/\">https://www.nray.dev/blog/300ms-faster-reducing-wikipedias-total-blocking-time/</a>\"</p><p>&nbsp;</p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/09cb2a1d3c5cd5b93c0da4476\">维基百科技术架构</a>\"</p><p><a href=\"https://xie.infoq.cn/article/9919e3bc478edd1f8c977d629\">怎样搭建企业内部维基百科</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxODcyNjEzNQ==&amp;mid=2247541219&amp;idx=4&amp;sn=b0e51a1136ca72d6d71234141d774bb3&amp;chksm=9bd38c7baca4056dd2655d7bfb12204289ced324067453edbb85a9118ba1cd99c555a989a42d&amp;scene=27#wechat_redirect\">如何自己搞一个维基百科？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ca3d93fc3e4b05d5ecdbb19cd\">spark 实战之：分析维基百科网站统计数据 (java 版)</a>\"</p>",
    "publish_time": "2023-06-27 09:43:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "面向AI大模型，腾讯云首次完整披露自研星脉高性能计算网络",
    "url": "https://www.infoq.cn/article/y3iGlmRCOgSvBqWoO03W",
    "summary": "<p></p><blockquote>&nbsp;AIGC的爆发除了带来算力上的挑战，对网络的要求也达到了前所未有的高度。</blockquote><p></p><p></p><h2>星脉高性能计算网络：最大带宽达3.2T，可支持10万卡超大算力集群</h2><p></p><p>&nbsp;</p><p>6月26日，<a href=\"https://www.infoq.cn/article/lUP7E2xx5OCMIUEMdhEr\">腾讯云</a>\"首次对外完整披露自研<a href=\"https://www.infoq.cn/article/N8Wcppi2vFxkh3C0VdAw\">星脉高性能计算网络</a>\"：星脉网络具备业界最高的3.2T通信带宽，能提升40%的GPU利用率，节省30%~60%的模型训练成本，为AI大模型带来10倍通信性能提升。基于腾讯云新一代算力集群HCC，可支持10万卡的超大计算规模。</p><p>&nbsp;</p><p>AIGC的火爆带来AI大模型参数量从亿级到万亿级的飙升。为支撑海量数据的大规模训练，大量服务器通过高速网络组成算力集群，互联互通，共同完成训练任务。大集群不等于大算力，相反，GPU集群越大，产生的额外通信损耗越多。大带宽、高利用率、信息无损，是AI大模型时代网络面临的核心挑战。</p><p>&nbsp;</p><p>千亿、万亿参数规模的大模型，训练过程中通信占比最大可达50%，传统低速网络的带宽远远无法支撑。同时，传统网络协议容易导致网络拥塞、高延时和丢包，而仅0.1%的网络丢包就可能导致50%的算力损失，最终造成算力资源的严重浪费。</p><p>&nbsp;</p><p>基于全面自研能力，腾讯云在交换机、通信协议、通信库以及运营系统等方面，进行了软硬一体的升级和创新，率先推出业界领先的大模型专属高性能网络——星脉网络。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/76/76f78675e9b823b413a7cdb208711259.png\" /></p><p></p><p>腾讯云副总裁王亚晨表示：“星脉网络是为大模型而生。它所提供的大带宽、高利用率以及零丢包的高性能网络服务，将助力算力瓶颈的突破，进一步释放AI潜能，全面提升企业大模型的训练效率，在云上加速大模型技术的迭代升级和落地应用。”</p><p>&nbsp;</p><p>在硬件方面，星脉网络基于腾讯的网络研发平台，采用全自研设备构建互联底座，实现自动化部署和配置。</p><p>&nbsp;</p><p>在软件方面，腾讯云自研的TiTa网络协议，采用先进的拥塞控制和管理技术，能够实时监测并调整网络拥塞，满足大量服务器节点之间的通信需求，确保数据交换流畅、延时低，实现高负载下的零丢包，使集群通信效率达90%以上。</p><p>&nbsp;</p><p>此外，腾讯云还为星脉网络设计了高性能集合通信库TCCL，融入定制化解决方案，使系统实现了微秒级感知网络质量。结合动态调度机制合理分配通信通道，可以避免因网络问题导致的训练中断等问题，让通信时延降低40%。</p><p>&nbsp;</p><p>网络的可用性，也决定了整个集群的计算稳定性。为确保星脉网络的高可用，腾讯云自研了端到端的全栈网络运营系统，通过端网立体化监控与智能定位系统，将端网问题自动定界分析，让整体故障的排查时间由天级降低至分钟级。同时，大模型训练系统的整体部署时间从19天缩减至4.5天，保证基础配置100%准确。</p><p></p><h2>腾讯数据中心网络技术演进历程</h2><p></p><p>&nbsp;</p><p>星脉网络全方位的升级背后，是腾讯数据中心网络历经三代技术演进的成果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/756c4522df28f945ad51839068cce57d.png\" /></p><p></p><p>在腾讯发展初期，数据中心网络流量主要由用户访问数据中心服务器的南北向流量构成，网络架构以接入、汇聚、出口为主。这一阶段主要使用了商用网络设备，搭建标准化数据中心网络，支撑QQ在线人数增长超过1亿，服务器规模增长超10万。</p><p>&nbsp;</p><p>随着大数据和云计算的兴起，服务器之间的东西向流量逐渐增多，云租户对网络产生了虚拟化和隔离的要求。数据中心网络架构逐渐演变为同时承载南北向和东西向流量的云网络架构，腾讯云构建了全自研网络设备与管理系统，打造超大规模数据中心网络，服务器规模近200万台。</p><p>&nbsp;</p><p>如今随着<a href=\"https://www.infoq.cn/article/gsdQ55Gf5BkbiWx0NuWT\">AI大模型</a>\"的出现，腾讯云在国内率先推出高性能计算网络，采用东西向、南北向流量的分离架构。构建了独立的超大带宽、符合AI训练流量特征的网络架构，并配合自研软硬件设施，实现整套系统的自主可控，满足超强算力对网络性能的新需求。</p><p>&nbsp;</p><p>日前，腾讯云发布的新一代HCC高性能计算集群，正是基于星脉高性能网络打造，可以实现3.2T超高互联带宽，算力性能较前代提升3倍，为AI大模型训练构筑可靠的高性能网络底座。</p><p>&nbsp;</p><p>未来，腾讯云还将持续投入基础技术的研发，为各行各业的数智化转型提供有力的技术支撑。</p>",
    "publish_time": "2023-06-27 10:04:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI 如何使能千行百业？探秘华为云盘古大模型",
    "url": "https://www.infoq.cn/article/t9SEK7arwsmEIMPA0zBB",
    "summary": "<p>人工智能新的爆发点会出现在哪里？预训练大模型究竟能做些什么？对我们的生产生活又会有哪些影响？华为云和 InfoQ 联合出品《探秘云新知》，带你探秘「华为云盘古大模型」，解构最前沿的人工智能技术与行业应用！</p>",
    "publish_time": "2023-06-27 10:18:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "王志龙确认出席 ArchSummit 深圳，将分享《计算密集型应用以 Service Mesh 为支点解决分布式问题的探索与实践》话题",
    "url": "https://www.infoq.cn/article/HTZpkKeTBD1KADAAJTMu",
    "summary": "<p>7&nbsp;月&nbsp;21&nbsp;日&nbsp;-&nbsp;22&nbsp;日，&nbsp;在&nbsp;<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">ArchSummit&nbsp;全球架构师峰会（深圳站）</a>\"，京东搜索与推荐部架构师王志龙，将于会上发表题为《计算密集型应用以&nbsp;Service&nbsp;Mesh&nbsp;为支点解决分布式问题的探索与实践》的演讲，分享和探讨京东以搜索业务为先行载体，落地&nbsp;Service&nbsp;Mesh&nbsp;的技术挑战和经验，以及在多个方面取得技术收益赋能业务的探索与实践。</p><p></p><p>王志龙拥有&nbsp;10&nbsp;年互联网一线开发经验，Kubernetes&nbsp;Contributor，Layotto&nbsp;Committer，专注云原生领域，擅长性能极限优化。曾工作于腾讯、阿里，参与过微信云平台从&nbsp;0&nbsp;到&nbsp;1&nbsp;建设，阿里 Serverless&nbsp;C++ 和&nbsp;Golang&nbsp;Runtime&nbsp;研发及落地。目前在京东负责搜索微服务治理和新一代&nbsp;Serverless&nbsp;云化平台研发工作。</p><p></p><p>相信通过王志龙的分享，你将收获搜推广等计算密集型应用落地&nbsp;Mesh&nbsp;的挑战和经验，了解到使用&nbsp;Mesh&nbsp;技术解决分布式问题提升性能的典型案例。</p><p></p><p>除上述议题外&nbsp;，ArchSummit&nbsp;深圳还将围绕<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">基础架构技术</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1532?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">DataOps、Data&nbsp;Fabric&nbsp;等高效数据开发与服务模式</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1534?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">Mesh&nbsp;技术实践案例</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1535?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">QUIC&nbsp;传输和架构优化</a>\"等进行分享。</p><p></p><p>数十位业界专家，上百个国内外一线大厂前沿技术案例，一定会给你带来很多全新的开发灵感。期待与你线下交流！&nbsp;现在购票，享&nbsp;9&nbsp;折特惠，立省&nbsp;¥880！咨询购票请联系&nbsp;18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/9d/aa/9d6a27547062ee2e089f91bdc4ba1eaa.png\" /></p><p></p>",
    "publish_time": "2023-06-27 11:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI模型只能部署在云端？高通白皮书发布：混合AI是AI的未来",
    "url": "https://www.infoq.cn/article/KqqfSwMXaXGqoL8YcQOa",
    "summary": "<p>近日，<a href=\"https://www.infoq.cn/article/NjuBjKw2daCjtHzqejcj\">高通</a>\"技术公司正式发布白皮书《混合AI是AI的未来》。高通在白皮书中提到，随着生成式 AI 正以前所未有的速度发展以及计算需求的日益增长，AI 处理必须分布在云端和终端进行，才能实现 AI 的规模化扩展并发挥其最大潜能——正如传统计算从大型主机和瘦客户端演变为当前云端和边缘终端相结合的模式。与仅在云端进行处理不同，混合 AI 架构在云端和边缘终端之间分配并协调 AI 工作负载。云端和边缘终端如智能手机、 汽车、个人电脑和物联网终端协同工作，能够实现更强大、更高效且高度优化的 AI。</p><p>&nbsp;</p><p>高通产品管理高级副总裁兼AI负责人Ziad Asghar在媒体沟通会上进一步分享了高通对于混合AI的愿景，以及如何结合自身的产品技术优势，让混合AI的愿景成为现实。</p><p>&nbsp;</p><p>Ziad Asghar表示，“我们正在引领混合AI愿景的实现。对隐私和安全要求比较高的终端侧工作负载，可以继续通过边缘云，完全在终端侧完成。对于其它的模型工作，我们也可以和云服务供应商合作完成。通过在云端和边缘侧终端分布工作负载，我们能够大幅度减少云端的处理量。混合AI的优势在于，即使不同终端处理能力不尽相同，但仍然能够提供相近的体验，同时带来包括成本、能耗、隐私与安全、个性化等优势；还能通过出色的5G连接技术确保信息在端到端之间进行高效传输。”</p><p></p><h2>混合 AI 对生成式 AI 规模化扩展至关重要</h2><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/FRcz5vjOvl3bM2d57opX\">ChatGPT</a>\" 的爆火掀起生成式AI热潮。自 2022 年 11 月推出后，ChatGPT仅用了短短两个月时间月活用户便达到 1 亿，成为有史以来增长速度最快的消费类应用和第一个杀手级的生成式 AI 应用。</p><p>&nbsp;</p><p>作为一项变革性的技术，生成式AI颠覆了原有的工作、娱乐方式，并拥有非常丰富的应用领域，应用数量也在不断激增。具体而言，生成式AI的应用主要包括搜索、内容生成、生产力、代码编写等等，能够在数秒之内通过大型基础模型创作内容。数据显示，AI 正迎来大爆发时期，目前已有超过 3000 个可用的生成式 AI 应用和特性。</p><p>&nbsp;</p><p>据初步估计显示，生成式 AI 市场规模将达到 1 万亿美元，广泛覆盖生态链的各个参与方。为把 握这一巨大机遇，并推动 AI 成为主流，计算架构需要不断演进并满足大规模生成式 AI 日益增长的处理和性能需求。</p><p>&nbsp;</p><p>拥有数十亿参数的众多生成式 AI 模型对计算基础设施提出了极高的需求。因此，无论是为 AI 模型优化参数的 AI 训练，还是执行该模型的 AI 推理，至今都一直受限于大型复杂模型而在云端部署。</p><p>&nbsp;</p><p>AI 推理的规模远高于 AI 训练。尽管训练单个模型会消耗大量资源，但大型生成式 AI 模型预计每年仅需训练几次。然而，这些模型的推理成本将随着日活用户数量及其使用频率的增加而增加。 在云端进行推理的成本极高，这将导致规模化扩展难以持续。</p><p>&nbsp;</p><p>高通认为，混合 AI 能够解决上述问题，正如传统计算从大型主机和瘦客户端演变为当前云端和 PC、智能手机等边缘终端相结合的模式。</p><p>&nbsp;</p><p>具体来说，混合 AI 指终端和云端协同工作，在适当的场景和时间下分配 AI 计算的工作负载，以提供更好的体验，并高效利用资源。在一些场景下，计算将主要以终端为中心，在必要时向云端分流任务。 而在以云为中心的场景下，终端将根据自身能力，在可能的情况下从云端分担一些 AI 工作负载。混合 AI 架构(或仅在终端侧运行 AI)，能够在全球范围带来成本、能耗、性能、隐私、安全和个性化优势。</p><p></p><h2>利用边缘侧终端规模化扩展生成式AI</h2><p></p><p>&nbsp;</p><p>Ziad Asghar表示，当前很多人将生成式AI和云端联系在一起，通过高通的技术，能够让这些出色的用例在边缘侧实现。“生成式AI对众多领域产生了广泛影响，目前有大量的新兴应用需要生成式AI能力，且已经拥有了庞大的用户规模，市场上也出现了众多非常庞大的模型。我们认为，要真正释放生成式AI的全部潜能，AI需要在边缘侧运行，这也是高通一直努力的方向，我们相信凭借我们的技术，我们能够带来遥遥领先的终端侧生成式AI体验。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6aee7603fbe8ae8b6a5c42a25217c5d9.png\" /></p><p>&nbsp;</p><p>据了解，目前生成式AI的应用能够在高通所推出的几乎所有主要产品线中使用。举例来说，手机作为高度个性化的设备，能够通过生成式AI成为消费者真正意义上的数字助手，它可以接受用户的所有需求，且甚至无需联网就能够完成任务，并完全通过大型基础模型（例如文本生成文本模型LLaMA）与用户交流。此外，生成式AI能够基于视频会议的语音转录内容，制定任务清单，并自动生成完整的演示文稿直接供用户使用，使生产力能够成倍增长。骁龙计算平台拥有专用的硬件单元，能够原生支持生成式AI在本地使用。</p><p>&nbsp;</p><p>在XR方面，生成式AI能够根据终端侧所提供的用户信息进行定制和优化，为用户带来完全不同的独特虚拟世界体验。Ziad Asghar表示，如果只在云端运行，则不具备终端侧的情境信息，因此利用终端能够带来更好的用户体验。</p><p>&nbsp;</p><p>汽车领域的用例也非常丰富。在座舱中使用对话式AI，能够帮助用户规划路线，在去餐厅的路上推荐用餐选项，或者在上班途中列出今日的工作事项。生成式AI还可以根据出发点和目的地信息，结合汽车的丰富传感器数据制定不同的路线规划，找到最佳路线。</p><p>&nbsp;</p><p>物联网领域，生成式AI能够助力打造面向专业领域的GPT类型模型，以及帮助用户完成不同任务的IoT助手。如果来到一个新的城市，生成式AI能够帮助提供旅行目的地推荐。此外它还适用于其他的垂直领域，如医疗、零售、酒店管理等等。</p><p>&nbsp;</p><p>随着强大的生成式AI模型不断缩小，以及终端侧处理能力的持续提升，混合AI的潜力将会进一步增长。参数超过10亿的AI模型已经能够在手机上运行，且性能和精度达到与云端相似的水平。不久的将来，拥有100亿或更高参数的模型将能够在终端上运行。</p><p></p><h4>全栈AI优化</h4><p></p><p>&nbsp;</p><p>Ziad Asghar表示，目前高通已经实现了全球首个Android手机上的Stable Diffusion终端侧演示。Stable Diffusion是一个参数超过10亿的超大神经网络基础模型，能够基于输入的文本提示生成图片。高通的这一终端侧演示是在飞行模式下进行的，通过高通的全栈AI优化，这一模型能够完全在终端侧运行，实现在15秒内完成20步推理，生成饱含细节的图像。</p><p>&nbsp;</p><p>高通面向Stable Diffusion进行了全栈AI优化。2022年6月，高通推出了专门面向边缘侧AI的领先软件栈产品——高通AI软件栈，能够从软件层面进行模型优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/47/478da8f0ce15cc4c8ee1782736f96990.png\" /></p><p></p><p>Ziad Asghar表示，在云端服务器上训练的模型一般采用32位浮点运算（FP32），这意味着完成模型推理需要大量的处理工作。“我们希望通过整数运算模式和量化技术进行AI推理，即时获取模型推理结果。针对Stable Diffusion，我们所采用的是8位整数运算（INT8）。去年年底在第二代骁龙8移动平台上，我们已经进一步支持了4位整数运算（INT4）能力。我们的硬件、软件以及工具设计也都考虑了如何充分利用这一关键优势。”</p><p>&nbsp;</p><p>目前高通能够支持<a href=\"https://www.infoq.cn/article/hzqWDTkJbUCLghFPXmel\">Stable Diffusion</a>\"这一超过10亿参数的模型在终端侧运行，但许多关键的生成式AI模型，比如文本生成图像、自然语言处理、编程、图像理解、图像创作等，模型规模一般在10亿到100亿参数之间。Ziad Asghar表示未来几个月内，高通将有望支持参数超过100亿的模型在终端侧运行。</p><p></p><h2>对话Ziad Asghar：AI大模型会在C端和B端同步落地</h2><p></p><p>&nbsp;</p><p>在媒体沟通会上，Ziad Asghar接受了 InfoQ 在内的部分媒体采访。以下为采访实录，经编辑。</p><p>&nbsp;</p><p>问：刚才说到，高通在几个月之后就可以实现在终端侧处理参数规模达100亿的模型，你们会用什么样的大语言模型？</p><p>&nbsp;</p><p>Ziad Asghar：我们看到目前大语言模型的模态非常丰富，并且已经出现了多模态模型，包括文本生成图片、文本生成文本、文本生成视频，甚至还有图片生成文本、图片生成视频等方式。这将揭开新的序幕，开启许多人们未曾想象过的全新用例。我们已经开始面向不同场景和用例需求的模型展开工作。</p><p>&nbsp;</p><p>问：对于文本生成文本模型，会不会考虑使用来自于Meta的开源LLaMA模型？</p><p>&nbsp;</p><p>Ziad Asghar：我们对模型的应用持有非常开放的态度。针对中国市场的模型，我们会专注于面向本地语言和使用场景的模型调优和训练，以让用户能够根据不同的需求，随时随地地使用模型。我们当前在关注不同的开源模型，同时我们也将与众多的中国合作伙伴携手，实现这些模型在本土市场的终端侧部署。</p><p>&nbsp;</p><p>问：智能手机端侧运行生成式AI会成为未来的大趋势吗？</p><p>&nbsp;</p><p>Ziad Asghar：我们认为这将是一个非常值得期待的重要趋势。所有不同的终端在生成式AI的助力之下，将为消费者带来更强大的吸引力。终端的可用性、娱乐性和生产力价值将远远超越当前的水平。</p><p>&nbsp;</p><p>问：您刚刚提到的Stable Diffusion成功在安卓手机上运行，不到15秒生成图片，这个是完全在终端侧上就能运行吗？我们大约什么时候能用上？以什么样的方式用上？</p><p>&nbsp;</p><p>Ziad Asghar：目前我们已经能够完全在终端侧运行Stable Diffusion，无需连接云端，即使是将手机调到飞行模式也可以。但是目前只有采用高通技术的终端能够实现。对于具体的用例，举例来说，我们可以将Stable Diffusion的能力集成到相机应用中，用户在任何一个地点拍摄照片，再提出需求，例如将照片背景改为夕阳之下的万里长城，Stable Diffusion就能够完成这一任务。此外还有其他的用例，比如数字助手、生产力应用等。我们相信通过与合作伙伴的共同努力，用户将能在今年体验到这些终端侧用例。</p><p>&nbsp;</p><p>问：高通实现终端侧运行AI大模型，在硬件、软件层面的核心技术优势是什么？未来基于其他移动芯片平台的产品是否也会跟进这一能力？</p><p>&nbsp;</p><p>Ziad Asghar：长期以来，高通致力于持续基于我们所打造的硬件、软件和工具资源，驱动生成式AI在终端侧的规模化扩展。首先在硬件方面，我们在既定功耗下的处理能力领先于市场上的其他竞争对手，这让我们能够在运行生成式AI用例时实现非常出色的表现。凭借我们的研究投入，我们能够在终端侧利用量化技术，在处理相同用例时大幅节省功耗和算力，同时完全不影响准确性，这是我们的竞争对手做不到的。另外一个优势在软件方面，我们提供高通AI引擎Direct以及Qualcomm AI Studio等软件工具，让这些模型能够完全在终端侧运行。</p><p>&nbsp;</p><p>问：以聊天机器人对代表的生成式AI应用要有好的使用体验，一个比较大的挑战是时延，每个指令（token）的时延需要在毫秒级别，如何才能将这类应用部署在终端侧，并且拥有不错的体验？</p><p>&nbsp;</p><p>Ziad Asghar：我们能够提供非常高效的token生成速率，完全不会因为时延影响到用户的体验。时延对于用户体验的确至关重要，而得益于我们的技术，我们的每秒token生成速率能够为用户提供流畅的体验。</p><p>&nbsp;</p><p>问：高通的AI硬件在处理AI应用时比CPU有明显优势，接下来是否会增加transformer核心让端侧生成式AI的体验更好？</p><p>&nbsp;</p><p>Ziad Asghar：高通AI引擎涵盖了CPU、GPU以及Hexagon处理器，从而能够在最合适的位置进行AI处理。谈到在高通AI硬件上进行AI处理的优势，除了我们的硬件引擎有着非常强大的处理能力外，我们也在去年推出了专门面向transformer处理的领先技术，能够大幅提升transformer处理效率。所以在硬件层面我们的技术已经完备，能够支持在终端侧获得大幅提升的transformer处理表现。</p><p>&nbsp;</p><p>问：我们注意到高通今天正式将自研AI技术的起步时间点定在了“十年前”，也就是曾经的Zeroth处理器。我们想知道，当年Zeroth的SNN网络架构在如今的骁龙移动平台上得到了多大程度的继承？</p><p>&nbsp;</p><p>Ziad Asghar：高通长期专注于脉冲神经网络（SNN）研究，骁龙820平台是我们最先应用这一技术的产品。我们的大量工作也得益于这一技术研究，我们在这一技术基础之上不断积累，覆盖了各个领域，包括技术、硬件增强和软件等等。我认为这也是我们今天能够在终端侧取得如此领先和丰富的AI能力的原因之一。</p><p>&nbsp;</p><p>问：您认为目前的AI大模型在C端和B端，哪侧会更快落地？</p><p>&nbsp;</p><p>Ziad Asghar：我认为应该会在C端和B端同步落地，同时高通也有能力来支持这些模型落地。无论是智能手机、VR、AR还是汽车等面向消费者的智能设备，亦或是企业级的搭载骁龙计算平台的PC、智能手机等设备。我们的产品和技术能够支持面向专业领域的GPT模型以及丰富的模型模态（比如文本生成图片等），这能够为C端和B端都带来巨大可能性，为所有人带来出色体验，无论是在家中还是在工作场所。甚至只要人们用手机，就可以感受到AI带来的优势。</p><p>&nbsp;</p><p>问：生成式AI在汽车座舱、智能驾驶上的应用进展如何？需要调用数据量和模型形式和手机端有哪些本质不同？边缘侧的低功耗、低时延，是结合5G座舱芯片或大算力芯片共同实现的吗？</p><p>&nbsp;</p><p>Ziad Asghar：第一个问题，关于生成式AI赋能的数字座舱体验，大家可以想象一下，用户可以体验到真正意义上的“和自己的车对话”。你可以告诉你的车：导航带我去机场，但是在去机场的路上，我要找个地方吃个汉堡，再找个地方喝某种口味的咖啡，顺便把我之前干洗的衣服取了。在数字座舱里，我们可以为用户提供真正意义上的虚拟助手。对于汽车应用的不同模态，其要求会更加严格，并且需要更高的准确性。因为与其他商用终端不同，在汽车里出现任何一个小错误都可能带来非常严重的后果。所以我们在确保提供最佳体验的同时，也要确保极高的准确性。</p><p>&nbsp;</p><p>第二个问题，在汽车领域我们需要将多模态相结合，同时结合雷达、激光雷达、以及摄像头等传感器数据，从而让我们在使用生成式AI规划路线时，获得最佳的效果。</p><p>&nbsp;</p><p>第三个问题，汽车需要非常强大的处理能力。一方面，汽车领域的生成式AI用例需要非常丰富的终端侧处理能力，同时，它还需要通过高速低时延的5G连接，在需要的情况下利用云端资源进行处理。与我们其他产品线的产品相比，我们的汽车产品通常能够提供更多的生成式AI处理能力。</p><p>&nbsp;</p><p>问：目前在PC以及其他平台上，NPU通常是一个独立于CPU、GPU的计算单元。但是在骁龙移动平台上，CPU、GPU、DSP、ISP、甚至调制解调器都具备一定的AI计算能力，这就意味着骁龙平台的AI计算架构实际上是一种分布式的设计。那么这是否会加大软件开发的难度？或者是否会出现某些应用不能完整调用全部AI计算单元的情况？</p><p>&nbsp;</p><p>Ziad Asghar：我们的平台采用的是异构计算架构，高通AI引擎包括Hexagon处理器、CPU、GPU以及ISP。我们相信AI是能够赋能整个平台的通用技术，无论是摄像头还是图像、调制解调器、视频、音频、语音等等都可以利用AI技术。同时，基于我们在软件方面进行的大量投入，无论要在终端侧运行何种应用，高通AI引擎都能提供充沛、强大的算力。</p><p>&nbsp;</p><p>问：终端设备上的AI模型是否对用户的个人数据进行处理？</p><p>&nbsp;</p><p>Ziad Asghar：针对用户所担心的个人隐私数据保护，终端侧处理恰恰能够解决这一问题。正如我刚刚所讲，无论是10亿参数的模型，还是100亿参数的模型，如果我们能够完全在终端侧来运行，比如用户发出一个查询，终端接收之后能够独立完成推理，那么所有相关的查询信息和数据都会留在终端上，不会离开终端，这也是边缘处理相对于云端处理的独特优势所在，因为如果要在云端进行查询，那么数据就要先发送到云端，处理完再从云端回到终端。</p>",
    "publish_time": "2023-06-27 11:55:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "倒计时3天！中国信通院2023云边协同大会详细议程公布 | Q推荐",
    "url": "https://www.infoq.cn/article/aYlF2xGAfiughkleXaNA",
    "summary": "<p>由中国信息通信研究院、中国通信标准化协会主办的第三届“云边协同大会”将在京举办。本届大会以“云智物联，边筑算新”为主题，聚焦<a href=\"https://xie.infoq.cn/article/f4892c13764e68aa8a3c918ae\">分布式云</a>\"、<a href=\"https://xie.infoq.cn/article/c574df54e396b1973376ba537\">边缘计算</a>\"、AIoT 平台等领域前沿焦点，搭建政产学研用交流对接平台，深化协同开放合作，推动产业创新发展。</p><p></p><p>本届大会将于 2023 年 6 月 30 日在北京歌华开元大酒店举办，共设置一场开幕式及主论坛，三场高质量发展论坛，邀请政产学研用行业内大咖齐聚一堂，深入探讨产业发展现状，洞察未来演进趋势。同时，中国信通院将在本届大会中发布白皮书及研究报告、技术创新与最佳实践案例、评估评测等最新成果，全面推动我国云边协同应用建设走深向实。</p><p></p><p>目前大会持续火热报名中，期待您的莅临！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/efb3707d68d98089a80074473c71f434.jpeg\" /></p><p></p><p>洞察前沿，透视未来。中国信通院2023云边协同大会，与行业“先行者”共同把握云边协同产业发展新机遇。如果你想现场倾听“算力时延圈、分布式云、边缘计算、AIoT平台”等前沿焦点的干货内容，想与行业大咖深入探讨产业发展现状、创新实践方案及未来发展趋势，赶紧扫描图中二维码或者<a href=\"http://www.idcquan.com/Special/CECS2023/\">点击进行报名</a>\"吧！</p>",
    "publish_time": "2023-06-27 11:57:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC领域最大收购：Databricks花费13亿美元买下只有15名研发的小公司！",
    "url": "https://www.infoq.cn/article/CDDWy25DO5z0dvcqRhkB",
    "summary": "<p>智能湖仓开发商Databricks正着手以13亿美元收购生成式AI初创公司MosaicML，希望帮助自家客户在数据之上构建和部署AI模型。值得注意的是，Databricks本身也是一家初创公司，成立于2013年，通过多轮融资筹得36亿美元。此次的收购成本当中，包含挽留MosaicML员工的相应支出。</p><p>&nbsp;</p><p>大语言模型（LLM）正在AI领域掀起新一波浪潮，它能够理解查询、分析多个数据源并用自然语言给出回应和答案，甚至能够输出编程语言。当然，这些模型也可能产生错误或虚构的答案，而且需要大量GPU资源才能运行。MosaicML的主要业务就是帮助客户在小规模系统上运行模型，并使用自己的数据和非公开数据对模型进行训练和微调。</p><p>&nbsp;</p><p>Databricks公司CEO Ali Ghodsi表示，“每个组织都应该从AI革命当中受益，并更好地控制数据的使用方式。”</p><p>&nbsp;</p><p>今年4月，Databricks公布了其更新之后的开源Dolly大语言模型，标志着公司的AI设施已可用于商业应用，且无需大量GPU资源或者昂贵的API。这款聊天机器人能够响应客户查询，根据Databricks智能湖仓内的数据给出答案。</p><p>&nbsp;</p><p></p><h2>MosaicML的来历</h2><p></p><p>&nbsp;</p><p>MosaicML则创立于2021年，联合创始人分别为担任CEO的Naveen Rao（前英特尔副总裁兼AI产品事业部总经理）和CTO Hanling Tang（前英特尔AI实验室高级总监），员工仅62人，其中<a href=\"https://www.latent.space/p/mosaic-mpt-7b#details\">研究人员（researchers）约为 15 名</a>\"，“与 Brain 或 Deep Mind 的庞大研究人员队伍相比，我们规模很小”。</p><p>&nbsp;</p><p>MosaicML公司目前只公开披露过一轮融资，为6400万美元，其开源大语言模型基于MPT-7B架构，即拥有70亿参数且上下文窗口为6.4万token。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f7d03096c5a00d983258e3c5a26fe7e.png\" /></p><p></p><p>Naveen Rao与Hanlin Tang</p><p>&nbsp;</p><p>&nbsp;</p><p>MPT-7B和<a href=\"https://www.mosaicml.com/blog/mpt-30b\">最近发布</a>\"的MPT-30B目前下载量已经超过330万次。MPT-30B要比MPT-7B更加强大，且性能已经超越初版GPT-3。MosaicML表示，MPT-30B的大小是精心选择的结果，能够轻松部署在单个GPU上——可以在16位精度对应1块A100-80 GB，也可以在8位精度对应1块A100-40 GB。MosaicML公司指出，其他类似的大语言模型（例如Falcon-40B）往往拥有更多参数，因此无法在单个数据中心GPU上提供服务。而一旦需要的GPU超过2个，就会增加推理系统的最低实现成本。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af9a582d7e51c0c51c2a64baf9363a14.jpeg\" /></p><p></p><p>&nbsp;</p><p>MosaicML 还特别提到MPT-30B 在编程方面表现优异，“这归功于包括大量代码的预训练数据。我们希望这种文本和编程功能的结合使 MPT-30B 模型成为社区的流行选择。”</p><p>&nbsp;</p><p>MosaicML是一家美国公司，在旧金山、纽约、帕洛阿尔托和圣迭戈设有办事处。其客户则包括AI2（艾伦AI研究所）、Generally Intelligence、Hippocratic AI、Replit和Scatter Labs。</p><p>Databricks表示，MosaicML的技术将为客户提供一种简单快速的方式，能够在保留对自身数据的控制、安全保护和所有权的前提下，享受到成本低廉的语言模型服务。</p><p>&nbsp;</p><p>与标准方案相比，MosaicML的优化成果将模型训练速度提升了2到7倍，而且能够线性扩展。该公司声称，其数十亿参数的模型在几小时内即可完成训练，远低于一般模型长达数天的训练周期。</p><p>&nbsp;</p><p>Naveen Rao解释道，“我们之所以创办MosaicML，就是希望解决困难的工程和研究问题，帮助每个人更轻松地进行大规模训练。随着近期掀起的生成式AI浪潮，这项工作也成为关注的焦点。我们将与Databricks一道，推动天平向着更有利于大多数人的方向倾斜——我们是志同道合的伙伴，同样肩负让研究人员转型为企业家的使命。”</p><p>&nbsp;</p><p></p><h2>收购意欲何为?</h2><p></p><p>&nbsp;</p><p>根据Naveen Rao的说法，MosaicML一直致力于降低生成式 AI 的使用成本，从数千万美元降至数十万美元。</p><p>&nbsp;</p><p>生物制药服务公司<a href=\"https://www.wsj.com/market-data/quotes/SYNH\">Syneos Health</a>\"的首席信息和数字官 Larry Pickett表示，目前根据专业健康数据训练模型的成本估计为 100 万至 200 万美元。分析师表示，这类“特定领域”模型对公司来说可能比 ChatGPT 更有用，因为它们拥有更多的行业术语和专业知识。但 Pickett 预计，Syneos Health 通过使用较小的预训练模型，“而不是在 OpenAI 拥有的整个数据集之上构建”，花费会大大减少。他说，其中一些模型已经在开源库中可用，例如机器学习初创公司 Hugging Face 提供的库。</p><p>&nbsp;</p><p>“并不是每个人、每个应用程序都需要 GPT-4，”Krishna 说，他指的是 OpenAI 的大型语言模型。他说，大型语言模型正在针对非常具体的应用进行微调，“到那时，它就会变得非常小，可以嵌入到任何手机中。”</p><p>&nbsp;</p><p>但Databricks收购MosaicML的目的却仍然让很多人感到迷惑，Hacker News上<a href=\"https://news.ycombinator.com/item?id=36478734\">不少网友一致认为Databricks是在炒作</a>\"，表示通过新闻稿看不明白Databricks要将LLM整合来做什么。</p><p>&nbsp;</p><p>虽然现在Databricks称公司的主要技术方向为&nbsp;Lakehouse，但实际上它是由 Apache Spark 创建者创立，因此有人认为其护城河和核心价值主张是云中的 Apache Spark，主要是用Spark来处理大规模集群上的数据，其中包括机器学习训练和推理管道，在这种情况下，Databricks整合LLM的价值主张是不够明确的。甚至还有人认为，Databricks不过是在借当前大模型热度进行炒作，MosaicML迟早会被注销掉。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c8622167713515c419ba6661323c883.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>另外不得不提的是，Databricks本身成立也仅十年，去年公布年收入刚超过 10 亿美元。Mosaic 上一轮融资的估值为 1.36 亿美元，无论是以股权还是现金收购，13亿美元的交易价格对Mosaic来说都是一个巨大的飞跃。</p><p>&nbsp;</p><p>该交易预计将在第二季度期间完成（截至 7 月 31 日）。</p><p>&nbsp;</p><p>在交易完成后，整个MosaicML团队预计都将加入Databricks。随时间推移，MosaicML的平台将得到支持、扩展和集成。根据Databricks的说法，客户将获得一套统一平台，可以在该平台之上构建、拥有和保护自己的生成式AI模型，并使用自有数据做进一步模型训练。</p><p>&nbsp;</p><p>拟议的这项收购案须满足成交惯例，包括遵循相关监管许可。目前可能还有其他生成式AI初创公司在与Databricks的竞争对手们洽谈收购方案。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.mosaicml.com/blog/mpt-30b\">https://www.mosaicml.com/blog/mpt-30b</a>\"</p><p><a href=\"https://twitter.com/mosaicml?lang=en\">https://twitter.com/mosaicml?lang=en</a>\"</p><p><a href=\"https://www.wsj.com/articles/databricks-strikes-1-3-billion-deal-for-generative-ai-startup-mosaicml-fdcefc06\">https://www.wsj.com/articles/databricks-strikes-1-3-billion-deal-for-generative-ai-startup-mosaicml-fdcefc06</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=36478734\">https://news.ycombinator.com/item?id=36478734</a>\"</p><p><a href=\"https://www.latent.space/p/mosaic-mpt-7b#details\">https://www.latent.space/p/mosaic-mpt-7b#details</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/5IU3aWsb8Dyyv0evWg2b\">Databricks 来搅局了：0 门槛克隆 ChatGPT，完全开源可随意修改商用</a>\"</p><p><a href=\"https://www.infoq.cn/article/bYfGp9MZACDwlUhAxQDI\">Databricks 在构建统一数据分析平台上的新一轮实践</a>\"</p><p><a href=\"https://xie.infoq.cn/article/f058e6f8c26741b5908d4f0b6\">超级独角兽 Databricks 的崛起之路</a>\"</p><p><a href=\"https://www.infoq.cn/article/ZJAptZ790dLaWSXN-Nox\">Spark 背后公司 Databricks 获 2.5 亿融资，估值 27.5 亿美元</a>\"</p><p><a href=\"https://www.infoq.cn/article/s2a1EQomEYyaoktLPvdT\">估值高达 380 亿美元！大数据独角兽 Databricks 官宣 16 亿美元新融资</a>\"</p>",
    "publish_time": "2023-06-27 13:02:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC如何掀起智能客服“新革命” | InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/6qwddHm0KRDQ7OUOTUjT",
    "summary": "<p>想要将 LLM 大语言模型与智能客服产品进行结合，或者将前者落地于 ToB SaaS 应用软件领域，该如何着手搭建技术栈？AIGC 大模型在智能客服产品中该如何落地？</p>\n<p>本期InfoQ《极客有约》邀请 bothub 创始人、布奇托网络科技创始人兼 CTO 徐文浩，京东云言犀 KA 产品负责人王超、中关村科金智能交互研发总监王素文、华院计算技术总监兼数字人事业部联合负责人贾皓文和大家聊聊 AIGC 将如何掀起智能客服新革命。</p>",
    "publish_time": "2023-06-27 14:00:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《英特尔®️ 至强®️ 实战课》AI驱动的生命科学与医药创新",
    "url": "https://www.infoq.cn/article/a6hfAA3CuciFgYpezdP0",
    "summary": "<p>英特尔与行业领先技术媒体共同打造《英特尔®️ 至强®️ 实战课》系列课程，为互联网、医疗、金融、制造等行业提供有启发、可借鉴的实战案例，并分享基于第四代英特尔®️ 至强®️ 及英特尔数据中心产品组合成功落地实践的经验，为 IT 决策者、架构师和相关从业者输出最前沿的技术干货内容。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2f/bf/2f362179caea0a90989e86bb188ff5bf.jpg\" /></p><p></p><h1>课程内容</h1><p></p><p></p><p>AI大模型驱动的生命科学与医药创新</p><p>主讲人：尤洋 - 潞晨科技创始人兼董事长，NUS校长青年教授</p><p></p><p>基于第四代至强®️&nbsp;可扩展平台实现 AlphaFold2 端到端优化</p><p>主讲人：杨威 - 英特尔人工智能架构师</p><p></p><p>&nbsp;AI Generated Protein设计全新蛋白 解码生命问题</p><p>主讲人：王太峰 - 百图生科 AI 算法负责人、资深总监</p><p></p><p><a href=\"https://s2.uao.so/2C01KKY0\">点击观看完整视频</a>\"https://s2.uao.so/2C01KKY0 </p>",
    "publish_time": "2023-06-27 14:39:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁推动“贞仪”项目研发；度小满在大模型技术领域深度布局；深圳市启动数字人民币试点推广；南京银行江北新区分行上线首例“云财资”系统 | 金融科技新闻速览...",
    "url": "https://www.infoq.cn/article/ifxbQcSwAyLBaXE2oNiB",
    "summary": "<p>度小满利用大模型技术，推动<a href=\"https://xie.infoq.cn/article/5e5157061b47f13bd33f33050\">生成式 AI</a>\" 在金融领域的应用，为数字经济发展开辟新思路；联易融科技在大模型加持下重新定义<a href=\"https://xie.infoq.cn/article/c7800e18ec49eb8331960ed6b\">供应链金融</a>\"；香港金管局深入研究数码港元应用，携手人民银行推进数字货币合作；南京银行江北新区分行成功上线“云财资”系统，推动金融科技赋能实体经济……本周金融科技领域有哪些新闻，一起来看。</p><p></p><p></p><h2>度小满深度布局，以大模型技术助推金融场景落地应用</h2><p></p><p></p><p>最近，新型人工智能技术 ChatGPT 迅速引起了公众的关注，激发了对数字经济发展的各种想象，并为产业数字化和经济数字化开辟了新的发展思路。对此，度小满在类 ChatGPT 应用所依托的大模型技术领域布局更深一步，推动其在金融场景的落地应用。</p><p></p><p>依托百度人工智能技术，度小满已开展一系列基于大模型的应用。在风险管理方面，他们成功将大型语言模型 LLM 应用于解读互联网文本数据和征信报告，通过预训练模型和 AI 算法，能够识别出 40 万维的风险变量，有效评估小微企业主的信贷风险。随着模型的不断迭代，大模型在智能风控上的潜力将进一步发挥。</p><p></p><p>在金融领域，生成式 AI 如 ChatGPT 有着广阔的应用前景，有望改变客户体验和金融业务格局。作为国内较早布局类 ChatGPT 应用的金融科技企业，度小满在大模型领域积累了一定技术实力，相信在未来度小满能带来更多惊喜。</p><p></p><p></p><h2>蚂蚁自研“贞仪”，进军生成式人工智能竞争舞台</h2><p></p><p></p><p>蚂蚁集团正在自研语言和多模态大模型，内部命名为“贞仪”。该项目正在由一个专门的部门创建，并将部署内部研究。随着谷歌和微软等美国企业探索生成式人工智能，蚂蚁也加入了百度和商汤等科技公司的竞争行列，人工智能已成为中美科技竞争的下一个大舞台。</p><p></p><p></p><h2>信也科技推动金融科技创新，助力小微企业发展数字经济</h2><p></p><p></p><p>信也科技借助大数据、人工智能等技术成功构建了 B 端金融机构与 C 端个人及小微商户间的借贷桥梁。通过风险识别和流程结合，信也科技能够从源头控制风险，为用户，尤其是小微企业主，提供快捷高效的资金撮合服务，帮助他们实现数字化升级。</p><p></p><p>与直接帮助小微企业相比，信也科技还持续支持银行业金融机构的发展。他们充分发挥自身技术优势，为中小金融机构提供数字经济工具，提升机构对中小微企业的信息识别和风险控制能力，从而间接增强对小微企业的金融支持。</p><p></p><p>信也科技的举措取得了显著成果。财报显示，信也科技 2023 年第一季度服务了 42.5 万户小微用户，其信贷服务占总交易量的 24% ，用数字科技帮助小微企业实现成本降低、效率提高和覆盖扩大。未来，信也科技将继续聚焦实体经济的积极发展，不断提升金融科技服务水平，成为广大中小微企业的强大后盾，助力他们实现长期可持续发展。</p><p></p><p></p><h2>联易融科技重塑供应链金融</h2><p></p><p></p><p>联易融科技是 A 股和港股两地唯一的供应链金融科技上市公司，致力于通过科技和创新来重新定义和改造供应链金融。</p><p></p><p>传统供应链金融存在诸多痛点，包括信用评估难以开展，贸易背景真实性审核难度大等。联易融科技作为金融科技领域的领头羊，多年以来一直用科技手段帮助供应链金融企业大幅改善以上的痛点。比如，联易融的平台通过 OCR 和 NLP 等人工智能技术（图像识别）为中小企业实现发票、法律文件的电子化，减少人工输入耗时；通过区块链技术，构建不可篡改、去中心化的供应链资产记账体系，便于融资及多级流转。</p><p></p><p>而随着大模型成熟，供应链金融科技可能加速成长，并进入发展的新阶段。大模型需要高质海量的数据集作为训练素材，因此金融领域的大模型专业壁垒极强，这构建了较高的竞争门槛，而这对联易融科技来说具有很大优势。联易融科技积累了丰富的 Know-How 知识和海量的客户数据，更值得一提的是，公司已针对不同应用场景打造了十几个成熟的 AI 小模型，通过叠、串联、平均等模型融合技术，训练好的小模型可以整合到一个大模型中。</p><p></p><p>在大模型的加持下，供应链金融行业可能会进一步被颠覆。联易融将不断加码研发，以技术综合地去挖掘数据带来的深层次价值，发挥科技在供应链金融业务中的价值。</p><p></p><p></p><h2>香港金管局与人民银行的合作进入第二阶段测试</h2><p></p><p></p><p>香港金融管理局正在研究数码港元的应用，并与人民银行合作测试<a href=\"https://xie.infoq.cn/article/82aed9b89ba19b776079df33f\">数字人民币</a>\"跨境零售支付。他们的合作进入了第二阶段测试。此外，香港金管局还启动了“数码港元”先导计划，与来自金融、支付和科技领域的 16 家入选公司合作进行试验。他们正在深入研究数码港元在全面支付、编程支付、离线支付、代币化存款、第三代互联网交易结算和代币化资产结算等六个范畴的潜在用例。同时，国际清算银行、中国人民银行数字货币研究所、香港金管局、泰国中央银行和阿联酋中央银行也积极推进多边央行数字货币桥项目。香港金管局还表示，他们发行了绿色债券，并创下了亚洲区内同类债券发行金额的纪录。此外，他们还发行了全球最大规模的绿色零售债券和全球首创的代币化绿债，展示了他们在推动普惠金融和金融创新方面的决心。香港银行体系保持稳健，存款总额增长约 2% ，超过 15.5 万亿港元，银行总资本比率为 20.8% ，远高于国际要求的 8% 水平。</p><p></p><p></p><h2>深圳启动数字人民币试点推广，保障预付式经营和消费者权益</h2><p></p><p></p><p>深圳市启动数字人民币在预付式经营领域的试点推广，以解决与预付式经营相关的问题。此举旨在保障消费者权益并降低监管成本。数字人民币的特点包括可控匿名、不可篡改和可编程等，可以应用于预付式经营的监管中。通过数字人民币智能合约管理预付资金，确保资金安全，并实现交易的顺利完成。此举还可以追溯预付资金的使用情况，防止资金挪用，为消费者提供维权保障。在深圳，银行已经开始在预付式教育培训和预付卡等领域探索数字人民币的应用场景。深圳市计划进一步拓展数字人民币在预付式消费监管中的应用领域，并构建开放应用生态，加强宣传推广，加大政策和金融支持，以提升公众对数字人民币的信心。</p><p></p><p></p><h2>南京银行江北新区分行上线首例“云财资”系统</h2><p></p><p></p><p></p><p>近日，南京银行江北新区分行成功为江苏某智慧城市集团上线了“云财资”系统，为该分行首例。云财资是一个依托云原生技术的综合金融服务平台，专注于为集团类企业提供云端资金管理服务。</p><p></p><p>南京银行江北新区分行以客户为中心，不断探索金融科技赋能实体经济的新途径。他们根据实体企业的需求灵活运用金融产品，帮助解决困难和痛点，促进业务发展与企业共同成长。未来，分行将坚持科技赋能的发展思路，建立专业团队，推动金融科技服务实体经济，为企业提供全方位的金融科技支持，成为企业发展的良好伙伴，共同促进江北新区的高质量发展。</p>",
    "publish_time": "2023-06-27 15:26:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC浪潮下，如何推动企业应用及落地？",
    "url": "https://www.infoq.cn/article/uui4NVydvW9GO32OA3tB",
    "summary": "<p>AIGC 是当前 AI 领域最热的技术话题，并在全球范围内掀起了一股热潮。可以看到，越来越多的企业开始重视 AIGC 相关技术创新和技术实践，并积极探索应用落地。有预测数据显示，到 2030 年，AIGC 的市场规模或将超过万亿人民币。</p><p>&nbsp;</p><p>那么，AIGC 技术典型的应用场景有哪些？不同行业如何落地 AIGC 应用？最适合大模型的商业模式是什么？未来是否会“人人都是提示工程师”？近日，InfoQ《极客有约》邀请到了星汉未来联合创始人&amp;CPO 胡忠想老师，为大家分享 AIGC 浪潮下，企业应用及落地经验。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href=\"https://www.infoq.cn/video/w6IoL0J8OBlcCC28SoPB\">https://www.infoq.cn/video/w6IoL0J8OBlcCC28SoPB</a>\"</p><p>&nbsp;</p><p>姜雨生：首先有请胡老师做下简单的自我介绍，也请您给大家介绍一下星汉未来这家公司。</p><p>&nbsp;</p><p>胡忠想：大家好，我是星汉未来的 CPO 胡忠想，主要负责星汉未来的产品。今天我想与大家分享我们在 AIGC 应用方面的见解以及目前的进展，同时，我也想给大家展示一些我们正在做的一些工作成果。</p><p>&nbsp;</p><p>大家可能对星汉未来还不太了解，在这里我想简单介绍下我们公司目前的一些合作项目。</p><p>&nbsp;</p><p>我们公司成立于2021年，是一家以技术驱动的专注于 Serverless 领域的公司。今年三月份，我们推出了一个“AIGC应用市场”的产品。我们结合了自身的技术创新精神，以让更多人体验到 AIGC 技术的价值和影响力为目标，通过我们这个应用市场，大家可以直接亲身体验 AIGC应用的能力。</p><p>&nbsp;</p><p>星汉未来应用市场：</p><p><a href=\"https://apps.galaxy-future.com/#/platform/explore&amp;utm_source=c\">https://apps.galaxy-future.com/#/platform/explore&amp;utm_source=c</a>\"</p><p></p><h2>市场规模将超万亿，AIGC缘何爆火？</h2><p></p><p>&nbsp;</p><p>姜雨生：最近一年，AIGC 技术在全球范围内已经火成“顶流”，您是如何理解 AIGC 的？为什么 AIGC 突然爆火？</p><p>&nbsp;</p><p>胡忠想：AIGC从今年2月份开始进入大众的视野，主要原因是<a href=\"https://www.infoq.cn/article/aM5zaAOM7aGEsmNHIXxs\">ChatGPT</a>\"。ChatGPT在2月份的时候月活跃用户突破了1亿，真正让AIGC走进了大众的视野。在过去，除了专业相关的人群，大多数人对AIGC和大型模型的了解相对较少。但是随着ChatGPT的爆火出圈，人们开始意识到AIGC，包括大模型，已经发展到了非常高的水平，在机器学习和人工智能方面的技术应用有了显著的进展。</p><p>&nbsp;</p><p>ChatGPT背后的<a href=\"https://www.infoq.cn/article/3TR13Qp694BJ8TeOi1xV\">OpenAI公司</a>\"也提供了API，涉及文本、语音以及文生图多模态等领域的应用不断涌现。这使得更多普通用户能够见识并亲身感受AIGC的威力。我认为AIGC突然火爆的原因是因为它开始真正解决一些实际场景的问题，而不仅仅是停留在人工智能的层面。而之前的人工智能处于技术和工程的阶段，它隐藏在一些产品背后，比如信息流算法、排序算法以及机器学习、强化学习等，对于普通用户来说可能感受不到。这是我对这一现象的理解。</p><p>&nbsp;</p><p>姜雨生：Stable Diffusion、ChatGPT 等现象级应用的出现，证明了 AIGC 的巨大潜力，有预测数据显示，到 2030 年，AIGC 的市场规模或将超过万亿人民币。这一数字符合您的预期吗？如此庞大的市场规模，给各方参与者带来的机遇与挑战是什么？</p><p>&nbsp;</p><p>胡忠想：根据我的理解和分析，如果将AIGC市场从国内扩展到国际范围，我认为之前的估计还过于保守。AIGC、大模型在市场上受到了广泛关注。除了OpenAI最早提出这一概念之外，微软和谷歌也纷纷加入，并将其能力集成到自己的产品中。在过去几个月里，国内的BAT（百度、阿里、腾讯）以及其他一些厂商也宣布自己拥有AIGC大型模型能力。这表明人们非常重视AIGC，因为它能够提高生产效率并创造价值，所以吸引了如此多的参与者进入市场。</p><p>&nbsp;</p><p>我预计到2030年，AIGC市场的规模肯定会超过一万亿元人民币，因为它已经具备了爆发前夕的特征，并且已经出现了实际可商业化的应用。</p><p>&nbsp;</p><p>我认为这个市场涉及几个层次的参与者。首先是一些大型巨头，比如百度、阿里等国内大公司推出了自己的AIGC应用，主要是在机器人领域。同时，国外的谷歌和微软也深度整合了他们的产品，并加入了AIGC能力。</p><p>&nbsp;</p><p>其次，还有一些独立的大型模型服务提供商，他们提供API能力。例如国外的OpenAI和国内的MiniMax、ChatGLM等，他们提供了基于大型模型的API，用户可以在其基础上构建自己的应用。这些服务提供商主要依靠计费模式，如按token计费，以覆盖算力成本并实现盈利。</p><p>&nbsp;</p><p>最后，基于这些基础的大型模型API，不论是在文本对话领域还是多模态领域都可以构建成熟的应用。例如，针对个人用户的Midjourney、Stable Diffusion，让普通人只需输入合适的提示词，就能绘制出专业图像的软件。此外，Adobe在其Photoshop中也加入了AI能力。国内的美图也开始推出相关应用。这些成熟的AIGC应用表明市场已经处于爆发前夕。随着API能力的提升和市场的成熟，这个领域的能力将越来越强，最终产生的爆发效应也将更加显著。</p><p>&nbsp;</p><p>姜雨生：当前 AIGC 还处于早期发展阶段，也有很多声音质疑落地难，根据您的观察，AIGC 技术典型的应用场景有哪些？不同行业如何落地 AIGC 应用？落地成本如何？</p><p>&nbsp;</p><p>胡忠想：AIGC 技术落地场景非常丰富，包括我们日常使用的智能客服和AI绘图等产品。以前，在搜索产品或在淘宝购买商品时，我们需要进行搜索，并依赖于排序算法和机器学习进行优化排序。然而，这些算法可能无法准确理解我们的真实需求，无法返回真正需要的结果。AIGC通过学习能够理解我们搜索背后的目的，给出真正对我们有价值的结果。这一突破性进展将产生巨大影响。例如，一些巨头如微软和谷歌也希望将AIGC能力整合到他们的搜索引擎中。与以往的搜索方式不同，这些搜索引擎将通过AIGC整理出符合人类思维的结果，提供更好的用户体验。</p><p>&nbsp;</p><p>在实时翻译领域，AIGC能力将更加突出。以前的翻译准确度完全依赖于样本和初始数据集，而现在可以实现纯粹的实时翻译，并且达到人类或专家水平。在图像领域，例如文生图，已经有一些公司开始将一定比例的基础图案交由AIGC应用生成，而无需人工绘制，这大大提高了效率。在视频领域，虽然还处于早期阶段，但已经有一些公司在探索，如根据文字生成相应的视频，实时识别视频内容并进行字幕生成和翻译等应用场景。</p><p>&nbsp;</p><p>从文字到语音再到图像和视频，这些应用的成本必然越来越高，因为训练和算力成本都非常昂贵。例如，之前提到的OpenAI的GPT-3模型，需要接近上万张显卡，并且一次训练的成本可能高达数百万美元，对于普通公司来说，这是非常昂贵的。然而，如果只是针对特定领域进行模型微调，训练成本相对可控，参数和数据集的需求也不大。</p><p>&nbsp;</p><p>最终的效果取决于特定行业的落地场景和专业度。在法律等垂直领域，数据量相对较小，最终产生的效果对于该行业已经足够好，并且成本可控。因此，对于不同的应用场景和专业度，需要权衡所需的效果和成本，包括参数数量、数据集大小和训练时间。</p><p>&nbsp;</p><p>姜雨生：目前很多公司都在布局 AIGC，星汉未来在 AIGC 方面取得了哪些进展？为什么会关注这一领域？</p><p>&nbsp;</p><p>胡忠想：我们公司专注于算力优化和提供Serverless技术，以帮助中小企业和个人开发者发展业务，而无需管理算力和相关运维，从而节省算力成本和运维成本。AIGC与这一趋势非常相似，因为在AIGC应用的发展中，需要大量的GPU算力。对于中小型开发者来说，如果他们想开发AIGC应用，需要管理这些算力的部署和运维。这与我们目前正在做的工作非常契合，我们正式专注于为AIGC应用提供算力优化。</p><p>&nbsp;</p><p>我们创建应用市场是想让更多普通用户能够体验AIGC应用。因为想要体验AIGC应用，需要部署一个开源模型。这个模型可能效果很好，可以提供很好的问答体验、优化和文案编程等。但是对于普通用户来说，想要体验这些应用，需要具备两个条件：一是将模型部署起来，二是购买和管理算力。</p><p>&nbsp;</p><p>对于中小型开发者来说，这方面是非常复杂的工作。而我们的技术可以简化这一过程，对于普通用户来说，他们只需要点击应用就能快速完成安装，然后立即体验。</p><p>&nbsp;</p><p>不论是绘图应用、聊天机器人还是某些专业领域的应用，比如根据文字生成PPT、生成图表或总结PDF内容等，用户可以随时安装，随时体验，随时使用这些AIGC应用，并判断它们是否能为自己的工作或公司带来价值。</p><p></p><h2>AIGC 应用探索</h2><p></p><p>&nbsp;</p><p>姜雨生：星汉未来应用市场里有多款 AIGC 应用，比如 AI 智能对话、AI 绘画、AI 编程等，您能简单介绍下这些 AIGC 应用吗？企业/个人用户如何使用？</p><p>&nbsp;</p><p>胡忠想：我们的应用市场具有清晰的分类，用户可以在应用市场中找到AIGC文字、AIGC语音和AIGC图像等常用类别的应用。此外，我们还提供AIGC API，以满足企业和中小开发者构建应用的需求。还有一些企业应用和中间件功能也可在应用市场中找到。</p><p>&nbsp;</p><p>我们还为各个领域选择了一些典型的应用，例如AIBot（文字类别的AIGC应用），AIGC的语音应用SpeechGPT（可与机器人进行对话），以及AIGC图像类别的应用，例如StableDiffDiffusion Pro（能根据提示词绘图）和ChartGenie（根据自然语言输入生成柱状图或饼图等）。AIGC API可供用户在其上构建自己的应用。此外，我们还提供常用的企业应用程序，如WordPress和文件管理系统，以及中间件服务，如MySQL、Kafka和Redis。</p><p>&nbsp;</p><p>我们的软件安装后不用再自己部署开源软件所需的算力，就可将应用部署起来。实际上，我们在云上购买了机器算力，并将应用部署在其中。安装完后，您可以打开应用并在其中体验应用的功能。</p><p>&nbsp;</p><p>AIBot应用利用开放的AI能力，可以调用国内的AI供应商提前训练好的数据和模型，以帮助你整理信息。以前像“三天走完五岳”这样的旅游策划肯定需要人工来做，但现在，AIGC应用可以告诉您，“三天走完五岳”是非常困难甚至不可能的，因为五座山分布在不同的地区。</p><p>&nbsp;</p><p>姜雨生：目前上线的哪些应用活跃度会比较高呢？</p><p>&nbsp;</p><p>胡忠想：目前我们的主要应用面向普通用户。从目前的情况来看，有两类应用得到了广泛使用。一类是类似于ChatGPT的AIBot应用，它们能够理解自然语言和逻辑，并能回答用户提出的问题。这类应用在日常学习和工作中非常受欢迎，无论是用于文案的改进还是论文的润色和翻译等方面的需求都很大。</p><p>&nbsp;</p><p>另一类应用是文生图相关的应用，许多人希望在自己的工作中使用这样的工具。这类应用更多地被某些专业领域的人使用，他们希望借助这些工具来生成设计或绘画，因为这些工具能够满足他们专业领域的需求，而又无需自己购买GPU服务器。</p><p>&nbsp;</p><p>目前来看，这两类应用比较常见。然而，随着AIGC技术在更多领域产生更多价值，并且在一些垂直领域进行调优以及模型效果的不断提高，基于基础构建的应用会更多地涌现出来，可用性也将达到日常使用的水平，进一步丰富我们的应用库。</p><p>&nbsp;</p><p>姜雨生：在实际使用产品时，特别是与AIGC这类大模型相关的日常开发中，我们常常对模型有一些特定要求。例如，用户可能希望使用GPT-3或GPT-4模型。企业上会不会有类似这种需求，目前有相关技术支持吗？</p><p>&nbsp;</p><p>胡忠想：针对不同的模型，我们提供了相应的API支持，正如之前给大家演示的那样。我们的API可以用于部署各种不同的模型，甚至为企业提供定制化模型，并提供专门的API接口。我们的关注点主要是模型的部署以及在此基础上进行微调。核心是如何快速而准确地调度大规模的算力，并实时进行优化。</p><p>&nbsp;</p><p>举个例子，当GPU的使用量较小时，能否快速缩减算力的使用；而在使用量较大时，能否弹性地调整算力分配。对于企业来说，在为用户提供服务的同时降低成本是非常关键的，而我们的主要目标就是解决这一层问题。企业可以通过调用我们的API来使用这些大模型，而无需自行维护背后的算力基础设施。</p><p>&nbsp;</p><p>姜雨生：AIGC 领域下面，我们可能会提到另一个关键词就是 <a href=\"https://www.infoq.cn/article/7FGafFFFYkbMxW11u0BT\">prompt engineer</a>\"，在与生成式 AI 交互的过程中，如何编写我们的问题，其实也是尤为关键的，在这方面我们星汉未来的产品，是如何引导用户编写 prompt 的呢？对于用户实际提交的 prompt，又会从哪些方面进行优化呢？</p><p>&nbsp;</p><p>胡忠想：我们在应用市场上架的大部分应用都是原生的，即将开源产品或模型部署为实际的应用供用户使用。对于用户提交的实际需求，通常不会进行修改，除非有特殊情况。当然，在一些常用的应用上，我们会提供教程，以帮助用户使用。我们的应用市场不仅提供应用下载，还提供应用预览和教程，其中会指导用户如何使用并分享经验。例如，对于ChartGenie应用，我们会提供教程，告诉用户输入什么样的提示词，以及应用会为其绘制出什么样的图像。</p><p>&nbsp;</p><p>对于类似的SD应用，我们也会提供教程，指导用户如何填写提示词，包括正向或反向参数的使用等。此外，我们还有一个独特之处。因为SD应用在输入英文提示词时效果较好，但对于许多普通用户来说，可能不擅长清楚描述对应准确的英语单词。针对这个问题，在SD应用中，我们实现了自动将用户输入的中文提示词翻译为英文提示词，从而提高生成图片的效果。</p><p>&nbsp;</p><p>姜雨生：目前的主流市场，包括 OpenAI、Azure OpenAI，目前都不是完全开放的状态，主要原因还是目前的算力和容量可能无法满足全部的客户使用，我们目前的产品上面，是完全开放的状态么，还是需要客户提交申请？我们的价格又是如何计算的呢？</p><p>&nbsp;</p><p>胡忠想：以AIBot为例，我们对接的是商业API。通常情况下，用户想要使用商业API，需要拥有自己的密钥等信息。对于普通用户来说，申请这些东西其实是很困难的，尤其是对于非开发者而言。</p><p>&nbsp;</p><p>我们帮助用户屏蔽了这一层复杂性。用户可以像使用普通应用一样使用我们的服务，而无需单独购买算力等。对于一些对外开放的应用，它们需要考虑背后算力的部署，即需要准备多少算力来支持服务。但对于我们来说，情况不同。我们底层基于Serverless，只有当你使用应用时，我们才会申请算力为你提供服务，无论是CPU还是GPU。当你暂停应用或者长时间不使用时，会自动降低算力分配，以避免浪费。因此，在应用成本方面，你无需担心，不需要提前囤积大量算力来提供服务。</p><p>&nbsp;</p><p>姜雨生：AIGC 包括多种内容形式，比如文生图、语音生成、写作、编程等，当前 AI 在这些方向的生成效果分别是什么样的？有做过一些 benchmark test 么？您使用过哪些 AIGC 应用，能和大家分享下您的使用感受吗？</p><p>&nbsp;</p><p>胡忠想：我们主要专注于应用市场，没有进行这方面的测试。不过，无论是国内还是国外，都有一些针对大型模型的成熟基准测试方法。最近出现了一个针对大型模型的排名，其中GPT 4.0排名第一，Claude开源模型排名第二，然后是GPT-3.5&nbsp;Tubor。这些测试从多个方面对模型进行评估，包括图像生成、语音转写、编程等方面的准确性和理解能力进行评分。国内也有专门的测试集，用于评估国内大型模型的能力。</p><p>&nbsp;</p><p>目前，在文本领域的自然语言理解和编程方面，AI已经相对成熟，并且已经达到了很高的水平。据测试结果显示，在编程文本理解方面，AI已经能够达到70%或80%的人类水平，已经取得了很好的效果。</p><p>&nbsp;</p><p>在其他领域，例如视频方面，AI的表现可能还有待提升，因为这方面的训练需要更大的算力和更大的数据集存储，所以目前尚未达到能够直接用于日常生产的水平。至于图像生成方面，基于我们的测试感受，我认为AI在该领域已经达到了相当高的水平。当然，在专业领域可能会有一些对细节的要求，此时进行二次修改可以取得非常好的效果。</p><p></p><h2>大模型的商业化路径</h2><p></p><p>&nbsp;</p><p>姜雨生：最近，ChatGPT 突然上架苹果应用商店，让我们看到了 OpenAI 在商业化上的又一探索。目前 AIGC 已经达到大规模商业化的条件了吗？在您看来，最适合大模型的商业模式是什么？</p><p>&nbsp;</p><p>胡忠想：我认为目前已经初步具备了商业化的条件，4月OpenAI还需要排队才能接入，但现在已经不需要排队了，这表明他们已经准备好商业化了。他们还开始开发一些客户端，并逐步在不同国家的应用市场上架，证明他们的商业化流程已经走通。月活跃用户也在不断增加，这说明它不仅仅是一个智能问答工具，而是成为了一种必需品。MG作为一家商业化公司，虽然只有20多个人，但他们的商业化能力已经非常强，这表明AIGC的应用已经走向商业化，处于初期阶段。</p><p>&nbsp;</p><p>我认为最适合的商业模式有两种。一种是基于开源通用大模型，在此基础上针对专业领域进行微调，为垂直领域的客户提供服务。例如，在法律领域和医疗领域等提供专业模型，这些模型具有专业参数和数据集，并能切实产生实际价值。另一种是各种提高生产力的工具，像MG就是一个明显的例子，它具有创意和革命性的应用，完全改变了普通人对绘画难度的认识。以前人们可能认为普通人根本不可能画出那么专业的图像，无论是使用Photoshop还是其他工具。现在，你只需要学习如何使用提示词，就能画出精美的图片。因此，如果能在大模型之上开发出革命性的应用，将会带来巨大的商业价值。</p><p>&nbsp;</p><p>姜雨生：如果说很多小公司规模不大，没有强大的算力来自主研发大型模型，那么它们是否能在这个领域进行创业呢？</p><p>&nbsp;</p><p>胡忠想：我认为这是非常有可能的。就像当初iPhone问世时，人们对于iPhone的应用商店的理解并不深入，但随着越来越多的开发者逐步开发各种应用来服务于人们的日常生活，如今人们已经离不开这些应用了。类似地，如果将这些应用基于AIGC或AGI的操作能力进行进一步扩展，从而产生更大的价值和更优的用户体验，那么这个领域还存在着巨大的开发潜力。</p><p>&nbsp;</p><p>姜雨生：大概是五六年前，当时很多人都提出了“人人都是产品经理”这个概念，希望大家都从事产品经理的角度去思考事情。未来是否会“人人都是提示工程师”？</p><p>&nbsp;</p><p>胡忠想：Prompt engineer是最近才出现的一个岗位，一些公司已经开始招聘这类工程师。我理解，这个角色实际上是基于对底层模型和大模型原理的理解，才能在这个岗位上做得好。因此，这个角色是非常专业的，不仅仅是懂得一些提示词之类的表面技能，而是需要相当专业的知识。目前来看，要具备这种能力的普通用户或想要从事这个领域的人，门槛是存在的。</p><p>&nbsp;</p><p>就像“人人都是产品经理”这句话，虽然说任何人都可以参与，但这并不意味着每个人都能够达到产品经理的要求。同样，Prompt engineer也并不意味着任何人只是给出一些提示词，就能够真正让模型发挥相应的作用，这需要专业的能力水平。因此，这方面肯定还是需要更专业领域的人来从事。当然，应用的趋势是对提示词的要求越来越低，降低普通用户的使用门槛。</p><p>&nbsp;</p><p>姜雨生：当前的AIGC技术在安全和伦理方面可能存在一些争议，这是一个被广泛讨论的话题。最近，包括美国政府在内的各方都提出了与OpenAI等公司进行合作以解决这些问题的想法。治理方面需要考虑数据安全和使用安全等方面的限制。在企业应用AIGC技术的过程中，我们应该如何应对这些治理挑战呢？</p><p>&nbsp;</p><p>胡忠想：首先，我们需要关注数据安全的几个方面。一方面是确保数据本身的安全，包括保护用户的隐私等方面。我们可以以三星公司的例子为警示，该公司的内部员工曾将公司数据意外暴露在公网上，可能是因为他们使用了ChatGPT等技术，而该技术本身并没有对数据施加限制。另一方面，一些欧洲国家已经开始对ChatGPT等技术进行限制，不允许使用，我国对ChatGPT也有一定的限制，这与当前全球范围内对数据保护的要求有关。企业应用ChatGPT等技术时需要考虑数据安全的要求。</p><p>&nbsp;</p><p>其次，道德伦理也是需要关注的方面。目前可能尚未到达迫切的阶段，但一些有远见的人已经开始关注这个问题。例如，OpenAI的创始人主动呼吁政府设立专门组织来管理大型模型的发展，以确保其进化方向符合道德伦理，避免产生违反人类基本道德伦理的知识或造成无法控制的后果。这是一个需要考虑的问题，尽管当前可能尚不构成问题。</p><p>&nbsp;</p><p>姜雨生：未来在 AIGC 方面我们有什么样的规划吗？比如说会不会考虑帮助企业从 Prompt 到输入到输出再到模型定制化，做一些真正的企业定制化的应用等。</p><p>&nbsp;</p><p>胡忠想：我们当前的应用市场主要针对那些满足刚需或真正对用户日常工作有价值的应用，以展示AIGC的能力。然而，对于开发者和企业而言，他们需要更专业的模型和定制化支持，特别是在处理大量用户和保证算力供应方面。</p><p>&nbsp;</p><p>为满足这些需求，我们的下一个重点是为企业提供定制化模型的能力，同时利用我们的Serverless能力。企业只需提供数据和调用API，模型将根据API调用提供相应的结果，无需担心模型部署和算力维护等方面的管理。这将成为我们未来关注的重点，同时我们也将致力于解决一些模型简化的问题。</p><p></p><h2>AIGC 时代下的人才规划</h2><p></p><p>&nbsp;</p><p>姜雨生：随着AIGC应用领域的不断扩大，甚至可能在很多领域广泛应用AIGC之后，AI和人类之间的交互关系将会是怎样的呢？我们会完全依赖AI的能力来完成日常工作吗？</p><p>&nbsp;</p><p>胡忠想：我认为AIGC领域的广泛应用将会提升各行各业的工作效率并创造更高的价值。随着AIGC应用的不断发展和算力的增强，成本也会降低，我们能够像使用智能手机上的应用程序一样使用AIGC应用程序，使其成为日常生活中不可或缺的工具，并且用户体验将越来越好。因此，AIGC肯定会给我们的日常生活和工作带来越来越大的价值。</p><p>&nbsp;</p><p>关于AIGC与人类的关系，我认为它不会完全取代人类，但会减少低层次和重复性劳动，并提高效率。另外，AI的发展方向应朝着对人类有益的方向发展，这需要整体上进行一些管理和监督。</p><p>&nbsp;</p><p>姜雨生：AIGC 最主要的应用价值是降本增效吗？这类大模型具体会如何改变我们的工作状态？</p><p>&nbsp;</p><p>胡忠想：刚才我们提到，AI在某些领域已经接近甚至达到了人类水平的60～70%。因此，在一些通用场景下，AI可以解放人类的双手和大脑。自然语言理解能力可以帮助运营人员；编程理解能力可以帮助程序员；对于文生案和绘画能力可以帮助设计师和艺术家提高效率。当然，人们也担心AI是否会取代一部分工作。</p><p>&nbsp;</p><p>我的看法是，AI确实会减少一些基础性的工作，就像机械化生产线替代了一部分体力劳动者的工作一样。但是通过AI辅助，人类在其基础上创造更高价值或更优质的工作，实际上会促进工作的发展。如果你掌握这些能力，就不会被取代，反而能提高工作效率并创造更大的价值。</p><p>&nbsp;</p><p>姜雨生：企业应用 AIGC 技术后，在招聘和人才规划上需要做哪些调整？AIGC 时代需要什么样的人才？</p><p>&nbsp;</p><p>胡忠想：有报道称某家公司只保留了20～30%的设计师，因为他们只需要非常熟练的设计师在AI生成的图像上进行调优和优化。基础的草稿图和基本元素图已经不需要外包或初级技术人员来完成。这种情况在绘画领域和编程领域都存在。在软件开发领域，一些基础性的任务可能会结合低代码开发和自然语言理解的能力来实现。因此，企业对于外包绘画和编程领域的需求可能会减少，这两个领域的人可能会首先受到影响。</p><p></p><h4>嘉宾介绍</h4><p></p><p>&nbsp;</p><p>特邀主持：</p><p>&nbsp;</p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p>&nbsp;</p><p>嘉宾：</p><p>&nbsp;</p><p>胡忠想，星汉未来联合创始人&amp;CPO。北航本硕，2012年加入微博，2015年作为技术负责人负责S级项目Feed核心业务的研发。2017年作为技术负责人带领团队完成公司级Weibo Mesh平台的研发并推广到多个核心业务，使得微博成为业界领先的Service Mesh实践者。2018年作为微博峰值热点应对项目的负责人，带领团队完成公司级热点应对联动机制的建设，保障了微博在后续多次热点事件中的稳定性。2021年作为联合创始人，成立星汉未来并任CPO。</p>",
    "publish_time": "2023-06-27 15:33:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "承接百亿保单的新基建，无界山到底是什么山｜InfoQ 《一探到底》",
    "url": "https://www.infoq.cn/article/ZhIJE05sJgdxkPdYci37",
    "summary": "<p>新基建的推动使得数字化转型成为各行各业的必然趋势，众安保险也从其中看到机遇，把握机遇并通过加强创新，探索出共建共赢的新模式。《一探到底》众安之行的第三期，带大家探访保险行业的一座“神山”——无界山的前世今生，并畅谈如何在新基建时代通过技术创新实现合作共赢。</p>",
    "publish_time": "2023-06-27 16:42:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "宁德核电：硬核工业数字化人才培训方法的全面解析",
    "url": "https://www.infoq.cn/article/QQM8yoEKGDjF9TjyA50N",
    "summary": "<p></p><p></p><p>演讲嘉宾｜徐峰涛，中国广核集团福建宁德核电有限公司培训部经理、数字化转型组培训负责人</p><p>编辑｜罗燕珊</p><p></p><p>宁德核电是我国工业领域的数字化典型企业。自 2021 年至今，宁德核电的数字化从快速启动期迈进了关键转型期。</p><p></p><p>在日前举办的<a href=\"https://app.jingsocial.com/microFrontend/leadGeneration/jsf-leads/list/webinar/P8KAMuAA7DmVHnBydPuGL8/C2CviyZ3WNSRYyMpEC98C4\">DTDS 全球数字人才发展线上峰会</a>\"上，中国广核集团福建宁德核电有限公司培训部经理、数字化转型组培训负责人徐峰涛介绍了宁德核电人才的系统化培训方法（SAT），这套路径和方法覆盖从需求分析、大纲设计、材料开发到培训实施、效果评价 5 个阶段。</p><p></p><p>据了解，该培训方法是宁德核电内部数字化人才梯队建设和人才培养规划非常重要的抓手。此外，面向数字化，宁德核电已经从战略、思维、执行三个层面梳理了 11 项人才能力维度，并且每个能力维度根据表现不同分为 4 个级别，从而构成了数字化人才胜任力模型。</p><p></p><p>徐峰涛强调，数字化转型一定要“转”，并且一定要按照自己的节奏“转”。而在这个过程中，数字化转型成功与否，与<a href=\"https://www.infoq.cn/article/wliPhHXvgVJrKIs52RYk\">人才队伍建设</a>\"密不可分。</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><p>本次分享的主题是《人人都是数字革命贡献者——硬核工业的数转之路》。下面我的分享主要分为四个部分，第一，在能源行业，尤其是核电行业，数字化转型到底是锦上添花还是势在必行；第二，宁德核电的人才培养之道；第三，宁德核电的数字化人才发展规划以及“300-30-3”人才梯队建设；第四，宁德核电这两年的阶段性成果，以及对数字化未来的展望。</p><p></p><h2>核电数转，锦上添花还是势在必行</h2><p></p><p></p><p>近年来，国家和行业都提出了大力发展数字化的要求，这也引发了我们公司内部对核电行业是否进行数字化转型的讨论。为此，我们进行了充分的分析和研讨，主要从以下因素考虑：</p><p></p><p>支持力度：国家在数字化转型方面给予了巨大支持，为我们提供了特别好的机会。集团规划：我们的集团已经对数字化转型进行了详细规划，为我们提供了指导和方向。核安全优先：在核能行业，核安全一直是至高无上的重要因素。然而，这也导致在业务流程中我们普遍采取较为保守的策略，这是我们面临的一个挑战。业务流程规范和数据完整：核电经过多年发展，我们内部拥有一套完整的业务流程、制度和体系，并且积累了大量的过程数据。复杂性和惯性：核电是一个庞大而复杂的系统，转型过程中存在较大的惯性，而且重资产转型也比较困难。因此，数字化转型不可能一蹴而就，需要谨慎进行。</p><p></p><p>基于以上考虑，我们公司最终达成了一致意见：必须开展数字化转型的工作，但必须按照我们自己的节奏来平稳、安全、可靠地进行。在这个背景下，中国广核集团已经制定了三步走的数字化转型规划，包括快速启动期、关键转型期和智能应用期。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/431601206f7da86bc4f16f21dbcfdfc9.png\" /></p><p></p><p>我们已经确定了数字化转型是核能行业的必选项，因此如何做好数字化转型成为摆在我们面前的难题。</p><p></p><h2>企业培训的基础“算法”</h2><p></p><p></p><p>数字化转型的成功与否与人才队伍建设密不可分。在核电行业，特别是在宁德核电，我们已经拥有一套完善的针对运行和维修技术人才培养的方法，称之为“核电厂系统化培训方法”（SAT）。这套方法主要分为五个阶段：需求分析、大纲设计、课程开发、培训实施和效果评价。这种培训方法本身也是一个严谨的“算法”，下面我给大家简单做个介绍。</p><p></p><p>首先，在培训正式开始之前，我们必须确定培训的需求。以宁德核电公司为例，我们公司内部有540个岗位，每个岗位有100至1000个工作任务，合计约10万个岗位任务。基于这10万个任务，我们进行整体的分析，使用DIF（Difficulty, Importance, Frequency）分析方法，确定需要培训的任务清单。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac4bf9895456072378ba93689d5e056b.png\" /></p><p></p><p>以我们电站的某个岗位为例，每个岗位都对应一个任务编号，并有具体描述和任务来源。我们使用DIF打分法对任务来源进行评估。</p><p></p><p>举例来说，如果员工在工作中需要具备某项能力，而该能力的难度或重要性较高，我们将判定该能力或任务是需要接受培训的。如果这项能力在工作中的使用频率较高，那么只需要进行一次性的初始培训。如果这项能力在工作中的使用频率较低，那么除初始培训外，还需要定期进行复训。通过DIF分析，我们可以确定需要培训的任务以及培训的方式和类别。</p><p></p><p>确定了培训类别后，我们将需要掌握的任务清单有机地分类组合，分为知识（K）、技能（S）和态度（A），生成培训大纲和培训课程。对于知识类，我们组合形成标准的课程培训课程，解决学员的知识问题。对于技能类，我们依靠课堂教学和实操相结合的方式，通过动手操作来解决技能掌握情况。一旦学员掌握了如何做某件事以及如何做好某件事，我们还需要培养他们在实际工作中正确应用相应知识和技能的态度，以提高工作质量。</p><p></p><p>通过上述整理，我们将形成一套标准的课程体系，包括高标准的培训教材。下图给大家做个展示，教员手册是教员在培训中使用的手册，其中包括课程要点简述、模块所需时间以及教学活动和要素分析。在课程中，教员根据这份清单，确保每个要素都能完整呈现，以保证培训与目标和效果的有机结合，确保达成培训目标。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/1522204e5b05598896594e4e4e155b6e.png\" /></p><p></p><p>高标准培训教材</p><p></p><p>培训结束后，我们必须关注培训效果。在培训评估中，在培训界大家都听说过柯式的四级评估，第一级是学员反应，第二级是学习效果。这两个阶段的问题都不大，较容易衡量是否达到目标。</p><p></p><p>然而，我们更应关注第三级评估，即培训结束后学员在业务场景中行为的改变，以及第四级评估，即业务结果。换句话说，学员行为改变后，对业务是否有贡献、绩效是否有提升，这是培训整体评估中不可或缺的两个方面。</p><p></p><h2>数字化人才发展规划与“300-30-3”</h2><p></p><p></p><p>基于上述宁德核电的培训逻辑和体系，我们将其应用于数字化人才培养和人才梯队建设。以数字化转型能力模型为例，我们首先对组织所需的数字化能力进行梳理，确定了 11 个数字化人才的能力维度，涵盖了战略、思维、执行等不同层面。每个能力维度根据表现强度分为 4 个级别，总计形成了 44 个不同的能力维度和表现强度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/662ef1e0523747f8b43526d661653198.png\" /></p><p></p><p>基于这些能力维度和表现强度，我们建立了完整的数字化人才胜任力模型。在整个系统中，我们有一个详细的能力模型分布。以战略层面为例，针对专业人才、应用人才、管理人才以及战略人才等不同类型，我们确定了不同维度的权重。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/41/41bf0e4210d3a9d1fa7da34bce87bb2b.png\" /></p><p></p><p>通过分析，我们制定了数字化人才培养的三年规划。首先，我们计划培养出300位数字化应用专家，他们具备基本的软件和编程技能。其次，从这300位专家中，我们将培养出30位数字化项目经理，他们不仅熟悉数字化应用，还了解业务流程，能够将业务流程与数字化工具和方法结合，进行流程再造和优化。最后，我们将培养出3位数字化战略人才，能够为公司未来的数字化转型提供整体规划。这就是我们300-30-3的数字化人才培养计划。</p><p></p><h2>认知培训先行</h2><p></p><p></p><p>以数字化培训课程为例，这是基于前面提到的KSA分析的数字化培训内容。针对不同的对象，我们确定了培训内容的重要性和培训方式。基于此，我们第一步率先开展了针对全员意识宣贯的培训。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/7646c2cdd4a8030458a26acd8b077429.png\" /></p><p></p><p>在项目初期，我们也进行了一系列需求调研，得出了四个需求。首先，转型动力不足，由于核电行业注重安全，容错空间较小，并且业绩相对良好，许多人担心数字化转型带来一些不确定性。其次，引入数字化转型体系可能需要对现有流程进行优化再造，可能带来较高成本。第三，过去很多员工往往将信息化误认为数字化，他们对数字化思维模式的转变尚不够彻底，因此需要进行宣传培训。</p><p></p><p>同时，我们希望通过这一系列培训，对数字化人员能力模型和体系进行迭代、升级和固化。</p><p></p><h2>开展应用培训，落地场景应用</h2><p></p><p></p><p>这是我们第一阶段全员数字化思维转换的课程清单。通过这个清单，我们对公司的1000多名员工进行了整体的数字化转型培训，统一了方向和思路。目前，这门课程已获得我们行业协会的认可，并成为电力行业数字化转型的标准化课程，在外部得到了广泛传播。这些都是我们取得的一些初步成果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89e6d80dfae95e2aa88c43784f9e76b3.png\" /></p><p></p><p>在员工思维统一之后，我们接下来在应用层面对员工和数字化人才进行培养。例如，我们提供了一门Python培训课程。这个课程包括网络课程、线下面授、编程实战应用和在岗实践，并最终展示培训成果。通过全方位的提升培训效果和达成培训目标，我们的培训能力和体系也得到了不断迭代和升级。</p><p></p><p>在前期工作的基础上，我们现场的员工和数字化核心团队成员已经具备了一定的数字化能力。然而，我们对培训效果的评估主要关注于第三阶段和第四阶段，即如何将学习心得与业务场景实际应用相结合，以提升公司绩效。</p><p></p><p>在这方面，我们取得了一些小的成果。2022年，我们的数字化团队为公司内部的18个业务场景提供了新的解决方案，例如堆芯数据的可视化项目等。我们还举办了一个创客大赛，通过比赛激励学习和转化，营造了数字化成果持续落地的氛围。</p><p></p><p>以下是一些具体的例子：首先是我们开发的网络课程，通过数字化项目的落地，我们将网络课程开发流程简化为四个步骤，包括新建课程、选择教员并拍摄形象、将标准化讲师手册作为剧本输入系统，然后一键生成带有视频和音频的网络课程。这大大提高了网络课程的质量和效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee48304b950d621624f7bf58e7c86642.png\" /></p><p></p><p>除了讲课过程，我们在培训设施中也广泛应用数字化技术。例如，我们建立了一个数字化急救培训基地，并充分利用3D和VR技术，对现场厂房设备进行全方位扫描，开发了全景VR教学场景。</p><p></p><p>此外，在实际生产中，我们的数字化学员和核心人才进行了许多尝试和探索，由点到面实现了小步快跑。左侧的图示是我们的AR头盔智能检修场景，工程师通过数字化培训后自主开发的系统，借助头盔可以在办公室内实时查看设备和系统的运行状态、参数以及故障预测、诊断和消除等。这个系统得到了央视财经频道的专题报道，并已推广到航天领域。右侧的图示是我们的机器狗自动巡检项目，它可以在厂房内全天候实时监测设备，极大地减少了人力成本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/eff14c7e7da08d3748ff085c23a863d6.png\" /></p><p></p><p>通过各个单点数字化应用场景的尝试，我们还建立了一套基于机器学习模型的设备健康状态智能预警系统。这个系统将数十万台设备和系统综合在一个画面中，可以在作战指挥中心随时调控各类设备的运行状态，为电站的安全可靠运行提供有力保障。</p><p></p><p>通过两年的不断摸索和前期成果的积累，我们的工作也得到了外部的认可。我们很荣幸成为新华网创建的元宇宙及AIGC创新联盟的成员单位，并与新华智云合作，计划打造核电行业内首个MGC实验室，持续进行数字化项目的创新工作。</p><p></p><p>近期，<a href=\"https://www.infoq.cn/article/xiFWKht6NdjACd91Be6V\">ChatGPT</a>\" 成为热门话题，我们在数字化转型中紧紧抓住这一热点，并与一些高校合作。通过挖掘核电内部过程数据和构建知识库工具，我们通过模型训练建立了智能知识库和知识图谱。未来，我们希望通过直观的人机交互方式，工程师能够快速访问和应用智能知识库中的信息，从而快速高效地运用我们的知识成果到业务场景中。</p><p></p><p>经过两年多的探索和实践，我们在硬核工业的数字化转型中充分调动了员工的积极性，营造了每个人都是数字革命贡献者的氛围。通过数字化转型项目的实施，我们显著提升了电厂设备的可靠性，降低了事故概率，有效帮助电厂实现了安全标准化。以上是宁德核电站在数字化转型和数字人才培养过程中的探索和实践成果，欢迎大家就数字化相关话题给我们提出宝贵的意见。</p><p></p><p>关注「InfoQ 数字化经纬」公众号，输入 “宁德核电” 即可下载本次演讲的 PPT。 我们将持续为您推送更多、更优质的数字化案例内容和线上线下活动。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/50b370bc595b9388c18d29dc1e593ebd.png\" /></p><p></p><p>延展阅读：</p><p><a href=\"https://www.infoq.cn/article/PQFkc7K5Dxdf3hbiWSgs\">《传统管理秩序消失，数字化下的组织和人才如何重塑》</a>\"</p><p><a href=\"https://www.infoq.cn/article/6rPg98g1JfvBEfRvsrDh\">《12 维度能力画像、1+3 认证体系，东亚银行如何搭建数字人才体系》</a>\"</p><p><a href=\"https://www.infoq.cn/article/zvcfOe2sHhl6wW094OvV\">《南京钢铁：用精准培养方式“填补”人才缺口》</a>\"</p>",
    "publish_time": "2023-06-27 17:12:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "MongoDB 官宣多项新产品和新计划，让现代化应用程序构建更轻松",
    "url": "https://www.infoq.cn/article/rbeNkyvWdjV8Qc88trPE",
    "summary": "<p>近日，MongoDB在MongoDB用户大会纽约站上宣布推出了一系列新产品和新计划，帮助客户更快、更轻松地为任何工作负载或用例构建现代应用程序。</p><p>&nbsp;</p><p>此次MongoDB推出的一系列新品和新计划包括：</p><p>1、MongoDB携手谷歌云发布AI计划。开发人员现在能够充分利用MongoDB Atlas以及与谷歌云Vertex AI大型语言模型（LLM）整合后带来的行业领先优势，并借助双方的专业服务，快速启动架构审核，加快软件开发进程。</p><p>&nbsp;</p><p>2、MongoDB推出“AI创新者计划”，为正在致力于开发AI技术的组织提供MongoDB Atlas使用积分，借助MongoDB合作伙伴生态系统的合作机会及丰富的产品上市推广活动，助力组织机构加快创新步伐并缩短产品进入市场时间。</p><p>&nbsp;</p><p>3、开发者数据平台MongoDB Atlas 推出了五项新功能，包括：MongoDB Atlas Vector Search的生成式AI功能，以实现高度相关的信息检索和个性化；为企业级搜索工作负载提供专用资源的MongoDB Atlas Search Nodes；用于处理高速复杂数据流的MongoDB Atlas Stream Processing；扩展性及效率得以显著提升的MongoDB 时间序列集合；以及借助MongoDB Atlas Data Federation可在Microsoft Azure上查询数据和隔离工作负载的新功能。</p><p>&nbsp;</p><p>4、MongoDB发布MongoDB Atlas行业计划，并推出面向金融行业的解决方案。MongoDB Atlas 行业计划为客户提供MongoDB专家主导的架构设计审阅、增强型解决方案以应对特定行业挑战的技术合作伙伴关系，以及针对特定行业的知识加速器，为开发团队提供相关技术的培训路径。</p><p>&nbsp;</p><p>5、MongoDB全面推出MongoDB Relational Migrator。MongoDB Relational Migrator 的推出实现了在不停机情况下，更快速、更轻松地从传统数据库技术迁移至MongoDB Atlas开发者数据平台。</p><p>&nbsp;</p><p>MongoDB CEO兼总裁Dev Ittycheria表示:“此次发布MongoDB Atlas各项新功能，正是响应了我们每天所听到的客户需求和建议。客户期待团队借助MongoDB Atlas加快创新步伐，助力整个企业实现更多发展。MongoDB Atlas各项新功能将进一步支持客户运行规模最大、要求最严苛且需要不断提高可扩展性和灵活性的任务关键型工作负载，借助下一代应用程序发挥软件和数据的价值，并利用MongoDB Atlas单一开发者数据平台，推动未来业务发展。”</p>",
    "publish_time": "2023-06-27 17:36:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱2023——云原生领域",
    "url": "https://www.infoq.cn/article/L5yRxWB8R8LkNbzeJhcv",
    "summary": "<p></p><h3>研究背景</h3><p></p><p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。</p><p></p><p>所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。</p><p></p><p>此篇是《中国开源生态图谱系列研究》的第四篇，聚焦在云原生领域，通过统计目前的云原生开源项目，并进行分类，同时结合开源基金会、开源产业联盟等生态，完整构成中国云原生开源的生态图谱。</p><p></p><p>随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，分析和评价现有云原生开源项目，并从中选择优秀案例供广大开发者和开源社区研究。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/4a/52/4ac3f1fa1207465f1b561a48cd7e2252.jpg\" /></p><p></p><p>截至目前，InfoQ 研究中心已经发布了 4 个领域的开源生态图谱，并预计在未来推出涵盖操作系统、数据库、人工智能、云原生、前端和中间件等七大领域的中国开源生态图谱全景图，将涵盖 500+中国开源项目，敬请期待。</p><p></p><h3>目录</h3><p></p><p>生态图谱解读生态图谱企业洞察</p><p></p><p>扫码领取资料</p><p><img src=\"https://static001.infoq.cn/resource/image/0y/2f/0yye8366f7a75720a5fd690e2f23742f.png\" /></p><p></p>",
    "publish_time": "2023-06-27 17:38:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国开源生态图谱2023——人工智能领域",
    "url": "https://www.infoq.cn/article/10MUR5RwXAUUgFeZvQiB",
    "summary": "<p></p><h3>研究背景</h3><p></p><p>2022 年 8 月，InfoQ 研究中心推出《中国开源发展研究分析 2022》。报告中对中国开源的宏观发展的背景、目前取得的成绩、整体发展的特征进行了分析。同时也推出了基于 InfoQ 研究中心研究成果的 InfoQ 开源项目指数。但是因为时间等因素，《中国开源发展研究分析 2022》聚焦研究了中国 TOP30 开源项目。我们深知，中国开源发展百花齐放，仍有大量的项目深植于各技术领域中，并且取得了亮眼的成绩。另外，不同的技术领域开源也具有各自独特的特征。</p><p></p><p>所以，InfoQ 研究中心策划启动了《中国开源生态图谱系列研究》工作，技术领域涉及操作系统、数据库、云原生、大数据、前端、架构等。希望系列研究能够帮助关注中国开源世界的朋友绘制更为完整的开源全景图谱。通过对不同技术领域的研究分析，帮助读者获得更为具体的开源领域洞察。</p><p></p><p>此篇是《中国开源生态图谱系列研究》的第三篇，聚焦在人工智能领域，通过统计目前的人工智能开源项目，并进行分类，同时结合开源基金会、开源产业联盟等生态，完整构成中国人工智能开源的生态图谱。</p><p></p><p>随后，通过在《中国开源发展研究分析 2022》中使用的 InfoQ 开源项目指数，和拓展的 Gitee 指数，分析和评价现有人工智能开源项目，并从中选择优秀案例供广大开发者和开源社区研究。相关指数的详细计算方式可见报告完整内容。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/67/9c/67e15df5bcdeyy92d73fb16c11aa129c.png\" /></p><p></p><p>截至目前，InfoQ 研究中心已经发布了 3 个领域的开源生态图谱，并预计在未来推出涵盖操作系统、数据库、人工智能、云原生、前端和中间件等七大领域的中国开源生态图谱全景图，将涵盖 500+中国开源项目，敬请期待。</p><p></p><h3>目录</h3><p></p><p>生态图谱解读生态图谱企业洞察</p><p></p><p>扫码领取资料</p><p><img src=\"https://static001.infoq.cn/resource/image/7d/b1/7dc762b386412450bacb5c12a7fd56b1.png\" /></p><p></p>",
    "publish_time": "2023-06-27 17:51:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]