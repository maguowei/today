[
  {
    "title": "Magic Pocket：Dropbox的EB级Blob存储系统",
    "url": "https://www.infoq.cn/article/SWRM4CuXmNHCTLbh27bU",
    "summary": "<p>在旧金山QCon大会上，我介绍了Dropbox的EB级Blob存储系统是如何存储所有客户数据的。<a href=\"https://dropbox.tech/infrastructure/inside-the-magic-pocket\">Magic Pocket</a>\"的核心是一个非常大的键值存储，其中的值可以是任意大小的Blob。</p><p>&nbsp;</p><p>我们的系统提供了超过12个9的持久性和99.99%的可用性，在北美的三个地理区域运营。系统针对4BM Blob、不可变写入和冷数据做了优化。</p><p>&nbsp;</p><p>Magic Pocket每秒处理数千万个请求，其中很多流量来自验证器和后台迁移。目前，我们部署了60多万个存储驱动器，运行着数千台计算机。</p><p>&nbsp;</p><p></p><h2>对象存储设备</h2><p></p><p></p><p>Magic Pocket主要聚焦对象存储设备（OSD）。这些设备的容量超过2PB，每台存储机包含大约100块采用了<a href=\"https://en.wikipedia.org/wiki/Shingled_magnetic_recording\">叠瓦式记录</a>\"（SMR）技术的磁盘。</p><p>&nbsp;</p><p>与传统的<a href=\"https://en.wikipedia.org/wiki/Perpendicular_recording\">磁记录驱动器</a>\"相比，SMR的不同之处在于它执行顺序写入而不是随机写入，提升了密度。</p><p>&nbsp;</p><p>SMR的缺点是，磁头会在走过下一条磁道时擦除它，以防在任何地方随机写入。</p><p>&nbsp;</p><p>不过，这非常适合我们的工作负载模式。SMR驱动器也有一个传统区域，允许在必要时缓存随机写操作。通常，该区域占驱动器总容量的不到1%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a805bb3ef22405e52ffc66154d0a5df.png\" /></p><p></p><p>图1：SMR磁道布局</p><p>&nbsp;</p><p>宏观上，Magic Pocket的架构由三个区域组成：西海岸、中部和东海岸。Pocket是该系统的核心概念，它代表了系统中所有东西的逻辑版本。Magic Pocket可以有多个实例，如测试Pocket或生产Pocket之前的过渡Pocket。Pocket是独立运行的，彼此之间不共享数据库和计算资源。</p><p></p><h2>区域</h2><p></p><p></p><p>以下是每个区域中Magic Pocket架构的不同组件。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/ad3dfb62cd24989134b5626f4f4a4ecc.png\" /></p><p></p><p>图2：区域的工作机制</p><p>&nbsp;</p><p>第一个服务是前端，它是与客户端交互的服务。客户端通常会发起PUT请求（带有键和Blob）、GET请求、删除调用或扫描系统中可用的散列。</p><p>&nbsp;</p><p>GET请求会查询散列索引（分片MySQL数据库的集合）。散列索引按散列分片，散列是Blob的键，每个散列都映射到一个单元格（cell）或一个桶（bucket），并有一个校验和。单元格是所有存储设备所在的隔离单元：它们可以超过100PB，并且规模上有一定的增长空间。当容量不足时，系统将开辟一个新的单元格，实现水平扩展。</p><p>&nbsp;</p><p>跨区域复制器是执行跨区域复制的组件，可以将数据存储在多个区域。该操作是异步完成的，一旦主区域发生了提交，数据就会排队等待复制到另一个区域。控制平面管理流量协调、生成迁移计划并控制机器重装。它还管理着单元格状态信息。</p><p></p><h2>单元格</h2><p></p><p></p><p>如果想获取一个Blob，就需要访问了解桶和卷的桶服务：当请求一个桶时，请求会被映射到一个卷，而这个卷会被映射到一组OSD。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8e6cc3f5dcbc78b585e040e06aad3fd7.png\" /></p><p></p><p>图3：单元格的运作机制</p><p>&nbsp;</p><p>一旦找到拥有所需Blob的OSD，就可以检索它了。对于写入数据，前端服务会找出哪些桶是打开的，并提交给处于就绪状态的桶。桶是预先创建好的，数据存储在卷内的一组OSD中。</p><p>&nbsp;</p><p>协调器是单元格中的一个重要组件，管理所有桶、卷和存储机。协调器不断检查存储机的运行状况，与桶服务和数据库协调信息，并执行擦除编码和修复：它通过在单元格内移动数据实现数据优化，并负责在必要时将数据移动到其他机器。卷管理器处理卷的读、写、修复和擦除编码。验证步骤在计算单元格内外都会执行。</p><p></p><h2>桶、卷和区段</h2><p></p><p></p><p>现在，我们可以更深入地研究Magic Pocket存储系统的组件了，即桶、卷和区段。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/5422c1f4b4cd5571c66d57d10ef7b342.png\" /></p><p></p><p>图4：桶、卷和区段</p><p>&nbsp;</p><p>桶是与卷和区段相关联的逻辑存储单元，代表磁盘上1到2GB的数据。当写入时，我们找到打开的桶和与之关联的OSD，然后写入区段。协调器管理桶、卷和区段信息，并可以通过为已删除的区段找到新的位置确保数据不会丢失。一个卷由一个或多个桶组成，它要么被复制，要么被擦除编码，它的状态可以是打开或关闭。一旦一个卷关闭后，就再也不会打开了。</p><p></p><h2>如何在对象存储设备上查找Blob</h2><p></p><p></p><p>在这一部分中，我们将介绍如何在存储机中查找Blob。为此，我们将OSD的地址存储在Blob中，并直接与这些OSD通信。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0111f74e73077cbc2ec8c12a43daaf23.png\" /></p><p></p><p>图5：查找Blob</p><p>&nbsp;</p><p>磁盘驱动器（OSD）会加载所有区段信息，并创建一个内存索引，其中有磁盘偏移量的哈希值。如果想要获取块，就需要知道卷以及哪些OSD拥有该Blob。对于PUT，过程是一样的，但我们是并行地对每个OSD进行写操作，并且要在所有存储机上的写操作都完成后才会返回。由于卷被复制了4份，所以所有四个OSD中都有完整的副本。</p><p></p><h2>擦除编码</h2><p></p><p></p><p>虽然故障不可避免，但2个区域4个副本的复制成本很高。我们来看一下复制卷和擦除编码卷之间的区别，以及如何处理它。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/41/41d68262c82888aa56a5ad69ae514aa2.png\" /></p><p></p><p>图6：擦除编码</p><p>&nbsp;</p><p>擦除编码可以降低复制成本，并保持与复制相似的持久性。在我们的系统中，当一个卷快满了的时候，它就会关闭，就可以进行擦除编码。我们使用了一个擦除码，就像<a href=\"https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction\">Reed-Solomon误码校正编码</a>\"（6，3），在一个卷组中有6个OSD和3个奇偶校验位。这是说，在一个数据区段中只有一个Blob，如果一个OSD出现故障，则可以重建它。重建可以发生在对数据的实时请求中，也可以作为修复的一部分在后台完成。擦除码有许多变体，它们在开销方面有各自的考量，例如，使用XOR作为擦除码可能很简单，但自定义擦除码可能更合用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/74/74e9b15832c398b4375fba7a5a710d8a.png\" /></p><p></p><p>图7：故障和擦除编码</p><p>&nbsp;</p><p>关于这个主题，Huang等人的论文“<a href=\"https://www.usenix.org/system/files/conference/atc12/atc12-final181_0.pdf\">Windows Azure存储中的擦除编码</a>\"”是一份不错的资源，我们的系统就使用了类似的技术。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e1c8d8f1ecb9e6adacac4185e8e1b107.png\" /></p><p></p><p>图8：Huang等人的论文“<a href=\"https://www.usenix.org/system/files/conference/atc12/atc12-final181_0.pdf\">Windows Azure存储中的擦除编码</a>\"”中的Reed-Solomon误码校正编码</p><p>&nbsp;</p><p>我在前面的例子中提到过的Reed-Solomon纠错码（6,3），有6个数据区段和3个奇偶校验位。另一个选项为本地重建代码，它优化了读取成本。Reed-Solomon纠错码（6,3）在出现任何故障时都要读取6次。但是，使用本地重建代码，对于同一类型的数据故障，虽然读取成本相同，但是与Reed Solomon的1.5倍复制因子相比，存储开销大约降低了1.33倍。虽然这看起来差别不大，但当规模比较大时，却可以节省大量的资金。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b7ff0c48f6d0e3fce2e7ea87dfd81bb.png\" /></p><p></p><p>图9：Huang等人的论文“<a href=\"https://www.usenix.org/system/files/conference/atc12/atc12-final181_0.pdf\">Windows Azure存储中的擦除编码</a>\"”中的重建代码比较</p><p>&nbsp;</p><p>本地重建代码针对组中出现一个故障的情况做了优化，生产环境中经常会遇到这种情况。做出这种权衡是可以接受的，因为在一个卷组中出现2个以上故障的情况很少见。</p><p>&nbsp;</p><p>这些编码的复制因子甚至可以更低：编码LRC-(12,2,2)可以容忍组内有三个故障，但不能容忍四个，此时只有部分故障可以重建。</p><p></p><h2>冷存储系统</h2><p></p><p></p><p>我们的系统还能做得更好吗？正如我们观察到的那样，90%的检索是针对去年上传的数据，80%的检索发生在前100天内，我们正在探索改进跨区域复制的方法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ae/ae1ea9dc50946bef5324378bf03187ab.png\" /></p><p></p><p>图10：文件访问分布</p><p>&nbsp;</p><p>我们有大量不经常访问的冷数据，因此，我们希望优化工作负载，减少读取，并保持延迟、持久性和可用性基本不变。根据我们的观察，为了实现这一点，我们不必对冷存储进行实时写入，并且可以利用多个区域将复制因子降至2x以下。</p><p>&nbsp;</p><p>让我们看一下我们的冷存储系统是如何工作的，其灵感来自Facebook的<a href=\"https://research.facebook.com/publications/f4-facebooks-warm-blob-storage-system/\">暖Blob存储系统</a>\"。这篇有关f4的论文提出了一种方法，将一个Blob分成两半（分别存储在不同的区域），并对这两半做<a href=\"https://www.khanacademy.org/computing/computer-science/cryptography/ciphers/a/xor-bitwise-operation\">XOR</a>\"。要检索完整的Blob，Blob1和Blob2的任意组合或XOR就必须在任意两个区域中可用。但是，要执行写入操作，所有区域都需要完全可用。请注意，由于迁移是在后台异步进行的，所以它们不会影响实时进程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6b/6b172db0c26898b7a7df956da0e6f010.png\" /></p><p></p><p>图11：分割Blob和冷存储</p><p>&nbsp;</p><p>这种冷存储系统有什么好处呢？通过将复制因子从2x降低到1.5x，我们节省了25%的成本。存储在冷存储中的片段仍然会在内部进行擦除编码，并且迁移是在后台进行的。为了减少主干带宽的开销，我们将请求发送到两个最近的区域，并且只在必要的时候才从其他区域获取。这又节省了大量的带宽。</p><p></p><h2>发布周期</h2><p></p><p></p><p>Magic Pocket是如何发布的？从所有过渡环境到生产环境，我们的发布周期大约为四周。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdfbab14fa53d6eeeed0e48423051bd0.png\" /></p><p></p><p>图12：Magic Pocket的发布周期</p><p>&nbsp;</p><p>在提交更改之前，我们会运行一系列单元测试和集成测试，包含所有的依赖项，以及一个对所有数据进行全面验证的持久性阶段。每个区域在每个阶段都会进行大约一周的验证：我们的发布周期是完全自动化的，它会做适当的检查，如果有任何警告，它就会中止或不再继续更改代码。只有在特殊情况下，我们才会停止自动部署过程，并人为干预。</p><p></p><h2>验证</h2><p></p><p></p><p>那么验证呢？在系统内部，为了确保数据的准确性，我们做了大量的验证。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7cd4f894f4dac60cdc63c1910cbb4f6c.png\" /></p><p></p><p>图13：验证</p><p>&nbsp;</p><p>其中一项是由跨区域验证器执行的，它负责在上游客户端和系统之间同步数据映射。另一个是索引验证器，它会扫描索引表，确认每台存储机中是否存在特定的Blob：我们只是询问存储机已加载的区段询中是否有这个Blob，而不获取实际的内容。观察器是另一个会对Blob本身进行全面验证的组件，按分钟、小时、天和周进行采样。我们还有一个垃圾检查器，确保区段被删除后，其中的所有散列都会被删除。</p><p>&nbsp;</p><p></p><h2>运营</h2><p></p><p></p><p>因为Magic Pocket跨多个数据中心，所以我们要处理大量的迁移。我们管理着一个非常大的存储机机群，知道每时每刻都发生了什么非常重要。混乱时有发生，我们会通过大量的灾难恢复活动来测试系统的可靠性：升级这种规模的系统和系统本身一样困难。管理后台流量是我们的关键运营工作之一，因为那占用了我们大部分的流量和磁盘IOPS。磁盘洗涤器不间断地扫描所有流量并检查区段的校验和。我们根据业务将流量划分为不同的层，并根据网络情况对实时流量做优先级排序。</p><p>&nbsp;</p><p>控制平面会根据我们对数据中心迁移所做的预测来规划大量的后台流量：我们会考虑正在进行的迁移类型，如冷存储，并相应地制定计划。</p><p>&nbsp;</p><p>系统中有许多故障需要我们处理：我们每秒要修复4个区段，大小从1GB到2GB不等。我们有相当严格的SLA（少于48小时），因为那是我们持久性模型的组成部分，我们希望尽可能地缩短修复时间。OSD会根据单元格的大小和当前的利用率自动分配到系统中。</p><p>&nbsp;</p><p>我们也有很多向不同数据中心的迁移。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc51e158cc2da866c93c26da6c5ad94e.png\" /></p><p></p><p>图14：迁移</p><p>&nbsp;</p><p>两年前，我们迁出了SJC地区，为此我们做了大量的计划。对于非常大的迁移，比如数百个PB，幕后会有大量的准备工作，我们会给自己额外留出时间，以确保迁移能够及时完成。</p><p>&nbsp;</p><p></p><h2>预测</h2><p></p><p></p><p>在管理这种规模的存储系统时，预测是一个很关键的部分。我们一直在应对存储增长的挑战，有时可能是意料之外的，那就需要我们能够快速做出调整并将新的数据接收到我们的系统中。此外，我们可能会因为供应链中断（如COVID大流行造成的供应链中断）而面临容量问题：一旦发现潜在的问题，我们就会开始制定备份计划，因为订购并向数据中心交付新容量需要相当长的时间。我们的预测直接集成到了控制平面中，这有助于我们根据容量团队提供的信息执行迁移。</p><p>&nbsp;</p><p></p><h2>小结</h2><p></p><p></p><p>在维护Magic Pocket的过程中，我们总结出了以下4条关键的经验：</p><p>保护和验证规模比较大时要慢慢来保持简单做最坏的打算</p><p>&nbsp;</p><p>首先，我们要优先考虑系统的保护和验证。这需要大量的开销，但端到端验证对于确保一致性和可靠性至关重要。</p><p>&nbsp;</p><p>在这种规模下，要慢慢来，稳定很重要。我们要优先考虑持久性，部署任何新内容之前都要等待验证完成。我们总是会考虑风险，为最坏的情况做准备。</p><p>&nbsp;</p><p>简单性也是一个关键因素。我们的目标是保持简单，特别是在大规模迁移期间，因为过多的优化可能会导致复杂的心智模型，增加计划和调试的难度。</p><p>&nbsp;</p><p>此外，我们总会准备一个备份计划，以防在迁移或部署期间出现故障或问题。我们会确保变更不是单向的，必要时可以逆转。总体而言，管理这种规模的存储系统需要综合考量保护、验证、简单性和准备工作等诸多方面。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/dropbox-magic-pocket-exabyte-storage/\">https://www.infoq.com/articles/dropbox-magic-pocket-exabyte-storage/</a>\"</p>",
    "publish_time": "2023-07-26 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]