[
  {
    "title": "自定义跟踪架构：Slack高效解决通知问题",
    "url": "https://www.infoq.cn/article/LAYRE8jxE2z42QuU5pNX",
    "summary": "<p>Slack<a href=\"https://slack.engineering/tracing-notifications/\">利用其自定义的跟踪架构</a>\"来协助排查通知发送问题。该跟踪架构的帮助下，他们解决通知问题的速度提高了30%，而且减少了将问题升级给开发团队的次数。该架构还简化了分析管道，并为数据科学团队解锁了新的应用场景。</p><p>&nbsp;</p><p>消息通知是Slack用户体验的关键组成部分。然而，由于通知流横跨Slack平台的许多组件，包括服务器端和客户端，所以要对客户体验团队收到的问题进行排查，有时候并不容易。开发团队经常不得不花费好几天的时间，查看多个具有不同日志记录后端、不同日志记录格式的系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d5ff117824b8288e4c74efab63091ecb.jpeg\" /></p><p></p><p>图片来源：<a href=\"https://slack.engineering/tracing-notifications/\">https://slack.engineering/tracing-notifications/</a>\"</p><p>&nbsp;</p><p>之前，Slack创建了<a href=\"https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/\">一个自定义的SlackTrace跟踪架构</a>\"，并使用它来跟踪日常的消息传递。他们用它跟踪了1%的客户端请求。接下来，该公司决定构建自己的跟踪解决方案，因为他们发现，没有一个现成的第三方解决方案能完全满足他们的需求。</p><p>&nbsp;</p><p>为了跟踪消息通知，团队识别出值得注意的事件并确定了属性映射，从而实现流和跟踪的映射。他们决定将通知跟踪与消息请求跟踪分开。这样，他们就可以支持通知流的100%采样，从而满足Slack客户体验团队的要求。</p><p>&nbsp;</p><p>通知跟踪改进了问题归类和调试。客户体验团队的成员自己就可以使用跟踪数据来了解出错的位置，不需要求助开发团队就可以解答客户的疑问。这个新功能也为iOS和Android工程师开始使用<a href=\"https://grafana.com/\">Grafana</a>\"来监控移动应用程序中的通知发送提供了帮助。最后，数据科学团队从跟踪数据中获得了洞察。他们通过漏斗分析来加深对通知打开率的理解，并利用历史通知跟踪数据来识别应用程序中的Bug和工具代码。</p><p>&nbsp;</p><p>Slack高级软件工程师<a href=\"https://www.linkedin.com/in/mansu/\">Suman Karumuri</a>\"将跟踪的好处总结如下：</p><p></p><blockquote>将产品分析数据建模为跟踪，可以在整个复杂的技术栈中以一致的数据格式提供高质量的数据。此外，内置的跟踪数据会话化免除了额外对跟踪数据进行去重和会话化的任务，简化了分析管道。</blockquote><p></p><p>&nbsp;</p><p>SlackTrace架构由一个Go Web服务器应用程序和一个Go消费者服务组成，前者负责向<a href=\"https://kafka.apache.org/\">Apache Kafka</a>\"发布跟踪span事件，后者负责将事件持久化到实时存储（<a href=\"https://www.elastic.co/\">ElasticSearch</a>\"）和数据仓库中。后端服务使用<a href=\"https://zipkin.io/\">Zipkin</a>\"和<a href=\"https://www.jaegertracing.io/\">Jaeger</a>\"工具库来报告span事件，并转换为内部span表示，而桌面和移动应用程序可以直接使用span API。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3c/3c0402a8f4fc0f647314593a7f6b32ef.jpeg\" /></p><p></p><p>图片来源：<a href=\"https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/\">https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/</a>\"</p><p>&nbsp;</p><p>Slack选用了一种比较简单的span表示，这使得他们的解决方案更加灵活，不用紧紧围绕请求和网络跟踪来开展。Span的结构简单，数据可以存储在单个表中，并且支持多种查询选项，工程师可以从中提取他们需要的数据来回答特定的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/slack-notification-tracing/\">https://www.infoq.com/news/2023/06/slack-notification-tracing/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/9lp0pYfij3bUxmHDXkxl\">Slack实时消息处理架构，更新、更快、更稳定</a>\"</p><p><a href=\"https://www.infoq.cn/article/hhh8OGLNbsz121H43Df4\">Slack工程师如何解决最常见的移动开发痛点</a>\"</p>",
    "publish_time": "2023-08-07 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "【实战篇】通过 SoFlu 构建一个低代码平台之简介",
    "url": "https://www.infoq.cn/article/FODhkbu3LbXZouV6f419",
    "summary": "<p>在现代企业开发中，初级开发者难以掌握开发企业级 Java 应用，中高级开发者则被繁琐任务所占据。为了让大家能快速上手可以解决这个难题的“ SoFlu 软件机器人”，前蚂蚁金服技术专家杨彪设计了《2 小时学会 10 个组件“取代”传统 Java 开发》课程，本节为该系列课程的第八讲。在经历了前 7 讲的干货输入，杨彪自本讲起为大家开始进行“ SoFlu 软件机器人”的实战展示——搭建一个低代码平台！</p>\n<p>大家都在说低代码平台，可是代码平台究竟有哪些核心能力呢？为什么 SoFlu 可以开发低代码平台？本讲便简单探讨业界低代码平台通常都有哪些核心功能，主要介绍使用 SoFlu 开发的低代码平台大概是什么样子，主要核心功能有哪些？</p>\n<p>大家在课后可以登录 Gitee 下载 SoFlu 软件机器人客户端进行实践：<a href=\"http://suo.im/8wROo\">点击下载</a></p>\n<p>大家可以扫码添加小助手，进学习群与专家一对一交流：<br />\n<img alt=\"\" src=\"https://static001.infoq.cn/resource/image/e8/c9/e8833a01ba0bc705acab14a572b57cc9.png\" /></p>",
    "publish_time": "2023-08-07 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全网首档操作系统探访体验栏目“龙蜥+超级探访”震撼上线！看国产 OS 如何乘风破浪",
    "url": "https://www.infoq.cn/article/hEEwclx5wCFgNHTfQO0o",
    "summary": "<p>如何联合开源技术与国际社区形成双向互动，消除 CentOS 策略变更带来的影响？“全网首档操作系统探访体验节目”《龙蜥+超级探访》重磅上线！统信软件既是实现开源和商业互补协作的成功典范，又是身先士卒引领伙伴投身开源的“大队长”，也是龙蜥+超级探访首期走进的企业。且看龙蜥社区联合行业内生态伙伴何实现从技术创新到商业变现的跨越</p>",
    "publish_time": "2023-08-07 10:04:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "传统风控难以为继，智能风控如何落地｜InfoQ 闭门会精选",
    "url": "https://www.infoq.cn/article/tXdg1xI1YWGYG6iGg4rj",
    "summary": "<p>金融行业的业务本质是通过经营“风险”实现盈利和发展，换言之，风险管理能力即是金融业的核心竞争力之一。</p><p></p><p>当然，风险管控的方式，会随着时代的发展不断迭代。如今，人工智能、大数据等技术趋于成熟，智能风控正在逐渐替代传统风控，成为金融机构识别风险、平衡创新与风控、提升风险管理能力的主要抓手。</p><p></p><p>那么，在新的时代背景下金融机构风险管控面临哪些具体挑战？智能风控在落地金融场景的过程中有哪些问题仍待解决？AIGC 技术在金融风控领域又有什么价值潜力？在日前 ArchSummit 全球架构师峰会·深圳站的“金融数字化高端闭门会”上，来自银行、证券、支付、跨境平台等金融行业不同领域的专家，围绕这一系列问题展开了深度探讨。</p><p></p><p>以下内容经 InfoQ 编辑、提炼和总结，希望为大家提供参考。</p><p></p><h2>金融风控面临哪些挑战？</h2><p></p><p></p><p>金融机构在经营过程中面临着一系列不同的风险类型，比如信用风险、市场风险、操作风险、欺诈风险、道德风险、声誉风险、法律风险、战略风险等等。一旦金融机构在交易活动中出现风险失控，轻则带来财务损失，重则威胁机构生存，甚至导致整个金融体系的系统性风险，引发次生危机。</p><p></p><p>其中，信用风险管理是金融机构风险管理最重要的一部分，在金融发展史上，大部分银行倒闭案例都是信用风险引起的。比如，银行卡“二三四要素”的验证，就是防范信用风险的手段之一。</p><p></p><p>近年来，外部经济和金融环境日益复杂多变，加上金融机构自身业务范围的延展，金融业面临的信用风险管控的挑战不断增多。比如，在互联网金融场景下，要求金融机构具备更强的防假反欺诈能力，想要获得更优质的客户源，获客成本变高；再比如，随着场景消费金融等新业态兴起，金融服务愈发无处不在，业务运行机制日益复杂，这意味着，风险也将日趋碎片化，管控难度和复杂度大大提升。</p><p></p><p>此外，市场风险也是影响金融业健康发展的关键。以年初轰动一时的美国硅谷银行倒闭事件为例，其就是市场风险管理失控的结果。作为与科技公司深度绑定的金融服务机构，硅谷银行的总资产规模超过了 2000 亿美元，但是，其中超过 50% 的资金被用于购买国债和房地产抵押贷款证券（MBS），流动性较差。随着 2022 年初美联储大幅大幅加息，一边债券和 MBS 等资产出现贬值，另一边银行需要支付的利息增加，获取资金等成本提升——于是，资产流动性风险暴露无遗，硅谷银行抛售资产引发挤兑，最终破产收场。</p><p></p><p>可见，让市场始终保持理智是一个难以完成的任务，这使得金融市场风险的预防和预测极为困难。而与此同时，在竞争日益激烈的市场环境下，金融机构还必须持续进行业务创新，并且确保客户体验——这与风险管控往往存在矛盾冲突——比如，为了确保信用风险可控，可能就要“牺牲”一部分客户体验；而对业务的管控过严，收益在一定程度上就会受到影响。</p><p></p><p>以跨境支付为例，核心要解决两大难题，一是拒付率过高，二是结算周期过长。为此，跨境支付机构就必须对商家资质等信息进行更为严格的审查。比如，国际卡组对跨境支付机构往往会设有严格的拒付率标准要求，如果拒付率高于一定水平，跨境支付机构就要承担高额罚款。为了降低拒付率，支付机构必须减少由于商家本身的产品问题造成的拒付风险。在这个过程中，如果卡口太严，就可能造成客户流失，如果卡口太松，就可能带来拒付率的提升。</p><p></p><p>因此，如何找到其中的“平衡点”，确保风险管控场景和标准的合理性，也是金融机构在风险管控过程中的一大痛点。</p><p></p><h2>智能风控解决了什么问题？</h2><p></p><p></p><p>面对这些挑战和难点，传统的风控手段变得难以为继。</p><p></p><p>最早，金融机构的风控管理主要依赖于人工，比如由人撰写规则，通过表格手工记录统计，由人工审批信用、资质等信息。以借贷场景为例，从调查、审查、审批，到贷款发放，贷中风险评估，贷后维护、管理以及贷款的回收等工作都像是一条人工作业流水线。</p><p></p><p>虽然经过信息化发展，大多数金融机构已经引入系统实现自动化审批，从而提高风控效率。但是传统风控更多考虑的仍然是信息间的强关联性，比如信用卡风险管理和评分主要关联卡号、IP 地址、征信等强变量，对客户的行为数据、消费数据、社交数据等弱变量的关联度较少。</p><p></p><p>如前所述，金融机构面临的风险点越来越多且分散，规则的制定越来越复杂，需要纳入评审分析的信息维度越来越多。在这一趋势下，普通信息系统的处理能力也开始显得捉襟见肘。</p><p></p><p>魔高一尺，道亦需高一丈。金融业从传统风控迈向大数据风控，而今，智能风控又应运而生，其背后，是技术的进化和演进。</p><p></p><p>智能风控以大数据平台为基础，以决策引擎为媒介，以智能模型为大脑，它能帮助金融机构做的，不再只是 0 或 1 的风险判断，还能进一步分析每笔交易申请的具体风险水平。这意味着，金融机构可以据此做更精细化的风险管理和定价，不必一棍子把一批客户拒之门外，有益于在业务拓展与风险管控之间寻找到那个平衡点，同时也能优化客户体验。</p><p></p><p>如果还以跨境支付为例，这意味着，支付机构可以在已有的数据基础上，更高效地获取更多的数据维度，比如更充足的渠道数据、商家数据等，从而对客户进行更精准的识别。</p><p></p><p>再比如，在向企业提供融资的场景下，金融机构可以利用大数据、AI 等技术自动实时抓取企业的财务、舆情、招聘、海关甚至是行业现状等公开信息，并交由 AI 辅助决策，判断是否进行风险预警。随着数据量大积累，还可以基于机器学习将各项预警信号与事后违约结果进行关联度分析，在复杂的信号中找出与该行业违约事件强相关的指标，从而不断优化信贷评估模型，提高审批的决策精度。</p><p></p><p>在这些场景下，技术所带来的，已经不仅仅是效率的提升，更重要的是价值的创造。</p><p></p><h2>智能风控落地顺利吗？</h2><p></p><p></p><p>不过，智能风控千好万好，在具体落地过程中，仍然存在不少的挑战和阻碍。</p><p></p><p>首先，大多数传统金融机构是有很多历史包袱的。</p><p></p><p>一方面是技术包袱。传统金融机构信息化系统间孤岛严重，数据难以打通共享，同时，大量历史系统和复杂冗余的技术架构很难在短时间内被替换和重塑——显然，这样的技术基础在支持海量数据的处理、分析和智能计算等方面的能力已经不足。因此，智能风控要彻底落地，必须还要“让子弹飞一会儿”，给技术升级一点时间；</p><p></p><p>另一方面是规则包袱。随着金融业务的快速发展，风险管控的规则一直在做加法，但是背后的依据和场景很难一一溯源，这导致系统不可控因素越来越多，可能影响业务快速发展。为了解决这个问题，需要对规则引擎进行升级，提高新的规则运行的可视化，同时加强日常管理。</p><p></p><p>其次，技术升级带来了另一个挑战，是人员技术能力的提升。</p><p></p><p>比如，传统金融机构技术人员掌握的能力主要面向的是集中式架构的开发、运维、管理，但现在，随着技术架构呈现开放化、分布式趋势，金融机构底层的数据库、中间件，包括开发工具、设计方法都要随之做出改变，而这，就对技术人员的能力象限提出了迭代的要求。于企业而言，既要对内部人员持续进行培训，同时也要具有从外部引入新的高端人才的计划、机制和能力。</p><p></p><p>再者，风控部门对智能技术的认可程度、使用意愿、资源投入度等等，也会直接影响智能风控模式的推行进度。</p><p></p><p>与所有数字化转型场景一样，智能风控的顺利落地绕不开相关业务人员。只有一线业务与技术人员在认知上达成一致，扫除自身的顾虑，并且迈出传统工作模式的“舒适区”，才能真正让技术工具为业务所用。否则，靠技术人员的“一厢情愿”注定阻力重重。</p><p></p><p>最后，还有一点不可忽略——技术和模型的适用性。</p><p></p><p>如何把机器学习中常用的模型应用到金融场景中，对于金融从业者的技能，以及企业自身的数据处理能力都提出了非常高的要求。一般来说，通用模型是相对抽象的，由于数据采样和训练不足，在面向具体行业或场景时（如个人信贷、企业融资、跨境支付或供应链金融）应用性也会出现不足。因此，在智能风控落地过程中，金融机构究竟需要什么样的模型，如何对它进行训练，要往什么方向持续迭代演进，参数如何调整等等...... 相关人员必须对此心中有数。</p><p></p><p>唯有以上障碍一一化解，智能风控的价值才能达到最大化。</p><p></p><h2>彩蛋：AIGC 在金融风控领域有什么可能性？</h2><p></p><p></p><p>一直以来，金融行业都是前沿技术应用的先行者，面对 AIGC 这波热浪，显然也不会例外。在闭门会上，大家对于 AIGC 在金融风控领域的可能性同样进行了一番探讨，以下是部分“脑暴碎片”，供参考：</p><p></p><p>利用 AIGC 帮助风控人员编写风控规则，简化流程，提高效率；通过 AIGC 对相关政策和监管文档进行解读，帮助风控人员更好、更快地理解；利用 AIGC 对客户资料（如拒付申诉资料）进行整理归档，提高效率；通过 AIGC 对原有的大数据 / 智能风控模型进行迭代升级，提升整体智能化水平。</p><p></p><p>总而言之，金融机构的风险管控讲求“看人、看表、看物”，其中的每一项工作，或许都值得用 AIGC 重做一遍。当然，在这个过程中，势必还会产生诸多衍生问题，比如“AI 幻觉”的问题如何解决，如何让 AI 分析过程更具有可解释性等等——对于金融行业而言，AIGC 的商业落地进程正在加速，但是鉴于严苛的安全合规要求，每一个细微的问题也都值得反复推敲和推演。&nbsp;</p>",
    "publish_time": "2023-08-07 11:16:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "汇丰科技：基于供应链金融贸易数据的信用风险实践解读",
    "url": "https://www.infoq.cn/article/5374J5rJxIgpXfbIL6Tx",
    "summary": "<p>综合营运资金融资方案为核心买家的供应商提供从装运前到装运后的全流程融资方案，令其可凭借经由核心买家确认的订单，向汇丰申请融资。与传统的商业贷款不同，该装运前融资依托供应链出运后融资，将融资节点提前至出运前即可起始。装运前融资计划与装运后供应链解决方案在系统和操作流程层面无缝联接，到期自动还款，极大地简化了整个流程。汇丰通过买卖双方的历史贸易数据将信贷决策数字化，并将对财务的依赖减少到几个关键检查点，该信用决策模型使供应商参与到信贷审批的准备时间从 1 个月以上大幅缩短至 2 周。本次分享便将为大家详细剖析整个技术实践过程。</p>\n<p><strong>演讲提纲：</strong></p>\n<ul>\n<li>应用业务场景- 综合营运资本融资方案</li>\n<li>基于历史供应链交易数据的信用决策模型-记分牌（含 AI 相关技术分享）</li>\n<li>信用决策架构</li>\n</ul>\n<p><strong>你将获得：</strong></p>\n<ul>\n<li>了解汇丰针对中小微供应商的贷款业务实施</li>\n<li>了解数字化授信决策流程</li>\n<li>了解基于贸易数据的信用决策模型</li>\n</ul>",
    "publish_time": "2023-08-07 11:23:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "汇丰科技：汇丰数据科学家平台的探索与实践",
    "url": "https://www.infoq.cn/article/mHCbYBSscyQ2Qs2KTEEj",
    "summary": "<p>作为全球规模最大的银行和金融服务机构之一，汇丰始终致力于实施国际化策略，为客户提供国际化的机遇和视野。这一策略推动了汇丰银行业务的持续发展，但同时也带来了合规方面的巨大挑战。</p>\n<p>在当前大数据、生成式 AI 以及混合云技术快速发展的背景下，作为数据平台部门的工程师，我们应如何在确保合规要求得到满足的前提下，让数据和人工智能更便捷、安全地被使用，以进一步支持汇丰银行的国际化策略呢？本议题将为大家进行分享。</p>\n<p><strong>演讲提纲：</strong></p>\n<p>背景</p>\n<ul>\n<li>混合云与数据监管</li>\n<li>生成式 AI 与模型合规</li>\n</ul>\n<p>属于汇丰的数据科学家工作平台</p>\n<ul>\n<li>汇丰数据科学家工作平台简介</li>\n<li>统一的数据访问监管流程</li>\n<li>基于 Kubernetes 的混合云全球部署架构</li>\n<li>基于数据虚拟化的多平台跨区域的数据访问</li>\n</ul>\n<p>数据科学家平台与生成式 AI 平台的集成</p>\n<ul>\n<li>数据科学家平台与生成式 AI 平台集成的设计与实践</li>\n<li>生成式AI的监管模型</li>\n</ul>\n<p><strong>你将获得：</strong></p>\n<ul>\n<li>参考与借鉴数据科学家平台在混合云环境下的设计与实践</li>\n<li>了解如何应用数据虚拟化解决跨区域多平台的实时数据访问</li>\n<li>了解金融机构对于生成式 AI 的思考及使用方式</li>\n</ul>",
    "publish_time": "2023-08-07 11:24:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "BERT能否被“平替”？ — 作业帮文本分类场景下的一次尝试",
    "url": "https://www.infoq.cn/article/iGXyEimwtMeKkC7z4Ggu",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/d6/d658cae590fdf021a0be37e84723e002.png\" /></p><p></p><p></p><h2>背景介绍</h2><p></p><p></p><p>近年来，在众多自然语言处理模型中最具有代表性的就是BERT，它以优异的性能赢得了广大算法工程师的青睐。但是，在有些生产环境中，BERT庞大的参数量不仅在推理时占用过多的计算资源，也不利于后续模型的扩展迭代。作业帮的业务体量较大，每天会生产大量的文本数据。这些数据均需要经过自然语言处理模型来生成业务可以直接使用的文本分类标签。在实际生产阶段，我们的场景具有如下特点：1.标签分了多期进行建设和产出，每期的标签在不同的场景有不同的阈值；2.每个时期的标签之间既存在独立性又存在依赖性；3.每个时期的有监督数据较少，一般的机器学习模型很难取得较好的线上效果，因此每一期的标签都是基于BERT进行fine-tune和部署。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3db2519487ee21ba3aac44b0350c3aed.jpeg\" /></p><p></p><p>目前我们的模型训练及部署流程如上图所示。结合作业帮的实际业务场景特点和使用方式，我们面临如下问题：&nbsp;&nbsp;&nbsp;1)每当新的任务需求提出后，都需要对BERT进行微调来满足。2)随着任务数量的增加，服务器上部署的BERT数量也会不断上升，导致占用较多的GPU计算资源，而且任务之间的调度也会变得更加复杂。因此，本文将以上述场景作为对象，探讨在研究平替BERT过程中的发现和结果，并对比它们的各项性能指标。最终目标是找到一个和BERT推理效果基本持平，但占用更少计算资源（特别是减少GPU计算资源），同时具有优秀扩展性的解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed2d5de8cd8f2ed6b3cf563de42f7093.png\" /></p><p></p><p></p><h2>可行的替换方案</h2><p></p><p></p><p></p><h4>2.1方案选择</h4><p></p><p></p><p>为了减少计算资源，将BERT进行轻量化处理是一个不错的选择。目前主要的轻量化方法如下：知识蒸馏：知识蒸馏是一种教师-学生（Teacher-Student）训练结构，通常是使用训练好的教师模型提供知识，学生模型通过蒸馏训练来获取教师模型的知识。它能够以轻微的性能损失为代价，将复杂教师模型中蕴含的知识迁移到相对简单的学生模型中。剪枝：在不改变模型结构的情况下，去掉模型中作用较小的连接，从而减小模型的维度。量化：将高精度的参数类型转换为精度较低的参数类型，从而减小模型的尺寸和计算时间消耗。我们希望新模型不仅可以尽量接近BERT的推理效果，而且还能够显著减少推理时计算资源的消耗。因此，新模型需要具有以下特征：1.模型参数量少。这样有利于进行快速迭代，且有较快的推理速度。2.在文本分类任务上表现良好。考虑到生产环境中积累了大量的基于BERT的标签数据以及平替模型的特点，我们选用的方案是：结合知识蒸馏中Teacher-Student方式和主动学习的思路，使用BERT对TextCNN的推理结果进行筛选，并根据TextCNN的输出loss和概率分布，通过数据增强的方式迭代TextCNN的训练，从而使TextCNN的推理效果逼近BERT。流程如下图所示：</p><p></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/9609056f1cad059965638952957b822d.jpeg\" /></p><p></p><p></p><h2>2.2&nbsp;训练数据</h2><p></p><p></p><p>作业帮的实际业务数据具有以下特征：a.在时间上呈现周期性；b.某些数据具有相同的表达范式；c.正负例样本之间存在较多的竞争性表述，如“表扬”相关的正例表述应该为“求表扬”，如“为什么我不在表扬榜上？”。而对于“谁在表扬榜”的表述则应该为负例；d.数据中的表情符号繁杂，具有某些特殊表情的数据可以代表某一类别；e.由于ASR或者OCR的识别精准度，会导致数据的质量较低。基于上面提到的特征，本文对数据进行了如下处理方式：1)拉取长时间范围内的数据，让时间范围尽量包括多个数据周期。2)为了保证训练数据的覆盖完整性，我们使用了如下采样方式：l多样性抽样：根据标签的类别，使用BERT的[CLS]符号对不同的标签数据进行聚类，得到每个标签下不同表述的种子数据，再使用种子数据进行多样性抽样。l随机抽样：对预处理后的语料进行随机抽样，保证训练数据的分布不会发生大的偏移。l不确定性抽样：根据BERT预测的结果，筛选出模型决策边界附近的数据。对于这部分数据，如果在时间和人力成本允许的情况下，可以进行人工标注，否则直接将这部分数据直接舍弃。3)去重语句中多余的符号：对表情符号做分类统计，筛选出一些特殊的范式作为语料的前处理，对无用语料进行过滤（如在我们的实际生产环境中，可以直接将满足这些范式的句子标记成负例）。4)训练集的划分：训练集和测试集的划分最好在时间上有一定的隔离，比如测试集选用时间范围距离较近的数据，训练集则选用时间范围较远的数据，这样既可以验证数据对周期性，也可以测试模型的泛化能力。</p><p></p><p></p><h2>2.3&nbsp;训练方式</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f4f9dbb8d472b7653704235554c58ce0.jpeg\" /></p><p></p><p>模型迭代训练的完整流程如上图所示：首先，训练基础的TextCNN。然后，根据决策指标评估TextCNN模型。如果模型满足指标需求，则在替换BERT前我们还需要采样近七天的数据测试TextCNN的泛化能力。若不满足指标需求，则需要分析TextCNN输出的loss以及标签得分，确定下一轮训练数据的采样策略来有目的性的富集训练数据。最后，迭代上述流程，迭代的轮次可以根据预设的指标设置。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/095370c2198233411349a37750c98818.png\" /></p><p></p><p></p><h2>效果评估</h2><p></p><p></p><h4>3.1评估指标</h4><p></p><p></p><p>评价指标主要基于测试集数据，以BERT推理结果为金标准，对比TextCNN和BERT之间的差距，具体指标如下：</p><p></p><p>a.精准率：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/adfd5c47d1df2421da2ab30105d757e9.png\" /></p><p></p><p>b.召回率：&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a99837790c3d4746c233b6eb30500ca6.png\" /></p><p></p><p>c.F1-score：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee7521cd6146a89a3d2aee6d9ed832dc.png\" /></p><p></p><p>d.推理速度：</p><p></p><p>主要评估TextCNN在CPU环境下的运行速度。其中，对于Preicison、Recall的指标，均以BERT推理结果为基准计算。3.2效果评估根据训练流程，我们可以观察某类标签在迭代三轮情况下，其指标的相关变化，如下表所示：</p><p></p><p></p><p></p><p>首先，在第一轮迭代时，使用和训练BERT一致的语料训练TextCNN，其相关指标都比较低。我们对loss排序后筛选出来低分loss发现TextCNN容易陷入到某些关键词的表述中。如我们需要识别“求表扬”的数据，在结果数据中我们找到两个比较典型的例子：Ø“谁在表扬单上呢”&nbsp;vs.&nbsp;“表扬单上没有我呢”对于两个句子，我们真正要筛选出后者的表达方式。那么，前者是典型的竞争性负例。在实验过程中产生的更多句子中，我们发现TextCNN由于竞争性负例，出现了严重的过拟合现象。进一步地，本着将TextCNN逼近BERT的目标，我们对抽样好的数据分别使用训练好的TextCNN和BERT进行打分，然后对比两个模型的得分结果，并进行如下操作：1.对于一个样本，若BERT分数高于某阈值而TextCNN分数低于某阈值，那么就认为该样本为竞争性负例；2.将在BERT决策边界附近的样本直接舍弃（在条件允许的情况下，这部分数据可以在进行人工重新标注后再放回训练集中）在经过上述步骤之后，我们进行第二轮迭代。从指标结果上看，该步骤可以有效的提升TextCNN的精确率。由于模型的召回率还比较低。比如对于“求表扬”标签，我们对BERT的预测该类别[CLS]向量进行简单的聚类分析发现，这类标签的表述有：Ø“为什么我没上榜”Ø“为什么我不在表扬榜”Ø“为什么我没有小红花”Ø“怎么不夸我呢”Ø“表扬时，怎么没有念到我的名字”同样的，对比TextCNN的聚类结果，我们发现召回的数据大部分集中在某一两类的表述上。基于此，我们进行了第三轮迭代：使用[CLS]向量对训练数据进行了召回，发现确实存在表述不平衡的情况。因此，针对低召回的问题，我们使用多样性抽样的方式对不同的标签的进行了数据增强。实验结果表明可以有效的提升模型的召回率。</p><p></p><h4>3.3&nbsp;总结</h4><p></p><p></p><p>总体流程可以分为训练TextCNN基础模型、提升TextCNN的精确率以及提升TextCNN的召回率三个主要部分。在每个流程部分中根据loss和模型输出的概率分布，针对不同问题采样不同的数据。在满足指标设定的前提下，让TextCNN逼近BERT的推理结果。在迭代过程中，我们舍弃了BERT决策边界附近的样本。从实际应用的角度考虑，这些数据基本也不会漏到下游的应用中，因此舍弃这部分样本也是合理的。在生产环境中还需要考虑一个问题是TextCNN输出概率阈值的设定。针对这个问题，我们拉取了不同日期的历史数据，在预设指标的情况下，动态的搜索了合理的阈值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4b947f3f89c67a9997ae72aa43796889.png\" /></p><p></p><p></p><h2>模型部署</h2><p></p><p></p><p>由于生产环境中涉及的模型较多，因此需要考虑模型同时推理时带来的计算资源的负载均衡问题。我们使用了任务队列的方式部署模型：首先，根据不同模型的优先级配置不同任务的先后顺序；然后，对不同任务数量的运行进行压力测试，设置合理的任务并发数量。具体部署如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d7854ae9527c20b8d2b32339ab28cd1.jpeg\" /></p><p></p><p>TextCNN部署上线后，同一分类任务在不同的配置下，推理速度(服务平均吞吐能力)对比如下表所示。</p><p></p><p></p><p></p><p>（备注：本方案因为是离线处理模式，因此忽略单条数据处理时延指标，仅考虑整体吞吐能力）我们基于400k~800k的数据量，对比了如下几种部署方式的吞吐能力：lBERT@GPU：GPU(2080&nbsp;Ti)单卡lBERT@CPU：1核&nbsp;CPUlTextCNN@CPU：1核、5核、20核从表中对比数据可以看出，TextCNN@CPU的吞吐能力很强大。5核CPU配置就可以接近4.8倍的BERT@GPU（单卡）吞吐能力。而且在实际场景中，CPU资源往往更廉价更容易扩容，可以轻松大幅缩短整体离线任务的处理时间。我们一个实际的线上任务，之前是BERT@2GPU，需要耗时2个小时完成，平替为TextCNN@20CPU方案后，只需要26分钟左右即可完成。大幅提前了数据产出时间，为下游应用争取到了更多的时间窗口。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c80f410816a24fb7ec8e588adc644587.png\" /></p><p></p><p></p><h2>技术总结展望</h2><p></p><p></p><p>本文涉及的场景具有以下三个主要的特征：1.有大量的历史数据积累，有利于采用不同的采样策略来针对性的增强训练数据；2.部署的BERT模型数量多，不利于后续标签的更新和维护。同时占用的GPU资源较多，不利于算力成本的维护；3.允许平替模型在指标上有较低的损失。因此，从场景特征出发，本文借鉴蒸馏中Teacher-Student的思想以及主动学习的方法，探索了TextCNN替换BERT的效果。经过几轮迭代后，平替版的TextCNN可以满足业务需求，释放了宝贵的GPU资源，处理时间大幅缩短，而且比较利于后续进一步扩展更多的文本分类任务。</p>",
    "publish_time": "2023-08-07 14:34:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "主动学习技术在作业帮业务场景中的创新与实践",
    "url": "https://www.infoq.cn/article/UI6Kq8ti4XgW9blbAjJp",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/d6/d658cae590fdf021a0be37e84723e002.png\" /></p><p></p><p></p><h2>业务背景</h2><p></p><p></p><p>虽然目前基于&nbsp;Transformers&nbsp;架构的自然语言处理模型在各类工业场景落地过程中都表现出了优异的成绩，但其背后所需要的标注数据依然是日常工作中的重点，也是费时费力的瓶颈所在。怎样在我们数据标注过程中降低成本，便成了我们必须面对的问题。主动学习(Active&nbsp;Learning,&nbsp;AL)&nbsp;便是我们尝试提高标注效率，降低整体标注和训练成本的重要技术手段。</p><p></p><p>主动学习，一种机器学习方法，是通过一定抽样策略找出对现有模型最有价值的样本数据，经人工标注后加入训练集，再次训练模型并继续以上步骤迭代的流程。简而言之，通过某种挑选方式，减少整体所需的标注数据量，尽快地接近业务需求的标准。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed2d5de8cd8f2ed6b3cf563de42f7093.png\" /></p><p></p><p></p><h2>解决方案</h2><p></p><p></p><p></p><h2>2.1&nbsp;常见考量维度：不确定性和多样性</h2><p></p><p></p><p>考虑到我们的业务场景，我们这里主要介绍pool-based的主动学习方法。在该场景中，我们在一个现有的数据池中通过挑选策略挑选新一批的无标签样本，交给标注人员(oracle)进行标注。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90a493a3b4e6d451ab42a852250ecd5a.png\" /></p><p></p><p>From&nbsp;(Active&nbsp;Learning&nbsp;Literature&nbsp;Survey[5])</p><p></p><p>主动学习的核心便是挑选策略(查询策略)，在&nbsp;pool-based&nbsp;下常见的两个策略是不确定性采样和多样性采样。</p><p></p><p>不确定性采样--What&nbsp;Model&nbsp;knows&nbsp;it&nbsp;doesn't&nbsp;know:</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d006fb16cfeac0551bb57adcd8c01ee.png\" /></p><p></p><p>From&nbsp;(Active&nbsp;Learning&nbsp;Literature&nbsp;Survey[5])</p><p></p><p>一般地，把模型输出概率作为不确定性的衡量依据。有以下三种常见方式:least&nbsp;confident、smallest-margin、entropy。</p><p></p><p>多样性采样&nbsp;--What&nbsp;model&nbsp;doesn't&nbsp;know&nbsp;that&nbsp;it&nbsp;doesn't&nbsp;know&nbsp;or&nbsp;the&nbsp;\"unknown&nbsp;unknows\":</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/9302f9dddd0e58ca05698fd4ef336194.png\" /></p><p></p><p>From&nbsp;Human-in-the-Loop&nbsp;Machine&nbsp;Learning[4]</p><p></p><p>如上图所示，如果使用随机抽样，数据点大概率是从中间最大的聚类簇中获取，通过先聚类，后在聚类簇中抽样，能更大可能性地保证采样数据点的多样性。</p><p></p><p>通常是抽取尽量与整体分布相似的子集，比如采用聚类簇的方式挑选最具代表性的数据子集。</p><p></p><h2>2.2&nbsp;既要又要，可否？</h2><p></p><p></p><p>不确定性和多样性，代表了一个机器学习模型当前的两个重要盲区。而经典的主动学习数据抽取策略，如前文所述的方法，往往只能顾及到一端。那么，这个问题是鱼和熊掌不可兼得的吗？其实并不然。近年来，研究者提出了一些创新性的融合技术方案。一个主流的思路是先把不确定性衡量表达为某种可计算表示(representation)，然后基于这种表示进行聚类计算，最后通过纳入更丰富的簇中心点来达到多样性。</p><p></p><p>其中聚类计算可以采用很多经典的方法，比如k-means等。而不确定性的可计算表示(uncertainty&nbsp;representation)是最为关键的。非常有启发性的两个不确定性表示技术是BADGE和ALPS。</p><p></p><p>ALPS&nbsp;[8]：利用了掩码语言模型的损失函数，MLM&nbsp;loss&nbsp;的直接输出当作不确定性的表达</p><p></p><p>其中BADGE很巧妙地把梯度作为了不确定性的表示，由此启发了很多后续的工作。而ALPS把预训练模型常用的掩码语言模型损失作为不确定性表达，能较好地解决冷启动的问题。</p><p></p><h3>2.3&nbsp;他山之石可以攻玉：通过虚拟对抗扰动来表示不确定性</h3><p></p><p></p><p>作业帮的很多业务场景，也需要同时考虑不确定性和多样性，因此我们在实践中也延续了这一思路。</p><p>BADGE和ALPS是两个很好的不确定性表示的方案，但不一定符合实践中的一些需求。比如，在很多我们面对的实际场景中，需要模型的鲁棒性比较强，也就是对噪音样本有较好的处理能力。因此，我们需要设计另外的不确定性表示方法。</p><p></p><p>我们看到，在很多图像处理场景中，会采用虚拟对抗扰动(Virtual&nbsp;Adversarial&nbsp;Perturbation)[6]来增加模型的鲁棒性和泛化能力。于是，我们将其借鉴过来，提出了一个基于虚拟对抗扰动的不确定性表示。具体地，通过针对BERT&nbsp;&nbsp;的隐层表达创建其一一对应的虚拟对抗扰动(VAP)来作为样本数据的不确定性表示。相应地，提出了一个新的主动学习方案VAPAL(Virtual&nbsp;Adversarial&nbsp;Perturbation&nbsp;based&nbsp;Active&nbsp;Learning)。在VAPAL中，我们依然是遵循了不确定性学习+聚类的主体思路。</p><p></p><p>VAPAL&nbsp;算法流程&nbsp;[9]:</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d5046285932ff4c71c41e3e2e4089380.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/095370c2198233411349a37750c98818.png\" /></p><p></p><h2>实验验证&nbsp;</h2><p></p><p></p><p>我们在英文数据&nbsp;PUBMED&nbsp;和&nbsp;SST-2&nbsp;验证了该方案的有效性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/46ce5bc70f68aa65039536b8405571fb.png\" /></p><p></p><p>From&nbsp;[9]</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c2b250fe2aaaac0f5658188eceda655c.png\" /></p><p></p><p>From&nbsp;[9]</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/bef90c6307dfae5355068bb4d69eefeb.png\" /></p><p></p><p>From&nbsp;[9]</p><p></p><p>实验表明：</p><p></p><p>在公开数据集上，和现在的主流的BADGE以及ALPS算法相比，VAPAL取得了可比性能，是一个强有竞争力的主动学习策略候选。</p><p></p><p>同时实验数据表明，初始阶段VAPAL表现更优，意味着在极端标注资源受限的情况下，VAPAL更胜任。</p><p>VAPAL相关算法已经集成在作业帮内部自研的人机协同标注平台中。</p><p></p><p>下图是一个实际标注项目的汇总信息。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/63/63d478b5bbf9c9f7786f804d1105d2ce.png\" /></p><p></p><p>从图中可以看出，整个人机协同的标注迭代流程中，通过主动学习抽取的样本，经人工标注后，补充到训练数据后，训出的新模型能更快地符合标注复核人员的要求，从而完成对很多类别的自动扩标，极大地提升标注效率(本例中是10+倍的效率提升)。</p><p></p><p>下图是展示了一个同类型同规模任务，分别采用纯人工标注和人机协同主动学习快速标注的项目效果对比。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a4f94fbe11626c562a07a1152ff4d4f.png\" /></p><p></p><p>从图中可以看出：</p><p></p><p>标注量方面，协同方案提升约9倍产出专题方面，协同方案有助于标注更加精细:&nbsp;7个专题到58个专题时间方面：协同方案提效6倍以上机器标注准确率方面：采用抽样人工再确认，准确率为93.8%</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4b947f3f89c67a9997ae72aa43796889.png\" /></p><p></p><h2>总结</h2><p></p><p></p><p>我们从业务需求出发，通过引入主动学习提高了业务的数据标注效率。引入虚拟对抗扰动(VAP)，提出了一个和当前最优技术方案可比甚至在初期表现更优秀的新改进方案&nbsp;VAPAL。在实际使用中，我们发现了现有抽选策略(除了随机之外)都对随机种子有着相对的敏感性，后期我们将在这个方向上更进一步的优化。关于VAPAL的详细介绍，可以参见文献[9]。</p><p></p><p>参考文献:</p><p>1.Scheffer,&nbsp;T.,&nbsp;Decomain,&nbsp;C.,&nbsp;&amp;&nbsp;Wrobel,&nbsp;S.&nbsp;(2001).&nbsp;Active&nbsp;hidden&nbsp;markov&nbsp;models&nbsp;for&nbsp;information&nbsp;extraction.&nbsp;Lecture&nbsp;Notes&nbsp;in&nbsp;Computer&nbsp;Science&nbsp;(Including&nbsp;Subseries&nbsp;Lecture&nbsp;Notes&nbsp;in&nbsp;Artificial&nbsp;Intelligence&nbsp;and&nbsp;Lecture&nbsp;Notes&nbsp;in&nbsp;Bioinformatics),&nbsp;2189,&nbsp;309–318.&nbsp;https://doi.org/10.1007/3-540-44816-0_31</p><p></p><p>2.Ido,&nbsp;D.,&nbsp;&amp;&nbsp;Sean&nbsp;P,&nbsp;E.&nbsp;(1995).&nbsp;Committee-Based&nbsp;Sampling&nbsp;For&nbsp;Training&nbsp;Probabilistic&nbsp;Classi&nbsp;ers.&nbsp;MACHINE&nbsp;LEARNING-INTERNATIONAL&nbsp;WORKSHOP&nbsp;THEN&nbsp;CONFERENCE,&nbsp;150–157.&nbsp;https://doi.org/10.5555/3091622.3091641</p><p></p><p>3.Culotta,&nbsp;A.,&nbsp;&amp;&nbsp;McCallum,&nbsp;A.&nbsp;(2005).&nbsp;Reducing&nbsp;labeling&nbsp;effort&nbsp;for&nbsp;structured&nbsp;prediction&nbsp;tasks.&nbsp;Proceedings&nbsp;of&nbsp;the&nbsp;National&nbsp;Conference&nbsp;on&nbsp;Artificial&nbsp;Intelligence,&nbsp;2,&nbsp;746–751.</p><p></p><p>4.Sampling,&nbsp;C.&nbsp;U.,&nbsp;Sampling,&nbsp;D.,&nbsp;Regression,&nbsp;L.,&nbsp;&amp;&nbsp;Trees,&nbsp;D.&nbsp;(n.d.).&nbsp;Monarch&nbsp;-&nbsp;Human-in-the-Loop&nbsp;Machine&nbsp;Learning_&nbsp;Active&nbsp;learning&nbsp;and&nbsp;annotation&nbsp;for&nbsp;human-centered&nbsp;AI-Manning.</p><p></p><p>5.Settles,&nbsp;B.&nbsp;(2009).&nbsp;Active&nbsp;Learning&nbsp;Literature&nbsp;Survey,&nbsp;(January).</p><p></p><p>6.Miyato,&nbsp;T.,&nbsp;Maeda,&nbsp;S.&nbsp;I.,&nbsp;Koyama,&nbsp;M.,&nbsp;&amp;&nbsp;Ishii,&nbsp;S.&nbsp;(2019).&nbsp;Virtual&nbsp;Adversarial&nbsp;Training:&nbsp;A&nbsp;Regularization&nbsp;Method&nbsp;for&nbsp;Supervised&nbsp;and&nbsp;Semi-Supervised&nbsp;Learning.&nbsp;IEEE&nbsp;Transactions&nbsp;on&nbsp;Pattern&nbsp;Analysis&nbsp;and&nbsp;Machine&nbsp;Intelligence,&nbsp;41(8),&nbsp;1979–1993.&nbsp;https://doi.org/10.1109/TPAMI.2018.2858821</p><p></p><p>7.Ash,&nbsp;J.&nbsp;T.,&nbsp;Zhang,&nbsp;C.,&nbsp;Krishnamurthy,&nbsp;A.,&nbsp;Langford,&nbsp;J.,&nbsp;&amp;&nbsp;Agarwal,&nbsp;A.&nbsp;(2019).&nbsp;Deep&nbsp;Batch&nbsp;Active&nbsp;Learning&nbsp;by&nbsp;Diverse,&nbsp;Uncertain&nbsp;Gradient&nbsp;Lower&nbsp;Bounds.&nbsp;Retrieved&nbsp;from&nbsp;http://arxiv.org/abs/1906.03671</p><p></p><p>8.Yuan,&nbsp;M.,&nbsp;Lin,&nbsp;H.-T.,&nbsp;&amp;&nbsp;Boyd-Graber,&nbsp;J.&nbsp;(2020).&nbsp;Cold-start&nbsp;Active&nbsp;Learning&nbsp;through&nbsp;Self-supervised&nbsp;Language&nbsp;Modeling,&nbsp;7935–7948.&nbsp;https://doi.org/10.18653/v1/2020.emnlp-main.637</p><p></p><p>9.Zhang,&nbsp;H.,&nbsp;Zhang,&nbsp;Z.,&nbsp;Jiang,&nbsp;H.,&nbsp;&amp;&nbsp;Song,&nbsp;Y.&nbsp;(2022).&nbsp;Uncertainty&nbsp;Sentence&nbsp;Sampling&nbsp;by&nbsp;Virtual&nbsp;Adversarial&nbsp;Perturbation.&nbsp;Retrieved&nbsp;from&nbsp;http://arxiv.org/abs/2210.14576</p>",
    "publish_time": "2023-08-07 14:34:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "复杂场景下的深度学习模型部署",
    "url": "https://www.infoq.cn/article/Uf6zoaszciVzX2BFXo2h",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p>深度学习是一种使用<a href=\"https://xie.infoq.cn/article/874e0d574d2d098a908f6429d\">神经网络</a>\"的机器学习，正迅速成为解决从文本分类到推荐系统等许多不同问题的有效工具。然而，将训练好的神经网络模型部署到应用程序和服务中可能给算法从业人员带来不小的挑战。算法框架的不同、计算资源稀缺和缺乏标准实现等挑战都可能导致模型部署的失败。</p><p></p><p>作业帮的业务体量较大，需要部署大量的<a href=\"https://xie.infoq.cn/article/5edc0dd7bf64eed32d8a0dd28\">深度学习</a>\"模型应对不同的任务。为满足实际使用，我们部署的模型服务需要具有以下特点：</p><p></p><p>高并发低时延模型多更新快业务方多</p><p></p><p>结合任务特点与实际应用场景，我们还面临以下问题：</p><p></p><p>数据预处理模块与模型版本强耦合，部署时需保持同步。多个版本的预处理模块相互之间易冲突。模型太多，如果全部加载至GPU浪费资源。</p><p></p><p>因此，本文将以上述场景作为对象，阐述三种不同的解决方案，并对比它们的性能指标。最终目标是找到一个适合作业帮任务的模型部署方案，在合理占用资源的情况下，满足不同业务方的需求。</p><p></p><h2>模型部署方案</h2><p></p><p></p><h4>1.&nbsp;Gunicorn&nbsp;+&nbsp;Flask&nbsp;+&nbsp;Transformers</h4><p></p><p></p><p>Transformers是一个十分方便的自然语言处理库，它的宗旨是让最先进的NLP技术人人易用。通过Transformers可以轻松地实现模型的训练、加载和推理。此外，使用Flask框架部署深度学习模型是最简单快捷的方式。但是Flask自带的Web服务性能极不稳定，无法满足高并发、低延迟的需求。因此，我们通过Gunicorn框架提供的WSGI服务来部署Flask的接口，也就是将Flask代码部署在Gunicorn提供的WSGI服务上，可以显著提升服务的并发能力和稳定性。</p><p></p><p>本部署方式的整体框架如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/761687dcaa2d68dc24650621dc3a8788.png\" /></p><p></p><p>利用Transformers加载所需模型，但通过LRU机制控制加载在内存中的模型数量最大为8，防止内存多大。通过Flask&nbsp;Web框架构建模型预测服务，包括数据的预处理、模型预测和结果后处理等步骤。最后用Gunicorn启动Flask服务，设置worker数量支持异步处理请求，提高并发能力。</p><p></p><p>Flask服务的构建十分简单，具体如下图所示。实现一个模型预测函数，并将其与’predict’接口绑定即可。另外，需注意的是线上服务需要实现’ready’接口返回200和’ok’以执行健康检查。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d12168a76e800a31d0defe4ebebef2c5.jpeg\" /></p><p></p><p>采用Gunicorn+Flask+Transformers部署深度学习服务的初衷是，该方式与我们的训练框架较为耦合，实现起来比较简单。但在使用线上服务的实际场景中，遇到了以下问题：</p><p></p><p>在每个worker中，执行模型的预测会有100+线程，导致资源的挤兑。Gunicorn确实提高了Flask服务的并发能力，但在并发请求较多时，并不能解决健康检查超时的问题，使得pod摘流并增加预测时延。Transformers模型推理方式的时延较长。LRU机制的存在使得模型需要重新加载，而模型加载比较慢导致预测时延增加。</p><p></p><p>最后，这种通过这种部署方式部署至serverless，共6个pod，每个pod的资源是16核16G的情况下，能达到150QPS。</p><p></p><h4>2.&nbsp;Tornado&nbsp;+&nbsp;PyTorch</h4><p></p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;Tornado是使用Python编写的一个强大的、可扩展的Web服务器。它在处理严峻的网络流量时表现得足够强健，在创建和编写时又足够轻量，并能够被用在大量的应用和工具中。Tornado在设计之初就考虑到了性能因素，支持异步非阻塞，可以接受多个用户的并发请求。本方案采用Tornado代替Flask以实现更好的异步通信。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;本部署方案直接采用<a href=\"https://xie.infoq.cn/article/ecb68618b571df7327e43bb8f\">PyTorch</a>\"模型预测的方式代替Transformers，避免Transformers内Trainer的冗余操作以减少预测时间。此外，将每个模型分开部署至子服务，取消模型的动态加载机制，避免模型切换造成的耗时。同时，这种方式也避免了模型过多导致的资源挤占问题。</p><p></p><p>本部署方式的整体框架如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e2ae6246c92664bb28e58cb43d3a49c.png\" /></p><p></p><p>每个模型通过Tornado包装成子服务，包含数据预处理，模型预测和结果后处理。分发服务负责接收所有请求，并根据数据类别将其分发至对应的模型预测服务进行预测。子服务的预测结果返回至分发服务，由分发服务返回所有数据的预测结果。Tornado部署Web服务同样十分便捷，同时可通过’async’和’await’关键字轻松实现异步。</p><p></p><p>采用Tornado+PyTorch的方案部署模型的原因是，组里同事有Tornado的踩坑经验，方便实现性能上的优化（如异步）。此外，本方式仍与训练框架有联系，从训练到部署的链路仍比较完整。然而在上线测试后，发现本部署方式仍有以下问题：</p><p></p><p>PyTorch预测时启动大量线程的问题仍未解决。模型多的情况下，需要申请大量模块并部署，管理太繁杂。没有动态Batching功能，无法更好地发挥机器性能。</p><p></p><h4>3.&nbsp;单模型性能测试</h4><p></p><p></p><p>为更方便地进行性能测试，我们采用单个模型进行性能的测试。通过不同部署方式在单模型上的性能进行对比，以挑选最优的部署方案。本节中，我们采用了ONNX和TorchScript两种模型格式，并分别用Tornado和Triton&nbsp;Inference&nbsp;Server进行部署。</p><p></p><p>ONNX&nbsp;(Open&nbsp;Neural&nbsp;Network&nbsp;Exchange)是一种针对机器学习所设计的开放式文件格式，用于存储训练好的模型。它使得不同的人工智能框架可以采用相同格式存储模型数据并交互。对于我们的场景，其优势在于可以控制线程数防止资源挤占。</p><p></p><p>Triton是英伟达等公司推出的开源推理框架，为用户提供在深度学习模型部署的解决方案。其主要职责和服务紧密相关，服务中常见的需求需要它做处理。比如Batching，Sequence，Pipeline和模型仓库的管理等</p><p>我们将各个服务部署在Docker中，限定CPU核数为16来模拟线上模块的资源。通过Locust工具进行压测，具体结果如下表所示：</p><p></p><p></p><p></p><p>在不考虑预处理的情况下，Triton&nbsp;+&nbsp;TorchScript的部署方式是最优的。Triton的动态Batching功能极大地提升了并行预测的功能。但是在我们的场景中，数据预处理与模型预测强耦合。于是，尝试利用  Python&nbsp;Backend将预处理部分同样集成在Triton中，实现完整的模型预测服务。但在实际测试过程中，发现集成预处理的Triton服务效果并不理想，怀疑是预处理的串行使得模型预测时无法进行动态Batching。因此，我们最终决定将部署方式确定为Tornado&nbsp;Web服务和Triton&nbsp;Inference&nbsp;Server。</p><p></p><p>本节对ONNX的处理比较粗糙，可能并没有发挥其优势。但Triton&nbsp;+&nbsp;TorchScript的方式已经满足业务需求，并没有对ONNX的实验进行深究。</p><p></p><h4>4.&nbsp;Tornado&nbsp;+&nbsp;Triton</h4><p></p><p></p><p>本节介绍最终确定的模型部署方式：Tornado&nbsp;+&nbsp;Triton。通过将不同版本的预处理模块打包成不同的Python&nbsp;模块解决版本冲突问题。另外，针对业务方多且需求不一致的问题，我们通过写多个接口的方式解决。比如，’/category_predict’支持单条预测，’/category_predict_nlp’支持批量预测。于是，本部署方式如下图所示：</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/05914482faca601764422e4a340ff7c1.png\" /></p><p></p><p></p><p>其中，Tornado&nbsp;Web服务接收应用方的请求，对数据进行预处理，然后异步调用Triton&nbsp;进行模型预测，最后对预测结果进行后处理并返回结果。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;本节中通过题型预测任务对新的部署方式进行测试，将对应的21个模型部署至限定16核的Triton&nbsp;Inference&nbsp;Server。产生随机数据模拟线上请求并用Locust工具进行压测，具体结果如下：</p><p></p><p></p><p></p><p>通过结果可以发现，将数据预处理和结果后处理集成在Web服务中并通过Triton进行模型部署的方案在我们复杂的业务场景下是最优的。这种方式不仅通过动态Batching的方式增加并发降低时延，并且方便了模型的管理与更新。其唯一的缺陷是无法解决预处理模块和模型版本强耦合的问题，在部署时需保持同步，且时间点最好是在服务低谷期。</p><p></p><h2>总结与展望</h2><p></p><p></p><p>本文涉及的场景比较复杂，主要有以下几个问题：</p><p></p><p>模型多，不同任务的模型总数接近100个，且更新频率随业务节奏变更。易冲突，数据预处理和模型版本强耦合，且不同版本预处理模块易冲突。性能要求高，任务面向用户，在部署至CPU环境下要求高并发和低时延。</p><p></p><p>因此，本文从实际应用场景触发，设计并尝试了多种部署方式，对比其在线上使用过程中的表现。经过几次实验后，找到了一种适用于我们复杂场景的深度学习模型部署方式，在不依赖GPU资源的情况下满足业务方的需求，并为后续的模型部署提供了一种可行的部署方案。</p><p></p><p>然而，由于业务节奏，本文在实验过程中仍忽略了一些问题。比如，ONNX格式的模型部署在Triton中的性能为何不如部署在Tornado&nbsp;Web服务中，或许是ONNX在Triton中需要一些特殊的配置。另外，数据预处理和模型版本强耦合的问题仍未得到合理的解决方案，目前的处理方式仍有版本对不齐的风险。本文未对这些问题进行深入研究，留待后续探讨。</p>",
    "publish_time": "2023-08-07 15:07:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "小灯塔系列-中小企业数字化转型系列研究-HR测评报告",
    "url": "https://www.infoq.cn/article/8a866ffe0d3bf3c5ffa3a2dbf",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1b554d32cd46175a6c62a6feefc9ec3b.png\" /></p><p></p><p>向量智库将持续发布三大系列测评报告：小灯塔系列、双碳系列、信创系列。</p><p>小灯塔系列：服务于中国中小企业数字化转型市场</p><p>双碳系列：服务于中国“碳达峰&amp;碳中和”服务市场</p><p>信创系列：服务于中国信息技术应用创新产业</p><p>首批系列报告超200份，将会面相不同服务主体，提供垂直细分领域的市场规模数据、行业应用渗透数据以及品牌选型基本信息数据。</p><p>向量智库希望通过可信、可靠、可溯源的数据，帮助不同人群更好地理解市场趋势与机会，并提供精准的参考依据，为企业和决策者解决精准第三方数据应用的困惑。</p><p>向量智库将持续更新垂直细分领域数据，确保报告的时效性和实用性。践行我们的目标——用数据丈量每一寸荆棘&amp;坦途，向数智化永不迷路！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bfcc74de9baa8cbcbcef0cd27da4c63b.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/68da4d8df5faf8ec29c1ceadae77aa71.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/862a73e4a2fdc1eb7dbb0180dcebbd7e.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/dfda037992a9ae452cca2d31102ba523.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1f/1f0d09b19ed7057f27a3650e5acde529.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5fb89eafff624244960a1068ac40dde5.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a2833d9697e2df62bc9075436f328b93.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8b510863320e5d180e441408d6edc821.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8f5c0484a71af360064b8139de24e35a.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c15580f70e0df2b3710cd0a3fbe4ce40.jpeg\" /></p><p></p><p>报告 PDF 获取方式</p><p>添加「 向量智库|小助手」客服账号：cniteyes，私信获得；</p><p>或者关注「向量智库」公众号，后台回复\" 报告\"或\"小灯塔\"获得下载链接；</p>",
    "publish_time": "2023-08-07 13:55:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "新 QQ NT 桌面版如何实现内存优化探索？",
    "url": "https://www.infoq.cn/article/ZkQy1ouqocjxzpCQvKTq",
    "summary": "<p></p><h2>背景</h2><p></p><p>QQ 作为国民级应用，从互联网兴起就一直陪伴着大家，是很多用户刚接触互联网就开始使用的应用。而 QQ 桌面版最近一次技术架构升级还是在移动互联网兴起之前，在多年迭代过程中，QQ 桌面版也积累了不少技术债务，随着业务的发展和技术的进步，当前的架构已经无法很好支撑对 QQ 的发展了。在 2022 年初，我们下定决心对 QQ 进行全面的技术架构升级，对于这样一个国民级应用的重构，挑战无疑是巨大的。</p><p></p><p>新版桌面 QQ 自内测以来也受到许多热心网友和行业人士的关注，非常感谢大家在内测过程中提的各种有建设性的建议和反馈。其中，也有一小部分有开发背景的用户对我们采用 Electron 框架表达担心：高内存占用、超大安装包、启动缓慢等。究其原因还是担心新版本 QQ 资源占用大、体验变差，针对用户的担心，我们在内存上进行了专项优化，也取得了一些阶段性的进展，过程中也积累了不少经验，也借此机会分享给大家。</p><p></p><p>新版 QQ 在内存上的挑战主要表现在以下 4 个方面：</p><p>产品形态：由 1 个复杂的大面板（100+ 复杂程度不等的模块）和一系列独立功能窗口构成。窗口与渲染进程一一对应，窗口进程数很大程度影响 Electron 的内存占用。对于那个复杂的大面板， 一旦没有精细控制就很容易导致内存持续走高。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/029591e46ffc663d334e439945fe7d12.png\" /></p><p></p><p>[Electron 窗口多进程]</p><p>使用习惯：用户长时间挂机。相比用完即走的 Web 页面，QQ 用户在一次登录后，可能会挂机一个月以上。这段期间，如果没有控制好 QQ 内存使用，那么结果可能是内存越占越大、用户交互响应变慢、甚至发生闪退。版本迭代：已经 24 岁的 QQ 拥有众多的功能和特性，过去一年我们一直做这件事：从核心特性开始快速补齐 Windows 版本的功能，同时也有一些高优先级的新功能要上。持续且快速的版本迭代，很可能产生新问题，使性能劣化。应用架构：新版 QQ 依赖一个 NT 核心数据模块（C++ addon），为 UI 提供本地化的数据服务。QQ 的加载体验能做到如此丝滑，这个模块起到了至关重要的作用。同时，与 NT 的联动优化，也需要拉通客户端 C++ 开发同学共同完成，当然，会存在一些沟通成本，但不可否认，能把内存占用压下来，客户端同学也付出了非常多的努力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d229c00080c5ea6902a079209bc8ad4.png\" /></p><p></p><p>[桌面端 QQ 整体架构]</p><p></p><p>在这篇文章中，我们将和大家分享新版 QQ 在内存优化方面的探索和阶段性优化进展。虽然本文的讨论主要集中在 Windows 平台，但由于 Electron 的跨平台特性，大部分优化措施也同样适用于 macOS 和 Linux 平台。</p><p></p><h2>内存现状与目标</h2><p></p><p></p><p>在着手优化之前，我们结合旧版 QQ 以及其他优秀的桌面应用，给新版 QQ 设定了优化目标：</p><p></p><p>第一阶段目标，单个进程内存 &lt; 300M。早先因为没有腾出手处理内存问题，代码中存在一些泄漏。长时间挂机后比较容易出现单个进程超过 300M 的情况。我们在去年 9 月份系统地处理过一波内存问题，基本可以保证单个进程的内存占用 &lt; 300M。</p><p></p><p>第二阶段目标，单进程 &lt;100M，整体 &lt; 300M。整体是指启动 QQ 聊天面板后，6 个进程内存占用之和。内存达标之后才允许交付新版 QQ Windows 版本：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b8c31836a334281ebc936160b044e7f2.png\" /></p><p></p><p>[Windows 任务管理器的 QQ 内存占用详情]</p><p></p><p>这些进程会随着 QQ 的启动一直存在。我们重点看下这 3 类进程，这也是内存优化的大头：</p><p>node：Electron 的主进程，负责窗口管理、跨进程通信等。包含 NT 核心数据模块，负责与服务端交互，为 UI 提供数据服务。renderer：Chromium 内核的渲染进程，负责渲染 UI、提供用户交互等。QQ 启动后，会有 2 个渲染进程：一个是 QQ 大面板，另一个是主进程的窗口池。窗口池是预创建的一个渲染进程。在新开窗口时，可以减少等待时间。gpu：Chromium 内核的 GPU 进程。它的主要作用是处理与图形相关的任务，例如渲染网页、播放视频、执行动画等。</p><p></p><p>设定了目标后，我们先对 QQ 的内存占用情况进行了摸底。我们从用户的角度出发，使用 Windows 任务管理器来观察 QQ 的内存占用情况。我们先从最简单的 “Hello World” 开始，看看 Electron 应用的最低内存需求是多少，以及上限在哪里。结果显示，只需要 68M，并没有达到传说中的几百 M 那么大。</p><p></p><p>然而，随着使用的深入，比如在 QQ 聊天场景中进行一些操作之后，主进程、GPU 进程和渲染进程三个进程的内存占用就已经达到了 600M。这意味着我们距离目标还有超过 50% 的优化空间。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/71/71a24b817f3b1b6e4c1166abcb88f605.png\" /></p><p></p><p>注：AIO 是聊天面板的简称。</p><p></p><p>这个初步的观察让我们看到了目前的挑战，同时也让我们看到了优化的可能性。我们有信心，通过精心设计和持续优化，逐步接近甚至超越我们设定的目标。</p><p></p><p></p><h2>内存优化我们都做了什么</h2><p></p><p></p><p>接下来，将重点介绍我们是如何掌控和优化 Electron 的内存的。我们的工作主要包括以下几个方面：</p><p>工具分析：首先，我们需要使用不同维度的内存分析工具，从 V8 引擎到进程，再到整个应用程序，打通整个链路进行多角度的细节分析，以此来定位内存使用的瓶颈。定向优化：在通过工具定位到问题之后，我们会采取一系列的针对性优化策略，包括缓存策略、按需加载、优雅降级等。具体的优化工作我们将在后面进行详细介绍。线上监控：在本地或小范围内验证通过之后，我们需要广大用户的验证来确认我们的优化措施是否适用于所有场景。然而，如何获取用户在 Windows 任务管理器中看到的内存使用量是一个挑战，我们已经做了大量的研究和验证。防止性能退化和自动化测试：为了保护我们辛苦得来的优化成果，并避免频繁的版本迭代影响 QQ 的内存目标，我们会借助开发框架、工具建设、代码审查等手段来预防性能退化。</p><p></p><h4>工具分析</h4><p></p><p>在进行性能优化之前，我们需要选择合适的工具来帮助我们分析问题。QQ 的代码不仅包含 V8 的 JS 部分，还包括许多 Native 的 C++ 模块。仅依靠 Chromium 开发者工具进行性能分析是不够的，因此我们需要组合使用多种工具来共同解决问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33dd359e46af9e00aad1265461cde235.png\" /></p><p></p><p>这些工具如何使用，由于篇幅的关系我们在这里不做详细介绍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7d/7d6a9b23d72d5e6ce20683805f39fa63.png\" /></p><p></p><p>[部分内存分析工具截图]</p><p></p><h4>定向优化</h4><p></p><p>&nbsp;1) 最大化资源使用率</p><p>代码及静态资源</p><p>桌面版 QQ 的功能逻辑非常复杂，代码量庞大。虽然代码不需要通过网络请求加载，本地加载速度通常较快，但加载如此庞大的代码会占用大量内存。因此，仍然需要进行代码瘦身、静态资源优化、分包和按需加载等优化措施。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e9112bb142ddfb69f5a1a379a677018d.png\" /></p><p></p><p>[Devtools &gt; Memory 分析 QQ 主窗口内存占用]</p><p></p><p>首先是代码瘦身。对于第三方包或 SDK，它们往往包含了完备的 Web 兼容性及能力，而这些对于 Electron 客户端来说并不是必需的。因此，我们会对它们进行定制裁剪或独立实现，以减少代码的加载。</p><p></p><p>对于 QQ 的业务代码，分包策略不完全按照每个页面（窗口）以及模块复用次数来进行制订，更多的情况是按照场景模块来进行细粒度的定制。以打开一个窗口到进入使用场景为例：1）窗口池中预启动的窗口页面只加载必须执行的基础代码；2）当打开具体窗口时加载对应的路由后页面入口代码；3）当具体使用不同功能时动态加载，如点击搜索、打开表情面板、转发消息激活好友选择器的时候才会分别加载对应功能模块代码。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/544ba03b1745f3dd1634b4c8845a266e.png\" /></p><p></p><p>[QQ 主窗口业务模块的拆解]</p><p></p><p>此外，其他静态资源（如 SVG、base64 图像）在加载时也会占用不少内存，所以我们采取了按需加载的策略：只在可见时加载，不可见时主动销毁和回收。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2f56e8c8a2b5d7aebe4ebac32da8c3d.png\" /></p><p></p><p>[svg 及 base64 资源的 string 内存占用]</p><p></p><p>为了提升执行效率和代码保护的目的，我们将 JS 代码转成了字节码。尽管跳过了源码编译，直接将字节码交给 V8 执行，但在程序报错还原堆栈等运行时步骤中，V8 仍然会引用源码字符串。为了去掉这份源码，我们使用和源码等长的空格来占位，但通过 devtool 检查发现这些空格字符串仍会占用不少内存空间。最终，我们采取修改和移除 V8 对源码字符串引用的方式，彻底解决了源码字符串的内存占用问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/60a21fa94a54c4d99405a98ca760df66.png\" /></p><p></p><p>图片资源</p><p>QQ 作为一款 IM 工具，会涉及到大量的图片收发。然而，图片的渲染会占用相当大的内存。举个例子，一张分辨率为 4000 x 2750 的图片，结合设备屏幕像素和聊天区设计尺寸，只需渲染宽度为 567 像素的分辨率图像即可清晰展示。如果以宽度为 4000 像素的分辨率渲染，理论上两者位图所占用的内存大小差距可达 50 倍，并且还会因为渲染带来性能损失。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/038fa2660ec94cb55892f3bb433a92e5.png\" /></p><p></p><p>[图片尺寸对内存影响举例]</p><p></p><p>在聊天消息列表中的大部分图片仅仅起到预览作用，缩略图渲染就满足了需要。而仅仅在用户真正打开图片查看器放大查看时，才会需要用原图渲染。</p><p></p><p>实测在聊天中多张不同大尺寸分辨率图片在展示时，渲染进程和 GPU 进程的内存占用有着明显差别。在收发图片时，我们会根据屏幕设备信息和计算展示区域所需实际渲染分辨率，当原图分辨率超出计算所需值，则先调用压缩服务进行图片压缩，生成渲染所需分辨率的缩略图，并在聊天区域进行渲染上屏。在这个策略的优化下，一般聊天图片场景测试下来，使用缩略图比原图约有 30M ~ 50M 的内存优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/92ec5f6ed4f3f505239399636cabdba3.png\" /></p><p></p><p>[QQ 优化图片上屏策略]</p><p>2）可视区域按需渲染</p><p>DOM 元素数量</p><p>在 DOM 元素使用数量我们也有严格的控制，总体采用”所见即占用“的 DOM 渲染策略。在 QQ 大面板中只有视口所见的内容才会渲染对应 DOM 元素。其他所有组件在不渲染展示时，均会移除组件及其 DOM 元素来避免其内存开销。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/622aab0e323ae3a6b879b67cfb9e7545.png\" /></p><p></p><p>[大虚拟列表控制 DOM 数量]</p><p></p><p>尤其对于各个大列表模块，比如联系人列表和群成员列表，DOM 元素都非常多。最开始的内测版本中，使用有大量好友和群聊的 QQ 号，窗口平均 DOM 数达到 13000。我们将 QQ 所有的普通分页列表替换为虚拟滚动列表，并且对列表滚动 buffer 进行极限压缩甚至是 0 buffer 。由于不再一味采取空间换时</p><p>间，没有 buffer 的情况下必然面对列表滑动性能挑战，因此也需不断优化各类 item 组件渲染性能。</p><p></p><p>此外，我们还通过精简组件 DOM 层级，移除非核心组件 keep-alive (重新优化渲染性能) 等方式，大账号使用下整体的 DOM 数量从 13000 减少到控制在平均 4000 以内，这部分优化减少约 20M 内存。</p><p></p><p>渲染图层</p><p>渲染图层方面，在渲染时满足某些特殊条件的渲染层，会被浏览器自动提升为合成层，达到提升渲染性能的目的。但是每个合成层都占用额外的内存，应当去掉过量且不必要的合成层来控制图层带来的内存占用。当然结合渲染性能考量，对于高频且列表等核心模块，是可以单独提升合成层。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/963c78c7b4b496b19d3c26cb621dc6a6.png\" /></p><p></p><p>[QQ 对于渲染合成层的优化处理]</p><p></p><p>在桌面端 QQ 中通过超级调色盘可以为进行色彩换肤，在这个场景中全局各模块有不少单独提升的合成层来实现毛玻璃、渐变和纹理效果。另外还有许多不经意间被提升的隐式合成层。通过对不必要的合成层进行移除与合并，整体也优化了约 9.3M 内存。</p><p></p><p>结构化消息</p><p>QQ 支持丰富的消息类型，从简单的文本、图文消息，到复杂的 lottie 表情、下图所示的业务可定制的结构化消息等。我们知道 JavaScript 是单线程的，这些消息同时上屏的时候可能会出现过长的上屏任务而导致 UI 卡顿，给到用户的感受就是切换消息列表卡顿，消息上屏慢等糟糕的体验。</p><p></p><p>新版 QQ 针对这类复杂消息上屏，使用了 JavaScript 事件机制结合 WebWorker 来实现消息异步上屏，并使用 OffscreenCanvas + &nbsp;Worker 池绘制来提升渲染性能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/8249ade1b6ca578cf8e7332a202af84d.png\" /></p><p></p><p>[QQ 结构化消息的处理方案]</p><p></p><p>为了在 Canvas 中实现 CSS 的 Flex 布局效果，我们采用了跨平台的布局解决方案，将 Yoga 编译成 WebAssembly 运行在 WebWorker 中。Yoga 官方编译采用的是 asm.js 的方案， 这种方案不支持动态分配内存，可以看到它默认分配了一个较高的内存，达到了 128M。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2c5a2c99e1e4e0bbd7b4c41711b4bfe.png\" /></p><p></p><p>[Yoga 渲染引擎的原始内存占用]</p><p></p><p>为了优化 WebAssembly 的内存占用，我们调整了编译方式，将 Yoga 编译成独立的 wasm 文件，这种方式相比 asm.js 支持动态内存分配。同时结合聊天窗口的消息卸载策略，经过不断的测试调优，在既要保证初始内存较少又要尽可能避免内存爆发式增长带来的性能损耗的前提下，我们把 WebAssembly 的初始内存分配优化到 2M，再加上对象共享、享元模式等策略，WebWorker 的内存占用有了非常可观的优化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a908fe10955a238611e3d5508f5adf86.png\" /></p><p></p><p>[QQ 结构化消息渲染引擎优化前后的内存占用对比]</p><p></p><p>复杂的聊天消息虽然是必不可少的功能，但是实际的消息量还是远少于普通的图文消息，因此在保证用户体验的前提下，在合适的条件下适时销毁 WebWorker 是一个合理的策略，而随着 WebWorker 被销毁这个线程所占用的内存也能被完全释放。</p><p></p><p>&nbsp;3）性能与体验平衡</p><p>Lottie 及动画方案选型</p><p>超级表情采用 Lottie 动画技术方案，有高清高帧率高质量特点，但同时也为我们带来了渲染的高成本。为了保证 Lottie 的高帧率和减少 CPU 占用，我们缓存了 Lottie 渲染器生成的动画帧，内存消耗成为了首要问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa0f6e90895aa205e2cb7ed654468470.png\" /></p><p></p><p>[QQ Lottie 动画示例]</p><p></p><p>对其进行定量分析，超级表情 Lottie 资源继承自手机 QQ，尺寸是 512 × 512，动画帧以 int8 数组存储，所以一帧动画为 512 × 512 × 4 / 1024 bit= 1024 Kb = 1Mb，一个普通大小的超级表情，例如庆祝表情，有 160 帧动画，依据缓存 9/10 帧动画的策略，庆祝表情会占用 144Mb 内存，虽然是可回收的，但也无疑是巨大的内存消耗。关注到 Lottie 渲染的内存消耗后，我们主要从以下 2 步入手：</p><p>缓存的动画帧尺寸：桌面端 lottie 渲染大小为 120 × 120，考虑到需要保持 Lottie 动画的高质量，缓存的动画帧尺寸调整为实际尺寸大小的两倍，即 240 × 240，降低内存消耗 72%。经设计确认，清淅度上也没有明显的差异；缓存策略：缓存 9/10 的动画帧减少到缓存 3/4，降低内存消耗 35%，而且调整之后帧率还能得到保障；</p><p></p><p>通过以上 2 步，一共降低内存消耗 81.8%，庆祝表情从 144 Mb 降低到 35 Mb。</p><p>最后，旧策略对于渲染过且暂时不用的 Lottie 表情，会 buffer 它的第一帧，总共 31 个 Lottie 表情：2.3k * 31 = 7M（最多），经评估之后，我们暂时也拿掉了该策略。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/87e0f7d1ab8d5cf26032cdece0335656.png\" /></p><p></p><p>[QQ Lottie 动画缓存首帧对内存的影响]</p><p></p><p>另外，桌面 QQ 左侧导航栏目，为了与移动端统一体验，使用 Lottie 动画来实现，从 memory 面板来看， 4 个 icon 导航条会占用约 6M 的内存。改用 CSS 实现，不仅效果与 Lottie 的几乎一致，而且这 6M 的内存占用就完全省掉了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/79d45683279114a04031764574489a70.png\" /></p><p></p><p>[QQ 导航条动画对内存的影响]</p><p></p><p>APNG 动画优化</p><p>APNG 是一个基于 PNG 的位图动画格式，后缀名也是.png，在一些类似背景图的场景下会使用；在早期的超级调色盘中，为了实现最佳炫彩效果，选用了由 300 张 15KB 静态图合成的配色渐变的 apng 图片，其大小达到了 4.2M；不过带来的问题是渲染的延迟感；经过和设计讨论，在不影响效果体验的基础上，进行了大量的压缩，压缩到了 157KB；压缩率超过 96%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cc80af426e0b4aeaa812e382ef5369e.png\" /></p><p></p><p>聊天列表与消息</p><p>聊天列表 AIO，作为 QQ IM 模块中最主要的承载消息数据展示模块，其滚动体验必然离不开用户体验与内存的权衡。</p><p></p><p>聊天列表在静态与滚动过程中，维持消息组件的数量多少决很大程度决定整个 QQ 的内存占用。消息数据从服务端拉取后会存储在本地 DB，根据策略会将当前会话的消息数据缓存在内存中。</p><p></p><p>随着滚动加载，消息缓存占用的内存也越多，所以也有一定动态阈值的策略，丢弃滚动方向相反的旧消息，从而将内存控制在可接受范围。如果用户重新操作又需要加载时，这请求底层向本地磁盘 DB 重新拉取。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/87007c21196a65f2763792ea2291a88e.png\" /></p><p></p><p>[QQ 聊天消息列表的加载策略]</p><p></p><p>消息组件实例是内存占用的大户，每条消息组件内部包含头像 / 昵称 / 状态 / 内容等多个实例，如果不对消息实例进行回收销毁，每百条消息约能带来 20M+ 的内存增量，因此消息实例的回收策略尤为关键。</p><p></p><p>最早版本中对消息上屏没有丢弃策略，内存增量没有很好控制。于是采用分页列表，屏内保持固定几页消息（约 30 ~ 50 条消息，视屏幕尺寸决定），超过范围的消息进行丢弃，列表高度由屏内消息直接撑起，用户通过触顶或触底进行上下一页消息的加载。</p><p></p><p>但这页带来些点问题：一方面随着触顶触底，滚动条频繁跳动的体验并不好；另一方面列表高度由不定高的组件渲染消息来维持，不得不始终保留 30 ~ 50 条消息以撑起滚动高度，不可见消息的那部分便造成内存的浪费。</p><p></p><p>使用虚拟列表维持计算高度后，列表不再依赖保持真实消息内容的渲染，理论上我们可以将可视区域以外的消息实例全部销毁，仅保留用户可见的消息，最大程度地压缩消息实例数量，指保留很少的 buffer 消息实例。在实际滚动中由于消息实例在滚动过程被不断创建和销毁，占用主线程，影响 UI 绘制和用户输入。因此我们还做了：1. 对创建销毁做一定聚合，批量处理消息上屏。2. 精简优化单条组件的渲染性能。3. 不同滚动方向调整上下不同 buffer 大小 等等措施。4、会话切换和窗口聚失焦最小化等操作时对不再使用的消息资源内存进行主动回收。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/87ee108caf93890a45677863a75f1c7c.png\" /></p><p></p><p>[QQ 聊天消息列表的上屏策略]</p><p></p><p>滚动性能和内存占用之间需要取得平衡，既要最大程度压缩上屏消息数量以节省内存，又要保证滚动性能体验。然而经过优化后，本地测试加载 200 条混合种类的消息场景下，从空状态进入聊天会话中，消息列表内存增量从最多 44.2M 降至 6.1M，且滚动静止后内存不会任意增长。</p><p></p><p>4）Electron 使用姿势</p><p>Electron 给主进程提供了不少对系统能力调用的 API，如托盘、系统通知、macOS 中 dock 栏设置等。但是如果对这些 Electron 能力的使用方式不对，就可能导致不必要的大量内存占用甚至是泄漏。</p><p>比如 QQ 中，我们通过短间隔定时调用 Tray setImage API 来实现 QQ 托盘的闪烁，如果不注意传入 string Path 则会每次创建 Image 对象导致内存占用，正确的方式应该创建 NativeImage 并缓存，调用 Tray setImage 传入指定 NativeImage，避免反复创建 Image 导致的内存问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1f/1f7149bd29341fbde6e9884155daa4e8.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/6739fb9efd910bf91dda432b856c0899.png\" /></p><p></p><p>[Windows 托盘图标内存泄漏定位]</p><p></p><p>类似的问题还有在 macOS 中调用 API dock.setIcon 也会持续占用约 20M 的 CGImage 位图内存，正确的方案应该是不通过 Electron API 指定，而是通过打包 plist(属性文件) 指定 dock 栏图标。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff4272ee8c213ad70bca04ef217d031e.png\" /></p><p></p><p>[macOS dock 图标内存泄漏定位]</p><p></p><p>在使用 Electron 的过程中，还存在类似会导致内存问题的使用方式，我们需要结合客户端内存工具进行深度挖掘和分析，才能发现和处理这些问题。</p><p></p><p>&nbsp;5）消灭内存泄漏</p><p>我们知道 V8 有自己的垃圾回收机制，虽然它在 GC（垃圾回收）方面有着其各种策略，并做了各种优化从而尽可能的确保垃圾得以回收，但我们仍应当避免任何可能导致无法回收的代码操作。常见的例子包括：</p><p>未移除的监听器和定时器：在监听事件处理函数其中引用的不被释放导致的泄漏。游离 DOM 未释放：移出 document 后游离 DOM 仍存在引用导致无法释放。较多发生于框架的组件销毁时，相关监听未取消导致组件没有释放的情况。监控 / 打点导致的泄漏：在使用 Performance.mark 打点监控时，产生 PerformanceMark 对象，在用完之后没有手动清除，也会导致内存泄漏。console.error 导致的泄漏：控制台持有被打印对象始终不释放，导致应用的泄漏。其他不当的闭包及隐式的全局变量。</p><p></p><p>以上是桌面 QQ 在早期遇到的常见问题。后续，我们通过代码检测手段来防范这类问题的出现。与一般的前端项目不同，由于桌面 QQ 的长周期使用特性，任何缓慢而微小的内存泄漏都可能被放大，这也是我们极力把控并阻止任何可能导致内存泄漏的代码引入的原因。</p><p></p><h4>优化结果与线上监控</h4><p></p><p></p><p>经过一系列组合优化之后，在我们自己的设备上来看，QQ 的内存使用基本是达标了，长时间挂机稳定在 300M 以下，在广大 QQ 用户侧能否保持这个水平？只有通过线上内存及性能的采集监控，才有数据指标来观测，从而才能对优化有效性进行验证和决定如何调整优化方向。好在 Electron 提供了 app.getMetics 、 process.getMemoryInfo 等 API 来采集内存指标。但需要注意的是这些 API 所采集返回的内存值的真实含义，如 getMetric 所采集的到 workingsetSize 和 privateBytes 均不是任务管理器用户所看到的内存。</p><p></p><p>这里我们通过 patch 定制改造 Electron getMetics API，来增加不同平台任务管理器的内存类型的指标，并且采集包含了主进程、渲染进程、GPU 进程和工具进程等所有内存指标。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/66bd1c95041358d138d1eff9c5a4e711.png\" /></p><p></p><p>为了避免频繁采集上报内存指标所带来的的性能消耗，我们设定了一定时间的采集间隔，同时针对使用场景的采集做了抽样。并将渲染进程 pid 映射寻找窗口名，只在若干次采集后再做聚合计算，通过 SDK 上报到 prometheus + grafana 的指标观测平台。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0fa8a8501bb6f3e79272b010b5b73465.png\" /></p><p></p><p>[QQ 内存监控整体方案]</p><p></p><p>经过若干次内存性能优化的迭代, 目前从线上数据指标来看, 新版 Windows QQ 运行的内存在主场景下基本控制在 300M, 这个值已经基本达到我们设定的目标。</p><p></p><p>从登录后使用过程中的内存指标如下：整体应用的内存平均占用约为 228M；其中中位数占用约为 211M，90% 分位用户内存占用约为 350M。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/852f8b9bec219586977711104ad9a35a.png\" /></p><p></p><p>[QQ 线上内存监控视图]</p><p></p><p>当然，这个目标只是阶段性的，我们还会持续针对更多使用场景进行内存优化。</p><p></p><h4>防劣化与自动化测试</h4><p></p><p></p><p>为了持续关注和保证新版 QQ 项目的性能达标且不劣化，除了比较常规的单元测试、代码检查、代码评审机制、框架内置一些开发规范等手段外，我们还在建设一个防劣化平台，主要通过自动化的端对端 (e2e) 测试来持续监控项目集成后的性能变化。</p><p></p><p>定时对主干上集成构建的程序进自动化 e2e 测试；除了对功能的冒烟测试外，针对重点关注的性能指标，构造了对应的帐号和环境，编辑特定的用例，用于采集性能指标；通过将采集和采样的指标上报到防劣化的监控平台，来监控项目集成后的性能变化，如会话切换响应时间、内存占用、CPU 使用率等；监控平台提供按版本和时间的指标曲线、对比，方便查看和分析性能变化情况。同时打通企业微信机器人，对性能指标情况进行实时推送告警；根据告警信息对应的版本信息和代码记录，排查情况，闭环问题。</p><p><img src=\"https://static001.geekbang.org/infoq/51/51b58f88305aa7f6d4f7a0536d67e21f.png\" /></p><p></p><p>[防劣化机制示意图]</p><p></p><p>这一套机制之前在 内测中的 QQ 频道桌面端的项目中尝试应用，运行发现了一些比较典型的代码异常、crash、oom 问题，证明确实有效。新版 QQ 业务和设计都更复杂，建设好防劣化机制无论是对发现问题的效率，还是对整体的性能和质量都是意义重大的，也是我们团队当前重点建设、未来持续迭代的重要任务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a0d7a8f67f0f6dac3a15ff6fb10db44c.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43e4c4fd61c4230acc18b0991ea80e0b.png\" /></p><p></p><p>[防劣化推送与告警实际应用图例]</p><p></p><h2>总结</h2><p></p><p>可能大家比较关心，为什么一定要选择 Electron？其实我们是经过深思熟虑的：</p><p></p><p>首先，全新 QQ 意味着我们应该专注在功能快速迭代上，否则，以 QQ 的体量战线会拉得非常长。我们希望最后选择的跨平台方案应该是足够成熟、低开发和使用成本，不需要为了使用框架本身，还需要投入额外巨大的人力成本。这个其实在 React Native、Flutter、Tauri 等跨平台框架的使用过程中，我们都遇到过类似的问题，除了功能开发，为了把框架生态、周边、工具链建设好，还需要投入巨大的额外成本，Qt 也有类似的问题。而使用 Electron，对于 Web 前端开发同学，基本上是 0 成本，现有的 Web 前端的大部分基建都可以直接复用，而且使用 Web 开发 UI 的效率，在主流技术栈里算是很高的了。并且这几年主流的桌面端应用基本都选择了 Electron，如 VScode、Discord、Slack、Skype、Whatsapp、Figma 等等，新的桌面应用基本上也是首选 Electron，另外，Electron 版本的迭代速度和社区氛围都很在线。</p><p></p><p>其次，从结果或者解决问题的角度来看，经过一系列优化之后基本可以将 QQ 核心聊天场景的内存控制在 300M 以内，150M 的安装包大小，与旧版纯 Native QQ 差别较小。不单单内存占用，其他核心体验，比如切 AIO 的流畅度上要优于旧版 QQ。即便是在今天，QQ 也坚定一年半之前选择了 Electron。</p><p></p><p>最后，让我们再次聚焦在内存优化的工作上，下图是我们在桌面 QQ 中针对 Electron 内存优化工作的一个概览。内存优化没有银弹，有的只是一步一个脚印深入做下去，芝麻西瓜都要捡，从量变到质变。未来我们完全有信心，凭着已有的经验和对其技术的理解，守住现在这些成果的同时，进一步优化 QQ 生态下的各个子业务、子模块的内存占用问题。因此，也希望通过我们实践经验分享， 让大家从更多辩证的视角来重新看待 Electron 或类 CEF 的技术方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/127d60c67c1612d7a8c9f96aacfed334.png\" /></p><p></p><p>[桌面 QQ 内存优化工作概览]</p><p></p><p>广告一下：新版 Windows QQ 体验地址 QQ PC 版官方网站(https://im.qq.com/pcqq)，欢迎感兴趣的同学试用，也欢迎大家多提宝贵意见。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177059&amp;idx=1&amp;sn=fccce19ed89f05e84672f023a98358f6&amp;chksm=bdb838308acfb126587a9c6edfe455072d2ed85a1ece8d5e413ba7a4b878d8e0c2d92f83c2fc&amp;scene=21#wechat_redirect\"></a>\"<a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177144&amp;idx=1&amp;sn=cbe8d903d4447b12f4fc761d8266d536&amp;chksm=bdb838eb8acfb1fd41c7546d8f4768b33844098ab9e8b5d163b3947d67c02ec74434332e1f30&amp;scene=21#wechat_redirect\">两个多月完成全自研：大模型之争，从 GPU 卷到了向量数据库</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177059&amp;idx=1&amp;sn=fccce19ed89f05e84672f023a98358f6&amp;chksm=bdb838308acfb126587a9c6edfe455072d2ed85a1ece8d5e413ba7a4b878d8e0c2d92f83c2fc&amp;scene=21#wechat_redirect\">都在追“新潮”技术，但你有大厂们的动作快吗？</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651176990&amp;idx=1&amp;sn=326cb11bceec9c7d880f39512102b48d&amp;chksm=bdb8384d8acfb15b85c799fccb7c956bff6f35949480f3af75a1da57f959632800bc28a0b4e8&amp;scene=21#wechat_redirect\">大模型竞争突然升级！亚马逊 CEO 亲自监督、组建新的核心技术团队，集中优势资源打造“最具野心”的大语言模型</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651176847&amp;idx=1&amp;sn=2619fa8a1901711057ca0aa3239f418c&amp;chksm=bdb847dc8acfcecaa3192a0f1d87074f368ec133fe1a74c34a1a2f58edb2cdca6ab689f9b41f&amp;scene=21#wechat_redirect\">一场 AI 引发的开源革命迫在眉睫？Hugging Face 更改文本推理软件许可证，不再“开源”</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-08-07 16:23:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "京东零售全渠道生态部移动前端团队负责人、京东零售前端开发通道委员桂琴琴，确认担任QCon北京大前端新场景探索专题出品人",
    "url": "https://www.infoq.cn/article/FMBT74P9XbyCQMhSZLSZ",
    "summary": "<p>9 月 3 日 - 5 日，在 <a href=\"https://qcon.infoq.cn/202309/beijing?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0807&amp;utm_content=guiqinqin\">QCon 全球软件开发大会（北京站）</a>\"，京东零售全渠道生态部移动前端团队负责人、京东零售前端开发通道委员桂琴琴将担任「大前端新场景探索」的专题出品人。在此次专题中，你将了解到大前端新场景探索中的前沿技术，如 LLM（大语言模型）在大前端场景的落地、多终端场景探索和自渲染场景探索，以及前沿技术对大前端领域的发展做出的贡献。</p><p></p><p>桂琴琴，目前负责京东全渠道生态部移动前端团队，京东零售前端开发通道委员。十余年前端领域从业经历，在大前端协同、跨端融合、低代码等领域均有过深入实践，涵盖线上线下包括移动、桌面、POS、PDA 等各终端场景，也在探索大前端新场景下的创新应用和提效。</p><p></p><p>相信桂琴琴的到来，可以帮助提升此专题的质量，让你了解到 LLM（大语言模型）能探索如何将大型语言模型与前端开发相结合，以实现更加智能、高效、个性化的用户界面和交互体验；多终端场景探索，能探索如何针对不同的终端设备开发一致的用户体验；自渲染场景探索，能探索如何使用自渲染技术打造高性能、可扩展的前端应用，以及如何利用自渲染技术实现更加灵活、高效的前端开发，为大前端领域的创新和发展提供了更多的思路和借鉴。</p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-07 16:32:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "IPv4 开始收费！新的 IT 灾难？",
    "url": "https://www.infoq.cn/article/F1vZQzL0bVO3OzdrzYiV",
    "summary": "<p></p><p></p><p></p><blockquote>目前的情况实在糟糕。我们要么向云服务商付更多的钱，要么坐视自己的网络服务崩溃。</blockquote><p></p><p></p><p>最近一段时间，新闻头条总会时不时提起 IP 地址。AWS 宣布要对每个 IPv4 地址每小时收费 0.005 美元，其他云服务商也纷纷行动，宣称要对公共 IPv4 这种“奢侈品”开价。Google Cloud Platform 开出的价格是 0.004 美元，跟 Azure 和 Hetzner 的每小时 0.001 欧元基本相当。很明显，云服务商自己掏钱购买 IPv4 地址空间的时代即将结束。随着时间推移，当这些地址越来越有价值，想要免费使用自然也就不可能了。</p><p></p><p>面对这样的压力，我们当然应该赶快切换到 IPv6。其实我第一次听说 IPv6 这东西，是在高中的第一堂思科技术课上。而现在我已经 36 岁了，大家可以想象当时老师说的“即将到来”究竟来得有多慢。到目前为止，我在 IPv6 方面还没做过多少实际工作，毕竟市场对此类技能的需求实在不旺。而且从业经历也还算丰富的我可以证明，没有任何一家雇主会认真强调 IPv6 方面的技能需求。所以我压根就没学，只能说很遗憾错过了这场网络领域的巨大变革和进步。</p><p></p><p>种一棵树最好的时间是十年前，其次是现在。学习 IPv6 当然也是如此，所以我打算试着把自己的博客迁移到 IPv6。我会继续把它放在 CDN 后面来处理 IPv4 流量，但无论如何，这个重要的历史时刻终于来临了！但我惊恐地发现，关于 IPv6 的一切几乎都没法开箱即用。主要依赖项会立即停止运行，而解决方案也很难称得上“生产就绪”。即使是人员充足的技术团队，在处理 IPv6 迁移过程时也是异常艰难，毕竟几乎没人在这方面的实践经验。我们多年来都忽略了这个问题，而现在是时候付出代价了。</p><p></p><p></p><h2>有必要折腾 IPv6 吗？</h2><p></p><p></p><p>当然有，但受篇幅所限，本文就不具体论述 IPv4 和 IPv6 的优劣了。网上这类文章很多，大家酌情参考。下面，我简要介绍一下为什么要转向 IPv6。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/20/2029dca7bcf03c14fc69c48ef3e16e06.png\" /></p><p></p><p>IPv6 数据包标头示例</p><p></p><p>地址空间标头字段数量较少（仅占为 8 个，IPv4 上是 13 个）</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/1d/1d1e459bdfe81866e798d93f14dc5fa6.png\" /></p><p></p><p>处理速度更快：不再需要校验和，所以路由器不必对每个数据包进行重新计算。路由速度更快：包含更多汇总路由和分层路由。（看不懂？没关系，汇总路由 = 将多个 IP 组合起来，这样就不需要处理所有地址，而仅仅只要根据地址开头部分的大致方向前进就行。路由也是如此，毕竟 IPv6 是我们唯一能够使用的小型、高效主干路由。）QoS：Traffic Class 和 Flow Label 两个字段，让 QoS 更轻松。自动寻址。允许 IPv6 主机在局域网上无需路由器或 DHCP 服务器即可实现连接。可以使用 Authentication 标头和封装安全负载将 IPsec 添加至 IPv6。</p><p></p><p>最后，也是最重要的一点：IPv6 地址是免费的，但 IPv4 地址却要付钱。</p><p></p><p></p><h2>设置仅支持 IPv6 的服务器</h2><p></p><p></p><p>其实具体设置过程非常简单。我配置了一台 Debian 设备并选择了“IPv6”，接下来第一个“惊喜”就出现了。这台设备没能获得 IPv6 地址。我只得到了一条 /64 地址，也就是 18,446,744,073,709,551,616。但好消息是，我的小型 ARM 服务器可以通过扩展，在所有公共地址上运行我之前工作过的每家公司的网络基础设施。</p><p></p><p>这看似有点浪费，但理解了 IPv6 的工作原理之后，大家会发现事实并非如此。因为 IPv6 比 IPv4 多得多，所以就算在网络上设置 1 万台主机也没问题。所以哪怕乍看上去有点浪费，保留所有 IPv6 空间也有其实际意义。所以，用不着考虑有多少地址被发送给了每台设备。</p><p></p><p>重要提示：控制住自己优化地址利用率的冲动。在跟经验丰富的网络专家交流时，我发现很多人都容易掉进优化地址的坑里。我们都花过很多时间考虑 IPv4 地址块里还剩多少空间，并围绕这个问题搞设计。但在 IPv6 中这个问题根本就不存在，/64 前缀就是我们应在接口上配置的最小前缀。</p><p></p><p>如果非要使用较小的前缀（有人确实尝试过），比如用 /68 或者 /96，可能会破坏无状态地址的自动配置。在心态上，我们应当将各个站点配置为 /48，这也是地区互联网注册管理机构在分配 IPv6 时的默认设定。而在设计网络组织时，则应考虑半字节边界——这可不是胡说八道，简单来讲，这就是一种让 IPv6 更易于阅读的形式。</p><p></p><p>假设我们要使用 2402:9400:10::/48，如果希望每台设备只使用 /64 作为平面网络结构，则可按如下方式进行划分：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/74/74e0a237af4de8bc8603725a14166040.png\" /></p><p></p><p>而 /52 的情况也差不多。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a0/a08f8dca6ab69a4c2d3e3167c976c139.png\" /></p><p></p><p>这样，我们就能一目了然看到自己正在查看哪个子网。</p><p></p><p>好了，现在设备已经准备就绪，接下来做点常规服务器设置操作。</p><p></p><p></p><h4>问题 1：我没法通过 SSH 登录</h4><p></p><p></p><p>其实这个问题完全可以预见。我的工作或家庭 ISP 都不支持 IPv6，所以我才需要设置自己的服务器，但现在却完全不起作用。好吧，我只能附加一个 IPv4 地址，通过 SSH 登录，再设置 cloudflared 来运行隧道。想必系统会自行处理其中的转换工作。</p><p></p><p>但 Cloudflare 显然没考虑到这种用法。所以当我删除 IPv4 地址时，隧道意外崩溃了。默认情况下，cloudflared 实用程序使用的是 IPv4，我们需要单独编辑 systemd 服务文件以添加：-edge-ip-version 6。完成之后隧道才能正常启动，我也可以通过 SSH 顺利登录了。</p><p></p><p></p><h4>问题 2：没法使用 GitHub</h4><p></p><p></p><p>现在我的小服务器已经开始运行，接下来该做具体设置了。我运行了自己的服务器设置脚本，但立马就报错了。它会尝试访问 history 的安装脚本，这是款很棒的 shell 历史记录实用程序，我在自己的所有计算设备上都会用它。它尝试从 GitHub 提取安装文件，但失败了。“这不对吧，GitHub 难道不支持 IPv6 吗？”</p><p></p><p>确实不支持。这简直让人心态爆炸，支撑整个互联网的软件发布服务居然不能兼容 IPv6……但也难怪，现在微软的心思全都放在人工智能身上，哪有功夫管你什么 GitHub 和 IPv6。最终，我选择使用 TransIP GitHub 代理，效果很好。现在我可以正常访问 GitHub 了。但随后 Python 也报错，显示 urllib.error.URLError:。好吧，我选择放弃。我猜是 Debian 中的 Python 3 版本不兼容 IPv6，但我现在实在没那个心情做故障排查。</p><p></p><p></p><h4>问题 3：无法设置 Datadog</h4><p></p><p></p><p>接下来是一些基本操作。我想设置 Datadog 来监控这台服务器的运行，其实关注的指标并不多，只是一些历史负载数字。我转向 Datadog，登录并开始执行设置，但它立刻就崩溃了。我运行 curl -L <a href=\"https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh\">https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh</a>\" 来实现这个简单设置，而且人家 S3 已经支持 IPv6 了，这到底是哪出的错？</p><p></p><p><code lang=\"nginx\">curl -v https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh\n*   Trying [64:ff9b::34d9:8430]:443...\n*   Trying 52.216.133.245:443...\n* Immediate connect fail for 52.216.133.245: Network is unreachable\n*   Trying 54.231.138.48:443...\n* Immediate connect fail for 54.231.138.48: Network is unreachable\n*   Trying 52.217.96.222:443...\n* Immediate connect fail for 52.217.96.222: Network is unreachable\n*   Trying 52.216.152.62:443...\n* Immediate connect fail for 52.216.152.62: Network is unreachable\n*   Trying 54.231.229.16:443...\n* Immediate connect fail for 54.231.229.16: Network is unreachable\n*   Trying 52.216.210.200:443...\n* Immediate connect fail for 52.216.210.200: Network is unreachable\n*   Trying 52.217.89.94:443...\n* Immediate connect fail for 52.217.89.94: Network is unreachable\n*   Trying 52.216.205.173:443...\n* Immediate connect fail for 52.216.205.173: Network is unreachable\n</code></p><p></p><p>问题不在 S3 或者我的服务器上，因为我可以正常使用 AWS 提供的 S3 存储桶连接测试。</p><p></p><p><code lang=\"markdown\">curl -v  http://s3.dualstack.us-west-2.amazonaws.com/\n*   Trying [2600:1fa0:40bf:a809:345c:d3f8::]:80...\n* Connected to s3.dualstack.us-west-2.amazonaws.com (2600:1fa0:40bf:a809:345c:d3f8::) port 80 (#0)\n&gt; GET / HTTP/1.1\n&gt; Host: s3.dualstack.us-west-2.amazonaws.com\n&gt; User-Agent: curl/7.88.1\n&gt; Accept: */*\n&gt;\n&lt; HTTP/1.1 307 Temporary Redirect\n&lt; x-amz-id-2: r1WAG/NYpaggrPl3Oja4SG1CrcBZ+1RIpYKivAiIhiICtfwiItTgLfm6McPXXJpKWeM848YWvOQ=\n&lt; x-amz-request-id: BPCVA8T6SZMTB3EF\n&lt; Date: Tue, 01 Aug 2023 10:31:27 GMT\n&lt; Location: https://aws.amazon.com/s3/\n&lt; Server: AmazonS3\n&lt; Content-Length: 0\n&lt;\n* Connection #0 to host s3.dualstack.us-west-2.amazonaws.com left intact\n</code></p><p></p><p>好吧，我打算通过 apt 手动进行设置。</p><p></p><p><code lang=\"apache\">0% [Connecting to apt.datadoghq.com (18.66.192.22)]\n</code></p><p></p><p>他奶奶的……行，Datadog 你滚吧。到这个时候，我终于意识到纯使用 IPv6 根本没有前途。如果不上代理和技术补丁，那原本简单的几乎一切组件都会跟我闹别扭。所以接下来我会尽可能使用 IPv6，但同样辅以 IPv4。</p><p></p><p></p><h2>NAT64</h2><p></p><p></p><p>为了从 IPv6 访问 IPv4 资源，我们需要借助 NAT64 服务。我最终用的是 <a href=\"https://nat64.net/%E3%80%82%E6%BC%82%E4%BA%AE%EF%BC%81%E6%89%80%E6%9C%89%E9%97%AE%E9%A2%98%E5%87%A0%E4%B9%8E%E7%AB%8B%E5%88%BB%E6%B6%88%E5%A4%B1%EF%BC%8C%E6%88%91%E8%83%BD%E5%A4%9F%E6%AD%A3%E5%B8%B8%E8%AE%BF%E9%97%AE%E5%90%84%E7%A7%8D%E8%B5%84%E6%BA%90%E4%BA%86%E3%80%82%E6%88%91%E5%85%B6%E5%AE%9E%E6%9C%89%E7%82%B9%E6%8B%85%E5%BF%83%EF%BC%8C%E6%AF%95%E7%AB%9F%E8%BF%99%E5%8F%AA%E6%98%AF%E4%B8%AA%E4%B8%9A%E4%BD%99%E9%A1%B9%E7%9B%AE%EF%BC%8C%E9%9D%A0%E5%AE%83%E6%9D%A5%E6%8E%A5%E5%85%A5%E6%89%80%E6%9C%89%E5%85%B3%E9%94%AE%E4%BA%92%E8%81%94%E7%BD%91%E8%B5%84%E6%BA%90%E5%88%B0%E5%BA%95%E8%A1%8C%E4%B8%8D%E8%A1%8C%E5%95%8A%EF%BC%9F%E4%BD%86\">https://nat64.net/。漂亮！所有问题几乎立刻消失，我能够正常访问各种资源了。我其实有点担心，毕竟这只是个业余项目，靠它来接入所有关键互联网资源到底行不行啊？但</a>\" IPv6 的兼容性问题实在太多，可行的选项又相当有限，所以我只能祈祷 NAT64 别耍流氓了。</p><p></p><p>我甚至惊讶于目前能够选择的方案居然如此有限。以下就是我能找到的最佳工具列表：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e3/e31ed66df6b8b76b6b0cdef4d3a8ec9b.png\" /></p><p></p><p>其中大多数工具似乎已经失效了。Dresel 链接无法工作，Trex 在测试中出现了问题，August Internet 彻底消失，大多数 Go5lab 测试设备离线。Tuxis 倒是可以工作，但在 2019 年推出之后似乎就没升级过。基本上，Kasper Dupont 似乎是互联网上唯一愿意继续支持 IPv6 的技术人。抱拳了，Kasper～</p><p></p><p>更直白地讲，Kasper 相当于是在用一臂之力支撑整个 IPv6 互联网。</p><p></p><p></p><h3>Kasper Dupont</h3><p></p><p></p><p>到时这里，我对 Kasper 不禁心生好奇，并给他发邮件提了几个问题。这位老兄也热心给出了回复。</p><p></p><p>我：我发现公共 NAT64 服务非常有用，你为什么会一直坚持维护这个项目呢？</p><p></p><p>Kasper：我这么做，主要是想推动 Ipv6 继续发展。有那么几年，我打算在家里搭建纯 IPv6 网络，并发现 DNS64 加 NAT64 的实际表现相当出色。我想让更多人也能有机会用上。在发布第一个 NAT64 网关时，它只是推广 NAT64 扩展的概念验证项目。NAT64 就这样延续了下来，没有太多复杂的故事。</p><p></p><p>几个月前，我终于在家里用上了原生 IPv6。所以现在，我可以用更类似自己当初的方式帮助目标用户解决问题。</p><p></p><p>我：NAT64 几乎是互联网上仅存的少数此类免费公共服务之一。能不能聊聊你为什么坚持了下来，这服务的运行成本是多少，总之多跟我说说具体情况吧。</p><p></p><p>Kasper：这是款个人产品，所以我在不同的地方一共用到七家虚拟机托管服务商。有些是从 Hetzner 购买的，价格是每月 4.51 欧元：<a href=\"https://hetzner.cloud/?ref=fFum6YUDlpJz\">https://hetzner.cloud/?ref=fFum6YUDlpJz</a>\"&nbsp;其他虚拟机要贵一点，但贵得不多。</p><p></p><p>在这些虚拟机中，有 4 个用于 NAT64 服务，其他则用于别的 IPv6 过渡相关服务。比如我自己也在其中一个虚拟机上运行这项服务：<a href=\"http://v4-frontend.netiter.com/\">http://v4-frontend.netiter.com/</a>\"</p><p></p><p>我希望以后能跟托管商达成协议，提高服务容量并把它变成能赚钱的业务，这样我就可以全职从事 IPv6 相关工作了。但目前我仅仅是拿出点个人时间鼓捣。我的目标是让纯 IPv4 内容提供商以传输带宽成本的方式支付这部分费用。</p><p></p><p>我：有没有愿意分享的其他技术细节呢？</p><p></p><p>Kasper：当然可以，只是这些技术细节可能有点复杂。</p><p></p><p>我觉得自己这项服务跟其他服务的主要区别，在于我的每个 DNS64 服务器都会自动根据所有网关的健康检查通过 NAT64 前缀进行更新。也就是说，任何单一 NAT64 网关的中断，基本都不会对用户体验造成影响。这对维护也有帮助。在我看来，这可能是 NAT64 服务在同类公共服务中获得认可的核心原因。</p><p></p><p>NAT64 服务的代码也完全由我自己开发而成，目前作为 Linux 上用户模式的守护进程。我正在考虑把大部分性能关键部分移植成内核模块。</p><p></p><p></p><h2>我的网站</h2><p></p><p></p><p>好了，到这里我已经介绍完毕、网站也开始运行了。为了通过 IPv6 拉取 docker 容器，我们需要在镜像名称前添加 registry.ipv6.docker.com/library/ 。例如：image: mysql:8.0 会变成 image: registry.ipv6.docker.com/library/mysql:8.0</p><p></p><p>Docker 会警告说此设置尚未达到生产就绪状态。我不太确定 Docker 想提醒什么，毕竟如果它停止运行，拉取操作照常执行也就可以了吧？</p><p></p><p>完成之后，我把自己这个网站设置为 AAA DNS 记录，并允许 Cloudflare 进行代理。也就是说，它们会处理 IPv6 的广告并将流量引向这里。我还做了另一项修改：之前我使用的是 Caddy Web 服务器，但因为现在大部分流量都转移到了 Cloudflare 这边来，所以我决定改用 Nginx。毕竟当主要流量都来自 Cloudflare，那自然得相应调整 SSL 的工作方式。</p><p></p><p>现在，我已经把 Cloudflare 的原始证书硬加载到了 Nginx 当中，同时设置了经过身份验证的源拉取，这样就可以确定所有流量均通过 Cloudflare 运行。该证书的签名有效期为 15 年，所以我可以放心把它保留在自己的 secret 管理系统当中、不再分心打理。好了，现在我的网站已经重新上线并正常运行。是的，你眼前看到的这篇内容，就是系统能够顺畅工作的最好证明。</p><p></p><p></p><h3>尚未解决的问题</h3><p></p><p></p><p>我的容器仍无法与 IPv4 资源通信，就算位于具有 IPv6 网桥的 IPv6 网络上也不行。DNS64 解析能正常工作，而且我已经把 fixed-cidr-v6 添加到了 Docker 当中。我可以跟 IPv6 资源正常通信，但 NAT64 转换过程却不起作用。之后我会继续研究可行的办法。</p><p></p><p>有些朋友可能想 ping 一下试试，但我在添加 NAT 时用到了 ip6tables。</p><p></p><p>SMTP 服务器问题。我还没找到拥有 AAAA 记录的商用 SMTP 服务。Mailgun 和 SES 都不行，我试过的其他一些小服务也宣告失败。即使 Fastmail 也帮不上忙。</p><p></p><p></p><h2>为什么不继续使用 IPv4？</h2><p></p><p></p><p>这里咱们先不扯地址不够用之类的闲话。如果能早一点尝试推广 IPv6，那我们的网络基础设施建设方式可能将完全不同。企业之所以经常用到负载均衡器和隧道等技术，并不是真的有什么硬性功能需求，而是希望在私有 IP 范围和公共 IP 地址之间做出某种逻辑划分，并将其保留在 DNS A 记录当中。</p><p></p><p>如果具体把负载均衡器拆开来看，它其实只负责两项工作。首先将传入的数据包分发到后端服务器上，之后检查这些服务器的运行状态并将不健康的服务器从轮换中剔除。当然，现在很多人也在用负载均衡器处理 SSL 终止和指标等事务，但这些并不是负载均衡器设计上的必要功能。负载均衡有多种实现方式，最常见的包括：</p><p></p><p>连接请求轮询。不同服务器的加权轮询。最少连接原则，即连接数最少的服务器会收到更多请求。加权最少连接，跟上一条类似，但也可以设置更多倾向性。</p><p></p><p>大家应该注意到了，跟公共 IP 地址相比，私有 IP 地址完全用不上这些功能。从计算角度看，将主机配置为仅接受某一个来源（负载均衡器）的流量只是更简单也相对更便宜。但出于这个目标，我们被迫在基础设施层面做出大量设计，例如 VPC、NAT 网关、公共子网和私有子网等等，其实这些原本都没必要、至少不用像现在这么重要。</p><p></p><p>更讽刺的是，IP 白名单目前是种并不完善的安全实践。它最大的问题就是浪费时间，因为我们使用的都是云服务商所拥有的 IP 地址。但面向未来，IP 的名单的存在仍然意义重大，因为企业会随需求增加而自行购买 /44，包括从美国互联网号码注册机构（ARIN）、Réseaux IP Européens 欧洲网络协调中心（RIPE）或者亚太网络信息中心（APNIC）购买一组 IP。</p><p></p><p>这样，我们就永远不需要担心“谷歌会不会再多买点 IP 地址”或者“我得关注 GitHub 的支持页面，确保他们不会添加更多 IP 地址”。未来我们将拥有自己的 IP 地址块，在使用期限之内随意用于整个业务体系。容器系统不需要在每个主机上分配内部 IP 地址，因为有公共 IP 地址块可供使用，需要时还能通过标准公共 DNS 轻松发布通告。</p><p></p><p>当然，我不是说专用网络不好。我的意思是，我们采用的很多网络设计并非出于硬性需求，反而是被“绑架”的设计产物。我猜测未来的应用程序会更多对接开放互联网，而不再完全依赖于私有 VPC 的安全保障。考虑到安全漏洞的基本原理，这样的设计可能更有益于整体安全。</p><p></p><p>所以，即使大家根本不关心成本和可用性问题，那让组织更多掌握网络功能的所有权和控制权，也必将带来可以量化的巨大价值。</p><p></p><p></p><h2>这一切，会好起来吗？</h2><p></p><p></p><p>总之，目前的情况实在糟糕。我们要么向云服务商付更多的钱，要么坐视自己的网络服务崩溃。希望那些不想付钱的朋友们能继续推动 IPv6 的普及，但遗憾的是我们居然花了这么多年才达到如今的水平。所有这些问题本来可以逐步解决，但在掌握资源的团队做出实际行动之前，很多朋友心里恐怕还是有个大大的问号。</p><p></p><p>我真心希望最终结果能更好一些，至少应该给那些希望永久掌握自己 IP 范围的小公司一点机会。而且随着 IPv6 逐步成为主流，也希望它的使用门槛能变得更低一些。反正目前的现状就是两个字：糟糕，出奇的糟糕。</p><p></p><p>如果你经营着一家小公司，又不想额外花钱使用 IP 地址，那当下唯一的办法就是多留点时间：各位将有无数问题需要解决。</p><p></p><p>期待看到大家的意见 / 更正 / 驳斥~</p><p></p><p>原文链接：</p><p>https://matduggan.com/ipv6-is-a-disaster-and-its-our-fault/</p>",
    "publish_time": "2023-08-07 16:38:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]