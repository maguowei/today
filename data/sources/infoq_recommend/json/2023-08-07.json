[
  {
    "title": "自定义跟踪架构：Slack高效解决通知问题",
    "url": "https://www.infoq.cn/article/LAYRE8jxE2z42QuU5pNX",
    "summary": "<p>Slack<a href=\"https://slack.engineering/tracing-notifications/\">利用其自定义的跟踪架构</a>\"来协助排查通知发送问题。该跟踪架构的帮助下，他们解决通知问题的速度提高了30%，而且减少了将问题升级给开发团队的次数。该架构还简化了分析管道，并为数据科学团队解锁了新的应用场景。</p><p>&nbsp;</p><p>消息通知是Slack用户体验的关键组成部分。然而，由于通知流横跨Slack平台的许多组件，包括服务器端和客户端，所以要对客户体验团队收到的问题进行排查，有时候并不容易。开发团队经常不得不花费好几天的时间，查看多个具有不同日志记录后端、不同日志记录格式的系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d5ff117824b8288e4c74efab63091ecb.jpeg\" /></p><p></p><p>图片来源：<a href=\"https://slack.engineering/tracing-notifications/\">https://slack.engineering/tracing-notifications/</a>\"</p><p>&nbsp;</p><p>之前，Slack创建了<a href=\"https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/\">一个自定义的SlackTrace跟踪架构</a>\"，并使用它来跟踪日常的消息传递。他们用它跟踪了1%的客户端请求。接下来，该公司决定构建自己的跟踪解决方案，因为他们发现，没有一个现成的第三方解决方案能完全满足他们的需求。</p><p>&nbsp;</p><p>为了跟踪消息通知，团队识别出值得注意的事件并确定了属性映射，从而实现流和跟踪的映射。他们决定将通知跟踪与消息请求跟踪分开。这样，他们就可以支持通知流的100%采样，从而满足Slack客户体验团队的要求。</p><p>&nbsp;</p><p>通知跟踪改进了问题归类和调试。客户体验团队的成员自己就可以使用跟踪数据来了解出错的位置，不需要求助开发团队就可以解答客户的疑问。这个新功能也为iOS和Android工程师开始使用<a href=\"https://grafana.com/\">Grafana</a>\"来监控移动应用程序中的通知发送提供了帮助。最后，数据科学团队从跟踪数据中获得了洞察。他们通过漏斗分析来加深对通知打开率的理解，并利用历史通知跟踪数据来识别应用程序中的Bug和工具代码。</p><p>&nbsp;</p><p>Slack高级软件工程师<a href=\"https://www.linkedin.com/in/mansu/\">Suman Karumuri</a>\"将跟踪的好处总结如下：</p><p></p><blockquote>将产品分析数据建模为跟踪，可以在整个复杂的技术栈中以一致的数据格式提供高质量的数据。此外，内置的跟踪数据会话化免除了额外对跟踪数据进行去重和会话化的任务，简化了分析管道。</blockquote><p></p><p>&nbsp;</p><p>SlackTrace架构由一个Go Web服务器应用程序和一个Go消费者服务组成，前者负责向<a href=\"https://kafka.apache.org/\">Apache Kafka</a>\"发布跟踪span事件，后者负责将事件持久化到实时存储（<a href=\"https://www.elastic.co/\">ElasticSearch</a>\"）和数据仓库中。后端服务使用<a href=\"https://zipkin.io/\">Zipkin</a>\"和<a href=\"https://www.jaegertracing.io/\">Jaeger</a>\"工具库来报告span事件，并转换为内部span表示，而桌面和移动应用程序可以直接使用span API。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3c/3c0402a8f4fc0f647314593a7f6b32ef.jpeg\" /></p><p></p><p>图片来源：<a href=\"https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/\">https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/</a>\"</p><p>&nbsp;</p><p>Slack选用了一种比较简单的span表示，这使得他们的解决方案更加灵活，不用紧紧围绕请求和网络跟踪来开展。Span的结构简单，数据可以存储在单个表中，并且支持多种查询选项，工程师可以从中提取他们需要的数据来回答特定的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/slack-notification-tracing/\">https://www.infoq.com/news/2023/06/slack-notification-tracing/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/9lp0pYfij3bUxmHDXkxl\">Slack实时消息处理架构，更新、更快、更稳定</a>\"</p><p><a href=\"https://www.infoq.cn/article/hhh8OGLNbsz121H43Df4\">Slack工程师如何解决最常见的移动开发痛点</a>\"</p>",
    "publish_time": "2023-08-07 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "【实战篇】通过 SoFlu 构建一个低代码平台之简介",
    "url": "https://www.infoq.cn/article/FODhkbu3LbXZouV6f419",
    "summary": "<p>在现代企业开发中，初级开发者难以掌握开发企业级 Java 应用，中高级开发者则被繁琐任务所占据。为了让大家能快速上手可以解决这个难题的“ SoFlu 软件机器人”，前蚂蚁金服技术专家杨彪设计了《2 小时学会 10 个组件“取代”传统 Java 开发》课程，本节为该系列课程的第八讲。在经历了前 7 讲的干货输入，杨彪自本讲起为大家开始进行“ SoFlu 软件机器人”的实战展示——搭建一个低代码平台！</p>\n<p>大家都在说低代码平台，可是代码平台究竟有哪些核心能力呢？为什么 SoFlu 可以开发低代码平台？本讲便简单探讨业界低代码平台通常都有哪些核心功能，主要介绍使用 SoFlu 开发的低代码平台大概是什么样子，主要核心功能有哪些？</p>\n<p>大家在课后可以登录 Gitee 下载 SoFlu 软件机器人客户端进行实践：<a href=\"http://suo.im/8wROo\">点击下载</a></p>\n<p>大家可以扫码添加小助手，进学习群与专家一对一交流：<br />\n<img alt=\"\" src=\"https://static001.infoq.cn/resource/image/e8/c9/e8833a01ba0bc705acab14a572b57cc9.png\" /></p>",
    "publish_time": "2023-08-07 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全网首档操作系统探访体验栏目“龙蜥+超级探访”震撼上线！看国产 OS 如何乘风破浪",
    "url": "https://www.infoq.cn/article/hEEwclx5wCFgNHTfQO0o",
    "summary": "<p>如何联合开源技术与国际社区形成双向互动，消除 CentOS 策略变更带来的影响？“全网首档操作系统探访体验节目”《龙蜥+超级探访》重磅上线！统信软件既是实现开源和商业互补协作的成功典范，又是身先士卒引领伙伴投身开源的“大队长”，也是龙蜥+超级探访首期走进的企业。且看龙蜥社区联合行业内生态伙伴何实现从技术创新到商业变现的跨越</p>",
    "publish_time": "2023-08-07 10:04:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "传统风控难以为继，智能风控如何落地｜InfoQ 闭门会精选",
    "url": "https://www.infoq.cn/article/tXdg1xI1YWGYG6iGg4rj",
    "summary": "<p>金融行业的业务本质是通过经营“风险”实现盈利和发展，换言之，风险管理能力即是金融业的核心竞争力之一。</p><p></p><p>当然，风险管控的方式，会随着时代的发展不断迭代。如今，人工智能、大数据等技术趋于成熟，智能风控正在逐渐替代传统风控，成为金融机构识别风险、平衡创新与风控、提升风险管理能力的主要抓手。</p><p></p><p>那么，在新的时代背景下金融机构风险管控面临哪些具体挑战？智能风控在落地金融场景的过程中有哪些问题仍待解决？AIGC 技术在金融风控领域又有什么价值潜力？在日前 ArchSummit 全球架构师峰会·深圳站的“金融数字化高端闭门会”上，来自银行、证券、支付、跨境平台等金融行业不同领域的专家，围绕这一系列问题展开了深度探讨。</p><p></p><p>以下内容经 InfoQ 编辑、提炼和总结，希望为大家提供参考。</p><p></p><h2>金融风控面临哪些挑战？</h2><p></p><p></p><p>金融机构在经营过程中面临着一系列不同的风险类型，比如信用风险、市场风险、操作风险、欺诈风险、道德风险、声誉风险、法律风险、战略风险等等。一旦金融机构在交易活动中出现风险失控，轻则带来财务损失，重则威胁机构生存，甚至导致整个金融体系的系统性风险，引发次生危机。</p><p></p><p>其中，信用风险管理是金融机构风险管理最重要的一部分，在金融发展史上，大部分银行倒闭案例都是信用风险引起的。比如，银行卡“二三四要素”的验证，就是防范信用风险的手段之一。</p><p></p><p>近年来，外部经济和金融环境日益复杂多变，加上金融机构自身业务范围的延展，金融业面临的信用风险管控的挑战不断增多。比如，在互联网金融场景下，要求金融机构具备更强的防假反欺诈能力，想要获得更优质的客户源，获客成本变高；再比如，随着场景消费金融等新业态兴起，金融服务愈发无处不在，业务运行机制日益复杂，这意味着，风险也将日趋碎片化，管控难度和复杂度大大提升。</p><p></p><p>此外，市场风险也是影响金融业健康发展的关键。以年初轰动一时的美国硅谷银行倒闭事件为例，其就是市场风险管理失控的结果。作为与科技公司深度绑定的金融服务机构，硅谷银行的总资产规模超过了 2000 亿美元，但是，其中超过 50% 的资金被用于购买国债和房地产抵押贷款证券（MBS），流动性较差。随着 2022 年初美联储大幅大幅加息，一边债券和 MBS 等资产出现贬值，另一边银行需要支付的利息增加，获取资金等成本提升——于是，资产流动性风险暴露无遗，硅谷银行抛售资产引发挤兑，最终破产收场。</p><p></p><p>可见，让市场始终保持理智是一个难以完成的任务，这使得金融市场风险的预防和预测极为困难。而与此同时，在竞争日益激烈的市场环境下，金融机构还必须持续进行业务创新，并且确保客户体验——这与风险管控往往存在矛盾冲突——比如，为了确保信用风险可控，可能就要“牺牲”一部分客户体验；而对业务的管控过严，收益在一定程度上就会受到影响。</p><p></p><p>以跨境支付为例，核心要解决两大难题，一是拒付率过高，二是结算周期过长。为此，跨境支付机构就必须对商家资质等信息进行更为严格的审查。比如，国际卡组对跨境支付机构往往会设有严格的拒付率标准要求，如果拒付率高于一定水平，跨境支付机构就要承担高额罚款。为了降低拒付率，支付机构必须减少由于商家本身的产品问题造成的拒付风险。在这个过程中，如果卡口太严，就可能造成客户流失，如果卡口太松，就可能带来拒付率的提升。</p><p></p><p>因此，如何找到其中的“平衡点”，确保风险管控场景和标准的合理性，也是金融机构在风险管控过程中的一大痛点。</p><p></p><h2>智能风控解决了什么问题？</h2><p></p><p></p><p>面对这些挑战和难点，传统的风控手段变得难以为继。</p><p></p><p>最早，金融机构的风控管理主要依赖于人工，比如由人撰写规则，通过表格手工记录统计，由人工审批信用、资质等信息。以借贷场景为例，从调查、审查、审批，到贷款发放，贷中风险评估，贷后维护、管理以及贷款的回收等工作都像是一条人工作业流水线。</p><p></p><p>虽然经过信息化发展，大多数金融机构已经引入系统实现自动化审批，从而提高风控效率。但是传统风控更多考虑的仍然是信息间的强关联性，比如信用卡风险管理和评分主要关联卡号、IP 地址、征信等强变量，对客户的行为数据、消费数据、社交数据等弱变量的关联度较少。</p><p></p><p>如前所述，金融机构面临的风险点越来越多且分散，规则的制定越来越复杂，需要纳入评审分析的信息维度越来越多。在这一趋势下，普通信息系统的处理能力也开始显得捉襟见肘。</p><p></p><p>魔高一尺，道亦需高一丈。金融业从传统风控迈向大数据风控，而今，智能风控又应运而生，其背后，是技术的进化和演进。</p><p></p><p>智能风控以大数据平台为基础，以决策引擎为媒介，以智能模型为大脑，它能帮助金融机构做的，不再只是 0 或 1 的风险判断，还能进一步分析每笔交易申请的具体风险水平。这意味着，金融机构可以据此做更精细化的风险管理和定价，不必一棍子把一批客户拒之门外，有益于在业务拓展与风险管控之间寻找到那个平衡点，同时也能优化客户体验。</p><p></p><p>如果还以跨境支付为例，这意味着，支付机构可以在已有的数据基础上，更高效地获取更多的数据维度，比如更充足的渠道数据、商家数据等，从而对客户进行更精准的识别。</p><p></p><p>再比如，在向企业提供融资的场景下，金融机构可以利用大数据、AI 等技术自动实时抓取企业的财务、舆情、招聘、海关甚至是行业现状等公开信息，并交由 AI 辅助决策，判断是否进行风险预警。随着数据量大积累，还可以基于机器学习将各项预警信号与事后违约结果进行关联度分析，在复杂的信号中找出与该行业违约事件强相关的指标，从而不断优化信贷评估模型，提高审批的决策精度。</p><p></p><p>在这些场景下，技术所带来的，已经不仅仅是效率的提升，更重要的是价值的创造。</p><p></p><h2>智能风控落地顺利吗？</h2><p></p><p></p><p>不过，智能风控千好万好，在具体落地过程中，仍然存在不少的挑战和阻碍。</p><p></p><p>首先，大多数传统金融机构是有很多历史包袱的。</p><p></p><p>一方面是技术包袱。传统金融机构信息化系统间孤岛严重，数据难以打通共享，同时，大量历史系统和复杂冗余的技术架构很难在短时间内被替换和重塑——显然，这样的技术基础在支持海量数据的处理、分析和智能计算等方面的能力已经不足。因此，智能风控要彻底落地，必须还要“让子弹飞一会儿”，给技术升级一点时间；</p><p></p><p>另一方面是规则包袱。随着金融业务的快速发展，风险管控的规则一直在做加法，但是背后的依据和场景很难一一溯源，这导致系统不可控因素越来越多，可能影响业务快速发展。为了解决这个问题，需要对规则引擎进行升级，提高新的规则运行的可视化，同时加强日常管理。</p><p></p><p>其次，技术升级带来了另一个挑战，是人员技术能力的提升。</p><p></p><p>比如，传统金融机构技术人员掌握的能力主要面向的是集中式架构的开发、运维、管理，但现在，随着技术架构呈现开放化、分布式趋势，金融机构底层的数据库、中间件，包括开发工具、设计方法都要随之做出改变，而这，就对技术人员的能力象限提出了迭代的要求。于企业而言，既要对内部人员持续进行培训，同时也要具有从外部引入新的高端人才的计划、机制和能力。</p><p></p><p>再者，风控部门对智能技术的认可程度、使用意愿、资源投入度等等，也会直接影响智能风控模式的推行进度。</p><p></p><p>与所有数字化转型场景一样，智能风控的顺利落地绕不开相关业务人员。只有一线业务与技术人员在认知上达成一致，扫除自身的顾虑，并且迈出传统工作模式的“舒适区”，才能真正让技术工具为业务所用。否则，靠技术人员的“一厢情愿”注定阻力重重。</p><p></p><p>最后，还有一点不可忽略——技术和模型的适用性。</p><p></p><p>如何把机器学习中常用的模型应用到金融场景中，对于金融从业者的技能，以及企业自身的数据处理能力都提出了非常高的要求。一般来说，通用模型是相对抽象的，由于数据采样和训练不足，在面向具体行业或场景时（如个人信贷、企业融资、跨境支付或供应链金融）应用性也会出现不足。因此，在智能风控落地过程中，金融机构究竟需要什么样的模型，如何对它进行训练，要往什么方向持续迭代演进，参数如何调整等等...... 相关人员必须对此心中有数。</p><p></p><p>唯有以上障碍一一化解，智能风控的价值才能达到最大化。</p><p></p><h2>彩蛋：AIGC 在金融风控领域有什么可能性？</h2><p></p><p></p><p>一直以来，金融行业都是前沿技术应用的先行者，面对 AIGC 这波热浪，显然也不会例外。在闭门会上，大家对于 AIGC 在金融风控领域的可能性同样进行了一番探讨，以下是部分“脑暴碎片”，供参考：</p><p></p><p>利用 AIGC 帮助风控人员编写风控规则，简化流程，提高效率；通过 AIGC 对相关政策和监管文档进行解读，帮助风控人员更好、更快地理解；利用 AIGC 对客户资料（如拒付申诉资料）进行整理归档，提高效率；通过 AIGC 对原有的大数据 / 智能风控模型进行迭代升级，提升整体智能化水平。</p><p></p><p>总而言之，金融机构的风险管控讲求“看人、看表、看物”，其中的每一项工作，或许都值得用 AIGC 重做一遍。当然，在这个过程中，势必还会产生诸多衍生问题，比如“AI 幻觉”的问题如何解决，如何让 AI 分析过程更具有可解释性等等——对于金融行业而言，AIGC 的商业落地进程正在加速，但是鉴于严苛的安全合规要求，每一个细微的问题也都值得反复推敲和推演。&nbsp;</p>",
    "publish_time": "2023-08-07 11:16:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "汇丰科技：基于供应链金融贸易数据的信用风险实践解读",
    "url": "https://www.infoq.cn/article/5374J5rJxIgpXfbIL6Tx",
    "summary": "<p>综合营运资金融资方案为核心买家的供应商提供从装运前到装运后的全流程融资方案，令其可凭借经由核心买家确认的订单，向汇丰申请融资。与传统的商业贷款不同，该装运前融资依托供应链出运后融资，将融资节点提前至出运前即可起始。装运前融资计划与装运后供应链解决方案在系统和操作流程层面无缝联接，到期自动还款，极大地简化了整个流程。汇丰通过买卖双方的历史贸易数据将信贷决策数字化，并将对财务的依赖减少到几个关键检查点，该信用决策模型使供应商参与到信贷审批的准备时间从 1 个月以上大幅缩短至 2 周。本次分享便将为大家详细剖析整个技术实践过程。</p>\n<p><strong>演讲提纲：</strong></p>\n<ul>\n<li>应用业务场景- 综合营运资本融资方案</li>\n<li>基于历史供应链交易数据的信用决策模型-记分牌（含 AI 相关技术分享）</li>\n<li>信用决策架构</li>\n</ul>\n<p><strong>你将获得：</strong></p>\n<ul>\n<li>了解汇丰针对中小微供应商的贷款业务实施</li>\n<li>了解数字化授信决策流程</li>\n<li>了解基于贸易数据的信用决策模型</li>\n</ul>",
    "publish_time": "2023-08-07 11:23:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "汇丰科技：汇丰数据科学家平台的探索与实践",
    "url": "https://www.infoq.cn/article/mHCbYBSscyQ2Qs2KTEEj",
    "summary": "<p>作为全球规模最大的银行和金融服务机构之一，汇丰始终致力于实施国际化策略，为客户提供国际化的机遇和视野。这一策略推动了汇丰银行业务的持续发展，但同时也带来了合规方面的巨大挑战。</p>\n<p>在当前大数据、生成式 AI 以及混合云技术快速发展的背景下，作为数据平台部门的工程师，我们应如何在确保合规要求得到满足的前提下，让数据和人工智能更便捷、安全地被使用，以进一步支持汇丰银行的国际化策略呢？本议题将为大家进行分享。</p>\n<p><strong>演讲提纲：</strong></p>\n<p>背景</p>\n<ul>\n<li>混合云与数据监管</li>\n<li>生成式 AI 与模型合规</li>\n</ul>\n<p>属于汇丰的数据科学家工作平台</p>\n<ul>\n<li>汇丰数据科学家工作平台简介</li>\n<li>统一的数据访问监管流程</li>\n<li>基于 Kubernetes 的混合云全球部署架构</li>\n<li>基于数据虚拟化的多平台跨区域的数据访问</li>\n</ul>\n<p>数据科学家平台与生成式 AI 平台的集成</p>\n<ul>\n<li>数据科学家平台与生成式 AI 平台集成的设计与实践</li>\n<li>生成式AI的监管模型</li>\n</ul>\n<p><strong>你将获得：</strong></p>\n<ul>\n<li>参考与借鉴数据科学家平台在混合云环境下的设计与实践</li>\n<li>了解如何应用数据虚拟化解决跨区域多平台的实时数据访问</li>\n<li>了解金融机构对于生成式 AI 的思考及使用方式</li>\n</ul>",
    "publish_time": "2023-08-07 11:24:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "BERT能否被“平替”？ — 作业帮文本分类场景下的一次尝试",
    "url": "https://www.infoq.cn/article/iGXyEimwtMeKkC7z4Ggu",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/d6/d658cae590fdf021a0be37e84723e002.png\" /></p><p></p><p></p><h2>背景介绍</h2><p></p><p></p><p>近年来，在众多自然语言处理模型中最具有代表性的就是BERT，它以优异的性能赢得了广大算法工程师的青睐。但是，在有些生产环境中，BERT庞大的参数量不仅在推理时占用过多的计算资源，也不利于后续模型的扩展迭代。作业帮的业务体量较大，每天会生产大量的文本数据。这些数据均需要经过自然语言处理模型来生成业务可以直接使用的文本分类标签。在实际生产阶段，我们的场景具有如下特点：1.标签分了多期进行建设和产出，每期的标签在不同的场景有不同的阈值；2.每个时期的标签之间既存在独立性又存在依赖性；3.每个时期的有监督数据较少，一般的机器学习模型很难取得较好的线上效果，因此每一期的标签都是基于BERT进行fine-tune和部署。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3db2519487ee21ba3aac44b0350c3aed.jpeg\" /></p><p></p><p>目前我们的模型训练及部署流程如上图所示。结合作业帮的实际业务场景特点和使用方式，我们面临如下问题：&nbsp;&nbsp;&nbsp;1)每当新的任务需求提出后，都需要对BERT进行微调来满足。2)随着任务数量的增加，服务器上部署的BERT数量也会不断上升，导致占用较多的GPU计算资源，而且任务之间的调度也会变得更加复杂。因此，本文将以上述场景作为对象，探讨在研究平替BERT过程中的发现和结果，并对比它们的各项性能指标。最终目标是找到一个和BERT推理效果基本持平，但占用更少计算资源（特别是减少GPU计算资源），同时具有优秀扩展性的解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed2d5de8cd8f2ed6b3cf563de42f7093.png\" /></p><p></p><p></p><h2>可行的替换方案</h2><p></p><p></p><p></p><h4>2.1方案选择</h4><p></p><p></p><p>为了减少计算资源，将BERT进行轻量化处理是一个不错的选择。目前主要的轻量化方法如下：知识蒸馏：知识蒸馏是一种教师-学生（Teacher-Student）训练结构，通常是使用训练好的教师模型提供知识，学生模型通过蒸馏训练来获取教师模型的知识。它能够以轻微的性能损失为代价，将复杂教师模型中蕴含的知识迁移到相对简单的学生模型中。剪枝：在不改变模型结构的情况下，去掉模型中作用较小的连接，从而减小模型的维度。量化：将高精度的参数类型转换为精度较低的参数类型，从而减小模型的尺寸和计算时间消耗。我们希望新模型不仅可以尽量接近BERT的推理效果，而且还能够显著减少推理时计算资源的消耗。因此，新模型需要具有以下特征：1.模型参数量少。这样有利于进行快速迭代，且有较快的推理速度。2.在文本分类任务上表现良好。考虑到生产环境中积累了大量的基于BERT的标签数据以及平替模型的特点，我们选用的方案是：结合知识蒸馏中Teacher-Student方式和主动学习的思路，使用BERT对TextCNN的推理结果进行筛选，并根据TextCNN的输出loss和概率分布，通过数据增强的方式迭代TextCNN的训练，从而使TextCNN的推理效果逼近BERT。流程如下图所示：</p><p></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/9609056f1cad059965638952957b822d.jpeg\" /></p><p></p><p></p><h2>2.2&nbsp;训练数据</h2><p></p><p></p><p>作业帮的实际业务数据具有以下特征：a.在时间上呈现周期性；b.某些数据具有相同的表达范式；c.正负例样本之间存在较多的竞争性表述，如“表扬”相关的正例表述应该为“求表扬”，如“为什么我不在表扬榜上？”。而对于“谁在表扬榜”的表述则应该为负例；d.数据中的表情符号繁杂，具有某些特殊表情的数据可以代表某一类别；e.由于ASR或者OCR的识别精准度，会导致数据的质量较低。基于上面提到的特征，本文对数据进行了如下处理方式：1)拉取长时间范围内的数据，让时间范围尽量包括多个数据周期。2)为了保证训练数据的覆盖完整性，我们使用了如下采样方式：l多样性抽样：根据标签的类别，使用BERT的[CLS]符号对不同的标签数据进行聚类，得到每个标签下不同表述的种子数据，再使用种子数据进行多样性抽样。l随机抽样：对预处理后的语料进行随机抽样，保证训练数据的分布不会发生大的偏移。l不确定性抽样：根据BERT预测的结果，筛选出模型决策边界附近的数据。对于这部分数据，如果在时间和人力成本允许的情况下，可以进行人工标注，否则直接将这部分数据直接舍弃。3)去重语句中多余的符号：对表情符号做分类统计，筛选出一些特殊的范式作为语料的前处理，对无用语料进行过滤（如在我们的实际生产环境中，可以直接将满足这些范式的句子标记成负例）。4)训练集的划分：训练集和测试集的划分最好在时间上有一定的隔离，比如测试集选用时间范围距离较近的数据，训练集则选用时间范围较远的数据，这样既可以验证数据对周期性，也可以测试模型的泛化能力。</p><p></p><p></p><h2>2.3&nbsp;训练方式</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f4f9dbb8d472b7653704235554c58ce0.jpeg\" /></p><p></p><p>模型迭代训练的完整流程如上图所示：首先，训练基础的TextCNN。然后，根据决策指标评估TextCNN模型。如果模型满足指标需求，则在替换BERT前我们还需要采样近七天的数据测试TextCNN的泛化能力。若不满足指标需求，则需要分析TextCNN输出的loss以及标签得分，确定下一轮训练数据的采样策略来有目的性的富集训练数据。最后，迭代上述流程，迭代的轮次可以根据预设的指标设置。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/095370c2198233411349a37750c98818.png\" /></p><p></p><p></p><h2>效果评估</h2><p></p><p></p><h4>3.1评估指标</h4><p></p><p></p><p>评价指标主要基于测试集数据，以BERT推理结果为金标准，对比TextCNN和BERT之间的差距，具体指标如下：</p><p></p><p>a.精准率：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/adfd5c47d1df2421da2ab30105d757e9.png\" /></p><p></p><p>b.召回率：&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a99837790c3d4746c233b6eb30500ca6.png\" /></p><p></p><p>c.F1-score：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee7521cd6146a89a3d2aee6d9ed832dc.png\" /></p><p></p><p>d.推理速度：</p><p></p><p>主要评估TextCNN在CPU环境下的运行速度。其中，对于Preicison、Recall的指标，均以BERT推理结果为基准计算。3.2效果评估根据训练流程，我们可以观察某类标签在迭代三轮情况下，其指标的相关变化，如下表所示：</p><p></p><p></p><p></p><p>首先，在第一轮迭代时，使用和训练BERT一致的语料训练TextCNN，其相关指标都比较低。我们对loss排序后筛选出来低分loss发现TextCNN容易陷入到某些关键词的表述中。如我们需要识别“求表扬”的数据，在结果数据中我们找到两个比较典型的例子：Ø“谁在表扬单上呢”&nbsp;vs.&nbsp;“表扬单上没有我呢”对于两个句子，我们真正要筛选出后者的表达方式。那么，前者是典型的竞争性负例。在实验过程中产生的更多句子中，我们发现TextCNN由于竞争性负例，出现了严重的过拟合现象。进一步地，本着将TextCNN逼近BERT的目标，我们对抽样好的数据分别使用训练好的TextCNN和BERT进行打分，然后对比两个模型的得分结果，并进行如下操作：1.对于一个样本，若BERT分数高于某阈值而TextCNN分数低于某阈值，那么就认为该样本为竞争性负例；2.将在BERT决策边界附近的样本直接舍弃（在条件允许的情况下，这部分数据可以在进行人工重新标注后再放回训练集中）在经过上述步骤之后，我们进行第二轮迭代。从指标结果上看，该步骤可以有效的提升TextCNN的精确率。由于模型的召回率还比较低。比如对于“求表扬”标签，我们对BERT的预测该类别[CLS]向量进行简单的聚类分析发现，这类标签的表述有：Ø“为什么我没上榜”Ø“为什么我不在表扬榜”Ø“为什么我没有小红花”Ø“怎么不夸我呢”Ø“表扬时，怎么没有念到我的名字”同样的，对比TextCNN的聚类结果，我们发现召回的数据大部分集中在某一两类的表述上。基于此，我们进行了第三轮迭代：使用[CLS]向量对训练数据进行了召回，发现确实存在表述不平衡的情况。因此，针对低召回的问题，我们使用多样性抽样的方式对不同的标签的进行了数据增强。实验结果表明可以有效的提升模型的召回率。</p><p></p><h4>3.3&nbsp;总结</h4><p></p><p></p><p>总体流程可以分为训练TextCNN基础模型、提升TextCNN的精确率以及提升TextCNN的召回率三个主要部分。在每个流程部分中根据loss和模型输出的概率分布，针对不同问题采样不同的数据。在满足指标设定的前提下，让TextCNN逼近BERT的推理结果。在迭代过程中，我们舍弃了BERT决策边界附近的样本。从实际应用的角度考虑，这些数据基本也不会漏到下游的应用中，因此舍弃这部分样本也是合理的。在生产环境中还需要考虑一个问题是TextCNN输出概率阈值的设定。针对这个问题，我们拉取了不同日期的历史数据，在预设指标的情况下，动态的搜索了合理的阈值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4b947f3f89c67a9997ae72aa43796889.png\" /></p><p></p><p></p><h2>模型部署</h2><p></p><p></p><p>由于生产环境中涉及的模型较多，因此需要考虑模型同时推理时带来的计算资源的负载均衡问题。我们使用了任务队列的方式部署模型：首先，根据不同模型的优先级配置不同任务的先后顺序；然后，对不同任务数量的运行进行压力测试，设置合理的任务并发数量。具体部署如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d7854ae9527c20b8d2b32339ab28cd1.jpeg\" /></p><p></p><p>TextCNN部署上线后，同一分类任务在不同的配置下，推理速度(服务平均吞吐能力)对比如下表所示。</p><p></p><p></p><p></p><p>（备注：本方案因为是离线处理模式，因此忽略单条数据处理时延指标，仅考虑整体吞吐能力）我们基于400k~800k的数据量，对比了如下几种部署方式的吞吐能力：lBERT@GPU：GPU(2080&nbsp;Ti)单卡lBERT@CPU：1核&nbsp;CPUlTextCNN@CPU：1核、5核、20核从表中对比数据可以看出，TextCNN@CPU的吞吐能力很强大。5核CPU配置就可以接近4.8倍的BERT@GPU（单卡）吞吐能力。而且在实际场景中，CPU资源往往更廉价更容易扩容，可以轻松大幅缩短整体离线任务的处理时间。我们一个实际的线上任务，之前是BERT@2GPU，需要耗时2个小时完成，平替为TextCNN@20CPU方案后，只需要26分钟左右即可完成。大幅提前了数据产出时间，为下游应用争取到了更多的时间窗口。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c80f410816a24fb7ec8e588adc644587.png\" /></p><p></p><p></p><h2>技术总结展望</h2><p></p><p></p><p>本文涉及的场景具有以下三个主要的特征：1.有大量的历史数据积累，有利于采用不同的采样策略来针对性的增强训练数据；2.部署的BERT模型数量多，不利于后续标签的更新和维护。同时占用的GPU资源较多，不利于算力成本的维护；3.允许平替模型在指标上有较低的损失。因此，从场景特征出发，本文借鉴蒸馏中Teacher-Student的思想以及主动学习的方法，探索了TextCNN替换BERT的效果。经过几轮迭代后，平替版的TextCNN可以满足业务需求，释放了宝贵的GPU资源，处理时间大幅缩短，而且比较利于后续进一步扩展更多的文本分类任务。</p>",
    "publish_time": "2023-08-07 14:34:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "主动学习技术在作业帮业务场景中的创新与实践",
    "url": "https://www.infoq.cn/article/UI6Kq8ti4XgW9blbAjJp",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/d6/d658cae590fdf021a0be37e84723e002.png\" /></p><p></p><p></p><h2>业务背景</h2><p></p><p></p><p>虽然目前基于&nbsp;Transformers&nbsp;架构的自然语言处理模型在各类工业场景落地过程中都表现出了优异的成绩，但其背后所需要的标注数据依然是日常工作中的重点，也是费时费力的瓶颈所在。怎样在我们数据标注过程中降低成本，便成了我们必须面对的问题。主动学习(Active&nbsp;Learning,&nbsp;AL)&nbsp;便是我们尝试提高标注效率，降低整体标注和训练成本的重要技术手段。</p><p></p><p>主动学习，一种机器学习方法，是通过一定抽样策略找出对现有模型最有价值的样本数据，经人工标注后加入训练集，再次训练模型并继续以上步骤迭代的流程。简而言之，通过某种挑选方式，减少整体所需的标注数据量，尽快地接近业务需求的标准。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed2d5de8cd8f2ed6b3cf563de42f7093.png\" /></p><p></p><p></p><h2>解决方案</h2><p></p><p></p><p></p><h2>2.1&nbsp;常见考量维度：不确定性和多样性</h2><p></p><p></p><p>考虑到我们的业务场景，我们这里主要介绍pool-based的主动学习方法。在该场景中，我们在一个现有的数据池中通过挑选策略挑选新一批的无标签样本，交给标注人员(oracle)进行标注。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90a493a3b4e6d451ab42a852250ecd5a.png\" /></p><p></p><p>From&nbsp;(Active&nbsp;Learning&nbsp;Literature&nbsp;Survey[5])</p><p></p><p>主动学习的核心便是挑选策略(查询策略)，在&nbsp;pool-based&nbsp;下常见的两个策略是不确定性采样和多样性采样。</p><p></p><p>不确定性采样--What&nbsp;Model&nbsp;knows&nbsp;it&nbsp;doesn't&nbsp;know:</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d006fb16cfeac0551bb57adcd8c01ee.png\" /></p><p></p><p>From&nbsp;(Active&nbsp;Learning&nbsp;Literature&nbsp;Survey[5])</p><p></p><p>一般地，把模型输出概率作为不确定性的衡量依据。有以下三种常见方式:least&nbsp;confident、smallest-margin、entropy。</p><p></p><p>多样性采样&nbsp;--What&nbsp;model&nbsp;doesn't&nbsp;know&nbsp;that&nbsp;it&nbsp;doesn't&nbsp;know&nbsp;or&nbsp;the&nbsp;\"unknown&nbsp;unknows\":</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/9302f9dddd0e58ca05698fd4ef336194.png\" /></p><p></p><p>From&nbsp;Human-in-the-Loop&nbsp;Machine&nbsp;Learning[4]</p><p></p><p>如上图所示，如果使用随机抽样，数据点大概率是从中间最大的聚类簇中获取，通过先聚类，后在聚类簇中抽样，能更大可能性地保证采样数据点的多样性。</p><p></p><p>通常是抽取尽量与整体分布相似的子集，比如采用聚类簇的方式挑选最具代表性的数据子集。</p><p></p><h2>2.2&nbsp;既要又要，可否？</h2><p></p><p></p><p>不确定性和多样性，代表了一个机器学习模型当前的两个重要盲区。而经典的主动学习数据抽取策略，如前文所述的方法，往往只能顾及到一端。那么，这个问题是鱼和熊掌不可兼得的吗？其实并不然。近年来，研究者提出了一些创新性的融合技术方案。一个主流的思路是先把不确定性衡量表达为某种可计算表示(representation)，然后基于这种表示进行聚类计算，最后通过纳入更丰富的簇中心点来达到多样性。</p><p></p><p>其中聚类计算可以采用很多经典的方法，比如k-means等。而不确定性的可计算表示(uncertainty&nbsp;representation)是最为关键的。非常有启发性的两个不确定性表示技术是BADGE和ALPS。</p><p></p><p>ALPS&nbsp;[8]：利用了掩码语言模型的损失函数，MLM&nbsp;loss&nbsp;的直接输出当作不确定性的表达</p><p></p><p>其中BADGE很巧妙地把梯度作为了不确定性的表示，由此启发了很多后续的工作。而ALPS把预训练模型常用的掩码语言模型损失作为不确定性表达，能较好地解决冷启动的问题。</p><p></p><h3>2.3&nbsp;他山之石可以攻玉：通过虚拟对抗扰动来表示不确定性</h3><p></p><p></p><p>作业帮的很多业务场景，也需要同时考虑不确定性和多样性，因此我们在实践中也延续了这一思路。</p><p>BADGE和ALPS是两个很好的不确定性表示的方案，但不一定符合实践中的一些需求。比如，在很多我们面对的实际场景中，需要模型的鲁棒性比较强，也就是对噪音样本有较好的处理能力。因此，我们需要设计另外的不确定性表示方法。</p><p></p><p>我们看到，在很多图像处理场景中，会采用虚拟对抗扰动(Virtual&nbsp;Adversarial&nbsp;Perturbation)[6]来增加模型的鲁棒性和泛化能力。于是，我们将其借鉴过来，提出了一个基于虚拟对抗扰动的不确定性表示。具体地，通过针对BERT&nbsp;&nbsp;的隐层表达创建其一一对应的虚拟对抗扰动(VAP)来作为样本数据的不确定性表示。相应地，提出了一个新的主动学习方案VAPAL(Virtual&nbsp;Adversarial&nbsp;Perturbation&nbsp;based&nbsp;Active&nbsp;Learning)。在VAPAL中，我们依然是遵循了不确定性学习+聚类的主体思路。</p><p></p><p>VAPAL&nbsp;算法流程&nbsp;[9]:</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d5046285932ff4c71c41e3e2e4089380.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/095370c2198233411349a37750c98818.png\" /></p><p></p><h2>实验验证&nbsp;</h2><p></p><p></p><p>我们在英文数据&nbsp;PUBMED&nbsp;和&nbsp;SST-2&nbsp;验证了该方案的有效性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/46ce5bc70f68aa65039536b8405571fb.png\" /></p><p></p><p>From&nbsp;[9]</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c2b250fe2aaaac0f5658188eceda655c.png\" /></p><p></p><p>From&nbsp;[9]</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/bef90c6307dfae5355068bb4d69eefeb.png\" /></p><p></p><p>From&nbsp;[9]</p><p></p><p>实验表明：</p><p></p><p>在公开数据集上，和现在的主流的BADGE以及ALPS算法相比，VAPAL取得了可比性能，是一个强有竞争力的主动学习策略候选。</p><p></p><p>同时实验数据表明，初始阶段VAPAL表现更优，意味着在极端标注资源受限的情况下，VAPAL更胜任。</p><p>VAPAL相关算法已经集成在作业帮内部自研的人机协同标注平台中。</p><p></p><p>下图是一个实际标注项目的汇总信息。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/63/63d478b5bbf9c9f7786f804d1105d2ce.png\" /></p><p></p><p>从图中可以看出，整个人机协同的标注迭代流程中，通过主动学习抽取的样本，经人工标注后，补充到训练数据后，训出的新模型能更快地符合标注复核人员的要求，从而完成对很多类别的自动扩标，极大地提升标注效率(本例中是10+倍的效率提升)。</p><p></p><p>下图是展示了一个同类型同规模任务，分别采用纯人工标注和人机协同主动学习快速标注的项目效果对比。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a4f94fbe11626c562a07a1152ff4d4f.png\" /></p><p></p><p>从图中可以看出：</p><p></p><p>标注量方面，协同方案提升约9倍产出专题方面，协同方案有助于标注更加精细:&nbsp;7个专题到58个专题时间方面：协同方案提效6倍以上机器标注准确率方面：采用抽样人工再确认，准确率为93.8%</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4b947f3f89c67a9997ae72aa43796889.png\" /></p><p></p><h2>总结</h2><p></p><p></p><p>我们从业务需求出发，通过引入主动学习提高了业务的数据标注效率。引入虚拟对抗扰动(VAP)，提出了一个和当前最优技术方案可比甚至在初期表现更优秀的新改进方案&nbsp;VAPAL。在实际使用中，我们发现了现有抽选策略(除了随机之外)都对随机种子有着相对的敏感性，后期我们将在这个方向上更进一步的优化。关于VAPAL的详细介绍，可以参见文献[9]。</p><p></p><p>参考文献:</p><p>1.Scheffer,&nbsp;T.,&nbsp;Decomain,&nbsp;C.,&nbsp;&amp;&nbsp;Wrobel,&nbsp;S.&nbsp;(2001).&nbsp;Active&nbsp;hidden&nbsp;markov&nbsp;models&nbsp;for&nbsp;information&nbsp;extraction.&nbsp;Lecture&nbsp;Notes&nbsp;in&nbsp;Computer&nbsp;Science&nbsp;(Including&nbsp;Subseries&nbsp;Lecture&nbsp;Notes&nbsp;in&nbsp;Artificial&nbsp;Intelligence&nbsp;and&nbsp;Lecture&nbsp;Notes&nbsp;in&nbsp;Bioinformatics),&nbsp;2189,&nbsp;309–318.&nbsp;https://doi.org/10.1007/3-540-44816-0_31</p><p></p><p>2.Ido,&nbsp;D.,&nbsp;&amp;&nbsp;Sean&nbsp;P,&nbsp;E.&nbsp;(1995).&nbsp;Committee-Based&nbsp;Sampling&nbsp;For&nbsp;Training&nbsp;Probabilistic&nbsp;Classi&nbsp;ers.&nbsp;MACHINE&nbsp;LEARNING-INTERNATIONAL&nbsp;WORKSHOP&nbsp;THEN&nbsp;CONFERENCE,&nbsp;150–157.&nbsp;https://doi.org/10.5555/3091622.3091641</p><p></p><p>3.Culotta,&nbsp;A.,&nbsp;&amp;&nbsp;McCallum,&nbsp;A.&nbsp;(2005).&nbsp;Reducing&nbsp;labeling&nbsp;effort&nbsp;for&nbsp;structured&nbsp;prediction&nbsp;tasks.&nbsp;Proceedings&nbsp;of&nbsp;the&nbsp;National&nbsp;Conference&nbsp;on&nbsp;Artificial&nbsp;Intelligence,&nbsp;2,&nbsp;746–751.</p><p></p><p>4.Sampling,&nbsp;C.&nbsp;U.,&nbsp;Sampling,&nbsp;D.,&nbsp;Regression,&nbsp;L.,&nbsp;&amp;&nbsp;Trees,&nbsp;D.&nbsp;(n.d.).&nbsp;Monarch&nbsp;-&nbsp;Human-in-the-Loop&nbsp;Machine&nbsp;Learning_&nbsp;Active&nbsp;learning&nbsp;and&nbsp;annotation&nbsp;for&nbsp;human-centered&nbsp;AI-Manning.</p><p></p><p>5.Settles,&nbsp;B.&nbsp;(2009).&nbsp;Active&nbsp;Learning&nbsp;Literature&nbsp;Survey,&nbsp;(January).</p><p></p><p>6.Miyato,&nbsp;T.,&nbsp;Maeda,&nbsp;S.&nbsp;I.,&nbsp;Koyama,&nbsp;M.,&nbsp;&amp;&nbsp;Ishii,&nbsp;S.&nbsp;(2019).&nbsp;Virtual&nbsp;Adversarial&nbsp;Training:&nbsp;A&nbsp;Regularization&nbsp;Method&nbsp;for&nbsp;Supervised&nbsp;and&nbsp;Semi-Supervised&nbsp;Learning.&nbsp;IEEE&nbsp;Transactions&nbsp;on&nbsp;Pattern&nbsp;Analysis&nbsp;and&nbsp;Machine&nbsp;Intelligence,&nbsp;41(8),&nbsp;1979–1993.&nbsp;https://doi.org/10.1109/TPAMI.2018.2858821</p><p></p><p>7.Ash,&nbsp;J.&nbsp;T.,&nbsp;Zhang,&nbsp;C.,&nbsp;Krishnamurthy,&nbsp;A.,&nbsp;Langford,&nbsp;J.,&nbsp;&amp;&nbsp;Agarwal,&nbsp;A.&nbsp;(2019).&nbsp;Deep&nbsp;Batch&nbsp;Active&nbsp;Learning&nbsp;by&nbsp;Diverse,&nbsp;Uncertain&nbsp;Gradient&nbsp;Lower&nbsp;Bounds.&nbsp;Retrieved&nbsp;from&nbsp;http://arxiv.org/abs/1906.03671</p><p></p><p>8.Yuan,&nbsp;M.,&nbsp;Lin,&nbsp;H.-T.,&nbsp;&amp;&nbsp;Boyd-Graber,&nbsp;J.&nbsp;(2020).&nbsp;Cold-start&nbsp;Active&nbsp;Learning&nbsp;through&nbsp;Self-supervised&nbsp;Language&nbsp;Modeling,&nbsp;7935–7948.&nbsp;https://doi.org/10.18653/v1/2020.emnlp-main.637</p><p></p><p>9.Zhang,&nbsp;H.,&nbsp;Zhang,&nbsp;Z.,&nbsp;Jiang,&nbsp;H.,&nbsp;&amp;&nbsp;Song,&nbsp;Y.&nbsp;(2022).&nbsp;Uncertainty&nbsp;Sentence&nbsp;Sampling&nbsp;by&nbsp;Virtual&nbsp;Adversarial&nbsp;Perturbation.&nbsp;Retrieved&nbsp;from&nbsp;http://arxiv.org/abs/2210.14576</p>",
    "publish_time": "2023-08-07 14:34:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "复杂场景下的深度学习模型部署",
    "url": "https://www.infoq.cn/article/Uf6zoaszciVzX2BFXo2h",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p>深度学习是一种使用<a href=\"https://xie.infoq.cn/article/874e0d574d2d098a908f6429d\">神经网络</a>\"的机器学习，正迅速成为解决从文本分类到推荐系统等许多不同问题的有效工具。然而，将训练好的神经网络模型部署到应用程序和服务中可能给算法从业人员带来不小的挑战。算法框架的不同、计算资源稀缺和缺乏标准实现等挑战都可能导致模型部署的失败。</p><p></p><p>作业帮的业务体量较大，需要部署大量的<a href=\"https://xie.infoq.cn/article/5edc0dd7bf64eed32d8a0dd28\">深度学习</a>\"模型应对不同的任务。为满足实际使用，我们部署的模型服务需要具有以下特点：</p><p></p><p>高并发低时延模型多更新快业务方多</p><p></p><p>结合任务特点与实际应用场景，我们还面临以下问题：</p><p></p><p>数据预处理模块与模型版本强耦合，部署时需保持同步。多个版本的预处理模块相互之间易冲突。模型太多，如果全部加载至GPU浪费资源。</p><p></p><p>因此，本文将以上述场景作为对象，阐述三种不同的解决方案，并对比它们的性能指标。最终目标是找到一个适合作业帮任务的模型部署方案，在合理占用资源的情况下，满足不同业务方的需求。</p><p></p><h2>模型部署方案</h2><p></p><p></p><h4>1.&nbsp;Gunicorn&nbsp;+&nbsp;Flask&nbsp;+&nbsp;Transformers</h4><p></p><p></p><p>Transformers是一个十分方便的自然语言处理库，它的宗旨是让最先进的NLP技术人人易用。通过Transformers可以轻松地实现模型的训练、加载和推理。此外，使用Flask框架部署深度学习模型是最简单快捷的方式。但是Flask自带的Web服务性能极不稳定，无法满足高并发、低延迟的需求。因此，我们通过Gunicorn框架提供的WSGI服务来部署Flask的接口，也就是将Flask代码部署在Gunicorn提供的WSGI服务上，可以显著提升服务的并发能力和稳定性。</p><p></p><p>本部署方式的整体框架如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/761687dcaa2d68dc24650621dc3a8788.png\" /></p><p></p><p>利用Transformers加载所需模型，但通过LRU机制控制加载在内存中的模型数量最大为8，防止内存多大。通过Flask&nbsp;Web框架构建模型预测服务，包括数据的预处理、模型预测和结果后处理等步骤。最后用Gunicorn启动Flask服务，设置worker数量支持异步处理请求，提高并发能力。</p><p></p><p>Flask服务的构建十分简单，具体如下图所示。实现一个模型预测函数，并将其与’predict’接口绑定即可。另外，需注意的是线上服务需要实现’ready’接口返回200和’ok’以执行健康检查。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d12168a76e800a31d0defe4ebebef2c5.jpeg\" /></p><p></p><p>采用Gunicorn+Flask+Transformers部署深度学习服务的初衷是，该方式与我们的训练框架较为耦合，实现起来比较简单。但在使用线上服务的实际场景中，遇到了以下问题：</p><p></p><p>在每个worker中，执行模型的预测会有100+线程，导致资源的挤兑。Gunicorn确实提高了Flask服务的并发能力，但在并发请求较多时，并不能解决健康检查超时的问题，使得pod摘流并增加预测时延。Transformers模型推理方式的时延较长。LRU机制的存在使得模型需要重新加载，而模型加载比较慢导致预测时延增加。</p><p></p><p>最后，这种通过这种部署方式部署至serverless，共6个pod，每个pod的资源是16核16G的情况下，能达到150QPS。</p><p></p><h4>2.&nbsp;Tornado&nbsp;+&nbsp;PyTorch</h4><p></p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;Tornado是使用Python编写的一个强大的、可扩展的Web服务器。它在处理严峻的网络流量时表现得足够强健，在创建和编写时又足够轻量，并能够被用在大量的应用和工具中。Tornado在设计之初就考虑到了性能因素，支持异步非阻塞，可以接受多个用户的并发请求。本方案采用Tornado代替Flask以实现更好的异步通信。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;本部署方案直接采用<a href=\"https://xie.infoq.cn/article/ecb68618b571df7327e43bb8f\">PyTorch</a>\"模型预测的方式代替Transformers，避免Transformers内Trainer的冗余操作以减少预测时间。此外，将每个模型分开部署至子服务，取消模型的动态加载机制，避免模型切换造成的耗时。同时，这种方式也避免了模型过多导致的资源挤占问题。</p><p></p><p>本部署方式的整体框架如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e2ae6246c92664bb28e58cb43d3a49c.png\" /></p><p></p><p>每个模型通过Tornado包装成子服务，包含数据预处理，模型预测和结果后处理。分发服务负责接收所有请求，并根据数据类别将其分发至对应的模型预测服务进行预测。子服务的预测结果返回至分发服务，由分发服务返回所有数据的预测结果。Tornado部署Web服务同样十分便捷，同时可通过’async’和’await’关键字轻松实现异步。</p><p></p><p>采用Tornado+PyTorch的方案部署模型的原因是，组里同事有Tornado的踩坑经验，方便实现性能上的优化（如异步）。此外，本方式仍与训练框架有联系，从训练到部署的链路仍比较完整。然而在上线测试后，发现本部署方式仍有以下问题：</p><p></p><p>PyTorch预测时启动大量线程的问题仍未解决。模型多的情况下，需要申请大量模块并部署，管理太繁杂。没有动态Batching功能，无法更好地发挥机器性能。</p><p></p><h4>3.&nbsp;单模型性能测试</h4><p></p><p></p><p>为更方便地进行性能测试，我们采用单个模型进行性能的测试。通过不同部署方式在单模型上的性能进行对比，以挑选最优的部署方案。本节中，我们采用了ONNX和TorchScript两种模型格式，并分别用Tornado和Triton&nbsp;Inference&nbsp;Server进行部署。</p><p></p><p>ONNX&nbsp;(Open&nbsp;Neural&nbsp;Network&nbsp;Exchange)是一种针对机器学习所设计的开放式文件格式，用于存储训练好的模型。它使得不同的人工智能框架可以采用相同格式存储模型数据并交互。对于我们的场景，其优势在于可以控制线程数防止资源挤占。</p><p></p><p>Triton是英伟达等公司推出的开源推理框架，为用户提供在深度学习模型部署的解决方案。其主要职责和服务紧密相关，服务中常见的需求需要它做处理。比如Batching，Sequence，Pipeline和模型仓库的管理等</p><p>我们将各个服务部署在Docker中，限定CPU核数为16来模拟线上模块的资源。通过Locust工具进行压测，具体结果如下表所示：</p><p></p><p></p><p></p><p>在不考虑预处理的情况下，Triton&nbsp;+&nbsp;TorchScript的部署方式是最优的。Triton的动态Batching功能极大地提升了并行预测的功能。但是在我们的场景中，数据预处理与模型预测强耦合。于是，尝试利用  Python&nbsp;Backend将预处理部分同样集成在Triton中，实现完整的模型预测服务。但在实际测试过程中，发现集成预处理的Triton服务效果并不理想，怀疑是预处理的串行使得模型预测时无法进行动态Batching。因此，我们最终决定将部署方式确定为Tornado&nbsp;Web服务和Triton&nbsp;Inference&nbsp;Server。</p><p></p><p>本节对ONNX的处理比较粗糙，可能并没有发挥其优势。但Triton&nbsp;+&nbsp;TorchScript的方式已经满足业务需求，并没有对ONNX的实验进行深究。</p><p></p><h4>4.&nbsp;Tornado&nbsp;+&nbsp;Triton</h4><p></p><p></p><p>本节介绍最终确定的模型部署方式：Tornado&nbsp;+&nbsp;Triton。通过将不同版本的预处理模块打包成不同的Python&nbsp;模块解决版本冲突问题。另外，针对业务方多且需求不一致的问题，我们通过写多个接口的方式解决。比如，’/category_predict’支持单条预测，’/category_predict_nlp’支持批量预测。于是，本部署方式如下图所示：</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/05914482faca601764422e4a340ff7c1.png\" /></p><p></p><p></p><p>其中，Tornado&nbsp;Web服务接收应用方的请求，对数据进行预处理，然后异步调用Triton&nbsp;进行模型预测，最后对预测结果进行后处理并返回结果。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;本节中通过题型预测任务对新的部署方式进行测试，将对应的21个模型部署至限定16核的Triton&nbsp;Inference&nbsp;Server。产生随机数据模拟线上请求并用Locust工具进行压测，具体结果如下：</p><p></p><p></p><p></p><p>通过结果可以发现，将数据预处理和结果后处理集成在Web服务中并通过Triton进行模型部署的方案在我们复杂的业务场景下是最优的。这种方式不仅通过动态Batching的方式增加并发降低时延，并且方便了模型的管理与更新。其唯一的缺陷是无法解决预处理模块和模型版本强耦合的问题，在部署时需保持同步，且时间点最好是在服务低谷期。</p><p></p><h2>总结与展望</h2><p></p><p></p><p>本文涉及的场景比较复杂，主要有以下几个问题：</p><p></p><p>模型多，不同任务的模型总数接近100个，且更新频率随业务节奏变更。易冲突，数据预处理和模型版本强耦合，且不同版本预处理模块易冲突。性能要求高，任务面向用户，在部署至CPU环境下要求高并发和低时延。</p><p></p><p>因此，本文从实际应用场景触发，设计并尝试了多种部署方式，对比其在线上使用过程中的表现。经过几次实验后，找到了一种适用于我们复杂场景的深度学习模型部署方式，在不依赖GPU资源的情况下满足业务方的需求，并为后续的模型部署提供了一种可行的部署方案。</p><p></p><p>然而，由于业务节奏，本文在实验过程中仍忽略了一些问题。比如，ONNX格式的模型部署在Triton中的性能为何不如部署在Tornado&nbsp;Web服务中，或许是ONNX在Triton中需要一些特殊的配置。另外，数据预处理和模型版本强耦合的问题仍未得到合理的解决方案，目前的处理方式仍有版本对不齐的风险。本文未对这些问题进行深入研究，留待后续探讨。</p>",
    "publish_time": "2023-08-07 15:07:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]