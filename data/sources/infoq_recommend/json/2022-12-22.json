[
  {
    "title": "Java近期新闻：JDK 20的JEP提升为Targeted状态，AWS推出Lambda SnapStart特性",
    "url": "https://www.infoq.cn/article/ZYvBqLEPNaY4FvJjyopx",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p>JEP 432，<a href=\"https://openjdk.org/jeps/432\">记录模式（第二次预览）</a>\"，已从JDK 20的Proposed to Target状态<a href=\"https://twitter.com/OpenJDK/status/1597674326425096192?cxt=HHwWgMCi1biViqwsAAAA\">提升</a>\"为Target状态。该JEP更新自JEP 405，<a href=\"https://openjdk.org/jeps/405\">记录模式（预览版）</a>\"，更新包括：增加了对通用记录模式类型参数推断的支持；增加了对记录模式出现在增强for语句条件判断中的支持；并删除了对命名记录模式的支持。</p><p>&nbsp;</p><p>JEP 433，<a href=\"https://openjdk.org/jeps/433\">switch模式匹配（第四次预览）</a>\"，已从JDK 20的Proposed to Target状态<a href=\"https://twitter.com/OpenJDK/status/1597674334771810304?cxt=HHwWgMCo7faViqwsAAAA\">提升</a>\"为Target状态。该JEP更新自JEP 427，<a href=\"https://openjdk.org/jeps/427\">switch模式模式匹配（第三次预览</a>\"），更新包括： 简化了switch标签语法；现在， switch表达式和语句以及支持模式的其他构造体都支持泛型类型模式和记录模式的类型参数推断。</p><p>&nbsp;</p><p>JEP 434，<a href=\"https://openjdk.org/jeps/434\">外部函数和内存API（第二次预览）</a>\"，已从JDK 20的Proposed to Target状态<a href=\"https://twitter.com/OpenJDK/status/1597674343168757760?cxt=HHwWgMCitbWWiqwsAAAA\">提升</a>\"为Target状态。该JEP在<a href=\"https://openjdk.java.net/projects/panama/\">Panama</a>\"项目的支持下不断演进：JEP 424，<a href=\"https://openjdk.org/jeps/424?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">外部函数和内存API（预览）</a>\"，在JDK 19中交付；JEP 419，<a href=\"https://openjdk.org/jeps/419?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">外部函数和内存API（第二个孵化器版本）</a>\"，在JDK 18中交付；以及JEP 412，<a href=\"https://openjdk.org/jeps/412?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">外部函数和内存API（孵化器）</a>\"在JDK 17中交付。该JEP提议基于反馈进行改进，并在JDK 20中提供第二次预览。更新包括：统一了 MemorySegment 和 MemoryAddress 接口，即，内存地址由零长度的内存段建模；并且增强了MemoryLayout 密封接口，以便于与JEP 427，<a href=\"https://openjdk.org/jeps/427?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">switch中的模式匹配（第三次预览）</a>\"一起使用。</p><p>&nbsp;</p><p>JEP 429，<a href=\"https://openjdk.org/jeps/429\">作用域值（孵化器）</a>\"，已从JDK 20的Candidate状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2022-November/007225.html\">提升</a>\"为Proposed to Target状态。这个<a href=\"https://openjdk.java.net/jeps/11\">正在孵化</a>\"的JEP，最初名为范围局部变量（孵化器，Extent-Local Variables），由<a href=\"https://wiki.openjdk.java.net/display/loom/Main\">Loom项目</a>\"赞助，提议在线程内部和线程之间共享不可变数据。这优于线程局部变量，尤其是在使用大量虚拟线程时。</p><p>&nbsp;</p><p>JEP 436，<a href=\"https://openjdk.org/jeps/436\">虚拟线程（第二次预览</a>\"），已从JDK 20的Candidate状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2022-November/007223.html\">提升</a>\"为建议Proposed to Target状态。在<a href=\"https://wiki.openjdk.org/display/loom/Main\">Loom项目</a>\"的支持下，该JEP基于JDK 19中提供的JEP 425，<a href=\"https://openjdk.org/jeps/425\">虚拟线程（预览版）</a>\"，提出了第二次的预览，以便有时间为该功能的演进提供更多的反馈和经验。需要注意的是，除了少量在JDK19中被固化的JEP 425 API外，本预览版本没有进行任何更改，因此没有在第二次预览中提出。</p><p>&nbsp;</p><p>JEP 437，<a href=\"https://openjdk.org/jeps/437\">结构化并发（第二个孵化器）</a>\"，在JDK 20中从Candidate状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2022-November/007224.html\">提升</a>\"为Proposed to Target状态。该JEP也是在<a href=\"https://wiki.openjdk.org/display/loom/Main\">Loom项目</a>\"的支持下，提议基于从JDK 19中提供的JEP 428，<a href=\"https://openjdk.org/jeps/428\">结构化并发（孵化器）</a>\"中重新孵化该特性，以便有时间提供更多的反馈和经验。唯一的变化是更新了 StructuredTaskScope 类，以支持在任务作用域中创建的线程继承作用域值。这简化了跨线程不可变数据的共享。</p><p>&nbsp;</p><p>这三个JEP的审查将于2022年12月6日结束。</p><p>&nbsp;</p><p></p><h4>JDK 20</h4><p></p><p>JDK20<a href=\"https://jdk.java.net/20/\">早期访问构建版本</a>\"中的第<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B26\">26</a>\"版也已于上周发布，其中包括对第25版各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2020%20and%20%22resolved%20in%20build%22%20%3D%20b23%20order%20by%20component%2C%20subcomponent\">问题</a>\"的修复和<a href=\"https://github.com/openjdk/jdk/compare/jdk-20%2B22...jdk-20%2B23\">更新</a>\"。有关该版本的更多详细信息，请参阅<a href=\"https://jdk.java.net/20/release-notes\">发</a>\"<a href=\"https://jdk.java.net/20/release-notes\">布</a>\"<a href=\"https://jdk.java.net/20/release-notes\">说明</a>\"。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"，鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java缺陷数据库</a>\"报告缺陷。</p><p>&nbsp;</p><p></p><h4>JavaFX 20</h4><p></p><p>JavaFX 20<a href=\"https://jdk.java.net/javafx20/\">早期访问构建版本</a>\"中的<a href=\"https://github.com/openjdk/jfx/releases/tag/20%2B10\">第10版</a>\"已提供给Java社区。JavaFX专为使用JDK 20早期访问构建版本而设计，应用程序开发人员可以在JDK 20上使用JavaFX 20构建和测试其应用程序。</p><p>&nbsp;</p><p></p><h4>Spring框架</h4><p></p><p><a href=\"https://spring.io/projects/spring-integration\">Spring Integration</a>\"&nbsp;6.0已<a href=\"https://spring.io/blog/2022/11/29/spring-integration-6-0-goes-ga\">发布</a>\"，其特性包括：JDK 17和Jakarta EE 9基线；支持GraalVM和Spring AOT引擎的原生镜像；具有千分尺和千分尺跟踪的可观测性仪器；以及对Jakarta EE 10的支持。有关该版本的更多详细信息，请参阅<a href=\"https://docs.spring.io/spring-integration/docs/current/reference/html/whats-new.html#whats-new\">最新动态页面</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-vault\">Spring Vault</a>\" 3.0也已<a href=\"https://spring.io/blog/2022/11/28/spring-vault-3-0-goes-ga\">发布</a>\"，其特点包括：JDK 17基线；支持额外的HTTP客户端，包括响应式JDK HTTP客户端；并支持使用版本化密钥/值机密引擎的Vault存储库。有关该版本的更多详细信息，请参阅<a href=\"https://github.com/spring-projects/spring-vault/wiki/Spring-Vault-3.0-Release-Notes\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-cloud\">Spring Cloud</a>\"&nbsp;2022.0.0的<a href=\"https://spring.io/blog/2022/12/02/spring-cloud-2022-0-0-release-candidate-3-codename-kilburn-has-been-released\">第三个候选版本</a>\"，代号为Kilburn，已向Java社区提供。该版本提供了对Spring Cloud子项目RC3版本的更新，如：Spring Cloud OpenFeign 4.0.0、Spring Cloud Commons 4.0.0、SpringCloud Function 4.0.0和Spring Cloud Starter Build 2022.0.0。但是，由于Spring Cloud CLI、Spring Cloud for Cloud Foundry和Spring Cloud Sleuth等子项目的移除，也引入了一些破坏性的更改。Spring Cloud 2022.0.0-RC3要求Spring Boot 3.0.0。有关该版本的更多详细信息，请参阅<a href=\"https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2022.0-Release-Notes\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>亚马逊云科技</h4><p></p><p>在最近的<a href=\"https://reinvent.awsevents.com/\">re:Invent</a>\"大会上，亚马逊云服务为其<a href=\"http://aws.amazon.com/lambda/\">AWS Lambda</a>\"项目<a href=\"https://aws.amazon.com/blogs/aws/new-accelerate-your-lambda-functions-with-lambda-snapstart/\">推出了</a>\"一项新特性<a href=\"http://docs.aws.amazon.com/lambda/latest/dg/snapstart.html\">Lambda SnapStart</a>\"，旨在减少Java函数的冷启动，并加速Lambda函数。<a href=\"https://quarkus.io/blog/quarkus-support-for-aws-lambda-snapstart/\">Quarkus</a>\"和<a href=\"https://micronaut.io/2022/11/28/leveraging-aws-lambda-snapstart-with-the-micronaut-framework/\">Micronaut</a>\"已经实现了对Lambda SnapStart的支持。更多详细信息请参阅<a href=\"https://www.infoq.cn/theme/164\">InfoQ的新闻报道</a>\"和亚马逊云科技<a href=\"https://aws.amazon.com/blogs/compute/reducing-java-cold-starts-on-aws-lambda-functions-with-snapstart/\">博客文章</a>\"。</p><p>&nbsp;</p><p></p><h4>Quarkus</h4><p></p><p>红帽（Red Hat）<a href=\"https://quarkus.io/blog/quarkus-2-14-2-final-released/\">发布</a>\"了Quarkus 2.14.2和2.13.5版本，主要修复了CVE-2022-4116漏洞，CVE-2022-4116是Dev UI配置编辑器中的一个漏洞，容易受到本地主机驱动攻击，导致远程代码执行。该版本还加强了对跨源资源共享（CORS）的处理，包括在CORS请求因来源无效而被拒绝时将 200 OK 更改为 403 FORBIDEN 。有关这些版本的更多详细信息，请参阅<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.14.2.Final\">2.14.2版</a>\"和<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.13.5.Final\">2.13.5版</a>\"的发布说明。</p><p>&nbsp;</p><p>在通往Quarkus 2.15.0的道路上，<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.15.0.CR1\">第一个候选版本</a>\"也提供了新特性，比如：与Quarkus CRaC/Firecracker的集成；将gRPC扩展迁移到新的Vert.x gRPC实现；使用Panache扩展支持在REST数据中根据命名查询进行过滤；以及对GraalVM 22.3.0、Mandrel 2.13和SmallRye GraphQL 1.9.0的依赖项升级。</p><p>&nbsp;</p><p></p><h4>Apache Camel</h4><p></p><p><a href=\"https://camel.apache.org/\">Apache Camel</a>\"&nbsp;3.18.4已<a href=\"https://camel.apache.org/blog/2022/12/RELEASE-3.18.4/\">发布</a>\"，对Spring Boot 2.7.6和HyperSQL DB 2.7.1进行了27个bug修复、改进和依赖项升级。有关该版本的更多详细信息，请参阅<a href=\"https://camel.apache.org/releases/release-3.18.4/\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JHipster</h4><p></p><p>在<a href=\"https://www.jhipster.tech/\">JHipster</a>\"，已经开始将平台升级到Spring Boot 3.0。Okta的开发人员倡导者、Java Champion和JHipster开发团队成员<a href=\"https://www.linkedin.com/in/mraible/\">Matt Raible</a>\"提供了团队迄今为止遇到的问题的<a href=\"https://twitter.com/mraible/status/1597248481596706816?cxt=HHwWgMDUuYHCyKosAAAA\">最新情况</a>\"，即：<a href=\"https://github.com/reactor/BlockHound/blob/master/README.md\">BlockHound</a>\"需要一个新的<a href=\"https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html\">WebFlux</a>\"的入口；健康检查在Elasticsearch 8+中不起作用；MongoDB和WebFlux与JUnit和MongoDB驱动程序4.2+死锁。有关这些问题的更多详细信息，请参阅此GitHub<a href=\"https://github.com/jhipster/generator-jhipster/pull/19791\">拉取请求</a>\"。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/12/java-news-roundup-nov28-2022/\">https://www.infoq.com/news/2022/12/java-news-roundup-nov28-2022/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/ATGFHsJa5HqNroEuNBLd\">对话Spring大神：Spring 生态系统的新时代来了！</a>\"</p><p><a href=\"https://www.infoq.cn/article/GGdb3Y7cO9Pw2Bf8NfIW\">Java 新闻汇总：Spring 发布，Resilience4j，Open Liberty，GlassFish，Kotlin 1.8-Beta</a>\"</p>",
    "publish_time": "2022-12-22 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "端云协同：万物智能化的未来，怎么“来”？",
    "url": "https://www.infoq.cn/article/58dW6BKuNZVUrJg4xmur",
    "summary": "<p>随着云计算、云原生的发展，大家逐渐意识到充分利用云端能力的优势赋能终端，打造端云协同的一体化服务，才是最终实现万物智能化未来的解决之道。现如今，端云协同已经成为开发社区竞相研究的前沿技术方向，OPPO 作为全球领先的智能终端企业，也积极地展开了探索。</p><p></p><p>在 12 月 14 日举办的 OPPO 2022未来科技大会（OPPO INNO DAY 2022）上，<a href=\"https://www.infoq.cn/article/JZw9tHzimGleWBq65lN6\">OPPO </a>\"发布了三大核心技术之一的安第斯智能云，致力于“让终端更智能”。在先前的 QCon 2022 全球软件开发大会（上海站），InfoQ 编辑对 OPPO 云服务中心高级总监韩建飞进行了采访，与他聊了聊端云协同相关的那些事儿，包括行业发展的现状以及未来的展望。</p><p>&nbsp;</p><p>以下是视频采访的全部内容，为方便读者查看，视频下方也附上了文字内容。</p><p></p><p></p><p>采访记者：InfoQ 资深编辑鲁冬雪</p><p>采访嘉宾：OPPO 云服务中心高级总监韩建飞</p><p>嘉宾简介：2019 年加入 OPPO，负责计算、网络、研发效能及云监控的开发管理工作，先后服务于中兴通讯、京东等公司。入职 OPPO 后完成了 OPPO 全球混合云的建设及全公司上云的战略。在云计算领域有丰富的研发管理和实践经验。</p><p>&nbsp;</p><p>InfoQ：您是如何定义“端云协同”的？端云协同技术核心要解决什么问题？</p><p>&nbsp;</p><p>韩建飞：当前终端行业面临的主要问题仍是硬件成本与用户体验之间无法达到平衡。在这种情况下，面对端侧的算力、空间、性能瓶颈，通过云侧计算、网络、存储能力，与端侧深度协同，进而提升端侧体验，是目前端云协同演进和发展的关键方向之一。在这一过程中，端云协同需要解决的最核心的问题在于，如何解决网络传输带来的时间损耗，为用户带来无感的产品体验。</p><p></p><p>OPPO作为一家领先的智能终端企业，在端云协同技术上有着深厚的技术积累。目前，我们正在研究和主要解决的问题是，在智能终端厂商设备计算和存储性能受限的情况下，如何通过云端的能力赋能终端；在网络传输延迟越来越低的情况下，如何通过云端，为终端用户带来无缝一致的一站式服务体验。</p><p>&nbsp;</p><p>InfoQ：端云协同赛道当前现状如何，将会往哪个方向或者趋势演进？</p><p>&nbsp;</p><p>韩建飞：在我看来，端云协同技术将会朝着两个方向演进，这两个方向既相互独立，又相互交织。第一，通过端云协同技术，扩展手机等智能终端的存储空间、计算能力和智能服务，在影像、AI、网络加速、云应用等方向赋能智能终端，为用户带来产品体验的整体提升；第二，通过端云协同技术，建设一云多端的生态服务体系，以多端融合、数据共享等方式，消弭用户在泛终端使用场景的割裂体验。基于此，OPPO将着力推动端云协同技术的演进与发展，构建数据共享与泛终端联动的万物互融生态体验。</p><p>&nbsp;</p><p>InfoQ：目前私有云、公有云、边缘云、混合云的发展分别对端云协同技术的演进有何影响？企业如何进行上云选型？</p><p>&nbsp;</p><p>韩建飞：业内其实基本形成了共识：无论是私有云、公有云、边缘云还是混合云，均可以支撑端云协同技术的发展，以何种云服务的形态上云，主要还是取决于企业的业务需求。</p><p></p><p>以 OPPO 为例，自一开始，OPPO 便坚定地选择了混合云基础设施的建设与发展路径。为什么 OPPO 会选择混合云？第一，ColorOS 在全球有逾5亿的月活用户，为了更好地为国内庞大的用户群提供服务，OPPO 在国内选择了自建私有云；另一方面，受海外政策和相关条例，以及运营策略的影响，OPPO 在海外选择了公有云。基于私有云和公有云，OPPO 构建了混合云基础设施，推动端云协同技术在业务场景落地。总的来说，OPPO 的混合云基础设施为业务场景服务，在满足业务发展的同时，实现了成本和效率的平衡。</p><p>&nbsp;</p><p>InfoQ：OPPO 混合云平台有何特点？在技术实现上有什么创新？</p><p>&nbsp;</p><p>韩建飞：OPPO 通过混合云基础设施构建了一套一体化平台，带来了全球一致的业务服务和运营体验。在技术实现上，OPPO 的产品线业务、中间件、数据库、网关、大数据实时及离线计算等均全量运行在容器上，基于此，OPPO 构建了独特的混合云实施策略，平台特点包括：</p><p>统一管理：多套异构云资源统一管理；公有云无缝切换，降低迁移成本；安全合规：一致的安全管控策略；无缝的网络策略融合；统一的组网模型；敏捷灵活：可利用公有云按需秒级计费特点；可支撑大规模临时紧急需求；海外快速开服；无感基础设施交付周期；常规算力下沉自建基础设施；统一架构：一致的混合云技术；降低运维管理复杂度；一致体验：统一技术架构交付；一致的业务使用体验；平台透明无感适配，统一API；可降低技术服务门槛。</p><p>&nbsp;</p><p>InfoQ：混合云的在端云协同技术中占据着什么样的地位？承担着怎样的角色？混合云基础架构的建设对于端云协同技术的发展有何作用？</p><p>&nbsp;</p><p>韩建飞：以 OPPO 的混合云基础设施建设为例，OPPO 自 2019 年开始，着力构建混合云基础设施，通过近 3 年来的努力，为 ColorOS 的 5 亿月活用户提供全球统一的混合云基础设施平台。为此，OPPO 制定了全球 8+N 可用区的目标，在全球主要的八大区域搭建自由的混合云基础设施，同时配合 N 各小区域的辅助性基础设施建设，实现公司内部系统的全面云化。</p><p></p><p>在混合云基础设施构建完成后，OPPO 基于底层技术能力构建了为全球用户提供端云协同服务的业务场景。在为庞大用户提供产业和服务的过程中，混合云基础设施是<a href=\"https://www.infoq.cn/article/HHPHzeKSlWI5vW8U3kpD\"> OPPO </a>\"业务全球化的关键技术底座，也是推动端云协同技术业务落地的关键技术支撑。</p><p>&nbsp;</p><p>InfoQ：据了解，关于端云协同，OPPO 目前在做很多事，比如 AIoT、音视频、一站式应用开发等等，所以有几个围绕这几个领域的问题想要问一下。</p><p>&nbsp;</p><p>Q1：您是如何定义 AIoT 的？AIoT 在工业领域有哪些主要的应用场景？OPPO 现在在该领域做了哪些探索？您觉得 AIoT 的落地应用，最难的部分是什么？有什么好的经验可以分享给大家？</p><p>&nbsp;</p><p>韩建飞：AIoT 可以从 IoT 生成的海量数据中挖掘更多的价值，用以改善业务及其服务。基于 AI 能力，IoT设备收集到的大数据，将更好地进行分析、推理与决策，而无需人工干预。AIoT 在工业领域落地的场景有很多，比如供应链智能管理、生产线智能监控、智能诊断、智能维护，工厂的智能管理、产品质量控制和智能故障诊断等，在这些场景中，目前在业内也不乏成功的落地案例。</p><p></p><p>在 AIoT 的落地过程中，OPPO 面临的最大的难题是，作为一家智能终端企业，需要去连接各种不同类型的设备，而在不同的设备之间，复杂的协议适配不仅是 OPPO，同时也是整个行业面临的关键阻力之一。在自有 IoT 设备的连接上，OPPO 有一套自研协议；在与不同厂商设备的连接过程中，OPPO 更希望的是通过统一规范进行连接。基于此，<a href=\"https://www.infoq.cn/article/vkHROQzRXNkX5uooNGgl\">OPPO</a>\" 积极加入了相关生态，希望可以实现不同设备间的统一连接。</p><p></p><p>以 OPPO 智能家居的场景探索为例，智能家居落地的难点之一，在于不同设备生态之间的隔阂，无法实现互联互通，而近年来的标准协议如 Matter 等，为互联互通带来了一丝希望。在这一领域，OPPO 仍处于探索状态，从现有经验看来，通过标准协议拥抱开放生态，不失为 AIoT 应用落地的关键路径之一。</p><p>&nbsp;</p><p>Q2：如今大多数音视频厂商主攻的技术主要有 RTC、IM 以及 CDN，OPPO 在这三方面做了哪些探索？“端云协同”又是如何赋能咱们业务的呢？</p><p>&nbsp;</p><p>韩建飞：在音视频技术领域，OPPO 起步得相对较晚，业界已有相当多成熟的技术方案可借鉴。基于此，OPPO 选择了 RTC + CDN 结合的方式，以满足 OPPO 在音视频领域的业务需求。之所以选择 RTC，是因为 RTC 在传输过程中时延极短，与 CDN 相互配合，可以让各个地区的用户，均能快速感受到音视频传输的实时性与便捷性。</p><p></p><p>端云协同在 OPPO 的业务场景中具有较多的应用，如远程协同、语音通话、语音共享、游戏互动以至于云游戏等业务场景，端云协同技术为 OPPO 的相关业务场景带来了窄带高清、低延迟的交互体验。</p><p>&nbsp;</p><p>Q3：一站式应用开发，有的人认为注重“一站式”的广度重要，也有人说注重“应用开发”的深度重要，那 OPPO 又是如何定义一站式应用开发的呢？目前 OPPO 一站式移动应用开发平台有哪些技术突破？</p><p>&nbsp;</p><p>韩建飞：无论是从广度还是深度上，OPPO 移动应用开发平台都达到了一个很好的平衡状态。目前，OPPO 移动应用开发平台是一个主要服务于内部互联网业务场景的一站式平台，为内部开发者提供 toolkit、热更新、云控、日志打捞、自升级、UIkit、网络库等中间件能力，并结合研发云提供上架、运营等服务。</p><p></p><p>在端云协同的场景下，OPPO 移动应用开发平台仍具有非常大的延展和想象空间，通过移动应用开发平台，更好地实现数据流转和设备融合，并在此基础上，为开发者和应用生态带来更加便捷高效的一站式服务。</p><p>&nbsp;</p><p>Q4：从 AIoT、音视频+、一站式应用开发等场景来看，从 0 到 1 构建端云协同，行业里面临的技术难点分别是什么？</p><p>&nbsp;</p><p>韩建飞：从发展趋势来看，端云协同即将迎来飞速发展期，网络从 2G 进化至 5G，端侧与云侧之间的传输速度加快。在网络传输速度大幅提升之后，云侧的无限算力和存储资源将为终端的发展带来新的生机。</p><p>具体地说，面对端侧与云侧之间的网络传输延迟问题，是推动端云协同技术发展和演进的关键探索方向。在解决这一问题之后，OPPO 以至于智能终端行业，将为用户提供更为无缝一致的交互体验。</p><p>&nbsp;</p><p>Q5：像 AIoT、音视频+等各个领域都在做“端云协同”，那数据安全问题要如何解决呢？</p><p>&nbsp;</p><p>韩建飞：在安全方面，业内已经形成了一些较为完备的解决方案，如数据安全（图形、音视频等）隐私保护、用户安全密钥、全链路加密等安全隐私保护体系。</p><p></p><p>ColorOS 在全球拥有超过5亿月活用户，在这一用户基础上，OPPO 首要满足的是各个地域的隐私安全和规避地缘政治的相关风险。以端云数据加密为例，OPPO 拥有全链路安全认证，可以在用户授权的情况下，加密访问部分用户数据，让用户即使在数据上云的情况下，也能享受全面的、坚实的安全隐私保护。</p><p>&nbsp;</p><p>InfoQ：关于端云协同技术的未来，您有什么看法？您理想中的“端云协同”是怎样的？</p><p>&nbsp;</p><p>韩建飞：通过端云协同技术的持续演进与发展，以云赋能终端，最终为用户提供端云融合的一致性服务与体验，让云成为整个终端的全新生产力。</p><p>&nbsp;</p>",
    "publish_time": "2022-12-22 09:45:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "红帽发布《中国企业数字化转型图谱》，六大方向探索转型下一步",
    "url": "https://www.infoq.cn/article/o9d0Fy2UxUVwagIw9Muq",
    "summary": "<p>前不久，2022 Red Hat Summit：Connect（红帽论坛）中国站召开。会上，红帽全球副总裁兼大中华区总裁曹衡康发表了《探索数字化转型的下一步》主题演讲，分析了数字化转型面临的机遇与挑战，以及红帽对未来趋势的分析和建议，并发布了《中国企业数字化转型图谱》，为企业数字化转型提供方向参考。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/907a23733c3dbd9de08ca07000322442.png\" /></p><p></p><p>&nbsp;</p><p>《中国企业数字化转型图谱》包含开放混合云架构、云原生应用现代化、自动化、开放性学习组织、开源赋能、信息化+产业化六大方向，是红帽通过与众多企业沟通以及对整体趋势的感知梳理而成。其中，这六大方向彼此相关但又互相独立，企业可根据实际需要选择单一或者多个维度切入，红帽在这个过程中也会向企业输出自己的技术和经验。</p><p>&nbsp;</p><p>以多云管理为例，由于不同云平台之间运营方式的不同导致操作人员在管理多云环境时会有不一致的感觉，红帽的所有产品和技术栈都做到了很好的向下兼容，操作人员通过红帽的平台可以享受到一致化的体验而不会受到底层设施的干扰，降低复杂性的同时也在降低成本，运维人员也不需要考虑大环境的兼容问题。</p><p>&nbsp;</p><p>此外，红帽已经将这种兼容性从多云环境，也就是公有云、私有云、本地云扩展到了边缘侧，OT管理也可以通过红帽一致性的IT管理方式或者两化融合的方式进行。</p><p>&nbsp;</p><p>曹衡康介绍，对于正在进行数字化转型的企业来说，红帽所能做的远不只是提供开源技术，还包括开源的文化、开放的思维的培养。目前，红帽正在中国市场重点推进三项工作来帮助企业加速数字化转型：</p><p>&nbsp;</p><p>建立数字化转型开放领导力思维工作坊，主要面向企业IT经理、CIO、CTO、CDO等管理层。红帽的专家会和他们共同探讨如何把开放思维、开放领导力、开放组织、开放文化导入到整个数字化转型过程。组建开放创新训练营，主要面向产品经理、业务分析师、技术运维等人员。红帽之前有开放创新实验室。在这个实验室里客户通过4-12周的时间来探索如何把开源技术与自己的业务结合，以帮助解决业务问题。开放创新训练营是开放创新实验室的先导，通常会用一天甚至半天的时间来跟客户来探索如何进行创新。红帽技术专家面对面+技术体验营，主要面向技术经理、应用开发人员、运维人员。由红帽的技术专家跟客户的技术专家一起，来体验如何使用红帽技术和市场上的其他开源技术解决业务问题。</p><p>&nbsp;</p><p>对很多企业而言，当前在数字化转型层面强调的是速度和质量。在这个前提下，曹衡康表示下一代技术底座最重要的特点首先是兼容，因为这不是一家厂商就能做到的事情，包括架构、应用、网络各个方向，所以兼容性非常重要，红帽过去三十年与全球5000多家硬件厂商、4000多家软件厂商均进行了认证兼容。</p><p>&nbsp;</p><p>其次，开放性。为什么很多企业会选择开源技术，因为这在一定程度上代表着创新，开源技术源自全球各地优秀的工程师们，而红帽将这些优秀的技术变成了企业级生产可用的产品，开放就是最大的创新。</p><p>&nbsp;</p><p>再次，敏捷性。数字化转型需要非常敏捷，这也是云原生应用开发模式得以流行的重要原因之一。以金融企业为例，消费者提出的需求可能几周之内就要把应用微服务开发完成。</p><p>&nbsp;</p><p>然后是便利性。平台移动要足够便利，任何环境下的体验要保持一致，管理要保持一致性，只需要一套方法和工具就可以管理所有的基础环境。</p><p>&nbsp;</p><p>最后是自动化。数字化转型很重要的底层技术服务就是自动化程度要足够高，这也是疫情造成数字化转型加速的原因之一，企业需要更自动化的技术辅助完成转型过程。在红帽看来，自动化是一个综合的体系，通常由多种产品与Ansible协作完成，自动化的成果如今已遍布各行各业，比如工业领域的一些专有、封闭的软件逐渐解耦为一个开放的体系，除提高生产效率之外，这也会催生更多新的应用场景。红帽也相信通过这种开放式的变化未来能带来更多场景的爆发以及行业革新。</p><p>&nbsp;</p><p>最后，《中国企业数字化转型图谱》是一个开放的、不断演进的、与生态伙伴共建的图谱。红帽在其中会发挥连接、支撑体系的作用，共同往下一步演进，让其真正变成所谓的未来愿景，并与更多国内的企业一起推出本土化解决方案，未来有更多机会与本土合作伙伴一起发布成果。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2022-12-22 11:02:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云原生时代，18岁的NGINX过时了吗？",
    "url": "https://www.infoq.cn/article/q00BxoNTXC1Qgobjyzs6",
    "summary": "<p>如今，全球半数以上（55%） 的网站都基于 NGINX 运行，差不多相同比例 (53.7%) 的中国网站在 NGINX 开源版上运行。作为最受欢迎的网络服务器，NGINX自发布到现在已经有18年了，它现在有什么样的发展规划呢？</p><p>&nbsp;</p><p>近日，NGINX Sprint China 2022大会于线上举行，F5 NGINX 讲解了NGINX在云原生下的产品路线图，宣布推出NGINX Kubernetes Gateway以及MARA参考架构1.0版本，并且HTTP3 和 QUIC也将合并到下一个版本中。“如果有人说原先的 NGINX 产品系列已经过时，那我只能说你并没有密切关注我们的动向”，F5 NGINX总经理Rob Whiteley在主题演讲中这样说道。</p><p>&nbsp;</p><p></p><h2>NGINX的进化</h2><p></p><p>&nbsp;</p><p>在数字化技术的推动下，应用现代化正成为产业发展的趋势与共识。F5中国区软件事业部总经理章澍分享了NGINX认为的，从传统应用向现代化应用发展过程中将会历经的三次浪潮：第一次浪潮实现了应用的大规模并发和扩展，而如今正在经历的第二次浪潮，其特征是实现应用解耦为微服务并通过 API 连接。这波浪潮将极大地推动自动化技术的发展，感知可控、随需而变的应用也将应运而生。也就是说，在不远的将来，全世界会迎来以感知可控、无人工干预的自适应应用为标志的第三次浪潮。</p><p>&nbsp;</p><p></p><h3>NGINX的诞生</h3><p></p><p>&nbsp;</p><p>NGINX 于 2004 年推出。在早期的互联网时代，随着Web 2.0的兴起，用户数量呈几何级数增长，互联网不再是单纯的浏览Web页面，逐渐开始进行交互，应用程序的逻辑也变的更复杂，从简单的表单提交，到即时通信和在线实时互动。这种用户体量的上升以及互动请求的增加，也给服务器带来了压力。</p><p>&nbsp;</p><p>NGINX 的诞生也是为了实现大规模的并发和扩展，相当多的企业看到了NGINX 的性能优势并开始使用它。 Igor Sysoev 于2011 年辞去了在 Rambler 的工作，并创立了 NGINX, Inc.。几年后，NGINX Plus 发布了，这是一个带有一些附加功能的版本，并且在商业上取得了巨大的成功。2019年， NGINX, Inc. 被F5 Networks 以 6.7 亿美元收购。</p><p>&nbsp;</p><p>NGINX采用异步模式，且轻量级，采用 C 进行编写，在性能上的出色表现是击败Apache网络服务器的关键。但 NGINX 取得成功，却不仅仅是因为NGINX是一个网络服务器，它还具备负载均衡器、反向代理、邮件代理和HTTP 缓存等功能，提供了构建安全、可靠的 Web 应用程序所需的几乎所有方面的能力。</p><p>&nbsp;</p><p>比如，在2000年代早期，一台硬件负载均衡服务器动辄从十几万到几十万不等，因此当服务规模不大时，直接采购硬件负载均衡服务器对于很多中小公司并不划算，而通过 Web 服务器的反向代理的方式却是当时比较经济的方式。一般 Web 服务器都有反向代理功能，NGINX 则是其中典型代表。</p><p>&nbsp;</p><p>在此基础上，NGINX 和NGINX Plus平台又由多个分散的同类最佳工具组成，当它们串联使用时，可以以各种“风格”进行部署，以满足企业的多种需求，从而成为了市场占有率第一的网络服务器。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/05ef09d260d8dedb97765a7afb1c996e.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>云原生时代的NGINX</h3><p></p><p>&nbsp;</p><p>如果说互联网的崛起导致应用的大规模并发和扩展，是我们经历的第一次浪潮，那么微服务和容器化的兴起，也可以算作是我们正在经历的第二次浪潮。</p><p>&nbsp;</p><p>在第二波浪潮下，企业更关注于 Kubernetes和容器的部署，但Kubernetes缺乏生产环境中的应用所需的应用交付、可观察性以及安全防护功能，因此一个好的生产级 Kubernetes平台需要进行深思熟虑的定制和调整。</p><p>&nbsp;</p><p>NGINX 2021 年的社区调查显示，2/3的人都已经或打算在生产环境中使用 Kubernetes，但是都有着对于自身知识技能以及对于 Kubernetes 的复杂性、安全防护和扩展性的担忧。为了构建坚实的 Kubernetes基础，NGINX通过添加 Ingress controller、WAF、服务网格以及一些其他云原生项目，提供了云原生的、Kubernetes 友好的开源和商业解决方案，来提升应用程序的扩展性、可见性、安全性......</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2a/2a97e705a53f1715fddef365149c7e6a.png\" /></p><p></p><p>&nbsp;</p><p>另一方面，微服务和应用的数量在快速增长，微服务之间以及集群内外之间的 API 数量也不断增加。一般来说，微服务之间的内部API 调用次数通常是应用到客户端之间的外部API调用次数的 10 倍或者更多。随着应用环境的扩张，复杂的环境可能有成百上千个 API，更复杂的 API 身份验证、授权、路由、整形和生命周期管理等问题就会随之而来，所以在云原生时代，网关功能更为重要。</p><p>&nbsp;</p><p>NGINX提供了API Gateway、Ingress Controller、Service Mesh<a href=\"https://www.nginx-cn.net/blog/how-do-i-choose-api-gateway-vs-ingress-controller-vs-service-mesh/\">多种选择</a>\"。其中，作为被普遍使用的反向代理工具，基于 NGINX 实现的 NGINX Ingress 也成为了 Kubernetes 集群中最广泛使用的Ingress网关。目前NGINX Ingress主要有两个版本，其中一个是 Kubernetes 社区所开发和维护的 NGINX Ingress Controller (kubernetes/ingress-NGINX)。而F5 NGINX 也开发和维护了 NGINX Ingress Controller (NGINXinc/kubernetes-ingress)，在数据平面上添加一些高级功能或商业支持。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/459e50686f328cbfad84536785642854.png\" /></p><p></p><p>&nbsp;</p><p>然而，开源版本和NGINX维护的版本之间存在一定差异，这也让用户感到困惑。为了消除这种困惑，NGINX基于 Kubernetes API Gateway SIG 参考架构，于今年早些时候推出了 NGINX Kubernetes Gateway。NGINX Kubernetes Gateway 由 Ingress controller 发展而来，是一种基于 Gateway API 规范内测版的新兴技术。Gateway API 终将取代 Kubernetes 架构中的 Ingress Controller，为了与云原生趋势保持一致，NGINX表示已决定将之前仅在开源版本中提供的 NGINX Kubernetes Gateway 作为下阶段的 Kubernetes 网络开发重点。</p><p>&nbsp;</p><p></p><h3>现代应用参考架构MARA</h3><p></p><p>&nbsp;</p><p>云原生基础设施和基于微服务的设计，能够高容错、松耦合，使得开发可快速迭代，让企业可以用敏捷的方式支持数字化转型。然而利用云原生构建现代化应用并不容易，“部署 Kubernetes 有很多不同的方法——网络、安全、身份验证，甚至像 API 网关这样的东西。对于大多数刚起步的企业来说，这还是比较复杂。” F5 NGINX总经理<a href=\"https://www.linkedin.com/in/rwhiteley/\">Rob Whiteley</a>\"在接受媒体采访时曾说。“如果没有很好地理解，很容易陷入错误的配置状态。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24a53b5a2a139712b080308e0fcc6ff2.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>“我们意识到，我们可以制作一个模版作为企业参考架构：给出真正的操作代码，而不是纸上的概念。”Whiteley说。因此，MARA诞生了。这种思路类似于构建一个“黄金镜像”，让用户从列表中自动拉取、组装和预集成所有脚本，然后通过一个命令进行部署。并且F5希望开发人员只需单击几下就能够在几分钟内配置和部署好一个Kubernetes 环境，形成一个完整并稳定可靠的开发环境。</p><p>&nbsp;</p><p>总之，MARA 是一个悉心设计的“稳定可靠、经过测试且可以部署到在 Kubernetes 环境中运行的实时生产应用”解决方案。该模块化架构集成了创建生产级云原生环境所需的一切——安全性、日志记录、网络、应用服务器、配置和 YAML 管理等。</p><p>&nbsp;</p><p>即使平台能够集成所有这些功能，但要完全满足生产环境要求还需要更多的工作。经过不断实验并探索如何帮助核心开发人员更高效、更轻松地部署现代应用，NGINX在去年的&nbsp;<a href=\"https://www.nginx-cn.net/blog/nginx-sprint-2-0-clear-vision-fresh-code-new-commitments-to-open-source/#mara\">Sprint&nbsp;</a>\"大会上宣布推出了MARA<a href=\"https://www.nginx-cn.net/blog/new-open-source-modern-apps-reference-architecture/\">参考架构</a>\"，一个现代应用的开源架构和部署模型。在今年的NGINX Sprint 上，Rob Whiteley 也在主题演讲中宣布了即将推出MARA 1.0<a href=\"https://www.nginx.com/blog/announcing-version-1-0-0-nginx-modern-apps-reference-architecture/\">版本</a>\"。</p><p>&nbsp;</p><p>在发布时，MARA 预配置了多种选择，使用<a href=\"https://www.elastic.co/log-monitoring\">Elastic</a>\"进行日志管理，使用<a href=\"https://prometheus.io/\">Prometheus</a>\"和<a href=\"https://grafana.com/\">Grafana</a>\"进行监控和仪表板，使用<a href=\"https://aws.amazon.com/?utm_content=inline-mention\">Amazon Web Services</a>\"的<a href=\"https://aws.amazon.com/eks/\">Elastic Kubernetes Service</a>\"&nbsp;(EKS) 作为部署目标，使用<a href=\"https://spinnaker.io/\">Spinnaker</a>\"进行持续交付，以及TLS<a href=\"https://cert-manager.io/docs/\">的证书管理器</a>\"，以及中间层的许多 NGINX 产品。</p><p>&nbsp;</p><p>另外，微服务相对单体服务，其故障定位难度完全不是一个等级，因此要使微服务监控和可观察性更上一层楼，就需要引入优秀的APM系统。CNCF管理的OpenTelemetry项目 （由OpenTracing 和 OpenCensus合并而成），它以一种综合的方式生成追踪、日志和指标，也成为了目前服务监控可观察性统一方案。MARA 1.0 版本也选择了<a href=\"https://www.nginx-cn.net/blog/integrating-opentelemetry-modern-apps-reference-architecture-progress-report/\">集成</a>\"OpenTelemetry，实现日分布式跟踪、指标收集等功能，这也是<a href=\"https://www.nginx-cn.net/blog/announcing-version-1-0-0-nginx-modern-apps-reference-architecture/\">1.0版本</a>\"中的一个重要变化。</p><p>&nbsp;</p><p></p><h2>NGINX的开源演进：兼顾稳定和高性能</h2><p></p><p>&nbsp;</p><p>NGINX作为纯C实现的软件，源码质量很高。创始人<a href=\"https://en.wikipedia.org/wiki/Igor_Sysoev\">Igor Sysoev</a>\"最开始也只专注于解决C10K问题，并一个人写了几乎所有的代码，独自管理到2011年。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d54b61c09ebe4fac6574f4d83862a40.png\" /></p><p></p><p>&nbsp;</p><p>2017年，当时的NGINX 首席执行官接受媒体采访时介绍说，这个轻量级软件，核心代码一直<a href=\"https://www.zdnet.com/home-and-office/networking/nginx-and-the-future-of-the-web-server/\">少于</a>\" 200,000 行。同时，开源版本依赖很少，仅有非常少的库，如Openssl、glib。这也是它高性能的原因之一。“性能为王”是它击败Apache网络服务器的原因，其模块化机制也始终可以让NGINX关注于可以为工程师提供“灵活度”，这也是让它在Web网关服务器领域中一直领先地位的原因。</p><p>&nbsp;</p><p>但云原生的到来正在改变 API 网关的角色，也给NGINX带来了新的挑战。很多其他API 网关解决方案都是基于NGINX 搭建的，比如开源和商用的 Kong API 网关以及开源的 OpenResty等，这些软件在敏捷开发行业很火。</p><p>&nbsp;</p><p>虽然这进一步验证了 NGINX 核心技术在这个领域的可用性，但也让人们思考NGINX在云原生技术下的优势。但相对来讲，NGINX使用C语言，代码空间封闭；而新兴的一些软件使用Lua，虽然可以随时编写功能插件，但通过解析String并立即返回调用函数，这样导致其代码空间是完全开放的。所以从这一点来说，NGINX的设计更加安全稳定。而传统行业也比一些敏捷行业更注重安全稳定的性能，所以NGINX仍然是传统行业的首选。就像Rob Whiteley在主题演讲中提到的那样，“开源安全性是开发人员的首要考虑事项”。</p><p>&nbsp;</p><p>他表示，“数以千计的企业正在生产环境中运行 NGINX 开源软件——这是一件好事，因为这充分表明了公司们对我们开源版本的高度信任，我们将带着这份信任再接再厉。对于核心 NGINX 开源版软件，我们一直在不断添加新特性和功能，并支持更多操作系统平台。在即将发布的下个版本中，我们将通过 HTTP3 和 QUIC 这两大功能来保障 Web 应用以及流量的安全性和可扩展性。”</p><p>&nbsp;</p><p>在 NGINX 的设计中，后端服务以静态配置文件的形式记录，里面使用了一些优化过的静态哈希表设计，因此性能也非常好。但在微服务时代，后端服务的 IP 发生变化的时候，都需更改配置文件，静态配置的方式也给网关实现“连接复用”增加了难度，而基于UDP的HTTP3 和 QUIC协议则可以实现跨IP迁移。各种网络技术实际上早已经成熟，但NGINX更多考虑的是稳定性，因此在QUIC第一份规范草案提交给 IETF 的五年之后，NGINX才选择合并QUIC到当前版本中。</p><p>&nbsp;</p><p>这同时说明NGINX也一直在跟进网络世界的重大变化。例如，NGINX 于 2015 年 9 月开始支持 HTTP/2，距协议修订标准化仅几个月。HTTP/2 服务器推送支持也于 2018 年推出，现在HTTP /3和QUIC也终于要实现到 NGINX 中。</p><p>&nbsp;</p><p>在开源崛起和迈向成功的过程中，NGINX 在这一二十年里发挥了至关重要的作用。现在，通过NGINX在云原生领域的重大发布，我们也可以看出NGINX一直在努力提升自身的竞争力，用Rob Whiteley的话来说，就是“NGINX 要想十年后还能广受欢迎，就需要不断做出改进......对自己的开源工作反躬自省，跟上开源运动的持续发展。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.nginx-cn.net/blog/future-of-nginx-getting-back-to-open-source-roots/\">https://www.NGINX-cn.net/blog/future-of-NGINX-getting-back-to-open-source-roots/</a>\"</p><p><a href=\"https://www.nginx-cn.net/blog/5-things-to-know-about-nginx-kubernetes-gateway/\">https://www.NGINX-cn.net/blog/5-things-to-know-about-NGINX-kubernetes-gateway/</a>\"</p><p><a href=\"https://www.bilibili.com/video/BV1wh41187De/\">https://www.bilibili.com/video/BV1wh41187De/</a>\"</p><p><a href=\"https://thenewstack.io/nginxs-reference-architecture-for-kubernetes-microservices/\">https://thenewstack.io/NGINXs-reference-architecture-for-kubernetes-microservices/</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s/UPaA6uRTVn2Nu2qJKA9soQ\">https://mp.weixin.qq.com/s/UPaA6uRTVn2Nu2qJKA9soQ</a>\"</p><p><a href=\"https://www.nginx.com/blog/our-roadmap-quic-http-3-support-nginx/\">https://www.NGINX.com/blog/our-roadmap-quic-http-3-support-NGINX/</a>\"</p>",
    "publish_time": "2022-12-22 11:04:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据烟囱亟需打破，云原生融合数据库雪中送炭｜解读云原生数据库的 2022",
    "url": "https://www.infoq.cn/article/BkVptJA3JSK5cf14RN3e",
    "summary": "<p>作者 | 郑思宇</p><p>采访嘉宾 |<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247557818&amp;idx=1&amp;sn=b05c906938e2645bfaba7cdca23911a1&amp;chksm=fbeb5b75cc9cd263515dbd3e95ac770302709b1b3d43c5355bd4e9ba018e632db30d6792e9d5&amp;scene=27#wechat_redirect\">&nbsp;矩阵起源创始人及 CEO 王龙</a>\"</p><p></p><p></p><blockquote>据 Gartner 预测，到 2025 年，基于云原生平台的数字化业务比例将达到 95%，将带来云原生数据库市场的快速增长。毫无疑问，云原生数据库即将成为云上数据库使用的标准范式。云原生数据库使得传统数据库得以充分结合云服务的免运维、高弹性、高可扩展、高可用、高性价比优势，又顺应了云端应用大爆发的历史趋势，在过去几年成为 IT 领域的大热门方向，但总体仍处于高速发展的早期阶段。过去一年中，企业对于云原生数据库的使用有哪些需求？云原生数据库厂商又该如何满足这些需求？我们将带你回顾 2022 年云原生数据库领域的重要动态以及技术进展。在本文写作过程中，特此感谢矩阵起源创始人及 CEO 王龙提供的宝贵观点与洞察。&nbsp;&nbsp;</blockquote><p></p><p></p><h1>云原生数据库领域的挑战和进展</h1><p></p><p></p><h3>知易行难，数据库从云服务到云原生还有很长的过程</h3><p></p><p></p><p>在过去相当长的一段时间里，云服务中被使用最多的是虚拟机，也就是物理服务器在云端的虚拟化服务。IT 用户享受到的好处是最朴素的自服务、免运维、高可用、高弹性、高可扩展性和按使用付费，当然这些都只是在虚拟机和 OS 层面（Infrastructrue as a service）。</p><p></p><p>为了更好地满足数据库用户的需求，云厂商开始提供数据库服务（Database as a service，DBaas）。为了降低开发成本和用户的学习难度及应用迁移门槛，最早的 DBaaS 大多是基于 IaaS 的数据库运维自动化，本质是传统数据库系统平移到云端的一种全托管服务，用户看到的和使用的数据库，除了在访问 OS 层面有所限制，其他并无二致。</p><p></p><p>在云计算（IaaS）成为趋势并逐渐成为 IT 行业的标准基础设施之后，上述这种做法导致的问题便浮出水面：</p><p></p><p>问题一：数据本身的爆炸式增长，数据库无论在物理环境还是云端虚拟环境，都需要具备更高的扩展性和更强悍的性能。传统数据库的架构由于受限于历史原因，本身具有的缺陷和瓶颈在云端则更加明显。例如，进行性能优化和执行调度时，传统数据库在资源有上限的物理环境中更在意资源的使用率，并且需要投入大量精力来考虑 RTO 和 RPO，那么整个系统就会因此增加复杂度。然而，这种复杂度在云端是没必要的，并且会造成巨大的资源浪费和开发投入。</p><p></p><p>问题二：随着数字化程度的进一步加深，业务敏捷度也成为 IT 系统最关注的核心指标之一，数据库的弹性能力是支撑应用和业务敏捷度最重要的基础能力。基于物理环境设计的传统数据库，即使平移到云端，也无法充分利用云端计算和存储资源的高弹性能力。例如物理环境和云端环境的硬件特性，包括但不限于吞吐量、读写特性、资源部署时间、资源回收时间以及消耗成本，都是不同的。如果没有考虑到这些不同点，就会严重影响数据库实际的弹性能力。</p><p></p><p>问题三：由于大多数用户在数据库上的预算增长超过其他组件的增长，使其更加看重数据库服务的性价比。然而，多云部署日益流行的今天，每个云厂商的定价策略，计算、存储和网络资源的计价方式和能力表现都有或多或少的差异。如果仅仅将一个传统数据库平移上云，很难基于这些复杂的因素做出最优的判断决策，也无法达到最优性价比。</p><p></p><p>因此，用户和市场都需要一款能充分利用云端的计算、存储和网络资源特性，具备真正的高弹性、高可用和高性价比优势的数据库，这就是云原生数据库发展的最核心驱动力。要实现这样的目的，一个基于云环境精心设计、打磨的技术架构是不可或缺的。这就需要长期持续的投入，才能对云环境足够的了解并对数据库生态充分适应。</p><p></p><h3>需求多样，云原生数据库需要跟上现代软件应用的步伐</h3><p></p><p></p><p>曾经有一种说法是“软件吞噬世界”，暂且不管这种说法是否正确，但物理世界愈发数字化是一个正在发生的事实。过去数十年，数字化的加速催生了数以亿级的软件，包括手机 App、SaaS 等等，这些软件对数据库提出了各种各样的需求，也驱动全球出现了多达数百种不同的数据库。</p><p></p><p>十几年前，我们就知道数字化进程的一大阻碍就是数据烟囱（Data Silo），其严重制约了企业的智能化和创新能力，并以几何级数提高了企业利用数据的门槛和成本，一直以来都是企业信息化部门最头痛的顽疾之一。然而，随着云计算、软件应用和数据库的蓬勃发展，非但没有使这个顽疾得到缓解，反而变得愈发严重。为了解决这个问题，在过去数年中，IT 从业者做了多种尝试。</p><p></p><p>第一种，简单粗暴地投入最强硬件，搭载一款数据库支持所有软件应用。目前，这种方式已经越来越少见了，因为纯硬件的纵向扩展能力是有限的，而软件应用的组合和变化是无穷的，大部分客户无法负担成本变成了最主要的阻碍。</p><p></p><p>第二种，整合多种集中式和分布式数据库系统，使用统一界面给数据和应用开发者提供各种能力和服务，并隐藏其底层管理运维的复杂度，这种方式常常被称作“数据中台”和“数据底座”。</p><p></p><p>基于过往多年的实践，虽然证明这种形式在某些领域和客户群体中是有其价值的，但在某些场景下便会出现局限性：一方面，当后台需要整合的数据库系统过多，又或者前台的应用变化过快、过于复杂时，它的运维、管理和开发成本增长会陷入失控，且相比第一种方案会带来数据实时处理性能的瓶颈；另一方面，当用户需求相对较小和简单时，使用这种方案又会过于臃肿和复杂，投入产出不成正比。</p><p></p><p>第三种，一些成熟的数据库厂商退而求其次，如果不能一蹴而就地整合这么多数据库系统，那么可以尝试在现有成熟数据库系统中添加新的能力，来减轻一小部分数据烟囱带来的痛苦。</p><p></p><p>湖仓一体、批流融合、HTAP 等都是这种思路下的产物，但融合的挑战也是巨大的。拿 HTAP 举例，由于 TP 和 AP 数据库在过去默认就是服务两个不同的应用团队，由不同的数据库管理团队来维护，因此对安全性、资源共享和性能隔离等都有各自的需求。当融合在一起的时候，想要性能完全隔离，就应该使用完全独立的计算和存储资源；想要资源利用最大化，就应该使用共享计算和存储资源；同时想要获得更低的处理时延，就应该只存一份数据；想要各自都有读写极致性能，就应该存多份数据...... 这里有太多矛盾的技术点，要想找到平衡点来解决上述提到的技术矛盾是一件不容易的事情。</p><p></p><p>当然第三种尝试还有很多其他类型的方案，这里就不一一列举了。在 2022 年，我们也注意到有众多云原生数据库厂商在朝着这个方向演进：</p><p></p><p>Snowflake 在其年度用户大会 Snowflake Summit 2022 上，宣布推出 Unistore 存储引擎，使得用户在 Snowflake 平台上运行 OLAP 的同时也可以确保数据的完整性和一致性，而这是 OLTP 的核心特性之一。在 2022 re:Invent 大会中，亚马逊云科技发布了一个新服务——“Zero ETL”，其在后台打通了 Aurora 数据库和 Redshift 数据仓库。用户无需自己开发 ETL，就可以轻松地进行数据分析和机器学习，这更像是数据中台和 HTAP 的结合体。最近刚刚完成 F 轮融资的 SingleStore，也号称其数据库系统能在云上通过结合事务和分析工作负载，消除了性能瓶颈和数据移动，以支持数据密集要求苛刻的工作负载。国内初创公司矩阵起源提出的“HSTAP”更为彻底， 将 HTAP 进行了重新定义，融入了串联 AP 和 TP 的 Streaming 能力，并完全重新开发了一款云原生的融合性数据库。目标是让企业只用一款数据库，就能覆盖大中小应用系统的的 TP 和 AP 需求，并能用最高性价比的方式建设好数据中台。</p><p></p><h3>放眼未来，云原生数据库还需要关注基础设施和软件应用的新动向</h3><p></p><p></p><p>在基础设施层面，其能力也在不断演进。<a href=\"https://www.infoq.cn/theme/124\">《2021 InfoQ 年度技术盘点与展望》</a>\"的“架构篇”里提到。在 2021 年，各个云服务商都在不断拓展 Serverless 产品线的能力。在 Web 应用、微服务、事件处理和批处理任务等场景，Serverless 正在成为最流行的架构。在 2022 年，这个趋势依旧在延续，并且还在不断深化，代表企业有亚马逊云科技、阿里云等，这也意味着 Serverless 已经从概念和愿景逐步演进至落地实践。</p><p></p><p>Serverless 能更好的提供计算资源的弹性和可扩展性，从而提高应用系统的性价比。对于计算成本占比较多的数据库系统来说，是一个不可忽略的重要技术趋势。同时，能否利用好 Serverless 服务也是架构是否云原生的重要指标之一。相比之下，国外企业的上云速度与规模快于国内，其已经累计了大量的云上数据以及应用场景，可以帮助数据库厂商打磨云原生数据库能力。以 Snowflake 为代表的云原生数据库，目前已经到达了收获的阶段，在 Snowflake 公布的 2023 财年第三季度的财务数据显示，Snowflake 第三财季的总营收为 5.57 亿美元，同比增长 67%。而在国内，Serverless 在数据库中还没有得到大规模应用。</p><p></p><p>离商用化尚有距离的 CXL 也是云厂商关注的热点技术趋势，此外，GPU、FPGA 等异构芯片在云端的使用也在加速增长，这些对数据库的技术演进也会有较大影响。</p><p></p><p>在软件应用方面，智能制造、机器人、自动驾驶和元宇宙等新应用还在不断成长之中，数据库厂商也需要关注这些领域的需求增长和变化。这些应用场景相关的图数据库、区块链数据库、时序数据库等也在持续迭代。</p><p></p><h1>云原生数据库值得关注的其他方向</h1><p></p><p></p><p>虽然云原生数据库所带来的价值不言而喻，但坦白讲，其也很难形成一统天下的局面。</p><p></p><p>对于金融、政府这类对于合规安全有较高要求或者定制化需求较多的客户来说，私有化数据库部署的需求还是普遍存在的，但这也并不意味着云原生数据库的增长空间会被挤压。随着数字经济时代的到来，金融、政府等机构还有很多个性化的业务依旧需要部署在云上。</p><p></p><p>那么在这种情况下，混合云架构是很多企业的首选，据 Cisco 发布的报告显示，在全球混合云趋势调查中，82% 的受访者目前使用混合多云架构来支撑其应用程序，并表示混合多云架构能够使组织实现更敏捷和高扩展性的开发环境，同时加速业务敏捷性和创新。</p><p></p><p>云原生数据库厂商若想帮助这些企业管理好数据，让他们更好地进行降本增效、实现创新，就要求数据库架构既能能兼容不同的公有云，也能够进行私有化部署，并且还要保障云上和云下的用户体验一致，数据可以自由的、低成本流动。这就对云原生数据库提出了更高的要求，既能充分利用云上的资源及其特性，也能高效地利用私有数据中心的资源及其特性。从这个角度来看，数据库云原生的下一步或许会是异构云原生，即数据库无论部署在公有云、私有云、物理服务器上都要有统一的技术架构并且共享调度数据和资源。</p><p></p><p>除此以外，DB 和 AI 的结合也是一些团队正在研究的方向，主要分为 AI for DB 以及 DB for AI 两类。</p><p></p><p>第一种模式是 AI for DB，即利用 AI 算法帮助 Database 提升产品能力。比如通过分析数据库日志或者视图，得出数据库的性能表现，然后去动态地调整数据库参数，来达到优化数据库性能的目的。此外，还可以在数据库中利用一些 AI 算法来进行内核优化、运维优化等。</p><p></p><p>另一种模式是 DB for AI，即在非结构化数据日益增多的情况下，为分布式数据库提供 AI 分析的能力。目前来看，由于传统的应用开发工程师、数据分析师与 AI 工程师在企业是分割开的，后者还处于探索阶段。但相信随着数据库人才与 AI 人才的交流，共性的需求会变得越来越多，比如如何对数据做预处理、做初步的建模等等，待这些知识融合到一定程度时，必然会出现一个系统来满足他们的需求。</p><p></p><h1>云原生数据库的选型建议</h1><p></p><p></p><p>虽然国内数据库的发展相比国外起步得要晚一些，但凭借着后发优势、业务环境等因素，国内数据库市场也逐渐呈现出百花齐放的态势，粗略判断，目前云原生数据库数量已达到几十甚至上百种。</p><p></p><p>因此，企业在进行选型时，首先要考虑清楚自身选择云原生数据库的驱动力是什么，既要让云原生数据库的特性与自己的业务类型相结合，又要与自己的团队能力进行匹配，是更看重低运维自治，还是更看重资源的弹性扩展，或者更看重性价比，需求不同往往会做出不同的选择。为了避免踩坑，也需要考虑厂商的技术实力和周边生态支持的能力。</p><p></p><p>其次，企业在选型时还要做好中长期的规划，绝对不要低估数据烟囱对未来业务的影响。随着企业的发展和数据规模的不断攀升，数据库带来的成本增长会远超于业务应用的成本。如果提前做好规划，在数据库的选型与后续建设中就可以做到游刃有余，将来的数字化道路就会事半功倍。比如，由<a href=\"https://www.infoq.cn/article/Dm4VbCIzLpDADad7GIPy\">矩阵起源打造的新一代超融合异构云原生数据库 MatrixOne</a>\"，便可以帮助用户降低数据的使用难度，提供极简的使用体验，让企业可以将精力从繁杂的技术细节中释放出来，最终达到降本增效的目标。</p><p></p><h1>写在最后</h1><p></p><p></p><p>回顾这一年，云原生数据库领域依然在稳健高速的发展中。云原生数据库依然还需要时间来达到成熟，但无论是使用创新架构来更好地使用云上资源，还是融合多种能力更好地服务数据应用，各云原生数据库厂商都在沿着帮助客户降本增效的初心，持续进化中。</p><p></p><p>采访嘉宾：</p><p>王龙，矩阵起源（Matrix Origin）创始人及 CEO。曾任职腾讯云副总裁，管理数百人的大数据和人工智能团队，从零构建腾讯云 to B 大数据人工智能产品矩阵和商业化体系，三年内完成产品线从百万到十亿级收入的飞跃。王龙之前在美国硅谷和德国慕尼黑工作过，也在北京上海有过创业经历。他具有多年跨国企业级产品建设和商业化经验，对于云计算、大数据、人工智能和多个行业应用场景有着丰富经验和深刻认识。</p>",
    "publish_time": "2022-12-22 13:28:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "TAPD 思享汇 | 鱼快创领：多场景软硬件一体化的敏捷项目管理实践",
    "url": "https://www.infoq.cn/article/Wi1qNwfy65FVNz4aadDy",
    "summary": "<p>鱼快创领从成立到成长，需要从 0-1 搭建起覆盖全面的项目管理体系，为软硬一体的产品矩阵服务。通过引入 TAPD，并结合自身项目管理的实际需求，鱼快完成了敏捷管理实践落地与体系构建，并不断拓展出更多场景的项目管理功能需求与实践。</p>\n<h1>演讲大纲</h1>\n<ul>\n<li>鱼快在项目管理过程中面临的痛点</li>\n<li>从产品、项目和团队三个维度搭建软硬件一体化管理体系</li>\n<li>鱼快使用 TAPD 的收益</li>\n</ul>\n<h1>观众收益</h1>\n<ul>\n<li>如何利用TAPD 实现项目从“人治”到“工具治”再到“流程治”的转变</li>\n<li>项目组合管理助力软硬件一体化解决方案落地</li>\n</ul>",
    "publish_time": "2022-12-22 14:39:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "TAPD 思享汇 | 创维 VR：敏捷研发过程中的管理实践",
    "url": "https://www.infoq.cn/article/9UFVaoLmabMU2ROxMM70",
    "summary": "<p>创维 VR 产品线在 VR 快速研发迭代过程中遇到了软硬件一体化研发管理的挑战，通过引入 TAPD 这一工具，创维完成了研发流程的闭环升级，实现了研发流程标准化、研发管理的精细化及研发效率的提升。</p>\n<h1>演讲大纲</h1>\n<p>创维 VR 软硬件开发流程框架及挑战<br />\n创维 VR 软硬件一体化研发解决方案<br />\nTAPD 对创维 VR 软硬件一体化研发的价值</p>\n<h1>观众收益</h1>\n<ul>\n<li>VR 软硬件一体化项目研发过程中的挑战与解决思路</li>\n<li>TAPD+创维 VR 的项目管理场景实践</li>\n</ul>",
    "publish_time": "2022-12-22 14:39:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "TAPD 思享汇 | 元年科技：敏捷项目管理如何帮助我们提升研发质量",
    "url": "https://www.infoq.cn/article/C6qMiKc2B2N18XqLU98I",
    "summary": "<p>元年科技在高速发展中遇到了研发管理瓶颈，需要完整的研发流程管理。使用 TAPD 后，元年科技完成了研发流程的闭环落地，并通过 TAPD 的 API 量化质量管理，推动了产品研发的质量改进。</p>\n<h1>演讲大纲</h1>\n<ul>\n<li>元年科技开发流程框架及 TAPD 使用场景概览</li>\n<li>元年科技使用 TAPD 的重点功能场景解剖</li>\n<li>元年科技如何进行量化质量管理，高效交付业务价值</li>\n</ul>\n<h1>观众收益</h1>\n<ul>\n<li>TAPD 的基础功能在日常开发流程中的应用</li>\n<li>TAPD API 在实际工作中的使用</li>\n</ul>",
    "publish_time": "2022-12-22 14:39:55",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "倚天 710 ARM 芯片的 Python+AI 算力优化技术实践",
    "url": "https://www.infoq.cn/article/M3OtpBbNjpjKrxe9oqtF",
    "summary": "<p></p><blockquote>在刚刚结束的 PyCon China 2022 大会上，龙蜥社区开发者朱宏林分享了主题为《ARM 芯片的 Python+AI 算力优化》的技术演讲。本文为演讲内容整理。</blockquote><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d262d2a68dff07cc01f965942ac6174b.png\" /></p><p></p><p>我们的场景是 ARM 平台的和 AI 相关的任务，主要的目标是进行性能优化，具体来说我们首先关注的是深度学习推理任务（inference task），主要原因也是来自于业务需求。</p><p></p><p>这里说的 ARM 平台不是我们理解的终端设备，比如手机之类的，而是指服务端平台。在大家印象中，AI 任务，特别是深度学习的程序一般是跑在 GPU 或者 x86 的 CPU 上，出于功耗、成本、性能等因素的考虑，云厂商逐步开始建设 ARM 架构的服务平台，这是一个趋势。当然 ARM 平台还不是很成熟，许多软件还无法成功跑起来，更不要说提升性能了。</p><p></p><p>我们想要吸引一部分用户将AI应用从原先的 x86 平台上迁移到 ARM 平台上。这就要求 ARM 平台能提供更好的性能，或者更好的性价比。所以说如何整合 Python+AI 的相关软件使其发挥最好的性能成为了我们关注的重点。</p><p></p><p>下文的分享整体分为两部分，一部分是介绍我们进行的优化工作，主要是跟矩阵乘法相关的优化，第二部分是关于 Python AI 应用在 ARM 云平台-倚天 710 上的最佳实践。</p><p></p><h3>一、优化工作介绍</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc21f069a16f32b6e693f840a141a575.png\" /></p><p></p><p>前面说我们的优化是和矩阵乘法相关的，那首先需要说明为什么我们会关注到这个。</p><p></p><p>这里有一个绕不开的场景就是深度学习，不管是前几年知名的 <a href=\"https://www.infoq.cn/article/wjca3csPbWSIO56jaHM7\">AlphaGo</a>\"，还是当前火热的 <a href=\"https://www.infoq.cn/article/AWWsrfb54zTvglZ0I5qS\">ChatGPT</a>\"，都用到了大量深度学习的技术，深度学习本身只是AI的一个分支，但却影响广泛，不容忽视。所以我们从深度学习开始切入，从当前最广泛使用的深度学习框架，TensorFlow 和 PyTorch 开始。此外，我们还需要结合硬件场景，即前面说到的 ARM 服务端平台，对于阿里云来说就是结合倚天 710 芯片。</p><p></p><p>深度学习的实现中包含大量的矩阵乘法，甚至有文章直接写出矩阵乘法是深度学习的核心。举个例子，我们熟知的卷积操作，实际上经过一系列的转换后，输入特征和卷积核会被转换为两个矩阵，然后进行矩阵乘法，输出的结果再解码成特征图，就完成了卷积。除此以外，全连接层也由矩阵乘法实现，当前流行的 Transformers 结构，被包括 ChatGPT 在内的各类 NLP 模型所使用，也包含大量矩阵乘法操作。</p><p></p><p>我们可以看一些例子：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b1439f063c4f669e3dea5609c3f4008b.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/708c1aab9985ebe152bc649baa6259d3.png\" /></p><p></p><p>可以看到，像 AlexNet、ResNet-50 之类的模型，在进行推理时，大约 90% 计算耗时在执行矩阵乘法。即使对矩阵乘法做一些微小的优化，影响也是很广泛的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c347884950b9886d8b3fdab6ecc9569.png\" /></p><p></p><p>我们前面说的矩阵乘法，更准确的叫法是 GEMM，通用矩阵乘法，其实还包含系数和累加操作。但是时间复杂度仍然是 MNK 级别，主要还在于 AB 两个矩阵相乘。直观来看，深度学习涉及的矩阵乘法计算量很大，比如常见的卷积操作可能就涉及 5000 万次计算，所以优化就显得很有必要，右下图是最朴素的三层循环迭代法，这种做法通常非常慢，计算机科学家做了许多努力，从优化内存布局和利用向量指令出发，能够将性能提升 10 倍以上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5c/5cfb5550c903977671c1b4e953025889.png\" /></p><p></p><p>内存布局主要分两步，第一步是对矩阵进行分块，即对于一个超大的矩阵，我们并不是一个一个按顺序计算，而是将矩阵切分为一个一个小块，分小块计算。第二步是对分出的小块，内部的元素序列进行重排，例如原来是按行排列的矩阵，那可能第一行四个计算好了，就需要取第二行的前四个，但是要取第二行就需要指针移动很长的距离，很容易造成 cache 不命中，于是需要重排，使得他们在内存上连续。优化内存布局主要目的是为了增加 cache 命中率，减少访存次数。</p><p></p><p>其次是利用向量化指令，类似 AVX 对于 x86 设备，NEON 对于 ARM 设备。向量化指令本质上是为了同时对多个数据进行计算，例如我们要对四组数据分别进行乘法，那么常规情况下需要执行四次，如果将它们对应放入向量寄存器中，只需要一条向量化指令，就可以同时得出四个结果，计算效率得到提升。当然这个是需要硬件支持。</p><p></p><p>因为 AI 推理大量使用了矩阵乘法，如今也有许多硬件对矩阵运算进行了加速：</p><p></p><p>NVIDIA Volta 架构引入了tensor core，可以高效地以混合精度处理矩阵乘Intel AMX(Advanced Matrix Extensions) 通过脉动阵列在硬件层面支持矩阵乘ARM SME(Scalable Matrix Extension) 支持向量外积运算，加速矩阵乘</p><p></p><p>目前市面上尚没有可以大规模使用的支持 AMX 或者 SME 的硬件，在这个阶段我们应该如何优化 CPU 上的 AI 推理算力呢？我们首先要了解 BF16 数据类型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bd11daf4525bcd0b3bbd780fa9428be.png\" /></p><p></p><p>BF16(全称 Brain Floating Point)，是由 Google Brain 开发设计的 16 位浮点数格式。</p><p></p><p>相比传统的 FP16 位浮点数，BF16 拥有和 FP32 一样的取值范围，但是精度较差。但对于深度学习来说，较低的精度并不显著影响结果，而较低的表示范围则会显著影响模型训练的好坏。</p><p></p><p>此外，BF16 还具有转换方便的特点，BF16 和 FP32 的互转只需要截断或填充尾数即可。</p><p></p><p>使用 BF16 还可以节约一半的内存，紧凑的内存表示通常意味着更高的计算吞吐。</p><p></p><p>最后，我们也有了硬件指令支持，可以直接对 BF16 数据进行操作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4cd53658678aed043fdc6978d89d7325.png\" /></p><p></p><p>需要说明的是 BF16 的扩展包含在 ARMv8.6 设备上，当然<a href=\"https://www.infoq.cn/article/fWqoFNWr5SCpFd0r41tJ\">倚天 710</a>\" 是 ARMv9 的指令集，同样支持。</p><p>我们主要通过 BFMMLA 来进行矩阵乘法计算，例如对于包含 128bit 的向量寄存器的设备来说：</p><p></p><p>输入 A: 大小为 2*4 的 BF16 矩阵，按行存储输入 B: 大小为 4*2 的 BF16 矩阵，按列存储输出 C: 大小为 2*2 的 FP32 矩阵</p><p></p><p>BFMMLA 单指令完成 16 次乘法和 16 次加法，计算吞吐非常高。</p><p></p><p>当然这时候如果我们需要 C 是 BF16 类型的话，就需要应用转换指令，例如向量化指令 BFCVT，加速转换过程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c1123b0e9bd929248c289d7978e488f.png\" /></p><p></p><p>我们的目标还是给 tensorflow 和 pytorch 用户提供加速，这是整体的流程图，对于一个 AI 推理任务，实际上不论是 TensorFlow 还是 PyTorch 都不会自己直接去计算，而是叫个专门的计算后端，在 ARM 主要是两个，一个是 ARM Compute Library，另一个是 OpenBLAS，他们之间的关系如右图。</p><p></p><p>TensorFlow 在最近的版本中开始采用 oneDNN + ACL 作为计算后端，oneDNN 也是一层皮，实际的计算仍然是 ACL。用户实际上只需要设置一个环境变量，就可以在不该动代码的情况下获得 BF16 加速。这个改进是由 ARM 公司的研发人员首先完成了。具体操作例子如下：</p><p><code lang=\"null\"># 假设 resnet.py 包含用户写的模型推理的代码\nDNNL_DEFAULT_FPMATH_MODE=BF16 python3 resnet.py</code></p><p></p><p>PyTorch的情况比较复杂，PyTorch 支持 OneDNN + ACL，但无法很好的发挥性能，同时 PyTorch 支持 OpenBLAS 后端，因此可以通过 OpenBLAS 来享受 ARM bf16 扩展带来的性能收益。</p><p></p><p>OpenBLAS 的 BF16 的 GEMM 优化是由龙蜥社区理事单位阿里巴巴贡献的，于此同时，我们为了方便用户使用，也在 PyTorch 中加入了一个API，用户在模型执行前添加一行torch.set_float32_fast_math_mode(\"BF16\")，就可以获得 BF16 加速，不必修改其他代码（需要说明，这个api还没有合入PyTorch，所以目前要使用我们提供的pytorch镜像才可以获得）。操作例子如下：</p><p></p><p><code lang=\"null\"># ...\n\n# 在模型执行前设置fast math mode\ntorch.set_float32_fast_math_mode(\"BF16\")\n# ...\n# 执行模型\npred = model(x)\n# ...</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6ea8308267b3b3425480e5ef8b0d2169.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/77c881a6c84ce7460707199bdc36f4fc.png\" /></p><p></p><p>之后是一些性能测试的展示，我们测试了 OpenBLAS 纯矩阵计算的性能对比。分别记录了 GFLOPS 和执行时间两个指标。</p><p></p><p>然后测试 TensorFlow 和 PyTorch 的性能对比，在对比中，我们可以看到，得益于 BF16 扩展，最新的 ECS ARM 平台上的性能优于 x86 平台（g7）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/254f6be7cb1280b5ba4b33d085891598.png\" /></p><p></p><h3>二、Python AI应用在ARM云平台-倚天710上的最佳实践</h3><p></p><p></p><p>现在介绍一下在 ARM 平台，特别是倚天 710 的用户，使用 TensorFlow 或 PyTorch 的最佳实践。</p><p></p><p>要知道软件版本的选择十分重要，随意选择 tensorflow 或者 pytorch 包可能遭遇：</p><p></p><p>未适配 ARM 架构，安装失败软件未适配 BF16 扩展或者环境参数有误，无法发挥硬件的全部算力，性能打折需要精心选择计算后端，例如目前 pytorch下OpenBLAS 较快</p><p></p><p>在 TensorFlow 上，我们可以选择最新的两个官方版本， 2.10.1 或者 2.11.0（最新版本），才能够获得 ACL 的 BF16 加速。用户也可以选择阿里云的镜像，这个和 pip 安装的其实是一样的，没有区别。</p><p></p><p>对于 PyTorch 用户，官方版本只有在最新的 1.13.0 才能够获得 ACL 加速，但是正如前面所说的，实际性能并不突出。阿里云则提供了带最新 OpenBLAS 的 PyTorch，在 docker 拉取时标注 torch_openblas 就可以获得。此外，我们也提供了modelzoo 镜像，包含模型的测试代码和验证代码。</p><p>目前我们仍然在进行相关的工作，期待后续能为大家提供更加完善的镜像。欢迎大家入群一起探索相关技术。</p><p></p><p>AI SIG 主页地址：<a href=\"https://openanolis.cn/sig/AI_SIG\">https://openanolis.cn/sig/AI_SIG</a>\"</p>",
    "publish_time": "2022-12-22 14:45:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "苹果将对 Foundation 框架用 Swift 重写并开源，网友：iOS/macOS 应用程序用吗？",
    "url": "https://www.infoq.cn/article/k1AOoQERSp4dTdm6pwOt",
    "summary": "<p>&nbsp;</p><p>近日，苹果宣布将用 <a href=\"https://www.infoq.cn/article/2014/06/apple-publish-swift\">Swift </a>\"为所有平台创建一个统一的开源 Foundation 框架实现。通过 Foundation 的原生 Swift 实现，该项目将消除框架在 C 和 Swift 之间的转换成本，从而提高性能。该项目会为服务器端应用程序提供体量更小、细化程度更高的模块选项，同时将作为统一规范的 Foundation 实现的核心。</p><p>&nbsp;</p><p></p><h2>Swift 应用的基础</h2><p></p><p>&nbsp;</p><p>Foundation框架为 App和其他框架提供了基础的能力，Foundation 定义的类、协议和数据类型等在整个macOS、iOS、watchOS和tvOS SDK中通用。</p><p>&nbsp;</p><p>2016年，Swift -corelibs- Foundation项目启动了Foundation的开源 Swift 版本，即在Foundation开源的C实现上封装了一个Swift层。Foundation项目团队认为，随着Swift的发展，该框架发展战略也需要进行调整。Swift 是由苹果开发的现代通用语言。虽然用途颇多，但最主要还是用在 iOS 和 Mac 应用程序开发上。</p><p>&nbsp;</p><p>前苹果工程师<a href=\"https://www.infoq.cn/article/TDE7JG8ohgtvKJugftTG\"> Chris Lattner</a>\" 在2010 年开始构建 Swift 语言时，只是作为一个业余项目去做。当时，Lattner 在使用编程语言 C++ 时遇到了挑战。“C++ 是一种复杂的语言，”Lattner 为此花费了很大力气。“C++ 和 Objective-C 都不坏，它们是环境的产物。但我们可以做得更好。”</p><p>&nbsp;</p><p>Lattner 从 Objective-C、Rust、Haskell、Ruby、Python、C#、CLU等语言中汲取灵感，并完成了基础架构设计。当 Lattner 意识到 Swift 可能是更好的选择时，他开始寻求资金并在苹果内组建了一个团队进行研究。之后，他便带领开发小组陆续完成语法设计、编译器、运行时、框架、IDE 和文档等相关工作。</p><p>&nbsp;</p><p>经过多年迭代，现在根据官网数据，在编写应用程序时，Swift 相较Objective-C&nbsp;快 2.6倍，相较 Python 2.7 要快8.4倍。</p><p>&nbsp;</p><p>在 Swift 之前，构建 iOS 应用程序的主要语言是 Objective-C，但越来越多的 iOS 项目都开始用 Swift 编写。移动设备市场的持续增长也为Swift 的持续发展提供了助力。<a href=\"https://www.tiobe.com/tiobe-index/\">TIOBE公布</a>\"的12月编程语言流行指数排名中，Swift 排名15，超过 Objective-C 的第19名。</p><p>&nbsp;</p><p>除苹果外，现在 Lyft、Uber、Airbnb、Square 等公司都在使用 Swift。随着 Swift 开发需求的提升，Swift 开发者的收入也在提高。国外网站 DevJobsScanner 最近对全球超 1000 万个开发岗进行了调研，结果显示，Swift 入选了前十大高收入编程语言，排名第七。Swift 开发者的平均年薪为 11.4 万美元，但上限报价也能达到每年 23 万美元水平。</p><p>&nbsp;</p><p>如今，几乎所有的Swift项目都使用了 Foundation框架。随着 Swift 应用的更多，Foundation框架的重要性也不言而喻。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d43b06b560f92263b59162dfc8154170.png\" /></p><p></p><p>Foundation 框架在iOS系统中的位置</p><p>&nbsp;</p><p></p><h2>Foundation 框架将如何发展？</h2><p></p><p>&nbsp;</p><p>Foundation 框架发展愿景中的一大重要部分，就是为服务器端应用程序提供体量更小、细化程度更高的模块选项。Foundation 团队也从模块分类开始，简单介绍了下阶段的发展思路。</p><p></p><h4>模块拆分</h4><p></p><p>以下是官方初步整理的模块划分思路，并非最终版本，团队也正在征求社区的反馈意见。</p><p>&nbsp;</p><p>FoundationEssentials</p><p>&nbsp;</p><p>这些模块类型在大多数应用程序中都有所使用，而且不存在额外的系统依赖性。此包可能依赖于Collections或Algorithms等关键Swift包，但后续出现的新依赖项保证会在不过多影响Essentials整体大小的前提下添加。</p><p>&nbsp;</p><p>以下列出的各个类型都是在将Foundation打造成一个整体库，进而提供基本实用程序类、为设计模式提供先例，并在一定程度上摆脱操作系统依赖以增强可移植性。具体包括：</p><p>&nbsp;</p><p></p><blockquote>URL，Data，UUID，Date、DateInterval，PropertyListSerialization，JSONSerialization，PropertyListDecoder，JSONDecoder及编码器，NotificationCenter，AttributedString，SortDescriptor，Measurement，Dimension，Unit，ProcessInfo，UserDefaults (范围可能受限，例如某些类型的域)，FileManager，FileHandle，Process，Pipe，Bundle。</blockquote><p></p><p>&nbsp;</p><p>考虑到语言本身的进步，部分API也到了需要更新升级的时候。比如，团队认为Process这类API应该使用async/await。通过将其包含在Essentials包中，团队希望与社区共同推进API迭代。不过在短期之内，现有API仍将正常服务于依赖它们的项目。</p><p>&nbsp;</p><p>FoundationInternationalization</p><p>&nbsp;</p><p>以下类型主要强调更好地处理日期/时间，或者为用户呈现格式化数据：FormatStyle 协议和所有具体格式样式类型，Locale、Calendar、TimeZone、DateComponents，Locale-specific String extensions，CharacterSet（此API未来可能会重新设计或扩充，以更好地适应Swift String），URLResource，LocalizedError，Morphology。</p><p>&nbsp;</p><p>&nbsp;</p><p>FoundationNetworking</p><p>&nbsp;</p><p>FoundationNetworking&nbsp;模块已经从Foundation中分离出来，并将继续提供相同的网络API。团队已经确定了Essential类型、特别是URL，因此下一步就是在Swift当中统一FoundationNetworking&nbsp;实现，主要包括 URLSession、URLRequest、URLResponse及其他关联类型，HTTPCookie、HTTPURLResponse及其他关联类型。</p><p>&nbsp;</p><p>FoundationXML</p><p>&nbsp;</p><p>FoundationXML&nbsp;模块已经从Foundation中分离出来，并将继续提供相同的XML解析API。在确定了Essential类型和FoundationNetworking之后，下一步就是在Swift中统一FoundationXML的实现，主要包括：XMLDocument、XMLDTD、XMLDTDNode、XMLElement、XMLNode、XMLParser。</p><p>&nbsp;</p><p>FoundationObjCCompatibility</p><p>&nbsp;</p><p>以下类型主要用于同Darwin Foundation或遗留代码的交叉编译，主要包括：NSObject</p><p>NSValue、NSNumber、NSError、NSNull、Geometry types、NS/CGRect、NS/CGPoint、NSEdgeInsets等。</p><p></p><h4>微模块还是单体设计？</h4><p></p><p>为什么不把Foundation中的每个类型都拆分成单独的包，以便随时可以独立导入？团队认为最好的办法应该是，在每个模块一个包跟所有模块一个包之间达到最佳平衡。</p><p>&nbsp;</p><p>如果将每个组件都当作单独的模块，那模块间的关系数量就会迅速增长，而且要求确保各模块间每个接口都必须稳定且公开。一旦发现本应只用于“亲密”模块的接口实际也被用在其他地方时，可能会不经意间限制团队对整体API接口的未来改进空间。</p><p>&nbsp;</p><p>因此，团队决定用外部依赖项来划分模块。外部依赖项通常是二进制文件显著膨胀的根源，如果对依赖项的传递控制不当，可能导致下游客户端发生冲突。</p><p></p><h4>移除的类型</h4><p></p><p>在Darwin平台上，团队需要保持全部现有API接口的兼容性。但团队将新的统一实现重点放在了实用性最强的那些Swift API上。“这代表着一种重要的思路转变，特别是对swift-corelibs-foundation当初提出的100%源兼容目标的颠覆。”团队在博客中说道。Foundation的很多功能都被包含在了语言的直接支持内，所以新包暂时不考虑引入以下类型：</p><p>&nbsp;</p><p>RunLoop、Lock、OperationQueue、Stream、Port、Timer等，由结构化并发替代。NS前缀集合类型- 最初是出于兼容性考虑而提供，但实用性一直不大NSCoding,&nbsp;NSKeyedArchiver&nbsp;–&nbsp;由Codable替代Progress&nbsp;- 不存在外部依赖，但与结构化并发的重合还没有完全设计好</p><p>&nbsp;</p><p>在Darwin上，Foundation框架将继续通过C、Objective-C和Swift的组合维护各个类型的实现。</p><p></p><h2>结束语</h2><p></p><p></p><p>该消息发布后，社区很多开发者表示很开心看到这样的变化。开发者Joakim_Hassila 留言道：“作为一个明确表示避免使用 Foundation的人，我只想发表一个简短的正面说明——这看起来是一种非常实用的方法，并且基于可能的外部依赖关系构建模块很有意义。”</p><p>&nbsp;</p><p>但是，这只是一个开始。Foundation 团队还需要为开发者解决更多技术细节上的问题，还有解答诸如哪个基金会负责、iOS/macOS 应用程序是否会使用这个新的 Swift Foundation等项目后续发展上的疑问。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://www.swift.org/blog/future-of-foundation/\">https://www.swift.org/blog/future-of-foundation/</a>\"</p><p><a href=\"https://forums.swift.org/t/what-s-next-for-foundation/61939\">https://forums.swift.org/t/what-s-next-for-foundation/61939</a>\"</p><p><a href=\"https://www.businessinsider.in/tech/enterprise/news/chris-lattner-who-created-the-programming-language-swift-when-he-was-at-apple-was-blown-away-by-how-fast-it-grew-now-he-says-swifts-future-is-in-machine-learning-/articleshow/74290705.cms\">https://www.businessinsider.in/tech/enterprise/news/chris-lattner-who-created-the-programming-language-swift-when-he-was-at-apple-was-blown-away-by-how-fast-it-grew-now-he-says-swifts-future-is-in-machine-learning-/articleshow/74290705.cms</a>\"</p>",
    "publish_time": "2022-12-22 15:12:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java近期新闻：JDK 20的JEP提升为Targeted状态，亚马逊云科技推出Lambda SnapStart特性",
    "url": "https://www.infoq.cn/article/ZYvBqLEPNaY4FvJjyopx",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p>JEP 432，<a href=\"https://openjdk.org/jeps/432\">记录模式（第二次预览）</a>\"，已从JDK 20的Proposed to Target状态<a href=\"https://twitter.com/OpenJDK/status/1597674326425096192?cxt=HHwWgMCi1biViqwsAAAA\">提升</a>\"为Target状态。该JEP更新自JEP 405，<a href=\"https://openjdk.org/jeps/405\">记录模式（预览版）</a>\"，更新包括：增加了对通用记录模式类型参数推断的支持；增加了对记录模式出现在增强for语句条件判断中的支持；并删除了对命名记录模式的支持。</p><p>&nbsp;</p><p>JEP 433，<a href=\"https://openjdk.org/jeps/433\">switch模式匹配（第四次预览）</a>\"，已从JDK 20的Proposed to Target状态<a href=\"https://twitter.com/OpenJDK/status/1597674334771810304?cxt=HHwWgMCo7faViqwsAAAA\">提升</a>\"为Target状态。该JEP更新自JEP 427，<a href=\"https://openjdk.org/jeps/427\">switch模式模式匹配（第三次预览</a>\"），更新包括： 简化了switch标签语法；现在， switch表达式和语句以及支持模式的其他构造体都支持泛型类型模式和记录模式的类型参数推断。</p><p>&nbsp;</p><p>JEP 434，<a href=\"https://openjdk.org/jeps/434\">外部函数和内存API（第二次预览）</a>\"，已从JDK 20的Proposed to Target状态<a href=\"https://twitter.com/OpenJDK/status/1597674343168757760?cxt=HHwWgMCitbWWiqwsAAAA\">提升</a>\"为Target状态。该JEP在<a href=\"https://openjdk.java.net/projects/panama/\">Panama</a>\"项目的支持下不断演进：JEP 424，<a href=\"https://openjdk.org/jeps/424?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">外部函数和内存API（预览）</a>\"，在JDK 19中交付；JEP 419，<a href=\"https://openjdk.org/jeps/419?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">外部函数和内存API（第二个孵化器版本）</a>\"，在JDK 18中交付；以及JEP 412，<a href=\"https://openjdk.org/jeps/412?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">外部函数和内存API（孵化器）</a>\"在JDK 17中交付。该JEP提议基于反馈进行改进，并在JDK 20中提供第二次预览。更新包括：统一了 MemorySegment 和 MemoryAddress 接口，即，内存地址由零长度的内存段建模；并且增强了MemoryLayout 密封接口，以便于与JEP 427，<a href=\"https://openjdk.org/jeps/427?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Njc5NTcwNzQsImZpbGVHVUlEIjoiSm5mVnpmNG5ZdThBcDFsdyIsImlhdCI6MTY2Nzk1Njc3NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.9fQ4yI44wd5QQLZv8_85Xhtv4-M8HKKZQzqwaeEbaM4\">switch中的模式匹配（第三次预览）</a>\"一起使用。</p><p>&nbsp;</p><p>JEP 429，<a href=\"https://openjdk.org/jeps/429\">作用域值（孵化器）</a>\"，已从JDK 20的Candidate状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2022-November/007225.html\">提升</a>\"为Proposed to Target状态。这个<a href=\"https://openjdk.java.net/jeps/11\">正在孵化</a>\"的JEP，最初名为范围局部变量（孵化器，Extent-Local Variables），由<a href=\"https://wiki.openjdk.java.net/display/loom/Main\">Loom项目</a>\"赞助，提议在线程内部和线程之间共享不可变数据。这优于线程局部变量，尤其是在使用大量虚拟线程时。</p><p>&nbsp;</p><p>JEP 436，<a href=\"https://openjdk.org/jeps/436\">虚拟线程（第二次预览</a>\"），已从JDK 20的Candidate状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2022-November/007223.html\">提升</a>\"为建议Proposed to Target状态。在<a href=\"https://wiki.openjdk.org/display/loom/Main\">Loom项目</a>\"的支持下，该JEP基于JDK 19中提供的JEP 425，<a href=\"https://openjdk.org/jeps/425\">虚拟线程（预览版）</a>\"，提出了第二次的预览，以便有时间为该功能的演进提供更多的反馈和经验。需要注意的是，除了少量在JDK19中被固化的JEP 425 API外，本预览版本没有进行任何更改，因此没有在第二次预览中提出。</p><p>&nbsp;</p><p>JEP 437，<a href=\"https://openjdk.org/jeps/437\">结构化并发（第二个孵化器）</a>\"，在JDK 20中从Candidate状态<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2022-November/007224.html\">提升</a>\"为Proposed to Target状态。该JEP也是在<a href=\"https://wiki.openjdk.org/display/loom/Main\">Loom项目</a>\"的支持下，提议基于从JDK 19中提供的JEP 428，<a href=\"https://openjdk.org/jeps/428\">结构化并发（孵化器）</a>\"中重新孵化该特性，以便有时间提供更多的反馈和经验。唯一的变化是更新了 StructuredTaskScope 类，以支持在任务作用域中创建的线程继承作用域值。这简化了跨线程不可变数据的共享。</p><p>&nbsp;</p><p>这三个JEP的审查将于2022年12月6日结束。</p><p>&nbsp;</p><p></p><h4>JDK 20</h4><p></p><p>JDK20<a href=\"https://jdk.java.net/20/\">早期访问构建版本</a>\"中的第<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B26\">26</a>\"版也已于上周发布，其中包括对第25版各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2020%20and%20%22resolved%20in%20build%22%20%3D%20b23%20order%20by%20component%2C%20subcomponent\">问题</a>\"的修复和<a href=\"https://github.com/openjdk/jdk/compare/jdk-20%2B22...jdk-20%2B23\">更新</a>\"。有关该版本的更多详细信息，请参阅<a href=\"https://jdk.java.net/20/release-notes\">发</a>\"<a href=\"https://jdk.java.net/20/release-notes\">布</a>\"<a href=\"https://jdk.java.net/20/release-notes\">说明</a>\"。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"，鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java缺陷数据库</a>\"报告缺陷。</p><p>&nbsp;</p><p></p><h4>JavaFX 20</h4><p></p><p>JavaFX 20<a href=\"https://jdk.java.net/javafx20/\">早期访问构建版本</a>\"中的<a href=\"https://github.com/openjdk/jfx/releases/tag/20%2B10\">第10版</a>\"已提供给Java社区。JavaFX专为使用JDK 20早期访问构建版本而设计，应用程序开发人员可以在JDK 20上使用JavaFX 20构建和测试其应用程序。</p><p>&nbsp;</p><p></p><h4>Spring框架</h4><p></p><p><a href=\"https://spring.io/projects/spring-integration\">Spring Integration</a>\"&nbsp;6.0已<a href=\"https://spring.io/blog/2022/11/29/spring-integration-6-0-goes-ga\">发布</a>\"，其特性包括：JDK 17和Jakarta EE 9基线；支持GraalVM和Spring AOT引擎的原生镜像；具有千分尺和千分尺跟踪的可观测性仪器；以及对Jakarta EE 10的支持。有关该版本的更多详细信息，请参阅<a href=\"https://docs.spring.io/spring-integration/docs/current/reference/html/whats-new.html#whats-new\">最新动态页面</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-vault\">Spring Vault</a>\" 3.0也已<a href=\"https://spring.io/blog/2022/11/28/spring-vault-3-0-goes-ga\">发布</a>\"，其特点包括：JDK 17基线；支持额外的HTTP客户端，包括响应式JDK HTTP客户端；并支持使用版本化密钥/值机密引擎的Vault存储库。有关该版本的更多详细信息，请参阅<a href=\"https://github.com/spring-projects/spring-vault/wiki/Spring-Vault-3.0-Release-Notes\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-cloud\">Spring Cloud</a>\"&nbsp;2022.0.0的<a href=\"https://spring.io/blog/2022/12/02/spring-cloud-2022-0-0-release-candidate-3-codename-kilburn-has-been-released\">第三个候选版本</a>\"，代号为Kilburn，已向Java社区提供。该版本提供了对Spring Cloud子项目RC3版本的更新，如：Spring Cloud OpenFeign 4.0.0、Spring Cloud Commons 4.0.0、SpringCloud Function 4.0.0和Spring Cloud Starter Build 2022.0.0。但是，由于Spring Cloud CLI、Spring Cloud for Cloud Foundry和Spring Cloud Sleuth等子项目的移除，也引入了一些破坏性的更改。Spring Cloud 2022.0.0-RC3要求Spring Boot 3.0.0。有关该版本的更多详细信息，请参阅<a href=\"https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2022.0-Release-Notes\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>亚马逊云科技</h4><p></p><p>在最近的<a href=\"https://reinvent.awsevents.com/\">re:Invent</a>\"大会上，亚马逊云服务为其<a href=\"http://aws.amazon.com/lambda/\">AWS Lambda</a>\"项目<a href=\"https://aws.amazon.com/blogs/aws/new-accelerate-your-lambda-functions-with-lambda-snapstart/\">推出了</a>\"一项新特性<a href=\"http://docs.aws.amazon.com/lambda/latest/dg/snapstart.html\">Lambda SnapStart</a>\"，旨在减少Java函数的冷启动，并加速Lambda函数。<a href=\"https://quarkus.io/blog/quarkus-support-for-aws-lambda-snapstart/\">Quarkus</a>\"和<a href=\"https://micronaut.io/2022/11/28/leveraging-aws-lambda-snapstart-with-the-micronaut-framework/\">Micronaut</a>\"已经实现了对Lambda SnapStart的支持。更多详细信息请参阅<a href=\"https://www.infoq.cn/theme/164\">InfoQ的新闻报道</a>\"和亚马逊云科技<a href=\"https://aws.amazon.com/blogs/compute/reducing-java-cold-starts-on-aws-lambda-functions-with-snapstart/\">博客文章</a>\"。</p><p>&nbsp;</p><p></p><h4>Quarkus</h4><p></p><p>红帽（Red Hat）<a href=\"https://quarkus.io/blog/quarkus-2-14-2-final-released/\">发布</a>\"了Quarkus 2.14.2和2.13.5版本，主要修复了CVE-2022-4116漏洞，CVE-2022-4116是Dev UI配置编辑器中的一个漏洞，容易受到本地主机驱动攻击，导致远程代码执行。该版本还加强了对跨源资源共享（CORS）的处理，包括在CORS请求因来源无效而被拒绝时将 200 OK 更改为 403 FORBIDEN 。有关这些版本的更多详细信息，请参阅<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.14.2.Final\">2.14.2版</a>\"和<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.13.5.Final\">2.13.5版</a>\"的发布说明。</p><p>&nbsp;</p><p>在通往Quarkus 2.15.0的道路上，<a href=\"https://github.com/quarkusio/quarkus/releases/tag/2.15.0.CR1\">第一个候选版本</a>\"也提供了新特性，比如：与Quarkus CRaC/Firecracker的集成；将gRPC扩展迁移到新的Vert.x gRPC实现；使用Panache扩展支持在REST数据中根据命名查询进行过滤；以及对GraalVM 22.3.0、Mandrel 2.13和SmallRye GraphQL 1.9.0的依赖项升级。</p><p>&nbsp;</p><p></p><h4>Apache Camel</h4><p></p><p><a href=\"https://camel.apache.org/\">Apache Camel</a>\"&nbsp;3.18.4已<a href=\"https://camel.apache.org/blog/2022/12/RELEASE-3.18.4/\">发布</a>\"，对Spring Boot 2.7.6和HyperSQL DB 2.7.1进行了27个bug修复、改进和依赖项升级。有关该版本的更多详细信息，请参阅<a href=\"https://camel.apache.org/releases/release-3.18.4/\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JHipster</h4><p></p><p>在<a href=\"https://www.jhipster.tech/\">JHipster</a>\"，已经开始将平台升级到Spring Boot 3.0。Okta的开发人员倡导者、Java Champion和JHipster开发团队成员<a href=\"https://www.linkedin.com/in/mraible/\">Matt Raible</a>\"提供了团队迄今为止遇到的问题的<a href=\"https://twitter.com/mraible/status/1597248481596706816?cxt=HHwWgMDUuYHCyKosAAAA\">最新情况</a>\"，即：<a href=\"https://github.com/reactor/BlockHound/blob/master/README.md\">BlockHound</a>\"需要一个新的<a href=\"https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html\">WebFlux</a>\"的入口；健康检查在Elasticsearch 8+中不起作用；MongoDB和WebFlux与JUnit和MongoDB驱动程序4.2+死锁。有关这些问题的更多详细信息，请参阅此GitHub<a href=\"https://github.com/jhipster/generator-jhipster/pull/19791\">拉取请求</a>\"。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/12/java-news-roundup-nov28-2022/\">https://www.infoq.com/news/2022/12/java-news-roundup-nov28-2022/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/ATGFHsJa5HqNroEuNBLd\">对话Spring大神：Spring 生态系统的新时代来了！</a>\"</p><p><a href=\"https://www.infoq.cn/article/GGdb3Y7cO9Pw2Bf8NfIW\">Java 新闻汇总：Spring 发布，Resilience4j，Open Liberty，GlassFish，Kotlin 1.8-Beta</a>\"</p>",
    "publish_time": "2022-12-22 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI 训练加速原理解析与工程实践分享",
    "url": "https://www.infoq.cn/article/87779441d52c12cf8e4d8df87",
    "summary": "<p>本文整理自同名线上分享，是 12 月份「百度百舸 -&nbsp;<a href=\"https://cloud.baidu.com/product/CCE/cloud-native-ai.html?track=infoq01\">云原生 AI</a>\"」技术公开课的第 2 期。</p><p></p><p>这次分享将系统性的分析在&nbsp;AI&nbsp;模型训练过程中的主要性能瓶颈，以及当前针对这些瓶颈的主要的加速方案和技术原理，并介绍百度智能云在这方面的一些实践成果。</p><p></p><p>本系列一共有 4 期。</p><p></p><p>第 1 期：<a href=\"https://xie.infoq.cn/article/6fd784a4e75b7d3468d9c0ffd\">《云原生 AI 的资源调度和 AI 工作流引擎设计分享》</a>\"</p><p></p><p>今天的分享，主要包括三个部分：</p><p></p><p>首先介绍我们为什么需要做&nbsp;AI&nbsp;训练加速，也就是整体背景和出发点是什么；</p><p></p><p>第二部分我们会系统性的分析实际训练过程中的可能会遇到的性能瓶颈问题，然后针对这些问题，介绍目前主要的加速方案；</p><p></p><p>第三部分介绍<a href=\"https://cloud.baidu.com/solution/ai-heterogeneus-computing-platform.html?track=infoq01\">百度百舸</a>\"平台的&nbsp;AI&nbsp;训练加速套件&nbsp;AIAK-Training&nbsp;在一些模型训练加速上的实践效果。</p><p></p><h1>1.&nbsp;为什么需要&nbsp;AI&nbsp;训练加速？</h1><p></p><p>在&nbsp;AI&nbsp;系统中，一个模型从生产到应用，一般包括离线训练和推理部署两大阶段。</p><p></p><p>离线训练阶段，就是产生模型的过程，用户需要根据自己的任务场景，准备好训练模型所需要的数据集，以及神经网络算法。</p><p></p><p>算法可以理解为是一个高度复杂的非凸数学函数，函数中包括很多变量以及参数。模型训练的过程其实就是在学习神经网络模型中的参数。</p><p></p><p>模型训练开始后，会读取数据，然后送入模型进行前向计算，并计算与真实值的误差。然后执行反向计算得到参数梯度，最后更新参数。训练会进行多轮的数据迭代。</p><p></p><p>训练完成之后，我们会保存训练好的模型，然后将模型做上线部署，接受用户的真实输入，通过前向计算，完成推理。</p><p></p><p>因此，无论是训练还是推理，核心都是数据计算。 为了加速计算效率，一般都是通过&nbsp;GPU&nbsp;等异构加速芯片来进行训练和推理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/16cb2f4be7d06a483a1f8363ffa6a43a.png\" /></p><p></p><p>另外，从深度学习模型发展历程来看，为了能够持续突破模型的精度上限，模型参数量其实在快速的膨胀。然而更大的参数量，就会带来更大的计算复杂度。</p><p></p><p>下图左侧是摘自一篇公开的论文，从这篇总结里，我们看到在 2010 年之前，模型的计算量大约 20 个月翻一番。在 2010~2015 年，常规模型计算每 5-6 个月翻一番。而在 2015 年之后，衍生了大模型训练的趋势，计算量增长 10~100 倍。</p><p></p><p>模型训练对算力以及基础设施的要求越来越高，训练需要更多的算力，也需要更长的时间，这也导致了需要更多的资源成本。这里我们列举了一些论文或研究中公开的成本数据，反应了模型训练的费用是非常高昂的。</p><p></p><p>因此，如何稳定的进行模型训练，如何持续降本增效其实至关重要。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c610d4633ac8a5616d6691da0f4b668.png\" /></p><p></p><p>在这样的大背景下，百度智能云推出了百度百舸 · AI 异构计算平台，目标是为&nbsp;AI&nbsp;场景提供软硬一体化的解决方案。通过&nbsp;AI&nbsp;计算、AI&nbsp;存储、AI&nbsp;加速、AI&nbsp;容器四层技术栈，满足上层业务场景的需求。</p><p></p><p>AI&nbsp;计算层，提供了包括高性能的&nbsp;GPU、以及昆仑等异构芯片资源，以及高性能的&nbsp;RDMA&nbsp;或&nbsp;IB&nbsp;网络，以及自研的超级&nbsp;AI&nbsp;计算机&nbsp;X-MAN&nbsp;等；AI&nbsp;存储层，包括对象存储&nbsp;BOS 满足数据湖存储的需求、以及专为&nbsp;AI&nbsp;设计的高性能并行文件系统&nbsp;PFS；AI&nbsp;加速层，包括数据湖存储加速套件&nbsp;RapidFS，AI&nbsp;训练加速套件&nbsp;AIAK-Training，AI&nbsp;推理加速套件&nbsp;AIAK-Inference；AI&nbsp;容器层，也即是资源调度层，利用云原生的技术能力，满足&nbsp;GPU、AI&nbsp;作业等弹性调度的需求。云原生 AI 的内容在我们上一期的技术公开课有专门分享。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0de3889f5678d33f6b1142bb810dd13.png\" /></p><p></p><p>当我们考虑做性能加速的时候，第一个想到的可能是使用更好的硬件。</p><p></p><p>这会带来一定的性能提升，但是大部分情况下可能并没有充分发挥出硬件的计算能力，核心的原因就是训练代码的执行效率并没有调到最优或更优的状态。</p><p></p><p>首先，框架为了兼容更多的需求，一般会提供一些较为通用的优化能力，更多的会关注易用性、生态建设等；其次，算法工程师在设计算法时，核心的精力在如何提高模型的精度，性能效率关注不足，某些情况下框架提供的一些通用优化能力，也都没有利用起来。</p><p></p><p>因此，当我们决定使用某种模型算法时，为了达到较好的资源效率和训练效率，我们需要有意识的去优化。不过这里也有很多的技术挑战：</p><p></p><p>性能影响因素比较多，需要根据不同模型自身的运行特点进行分析，没有完全固定的优化方案；性能优化需要理解工程实现的原理，比如当我们去做异构芯片计算的优化，需要专业的异构研发经验才能开展，技术门槛较高；还有些情况，当我们做分布式训练时，可能还需要结合集群硬件和网络拓扑，来优化分布式训练场景下的扩展性问题。</p><p></p><p>这些挑战极大地影响了模型训练性能的调优，因此我们推出了 AIAK-Training&nbsp;的加速套件，期望通过抽象易用性的接口降低优化成本，并通过软硬协同的优化手段，来充分加速客户在百度智能云上的模型训练性能。</p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b65f7fe6649725f63406d37014a5015.png\" /></p><p></p><h1>2. 训练性能开销分析和加速方案</h1><p></p><p>在介绍 AIAK-Training&nbsp;具体效果之前，我们先介绍下训练加速这个话题下关键的技术思路和方案原理是什么样的。</p><p></p><p>因为模型训练优化本身是一个软硬件综合的工作，技术栈相对比较复杂，今天的内容肯定没办法涵盖全部细节，我们尽量把关键思路讲到。</p><p></p><p>首先我们看下当前的模型训练方案。过去的发展阶段里，模型训练方案关键有两个层次的变化，一是从单卡训练到分布式训练的变化，二是从数据并行训练到多维混合并行训练的变化。这里的核心驱动点一个是训练的数据量，一个是模型的参数量。</p><p></p><p>单卡训练方式：实际采用这种模式，一般都是模型参数量和数据量相对比较少，单卡的训练时间可以接受。模型参数规模需要保证在训练过程中，单张卡的显存能够满足存储的上限。按照新的&nbsp;GPU&nbsp;卡的显存容量配置，一般可以放置的最大规模在10 亿级参数量；当数据集规模比较大的时候，因为训练过程中需要多次遍历全量的数据集，单卡训练就会需要比较长的时间，这时可以从单卡扩展到多卡，通过分布式的方式来加快训练。</p><p></p><p>这里应用最广泛的就是数据并行。在数据并行方案下，数据集会被平均切分成多份，然后每张卡上保存完整的模型，各自独立并行处理切分后的子数据集。</p><p></p><p>当模型参数量足够大的时候，比如参数量达到百亿、千亿级别，单卡放不下完整的模型，这里又出现了模型并行、或者同时使用数据并行和模型并行的混合并行方案。</p><p></p><p>模型并行，会将模型切分到不同的卡上，每个卡上放置模型的一部分，这里又根据切分的方式不同，比如层内切分或层间切分，又细分了 Tensor&nbsp;并行和流水线并行的方式。</p><p></p><p>因为在一般模型训练中，使用更多的还是数据并行，我们下面重点还是以数据并行为例，来介绍性能优化的思路。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/91/91e7fc84b75c157cabc29b462b6e1c88.png\" /></p><p></p><p>我们从软硬件整体视角先理解下单卡训练过程中存在的性能开销。</p><p></p><p>下图左边是我们从软件角度上的训练流程。单卡训练的过程，主要包括数据读取、数据预处理，前向计算输出并计算 loss，根据 loss 函数反向计算，得到每一层参数的梯度，最后根据梯度更新模型参数。持续该过程，直到训练收敛。</p><p></p><p>下图右边，是一个简化的节点硬件拓扑图。最上面是数据存储，可以是本地存储，也可以是网络存储。然后是 CPU、内存，CPU&nbsp;下通过多个 PCIe Switch&nbsp;连接着&nbsp;8 张&nbsp;GPU&nbsp;卡，编号从 0~7，8 张卡之间通过&nbsp;NVSwitch 互联。不同的计算实例，硬件的拓扑结构会有不同。</p><p></p><p>当训练启动时，首先数据读取，涉及从存储介质中读数据到内存中，主要是存储 I/O 开销。根据存储介质，要读取的数据量不同，这部分时间开销也不同；接下来是数据预处理，主要是对读入的数据进行一些数据增强的操作，比如对图片进行&nbsp;resize，对比度、饱和度调整等等，这部分工作大多数情况下是在&nbsp;CPU&nbsp;上。当在&nbsp;CPU&nbsp;上完成数据预处理操作之后，需要从主机内存拷贝到&nbsp;GPU&nbsp;显存上，涉及到主机和设备之间的内存拷贝开销；然后开始前向计算、反向计算、参数更新，这部分更多操作主要是通过&nbsp;GPU&nbsp;来进行，主要时间花费在&nbsp;GPU&nbsp;计算上面。这个过程里也可能会穿插一些&nbsp;CPU&nbsp;上的操作，可能也需要做主机和设备内存之间的拷贝。</p><p></p><p>因此，从单卡角度看，主要存在 I/O、CPU&nbsp;预处理、CPU 和 GPU 之间数据拷贝，GPU 计算等方面的开销。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/223809b70fee3faed2cb01134b1820c0.png\" /></p><p></p><p>接着我们看下数据并行的过程。</p><p></p><p>下图左边还是训练的主流程，右边展示了一个 3 机 24 卡的训练集群的硬件拓扑，3 台机器通过网络互联。</p><p></p><p>前面的部分我们也介绍到了，在数据并行里每个设备并行独立地执行前向和反向计算过程，因此，每个训练进程也都会遇到前面讲的单卡训练中的性能开销问题。</p><p></p><p>数据并行为了保证和单卡训练在数学上等价，需要确保每张卡的模型参数在迭代过程中始终保持一致。这里一方面需要让各 GPU 卡的模型参数初始化状态一致，这个一般是在训练开始前，通过广播的方式将第一张卡上的参数状态广播到其他的卡。</p><p></p><p>而在训练期间，由于每个设备处理的数据不同，前向计算所得到的模型损失值也是不同的，因此还需要在每个设备反向计算出梯度之后，进行梯度的平均，以平均后的梯度值来更新模型参数，从而保证每张卡的模型参数在迭代过程中始终保持一致。</p><p></p><p>梯度平均涉及到通信的过程，包括节点内部卡之间的通信，以及跨节点的网络通信开销。这里的通信又包括同步通信和异步通信，不过为了保证模型训练的收敛，一般都是采用同步通信的方案，后续的优化工作也都是基于同步通信的方式来展开。</p><p></p><p>由上可知，数据并行相比单卡训练，主要增加了额外的通信开销。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d766af09a0d5b9ae21142879c970cd2.png\" /></p><p></p><p>通过前述分析，我们知道加速AI&nbsp;训练不单是某一方面的工作，需要从数据加载、模型计算、分布式通信等系统维度综合考虑。这里说的数据加载，包括数据&nbsp;I/O、预处理、内存拷贝等过程。</p><p></p><p>在具体的优化实践中，给定一个待优化的模型，加速模型训练就是要不断提升训练的总体吞吐（每秒可以训练的样本数）。这个过程，我们一般可以先分析单卡的训练吞吐，当单卡的训练吞吐提升上来之后，我们再看从单卡扩展到多卡，看看如何提升多卡的训练加速比。</p><p></p><p>首先单卡的训练优化，极限优化目标是全部时间都在 GPU&nbsp;计算上，加速器利用率 100%。当然实际很难完全达到这个状态，但是我们可以按这个指标来牵引或衡量我们的工作。</p><p></p><p>单卡性能优化关键包括两部分：</p><p></p><p>首先数据加载效率的优化。从存储系统上，我们可以使用更高性能的存储介质，或者基于这些高速存储介质组成的并行文件系统，或者说一些缓存加速系统。前面介绍到的，百度百舸也提供了相应的存储系统方案，比如 PFS、RapidFS 等。除此之外，还需要在框架&nbsp;dataloader&nbsp;中优化数据的读取过程。其次就是模型计算效率的优化。主要考虑如何优化计算实现，怎么提升计算单元的利用效率，这块可能就需要结合模型具体分析。</p><p></p><p>然后从单卡扩展到多卡，目标是如何达到线性加速比。线性加速比这个指标，简单来说就是从 1 张卡扩到 2 张卡训练时，训练的性能是否是单卡的 2 倍。</p><p></p><p>这里核心就是优化分布式的通信效率，一方面是硬件层面的优化，另外一方面在实际通信中，需要考虑怎么利用好网络的带宽资源，或者是否能够将通信过程进行隐藏等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f32f0cb244ea259d7a6640ec2876b619.png\" /></p><p></p><p>下面我们分别从这几个方面详细展开。</p><p></p><p>首先是数据加载方面的优化。</p><p></p><p>当我们实例化一个 dataloader&nbsp;之后，我们会持续迭代&nbsp;dataloader&nbsp;来读取一个&nbsp;batch&nbsp;的数据进行模型训练。</p><p></p><p>如果这里不做任何优化，如下图上半部分所示，每个 batch&nbsp;的数据加载过程和每个&nbsp;batch&nbsp;的模型训练过程，实际上是串行进行的。从&nbsp;GPU&nbsp;视角来看，就会出现因为数据加载导致的计算间隙，计算资源存在时间上的浪费。</p><p></p><p>除了前面讲到我们用更好的硬件直接提升数据读的效率之外，还可以怎么优化呢？</p><p></p><p>实际在 AI&nbsp;训练过程中，数据访问上有两个关键特征：</p><p></p><p>当数据集做完&nbsp;shuffle&nbsp;之后，每轮训练所需要的&nbsp;batch&nbsp;数据以及访问顺序是已知的；任意两个 batch&nbsp;的数据读可以并行，因为数据之间没有任何依赖关系。</p><p></p><p>因此，我们在不依赖硬件层面改动的时候，可以做的一个优化工作就是数据预取，当训练第一个 batch&nbsp;数据的时候，可以提前加载下一个&nbsp;batch&nbsp;数据，让&nbsp;I/O&nbsp;的过程和 GPU&nbsp;上的计算充分并行起来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/deddc7e285c779f368dfad1cf4a35671.png\" /></p><p></p><p>首先，我们需要利用好 dataloader&nbsp;中已有的优化方案，一是合理设置&nbsp;num_workers&nbsp;超参数，通过多进程的方式读数据，这一步可以实现数据从存储系统中预取到主机内存。二是从主机内存拷贝到&nbsp;GPU显存，可以通过&nbsp;pinned&nbsp;memory 的机制来加速。</p><p></p><p>大概介绍下 pinned memory&nbsp;加速的主要原理：内存数据有两种类型&nbsp;pageable memory&nbsp;和&nbsp;pinned memory，pageable memory 中的数据有被换出到磁盘上的可能。这种情况下，当执行 H2D 时，可能需要先从磁盘读到内存，然后从内存拷贝到显存。另外，pageable memory 数据拷贝到&nbsp;GPU&nbsp;显存时，需要先创建一个临时的&nbsp;pinned memory&nbsp;缓冲区，把数据从&nbsp;pageable memory 拷贝 pinned memory，之后才能传输到&nbsp;GPU&nbsp;上，也多了额外的数据搬运的操作。</p><p></p><p>不过当我们使能了上述方案后，我们仅实现了从存储系统到主机内存的预取，加快了主机到设备的数据拷贝速度。但是主机到设备的内存拷贝，和实际计算 kernel&nbsp;在&nbsp;GPU&nbsp;上还是串行执行，也就是&nbsp;GPU&nbsp;上依然存在少量的时间间隙。</p><p></p><p>AIAK 针对这个问题，又做了进一步的优化，可以实现&nbsp;H2D&nbsp;和前向计算的&nbsp;overlap。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e18ee49848c35071c3caa66d1e37fe31.png\" /></p><p></p><p>在数据并行场景下，还需要注意的一个事情，数据需要均衡的切分。</p><p></p><p>如果每个训练进程分配的数据不均衡，计算量就会不同，也就导致每个进程前向计算和反向计算完成的时间不同，那么先完成计算的进程，在反向过程中就会先进入到梯度通信环节中，但是因为 Allreduce 通信是同步通信操作，需要所有进程同时开始并同时结束，因此先开始通信的进程，会一直等待其他所有进程也发起了&nbsp;AllReduce&nbsp;后才能一起完成通信操作。这里就会出现因为快慢不一导致的资源空闲问题。</p><p></p><p>为了解决这种问题，需要每个进程使用相同的 batchsize&nbsp;来读取数据，另外每个&nbsp;batch&nbsp;的数据量要均衡。图像类数据一般会固定尺寸进行训练，而像&nbsp;NLP&nbsp;类模型需要处理变长的语句，可能需要进行特殊的处理，比如可以将数据&nbsp;padding&nbsp;到相同的长度，或者通过样本长度排序的方式来均衡分配等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/efdc94c0c9f82df32ee773cf5333b198.png\" /></p><p></p><p>下面介绍计算效率的优化。</p><p></p><p>计算包括前向、反向、参数更新。优化计算的目标，是为了能够充分发挥出异构硬件的算力，理想情况就是让 GPU&nbsp;芯片实际计算时的性能达到理论峰值。</p><p></p><p>我们先从一个单算子的角度分析，当我们准备在 GPU&nbsp;上执行一个计算操作的时候，简化的流程有四步。</p><p></p><p>首先在&nbsp;CPU&nbsp;上异步发射一个 GPU&nbsp;计算 Kernel；当 Kernel&nbsp;调度执行时，需要先从&nbsp;GPU 上的&nbsp;Global Memory&nbsp;读取计算所需要的数据;开始执行计算;当计算完成后，需要将计算的结果写回&nbsp;Global Memory。</p><p></p><p>根据计算、访存的开销占比，一般会将算子分类为计算瓶颈或者访存瓶颈。</p><p></p><p>当从一个算子扩展到一个完整的模型训练时，因为要连续执行非常多的计算 Kernel，那么&nbsp;Kernel 计算之间就会出现很多因为&nbsp;Kernel Launch、中间结果的读写导致的计算间隙问题。</p><p></p><p>由上可知，优化模型计算效率，需要从访存优化、计算优化、其他开销的优化综合考虑。</p><p></p><p>访存优化，主要考虑如何减少数据在显存和计算单元之间搬运的时间。从算子实现角度上，需要利用好 GPU&nbsp;存储层次架构，比如把数据搬运到更快的存储器上比如&nbsp;share memory，减少对&nbsp;global memory&nbsp;的访问，从而节省访存时间。或者做好计算指令和访存指令的 overlap，来提升计算访存比。而从单算子扩展到多算子时，还需要考虑如何减少中间结果的读写，这种问题一般可以通过算子融合的手段来优化；计算优化，计算瓶颈问题多半应该是没有正确对任务进行分块，或者没有利用好&nbsp;GPU&nbsp;并行计算的优势，导致并行度不高。还有可能没有使用合并指令集导致计算效率低下，或者没有使用 Tensor Core 等高性能计算单元，造成资源浪费。不同的问题，也对应着不同的优化手段；Kernel Launch&nbsp;等其他开销，大量时间花费在访存或计算之外，可以考虑的优化手段如算子融合、Cuda Graph&nbsp;等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2da6c1fd3702cdf7fe1806477803df8f.png\" /></p><p></p><p>首先是算子融合。算子在底层 GPU 执行时，会发起一次或者多次 Kernel Launch，Kernel 之间交互数据也需要经过显存，而算子融合就是将多个 GPU&nbsp;Kernel&nbsp;融合成一个大 Kernel，统一发起和执行。</p><p></p><p>因为减少了需要执行的算子数量，从而可以减少&nbsp;Kernel 调度和发起的开销；通过融合，可以通过寄存器等来传递中间结果，避免从&nbsp;global memory&nbsp;的来回搬运，极大降低了显存等待的时间；在某些场景中，可以通过算子融合，可以更充分的利用计算资源，提升资源和计算的效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1dea59cbb7693e23ec2db1b7a38e340b.png\" /></p><p></p><p>算子融合具体如何实现呢？</p><p></p><p>一种方式，分析模型中的低效操作，专家经验手写融合算子。在 GPU&nbsp;上主要就是&nbsp;CUDA&nbsp;算子研发，这里存在一定的门槛。AIAK-Training&nbsp;会针对典型的模型结构，或者客户需求，提供高效优化的算子实现。</p><p></p><p>另一种方式，就是编译优化的方案。通过编译的方式进行计算优化，以及代码自动生成，从而降低在不同硬件上的手工优化成本。不过当前很多编译方案更多还是针对推理优化，训练上的方案还在快速演进过程中。不过从极致性能角度看，未来一段时间依然离不开手写融合算子的工作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1dea59cbb7693e23ec2db1b7a38e340b.png\" /></p><p></p><p>下面介绍一些算子融合的实际案例。第一个是针对典型模型网络结构的优化。</p><p></p><p>下图展示了我们在 SwinTransformer&nbsp;模型中针对核心模块&nbsp;WindowAttention&nbsp;进行的计算融合优化。</p><p></p><p>WindowAttention 结构，核心操作公式如下图所示。计算过程中，需要依次执行&nbsp;7 个计算 Kernel。再加上一些&nbsp;reshape&nbsp;等转换操作，总共需要&nbsp;launch 10 个 Kernel。通过性能分析发现，实际执行过程中 launch kernel&nbsp;的间隔冗余开销占到了端到端 80% 以上的时间，导致该模块存在着较大的优化空间。</p><p></p><p>通过将这些 Kernel 融合成一个，整个模块的执行时间从&nbsp;392&nbsp;微秒减少到 13 微秒，单算子加速了 30 倍。整个模型的训练效率，端到端加速了 20%&nbsp;以上。</p><p></p><p>这个优化的核心思路，主要有三点：</p><p></p><p>是利用好&nbsp;GPU&nbsp;的三级访存流水：显存、share memory、寄存器；通过分块策略，将 2 个矩阵乘法和&nbsp;softmax&nbsp;进行融合；针对前向中间结果的优化，充分利用 Tensor Core，在反向计算中使用重计算代替重加载，使得访存开销极大降低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/db961c89c34a36a33b8455c5d69a728e.png\" /></p><p></p><p>下图是一个数据操作的融合举例，是在 FCOS3D&nbsp;模型中对于坐标压缩操作的一个优化。</p><p></p><p>通过性能分析发现，这个操作过程中存在大量 GPU 空隙，GPU&nbsp;利用率较低。该操作主要的功能是根据 index 将 3D-Tensor 压缩成 2D-Tensor。在原生实现中会先在host 端生成 index，再进行 H2D 拷贝，最后完成 Tensor 压缩，这会造成额外的拷贝和等待开销。</p><p></p><p>为此，我们重新实现这部分操作，核心思路主要就是将操作全部迁移至 GPU&nbsp;上，直接在 GPU&nbsp;上完成&nbsp;index&nbsp;的生成和 Tensor&nbsp;的压缩，减少&nbsp;CPU&nbsp;参与，同时避免了非必要的&nbsp;CPU-GPU&nbsp;之间的内存拷贝。</p><p></p><p>单算子执行时间从 9.69 毫秒减少到 32 微秒，加速了 300 倍，整个模型端到端训练提升了 10% 以上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5e5ecbdb4e34de892ee3d24e056b48e8.png\" /></p><p></p><p>下面我们介绍另一个计算优化的思路，就是提高计算的并行度，充分利用 GPU&nbsp;并行计算的优势，同样借助一些实际案例来介绍。</p><p></p><p>我们发现在一些模型中，有一些操作是串行执行的。比如在一些目标检测模型里，在 loss&nbsp;计算过程中，有些操作并不是按照一个&nbsp;batch 进行操作，而是&nbsp;for-loop&nbsp;每张图片或一个样本，这种情况下，当我们去提升&nbsp;batchsize&nbsp;的时候，因为这里的串行，可能没法达到我们想要的性能效果。</p><p></p><p>以 YOLOv7 中的&nbsp;SimOTA&nbsp;操作举例，原生实现中，是通过 for-loop 遍历一个 batch 的每一张图片，然后为图片的&nbsp;gtbox&nbsp;执行 SimOTA 标签分配。这种串行的实现方式导致该部分操作的 GPU&nbsp;利用率非常低效。</p><p></p><p>而每张图片处理时数据之间是没有依赖的。因此，我们做的一项工作就是将串行计算改成 batch&nbsp;并行计算，通过对一个&nbsp;batch&nbsp;的数据进行并行化的标签分配，从而加速这部分计算的效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0df8a4e9a495569530812a51c1e6d9e.png\" /></p><p></p><p>最终效果上，SimOTA 操作的耗时从 384&nbsp;毫秒下降到&nbsp;69 毫秒，计算效率提升&nbsp;5.5 倍，整个模型的端到端训练效率提升了&nbsp;18%&nbsp;以上。</p><p></p><p>模型训练过程中也有其他的类似场景，比如说参数更新。参数更新时，默认也是通过循环的方式，遍历每个参数，然后每个参数都会启动一个参数更新的 Cuda Kernel，然后依次执行。</p><p></p><p>针对这种情况，AIAK 也增加了&nbsp;FusedOptimizer&nbsp;的优化，通过融合参数更新的算子，实现批量化更新参数，大幅减少 Kernel Launch&nbsp;次数。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7b/7bf72f108a9c782230df7b9e9ef408ab.png\" /></p><p></p><p>下面介绍了另一个优化手段 CUDA Graph，主要是为了减少 CPU Launch Kernel&nbsp;的开销。</p><p></p><p>CUDA Graph 是在 CUDA 10 版本中引入的特性，可以将一系列&nbsp;CUDA Kernel 封装成单个单元，可以通过一次&nbsp;CPU Launch&nbsp;操作来启动多个&nbsp;GPU Kernel，从而减少了CPU Launch Kernel&nbsp;的开销。</p><p></p><p>如下图所示，默认情况下 CPU 需要依次发射多个 Kernel，如果 Kernel 计算时间比较短，Kernel&nbsp;之间的 Launch 间隙可能成为性能瓶颈。通过 CUDA Graph，仅需要花费一些额外时间构建 Graph，后续通过一次&nbsp;Graph&nbsp;的发射，即可大幅缩短实际执行时 Kernel 之间的间隙。</p><p></p><p>现在很多框架也都增加了对&nbsp;CUDA Graph&nbsp;的支持，通过插入一些代码来使K能这个功能。不过也有一些使用上的限制，比如不支持动态&nbsp;shape、不支持动态控制流、过程中不能捕获 CPU 操作等等，可以根据模型情况，尝试使用下这种优化能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fa/fae5d85daaf4cf3dc713b475bfc8fdd7.png\" /></p><p></p><p>下面介绍最后一个计算优化手段，充分利用 Tensor Core&nbsp;计算单元。</p><p></p><p>一个 GPU&nbsp;内一般包含多个&nbsp;SM，每个&nbsp;SM&nbsp;包括不同数据类型的计算核心，以及各类存储资源等。下图左边所示，是一个&nbsp;NVIDIA A100 SM&nbsp;的示意图，一个&nbsp;SM&nbsp;包含&nbsp;64&nbsp;个&nbsp;FP32 CUDA Core，以及 4 个 Tensor Core。</p><p></p><p>模型训练时，默认情况主要是用到 FP32 CUDA Core&nbsp;计算，而 Tensor Core&nbsp;是一块特殊的硬件执行单元，是从&nbsp;Volta&nbsp;系列&nbsp;GPU&nbsp;开始引入，主要是用来加速矩阵或卷积的操作效率。</p><p></p><p>相比 FP32 CUDA Core&nbsp;一次只能执行两个标量的计算，Tensor Core&nbsp;可以一次对两个矩阵执行计算，因此&nbsp;Tensor Core&nbsp;的计算吞吐远高于&nbsp;FP32 CUDA Core。</p><p></p><p>在 A100 中，Tensor Core&nbsp;支持了多种浮点数据类型，对于深度学习训练来说，可能涉及到包括&nbsp;FP16、BF16、以及&nbsp;TF32 模式。</p><p></p><p>TF32 主要用于单精度训练场景，相比&nbsp;FP32 训练，在保持相同的访存带宽需求下，理论计算吞吐量提高了&nbsp;8&nbsp;倍。</p><p></p><p>FP16/BF16 主要用于混合精度训练场景，相比&nbsp;FP32 训练，访存需求减少了一半，理论计算吞吐量提高了&nbsp;16 倍。</p><p></p><p>使用 Tensor Core&nbsp;的方式，可以使用底层的&nbsp;cublas&nbsp;或&nbsp;cuda&nbsp;接口进行编程，而对于算法开发者来说，更直接的就是使用框架中提供的&nbsp;TF32 训练或混合精度训练方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/4536bf9eb4e22c77013b478084b6dc43.png\" /></p><p></p><p>首先是 TF32&nbsp;训练模式，TF32&nbsp;是&nbsp;Ampere&nbsp;开始引入。</p><p></p><p>TF32 在浮点数的表达中，有&nbsp;8 个指数位，10 个尾数位和 1 个符号位。指数位与 &nbsp;FP32 相同，即数据表示范围相同，但是尾数位低于&nbsp;FP32，和&nbsp;FP16&nbsp;相同。</p><p></p><p>需要注意的是，TF32 不是一个对外开放的数值类型，只是&nbsp;Tensor Core&nbsp;的一种计算模式，也就是用户不能去直接创建一个 TF32 类型的浮点数。</p><p></p><p>当使能 TF32 的时候，Tensor Core&nbsp;计算矩阵或卷积操作时，会自动将&nbsp;FP32 转换成&nbsp;TF32，计算完成之后，输出的数据类型依然是&nbsp;FP32 类型。</p><p></p><p>TF32 训练在某些框架版本中是默认开启，某些框架版本中可能需要通过环境变量或者参数配置来手工开启，具体需要参考框架的用户手册。</p><p></p><p>不过由于 TF32 相比&nbsp;FP32&nbsp;来说，精度范围降低了，实际训练时还需要关注对模型收敛精度的影响。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/edb98b60f1cf002fbbf3c4ab3d020d6b.png\" /></p><p></p><p>混合精度训练是指在尽可能减少模型精度损失的情况下，使用 FP32 和&nbsp;FP16&nbsp;混合精度进行训练。</p><p></p><p>混合精度训练的收益主要有：相比 FP32 训练，内存需求减少，可以训练更大的网络或使用更大的 batchsize。使用更少的内存带宽，可以加速数据传输，半精度的计算也可以让数学运算效率更快；</p><p></p><p>不过因为 FP16&nbsp;的指数位和尾数位的范围都比&nbsp;FP32&nbsp;要少，因此数值表示范围和精度都会有降低，在实际使用的时候，就可能出现因为表示范围狭窄导致的数值溢出问题，或者因为精度不足导致舍入误差。</p><p></p><p>为了优化类似问题，混合精度训练方案中有几个关键的技术工作：</p><p></p><p>算子黑白名单机制，框架使用黑白名单自动为算子选择精度，模型训练过程中，会自动的插入 cast&nbsp;操作进行类型转换，不需要开发者干预。针对数值精度敏感的计算，依然使用&nbsp;FP32 来算，而对于数值安全的计算，比如矩阵乘，则会使用 &nbsp;FP16&nbsp;来计算；训练过程中，会存储一份 FP32 的权重参数，用于训练时候的参数更新，优化舍入误差的问题；针对&nbsp;FP16&nbsp;容易溢出的问题，使用了 Loss scaling&nbsp;的方案，比如对&nbsp;Loss&nbsp;放大 n 倍，根据链式法则，梯度也会随之放大 n 倍，从而使其落到&nbsp;FP16&nbsp;的表示范围内，具体过程如下图左侧所示。</p><p></p><p>目前所有框架都已经支持混合精度。AIAK-Training 组件进一步引入 NVIDIA Apex&nbsp;中 &nbsp;AMP O2 混合精度模式，这个模式会更加激进的将更多计算转&nbsp;FP16&nbsp;来加速训练。相比默认&nbsp;O1 模式，速度会有进一步提升，不过精度可能会受影响，需要结合具体模型验证。</p><p></p><p>AIAK-Training 提供兼容 torch amp 原生用法的使用方式，方便使能&nbsp;O2 模式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc9de4606456d50fafe98baac4e73b8d.png\" /></p><p></p><p>下面介绍通信优化，这个也是一个非常大的话题，涉及到内容也非常多。</p><p></p><p>前面也介绍了，通信主要是分布式训练中引入的，因为从单卡扩展到多卡，需要进行多卡之间的一些数据同步，这些同步操作就是通过通信来实施的。</p><p></p><p>下图列了一个通信优化的整体架构：</p><p></p><p>最底层就是网络协议层，包括传统的&nbsp;TCP&nbsp;网络、以及在训练场景用的越来越多的&nbsp;RoCE&nbsp;或&nbsp;IB&nbsp;的 高性能&nbsp;RDMA&nbsp;网络。这些底层网络方案，百度百舸也都提供了支持。显然通过改善硬件基础设施来提升网络带宽、降低延迟，是最直接有效的优化通信性能的方法。然后是通信库层。因为需要使用底层的网络协议来进行通信，并且实际应用时可能涉及多种通信原语，比如点对点通信、集合通信等，因此也出现一些高度封装并且优化的通信库。在&nbsp;GPU&nbsp;训练场景中，我们一般都是使用&nbsp;NCCL&nbsp;通信库，性能比较优。基于底层通信库，上层框架可以比较方便的构建分布式训练的通信架构。常见的包括参数服务器架构，集合通信架构。目前&nbsp;CV/NLP&nbsp;等领域的模型，主要都是采用集合通信的架构方式。通信策略层，主要是在应用层上做一些通信效率的优化，比如通信隐藏、通信融合、通信压缩、通信降频等不同的思路，这些方式大部分也可以叠加一起使用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3c3462b6047ca6d81f516509b04fc2f.png\" /></p><p></p><p>我们先看通信策略层面的优化思路，首先是通信隐藏优化。</p><p></p><p>在数据并行中，梯度同步通信是在训练的反向过程中进行的，当反向算出梯度之后，就可以进行全局的梯度平均。</p><p></p><p>如果不做任何的机制优化，反向计算和通信就会串行的进行，就会存在计算上的时间间隙。</p><p></p><p>由于反向过程中，上一个梯度的通信和下一个梯度的计算，两者之间没有任何数据依赖，因此可以让上一个梯度通信和下一个梯度计算并行起来，让两者的耗时相互重叠，从而可以隐藏部分的通信耗时。</p><p></p><p>在实现层面上，通常是将通信和计算算子调度到不同的 cuda&nbsp;流上实现的，通信算子调度到通信流，计算算子调度到计算流，不同流上的算子可以并行发射执行，从而实现反向中梯度通信和计算的并行重叠。</p><p></p><p>目前这个优化能力，在框架中都是默认开启的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/626821bb1dfb3556cc3918ebbed85a7b.png\" /></p><p></p><p>其次，通信融合优化。</p><p></p><p>默认情况下，模型中的每个梯度都需要发起一次通信操作。如果单个梯度的 size&nbsp;比较小，那么小数据包在实际通信时，网络带宽的利用率就会非常低，通信性能较差。</p><p></p><p>通信融合，就是将将多个梯度融合到一起统一进行一次通信，从通信开销模型上分析，既能提升带宽利用率，又能减少通信的初始化延迟项。</p><p></p><p>现在很多分布式训练框架中，也默认都支持梯度融合的策略，不同框架实现方式有一定的区别，有的实现需要先进行梯度协商确定通信顺序，有的则是直接通过静态的通信分桶。</p><p></p><p>虽然框架中默认支持通信融合，但是梯度融合的大小一般可以通过参数来配置，用户可以根据物理环境和模型的需求，调节合适的融合阈值，应该可以取得更佳的收益。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9b/9bb09cef514eeb09a955397affd64edc.png\" /></p><p></p><p>如果在网络带宽较低的训练场景中，比如低带宽的 TCP&nbsp;环境中，梯度同步的延迟可能会成为训练的主要性能瓶颈。</p><p></p><p>这种情况下，可以考虑的一个优化手段就是通信压缩。通信压缩，主要有三种不同的压缩思路：量化压缩，比如使用更低精度来表示梯度，这种方式的压缩率较低，最大能从 &nbsp;32 位压缩到 1 位，也就是最大压缩 32 倍。稀疏化压缩，典型的算法比如&nbsp;DGC&nbsp;算法，核心思想是每轮迭代只传输重要的梯度，也就是梯度数值超过设定的某一个阈值，同时为了减少信息损失，会把剩下不重要的梯度在本地进行累积，只要时间足够，最终累积梯度就会超过所设定的阈值，再进行梯度交换。通过这种方式减少通信的数据量，降低对网络带宽的需求。同时为了减少对模型收敛的影响，还通过动量校正、梯度裁剪、动量因子掩蔽、预热训练等不同方式来缓解。这种方案目前主要支持&nbsp;SGD&nbsp;优化器。低秩矩阵压缩方式，典型的算法比如&nbsp;PowerSGD，核心思路是将一个大的梯度矩阵分解成多个小梯度矩阵，通过传输小矩阵来减少通信量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/188065df087f998e72d830b726b2fd20.png\" /></p><p></p><p>通信降频优化，最简单的思路就是增大 batchsize，每次迭代更多的数据，减少了迭代次数，也就是减少了通信量。</p><p></p><p>不过 batchsize&nbsp;也不是越大越好，越大的&nbsp;batchsize&nbsp;可能会导致模型收敛精度下降或者收敛速度变慢。针对类似问题，业界也提出比如&nbsp;LARS、LAMB 优化器算法，通过分层自适应调节学习率，缓解类似问题，AIAK-Training&nbsp;也增加了支持。</p><p></p><p>为了增大 batchsize，如果显存比较充足，可以直接调整&nbsp;batchsize&nbsp;超参。如果显存比较吃紧，还可以通过梯度累加的方式，跳过若干次梯度通信，实际也相当于增大了&nbsp;batchsize。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9eba83279fa5dae4805604e05d27020d.png\" /></p><p></p><p>下面介绍了一种针对通信拓扑的优化方案——分层拓扑通信，也是针对机间网络带宽比较低的情况。</p><p></p><p>通过分层通信，可以充分的利用机内的高的互联带宽，同时弱化机间低网络带宽的影响。</p><p></p><p>AIAK 中也实现了这种通信方案，在 25Gbps TCP&nbsp;环境下，实测 4 机 32 卡&nbsp;SwinTransformer&nbsp;训练，通过分层&nbsp;allreduce，性能可以加速&nbsp;85%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/468b3bc0d35be78cd9e408b07d58bde8.png\" /></p><p></p><p>最后介绍一个底层通信库层面的优化，GPU Direct RDMA 通信技术，这个技术需要硬件环境上支持&nbsp;RDMA&nbsp;网络。</p><p></p><p>RDMA 通信，允许本地应用程序直接读写远程应用程序的用户态虚拟内存，整个通信过程，除了最一开始提交发送请求这一步需要 CPU 的参与，其余都是由网卡硬件完成的，不需要内存拷贝、系统中断以及软件处理，因此可以实现极致的低时延和高带宽。</p><p></p><p>而在 GPU&nbsp;场景中，GPU Direct RDMA&nbsp;技术进一步增加了&nbsp;RDMA&nbsp;直接访问&nbsp;GPU&nbsp;显存的支持，避免通信的时候，数据在&nbsp;GPU&nbsp;显存和主机内存中来回拷贝，跨机通信延迟进一步降低。</p><p></p><p>不过在实际的案例中，我们发现有些用户购买了 RDMA&nbsp;的环境，但是实际并没有用起来&nbsp;GDR&nbsp;技术，导致通信效率不高。这里列出了几个关键的配置项，如果有类似问题的话，可以依次进行排查和设置。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c84a3288c6fcba0024e363e04122a401.png\" /></p><p></p><p>前面介绍了当前的一些主要的性能优化思路和方案，整体来看，无论 I/O 优化、计算优化、通信优化，最朴素的优化思路主要就是如何优化操作本身、或者是否可以减少操作发生的次数，或者是否可以操作和其他的过程并行起来从而隐藏开销等。</p><p></p><h1>3. AIAK-Tranining 加速套件实践</h1><p></p><p>前面介绍了很多的优化工作，要想正确的使能，需要每个用户对于框架的工程实现原理比较清晰。为了简化训练优化的成本，我们构建了 AIAK-Training&nbsp;这个加速套件。</p><p></p><p>AIAK-Training 会围绕数据加载、模型计算、通信等方面，构建全链路优化能力，同时我们会把这种优化能力，封装成简单易用的接口，用户插入几行代码，即可比较方便的集成使用。同时，我们也在构建自动化策略组合调优的机制，自动帮助用户选择有效的优化策略。</p><p></p><p>具体使用时，加速库组件可以独立的安装部署，也可以直接使用我们提供的容器镜像。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/21b9996a25e08348e96db10e8260a464.png\" /></p><p></p><p>下面是一些具体的应用案例。</p><p></p><p>下图所示，主要是针对 dataloader&nbsp;的优化。这个场景中，模型比较小，数据集规模也比较小，纯计算的速度其实比较快，但是跨 EPOCH 的数据加载时间比较长，导致 I/O 耗时成为了主要的瓶颈。</p><p></p><p>通过使用&nbsp;AIAK&nbsp;中提供的进程复用、以及充分预取机制，整个模型训练加速了 &nbsp;166%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/38d6aade82708fbf5646be318dc1f694.png\" /></p><p></p><p>下图是一个针对模型计算优化的案例。</p><p></p><p>Transformer 类模型训练场景，实际训练时通信扩展性接近线性，I/O 耗时占比也非常低，计算是主要的性能瓶颈。</p><p></p><p>针对这个模型，AIAK-Training 进行了一系列计算层面的优化，包括主要结构的算子融合、混合精度、大 batch&nbsp;调优等等，整个模型的训练效率提升了 169%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b8e82ae0732373b98b1ba908c9685882.png\" /></p><p></p><p>下图的案例主要是应用了通信层面的优化，在云上 TCP&nbsp;环境中使能针对低带宽网络的优化策略，在一些经典模型上比如 resnet50、bert、vgg16，可以加速&nbsp;26%~78%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f442f1fb8fd70ac93dd3e5094b5cf0cf.png\" /></p><p></p><p>在自动驾驶场景中，我们也针对典型的 2D&nbsp;视觉、3D&nbsp;视觉、激光雷达，以及前融合类的模型，做了一系列的模型训练性能优化，训练性能加速&nbsp;49%~ 391%。如果对这一类算法的加速感兴趣，也欢迎后续和我们交流。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f589b9b14435b07967ea13b1392b3228.png\" /></p><p></p><p>- - - - - - - - - - END - - - - - - - - - -</p><p>请关注微信公众号“百度智能云技术站”</p><p>以免错过后续精彩内容</p>",
    "publish_time": "2022-12-22 15:35:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Dynamo Data Transform开源：从手动走向自动化",
    "url": "https://www.infoq.cn/article/VMjsABfE05iVTdeAXEYZ",
    "summary": "<p>在设计自助开发工具产品时，通常会有一些限制——最常见的限制之一可能是规模。我们要确保我们的产品 Jit（一个安全即代码 SaaS 平台）规模化，我们不可能事后再来考虑这个问题，需要从写第一行代码开始就进行设计和处理。</p><p></p><p>我们希望工程师们能够专注于开发应用程序和提升用户体验，而不是让他们不断地被规模化等问题所困扰。在调研了能够帮助我们团队实现目标的基础设施之后，我们选择了 AWS 和基于无服务器的架构。</p><p></p><p>AWS Lambda 正在成为 SaaS 系统的热门选择，因为它的工具套件，包括支持这些系统的数据库（AWS DynamoDB），在扩展性和性能方面提供了许多好处。</p><p></p><p>它的主要优点之一是它已经是 AWS 生态系统的一部分，抽象了许多管理和维护方面的操作任务，例如维护与数据库的连接，并且只需要做很少的设置就可以在 AWS 环境中启动它。</p><p></p><p>作为一家快速增长的 SaaS 公司，我们需要基于用户和客户的反馈进行快速演化，并将其嵌入到我们的产品中。应用程序设计中的许多变更对数据结构和模式都有直接的影响。</p><p></p><p>随着应用程序设计和架构的快速演化，我们经常需要在 DynamoDB 中进行数据转换，对于现有的用户来说，我们的优先级是实现零停机数据转换。（在本文中，数据转换指的是将数据从状态 A 修改为状态 B）。</p><p></p><p></p><h2>数据转换挑战</h2><p></p><p></p><p>用终极格斗冠军赛运动员 Brendon Moreno 的话说：</p><p></p><p></p><blockquote>也许不是今天，也许不是明天，也许不是下个月，但我敢保证，总有一天你会需要进行数据转换。</blockquote><p></p><p></p><p>尽管数据转换在软件工程和数据工程中是一个已知的不变的事实，但要无缝地进行数据转换仍然是一个痛点。目前，我们还无法简单地通过可管理的编程方式在 DynamoDB 中实现数据转换，这令人感到惊讶。</p><p></p><p>虽然数据转换有很多种形式——从替换现有项的主键，到添加 / 删除属性，再到更新现有索引等等（这些只是其中的几个例子），但如果不使用临时或一次性脚本，仍然无法简单地通过可管理和可重复的方式进行这些转换。</p><p></p><h4>User 表数据转换示例</h4><p></p><p></p><p>下面，我们将深入探究一个使用生产数据进行数据转换的真实示例。</p><p></p><p>我们以将“FullName”字段拆分为“FirstName”和“LastName”两个字段为例。在下面的示例中，数据聚合操作将用户的名字写入表中的“FullName”字段。我们想要对 FullName 进行转换，将其分割为 FirstName 和 LastName 两个字段。</p><p></p><p>转换前</p><p><img src=\"https://static001.geekbang.org/infoq/46/466dafdf2c967cdbaf1e795f4e9c332a.png\" /></p><p></p><p>转换后</p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef3558b078da1bfdffacf31e8a74acec.png\" /></p><p></p><p>看起来很简单，对吧？事实并非如此。要实现这个简单的转换，需要在业务逻辑端执行以下这些步骤，才能成功地转换这个字段。</p><p></p><p>扫描 User 表记录；从每个记录中提取 FullName 属性；将 FullName 属性拆分为新的 FirstName 和 LastName 属性；保存新记录；清理 FullName 属性。</p><p></p><p>但在开始转换之前，我们需要考虑一些问题，例如，如何在不同的应用程序环境中执行和管理这些转换？特别是当访问每一个环境并不符合安全最佳实践时。此外，你还需要考虑服务依赖关系。例如，如果有另一个依赖于这种特定数据格式的服务，你应该怎么办？你的服务需要向后兼容，并且仍然需要向外部服务提供相同的接口。</p><p></p><p>如果在生产环境中有客户端，在修改代码之前需要问问自己——如何进行零停机的维护？</p><p></p><p>为了避免停机，你需要做好测试和验证的计划。如何测试数据转换脚本？对生产数据进行可靠的数据转换演练有哪些好的实践？</p><p></p><p>在转换数据之前，有很多事情需要考虑。</p><p></p><p>通常，这些操作在很大程度上都是手动完成的。这是一个多么容易出错、乏味的过程啊！看来我们需要一个细粒度的流程来防止出错，并帮助我们管理这些步骤。</p><p></p><p>为了避免手动出错，我们需要定义一个流程来帮助我们解决上面的挑战。</p><p></p><h4>重写流程</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f30316d9e9957c08cb048dc1bc23689e.png\" /></p><p></p><p>图 1 重写流程图</p><p></p><p>首先，我们调整后端代码，在保留旧格式的同时将新格式的数据写入数据库。先写入 FullName、FirstName 和 LastName 三个字段，确保向后兼容。如果出现了严重错误，我们可以恢复到以前的格式。</p><p><code lang=\"text\">async function createUser(item) {\n   // FullName = 'Guy Br'\n   // 'Guy Br'.split(' ') === ['Guy', 'Br']\n   // 对于这个示例，假设FullName在名和姓之间有一个空格\n   const [FirstName, LastName] = item.FullName.split(' ');\n   const newItemFormat = { ...item, FirstName, LastName };\n   return dynamodbClient.put({\n       TableName: 'Users',\n       Item: newItemFormat,\n   }).promise();\n};</code></p><p><a href=\"https://gist.github.com/Guy7B/070701d73964987733a12cee422fc4da.js\">GitHub链接</a>\"：<a href=\"https://gist.github.com/Guy7B/070701d73964987733a12cee422fc4da.js\">https://gist.github.com/Guy7B/070701d73964987733a12cee422fc4da.js</a>\"</p><p></p><p>接下来，我们写了一个数据转换脚本，扫描数据库中的旧记录，并将 FirstName 和 LastName 属性附加到每条记录上，参见下面的示例。</p><p></p><p><code lang=\"text\">async function appendFirstAndLastNameTransformation() {\n  let lastEvalKey;\n  let scannedAllItems = false;\n\n  while (!scannedAllItems) {\n    const { Items, LastEvaluatedKey } = await dynamodbClient.scan({ TableName: 'Users' }).promise();\n    lastEvalKey = LastEvaluatedKey;\n\n    const updatedItems = Items.map((item) =&gt; {\n      const [FirstName, LastName] = splitFullNameIntoFirstAndLast(item.FullName);\n      const newItemFormat = { ...item, FirstName, LastName };\n      return newItemFormat;\n    });\n\n    await Promise.all(updatedItems.map(async (item) =&gt; {\n      return dynamodbClient.put({\n        TableName: 'Users',\n        Item: item,\n      }).promise();\n    }));\n\n    scannedAllItems = !lastEvalKey;\n  };\n}</code></p><p><a href=\"https://gist.github.com/Guy7B/fe2154630dfce753ac28c0ddb8c185c1.js\">GitHub链接</a>\"：<a href=\"https://gist.github.com/Guy7B/fe2154630dfce753ac28c0ddb8c185c1.js\">https://gist.github.com/Guy7B/fe2154630dfce753ac28c0ddb8c185c1.js</a>\"</p><p></p><p>在写完脚本（这是比较容易的部分）之后，我们现在需要验证它是否执行了它应该执行的任务。为此，下一步是在测试环境中运行这个脚本，并确保它的行为符合预期。只有在确认了脚本的可用性之后，才能让它在应用程序环境中运行。</p><p></p><p>最后一步是清理，包括冒险将 FullName 列从数据库中完全删除。这样做是为了清除不再使用的旧数据格式，并减少混乱和未来对数据格式的滥用。</p><p></p><p><code lang=\"text\">async function cleanup() {\n  let lastEvalKey;\n  let scannedAllItems = false;\n\n  while (!scannedAllItems) {\n    const { Items, LastEvaluatedKey } = await dynamodbClient.scan({ TableName: 'Users' }).promise();\n    lastEvalKey = LastEvaluatedKey;\n\n    const updatedItems = Items.map((item) =&gt; {\n      delete item.FullName;\n      return item;\n    });\n\n    await Promise.all(updatedItems.map(async (item) =&gt; {\n      return dynamodbClient.put({\n        TableName: 'Users',\n        Item: item,\n      }).promise();\n    }));\n\n    scannedAllItems = !lastEvalKey;\n  };\n };</code></p><p><a href=\"https://gist.github.com/Guy7B/e56e170bba337f02e3dc91c3241c8430.js\">GitHub链接</a>\"：<a href=\"https://gist.github.com/Guy7B/e56e170bba337f02e3dc91c3241c8430.js\">https://gist.github.com/Guy7B/e56e170bba337f02e3dc91c3241c8430.js</a>\"</p><p></p><p>我们来快速回顾一下我们在这个过程中所做的事情：</p><p></p><p>调整后端代码，写入新的数据格式；创建一个更新所有记录的数据转换脚本；在测试环境中验证脚本；在应用程序环境中运行脚本；清理旧数据。</p><p></p><p>这个定义良好的流程帮助我们在数据转换过程中构建了安全性和护栏。正如我们前面提到的，在这个过程中，我们能够通过保留记录的旧格式来避免停机，直到我们不再需要它们。这为更复杂的数据转换提供了良好的基础和框架。</p><p></p><h4>使用外部资源转换现有全局二级索引（GSI）</h4><p></p><p></p><p>现在我们有了一个流程——老实说，真实世界中的数据转换几乎没有这么简单的。我们假设一个更可能的场景，即数据是从外部资源（如 GitHub API）中摄取的，而更高级的数据转换场景可能要求我们从多个数据源摄取数据。</p><p></p><p>我们来看看下面的这个例子，看看它是如何工作的。</p><p></p><p>在下面的表中，GSI 分区键为 GithubUserId。</p><p></p><p>对于这个数据转换示例，我们希望向现有表中添加一个“GithubUsername”列。</p><p></p><p>转换前</p><p><img src=\"https://static001.geekbang.org/infoq/c9/c934948822f4fddd7c1390e6df731eb5.png\" /></p><p>转换后</p><p><img src=\"https://static001.geekbang.org/infoq/42/421bc79d1a8d3a0445eb2533fb5d4780.png\" /></p><p></p><p>这个数据转换看起来和上一个示例一样简单，但有一点小变化。</p><p></p><p>如果我们没有这个信息，该怎么获得 Github 用户名？我们必须使用外部资源，在这里就是 Github API。</p><p></p><p>GitHub 提供了一个简单的 API 来获取这些数据（你可以参考 文档）。我们通过 GithubUserId 获取用户信息，其中就包含我们想要的 Username 字段。</p><p></p><p><a href=\"https://api.github.com/user/:id\">https://api.github.com/user/:id</a>\"</p><p></p><p>基本流程与上面的示例类似：</p><p>调整代码，使用新格式写入数据；假设我们在创建用户时有 Github 用户名；扫描用户记录（使用‘GithubUserId’调用 Github API 获取每条记录的‘GithubUsername’），并更新记录；在测试环境中运行脚本；在应用程序环境中运行脚本。</p><p></p><p>不过，与之前的例子相比，这个例子有一个问题，就是它不够安全。如果在进行数据转换时调用外部资源出了问题该怎么办？也许外部资源发生崩溃或 IP 阻塞，或者因为其他原因不可用？在这种情况下，你可能会遇到错误或只执行了部分转换，或者出现其他数据问题。</p><p></p><p>我们能够做些什么来让这个过程更安全呢？</p><p></p><p>虽然你可以在发生错误时恢复脚本，或者尝试修复脚本中的错误，但如果能在生产环境中运行脚本之前先使用外部资源数据进行演练是最好不过了。提前准备数据是一种更安全的措施。</p><p></p><p>下面是更安全的流程设计：</p><p></p><p>调整代码，使用新格式写入数据（创建 GithubUsername 字段创建一个用户）；为执行转换准备数据。</p><p></p><p>然后，我们扫描用户记录，使用 Github API 获取每个用户的 GithubUsername，将其附加到 JSON 对象“{ [GithubUserId]: GithubUsername }”，然后将该 JSON 写入文件。</p><p></p><p>这个流程是这样子的：</p><p><code lang=\"text\">async function prepareGithubUsernamesData() {\n  let lastEvalKey;\n  let scannedAllItems = false;\n\n  while (!scannedAllItems) {\n    const { Items, LastEvaluatedKey } = await dynamodbClient.scan({ TableName: 'Users' }).promise();\n    lastEvalKey = LastEvaluatedKey;\n\n    const currentIdNameMappings = await Promise.all(Items.map(async (item) =&gt; {\n      const githubUserId = item.GithubUserId;\n      const response = await fetch(`https://api.github.com/user/${githubUserId}`, { method: 'GET' });\n      const githubUserResponseBody = await response.json();\n      const GithubUsername = githubUserResponseBody.login;\n\n      return { [item.GithubUserId]: GithubUsername };\n    }));\n\n    currentIdNameMappings.forEach((mapping) =&gt; {\n      // append the current mapping to the preparationData object\n      preparationData = { ...preparationData, ...mapping };\n    });\n\n    scannedAllItems = !lastEvalKey;\n  };\n\n  await fs.writeFile('preparation-data.json', JSON.stringify(preparationData));\n};</code></p><p><a href=\"https://gist.github.com/Guy7B/48306ad4acfb1e7136b039635013d25b.js\">GitHub链接</a>\"：<a href=\"https://gist.github.com/Guy7B/48306ad4acfb1e7136b039635013d25b.js\">https://gist.github.com/Guy7B/48306ad4acfb1e7136b039635013d25b.js</a>\"</p><p></p><p>接下来，我们扫描用户记录（使用准备好的数据，通过 GithubUserId 获取每条记录的 GithubUsername），然后继续更新记录。</p><p></p><p><code lang=\"text\">async function appendGithubUsername() {\n  let lastEvalKey;\n  let scannedAllItems = false;\n\n  while (!scannedAllItems) {\n    const { Items, LastEvaluatedKey } = await dynamodbClient.scan({ TableName: 'Users' }).promise();\n    lastEvalKey = LastEvaluatedKey;\n\n    const updatedItems = Items.map((item) =&gt; {\n      const GithubUsername = preparationData[item.GithubUserId];\n      const updatedItem = currentGithubLoginItem ? { ...item, GithubUsername } : item;\n      return updatedItem;\n    });\n\n    await Promise.all(updatedItems.map(async (item) =&gt; {\n      return dynamodbClient.put({\n        TableName: 'Users',\n        Item: item,\n      }).promise();\n    }));\n\n    scannedAllItems = !lastEvalKey;\n  };\n};</code></p><p><a href=\"https://gist.github.com/Guy7B/15149835c7f4b18d368c69a1960ef6b1.js\">GitHub链接</a>\"：<a href=\"https://gist.github.com/Guy7B/15149835c7f4b18d368c69a1960ef6b1.js\">https://gist.github.com/Guy7B/15149835c7f4b18d368c69a1960ef6b1.js</a>\"</p><p></p><p>最后，与前面的流程一样，我们在测试环境中运行脚本，然后在应用程序环境中运行脚本。</p><p></p><h2>Dynamo 数据转换</h2><p></p><p></p><p>在构建了一个可靠的数据转换流程之后，我们就会明白，要想消除手动错误，最好的办法就是将其自动化。</p><p></p><p>我们意识到，手动流程可能适用于小规模的场景，但不具备可伸缩性。这不是一个可行的长期解决方案，它最终会随着组织规模的扩大而发生崩溃。因此，我们决定构建一个工具来帮助我们自动化和简化这个过程，这样一来，数据转换在我们产品的增长和演进中就不再是一个可怕和痛苦的过程了。</p><p></p><h2>使用开源工具进行自动化</h2><p></p><p></p><p>数据转换就是一段段的代码，帮助我们执行特定的数据库变更，但这些脚本最终必须出现在代码库中。</p><p></p><p>我们因此能够做一些重要的操作：</p><p></p><p>跟踪数据库的变化，每时每刻都可以了解数据的历史。这有助于排查错误和问题。不需要重新发明轮子——重用已经编写好的数据转换脚本可以简化流程。</p><p></p><p>对数据转换过程进行自动化基本上可以让每个开发人员都能够执行数据转换。虽然你可能不应该将生产环境的访问权限授予组织中的每一个开发人员，但应用数据变更总是发生在整个流程的最后一英里。当只有少数人能够访问生产环境，他们不需要承担编写脚本的繁重工作，但需要验证脚本并在生产环境中运行它们。我们知道这样做会消耗更多的时间，而且不安全。</p><p></p><p>当代码库中的脚本及其执行过程通过 CI/CD 管道进行自动化后，其他开发人员就可以检查它们。基本上，任何人都可以在所有环境中执行数据转换，从而缓解了瓶颈。</p><p></p><p>现在，我们了解了在代码库中管理脚本的重要性，我们还希望为每个数据转换开发人员创造最佳的开发体验。</p><p></p><h2>让每个开发人员都能进行数据转换</h2><p></p><p></p><p>每个开发人员都喜欢专注于他们的业务逻辑——很少有上下文中断和变化。这个工具可以帮助他们专注于业务逻辑，而不必在每次需要执行数据转换时都从头开始。</p><p></p><p>例如，dynamo-data-transform 提供了以下这些好处：</p><p></p><p>对大多数数据转换都有用的导出功能；数据转换脚本的版本控制；支持演练，可以轻松地测试数据转换脚本；在转换出错时回滚——很难轻松恢复到之前的状态；交互式 CLI——为了提升开发体验并与开发工作流保持一致。你可以使用命令 dynamodt up 来运行脚本，用 dynamodt down 进行回滚，用 dynamodt history 来显示执行了哪些命令。</p><p></p><h2>DynamoDT 示例</h2><p></p><p></p><p>无服务器安装：</p><p></p><p>dynamo-data-transform 可以作为一个独立的 npm 包使用。</p><p></p><p>要使用 DynamoDT，首先通过 NPM（也可以通过其他方式）来安装这个包：</p><p><code lang=\"text\">npm install dynamo-data-transform --save-dev</code></p><p></p><p>接下来，将这个工具添加到无服务器中：</p><p><code lang=\"text\">npx sls plugin install -n dynamo-data-transform</code></p><p></p><p>你也可以选择手动将它添加到 serverless.yml 中：</p><p><code lang=\"text\">plugins:\n  - dynamo-data-transform</code></p><p></p><p>你还可以通过执行这个命令</p><p><code lang=\"text\">sls dynamodt --help</code></p><p></p><p>来查看 DynamoDT 支持的所有特性。</p><p></p><p>我们现在使用 DynamoDT 运行一个示例。我们从代码库的代码示例中选择了一个示例 v3_insert_users.js，你也可以使用在这里找到的其他示例进行测试（https://github.com/jitsecurity/dynamo-data-transform/tree/main/examples/serverless-localstack/data-transformations/UsersExample）。</p><p></p><p>我们通过运行命令</p><p><code lang=\"text\">npx sls dynamodt init --stage local</code></p><p></p><p>对包含了相关表的数据转换目录进行初始化。</p><p></p><p>对于无服务器，serverless.yml 中将会出现：</p><p><code lang=\"text\">resources:\n Resources:\n   UsersExampleTable:\n     Type: AWS::DynamoDB::Table\n     Properties:\n       TableName: UsersExample</code></p><p></p><p>生成的目录 data-transformations 中有一个模板脚本（https://github.com/jitsecurity/dynamo-data-transform#data-transformation-script-format）。</p><p></p><p>使用模板脚本生成的数据转换文件夹可以在这里找到。</p><p></p><p>我们将模板文件 v1_script-name.js 中的代码替换为：</p><p><code lang=\"text\">const { utils } = require('dynamo-data-transform');\n\nconst TABLE_NAME = 'UsersExample';\n\n/**\n * The tool supply following parameters:\n * @param {DynamoDBDocumentClient} ddb - dynamo db document client https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-dynamodb\n * @param {boolean} isDryRun - true if this is a dry run\n */\nconst transformUp = async ({ ddb, isDryRun }) =&gt; {\n  const addFirstAndLastName = (item) =&gt; {\n    // Just for the example:\n    // Assume the FullName has one space between first and last name\n    const [firstName, ...lastName] = item.name.split(' ');\n    return {\n      ...item,\n      firstName,\n      lastName: lastName.join(' '),\n    };\n  };\n  return utils.transformItems(ddb, TABLE_NAME, addFirstAndLastName, isDryRun);\n};\n\nmodule.exports = {\n  transformUp,\n  transformationNumber: 1,\n};</code></p><p><a href=\"https://gist.github.com/Guy7B/495732ecc3c9915bf160de97940e2a28\">GitHub链接</a>\"：<a href=\"https://gist.github.com/Guy7B/495732ecc3c9915bf160de97940e2a28\">https://gist.github.com/Guy7B/495732ecc3c9915bf160de97940e2a28</a>\"</p><p></p><p>对于大多数常规的数据转换，你可以使用 dynamo-data-transform 包提供的辅助功能，不需要自己管理数据转换脚本的版本，这个包将为你完成这项工作。在自定义了你想要转换的数据之后，就可以通过运行下面的命令来测试脚本：</p><p><code lang=\"text\">npx sls dynamodt up --stage local --dry</code></p><p></p><p>脚本将在控制台打印出记录，这样你就可以立即看到脚本的结果，并确保没有发生数据中断或其他问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6053ea11cc760f0835349e0fef3f6583.png\" /></p><p></p><p>如果你对测试结果感到满意，就可以删除 --dry 选项，并再次运行它。这一次，它将在生产环境的数据上运行脚本，因此请确保对结果进行验证。</p><p></p><p>在创建了数据转换文件之后，接下来你可能想要将其添加到 CI/CD 管道中。你可以将这个命令添加到生产环境的工作流 /CI 文件中。</p><p></p><p>这个命令将在 sls deploy 命令之后立即运行，这对于无服务器应用程序来说非常有用。</p><p></p><p>最后，所有这些都会被保存起来。如上所述，如果你想查看数据转换的历史，可以运行：</p><p><code lang=\"text\">npx sls dynamodt history --table UserExample --stage local</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8eba41ce6e67a89fc6483665fcd5e44c.png\" /></p><p></p><p>这个工具还提供了一个交互式 CLI。</p><p></p><p>以上所有的命令都可以通过 CLI 来实现。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/7978fdbcdec3b971dee1375b1d48b714.png\" /></p><p></p><p>有了 Dynamo Data Transform，你可以获得额外的好处，可以对数据转换操作进行排序和版本化，并在一个地方管理它们。如果回滚了某个操作，你还可以查看数据转换操作的历史记录。最后，你还可以重用和查看以前的数据转换。</p><p></p><p>我们已经开源了 Dynamo Data Transform，在内部，我们用它在 DynamoDB 和基于无服务器的环境上执行数据转换，并安全地管理以前只能通过手动的方式来执行的过程。</p><p></p><p>这个工具可以作为无服务器插件使用，也可以作为独立的 NPM 包使用。</p><p></p><p>NPMGitHub</p><p></p><p>如果你觉得有用，请随时提供反馈并参与项目的贡献。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/cae2a8779ab6901174ff410bb3f87195.png\" /></p><p></p><p>图 2 数据转换流程图</p><p></p><p></p><h5>原文链接：</h5><p></p><p>https://www.infoq.com/articles/dynamoDB-data-transformation-safety/</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/9ea2076da592ff48b660f8f34\">云原生数据库 Amazon DynamoDB 十年创新回顾</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzIxMzEzMjM5NQ==&amp;mid=2651069350&amp;idx=1&amp;sn=fe0e41b25f1869407f0981c38ab02c1e&amp;chksm=8c4bcea2bb3c47b4bc04e7f731f09bb13d877de34fa49d9f42b4516fb318072a3da3932e7f2f&amp;scene=27#wechat_redirect\">分布式系统设计模式，你用过哪些？</a>\"</p><p><a href=\"https://www.infoq.cn/article/p8OyeDyoRs5jZSD2lvcN\">大数据技术演进实录：云原生大数据、湖仓一体、AI for Data，未来“谁主沉浮”？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/84dbb6b51c5381c0dc2088d77\">ServerLess Aws Lambda 攻击与横向方法研究</a>\"</p>",
    "publish_time": "2022-12-22 18:27:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Swift 2023：所有权、宏和C++互操作",
    "url": "https://www.infoq.cn/article/fm6b2sg8QHUrDS6GhK48",
    "summary": "<p>Swift 语言工作组已经详细说明了 2023 年及以后 Swift 重点发展的主要领域，其中包括定义内存管理的所有权模型、不可复制类型、宏系统和 C++ 互操作。</p><p></p><p>所有权是一种内存管理方法，最近由于 Rust 而变得流行起来。它是 Rust 最具有决定性的语言特征之一，是其提供内存安全保证的基础。</p><p></p><p></p><blockquote>Rust 使用了第三种方法：内存是通过一个所有权系统来管理的，该系统有一套编译器检查规则。如果违反了任何规则，程序就不会编译。所有权的所有特性都不会拖慢程序运行时的速度。</blockquote><p></p><p></p><p>关于在 Swift 中引入所有权的讨论始于 2017 年，并最终定义了所有权宣言。</p><p></p><p>虽然关于所有权的工作没有取得任何进展，但 Swift 语言工作组将其带回来，目的是为了让程序员对内存中的值有更多的控制权。这可以包括禁止隐式复制、允许所有权转移以及在不进行复制的情况下借用值。此外，Swift 可以支持不可复制类型，以限制临界值的生命周期。</p><p></p><p></p><blockquote>这些控制将实现在内存中处理数据的新方法，将当前“不安全”结构的性能与 Swift 标准库功能的安全性相结合。</blockquote><p></p><p></p><p>Swift 另一个有前途的领域是创建一个过程宏系统，以创建高级库和 DSL。宏是一种代码生成机制，可以在词法、句法或语义层面对源代码进行转换。Swift 中现有的一些功能可以实现为宏，如 Codable 协议、字符串插值、属性封装器和结果构建器。根据 Swift 语言团队的说法，使用宏来构建像上面列出的那些新特性，可以释放出更多的资源，用于语言及其工具相关的其他工作。</p><p></p><p>虽然 Swift 宏的相关工作仍处于非常早期的阶段，但关于它们在 Swift 中应该是什么样子，人们已经有一些想法。下面是一个可能的 stringify 宏的例子：</p><p></p><p><code lang=\"swift\">macro(contexts: [.expression, .parameter], external: \"MyMacros.Stringify\")\nfunc stringify(_ value: T) -&gt; (T, String)</code></p><p></p><p>C++ 互操作旨在实现从 Swift 调用 C++ 代码，以及从 C++ 调用 Swift 代码。由于这个特性的相关工作进展得比较快，所以它有可能会首先出现在 Swift 官方版本中。根据 Swift 语言团队的说法，当前的 C++ 互操作实现已经支持自有值类型、普通值类型、外部引用类型和迭代器，并为方法、指针、l- 值和 r- 值引用有关的基本问题提供了答案。</p><p></p><p>虽然上述三个领域最具创新性，但 Swift 的演进也将专注于改进现有的功能，比如并发和泛型。</p><p></p><p>特别是在并发方面，其目标将是改善 Sendable 和 actors 提供的数据隔离，而泛型方面的工作将带来对变量泛型的支持，即拥有不同数量占位符类型的泛型。</p><p></p><p></p><h2>重写 Foundation 框架</h2><p></p><p></p><p>Swift 团队已着手开始 Foundation 框架的一个新的开源实现。新的实现完全采用 Swift 编写，旨在通过消除 Objective-C 和 Swift 之间的转换成本来提升性能，并对其进行模块化和清理。</p><p></p><p></p><blockquote>因为采用了原生的 Swift 实现，Foundation 框架不再需要在 C 和 Swift 之间进行转换，从而获得更高的性能。</blockquote><p></p><p></p><p>正如苹果工程师 Tony Parker 在 Swift 论坛上所解释的那样，性能方面带来的好处是非常显著的。</p><p></p><p></p><blockquote>使用 Swift 重新实现的日历应用的速度是 C 的 1.5 到 18 倍（在创建、日期计算等各种测试基准中调用 Swift）。</blockquote><p></p><p></p><p>Foundation 框架是大多数 macOS 和 iOS 应用程序的基石，提供了大量的抽象，包括 NSObject、NSString、NSArray 和 NSDictionary 等等。</p><p></p><p>Foundation 最初是用 Objective-C 编写的，很大程度上利用了它的动态特性，由于 Swift 与 Objective-C 运行时的互操作性，Foundation 目前可以用于运行苹果操作系统上的 Swift 应用程序。不过，对于其他支持 Swift 但只有原始 GNUstep Objective-C 运行时的平台（如 Linux）来说就不是这样了。为了规避这一限制，苹果在开源 Swift 时推出了 swift-corelibs-foundation，一个 Core Foundation 的 Swift 包装器，一个位于 Foundation 核心的底层 C API，并提供了映射到 Foundation 的“桥接”抽象，包括 CFString、CFDictionary 等。</p><p></p><p>如前所述，使用 Swift 重写将是一个重构 Foundation 框架的机会。目前，有五个不同的包进行了重新设计，包括 FoundationEssentials、FoundationInternationalization、FoundationNetworking、FoundationXML 和 FoundationObjCCompatibility。开发者可以只引入特定应用程序实际需要的依赖项。例如，一个从头开始开发应用程序并且想要切断与 Darwin Foundation 层所有联系的开发者，可以完全跳过 Obj-C 兼容层。</p><p></p><p>在做出重写 Foundation 决定的同时，苹果正在调整 C、Objective-C 和 Swift 层之间的顺序，并让 Swift 层成为 Foundation 的基础。这将有机会删除一些目前可以直接由 Swift 语言和标准库提供的特性，如 NSCoding 和 NSKeyedArchiver，取而代之的是 Codable，或者 Lock、OperationQueue，以及其他被 Swift 结构化并发取代的并发原语。不过，这些特性将由 Darwin 平台上的一个单独包提供，以确保与现有代码兼容。</p><p></p><p>这一声明在 Swift 论坛上引发了许多评论，其中大多数都暗示重新考虑现有的几个 API 的便利性，确保 Swift 类型（如 Codable）或结构化并发足够强大，以便在核心基础层完全取代 Obj-C 类型。</p><p></p><p>可以预见的是，Foundation 重写是一项中期的努力，目前还没有宣布具体的时间表，苹果正在邀请 Swift 社区加入讨论，一起定义它的未来。</p><p></p><p>对于 Swift 的发展，InfoQ 将继续跟踪报道。</p><p></p><p></p><h5>参考链接：</h5><p></p><p><a href=\"https://www.infoq.com/news/2022/12/apple-swift-foundation-rewrite/\">https://www.infoq.com/news/2022/12/apple-swift-foundation-rewrite/</a>\"</p><p><a href=\"https://www.infoq.com/news/2022/12/swift-ownership-macros-cpp-api/\">https://www.infoq.com/news/2022/12/swift-ownership-macros-cpp-api/</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/d3843ba34967a2c33e81a0664\">C++ 开发，这些 GUI 库一定不要错过！</a>\"</p><p><a href=\"https://xie.infoq.cn/article/a90dfad02a8148e0f2c1c8cb4\">学 C++ 的以后都能从事哪些岗位？</a>\"</p><p><a href=\"https://www.infoq.cn/article/KsctTt5cIpCCl5T2SmtJ\">现代化工具链在大规模 C++ 项目中的技术实践</a>\"</p><p><a href=\"https://xie.infoq.cn/article/1b1bd56c2ca2580200c9276aa\">C/C++ 的类型转换</a>\"</p>",
    "publish_time": "2022-12-22 18:27:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]