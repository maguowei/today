[
  {
    "title": "DoorDash 的移动应用发布流程",
    "url": "https://www.infoq.cn/article/UiWCX8hE6wtOgx258xER",
    "summary": "<p>DoorDash 公司的<a href=\"https://doordash.engineering/2023/11/28/how-doordash-manages-mobile-releases/\">移动应用发布流程</a>\"基于团队间明确的分工职责、有效的沟通、测试以及严格的回归问题处理和紧急修复规则。DoorDash 工程师 Manolo Sañudo 解释说，尽管并非所有的企业都具备 DoorDash 这样庞大的规模，但他们的解决方案的许多方面对规模较小的企业也有所帮助。</p><p></p><p>DoorDash 遵循的是相对简单的周发布周期。每个新的发布候选版本都会有一个发布分支，经过为期一周的测试和修复过程，最终正式发布。</p><p></p><p>每个新的发布候选版本都会分配一个发布经理来监督整个过程，确保一切顺利进行。发布经理的人员池要足够大，不会出现有人被工作量所拖累的情况，但也不至于过大，以至于无法跨各个发布版本做出一致的决策，或者危及发布流程的发展和改进。</p><p></p><p>每个发布候选版本都有自己的 Slack 频道，便于将状态更新和会话集中到一个地方，防止生产环境的漏洞热修复产生噪音。</p><p></p><p>对于测试，Sañudo 表示，由于无法在一周内进行完全的回归测试，因此“组件所有者”会单独负责测试所有组件，并使用移动发布管理平台 Runway 来跟踪测试状态。</p><p></p><p>每个组件所有者需要在批准组件之前执行特定的测试任务。在提交评审之前，每个组件都必须得到批准。</p><p></p><p>Sañudo 表示，在测试阶段会不时地发现回归问题。在这种情况下，发布经理与受影响的团队合作修复问题，并推送到主开发分支，只有当回归影响用户体验时，这个修复才会被合并到发布候选分支上。在这个阶段，既不允许出现对用户没有影响的 bug，也不允许添加新特性，每个精心挑选的修复都必须经过团队的论证，并由发布经理批准。</p><p></p><p>如果在流程的后期发现了漏洞，即在应用程序提交审核之后，甚至会采取更严格的规则，因为实施热修复可能会导致发布延迟。</p><p></p><p>虽然更新还没有发布，但可能正在等待评审或已经获得批准，要实施修复，我们将不得不拒绝构建并重新提交应用程序。因为这可能会导致延迟发布，我们会根据具体情况评估修复是否值得以及如何根据具体情况进行修复。</p><p></p><p>在获得苹果公司的批准后，新版本将向 1% 的用户发布，确保没有出现重大问题，并在几天后推向整个用户群。在这个阶段，团队使用一些关键指标来了解新版本的组件可能出现的问题。同样，发布经理使用 <a href=\"https://sentry.io/\">Sentry</a>\" 跟踪更高级别的指标，如崩溃率和趋势性问题。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/12/doordash-mobile-release-process/\">https://www.infoq.com/news/2023/12/doordash-mobile-release-process/</a>\"</p><p></p><p></p>",
    "publish_time": "2023-12-22 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI Agent与行业融合应用的前景及创新应用案例 ｜InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/DLBK3rdpsbvDxCgVHzRl",
    "summary": "<p></p><blockquote>嘉宾｜周健，澜码科技创始人兼CEO；周元剑，澜码科技联合创始人特邀主持人｜吴少杰，InfoQ社区编辑、高级算法专家</blockquote><p></p><p>&nbsp;</p><p>在数字化、智能化的时代，人工智能（AI）已经渗透到各个行业领域，其中AI Agent作为人工智能的重要分支，正在引发一场前所未有的行业融合革命。AI Agent以其智能交互、自主学习、灵活适应等特点，在各个行业中展现出巨大的潜力和价值。</p><p>&nbsp;</p><p>作为一个比较新的概念，AI Agent与行业融合应用的前景非常广阔，它们可以应用于各个领域，如医疗、金融、教育、零售等。本期《极客有约》，我们邀请到了澜码科技创始人兼CEO周健和澜码科技联合创始人周元剑，一同来探讨AI Agent目前的落地情况以及未来的发展趋势。我们还将为大家分享一些成功的AI Agent应用案例，以及探讨如何应对AI Agent应用中可能出现的挑战和问题，如数据隐私、算法透明性等。</p><p>&nbsp;</p><p>InfoQ：请问下您是如何看待最近两年AIGC引发的技术变革的？</p><p>&nbsp;</p><p>周健：我认为当前技术领域正在经历一场巨大的变革。过去，人们一直担心深度学习是否会面临瓶颈，但从GPT-1到GPT-3，尤其是ChatGPT在去年11月30日推出后，社会很快达成了共识，如Sam Altman所说：“我们可以期待未来5到10年内，以大型模型为核心的算力将成为智能基础设施的主要驱动力。”这种算力将带来智能水平指数级的成本下降，正如OpenAI在过去一年中三次降价所示。这一趋势类似于过去CPU价格的下降，对产业发展具有强大的推动力，随着智能的发展，生产力将迅速增加。</p><p>&nbsp;</p><p>另一方面，我们看到OpenAI正在不断提升算法和数据，人们猜测可能会有Q*算法和GPT-5等新的发展。随着算法、数据和算力的增加，我们可以期待更强大的智能。如果我们将智能看作是通用编码能力的衡量标准，其提升可能呈指数级增长。这意味着在ToB和ToC领域，很多业务和事务都将发生重大改变。对于澜码而言，我们将迎来一场波澜壮阔的变革，这也是为什么我们选择这个名字。我们相信，就像海啸即将抵达海岸一样，这一变革将迅速改变我们的工作和生活。</p><p>&nbsp;</p><p>周元剑：从一个更技术的角度来看。近两年，AIGC似乎最初主要在图像领域崭露头角，例如Stable Diffusion和Midjourney等，这确实为我们带来了一些与之前的人工智能不同的地方。在我看来，以前的人工智能更多地属于理解能力，可能有助于解答选择题、判断题或填空题等。而随着AIGC的出现，它可能具备了一些处理问答题的能力。在最初的图像领域，这种能力可能还局限在某一种类型的问答题。</p><p>&nbsp;</p><p>去年ChatGPT取得巨大成功之后，我们看到了大语言模型不仅在单一任务上能够进行问答，甚至可以在更通用的领域中执行问答任务，这基本上等于是智能的雏形。有了这个智能雏形之后，整个想象空间就被打开了。在以前，进行人工智能的整个流程非常漫长，从收集数据、训练模型到不断迭代，需要花费大量时间。如今，大语言模型具有一定的通用性，我们只需通过简单的提示工程，就能在一些常见或相对简单的场景中完成任务。这无疑将极大地提高生产力。</p><p>&nbsp;</p><p>InfoQ：请简要介绍一下澜码科技的背景，为什么要选择AI这条赛道，您看到了哪些机遇？</p><p>&nbsp;</p><p>周健：澜码科技的创建过程有许多偶然和机遇。去年年底，随着整个经济和投资环境的巨大变革，我当时就职公司的内部对AI业务的投资意见分歧较大。当时我们觉得公司可能不会继续发展AI业务了，于是考虑出来创业。</p><p>&nbsp;</p><p>初期想法可能是在自动化方向上，因为在RPA领域，我们看到了很多自动化的潜力。恰好在去年11月30日OpenAI推出ChatGPT，我们突然发现，传统自动化那些“代价”很大的事情在有了大语言模型的支持下变得容易实现了。虽然现在看起来似乎很容易，但实际上在这个过程中，我们经历了不少的迭代和反思。我们最初的想法是在大语言模型上构建一个超级自动化平台，这个方向至今仍然是我们正在努力实现的目标。</p><p>&nbsp;</p><p>InfoQ：在您看来，AI Agent到底是什么？它到底能解决什么技术问题？</p><p>&nbsp;</p><p>周元剑：我认为真正的Agent，其中一个关键点是“tool using”，即使用工具的能力，这是一种与众不同的技能。这也回到了我一开始提到的大语言模型具备了一些初级智能的特质。大语言模型带来的主要变革在于，它可以极大地释放其模型本身的能力，它不仅仅能执行单一任务，而且配合一些工具，比如调用RPA、API或其他工具，完成更智能的工作。以前，如果纯粹采用模型的方法，需要投入大量的精力才能完成任务。但在现有的大语言模型框架下，再加上一些工具，就能够轻松实现。这带来了一种独特的变革，如果脱离这种能力，大概就无法展现其智能的一面，技术上也就没有太多的特别之处。</p><p>&nbsp;</p><p>周健：刚才我们主要从技术供给的角度讨论了一些关于大语言模型的变化。关于Agent的概念，实际上在AI领域很早就被提及了。有人追溯到过去的AI理论，比如马文·明斯基（Marvin Minsky）提出的“Society of Mind”，这个理论认为，人类在执行某些任务时，比如驾驶，实际上有时候是自动进行的，你不需要完全关注驾驶，还可以同时考虑其他事情，比如工作或与家人聊天。这里有一个重要的概念是Autonomous&nbsp;Agent，即能够自主地执行任务。从这个自主执行的角度来看，我们未来的软件在某种程度上可能会脱离目前的束缚，会变得更像一个机器人。</p><p>&nbsp;</p><p>实际上，未来软件应该是具备大语言模型的语言理解能力，能够听懂并与用户互动。同时，按照Autonomous的理论，软件自身具有一定的领域知识，能够规划并提出计划，评估计划的好坏，执行计划，并在遇到困难时进行调整，使其变得越来越智能。</p><p>&nbsp;</p><p>从人类的角度来看，AI Agent的未来是否有意识，是否会演变成硅基生命，或者是否会与人类相互竞争，这些问题可能还需要更深入的思考。但至少从社会的角度来看，不仅是人与AI Agent之间，还是Agent之间相互互动，无论是在生产还是消费端，这里都是一个充满潜力的领域。从技术的角度来看，刚才提到的事情从逻辑上是可以实现的，但是目前还需要更多的资本投入和商业形态的调整，以及相应的模式调整，以便让这些技术能够成功落地。</p><p></p><h2>澜码科技AI Agent自动化平台技术实践</h2><p></p><p>&nbsp;</p><p>InfoQ：请老师和我们分享一下澜码科技的AI Agent自动化平台的落地情况。</p><p>&nbsp;</p><p>周元剑：我们创建澜码科技的初衷是构建一个基于大语言模型的自动化平台，最终的目标是建立一个分为三个层次的平台。</p><p>&nbsp;</p><p>首先是底层的基础AI能力层，然后是中间的Agent构建层，最上面是一些具体的业务应用层。在实践中，我们并不是一层一层地堆叠，而是强调与真实业务场景的对接。我们与客户紧密合作，了解他们的实际业务场景，并根据需求调整平台能力。</p><p>&nbsp;</p><p>我们平台的核心是提供对话的形式，使用户可以通过自然语言进行交流。同时，我们也考虑提供一些CUI形式，结合自然语言和图形用户界面，以更高效地满足用户需求。在平台的搭建过程中，我们遇到了一些挑战，主要集中在两个方面：客户对私有化的需求和稳定性的问题。</p><p>&nbsp;</p><p>客户通常期望平台能够提供私有化解决方案，涉及到数据时可能需要微调，这是一项复杂的任务。客户的数据可能不完整，我们需要与他们一起整理领域知识和数据。此外，私有化还涉及到算力的问题，我们提供了租赁机器的方式以提高灵活性。</p><p>&nbsp;</p><p>另一个挑战是平台的稳定性和性能。我们采用合成式AI的思路，强调符号主义（基于符号和规则的方法）。这意味着我们需要预先梳理用户的知识结构和推理分析的结构，以引导系统。通过这种方法，我们能更好地解决大语言模型应用中的“幻觉”问题。</p><p>&nbsp;</p><p>总的来说，我们的平台不仅提供基础的AI能力，还强调与真实业务场景的对接，以满足客户的需求。在私有化和稳定性方面，我们通过与客户合作、提供机器租赁和采用合成式AI的思路来应对挑战。</p><p>&nbsp;</p><p>周健：我们从自动化的角度出发，早早就在关注如何将人的技能复制出去。我们意识到，当前的大语言模型需要评估其智能的边界。因此，我们与一些AI Agent框架进行了比较，例如AutoGPT、MetaGPT等，发现它们的理念与我们不太一样。它们主要基于大语言模型进行理解。</p><p>&nbsp;</p><p>相比之下，我们更注重核心业务逻辑的编写，代码是有效的。我们坚持使用传统的工程方法，结合像Java、Python这样的代码和大语言模型，以获得最高的性价比。与其他框架（如Langchain、MetaGPT、Dify）不同，我们关注业务逻辑的核心，强调在一开始就编写代码。从编码的角度来看，我们认为这是百分之百准确的方式。例如，无论是工资计算还是银行转账，我们都不愿意接受大语言模型的“下一个对话预测”带来的不确定性。因此，我们强调专家的知识技能应该以某种方式通过编码或传统数据库的方式实现，然后依赖大语言模型的语言理解能力进行互动，这样可以更好地对接那些技能水平可能不太高的一线业务员。总的来说，我们的理念是，通过让专家指导我们的AI Agent，使其能够赋能一线业务人员，提高他们的水平。</p><p>&nbsp;</p><p>InfoQ：目前，大模型出来后，AI Agent的应用落地案例也层出不穷，两位老师如何看待这个问题？</p><p>&nbsp;</p><p>周健：在这个领域，最主要的问题是底层技术突然迎来了一波红利，这是以前未曾发生的，可能是因为当今社会信息传播速度的加快，以及像ChatGPT这样的形态使得每个人都能够尝试使用，因此它带来的社会影响是巨大的。这实际上为所有基于大模型进行应用开发的人提供了巨大的优势，就像突然间开启了一个新的时代。目前很多大公司都在尝试探索它的边界和应用，这在某种意义上是一个红利。</p><p>&nbsp;</p><p>反过来看，我们在行业内已经做了很长时间，但在实际落地时仍然遇到了很多困难。这里有一个误解，比如AutoGPT提到的“AI毁灭世界需要几步”，实际上，AI软件与传统软件非常不同，它并不是通过一个简单的标准路径就可以实现的。因为AI需要高准确率，就像当年深度学习在人脸识别时所面临的问题一样。虽然创建标准路径是容易的，但实际上在生产中使用起来是困难的，因为它对数据的要求和准确率要求都很高。</p><p>&nbsp;</p><p>从结果来看，能够实现良好ROI的AI Agent并不多见。像微软的GitHub Copilot，虽然广受欢迎，但实际上是亏本的。ChatGPT本身也是亏本的。飞书、钉钉，我相信他们算过成本和利润后可能也是不挣钱的。因此，今天从商业成功的角度来看，AI在实际应用中并没有变得非常普及。这需要大家共同努力。</p><p>&nbsp;</p><p>从技术的新颖性角度来看，可能确实有很多创新，但从实际应用的角度来看，与大模型、ChatGPT相比，AI Agent带来的额外增值可能并不是那么明显。</p><p>&nbsp;</p><p>周元剑：虽然我们可以找到一些AI落地的案例，但真正为企业带来额外的价值在实际中并没有如此明显。在这个领域，我们仍然面临着一些挑战，特别是在尝试解决企业实际业务中的问题时。这表明AI的落地并不是一件容易的事情。</p><p></p><h2>AI Agent落地的瓶颈是什么？</h2><p></p><p>&nbsp;</p><p>InfoQ：目前了解到的有哪些已经落地的AI Agent可以分享吗，有哪些技术瓶颈？</p><p>&nbsp;</p><p>周元剑：从技术的角度来看，我个人认为目前在落地应用方面的瓶颈主要存在于这些大型模型本身，它们在理解和逻辑推理方面的能力仍然不够强。因此，在实际应用的过程中，技术团队通常需要为这些大型模型的不足之处提供支持。这种支持不仅仅包括算法和模型方面的调优，还可能需要产品设计和工程方面的能力来保障整个流程。</p><p>&nbsp;</p><p>从另一个角度来看，落地不仅仅是技术层面的问题。我们在实际应用中也遇到了专业知识的问题。即使是一些专业咨询服务的客户，他们内部的专业知识可能并不清晰。因此，我们经常需要与客户合作，共同讨论和梳理专业知识的结构和应用方式。</p><p>&nbsp;</p><p>在长期来看，我认为专业知识和AI Agent的结合是不可避免的。数字化专业知识的转化过程将成为Agent在实际应用中关键的一环。我了解到行业内的一些Agent主要集中在一些大公司内部。这可能是因为这些公司在技术团队、算力和数据方面相对准备充足，同时内部团队更容易推动知识梳理的工作，因此相对容易在内部实现。</p><p>&nbsp;</p><p>另外，有一个有趣的案例是国外的一个Agent，它能够帮助维护代码库。例如，它可以阅读GitHub上某个项目的issue，并分析项目的代码，然后自动回复和编辑者的意图相关的内容，提供建议，甚至执行一些“魔法任务”。在GitHub上已经有一些开源项目正在使用这种Agent协助代码库的维护，这确实是一个令人兴奋的领域。</p><p>&nbsp;</p><p>周健：我认为目前算力是一个相当大的瓶颈。由于一些原因，例如在紧急情况下无法及时交付，我们通常采取私有化部署的方式。一些应用场景对算力要求是起伏不定的，例如为客户生成汉语考试题目，或者支持银行员工销售保险。在这方面，我们遇到了一些具体的问题，比如在实施过程中观察到训练和推理之间的差异，以及在移动设备上可能出现的端侧算力与云端算力之间的调度问题。</p><p>&nbsp;</p><p>此外，算力问题还受到地缘政治和供应链状况的影响，有些先进的显卡可能难以获取。企业内部的IT建设需要考虑如何搭建与大模型相关的技术设施，同时需要解决包括碳排放和绿色环保在内的可持续性问题。在商业运作中取得经济效益，仍然是一个巨大的挑战。我认为，解决这些问题需要至少2～3年的时间，才有可能看到一个能够达到及格标准的解决方案。</p><p>&nbsp;</p><p>InfoQ：根据您们的观察，哪个行业目前在AI Agent商业化落地大模型方面走得最快？</p><p>&nbsp;</p><p>周健：最终我觉得有两个方面，一方面，在大企业服务领域，资金实力较为雄厚。我们今天已经明显感受到信息化是数字化的基础，没有信息化就无法进行数字化。对于我们今天所提到的数智化或者AI Agent，其前提条件必然是数字化程度相对较高的领域，这是可以逆向推导的。比如金融、能源、零售等行业，实际上都处于数字化程度相对较高的状态。在这种场景下，AI Agent的实际落地可能会更加容易一些。</p><p>&nbsp;</p><p>另一方面可能与个体相关，我们可以看到整个灵活用工正在从传统的劳动力市场，如快递、滴滴司机等，逐渐扩展到企业端。企业可能有一些工作是可以外包的，这也符合灵活用工的需求。以前可能会有猪八戒网这样的平台，它允许我们将一些工作外包出去。今天有很多政府园区也在考虑GPT技术的应用，即我们是否可以通过有限的专业人员，再通过大语言模型，提供一些人才法务等方面的支持，以支持中小企业的发展。</p><p>&nbsp;</p><p>因此，在SaaS领域，将专业服务以SaaS化的方式快速复制出去，可能是一个更大的趋势。通过算力，将服务以“Agent as a Service”的形式提供，实际上相当于迅速释放了大量生产力。我们认为在一两年内，可能会形成一些重要的示范效应和案例。</p><p></p><h2>如何规范地引导AI Agent的发展？</h2><p></p><p>&nbsp;</p><p>InfoQ：AI Agent的伦理和数据隐私问题如何解决？我们应该如何规范和引导AI Agent的发展？</p><p>&nbsp;</p><p>周元剑：我认为这个问题本身是比较开放的，目前还没有一个被广泛认可的解决方案。随着应用场景的增加，新的问题也会不断涌现。</p><p>&nbsp;</p><p>对于数据隐私问题，脱敏和私有化是最基本的解决方案。我们可以允许系统和模型在客户的环境中运行，需要微调时，微调的数据和过程也可以保留在客户端。我们更愿意将计算资源移动到客户端，同时确保数据的安全性。通过这些手段，我们可以最基本地保障数据的安全。</p><p>&nbsp;</p><p>在实践中，可能会涉及一些更详细的需求，例如权限管理和数据权限管理，这些与隐私相关。但是，如果数据已经进入模型，管理起来可能会比较困难。因此，通常我们会考虑一些策略，例如通过外部的支付方式，采用应用内购的方式，或者让Agent的设计和执行过程分离等。在我们的解决方案中，核心思想是将整个过程构建成一个多阶段的流水线，而不仅仅是一个AIM（人工智能模型）。这样我们就可以在策略层面上对数据进行更好的管理。</p><p>&nbsp;</p><p>通过这种方式，我们对模型的需求变成了让模型具有能力，比如解答问题、写作、总结文档等，而不仅仅是对知识数据的依赖。模型的能力是局部的，而全量的支持数据都保留在外部。在需要获取数据时，可以通过信息系统进行传统的获取方式，同时可以在信息系统上实施各种隐私保护和权限管理。</p><p>&nbsp;</p><p>此外，随着算法流水线的开放，算法的透明度也得到了一些缓解。用户可以看到Agent是如何一步一步地工作的，每一步的输出都可以供用户审查，确保最终结果基本合理。</p><p>&nbsp;</p><p>未来随着专家知识的沉淀和数据的积累，个体、组织可以选择性地开放独特且稀有的知识和数据，让他人去查阅。这种情况下，个体和组织可以通过让他人使用自己的知识来获取一定的回报。</p><p>&nbsp;</p><p>周健：关于数据的问题，我们在实际的落地过程中遇到了一些挑战，特别是在初期阶段。我们最初考虑在法律行业进行落地，但后来发现可能会面临较大的困难。律师认为他们过去处理的案件是最核心的竞争壁垒，因此他们可能不愿意分享这些信息。正如我们之前提到的，专家知识的数字化是AI Agent落地的必要条件。如果由于经济原因或竞争原因，专家不愿分享知识，就必须设计相应的机制。</p><p>&nbsp;</p><p>国家目前也已经设立了大数据局，对于数据要素和隐私方面也在不断进行探讨。在B端，不同的Agent可能扮演不同的角色，负责不同的任务，拥有不同的权限和责任。类似于企业中对于人员的管理、运用和安排，Agent是否会有相应的权责利体系，是一个需要考虑的问题。因为在没有这些设计的情况下，Agent只是一个程序，这时去处理隐私问题是非常困难的。Agent作为一个能够获取知识、数据和做决策的实体，必然会涉及一些隐私问题。在处理Agent性质和相关隐私问题的过程中，需要进行更深入的讨论，以达成共识。</p><p>&nbsp;</p><p>InfoQ：未来，AI Agent的发展趋势和前景是什么？您看好AI Agent未来的发展吗？您认为多久我们会迎来AI Agent的大规模落地？</p><p>&nbsp;</p><p>周健：目前大语言模型的能力还不够，我们期待它的实施门槛能够降低，让绝大多数人都能够使用。我认为这可能要等到至少GPT-5发布之后，在国外可能需要一年左右，而在国内可能需要2～3年的时间才能实现。</p><p>&nbsp;</p><p>我认为，关键的能力之一是让大语言模型知道自己擅长和不擅长的领域。一旦大语言模型具备了这个能力，它就会更像一个“人”，即使只有高中水平的知识，但它知道自己的能力范围，这就为咨询顾问、企业内部专家构建一个自己的虚拟助手创造了可能。目前，一些知名人士和KOL都在尝试构建自己的虚拟助手。如果大语言模型真的具备了这样的能力，并且成为一种普遍的基础设施，那么我相信这样的应用将会迅速普及。</p><p>&nbsp;</p><p>周元剑：目前，Agent发展的主要制约还在于大模型本身的限制。然而，Agent作为一种有效的治理承载形式，我相信未来它会逐渐发展壮大，尽管这可能需要一些时间。正如之前提到的，可能国外明年可能会有GPT-5，使用起来会更加便捷，而国内可能需要更多时间。但无论如何，我相信那个时刻终将到来。</p><p>&nbsp;</p><p>InfoQ：对于想要进入这个领域的公司或个人来说，需要了解哪些相关知识？您有什么意见给到这些人吗？</p><p>&nbsp;</p><p>周元剑：这个问题涉及到个体和组织之间的相关性，因此很难提出通用的解决方案，因为每个人或组织的需求和情境都可能有所不同。然而，从技术的角度来看，对大语言模型算法的一些基本了解是必要的。尽管不要求深入进行类似算法的工作，但了解其基本原理，对于在技术判断中理解模型的边界非常有帮助。</p><p>&nbsp;</p><p>此外，了解与Agent相关的应用知识也很重要，目前业界最流行的是RAG。关于这方面的知识，有很多可以学习的资源，包括经验总结、文献以及与业务相关的学术论文。阅读一些InfoQ的文章也是一个不错的选择，因为它们通常总结得比较全面。</p><p>&nbsp;</p><p>另外，与业务紧密联系也是至关重要的。无论使用什么技术，最终目的都是解决业务问题。脱离业务的技术发展是缺乏基础的。因此，深入了解业务需求，与业务团队密切合作，将有助于更好地应用技术解决实际问题。</p><p>&nbsp;</p><p>周健：这次的变革实际上是一个底层的颠覆，我们公司的起名来源于波澜壮阔的代码，象征着激荡的变化。在这个高速变化的时代，不变的是高速的变化。在真正领先的AI Agent领域中，知识的迭代速度会更快，大约两、三个月就可能刷新一次。</p><p>&nbsp;</p><p>要进入AI领域，定制AI是一个关键的步骤。在过去的经验中，我们学到的是尽量不要做过多的功能，更关注数据集和准确性。与以前编写工程代码时编写测试用例一样，你应该关注采样的数据集是什么样的，它的分布是什么样的，以及训练出的Agent的表现如何。</p><p>&nbsp;</p><p>新的软件越来越像人一样，因此设计AI Agent或者像人一样的软件仍然是重要的。了解人是如何完成任务、学习、进步和成长，将有助于设计人机交互。从命令行到GUI，交互方式一直在不断摸索。在新时代，我们需要思考人与机器之间如何进行更智能的交互，结合命令行、GUI、自然语言理解等技术，形成全新的交互方式。</p><p>&nbsp;</p><p>最后，我们可以从科幻小说中借鉴一些设计思路，例如《钢铁侠》中的贾维斯。这个时代有点像文艺复兴，我们需要重新思考人与机器之间的互动，努力创造出更好的AI Agent。</p><p>&nbsp;</p><p>InfoQ：这里有个观众提问：“ AI Agent跟RPA在能力上有什么区别？”</p><p>&nbsp;</p><p>周健：它们之间的差距很大。RPA相比之下有点“傻”，只能执行预定的任务，比如通过Java或Python进行简单的循环和变量设置。而大语言模型则具有更强大的能力，它能够真正理解对话，生成行动计划，具有更强的通用性和复用性。</p><p>&nbsp;</p><p>InfoQ：这里有个专注于开发垂直行业的对话机器人的观众，他的方法是基于本地知识库构建。他希望通过采用一些技术手段来提高系统提供精确答案的准确度。老师有哪些建议可以帮助解决这个问题？</p><p>&nbsp;</p><p>周元剑：从技术角度来看，我们目前更倾向于采用一种合成式AI的方法。这意味着，首先对所涉及领域的知识进行一定的理解或结构化整理，这有助于更好地完成当前的任务。举个例子，法律领域的文档通常具有特定的结构，是按条目编写的，这是一个独特的特征。整个文档可以根据这些条目进行结构化处理。另外，对于报表等文档，其结构可能涉及行和列，有标题和表头信息。这些表头信息可以单独提取，并作为关键信息用于抽象或关键词匹配等任务。</p><p>&nbsp;</p><p>对于文档Agent，当进行切片时，你需要关注是否存在更好的策略来合并最相关的切片，这有助于提升效果。此外，在传统搜索中，采用多路召回的方法，不仅仅通过单一向量进行搜索结果的获取，还可能结合其他关键字或方式，进行综合排序。</p><p>&nbsp;</p><p>InfoQ：有观众问：“AI Agent之间如何协同工作？”</p><p>&nbsp;</p><p>周健：我认为目前处于很早期的阶段，企业服务可能会划分为不同类型的功能Agent。在我们目前的领域中，我们更多地专注于标准操作流程的自动化，涵盖数据、文档和应用流程等不同方面。</p><p>&nbsp;</p><p>在这个设想中，一些Agent专注于处理数据，而另一些专注于分析流程。例如，数据Agent可能负责回答与数据相关的问题，而文档Agent可能负责整合文档。这也引出了一个关键问题，即如何在团队中设置不同角色，并将数据和流程的权限进行切片，以分配不同的职责。</p><p>&nbsp;</p><p>尽管我认为目前还处于较早的阶段，但在更抽象的层面上，我觉得重要的是因为单个Agent在实现上可能做得不够出色，在技术本身或概念层面上，多个Agent的实质意义尚不清晰。将两段代码组合在一起实际上等同于一段代码。因此，对于多个Agent的概念，我认为在技术和概念层面上仍然需要更深入的思考。</p><p>&nbsp;</p><p>周元剑：我赞同这种观点，即在当前阶段，大语言模型的智能水平尚未达到足够的程度。为了提升端到端效果，可能会考虑引入一些类似反问者或审查者的角色，并使多个角色协同工作以更好地完成任务。然而，在当前大语言模型的能力尚不强大的情况下，这样做可能事倍功半。你可能会花费大量时间来调整不同部分的性能，协调它们的合作方式，但这过程会很耗费精力。举例而言，如果单个模型的准确率只有70%，那么将三个模型串在一起可能会导致总体准确率更低，因为错误的概率会更高。</p><p>&nbsp;</p><p>另外，即使你投入更多精力进行调优，仍然存在一个风险，即大语言模型的能力会不断提升。我们有过这方面的经验，使用了类似的思路，例如在使用GPT-3.5时，尝试通过多个Agent协同工作来完成任务。然而，当我们使用新模型GPT-4时，发现之前的系统，即使经过大量努力进行调整，其结果可能还不如GPT-4效果好。因此，我们对内部采用多个Agent协同工作的做法持谨慎态度。</p><p>&nbsp;</p><p>嘉宾简介：</p><p>&nbsp;</p><p>周健，澜码科技创始人兼CEO。毕业于上海交通大学计算机系，在校期间荣获ACM国际大学生程序设计竞赛的世界冠军，是首个在此项竞赛夺冠的亚洲团队成员。曾在谷歌和阿里云从事搜索领域的工作，涉及底层基础分布和基础设施相关的工作。后加入依图，是公司的第十位员工，负责开展AI工程和产品方面的工作；也曾在RPA公司弘玑担任CTO。2023年初创业，创立了澜码科技公司。</p><p>&nbsp;</p><p>周元剑，澜码科技联合创始人人。本科毕业于上海交通大学。作为依图科技的第6号员工在依图工作10年左右，工作领域涵盖算法、工程、运维、产品经理，以及SaaS业务技术等方面。主导了多个AI落地相关业务，对于这一领域有深刻的理解。在过去的两年在弘玑工作，负责AI产品线。今年初随周健一起创办澜码，投身新创业项目。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-12-22 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Katalyst Custom Config：轻松管理上万节点的差异化配置",
    "url": "https://www.infoq.cn/article/5Vpsr1PJ0J1cMiIVEgRj",
    "summary": "<p>Katalyst 是一个以 QoS 保障为核心的开源资源管理系统，是字节跳动对大规模在离线混部实践的总结。大规模的混部场景对配置管理的自动化和灵活度有很高的要求，本文通过讲解 Katalyst 中的 Katalyst Custom Config 方案，介绍了 Katalyst 实现复杂配置管理的思路以及实际的使用场景。</p><p></p><p></p><h2>背景</h2><p></p><p></p><p>在大规模集群中，往往存在各种不同的机型和业务，这就需要管理员对不同节点进行差异化配置——</p><p>对于 CPU 密集型的业务的节点，我们可能需要调高 CPU 的驱逐阈值，以保证业务的稳定运行；对于 IO 密集型的业务的节点，我们可能需要调低 IO 的驱逐阈值，以防止 IO 饥饿；此外，还需要根据业务的安全需求，对不同节点的 Agent 接口权限进行精细化配置。</p><p></p><p>在上述过程中，AdminQoSConfiguration 和 AuthConfiguration 是比较常见的配置：</p><p></p><p>AdminQoSConfiguration&nbsp;是用于管理 QoS 相关管控手段的配置。例如，它可以配置 cpu/memory/io/network 等多个资源维度的压制驱逐策略，包括各种驱逐开关、驱逐阈值等。它也可以配置混部算法相关的管控策略，如混部开关、混部算法参数等；AuthConfiguration&nbsp;是用于管理 Agent 各类接口的权限策略的配置。例如，它可以配置 out-of-tree plugin 的准入权限，端口访问权限等。这对于保证系统的安全性和稳定性非常重要。</p><p></p><p>然而这些配置在管理层面仍然存在复杂度过高的问题——对于通过 DaemonSet 部署的单机 Agent 而言，传统的基于启动参数的静态配置管理方式只能通过滚动重启实例进行配置变更，存在生效时间长、实例重启存在风险等问题。另外，面对集群中存在的的差异化配置需求，这种方式也只能通过部署多个 DaemonSet 实例的方式实现，存在运维负担较重的问题。因此对于单机管控系统而言，动态配置管理已经成为不可或缺的功能。</p><p></p><p>针对上述需求，原生 Kubernetes 提出了 Dynamic Kubelet Configuration 的动态配置管理方案（v1.11 开始 Alpha 支持，v1.22 之后被废弃），该方案为集群管理员提供了能够通过 Kubernetes API 动态改变 Kubelet 运行时配置的动态配置管理方案。</p><p></p><p>Dynamic Kubelet Configuration 的工作流程大致如下：</p><p></p><p>创建一个 ConfigMap，其中包含了想要在 Kubelet 上应用的配置。将这个 ConfigMap 关联到一个或多个节点。Kubelet 在后台检查这个 ConfigMap，并且在检测到任何改变时，它会重启并使用新的配置。</p><p>然而，Dynamic Kubelet Configuration 也存在一些局限性：</p><p>动态配置的生效需要 Kubelet 重启，这可能会导致正在运行的 Pod 中断，影响应用的稳定性。动态配置只能应用于 Kubelet，对于 out-of-tree 的 agent 如各种 device plugin 等，无法进行动态配置。对于集群内存在机型或业务差异的场景，并没有提供自动化配置的扩展和支持。</p><p></p><p></p><h2>什么是 KCC</h2><p></p><p></p><p>Katalyst 作为字节跳动开源的提高资源利用率的通用资源管控系统，能通过精细化的单机管控手段，实现细粒度的资源隔离与业务 SLA 保障。</p><p></p><p>针对上述社区方案存在的问题，Katalyst 推出一种全新的解决方案 ——&nbsp;Katalyst Custom Config（KCC），这是字节跳动在大规模集群单机管控实践中，总结并设计出了一套高度自动化、扩展性强的单机动态配置管理方案。</p><p></p><p></p><h4>设计目标</h4><p></p><p></p><p>KCC 旨在解决现有方案的局限性，提供一种更加灵活、高效和可扩展的单机动态配置管理方案，以满足日益增长的单机管控需求。以下是 KCC 的主要设计目标：</p><p></p><p>动态配置：KCC 应能够实时响应配置更改，无需重启，从而避免影响正在运行的 Pod 和应用的稳定性。差异化配置：KCC 应能够支持集群内存在机型或业务差异的场景，提供差异化配置的能力，以满足不同节点可能需要的不同配置。自动化管理：KCC 应能够根据节点差异化配置自动下发节点配置，减轻大规模集群管理的工作负担，避免手动操作导致的错误。易于运维：KCC 应提供简单易用的接口和工具，使运维人员能够方便地管理和监控配置的状态和变更。易于扩展：KCC 不仅应用于 Katalyst 自身，还能以 SDK 的形式支持 out-of-tree 的 agent，如各种 device plugin 等，以满足更广泛的配置需求。</p><p></p><p></p><h4>基本架构</h4><p></p><p></p><p>KCC 方案中 Agent 的动态配置都是基于 CRD，而不是 ConfigMap，这能提高动态配置的可靠性和易用性。其各组件或模块的职责如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89d95ebc6d8c18336edb9b41efcf00b7.png\" /></p><p></p><p>KatalystCustomConfig (KCC)：由管理员创建，描述需要托管的动态配置 CRD 信息（如前文提到的 AdminQosConfiguration 和 AuthConfiguration 的 GVR） 和托管行为。KatalystCustomConfig Target (KCCT)：托管的动态配置（如前文提到的 AdminQosConfiguration 和 AuthConfiguration 的 CR），包含实际配置字段以及支持差异化配置的通用字段。CustomNodeConfig (CNC)：每个节点创建的同名 CR，实时同步节点 Labels，保存当前节点匹配的动态配置信息。KCC Controller：管理托管的动态配置 CRD 注册，更新匹配节点的动态配置信息到 &nbsp;CNC。KCC SDK：定时轮询 CNC，自动加载最新动态配置 CR。</p><p></p><p>该方案通过动态配置注册机制，允许管理员动态定义和管理配置，从而实现灵活地按需扩展动态配置。同时，通过使用标签选择器和临时选择器，KCC 也可以灵活实现集群节点差异化自动配置。</p><p></p><p>除此之外，Agent 与 APIServer 无需建立 list/watch 的长链接，避免了在大规模集群场景下过多长链接对 APIServer 的负担。基于 CNC 的配置 hash 缓存策略，也可以有效减少直接查询动态配置的 APIServer 请求。</p><p></p><p></p><h2>KCC 方案详解</h2><p></p><p></p><p></p><h4>动态注册机制</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/09e8b6799ed1a8f45126b45410b16914.png\" /></p><p></p><p>为了方便管理员动态的扩展动态配置的需求，KCC&nbsp;支持动态配置 CRD 的注册，支持管理员将 Agent 的动态配置划分到不同 CRD 中，同时方便进行权限管控。</p><p></p><p>管理员在 KatalystCustomConfig 中通过 TargetType 描述需要被托管的动态配置 CRD 的 GVR（Group Version Resource）。</p><p></p><p>当 KCC Controller 监听到 KatalystCustomConfig CR 的创建，就会根据其配置的 GVR 信息动态创建一个 &nbsp;dynamic Informer，这样 KCC Controller 就可以通过 list/watch 的方式动态发现 &nbsp;KCCT 创建和更新。</p><p></p><p>KCC Controller 会校验 KCCT 配置是否有效，同时更新 KCCT 的 hash 值，并根据 KCCT 中差异化配置将其同步到所匹配的节点的 CNC 的 KatalystCustomConfigList 中。</p><p></p><p>KatalystCustomConfigList 表示当前节点所匹配动态配置信息列表，即多个 TargetConfig，每个 TargetConfig 描述对应 KCCT 的 GVR 信息以及节点当前所匹配动态配置 CR 的信息和其配置的 hash 值。</p><p></p><p></p><h4>差异化配置</h4><p></p><p></p><p>为了满足集群中差异化配置的需求，KCC 方案支持 LabelSelector 或节点列表的配置，充分利用 K8s 原生的 label 和 labelSeletor 特性，将动态配置的差异化划分成三个粒度：全局粒度、LabelSelector 粒度、节点粒度。同时一个节点匹配的配置顺序是节点 &gt; LabelSelector &gt; 全局。</p><p></p><p>全局粒度配置：即动态配置无需指定 NodeLabelSelector 和 EphemeralSelector，同一个集群只能有一个全局配置LabelSelector 粒度配置：即动态配置指定了 NodeLabelSelector，其采用 K8s 原生的 labelSeletor 语法，并支持 =, ==, !=, in, notin 等选择算子，且支持多个 key 的组合，例如 \"key1=value1,key2!=value2\"节点粒度配置：即动态配置指定了 EphemeralSelector，其指定配置所匹配的节点列表，为了避免这种临时配置长期存在导致配置不可维护，要求一定需要配置持续时间，当该配置过期之后会被自动清理。</p><p></p><p></p><h4>配置冲突检测</h4><p></p><p></p><p>一个集群里有一个全局粒度配置、多个 LabelSelector 粒度配置和节点粒度配置，但单一节点所匹配的动态配置只能有一个，因此需要对所有配置进行冲突检测。</p><p></p><p>节点粒度配置冲突检测比较简单，即两个不同配置的节点列表集合不能有交集，但 LabelSelector 粒度配置的冲突检测较为复杂。</p><p></p><p>NodeLabelSelector 支持相等运算符（=/==）、非相等运算符（!=）以及集合运算符（in/notin）来匹配 Label，且支持多个匹配算子组合的复合选择器。然而对于一个 key 而言，所对应的 value 可能是无穷的，selector 中包含可能的 key 越多，出现冲突的可能性越大，配置的维护就越复杂。因此管理员可以通过 KCC 的 NodeLabelSelectorAllowedKeyList 对 NodeLabelSelector 支持的 key 进行约束。</p><p>为了判断两个 LabelSelector 粒度配置是否冲突，我们设计了基于等值集合和不等值集合的冲突检测算法，该算算法基本思路如下：</p><p></p><p>1.&nbsp;对两个配置的选择器 (selectorA 和 selectorB) 遍历所有支持 key ：</p><p>获取selectorA中对于 key 的相等和不等的值集合（equalValueSetA和inEqualValueSetA）。获取selectorB中对于 key 的相等和不等的值集合（equalValueSetB和inEqualValueSetB）。</p><p></p><p>2.&nbsp;检查这两个选择器的值集合满足以下条件则可能存在冲突，需要继续遍历下一个 key：</p><p>如果equalValueSetA和equalValueSetB的交集非空。如果equalValueSetA和equalValueSetB都为空。如果inEqualValueSetA与equalValueSetB的交集不等于equalValueSetB。如果inEqualValueSetB与equalValueSetA的交集不等于equalValueSetA。如果equalValueSetA非空，但equalValueSetB和inEqualValueSetB都为空，即selectorB可以匹配该 key 的任意值。如果equalValueSetB非空，但equalValueSetA和inEqualValueSetA都为空，即selectorA可以匹配该 key 的任意值。</p><p></p><p>3.&nbsp;如果存在一个 key 不满足以上任何条件，则表示两个选择器没有冲突，因为算子之间的关系是 AND。</p><p></p><p>4.&nbsp;如果所有键都满足以上条件，表示两个选择器可能存在冲突。</p><p>基于该算法，我们可以提前判断两个 LabelSelector 可能存在冲突告知用户，避免在新加节点的时候出现匹配到多个配置的情况。</p><p></p><h4>配置优先级</h4><p></p><p></p><p>根据上述冲突检测算法，可以看出如果允许配置多个 key 的情况下，要求每个动态配置尽可能包含所有的 key，这时候对于用户而言在某些场景下可能很难充分考虑到所有可能冲突的情况，比如紧急降级等。因此我们对 LabelSeletor 配置引入了配置优先级的概念，即冲突仅存在相同优先级，一个节点优先匹配高优先级的配置。</p><p></p><p>除此之外，KCC 方案也支持在 NodeLabelSelectorAllowedKeyList 中对不同优先级配置允许的 key，这样可以更好地规范用户的使用。基于生产实践，对于同一个配置，我们推荐最多配置两个优先级，每个优先级的最多配置两个 key，这样才不会导致配置爆炸。</p><p></p><p></p><h4>Agent SDK</h4><p></p><p></p><p>对于 Agent 而言，我们希望其不需要感知节点层差异化配置，只需要根据其需要的动态配置的 GVR 就可以获得当前该节点所匹配的最新的动态配置。</p><p></p><p>因此我们设计了 Agent SDK，负责定时轮询 CustomNodeConfig，只有当 KatalystCustomConfigList 中 TargetConfig 与 cache 中相同 GVR 的 TargetConfig 的配置 hash 值不一致时，才会访问 APIServer 将 TargetConfig 所对应的动态配置最新的 CR 加载到 cache 中。</p><p></p><p>Agent 通过 KCC SDK 只需要动态配置 GVR 信息就可以获取该节点所匹配的动态配置的 CR，而不需要关心当前节点差异化的信息，这有效降低了单机侧识别配置的复杂度。</p><p></p><p></p><h4>Agent 动态配置框架</h4><p></p><p></p><p>基于 KCC 的动态配置管理方案，Katalyst 实现了灵活的 Agent 动态配置框架。该框架涉及以下组件：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5fdba918062d1771d5cde3e932d6d410.png\" /></p><p></p><p>DynamicConfigCRD：包含所有动态配置 CRD api 的结构体，若需要扩展动态配置，需要将动态配置 CRD 的 api 定义加到该结构体中。</p><p></p><p>DynamicConfigManager：位于 MetaServer 中，负责管理 Agent 模块需要监听的动态配置的注册，并通过 KCC SDK 自动获取所需的动态配置，并对获取的动态配置以 DynamicConfigCRD 的结构保存 checkpoint，避免 Agent 重启时无法访问 APIServer 导致无法获取到当前节点的配置。除此之外，其还会将当前节点的 DynamicConfigCRD 与启动时初始的 DynamicAgentConfiguration 通过 ApplyConfiguration 接口进行覆盖，并通过 SetDynamicConfiguration 方法写到全局 Configuration 中，这样 Agent 各个模块就可以直接通过全局 Configuration 的 GetDynamicConfiguration 方法获取到最新的动态配置。</p><p></p><p>DynamicAgentConfiguration：位于全局 Configuration 中，通过 GetDynamicConfiguration 和SetDynamicConfiguration 方法进行读写。其包含 Agent 需要的所有动态配置的实体，其的所有成员对象都需要提供 ApplyConfiguration 的方法，即将 DynamicConfigCRD 应用到该成员对象的策略。</p><p>基于该框架，Katalyst Agent 中的各个模块可以像使用静态配置一样使用动态配置。</p><p></p><p></p><h2>KCC 应用案例</h2><p></p><p></p><h4>混部降级</h4><p></p><p></p><p>在春节活动、线上事故等场景，研发团队需要对集群中的混部资源进行紧急回收，这时就可以通过 KCC 实现快速混部降级，即创建 AdminQoSConfiguration 的全局配置，将 EnableReclaim 设为 false，如下所示：</p><p></p><p><code lang=\"text\">apiVersion: config.katalyst.kubewharf.io/v1alpha1\nkind: AdminQoSConfiguration\nmetadata:\n  name: global\n  namespace: default\nspec:\n  config:\n    reclaimedResourceConfig:\n      enableReclaim: false</code></p><p></p><p></p><h4>策略灰度</h4><p></p><p></p><p>在一些新的管控策略上线的场景，研发团队可以通过 KCC 在部分节点上对该策略进行灰度。例如新驱逐策略上线，管理员可以对部分节点开启 72 小时 Numa 粒度的内存驱逐策略，即创建 AdminQoSConfiguration 的节点级别配置将 enableNumaLevelEviction 设为 true，如下所示：</p><p><code lang=\"text\">apiVersion: config.katalyst.kubewharf.io/v1alpha1\nkind: AdminQoSConfiguration\nmetadata:\n  name: numa-memory-eviction-gray\n  namespace: default\nspec:\n  ephemeralSelector:\n    nodeNames:\n    - node1\n    - node2\n    lastDuration: 72h\n  config:\n    evictionConfig:\n      memoryPressureEvictionConfig: \n        enableNumaLevelEviction: true</code></p><p></p><p></p><h4>机型定制化策略</h4><p></p><p></p><p>在集群中节点的机型存在性能差异的场景，如集群中同时存在 HDD 盘和 SSD 盘的机器，对于 HDD 盘的机器，Kswapd 内存回收对业务指标影响较大，管理员可以通过 KCC 调整 HDD 盘机器的内存驱逐阈值，提前驱逐离线的 pod，即创建 AdminQoSConfiguration 的 LabelSelector 配置，增大 systemFreeMemoryThresholdMinimum 阈值到 20Gi，如下所示：</p><p></p><p><code lang=\"text\">apiVersion: config.katalyst.kubewharf.io/v1alpha1\nkind: AdminQoSConfiguration\nmetadata:\n  name: hybrid-for-disk-hdd\n  namespace: default\nspec:\n  nodeLabelSelector: diskMode=hdd\n  config:\n    evictionConfig:\n      memoryPressureEvictionConfig: \n        systemFreeMemoryThresholdMinimum: 20Gi</code></p><p></p><p></p><h4>后续规划</h4><p></p><p></p><p>Katalyst Custom Config（KCC）方案的动态注册机制允许管理员动态定义和管理配置，差异化配置通过 LabelSelector 和节点列表实现，配置冲突检测算法帮助管理员避免配置匹配冲突，配置优先级允许节点匹配高优先级配置。Agent SDK 简化了 Agent 获取动态配置的过程，隐藏了节点层的差异化配置细节。</p><p></p><p>当前，KCC 已在 Katalyst 中应用，例如管理员可以灵活控制混部配置，实现快速降级或灰度测试。在 Katalyst 后续的版本中，我们将持续迭代 Katalyst Custom Config 方案，使其能够更好地方便集群管理员进行 Agent 动态配置管理。</p><p></p><p>可观测性：当前 KCC 的动态配置的下发暂时没有提供进度显示，对于管理员来说不够友好，因此 KCC 的可观测性增强将作为后续迭代的主要方向之一历史版本：对于动态配置方案而言，为了避免误操作，必然存在回滚的需求，因此历史版本支持也将是 KCC 未来需要补充的能力</p><p></p><p>如需企业交流和合作：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png\" /></p><p>添加字节跳动云原生小助手，加入云原生社群：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4a9e8cba206a158b0404d888f080602.png\" /></p><p></p>",
    "publish_time": "2023-12-22 10:02:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "年终收官！华为云开发者日·2023 年度创享峰会成功举办",
    "url": "https://www.infoq.cn/article/1eY7fD1MyipT3bshCtvT",
    "summary": "<p>12 月 20 日，<a href=\"https://xie.infoq.cn/article/6b291db25639d747804cf9242?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">华为云开发者日</a>\"·2023 年度创享峰会成功举办，众多开发者与技术爱好者齐聚一堂，在现场，有 600 余名开发者与华为云技术专家共同就大模型应用、<a href=\"https://xie.infoq.cn/article/32d167304b2735b51bec5fe3f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">CodeArts </a>\"软件开发等技术话题进行深入探讨，分享实战技巧与解决方案。此外，华为云还精心设置了<a href=\"https://xie.infoq.cn/article/a15065ba41030825c0d72b91b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\"> KooLabs </a>\"工作坊、产品体验官、展区等环节，让开发者亲身体验华为云产品的技术魅力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bbeaddb6a1f9970b1d2e61fa2e903223.jpeg\" /></p><p></p><p>华为云开发者日是面向全球开发者的旗舰活动，旨在全方位服务与赋能开发者，围绕华为云生态“知、学、用、创、商”成长路径，通过前沿技术分享、场景化动手体验、优秀应用创新推介，开发者提供沉浸式学习与交流平台。</p><p></p><p>中关村科学城管委会副主任、海淀区副区长武凯在致辞中表示，海淀是北京国际科技创新中心核心区，拥有丰富的科技创新资源和基础优势。今年，海淀区人工智能产业获评国家战略性新兴产业集群优秀等级，全力打造“中关村人工智能大模型产业集聚区”，并建设多个人工智能特色产业园。华为云作为国内领先的云服务提供商，与海淀区展开合作，为海淀区的数字化转型和科技创新提供了有力的支撑，助力全市数字经济蓬勃发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/505b6392a9a24ffff855223d85eaa749.jpeg\" /></p><p>中关村科学城管委会副主任、海淀区副区长 武凯</p><p></p><p>北京市海淀区东升镇博展股份社党委副书记、总经理代庆在致辞中表示，中关村东升科技园与华为云建立了战略合作伙伴关系，共同推动全链条创新创业生态体系的发展，为全球科技创新型企业服务，激发科技创新人才活力，加速推动创新资源向海淀和东升集聚，汇聚科技创新力量，助力首都高质量发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/13/1347fd97cf70e5afc3f8ac16e53a4bb6.jpeg\" /></p><p> 北京市海淀区东升镇博展股份社党委副书记、总经理代庆</p><p></p><h4>携手开发者，同成长、共进步、望未来</h4><p></p><p></p><p>万千开发者是华为云生态的中流砥柱，华为云为开发者提供全方位的成长和赋能平台，携手开发者共同构建一个更加开放、创新、共享的云生态。</p><p></p><p>会上，华为云开发者联盟总裁王希海表示，开发者是华为云生态建设的核心，华为云将从“多元生态协同”和“全链路赋能”两个方面赋能开发者，从应用开发、部署、运营到商业变现的全流程支持，为开发者铺好云上成长之路。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04f9a2edaa3078383aeb18fa487d067c.jpeg\" /></p><p>华为云开发者联盟总裁 王希海</p><p></p><p>在这一年里，华为云携手开发者共同推动行业发展，众多领域的开发者汇聚在华为云生态里，借助云原生、AI等新技术，创造出了充满想象力的智能世界。本次活动还特别举办了“2023年度华为云开发者生态贡献人物颁奖仪式”，华为云开发者联盟总裁王希海、华为开发者关系部部长许劲松出席颁奖仪式并为开发者颁奖。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/066aa49fe1a1e46a3575208188ec6dd9.jpeg\" /></p><p>2023年度华为云开发者生态贡献人物颁奖现场</p><p></p><p>会上，华为开发者发展运营总监谢文龙也发表了《与每一位开发者共成长》的主题演讲并表示，华为通过昇腾、盘古大模型、CodeArts Snap、HarmonyOS 等产品和技术能力，支撑各行各业软硬件技术发展，同时，通过举办华为开发者大赛、华为开发者体验官、华为开发者训练营等活动，多路径、多维度助力开发者商业成功，为开发者的成长和发展提供了强有力支持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/84/84cff85c3d870555967071efd458510e.jpeg\" /></p><p> 华为开发者发展运营总监 谢文龙</p><p></p><p>值得一体的是，本次大会还重磅发布了《中国开发者关系白皮书》。《中国开发者关系白皮书》是由思否社区和华为合作推出，对开发者行业进行分析和研究，通过对开发者运营人员的赋能，助力开放者行业高质量发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/707c2f5b208702bce56196e9f7516952.jpeg\" /></p><p>《中国开发者关系白皮书》发布现场</p><p></p><h4>全面赋能企业数字化转型，大咖解析最新技术趋势 </h4><p></p><p></p><p>华为云人工智能算法专家夏飞在本次活动上分享了《全栈自主昇腾云服务，加速大模型应用快速落地》的主题演讲并表示，当人工智能进入大模型时代，随着 AI 大模型技术快速成熟，AI 算法与应用的开发、上线部署与业务发放等过程均大幅简化，使用门槛大幅降低。大模型需要大算力，昇腾云服务构筑全栈 AI 自主可控，为大模型创新应用构筑坚实底座，支持多种大模型应用开发场景，并提供全流程迁移工具，可快速实现大模型和应用的适配，赋能百模千态茁壮成长，加速行业智能化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee93e222cf88ca6cc8aa9b1e820a4818.jpeg\" /></p><p>华为云人工智能算法专家 夏飞</p><p></p><p>薪人薪事产品总监张好在本次活动上分享了《薪人薪事基于华为云盘古大模型的用户产品体验变革》的主题演讲，他指出，基于盘古大模型的产品能力框架，华为云为薪人薪事在数据安全、HR SaaS 服务稳定性等方面持续赋能，加速企业数字化转型，为更科学、高效的人力资源管理及更具竞争力的人力资源运营奠定基础。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11cb078b4c665805fa9fd53aa98864f7.jpeg\" /></p><p>薪人薪事产品总监 张好</p><p></p><p>百模千态开源大模型 AI 挑战赛历时 1 个月，吸引了 1500 余名开发者参赛，其中不仅有人工智能爱好者、企事业单位开发者，还有众多高校师生，共计组成 300 多支参赛队伍，经过初赛、复赛层层激烈角逐，最终 10 支队伍进入决赛。这次大赛的成功举办，不仅展示了华为全栈AI技术的实力，更为开源大模型的研发和应用注入了新的活力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c9be1f5ce0d0fc562739ed9f308f914.jpeg\" /></p><p>百模千态开源大模型AI挑战赛颁奖仪式</p><p></p><p>华为云高级产品经理赵彦在本次活动上分享了《华为云 CodeArts Snap，AI 时代的编码革命》的主题演讲，他认为，数字时代竞争激烈，基于AI大模型的应用研发效率提升在企业竞争力构建中扮演着重要角色，华为云 CodeArts Snap 作为集华为智能算力、模型算法和研发知识沉淀于一体的智能开发助手，通过将自然语言转化为程序代码，提升开发者编程效率，助力企业快速响应市场需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d14f07fab822e514ac029cc15f4b9758.jpeg\" /></p><p>华为云高级产品经理 赵彦</p><p></p><p>作为 2023 年度华为开发者大赛总决赛企业赛道金奖得主，北京天图万境科技有限公司创始人、HCDE 图拉古在现场分享了《用 AI 视听技术构建下一代空间文娱新生态》的主题演讲并表示，全感超空间和超感影游，正是顺应新时代产物，它以 AI 为底层技术支撑，带来了娱乐消费方式的新变革，通过与华为云的紧密合作，共同推动产业发展，为用户带来更加丰富、更加沉浸式的娱乐体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78dd64fca3cb43d25129cd5cec0087fb.jpeg\" /></p><p>北京天图万境科技有限公司创始人、HCDE 图拉古</p><p></p><p>大会对北京 HCDG（华为云开发者社区）、HCDE（华为云开发者专家）分别进行了表彰，华为云开发者联盟总裁王希海、华为云开发者生态运营总监胡志学分别为本地 HCDG、HCDE 进行授旗、授牌。未来，华为云将持续推动开发者社区建设，携手各行各业技术专家共同构建开发者生态，为整个开发者生态的繁荣和发展做出更大的贡献。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6244eb444a82a215414d4855f73c725.jpeg\" /></p><p> 北京HCDG（华为云开发者社区）授旗现场</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0bafc8a0392141b21e17b0bac55cbf3.jpeg\" /></p><p>北京HCDE（华为云开发者专家）授牌现场</p><p></p><h4>体验技术盛宴，人人争当技术领跑者</h4><p></p><p></p><p>本次活动还设置了 KooLabs 工作坊、产品体验官、展区等多个环节，为广大学生开发者量身打造了多项技术赋能实操类活动，吸引了北京航空航天大学、北京理工大学，北京邮电大学，北京科技大学，北京工业大学等众多知名高校学生踊跃参与，同学们表示看到了许多具有创新性的开发设计，参与不同学科的理论创享，丰富了眼界和知识。</p><p></p><p>在开发者体验官环节，开发者们亲身体验华为明星产品，华为团队倾听开发者声音，体验反馈帮助华为不断优化和改进产品，以更好地满足开发者，实现产品共创，生态共赢。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/caa9a5dc9618f34ef74827f81f50251e.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdc28249bcb9b0748106bfea74220bda.jpeg\" /></p><p>开发者体验官</p><p></p><p>在现场还有机会体验华为云技术专家为参与者提供的专业的实验指导，深度体验华为云服务，在云端实现云服务的实践、调测和验证。KooLabs 工作坊不仅给开发者来了技术赋能，也为数字产业人才生态的发展提供了有力支持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2024a300f8092de497cabf9c7966236.jpeg\" /></p><p>KooLabs工作坊</p><p></p><p>未来，华为云将继续与广大开发者携手，共同构建产业新生态，对开发者持续赋能，助力开发者行业实现高质量的发展，为开发者提供全方位的支持，助力开发者提升自我，挑战自我和实现自我，让千万开发者在云上创新，释放数字创新源动力。</p>",
    "publish_time": "2023-12-22 10:46:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖确认出席 QCon 上海，分享借助 MetaGPT 之力，实践自然语言编程的前沿探索",
    "url": "https://www.infoq.cn/article/QecDLpTkeOGuGW938r64",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1222&amp;utm_content=wuchenglin\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖将发表题为《<a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5671?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1222&amp;utm_content=wuchenglin\">借助 MetaGPT 之力，实践自然语言编程的前沿探索</a>\"》主题分享，探讨 MetaGPT 如何成为自然语言编程的桥梁，推动智能体社会的发展，以及如何引领自然语言编程迈向更高效、更智能的阶段。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5671?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1222&amp;utm_content=wuchenglin\">吴承霖</a>\"，深度赋智创始人兼 CEO。拥有在腾讯等公司十亿级用户、千亿级数据的大规模复杂 AI 落地经验；开源多智能体框架 MetaGPT 作者；NeurIPS AutoDL/NeurIPS AutoWSL 等顶级竞赛世界冠军；多篇论文发表于 TPAMI/KDD/CVPR/ACL 等顶会顶刊；曾获得福布斯 30U30，腾讯、华为内部数十奖项荣誉。他在本次会议的演讲内容如下：</p><p></p><p>演讲：借助 MetaGPT 之力，实践自然语言编程的前沿探索</p><p></p><p>MetaGPT，作为自然语言与编程之间的催化剂，正在推动着我们走向一个更加智能、高效的编程未来。本次演讲将深入 MetaGPT 如何成为自然语言编程的桥梁，推动智能体社会的发展，以及如何引领自然语言编程迈向更高效、更智能的阶段。</p><p></p><p>在这次演讲中，我将深入研究记忆的重要性、近因性和相关性，并分享关于经验获取和记忆学习的技术见解。</p><p></p><p>演讲提纲：</p><p></p><p>MetaGPT 的发展与影响智能体社会与人机协同</p><p>○ 智能体社会：Jürgen Schmidhuber 携手 MetaGPT</p><p>○ 多智能体将成为社会中的一个重要构成</p><p>○ 记忆压缩</p><p>○ 经验获取</p><p>○ 技能学习</p><p>○ 人机协同新范式：智能体与我们的共创未来</p><p>○ 99% 的互联网入口将由 App 变为智能体</p><p>技术挑战与未来展望</p><p></p><p>听众收益点：</p><p></p><p>○ 了解 MetaGPT 在自然语言编程中扮演的角色</p><p>○ 了解 MetaGPT 如何影响智能体社会和自然语言编程的未来</p><p>○ 了解智能体社会中，自然语言编程将如何被进一步发展</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart\">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的性能优化</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>12 月 28-29 日，QCon 全球软件开发大会即将落地上海，中国科学院外籍院士、国际数据库专家樊文飞院士，英特尔大数据技术全球 CTO 戴金权等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。</p><p></p><p>这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，感兴趣的朋友可以扫描下方二维码，查看大会详细日程。咨询购票可联系票务经理 18514549229。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-12-22 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度 Comate：提升编码效率，释放“十倍”软件生产力",
    "url": "https://www.infoq.cn/article/NlLHGHXdGAg4XW61U3Z2",
    "summary": "<p>百度资深工程师、百度 Comate 架构师徐晓强将为你深度解读 Comate 应用实践——全面解析智能代码助手「百度Comate」全流程提效方法、效果及其典型使用场景，并通过现场实操，帮助开发者高效编程！</p>",
    "publish_time": "2023-12-22 11:40:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里云 CTO 周靖人确认出席 QCon 上海，深谈云计算崭新时代",
    "url": "https://www.infoq.cn/article/KUzOm728U94NC6C4Vvvz",
    "summary": "<p></p><blockquote>在这个充满创新和变革的时代，阿里云CTO周靖人即将在12月28日的<a href=\"https://qcon.infoq.cn/202312/shanghai/schedule\">QCon全球软件开发大会上海站</a>\"上，发表题为<a href=\"https://qcon.infoq.cn/202312/shanghai/presentation/5698\">《拓展智能边界，开启云计算崭新时代》</a>\"的主题演讲。大模型时代，看看周靖人对于云计算的思考，与你一同探寻在这一浪潮中我们能够把握的挑战与机遇。&nbsp;周靖人曾是前微软研发合伙人，达摩院智能计算实验室、大数据智能计算和搜索推荐平台负责人、淘宝事业群搜索推荐事业部负责人。2022年12月，任阿里云CTO兼任达摩院副院长。2023年4月，接任阿里巴巴华东有限公司法定代表人、执行董事、总经理。</blockquote><p></p><p>&nbsp;</p><p>随着人工智能技术的迅速发展，我们正迎来大模型时代的曙光。这些模型，以其庞大的数据处理能力和深入的学习机制，正改变着科技界的面貌。在AI领域，大模型不仅代表了数据处理能力的巨大飞跃，也带来了前所未有的创新机遇和挑战。</p><p>&nbsp;</p><p>前不久，周靖人曾谈到过关于阿里云大模型的设想：将 AI 模型的能力开放给更多开发者和合作伙伴。“我们的目标不仅是服务C端产品，更是要打造一个服务各类AI时代创业者、开发者和企业客户的平台”。阿里云以其前沿的 AI 基础设施和模型能力，致力于成为这些创新者最坚强的后盾。</p><p>&nbsp;</p><p></p><h4>全面升级的技术革命：迈向新时代的技术体系</h4><p></p><p>&nbsp;</p><p>周靖人曾谈到，当前的 AI 技术变革，实质上是一场技术体系的全面升级。他提出了三大核心纬度：首先，系统优化，即利用模型能力优化复杂庞大的分布式系统，实现“自动驾驶的云”；其次，通过模型提升开发效率，让云计算变得更智能；最后，以模型为中心，打造最优秀的AI基础设施，提供低成本、高效能的模型训练、微调、推理服务。</p><p>&nbsp;</p><p>那么，云计算的未来将如何演进？在大模型的加持下，我们将面临哪些挑战和机遇？让我们在QCon大会上聆听周靖人的深度解读，探索这个精彩纷呈的新时代。不容错过的洞察，期待与您现场交流。为让无法到场的朋友也能感受本届盛会，12 月 28 日 09:00，QCon上海站还将在 InfoQ视频号对主会场全程直播，赶快扫描下方二维码预约吧。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/24/a6/245aa5bba00b64ff47f21f53f27768a6.png\" /></p><p>&nbsp;</p><p>关于 QCon：作为极客邦科技旗下InfoQ中国主办的综合性技术盛会，QCon全球软件开发大会自2007年以来，每年在全球多个城市召开。12月28-29日，QCon 将落地上海，本次既是技术人的年终聚会，也是 QCon15 周年庆典。欢迎您前来与近千位技术领袖围绕 <a href=\"https://qcon.infoq.cn/202312/shanghai/track/1595\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/202312/shanghai/track/1596\">AI Agent与行业融合应用的前景、</a>\"<a href=\"https://qcon.infoq.cn/202312/shanghai/track/1597\">LLM时代的性能优化</a>\"等多个前沿话题进行深入交流。</p><p>&nbsp;</p>",
    "publish_time": "2023-12-22 12:38:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "创始人3天狂砍5万行代码后，应用程序更快、更易使用了",
    "url": "https://www.infoq.cn/article/empFkGQ183cHqHp5HGDR",
    "summary": "<p>软件开发和数据科学学习平台 Jovian 的创始人兼首席执行官、前 Twitter 软件工程师 Aakash N S 日前发文表示，自己在 Jovian 的代码库中删除了 5 万多行代码，这些代码占前端代码库的 70%。让人惊喜的是，删完这些代码后并没有破坏应用程序，也几乎没有造成任何实质性的损失，反而使得应用程序和代码库彻底简化，平台更轻、更容易使用。</p><p>&nbsp;</p><p>正如 Antoine de Saint-Exupery 所言：“达到完美，不是没有东西可添加了，而是再也没有什么可去掉时”。软件开发也是一样的道理，有些时候，化繁为简才会让软件变得更好。</p><p></p><h2>典型软件应用程序中超过 80% 的功能几乎从未被使用过</h2><p></p><p>&nbsp;</p><p>最近，我从 Jovian 的代码库中删除了 50000 多行代码，这些代码之前都在生产环境中运行，为我们的 Web 应用程序提供支持，该 Web 应用程序每天处理数十万个请求。被删除的代码约占我们前端代码库的 70%，是用 React 和 Next.js 编写的 JavaScript。</p><p>&nbsp;</p><p>我惊喜地发现，我可以在短短的三天内删除超过三分之二的代码，并且完全不会破坏应用程序。然而，我发现更令人惊讶的是，在这个过程中，几乎没有任何实质性的损失。另一方面，清理导致了应用程序和代码库的彻底简化，使得平台更轻、更容易使用了。</p><p>&nbsp;</p><p>自 2019 年以来，我们一直在构建 Jovian，多年来该平台经历了各个不同的发展阶段。我们在需要的时候添加了新功能、页面、按钮和设置，但我们却很少考虑删除（谁会这么做呢？）。我们有意识地努力保持应用程序的简单易用性，但它还是积累了很多“功能债”。</p><p>&nbsp;</p><p>我曾在某处读到过，在一个典型的软件应用程序中，80% 以上的功能几乎从未被使用过，我记得看到过这张截图，该图显示了 Microsoft Word 中所有可用的工具：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/93b66b0d6f46dc8d1b8c3a7384146a42.png\" /></p><p></p><p>虽然 Jovian 的情况并没有那么糟糕，但复杂的应用程序和庞大的代码库确实会使变更、升级库和添加可能真正重要的新功能变得困难。</p><p></p><h2>删除 5 万行代码后，应用程序还好吗？</h2><p></p><p>&nbsp;</p><p>几周前，我开始实验性地重写我们的 Web 应用程序，使用最新版本的 Next.js 构建，并部署到 Cloudflare Pages。我很快意识到，从头开始全面重写是行不通的。重新构建整个应用程序需要花费几个月的时间，而且几乎可以肯定，它会导致数百个缺陷。大爆炸式的迁移很少能奏效，即使奏效，也会比计划的时间要长得多。我在 Twitter 工作时亲身体验过了，在那里我花了几个月的时间将代码从 Ruby on Rails 迁移到 Scala。</p><p>&nbsp;</p><p>然而，增量迁移也会带来了一系列的挑战。我创建的新页面在结构和内容上都非常简单，而我们应用程序中的现有页面有多个交互式选项卡、按钮和小部件。我们现有的 Web 应用程序使用由 Python 后端提供的 REST API，而我想利用 server actions 直接通过 Next.js 与我们的数据库进行交互，以使应用程序更快，同时消除托管后端的成本。这需要将数百个包含数万行重要业务逻辑的 API 端点从 Python 迁移到 JavaScript，而我并不希望这样做。</p><p>&nbsp;</p><p>一时之间，似乎已经没有办法摆脱这种局面了。该应用程序及其代码库看起来就像是一头巨大的大象，只能缓慢而小步前进，它根本不愿意移动。每次在 VS Code 中打开代码库时，我都会遇到巨大的阻力，因为它所涉及的工作量实在是太多了。</p><p>&nbsp;</p><p>然后，几天前，我突然意识到，通过减肥我可以走得更快。我可以从屏幕上删除不必要的小部件和按钮，将其迁移到新的技术栈（幸运的是，Next.js 支持增量迁移），然后再添加回删除的元素。然而，接下来发生的事情只能用“大屠杀”来形容：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/026f13c1a8424306b222f72c549f29a0.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e0bbcd296044215531c573c3d40aa7b.png\" /></p><p></p><p>我并没有打算删除代码库中三分之二的代码。我想删除的只是能足够开始迁移的一个模块而已。我通过 Google Analytics 查看了过去 30 天每个模块的页面访问量，以确定从哪里开始。虽然我知道有些页面的访问频率低于其他页面，但我惊讶地发现，有些模块的访问量占比不到总页面访问量的 0.1%。这意味着我可以完全删除它们，而不会影响 99.9%的用户。我可以删除包含数十个文件和数千行代码的整个目录。</p><p>&nbsp;</p><p>当我从应用程序的其余部分中删除了那些很少被使用的模块及其入口点时，我慢慢地发现应用程序变得越来越简单了，选项卡、页面和菜单项也越来越少了。删除未使用代码的感觉也很好。随着代码库变得越来越小，我对迁移的阻力和焦虑也开始减轻。最初，我对删除我们花了几周或几个月时间构建的模块而感到很难过，但我总能从 Git 历史中恢复我将来需要的任何东西。</p><p>&nbsp;</p><p>在删除了那些很少被使用的整个模块后，我继续删除了占总流量不到 0.5% 的单个屏幕和弹出窗口。再次，我惊讶地发现，有数百个文件可以被删除，而不会影响 99.5% 的用户。事实上，这对用户也产生了积极的影响。五个选项卡变成了三个，然后是两个，再是一个，此时，选项卡可以完全从页面中删除。许多页面现在有了更直接的结构和单一的主要操作。更少的API调用导致更快的页面加载和屏幕转换。感觉好极了！</p><p>&nbsp;</p><p>一旦删除了所有不必要的模块和页面，我就通过 Google Analytics 进一步挖掘，以确定剩余页面上的功能使用频率。我发现了一些很少使用的功能，几乎从未点被击过的按钮，甚至从未可见过的菜单项。所以，我开始删除它们。我拿走的越多，我就越喜欢剩下的。我删除了几个侧边栏、下拉菜单、按钮、小部件、弹出窗口以及支持它们所需的内部应用程序状态和条件逻辑。这也使得代码更易于理解了，这将进一步简化迁移。最初看似需要几个月的努力，现在感觉可以在几周内完成。</p><p>&nbsp;</p><p>沉没成本谬论和损失厌恶是人类的偏见，使我们很难放弃不再需要的东西。我仍然担心我可能删除了太多或删除了一些重要的东西，但分析表明情况并非如此。今天访问该网站的用户数量与清理之前一样多，而且他们可以（并且正在）做和之前几乎一样的所有事情。毫无疑问，新用户会发现该平台更易于导航和使用了。</p><p>&nbsp;</p><p>我知道每隔几年删除 70% 的代码可能会让很多其他应用程序受益。它能降低维护开销，使它们能够更快地移动，减少加载时间，并传送更小的包，同时改善用户体验，使软件变得更好。</p><p></p><p>原文链接：</p><p><a href=\"https://aakashns.com/delete\">https://aakashns.com/delete</a>\"</p>",
    "publish_time": "2023-12-22 14:41:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "历时9个月、从零开始训练，Midjourney V6来了！号称比以往所有版本都强大",
    "url": "https://www.infoq.cn/article/PUBbtUKvLOw5JB05B4Jd",
    "summary": "<p></p><blockquote>新模型将带来更强大的enhancer、upscaler、提示词遵循以及文本生成功能。当然，审查机制也更加严格。</blockquote><p></p><p>&nbsp;</p><p>圣诞前夕，又一份大礼从天而降：由David Holz主导开发团队打造的高人气图像生成AI模型的最新、最强版本Midjourney V6现已发布，目前处于alpha测试阶段，并立即得到众多高级用户的关注。</p><p>&nbsp;</p><p>新版本带来一系列令人欣喜的改进，也帮助那些已经在通过Midjourney乃至其他AI艺术工具创作图像的用户巩固了信心。</p><p>&nbsp;</p><p>在官方发布的Discord帖子中，该公司将V6版本定位为重大革新成果。</p><p>&nbsp;</p><p>公告解释称，“提示词遵循效果将更加准确，可容纳的提示词更长、连贯性更高、模型知识也更为丰富。”此外，公告还强调了V6版本相较于2023年5月推出的V5.1版模型的进步之处。V5模型的主要亮点在于出色的易用性，可支持简短提示词并带来美学效果提升，这也为处理能力更强、更复杂的V6版本奠定了基础。</p><p>&nbsp;</p><p>实际上OpenAI DALL-E 3以及Ideogram等竞争对手AI图像生成器已经推出了此类功能，但Midjourney自2022年亮相以来却始终未能实现。</p><p>&nbsp;</p><p>Holz在Midjourney Discord服务器（目前已拥有超1700万会员）发帖指出，“这套模型生成的图像在真实度方面远超我们以往发布的任何版本。”Holz还提到，V6实际是“我们在AI超级集群上从零开始训练而成的第三套模型”，整个开发周期长达九个月。</p><p>&nbsp;</p><p></p><h2>同类型产品相比，MJ V6表现如何？</h2><p></p><p>&nbsp;</p><p>V6模型最值得关注的功能之一，就是其文本绘制功能。虽然并不属于本次升级的重点（开发团队表示这仍属于「次要」功能），但这仍令MidJourney获得了直接与DALL-E 3乃至Ideogram等其他领先模型直接竞争的资格。更重要的是，MidJourney采取了一种截然不同的独特文本生成方法。</p><p>&nbsp;</p><p>MidJourney表示这是一种“次要文本绘制能力，用户必须在「引号」内编写文本，并配合—style raw或者更低的—stylize值来实现生成。”</p><p>&nbsp;</p><p>这里使用Decrypt对MidJourney与以文本生成准确性而闻名的DALL-E 3进行了测试比较。从结果来看，MidJourney似乎优先考虑风格和美观度，有时甚至会为此而牺牲文本准确性。大多数时候，它生成的文本要么不够准确、要么无法生成。但只要能够顺利输出，其图像质量至少与DALL-E 3的结果相当、甚至更好。顺带一提，DALL-E 3是专为ChatGPT和微软Bing提供技术支持的文本到图像AI模型。</p><p>&nbsp;</p><p><a href=\"https://shimo.im/docs/L9kBBjpQ5JU8Z6kK/\"></a>\"</p><p><img src=\"https://static001.geekbang.org/infoq/6e/6ef30ba98fc441a800ba089c9d70d710.png\" /></p><p></p><p>将MidJourney、DALL-E 3、SDXL加Harrlogos以及Ideogram AI的文本生成功能进行比较，最简单的概括就是MidJourney更适合那些以美观为优先考量的需求，DALL-E 3在易用性和卡通风格数字创作上表现较好，SDXL主要面向那些精通A1111&nbsp;WebUI的用户，而Ideogram AI则更善于牺牲一点美学效果来换取文本还原效果。</p><p>&nbsp;</p><p>MidJourney和ChatGPT上的DALL-E 3目前均需要付费使用，但SDXL和Ideogram AI则免费开放。Bing版本的DALL-E 3倒是提供免费使用，但仅支持生成矩形图像，而且用户只能修改提示词、无法直接使用OpenAI提供的自然对话方式。</p><p>&nbsp;</p><p>V6的速度比V5略慢一些、成本也更高，但该团队希望能随时间推移而加快模型速度。V6模型还拥有更加“微妙”且“创意性”的upscaler，能够将图像分辨率提高至2倍。</p><p>&nbsp;</p><p>将这些功能与各种受支持的参数（例如用于更改分辨率的—ar、用于在每次生成结果间体现差异的—chaos、用于更改模型创意程度的—stylize等）相结合，将为用户带来广泛探索创意空间的可能性。但图像修复、覆盖和图像描述等功能尚不可用。据MidJourney介绍，这些功能应该会在下个月逐一补全。</p><p>&nbsp;</p><p>公告鼓励用户们运用这些“令人难以置信的力量，但在享受愉悦与惊奇也应保持负责和尊重的态度”，这也一直是MidJourney抱持的宗旨所在。而且后半部分所言非虚，官方的审查制度也将更加严格。</p><p>&nbsp;</p><p>公告中写道，“别干坏事，也不要创作有争议的图像。”这很可能是指MidJourney将阻止创作色情或跟政治相关的Deepfake图像。</p><p></p><h2>如何使用MJ V6新模型？</h2><p></p><p>值得一提的是，此次更新似乎不会默认对用户开放。大家需要在Midjourney Discord服务器中、或者在Midjourney机器人的直接消息（DM）栏中输入斜杠命令“/settings”，之后在上方的下拉菜单中选择V6。或者，也可以按照传统方式进行操作，在提示词后方手动输入“—v 6”。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1d/1d1413dd387af405016377a1bf65a262.png\" /></p><p></p><h2>MJ V6有什么新功能？</h2><p></p><p>具体来讲，Holz公布了以下几项新功能：</p><p>&nbsp;</p><p>更准确的提示词遵循效果，并可容纳更长的提示词；提高了输出一致性和模型知识储备；改进了图像提示与重新混合；次要文本绘制能力（用户需要在「引号」内编写文本，配合—style raw或者更低的—stylize值可能效果更好）。改进的upscaler，提供“subtle”（微妙）与“creative”（创意）两种模式（可将分辨率提升至2倍）。</p><p></p><h2>鼓励新的提示词编写方式</h2><p></p><p>作为Midjourney项目的创始人和负责人，Holz还公布了一种全新的提示词编写方法。</p><p>&nbsp;</p><p>长期以来，Midjourney要求用户在Discord服务器或者Alpha版本的网站中输入特定的文本描述加关键词来生成图像，但很多使用者反映体验深奥而且相当考验技术。为此，用户们还专门在社交媒体上分享了比较好用的提示词编写范式，例如引用相机名称（例如徕卡M11）、胶片格式（35毫米）和分辨率（8k），以便从AI模型中获取高质量、逼真甚至趋近电影的视觉效果。</p><p>&nbsp;</p><p>但Holz在他的Discord帖子中明确指出，这类提示词编写方式在V6上将呈现出与期望相背的效果。“大家需要重新学习如何编写提示词。”</p><p>&nbsp;</p><p>V6模型的使用方式与V5差异较大，您需要“重新学习”如何编写提示词。V6对于提示词的内容更加敏感，请勿使用诸如“广受好评、逼真、4k、8k”之类的“垃圾描述”。请明确表达需求。V6可能表现得不那么机灵，但只要提供明确的提示，它现在可以更好地理解您的意图。如果希望生成摄影风格/少点自由发挥/多点忠于提示词的内容，则应默认使用—style raw。将—stylize的值设置得更低（默认为100）往往有助于改善提示词理解效果，而较高的值（最高1000）则倾向于牺牲还原度来换取美学效果。您可以在prompt-chat中通过聊天来了解如何使用V6新模型。</p><p></p><h2>MJ V6用起来怎么样？</h2><p></p><p>模型刚发布不久，就已经有国外网友简单测试了MJ V6。该名网友表示，“至少就个人使用体验来讲，此次更新只能说是平淡无奇。虽然确实看到了更多的细节和更逼真的生成效果，但区别跟上代模型并不是很大。反正我是没办法一眼就看出哪张图片是V5.2生成的、哪张是V6生成的。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/59/59368f7f90361337ca39cc7802f0348f.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e09a705ea68b88b34f5ee3d3170602a9.png\" /></p><p></p><p>但不可否认，V6生成的灯光效果和反射细节确实让人深刻印象。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/ec/ecab4a76b900038f78f90a388f3e7b5c.png\" /></p><p></p><p>包括恐怖片导演兼数字艺术家Chris Perna在内的其他狂热用户，已经开始对MJ V6的生成功能进行全面测试，并将成果发布到了Instagram及其他社交媒体网站之上。从早期示例来看，V6的文本生成效果确实相当出彩。</p><p>&nbsp;</p><p>Chris Perna发文并配图称，“刚开始，“克苏鲁觉醒”还真让新版V6有点懵。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3243bfa1f8d3c2a0674db1800fba682.png\" /></p><p></p><p>一些网友也晒图并发表了自己对于V6的看法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5fb1021ca004e44faf1822f9f9efdabc.png\" /></p><p></p><p>Midjourney V6……终于可以绘制文字啦！也许效果还不完美，但我一直在探索要如何实现。这四张图都是一次生成的结果，可能是我运气好吧🤷‍♂️</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/9b/9ba1eb6915a3efb86658cac696b32295.png\" /></p><p></p><p>Midjourney V6中的皮肤细节令人难以置信。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c0/c0233bf16dc36d8cec721085a8919549.png\" /></p><p></p><p>Midjourney V6的生成效果非常出色！同等分辨率下的细节大幅增加。请注意，这并不是最终模型图像，也没有经过upscale处理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/599598b04618b1787d8f1432af539a0f.png\" /></p><p></p><p>使用相同提示词，从Midjourney V1到V6的生成效果区别：白色背景、苍老刻薄的男性肖像特定，92岁，皱纹，逼真的皮肤质感，室内照明，佳能f/4。</p><p>&nbsp;</p><p>Holz在发布V6的Discord帖子中指出，新模型“尚处于alpha测试阶段。期间会有很多调整变更，恕不另行通知……在最终正式发布V6时，很多情况将发生重大变化……V6也不会是Midjourney的终点，希望大家能够感受到这套满载我们集体智慧与创意结晶的模型的一路发展和演进。”</p><p>&nbsp;</p><p>此外，V6目前还缺少V5.2模型中的一些功能，包括左右平衡和缩小，但Holz表示这些功能将在V6的后续更新中实现。</p><p>&nbsp;</p><p>作为许多人眼中最卓越、质量最出色、也最具创意的AI艺术生成器，Midjourney的此次更新表明其从未停止技术探索和模型改进的脚步，而且在市场上也始终保持着领先地位。目前挑战Midjourney的竞争对手要么使用内部自有模型，要么选择开源Stable Diffusion模型——这是一种流行的AI底层技术，其中的扩展算法经过训练以从视觉“噪声”中重新创建图像。</p><p>&nbsp;</p><p>与此同时，Midjourney和其他基于扩散技术的AI艺术生成器也面临着艺术家们发起的版权侵犯集体诉讼。这些艺术家指控对方在未经自己明确同意、或提供补偿的情况下，利用他们公开发表的作品训练AI模型。但AI厂商也没有坐以待毙，正在积极探索在AI艺术创作工具中建立强大的“安全使用”防侵权机制。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://venturebeat.com/security/in-todays-global-threat-landscape-it-pays-to-go-back-to-basics/\">https://venturebeat.com/security/in-todays-global-threat-landscape-it-pays-to-go-back-to-basics/</a>\"</p><p><a href=\"https://decrypt.co/210637/midjourney-v6-base-model-upgrade-text-generation\">https://decrypt.co/210637/midjourney-v6-base-model-upgrade-text-generation</a>\"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-12-22 15:13:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "云智融合转型期，国产服务器操作系统路在何方？",
    "url": "https://www.infoq.cn/article/Jys1CmkLHLLvGTNsiIKx",
    "summary": "<p></p><p></p><p>纵观当下&nbsp;IT&nbsp;产业局面，云与&nbsp;AI&nbsp;已经成为产业升级的两大公认核心驱动力，尤其是生成式&nbsp;AI&nbsp;快速席卷整个行业后，将云计算与&nbsp;AI&nbsp;技术融合来激发业务创新的理念也很快得到了普遍认可。服务器操作系统作为云和&nbsp;AI&nbsp;的底层基础设施关键软件，在这场变革中所扮演的角色是非常重要的。云服务厂商与&nbsp;AI&nbsp;应用开发者都对服务器操作系统提出了很多新的需求，这些需求推动操作系统领域技术不断推进，使整个服务器操作系统行业逐渐步入&nbsp;2.0&nbsp;时代。</p><p></p><p>在云智融合转型趋势引领服务器操作系统迭代升级的时期，近年来发展势头欣欣向荣的国产服务器操作系统社区该如何应对？怎样把握时机，解决挑战，突破用户固有认知，从而实现快速崛起，并为云智融合时代构筑行业所需的软件基础平台？在近日召开的首届龙蜥操作系统大会期间，InfoQ&nbsp;采访了龙蜥社区理事长单位阿里云基础软件部产品总监张鹏程与阿里云操作系统专家贾正华，探讨了阿里云与龙蜥社区作为国产服务器操作系统的核心阵容之一，如何借助云智融合机遇推动国产力量发展，未来将有哪些长远规划等主题。</p><p></p><p></p><h2>从&nbsp;Alibaba&nbsp;Cloud&nbsp;Linux&nbsp;到&nbsp;Anolis&nbsp;OS，龙蜥社区的起源与现状</h2><p></p><p></p><p>早在&nbsp;2009&nbsp;年，阿里云已经开始自行研发服务器操作系统用于内部业务。经过多年研发和实践，阿里云的内部操作系统逐渐走向成熟，而云端用户也开始对这款操作系统有了一定需求，因此&nbsp;2017&nbsp;年，阿里云将其正式开源，向阿里云用户开放使用，这就是&nbsp;Alibaba&nbsp;Cloud&nbsp;Linux（简称ALinux）。</p><p>&nbsp;</p><p>2020&nbsp;年，面对国产化生态发展的大趋势，阿里云联合多家理事单位共同建设了龙蜥社区，龙蜥社区的建立恰逢其时的在&nbsp;CentOS&nbsp;停服过程中发挥了关键作用。根据中国信息通信研究院面向用户群体的调研显示，龙蜥操作系统位列用户意愿迁移系统之首，比例超过半数达到53%。龙蜥社区初期阿里云将&nbsp;ALinux&nbsp;操作系统积累的技术和经验大量投入龙蜥社区，使龙蜥社区在&nbsp;2021&nbsp;年初就发布了社区的第一代操作系统，名为&nbsp;Anolis&nbsp;OS。Anolis&nbsp;OS&nbsp;是一款主要面向多样化场景需求，能够为社区成员提供基础平台，能够满足云智融合需求的新型操作系统。相比之下，ALinux&nbsp;以服务阿里云和阿里云用户为目标，主要负责释放阿里云上软硬件协同优势和服务能力，而&nbsp;Anolis&nbsp;OS&nbsp;则是龙蜥社区内开源项目的载体，承载着社区内多样化技术创新的孵化任务。</p><p>&nbsp;</p><p>当然，ALinux&nbsp;与&nbsp;Anolis&nbsp;OS&nbsp;是互相学习、共同进步发展的。ALinux&nbsp;的许多技术创新，经过实践认可与成熟后会经由阿里云贡献到龙蜥社区，而龙蜥社区各成员为&nbsp;Anolis&nbsp;OS&nbsp;所做的各种改进和功能，ALinux&nbsp;也会选择吸收，为自身注入新的活力。</p><p>&nbsp;</p><p>ALinux&nbsp;与&nbsp;龙蜥社区和&nbsp;Anolis&nbsp;OS&nbsp;的关系，代表了国产服务器操作系统的一条非常典型的发展路径：拥有较强技术实力的厂商在自身实践中开发出水平较高的操作系统并开源，之后牵头发起新的系统生态社区，并将自有系统的成熟技术注入社区形成原动力，吸引和聚合产业力量共同投入社区的开源项目，助力社区生态走向繁荣。这条路径的优势在于，阿里云这样的厂商在过往的实践中积累了大量生产经验，尤其在淘宝双十一这样的超大流量场景中对自有系统做了充分历练，其运维稳定性、可靠性、安全性都可以满足极高的要求。这些高水平的技术成果输出到新成立的开源服务器操作系统社区后，后者就能够站在很高的平台上起步，大大缩减了新品生态早期发展的周期，也更容易吸引不同领域的佼佼者加入社区作出贡献。</p><p>&nbsp;</p><p>正是因为有了来自&nbsp;ALinux&nbsp;输出的，较为充足的技术储备，Anolis&nbsp;OS&nbsp;在短时间内就具备了足够的能力应对服务器操作系统领域的转型挑战。随着云计算与&nbsp;AI&nbsp;融合的脚步临近，龙蜥社区开始运筹帷幄，努力将&nbsp;Anolis&nbsp;OS&nbsp;打造为引领变革的先锋力量。</p><p></p><p></p><h2>云智融合，云计算产业的第三次浪潮</h2><p></p><p></p><p>在云计算技术诞生初期，很多行业中很早就开始在互联网上面向公众提供服务的企业自然而然地开始使用云服务，他们的需求造就了云计算产业的第一次浪潮，这一阶段云计算的行业增长主要是由互联网企业带动的。随着云计算技术逐渐成熟，一批传统企业也开始基于信息技术发展成果开始了对企业的信息化升级过程，开启数字化转型，并引发了云计算产业的第二次浪潮。而今天，生成式&nbsp;AI&nbsp;技术的飞速发展又对云计算行业起到了极大的推动作用，使行业即将迎来第三次大跨越。</p><p>&nbsp;</p><p>从服务器操作系统的角度来看，云计算的发展过程中，操作系统一直扮演着向下释放底层硬件能力，向上支撑更多软件和业务生态的角色。如今，随着生成式&nbsp;AI&nbsp;技术的崛起，服务器操作系统需要向上更好地支撑&nbsp;AI&nbsp;应用发挥作用，向下则需要更好地利用超大规模的计算能力来为&nbsp;AI&nbsp;提供资源。而在面向开发者和用户的角度上，服务器操作系统也要帮助他们更好地利用云端&nbsp;AI&nbsp;能力。</p><p>&nbsp;</p><p>但是，生成式&nbsp;AI&nbsp;技术对算力资源的庞大需求和需求增长的高速度，意味着服务器操作系统要面对一项前所未有的挑战，那就是操作系统需要很好地应对通用计算与异构计算共存的硬件局面，同时能够充分适应异构计算能力高速增长和多样化发展的状态，为大幅膨胀的算力资源做好优化适配工作。另一方面，生成式&nbsp;AI&nbsp;所需要的庞大数据量，结合超大的算力规模，都对操作系统的安全性、稳定性提出了极高的要求。</p><p>&nbsp;</p><p>在这样的背景下，龙蜥社区恰恰有着非常大的天生优势。由于&nbsp;Anolis&nbsp;OS&nbsp;的起步动力源自&nbsp;ALinux，而后者又是经过阿里云多年的大规模云端服务经历磨炼而成，Anolis&nbsp;OS&nbsp;就天然具备了解决超大规模算力资源挑战的能力，无论是软件优化工作还是安全、稳定性保障都得心应手。另外ALinux当前正在构建AI优化容器镜像为用户提供开箱即用的AI运行环境，以解决当前AI场景下客户面临的南北向生态兼容性问题和性能优化诉求，未来也会将其中打磨的镜像发布平台、性能优化技术、生态兼容能力贡献到龙蜥社区。在此基础上，龙蜥社区的理事单位与社区成员又为&nbsp;Anolis&nbsp;OS&nbsp;贡献了大量核心技术，使&nbsp;Anolis&nbsp;OS&nbsp;从一开始就集成了许多&nbsp;AI&nbsp;软件栈能力。例如，英特尔公司作为龙蜥社区理事单位，就帮助&nbsp;Anolis&nbsp;OS&nbsp;很早适配了英特尔第四代可扩展至强处理器的&nbsp;AMX&nbsp;加速引擎特性，大大提升了上层&nbsp;AI&nbsp;应用的推理性能。</p><p>&nbsp;</p><p>凭借这些优势，Anolis&nbsp;OS&nbsp;就成为服务器操作系统领域最容易适应云智融合转型的系统品牌之一，龙蜥社区也成为服务器操作系统步入&nbsp;2.0&nbsp;时代后，社区活跃度与创新能力均排在行业前列的系统生态。</p><p>&nbsp;</p><p>对于国产服务器操作系统产业来说，龙蜥社区牢牢把握住云智融合这一历史机遇，也为国产生态步入世界前列打下了良好基础。虽然与国外相比，国产操作系统生态普遍起步晚、能力弱、技术不足，但龙蜥社区今天正在迎头赶上，充分利用来自各行业的优秀技术成果来提升水平，助力国产系统向着全球领先的地位不断迈进。当然，龙蜥社区能够形成这样的健康发展模式，其公平、开放、包容的社区氛围同样是非常重要的。在这种氛围下，龙蜥社区也为未来的长期发展描绘了一幅多彩画卷。</p><p></p><p></p><h2>基于公平开放原则，龙蜥社区走上发展快车道</h2><p></p><p></p><p>今天的龙蜥社区拥有&nbsp;24&nbsp;家理事单位，来自芯片、服务器组件、操作系统、云服务等各个领域，而所有理事单位的话语权都是平等公正的。每一家理事单位都可以站在自身视角上提出产业发展需求，并在一个透明的平台下公开讨论，经过这样的持续讨论，龙蜥社区就有了统一、具有延续性的发展路线。与此同时，社区也在不断扩大合作伙伴规模，吸纳更多成员加入社区。很多社区伙伴还会基于&nbsp;Anolis&nbsp;OS&nbsp;推出自己的操作系统衍生版本，到目前为止已经有&nbsp;12&nbsp;个版本面世。这些版本保持了对龙蜥生态的兼容性，同时各自有自己擅长的行业和场景。衍生版之间也会形成良性竞争关系，使社区在健康的状态下脚踏实地向前进步。</p><p>&nbsp;</p><p>面向开发者，龙蜥社区与很多高校有着密切合作，会频繁组织各领域的&nbsp;MeetUp&nbsp;活动，并同头部厂商联合发起技术培训计划，龙蜥社区内部还提供了诸如龙蜥实验室这样的基础设施环境。所有这些方面的投入，都是为了让高校、产业单位乃至个人的开发者群体能够使用龙蜥社区系统，参与系统演进过程，将这些有贡献能力的群体凝聚起来。</p><p>&nbsp;</p><p>龙蜥社区的技术委员会与运营委员会则形成了一套稳定的治理框架，对重点运营决策、产品决策、生态决策发起定期开放讨论，确保在最兼顾所有成员利益的前提下达成一致。这一治理机制中，顶层是社区理事会，中层是多个领域的委员会，分别负责社区的重大决策讨论与细分领域的技术方向讨论。由于治理框架整体高度透明，加之社区对外部引入的企业成员持高度开放态度，龙蜥社区就能够保障每一位成员的利益，尽可能建立社区成员的广泛共识。</p><p>&nbsp;</p><p>虽然阿里云是龙蜥社区的发起单位，但阿里云自身在社区&nbsp;24&nbsp;家理事单位中也只有一票投票权。这样以来，即便社区最终决策与阿里云的最初期望有所偏差或冲突，社区成员也能获得对伙伴利益最大化的决策，避免一家独大的状况产生。</p><p>&nbsp;</p><p>展望未来，龙蜥社区将始终坚持三个大方向。首先是解决国内服务器操作系统的供应链安全问题，为国内构建更加健壮的操作系统供应链，不断提升社区生态的健壮性。其次是进一步加强自主替代方向的研发和技术落地，推动自主替代进程早日完成目标。最后一点，龙蜥社区将深刻把握云智融合带来的机遇，凭借云智融合场景开发的创新技术帮助用户构建差异化竞争力。</p><p>&nbsp;</p><p>龙蜥社区与国际化接轨的动作也从未停止。一方面，社区对国内外的合作伙伴与社区成员一视同仁，不会对国外企业的加入持歧视态度；另一方面，社区成员也会积极参与国际上游社区的技术演进过程，在这些社区中争取更多话语权和影响力。</p><p>&nbsp;</p><p>目前，龙蜥社区虽然建立只有短短数年，但已经获得了广泛的用户，建立起了比较成熟的生态环境，社区开发者群体规模增速高达&nbsp;110%，社区创新力和活跃度国内首屈一指，成立近&nbsp;60&nbsp;个特别兴趣小组，拉取请求数月均&nbsp;5000&nbsp;余次。社区每两年发布一个社区版本，合作伙伴的衍生版本则服务了超过&nbsp;80&nbsp;万用户。</p><p>&nbsp;</p><p>当下，龙蜥社区已经驶上了发展的快车道，又恰逢云智融合的东风，社区前景被广泛看好。龙蜥社区的成功也为国产服务器操作系统的发展作出了表率，证明国产系统即便起步落后，也能凭借开放、公平、合作共赢的生态环境迅速追赶上来，成为全球产业的后起之秀。随着服务器操作系统进入&nbsp;2.0&nbsp;时代，我们将见证国产品牌与国际一流水平同台竞技，并为更多行业的云智融合产业升级提供坚实的平台支撑。</p><p></p><p></p><p>&nbsp;</p>",
    "publish_time": "2023-12-22 16:11:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "要卷就卷应用，百度智能云千帆大模型平台应用开发挑战赛收官！",
    "url": "https://www.infoq.cn/article/xIOVsMwUX9FbltjLb53u",
    "summary": "<p>为鼓励行业用户与开发者发挥想象力和创新精神，也为了进一步助力大模型应用落地，百度智能云千帆大模型社区于2023 年9 月发起了大模型平台应用开发挑战赛，吸引了业内关注生成式AI 技术创新和应用的企业、个人开发者等组成的数百支参赛队伍。历经报名、初筛、应用开发、专家评审等阶段，本次百度智能云千帆大模型平台应用开发挑战赛圆满收官。</p><p>&nbsp;</p><p>12 月20 日，在2023 百度云智大会·智算大会上公布了本届大赛的获奖情况，课通天下团队的《猴动力》项目荣获一等奖，市场易、城市漫步指南、格沃智能团队获得二等奖，奇融谷、亨利教育AI、海探、Touch fisher团队和王翊仰选手获得三等奖。</p><p><img src=\"https://static001.geekbang.org/infoq/f7/f79dd713960876a27f4427b065541333.png\" /></p><p></p><h2>要卷就卷应用，释放大模型真正价值</h2><p></p><p>&nbsp;</p><p>百度创始人、董事长兼首席执行官李彦宏提出，中国的大模型很多，但是基于大模型开发出来的AI原生应用却非常少。AI原生时代，我们需要100万量级的AI原生应用，但是不需要100个大模型。</p><p>&nbsp;</p><p>在本次比赛中，我们看到一批勇于为AI原生应用贡献力量的创新者。基于作品的创新性、实用性、完成度、展示度等多个维度，本届大赛评委在数百个大模型应用中遴选出了九个决赛项目。</p><p>&nbsp;</p><p>他们中有，市场易团队开发的面向B2B 企业营销文案创作工作流，借助千帆大模型为文案创作者提供AI 文案辅助创作能力的《AI 文案助手》；城市漫步指南团队基于上海市优秀历史建筑的基础数据，利用大模型能力，做出了可以智能化Citywalk 路线规划的《城语APP》；海探团队的作品《小蓝鲸》是一款面向油气勘探开发领域的文心大模型问答应用……</p><p>&nbsp;</p><p>决赛项目领域多样、方向各异，覆盖了工业、金融、教育、营销、旅游、生活娱乐等诸多场景，涉及智能文档分析、智能客服、文本生成、图片生成、方案规划等多种生成式AI 技术形态，为大模型应用开发者开拓了创新思路，释放大模型真正价值，展现了百度智能云千帆大模型平台的能力和潜力。</p><p>&nbsp;</p><p></p><h2>百度智能云助力，创新者驭风前行</h2><p></p><p>&nbsp;</p><p>本届大赛中荣获一等奖的课通天下团队一直专注于培训行业的SaaS 学习平台研发与企业内部培训需求。课通天下研发负责人徐生吉表示，百度文心一言大模型与百度智能云千帆大模型平台大大降低了在AI 领域研发的门槛和成本，像是课通天下这样的小企业也能随时调用大模型的能力，加上细分教育场景的数据集，就能做出效果很好的教育产品。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/0e/0efc2b4c1f410d847419de23a2f362f7.png\" /></p><p>课通天下研发负责人徐生吉</p><p></p><p>课通天下团队通过对培训行业的业务需求进行深度拆解，再结合公司自有的数据积累对模型进行训练，从而打造出可以为培训讲师智能辅助出题、对出题质量打分的细分领域智能应用出题通，能够结合知识库针对性地出题、判卷、给出解题思路。产品还没正式上线，已经有多家机构表达了浓厚的购买意向。接下来，他们还准备推出帮助技工院校老师制作工学一体化标准教案的“工学通”、生成知识大纲和教材的“技能通”，以及一系列的职业教育“AI工具集”。</p><p>&nbsp;</p><p>在这些工具的研发过程中，百度智能云千帆大模型平台的完整工具链、开发工作流让课通天下团队节省了大量工作，他们还参加了百度智能云千帆AI加速器第一期，获得了包括技术布道、专家辅导、千帆大模型平台专属资源等一系列支持。徐生吉表示，中小企业与创新开发者选择百度千帆平台开发垂直领域智能应用，可以很轻松地找到大量社区经验、最佳实践与疑难解析，让新手团队可以快速上手，缩短企业的开发周期。</p><p>&nbsp;</p><p>课通天下的感受也得到了其他参赛团队的共鸣，他们对百度智能云千帆开发社区的丰富内容、完善资料与千帆大模型平台的易用性留下了深刻印象。城市漫步指南团队负责人Richard是一名资深Python 算法工程师，他认为百度智能云千帆大模型平台经过深度打磨，非常符合程序员的使用习惯，其完整的工具链可以有效提升AI 应用开发效率。三等奖作品《动漫助手》，甚至是一名在校大学生用两天时间通过百度智能云千帆大模型平台开发出来的。</p><p>&nbsp;</p><p>在近期2023百度智算大会，百度智能云公布了最新“成绩单” ，自8月31日文心大模型向全社会全面开放以来，在千帆大模型平台上，大模型API日调用量增长10倍。目前千帆平台已经累计服务超过4万家企业用户，累计帮助企业用户精调近1万个大模型。与此同时，百度智能云积极深耕技术与生态建设，打造国内第一个大模型全链路生态支持体系，贴身围绕为处于不同成长阶段的创新企业和开发者，提供AI加速器、AI原生应用商店等支持，并通过应用开发挑战赛、黑客马拉松等形式吸引更多创业者、开发者迈入AI 原生应用开发领域。百度智能云将持续推动大模型在各行业全面普及，赋能企业与开发者不断创新、驭风前行。</p><p></p><p></p><blockquote>百度智能云千帆大模型应用开发挑战赛获奖名单一等奖：课通天下·《猴动力》二等奖：市场易·《AI文案助手》、城市漫步指南·《城语APP》，格沃智能·《Wow数字助手》三等奖：奇融谷·《小奇智询》、亨利教育AI·《财报检析》、海探·《小蓝鲸》、Touch fisher·《动漫助手》、王翊仰·《反诈小助手》&nbsp;</blockquote><p></p><p></p>",
    "publish_time": "2023-12-22 17:02:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "智能演进：个性化广告精准投放，机器学习与广告推荐的深度融合",
    "url": "https://www.infoq.cn/article/hLB1UD2FZ4YJaFLzAYQa",
    "summary": "<p>在这个智能化的时代，大数据越来越能准确了解每个人的喜好。或许你也曾有这样的经历：刚想买一件入冬御寒的衣服，打开购物APP首页就推送了“卫衣”“羽绒服”，准确地推荐出你喜欢的风格，仿佛是一位贴心的时尚顾问；刚跟朋友感慨好久没吃火锅了，随后就能收到“本地火锅推荐”“火锅必吃榜”等新闻推送。这并非巧合，而是个性化广告推荐通过深度学习和智能算法所带来的精准广告体验。</p><p></p><p>然而，当我们意识到自己已深陷于大数据的牢笼，试图关闭所有平台的个性化推荐时，却不得不承认在某些时候，个性化广告似乎比我们自己还更了解我们的需求。这是因为在广告投放方面，机器学习已逐渐取代传统方式——依赖从业者经验和直觉的方式来判断广告内容，渠道选择及受众定位。而是凭借其强大的数据处理和分析能力，为行业注入新的活力，从海量数据中提取潜在的关联和规律，为广告主和营销人员提供了创意生成、推荐优化和出价策略优化等全新可能性。</p><p></p><p>这种技术驱动的变革不仅仅是为了提高广告投放的效果，更是对行业未来发展方向的准确把握。在这个充满挑战和机遇的时代，技术驱动的广告营销成为企业增长的重要推动力，而机器学习则是引领这一浪潮的领军者。</p><p></p><h2>效果为王：挖掘个性化广告的长效价值</h2><p></p><p></p><p>在全球经济增长放缓的市场环境中，广告主的投放策略也随之发生了变化。不论是品牌广告还是效果广告，广告主都更加注重广告投放的实际效果，而不仅仅追求广告的覆盖范围。ROI 成为决策性指标，让每份广告预算都能实现最大化效益。</p><p></p><p>对于品牌广告，广告主在经济下行时更注重活动的长期价值。不再仅仅扩大品牌知名度，而是专注于建立深层次的品牌关系和提升品牌价值。通过巧妙设计的广告创意和更精准的受众定位，使得品牌广告在有效的广告预算内取得更显著的品牌效益, 从而实现更加可持续的发展。</p><p></p><p>效果广告方面，广告主在激烈竞争中更加关注实际转化和业务价值，点击率和曝光量不再是唯一关注点。实际效果和业务价值成为广告主的首要考量，确保广告为业务创造真实价值。</p><p></p><p>在竞争激烈、预算有限的市场环境中，广告主认识到采用更个性化、精准的广告内容是吸引目标受众的关键。通过调整策略，品牌广告和效果广告都变得更加精准和可衡量，让每一项广告投放都更有针对性，更紧密地与实际业务目标相契合。这种策略不仅提高了广告的针对性，也使得广告主能够更有效地在有限的预算下实现更高的 ROI。</p><p></p><p>个性化一直是营销的关键要素，而在当前技术不断进步的背景下，机器学习正引领个性化广告走向新的高度。借助机器学习，广告主将创造超个性化的体验，通过实时数据分析、智能化投放决策、预测性分析和个性化推荐系统等技术，不仅提高了广告效果，以适应市场变化，更为广告主带来更灵活、更适应市场的策略和更高转化率。</p><p></p><p>实时数据分析与优化：机器学习通过对实时数据的深入分析，使广告主能够在广告活动进行中实时了解受众反馈和效果。这种实时的数据分析能力为广告主提供了更及时、精准的优化方案，确保广告活动的灵活性和高效性。预测性分析优化：机器学习通过对历史广告数据的深度学习，具备了预测广告效果的能力。广告主可以依据这些预测性分析，更有针对性地调整广告策略，以提高广告的实际效果和用户互动率。智能化投放决策：在效果广告的领域，机器学习不仅仅是数据的分析工具，更是智能化投放决策的关键。通过对大量数据的学习和分析，机器学习系统能够自动调整广告投放策略，以最大程度地提高广告效果，降低成本。个性化推荐系统：&nbsp;借助机器学习的算法，广告主能够建立更为高效的个性化推荐系统。通过深入挖掘用户行为数据，系统能够准确预测用户兴趣，从而为用户呈现更符合其期望的广告内容，提高用户互动和转化率。</p><p></p><p></p><blockquote>然而，随着信息爆炸时代的到来，个性化广告推荐也面临着一系列挑战，其中之一是广告点击率（CTR）、转化率（CVR）的预测问题。在这一问题中，数据的庞大、稀疏和异常形成了前所未有的难题。让我们深入探讨机器学习如何应对广告点击率和转化率预测中的挑战。</blockquote><p></p><p></p><p>在讨论广告推荐时，自然涉及到广告 CTR（点击率）、CVR（转化率）问题。而在广告领域中，CTR和CVR的预测问题被认为是其中最为重要的课题之一，同时却也面临着诸如数据量庞大、数据稀疏、数据异常等难题。&nbsp;</p><p></p><p>尽管存在这些挑战，但广告市场中越来越多的公司已经认识到，借助机器学习能够更好地理解消费者行为、实时调整广告策略，以及更有效地管理广告预算。这一趋势的兴起不仅仅体现了机器学习在广告市场中的实际效果，更反映了整个行业对于这一技术的广泛认可和接受。</p><p></p><h2>智能广告引擎：机器学习加速提效广告推荐</h2><p></p><p></p><p>随着数据量的急剧增加和计算能力的提升，机器学习能够处理和分析大规模的广告数据，为广告主提供更智能、精准的广告投放方案。许多广告科技公司和数字营销平台积极采用机器学习算法，以优化广告创意、提高广告推荐的个性化程度，从而提升整体广告ROI。这种趋势推动了广告行业不断迈向更智能、创新的方向，使得机器学习在广告推荐领域发挥着日益重要的作用。</p><p></p><p>作为广告行业的参与者，汇量科技致力于将对技术的投入转化为实际的广告投放效果。通过运用先进的机器学习技术，我们看到了广告推荐领域的一系列令人振奋的进展。本篇内容，我们会深入探讨广告投放第二步【广告推荐】，看看机器学习如何在这一关键环节发挥作用：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7f62367611952a8a1ffa0400ff442d8a.png\" /></p><p></p><p>随着数字广告行业的迅速发展，程序化广告成为了推动广告交易效率的引擎。通过自动化购买和实时竞价等技术，程序化广告赋予了广告主更精准、实时的广告投放能力。在此背景下，机器学习的引入为程序化广告注入了更为智能的元素。汇量科技旗下程序化广告平台 Mintegral&nbsp;充分利用机器学习的强大算法，通过对用户行为的深度学习和行为模型预测，实现了广告推荐的个性化和智能化。这不仅提高了广告投放的效果，还为广告主创造了更具吸引力的广告体验。</p><p></p><p>广告推荐旨在将广告内容与潜在受众相匹配，其目标是确保广告能够出现在最相关和有影响力的场景中；其中，机器学习以可扩展目标受众和成效预估入手，全力提升广告效果：</p><p></p><p>受众定向：</p><p></p><p>精准预估用户兴趣，支持手动或自动地细分用户，并按需缩放人群规模，实现在全球范围内定向目标人群。</p><p></p><p>通过数据分析及机器学习算法，系统可以深度分析用户行为，准确预测用户兴趣和偏好，除了作为特征应用于下游预估建模，还可以用来支持广告的“白盒化”定向，以便更精准的定位目标模型。广告主可手动或自动细分用户，保持投放的个性化和定制性，同时在不同时间和地域按需缩放人群规模，最大化广告效果。</p><p></p><p>机器学习技术的全球应用使得受众定向能够在全球范围内实现高效目标定向，为广告主提供高度智能化和个性化的投放解决方案，提升广告效果并增加广告投放的精准性。</p><p></p><p>行为模型预测：</p><p></p><p>建立全面的用户互动行为预测模型，覆盖广告生命周期的各个环节，旨在通过最有效的成本来最大程度地提高广告与用户之间的匹配效率。</p><p></p><p>这包括在广告前链路，采用深度学习预测模型对广告点击率（CTR）、转化率（CVR）、每千次展示安装量（IPM）等关键指标进行精准预测；同时在广告后链路，通过用户参与率（EGR）和生命周期价值（LTV）预测模型，进一步提升广告活动的效果评估。这一全方位的行为模型预测系统有助于在最有效的成本下优化广告投放，实现更精准的广告与用户匹配。</p><p></p><h2>未来广告新篇章：机器学习赋能下的技术探索</h2><p></p><p></p><p>在广告投放过程中，机器学习技术如何学习并适应受众行为的变化是一个至关重要的问题。汇量科技旗下程序化广告平台 Mintegral 通过先进的DMP系统将用户的长周期和短周期行为数据转化为用户特征，以捕捉用户在不同时效性下的偏好。在广告响应阶段，运用上下文信息，准确描绘用户在不同场景和时间上的行为倾向。</p><p></p><p>该系统具备精准的人群定向能力，不仅限于定向DMP产生的用户偏好，还支持客户上传的数据，并能够应用基于相似特征的人群扩展功能，进一步提升广告触达效果。</p><p></p><p>在前链路特征模型的基础上，我们不断研发完善，构建了一套全新的用户深层行为特征模型。这套模型通过精准抽取与广告转化收益最为关联的行为特征，为广告主和平台带来了显著的双赢局面。具体而言，我们通过深度建模用户在应用内的行为，为 ROAS（投放回报率）提供了有力的支持，使客户广告回收和投放量都实现了明显的增长。</p><p></p><p>以某位超休闲游戏广告主为例，在其广告活动应用的机器学习能力升级后，获客表现稳定的同时，广告主逐步提升了投放预算。令人振奋的是，广告下载量在短期内实现了超过 60% 的增长，同时 ROAS 表现保持平稳，为该广告主实现了高效的高量级、高回收增长。这也印证了我们先进的行为模型如何覆盖广告生命周期的各个环节，助力广告主在竞争激烈的市场中脱颖而出。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/45e7410f68a3733906fb0eb34736044d.png\" /></p><p></p><p>同时，在广告投放这一过程中，日益完善的机器学习平台也会充分考虑用户的隐私安全。通过对受众特征的匿名化处理、确保系统获取和处理的数据不涉及敏感信息、以及在系统内实施严格的数据访问控制，致力于确保用户的隐私安全。</p><p></p><p>得益于机器学习的不断发展，个性化广告已经不再是简单的趋势，而是成为数字广告领域的主导力量。这也使得广告推荐能够更深入地了解用户需求、习惯和喜好，为品牌和用户之间建立更紧密的连接。通过对海量数据的智能分析，机器学习不仅提高了广告投放效果，还为广告主提供了更丰富的广告策略选择。</p><p></p><p>未来，随着机器学习技术的不断演进和创新，个性化广告推荐将更加智能、精准，使广告投放成为品牌塑造和用户体验提升的关键工具。</p>",
    "publish_time": "2023-12-22 17:27:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI 技术如何激发企业研发的创新潜能？｜专访 Atlassian 大中华区负责人",
    "url": "https://www.infoq.cn/article/TpNQKuBWUbxScsQ4Sfru",
    "summary": "<p>什么部门才是企业的核心竞争力来源？这个问题放在十年、二十年前，很多企业老板会脱口而出：“销售”，但经过十余年的激烈行业竞争洗礼，大浪淘沙沉淀下来的幸存者更有可能给出“研发”这个答案。</p><p></p><p>在最近一次对 Atlassian 大中华区负责人 Kerwin Chung 的采访中，InfoQ 与 Kerwin 深入探讨了研发效能在生成式 AI 蓬勃发展的时代愈加重要的地位。随着公司寻求在复杂的研发领域找到出路，Atlassian 推出了其最新创新产品——Atlassian Intelligence，这是一套以生成式 AI 为动力的套件，旨在提升研发效能并简化价值流程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a5af7c79149312d198d6f79155ea4bbe.png\" /></p><p></p><p></p><h2>研发效能：科技企业乃至所有企业的核心竞争力来源</h2><p></p><p></p><p>软件正在重塑全球格局，使得所有企业逐渐转型为软件和信息技术公司。每一个行业现今以及未来都需借助应用程序来建立与客户的联系并推动业务发展。因此，负责应用程序研发和运维的 IT 部门已经成为，或者即将晋升为企业中不可或缺的核心角色。</p><p></p><p>如果将企业研发生产过程比作一条河流，那么其中流淌的便是持续增长的价值。根据价值流管理的观点，当一项产品或服务的原型在各个团队或部门之间传递时，它会在每个环节中获得增值，最终流经所有部门后完成价值创造。优化这个过程，即疏通这条河流，就是提高研发效能的关键。像任何河流一样，这里也会有弯道、浅滩、暗礁和狭窄河道，它们都会降低价值流的流速。移除这些障碍，就能提升效能、降低成本。当企业成功清除那些阻碍价值流的主要瓶颈时，研发部门就能顺畅地输出符合市场需求和企业战略的产品成果。</p><p></p><p>然而，如何精准识别和消除这些价值流中的障碍呢？许多企业因为缺乏专业的框架、团队和工具组合而感到力不从心。而这也是 Atlassian 这样的企业应用服务公司能发挥所长的地方。专业的问题需要交给专业的人来解决，Atlassian 正是这样一家擅长为企业价值河流进行疏浚和拓宽的服务商。</p><p></p><p>现在，Atlassian 发布的基于生成式 AI 技术的全新一代企业服务产品——Atlassian Intelligence，为企业开辟了一条借助 AI 技术提升研发效能、疏通价值流的新途径。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2b/2be824b8e059596ab76adf4d39368921.png\" /></p><p></p><p></p><h2>生成式 AI 是否能成为企业降本增效的银弹？</h2><p></p><p></p><p>成立超过 20 年的 Atlassian，今天已经服务了超过 80% 的全球五百强企业，用户总量达 23 万。Atlassian 的 Jira、Confluence 等产品系列，可以落地到企业价值流创造的各个环节、不同部门和团队中，无缝协助这些部门实现互联互通，为企业带来非常直观的改进提升效果。</p><p></p><p>Atlassian 在统一的许可证下提供了大量开箱即用的模板，业务部门、后勤部门、行政部门都可以轻松使用这些模板创建新的、效率更高的工作流，而每个部门之间的工作流又能够轻松对接。当不同部门都在使用 Atlassian 的工具提升自己的效能，部门之间就会自然而然形成一种一致性。例如，开发和测试部门都使用 Jira 来管理软件项目，管理层、财务部门也能使用 &nbsp;Jira 随时了解开发和测试团队的进度与需求，提供必要的支持。通过 Atlassian 的工具组合，不同部门很容易对齐目标、协调规划，并将其他部门的资源约束和挑战纳入自己的考量范围，使整个企业实现真正的协作氛围。如果说一般的企业服务软件所做的事情是在价值流河道中东挖一坑，西掘一洞，各个部门只顾着清除眼前的障碍，不管疏浚工作对上下游的影响，导致整体流程变得更糟，那么 Atlassian 所做的事情就是从宏观全局出发来安排整条河道的工程，尽可能提升企业整体的效率。</p><p></p><p></p><h2>Atlassian Intelligence：用生成式 AI 改变研发</h2><p></p><p></p><p>Atlassian Intelligence 延续了 Atlassian 一贯的服务理念，将生成式 AI 技术融合到了现在的增效框架中。如今，人们对于生成式 AI 的能力边界已经有了比较成熟的认知，问题在于这些能力究竟如何融入企业流程，提升价值河流的流速？</p><p></p><p>Atlassian 的答案是从每一位企业员工做起，提升员工个人生产力。在 Atlassian Intelligence 的帮助下，团队只需使用自然语言向软件提出问题即可自动化日常任务，快速总结长篇内容，获取项目背景信息，或者构建复杂查询，从企业的数据资源中获取深度洞察。这款服务还集成到了 &nbsp;Jira 和 Confluence 等已有产品中，让用户可以在 Jira Software 的工单中即时生成用户故事，在 Jira Service Management 中将回复客服的答案调整为更具同理心的版本，或者在 Confluence 中为测试计划制定起始点，等等。与此同时， Jira Service Management 的虚拟助手为每一位员工带来了人性化的工作辅助能力，让他们从琐碎的任务中抽身出来。此外，Atlassian Intelligence 也具备代码辅助生成、图表绘制等功能，帮助程序员在日常工作中提升效率。</p><p></p><p>Atlassian Intelligence 总是站在企业全局视角，它给出的总结、优化、建议、洞察都是来自企业各个部门的数据汇总，是与企业价值观而非单个团队或部门的利益取向一致的。那么，当员工习惯了 Atlassian Intelligence 的便利性，他们也就潜移默化地将企业价值观融入了工作的每一个环节。</p><p></p><p>另一方面，生成式 AI 技术让企业员工和团队的自主能力边界有了明显扩展。技术团队能够更多获取行政、后勤部门的知识，业务部门也能自行解决很多原本需要技术支持的任务。由此以来，各个部门用于支持其他部门低级重复性工作的资源投入就可以收缩，从而集中到更具价值的事务中。通过这样的改变，公司团队也能变得更加敏捷。</p><p></p><p>如果说原有的 Atlassian 产品矩阵是为价值河流的疏浚带来了整体的方案和工具组合，那么现在的 Atlassian Intelligence 就是基于生成式 AI 技术将所有工具打磨得更加锋利好用，为每一位员工送上了更加全面的装备套件。在全新 AI 能力的支持下，企业团队就能做到更加灵活、能力更全面、协作更顺畅，自然而然实现降本增效目标。或许并不是所有的生成式 AI 产品都是企业提升竞争力的银弹，但 Atlassian 正在努力向这一目标靠近。</p><p></p><p></p><h2>克服生成式 AI 实施的挑战</h2><p></p><p></p><p>一项创新技术，不管前景多么诱人，承诺多么可靠，要真正为企业带来实际收益的前提是技术能够切实落地。Atlassian 是一家擅长落地的公司，其很重要的理念就是平台化。Atlassian 不追求内卷，不会对竞争对手恶意伤害，而是会充分利用可用的资源为用户带来更好的效果。</p><p></p><p>具体到生成式 AI 这个领域，Atlassian 首先非常重视 AI 应用涉及的企业隐私数据安全问题。Atlassian 引入了 OpenAI 的业内最高等级的安全标准，可以在整个软件链条上确保企业数据不会因为使用了生成式 AI 而被泄露、恶意利用。</p><p></p><p></p><h3>Atlassian 非常重视中国市场和中国客户</h3><p></p><p></p><p>Atlassian 对产品和服务的本地化也非常用心，其目标是让创新技术真正为各区域用户创造价值的关键一环，只有为当地用户带来符合国情和市场、行业地域化特点需求的优化和改进，才能打破创新“水土不服”的僵局。</p><p></p><p>在中国市场，Atlassian 从多个维度下手，将本地化工作推进到了相当高的水平。2024 年 2 月，Atlassian Server 产品就会终止服务，但 Atlassian 为中国客户提供了平滑的升级路线，不同规模的 Server 客户都可以顺利升级到本地部署的 Data Center 版本，或者 Atlassian 在中国的合作云厂商代托管的云平台上，带来更高的可用性和可扩展性。这一升级也为 Atlassian Intelligence 的落地应用打下了更好的基础，使企业在应用 AI 的各项能力时不至于遭遇基础设施瓶颈。</p><p></p><p>特别针对中国市场，Atlassian Data Center 本地化部署版本的产品，如 Jira 和 Confluence，能够适用于所有团队规模，使中国市场上的大小企业团队都能获得符合自身规模的本地部署产品。</p><p></p><p>产品的多语种支持是本地化工作的重中之重，Atlassian 在这方面也做了大量投入来为中国用户营造最高水平的使用体验。一方面，Atlassian 的热门产品均有翻译质量出色的中文版本，用户无需担心语言障碍造成学习门槛或带来操作不便、理解偏差等问题。另一方面，Atlassian 引入了专业的合作伙伴支持团队为中国用户提供本地化服务支持，使中国用户能够像欧美用户一样获得高质量的母语服务，并在出现问题疑难时得到高级别的及时响应。</p><p></p><p>另外，对于中国客户来说，出于安全与合规的要求，Atlassian 用户可能无法使用境外的云服务，对此 Atlassian 也与国内合作伙伴以及国内云厂商合作，推出代托管的模式，通过安迈无限，荣尧泰，范德敏特等 Atlassian 认证的解决方案合作伙伴，可以帮助企业将 Atlassian 产品部署在中国的云服务平台上，依旧保持对数据隐私安全的信心。</p><p></p><p></p><h3>丰富的插件及生态圈</h3><p></p><p></p><p>Atlassian 的平台化理念还体现在丰富、活跃的生态圈上。在 Atlassian 的 Marketplace 上有超过 5000 家商户在提供多样化的工具和插件，满足各行业企业在细分领域和场景的不同需求。这一生态圈对生成式 AI 的落地意义重大，它意味着将会有数千家合作伙伴开始研究如何使用全新的 Atlassian Intelligence 服务来更好地应对各类企业挑战，这样当用户在实践中落地 AI 能力时，很多场景中可能已经有 Marketplace 厂商做好了全套流程，免去了用户再摸索和打磨的麻烦。另一方面，Atlassian 自身也在随时收集不同行业、领域的客户反馈和实践经验，将它们汇集为可以复用的方法论和最佳实践库供用户使用。当 Atlassian Intelligence 开始在全行业推广后，这一知识库也会很快更新，帮助用户抚平新技术落地的学习曲线。</p><p></p><p>Atlassian 的另一大优势在于，公司为客户提供了非常弹性的空间来落地各种工具。企业用户不需要局限在特定的框架中，而是根据自己的业务特点和需求，将 Atlassian 的工具组合为更加合适的体系。这一理念对新的生成式 AI 工具同样适用，也就是说企业并不需要一夜之间在所有流程中引入 Atlassian Intelligence，而是根据自己的习惯和痛点，在最需要的领域先部署一部分功能，之后再逐渐推广到所有流程中。这样的设计也大大降低了生成式 AI 落地企业的难度，使员工可以无负担地享受最新技术带来的便利。</p><p></p><p></p><h2>AI 驱动未来</h2><p></p><p></p><p>如今，Atlassian 正在通过最新的生成式 AI 创新，为企业研发和全流程的效能提升带来更加强大的工具组合。Atlassian 并没有给企业描绘非常壮丽的图景，但这家公司相信自己在这个领域的努力可以切实改变员工和团队现有的工作方式，甚至组织内的协作氛围。随着 Atlassian 遍布全球的客户开始采用 Atlassian Intelligence，我们也将看到生成式 AI 究竟能带来多大的效能进步，为企业竞争力提升创造持续动力——至少在目前，我们在这个问题上可以保持非常乐观的预期。</p><p></p><p>如果你对 Atlassian 提供的相关服务感兴趣，欢迎咨询官方合作伙伴：</p><p></p><p>安迈无限：https://www.unlimax.com/contact/</p><p></p><p>荣尧泰：https://www.igsl-group.com.cn/contact-us/</p><p></p><p>范德敏特：https://www.devpod.cn/contact/</p><p></p>",
    "publish_time": "2023-12-22 18:45:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "作业帮基于 DolphinScheduler 的数据开发平台实践",
    "url": "https://www.infoq.cn/article/gh2xDWqIlAY9CSIxuNSs",
    "summary": "<p></p><p></p><p></p><h2>摘要</h2><p></p><p></p><p>随着任务数量、任务类型需求不断增长，对我们的数据开发平台提出了更高的要求。本文主要分享我们将调度引擎升级到&nbsp;Apache DolphinScheduler 的实践经验，以及对数据开发平台的一些思考。</p><p></p><h2>1. 背景</h2><p></p><p>首先介绍下我们的大数据平台架构：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e9/6f/e9f36c5eb6c863852d6ae1653b10816f.png\" /></p><p></p><p>数据计算层承接了全公司的数据开发需求，负责运行各类指标计算任务。</p><p>其中批计算任务运行在&nbsp;UDA 数据开发平台，支持任务全链路的开发场景：开发、调试、环境隔离、运维、监控。这些功能的支持、任务的稳定运行，强依赖底层的调度系统。</p><p>&nbsp;</p><p>原有调度系统是&nbsp;2015 年(抑或更早)自研的，随着任务类型新增、任务数量增多，暴露出诸多问题：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f4/04/f45e6f02184c3c85de6ddd2d64fa6404.png\" /></p><p></p><p></p><p>1.&nbsp;稳定性：频繁出现&nbsp;mysql 连接不释放、锁超时等问题；数据库压力进一步导致调度性能瓶颈，任务无法及时调度。</p><p>2.&nbsp;可维护性：核心调度器通过&nbsp;php 开发，代码古老又经历多次交接，外围模块实现时采用了&nbsp;go java python 多种语言；再加上功能上也存在单点，维护成本很高。</p><p>3.&nbsp;扩展性：业务高速发展，不同任务类型需求越来越多，但是调度作为底层服务在支撑上一直力不从心。</p><p>4.&nbsp;可观测性：由于是定时&nbsp;nohup 启动任务进程的方式，经常出现任务跑飞了的情况，系统暴露出来的可观测指标几乎为0。</p><p>&nbsp;</p><p>对调度系统的核心诉求，我觉得分为功能和系统两部分：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/18/e0/18ec089e7f0f070051b7021f5e64cae0.png\" /></p><p></p><p></p><p>功能上，调度系统的核心能力是解决数仓构建的依赖调度问题，因此需要支持多种依赖形式；支持丰富的任务类型，同时可扩展自定义新的任务类型。以及上线管控、历史版本回滚、任务血缘等提高易用性的能力。</p><p></p><p>系统上，稳定性是第一位的，因此需要具备高可用的能力。同时支持租户隔离、线性扩展、可观测，以方便的对系统进行开发、维护和预警。</p><p>&nbsp;</p><p>历史上我们调研过&nbsp;Airflow、DolphinScheduler 等多种选型，在过去大概一年的时间里，我们将大部分任务从自研调度系统迁移到了&nbsp;DolphinScheduler 上。</p><p></p><p>当前调度系统概况如下：</p><p>1.&nbsp;任务类型上：HiveSQL、SparkSQL、DorisSQL、PrestoSQL、部分&nbsp;shell 任务，均通过&nbsp;DolphinScheduler 调度；遗留部分&nbsp;shell 任务在原调度系统。</p><p>2.&nbsp;任务数量上：DolphinScheduler 天级别调度数万工作流实例，数十万任务实例，高峰时期同时运行&nbsp;4K+ 工作流实例。迁移完成后，预计工作流实例实例数翻倍。</p><p></p><h2>2. 数据开发平台实践</h2><p></p><p></p><h3>2.1. 基于&nbsp;DolphinScheduler 的改造</h3><p></p><p>对&nbsp;DolphinScheduler 的改造围绕稳定性和易用性展开，对于原有调度系统设计良好的功能，需要兼容以降低任务迁移成本。</p><p></p><p>我们基于&nbsp;DolphinScheduler 做了如下升级：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2d/f2/2d3f82278dcbf9dd2a5afdb1cdbc2ef2.png\" /></p><p></p><p></p><p>由于&nbsp;DolphinScheduler 的架构设计比较好，优化基本上可以围绕单点或者复用现有能力展开，而无需对架构进行大刀阔斧的改造。</p><p>&nbsp;</p><p>我们的&nbsp;SQL 任务都是多个&nbsp;SQL 组成，但是原生的&nbsp;SQL 任务只能提交单个。为了确保系统简洁，我没有引入各类&nbsp;client(hive-client、spark-client等)，而是通过&nbsp;SQL 解析、连接池管理方式重构等方式，通过&nbsp;JDBC 协议支持了单任务多&nbsp;SQL 的提交。</p><p>&nbsp;</p><p>同时充分复用了&nbsp;DolphinScheduler 对于数据源的设计，赋予数据源更多的属性，比如连接不同的&nbsp;HiveServer2、Kyubbi、Presto Coordinator 等，对于计算运行在&nbsp;Yarn 上的任务，单个数据源也只允许使用单个队列。对数据源增加权限控制，这样不同任务就只能使用有权限的集群资源。</p><p>&nbsp;</p><p>我们将资源文件、DQL运行的结果数据，都统一上传到了腾讯云的&nbsp;COS 对象存储，以确保做到&nbsp;Worker 真正的无状态。(注：日志上传进行中)</p><p>&nbsp;</p><p>此外包括对负载均衡进行优化、多业务线的租户调度隔离、数据库使用优化等。</p><p></p><p></p><h3>2.2. 平滑的大规模迁移</h3><p></p><p>尽管两个调度系统，在功能以及架构上存在巨大差异，但是需要做到平滑的迁移，主要三个原因：</p><p>1.&nbsp;原有调度系统服务多年，用户对于功能设计、系统专有字段名词等都已经养成习惯</p><p>2.&nbsp;2W+工作流的迁移预计耗时较久，涵盖公司众多重要数据流，问题影响程度高</p><p>3.&nbsp;用户覆盖了公司众多业务线(平台、直播课、硬件、图书)，问题影响面广</p><p></p><p>如此大规模的迁移我们做到了对用户几乎无感知，主要依赖新旧调度系统的打通和DIFF。</p><p></p><p>接下来介绍下具体是怎么做的。</p><p></p><p></p><h4>2.2.1. 新旧调度系统打通</h4><p></p><p></p><p>任务迁移阶段，一部分任务运行在新的调度系统上，一部分运行在原有调度系统上，就需要解决两个问题：</p><p>1.&nbsp;用户能够查看所有任务实例的运行情况，包括一些内部已经习惯的调度名词(run_index、result_ftp、log_ftp、csv_result_path等)，这部分信息在&nbsp;DolphinScheduler 调度里显然没有</p><p>2.&nbsp;任务和任务之间有依赖关系，两个系统间调度任务时，也需要查询对方系统调度的任务实例状态，用于判断当前任务依赖是否就绪。</p><p>&nbsp;</p><p>因此，我们在迁移阶段，架构是这样：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/32/16/32bbc5299f44ab4cc69dbef5f9ea2c16.png\" /></p><p></p><p></p><p>核心设计有两处。</p><p></p><p>首先任务实例状态统一到原调度系统数据库，对平台而言：</p><p>1.&nbsp;查询方式、字段、API跟之前一致</p><p>2.&nbsp;任务更新时，如果该任务已经迁移到了新调度系统，则同时更新&nbsp;DolphinScheduler 里的工作流定义</p><p>因此平台在使用上，对用户没有感知。</p><p>&nbsp;</p><p>其次我们修改了&nbsp;DolphinScheduler DependentTaskProcessor 的代码，支持查询&nbsp;DolphinScheduler 及原有调度系统的任务实例状态。这样&nbsp;DolphinScheduler 调度的任务，就可以自由依赖两个调度系统的任务实例了。</p><p></p><p>因此在调度能力上，也做到了对用户没有感知。</p><p>&nbsp;</p><p>上述架构，未来在迁移完成后，就可以仅通过&nbsp;UDA-API + DolphinScheduler 提供完整的调度能力了。</p><p>&nbsp;</p><p>同时，我们在配置依赖的易用性上也做了优化，历史上支持了多种依赖方式：文件依赖、任务依赖、hql依赖、prestosql依赖等。后两者都需要用户手动配置查询对应表，我们都优化为了表依赖。平台解析用户的sql，针对读取的表，自动添加对应的依赖。既提高了易用性，也对用户屏蔽了底层具体表存储类型(Hive/Presto/Iceberg/...)的细节：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0c/e0/0cfd8ba65c2f8c2582bde94a5f74bde0.png\" /></p><p></p><p></p><p>&nbsp;</p><p>对任务依赖，也支持了全局搜索、偏移量、偏移单位以进一步提高易用性。</p><p>&nbsp;</p><p></p><h4>2.2.2. 新旧调度系统DIFF</h4><p></p><p></p><p>其次是新旧调度系统的&nbsp;DIFF.</p><p></p><p>作为基础平台，服务的业务线众多，再加上&nbsp;YARN 资源极其紧张，因此我们对调度系统的稳定性要求很高。为了确保迁移顺利，专门基于&nbsp;DolphinScheduler DryRun 的能力做了一版定制：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/dc/08/dc96956863e87a8b6f6f98f282171a08.png\" /></p><p></p><p></p><p>所谓镜像任务，是指我们在迁移新调度之前，会先在&nbsp;DolphinScheduler 镜像一份完全相同的任务，任务同样经过变量替换等操作，只是该任务标记了不真正执行。</p><p>&nbsp;</p><p>这样我们就可以比较两个系统间的&nbsp;DIFF，主要包括：</p><p>·&nbsp;调度时间是否基本一致：用于验证依赖配置、定时设置等的兼容性</p><p>·&nbsp;SQL是否完全一致：验证变量替换、SQL屏蔽、队列配置后，真正提交的&nbsp;SQL 是否完全相同</p><p>经过上述空跑观察一段时间，确保无&nbsp;diff 后，线上任务就真正迁移到新的调度引擎上了。</p><p>&nbsp;</p><p></p><h4>2.2.3. 系统的可观测性</h4><p></p><p></p><p>在有限的时间里，我们做了上述准备，但是仍然不够充分。</p><p></p><p>系统需要具备良好的可观测性，DolphinScheduler 对外提供了&nbsp;Prometheus 格式的基础指标。我们增加了一些高优指标，同时转化为&nbsp;Falcon 格式对接到公司内部的监控系统。</p><p></p><p>通过监控大盘来查看调度系统的健康状况，并针对不同级别的指标和阈值，配置电话/钉钉报警：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/41/d0/41bc138fb745b270cbdd2b07fefe55d0.png\" /></p><p></p><p></p><p>&nbsp;</p><p>可观测性提高后，分析问题的人力成本也得到控制，例如对于这种曲线:</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/5f/1d/5f1b0e38534aaf8514ca571205309c1d.png\" /></p><p></p><p></p><p>&nbsp;</p><p>容易观察到在非工作时间曲线值基本为0，因此就能判断指标异常(=1)很可能是用户修改后触发的，相比之前出现问题只能靠猜和逐台机器登录分析日志的方式，通过&nbsp;metrics 分析能够更早发现和预警问题。</p><p>在迁移启动后，对于&nbsp;misfire、worker线程池饱和度、连接池饱和度、io-util、overload 等指标，都重点关注和评估，以确保迁移顺利。</p><p></p><p></p><h3>2.3. 迁移收益</h3><p></p><p></p><p>目前迁移已经进行了一大半，我们针对新旧调度系统的数据库以及调度机资源使用做了对比：</p><p>1.&nbsp;数据库：</p><p>a.&nbsp;QPS: 10000+ -&gt; 500</p><p>b.&nbsp;负载：4.0 -&gt; 1.0</p><p>2.&nbsp;资源使用降低&nbsp;65%</p><p></p><p>我们在迁移过程中，通过&nbsp;DolphinScheduler 以极低的开发成本支持了&nbsp;SparkSQL、DorisSQL，以及高版本&nbsp;PrestoSQL 这类业务新的调度需求。</p><p></p><p>功能上的其他对比：</p><p></p><p></p><h2>3. 未来规划</h2><p></p><p>1.&nbsp;例行任务、调试能力全部迁移&nbsp;DolphinScheduler，沉淀线上操作&nbsp;SOP</p><p>2.&nbsp;结合社区的容器化进度，实现模块&nbsp;K8S 部署。当前&nbsp;API 模块已经在生产环境使用，Worker、Master 进行中</p><p>3.&nbsp;全链路的一键数据回溯能力</p><p>4.&nbsp;离线、实时平台打通</p>",
    "publish_time": "2023-12-22 19:05:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]