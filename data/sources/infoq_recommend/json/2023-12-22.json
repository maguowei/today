[
  {
    "title": "DoorDash 的移动应用发布流程",
    "url": "https://www.infoq.cn/article/UiWCX8hE6wtOgx258xER",
    "summary": "<p>DoorDash 公司的<a href=\"https://doordash.engineering/2023/11/28/how-doordash-manages-mobile-releases/\">移动应用发布流程</a>\"基于团队间明确的分工职责、有效的沟通、测试以及严格的回归问题处理和紧急修复规则。DoorDash 工程师 Manolo Sañudo 解释说，尽管并非所有的企业都具备 DoorDash 这样庞大的规模，但他们的解决方案的许多方面对规模较小的企业也有所帮助。</p><p></p><p>DoorDash 遵循的是相对简单的周发布周期。每个新的发布候选版本都会有一个发布分支，经过为期一周的测试和修复过程，最终正式发布。</p><p></p><p>每个新的发布候选版本都会分配一个发布经理来监督整个过程，确保一切顺利进行。发布经理的人员池要足够大，不会出现有人被工作量所拖累的情况，但也不至于过大，以至于无法跨各个发布版本做出一致的决策，或者危及发布流程的发展和改进。</p><p></p><p>每个发布候选版本都有自己的 Slack 频道，便于将状态更新和会话集中到一个地方，防止生产环境的漏洞热修复产生噪音。</p><p></p><p>对于测试，Sañudo 表示，由于无法在一周内进行完全的回归测试，因此“组件所有者”会单独负责测试所有组件，并使用移动发布管理平台 Runway 来跟踪测试状态。</p><p></p><p>每个组件所有者需要在批准组件之前执行特定的测试任务。在提交评审之前，每个组件都必须得到批准。</p><p></p><p>Sañudo 表示，在测试阶段会不时地发现回归问题。在这种情况下，发布经理与受影响的团队合作修复问题，并推送到主开发分支，只有当回归影响用户体验时，这个修复才会被合并到发布候选分支上。在这个阶段，既不允许出现对用户没有影响的 bug，也不允许添加新特性，每个精心挑选的修复都必须经过团队的论证，并由发布经理批准。</p><p></p><p>如果在流程的后期发现了漏洞，即在应用程序提交审核之后，甚至会采取更严格的规则，因为实施热修复可能会导致发布延迟。</p><p></p><p>虽然更新还没有发布，但可能正在等待评审或已经获得批准，要实施修复，我们将不得不拒绝构建并重新提交应用程序。因为这可能会导致延迟发布，我们会根据具体情况评估修复是否值得以及如何根据具体情况进行修复。</p><p></p><p>在获得苹果公司的批准后，新版本将向 1% 的用户发布，确保没有出现重大问题，并在几天后推向整个用户群。在这个阶段，团队使用一些关键指标来了解新版本的组件可能出现的问题。同样，发布经理使用 <a href=\"https://sentry.io/\">Sentry</a>\" 跟踪更高级别的指标，如崩溃率和趋势性问题。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/12/doordash-mobile-release-process/\">https://www.infoq.com/news/2023/12/doordash-mobile-release-process/</a>\"</p><p></p><p></p>",
    "publish_time": "2023-12-22 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI Agent与行业融合应用的前景及创新应用案例 ｜InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/DLBK3rdpsbvDxCgVHzRl",
    "summary": "<p></p><blockquote>嘉宾｜周健，澜码科技创始人兼CEO；周元剑，澜码科技联合创始人特邀主持人｜吴少杰，InfoQ社区编辑、高级算法专家</blockquote><p></p><p>&nbsp;</p><p>在数字化、智能化的时代，人工智能（AI）已经渗透到各个行业领域，其中AI Agent作为人工智能的重要分支，正在引发一场前所未有的行业融合革命。AI Agent以其智能交互、自主学习、灵活适应等特点，在各个行业中展现出巨大的潜力和价值。</p><p>&nbsp;</p><p>作为一个比较新的概念，AI Agent与行业融合应用的前景非常广阔，它们可以应用于各个领域，如医疗、金融、教育、零售等。本期《极客有约》，我们邀请到了澜码科技创始人兼CEO周健和澜码科技联合创始人周元剑，一同来探讨AI Agent目前的落地情况以及未来的发展趋势。我们还将为大家分享一些成功的AI Agent应用案例，以及探讨如何应对AI Agent应用中可能出现的挑战和问题，如数据隐私、算法透明性等。</p><p>&nbsp;</p><p>InfoQ：请问下您是如何看待最近两年AIGC引发的技术变革的？</p><p>&nbsp;</p><p>周健：我认为当前技术领域正在经历一场巨大的变革。过去，人们一直担心深度学习是否会面临瓶颈，但从GPT-1到GPT-3，尤其是ChatGPT在去年11月30日推出后，社会很快达成了共识，如Sam Altman所说：“我们可以期待未来5到10年内，以大型模型为核心的算力将成为智能基础设施的主要驱动力。”这种算力将带来智能水平指数级的成本下降，正如OpenAI在过去一年中三次降价所示。这一趋势类似于过去CPU价格的下降，对产业发展具有强大的推动力，随着智能的发展，生产力将迅速增加。</p><p>&nbsp;</p><p>另一方面，我们看到OpenAI正在不断提升算法和数据，人们猜测可能会有Q*算法和GPT-5等新的发展。随着算法、数据和算力的增加，我们可以期待更强大的智能。如果我们将智能看作是通用编码能力的衡量标准，其提升可能呈指数级增长。这意味着在ToB和ToC领域，很多业务和事务都将发生重大改变。对于澜码而言，我们将迎来一场波澜壮阔的变革，这也是为什么我们选择这个名字。我们相信，就像海啸即将抵达海岸一样，这一变革将迅速改变我们的工作和生活。</p><p>&nbsp;</p><p>周元剑：从一个更技术的角度来看。近两年，AIGC似乎最初主要在图像领域崭露头角，例如Stable Diffusion和Midjourney等，这确实为我们带来了一些与之前的人工智能不同的地方。在我看来，以前的人工智能更多地属于理解能力，可能有助于解答选择题、判断题或填空题等。而随着AIGC的出现，它可能具备了一些处理问答题的能力。在最初的图像领域，这种能力可能还局限在某一种类型的问答题。</p><p>&nbsp;</p><p>去年ChatGPT取得巨大成功之后，我们看到了大语言模型不仅在单一任务上能够进行问答，甚至可以在更通用的领域中执行问答任务，这基本上等于是智能的雏形。有了这个智能雏形之后，整个想象空间就被打开了。在以前，进行人工智能的整个流程非常漫长，从收集数据、训练模型到不断迭代，需要花费大量时间。如今，大语言模型具有一定的通用性，我们只需通过简单的提示工程，就能在一些常见或相对简单的场景中完成任务。这无疑将极大地提高生产力。</p><p>&nbsp;</p><p>InfoQ：请简要介绍一下澜码科技的背景，为什么要选择AI这条赛道，您看到了哪些机遇？</p><p>&nbsp;</p><p>周健：澜码科技的创建过程有许多偶然和机遇。去年年底，随着整个经济和投资环境的巨大变革，我当时就职公司的内部对AI业务的投资意见分歧较大。当时我们觉得公司可能不会继续发展AI业务了，于是考虑出来创业。</p><p>&nbsp;</p><p>初期想法可能是在自动化方向上，因为在RPA领域，我们看到了很多自动化的潜力。恰好在去年11月30日OpenAI推出ChatGPT，我们突然发现，传统自动化那些“代价”很大的事情在有了大语言模型的支持下变得容易实现了。虽然现在看起来似乎很容易，但实际上在这个过程中，我们经历了不少的迭代和反思。我们最初的想法是在大语言模型上构建一个超级自动化平台，这个方向至今仍然是我们正在努力实现的目标。</p><p>&nbsp;</p><p>InfoQ：在您看来，AI Agent到底是什么？它到底能解决什么技术问题？</p><p>&nbsp;</p><p>周元剑：我认为真正的Agent，其中一个关键点是“tool using”，即使用工具的能力，这是一种与众不同的技能。这也回到了我一开始提到的大语言模型具备了一些初级智能的特质。大语言模型带来的主要变革在于，它可以极大地释放其模型本身的能力，它不仅仅能执行单一任务，而且配合一些工具，比如调用RPA、API或其他工具，完成更智能的工作。以前，如果纯粹采用模型的方法，需要投入大量的精力才能完成任务。但在现有的大语言模型框架下，再加上一些工具，就能够轻松实现。这带来了一种独特的变革，如果脱离这种能力，大概就无法展现其智能的一面，技术上也就没有太多的特别之处。</p><p>&nbsp;</p><p>周健：刚才我们主要从技术供给的角度讨论了一些关于大语言模型的变化。关于Agent的概念，实际上在AI领域很早就被提及了。有人追溯到过去的AI理论，比如马文·明斯基（Marvin Minsky）提出的“Society of Mind”，这个理论认为，人类在执行某些任务时，比如驾驶，实际上有时候是自动进行的，你不需要完全关注驾驶，还可以同时考虑其他事情，比如工作或与家人聊天。这里有一个重要的概念是Autonomous&nbsp;Agent，即能够自主地执行任务。从这个自主执行的角度来看，我们未来的软件在某种程度上可能会脱离目前的束缚，会变得更像一个机器人。</p><p>&nbsp;</p><p>实际上，未来软件应该是具备大语言模型的语言理解能力，能够听懂并与用户互动。同时，按照Autonomous的理论，软件自身具有一定的领域知识，能够规划并提出计划，评估计划的好坏，执行计划，并在遇到困难时进行调整，使其变得越来越智能。</p><p>&nbsp;</p><p>从人类的角度来看，AI Agent的未来是否有意识，是否会演变成硅基生命，或者是否会与人类相互竞争，这些问题可能还需要更深入的思考。但至少从社会的角度来看，不仅是人与AI Agent之间，还是Agent之间相互互动，无论是在生产还是消费端，这里都是一个充满潜力的领域。从技术的角度来看，刚才提到的事情从逻辑上是可以实现的，但是目前还需要更多的资本投入和商业形态的调整，以及相应的模式调整，以便让这些技术能够成功落地。</p><p></p><h2>澜码科技AI Agent自动化平台技术实践</h2><p></p><p>&nbsp;</p><p>InfoQ：请老师和我们分享一下澜码科技的AI Agent自动化平台的落地情况。</p><p>&nbsp;</p><p>周元剑：我们创建澜码科技的初衷是构建一个基于大语言模型的自动化平台，最终的目标是建立一个分为三个层次的平台。</p><p>&nbsp;</p><p>首先是底层的基础AI能力层，然后是中间的Agent构建层，最上面是一些具体的业务应用层。在实践中，我们并不是一层一层地堆叠，而是强调与真实业务场景的对接。我们与客户紧密合作，了解他们的实际业务场景，并根据需求调整平台能力。</p><p>&nbsp;</p><p>我们平台的核心是提供对话的形式，使用户可以通过自然语言进行交流。同时，我们也考虑提供一些CUI形式，结合自然语言和图形用户界面，以更高效地满足用户需求。在平台的搭建过程中，我们遇到了一些挑战，主要集中在两个方面：客户对私有化的需求和稳定性的问题。</p><p>&nbsp;</p><p>客户通常期望平台能够提供私有化解决方案，涉及到数据时可能需要微调，这是一项复杂的任务。客户的数据可能不完整，我们需要与他们一起整理领域知识和数据。此外，私有化还涉及到算力的问题，我们提供了租赁机器的方式以提高灵活性。</p><p>&nbsp;</p><p>另一个挑战是平台的稳定性和性能。我们采用合成式AI的思路，强调符号主义（基于符号和规则的方法）。这意味着我们需要预先梳理用户的知识结构和推理分析的结构，以引导系统。通过这种方法，我们能更好地解决大语言模型应用中的“幻觉”问题。</p><p>&nbsp;</p><p>总的来说，我们的平台不仅提供基础的AI能力，还强调与真实业务场景的对接，以满足客户的需求。在私有化和稳定性方面，我们通过与客户合作、提供机器租赁和采用合成式AI的思路来应对挑战。</p><p>&nbsp;</p><p>周健：我们从自动化的角度出发，早早就在关注如何将人的技能复制出去。我们意识到，当前的大语言模型需要评估其智能的边界。因此，我们与一些AI Agent框架进行了比较，例如AutoGPT、MetaGPT等，发现它们的理念与我们不太一样。它们主要基于大语言模型进行理解。</p><p>&nbsp;</p><p>相比之下，我们更注重核心业务逻辑的编写，代码是有效的。我们坚持使用传统的工程方法，结合像Java、Python这样的代码和大语言模型，以获得最高的性价比。与其他框架（如Langchain、MetaGPT、Dify）不同，我们关注业务逻辑的核心，强调在一开始就编写代码。从编码的角度来看，我们认为这是百分之百准确的方式。例如，无论是工资计算还是银行转账，我们都不愿意接受大语言模型的“下一个对话预测”带来的不确定性。因此，我们强调专家的知识技能应该以某种方式通过编码或传统数据库的方式实现，然后依赖大语言模型的语言理解能力进行互动，这样可以更好地对接那些技能水平可能不太高的一线业务员。总的来说，我们的理念是，通过让专家指导我们的AI Agent，使其能够赋能一线业务人员，提高他们的水平。</p><p>&nbsp;</p><p>InfoQ：目前，大模型出来后，AI Agent的应用落地案例也层出不穷，两位老师如何看待这个问题？</p><p>&nbsp;</p><p>周健：在这个领域，最主要的问题是底层技术突然迎来了一波红利，这是以前未曾发生的，可能是因为当今社会信息传播速度的加快，以及像ChatGPT这样的形态使得每个人都能够尝试使用，因此它带来的社会影响是巨大的。这实际上为所有基于大模型进行应用开发的人提供了巨大的优势，就像突然间开启了一个新的时代。目前很多大公司都在尝试探索它的边界和应用，这在某种意义上是一个红利。</p><p>&nbsp;</p><p>反过来看，我们在行业内已经做了很长时间，但在实际落地时仍然遇到了很多困难。这里有一个误解，比如AutoGPT提到的“AI毁灭世界需要几步”，实际上，AI软件与传统软件非常不同，它并不是通过一个简单的标准路径就可以实现的。因为AI需要高准确率，就像当年深度学习在人脸识别时所面临的问题一样。虽然创建标准路径是容易的，但实际上在生产中使用起来是困难的，因为它对数据的要求和准确率要求都很高。</p><p>&nbsp;</p><p>从结果来看，能够实现良好ROI的AI Agent并不多见。像微软的GitHub Copilot，虽然广受欢迎，但实际上是亏本的。ChatGPT本身也是亏本的。飞书、钉钉，我相信他们算过成本和利润后可能也是不挣钱的。因此，今天从商业成功的角度来看，AI在实际应用中并没有变得非常普及。这需要大家共同努力。</p><p>&nbsp;</p><p>从技术的新颖性角度来看，可能确实有很多创新，但从实际应用的角度来看，与大模型、ChatGPT相比，AI Agent带来的额外增值可能并不是那么明显。</p><p>&nbsp;</p><p>周元剑：虽然我们可以找到一些AI落地的案例，但真正为企业带来额外的价值在实际中并没有如此明显。在这个领域，我们仍然面临着一些挑战，特别是在尝试解决企业实际业务中的问题时。这表明AI的落地并不是一件容易的事情。</p><p></p><h2>AI Agent落地的瓶颈是什么？</h2><p></p><p>&nbsp;</p><p>InfoQ：目前了解到的有哪些已经落地的AI Agent可以分享吗，有哪些技术瓶颈？</p><p>&nbsp;</p><p>周元剑：从技术的角度来看，我个人认为目前在落地应用方面的瓶颈主要存在于这些大型模型本身，它们在理解和逻辑推理方面的能力仍然不够强。因此，在实际应用的过程中，技术团队通常需要为这些大型模型的不足之处提供支持。这种支持不仅仅包括算法和模型方面的调优，还可能需要产品设计和工程方面的能力来保障整个流程。</p><p>&nbsp;</p><p>从另一个角度来看，落地不仅仅是技术层面的问题。我们在实际应用中也遇到了专业知识的问题。即使是一些专业咨询服务的客户，他们内部的专业知识可能并不清晰。因此，我们经常需要与客户合作，共同讨论和梳理专业知识的结构和应用方式。</p><p>&nbsp;</p><p>在长期来看，我认为专业知识和AI Agent的结合是不可避免的。数字化专业知识的转化过程将成为Agent在实际应用中关键的一环。我了解到行业内的一些Agent主要集中在一些大公司内部。这可能是因为这些公司在技术团队、算力和数据方面相对准备充足，同时内部团队更容易推动知识梳理的工作，因此相对容易在内部实现。</p><p>&nbsp;</p><p>另外，有一个有趣的案例是国外的一个Agent，它能够帮助维护代码库。例如，它可以阅读GitHub上某个项目的issue，并分析项目的代码，然后自动回复和编辑者的意图相关的内容，提供建议，甚至执行一些“魔法任务”。在GitHub上已经有一些开源项目正在使用这种Agent协助代码库的维护，这确实是一个令人兴奋的领域。</p><p>&nbsp;</p><p>周健：我认为目前算力是一个相当大的瓶颈。由于一些原因，例如在紧急情况下无法及时交付，我们通常采取私有化部署的方式。一些应用场景对算力要求是起伏不定的，例如为客户生成汉语考试题目，或者支持银行员工销售保险。在这方面，我们遇到了一些具体的问题，比如在实施过程中观察到训练和推理之间的差异，以及在移动设备上可能出现的端侧算力与云端算力之间的调度问题。</p><p>&nbsp;</p><p>此外，算力问题还受到地缘政治和供应链状况的影响，有些先进的显卡可能难以获取。企业内部的IT建设需要考虑如何搭建与大模型相关的技术设施，同时需要解决包括碳排放和绿色环保在内的可持续性问题。在商业运作中取得经济效益，仍然是一个巨大的挑战。我认为，解决这些问题需要至少2～3年的时间，才有可能看到一个能够达到及格标准的解决方案。</p><p>&nbsp;</p><p>InfoQ：根据您们的观察，哪个行业目前在AI Agent商业化落地大模型方面走得最快？</p><p>&nbsp;</p><p>周健：最终我觉得有两个方面，一方面，在大企业服务领域，资金实力较为雄厚。我们今天已经明显感受到信息化是数字化的基础，没有信息化就无法进行数字化。对于我们今天所提到的数智化或者AI Agent，其前提条件必然是数字化程度相对较高的领域，这是可以逆向推导的。比如金融、能源、零售等行业，实际上都处于数字化程度相对较高的状态。在这种场景下，AI Agent的实际落地可能会更加容易一些。</p><p>&nbsp;</p><p>另一方面可能与个体相关，我们可以看到整个灵活用工正在从传统的劳动力市场，如快递、滴滴司机等，逐渐扩展到企业端。企业可能有一些工作是可以外包的，这也符合灵活用工的需求。以前可能会有猪八戒网这样的平台，它允许我们将一些工作外包出去。今天有很多政府园区也在考虑GPT技术的应用，即我们是否可以通过有限的专业人员，再通过大语言模型，提供一些人才法务等方面的支持，以支持中小企业的发展。</p><p>&nbsp;</p><p>因此，在SaaS领域，将专业服务以SaaS化的方式快速复制出去，可能是一个更大的趋势。通过算力，将服务以“Agent as a Service”的形式提供，实际上相当于迅速释放了大量生产力。我们认为在一两年内，可能会形成一些重要的示范效应和案例。</p><p></p><h2>如何规范地引导AI Agent的发展？</h2><p></p><p>&nbsp;</p><p>InfoQ：AI Agent的伦理和数据隐私问题如何解决？我们应该如何规范和引导AI Agent的发展？</p><p>&nbsp;</p><p>周元剑：我认为这个问题本身是比较开放的，目前还没有一个被广泛认可的解决方案。随着应用场景的增加，新的问题也会不断涌现。</p><p>&nbsp;</p><p>对于数据隐私问题，脱敏和私有化是最基本的解决方案。我们可以允许系统和模型在客户的环境中运行，需要微调时，微调的数据和过程也可以保留在客户端。我们更愿意将计算资源移动到客户端，同时确保数据的安全性。通过这些手段，我们可以最基本地保障数据的安全。</p><p>&nbsp;</p><p>在实践中，可能会涉及一些更详细的需求，例如权限管理和数据权限管理，这些与隐私相关。但是，如果数据已经进入模型，管理起来可能会比较困难。因此，通常我们会考虑一些策略，例如通过外部的支付方式，采用应用内购的方式，或者让Agent的设计和执行过程分离等。在我们的解决方案中，核心思想是将整个过程构建成一个多阶段的流水线，而不仅仅是一个AIM（人工智能模型）。这样我们就可以在策略层面上对数据进行更好的管理。</p><p>&nbsp;</p><p>通过这种方式，我们对模型的需求变成了让模型具有能力，比如解答问题、写作、总结文档等，而不仅仅是对知识数据的依赖。模型的能力是局部的，而全量的支持数据都保留在外部。在需要获取数据时，可以通过信息系统进行传统的获取方式，同时可以在信息系统上实施各种隐私保护和权限管理。</p><p>&nbsp;</p><p>此外，随着算法流水线的开放，算法的透明度也得到了一些缓解。用户可以看到Agent是如何一步一步地工作的，每一步的输出都可以供用户审查，确保最终结果基本合理。</p><p>&nbsp;</p><p>未来随着专家知识的沉淀和数据的积累，个体、组织可以选择性地开放独特且稀有的知识和数据，让他人去查阅。这种情况下，个体和组织可以通过让他人使用自己的知识来获取一定的回报。</p><p>&nbsp;</p><p>周健：关于数据的问题，我们在实际的落地过程中遇到了一些挑战，特别是在初期阶段。我们最初考虑在法律行业进行落地，但后来发现可能会面临较大的困难。律师认为他们过去处理的案件是最核心的竞争壁垒，因此他们可能不愿意分享这些信息。正如我们之前提到的，专家知识的数字化是AI Agent落地的必要条件。如果由于经济原因或竞争原因，专家不愿分享知识，就必须设计相应的机制。</p><p>&nbsp;</p><p>国家目前也已经设立了大数据局，对于数据要素和隐私方面也在不断进行探讨。在B端，不同的Agent可能扮演不同的角色，负责不同的任务，拥有不同的权限和责任。类似于企业中对于人员的管理、运用和安排，Agent是否会有相应的权责利体系，是一个需要考虑的问题。因为在没有这些设计的情况下，Agent只是一个程序，这时去处理隐私问题是非常困难的。Agent作为一个能够获取知识、数据和做决策的实体，必然会涉及一些隐私问题。在处理Agent性质和相关隐私问题的过程中，需要进行更深入的讨论，以达成共识。</p><p>&nbsp;</p><p>InfoQ：未来，AI Agent的发展趋势和前景是什么？您看好AI Agent未来的发展吗？您认为多久我们会迎来AI Agent的大规模落地？</p><p>&nbsp;</p><p>周健：目前大语言模型的能力还不够，我们期待它的实施门槛能够降低，让绝大多数人都能够使用。我认为这可能要等到至少GPT-5发布之后，在国外可能需要一年左右，而在国内可能需要2～3年的时间才能实现。</p><p>&nbsp;</p><p>我认为，关键的能力之一是让大语言模型知道自己擅长和不擅长的领域。一旦大语言模型具备了这个能力，它就会更像一个“人”，即使只有高中水平的知识，但它知道自己的能力范围，这就为咨询顾问、企业内部专家构建一个自己的虚拟助手创造了可能。目前，一些知名人士和KOL都在尝试构建自己的虚拟助手。如果大语言模型真的具备了这样的能力，并且成为一种普遍的基础设施，那么我相信这样的应用将会迅速普及。</p><p>&nbsp;</p><p>周元剑：目前，Agent发展的主要制约还在于大模型本身的限制。然而，Agent作为一种有效的治理承载形式，我相信未来它会逐渐发展壮大，尽管这可能需要一些时间。正如之前提到的，可能国外明年可能会有GPT-5，使用起来会更加便捷，而国内可能需要更多时间。但无论如何，我相信那个时刻终将到来。</p><p>&nbsp;</p><p>InfoQ：对于想要进入这个领域的公司或个人来说，需要了解哪些相关知识？您有什么意见给到这些人吗？</p><p>&nbsp;</p><p>周元剑：这个问题涉及到个体和组织之间的相关性，因此很难提出通用的解决方案，因为每个人或组织的需求和情境都可能有所不同。然而，从技术的角度来看，对大语言模型算法的一些基本了解是必要的。尽管不要求深入进行类似算法的工作，但了解其基本原理，对于在技术判断中理解模型的边界非常有帮助。</p><p>&nbsp;</p><p>此外，了解与Agent相关的应用知识也很重要，目前业界最流行的是RAG。关于这方面的知识，有很多可以学习的资源，包括经验总结、文献以及与业务相关的学术论文。阅读一些InfoQ的文章也是一个不错的选择，因为它们通常总结得比较全面。</p><p>&nbsp;</p><p>另外，与业务紧密联系也是至关重要的。无论使用什么技术，最终目的都是解决业务问题。脱离业务的技术发展是缺乏基础的。因此，深入了解业务需求，与业务团队密切合作，将有助于更好地应用技术解决实际问题。</p><p>&nbsp;</p><p>周健：这次的变革实际上是一个底层的颠覆，我们公司的起名来源于波澜壮阔的代码，象征着激荡的变化。在这个高速变化的时代，不变的是高速的变化。在真正领先的AI Agent领域中，知识的迭代速度会更快，大约两、三个月就可能刷新一次。</p><p>&nbsp;</p><p>要进入AI领域，定制AI是一个关键的步骤。在过去的经验中，我们学到的是尽量不要做过多的功能，更关注数据集和准确性。与以前编写工程代码时编写测试用例一样，你应该关注采样的数据集是什么样的，它的分布是什么样的，以及训练出的Agent的表现如何。</p><p>&nbsp;</p><p>新的软件越来越像人一样，因此设计AI Agent或者像人一样的软件仍然是重要的。了解人是如何完成任务、学习、进步和成长，将有助于设计人机交互。从命令行到GUI，交互方式一直在不断摸索。在新时代，我们需要思考人与机器之间如何进行更智能的交互，结合命令行、GUI、自然语言理解等技术，形成全新的交互方式。</p><p>&nbsp;</p><p>最后，我们可以从科幻小说中借鉴一些设计思路，例如《钢铁侠》中的贾维斯。这个时代有点像文艺复兴，我们需要重新思考人与机器之间的互动，努力创造出更好的AI Agent。</p><p>&nbsp;</p><p>InfoQ：这里有个观众提问：“ AI Agent跟RPA在能力上有什么区别？”</p><p>&nbsp;</p><p>周健：它们之间的差距很大。RPA相比之下有点“傻”，只能执行预定的任务，比如通过Java或Python进行简单的循环和变量设置。而大语言模型则具有更强大的能力，它能够真正理解对话，生成行动计划，具有更强的通用性和复用性。</p><p>&nbsp;</p><p>InfoQ：这里有个专注于开发垂直行业的对话机器人的观众，他的方法是基于本地知识库构建。他希望通过采用一些技术手段来提高系统提供精确答案的准确度。老师有哪些建议可以帮助解决这个问题？</p><p>&nbsp;</p><p>周元剑：从技术角度来看，我们目前更倾向于采用一种合成式AI的方法。这意味着，首先对所涉及领域的知识进行一定的理解或结构化整理，这有助于更好地完成当前的任务。举个例子，法律领域的文档通常具有特定的结构，是按条目编写的，这是一个独特的特征。整个文档可以根据这些条目进行结构化处理。另外，对于报表等文档，其结构可能涉及行和列，有标题和表头信息。这些表头信息可以单独提取，并作为关键信息用于抽象或关键词匹配等任务。</p><p>&nbsp;</p><p>对于文档Agent，当进行切片时，你需要关注是否存在更好的策略来合并最相关的切片，这有助于提升效果。此外，在传统搜索中，采用多路召回的方法，不仅仅通过单一向量进行搜索结果的获取，还可能结合其他关键字或方式，进行综合排序。</p><p>&nbsp;</p><p>InfoQ：有观众问：“AI Agent之间如何协同工作？”</p><p>&nbsp;</p><p>周健：我认为目前处于很早期的阶段，企业服务可能会划分为不同类型的功能Agent。在我们目前的领域中，我们更多地专注于标准操作流程的自动化，涵盖数据、文档和应用流程等不同方面。</p><p>&nbsp;</p><p>在这个设想中，一些Agent专注于处理数据，而另一些专注于分析流程。例如，数据Agent可能负责回答与数据相关的问题，而文档Agent可能负责整合文档。这也引出了一个关键问题，即如何在团队中设置不同角色，并将数据和流程的权限进行切片，以分配不同的职责。</p><p>&nbsp;</p><p>尽管我认为目前还处于较早的阶段，但在更抽象的层面上，我觉得重要的是因为单个Agent在实现上可能做得不够出色，在技术本身或概念层面上，多个Agent的实质意义尚不清晰。将两段代码组合在一起实际上等同于一段代码。因此，对于多个Agent的概念，我认为在技术和概念层面上仍然需要更深入的思考。</p><p>&nbsp;</p><p>周元剑：我赞同这种观点，即在当前阶段，大语言模型的智能水平尚未达到足够的程度。为了提升端到端效果，可能会考虑引入一些类似反问者或审查者的角色，并使多个角色协同工作以更好地完成任务。然而，在当前大语言模型的能力尚不强大的情况下，这样做可能事倍功半。你可能会花费大量时间来调整不同部分的性能，协调它们的合作方式，但这过程会很耗费精力。举例而言，如果单个模型的准确率只有70%，那么将三个模型串在一起可能会导致总体准确率更低，因为错误的概率会更高。</p><p>&nbsp;</p><p>另外，即使你投入更多精力进行调优，仍然存在一个风险，即大语言模型的能力会不断提升。我们有过这方面的经验，使用了类似的思路，例如在使用GPT-3.5时，尝试通过多个Agent协同工作来完成任务。然而，当我们使用新模型GPT-4时，发现之前的系统，即使经过大量努力进行调整，其结果可能还不如GPT-4效果好。因此，我们对内部采用多个Agent协同工作的做法持谨慎态度。</p><p>&nbsp;</p><p>嘉宾简介：</p><p>&nbsp;</p><p>周健，澜码科技创始人兼CEO。毕业于上海交通大学计算机系，在校期间荣获ACM国际大学生程序设计竞赛的世界冠军，是首个在此项竞赛夺冠的亚洲团队成员。曾在谷歌和阿里云从事搜索领域的工作，涉及底层基础分布和基础设施相关的工作。后加入依图，是公司的第十位员工，负责开展AI工程和产品方面的工作；也曾在RPA公司弘玑担任CTO。2023年初创业，创立了澜码科技公司。</p><p>&nbsp;</p><p>周元剑，澜码科技联合创始人人。本科毕业于上海交通大学。作为依图科技的第6号员工在依图工作10年左右，工作领域涵盖算法、工程、运维、产品经理，以及SaaS业务技术等方面。主导了多个AI落地相关业务，对于这一领域有深刻的理解。在过去的两年在弘玑工作，负责AI产品线。今年初随周健一起创办澜码，投身新创业项目。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-12-22 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Katalyst Custom Config：轻松管理上万节点的差异化配置",
    "url": "https://www.infoq.cn/article/5Vpsr1PJ0J1cMiIVEgRj",
    "summary": "<p>Katalyst 是一个以 QoS 保障为核心的开源资源管理系统，是字节跳动对大规模在离线混部实践的总结。大规模的混部场景对配置管理的自动化和灵活度有很高的要求，本文通过讲解 Katalyst 中的 Katalyst Custom Config 方案，介绍了 Katalyst 实现复杂配置管理的思路以及实际的使用场景。</p><p></p><p></p><h2>背景</h2><p></p><p></p><p>在大规模集群中，往往存在各种不同的机型和业务，这就需要管理员对不同节点进行差异化配置——</p><p>对于 CPU 密集型的业务的节点，我们可能需要调高 CPU 的驱逐阈值，以保证业务的稳定运行；对于 IO 密集型的业务的节点，我们可能需要调低 IO 的驱逐阈值，以防止 IO 饥饿；此外，还需要根据业务的安全需求，对不同节点的 Agent 接口权限进行精细化配置。</p><p></p><p>在上述过程中，AdminQoSConfiguration 和 AuthConfiguration 是比较常见的配置：</p><p></p><p>AdminQoSConfiguration&nbsp;是用于管理 QoS 相关管控手段的配置。例如，它可以配置 cpu/memory/io/network 等多个资源维度的压制驱逐策略，包括各种驱逐开关、驱逐阈值等。它也可以配置混部算法相关的管控策略，如混部开关、混部算法参数等；AuthConfiguration&nbsp;是用于管理 Agent 各类接口的权限策略的配置。例如，它可以配置 out-of-tree plugin 的准入权限，端口访问权限等。这对于保证系统的安全性和稳定性非常重要。</p><p></p><p>然而这些配置在管理层面仍然存在复杂度过高的问题——对于通过 DaemonSet 部署的单机 Agent 而言，传统的基于启动参数的静态配置管理方式只能通过滚动重启实例进行配置变更，存在生效时间长、实例重启存在风险等问题。另外，面对集群中存在的的差异化配置需求，这种方式也只能通过部署多个 DaemonSet 实例的方式实现，存在运维负担较重的问题。因此对于单机管控系统而言，动态配置管理已经成为不可或缺的功能。</p><p></p><p>针对上述需求，原生 Kubernetes 提出了 Dynamic Kubelet Configuration 的动态配置管理方案（v1.11 开始 Alpha 支持，v1.22 之后被废弃），该方案为集群管理员提供了能够通过 Kubernetes API 动态改变 Kubelet 运行时配置的动态配置管理方案。</p><p></p><p>Dynamic Kubelet Configuration 的工作流程大致如下：</p><p></p><p>创建一个 ConfigMap，其中包含了想要在 Kubelet 上应用的配置。将这个 ConfigMap 关联到一个或多个节点。Kubelet 在后台检查这个 ConfigMap，并且在检测到任何改变时，它会重启并使用新的配置。</p><p>然而，Dynamic Kubelet Configuration 也存在一些局限性：</p><p>动态配置的生效需要 Kubelet 重启，这可能会导致正在运行的 Pod 中断，影响应用的稳定性。动态配置只能应用于 Kubelet，对于 out-of-tree 的 agent 如各种 device plugin 等，无法进行动态配置。对于集群内存在机型或业务差异的场景，并没有提供自动化配置的扩展和支持。</p><p></p><p></p><h2>什么是 KCC</h2><p></p><p></p><p>Katalyst 作为字节跳动开源的提高资源利用率的通用资源管控系统，能通过精细化的单机管控手段，实现细粒度的资源隔离与业务 SLA 保障。</p><p></p><p>针对上述社区方案存在的问题，Katalyst 推出一种全新的解决方案 ——&nbsp;Katalyst Custom Config（KCC），这是字节跳动在大规模集群单机管控实践中，总结并设计出了一套高度自动化、扩展性强的单机动态配置管理方案。</p><p></p><p></p><h4>设计目标</h4><p></p><p></p><p>KCC 旨在解决现有方案的局限性，提供一种更加灵活、高效和可扩展的单机动态配置管理方案，以满足日益增长的单机管控需求。以下是 KCC 的主要设计目标：</p><p></p><p>动态配置：KCC 应能够实时响应配置更改，无需重启，从而避免影响正在运行的 Pod 和应用的稳定性。差异化配置：KCC 应能够支持集群内存在机型或业务差异的场景，提供差异化配置的能力，以满足不同节点可能需要的不同配置。自动化管理：KCC 应能够根据节点差异化配置自动下发节点配置，减轻大规模集群管理的工作负担，避免手动操作导致的错误。易于运维：KCC 应提供简单易用的接口和工具，使运维人员能够方便地管理和监控配置的状态和变更。易于扩展：KCC 不仅应用于 Katalyst 自身，还能以 SDK 的形式支持 out-of-tree 的 agent，如各种 device plugin 等，以满足更广泛的配置需求。</p><p></p><p></p><h4>基本架构</h4><p></p><p></p><p>KCC 方案中 Agent 的动态配置都是基于 CRD，而不是 ConfigMap，这能提高动态配置的可靠性和易用性。其各组件或模块的职责如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89d95ebc6d8c18336edb9b41efcf00b7.png\" /></p><p></p><p>KatalystCustomConfig (KCC)：由管理员创建，描述需要托管的动态配置 CRD 信息（如前文提到的 AdminQosConfiguration 和 AuthConfiguration 的 GVR） 和托管行为。KatalystCustomConfig Target (KCCT)：托管的动态配置（如前文提到的 AdminQosConfiguration 和 AuthConfiguration 的 CR），包含实际配置字段以及支持差异化配置的通用字段。CustomNodeConfig (CNC)：每个节点创建的同名 CR，实时同步节点 Labels，保存当前节点匹配的动态配置信息。KCC Controller：管理托管的动态配置 CRD 注册，更新匹配节点的动态配置信息到 &nbsp;CNC。KCC SDK：定时轮询 CNC，自动加载最新动态配置 CR。</p><p></p><p>该方案通过动态配置注册机制，允许管理员动态定义和管理配置，从而实现灵活地按需扩展动态配置。同时，通过使用标签选择器和临时选择器，KCC 也可以灵活实现集群节点差异化自动配置。</p><p></p><p>除此之外，Agent 与 APIServer 无需建立 list/watch 的长链接，避免了在大规模集群场景下过多长链接对 APIServer 的负担。基于 CNC 的配置 hash 缓存策略，也可以有效减少直接查询动态配置的 APIServer 请求。</p><p></p><p></p><h2>KCC 方案详解</h2><p></p><p></p><p></p><h4>动态注册机制</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/09e8b6799ed1a8f45126b45410b16914.png\" /></p><p></p><p>为了方便管理员动态的扩展动态配置的需求，KCC&nbsp;支持动态配置 CRD 的注册，支持管理员将 Agent 的动态配置划分到不同 CRD 中，同时方便进行权限管控。</p><p></p><p>管理员在 KatalystCustomConfig 中通过 TargetType 描述需要被托管的动态配置 CRD 的 GVR（Group Version Resource）。</p><p></p><p>当 KCC Controller 监听到 KatalystCustomConfig CR 的创建，就会根据其配置的 GVR 信息动态创建一个 &nbsp;dynamic Informer，这样 KCC Controller 就可以通过 list/watch 的方式动态发现 &nbsp;KCCT 创建和更新。</p><p></p><p>KCC Controller 会校验 KCCT 配置是否有效，同时更新 KCCT 的 hash 值，并根据 KCCT 中差异化配置将其同步到所匹配的节点的 CNC 的 KatalystCustomConfigList 中。</p><p></p><p>KatalystCustomConfigList 表示当前节点所匹配动态配置信息列表，即多个 TargetConfig，每个 TargetConfig 描述对应 KCCT 的 GVR 信息以及节点当前所匹配动态配置 CR 的信息和其配置的 hash 值。</p><p></p><p></p><h4>差异化配置</h4><p></p><p></p><p>为了满足集群中差异化配置的需求，KCC 方案支持 LabelSelector 或节点列表的配置，充分利用 K8s 原生的 label 和 labelSeletor 特性，将动态配置的差异化划分成三个粒度：全局粒度、LabelSelector 粒度、节点粒度。同时一个节点匹配的配置顺序是节点 &gt; LabelSelector &gt; 全局。</p><p></p><p>全局粒度配置：即动态配置无需指定 NodeLabelSelector 和 EphemeralSelector，同一个集群只能有一个全局配置LabelSelector 粒度配置：即动态配置指定了 NodeLabelSelector，其采用 K8s 原生的 labelSeletor 语法，并支持 =, ==, !=, in, notin 等选择算子，且支持多个 key 的组合，例如 \"key1=value1,key2!=value2\"节点粒度配置：即动态配置指定了 EphemeralSelector，其指定配置所匹配的节点列表，为了避免这种临时配置长期存在导致配置不可维护，要求一定需要配置持续时间，当该配置过期之后会被自动清理。</p><p></p><p></p><h4>配置冲突检测</h4><p></p><p></p><p>一个集群里有一个全局粒度配置、多个 LabelSelector 粒度配置和节点粒度配置，但单一节点所匹配的动态配置只能有一个，因此需要对所有配置进行冲突检测。</p><p></p><p>节点粒度配置冲突检测比较简单，即两个不同配置的节点列表集合不能有交集，但 LabelSelector 粒度配置的冲突检测较为复杂。</p><p></p><p>NodeLabelSelector 支持相等运算符（=/==）、非相等运算符（!=）以及集合运算符（in/notin）来匹配 Label，且支持多个匹配算子组合的复合选择器。然而对于一个 key 而言，所对应的 value 可能是无穷的，selector 中包含可能的 key 越多，出现冲突的可能性越大，配置的维护就越复杂。因此管理员可以通过 KCC 的 NodeLabelSelectorAllowedKeyList 对 NodeLabelSelector 支持的 key 进行约束。</p><p>为了判断两个 LabelSelector 粒度配置是否冲突，我们设计了基于等值集合和不等值集合的冲突检测算法，该算算法基本思路如下：</p><p></p><p>1.&nbsp;对两个配置的选择器 (selectorA 和 selectorB) 遍历所有支持 key ：</p><p>获取selectorA中对于 key 的相等和不等的值集合（equalValueSetA和inEqualValueSetA）。获取selectorB中对于 key 的相等和不等的值集合（equalValueSetB和inEqualValueSetB）。</p><p></p><p>2.&nbsp;检查这两个选择器的值集合满足以下条件则可能存在冲突，需要继续遍历下一个 key：</p><p>如果equalValueSetA和equalValueSetB的交集非空。如果equalValueSetA和equalValueSetB都为空。如果inEqualValueSetA与equalValueSetB的交集不等于equalValueSetB。如果inEqualValueSetB与equalValueSetA的交集不等于equalValueSetA。如果equalValueSetA非空，但equalValueSetB和inEqualValueSetB都为空，即selectorB可以匹配该 key 的任意值。如果equalValueSetB非空，但equalValueSetA和inEqualValueSetA都为空，即selectorA可以匹配该 key 的任意值。</p><p></p><p>3.&nbsp;如果存在一个 key 不满足以上任何条件，则表示两个选择器没有冲突，因为算子之间的关系是 AND。</p><p></p><p>4.&nbsp;如果所有键都满足以上条件，表示两个选择器可能存在冲突。</p><p>基于该算法，我们可以提前判断两个 LabelSelector 可能存在冲突告知用户，避免在新加节点的时候出现匹配到多个配置的情况。</p><p></p><h4>配置优先级</h4><p></p><p></p><p>根据上述冲突检测算法，可以看出如果允许配置多个 key 的情况下，要求每个动态配置尽可能包含所有的 key，这时候对于用户而言在某些场景下可能很难充分考虑到所有可能冲突的情况，比如紧急降级等。因此我们对 LabelSeletor 配置引入了配置优先级的概念，即冲突仅存在相同优先级，一个节点优先匹配高优先级的配置。</p><p></p><p>除此之外，KCC 方案也支持在 NodeLabelSelectorAllowedKeyList 中对不同优先级配置允许的 key，这样可以更好地规范用户的使用。基于生产实践，对于同一个配置，我们推荐最多配置两个优先级，每个优先级的最多配置两个 key，这样才不会导致配置爆炸。</p><p></p><p></p><h4>Agent SDK</h4><p></p><p></p><p>对于 Agent 而言，我们希望其不需要感知节点层差异化配置，只需要根据其需要的动态配置的 GVR 就可以获得当前该节点所匹配的最新的动态配置。</p><p></p><p>因此我们设计了 Agent SDK，负责定时轮询 CustomNodeConfig，只有当 KatalystCustomConfigList 中 TargetConfig 与 cache 中相同 GVR 的 TargetConfig 的配置 hash 值不一致时，才会访问 APIServer 将 TargetConfig 所对应的动态配置最新的 CR 加载到 cache 中。</p><p></p><p>Agent 通过 KCC SDK 只需要动态配置 GVR 信息就可以获取该节点所匹配的动态配置的 CR，而不需要关心当前节点差异化的信息，这有效降低了单机侧识别配置的复杂度。</p><p></p><p></p><h4>Agent 动态配置框架</h4><p></p><p></p><p>基于 KCC 的动态配置管理方案，Katalyst 实现了灵活的 Agent 动态配置框架。该框架涉及以下组件：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5fdba918062d1771d5cde3e932d6d410.png\" /></p><p></p><p>DynamicConfigCRD：包含所有动态配置 CRD api 的结构体，若需要扩展动态配置，需要将动态配置 CRD 的 api 定义加到该结构体中。</p><p></p><p>DynamicConfigManager：位于 MetaServer 中，负责管理 Agent 模块需要监听的动态配置的注册，并通过 KCC SDK 自动获取所需的动态配置，并对获取的动态配置以 DynamicConfigCRD 的结构保存 checkpoint，避免 Agent 重启时无法访问 APIServer 导致无法获取到当前节点的配置。除此之外，其还会将当前节点的 DynamicConfigCRD 与启动时初始的 DynamicAgentConfiguration 通过 ApplyConfiguration 接口进行覆盖，并通过 SetDynamicConfiguration 方法写到全局 Configuration 中，这样 Agent 各个模块就可以直接通过全局 Configuration 的 GetDynamicConfiguration 方法获取到最新的动态配置。</p><p></p><p>DynamicAgentConfiguration：位于全局 Configuration 中，通过 GetDynamicConfiguration 和SetDynamicConfiguration 方法进行读写。其包含 Agent 需要的所有动态配置的实体，其的所有成员对象都需要提供 ApplyConfiguration 的方法，即将 DynamicConfigCRD 应用到该成员对象的策略。</p><p>基于该框架，Katalyst Agent 中的各个模块可以像使用静态配置一样使用动态配置。</p><p></p><p></p><h2>KCC 应用案例</h2><p></p><p></p><h4>混部降级</h4><p></p><p></p><p>在春节活动、线上事故等场景，研发团队需要对集群中的混部资源进行紧急回收，这时就可以通过 KCC 实现快速混部降级，即创建 AdminQoSConfiguration 的全局配置，将 EnableReclaim 设为 false，如下所示：</p><p></p><p><code lang=\"text\">apiVersion: config.katalyst.kubewharf.io/v1alpha1\nkind: AdminQoSConfiguration\nmetadata:\n  name: global\n  namespace: default\nspec:\n  config:\n    reclaimedResourceConfig:\n      enableReclaim: false</code></p><p></p><p></p><h4>策略灰度</h4><p></p><p></p><p>在一些新的管控策略上线的场景，研发团队可以通过 KCC 在部分节点上对该策略进行灰度。例如新驱逐策略上线，管理员可以对部分节点开启 72 小时 Numa 粒度的内存驱逐策略，即创建 AdminQoSConfiguration 的节点级别配置将 enableNumaLevelEviction 设为 true，如下所示：</p><p><code lang=\"text\">apiVersion: config.katalyst.kubewharf.io/v1alpha1\nkind: AdminQoSConfiguration\nmetadata:\n  name: numa-memory-eviction-gray\n  namespace: default\nspec:\n  ephemeralSelector:\n    nodeNames:\n    - node1\n    - node2\n    lastDuration: 72h\n  config:\n    evictionConfig:\n      memoryPressureEvictionConfig: \n        enableNumaLevelEviction: true</code></p><p></p><p></p><h4>机型定制化策略</h4><p></p><p></p><p>在集群中节点的机型存在性能差异的场景，如集群中同时存在 HDD 盘和 SSD 盘的机器，对于 HDD 盘的机器，Kswapd 内存回收对业务指标影响较大，管理员可以通过 KCC 调整 HDD 盘机器的内存驱逐阈值，提前驱逐离线的 pod，即创建 AdminQoSConfiguration 的 LabelSelector 配置，增大 systemFreeMemoryThresholdMinimum 阈值到 20Gi，如下所示：</p><p></p><p><code lang=\"text\">apiVersion: config.katalyst.kubewharf.io/v1alpha1\nkind: AdminQoSConfiguration\nmetadata:\n  name: hybrid-for-disk-hdd\n  namespace: default\nspec:\n  nodeLabelSelector: diskMode=hdd\n  config:\n    evictionConfig:\n      memoryPressureEvictionConfig: \n        systemFreeMemoryThresholdMinimum: 20Gi</code></p><p></p><p></p><h4>后续规划</h4><p></p><p></p><p>Katalyst Custom Config（KCC）方案的动态注册机制允许管理员动态定义和管理配置，差异化配置通过 LabelSelector 和节点列表实现，配置冲突检测算法帮助管理员避免配置匹配冲突，配置优先级允许节点匹配高优先级配置。Agent SDK 简化了 Agent 获取动态配置的过程，隐藏了节点层的差异化配置细节。</p><p></p><p>当前，KCC 已在 Katalyst 中应用，例如管理员可以灵活控制混部配置，实现快速降级或灰度测试。在 Katalyst 后续的版本中，我们将持续迭代 Katalyst Custom Config 方案，使其能够更好地方便集群管理员进行 Agent 动态配置管理。</p><p></p><p>可观测性：当前 KCC 的动态配置的下发暂时没有提供进度显示，对于管理员来说不够友好，因此 KCC 的可观测性增强将作为后续迭代的主要方向之一历史版本：对于动态配置方案而言，为了避免误操作，必然存在回滚的需求，因此历史版本支持也将是 KCC 未来需要补充的能力</p><p></p><p>如需企业交流和合作：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png\" /></p><p>添加字节跳动云原生小助手，加入云原生社群：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4a9e8cba206a158b0404d888f080602.png\" /></p><p></p>",
    "publish_time": "2023-12-22 10:02:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "年终收官！华为云开发者日·2023 年度创享峰会成功举办",
    "url": "https://www.infoq.cn/article/1eY7fD1MyipT3bshCtvT",
    "summary": "<p>12 月 20 日，<a href=\"https://xie.infoq.cn/article/6b291db25639d747804cf9242?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">华为云开发者日</a>\"·2023 年度创享峰会成功举办，众多开发者与技术爱好者齐聚一堂，在现场，有 600 余名开发者与华为云技术专家共同就大模型应用、<a href=\"https://xie.infoq.cn/article/32d167304b2735b51bec5fe3f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">CodeArts </a>\"软件开发等技术话题进行深入探讨，分享实战技巧与解决方案。此外，华为云还精心设置了<a href=\"https://xie.infoq.cn/article/a15065ba41030825c0d72b91b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\"> KooLabs </a>\"工作坊、产品体验官、展区等环节，让开发者亲身体验华为云产品的技术魅力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bbeaddb6a1f9970b1d2e61fa2e903223.jpeg\" /></p><p></p><p>华为云开发者日是面向全球开发者的旗舰活动，旨在全方位服务与赋能开发者，围绕华为云生态“知、学、用、创、商”成长路径，通过前沿技术分享、场景化动手体验、优秀应用创新推介，开发者提供沉浸式学习与交流平台。</p><p></p><p>中关村科学城管委会副主任、海淀区副区长武凯在致辞中表示，海淀是北京国际科技创新中心核心区，拥有丰富的科技创新资源和基础优势。今年，海淀区人工智能产业获评国家战略性新兴产业集群优秀等级，全力打造“中关村人工智能大模型产业集聚区”，并建设多个人工智能特色产业园。华为云作为国内领先的云服务提供商，与海淀区展开合作，为海淀区的数字化转型和科技创新提供了有力的支撑，助力全市数字经济蓬勃发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/505b6392a9a24ffff855223d85eaa749.jpeg\" /></p><p>中关村科学城管委会副主任、海淀区副区长 武凯</p><p></p><p>北京市海淀区东升镇博展股份社党委副书记、总经理代庆在致辞中表示，中关村东升科技园与华为云建立了战略合作伙伴关系，共同推动全链条创新创业生态体系的发展，为全球科技创新型企业服务，激发科技创新人才活力，加速推动创新资源向海淀和东升集聚，汇聚科技创新力量，助力首都高质量发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/13/1347fd97cf70e5afc3f8ac16e53a4bb6.jpeg\" /></p><p> 北京市海淀区东升镇博展股份社党委副书记、总经理代庆</p><p></p><h4>携手开发者，同成长、共进步、望未来</h4><p></p><p></p><p>万千开发者是华为云生态的中流砥柱，华为云为开发者提供全方位的成长和赋能平台，携手开发者共同构建一个更加开放、创新、共享的云生态。</p><p></p><p>会上，华为云开发者联盟总裁王希海表示，开发者是华为云生态建设的核心，华为云将从“多元生态协同”和“全链路赋能”两个方面赋能开发者，从应用开发、部署、运营到商业变现的全流程支持，为开发者铺好云上成长之路。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04f9a2edaa3078383aeb18fa487d067c.jpeg\" /></p><p>华为云开发者联盟总裁 王希海</p><p></p><p>在这一年里，华为云携手开发者共同推动行业发展，众多领域的开发者汇聚在华为云生态里，借助云原生、AI等新技术，创造出了充满想象力的智能世界。本次活动还特别举办了“2023年度华为云开发者生态贡献人物颁奖仪式”，华为云开发者联盟总裁王希海、华为开发者关系部部长许劲松出席颁奖仪式并为开发者颁奖。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/066aa49fe1a1e46a3575208188ec6dd9.jpeg\" /></p><p>2023年度华为云开发者生态贡献人物颁奖现场</p><p></p><p>会上，华为开发者发展运营总监谢文龙也发表了《与每一位开发者共成长》的主题演讲并表示，华为通过昇腾、盘古大模型、CodeArts Snap、HarmonyOS 等产品和技术能力，支撑各行各业软硬件技术发展，同时，通过举办华为开发者大赛、华为开发者体验官、华为开发者训练营等活动，多路径、多维度助力开发者商业成功，为开发者的成长和发展提供了强有力支持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/84/84cff85c3d870555967071efd458510e.jpeg\" /></p><p> 华为开发者发展运营总监 谢文龙</p><p></p><p>值得一体的是，本次大会还重磅发布了《中国开发者关系白皮书》。《中国开发者关系白皮书》是由思否社区和华为合作推出，对开发者行业进行分析和研究，通过对开发者运营人员的赋能，助力开放者行业高质量发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/707c2f5b208702bce56196e9f7516952.jpeg\" /></p><p>《中国开发者关系白皮书》发布现场</p><p></p><h4>全面赋能企业数字化转型，大咖解析最新技术趋势 </h4><p></p><p></p><p>华为云人工智能算法专家夏飞在本次活动上分享了《全栈自主昇腾云服务，加速大模型应用快速落地》的主题演讲并表示，当人工智能进入大模型时代，随着 AI 大模型技术快速成熟，AI 算法与应用的开发、上线部署与业务发放等过程均大幅简化，使用门槛大幅降低。大模型需要大算力，昇腾云服务构筑全栈 AI 自主可控，为大模型创新应用构筑坚实底座，支持多种大模型应用开发场景，并提供全流程迁移工具，可快速实现大模型和应用的适配，赋能百模千态茁壮成长，加速行业智能化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee93e222cf88ca6cc8aa9b1e820a4818.jpeg\" /></p><p>华为云人工智能算法专家 夏飞</p><p></p><p>薪人薪事产品总监张好在本次活动上分享了《薪人薪事基于华为云盘古大模型的用户产品体验变革》的主题演讲，他指出，基于盘古大模型的产品能力框架，华为云为薪人薪事在数据安全、HR SaaS 服务稳定性等方面持续赋能，加速企业数字化转型，为更科学、高效的人力资源管理及更具竞争力的人力资源运营奠定基础。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11cb078b4c665805fa9fd53aa98864f7.jpeg\" /></p><p>薪人薪事产品总监 张好</p><p></p><p>百模千态开源大模型 AI 挑战赛历时 1 个月，吸引了 1500 余名开发者参赛，其中不仅有人工智能爱好者、企事业单位开发者，还有众多高校师生，共计组成 300 多支参赛队伍，经过初赛、复赛层层激烈角逐，最终 10 支队伍进入决赛。这次大赛的成功举办，不仅展示了华为全栈AI技术的实力，更为开源大模型的研发和应用注入了新的活力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c9be1f5ce0d0fc562739ed9f308f914.jpeg\" /></p><p>百模千态开源大模型AI挑战赛颁奖仪式</p><p></p><p>华为云高级产品经理赵彦在本次活动上分享了《华为云 CodeArts Snap，AI 时代的编码革命》的主题演讲，他认为，数字时代竞争激烈，基于AI大模型的应用研发效率提升在企业竞争力构建中扮演着重要角色，华为云 CodeArts Snap 作为集华为智能算力、模型算法和研发知识沉淀于一体的智能开发助手，通过将自然语言转化为程序代码，提升开发者编程效率，助力企业快速响应市场需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d14f07fab822e514ac029cc15f4b9758.jpeg\" /></p><p>华为云高级产品经理 赵彦</p><p></p><p>作为 2023 年度华为开发者大赛总决赛企业赛道金奖得主，北京天图万境科技有限公司创始人、HCDE 图拉古在现场分享了《用 AI 视听技术构建下一代空间文娱新生态》的主题演讲并表示，全感超空间和超感影游，正是顺应新时代产物，它以 AI 为底层技术支撑，带来了娱乐消费方式的新变革，通过与华为云的紧密合作，共同推动产业发展，为用户带来更加丰富、更加沉浸式的娱乐体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78dd64fca3cb43d25129cd5cec0087fb.jpeg\" /></p><p>北京天图万境科技有限公司创始人、HCDE 图拉古</p><p></p><p>大会对北京 HCDG（华为云开发者社区）、HCDE（华为云开发者专家）分别进行了表彰，华为云开发者联盟总裁王希海、华为云开发者生态运营总监胡志学分别为本地 HCDG、HCDE 进行授旗、授牌。未来，华为云将持续推动开发者社区建设，携手各行各业技术专家共同构建开发者生态，为整个开发者生态的繁荣和发展做出更大的贡献。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6244eb444a82a215414d4855f73c725.jpeg\" /></p><p> 北京HCDG（华为云开发者社区）授旗现场</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0bafc8a0392141b21e17b0bac55cbf3.jpeg\" /></p><p>北京HCDE（华为云开发者专家）授牌现场</p><p></p><h4>体验技术盛宴，人人争当技术领跑者</h4><p></p><p></p><p>本次活动还设置了 KooLabs 工作坊、产品体验官、展区等多个环节，为广大学生开发者量身打造了多项技术赋能实操类活动，吸引了北京航空航天大学、北京理工大学，北京邮电大学，北京科技大学，北京工业大学等众多知名高校学生踊跃参与，同学们表示看到了许多具有创新性的开发设计，参与不同学科的理论创享，丰富了眼界和知识。</p><p></p><p>在开发者体验官环节，开发者们亲身体验华为明星产品，华为团队倾听开发者声音，体验反馈帮助华为不断优化和改进产品，以更好地满足开发者，实现产品共创，生态共赢。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/caa9a5dc9618f34ef74827f81f50251e.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdc28249bcb9b0748106bfea74220bda.jpeg\" /></p><p>开发者体验官</p><p></p><p>在现场还有机会体验华为云技术专家为参与者提供的专业的实验指导，深度体验华为云服务，在云端实现云服务的实践、调测和验证。KooLabs 工作坊不仅给开发者来了技术赋能，也为数字产业人才生态的发展提供了有力支持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2024a300f8092de497cabf9c7966236.jpeg\" /></p><p>KooLabs工作坊</p><p></p><p>未来，华为云将继续与广大开发者携手，共同构建产业新生态，对开发者持续赋能，助力开发者行业实现高质量的发展，为开发者提供全方位的支持，助力开发者提升自我，挑战自我和实现自我，让千万开发者在云上创新，释放数字创新源动力。</p>",
    "publish_time": "2023-12-22 10:46:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖确认出席 QCon 上海，分享借助 MetaGPT 之力，实践自然语言编程的前沿探索",
    "url": "https://www.infoq.cn/article/QecDLpTkeOGuGW938r64",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1222&amp;utm_content=wuchenglin\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖将发表题为《<a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5671?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1222&amp;utm_content=wuchenglin\">借助 MetaGPT 之力，实践自然语言编程的前沿探索</a>\"》主题分享，探讨 MetaGPT 如何成为自然语言编程的桥梁，推动智能体社会的发展，以及如何引领自然语言编程迈向更高效、更智能的阶段。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5671?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=10&amp;utm_term=1222&amp;utm_content=wuchenglin\">吴承霖</a>\"，深度赋智创始人兼 CEO。拥有在腾讯等公司十亿级用户、千亿级数据的大规模复杂 AI 落地经验；开源多智能体框架 MetaGPT 作者；NeurIPS AutoDL/NeurIPS AutoWSL 等顶级竞赛世界冠军；多篇论文发表于 TPAMI/KDD/CVPR/ACL 等顶会顶刊；曾获得福布斯 30U30，腾讯、华为内部数十奖项荣誉。他在本次会议的演讲内容如下：</p><p></p><p>演讲：借助 MetaGPT 之力，实践自然语言编程的前沿探索</p><p></p><p>MetaGPT，作为自然语言与编程之间的催化剂，正在推动着我们走向一个更加智能、高效的编程未来。本次演讲将深入 MetaGPT 如何成为自然语言编程的桥梁，推动智能体社会的发展，以及如何引领自然语言编程迈向更高效、更智能的阶段。</p><p></p><p>在这次演讲中，我将深入研究记忆的重要性、近因性和相关性，并分享关于经验获取和记忆学习的技术见解。</p><p></p><p>演讲提纲：</p><p></p><p>MetaGPT 的发展与影响智能体社会与人机协同</p><p>○ 智能体社会：Jürgen Schmidhuber 携手 MetaGPT</p><p>○ 多智能体将成为社会中的一个重要构成</p><p>○ 记忆压缩</p><p>○ 经验获取</p><p>○ 技能学习</p><p>○ 人机协同新范式：智能体与我们的共创未来</p><p>○ 99% 的互联网入口将由 App 变为智能体</p><p>技术挑战与未来展望</p><p></p><p>听众收益点：</p><p></p><p>○ 了解 MetaGPT 在自然语言编程中扮演的角色</p><p>○ 了解 MetaGPT 如何影响智能体社会和自然语言编程的未来</p><p>○ 了解智能体社会中，自然语言编程将如何被进一步发展</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart\">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的性能优化</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>12 月 28-29 日，QCon 全球软件开发大会即将落地上海，中国科学院外籍院士、国际数据库专家樊文飞院士，英特尔大数据技术全球 CTO 戴金权等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。</p><p></p><p>这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，感兴趣的朋友可以扫描下方二维码，查看大会详细日程。咨询购票可联系票务经理 18514549229。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-12-22 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度 Comate：提升编码效率，释放“十倍”软件生产力",
    "url": "https://www.infoq.cn/article/NlLHGHXdGAg4XW61U3Z2",
    "summary": "<p>百度资深工程师、百度 Comate 架构师徐晓强将为你深度解读 Comate 应用实践——全面解析智能代码助手「百度Comate」全流程提效方法、效果及其典型使用场景，并通过现场实操，帮助开发者高效编程！</p>",
    "publish_time": "2023-12-22 11:40:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里云 CTO 周靖人确认出席 QCon 上海，深谈云计算崭新时代",
    "url": "https://www.infoq.cn/article/KUzOm728U94NC6C4Vvvz",
    "summary": "<p></p><blockquote>在这个充满创新和变革的时代，阿里云CTO周靖人即将在12月28日的<a href=\"https://qcon.infoq.cn/202312/shanghai/schedule\">QCon全球软件开发大会上海站</a>\"上，发表题为<a href=\"https://qcon.infoq.cn/202312/shanghai/presentation/5698\">《拓展智能边界，开启云计算崭新时代》</a>\"的主题演讲。大模型时代，看看周靖人对于云计算的思考，与你一同探寻在这一浪潮中我们能够把握的挑战与机遇。&nbsp;周靖人曾是前微软研发合伙人，达摩院智能计算实验室、大数据智能计算和搜索推荐平台负责人、淘宝事业群搜索推荐事业部负责人。2022年12月，任阿里云CTO兼任达摩院副院长。2023年4月，接任阿里巴巴华东有限公司法定代表人、执行董事、总经理。</blockquote><p></p><p>&nbsp;</p><p>随着人工智能技术的迅速发展，我们正迎来大模型时代的曙光。这些模型，以其庞大的数据处理能力和深入的学习机制，正改变着科技界的面貌。在AI领域，大模型不仅代表了数据处理能力的巨大飞跃，也带来了前所未有的创新机遇和挑战。</p><p>&nbsp;</p><p>前不久，周靖人曾谈到过关于阿里云大模型的设想：将 AI 模型的能力开放给更多开发者和合作伙伴。“我们的目标不仅是服务C端产品，更是要打造一个服务各类AI时代创业者、开发者和企业客户的平台”。阿里云以其前沿的 AI 基础设施和模型能力，致力于成为这些创新者最坚强的后盾。</p><p>&nbsp;</p><p></p><h4>全面升级的技术革命：迈向新时代的技术体系</h4><p></p><p>&nbsp;</p><p>周靖人曾谈到，当前的 AI 技术变革，实质上是一场技术体系的全面升级。他提出了三大核心纬度：首先，系统优化，即利用模型能力优化复杂庞大的分布式系统，实现“自动驾驶的云”；其次，通过模型提升开发效率，让云计算变得更智能；最后，以模型为中心，打造最优秀的AI基础设施，提供低成本、高效能的模型训练、微调、推理服务。</p><p>&nbsp;</p><p>那么，云计算的未来将如何演进？在大模型的加持下，我们将面临哪些挑战和机遇？让我们在QCon大会上聆听周靖人的深度解读，探索这个精彩纷呈的新时代。不容错过的洞察，期待与您现场交流。为让无法到场的朋友也能感受本届盛会，12 月 28 日 09:00，QCon上海站还将在 InfoQ视频号对主会场全程直播，赶快扫描下方二维码预约吧。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/24/a6/245aa5bba00b64ff47f21f53f27768a6.png\" /></p><p>&nbsp;</p><p>关于 QCon：作为极客邦科技旗下InfoQ中国主办的综合性技术盛会，QCon全球软件开发大会自2007年以来，每年在全球多个城市召开。12月28-29日，QCon 将落地上海，本次既是技术人的年终聚会，也是 QCon15 周年庆典。欢迎您前来与近千位技术领袖围绕 <a href=\"https://qcon.infoq.cn/202312/shanghai/track/1595\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/202312/shanghai/track/1596\">AI Agent与行业融合应用的前景、</a>\"<a href=\"https://qcon.infoq.cn/202312/shanghai/track/1597\">LLM时代的性能优化</a>\"等多个前沿话题进行深入交流。</p><p>&nbsp;</p>",
    "publish_time": "2023-12-22 12:38:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]