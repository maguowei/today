[
  {
    "title": "Spotify 的平台迁移经验：从小事做起，关注利益相关者，寻求自动化",
    "url": "https://www.infoq.cn/article/wtufrhBsNxaTlJi70DCK",
    "summary": "<p>在管理不断壮大的开发团队与愈发复杂的代码库的同时，还要提供更快也更可靠的交付，这似乎是飞速发展的科技公司都难逃的挑战，对平台团队而言也是一样。在代码库和组织不断增长的现状下，我们要如何快速推陈出新、更安全地引入新技术呢？</p><p></p><h3>问题域</h3><p></p><p>从2019年的29%，2020年的49%，到2021年的23%，Spotify的移动端代码库规模一直在急速扩大。随着我们在业务上的扩展，Spotify Live等新体验的加入，代码库也在不断演化。而于此同时，Spotify也在不断招收新工程师，致使移动端代码库的修改次数逐渐增加。</p><p>&nbsp;</p><p>我们的移动工程策略项目于一年半前发起了一项倡议，通过多次迁移让客户端功能得以在独立环境中进行开发，类似于后端微服务。</p><p>&nbsp;</p><p>自此之后，我们将系统与约2,200个安卓及iOS组件相关联，将大部分安卓与iOS代码库迁移至谷歌的开源构建系统<a href=\"https://bazel.build/\">Bazel</a>\"进行构建。这一大工程影响了公司上下上百的项目组。</p><p></p><h3>主要难点</h3><p></p><p>我们认为，代码库和组织规模的增长与复杂性的加剧将会是迁移的主要难点所在。如果我们期望能在Spotify的移动端高效且规模化交付平台解决方案，就需要常态化迁移所带来的挑战。</p><p>&nbsp;</p><p>以下是我们在迁移路上所遇到的挑战，其中囊括了场景和问题表征，以及面对问题时应当避免或鼓励的行为。</p><p></p><h4>挑战之一：界定范围</h4><p></p><p>如何在创造变化并提出可支持大部分用例的标准化架构解决方案的同时，不影响其他用例？</p><p>&nbsp;</p><p>场景：</p><p>这一范围看似很广，给人一种无数用例亟待处理的感觉。在面对大量的技术债务、需要的改动、需要支持的用例时，往往会让人无从着手。利益相关者们即使是在多次询问后，也还是对迁移时自己要做的事云里雾里。</p><p>&nbsp;</p><p>应避免：</p><p>在还未确定所有可能情况之前便试图推出解决方案。在尚未预估路线图或成功的界定之前便开始大规模迁移。在没有明确定义利益相关者需要做的事之前便接触他们。</p><p>&nbsp;</p><p>应鼓励：</p><p>清楚目标。在产品简介中写明你的原因、方法，以及目的。注重价值。价值的转变是很耗时的，但需要的时候也要对其调整以适应你的目标。应对受众。了解受众的心智模式才能给出相关的回应、对接他们的需求，并寻找到合适的代理以扩大自己的影响。从小事做起。创建一份概念证明，和利益相关者们核对其内容，并让迁移经历alpha、beta，以及GA产品循环。从最常见的用例开始，随进展不断添加新发现的用例。这一步会让你收到足够的反馈，并渐渐能够支撑不同用例情况。合作！在早期阶段，合作是关键。寻找愿意积极试用早期解决方案的人，他们将成为自己群组内的传播者。这点最后也将助力于方案的可扩展性。&nbsp;</p><p></p><h4>挑战之二：扩大规模</h4><p></p><p>在飞速扩张的组织内，我们要如何才能更快地推动大型架构和基础设施的变化？</p><p>&nbsp;</p><p>场景：&nbsp;</p><p>大量项目组将会受变动影响（超过百个项目组）工作量巨大。其中包括迁移中途需要团队手动重构的任务进展缓慢。涉及无数利益相关者与依赖关系利益相关者被进行中的迁移工作重担压垮</p><p>&nbsp;</p><p>应避免：</p><p>认为大型组织中的大型基础设施和架构中的变动是不可能或不需要的在进行架构或基础设施变动时要缓慢前行</p><p>&nbsp;</p><p>应鼓励：</p><p>关注利益相关者的管理。明确不同利益相关者的优先级，通过邮件或Slack等形式保持联络，并将自己工作的重点相告知。沟通。通过邮件及工作区发帖将自己的进度共享，让受众保持参与感。寻求自动化。预先投入的自动化会简化迁移进程。我们是否需要重构代码？是否能通过脚本将重复步骤自动化？为敏捷的spike周腾出时间。团队与贡献者组队，用一周时间协同合作迁移。通过不同资源提升影响力。培训项目可以帮助团队了解迁移和新的概念，从而能够立刻将其投入应用。在公司的入职计划中介绍公司的工程实践，让新员工能从一开始就理解并遵循所推荐的最佳实践。</p><p></p><h4>挑战之三：事件优先级</h4><p></p><p>平台团队是需要努力减少技术债务并引入新技术，还是要让其对自己代码质量负责？这二者之间要如何平衡？</p><p>&nbsp;</p><p>场景：</p><p>利益相关者参与了优先级更高的项目，而无暇采用你的解决方案。利益相关者认为平台迁移拖慢了他们的脚步，缺乏采用新技术的动力。项目被稀释，推动迁移进行的团队没有动力，甚至有成员选择离开团队。必要的代码修改与迁移方向相左。</p><p>&nbsp;</p><p>应避免：</p><p>自信利益相关者明白迁移的重要性和影响程度，并将其优先处理。实际上他们也有很多其他任务要处理。放弃。“我们忙于抢占更高优先级”，这是个平台迁移中很常见的放弃理由。认为指标和目标是固定且很难变动的。</p><p>&nbsp;</p><p>应鼓励：</p><p>应激励，向利益相关者展示迁移的积极影响，鼓励他们完成迁移任务。持续评估。季度内定期的检查点可以评估迁移进行的速度，判断是否能达成季度、半年度或年度的指标。风险管理。如果迁移进展过慢，要如何调整方法以达成目标？是否能精简流程？是否需要更多工程师？能否雇佣承包商？能否影响其他团队，把自己的任务加入他们积压的工作之中？替平台承担。可能的情况下，为组织做出必要变动，以便组织能专注于为用户创造价值。密切关注与迁移KPI相违背的变动，建立与团队之间的渠道以提供支持。</p><p>&nbsp;</p><p></p><h4>挑战之四：承担责任</h4><p></p><p>如何才能在需要大量变动和重构基础设施的新技术采用过程中，让团队承担起责任？</p><p>&nbsp;</p><p>场景：</p><p>在新技术的采用过程中缺乏责任感，导致迁移进展缓慢。</p><p>无法预估迁移何时结束。</p><p>&nbsp;</p><p>应避免：</p><p>在驱动大量改动时指望内外部能保持一致性。认为基础设施上的变动和影响很难衡量。</p><p>&nbsp;</p><p>应鼓励：</p><p>明确“完成”的定义。借助数据和趋势图给出预测。我们需要清楚自己的出发点和进展速度，才能预估未来的趋势，并确定是否需要调整策略以加快迁移进展。进度展示。掌控成功的定义且不断沟通，才能保持受众参与并接受我们所做的变动。使用看板。通过指标和看板沟通进展和影响，并规模化确定一段时间内的工作优先顺序。维护时间线并实时更新路线图。随着时间的推移，团队和利益相关者可能会有变动，他们需要了解我们的迁移进程和时间线。路线图也有助于透明化，让反馈和协作成为可能，也能帮助发现障碍。</p><p></p><h3>结论</h3><p></p><p>这种性质和规模的迁移工作可能会成为未来的常态，若非如此，公司可能将无法执行特定的变动。新科技将会不断出现，迁移也将成为必然，但我们也应减少这些动作对团队的干扰。有的平台产品或许会让迁移无可避免，且可能会规模不小，我们应将其与测试、设计共同视作是开发周期的一部分。</p><p>&nbsp;</p><p>我们从这项工作中学到了很多，希望这些经验能对其他团队的大型迁移有所帮助。如果你想了解更多关于我们所遇到的挑战和相应的解决方法，请随时联系我们。</p><p>&nbsp;</p><p>特别感谢Marvin、Foundation、BoB，以及Rubik，为我们的工作提供助力。</p><p></p><p>原文链接：</p><p><a href=\"https://engineering.atspotify.com/2022/11/strategies-and-tools-for-performing-migrations-on-platform/\">Strategies and Tools for Performing Migrations on Platform</a>\" </p><p></p>",
    "publish_time": "2024-01-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "KSP2 致力于改善 Kotlin 元编程，并添加对 K2 Kotlin 编译器的支持",
    "url": "https://www.infoq.cn/article/PPC5WXDEyXlVoS18ZGP9",
    "summary": "<p>KSP 2.0 是 Kotlin 符号处理（Kotlin Symbol Processing）的演进版本，目前处于预览状态，谷歌的软件工程师 Ting-Yuan Huang 和 Jiaxiang Chen 说到，它引入了新的架构，旨在解决 KSP 1.0 中的一些局限性，并增加了对新的 K2 Kotlin 编译器的支持。</p><p></p><p>KSP1 是作为编译器插件的形式实现的，而 KSP2 是一个独立的库，无需设置编译器即可运行，并能完全控制其生命周期。Huang 和 Chen 说，这使得以编程方式调用 KSP 以及在 KSP 处理器中设置断点变得更容易。下面的代码展示了如何配置 KSP2 并执行它来处理符号的列表：</p><p></p><p><code lang=\"kotlin\">val kspConfig = KSPJvmConfig.Builder().apply {\n  // All configurations happen here.\n}.build()\nval exitCode = KotlinSymbolProcessing(kspConfig, listOfProcessors, kspLoggerImpl).execute()\n</code></p><p></p><p>KSP2 中另外一个值得注意的差异是，它使用了仍处于 beta 状态的 Kotlin K2 编译器来处理源码。不过，如果你愿意的话，也可以通过在gradle.properties中设置languageVersion属性，从而以 K1 的方式使用 KSP。</p><p></p><p>除此之外，KSP2 还旨在解决 KSP1 中的一个缺陷，即同一个源文件可能会被编译多次。借助与 K2 的集成，KSP2 尝试调整 K2 编译文件的方式，使其只处理一次，从而能够提升性能。</p><p></p><p>KSP2 还引入了一些行为的变化，以提高开发人员的工作效率，以及可调试性和错误恢复能力。</p><p></p><p>在 KSP 1.0.14 或更新的版本中，可以在gradle.properties中使用一个标记来启用新的 KSP 预览版本：</p><p></p><p></p><blockquote>ksp.useKSP2=true</blockquote><p></p><p></p><p>KSP 是一个支持创建插件来扩展 Kotlin 编译器的 API。它以独立于编译器的方式理解 Kotlin 的语言特性，如扩展函数、声明处型变（declaration-site variance）和局部函数。</p><p></p><p>该 API 根据 Kotlin 语法在符号层对 Kotlin 程序结构进行建模。当基于 KSP 的插件处理源程序时，处理器可以访问类、类成员、函数和相关参数等构造结构，而 if 代码块和 for 循环等则无法访问。</p><p></p><p>这使得基于 KSP 的插件不像建立在kotlinc之上的插件那样脆弱，后者功能更强大，但是严格依赖于编译器的版本。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/ksp2-kotlin-metaprogramming/\">https://www.infoq.com/news/2024/01/ksp2-kotlin-metaprogramming/</a>\"</p>",
    "publish_time": "2024-01-30 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Neuralink完成全球首例人类脑机芯片植入手术，马斯克：植入者恢复良好",
    "url": "https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB",
    "summary": "<p>1月30日，脑机接口公司Neuralink创始人埃隆·马斯克在X（Twitter）上宣布：在昨天，人类首次接受脑机接口（Neuralink）芯片植入，植入者恢复良好。马斯克在随后的帖子中表示，Neuralink的第一款产品名为“心灵感应”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3a4d210f40b261d175830b696a7a9c12.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97165c85614dbc3f3355c3a7c5dc1383.png\" /></p><p></p><p>&nbsp;</p><p>资料显示，Neuralink由马斯克和其他一群科学家和工程师于 2016 年创立。它致力于开发脑机接口 (BCI)，将人脑连接到能够破译神经信号的计算机，旨在帮助严重瘫痪的患者仅使用神经信号来控制外部技术。马斯克称，Neuralink 的设备可以实现“超人认知”，使瘫痪的人有一天能够用他们的思想操作智能手机或机器人肢体，并“解决”自闭症和精神分裂症。</p><p>&nbsp;</p><p>如果这项技术功能正常，患有严重退行性疾病(如ALS)的患者有一天可以使用植入物通过移动光标和用大脑打字来交流或访问社交媒体。马斯克写道：“想象一下，如果斯蒂芬·霍金的沟通速度比打字员或拍卖师还快。”“这就是我们的目标。”</p><p>&nbsp;</p><p>2022 年11月30日，马斯克曾在 Neuralink 发布会上展示了一只头骨上装有电脑芯片的猴子“Sake”，在玩心灵感应视频游戏，并要到了葡萄零食的镜头。猴子“Sake”通过屏幕和脑里植入的 Neuralink 的 N1 设备，来追踪屏幕上的移动光标，拼出了“Can I please have snacks”的英文短句，全程和键盘没有物理接触。</p><p>&nbsp;</p><p>马斯克说，猴子其实不会拼写，它们只是将大脑信号转化为光标移动，迭代到下一个版本才能让猴子具备拼写能力。马斯克还在演讲中表示，Neuralink 还将致力于恢复视力。“即使有人从未有过视力，他们先天就失明了，我们相信我们的设备仍然可以帮助其恢复视力”。</p><p>&nbsp;</p><p>当时，Neuralink 还未获得美国食品和药物管理局（FDA）的批准，因此一直在在对动物进行测试。马斯克说：“我们希望非常小心，并确保它在将设备放入人体之前能正常工作。”</p><p>&nbsp;</p><p>马斯克还表示，在人体测试开始后，他将在自己的大脑中植入脑机接口设备。“你可能现在就植入了一个 Neuralink 设备，而你甚至都不知道……在（未来）其中一个演示中，我会的” 他说。活动结束后，他在推特上重申了这一点。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cf414be80d775a1718eb6f6c6fca114b.png\" /></p><p></p><p>2023年5月，Neuralink获得美国食品和药物管理局(FDA)批准。9月19日，Neuralink宣布，该公司已获得了独立机构审查委员会的批准，开始首次人体临床试验招募人员。Neuralink表示，因颈部脊髓损伤或肌萎缩侧索硬化症（ALS）而瘫痪的患者可能符合参加这项试验的条件。</p><p>&nbsp;</p><p>Neuralink没有透露将有多少人类患者参与其最初的人体试验。据悉，人体临床试验只是Neuralink走向商业化道路上的一步。去年的融资情况显示，Neuralink的估值已高达50亿美元（约合人民币359亿元）。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.cnbc.com/2024/01/29/elon-musks-neuralink-implants-brain-tech-in-human-patient-for-the-first-time.html\">https://www.cnbc.com/2024/01/29/elon-musks-neuralink-implants-brain-tech-in-human-patient-for-the-first-time.html</a>\"</p><p><a href=\"https://www.infoq.cn/article/6s4aDChIwYEOjVH62lGa\">https://www.infoq.cn/article/6s4aDChIwYEOjVH62lGa</a>\"</p>",
    "publish_time": "2024-01-30 11:19:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度大模型驱动下的智能代码助手提效实践",
    "url": "https://www.infoq.cn/article/biw3d0tLbjlQXkyyHwBV",
    "summary": "<p>Qcon上海站百度工程效能部资深研发工程师李杨坐镇，分享介绍百度大模型在智能代码助手领域的实现和落地场景。</p>\n<p>听众收益点：</p>\n<p>1.AIGC大潮下，帮助软件工程师们知道如何利用工具来进化自己<br />\n2.帮助大家了解工程师们如何用代码生成产品来提升自己的开发效率<br />\n3.了解百度代码生成工具Comate是怎么实现的，除了大模型本身，工程化方面的优化措施</p>",
    "publish_time": "2024-01-30 11:50:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "文本检索性能提升 40 倍，Apache Doris 倒排索引深度解读",
    "url": "https://www.infoq.cn/article/mow1CJlqo2IH6zkDoZes",
    "summary": "<p>在 OLAP 领域，<a href=\"https://doris.apache.org/\">Apache Doris</a>\" 已成为高性能、高并发以及高时效性的代名词。在面向海量数据的复杂查询需求时，除硬件配置、集群规模、网络带宽等因素外，提升性能的核心在于如何最大程度地降低 SQL 执行时的 CPU、内存和 IO 开销，而这其中数据库索引扮演着至关重要的角色。合理的索引结构设计可以跳过大量不必要的底层数据读取、快速检索定位到所需数据，并进一步提升后续计算的执行效率、降低查询 SQL 的运行时间和资源消耗。</p><p></p><p>Apache Doris 提供了丰富的索引以加速数据的读取和过滤，依据是否需要用户手工创建，索引类型大体可以分为智能内建索引和用户创建索引两类，其中智能内建索引是指在数据写入时自动生成的索引，无需用户干预，包括前缀索引和 ZoneMap 索引。用户创建索引需要用户根据业务特点手动创建，包括 Bloom Filter 索引和 2.0 版本新增的倒排索引与 NGram Bloom Filter 索引。</p><p></p><p>相较于用户比较熟悉的前缀索引、Bloom Filter 索引，2.0 版本所新增的<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247519079&amp;idx=1&amp;sn=a232a72695ff93eea0ffe79635936dcb&amp;chksm=cf2f8560f8580c768bbde99ef8ca97d3a42ecc03b5d8d106b85f5474c90b6068781a79b3611e&amp;scene=21#wechat_redirect\">倒排索引</a>\"和 NGram Bloom Filter 在文本检索、模糊匹配以及非主键列检索等场景有着更为明显的性能提升。本文将以 Amazon customer reviews 数据集为例，介绍 Apache Doris 在查询该数据集以及类似场景中，如何充分利用倒排索引以及 NGram Bloom Filter 索引进行查询加速，并详细解析其工作原理与最佳实践。</p><p></p><h2>数据集样例</h2><p></p><p>在本文中，我们使用的数据集包含约 1.3 亿条亚马逊产品的用户评论信息。该数据集以 Snappy 压缩的 Parquet 文件形式存在，总大小约为 37GB。以下为数据集的样例：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfa4cb6e2693f8e899bfeb6379554486.png\" /></p><p></p><p>在子集中，每行包含用户 ID（customer_id）、评论 ID（review_id）、已购买产品 ID（product_id）、产品分类（product_category）、评分（star_rating）、评论标题（review_headline）、评论内容（review_body）等 15 列信息。根据上述可知，列中包含了适用于索引加速的各种特征。例如，customer_id 是高基数的数值列，product_id 是低基数的定长短文本列，product_title 是适合文本检索的短文本列，review_body 则是适合文本搜索的长文本列。</p><p></p><p>通过这些列，我们可以模拟两个典型索引查询场景，具体如下：</p><p></p><p>文本搜索查询：搜索 review body 字段中包含特定内容的产品信息。非主键列明细查询：查询特定产品 ID（product_id）或者特定用户 ID（customer_id）的评论信息。</p><p></p><p>接下来，我们将以文本搜索和非主键列明细查询为主要方向，对比在有索引和无索引的情况下查询性能的差异。同时，我们也将详细解析索引减少查询耗时、提高查询效率的原理。</p><p></p><h2>环境搭建</h2><p></p><p>为了快速搭建环境，并进行集群创建和数据导入，我们使用单节点集群（1FE、1BE）并按照以下步骤进行操作：</p><p></p><p>搭建 Apache Doris ：具体操作请参考：<a href=\"https://doris.apache.org/zh-CN/docs/get-starting/quick-start/\">快速开始</a>\"创建数据表：按照下列建表语句进行数据表创建</p><p></p><p><code lang=\"sql\">CREATE TABLE `amazon_reviews` (  \n  `review_date` int(11) NULL,  \n  `marketplace` varchar(20) NULL,  \n  `customer_id` bigint(20) NULL,  \n  `review_id` varchar(40) NULL,\n  `product_id` varchar(10) NULL,\n  `product_parent` bigint(20) NULL,\n  `product_title` varchar(500) NULL,\n  `product_category` varchar(50) NULL,\n  `star_rating` smallint(6) NULL,\n  `helpful_votes` int(11) NULL,\n  `total_votes` int(11) NULL,\n  `vine` boolean NULL,\n  `verified_purchase` boolean NULL,\n  `review_headline` varchar(500) NULL,\n  `review_body` string NULL\n) ENGINE=OLAP\nDUPLICATE KEY(`review_date`)\nCOMMENT 'OLAP'\nDISTRIBUTED BY HASH(`review_date`) BUCKETS 16\nPROPERTIES (\n\"replication_allocation\" = \"tag.location.default: 1\",\n\"compression\" = \"ZSTD\"\n);\n</code></p><p></p><p>3.下载数据集：从下方链接分别下载数据集，数据集为 Parque 格式，并经过 Snappy 压缩，总大小约为 37GB</p><p></p><p><a href=\"https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2010.snappy.parquet\">amazon_reviews_2010</a>\"<a href=\"https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2011.snappy.parquet\">amazon_reviews_2011</a>\"<a href=\"https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2012.snappy.parquet\">amazon_reviews_2012</a>\"<a href=\"https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2013.snappy.parquet\">amazon_reviews_2013</a>\"<a href=\"https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2014.snappy.parquet\">amazon_reviews_2014</a>\"<a href=\"https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2015.snappy.parquet\">amazon_reviews_2015</a>\"</p><p></p><p>4.导入数据集：下载完成后，分别执行以下命令，导入数据集</p><p></p><p><code lang=\"bash\">curl --location-trusted -u root: -T amazon_reviews_2010.snappy.parquet -H \"format:parquet\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\ncurl --location-trusted -u root: -T amazon_reviews_2011.snappy.parquet -H \"format:parquet\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\ncurl --location-trusted -u root: -T amazon_reviews_2012.snappy.parquet -H \"format:parquet\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\ncurl --location-trusted -u root: -T amazon_reviews_2013.snappy.parquet -H \"format:parquet\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\ncurl --location-trusted -u root: -T amazon_reviews_2014.snappy.parquet -H \"format:parquet\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\ncurl --location-trusted -u root: -T amazon_reviews_2015.snappy.parquet -H \"format:parquet\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\n</code></p><p></p><p>5.查看与验证：完成上述步骤后，可以在 MySQL 客户端执行以下语句，来查看导入的数据行数和所占用空间。从下方代码可知：共导入 135589433 行数据，在 Doris 中占用空间 25.873GB，比压缩后的 Parquet 列式存储进一步降低了 30%。</p><p></p><p><code lang=\"sql\">mysql&gt; SELECT COUNT() FROM amazon_reviews;\n+-----------+\n| count(*)  |\n+-----------+\n| 135589433 |\n+-----------+\n1 row in set (0.02 sec)\nmysql&gt; SHOW DATA FROM amazon_reviews;\n+----------------+----------------+-----------+--------------+-----------+------------+\n| TableName      | IndexName      | Size      | ReplicaCount | RowCount  | RemoteSize |\n+----------------+----------------+-----------+--------------+-----------+------------+\n| amazon_reviews | amazon_reviews | 25.873 GB | 16           | 135589433 | 0.000      |\n|                | Total          | 25.873 GB | 16           |           | 0.000      |\n+----------------+----------------+-----------+--------------+-----------+------------+\n2 rows in set (0.00 sec)\n</code></p><p></p><h2>文本搜索查询加速</h2><p></p><p></p><h3>无索引硬匹配</h3><p></p><p>环境及数据准备就绪后，我们尝试对 review_body 列进行文本搜索查询。具体需求是在数据集中查出评论中包含“is super awesome”关键字的前 5 种产品，并按照评论数量降序排列，查询结果需显示每种产品的 ID、随机一个产品标题、平均星级评分以及评论总数。review_body 列的特征是评论内容比较长，因此进行文本搜索会有一定的性能压力。</p><p></p><p>首先我们直接进行查询，以下是查询的示例语句：</p><p></p><p><code lang=\"sql\">SELECT\n    product_id,\n    any(product_title),\n    AVG(star_rating) AS rating,\n    COUNT() AS count\nFROM\n    amazon_reviews\nWHERE\n    review_body LIKE '%is super awesome%'\nGROUP BY\n    product_id\nORDER BY\n    count DESC,\n    rating DESC,\n    product_id\nLIMIT 5;\n</code></p><p></p><p>执行结果如下，查询耗时为 7.6 秒</p><p></p><p><code lang=\"sql\">+------------+------------------------------------------+--------------------+-------+\n| product_id | any_value(product_title)                 | rating             | count |\n+------------+------------------------------------------+--------------------+-------+\n| B00992CF6W | Minecraft                                | 4.8235294117647056 |    17 |\n| B009UX2YAC | Subway Surfers                           | 4.7777777777777777 |     9 |\n| B00DJFIMW6 | Minion Rush: Despicable Me Official Game |              4.875 |     8 |\n| B0086700CM | Temple Run                               |                  5 |     6 |\n| B00KWVZ750 | Angry Birds Epic RPG                     |                  5 |     6 |\n+------------+------------------------------------------+--------------------+-------+\n5 rows in set (7.60 sec)\n</code></p><p></p><h3>利用 Ngram BloomFilter 索引加速查询</h3><p></p><p>接下来，我们尝试使用 Ngram BloomFilter 索引进行查询加速</p><p></p><p><code lang=\"sql\">ALTER TABLE amazon_reviews ADD INDEX review_body_ngram_idx(review_body) USING NGRAM_BF PROPERTIES(\"gram_size\"=\"10\", \"bf_size\"=\"10240\");\n</code></p><p></p><p>添加 Ngram BloomFilter 索引之后，再次执行相同的查询。执行结果如下，查询耗时缩短至 0.93 秒，相较于未开启索引，查询效率提高了 8 倍。</p><p></p><p><code lang=\"sql\">+------------+------------------------------------------+--------------------+-------+\n| product_id | any_value(product_title)                 | rating             | count |\n+------------+------------------------------------------+--------------------+-------+\n| B00992CF6W | Minecraft                                | 4.8235294117647056 |    17 |\n| B009UX2YAC | Subway Surfers                           | 4.7777777777777777 |     9 |\n| B00DJFIMW6 | Minion Rush: Despicable Me Official Game |              4.875 |     8 |\n| B0086700CM | Temple Run                               |                  5 |     6 |\n| B00KWVZ750 | Angry Birds Epic RPG                     |                  5 |     6 |\n+------------+------------------------------------------+--------------------+-------+\n5 rows in set (0.93 sec)\n</code></p><p></p><p>接下来，我们根据代码示例展开说明。使用 ALTER TABLE 语句为表增加 Ngram BloomFilter 索引时，gram_size 和 bf_size 参数具有特定的含义：</p><p></p><p>gram_size：表示 n-gram 中的 n 值，即连续字符的长度。在上述代码示例中，\"gram_size\"=\"10\" 表示每个 n-gram 包含 10 个字符。这意味着文本将被切割成数个字符长度为 10 的字符串，这些字符串将用于构建索引。bf_size：表示 Bloom Filter 的大小，以字节（Byte）为单位。例如，\"bf_size\"=\"10240\" 表示所使用 Bloom Filter 数据大小占用空间为 10240 字节。</p><p></p><p>在了解基本的参数定义后，我们来探索 Ngram BloomFilter 加速查询的原理：</p><p></p><p>Ngram 分词：使用 gram_size 对每行数据进行分词，当 gram_size=5 时，\"hello world\" 被切分为 [\"hello\"， \"ello \"， \"llo w\"， \"lo wo\"， \"o wor\"， \" worl\"， \"world\"]。这些子字符串经过哈希函数计算后，将被添加到相应大小（bf_size）的 Bloom Filter 中。由于 Doris 数据是按页面（page）组织存储，相应的 Bloom Filter 也会按页面（page）生成。查询加速：以“hello”为例，在匹配过程中也将被切分并生成对应的 Bloom Filter，用于与各页面的 Bloom Filter 进行对比。如果 Bloom Filter 判断为包含匹配字符串（可能会出现假阳性），则加载相应的页面以进一步匹配；否则，将跳过该页面。其原理即通过跳过不需要加载的页面（page），减少需要扫描的数据量，从而显著降低了查询延时。</p><p></p><p><img src=\"\" /></p><p></p><p></p><div><img alt=\"Ngram Bloom Filter 示意图\" src=\"https://cdn.selectdb.com/static/Ngram_Bloom_Filter_5263c434b4.png\" /></div><p></p><p></p><p>通过上述原理描述可以看出，针对不同的场景合理的配置 Ngram BloomFilter 的参数会达到更好的效果，gram_size 的大小直接影响匹配时效率，而 bf_size 的大小影响存储容量和误判率。通常情况下，较大的 bf_size 可以降低误判率，但这样也会占用更多的存储空间。因此，我们建议从以下两方面综合考量配置参数：</p><p></p><p>数据特性： 考虑要索引的数据类型。对于文本数据，需要根据文本的平均长度和字符分布来确定。</p><p></p><p>对于较短的文本（如单词或短语）：较小的 gram_size（例如 2-4）和较小的 bf_size 可能更合适。对于较长的文本（如句子或大段描述：较大的 gram_size（例如 5-10）和较大的 bf_size 可能更有效。</p><p></p><p>查询模式： 考虑查询的典型模式。</p><p></p><p>如果查询通常包含短语或接近完整的单词，较大的 gram_size 可能更好。对于模糊匹配或包含多种变化的查询，较小的 gram_size 可以提供更灵活的匹配。</p><p></p><h3>利用倒排索引加速查询</h3><p></p><p>除了采用 Ngram BloomFilter 索引进行查询加速，还可以选择基于 <a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247519079&amp;idx=1&amp;sn=a232a72695ff93eea0ffe79635936dcb&amp;chksm=cf2f8560f8580c768bbde99ef8ca97d3a42ecc03b5d8d106b85f5474c90b6068781a79b3611e&amp;token=533653942&amp;lang=zh_CN#rd\">倒排索引</a>\"  进一步加速文本搜索的效率。可以通过以下步骤来构建倒排索引：</p><p></p><p>1.新增倒排索引： 对 amazon_reviews 表的 review_body 列添加倒排索引，该索引采用英文分词，并支持 Phrase 短语查询，短语查询即进行文本搜索时，分词后的词语顺序将会影响搜索结果。2.为历史数据创建索引： 按照新增索引信息对历史数据进行索引构建，使历史数据就也可以使用倒排索引进行查询。</p><p></p><p><code lang=\"sql\">ALTER TABLE amazon_reviews ADD INDEX review_body_inverted_idx(`review_body`) \n    USING INVERTED PROPERTIES(\"parser\" = \"english\",\"support_phrase\" = \"true\"); \nBUILD INDEX review_body_inverted_idx ON amazon_reviews;\n</code></p><p></p><p>3.查看及验证： 构建完索引之后，可以通过以下方式对索引构建情况进行查看：</p><p></p><p><code lang=\"sql\">mysql&gt; show BUILD INDEX WHERE TableName=\"amazon_reviews\";\n+-------+----------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+---------------+----------+------+----------+\n| JobId | TableName      | PartitionName  | AlterInvertedIndexes                                                                                                              | CreateTime              | FinishTime              | TransactionId | State    | Msg  | Progress |\n+-------+----------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+---------------+----------+------+----------+\n| 10152 | amazon_reviews | amazon_reviews | [ADD INDEX review_body_inverted_idx (\nreview_body\n) USING INVERTED PROPERTIES(\"parser\" = \"english\", \"support_phrase\" = \"true\")],  | 2024-01-23 15:42:28.658 | 2024-01-23 15:48:42.990 | 11            | FINISHED |      | NULL     |\n+-------+----------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+---------------+----------+------+----------+\n1 row in set (0.00 sec)\n</code></p><p></p><p>如果对分词效果不确定，可以使用 TOKENIZE 函数进行分词测试。TOKENIZE 函数接收两个输入：一个是需要进行分词的文本，一个是分词的属性字段。</p><p></p><p><code lang=\"sql\">mysql&gt; SELECT TOKENIZE('I can honestly give the shipment and package 100%, it came in time that it was supposed to with no hasels, and the book was in PERFECT condition.\nsuper awesome buy, and excellent for my college classs', '\"parser\" = \"english\",\"support_phrase\" = \"true\"');\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| tokenize('I can honestly give the shipment and package 100%, it came in time that it was supposed to with no hasels, and the book was in PERFECT condition. super awesome buy, and excellent for my college classs', '\"parser\" = \"english\",\"support_phrase\" = \"true\"')                                              |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| [\"i\", \"can\", \"honestly\", \"give\", \"the\", \"shipment\", \"and\", \"package\", \"100\", \"it\", \"came\", \"in\", \"time\", \"that\", \"it\", \"was\", \"supposed\", \"to\", \"with\", \"no\", \"hasels\", \"and\", \"the\", \"book\", \"was\", \"in\", \"perfect\", \"condition\", \"super\", \"awesome\", \"buy\", \"and\", \"excellent\", \"for\", \"my\", \"college\", \"classs\"] |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.05 sec)\n</code></p><p></p><p>在倒排索引创建完成后，我们使用 MATCH_PHRASE  来查询包含关键词\"is super awesome\"的产品评论信息（具体需求可回顾前文）。</p><p></p><p><code lang=\"sql\">SELECT\n    product_id,\n    any(product_title),\n    AVG(star_rating) AS rating,\n    COUNT() AS count\nFROM\n    amazon_reviews\nWHERE\n    review_body MATCH_PHRASE 'is super awesome'\nGROUP BY\n    product_id\n\nORDER BY\n    count DESC,\n    rating DESC,\n    product_id\nLIMIT 5;\n</code></p><p></p><p>以上述代码示例进行说明，review_body MATCH_PHRASE 'is super awesome'  表示对 review_body  列进行短语匹配查询。具体而言，查询会在 review_body 中按照英文分词后，寻找同时包含 \"is\"、\"super\" 和 \"awesome\" 这三个词语的文本片段，同时要求这三个词语的顺序是 \"is\" 在前，\"super\" 在中间，\"awesome\" 在后，并且词语之间没有间隔（不区分大小写）。</p><p></p><p>这里需要说明的是，MATCH 与 LIKE 查询的差异在于，MATCH 查询时会忽略大小写，把句子切分成一个个词来匹配，能够更快速定位符合条件的结果，特别是在大规模数据集情况下，MATCH 的效率提升更为明显。</p><p></p><p>执行结果如下所示，开启倒排索引后查询耗时仅 0.19 秒，性能较仅开启 Ngram BloomFilter 索引时提升了 4 倍，较未开启索引时提升了近 40 倍，极大幅度提升了文本检索的效率。</p><p></p><p><code lang=\"sql\">+------------+------------------------------------------+-------------------+-------+\n| product_id | any_value(product_title)                 | rating            | count |\n+------------+------------------------------------------+-------------------+-------+\n| B00992CF6W | Minecraft                                | 4.833333333333333 |    18 |\n| B009UX2YAC | Subway Surfers                           |               4.7 |    10 |\n| B00DJFIMW6 | Minion Rush: Despicable Me Official Game |                 5 |     7 |\n| B0086700CM | Temple Run                               |                 5 |     6 |\n| B00KWVZ750 | Angry Birds Epic RPG                     |                 5 |     6 |\n+------------+------------------------------------------+-------------------+-------+\n5 rows in set (0.19 sec)\n</code></p><p></p><p>究其加速原因可知，倒排索引是通过将文本分解为单词，并建立从单词到行号列表的映射。这些映射关系按照单词进行排序，并构建跳表索引。在查询特定单词时，可以通过跳表索引和二分查找等方法，在有序的映射中快速定位到对应的行号列表，进而获取行的内容。这种查询方式避免了逐行匹配，将算法复杂度从 O（n） 降低到 O（logn），在处理大规模数据时能显著提高查询性能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e465b26d5d7accafbdf958b0cfa746ff.png\" /></p><p></p><p>为深入了解倒排索引的加速原理，需从倒排索引内部引读写逻辑说起。在 Doris 中，从逻辑角度来看，倒排索引应用于表的列级别，而从物理存储和实现角度来看，倒排索引实际是建立在数据文件级别上的。具体如下：</p><p></p><p>写入阶段： 数据在写入数据文件的同时，也将同步写入排索引文件中，对于每个写入数据的行号，均与倒排索引中的行号一一对应的。查询阶段： 如果查询 WHERE 条件中包含已建立倒排索引的列，Doris 会自动查询索引文件，返回满足条件的行号列表，再利用 Doris 通用的行号过滤机制，跳过不必要的行和页面，只读取满足条件的行，以达到查询加速的效果。</p><p></p><p>总的来说，Doris 的倒排索引机制在物理层面是通过数据文件和索引文件配合工作，而在逻辑层面则通过列和行的映射来实现高效的数据检索和查询加速。</p><p></p><h2>非主键列查询加速</h2><p></p><p>为了进一步验证倒排索引对非主键列查询加速的影响，我们选择对产品 ID 和用户 ID 的维度信息进行查询。</p><p></p><h3>未开启倒排索引</h3><p></p><p>当查询用户 13916588 对产品 B002DMK1R0 的评论信息时，执行以下 SQL 语句进行查询时，需要对全表数据进行扫描，查询耗时为 1.81 秒。</p><p></p><p><code lang=\"sql\">mysql&gt; SELECT product_title,review_headline,review_body,star_rating \nFROM amazon_reviews \nWHERE product_id='B002DMK1R0' AND customer_id=13916588;\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\n| product_title                                                   | review_headline      | review_body                                                                                                                 | star_rating |\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\n| Magellan Maestro 4700 4.7-Inch Bluetooth Portable GPS Navigator | Nice Features But... | This is a great GPS. Gets you where you are going. Don't forget to buy the seperate (grr!) cord for the traffic kit though! |           4 |\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\n1 row in set (1.81 sec)\n</code></p><p></p><h3>倒排索引查询加速</h3><p></p><p>接下来，我们为 product_id 和 customer_id 添加倒排索引。在这个场景中，倒排索引的使用与文本搜索时不同，该场景无需对 product_id 和 customer_id 进行分词，只需对这两列的 Value→RowID 的创建倒排映射表。</p><p></p><p>首先，通过执行以下 SQL 语句创建倒排索引：</p><p></p><p><code lang=\"sql\">ALTER TABLE amazon_reviews ADD INDEX product_id_inverted_idx(product_id) USING INVERTED ;\nALTER TABLE amazon_reviews ADD INDEX customer_id_inverted_idx(customer_id) USING INVERTED ;\nBUILD INDEX product_id_inverted_idx ON amazon_reviews;\nBUILD INDEX customer_id_inverted_idx ON amazon_reviews;\n</code></p><p></p><p>其次，当索引构建完成后，执行同样的查询语句，查询耗时从 1.81 秒降到了 0.06 秒，查询耗时显著降低，相比未添加索引的情况，查询效率提升了约 30 倍。</p><p></p><p><code lang=\"sql\">mysql&gt; SELECT product_title,review_headline,review_body,star_rating FROM amazon_reviews WHERE product_id='B002DMK1R0' AND customer_id='13916588';\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\n| product_title                                                   | review_headline      | review_body                                                                                                                 | star_rating |\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\n| Magellan Maestro 4700 4.7-Inch Bluetooth Portable GPS Navigator | Nice Features But... | This is a great GPS. Gets you where you are going. Don't forget to buy the seperate (grr!) cord for the traffic kit though! |           4 |\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\n1 row in set (0.06 sec)\n</code></p><p></p><p>通过观察可发现，倒排索引在于类似非主键列的维度查询中具有非常出色的加速效果。为更深入且直观的查看加速效果，可通过 Doris Profile 信息来进一步探索。</p><p></p><h3>Profile 分析</h3><p></p><p>需要注意的是，在开启查询的 Profile 之前，需先在 MySQL 客户端执行 SET enable_profile=true;  命令。完成后再执行查询语句，并访问 http://FE_IP:FE_HTTP_PORT/QueryProfile， 来查看与本次查询相关的 Profile ID 以及详细的 Profile 信息。</p><p></p><p>本文中仅截取一个特定片段的 SegmentIterator Profile 信息来说明倒排索引查询加速原因。</p><p></p><p><code lang=\"yaml\">SegmentIterator:\n  - FirstReadSeekCount: 0\n  - FirstReadSeekTime: 0ns\n  - FirstReadTime: 13.119ms\n  - IOTimer: 19.537ms\n  - InvertedIndexQueryTime: 11.583ms\n  - RawRowsRead: 1\n  - RowsConditionsFiltered: 0\n  - RowsInvertedIndexFiltered: 16.907403M (16907403)\n  - RowsShortCircuitPredInput: 0\n  - RowsVectorPredFiltered: 0\n  - RowsVectorPredInput: 0\n  - ShortPredEvalTime: 0ns\n  - TotalPagesNum: 27\n  - UncompressedBytesRead: 3.71 MB\n  - VectorPredEvalTime: 0ns\n</code></p><p></p><p>从上述 Profile 中的 RowsInvertedIndexFiltered: 16.907403M (16907403)以及RawRowsRead: 1，我们可以观察到：倒排索引过滤了 16907403 行数据，最终只保留 1 行数据（即命中的那条数据）。根据 FirstReadTime: 13.119ms 可知，在读取这行数据所在的页面（page）耗时 13.119 ms，而根据InvertedIndexQueryTime: 11.583ms 可知，倒排索引执行时间仅耗时 11.58 ms。这意味着倒排索引仅在 11.58 ms 内过滤了 16907403 行数据，执行效率非常高。</p><p></p><p>为更直接对比，接下来展示未增加倒排索引情况下 SegmentIterator 的执行情况：</p><p></p><p><code lang=\"yaml\">SegmentIterator:\n  - FirstReadSeekCount: 9.374K (9374)\n  - FirstReadSeekTime: 400.522ms\n  - FirstReadTime: 3s144ms\n  - IOTimer: 2s564ms\n  - InvertedIndexQueryTime: 0ns\n  - RawRowsRead: 16.680706M (16680706)\n  - RowsConditionsFiltered: 226.698K (226698)\n  - RowsInvertedIndexFiltered: 0\n  - RowsShortCircuitPredInput: 1\n  - RowsVectorPredFiltered: 16.680705M (16680705)\n  - RowsVectorPredInput: 16.680706M (16680706)\n  - RowsZonemapFiltered: 226.698K (226698)\n  - ShortPredEvalTime: 2.723ms\n  - TotalPagesNum: 5.421K (5421)\n  - UncompressedBytesRead: 277.05 MB\n  - VectorPredEvalTime: 8.114ms\n</code></p><p></p><p>根据上述 Profile 观察可知，由于没有索引进行过滤， FirstRead 需要花费 3.14s 的时间来加载 16680706 行数据，然后使用 Predicate Evaluate 进行条件过滤，过滤掉其中 16680705 行，而条件过滤本身只消耗了不到 10ms 的时间，由此可见，大部分时间被消耗在加载原始数据上。</p><p></p><p>通过对比可知，建立倒排索引可以大大减少加载原始数据的时间，提高查询的执行效率。索引能够快速定位满足条件的行，从而减少不必要的数据加载和处理，节省时间和资源。</p><p></p><h2>低基数文本列索引加速</h2><p></p><p>众所周知，倒排索引对于高基数文本列的查询来说，加速效果十分显著。然而，在低基数列的情况下，可能由于需创建过多的索引项而导致更大的开销，从而对查询性能产生负面影响。接下来，我们将以 product_category 作为谓词列进行过滤，来检验 Apache Doris 倒排索引在低基数文本列的加速效果如何。</p><p></p><p><code lang=\"sql\">mysql&gt; SELECT COUNT(DISTINCT product_category) FROM amazon_reviews ;\n+----------------------------------+\n| count(DISTINCT product_category) |\n+----------------------------------+\n|                               43 |\n+----------------------------------+\n1 row in set (0.57 sec)\n</code></p><p></p><p>通过上述操作可知，到 product_category 仅有 43 种分类，是一个典型的低基数文本列。接下来，我们对其增加倒排索引</p><p></p><p><code lang=\"sql\">ALTER TABLE amazon_reviews ADD INDEX product_category_inverted_idx(`product_category`) USING INVERTED;\nBUILD INDEX product_category_inverted_idx ON amazon_reviews;\n</code></p><p></p><p>添加倒排索引之后，运行如下 SQL 查询，指查询产品分类为 Mobile_Electronics 产品中评价数量最多的前三名产品信息</p><p></p><p><code lang=\"sql\">SELECT \n    product_id,\n    product_title,\n    AVG(star_rating) AS rating,\n    any(review_body),\n    any(review_headline),\n    COUNT(*) AS count \nFROM \n    amazon_reviews \nWHERE \n    product_category = 'Mobile_Electronics' \nGROUP BY \n    product_title, product_id \nORDER BY \n    count DESC \nLIMIT 10;\n</code></p><p></p><p>从下方结果可知，增加倒排索引之后，查询耗时为 1.54s。</p><p></p><p><code lang=\"sql\">+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-------+\n| product_id | product_title                                                                                                                                                                                          | rating             | any_value(review_body)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | any_value(review_headline)      | count |\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-------+\n| B00J46XO9U | iXCC Lightning Cable 3ft, iPhone charger, for iPhone X, 8, 8 Plus, 7, 7 Plus, 6s, 6s Plus, 6, 6 Plus, SE 5s 5c 5, iPad Air 2 Pro, iPad mini 2 3 4, iPad 4th Gen [Apple MFi Certified](Black and White) | 4.3766233766233764 | Great cable and works well. Exact fit as Apple cable. I would recommend this to anyone who is looking to save money and for a quality cable.                                                                                                                                                                                                                                                                                                                                                             | Apple certified lightning cable |  1078 |\n| B004911E9M | Wall AC Charger USB Sync Data Cable for iPhone 4, 3GS, and iPod                                                                                                                                        | 2.4281805745554035 | A total waste of money for me because I needed it for a iPhone 4.  The plug will only go in upside down and thus won't work at all.                                                                                                                                                                                                                                                                                                                                                                      | Won't work with a iPhone 4!     |   731 |\n| B002D4IHYM | New Trent Easypak 7000mAh Portable Triple USB Port External Battery Charger/Power Pack for Smartphones, Tablets and more (w/built-in USB cable)                                                        | 4.5216095380029806 | I bought this product based on the reviews that i read and i am very glad that i did. I did have a problem with the product charging my itouch after i received it but i emailed the company and they corrected the problem immediately. VERY GOOD customer service, very prompt. The product itself is very good. It charges my power hungry itouch very quickly and the imax battery power lasts for a long time. All in all a very good purchase that i would recommend to anyone who owns an itouch. | Great product &amp; company         |   671 |\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-------+\n3 rows in set (1.54 sec)\n</code></p><p></p><p>接下来，我们关闭倒排索引，以观察未加倒排索引时的查询耗时。这里需要说明的是，当需要关闭索引或在增加索引后发现效果不理想，可以在 MySQL 客户端中执行 set enable_inverted_index_query=false;，便捷且快速地临时关闭倒排索引。我们再次运行查询 SQL，如下所示，查询耗时为 1.8s。</p><p></p><p><code lang=\"sql\">+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+-------+\n| product_id | product_title                                                                                                                                                                                          | rating             | any_value(review_body)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | any_value(review_headline)            | count |\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+-------+\n| B00J46XO9U | iXCC Lightning Cable 3ft, iPhone charger, for iPhone X, 8, 8 Plus, 7, 7 Plus, 6s, 6s Plus, 6, 6 Plus, SE 5s 5c 5, iPad Air 2 Pro, iPad mini 2 3 4, iPad 4th Gen [Apple MFi Certified](Black and White) | 4.3766233766233764 | These cables are great. They feel quality, and best of all, they work as they should. I have no issues with them whatsoever and will be buying more when needed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Just like the original from Apple     |  1078 |\n| B004911E9M | Wall AC Charger USB Sync Data Cable for iPhone 4, 3GS, and iPod                                                                                                                                        | 2.4281805745554035 | I ordered two of these chargers for an Iphone 4. Then I started experiencing weird behavior from the touch screen. It would select the wrong area of the screen, or it would refuse to scroll beyond a certain point and jump back up to the top of the page. This behavior occurs whenever either of the two that I bought are attached and charging. When I remove them, it works fine once again. Needless to say, these items are being returned.                                                                                                                                                                                                                                                                                                                                                                              | Beware - these chargers are defective |   731 |\n| B002D4IHYM | New Trent Easypak 7000mAh Portable Triple USB Port External Battery Charger/Power Pack for Smartphones, Tablets and more (w/built-in USB cable)                                                        | 4.5216095380029806 | I received this in the mail 4 days ago, and after charging it for 6 hours, I've been using it as the sole source for recharging my 3Gs to see how long it would work.  I use my Iphone A LOT every day and usually by the time I get home it's down to 50% or less.  After 4 days of using the IMAX to recharge my Iphone, it finally went from 3 bars to 4 this afternoon when I plugged my iphone in.  It charges the iphone very quickly, and I've been topping my phone off (stopping around 95% or so) twice a day.  This is a great product and the size is very similar to a deck of cards (not like an iphone that someone else posted) and is very easy to carry in a jacket pocket or back pack.  I bought this for a 4 day music festival I'm going to, and I have no worries at all of my iphone running out of juice! | FANTASTIC product!                    |   671 |\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+-------+\n3 rows in set (1.80 sec)\n</code></p><p></p><p>综上可知，倒排索引对于低基数列场景也有 15% 的查询性能提升，虽不如高基数列场景的提升效果，但并未产生退化效果或负面影响。此外，Apache Doris 针对低基数列采用了较好的编码（如字典编码）方式和压缩技术，并且可以通过内置索引（如 zonemap）进行有效过滤。因此，即使不添加倒排索引仍能展现较好的查询效果。</p><p></p><h2>总结语</h2><p></p><p>总而言之，Apache Doris 中的倒排索引显著优化了针对谓词列的过滤操作，即 SQL 查询中的 Where 子句。通过精确匹配行号，减少了存储层需要扫描的数据量，从而提高了查询性能。即使在性能提升有限的情况下，倒排索引也不会对查询效率产生负面影响。此外，倒排索引还支持轻量级的索引管理操作，如对增加或删除索引（ADD/DROP INDEX）以及构建索引（BUILD INDEX）操作进行管理。同时，还提供了在 MySQL 客户端便捷地启用或关闭索引（enable_inverted_index_query=true/false）的功能，使用户能够轻松利用倒排索引来检验查询加速效果。</p><p></p><p>倒排索引和 NGram Bloom Filter 索引为不同场景提供了查询加速方案，在选择索引类型时，数据集的特定特征和查询模式是关键考虑因素。以下是一些常见的适配场景：</p><p></p><p>大规模数据非主键列点查场景： 在这种场景下，往往存在大量分散的数值列在值，且查询的值命中量很低。为了加速查询，除了在建表时利用 Doris 内置的智能索引能力之外，还可以通过给对应的列增加倒排索引来加速查询。倒排索引对字符类型、数值类型、日期等标量类型支持比较完整。短文本列的文本检索场景： 如果短文本分布比较离散（即文本之间相似度低），则适合使用 Ngram Bloom Filter 索引，能够有效地处理短文本的模糊匹配查询（LIKE）。同时，在短文本场景下 Apache Doris 的向量化处理能力可以得到更加充分和高效的应用和发挥。如果短文本分布比较集中（如大量文本相似，少量文本不同），则适合使用倒排分词索引，这样可以保证词典比较小，适合快速检索获取行号列表。长文本列的文本搜索场景： 针对长文本列，倒排分词索引是更好的方案。相比于暴力字符串匹配，倒排索引提供了更高效的查询性能，避免了大量的 CPU 资源消耗。</p><p></p><p>自 Apache Doris 最早引入倒排索引至今已有近一年时间，从 早期 2.0 Preview 版本至最近发布的 2.0.4，这一年间经历了大量开源用户在真实业务环境海量数据下的打磨和验证，性能与稳定性已经得到充分验证。而在后续的规划中，我们也将持续在现有基础上进行迭代和优化，包括：</p><p></p><p>自定义倒排索引分词能力， 针对用户在不同场景下分词效果的需求，提供用户对自定义分词器。支持更多类型的倒排索引， 后续会增加对 Array、Map 等复杂数据类型的支持，以更全面地满足各类查询需求。</p>",
    "publish_time": "2024-01-30 14:30:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "不敢把数据库运行在 K8s 上？容器化对数据库性能有影响吗？",
    "url": "https://www.infoq.cn/article/Sh2TJYW1dKI4ZqpakUJJ",
    "summary": "<p>引言：</p><p>容器化是一种将应用程序及其依赖项打包到一个独立、可移植的运行环境中的技术。容器化技术通过使用容器运行时引擎（比如 Docker/Containerd）来创建、部署和管理容器。Kubernetes（通常简称为 K8s）是一个开源的容器编排和管理平台，它提供了一个集中式的、可伸缩的平台来自动化容器的部署、扩展、管理和调度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/30/303f54f14c1be9c80302bb39a78c89c6.png\" /></p><p>Fig. 1. Usage of containerized workloads by category[4]</p><p></p><p>数据库容器化的趋势已经非常明显，如图.1 所示，数据库 + 分析类的 workload 已经占据了半壁江山，但是依然有很多人在做技术选型时面临一个难题：容器化是否对数据库性能有影响？如果有，影响的因素是什么？如何面对容器化带来的性能甚至是稳定性的问题？</p><p></p><p></p><h2>容器化优势和技术原理：</h2><p></p><p></p><p></p><h3>容器化的优势</h3><p></p><p></p><p>灵活性和可移植性：容器化技术提供了灵活性和可移植性的优势，使得数据库的部署和迁移变得更加简单和可靠。</p><p></p><p>资源隔离和可扩展性：容器化技术通过使用容器运行时引擎提供了资源隔离和可扩展性的优势。每个容器都有自己的运行时环境和资源分配，因此数据库实例可以在容器中独立运行，相互之间影响降到最低。这种资源隔离使得数据库实例能够更好地利用计算资源，并提供更好的性能和可靠性。</p><p></p><p>更友好的调度策略：由于容器化后的资源粒度更小，对上层调度更为友好，可以在不同场景应用不同的调度策略，比如通过离线和在线混合部署来错峰使用计算资源，通过提升部署密度来降低计算成本。</p><p></p><p></p><h3>容器化技术原理和分类</h3><p></p><p></p><p></p><h4>虚拟化：</h4><p></p><p></p><p>说到容器，那就不得不提虚拟化，虚拟化是一种将计算资源进行抽象和隔离的技术，使得多个虚拟实例可以在同一物理服务器上同时运行。它通过在硬件和操作系统之间引入虚拟机监视器（Hypervisor）的软件层，将物理服务器分割为多个虚拟机，并为每个虚拟机提供独立的操作系统和资源。每个虚拟机都可以运行完整的操作系统，并具有独立的内核和资源，类似于在物理服务器上运行一个完整的计算机。</p><p></p><p>容器化是一种更为轻量的虚拟化技术，它使用操作系统级别的虚拟化来隔离和运行应用程序及其依赖的环境。容器化和虚拟化一般搭配使用，以满足用户对不同隔离场景的需求。</p><p></p><p></p><h4>虚拟化 + 容器化分类：</h4><p></p><p></p><p>根据容器运行时的资源隔离和虚拟化方式，可以将目前的主流虚拟化 + 容器技术分为这么几类：</p><p></p><p>标准容器，符合 OCI （Open Container Initiative）规范，如 docker/containerd，容器运行时为 runc，这是目前 k8s workload 的主要形态</p><p></p><p>用户态内核容器，如 gVisor，也符合 OCI 规范，容器运行时为 runsc，有比较好的隔离性和安全性，但是性能比较差，适合比较轻量的 workload</p><p></p><p>微内核容器，使用了 hypervisor，如 Firecracker、Kata-Container，也符合 OCI 规范，容器运行时为 runc 或 runv，有比较好的安全性和隔离性，性能介于标准容器和用户态内核容器之间</p><p></p><p>纯虚拟机，如 KVM、Xen、VMWare，是主流云厂商服务器的底层虚拟化技术，一般作为 k8s 中的 Node 存在，比容器要更低一个层次</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/65/65b5ce958c9764e652b22dab429f8c08.png\" /></p><p></p><p>Fig. 2. Comparison of system architecture of various lightweight virtualization methods. Orange parts are kernel space, while green parts are user space.[2]</p><p></p><p></p><h4>OCI 主流容器技术实现：</h4><p></p><p></p><p>下面我们对符合 OCI 规范的几款主流容器化技术做一下分析</p><p></p><p>1. runc:</p><p></p><p>runc 是一个符合 OCI 标准的容器运行时，它是 Docker/Containerd 核心容器引擎的一部分。它使用 Linux 的命名空间（Namespace）和控制组（Cgroup）技术来实现容器的隔离。</p><p></p><p>在运行容器时，runc 使用命名空间隔离容器的进程、网络、文件系统和 IPC（进程间通信）。它还使用控制组来限制容器内进程的资源使用。这种隔离技术使得容器内的应用程序可以在一个相对独立的环境中运行，与宿主机和其他容器隔离开来。</p><p></p><p>runc 的隔离技术虽然引入了一定开销，但是这种开销仅限于命名空间映射、限制检查和一些记账逻辑，理论上影响很小，而且当 syscall 是长耗时操作时，这种影响几乎可以忽略不计，一般情况下，基于 Namespace+Cgroup 的隔离技术对 CPU、内存、I/O 性能的影响较小。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6f/6f4487e8baacf0cd06201292a70315b8.png\" /></p><p></p><p>Fig. 3. Runc Architecture</p><p></p><p>Kata Containers:</p><p></p><p>Kata Containers 是一个使用虚拟机技术实现的容器运行时，它提供了更高的隔离性和安全性。Kata Containers 使用了 Intel 的 Clear Containers 技术，并结合了轻量级虚拟机监控器和容器运行时。</p><p></p><p>Kata Containers 在每个容器内运行一个独立的虚拟机，每个虚拟机都有自己的内核和用户空间。这种虚拟化技术能够提供更严格的隔离，使得容器内的应用程序无法直接访问宿主机的资源。然而，由于引入了虚拟机的启动和管理开销，相对于传统的容器运行时，Kata Containers 在系统调用和 I/O 性能方面可能会有一些额外的开销。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0d/0d65d1b899e46f1a4edab75da77e6417.png\" /></p><p></p><p>Fig. 4. Kata Containers Architecture</p><p></p><p>gVisor:</p><p></p><p>gVisor 是一个使用用户态虚拟化技术实现的容器运行时，它提供了更高的隔离性和安全性。gVisor 使用了自己的内核实现，在容器内部运行。</p><p></p><p>gVisor 的内核实现，称为 “Sandboxed Kernel”，在容器内部提供对操作系统接口的模拟和管理。容器内的应用程序和进程与宿主内核隔离开来，无法直接访问或影响宿主内核的资源。这种隔离技术在提高安全性的同时，相对于传统的容器运行时，可能会引入一些额外的系统调用和 I/O 性能开销。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3e/3e919f61b086b1da9b03e7e6931a2ee7.png\" /></p><p></p><p>Fig. 5. gVisor Architecture</p><p></p><p>Firecracker:</p><p></p><p>Firecracker 是一种针对无服务器计算和轻量级工作负载设计的虚拟化技术。它使用了微虚拟化技术，将每个容器作为一个独立的虚拟机运行。</p><p></p><p>Firecracker 使用 KVM（Kernel-based Virtual Machine）技术作为底层虚拟化技术。每个容器都在自己的虚拟机中运行，拥有独立的内核和根文件系统，并使用独立的虚拟设备模拟器与宿主机通信。这种隔离技术提供了较高的安全性和隔离性，但相对于传统的容器运行时，Firecracker 可能会引入更大的系统调用和 I/O 性能开销。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b8/b82116bf27d4505a55a903e9bf494ad8.png\" /></p><p></p><p>Fig. 6. Firecracker Architecture</p><p></p><p></p><h4>实现原理对比：</h4><p></p><p></p><p>Table. 1: Overview of implementations of virtualization and isolation in Containerization</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/aa/aa2e210806528efe631f1637190cfe15.png\" /></p><p></p><p>还有人对 Container Engine 的不同实现做了对比，比如 Containerd 和 CRI-O [3][5]，这个对比也不在本文讨论范围内，留给感兴趣的读者自己去了解。</p><p></p><p></p><h2>k8s+ 容器化对数据库的影响：</h2><p></p><p></p><p>容器化对数据库有很多正面的影响：比如容器化可以简化数据库的部署和管理、为数据库提供标准的隔离运行环境、可以让数据库在不同的复杂环境中轻松部署和灵活迁移、对数据库的版本管理也更加规范和方便。而且在 k8s 的加持下，数据库中的多种角色和组件可以被灵活有机地编排在一起。</p><p></p><p></p><h3>容器化对数据库的挑战：</h3><p></p><p></p><p>但是，k8s+ 容器化对数据库也带来了很多挑战，这和数据库本身的特点也有很大关系，与普通的无状态应用相比，数据库有如下特点：</p><p></p><p>数据库是一个有多种角色的复杂应用：一个完整的数据库有多种不同的角色，比如 MySQL 主备形态中，同样是两个 MySQL 容器，一个是主，一个是备，角色并不对等，这种不对等的关系需要被正确表达，而且在创建、重启、删除、备份、高可用等各种运维操作中都要被正确管理，本质上这是一种容器之间对于数据状态的互相依赖，对于这种依赖目前的容器和 k8s 都没有很好地抽象与解决。数据库对数据的持久性和一致性有很高的需求：数据库对存储有很高的需求，简单的容器化并不能满足一个生产级别的 workload，还需要配套的 CSI 和 PersistentVolume，对存储的选型也影响着数据库可选的操作选项，比如云盘有很高的 durability，提供 snapshot 备份功能，并能在不同的计算节点上 attach 和 detach，对数据库的备份恢复和高可用操作非常友好；但在本地盘上，这种选择就会更窄一些，比如 node 宕机时我们可能就会永远失去一个数据副本，高可用操作处理起来会更有挑战，备份操作也只能选择物理 / 文件（physical）备份或逻辑（logical）的方式。不同的存储方案对应着不同的持久化能力和不一样的数据库架构。数据库对性能也有很高的需求：数据库对性能的需求比较多样，如果从 CPU、内存、网络、存储这几个方面来进行划分，有 CPU + 存储 I/O 密集型，如 OLAP 产品 ClickHouse、Greenplum 等；有内存 + 网络 I/O 密集型，如 Redis 和内存数据库；有 CPU + 存储 I/O 密集型，如 MySQL、Postgresql 等传统 OLTP 数据库。而且根据查询场景不同，即使是同一个数据库进程在不同的 SQL 中对资源的需求也大相径庭。数据库对安全性的要求：数据库中的数据一般都比较核心和敏感，因此对运行环境隔离、数据访问控制、日志审计都有一定的规范化要求。</p><p></p><p>总而言之，将数据库跑在容器 +k8s 上，对数据库是一种很大的挑战，比如数据库要去适应生命周期短暂的容器、浮动的 IP、频繁更新的基础设置、复杂的性能环境；对容器 +k8s 也是很大的挑战，比如角色的引入、容器之间的状态和数据依赖、对性能的苛刻需求、对完整安全体系的合规要求。</p><p></p><p>关于上面提到的 1,2,4 这三点，在我们开发的 KubeBlocks 项目中已经有比较成体系的解决方案，感兴趣的可以去围观 kubeblocks.io。回到本文的主题，在下面的部分会对容器化对数据库性能的影响做进一步的深入分析。</p><p></p><p></p><h3>k8s+ 容器化对数据库性能的影响</h3><p></p><p></p><p>如上所述，数据的性能主要受 CPU、内存、存储、网络 这几个因素的影响，我们将围绕这几个方面分析 k8s 和容器化对数据库性能的影响，虽然 k8s 中的一些调度和亲和性策略也会对性能有潜在影响，但这些策略和容器化无关，因此并不在本次讨论范围内。</p><p></p><p>我们从上述几个维度来综述一下容器化对应用（包括数据库）性能的影响，在本次综述中我们整理了业界近几年的一些论文和测试数据，会对其中的一些测试数据和偏移项进行分析，指出其中的原因与不合理之处，针对一些缺乏的场景我们补了一些测试，比如 k8s CNI 对网络性能的影响。</p><p></p><p></p><h4>CPU：</h4><p></p><p></p><p>测试服务器：Quad-Core Hyper Thread 4 Intel Core i5-7500 8GB RAM，1TB disk，Ubuntu 18.04 LTS</p><p></p><p>测试场景：这里的数据和测试场景来自论文 [1]，case1 是用 sysbench 4 并发做质数计算，最终汇报每秒发生的 events，属于纯计算场景，几乎所有指令都跑在 user space， syscall 调用可忽略，所以理论上几种容器技术的表现会差不多</p><p></p><p>测试结果：几种容器的 CPU 表现差不多，相比裸金属，其他场景性能下降都在 4% 左右</p><p></p><p>分析：这个 4% 的下降应该是 Cgroup 对 CPU 的限制所造成，当 Sysbench 并发等于 Hyper Thread 数量时，被 Cgroup Thottle 的概率非常高，当有 Cgroup Throttle 发生时，进程会被强制等待一个 jiffy（10ms），Cgroup 对资源的计算周期是以 jiffy 为粒度，不是以秒为粒度，所以 4 vCPU 的容器几乎不可能跑到 400%，有一定 loss 是正常的，一般被 Throttle 的次数可以从 Cgroup 的 cpu.stat 文件中查找到</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/95/95e58ce471c91ba926da56c88fd219d8.png\" /></p><p></p><p>Fig. 7. CPU performance (Sysbench benchmark) (Xingyu Wang 2022)</p><p></p><p>测试场景：Davi1d 视频解码，视频大小在几百兆左右，在该测试中，由于需要读取磁盘上的数据，所以会有大量的 syscall 调用，syscall 调用会对应用性能造成一部分影响</p><p></p><p>测试结果：runc 和 kata-qemu 损失 4% 左右，和质数测试结果类似；gVisor-ptrace 损失有 13%，而 gVisor-KVM 能和裸金属持平</p><p></p><p>分析：视频解码属于顺序读取，顺序读取在 linux 中会有 read ahead 优化，所以绝大部分 I/O 都是直接从 page cache 中读取数据，runc 主要还是受 Cgroup 影响，其他三个方案主要受 syscall 实现方案的影响，论文中并没有就 gVisor-ptrace 和 gVisor-KVM 差别做进一步分析，gVisor 使用了 gofer 做文件系统，gofer 还有自己一些的 cache 策略，进一步的分析可能要从 gVisor syscall 和 cache 策略入手</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/42/425f14a7aadd1a9c58aef70081b97452.png\" /></p><p></p><p>Fig. 8. CPU performance (Dav1d benchmark) (Xingyu Wang 2022)</p><p></p><p></p><h4>Memory：</h4><p></p><p></p><p>测试场景：RAMSpeed，有 4 个子场景（Copy，Scale，Add，Triad），具体原理就不展开了</p><p></p><p>测试结果：几种方案都差不多</p><p></p><p>分析：当内存分配好并做好缺页中断（page fault）后，理论上容器化对内存访问没有影响，真正影响内存性能的是 mmap 和 brk 之类的 syscall 调用，但是这个测试中，此类 syscall 占比极小</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7f/7f5d6e29969ce22115aa9684ca342027.png\" /></p><p></p><p>Fig. 9. Memory access performance (Xingyu Wang 2022)</p><p></p><p>测试场景：Redis-Benchmark，测试了子场景 GET, SET, LPUSH, LPOP, SADD</p><p></p><p>测试结果：runc 和 kata-qemu 影响很小，gVisor 受影响很大，gVisor-ptrace 损失在 95% 左右，gVisor-KVM 损失在 56% 左右</p><p></p><p>分析：redis 是单线程重网络 I/O 的一种应用，网络 I/O 都是通过 syscall 进行，所以 gVisor 会有很大的性能损失，原论文中认为损失主要是由内存分配引起，这应该是一种误解，Redis 内部使用用户态内存管理工具 jemalloc，jemalloc 会通过调用 mmap syscall 来向 OS 批发大块内存，然后再做本地小块分配，由于 jemalloc 有比较成熟的内存分配和缓存机制，所以调用 mmap 的几率会很小。当 redis 满载时，网络 I/O 消耗的 CPU (CPU sys) 在 70% 左右，所以这里 gVisor 的性能损耗主要由 syscall 劫持和内部的网络栈 netstack 引起，这个测试也说明 gVisor 目前并不适合重网络 I/O 的场景</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b6/b68e0d420b3c4ae5efd309d9fbc0d16d.png\" /></p><p></p><p>Fig. 10. Redis performance for different container runtimes (Xingyu Wang 2022)</p><p></p><p></p><h4>Disk I/O:</h4><p></p><p></p><p>测试场景：IOZone 读写 16GB 文件</p><p></p><p>测试结果：顺序读写影响不大，kata-qemu 受影响较大，影响范围为 12-16%</p><p></p><p>分析：大块读写其实就是顺序读写，如前所述，顺序读 OS 有 read ahead，顺序读写其实操作的大部分是 page cache，原论文对 kata-qemu 做了分析，认为和 virtio-9p 文件系统有关，virtio-9p 针对网络设计，对虚拟化并没有做针对优化</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/29/29060bacff8671e96bfa6772569884f9.png\" /></p><p></p><p>Fig. 11. Disk read and write performance (Xingyu Wang 2022)</p><p></p><p>测试场景：直接基于 tmpfs（shared memory）做测试，单纯用来衡量 syscall+ 内存 copy 对性能的影响</p><p></p><p>测试结果：除了 gVisor，其他都差不多</p><p></p><p>分析：gVisor syscall 成本比较高，结果和 redis-benchmark 类似</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/27/2787b11479e8d7c725caa69f2cc71cc6.png\" /></p><p></p><p>Fig. 12. Disk read and write performance (tmpfs overlay) (Xingyu Wang 2022)</p><p></p><p>测试场景：SQLite 单线程插入测试，耗时越少越好</p><p></p><p>测试结果：runc 和裸金属相近，kata 耗时多出 17%，gVisor 耗时多出 125%</p><p></p><p>分析：数据库 workload 比较复杂，是 CPU、内存、网络、Disk I/O 的综合影响，对 syscall 的调用非常频繁，gVisor 不是很适合此类场景</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cc/ccbecbfe2bafcd47f358ca0a259e07c3.png\" /></p><p></p><p>Fig. 13. Database record insertion performance (Xingyu Wang 2022)</p><p></p><p></p><h4>Network I/O：</h4><p></p><p></p><p>测试场景：TCP stream 吞吐测试，throughtput 越多越好</p><p></p><p>测试结果：gVisor 网络性能比较差，和在 redis-benchmark 中看到的类似，其他几个影响不大</p><p></p><p>分析：gVisor 受限于 syscall 机制和 netstack 实现，整体吞吐较差</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3a/3ab3b0cb73106f21b0a38965d91f526e.png\" /></p><p></p><p>Fig. 14. TCP_STREAM network performance (Xingyu Wang 2022)</p><p></p><p>测试场景：测试 TCP_RR, TCP_CRR, UDP_RR，RR 是 request &amp; response 的缩写，指的是一个 TCP 请求来回，TCP 链接只建立一次，后续复用，CRR 是指每次测试都创建一条新的 TCP 链接，TCP_RR 对应着长链接的场景，TCP_CRR 对应着短链接的场景</p><p></p><p>测试结果：runc 和裸金属相近，kata 有较小的损失，gVisor 损失很大，原理同上</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/78/7852a52700931a1225a3baf8594f0410.png\" /></p><p></p><p>Fig. 15. TCP_RR, TCP_CRR and UDP_RR performance (Xingyu Wang 2022)</p><p></p><p></p><h4>CNI Network：</h4><p></p><p></p><p>容器一般搭配 k8s 使用，基于 k8s 的容器编排已经是事实上的标准，在 k8s 环境中，网络一般由 CNI + 容器技术共同实现，常见的 CNI 有很多，比较主流的有 Calico, Flannel, Cilium…，在最新的版本中 Calico 和 Cilium 都大量使用了 eBPF 的技术，虽然具体实现不同，但是这两个 CNI 在很多测试场景中性能表现相当，具体的测试数据见 [6]。</p><p></p><p>在接下来的测试中，我们选取 Cilium eBPF legacy host-routing 和 Cilium eBPF 两种模式做对比，测试 CNI 对数据库性能的具体影响。</p><p></p><p>legacy host-routing：</p><p></p><p>在传统主机路由模式（legacy host-routing）下，Cilium 使用 iptables 来进行包过滤和转发，iptables 仍然是必需的，并且用于配置和管理网络流量的转发规则，Cilium 通过 iptables 规则将流量引导到 Cilium 代理，然后由代理进行处理和转发。</p><p></p><p>在传统主机路由模式下，Cilium 会利用 iptables 的 NAT 功能来修改源 IP 地址和目标 IP 地址，以实现网络地址转换（NAT）和服务负载均衡。</p><p></p><p>eBPF-based host-routing：</p><p></p><p>在新的 eBPF-based 路由模式下，Cilium 不再依赖 iptables，它使用 Linux 内核的扩展 BPF（eBPF）功能来进行包过滤和转发。eBPF 主机路由允许绕过主机命名空间中的所有 iptables 和上层栈开销，以及在遍历虚拟网卡时的一部分上下文切换开销。网络数据包尽早从面向网络的网络设备中捕获，并直接传递到 Kubernetes Pod 的网络命名空间中。在出口方面，数据包仍然通过 veth pair 进行遍历，被 eBPF 捕获并直接传递到外部面向网络接口。路由表直接由 eBPF 查询，因此此优化完全透明，并与系统上运行的任何其他提供路由分发的服务兼容。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/91/91dd0e4569a08ad21c7065144cd6bdd4.png\" /></p><p></p><p>Fig. 16. Comparison of legacy and eBPF container networking [6]</p><p></p><p>测试环境：</p><p></p><p>Kubernetes: v1.25.6 CNI: cilium:v1.12.14</p><p></p><p>Node CPU: Intel® Xeon® CPU E5-2680 v4 @ 2.40GHz RAM 128G</p><p></p><p>Redis: 7.0.6, 2 vCPU, Maxmemory: 2Gi</p><p></p><p>测试场景：</p><p></p><p>Table. 2. Overview of different service routing paths in k8s</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cd/cd555192ff6cc36fa2127f5a673331ca.png\" /></p><p></p><p>点击图片可查看完整电子表格</p><p></p><p>测试结果：</p><p></p><p>Legacy host-routing with iptables:</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6d/6d321241cd77bb934033d6ab144deeaa.png\" /></p><p></p><p>Fig. 17. Redis benchmark under legacy host-routing with iptables</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3c/3cfe8dc36e545e7d23405246d6e8c999.png\" /></p><p></p><p>Fig. 18. Comparison between Host network and Pod network under legacy host-routing</p><p></p><p>eBPF-based host-routing</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6c/6c123fa6edce2f62fb48d279576f9bb6.png\" /></p><p></p><p>Fig. 19. Redis benchmark under eBPF-based host-routing</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/27/279a346e3afad9f751055c10ba7571bf.png\" /></p><p></p><p>Fig. 20. Comparison between Host network and Pod network under eBPF-based host-routing</p><p></p><p>分析：legacy host-routing 对网络性能的影响较大，Pod 网络和 host 网络性能能差出 40%，eBPF-base host-routing 基本能将 Pod 网络延迟和 host 网络打平。eBPF-based host-routing 能将延迟做到和路由规则数量无关，并彻底消除 host 网络和 Pod 网络之间的差距，是一种普适性的提升，当然也非常适合 redis 之类的重网络 I/O 应用。</p><p></p><p></p><h3>总结：</h3><p></p><p></p><p>在 CPU、内存和 Disk I/O 几个维度，runc 的性能最接近 bare metal，kata-containers 性能略低于 runc，但是在安全性和隔离性上有更好的表现，gVisor 由于受 syscall 实现的影响，性能表现最差，这可能和 gVisor 更关注安全特性有关，不过 gVisor 新版本也在一直不断提升性能。</p><p></p><p>网络比较特殊，因为还需要考虑 k8s CNI 的影响，在 Cilium eBPF + runc 的组合测试中，容器网络能够做到和 Host 网络一样的性能，Cilium 也支持 kata-containers，不过对其他容器技术的支持较少。</p><p></p><p>总体而言，runc 在各个层面都能达到和裸金属相当的性能表现，也是目前 k8s workload 最常用的选择；kata-containers 性能略低于 runc，但是有比较好的隔离性，是性能和安全都能兼顾的一种选择；gVisor 有比较灵活的隔离性，但是性能还比较差，比较适合对安全性要求很高，对性能要求不那么苛刻的场景；Firecracker 的应用场景和 kata-containers 比较类似。</p><p></p><p>所以，如果是跑数据库的 workload，优先推荐 runc 和 kata-containers。</p><p></p><p></p><h3>常见的数据库性能问题：</h3><p></p><p></p><p>很多人常常受数据库性能问题的困扰，在此，我们对常见的数据库性能问题场景做了总结和原理分析，也可以让大家一窥数据库和基础设施的复杂性以及我们努力的方向。</p><p></p><p></p><h4>Disk IO hang：</h4><p></p><p></p><p>当有大量 BufferedIO 下发时，比如 MySQL 写外排临时文件的场景，写的是 page cache，并会频繁更新 Ext4 文件系统的元数据，此时 CPU 和 I/O 可能都会很忙，MySQL 进程会被频繁 CPU Throttle，脏页持续增多，然后触发文件系统 flush 脏页，大量刷脏 I/O 占满硬件通道，如果进程被 CPU Throttle 调度走时又恰巧持有了 Ext4 Journal Lock，那么其他共享该 Ext4 文件系统的进程都被挂起，当被挂起的次数和时间足够久，就会造成 IO hang，这种现象常见于共享本地盘的场景，如 bare metal 和 hostpath CSI。主流的解决方案就是给 BufferedIO 限流，Cgroup V2 已支持该功能。</p><p></p><p>通过这个例子也可以看出，有时候瓶颈并不是某个单项因素所决定的，是由多个关联的因素联动产生，在 Disk IO hang 中，page cache 和内存、Disk I/O 相关，CPU Throttle 和 CPU 调度相关，Ext4 Journal 又和 Lock 有关，所以这些因素共同作用、互相影响才形成了一个完整的 IO hang。</p><p></p><p>值得一提的是，为了优化 I/O 操作，很多数据库厂商都推荐将 XFS 作为文件系统的第一选择。对于 Disk I/O 对数据库的深度影响，可以参考《PosgreSQL@k8s 性能优化记》[6]。</p><p></p><p></p><h4>Out Of Memory (OOM):</h4><p></p><p></p><p>当使用 Cgroup 对内存进行隔离后，OS 的内存管理路径会和 bare metal 变得不同，在内存分配（page allocation）和内存回收（page reclaim）上面临的压力会比 bare metal 更高。</p><p></p><p>比如有一个 Pod，内存 request 和 limit 都为 1G ，内存的分配和回收都要在 1G 的物理内存空间内进行，而数据库又是一个对内存资源要求比较高的负载类型，仅仅启动一个空的数据库进程可能就要消耗数百兆的内存，所以其实留给实际应用的空间非常小，此时如果再搭配上监控或日志采集之类的 sidecar，数据库内存耗尽的概率就会非常高。</p><p></p><p>但是真正可怕的并不是 OOM，而是在 OOM 之前慢慢步入死亡的过程，这个过程可能会无比漫长，在真正触发 OOM 之前，page reclaim 模块会尝试一切办法去回收足够的内存，并调用耗时很久的 slow path，然后一遍又一遍地重复整个过程，直到超过限定次数退出，在这个过程中数据库客户端可能会观测到大量事务超时退出。</p><p></p><p>Page reclaim slow path 还不仅仅影响一个 Cgroup Namespace，由于 OS 中的很多数据结构在 Host 这一层是共享的， 比如虽然 Pod 内存逻辑上属于某个 Cgroup Namespace，但是在 Host Kernel 中，真实的内存管理还是基于同一个 Buddy System，对这些物理内存的管理需要全局的锁机制，所以一个内存压力很大的 Pod 触发的 page reclaim slow path 也会影响其它健康 Pod 的内存管理路径，有的时候整个 Node 上的数据库都变慢可能只是因为有一个 Pod 的 limit 内存太小了。</p><p></p><p>彻底解决此类问题就需要一些更高级别的隔离方案，比如基于微内核或 VM 的隔离方案，让两个 Pod 属于不同的内存管理空间；还有一种改进方案是当内存回收变得不可避免时，尽量在数据库的层面对各种运行指标进行判断，做到 fail fast。</p><p></p><p></p><h4>Too many Connections：</h4><p></p><p></p><p>对于 OLTP 数据库，一般都有专属的预分配的 buffer pool，这部分内存相对是固定的，可变的部分主要来自 Connection 结构体、work mem 中间计算结果、页表、page cache 等。</p><p></p><p>对于多进程模型的数据库如 Postgresql 和 Oracle，一条 Connection 对应一个进程，当使用的 buffer pool 本身就很大时，fork 一个进程所需的页表项也非常可观，假设 page 4k，页表项 8 字节，那么页表和 buffer pool 的比例关系是 8/4k = 1/512，当有 512 条链接时，OS 需要的页表内存就和 buffer pool 一样大，这种多进程模型严重影响了数据库的扩展性，在需要大并发量的场景会有比较高的额外内存成本，但是这种成本一般很容易被人忽略。</p><p></p><p>解决方案一般分为两种，一是在数据库前面增加一层 proxy，通过 proxy 来承接大量的链接，proxy 和数据库之间只建立比较少的链接，比如 proxy 和后端 db 之间只建立 P 条链接，proxy 从应用侧承接 C 条链接（C &gt;&gt; P）, 通过这种链接复用来降低后端 db 的链接压力；还有一种是采用 Hugepage 的方案，假设 Hugepage size 为 2M，那么页表和 buffer pool 的比例关系就是 8/2M = 1/256k，页表成本几乎可以忽略不计，多进程模型可承载的链接数也大大增加，但是 Hugepage 也是一种副作用很多的技术方案，给资源管理也带来了不小的负担。所以 proxy 方案一般是更友好的选择。</p><p></p><p>多线程模型又分两种，一种是一个 Connection 对应一个线程，当 Connection 增多时虽然没有页表 copy 的问题，但是也会导致资源争抢、context switch 过多等问题，这些问题又会导致性能持续恶化，当然这种问题也可以通过加 proxy 来解决；一种是 C 条链接对应 P 个线程（C &gt;&gt; P），这种方案一般叫作线程池（Thread Pool），比如 Percona MySQL 就采用了此类方案。</p><p></p><p>Proxy 和 Thread Pool 本质上是相同的，都是做链接复用，只是实现的地方不同，而且这两个方案也可以搭配使用，进一步提升容量和降低负载。</p><p></p><p>Table. 4. Overview of different database process-connection models</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/44/44f26e05d61bb90a67418d818bb57ad5.png\" /></p><p></p><p>点击图片可查看完整电子表格</p><p></p><p></p><h4>TCP Retran:</h4><p></p><p></p><p>网络对数据库的影响主要体现在两个方面：</p><p></p><p>延迟：网络延迟会影响数据的传输时长，进而会影响客户端的整体响应时间，当客户侧的请求延迟变高时，单位时间内完成相同请求数量所需要的链接数会变多，客户端链接数变多又会导致内存消耗变大、context switch 变多、争抢更加剧烈，最终导致性能的逐步下降。</p><p></p><p>带宽：无论是单个 TCP 链接的有效带宽，还是网卡和交换机网口的最大传输带宽，都对网络传输质量和延迟有重大的影响，当某个 TCP 链接、交换机网口或网卡队列变得拥塞时，会在 OS Kernel 或硬件层面触发丢包行为，丢包又会触发重传和乱序，重传和乱序又导致延迟上升，进而引发后续一系列的性能问题。</p><p></p><p>网络问题触发的不仅是性能问题，还有可用性和稳定性的问题，比如因网络延迟过大心跳超时导致的主备切换、主备之间的复制延迟过大等问题。</p><p></p><p></p><h4>CPU schedule wait：</h4><p></p><p></p><p>在一些基于 VM 的容器化方案中，容器中的进程和 Host Kernel 中的进程并不是能一一对应的，在 Host Kernel 看来，看到的只有 VM 虚拟化相关的进程，当你在 VM 内部看到一个 process 处于 running 状态，并不意味着它已经在 Host 上获取到资源并运行，Host 和 VM 是两套独立的 CPU 调度系统，只有当 VM 内部 process 处于就 running 并且所在 Host 上对应的 VM process 也处于 running 状态时，VM 内部 process 才真正得到运行。</p><p></p><p>从 process 变成 running 状态到真正被执行到的这段时间就是额外的调度等待时间，这个等待时间对数据库的性能也会产生影响，对性能要求比较苛刻的场景可以采取降低 Host 负载或设置 VM CPU affinity 的方法来降低影响。</p><p></p><p></p><h4>Lock &amp; Latch:</h4><p></p><p></p><p>在数据库领域，Lock 一般保护的是资源（Resource），Latch 保护的是临界区（Critical Region），但是两种技术最终在 OS 层面的内部实现是相同的，在 Linux 中，一般用 futex 来实现上层的互斥锁和等待变量。</p><p></p><p>当 CPU、I/O、内存都无限供应的时候，数据库的扩展性一般受限于自己内部的事务 + 锁机制，比如在 TPC-C 测试中，大部分单机数据库的扩展性一般都在 32 Core (64 Hyper Threads) ~ 64 Core (128 Hyper Threads) 之间，超过 32 Core 之后，CPU 数量对数据库性能的边际贡献会非常低。</p><p></p><p>这个话题和容器的关系不是那么密切，所以在本文中也就不做展开。</p><p></p><p></p><h4>几种数据库的性能瓶颈分析：</h4><p></p><p></p><p>Table. 3. Overview of different database performance bottlenecks</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/2e/2e9a348aac0cc015f6c81a1a0ed8c983.png\" /></p><p></p><p>MySQL 需要特别关注临时文件，由于临时文件使用的是 BufferedIO，如果没有 Cgroup 限制，会很快触发 OS 大量的脏页刷脏，这个刷脏过程会占用存储设备的几乎所有通道，造成正常请求卡住，这种现象是比较经典的 Disk IO hang。PostgreSQL 是多进程模式，所以需要十分关注链接数和页表大小，虽然使用 Hugepage 方案可以降低页表的负担，但是 Hugepage 本身还是有比较多的副作用，利用 pgBouncer 之类的 proxy 做链接复用是一种更好的解法；当开启 full page 时，PostgreSQL 对 I/O 带宽的需求非常强烈，此时的瓶颈为 I/O 带宽；当 I/O 和链接数都不是瓶颈时，PostgreSQL 在更高的并发下瓶颈来自内部的锁实现机制。具体可以参考《Postgresql@k8s 性能优化记》[7]。MongoDB 整体表现比较稳定，主要的问题一般来自 Disk I/O 和链接数，WiredTiger 在 cache 到 I/O 的流控上做得比较出色，虽然有 I/O 争抢，但是 IO hang 的概率比较小，当然 OLTP 数据库的 workload 会比 MongoDB 更复杂一些，也更难达到一种均衡。Redis 的瓶颈主要在网络，所以需要特别关注应用和 Redis 服务之间的网络延迟，这部分延迟由网络链路决定，Redis 满载时 70%+ 的 CPU 消耗在网络栈上，所以为了解决网络性能的扩展性问题，Redis 6.0 版本引入了网络多线程功能，真正的 worker thread 还是单线程，这个功能在大幅提升 Redis 性能的同时也保持了 Redis 简单优雅的特性。</p><p></p><p></p><h2>总结：</h2><p></p><p></p><p>本文在综合业界研究成果的基础上，补足了容器 + 网络 CNI 部分的测试，对容器化在 CPU、Memory、Disk I/O、Network 几个方面的影响做了进一步的分析，借机阐明了容器化对性能的影响机制和解决方法，并且通过分析测试数据，我们发现 runc + cilium eBPF 是一种和 bare metal 性能几乎持平的容器化方案，如果考虑到更好的安全性和隔离性，kata-containers 也是一种很好的选择。</p><p></p><p>然后在容器化的基础上，对数据库的常见的性能瓶颈做了原理分析，并指出数据库这种 heavy workload 对 Host kernel 的复杂依赖，引导人们重新关注页表、Journal Lock、TCP Retran、CPU schedule wait 这些容易被忽视的因素，当然这里的很多问题和容器化无关，是一种普遍的存在。最后我们对几款流行的数据库做了定性分析，也根据我们团队多年的运维经验对一些常见问题做了总结，希望这些问题能被持续关注并从架构层面得到解决。</p><p></p><p>数据库容器化是最近经常被提起的话题，to be or not to be 也是萦绕在每个决策者心中的问题，在我们看来，数据库容器化面临的性能、稳定性、有状态依赖等关键问题都在被一一解决，每个问题都会有一个完美的答案，只要有需求在。</p><p></p><p>作者简介</p><p></p><p>蔡松露（子嘉），云猿生数据 CTO &amp; 联合创始人，前阿里云数据库研发资深技术专家，云数据库架构师。在职期间主要负责阿里云 PolarDB、NoSQL 技术以及阿里云数据库架构等工作。在数据库领域有深厚积累与丰富的经验。目前负责云猿生数据产品研发工作，并带领团队完成云原生数据库管理系统 KubeBlocks 的架构和开发工作。Redis 中文社区核心发起人；BDTC（2019）演讲嘉宾；DTCC（2018）演讲嘉宾；Gopher China （2023）演讲嘉宾。</p><p></p><p>[1] Wang, Xing et al. “Performance and isolation analysis of RunC, gVisor and Kata Containers runtimes.” Cluster Computing 25 (2022): 1497-1513.</p><p></p><p>[2] Goethals, Tom et al. “A Functional and Performance Benchmark of Lightweight Virtualization Platforms for Edge Computing.” 2022 IEEE International Conference on Edge Computing and Communications (EDGE) (2022): 60-68.</p><p></p><p>[3] Espe, Lennart et al. “Performance Evaluation of Container Runtimes.” International Conference on Cloud Computing and Services Science (2020).</p><p></p><p>[5]<a href=\"https://www.reddit.com/r/kubernetes/comments/x75sb4/kube_container_performance_crio_vs_containerd/\">https://www.reddit.com/r/kubernetes/comments/x75sb4/kube_container_performance_crio_vs_containerd/</a>\"</p><p></p>",
    "publish_time": "2024-01-30 14:46:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "碾压前辈！Meta发布“最大、性能最好”的开源Code Llama 70B，但开发者纷纷喊穷：玩不起！",
    "url": "https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg",
    "summary": "<p></p><p>当地时间 1 月 29 日，Meta 发布了 Code Llama 70B，Meta 表示这是“Code Llama 家族中体量最大、性能最好的模型版本”。Code Llama 70B 与先前其他家族模型一样提供三种版本，且均可免费用于研究和商业用途：</p><p></p><p>CodeLlama – 70B，基础编码模型；CodeLlama – 70B – Python，专门用于 Python 编码的 70B 模型；Code Llama – 70B – Instruct 70B，针对自然语言指令理解进行微调的版本。</p><p></p><p>为了对比现有解决方案测试 Code Llama 的性能表现，Meta 选择了两项流行的编码基准：HumanEval 与 Mostly Basic Ptyon Programming（MBPP）。其中 HumanEval 主要测试模型根据文档字符串补全代码的能力，而 MBPP 则测试模型根据描述编写代码的能力。</p><p></p><p>从基准测试结果来看，Code Llama 的表现优于编码专用的开源 Llama，甚至超越了 Llama 2。例如，Code Llama 34B 在 HumanEval 上的得分为 53.7%，优于 GPT-3.5 的 48.1%，更接近 OpenAI 论文报告的 GPT-4 的 67%。在 MBPP 上，Code Llama 34B 得分为 56.2%，超越了其他最先进的开源解决方案，已经与 ChatGPT 基本持平。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e4/e47f97d709006f1d2195394c47b6a053.png\" /></p><p></p><p>扎克伯格在 Facebook 上说道，“编写和编辑代码已经成为当今人工智能模型最重要的用途之一。编码能力也被证明对人工智能模型更严格、更符合逻辑地处理其他领域的信息非常重要。我对这个进展感到自豪，并期待 Llama 3 和未来的模型中包括这些进展。”</p><p></p><h4>Code Llama 实现原理</h4><p></p><p></p><p>Code Llama 是 Llama 2 模型的编码专用版本，是后者在编码数据集之上接受进一步训练的产物，且数据采集周期更长。从本质上讲，Code Llama 拥有比 Llama 2 更强的编码功能。它可以根据代码和自然语言提示词生成代码及与代码相关的自然语言（例如，“为我这一条输出斐波那契序列的函数”），亦可用于代码补全和调试。</p><p></p><p>Code Llama 支持当今多种高人气编程语言，包括 Python、C++、Java、PHP、Typescript (Javascript)、C# 和 Bash。</p><p></p><p></p><p></p><p>这次，Meta 将发布四种 Code Llama 模型版本，参数分别为 7B、13B、34B 和 70B。各模型版本使用 500B 代码 token 与代码相关数据进行训练，且 70B 模型则采用 1TB token 进行训练。7B 与 13B 基础与指令模型还经过 fill-in-the-middle（FIM）训练，允许向现有代码中插入新代码，因此可以支持开箱即用的代码补全等任务。</p><p></p><p>三种模型分别能够满足不同的服务与延迟要求。例如，7B 模型可以在单一 GPU 上运行。34B 和 70B 模型则可返回最佳结果并提供更好的编码辅助功能。其中 7B 与 13B 模型运行速度更快，适合实时代码补全等强调低延迟的编码任务。</p><p></p><p>Code Llama 模型可实现最多 10 万个上下文 token 的稳定生成能力。所有模型均在 1.6 万个 token 的序列上进行训练，并在最多 10 万个 token 的输入场景下表现出性能提升。</p><p></p><p></p><p></p><p>除了能够生成更长的编码程序之外，更长的输入序列窗口还能为编码大模型解锁其他令人兴奋的新用例。例如，用户可以向模型输入来自代码库的更多上下文信息，确保生成结果的相关性更强。这还有助于在体量更大的代码库中进行调试，帮助开发人员快速找到与特定问题相关的所有代码。现在，开发人员可以将完整代码直接提交至大模型，高效完成涉及大量代码的调试任务。</p><p></p><p>此外，Meta 还进一步微调了 Code Llama 的两个附加变体：Code Llama – Python 与 Code Llama – Instruct。</p><p></p><p>Code Llama – Python 是 Code Llama 的特定语言专用变体，使用 100B 个 Python 代码 token 上接受了进一步微调。由于 Python 是代码生成领域的基准测试语言，并且通过 PyTorch 在 AI 社区中发挥着重要作用，所以 Meta 相信这套专用模型将提供更有针对性的实际功能。</p><p></p><p>Code Llama – Instruct 则是 Code Llama 的指令微调与对齐变体。指令微调同样属于继续训练过程，能够满足其他特定目标。该模型接受“自然语言指令”输入与预期输出组合的持续训练，因此能够更好地理解人们对于提示词的生成期望。由于 Code Llama – Instruct 专门就生成实用、安全的自然语言回答进行了微调，因此在使用 Code Llama 进行代码生成时，Meta 建议开发者优先选择 Code Llama – Instruct。</p><p></p><p>Meta 并不建议开发者使用 Code Llama 或者 Code Llama – Python 执行常规自然语言任务，因为这两套模型并不是为遵循自然语言指令所设计。Code Llama 专门用于特定编码任务，不适合作为其他通用任务的基础模型。</p><p></p><p>另外，在使用 Code Llama 模型时，用户须遵守 Meta 指定的许可证与可接受使用政策。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/bc/bc80cbf1775d28c3d3bd605e52d10882.png\" /></p><p></p><p></p><h3>更大参数会带来更高的硬件要求？</h3><p></p><p></p><p>没有意外，Code Llama 70B 赢得了开发者们的赞扬，甚至有人称“Code Llama 70B 是代码生成领域的游戏规则改变者。”</p><p></p><p>但有自称使用过的开发者表示，“我使用 Ollama 尝试了 70B 模型，即使经过多次尝试，它也无法编写贪吃蛇游戏。而 ChatGPT 一次尝试就给出了一款可以运行的游戏。”</p><p></p><p>另一方面，随着模型参数的增加，开发者们也担心自己手头没有足够装备来满足运行 Code Llama 70B。有人指出，在 A100-80GB 上训练所有 12 个 Code Llama 模型需要 1400K GPU 小时。</p><p></p><p>运行大模型几乎可以归结为两个因素：内存带宽和计算能力，足够高的内存带宽才能“提供”计算，足够强大的计算才能跟上内存提供的数据。对于个人开发者来说，可能并没有完美设备，因此很开发者也在寻求更容易配置的量化模型版本。</p><p></p><p>也有开发者支招：可以在 64GB RAM 的 Macbook M1/M2 上运行 70B 模型。</p><p></p><p>开发者“tinco”表示，“据我所知，市场上没有其他笔记本电脑具有与 64GB MBP 一样多的 VRAM。您可以使用两个 3090 制成一台 Linux 台式计算机，将它们连接在一起提供 48GB 的 VRAM。这样显然可以运行 4 位量化的 6k 上下文 70B Llama 模型。”Tinco 进一步表示，人们推荐 Macbook 是因为它们是一种相对便宜且简单的方法，可以将大量 RAM 连接到加速器上。</p><p></p><p>Tinco 提醒道，这些是模型的量化版本，因此它们不如原始 70B 模型好，尽管人们声称它们的性能非常接近原始性能。要在不进行量化的情况下运行，则需要大约 140GB 的 VRAM。这只有一台 NVidia H100（不知道价格）或两台 A100（每台 18,000 美元）才能实现。</p><p></p><p>也有开发者分析称，理论上，单个 Nvidia 4090 能够以“可用”速度运行量化的 70B 模型。Mac 硬件在人工智能方面如此强大的原因是因为统一的架构，这意味着内存在 GPU 和 CPU 之间共享。还有其他因素，但本质上归结为每秒代币的优势。用户可以在内存带宽较低的旧 GPU 上运行这些模型中的任何一个，但每秒的令牌速度对于大多数人认为“可用”的速度来说太慢了，而且必要的量化可能会明显影响质量。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>代码生成一直既被开发者叫好又被吐槽，即使是 ChatGPT 和 Copilot，因为虽然可以提效，但是质量问题一言难尽。</p><p></p><p>有开发者在 Hacker News 上表示，“两个月后我取消了订阅（Copilot），因为我花了太多的精力去检查所有代码并修复所有错误。当尝试处理任何不琐碎的或与 SQL 有关的事情时（即使我用整个模式预先加载它），它基本上是无用的。自己编写所有内容要省力得多，因为我实际上知道自己想写什么，并且修复自己的错误比修复机器人的错误更容易。”</p><p></p><p>使用 ChatGPT 的“ben_w”表示，“我对它（ ChatGPT）的功能感到惊讶，但即便如此，我也不会称其为‘好代码’。我将它用于 JavaScript 编程，因为虽然我可以阅读（大部分） JS 代码，但过去 14 年我一直专业从事 iOS 工作，因此不知道什么是浏览器领域的最佳实践。尽管如此，我得到工作代码后，我也可以发现它产生了错误的选择和（看起来）奇怪的东西。”</p><p></p><p>类似的问题我们也在之前的文章<a href=\"http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247602048&amp;idx=1&amp;sn=56bd059b8bf046ecfdb509a68c809baf&amp;chksm=fbebf64fcc9c7f594affa5928ee74adffc1c36bce4adcc521fbd2c43f5b4e4f7c0e586dd2ede&amp;scene=21#wechat_redirect\">《代码屎山噩梦加速来袭，都是 AI 生成代码的锅？》</a>\"中讨论过。最新研究也表示，GitHub Copilot “代码质量面临下行压力”，代码可维护性的趋势令人不安。</p><p></p><p>开源领域一直在进行生成更好代码的研究。在 Hugging Face 的“Big Code Models Leaderboard”上，也有很多被开发者认可的模型。</p><p></p><p>比如北京大学推出了⼀系列从 1.3B 到 33B 的 &nbsp;DeepSeek-Coder 开源代码模型。DeepSeek-Coder 基于 2 万亿个代币上从头训练，都使用 16K 的窗口大小和额外的填空任务在项目级代码语料库上进行预训练，以支持项目级代码补全和填充。测评结果表明，DeepSeek-Coder 不仅在多个基准测试中实现了开源代码模型中最先进的性能，⽽且还超越了 Codex 和 GPT-3.5 等现有的闭源模型。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4c/4cf3e0e23b2e252c922e1b9bfd21f949.png\" /></p><p></p><p>对于有开发者提出“当前 SOTA 本地代码生成模型是什么”的问题，可能现在还没有标准答案，大家还在努力想办法优化现在模型，现在远不是终点。</p><p></p><p>你心中的 SOTA 代码生成模型是什么？欢迎评论区说出你的使用感受和经验！</p><p></p><p>相关链接：</p><p></p><p>Meta 在研究论文中披露了 Code Llama 的开发细节以及基准测试的具体步骤，感兴趣的用户可以查看：</p><p></p><p><a href=\"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\">https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/</a>\"</p><p></p><p>感兴趣的朋友可以在 GitHub 上参阅 Code Llama 训练 recipes：</p><p></p><p><a href=\"https://github.com/facebookresearch/codellama\">https://github.com/facebookresearch/codellama</a>\"</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://ai.meta.com/blog/code-llama-large-language-model-coding/https://news.ycombinator.com/item?id=39178886\">https://ai.meta.com/blog/code-llama-large-language-model-coding/https://news.ycombinator.com/item?id=39178886</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2024-01-30 14:51:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于 RAG 构建生成式 AI 应用最佳实践与”避坑指南“",
    "url": "https://www.infoq.cn/article/6SUKUe8Ita2u7eYuuCHV",
    "summary": "<p>Qcon上海站亚马逊云科技 人工智能技术专家李元博，分享介绍 RAG 实践中的一些经验积累，包括一些通用思路以及如何利用亚马逊云科技的服务 (如 Amazon Bedrock/Amazon OpenSearch) 优化基于 RAG 的生成式 AI 应用开发。</p>\n<p>听众收益</p>\n<p>1.深入了解 RAG 的项目落地的挑战和解决思路<br />\n2.更好的构建基于 RAG 场景的应用</p>",
    "publish_time": "2024-01-30 15:15:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "为什么我们需要全能力云原生网关？",
    "url": "https://www.infoq.cn/article/ZltFTNz44gRHYxNIUw1B",
    "summary": "<p></p><p>作为现代云原生业务数据流转的重要通道，网关在行业数字化转型中发挥着日益重要的价值。网易数帆自 2017 年开始探索基于 Envoy 的云原生网关，并实现在互联网、金融等多个领域推广和规模化落地。本文结合网易数帆近期在金融场景下的实践，分享团队对于云原生网关建设的最新思考，以及在业务场景下解决用户痛点的方案与心得。要点如下：</p><p></p><p>网关能为业务带来什么价值；网易数帆如何建设云原生网关；云原生网关在金融场景如何解决业务痛点，及落地经验分享；云原生网关的后续发展。</p><p></p><h3>网关的背景与价值</h3><p></p><p></p><p>在微服务场景下，服务之间的调用错综复杂。微服务如果需要一些治理能力，要么自己实现治理能力，比如认证、限流缓存等；要么借助一些第三方 SDK 提供，以 Spring Cloud 体系为主。</p><p></p><p>这种架构带来了业务不够聚焦的问题，微服务除了要关注自己的业务逻辑，还需要关注如何实现一些治理的诉求。网易数帆认为，可以将这部分逻辑上移，将治理能力通用化，并提供统一的可观测、监控追踪体系。这就是在微服务场景下网关的意义。在微服务场景下，网关可以简化架构；可以提供统一的安全方案，包括不限于认证鉴权、黑白名单控制；可以提供通用的监控能力，监控系统的运行；还可以提供灵活的通信协议。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/27/2722ba06a7b9e79c12e09626ad32ca71.png\" /></p><p></p><p>云原生场景对网关提出了更高的要求，主要包括 6 个方面：</p><p></p><p>服务发现方式：和传统的静态发现方式相比，需要具备更灵活的动态服务发现方式，包括不限于对接到 K8s，以及 Eureka、Nacos 等微服务注册中心；更卓越的性能：随着 K8s 以及服务网格化的微服务体系，网关的卓越性能可以减少整个链路的 RT；云原生架构兼容：网关的部署架构是否可以无缝和容器以及微服务场景下的服务网格，云原生可观测 SkyWalking、Prometheus 进行对接；动态配置：云原生场景下，代理的配置变化更加频繁，服务的实例地址，服务的连接池配置、重试策略等频繁变更，动态配置的下发能力尤为关键；部署形态与架构：是否能够支持在云场景下快速弹性，是否能够支持从业务混部到独立部署的快速迁移能力；监控追踪体系：良好的可观测便于运维人员快速定位问题。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d5/d5c74648d0a95a7f0162a3cb037e7e21.png\" /></p><p></p><p></p><h3>网易数帆云原生网关架构</h3><p></p><p></p><p>网易数帆云原生网关以 Envoy 为数据引擎。我们与 Envoy 结缘始于 sidecar 体系，Envoy 具有出色的代理性能，其 xDS 协议支持动态配置，并提供一系列 filter-chain，用于扩展，还可对接多种可观测系统，提供良好的指标体系以及日志体系，因而可以成为优秀的数据引擎。</p><p></p><p>在 2019 年，我们基于 Envoy 扩展调用链，提供细粒度指标，提供 dubbo filter, soap filter，建设高性能微服务网关，用以替代网易集团内一些 Java 异步网关、一些同步网关的迁移，为集团业务容器化改造提供通用的流量入口支持。</p><p></p><p>在网易集团，需要使用网关数据面引擎的场景，主要有如下三类：</p><p></p><p>存量网关迁移：如果项目已存在一些网关，存在一些性能、可观察、可扩展的痛点，需要进行网关基础组件的升级；云原生改造：在东西向流量进行云原生容器化改造，希望网关能够纳管容器流量，同时可以兼容后续微服务体系服务网格化；多流量管理：不仅仅承接南北向流量，希望网关可以实现云内云外互通，多云管理的统一治理。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a1/a13d5ecac53fc50bf872230a472d1862.png\" /></p><p></p><p>此时的网关，可以称为“微服务网关”。而随着业务的发展，业务对网关的诉求已不仅仅在微服务场景，因此我们逐步建设全能力云原生网关，主要有四点目标：</p><p></p><p>通用流量治理：网关不仅仅是微服务流量的入口，还要成为安全网关、流量网关、通用总线等全能力 7 层流量治理平台。异构服务接入：越来越多的微服务场景不仅仅局限在某一个协议的治理，网关应该能够提供一种通用的协议转换能力。这里包括两层含义，第一是暴露通用的协议，作为流量入口，第二是支持协议转换互转、协议互调。多数据源支持：不仅支持容器以及虚拟机服务的接入，还支持微服务体系的多注册中心的数据源接入。自主可控：在当前复杂形势下，云原生网关需要能够运行在国产化操作系统，能够支持对接国产化中间件。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/fd/fd865450907582c0d07fc4e8aa8f8873.png\" /></p><p></p><p>基于全能力网关的目标，网易数帆建设全能力融合云原生网关体系，其架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/87/87c28cb373a3ae15b13f479db1e774f7.png\" /></p><p></p><p>全能力云原生网关能力的设计，不局限于微服务网关，还能够支持通用 7 层流量，支持 Dubbo、gRPC、webservice 等多协议流量；可对接多数据源，已支持 Eureka/Nacos/ZK/Consul 的微服务注册中心；支持通用协议的互转；提供 40 余个内置插件，且支持灵活的通用插件扩展；提供丰富的可观测能力，可支持方法级别的观测体系；同时作为企业级网关支持多租户能力。</p><p></p><p></p><h3>金融场景的落地实践经验</h3><p></p><p></p><p>以下通过六个案例，分享网易数帆全能力云原生网关在金融场景下的落地实践经验与心得。</p><p></p><p></p><h4>案例一：私有协议扩展</h4><p></p><p></p><p>第一个案例是私有协议扩展，越来越多的大型证券，头部银行金融企业，要求核心交易系统具备协议私有化，自主可控的能力。主要有 2 点诉求，要求能够支持统一的协议入口，作为端网关暴露。</p><p></p><p>基于此，网易数帆对于入口网关提供统一方案。其架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f0/f0bfbd5869d5b7a17c0b7459c57886dd.png\" /></p><p></p><p>复用 Envoy 的良好引擎，通过 http connection manager 通过 listener，我们提供通用的 geneirc filter 能力，实现 HTTP- 私有协议的编解码；提供通用的 bridge，作用在 router 模块之间。可以把已有的请求、响应插件链进行复用。当出现一个新的协议，只需要通过标准的 DSL 定义协议转换的定义，编写协议转换的编解码即可。聚焦在转换配置下发，我们抽象插件链的实现，不需要侵入到核心的链路，通过简单声明式的 EnvoyPlugin 的配置，即可以完成将协议转换配置下发到 Envoy 的 per-filter-config 中进行生效。</p><p></p><p>不局限在南北向入口，诉求中还会存在协议互转的诉求，在一些中台业务之间的互相调用，能够支持协议互转。这种场景下，无法复用 http connection manager 的能力。如果要在现有的基础上扩展，需要完备的实现一套 network filter，类似现在社区的 dubbo proxy 能力。考虑到这样完整的实现有比较高的代价。</p><p></p><p>大部分的 RPC 协议都有类似的请求 / 响应的架构体系。请求阶段，根据不同的请求属性将请求路由到不同的上游服务。对于这些相似的协议，一个功能完备的 network filter 可以被抽象为下面的几个模块：</p><p></p><p>编解码器：将二进制文件解析为请求 / 响应对象；流管理：处理请求 / 响应的关系并管理其声明周期；路由：基于请求的属性，路由到不同的上游服务；L7 filter 链：对请求和响应进行处理的过滤器；观测：提供调用链、指标、日志等可观测能力。</p><p></p><p>其中，除了编解码器无法脱离协议本身做到公用，其他的模块都可以被抽象为公共模块，并为所有的协议所复用。因此，我们抽象定义 Generic Proxy，可用于通用扩展的通用协议代理。提供了可以让用户根据自己协议配置的通用编解码扩展点。开发人员只需要实现解析二进制的编解码器，使用者通过通用的配置完成对应的配置。其他的诸如路由，可观测，流处理全部由 Generic Proxy 进行实现。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0c/0c32c0741180c3abc290b5706093d328.png\" /></p><p></p><p>这部分能力，我们已经完全贡献到了 Envoy 社区，通过 generic_proxy 的扩展，可以比较便捷的完成扩展。如下是一个扩展配置实现图：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f3/f3fc09ddaf84c6ee5455c90025542c05.png\" /></p><p></p><p>我们扩展 Gateway 以及 VirtualService，通过结合自定义 EnvoyPlugin 以及 PluginManager，扩展并生成 generic route，完成对 Envoy generic listener 以及 generic route 的下发。其协议转换被生效在 generic route 的 per filter config。完成整个路由及协议转换的生效。</p><p></p><p>目前，通过这套标准的 generic proxy 的 API 扩展，可以很便捷的完成私有协议的扩展，研发提效 50% 以上。某头部券商通过该套框架，已落地 3 个私有协议的扩展。</p><p></p><h4>案例二：全链路灰度</h4><p></p><p></p><p>第二个案例是全链路灰度，在复杂的微服务集群，如何快速拉起一套全链路灰度，用于线下环境测试或线上问题复现。</p><p></p><p>全链路灰度的场景并不陌生，一套标准的基准环境，通过云原生网关为流量入口，下游为 A,B,C 三个微服务。如果服务 A，C 有灰度的诉求，在没有全链路灰度方案的时候，需要完整 copy 一套环境，其中的资源及机器部署申请，时间不可控。</p><p></p><p>在全链路灰度的场景下，业务只需要针对 A,C 服务进行灰度部署，由网关入口和微服务体系进行流量灰度穿梭，指定标签的流量可以打到灰度节点上，如果不存在灰度节点可降级到基准环境中的服务。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3e/3ec9516f3a5c67fd136be054a17becbc.png\" /></p><p></p><p>这个场景下，我们通过标签同步、条件打标、流量负载来进行全链路灰度。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/10/10d042f7b1116b624923b1babb571da0.png\" /></p><p></p><p>通过 mesh-registry 组件，对接多注册中心，将不同注册中心中的实例进行聚合组装，提取其中的标签、metadata 信息，组装为 serviceentry 最终下发给 envoy eds 配置。</p><p></p><p>通过自研 header write filter，用于对不同的流量标识进行条件打标，支持 header/query/body 中的条件匹配。</p><p></p><p>通过结合 Envoy LB subset 的能力，提供基于标签的二级负载能力，进行流量的灰度。</p><p></p><h4>案例三：插件扩展</h4><p></p><p></p><p>第三个案例，在金融特性场景下，需要具备一定的功能拓展。提供插件扩展能力，考量主要有三点：1）敏捷扩展能力 ；2）插件之间相互独立，不受影响，能够进行配置之间的隔离；3）动态生效加载，不需要进行二次编译。</p><p></p><p>我们将插件扩展分为两层：1）开发人员，聚焦在插件开发，生成插件代码片段或可执行文件；2）插件使用人员聚焦在插件的使用、配置，并上线生效。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/5a/5a03155db827907e6b3458290b2e51f2.png\" /></p><p></p><p>我们设计一套足够敏捷的可扩展插件体系，在代码编写上，提供基于 Rider 的 LUA 扩展脚手架，基于 FFI 实现，较社区提供更高性能及更灵活的扩展，提供基于 WASM 的脚手架，便于用户扩展，同时在可视化编程上，提供基于 schema 的前端渲染能力，使得用户可以敏捷的开发前端；在上传应用角度，提供不局限文件，OCI 的多种上传方式，提供 filter 的能力，扩展 EnvoyPlugin 和 PluginManger，用户可以指定插件作用顺序以及控制插件是否启用，灵活的生效配置及隔离化。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/92/92ac5ee8546fcd22f93a26d9c9183557.png\" /></p><p></p><p>以上两个设计，我们都在社区进行了回馈，插件的扩展脚本开源于 Hango Rider(<a href=\"https://github.com/hango-io/rider\">https://github.com/hango-io/rider</a>\")，插件的 Plugin 模块定义，开源于 Slime-io/slime(<a href=\"https://github.com/slime-io/slime\">https://github.com/slime-io/slime</a>\"_)_。</p><p></p><p>基于可扩展插件体系，要实现在证券行业的开市时间要求，在维护周期内，需要对某些请求进行确定周期的友好化响应。用户只需要进行插件的代码编写，简单的脚手架方式就可以编写完成对应的插件；通过平台化的方式，导入对应的代码，可选择文件或镜像；之后通过简单的拖拽方式，自动渲染生成可视化插件前端即可完成插件的开发，上架，交付。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f9/f9ace844f43eb456065d9802ec5faa97.png\" /></p><p></p><p>案例四：业务平滑迁移</p><p></p><p>在金融场景的稳定性要求下，业务云原生改造不是一蹴而就的，而是根据业务敏感性要求，灰度迁移到云原生 K8s 中。云原生网关协助金融业务完成云原生改造，主要有三个痛点：</p><p></p><p>注册中心的复杂：传统行业注册中心不单一，包括存量的 Eureka、Nacos 以及虚拟机注册行为。服务模型不统一：部署形态决定服务模型不同，例如虚拟机场景下的 IP,Port 服务属性，K8s 场景下的 K8s svc 服务模型；协议不同决定服务模型不同，例如 HTTP 场景下的域名 / 服务名，Dubbo 协议的接口粒度的服务模型。存量网关迁移：企业中往往已经存在网关，如何敏捷且快速将存量的网关数据迁移到云原生网关上。</p><p></p><p>基于以上痛点，我们提供了标准的云原生网关迁移方案，其架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3f/3f7182471365c3e45be5f6ddfb5d11d8.png\" /></p><p></p><p>mesh-registry 对接多注册中心，提供对接 ZK、Eureka、Nacos 等注册中心，支持对接能力的扩展。提供标准的 upstream 管理，抽象服务模型，对接静态 serviceentry 进行抽象，对接 envoy eds。同时，基于业务平滑上云，提供同协议多服务版本，用户可进行权重分流、版本分流用于灰度迁移。标准的 sync 服务，对接已有结构化数据，协助企业完成存量网关数据迁移到我们的云原生网关体系。</p><p></p><h4>案例五：多租户及配置隔离</h4><p></p><p></p><p>在金融行业下，业务分级比较明显，需要完成对不同级别的系统在网关上的配置隔离。</p><p></p><p>Envoy Proxy 的核心处理流程就是接收客户端的下游请求，通过一系列过滤器 / 模块，将流量进行转发。Envoy 将接收下游请求抽象成为 Listener，对上游的转发成为 Cluster。其提供了多 Listener 的能力，可以支持在一个进程中动态多个 Port，开启多个 Listener，每一个 Listener 可支持动态刷新，且其 filter-chain 以及 route 相互隔离，互不影响。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c2/c2be8c607c2e834f78f3e55048da6e6a.png\" /></p><p></p><p>因此基于 Envoy 的多 Listener，我们可以对网关从物理隔离抽象为逻辑隔离，在一个物理网关进程上，抽象多个逻辑网关。使得流量可以在不同的逻辑网关进行 filter 及 route 路由。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ad/ad67a938d7a2615b5dd0e85258b4dd00.png\" /></p><p></p><p></p><h4>案例六：多集群建设</h4><p></p><p></p><p>最后是我们如何建设金融级别稳定性保障体系，以满足金融行业的特殊行业属性以及政策指导要求。我们基于同城双活以及异地备份建设高可用架构，在同一个物理区域内建设不同的可用区，支持不同的集群 cluster，使得业务可以选择在同一个集群或者在同城双活的不同集群进行流量区域负载穿梭。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8f/8f42cf97d854068a6b9122b1b70dafa2.png\" /></p><p></p><p>在网关金融行业大规模实践中，最核心的是以下两点：</p><p></p><p>丰富的能力建设：在支撑金融场景下，有一些特有的场景，因此要求云原生网关能够快速迭代，敏捷适配，提供标准的通用方案扩展。</p><p></p><p>稳定性体系建设：在事前，聚焦在降发生，聚焦在故障预防，提供全方位巡检方式，故障兜底以及灾备设计架构；如果事故发生，则从故障快速感知及恢复，快速的定位问题及恢复问题着手，降低事故影响，借助全链路排障以及 AIOps 能力，提供便捷的问题恢复手段。在事故后，进行经验沉淀及回归，完善经验知识库，指导高可用稳定性体系建设。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9b/9b892bd5ef31a69ed20cccbae2ac3601.png\" /></p><p></p><p></p><h3>总结与展望</h3><p></p><p></p><p>展望未来两三年，云原生网关的发展将主要围绕如下三项能力的完善：</p><p></p><p>全能力网关建设：不局限于南北向流量，网关结合编排、多协议转换能力，去分布式化，应用于东西向流量中。生产级别稳定性保障：聚焦在精准化监控，细粒度观测，提供精准化告警。提升问题快速恢复，确保流量无损。AI 洞察：结合已有的专家经验，结合 LLM 大模型的能力，完善根因分析体系，构建分析平台，更好地做到问题前置，洞察问题及故障。</p><p></p><p>作者介绍</p><p></p><p>韩佳浩，网易数帆云原生网关团队负责人，负责网易数帆云原生网关及服务网格在集团内部、金融重点客户大规模落地及产品化建设。具有五年以上相关研发及大规模实践经验。目前主要聚焦在云原生网关金融场景的落地实践以及通用网关建设。</p><p></p>",
    "publish_time": "2024-01-30 15:18:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "业内首个代码大模型标准正式发布",
    "url": "https://www.infoq.cn/article/2a1ZKWTuI0HCfSt7mktQ",
    "summary": "<p>为进一步发挥大模型等 AI 技术在软件工程领域的潜力，推动智能化软件工程（AI4SE）行业健康有序发展，2024 年 1 月 25 日，中国信息通信研究院（以下简称“中国信通院”）与中国工商银行联合牵头发起的《智能化软件工程技术和应用要求 第 1 部分：代码大模型》（标准编号 AIIA/PG 0110-2023）标准正式发布。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/dd/dda92851d33571a9635ff163005645e0\" /></p><p></p><p>图 1：《智能化软件工程技术和应用要求》系列标准结构</p><p></p><p>本标准规定了代码大模型的技术能力和应用要求，适用于企业在代码大模型的研发、评估和验收等过程中，为代码大模型能力的建设和改进提供参考，为代码大模型的技术选型提供指引。本标准分为通用能力、专用场景能力和应用成熟度三大部分，包括 16 个能力项，100 多个能力要求，主要从输入多样性、任务多样性、语言完备度、结果可接收性、结果准确度等维度，对代码大模型提出了全栈技术能力要求。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b9/b93266478fb2aa40f2e1b7b70673964e\" /></p><p></p><p>本标准的编制过程历经数月，汇聚了 30 多家企业提供的最佳实践和经验，及近百名专家老师的知识及力量，经过了几十次的研讨和评审，最终形成发布稿，在此感谢所有参编企业和专家的贡献。核心参编单位有中国工商银行、科大讯飞、华为、平安银行、阿里云、三六零、蚂蚁、天翼数字生活、交通银行、网易数帆、深圳集义。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/01/017e9aa52cb77e125d8ab97f028a3978\" /></p><p></p><p>注：</p><p></p><p>中国信通院长期深耕于人工智能与软件工程交叉领域的研究，并将持续跟踪 AI4SE 最新动态，陆续开展智能开发标准的发布、代码数据集构建、代码大模型评测、智能测试和智能运维的研究等相关工作。</p><p></p><p>其中，中国人工智能产业发展联盟（AIIA）智能化软件工程工作组（AI4SE 工作组），于 2023 年 9 月正式成立，旨在进一步发挥生成式 AI、大模型等人工智能技术在软件工程领域的潜力，充分释放 AI 赋予软件工程的价值。目前，AI4SE 工作组已吸纳 100+ 成员单位，覆盖金融、电信、软件等诸多行业。</p><p></p><p>业务联系人：</p><p></p><p>中国信通院人工智能研究中心</p><p>胡老师，17371328072（微信同号）</p><p>秦老师，13488684897（微信同号）</p><p></p>",
    "publish_time": "2024-01-30 17:27:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型应用加速进行中，智能化软件工程势头强劲",
    "url": "https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371",
    "summary": "<p>智能化软件工程领域发展迅速，其智能化赋予软件研发流程更高的自动化和智能化水平。为加强AI与软件工程领域的交流互通，推动行业多融合发展，2024年1月25日，由中国信通院人工智能研究中心、中国人工智能产业发展联盟（以下简称AIIA）和360集团联合主办的“大模型应用加速行动之走进360 暨首站‘AI4SE创新巡航’”活动在京成功举办。</p><p></p><p>会议首先由中国信通院人工智能研究中心常务副主任魏凯、360集团首席运营官叶健分别发表致辞。魏所表示，大模型在消费市场（2C）和企业市场（2B）应用落地正在加速进行中，应用效果逐渐明显，中国信通院将联合中国人工智能产业发展联盟及产学研各方，以构建人工智能产业生态协作网络为目标，持续推动人工智能与行业融合应用，更好得赋能新型工业化。叶总表示，作为国内唯一兼具大模型和安全能力的公司，360瞄准大模型安全等关键“卡脖子”短板，积极参与信通院组织的系列活动，不断发掘AI技术作为新质生产力的潜力，共同推动人工智能大模型赋能“千行百业”。</p><p><img src=\"https://static001.geekbang.org/infoq/b1/b115c1603dd823ba3a8e895e08fb7bc3.png\" /></p><p></p><p>&nbsp;</p><p>为公正客观得评测代码大模型，提升评测数据集的丰富度、质量和多样性，中国信通院集结产学研各方力量，正式启动代码大模型数据集共建项目，会上由魏所为共建单位颁发证书。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/e5/e56d1069f47a63ae3eed0f66dc1766ec.png\" /></p><p>&nbsp;</p><p>代码大模型数据集共建单位名单</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/28/282f9b603e07863f3b8c0cc3935d8584.png\" /></p><p>&nbsp;</p><p>代码大模型数据集共建单位证书颁发仪</p><p></p><h2>业内首个代码大模型标准正式发布</h2><p></p><p>&nbsp;</p><p>随后，由中国信通院和中国工商银行联合牵头的《智能化软件工程技术和应用要求第一部分:代码大模型》标准正式发布，这标志着国内代码大模型迈向规范化与标准化的重要一步。中国信通院人工智能研究中心平台与工程化部主任曹峰，和中国工商银行专家范威，分别从标准的编制背景、编制过程、参编单位，及标准的通用能力、专用场景能力、应用成熟度等维度进行详细解读。会上由魏所为核心参编单位颁发证书。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/28/284728f51d9701766d000391f4d9fa63.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa870000a4d2d7538b1814dd352ddb1a.png\" /></p><p>&nbsp;</p><p>代码大模型标准核心参编单位列表</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/59002492214d9897b0178180cce55864.png\" /></p><p>&nbsp;</p><p>代码大模型标准核心参编单位证书颁发仪式</p><p>&nbsp;</p><p></p><h2>发布2023年AI4SE“银弹”优秀案例</h2><p></p><p>&nbsp;</p><p>会上正式发布了2023年AI4SE“银弹”优秀案例，中国信通院组织评委组从技术创新、应用创新、落地成效等维度综合评选出21项“银弹”优秀案例。会上由曹主任为优秀案例企业颁发奖杯。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/afde699d41e17307eece918130dd70e8.png\" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例列表（第一批）</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/21/21c8048e57aab4740338027277a60201.png\" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例奖牌颁发仪式（第一批）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4435379073923960be6128a911a17eb.png\" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例列表（第二批）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/30/30ef5dd9224d24f7c07e212a5c8fe243.png\" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例奖牌颁发仪式（第二批）</p><p></p><p></p><h2>主旨演讲分享</h2><p></p><p>&nbsp;</p><p>360智脑资深产品专家葛灿辉和360 Al数字人产品线负责人吕欣鸿带来“360集团大模型建设和应用”的主旨演讲，立足于生成式大模型应用，介绍了基于360智脑的Agent数字人在多种场景下辅助人类生活的应用，同时展示了结合数字人与其他AI&nbsp;Agent协同情境下，为生活提供的“五智”应用服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c0/c047a101d05113f884734d0e664ca728.png\" /></p><p>&nbsp;&nbsp;</p><p>360智脑资深产品专家葛灿辉 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f3/f32663f718767b51b1ca7a662c883ff0.png\" /></p><p></p><p>&nbsp;360 Al数字人产品线负责人吕欣</p><p>&nbsp;</p><p>中国信通院云大所人工智能部高级业务主管秦思思发表了“AI4SE行业洞察”的主题演讲。她从智能化软件工程落地场景分布、各环节AI技术应用比例及提效数据、智能开发工具应用成熟度和应用效果指标等维度解读了行业落地现状和趋势，并对中国信通院AI4SE的业务方向和布局进行了详细介绍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b0ac64c3d325490cc403269fcfa060e6.png\" /></p><p>&nbsp;</p><p>中国信通院人工智能研究中心高级业务主管 秦思思</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bd8f448e6241e2d184f8b180abdfd349.png\" /></p><p>&nbsp;</p><p>软件工程各阶段AI技术应用情况及提效数据</p><p>&nbsp;</p><p>本次论坛还邀请了中金公司算法专家雷涛、360集团产品专家苗广英、汇丰科技中国交付总监梁曜麟、蚂蚁集团Al算法专家单硕、百度资深研发工程师王初晴、阿里通义灵码产品负责人陈鑫、aiXcoder COO李力行进行主题分享，为大家呈现智能化软件工程领域的探索情况和实践效果。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d7ffd220babda9a9f08b49088bf11398.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b084b8d8b713b4929ad641140de0494.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>本次活动还举办了首站“AI4SE创新巡航”活动--走进360， AI4SE工作组成员单位代表们共同了参观360集团的展厅。期间，360集团向AI4SE工作组成员代表们展示了自己在人工智能融合软件工程领域的创新成果和应用案例。大家对如何利用人工智能提高企业开发效率等表现出很大兴趣，代表们也积极分享了自己的经验和观点。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac45ad78ad74f44e5cf6d8ef54c4365c.png\" /></p><p>&nbsp;</p><p>首期“AI4SE创新巡航”参观人员合影</p><p></p><p>&nbsp;</p><p>未来，中国信通院将持续推进大模型等AI技术赋能软件工程的产业研究、标准制定、评估测试、案例征集、产业活动等工作，与产学研用各方单位携手推进大模型工程化、产业化进程，共筑AI4SE可信生态。</p><p>&nbsp;</p><p>&nbsp;</p><p>联系人：</p><p>中国信通院人工智能研究中心</p><p>胡老师：17371328072(微信同号)</p><p>秦老师：13488684897(微信同号)</p><p>&nbsp;</p><p>写在最后：</p><p></p><p>中国人工智能产业发展联盟AI4SE工作组：</p><p>中国人工智能产业发展联盟（AIIA）智能化软件工程工作组（AI4SE工作组），于2023年9月正式成立，旨在进一步发挥生成式AI、大模型等人工智能技术在软件工程领域的潜力，充分释放AI赋予软件工程的价值。AI4SE工作组已吸纳100+成员单位，覆盖金融、电信、软件等诸多行业，欢迎更多企业加入，请联系胡老师、秦老师。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/8287676c415c23b29cc53e845fdd1000.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2024-01-30 17:51:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何加快大型遗留应用程序的开发速度？",
    "url": "https://www.infoq.cn/article/aPVYBUrtgqlGNsGqufAa",
    "summary": "<p></p><blockquote>本文深入探讨了大型企业和遗留应用程序开发速度缓慢的现象，并分析了导致这一问题的根本原因。随着项目规模的扩大，复杂性、跨团队依赖、安全与合规性要求、编码标准、会议文化、金丝雀部署策略、数据驱动决策以及非工作相关活动等因素，都在不同程度上影响了软件开发的效率。同时，本文也分享了一些实用的策略和建议，帮助开发者在这些挑战中找到平衡，提高生产力。无论你是初创公司的创始人还是大型企业的工程师，这篇文章都能为你提供宝贵的见解，帮助你更好地理解并应对软件开发中的这些挑战。</blockquote><p></p><p>&nbsp;</p><p>软件公司的规模多种多样，包括小型初创公司、中型企业和大型企业。初创公司通常具有灵活性和迅速响应的特点，而大型公司则在开发庞大应用程序时进展较为缓慢。这些大型应用可能由数百名开发者耗费数年甚至几十年的时间开发而成，例如亚马逊市场、AutoCAD 或各种操作系统。考虑到它们雇佣的工程师数量，这些产品发布新功能或修复错误需要相当长的时间。以苹果为例，尽管 iOS 可能有数千名开发者，但每个版本之间的差异相对较小。</p><p>&nbsp;</p><p>一些大型项目被称为 “遗留应用程序”，因为它们采用旧技术、积累了大量技术债务，难以进行修改。然而，进展缓慢、有传统感的项目并不仅限于企业、老系统，甚至不仅限于大型开发团队。许多现代应用程序也遭受相同的症状，即使它们是新的、采用最现代技术。可能存在复杂的业务逻辑，使得很难在不破坏其他部分的情况下进行修改。或者组织可能充斥着各种政策和官僚主义，导致工程师花费更多时间等待而非编写代码。又或者可能存在有缺陷的 CI/CD 流程，因此大量时间用于解决构建问题和冲突，而非开发新功能。许多问题都可能使项目变得像遗留项目一样。那么，是什么导致项目开发缓慢呢？这是否是任何老产品都难以避免的命运？我们是否能够预防或解决其中的一些问题？</p><p>&nbsp;</p><p>下面，我们来讨论一下大型和遗留应用程序开发过程缓慢的八个原因。</p><p></p><h2>原因一：复杂性</h2><p></p><p>&nbsp;</p><p>我们先来具体定义一下这种复杂性。到底是什么让这些项目变得复杂并减缓了开发呢？</p><p></p><h3>1、无人全知产品，因此容易出错</h3><p></p><p></p><p>在一个庞大的产品中，有太多的功能和微妙之处，没有人能够全面了解。因此，当你添加新功能或进行小改动时，很容易出错。有时可能只是一个失败的测试，但有时会导致客户投诉或在生产中发现一些问题。发现缺陷的阶段越晚，修复的时间就越长。最糟糕的情况是客户在生产中发现问题。当你拥有成千上万的功能和数千万的用户时，这种情况经常发生，而且会花费大量时间。大型项目工作的一大部分时间都花在修复缺陷上。</p><p></p><h3>2、复杂的架构和抽象</h3><p></p><p></p><p>随着时间的推移，产品经理提出了新功能。这些功能往往是草草加入的，而没有对系统进行重构，使得抽象层能够匹配新的功能集合。于是，这些添加、修改和修补是不断堆积的，使得代码库变得杂乱无章。当然，有一些技术手段可以最小化这种情况。你可以将项目拆分成微服务，分配更多时间进行重构，并实践 TDD 以设计一个松散耦合的系统。但现实与理论并不总是相符。当开发人员面临在 1 天内完成任务的选择，而不是额外请求两周的时间进行重构时，结果通常是选择前者。将这种情况乘以 1000，持续五年，你就得到了一个典型的企业软件项目。</p><p></p><h3>3、定制构建和自动化</h3><p></p><p></p><p>尽管理论上自定义自动化应该成为项目构建流程的一部分，例如代码检查工具、代码生成机制、文件签名自动化等，但实际上这些机制经常出现问题。工程师需要花费大量时间了解如何修复它们或绕过它们，尤其是对于新工程师，当项目足够大时，他们可能在 “新手” 状态下停留多年。</p><p></p><h3>4、工具溢出</h3><p></p><p></p><p>除了自定义构建流程之外，大型项目还伴随着大量的内部工具，包括但不限于自定义测试工具、自定义部署工具、代码检查工具、转译器、静态分析工具、配置控制器、CI/CD 系统、专有源代码控制系统、文档引擎、代码审查工具、安全工具、监控工具和许可证生成器。了解所有这些工具是一项全职工作，而且随着每个自定义工具的增加，新工程师的入职时间也会变得更长。此外，这些工具并不总是完全可用，经常需要停止开发来修复它们中的错误，或者等待其他人来解决问题。</p><p></p><h3>5、冲突</h3><p></p><p></p><p>随着开发人员数量的增加，项目中的冲突也会增多。这些冲突可能是实际的合并冲突，也可能是由另一个团队引入的错误。对于每一个冲突，都需要停止当前工作并解决冲突，通常需要与另一个团队合作，这可能导致一系列延迟（详见下一节）。</p><p>&nbsp;</p><p>有一些分支策略可以让你 “分支” 一段时间并在没有中断的情况下工作，但这并非解决所有问题的灵丹妙药。如果分支时间过长，合并将变得更加痛苦。如果分支时间太短，问题会更加频繁地出现。</p><p>&nbsp;</p><p></p><h3>6、较长的构建时间</h3><p></p><p></p><p>正如预期的那样，项目越大，构建时间就越长。在庞大的项目中，构建时间同样庞大。虽然构建时间可以通过各种方式进行优化，甚至在大型项目中可能有专门的团队负责此事，但优化构建时间通常不是首要任务，因此相对较少的资源被投入其中。此外，致力于一个不断变化的目标，一个正在积极开发的项目是困难的，而且优化的速度通常慢于新问题出现的速度。</p><p>&nbsp;</p><p>所以，我们如何处理这些问题呢？当一个应用程序变得足够庞大时，其中一些问题似乎是无法避免的。很多这些问题不管怎样都不会消失，但你可以在一定程度上减轻它们。以下是一些建议，来自我的个人经验：</p><p>&nbsp;</p><p>将内部开发流程视为一等公民。在处理构建优化、内部工具、CI/CD 以及整体开发者体验的团队中，将一些资深工程师置于前沿。将工程工具看待为公共工具。例如，如果你正在创建一个内部 API，就像对待为付费客户创建的 API 一样。这包括高可用性、出色的文档、向后兼容性等。打造出色的新工程师入职流程，并保持清晰而全面的文档。不要忽视技术债务（在合理范围内）。这可能涉及定期进行大规模重构努力或偶尔冻结功能以应对技术债务。允许工程师参与黑客马拉松和创意周。尽管很多项目将关注产品，但每家公司都有充满激情的工程师，他们愿意致力于改善构建时间或修复内部工具。</p><p>&nbsp;</p><p>我感觉我只是触及到了大型项目复杂性的冰山一角。我相信每家公司都有自己的痛点，如果你有一些有趣的案例，请随时在评论中分享。</p><p>&nbsp;</p><p>然而，大型项目也有可能加速开发的优势。当某个项目经过几十年的发展时，通常会包含许多能够让你的生活变得更轻松的系统。这可能包括有用的内部包、出色的测试环境、内部配置工具，或是定制的 IDE 扩展。例如，你可能可以很快地启动一个新应用程序，因为你可以访问具有无限计算能力的云账户、快速设置 CI/CD 的方式，以及可以自动使用的监控框架。</p><p></p><h2>原因二：跨团队依赖</h2><p></p><p>&nbsp;</p><p>一个大型项目可能拥有成千上万的开发人员、DevOps 工程师、项目 / 程序 / 产品经理等。通常，这些团队分布在世界各地。你不可能认识每个人，甚至离得很远。当你需要修改由另一个团队负责的内容时，你要么需要请求他们进行更改，要么自己去做并获得他们的批准。由于优先级并不总是一致，这可能需要一些时间。在不同的组织层次结构下，你可能会有完全不同的激励和目标。这意味着你需要进行一些政治活动，以便快速完成任务。建立关系，进行一些说服工作，在必要时升级，有时甚至需要乞求帮助。</p><p>&nbsp;</p><p>如果对方居住在另一个国家，问题就变得更加困难。文化差异可能会成为一个问题。例如，印度的工程师与西方国家的工程师非常不同。印度的工程师可能会同意某事，以取悦西方同事，即使他们有异议，没有完成任务，或者甚至一开始就没有正确理解。时区问题又是另一回事。如果一个团队正在工作，而另一个团队正在休息，每次消息往返将需要整整一天的时间。</p><p>&nbsp;</p><p>有一些方法可以帮助解决跨团队协作的问题。</p><p>&nbsp;</p><p>限制开发仅限于一个地点对于大公司来说是不切实际的。在不同的国家设立分支有很多好处，比如更便宜的劳动力、多样性、税收或者帮助在当地市场推广。但我建议在每个产品垂直领域尽量减少地点的数量。例如，如果你在谷歌工作，可以将 Android 的开发限制在印度和英国，搜索引擎限制在美国和以色列，YouTube 限制在加拿大。这样，大部分的协作可以在本地或尽量少的地方进行。</p><p>&nbsp;</p><p>鼓励一种乐于助人的文化。如果你激励开发人员相互帮助，他们就会这样相互帮助彼此。如果你鼓励一种竞争文化，他们可能不太愿意提供帮助，甚至可能会相互破坏彼此的努力。鼓励乐于助人的一种方法是将其纳入公司的使命宣言中，或者更好的做法是将乐于助人列为每位员工绩效评估目标之一。</p><p></p><p>此外，还有一些有趣的项目可以通过游戏化团队合作进行，比如 <a href=\"https://bonusly.com/\">bonusly</a>\" 和 <a href=\"https://www.kudos.com/\">Kudos</a>\"。例如，Bonusly 允许员工互相授予奖励积分，你可以向具有高积分的员工提供奖品或更高的奖金。在我看来，这也打开了一些腐败的大门。朋友们互相授予积分，没有实质性的功绩，形成了一种你给我，我给你的协议。</p><p></p><h2>原因三：安全、隐私与合规性</h2><p></p><p>&nbsp;</p><p>“钱多了，合规担忧就多了”。对于科技公司来说，这一点绝对正确。在初创公司，隐私、安全和合规性可能被视为次要问题，但在大型企业中，它们却是头等大事。为了保持合规性，大公司付出了极大的努力，而这是有代价的。下面，我们来详细探讨每个合规性类别所带来的代价。</p><p></p><h3>安全</h3><p></p><p>&nbsp;</p><p>安全问题究竟如何拖慢开发进程？</p><p>&nbsp;</p><p>在开发新功能时，大公司会实施安全政策，证明你的新功能不会引发漏洞。这可能需要填写包含一百个问题的表格，绘制多个 UML 图表，并经历一次安全审查，通常在每个步骤之间都有相当长的等待时间。新功能是最大的障碍，但简单的代码更改也可能增加漏洞。这意味着每当你更改与攻击面积附近的任何内容时，你都需要从认证的安全官员那里获得安全批准，这可能是向走廊尽头的某人简单提问，也可能是一个充满官僚主义的漫长过程。大公司热衷于他们的安全政策，其中一些最喜欢的是对社区项目的限制。如果你认为你可以使用你最喜欢的开源软件包，那就再想想吧。它安全吗？符合规定吗？许可证呢？是时候填写一些表格并安排委员会审查了。</p><p></p><h3>隐私</h3><p></p><p>&nbsp;</p><p>如果在当今社会观念中有一样比安全更重要的东西，那就是隐私。我们曾经目睹了近年来多起引起公众关注的与隐私有关的丑闻（Meta，我在看着你）。如今，每位开发者都必须了解数据分类、GDPR 规定以及公司的政策，其中肯定有很多条款。这可能意味着拉取请求需要经过隐私审查。或者你对客户数据（如日志和遥测）的访问受到限制。或者遥测数据只能从世界上的某一个区域获取。这真是复杂。在一个没有客户的初创公司工作肯定更容易一些。</p><p></p><h3>合规的其他要求</h3><p></p><p>&nbsp;</p><p>合规是指在各个方面遵守标准、法规和内部政策。安全和隐私是最关键的两个要求，此外还包括符合 SOC 和 HIPAA 等标准，遵守你所在地的法律规定，实现对客户的承诺，以及提高可访问性。不同的领域有不同的合规内容，但都不免要应对一些繁琐的程序。</p><p></p><h3>那么，解决方案是什么呢？</h3><p></p><p>&nbsp;</p><p>忽视隐私和安全问题并不是一个好的解决方案，假设这两者必须得考虑，以下是一些建议：</p><p>&nbsp;</p><p>在你自己的团队或小组中有工程师可以签署安全和隐私审查。让合规的官僚主义和流程尽可能简单。如果需要填写表格，让它们简短易懂。如果需要审查，提供简便的安排方式。</p><p></p><h2>原因四：编码规范与代码审查</h2><p></p><p>&nbsp;</p><p>当一家公司的程序员超过一定数量时，自然而然地会出现重大意见分歧。有些开发者喜欢使用制表符，而另一些喜欢使用空格。有些人喜欢在私有成员前加上 m_ 前缀，而另一些只加 _ 前缀，还有一些人不喜欢使用前缀。你可以允许每个人按照他们的喜好去做，但这将导致冲突，让没有人满意。更不用说，使用统一的风格编写的代码更容易阅读。</p><p>&nbsp;</p><p>一些公司有编码规范，比如谷歌。为了避免任何混淆，谷歌对一切都提供了规则。你可以查看这个简短的 <a href=\"https://google.github.io/styleguide/javaguide.html\">Java 编码指南</a>\"，或者如果你睡不着觉的话，可以看看这个长达 82 页的 C++ 编码指南。每次代码审查都应该遵循这些指南；这就是为什么谷歌有 <a href=\"https://www.pullrequest.com/blog/google-code-review-readability-certification/\">Certified Readability Reviewers</a>\"，而且你需要确保每个代码审查都得到至少一个这样的审阅者的签署。</p><p>&nbsp;</p><p>大公司的代码审查需要花费更多时间，你必须经历可读性、安全性和隐私性审查。此外，我发现大公司对代码审查的文化与小公司不同。在初创公司，你会匆忙创建一个 MVP 或者满足某个截止日期。而大公司则不会匆忙。质量和合规性被优先考虑，因为有数百万现有客户，每一个小错误都将代价高昂。</p><p>&nbsp;</p><p>但情况并不那么糟糕，有很多方法可以加速代码审查。其中一种方式是投资于静态代码分析工具，这些工具可以自动修复你的代码，使其符合公司的编码风格。例如，可以参考 <a href=\"https://eslint.org/\">ESLint</a>\" 用于 JavaScript 和 <a href=\"https://github.com/DotNetAnalyzers/StyleCopAnalyzers\">StyleCop</a>\" 用于 C#。另一个想法是，如果你有经过认证的审阅者，最好在附近有一位经过认证的人，并且最好是在你自己的团队内。</p><p></p><h2>原因五：会议</h2><p></p><p>&nbsp;</p><p>庞大的团队需要通过会议来进行协作。会议是决定项目时间表、资源分配、设计审查、状态会议、回顾等方面必不可少的。一些协作可以通过电子邮件或聊天完成，但没有什么能比得上实时会议。在复杂的讨论中，即时响应对话至关重要。它允许你迅速澄清，提出更多问题，并更容易挑战想法。会议通过肢体语言和语调传达更丰富的沟通细微差别。它们有助于建立关系，对于健康和高效的工作环境来说是无价的。</p><p>&nbsp;</p><p>如果你在一家大公司工作，你将参加很多会议。如果你是经理或高级个人贡献者，超过一半的时间都在参加会议。但有一些方法可以充分利用它，比如制定一个健康的会议文化，按时开始，制定议程，限定会议时间，并邀请仅必要的参与者。你可以发送会前准备文件，避免花费前 30 分钟解释问题。还可以设置主持人角色，主持人将允许每个人依次发言，保持主题的一致性。最后，每次会议后都要总结所做的决定，否则你可能需要另一次会议。</p><p></p><h2>原因六：金丝雀困扰</h2><p></p><p>&nbsp;</p><p>在初创公司，当你从零客户开始时，你可以通过这个方便的数字取得很大的进展。没有客户意味着没有投诉。要花一些时间才能拥有一些客户，更需要时间才能获得数十个客户，然后是数百个，接着是数千个，依此类推。在规模上的每一次跃升都伴随着新的保障措施。在现代软件公司，减少手动测试的趋势是实施自动化测试和金丝雀部署。金丝雀部署意味着每个版本更新首先部署给一小部分客户，然后逐渐扩大到更大的范围，再逐渐扩大，如果一切正常，最终会到达所有客户。</p><p>&nbsp;</p><p>这听起来不错，而且有很多优点，但任何事物都有代价。在这种情况下，代价就是开发时间。通常情况下，直到某个变更影响到所有的生产环境，你才能继续开发。原因可能是由于 API 更改或其他原因，你需要来自生产环境的遥测数据。除此之外，还会有偶尔的回滚。想象一下，你 3 周前的变更因为在后期发现了一个错误而被还原了。到了这个时候，你可能已经编写了许多依赖于那段旧代码的新代码，甚至部署了它。现在，你需要进行一次明智的修复，解开这个混乱，部署它，并等待三个星期以确保一切正常。</p><p>&nbsp;</p><p>在这个现实中，有一些提高生产力的小窍门。大多数公司使用功能标志，这意味着代码更改可以远程立即关闭。你可以将功能标志默认设置为 “关闭”，然后按照自己的步调激活你的功能，而不是与金丝雀部署同时进行。但这些只是缓解工具。每个变更仍然需要一些时间才能到达客户，你将花费时间等待。避免浪费时间的唯一方法是并行处理多个任务。</p><p></p><h2>原因七：数据驱动的决策</h2><p></p><p>&nbsp;</p><p>小公司更愿意冒险，推出创新性产品。拥有成功产品的成熟公司不想冒险失去这只金鹅。每一次变更都经过仔细考虑，决策依赖于客户反馈和遥测数据，而不是某位产品经理的直觉。</p><p>&nbsp;</p><p>这种流程是明智的，但也很慢，整理客户反馈或构建一个良好的 A/B 测试需要时间，在运行这样的测试时，首先需要提前仔细计划，因为如果出了什么问题，将浪费很多时间。通常情况下，还需要在代码中为测试添加新的遥测事件，并等待它们被部署。在初始准备之后，需要运行足够长的时间来获得具有统计学意义的结果，然后分析这些结果，而不会陷入已知的偏见或受到外部因素的影响。在这个过程中可能需要几周，甚至可能几个月。有时，A/B 测试没有产生确切的结果，需要进行另一次测试。其他时候，利益相关者不会批准任何建议的变更，你所有的努力都将白费。</p><p></p><h2>原因八：花在“工作”上的时间更少</h2><p></p><p>&nbsp;</p><p>公司越大，你花在实际工作上的时间就越少。我说的不是与工作相关的会议，而是与你日常工作无关的活动。这可能包括团队活动、集体活动、公司活动、市政厅会议、全员会议、安全培训、隐私培训、多元包容研讨会、公司文化日、编程马拉松、特邀演讲、欢乐时光、绩效评估、与直接经理的一对一会议、与直接上级的一对一会议，以及与你一起工作的其他五位利益相关者的偶尔一对一会议。这些可能很重要，但它们总会累积成大量时间，而不是用于软件开发。</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>在软件开发中，我们追求尽可能高的生产力，但考虑到这份清单，我认为大公司在很多方面做得相当不错。或者至少对它们而言是正确的。例如，初创公司可能不应花费太多时间讨论编码风格，因为等到他们摸清楚的时候，资金可能已经耗尽。而在大公司，由于有许多拥有不同意见的开发人员，花费大量时间编写指南是有道理的。清单上的其他项目，如金丝雀部署和特性门，也是如此。一个只有五个客户的初创公司不需要五个生产环境或复杂的特性门工具。但是，对于有一千万客户的应用，其中的问题可能会造成数十亿的损失，这是有必要的。</p><p>&nbsp;</p><p>有些事情对任何人都不太愉快。没有人喜欢漫长的构建时间。我们大多数人不喜欢涉及政治的事务。会议在一定程度上可能很好，但有时整整一天过去了，什么都没做成。</p><p>&nbsp;</p><p>尽管在传统应用程序或任何大公司中，开发可能会慢一些，但大公司也有很多优势。例如，薪酬更好，有更多交友的机会，出色的设施，很多人可以向他们学习，而且你有机会参与一些世界上最重要的产品的开发。</p><p>&nbsp;</p><p>作者简介：</p><p>&nbsp;</p><p>Michael Shpilt，在以色列工作的软件开发人员和博主，目前在微软任职。他的博客主要涵盖 .NET、Web 技术、性能、调试和职业等方面的内容。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://michaelscodingspot.com/slow-development-in-big-companies/</p>",
    "publish_time": "2024-01-30 18:15:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "讯飞星火正式发布语音大模型V3.5，数学、语义理解、代码能力持续提升",
    "url": "https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K",
    "summary": "<p>1月30日，科大讯飞举行星火认知大模型V3.5升级发布会。科大讯飞董事长刘庆峰、研究院院长刘聪正式发布基于首个全国产算力训练的讯飞星火V3.5，七大核心能力全面提升，数学、语言理解、语音交互能力超GPT-4 Turbo，重磅升级星火智慧黑板；正式发布星火语音大模型，首批37个主流语种效果超过OpenAI Whisper V3，赋能讯飞翻译机迎来全新升级助力更自由沟通，推动万物互联时代下客服、汽车、机器人等场景人机交互变革。</p><p></p><p>大模型应用加速落地，星火开发者超35万生态增长迅猛，打造个人应用赋能亿万用户；讯飞星火赋能千行百业，携手保险、银行、能源、汽车、通信等领域联合龙头企业，打造大模型赋能的应用标杆。此外，深度适配国产算力的讯飞星火开源大模型“星火开源-13B”首次发布，场景应用效果领先，昇思开源社区联合首发上线。</p><p></p><p>“通过这场发布会，我们展望一个充满希望、孕育着生长能量的春天。相信2024年一定可以实现星火燎原，通用人工智能不仅能够在中国各大领域深度而广泛应用，而且我们在源头技术创新、在大模型的底层能力上也会站上全新台阶。”刘庆峰说道。</p><p></p><p>基于全国产化算力平台训练 讯飞星火V3.5七大能力全面提升</p><p></p><p>2023年10月24日，科大讯飞携手华为，宣布首个支撑万亿参数大模型训练的万卡国产算力平台“飞星一号”正式启用。启用后的90多天里，讯飞星火步履不停，基于“飞星一号”，启动了对标GPT-4的更大参数规模的大模型训练，带来了1月30日这场讯飞星火V3.5升级发布。</p><p></p><p>首个基于全国产算力训练的全民开放大模型讯飞星火V3.5在语言理解、文本生成、知识问答、逻辑推理、数学能力、代码能力和多模态能力七个方面进行了全面升级。其中语言理解、数学能力超过GPT-4 Turbo，代码达到GPT-4 Turbo 96%，多模态理解达到GPT-4V 91%。</p><p>&nbsp;</p><p>“在更好的数据、更强的人机协同训练中，我们不能只看单个的‘原子’能力，而是要以技术进步来解决真实世界的刚需。”</p><p></p><p>技术进步如何为人类生活带来真正有效的解决方案？刘庆峰从全新赋能万物互联时代人机交互、全新赋能知识学习与内容创作、全新提升数智化生产力三个方面，向现场观众展示讯飞星火V3.5的能力提升。</p><p></p><p>大模型全新赋能万物互联时代人机交互体验，超拟人合成效果惊艳。讯飞星火V3.5不仅在语义理解、指令跟随和多轮对话的演示中，展现出优异的能力，更是在情绪感知和拟人合成方面表现出色。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b2/c6/b2440ddf18c66124466e5014068d5ec6.png\" /></p><p></p><p>“听说今年尔滨特别火，作为南方小土豆还挺想去玩一下的。要不你用东北话介绍下有啥好玩的呗？”</p><p></p><p>在实操演示环节，科大讯飞研究院院长刘聪和讯飞星火V3.5现场互动，讯飞星火V3.5一口地道的东北话逗乐现场观众。幽默之余，讯飞星火V3.5快速为刘聪定制了旅游攻略，还催促他赶快买票，春运机票紧俏。它不仅能够帮助用户带来解决方案，还能作为“知冷知热”的朋友，带入情绪互动，超高的拟人度让大模型更具人情味。</p><p></p><p>大模型全新赋能知识学习与内容创作。讯飞星火V3.5对年终总结计划、述职PPT、活动策划、政策问答等任务“信手拈来”。基于此，科大讯飞推出了可以一键快速自动生成文档和PPT的办公产品——讯飞智文，这款产品主要功能有文档一键生成、AI撰写助手、多语种文档生成、AI自动配图、多种模板选择、提供演讲备注功能等。刘聪现场演示了使用讯飞智文制作的“合肥市2024年春节旅游推广策略”PPT，短时间内超20页内容丰富的PPT一气呵成，此等“打工神器”获得台下一阵掌声点赞。</p><p></p><p>大模型还能够结合外部知识进行合理拓展，做到“旁征博引”。要素抽取、问题生成等能力的进步，能够帮助每个人以测助学形成思考的闭环，在越来越多的服务领域及学习知识场所中产生更多好用的智能体。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a1/87/a15e84dbba6725bfb3e147f46f2edc87.png\" /></p><p></p><p>大模型全新提升数智化生产力，可以更好助力科研、工业等民生刚需领域提质增效。随着数学和推理能力的升级，多模态能力逐步进阶，讯飞星火V3.5在视觉问答、联想推理等方面实现了“高分”应对，理解更加精确，表述也更好。</p><p></p><p>“讯飞星火V3.5能力的提升，已经达到了量质齐飞的关键点。”刘庆峰表示，2024年讯飞星火认知大模型的应用，一定会在越来越多的场景和领域中大放异彩。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/7a/21/7a01200a0c04da4yy21145946887fe21.png\" /></p><p></p><p>首先赋能的场景，就是教育领域——科大讯飞此次重磅推出星火智慧黑板，具备多模态理解与推荐、全自然交互、虚拟人辅学、智慧化录课与分享四大功能。发布现场，爱因斯坦出现在星火智慧黑板上，用“吸力巨大的吸尘器”来比喻黑洞，深入浅出地为现场观众解释“黑洞是什么”。在立体几何等知识的教学中，星火智慧黑板通过解构立方体，将枯燥的理论视觉化，教学课堂更加生动有趣。</p><p></p><p>“黑板不再是简单的板书工具，而是跃迁成为教师的AI助手。”刘庆峰提及，在最近的实际展示与使用中，升级后的星火智慧黑板得到了师生、教育专家和业务合作伙伴们的高度评价。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/03/1d/033945ff0cbef2f74a5d2654fd689d1d.png\" /></p><p></p><p>为什么人工智能的每次进步都有对教育领域的赋能？刘庆峰解释，通用人工智能作为能够改变世界生产生活方式的全新技术，可以推动人类进步。“而教育是人类进步的根本，关乎每一位个体，是真正的全民刚需。”</p><p>&nbsp;</p><p>正式发布星火语音大模型</p><p>首批37个主流语种效果超过OpenAI Whisper V3</p><p></p><p>“科大讯飞从创业之初的梦想和使命，就是要实现沟通无障碍。25年了，我们目标和梦想一天都没有变。”</p><p>智能语音起家的科大讯飞，创业25年来在这条赛道上一路驰骋，持续走在世界前列。2006-2019年，连续十四年荣获国际语音合成大赛冠军；2016-2023年，连续4届获得国际多通道语音分离和识别比赛CHiME冠军；2021-2023年，连续三年获得国际语音翻译比赛IWSLT冠军……此外，还参与承建首批国家新一代人工智能开放创新平台、语音及语言信息处理国家工程研究中心等，在语音领域的持续积累。</p><p></p><p>“大模型带来了语音技术发展的全新机会。”刘庆峰强调，让机器具备学习、推理和决策的能力，就是认知大模型要干的主要工作。“简单来说，借助大模型，我们让一段语音具备更加丰富的属性，有语种、有内容、有韵律、有音色，还有情绪。”</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/21/38/21c65e42f929c144f3fb181bb0a44c38.png\" /></p><p></p><p>他介绍，星火语音大模型效果国际领先，中文、英语、法语、俄语等首批37个主流语种的语音识别效果超过OpenAI Whisper V3，而在多语种语音合成方面，星火语音大模型的首批40个语种拟人度超83%。</p><p></p><p>“通过星火语音大模型的评测效果，我们非常自豪地告诉大家，科大讯飞继续保持了全球领先的水平。”</p><p>在此优势下，语音大模型的能力升级也应用于C端硬件产品。会上，刘庆峰介绍了搭载语音大模型的讯飞翻译机，即将上线多语种自动识别和增强式翻译两个重要功能，分别于今年1月底和3月中旬完成升级。多语种自动识别让国际沟通更加便捷，增强式翻译技术让翻译机化身AI翻译助手。据介绍，此次讯飞翻译机多语自动识别升级，将支持35种语言，为跨语言沟通提质增效；增强式翻译提供中英双语服务，让跨语言交流更加省心出彩。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/5a/aa/5afedc57b8a323b9c2db8fc602d64baa.png\" /></p><p></p><p>星火语音大模型不止助力国际沟通，还能“百搭”更多场景，赋能实际应用。刘庆峰介绍，在汽车、客服、家庭、陪伴机器人等场景中，星火语音大模型还有更多用武之地，带来人机交互变革。如赋能汽车，智能驾舱、智能座舱、智能导航、音乐控制等交互体验将进一步优化；陪伴机器人、导购机器人、辅诊机器人、智能家居、穿戴式设备等产业也将随着语音大模型的赋能进一步被引爆。</p><p>&nbsp;</p><p>讯飞星火赋能亿万用户 加速赋能千行百业</p><p></p><p>“大模型发展，应用才是硬道理。”刘庆峰强调。讯飞星火自去年5月诞生以来，不断迭代升级其大模型能力，深耕千行百业的应用刚需。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/4e/00/4ebde0333c9584b34a1063a0e6796500.png\" /></p><p></p><p>大模型未来，要从教育开始做起。让孩子站在人工智能肩膀上学习的讯飞AI学习机，能够提高学习效率、激发学习兴趣、提升综合素养，产品用户净推荐值（NPS）持续保持行业第一，获得2023年京东&amp;天猫双11销售额冠军。</p><p></p><p>大模型也在持续赋能办公硬件，自去年5月讯飞星火正式发布起，有超过60万讯飞智能办公本、智能录音笔用户累计调用语篇规整、会议纪要、自动写稿等能力650万次，让工作更高效。市场用销量表达好评，产品蝉联京东&amp;天猫双11品类销售额冠军。</p><p></p><p>除了硬件产品，软件应用也不在话下。星火赋能个人应用打造，目前基于讯飞听见、讯飞星火APP、讯飞输入法等应用，已累计赋能亿万用户。在星火大模型赋能下，越来越多的用户拥有专业AI助手。一位浙江的小学老师，使用智能问答高效完成教学方案设计，已累计使用APP262天、超2300篇；一位湖南的“情感咨询师”，通过大模型咨询人际关系等生活中的烦恼，现已累计使用161天、超9500次……这样的例子还有很多，大模型正切实走入日常生活。</p><p></p><p>打造内容创作平台，星火赋能百万内容生产者。音视频创作工具“讯飞智作”自去年8月15日发布以来，新增了21万会员用户，生成了160万音视频内容；图文创作工具“星火内容运营大师”自去年10月24日发布以来，已服务了5000+企业，生成了超150万篇文章，助力内容创作者高效生产。</p><p>&nbsp;</p><p>今天，在讯飞开放平台之上，大模型总开发者超35万，其中企业开发者超22万，开发者数增长迅猛，大模型应用加速落地，持续夯实第一开发者生态。“所以我非常自豪，讯飞星火大模型的开发者数量，在中国是最高的，而且用户口碑非常好。”刘庆峰说道。</p><p></p><p>在此基础上，科大讯飞也在加速赋能各个行业的头部企业客户。现场，刘庆峰提及了讯飞与奇瑞的合作。“奇瑞是安徽的骄傲，连续十几年都是中国汽车出口量的第一名。”刘庆峰介绍，奇瑞汽车出口的国家覆盖英语、俄罗斯语、西班牙语、阿拉伯语、葡萄牙语等数十个语种，汽车中应用的智能语音交互技术全都由科大讯飞提供。“我们用大模型全力支撑中国汽车的全球化出海战略，要让汽车变得更聪明、更智能、更面向未来。”</p><p></p><p>讯飞星火大模型加速赋能千行百业，除了汽车行业，还与太平洋保险合作，通过星火太平洋计划赋能内部知识服务、办公、审计、展业等，打造数字劳动力在保险领域的应用标杆；联合交通银行，赋能客服、展业、办公、研发等，重点打造银行领域的代码能力应用标杆；与国家能源集团联合，赋能煤炭、电力、运输、化工等方面，打造央企集团一体化联动的大模型应用标杆等。</p><p>&nbsp;</p><p>科大讯飞与中国移动强强联合，全面助力中国移动数智化转型。在赋能“5G新通话”方面，中国移动携手讯飞星火推出5G新通话创新应用“商务速记”，能够实现通话语音同步纪要，关键事项精准提炼，且无需下载APP，所有手机都支持。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/76/a0/766bc5509da6bf2be915fc35d609c2a0.png\" /></p><p></p><p>利用大模型赋能千行百业，把大模型技术的创造力转化为促进产业高质量发展的新质生产力，正在成为行业的共同选择。发布会现场，还举行了“大模型+5G新通话商务速记应用体验”启动仪式，科大讯飞高级副总裁江涛和中国移动市场部副总经理孙世伟共同参与，5G新通话创新应用“商务速记”向广大移动用户开放体验。</p><p>&nbsp;</p><p>刘庆峰表示，为加速企业大模型应用价值落地，讯飞星火V3.5将提供全栈自主可控的优化套件。基于全国国产化算力打造的讯飞星火V3.5支持异构算力调度，可实现行业大模型训练提效90%，支持23个企业应用场景的敏捷优化。</p><p></p><p>讯飞星火大模型的全面赋能，无疑将为各行业的数字化转型提供强大的技术支持，引领新一轮的数字化浪潮。</p><p>&nbsp;</p><p>星火开源-13B正式发布 深度适配国产算力</p><p>&nbsp;</p><p>共享源代码、开发出更好的软件，是广大开发者、高校、企业自主研发热衷于开源的关键原因，也是共建第一开发者生态的重要途径。会上，首个基于全国产化算力平台”飞星一号”的开源大模型——星火开源-13B正式发布。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f3/f7/f32a7d28751247a2451720a3d80364f7.png\" /></p><p></p><p>本次开源拥有130亿稠密参数（13B），包含基础模型iFlytekSpark-13B-base、精调模型iFlytekSpark-13B-chat，开源了微调工具iFlytekSpark-13B-Lora、人设定制工具iFlytekSpark-13B-Charater。学术企业研究可以基于全栈自主可控的星火优化套件，更便利地训练自己的专用大模型。</p><p></p><p>刘庆峰透露，星火开源大模型在技术上形成了差异化优势。星火开源-13B在多项知名公开评测任务中名列前茅，在文本生成、语言理解、文本改写、行业问答、机器翻译等企业典型场景中，通过对学习辅助、语言理解等领域的深入研究和优化，实用性大幅提升，在处理复杂的自然语言任务时更加得心应手。</p><p></p><p>基于“飞星一号”训练，星火开源大模型全栈国产适配优化，简单易用，场景应用效果领先，训练策略针对昇腾算力极致优化，训练效率达A100的 90%。这不仅是对昇腾AI硬件的进一步深度优化，也展示了国产算力在追赶国际先进水平方面的决心和能力。</p><p></p><p>开源只有更多的场景落地，才能更好地增进生态合作。目前，华为昇思开源社区已正式上架星火大模型开源版-13B，面对学术、企业研究完全免费，增进学术合作的同时，增进产业探索。</p><p>&nbsp;</p><p>展望2024年星火发展，这三点很重要</p><p></p><p>“在今天的大模型时代，我们绝不能只用开源模型做应用落地，在通用大模型的底座上一定要有国家队站出来。”刘庆峰强调。</p><p></p><p>当前，发展通用人工智能是我们必须要做的事情，否则工业、科研、民生等各个专用领域都会极大落后于世界，而中国是世界唯一有望成为智慧涌现第二极的国家。</p><p></p><p>展望2024年讯飞星火大模型发展，刘庆峰指出三点：“首先，一定要在通用大模型的底层能力上持续对标国际最先进水平，从算法研究包括更小的算力上做出相对更优效果。”我们要清醒理智看到差距，当前在小样本快速训练、多模态深度学习训练、超复杂深度理解等领域距离GPT-4的最好水平还有差距，讯飞星火有信心在今年上半年赶上GPT-4目前最好水平。“今天的通用大模型并不一定代表人工智能的全部未来，还有很多创新要做，如脑科学互动、对抗网络的深度连接等需要整个创新的生态，但我们一定要有勇气、有期许走在最前列。”刘庆峰指出。</p><p></p><p>“第二，2024年要真正让大模型量质齐飞，不仅是行业应用，还要在很多关键技术创新上联动大模型，中国企业界、科学界有信心实现超越。”</p><p></p><p>“第三，高楼大厦要建立在安全可控的平台之上，我们要实现自主可控平台上的生态繁荣。”刘庆峰表示，自己有信心，能够实现从算法、数据、应用场景到算力，构建一个完全自主可控的繁荣的人工智能生态。</p><p>“通用人工智能大未来刚刚开启，它必将深刻改变世界的未来，需要顶天立地的长期主义精神。”顶天，指的是希望源头核心技术在底层大模型能力上向国际最先进的能力看齐；立地，指的是大规模实现产业化。而这也是科大讯飞自成立起就一以贯之的信仰，需要在长期主义精神指引下推进各方面工作。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/30/9d/3000846132e434c65fa0c9a1ba1f829d.png\" /></p><p></p><p>会上，新书《星火相传》正式发布。“这本书讲述了科大讯飞创业的很多真实故事，但更是讲述了科大讯飞奠基人、我的导师王仁华老师的教书育人经历，希望能够给讯飞人、给讯飞的合作伙伴、包括有志于做高校科技成果转化、有志于判断人工智能未来的朋友们以更多启发。”刘庆峰说道。</p><p></p><p>【活动推荐】</p><p></p><p>目前，极客邦科技正在策划 2024 年 6 月 14-15 日 <a href=\"https://archsummit.infoq.cn/2024/shenzhen/\">ArchSummit 架构师峰会深圳站</a>\"会议，本次会议将围绕“智能进阶. 架构重塑”大主题方向，探讨在 AI 浪潮下，架构设计如何匹配智能化趋势，以适应日益复杂的业务需求。会议上将讨论大模型、AI 运维、数据架构、应用架构、业务架构、微服务、高可用以及数字化转型等多个层面的话题，感兴趣的可以点击 ArchSummit 会议官网。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/13/04/1366c74a2e873bfb936e4f8cd3ae6c04.jpg\" /></p><p></p>",
    "publish_time": "2024-01-30 19:22:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]