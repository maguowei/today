[
  {
    "title": "使用Strimzi提高Kafka集群的安全性",
    "url": "https://www.infoq.cn/article/CpfvECIb5gWdditBBYy7",
    "summary": "<p>在本系列的<a href=\"https://www.infoq.cn/article/V4kqPgrQlqqPgWQuMmDP\">第3部分</a>\"，我们学习了双写问题以及如何使用变更数据捕获模式解决这些问题，特别是使用<a href=\"https://debezium.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2Nzg3NTg3NzAsImZpbGVHVUlEIjoiRzNHOWJOVzN3YW82U1lEbCIsImlhdCI6MTY3ODc1ODQ3MCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.qjOJBovg9TEEKFV_u47LWZ8lfVkqnISLgq7qNqCjVWg\">Debezium</a>\"读取数据库中所做的变更（通过事务日志）并将它们填充到Kafka主题中。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f98928c3fd3e9385d65df0502eaba7b3.webp\" /></p><p></p><p>在本系列的<a href=\"https://www.infoq.cn/article/ElNtSM5ISobpMB8fMC0j\">第4部分</a>\"，我们将示例又向前推进了一步，将应用程序从本地开发环境部署到Kubernetes（生产环境）中。我们使用<a href=\"https://strimzi.io/\">Strimzi</a>\"来部署和配置Kafka和Debezium。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/183604d98c9728dcff64ac409874023c.webp\" /></p><p></p><p>但总的来说，我们忽略了一个重要的东西——当时我们没有把它简化，但它却非常重要——安全性问题。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/28/281cb84835d07fcba18c2e0947eec2f4.webp\" /></p><p></p><p>如何在不直接将用户名/密码硬编码在部署文件中的情况下保护MySQL实例。如何使用Strimzi在Kafka集群中添加authn。如何配置Debezium，以便对Kafka和MySQL实例进行安全身份验证。在本文中，我们将通过保护在上一篇文章中开发的应用程序（使用Debezium Server方法）来回答所有这些问题。</p><p></p><h2>Kubernetes</h2><p></p><p>我们需要一个安装了Strimzi的Kubernetes集群。我们在本系列的第4部分中对此进行了介绍，如果你要重用它，需要删除应用程序、MySQL数据库、Kafka集群和Debezium实例。</p><p></p><p>重要提示：如果第4部分中使用的集群还在，需要执行下面的步骤。如果集群已经被删除，请从介绍如何删除集群的部分之后继续阅读。</p><p></p><p>在终端窗口执行如下命令来删除它们：</p><p></p><p><code lang=\"plain\">kubectl delete deployment movie-plays-producer-debezium-server -n kafka\nkubectl delete service movie-plays-producer-debezium-server -n kafka\nkubectl delete -f mysql-deployment.yaml -n kafka\nkubectl delete -f debezium-kafka-connector.yaml -n kafka\nkubectl delete -f debezium-kafka-connect.yaml -n kafka\nkubectl delete -f kafka.yaml -n kafka\n</code></p><p></p><p>重要提示：如果你还没有Kuberntes集群，则只需要执行下面的步骤。</p><p></p><p>如果集群已经被销毁，请按照指示创建一个新的集群。在终端窗口中运行以下命令：</p><p></p><p><code lang=\"plain\">minikube start -p strimzi --kubernetes-version='v1.22.12' --vm-driver='virtualbox' --memory=12096 --cpus=3\n\nkubectl create namespace kafka\n\nkubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka\n</code></p><p></p><p>执行下面的命令验证Operator是否安装正确：</p><p></p><p><code lang=\"plain\">kubectl get pods -n kafka\n\nNAME                                        READY   STATUS    RESTARTS   AGE\nstrimzi-cluster-operator-597d67c7d6-ms987   1/1     Running   0          4m27s\n</code></p><p></p><p>等待Operator运行并准备就绪。</p><p></p><p>此时，我们可以开始使用身份验证和授权（而不是匿名访问）来安装所有组件。</p><p></p><h2>MySQL</h2><p></p><p>在前一篇文章中，我们部署了MySQL实例，将用户名/密码作为环境变量硬编码在部署文件中：</p><p></p><p><code lang=\"plain\">env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: alex\n    - name: MYSQL_DATABASE\n      value: moviesdb\n    - name: MYSQL_USER\n      value: alex\n    - name: MYSQL_PASSWORD\n      value: alex\n</code></p><p></p><p>我们创建一个Kubernetes Secret来存储这些敏感数据。Kubernetes 密钥文件中的数据必须采用base64格式编码。alex的base64编码为YWxleA==。</p><p></p><p>要生成这个值，执行下面的命令：</p><p></p><p><code lang=\"plain\">echo -n 'alex' | base64\nYWxleA==\n</code></p><p></p><p>在mysql-secret.yaml文件中填入编码的密钥：</p><p></p><p><code lang=\"plain\">apiVersion: v1\nkind: Secret\nmetadata:\n name: mysqlsecret\ntype: Opaque\ndata:\n mysqlrootpassword: YWxleA==\n mysqluser: YWxleA==\n mysqlpassword: YWxleA==\n</code></p><p></p><p>将其应用到集群：</p><p></p><p><code lang=\"plain\">kubectl apply -f mysql-secret.yaml -n kafka\n</code></p><p></p><p>然后更新MySQL部署文件，使用value中的secretKeyRef字段读取在上一步中创建的密钥：</p><p></p><p><code lang=\"plain\">apiVersion: v1\nkind: Service\nmetadata:\n name: mysql\n labels:\n   app: mysql\nspec:\n ports:\n   - port: 3306\n selector:\n   app: mysql\n clusterIP: None\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: mysql\n labels:\n   app: mysql\nspec:\n selector:\n   matchLabels:\n     app: mysql\n strategy:\n   type: Recreate\n template:\n   metadata:\n     labels:\n       app: mysql\n   spec:\n     containers:\n     - image: mysql:8.0.30\n       name: mysql\n       env:\n       - name: MYSQL_ROOT_PASSWORD\n         valueFrom:\n           secretKeyRef:\n             key: mysqlrootpassword\n             name: mysqlsecret\n       - name: MYSQL_DATABASE\n         value: moviesdb\n       - name: MYSQL_USER\n         valueFrom:\n           secretKeyRef:\n             key: mysqluser\n             name: mysqlsecret\n       - name: MYSQL_PASSWORD\n         valueFrom:\n           secretKeyRef:\n             key: mysqlpassword\n             name: mysqlsecret\n       ports:\n       - containerPort: 3306\n         name: mysql\n</code></p><p></p><p>在secretKeyRef中，我们指定了密钥名称。在本例中，我们在mysql-secret.yaml文件中指定的是mysqlsecret。</p><p></p><p>将MySQL实例部署到Kubernetes集群：</p><p></p><p><code lang=\"plain\">kubectl apply -f mysql-deployment.yaml -n kafka\n</code></p><p></p><p>我们可以通过导出环境变量来验证注入的密钥是否正确。首先，我们获取Pod的名称：</p><p></p><p><code lang=\"plain\">kubectl get pods -n kafka\nNAME                                        READY   STATUS    RESTARTS   AGE\nmysql-7888f99967-4cj47                      1/1     Running   0          90s\n</code></p><p></p><p>然后在终端窗口中运行下面的命令：</p><p></p><p><code lang=\"plain\">kubectl exec -n kafka -ti mysql-7888f99967-4cj47 /bin/bash\n\nbash-4.4# export\ndeclare -x GOSU_VERSION=\"1.14\"\ndeclare -x HOME=\"/root\"\ndeclare -x HOSTNAME=\"mysql-7888f99967-4cj47\"\ndeclare -x KUBERNETES_PORT=\"tcp://10.96.0.1:443\"\ndeclare -x KUBERNETES_PORT_443_TCP=\"tcp://10.96.0.1:443\"\ndeclare -x KUBERNETES_PORT_443_TCP_ADDR=\"10.96.0.1\"\ndeclare -x KUBERNETES_PORT_443_TCP_PORT=\"443\"\ndeclare -x KUBERNETES_PORT_443_TCP_PROTO=\"tcp\"\ndeclare -x KUBERNETES_SERVICE_HOST=\"10.96.0.1\"\ndeclare -x KUBERNETES_SERVICE_PORT=\"443\"\ndeclare -x KUBERNETES_SERVICE_PORT_HTTPS=\"443\"\ndeclare -x MYSQL_DATABASE=\"moviesdb\"\ndeclare -x MYSQL_MAJOR=\"8.0\"\ndeclare -x MYSQL_PASSWORD=\"alex\"\ndeclare -x MYSQL_ROOT_PASSWORD=\"alex\"\ndeclare -x MYSQL_SHELL_VERSION=\"8.0.30-1.el8\"\ndeclare -x MYSQL_USER=\"alex\"\ndeclare -x MYSQL_VERSION=\"8.0.30-1.el8\"\ndeclare -x OLDPWD\ndeclare -x PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\ndeclare -x PWD=\"/\"\ndeclare -x SHLVL=\"1\"\ndeclare -x TERM=\"xterm\"\n</code></p><p></p><p>现在可以退出容器：</p><p></p><p><code lang=\"plain\">exit\n</code></p><p></p><p>现在，MySQL数据库的凭证使用的是Kubernetes Secret配置，这比在部署文件中硬编码要好得多。应用程序也需要修改，因为它现在需要从Secret读取凭证，而不是读取配置文件中的静态凭证。</p><p></p><h2>Move Play Producer Debezium</h2><p></p><p>数据库用户名和密码硬编码在application.properties文件中，如果应用程序能够在部署到Kubernetes时自动配置用户名和密码，那就更好了。</p><p></p><p>一种方法是将密钥作为环境变量注入到应用程序Pod中，就像部署MySQL那样。例如，对于密码，部署文件的env部分可能是这样的：</p><p></p><p><code lang=\"plain\">- name: MYSQL_PASSWORD\n  valueFrom:\n    secretKeyRef:\n      key: mysqlpassword\n      name: mysqlsecret\n</code></p><p></p><p>现在更新application.properties文件，从环境变量中获取密码：</p><p></p><p><code lang=\"plain\">%prod.quarkus.datasource.password=${mysql-password}\n</code></p><p></p><p>虽然这样做可以奏效，但将密钥作为环境变量并不是最安全的做法，因为任何一个可以列出环境变量的人都可以很容易地窃取它们。</p><p></p><p>虽然这样做可以奏效，但将密钥作为环境变量并不是最安全的做法，因为任何一个可以列出环境变量的人都可以很容易地窃取它们。</p><p></p><p>Quarkus有一个<a href=\"https://quarkus.io/guides/kubernetes-config\">kubernetes-config</a>\"扩展，应用程序可以用它直接从Kubernetes API服务器读取Kubernetes ConfigMaps和Secrets。通过这种方式，密钥可以安全地从Kubernetes集群传到应用程序中，而不需要任何中间步骤，如将它们作为环境变量传入或作为卷挂载。</p><p></p><h4>Kubernetes配置扩展</h4><p></p><p>首先要做的是注册kubernetes-config扩展。打开pom.xml文件，并添加以下依赖项：</p><p></p><p><code lang=\"plain\">\n  io.quarkus\n  quarkus-kubernetes-config\n\n</code></p><p></p><p>然后，让应用程序直接从Kubernetes API读取Kubernetes Secrets（在我们的例子中，Secret的名字是mysqlsecret）。</p><p></p><p>打开src/main/resources/application.properties，加入下面的内容：</p><p></p><p><code lang=\"plain\">%prod.quarkus.kubernetes-config.secrets.enabled=true                           \nquarkus.kubernetes-config.secrets=mysqlsecret\n</code></p><p></p><p>然后更新quarku.datasource.username和quarku.datasource.password属性，读取mysqlsecret Secret中的mysqluser和mysqlpassword。</p><p></p><p>在application. properties文件中更新这些属性：</p><p></p><p><code lang=\"plain\">%prod.quarkus.datasource.username=${mysqluser}\n%prod.quarkus.datasource.password=${mysqlpassword}\n</code></p><p></p><p>这两个属性分别使用mysqlsecret Secret中的值进行了赋值。</p><p></p><p>由于读取Kubernetes Secrets需要与Kubernetes API Server发生交互，因此，当集群启用了RBAC（基于角色的访问控制）时，用于运行应用程序的ServiceAccount必须具有适当的访问权限。</p><p></p><p>这两个属性分别使用mysqlsecret Secret中的值进行了赋值。</p><p></p><p>由于读取Kubernetes Secrets需要与Kubernetes API Server发生交互，因此，当集群启用了RBAC（基于角色的访问控制）时，用于运行应用程序的ServiceAccount必须具有适当的访问权限。</p><p></p><p>因为我们在前一篇文章中注册了<a href=\"https://quarkus.io/guides/deploying-to-kubernetes\">Kubernetes扩展</a>\"，所以自动生成了所有必要的Kubernetes资源，所以现在不需要做任何事情。</p><p></p><p>现在在终端窗口运行下面的命令部署应用程序：</p><p></p><p><code lang=\"plain\">./mvnw clean package -DskipTests -Dquarkus.kubernetes.deploy=true\n\n…\n[INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Deploying to kubernetes server: https://192.168.59.104:8443/ in namespace: kafka.\n[INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Service movie-plays-producer-debezium-server.\n[INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Deployment movie-plays-producer-debezium-server.\n[INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 9537ms\n</code></p><p></p><p>为了验证部署是否正确，我们检查Pod的日志，确保没有出现错误，并且SQL语句执行是正确的：</p><p></p><p><code lang=\"plain\">kubectl get pods -n kafka\n\nNAME                                                         READY   STATUS      RESTARTS   AGE\nmovie-plays-producer-debezium-server-auth-7cc69fb56c-nc8tx   1/1     Running     0          44s\n\n\n\nkubectl logs movie-plays-producer-debezium-server-auth-7cc69fb56c-nc8tx -n kafka\n\n__  ____  __  _____   ___  __ ____  ______\n --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/\n -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\\ \\\n--\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/\n2022-08-21 21:00:41,277 INFO  [io.deb.out.qua.int.AdditionalJaxbMappingProducerImpl] (main) Contributed XML mapping for entity: io.debezium.outbox.quarkus.internal.OutboxEvent\n\n…\nHibernate:\n\n    create table Movie (\n       id bigint not null,\n        director varchar(255),\n        genre varchar(255),\n        name varchar(255),\n        primary key (id)\n    ) engine=InnoDB\nHibernate:\n\n    create table OutboxEvent (\n       id binary(255) not null,\n        aggregatetype varchar(255) not null,\n        aggregateid varchar(255) not null,\n        type varchar(255) not null,\n        timestamp datetime(6) not null,\n        payload varchar(8000),\n        tracingspancontext varchar(256),\n        primary key (id)\n    ) engine=InnoDB\n</code></p><p></p><p>在下图中可以看到我们做了安全性保护的部分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/84/84dbad7e319fc25fb996eb441fd1ec1b.webp\" /></p><p></p><p>现在，应用程序正在运行中，MySQL凭证也被保护起来了，下面我们继续为Kafka和Debezium提供保护。</p><p></p><h2>Kafka</h2><p></p><p>到目前为止，我们已经部署了一个开放的Kafka集群，没有启用身份验证或授权逻辑。</p><p></p><p>Strimzi支持使用以下<a href=\"https://strimzi.io/docs/operators/latest/configuring.html#assembly-securing-kafka-str\">认证机制</a>\"来部署Kafka集群：</p><p></p><p>SASL SCRAM-SHA-512；TLS客户端认证；OAuth 2.0基于令牌的身份验证。由于Strimzi Operator已经安装在Kubernetes集群中了，所以我们可以使用Kafka自定义资源。Kafka资源配置了集群部署，并启用了TLS客户端身份验证。</p><p></p><p>Strimzi可以在listeners中设置监听器，使用mTLS作为通信协议（tls=true）和认证方法类型（authentication字段）。</p><p></p><p>创建一个叫作kafka.yaml的新文件，使用下面的内容来配置一个安全的Kafka：</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n name: my-cluster\n namespace: kafka\nspec:\n kafka:\n   version: 3.2.0\n   replicas: 1\n   listeners:\n     - name: demo\n       port: 9092\n       type: internal\n       tls: false\n     - name: secure\n       port: 9093\n       type: internal\n       tls: true\n       authentication:\n         type: tls\n   authorization:\n     type: simple\n   config:\n     offsets.topic.replication.factor: 1\n     transaction.state.log.replication.factor: 1\n     transaction.state.log.min.isr: 1\n     default.replication.factor: 1\n     min.insync.replicas: 1\n     inter.broker.protocol.version: \"3.2\"\n   storage:\n     type: ephemeral\n zookeeper:\n   replicas: 1\n   storage:\n     type: ephemeral\n entityOperator:\n   topicOperator: {}\n   userOperator: {}\n</code></p><p></p><p>将其应用到Kubernetes集群：</p><p></p><p><code lang=\"plain\">kubectl apply -f kafka.yaml -n kafka\n\nkafka.kafka.strimzi.io/my-cluster created\n</code></p><p></p><p>现在我们来验证Kafka集群已启动并在运行当中：</p><p></p><p><code lang=\"plain\">kubectl get pods -n kafka\n\nNAME                                         READY   STATUS    RESTARTS   AGE\nmy-cluster-entity-operator-d4db5ff58-rt96n   3/3     Running   0          2m26s\nmy-cluster-kafka-0                           1/1     Running   0          2m58s\nmy-cluster-zookeeper-0                       1/1     Running   0          3m31s\n</code></p><p></p><p>由于我们将监听器设置为使用TLS，所以Strimzi已经自动创建了一个Kubernetes Secret，其中包含集群证书、pkcs12信任存储和相关的密码。</p><p></p><p><code lang=\"plain\">kubectl get secrets -n kafka\n\nmy-cluster-clients-ca                        Opaque                                1      9m14s\nmy-cluster-clients-ca-cert                   Opaque                                3      9m14s\nmy-cluster-cluster-ca                        Opaque                                1      9m14s\nmy-cluster-cluster-ca-cert                   Opaque                                3      9m14s\nmy-cluster-cluster-operator-certs            Opaque                                4      9m14s\nmy-cluster-entity-operator-dockercfg-5wwb5   kubernetes.io/dockercfg               1      8m9s\nmy-cluster-entity-operator-token-h9xkq       kubernetes.io/service-account-token   4      8m9s\nmy-cluster-entity-operator-token-npvfc       kubernetes.io/service-account-token   4      8m9s\nmy-cluster-entity-topic-operator-certs       Opaque                                4      8m9s\nmy-cluster-entity-user-operator-certs        Opaque                                4      8m8s\nmy-cluster-kafka-brokers                     Opaque                                4      8m41s\nmy-cluster-kafka-dockercfg-fgpx2             kubernetes.io/dockercfg               1      8m41s\nmy-cluster-kafka-token-2x7s8                 kubernetes.io/service-account-token   4      8m41s\nmy-cluster-kafka-token-6qdgk                 kubernetes.io/service-account-token   4      8m41s\nmy-cluster-zookeeper-dockercfg-p296g         kubernetes.io/dockercfg               1      9m13s\nmy-cluster-zookeeper-nodes                   Opaque                                4      9m13s\nmy-cluster-zookeeper-token-dp9sc             kubernetes.io/service-account-token   4      9m13s\nmy-cluster-zookeeper-token-gbrxg             kubernetes.io/service-account-token   4      9m13s\n</code></p><p></p><p>这里最为重要的是-cluster-ca-cert（在本例中是my-cluster-cluster-ca-cert）这个密钥。</p><p></p><p>在终端窗口中运行下面的命令列出密钥的内容：</p><p></p><p><code lang=\"plain\">kubectl get secret my-cluster-cluster-ca-cert -o yaml -n kafka\n\napiVersion: v1\ndata:\n  ca.crt: LS0tLS1CRUdJTiBDRVJU\n  ca.p12: MIIGkwIBAzCCBk==\n  ca.password: azJjY2tIMEs1c091\nkind: Secret\nmetadata:\n  annotations:\n    strimzi.io/ca-cert-generation: \"0\"\n  creationTimestamp: \"2022-08-21T19:32:55Z\"\n  labels:\n    app.kubernetes.io/instance: my-cluster\n    app.kubernetes.io/managed-by: strimzi-cluster-operator\n    app.kubernetes.io/name: strimzi\n    app.kubernetes.io/part-of: strimzi-my-cluster\n    strimzi.io/cluster: my-cluster\n    strimzi.io/kind: Kafka\n    strimzi.io/name: strimzi\n  name: my-cluster-cluster-ca-cert\n  namespace: kafka\n  ownerReferences:\n  - apiVersion: kafka.strimzi.io/v1beta2\n    blockOwnerDeletion: false\n    controller: false\n    kind: Kafka\n    name: my-cluster\n    uid: 23c84dfb-bb33-47ed-bd41-b4e87e0a4c3a\n  resourceVersion: \"49424\"\n  uid: 6c2679a8-216f-421b-880a-de0e6a0879fa\ntype: Opaque\n</code></p><p></p><p>我们来创建一个mTLS授权的用户。</p><p></p><h2>安全和Debezium</h2><p></p><p>Kafka已经被保护起来了，现在我们来创建一个<a href=\"https://strimzi.io/docs/operators/latest/configuring.html#assembly-using-the-user-operator-str\">KafkaUser</a>\"资源，将授权角色赋给使用mTLS模式为用户进行身份验证的组和主题。</p><p></p><p>创建一个叫作kafka-user-connect-all-topics.yaml的文件，包含以下内容：</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\n name: my-connect\n namespace: kafka\n labels:\n   # Cluster name set previously\n   strimzi.io/cluster: my-cluster\nspec:\n authentication:\n   type: tls\n authorization:\n   type: simple\n   acls:\n   # Kafka Connects internal topics used to store configuration, offsets or status\n   - resource:\n       type: group\n       name: outbox-viewer\n     operation: Read\n   - resource:\n       type: group\n       name: outbox-viewer\n     operation: Describe\n   - resource:\n       type: group\n       name: mysql-dbhistory\n     operation: Read\n   - resource:\n       type: group\n       name: mysql-dbhistory\n     operation: Describe\n   - resource:\n       type: group\n       name: connect-cluster\n     operation: Read\n   - resource:\n       type: group\n       name: connect-cluster\n     operation: Describe\n   - resource:\n       type: topic\n       name: connect-cluster-configs\n     operation: Read\n   - resource:\n       type: topic\n       name: connect-cluster-configs\n     operation: Describe\n   - resource:\n       type: topic\n       name: connect-cluster-configs\n     operation: Write\n   - resource:\n       type: topic\n       name: connect-cluster-configs\n     operation: Create\n   - resource:\n       type: topic\n       name: connect-cluster-status\n     operation: Read\n   - resource:\n       type: topic\n       name: connect-cluster-status\n     operation: Describe\n   - resource:\n       type: topic\n       name: connect-cluster-status\n     operation: Write\n   - resource:\n       type: topic\n       name: connect-cluster-status\n     operation: Create\n   - resource:\n       type: topic\n       name: connect-cluster-offsets\n     operation: Read\n   - resource:\n       type: topic\n       name: connect-cluster-offsets\n     operation: Write\n   - resource:\n       type: topic\n       name: connect-cluster-offsets\n     operation: Describe\n   - resource:\n       type: topic\n       name: connect-cluster-offsets\n     operation: Create\n   - resource:\n       type: group\n       name: connect-cluster\n     operation: Read\n   # Debezium topics\n   - resource:\n       type: topic\n       name: \"*\"\n     operation: Read\n   - resource:\n       type: topic\n       name: \"*\"\n     operation: Describe\n   - resource:\n       type: topic\n       name: \"*\"\n     operation: Write\n   - resource:\n       type: topic\n       name: \"*\"\n     operation: Create\n</code></p><p></p><p>在终端窗口中应用这个资源：</p><p></p><p><code lang=\"plain\">kubectl apply -f kafka-user-connect-all-topics.yaml -n kafka\nkafkauser.kafka.strimzi.io/my-connect created\n</code></p><p></p><p>在注册了这个Kafka用户后，Strimzi创建了一个与KafkaUser资源（my-connect）同名的新密钥，并使用pkcs12密钥存储库保存客户端的私钥和访问它的密码。</p><p></p><p><code lang=\"plain\">kubectl get secret my-connect -n kafka -o yaml\n\napiVersion: v1\ndata:\n  ca.crt: LS0tLS1CK\n  user.crt: LS0tLS1CRUdJTiB==\n  user.key: LS0tLS1CRUdJTiBQUklWQVRK\n  user.p12: MIILNAIBAzCAA==\n  user.password: UUR4Nk5NemsxUVFF\nkind: Secret\nmetadata:\n  creationTimestamp: \"2022-08-21T20:12:44Z\"\n  labels:\n    app.kubernetes.io/instance: my-connect\n    app.kubernetes.io/managed-by: strimzi-user-operator\n    app.kubernetes.io/name: strimzi-user-operator\n    app.kubernetes.io/part-of: strimzi-my-connect\n    strimzi.io/cluster: my-cluster\n    strimzi.io/kind: KafkaUser\n  name: my-connect\n  namespace: kafka\n  ownerReferences:\n  - apiVersion: kafka.strimzi.io/v1beta2\n    blockOwnerDeletion: false\n    controller: false\n    kind: KafkaUser\n    name: my-connect\n    uid: 882447cc-7759-4884-9d2f-f57f8be92711\n  resourceVersion: \"60439\"\n  uid: 9313676f-3417-42d8-b3fb-a1b1fe1b3a39\ntype: Opaque\n</code></p><p></p><p>现在，我们有了一个新的Kafka用户，拥有访问Kafka主题所需的权限。</p><p></p><p>在部署Debezium Kafka Connector之前，我们需要允许Kafka Connector对象使用Kubernetes API直接从mysqlsecret Secret对象中读取MySQL密钥（就像我们在应用程序中所做的那样），这样Connector就可以通过数据库身份验证并读取事务日志。</p><p></p><p>创建kafka-role-binding.yaml文件，内容如下：</p><p></p><p><code lang=\"plain\">apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n name: connector-configuration-role\n namespace: kafka\nrules:\n- apiGroups: [\"\"]\n resources: [\"secrets\"]\n resourceNames: [\"mysqlsecret\", \"my-connect\", \"my-cluster-cluster-ca-cert\"]\n verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n name: connector-configuration-role-binding\n namespace: kafka\nsubjects:\n- kind: ServiceAccount\n name: debezium-connect-cluster-connect\n namespace: kafka\nroleRef:\n kind: Role\n name: connector-configuration-role\n apiGroup: rbac.authorization.k8s.io\n</code></p><p></p><p>注意，subjects下面的name是运行Debezium Kafka Connect Pod所需的服务帐户。我们还没有部署Pod，不过在部署KafkaConnect组件时，创建的服务帐户需要遵循$KafkaConnectName-connect的格式。由于Debezium Kafka Connect的名称是debezium-connect-cluster-connect，因此创建的服务帐户就是my-connect-connect，并且我们授予这个帐户直接读取Kubernetes Secrets的权限。</p><p></p><p>在部署Debezium Kafka Connect之前应用kafka-role-binding.yaml文件：</p><p></p><p><code lang=\"plain\">kubectl apply -f kafka-role-binding.yaml -n kafka\n\nrole.rbac.authorization.k8s.io/connector-configuration-role created\nrolebinding.rbac.authorization.k8s.io/connector-configuration-role-binding created\n</code></p><p></p><p>下图总结了目前的安全通信：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed2041090efd0f54664c99a73a9ae5a2.webp\" /></p><p></p><p>为了部署Debezium Kafka Connect，我们需要再次使用Strimzi提供的<a href=\"https://strimzi.io/docs/operators/latest/deploying.html#deploying-kafka-connect-str\">KafkaConnect</a>\"对象，但需要做一些修改，以便通过Kafka集群的身份验证，并允许从Kubernetes Secrets读取配置参数（主要目的是读取MySQL凭证进行身份验证）。</p><p></p><p>配置如下字段：</p><p></p><p>端口现在是9093。设置用于与集群通信的mTLS证书（tls字段）。设置证书和密钥用户（authentication字段），以便进行身份验证。设置config.providers，让MySQL Connector从Kubernetes Secrets读取配置。externalConfiguration用于将信任存储库和密钥存储库物化到文件中。它们被物化在/opt/kafka/external-configuration/目录下。MySQL Connector会访问这些文件。创建kafka-connect.yaml文件，内容如下所示：</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\n name: debezium-connect-cluster\n namespace: kafka\n annotations:\n   strimzi.io/use-connector-resources: \"true\"\nspec:\n version: 3.2.0\n image: quay.io/lordofthejars/debezium-connector-mysql:1.9.4\n replicas: 1\n bootstrapServers: my-cluster-kafka-bootstrap:9093\n logging:\n   type: inline\n   loggers:\n     connect.root.logger.level: \"INFO\"\n tls:\n   trustedCertificates:\n     - secretName: my-cluster-cluster-ca-cert\n       certificate: ca.crt\n authentication:\n   type: tls\n   certificateAndKey:\n     secretName: my-connect\n     certificate: user.crt\n     key: user.key\n config:\n   config.providers: secrets\n   config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider\n   group.id: connect-cluster\n   offset.storage.topic: connect-cluster-offsets\n   offset.storage.replication.factor: 1\n   config.storage.topic: connect-cluster-configs\n   config.storage.replication.factor: 1\n   status.storage.topic: connect-cluster-status\n   status.storage.replication.factor: 1\n externalConfiguration:\n   volumes:\n     - name: cluster-ca\n       secret:\n         secretName: my-cluster-cluster-ca-cert\n     - name: my-user\n       secret:\n         secretName: my-connect\n</code></p><p></p><p>trustedCertificates设置为使用Kafka对象部署Kafka集群时创建的密钥。</p><p></p><p>authentication下面的certificateAndKey设置为注册KafkaUser时创建的密钥。</p><p></p><p>部署资源并验证其正确性：</p><p></p><p><code lang=\"plain\">kubectl apply -f kafka-connect.yaml -n kafka\nkafkaconnect.kafka.strimzi.io/debezium-connect-cluster created\n</code></p><p></p><p>创建一个叫作debezium-kafka-connector.yaml的新文件，用于配置Debezium，允许MySQL Connector访问MySQL实例的事务日志。在本例中，我们在连接器配置中没有使用明文的用户名和密码，而是引用前面用MySQL凭证创建的Secret对象。Secret的访问格式为secrets:/:。此外，在应用了KafkaConnect定义后，它会读取物化的信任存储库和密钥库。</p><p></p><p><code lang=\"plain\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnector\nmetadata:\n name: debezium-connector-mysql\n namespace: kafka\n labels:\n   strimzi.io/cluster: debezium-connect-cluster\nspec:\n class: io.debezium.connector.mysql.MySqlConnector\n tasksMax: 1\n config:\n   group.id: connect-cluster\n   tasks.max: 1\n   database.hostname: mysql\n   database.port: 3306\n   database.user: root\n   database.password: ${secrets:kafka/mysqlsecret:mysqlpassword}\n   database.server.id: 184054\n   database.server.name: mysql\n   database.include.list: moviesdb\n   database.allowPublicKeyRetrieval: true\n   table.include.list: moviesdb.OutboxEvent\n   database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9093\n   database.history.kafka.topic: schema-changes.movies\n   database.history.producer.security.protocol: SSL\n   database.history.producer.ssl.keystore.type: PKCS12\n   database.history.producer.ssl.keystore.location: /opt/kafka/external-configuration/my-user/user.p12\n   database.history.producer.ssl.keystore.password: ${secrets:kafka/my-connect:user.password}\n   database.history.producer.ssl.truststore.type: PKCS12\n   database.history.producer.ssl.truststore.location: /opt/kafka/external-configuration/cluster-ca/ca.p12\n   database.history.producer.ssl.truststore.password: ${secrets:kafka/my-cluster-cluster-ca-cert:ca.password}\n\n   database.history.consumer.security.protocol: SSL\n   database.history.consumer.ssl.keystore.type: PKCS12\n   database.history.consumer.ssl.keystore.location: /opt/kafka/external-configuration/my-user/user.p12\n   database.history.consumer.ssl.keystore.password: ${secrets:kafka/my-connect:user.password}\n   database.history.consumer.ssl.truststore.type: PKCS12\n   database.history.consumer.ssl.truststore.location: /opt/kafka/external-configuration/cluster-ca/ca.p12\n   database.history.consumer.ssl.truststore.password: ${secrets:kafka/my-cluster-cluster-ca-cert:ca.password}\n</code></p><p></p><p>在终端窗口中执行下面的命令应用这个文件注册MySQL Connector：</p><p></p><p><code lang=\"plain\">kubectl apply -f kafka-connector.yaml -n kafka\nkafkaconnector.kafka.strimzi.io/debezium-connector-mysql created\n</code></p><p></p><p>最后，所有的通信通道都被保护起来了。</p><p></p><h2>演示</h2><p></p><p>现在，我们有了一个与上一篇文章中介绍的同样的示例，但现在它更安全了。</p><p></p><p>我们用一个叫作outbox-viewer的Quarkus应用程序来测试它，它将OutboxEvent主题的所有内容打印到控制台。部署下面的YAML文件：</p><p></p><p><code lang=\"plain\">---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n annotations:\n   app.quarkus.io/commit-id: ebe139afdc9f7f956725af5c5a92cf3c03486bca\n   app.quarkus.io/build-timestamp: 2022-08-23 - 11:14:36 +0000\n labels:\n   app.kubernetes.io/name: outbox-viewer\n   app.kubernetes.io/version: 1.0.0-SNAPSHOT\n name: outbox-viewer\n namespace: kafka\n---\napiVersion: v1\nkind: Service\nmetadata:\n annotations:\n   app.quarkus.io/commit-id: ebe139afdc9f7f956725af5c5a92cf3c03486bca\n   app.quarkus.io/build-timestamp: 2022-08-23 - 11:14:36 +0000\n labels:\n   app.kubernetes.io/name: outbox-viewer\n   app.kubernetes.io/version: 1.0.0-SNAPSHOT\n name: outbox-viewer\n namespace: kafka\nspec:\n ports:\n   - name: http\n     port: 80\n     targetPort: 8080\n selector:\n   app.kubernetes.io/name: outbox-viewer\n   app.kubernetes.io/version: 1.0.0-SNAPSHOT\n type: ClusterIP\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n name: view-secrets\n namespace: kafka\nrules:\n - apiGroups:\n     - \"\"\n   resources:\n     - secrets\n   verbs:\n     - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n name: outbox-viewer-view\n namespace: kafka\nroleRef:\n kind: ClusterRole\n apiGroup: rbac.authorization.k8s.io\n name: view\nsubjects:\n - kind: ServiceAccount\n   name: outbox-viewer\n   namespace: kafka\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n name: outbox-viewer-view-secrets\n namespace: kafka\nroleRef:\n kind: Role\n apiGroup: rbac.authorization.k8s.io\n name: view-secrets\nsubjects:\n - kind: ServiceAccount\n   name: outbox-viewer\n   namespace: kafka\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n annotations:\n   app.quarkus.io/commit-id: ebe139afdc9f7f956725af5c5a92cf3c03486bca\n   app.quarkus.io/build-timestamp: 2022-08-23 - 11:14:36 +0000\n labels:\n   app.kubernetes.io/name: outbox-viewer\n   app.kubernetes.io/version: 1.0.0-SNAPSHOT\n name: outbox-viewer\n namespace: kafka\nspec:\n replicas: 1\n selector:\n   matchLabels:\n     app.kubernetes.io/name: outbox-viewer\n     app.kubernetes.io/version: 1.0.0-SNAPSHOT\n template:\n   metadata:\n     annotations:\n       app.quarkus.io/commit-id: ebe139afdc9f7f956725af5c5a92cf3c03486bca\n       app.quarkus.io/build-timestamp: 2022-08-23 - 11:14:36 +0000\n     labels:\n       app.kubernetes.io/name: outbox-viewer\n       app.kubernetes.io/version: 1.0.0-SNAPSHOT\n     namespace: kafka\n   spec:\n     containers:\n       - env:\n           - name: KUBERNETES_NAMESPACE\n             valueFrom:\n               fieldRef:\n                 fieldPath: metadata.namespace\n         image: quay.io/lordofthejars/outbox-viewer:1.0.0-SNAPSHOT\n         imagePullPolicy: Always\n         name: outbox-viewer\n         ports:\n           - containerPort: 8080\n             name: http\n             protocol: TCP\n         volumeMounts:\n           - mountPath: /home/jboss/cluster\n             name: cluster-volume\n             readOnly: false\n           - mountPath: /home/jboss/user\n             name: user-volume\n             readOnly: false\n     serviceAccountName: outbox-viewer\n     volumes:\n       - name: cluster-volume\n         secret:\n           optional: false\n           secretName: my-cluster-cluster-ca-cert\n       - name: user-volume\n         secret:\n           optional: false\n           secretName: my-connect\n</code></p><p></p><p>然后在终端窗口中可以看到应用程序Pod的日志。</p><p></p><p><code lang=\"plain\">kubectl logs outbox-viewer-684969f9f6-7snng -f\n</code></p><p></p><p>将Pod名称替换为你的Pod的名称。</p><p></p><p>在终端中运行下面的命令查找Movie Player Producer应用程序的IP和端口：</p><p></p><p><code lang=\"plain\">minikube ip -p strimzi\n\n192.168.59.106\n</code></p><p></p><p>获取movie-plays-producer-debezium暴露的端口（第二个端口）。</p><p></p><p><code lang=\"plain\">kubectl get services -n kafka\n\nmovie-plays-producer-debezium   LoadBalancer   10.100.117.203        80:32460/TCP                 67m\n</code></p><p></p><p>向Movie Play Producer应用程序发送curl请求：</p><p></p><p><code lang=\"plain\">curl -X 'POST' \\\n  'http://192.168.59.106:32460/movie' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"name\": \"Minions: The Rise of Gru\",\n  \"director\": \"Kyle Balda\",\n  \"genre\": \"Animation\"\n}'\n</code></p><p></p><p>根据你的示例调整IP和端口。</p><p></p><p>最后，检查outbox-viewer Pod的输出，可以看到数据从数据库传输到Kafka。</p><p></p><p><code lang=\"plain\">{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"bytes\",\"optional\":false,\"field”\n…\n,\"aggregatetype\":\"Movie\",\"aggregateid\":\"1\",\"type\":\"MovieCreated\",\"timestamp\":1661339188708005,\"payload\":\"{\\\"id\\\":1,\\\"name\\\":\\\"Minions: The Rise of Gru\\\",\\\"director\\\":\\\"Kyle Balda\\\",\\\"genre\\\":\\\"Animation\\\"}\",\"tracingspancontext\":null},\"source\":{\"version\":\"1.9.4.Final\",\"connector\":\"mysql\",\"name\":\"mysql\",\"ts_ms\":1661339188000,\"snapshot\":\"false\",\"db\":\"moviesdb\",\"sequence\":null,\"table\":\"OutboxEvent\",\"server_id\":1,\"gtid\":null,\"file\":\"binlog.000002\",\"pos\":2967,\"row\":0,\"thread\":15,\"query\":null},\"op\":\"c\",\"ts_ms\":1661339188768,\"transaction\":null}}\n</code></p><p></p><h2>Debezium Embedded</h2><p></p><p>到目前为止，我们已经保护了应用程序和MySQL数据库、Debezium服务器和MySQL、Debezium服务器和Kafka之间的通信。</p><p></p><p>你可能会想，如果使用部署在Quarkus应用程序中的Debezium Embedded而不是Debezium Server该怎么办？我们该如何配置Kafka连接使用mTLS？</p><p></p><p>Quarkus提供了两种连接Kafka的方式——<a href=\"https://quarkus.io/guides/kafka#kafka-bare-clients\">Kafka客户端</a>\"或<a href=\"https://quarkus.io/guides/kafka\">响应式消息客户端</a>\"。我们来看一下在使用这两种方式时通过mTLS认证方法验证Kafka集群所需的属性。</p><p></p><h4>KeyStore和TrustStore</h4><p></p><p>要在客户端配置mTLS，需要四样东西：</p><p></p><p>建立mTLS连接所需的集群TrustStore；TrustStore的密码；用于身份验证的Kafka User KeyStore；KeyStore的密码。前两个元素保存在之前应用Strimzi资源时创建的my-cluster-cluster-ca-cert Kubernetes Secret中。要获取它们，在终端窗口中运行下面的命令：</p><p></p><p><code lang=\"plain\">kubectl get secret my-cluster-cluster-ca-cert -n kafka -o jsonpath='{.data.ca\\.p12}' | base64 -d &gt; mtls-cluster-ca.p12\n</code></p><p></p><p>获取密码：</p><p></p><p><code lang=\"plain\">kubectl get secret my-cluster-cluster-ca-cert -n kafka -o jsonpath='{.data.ca\\.password}' | base64 -d\nk2cckH0K5sOu\n</code></p><p></p><p>后面的元素保存在my-connect Kubernetes Secret中。要获取它们，在终端窗口中运行下面的命令：</p><p></p><p><code lang=\"plain\">kubectl get secret my-connect -n kafka -o jsonpath='{.data.user\\.p12}' | base64 -d &gt; mtls-user.p12\n</code></p><p></p><p>获取密码：</p><p></p><p><code lang=\"plain\">kubectl get secret my-connect -n kafka -o jsonpath='{.data.user\\.password}' | base64 -d\nQDx6NMzk1QQE\n</code></p><p></p><p>现在，设置Quarkus Kafka配置属性，使用前面的凭证进行Kafka集群身份认证：</p><p></p><p><code lang=\"plain\">%prod.kafka.ssl.truststore.location=mtls-cluster-ca.p12\n%prod.kafka.ssl.truststore.password=k2cckH0K5sOu\n%prod.kafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=mtls-user.p12\n%prod.kafka.ssl.keystore.password=QDx6NMzk1QQE\n%prod.kafka.ssl.keystore.type=PKCS12\n%prod.kafka.security.protocol=SSL\n\n%prod.mp.messaging.incoming.movies.ssl.truststore.location=mtls-cluster-ca.p12\n%prod.mp.messaging.incoming.movies.ssl.truststore.password=k2cckH0K5sOu\n%prod.mp.messaging.incoming.movies.ssl.truststore.type=PKCS12\n%prod.mp.messaging.incoming.movies.ssl.keystore.location=mtls-user.p12\n%prod.mp.messaging.incoming.movies.ssl.keystore.password=QDx6NMzk1QQE\n%prod.mp.messaging.incoming.movies.ssl.keystore.type=PKCS12\n%prod.mp.messaging.incoming.movies.security.protocol=SSL\n</code></p><p></p><p>我们可以像使用MySQL凭证一样，用Quarkus Kubernetes Config扩展来直接注入凭证，但为了简化起见，我们没有这么做。</p><p></p><p>不过，在安全性方面，仍然有一个重要的缺失点：如何正确地在YAML文件中存储密钥，以及如何在Kubernetes集群中安全地保存密钥？</p><p></p><h2>加密密钥</h2><p></p><p>在本文开始时，我们使用MySQL凭证创建了一个Kubernetes Secret对象，但它是一个包含Base64编码的敏感信息的YAML文件，所以并不安全。这个YAML文件可能最终会保存在Git存储库中，任何有权访问存储库的人都可以使用这些密钥。在下一节中，我们将解决这个问题。</p><p></p><h4>Sealed Secrets</h4><p></p><p><a href=\"https://sealed-secrets.netlify.app/\">Sealed Secrets</a>\"是一个Kubernetes控制器，允许在客户端（本地机器）加密Kubernetes Secrets资源，并在应用后在Kubernetes集群内解密它们。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f173d589e77ab20d38abb683cdd5892.webp\" /></p><p></p><p>Sealed Secrets需要用到两个组件，第一个是用于加密密钥的kubeseal CLI工具。</p><p></p><p>要安装kubeseal，请根据你的操作系统从这个<a href=\"https://github.com/bitnami-labs/sealed-secrets/releases/tag/v0.18.1\">链接</a>\"下载软件包。</p><p></p><p>第二个是kubeseal Kubernetes控制器。在命令行中执行下面的命令来安装它：</p><p></p><p><code lang=\"plain\">kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.1/controller.yaml -n kube-system\n\nrole.rbac.authorization.k8s.io/sealed-secrets-service-proxier created\nclusterrole.rbac.authorization.k8s.io/secrets-unsealer created\ndeployment.apps/sealed-secrets-controller created\ncustomresourcedefinition.apiextensions.k8s.io/sealedsecrets.bitnami.com created\nservice/sealed-secrets-controller created\nrole.rbac.authorization.k8s.io/sealed-secrets-key-admin created\nclusterrolebinding.rbac.authorization.k8s.io/sealed-secrets-controller created\nserviceaccount/sealed-secrets-controller created\nrolebinding.rbac.authorization.k8s.io/sealed-secrets-service-proxier created\nrolebinding.rbac.authorization.k8s.io/sealed-secrets-controller created\n</code></p><p></p><p>运行下面的命令检查控制器是否正确部署并运行：</p><p></p><p><code lang=\"plain\">kubectl  get pods -n kube-system\n\nsealed-secrets-controller-554d94cb68-xr6mw                                1/1     Running   0          8m46s\n</code></p><p></p><p>在那之后，我们可以基于mysql-secret.yaml文件使用kubeseal工具自动创建一个新的Kubernetes资源SealedSecret，其中数据字段是加密的。</p><p></p><p><code lang=\"plain\">kubeseal -n kube -o yaml  mysql-secret-encrypted.yaml\n</code></p><p></p><p>生成的新文件叫作mysql-secret-encrypted.yaml，其中每个密钥的值都经过加密：</p><p></p><p><code lang=\"plain\">apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n creationTimestamp: null\n name: mysqlsecret\n namespace: kube\nspec:\n encryptedData:\n   mysqlpassword: AgBl721mnowwPlC35FfO26zP0\n   mysqlrootpassword: AgAKl1tWV8hahn00yGS4ucs\n   mysqluser: AgCWrWFl1/LcS\ntemplate:\n   data: null\n   metadata:\n     creationTimestamp: null\n     name: mysqlsecret\n     namespace: kafka\n   type: Opaque\n</code></p><p></p><p>现在，你可以安全地删除mysql-secret.yaml文件，因为我们不再需要它了。</p><p></p><p>像应用其他Kubernetes资源文件一样应用加密的资源，Sealed Secrets Kubernetes控制器将解密并将其作为正常的密钥保存在Kubernetes中。</p><p></p><p>你可以通过下面的命令验证Secret：</p><p></p><p><code lang=\"plain\">kubectl  get secret mysqlsecret -n kafka -o yaml\n\napiVersion: v1\ndata:\n  mysqlpassword: YWxleA==\n  mysqlrootpassword: YWxleA==\n  mysqluser: YWxleA==\nkind: Secret\nmetadata:\n  creationTimestamp: \"2022-08-21T19:05:21Z\"\n  name: mysqlsecret\n  namespace: kafka\n  ownerReferences:\n  - apiVersion: bitnami.com/v1alpha1\n    controller: true\n    kind: SealedSecret\n    name: mysqlsecret\n    uid: 2a5ee74b-c2b2-49b3-9a9f-877e7a77b163\n  resourceVersion: \"41514\"\n  uid: 494cbe8b-7480-4ebd-9cc5-6fe396795eaa\ntype: Opaque\n</code></p><p></p><p>需要注意的是，这是一个解密的Kubernetes Secret，引用了负责创建它的SealedSecret。因此，SealedSecret的生命周期也与Secret紧密相关。</p><p></p><p>我们已经解决了正确存储YAML文件而不泄露敏感数据的问题，但是当Secret被应用到Kubernetes集群，它是以Base64编码格式存储的，所以它不是密钥。</p><p></p><h4>静态密钥</h4><p></p><p>默认情况下，Kubernetes不会在etcd数据库中存储加密的密钥。静态加密密钥数据是一个很大的话题，值得专门为其写一篇文章（事实上，“<a href=\"https://www.manning.com/books/kubernetes-secrets-management#:~:text=Kubernetes%20Secrets%20Management%20reveals%20how,to%20strengthen%20applications%20and%20infrastructure.\">Kubernetes Secret Management</a>\"”一书专门讨论了这个话题）。每一种Kubernetes实现都有可能使用不同的方式来启用静态密钥加密，尽管最后都是一个被复制到每个kube-apiserver节点中的配置文件（<a href=\"https://kubernetes.io/docs/reference/config-api/apiserver-encryption.v1/\">EncryptionConfiguration</a>\"）。</p><p></p><p>该文件的格式为：</p><p></p><p><code lang=\"plain\">apiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n - resources:\n     - secrets\n   providers:\n     - identity: {}\n     - aesgcm:\n         keys:\n           - name: key1\n             secret: c2VjcmV0IGlzIHNlY3VyZQ==\n           - name: key2\n             secret: dGhpcyBpcyBwYXNzd29yZA==\n     - aescbc:\n         keys:\n           - name: key1\n             secret: c2VjcmV0IGlzIHNlY3VyZQ==\n           - name: key2\n             secret: dGhpcyBpcyBwYXNzd29yZA==\n     - secretbox:\n         keys:\n           - name: key1\n             secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=\n</code></p><p></p><p>在下图中，我们可以看到在kube-apiserver中注册EncryptionConfiguration文件的流程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/939a16eb0a2114d0cd855ba193c983f3.webp\" /></p><p></p><p>现在，我们已经使用SealedSecrets对象来加密YAML文件中的密钥，还使用EncryptionConfiguration文件来保护静态密钥。</p><p></p><h2>结论</h2><p></p><p>保护好所有的基础设施是一件很重要的事情，我们已经在本文中学习了如何使用Kubernetes Secrets来保护对数据库和Kafka的访问。</p><p></p><p>我们不仅可以使用Strimzi定义身份验证，还可以定义授权，提供一些规则，规定谁可以对Kafka主题做什么。</p><p></p><p>访问这些密钥也是一个重要的部分，Quarkus和Debezium允许你以一种高效而安全的方式访问这些密钥，而不需要将密钥持久化在文件系统中（或作为环境变量），而是直接将它们注入内存。</p><p></p><p>安全性是一个重要的话题，当需要在Kafka集群中管理安全性时，Strimzi是一个完美的选择。</p><p></p><p>源代码可以在<a href=\"https://github.com/lordofthejars/movie-plays-kafka/tree/main/strimzi\">GitHub</a>\"上找到。</p><p></p><p>作者简介：</p><p>Alex Soto是Red Hat的开发者体验总监。他对Java、软件自动化充满了热情，并且深信开源软件模式。Soto是“Testing Java Microservices”（Manning）和“Quarkus Cookbook”（O'Reilly）的合著者，也是几个开源项目的贡献者。自2017年以来，他获得了Java Champion称号，也是萨尔Universidad Ramon Llull大学的国际演讲者和教师。如果你想继续关注Kubernetes和Java的动态，可以在Twitter上关注他（<a href=\"https://twitter.com/alexsotob\">https://twitter.com/alexsotob</a>\"）。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/secure-kafka-cluster-strimzi/\">https://www.infoq.com/articles/secure-kafka-cluster-strimzi/</a>\"</p>",
    "publish_time": "2023-03-15 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]