[
  {
    "title": "PyTorch 2.1正式发布，带来自动动态Shape支持和分布式训练增强",
    "url": "https://www.infoq.cn/article/73kmJRwA4pJ8oO58Sy5w",
    "summary": "<p>最新版本的 PyTorch 带来了自动动态shape支持和分布式训练增强。<a href=\"https://pytorch.org/blog/pytorch-2-1/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">PyTorch 2.1</a>\" 在最近举行的 <a href=\"https://events.linuxfoundation.org/pytorch-conference/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">PyTorch 2023 大会</a>\" 上正式发布，新版本引入了 <a href=\"https://github.com/pytorch/executorch?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">ExecuTorch</a>\" ，用于提升 PyTorch 在移动和边缘设备上的性能。此外，主题演讲还公布了 PyTorch 基金会的新成员以及 11 月份的 Docathon 活动。</p><p></p><p></p><h3>PyTorch 2.1</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/536f191fb679b1f22fd490bf872f8ae1.webp\" /></p><p></p><p></p><blockquote>torch.compile() 在许多 PyG 模型上表现出色。总体而言，我们看到了几乎 300% 的运行时改进。—— Matthias Fey（PyG开发者）</blockquote><p></p><p></p><p>PyTorch 2.1 的一个显著特性是 torch.compile 中的自动动态shape支持，可以在模型架构中使用动态输入形状。该功能打破了固定输入形状的限制，提供了更大的灵活性。</p><p></p><p>在分布式训练方面，通过 <a href=\"https://pytorch.org/docs/stable/distributed.checkpoint.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">torch.distributed.checkpoint</a>\" 引入了增强功能，可以在多个排名之间并行保存和加载训练任务来提高分布式训练效率。这一功能对于管理长时间运行的训练任务和确保更顺畅的训练流程来说至关重要。</p><p></p><p>PyTorch 2.1 还增加了对在 torch.compile 中调用 NumPy API 的支持，增强了 PyTorch 和 <a href=\"https://numpy.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">NumPy</a>\" 之间的互操作性。这一集成有助于在各种设备上执行 NumPy 代码，使代码生成更高效。新版本还带来了性能方面的改进，包括 CPU 引导程序增强、AVX512 支持以及缩放点积注意机制的增强实现。此外，还引入了 torch.export 的原型版本，提供了一种捕获完整图的机制，启用基于 torch.export 的量化来减小模型大小，以及提升边缘设备和移动平台的推理速度。</p><p></p><p></p><h3>ExecuTorch</h3><p></p><p></p><p>引入 ExecuTorch 标志着 PyTorch 在移动和边缘设备上改进性能的重要进展。ExecuTorch 的一个显著特性是 <a href=\"https://pytorch.org/blog/pytorch-edge/?utm_content=268296368&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-776585502606721024&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">Lightweight Operator Registry</a>\"，是为管理各种 PyTorch 模型量身定制的。它简化了Operator的处理，而Operator是 PyTorch 模型确保最佳运行时性能的核心构建块。</p><p></p><p>新版本引入了在目标设备上分析和优化模型性能的设备模型分析功能。这种实时分析对于识别性能瓶颈并通过调整模型来提高效率和降低延迟来说至关重要，特别在各个领域的实时应用中，如增强现实、虚拟现实和物联网。</p><p></p><p></p><h3>PyTorch 基金会迎来新成员</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed366963addbd9f0379b32ba2bb64fd8.webp\" /></p><p></p><p></p><blockquote>我们很高兴成为 PyTorch 基金会的创始成员，并期待与人工智能领域的其他领袖紧密合作来一起发展这个令人惊叹的创新社区。——<a href=\"https://www.linkedin.com/posts/pytorch_google-has-supported-the-pytorch-foundation-activity-7074401572457226240-4Kqm/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">谷歌</a>\"</blockquote><p></p><p></p><p>PyTorch 基金会欢迎 <a href=\"https://www.huawei.com/en/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">华为</a>\" 和 <a href=\"https://lightning.ai/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">Lightning AI</a>\" 成为新的首席成员。华为的加入旨在优化 PyTorch 以发挥其 Ascend 计算平台的潜力，该平台以其在人工智能应用中的强大计算性能而闻名。</p><p></p><p>Lightning AI 是 PyTorch Lightning 的开发商，这是 PyTorch 的一个轻量级封装器，已经在代码结构化和可重用方面发挥了关键作用，简化了研究人员和开发人员的工作。Lightning AI 加入 PyTorch 基金会重在加强 PyTorch 生态系统的发展。</p><p></p><p></p><h3>Docathon</h3><p></p><p></p><p>PyTorch 社区组织了一个 <a href=\"http://pytorch.org/blog/announcing-docathon-h2-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">Docathon</a>\"，计划于 2023 年 11 月举行。这一举措旨在完善和扩展框架的文档，确保文档保持最新和对用户友好。希望更多地参与 PyTorch 2.1 的开发者们可以观看 <a href=\"https://www.youtube.com/pytorch?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">PyTorch YouTube 频道</a>\" 上的大会视频，或者查看 <a href=\"https://pytorch2023.sched.com/?iframe=no&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTk4NjM2OTMsImZpbGVHVUlEIjoiUktBV015bGJRcmh2blFxOCIsImlhdCI6MTY5OTg2MzM5MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.m_r4Zw-31BLB8_wKudmLQnKZIL_rJPKxkg4YZp1BK9U\">活动日程</a>\" 以获取一些演讲者的信息。</p><p></p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/10/pytorch21-at-pytorch-con-2023/\">https://www.infoq.com/news/2023/10/pytorch21-at-pytorch-con-2023/</a>\"</p>",
    "publish_time": "2023-11-14 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全票通过！Seata进入Apache孵化器",
    "url": "https://www.infoq.cn/article/p2OhRzr7HxKhfgJor6zx",
    "summary": "<p>北京时间2023年10月29日，分布式事务开源项目Seata正式通过Apache基金会的投票决议，以全票通过的优秀表现正式成为Apache孵化器项目！</p><p></p><p>根据Apache基金会邮件列表显示，在包含13个约束性投票(binding&nbsp;votes)和6个无约束性投票(non-binding&nbsp;votes)的投票全部持赞同意见，无弃权票和反对票，投票顺利通过。</p><p></p><p>“Welcome&nbsp;Seata&nbsp;to&nbsp;the&nbsp;ASF&nbsp;incubator.”</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a026e521f8f62735aea2a8ff2ba97616.png\" /></p><p></p><p></p><h3>项目历史</h3><p></p><p></p><p>早在2007年，阿里巴巴和蚂蚁集团内部开发了分布式事务中间件，用于解决电商、支付、物流等业务场景中应用数据的一致性问题。内部项目分别被称为TXC&nbsp;(Taobao&nbsp;Transaction&nbsp;Constructor)/XTS&nbsp;(eXtended&nbsp;Transaction&nbsp;Service)，该项目几乎在每笔订单的交易支付链路几乎都有使用。&nbsp;自2013年以来，阿里巴巴和蚂蚁集团已在阿里云和金融云上向企业客户分别发布了分布式事务云服务产品GTS(global&nbsp;transaction&nbsp;service)/DTX(Distributed&nbsp;Transaction-eXtended)，在各个行业领域积累了大量用户。2019年1月，阿里巴巴集团正式开源了该项目，项目命名为&nbsp;Fescar&nbsp;(Fast&nbsp;&amp;&nbsp;Easy&nbsp;Commit&nbsp;and&nbsp;Rollback)）。项目开源以来，它受到了众多开发人员的热烈欢迎和赞扬，开源一周收获了超3k&nbsp;star，曾一度蝉联GitHub&nbsp;Trending排行榜第一。2019年4月，蚂蚁集团数据中间件团队加入了Fescar社区。为了创建一个更加开放和中立的社区，Fescar改名为Seata（Simple&nbsp;Extensible&nbsp;Autonomous&nbsp;Transaction&nbsp;Architecture），代码仓库从Alibaba&nbsp;organization迁移到其独立的Seata&nbsp;organization。&nbsp;2019年12月，Seata开源项目正式发布1.0.0&nbsp;GA版本，标志着项目已基本可生产使用。2023年10月，为了更好的通过社区驱动技术的演进，阿里和蚂蚁集团正式将Seata捐赠给Apache基金会，提案通过了Apache基金会的投票决议。</p><p></p><h3>项目现状</h3><p></p><p></p><p>Seata开源4年来主项目在GitHub累计收获star超24k，累计发布版本超40次，参与代码贡献人数超300人。Seata被各领域企业/组织广泛应用于解决分布式事务问题，在GitHub「Used&nbsp;by」超过3.1k的仓库依赖，金融领域企业纷纷试点使用。Seata&nbsp;对于市面上主流的关系数据库，RPC框架做了广泛的支持，同时被许多第三方社区做了主动和被动集成。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/98d5852f5d03fe57d6184df8d7375583.png\" /></p><p></p><h3>项目特性</h3><p></p><p>提供AT、TCC、Saga&nbsp;和&nbsp;XA事务模式，支持事务模式的混用，满足不同业务场景的数据一致性需求。提供Java、Golang等多语言SDK支持。支持了Apache&nbsp;Dubbo、Spring&nbsp;Cloud&nbsp;Alibaba、gRPC、Motan、sofaRPC、HttpClient等服务调用框架。支持了MySQL、MariaDB、Oracle、PostgreSQL、OceanBase、TiDB、SQLServer、PolarDB、Dameng等关系数据库无侵入AT事务模式的支持。支持基于多种关系数据库、Redis存储的存算分离的集群模式，支持基于Raft的存算不分离集群模式，满足不同运维场景下的集群高可用需求。支持了市面上主流的注册中心和配置中心。提供了丰富的插件化扩展机制，支持用户自定义SDK侧30多个扩展点。</p><p></p><p></p><h3>致谢</h3><p></p><p></p><p>感谢所有曾经参与到社区的贡献者。</p><p></p><p>特别感谢愿意给Seata提供指导的champion和mentors:</p><p></p><p>Champion：</p><p>Sheng&nbsp;Wu(wusheng&nbsp;at&nbsp;apache&nbsp;dot&nbsp;org)</p><p></p><p>Mentors：</p><p>Sheng&nbsp;Wu(wusheng&nbsp;at&nbsp;apache&nbsp;dot&nbsp;org)Justin&nbsp;Mclean(justin&nbsp;at&nbsp;classsoftware&nbsp;dot&nbsp;com)Huxing&nbsp;Zhang(huxing&nbsp;at&nbsp;apache&nbsp;dot&nbsp;org)Heng&nbsp;Du(duhengforever&nbsp;at&nbsp;apache&nbsp;dot&nbsp;org)</p><p></p><p>我们坚信将Seata引入ASF可以推动更强大，更多元化的开源社区的发展。我们将努力践行Apache&nbsp;Way，同时欢迎更多的公司和个人加入到开发者队伍中来，让Seata社区更加健康和健壮的成长，让更多人享受开源带来的技术红利！</p>",
    "publish_time": "2023-11-14 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "7 折即将结束，来看 QCon 上海 2023 大会确认了哪些专家和话题",
    "url": "https://www.infoq.cn/article/GpVwbFGKzC3EHpJbI20C",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai?utm_source=infoqweb&amp;utm_medium=qidongart&amp;utm_campaign=7&amp;utm_term=1114\">QCon 全球软件开发大会</a>\"将于 2023 年 12 月 28~29 日在上海举行，相关的内容工作在有条不紊地进行，包括各技术专题主持人的确定和相关的讲师邀请。15 个技术专题的主持人已经基本确定。此外，QCon 组委会还确认了包括中国科学院外籍院士，国际数据库专家 <a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5623?utm_source=infoqweb&amp;utm_medium=qidongart&amp;utm_campaign=7&amp;utm_term=1114\">樊文飞老师</a>\"、美的集团 首席信息安全官兼软件工程院院长，欧洲科学院院士，IEEE Fellow <a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5630?utm_source=infoqweb&amp;utm_medium=qidongart&amp;utm_campaign=7&amp;utm_term=1114\">刘向阳老师</a>\"、英特尔 大数据技术全球 CTO <a href=\"https://qcon.infoq.cn/2023/shanghai/presentation/5586?utm_source=infoqweb&amp;utm_medium=qidongart&amp;utm_campaign=7&amp;utm_term=1114\">戴金权老师</a>\"，以及顺丰集团 CTO <a href=\"https://qcon.infoq.cn/2023/shanghai/speaker/8328?utm_source=infoqweb&amp;utm_medium=qidongart&amp;utm_campaign=7&amp;utm_term=1114\">耿艳坤老师</a>\"在内的部分 Keynote 主题演讲。</p><p></p><p>11 月 17 日之前报名可以享受 7 折优惠，单张门票节省 2040 元（原价 6800 元）。</p><p></p><p>和以往的 QCon 大会一样，为了确保每个专题的内容质量，我们邀请业内有多年实践经验的技术专家，担任主持人，和 QCon 内容组委会一起策划专题并邀请靠谱的讲师来分享。这次的 15 个专题中，已经确认的主持人和部分讲师如下：</p><p></p><p>出品人已确认 13 位，已上线 13 位 ：</p><p></p><p>GenAI 和通用大模型应用探索：出品人徐羽，腾讯 信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人AI Agent 与行业融合应用的前景：出品人杨晶生 字节跳动 飞书 技术 LeaderLLM 时代的性能优化：出品人周经森（Kingsum Chow）智能化信创软件 IDE：出品人王亚伟 华为云 开发工具和效率领域首席专家LLM 时代的大前端技术：出品人樊中恺 百度 资深研发工程师、文心一言 APP 技术负责人高性能网关设计：出品人刘彦梅（喵吉） 淘天集团 资深技术专家面向人工智能时代的架构：出品人冯勇 通明智云 技术 VP构建本土编程语言生态的实践：出品人柴树杉 KusionStack 项目开源负责人性能工程：提升效率和创新的新方法：出品人熊刚 快手 基础平台部系统软件中心 / 系统软件负责人LLM 推理加速和大规模服务：出品人黄国平 博士 腾讯 AI Lab 专家研究员现代数据架构演进：出品人孙斌 爱奇艺 VP建设具备战略思维和弹性文化的组织：出品人石东海 要务科技 CEOServerless 化云产品架构设计与实践：出品人郭瑞杰（国泊） 阿里巴巴 总监</p><p></p><p>同时组委会也欢迎业界对这些专题有研究的专家，在 QCon 官网“<a href=\"https://qcon.infoq.cn/2023/shanghai/topic?utm_source=infoqweb&amp;utm_medium=qidongart&amp;utm_campaign=7&amp;utm_term=1114\">议题提交</a>\"”页面提交自己认为值得和参会者分享的话题（包括题目、内容介绍、专家介绍、面向群体、为什么参会者会感兴趣等，越详细越好！），对于 NB 的专家和内容，QCon 内容组委会一直持开放态度。</p><p></p><p>在这次的活动中，我们已经邀请到的讲师包括：</p><p></p><p>Masak Carl Wilhelm：华为 中央软件院编程语言实验室架构师</p><p>徐正：工银科技 技术二部副总经理</p><p>刘中杰：平安租赁 风险政策团队主管</p><p>张炜昕 博士：网易 杭州研究院 / 编程语言实验室 / 负责人</p><p>张佶：阿里巴巴 通义实验室 NLP 资深算法专家</p><p>熊刚：快手 基础平台部系统软件中心 / 系统软件负责人</p><p>王慧祥：字节跳动 火山引擎 DataLeap 资深架构师</p><p>孙建业：作业帮 大数据平台架构负责人</p><p>黄益聪：楷同科技有限公司 CEO</p><p>何普江：英特尔 数据中心与人工智能事业部 AI 软件架构师</p><p>魏永明：北京飞漫软件技术有限公司 创始人兼 CEO</p><p>李盛雁：上海微盟 运维部 / 容器运维专家</p><p>……</p><p></p><p>自 10 月上旬 QCon 上海 2023 大会开放报名以来，已经有近 200 人注册并确认参会，来自的企业包括微众银行、途虎、得物、七猫、招银网络、腾讯音乐、猫眼、广发证劵、华为、陌陌、中航信、美的、理想汽车等数百家企业。2023 年 11 月 17 日前报名可以享受 7 折优惠，单张门票节省 2040 元（原价 6800 元），团购（5 人以上，含 5 人）享有更多优惠，详情请致电 18514549229（同微信）。此外，本次 QCon 大会也已经邀请多位知名技术专家和公司，在 QCon 上海 2023 大会上安排闭门交流会议，包括大模型技术探索、架构设计等，敬请关注！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-11-14 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "富滇银行：用双中台战略支撑业务价值驱动的数字化需求",
    "url": "https://www.infoq.cn/article/7cp5DSABZFrYVWcr3tbH",
    "summary": "<p>富滇银行作为中小银行的典型代表，面临着息差收窄、产品同质化、技术和数据能力不足等诸多挑战。为了实现数字化转型，提升自身的竞争力，其在内部开展了“滇峰”计划项目，通过将传统的核心系统项目群进行重建和提升，形成以核心系统为中心的稳态架构落地，并实现了以用户为中心的敏态转型。</p><p></p><p>在这一过程中，富滇银行将“双中台”（数据中台 + 业务中台）能力建设作为重点，通过对企业能力的抽象，改变了传统的项目建设方式，从而更快速地响应用户需求，实现用户价值，为数字化转型打下了坚实的基础。</p><p></p><p>在日前的 InfoQ《超级连麦·数智大脑》直播节目中，富滇银行数字金融中心副主任李涛深入剖析了中小银行当前的发展现状和面临的挑战，他表示，基于这一系列独特的挑战，富滇银行在数字化转型的道路突破了中小银行一贯“跟随战略”，采用了领先一步的“一体化”全面转型战略。在战略制定过程中，也并不只是纯粹地追求先进技术，而是注重用户价值，聚焦于解决用户的实际问题。</p><p></p><p>尽管近年来围绕中台的争议不断，但“双中台”在富滇银行的落地实践，则恰恰满足了其满足用户需求的目标和初衷，是其数字化转型的关键基础和抓手。</p><p></p><p>以下是对话全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>InfoQ：富滇银行是中小银行的典型代表，请您介绍一下我国中小银行目前整体的发展现状以及面临的主要挑战？</h5><p></p><p></p><p>李涛：富滇银行的定位是服务地方经济，提供具有本土特色的服务。今年受内外部经济环境影响，许多银行面临息差收窄的压力，大型银行虽有较大的调整空间，但对于像富滇银行这样的小型银行来说，影响较大。地方性商业银行的风险抵御能力相对较弱，这就要求我们不断加强整体风险管理能力。</p><p></p><p>与此同时，中小银行面临的主要挑战还表现在以下几个方面：</p><p></p><p>区域化基因：以富滇银行为例，在云南省除了富滇之外，还有国有股份制银行、其他城市商业银行以及农村商业银行，这些金融机构之间的竞争激烈，人口和经济结构问题是影响区域银行发展的关键因素；</p><p></p><p>组织架构的适应性问题：虽然一些中小银行开始探索组织和部门的创新，但许多城市商业银行在这方面仍相对滞后，这限制了它们数字化发展的步伐；</p><p></p><p>产品同质化：尽管富滇银行利用地域特色和灵活政策优势在三农、小微企业和地方经济方面不断提升服务能力，但从全国范围来看，银行间的服务和产品高度同质化，缺乏差异化竞争；</p><p></p><p>技术和数据能力：中小银行普遍缺乏自主研发能力，大多依赖与金融科技厂商的合作，这在数据治理和技术生态合作方面没有太大优势，影响了对用户需求的快速响应；</p><p></p><p>金融科技人才的严重不足：富滇银行位于西南地区，人才支持不足，加之区域化经营的限制，使得人才招募受到地域和当地经济发展的限制。与四大国有银行和其他标杆银行相比，我们在金融科技人才比例上还有很大的差距。</p><p></p><h5>InfoQ：在加入富滇银行之前，您也曾在工行负责过业务架构设计等相关工作。对比这两段经历以及您的观察，国有股份制、全国性股份制、区域性银行（城商行、农商行）在数字化转型过程中存在哪些差异点？</h5><p></p><p></p><p>李涛：首先，资源投入方面，根据最近的调研报告，工农中建和交通银行等国有大行在 2022 年至 2023 年上半年的金融科技投入占所有银行总投入的 50% 以上。这一数据突显了中小银行在金融科技投入上的不足，许多甚至未达到行业平均水平。受疫情冲击和经济下行影响，中小银行的投入不仅没有增长，反而出现了收缩，使得其在金融科技投入上的比例远低于行业水平。</p><p></p><p>其次，金融科技人员占比也有较大差异。以城市商业银行的几个标杆为例，如宁波银行 2022 年报告显示其科技从业人员达到 1700 多人，上海银行为 1200 多人。科技人员与外包人员的比例在一些银行可以达到 1:5，甚至 1:7，这么算下来宁波银行的金融科技从业人员总数接近 8000 至 1 万人。相比之下，富滇银行即使经过几次扩招，目前的金融科技人员也仅 200 余人，人员占比的差异显而易见。</p><p></p><p>最后，战略规划方面。大型银行无论是在信息化还是数字化时代，都是从全行层面进行顶层设计，制定企业级的战略规划，并明确转型的目标、原则、路径、重点和方法，然后逐步执行。而中小银行由于更突出的经营压力，通常会重视短期业绩，对长期战略的培育和整体创新能力的关注不足，战略层面相对薄弱。中小银行的战略往往是跟随性，导致产品同质化严重。此外，人事变动可能导致战略无法持续执行，造成战略不连贯，进一步加剧了战略上的差异。</p><p></p><h5>InfoQ：对于富滇银行这样的区域性银行而言，在数字化转型过程中如何找准自身优势，避免陷入同质化竞争漩涡？</h5><p></p><p></p><p>李涛：众所周知，任何转型的基础都是组织转型，而技术转型仅是手段，组织转型才是根本。我们比较幸运，富滇银行党委对我们的定位和发展有着清晰的认识。早在几年前，就已经明确提出了我们的“一二三四”战略，即：以零售金融为基石，聚焦产业和公共事业、金融服务乡村和普惠金融，发展三个特色业务——绿色、文旅和跨境金融，以及推进四大工程——风险控制、科技创新、人才发展和客户服务。在这个战略中，科技创新、人才发展、风险控制被特别强调，以支撑整个数字化战略。</p><p></p><p>在最近召开的全国金融工作会议上，也提到了五个关键点，就包括数字化和养老服务等，这些都是顶层设计战略转型的全面支撑。因此，在数字化转型的过程中，我们相对来说更加轻松。银行领导也明确提出了“再造一个线上富滇银行”的口号，我们也因此有了用数字化方式全面革新所有业务领域的决心。</p><p></p><p>由于战略指导的明确，富滇银行在数字化转型的道路突破了中小银行一贯“跟随战略”，采用了领先一步的“一体化”全面转型战略。在数字化竞赛中，我们所做的就是保持领先，保持快速响应！</p><p></p><h5>InfoQ：“滇峰”计划作为富滇银行数字化转型项目，为中小银行业数字化树立了一个标杆，该项目的核心和亮点是什么？促使富滇银行开展这样一项计划背后的动因和驱动力又是什么？</h5><p></p><p></p><p>李涛：富滇银行 2016 年前的核心系统已经使用了十年，存在代际断层，业务流程中涉及大量人工操作，整体信息化水平落后于其他城商行。因此，2016 年开始我们制定了“一体两翼”战略，聚焦稳态和敏态两个方面。稳态是指以核心项目系统为中心的信息化全面能力的重建和提升；敏态则是指对客服务和产品方面的优化。</p><p></p><p>2019 年，我们完成了整个核心系统项目群的建设，形成了以核心系统为中心的传统金融业务群的稳态架构落地，这标志着我们解决了银行发展历史上的信息化支撑问题。</p><p></p><p>解决这个问题后，我们还有很多痛点，银行的信息化能力主要集中在内部效率提升和管理的规范化方面。到了 2018 年左右，我们发现用户服务存在很多痛点，因此着重落地了敏态架构。敏态架构旨在解决生存发展问题，通过数字化转型推进业务流程优化，以用户为中心实现银行业务价值的重构。</p><p></p><p>总结来说，在信息化阶段，我们解决了生存问题，而在敏态架构落地后，我们逐步解决发展问题和解决发展问题的能力。</p><p></p><p>自 2020 年以来，数字化已成为时代的必考题、必刷题。没有数字化的企业没有未来。敏态架构的落地代表着我们的发展有了强力的支撑，敏稳双态和信息化、数字化为我们提供了对客服务和高质量发展的支撑。</p><p>具体而言，“滇峰”计划大致分为几个阶段：</p><p></p><p>第一是调研阶段，通过广泛的交流和调研形成了整体架构设计；</p><p></p><p>第二是架构驱动阶段，以产品价值为导向，关注技术架构体系，包括云原生架构和基础服务能力，以及中台能力的建设和个人 APP 的上线；</p><p></p><p>第三阶段转向业务价值驱动。在基础设施和中台能力完备之后，便可以开始关注业务价值。比如产品竞争或企业服务的价值。通过这种方式，我们在整个过程中不断调整架构设计和方案，同时确保业务目标围绕几个核心目标展开。正是基于这种多层面的考虑和团队成员的努力，我们能够保持方向不变，控制需求范围，从而在两年时间内成功实施项目。</p><p></p><h5>InfoQ：富滇银行在制定数字化转型战略时，是如何考虑与自身业务和当前企业发展阶段相匹配的问题的？</h5><p></p><p></p><p>李涛：首先，金融政策是制定银行战略时必须考虑的关键因素；其次，地域化的经济结构差异也是一个重要考虑点；第三，随着金融科技行业的快速发展，我们已从大数据时代进入了以模型为基础的数智化时代。在战略制定过程中，我们不应仅追求高科技或先进技术，如大型模型的研发，而应更加注重用户价值。</p><p></p><p>在不同的时代，银行的战略焦点也有所不同。在信息化时代，产品即价值；在数字化时代，则转向以业务价值为驱动；而数智化时代，是用户价值驱动的时代。在这个时代，银行可以通过多种方式接触客户，了解他们的需求和情绪，从而提供符合监管合规要求的定制化服务。</p><p></p><p>因此，在制定战略时，银行应考虑内部和外部的痛点。特别是对于城市商业银行，经营压力巨大，因此战略应聚焦于解决这些实际问题。技术人员的工作目标就是解决问题。银行战略的制定也遵循这一逻辑。当然，高层领导在制定战略时还会考虑更长远的目标。</p><p></p><h5>InfoQ：富滇银行这边为了快速响应、快速迭代做了一些什么具体的措施？</h5><p></p><p></p><p>李涛：整个“滇峰计划”致力于实施一体化的数字化转型，重点在于构建基于阿里云平台的全面双中台（数据中台 + 业务中台）系统。尽管 2021 年围绕中台战略存在争议，我们仍视其为重要的发展方向。我们认为，对于银行来说，中台战略适合金融服务的落地。双中台战略支撑了整体架构，涵盖了接触客户的各种渠道，如 APP、小程序和短视频等等。</p><p></p><p>中台是企业级能力的服用中心，它改变了传统的项目建设方式，使我们能够更快速地响应用户需求。中台包括用户中心、产品中心、账户中心、支付中心和交易中心等，使这些能力可以被有效组合，以适应不同的需求。</p><p></p><p>数据中台在数字化时代变得至关重要，它整合来自业务中台、前端 APP 和小程序的大量数据，通过业务数据化，数据资产化，资产服务化，服务产品化形成了数字化运营的核心能力。我们通过不断的优化和调整流程，以满足用户的深层需求，并快速实现用户价值。</p><p></p><h5>InfoQ：在 FCon 大会的《金融领域数字化转型挑战探索》专题上，您将带来《业务价值驱动银行全面数字化转型实践》的分享。可以介绍一下，所谓“业务价值驱动”背后的含义和必要性吗？</h5><p></p><p></p><p>李涛：在银行业界，关于业务价值驱动和用户价值驱动的争论一直存在。实际上，我们必须承认，在信息化时代，技术的引领作用对业务发展至关重要。一旦拥有了产品，就能开展相关业务，而这些产品的建设依赖于技术的支持，因此可以说是技术在引领业务的发展。</p><p></p><p>进入数字化时代，尤其是随着数智化时代的到来，技术发展迅速，特别是在垂直领域。例如，随着高效编码工具和大模型的普及，技术不再是业务发展的限制因素。因此，逐渐转变为业务价值来驱动发展。这并不意味着业务完全驱动技术，但我们的业态确实发生了重要变化。</p><p></p><p>比如，从去年下半年开始，各种大会上都开始讨论业务价值驱动，而不仅仅是技术发展。人们从谈论 DevOps 转向讨论 BizDevOps，将业务放在更加重要的位置。</p><p></p><h5>InfoQ：回顾富滇银行的整个数字化转型历程，您认为最难或者最具有挑战的工作是什么？</h5><p></p><p></p><p>李涛：从架构设计的角度来看，我们从 2020 年开始着手进行数字化架构设计，这是基于 2016 年形成的战略规划。当时，敏态的规划相对宽泛。到了 2020 年，在实际架构设计过程中，我们发现全国范围内的企业级数字化转型架构设计还未出现。</p><p></p><p>但是，到了 2021 年，中台概念在银行界开始受到关注，同时也有一些对中台不看好的声音出现，导致一些银行停止了相关尝试。在架构设计方面，整个数字化转型的企业级架构更多是理论上的探讨，实践相对较少。</p><p></p><p>其次，企业级的敏捷项目管理也是我们关注的重点。原先的项目管理方式被改进，融入了敏捷理念。在项目高峰期，我们的团队规模接近 500 人，涉及数个供应商和多个前端平台。这种一体化的设计带来了达成目标一致性和交付一致性的挑战。</p><p></p><p>另外，中台的一个重要作用是解决前后台的速度差异问题。我们在实践中深切感受到前端、中台、服务端之间的速度差异。为此，我们不断强化敏捷文化，调整 DevOps 交付流程，并尝试了实例化需求，建立从需求条目到产品设计再到项目研发任务的完整矩阵，以此提高管理效能。如果没有这种企业级的敏捷项目管理，完成这样庞大的工程几乎是不可能的。</p><p></p><p>其中，数字化转型的目标和文化是一大挑战。一个关键问题是，不同人对业务中台、数据中台和数字化的理解各不相同，这使得达成统一目标变得困难。</p><p></p><p>最后，数字化产品经理的培养也是重要的一环。我们要求产品经理具备架构观，这对于数字化产品的设计至关重要。例如，我们拆解了 18 个能力中心，如用户中心、账户中心、交易中心、支付中心等，产品经理需要将这些能力有效串联起来。同时，我们希望业务人员具备产品化思维和架构观，技术人员也需具备产品思维和架构观，以便更好地协作。</p><p></p><h5>InfoQ：富滇银行业务中台拆解成了 18 个能力中心，拆解的依据是什么呢？</h5><p></p><p></p><p>李涛：业务中台的定位始终聚焦于作为企业能力复用的平台，这与我们最初的建设目标一致。然而，在实施过程中，存在对数字化、中台战略、业务中台和数据中台的不同理解，这些都影响了我们的进程。例如，对于营销和运营的认识，在 2021 年的规划和设计阶段，我们注意到许多银行案例中缺乏独立的运营平台，常常将营销活动的后台数据统计视为运营。</p><p></p><p>在实施“滇峰”计划时，我们决定将营销和运营拆分成两个独立的平台。</p><p></p><p>认知上的差异导致了我们在实施过程中遇到了一些弯路。然而，这些弯路实际上可能并不完全是误区，而更像是实施过程中的宝贵经验和总结，对于整个项目来说颇具价值和意义。</p><p></p><p>关于如何拆解的问题，起初，我们基于对互联网公司中台建设的调研和学习，模仿他们处理传统的用户账户和交易等方面。但在一段时间后，我们开始重新思考业务中台的定位。我们从整个数字化转型的目标出发，考虑了多项业务目标，包括 APP 重构、企业 APP 重构、场景金融建设、线上信贷产品建设以及小程序和营销活动的建设。</p><p></p><p>对这些业务目标进行分析时，我们考虑了银行层面所需的支撑能力。首先，我们通过业务建模进行分析，然后将这些能力按照领域逐一抽象，形成了中台的不同领域服务。这些服务进一步下沉为最基础的原子服务。这个过程实际上借鉴了领域驱动设计的思想，最终形成了包含 18 个能力中心的业务中台。</p><p></p><p></p><h5>嘉宾介绍：</h5><p></p><p>李涛 富滇银行数字金融中心副主任，富滇银行“滇峰”计划架构设计负责人。具有 20 年金融 IT 架构和敏捷实践经验，曾任职中国工商银行软件开发中心主要从事对私、对公核心业务架构设计和敏捷实践。2015 年加入富滇银行主要担任新一代核心系统建设架构师、项目经理。富滇银行全面数字化转型“滇峰”计划项目架构和产品团队负责人，并牵头负责“滇峰”计划项目组织敏捷改进相关工作。</p><p></p><p></p><h4>关于 FCon</h4><p></p><p>首届<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">FCon全球金融科技大会</a>\"将于 11 月 19-20 日在上海举办。本次大会已邀请到工商银行、招商银行、汇丰银行、兴业银行、中信银行、北京银行、平安人寿、度小满、蚂蚁集团等业界知名银行以及金融机构的大咖，前来分享大模型、 Web 3.0 、隐私计算、数字货币、区块链等前沿技术在金融领域的落地案例。</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，扫码或<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">点击链接</a>\"即可查看全部演讲专题。</p><p></p><p>目前是<a href=\"https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">9折特惠购票</a>\"，报名立减 ¥680，咨询购票可联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png\" /></p><p></p>",
    "publish_time": "2023-11-14 15:07:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "夸克发布自研大模型，加速下一代搜索体验创新",
    "url": "https://www.infoq.cn/article/DRxQfE7fMEg9qCUtbyha",
    "summary": "<p>国产大模型阵营再添新锐选手。11月14日，阿里巴巴智能信息事业群发布全栈自研、千亿级参数的夸克大模型，将应用于通用搜索、医疗健康、教育学习、职场办公等众多场景。夸克App将借助自研大模型全面升级，加速迈向年轻人工作、学习、生活的AI助手。</p><p></p><p>近期，在CMMLU权威大模型性能评测中，夸克大模型成绩位列榜首。最新评测显示，夸克大模型整体能力已经超过GPT-3.5，在写作、考试等部分场景中优于GPT-4。</p><p></p><h3>国产自研大模型中的“学霸”</h3><p></p><p></p><p>夸克大模型是基于Transformer架构、自主研发的多模态大模型，每天会对亿级的图文数据进行训练和精调，具有低成本、高响应、综合能力强等特点。同时，夸克大模型还将衍生出通识、医疗、教育等垂类模型，可以提供AIGC、智能检索的专业服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6e23b3e4d8e207f504ce60e3a4cb58ea.png\" /></p><p></p><p>性能方面，凭借语义理解、逻辑推理、内容生成等技术优势，夸克大模型在CMMLU权威榜单的最新评测结果中，取得排名第一的优异成绩。在国内专业考试测试中，夸克大模型高考成绩接近满分，并以486分的高水平通过临床执业医师资格考试，是名副其实的“学霸”。</p><p></p><p>同时，夸克大模型具备了对不良、虚假信息识别、回答和指引的出色能力。知识能力方面，夸克大模型拥有广泛的知识覆盖、上下文理解、创造性表达、信息搜集和整合、多语言支持等，同时具备外接专业知识增强、检索增强能力，进一步提升跨领域、时效性的知识和语言理解能力。此外，夸克大模型还具有撰写各类文本的强大文学创作能力，以及准确、合理、连贯的对话回复能力。</p><p></p><p>整体能力超过GPT-3.5，部分场景优于GPT-4，夸克大模型能力“爆表”源于数据、行业、知识正确性、平台等四方面优势。首先，夸克大模型拥有最全面的中文数据库，能更好地理解、评估、提炼中文知识体系；第二，夸克自建及拥有各类题库、知识点、医疗知识图谱、书籍及出版物等资料，沉淀了非常丰富的数据及用户场景；第三，在通用知识、写作增强等方面，夸克建立了从内容、搜索再到推理的一套可辨别知识真伪的技术体系。第四，夸克组建了数百人的研发团队，在搜索、教育、医疗等垂直领域中进行大模型的预训练与精调。</p><p></p><p>据介绍，坚持自研大模型的研发路线是服务于夸克的业务战略，也是持续推动夸克App在产品体验创新和迈向新一代搜索的技术底座。</p><p></p><h3>夸克将借助自研大模型全面升级</h3><p></p><p></p><p>今年以来，人工智能技术已经逐步融入到夸克App的产品迭代中。夸克扫描王能够在复杂场景下模仿人类思维，更精准地识别、分析和提取文字、公式及图片，实现更完美的扫描效果。夸克网盘上线的AI自然语言搜索功能，仅通过模糊词、形容词等关键信息，就能快速找到照片、文档等云端资料，进一步提升搜索效率。</p><p>作为最受年轻人青睐的智能产品，夸克App为数千万95后职场人和大学生提供了跨场景的智能效率工具。根据QuestMobile发布的《2023年轻人群智能效率应用研究》报告显示，夸克App在泛学生人群和新生代职场人群的用户占比最高，年轻用户使用时长位列行业第一。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7fffd1df832ac31de90ab2560852cea8.png\" /></p><p></p><p>据悉，夸克大模型将会优先落地在通识问答、专业搜索等信息服务领域，满足年轻人学习知识和提升自我的需求。未来，夸克大模型应用于搜索、智能工具和资产管理助手等场景，一系列AI原生应用将为年轻人工作、学习、生活提供更全面的服务。</p><p></p><p>今年9月，阿里集团宣布了用户为先、AI驱动的两大战略重心，将加大对“技术驱动的互联网平台业务”、“AI驱动的科技业务”等业务的战略性投入。近日举办的2023世界互联网大会上，阿里巴巴集团CEO吴泳铭预判，在可见的未来，会有更智能的下一代产品进入人们的生活，AI助理会无处不在，成为每个人工作、生活、学习中的助手。</p><p></p><p>“AI时代已经来临，大模型应用的全新体验临界点近在咫尺。”阿里巴巴智能信息事业群总裁吴嘉表示，基于大模型的AIGC技术将会给搜索产品带来全新变化，加速迈向下一代搜索。夸克借助自研大模型将全面升级，全新的夸克很快会和大家见面。</p>",
    "publish_time": "2023-11-14 16:32:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "囤H100的都成了大冤种！英伟达发布最新AI芯片H200：性能提升2倍，成本下降50%",
    "url": "https://www.infoq.cn/article/SSfWyUYdRbCtv86tNPcL",
    "summary": "<p>周一，半导体行业巨头英伟达发布了新一代人工智能芯片H200，旨在为各种AI模型提供训练和部署支持。</p><p>&nbsp;</p><p>H200芯片是目前用于训练最先进的大型语言模型H100芯片的升级版，搭载了141GB的内存，专注于执行“推理”任务。在进行推理或生成问题答案时，H200的性能相比H100提升了1.4至1.9倍不等。</p><p>&nbsp;</p><p></p><h2>性能拉升无极限？</h2><p></p><p>&nbsp;</p><p>据英伟达官网消息，基于英伟达的“Hopper”架构，H200是该公司首款采用HBM3e内存的芯片。这种内存速度更快、容量更大，使其更适用于大语言模型。相信过去一年来花大价钱购买过Hopper H100加速器的朋友都会为自己的冲动而后悔。为了防止囤积了大量H100的客户们当场掀杆而起，英伟达似乎只有一种办法：把配备141 GB HBM3e内存Hopper的价格，定为80 GB或96 GB HBM3内存版本的1.5到2倍。只有这样，才能让之前的“冤种”们稍微平衡一点。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a0311ec0902ccb16d73f6d5320809b05.png\" /></p><p></p><p>&nbsp;</p><p>下图所示，为H100与H200在一系列AI推理工作负载上的相对性能比较：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/82e1d5268e90a817ade6b6f5c3355e31.png\" /></p><p></p><p>&nbsp;</p><p>可以看到，相较于 H100，H200 的性能提升最主要体现在大模型的推理性能表现上。在处理 Llama 2 等大语言模型时，H200 的推理速度比 H100 提高了接近 2 倍。</p><p>&nbsp;</p><p>很明显，如果能在相同的功率范围之内实现2倍的性能提升，就意味着实际能耗和总体拥有成本降低了50%。所以从理论上讲，英伟达似乎可以让H200 GPU的价格与H100持平。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/3197a5e03741e2471e63ebe9fdce5fb2.png\" /></p><p></p><p>&nbsp;</p><p>得益于Tansformer引擎、浮点运算精度的下降以及更快的HBM3内存，今年起全面出货的H100在GPT-3 175B模型的推理性能方面已经较A100提升至11倍。而凭借更大、更快的HBM3e内存，无需任何硬件或代码变更的H200则直接把性能拉升至18倍。</p><p>&nbsp;</p><p>哪怕是与H100相比，H200的性能也提高至1.64倍，而这一切都纯粹源自内存容量和带宽的增长。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8cdd4d52bd633802e7d1d9c9c1c0bfcb.png\" /></p><p></p><p>&nbsp;</p><p>想象一下，如果未来的设备拥有512 GB HBM内存和10 TB/秒带宽，性能又会来到怎样的水平？大家愿意为这款能够全力施为的GPU支付多高的价钱？最终产品很可能要卖到6万甚至是9万美元，毕竟很多朋友已经愿意为目前未能充分发挥潜力的产品掏出3万美元了。</p><p>&nbsp;</p><p></p><h2>英伟达需要顺应大内存的发展趋势</h2><p></p><p>&nbsp;</p><p>出于种种技术和经济方面的权衡，几十年来各种处理器在算力方面往往配置过剩，但相应的内存带宽却相对不足。实际内存容量，往往要视设备和工作负载需求而定。</p><p>&nbsp;</p><p>Web基础设施类负载和那些相对简单的分析/数据库工作负载大多能在拥有十几条DDR内存通道的现代CPU上运行良好，但到了HPC模拟/建模乃至AI训练/推理这边，即使是最先进GPU的内存带宽和内存容量也相对不足，因此无法实质性提升芯片上既有向量与矩阵引擎的利用率。于是乎，这些GPU只能耗费大量时间等待数据交付，无法全力施展自身所长。</p><p>&nbsp;</p><p>所以答案就很明确了：应该在这些芯片上放置更多内存！但遗憾的是，高级计算引擎上的HBM内存成本往往比芯片本身还要高，因此添加更多内存自然面临很大的阻力。特别是如果添加内存就能让性能翻倍，那同样的HPC或AI应用性能将只需要一半的设备即可达成，这样的主意显然没法在董事会那边得到支持。这种主动压缩利润的思路，恐怕只能在市场供过于求，三、四家厂商争夺客户预算的时候才会发生。但很明显，现状并非如此。</p><p>&nbsp;</p><p>好在最终理性还是占据了上风，所以英特尔才推出了“Sapphire Rapids”至强SP芯片变体，配备有64 GB HBM2e内存。虽然每核分配到的内存才刚刚超过1&nbsp;GB，但总和内存带宽却可达到每秒1&nbsp;TB以上。对于各类对内存容量要求较低的工作负载，以及主要受带宽限制、而非容量限制的工作负载（主要体现在HPC类应用当中），只需转向HBM2e即可将性能提升1.8至1.9倍。于是乎，Sapphire Rapids的HBM变体自然成为1月份产品发布中最受关注、也最具现实意义的内容之一。英特尔还很有可能在接下来推出的“Granite Rapids”芯片中发布HBM变体，虽然号称是以多路复用器组合列（MCR）DDR5内存为卖点，但这种内存扩容的整体思路必将成为Granite Rapids架构中的重要部分。</p><p>&nbsp;</p><p>英伟达之前在丹佛举行的SC23超级计算大会上宣布推出新的“Hopper”H200 GPU加速器，AMD则将于12月6日发布面向数据中心的“Antares”GPU加速器系列——包括搭载192 GB HBM3内存的Instinct MI300X，以及拥有128 GB HBM3内存的CPU-GPU混合MI300A。很明显，英伟达也必须顺应这波趋势，至少也要为Hopper GPU配备更大的内存。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f5f872e3aebfff97720181f235ad821e.png\" /></p><p></p><p>&nbsp;</p><p>英伟达在一个月前的财务会议上放出技术路线图时，我们都知道GH200 GPU和H200 GPU加速器将成为“Blackwell”GB100 GPU及B100 GPU之前的过渡性产品，而后者计划在2024年内发布。人们普遍认为H200套件将拥有更大的内存，但我们认为英伟达应该想办法提升GPU引擎本身的性能。事实证明，通过扩大HBM内存并转向速度更快的HBM3e内存，英伟达完全可以在现有Hopper GPU的设计之上带来显著的性能提升，无需添加更多CUDA核心或者对GPU超频。</p><p>&nbsp;</p><p></p><h2>明年还有新的大冤种？</h2><p></p><p>&nbsp;</p><p>身处摩尔定律末期，在计算引擎中集成HBM内存所带来的高昂成本已经严重限制了性能扩展。英伟达和英特尔在Sapphire Rapids至强Max CPU上都公布了相应的统计数字。而无论英伟达接下来的Blackwell B100 GPU加速器具体表现如何，都基本可以断定会带来更强大的推理性能，而且这种性能提升很可能来自内存方面的突破、而非计算层面的升级。下面来看B100 GPU在GPT-3 175B参数模型上的推理能力提升：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d76e947f2ffdbb4fd491c62f22d2175f.png\" /></p><p></p><p>&nbsp;</p><p>因此，从现在到明年夏季之间砸钱购买英伟达Hopper G200的朋友，肯定又要被再割一波“韭菜”（当然，这也是数据中心持续发展下的常态）。</p><p>&nbsp;</p><p>最后：H200 GPU加速器和Grace-Hopper超级芯片将采用更新的Hopper GPU，配备更大、更快的内存，且计划于明年年中正式上市。也正因为如此，我们才认定Blackwell B100加速器虽然会在明年3月的GTC 2024大会上首次亮相，但实际出货恐怕要等到2024年底。当然，无论大家决定为自己的系统选择哪款产品，最好现在就提交订单，否则到时候肯定会一无所获。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.nvidia.com/en-us/data-center/h200/\">https://www.nvidia.com/en-us/data-center/h200/</a>\"</p><p><a href=\"https://www.nextplatform.com/2023/11/13/nvidia-pushes-hopper-hbm-memory-and-that-lifts-gpu-performance/?td=rt-3a\">https://www.nextplatform.com/2023/11/13/nvidia-pushes-hopper-hbm-memory-and-that-lifts-gpu-performance/?td=rt-3a</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-11-14 16:54:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "GitHub年度报告：生成式AI项目暴涨2倍，个人贡献者激增148%，从趋势看机遇何在？",
    "url": "https://www.infoq.cn/article/fuN7U32Tg1NsFoPozJAm",
    "summary": "<p></p><p>一项新技术成为主流意味着什么？</p><p></p><p>Git于2005年首次发布，在我们创建GitHub时，它仍然是一个新的开源版本控制系统。如今，Git是现代开发者体验的一个基础元素——93%的开发者使用它来构建和部署软件。</p><p>&nbsp;</p><p>在2023年，GitHub的数据强调了另一项技术如何迅速开始重塑开发者体验：人工智能。在过去的一年里，越来越多的开发者开始使用人工智能，同时也在尝试构建人工智能驱动的应用程序。Git从根本上改变了今天的开发者体验，现在人工智能正在为软件开发的下一步奠定基础。</p><p>&nbsp;</p><p>在GitHub，我们知道开发人员喜欢边做边学，开源可以帮助开发人员更快地采用新技术，将其集成到工作流程中，并构建下一步。开源也为几乎每一款现代软件提供了动力，包括大部分数字经济。在我们探索技术如何成为主流的过程中，GitHub继续在弥合实验和广泛采用开源技术之间的差距方面发挥着关键作用，开源技术是我们软件生态系统的基础。</p><p>&nbsp;</p><p>在今年的报告中，我们将研究围绕人工智能、云和Git的开源活动如何改变了开发者体验，并在开发者和组织中产生越来越大的影响。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fa/fad04cbba536bacf063ed07f07148972.png\" /></p><p></p><p>&nbsp;</p><p>我们发现了三大趋势：</p><p>一、开发人员正在大量使用生成式人工智能进行构建应用。我们看到越来越多的开发人员尝试OpenAI和其他人工智能参与者的基础模型，开源生成式人工智能项目甚至在2023年进入了贡献者数量排名前十的最受欢迎的开源项目。随着几乎所有开发者（92%）都在使用或试验人工智能编码工具，我们预计开源开发者将在GitHub上推动下一波人工智能创新。</p><p>二、开发人员正在大规模操作云原生应用程序。 我们看到，使用基于Git的基础设施作为代码（IaC）工作流的声明性语言越来越多，云部署的标准化程度越来越高，开发人员使用Dockerfiles和容器、IaC以及其他云原生技术的速度也急剧增加。</p><p>三、2023年首次开源贡献者数量最多。我们继续看到商业支持的开源项目在首次贡献者和整体贡献者中占据了最大份额——但今年，我们也看到生成式人工智能项目进入了首次贡献者最受欢迎的十大项目。我们还看到GitHub上的私人项目显著增长，同比增长38%，占GitHub所有活动的80%以上。</p><p>&nbsp;</p><p></p><h2>基于GitHub的全球开发者社区</h2><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/065d258e67728e5a514699afc66e7c54.png\" /></p><p></p><p>&nbsp;</p><p>在全球范围内，开发人员使用GitHub构建软件并进行协作的人数比以往任何时候都多，而且这种协作跨越了公共和私人项目。这不仅证明了Git在当今开发者体验中的基础价值，也展示了全球开发者社区在使用GitHub构建软件。</p><p>&nbsp;</p><p>&nbsp;</p><p>在过去的一年里，美国有2020万开发者，开发者增长了21%，仍然是全球最大的开发者社区。但自2013年以来，我们继续看到其他社区在整个平台上占据了更多的增长，我们预计增长将继续下去。GitHub上开发者的全球分布显示了哪些地区拥有最多的开发者。</p><p>&nbsp;</p><p>我们认为谁是开发人员？</p><p>我们将“开发者”定义为任何拥有GitHub帐户的人。</p><p>为什么？</p><p>开源和开发人员社区是一个越来越多样化的全球性群体，他们修补代码、做出非代码贡献、进行科学研究等等。GitHub用户推动开源创新，他们跨行业工作——从软件开发到数据分析和设计。</p><p>&nbsp;</p><p>亚太、非洲、南美和欧洲的开发商社区逐年扩大，其中印度、巴西和日本处于领先地位。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff5370d511e81bf8d2b5013c226fe43a.png\" /></p><p></p><p>&nbsp;</p><p>预测未来五年排名前十的开发者社区</p><p>为了了解哪些开发者社区在未来五年内增长最快，我们根据当前的增长率进行了预测。根据这一准则，我们预计到2027年，印度将超过美国，成为GitHub上最大的开发者社区。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/319120db793fa8d48d7605382165b868.png\" /></p><p></p><p>这些预测假设线性增长，以预测到2028年哪些开发者社区将成为GitHub上最大的开发者社区</p><p>&nbsp;</p><p>亚太地区发展最快的开发者社区</p><p>在印度、日本和新加坡的经济中心的推动下，我们继续看到亚太地区的可观增长。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5f/5faf64ecf6284ee51dbe864a427cad27.png\" /></p><p></p><p>表1:2023年开发商总增长率，比2022年增长%</p><p>&nbsp;</p><p>印度的开发者群体继续保持着巨大的同比增长</p><p>在去年的Octoverse中，我们预测印度的开发者总人口将超过美国。这仍在继续。印度的开发者人数同比增长36%，2023年有350万新开发者加入GitHub。</p><p>作为联合国支持的数字公共产品联盟的一部分，印度一直在用开放材料建设其数字公共基础设施，从软件代码到人工智能模型，以改善数字支付和电子商务系统。以下是印度开发人员在GitHub上构建并参与的开源软件（OSS）项目列表。</p><p>新加坡是亚太地区今年开发者人口增长最快的国家，在全球排名第一，开发者占总人口比例最高。</p><p>由于对技术和初创公司的投资，我们还可能看到日本的开发者在未来一年继续增长。</p><p>&nbsp;</p><p>非洲发展最快的开发者社区</p><p>非洲地区是世界上人口增长最快的地区，开发人员数量不断增加，已被确定为科技公司的重要枢纽。（例如，在肯尼亚，小学和中学必须教授编程。）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf38491dd4cf21c3ce55d3d84e7c1346.png\" /></p><p></p><p>表2:2023年开发者总增长率，比2022年增长%</p><p>&nbsp;</p><p>尼日利亚是开放源码软件采用和技术投资的热点，其45%的同比增长率——这是全球最大的增长率——反映了这一点。GitHub上还有一个由尼日利亚开发者制作的至少200个项目的集合，可以在“非洲制造”集合（<a href=\"https://github.com/collections/made-in-africa\">Collection: Made in Africa · GitHub</a>\"）中找到。</p><p>&nbsp;</p><p>南美洲发展最快的开发者社区</p><p>南美洲的开发者增长率与亚太和非洲一些增长最快的开发者社区不相上下。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/465a6a720cc7c616264fcecd812cb77b.png\" /></p><p></p><p>表3：2023年开发者总增长率，比2022年增长%</p><p>2023年，巴西的开发者人口是该地区最多的，并继续以两位数的速度增长，同比增长30%。此前，私人和公共组织继续在巴西进行投资。查看巴西开发人员在GitHub上制作并参与的OSS项目列表。</p><p>我们还看到阿根廷和哥伦比亚的经济持续增长，这两个国家在过去几年中已成为各组织的热门投资目标。</p><p>欧洲发展最快的开发者社区</p><p>整个欧洲的社区的总体开发者数量继续增加，但随着南美洲、非洲和亚太地区的社区增长超过他们，他们的发展现在更接近美国。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e676833eac4762cd63365e5ae65fea6f.png\" /></p><p></p><p>表4:2023年开发者总增长率，比2022年增长%</p><p>值得注意的是，法国的增长是在政府推动吸引更多科技初创企业之后实现的。我们还看到西班牙和意大利的增长有所上升，这表明这两个地区为支持其国内技术市场所做的努力。</p><p></p><h2>2023年生成人工智能的爆炸式增长</h2><p></p><p>虽然生成式人工智能在2023年的新闻头条上引起了轰动，但对GitHub上的开发者来说，这并不完全是新鲜事。事实上，在过去几年里，我们已经看到GitHub上出现了几个生成式人工智能项目，还有很多其他以人工智能为重点的项目。</p><p>但GitHub 2023年的数据反映了这些人工智能项目如何从更专业的工作和研究发展到更主流的被采用，开发者越来越多地使用预先训练的模型和API来构建生成式的人工智能应用程序。</p><p>就在过去一年的一半时间里，我们看到2023年生成式人工智能项目的数量是2022年全年的两倍多。我们知道这只是冰山一角。</p><p>随着越来越多的开发人员尝试这些新技术，我们希望他们能够推动软件开发中的人工智能创新，并继续将技术的快速发展能力纳入主流。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18a070ff20c40dbea23013130d4c228f.png\" /></p><p></p><p>开发人员越来越多地尝试人工智能模型。在过去的几年里，我们看到开发人员使用tensorflow、pytorch等机器学习库构建项目，而现在我们看到更多的开发人员在尝试人工智能模型和LLM，如ChatGPT API。</p><p>保持智能：我们预计企业和组织也会利用预先训练好的人工智能模型，尤其是随着越来越多的开发人员熟悉使用它们进行构建。</p><p>开源人工智能创新是多样化的，顶尖的人工智能项目由个人开发者所有。通过分析GitHub上排名前20位的开源生成式人工智能项目，其中一些排名前20的项目为个人所有。这表明，GitHub上的开源项目将继续推动创新，并向我们展示行业的下一步，社区将围绕最令人兴奋的进步进行建设。</p><p>生成式人工智能正在推动生成人工智能项目的个人贡献者在全球范围内大幅增长，同比增长148%，生成人工智能的项目总数也同比增长248%。值得注意的是，美国、印度和日本在开发者社区中处于领先地位，包括香港、英国和巴西在内的其他地区紧随其后。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e2/e251e0d0990f9e498321b0c77580a737.png\" /></p><p></p><p>学习生成式人工智能的开发人员数量的大幅增加将影响企业。随着越来越多的开发人员熟悉构建生成式人工智能应用程序，我们预计不断增长的人才库将支持那些寻求开发自己的人工智能产品和服务的企业。</p><p>最重要的是：在过去的一年里，随着开发人员使用这些LLM开发面向用户的工具，如API、机器人、助手、移动应用程序和插件，我们看到在基础模型之上构建的应用程序（如ChatGPT）呈指数级增长。全球的开发人员正在帮助为主流应用奠定基础，而实验正在帮助为组织建立人才库。</p><p></p><h2>最流行的编程语言</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9de89d211f7f14ca0d1bbab264053aa3.png\" /></p><p></p><p>自2019年我们看到云原生开发的巨大增长以来，IaC在开源方面持续增长。2023年，Shell和Hashicorp配置语言（HCL）再次成为开源项目中的顶级语言，这表明操作和IaC工作在开源领域越来越突出。</p><p>HCL的采用率同比增长36%，这表明开发人员正在为他们的应用程序使用基础设施。</p><p>HCL的增加表明，开发人员越来越多地使用声明性语言来决定他们如何利用云部署。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/5345290c201a6933089a4e747591f507.png\" /></p><p></p><p>JavaScript再次成为最受欢迎的语言，我们继续看到熟悉的语言，如Python和Java，年复一年地保持在前五名。</p><p>TypeScript越来越受欢迎。今年，TypeScript的用户群增长了37%，首次超过Java，成为GitHub上OSS项目中第三受欢迎的语言。TypeScript是一款集语言、类型检查器、编译器和语言服务于一体的软件，于2012年推出，标志着渐进类型的诞生，它允许开发人员在代码中采用不同级别的静态和动态类型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/398fb737cc52a8c731828e36d4fd1acd.png\" /></p><p></p><p>用于数据分析和操作的流行语言和框架显著增加。T-SQL和TeX等受人尊敬的语言在2023年发展起来，这突出了数据科学家、数学家和分析师如何越来越多地参与开源平台和工具。</p><p>最重要的是：编程语言不再局限于传统软件开发领域。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52ed90b936beb5c2c8eba124a1421e7c.png\" /></p><p></p><p>与GitHub中使用的总体最流行语言相比，我们发现2023年创建的项目中使用的最流行语言具有显著的对等性。一些显著的异常包括Kotlin、Rust、Go和Lua，它们在GitHub上的新项目中有更大的增长。</p><p>Rust和Lua都以其内存安全性和效率而闻名，它们都可以用于系统和嵌入式系统编程，这可以归因于它们的增长。Go最近的增长是由云原生项目推动的，如Kubernetes和Prometheus。</p><p></p><h2>开发者活动是新技术采用的风向标</h2><p></p><p>2023年初，我们庆祝了超过1亿开发者使用GitHub的里程碑——自去年以来，我们看到GitHub上的所有全球开发者账户增长了近26%。比以往任何时候都更多的开发人员跨时区协作并构建软件。私人和公共存储库中的开发人员活动强调了哪些技术正在被广泛采用，以及哪些技术将被更广泛地采用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/0395310707a6fb564de2799d0db0aded.png\" /></p><p></p><p>开发人员正在自动化更多的工作流程。在过去的一年里，开发人员在公共项目中自动化任务、开发CI/CD管道等方面使用的GitHub Actions分钟数增加了169%。</p><p>平均而言，开发人员在公共项目中每天使用超过2000万分钟的GitHub操作。随着2023年GitHub市场中GitHub操作的数量突破20000大关，社区不断增长。</p><p>这突出了开源社区对CI/CD自动化和社区管理的日益认识。</p><p>GitHub 80%以上的贡献都来自于私人存储库。这是对私人项目的42亿美元捐款，对公共和开源项目的3.1亿美元捐款。这些数字显示了通过免费、团队和GitHub Enterprise帐户在公共、开源和私有存储库中进行的活动的规模。丰富的私人活动表明了内部源代码的价值，以及基于Git的协作不仅有利于开源的质量，也有利于专有代码的质量。</p><p>事实上，在GitHub最近发起的一项调查中，所有开发人员都表示，他们的公司至少采用了一些内部源代码做法，超过一半的人表示他们的组织中有一种活跃的内部源代码文化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/ca7baa31ba5dac56dfd90ff604d5a4df.png\" /></p><p></p><p>GitHub是开发人员操作和扩展云原生应用程序的地方。2023年，430万个公共和私人存储库使用了Dockerfiles，超过100万个公共存储库使用Dockerfile创建容器。在过去几年中，我们在Terraform和其他云原生技术中看到了越来越多的使用。IaC的实践日益采用也表明，开发人员正在为云部署带来更多的标准化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/ad362b7cf73fd62a68d90561179e92f9.png\" /></p><p></p><p>Generative AI进入GitHub Actions。人工智能在开发者社区中的早期采用和协作能力在GitHub市场中的300多个人工智能驱动的GitHub动作和30多个GPT驱动的GitHub动作中表现得很明显。开发人员不仅继续尝试人工智能，还通过GitHub Marketplace将其引入开发人员体验和工作流程的更多部分。</p><p>重要的是：开发人员尝试新技术，并在公共和私人存储库中分享他们的经验。这项相互依存的工作揭示了容器化、自动化和CI/CD在开源社区和公司之间打包和运送代码的价值。</p><p></p><h2>开源代码中的安全状态</h2><p></p><p>今年，我们看到开发人员、OSS社区和公司都通过自动警报、工具和主动安全措施更快地响应安全事件，这有助于开发人员更快地获得更好的安全结果。我们也看到负责任的人工智能工具和研究在GitHub上共享。</p><p>越来越多的开发人员正在使用自动化来保护依赖关系。2023年，开源开发者为易受攻击的软件包合并了比2022年多60%的自动可靠拉取请求，这突出了共享社区对开源和安全的执着。由于GitHub上的免费工具，如可靠、代码扫描和秘密扫描，开源社区的开发人员正在修复更多易受攻击的包，并解决代码中的更多漏洞。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee261f2c9562705fe32ff53e49c8ea57.png\" /></p><p></p><p>我们通过一个名为Mona Rank的准则来计算前1000个公共项目，该准则评估了明星、分叉和独特问题作者的数量。我们采用所有具有许可证的公共、非分叉存储库，计算上述三个指标中的每一个的排名，然后使用总和来显示排名靠前的Mona排名项目。</p><p>&nbsp;</p><p>越来越多的开源维护人员正在保护他们的分支。受保护的分支为维护人员提供了更多的方法来确保其项目的安全，我们已经看到超过60%的最受欢迎的开源项目都在使用它们。自从今年早些时候我们在GA的GitHub上推出了存储库规则以来，大规模管理这些规则应该会变得更加容易。</p><p>&nbsp;</p><p>开发人员正在GitHub上共享负责任的人工智能工具。在实验生成式人工智能时代，我们看到了人工智能信任和安全工具的发展趋势。开发人员正在围绕负责任的人工智能、人工智能的公平性、负责任的机器学习和道德人工智能创建和共享工具。</p><p>&nbsp;</p><p>乔治城大学安全与新兴技术中心也在确定哪些国家和机构是值得信赖的人工智能研究的顶级生产者，并在GitHub上分享其研究代码。</p><p>&nbsp;</p><p>重要的是：为了帮助OSS社区和项目保持更安全，我们投资了向公共项目免费提供可靠、受保护的分支、CodeQL和秘密扫描。2023年的新采用指标显示了这些投资如何成功地帮助更多开源项目提高其整体安全性。我们也看到了软件开发人员和机构研究人员对创建和共享负责任的人工智能工具的兴趣。</p><p>&nbsp;</p><p></p><h2>开源代码的状态</h2><p></p><p>2023年，开发者为GitHub的开源项目贡献了3.01亿美元，这些项目从Mastodon等热门项目到Stable Diffusion和LangChain等生成式人工智能项目。</p><p>商业支持的项目继续吸引着一些最开源的贡献，但2023年是生成式人工智能项目首次进入GitHub十大最受欢迎项目。说到生成式人工智能，几乎三分之一至少有一颗星的开源项目都有一个使用GitHub Copilot的维护人员。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dd/ddbe66614573054e3eaa872fc02a76f1.png\" /></p><p></p><p>商业支持的项目继续处于领先地位。2023年，贡献者总数中最大的项目获得了压倒性的商业支持。与去年相比，这是一个持续的趋势，microsoft/vscode、flutter/flutter和vercel/next.js在2023年再次成为我们的前十名。</p><p>生成式人工智能在开源和公共项目中快速增长。2023年，我们看到基于人工智能的生成式OSS项目，如langchain AI/langchain和AUTOMATIC111/稳定扩散webui，在GitHub上排名第一。越来越多的开发人员正在使用预先训练的人工智能模型构建LLM应用程序，并根据用户需求定制人工智能应用程序。</p><p>开源维护人员正在采用生成式人工智能。几乎三分之一至少有一颗星的开源项目的维护人员正在使用GitHub Copilot。这是继我们向开源维护者免费提供GitHub Copilot的计划之后，显示了生成式人工智能在开源中的日益普及。</p><p>开发人员看到了组合包和容器化的好处。正如我们前面提到的，2023年有430万个存储库使用了Docker。另一方面，Linux发行版NixOS/nixpkgs在过去两年中一直是开源项目的首选。</p><p>首次贡献者继续青睐商业支持的项目。去年，我们发现，与其他项目相比，受欢迎的商业支持项目的品牌认可力吸引了更多的首次贡献者。2023年，微软、谷歌、Meta和Vercel支持了一些在首次贡献者中最受欢迎的开源项目。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/46/464c222a94b7710ad70d418b37559630.png\" /></p><p></p><p>在社区驱动的开源项目，从home-assistant/core到AUTOMATIC111/stable-diffusion-webui、langchain-ai/langchain和signifcant-gravitas/Auto-GPT，首次贡献者的活动也激增。这表明，基础模型的开放实验增加了生成式人工智能的可访问性，为新的创新和更多的合作打开了大门。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c58d485e3849eef206900cfebc3f6e35.png\" /></p><p></p><p>2023年，首次为开源项目捐款的人数最多。新的开发人员通过freeCodeCamp、First Contributions和GitHub Education等项目加入了开源社区。我们还看到大量开发人员参与了谷歌和IBM等公司的在线开源教育项目。</p><p>重要的是：开发人员正在为开源生成人工智能项目做出贡献，开源维护人员正在采用生成人工智能编码工具，公司继续依赖开源软件。这些都表明，无论是在公共还是私人存储库中工作，公开学习并分享新技术实验的开发人员都提升了整个全球开发人员网络。</p><p>&nbsp;</p><p>正如Git已经成为当今开发者体验的基础一样，我们现在看到了人工智能主流出现的证据。仅在过去一年，就有92%的开发者报告在工作内外使用了基于人工智能的编码工具。在过去的一年里，GitHub上托管的各种开源项目中的人工智能实验也出现了爆炸性的激增。</p><p>我们给您留下三条路径：</p><p>1、GitHub是生成式人工智能的开发者平台。2023年，生成式人工智能从一个专业领域发展成为主流技术，开源活动的激增反映了这一点。随着越来越多的开发人员构建和实验生成式人工智能，他们正在使用GitHub进行协作和集体学习。</p><p>2、开发人员正在GitHub上大规模操作云原生应用程序。2019年，我们开始看到在开源中使用基于容器技术的开发人员数量大幅增加，2023年，开发人员越来越多地使用基于Git的IaC工作流、容器编排和其他云原生技术。这一巨大的活动表明，开发人员正在使用GitHub来标准化他们如何将软件部署到云上。</p><p>3、GitHub是开源社区、开发人员和公司构建软件的地方。2023年，我们看到私人存储库的数量增加了38%，占GitHub所有活动的81%以上。但我们看到开源社区的持续增长，他们正在使用GitHub构建下一步，推动行业向前发展。数据显示，新的开源开发人员不断增加，开放社区的创新步伐也很快，很明显，开源从未如此强大。</p><p></p><h2>报告研究背景</h2><p></p><p>本报告利用了2022年10月1日至2023年9月30日期间从GitHub获取的匿名用户和产品数据。我们在GitHub上通过683个存储库主题术语定义人工智能项目，您可以在我们2023年进行的研究中了解更多信息。我们还通过一种称为“Mona Rank”的指标来评估开源项目，这是一种基于排名的项目社区规模和受欢迎程度分析。</p><p>更多数据可在GitHub创新图上公开获取，这是GitHub为对整个GitHub的软件开发状态感到好奇的组织和个人提供的研究工具。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://github.blog/2023-11-08-the-state-of-open-source-and-ai/#take-this-with-you\">Octoverse: The state of open source and rise of AI in 2023 - The GitHub Blog</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-11-14 17:04:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "零一万物回应“套壳Llama”争议：基于GPT研发，对模型和训练的理解做了大量工作",
    "url": "https://www.infoq.cn/article/cVfuQaHVJ0SDPtP2jb7m",
    "summary": "<p>11月14日，<a href=\"https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8\">李开复</a>\"旗下AI企业<a href=\"https://www.infoq.cn/news/3m7F87QpDVsu8zv68k1b\">零一万物开源大模型Yi-34B</a>\"被指责完全使用LLaMA的架构 ，只对两个张量 (Tensor) 名称进行修改。</p><p>&nbsp;</p><p>对此，零一万物表示：GPT是一个业内公认的成熟架构，Llama在GPT上做了总结。零一万物研发大模型的结构设计基于GPT成熟结构，借鉴了行业顶尖水平的公开成果，由于大模型技术发展还在非常初期，与行业主流保持一致的结构，更有利于整体的适配与未来的迭代。同时零一万物团队对模型和训练的理解做了大量工作，也在持续探索模型结构层面本质上的突破。</p><p>&nbsp;</p><p>此事起源于贾扬清在朋友圈的一个吐槽，贾扬清提到，有个“大厂新模型exactly就是LLaMA的架构，但是为了表示不一样，把代码里面的名字从LLaMA改成了他们的名字，然后换了几个变量名。然后，海外有工程师直接指了这一点出来... 还有人在HF上面放了个把名字改回去的checkpoint，说好了，现在你们可以直接用LLaMA的代码来load这个checkpoint了”。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3a97b7c7b72dfa9bfa6e22d9a363e53.png\" /></p><p></p><p>一时间，大家纷纷猜测这个基于Llama魔改的大模型到底是哪个。贾扬清随后专门留言表示不是自己的老东家阿里的。后来，有人扒到<a href=\"https://huggingface.co/01-ai/Yi-34B/discussions/11\">Hugging Face社区的Yi-34B项目下讨论区的留言</a>\"，留言指出，“除了两个张量被重新命名外，Yi完全使用了Llama的架构。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc0f681a1b788a18b50b58e923f37e02.png\" /></p><p></p><p>&nbsp;</p><p>有网友评论称，“如果他们使用了确切的 Meta LLaMA 结构、代码库和所有相关资源，则还需要遵守 LLaMA 规定的许可协议。要求以 LLaMA 形式正式发布 Yi 模型是有问题的，因为它破坏了 Yi 许可条款的可执行性。”</p>",
    "publish_time": "2023-11-14 17:16:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Moby 项目进展迅猛：2023年三个大版本",
    "url": "https://www.infoq.cn/article/eyvkUvaV66Ww8vctLQx0",
    "summary": "<p></p><h1>Docker Swarm 有新特性了：Moby 项目更新频繁，2023年三个大版本</h1><p></p><p>作者 | Loraine Lawson</p><p></p><p>译者 | 王强</p><p></p><p>策划 | Tina</p><p></p><p>Moby 是一个从 Docker 衍生出来的开源项目，它在今年计划发布三个主要版本更新。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/99/99ed1ef0c827bbab9df5c66e1763419c.png\" /></p><p></p><p>Docker 工程师 Sebastiaan van Stijn（左）与 Bjorn Neeargaard 在 Dockercon 23 上的演讲。</p><p></p><p>开源 Moby 项目的最后一个主要版本是在 2020 年发布的，但据两位 Moby 贡献者称，今年该项目将发布三个主要版本。</p><p></p><p>Moby 项目是一个组件集合，用于构建基于容器的系统。项目中包括了容器运行时、容器注册表、容器构建工具、编排工具以及网络、日志记录和监控工具。这些组件可用于构建基于容器的系统，例如云原生应用程序、微服务架构、CI/CD 管道和本地容器平台等。</p><p></p><p>Moby 维护者 Bjorn Neergaard 是 Docker 的高级软件工程师。本月早些时候，他与技术指导委员会成员 Sebastiaan van Stijn（也是 Docker 的软件工程师）一起在 DockerCon 上介绍了 Moby 项目的最新进展，并公布了 2023 年将要发布的主要版本的细节以及未来的计划。</p><p></p><p>Moby 诞生背景简介</p><p></p><p>Neergaard 和 van Stijn 在 DockerCon 演讲中首先简要介绍了 Moby 这个开源项目的来历。van Stijn 表示，它可以追溯到开发人员最早使用容器作为轻量级虚拟机的时代，那时候这种虚拟机很难用且非常小众。</p><p></p><p>“它没有流行起来，因为它太复杂了，”van Stijn 说。“它很难保持同步；没有镜像分发，啥分发都没有。”</p><p></p><p>然后 dotCloud（一个小型的平台即服务，后来成为 Docker）开始向用户提供服务了。不过实际上，技术人员真正感兴趣的是 dotCloud 在幕后所做的事情：他们正在部署很多技术容器，其中大部分是用 Python 编写的，得写大量脚本才能把容器跑起来，van Stijn 解释道。后来 dotCloud 决定把他们一直在内部使用的东西开源。</p><p></p><p>然后，在 2013 年，Docker 创始人 Solomon Hykes 在 PyCon 的闪电演讲中介绍了 Linux 容器。</p><p></p><p>“虽然只提了五分钟，但它在业界引起了不小的震动，因为在这五分钟内他第一次展示了一个跑起来的 Docker，”van Stijn 说。“Docker 包揽了他原本要用 LXC 来做的大量工作，但只需一条命令就搞定了。”</p><p></p><p>当时 Docker 仍然是 LXC 的一个包装器，由 LXC 承担所有繁重的任务。它简化了用户操作体验，而且还提供了一种镜像格式——这是一大进步，因为现在开发人员可以给容器用镜像了，用不着为容器创建专属的文件系统。这时候还没有构建。他补充说，它还提供了一个 API，让开发人员可以做一些“很酷的事情”。</p><p></p><p>“它确实对市场造成了很大的影响，因为 Linux 容器第一次成为现实，并走进了开发人员的工作中，”他说。</p><p></p><p>van Stijn 表示，LXC 用起来挺好的，但 Docker 还是决定重写运行时，这样就可以有内置于 Docker 引擎中的原生运行时了。后来随着更多功能（例如网络）进入 Docker，这一步被证明是非常正确的。容器流行了起来，但每个容器都有一个任务，这意味着程序员在大多数堆栈中都需要多个容器。于是有人开始尝试做编排，后来编排也有了组合，而且让用户可以定义 YAML 文件了。</p><p></p><p>Docker 收购了 Fig，后者成为了 Docker Compose。然后 Docker 推出了 Swarm，它的第一个版本让开发人员可以在机器集群中运行他们的容器。van Stijn 表示，随后 Kubernetes 上线并决定使用 Docker 作为运行时，因为后者是运行容器的事实标准。他补充说，这也导致了一些问题，因为越来越多的人要求的各种功能显然超出了该项目的设计目标。</p><p></p><p>“Kubernetes 不需要 Docker 的网络堆栈，他们不需要我们提供的其他东西，但他们仍在使用运行时，有时就会出各种问题，”他说。“引擎的单体架构问题越来越大。”</p><p></p><p>此外，虽然 Docker 是事实标准，但这方面并没有关于容器镜像或运行时运行方式的正式规范，他说。</p><p></p><p>“实现是规范，但并不总是让人满意，”他说。</p><p></p><p>Docker 决定把实际的运行时分割出去。大约在同一时间，标准组织 OCI 成立。Docker 向组织捐赠了用于分发镜像的规范，以及运行时规范和镜像。</p><p></p><p>“现在其他人也可以实现运行时、镜像和注册表了，Docker 不再是唯一选项。”</p><p></p><p>Docker 还开始与几个合作伙伴公司从头开始重写运行时，结果就是 containerd（发音为“container D”），这是对 Docker 运行时部分的完全重写。</p><p></p><p>Moby 项目的诞生</p><p></p><p>van Stijn 在演讲中告诉观众，当 Docker 决定将项目进一步拆分为多个更小的组件时，Moby 项目就开始了，因为人们想要使用 Containerd 和 Docker 引擎的其他部分。这也催生了用于构建用途的 Build Kits、用于编排的 Swarm Kit 以及 Docker 引擎。他补充说，作为一个单独的项目，CLI 成为了 Docker 产品的一部分。运行时本身演变为 Moby 项目。</p><p></p><p>他说：“这可以让大家在其基础上来做构建、参与，但也让人们更容易接受一些可能不会直接让 Docker 这个产品获益、但可以被他人用上的更改，反之亦然。”</p><p></p><p>随着企业产品转向 Mirantis，Docker 本身也发生了变化，它又变回了面向开发者的产品。他补充说，Docker 开始专注于 Docker 桌面，而 Moby 项目的工作进展缓慢，直到过去 18-24 个月，Mirantis 和微软的维护人员加入了这项工作后情况开始变化。</p><p></p><p>“有一件事情让大家很是困惑，那就是名为 Docker 的开源代码到底变成了什么样，”Neergaard 解释道。“但也许这也能说明一个问题：在 Docker, Inc. 之外，现在项目有了更多的参与者——而且不仅仅是参与者，还是项目的利益相关者。”</p><p></p><p>Neergaard 补充道，除了 Mirantis 和微软之外，英伟达最近也贡献了容器设备接口支持。</p><p></p><p>现在是什么情况</p><p></p><p>“最近，我们也看到该项目有了更多的活动，”Neergaard 说。“可以从多种形式看出这一变化，不过下图可能还不全。”</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6e/6ee3e89de33037e1ac3a5b3ba8d5650b.png\" /></p><p></p><p>Moby 项目的最新活动</p><p></p><p>Docker 引擎的最新版本是在 2020 年发布的。时至今日又出现了很多代码和改进，但一直没出可以搭载这些改进的新版本，他补充道。</p><p></p><p>今年已经发布了两个主要版本——版本 23.0 和 24.0，其主要特性包括：</p><p></p><p>BuildKit，默认开启（不再有 DOCKER_BUILDKIT=1）。Neergaard 说，BuildKit 是对构建器的重写。“BuildKits 最初的任务和计划的一部分是让它取代 Docker 引擎中的经典遗留构建器，并提供更丰富、更灵活的构建平台，并且依旧像 Docker 构建一样简单，”他说。“所以 BuildKit 现在默认处于启用状态。”Swarm 中的 CSI（容器存储接口）可选 containerd shim：gVisorKata 容器WebAsseambly</p><p></p><p>Neergaard 说，备选 shim “可能很无聊”，但开辟了很多可能性，“特别是对于那些想搞出一些新方法来运行容器或看起来有点像容器的东西（例如 WebAssembly）的人们来说是这样的。”</p><p></p><p>该团队曾希望在 DockerCon 之前提供第三个版本 25.0，但结果没有成行。根据演示内容，预计这个版本现在会随时发布。该版本将包括：</p><p></p><p>CDI（容器设备接口）OTEL（OpenTelemetry）集成到引擎中Neergaard 说，“优雅”的健康检查与健康开始间隔长期以来一直是一个痛点。</p><p></p><p>“今年我们的确准备发布三个版本的引擎更新，不管怎样这是很大的挑战；我们每次都在进步。”Neergaard 说道。</p><p></p><p>“另一件有趣且让人想不到的事情是 Docker Swarm 中还有新特性，”他说。他解释说，Swarm 是 Docker 对 Kubernetes 的回应。</p><p></p><p>Neergaard 说：“在这一点上，我想说 Kubernetes 在很大程度上是编排平台的事实标准，除非有充分的理由，否则你可能不应该选择 Kubernetes 以外的其他平台。” “有一小部分颇有声量的用户群体喜欢使用 Swarm，并且希望 Swarm 能做更多事情，甚至与 Kubernetes 现有的许多附加组件和扩展兼容。”</p><p></p><p>Moby 项目的未来计划包括在 Containerd 中添加多个快照程序和原生多架构，以及重新设计的 CLI、错误修复和新的网络功能，还有将协调器逻辑从 Compose 移动到声明式 Docker 的守护进程中，两人补充道。</p><p></p><p>原文链接：</p><p></p><p><a href=\"https://thenewstack.io/the-moby-project-post-kubernetes-3-new-releases-in-2023/\">https://thenewstack.io/the-moby-project-post-kubernetes-3-new-releases-in-2023/</a>\"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651185730&amp;idx=1&amp;sn=f99df153453ad8f82aefe434937b14bb&amp;chksm=bdb81a118acf9307f9ff425083612c1cc2b13a222273bf20c8888672fbc3212bd4560ee51f12&amp;scene=21#wechat_redirect\">“我不懂的、总有人懂”，软件开发行业已经开始扭曲变形</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651185651&amp;idx=1&amp;sn=7e0d67d98a40ecab7e2982545fea7652&amp;chksm=bdb819a08acf90b68d5a5f4a646032385d5b98153eafb7b2fd248022b058d0563a9c77265cac&amp;scene=21#wechat_redirect\">OpenAI 刚刚又杀死了一批初创公司</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651185277&amp;idx=1&amp;sn=0fa19938094da0d3f9107f8042a407d0&amp;chksm=bdb8182e8acf913865bd6d303c523da7212d3f59c7cd08d7d2288de5fcb044cd0417baa4c9f0&amp;scene=21#wechat_redirect\">疯狂马斯克的“极限”计划居然成功了？！“下云”后成本降低 60%，部分功能代码精简 90%，30 天急速迁移服务器</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651185252&amp;idx=1&amp;sn=f550284115f84cb0c65112b7684957c7&amp;chksm=bdb818378acf91213654f673e6122ed31d72a5ec61ac3089e48bda6f92a21b4be5cc001b6049&amp;scene=21#wechat_redirect\">程序员篡改 ETC 余额，一年私吞 260 余万元；语雀公布故障原因及赔偿方案；各家财报发布，创始人们：就很难受｜Q资讯</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/93/93a6e30a75e8e663d639c54513765ef5\" /></p><p></p>",
    "publish_time": "2023-11-14 17:39:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]