[
  {
    "title": "架构师（2022 年 10 月）",
    "url": "https://www.infoq.cn/article/kHgvXNRNW124to0AJFeG",
    "summary": "<h2>卷首语：Go 语言专家曹大谈十年技术生涯：有理想，但不理想化</h2>\n<p><strong>作者｜贾亚宁</strong></p>\n<p>每一个互联网从业者可能都思考过这样的问题：如何能从人才大军中脱颖而出，如何能更高效率地获得成长和发展？关于这个话题，我们对 Go 语言专家曹大（曹春晖）老师进行了采访，一起来看看他的实践和思考吧。</p>\n<p><strong>InfoQ：你的职业生涯中印象最深刻的挑战是什么？你是如何解决的呢？</strong></p>\n<p>曹春晖：技术的挑战都不算什么大事，工程方面的问题一定可以找到解决方案，找不到方案的那就是世界难题了。在公司里最大的挑战是有 idea 的时候，要说服老板去做（碰到太多次了），因为公司里的老板背景各异，不同的人不可能对一件事情有相同的看法，这时候要达成共识就需要反复沟通，拉齐、磨合、开会，摆出各种软件工程系统架构理论，再搬出硅谷的 FAANG 是怎么做的，很心累。能说服老板的话，其它事情都是小事，这很难避免，因为大公司都是这样的。</p>\n<p><strong>InfoQ：方便分享一下你的职业发展路径吗？你认为影响技术人的职业发展的重要因素有哪些？</strong></p>\n<p>曹春晖：我的职业发展参考价值不大，其实这么多年看下来，在同样靠谱的前提下，职业的发展顺利与否主要都是看运气……了解了一些公司运作的规律和招人的潜规则以后，很多事情比较无奈，不太好说（真心话）。</p>\n<p>学习方面给各位技术人的建议是要养成终生学习的习惯，不要学到某个程度就固步自封，即使是后端技术，五年前和现在其实差别已经挺大的了。我以前写过一篇《工程师应该如何学习》，介绍过一些学习的渠道和方法，可以参考。工作中打过交道的人，其实很多工作十年的人跟刚毕业的学生没什么区别，你看看你们公司的架构师，CRUD 都能搬出 100 种理论来说明为什么要这么设计，这才是真水平。</p>\n<p>另外，写代码以外的技能也是要刻意训练的，比如口才、演技等，很多踏实写代码的人其实在大公司里很吃亏，要理解什么是一定程度上的按闹分配。</p>\n<p><strong>InfoQ：作为常年在 Go 语言世界里探索的人，你认为什么是学习 Go 语言最高效的方法？</strong></p>\n<p>曹春晖：入门的话就看《The Go Programming Language》，最好的学习方法就是把书上的例子都抄一遍。可以翻翻优秀的 Go 项目来学习他们是如何使用这门语言的，前面也提到了，awesome-go 里有很多这样的项目。直到你觉得 Go 的所有语法都了熟于心，就算是入门了吧。</p>\n<p>进阶要去学习怎么定位 Go 在线上系统的问题，成为一个 Go 的高级工程师。这部分需要大家了解一些 Go 的底层知识，学习基于 goroutine 和 channel 的各种并发编程模式，以及常用的工具链：比如 pprof 怎么用，怎么用 --base 去找内存泄露，出了性能问题怎么做优化等等。要达到的目标是：线上的 Go 系统出了问题，能够通过自己的知识储备快速定位。Go 的底层知识现在国内比 java 圈还卷，文章很泛滥，可以随意搜搜，择优阅读。</p>\n<p>下一步是要通过多接触各种场景（比如大流量，高并发的，业务的，基础设施的等等），同时与其它语言横向做对比，了解 Go 语言在各种场景下的优缺点，不要成为一个语言原教旨主义者，比如我在工作的过程中就看到过不少 Go 其实就没法应付的场景，大家硬着头皮用，硬着头皮 hack，项目搞到最后优化起来也很痛苦，可能还不如直接去用 Rust。尽量多思考，也不要忌讳与其它语言的熟手交流。</p>\n<p>相比 PHP 之类的语言，Go 的最大好处是可以跨越很多领域，从业务到基础设施，如果你业务干腻了，技术储备又足够，也可以去玩玩基础设施开发 (当然，我不建议你总是换， 实在手痒可以业余时间给一些开源基础设施项目提提 PR)。也就是大家的工作内容选择范围会大很多，这又需要考验你的智慧了，选择哪个领域才能赚到钱，我没法给出很具体的建议，哈哈。</p>\n<p><strong>InfoQ：你是如何做到长期内容输出的呢？对想要开始的小伙伴有什么建议吗？</strong></p>\n<p>曹春晖：要给自己设立一个简单的目标，同时不要影响到工作，我之前写了很多年 blog，体感最合适的是一个月两篇文章。内容可以和工作相关，也可以无关。两周一篇，压力也不太大，可以没事想起来就写写。</p>\n<p>输出是很重要的，费曼学习法告诉我们，你要把知识讲得明白，才能确定自己是真的会了，只是应付工作里的那点东西，很多时候人会变得懒惰，慢慢也就不想思考了。我曾经在某公司听到一个很高级别的人自豪地说他很多年没看一本书，还很骄傲地讲给他的下属听，我不认为新进入行业的人能够有像这位一样靠公司起飞的运气，还是要把学习当回事的。</p>\n<p>因为增加了输入，这些输入一定会在某个时间点对你的工作产生帮助。在某大公司的时候我也看到内网上一些优秀的文章作者，他们基本上都保持了持续内容输出的习惯，这也意味着他们看了非常非常多的书。有些人的读书速度能快到匪夷所思的程度 (一年 68 本)，不知道他们是怎么做到的。</p>\n<p>多积累一些知识以后的输出是很有价值的，我之前写的系统设计的文章被人看到以后会来问我是怎么做的，说不定聊着聊着就成同事了。</p>\n<h2>目录</h2>\n<p><strong>热点 | Hot</strong><br />\nJavaScript 框架大战已结束，赢家只有一个<br />\n接手了一座年收入 2000 万美元的代码“屎山”，我到底是该重写还是该跳槽？<br />\n谷歌推出 Carbon 后，我在思考为什么 Rust 没能成为 C++ 的正式继任者</p>\n<p><strong>理论派｜Theory</strong><br />\n理想汽车：从 Hadoop 到云原生的演进与思考<br />\n去哪儿旅行微服务架构实践</p>\n<p><strong>推荐文章 | Article</strong><br />\n从一线研发到公司创始人，基础软件创业者迷雾中与市场赛跑<br />\n作为现代开发的基础，为什么 TDD 没有被广泛采用？</p>\n<p><strong>观点 | Opinion</strong><br />\nReact 老矣，我建议大家用用别的框架</p>",
    "publish_time": "2022-10-08 09:34:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "去哪儿网 Service Mesh 落地实践：100%容器化打底，业务友好是接入关键",
    "url": "https://www.infoq.cn/article/USjDHBrY4DtG9BjOKE3A",
    "summary": "<p>自 2011 年引入 Dubbo 框架至今，去哪儿网的微服务架构已经有了十多年的历史。时至今日，去哪儿网架构已经形成了 Dubbo、HTTP 协议为主，多语言共存的格局。</p><p></p><p>根据去哪儿网基础架构部高级技术总监朱仕智分享的数据，现在去哪儿网 Dubbo 服务接口有 18,000 多个、HTTP 域名超过 3500 个，内部大约有 Java、Golang、Node 和 Python 等 5 种语言的技术栈，其中以 Java 和 Node 为主。此外，去哪儿网在线运行的应用大约 3000 多个、MQ 应用主题超 13,000 个。</p><p></p><p>随着业务的不断发展，多语言、多协议给基础架构团队带来了不小的治理问题。</p><p></p><p>首先，多语言的复杂性主要在于 SDK 的重复实现，每个治理逻辑需要开发不同的语言版本，此后还要一直对其进行维护，这是一份冗余且负担很重的工作。</p><p></p><p>在多协议治理上，一方面，去哪儿网内部有 Cactus 平台对 Dubbo 进行治理，治理功能相对完善。另一方面，去哪儿网对 HTTP 的治理主要通过 OpenRestry，治理能力相对薄弱，一些必需的治理需求零散分布在多个平台，对开发同学很不友好。</p><p></p><p>Dubbo 和 HTTP 治理能力不统一导致了重复造轮子情况，增加了业务的开发和维护成本。</p><p></p><p>此外，业务和中间件强耦合，但业务部门与基础架构部门对升级更新的意愿并不同步。基础架构部门不断升级更新来完善中间件，但业务研发在不影响使用情况下并不希望频繁更新。所以每次发布新版本后，基础架构团队要花很长的时间进行推广，致使整个迭代周期变长。</p><p></p><p>而 Service Mesh 将服务治理能力下沉到了 sidecar，SDK 只需要负责编解码、与 sidecar 通信，通用逻辑基本不用修改，只要对 sidecar 修改就可以应用到所有语言和框架，于是在去年，去哪儿网开始组织团队引入 Service Mesh 来解决自身多语言、多框架的治理难题。</p><p></p><h2>容器化基础</h2><p></p><p></p><p>在去哪儿网资深开发工程师李佳龙看来，容器化下比较适合引入 Service Mesh。</p><p></p><p>大概从 2014 年起，去哪儿网便开始使用 Docker、Mesos、Kubernetes（以下简称 K8s）等来解决测试环境构建困难的问题，也逐渐尝试基于容器部署 ES、MySQL 等中间件服务。到了 2021 年底，去哪儿网全面实现容器化，业务服务基本完成上云。从效果数据来看，容器化虚拟比例从 1：17 提升到 1：30，资源利用率提升 76%，宿主运维时间由天变成了分钟级，环境稳定性差、交付慢、可扩展性差和服务器成本高等问题都得到了有效解决。</p><p></p><p>经过多年积累，去哪儿网已经为引入 Service Mesh 打下了坚实的基础。引入 Service Mesh，本质上也是去哪儿网步入云原生实践新阶段——多方向探索的体现之一。</p><p></p><p>在去年加入去哪儿网后，李佳龙主要负责 Service Mesh 的落地工作。团队基于去哪儿网的现状，明确了对新服务治理体系的期望：多语言、多协议统一治理、治理能力增强，同时可以将治理能力下沉，实现基础组件开发周期和业务开发周期解耦。</p><p></p><p></p><h2>如何做技术选型</h2><p></p><p></p><p>整个实践过程主要分为三个阶段：调研、开发和推广。调研阶段的主要任务是技术选型。去哪儿网团队用了两周左右的时间，调研了其他企业的落地情况。</p><p></p><p>自 2011 年引入 Dubbo 框架至今，去哪儿网的微服务架构已经有了十多年的历史。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9b/9bf57db4a99936c49b30d031c5d32f25.png\" /></p><p></p><p>如何找到适合自己的产品呢？首先，由于业界已经有成熟的产品，因此去哪儿网没有选择重复造轮子，而是基于业内已有开源软件进行选型。其次，去哪儿网团队还从产品是否成熟、社区活跃度、性能是否满足公司需求，以及开发和维护难度等方面进行了考察。</p><p></p><p>最终，去哪儿网选择了蚂蚁开源的 MOSN 作为数据面。MOSN 经过了阿里双十一几十万容器生产级别验证，产品自身已经成熟。另外，MOSN 社区比较活跃，迭代也较快，更新周期基本维持在了两个月左右。</p><p></p><p>语言也是重要的考虑因素。由于在 C++ 技术栈方面人才储备有限，团队放弃了流行的 Envoy。MOSN 的开发语言是 Go，在性能、开发、维护难度和灵活性上有优势，虽然不是去哪儿网使用的主要语言，但开发人员入门和上手都比较快。MOSN 既可以替代 Envoy，也可以集成 Envoy 在网络层高性能的优势，并将其作为自己的网络插件使用。</p><p></p><p>控制面上，去哪儿网选择了 Istio，并根据自身需要对其进行了二次开发。Istio 使用 XDS 协议与数据面进行交互，是很多企业的首选，去哪儿网选择紧跟社区的脚步。但原生的 Istio 有个问题，就是和 K8s 耦合非常严重：主要体现在 K8s 既是注册中心又是配置中心，以及 sidecar 注入、启动配置和网络模型等。这导致去哪儿网团队面临着以下问题：</p><p></p><p>K8s 存储是以应用为维度，但 Dubbo 是以 Service 为维度，维度不统一问题怎么解决？去哪儿网内部容器要和 KVM 共存，如何解决兼容问题?公司内部已有比较成熟的注册中心和配置中心，一刀切必然会引入很多适配和运维成本问题。如果遇到定制化需求，是否会影响 K8s 集群的稳定？</p><p></p><p>去哪儿网希望 K8s 更多的用于服务编排，因此在最初商定方案时，选择解耦 K8s，采用内部使用多年的注册中心和配置中心。于一些启动配置，则依赖去哪儿网内部的配置中心（QConfig），同时团队自研了 McpServer 模块替代 K8s 来对接 Istio。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/18/1810e4652b6f805e8aff74a9ad9bc3dd.png\" /></p><p></p><p>整体架构</p><p></p><p></p><h2>具体实践如何？</h2><p></p><p></p><p>接入 Mesh 后，业务主要关心的就是性能、治理能力和问题排查。因此在做好技术选型后，去哪儿网团队便开始设计实践方案，并花了数月的时间进行研发。</p><p></p><p></p><h3>流量管理</h3><p></p><p></p><p>目前，业界将流量从 SDK 拦截到 sidecar 的方式主要有两种：iptables 拦截和流量转发。但由于 iptables 的可运维性和可观察性较差，配置较多时还会出现性能下降的问题，因此很多公司选择了流量转发的方式，去哪儿网也是如此。</p><p></p><p>去哪儿网主要使用了两种流量转发方式：一是升级 SDK，在 SDK 中直接将请求转发到本机 sidecar；二是域名拦截，例如使用 dnsMasa 将 xx.qunar.com 的请求转发到本机 sidecar。</p><p></p><p>李佳龙提醒道，引入 Service Mesh 后，流量安全值得特别关注，因为之前两个服务的调用变成了四个服务间的调用，大大增加了不稳定性。对此，去哪儿网主要采取了四方面的措施来保证流量的安全：</p><p></p><p>SDK 自动调用降级。当业务进程与 sidecar 连接断开 (基于健康检查) 时，调用降级为直连调用。sidecar 节点故障剔除。sidecar 会对 upstream 的节点列表进行健康检查，一旦发现连接异常就快速剔除。用户可以在新的统一治理平台 Captain 上一键切入 / 切出 Mesh 模式。柔性可用。为保证可用性，当控制面出现问题时，sidecar 会使用缓存数据；当注册中心或配置中心出现异常，MCPServer 以及 Istio 也可以使用缓存数据。</p><p></p><p></p><h3>Sidecar 管理</h3><p></p><p></p><p>对于 sidecar 的配置、部署、升级、回滚和灰度等，去哪儿网团队将其整合到了运维面上。</p><p></p><p>以 sidecar 的升级为例，Sidecar 的升级可以分为两个场景：原地升级和服务部署时升级。前者不需要重新部署服务，但需要确保升级对用户透明、流量无损。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ef/ef4d992e6d64bec1e16ab805754ce4e7.png\" /></p><p></p><p>Sidecar 的注入和升级</p><p></p><p>对于原地升级，由于 sidecar 容器和业务容器是同属一个 pod 的两个独立容器，基础架构团队将 MosnAgent 作为 sidecar 容器的 1 号进程，用于管理 sidecar 的生命周期，例如启停、健康状态检查和升级等。新版本发布时，会将 sidecar 镜像推送到镜像仓库，同时将二进制包推送到对象存储。</p><p></p><p>基础架构团队通过 sidecar 管理平台，可以对 sidecar 容器发送升级指令，MosnAgent 会拉取对应版本的 mosn 包，之后启动新版本 mosn，并通过 fd 迁移机制，完成无损升级流程。</p><p></p><p>而部署时升级，发布平台会请求 sidecar 管理平台来获取 sidecar 配置，包括是否注入、sidecar 版本、环境变量、配额信息等，然后生成服务的 yaml 配置。注意在部署时，有可能遇到 sidecar 容器、业务容器的启用顺序问题。如果 sidecar 容器未启动成功或者配置未拉取成功、但业务容器已经 ready，那么请求就会失败。基础架构团队设置成 sidecar 先于业务容器启动，并通过配置 K8s 的 postStart 钩子函数，来保证正确的启动顺序。</p><p></p><p>需要注意的是，Istio 下发配置时，也可能发生顺序异常问题。去哪儿网团队通过 Istio merkeltree 来跟踪资源下发进度，通过暴露接口来查询每个资源是否全量下发到 sidecar。</p><p></p><p>另外，运维面还负责对接内部系统，如内部配置中心、全链路监控系统、报警系统、镜像仓库及对象存储等。</p><p></p><p></p><h3>性能优化</h3><p></p><p></p><p>在性能优化方面，首先由于业务容器与 sidecar 容器同属一个 pod，基础架构团队选择了使用 unix domain socket 进行通信，以此规避网络栈、优化性能。</p><p></p><p>其次，团队对 Dubbo 协议进行了优化。sidecar 接收到请求后的第一步是获取路由信息，用以服务寻址。按照 Dubbo 原来的设计，路由信息 (service/method/group 等) 存放在 body 中，但为了避免不必要的 body 反序列化，团队扩展了内部 Dubbo 协议，将路由信息放置到了扩展 header 中。</p><p></p><p>再者，团队选择按需加载 xDS 数据方式。Sidecar 启动时会请求控制面拉取 xds 数据，但是原生的 Istio 会拉取当前集群中所有服务数据，导致 sidecar 资源占用过多以及推送风暴，大大制约了 Service Mesh 的集群规模，这也是很多公司落地 Service Mesh 要解决的问题。去哪儿网的解决方式是配合内部 Spring Cloud Qunar 框架，在编译器采集订阅关系，并扩展 xds 协议，来实现按需加载。</p><p></p><p>除了按需加载配置，基础架构团队也在优化推送性能，统一服务注册模型等。</p><p></p><p></p><h3>治理能力</h3><p></p><p></p><p>针对之前治理功能分散、能力不统一的问题，基础架构团队将 Dubbo 和 HTTP 统一到了新的 Captain 平台。</p><p></p><p>“在设计时，Captain 要尽量做到服务治理与协议无关。”李佳龙说道。比如，之前 Dubbo 服务需要在项目中配置参数支持调权预热、HTTP 在发布平台配置支持引流预热或者自定义接口预热，引入 Service Mesh 后就可以很方便地支持 Dubbo/HTTP 调权、引流两种预热模式。</p><p></p><p>此外，Captain 增强了很多服务治理能力，例如多维度的限流策略，包括基于应用粒度限流和基于优先级限流。业务团队既可以根据应用的不同来设置不同的限流阈值，也可以根据不同的流量来源设置不同的优先级，优先处理有价值的请求。</p><p></p><p>如下图所示，当流量达到设定的阈值时会触发限流，这时流量会进入到多个优先级队列，可以按照优先级处理。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/50/50bbf6e1dea36c7366f388949b3f3f77.png\" /></p><p></p><p>此外很多情况下，业务会将超时时间、限流阈值设置得比较大或比较小，这未能起到应尽的作用，基础架构团队根据监控数据和合理算法，会智能推荐超时、限流等参数。</p><p></p><p></p><h2>如何进行推广</h2><p></p><p></p><p>目前，去哪儿网已经在基础平台、机票等部门开始推广和试行 Service Mesh。</p><p></p><p>“推广是最为关键、也是比较容易出现阻塞的地方。因为对于用户来说，让系统最稳定的做法就是不轻易做变动。”李佳龙说道。</p><p></p><p>那么，如何让业务更容易接受 Service Mesh 呢？去哪儿网团队先是在内部做了关于 Service &nbsp;Mesh 基本认识的分享，让业务了解接入 Mesh 后都能得到哪些收益。</p><p></p><p>然后对于新服务治理平台，基础架构团队也注意保留了很多业务的使用习惯。比如在实际使用中，超时时间等治理参数与路由的相关度较低，业务更习惯作为服务提供方配置某个接口的超时时间，或者作为服务调用方配置要调用服务的超时时间。但在 xDS 协议中，超时时间等治理参数配置在服务提供方，并且与路由绑定。因此，基础架构团队做了部分扩展，业务可以从服务提供方、调用方两个角度来进行配置，并以接口为维度，这些配置包括超时、重试、备份请求、负载均衡策略等。</p><p></p><p>便捷性对业务来说也是非常重要的，业务希望切到 Mesh 后能够方便地查询问题，日志、trace、监控报警等内部系统的打通也是推广的前提。</p><p></p><p>基础架构团队现在提供了两种 Mesh 接入方式：第一种是接入新的开发框架 Spring Cloud Qunar，业务需要做部分改动；第二种是无侵入地一键接入，业务不需要修改代码，这也是接受度较高的一种方式。</p><p></p><p>另外，去哪儿网提供了按照接口级别的接入方式，业务可以按照百分比来切入 Mesh，而不是一刀切地接入。基础架构团队的策略是让业务先从一个小的接口、小的流量来接入，业务接受了之后再扩大接入范围。</p><p></p><p>在李佳龙看来，便捷易用性问题一定程度上确实阻碍了 Service Mesh 的落地。</p><p></p><p>一方面，从业务角度看，使用者更加在意 Mesh 接入后是不是好用、操作是不是简单、切换起来是不是安全和方便、需求是不是能够满足、能不能保留之前的研发习惯等等。另一方面，从开发角度看，企业开发 Service Mesh 时通常有很多工作，比如进行二次开发并与内部系统打通，还有诸如 xDS 按需下发，sidecar 管理等目前没有统一标准的工作，这些都需要维护成本。公司基础设施之间有差异，这些都需要决策者考虑清楚。</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>Service Mesh 不是一个很新的概念，它的本质就是解耦，具体实现起来就是语言 SDK 负责编码以及与 sidecar 的通信，这部分比较固定，不包含治理功能。虽然还是多个实现，但是复杂度降低了一个量级，从而将更多的功能实现，抽离到独立的 sidecar 进程，只需要实现一次。“所以本质上，解决多语言多协议的复杂问题，是分离了变与不变，这也是去哪儿网进行软件架构时的常用方式。”李佳龙说道。</p><p></p><p>那么对于企业来说，是不是要容器化后才能够接入 Service Mesh 呢？李佳龙表示这不一定。</p><p></p><p>“只能说，Service Mesh 天然比较适合容器化的场景，比如 Istio 方案默认使用者已经具备 K8s 的基础设施了，但是我们仍然可以有多种方式支持虚机接入服务网格，只要解决掉 sidecar 注入、升级，流量拦截，服务注册发现等。而且我们设计之初去除了 K8s 的耦合，接入虚机也变得更方便。按照去哪儿的云原生的发展来看，容器化作为云原生架构的底座，在容器之上能更好的支持 Service Mesh、Serverless 等。”</p><p></p><p>就中小企业而言，李佳龙表示，中小企业首先考虑的更多是业务。业务越来越多、越来越复杂后，才可能会出现多语言、多框架的问题。只有确实出现这个问题时，才应该开始考虑是否引入 Service Mesh。此外还需要考虑自身的基础设施、团队技术储备等是否支持落地。Service Mesh 是利用低复杂的技术去解决高复杂度的问题，如果本身复杂度不高，引入 Service Mesh 这样的技术只会增加复杂度，得不偿失。</p><p></p><p>还有一个值得关注的方面就是性价比。引入 Service Mesh 的新增成本包括 sidecar 所占有的资源，比如每个 sidecar 占用的 CPU、内存、磁盘以及其他如 trace 存储、日志存储等存储资源。另外，像请求耗时增加、系统开发维护等会产生相应成本，因此去哪儿网目前对于各类日志是按需开启，只打印耗时高的 trace 信息。</p><p></p><p>”不过目前看，Service Mesh 确实带来了很多好处。”李佳龙说道，“首先便是统一了多语言、多框架的治理能力，提供丰富的治理功能。另外对业务开发透明，减少了业务的使用成本。与业务解耦后，我们的推广周期也大大缩短。”</p><p></p><p>“Service Mesh 应该会是未来的一个发展趋势。”李佳龙表示，“之后去哪儿网会在可观察性、性能优化、多语言支持上持续发力。”</p><p></p><p>嘉宾介绍</p><p></p><p>李佳龙，2021 年加入去哪儿网基础架构部，担任资深开发工程师，近五年专注于云原生、基础架构等领域，负责去哪儿内部 Service Mesh 和新服务治理平台 Captain 的研发落地等工作。</p><p></p><p></p>",
    "publish_time": "2022-10-08 10:37:39",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Unstructured Data Summit 2022《2022首届非结构化数据峰会》1",
    "url": "https://www.infoq.cn/article/De3kqyQGPuD0vwdfoWOc",
    "summary": "<p>2022 年 9 月 24-25 日，首届非结构化数据峰会（2022 Unstructured Data Summit）在线上举行。本次峰会由 Zilliz 主办，主题为「矩阵革命，向量连接世界」，峰会设置了一系列 Keynote 和分论坛演讲，围绕人工智能在非结构化搜索领域的顶尖技术、热门话题、前沿观察展开分享和探讨，共同探索行业发展的新风向。<br />\n本期视频为非结构化数据峰会 24 日上午场。</p>",
    "publish_time": "2022-10-08 14:47:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Unstructured Data Summit 2022《2022首届非结构化数据峰会》3",
    "url": "https://www.infoq.cn/article/JtGhroQY6HYwaK17CKdT",
    "summary": "<p>2022 年 9 月 24-25 日，首届非结构化数据峰会（2022 Unstructured Data Summit）在线上举行。本次峰会由 Zilliz 主办，主题为「矩阵革命，向量连接世界」，峰会设置了一系列 Keynote 和分论坛演讲，围绕人工智能在非结构化搜索领域的顶尖技术、热门话题、前沿观察展开分享和探讨，共同探索行业发展的新风向。<br />\n本期视频为非结构化数据峰会 25 日上午场。</p>",
    "publish_time": "2022-10-08 14:47:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Unstructured Data Summit 2022《2022首届非结构化数据峰会》4",
    "url": "https://www.infoq.cn/article/QDRmW3Mp1Q1WGyZ22rxt",
    "summary": "<p>2022 年 9 月 24-25 日，首届非结构化数据峰会（2022 Unstructured Data Summit）在线上举行。本次峰会由 Zilliz 主办，主题为「矩阵革命，向量连接世界」，峰会设置了一系列 Keynote 和分论坛演讲，围绕人工智能在非结构化搜索领域的顶尖技术、热门话题、前沿观察展开分享和探讨，共同探索行业发展的新风向。<br />\n本期视频为非结构化数据峰会 25 日下午场。</p>",
    "publish_time": "2022-10-08 14:47:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Unstructured Data Summit 2022《2022首届非结构化数据峰会》2",
    "url": "https://www.infoq.cn/article/hTJ3SXq5vS6mFa8UxTbP",
    "summary": "<p>2022 年 9 月 24-25 日，首届非结构化数据峰会（2022 Unstructured Data Summit）在线上举行。本次峰会由 Zilliz 主办，主题为「矩阵革命，向量连接世界」，峰会设置了一系列 Keynote 和分论坛演讲，围绕人工智能在非结构化搜索领域的顶尖技术、热门话题、前沿观察展开分享和探讨，共同探索行业发展的新风向。<br />\n本期视频为非结构化数据峰会 24 日下午场。</p>",
    "publish_time": "2022-10-08 14:47:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "深圳国际金融科技大赛「绿色金融科技分享」",
    "url": "https://www.infoq.cn/article/zoWTxiawoZwaHoSscI3Y",
    "summary": "<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/1f/22/1f34cba351e799a6dac2f29348ce2422.jpg\" /></p>",
    "publish_time": "2022-10-08 15:39:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "MySQL 分布式事务的“路”与“坑”",
    "url": "https://www.infoq.cn/article/yGfFSW7sPVRCIoKOkXiD",
    "summary": "<p></p><h2>一、数据库事务</h2><p></p><p></p><h4>1.1 普通本地事务</h4><p></p><p></p><p>分布式事务也是事务，事务的 ACID 基本特性依旧必须符合：</p><p>A：Atomic，原子性，事务内所有 SQL 作为原子工作单元执行，要么全部成功，要么全部失败；C：Consistent，一致性，事务完成后，所有数据的状态都是一致的。如事务内A给B转100，只要A减去了100，B账户则必定加上了100；I：Isolation，隔离性，如果有多个事务并发执行，每个事务作出的修改必须与其他事务隔离；D：Duration，持久性，即事务完成后，对数据库数据的修改被持久化存储。</p><p></p><p>普通的非分布式事务，在一个进程内部，基于锁依赖于快照读和当前读，比较好实现 ACID 来保证事务的可靠性。但分布式事务参与方通常在不同机器的不同实例上，原来的局部事务的锁不能保证分布式事务的 ACID 特性，需要引入新的事务框架，MySQL 的分布式事务是基于 2PC（二阶段提交）实现，下面详细介绍下 2pc 分布式事务。</p><p></p><h4>1.2 基于 2pc 的分布式事务</h4><p></p><p></p><p>分布式事务有多种实现方式，如2PC（二阶段提交）、3PC（三阶段提交）、TCC（补偿事务）等，MySQL 是基于 2PC 实现的分布式事务，下面介绍 2PC 分布式事务实现方式。</p><p></p><p></p><blockquote>两阶段提交：Two-Phase Commit , 简称 2PC，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法。&nbsp;</blockquote><p></p><p></p><p>2PC的算法思路可以概括为，参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报，决定各参与者是否要提交操作还是中止操作。这里的参与者可以理解为 Resource Manager (RM)，协调者可以理解为 Transaction Manager（TM)。</p><p></p><p>下图说明了RM和TM在分布式事务中的运作过程：</p><p>第一阶段提交：TM 会发送 Prepare 到所有RM询问是否可以提交操作，RM 接收到请求，实现自身事务提交前的准备工作并返回结果。&nbsp;第二阶段提交：根据RM返回的结果，所有RM都返回可以提交，则 TM 给 RM 发送 commit 的命令，每个 RM 实现自己的提交，同时释放锁和资源，然后 RM 反馈提交成功，TM 完成整个分布式事务；如果任何一个 RM 返回不能提交，则涉及分布式事务的所有 RM 都需要回滚。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/45c019bb573c9398a6c947b4af392789.png\" /></p><p></p><p></p><h2>二、MySQL 分布式事务 XA</h2><p></p><p></p><p>MySQL 分布式事务 XA 是基于上面的 2pc 框架实现，下面详细介绍 MySQL XA 相关内容。</p><p></p><h4>2.1 XA事务标准</h4><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52f92590b12aebfa8d6dd70bc9fde97d.png\" /></p><p></p><p></p><blockquote>X/Open 这个组织定义的一套分布式XA事务的标准，定义了规范和API接口，然后由厂商进行具体的实现。</blockquote><p></p><p></p><p>XA 规范中分布式事务由 AP、RM、TM 组成：</p><p></p><p>如上图，应用程序 AP 定义事务边界（定义事务开始和结束），并访问事务边界内的资源。资源管理器 RM 管理共享的资源，也就是数据库实例。事务管理器 TM 负责管理全局事务，分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚、失败恢复等。MySQL 实现了 XA 标准语法，提供了上面的 RMs 能力，可以让上层应用基于它快速支持分布式事务。</p><p></p><h4>2.2 MySQL XA语法</h4><p></p><p></p><p>XA START xid:开启一个分布式事务 xid。XA END xid: 将分布式事务 xid 置于 IDLE 状态，表示事务内的 SQL 操作完成。XA PREPARE xid: 事务 xid 本地提交，成功状态置于 PREPARED 失败则回滚。XA COMMIT xid:&nbsp; 事务最终提交，完成持久化。XA ROLLBACK xid: 事务回滚终止。XA RECOVER: 查看 MySQL 中存在的 PREPARED 状态的 XA 事务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/60f4326f8bfc5f62bd252e79f1859dce.png\" /></p><p></p><p></p><h5>（1）语法要点</h5><p></p><p></p><p>参与分布式事务的实例之间，在数据库内核视角没有直接关联，互相不感知状态，且一个分布式事务中各个节点上的子事务均可单独执行无依赖，他们之间的关联是通过全局事务号在应用层建立的。</p><p></p><p>与普通事务比，XA 事务开启时多了一个全局事务号，结束时多了一个 end 动作 和 prepare 动作。</p><p></p><p>XA START, 开启一个分布式事务，需要指定分布式事务号。XA END，在内部仅是一个状态变化，声明当前XA事务结束，不允许追加新的sql语句，无其它作用，业界有人提出XA事务框架去掉这一步，减少一次网络交互，提高性能。XA PREPARE，写 binlog 和 redo log，预提交事务，并将分布式事务信息保存到全局内存结构，让其它连接可以查询、回滚、提交，如果 prepare 失败则回滚。XA COMMIT，真正提交事务，修改事务状态，释放锁资源。如果实例上 XA PREPARE 已经成功，那么它的 XA COMMIT 一定能成功。</p><p></p><p>XA 事务示例：201 用户给 202 用户转账 1000 元，简化如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f5b72aff7edff971619bdd692776651a.png\" /></p><p></p><p>第 1 步，开启一个分布式事务，xa_ts:10001 是应用层定义的全局事务号，实例 1 和实例 2 通过它来构建分布式事务。</p><p></p><p>第 2、3 步是普通事务语句。</p><p></p><p>第 4 步，声名 xa 事务结束，在此之后不能再追加更新插入查询等语句，不属于这个分布式事务也不允许，其它语句放在 xa commit 或 xa rollback 之后。</p><p></p><p>第5步，prepare 成功后，上层应用可以发起第 6 步提交事务。注意，必须是所有参与这个分布式事务的全部节点均 prepare 成功，即实例 1 和实例 2 都完成 prepare，应用端才能发起提交，两阶段提交的框架核心点就在此。</p><p></p><p>如果有节点在前 5 步不能成功，所有参与分布式事务的节点都必须回滚。如实例 2 是账户加 1000 元，基本上什么情况都能成功，肯定能成功执行第 5 步，但实例 1 就未必了，账户要扣 1000 元，可能资金不够，会出错回滚，若实例 1 不能执行到 prepare，所有分布式事务参与者也必须回滚，所以实例 2 也要回滚。如果第 5 步全部成功，有一个节点执行了第 6 步提交了事务，那么所有节点必须要均提交，否则就会导致数据不一致。处于 xa prepare 不提交会占用资源，残留 xa 事务等价于存在长事务，对刷脏和 purge 等都有影响，业务层最好要立即提交。</p><p></p><p>（2）残留 XA 事务如何处理</p><p></p><p>上面说到 xa 事务不提交等价于长事务，一旦 prepare 成功要立即提交，否则会带来很多问题。但是数据库 crash 或应用系统出错 crash 等原因都可能导致 xa 事务未能全部提交，这些残存 XA 事务如何处理？这就要用到上面的 XA RECOVER 语法了，执行 xa recover 查看未提交 XA 事务，选择对应的进行 rollback 或 commit。如果仅 gtrid_length 字段有值一般可以直接 xa rollback/commit &nbsp;xid 方式回滚或提交，xid 就是 xa recover 中 data。</p><p></p><p>如果 gtrid_length 和 bqual_length 都有值，回滚或提交则相对复杂一些，需要以下面方式提交或回滚：</p><p><img src=\"https://static001.geekbang.org/infoq/58/58d358ada7a273e20a5e5b72c33a63e1.png\" /></p><p></p><p>gtrid 和 bqual 被拼接在 data 字段中，需要按他们长度切分，以下面未提交 xa 事务里第一个为例，gtrid_length 为 34，表示 data 中前 34 个字符为 gtrid, bqual_length 为22，表示 data 中后 22 个字符为 bqual，那么对对其回滚或提交方式可表示如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e96d15eced27cb6d2c00b7b2569dba62.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7c17a7542f9d6d2decbe8840df7dba0a.png\" /></p><p></p><p>如果 data 中有其它特殊字符，也可以转成 16 进制整数方式处理，执行语句如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18e64405f70ed664569e77d1c87b10cf.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/0699a8da5d924fde01d937e52efcb88c.png\" /></p><p></p><p>因为是 16 进制数，字符做了转换，data 中字符数会翻倍，回滚或提交内容要同步调整，将 data 中字符也要翻倍再拆分，如上 grtrid 长度 34，则 data 中前 34*2 个 16 进制数字是 gtrid，bqual 长度 22，则后 44 个 16 进制数字是 bqual，回滚或提交语法如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b4ee768b6c5f3fd64a0df664d2fbe9fb.png\" /></p><p></p><p>注意：上面的提交或回滚都可能报 xid 不存在，这不一定是 xid 写错了，也可能是开启这个 XA 事务的连接并未断开，其它连接不能处理这个 XA 事务，这里是 MySQL 报错不准确。</p><p></p><h5>（3）提交还是回滚的依据</h5><p></p><p></p><p>上面给出如何进行提交或回滚的方法，但是提交 or 回滚应该选择哪个？</p><p></p><p>残留 XA 事务是提交还是回滚，必须要由业务决定，谁开启 XA 事务，构建了分布事务管理器 TM，谁就必须为这个事务负责到底。</p><p></p><p>单个数据库视角无法判断出这个XA事务是应该提交还是应该回滚，不管选哪种都可能会导致全局数据出错，运维同学在处理时一定要与业务方确定好该事务是提交还是回滚，获得授权后再操作。以上面转账为例，201 用户给 202 转 1000 元，都 prepare 成功，发起 commit，此时 202 用户实例发生故障重启，未完成 commit，重启之后有残留 XA 事务，此时若 201 提交成功，那么 202 必须提交，如果 201 未成功，202 可以先 201 一起提交或一起回滚，由应用层事务管理器 TM 来决定。假如 201 提交成功，202 回滚则 201 扣了 1000，202 未收到，对账则钱少了。如 201 回滚了，202 提交，则 202 加了1000，201 未扣，对账则钱多了。</p><p></p><h4>2.3 MySQL XA 事务设计上的“坑”</h4><p></p><p></p><h5>（1）设计上的缺陷</h5><p></p><p></p><p>基于 binlog 的主从复制是<a href=\"https://xie.infoq.cn/article/7ae9b597e18981ae58c9db7dd\"> MySQL </a>\"高可用的基石，这也是<a href=\"https://xie.infoq.cn/article/ad357dffe11eeda5ebfa265ec\"> MySQL </a>\"能广泛流行使用的最重要因素。在 MySQL 内部，对于普通事务(非XA事务），innodb 等引擎和 binlog 为了保持数据的一致性，就是用的 2PC ，为了区分于 XA 事务的 2PC ，称之为内部两阶段提交。内部 2pc 使用 binlog 是作为协调者（TM），内部 prepare 时先写 redo 再写 binlog，都持久化（受刷盘参数策略影响）后再提交。当发生 Crash 重启时，会先恢复出所有 prepare 成功的事务，把里面的 xid 事务号取出来，再到协调者 Binlog 中去找，如果 binlog 中有这个 xid 则说明 innodb 和 binlog 都执行成功，等价于外部 xa 事务两个参与节点都 prepare 成功，则继续提交，如果 binlog 中找不到，刚说明只在引擎层完成，需要回滚，如果某个进行的事务 xid 在 prepare 中未找到，则说明 prepare 未完成，直接回滚，这个顺序一定是先写 Redo log，最后写 Binlog。</p><p></p><p>那么处于 XA prepare 状态的分布式事务到底是一个什么样的状态？分布式 XA 事务也是基于普通事务实现，实际上就是一个支持挂起，支持让其它会话继续提交或回滚，支持 crash 或重启之后还能恢复这种挂起状态的普通事务。</p><p></p><p>普通事务的 prepare 动作是发生在显式 commit 之后，先写 redo 后再写 binlog。XA 事务的 prepare 发生在显式 XA commit 之前，它需要生成 binlog，然后再写 redo，这与普通事务是相反的，这就导致这个外部 2pc 事务的内部 2pc 提交缺少了一个协调者，某些情况下会导致数据库不一致。</p><p></p><p>一个 XA 事务的 binlog 由两部分组成，从 xa start 到 xa prepare 是一个不可分原子语句块，xa commit 又是一个原子语句块，且分别有各自的 gtid，如下图 binlog：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8d6cc4269021af5c3d227678bb813471.png\" /></p><p></p><p>事务号为 &nbsp;X'7831',X'',1 的分布式事务 prepare 之后，中间插入了很多普通事务，然后再执行的 xa commit。</p><p></p><p>一个 XA 事务的 binlog 被切分成了两个独立的部分，如果在主节点在生成 XA prepare binlog 之后发生crash， 还没有在引擎层做 prepare，重启之后引擎层中因没有完成 prepare 动作而回滚。但在主从架构中，只要 binlog 正常产生就可能会同步到 Slave 机，这种情况下会导致 slave 机上多了这个 xa prepare 的中间状事务，最终复制出现问题。这个问题已经被发现多年，官方确认了 bug，一直未修复（<a href=\"https://bugs.mysql.com/bug.php?id=87560\">https://bugs.mysql.com/bug.php?id=87560</a>\"）。</p><p></p><h5>（2）遇到该问题处理思路</h5><p></p><p></p><p>虽然我们要尽量避免出现故障，但也做好面对任何故障的准备，谋而后动，有招不乱！</p><p></p><p>在常规连接中，MySQL 的 XA 事务执行 prepare 之后，通常不能执行其它非 xa 语句，会报错提醒当前正在 xa 事务中。但在复制的 sql 回放线程中，执行完 xa prepare 之后，可以直接执行其它非此 xa 事务的 sql，因为在 master 端生成的 XA 事务 Binlog 可能就是分开的，如上图例子就是。所以 slave 机 sql线程执行完 xa prepare 的 binlog 后，是被允许接着正常执行其它事务的 binlog 的。如果 xa preapre 过程 master 上发生 crash，刚好生成了 binlog，但没有做完后续的 prepare 动作，备机收到了这个 xa  preare 动作的 binlog，master 重启后会回滚掉这个事务，不会再生成这个 xa 事务后续 binlog，这会导致备机执行完 xa prepare 后一直挂起，占用的锁等资源不会释放，直到新同步过来的 binlog 与之冲突报错，才会暴露问题。</p><p></p><p>要修复分两种情况处理：</p><p></p><p>情况1：基于 gtid 的复制，应该直接会报 gtid 重复错误（推测，本地没能复现）。master上重启应该会回滚掉了前半个 XA 事务，后面事务会重新生成这个相同 gtid 的事务，导致复制出错，此时停止复制，将备机上这半个 XA 事务回滚，并 reset gtid 到之前的 gtid，重建复制即可。注意这里可能有多个 XA 事务在 Binlog 中处于 prepare 状态，需要解析 binlog 仔细确定要回滚的事务是哪个。</p><p></p><p>情况2：未开 gtid 的复制，此时比上面情况要麻烦，没有 gtid 来确定 binlog 事务是否重复，只要后面事务不涉及到这半个 xa 事务锁定的资源，备机就可以正常维持复制体系，一直同步数据，等到有冲突数据出现错误，回放线程重试超过一定次数后（slave_transaction_retries 重试参数控制），sql 线程报出相应错误，复制中断后才能被感知。恢复数据和上面差不多，回滚这个 XA 事务，重建主从，但是这个事务的 binlog 不一定能找到，因为没有 gtid 不会立即报错，可能几分钟后报错，也可能几个月后报错，取决于业务什么时候产生冲突数据。并且在这个事务之后，从机又同步了很多数据，这些数据是否可靠需要评估。线上强烈建议开启 Gtid 复制模式，非 gtid 的复制官方已经在淘汰！</p><p></p><p></p><h2>三、分布式事务的一致性</h2><p></p><p></p><p>使用到分布式事务，就必须要保证分布式事务的一致性。</p><p></p><p>分布式事务的一致性又分写一致性和读一致性，写一致性 XA 框架 XA prepare 和XA commit已经解决，只要保证有提交全提交，有回滚全回滚就能保证写一致性。</p><p></p><p>读一致性则要复杂的多，先看看 MySQL 官方对 XA 事务在读一致性上的“只言片语”：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d485aba33f773b7794c44ffadc92f27.png\" /></p><p></p><p>上面内容是从官方说明文档里截取，里面对 XA 读一致性略有介绍：如果应用程序对读敏感，首选 SERIALIZABLE 隔离级别，RR 级别不足以用于分布式事务，官方没有对这里的不足做具体说明，但我们可以构建一个例子来分析这个“may not be sufficien”来描述读一致性是否恰当。</p><p></p><p>如下图，有 A、B 两个账户在两个实例上，假设每个账户初始都 100 块，A 给 B 转账 20，时间线左边为 A 账户实例上的操作，右边为 B 账户实例上的操作，中间 T1 到 T6 为不同时间点。</p><p><img src=\"https://static001.geekbang.org/infoq/09/09a1a0034389a864f59c13de564b1b82.png\" /></p><p></p><p>T1 时刻：初始均 100。T2 时刻：AB账户均完成 xa prepare 操作，一个减 20，一个加 20。T3 时刻：A帐户节点 XA commit 成功。T5 时刻：B帐户 XA commit 成功。</p><p></p><p>当处在 RR 或 RC 隔离级别时，发起一个对账操作，统计 AB 帐户资金总额，当只有他们相互转账时，总金额应该恒为 200。T6 时刻时，查询 A 为 80，B 为 120，总账为 200，无问题。T4 时刻查询 A 账户为80，查询 B 账户时由于 MVCC 机制，会读到上个快照中的值 100，加一起为 180，总账不对。因为是操作不同实例，当开始做 xa commit 之后，可能由于网络等原因，并不能保证所有节点的 XA commit 同时到达所有节点，在一个高并发场景，导致上面的问题几乎是必然的。因此，当使用 <a href=\"https://xie.infoq.cn/article/ff7b6300bcbdf50d05b48f0e3\">MySQL </a>\" 原生 XA 分布式事务时，若无其它手段来保障读一致性，而应用又有跨节点读的应用场景，应当使用序列化（SERIALIZABLE）隔离级别，“may not be sufficien”显然是不恰当的，没有任何一个业务能接受这种数据统计不对的。</p><p></p><p>如果是序列化隔离级别，T4 时刻读到 A 为 80，读 B 时会等待，直到 T5 时刻 XA commit 成功之后， 才能读到 B 为 120，总账 200，无问题。序列化隔离级别只有读-读不阻塞，读-写，写-读，写-写均会阻塞，而 RC、RR 仅写-写阻塞，因此只有序列化隔离级才能充分保障 MySQL XA 事务的读一致性。但它阻塞太多，性能也是各种隔离级别中最差的，所以如无必要，通常不会使用这一隔离级别。业界有很多方案来解决分布式事务 RR、RC 下的读一致性问题，以提高数据库性能，但原生的 MySQL 不具备这种能力，因此使用 MySQL 原生 XA 事务的业务需要谨慎选择隔离级别。</p><p></p><h2>四、小结</h2><p></p><p></p><p>只要我们小心面对残留 XA 事务，谨慎处理 Crash 之后的可能存在的多余 binlog 数据，认真评估使用 RR、RC 隔离级别是否有读一致性读问题等问题之后，MySQL 的 XA 事务基本没有其它问题，可以作为 RM 完备提供跨节点分布式事务能力，MySQL 已经实现了 X/Open 组织定义的分布式事务处理规范中的语法功能，完全可以放心放业务在这条路上奔跑！</p><p></p><p></p><p>本文作者简介：</p><p>Flyfox，高级后端工程师。从事数据库内核工作十多年，深度参与多个基于 PostgreSQL、MySQL 自研数据库项目，目前负责 RDS 产品研发团队工作。</p>",
    "publish_time": "2022-10-08 16:06:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OPPO 自研云原生分布式任务调度平台",
    "url": "https://www.infoq.cn/article/HHPHzeKSlWI5vW8U3kpD",
    "summary": "<p></p><h2>一、概述</h2><p></p><p></p><p>在软件开发过程中，经常会遇到需要执行定时任务的场景。目前业界执行定时任务的分布式任务调度平台主要有 XXL-Job 和 Elastic-Job，两者都属于轻量级的调度平台，能满足一定任务数量的作业同时调度，但是如果任务调度量增加到 1 万 TPS 甚至 10 万 TPS，就会遭遇性能瓶颈，出现很多超时任务。</p><p></p><p>在 <a href=\"https://xie.infoq.cn/article/38bcdc0a81d64e8b584db2649\">OPPO </a>\"内部，有些业务部门存在海量作业同时调度的场景，目前业界的任务调度框架难以满足业务需求，所以 <a href=\"https://xie.infoq.cn/article/e633a9d506f87438df71c9abe\">OPPO</a>\" 公司的中间件团队自主研发了一个分布式任务调度平台 CloudJob，它是一个高性能(百万级 TPS)、低延迟(毫秒级)、统一、稳定、精准并满足复杂多样定时任务场景的调度平台。</p><p></p><p>它的特点如下：</p><p>（1）简单：用户可以通过页面对任务进行 CRUD 操作，也可以通过提供的 SDK 对任务进行管理，方便快捷。</p><p>（2）动态：支持动态修改任务状态、启动/停止任务，以及终止运行中任务，即时生效。</p><p>（3）一致性：“调度中心”通过分布式锁保证集群分布式调度的一致性, 一次任务调度只会触发一次执行。</p><p>（4）高性能、低延迟：支持百万级任务同时调度执行，且延迟在毫秒级。</p><p></p><h4>1.1 和开源产品对比</h4><p></p><p></p><p>CloudJob 设计的初衷是为了支持海量任务同时调度，它和其它任务调度平台的对比如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/79/02/799ab6636bb46e7cb057149bb1166f02.jpg\" /></p><p></p><h4>1.2 CloudJob 的性能</h4><p></p><p></p><p>在公司内部对 CloudJob 进行了多轮性能测试，通过对测试数据进行分析，CloudJob 的性能如下：</p><p>（1）低时延：CloudJob 在处理 TPS 为 50W 的作业调度时，99.11% 的作业调度延时在 1 秒以内； CloudJob 处理上亿次调度，最大调度延时不超过 2.4 秒。</p><p>（2）高性能：对比 Elastic-Job 的单个执行器执行上千个任务就会出现大量延时，CloudJob 的单个执行器处理上万个任务仍然可以保证毫米级调度任务而不超时。</p><p>（3）扩展能力强：性能测试场景 TPS 由 10 万增加到 50 万，系统只需要按比例增加执行器个数，依然可以保证作业正常调度而不出现严重超时。</p><p>（4）高可用：测试过程中将某个执行器宕机，该执行器的作业可以转移到其它设备调度，并且调度延时最大不超过3秒。</p><p></p><p></p><h2>二、系统架构</h2><p></p><p></p><p>CloudJob 的总统架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfd0d628ff4eb7c849010b46d35238ac.png\" /></p><p></p><h4>2.1 名称解释</h4><p></p><p></p><p>作业元数据：指作业执行的时间规则以及业务执行需要透传的参数，存在于 mongodb 数据库及缓存中。</p><p>作业触发消息：指作业按照时间规则计算出的产生触发执行动作的每一条记录，包含作业主键和执行绝对时间时间戳，比如每 5s 执行一次任务，第 5s 和第 10s 是两个触发器。</p><p></p><p>执行器：用于扫描符合条件的作业触发器的服务所在的容器。</p><p></p><p>分片：为了让系统可以横向扩展，需要将作业划分到不同的分组，每个分组就是一个分片，同时每个执行器设置一个分片属性（和作业的分片属性相对应），执行器只处理自己所在分片对应的作业。</p><p></p><p>定时任务执行周期：运行在执行器的定时任务每隔多久运行一次。本方案中该值的设置主要和触发器存储选型有关，应该设置合适的频率，避免过于频繁导致写入和存储触发器时延时较大，同时也不能因为过大，导致一些本应该执行的任务不能被及时获取，出现触发延迟太多。</p><p></p><p>时间窗口：定时任务扫描触发器的时间条件，选出将来一定时间范围内的数据。注意这个窗口最好不一定等于定时任务的执行周期。</p><p></p><h4>2.2 服务模块组成</h4><p></p><p></p><p>作业管理服务：负责作业增删改查，用户可以通过作业管理服务将作业注册到平台，平台将作业持久化到 mongodb 数据库并在 redis 中缓存。</p><p></p><p>执行器负载监控服务：在 CloudJob 平台中每个执行器会处理一个数据分片，执行器负载监控服务会将作业划分到不同分片，分片内作业数量将维持在合理的数量范围，保证作业按照时间规则发送而不延迟。该服务负责分片的作业容量管理以及分片扩缩容等功能。</p><p></p><p>触发器存储：支持可插拔的存储，提供高可用方案，保证数据零丢失。</p><p></p><p>触发器定时任务：执行器定时执行的操作，主要是扫描 mongodb 数据库，生成作业<a href=\"https://xie.infoq.cn/article/f85953b504c93f322b6d8a564\">触发</a>\"消息。</p><p></p><p>执行记录存储：记录发送到业务 MQ 的消息，核对是否有漏发送、发送是否有延迟等，发现系统可能存在的问题并及时对整个系统完善优化。</p><p></p><p>执行记录可视化：通过页面查看、查询作业的历史记录，查看作业是否超时，是否由漏执行。</p><p></p><p>通过对上面几个服务模块的说明，可以看出作业在系统中的流转过程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0987346e06617510490a201e9a389f6.png\" /></p><p></p><p>用户先通过作业管理服务将作业注册到 mongodb 数据库中，并通过 redis 来缓存作业。执行器负载监控服务对新添加的作业设置分片，获取当前系统未饱和的最小分片，将这个分片的 id 设置为这个作业的分片，同时将分片对应的作业数量加 1。多个执行器会根据设置好的分片参数定时从 mongodb 数据库中扫描出符合条件的作业，然后根据作业的时间表达式生成作业触发消息，然后将触发消息写入到时间轮中，最后在作业达到执行时间时将作业的基本信息投送到消息队列中，让用户从消息队列取出消息并执行自己的业务逻辑，从而达到触发作业调度的目的。</p><p></p><p></p><h2>三、数据流转举例</h2><p></p><p></p><p>定时任务按照执行次数可以分为固定周期类型和固定延迟类型。固定周期类型是指作业按照一定的周期每隔一段时间执行，固定延迟类型是指会在延迟一段时间后，执行一次，随后就不会再执行。下面举例说明这两种类型的任务是如何流转的。</p><p></p><h4>3.1 固定周期类型</h4><p></p><p></p><p>用户创建了一个固定周期类型的任务，每隔 5s 执行一次，携带的参数为：</p><p></p><p></p><p>作业管理服务先投递到 MQ 普通消息。消费者持久化该条数据，获得主键 jobpk1，计算出来该定时任务后续的触发时间戳为 1612493460，存储到触发器的存储中，必要字段为：</p><p></p><p></p><p>执行器上的定时任务在做扫描时，扫描到了该条触发器数据，判断是否到了预期投递时间，如果已经到了直接投递到业务 MQ，否则将它压入内存时间轮中，时间轮中到了预期投递时间，再投递到业务 MQ。再计算下次触发时间戳为 1612493465，将 redis 中的数据修改为：</p><p></p><p></p><p>执行器上的本轮定时任务处理完毕后 1，处理下一轮：拉取到 1612493465 这一次时间戳，随后重复上面的逻辑，或者发送 MQ 消息或者压入时间轮，依次往下。</p><p></p><h4>3.2 固定延迟类型</h4><p></p><p></p><p>用户创建了一个固定延迟类型的任务，再 15s 钟之后执行，携带的参数如下：</p><p></p><p></p><p>作业管理服务先投递到 MQ 普通消息。消费者持久化该条数据，获得主键 jobpk1，计算出来该定时任务后续的触发时间戳为 1612493475，存储到触发器的存储中，必要字段为：</p><p></p><p></p><p>执行器上由 cloudjob 调度的定时任务在做扫描时，扫描到了该条触发器数据，判断是否到了预期投递时间，如果已经到了直接投递到业务 MQ，否则将它压入内存时间轮中，时间轮中到了预期投递时间，再投递到业务 MQ。由于是固定延迟，没有下次执行，将 redis 中的数据修改为：</p><p></p><p></p><p>执行器上的本轮定时任务处理完毕后，处理下一轮：拉取的时间戳要大于 0，则这个作业以后不会被扫描到。在异步记录任务时，会将该 redis 中为 0 的这个 member 删除，并把元数据该作业的状态设置为完结。</p><p></p><h2>四、服务部署及实施流程</h2><p></p><p></p><p>通过前面的介绍大家知道了 CloudJob 的工作原理，下面通过几个模块的部署来说明一下具体的实施过程。</p><p></p><h4>4.1 作业初始化</h4><p></p><p></p><p>业务的作业通过作业管理服务接口批量注册。作业管理接收到请求后发送到 MQ 普通消息，由消费者完成如下步骤：</p><p></p><p>由于作业总数百万级别，需要将作业划分到不同分片上，每一个作业在注册进来时需要获取到尚未饱和的分片。分片的数量是由执行器负载服务管理的。假设一个分片的负载容量为 1 万，在分片承载的作业没达到 1 万之前，作业都可以被分到这个分片上。如果达到这个阈值，分片被设置为饱和状态，需要分配新的分片，新的作业将被分配到新的分片上。具体过程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e7a91f7ada1697354817bc1880cc4eba.png\" /></p><p></p><p>假设新增一个 5s 执行一次的作业时，获取到了 sharding1 分片，将会在 DB 中存储元信息同时缓存元信息。数据为：</p><p></p><p></p><p>Version0 表示该作业元信息是首次存入，以后每修改一次这个 version 递增。计算下次触发时间并在触发器缓存中存入如下数据：zset 名称 sharding1，member 为 jobpk1_0，是作业主键和 version 的组合，可以采用高位存主键，地位存版本号的方式。score 为下次触发时间戳 1612493460。</p><p></p><h4>4.2 执行器高可用</h4><p></p><p></p><p>执行器会定时扫描 mongodb 数据库，从数据库中获取符合条件的任务，这个定时任务是由 Elastic-Job 来分配执行的，Elastic-Job 将分片分配到了各个执行器中，假设是下面这种分配模型：两个执行器均分了两个分片，执行器 1 只处理具有分片属性 sharding1 的触发器，执行器 2 同理。当执行器 1 出现宕机时，Elastic-Job 将会触发失效转移，分片 1 将会分配给执行器 2，此时执行器 2 会有两个线程分别处理分片 1 和分片 2。如果执行器后面启动成功，Elastic-Job 将会重新分片，两个执行器又会均分分片。</p><p><img src=\"https://static001.geekbang.org/infoq/56/569ea2667af8cf37ee676678be0ddb9f.png\" /></p><p></p><h4>4.3 执行器线程模型</h4><p></p><p></p><p>执行器在运行时内部有两种线程，一个是定时任务扫描线程，另一个是消息队列消费线程，它们的工作模型如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1daa8688aa54e1a058bb9055216450db.png\" /></p><p></p><p>定时任务扫描线程：主要负责定时拉取触发器，随后均衡投递到对应时间轮中，一个时间轮由一个工作线程负责处理。如果出现工作线程处理时间轮中作业比较慢，出现大量堆积的情况，需要将对应分片属性设置为饱和状态，此时不会有新的作业被分配到该执行器，直到该分片重新恢复为非饱和状态。</p><p></p><p>消息队列消费线程：主要负责处理定时任务无法 cover 到的马上要执行的触发消息，比如定时任务的处理周期是 20s，现在用户提交了一个每隔10s 执行一次的任务，这个时候系统就需要生成一个 10s 之后执行的触发消息并把它写入到消息队列中，执行器的消费线程就可以立即得到这个触发消息，及时加入到执行器的时间轮中，确保消息能够按时执行。</p><p></p><h2>五、CloudJob 使用实践</h2><p></p><p></p><p>在 CloudJob 分布式任务调度平台搭建好以后，用户就可以将任务部署到这个平台上。下面介绍一下用户在使用 CloudJob 过程中遇到的问题和优化方案。</p><p></p><h4>5.1 集群隔离</h4><p></p><p></p><p>相比与其它分布式任务调度框架，CloudJob 是一个“重量级”的调度平台，搭建一套 CloudJob 需要 MongoDB 数据库、redis 集群，消息队列以及多台主机作为执行器，如果用户的任务量非常少，这将会导致资源的浪费。所以可以采用集群隔离的方法，将各个部门的用户作业部署到一个 CloudJob 集群中，同时采用一种隔离方式让用户的作业互不影响，这样就可以合理的利用资源。</p><p></p><p>集群隔离的具体思路是先搭建一套物理集群，用户在创建作业之前需要先创建一个逻辑集群（或者选择之前已经创建好的逻辑集群），在这个逻辑集群设置限流策略、超时机制，并与物理集群关联起来，用户之后将自己的任务注册到这个逻辑集群中，任务就可以被物理集群调度，任务的限流策略、超时机制等又是以逻辑集群为单位管理的，这样就实现了集群隔离。</p><p></p><h4>5.2 链路追踪与作业历史可视化</h4><p></p><p></p><p>在 CloudJob 集群中，一个任务从注册到最终执行需要经过多个处理流程，为了便于排查问题和进行作业历史统计，平台需要对任务流转的各个阶段进行链路追踪，同时需要将监控数据进行持久化，便于查询作业的历史记录。链路追踪与作业历史可视化的框架如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3c/3c54dd2bba8453f8535482a87eedaef4.png\" /></p><p></p><p>链路追踪的方案是通过埋点的方式，对系统中的主要处理流程进行记录，系统中的模块每进行一次处理就生成一条记录数据，记录数据通过消息队列定时发送到任务监控平台，任务监控平台将这些数据处理后存储到 Elasticsearch 中，为后续的统计、查询做准备。</p><p></p><p>同时平台提供了前端页面给用户进行作业历史可视化，用户可以在页面上查看作业的历史记录，查看每一次执行的调度情况，通过页面还可以查看到作业是否有漏发、严重超时。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d153aadd7b1644965516789139e15a3e.png\" /></p><p></p><p></p><h2>六、 总结与展望</h2><p></p><p></p><p>CloudJob 作为一个高性能、低延迟的分布式任务调度平台，通过将任务划分到不同的分片、每个执行器处理对应分片的任务，实现了系统的动态扩展和拥有海量任务调度的能力；使用定时任务扫描、时间轮、系统内部的消息队列来保证任务及时触发，保证了任务执行的低延迟；同时 CloudJob 通过 Elastic-Job 来执行系统内部的定时任务，保证执行器的高可用。</p><p></p><p>当然 CloudJob 还是有些不足，如果用户将任务部署到 CloudJob 平台上，还需要将自己的业务处理代码运行到自己的主机上，这会造成主机资源的浪费。后续 CloudJob 的演进方向就是将任务平台接入到 Serverless 平台，用户只需要在页面编辑自己的业务代码，然后点击保存，平台就会在 Serverless 中新建任务实例，将用户的代码运行在任务实例中等待接收触发消息，执行完任务后自动释放任务实例。这样既可以方便用户快速部署任务，又可以充分利用资源。</p><p></p><p></p><p>本文作者简介：</p><p>Xinchun，OPPO高级后端工程师。目前负责分布式作业调度系统的开发，关注消息队列、redis数据库、ElasticSearch等中间件技术。</p>",
    "publish_time": "2022-10-08 16:51:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIoT热度持续，企业怎么做到既不盲从又不掉队？",
    "url": "https://www.infoq.cn/article/Dx3Jov25cFcUKvPL0sjv",
    "summary": "<p>AIoT是近两年来的一个热门关键词。</p><p></p><p>看两组数据：根据市场调查公司MarketsAndMarkets统计报告，AIoT自2017年首次提出，到2019年全球市场规模达到了51亿美元，以此预估，到2024年还将增长至162亿美元；麦肯锡则更为大胆，据其预计，到2025年，全球AIoT市场规模将达到11.2万亿美元。</p><p></p><p>这样的抢眼数据自然引来大量玩家加入AIoT产业链赛道，除了原来在物联网领域的企业之外，博世作为传统工业巨头代表，在今年也宣布把AIoT作为2025年发展战略——目标是到2025年，让博世的每款产品都带有AI功能，或者在开发和生产过程中运用AI技术。</p><p></p><p>博世此举让更多的传统制造企业看到并开始关注AIoT的概念及其背后的价值。通过AI与IoT的结合，AIoT 追求的是“万物智联”，对于工业领域来说，这意味着更多可能性，比如提升生产效率、改善良品率、加快产线部署、实现柔性生产等等。换句话说，在工业领域，AIoT是一件雪中送炭而不是锦上添花的事情，所以，相较于消费领域，AIoT反而在工业领域受到越来越多的关注。</p><p></p><p>但是，热闹之余也有企业提出AIoT离自己还非常遥远，基础信息化、工业自动化、工业物联网才是眼下的核心任务。其实这背后投射出的，仍然是我国工业企业发展阶段的差异化。</p><p></p><p>那么，对于这样一个被市场认为是未来必然趋势的理念，不同发展程度的企业，如何做到既不盲从又不掉队、适当加码又不浪费资源、补短板的同时加速赶超？围绕这些问题，InfoQ采访了清华大学智能制造顾问/原北重汽轮CIO欧阳亮老师。</p><p></p><p>以下内容根据 InfoQ 与欧阳亮老师的对话编辑整理，以飨读者：</p><p>&nbsp;</p><p></p><h3>AIoT投产比受生产规模影响，不是每个企业每个场景都合适</h3><p></p><p></p><p>InfoQ：您怎么理解AIoT这个概念？</p><p></p><p>欧阳亮：AI和IoT过去实际上是两个独立的概念，但现在，从技术角度来说，它们正在逐渐融合。举例来说，在原来一些IoT的应用场景，基于数据采集、数据传输、数据分析，加上一些智能化的模块，大体上就可以理解为是AIoT。但是，它的具体应用效果需要根据业务需求，经过一步步迭代才能逐步改善。</p><p></p><p>InfoQ：具体来说，AIoT在工业领域有哪些主要的应用场景？</p><p></p><p>欧阳亮：我现在接触的企业基本上是点状的应用比较多。</p><p></p><p>比如某发动机制造商，他们正在尝试把一些智能化功能嵌入到新的柴油机中，从而采集柴油机的一些技术参数，实现预测性维护这样的目的。</p><p></p><p>另一个比较有意思的案例，是国内的某茶厂，他们想把制茶的过程变得更加标准化和智能化。一般来说，制茶工艺需要经过采摘、晾青、炒青、揉捻、闷堆、发酵、干燥等环节，以前茶产业的标准化比较低，这些环节基本上是依赖老师傅的经验去完成。比如，揉捻的过程需要根据环境气温等气候条件，判断揉茶机滚动的次数、频率等等，发酵的程度需要靠手摸、鼻闻甚至嘴尝。</p><p></p><p>虽然在这之前，这个茶厂通过固定标准做了一些程序化的改进，比如，什么气温条件下要杀青多长时间、揉捻多长时间、发酵多长时间等等。但是，由于制茶的品质受到环境、气候等条件的影响特别大，粗略的程序化很难达到高品质的制茶标准。这时候，就需要引入传感器、数据分析、人工智能等技术。</p><p></p><p>举例来说，通过在滚筒杀青机里装入传感器，一方面可以测滚筒里的温度、湿度，另一方面还可以通过转动的离心力对茶叶称重，从而判断它的失水情况，确定杀青的程度。</p><p></p><p>另外，AI质量检测也是典型的场景，包括检测产品本身，也包括有些企业会通过摄像头对做流水线上工人的操作进行智能识别、分析、诊断，如果工人的动作不够规范，可能质量就有偏差。</p><p></p><p>除此之外，能源管理是AI应用相对做得比较多、比较好的。因为能源管理涉及的是长时间的运行，效益主要是体现在能耗的基础上，传感器的布置位置也比较标准，整体下来，它的推广和落地会比较顺畅。</p><p></p><p>InfoQ：这些场景有没有什么共同的特点？对于传统企业来说，是不是每个场景都适合智能化，会不会有些情况下反而人工效率更高？</p><p></p><p>欧阳亮：场景合不合适有很多问题要考虑。</p><p></p><p>首先，无论是机械化还是智能化，在考虑是否用机器去替代人工的问题时，一定要搞清楚你的目的是什么。还拿茶产业举例——比如说采茶，人工可以分清不同的牙瓣，根据需要采摘成不同的等级。但是，机械是一刀切，很难分辨出来。如果茶厂想做普通的口粮茶，那可能无所谓，但如果要做精细的高端茶，那肯定是有差异的。</p><p></p><p>其次，成本的考虑也是必不可少的。AIoT的投入产出比，很多情况下会和企业的生产规模挂钩。举例来说，我最近接触了一个发电机的制造企业，他们在智能化的过程中就遇到了一个问题——发电机并不是规模化生产，一般是单点小批生产，一年可能就做几台，这时候，它要做智能化就没有规模效益——换句话说，它的成本均摊下来就会很高。</p><p></p><p>而且，由于机器数量有限，其中能采集到的数据量也很少。比如，在AI典型的应用场景——智能维护，如果采集数据不够，那AI就很难判断机器出现某一个问题是个例还是普遍问题，所以它对设备的维护指导以及设备性能的提升效果都不明显。</p><p></p><p>也就是说，企业在找智能化场景的时候，还要从自己的现实情况出发，要做匹配自己企业体量和生产规模的投入。</p><p></p><h3>AIoT落地最难的是想法的量化、模型调优和设备改造</h3><p></p><p></p><p>InfoQ：您觉得AIoT的落地应用，最难的部分是什么？</p><p></p><p>欧阳亮：最难的是人的想法如何去量化。因为每个人的感受和经验是比较主观的，比如，牛排要几成熟，每个人的标准不一样；再比如，中医诊断讲究望闻问切，那么望闻问切的结果怎么量化，每个医生的标准也不一样。在这个过程中，算法模型的构建就会变成很大的问题。</p><p></p><p>如果具体到AIoT这个领域，还会涉及怎么采集数据，采集来的数据怎么去归类，以及算法模型的参数怎么按照工业机理去调整等等。</p><p></p><p>以工业场景的供应链采购为例：订货量多少合适，不同阶段的订货量怎么拆分，面向不同供应商的订货量如何分配，报价均衡点如何把握，决定这些信息的主要是企业的排产，基于排产计划，还可能需要预留一定的供应链响应时间。而这整个过程会有很多人为因素的影响，不见得每一个相关数据都能被采集、记录并且输入到模型里头去。这就会非常影响算法模型的准确性和可参考性。</p><p></p><p>此外，在AIoT场景下，企业需要采集比以往还要更多、更细的数据，这时候会出现数据堆叠。比方说，我们要采集电梯的运行数据，包括电流、电压、平稳度等等，才能及时对电梯做保养、维修，预防突发故障。但其实很多电梯原本自己就有一套传感器，这些数据本来就是存在的，只不过它可能是封闭不对客户开放的，或者它是老的传感器，精度达不到相应的标准，这时候就不得不再装一套传感器。</p><p></p><p>对工业企业来说，也是相似的，传统制造产线上有大量的老旧设备，从设备的采购、运行到报废不是两三年的时间，可能是十几年甚至几十年，在引入新技术的时候就必须做改造。</p><p></p><p>InfoQ：如果涉及大量的设备改造，是不是意味着它的经济性也没有那么好？</p><p></p><p>欧阳亮：主要矛盾是这笔钱从哪块出。对于企业来说，投入一笔钱就必须有出处，换句话说，就是从哪个角度去立项。</p><p></p><p>举例来说，如果从设备的维修维护角度立项就会比较难，因为大部分企业每年在维修维护方面的投入并不是特别多；而如果从研发的角度去立项，那就需要管理层领导自上而下有强烈的研发诉求，通常来说，需要这个东西有广泛的应用前景和市场机会，企业才愿意去做研发的立项。</p><p></p><p>对于AIoT而言，它背后会涉及很多技术，包括物联网、算法模型、大数据等等，所以，它是很大的一个投入，而且算法研究完了以后，还要用到实践中，反复地迭代，这个过程也是漫长的，很难在短期内看到成效。</p><p></p><p>但国内的大部分企业还是追求短期效益，这和AIoT的落地周期又是相背的。拿模型来说，它一定是数据积累越多精准度越高，但是这一方面需要时间的积累，另一方面还需要覆盖面的积累。有时候，我们的企业缺乏这种耐心。</p><p></p><p>所以，是不是要做设备改造或者落地AIoT更多技术，包括背后的经济性，需要企业纳入到战略层面，自上而下、由内而外去做全面的考虑，它需要管理层有决心也有耐心。</p><p></p><p>InfoQ：那对企业来说，怎么找到这种长期投入和短期效益之间的平衡？</p><p></p><p>欧阳亮：首先，大家需要慢慢从根本上改变这种意识。其次，可以先做小范围的投入和试错，慢慢再推广和迭代。比如刚开始，除了算法以外其他投入不会太高，拿传感器来说，市场上现在大部分是自带蓝牙、存储、智能化等功能的，只要把它嵌入到指定设备，能够输出数据就行；再比如算法模型的投入，刚开始可以做得粗一点、简单点，后面再逐渐细化和优化。</p><p></p><p>而且，现在有一些技术手段是可以帮助企业缩短这个试错周期的，比如说数字孪生等等，它可以让企业迭代的动作更快一点。但话说回来，还有很多基础工作还是绕不开的，是企业必须要去做的。</p><p></p><h3>AIoT没有捷径，该补的IT基础课依旧绕不开</h3><p></p><p></p><p>InfoQ：比如说什么样的基础工作是企业绕不开的？</p><p></p><p>欧阳亮：首先，IT基础是一定要有的。因为AIoT是底层的东西，它非常注重现场的数据管理，这时候就需要一些基础技术做支撑。比如，要做边缘计算就一定要有边缘计算的网关、路由，才能把数据采集出来。</p><p>与此同时，对于工业企业来说，最基本的包括ERP、MES等等，这些系统中承载了企业的大量财务数据、运营数据、管理数据、生产数据等等，如果没有这些系统和其中的数据，那算法模型就没法做，边缘的数据即便采集出来也没有意义，因为边缘数据必须要融入到系统中去做统一分析，才能指导生产、做能源管理、做设备维护等等。</p><p></p><p>再拿工业互联网来说，从某种角度来看，它实际上是对ERP、MES这些传统系统的分拆，把它们进行APP化或者微服务化，通过筛选把可以开放的数据和模型放到里面，再和外部做对接，实现信息的共通和交互。所以，在这个场景下，IT系统也是非常基础的工作。</p><p></p><p>其次，系统的整合也很重要。因为以前很多企业的系统都是业务提需求去实施的，不同业务部门或者业务条线又可能提出类似的需求，这就导致企业上了大量的系统，并且系统之间还是割裂的。</p><p></p><p>我接触过的一个企业，他们内部总共有200多套系统，比如其中仅仅供应链就有4套不同的系统，有管基础件的、有管配套件的、有管精品件的、还有管核心件的；除此之外，还有n套销售系统、n套采购系统......</p><p></p><p>这时候就有两个问题，第一，底层的数据需要打通，数据要统一做梳理；第二，底层数据打通后怎么用。这是让企业很头疼的问题，但是必须花时间去做。</p><p></p><p>InfoQ：有没有什么方法或者路径可以让企业快速把基础打好，把短板补齐？</p><p></p><p>欧阳亮：其实现在企业做起来已经不是很难了，市场上有很多成熟的产品可选，企业根据自己的实际情况做选择就好。比如，对于中小企业来说，可能不需要花重金购买国外的高端ERP产品，现在很多国产ERP可能更适合国内企业。因为它们是按照国内企业的发展阶段、行业特点去做设计的。</p><p></p><p>当然，如果是大型企业，可能就要考虑更多的因素，除了通用的软件，还需要做一些定制化的开发。这时候，比较低成本和高效的做法是让行业中的第三方去做。虽然很多企业会在内部养一些专业的开发人才，但是一方面是目前市场上这样的人很少，另一方面，传统企业培养出了这些人，流失率非常高。这个企业可以去权衡考量。</p><p></p><p>总之，一定是要从实际出发，把自己核心的能力打造完整，一点点去做，这个工作其实没有太多捷径可走。</p><p></p><p>InfoQ：对于这些真金白银的投入，从实际出发确实很重要，那企业怎么才能更精准地评估自己的需求，而不是盲目跟风呢？</p><p></p><p>欧阳亮：我们说AIoT是AI和IoT的结合，IoT里面会涉及网络连接能力。但是，拿我们刚刚说的茶厂的例子来说，他们使用4G网就已经够了，这种情况下就不一定要用5G或者立即上云。这就是从实际出发。</p><p></p><p>当然，有时候我觉得企业做很多事情都是市场逼的。比如，对于很多中小民营企业来说，要让自己跟上大企业的步伐，就要更快地去发展，这时候他们的眼界、心态可能更开放；相反，对于很多大企业来说，因为没有太大的市场压力，对于新技术、新趋势的反应反倒比较慢。</p><p></p><p>这就是为什么转型期特别考验企业的战略眼光的原因，大多数企业是市场倒逼，不是主动变革，但要实现突破，主动变革又是必须的。</p>",
    "publish_time": "2022-10-08 17:23:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大数据 SQL 优化之数据倾斜解决案例全集",
    "url": "https://www.infoq.cn/article/ZONedwHlqAHoMsSg66Uy",
    "summary": "<p></p><h2>一、什么是数据倾斜</h2><p></p><p>&nbsp;</p><p>数据倾斜即指在大数据计算任务中某个处理任务的进程（通常是一个 JVM 进程）被分配到的任务量过多，进程运行时间超长甚至最终失败，进而导致整个计算任务超长时间运行或者失败。外部表现的话，在 MR 任务（如跑HiveSQL）里看到 map 或者 reduce 的进度一直是 99% 持续数小时没有变化；在 SparkSQL 任务里则是某个 stage 里，正在运行的任务数量长时间是 1 或者 2 不变。总之如果任务进度信息一直在输出，但内容长时间没有任何变化的时候，大概率是出现数据倾斜了。有个特例需要注意，有时候大家会看到 SparkSQL 的任务信息也显示有 1 到 2 个任务在运行中，但进度信息不再刷新而表现为假死很久的时候，这通常是在进行最后阶段的文件操作，并不是数据倾斜（虽然这通常意味着小文件问题严重）。</p><p>&nbsp;</p><p>再细分一下，倾斜可以分为以下四类：</p><p></p><p>1）读倾斜。即某个 map（HiveSQL）或者 task（SparkSQL）在读取数据阶段长期无法完成。这通常是因为文件分块过大或者此分块数据有异常。这种场景出现频率较小。</p><p></p><p>2）算倾斜。即在某个需要排序（如开窗函数或者非广播关联时）或者聚合操作的时候，同一个 key（通常是一个或者多个字段或者表达式的组合）的处理耗时过长。这通常是最多的情况，情况也较为复杂。</p><p></p><p>3）写倾斜。即某个操作需要输出大量的数据，比如超过几亿甚至几十亿行。主要出现在关联后数据膨胀及某些只能由一个 task 来操作（如 limit）的情况。</p><p></p><p>4）文件操作倾斜。即数据生成在临时文件夹后，由于数量巨大，重命名和移动的操作非常耗时。这通常发生在动态分区导致小文件的情况。<a href=\"https://www.infoq.cn/article/yGfFSW7sPVRCIoKOkXiD\">OPPO</a>\" 已在国内和印度及新加坡三区域集群上线了自动小文件合并的功能，其中基于亚马逊科技的小文件合并功能尚属业界首创，相关分享实践参看<a href=\"https://aws.amazon.com/cn/blogs/china/application-and-practice-of-spark-small-file-merging-function-on-aws-s3/\">Spark小文件合并功能在AWS S3上的应用与实践</a>\"。</p><p></p><h2>二、为什么会有数据倾斜</h2><p></p><p>&nbsp;</p><p>大数据计算依赖多种分布式系统，需要将所有的计算任务和数据经过一定的规则分发到集群中各个可用的机器或节点上去执行，最后可能还需要进行汇总到少数节点进行最后的聚合操作，以及将数据写到 HDFS/S3 等分布式存储系统里做持久化。这个过程的规则被设计来应对大多数情况，并不能应对所有的情况。它具有以下几个限制：</p><p></p><p>1）业务数据分布规律无法预知。比如系统无法不经过计算而提前知道某个表的某个字段的取值分布是否大致均匀。</p><p></p><p>2）计算结果数量无法预知。比如两表关联的结果对于某些 key（关联的一个字段或者多个字段组合）的输出行数无法不经过计算而预知进而针对性处理；又比如对某个字段的值进行 split 操作或者 explode 等操作后产生的结果数量无法预知而进行针对性的应对。</p><p></p><p>3）某些操作只能由单一节点进行。一切需要维护一个全局状态的大多数操作，如排序、Limit、count distinct，全局聚合等，一般会安排到一个节点来执行。</p><p>&nbsp;</p><p>上述限制有概率导致在单结点上处理巨量的数据，造成了所谓的倾斜问题。当然，这些困难并不是理论上不可解决的。随着时间的推移，越来越多的针对性的优化措施已逐渐出现（如 Spark3 已经能自动应对部分数据倾斜的情况了），也许在不久的将来业务同学不会再被倾斜问题烦恼，但现阶段还需要数据开发工程师主动关注并应对。</p><p></p><h2>三、典型案例</h2><p></p><p>&nbsp;</p><p>以下展示一些日常工作较为典型或者困难的数据倾斜样例，供大家参考。由于 <a href=\"https://www.infoq.cn/article/HHPHzeKSlWI5vW8U3kpD\">OPPO </a>\"主推 SparkSQL，因此以下案例将主要以 SparkSQL 的角度来展示。</p><p>&nbsp;</p><p>在这里我必须要强调一个原则：如果发生了数据倾斜，有不少的概率是业务逻辑不合理，因此需要跟业务方反复确认逻辑的合理性。优化业务逻辑一定要高过优化技术方案。</p><p></p><h4>3.1&nbsp;写倾斜 - 事实表关联事实表数据膨胀</h4><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/vkHROQzRXNkX5uooNGgl\">OPPO </a>\"有业务同学提出一个比较麻烦的问题，就是事实表关联事实表，其中有若干个 key 的输出达数十亿行，数据膨胀严重，造成数据输出的倾斜。比如以下场景：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/54/54/54347420e2be1cd5fa1aca63129b4454.png\" /></p><p></p><p>在反复确认了业务逻辑是合理的前提下，我们统计了两个表的倾斜 KEY 值分布：</p><p></p><p>a表：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6e/f1/6ea2eeeccb3a5663cc54955fa5c991f1.png\" /></p><p></p><p>b表：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6e/7a/6eafafyyc86c2196e64b05d1ba20df7a.png\" /></p><p></p><p>大家可以看出，只看 option_id=7 的关联结果最后是</p><p>46839*130836=6128227404，即 61 亿行；</p><p></p><p>option_id=2 的关联结果是</p><p>71080*125541=8923454280，即 89 亿行。</p><p></p><p>属于严重倾斜的情况。</p><p></p><p>这种事实表关联事实表的情况在非报表类的计算任务偶尔会遇到。平时我们解决数据倾斜主要是计算结果的过程涉及太多数据要处理导致慢，但通常输出的行数可能并不多，不存在写的困难，所以类似过滤异常数据或者广播关联等方法都不起作用。</p><p>&nbsp;</p><p>这个问题的本质是一个 task 最多由一个进程来执行，而相同的 key 也必须在同一个 task 中处理，因此在无法改变这个机制的前提下，我们只有想办法减少一个 task 输出的行数。</p><p>&nbsp;</p><p>那如何在不影响最终结果的前提下，减少单个 task 所需要处理数据行数呢？</p><p>&nbsp;</p><p>其实网上也有许多建议，都是单独处理倾斜的 key，通过加前缀后缀等方式打散 key，再最后合并处理，但这样做法太麻烦了，不够优雅。我们要追求对业务同学更友好，代码更优雅的方式。</p><p>&nbsp;</p><p>最后我寻遍所有可用的系统函数，发现了 collect_set/collect_list 这个聚合函数，可以在保证数据关系不丢失的前提下将数据收拢减少行数。比如以下两行：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/25/1c/2595f6e291b6eebaaaaf234f2ed5af1c.png\" /></p><p></p><p>&nbsp;</p><p>可以收拢成一行：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c9/af/c97356cf6d145c0bd64f4cdb647164af.png\" /></p><p></p><p>最后再通过 explode+lateral view 的方式，可以实现一行展开为多行，从而还原成用户最后期望的明细结果方式。</p><p>&nbsp;</p><p>上述办法的核心是将原来倾斜的操作（同一个 key 关联），修改为不再相互依赖的操作（一行变多行）。</p><p>&nbsp;</p><p>最终代码如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e1/bd/e1740227295c26606f0ec1ec043828bd.png\" /></p><p></p><p>以上代码里值得注意的地方：</p><p></p><p>代码里的 hint（repartition(1000)）的作用是考虑到经过 collect_list 聚合后的数据单行携带的数据经过一行变多行的展开操作后会膨胀很多倍，因此单个任务处理的数据量必须很小，才能保证处理速度够快。这个hint的作用是告诉系统将上一阶段关联后的结果分成 1000 份，交给下游处理；group by 语句里的 ceil(rand()*N) 作用是将一个 key 分成最多 N 行，这样可以限制最后按 key 关联后生成的行数的上限。</p><p>&nbsp;</p><p>经过验证，原来无法完成的任务，20 分钟任务就完成了，生成了近 800 亿行的数据，其中包括了 19 个超十亿行的 key，满足了业务需求。</p><p></p><h4>3.2&nbsp;算倾斜 - 避免排序</h4><p></p><p>&nbsp;</p><p>在单机数据库里（mysql,oracle等）进行排序操作是非常常见的；但大数据中动辄上亿行的数据，如果有排序（order by和sort by）的需求而且数据量巨大的话，通常会产生溢写磁盘的操作，非常耗时，且容易造成 OOM 异常。幸运的是 99.99% 的大数据场景下，完全的排序都是不必要的，因为业务通常需要的是统计信息而非具体某一条记录的细节信息。由于多数大数据数据开发同学是从传统单机数据开发转行过来的，经常会写上排序关键字，引发性能问题。下面介绍 2 个通过改写代码从而避免排序的案例。</p><p></p><h5>（1）用 max 函数替换排序</h5><p></p><p>&nbsp;</p><p>最近收到一个同事的业务需求，需要对某个业务的埋点数据做一次样本展示，要在约 1200 亿行数据中，捞出约1万条数据。很简单的一个 SQL 如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2e/22/2e83b6d99c1fbb63fcbbb51ab562d022.png\" /></p><p></p><p>稍微解释一下 SQL 的意思：希望取出上报数据里针对某个维度组合的一条内容较为丰富的样本数据，因此以某字段的size作为降序排序并取结果的第一条。</p><p>&nbsp;</p><p>这个 SQL 当然跑失败了。我对 partition by 的字段集合（后续简称 key）进行了统计，最大的 key 有 137 亿行，另外还有至少 10 个 key 的数据量超过 20 亿行。这样 executor 的内存加得再大都无法跑成功了。</p><p>&nbsp;</p><p>既然用户只需要排序后的最大的一条，本质上不就是取某个 key 的最大值嘛。取出这个最大值，最后再跟源表进行关联，就可以取出最大值对应的那一条数据了。其他的记录的序号计算其实都是多余的操作。</p><p>&nbsp;</p><p>但这里有个前提条件，要想在第二步关联回源表数据的时候干掉排序，我们只有走一条路：广播关联（如果走 sort-meger 关联，还是会避免不了 sort 步骤）。这就要求我们的小表（key-最大值）要足够小。通常这个条件都会满足的，因为如果不满足的话，说明key值非常多，非常稀疏，也不会产生倾斜的困境了。如开始就说明了，最后 Key 的去重数据量不到 1 万条，完全可以走广播关联。</p><p>&nbsp;</p><p>最后的代码如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6c/10/6c78c07df74f91b00242b80yy292b110.png\" /></p><p></p><p>注意上述 SQL 有两点说明：</p><p>&nbsp;</p><p>我们使用了 semi join，这在日常代码中比较少见。它的意思是，左表去匹配右表，如果一旦发现左表的某条数据的关联 key 在右表，便保留此条左表的数据，不再继续在右表里查找了。这样做有两个结果：1）速度更快；2）不会把右表的数据放到结果里。</p><p></p><p>它等价于 select * from left_table where key in (select key from right_table)。但大数据发展过程中一度不支持 in 的用法（现在部分支持了），因此有这种语法，从效率上看，一般认为这样更高效。</p><p>&nbsp;</p><p>因为能匹配到最大值的数据可能有许多条，所以对最后结果再做一次 row_number 的开窗并取其中一条即可。这个时候由于 size(xxxx)  的值都是一样的，因此任意取一条均符合业务需求。</p><p></p><p>在一般情况下，上述 SQL 能较好的运行。但我们这次情况出了点意外：经过上述操作后，我们得到的数据还有 800 多亿行。因为 max(size(xxxx) = size(xxxx) 的数据占了绝大多数，导致我们匹配回去无法有效的筛选出少量结果。我们必须找到一个能有效区分各行数据的字段，这个字段的值必须很松散。最后我发现比较好的是 userid。因此将 max(size(xxxx)) 替换成了 max(userid)，任务很快就跑完了。因为不影响我们讲述优化的原理，所以不再描述这部分细节。</p><p></p><h5>（2）用分位函数替换排序</h5><p></p><p>&nbsp;</p><p>在一个画像任务相关跑得很慢时，业务同学求助于我们，发现慢的代码如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/78/21/784fb19b5741600cf5b0601d9c497621.png\" /></p><p></p><p>问题点：上面的代码是想做一个全局排序，然后使用其序号所在位置来进行分类打标。上述代码在排序数据小于 5 亿 5 千万行的情况下勉强能运行出结果。但在某一天数据量到了 5 亿 5 千万行后就跑不出来，加了 reducer 的内存到 10G 也不行。</p><p>&nbsp;</p><p>新思路：虽然可能还有一些参数能调整，但我认为这不是正确的方向，于是停止了研究，把方向转为干掉全局排序。在和一位前辈沟通的时候，突然意识到，既然业务是想做一个分档，本质上就并不需要具体的排序号，所以理论上完全的排序是可以省掉的。于是自然想到了分位数函数，立马想到了新方案。分位函数用于计算出某个值，大于这个值的数据行才能处于整个数据排序的某个百分比位置。</p><p>&nbsp;</p><p>改之后代码如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/bc/e9/bc6ea34b15392ce057b38bd97174ede9.png\" /></p><p></p><p>注意上述代码有个小技巧，即与只有一行的子查询结果进行笛卡尔积关联，从而变相的实现了引入 p2 到 p8 等 4 个变量的效果，还算实用。</p><p>&nbsp;</p><p>效果：对比了新旧算法的结果，差异极小，也在预期范围内，业务表示接受。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/02/8c/02721e2628608cff00cce603f3abb48c.png\" /></p><p></p><p>再对比了任务执行时间，约有 87% 的降幅：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/35/2a/35e778b11f84af4f03f2a14761d4de2a.png\" /></p><p></p><p>这个案例的本质在于识别出了费尽资源计算的全局序号是完全不必要的，业务真正的目的只是找到一个评价的档位而已。</p><p></p><h5>（3）通过广播关联彻底避免排序</h5><p></p><p>&nbsp;</p><p>SparkSQL 目前处理关联( join )的方法主要有两种：</p><p>&nbsp;</p><p>a)&nbsp;广播关联。小表（通过参数 spark.sql.autoBroadcastJoinThreshold 控制，目前我们的默认值是 20M）的话会采用广播关联，即将小表的全部数据传输到各节点的内存中，通过直接的内存操作快速完成关联。这种方式最大的好处是避免了对主表的数据进行 shuffle，但会增加任务使用的内存量。另外特别说明3点：</p><p>目前sparksql优化器尚不能非常准确地判断一个子查询结果（也被当成一张小表）是否适合进行广播；左表无论大小都不能被广播；某些情况下会有类似：Kryo serialization failed: Buffer overflow 这样的 OOM 出现，并 “To avoid this, increase spark.kryoserializer.buffer.max value”。但其实这样设置会无效。实质原因是：虽然某张表小于 32M，但由于高度压缩后，解压结果的行数达到了数千万，造成了节点的 OOM。这个时候，只能手动禁掉广播关联。</p><p>&nbsp;</p><p>b)&nbsp;Sort-Merge关联。即先将两表按 join 字段进行排序，然后在此基础上进行匹配关联。由于数据是排序过的，只需要一次性的匹配即可完成最终的关联，不需要往复查验，速度较快。但这种方法的弊端是要进行对关联 key 的排序，并且每个相同的Key和对应的数据必须分配到一个 executor 里，引发大量的 shuffle 操作；另一方面如果一个 executor 需要处理一个巨量的 key，通常会花费大量的时间以及大量的磁盘 IO。</p><p>&nbsp;</p><p>通过上述原理描述可以看出如果采用广播关联，引擎完全不用做任何排序和shuffle，自然也不会有相应的倾斜的可能性，这是效率巨大的提升，当然代价就是会增加内存占用。一般来说这种内存使用的增加被认为是划算的。</p><p>&nbsp;</p><p>如果引擎没有识别出来，我们可以通过主动指示（hint）的办法影响执行计划。比如以下代码：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e0/8e/e0b4656da59639cc5983bd99199bbd8e.png\" /></p><p></p><p>要让执行计划改成广播s子查询结果，加 hint mapjoin （也可以是 broadcast）就可以了。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/da/4e/da38a4caf447d82c5c433b5fcea1fc4e.png\" /></p><p></p><p>从实际的结果看，广播关联的提速都非常喜人。</p><p></p><h4>3.3&nbsp;算倾斜 - 非法值过滤</h4><p></p><p>&nbsp;</p><p>这应该是网上讲得比较多的情形，我也简略说下。在优化某任务的时候，我们发现运行时间一直在增长，一度达到 7 个小时，直到 8 月 1 号便再也跑不成功，总是 OOM（内存不够），即使将 executor 的内存调高到 10G 依然解决不了问题。经过仔细诊断，发现任务慢在一个开窗函数阶段，代码如下：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2f/71/2febaf811ef134f7b7050beac2596271.png\" /></p><p></p><p>在对 guid 这个 key 进行初步统计后，发现为空值的数量竟然有数亿行，并不断增长：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b2/05/b2e3bf6344fca7bdaa7c9475bb9d0105.png\" /></p><p></p><p>这也就解释了运行时长不断增长，排序的内存开销和时长都不断增长，直到某个阈值连加内存也不管用了。经过和业务同学的沟通，确认空值无意义，进行排除过滤。然后在默认的参数下进行了重跑，30 分钟内就跑完了。耗时下降约 90%，效果明显。</p><p>&nbsp;</p><p>这个例子里，倾斜值恰好是无效的可以直接过滤，比较幸运。那同学们会问，如果倾斜值是有价值的怎么办？通常来说是需要将这类倾斜值单独拎出来以另外一套针对性的逻辑来计算，然后将结果 union all 回到其他非倾斜的数据计算结果里。我不推荐大家使用对倾斜字段加盐（加上随机的前缀和后缀以伪装成不同的 key），因为会让代码不够优雅，同时掩盖了数据质量问题。更重要的还是优先从业务上排查这些倾斜的值的合理性，尽量保证从最初源头消灭异常。</p><p></p><h2>四、结语</h2><p></p><p>&nbsp;</p><p>数据倾斜处理的情况基本上局限在上述案例分类里，相信大家稍加学习都能掌握。总结一下核心的点：</p><p>1、分布式数据处理系统无法预知输入和输出的数据量，加上单节点处理有限，造成了倾斜；</p><p>2、大多数情况下业务逻辑不需要完全排序；</p><p>3、一切的优先从业务逻辑着手，从源头处理脏数据，不要把倾斜带往下游。</p><p>&nbsp;</p><p></p><p>本文作者简介：</p><p>Luckyfish，OPPO 大数据基础平台 SRE 及服务负责人。主要负责公司大数据平台 SRE 及用户支持工作，曾供职于京东科技，有较丰富的大数据任务开发和性能优化经验，同时对产品体验及成本优化工作有较多兴趣和经验。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2022-10-08 17:38:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "逃离AWS后，我们将服务器的年开支省下了80%",
    "url": "https://www.infoq.cn/article/HHzXs5UTFXcbsAdphEPr",
    "summary": "<p>近日，skilled.dev和gitconnected.com网站的创始人Trey Huffine采访了Prerender.io的首席工程师兼经理Zsot Varga。Zsot告诉大家，Prerender设法摆脱了对AWS的依赖，并构建了内部基础设施来处理流量和缓存数据，这一做法为他们节约了80万美元的年开支。</p><p>&nbsp;</p><p></p><blockquote>“目标是降低成本，同时保持相同的渲染速度和服务质量。像这样的迁移工作需要周密的计划和认真的执行，因为不正确的配置或执行不力会让客户网页和社交媒体按钮挂掉，影响他们的搜索排名，并可能增加我们的流失率。”</blockquote><p></p><p></p><h2>Pretender公司简介</h2><p></p><p></p><p>Prerender提供的服务是缓存和预渲染客户的<a href=\"https://www.infoq.cn/article/CSRgKxyZK0XNK9ZARYEp\">JavaScript</a>\"页面，给搜索引擎提供一个纯HTML文件来抓取和索引。客户只需在自己的网站上安装一个中间件就能搞定了。这个服务需要云端处理数据，所以Pretender选择了AWS，之前每分钟能处理7万个页面，存储了5.6亿个页面数据，每年的AWS费用超过一百万美元。而三个多月的迁移工作完成后，总成本下降了80%。</p><p></p><h2>迁移计划</h2><p></p><p></p><p>Pretender在<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651124439&amp;idx=4&amp;sn=62a689cb15df69b8fb7c0471e8b7c791&amp;chksm=bdb90a848ace83927afe475b487935cd6c69b611088cded812504b180989a149b876e894c3a7&amp;scene=27#wechat_redirect\">AWS的开销</a>\"主要分为两部分：</p><p>&nbsp;</p><p>数据缓存。预渲染的页面都得存在AWS服务器上，然后搜索引擎爬取的时候提供静态页面，网站用户点击时提供动态JS页面。存储、维护费用是很高的。流量成本。把数据导入AWS是免费的，问题是实践中这些数据还要被经常读取，这个费用也不容小觑。</p><p>&nbsp;</p><p>所以他们的解决方案也很简单，将缓存的页面和流量迁移到Prerender自有的内部服务器上，尽量摆脱对AWS的依赖。一开始Pretender估计成本能砍掉40%，但除了降本，渲染速度和服务质量是不能打折的。</p><p>&nbsp;</p><p>谨慎起见，Pretender规划了三阶段的迁移计划，可以随时在客户无感知情况下回滚。不过系统性测试需要持续跑几周甚至几个月。</p><p></p><h2>分阶段迁移</h2><p></p><p></p><h3>第1阶段：测试（4到6周）</h3><p></p><p></p><p>第1阶段的主要工作是设置裸金属服务器，先在容易管理的小规模集群上测试，然后再扩展上去。这个阶段需要的软件适配工作是最少的，所以团队决定跑在Linux上的KVM虚拟化环境里。</p><p>&nbsp;</p><p>5月初，第一批服务器上线，1%的Prerender流量被定向到新服务器。迁移两周后，日成本就已经下降了800美元。到月底，大部分流量负载都从<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651124439&amp;idx=4&amp;sn=62a689cb15df69b8fb7c0471e8b7c791&amp;chksm=bdb90a848ace83927afe475b487935cd6c69b611088cded812504b180989a149b876e894c3a7&amp;scene=27#wechat_redirect\">AWS</a>\"迁移走了，每日chrome渲染负载成本降低了45%。</p><p>&nbsp;</p><p>当时的服务端成本是每月13,000美元，综合来算开支削减了22%。</p><p><img src=\"https://static001.geekbang.org/infoq/30/3053deb4e43cdfc264cb39c147eae23f.png\" /></p><p></p><p>测试阶段对后面的工作非常重要。团队在已有监控板之外还做了一个新的渲染监控仪表板来加强<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651119126&amp;idx=1&amp;sn=849ed6d3fb51e03c904f1a12a425d1c6&amp;chksm=bdb926458aceaf53fb18c09beca81de8d66f8d55bfd3951d77f887fc86a7a79cccb82473f4c6&amp;scene=27#wechat_redirect\">可观测性</a>\"。之后一切顺利，第二阶段启动。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/02b59638955e6089047c5579fb15528b.png\" /></p><p></p><h3>第2阶段：技术设置（4周）</h3><p></p><p></p><p>第二阶段时间是6月至7月初。主要工作是将缓存移动到裸金属服务器上。</p><p>&nbsp;</p><p>6月中旬，Pretender自有的300台服务器缓存页面总数达到了2亿。注意这些服务器上都使用了Apache <a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651134324&amp;idx=5&amp;sn=846912161d3ce1fa4a124ad3effc3b57&amp;chksm=bdb8e1278acf683154b4cee7fe6ada0182cd6c8b95b1469c320a39c91a3f35192e66e1022bf0&amp;scene=27#wechat_redirect\">Cassandra</a>\"节点，这种节点是和AWS S3兼容的，确保平滑迁移。</p><p>&nbsp;</p><p>线上迁移分为四步，每两步之间相隔一两周。团队首先测试了Prerender页面是否可以同时缓存在S3和minio对象存储中，显然没什么问题。之后流量逐渐从AWS S3转移到minio上。迁移完成后又节省了每天200美元的S3 API费用，缓存在Cassandra集群中的数据也准备删掉了。现在服务器成本从AWS上的3.5万美元每月降到了自有服务器的1.4万美元每月。</p><p>&nbsp;</p><p>AWS上还剩下一点数据，每天开销大约60美元，但过几星期这些数据就自然过时不用了。之所以不选择一次性把这些数据都迁走，是因为数据迁移出AWS时还要一次性浪费5000美元的转出流量费用。所以这里就是一个坑：数据导入AWS不收钱，提取出来可就不一样了。另外不同地区的流量费用可能还有差异，比如说亚太的流量费用就比北美的要高不少。很多公司都对这方面的成本稀里糊涂，莫名其妙就花了冤枉钱。还好Pretender算好了这部分支出，没有一激动就把数据迁走而是等它自己过期。第二阶段结束后，总成本下降幅度从第一阶段的22%来到了41.2%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5b2626f668bac0204c70f035108eb983.png\" /></p><p></p><h3>第3阶段：实施和扩展（4到6周）</h3><p></p><p></p><p>剩下的工作就是把其他数据都迁到自有服务器上。这一步需要迁移所有Amazon RDS实例，而且得一个切片（shard）一个切片移动。这一步最容易出错，不过因为剩下的数据也不多了，所以就算出问题也没什么大影响。具体的步骤是：</p><p>&nbsp;</p><p>在Cassandra集群中存储cached_urls表的PostgreSQL分片镜像；将service.prerender.io切换到Cloudflare负载均衡器，提供动态流量分配；启动新的私有重缓存服务器，位于欧洲；持续压测，解决各种性能问题。</p><p>&nbsp;</p><p>最终迁移圆满成功。所有缓存页面都被重定向后，每月的服务器费用下降了80%，远超之前40%的预测。</p><p></p><h2>迁移经验和总结</h2><p></p><p></p><p>大规模迁移过程中一旦出现问题或者进度落后，都可能造成巨大风险。所以Pretender在每个迁移阶段都有故障安全措施，一旦出问题就会回退。另外迁移之前要先在小集群上做好测试。迁移的计划都要精心制定，每一步扩展到大规模之前都要做测试，这样风险就是可控的。</p><p>&nbsp;</p><p>Zsot还谈到了关于Pretender公司与他个人的一些事情。</p><p>&nbsp;</p><p>Pretender提供的服务能让客户把关注的焦点从网站SEO转移到用户体验上。这样客户想用JS，想用React，想用什么动态页面都不用担心影响SEO排名了。公司使用的技术栈就是JS，毕竟业务就是针对JS页面的。针对全球部署，公司选择的是CloudFlare；公司还选择了DigitalOcean来保证可用性，以及其他很多SaaS服务来提升效率。Zsot自己家里放了8台服务器，不过平时的工作都是用Macbook完成的。玩游戏的时候就找Windows笔记本，平板自然也不缺。Zsot日常主要用VSCode，最近还在尝试新出的Copilot。代码仓库主要用GitHub，GitLab也会用。消息应用主要是Slack，这些天Zsot还发现了Spike这个电子邮件客户端很好用。主力容器工具是Docker，K8s也在适应中。</p><p>&nbsp;</p><p>原文链接：</p><p></p><p><a href=\"https://levelup.gitconnected.com/how-we-reduced-our-annual-server-costs-by-80-from-1m-to-200k-by-moving-away-from-aws-2b98cbd21b46\">https://levelup.gitconnected.com/how-we-reduced-our-annual-server-costs-by-80-from-1m-to-200k-by-moving-away-from-aws-2b98cbd21b46</a>\"</p><p></p><p></p>",
    "publish_time": "2022-10-08 18:11:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]