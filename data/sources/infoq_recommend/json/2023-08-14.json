[
  {
    "title": "每秒处理480万会员资料请求，LinkedIn是如何实现的",
    "url": "https://www.infoq.cn/article/vXAjOHKZIA1Sbg7vXrzF",
    "summary": "<p>LinkedIn<a href=\"https://engineering.linkedin.com/blog/2023/upscaling-profile-datastore-while-reducing-costs?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">将Couchbase作为集中式缓存层</a>\"，用于伸缩会员资料读取操作，以应对日益增长的、超出现有数据库集群处理能力的流量。新方案实现了超过99%的命中率，将尾延迟降低了60%以上，将年度成本降低了10%。</p><p></p><p>多年来，LinkedIn直接从其<a href=\"https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Espresso文档平台</a>\"提供会员资料。Espresso平台建立在<a href=\"https://www.mysql.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">MySQL</a>\"之上，并使用<a href=\"https://avro.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Avro</a>\"进行序列化，还包含了<a href=\"https://helix.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Apache Helix</a>\"和<a href=\"https://github.com/linkedin/databus?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Databus</a>\"（LinkedIn的变更捕获系统）组件。Espresso路由器处理资料读取请求，将读/写请求定向到正确的存储节点，并使用堆外缓存（OHC）进行热键缓存。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/07/linkedin-member-profile-caching/en/resources/1linkedin-espresso-1688241494720.jpeg\" /></p><p></p><p>图片来源：<a href=\"https://engineering.linkedin.com/blog/2023/upscaling-profile-datastore-while-reducing-costs?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">https://engineering.linkedin.com/blog/2023/upscaling-profile-datastore-while-reducing-costs</a>\"</p><p></p><p>随着存储请求量每年翻倍，峰值超过每秒480万次，为会员资料提供服务的Espresso集群已经达到了伸缩性的极限。团队决定引入一个基于<a href=\"https://www.couchbase.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Couchbase</a>\"的缓存层，而不是重构Espresso平台的核心组件，因为超过99%的请求都是读取操作。</p><p></p><p>LinkedIn软件工程师<a href=\"https://www.linkedin.com/in/estella-pham/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Estella Pham</a>\"和<a href=\"https://www.linkedin.com/in/guanlin-lu-bbba1517/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Guanlin Lu</a>\"解释了团队为什么选择Couchbase作为缓存：</p><p></p><p></p><blockquote>在LinkedIn，我们已经将Couchbase用作各种应用程序的分布式键值缓存。它被选中是因为它比memcached更强大，包括用于保存服务器重启之间持久化的数据，在集群中的个体节点发生故障时所有文档仍然可用的复制功能，以及可以在不停机的情况下添加或删除节点的动态可伸缩性。</blockquote><p></p><p></p><p>新的缓存层结合了OCH和Couchbase，并被集成到了Espresso中，不需要客户端做出修改。其设计重点是Couchbase的故障弹性、缓存数据可用性和数据分歧预防。Espresso路由器会在发生暂时性故障时重试请求，并监控Couchbase健康状况以避免将请求发送到不健康的桶。会员资料数据被复制了三次，如果首领副本不可用，路由器会将其转移到其中的一个跟随者副本。</p><p></p><p>所有的会员资料数据都缓存在每一个数据中心里，由<a href=\"https://samza.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Apache Samza</a>\"作业根据Espresso捕获的写操作进行实时的更新，以及根据数据库快照进行定期的更新。所有的缓存更新都使用了<a href=\"https://docs.couchbase.com/java-sdk/current/howtos/concurrent-document-mutations.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">Couchbase Compare-And-Swap</a>\"（CAS）来检测并发更新，并在必要时重试更新。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e56046646710ea59d192e304223743a3.webp\" /></p><p></p><p>图片资料：<a href=\"https://engineering.linkedin.com/blog/2023/upscaling-profile-datastore-while-reducing-costs?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">https://engineering.linkedin.com/blog/2023/upscaling-profile-datastore-while-reducing-costs</a>\"</p><p></p><p>经过调整之后，Profile Backend服务将负责处理一些原先由Espresso处理的操作。它会动态评估请求字段并返回保存在缓存中的完整资料数据的子集。它还会处理Avro模式转换，并在必要时从注册表获取模式版本。</p><p></p><p>LinkedIn的团队进行了进一步的性能优化，简化了Avro/二进制格式的数据读取，并在反序列化性能方面实现了约30%的改进。因为引入了新的混合缓存方案，Espresso的节点数减少了90%。考虑到运行Couchebase集群、缓存更新作业所需的新基础设施和运行后端服务新增的计算资源，为会员资料请求提供服务的总成本每年下降了10%。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/linkedin-member-profile-caching/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTE3MjIzODksImZpbGVHVUlEIjoiR2FLVlFMTEs3WVlNeW9HZSIsImlhdCI6MTY5MTcyMjA4OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.l20WpuKCWg1YMp58-JLJVG-QQDn3w9P16C18szuxgF4\">https://www.infoq.com/news/2023/07/linkedin-member-profile-caching/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/ZQEP69byw99C818CzmF6\">领英采用 Protobuf 进行微服务集成，将延迟降低了 60%</a>\"</p>",
    "publish_time": "2023-08-14 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "再问低代码：AI巨浪下",
    "url": "https://www.infoq.cn/article/67FD5gszt7PPmv1ZUlj7",
    "summary": "<p>AI狂飙的100多天里，<br />\n有人神化，有人漠视，<br />\n有人恐慌，有人兴奋，<br />\n有人坐而论道，<br />\n有人早已躬身入局，<br />\n当AI涌入低代码，<br />\n新的问题，能否打败旧的争议？</p>\n<p>近期，InfoQ 联合钉钉围绕“AI浪潮下的低代码”，邀请了多位低代码赛道的资深从业者，他们分别是：钉钉宜搭平台负责人叶周全；易鲸云CEO刘金柱；伙伴云产品总监吴杨；轻流产研负责人陆琦；氚云产品总经理詹萧；简道云运营负责人沈涛以及主持嘉宾极客邦科技创始人兼CEO霍太稳，7位嘉宾共同打造了这期《再问低代码：AI巨浪下》，共同探讨AI新时代，低代码的未来方向。</p>",
    "publish_time": "2023-08-14 14:18:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "我的20年职业生涯：全是技术债",
    "url": "https://www.infoq.cn/article/MWPVDU3lqV9FH4x8Dm7a",
    "summary": "<p></p><blockquote>那些年，写过的代码终将成为技术债？</blockquote><p></p><p>&nbsp;</p><p>1992 年，Ward Cunningham 在敏捷宣言中首次提出了“技术债”概念，主要指有意或无意地做了错误的或不理想的技术决策所累积的债务。随后，《重构》一书的作者 Martin Fowler 基于 Cunningham 的比喻，创建了一个“技术债务四象限”，包括：</p><p>&nbsp;</p><p>鲁莽/有意：“我们没有时间去设计”；谨慎/有意：“我们必须现在交付，之后再处理因为追求速度所产生的结果”；鲁莽/无意：“什么是分层？”；谨慎/无意：“我们现在知道应该怎么做了”。</p><p>&nbsp;</p><p>前段时间，Reddit 上有关技术债的话题再次引起程序员的广泛讨论。用户 spo81rtyOP 表示，“大多数软件的实际使用寿命也就 5 到 10 年。即便软件能幸存下来，完全由过时技术栈编写这一现实也会让它的路子变得很窄。这就是软件工程师的真实命运。”</p><p>&nbsp;</p><p>创业公司 CTO Matt Watson&nbsp;则直言，他过去 20 年的职业生涯全是技术债。</p><p>&nbsp;</p><p>Watson&nbsp;13 岁开始编程，22 岁时创办了自己的第一家科技公司 VinSolutions，实现了 3000 多万美元的 ARR，并于 2011 年以 1.5 亿美元的价格将其出售给 AutoTrader。离开 VinSolutions 后，Watson&nbsp;创办了一家名为 Stackify 的公司，为软件开发人员提供应用程序监控。此外，Watson 还在菲律宾创办了离岸开发公司 Full Scale ，以支持他的 SaaS 公司。Watson 透露，Full Scale 已发展到拥有 300 多名员工。</p><p>&nbsp;</p><p>Watson&nbsp;在博客中提到，当他听到有人说“我们正在快速开发 MVP，同时最大限度地减少技术债”时，他只是笑笑，因为他知道，最终所有东西都会变成技术债。Watson&nbsp;在博客中介绍了自己的 20 年职业生涯发生的变化，他悲观地表示：“如果时间足够长的话，你的所有代码都将被删除。”</p><p></p><h2>“如果时间足够长，你所有的代码都将被删除”</h2><p></p><p></p><h3>&nbsp;那些早被遗忘的技术和过时的编程语言</h3><p></p><p>&nbsp;</p><p>Watson&nbsp;的职业生涯始于 Visual Basic 6 的开发。从 1999 年到 2003 年，Watson&nbsp;构建了多个不同的应用程序。后来，Watson&nbsp;又花了很多时间进行经典的动态服务器页面（ASP）开发，自己也成为了在 Internet Explorer 6 和 Netscape Navigator 制作兼容网站的专家。</p><p>&nbsp;</p><p>但在今天来看，Visual Basic、ASP、IE6 和 Netscape 都是早已被遗忘的技术了。</p><p>&nbsp;</p><p>与此同时，在过去的 20 多年里，很多编程语言也都“失宠”了，比如 Perl、Delphi、Fortran、FoxPro、ColdFusion。也许这些古老的编程语言还存在某些应用程序中，但大多数情况下，还应用这些编程语言的公司必须要对旧的应用程序进行现代化改造，并将其淘汰。如果你用这些过时的编程语言构建程序，最终的结果可能只有重写，因为很难再找到使用这些语言的程序员了。</p><p>&nbsp;</p><p>在 21 世纪初，人们认为 Adobe ColdFusion 是最热门的产品，但在今天呢？Ruby on Rails 也可能走上 Adobe ColdFusion 的老路，它已经失宠了，并且很难找到使用它的开发人员。曾经 Ruby on Rails 独有的东西，现在也可以在其他语言中使用了。</p><p>&nbsp;</p><p>Watson&nbsp;表示，编程语言来来往往，开发人员不希望学习工作中不需要的技能。同时，开发人员跳槽的速度也很快，他们总是希望自己的简历上有一些热门的新东西。</p><p></p><h3>曾辉煌过的ActiveX、Java Applets、Flash和Silverlight</h3><p></p><p>&nbsp;</p><p>Watson&nbsp;最初开发的一些应用程序使用了 Internet Explorer 6 中的 ActiveX 控件。当时，需要用它们来做打印和其他一些非常不安全的黑客工作。PDF 在当时并不常见，用浏览器打印简直就是一场噩梦。</p><p>&nbsp;</p><p>Java Applets 也曾辉煌过，但它运行缓慢，并且在电脑上安装正确版本的 Java 总是一团糟。Watson&nbsp;称自己永远不会忘记处理 Java 小程序网络防火墙的噩梦。“我一点也不怀念它们，幸运的是，它消失了。”</p><p>&nbsp;</p><p>此外还有 Macromedia/Adobe Flash。当时 Flash 游戏层出不穷，许多软件都是用 ActionScript 在 Flash 中构建的。现在，一个名为 CheerpX 的产品允许使用 WebAssembly 运行旧的 Flash 应用程序。</p><p>&nbsp;</p><p>微软曾推出一个名为 Silverlight 的 Flash 竞品。对于 C# 开发人员来说，这实际上是一个非常棒的框架。Watson&nbsp;的公司也曾用 Silverlight 构建了一些非常棒的东西。不过后来，苹果在浏览器中放弃了对 Flash 和 Silverlight 的支持，从而终结了它们。</p><p>&nbsp;</p><p>下图是十多年前，Watson&nbsp;在 VinSolutions 中使用 Silverlight 构建财务计算器的屏幕截图。Silverlight 现在早已不复存在，他们用 JavaScript 重写了它，但 Watson&nbsp;认为，新版本没有旧版本酷了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f64258df9144a05ea50fa957d66c6081.webp\" /></p><p></p><p></p><h3>开发工具的变化有多快？</h3><p></p><p>&nbsp;</p><p>2004 年还没有 iOS 和 Android，当时，Watson&nbsp;曾为 Compaq PDA 编写了一个应用程序，用于跟踪汽车经销商的库存。它是用 C# 编写的，用于在 Windows CE 上运行的 .NET Compact Framework 中。</p><p>&nbsp;</p><p>这个 PDA 有一个 100万 像素的摄像头，只要外面是阴天，照片就会糟糕些。这个应用程序很早以前就被淘汰了，但在 2005 年时它还很前卫。</p><p>&nbsp;</p><p>Swift</p><p>&nbsp;</p><p>Swift 是另一个很好地说明开发工具变化速度之快的例子。苹果公司发布 Swift 后，就很难再证明用 Objective C 编写代码是合理的了。虽然在某些用例中仍然需要用 Objective C，但 Swift 明显更易于开发，并且是向前迈出的重要一步。</p><p>&nbsp;</p><p>Watson&nbsp;认为，现在任何用 Objective C 编写的应用程序都可能是技术债了。</p><p>&nbsp;</p><p>WebForms</p><p>&nbsp;</p><p>在为构建 Web 应用程序编写了疯狂的内联脚本之后，Watson&nbsp;很乐意使用新的 ASP.NET Web 表单，其服务器端控件大大简化了开发。它们的目标是让创建 Web 应用程序变得像在 Visual Basic 6 中一样简单。开发者可以在服务器端构建可重用的 UI 组件以呈现给浏览器，就像今天使用 100% 的 JavaScript 所做的那样。</p><p>&nbsp;</p><p>WebForms 并不完美，但它是一个相当大的提升。在 Ruby on Rails 出现并普及了用于开发 Web 应用程序的 MVC（Model-View-Controller，模型-视图-控制器）框架之前，它一直运行得很好。</p><p>&nbsp;</p><p>MVC 很快就淘汰了开发者制作的所有 Web 表单应用程序。Watson&nbsp;认为，任何网页形式的东西都绝对是技术债。</p><p>&nbsp;</p><p>MVC</p><p>&nbsp;</p><p>不知不觉中，每种编程语言就都支持 MVC 框架了。Watson&nbsp;也曾转而使用 ASP.NET MVC 做所有的新功能。它无处不在，包括 Django、Laravel、Symfony、Spring 等。</p><p>&nbsp;</p><p>快进到今天，MVC已经过时了。现在一切都是在 React、Angular、Vue 和其他框架中完成的。在此之前，开发者还会使用 Javascript 框架。在 Stackify 工作时，Watson&nbsp;还曾使用过 Knockout，这是一个相当流行的前端框架。</p><p>&nbsp;</p><p>但在今天，还有人记得 Knockout、Ember、Aurelia、Meteor、Backbone、Handlebars 这些框架吗？它们都“失宠”了，甚至被划分为技术债。毫无疑问，第一代前端框架输给了 React 和 Angular。</p><p>&nbsp;</p><p>Angular JS</p><p>&nbsp;</p><p>2015 年，谷歌创建了 Angular，Angular 迅速成为最受欢迎的前端框架。2016 年，Angular 进行了一次重大升级，不再向后兼容。这意味着，原始版本中的任何内容现在都是技术债。</p><p>&nbsp;</p><p>Watson&nbsp;曾在项目中使用过旧版本的 Angular，如今却成了他必须升级的主要技术债。</p><p></p><h3>过时的SOAP和WCF</h3><p></p><p>&nbsp;</p><p>在 REST API 和 JSON 成为事实上的标准之前，另一种选择是 SOAP，它代表简单对象访问协议，主要由基于 XML 的 Windows 通信框架（WCF）来使用。它使得调用 Web 服务并通过自动代码生成代理类来正确调用服务变得更容易。</p><p>&nbsp;</p><p>Watson&nbsp;职业生涯中最糟糕的一个项目，就是要弄清楚如何在他的公司和另一家供应商之间通过 WCF 和 SOAP 使用安全证书。SOAP 和 WCF 的承诺令人惊叹，但随着时间的推移，维护它简直是一场噩梦。</p><p>&nbsp;</p><p>微软决定不再支持 .NET Core 中的 WCF，REST、gRPC 和 GraphQL 现在才是首选。尽管如此，有个社区项目最终使 CoreWCF 得以继续发展。</p><p>&nbsp;</p><p>随着时间的推移，开发者用来调用 Web 服务的技术类型已经发生了变化。旧的方式仍然有效，但大多数人可能更愿意淘汰它们。</p><p>&nbsp;</p><p>此外还有编程语言的版本更改问题。无论是 Ruby、PHP、.NET 还是其他语言，它们通常需要改写大量的代码，甚至是完全重写。</p><p>&nbsp;</p><p>当 .NET Core 刚发布时，它是专为在 Linux 上运行而设计的更新、更轻、更快的 .NET 版本。基本的 C# 代码都很容易移植过来，但没有人会在真实的应用程序中只使用基本代码。然而，在复杂的企业应用程序中，想要升级时可能会出现许多潜在的问题。这就成为了一笔必须解决的重大技术债。否则，开发者最终会陷在一个古老的版本中。</p><p>&nbsp;</p><p>这些主要版本的更新，最终会成为重大的技术债项目。</p><p>&nbsp;</p><p>Watson&nbsp;在 Stackiy 遇到的最大挑战之一是卡在了旧版本的 Elasticsearch 上。有一次，它们对其工作方式进行了一些重大的更改，但这些更改并不完全向后兼容。Watson&nbsp;的团队大量使用了它，于是所有的升级工作都变成了海量的技术债和升级项目。</p><p></p><h3>所有的代码都将被替换</h3><p></p><p>&nbsp;</p><p>在 Stactify 时，Watson&nbsp;曾为 6 种编程语言构建了自己的跟踪/测评分析库，这项工作的工作量令人难以置信。随着 OpenTelemetry 出现，Watson&nbsp;过去的这些工作变得毫无用处。既然可以使用开源的行业标准，为什么还要自己管理呢？Stackiy 正在慢慢地消除那些 Watson&nbsp;帮忙构建的.NET测评分析器。</p><p>&nbsp;</p><p>Watson&nbsp;在职业生涯早期开发的几个应用程序都已经被终止了，因为这些公司被收购了，并且决定使用完全不同的技术。</p><p>&nbsp;</p><p>Watson&nbsp;认为，随着时间的推移，你会看到你创造的几乎所有的东西都会因为各种原因而被废弃和替换，或者现在就已经都是基于旧技术的了。大多数软件的使用寿命都很有限，比你想象的要短。所有的代码最终都变成了技术债，每个人都想用更现代的方式重写，或者业务需求发生重大的变化。</p><p>&nbsp;</p><p>诚然，在企业界，更有可能拥有似乎永远存在的内部应用程序。像铁路或大型银行这样的公司使用同样的基于大型机的软件已经有 40 年了。</p><p>&nbsp;</p><p>Watson&nbsp;预测，WebAssembly 最终会超越当今的前端开发，一个全新的世界将不断发展。</p><p></p><h2>一切终将变成技术债？</h2><p></p><p>&nbsp;</p><p>在做新项目时，大家总是希望将技术债降至最低。但 Watson&nbsp;认为，不可能不产生技术债，因为根本没有十全十美的东西。随着时间的推移，今天完美的东西将来也会不完美，因此，我们需要学会与不完美共存。</p><p>&nbsp;</p><p>而技术债的另一面是，随着时间的推移，一切都会慢慢“腐烂”——要么在升级到最新版本方面存在重大问题，要么由于更新的操作方式而最终失宠。</p><p>&nbsp;</p><p>“一切最终都会变成技术债，否则项目就会夭折。如果幸运的话，你的代码能存活足够长的时间，从而成为别人的技术债。如果时间足够长的话，你的所有代码都将被删除。”Watson&nbsp;在博文的最后说道。</p><p>&nbsp;</p><p>Watson&nbsp;的观点引发了很多开发者的讨论。赞同者认为自己过去做的很多工作最终都被取代了，辛苦编写的代码可能几年后就没有了用武之地；反对者则认为不应如此悲观，因为有些代码真的可以长青不老。</p><p>&nbsp;</p><p>Reddit 用户、前几年刚退休的开发者 vital_chaos 提到，他这辈子在编写代码方面投入了 40 年时间，在他参与过的所有技术要素当中，只有一种至今仍在得到实际应用：</p><p>&nbsp;</p><p></p><blockquote>“我的团队从 1988 年起着手开发一款应用程序，直到 1994 年正式开发完成，这就是 Deltagraph。如今，它的持有公司已经在新冠疫情的冲击下倒闭。据我所知，我做过的所有其他工作最终都被取代了，或者是雇主倒闭，总之成果消失在了历史的长河中。当然，有些可能仍被使用，这个我也不敢完全确定。&nbsp;所以我觉得虽然很多事情在做的时候看似无比重要，老板也总在催促要加班加点完成任务，但事后回头再看，这些辛苦编写的代码很可能几年之后就丧失了生命力！”</blockquote><p></p><p>&nbsp;</p><p>用户 spo81rtyOP 也非常认可 Watson&nbsp;的观点：“感谢你让我确定，有这种感觉的不单是我自己。我觉得大多数软件的实际使用寿命也就 5 到 10 年。之后，因为企业倒闭或者其他原因，软件被替代的可能性会非常高。即便它能幸存下来，完全由过时技术栈编写这一现实也会让它的路子变得很窄。这就是软件工程师的真实命运。”</p><p>&nbsp;</p><p>用户 com2kid 表示，他曾于 2008 年前后在微软工作，当时他看到过一个版权为 1994 年的头文件，里面还有作者姓名。搜索后发现，那位程序员已经在微软当上副总裁了。所以他认为，有些代码真的可以长青不老。</p><p>&nbsp;</p><p>用户 chesterriley 则想象了一个极端可能：也许未来终有一天，人们会继续使用 100 年前就编写出来的代码。最终的大赢家可能会是 Unix 实用程序或者 TCP/IP 代码之类，又或者是某些编译器、运行时引擎或解释器。还有来自 Linux 或 Windows 等操作系统的代码。人们可能突然发现，自己修复的错误居然诞生自 100 多年前。</p><p>&nbsp;</p><p>也有开发者认为，有些代码受到当前炒作趋势的影响很大，而 Web 开发应该就是其中最典型的代表了。考虑到过去二、三十年间 Web 开发领域发生的一系列根本性变化，这种情况也在情理之中。</p><p>&nbsp;</p><p>用户 Otis_Inf 拥有 28 年从业经历，他表示，他还记得网景（Netscape）发布背景图像的那一天，cgi 处理程序中的 Perl 脚本也曾经是常态。无论是当年还是现在，技术的发展速度都相当惊人，开发者必须适应新的做事方式——包括提交给 cgi 处理程序的静态 html 页面，也包括异步获取部分新元素来构成视图的客户端渲染页面。</p><p>&nbsp;</p><p>当然，也有些代码并没真正受到当今炒作的影响。有趣的是，这类代码大多集中在服务器端。虽然一直有强大的力量在“颠覆”微服务、Lambda 函数等服务构建方式，但如果忽略掉这些实现细节，那服务器的内存空间里肯定还有 db+ 服务在运行、也还有空闲周期没有利用起来。</p><p>&nbsp;</p><p>Otis_Inf 认为，IBM DB2 仍能运行 30 年前的 SQL 代码是有原因的，这个原因就是组织仍然依赖这些功能。或者说，根本就没有足够多的人把它“重写”成新代码。那这些代码是“烂代码”或者说“技术债”吗？还是得看具体情况。你家的锤子可能也用了十来年了，它过时了吗？如果还能干活，那就没过时。只有当代码确实需要变更，但却没人处理这项工作时，它才会真正沦为“烂代码”。</p><p>&nbsp;</p><p>“我希望看到当下诞生的新项目能始终牢记长期可维护性的重要意义，甚至把它当作一项基本设计前提。毕竟真的没多少人有能力维护陈旧软件项目。尽管地球人口仍在增加，但掌握足够技能来维护这些古早软件的开发者数量一直都跟不上。”</p><p></p><h2>国内技术从业者怎么看？</h2><p></p><p>&nbsp;</p><p>针对技术债问题，InfoQ 曾采访过国内一些技术从业者。</p><p>&nbsp;</p><p>百分点 CTO 刘译璟认为，判断技术债务的重点在于“哪些事情是应该做的”，它是一个因组织而异、因项目而异、因人而异的过程，例如以下一些方面：</p><p>&nbsp;</p><p>组织上要求做但没做的：制度、流程、规范、分享学习等；业务和技术上要求做但没有做的：功能、性能、安全、高可用、扩展、监控、辅助工具等。</p><p>&nbsp;</p><p>如果按照软件工程环节分类，技术债务可以分为：需求分析、方案设计、架构设计（逻辑架构、功能架构、数据架构、部署架构、运行架构等等）、编码、测试、发布等。如果按照产出物类型分，可以分为：</p><p>&nbsp;</p><p>文档类：管理过程文档、需求分析文档、设计文档、测试案例文档等；代码类：代码、脚本、规范等；软件包类：产品软件包、依赖软件、依赖资源等；环境类：开发环境、测试环境、预上线环境、生产环境等。</p><p>&nbsp;</p><p>至于如何决定要重写还是继续维护，需要判断“继续维护的收益”和“重写的收益”哪个更大，来决定继续维护还是重写。可以综合考虑如下几方面的收益：</p><p>&nbsp;</p><p>开源：提升现有业务收入、支持新业务的开拓；节流：节省维护人员、节省运营费用；组织：人员结构调整、组织能力培养。</p><p>&nbsp;</p><p>债务是避免不了的，时刻判断“持有债务的价值”，当价值很低时要尽快处理。</p><p>&nbsp;</p><p>腾讯研发总监王辉表示，如果人力、物力和工期等资源丰富，能去优化的就都可以做到极致。但通常，资源都是不丰富的，或者说是捉襟见肘的，那就要根据实际业务情况来看。腾讯一向的方式是“先抗住再优化”，项目是否真的到了非优化不可的地步，是否真的到了不优化随时都可能宕机的时候，如果先抗住了，就等业务占领了市场，站住了用户，到了项目进度慢下来之后，一些优化再开展起来，此时可以要求高可用、高性能、高并发等。</p><p>&nbsp;</p><p>“如果项目资源允许，一些稍微过度的优化和重构，个人认为是可以被接受的，保持团队的技术热情是不错的，但如果资源不允许，就要数着钱花，判断技术债务的合理性，如何更好的还债，是否真的到了非还不可，是否真的到了影响业务发展，需要与业务优先级一起看，业务错过一个时间窗就可能永远错过，有些技术债务还可以后期再还。”王辉总结道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://blog.visionarycto.com/p/my-20-year-career-is-technical-debt\">https://blog.visionarycto.com/p/my-20-year-career-is-technical-debt</a>\"</p><p><a href=\"https://www.infoq.cn/article/xgP9W*MC6Svi9Zcqd5KX\">https://www.infoq.cn/article/xgP9W*MC6Svi9Zcqd5KX</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=35955336\">https://news.ycombinator.com/item?id=35955336</a>\"</p><p><a href=\"https://www.reddit.com/r/programming/comments/13ihrtx/my_20_year_career_is_technical_debt_or_deprecated/\">https://www.reddit.com/r/programming/comments/13ihrtx/my_20_year_career_is_technical_debt_or_deprecated/</a>\"</p>",
    "publish_time": "2023-08-14 14:29:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI或于2024年底破产？大模型太烧钱了，快把OpenAI烧没了！",
    "url": "https://www.infoq.cn/article/datmEqVmS134ewFO7wel",
    "summary": "<p>作为让人工智能（AI）真正家喻户晓的传奇企业，OpenAI可能面临着新的严峻考验。根据《Analytics India Magazine》报道，如果Sam Altman执掌的这家公司继续以目前的速度烧钱，那么最快到2024年底就可能宣告破产。</p><p>&nbsp;</p><p>《Analytics India Magazine》指出，单是运行AI服务<a href=\"https://www.infoq.cn/article/bQHRqFcQ1TlJCqHuczGR\">ChatGPT</a>\"一项，每天就要花掉OpenAI公司近70万美元。尽管Altman一直努力推动GPT-3.5和GPT-4转为收益，但OpenAI仍远未能达到收支平衡，至于走向盈利更是天方夜谭。</p><p>&nbsp;</p><p>自去年发布之后，<a href=\"https://qcon.infoq.cn/202309/beijing/track/1560\">ChatGPT</a>\"已经成为有史以来增长最快的AI平台之一。但最近几个月，初期令人欣喜若狂的业绩数字已经停止增长，进一步削弱了该公司创造可观收入的期望。</p><p>&nbsp;</p><p>当ChatGPT6月份的用户数量较5月份有所下降时，有人猜测可能是因为学生放假，检索信息动作有所停滞才导致的下降。另一方面，也可能是因为自从该公司为用户发布 ChatGPT API 以来，人们开始构建自己的机器人，而不是使用原来的产品。&nbsp;</p><p>&nbsp;</p><p>根据外媒SimilarWeb介绍，与今年6月的17亿用户相比，ChatGPT在7月的用户数量环比下降12%、目前为15亿。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/de/dedf60d961bc6cc0d2c28bc159dbf7dd.png\" /></p><p></p><h2>开源大模型的兴起，让OpenAI处境更加艰难</h2><p></p><p>&nbsp;</p><p>X（原Twitter）上的一位用户表示，ChatGPT无法产生收入的一大原因就是API（应用程序编程接口）正蚕食其利润。虽然很多企业禁止员工使用ChatGPT处理工作，但人们正使用API在不同的工作流程中运用大语言模型（LLM）。而所谓API，代表的正是让一款程序向其他程序提供服务的结构化方式。</p><p>&nbsp;</p><p>OpenAI 认为用户数量下降只是因为人们开始使用 API 来构建自己的产品，但外界很多人却不这么认为。</p><p>&nbsp;</p><p>外界不少声音认为，OpenAI产品受欢迎程度下降是从开源大模型模型的兴起开始的。Meta 的 Llama 2 与微软合作，允许人们将该模型用于商业目的。&nbsp;</p><p>&nbsp;</p><p>那么，人们为什么不选择易于修改的 <a href=\"https://ai.meta.com/llama/\">Llama 2</a>\"，而去选择OpenAI提供的付费、专有和受限版本呢？可以说，与GPT相比，Llama 2在某些用例中也要好得多。人工智能开发人员Santiago表示，“我已经与两家初创公司进行了交谈，他们正在从专有模型迁移到 Llama 2。”</p><p>&nbsp;</p><p>另外，值得注意的是，尽管首席执行官 <a href=\"https://www.infoq.cn/article/iUfTz6eQa2k8KNtxlW4M\">Sam Altman</a>\" 并不拥有 OpenAI 的股权，但该公司很久以前就从非营利组织转变为盈利组织。尽管Altman可能并不关心利润，但作为一家盈利公司OpenAI不得不关心利润率。</p><p></p><h2>目前尚无可行的上市路线</h2><p></p><p>&nbsp;</p><p>报道还补充称，对于<a href=\"https://www.infoq.cn/article/SsGgljWxPa8VsAIElksJ\">OpenAI</a>\"、Anthropic或者Inflection等任何领先的AI厂商来说，通过首轮公开募股（IPO）从股市上寻求资金注入仍然为时过早。Investopedia发布的报道指出，任何成功的IPO都至少需要保持10年运营、再加上1亿美元的收入作为前提。</p><p>&nbsp;</p><p>OpenAI之所以眼下还没有“爆雷”，主要得益于微软慷慨拿出的100亿美元投资。然而，考虑到用户规模的直线下降，该公司提出的2023年预计收入2亿美元、2024年预计收入10亿美元的目标已经变得过于乐观，甚至可以断言其亏损态势只会进一步恶化。而且自从推出ChatGPT聊天机器人以来，OpenAI的财政状况其实越来越差，今年5月的亏损额已经翻番达5.4亿美元。</p><p>&nbsp;</p><p>目前市场上缺乏充足的企业级GPU（图形处理单元）供应，大国之间的<a href=\"https://qcon.infoq.cn/202309/beijing/track/1562\">技术战</a>\"也加剧了OpenAI面临的困境。Altman曾多次提到，GPU资源的稀缺已经在阻碍该公司训练更多新模型。</p><p>&nbsp;</p><p>但OpenAI也并非就“无药可救”。OpenAI向付费版本的转变可能为他们带来了大量的金钱。预期收入的预测可能来自购买 API 和使用基于 GPT-4 的聊天机器人或其他产品（例如 DALL-E2）的人们。但这方面的财务数据仍然模糊。</p><p>&nbsp;</p><p>即便如此，如果这家以大模型为重点的公司进行首次公开​​募股，它可能会被更大的公司收购。至于员工，虽然很多人可能会离开OpenAI去加入竞争对手，但该公司仍在以丰厚的薪水招聘人员，甚至将办事处扩展到了伦敦。</p><p></p><h2>大模型太烧钱了！</h2><p></p><p>&nbsp;</p><p>众所周知，训练大模型所需的计算资源是非常巨大的，并且在大型模型上运行推理的计算成本也会很高。大模型需要专门的硬件和软件才能有效运行。企业可能需要投资昂贵的基础设施来处理大型语言模型的处理要求。有专家估计，GPT3 的培训成本可能在 400 万美元到 1000 万美元之间，甚至更多。</p><p>&nbsp;</p><p>训练人工智能模型需要大量数据，并且标记这些数据可能既耗时又昂贵。此外，获取数据的成本也越来越高。例如，一项研究发现，为机器学习创建高质量数据集的成本从每项任务1美元到100美元不等，具体取决于任务的复杂性和所需的专业知识水平。斯坦福大学的另一项研究发现，为深度学习标记单个图像数据集的成本可能高达每张图像 3.50 美元。OpenAI 的 GPT-3 训练数据集大小为 45 TB。这些数字表明，训练人工智能模型的成本（包括数据采集和标记）是一项重大投资。</p><p>&nbsp;</p><p>更重要的是，想要训练一款大模型，需要付出的成本并不仅限于硬件和数据。训练、微调和部署大型语言模型需要高度专业的技能，而组织的员工可能不具备这些技能。雇用这样的人可能会很昂贵，并进一步增加整体研发成本。此外，近年来，对人工智能和机器学习专业人才的需求大幅上升，导致市场上此类人才短缺。结果就是雇用和留住这些人才的成本大幅增加。根据LinkedIn的一份报告，人工智能专家职位是最热门的新兴职位之一，在美国每年的平均基本工资为13.6万美元。同一份报告还发现，过去四年对人工智能专家的需求每年增长 74%。</p><p>&nbsp;</p><p>据微软第二季度财报显示，<a href=\"https://www.infoq.cn/article/ljYBv7FS7UCtPpIlkMdx\">微软</a>\"在今年该季度的资本支出（主要用于购买硬件和软件）超过了107亿美元，较去年同期相比增长了23％，是公司历史上最高的季度资本支出。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/9wzrGiMTl8XClRgSfL9A\">微软</a>\"首席财务官Amy Hood告诉投资者，这个数字在接下来的四个季度中会进一步攀升。此外，谷歌首席财务官Ruth Porat也释放出了同样的信号，Porat称公司的资本支出将在2023年下半年和2024年增加，主要是用于购买服务器和AI研发上。</p><p>&nbsp;</p><p>财力雄厚如微软、谷歌，也被大模型“削掉了一层皮”，更别提OpenAI了。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/1UyH2okZUKWlbwby3dQV\">GPU </a>\"短缺更是让OpenAI雪上加霜。Altman 曾表示，市场上 GPU 的短缺导致该公司无法训练进一步的模型并对其进行改进。现在该公司已经在“ GPT-5&nbsp;”上申请了商标，很明显该公司想要对其进行培训。这反过来又导致 ChatGPT 输出质量的大幅下降。</p><p>&nbsp;</p><p>因此，如果 OpenAI 不能很快获得更多资金，该公司可能不得不在 2024 年底之前申请破产。到那时，我们将面对的局面可能是：大模型市场竞争越来越激烈，损失越来越大，用户越来越少，诉讼越来越多，大模型质量越来越差。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.wionews.com/science/openai-may-go-bankrupt-by-end-of-2024-costs-700k-to-operate-chatgpt-daily-report-625016\">https://www.wionews.com/science/openai-may-go-bankrupt-by-end-of-2024-costs-700k-to-operate-chatgpt-daily-report-625016</a>\"</p><p></p><p><a href=\"https://analyticsindiamag.com/openai-might-go-bankrupt-by-the-end-of-2024/\">https://analyticsindiamag.com/openai-might-go-bankrupt-by-the-end-of-2024/</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-08-14 15:24:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "美团前端技术专家张舒迪，确认担任QCon北京大前端融合提效专题出品人",
    "url": "https://www.infoq.cn/article/FqC2B7lRaBzYaeKYOAze",
    "summary": "<p>9 月 3 日 - 5 日，在 <a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=teacheart&amp;utm_campaign=9&amp;utm_term=0814&amp;utm_content=zhangshudi\">QCon 全球软件开发大会（北京站）</a>\"，美团前端技术专家张舒迪将担任「大前端融合提效」的专题出品人。在此次专题中，你将了解到新时期涌现出的优秀框架工具以及开发模式。目前专题已确认以下演讲：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c34f4629dc23bf9223239a808595652.png\" /></p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/track/1564?utm_source=infoqweb&amp;utm_medium=teacheart&amp;utm_campaign=9&amp;utm_term=0814&amp;utm_content=zhangshudi\">张舒迪</a>\"，美团前端技术专家，致力于移动前端、中后台技术基建及在本地生活场景的业务落地，着力推进前端标准化、配置化、智能化相关方向建设，沉淀平台及系统配套能力。</p><p></p><p>相信张舒迪的到来，可以帮助提升此专题的质量，让你了解到新时期对大前端研发效率提出了新的挑战，以及本时期涌现出的优秀框架工具以及开发模式。</p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-14 16:32:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris 2.0.0 版本正式发布：盲测性能 10 倍提升，更统一多样的极速分析体验",
    "url": "https://www.infoq.cn/article/ViPk0dz7sSsBLwkVijoH",
    "summary": "<p><a href=\"http://doris.apache.org/\">Apache Doris</a>\" 2.0.0 版本已于 2023 年 8 月 11 日正式发布，有超过 275 位贡献者为 Apache Doris 提交了超过 4100 个优化与修复。</p><p></p><p>在 2.0.0 版本中，Apache Doris 在标准 Benchmark 数据集上盲测查询性能得到超过 10 倍的提升、在日志分析和数据湖联邦分析场景能力得到全面加强、数据更新效率和写入效率都更加高效稳定、支持了更加完善的多租户和资源隔离机制、在资源弹性与存算分离方向踏上了新的台阶、增加了一系列面向企业用户的易用性特性。在经过近半年的开发、测试与稳定性调优后，这一版本已经正式稳定可用，欢迎大家下载使用。</p><p></p><p>GitHub下载：</p><p></p><p>https://github.com/apache/doris/releases/tag/2.0.0-rc04</p><p></p><p>官网下载页：</p><p></p><p>https://doris.apache.org/download</p><p></p><p>Release Note：</p><p></p><p>https://github.com/apache/doris/issues/22647</p><p></p><h1>盲测性能 10 倍以上提升</h1><p></p><p>在 Apache Doris 2.0.0 版本中，我们引入了全新查询优化器和自适应的并行执行模型，结合存储层、执行层以及执行算子上的一系列性能优化手段，实现了盲测性能 10 倍以上的提升。以 SSB-Flat 和 TPC-H 标准测试数据集为例，在相同的集群和机器配置下，新版本宽表场景盲测较之前版本性能提升 10 倍、多表关联场景盲测提升了 13 倍，实现了巨大的性能飞跃。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dd/dd5b37a088766f52662b5a982e62320c.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc6f205c6c52c66f947e0abc9285b220.png\" /></p><p></p><h2>01  更智能的全新查询优化器</h2><p></p><p>全新查询优化器采取了更先进的 Cascades 框架、使用了更丰富的统计信息、实现了更智能化的自适应调优，在绝大多数场景无需任何调优和 SQL 改写即可实现极致的查询性能，同时对复杂 SQL 支持得更加完备、可完整支持 TPC-DS 全部 99 个 SQL。通过全新查询优化器，我们可以胜任更多真实业务场景的挑战，减少因人工调优带来的人力消耗，真正助力业务提效。</p><p></p><p>以 TPC-H 为例，全新优化器在未进行任何手工调优和 SQL 改写的情况下，绝大多数 SQL 仍领先于旧优化器手工调优后的性能表现！而在超过百家 2.0 版本提前体验用户的真实业务场景中，绝大多数原始 SQL 执行效率得以极大提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c7ff9bede1bccc63ed2bee7a2b4a8cd0.png\" /></p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/query-acceleration/nereids</p><p></p><p>如何开启：SET enable_nereids_planner=true 在 Apache Doris 2.0.0 版本中全新查询优化器已经默认开启</p><p></p><h2>02  倒排索引支持</h2><p></p><p>在 2.0.0 版本中我们对现有的索引结构进行了丰富，引入了倒排索引来应对多维度快速检索的需求，在关键字模糊查询、等值查询和范围查询等场景中均取得了显著的查询性能和并发能力提升。</p><p></p><p>在此以某头部手机厂商的用户行为分析场景为例，在之前的版本中，随着并发量的上升、查询耗时逐步提升，性能下降趋势比较明显。而在 2.0.0 版本开启倒排索引后，随着并发量的提升查询性能始终保持在毫秒级。在同等查询并发量的情况下，2.0.0 版本在该用户行为分析场景中并发查询性能提升了 5-90 倍！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5a/5a44bb10202eede7fdbd7ca7b747c88e.png\" /></p><p></p><h2>03  点查询并发能力提升 20 倍</h2><p></p><p>在银行交易流水单号查询、保险代理人保单查询、电商历史订单查询、快递运单号查询等 Data Serving 场景，会面临大量一线业务人员及 C 端用户基于主键 ID 检索整行数据的需求，同时在用户画像、实时风控等场景中还会面对机器大规模的程序化查询，在过去此类需求往往需要引入 Apache HBase 等 KV 系统来应对点查询、Redis 作为缓存层来分担高并发带来的系统压力。</p><p></p><p>对于基于列式存储引擎构建的 Apache Doris 而言，此类的点查询在数百列宽表上将会放大随机读取 IO，并且查询优化器和执行引擎对于此类简单 SQL 的解析、分发也将带来不必要的额外开销，负责 SQL 解析的 FE 模块往往会成为限制并发的瓶颈，因此需要更高效简洁的执行方式。</p><p></p><p>在 Apache Doris 2.0.0 版本，我们引入了全新的行列混合存储以及行级 Cache，使得单次读取整行数据时效率更高、大大减少磁盘访问次数，同时引入了点查询短路径优化、跳过执行引擎并直接使用快速高效的读路径来检索所需的数据，并引入了预处理语句复用执行 SQL 解析来减少 FE 开销。</p><p></p><p>通过以上一系列优化，Apache Doris 2.0.0 版本在并发能力上实现了数量级的提升，实现了单节点 30000 QPS 的并发表现，较过去版本点查询并发能力提升超 20 倍！</p><p></p><p>基于以上能力，Apache Doris 可以更好应对高并发数据服务场景的需求，替代 HBase 在此类场景中的能力，减少复杂技术栈带来的维护成本以及数据的冗余存储。</p><p></p><p>参考文章：<a href=\"http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247523354&amp;idx=1&amp;sn=c9d0ed3fad3c351400e083e7d0afeacc&amp;chksm=cf2f921df8581b0b14ca03b19de9bcffdde5a87cbf393e6d346814baf92b148d86349cd7a6ec&amp;scene=21#wechat_redirect\">Apache Doris 2.0 高并发特性解读，查询并发提升 20 倍、单节点数万 QPS</a>\"</p><p></p><h2>04  自适应的并行执行模型</h2><p></p><p>在实现极速分析体验的同时，为了保证多个混合分析负载的执行效率以及查询的稳定性，在 2.0.0 版本中我们引入了 Pipeline 执行模型作为查询执行引擎。在 Pipeline 执行引擎中，查询的执行是由数据来驱动控制流变化的，各个查询执行过程之中的阻塞算子被拆分成不同 Pipeline，各个 Pipeline 能否获取执行线程调度执行取决于前置数据是否就绪，实现了阻塞操作的异步化、可以更加灵活地管理系统资源，同时减少了线程频繁创建和销毁带来的开销，并提升了 Apache Doris 对于 CPU 的利用效率。因此 Apache Doris 在混合负载场景中的查询性能和稳定性都得到了全面提升。参考文档：https://doris.apache.org/zh-CN/docs/dev/query-acceleration/pipeline-execution-engine</p><p></p><p>如何开启 Set enable_pipeline_engine = true</p><p></p><p>该功能在 Apache Doris 2.0.0 版本中将默认开启，BE 在进行查询执行时默认将 SQL 的执行模型转变 Pipeline 的执行方式。parallel_pipeline_task_num 代表了 SQL 查询进行查询并发的 Pipeline Task 数目。Apache Doris 默认配置为0，此时 Apache Doris 会自动感知每个 BE 的 CPU 核数并把并发度设置为 CPU 核数的一半，用户也可以实际根据自己的实际情况进行调整。对于从老版本升级的用户，会自动将该参数设置成老版本中 parallel_fragment_exec_instance_num 的值。</p><p></p><h1>更统一多样的分析场景</h1><p></p><p>作为最初诞生于报表分析场景的 OLAP 系统，Apache Doris 在这一擅长领域中做到了极致，凭借自身优异的分析性能和极简的使用体验收获到了众多用户的认可，在诸如实时看板（Dashboard）、实时大屏、业务报表、管理驾驶舱等实时报表场景以及自助 BI 平台、用户行为分析等即席查询场景获得了极为广泛的运用。</p><p></p><p>而随着用户规模的极速扩张，越来越多用户开始希望通过 Apache Doris 来简化现有的繁重大数据技术栈，减少多套系统带来的使用及运维成本。因此 Apache Doris 也在不断拓展应用场景的边界，从过去的实时报表和 Ad-hoc 等典型 OLAP 场景到湖仓一体、ELT/ETL、日志检索与分析、高并发 Data Serving 等更多业务场景，而日志检索分析、湖仓一体也是我们在 Apache Doris 最新版本中的重要突破。</p><p></p><h2>01  10倍以上性价比的日志检索分析平台</h2><p></p><p>在 Apache Doris 2.0.0 版本中，我们提供了原生的半结构化数据支持，在已有的 JSON、Array 基础之上增加了复杂类型 Map，并基于 Light Schema Change 功能实现了 Schema Evolution。与此同时，2.0.0 版本新引入的倒排索引和高性能文本分析算法全面加强了 Apache Doris 在日志检索分析场景的能力，可以支持更高效的任意维度分析和全文检索。结合过去在大规模数据写入和低成本存储等方面的优势，相对于业内常见的日志分析解决方案，<a href=\"http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247522956&amp;idx=1&amp;sn=67403537867eafaae6c02a7c7c5073c6&amp;chksm=cf2f948bf8581d9dd028b6b7e48609f74689a57fd846cb46432fcdf3397a5d5caa2afedc070b&amp;scene=21#wechat_redirect\">基于 Apache Doris 构建的新一代日志检索分析平台</a>\"实现了 10 倍以上的性价比提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d3e866ad271475ce25477bdb0dc0a09.png\" /></p><p></p><h2>02  湖仓一体</h2><p></p><p>在 Apache Doris 1.2 版本中，我们引入了 Multi-Catalog 功能，支持了多种异构数据源的元数据自动映射与同步，实现了便捷的元数据和数据打通。在 2.0.0 版本中，我们进一步对数据联邦分析能力进行了加强，引入了更多数据源，并针对用户的实际生产环境做了诸多性能优化，在真实工作负载情况下查询性能得到大幅提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/1299c5556db37bc5fdc7b96c7a45d36d.png\" /></p><p></p><p>在数据源方面，Apache Doris 2.0.0 版本支持了 Hudi Copy-on-Write 表的 Snapshot Query 以及 Merge-on-Read 表的 Read Optimized Query，截止目前已经支持了 Hive、Hudi、Iceberg、Paimon、MaxCompute、Elasticsearch、Trino、ClickHouse 等数十种数据源，几乎支持了所有开放湖仓格式和 Metastore。同时还支持通过 Apache Range 对 Hive Catalog 进行鉴权，可以无缝对接用户现有的权限系统。同时还支持可扩展的鉴权插件，为任意 Catalog 实现自定义的鉴权方式。</p><p></p><p>在性能方面，利用 Apache Doris 自身高效的分布式执行框架、向量化执行引擎以及查询优化器，结合 2.0 版本中对于小文件和宽表的读取优化、本地文件 Cache、ORC/Parquet 文件读取效率优化、弹性计算节点以及外表的统计信息收集，Apaceh Doris 在 TPC-H 场景下查询 Hive 外部表相较于 Presto/Trino 性能提升 3-5 倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a22a6eb5ffa594127e84a22ca5505f40.png\" /></p><p></p><p>通过这一系列优化，Apache Doris 湖仓一体的能力得到极大拓展，在如下场景可以更好发挥其优异的分析能力：</p><p></p><p>湖仓查询加速：为数据湖、Elasticsearch 以及各类关系型数据库提供优秀的查询加速能力，相比 Hive、Presto、Spark 等查询引擎实现数倍的性能提升。数据导入与集成：基于可扩展的连接框架，增强 Apache Doris 在数据集成方面的能力，让数据更便捷的被消费和处理。用户可以通过 Apache Doris 对上游的多种数据源进行统一的增量、全量同步，并利用 Apache Doris 的数据处理能力对数据进行加工和展示，也可以将加工后的数据写回到数据源，或提供给下游系统进行消费。统一数据分析网关：利用 Apache Doris 构建完善可扩展的数据源连接框架，便于快速接入多类数据源。提供基于各种异构数据源的快速查询和写入能力，将 Apache Doris 打造成统一的数据分析网关。</p><p></p><h1>高效的数据更新</h1><p></p><p>在实时分析场景中，数据更新是非常普遍的需求。用户不仅希望能够实时查询最新数据，也希望能够对数据进行灵活的实时更新。典型场景如电商订单分析、物流运单分析、用户画像等，需要支持数据更新类型包括整行更新、部分列更新、按条件进行批量更新或删除以及整表或者整个分区的重写（inser overwrite）。</p><p></p><p>高效的数据更新一直是大数据分析领域的痛点，离线数据仓库 Hive 通常只支持分区级别的数据更新，而 Hudi 和 Iceberg 等数据湖，虽然支持 Record 级别更新，但是通常采用 Merge-on-Read 或 Copy-on-Write 的方式，仅适合低频批量更新而不适合实时高频更新。</p><p></p><p>在 Apache Doris 1.2 版本，我们在 Unique Key 主键模型实现了 Merge-on-Write 的数据更新模式，数据在写入阶段就能完成所有的数据合并工作，因此查询性能得到 5-10 倍的提升。在 Apache Doris 2.0 版本我们进一步加强了数据更新能力，主要包括：</p><p></p><p>对写入性能进行了大幅优化，高并发写入和混合负载写入场景的稳定性也显著提升。例如在单 Tablet 7GB 的重复导入测试中，数据导入的耗时从约 30 分钟缩短到了 90s，写入效率提升 20 倍；以某头部支付产品的场景压测为例，在 20 个并行写入任务下可以达到 30 万条每秒的写入吞吐，并且持续写入十几个小时后仍然表现非常稳定。支持部分列更新功能。在 2.0.0 版本之前 Apache Doris 仅支持通过 Aggregate Key 聚合模型的 Replace_if_not_null 进行部分列更新，在 2.0.0 版本中我们增加了 Unique Key 主键模型的部分列更新，在多张上游源表同时写入一张宽表时，无需由 Flink 进行多流 Join 打宽，直接写入宽表即可，减少了计算资源的消耗并大幅降低了数据处理链路的复杂性。同时在面对画像场景的实时标签列更新、订单场景的状态更新时，直接更新指定的列即可，较过去更为便捷。支持复杂条件更新和条件删除。在 2.0.0 版本之前 Unique Key 主键模型仅支持简单 Update 和 Delete 操作，在 2.0.0 版本中我们基于 Merge-on-Write 实现了复杂条件的数据更新和删除，并且执行效率更加高效。基于以上优化，Apache Doris 对于各类数据更新需求都有完备的能力支持！</p><p></p><h1>更加高效稳定的数据写入</h1><p></p><p></p><h2>01  导入性能进一步提升</h2><p></p><p>聚焦于实时分析，我们在过去的几个版本中在不断增强实时分析能力，其中端到端的数据实时写入能力是优化的重要方向，在 Apache Doris 2.0 版本中，我们进一步强化了这一能力。通过 Memtable 不使用 Skiplist、并行下刷、单副本导入等优化，使得导入性能有了大幅提升：</p><p></p><p>使用 Stream Load 对 TPC-H 144G lineitem 表原始数据进行三副本导入 48 buckets Duplicate 表，吞吐量提升 100%。使用 Stream Load 对 TPC-H 144G lineitem 表原始数据进行三副本导入 48 buckets Unique Key 表，吞吐量提升 200%。使用 insert into select 对 TPC-H 144G lineitem 表进行导入 48 buckets Duplicate 表，吞吐量提升 50%。使用 insert into select 对 TPC-H 144G lineitem 表进行导入 48 buckets Unique Key 表，吞吐提升 150%。</p><p></p><h2>02  数据高频写入更稳定</h2><p></p><p>在高频数据写入过程中，小文件合并和写放大问题以及随之而来的磁盘 I/O和 CPU 资源开销是制约系统稳定性的关键，因此在 2.0 版本中我们引入了 Vertical Compaction 以及 Segment Compaction，用以彻底解决 Compaction 内存问题以及写入过程中的 Segment 文件过多问题，资源消耗降低 90%，速度提升 50%，内存占用仅为原先的 10% 。</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A</p><p></p><h2>03  数据表结构自动同步</h2><p></p><p>在过去版本中我们引入了毫秒级别的 Schema Change，而在最新版本 Flink-Doris-Connector 中，我们实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。在实际测试中单个同步任务可以承载数千张表的实时并行写入，从此彻底告别过去繁琐复杂的同步流程，通过简单命令即可实现上游业务数据库的表结构及数据同步。同时当上游数据结构发生变更时，也可以自动捕获 Schema 变更并将 DDL 动态同步到 Doris 中，保证业务的无缝运行。</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/Ur4VpJtjByVL0qQNy_iQBw</p><p></p><h1>更加完善的多租户资源隔离</h1><p></p><p>多租户与资源隔离的主要目的是为了保证高负载时避免相互发生资源抢占，Apache Doris 在过去版本中推出了资源组（Resource Group）的硬隔离方案，通过对同一个集群内部的 BE 打上标签，标签相同的 BE 会组成一个资源组。数据入库时会按照资源组配置将数据副本写入到不同的资源组中，查询时按照资源组的划分使用对应资源组上的计算资源进行计算，例如将读、写流量放在不同的副本上从而实现读写分离，或者将在线与离线业务划分在不同的资源组、避免在离线分析任务之间的资源抢占。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/40/40a7a7355896b7e53dec58536cae2392.png\" /></p><p></p><p>资源组这一硬隔离方案可以有效避免多业务间的资源抢占，但在实际业务场景中可能会存在某些资源组紧张而某些资源组空闲的情况发生，这时需要有更加灵活的方式进行空闲资源的共享，以降低资源空置率。因此在 2.0.0 版本中我们增加了 Workload Group 资源软限制的方案，通过对 Workload 进行分组管理，以保证内存和 CPU 资源的灵活调配和管控。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df786c0c583fec3e050eb7c3a9f2198e.png\" /></p><p></p><p>通过将 Query 与 Workload Group 相关联，可以限制单个 Query 在 BE 节点上的 CPU 和内存资源的百分比，并可以配置开启资源组的内存软限制。当集群资源紧张时，将自动 Kill 组内占用内存最大的若干个查询任务以减缓集群压力。当集群资源空闲时，一旦 Workload Group 使用资源超过预设值时，多个 Workload 将共享集群可用空闲资源并自动突破阈值，继续使用系统内存以保证查询任务的稳定执行。Workload Group 还支持设置优先级，通过预先设置的优先级进行资源分配管理，来确定哪些任务可正常获得资源，哪些任务只能获取少量或没有资源。</p><p></p><p>与此同时，在 Workload Group 中我们还引入了查询排队的功能，在创建 Workload Group 时可以设置最大查询数，超出最大并发的查询将会进行队列中等待执行，以此来缓解高负载下系统的压力。</p><p></p><h1>极致弹性与存算分离</h1><p></p><p>过去 Apache Doris 凭借在易用性方面的诸多设计帮助用户大幅节约了计算与存储资源成本，而面向未来的云原生架构，我们已经走出了坚实的一步。</p><p></p><p>从降本增效的趋势出发，用户对于计算和存储资源的需求可以概括为以下几方面：</p><p></p><p>计算资源弹性：面对业务计算高峰时可以快速进行资源扩展提升效率，在计算低谷时可以快速缩容以降低成本；存储成本更低：面对海量数据可以引入更为廉价的存储介质以降低成本，同时存储与计算单独设置、相互不干预；业务负载隔离：不同的业务负载可以使用独立的计算资源，避免相互资源抢占；数据管控统一：统一 Catalog、统一管理数据，可以更加便捷地分析数据。</p><p></p><p>存算一体的架构在弹性需求不强的场景具有简单和易于维护的优势，但是在弹性需求较强的场景有一定的局限。而存算分离的架构本质是解决资源弹性的技术手段，在资源弹性方面有着更为明显的优势，但对于存储具有更高的稳定性要求，而存储的稳定性又会进一步影响到 OLAP 的稳定性以及业务的存续性，因此也引入了 Cache 管理、计算资源管理、垃圾数据回收等一系列机制。</p><p></p><p>而在与 Apache Doris 社区广大用户的交流中，我们发现用户对于存算分离的需求可以分为以下三类：</p><p></p><p>目前选择简单易用的存算一体架构，暂时没有资源弹性的需求；欠缺稳定的大规模存储，要求在 Apache Doris 原有基础上提供弹性、负载隔离以及低成本；有稳定的大规模存储，要求极致弹性架构、解决资源快速伸缩的问题，因此也需要更为彻底的存算分离架构；</p><p></p><p>为了满足前两类用户的需求，Apache Doris 2.0 版本中提供了可以兼容升级的存算分离方案：</p><p></p><p>第一种，计算节点。2.0 版本中我们引入了无状态的计算节点 Compute Node，专门用于数据湖分析。相对于原本存储计算一体的混合节点，Compute Node 不保存任何数据，在集群扩缩容时无需进行数据分片的负载均衡，因此在数据湖分析这种具有明显高峰的场景中可以灵活扩容、快速加入集群分摊计算压力。同时由于用户数据往往存储在 HDFS/S3 等远端存储中，执行查询时查询任务会优先调度到 Compute Node 执行，以避免内表与外表查询之间的计算资源抢占。</p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/advanced/compute_node</p><p></p><p>第二种，冷热数据分层。在存储方面，冷热数据往往面临不同频次的查询和响应速度要求，因此通常可以将冷数据存储在成本更低的存储介质中。在过去版本中 Apache Doris 支持对表分区进行生命周期管理，通过后台任务将热数据从 SSD 自动冷却到 HDD，但 HDD 上的数据是以多副本的方式存储的，并没有做到最大程度的成本节约，因此对于冷数据存储成本仍然有较大的优化空间。在 Apache Doris 2.0 版本中推出了<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247521087&amp;idx=1&amp;sn=b8b36da7bc4a0fe4145d691816ce672d&amp;scene=21#wechat_redirect\">冷热数据分层功能</a>\"，冷热数据分层功能使 Apache Doris 可以将冷数据下沉到存储成本更加低廉的对象存储中，同时冷数据在对象存储上的保存方式也从多副本变为单副本，存储成本进一步降至原先的三分之一，同时也减少了因存储附加的计算资源成本和网络开销成本。通过实际测算，存储成本最高可以降低超过 70%！</p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/advanced/cold_hot_separation</p><p></p><p>面对更加彻底的存储计算分离需求，<a href=\"https://selectdb.com/\">飞轮科技（SelectDB）</a>\"技术团队设计并实现了全新的云原生存算分离架构（SelectDB Cloud），近一年来经历了大量企业客户的大规模使用，在性能、功能成熟度、系统稳定性等方面经受了真实生产环境的考验。在 Apache Doris 2.0.0 版本发布之际，<a href=\"http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247523004&amp;idx=1&amp;sn=ac29830fb959ea2c62859f8c35f484b6&amp;chksm=cf2f94bbf8581dadd97b584552f06c00bc14667198d83707c03245fbca65c4a92c0c96e614f3&amp;scene=21#wechat_redirect\">飞轮科技宣布将这一经过大规模打磨后的成熟架构贡献至 Apache Doris 社区</a>\"。这一工作预计将于 2023 年 10 月前后完成，届时全部存算分离的代码都将会提交到 Apache Doris 社区主干分支中，预计在 9 月广大社区用户就可以提前体验到基于存算分离架构的预览版本。</p><p></p><h1>易用性进一步提升</h1><p></p><p>除了以上功能需求外，在 Apache Doris 还增加了许多面向企业级特性的体验改进。</p><p></p><h2>01  支持 Kubernetes 容器化部署</h2><p></p><p>在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。</p><p></p><h2>02  跨集群数据复制</h2><p></p><p>在 Apache Doris 2.0.0 版本中，我们可以通过 CCR 的功能在库/表级别将源集群的数据变更同步到目标集群，可根据场景精细控制同步范围；用户也可以根据需求灵活选择全量或者增量同步，有效提升了数据同步的灵活性和效率；此外 Dors CCR 还支持 DDL 同步，源集群执行的 DDL 语句可以自动同步到目标集群，从而保证了数据的一致性。Doris CCR 配置和使用也非常简单，简单操作即可快速完成跨集群数据复制。基于 Doris CCR 优异的能力，可以更好实现读写负载分离以及多机房备份，并可以更好支持不同场景的跨集群复制需求。</p><p></p><h2>03  其他升级注意事项</h2><p></p><p>1.2-lts 需要停机升级到 2.0.0，2.0-alpha 需要停机升级到 2.0.0查询优化器开关默认开启 enable_nereids_planner=true ；系统中移除了非向量化代码，所以 enable_vectorized_engine 参数将不再生效；新增参数 enable_single_replica_compaction ；默认使用 datev2, datetimev2, decimalv3 来创建表，不支持 datev1，datetimev1， decimalv2 创建表；在 JDBC 和 Iceberg Catalog 中默认使用decimalv3；date type 新增 AGG_STATE；backend 表去掉 cluster 列；为了与 BI 工具更好兼容，在 show create table 时，将 datev2 和 datetimev2 显示为 date 和 datetime。在 BE 启动脚本中增加了 max_openfiles 和 swap 的检查，所以如果系统配置不合理，be 有可能会启动失败；禁止在 localhost 访问 FE 时无密码登录；当系统中存在 Multi-Catalog 时，查询 information schema 的数据默认只显示 internal catalog 的数据；限制了表达式树的深度，默认为 200；array string 返回值 单引号变双引号；对 Doris 的进程名重命名为 DorisFE 和 DorisBE；</p><p></p><h1>正式踏上 2.0 之旅</h1><p></p><p>在 Apache Doris 2.0.0 版本发布过程中，我们邀请了数百家企业参与新版本的打磨，力求为所有用户提供性能更佳、稳定性更高、易用性更好的数据分析体验。后续我们将会持续敏捷发版来响应所有用户对功能和稳定性的更高追求，预计 2.0 系列的第一个迭代版本 2.0.1 将于 8 月下旬发布，9 月会进一步发布 2.0.2 版本。在快速 Bugfix 的同时，也会不断将一些最新特性加入到新版本中。9 月份我们还将发布 2.1 版本的尝鲜版本，会增加一系列呼声已久的新能力，包括 Variant 可变数据类型更好满足半结构化数据 Schema Free 的分析需求，多表物化视图在实现查询加速的同时也能简化数据调度和处理链路，在导入性能方面持续优化、增加新的更加简洁的数据导入方式，通过自动攒批实现更加实时的数据写入，复合数据类型的嵌套能力等。</p><p></p><p>期待 Apache Doris 2.0 版本的正式发布为更多社区用户提供实时统一的分析体验，我们也相信 Apache Doris 2.0 版本会成为您在实时分析场景中的最理想选择。</p><p></p><p>最后，Apache Doris 社区年度峰会 Doris Summit 2023 已经启程，我们期待将有更多关于 2.0 版本的最佳应用实践在 Doris Summit 2023 被所有用户所看到。</p>",
    "publish_time": "2023-08-14 16:41:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "解读 Linux 内存管理新特性 Memory folios",
    "url": "https://www.infoq.cn/article/kCRXZhKLOZ9lYJzaasZ0",
    "summary": "<p></p><blockquote>本文内容基于 Linux 5.16，folio 基础部分开始合入。截止到 Linux 6.5，folio 已经有很大进展，会在后续文章中介绍。</blockquote><p></p><p></p><h2>folio 技术特性</h2><p></p><p></p><p>引用 LWN:：Memory folios ：<a href=\"https://lwn.net/Articles/856016/\">https://lwn.net/Articles/856016/</a>\"&nbsp;和 Merge tag 'folio-5.16'：<a href=\"https://github.com/torvalds/linux/commit/49f8275c7d92\">https://github.com/torvalds/linux/commit/49f8275c7d92</a>\"。</p><p></p><h3>如何理解 folio？</h3><p></p><p></p><p></p><blockquote>Add memory folios, a new type to represent either order-0 pages or the head page of a compound page.folio 可以看成是 page 的一层包装，没有开销的那种。folio 可以是单个页，也可以是复合页。</blockquote><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1a/1a6ba737f891e9c0af25bb0e9bd50fc4.png\" /></p><p>（图片引用<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg2OTc0ODAzMw==&amp;mid=2247502699&amp;idx=1&amp;sn=595089e64e8fc6a910bfbe0cb398db49&amp;source=41&amp;scene=21#wechat_redirect\">围绕 HugeTLB 的极致优化</a>\"）</p><p></p><p>上图是 page 结构体的示意图，64 字节管理 flags, lru, mapping, index, private, {ref_, map_}count, memcg_data 等信息。当 page 是复合页的时候，上述 flags 等信息在 head page 中，tail page 则复用管理 compound_{head, mapcount, order, nr, dtor} 等信息。</p><p></p><p><code lang=\"plain\">struct folio {\n        /* private: don't document the anon union */\n        union {\n                struct {\n        /* public: */\n                        unsigned long flags;\n                        struct list_head lru;\n                        struct address_space *mapping;\n                        pgoff_t index;\n                        void *private;\n                        atomic_t _mapcount;\n                        atomic_t _refcount;\n#ifdef CONFIG_MEMCG\n                        unsigned long memcg_data;\n#endif\n        /* private: the union with struct page is transitional */\n                };\n                struct page page;\n        };\n};</code></p><p></p><p>folio 的结构定义中，flags, lru 等信息和 page 完全一致，因此可以和 page 进行 union。这样可以直接使用&nbsp;folio-&gt;flags&nbsp;&nbsp;而不用&nbsp;folio-&gt;page-&gt;flags。</p><p></p><p><code lang=\"plain\">#define page_folio(p)           (_Generic((p),                          \\\n        const struct page *:    (const struct folio *)_compound_head(p), \\\n        struct page *:          (struct folio *)_compound_head(p)))\n\n#define nth_page(page,n) ((page) + (n))\n#define folio_page(folio, n)    nth_page(&amp;(folio)-&gt;page, n)</code></p><p></p><p>第一眼看&nbsp;page_folio&nbsp;可能有点懵，其实等效于：</p><p></p><p><code lang=\"plain\">switch (typeof(p)) {\n  case const struct page *:\n    return (const struct folio *)_compound_head(p);\n  case struct page *:\n    return (struct folio *)_compound_head(p)));\n}</code></p><p></p><p>就这么简单。</p><p></p><p>_Generic 是 C11 STANDARD - 6.5.1.1 Generic selection（<a href=\"https://www.open-std.org/JTC1/sc22/wg14/www/docs/n1570.pdf\">https://www.open-std.org/JTC1/sc22/wg14/www/docs/n1570.pdf</a>\"） 特性，语法如下：</p><p></p><p><code lang=\"plain\">Generic selection\nSyntax\n generic-selection:\n  _Generic ( assignment-expression , generic-assoc-list )\n generic-assoc-list:\n  generic-association\n  generic-assoc-list , generic-association\n generic-association:\n  type-name : assignment-expression\n  default : assignment-expression</code></p><p></p><p>page 和 folio 的相互转换也很直接。不管 head，tail page，转化为 folio 时，意义等同于获取 head page 对应的 folio；folio 转化为 page 时，folio-&gt;page&nbsp;用于获取 head page，folio_page(folio, n)&nbsp;可以用于获取 tail page。</p><p></p><p>问题是，本来 page 就能代表 base page，或者 compound page，为什么还需要引入 folio？</p><p></p><h3>folio 能做什么？</h3><p></p><p></p><p></p><blockquote>The folio type allows a function to declare that it's expecting only a head page. Almost incidentally, this allows us to remove various calls to VM_BUG_ON(PageTail(page)) and compound_head().page 的含义太多了，可以是 base page，可以是 compound head page，还可以是 compound tail page。</blockquote><p></p><p></p><p>如上述所说，page 元信息都存放在 head page（base page 可以看成是 head page）上，例如 page-&gt;mapping, page-&gt;index 等。但在 mm 路径上，传递进来的 page 参数总是需要判断是 head page 还是 tail page。由于没有上下文缓存，mm 路径上可能会存在太多重复的 compound_head 调用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5b8ecdfe60d4e4c39047207828e583b.png\" /></p><p></p><p>这里以 mem_cgroup_move_account 函数调用举例，一次 mem_cgroup_move_account 调用，最多能执行 7 次 compound_head。</p><p></p><p><code lang=\"plain\">static inline struct page *compound_head(struct page *page)\n{\n        unsigned long head = READ_ONCE(page-&gt;compound_head);\n\n        if (unlikely(head &amp; 1))\n                return (struct page *) (head - 1);\n        return page;\n}</code></p><p></p><p>再以&nbsp;page_mapping(page)&nbsp;为例具体分析，进入函数内部，首先执行&nbsp;compound_head(page)&nbsp;获取 page mapping 等信息。另外还有一个分支&nbsp;PageSwapCache(page)&nbsp;，当执行这个分支函数的时候，传递的是 page，函数内部还需执行一次&nbsp;compound_head(page)&nbsp;来获取 page flag 信息。</p><p></p><p><code lang=\"plain\">struct address_space *page_mapping(struct page *page)\n{\n        struct address_space *mapping;\n\n        page = compound_head(page);\n\n        /* This happens if someone calls flush_dcache_page on slab page */\n        if (unlikely(PageSlab(page)))\n                return NULL;\n\n        if (unlikely(PageSwapCache(page))) {\n                swp_entry_t entry;\n\n                entry.val = page_private(page);\n                return swap_address_space(entry);\n        }\n\n        mapping = page-&gt;mapping;\n        if ((unsigned long)mapping &amp; PAGE_MAPPING_ANON)\n                return NULL;\n\n        return (void *)((unsigned long)mapping &amp; ~PAGE_MAPPING_FLAGS);\n}\nEXPORT_SYMBOL(page_mapping);</code></p><p></p><p>当切换到 folio 之后，page_mapping(page)&nbsp;对应&nbsp;folio_mapping(folio)&nbsp;，而 folio 隐含着 folio 本身就是 head page，因此两个&nbsp;compound_head(page)&nbsp;的调用就省略了。</p><p></p><p>mem_cgroup_move_account 仅仅是冰山一角，mm 路径上到处是 compound_head 的调用。积少成多，不仅执行开销减少了，开发者也能得到提示，当前 folio 一定是 head page，减少判断分支。</p><p></p><h3>folio 的直接价值</h3><p></p><p></p><p>1）减少太多冗余 compound_head 的调用。</p><p>2）给开发者提示，看到 folio，就能认定这是 head page。</p><p>3）修复潜在的 tail page 导致的 bug。</p><p></p><p><code lang=\"plain\">Here's an example where our current confusion between \"any page\"\nand \"head page\" at least produces confusing behaviour, if not an\noutright bug, isolate_migratepages_block():\n\n        page = pfn_to_page(low_pfn);\n        if (PageCompound(page) &amp;&amp; !cc-&gt;alloc_contig) {\n                const unsigned int order = compound_order(page);\n\n                if (likely(order &lt; MAX_ORDER))\n                        low_pfn += (1UL &lt;&lt; order) - 1;\n                goto isolate_fail;\n        }\n\ncompound_order() does not expect a tail page; it returns 0 unless it's\na head page.  I think what we actually want to do here is:\n\n        if (!cc-&gt;alloc_contig) {\n            struct page *head = compound_head(page);\n            if (PageHead(head)) {\n                const unsigned int order = compound_order(head);\n\n                low_pfn |= (1UL &lt;&lt; order) - 1;\n                goto isolate_fail;\n            }\n        }\n\nNot earth-shattering; not even necessarily a bug.  But it's an example\nof the way the code reads is different from how the code is executed,\nand that's potentially dangerous.  Having a different type for tail\nand not-tail pages prevents the muddy thinking that can lead to\ntail pages being passed to compound_order().</code></p><p></p><h3>folio-5.16 已经合入</h3><p></p><p></p><p></p><blockquote>This converts just parts of the core MM and the page cache.willy/pagecache.git 共有&nbsp;209&nbsp;commit。这次 5.16 的合并窗口中，作者&nbsp;Matthew Wilcox (Oracle) <a href=\"mailto:willy@infradead.org\">willy@infradead.org</a>\"&nbsp;先合入 folio 基础部分，即 Merge tag folio-5.16，其中包含&nbsp;90&nbsp;commits，74&nbsp;changed files with&nbsp;2914&nbsp;additions and&nbsp;1703&nbsp;deletions。除了 folio 定义等基础设施之外，这次改动主要集中在 memcg, filemap, writeback 部分。</blockquote><p></p><p></p><p>folio-5.16 用 folio 逐步取代 page 的过程，似乎值得一提。mm 路径太多了，如果强迫症一次性替换完，就得 top-down 的方式，从 page 分配的地方改成 folio，然后一路改下去。这不现实，几乎要修改整个 mm 文件夹了。</p><p></p><p>folio-5.16 采用的是 bottom-up 的方式，在 mm 路径的某个函数开始，将 page 替换成 folio，其内部所有实现都用 folio，形成一个“闭包”。然后修改其 caller function，用 folio 作为参数调用该函数。直到所有 caller function 都改完了，那么这个“闭包”又扩展了一层。有些函数的调用者很多，一时改不完，folio-5.16 就提供了一层 wrapper。这里以 page_mapping/folio_mapping 为例。</p><p></p><p>首先闭包里是 folio_test_slab(folio)，folio_test_swapcache(folio) 等基础设施，然后向上扩展到 folio_mapping。page_mapping 的调用者很多，mem_cgroup_move_account 能顺利地调用 folio_mapping，而 page_evictable 却还是保留使用 page_mapping。那么闭包在这里停止扩展。</p><p></p><p><code lang=\"plain\">struct address_space *folio_mapping(struct folio *folio)\n{\n        struct address_space *mapping;\n\n        /* This happens if someone calls flush_dcache_page on slab page */\n        if (unlikely(folio_test_slab(folio)))\n                return NULL;\n\n        if (unlikely(folio_test_swapcache(folio)))\n                return swap_address_space(folio_swap_entry(folio));\n\n        mapping = folio-&gt;mapping;\n        if ((unsigned long)mapping &amp; PAGE_MAPPING_ANON)\n                return NULL;\n\n        return (void *)((unsigned long)mapping &amp; ~PAGE_MAPPING_FLAGS);\n}\nstruct address_space *page_mapping(struct page *page)\n{\n        return folio_mapping(page_folio(page));\n}\n\nmem_cgroup_move_account(page, ...) {\n  folio = page_folio(page);\n  mapping = folio_mapping(folio);\n}\n\npage_evictable(page, ...) {\n  ret = !mapping_unevictable(page_mapping(page)) &amp;&amp; !PageMlocked(page);\n}</code></p><p></p><h2>关于 folio，还需要了解什么？</h2><p></p><p></p><p>很多小伙伴看到这里是不是和我有一样的感受：就这些吗？仅仅是 compound_head 的问题吗？</p><p></p><p>我不得不去学习 LWN: A discussion on folios（<a href=\"https://lwn.net/Articles/869942/\">https://lwn.net/Articles/869942/</a>\"），LPC 2021 - File Systems MC（<a href=\"https://www.youtube.com/watch?v=U6HYrd85hQ8&amp;t=1475s\">https://www.youtube.com/watch?v=U6HYrd85hQ8&amp;t=1475s</a>\"）&nbsp;大佬关于 folio 的讨论。然后发现 Matthew Wilcox 的主题不是《The folio》，而是《Efficient buffered I/O》。事情并不简单。</p><p></p><p>这次 folio-5.16 合入的都是 fs 相关的代码，组里大佬提到 “Linux-mm 社区大佬不同意全部把 page 替换成 folio，对于匿名页和 slab，短期内还是不能替换”。于是我继续翻阅 Linux-mm 邮件列表。</p><p></p><h3>关于 folio 的社区讨论</h3><p></p><p></p><h4>命名</h4><p></p><p></p><p>首先是 Linus，Linus 表示他不讨厌这组 patch，因为这组 patch 确实解决了 compound_head 的问题；但是他也不喜欢这组 patch，原因是 folio 听起来不直观。</p><p></p><p>经过若干关于取名的讨论，当然命名最后还是 folio。</p><p></p><h4>FS 开发者的意见</h4><p></p><p></p><p>目前 page cache 中都是 4K page，page cache 中的大页也是只读的，例如代码大页（<a href=\"https://openanolis.cn/sig/Cloud-Kernel/doc/475049355931222178\">https://openanolis.cn/sig/Cloud-Kernel/doc/475049355931222178</a>\"）特性。为什么 Transparent huge pages in the page cache 一直没有实现，可以参考这篇 LWN（<a href=\"https://lwn.net/Articles/686690/\">https://lwn.net/Articles/686690/</a>\"）。其中一个原因是，要实现 读写 file THP，基于 buffer_head 的 fs 对 page cache 的处理过于复杂。</p><p></p><p>buffer_head：buffer_head 代表的是物理内存映射的块设备偏移位置，一般一个 buffer_head 也是 4K 大小，这样一个 buffer_head 正好对应一个 page。某些文件系统可能采用更小的block size，例如 1K，或者 512 字节。这样一个 page 最多可以有 4 或者 8 个buffer_head 结构体来描述其内存对应的物理磁盘位置。这样，在处理 multi-page 读写的时候，每个 page 都需要通过&nbsp;get_block&nbsp;获取 page 和 磁盘偏移的关系，低效且复杂。iomap：iomap 最初是从 XFS 内部拿出来的，基于 extent，天然支持 multi-page。即在处理 multi-page 读写的时候，仅需一次翻译就能获取所有 page 和 磁盘偏移的关系。通过 iomap，文件系统与 page cache 隔离开来了，例如，它们在表示大小的时候都使用字节，而不是有多少 page。因此，Matthew Wilcox 建议任何直接使用 page cache 的文件系统都应该考虑要换到 iomap 或 netfs_lib 了。隔离 fs 与 page cache 的方式或许不止 folio，但是例如 scatter gather 是不被接受的，抽象太复杂。这也是为什么 folio 先在 XFS/AFS 中落地了，因为这两个文件系统就是基于 iomap 的。这也是为什么 FS 开发者都强烈希望 folio 被合入，他们可以方便地在 page cache 中使用更大的 page，这个做法可以使文件系统的 I/O 更有效率。</p><p></p><p>buffer_head 有一些功能是当前 iomap 仍然缺乏的。而 folio 的合入，能让 iomap 得到推进，从而使 block-based 文件系统能够改成使用 iomap。</p><p></p><h4>MM 开发者的意见</h4><p></p><p></p><p>最大的异议来自 Johannes Weiner，他承认 compound_head 的问题，但觉得修复该问题而引入这么大的改动不值得；同时认为 folio 对 fs 的所做的优化，anonymous page 不需要。</p><p></p><p></p><blockquote>Unlike the filesystem side, this seems like a lot of churn for very little tangible value. And leaves us with an end result that nobody appears to be terribly excited about.But the folio abstraction is too low-level to use JUST for file cache and NOT for anon. It's too close to the page layer itself and would duplicate too much of it to be maintainable side by side.最后在 &nbsp;Kirill A. Shutemov、Michal Hocko 等大佬的力挺 folio 态度下，Johannes Weiner 也妥协了。</blockquote><p></p><p></p><p>社区讨论到最后，针对 folio 的反对意见在 folio-5.15 的代码中都已经不存在了，但错过了 5.15 的合并窗口，因此这次 folio-5.16 原封不动被合入了。</p><p></p><h3>folio 的深层价值</h3><p></p><p></p><p></p><blockquote>I think the problem with folio is that everybody wants to read in her/his hopes and dreams into it and gets disappointed when see their somewhat related problem doesn't get magically fixed with folio.Folio started as a way to relief pain from dealing with compound pages. It provides an unified view on base pages and compound pages. That's it.It is required ground work for wider adoption of compound pages in page cache. But it also will be useful for anon THP and hugetlb.Based on adoption rate and resulting code, the new abstraction has nice downstream effects. It may be suitable for more than it was intended for initially. That's great.But if it doesn't solve your problem... well, sorry...The patchset makes a nice step forward and cuts back on mess I created on the way to huge-tmpfs.I would be glad to see the patchset upstream.--Kirill A. Shutemov</blockquote><p></p><p></p><p>大家都知道“struct page 相关的混乱”，但没有人去解决，大家都在默默忍受这长期以来的困扰，在代码中充斥着如下代码。</p><p></p><p><code lang=\"plain\">if (compound_head(page)) // do A;\nelse                     // do B;</code></p><p></p><p>folio 并不完美，或许因为大家期望太高，导致少数人对 folio 的最终实现表示失望。但多数人认为 folio 是在正确方向上的重要一步。毕竟后续还有更多工作要实现。</p><p></p><h2>写在最后</h2><p></p><p></p><p>folio 后续的开发计划如下：</p><p></p><p></p><blockquote>对于5.17，我们打算转换各种文件系统(XFS和AFS已经准备好了；其他文件系统可能会做到)，也可以将更多的MM和页面缓存转换为对开页。对于5.18，应该准备好多页的开本。</blockquote><p></p><p></p><p>此外，folio 还能提升性能：</p><p></p><p></p><blockquote>80%的优势是真实的，但似乎是一个人为的基准(postgres启动，这不是一个严重的工作量)。实际工作负载(例如构建内核，在稳定状态下运行postgres等)似乎在0-10%之间受益。folio-5.16 减少大量 compound_head 调用，在 sys 高的 micro benchmark 中应当有性能提升。未实测。</blockquote><p></p><p></p><p>folio-5.18 multi-page folios 支持之后，理论上 I/O 效率能提升，拭目以待。</p><p></p><p>开发者要想使用 folio，FS 开发者最应该做的就是把那些仍然使用 buffer head 的文件系统转换为使用 iomap 进行 I/O，至少对于那些 block-based 文件系统都应该这么做。其他开发者欣然接受 folio 即可，基于 5.16+ 开发的新特性能用 folio 就用 folio，熟悉一下 API 即可，内存分配回收等 API 本质没有改变。</p><p></p><h4>作者介绍</h4><p></p><p></p><p>徐宇，阿里云研发工程师。</p>",
    "publish_time": "2023-08-14 16:43:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "处理时延降低24倍，联通云粒数据引擎优化实践",
    "url": "https://www.infoq.cn/article/w5Pu2MfhdT1ch9ZCJwgX",
    "summary": "<p></p><blockquote>作者：郑扬勇，云粒星河数据中台产品负责人</blockquote><p></p><p></p><p>云粒智慧科技有限公司成立于 2018 年 6 月，是中国联通集团混改以来成立的首家合资公司，是中国智慧城市数智化建设者。一直以来，云粒智慧以数字化、智能化、集约化产品为核心，全面融合“5G+大数据+AI+CIM”等最新技术，致力于构建未来城市数字化基础设施平台，打造“绿色、互联、智能”的现代化智慧城市，为政企提供符合政策导向及智慧城市发展趋势的“三中台+智能化应用”解决方案，实现城市智脑与生态环境可持续发展。</p><p></p><p>这里说到的“三中台”，其最重要的中台即云粒星河数据中台，是一套集“数据建设与运营方法论、软件+行业资产包和数据技术服务”的中台体系，提供数据采集、融合、治理、计算、分析、服务、可视化的全链路一站式管理与服务。经过四年 4 个大版本的迭代，目前已累计完成 80+ 客户项目的落地交付，实现产品销售总额超过 1.2 亿元的好成绩。</p><p></p><h2>Hive存在四大痛点交付成本显著增加</h2><p></p><p></p><p>云粒星河数据中台作为大数据处理系统，数据引擎是其最重要的核心中间件。云粒星河数据中台的数据引擎一直选用开源的 Apache Hive，自诞生，到 3.x 系列最后一个版本。总体上 Apache Hive 是一个非常优秀、久经考验的 OLAP 引擎，但在项目落地实施的过程中，我们也遇到了诸多痛点，导致最终交付成本偏高，拉低了项目的毛利率。</p><p></p><p>痛点 1：组件众多，运维困难，云原生化不友好</p><p></p><p>Hive 依赖 Hadoop，我们使用 HDFS 存储数据，YARN 作为资源管理框架，Tez 优化 Hive DAG 任务；由于需要高可用，每个节点都需要启动好几个相关进程，这些进程的配置、监控、伸缩、保活等都极大地增加了运维工作量。由于 Hive 和 Hadoop 使用的是已经老旧的按节点方式扩缩容的架构设计，因此云原生非常不友好，社区至今也没有提供容器化部署方案；自行尝试通过 Statefulset 方式运行在 Kubernetes 中并进行性能测试，发现性能竟然有 30% 以上的下降，因此我们仍然使用物理机或 VM 方式部署。</p><p></p><p>痛点 2：资源利用率低, 任务调优繁琐复杂</p><p></p><p>由于 YARN 是双层悲观并发资源管理（调度）框架，经过 Tez 优化后的 Hive DAG 任务向 YARN 申请资源仍然是按固定配额（vCore 和 Mem）的方式进行，为了能够最大化利用资源提高并发，需要在项目中根据任务处理数据量情况 Case By Case 做配置调优，并且随着数据中台数据处理量的不断变化（通常情况是逐步增加），配置调优的工作需要持续进行，无法一劳永逸。</p><p></p><p>痛点 3：数据处理时延大，用户体验差</p><p></p><p>由于诸多原因，我们没有使用 Hive 的 LLAP 特性，这会导致 Hive 即使处理极小的数据量如数百条记录，由于需要冷启动最低两个 YARN Container（含一个 App Master），至少需要数秒才能返回，无法做到亚秒级交互式查询，难以支持数据大屏等实时性要求较高的下游应用，为了解决这个问题，我们追加部署了基于 MPP 架构的 Presto 引擎解决了这个问题，但这也带来新的问题，即对内存资源的需求也大大增加了，这种成本的增加最终还是会转变为交付成本，降低项目利润。</p><p></p><p>痛点 4：不支持行级更新，灵活性较低</p><p></p><p>Hive 是一个为数仓而生经典的 OLAP 引擎，数据更新仅支持全表/分区级覆盖，极低的情况下如果需要对远景冷区部分数据进行更新，处理较为麻烦；另外分区设置策略也颇为费脑——粒度太大更新效率较低，粒度太小又容易发生分区和小文件数据量爆炸，表现为还是效率低下……</p><p></p><p>正是由于以上一些挑战，自云粒星河数据中台 3.0 大版本发布支持多引擎并行能力开始，公司内部一直在寻找一款稳定可靠、AP 和 TP 兼备、能够在集约资源环境下有较高效率表现的数据引擎。</p><p></p><h2>选型OceanBase，并发性能提升10倍+</h2><p></p><p></p><p>但数据引擎作为基础软件百花齐放，我们如何在一堆“好”软件中最好的只有更挑选更适合自己的以及怎么判断适合？云粒总结了如下五点：</p><p></p><p>开源软件，友好的商业 License；支持云原生；支持集群模式；支持私有化部署；有较高成熟度（社区、生态等）。</p><p></p><p>经过较长时间的调研和比较，初步满足条件的数据引擎仅剩以下&nbsp; CockroachDB、YugabyteDB、PingCap TiDB、OceanBase 四款。其中，CockroachDB 社区版限制较多，例如，较为基础的索引功能都需要获取商业版License 解锁；YugabyteDB 在内部性能测试对比过程中表现较差，因此两者排除较早；而对于后两款，OceanBase 相比 TiDB，更适合我们的点在于以下三个方面：</p><p></p><p>第一，OceanBase 的架构较为简洁，只有 OBServer 和 OBProxy。而 TiDB 由PD、TiDB、TiKV、TiFlash 四个组件构成。如果只是部署一套集群用于内部服务，那么二者的区别不大，但我们需要部署和运维几十甚至上百套集群，配置、部署、运维等方面用 OceanBase 较为便利。</p><p></p><p>第二，OceanBase 原生支持多租户，资源隔离和控制模型也比较清晰。而 TiDB 对于多租户支持很晚（生产可用应该是在 V7.0+），至今仍处于完善阶段。云粒数据中台作为一个原生多租户系统，使用 OceanBase 的多租户体验更舒服。</p><p></p><p>第三，OceanBase 的生态策略感觉更开放。例如，数据集成方面专门为 DataX 开发了插件，更贴合我们现有技术路线。TiDB 虽然提供了更丰富的数据集成组件包含 TiCDC、TiDB Data Migration、TiDB Lightning，但我们整合进产品会比较重，工作量会比较大。</p><p></p><p>基于上述因素，自 2021 年 OceanBase 宣布开源开始，其进入我们的候选名单，2022 年，OceanBase 发布 4.0 版本，其迭代速度和性能改进更是让我们惊叹，正是那时，我们果断确定产品选型并启动适配工作。</p><p></p><p>因为项目体量较大及产品功能较多，且大多数都与数据引擎相关，整个适配过程大概持续了两个多月完美收工。数据引擎更换为 OceanBase 后的云粒星河数据中台得到了如下优化，极大缓解甚至消除了之前的痛点。</p><p></p><p>优化 1：更简介的架构，更好的云原生</p><p><img src=\"https://static001.geekbang.org/infoq/eb/eb770ce96e26df7286e20a73a24180fa.png\" /></p><p>左-更换前（Hive+Presto）；右-更换后（单一OceanBase）</p><p></p><p>从上图可以看出，相比 Hive 引擎，OceanBase 只需要在每个节点上启动 OBProxy 和 OBServer 两个进程即可，通过 Prometheus 导出 Metrics，监控运维便捷省力。得益于架构的简洁，OceanBase 很容易实现云原生化，官方已提供在 Kubernetes 中部署运行的详细方案，这对云粒星河数据中台本身实现彻底云原生化至关重要。</p><p></p><p>优化2：让每一核 CPU 发挥最大价值</p><p></p><p>私有化环境交付，客户能够提供的资源不足已经是“家常便饭“，这就要求云粒星河数据中台必须具备“螺蛳壳里做道场”的能力，即在较低资源配置下也能有良好的处理能力表现。例如，我们甚至遇到个别客户仅提供三台 8C32GB 规格的服务器部署数据引擎。以往采用 Hive 结合 Presto 作为数据引擎。部署完各类组件，每个节点能够提供给 YARN 调度的内存往往就只剩下 10GB 左右，每个作业（Job）还需要启动一个独立的用于协调的AppMaster（通常占用 1GB 内存），使得在小数据量高并发场景下的性能表现雪上加霜。</p><p></p><p>前文也提到需要对于 YARN 资源分配的参数反复调校，费时费力。采用 OceanBase 作为数据引擎后，单租户模式下，为 OBProxy 分配 2GB 内存，系统租户和租户 META 租户各分配 3GB 内存，剩余内存全部用于租户本身，通过试验，小数据量场景（单次处理数据量低于 1GB）并发能力相比 Hive 有十数倍提升，在较大的数据量（单次处理数据量超过 10GB）场景下也能做好处理，轻松榨干 CPU 每一核。</p><p></p><p>优化 3：数据治理从分钟级到准实时</p><p></p><p>准实时数据治理单次需要处理的数据量往往都较小，得益于高效的分布式计算调度和数据存储结构，即使是逻辑较为复杂的数据治理 SQL，OceanBase 也能游刃有余地快速完成，以下是测试数据治理工作流执行时间对比，它由一个数据接入节点和两个数据更新写入节点构成，每次处理的数据量接近 1GB，资源配置同为三台 8C32GB 服务器集群。</p><p></p><p></p><p>可以看出，OceanBase 在小数据量场景下各方面的时延都远低于 Hive。而相比定位为单一 OLAP 引擎的 Hive，定位为 HTAP 引擎的 OceanBase 在 TP 方面的诸多优势不再赘述，对于冷数据行级更新更不在话下。</p><p></p><h2>从Hive到OceanBase，我们的几点建议</h2><p></p><p></p><p>当然，对于团队中习惯使用 Hive 做数据交付的同学，在使用 OceanBase 的过程中，也有少量感觉不太方便的地方，主要有两点：</p><p></p><p>第一，OceanBase 不支持 Insert Overwrite，还好可以使用 Truncate/Delete + Insert 曲线支持，问题不大；</p><p></p><p>第二，OceanBase 不支持使用 List 分区策略时动态分区，因此每次插入数据时，都需要检查对应的分区是否存在，如果不存在，则需要先 ALTER TABLE· ADD PARTITION，很不方便，希望未来能尽快支持。</p><p></p><p>另外，不可否认，当单次需要处理的数据量上升到一定级别如 100GB 以上，凭借 ORC 或 Parquet 列存格式优势，Hive执行数据分析的性能表现是优于 OceanBase 的，不过可喜的是，列存计划已列入产品 roadmap，希望在不久后可以看到更强的 AP 性能能力。</p><p></p><h2>写在最后</h2><p></p><p></p><p>目前，更换为 OceanBase 作为数据引擎的云粒星河数据中台 4.0 已经在项目上实施落地。总的来说，OceanBase 更简洁的架构、更轻便的运维，帮助我们加速了数据中台云原生的进程，提升资源利用率的同时，并发性能提升 10+ 倍，数据处理时延降低 1.5-24 倍。这带来的直观效益是机器成本与运维人力的节约，进而带来了 20% 的毛利率提升。</p><p></p><p></p><p></p>",
    "publish_time": "2023-08-14 16:51:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]