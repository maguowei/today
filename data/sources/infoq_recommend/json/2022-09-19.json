[
  {
    "title": "Stability AI开源图像生成模型Stable Diffusion",
    "url": "https://www.infoq.cn/article/p1DHKrHQjEsQvDcr9jlM",
    "summary": "<p><a href=\"https://stability.ai/\">Stability AI</a>\"对外发布了<a href=\"https://stability.ai/blog/stable-diffusion-public-release\">Stable Diffusion</a>\"的预训练模型权重，这是一个文本至图像的AI模型。根据文本提示，Stable Diffusion能够生成逼真的512x512像素的图像以描述提示中的场景。</p><p></p><p>在模型权重公开发布之前，它的代码已经<a href=\"https://stability.ai/blog/stable-diffusion-announcement\">发布</a>\"，模型权重也有限发布给了研究社区。在最新的版本中，任何用户都可以在消费者级别的硬件中下载并运行Stable Diffusion。除了文本至图像的生成，该模型还支持图像至图像的风格转换以及图像质量提升。在发布该版本的同时，Stable AI还发布了beta版本的API以及模型的Web UI，名为<a href=\"https://beta.dreamstudio.ai/\">DreamStudio</a>\"。Stable AI这样说到：</p><p></p><p></p><blockquote>Stable Diffusion是一个文本至图像的模型，它能让数十亿人在几秒钟内创建出令人赞叹的艺术品。在速度和质量方面，它都有所突破，这意味着它能在消费者级别的GPU上运行……这能够让研究人员和……公众在各种条件下运行，使图像生成技术走向大众。我们期待围绕该模型和其他模型出现一个开放的生态系统，以探索潜在空间的边界。</blockquote><p></p><p></p><p>Stable Diffusion基于名为<a href=\"https://ommer-lab.com/research/latent-diffusion-models/\">潜在扩散模型（latent diffusion models，LDMs）</a>\"的图像生成技术。与其他的流行的图像合成方法不同，如<a href=\"https://en.wikipedia.org/wiki/Generative_adversarial_network\">生成对抗网络（generative adversarial networks，GANs）</a>\"和<a href=\"https://openai.com/dall-e-2/\">DALL-E</a>\"使用的自动回归技术，LDMs通过在一个潜在表示空间中迭代“去噪”数据来生成图像，然后将表示结果解码为完整的图像。LDM是由<a href=\"https://www.lmu.de/\">Ludwig Maximilian University of Munich</a>\"的<a href=\"https://ommer-lab.com/\">机器视觉与学习（Machine Vision and Learning）</a>\"研究组开发的，并在最近的IEEE / CVF&nbsp;<a href=\"https://arxiv.org/abs/2112.10752\">计算机视觉和模式识别会议（Computer Vision and Pattern Recognition Conference）</a>\"上发表的一篇论文中进行了阐述。在今年早些时候，InfoQ曾经报道过谷歌的<a href=\"https://www.infoq.cn/article/QhKzahCQ9bdTgAUobYUg\">Imagen</a>\"模型，它是另一个基于扩散的图像生成AI。</p><p></p><p>Stable Diffusion模型支持多种操作。与DALL-E类似，它能够根据所需图像的文本描述，生成符合匹配该描述的高质量图像。它还可以根据一个简单的草图再加上所需图像的文本描述，生成一个看起来更逼真的图像。Meta AI最近发布了名为<a href=\"https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/\">Make-A-Scene</a>\"的模型，具有类似的图像至图像的功能。</p><p></p><p>Stable Diffusion的很多用户已经公开发布了生成图像的样例，Stability AI的首席开发者Katherine Crowson在推特上分享了<a href=\"https://twitter.com/RiversHaveWings\">许多图像</a>\"。基于AI的图像合成可能会对艺术家和艺术领域带来一定的影响，有些评论者对此感到不安。就在Stable Diffusion发布的同一周，一幅由AI生成的艺术品在科罗拉多州博览会的<a href=\"https://news.artnet.com/art-world/colorado-artists-mad-ai-art-competition-2168495\">艺术比赛中获得了一等奖</a>\"。Django框架的共同创建者Simon Williamson<a href=\"https://twitter.com/simonw/status/1563914121883488258\">认为</a>\"：</p><p></p><p></p><blockquote>我见过一种说法，认为AI艺术没有资格获得版权保护，因为“它必须归功于全人类”——如果基于文本生成的设计尚不足以说服公众的话，那[图像至图像]技术可能会打破这种平衡。</blockquote><p></p><p></p><p>Stable AI的创始人Emad Mostaque在推特上回答了一些关于该模型的问题。在回答一位试图<a href=\"https://twitter.com/EMostaque/status/1563870674111832066\">估算训练模型所需的计算资源和成本的</a>\"用户时，Mostaque说到：</p><p></p><p></p><blockquote>实际上，我们为这个模型使用了256个A100显卡，总共15万小时，所以按市场价格计算为60万美元。</blockquote><p></p><p></p><p>Mostaque给出了Reddit上一篇文章的链接，其中给出了如何最好地使用该模型来生成图像的<a href=\"https://www.reddit.com/r/StableDiffusion/comments/x41n87/how_to_get_images_that_dont_suck_a/\">技巧</a>\"。</p><p>Stable Diffusion的代码可以在<a href=\"https://github.com/CompVis/stable-diffusion\">GitHub上找到</a>\"。<a href=\"https://huggingface.co/CompVis/stable-diffusion\">模型的权重</a>\"以及<a href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\">Colab notebook</a>\"和<a href=\"https://huggingface.co/spaces/stabilityai/stable-diffusion\">示例Web UI</a>\"都可以在HuggingFace上找到。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/09/stable-diffusion-image-gen/\">Stability AI Open-Sources Image Generation Model Stable Diffusion</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/QhKzahCQ9bdTgAUobYUg\">谷歌最新 Imagen&nbsp;AI&nbsp;在文本至图像生成方面优于 DALL-E</a>\"</p>",
    "publish_time": "2022-09-19 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]