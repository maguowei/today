[
  {
    "title": "平台工程应知应会",
    "url": "https://www.infoq.cn/article/TbxS2My9fOIL1GMpHjKD",
    "summary": "<p>随着社区和工具生态的快速发展，平台工程显然会继续存在下去。但是，和任何（相对）新的趋势一样，平台工程仍有许多未解之谜。最近，Humanitec发布了第一份《平台工程现状报告》——以下是该报告的关键内容以及平台工程的重要趋势。</p><p>&nbsp;</p><p>平台工程是软件工程中最大的趋势之一。<a href=\"https://platformcon.com/\">PlatformCon</a>\" 2022 ——有史以来的第一个平台工程大会——吸引了超过6000名参会者，而当地的聚会小组也有数千名会员，很显然，这一趋势将持续下去。</p><p>&nbsp;</p><p>自然，与软件工程的任何趋势一样，会有许多社区内外的人都好奇的未解问题。第一份《<a href=\"https://humanitec.com/whitepapers/state-of-platform-engineering-report-volume-1\">平台工程现状报告</a>\"》试图回答这些持续存在的问题，提供关于平台工程现状的信息，并从社区的不同角落汇编了一些关键资源。</p><p>&nbsp;</p><p>所以，话不多说，以下是你需要知道的关于平台工程的关键内容。</p><p>&nbsp;</p><p></p><h2>为什么平台工程是一个重要的趋势？</h2><p></p><p></p><p>在过去十年中，复杂的微服务架构、Kubernetes这样的技术以及IaC（基础设施即代码）这样的方法已经成为行业标准。现在，即使是简单的任务也要求开发人员对工具链有一个端到端的理解，这极大地增加了他们的认知负荷，并导致组织效率低下，比如影子运维。</p><p>&nbsp;</p><p>认知负荷是指“工作记忆中消耗的脑力劳动总和。”这一理论是由心理学家John Sweller在20世纪80年代末首次提出的。<a href=\"https://my.chartered.college/impact_article/%20cognitive-load-theory-and-its-application-in-the-classroom/\">该理论认为</a>\"，“当学习任务超出工作记忆容量时，学习就会受阻。”</p><p>&nbsp;</p><p>2019年，Manuel Pais和Matthew Skelton认为，认知负荷是当前DevOps设置问题的主要副产品。这一发现推动他们创建了<a href=\"https://teamtopologies.com/\">团队拓扑</a>\"，其中有一个专门的产品团队构建了抽象层或平台，使开发人员可以免受底层技术复杂性的影响，降低了他们的认知负荷。</p><p>&nbsp;</p><p>团队拓扑设计用于防止某些DevOps设置中反复出现的特定的<a href=\"https://web.devopstopologies.com/\">反模式</a>\"。在一种被我们称为“影子运维”的反模式中——<a href=\"https://humanitec.com/whitepapers/2021-devops-setups-benchmarking-report#whitepaper\">Humanitec 2021年开展的DevOps基准研究</a>\"证实了这一点——工程组织断定他们不再需要专职的运维专家（或至少减少人数），并将这些任务转给开发人员。通常，这会导致缺乏经验的开发人员向高级后端工程师寻求帮助。这会导致组织中一些最有才能、最重要的资源错配，工程团队的生产力因此降低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa10b57af867113aa536fc46cc516751.jpeg\" /></p><p></p><p>受<a href=\"https://www.youtube.com/watch?v=zUpYEhaUJnM&amp;t=138s\">Daniel Bryant在PlatformCon 2022大会的演讲</a>\"启发</p><p>&nbsp;</p><p>因此，一些组织被迫寻找更好的方法。他们成立了平台工程团队来构建内部开发平台，或者将技术和工具组合在一起，降低开发人员的认知负荷，而又不用抽象出底层技术的上下文。<a href=\"https://platformengineering.org/talks-library/netflix-platform-console-to-unify-engineering-experience\">Netflix</a>\"利用他们的平台解决了开发人员在管理多种服务和软件、了解存在哪些工具以及在不同工具之间切换上下文时所面临的挑战。<a href=\"https://platformengineering.org/talks-library/internal-platform-enterprise-courtney-kissler\">Nordstrom</a>\"打造了一个平台，成了“自助服务的北极星”。他们的平台帮助消除了环境准备过程的瓶颈，人们不再需要登录到生产环境。这些变化使组织能够更快地前进。<a href=\"https://platformengineering.org/talks-library/sunrise-zalandos-internal-developer-platform\">Zalando</a>\"利用他们的平台统一了开发体验，默认启用了合规性增强，并随着时间的推移改进了公司的运营方式。</p><p>&nbsp;</p><p>这些只是其中几个成功的平台项目的示例。Puppet <a href=\"https://puppet.com/resources/report/2020-state-of-devops-report\">2020</a>\"年和<a href=\"https://puppet.com/resources/report/2021-state-of-devops-report\">2021</a>\"年的“DevOps现状报告”以及Humanitec 2021年的“DevOps基准报告”等研究表明，使用IDP的团队，其DORA指标显示出了更高程度的DevOps演进和效能。</p><p>&nbsp;</p><p>那么到底什么是平台工程呢？</p><p>&nbsp;</p><p>虽然就平台工程的通用定义达成一致很困难，但这项任务很重要。我<a href=\"https://platformengineering.org/blog/what-is-platform-engineering\">最近将平台工程定义为</a>\"：</p><p></p><blockquote>设计和构建工具链及工作流的学科，为云原生时代的软件工程组织提供自助服务能力。平台工程师提供一个通常称为“内部开发平台”的集成化产品，涵盖应用程序整个生命周期的操作需求。</blockquote><p></p><p>&nbsp;</p><p>内部开发平台由专门的平台团队构建。团队拓扑讨论团队是什么，不是什么。平台团队不是SRE的替身，也不是处理开发者工单的影子运营团队。它不应该关注个人挑战或生产可靠性。相反，它应该以产品思维为驱动，将开发者视为平台客户。</p><p>&nbsp;</p><p></p><h2>平台工程原则和最佳实践</h2><p></p><p></p><p>在平台团队开始构建他们的产品之前，需要定义一个明确的使命宣言来指导整个过程。宣言应符合组织的总体目标，并主动定义好平台团队在组织中的角色。它应该可以激励你的工程师。Hashicorp平台工程基础设施总监<a href=\"https://medium.com/@michael.roy.galloway/your-platform-org-needs-a-purpose-heres-how-to-find-it-64874b082d80\">Michael Galloway</a>\"有一个很好的总结：“它应该是有感染力且鼓舞人心的……它应该是简单而有意义的。”</p><p>&nbsp;</p><p>你可以从定义目标着手。这可能包括：在不增加认知负荷的情况下，实现所需程度的开发人员自助服务，或者在不强迫开发人员学习<a href=\"https://score.dev/blog/workload-centric-over-infrastructure-centric-development\">以基础设施为中心的端到端技术</a>\"的情况下，实现所需的操作成本节省。在此之后，你可能会得出这样的结论：“我们的任务是标准化工作流程，改善开发体验，加快创新周期，缩短工程组织的上市时间。”但这是描述性的，不够鼓舞人心。完善你的使命宣言，实现良好的平衡。例如：“我们的使命是构建开发者喜爱的平台，提高他们的创新速度。”</p><p>&nbsp;</p><p>构建平台的最佳方法是遵循产品方法。通过用户研究、用户反馈征集，从利益相关者那里获得内部支持，平台团队可以全面了解开发人员的痛点和整个组织共同面临的挑战。它可以确定开发人员需要什么功能，并构建一个可以实现这些解决方案的黄金路径。但平台之旅并未就此止步。成功的平台团队会与开发人员保持开放的沟通，并度量工程KPI，确保开发人员采用了平台，而平台使开发人员的工作变得更轻松。</p><p>&nbsp;</p><p><a href=\"https://medium.com/wise-engineering/platform-engineering-kpis-6a3215f0ee14\">Lambros Charissis</a>\"分享了Wise的平台工程团队如何制定了一套可执行的KPI来识别挑战和度量绩效。他们使用KPI树来明确KPI：生产力、交货时间、部署频率、开发人员满意度、稳定性、效率和风险。</p><p>&nbsp;</p><p>你还应该监控平台采用KPI，如市场份额（开发人员是否在实际的工作中使用了你的平台，还是更喜欢直接使用他们自己的技术和工具？），使用平台节省的时间，以及团队之间关系的改善（例如开发人员和运营人员）。定期进行调查，看看平台是否改善了开发体验。</p><p>&nbsp;</p><p>平台团队和他们提供的黄金路径是将这些复杂的设置整合在一起的粘合剂。然而，由于平台团队通常不交付面向客户的特性，所以许多组织错误地将它们视为成本中心。平台团队应该争取相关涉众团体的内部支持，确保其内部平台项目可以长久地运行下去。</p><p>&nbsp;</p><p>最后，也许也是最重要的一点，成功的平台团队不会重新发明轮子。有时，最好的解决方案必须从头构建。然而，许多企业正在将他们的资源投入到针对特定的用途构建有效的解决方案。为了解决各种各样的问题，工具领域正在快速发展。只要有可能，平台团队就可以通过裁剪现成的解决方案来节省时间并创造更多的价值。</p><p>&nbsp;</p><p></p><h2>平台工具生态</h2><p></p><p></p><p>平台工程是指构建内部开发平台。Kaspar von Grünberg解释说，内部开发平台是“平台工程团队将所有技术和工具整合在一起，铺就一条黄金道路。”但是，都有哪些技术和工具呢？如何确定哪些最适合你的组织？</p><p>&nbsp;</p><p>构建平台的方法有很多种，没有通用的解决方案，尽管有些供应商可能会做出这样的承诺。有些提供PaaS类体验的供应商（如Heroku）可能适合小型初创公司，特别是那些想要快速推出POC或MVP的公司。</p><p>&nbsp;</p><p>但对于大型企业或中型工程组织来说，在大多数情况下，这样的PaaS是行不通的，因为他们仍然需要支持他们的棕地设置，而不能启用绿地。</p><p>&nbsp;</p><p>然而，平台工程专家通常会建议平台开发遵循产品方法。也就是说，你不一定要从头开始。现在有很多平台工具产品，其中许多是开源的，也有一部分是专有的。为了节省组织的时间和金钱，只要可能，就要考虑购买和定制预构建的解决方案。这就为平台团队留出了时间和资源，让他们专注于定制组织需要的特性。</p><p>&nbsp;</p><p>为了更好地理解所有选项，平台工程现状报告设法对平台工具的生态进行了划分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/40/4040331424fa336747103b9e7fe3c23f.jpeg\" /></p><p></p><p>在一份题为“<a href=\"https://www.gartner.com/document/4017457\">软件工程领导者改善开发体验指南</a>\"”的报告中，Gartner总结了指导平台工程建设的三个关键思想：</p><p>通过构建内部开发平台来减少开发人员的认知负荷、艰苦的重复性手工工作，从而改善开发体验。平台不会强制使用特定的工具集或方法——它是为了让开发人员可以更容易地构建和交付软件，同时又不用抽象出底层核心服务中实用的差异化功能。平台工程团队将平台视为产品（供开发人员使用），并设计出可以自助使用的平台。</p><p>&nbsp;</p><p>这里还需要澄清下内部开发平台和内部开发门户之间的区别。Gartner的报告也提到了这个问题。作者写道：</p><p></p><p></p><blockquote>内部开发门户是开发人员发现和访问内部开发平台能力的界面。</blockquote><p></p><p>&nbsp;</p><p>内部开发门户提供了一个具有服务目录功能的UI，开发人员可以使用它访问底层的内部开发平台。反过来，内部开发平台消除了艰苦的重复性工作。</p><p>&nbsp;</p><p>构建开发门户并不能解决降低开发体验的底层配置和工作流问题。这就是内部开发平台和开发门户成为理想组合的原因。</p><p>&nbsp;</p><p></p><h2>社区与职业</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1be0cee92ecf4325e19eba90de713ff1.jpeg\" /></p><p></p><p>平台工程社区始于2021年，当时在奥斯汀和柏林有多个聚会小组。如今，它在全球有19个聚会小组，活跃的平台工程师超过10000名。这种社区发展势头表明，组织越来越重视平台工程。</p><p>&nbsp;</p><p>平台工程也是一条很有前途的职业道路。许多公司开始意识到，构建内部开发平台可以增加他们的利润。此外，构建内部开发平台的人平均工资也高于对应的DevOps岗位。</p><p>&nbsp;</p><p>尽管如此，像平台工程师或平台负责人这样的职位还不是特别常见。平台工程现状报告发现，大多数正在构建组织平台的受访者都拥有像高级软件工程师、IT架构师、首席工程师或高级DevOps工程师等职位。因此，虽然行业开始拥抱平台工程，但要恰当地定义平台角色仍然很困难。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78ae67c584157dce599450a5a3f44b50.jpeg\" /></p><p></p><p>考虑从事平台工程师职业的人经常会询问社区，他们需要掌握哪些技术。根据我们的调查，超过一半的受访者表示，他们最关注Docker、Kubernetes、IaC和CI/CD。数据库、存储和网络技术也在清单上，不过是在不到四分之一的受访者的清单上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7f805254d9d48f5fc77f4cd45dc9add3.jpeg\" /></p><p></p><p>说到工作安排灵活性方面，平台工程走在了前列。超过80%的美国受访者是完全远程工作。欧洲受访者在完全远程办公和混合办公之间取得了更好的平衡。总的来说，100%在办公室办公已经成为过去式。</p><p>&nbsp;</p><p></p><h2>未来展望</h2><p></p><p>在过去的几年中，平台工程已经取得了长足的进步，而且还没有放缓的迹象。随着工具生态和社区的日益发展，会有很多的资源可以帮助你启动平台工程或让你在事业上更进一步。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/platform-engineering-primer/\">https://www.infoq.com/articles/platform-engineering-primer/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/video/rt4CNXXlVbzmRCPumWPe\">从 DevOps 到平台工程｜InfoQ《极客有约》</a>\"</p><p><a href=\"https://www.infoq.cn/article/5JuuFYFFNH0WT2aDJgA3\">平台工程的 2023：助力云原生重构研发组织文化与组织架构</a>\"</p><p><a href=\"https://www.infoq.cn/article/q6JwiNRJIZ8C0h1WCHVQ\">Puppet 2023 DevOps 现状报告：平台工程有助于提升开发效率</a>\"</p><p></p>",
    "publish_time": "2023-05-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "GitHub增加SBOM导出功能，使其更易于符合安全性需求",
    "url": "https://www.infoq.cn/article/vFFev9f5RB0sS7RHtSz0",
    "summary": "<p>GitHub<a href=\"https://github.blog/2023-03-28-introducing-self-service-sboms/\">宣布了</a>\"一项新的SBOM导出特性，旨在作为安全合规工作流和工具的一部分。GitHub声称，这项新特性能够让我们轻松导出符合<a href=\"https://ntia.gov/page/software-bill-materials\">NTIA</a>\"标准的SBOM。</p><p></p><p>用户可以通过一些不同的方式导出SBOM，可以手动进行，也可以使用自动化的进程。要手动生成SBOM，可以访问仓库的<a href=\"https://www.infoq.com/news/2017/10/github-dependency-graph-security/\">依赖关系图</a>\"，然后点击新的Export SBOM按钮。这样会按照SPDX格式创建一个机器可读的SBOM。</p><p></p><p><a href=\"https://spdx.dev/\">SPDX</a>\"是软件包数据交换（Software Package Data Exchange）的缩写，是一种专门用来描述软件物料清单（bill of materials）的开放格式，包括依赖关系、许可证、版权和安全引用。有许多工具（包括<a href=\"https://spdx.dev/tools-commercial/\">商业的</a>\"和<a href=\"https://spdx.dev/tools-community/\">开源</a>\"的）可以用来消费SPDX文件，以验证、分析或将其转换为其他格式。</p><p></p><p>除了使用GitHub Web UI，还可以使用<a href=\"https://github.com/advanced-security/gh-sbom\">GitHub CLI的扩展</a>\"或<a href=\"https://docs.github.com/code-security/supply-chain-security/understanding-your-software-supply-chain/using-the-dependency-submission-api#generating-and-submitting-a-software-bill-of-materials-sbom\">GitHub Action</a>\"来导出SBOM。</p><p></p><p>GitHub CLI扩展可以通过运行gh ext install advanced-security/gh-sbom来安装。然后，通过gh sbom -l命令可以按照SPDX格式输出 SBOM，而gh sbom -l -c命令则会使用 CycloneDX 格式。</p><p></p><p>作为GitHub CLI的替代方案，我们还可以在构建时使用GitHub Action来输出SBOM。GitHub提供了自己的<a href=\"https://github.com/marketplace/actions/sbom-generator-action\">GitHub Action</a>\"，以便于从依赖关系图中导出SBOM。如果愿意的话，还可以使用微软的<a href=\"https://github.com/microsoft/sbom-tool\">sbom-tool</a>\"，或者基于<a href=\"https://github.com/anchore/syft\">Syft</a>\"的<a href=\"https://github.com/marketplace/actions/anchore-sbom-action\">Anchore SBOM Action</a>\"。</p><p></p><p>该公司说，未来还可以通过特定的REST API导出SBOM。</p><p></p><p>GitHub提供的另一种可能性是将现有的SBOM上传到一个仓库，以生成依赖关系图。这对于那些不愿意公开在软件中使用的所有依赖关系的组织来说是很有用的。生成了依赖关系图之后，就有可能收到Dependabot对仓库及其依赖关系发现的漏洞发出的告警。</p><p></p><p>很重要的一点需要注意，SBOM虽然是许多行业和<a href=\"https://www.nist.gov/itl/executive-order-14028-improving-nations-cybersecurity\">美国政府</a>\"的要求，但它只是用来保护软件供应链的众多工具之一。它本身并不能解决依赖关系可能违规的问题，但它可以帮助你更好地衡量所选的实现方案所带来的安全风险，并且能够帮助你理解通过为系统引入给定的依赖都信任了哪些人。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/04/GitHub-sbom-export/\">GitHub Adds SBOM Export to Make it Easier to Comply with Security Requirements</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/O8q1Eiz7Yil74398dp1i\">仅仅发布&nbsp;SBOM&nbsp;是不够的，质量和可用性因项目而异</a>\"</p><p><a href=\"https://www.infoq.cn/video/9b1ia67t2O8Ll1IZ02wa\">Linux 基金会《软件材料清单（SBOM） 与网络安全准备度》报告深度解读 |InfoQ《极客有约》</a>\"</p><p></p>",
    "publish_time": "2023-05-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "美团大规模容器集群降本增效实践",
    "url": "https://www.infoq.cn/article/ZNVFzqNAKzIRpW4Oyftj",
    "summary": "<p></p><blockquote>本文整理自美团集群调度系统负责人谭霖在QCon 2022 北京站的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2023/beijing/presentation/4392\">大规模容器集群降本增效实践</a>\"”。</blockquote><p></p><p>&nbsp;</p><p>随着 Kubernetes 广泛应用，越来越多公司基于 Kubernetes 落地集群调度系统。在集群规模增长同时，社区方案局限性也逐步体现，集群规模受限和只支持按照资源配额调度等问题日渐凸显。其中，资源配额调度问题又是制约集群资源利用率提升的最大痛点。</p><p>&nbsp;</p><p>本文将从真实业务场景出发，讲述如何通过落地动态负载均衡服务，实现动态实时调整容器资源实例，解决资源配额调度问题带来的集群内物理机负载不均衡的问题，并保障业务服务质量，充分实现云计算成本优化。</p><p>&nbsp;</p><p>今天我将分享四个主要内容。第一个主题是在线集群调度，我将介绍它的核心挑战以及为什么各个大型企业都需要投入大量人力去解决这个复杂的问题。第二个主题是美团系统LAR，它专门解决应用资源利用率提升的问题，并且已经在我们的实际落地中得到了应用。第三部分将分享在线集群调度的运营实践。最后一部分分享集群调度的演进思考。</p><p>&nbsp;</p><p></p><h2>在线集群调度的核心挑战</h2><p></p><p></p><h3>集群调度领域介绍</h3><p></p><p>在线集群调度系统又叫做数据中心资源调度系统，目标是将离散和异构的服务器整合到一个弹性资源池中，以提高数据中心的资源利用率，并为业务方提供自动化的运维能力。通过网络和存储系统，我们可以根据业务需求动态地分配计算实例，从而降低整体计算成本。</p><p>&nbsp;</p><p>如右图所示，我们将服务器整合成一个弹性资源池，并与网络和存储结合，例如像快存文件存储，&nbsp;vxlan大二层网络等技术。最后，我们可以按需为业务分配计算实例，并通过集群的规模效应降低公司的计算成本。</p><p>&nbsp;</p><p></p><h3>集群调度的挑战与困难</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/3805000d10cce57f1cc2c0280e8c5e07.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>至于集群调度系统，它的复杂性体现在哪些方面呢？首先，随着云原生化、微服务化和容器化趋势的普及，所有业务的容器都需要在集群调度系统的管控下运行。在这种情况下，确保整个应用的服务质量是至关重要的。同时，我们也面临着一个巨大的挑战，即如何在保证服务质量的同时提高资源利用率，以降低公司的成本。因此，需要在这两个方面之间找到一个平衡点。</p><p>&nbsp;</p><p>其次，在美团我们有外卖、酒旅、优选等各种业务，每个业务都有其个性化的调度需求。为了快速满足他们的需求，同时不阻塞迭代过程，我们需要以一个高效的方式为其提供服务。</p><p>&nbsp;</p><p>还需要解决的问题是，有状态服务如数据库&nbsp;RDS 或 KV 存储进行云原生化改造。与无状态服务相比，这些有状态服务的状态机复杂，无法像无状态服务一样快速扩容或销毁。因此，我们需要提供自动化变更和异常处理的能力，以解决这些挑战，这也是集群调度需要解决的问题之一。</p><p>&nbsp;</p><p>今天我们的重点会关注成本，也就是如何在提高在线集群资源利用率和同时保障应用服务质量上做出一些工作。</p><p>&nbsp;</p><p></p><h3>行业资源利用率水平</h3><p></p><p>&nbsp;</p><p>2020年全球数据中心的服务器总量达到1800万台，并且正以每年100万台的速度增长。&nbsp;统计数据表明，目前全球数据中心资源利用率仅为10%~20%，如此低的资源利用率意味着数据中心大量的资源浪费，导致目前数据中心的成本效率极低。</p><p>&nbsp;</p><p>这么低的资源利用率意味着什么呢？它意味着如果您购买了一个 64 核的服务器，实际上可能只有 6.4 核被平均使用，最多只有 10 核。这意味着我们的成本或产出在财务上没有得到充分利用。</p><p>&nbsp;</p><p></p><h3>美团业务特征</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/943d39ee42f3d10a2200540caa56bdea.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>美团的外卖业务，特别是在午高峰和晚高峰时间点，在线需求呈现出非常明显的双峰特征。对于这样的业务，我们分配给它的资源在时间和空间维度上都存在相当程度的闲置。如何充分利用这些资源是一个非常关键的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>资源利用率提升难点</h2><p></p><p>&nbsp;</p><p>资源利用存在一个关键的矛盾：对于在线服务来说，一台物理机上的资源是有限的，特别是对于共享资源，例如&nbsp;CPU、内存、磁盘IO、网络IO以及末级缓存，它们的使用会相互干扰。这在离线环境中可能会好一些，但在线服务的情况特别明显，这使得服务质量非常容易受到影响。如果我们提高资源利用率水平，但服务质量不能得到保证，那么一切都是有问题的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/526ddc7deb2229c9ff65d89efcf72e98.png\" /></p><p></p><p>这里是在实际生产环境中，我们经常遇到的一些高频问题。第一个是热点宿主的问题。尽管集群利用率可能不是特别高，但我们通常会发现一些物理机或宿主机的利用率较高，并成为整个集群利用率的瓶颈。如果我们扩容到这些热点宿主上，就会出现服务质量问题，导致很多业务不再希望我们扩容。</p><p>&nbsp;</p><p>第二个场景涉及到集中调度。之前我提到的是以集群维度为维度，但是现在我们要切换到业务维度。如果某个业务实例出现了问题，那么它可能会被调度到某个特定的宿主机或者交换机下面，一旦出现硬件故障，这会给业务带来非常大的影响。尽管我们现在采用的是多实例的方式，但如果出现了10%、20%或者30%的实例故障或损坏，那么业务依然难以承受。</p><p>&nbsp;</p><p>第三个场景，即资源抢占的问题。业务肯定都有核心链路和非核心链路，或者我们称之为旁路。在调度时，我们通常没有明确区分核心链路和非核心链路，因此很可能会将它们调度在同一台机器上。这可能会导致核心链路和非核心链路之间的资源冲突，甚至可能出现非核心链路占用资源来抢核心链路的情况。这些问题在我们的实际生产环境中经常出现。</p><p></p><h3>问题根因：原生调度能力无法满足大规模调度需求</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/1f/1f8cceda02dd68086a4c40b7ea0760c4.png\" /></p><p></p><p>这个问题的根本原因是什么呢？实际上，尽管大家现在都使用 Kubernetes 作为集群调度系统，但它本质上还是不够完善的，无法满足大规模场景下的某些问题。具体而言，首先在大规模调度中有一些非常重要的问题，例如打散需求。就像刚才所说的，我们的服务需要按照故障域进行分布，而故障域可能会涉及到宿主机、交换机、IDC，甚至是Region等概念。为了满足这些需求，我们需要考虑服务和服务之间以及服务内部的一些亲和性和反亲和性的调度需求。此外，在容量需求方面，为了确保业务的可用性，业务往往会申请冗余资源。但是，我们应该如何尽可能地减少这些冗余资源呢？因为如果每个业务都有 20% 的冗余，那么整个成本将会非常高。另外，从集群本身来看，业务可能会出现突然增长的需求，因此我们需要保障一定的容量，以满足业务整体弹性的需求。</p><p>&nbsp;</p><p>综上所述，当前的集群调度系统存在以下几个问题。第一，集群调度系统存在资源水位失真的问题。我们仍然按照业务申请的资源数量进行分配，而没有考虑实际负载情况。这也是问题一的根因。第二，单个集群始终存在局限性，尽管规模可能达到四五千台，但当资源水位上升时，我们依然会发现在单个群集中无法找到合适的宿主机。第三，优先级不区分的问题，这不仅存在于集群调度中，也存在于内核单机层面。例如，资源能力、隔离能力不足以及内存溢出后的不可控等问题。目前业内通常只能通过资源冗余来保证服务质量，但这也带来了成本较高、利用率偏低的问题。</p><p>&nbsp;</p><p></p><h3>解决方案：全局视角下的精细化管控</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/208ad9ff6670f87b4b978eec91720ffa.png\" /></p><p></p><p>在美团，我们的解决方案是在全局视角下进行精细化管控，以解决整个调度均衡的问题。我们通过掌握集群的实际负载数据，实现全局调度。同时，我们通过提升单机资源隔离能力，实现资源化管控。具体来说，我们采用了多层多集群的维度，但对业务进行屏蔽，并在单集群维度上增强了调度器和优化调度策略。此外，我们还提升了节点和内核的隔离能力。最后，集群运营数据中心帮助我们收集集群和服务的状态数据，并为我们提供数据支持。</p><p>&nbsp;</p><p></p><h2>集群负载自动管理系统&nbsp;（ LAR ，Load Auto Regulator）&nbsp;落地实践</h2><p></p><p>&nbsp;</p><p>下面我将详细介绍美团落地的一个集群负载自动管理系统——Load Auto Regulator（LAR）。 LAR 主要作用在单机层面，可以解决 Kubernetes 默认将单机资源按照业务申请分配&nbsp;quota 的问题。由于容器的&nbsp;quota 维度相对有限，当所有容器一起增强整体单机资源时，很容易导致服务质量无法保障。此外，从业务角度来看，对&nbsp;quota 是否合理也难以评估。因此，就会出现业务申请多少，我们给多少。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4a1b125df19f6117e31a130d9fb01ef.png\" /></p><p></p><p>LAR的核心思路并不复杂。我们将单机资源按照优先级分为多个资源池，包括高、低优先级和离线资源池等。然后，我们将业务容器归属到不同的资源池，以平衡服务质量和成本。由于业务负载是动态变化的，因此在分配资源池资源时，我们支持动态流动的过程。如果某个资源池的资源过剩或不足，将导致资源浪费或短缺。</p><p>&nbsp;</p><p></p><h3>举个例子：资源池CPU资源变化</h3><p></p><p>&nbsp;</p><p>例如，如果我们有一个单机64核和两个Numa服务器，我们会将其分为三个资源池，包括高优先级的核心业务资源池（Pool&nbsp;0），普通在线业务资源池（Pool&nbsp;1）和离线业务资源池（Pool&nbsp;2）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a363ab189b809dc99c683d3f0508a728.png\" /></p><p></p><p>可以看到，最开始的Pool 0有12核，Pool 1有20核，Pool 2有32核。随着在线业务负载的增加，我们优先减少离线业务资源池的容量。因此，核心业务资源池有了16核，普通在线资源池使用了34核，离线资源池缩减为24核。</p><p>&nbsp;</p><p></p><h3>集群负载自动管理系统架构</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac00ebcce764d313f352533df43bfbf1.png\" /></p><p></p><p>&nbsp;</p><p>上图是我们LAR的整体架构，它是基于Kubernetes原生架构进行的研发和扩展。如整体架构所示，在Kubernetes的基础上，一方面修改和扩展了Scheduler和Controller，同时还新增了两个功能模块，分别是Recommender和QosAdapter。总体而言，我们的产品分为三个核心模块，分别是分级资源池调度Scheduler；Recommender负载分析预测；以及QosAdapter，它的作用是保证服务质量。接下来，我将详细介绍这些功能模块的工作原理。</p><p>&nbsp;</p><p></p><h3>核心概念1：分级池化资源</h3><p></p><p>&nbsp;</p><p>首先是分级池化资源。我们将单节点的资源池分为 0、1、2 三类资源池，每个池子的优先级依次降低。每个池子都会部署多个容器实例。例如，在图中，我们一开始有 8 个容器。当发现有空闲资源时，我们会给Pool 2增加一些资源并且调度离线容器上来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/dac9348ee6da739b68d388cbffaab6ad.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>核心概念2：动态资源视图</h3><p></p><p>我们有一个动态资源视图。从&nbsp;schedule 的层面，它不再只是一个单机，例如，64</p><p><img src=\"https://static001.geekbang.org/infoq/92/92d5a7a2b59b0fdbb497c1a3edf06d6d.png\" /></p><p>&nbsp;个资源可以被看作是3个资源池，每个资源池都对应一些资源。因此，在调度时，可以按需进行调度。我们有一些策略，例如，所有资源池类分配资源之和是整个节点的可用资源。如图所示，当负载升高时，我们会向下抢占低优先级的资源。当负载降低时，我们会把闲置的资源提供给低优先级的资源去复用。</p><p>&nbsp;</p><p></p><h3>核心概念3：QoS服务质量保障机制</h3><p></p><p>&nbsp;</p><p>我们还有一个&nbsp;QoSAdapter，它的主要作用是服务质量保障。我们现在主要关注的是&nbsp;CPU、Memory、磁盘&nbsp;IO 和网络&nbsp;IO 资源。我们通过一些调整手段，在&nbsp;CPU 上进行配额、调度优先级和&nbsp;CPU 绑核等操作；在 Memory&nbsp;上，我们用容量 OOM Score 来调整进程，还利用&nbsp;Cgroup V2&nbsp;提供的能力，基于此做了 Memory&nbsp;带宽的限制。但是，我们还发现 OOM 不太可控，所以我们落地了 Memory&nbsp;提前回收的机制。在磁盘上，我们正在实施包括磁盘的限速调度算法等措施。在网络上，我们也有带宽限速和带宽共享，通过&nbsp;quota 限制。</p><p><img src=\"https://static001.geekbang.org/infoq/62/6214529d6af242481f785d1c9c807d7b.png\" /></p><p></p><p></p><h3>再举个例子：容器负载异常处理策略</h3><p></p><p>&nbsp;</p><p>最后，作为兜底方案，我们还有容器驱逐策略。举例来说，在右侧的图中，其主要工作原理是通过cadvisor、node-exporter等监控来源，收集整个宿主和容器的负载情况。然后，我们会使用各种策略进行计算，例如资源释放、资源抢占，以及强制抢占、CPU 降级甚至 pod 驱逐等策略。接下来，我们会将这些策略下发给操作系统，以保证服务质量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec3bde56f0f73a2777ca1050febb1d6e.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>核心概念4：基于负载预测资源用量</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/95eea1c943c86eb55a42b509ce02fc0a.png\" /></p><p></p><p>接下来，会再详细介绍一下&nbsp;Recommender的具体工作。它的主要任务是：在每天凌晨，根据历史负载进行预测计算，包括宿主、服务和容器层面。此外，在每次调度之前，我们会实时进行负载校准，每 10 秒一次。根据调度策略，我们将其下发到每个宿主机 Kubernete node 等，并在进行调度时使用这些策略，以提高资源池资源保障的稳定性。</p><p>&nbsp;</p><p></p><h3>LAR落地效果</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/22897569238ec070128361ba77542965.png\" /></p><p></p><p>&nbsp;</p><p>让我们来看一下 LAR 的实际效果。橙色部分表示非 LAR 集群的资源利用率情况，而蓝色则是&nbsp;LDR 的整体利用率情况。可以看到，通过使用&nbsp;LDR，我们的日均利用率提高了近 10 个百分点，效果非常显著。</p><p>&nbsp;</p><p></p><h2>在线集群调度的运营实践</h2><p></p><p>&nbsp;</p><p>虽然刚才我们讲了很多技术上的细节，但实际上集群调度还是一个重运营工作。因此，接下来我想分享一些集群调度的运营实践。</p><p>&nbsp;</p><p></p><h3>资源运营模型</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d3639fa890c5bd42ceb8e2f0dec9caf.png\" /></p><p></p><p>&nbsp;</p><p>在我的理解中，我把资源利用率这个模型进行了拆解，因为我们不仅要关注业务使用情况，还要更细致地看待资源的组成部分。当然，业务使用资源是分子中最重要的一部分。但实际上，分母还包括分配给业务但未被使用的资源以及未分配出去的资源。除此之外，系统损耗和兜底资源也是分母的组成部分。</p><p>&nbsp;</p><p>为了提升资源利用率，我们需要准确地了解这些资源的成本和使用情况。只有这样，我们才能通过技术和运营手段提升资源调度能力。因此，我们建立了一个运营大盘，包括业务端到端成本、业务配额利用率、集群利用率、数据分配率数据和碎片率数据等。</p><p>&nbsp;</p><p></p><h3>又又举个例子：集群资源运营挑战</h3><p></p><p>&nbsp;</p><p>举个例子，很多公司在管理集群节点资源时都需要进行预算提报。然而，预算提报往往依赖于经验和估计，特别是在当前情况下，很难做到精准预测。因此，仅仅将业务上报的预算加和获得总量往往会导致与实际使用情况偏差较大。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3b8b930e7baba4d157e6795335e7931.png\" /></p><p></p><p>&nbsp;</p><p>在我看来更合理的方式是什么呢？第一，我们希望缩短预算周期，因为按年计算很难准确估算。第二，我们需要对业务的实际使用资源进行预测，并监控实时资源水平，以便进行动态释放和回收。举个例子，如果您使用公有云，最好的方法可能是将闲置资源退还给云服务提供商。在美团这样的私有云环境中，我们可以在不同的业务之间进行资源调配，并向搜索推荐等业务提供闲置资源，因为资源越多，精度就越高。</p><p>&nbsp;</p><p></p><h3>集群运营数据中心（Cluster Pulse）</h3><p></p><p>&nbsp;</p><p>对于美团这样的私有云环境，我们需要及时反馈给供应链，以调整采购计划。因此，我们的关键在于建立一个集群运营数据中心，将资源成本和服务质量的数据运营体系化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c4df283c9c690b305f2d3aac4403adb.png\" /></p><p></p><p>通常我们在监控业务时会看到业务数据或基础设施数据。我们的目标是将集群、容器、数组和业务的数据进行一体化，以便向下钻取到特定容器的利用率和资源情况。我们可以将这些数据汇聚到一个结算单元中，例如在美团可能是一个较大的业务单元，以更好地分析业务实际使用资源情况，以及是否存在资源闲置问题。通过这些数据支持，我们可以制定一些以服务质量为优先的个性化调度策略，包括基于负载的一次调度和二次调度等。我们的整体流程如右图所示：首先分析数据，然后优化调度策略，以便更好地管理整个集群状态。</p><p>&nbsp;</p><p></p><h3>运营指标大盘</h3><p></p><p>&nbsp;</p><p>我们还有一个运营指标大盘，其中包括资源碎片率、分配率和利用率等。在图右侧，我们还包括了一些重调度、资源缓冲池和业务Appkey打散等情况。这里面的内容还有很多，就不详细展开了。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19a83856ca322bce02d26cd8c2e782e5.png\" /></p><p></p><p></p><h2>在线集群调度的演进思考</h2><p></p><p>&nbsp;</p><p></p><h3>集群调度器核心逻辑</h3><p></p><p>&nbsp;</p><p>在我看来，集群调度本质上是一个数据驱动的决策过程。如何获取高质量的数据非常关键，这对于提高集群利用率至关重要。但是，这不仅仅是集群调度可以解决的，它还需要与系统内核和服务可观测性等多个维度进行深度协同建设。</p><p>&nbsp;</p><p></p><h3>在线集群调度能力</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f037a2afb8c06a8fc571e5c010e19a3e.png\" /></p><p></p><p>如何构建一个优秀的在线集群呢？我认为可以分为以下几个步骤。首先，我们可以基于历史负载进行一定的超卖。其次，像我之前提到的，我们可以根据服务优先级对应用进行分类，并引入动态资源调整机制。第三，与硬件和软件相结合的能力一起获取服务信息。例如，利用eBPF分析微服务，或者使用CPI来分析服务是否受到干扰。第四步是离线混部。我们需要创建立统一的资源视图，以更好地规划公司的整体资源。第五步，我们需要预测集群和服务资源的需求，并充分利用公有云的资源。</p><p>&nbsp;</p><p></p><h3>集群调度待解决的需求</h3><p></p><p>&nbsp;</p><p>接下来，我认为需要解决以下几个紧迫问题。首先是裸金属容器和虚拟机的融合。虽然在公有云中，容器普遍嵌套在虚拟机内，但在美团这样的私有云场景中，虚拟化需求仍然存在。我们不能将所有实例迁移到容器中。因此，我们需要同时提供虚拟机和容器场景，并解决这个问题。在美团，我们之前使用OpenStack来管理虚拟机，使用Kubernetes来管理容器。但由于这两套系统都是我们团队负责的，我们发现在整个行业降本增效的趋势下，使用两套系统来管理成本非常高，无论是管理成本还是资源成本。因此，我们现在正在尝试将虚拟机和容器统一纳管到Kubernetes中。</p><p>&nbsp;</p><p>其次，我们需要解决的是边缘场景的需求。美团正在探索边缘场景的业务，但是与中心机房相比，边缘场景面临更多的限制，比如弱网，宿主机规模较小等，我们需要解决这些挑战。</p><p>&nbsp;</p><p>第三个问题是，国家正在推广东数西算。一旦这个项目真正落地，我们将面临大量跨机房、跨地域的数据同步。目前，专线成本非常高，我们需要减少跨地域机房的数据同步，并充分利用西部地区的资源，为我们的在线机房提供可用资源，以降低成本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/4249ecc59049fefcbfa83946afde5523.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>这些是我们要持续探索的问题，我希望可以与大家分享这些热门研究领域的话题。谢谢大家！</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/kdiaQ7QATUtZXj12eYuu\">亚马逊 CEO：我们将重视“降本增效”</a>\"</p><p><a href=\"https://xie.infoq.cn/article/b14696263a58d961487c9368e\">多云之下，京东云的降本增效之道</a>\"</p>",
    "publish_time": "2023-05-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ASPLOS 23系统顶会论文 Plugsched ：安全、高效的多场景调度器热升级详解",
    "url": "https://www.infoq.cn/article/p4jahFJoZxwN5mL8kzYu",
    "summary": "<p></p><h2>前言</h2><p></p><p></p><p>阿里云基础软件/达摩操作系统实验室的论文&nbsp;\"Efficient Scheduler Live Update for Linux Kernel with Modularization\"&nbsp;被系统领域著名会议&nbsp; 28th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'2（3)&nbsp;录用为长论文（Full Paper）。</p><p></p><p>ASPLOS 会议在体系结构领域被认为是顶会之一，同时也是系统领域最重要的会议，被中国计算机协会 CCF 认证为 A 类会议，同时在阿里内部会议列表中也被选为 1 类会议。目前已经举办至 28 届，吸引了来自学术及产业界的大量投稿。该会议的参会人员不乏来自国外顶级高校如 MIT、UC Berkeley、University of Chicago、普林斯顿以及国内清北交大等知名院校。</p><p></p><p>该会议均为学术相关论文，没有专门的 Industry track，在 2022 年设置了三次投稿机会，分别是 spring cycle、summer cycle 和 fall summer。这篇论文投稿了summer cycle，并获得了三位评委 accept 的评价，因此给了一次进行 revision 的机会。在 revision 阶段，Plugsched 实现了评委提出的意见，获得了一致肯定，最终被接收在 ASPLOS 23 会议上，论文所在的 session 是 OS/Virtualization。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c42cbe9e61bde3437916ca138fbd12d3.png\" /></p><p></p><p>文章主要介绍了专门针对<a href=\"http://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;mid=2247487041&amp;idx=1&amp;sn=594dbf0e3006bd067701821b3128e159&amp;chksm=f9aa3c98ceddb58e250830749af358d50ab43fe72bb2ba31ddf40a6f7c9e30b7279022ba751c&amp;scene=21#wechat_redirect\">调度器的热升级系统 Plugsched</a>\"，该项目已经开源（https://github.com/aliyun/plugsched）。调度器是操作系统的一个重要组成部分，与 Linux 内核紧密耦合。生产环境下的云经常承载各种工作负载，这些工作负载需要不同的调度器来实现高性能。因此，在不重启操作系统的情况下在线地升级调度器的能力对生产环境至关重要。然而，目前已有的在线热升级技术只适用于细粒度的功能级升级或需要额外的约束，如限定在微内核中。当前的技术并不能够支持对内核调度子系统的热升级。</p><p></p><p>因此，我们提出了 Plugsched 来实现调度器的实时更新，其中有两个关键的创新点。首先，利用模块化的思想，Plugsched 将调度器与 Linux 内核解耦，成为一个独立的模块；其次，Plugsched 使用数据重建技术将状态从旧的调度器迁移到新的调度器。这个方案可以直接应用于生产环境中的 Linux 内核调度器，而无需修改内核代码。与目前的函数级实时更新方案不同，Plugsched 允许开发者通过重建技术更新整个调度器子系统并修改内部调度器数据。此外，还引入了优化的堆栈检测方法，以进一步有效减少因更新而导致的停机时间。我们使用三个新的调度器进行升级，来评估 Plugsched 的性能。实验结果表明，Plugsched 能够有效地更新内核调度器，并且停机时间（halting time）小于几十毫秒。</p><p></p><p>详细论文内容参见或者会议官网议程（https://asplos-conference.org/program/）。下面将对论文进行详细的解析：</p><p></p><h2>背景</h2><p></p><p></p><h3>Linux 调度器</h3><p></p><p></p><p>Linux 调度器通过决定一个旧的进程何时停止以及一个新的进程何时运行，来提供抢占式的多任务。调度器是内核中最基础和最复杂的子系统之一。调度器子系统包含超过 27K LOC 和 63 个文件，而且还有许多复杂的子系统与调度器交互，因此它与内核的结构和功能是紧密耦合的。虽然默认的 Linux 调度器很强大，但应用程序往往需要不同的调度器来实现最佳性能。例如，微秒级的工作负载，如memcached、silo，对调度器的开销极为敏感。</p><p></p><p>即使是单一的应用类型，不同的工作负载在不同的调度策略下表现往往有很大差异性。云计算和数据中心的工作负载更多的是混部应用，因此调度器的效率更加重要。随着工作负载的混部化，对延迟敏感的任务和长期运行的批处理任务在同一个节点上共存，为满足各种应用的 SLO，需要考虑到新的调度器策略。为了实现高性能并适应不同的场景，应该允许用户将定制的调度策略整合到生产环境中，例如，使用上游的\"core scheduling\"patch（将近 4KLOC 的代码修改）来修复潜在的性能问题。实际生产环境的经验还表明，在无服务器（Serverless）计算场景中，当容器部署密度较高时，通过应用新的负载平衡机制，CPU 调度开销可以从 6% 降低到 1.5%。相反，如果部署密度低，该机制会对 CPU 开销产生负面影响。</p><p></p><p>在生产环境中实现调度器的热升级是有利于整体性能的，但直接的方法是给操作系统（OS）打补丁，重新启动操作系统，重新初始化硬件，并重新启动上层应用。传统的内核升级技术，如虚拟机迁移，是通过 checkpoint-and-restore（C/R）的方法，内核的停机时间随着进程/线程的数量线性增加。虽然在目前的云原生时代，服务器上有超过 10,000 个线程是很常见的场景，但根据我们的测量，超过 10,000 个线程带来的停机时间仍然达到数分钟。</p><p></p><p>对于延迟敏感的应用（如金融交易应用）和长期运行的应用来说，漫长的停机时间是不可接受的。虽然对延迟敏感的应用只能承受较短的停机时间，因此生产环境上需要有一种调度器的热升级方法能够将停机时间减少到几十毫秒的水平。</p><p></p><h3>State-of-the-art 热升级工具的缺陷</h3><p></p><p></p><p>学术界和工业界都提出了一些内核热升级的解决方案，以保证内核代码更新到最新的安全版本，而不破坏服务器的现有状态（如重新启动）。图 1 显示了现有内核热升级技术的特点。X 轴的 \"表现力 \"是指开发人员可以升级内容的范围，而 Y 轴的 \"通用性 \"是指新的功能在没有额外约束的情况下在现有 Linux 内核上实用性。为了设计一个合适的调度器热升级方案，我们应该考虑三个主要的约束：</p><p></p><p>1）不应该带来内核代码的额外修改。</p><p>2）必须能够部署在商业化 Linux 服务器中。</p><p>3）不应该只局限在虚拟化环境中使用。</p><p></p><p>如图 1 所示，大多数实时补丁工具都是功能级的解决方案（例如，livepatch、kpatch、Ksplice），这些工具主要解决要升级的代码变化很少（少于 100 LOC）的小型安全补丁，并且缺乏对数据状态迁移的考虑。尽管这些工具对大多数情况都是通用的，但它们的表达能力很差，不能支持集成可能修改许多函数和数据结构的新调度器功能。例如，集成上文提到的 \"core scheduling \"功能需要修改内核中的 19 个文件（超过 4000 行的代码）。</p><p></p><p>一些先前的工作也支持组件级的实时热升级。PROTEOS 在很大程度上依赖于微内核操作系统的特点，这意味着它不能够被用于商业化 Linux 服务器。针对 KVM 而言，由于 KVM 是一个独立的内核模块，Orthus 通过双 KVM 和 VM 之间迁移实现了 KVM 的热升级。ghOSt 将调度策略迁移到了用户空间进程，它提供了状态封装、通信和行动机制，使用户空间进程可以表达复杂的调度策略。相应地，在用户空间切换到一个新的调度器也很容易。然而，热升级的范围仅限于调度器类（如CFS），而且用户空间和内核空间之间的额外通信开销（上下文切换等），增加了端到端的延迟（原始论文中报告的延迟是正常调度器延迟的 2 倍左右）。另外需要注意的是，运行中的服务器的内核也应该被修改以适应 ghOSt。而操作系统级的实时更新解决方案，比如 VM-PHU，依赖于虚拟机管理器（VMM），它只适用于更新虚拟机中的客户内核。我们可以总结出 State-of-the-art 的热升级解决方案不能满足调度器热升级的需求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f369c11893eac7a335d7e71d6d1a3e5.png\" /></p><p></p><h2>系统设计原则</h2><p></p><p></p><p>基于以上的讨论，Plugsched 的定位也如图 1 所示。Plugsched 是一个调度器的实时更新系统，支持组件级的实时更新（即充分的表达能力）。Plugsched 适用于多种场景，包括几乎所有的商用 Linux 服务器和灵活的生产环境（即高通用性）。除了表达性和通用性之外，这样的解决方案还必须满足短停机时间和安全性的要求，即要求该解决方案不应损害应用程序或带来潜在的安全问题。最后，在开发和部署阶段，它应该是用户友好的。</p><p></p><p>充分的表达能力：Plugsched 支持组件级的实时热升级。具体来说，它可以用一个新的调度器代替旧的调度器，必须能够支持修改大量的函数和数据结构。同时，数据状态能够从旧的调度器自动迁移到新的调度器上。</p><p></p><p>高通用性：Plugsched可用于大部分的商用 Linux 服务器，而且没有额外的约束条件。其他一些实时更新解决方案需要内核在虚拟机中运行或被限制在微内核中，因此它们的使用场景是有限的，而 Plugsched 可以在各种包括物理机和虚拟机的生产环境中使用。另外一点需要注意的是，要直接将 Plugsched 部署到运行旧内核的服务器上，应避免修改内核代码。</p><p></p><p>同时实现短停机时间和安全性：停机时间是衡量效率的主要标准。在调度器升级期间，应用程序不应该有明显的服务中断。为了确保安全性，调度器应该被视为一个有明确边界的完整模块，因此它可以以 \"全有或全无（all-or-nothing）\"的方式被更新（也就是原子性）。此外，还需要一个有效的检查机制来决定内核是否可以安全地更新，并且该机制不应该带来大量的开销。</p><p></p><p>易于使用：Plugsched 简化了调度器的开发和部署流程，因此开发者可以专注于调度策略的开发。在热升级中，Plugsched 提供常见的热升级辅助技术，如堆栈检查、数据更新、内存池等等，开发者可以直接使用它们，而不需要任何调整。在Plugsched 中，调度器的升级和回滚可以通过安装和卸载调度器 RPM 包轻松完成，所以非常方便开发者在生产环境中快速验证他们的调度策略。</p><p></p><h2>设计总览</h2><p></p><p></p><p>图 2 给出了 Plugsched 的整体架构。整个架构包括三个部分：调度器模块化的预处理阶段；调度器开发者的开发阶段；以及应用新调度器的部署阶段。</p><p></p><p>在预处理阶段，使用根据内核版本手工生成的配置，Plugsched 在编译阶段自动收集调度器的相关信息，如函数和数据结构的符号（1）。然后运行边界分析算法，确定调度器和内核之间的清晰边界，并将函数进一步细分为内部函数、接口函数和外部函数（2）。在代码提取阶段，Plugsched 通过代码生成技术将调度器相关的代码（如内部函数和接口函数）重新组织到一个单独的目录中，调度器被解耦到一个新的模块中（3）。在开发阶段，开发者可以像往常一样在该目录下自由开发调度器或集成新的功能（4），然后编译新的调度器，生成一个 RPM 包（5）。在部署阶段，Plugsched 需要找到一个能够安全更新的机会（即 state quiescence），用新的调度器替换原来的调度器。因此，Plugsched 需要首先调用 stop_machine 来停止所有线程。然后，Plugsched 使用堆栈检测来检查所有的内核堆栈，并检查是否有不安全的线程( 6 )。在堆栈检查完成后，Plugsched 将原函数的 prologue （即第一条指令）替换为重定向到新函数的跳转指令（函数重定向，7）。同时，Plugsched将数据从旧的调度器迁移到新的调度器，通过基于重建的方法保证数据是最新的（数据更新，8）。从 stop_machine 返回后，原调度器被旁路。需要注意的是，回滚阶段与安装新调度器的部署阶段基本相同。Plugsched 还提供了一些机制（例如 sidecar）来适应生产环境中的实际部署（详情见论文）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a98ee1252b71e1ad8902abcc174e70f3.png\" /></p><p></p><p>Plugsched 有两个关键的创新点，即调度器模块化和数据重建。具体来说，模块化允许将调度器与内核解耦，这是实现热升级的先决条件。另外我们进一步利用数据重建将数据状态从旧调度器迁移到新调度器。这使得热升级前后的调度器之间能够协作，并确保调度状态的正确性。有了这样一个通用的解决方案，调度器可以针对具体场景进行定制，并支持任何功能的整合，甚至是更换一个全新的调度器。实现调度器模块化并非易事，因为一些热点路径如调度器中的 try_to_wake_up()、schedule()、cpu_set()、scheduler_tick() 和 scheduler_ipi() 等函数被许多其他内核子系统如 time、cgroup 和 IPI 直接调用。如果不进行详尽的代码分析，调度器的模块化可能会导致内核故障。</p><p></p><p>因此，我们提出一种边界分析方法，通过静态分析来分析模块的依赖关系。因此，模块化使开发者能够根据调度器和内核之间清晰的边界自由开发自定义的调度器。在之前的工作中，数据状态在更新内核的时候没有被维护。一些工作使用 object 或 Shadow data structure 来避免数据状态迁移。然而，由于不同的数据结构（上百规模场景下）会涉及大量不同的状态，这些解决方案并不具有足够的通用性。我们观察到调度器中的数据结构状态可以通过操作 running queue 和 task_struct 的两个稳定的 API 进行重构。基于这一观察，Plugsched 在调度器更新后安全地重建了数据结构。</p><p></p><p>我们在 Plugsched 中引入了以下四种设计范式，以支持组件级调度器的热升级，而不会产生安全问题，并避免额外的开销：</p><p></p><p>1）Plugsched 在预处理阶段使用函数调用依赖图来确定调度器和其余内核之间的明确边界。它利用 GCC 插件将调度器的相关代码提取到子目录中，作为新调度器的 Codebase。</p><p>2）为了减少停机时间，Plugsched 从两个维度充分挖掘多核计算的性能，优化了堆栈检测方法。</p><p>3）Plugsched 替换了原函数的 prologue，重定向到新函数。这个有一个特例，为了处理不能直接替换的上下文切换逻辑（即__schedule()函数），Plugsched 对__schedule() 函数的分割使用了栈迁移和 ROP 等技术。</p><p>4) Plugsched 提出了一种基于重建的方法来处理数据状态迁移 。</p><p></p><h2>实验效果</h2><p></p><p></p><p>如图 3 所示，我们总结了潜在的解决方案，并将 Plugsched 和它们进行了比较。Plugsched 安全地支持组件级的热升级，而不需要内核修改代码以及额外的开销，与此同时它也有很好的表达能力和通用性，开发者可以自由地开发新的调度器并部署到商用的 Linux 服务器上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d904c407ef898403145a9d0b8ea86e1d.png\" /></p><p></p><p>Plugsched 已经部署在生产环境的 4000 台服务器上，为这些机器提供安全和高效的组件级别热升级能力，并且仅仅带来很短的停机时间。在我们的实验中，选择了几个 Linux 社区中的新功能和一个全新的 Tiny Scheduler 来取代原生的内核调度器，实验结果表明热升级带来的停机时间只有 2.1∼2.6𝑚𝑠，回滚的停机时间为1.8∼2.5𝑚𝑠。即使在工作负荷很重的情况下，停机时间的增量也不超过 61%。与 kpatch（一个常用的内核补丁工具）相比，Plugsched 在提供组件级热升级能力支持的同时，将升级和回滚的停机时间分别减少了约 24% 和 26%。另外一方面，Plugsched 的整套解决方案也可以扩展到其他 Linux 子系统之中用来做热升级，比如 eBPF、内存管理、网络等子系统。</p><p></p><p>特别感谢阿里云基础软件，达摩院操作系统实验室的同事以及上海交通大学老师在整体方案设计和实现上的帮助和指导，文章的最早一版曾经投稿过 OSDI' 22，经过修改历时一年半，最终被系统领域顶会 ASPLOS 认可！同时也特别感谢所有贡献作者的建议和指导！这是阿里集团的原创性工作，并且项目已经开源在 Github（https://github.com/aliyun/plugsched）、Gitee（https://gitee.com/anolis/plugsched）以及龙蜥社区（https://openanolis.cn/sig/Cloud-Kernel/doc/614810558709245780）。感兴趣的同学多多交流，多多关注。</p>",
    "publish_time": "2023-05-01 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]