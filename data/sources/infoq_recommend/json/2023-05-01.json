[
  {
    "title": "平台工程应知应会",
    "url": "https://www.infoq.cn/article/TbxS2My9fOIL1GMpHjKD",
    "summary": "<p>随着社区和工具生态的快速发展，平台工程显然会继续存在下去。但是，和任何（相对）新的趋势一样，平台工程仍有许多未解之谜。最近，Humanitec发布了第一份《平台工程现状报告》——以下是该报告的关键内容以及平台工程的重要趋势。</p><p>&nbsp;</p><p>平台工程是软件工程中最大的趋势之一。<a href=\"https://platformcon.com/\">PlatformCon</a>\" 2022 ——有史以来的第一个平台工程大会——吸引了超过6000名参会者，而当地的聚会小组也有数千名会员，很显然，这一趋势将持续下去。</p><p>&nbsp;</p><p>自然，与软件工程的任何趋势一样，会有许多社区内外的人都好奇的未解问题。第一份《<a href=\"https://humanitec.com/whitepapers/state-of-platform-engineering-report-volume-1\">平台工程现状报告</a>\"》试图回答这些持续存在的问题，提供关于平台工程现状的信息，并从社区的不同角落汇编了一些关键资源。</p><p>&nbsp;</p><p>所以，话不多说，以下是你需要知道的关于平台工程的关键内容。</p><p>&nbsp;</p><p></p><h2>为什么平台工程是一个重要的趋势？</h2><p></p><p></p><p>在过去十年中，复杂的微服务架构、Kubernetes这样的技术以及IaC（基础设施即代码）这样的方法已经成为行业标准。现在，即使是简单的任务也要求开发人员对工具链有一个端到端的理解，这极大地增加了他们的认知负荷，并导致组织效率低下，比如影子运维。</p><p>&nbsp;</p><p>认知负荷是指“工作记忆中消耗的脑力劳动总和。”这一理论是由心理学家John Sweller在20世纪80年代末首次提出的。<a href=\"https://my.chartered.college/impact_article/%20cognitive-load-theory-and-its-application-in-the-classroom/\">该理论认为</a>\"，“当学习任务超出工作记忆容量时，学习就会受阻。”</p><p>&nbsp;</p><p>2019年，Manuel Pais和Matthew Skelton认为，认知负荷是当前DevOps设置问题的主要副产品。这一发现推动他们创建了<a href=\"https://teamtopologies.com/\">团队拓扑</a>\"，其中有一个专门的产品团队构建了抽象层或平台，使开发人员可以免受底层技术复杂性的影响，降低了他们的认知负荷。</p><p>&nbsp;</p><p>团队拓扑设计用于防止某些DevOps设置中反复出现的特定的<a href=\"https://web.devopstopologies.com/\">反模式</a>\"。在一种被我们称为“影子运维”的反模式中——<a href=\"https://humanitec.com/whitepapers/2021-devops-setups-benchmarking-report#whitepaper\">Humanitec 2021年开展的DevOps基准研究</a>\"证实了这一点——工程组织断定他们不再需要专职的运维专家（或至少减少人数），并将这些任务转给开发人员。通常，这会导致缺乏经验的开发人员向高级后端工程师寻求帮助。这会导致组织中一些最有才能、最重要的资源错配，工程团队的生产力因此降低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa10b57af867113aa536fc46cc516751.jpeg\" /></p><p></p><p>受<a href=\"https://www.youtube.com/watch?v=zUpYEhaUJnM&amp;t=138s\">Daniel Bryant在PlatformCon 2022大会的演讲</a>\"启发</p><p>&nbsp;</p><p>因此，一些组织被迫寻找更好的方法。他们成立了平台工程团队来构建内部开发平台，或者将技术和工具组合在一起，降低开发人员的认知负荷，而又不用抽象出底层技术的上下文。<a href=\"https://platformengineering.org/talks-library/netflix-platform-console-to-unify-engineering-experience\">Netflix</a>\"利用他们的平台解决了开发人员在管理多种服务和软件、了解存在哪些工具以及在不同工具之间切换上下文时所面临的挑战。<a href=\"https://platformengineering.org/talks-library/internal-platform-enterprise-courtney-kissler\">Nordstrom</a>\"打造了一个平台，成了“自助服务的北极星”。他们的平台帮助消除了环境准备过程的瓶颈，人们不再需要登录到生产环境。这些变化使组织能够更快地前进。<a href=\"https://platformengineering.org/talks-library/sunrise-zalandos-internal-developer-platform\">Zalando</a>\"利用他们的平台统一了开发体验，默认启用了合规性增强，并随着时间的推移改进了公司的运营方式。</p><p>&nbsp;</p><p>这些只是其中几个成功的平台项目的示例。Puppet <a href=\"https://puppet.com/resources/report/2020-state-of-devops-report\">2020</a>\"年和<a href=\"https://puppet.com/resources/report/2021-state-of-devops-report\">2021</a>\"年的“DevOps现状报告”以及Humanitec 2021年的“DevOps基准报告”等研究表明，使用IDP的团队，其DORA指标显示出了更高程度的DevOps演进和效能。</p><p>&nbsp;</p><p>那么到底什么是平台工程呢？</p><p>&nbsp;</p><p>虽然就平台工程的通用定义达成一致很困难，但这项任务很重要。我<a href=\"https://platformengineering.org/blog/what-is-platform-engineering\">最近将平台工程定义为</a>\"：</p><p></p><blockquote>设计和构建工具链及工作流的学科，为云原生时代的软件工程组织提供自助服务能力。平台工程师提供一个通常称为“内部开发平台”的集成化产品，涵盖应用程序整个生命周期的操作需求。</blockquote><p></p><p>&nbsp;</p><p>内部开发平台由专门的平台团队构建。团队拓扑讨论团队是什么，不是什么。平台团队不是SRE的替身，也不是处理开发者工单的影子运营团队。它不应该关注个人挑战或生产可靠性。相反，它应该以产品思维为驱动，将开发者视为平台客户。</p><p>&nbsp;</p><p></p><h2>平台工程原则和最佳实践</h2><p></p><p></p><p>在平台团队开始构建他们的产品之前，需要定义一个明确的使命宣言来指导整个过程。宣言应符合组织的总体目标，并主动定义好平台团队在组织中的角色。它应该可以激励你的工程师。Hashicorp平台工程基础设施总监<a href=\"https://medium.com/@michael.roy.galloway/your-platform-org-needs-a-purpose-heres-how-to-find-it-64874b082d80\">Michael Galloway</a>\"有一个很好的总结：“它应该是有感染力且鼓舞人心的……它应该是简单而有意义的。”</p><p>&nbsp;</p><p>你可以从定义目标着手。这可能包括：在不增加认知负荷的情况下，实现所需程度的开发人员自助服务，或者在不强迫开发人员学习<a href=\"https://score.dev/blog/workload-centric-over-infrastructure-centric-development\">以基础设施为中心的端到端技术</a>\"的情况下，实现所需的操作成本节省。在此之后，你可能会得出这样的结论：“我们的任务是标准化工作流程，改善开发体验，加快创新周期，缩短工程组织的上市时间。”但这是描述性的，不够鼓舞人心。完善你的使命宣言，实现良好的平衡。例如：“我们的使命是构建开发者喜爱的平台，提高他们的创新速度。”</p><p>&nbsp;</p><p>构建平台的最佳方法是遵循产品方法。通过用户研究、用户反馈征集，从利益相关者那里获得内部支持，平台团队可以全面了解开发人员的痛点和整个组织共同面临的挑战。它可以确定开发人员需要什么功能，并构建一个可以实现这些解决方案的黄金路径。但平台之旅并未就此止步。成功的平台团队会与开发人员保持开放的沟通，并度量工程KPI，确保开发人员采用了平台，而平台使开发人员的工作变得更轻松。</p><p>&nbsp;</p><p><a href=\"https://medium.com/wise-engineering/platform-engineering-kpis-6a3215f0ee14\">Lambros Charissis</a>\"分享了Wise的平台工程团队如何制定了一套可执行的KPI来识别挑战和度量绩效。他们使用KPI树来明确KPI：生产力、交货时间、部署频率、开发人员满意度、稳定性、效率和风险。</p><p>&nbsp;</p><p>你还应该监控平台采用KPI，如市场份额（开发人员是否在实际的工作中使用了你的平台，还是更喜欢直接使用他们自己的技术和工具？），使用平台节省的时间，以及团队之间关系的改善（例如开发人员和运营人员）。定期进行调查，看看平台是否改善了开发体验。</p><p>&nbsp;</p><p>平台团队和他们提供的黄金路径是将这些复杂的设置整合在一起的粘合剂。然而，由于平台团队通常不交付面向客户的特性，所以许多组织错误地将它们视为成本中心。平台团队应该争取相关涉众团体的内部支持，确保其内部平台项目可以长久地运行下去。</p><p>&nbsp;</p><p>最后，也许也是最重要的一点，成功的平台团队不会重新发明轮子。有时，最好的解决方案必须从头构建。然而，许多企业正在将他们的资源投入到针对特定的用途构建有效的解决方案。为了解决各种各样的问题，工具领域正在快速发展。只要有可能，平台团队就可以通过裁剪现成的解决方案来节省时间并创造更多的价值。</p><p>&nbsp;</p><p></p><h2>平台工具生态</h2><p></p><p></p><p>平台工程是指构建内部开发平台。Kaspar von Grünberg解释说，内部开发平台是“平台工程团队将所有技术和工具整合在一起，铺就一条黄金道路。”但是，都有哪些技术和工具呢？如何确定哪些最适合你的组织？</p><p>&nbsp;</p><p>构建平台的方法有很多种，没有通用的解决方案，尽管有些供应商可能会做出这样的承诺。有些提供PaaS类体验的供应商（如Heroku）可能适合小型初创公司，特别是那些想要快速推出POC或MVP的公司。</p><p>&nbsp;</p><p>但对于大型企业或中型工程组织来说，在大多数情况下，这样的PaaS是行不通的，因为他们仍然需要支持他们的棕地设置，而不能启用绿地。</p><p>&nbsp;</p><p>然而，平台工程专家通常会建议平台开发遵循产品方法。也就是说，你不一定要从头开始。现在有很多平台工具产品，其中许多是开源的，也有一部分是专有的。为了节省组织的时间和金钱，只要可能，就要考虑购买和定制预构建的解决方案。这就为平台团队留出了时间和资源，让他们专注于定制组织需要的特性。</p><p>&nbsp;</p><p>为了更好地理解所有选项，平台工程现状报告设法对平台工具的生态进行了划分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/40/4040331424fa336747103b9e7fe3c23f.jpeg\" /></p><p></p><p>在一份题为“<a href=\"https://www.gartner.com/document/4017457\">软件工程领导者改善开发体验指南</a>\"”的报告中，Gartner总结了指导平台工程建设的三个关键思想：</p><p>通过构建内部开发平台来减少开发人员的认知负荷、艰苦的重复性手工工作，从而改善开发体验。平台不会强制使用特定的工具集或方法——它是为了让开发人员可以更容易地构建和交付软件，同时又不用抽象出底层核心服务中实用的差异化功能。平台工程团队将平台视为产品（供开发人员使用），并设计出可以自助使用的平台。</p><p>&nbsp;</p><p>这里还需要澄清下内部开发平台和内部开发门户之间的区别。Gartner的报告也提到了这个问题。作者写道：</p><p></p><p></p><blockquote>内部开发门户是开发人员发现和访问内部开发平台能力的界面。</blockquote><p></p><p>&nbsp;</p><p>内部开发门户提供了一个具有服务目录功能的UI，开发人员可以使用它访问底层的内部开发平台。反过来，内部开发平台消除了艰苦的重复性工作。</p><p>&nbsp;</p><p>构建开发门户并不能解决降低开发体验的底层配置和工作流问题。这就是内部开发平台和开发门户成为理想组合的原因。</p><p>&nbsp;</p><p></p><h2>社区与职业</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1be0cee92ecf4325e19eba90de713ff1.jpeg\" /></p><p></p><p>平台工程社区始于2021年，当时在奥斯汀和柏林有多个聚会小组。如今，它在全球有19个聚会小组，活跃的平台工程师超过10000名。这种社区发展势头表明，组织越来越重视平台工程。</p><p>&nbsp;</p><p>平台工程也是一条很有前途的职业道路。许多公司开始意识到，构建内部开发平台可以增加他们的利润。此外，构建内部开发平台的人平均工资也高于对应的DevOps岗位。</p><p>&nbsp;</p><p>尽管如此，像平台工程师或平台负责人这样的职位还不是特别常见。平台工程现状报告发现，大多数正在构建组织平台的受访者都拥有像高级软件工程师、IT架构师、首席工程师或高级DevOps工程师等职位。因此，虽然行业开始拥抱平台工程，但要恰当地定义平台角色仍然很困难。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78ae67c584157dce599450a5a3f44b50.jpeg\" /></p><p></p><p>考虑从事平台工程师职业的人经常会询问社区，他们需要掌握哪些技术。根据我们的调查，超过一半的受访者表示，他们最关注Docker、Kubernetes、IaC和CI/CD。数据库、存储和网络技术也在清单上，不过是在不到四分之一的受访者的清单上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7f805254d9d48f5fc77f4cd45dc9add3.jpeg\" /></p><p></p><p>说到工作安排灵活性方面，平台工程走在了前列。超过80%的美国受访者是完全远程工作。欧洲受访者在完全远程办公和混合办公之间取得了更好的平衡。总的来说，100%在办公室办公已经成为过去式。</p><p>&nbsp;</p><p></p><h2>未来展望</h2><p></p><p>在过去的几年中，平台工程已经取得了长足的进步，而且还没有放缓的迹象。随着工具生态和社区的日益发展，会有很多的资源可以帮助你启动平台工程或让你在事业上更进一步。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/platform-engineering-primer/\">https://www.infoq.com/articles/platform-engineering-primer/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/video/rt4CNXXlVbzmRCPumWPe\">从 DevOps 到平台工程｜InfoQ《极客有约》</a>\"</p><p><a href=\"https://www.infoq.cn/article/5JuuFYFFNH0WT2aDJgA3\">平台工程的 2023：助力云原生重构研发组织文化与组织架构</a>\"</p><p><a href=\"https://www.infoq.cn/article/q6JwiNRJIZ8C0h1WCHVQ\">Puppet 2023 DevOps 现状报告：平台工程有助于提升开发效率</a>\"</p><p></p>",
    "publish_time": "2023-05-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "GitHub增加SBOM导出功能，使其更易于符合安全性需求",
    "url": "https://www.infoq.cn/article/vFFev9f5RB0sS7RHtSz0",
    "summary": "<p>GitHub<a href=\"https://github.blog/2023-03-28-introducing-self-service-sboms/\">宣布了</a>\"一项新的SBOM导出特性，旨在作为安全合规工作流和工具的一部分。GitHub声称，这项新特性能够让我们轻松导出符合<a href=\"https://ntia.gov/page/software-bill-materials\">NTIA</a>\"标准的SBOM。</p><p></p><p>用户可以通过一些不同的方式导出SBOM，可以手动进行，也可以使用自动化的进程。要手动生成SBOM，可以访问仓库的<a href=\"https://www.infoq.com/news/2017/10/github-dependency-graph-security/\">依赖关系图</a>\"，然后点击新的Export SBOM按钮。这样会按照SPDX格式创建一个机器可读的SBOM。</p><p></p><p><a href=\"https://spdx.dev/\">SPDX</a>\"是软件包数据交换（Software Package Data Exchange）的缩写，是一种专门用来描述软件物料清单（bill of materials）的开放格式，包括依赖关系、许可证、版权和安全引用。有许多工具（包括<a href=\"https://spdx.dev/tools-commercial/\">商业的</a>\"和<a href=\"https://spdx.dev/tools-community/\">开源</a>\"的）可以用来消费SPDX文件，以验证、分析或将其转换为其他格式。</p><p></p><p>除了使用GitHub Web UI，还可以使用<a href=\"https://github.com/advanced-security/gh-sbom\">GitHub CLI的扩展</a>\"或<a href=\"https://docs.github.com/code-security/supply-chain-security/understanding-your-software-supply-chain/using-the-dependency-submission-api#generating-and-submitting-a-software-bill-of-materials-sbom\">GitHub Action</a>\"来导出SBOM。</p><p></p><p>GitHub CLI扩展可以通过运行gh ext install advanced-security/gh-sbom来安装。然后，通过gh sbom -l命令可以按照SPDX格式输出 SBOM，而gh sbom -l -c命令则会使用 CycloneDX 格式。</p><p></p><p>作为GitHub CLI的替代方案，我们还可以在构建时使用GitHub Action来输出SBOM。GitHub提供了自己的<a href=\"https://github.com/marketplace/actions/sbom-generator-action\">GitHub Action</a>\"，以便于从依赖关系图中导出SBOM。如果愿意的话，还可以使用微软的<a href=\"https://github.com/microsoft/sbom-tool\">sbom-tool</a>\"，或者基于<a href=\"https://github.com/anchore/syft\">Syft</a>\"的<a href=\"https://github.com/marketplace/actions/anchore-sbom-action\">Anchore SBOM Action</a>\"。</p><p></p><p>该公司说，未来还可以通过特定的REST API导出SBOM。</p><p></p><p>GitHub提供的另一种可能性是将现有的SBOM上传到一个仓库，以生成依赖关系图。这对于那些不愿意公开在软件中使用的所有依赖关系的组织来说是很有用的。生成了依赖关系图之后，就有可能收到Dependabot对仓库及其依赖关系发现的漏洞发出的告警。</p><p></p><p>很重要的一点需要注意，SBOM虽然是许多行业和<a href=\"https://www.nist.gov/itl/executive-order-14028-improving-nations-cybersecurity\">美国政府</a>\"的要求，但它只是用来保护软件供应链的众多工具之一。它本身并不能解决依赖关系可能违规的问题，但它可以帮助你更好地衡量所选的实现方案所带来的安全风险，并且能够帮助你理解通过为系统引入给定的依赖都信任了哪些人。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/04/GitHub-sbom-export/\">GitHub Adds SBOM Export to Make it Easier to Comply with Security Requirements</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/O8q1Eiz7Yil74398dp1i\">仅仅发布&nbsp;SBOM&nbsp;是不够的，质量和可用性因项目而异</a>\"</p><p><a href=\"https://www.infoq.cn/video/9b1ia67t2O8Ll1IZ02wa\">Linux 基金会《软件材料清单（SBOM） 与网络安全准备度》报告深度解读 |InfoQ《极客有约》</a>\"</p><p></p>",
    "publish_time": "2023-05-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "美团大规模容器集群降本增效实践",
    "url": "https://www.infoq.cn/article/ZNVFzqNAKzIRpW4Oyftj",
    "summary": "<p></p><blockquote>本文整理自美团集群调度系统负责人谭霖在QCon 2022 北京站的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2023/beijing/presentation/4392\">大规模容器集群降本增效实践</a>\"”。</blockquote><p></p><p>&nbsp;</p><p>随着 Kubernetes 广泛应用，越来越多公司基于 Kubernetes 落地集群调度系统。在集群规模增长同时，社区方案局限性也逐步体现，集群规模受限和只支持按照资源配额调度等问题日渐凸显。其中，资源配额调度问题又是制约集群资源利用率提升的最大痛点。</p><p>&nbsp;</p><p>本文将从真实业务场景出发，讲述如何通过落地动态负载均衡服务，实现动态实时调整容器资源实例，解决资源配额调度问题带来的集群内物理机负载不均衡的问题，并保障业务服务质量，充分实现云计算成本优化。</p><p>&nbsp;</p><p>今天我将分享四个主要内容。第一个主题是在线集群调度，我将介绍它的核心挑战以及为什么各个大型企业都需要投入大量人力去解决这个复杂的问题。第二个主题是美团系统LAR，它专门解决应用资源利用率提升的问题，并且已经在我们的实际落地中得到了应用。第三部分将分享在线集群调度的运营实践。最后一部分分享集群调度的演进思考。</p><p>&nbsp;</p><p></p><h2>在线集群调度的核心挑战</h2><p></p><p></p><h3>集群调度领域介绍</h3><p></p><p>在线集群调度系统又叫做数据中心资源调度系统，目标是将离散和异构的服务器整合到一个弹性资源池中，以提高数据中心的资源利用率，并为业务方提供自动化的运维能力。通过网络和存储系统，我们可以根据业务需求动态地分配计算实例，从而降低整体计算成本。</p><p>&nbsp;</p><p>如右图所示，我们将服务器整合成一个弹性资源池，并与网络和存储结合，例如像快存文件存储，&nbsp;vxlan大二层网络等技术。最后，我们可以按需为业务分配计算实例，并通过集群的规模效应降低公司的计算成本。</p><p>&nbsp;</p><p></p><h3>集群调度的挑战与困难</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/3805000d10cce57f1cc2c0280e8c5e07.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>至于集群调度系统，它的复杂性体现在哪些方面呢？首先，随着云原生化、微服务化和容器化趋势的普及，所有业务的容器都需要在集群调度系统的管控下运行。在这种情况下，确保整个应用的服务质量是至关重要的。同时，我们也面临着一个巨大的挑战，即如何在保证服务质量的同时提高资源利用率，以降低公司的成本。因此，需要在这两个方面之间找到一个平衡点。</p><p>&nbsp;</p><p>其次，在美团我们有外卖、酒旅、优选等各种业务，每个业务都有其个性化的调度需求。为了快速满足他们的需求，同时不阻塞迭代过程，我们需要以一个高效的方式为其提供服务。</p><p>&nbsp;</p><p>还需要解决的问题是，有状态服务如数据库&nbsp;RDS 或 KV 存储进行云原生化改造。与无状态服务相比，这些有状态服务的状态机复杂，无法像无状态服务一样快速扩容或销毁。因此，我们需要提供自动化变更和异常处理的能力，以解决这些挑战，这也是集群调度需要解决的问题之一。</p><p>&nbsp;</p><p>今天我们的重点会关注成本，也就是如何在提高在线集群资源利用率和同时保障应用服务质量上做出一些工作。</p><p>&nbsp;</p><p></p><h3>行业资源利用率水平</h3><p></p><p>&nbsp;</p><p>2020年全球数据中心的服务器总量达到1800万台，并且正以每年100万台的速度增长。&nbsp;统计数据表明，目前全球数据中心资源利用率仅为10%~20%，如此低的资源利用率意味着数据中心大量的资源浪费，导致目前数据中心的成本效率极低。</p><p>&nbsp;</p><p>这么低的资源利用率意味着什么呢？它意味着如果您购买了一个 64 核的服务器，实际上可能只有 6.4 核被平均使用，最多只有 10 核。这意味着我们的成本或产出在财务上没有得到充分利用。</p><p>&nbsp;</p><p></p><h3>美团业务特征</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/94/943d39ee42f3d10a2200540caa56bdea.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>美团的外卖业务，特别是在午高峰和晚高峰时间点，在线需求呈现出非常明显的双峰特征。对于这样的业务，我们分配给它的资源在时间和空间维度上都存在相当程度的闲置。如何充分利用这些资源是一个非常关键的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>资源利用率提升难点</h2><p></p><p>&nbsp;</p><p>资源利用存在一个关键的矛盾：对于在线服务来说，一台物理机上的资源是有限的，特别是对于共享资源，例如&nbsp;CPU、内存、磁盘IO、网络IO以及末级缓存，它们的使用会相互干扰。这在离线环境中可能会好一些，但在线服务的情况特别明显，这使得服务质量非常容易受到影响。如果我们提高资源利用率水平，但服务质量不能得到保证，那么一切都是有问题的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/526ddc7deb2229c9ff65d89efcf72e98.png\" /></p><p></p><p>这里是在实际生产环境中，我们经常遇到的一些高频问题。第一个是热点宿主的问题。尽管集群利用率可能不是特别高，但我们通常会发现一些物理机或宿主机的利用率较高，并成为整个集群利用率的瓶颈。如果我们扩容到这些热点宿主上，就会出现服务质量问题，导致很多业务不再希望我们扩容。</p><p>&nbsp;</p><p>第二个场景涉及到集中调度。之前我提到的是以集群维度为维度，但是现在我们要切换到业务维度。如果某个业务实例出现了问题，那么它可能会被调度到某个特定的宿主机或者交换机下面，一旦出现硬件故障，这会给业务带来非常大的影响。尽管我们现在采用的是多实例的方式，但如果出现了10%、20%或者30%的实例故障或损坏，那么业务依然难以承受。</p><p>&nbsp;</p><p>第三个场景，即资源抢占的问题。业务肯定都有核心链路和非核心链路，或者我们称之为旁路。在调度时，我们通常没有明确区分核心链路和非核心链路，因此很可能会将它们调度在同一台机器上。这可能会导致核心链路和非核心链路之间的资源冲突，甚至可能出现非核心链路占用资源来抢核心链路的情况。这些问题在我们的实际生产环境中经常出现。</p><p></p><h3>问题根因：原生调度能力无法满足大规模调度需求</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/1f/1f8cceda02dd68086a4c40b7ea0760c4.png\" /></p><p></p><p>这个问题的根本原因是什么呢？实际上，尽管大家现在都使用 Kubernetes 作为集群调度系统，但它本质上还是不够完善的，无法满足大规模场景下的某些问题。具体而言，首先在大规模调度中有一些非常重要的问题，例如打散需求。就像刚才所说的，我们的服务需要按照故障域进行分布，而故障域可能会涉及到宿主机、交换机、IDC，甚至是Region等概念。为了满足这些需求，我们需要考虑服务和服务之间以及服务内部的一些亲和性和反亲和性的调度需求。此外，在容量需求方面，为了确保业务的可用性，业务往往会申请冗余资源。但是，我们应该如何尽可能地减少这些冗余资源呢？因为如果每个业务都有 20% 的冗余，那么整个成本将会非常高。另外，从集群本身来看，业务可能会出现突然增长的需求，因此我们需要保障一定的容量，以满足业务整体弹性的需求。</p><p>&nbsp;</p><p>综上所述，当前的集群调度系统存在以下几个问题。第一，集群调度系统存在资源水位失真的问题。我们仍然按照业务申请的资源数量进行分配，而没有考虑实际负载情况。这也是问题一的根因。第二，单个集群始终存在局限性，尽管规模可能达到四五千台，但当资源水位上升时，我们依然会发现在单个群集中无法找到合适的宿主机。第三，优先级不区分的问题，这不仅存在于集群调度中，也存在于内核单机层面。例如，资源能力、隔离能力不足以及内存溢出后的不可控等问题。目前业内通常只能通过资源冗余来保证服务质量，但这也带来了成本较高、利用率偏低的问题。</p><p>&nbsp;</p><p></p><h3>解决方案：全局视角下的精细化管控</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/208ad9ff6670f87b4b978eec91720ffa.png\" /></p><p></p><p>在美团，我们的解决方案是在全局视角下进行精细化管控，以解决整个调度均衡的问题。我们通过掌握集群的实际负载数据，实现全局调度。同时，我们通过提升单机资源隔离能力，实现资源化管控。具体来说，我们采用了多层多集群的维度，但对业务进行屏蔽，并在单集群维度上增强了调度器和优化调度策略。此外，我们还提升了节点和内核的隔离能力。最后，集群运营数据中心帮助我们收集集群和服务的状态数据，并为我们提供数据支持。</p><p>&nbsp;</p><p></p><h2>集群负载自动管理系统&nbsp;（ LAR ，Load Auto Regulator）&nbsp;落地实践</h2><p></p><p>&nbsp;</p><p>下面我将详细介绍美团落地的一个集群负载自动管理系统——Load Auto Regulator（LAR）。 LAR 主要作用在单机层面，可以解决 Kubernetes 默认将单机资源按照业务申请分配&nbsp;quota 的问题。由于容器的&nbsp;quota 维度相对有限，当所有容器一起增强整体单机资源时，很容易导致服务质量无法保障。此外，从业务角度来看，对&nbsp;quota 是否合理也难以评估。因此，就会出现业务申请多少，我们给多少。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4a1b125df19f6117e31a130d9fb01ef.png\" /></p><p></p><p>LAR的核心思路并不复杂。我们将单机资源按照优先级分为多个资源池，包括高、低优先级和离线资源池等。然后，我们将业务容器归属到不同的资源池，以平衡服务质量和成本。由于业务负载是动态变化的，因此在分配资源池资源时，我们支持动态流动的过程。如果某个资源池的资源过剩或不足，将导致资源浪费或短缺。</p><p>&nbsp;</p><p></p><h3>举个例子：资源池CPU资源变化</h3><p></p><p>&nbsp;</p><p>例如，如果我们有一个单机64核和两个Numa服务器，我们会将其分为三个资源池，包括高优先级的核心业务资源池（Pool&nbsp;0），普通在线业务资源池（Pool&nbsp;1）和离线业务资源池（Pool&nbsp;2）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a363ab189b809dc99c683d3f0508a728.png\" /></p><p></p><p>可以看到，最开始的Pool 0有12核，Pool 1有20核，Pool 2有32核。随着在线业务负载的增加，我们优先减少离线业务资源池的容量。因此，核心业务资源池有了16核，普通在线资源池使用了34核，离线资源池缩减为24核。</p><p>&nbsp;</p><p></p><h3>集群负载自动管理系统架构</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac00ebcce764d313f352533df43bfbf1.png\" /></p><p></p><p>&nbsp;</p><p>上图是我们LAR的整体架构，它是基于Kubernetes原生架构进行的研发和扩展。如整体架构所示，在Kubernetes的基础上，一方面修改和扩展了Scheduler和Controller，同时还新增了两个功能模块，分别是Recommender和QosAdapter。总体而言，我们的产品分为三个核心模块，分别是分级资源池调度Scheduler；Recommender负载分析预测；以及QosAdapter，它的作用是保证服务质量。接下来，我将详细介绍这些功能模块的工作原理。</p><p>&nbsp;</p><p></p><h3>核心概念1：分级池化资源</h3><p></p><p>&nbsp;</p><p>首先是分级池化资源。我们将单节点的资源池分为 0、1、2 三类资源池，每个池子的优先级依次降低。每个池子都会部署多个容器实例。例如，在图中，我们一开始有 8 个容器。当发现有空闲资源时，我们会给Pool 2增加一些资源并且调度离线容器上来。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/dac9348ee6da739b68d388cbffaab6ad.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>核心概念2：动态资源视图</h3><p></p><p>我们有一个动态资源视图。从&nbsp;schedule 的层面，它不再只是一个单机，例如，64</p><p><img src=\"https://static001.geekbang.org/infoq/92/92d5a7a2b59b0fdbb497c1a3edf06d6d.png\" /></p><p>&nbsp;个资源可以被看作是3个资源池，每个资源池都对应一些资源。因此，在调度时，可以按需进行调度。我们有一些策略，例如，所有资源池类分配资源之和是整个节点的可用资源。如图所示，当负载升高时，我们会向下抢占低优先级的资源。当负载降低时，我们会把闲置的资源提供给低优先级的资源去复用。</p><p>&nbsp;</p><p></p><h3>核心概念3：QoS服务质量保障机制</h3><p></p><p>&nbsp;</p><p>我们还有一个&nbsp;QoSAdapter，它的主要作用是服务质量保障。我们现在主要关注的是&nbsp;CPU、Memory、磁盘&nbsp;IO 和网络&nbsp;IO 资源。我们通过一些调整手段，在&nbsp;CPU 上进行配额、调度优先级和&nbsp;CPU 绑核等操作；在 Memory&nbsp;上，我们用容量 OOM Score 来调整进程，还利用&nbsp;Cgroup V2&nbsp;提供的能力，基于此做了 Memory&nbsp;带宽的限制。但是，我们还发现 OOM 不太可控，所以我们落地了 Memory&nbsp;提前回收的机制。在磁盘上，我们正在实施包括磁盘的限速调度算法等措施。在网络上，我们也有带宽限速和带宽共享，通过&nbsp;quota 限制。</p><p><img src=\"https://static001.geekbang.org/infoq/62/6214529d6af242481f785d1c9c807d7b.png\" /></p><p></p><p></p><h3>再举个例子：容器负载异常处理策略</h3><p></p><p>&nbsp;</p><p>最后，作为兜底方案，我们还有容器驱逐策略。举例来说，在右侧的图中，其主要工作原理是通过cadvisor、node-exporter等监控来源，收集整个宿主和容器的负载情况。然后，我们会使用各种策略进行计算，例如资源释放、资源抢占，以及强制抢占、CPU 降级甚至 pod 驱逐等策略。接下来，我们会将这些策略下发给操作系统，以保证服务质量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec3bde56f0f73a2777ca1050febb1d6e.png\" /></p><p></p><p>&nbsp;</p><p></p><h3>核心概念4：基于负载预测资源用量</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/95eea1c943c86eb55a42b509ce02fc0a.png\" /></p><p></p><p>接下来，会再详细介绍一下&nbsp;Recommender的具体工作。它的主要任务是：在每天凌晨，根据历史负载进行预测计算，包括宿主、服务和容器层面。此外，在每次调度之前，我们会实时进行负载校准，每 10 秒一次。根据调度策略，我们将其下发到每个宿主机 Kubernete node 等，并在进行调度时使用这些策略，以提高资源池资源保障的稳定性。</p><p>&nbsp;</p><p></p><h3>LAR落地效果</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/22897569238ec070128361ba77542965.png\" /></p><p></p><p>&nbsp;</p><p>让我们来看一下 LAR 的实际效果。橙色部分表示非 LAR 集群的资源利用率情况，而蓝色则是&nbsp;LDR 的整体利用率情况。可以看到，通过使用&nbsp;LDR，我们的日均利用率提高了近 10 个百分点，效果非常显著。</p><p>&nbsp;</p><p></p><h2>在线集群调度的运营实践</h2><p></p><p>&nbsp;</p><p>虽然刚才我们讲了很多技术上的细节，但实际上集群调度还是一个重运营工作。因此，接下来我想分享一些集群调度的运营实践。</p><p>&nbsp;</p><p></p><h3>资源运营模型</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d3639fa890c5bd42ceb8e2f0dec9caf.png\" /></p><p></p><p>&nbsp;</p><p>在我的理解中，我把资源利用率这个模型进行了拆解，因为我们不仅要关注业务使用情况，还要更细致地看待资源的组成部分。当然，业务使用资源是分子中最重要的一部分。但实际上，分母还包括分配给业务但未被使用的资源以及未分配出去的资源。除此之外，系统损耗和兜底资源也是分母的组成部分。</p><p>&nbsp;</p><p>为了提升资源利用率，我们需要准确地了解这些资源的成本和使用情况。只有这样，我们才能通过技术和运营手段提升资源调度能力。因此，我们建立了一个运营大盘，包括业务端到端成本、业务配额利用率、集群利用率、数据分配率数据和碎片率数据等。</p><p>&nbsp;</p><p></p><h3>又又举个例子：集群资源运营挑战</h3><p></p><p>&nbsp;</p><p>举个例子，很多公司在管理集群节点资源时都需要进行预算提报。然而，预算提报往往依赖于经验和估计，特别是在当前情况下，很难做到精准预测。因此，仅仅将业务上报的预算加和获得总量往往会导致与实际使用情况偏差较大。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3b8b930e7baba4d157e6795335e7931.png\" /></p><p></p><p>&nbsp;</p><p>在我看来更合理的方式是什么呢？第一，我们希望缩短预算周期，因为按年计算很难准确估算。第二，我们需要对业务的实际使用资源进行预测，并监控实时资源水平，以便进行动态释放和回收。举个例子，如果您使用公有云，最好的方法可能是将闲置资源退还给云服务提供商。在美团这样的私有云环境中，我们可以在不同的业务之间进行资源调配，并向搜索推荐等业务提供闲置资源，因为资源越多，精度就越高。</p><p>&nbsp;</p><p></p><h3>集群运营数据中心（Cluster Pulse）</h3><p></p><p>&nbsp;</p><p>对于美团这样的私有云环境，我们需要及时反馈给供应链，以调整采购计划。因此，我们的关键在于建立一个集群运营数据中心，将资源成本和服务质量的数据运营体系化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c4df283c9c690b305f2d3aac4403adb.png\" /></p><p></p><p>通常我们在监控业务时会看到业务数据或基础设施数据。我们的目标是将集群、容器、数组和业务的数据进行一体化，以便向下钻取到特定容器的利用率和资源情况。我们可以将这些数据汇聚到一个结算单元中，例如在美团可能是一个较大的业务单元，以更好地分析业务实际使用资源情况，以及是否存在资源闲置问题。通过这些数据支持，我们可以制定一些以服务质量为优先的个性化调度策略，包括基于负载的一次调度和二次调度等。我们的整体流程如右图所示：首先分析数据，然后优化调度策略，以便更好地管理整个集群状态。</p><p>&nbsp;</p><p></p><h3>运营指标大盘</h3><p></p><p>&nbsp;</p><p>我们还有一个运营指标大盘，其中包括资源碎片率、分配率和利用率等。在图右侧，我们还包括了一些重调度、资源缓冲池和业务Appkey打散等情况。这里面的内容还有很多，就不详细展开了。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19a83856ca322bce02d26cd8c2e782e5.png\" /></p><p></p><p></p><h2>在线集群调度的演进思考</h2><p></p><p>&nbsp;</p><p></p><h3>集群调度器核心逻辑</h3><p></p><p>&nbsp;</p><p>在我看来，集群调度本质上是一个数据驱动的决策过程。如何获取高质量的数据非常关键，这对于提高集群利用率至关重要。但是，这不仅仅是集群调度可以解决的，它还需要与系统内核和服务可观测性等多个维度进行深度协同建设。</p><p>&nbsp;</p><p></p><h3>在线集群调度能力</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f037a2afb8c06a8fc571e5c010e19a3e.png\" /></p><p></p><p>如何构建一个优秀的在线集群呢？我认为可以分为以下几个步骤。首先，我们可以基于历史负载进行一定的超卖。其次，像我之前提到的，我们可以根据服务优先级对应用进行分类，并引入动态资源调整机制。第三，与硬件和软件相结合的能力一起获取服务信息。例如，利用eBPF分析微服务，或者使用CPI来分析服务是否受到干扰。第四步是离线混部。我们需要创建立统一的资源视图，以更好地规划公司的整体资源。第五步，我们需要预测集群和服务资源的需求，并充分利用公有云的资源。</p><p>&nbsp;</p><p></p><h3>集群调度待解决的需求</h3><p></p><p>&nbsp;</p><p>接下来，我认为需要解决以下几个紧迫问题。首先是裸金属容器和虚拟机的融合。虽然在公有云中，容器普遍嵌套在虚拟机内，但在美团这样的私有云场景中，虚拟化需求仍然存在。我们不能将所有实例迁移到容器中。因此，我们需要同时提供虚拟机和容器场景，并解决这个问题。在美团，我们之前使用OpenStack来管理虚拟机，使用Kubernetes来管理容器。但由于这两套系统都是我们团队负责的，我们发现在整个行业降本增效的趋势下，使用两套系统来管理成本非常高，无论是管理成本还是资源成本。因此，我们现在正在尝试将虚拟机和容器统一纳管到Kubernetes中。</p><p>&nbsp;</p><p>其次，我们需要解决的是边缘场景的需求。美团正在探索边缘场景的业务，但是与中心机房相比，边缘场景面临更多的限制，比如弱网，宿主机规模较小等，我们需要解决这些挑战。</p><p>&nbsp;</p><p>第三个问题是，国家正在推广东数西算。一旦这个项目真正落地，我们将面临大量跨机房、跨地域的数据同步。目前，专线成本非常高，我们需要减少跨地域机房的数据同步，并充分利用西部地区的资源，为我们的在线机房提供可用资源，以降低成本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/4249ecc59049fefcbfa83946afde5523.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>这些是我们要持续探索的问题，我希望可以与大家分享这些热门研究领域的话题。谢谢大家！</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/kdiaQ7QATUtZXj12eYuu\">亚马逊 CEO：我们将重视“降本增效”</a>\"</p><p><a href=\"https://xie.infoq.cn/article/b14696263a58d961487c9368e\">多云之下，京东云的降本增效之道</a>\"</p>",
    "publish_time": "2023-05-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]