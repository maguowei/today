[
  {
    "title": "改善开发者体验，小公司也能在技术竞争中崭露头角",
    "url": "https://www.infoq.cn/article/vAOw5SZdk5OkQYbiS98j",
    "summary": "<p>一种改善开发者体验的策略是消除开发人员和为他们提供支持的平台团队的耗时任务和瓶颈。如何引入改变很重要，在实现变更之前理解“为什么”可以让事情进展得更加顺利。</p><p></p><p>Jessica Andersson在<a href=\"https://ndcoslo.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODg3MTUxMjAsImZpbGVHVUlEIjoiY1c0ZG9NNnRycDRHbTA1NCIsImlhdCI6MTY4ODcxNDgyMCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.zr6hTdl8Y6F9Q6OcpcXTn54Wwj5fR42-mmDfX0tqmn0\">NDC 2023大会</a>\"（于挪威首都奥斯陆举行）上谈到了如何改善小型团队的开发者体验。</p><p></p><p>Andersson解释说，开发者体验包括软件开发人员在开发和维护软件的工作中所做的所有事情，例如编写代码、测试、构建、部署、监控和维护：</p><p></p><p></p><blockquote>我经常从产品的角度来考虑这个问题，因为开发团队对产品的生命周期负责。良好的开发者体验可以让开发人员专注于让产品在竞争中脱颖而出，并为用户创造价值。</blockquote><p></p><p></p><p>他们改善开发者体验的策略是消除耗时的任务和瓶颈。首先是让开发者可以放开手脚去干。Andersson说，如果一名开发者的进展必须等待团队之外的人，那么他们就无法作为一个自主的团队，也无法完全掌握自己的产品生命周期。</p><p></p><p>接下来，从平台团队中移除耗时的任务。为了能够持续地向开发者提供更好的开发者体验，他们需要确保平台团队不会陷入无休止的升级和迁移循环中。</p><p></p><p>在将平台团队解放出来之后，重点转向了帮助开发人员移除耗时的任务，从而让他们获得更好的开发者体验。</p><p></p><p>Andersson提到，如何引入变更很重要，如果你在引入变更之前理解“为什么”要这么做，那么实现变更就会更容易。他们为开发者引入了一个完全不同的工作流程，他们认为这将是一个巨大的改进，但在开发者理解为什么以及如何做出改进之前遇到了一些阻力：</p><p></p><p></p><blockquote>从长远来看，它变成了一种非常受欢迎的工作方式，但是如果我们在实现变更之前花更多的精力在思考为什么引入变更上，那么交付可能会变得更加顺利。</blockquote><p></p><p></p><p>Andersson说，你需要让开发者相信你能够为他们带来价值，与开发者保持良好的关系是了解他们的问题以及如何改善他们日常工作的关键。</p><p></p><p>InfoQ就如何改善开发者体验采访了<a href=\"https://www.linkedin.com/in/anderssonjessica/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODg3MTUxMjAsImZpbGVHVUlEIjoiY1c0ZG9NNnRycDRHbTA1NCIsImlhdCI6MTY4ODcxNDgyMCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.zr6hTdl8Y6F9Q6OcpcXTn54Wwj5fR42-mmDfX0tqmn0\">Jessica Andersson</a>\"。</p><p></p><p>InfoQ：你在为小团队改善开发者体验方面遇到了哪些挑战？</p><p></p><p></p><blockquote>Jessica Andersson：我们不可能什么都做，也不可能一次性做完所有事情。我们每次只做一件事，简化它并做好它，一旦做到“刚刚好”，就可以转向下一件事。我们还需要注意我们引入的依赖关系和所使用的工具，所有东西都需要保持最新，处在一种不断更新却没有新改进空间的状态是一种风险。</blockquote><p></p><p></p><p>InfoQ：你能举个例子说明你是如何改善开发者体验的吗？</p><p></p><p></p><blockquote>Andersson：在为开发人员清除阻碍方面，我以我们使用DNS进行服务发现为例。DNS需要手动处理，只有两个人可以访问Cloudflare，我就是其中之一。这意味着每当开发人员想要部署新服务或更新或删除现有服务时，他们都必须来找我。这并不是我们想要的理想的工作方式，所以我们开始研究如何在Kubernetes环境中采用不同的方式来解决这个问题。我们对ExternalDNS做了调研，发现它支持通过Kubernetes资源来管理DNS记录。对于我们来说，启动和运行它真的很简单，并且迁移手动创建的现有DNS记录到ExternalDNS也很容易。开发人员很快就适应了新流程，我们在几周内就看到了这么做的好处！</blockquote><p></p><p></p><p>InfoQ：“黄金路径”或“平坦路径”能为开发人员带来什么好处？</p><p></p><p></p><blockquote>Andersson：开发人员可以重用用于解决已知问题的黄金路径，例如，对不同的应用程序使用相同的监控解决方案。另一个好处是降低认知负担，在不同的应用程序上应用相同的方法，维护多个应用程序也会变得更容易。</blockquote><p></p><p></p><p>InfoQ：对于想要改善开发者体验的小型团队或组织，你有什么建议？</p><p></p><p></p><blockquote>Andersson：我最强烈的建议是，在决定做什么之前，先评估一下你自己的组织和环境。找出你可以对开发者体验产生影响的地方，选择做一件事并加以改进！避免抄袭别人的做法，除非它对于你的组织来说也有意义。</blockquote><p></p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/developer-experience-small-org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODg3MTUxMjAsImZpbGVHVUlEIjoiY1c0ZG9NNnRycDRHbTA1NCIsImlhdCI6MTY4ODcxNDgyMCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.zr6hTdl8Y6F9Q6OcpcXTn54Wwj5fR42-mmDfX0tqmn0\">https://www.infoq.com/news/2023/06/developer-experience-small-org/</a>\"</p>",
    "publish_time": "2023-07-12 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "加速大模型应用落地最后一公里，AI基础设施如何再升级？",
    "url": "https://www.infoq.cn/article/6EWvNa5y5YP97buj3Rdw",
    "summary": "<p>AI 大模型热潮来袭，当下，越来越多的大模型在千行百业中落地应用。作为 AI 大模型的“底座”，AI 基础设施承载着顶层大模型的建设，也是大模型应用落地的关键。在算力、数据和基础软件三大 AI 基础设施中，<a href=\"https://www.infoq.cn/article/FRXOJ4dDWCK34mzO9EDM\">算力</a>\"是驱动大模型发展的底层动力，数据是训练大模型的信息基础，基础软件则是大模型应用落地的主要效率支撑。</p><p>&nbsp;</p><p>6 月 30 日，九章云极DataCanvas 举办“New AI · New Data · New Software”主题发布会，发布了“AIFS（AI Foundation Software）”、“DataPilot”两款新系列产品。</p><p>&nbsp;</p><p>在发布会上，九章云极DataCanvas 董事长方磊提到，“大模型时代需要完整基础设施升级，而不是依靠单个大模型解决所有的问题；大模型的落地将解决更困难的问题，引起更深远的影响，而它并不比小模型更容易。”</p><p>&nbsp;</p><p>大模型时代的 AI 技术依然需要“最后一公里”的结合。方磊指出，软件、模型和硬件的统一优化空间巨大，是创新最活跃的地方。强大灵活的基础软件，开放弹性的白盒模型，精通业务的专业人才，将加速实现最后一公里的跨越。</p><p></p><h2>大模型时代，AI 基础软件扮演什么角色？</h2><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/bQHRqFcQ1TlJCqHuczGR\">ChatGPT</a>\" 的爆火加速 AI 大模型应用落地。今年以来，越来越多的企业涌入 AI 大模型赛道，行业进入“白热化”竞争状态。有不少专家表示，大模型背后的技术理念早已存在多年，因此 AI 大模型并不存在技术壁垒。</p><p>&nbsp;</p><p>“现在训练 AI 大模型最大的挑战主要集中在算力和数据层面”，九章云极DataCanvas 副总裁于建岗在接受 InfoQ 采访时表示，大模型的构建本身并没有不可逾越的基础鸿沟，主要是基于 Transformer 进行构建，对企业而言，如何更好且更有效率地运用算力和数据才是核心痛点。“大模型对算力的需求巨大，对于中小企业而言，本身没有足够的 GPU 支撑其去搭建大模型。此外，大模型对数据的需求量也有一定的要求，中小企业很难获得足够的数据训练大模型。”</p><p>&nbsp;</p><p>在这一背景下，AI 基础软件的重要性凸显。未来随着算力性能逐渐同质化和标准化，数据的差异性和企业需求的个性化逐渐加大，“AI 基础软件”将成为模型训练效率和算力使用效率的决定性因素。作为模型生态系统的中坚力量，AI 基础软件将会成为大模型应用落地的最主要的效率支撑，并通过大模型+小模型的方式，形成模型训练新范式。</p><p>&nbsp;</p><p>与构建传统应用相比，构建大模型应用的基础软件主要解决系统和架构层优化，通过一系列工具和服务，帮助企业快速、高效构建上层 AI 应用。于建岗表示，“训练大模型需要庞大的计算集群，如何更好的处理集群之间的通信？如何更好的进行模型切分？如何更好的利用内存和显存？这些都需要 AI 基础软件来解决。AI 大模型时代对基础软件最大的诉求就是如何实现模型训练优化，包括运行时推理效率的优化等等。”</p><p></p><h2>模型训练新范式：大模型+小模型</h2><p></p><p>&nbsp;</p><p>AI 基础软件使得 AI 技术的应用更加广泛和高效，并为各个行业和领域的发展提供了强有力的支持。当前，企业对 AI 技术的关注度高涨，也为 AI 基础软件市场带来新的增长机遇。Gartner 预测，中国的 AI 软件市场将保持快速增长。预计未来五年，该市场的营收将从 47.67 亿美元增长至 138.58 亿美元，年复合增长率(CAGR)达到 28%。</p><p>&nbsp;</p><p>“现在，越来越多的企业意识到 AI 基础软件的重要性。九章云极DataCanvas在基础软件领域耕耘多年，我们认为只要针对企业在训练和应用大模型的过程中的痛点提供切实有效的解决方案，企业完全可以做出自己的 AI 大模型并且落地本企业的应用。”于建岗说道。</p><p>&nbsp;</p><p>基于这种认知，九章云极DataCanvas 正式发布了人工智能应用构建基础设施平台 AIFS（AI Foundation Software），其覆盖了大模型的训练、精调、压缩、部署、推理和监控以及小模型的全生命周期过程，为数据科学家、应用程序开发人员和业务专家提供了一套工具，使不同角色的人员可以相互协作，轻松地处理数据并使用这些数据来开发、训练和部署任何规模的模型。</p><p>&nbsp;</p><p>作为人工智能基础软件体系，AIFS 主要包括 DataCanvas Alaya 九章元识大模型、DataCanvas APS 机器学习平台、DataCanvas BAP 面向业务自动建模平台、开源 DAT 自动机器学习软件、开源 YLearn 因果学习软件等一系列全开放、高自动、高协同的软件工具，为用户自主构建全生命周期的“大+小”模型提供一站式支持。</p><p>&nbsp;</p><p>“我们认为一个足够智能的、能够覆盖所有小模型的大模型时代还没有到来，比如在精准科学计算和符号推理方面大模型并不比小模型可靠，所以当前还是大小模型并存的时代。”在于建岗看来，大模型可以基于对通用知识的理解变得更广泛，也可以通过压缩或者知识蒸馏，部署到小模型环境中去替代一部分能力。但整体而言大小模型并存的时代还会延续一段时间。</p><p>&nbsp;</p><p>九章云极 DataCanvas 董事长方磊也曾在一场演讲中表示，尽管大模型当前表现优异，但对于各行业使用者来说，实际应用于业务场景仍然存在较高的技术和成本门槛。方磊指出，当前迎来“大+小”的新纪元，不仅仅是<a href=\"https://www.infoq.cn/news/g945FdRPEtDy3dHckEIU\">大模型和小模型</a>\"的融合使用，大模型的小型化，或者说以大模型为底座的小型化微调，也是一种趋势，这种方式能够以低廉的成本解决大量的问题。</p><p>&nbsp;</p><p>“大和小是一个相对的变化。”当前大模型的参数标准并不统一，相对于参数级，模型的效果且是否能够支持快速迭代对于用户实际应用来说更为重要。用户能够在一个白盒大模型基础上快速地、低成本地微调和迭代出客制化的小模型，才能高效地实现丰富场景的大模型应用。这就再次点明了 AI 基础软件工具链的重要性。</p><p>&nbsp;</p><p>值得一提的是，本次发布的 DataCanvas Alaya九章元识大模型，具有“通识+产业”系列模型矩阵、多模态大模型、优化的训练机制和友好的开源协议管理等特点。在开源支持方面，九章元识不仅支持 Apache2.0 协议，还为用户提供白盒模型。于建岗强调，这是公司对产品“开放性”的坚守，旨在为用户赋予更大自由度的 AI 创新能力，以求加速实现大模型在多元业务场景中的应用。</p><p></p><h2>数据处理新范式：DataPilot</h2><p></p><p>&nbsp;</p><p>在过去的十几年，数据通常被认为是 AI 的原料、基础要素。而大模型的出现，让数据得到了AI的反向赋能。</p><p>&nbsp;</p><p>利用 DataCanvas Alaya 九章元识大模型的通用文本的理解和生成能力以及在数据领域的微调优化，九章云极DataCanvas 发布了数据处理新范式——DataPilot 数据领航员，可以帮助用户实现数据在建模全生命周期的智能化与自动化。</p><p>&nbsp;</p><p>九章云极 DataCanvas 公司副总裁周晓凌介绍，DataPilot 的特性包括多模“向量海”数据架构，按需自动化数据集成、代码生成、流程编排和分析计算，以及基于自然语言的数据获取、分析和机器学习建模能力。DataPilot 能够大幅降低数据集成、治理、建模、计算、查询、分析、机器学习建模全链路的技术门槛，降低数据驱动业务发展的成本，加快数字化创新的进程。</p><p>&nbsp;</p><p>基于“向量海”理念，DataPilot 所包含的 DataCanvas RT 实时决策中心平台、开源 DingoDB 多模向量数据库等各类数据软件，让用户具备 AI 技术突破情况下亟需的实时、多模态的数据能力。</p><p>&nbsp;</p><p>其中，DingoDB 作为一款开源的多模态向量数据库，将是向量海时代的强大引擎。它结合了数据湖和向量数据库的特性，支持存储任何类型（键值、PDF、音频、视频等）和任何大小的数据。通过 DingoDB，用户可以构建专属的数据“向量海”，不论是结构化还是非结构化数据，仅通过 1 套 SQL 即可完成多模态数据的分析与科学计算。</p><p>&nbsp;</p><p>“未来，AI 基础软件的发展会更加多样化，模型之间的数据交换也会更加频繁，也许会产生新的连接方式与生态。此外，在国家战略与政策推动下，未来可能会出现一些相对统一化的模型，这些都会驱动 AI 基础设施进一步升级。”周晓凌说道。</p>",
    "publish_time": "2023-07-12 09:41:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "探索大模型智能：众安保险基于AIGC 的应用实践",
    "url": "https://www.infoq.cn/article/9SwCpK6cIk4sReXDDjq5",
    "summary": "<p>采访嘉宾｜蒋纪匀，众安保险 CTO</p><p></p><p>每一次技术浪潮的兴起，或多或少都会对各行各业产生深远的影响，并带来突破和进步的机会。此时行业中总有一些敏锐的从业者会及时抓住机遇，积极采用新技术，优化业务流程，改进产品和服务，从而获得更大的竞争优势。</p><p></p><p>众安保险，是当下 <a href=\"https://www.infoq.cn/minibook/g1WRWQi0OhjlSdUI6kPI\">AIGC</a>\" （ AI-generated Content，人工智能生成内容）浪潮里其中一个敏锐的先行者，它自 2022 年开始调研 ChatGPT 并探索 AIGC 在自身业务的落地应用。</p><p></p><p>“随着 AIGC 浪潮的兴起，我们认为，协作智能将成为数字化向智能化转型的科学前沿。而在这个趋势下，特别是金融保险这类数据密集、并且直接服务于人的行业，将迎来快速突破和进展。”众安保险 CTO 蒋纪匀近日接受 InfoQ 采访时如是说。</p><p></p><h2>探索 AIGC 在保险业务的应用，技术与业务双向奔赴</h2><p></p><p></p><p>作为互联网保险公司，<a href=\"https://www.infoq.cn/article/xAWCqCUkkWKqFQTdhhsQ\">众安保险</a>\"一直都侧重于通过科技手段打破传统保险业的行业壁垒。据了解，目前众安保险已将 AIGC 技术应用到内部多个流程环节中，尤其在营销服务场景， AIGC 已经做到能够辅助众安保险的员工快速生成各类营销素材、加速产品上线，比如制作文案内容、视觉设计、优化推文效果等等。</p><p></p><p>其中，在制作文案内容方面，员工可以利用 AIGC 工具生成符合品牌形象和市场趋势的文案、标题、描述、标语等内容，提高广告文案的工作效率，并快速批量生成科普类文章。</p><p></p><p>视觉设计方面，通过 AIGC 相关的工具和平台，能够快速生成符合品牌形象和市场趋势的视觉设计元素，大幅提升了营销素材的设计效率。比如，以前需要三天才能完成的产品海报图，现在只需两三个小时就能生成多套不同风格的图片。此外，在直播背景墙、活动营销图片、图标、动画等多个场景中，员工都可以迅速应用 AIGC 工具为营销的各个环节赋能，加快产品和活动的上线速度。</p><p></p><p>与此同时，众安保险还可以利用 AIGC 的文本数据分析能力优化营销内容，根据社交媒体平台的数据分析，优化现有的营销内容，生成更具吸引力的推文标题、内容和标签，以提高推文的点击率和曝光率。</p><p></p><p>此外，除了内容生成相关的能力，AIGC 还被应用在智能客服领域，充当辅助助手的角色。当坐席与用户沟通的时候，它可以帮助坐席自动生成回答，并在通话结束后自动生成会话总结，根据场景和语义提取新线索。</p><p></p><p>据了解，众安保险一直都有配智能客服服务，但传统智能客服是基于自然语言处理（NLP）技术，如今团队则基于<a href=\"https://www.infoq.cn/article/kPUyVsWATRdiJuGWKCSt\">LLM</a>\"开发了新的智能客服，并通过AB测试发现新的智能客服在准确率方面提高了25%，实现了更高级别的智能化。</p><p></p><p>“这些应用都取得了积极的效果，并得到各业务部门的肯定和积极反馈。”蒋纪匀表示，目前众安保险内部的业务人员对数据 AI 模型的认知不断提升，许多人都积极学习 AI 技术，包括 AIGC 的使用，“技术和业务团队之间开始展现双向奔赴的趋势”。</p><p></p><h2>有针对性地应用 AIGC</h2><p></p><p></p><p>当我们使用像 ChatGPT 或 Midjourney 这类 AIGC 应用时，可能会觉得这很简单，就像雇了一个数字员工。然而，在垂直行业里的实际应用情况并没有宣传片中看起来那么简单。</p><p></p><p>面对 AIGC 的应用，不同公司会采取不同策略，比如自建大模型、fine-tuning（微调）和embedding（嵌入）等，每种方式都有其优缺点。众安保险认为，中小型保险公司采用embedding是较为合适的路径，因为自建大模型需要大量的数据、资源和人力来标注，这对普通公司来说是难以承载的。此外，数据的合法性和合规性也是一个核心问题，尤其对于像保险公司这样的金融机构来说，客户数据的安全和隐私不可忽视，因此不可以拿客户数据来训练。</p><p></p><p>此外，由于ChatGPT 3.5已经无法很好地支持fine-tuning，所以众安保险最终选择基于embedding的思路打造了自己的 AIGC 中台——灵犀，为企业提供了一套 AIGC 应用场景研发落地的标准范式，其 embedding 能力主要基于一站式文本嵌入、知识库切片的复杂性屏蔽，以及知识库向量化复杂度的屏蔽等。</p><p></p><p>蒋纪匀指出，将 AIGC 应用于保险业务场景的过程中还面临着一些比较大的挑战。</p><p></p><h4>寻找价值点，“对话”不是唯一形式</h4><p></p><p></p><p>首先，众安保险团队需要根据AIGC场景下的最佳实践进行产品化，并进行内部调优和提升。</p><p></p><p>对此，蒋纪匀进一步解释称，虽然 AIGC 在处理自然语言和逻辑推理方面非常强大，但若面对一些感性或过于复杂难以理解的问题，它可能无法给出准确的答案。这也是通用人工智能的一个比较显著的问题。</p><p></p><p>因此，团队首先需要对复杂问题进行理解和抽象，将其简化为大模型可以解决的问题。在这个过程中，众安保险团队进行了很多适应不同场景的提示工程和知识库的开发。这些提示工程和知识库的目的是帮助用户更好地使用 AIGC ，而不是仅依赖通用化的人工智能来解决问题。</p><p></p><p>对于上述挑战，蒋纪匀分享了一些实践经验。一方面，始终坚持以价值为导向，寻找 AIGC 在落地应用中的价值点。</p><p></p><p>“将 AIGC 应用到实际业务中是为了增加业务价值，而不是为了验证技术的可行性。”其指出，很多时候，我们可能会陷入一种误区，就是认为任何问题都可以通过 AIGC 来解决。然而，最核心的问题其实是要判断某个问题是否适合使用 AIGC 来解决。</p><p></p><p>而在此判断过程中，需要进行价值增量的分析和推演，判断是否简化了原有流程、提升了用户体验、增加了整体吞吐量，以及对业务价值和成本降低的贡献。如果最终答案不能令人满意，那就不应该在这个点上投入资源。</p><p></p><p>其次，要意识到对话是一种选项，但不是唯一的选项。虽然在使用AIGC时，默认的交互方式是对话，许多人认为只要不断与之对话并细化需求，最终问题就能得到解决。然而，在实际应用中，提示工程对微小变化非常敏感，而对大多数用户来说，掌握这些技巧并不容易。</p><p></p><p>因此，如果仅仅以对话的方式供用户使用，效率并不高。因此，众安保险的工程团队需要不断积累针对具体问题的提示工程，让终端用户无需重复输入；同时，设计一些图形化的用户界面，隐藏背后的复杂性，让业务人员专注于他们擅长的领域，而不必关注繁琐的工程问题。</p><p></p><p>简言之，众安保险主要通过封装和提示工程来聚焦和优化 AIGC 的回答，使其更加高效和专注于解决问题，而不会“胡言乱语”。</p><p></p><h4>如何确保可靠性和安全性</h4><p></p><p></p><p>除了针对特定场景定制化应用，AIGC 的另一大落地难点在于敏感信息的安全性。</p><p></p><p>蒋纪匀提到，应用 AIGC 难免涉及到大量的数据和训练模型。由于在 ChatGPT 3.5 及以上版本中无法进行调优式训练，因此通常采用提示工程与 ChatGPT 进行交互。在提示工程中，需要向 AIGC 的供应商提供问题和一些上下文信息。然而，这种情况存在数据隐私泄露的风险。</p><p></p><p>为了解决这个问题，众安保险需要建立相应的数据安全保障机制。首先，如果直接让每个人直接调用 AIGC ，肯定会出现各种问题，因此众安保险在平台架构层面采取了一些封装措施，使其输入和输出更可控。</p><p></p><p>灵犀的产品架构划分为模型即服务层（MaaS层）、大模型应用框架层和平台层，通过不同的分层能力为企业内部提供基础的模型服务和开发能力。而通过这样的封装，他们在输入和输出端便实现了安全可控的拦截机制。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b92e7a28b83f0bbd3983ca85fb8596b0.png\" /></p><p></p><p>平台能力方面，蒋纪匀首先介绍了审计能力，对于大模型服务的使用，他们需要审计用户输入的合理性以及模型服务的输出的合理性，这是在 AIGC 服务网关中必须具备的功能。如此一来，当发生风险事件时，可以根据审计日志进行追溯，这也有助于规范用户对 AIGC 相关应用的使用。</p><p></p><p>其次，敏感信息的检测和脱敏也是核心举措之一。为了应对敏感信息泄露的挑战，众安保险采用了正则表达式和敏感词知识库两种方式进行敏感词检测，以拒绝非法的输入和输出。这有效地规避了政治敏感、数据敏感和用户敏感信息泄漏等事件的发生。</p><p></p><p>第三，API 的安全性。众安保险要求所有的大模型服务 API 都必须进行身份验证和健全，无论是用户调用还是应用程序调用，以确保用户使用的合法性和可追溯性。</p><p></p><h2>保险行业如何应对技术发展</h2><p></p><p></p><p>谈及众安保险如何保持对技术的敏锐度和关注，蒋纪匀指出，保险行业一直注重数据分析和运用，但该行业过去主要使用老旧的机器学习范式，整体模型是分散的，并且需要从零开始构建，导致效率相对较低。</p><p></p><p>尽管一些头部的保险公司逐步采用机器学习平台和特征工程化平台等方法来降低模型研发成本，但基于这种范式的效率仍然偏低，限制了模型的发展潜力。</p><p></p><p>于众安保险而言，从 2010 年开始，其采用了文本加结构化数据的多模态模型的方式，显著提高了健康险理赔风控模型的性能。从 2022 年开始，众安保险还基于Transformer结构的用户行为序列模型，在头部互联网媒体的实时广告投放筛选模型中取得了传统模型无法达到的水平，并简化了在海量稀疏特征场景下的特征工程开发。</p><p></p><p>“这些都是采用新的模型策略来应对技术发展速度变化、并在尝试过程中取得的进步。”蒋纪匀感慨道。</p><p></p><p>他亦再次强调场景驱动的重要性，“不能因为现在有了 AIGC 就认为所有场景都可以用 AIGC 解决，需要判断它是否适合。”</p><p></p><p>不同的模型在不同场景有各自的优势，企业可以通过对大模型的调优，或者提示工程来满足定制化需求。而一旦找到合适的场景，如何实施则因企业而异，这也是企业未来最重要的核心竞争力。</p><p></p><p>对于众安保险而言，目前他们在 AIGC 方面的应用和服务还没有对外销售或提供，仍处于自我探索阶段，需要持续研究如何更好地应用 AIGC 技术。</p><p></p><p>短期内，众安保险计划在智能营销、保险核心业务系统、研发运维一体化平台和数据产品等方面应用 AIGC。从中长期来看，众安保险将继续基于 AI 和 AIGC 构建具有差异化的核心竞争力。“只有具备差异化的核心竞争力，公司才能在国内竞争激烈的保险行业中脱颖而出。”</p><p></p><p>据悉，未来，众安保险还可能会孵化一些产品，并将这些成果输出到保险行业中。在众安保险看来，AIGC的普及将缩小中小型保险公司与大型保险公司在 AI 技术能力方面的差距，同时也为中小型保险公司提供了“弯道超车”的机会。</p>",
    "publish_time": "2023-07-12 10:33:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据引擎，One Size Fits All真的能实现么？",
    "url": "https://www.infoq.cn/article/CPkG7swG4NONLDxf6S6N",
    "summary": "<p>随着数据的爆炸性增长和多样化，传统的数据处理方式面临着诸多挑战。近年来，我们经常听到关于流处理和批处理的讨论，以及一体化数据融合平台的概念。但是，是否真的存在一种适用于所有场景的解决方案？在组装式Lambda架构和一体化Kappa架构间，为何越来越多的组织选择后者？多云环境中，企业在选择数据平台时应该考虑哪些关键因素？以上问题将是本场直播探讨的重点。</p>",
    "publish_time": "2023-07-12 11:30:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI 遭遇离职潮：员工对 ChatGPT 进展缓慢失望，痛批 CEO 不务正业",
    "url": "https://www.infoq.cn/article/iUfTz6eQa2k8KNtxlW4M",
    "summary": "<p>ChatGPT 让世界认识到了人工智能带来的无限可能，也让其背后的公司 OpenAI 走到了聚光灯下。这款聊天机器人成功地将 OpenAI 的估值从最初的 2.6 亿美元推高至近 300 亿美元。然而，这种对 ChatGPT 的推动似乎疏远了该公司的许多员工，因为他们现在正在离开去寻找更好的工作机会。</p><p></p><p>近日，据德国广播公司 Bayerischer Rundfunk（BR24）报道称，OpenAI 的核心员工正在流失到谷歌。其中一些员工已经辞职并与谷歌签订了合同，另外一部分人也将在近期离开 OpenAI。BR24 声称其信息来自对 OpenAI 前员工以及现员工的采访。</p><p></p><h2>OpenAI 核心员工：从谷歌来，回到谷歌去</h2><p></p><p>今年早些时候，The Platformer 的一份报告称，OpenAI 中有超过 12 名前谷歌员工。</p><p></p><p>2017 年，谷歌研究人员发表了一篇名为《Attention Is All You Need》的论文，论文中提出了一种名为“Transformer”的用于分析文本的新型网络架构，OpenAl 的 ChatGPT 就是基于该技术构建的。该篇论文的 8 名合著者中，有 6 人离开了谷歌自己创业，还有一名加入了 OpenAI。</p><p></p><p>OpenAl 意识到，只是基于 Transformer 架构来构建 ChatGPT 还远远不够，有了理论基础还要有更多技术人才来支持，因此早在 ChatGPT 推出之前，OpenAl 就“挖”来了许多谷歌研究人员来开发人工智能工具。其中包括 Barret Zoph、Liam Fedus、Luke Metz、Jacob Menick 和 Rapha Gontijo Lopes 等多位谷歌资深 AI 大牛。</p><p></p><p>被“挖墙脚”的谷歌 CEO 皮查伊曾在接受彭博社采访时表示，对于员工离开谷歌加入到竞争对手或 OpenAI 等其他公司并没有感到悲观。皮查伊称：</p><p></p><p></p><blockquote>“据我统计，谷歌员工离开后创建了超过 2000 家初创公司，我认为这很棒。其中一些是我们的云客户，他们中的一些人以另一种方式回来了。我认为这很健康。”</blockquote><p></p><p></p><p>虽然这似乎是一个典型的反向人才流失案例，但离职员工也暴露了 Sam Altman 作为公司首席执行官的一些问题。OpenAI 员工对于公司管理不善和秩序混乱等问题表示担忧，这也揭开了 OpenAI 内部不为人知的一幕。</p><p></p><h2>快速扩张带来管理问题，员工批评 CEO 不务正业</h2><p></p><p></p><p>2016 年，OpenAI 共有 6 名员工。经过积极招聘和发展，截至 2023 年 1 月，公司已经拥有 655 名员工，每位员工平均年收入约 13.5 万美元（数据来源：数据信息服务网站 Growjo.com）。</p><p></p><p>虽然此次核心员工离职潮看起来是 OpenAI 爆炸式增长引发的一系列管理问题，但深究本质，或许是员工对 CEO 管理的不满以及对 ChatGPT 技术举步不前的失望造成的。</p><p></p><p>凭借着微软 100 亿美元的资金，OpenAI 似乎满足于追逐他们最大的摇钱树 ChatGPT。但对于 ChatGPT 的进一步开发计划迟迟没有下文。</p><p></p><p>员工似乎已经习惯了 ChatGPT 推出之前 OpenAI 的这种快速的创新步伐，一旦技术进展滞后，就会引发员工不满。此前 OpenAI Sam Altman 曾在公开场合谈及明年的路线图仅关注通过改进 GPT-4 来发展 ChatGPT。</p><p></p><p>有趣的是，Sam Altman 也被认为是员工离开公司的另一个主要原因。据报道，员工表示他“对很多话题只有肤浅的了解，几乎不关心日常业务”。此外，他们还表示，Sam Altman 在为 OpenAI 算法制定有利的监管方面所做的努力是浮于表面的，没有什么实质进展。许多人表示，他正在加大人工智能的颠覆性影响，以期为大型科技公司创造更有利的监管环境，从而将小公司甩在后面。</p><p></p><p>在外界很多人眼中，Altman 是一位伪君子。表面上他极力主张国际社会建立强有力的人工智能法规，但实际上他也担心有人最终决定对其进行监管。</p><p></p><p>由于管理不善，OpenAI 似乎也在失去其在研究领域的声誉。该公司的研究速度已大幅放缓，从 2018 年发表 39 篇论文的峰值降至 2023 年的区区 5 篇。</p><p></p><p>此外，随着 DeepMind 首席执行官 Demis Hassabis 宣布其下一个算法将“让 ChatGPT 黯然失色”，这也被外界认为是 OpenAI 关键员工离职的另一个隐性因素。</p><p></p><h2>ChatGPT 停滞不前，但 DeepMind 有了新进展</h2><p></p><p>这一波从 OpenAI 转向谷歌的浪潮，应该与谷歌在生成式 AI 领域的野心不无关系。</p><p></p><p>2022 年底，当 OpenAI 凭借 ChatGPT 令世界惊叹时，其他公司，尤其是 DeepMind，却被认为落后了。然而，这种情况在谷歌 2022 财年第三季度财报电话会议上发生了变化，谷歌宣布将把 DeepMind 整合到自身中，以增强其人工智能工作。然而，此次的重组在 Demis Hassabis 宣布取得重大的新进展后见到了回报。</p><p></p><p>据报道，这个名为 Gemini 的大型语言模型将典型的 Transformer 架构与 AlphaGo 中使用的技术相结合。这将使算法具有更好的解决问题的能力，以及许多其他改进。Hassabis 表示：“从较高的层面来看，你可以认为 Gemini 结合了 AlphaGo 类系统的一些优势与大型模型令人惊叹的语言能力。我们还有一些非常有趣的新创新。”</p><p></p><p>由于该模型使用了谷歌首创的 Transformer 架构，OpenAI 的员工不仅可以为新项目增加价值，还可以更深入地研究 OpenAI 的优化技术。结合 DeepMind 在深度强化学习方面现有的专业知识，Gemini 可能成为第一个真正击败 GPT-4 的强劲对手。这可能是 OpenAI 研究人员加入谷歌的最大吸引力之一。</p><p></p><p>据外界透露，谷歌目前正在为其伦敦办事处大规模招聘研究科学家和工程师。</p><p></p><h2>OpenAI 失去魅力了吗？</h2><p></p><p></p><p>尽管 OpenAI 目前备受瞩目，但人才流失可能会慢慢稀释该公司为市场带来的价值。</p><p></p><p>但要说 OpenAI 和 ChatGPT 失去魅力了吗？其实不然。</p><p></p><p>自推出以来，ChatGPT 6 月份的全球移动和 PC 端下载量首次环比下降 9.7%。美国银行分析师指出，5 月份 ChatGPT 或 Bing 应用程序的 iPhone 客户端下载量下降了 38%（ChatGPT 应用程序目前仅适用于 iPhone），而 ChatGPT 网站的访问量每月下降 11%。</p><p></p><p>分析师还指出，Google 的搜索引擎市场份额同比有所上升，达到 92% 以上，而 Bing 的份额仅为 2.8%。事实上，Bing 的市场份额达到顶峰是在 ChatGPT 发布前的一个月，但那时的市场份额也仅为 3.59%，如果只算 PC 端，全球市场份额为 7.14%。</p><p></p><p>至于 ChatGPT 网站的访问量，有不少好奇的围观者最初访问 ChatGPT 网站是为了看看热闹是什么，现在他们又转向了另一个值得关注的事物上。有人指出，ChatGPT 的网络访问量突然下降是因为大学生和 K12 学校的学生暑假外出，不再使用它来写论文和完成作业。</p><p></p><p>参考链接：</p><p></p><p>https://www.br.de/nachrichten/netzwelt/unmut-bei-openai-fuehrende-mitarbeiter-wandern-zu-google-ab,TiH4N1t</p><p></p><p>https://the-decoder.com/key-openai-employees-join-google-report/</p><p></p><p>https://growjo.com/company/OpenAI</p><p></p><p>https://www.spiceworks.com/tech/artificial-intelligence/articles/is-openai-losing-popularity/</p><p></p><p></p><p></p><p></p>",
    "publish_time": "2023-07-12 14:40:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一文详解华夏银行数据库变迁之路",
    "url": "https://www.infoq.cn/article/sKl6w7KhhFr7DMQkCrVf",
    "summary": "<p>随着科技的飞速发展，银行业和金融科技都面临着巨大的挑战和机遇。在这个数字化时代，消费者对银行业和金融科技的服务提出了更高的要求，要求更便捷、更高效、更安全的金融服务。</p><p></p><p>金融科技公司凭借其先进的科技技术和灵活的创新模式，在支付、借贷、投资等领域不断拓展市场份额，给传统银行业带来了巨大的压力。但同时，银行业也从金融科技中看到了巨大的机遇。银行业可以利用金融科技的技术和模式，提升服务效率，优化客户体验，扩大市场份额，实现业务的数字化和<a href=\"https://www.infoq.cn/article/xPds1taN1Xcgx1jWBXH4\">智能化</a>\"。</p><p></p><p>华夏银行就是这场数智化浪潮中紧跟时代的先行者。经过几年的自研<a href=\"https://www.infoq.cn/article/IXyjIQka6rVgQxG3RTHP\">数据库</a>\"迁移与改造，目前华夏银行已经完成了40%以上的老旧数据库系统替换，实现了由管理类业务系统成熟的应用，逐步向核心系统的试点推进。那么，是什么样的契机推动了华夏银行对数据库系统进行改造？在对数据库进行调研、测试、选型与推广方面都做了哪些工作？针对自研数据库的建设上华夏银行又有哪些经验值得分享？在此次2023 可信数据库发展大会上，InfoQ有幸采访到了华夏银行数据库专家王辉，听他来聊一聊华夏银行的数据库迁移改造技术实践。</p><p></p><h2>为什么要改造数据库系统？</h2><p></p><p></p><p></p><blockquote>InfoQ：很高兴采访到您，您能简单介绍下自己吗？包括之前的一些工作经历？</blockquote><p></p><p></p><p>王辉：你好，非常荣幸能够接受InfoQ的专访，我叫王辉，来自华夏银行信息科技部基础技术研究室，主要负责国产分布式数据库的调研、测试、选型与推广相关工作，之前主要从事<a href=\"https://www.infoq.cn/article/o6Azoc6yAATTsNPioss9\">Oracle</a>\"、MySQL等商用数据库的开发与运维相关工作。</p><p></p><p></p><blockquote>InfoQ：以您的视角来看，近些年来银行业经历了哪些变革？是什么需求促进了华夏银行数据库系统的改造？</blockquote><p></p><p></p><p>王辉：<a href=\"https://www.infoq.cn/article/vC4h2rWRyOylw0I4soj1\">数据库</a>\"在银行主要经历了以商用为主的集中式，逐步向开源和分布式试点，最终采用国产分布式为主的转变，主要需求来自于业务驱动、技术驱动两方面，业务驱动主要是由于随着数字化时代的来临，数据呈现爆发式增长，数据的存储和访问都面临重大的挑战，<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537\">传统数据库架构</a>\"受限于单机资源瓶颈，不具备水平扩展能力，使其海量<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1536\">数据的处理能力</a>\"较弱，难以满足新业务场景需求。技术驱动主体现在实现金融科技创新对核心技术实现自主掌控的要求，同时进行深入研究和场景适配的攻关，推进金融业信息化核心技术安全可控标准建设的目标。</p><p></p><p></p><blockquote>InfoQ：在数据库替换之前，华夏银行的IT架构和采用的数据库是什么，在业务中老旧的架构和数据库遇到了怎样的问题？</blockquote><p></p><p></p><p>王辉：IT架构主要以商用的IOE集中式架构为主，数据库在TP场景绝大部分采用的Oracle，AP场景以TD、Oracle一体机为主。主要存在集中式架构扩展能力差，成本较高，<a href=\"https://www.infoq.cn/article/YsMkRaThIKaWdKyhX5sQ\">数据库</a>\"高可用切换能力弱，同城数据复制存在数据丢失风险等问题。</p><p></p><p></p><blockquote>InfoQ：华夏银行的数据库替换是由管理类业务系统成熟的应用，逐步向核心系统的试点，这两种系统在对数据库的要求上有什么区别吗？</blockquote><p></p><p></p><p>王辉：在容灾级别、高可用、高性能、高扩展能力等方面要求不同。核心系统容灾级别为最高级别，需满足生产、同城、灾备三数据中心机房，同城RPO=0，RTO&lt;1分钟，数据强一致、零丢失，又要具备自动高可用切换能力，高并发访问性能，数据库响应时间延迟低，及更灵活的弹性扩展能力等要求，对保障业务的连续性与稳定性能力也要求更高。而管理类业务系统要求相对较低，大部分为内部使用，属于次重要或一般类业务系统，等保和容灾级别也要求较低。虽然不乏有金融机构直接拿核心类业务进行试点，但为了稳妥起见，大多数金融机构都以管理类业务系统试点为切入点。</p><p></p><p></p><blockquote>InfoQ：数据库选型过程中，您和团队都考虑了哪些因素？进行了哪些前期工作？为什么最终会选择分布式？</blockquote><p></p><p></p><p>王辉：数据库选型主要考虑产品能力、服务能力、同业案例、生态建设和公司资质等方面。前期工作主要包括：技术调研、技术交流、市场调研、产品测试、产品选型等方面。</p><p></p><p>集中式数据库基本都不具备分布式能力，无法满足高并发、大数据量的业务场景需求，而绝大部分分布式数据库都具备集中式和分布式两种部署模式，可以满足不同业务不同场景的要求。</p><p></p><p></p><blockquote>InfoQ：采用了新架构和新的数据库后，业务上带来了哪些改善？收益是否有明显提升？</blockquote><p></p><p></p><p>王辉：<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1574\">分布式</a>\"的优势在于存算分离，将数据分散到多个节点进行存储与计算，从而提升数据库的整体处理能力。因此带来的改善首先体现在性能方面，具体实践中虽然对于本身业务压力就比较小的系统提升并不明显，但对于交易量大的系统提升较为明显，TP场景下，以核心借记卡系统为例，采用分布式架构后，TPS提升1.5倍。AP场景由于其数据量大，数据节点多等特点，更能体现出分布式计算的优势，替换后提升明显，平均性能提升1.5-2倍，以CRM系统为例用户响应时间由30s下降到3s，极大的提升了用户体验。其次在成本方面，对比商用小型机、企业级存储，分布式数据库只需要运行在X86、ARM等架构的PC服务器上，同时软件费用相比较商用数据库较低，相关维保费用也较低，虽然分布式部署设备较多，但总体上成本还是有所降低的。</p><p></p><p>当然，分布式其实是一把双刃剑，用好了会事半功倍，但如果前期规划设计不好，反而会造成性能下降。在使用分布式架构时我们需要进行全面的评估分析，尽量避免分布式事务，分片键设计不合理等情况的发生。另外需要指出分布式数据库的集中式部署模式能满足大部分业务需求，我行也占有较大的比重，我们完全没有必要为了分布式而分布式。</p><p></p><h2>自研数据库迁移平台的建设实践</h2><p></p><p></p><p></p><blockquote>InfoQ：整个数据库改造过程共耗时多久？分为哪几个阶段？这其中遇到过哪些技术上问题？又是如何解决的？</blockquote><p></p><p></p><p>王辉：整个数据库改造过程大概分成如下几个阶段：</p><p></p><p>•  改造评估：收集相关信息评估改造成本，评估报告编写；</p><p>•  规划设计：数据库选型、架构设计、模型设计、环境部署；</p><p>•  适配测试：业务代码与数据库语法改造，功能、非功能及高可用测试；</p><p>•  数据迁移：数据对象与数据迁移，数据一致性比对与验证；</p><p>•  服务交割：系统上线与回退策略制定及后期运维、监控与故障处置等。</p><p></p><p>耗时大概在半年左右。数据库改造过程耗时主要取决于业务复杂度、数据量大小、兼容性及运维备份要求等方面，一般情况下业务系统数据库改造的平均时间在半年左右。</p><p></p><p>在数据库改造过程中遇到的主要问题是业务开发代码与新数据库的兼容性适配，包括对象、语法、链接驱动等适配改造，另外就是商用数据库如何完整、高效的迁移到国产数据库问题，最后还涉及到上线后的运维监控与备份等问题。</p><p></p><p>我们的解决方案是自研了几个工具平台来完成这项工作。</p><p></p><p>我们搭建了开发工具平台——自建龙跃（iGO）数据库迁移平台。实现了在业务代码不修改或少量修改的情况下与新数据库的适配，同时具备数据平滑、高效的数据迁移能力解决了数据库改造过程遇到的两大难题，提升了改造效率。</p><p></p><p>还建立了统一运维平台，实现现有数据库与我行一体化监控平台的对接，并建立统一的数据库运维管理平台。此外我们还推动了平台生态建设，推动了国产备份、安全等服务厂商与国产数据库的适配认证。</p><p></p><p></p><blockquote>InfoQ：您能介绍下华夏银行在数据库改造过程中自研了哪些平台或工具？效率是怎样的？为什么没有选择市场上现成的平台或工具而要自研呢？</blockquote><p></p><p></p><p>王辉：主要研发了数据库迁移平台，存储过程覆盖率检测工具，SQL审计平台，应该说为数据库的改造既降低了成本又提高了改造效率，随着平台不断地成熟与完善，原来的改造要用年来计算，现在可以实现月甚至周来计算。</p><p></p><p>因为我们非常明确数据库的改造需求，也具备有实力的开发团队。另外市场调研结果显示针对国产数据库的迁移平台并不多，也很难覆盖我行的数据库种类，虽然各数据库也有自己的迁移工具，但都各自分散不便于统一管理，而且在数据迁移过程中遇到特殊情况自己研发的平台可以快速响应，可定制开发，比较反馈给厂商效率要高很多，同时还实现了自主可控与技术创新。另外两个平台也有类似同样的思考。</p><p></p><p></p><blockquote>InfoQ：这款自研的数据库迁移平台是如何搭起来的？它有哪些关键的部分？其中的技术难点有哪些？</blockquote><p></p><p></p><p>王辉：2019年我们认识到数据库的改造，迁移是重要且必须的一环，未来一定有很多的需求，因此在同年开始组建团队，讨论需求、设计与研发，那个时候国产数据库引入刚开始，数据库改造并不多，也给了我们更多的时间。那个时候市场的迁移工具并不多，且对国产数据库的支持很少。</p><p></p><p>这过程中的关键部分有两大块：数据库迁移平台架构分为monitor 和 worker两大组件，monitor主要是web页面进行迁移任务操作和日常系统管控还有任务调度能力等。worker主要负责主要对象语法转换、数据迁移和数据校验。</p><p></p><p>遇到的技术难点就是各种字符集差异、时区差异转换，全量数据的快速同步，增量数据断点续传，数据库快速回退技术等。</p><p></p><h2>数据库改造后为业务带来了哪些价值？</h2><p></p><p></p><p></p><blockquote>InfoQ：数据库迁移改造后，要把技术价值转变为业务价值，还会存在哪些阻力？怎么解决这些问题？</blockquote><p></p><p></p><p>王辉：技术与业务之间的沟通障碍，导致难以理解和传达技术变革对业务的实际影响。通过建立良好的技术和业务团队之间的沟通渠道，确保彼此了解对方的需求和期望。可以组织定期的会议，交流小组或培训来加强沟通和理解。</p><p></p><p>技术变革带来人才转型，员工对变革可能存在抵触情绪，在接触新技术时需要适应和调整自己的角色和技能。一般通过提供培训和支持，帮助员工适应和接受新的工作方式，建立反馈机制，鼓励和回应员工的意见和关切。对创新标兵给与奖励以鼓舞大家的热情。</p><p></p><p>数据库迁移改造后会给业务带来性能和用户体验的提升，但也会存在一定风险，因此需要我们在上线前做好全面压力测试，同时要建立上线后的快速应急处置方案。</p><p></p><p></p><blockquote>InfoQ：那么，在未来数字经济背景下，您认为科技金融该如何更好地服务于客户？</blockquote><p></p><p></p><p>王辉：在未来数字经济背景下，科技金融需要不断创新和改进。通过大数据、人工智能、生物识别、区块链等技术平台，实现数字化创新、数据驱动决策、信息安全保护的客户服务，面推进金融服务线上化、移动化、智能化，提升场景和生态对接能力，满足客户的多样化需求，为客户提供更便捷、高效、安全的金融服务。</p><p></p><p></p><blockquote>InfoQ：未来，在数字化进程中，华夏银行还有哪些动作和布局？</blockquote><p></p><p></p><p>王辉：在国家大力发展数字化经济背景下，产业数字金融是一个重要发力方向，华夏银行未来将在金融数字化促进产业升级领域有更多布局。以产业链协作体系中的商流、物流、服务流、信息流、资金流等数字信息为基础，创新运用包括大数据、区块链、5G、物联网、隐私计算等在内的数字化技术，整合各条线金融服务，为产业链各类参与主体提供综合化的产业数字金融服务。</p><p></p><p>采访嘉宾：</p><p></p><p>王辉，华夏银行数据库专家</p>",
    "publish_time": "2023-07-12 15:54:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "小米机器学习平台：基于Fluid的高效Serverless 混合云容器AI平台",
    "url": "https://www.infoq.cn/article/kco7hi5TcVE08ySwNIw7",
    "summary": "<p>&nbsp;作者：小米机器学习平台何逸凡、刘国明</p><p></p><h2>业务背景</h2><p></p><p></p><p><a href=\"https://www.infoq.cn/article/mIO82YAD19sZUKWR4vzT\">小米机器学习平台</a>\"（以下简称CloudML），是小米针对机器学习进行全流程优化的高性能、分布式云服务。开发者可以在云端使用GPU训练模型，秒级启动分布式训练任务，兼容TensorFlow、PyTorch、PaddlePaddle、DeepSpeed等深度学习框架，可以一键部署训练好的模型，或者创建基于GPU的开发环境，提供模型开发、训练、调优、测试、部署和预测等一站式解决方案。CloudML还开放了API、SDK、命令行和Web控制台等多种访问方式，支持灵活的秒级计费，方便人工智能专家使用。但是近年来由于需求规模的不断扩大，越来越多地面临算力资源不足的挑战。</p><p></p><h2>技术选型:&nbsp;混合云下的Serverless容器</h2><p></p><p>&nbsp;</p><p>CloudML所依托的自建算力集群由于资源池容量、资源弹性能力相对有限，导致在业务低谷时存在高昂的资源闲置成本，业务高峰时存在资源紧张的问题。在此背景下，我们考虑引入公共云上的算力资源作为自建资源池的补充，但是考虑到业务自身的演进和数据安全可控，CloudML采用了混合云架构。资源调度策略是以自建集群的资源为主，按照业务对资源的需求弹性使用公有云资源。具体实现方法是在资源需求高峰时使用公有云资源，在低谷时释放掉公有云资源。这样一方面能够满足负载高峰时业务的算力需求，另一方面也兼顾了数据安全、成本等多方面的要求。</p><p>&nbsp;</p><p>混合云场景下我们优先采用了<a href=\"https://www.infoq.cn/theme/182\">Serverless</a>\"容器进行落地，这主要其具有两个重要的优势：在操作方面，能够屏蔽混合云的基础设施差异性，降低运维复杂度。在成本方面，这类型容器能够实现快速按需扩容和销毁，按资源实际有效使用时长付费，最大化的降低成本。</p><p></p><h2>混合云下使用Serverless容器的技术挑战</h2><p></p><p>&nbsp;</p><p>迁移到基于Serverless容器架构的混合云之后，我们获得了Serverless容器带来的敏捷、安全、弹性、低成本等优势，然而我们也遇到了几个重要的技术挑战：</p><p>&nbsp;</p><p>目前公有云平台不支持对&nbsp;Serverless&nbsp;容器平台中的存储类型进行定制扩展：公有云只支持云厂商自带的公有云存储类型（如NAS、对象存储等），无法直接适配公司内部自研的分布式文件存储产品（以下简称StarFS）。缺乏可信透明的数据接入方式：如何在Serverless容器的黑盒系统使用过程中规避数据泄露，如何确保数据在存储、传输、访问过程中安全可靠。CloudML平台用户不应感知基础设施层面的差异：切换到混合云后，用户提交的计算任务会运行在公有云或自建集群中。当用户任务在公有云和自建集群之间进行迁移时，用户使用体验需要与自建集群上使用体验保持一致。例如混合云场景中，最好沿用原来在自建集群中已有的PVC、PV资源而不需要做过多的变更。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/27/c2/271e6f5cac0607d85cf7ed296beyy1c2.png\" /></p><p></p><h2>解决方案：ACK&nbsp;Fluid打通混合云下数据访问</h2><p></p><p>&nbsp;</p><p>经过调研选择之后，我们发现基于<a href=\"https://www.infoq.cn/article/WAuJUZKYQDEDTGWBvYJF\">Fluid</a>\"架构的方案几乎可以完美解决上述难题，Fluid是云原生计算基金会旗下一个面向数据密集型应用的高效支撑的开源项目。</p><p>&nbsp;</p><p>简单可扩展：Fluid最新的版本中ThinRuntime开发简单，无需具备复杂的Kubernetes开发知识。基于这套方案，只需要了解Dockerfile构建就可以完成，一般开发工作2-3小时左右，从而显著降低了接入StarFS的工作成本。安全可控的数据访问：Fluid中ThinRuntime以容器化的方式支持我们以自定义方式实现数据访问。整个数据访问过程可控，无需向公共云平台提供实现细节。无侵入的客户端部署：Fluid同时支持CSI和FUSE&nbsp;Sidecar两种客户端部署模式，在Serverless&nbsp;Container的模式下FUSE&nbsp;Sidecar是一种对公共云平台无侵入式接入的选择。无缝接入开源K8s体系、兼具可观测性和可控制性：第三方存储客户端只需要实现自身的容器化，就可以无缝接入Kubernetes体系，并获得可观测性和计算资源可控制性。</p><p>&nbsp;</p><p>但是，我们在实践中发现：</p><p>&nbsp;</p><p>开源Fluid中的FUSE&nbsp;Sidecar需要依赖privileged权限，这在多数的Serverless&nbsp;Container平台下都是不支持的；Fluid的抽象层相关资源对象生命期的管理比较复杂。比如需要创建和管理Fluid&nbsp;Dataset资源的生命周期。导致使用人员有一定的学习成本，业务也有相应改造成本。</p><p>&nbsp;</p><p>这时我们发现，阿里云ACK云原生AI套件中提供的ack-fluid存储系统接入方案可以很好的解决这个问题：</p><p>&nbsp;</p><p>ack-fluid基于开源Fluid标准对于ThinRuntime提供了完整的支持，只要满足开源要求就可以适配ack-fluid。StarFS接入只需在开源Fluid下即可完成调试，同时借助ACK&nbsp;One注册集群模式可获得阿里云商业版Fluid全部功能。ack-fluid与阿里云的ECI做了无缝支持，无需开启privileged权限，就可以满足云上弹性容器实例ECI访问云下自建存储系统的需求。ack-fluid提供对于StarFS&nbsp;自建pvc的丝滑兼容，无需了解Fluid的使用方式，只需要pvc中添加特定label即可，满足了CloudML用户无需感知基础设施层面的差异的需求。而在开源Fluid中这个工作就非常复杂，需要手动创建和管理Dataset和ThinRuntime的生命周期。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/76/77/76e59a94a2155afe169592efe9c10e77.png\" /></p><p></p><p>最终通过Fluid提供的通用标准协议，我们只需要按照Fluid中的ThinRuntime开发规范对接StarFS的存储协议，就可以实现云上弹性计算资源ECI访问云下自建存储系统的能力。ack-fluid可以将Serverless&nbsp;Pod&nbsp;中数据访问从CSI协议转换成sidecar协议，同时也可以通过ThinRuntime的资源设置容器中的数据访问的资源分配情况。</p><p></p><h2>操作步骤</h2><p></p><p></p><p>部署fluid管理组件dataset-controller、fluid-webhook、fluid-pvc-wrapper、thinruntime-controller,&nbsp;安装ack-fluid，安装过程可以参考文档：<a href=\"https://help.aliyun.com/document_detail/208336.html\">https://help.aliyun.com/document_detail/208336.html</a>\"配置Starfs相关的k8s原生的pv、pvc自动创建Fluid&nbsp;Dataset和ThinRuntime</p><p>执行以下命令在PVC上打上特殊标签,触发Fluid使用上述创建的名为&nbsp;ossfs-profile&nbsp;的ThinRuntimeProfile作为模版,自动创建Dataset和ThinRuntime。</p><p>$&nbsp;kubectl&nbsp;label&nbsp;pvc&nbsp;oss-pvc&nbsp;fluid.io/runtime-profile=ossfs-profile</p><p>创建ThinRuntimeProfile,&nbsp;通过其注入fuse-client启动方式</p><p>apiVersion:&nbsp;data.fluid.io/v1alpha1</p><p>kind:&nbsp;ThinRuntimeProfile</p><p>metadata:</p><p>&nbsp;&nbsp;name:&nbsp;ossfs-profile</p><p>spec:</p><p>&nbsp;&nbsp;fileSystemType:&nbsp;starfs</p><p>&nbsp;&nbsp;fuse:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;command:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;sh</p><p>&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;-c</p><p>&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;python3&nbsp;/fluid_config_init.py&nbsp;&amp;&amp;&nbsp;chmod&nbsp;u+x&nbsp;/mount-starfs.sh&nbsp;&amp;&amp;&nbsp;/mount-starfs.sh</p><p>&nbsp;&nbsp;&nbsp;&nbsp;image:&nbsp;micr.cloud.cn/cloudml/starfs-eci-mount</p><p>&nbsp;&nbsp;&nbsp;&nbsp;imagePullPolicy:&nbsp;Always</p><p>&nbsp;&nbsp;&nbsp;&nbsp;imageTag:&nbsp;v1.4.1</p><p>&nbsp;&nbsp;&nbsp;&nbsp;resources:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;requests:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cpu:&nbsp;500m</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory:&nbsp;1Gi</p><p>&nbsp;&nbsp;nodePublishSecretPolicy:&nbsp;CopyNodePublishSecretAndMountIfNotExists</p><p>&nbsp;</p><p></p><h2>结果收益</h2><p></p><p>&nbsp;</p><p>混合云场景下Serverless容器方案完美落地，很好地满足了我们简单、安全、弹性、低成本等诉求，小米CloudML深度学习平台可以稳定高效地响应业务需求。</p><p>&nbsp;</p><p>尤其值得一提的是，通过引入阿里云容器服务ACK&nbsp;Fluid很好地解决了相关技术难点：&nbsp;</p><p></p><p>首先，对于自有定制化存储StarFS的访问提供了很好的扩展支持，并且得益于Fluid提供的数据集可观测性功能，我们能够获取云上工作负载的数据访问特性，从而支持数据热加载和资源分配调优。</p><p></p><p>其次，方案接入简单、管理便捷。我们自行完成StarFS与Kubernetes环境的对接工作（编写自定义的CSI插件或编写Runtime&nbsp;Controller与Fluid对接），整个thinRuntime开发简单，无需我们具备复杂的Kubernetes定制开发知识。基于这套方案，我们只需要了解Dockerfile构建就可以完成，开发工作2-3小时左右，显著降低了使用ECI接入StarFS的工作成本。</p><p>&nbsp;</p><p></p><h2>未来展望</h2><p></p><p>&nbsp;</p><p>未来，小米机器学习平台计划进一步拓展不同的应用场景和优化用户体验，提升训练任务性能及资源效率。具体来说：</p><p>&nbsp;</p><p>降低专线网络成本：我们目前已经在自建集群接入了云上弹性计算资源，同时也打通了云上算力与自建数据集通道。但是实际成本和性能还存在优化空间。因此，下一步需要持续优化这项工作，比如考虑使用分布式缓存运行时来降低专线网络成本。协同任务调度与数据缓存弹性：使得我们的业务系统有能力识别一段时间内使用相同数据集的任务并发量，并在任务排队过程中执行数据预热与弹性扩缩容。当数据缓存或访问吞吐达到一定条件时，将排队任务从等待转换为可用状态，从而加速计算对数据的访问。&nbsp;</p><p>&nbsp;</p><p>同时，由于ack-fluid基于开源Fluid构建，拥有灵活性、相对透明及有开源社区支持的好处。感谢&nbsp;Fluid&nbsp;社区以及阿里云车漾、徐之浩和南京大学的顾荣老师的帮助。未来我们计划一起共建&nbsp;Fluid&nbsp;社区，助力Fluid开源技术的发展。</p><p></p><p>&nbsp;</p>",
    "publish_time": "2023-07-12 16:03:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "探讨大模型趋势对行业数据平台的影响 ｜ArchSummit",
    "url": "https://www.infoq.cn/article/lWaQgbckHle90TQrVtLE",
    "summary": "<p></p><p></p><blockquote>大模型的浪潮正在改变我们处理数据的方式。当我们谈论“大模型”时，我们指的是使用海量数据进行训练的复杂模型。这一趋势对行业数据平台带来了全新的挑战与机遇，改变了我们对数据存储、处理和标注的理解。在ArchSummit全球架构师深圳站上，我们策划了《大模型趋势下的行业数据平台的难点与解决思路探讨》闭门交流会，以下是详细介绍。</blockquote><p></p><p>&nbsp;</p><p>数据存储和处理是大模型对数据平台的主要影响之一。大模型对于数据的需求量大，带来了数据存储和处理的挑战。如何有效地存储、获取和分析这些数据？如何处理这些数据以供模型训练？如何优化数据架构来满足大模型的需求？这些问题将是我们交流会的讨论焦点。</p><p>&nbsp;</p><p>同时，数据标注和质量管理也是大模型时代的关键。大模型的数据量越大，对数据标注的精度和质量的要求也越高。如何在保证数据标注准确的同时，有效管理数据质量？如何使用新技术和方法提高数据标注的效率和准确性？我们也将在此次会议中寻找答案。</p><p>&nbsp;</p><p>我们欢迎各位专家和研究者加入我们，共同探讨这些问题。我们相信，通过集思广益，可以为行业发展提供新的视角和解决方案。在大模型的浪潮中，我们必须了解和适应新的挑战。让我们在这次技术交流会上，共同为未来的数据平台献上您的智慧和见解。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab83a92b19528c4306c9a2c627965e21.jpeg\" /></p><p></p><p>&nbsp;</p><p>温馨提示：闭门交流会，仅对购票听众开放申报，名额有限。报名链接<a href=\"https://jinshuju.net/f/pF3JOd\">https://jinshuju.net/f/pF3JOd</a>\"。</p><p></p><p>如想咨询<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=bmhwenzhang&amp;utm_campaign=10&amp;utm_term=0712\"> ArchSummit 大会</a>\"，也可联系票务经理瑞丽：18514549229（微信同手机号）。</p>",
    "publish_time": "2023-07-12 16:45:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "喜马拉雅直播秒开优化实践",
    "url": "https://www.infoq.cn/article/baJrvmHAWaVQ3BK61dWS",
    "summary": "<p></p><p>作者：喜马拉雅直播技术客户端 杜若尘</p><p>&nbsp;</p><p></p><blockquote>随着互联网的发展，越来越多人喜欢直播，<a href=\"https://www.infoq.cn/article/WdWsV9v1vSMeRwMfYJgw\">喜马拉雅</a>\"直播也在快速发展中，现如今已有秀场直播、课程直播、语音聊天室等类型。为了提升用户的使用体验，本文针对喜马拉雅直播的复杂流程进行了整体梳理，并详细说明了开展的一系列直播启播优化工作。&nbsp;</blockquote><p></p><p></p><h3>背景</h3><p></p><p></p><p>在直播场景中，QoE(Quality of Experience) 很重要，也就是用户体验，而 QoE 和 QoS 又是紧密相关的，作为技术开发，需要从技术的角度去思考如何提升用户体验。直播场景中，用户的体验指标有很多种：首帧、卡顿、延迟、清晰度等等。站在用户的角度出发，在进入直播间后，肯定希望能够马上看到画面，也就是启播速度要快。所以首帧优化成为首要任务。</p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3d04d3e0af7d39d56a4fd6e76e9fafa.png\" /></p><p></p><h3>统计口径</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/118a525053eac278cc6aafb589079c1a.png\" /></p><p></p><p>在做直播启播优化前，首先要思考如何去衡量启播体验，也就是说要将启播体验转化为可量化的指标，并制定体验标准以便衡量体验的好坏。启播体验问题可以转化为首帧耗时问题，首帧耗时越短，启播体验也就越好，比如以下标准：</p><p></p><p></p><p>制定标准后，体验优化问题也就转化成了指标优化问题。要做指标优化，需要获取准确可靠的统计数据作为开展工作的支持，明确统计口径是什么，只有这样客户端上报的埋点数据才是有参考价值的。对于时长的统计，需要明确开始和结束时间的定义，其差值就是需要上报的首帧时间了。</p><p></p><p>对于开始时间，选择不同的入口，统计方式也不相同。直播启动入口可以分为两个类：</p><p></p><p>1.&nbsp;一类是从其他页面跳转进入直播间页面的，这类方式可以将页面的创建时间作为开始时间，在Android 中对应 Fragment 的 onCreate 回调。</p><p>2.&nbsp;另一类是在直播间页面内上下滑进入其他直播间，这类方式可以将下个页面被选中时作为开始时间，在Android 中对应 ViewPager 的 onPageSelected 回调。</p><p></p><p>对于结束时间，可以将播放器的首帧回调作为结束时间，首帧耗时即为结束时间减去开始时间。</p><p></p><h3>直播全链路分析</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc5bcae3a491f9ad2606d7cfd0dd33fa.png\" /></p><p>直播整体流程环节较多，主播推流到流媒体服务器，然后流媒体服务器分发到各地的CDN ，用户通过播放器再从CDN 进行拉流播放，即主播端推流 -&gt; 流媒体服务器 -&gt; CDN 边缘节点 -&gt; 终端设备。其中终端拉流又可以细化成许多环节，像网络请求、读取数据、解封装、解码、渲染。</p><p></p><p>可以根据不同的端大致分为三个部分：推流部分、流服务部分和拉流部分。客户端所做的主要优化工作集中在拉流部分，但推流部分、流服务部分同样也会间接对启播速度产生影响，这里也一并提一下。</p><p></p><h3>现状</h3><p></p><p></p><h4>直播协议选择</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/854648518e89fe5944b34bc4e006b062.png\" /></p><p></p><p><a href=\"https://xie.infoq.cn/article/9d607e3cf0f169111a9e5f4cd\">RTMP</a>\" 协议为流媒体而设计，在推流中用得比较多，同时大多 CDN 厂商支持 RTMP 协议。</p><p></p><p><a href=\"https://xie.infoq.cn/article/a07e156e60d10bda16c876a51\">HTTP-FLV </a>\"使用类似 RTMP 流式的 HTTP 长连接，由特定流媒体服务器分发，兼顾两者的优点，还可以复用现有HTTP 分发资源的流式协议。它的实时性和 RTMP 相当，与 RTMP 相比又省去了部分协议交互时间，首帧时间更短、可拓展的功能也更多。</p><p></p><p><a href=\"https://xie.infoq.cn/article/111c3827b437ec6015125685d\">HLS </a>\"由苹果公司提出基于 HTTP 的流媒体网络传输协议，可用于直播场景，Android 端也同时提供相应的支持。</p><p></p><p>喜马直播移动端使用的RTMP 推流、HTTP-FLV 拉流。</p><p></p><h4>推流端—动态码率</h4><p></p><p></p><p>流量控制功能可以让主播根据自己当前网络环境状态来动态调整视频推流的码率、帧率、分辨率，以及音频码率，自动适应当前网络环境及网络波动，从而保证视频能流畅发布。</p><p></p><p>流量控制的原理是基于当前网络情况，对用户的网络环境建模并估算它的上行带宽。如果当前上行带宽小于设置的推流码率，则会通过配置的选项分别从视频码率、分辨率、帧率、音频码率几处循序渐进地降低，以减少最终推流的上行码率，保证直播的流畅性。在网络环境恢复正常以后，上行码率也会重新恢复到初始设置值。</p><p></p><h4>流媒体服务器优化—缓存GOP</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2f38f4f0eb50b90d5009198d44e2775.png\" /></p><p></p><p>视频是由一帧帧的图像组成的，GOP（Group of Pictures）是一组可以被独立解码的图像序列，它由一组不同类型的帧组成，包括关键帧（I 帧）、预测帧（P 帧）和双向预测帧（B 帧）。关键帧是视频序列中的完整帧，没有依赖其他帧进行解码。预测帧和双向预测帧则根据前面的关键帧或其他预测帧进行编码和解码。如果拉流时返回的第一个视频帧是 B 帧或 P 帧，那是无法被成功解码的，直到收到下一个 I 帧才能被解码展示画面，从而导致首帧时间变长。因此，一般 CDN 都会缓存一到两组最近的 GOP，在收到客户端的资源访问请求时返回最近的 GOP，这样能确保客户端收到的第一个视频帧就是 I 帧，可以立马进行解码和渲染。</p><p></p><h3>启播优化方案</h3><p></p><p></p><h4>拉流各阶段耗时拆分</h4><p></p><p></p><p>对于拉流过程，可以细化成许多环节，如获取拉流地址、网络请求、读取数据、解封装、解码、渲染。因此，需要把直播启播耗时进行阶段拆解、细化各个流程，并进行数据上报。在实际数据统计中，我们会对各个环节（如网络请求部分、解封装部分）进行更精细的统计。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d1eb23d21bb723b795db103ea5a06b9b.png\" /></p><p></p><p>&nbsp;下方表格列出了喜马直播业务的启播耗时各阶段的数据统计：</p><p></p><p></p><p></p><p>上表中步骤4-7 需要读取解析 flv 头部信息，其具体结构组成如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b8c0006a458a2600dab1144b14dc88d.png\" /></p><p></p><p>从用户点击跳转到直播间或者在直播间内上下滑动切换直播间，到最终的直播播放成功，预计会有十多个的点位进行一系列的追踪分析。详细的数据表格呈现启播过程中每一步骤的耗时，然后针对于耗时多的地方进行优化。</p><p></p><h4>预获取拉流地址</h4><p></p><p></p><p>一般而言，用户进入直播间的业务场景，是从一个直播列表页面点击某一个直播卡片进入直播间。这个过程中，数据流是怎么走的呢？</p><p></p><p>在启播优化改造之前，观众从直播列表页点击某个直播卡片到直播间后，需要先向服务端请求一个直播间详情接口，详情接口发现直播间当前是处于开播中状态的话，再去请求该直播间拉流地址接口，交给播放器播放。</p><p></p><p>在这个过程中，播放器必须等到进入直播间，串行请求完两个接口拿到直播流地址后才能开始播放，这个时间点是可以提前的：在直播列表页及其他直播入口拿到直播间对应的流地址，在进入直播间时直接传过去，这样一进入直播间播放器就可以拿着直播流地址开始播放了，省去了从服务器请求直播流地址的时间。对于在直播间内上下滑切换房间的情况，就需要改造上下滑接口、增加拉流地址信息。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3aff17ca32c83141455d6d7f80926270.png\" /></p><p></p><h4>组件渐进式加载</h4><p></p><p></p><p>直播间功能组件数量多达34 个（如视频播放组件、红包功能组件、礼物动画功能组件等），页面层级复杂，在一些中低端机型上加载较慢，可以采用渐进式的加载方案，让优先级高的组件先进行展示。组件加载方式遵循以下三条原则：</p><p></p><p>1.&nbsp;组件按需加载，用到的时候再进行加载；</p><p>2.&nbsp;组件视图如果复杂，耗时问题不好解决，可以使用异步加载；</p><p>3.&nbsp;组件可以按优先级串行加载，先展示主要视图，而后加载次要视图，最后加载特定条件才触发动态添加的视图。</p><p></p><p>这样设计的好处是可以减缓同一时刻加载View 带来的压力，实现先加载核心部分的 View，再逐步去加载其他 View。比如在喜马直播间就可以先加载视频播放组件，然后再依次加载其他的功能组件。&nbsp;</p><p></p><h4>预渲染</h4><p></p><p></p><p>1.&nbsp;从外部页面进入直播间场景</p><p></p><p>直播间页面是在一个上下滑容器中进行加载的，从其他页面进入直播间遵循这样的时间线：直播间容器页面创建-&gt; 直播间容器页面渲染 -&gt; 直播间页面创建 -&gt; 直播间页面渲染。当点击入口跳转的时候调用播放器进行播放，如果在直播间页面渲染出来之前第一帧画面已经解码好了，这时由于直播间页面的 TextureView 的 Surface 仍然是不可用的状态，会导致解码的视频帧无法去送显。</p><p></p><p>因此，可以在直播间容器页面创建一个SurfaceView ，将解码的视频帧先渲染至容器页面的 SurfaceView 上（之所以用 SurfaceView ，因为它 Surface 的渲染可以放到单独线程去做，避免主线程卡顿造成画面无法渲染的问题），等到直播间 TextureView 的 Surface 可用后，再将视频帧渲染到 TextureView 上，实现无缝切换。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fb/fba81794a85eae4dc745d1c25c49d19a.png\" /></p><p></p><p>2.&nbsp;直播间上下滑场景</p><p></p><p>针对直播间上下滑场景，因历史原因，播放器是单例，所以在直播间上下滑场景中无法进行下一个直播间的预加载，只能等到下个直播间被选中时才能进行播放，也就是上下滑过程中手指抬起后开始播放下一个视频。同时，由于直播间view 是复用的，要等到屏幕停止滑动后，才可以去复用上个直播间的 view 。</p><p>因此，如果屏幕在滚动过程中，首帧已解码成功后，可先渲染至容器页面的SurfaceView 上，待屏幕停止滚动直播间 TextureView 的 Surface 可用后，再将视频帧渲染到 TextureView 上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/48345cdbe357d8a0af65828f035633d2.png\" /></p><p></p><h4>使用HttpDNS</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/2096fbe7946c9d8df69d622f7f35cf50.png\" /></p><p></p><p>网络请求中，DNS 解析是一个相对耗时的过程。在传统的 localDNS 方案中，APP 向所在网络运营商的 DNS Server 发起域名解析的请求，而运营商 DNS Server 会向 CDN 的 GSLB （Global Server Load Balancing）全局负载均衡系统发起递归查询，GSLB 通过运营 DNS Server 所属 IP 地址判断查询来自于哪个运营商和地理位置，然后返回若干合适的 CDN 边缘节点 IP 给它。</p><p></p><p>HttpDNS 使用 HTTP 协议进行域名解析，把域名解析请求直接发送到 HttpDNS 服务器，从而绕过运营商的 Local DNS ，能够避免 Local DNS 造成的域名劫持问题和调度不精准问题。由于中间少了一条链路，所以这部分的耗时大致可以省却几十毫秒。同时将节点 ip 进行缓存，下次有相同域名的请求时直接返回该 ip，省去 DNS 解析耗时。</p><p></p><h4>优化播放器缓存水位管理</h4><p></p><p></p><p>播放器缓冲区主要有两个水位管理概念，一个是启播水位，另一个是卡顿后启播水位。顾名思义，启播水位表示首次拉流播放时，缓冲区达到多长时间的数据可以开始播放。卡顿后启播水位表示如果缓冲区没有数据发生卡顿了，需要再下载达到多长时间的数据才可以继续播放。对于启播优化来讲，为了让用户能够更快地启播，可以将启播水位调的尽可能小。因此在直播场景下，我们将播放器的启播水位设置为了100ms，以达到更快的启播速度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4aa35299d18cea469c77b0ee92935308.png\" /></p><p></p><h3>成果与思考</h3><p></p><p></p><p>经过一个季度的优化，目前喜马音频直播的秒开率达到了&nbsp;90%&nbsp;以上：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c3/c3e3164840dfef02671ec233f9c92cb7.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>视频直播的秒开率达到了&nbsp;85% 以上：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7f2895bede1a3f7869b2e198ebaaaa4c.png\" /></p><p></p><h4>体验导向的应用理念</h4><p></p><p></p><p>通过优化直播启播速度，不同渠道直播入口的播放转化率都得到了不同程度的提升，因此可以得出一个体验导向的应用理念：在引入任何优化时，首先需要评估方案的整体收益；之后需要设计整体优化方案进行开发调优；通过AB 实验评估每项技术的业务指标影响，刻画性能指标与业务指标的关联；反复以上迭代过程。</p><p></p><h3>未来展望</h3><p></p><p></p><h4>H.265</h4><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8e4ebf973b9802214c60983b57a8bf5.png\" /></p><p></p><p>H.265 旨在在有限带宽下传输更高质量的网络视频，仅需 H.264 的一半带宽即可播放相同质量的视频。因此，对于相同质量的视频，H.265 只需下载 H.264 约一半的数据量即可进行解码渲染，有利于提高启播速度。</p><p>&nbsp;</p><p></p><h4>上下滑多播放器实例</h4><p></p><p></p><p>上下滑是喜马切换直播间的一个重要的方式，后续可以使用多个播放器实例，在手指按下开始滑动的时候就去静音播放下一个直播间的资源，待手指抬起确定滑入下个直播间后再取消静音播放，同时停止上个直播间的播放。相比于单播放器实例，这样可以省去拖动屏幕这段时间的耗时。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7d/7da8997dd257b7f2b43c0aa5245ee5b0.png\" /></p><p></p><h4>解码器复用</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e844b85866ef80b3b1d9e6c7559b429d.png\" /></p><p></p><p>&nbsp;</p><p>通常情况下，视频正常播放时解码器都需要进行create()、configure()、start() 等初始化操作，而播放其他视频时，每次播放都必须重新进行这一初始化流程。根据上面的解码器状态流转图，如果播放完一个视频后不去 release 、stop解码器资源，而是直接用于解码下一个视频，实现解码器的复用，就可以省去创建及初始化解码器的耗时。</p><p></p><p>解码器复用的核心条件是支持自适应播放属性，此属性是指Android 提供的一种无缝切换不同分辨率视频的能力，可以由系统接口(android.media.MediaCodecInfo.CodecCapabilities.isFeatureSupported(String)) 查询是否支持。同时还要求当前视频编码格式要保持相同，分辨率不能超过上个视频的分辨率。</p><p>&nbsp;</p>",
    "publish_time": "2023-07-12 17:20:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "为中国市场定制！英特尔发布7nm Gaudi2处理器，为深度学习训练和推理而生",
    "url": "https://www.infoq.cn/article/NYjsocRqzkxlpty1KWqe",
    "summary": "<p>2022年11月，<a href=\"https://www.infoq.cn/article/bQHRqFcQ1TlJCqHuczGR\">ChatGPT</a>\"发布后，国内多家互联网大厂宣布推出生成式AI大模型，这让原本有些沉寂的AI赛道一下子又火热起来。在底层硬件方面，中国市场的大模型厂商们急需一款在价格、性能、功耗上具有竞争力的芯片产品。</p><p></p><p>7月11日，<a href=\"https://www.infoq.cn/article/3yHIZDfmJI50IftLTohG\">英特尔</a>\"正式发布了面向中国市场的7nm制程第二代Gaudi深度学习加速器——Habana®&nbsp;Gaudi®2。</p><p></p><p>据英特尔称，这是一款为深度学习而生的全新Gaudi2训练加速器。该处理器以第一代Gaudi高性能架构为基础，以多方位性能与能效比提升，加速高性能大语言模型运行。该加速器具备：</p><p></p><p>24个可编程Tensor处理器核心（TPCs）21个100 Gbps（RoCEv2）以太网接口96GB HBM2E内存容量2.4TB/秒的总内存带宽48MB片上SRAM集成多媒体处理引擎</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cf4857afa9afdf0e9d0fcaef97872ac5.png\" /></p><p></p><p>Habana Lab是一家以色列公司，专注于高性能人工智能计算和机器学习加速器，其产品广泛应用于云计算、数据中心、嵌入式系统等领域。英特尔和Habana在人工智能领域的合作可以追溯到2018年，当时Habana Lab发布了其首款人工智能加速器Goya，该产品基于FPGA技术，能够加速深度学习算法，自此英特尔与Habana Lab建立了战略合作关系，共同开发和推广AI加速器。</p><p></p><p>2019年，Habana推出了第一代Gaudi芯片，该产品基于ASIC技术，具有更高的性能和能效。同年12月，Habana被英特尔收购。</p><p></p><p>早在2015年，英特尔就已经推出了第二代Gaudi加速器。该产品基于7nm工艺制造，采用了Habana的高能效架构，主要针对大型训练和大规模分布式训练应用场景。与Gaudi 1相比，Gaudi 2的性能和功能都有所提升，同时支持更多的深度学习框架和算法，能够更好地满足人工智能应用的需求。</p><p></p><p>此次在北京发布会上官宣的Gaudi 2，实际上是面向中国市场的“定制版”。据英特尔介绍，与国际通用版Gaudi2相比，中国市场的“定制版”在性能和功能上没有太大差异。</p><p></p><p>英特尔执行副总裁兼数据中心与人工智能事业部总经理Sandra Rivera指出，“英特尔致力于通过为客户提供广泛的硬件选择，并支持开放的软件环境，加速人工智能技术的发展。凭借包括至强可扩展处理器和Gaudi2深度学习加速器在内的产品组合，英特尔正在降低人工智能的准入门槛，并强化客户在云端通过网络和智能边缘部署这一关键业务技术的能力，从而帮助构建中国人工智能的未来。”</p><p></p><p>在6月公布的<a href=\"https://mlcommons.org/en/training-normal-30/\">MLCommons® MLPerf®基准测试</a>\"中，Gaudi2加速器的性能得到了充分认证，其在GPT-3模型、计算机视觉模型ResNet-50（使用8个加速器）、Unet3D（使用8个加速器），以及自然语言处理模型BERT（使用8个和64个加速器）上均取得了优异的训练结果。Gaudi2在帮助用户提升运营效率和降低运营成本方面表现不俗。</p><p></p><h2>满足大语言和多模态模型的需求</h2><p></p><p></p><p>Gaudi2深度学习加速器的架构旨在高效扩展，以满足大规模语言模型及生成式AI模型的需求。其每张芯片集成了21个专用于内部互联的100Gbps（RoCEv2 RDMA）以太网接口，从而实现低延迟服务器内扩展。</p><p></p><p>在Stable Diffusion训练上，Gaudi2展示了从1张卡至64张卡近线性99%的扩展性。此外，MLCommons刚刚公布的MLPerf训练3.<a href=\"https://mlcommons.org/en/training-normal-30/\">0</a>\"<a href=\"https://mlcommons.org/en/training-normal-30/\">结果</a>\"，亦验证了Gaudi2处理器在1750亿参数的GPT-3模型上，从256个加速器到384个加速器可实现接近线性95%的扩展效果。</p><p></p><h2>具备成熟的软件支持</h2><p></p><p></p><p>伴随日益增长的生成式AI及大语言模型需求，英特尔也在充分释放Gaudi2深度学习加速器性能。</p><p></p><p>为支持客户轻松构建模型，或将当前基于GPU的模型业务和系统迁移到基于全新Gaudi2服务器，并帮助保护软件开发投入，SynapseAI®软件套件针对Gaudi平台深度学习业务进行了优化，旨在与广泛的软件生态系统一起，帮助简化模型的开发和迁移。SynapseAI集成了对TensorFlow和PyTorch框架的支持，并提供众多流行的计算机视觉和自然语言参考模型，能够满足深度学习开发者的多样化需求。</p><p></p><p>目前，英特尔正与浪潮信息合作，打造并发售基于Gaudi2深度学习加速器的浪潮信息AI服务器NF5698G7。该服务器集成了8颗Gaudi2加速卡HL-225B，还包含双路第四代英特尔至强可扩展处理器。</p>",
    "publish_time": "2023-07-12 18:02:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "朱红林确认出席 ArchSummit 深圳，分享《如何在软件开发生命周期中高效管理开源组件》",
    "url": "https://www.infoq.cn/article/01cYqsoI5cOpVaoRM19y",
    "summary": "<p>7&nbsp;月&nbsp;21&nbsp;日&nbsp;-&nbsp;22&nbsp;日，&nbsp;在&nbsp;<a href=\"https://archsummit.infoq.cn/2023/shenzhen?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">ArchSummit&nbsp;全球架构师峰会（深圳站）</a>\"，OPPO&nbsp;软件工程部安全专家朱红林，将于会上发表题为《如何在软件开发生命周期中高效管理开源组件》的演讲，针对开源整改和提交代码过程中的痛点和难点，分享“Kernel&nbsp;代码自动化开源工具系统”的原理和实践。</p><p></p><p>朱红林的主要研究领域为“端管云”安全架构与设计、数据安全与隐私保护关键技术、网络安全国际和国家标准等，拥有丰富的安全工作经验。先后在华为和&nbsp;OPPO&nbsp;担任安全技术专家，负责多个核心产品的安全架构与设计，安全与隐私保护技术改进&amp;改善。拥有多项认证，PMP、CIPT等。2022&nbsp;年带领完成的“自动化开源工具系统”项目获得公司级技术型一等奖，指导多个团队完成系统改善、疑难问题攻坚，积累了大量经验。</p><p></p><p>相信通过朱红林的分享，你将了解到基于&nbsp;SDL&nbsp;流程下，常规开源&nbsp;Kernel&nbsp;的挑战与痛点，在开源治理过程中，会碰到的技术问题及对应的解法，收获&nbsp;Kernel&nbsp;代码自动化开源工具系统的最佳实践经验。</p><p></p><p>除上述议题外&nbsp;，ArchSummit&nbsp;深圳还将围绕<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1537?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">基础架构技术</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1532?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">DataOps、Data&nbsp;Fabric&nbsp;等高效数据开发与服务模式</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1534?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">Mesh&nbsp;技术实践案例</a>\"、<a href=\"https://archsummit.infoq.cn/2023/shenzhen/track/1535?utm_source=infoqweb&amp;utm_medium=teacherarticle&amp;utm_campaign=8&amp;utm_term=0531\">QUIC&nbsp;传输和架构优化</a>\"等进行分享。</p><p></p><p>数十位业界专家，上百个国内外一线大厂前沿技术案例，一定会给你带来很多全新的开发灵感。期待与你线下交流！咨询购票请联系&nbsp;18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/9d/aa/9d6a27547062ee2e089f91bdc4ba1eaa.png\" /></p><p></p>",
    "publish_time": "2023-07-12 18:05:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "LLM对程序员的冲击和影响",
    "url": "https://www.infoq.cn/article/13JGqTrdEdghk8mEwyRP",
    "summary": "<p></p><p>作者 | 茹炳晟，腾讯 Tech Lead</p><p></p><h2>LLM 在软件开发过程中的单点提效</h2><p></p><p></p><p>LLM 对软件研发的单点提效，我之前录制过一段视频，大家可以直接观看，里面有详细的演示，我在这里就不再赘述了。（视频可在微信观看：https://mp.weixin.qq.com/s/_Kh8IzsfghT4fPWknesnzA）</p><p></p><p>除此以外，我这里罗列一些更多的可能用途：</p><p></p><p>智能代码提示代码片段智能生成SQL 语句的智能生成与调优更高效更精准的静态代码检查与自动修复（非 rule-based）智能辅助的代码评审与代码重构单元测试和接口测试代码的自动生成更高级的重复代码检查（语义重复检查）失败用例的自动分析与归因更精准的技术问答…</p><p></p><p>看到这里，你有可能会得出一个结论：完蛋了，程序员要大面积失业了。真的是这样吗？要回答这个问题，我们需要从全局来看问题，首先我们要搞清楚，LLM 对于软件研发，什么变了？什么没有变？</p><p></p><p></p><h2>LLM 对于软件研发，什么变了？</h2><p></p><p></p><p>看了上面的案例，你应该已经能够体会到 LLM 对于软件研发单点效率提升的各种可能性，这些能力让我们看到了软件研发的变化，我把这些变化总结为：基础编码能力的知识平权，进而带来局部效率的提升。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b6/b6f922a22e35879032758b95ab7531b1.png\" /></p><p></p><p>以前工程师个体学习掌握一门计算机语言以及相应的数据结构和算法，需要较长的学习周期，很多经验和模式还需要工程师个体在大量实践中进行总结，每个工程师个体都在重复着这个过程，现在 LLM 让一个没有接受过系统培训的个体也能拥有同样的能力，个体和个体之间的能力差异被 LLM 拉平了，这就是知识平权。</p><p></p><p>如果说 ChatGPT 实现了数字时代知识的平权，codex 类的代码语言大模型实现了基础编码能力的知识平权，进而带来软件研发的局部效率提升。</p><p></p><p>可以说，LLM 降低了软件开发的门槛，可以让更多对软件开发感兴趣的人更加轻松地参与到软件开发工作中，同时，LLM 提高了编程的效率和质量，使我们可以在更短的时间内完成更多的工作，因而能留出更多的时间让我们思考。</p><p></p><p>前段时间，前哈佛大学计算机科学教授，曾在 Google 和 Apple 担任高级工程职位的 Matt Welsh 发表了一个视频，其中的主要观点是“LLM 将会代表着编程的终结”，他认为程序员会被淘汰，未来只有产品经理和代码审查员。我不知道大家对这个怎么看？</p><p></p><p>我的观点是，在抱有敬畏之心的同时，我们不要轻易下结论。为什么，因为软件研发还有很多东西是没有变的，而这些没有变的才是软件工程中的核心问题和主要矛盾。</p><p></p><p></p><h2>LLM 对于软件研发，什么没有变？</h2><p></p><p></p><p>我们面对的是软件工程的问题，编程不等于软件工程，编程只是软件工程的一部分。软件工程的四大内在特性（复杂度、不一致性、可变性、不可见性）并没有因为 LLM 的出现而发生本质上的变化，这才是软件工程面临的主要矛盾。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/67/674dbdda3fa5ea4c7f51fceeff7734f8\" /></p><p></p><p>从复杂度的角度来看，问题域本身的复杂度并没有变，本质复杂度也没有变，变的可能只是一部分的随机复杂度。虽然说局部编码变简单，或者说更高效了，但是需求分析和软件设计并没有因为 LLM 而变得简单，这个稍后我们来详聊。</p><p></p><p>从一致性的角度来看，由于软件研发的本质依然是“知识手工业者的大规模协作”，所以我们非常需要一致性。如果系统是一致的，则意味着相似的事情以相似的方式完成，错并不可怕，怕的是错的千变万化。LLM 的出现并没有提升软件研发的一致性，甚至由于 LLM 本身的概率属性，使用 LLM 实现代码生成的不一致性问题反而是被放大了，这点我们后面展开讲。</p><p></p><p>从可变性的角度来看，软件会随着需求不断演进和变化，所以架构设计和模块抽象只能面向当下，它天然是短视的，或者说是有局限性的，这种局限性即使是最优秀的架构师也是不可逾越的。</p><p></p><p>在敏捷开发模式下这个问题更被凸显了出来，而且需求本身就是零散的，目标也是模糊的，在没有全局视图的情况下，架构自然就是有局限性，所以需要不断迭代变更。每个迭代，你能拿到的信息仅仅是宏大视图中的小小一角，根本没有全貌，LLM 对此也是无能为力的。</p><p></p><p>从不可见性的角度来看，软件的客观存在不具有空间的形体特征，不同的关注点，会有不同的图。综合叠加这些图是困难的，而且强行可视化的效果会造成图的异常复杂，反而失去了可视化的价值。设计无法可视化就限制了有效的沟通和交流。</p><p></p><p>如果以上四点再叠加上大型软件的规模效应，其中包含软件系统本身的规模和软件研发团队的规模，问题就更严重了，即会显著提升软件研发过程中的沟通成本、决策成本、认知成本和试错成本，而这些才是软件工程问题的本质，这些本质问题自始至终都没有变过，LLM 对此也基本无能为力。</p><p></p><p>基于上述的分析，我们可以看到，软件工程的核心矛盾并没有改变，现代软件工程应对的是规模化场景下的各种问题，基于 LLM 实现的编程提效只是其中的一小部分，而其中最重要的需求和代码演进模式都没有发生本质变化，我们接下里分别展开讨论一下。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b2/b27c75e30b597e7c3299627ed536b4dc.png\" /></p><p></p><p></p><h2>需求的重要性没有变，在 LLM 时代还被放大了</h2><p></p><p></p><p>只有我们的需求足够的清楚，那么生成的代码才会准确。如何准确全面描述需求成为了关键。面向自然语言编程，首先你要有能力把话讲清楚。但是问题是：你能讲清楚吗？</p><p></p><p>我们通过一些实践发现，要把需求描述到让它能正确写出来代码，需要的工作量似乎已经接近甚至超过编码了。为什么会这样，有两个方面的原因。</p><p></p><p>一是因为大多数的代码实现是 imperative 的，而需求描述是 declarative 的，这两者对人的要求完全不一样。我们程序员群体接受的教育是编程，而不是需求描述，也就是说程序员本来更擅长写代码，而不是描述需求。</p><p></p><p>二是因为在当前的开发模式下，程序员直接用代码默认帮需求（产品经理）做了很多代偿。很多在需求中没有明确提及的内容被程序员用代码直接实现了（代偿）。而现在要倒过来先把需求的细节完全理清楚这个可能不是程序员当前的工作习惯。而且代码的信息熵其实要大于自然语言，程序员更善于用代码而非自然语言来描述事务。</p><p></p><p>举个例子：如何清楚地描述一个排序函数 sort 的需求？sort 输出的数字必须是从小到大排列的，这样描述需求就够了吗？其实远远不够，重复数字怎么处理？排序数据的数量有没有上限？有的话如何提示？排序时长需要有超时设计吗？是预先判定还是中途判断？算法复杂度有明确的要求吗？算法需要应对并发吗？并发的规模怎么样？等等。</p><p></p><p>一个软件的需求，不仅仅是功能性的，还有很多非功能的需求，这些都是需要描述清楚的。另外代码实现的时候，还要考虑为可测试而设计，为可扩展而设计，为可运维而设计，为可观测而设计等等。原本这些很多是开发代偿了，现在要从需求生成代码，你必须要提前讲清楚。</p><p></p><p>所以，我们的总结是：“软件从业者高估了编程的复杂度，但是却低估了功能和设计的深刻度”。</p><p></p><p></p><h2>代码是持续“生长”出来的，需要持续更新</h2><p></p><p></p><p>对于现行的软件开发范式，当需求发生变动后，一般是会在原有代码基础上改动，而不是直接从头全量生成全部代码，这个时候，LLM 本质上做的是局部编程的辅助（pair programming）。局部编程辅助过程中，经常需要对代码做局部修改，而这个往往并不容易。</p><p></p><p>我们知道，代码的信息熵大于自然语言，用信息熵更低的自然语言去描述代码，尤其是准确描述大段代码中的若干个位置往往是困难的。想象一下，如果只用在线聊天的方式跟别人讲在代码的什么地方做修改的效率是何其低下，相比指着屏幕，或者使用专门的 CR 工具，效率的差距是巨大的。</p><p></p><p>如果需要进一步描述如何修改就会更困难，因为大概率需要用到很多代码上下文的相关描述，所以对于 prompt 的表述要求以及长度要求都很高。</p><p></p><p>另外，LLM 接纳修改意见（prompt）后的输出本身也是不稳定和不收敛的，同时也具有不可解释性，LLM 本质上不是基于修改意见（prompt）进行改写，而是基于修改意见（prompt）重新写了一份。输出的代码需要人重复的阅读和理解，使得认知成本变高了。</p><p></p><p>同时，LLM 的原理决定了其会“一本正经的胡说八道”的本质，会混合捏造一些不存在的东西，可以说人工智能的混合捏造是人工智能在无知情况下的“自信”反应，而这个点在代码生成上是灾难性的，比如会将不同类型的 SQL 语句混在一起使用，会分不清 Go 语言的 os.Kill 和 Python 语言的 os.kill()。这个问题可能需要使用 AI 审计 AI 的方式来缓解了。</p><p></p><p>刚才提到，要在原有代码基础上修改，就需要利用已有的代码上下文，而不是从 0 开始。要实现这一点，一个最朴素的做法就是把整个项目的代码都贴到 prompt 里，但这样并不现实。因为 GPT-3.5 限制最多只能 4096 个 tokens，GPT-4 最多 8192 个，除非项目非常小，否则根本放不下。这个问题可能需要用 langchain 来解决了。</p><p></p><p>LangChain 是一个链接面向用户程序和 LLM 之间的一个中间层，通过输入自己的知识库来“定制化”自己的 LLM。langchain 使用 embedding 建立基于项目特定的向量知识库，实现“基于特定文档的问答”。</p><p></p><p></p><h2>LLM 时代，对软件研发更多的思考</h2><p></p><p></p><p></p><h4>思考 1：替代的是码农，共生的是工程师</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/04/04225893a2587291d72148d8f7e63eb6.png\" /></p><p></p><p>在软件开发过程中，当伪代码级别的设计完成后，最后一公里的编码实现会被 LLM 替代，因为基于记忆的简单重复编码不是人的优势，而是机器的优势。</p><p></p><p>这部分工作现在属于码农，也就是我们俗称的 CRUD 仔和 API Boy，所以很多不涉及设计的码农可能会被大模型替代。</p><p></p><p>而工程师需要关注业务理解、需求拆分、架构设计、设计取舍，并在此基础上通过 prompt 学会与 AI 合作，从而实现“工程师 + LLM”形成 1+1 &gt;2 的效果。这就是共生。</p><p></p><p>需要注意的是，这种共生必须始终保持人的主观能动性，机器必须是 Copilot，也就是智能副驾驶，主驾驶位置必须是人，这样的人 - 机关系才能长期健康发展。这也就是为什么说微软现任 CEO 萨提亚强调 Copilot（智能副驾驶）是比 Autopilot（自动驾驶）还先进的根本原因。</p><p></p><p>另外，特别要提的是：短期内率先学会使用 LLM 的工程师必将获益，但是很快大家都会掌握，这个时候能力水平就再次被拉平了，这个很像之前“外卖骑手困在系统里”那篇文章中的观点，所以作为共生的工程师，我们更需要从以下三个方面来强化自己的能力：</p><p></p><p>需求理解、需求分析、需求拆解的能力架构设计、架构分析、设计取舍的能力，并推动设计的文档化和规范化理解问题本质，而不是单纯学习应用（授人以鱼不如授人以渔）</p><p></p><p></p><h4>思考 2：有利于控制研发团队规模，保持小团队的优势</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c3/c3fb829156062a464c374c0bdaf79e5f.png\" /></p><p></p><p>随着一个软件规模的扩展，软件项目参与的人越来越多，分工越来越细，人和人之间需要的沟通量，也指数增长。很快你会发现，沟通花费的时间，渐渐地就比分工省下来的时间还要多。说白了，过了一个临界点，人越多不是越帮忙，而是人越多越添乱。一个人 12 个月能完成的事，不见得上 12 个人 1 个月就能完成，甚至 12 个月也未必能完成。</p><p></p><p>《人月神话》里建议了一种组织方式，叫“外科手术式的队伍”。就像一台外科手术一样，有一个主刀大夫，软件项目也应该有一个首席程序员，其他人都是给他提供支持的。这样，就既能获得由少数头脑产生的产品完整性，又能得到多位协助人员的总体生产率，还彻底地减少了沟通的工作量。</p><p></p><p>但是软件规模大了之后，需要的程序员数量必然会更多，团队规模一定会加速扩展。但是 LLM 的出现，让基础编程工作一定程度上实现了自动化，这样非常有利于控制研发团队规模，保持小团队的效率优势。</p><p></p><p></p><h4>思考 3：暗知识</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b6/b6e7318df38dc68464e732906e14357f.png\" /></p><p></p><p>大模型的成功很大程度上来自于对已有的互联网文本语料和专业书籍等资料的学习。相对应，在软件工程领域，需要学习的不仅仅是代码，还应该包括需求，设计。</p><p></p><p>但是，很多需求和设计并不以文档的形式存在，往往会存在于程序员和架构师的脑子里，或者在讨论的过程中。就算有文档，文档和代码大概率不同步。就算文档同步，文档（需求和设计）背后经常有大量的方案对比和推敲，甚至有很多要在原有债务基础上的设计妥协，这些决策过程一般都不会明确地被记录下来。这些没有被文档化下来的知识，我们称其为“暗知识”。</p><p></p><p>虽然我们说只要有足够的数据，大模型就可以学到需求和设计知识。但这些“暗知识”本身就很难被捕捉到，“足够的数据”这一前提在需求分析和软件设计可能难以满足。</p><p></p><p>另外，在实际软件开发中，需求可能一次不能表达得很清楚，需要一边开发一边逐步写清楚需求。尤其是敏捷开发更是如此。所以一些通用的，不需要特定领域知识的问题，LLM 的表现会比较好，但是那些专用的，需要特定领域知识（私域知识）的问题，LLM 就可能不是很擅长。</p><p></p><p>总结来看：“你能想到的多过你能说出来的，你能说出来的多过你能写下来的。”所以这就天然限制了 LLM 能力的上限。</p><p></p><p></p><h4>思考 4：Prompt 即代码，代码不再是代码</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/31/31e33ab40a1249d3210c2eff7e0eacfb.png\" /></p><p></p><p>让我们做个大胆的设想，如果当软件需求发生变化的时候，我们不再是去改代码，而是直接修改需求对应的 prompt，然后基于 prompt 直接生成完整的代码，这个将是软件开发范式的改变。</p><p></p><p>在这种范式下，我们需要确保代码不能有人为修改，必须都是由 prompt 直接生成，此时我们还需要对 prompt 做版本管理，或许会出现类似今天 git 的 prompt 版本管理的新物种。</p><p></p><p>此时，从本质上来看 prompt 即是代码，而原本的代码不再是代码了，这就真正实现了基于自然语言（prompt）的编程，此时的编程范式将从 prompt to code 转变为 prompt as code。</p><p></p><p>更进一步思考，当实现了 prompt as code，我们是否还需要 code，关于代码的很多工程化实践还重要吗？现在我们之所以认为代码工程化很重要，因为代码是人来编写，代码是人来维护的。而当代码由 LLM 来编写，由 LLM 来维护的话，那么现有的软件架构体系还适用吗？这个时候或许才真正实现了软件研发范式的进化。</p><p></p><p></p><h4>思考 5：直接可运行，prompt to executable 软件开发范式的可能性</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3e/3e106b0a70b5dcd08cf2301a36790233.png\" /></p><p></p><p>在深入一步思考，直接可运行，prompt to executable 的基础设施会出现吗？</p><p></p><p>代码只是软件工程中的一部分，远远不是软件工程的全部，你想想你有多少时间占比是在编码的。一般来讲，编码完成后往往要经历 CI 和 CD 等一些列的工程实践才能向终端用户交付价值。</p><p></p><p>所以全新的软件范式是否可以实现从 prompt 直接到可运行的程序实例？目前来看，或许 Serverless 是可能的架构之一。</p><p></p><p></p><h4>思考 6：计算机教育的反思</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a5/a5e2ab8ac181db106eab39c2797ee2ec.png\" /></p><p></p><p>LLM 出现后，关于对计算机教育的反思我认为有两个层面的思考：</p><p></p><p>首先是，计算机学科研究方向的变化，之前 NLP、知识图谱、代码理解，代码缺陷发现、test oracle 生成等都是独立的研究方向，但是 LLM 表现出的 AGI 能力似乎让这些垂直领域的研究失去的意义，因为 LLM 的 AGI 能力都能解决，或许效果还更好。</p><p></p><p>所以这些研究方向将何去何从是需要我们思考的。有人说 LLM 是 NLP 的新里程碑，但也有人认为其更像是 NLP 的墓志铭，这句话很好的表达了我的观点。</p><p></p><p>其次，LLM 一次次地证明了通过“死记硬背 + 简单推理”就能通过大多数人类的考试和技术面试。那教育的终极目标又是什么？先进的人工智能尝试把机器培养成人，而落后的教育试图把人培养成机器。计算机教育，其实我们整个教育到了需要彻底反思的时刻了。</p><p></p><p>或者我们都错了？！</p><p></p><p>彼得德鲁克说过“动荡时代的最大风险不是动荡本身，而是企图以昨天的逻辑来应对动荡”，今天 LLM 对软件工程的影响，我还是在以以往的逻辑在做分析，这个基础可能本来就是错误，全新的时代需要全新的思维模式，然后我们拭目以待。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174511&amp;idx=1&amp;sn=e33ac1a3fff2cea0658f937a1c75ff84&amp;chksm=bdb84e3c8acfc72a27a7754b298c859e8e770a9c2caaa855962e0a1765446b4115afcbd7b612&amp;scene=21#wechat_redirect\">用计算机视觉识别模型种生菜？“科技+农业”还能这么玩！</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174439&amp;idx=1&amp;sn=e09cfe8c92a5d835da140b51ee04f1fb&amp;chksm=bdb84e748acfc762a10c11c3ed34bb997b3e075208fa8b1a5062eee9fc4e57747c9ea1b75110&amp;scene=21#wechat_redirect\">对话贾扬清、关涛、张伯翰：AI 平民化趋势下，数据架构将被彻底颠覆？</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174039&amp;idx=1&amp;sn=75e4852f03bb624621077065d78d0b15&amp;chksm=bdb84cc48acfc5d24605dfe5ae0cb94f405a328b302c86bef8bf869038b45d9127135bdbc6b8&amp;scene=21#wechat_redirect\"></a>\"<a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174083&amp;idx=1&amp;sn=74d9e3bab9e8f47afff78493f35c59a5&amp;chksm=bdb84c908acfc586a3cf83b4acc91d4ed0a11e6afb5981d6d2739c49068ee205dd55b4bd9050&amp;scene=21#wechat_redirect\">一场马斯克的反爬闹剧：Twitter一夜回到五年前？</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651174039&amp;idx=1&amp;sn=75e4852f03bb624621077065d78d0b15&amp;chksm=bdb84cc48acfc5d24605dfe5ae0cb94f405a328b302c86bef8bf869038b45d9127135bdbc6b8&amp;scene=21#wechat_redirect\">对话开源泰斗陆首群教授：中国开源发展应追求0到1的爆发性创新，而不是0到0的假创新</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-07-12 20:47:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]