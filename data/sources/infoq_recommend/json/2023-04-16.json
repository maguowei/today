[
  {
    "title": "Databricks来搅局了：0门槛克隆ChatGPT，完全开源可随意修改商用",
    "url": "https://www.infoq.cn/article/5IU3aWsb8Dyyv0evWg2b",
    "summary": "<p></p><blockquote>全球首个完全开源的大语言模型，性能堪比GPT3.5！</blockquote><p></p><p>&nbsp;</p><p>大数据热潮催生了许多成功的公司，例如 Snowflake、Databricks、Splunk 和 Cloudera。现在我们进入了生成式人工智能时代，那么会不会有新的“人工智能和大数据”结合方式？</p><p>&nbsp;</p><p>最近，<a href=\"https://www.infoq.cn/article/s2a1EQomEYyaoktLPvdT\">大数据公司Databricks</a>\"就在生成式人工智能领域采取了行动。两周前，该公司发布了一个名为 Dolly 的开源大型语言模型，旨在应对市场对于生成式AI及相关应用的旺盛需求，我们可以称之为 Dolly 1.0。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14fb978843e4d61a7d697967b69fd93b.png\" /></p><p></p><p>&nbsp;</p><p>像 ChatGPT 和 Bard 这样的生成式 AI，它们使用的数据通常来自于在成千上万不同网站，使用的数据量十分惊人，而且想要使用这些数据训练 AI 还需要数以千计的强大 GPU 在背后提供支持。Databricks 希望通过开源 Dolly 1.0及其训练数据，让任何人都能开发出一个真正像人类的 AI，而无需投资数百万美元，这让这类 AI 不再是只有大型科技公司才能负担得起的东西，数以百万计的小公司也将能够从中受益。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/0jlMtAfeJUy5T1R*GYGS\">Databricks</a>\" 首席执行官 Ali Ghodsi 表示，Dolly 1.0 只需要非常少的数据和非常短的时间就能完成训练，“只需 30 美元、一台服务器和三个小时，我们就能教 Dolly 开始进行人类级别的交互。”</p><p>&nbsp;</p><p>4月12日，Databricks再次发布了该大语言模型（LLM）的开源迭代版本，并命名为Dolly 2.0。Databricks 表示，Dolly 2.0 是业内第一个开源、遵循指令的 LLM，它在透明且免费提供的数据集上进行了微调，该数据集也是开源的，可用于商业目的。这意味着 Dolly 2.0 可用于构建商业应用程序，无需支付 API 访问费用或与第三方共享数据。</p><p>&nbsp;</p><p></p><h2>Dolly 2.0 的诞生</h2><p></p><p>&nbsp;</p><p>Dolly 1.0 基于 EleutherAI 在 2021 年开源的自然语言处理模型GPT-J。GPT-J 是一个基于 GPT-3，由 60 亿个参数组成的自然语言处理 AI 模型。但该模型使用了来自 StanfordAlpaca 项目的 5.2 万个问答数据集，是根据 OpenAI 的 ChatGPT 的输出进行训练的，因为OpenAI 的使用条款限制，Dolly 1.0 并不能用于商业用途。</p><p>&nbsp;</p><p>Databricks在官方博文中指出，“用于训练Dolly 1.0的数据集中，包含来自ChatGPT的输出。斯坦福团队明确提到，OpenAI的服务条款试图阻止任何人创建能够与其竞争的AI模型。”</p><p>&nbsp;</p><p>Dolly 2.0建立在Databricks公司首版Dolly的基础之上，为了规避这个问题并建立起可供商用的模型，Databricks使用基于EleutherAI的Pythia模型家族中的120亿参数语言模型，成功构建起了Dolly 2.0。</p><p>&nbsp;</p><p>该公司表示，他们专门在5000名Databricks员工之内开展了众包，通过高质量的人工生成指令建立起训练数据集，借此完成了模型训练和微调。该公司将这套高质量的人工生成响应/揭示数据集称为databricks-dolly-15k，其使用Creative Commons Attribution-ShareAlike 3.0&nbsp;Unported License许可。</p><p>&nbsp;</p><p>“任何人均可出于任何目的使用、修改或扩展这套数据集，包括商业应用程序。”Databricks还强调，该数据集可通过GitHub页面（<a href=\"https://github.com/databrickslabs/dolly/tree/master/data\">https://github.com/databrickslabs/dolly/tree/master/data</a>\"）直接下载。</p><p>&nbsp;</p><p>模型权重则可通过Databricks Hugging Face页面（<a href=\"https://huggingface.co/databricks\">https://huggingface.co/databricks</a>\"）处下载获取。</p><p>&nbsp;</p><p></p><h2>Dolly 2.0想成为大小公司的福音</h2><p></p><p>&nbsp;</p><p>Databricks之所以发布基于开源数据的大语言模型，主要是考虑到企业客户对控制模型并引入针对性场景/特定用例的需求。这也与行业常见的商业闭环训练模型（例如ChatGPT）形成了鲜明对比。</p><p>&nbsp;</p><p>市场调研公司Omdia首席分析师Bradley Shimmin表示，“Dolly 2.0这类模型大多是开放的，不需要在大规模GPU集群上进行长达数月的训练，因此为那些希望构建内部生成式AI方案的企业打开了新世界的大门。”</p><p>&nbsp;</p><p>Shimmin指出，“这些小型（即训练参数的规模较小）模型使用大量提示/响应对作为训练数据，因此特别适合希望控制整个解决方案、支持针对性用例的企业客户。例如，他们可以利用现有问答配对建立的帮助台数据库训练自己的AI模型。”</p><p>&nbsp;</p><p>根据咨询公司Amalgam Insights首席分析师Hyoun Park的说法，开源大语言模型的另一大优势，在于Dolly 2.0这类成果能够让企业更好地跟踪数据治理和驻留，并与所支持的用例保持良好的关联性。</p><p>&nbsp;</p><p>Park还专门拿OpenAI的名称打趣，说“因为OpenAI的ChatGPT等其他模型在使用时要依赖于API。对某些企业而言，这种依赖性可能引发关于API的合规性、治理或数据安全问题。”</p><p>&nbsp;</p><p>这也相当于，Dolly 2.0和其他基于开源的大语言模型将在受严格监管的行业中成为各企业的福音。这是个良好的开端，让企业意识到他们也可以创建并拥有自己的模型，且无需支付API访问费或与大语言模型提供商共享数据。这些在受到严格监管的行业中都可能产生巨大的问题。</p><p>&nbsp;</p><p></p><h3>开源与闭源大语言模型间的区别</h3><p></p><p>&nbsp;</p><p>与闭源大语言模型相比，基于开源的模型所使用的训练数据对公众开放，因此可根据业务进行微调和定制以满足企业需求。相比之下，ChatGPT等闭源模型则根据其开发者OpenAI所掌握的训练进行训练，模型可通过API付费访问，且禁止直接用于商业用途。</p><p>&nbsp;</p><p>Chandrasekaran认为，“「开放式大语言模型」可以有多种理解方式。最明显也最重要的一点，就是对这些模型的源代码和部署灵活性做出调整。除此之外，开放的范围还可以涵盖模型权重、训练数据集以及开放/协作方式层面的决策。”</p><p>&nbsp;</p><p>IDC的Schubmehl表示，Dolly 2.0就遵循基于开源的模型这一理念。“Dolly 2.0是一套大语言模型，模型本体、训练代码、数据集和模型权重都可作为开源资源从Databricks处获取，以供企业根据业务需求创建自己的定制化大语言模型。”Schubmehl同时提到，这种方法与其他大语言模型形成了鲜明对比，后者往往并不开放模型构建中的各类组成要素。</p><p>&nbsp;</p><p>分析人士还提到，闭源与开源大语言模型间的另一个区别，主要体现在训练的参数量上。其中闭源大语言模型的参数规模往往更大。以ChatGPT4为例，其训练中使用到100万亿个参数；相比之下，Dolly 2.0的参数量只有区区120亿个。</p><p>&nbsp;</p><p></p><h2>Dolly 2.0如何融入Databricks的生成式AI战略</h2><p></p><p>&nbsp;</p><p>Constellation Research 的&nbsp;Thurai&nbsp;表示，Databricks此次推出Dolly 2.0可以算是其夺取生成式AI市场份额的一项重要战略。</p><p>&nbsp;</p><p>“从本质上讲，众多大语言模型和基础模型业务都被掌握在超大规模企业手中。每家企业都有自己的变体——微软有ChatGPT、谷歌有Bard，AWS则通过Huggingface合作伙伴关系提供基础设施、流程、工具及模型共享和目录服务。Databricks当然不能坐以待毙，必须在热火朝天的大语言模型市场上分一杯羹。”</p><p>&nbsp;</p><p>其他分析师则认为，Dolly的发布符合Databricks公司向市场投放开源产品的战略。</p><p>&nbsp;</p><p>IDC的Schubmehl表示，“Databricks的专长，就是通过各种开源AI工具和服务帮助客户充分利用自己的数据和运营体系。Dolly是另一个绝佳安全，能够为组织提供基于最新AI技术的选项，也就是大语言模型。”但分析师们也承认，Databricks的Dolly 2.0恐怕无法立刻对ChatGPT或Bard等竞争对手产生影响。</p><p>&nbsp;</p><p>Omdia公司的Shimmin认为，“Dolly乃至其他开源生成式AI大语言模型的出现，将彻底颠覆Bard、ChatGPT和Galactica等现有大语言模型的未来前景。但从中短期来说，这些成果在Google Workplace、微软Office等产品中的地位还将稳固地维持下去。”</p><p>&nbsp;</p><p>Amalgam Insights的Park则给出了不同意见，认为Dolly最终会成为ChatGPT这类通用工具的功能伴侣。“人们会从通用工具中学习如何使用和提示生成式AI，而Dolly这类模型则负责帮助用户处理更具体、更专业的特定工作用例。”</p><p>&nbsp;</p><p>另外，也有评论指出，Dolly-like LLM 的一个能力是可以用来编写代码，特别是SQL 代码。这可能会导致非 SQL 专家能够在 Databricks lakehouse 上设置和运行查询。&nbsp;</p><p>&nbsp;</p><p>这可以从两方面来理解：第一，SQL 开发人员可以使用它来提高工作效率，第二，你不需要那么多 SQL 开发人员。Dolly 可以减少 Databricks 对 SQL 程序员的需求。将这种想法扩展到 Snowflake 和所有其他数据仓库环境，SQL 技能在未来可能会变得不那么有价值。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\">https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a>\"</p><p><a href=\"https://www.infoworld.com/article/3693349/why-did-databricks-open-source-its-llm-in-the-form-of-dolly-2-0.html\">https://www.infoworld.com/article/3693349/why-did-databricks-open-source-its-llm-in-the-form-of-dolly-2-0.html</a>\"</p>",
    "publish_time": "2023-04-16 12:20:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "集群算力提升3倍，腾讯云发布新一代高性能计算集群",
    "url": "https://www.infoq.cn/article/N8Wcppi2vFxkh3C0VdAw",
    "summary": "<p>4月14日，腾讯云正式发布面向大模型训练的新一代HCC（High-Performance Computing Cluster）高性能计算集群。该集群采用最新一代腾讯云星星海自研服务器，搭载了NVIDIA H800 Tensor Core GPU，并提供业界目前最高的3.2T超高互联带宽。</p><p>&nbsp;</p><p>实测结果显示，腾讯云新一代集群的算力性能较前代提升高达3倍。</p><p>&nbsp;</p><p>去年10月，腾讯完成首个万亿参数的AI大模型——混元NLP大模型训练。在同等数据集下，将训练时间由50天缩短到11天。如果基于新一代集群，训练时间将进一步缩短至4天。</p><p>&nbsp;</p><p>大模型进入万亿参数时代，对算力的需求陡增。在单体服务器计算能力有限的情况下，需要将上千台服务器相连，打造大规模、分布式的高性能计算集群。腾讯云新一代集群通过对单机算力、网络架构和存储性能进行协同优化，能够为大模型训练提供高性能、高带宽、低延迟的智算能力支撑。</p><p>&nbsp;</p><p>计算层面，服务器的单机性能是集群算力的基础。在非稀疏规格情况下，新一代集群单GPU卡支持输出最高&nbsp;495 TFlops（TF32）、989 TFlops （FP16/BF16）、1979 TFlops（FP8）的算力。针对大模型训练场景，腾讯云星星海服务器采用6U超高密度设计，相较行业可支持的上架密度提高30%；利用并行计算理念，通过CPU和GPU节点的一体化设计，将单点算力性能提升至最强。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4a4cd29400890015d02823e7bb8e966.png\" /></p><p></p><p>网络层面，计算节点间存在海量的数据交互需求，随着集群规模扩大，通信性能会直接影响训练效率。腾讯自研的星脉网络，为新一代集群带来了业界最高的3.2T的超高通信带宽。节点内外统一的AllReduce通信带宽，实现网络和算力的最大协同。实测结果显示，搭载同样的GPU，最新的3.2T星脉网络相较1.6T网络，能让集群整体算力提升20%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/06382e2beb46d6e1f612deda75989592.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/764e8989343128fe9bc599283a32d0bd.png\" /></p><p></p><p>&nbsp;</p><p>基于多轨道聚合的无阻塞网络架构、主动拥塞控制和定制加速通信库，腾讯云能提供业界领先的集群构建能力，支持单集群高达十万卡级别的组网规模。在超大集群场景下，仍然能保持优秀的通信开销比和吞吐性能，满足大模型训练以及推理业务的横向扩展。</p><p>&nbsp;</p><p>同时，腾讯自研高性能集合通信库TCCL，基于星脉网络硬件平台深度优化，在全局路径规划、拓扑感知亲和性调度、网络故障实时告警/自愈等方面融入了定制设计的解决方案。相对业界开源集合通信库，为大模型训练优化40%负载性能，消除多个网络原因导致训练中断问题。</p><p>&nbsp;</p><p>存储层面，训练场景下，几千台计算节点会同时读取一批数据集，需要尽可能缩短数据集的加载时长。新一代集群，引入了腾讯云最新自研存储架构，支持不同场景下对存储的需求。</p><p>&nbsp;</p><p>COS+GooseFS对象存储方案，提供多层缓存加速，大幅提升端到端的数据读取性能；将公开数据集、训练数据、模型结果统一存储到对象存储COS中，实现数据统一存储和高效流转。同时，GooseFS按需将热数据缓存到GPU内存和本地盘中，利用数据本地性提供高性能访问。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1f/1f33057811c49e92f7e220dc1a3c6283.png\" /></p><p></p><p>&nbsp;</p><p>CFS Turbo高性能并行文件存储方案，采取多级缓存加速，基于全分布式架构，提供100GB/s带宽、1000万IOPS的极致性能。并通过持久化客户端缓存技术，将裸金属服务器本地NVMe&nbsp;SSD和Turbo文件系统构成统一命名空间，实现微秒级延时，解決大模型场景大数据量、高带宽、低延时的诉求。同时，通过智能分层技术，自动对冷热数据分层，节省80%的存储成本，提供极致的性价比。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0ceb1e8ef204249025cc6301d69628c.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>底层架构之上，针对大模型训练场景，新一代集群集成了腾讯云自研的TACO Train训练加速引擎，对网络协议、通信策略、AI框架、模型编译进行大量系统级优化，大幅节约训练调优和算力成本。</p><p>&nbsp;</p><p>腾讯混元大模型背后的训练框架AngelPTM，也已通过腾讯云对外提供服务，帮助企业加速大模型落地。目前，腾讯混元AI大模型已经覆盖了自然语言处理、计算机视觉、多模态等基础模型和众多行业、领域模型。</p><p>&nbsp;</p><p>在腾讯云上，企业基于TI 平台的大模型能力和工具箱，可结合产业场景数据进行精调训练，提升生产效率，快速创建和部署 AI 应用。</p><p>&nbsp;</p><p>此前，腾讯多款自研芯片已经量产。其中，用于AI推理的紫霄芯片、用于视频转码的沧海芯片已在腾讯内部交付使用，性能指标和综合性价比显著优于业界。其中，紫霄采用自研存算架构，增加片上内存容量并使用更先进的内存技术，消除访存能力不足制约芯片性能的问题，同时内置集成腾讯自研加速模块，减少与CPU握手等待时间。目前，紫霄已经在腾讯头部业务规模部署，提供高达3倍的计算加速性能，和超过45%的整体成本节省。</p><p>&nbsp;</p><p>目前，腾讯云的分布式云原生调度总规模超过1.5亿核，并提供16 EFLOPS（每秒1600亿亿次浮点运算）的智算算力。未来，新一代集群不仅能服务于大模型训练，还将在自动驾驶、科学计算、自然语言处理等场景中充分应用。</p><p>&nbsp;</p><p>以新一代集群为标志，基于自研芯片、星星海自研服务器和分布式云操作系统遨驰，腾讯云正通过软硬一体的方式，打造面向AIGC的高性能智算网络，持续加速全社会云上创新。</p>",
    "publish_time": "2023-04-16 12:33:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]