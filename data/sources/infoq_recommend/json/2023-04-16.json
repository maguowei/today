[
  {
    "title": "Databricks来搅局了：0门槛克隆ChatGPT，完全开源可随意修改商用",
    "url": "https://www.infoq.cn/article/5IU3aWsb8Dyyv0evWg2b",
    "summary": "<p></p><blockquote>全球首个完全开源的大语言模型，性能堪比GPT3.5！</blockquote><p></p><p>&nbsp;</p><p>大数据热潮催生了许多成功的公司，例如 Snowflake、Databricks、Splunk 和 Cloudera。现在我们进入了生成式人工智能时代，那么会不会有新的“人工智能和大数据”结合方式？</p><p>&nbsp;</p><p>最近，<a href=\"https://www.infoq.cn/article/s2a1EQomEYyaoktLPvdT\">大数据公司Databricks</a>\"就在生成式人工智能领域采取了行动。两周前，该公司发布了一个名为 Dolly 的开源大型语言模型，旨在应对市场对于生成式AI及相关应用的旺盛需求，我们可以称之为 Dolly 1.0。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14fb978843e4d61a7d697967b69fd93b.png\" /></p><p></p><p>&nbsp;</p><p>像 ChatGPT 和 Bard 这样的生成式 AI，它们使用的数据通常来自于在成千上万不同网站，使用的数据量十分惊人，而且想要使用这些数据训练 AI 还需要数以千计的强大 GPU 在背后提供支持。Databricks 希望通过开源 Dolly 1.0及其训练数据，让任何人都能开发出一个真正像人类的 AI，而无需投资数百万美元，这让这类 AI 不再是只有大型科技公司才能负担得起的东西，数以百万计的小公司也将能够从中受益。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/0jlMtAfeJUy5T1R*GYGS\">Databricks</a>\" 首席执行官 Ali Ghodsi 表示，Dolly 1.0 只需要非常少的数据和非常短的时间就能完成训练，“只需 30 美元、一台服务器和三个小时，我们就能教 Dolly 开始进行人类级别的交互。”</p><p>&nbsp;</p><p>4月12日，Databricks再次发布了该大语言模型（LLM）的开源迭代版本，并命名为Dolly 2.0。Databricks 表示，Dolly 2.0 是业内第一个开源、遵循指令的 LLM，它在透明且免费提供的数据集上进行了微调，该数据集也是开源的，可用于商业目的。这意味着 Dolly 2.0 可用于构建商业应用程序，无需支付 API 访问费用或与第三方共享数据。</p><p>&nbsp;</p><p></p><h2>Dolly 2.0 的诞生</h2><p></p><p>&nbsp;</p><p>Dolly 1.0 基于 EleutherAI 在 2021 年开源的自然语言处理模型GPT-J。GPT-J 是一个基于 GPT-3，由 60 亿个参数组成的自然语言处理 AI 模型。但该模型使用了来自 StanfordAlpaca 项目的 5.2 万个问答数据集，是根据 OpenAI 的 ChatGPT 的输出进行训练的，因为OpenAI 的使用条款限制，Dolly 1.0 并不能用于商业用途。</p><p>&nbsp;</p><p>Databricks在官方博文中指出，“用于训练Dolly 1.0的数据集中，包含来自ChatGPT的输出。斯坦福团队明确提到，OpenAI的服务条款试图阻止任何人创建能够与其竞争的AI模型。”</p><p>&nbsp;</p><p>Dolly 2.0建立在Databricks公司首版Dolly的基础之上，为了规避这个问题并建立起可供商用的模型，Databricks使用基于EleutherAI的Pythia模型家族中的120亿参数语言模型，成功构建起了Dolly 2.0。</p><p>&nbsp;</p><p>该公司表示，他们专门在5000名Databricks员工之内开展了众包，通过高质量的人工生成指令建立起训练数据集，借此完成了模型训练和微调。该公司将这套高质量的人工生成响应/揭示数据集称为databricks-dolly-15k，其使用Creative Commons Attribution-ShareAlike 3.0&nbsp;Unported License许可。</p><p>&nbsp;</p><p>“任何人均可出于任何目的使用、修改或扩展这套数据集，包括商业应用程序。”Databricks还强调，该数据集可通过GitHub页面（<a href=\"https://github.com/databrickslabs/dolly/tree/master/data\">https://github.com/databrickslabs/dolly/tree/master/data</a>\"）直接下载。</p><p>&nbsp;</p><p>模型权重则可通过Databricks Hugging Face页面（<a href=\"https://huggingface.co/databricks\">https://huggingface.co/databricks</a>\"）处下载获取。</p><p>&nbsp;</p><p></p><h2>Dolly 2.0想成为大小公司的福音</h2><p></p><p>&nbsp;</p><p>Databricks之所以发布基于开源数据的大语言模型，主要是考虑到企业客户对控制模型并引入针对性场景/特定用例的需求。这也与行业常见的商业闭环训练模型（例如ChatGPT）形成了鲜明对比。</p><p>&nbsp;</p><p>市场调研公司Omdia首席分析师Bradley Shimmin表示，“Dolly 2.0这类模型大多是开放的，不需要在大规模GPU集群上进行长达数月的训练，因此为那些希望构建内部生成式AI方案的企业打开了新世界的大门。”</p><p>&nbsp;</p><p>Shimmin指出，“这些小型（即训练参数的规模较小）模型使用大量提示/响应对作为训练数据，因此特别适合希望控制整个解决方案、支持针对性用例的企业客户。例如，他们可以利用现有问答配对建立的帮助台数据库训练自己的AI模型。”</p><p>&nbsp;</p><p>根据咨询公司Amalgam Insights首席分析师Hyoun Park的说法，开源大语言模型的另一大优势，在于Dolly 2.0这类成果能够让企业更好地跟踪数据治理和驻留，并与所支持的用例保持良好的关联性。</p><p>&nbsp;</p><p>Park还专门拿OpenAI的名称打趣，说“因为OpenAI的ChatGPT等其他模型在使用时要依赖于API。对某些企业而言，这种依赖性可能引发关于API的合规性、治理或数据安全问题。”</p><p>&nbsp;</p><p>这也相当于，Dolly 2.0和其他基于开源的大语言模型将在受严格监管的行业中成为各企业的福音。这是个良好的开端，让企业意识到他们也可以创建并拥有自己的模型，且无需支付API访问费或与大语言模型提供商共享数据。这些在受到严格监管的行业中都可能产生巨大的问题。</p><p>&nbsp;</p><p></p><h3>开源与闭源大语言模型间的区别</h3><p></p><p>&nbsp;</p><p>与闭源大语言模型相比，基于开源的模型所使用的训练数据对公众开放，因此可根据业务进行微调和定制以满足企业需求。相比之下，ChatGPT等闭源模型则根据其开发者OpenAI所掌握的训练进行训练，模型可通过API付费访问，且禁止直接用于商业用途。</p><p>&nbsp;</p><p>Chandrasekaran认为，“「开放式大语言模型」可以有多种理解方式。最明显也最重要的一点，就是对这些模型的源代码和部署灵活性做出调整。除此之外，开放的范围还可以涵盖模型权重、训练数据集以及开放/协作方式层面的决策。”</p><p>&nbsp;</p><p>IDC的Schubmehl表示，Dolly 2.0就遵循基于开源的模型这一理念。“Dolly 2.0是一套大语言模型，模型本体、训练代码、数据集和模型权重都可作为开源资源从Databricks处获取，以供企业根据业务需求创建自己的定制化大语言模型。”Schubmehl同时提到，这种方法与其他大语言模型形成了鲜明对比，后者往往并不开放模型构建中的各类组成要素。</p><p>&nbsp;</p><p>分析人士还提到，闭源与开源大语言模型间的另一个区别，主要体现在训练的参数量上。其中闭源大语言模型的参数规模往往更大。以ChatGPT4为例，其训练中使用到100万亿个参数；相比之下，Dolly 2.0的参数量只有区区120亿个。</p><p>&nbsp;</p><p></p><h2>Dolly 2.0如何融入Databricks的生成式AI战略</h2><p></p><p>&nbsp;</p><p>Constellation Research 的&nbsp;Thurai&nbsp;表示，Databricks此次推出Dolly 2.0可以算是其夺取生成式AI市场份额的一项重要战略。</p><p>&nbsp;</p><p>“从本质上讲，众多大语言模型和基础模型业务都被掌握在超大规模企业手中。每家企业都有自己的变体——微软有ChatGPT、谷歌有Bard，AWS则通过Huggingface合作伙伴关系提供基础设施、流程、工具及模型共享和目录服务。Databricks当然不能坐以待毙，必须在热火朝天的大语言模型市场上分一杯羹。”</p><p>&nbsp;</p><p>其他分析师则认为，Dolly的发布符合Databricks公司向市场投放开源产品的战略。</p><p>&nbsp;</p><p>IDC的Schubmehl表示，“Databricks的专长，就是通过各种开源AI工具和服务帮助客户充分利用自己的数据和运营体系。Dolly是另一个绝佳安全，能够为组织提供基于最新AI技术的选项，也就是大语言模型。”但分析师们也承认，Databricks的Dolly 2.0恐怕无法立刻对ChatGPT或Bard等竞争对手产生影响。</p><p>&nbsp;</p><p>Omdia公司的Shimmin认为，“Dolly乃至其他开源生成式AI大语言模型的出现，将彻底颠覆Bard、ChatGPT和Galactica等现有大语言模型的未来前景。但从中短期来说，这些成果在Google Workplace、微软Office等产品中的地位还将稳固地维持下去。”</p><p>&nbsp;</p><p>Amalgam Insights的Park则给出了不同意见，认为Dolly最终会成为ChatGPT这类通用工具的功能伴侣。“人们会从通用工具中学习如何使用和提示生成式AI，而Dolly这类模型则负责帮助用户处理更具体、更专业的特定工作用例。”</p><p>&nbsp;</p><p>另外，也有评论指出，Dolly-like LLM 的一个能力是可以用来编写代码，特别是SQL 代码。这可能会导致非 SQL 专家能够在 Databricks lakehouse 上设置和运行查询。&nbsp;</p><p>&nbsp;</p><p>这可以从两方面来理解：第一，SQL 开发人员可以使用它来提高工作效率，第二，你不需要那么多 SQL 开发人员。Dolly 可以减少 Databricks 对 SQL 程序员的需求。将这种想法扩展到 Snowflake 和所有其他数据仓库环境，SQL 技能在未来可能会变得不那么有价值。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\">https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a>\"</p><p><a href=\"https://www.infoworld.com/article/3693349/why-did-databricks-open-source-its-llm-in-the-form-of-dolly-2-0.html\">https://www.infoworld.com/article/3693349/why-did-databricks-open-source-its-llm-in-the-form-of-dolly-2-0.html</a>\"</p>",
    "publish_time": "2023-04-16 12:20:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "集群算力提升3倍，腾讯云发布新一代高性能计算集群",
    "url": "https://www.infoq.cn/article/N8Wcppi2vFxkh3C0VdAw",
    "summary": "<p>4月14日，腾讯云正式发布面向大模型训练的新一代HCC（High-Performance Computing Cluster）高性能计算集群。该集群采用最新一代腾讯云星星海自研服务器，搭载了NVIDIA H800 Tensor Core GPU，并提供业界目前最高的3.2T超高互联带宽。</p><p>&nbsp;</p><p>实测结果显示，腾讯云新一代集群的算力性能较前代提升高达3倍。</p><p>&nbsp;</p><p>去年10月，腾讯完成首个万亿参数的AI大模型——混元NLP大模型训练。在同等数据集下，将训练时间由50天缩短到11天。如果基于新一代集群，训练时间将进一步缩短至4天。</p><p>&nbsp;</p><p>大模型进入万亿参数时代，对算力的需求陡增。在单体服务器计算能力有限的情况下，需要将上千台服务器相连，打造大规模、分布式的高性能计算集群。腾讯云新一代集群通过对单机算力、网络架构和存储性能进行协同优化，能够为大模型训练提供高性能、高带宽、低延迟的智算能力支撑。</p><p>&nbsp;</p><p>计算层面，服务器的单机性能是集群算力的基础。在非稀疏规格情况下，新一代集群单GPU卡支持输出最高&nbsp;495 TFlops（TF32）、989 TFlops （FP16/BF16）、1979 TFlops（FP8）的算力。针对大模型训练场景，腾讯云星星海服务器采用6U超高密度设计，相较行业可支持的上架密度提高30%；利用并行计算理念，通过CPU和GPU节点的一体化设计，将单点算力性能提升至最强。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4a4cd29400890015d02823e7bb8e966.png\" /></p><p></p><p>网络层面，计算节点间存在海量的数据交互需求，随着集群规模扩大，通信性能会直接影响训练效率。腾讯自研的星脉网络，为新一代集群带来了业界最高的3.2T的超高通信带宽。节点内外统一的AllReduce通信带宽，实现网络和算力的最大协同。实测结果显示，搭载同样的GPU，最新的3.2T星脉网络相较1.6T网络，能让集群整体算力提升20%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/06382e2beb46d6e1f612deda75989592.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/764e8989343128fe9bc599283a32d0bd.png\" /></p><p></p><p>&nbsp;</p><p>基于多轨道聚合的无阻塞网络架构、主动拥塞控制和定制加速通信库，腾讯云能提供业界领先的集群构建能力，支持单集群高达十万卡级别的组网规模。在超大集群场景下，仍然能保持优秀的通信开销比和吞吐性能，满足大模型训练以及推理业务的横向扩展。</p><p>&nbsp;</p><p>同时，腾讯自研高性能集合通信库TCCL，基于星脉网络硬件平台深度优化，在全局路径规划、拓扑感知亲和性调度、网络故障实时告警/自愈等方面融入了定制设计的解决方案。相对业界开源集合通信库，为大模型训练优化40%负载性能，消除多个网络原因导致训练中断问题。</p><p>&nbsp;</p><p>存储层面，训练场景下，几千台计算节点会同时读取一批数据集，需要尽可能缩短数据集的加载时长。新一代集群，引入了腾讯云最新自研存储架构，支持不同场景下对存储的需求。</p><p>&nbsp;</p><p>COS+GooseFS对象存储方案，提供多层缓存加速，大幅提升端到端的数据读取性能；将公开数据集、训练数据、模型结果统一存储到对象存储COS中，实现数据统一存储和高效流转。同时，GooseFS按需将热数据缓存到GPU内存和本地盘中，利用数据本地性提供高性能访问。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1f/1f33057811c49e92f7e220dc1a3c6283.png\" /></p><p></p><p>&nbsp;</p><p>CFS Turbo高性能并行文件存储方案，采取多级缓存加速，基于全分布式架构，提供100GB/s带宽、1000万IOPS的极致性能。并通过持久化客户端缓存技术，将裸金属服务器本地NVMe&nbsp;SSD和Turbo文件系统构成统一命名空间，实现微秒级延时，解決大模型场景大数据量、高带宽、低延时的诉求。同时，通过智能分层技术，自动对冷热数据分层，节省80%的存储成本，提供极致的性价比。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0ceb1e8ef204249025cc6301d69628c.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>底层架构之上，针对大模型训练场景，新一代集群集成了腾讯云自研的TACO Train训练加速引擎，对网络协议、通信策略、AI框架、模型编译进行大量系统级优化，大幅节约训练调优和算力成本。</p><p>&nbsp;</p><p>腾讯混元大模型背后的训练框架AngelPTM，也已通过腾讯云对外提供服务，帮助企业加速大模型落地。目前，腾讯混元AI大模型已经覆盖了自然语言处理、计算机视觉、多模态等基础模型和众多行业、领域模型。</p><p>&nbsp;</p><p>在腾讯云上，企业基于TI 平台的大模型能力和工具箱，可结合产业场景数据进行精调训练，提升生产效率，快速创建和部署 AI 应用。</p><p>&nbsp;</p><p>此前，腾讯多款自研芯片已经量产。其中，用于AI推理的紫霄芯片、用于视频转码的沧海芯片已在腾讯内部交付使用，性能指标和综合性价比显著优于业界。其中，紫霄采用自研存算架构，增加片上内存容量并使用更先进的内存技术，消除访存能力不足制约芯片性能的问题，同时内置集成腾讯自研加速模块，减少与CPU握手等待时间。目前，紫霄已经在腾讯头部业务规模部署，提供高达3倍的计算加速性能，和超过45%的整体成本节省。</p><p>&nbsp;</p><p>目前，腾讯云的分布式云原生调度总规模超过1.5亿核，并提供16 EFLOPS（每秒1600亿亿次浮点运算）的智算算力。未来，新一代集群不仅能服务于大模型训练，还将在自动驾驶、科学计算、自然语言处理等场景中充分应用。</p><p>&nbsp;</p><p>以新一代集群为标志，基于自研芯片、星星海自研服务器和分布式云操作系统遨驰，腾讯云正通过软硬一体的方式，打造面向AIGC的高性能智算网络，持续加速全社会云上创新。</p>",
    "publish_time": "2023-04-16 12:33:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "BigDL 教程：使用 Stable Diffusion 从文本生成你自己的图像",
    "url": "https://www.infoq.cn/article/sbYgeIQPwWH6q7DXvLsj",
    "summary": "<p>本文最初发表于 Medium 博客，经原作者授权，InfoQ 翻译并分享。</p><p>&nbsp;</p><p>导读：本文介绍了如何使用 Stable Diffusion 模型从文本生成图像。作者详细介绍了该模型的架构概述以及学习过程，并指出该模型需要大量计算，但可以使用英特尔提供的优化技术缩短运行时间。同时，文章还提供了文本到图像的实现步骤，并鼓励读者在 GitHub 上跟随实现。</p><p>&nbsp;</p><p>如果说每种技术都有其季节，那么人工智能已经迎来了“夏天”。人工智能的一系列进步引领了该学科目前的繁荣，并带来了对未来的巨大期望。</p><p>&nbsp;</p><p>计算机视觉就是一个典型例子。尽管对计算资源的需求很高，但在图像生成（Huang 等，2018）领域已经取得了巨大的进展。图像生成始于<a href=\"https://en.wikipedia.org/wiki/Generative_adversarial_network\">生成式对抗网络</a>\"（Generative Adversarial Network，GAN）范式，然后逐步发展到如今的扩散模型。这种进化为数据科学家提供了易于训练、快速收敛并能可靠生成高质量图像的模型。</p><p>&nbsp;</p><p>这种技术在<a href=\"https://generativeai.net/\">人工智能</a>\"内容生成（generative AI，AIGC）中发挥着重要作用，它能够生成各种数据，包括音频、代码、图像、文本、模拟、3D 对象、视频等等。它通过训练算法来基于以前的训练数据生成新的信息。AIGC 有许多用途包括文本生成（如 <a href=\"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\">GPT</a>\"，Bidirectional Encoder Representations from Transformer（<a href=\"https://en.wikipedia.org/wiki/BERT_(language_model)\">BERT</a>\"）或最近的 <a href=\"https://openai.com/blog/chatgpt\">ChatGPT</a>\"）、<a href=\"https://soundcloud.com/openai_audio/jukebox-86115728\">音频生成</a>\"、文本到图像的创建（<a href=\"https://openai.com/product/dall-e-2\">DALL-E</a>\" 或 <a href=\"https://stablediffusionweb.com/\">Stable Diffusion</a>\"）等。</p><p></p><p>在本文中，我们将展示如何借助 <a href=\"https://www.intel.com/content/www/us/en/developer/tools/bigdl/overview.html\">BigDL</a>\"（ <a href=\"https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html\">BigDL Nano</a>\" 中的优化）在 Intel 笔记本电脑上运行优化后的 Stable Diffusion模型，从而实现文本到图像的生成。</p><p></p><p></p><h1>使用Stable Diffusion 的两种方法</h1><p></p><p>&nbsp;</p><p>使用 Stable Diffusion 生成图像有两种方式：无条件和有条件。</p><p>&nbsp;</p><p>无条件图像生成：可以从噪声种生成新的图像而不需要任何条件（例如提示文本或其他图像）。模型在训练之后可以生成新的随机图片。相关详细信息，请查看此使用蝴蝶图像训练模型的示例。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2a/2af4b21f83d75f73600a574d1f523f53.jpeg\" /></p><p></p><p>训练集</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f6/f6d9d32eb36f75279192d118bd8cf17a.png\" /></p><p></p><p>生成的图像</p><p></p><p>有条件图像生成：该模型可以根据输入条件生成新的图像，而有条件图像生成的具体应用包括文本到图像、图像到图像、语义、修补和补全等。让我们来详细看一下：</p><p>&nbsp;</p><p>文本到图像（txt2img）：基于输入文本生成图像。输入：文本-&gt; 输出：图像</p><p>&nbsp;</p><p>以下是一个输入文本的示例：一只戴眼镜的狗。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/7381b5a0c31d660a4aee2e57497f10f1.png\" /></p><p></p><p></p><p></p><p>图像到图像：该模型基于低分辨率图像生成高分辨率图像。下面展示了一种<a href=\"https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale\">上采样扩散模型</a>\"的实现。输入：图像-&gt;输出：图像。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/96d03c75ba0dc8d3b62578c5d42a9877.jpeg\" /></p><p></p><p>摘自《<a href=\"https://arxiv.org/pdf/2112.10752.pdf\">基于潜在扩散模型的高分辨率图像合成</a>\"》（High-Resolution Image Synthesis with Latent Diffusion Models）</p><p>&nbsp;</p><p>语义增强（img2img）：该模型允许你基于输入的图像和文本生成新的图像。你可以尝试使用这个<a href=\"https://platform.stability.ai/docs/features/image-to-image?tab=\">图像到图像教程</a>\"。在下面的示例中，我们要求它生成一个美丽的海滩。然而，由于我们对理想的度假场景不满意，我们要求添加一个高尔夫球场。该模型以生成的海滩作为输入，并在其上添加了一个高尔夫球场。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7d481aedbe990007581417e2874b27f.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da08ae2803c474a4a5ff5cab61f1d728.jpeg\" /></p><p>海滩（上图）带高尔夫球场的海滩（下图）</p><p>&nbsp;</p><p>图像修补：该模型可以使用新内容填充图像中被遮蔽的区域，无论是因为图像的某些部分已经损坏，还是为了替换图像中现有但不需要的内容。使用这个<a href=\"https://huggingface.co/spaces/multimodalart/stable-diffusion-inpainting\">多修补模型</a>\"，下面的挂钟被替换成了蝙蝠侠面具。（对于替换图像，你可以使用你的想象力添加任何图像。）</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/38cf6ae86a6f680b63def35329d2472d.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0c/0c341bca03fd5f0baba470681f65232b.jpeg\" /></p><p>家庭办公室（来自 UnSplash 的原始照片）；带有生成的斗篷十字军面具的家庭办公室</p><p>&nbsp;</p><p>扩展绘制：这里的绘制发生在原始图像的外部区域。该模型会人工“填充”图像到所需的大小。在下面的示例中，我们要求模型生成一个名为“街上的汽车”的图像，并使用扩展绘制来填充图像的左下部分。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e7f9529b774e8053f6b583422844fed3.jpeg\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c9/c9e7b53d8faa0d821a48dfed52585c4c.jpeg\" /></p><p></p><h2>Stable Diffusion 的工作原理：概述</h2><p></p><p>&nbsp;</p><p><a href=\"https://en.wikipedia.org/wiki/Stable_Diffusion\">Stable Diffusion</a>\" 是用于高分辨率图像生成的模型。为了理解扩散模型的工作原理，而不深入复杂的数学原理，我们将一个 txt2img 稳定扩散模型分解为三个主要部分：</p><p>&nbsp;</p><p>文本编码器：是一个基于 <a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model\">Transformer</a>\" 的 <a href=\"https://openai.com/research/clip\">ClipText</a>\" 模型，采取和 GPT一样的架构。Transformer已经证明了对语言的良好理解，因此它们可以基于你文本提示的意图轻松地识别和转换。图像信息生成器（基于文本信息的 <a href=\"https://arxiv.org/abs/1505.04597v1\">UNET</a>\"）：这是扩散发生的地方。在这一部分中使用了 U-Net（<a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\">Resnet-CNN 架构</a>\"）网络，在推理之前该网络已经进行了训练。扩散理论可以分解为前向扩散和反向扩散两个主要过程。工作原理是首先通过逐渐添加<a href=\"https://en.wikipedia.org/wiki/Gaussian_noise\">高斯噪声</a>\"来破坏训练数据，然后通过学习反转噪声来恢复数据。</p><p>预处理阶段：正向扩散过程，通过不断添加高斯噪声来破坏训练数据，以生成训练样本，对应下图从右到左的过程。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/49faeca4688fd3b015bdcb36c829f3f9.jpeg\" /></p><p>图片来源（Ho 等人，2020 年）</p><p>&nbsp;</p><p>训练/推理阶段：反向扩散过程，模型学习从噪声中恢复数据，对应上图从左到右的过程。</p><p>&nbsp;</p><p>图像解码器（VAE 解码器）：接收图像信息生成器生成的向量，将其转换为图像，并将最终图像转换为所需的格式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/ca8cfd462c47bd667b74cd2da973cec4.jpeg\" /></p><p>架构概述。图片来源：Ezequiel Lanza</p><p>&nbsp;</p><p></p><h2>BigDL</h2><p></p><p>&nbsp;</p><p>你可以使用上述的架构来获得图像，但是你是否注意到这个过程需要很长时间，有时需要几分钟？</p><p>&nbsp;</p><p>这是因为我们使用的模型很大，但可以通过优化来减少处理时间。具体哪些部分可以进行优化，这里不再详细介绍。优化已经被集成到了 <a href=\"https://www.intel.com/content/www/us/en/developer/tools/bigdl/overview.html\">BigDL</a>\" 中，其考虑了多种优化，例如 <a href=\"https://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html\">Intel® Optimization for TensorFlow</a>\"，<a href=\"https://github.com/intel/intel-extension-for-pytorch\">Intel® Extension for PyTorch</a>\"，<a href=\"https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html\">Intel® Distribution of OpenVINO</a>\"，<a href=\"https://www.intel.ca/content/www/ca/en/architecture-and-technology/avx-512-overview.html\">Intel® AVX-512</a>\" 等等。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2a/2ab7a69c85b849f0c505d4b2d7e13bfa.jpeg\" /></p><p>BigDL 架构。图：Ruonan Wang</p><p>&nbsp;</p><p></p><h1>使用 BigDL 生成图像</h1><p></p><p>&nbsp;</p><p>现在轮到你了。我们将在这里为你提供步骤，或者你可以在 <a href=\"https://github.com/intel-analytics/BigDL/tree/main/python/nano/client-application/StableDiffusion\">GitHub</a>\" 上跟随实现。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/2420b0de9ec3687c578d25c0e975be85.jpeg\" /></p><p></p><h2>安装</h2><p></p><p>&nbsp;</p><p>我们建议使用至少 16GB RAM 和 15GB 可用磁盘空间的 Intel 笔记本电脑/台式机。</p><p>&nbsp;</p><p>为了访问我们提供的实现，建议使用新的虚拟环境运行，并安装必要的先决条件。</p><p></p><p><code lang=\"null\">conda create -n sd python=3.8 \nconda activate sd \npip install -r requirements.txt </code></p><p></p><h2>启动 Web UI</h2><p></p><p>打开你下载文件的文件夹，然后运行启动脚本：</p><p><code lang=\"null\">python launch.py </code></p><p>&nbsp;</p><p>随后应用程序将在你的设备上运行，并且你可以在浏览器中输入此地址：http://127.0.0.1:7860/</p><p></p><p></p><h2>优化模型</h2><p></p><p>&nbsp;</p><p>在生成图像之前，你需要获取优化后的模型。 请转到“优化模型”选项卡执行操作。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/0435bde660fee3a66b8c2b2a37d3cdf6.jpeg\" /></p><p></p><p>&nbsp;</p><p>现在你可以选择以下两个选项：</p><p>&nbsp;</p><p>CPU-FP32 将为 CPU 生成优化后的 fp32 模型，后面会出现“CPU FP32”选项（例如“v2.1-base CPU FP32”）。CPU / iGPU FP16 将为 CPU 和 iGPU 生成优化后的 fp16 模型，后面将出现两个“FP16”选项（例如“v2.1-base CPU FP16”，“v2.1-base CPU+iGPU FP16”）。</p><p>&nbsp;</p><p>注意：这一步可能需要一些时间，因为应用程序会实时下载原始模型并为你进行优化。</p><p>&nbsp;</p><p>模型优化完成后，你可以键入任何文本以生成原始图像。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e716934a905db4bcf4ac7980bd7f0bda.jpeg\" /></p><p></p><p>&nbsp;</p><p>注意：由于我们正在使用 Hugging Face 模型，你需要按照上图所示添加<a href=\"https://huggingface.co/docs/hub/security-tokens\">访问令牌</a>\"。</p><p>&nbsp;</p><p>现在你的模型已准备就绪，你可以从“txt2img”选项卡开始生成图像。此外，应用程序还提供了其他选项。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a718144bce26bb13b177f1b9774c4d41.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p></p><p>Stable Diffusion 是一种功能强大的工具，具有革命性的潜力，可应用于许多现实世界的场景。本文介绍的模型及其学习过程需要大量计算，Intel 提供的优化技术可以缩短处理时间。如果想获取更多来自 Intel 的开源内容，请访问 <a href=\"https://www.intel.com/content/www/us/en/developer/topic-technology/open/overview.html\">open.intel</a>\" 或关注我们的 <a href=\"https://twitter.com/OpenAtIntel?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\">Twitter</a>\"。</p><p>&nbsp;</p><p>作者简介：</p><p>&nbsp;</p><p>Ezequiel Lanza 是英特尔开放生态系统团队的开源倡导者，热衷于帮助人们发现令人兴奋的人工智能世界。他还是经常参加人工智能会议的演讲者，创建使用案例、教程和指南，帮助开发人员采用像 TensorFlow 和 Hugging Face这 样的开源人工智能工具。</p><p>&nbsp;</p><p>Ruonan Wang 是英特尔 AIA 的人工智能框架工程师，目前专注于开发 BigDL-Nano，这是一个 Python 包，可以在英特尔硬件上透明加速 PyTorch 和 TensorFlow 应用程序。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://medium.com/intel-tech/bigdl-tutorial-generate-your-own-images-from-text-with-stable-diffusion-63f45634ab2c</p>",
    "publish_time": "2023-04-16 13:52:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "马斯克爆料Twitter裁了八成员工；OpenAI推出漏洞赏金计划，奖励最高2万美元；小鹏被曝年终奖打0.5折 | AI一周资讯",
    "url": "https://www.infoq.cn/article/BK8WVtwb5SNh5zdXNVLs",
    "summary": "<p></p><blockquote>微软宣布开源 Deep Speed Chat；消息称软银旗下Arm启动赴美IPO；国家网信办出台生成式AI管理办法；前理想AI芯片一号位骄旸加入三星，负责组建GPU团队……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>OpenAI CEO：GPT-5根本不存在</h4><p></p><p></p><p>4月13日，OpenAI CEO Altman在接受视频采访时表示：我们现在并没有训练<a href=\"https://www.infoq.cn/article/XBsThbej4O7u2EbTqlzS\">GPT-5</a>\"，目前只是在GPT-4的基础上进行更多的工作而已。此前有传闻称，GPT-5正在秘密训练中，预计年底上线。</p><p></p><h4>法国、西班牙对ChatGPT展开调查</h4><p></p><p></p><p>当地时间4月13日，法国国家信息自由委员会（CNIL）决定对ChatGPT提出5项指控，并展开调查。此外，欧洲数据保护委员会宣布成立专门工作组，以促进该调查在欧洲地区的合作事宜。欧洲数据保护委员会称，此次行动是在意大利数据保护局开始对ChatGPT立案调查之后做出的，并表示希望欧洲各国之间就针对ChatGPT采取的行动进行充分沟通。</p><p></p><p>同日，西班牙国家数据保护局发表声明，称该机构已经正式对ChatGPT可能的违反法律行为展开初步调查程序。</p><p></p><h4>美国司法部：如果不是谷歌垄断，ChatGPT等创新可以来得更早</h4><p></p><p></p><p>当地时间周四，美国司法部对美国一家联邦法院表示，如果不是谷歌垄断搜索市场，ChatGPT和其他技术创新本可能在几年前就已经出现。</p><p></p><p>在微软宣布将把OpenAI的人工智能聊天技术纳入必应搜索引擎的几天后，谷歌也宣布将发布自己的会话式人工智能产品。</p><p></p><p>美国司法部谷歌反垄断案的首席律师Kenneth Dintzer对法官Amit Mehta表示，这表明“真正的竞争将会带来什么”。律师表示：“过去12年来，谷歌一直在维持垄断地位。我们会在6年前看到ChatGPT吗？我们会看到其他5家竞争对手在搜索领域展开竞争吗？这些都是我们无法回答的问题。”</p><p></p><h4>马斯克爆料Twitter裁了八成员工</h4><p></p><p></p><p>4月12日，马斯克接受BBC采访时被问及自收购<a href=\"https://www.infoq.cn/article/646d32tsTQk61TlvyTQk\">Twitter</a>\"以来共解雇了多少人，他回答道，自己收购该公司时，员工数量“略低于8000人”，现在有“1500人”。这也意味着，Twitter八成以上的员工都被裁了。</p><p></p><p>当被问及解雇如此多员工是否感到艰难时，马斯克说表示这“一点也不有趣”，有时会“痛苦”。他还表示，自己不会亲自去解雇每个人，“不可能与那么多人面对面交谈”。</p><p></p><p>此外，据彭博报道，Twitter公司与新成立壳公司X Corp.合并之后，已经不再是一家独立公司。为一宗案件向加州法院提交的日期为4月4日的文件称，在和X Corp.合并后，Twitter“不复存在”。马斯克此前曾表示，收购Twitter将是创建X的“促进剂”，他称之为“万能应用”(everything app)。当地时间周二，他在Twitter上为此发帖，只用了一个字符“X”。</p><p></p><h4>亚马逊推出大语言模型Titan</h4><p></p><p></p><p>4月13日晚，亚马逊在其官方平台宣布，推出生成式人工智能服务Bedrock以及自有的大语言模型Titan。据悉，Bedrock是一项用于打造和扩展生成式人工智能应用的新服务，这些应用可以根据提示生成文本、图像、音频和合成数据。</p><p></p><p>Titan则包括亚马逊机器学习开发的两个新的基础模型，一种可以为博客帖子、电子邮件或其它文档生成文本，另一种是嵌入式大语言模型，可以将文本输入进行转换，适合搜索等用途。</p><p></p><h4>OpenAI推出漏洞赏金计划，奖励最高2万美元</h4><p></p><p></p><p>4月11日，OpenAI发布了一项“漏洞悬赏计划（Bug Bounty Program ）”，以鼓励外界帮其寻找人工智能系统中的漏洞和错误，奖金最高为2万美元。</p><p></p><p>OpenAI称，其使命是创建造福所有人类的AI系统，该公司已经大力投资研发，以确保AI系统安全可靠，“然而，与其他复杂技术一样，漏洞和缺陷可能会出现。”</p><p></p><p>“透明度和协作（transparency and collaboration）对解决这一问题至关重要。”OpenAI写道，“这也是为什么我们想要邀请全球的安全研究人员、道德黑客和技术爱好者帮助我们识别和解决系统中的漏洞。我们很乐意为报告符合条件的漏洞的人提供奖励。”</p><p></p><p>OpenAI称已经与漏洞赏金平台Bugcrowd进行了合作，让其帮忙管理提交和奖励流程。该公司表示，将根据对方报告问题的严重性提供现金奖励，金额为200美元到2万美元不等。</p><p></p><h4>小米任命栾剑担任大模型团队负责人</h4><p></p><p></p><p>4月14日晚间消息，小米近日发布内部邮件，宣布任命栾剑担任技术委员会AI实验室大模型团队负责人，向技术委员会AI实验室主任王斌汇报。</p><p></p><p>另外，在此次内部邮件中，还涉及到了国际业务部的人事任命。任命人力资源部总经理王栋兼任国际业务部HR负责人；任命林恩担任国际业务部政委，向集团总干部部部长刘德和国际业务部总裁卢伟冰双线汇报；任命刘宇翔担任国际业务部参谋长，并继续兼任小米印度政委，向国际业务部总裁卢伟冰汇报。</p><p></p><h4>张勇：阿里巴巴所有产品未来将接入大模型全面改造</h4><p></p><p></p><p>4 月 11 日，阿里巴巴集团董事会主席兼 CEO、阿里云智能集团 CEO 张勇在云峰会上表示，阿里巴巴所有产品未来将接入<a href=\"https://www.infoq.cn/article/MBhLRG3KXdBw3QSJL2gR\">“通义千问”</a>\"大模型，进行全面改造。张勇表示，面向 AI 时代，所有产品都值得用大模型重做一次，基于这一信念，阿里云希望帮助更多企业用上大模型，让每家企业都能基于“通义千问”，拥有具备自己行业能力的专属大模型。</p><p></p><p>张勇表示，AI 大模型的出现是一个划时代的里程碑，人类将进入到一个全新的智能化时代，就像工业革命一样，大模型将会被各行各业广泛应用，带来生产力的巨大提升，并深刻改变我们的生活方式。</p><p></p><h4>国家网信办出台生成式AI管理办法</h4><p></p><p></p><p>4月11日，国家互联网信息办公室（下称“网信办”）就《生成式人工智能服务管理办法（征求意见稿）》（下称《办法》），面向社会公开征求意见。</p><p></p><p>《办法》重点强调了生成式人工智能产品训练数据及生成内容的真实性。其中，就提供生成式人工智能产品或服务作出五大要求，提出“利用生成式人工智能生成的内容应当真实准确，采取措施防止生成虚假信息”。同时，也对产品的预训练数据、优化训练数据等作出五项要求，提出“能够保证数据的真实性、准确性、客观性、多样性”。</p><p></p><p>《办法》明确：提供者①不得非法留存能够推断出用户身份的输入信息，不得向他人提供用户输入信息；②不得利用AI生成内容损害他人形象、名誉及其他合法权益；③建立用户投诉接收处理机制，及时处置个人关于更正、删除、屏蔽其个人信息的请求。</p><p></p><p>《办法》意见反馈截止时间为5月10日。</p><p></p><h4>王小川回怼百度副总裁：“你脱离一线更久！”</h4><p></p><p></p><p>日前，36氪专访王小川时，谈及李彦宏称“百度的文心一言和OpenAI差距可能在两个月左右”，王小川回应表示“你们采访的可能是平行世界的他（李彦宏），不是我们这个世界里的。”</p><p></p><p>对于王小川的评论，百度集团副总裁、搜索平台负责人肖阳对此发表回应称：“‘天上一天，人间一年’，王小川脱离一线太久，确实跟我们不在一个宇宙，自然对国内人工智能技术的发展缺乏了解。我们当然非常希望中国如果能够跑出一家像OpenAI一样的公司，但怎么说呢，当年搜狗也立志取代百度搜索，结果也是显而易见的。所以很难评价，那我祝他成功吧！”</p><p></p><p>4月14日凌晨，王小川再次公开回怼，他称：“什么样的平行宇宙里，才能做到让一个脱离一线15年的人，去怼一个脱离一线1.5年的人。”</p><p></p><h4>消息称软银旗下Arm启动赴美IPO</h4><p></p><p></p><p>据英国《金融时报》在当地时间4月11日的消息，软银集团创始人兼CEO孙正义在当日将与美国纽交所就Arm的上市计划达成了初步协议，Arm的IPO最早将在今年秋季发生，并计划于美国纳斯达克上市。</p><p></p><p>据两名知情人士透露，孙正义将于本周正式签署该协议。此举意味着Arm公司IPO流程已正式迈出第一步，软银集团后续将为Arm提交相关上市文件。</p><p></p><h4>前理想AI芯片一号位骄旸加入三星，负责组建GPU团队</h4><p></p><p></p><p>据36氪报道，前理想汽车AI芯片一号位骄旸已离职，目前已加入三星，成为其GPU团队的核心成员，负责项目规划、团队创建。在理想汽车之时，骄旸向理想汽车系统研发部负责人谢炎汇报。一位行业人士称，骄旸离开或许是因为“AI芯片性价比低，不是理想汽车造芯的优先选择”。</p><p></p><p>在加入理想汽车之前，骄旸曾是阿里达摩院芯片技术部负责人，重点研究方向是人工智能芯片，主导了“含光800”芯片的推出，这是一颗人工智能推理芯片，基于12nm工艺。更早期的时候，骄旸还曾在华为参与创建美国和上海团队，推进GPU方向开发。</p><p></p><h4>商汤科技推出“日日新SenseNova”大模型体系</h4><p></p><p></p><p>4月10日，商汤科技分享了以“大模型+大算力”推进AGI（通用人工智能）发展的战略布局，并公布了商汤在该战略下的“日日新SenseNova”大模型体系，推出自然语言处理、内容生成、自动化数据标注、自定义模型训练等多种大模型及能力。</p><p></p><p>商汤不仅展示了“日日新SenseNova”大模型体系下的语言大模型，还展示了AI文生图创作、2D/3D数字人生成、大场景/小物体生成等一系列生成式AI模型及应用，还揭开了依托商汤AI大装置SenseCore实现“大模型+大算力”融合创新的研发体系。</p><p></p><p>其中，商汤所演示的精选模型中出现AI模型站civitai的图片，引发了网络质疑。对此，商汤方面表示：“秒画SenseMirage包含商汤自研AIGC大模型，也提供第三方社区开源模型，支持导入多个平台的开源模型或上传用户本地模型。用户可免除本地化部署流程，并基于开源模型自训练模型，高效地生成更多样的内容。”据了解，在现场演示中，该模型底部有显示civitai的文字注释。</p><p></p><h2>AI 开源工具</h2><p></p><p></p><h4>微软宣布开源 Deep Speed Chat</h4><p></p><p></p><p>4月12日，微软宣布开源Deep Speed Chat，帮助用户训练类ChatGPT等大语言模型。据了解，Deep Speed Chat基于微软Deep Speed深度学习优化库开发，具备训练、强化推理等功能，使用RLHF（人工反馈机制的强化学习）技术，可将训练速度提升15倍以上，并大幅降低成本。</p><p></p><p>Deep Speed Chat的开源，将显著降低用户获得大模型的成本，加速大模型在千行百业的应用落地。</p><p></p><p>项目地址：https://github.com/microsoft/DeepSpeed</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>小鹏年终奖被曝打0.5折</h4><p></p><p></p><p>据在社交平台上认证为“前小鹏汽车”的网友爆料。小鹏汽车年终奖出炉，为0.5折，而其他两家造车新势力蔚来的年终奖为3.3折，理想的年终奖为5折。也有认证为“小鹏汽车”的网友表示，自己的年终奖发了500元。另有小鹏汽车员工补充表示，绩效评定为b不到2000元。</p><p></p><p>网友热评：感恩小鹏，明明可以赖账，却还是给了点银子打发叫花子。</p><p></p><h4>小伙年会喜提365天带薪休假</h4><p></p><p></p><p>近日，广东深圳一男子在公司年会上抽到“365天带薪假期”。据了解，因种种原因该公司已经3年没有开年会了，为缓解工作压力，今年年会特别设置了抽奖环节。公司行政部陈女士回应称，“跟老板商量了一下，把惊喜搞大一点。”当时设置的最大奖项就是“全年带薪休假”，其他都是小家电、现金红包等。陈女士透露，中奖男子“不是小职员还算是个管理层”。对于奖项能否照常兑现，她表示，老板是认可这个奖项的，后期会跟员工再商议是休假还是折现。</p><p></p><p>4月12日下午，中奖员工在网络平台发文回应此事称，选择休假还是折现还需要和家人再商量一下，目前还没有考虑好。“休假肯定是想的，但还得看实际的工作安排”。</p><p></p><p>网友热评：一半折现一半休假，休息工作两不误，还有钱花，想想就美。</p><p></p><h4>人均奖励400万元，东方甄选发股权“大红包”</h4><p></p><p></p><p>4月11日晚间，东方甄选公告向154名合格参与者（承授人）授出股份奖励3045.9万股。按照东方甄选当天收盘价29港元计算，本次股份奖励总价值约8.83亿港元，约7.75亿元人民币。</p><p></p><p>公告显示，新东方创始人俞敏洪以及东方甄选高管孙东旭、尹强均在此次股份奖励名单中。孙东旭获得300万股，市值高达8700万港元；俞敏洪获得150万股，尹强获得60万股，市值分别约4350万港元、1740万港元。除开3名高管，平均算下来，其余151人参与的员工，每位将获得市值为487万港元（约合427万元）的股票。</p><p></p><p>网友热评：壕气！老俞格局很大。</p>",
    "publish_time": "2023-04-16 14:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Uber实践：运维大型分布式系统的一些心得",
    "url": "https://www.infoq.cn/article/dYI4IwMwXxhf4tN0csFr",
    "summary": "<p></p><blockquote>本文是 Uber 的工程师 Gergely Orosz 的文章，原文地址在：https://blog.pragmaticengineer.com/operating-a-high-scale-distributed-system/</blockquote><p></p><p></p><p>在过去的几年里，我一直在构建和运营一个大型分布式系统：优步的支付系统。在此期间，我学到了很多关于分布式架构概念的知识，并亲眼目睹了高负载和高可用性系统运行的挑战（一个系统远远不是开发完了就完了，线上运行的挑战实际更大）。构建系统本身是一项有趣的工作。规划系统如何处理10x / 100x流量的增加，确保数据持久，面对硬件故障处理等等，这些都需要智慧。不管怎样，运维大型分布式系统对我来说是一次令人大开眼界的体验。</p><p></p><p>系统越大，墨菲的“什么可能出错，就会出错”的定律就越会体现。频繁部署、部署代码的开发人员数量很多，涉及多个数据中心、系统被大量全球用户使用，这种出错概率越大。在过去的几年里，我经历过各种各样的系统故障，其中很多让我感到惊讶。有些来自可预测的事情，比如硬件故障或一些看起来无害的Bug，还有数据中心线缆被挖断、同时发生多个级联故障。我经历了数十次业务停摆，系统的某些部分无法正常工作，从而导致巨大的业务影响。</p><p></p><p>这篇文章是我在Uber工作时总结的，可以有效运维大型系统的实践的集合。我的经验并不是独一无二的 - 在类似规模的系统上工作的人也经历了类似的旅程。我与Google，Facebook和Netflix的工程师进行了交谈，他们分享了类似的经验和解决方案。这里列出的许多想法和流程应该适用于类似规模的系统，无论是在自己的数据中心（如Uber大多数情况下）上运行，还是在云上运行（Uber 有时会把部分服务弹性部署到云上）。但是，对于规模较小或较少关键任务的系统而言，这些做法可能过于苛刻。</p><p></p><p>涉及的内容很多——我将讨论以下主题：</p><p>监控值班，异常检测和警报故障和事件管理流程事后分析，事件回顾和持续改进文化故障演习，容量规划和黑盒测试SLOs、SLAs 及其报告SRE 作为独立团队可靠性作为持续投资更多推荐阅读</p><p></p><h2>监控</h2><p></p><p>要知道系统是否健康，我们需要回答“我的系统是否正常工作”的问题？为此，收集系统关键部分的数据至关重要。对于在多台计算机和数据中心上运行多个服务的分布式系统，可能很难确定要监控的关键内容是什么。</p><p></p><p>基础设施健康监测&nbsp;如果一个或多个计算机/虚拟机过载，则分布式系统的某些部分可能会降级。机器的健康状况，CPU利用率、内存使用情况，是值得监控的基础内容。有些平台可以开箱即用地处理这种监控和自动扩展实例。在优步，我们拥有一支优秀的核心基础设施团队，提供开箱即用的基础设施监控和警报。不管技术层面如何实现，实例或基础设施出问题的时候，监控平台需要提供必要的信息。</p><p></p><p>服务运行状况监控：流量，错误，延迟。我们经常需要回答“这个后端服务是否健康？”这样的问题。观察访问端点的请求流量、错误率和端点延迟等事项都可以提供有关服务健康状况的有价值信息。我更喜欢将这些都显示在仪表板上。在构建新服务时，通过使用正确的HTTP响应映射并监视相关代码可以对系统有很多了解。因此，确保客户端错误能返回4XX，以及如果服务器错误则返回5xx，这种监控易于构建且易于解释。</p><p></p><p>监测延迟值得再考虑一下。对于生产服务，目标是让大多数最终用户获得良好的体验。事实证明，测量平均延迟并不是一个很好的指标，因为这个平均值可以隐藏一小部分高延迟请求。测量p95，p99或p999 - 第95百分位，第99百分位或第99.9百分位的请求所经历的延迟 - 是一个更好的指标。这些数字有助于回答诸如“99％的人的请求有多快？”之类的问题（p99）。或“1000人中，至少有一人经历了多慢的延迟？” （p999）。对于那些对这个主题更感兴趣的人，这篇<a href=\"https://igor.io/latency/\">延迟入门文章</a>\"可以进一步阅读。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/73c97418ca3d08ea281a7b6d21ded019.png\" /></p><p></p><p>从图上可以明显看出，平均延迟、p95、p99 差异还是比较大的。所以平均延迟有可能掩盖一些问题。</p><p>围绕监控和可观察性有很多更有深度的内容。值得一读的两个资源是Google的<a href=\"https://landing.google.com/sre/sre-book/\">SRE书</a>\"和关于分布式系统监控的<a href=\"https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/%3Ca%20href='/query/searchAction.shtml?query=xref%27%20target=_blank%3E%3Cb%3Exref%3C/b%3E%3C/a%3E_monitoring_golden-signals\">四个黄金指标</a>\"的部分。他们建议，如果您只能测量面向用户的系统的四个指标，请关注流量，错误，延迟和饱和度。比较简短的材料的话，推荐来自<a href=\"https://twitter.com/copyconstruct\">Cindy Sridharan</a>\"的<a href=\"https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/\">分布式系统可观察性</a>\"电子书，它涉及其他有用的工具，如事件日志，指标和跟踪最佳实践。</p><p></p><p>业务指标监控。监控服务模块，可以告诉我们服务模块运行的如何如何正常，但无法告知我们业务是否按预期工作，是否“照常营业”。在支付系统，一个关键问题是，“人们可以使用特定的支付方式进行支付业务吗？”。识别业务事件并对其监控，是最重要的监控步骤之一。</p><p></p><p>虽然我们建立了各种监控，有些业务问题仍然无法探测到，这让我们遭受了巨大的痛苦，最终建立了业务指标的监控。有时我们所有的服务看起来都在正常运行，但关键产品功能不可用！这种监控对我们的组织和领域来说非常有用。因此，我们必须在Uber的<a href=\"https://eng.uber.com/observability-at-scale/\">可观察性技术堆栈</a>\"的基础上，为自己定制这种类型的监控做了大量的思考和努力。</p><p></p><p></p><blockquote>译者注：业务指标监控这一点，我们实在是太深有同感了，之前在滴滴有时就是发现所有服务都正常，但是业务不好使。我们现在创业做的北极星系统，就是专门应对这个问题的。感兴趣的朋友可以在公众号后台给我留言，或加我好友 picobyte 交流试用。</blockquote><p></p><p></p><h2>Oncall，异常检测和警报</h2><p></p><p>监控对于洞察系统的当前状态而言，是一个很棒的工具。但这只是自动检测问题并发出警报以供人们采取行动的一个垫脚石。</p><p></p><p>Oncall 本身是一个广泛的话题 - Increment 杂志在其 “<a href=\"https://increment.com/on-call/?ref=blog.pragmaticengineer.com\">On-Call 问题</a>\"”中涵盖了许多方面的内容。我的强烈认为，如果你拥有了\"you build it, you own it\"的心态，那随着而来的就是 OnCall。构建服务的团队拥有这些服务，也负责值班。我们的团队负责支付服务的值班。因此，每当出现警报时，值班工程师都会响应并查看详细信息。但是如何从监控到警报呢？</p><p></p><p>从监控数据中检测异常是一个艰巨的挑战，也是机器学习可以发光的领域。有很多第三方服务提供异常检测。再次幸运的是，我们团队有一个内部机器学习团队与之合作，他们根据Uber的使用情况量身定制了解决方案。位于纽约的 Observability 团队撰写了一篇有用的文章，介绍&nbsp;<a href=\"https://eng.uber.com/anomaly-detection/?ref=blog.pragmaticengineer.com\">Uber 的异常检测工作原理</a>\"。从我的团队的角度来看，我们将监控数据推送到该团队的管道，并获得具有各自置信度的警报。然后我们决定是否应该呼叫工程师。</p><p></p><p>何时触发警报是一个有趣的问题。警报太少可能意味着错过有影响的中断。太多会导致不眠之夜并使人筋疲力尽。跟踪和分类警报以及测量信噪比对于调整警报系统至关重要。检查警报并标记它们是否可操作，然后采取措施减少不可操作的警报，这是朝着实现<a href=\"https://increment.com/on-call/crafting-sustainable-on-call-rotations/?ref=blog.pragmaticengineer.com\">可持续的随叫随到轮换</a>\"迈出的良好一步。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be665e239f5d7fa02ab71d4305500049.png\" /></p><p></p><p>Uber 使用的<a href=\"https://eng.uber.com/on-call-dashboard/?ref=blog.pragmaticengineer.com\">内部 oncall 仪表板</a>\"示例，由 Vilnius 的 Uber 开发人员体验团队构建。</p><p></p><p>位于 Vilnius 的Uber开发工具团队构建了整洁的呼叫工具，我们用它来注释警报并可视化呼叫班次。我们的团队每周对上一次值班班次进行回顾，分析痛点并花时间改善值班体验，一周又一周。</p><p></p><p></p><blockquote>译者注：告警事件的聚合、降噪、排班、认领、升级、协同、灵活的推送策略、多渠道推送、和IM打通，是很通用的需求，可以参考&nbsp;<a href=\"https://flashcat.cloud/product/flashcat-duty/\">FlashDuty</a>\"&nbsp;这个产品，体验地址：<a href=\"https://console.flashcat.cloud/\">https://console.flashcat.cloud/</a>\"</blockquote><p></p><p></p><h2>故障和事件管理流程</h2><p></p><p>想象一下：你是本周的值班工程师。半夜，一个警报把你吵醒了。你调查是否有生产中断发生。糟糕，似乎系统的某个部分出现了故障。现在怎么办？监控和警报真实发生了。</p><p></p><p>对于小型系统来说，中断可能不是什么大问题，值班工程师可以了解正在发生的事情以及原因。它们通常易于理解且易于缓解。对于具有多个（微）服务的复杂系统，许多工程师将代码推向生产，仅仅查明潜在问题发生的位置就已经足够具有挑战性了。有一些标准流程来帮助解决这个问题会产生巨大的改观。</p><p></p><p>附加到警报的Runbook手册，描述简单的缓解步骤是第一道防线。对于拥有良好Runbook手册的团队，即使值班工程师不深入了解系统，也很少会成为问题。Runbook 需要保持最新、更新，并在故障出现时使用新型缓解措施进行处理。</p><p></p><p></p><blockquote>译者注：Nightingale 和 Grafana 的告警规则配置中，可以支持自定义字段，但是有些附加字段是默认就会提供的，比如 RunbookUrl，核心就是想传达SOP手册的重要性。另外，稳定性治理体系里，告警规则是否预置了RunbookUrl，是一个很重要的告警健康度的衡量指标。</blockquote><p></p><p></p><p>一旦有超过几个部署服务的团队，跨组织进行故障交流就变得至关重要。在我工作的环境中，成千上万的工程师会根据自己的判断将他们所开发的服务部署到生产环境中，每小时可能会有数百次部署。一个看似不相关的服务部署可能会影响另一个服务。在这种情况下，标准化的故障广播和通信渠道可以起到很大作用。我曾经遇到过多种罕见的警报信息 - 意识到其他团队中的人也看到了类似奇怪现象。通过加入一个集中式聊天群组来处理故障，我们迅速确定了导致故障的服务并解决了问题。我们做得比任何单独一人更快地完成了任务。</p><p></p><p>现在缓解，明天调查。在故障期间，我经常会感到“肾上腺素飙升”，想要修复出现问题的地方。通常根本原因是糟糕的代码部署，在代码更改中存在明显的错误。过去，我会立即跳进去修复错误、推送修复并关闭故障，而不是回滚代码更改。然而，在故障期间修复根本原因是一个可怕的想法。采用前进式修复收益甚微，损失却很大。因为新的修复需要迅速完成，所以必须在生产中进行测试。这就是引入第二个错误 - 或者在现有错误之上再出现一个故障 - 的原因。我见过像这样的故障不断恶化。只需先集中精力缓解，抵制修复或调查根本原因的冲动。适当的调查可以等到下一个工作日。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a6d233de83c0742855e71e9e39cc971.png\" /></p><p></p><p></p><blockquote>译者注：这一点老司机应该也深有感触，不要在线上Debug，出现问题立即回滚而不是尝试发布hotfix版本来修复！</blockquote><p></p><p></p><h2>事后分析，事件回顾和持续改进文化</h2><p></p><p>这是在说一个团队如何处理故障的后续。他们会继续工作吗？他们会做小规模的调查吗？他们是否会在后续工作中花费惊人的精力，停止产品工作以进行系统级修复？</p><p></p><p>正确进行的事后分析是构建强大系统的基石。一份好的事后分析既无指责，又十分彻底。Uber 的事后分析模板随着工程技术的发展而不断演变，包括事件概述、影响总览、时间线、根本原因分析、经验教训以及详细跟进清单等部分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/acdc0799e6293998244d2bc5f60404bd.png\" /></p><p></p><p>这是一个类似我在 Uber 工作中用到的复盘模板。</p><p></p><p>良好的事后分析深入挖掘根本原因并提出改进措施，以更快地预防、检测或缓解所有类似的故障。当我说深入挖掘时，我的意思是他们不会停留在根本原因上，即错误的代码更改和代码审查者没有发现错误。</p><p>他们使用“5why”探索方式进行更深入的挖掘，以达到更有意义的结论。举个例子：</p><p>为什么会出现这个问题？–&gt; 因为代码里引入了bug。为什么其他人没有发现这个错误？–&gt; 代码审查员没有注意到代码更改可能会导致此类问题。我们为什么只依赖于代码审查员捕获此错误？–&gt; 因为我们没有针对此用例的自动化测试。“为什么我们没有针对此用例的自动化测试？” –&gt; 因为在没有测试帐户的情况下很难进行测试。我们为什么没有测试帐户？ –&gt; 因为该系统尚不支持它们结论：这个问题指向了缺乏测试账户的系统性问题。建议将测试账户支持添加到系统中。接下来，编写所有未来类似代码更改的自动化测试。</p><p></p><p>事件回顾是事后分析的重要配套工具。虽然许多团队在事后分析方面做得很彻底，但其他团队可以从额外的输入和对预防性改进的挑战中受益。同样重要的是，团队要有责任感并有权执行他们提出的系统级改进。</p><p></p><p>对于认真对待可靠性的组织，最严重的故障会由经验丰富的工程师进行审查和挑战。组织级别的工程管理人员也应该出席，以提供授权来完成修复——尤其是当这些修复很耗时并阻碍其他工作时。健壮的系统不是一蹴而就的：它们是通过不断的迭代构建的。怎么才能持续迭代？这需要组织层面有持续改进、从故障中学习的文化。</p><p></p><h2>故障演习，容量规划和黑盒测试</h2><p></p><p>有一些常规活动需要大量投资，但对于保持大型分布式系统的正常运行至关重要。这些是我在优步第一次接触到的概念——在以前的公司，我们不需要使用这些，因为我们的规模和基础设施没有促使我们这样做。</p><p></p><p>一个数据中心故障演练是我认为很无聊的事情，直到我观察了其中几个实践。我最初的想法是，设计强大的分布式系统正是为了能够在数据中心崩溃时保持弹性。如果理论上它可以正常工作，为什么要经常测试呢？答案与规模有关，并且需要测试服务是否能够有效地处理新数据中心中突然增加的流量。</p><p></p><p>我观察到的最常见的故障场景是在发生故障转移时，新数据中心的服务没有足够的资源来处理全球流量。假设ServiceA和ServiceB分别从两个数据中心运行。假设资源利用率为60％，每个数据中心都有数十或数百台虚拟机在运行，并设置警报以在70％时触发。现在让我们进行故障转移，将所有流量从DataCenterA重定向到DataCenterB。 在没有提供新机器的情况下，DataCenterB突然无法承受负载。提供新机器可能需要足够长的时间，以至于请求会堆积并开始丢弃。这种阻塞可能会开始影响其他服务，导致其他系统的级联故障，这些系统甚至不是此故障转移的一部分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/60c41c9fe1442906ff94d081fff4ef65.png\" /></p><p></p><p>其他常见的故障场景包括路由级别问题、网络容量问题或背压痛点。数据中心故障转移是任何可靠分布式系统应该能够在没有任何用户影响的情况下执行的演习。我强调“应该”——这个演习是测试分布式系统可靠性最有用的练习之一。</p><p></p><blockquote>译者注：切流量，本就是预案“三板斧”之一。出故障的时候，要保证预案是可用的，那平时就少不了演练。重视起来吧，老铁们。</blockquote><p></p><p></p><p>计划的服务停机时间练习是测试整个系统弹性的极好方法。这些也是发现特定系统的隐藏依赖项或不适当/意外使用的好方法。虽然对于面向客户且依赖较少的服务，这种练习相对容易完成，但是对于需要高可用性或被许多其他系统所依赖的关键系统来说，则不那么容易喽。但是，当某一天这个关键系统不可用时会发生什么？最好通过受控演练来验证答案，所有团队都知道并准备好应对意外中断。</p><p></p><p>黑盒测试是一种测量系统正确性的方法，尽可能接近最终用户所看到的条件。这种类型的测试类似于端到端测试，但对于大多数产品来说，拥有适当的黑盒测试需要单独投入。关键用户流程和最常见的面向用户的测试场景是好的黑盒可测性示例：以这种方式进行设置可以随时触发它们，以检查系统是否正常工作。</p><p>以优步为例，一个明显的黑盒测试是检查乘客-司机流程是否在城市层面上正常工作。也就是说，在特定城市内的乘客能否请求优步，并与司机合作并完成行程？一旦这种情况被自动化，这个测试可以定期运行，模拟不同的城市。拥有强大的黑盒测试系统使得验证系统或部分系统是否正确工作更加容易。它还对故障转移演练非常有帮助：获取故障转移反馈最快捷的方法是运行黑盒测试。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/257439e3b606b83f2f6515db455b04ef.png\" /></p><p></p><p>上图是在故障转移演练失败时，使用黑盒测试的示例，在演练几分钟后手动回滚。</p><p></p><p>容量规划对于大型分布式系统同样重要。所谓大型，是指计算和存储成本每月达到数万或数十万美元。在这个规模下，使用固定数量的部署可能比使用自动扩展的云解决方案更便宜。至少，固定部署应该处理“业务常态”流量，并在高峰负载时进行自动扩展。但是，在接下来的一个月内、未来三个月内以及明年需要运行多少最小实例呢？</p><p></p><p>预测成熟且具有良好历史数据的系统的未来流量模式并不困难。这对于预算、选择供应商或锁定云提供商的折扣都很重要。如果您的服务费用很高，而您没有考虑容量规划，那么您就错过了降低和控制成本的简单方法。</p><p></p><h2>SLOs, SLAs 以及相关报告</h2><p></p><p>SLO 代表服务级别目标 - 系统可用性的数字目标。对于每个独立的服务，定义服务级别 SLO（例如容量、延迟、准确性和可用性的目标）是一种很好的做法。然后，这些 SLO 可以作为警报的触发器。服务级别 SLO 示例可能如下所示：</p><p></p><p>业务级 SLO 或功能 SLO 是服务之上的抽象。它们将涵盖用户或面向业务的指标。例如，业务级 SLO 可能是这样的：期望 99.99% 的电子邮件收据在旅行完成后的 1 分钟内发送。此 SLO 可能映射到服务级别 SLO（例如支付和电子邮件接收系统的延迟），或者它们可能需要以不同方式衡量。</p><p></p><p>SLA - 服务水平协议 - 是服务提供者和服务消费者之间更广泛的协议。通常，多个 SLO 组成一个 SLA。例如，99.99% 可用的支付系统可以是 SLA，它分解为每个支持系统的特定 SLO。</p><p></p><p>定义 SLO 后，下一步是衡量这些并报告它们。对 SLA 和 SLO 进行自动化监控和报告通常是一项复杂的项目，工程团队和业务部门都倾向于降低其优先级。工程团队可能不太感兴趣，因为他们已经有各种级别的监控来实时检测故障。另一方面，业务部门更愿意将重点放在提供功能上，而不是投资于一个没有立即商业影响的复杂项目中。</p><p></p><p>这就引出了下一个话题：运营大型分布式系统的组织迟早需要为这些系统的可靠性投入专门的人员。让我们来谈谈网站可靠性工程团队。</p><p></p><h2>SRE 作为独立团队</h2><p></p><p>网站可靠性工程（Site Reliability Engineering）起源于谷歌，大约在2003年左右开始，现在已经发展成为拥有超过1,500名SRE工程师的团队。随着生产环境运营变得越来越复杂，并需要更多自动化操作，这项工作很快就会成为一项全职工作。公司意识到他们有工程师正在全职从事生产自动化的时间因情况而异：这些系统越关键、故障越多，则此类情况出现得越早。</p><p></p><p>快速发展的科技公司通常会在早期组建 SRE 团队，由他们制定自己的路线图。在优步，SRE 团队成立于 2015 年，其使命是随着时间的推移管理系统复杂性。其他公司可能会在创建专门的基础架构团队时同时启动这样的团队。当一家公司发展到跨团队的可靠性工作占用了很多工程师的时间时，是时候为此设立一个专门的团队了。</p><p></p><p>有了 SRE 团队，这个团队可以让所有工程师更轻松地维护大型分布式系统的操作方面。SRE 团队可能拥有标准的监控和警报工具。他们可能购买或构建 oncall 工具，并且是 oncall 最佳实践的 goto 团队。他们可能会促进故障复盘并构建系统，以更轻松地检测、缓解和防止故障。他们当然有助于故障转移演练，经常推动黑盒测试，并参与容量规划。他们推动选择、定制或构建标准工具来定义和衡量 SLO 并报告它们。</p><p></p><p>鉴于公司有不同的痛点，他们寻求 SRE 来解决，因此 SRE 组织在公司之间是不同的。这个团队的名称通常也会不同：可能被称为运维、平台工程或基础设施。Google 出版了<a href=\"https://landing.google.com/sre/books/?ref=blog.pragmaticengineer.com\">两本关于站点可靠性的必读书籍</a>\"，这些书籍可免费获取，是深入了解 SRE 的绝佳读物。</p><p></p><h2>可靠性作为持续投资</h2><p></p><p>在构建任何产品时，构建第一个版本只是一个开始。在 v1 之后，新功能会添加到即将到来的迭代中。如果产品成功并带来业务成果，那么工作就会继续进行。</p><p></p><p>分布式系统具有相似的生命周期，只是它们需要更多投资，不仅仅是为了新功能，还要跟上扩展的步伐。随着系统开始承受更多负载、存储更多数据、更多工程师对其进行处理，它需要持续关注以保持平稳运行。很多人第一次构建分布式系统时会认为这个系统就像一辆汽车：一旦建好，只需要每几个月进行必要的维护。但是这种比较与实际情况相去甚远。</p><p></p><p>我喜欢把操作分布式系统的努力比作经营大型组织，例如医院。为了确保医院运转良好，需要进行持续的验证和检查（监控、警报、黑盒测试）。不断有新员工和设备加入：对于医院来说，这是像护士和医生这样的员工以及新的医疗设备；对于分布式系统来说，则是招募新工程师和添加新服务。随着人数和服务数量的增长，旧有做事方式变得低效：就像乡村小诊所与大都市中的大型医院不同一样。提出更有效率的方法成为全职工作，并且测量并报告效率变得重要。就像大型医院有财务、人力资源或安全等支持性质的办公室人员一样，经营较大规模分布式系统也依赖基础架构和SRE等支持团队。</p><p></p><p>为了让团队运行可靠的分布式系统，组织需要持续投资于这些系统的操作以及它们所构建的平台。</p><p></p><h2>更多推荐阅读</h2><p></p><p>虽然这篇文章的内容很长，但它仍然只是触及表面。要更深入地了解操作分布式系统，我推荐以下资源：</p><p></p><h3>图书</h3><p></p><p><a href=\"https://landing.google.com/sre/?ref=blog.pragmaticengineer.com\">Google SRE Book</a>\"&nbsp;- 一本来自 Google 的优秀免费书籍。<a href=\"https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/?ref=blog.pragmaticengineer.com\">《监控分布式系统》章节</a>\"对本文尤为相关。</p><p><a href=\"https://twitter.com/copyconstruct?ref=blog.pragmaticengineer.com\">Cindy Sridharan</a>\"&nbsp;的《<a href=\"https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/?ref=blog.pragmaticengineer.com\">掌握分布式跟踪</a>\"》是另一本优秀的免费书籍，其中提出了关于监控分布式系统的重要观点。</p><p><a href=\"https://twitter.com/martinkl?ref=blog.pragmaticengineer.com\">Martin Kleppmann</a>\"&nbsp;博士的《<a href=\"https://www.amazon.com/gp/product/1449373321/ref=as_li_tl?ie=UTF8&amp;tag=gregdoesit-20&amp;camp=1789&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1449373321&amp;linkId=adc1dc62fd3463b173cfd92dbe4ed821&amp;ref=blog.pragmaticengineer.com\">数据密集型应用系统设计</a>\"》是我迄今为止发现的关于分布式系统概念最实用的书籍。然而，这本书并没有涉及到文章中讨论的操作方面。</p><p></p><h3>在线资源</h3><p></p><p><a href=\"https://increment.com/on-call/?ref=blog.pragmaticengineer.com\">《Increment》杂志的“On-Call”问题</a>\"：一系列关于亚马逊、Dropbox、Facebook、Google和Netflix等公司事故响应流程的文章。</p><p><a href=\"http://brooker.co.za/blog/2019/04/03/learning.html?ref=blog.pragmaticengineer.com\">学习构建分布式系统</a>\"&nbsp;- AWS工程师<a href=\"https://twitter.com/MarcJBrooker?ref=blog.pragmaticengineer.com\">Marc Brooker</a>\"发表的一篇文章，回答了“我如何学习构建大型分布式系统”的问题。</p><p></p><h2>One more thing</h2><p></p><p>Uber 这位仁兄写的内容挺丰富了，感觉非常有共鸣，前段时间我也写了一个《<a href=\"https://mp.weixin.qq.com/s/CoYJ6WL2a1MwVtVhBRfaGQ\">稳定性体系建设白皮书</a>\"》，很多思路是相通的，各位看官可以一起查阅，查漏补缺，希望对你有些帮助:)</p><p></p><p>本文经授权转载自：https://flashcat.cloud/blog/operating-a-high-scale-distributed-system/</p>",
    "publish_time": "2023-04-16 14:14:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]