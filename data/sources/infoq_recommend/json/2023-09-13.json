[
  {
    "title": "开源 ML 社区的新星，Kubeflow 正式成为 CNCF 项目",
    "url": "https://www.infoq.cn/article/qtLPqywUBdDVuwXiSc6b",
    "summary": "<p><a href=\"https://www.cncf.io/\">云原生计算基金会（CNCF）</a>\"最近<a href=\"https://www.cncf.io/blog/2023/07/25/kubeflow-brings-mlops-to-the-cncf-incubator/\">宣布</a>\"，在<a href=\"https://www.cncf.io/people/technical-oversight-committee/\">技术监督委员会（TOC）</a>\"投票后，已接受<a href=\"https://www.kubeflow.org/\">Kubeflow</a>\"，用于在<a href=\"https://kubernetes.io/\">Kubernetes</a>\"上部署机器学习（ML）工作流的工具包，成为CNCF孵化项目。</p><p>&nbsp;</p><p>Kubeflow提供了一个开源的Kubernetes原生MLOps平台，用于为最流行的框架来开发和部署分布式机器学习（ML）：<a href=\"https://www.tensorflow.org/\">TensorFlow</a>\"、<a href=\"https://pytorch.org/\">PyTorch</a>\"、<a href=\"https://xgboost.readthedocs.io/en/stable/\">XGBoost</a>\"、<a href=\"https://mxnet.apache.org/versions/1.9.1/\">Apache MXNet</a>\"等等。</p><p>&nbsp;</p><p>Kubeflow由谷歌于2017年创建，自2017年以来，该社区现拥有150家公司、28K+ GitHub Stars、15+ 提交者以及15个版本。该项目分为六个半独立的小组：</p><p>&nbsp;</p><p>Notebooks工作组：负责开发界面和交互式部署环境训练Operator小组：开发并训练operator，以便在Kubernetes上进行分布式ML训练AutoML小组：开发了自动化模型开发软件KatibKubeflow Pipeline工作组：开发了将Python ML脚本转换为工作流模板的软件Manifest工作组：开发安装过程KServe项目：在Kubernetes上开发了高度可扩展的模型推理平台</p><p>&nbsp;</p><p>当前Kubeflow的架构如下图所示：</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b3fda0359f4a036dedf4ba78c1adcd7d.png\" /></p><p></p><p>&nbsp;</p><p><a href=\"https://www.kubeflow.org/docs/started/architecture/\">Kubeflow架构</a>\"</p><p>&nbsp;</p><p>使用Kubeflow配置接口，可以指定工作流所需的ML工具，并且可以将其部署到各种云、本地和on-premises平台上，用于实验和生产。</p><p>&nbsp;</p><p>TOC赞助商Ricardo Rocha表示：</p><p>&nbsp;</p><p></p><blockquote>Kubernetes环境提供了可重复性、可扩展性和快速交付，使其成为运行AI和ML计划的完美场所。Kubeflow通过提供机器学习管道和MLOps来填补了这一空白，同时与其广泛的社区和其他工具及计划密切合作，以创建一个更具凝聚力的生态系统。我们很高兴看到Kubeflow项目在CNCF中的发展，并看到它在MLOps领域的进步。</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>云原生计算基金会为项目定义了三个成熟度级别：沙箱阶段、孵化阶段和毕业阶段。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/014a6c279d56ccb130b582c90ad99855.png\" /></p><p></p><p>&nbsp;</p><p>项目阶段</p><p>&nbsp;</p><p>每个被提议的项目都要经过一个后备（fallback）投票过程，该过程由<a href=\"https://github.com/cncf/toc/blob/main/process/graduation_criteria.md\">TOC毕业标准</a>\"来描述：</p><p>&nbsp;</p><p></p><blockquote>一个项目需要有高于三分之二的绝对多数赞成才能被接受为孵化或毕业。如果没有绝对多数的赞成选票来支持项目进入毕业阶段，那么任何毕业的选票都会被重新计算为项目进入孵化阶段的选票。如果没有绝对多数的选票来支持项目进入孵化阶段，那么任何毕业或孵化的选票都会被重新计算为项目进入沙箱阶段的赞成选票。如果没有足够的赞成选票来支持项目进入沙盒阶段，该项目将被拒绝。</blockquote><p></p><p>&nbsp;</p><p>云原生计算基金会（CNCF）生态系统负责人Taylor D.在LinkedIn上<a href=\"https://www.linkedin.com/posts/onlydole_kubeflow-brings-mlops-to-the-cncf-incubator-activity-7089722163364016131-csGZ?trk=public_profile_like_view\">发表了一篇专门的帖子</a>\"，以庆祝Kubeflow作为孵化项目加入CNCF。</p><p>&nbsp;</p><p>&nbsp;</p><p>Kubeflow的主要替代方案是亚马逊的<a href=\"https://aws.amazon.com/sagemaker/?nc=sn&amp;loc=1\">Sagemaker</a>\"，这是由AWS完全管理的机器学习平台。</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/08/kubeflow-cncf-project/\">https://www.infoq.com/news/2023/08/kubeflow-cncf-project/</a>\"</p>",
    "publish_time": "2023-09-13 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI大模型背后的惊人数字：问ChatGPT 5个问题，耗水500毫升？训练一次GPT-3，碳排放量相当于开车往返月球？",
    "url": "https://www.infoq.cn/article/NuKxISZRb5sjg1lXgmeN",
    "summary": "<p>ChatGPT 的爆火掀起了 AI 大模型热潮，但科技进步始终是把双面剑，AI 大模型带来诸多便利的同时，也引发了人们关于能源消耗与环境污染的担忧。此前，曾有研究称训练 GPT-3 的碳排放量等同于开车往返月球，也有研究称训练人工智能模型比长途飞行排放的碳要多得多。</p><p>&nbsp;</p><p>能源消耗方面，构建大语言模型需要分析海量人类文章内容中蕴藏的模式，所有这些计算都要消耗大量电力并生成可观的热量。为了保持凉爽，数据中心需要泵水冷却，并将其存储在机房之外如仓库般大小的冷却塔中。</p><p>&nbsp;</p><p>近日，外媒报道称，微软用于支持 OpenAI 的技术设施需要大量用水，这些水抽取自爱荷华州中部浣熊河和得梅因河的分水岭处，被用于冷却一台强大的超级计算机。一份研究显示，ChatGPT 这类 AI 大模型耗水量惊人：用户每向 ChatGPT 提出 5-50 条提示词或问题，ChatGPT 就会消耗掉 500 毫升的水。</p><p></p><h2>问ChatGPT 5-50个问题，耗水500毫升</h2><p></p><p>&nbsp;</p><p>日前，微软在其最新环境报告中透露，从 2021 年到 2022 年，公司全球设施用水量猛增了 34%（达到近 17 亿加仑，相当于 2500 多个奥运会级别的赛级泳池）。这个数字远远高于几年前外部研究人员的统计，而背后的驱动力量自然就是 AI 构建的需求。</p><p>&nbsp;</p><p>对此，加州大学河滨分校研究员 Shaolei Ren表示，“可以合理推断，（用水量）大部分增长要归功于 AI”，包括“对生成式 AI 的大量投入以及同 OpenAI 公司的合作。”</p><p>&nbsp;</p><p>谷歌报告称用水量同比增长了 20%，Ren 认为这很大程度上也源自 AI 研究需求。当然，谷歌的用水量增长并不均匀——其俄勒冈州基础设施的用水量保持稳定，但拉斯维加斯周边地区的用量则翻了一番。爱荷华州同样成为用水大户，谷歌在这里的康瑟尔布拉夫斯数据中心消耗的水资源比其他任何地方都要多。</p><p>&nbsp;</p><p>在即将于今年晚些时候发表的论文中，Ren 研究团队估计用户每向 ChatGPT 提出 5-50 条提示词或问题，ChatGPT 就会消耗掉 500 毫升的水（具体数字取决于基础设施所在位置和季节气候）。这一估算还未包含未经测量的间接用水，例如数据中心冷却电力所对应的发电耗水。</p><p>&nbsp;</p><p>Ren 表示，“大多数人并不清楚 ChatGPT 的资源消耗情况。但如果我们不了解资源用量，就没办法帮助节约资源。”</p><p>&nbsp;</p><p>据了解，微软于 2019 年向总部位于旧金山的 OpenAI 划拨了首笔 10 亿美元投资。随后，OpenAI 正式发布了 ChatGPT。作为合作协议的一部分，微软负责为 OpenAI 提供 AI 模型训练所需要的算力。</p><p>&nbsp;</p><p>为了践行承诺，两家公司纷纷将目光投向爱荷华州的西得梅因——十多年来，这座拥有 6.8 万人口的市镇一直是微软的数据中心聚集地，负责为其云计算服务提供支持。微软的第四和第五处数据中心将于今年晚些时候在这里开放。</p><p>&nbsp;</p><p>据了解，一年中的大部分时间里，爱荷华州当地的气候都相当凉爽，微软可以直接利用室外空气来保持超级计算机正常运行，并将产生的热量直接排放出去。该公司在一份披露报告中表示，只有在温度超过 29.3 摄氏度时，他们才需要切换为水冷模式。</p><p>&nbsp;</p><p>但即便如此，当地设施在夏天的用水量仍然相当惊人。据西得梅因水厂介绍，2022 年 7 月，也就是 OpenAI 正式完成 GPT-4 训练的前一个月，微软向其爱荷华州数据中心集群泵入约 1150 万加仑的水，约占该地区总用水量的 6%。</p><p>&nbsp;</p><p>2022 年，该水厂的一份文件提到，除非微软能够“证明并落实能够显著降低峰值期用水量的技术”，否则该公司及当地政府将不再“考虑批准微软未来的数据中心项目”。因为只有这样，他们才能保障当地住宅和其他商业运营的供水需求。</p><p>&nbsp;</p><p>微软表示，他们正与水厂直接合作以解决对方反馈的问题。水厂方面则通过书面声明指出，微软一直是其良好合作伙伴，也始终在与当地官员合作，探讨如何在满足需求的同时减少水资源消耗。</p><p></p><h2>大模型的碳排放量有多少？</h2><p></p><p>&nbsp;</p><p>除了能源消耗，ChatGPT 这类 AI 大模型的碳排放量也曾引发大众担忧。此前曾有计算机科学家称，GPT-3 整个训练周期的碳排放量，相当于开车到月球再返回地球；GPT-3 一轮训练所消耗的电量，足以支撑丹麦 126 个普通家庭度过一整年。</p><p>&nbsp;</p><p>做出这一猜测的专家来自丹麦哥本哈根大学，他们开发出名为 Carbontracker 的开源工具，用于预测 AI 算法的碳足迹。Carbontracker 估计，微软数据中心内使用英伟达 GPU 构建的神经超级网络运行功率约为 19 万千瓦时，如果按照美国的平均碳排放水平计算，这将产生 8.5 万公斤（85 吨）的二氧化碳，相当于 2017 年制造一辆新车所产生的排放量。这样的排放量相当于车辆在欧洲行驶 80 万公里，基本相当于开车到月球再返回地球的总行驶距离。</p><p>&nbsp;</p><p>Carbontracker 的创造者之一、AI 电力消耗研究论文联合作者 Lasse Wolff Anthony 认为，社区必须认真对待资源消耗问题。文章提到，从 2012 年到 2018 年之间，AI 研究的能源成本增长了约 30 万倍。</p><p>&nbsp;</p><p>Anthony 在采访中表示，“二氧化碳估值是根据模型训练期间，当地发电的平均碳排放量再加上运行模型的硬件总功耗所计算得出。”“我们通过多个 API&nbsp;来跟踪碳排放强度。如果模型训练所在地区没有 API 可用时，我们则会默认取欧洲平均值，因为目前还没有免费开放的全球监测数据。这些 API 会在训练期间定期查询硬件能耗，以准确估算总体碳足迹。”</p><p>&nbsp;</p><p>当然，上述结果的前提是假设训练 GPT-3 的数据中心完全依赖于化石燃料，这跟实际情况可能有所出入。</p><p>&nbsp;</p><p>有分析认为，当前大模型的碳排放量可能被严重夸大。事实上，全球科技行业占总体温气体排放量的比例仅为 1.8%-3.9%，而其中又只有一小部分与 AI 相关。在规模层面，AI 的碳排放还远无法与航空等其他主要碳源头相提并论。相较于随时运行的汽车和飞机，训练 GPT 这类模型所对应的碳排放量绝对称不上主要矛盾。</p><p>&nbsp;</p><p>相较于随时运行的汽车和飞机，训练 GPT 这类模型所对应的碳排放量绝对称不上主要矛盾。</p><p>诚然，目前我们并不清楚到底有多少大 AI 模型正在训练当中，但如果只考虑 GPT-3 或其他规模更大的模型，那么此类模型成果总计还不到 1000 个。这里我们可以做个简单计算：</p><p>&nbsp;</p><p></p><blockquote>最近一项评估认为，训练 GPT-3 排放了 500 吨二氧化碳，Meta 的 Llama 模型则估计排放 173 吨。如果训练 1000 个这样的模型，那么总二氧化碳排放量约为 50 万吨。2019 年，商业航空业排放了约 9.2 亿吨二氧化碳，几乎是大语言模型训练的 2000 倍。而且要注意，这是一年的航空业运营对比多年来的大语言模型训练。虽然后者的环境影响值得关注，但过度夸大明显有违客观公平，需要更细致地斟酌考量。</blockquote><p></p><p>&nbsp;</p><p>当然，这里讨论的还只是模型训练阶段。模型的运行和使用同样要消耗电力并产生相关排放。根据一项分析，ChatGPT 运行一年可能会排放约 1.5 万吨二氧化碳。但另一项分析结果则乐观得多，认为约在 1400 吨左右。但无论取哪个数字，虽然没有低到忽略不计的程度，但与航空业相比仍有几个数量级的差距。</p><p>&nbsp;</p><p>需要强调的是，问题的重点并不在于探索 GPT-3 这类大模型的碳足迹，而是希望引起人们对于训练先进神经网络所消耗的巨量资源的关注。</p><p>&nbsp;</p><p>目前，不少企业已开始重视能源消耗和环境污染问题，并在制定相应解决方案。微软在一份声明中称，正在资助研究以测量 AI 开发所对应的能耗和碳足迹，“同时致力于提升大语言模型系统的训练和应用效率。”</p><p>&nbsp;</p><p>微软表示，“我们将继续监测自身排放、加快进展，同时更多使用清洁能源为数据中心供电、采购可再生能源，借此实现到 2030 年的碳负排放、水资源正循环和零浪费的可持续发展目标。”</p><p>&nbsp;</p><p>OpenAI 也回应了这些评论，称正“认真考虑”如何更好地运用宝贵算力。“我们意识到训练大模型可能会消耗电力和水资源”，因此正在努力提高效率。”</p><p></p><h2>需要建立透明的排放制度</h2><p></p><p>&nbsp;</p><p>随着 AI 系统的不断开发和应用，我们的确需要关注它对环境的影响。除了传统上行之有效的实践之外，我们还应探索出特定于生成式 AI 的减排思路。</p><p>&nbsp;</p><p>首先，透明排放将至关重要。有了这种透明度保障，我们才能监控与 AI 模型训练和使用相关的碳排放量，确保模型部署者和最终用户能够根据这些数字制定 AI 使用策略。此外，还应将 AI 相关排放纳入温室气体清单与净零目标，将此作为AI整体透明制度的组成部分。</p><p>&nbsp;</p><p>法国最近就通过一项法律，要求电信企业提交关于其可持续发展的透明度报告。类似法律未来可能要求采用 AI 技术的产品向客户报告其碳排放量，并要求模型提供商通过 API 开放碳排放数据。</p><p>&nbsp;</p><p>更高的透明度将会带来更强有力的激励措施，借此建立起愈发节能的生成式 AI 系统，同时探索新的效率提升途径。InfoQ 最近发表的一篇文章提到，微软高级软件工程师 Sara Bergman 呼吁人们关注 AI 系统的整个生命周期，并建议采用绿色软件基金会提出的工具和实践以改善 AI 系统的能源效率。具体条款包括认真考量服务器硬件与架构选择、关注时间/区域间的发电排碳量差异等。更重要的是，生成式 AI 本身也有望在提高能效当中做出独特的贡献。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4\">https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4</a>\"</p><p><a href=\"https://www.infoq.com/articles/carbon-emissions-generative-ai/\">https://www.infoq.com/articles/carbon-emissions-generative-ai/</a>\"</p><p><a href=\"https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/\">https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/</a>\"</p>",
    "publish_time": "2023-09-13 14:00:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "信也科技副总裁陈磊：数据应用与数据合规如何兼顾？",
    "url": "https://www.infoq.cn/article/hSjZFzB98kH74Jrm5XJH",
    "summary": "<p>随着科技的快速进步，金融行业正在经历一场前所未有的变革。无论是客户管理、服务体验还是信贷风控，智能化的脚步已经深度渗透到每一个环节。那么，金融行业的智能应用水平如何？实时平台化的数据建设应该如何做？数据应用与数据合规如何兼顾？</p><p></p><p>在 9 月 7 日的 InfoQ 超级连麦·数智大脑直播中，极客邦科技创始人 &amp;CEO 霍太稳与信也科技副总裁<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577\">陈磊</a>\"展开了主题为《如何处理好金融数智化过程中的数据「养料」》的讨论。</p><p></p><p>陈磊表示，为了满足金融机构在数据安全和合规性方面的需求，在满足合规要求的基础上，可以尝试以下三个方向：首先，通过多方安全计算（MPC）等隐私计算技术来确保敏感数据在安全的前提下被充分使用。其次，对数据进行分类和分级，针对不同敏感度的数据应采取不同的处理方式。最后，培养企业内部的数据安全文化是非常关键的。</p><p></p><p>以下是对话全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>InfoQ：目前整个金融行业的智能应用水平呈现哪些特点？</h5><p></p><p></p><p>陈磊：首先，金融行业和电信业及航空业是数字化最早的领域，和其他行业相比，具有一定的先发优势。这也使得金融行业在接纳新技术方面具有相对开放的态度。例如，在信息化早期阶段，金融行业便已经开始采用小型机和大型机等先进技术。近年来，整个金融行业在智能应用方面表现出“全面开花，全链条覆盖”的特点。无论是客户管理、服务体验、信贷风控，还是企业生态化服务，都有大量典型应用。举几个例子：</p><p></p><p>在存贷汇业务方面，智能化改造极大地提升了业务效率。例如，以前申请信用卡需要一个月的审批时间，而现在审批过程快速得多。</p><p></p><p>从消费者或银行客户的角度看，服务体系也经历了大幅改造。线上服务的比例大幅提升，和移动互联网的普及有着密切关系。例如，现在有大量的 7*24 小时智能客服在线提供服务。</p><p></p><p>智能化已经深度渗透到各个生活场景中，比如，线下面部识别支付方式提升了金融服务在零售场景中的覆盖率。</p><p></p><p>总体而言，金融行业在智能应用方面虽然还在发展中，但已经达到了相当高的水平。</p><p></p><h5>InfoQ：您认为一个金融机构要进行数据管理和应用，主要有哪些难题？</h5><p></p><p></p><p>陈磊：我认为金融机构在数据管理和应用上面临的挑战主要有以下几个方面：</p><p></p><p>数据来源的多样性：金融机构的数据来源非常丰富，与信息化的发展阶段和众多服务提供商有着紧密的关系，导致数据存在多种格式和标准。因此，如何实现统一的数据规范和标准成为一个关键问题。数据安全与合规：金融行业对数据安全的要求极高，因为其中涉及大量客户的敏感信息。任何数据泄露都可能给客户带来损失，并对金融机构的声誉和持续经营带来威胁。因此，如何在确保数据安全的同时进行有效的应用，是金融机构需要深入考虑的问题。数据质量问题：与其他行业一样，金融行业也面临数据质量的挑战。尤其是那些历史遗留问题，从线下到线上转型的过程中可能导致的数据增长问题。这些问题与纯互联网起步的企业相比，会带有更多历史负载。满足多元化需求：随着用户或消费者对金融服务需求的多元化，如何满足这些多样化的需求也是金融机构面临的一个重要挑战。人才问题：对于一些中小银行，他们面临着人才梯队的问题。这些机构普遍缺乏专业的数据管理和应用人才，如何有效地利用数据，对他们来说是一个更大的挑战。</p><p></p><h5>InfoQ：企业可以从哪些方面去解决这些挑战？尤其是人才梯度的建设方面。</h5><p></p><p></p><p>陈磊：关于数据工作和数字化转型，以及<a href=\"https://www.infoq.cn/article/7d4DwesFkH9sMaXYf4pa\">智能化</a>\"转型的推动方式。实际上，这些转型不是简单地通过自下而上的方式就能完成的。最有效的推动方式是自上而下与自下而上相结合，以形成合力。关键在于，数据建设的成果必须在业务中得以体现。没有顶层的支持和规划，长期的数据建设是很难落地的。因此，顶层设计是第一要务。这通常需要董事长或行长的决策，并需要持续三到五年的努力才能见到成果。</p><p></p><p>关于数据人才的吸引问题。首先，公司或行业需要建立一种支持数据文化的氛围。在这样的环境下，不仅需要数据工程师，还需要数据分析师和算法工程师共同努力。单一角色的招聘是难以实现数字化深度应用的。</p><p></p><p>另外，持续的数字化转型还需要注重流程和规范的建设，以确保成功能够被复制。与此同时，工具和平台的建设也是关键，它能降低数据工程和数据应用的门槛，从而提高效率。</p><p></p><p>很多年前，当我们开始使用大数据技术时，上手难度和学习成本都相对较高。但在最近几年，由于平台化建设取得了显著进步，这些成本逐渐降低。</p><p></p><p>最后，我想强调的是，整个数据工作，无论是从理念还是从落地的角度来看，都是一个长期的过程。这不仅需要耐心和适当的节奏，还需要阶段性的成果来推动进程。这其中，推动者和执行者的配合和规划是至关重要的。</p><p></p><h5>InfoQ：请您分享一下信也科技在数据管理、数据应用方面的成功经验？</h5><p></p><p></p><p>陈磊：从一开始，我们的核心理念就是要将数据和算法应用于商业决策，因为只有到达决策这一层，数据的价值才能得以体现。</p><p></p><p>在金融场景中，主要有几个关注点：首先是获客，传统金融机构主要通过线下网点进行客户获取。但随着移动互联网的兴起，如何通过线上方式吸引客户成为他们新的关注焦点；其次是风控定价，这是金融的核心模块，尤其在信贷场景中，如何决定一个用户或企业的授信额度和利率是至关重要的；最后是客户服务，在用户使用金融服务后，如何长期高效地维护与客户的关系。</p><p></p><p>在数据应用体系方面，我们形成了不同的层次，包括数据层和算法层。这些层次涵盖了数据计算和存储资源、离线和实时数据仓库、业务特征存储，以及上层的算法引擎。最终，这些都会体现在数据开发平台和业务智能（BI）平台上，在这样的框架下，数据更容易被加工和应用，同时业务和技术团队也能更顺畅地利用这些数据进行交流。</p><p></p><p>在早期，由于系统尚不完善，业务的数据需求常常需要较长时间才能得到满足。这种延迟会导致商机的丧失，给业务带来巨大机会损失。</p><p></p><p>另外我们注意到在获客方面，信息流个社交媒体逐渐成为搜索引擎和电商以外重要的新渠道。为此，我们开发了一套流量管理平台，能有效整合媒体和业务转化数据，这样做不仅提高了数据的时效性和全面性，还进一步使广告投放的效果评估更加及时和准确。</p><p></p><p>当然，我们在这个过程中也遇到了不少挑战。比如，与多家广告媒体对接时，由于它们的数据格式和 API 接口差异较大，我们需要花费额外时间和精力进行标准化和归一化，这也是考验数据架构能力的。</p><p></p><p>最后，在<a href=\"https://www.infoq.cn/article/tXdg1xI1YWGYG6iGg4rj\">风控</a>\"模块方面，我们的风控系统在多年的迭代后，已经能够处理大规模特征生成，精细化客群划分以及先进算法的自适应应用。这使得每一笔信贷申请都能得到精准的审批，起到了“风险守门员”的角色。其中的一个难点是如何利用非结构化数据，例如声纹数据、图像数据和关联关系数据。以上就是我们在数据管理和应用方面的一些经验。如果您对更多细节感兴趣，欢迎来 Fcon 现场与我们进行交流。</p><p></p><h5>InfoQ：最近财政部发布文件将数据作为资产被纳入会计报表，助力推动数据要素资产化。您认为这一举措对于金融行业意味着什么，会从哪些方面影响金融业数字化进程？</h5><p></p><p></p><p>陈磊：当财政部的新文件发布后，我们第一时间进行了学习和研读。该文件特别强调，<a href=\"https://www.infoq.cn/article/fC0VKxtiIoH096FrfD3U\">数据资产</a>\"在企业财务报表中的体现将成为一项不可或缺的义务。</p><p></p><p>首先，这无疑是一个重要的里程碑。它符合国家的十四五规划，即让数据成为生产要素，并在实际业务中流通。这从顶层设计上肯定了数据的价值，并推动了数据价值的量化。</p><p></p><p>其次，这将明显提高金融机构的数据管理水平。无论数据是外部采购的，还是由自家业务生成，或是用于内部业务或外部服务，都需要在财务报表中反映其价值。这将促使金融机构更加重视数据资源，并推动数据的收集、整理和应用。</p><p></p><p>第三，这将推动数据价值的深度应用。金融机构不仅需要理解数据的价值，还需要深入挖掘和应用数据，以提高金融服务的效率和质量。</p><p></p><p>第四，这也将促进金融科技和其他企业的发展。以往企业资产的评估并不包括数据，而现在，数据将成为企业附加值的一部分，有助于企业获得更好的金融服务，进而推动企业的创新和快速发展。</p><p></p><p>最后，虽然数据资产在财务报表中的体现是一个重要步骤，但仍有很多工作需要做，比如数据确权、定价和合理分类等。这是一个非常重要的开始，我们希望与合作伙伴和业内朋友共同探讨如何在各自的业务中实施这一新政。</p><p></p><h5>InfoQ：如何平衡数据的安全、可信和合规性，同时又不过多限制数据的使用？</h5><p></p><p></p><p>陈磊：首先，为了满足金融机构对数据安全和合规性的要求，我们遵循各项监管准则。例如，若规定数据不能外流，我们会通过技术手段确保数据在可信环境中进行运算，同时保证数据不离开这一环境。这涉及到近年来比较流行的隐私计算和多方安全计算技术。这些技术目前还在金融“沙盒”测试阶段，大规模推广还需法规支持。</p><p></p><p>从业务角度看，短期内可能需要承受一定的数据价值折损，但长远来看，我们期望在技术上能有突破，以在保证数据安全的同时，更合理地使用数据。</p><p></p><p>第二点，我们需要对数据进行分类和分级。并非所有数据都是敏感信息，也不都需要通过隐私计算进行处理。目前，隐私计算效率相对较低，因此，针对不同敏感度的信息，应采取不同处理方式。对于敏感信息，可以使用隐私计算；对于非敏感信息，可以进行脱敏后合理地进行加工和计算。</p><p></p><p>第三点，无论是金融科技公司还是金融机构，都需要培养数据安全文化。一旦大家明确了安全底线，日常操作将更加安全。</p><p></p><p>总体而言，实现数据使用效率和数据安全之间的平衡需要大量的监管工作、行业研究和长期实践，以找到一个相对平衡的点。</p><p></p><h5>InfoQ：在金融数字化、智能化的过程里，实时平台化的数据建设会发挥什么样的作用？</h5><p></p><p></p><p>陈磊：在移动互联网发展和新应用出现的背景下，金融场景对实时服务的需求越来越大，对应地，对数据的实时处理需求也在增加。在这种环境下，数据平台化变得尤为重要，具体可以从以下几个方面来看：</p><p></p><p>第一点，平台化的最终目标是提高数据的质量和时效性。通过数据平台，金融机构可以实现实时的数据采集、处理和分析，从而提高数据质量和实时性。</p><p></p><p>第二点，拥有高质量的数据后，不仅能更好地支持决策，还能通过数据分析来发掘新的商业机会。一个成熟的金融数据平台能为金融机构提供更丰富、更全面的数据资源，为数据分析和挖掘提供丰富的“养料”，进而帮助业务发掘新的机会点。</p><p></p><p>第三点，平台化的优势是加强数据安全和可靠性的管理。具体来说，通过数据平台可以实现权限的统一分配、合理管控以及任务的实时监控。这不仅增强了数据的安全性，还提高了运维效率。因为实时数据服务对任务的健壮性要求很高，平台化能让我们及时发现并解决问题。</p><p></p><p>总体而言，数据平台化能为<a href=\"https://www.infoq.cn/theme/213\">金融</a>\"机构提供更高效、更可靠，以及更安全和健壮的数据服务。这最终将体现在业务增长和效率提升上。</p><p></p><h5>InfoQ：FCon 金融实时数据平台建设之路专题的亮点有哪些？</h5><p></p><p></p><p>陈磊：本次 FCon 会议我们主要关注实时数据应用，涵盖从底层框架到上层应用的广泛领域。讨论将集中在以下几个主题：</p><p></p><p>从业务角度考虑，我们会探讨实时数据服务的应用场景和价值，以及哪些场景中离线或准实时数据就足够用，无需实时服务；</p><p></p><p>关于实时数据的开发平台，我们将讨论如何保证服务的阶段性和稳定性，以及如何提高开发效率。同时，会深入到实时数仓的设计和实现，以及与离线数仓的运维难度对比；</p><p></p><p>我们还会关注实时数据服务的资源管控和任务优化，特别是在 ROI 不高的场景中，如何更有效地使用计算和存储资源；</p><p></p><p>最后，我们将集中讨论实时数据平台的设计理念、技术难题，以及组件选择等。我们也邀请了金融和金融科技行业的专家，以进行更广泛和有代表性的讨论。</p><p></p><h5>InfoQ：目前大模型在整个金融行业的应用需要注意哪些问题？</h5><p></p><p></p><p>陈磊：从年初开始，我们已经在大模型应用方面进行了一些尝试。这些初步应用主要集中在内部服务和应用。从金融行业的角度来看，大模型落地仍面临多重挑战。</p><p></p><p>首先，大模型的可控性相对较差，而金融行业对结论可靠性有很高的要求。同时，模型的逻辑或结果也需要高度可解释性；</p><p></p><p>其次，大模型在时效性方面也面临挑战，特别是在需要实时响应的场景。不过，随着底层架构和硬件能力的提升，这些问题有望得到解决；</p><p></p><p>第三，考虑到金融行业对数据安全和隐私保护有严格要求，如何在输入数据时确保用户隐私和信息安全也是一大挑战。因此，我们也在进行相关的保护措施；另一个关键问题是如何将金融行业的专业知识和数据融入大模型中。这依然是一个探索和应用阶段的问题，目标是形成一个更精准和针对性的行业模型；</p><p></p><p>从实践和行业交流来看，我们认为大模型有很大的潜力与现有的数据智能技术结合，带来颠覆性的改变。在系统和工程层面，大模型将改进设计、开发、测试以及运维流程。在算法应用层面，它将提升传统机器学习和深度学习方法，比如在文本分析和智能客服应用方面；</p><p></p><p>最终，<a href=\"https://www.infoq.cn/article/TSJbOkWweAvFh44Oo2dL\">大模型</a>\"和数据技术的结合可能实现从数据到决策的自动化流程。例如，通过对话交互界面进行大量的数据分析和洞察，甚至直接生成详细报表和决策；</p><p></p><p>总体而言，尽管短期内仍存在挑战，但随着技术和应用场景的持续迭代，我们对大模型在应用场景落地方面持有很大的信心。</p><p></p><h4>关于FCon</h4><p></p><p></p><p>首届<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">FCon全球金融科技大会</a>\"将于 11 月 19-20 日在上海举办。大会将围绕金融领域数字化转型挑战探索、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等 10+专题进行交流。</p><p></p><p>目前大会邀请了汇丰科技中国区的代理总经理马国栋、度小满金融数据智能部总经理杨青先、蚂蚁集团副总裁 &amp; 首席技术安全官韦韬博士、恒生聚源总经理吴震操担任大会联席主席。更多嘉宾仍在邀请中......</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">点击链接</a>\"即可查看全部演讲专题。</p><p></p><p>目前 <a href=\"https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">5 折 优惠购票</a>\"，仅限前 100 人，咨询购票可联系：17310043226（微信同手机号）。</p>",
    "publish_time": "2023-09-13 14:02:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "太平洋健康险北京研发中心负责人周小三确认出席 FCon ，分享智能化服务式营销保险平台的落地实践",
    "url": "https://www.infoq.cn/article/IIbghuEOTSOOqcLMK26v",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。太平洋健康险北京研发中心负责人周小三将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5495?utm_source=infoqweb&amp;utm_medium=article\">智能化服务式营销保险平台的落地实践</a>\"》主题分享，介绍全流程管家服务式营销平台的业务逻辑和整体技术架构，让你了解数据智能和人工智能如何赋能业务、服务式营销方式、公域引流和私域经营通路、用户全生命周期管家式服务建设以及用户经营、转化、裂变逻辑的经验。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5495?utm_source=infoqweb&amp;utm_medium=article\">周小三</a>\"，目前是太平洋健康险北京研发中心负责人，深耕保险 + 科技。曾在平安健康险移动事业部负责整体研发，科技赋能互联网保险业务数字化，推动保险产品、医疗、健康和综合金融生态业务创新，并成为行业标杆。</p><p></p><p>曾在京东安联负责数据平台（京东技术委员会数据工作组成员），科技赋能互联网场景保险业务数字化，推动保险和商城场景化融合和优势互补。曾在泰康养老职域业务事业部科技创新部总经理，科技赋能传统寿险业务流程再造数字化，推动流程标准化的数字化效率变革。他在本次会议的演讲内容如下：</p><p></p><p>演讲：智能化服务式营销保险平台的落地实践</p><p></p><p>从行业来讲，保险行业进入存量市场争夺时代，简单暴力的销售手段已经不能适应市场环境，以客户为中心，提供个性化和差异化的服务式营销成为趋势。从客户来讲，过往客户买完保险后与保司很少互动，保险服务偏线下，大部分客户感知不到保司服务，很容易脱落。</p><p></p><p>太平洋健康险秉承蓝医保”产品即服务”的理念，为用户提供售前、售中、售后全流程服务，让客户买的放心，蓝医保客户买完即可享受服务，通过多种服务陪伴客户，实现客户黏度和复购的提升。经过探索，太保健康险形成了一套以客户为中心全流程管家式服务营销解决方案，既服务用户又达到营销效果和目的。</p><p></p><p>主要涉及的创新技术：</p><p></p><p>○ 基于大数据特征计算和智能派单算法技术，对用户、线索、顾问、保险产品等多维特征计算，并结合保险场景构建特征匹配系统，达到精准转化的效果</p><p>○ 基于通用大模型技术，对保险知识大模型和保险业务操作大模型的技术探索和应用</p><p></p><p>演讲提纲：</p><p></p><p>全流程管家服务式营销平台的业务逻辑 全流程管家服务式营销平台整体技术架构介绍 </p><p>○ 基于微信生态的场景触达和用户连接 </p><p>○ 智能工单调度系统建设 </p><p>○ 基于用户画像的线索跟进体系</p><p>○ 数据智能赋能经营决策 </p><p>○ 基于人工智能提升工作效率和转化能力的探索</p><p>技术赋能业务的思考</p><p></p><p>你将获得：</p><p></p><p>○ 了解数据智能和人工智能赋能业务</p><p>○ 了解服务式营销方式</p><p>○ 公域引流和私域经营通路</p><p>○ 了解用户经营、转化、裂变逻辑的经验</p><p>○ 了解用户全生命周期管家式服务建设</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-09-13 14:21:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一文解读分布式一致性协议Paxos",
    "url": "https://www.infoq.cn/article/Bmf7QukS2rP887GHAZA3",
    "summary": "<p></p><h2>一、Paxos协议简介</h2><p></p><p></p><p>Paxos算法由Leslie Lamport在1990年提出，它是少数在工程实践中被证实的强一致性、高可用、去中心的分布式协议。Paxos协议用于在多个副本之间在有限时间内对某个决议达成共识。Paxos协议运行在允许消息重复、丢失、延迟或乱序，但没有拜占庭式错误的网络环境中，它利用“大多数 (Majority)机制”保证了2F+1的容错能力，即2F+1个节点的系统最多允许F个节点同时出现故障。</p><p></p><p></p><blockquote>拜占庭式错误释义：一般地把出现故障但不会伪造信息的情况称为“非拜占庭错误”(Non-Byzantine Fault)或“故障错误”(Crash Fault)；而伪造信息恶意响应的情况称为“拜占庭错误”(Byzantine Fault)。</blockquote><p></p><p></p><h4>1、核心概念</h4><p></p><p></p><p>Proposal：提案（提案 = 提案编号acceptNumber + 提案值acceptValue）;Proposal Number：提案编号;Proposal Value：提案值。</p><p></p><h4>2、参与角色</h4><p></p><p></p><p>Proposer（提案者）：处理客户端请求，主动发起提案；Acceptor (投票者)：被动接受提案消息，参与投票并返回投票结果给Proposer以及发送通知给Learner；Learner（学习者）：不参与投票过程，记录投票相关信息，并最终获得投票结果。</p><p></p><p>在实际的分布式业务场景中，一个服务器节点或进程可以同时扮演其中的一种或几种角色，而且在分布式环境中往往同时存在多个Proposer、多个Acceptor和多个Learner。</p><p></p><h4>3、基础逻辑</h4><p></p><p></p><p>Paxos算法是指一个或多个提案者针对某项业务提出提案，并发送提案给投票者，由投票者投票并最终达成共识的算法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8bd822a5843a79edb90089b642b9b267.jpeg\" /></p><p></p><p>“达成共识”过程的特点：</p><p>（1）、可以由一个或多个提案者参与；</p><p>（2）、由多个投票者参与；</p><p>（3）、可以发起一轮或多轮投票；</p><p>（4）、最终的共识结果是一个值，且该值为提案者提出的其中某个值。</p><p></p><h2>二、Basic Paxos</h2><p></p><p></p><h4>1、两个阶段</h4><p></p><p></p><p>Basic Paxos算法分为两个阶段：Prepare阶段和Accept阶段。</p><p></p><p>(1). Prepare阶段</p><p></p><p>该阶段又分为两个环节：</p><p></p><p>a、Proposer发起广播消息给集群中的Acceptor发送一个提案编号为n的prepare提案请求。b、Acceptor收到提案编号为n的prepare提案请求，则进行以下判断：如果该Acceptor之前接受的prepare请求编号都小于n或者之前没有接受过prepare请求，那么它会响应接受该编号为n的prepare请求并承诺不再接受编号小于n的Accept请求，Acceptor向Proposer的响应数据包含三部分内容：接受编号n的提案状态信息，之前接受过的最大提案编号和相应提案值；如果该Acceptor之前接受过至少一个编号大于n的prepare请求，则会拒绝该次prepare请求。</p><p></p><p>通过以上prepare阶段处理流程可以知道：</p><p></p><p>a、prepare请求发送时只包含提案编号，不包含提案值；b、集群中的每个Acceptor会存储自己当前已接受的最大提案编号和提案值。</p><p></p><p>假设分布式环境中有一个Proposer和三个Acceptor，且三个Acceptor都没有收到过Prepare请求，Prepare阶段示意图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e6075f7f28bab0256308c9b6a102f341.jpeg\" /></p><p></p><p>假设分布式环境中有两个Proposer和三个Acceptor，ProposerB成功发送prepare请求，在发送Accept请求时出现故障宕机，只成功给Acceptor1发送了accept请求并得到响应。当前各个Acceptor的状态分别为：Acceptor1，同意了ProposerB发送的提案编号2的Accept请求，当前提案值为：orange；Acceptor2，接受了ProposerB发送的提案编号2的Prepare请求；Acceptor3，接受了ProposerB发送的提案编号2的Prepare请求；此时ProposerA发起Prepare请求示意图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3a1993ca4fb8965cb0080937620547ad.jpeg\" /></p><p></p><p>流程说明：</p><p></p><p>a、ProposerA发起prepare(1)的请求，由于该编号小于提案编号2，所以请求被拒绝；b、ProposerA发起prepare(3)的请求，该编号大于编号2，则被接受，Accetpor1返回Promised(3,2,'orange')，表示接受编号3的提案请求，并将之前接受过的最大编号提案和提案值返回。c、Acceptor2和Acceptor3均返回Promised(3)，表示接受编号3的提案请求。</p><p></p><p>(2). Accept阶段</p><p></p><p>如果Proposer接收到了超过半数节点的Prepare请求的响应数据，则发送accept广播消息给Acceptor。如果Proposer在限定时间内没有接收到超过半数的Prepare请求响应数据，则会等待指定时间后再重新发起Prepare请求。</p><p></p><p>Proposer发送的accept广播请求包含什么内容：</p><p></p><p>a、accept请求包含相应的提案号；b、accept请求包含对应的提案值。如果Proposer接收到的prepare响应数据中包含Acceptor之前已同意的提案号和提案值，则选择最大提案号对应的提案值作为当前accept请求的提案值，这种设计的目的是为了能够更快的达成共识。而如果prepare返回数据中的提案值均为空，则自己生成一个提案值。</p><p></p><p>Acceptor接收到accept消息后的处理流程如下：</p><p></p><p>a、判断accept消息中的提案编号是否小于之前已同意的最大提案编号，如果小于则抛弃，否则同意该请求，并更新自己存储的提案编号和提案值。b、Acceptor同意该提案后发送响应accepted消息给Proposer，并同时发送accepted消息给Learner。Learner判断各个Acceptor的提案结果，如果提案结果已超过半数同意，则将结果同步给集群中的所有Proposer、Acceptor和所有Learner节点，并结束当前提案。当Acceptor之前没有接受过Prepare请求时的响应流程图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d735f96927c1b200d762a663321ac2d.jpeg\" /></p><p></p><p>当Acceptor之前已存在接受过的Prepare和Accept请求时的响应流程图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4d2780de523cb58887229dbd41d1fb3.jpeg\" /></p><p></p><p>该示例中prepare请求返回数据中已经包含有之前的提案值（1,'apple'）和（2,'banana'），Proposer选择之前最大提案编号的提案值作为当前的提案值。</p><p></p><h4>2、关于提案编号和提案值</h4><p></p><p></p><p>提案编号</p><p></p><p>在Paxos算法中并不自己生成提案编号，提案编号是由外部定义并传入到Paxos算法中的。我们可以根据使用场景按照自身业务需求，自定义提案编号的生成逻辑。提案编号只要符合是“不断增加的数值型数值”的条件即可。比如：在只有一个Proposer的环境中，可以使用自增ID或时间戳作为提案编号；在两个Proposer的环境中，一个Proposer可以使用1、3、5、7...作为其编号，另一个Proposer可以使用2、4、6、8...作为其提案编号；在多Proposer的环境中，可以为每个节点预分配固定ServerId(ServerId可为1、2、3、4...)，使用自增序号 + '.'  + ServerId或timestamp + '.' + ServerId的格式作为提案编号，比如：1.1、1.2、2.3、3.1、3.2或1693702932000.1、1693702932000.2、1693702932000.3；每个Proposer在发起Prepare请求后如果没有得到超半数响应时，会更新自己的提案号，再重新发起新一轮的Prepare请求。</p><p></p><p>提案值</p><p></p><p>提案值的定义也完全是根据自身的业务需求定义的。在实际应用场景中，提案值可以是具体的数值、字符串或是cmd命令或运算函数等任何形式，比如在分布式数据库的设计中，我们可以将数据的写入操作、修改操作和删除操作等作为提案值。</p><p></p><h4>3、最终值的选择</h4><p></p><p></p><p>Acceptor每次同意新的提案值都会将消息同步给Learner，Learner根据各个Acceptor的反馈判断当前是否已超过半数同意，如果达成共识则发送广播消息给所有Acceptor和Proposer并结束提案。在实际业务场景中，Learner可能由多个节点组成，每个Learner都需要“学习”到最新的投票结果。关于Learner的实现，Lamport在其论文中给出了下面两种实现方式：</p><p></p><p>（1）、选择一个Learner作为主节点用于接收投票结果（即accepted消息），其他Learner节点作为备份节点，Learner主节点接收到数据后再同步给其他Learner节点。该方案缺点：会出现单点问题，如果这个主节点挂掉，则不能获取到投票结果。</p><p></p><p>（2）、Acceptor同意提案后，将投票结果同步给所有的Learner节点，每个Learner节点再将结果广播给其他的Learner节点，这样可以避免单点问题。不过由于这种方案涉及很多次的消息传递，所以效率要低于上述的方案。</p><p></p><h2>三、活锁问题</h2><p></p><p></p><h4>1、什么是活锁?</h4><p></p><p></p><p>“活锁”指的是任务由于某些条件没有被满足，导致一直重复尝试，失败，然后再次尝试的过程。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，而处于死锁的实体表现为等待（阻塞）；活锁有可能自行解开，而死锁不能。</p><p></p><h4>2、为什么Basic-Paxos可能会出现活锁?</h4><p></p><p></p><p>由于Proposer每次发起prepare请求都会更新编号，那么就有可能出现这种情况，即每个Proposer在被拒绝时，增大自己的编号重新发起提案，然后每个Proposer在新的编号下不能达成共识，又重新增大编号再次发起提案，一直这样循环往复，就成了活锁。活锁现象就是指多个Proposer之间形成僵持，在某个时间段内循环发起preapre请求，进入Accept阶段但不能达成共识，然后再循环这一过程的现象。活锁现象示例图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6d900a7ce063ef2b3f738fb88ce34328.jpeg\" /></p><p></p><h4>3、活锁如何解决？</h4><p></p><p></p><p>活锁会导致多个Proposer在某个时间段内一直不断地发起投票，但不能达成共识，造成较长时间不能获取到共识结果。活锁有可能自行解开，但该过程的持续时间可长可短并不确定，这与具体的业务场景实现逻辑、网络状况、提案重新发起时间间隔等多方面因素有关。</p><p></p><p>解决活锁问题，有以下两种常见的方法：</p><p>（1）、当Proposer接收到响应，发现支持它的Acceptor小于半数时，不立即更新编号发起重试，而是随机延迟一小段时间，来错开彼此的冲突。</p><p>（2）、可以设置一个Proposer的Leader，集群全部由它来进行提案，等同于下文的Multi-Paxos算法。</p><p></p><h2>四、Multi-Paxos</h2><p></p><p></p><p>上文阐述了Paxos算法的基础运算流程，但我们发现存在两个问题：（1）、集群内所有Proposer都可以发起提案所以Basic Paxos算法有可能导致活锁现象的发生；（2）、每次发起提案都需要经过反复的Prepare和Accept流程，需要经过很多次的网络交互，影响程序的执行效率。</p><p></p><p>考虑到以上两个问题，能不能在保障分布式一致性的前提下可以避免活锁情况的发生，以及尽可能减少达成共识过程中的网络交互，基于这种目的随即产生了Multi-Paxos算法。</p><p></p><p>首先我们可以设想一下：在多个Proposer的环境中最理想的达成共识的交互过程是什么样子的？就是这样一种情况：集群中的某个Proposer发送一次广播prepare请求并获得超半数响应，然后再发送一次广播accept请求，并获得超过半数的同意后即达成共识。但现实中多个Proposer很可能会互相交错的发送消息，彼此之间产生冲突，而且在不稳定的网络环境中消息发送可能会延迟或丢失，这种情况下就需要再次发起提案，影响了执行效率。Multi-Paxos算法就是为了解决这个问题而出现。</p><p></p><p>Multi-Paxos算法是为了在保障集群所有节点平等的前提下，依然有主次之分，减少不必要的网络交互流程。Multi-Paxos算法是通过选举出一个Proposer主节点来规避上述问题，集群中的各个Proposer通过心跳包的形式定期监测集群中的Proposer主节点是否存在。当发现集群中主节点不存在时，便会向集群中的Acceptors发出申请表示自己想成为集群Proposer主节点。而当该请求得到了集群中的大多数节点的同意后随即该Proposer成为主节点。</p><p></p><p>集群中存在Proposer主节点时，集群内的提案只有主节点可以提出，其他Proposer不再发起提案，则避免了活锁问题。由于集群中只有一个节点可以发起提案，不存在冲突的可能，所以不必再发送prepare请求，而只需要发送accept请求即可，因此减少了协商网络交互次数。</p><p></p><h2>五、Paxos应用场景示例</h2><p></p><p></p><p>上文对Paxos算法的处理流程就行了阐述，为了加深理解，下面以一个分布式数据库的使用案例来阐述Paxos算法在实际业务场景中的使用。</p><p></p><p>场景描述：分布式数据库中假设包含3个节点，客户端访问时通过轮询或随机访问的方式请求到其中的某个节点，我们要通过Paxos算法保证分布式数据库的3个节点中数据的一致性。实际的分布式数据一致性流程更为复杂，我这里为了方便阐述将这个过程进行一些简化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c2e9ab23312693a0a06d23a7e7c4c2ba.jpeg\" /></p><p></p><p>分布式数据库中的每个节点都存储三份数据，一是事务日志数据，二是DB数据，三是事务日志执行位置。事务日志表存储着数据库的操作日志记录，包括：写入Put、修改Update和删除Delete等相关的操作日志，有些文章资料将事务日志表称为状态机其实是一个意思。DB数据表存储具体的业务数据。事务日志执行位置用于记录当前节点执行到了哪一条操作记录。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f9df260556436771a1010bf02db49325.jpeg\" /></p><p></p><p>整体设计思想：我们只要通过Paxos算法保证各个节点事务日志表数据一致就可以保证节点数据的一致性。</p><p></p><p>假设，当前各个节点的事务日志表和数据表均为空，现在客户端1对数据库发起写入操作请求：{'Op1','Put(a,'1')'}，这里的Op1代表操作的ID(为了简单起见，直接使用自增ID表示，该数值对应Paxos算法中的提案编号)，Put(a,'1')代表操作内容，对应Paxos中的提案值，假设该请求被随机分配到了Server1处理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8d/8da073be6e5c1bd87fb1a9c5ec7ae14c.jpeg\" /></p><p></p><p>流程说明：</p><p>1、Server1接受到Put(a,'1')请求，并不是直接写入数据表，而是首先通过Paxos算法判断集群节点是否达成写入共识。</p><p>2、当前三个节点的OperateIndex均为0，事务日志表和数据表均为空，Server1的Proposer首先向三个节点发起Prepare(OperateIndex + 1)，即Prepare(1)请求。</p><p>3、接收到过半数的Prepare请求反馈后，发送Accept(1,'Put(a,'1')')请求，并得到Accepted请求反馈，则此时三个节点达成共识，当前三个节点的事务日志表均为：{'Op1','Put(a,'1')'}，数据表均为空。</p><p>4、达成共识后，Server1执行写入操作并更新当前节点的OperateIndex，此时Server1的OperateIndex为1，其他节点仍为0，Server1的数据表为：a = 1，另外两个节点为空，三个节点的事务日志表相同，当前写入流程结束。</p><p></p><p>假设，此时Server2节点接收到Put(b,'1')的请求，处理流程如下：</p><p>1、Server2接收到Put(b,'1')请求，由于当前Server2的OperateIndex仍为0，则首先发起Prepare(1)的请求。</p><p>2、由于当前三个节点的Acceptor的提案编号均为1，所以会拒绝Server2的Prepare(1)请求。</p><p>3、Server2未能得到超过半数的prepare响应，则会查看当前事务日志表发现已存在Op1操作，则从当前节点的事务日志表中取出相应操作并执行，然后将当前节点OperateIndex修改为1。</p><p>4、Server2随即再次发起Prepare(OperateIndex+1)，即Prepare(2)的请求。</p><p>5、此时三个节点达成共识，并更新各自的事务日志表。</p><p>6、Server2执行写入操作，此时Server1节点状态为OperateIndex：1，数据表：a=1；Server2节点状态为OperateIndex:2, 数据表：a=1和b=1；Server3的节点状态为OperateIndex：0，数据表为空；三个节点的事务日志表相同，均为：{'Op1','Put(a,'1')'}；{'Op2','Put(b,'1')'}。当前流程执行结束。</p><p></p><p>假设，此时Server3接收到Get(a)请求，处理流程如下：1、Server3接收到Get(a)请求，并不是直接查询数据表然后返回，而是要将当前节点的OperateIndex和事务日志表中的记录进行比对，如果发现有遗漏操作，则按照事务日志表的顺序执行遗漏操作后再返回。由于Get请求并不涉及对数据的写入和修改，所以理论上不需要再次发起Paxos协商。2、此时Server1节点的状态为OperateIndex：1，数据表：a=1；Server2的节点状态为OperateIndex:2,  数据表：a=1和b=1；Server3的节点状态为OperateIndex：2，数据表为a=1和b=1；三个节点的事务日志表相同，均为：{'Op1','Put(a,'1')'}；{'Op2','Put(b,'1')'}。当前流程执行结束。</p><p></p><p>执行流程示意图如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e2527823fb39bcf0d8e01ca3a922358.jpeg\" /></p><p></p><p>随着数据的不断写入，事务日志表的数据量不断增加，可以通过快照的方式，将某个时间点之前的数据备份到磁盘（注意此处备份的是数据表数据，不是事务日志数据，这样宕机恢复时直接从快照点开始恢复，可以提高恢复效率，如果备份事务日志数据，宕机恢复时需要从第一条日志开始恢复，导致恢复时间会比较长），然后将事务日志表快照前的数据清除即可。</p><p></p><h2>六、对一些问题的解释</h2><p></p><p></p><h4>1、投票过程为什么要遵循选择最大提案号的原则？</h4><p></p><p></p><p>Paxos投票虽然叫作“投票”，但其实与我们现实中的“投票”有很大的区别，因为它的运算过程中并不关心提案内容本身，而完全依据哪个提案号大就选择哪个的原则，因为只有这样才能达成共识。</p><p></p><h4>2、为什么Proposer每次发起prepare都要变更提案号？</h4><p></p><p></p><p>这个问题其实很容易理解，也是为了达成共识。假设ProposerA、ProposerB、ProposerC分别同时发起了prepare(1)、prepare(2)、prepare(3)的提案，而此时ProposerC出现故障宕机，如果ProposerA、ProposerB在后续的每一轮投票中都不变更提案号，那永远都不可能达成共识。</p><p></p><h4>3、为什么Paxos算法可以避免脑裂问题？</h4><p></p><p></p><p>Paxos算法可以避免分布式集群出现脑裂问题，首先我们需要知道什么是分布式集群的脑裂问题。脑裂是指集群出现了多个Master主节点，由于分布式集群的节点可能归属于不同的网络分区，如果网络分区之间出现网络故障，则会造成不同分区之间的节点不能互相通信。而此时采用传统的方案很容易在不同分区分别选出相应的主节点。这就造成了一个集群中出现了多个Master节点即为脑裂。而Paxos算法是必须达到半数同意才能达成共识，这就意味着如果分区内的节点数量少于一半，则不可能选出主节点，从而避免了脑裂状况的发生。</p><p></p><h2>七、开发、运维超实用工具推荐</h2><p></p><p></p><p>接下来向大家推荐一款对日常开发和运维，极具有实用价值的好帮手XL-LightHouse。</p><p></p><p>一键部署，一行代码接入，无需大数据相关研发运维经验就可以轻松实现海量数据实时统计，使用XL-LightHouse后：</p><p></p><p>再也不需要用Flink、Spark、ClickHouse或者基于Redis这种臃肿笨重的方案跑数了；再也不需要疲于应付对个人价值提升没有多大益处的数据统计需求了，能够帮助您从琐碎反复的数据统计需求中抽身出来，从而专注于对个人提升、对企业发展更有价值的事情；轻松帮您实现任意细粒度的监控指标，是您监控服务运行状况，排查各类业务数据波动、指标异常类问题的好帮手；培养数据思维，辅助您将所从事的工作建立数据指标体系，量化工作产出，做专业严谨的职场人，创造更大的个人价值；</p><p></p><p>XL-LightHouse简介</p><p></p><p>XL-LightHouse是针对互联网领域繁杂的流式数据统计需求而开发的一套集成了数据写入、数据运算、数据存储和数据可视化等一系列功能，支持超大数据量，支持超高并发的【通用型流式大数据统计平台】。XL-LightHouse目前已涵盖了常见的流式数据统计场景，包括count、sum、max、min、avg、distinct、topN/lastN等多种运算，支持多维度计算，支持分钟级、小时级、天级多个时间粒度的统计，支持自定义统计周期的配置。XL-LightHouse内置丰富的转化类函数、支持表达式解析，可以满足各种复杂的条件筛选和逻辑判断。XL-LightHouse是一套功能完备的流式大数据统计领域的数据治理解决方案，它提供了比较友好和完善的可视化查询功能，并对外提供API查询接口，此外还包括数据指标管理、权限管理、统计限流等多种功能。XL-LightHouse支持时序性数据的存储和查询。GitHub搜索XL-LightHouse了解更多！</p><p></p><p>如本文有所疏漏或您有任何疑问，欢迎访问dtstep.com与我本人沟通交流！</p><p></p><h4>作者介绍</h4><p></p><p></p><p>xl-xueling，开源项目通用型流式大数据统计系统xl-lighthouse作者，曾任职多家国内大型互联网企业，个人比较熟悉和擅长大型分布式系统的架构设计、擅长企业大数据数仓平台建设，对企业数据化运营转型有较为深入的研究和实施经验。</p>",
    "publish_time": "2023-09-13 15:20:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蓝色光标发布营销行业模型“Blue AI”，已接入微软云、百度、智谱AI等生态体系",
    "url": "https://www.infoq.cn/article/VSNsDCxZH11SrKKMmaCY",
    "summary": "<p></p><p>9月12日，蓝色光标的举办Blue AI 行业模型发布会在京召开。</p><p>&nbsp;</p><p>“Blue AI 行业模型的发布，标志着蓝色光标Al²战略迈入新阶段”。蓝色光标集团CEO潘飞在发布会现场表示，把握好AI机会，会让蓝色光标更加接近于一家全球化的科技公司。今后，我们将继续沿着三个方面迭代与迈进：一是生产效率与方式的改变，全面赋能我们的业务，这一点我们已经取得阶段性成果，基本实现员工渗透率100%，业务使用率70%。二是创造新模式和新能力：包括但不限于AI 原生内容大比率提升，行业模型易用性和领先性，新的生成式广告形态。三是打造AI时代的超级个体与组织，用新范式重构蓝色光标的方方面面。</p><p>&nbsp;</p><p>据悉，作为蓝色光标AI²战略的整体落地框架，Blue AI现已接入微软云、百度、智谱AI底层模型作为技术支撑和底座。</p><p></p><h2>以专业驱动行业模型Blue AI加速人机协同</h2><p></p><p>生成式AI浪潮下，新一轮科技革命和产业变革深入发展，营销变革加速演进，蓝色光标快速布局，发布AI²战略，全力拥抱AIGC，打造AI+内容的新型生产方式。上半年，蓝色光标70%以上客户已使用生成式AI相关服务，深度使用并带来一定收入的客户约20%，整体业务提效约30%。</p><p></p><p>如今，在AI²战略下，蓝色光标依托深厚的营销积累及内容沉淀，正式推出营销行业模型Blue AI，实现了AI时代的快速起跑。</p><p>&nbsp;</p><p>“Blue AI的建设方向是以专业驱动AI，深度应用大模型的能力，将传统作业过程中人的专业性，逐步转化为人机协作的相互迭代过程。”Blue AI行业模型负责人李林波在发布会现场介绍，“Blue AI聚焦内容生成、思维助手、体验创新三大主要垂直场景，实现了三个融合：融合大模型与私有数据调度，让特定场景数据融入大模型，融合全球受众分析与个性化社媒创作，融合AI创意生成与创意场景衍生，实现特定人机协作方式融入大模型。”</p><p></p><p>值得一提的是，作为介于通用大模型基础层和更具体的AI应用服务层之间的行业模型，Blue&nbsp;AI已经接入多家TOP合作伙伴，建立了较为完善的行业生态体系。</p><p></p><p>在LLM底层，蓝色光标与百度、智谱华章、微软（中国）等展开专项共创合作，探索大模型与营销行业的结合点；在MaaS层，蓝色光标与阿里云、腾讯云、火山云、微软云等合作，实践AI应用的部署、交付和资源支撑。依托MaaS生态伙伴，Blue&nbsp;AI将实现更好的垂直需求理解、更好的知识搜索及运用、更好的归纳输出及反馈、更好的模式化创作。</p><p></p><p>“行业模型距离客户更近，像蓝色光标这样拥有行业专业知识的企业，在大模型加持下，将迎来巨大机遇。”发布会现场，百度集团副总裁袁佛玉表示，“大模型时代更需要生态，更需要合作和分工，共同服务客户，为客户创造价值。百度致力于推动千行百业的AI普惠，愿意为生态伙伴提供坚实的技术支撑，我相信在Blue AI的赋能下，蓝色光标会持续迭代和发展，实现AI时代下真正的改变。很期待在各行各业都有像蓝色光标这样的企业，在AI领域进行更加深入的探索和实践，推动整个行业的发展和进步。”</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/45/5a/45f2b847ff2c7f06eyy3ebfe0fd5a45a.jpg\" /></p><p>蓝色光标集团董事长&nbsp; 赵文权</p><p></p><p>蓝色光标集团董事长赵文权表示，“站在生成式AI爆发的临界点，AI服务能力已经深入到更广阔的实体经济，将推动科技创新、产业升级和生产力跃迁。拥抱AI技术变革带来的红利，不仅会把蓝色光标带到新的阶段，更将实现营销行业商业模式重构，进入新的产业格局。”</p>",
    "publish_time": "2023-09-13 16:39:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "为什么说 eBPF 是实现可观测性的关键技术？",
    "url": "https://www.infoq.cn/article/jlHouGG11TG87Xqz8BoQ",
    "summary": "<p>控制理论中的可观测性是指：系统可以由其外部输出确定其内部状态的程度。在复杂 IT 系统中，具备可观测性是为了让系统能达到某个预定的稳定性、错误率目标。随着微服务数量的急速膨胀和云原生基础设施的快速演进，建设可观测性已经成为了保障业务稳定性的必要条件。</p><p></p><p>然而，传统的 APM 无法实现真正的可观测性：一方面插桩行为已经修改了原程序，逻辑上已无法实现原程序的可观测性；另一方面云原生基础设施组件越来越多，基础服务难以插桩导致观测盲点越来越多。实际上，插桩的方式在金融、电信等重要行业的核心业务系统中几乎无法落地。eBPF 由于其零侵扰的优势，避免了 APM 插桩的缺点，是云原生时代实现可观测性的关键技术。</p><p></p><p>本文依次论述 APM 无法实现真正可观测性的原因，分析为什么 eBPF 是可观测性的关键技术，介绍 DeepFlow 基于 eBPF 的三大核心功能，并进一步阐述如何向 eBPF 的观测数据中注入业务语义。在此之后，本文分享了 DeepFlow 用户的九大类真实使用案例，总结了用户在采用 eBPF 技术前的常见疑问。最后，本文进一步分析了 eBPF 对新技术迭代的重大意义。同时，欢迎大家报名参与 9月16日DeepFlow开展的线下活动《可观测性 Meetup》。</p><p></p><h2>使用 APM 无法实现真正的可观测性</h2><p></p><p></p><p>APM 希望通过代码插桩（Instrumentation）的方式来实现应用程序的可观测性。利用插桩，应用程序可以暴露非常丰富的观测信号，包括指标、追踪、日志、函数性能剖析等。然而插桩的行为实际上改变了原始程序的内部状态，从逻辑上并不符合可观测性「从外部数据确定内部状态」的要求。在金融、电信等重要行业的核心业务系统中，APM Agent 落地非常困难。进入到云原生时代，这个传统方法也面临着更加严峻的挑战。总的来讲，APM 的问题主要体现在两个方面：Agent 的侵扰性导致难以落地，观测盲点导致无法定界。</p><p></p><p>第一，探针侵扰性导致难以落地。插桩的过程需要对应用程序的源代码进行修改，重新发布上线。即使例如 Java Agent 这类字节码增强技术，也需要修改应用程序的启动参数并重新发版。然而，对应用代码的改造还只是第一道关卡，通常落地过程中还会碰到很多其他方面的问题：</p><p></p><p>代码冲突：当你为了分布式追踪、性能剖析、日志甚至服务网格等目的注入了多个 Java Agent 时，是否经常遇到不同 Agent 之间产生的运行时冲突？当你引入一个可观测性的 SDK 时，是否遇到过依赖库版本冲突导致无法编译成功？业务团队数量越多时，这类兼容性问题的爆发会越为明显。维护困难：如果你负责维护公司的 Java Agent 或 SDK，你的更新频率能有多高？就在此时，你们公司的生产环境中有多少个版本的探针程序？让他们更新到同一个版本需要花多长时间？你需要同时维护多少种语言的探针程序？当企业的微服务框架、RPC 框架无法统一时，这类维护问题还将会更加严重。边界模糊：所有的插桩代码严丝合缝的进入了业务代码的运行逻辑中，不分你我、不受控制。这导致当出现性能衰减或运行错误时，插桩代码往往难辞其咎。即使探针已经经过了长时间的实战打磨，遇到问题时也免不了要求排除嫌疑。</p><p></p><p>实际上，这也是为什么侵扰性的插桩方案少见于成功的商业产品，更多见于活跃的开源社区。OpenTelemetry、SkyWalking 等社区的活跃正是佐证。而在部门分工明确的大型企业中，克服协作上的困难是一个技术方案能够成功落地永远也绕不开的坎。特别是在金融、电信、电力等承载国计民生的关键行业中，部门之间的职责区分和利益冲突往往会使得落地插桩式的解决方案成为「不可能」。即使是在开放协作的互联网企业中，也少不了开发人员对插桩的不情愿、运维人员在出现性能故障时的背锅等问题。在经历了长久的努力之后人们已经发现，侵入性的解决方案仅仅适合于每个业务开发团队自己主动引入、自己维护各类 Agent 和 SDK 的版本、自己对性能隐患和运行故障的风险负责。当然，我们也看到了一些得益于基建高度统一而取得成功的大型互联网公司案例，例如 Google 就在 2010 年的 Dapper 论文中坦言：</p><p></p><p></p><blockquote>True application-level transparency, possibly our most challenging design goal, was achieved by restricting Dapper’s core tracing instrumentation to a small corpus of ubiquitous threading, control flow, and RPC library code.</blockquote><p></p><p></p><p>再例如字节跳动在 2022 年的对外分享《分布式链路追踪在字节跳动的实践》中也表示：</p><p></p><p></p><blockquote>得益于长期的统一基建工作，字节全公司范围内的所有微服务使用的底层技术方案统一度较高。绝大部分微服务都部署在公司统一的容器平台上，采用统一的公司微服务框架和网格方案，使用公司统一提供的存储组件及相应 SDK。高度的一致性对于基础架构团队建设公司级别的统一链路追踪系统提供了有利的基础。</blockquote><p></p><p></p><p>第二，观测盲点导致无法定界。即使 APM 已经在企业内落地，我们还是会发现排障边界依然难以界定，特别是在云原生基础设施中。这是因为开发和运维往往使用不同的语言在对话，例如当调用时延过高时开发会怀疑网络慢、网关慢、数据库慢、服务端慢，但由于全栈可观测性的缺乏，网络、网关、数据库给出的应答通常是网卡没丢包、进程 CPU 不高、DB 没有慢日志、服务端时延很低等一大堆毫无关联的指标，仍然解决不了问题。定界是整个故障处理流程中最关键的一环，它的效率至关重要。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb4cbe476b66049796429ed998085685.png\" /></p><p></p><p>这里我们想澄清两个概念：排障边界和职责边界。虽然开发的职责边界是应用程序本身，但排障边界却需要延展到网络传输上。举个例子：微服务在请求 RDS 云服务时偶现高达 200ms 的时延，如果开发以此为依据向云服务商提交工单，得到的应答大概率会是「RDS 没有观察到慢日志，请自查」。我们在很多客户处碰到了大量此类案例，根因有的是 RDS 前的 SLB 导致、有的是 K8s Node 的 SNAT 导致，背后的原因千奇百怪，但若不能在第一时间完成故障定界，都会导致租户（开发）和云服务商（基础设施）之间长达数天乃至数周的工单拉锯战。从排障边界的角度来讲，若开发能给出「网卡发送请求到收到响应之间的时延高达 200ms」，就能快速完成定界，推动云服务商排查。找对了正确的人，之后的问题解决一般都非常快。我们在后文也会分享几个真实案例。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4aade9ed1fc9d7ad6070e1d7d0226bb.png\" /></p><p></p><p>上图中我们对不同场景下的排障边界进行了总结：如果你是一个业务开发工程师，除了业务本身以外，还应该关心系统调用和网络传输过程；如果你是一个 Serverless 租户，你可能还需要关注服务网格边车及其网络传输；如果你直接使用虚拟机或自建 K8s 集群，那么容器网络是需要重点关注的问题点，特别还需注意 K8s 中的 CoreDNS、Ingress Gateway 等基础服务；如果你是私有云的计算服务管理员，应该关心 KVM 宿主机上的网络性能；如果你是私有云的网关、存储、安全团队，也需要关注服务节点上的系统调用和网络传输性能。</p><p></p><p>实际上更为重要的是，用于故障定界的数据应该使用类似的语言进行陈述：一次应用调用在整个全栈路径中，每一跳到底消耗了多长时间。通过上述分析我们发现，开发者通过插桩提供的观测数据，可能只占了整个全栈路径的 1/4。在云原生时代，单纯依靠 APM 来解决故障定界，本身就是妄念。</p><p></p><h2>eBPF 是可观测性的关键技术</h2><p></p><p></p><p>本文假设你对 eBPF 有了基础的了解，它是一项安全、高效的通过在沙箱中运行程序以实现内核功能扩展的技术，是对传统的修改内核源代码和编写内核模块方式的革命性创新。你可访问 ebpf.io 以了解更多的 eBPF 相关知识，本文聚焦于讨论 eBPF 对云原生应用可观测性的革命性意义。</p><p></p><p>eBPF 程序是事件驱动的，当内核或用户程序经过一个 eBPF Hook 时，对应 Hook 点上加载的 eBPF 程序就会被执行。Linux 内核中预定义了一系列常用的 Hook 点，你也可以利用 kprobe 和 uprobe 技术动态增加内核和应用程序的自定义 Hook 点。得益于 Just-in-Time (JIT) 技术，eBPF 代码的运行效率可媲美内核原生代码和内核模块。得益于 Verification 机制，eBPF 代码将会安全的运行，不会导致内核崩溃或进入死循环。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d9378fb4f0333276d80599479308c47.png\" /></p><p></p><p>回到可观测性上，沙箱机制是 eBPF 有别于 APM 插桩机制的核心所在，「沙箱」在 eBPF 代码和应用程序的代码之间划上了一道清晰的界限，使得我们能在不对应用程序做任何修改的前提下，通过获取外部数据就能确定其内部状态。下面我们来详细分析下为何 eBPF 是解决 APM 代码插桩缺陷的绝佳解决方案：</p><p></p><p>第一，零侵扰解决落地难的问题。由于 eBPF 程序无需修改应用程序代码，因此不会有类似 Java Agent 的运行时冲突和 SDK 的编译时冲突，解决了代码冲突问题；由于运行 eBPF 程序无需改变和重启应用进程，不需要应用程序重新发版，不会有 Java Agent 和 SDK 的版本维护痛苦，解决了维护困难问题；由于 eBPF 在 JIT 技术和 Verification 机制的保障下高效安全的运行，因此不用担心会引发应用进程预期之外的性能衰减或运行时错误，解决了边界模糊问题。另外从管理层面，由于只需要在每个主机上运行一个独立的 eBPF Agent 进程，使得我们可以对它的 CPU 等资源消耗进行单独的、精确的控制。</p><p></p><p>第二，全栈能力解决故障定界难的问题。eBPF 的能力覆盖了从内核到用户程序的每一个层面，因此我们得以跟踪一个请求从应用程序出发，经过系统调用、网络传输、网关服务、安全服务，到达数据库服务或对端微服务的全栈路径，提供充足的中立观测数据，快速完成故障的定界。</p><p></p><p>然而，eBPF 并不是一个易于掌握的技术，它需要开发者有一定的内核编程基础，它获取的原始数据缺乏结构化信息。下文将会以我们的产品 DeepFlow 为例，介绍如何扫清这些障碍，充分发挥 eBPF 对可观测性工程的关键作用。</p><p></p><h2>DeepFlow 基于 eBPF 的三大核心功能</h2><p></p><p></p><p>DeepFlow [<a href=\"https://github.com/deepflowio/deepflow\">GitHub</a>\"] 旨在为复杂的云原生应用提供简单可落地的深度可观测性。DeepFlow 基于 eBPF 和 Wasm 技术实现了零侵扰（Zero Code）、全栈（Full Stack）的指标、追踪、调用日志、函数剖析数据采集，并通过智能标签技术实现了所有数据的全关联（Universal Tagging）和高效存取。使用 DeepFlow，可以让云原生应用自动具有深度可观测性，从而消除开发者不断插桩的沉重负担，并为 DevOps/SRE 团队提供从代码到基础设施的监控及诊断能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c76a2ec9884666bb5236850a693cd3db.png\" /></p><p></p><p>通过利用 eBPF 和 cBPF 采集应用函数、系统调用函数、网卡收发的数据，DeepFlow 首先聚合成 TCP/UDP 流日志（Flow Log）；通过应用协议识别，DeepFlow 聚合得到应用调用日志（Request Log），进而计算出全栈的 RED（Request/Error/Delay）性能指标，并关联调用日志实现分布式追踪。除此之外，DeepFlow 在流日志聚合过程中还计算了 TCP 吞吐、时延、建连异常、重传、零窗等网络层性能指标，以及通过 Hook 文件读写操作计算了 IO 吞吐和时延指标，并将所有这些指标关联至每个调用日志上。另外，DeepFlow 也支持通过 eBPF 获取每个进程的 OnCPU、OffCPU 函数火焰图，以及分析 TCP 包绘制 Network Profile 时序图。所有这些能力最终体现为三大核心功能：</p><p></p><p>Universal Map for Any Service，任意服务的全景图Distributed Tracing for Any Request，任意调用的分布式追踪Continuous Profiling for Any Function，任何函数的持续性能剖析</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/5675d5a0d34754478083255b33723657.png\" /></p><p></p><p>核心功能一：任意服务的全景图。全景图直接体现出了 eBPF 零侵扰的优势，对比 APM 有限的覆盖能力，所有的服务都能出现在全景图中。但 eBPF 获取的调用日志不能直接用于拓扑展现，DeepFlow 为所有的数据注入了丰富的标签，包括云资源属性、K8s 资源属性、自定义 K8s 标签等。通过这些标签可以快速过滤出指定业务的全景图，并且可以按不同标签分组展示，例如 K8s Pod、K8s Deployment、K8s Service、自定义标签等。全景图不仅描述了服务之间的调用关系，还展现了调用路径上的全栈性能指标，例如下图右侧为两个 K8s 服务的进程在相互访问时的逐跳时延变化。我们可以很快的发现性能瓶颈到底位于业务进程、容器网络、K8s 网络、KVM 网络还是 Underlay 网络。充足的中立观测数据是快速定界的必要条件。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/1181131c6ea88536a212b0959e43e7e6.png\" /></p><p></p><p>核心功能二：任意调用的分布式追踪。零侵扰的分布式追踪（AutoTracing）是 DeepFlow 中的一个重大创新，在通过 eBPF 和 cBPF 采集调用日志时，DeepFlow 基于系统调用上下文计算出了 syscall_trace_id、thread_id、goroutine_id、cap_seq、tcp_seq 等信息，无需修改应用代码、无需注入 TraceID、SpanID 即可实现分布式追踪。目前 DeepFlow 除了跨线程（通过内存 Queue 或 Channel 传递信息）和异步调用以外，都能实现零侵扰的分布式追踪。此外也支持解析应用注入的唯一 Request ID（例如几乎所有网关都会注入 X-Request-ID）来解决跨线程和异步的问题。下图对比了 DeepFlow 和 APM 的分布式追踪能力。APM 仅能对插桩的服务实现追踪，常见的是利用 Java Agent 覆盖 Java 服务。DeepFlow 使用 eBPF 实现了所有服务的追踪，包括 Nginx 等 SLB、Spring Cloud Gateway 等微服务网关、Envoy 等 Service Mesh 边车，以及 MySQL、Redis、CoreDNS 等基础服务（包括它们读写文件的耗时），除此之外还覆盖了 Pod NIC、Node NIC、KVM NIC、物理交换机等网络传输路径，更重要的是对 Java、Golang 以及所有语言都可无差别支持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/6179ae4a1ce13e23f8ae0fa1cbc66403.png\" /></p><p></p><p>注意 eBPF 和 APM 的分布式追踪能力并不是矛盾的。APM 能用于追踪应用进程内部的函数调用路径，也擅长于解决跨线程和异步场景。而 eBPF 有全局的覆盖能力，能轻松覆盖网关、基础服务、网络路径、多语言服务。在 DeepFlow 中，我们支持调用 APM 的 Trace API 以展示 APM + eBPF 的全链路分布式追踪图，同时也对外提供了&nbsp;Trace Completion API&nbsp;使得 APM 可调用 DeepFlow 以获取并关联 eBPF 的追踪数据。</p><p></p><p>核心功能三：任意函数的持续性能剖析。通过获取应用程序的函数调用栈快照，DeepFlow 可绘制任意进程的 CPU Profile，帮助开发者快速定位函数性能瓶颈。函数调用栈中除了包含业务函数以外，还可展现动态链接库、内核系统调用函数的耗时情况。除此之外，DeepFlow 在采集函数调用栈时生成了唯一标识，可用于与调用日志相关联，实现分布式追踪和函数性能剖析的联动。特别地，DeepFlow 还利用 cBPF 对网络中的逐包进行了分析，使得在低内核环境中可以绘制每个 TCP 流的 Network Profile，剖析其中的建连时延、系统（ACK）时延、服务响应时延、客户端等待时延。使用 Network Profile 可推断应用程序中性能瓶颈的代码范围，我们在后文中也会分享相关案例。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f94309403745c4a4d9e849cb314d2371.png\" /></p><p></p><p>本文无法完整的解释这些激动人心的特性背后的原理，DeepFlow 同时也是一个开源项目，您可以阅读我们的 GitHub 代码和文档了解更多信息，也可阅读我们发表在网络通信领域顶级会议&nbsp;ACM SIGCOMM 2023&nbsp;上的论文&nbsp;<a href=\"https://dl.acm.org/doi/10.1145/3603269.3604823\">Network-Centric Distributed Tracing with DeepFlow: Troubleshooting Your Microservices in Zero Code</a>\"。</p><p></p><h2>向 eBPF 观测数据中注入业务语义</h2><p></p><p></p><p>使用 APM Agent 的另一个诉求是向数据中注入业务语义，例如一个调用关联的用户信息、交易信息，以及服务所在的业务模块名称等。从 eBPF 采集到的原始字节流中很难用通用的方法提取业务语义，在 DeepFlow 中我们实现了两个插件机制来弥补这个不足：通过 Wasm Plugin 注入调用粒度的业务语义，通过 API 注入服务粒度的业务语义。</p><p></p><p>第一、通过 Wasm Plugin 注入调用粒度的业务语义：DeepFlow Agent 内置了常见应用协议的解析能力，且在持续迭代增加中，下图中蓝色部分均为原生支持的协议。我们发现实际业务环境中情况会更加复杂：开发会坚持返回 HTTP 200 同时将错误信息放到自定义 JSON 结构中，大量 RPC 的 Payload 部分使用 Protobuf、Thrift 等依赖 Schema 进行解码的序列化方式，调用的处理流程中发生了跨线程导致 eBPF AutoTracing 断链。为了解决这些问题 DeepFlow 提供了 Wasm Plugin 机制，支持开发者对 Pipeline 中的 ProtocolParser 进行增强。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1dfe75c6e95742e437724af8a5bc9770.png\" /></p><p></p><p>实际上，我们也观察到在金融、电信、游戏等行业中，已经存在了「天然」的分布式追踪标记，例如金融业务中的全局交易流水号，电信核心网中的呼叫 ID、游戏业务中的业务请求 ID 等等。这些 ID 会携带在所有调用中，但具体的位置是业务自身决定的。通过 Wasm Plugin 释放的灵活性，开发者可以很容易的编写插件支持将这些信息提取为 TraceID。</p><p></p><p>第二、通过 API 注入服务粒度的业务语义：默认情况下，DeepFlow 的 SmartEncoding 机制会自动为所有观测信号注入云资源、容器 K8s 资源、K8s 自定义 Label 标签。然而这些标签体现的只是应用层面的语义，为了帮助用户将 CMDB 等系统中的业务语义注入到观测数据中，DeepFlow 提供了一套用于业务标签注入的 API。</p><p></p><h2>DeepFlow 用户的实战案例</h2><p></p><p></p><p>在本章节中，我们将为大家分享 DeepFlow 用户的九大类实战案例。这些案例都是一些难以提前预料的疑难杂症，我们将会看到在仅有 APM 数据时，它们通常持续了数天甚至数周都还找不到方向，而依靠 eBPF 的能力往往能在 5 分钟之内完成故障定界。在开始介绍它们之前我还想澄清一下，这并不意味着 eBPF 的能力只擅长于解决疑难杂症，我们现在已经知道 eBPF 能够零侵扰的采集 Metrics、Request Logs、Profiles 等观测信号，DeepFlow 也已经基于这些信号实现了通用的全景图（包括性能指标、调用日志等）、分布式追踪、持续性能剖析功能。</p><p></p><p>第一类案例，快速定位引发问题的服务：</p><p></p><p>案例 1：5 分钟定位访问共享服务的 Top 客户端。MySQL、Redis、Consul 等基础设施通常被很多微服务共享使用，当它们的负载过高时通常很难判断是哪些客户端造成的。这是因为容器 Pod 访问这些共享服务时通常会做 SNAT，服务端看到的是容器节点的 IP；非容器环境下每个主机上也会有大量进程共享使用主机的 IP。可以想象从服务端的调用日志中分析 IP 地址是十分低效的，而我们也不能寄期望于所有客户端都注入了 APM Agent。使用 DeepFlow，我们的一个大型银行客户在 5 分钟内从近十万个 Pod 中快速定位了请求 RDS 集群最高频的微服务，我们的一个智能汽车客户在 5 分钟内从上万个 Pod 中快速定位了请求 Consul 最高频的微服务。案例 2：5 分钟定位被忽视的共享服务。DeepFlow 的一个大型银行客户在进行「分布式核心交易系统」上线测试时，发现由物理环境迁移到私有云上的交易系统性能非常差。经过了两周的排查、在注入了一大堆 APM Agent 以后，最终只能定位到问题位于名为&nbsp;cr****rs&nbsp;的服务访问授权交易服务&nbsp;au****in&nbsp;的链路上，但这两个服务在迁移上云之前没有任何性能问题。开发团队一度开始怀疑私有云基础设施，但没有任何数据支撑。毫无头绪时找到了 DeepFlow 团队，在部署 eBPF Agent 以后所有微服务之间的访问关系和性能指标全部呈现在了眼前，立即发现了在&nbsp;cr****rs&nbsp;访问授权交易服务&nbsp;au****in&nbsp;时，还会经过 Spring Cloud Gateway，而后者正在以极高的速率请求服务注册中心 Consul。至此问题明确了，这是由于网关的缓存配置不合理，导致服务注册中心成为了瓶颈。案例 3：5 分钟定位被忽视的背景压力。在软件开发过程中，压力测试环境通常由多人共享，甚至开发、测试等多个团队也会使用同一套压测环境。DeepFlow 的一个智能汽车客户的测试人员在压测某服务中，发现总是有少量的 HTTP 5XX 错误出现，而这将直接导致一次压测结果作废。正当测试人员一筹莫展时，打开 DeepFlow 全景图后马上发现还有其他服务正在以不可忽视的速率访问着被测服务。</p><p><img src=\"https://static001.geekbang.org/infoq/93/93d22d9a88c32953731aabde7c90555a.png\" /></p><p></p><p>第二类案例，快速定界访问托管服务的故障：</p><p></p><p>案例 1：5 分钟定界托管云服务故障&nbsp;- SLB 会话迁移。由于托管服务无法插桩，以往通常会给故障排查带来困难。DeepFlow 的一个智能汽车客户，充电业务每 10min 发生一次高时延现象。通过 APM Agent 只能定位到问题由充电核心服务访问 RDS 导致，但公有云服务商在仔细检查慢日志和 RDS 性能指标之后关闭了工单，因为没有发现任何异常。这个问题持续了一周仍未解决，而通过 DeepFlow 的全栈指标数据，清晰的看到故障发生时从系统调用、Pod 网卡、Node 网卡观测到的 RDS 访问时延均超过了 200ms，并伴随着网络指标中的「服务端 RST」数量激增。这些数据使得公有云服务商重新开始排查此问题，最终发现 RDS 之前的 SLB 集群在高并发时触发会话迁移导致了此问题。可以看到全栈可观测性是跨团队排查问题的关键。案例 2：5 分钟定界托管云服务故障&nbsp;- K8s SNAT 冲突。这个案例中同样也出现了 SLB，但根因大不相同。DeepFlow 的一个智能汽车客户，车控服务在访问账户服务时偶发超时，每个 Pod 每天发生 7 次。公有云服务商同样也没有看到任何 SLB 异常指标，此工单持续一个月仍未解决。查看 DeepFlow 全景图之后又一次快速完成了定界，可以看到故障发生时网络指标中的「建连异常」数量激增，进一步查看关联的流日志发现此时 TCP 连接的失败原因为「SNAT 端口冲突」。可以看到即使对于「没有调用日志」的超时类故障，利用全栈性能指标也能快速定界故障原因。</p><p><img src=\"https://static001.geekbang.org/infoq/f3/f39741c37660dd2458c2b55c849c0ed5.png\" /></p><p></p><p>第三类案例，快速定界各类网关和云基础设施的问题：</p><p></p><p>案例 1：5 分钟定界造成性能瓶颈或故障的网关。为了集中实现负载均衡、安全审计、微服务拆分、限流和熔断等功能，云原生基础设施中通常会部署各类网关。DeepFlow 的一个游戏客户使用 KNative 作为 Serverless 基础设施，在该环境下任何一个客户端在访问微服务时，都要穿越 Envoy Ingress Gateway、K8s Service、Envoy Sidecar、Queue Sidecar 共四种网关。当客户端侧的调用时延远高于服务端侧的调用时延，或者发生 HTTP 5XX 调用故障时，以往客户主要通过检索日志文件、tcpdump 抓包来排查问题，而利用 DeepFlow 可以在 5 分钟内定位网关路径上的性能瓶颈或故障位置。例如某一次就快速发现了 Envoy Sidecar 配置不合理导致的慢请求问题。案例 2：5 分钟定界私有云基础设施性能瓶颈。DeepFlow 的一个大型银行客户在「分布式核心交易系统」上线私有云之前进行了大量的性能测试，期间发现 K8s 集群中的微服务访问裸金属服务器上的 MySQL 服务时，客户端侧的时延（3ms）与 DBA 团队看到的时延（1ms）有较大的差距，这意味着整个过程中基础设施的耗时占了 67%，但并不清楚具体是哪个环节引入的。通过 DeepFlow 可以看到，整个访问过程中的主要时延消耗在 KVM 宿主机上。这些数据反馈到私有云供应商以后进行了快速的排查，发现该环境下宿主机使用了 ARM CPU 和 SRIOV 网卡，并开启了 VXLAN Offloading，复杂的环境下一些不合理的配置导致流量转发时延过高。通过修改配置，DeepFlow 观测到 KVM 处的时延下降了 80%，有效的保障了整个分布式核心交易系统的顺利上线。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f832116a56992266f81937647a7a7ec.png\" /></p><p></p><p>第四类案例，快速定位代码问题：</p><p></p><p>案例 1：利用调用日志发现祖传代码的问题。这里的祖传代码指的是那些开发人员已经离职，或者接手它的开发者已经更换了好几次，又或者是一个外部供应商提供的没有源代码的服务。即使客户想通过插桩的方式提升服务的可观测性，对此类服务也是力不从心。我们的很多客户在部署 DeepFlow 的第一天就能立即发现此类服务的一些问题，例如一个游戏客户发现某个游戏的 Charge API 正在报错，虽然对玩家没有任何影响，但却给公司带来着持续的经济损失。例如一个云服务商的开发团队发现某个服务正在写入一个不存在的数据库表，而这个服务的负责团队已经更换了好几次，它没有造成业务的故障，但却导致了运营数据的错误。案例 2：利用 Network Profile 发现代码性能瓶颈。在 Linux 内核 4.9 以上的运行环境中，利用 eBPF Profile 定位代码性能瓶颈是一个非常方便的能力。而 DeepFlow 的 Network Profile 在更普遍的内核环境下也能实现一部分效果。例如我们的一个游戏客户在压测 Redis 托管服务时发现压测程序打印的时延高达 200ms，查看 DeepFlow 性能指标后显示主机网卡上观测到的时延只有不到 3ms。压测人员并不是压测程序的编写者，压测程序所在的服务器内核也不具备 eBPF 能力。为了弄清楚原因，压测人员查看通过 cBPF 数据生成的 Network Profile，马上发现了客户端等待时延（Client Wait Time）高达 200ms。这意味着压测程序在两次调用的间隙中花费了太多的时间，这个信息反馈给压测程序的开发团队时对方非常惊喜，立即进行了优化并取得了立杆建议的效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/1768a812719f1ac4afb0d9b93d1cf339.png\" /></p><p></p><p>本章节介绍的所有案例均为 DeepFlow 客户实际工作中的真实案例，希望能让你更真实的感受 eBPF 技术对可观测性的重要性。</p><p></p><h2>使用 eBPF 技术前的常见疑问</h2><p></p><p></p><p>问题一，eBPF Agent 能在多大程度上替代 APM Agent？如果我们仅考虑分布式追踪目的，即使存在跨线程和异步调用，也可在 Wasm Plugin 的加持下，充分利用金融、电信、游戏等典型业务的请求头中的唯一 ID 字段完成追踪，同时 Wasm Plugin 也可用于业务语义的提取，因此使用 eBPF Agent 可完全替代 APM Agent。对于追踪应用内部函数之间调用路径的需求，一般聚焦在对微服务框架、RPC 框架、ORM 框架的追踪，由于这类函数相对标准，我们相信未来可实现基于 Wasm plugin 驱动的 eBPF 动态 Hook，以获取程序内部的 Span 数据。</p><p></p><p>问题二，eBPF Agent 对内核的要求很高吗？DeepFlow Agent 中超过一半的能力基于内核 2.6+ 的 cBPF 即可实现，当内核达到 4.9+ 时可支持函数性能剖析功能，当内核达到 4.14+ 时可支持 eBPF AutoTracing 以及 SSL/TLS 加密数据采集功能。另外在 Wasm Plugin 的加持下，AutoTracing 并不是强依赖 4.14+ 内核的，通过提取请求中现有的唯一 ID 字段可以在任何 2.6+ 的内核上实现 AutoTracing。</p><p></p><p>问题三，采集全栈数据是否会占用大量的存储空间？四层网关不会改变一个调用的内容，七层网关一般只会修改一个调用的协议头。因此网络流量中采集到的调用日志可以非常简单，仅包含少部分关联信息和时间戳信息即可，无需保留详细的请求和响应字段。这样计算下来，网络转发路径上采集到的 Span 只会增加很小的存储负担。</p><p></p><p>问题四，eBPF 能用于实现 RUM 吗？eBPF 并不是一项浏览器上的技术，因此不适用于 Web 侧。eBPF 是一项主机范围的数据采集技术，因此不适合运行在个人移动设备上采集所有 APP 的数据。但对于由企业完全控制的终端系统来讲，eBPF 是有着广泛的应用场景的，例如基于 Linux 或 Andriod 操作系统的 IoT 终端、智能汽车的车载娱乐系统等。</p><p></p><h2>eBPF 对新技术迭代的重大意义</h2><p></p><p></p><p>以往 APM Agent 无法实现基础设施的可观测性，使得用户会倾向于追求基础设施的稳定和低频变更，但这必然会导致创新被抑制。因此，基于 eBPF 实现可观测性对新技术的迭代发展有着重大意义。各行各业的创新正在解决业务面临的痛点，人们看到收益之后也会加快对创新的采纳速度，零侵扰的可观测性是对创新速度的有力保障。</p><p></p><p>云原生基础设施的持续创新：以网关为例，云原生环境下微服务接入的网关数量可能会令你大吃一惊，下面这张漫画非常形象的表达了这个现状。这些网关正在解决着业务上遇到的实际问题，负载均衡器避免了单点故障；API 网关保障了 API 暴露的安全性；微服务网关让同一个业务系统中的前端可以很方便的访问到后端的任意一个微服务；Service Mesh 提供了限流、熔断、路由能力，减少了业务开发的重复工作。纵使不同的网关可能存在能力的交叠，这也是技术发展过程中不可避免的中间态。另外，不同的网关往往由不同的团队负责管理，且管理人员通常没有二次开发能力。若无法实现网关的零侵扰可观测性，对故障排查会带来灾难性的后果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/3367909ad269125925dece915376db64.png\" /></p><p></p><p>金融核心交易系统的分布式改造：以往金融业务的核心交易系统是由专用硬件来承载的，不易于扩展迭代且价格昂贵。DeepFlow 的银行、证券、保险客户近两年纷纷开启了核心交易系统的分布式改造，这些系统关系着国计民生，零侵扰的可观测性正是保障这类系统顺利上线的前提。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ea/ea5ba6160d10a663ba9e2cdf00768d9c.png\" /></p><p></p><p>电信核心网面向服务的架构改造：与金融类似的是，电信核心网以往也是由专用硬件来承载的。然而从 5G 核心网开始，3GPP 已经明确的提出了面向服务的架构（SBA）规范，核心网网元已经拆分为一系列微服务运行在了 K8s 容器环境中。同样，零侵扰的可观测性也是保障电信核心业务系统顺利上线的前提。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/79dd9e17cad0af874ba054eebccd125a.png\" /></p><p></p><p>智能网联汽车的发展：智能汽车网络由中心云、边缘云（工厂/园区）、终端（车载系统）组成。为了给用户带来持续更新的软件体验，整个智能汽车网络中的服务均采用微服务架构、云原生部署。一个具备可观测性的基础设施同样也是这张大网持续迭代的前提。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9ab2c3bd01f51a74f5048ddace1a211b.png\" /></p><p></p><p>对 AIOps 发展的重要意义：以往，AIOps 方案落地之前，观测数据（通常是指标和日志）需要进行集中和清洗。这是一个漫长的过程，通常耗时数月都难以完成。eBPF 有望对这一现状进行根本上的改变，由于 eBPF 采集的数据覆盖了所有服务、具有高度一致的标签信息和数据格式，将会极大降低 AIOps 解决方案的落地门槛。</p><p></p><h2>总结</h2><p></p><p></p><p>APM Agent 由于其侵扰性，难以在金融、电信、电力等行业的核心业务系统中落地，难以在云原生基础设施中插桩。eBPF 的零侵扰优势很好的解决了这些痛点，是云原生时代实现可观测性的关键技术。DeepFlow 基于 eBPF 的全景图、分布式追踪、持续性能剖析能力已服务于各行各业，帮助金融行业的分布式核心交易系统、电信行业的 5G 核心网、能源行业的分布式电力交易系统、智能网联汽车、云原生游戏服务等快速实现了零侵扰的可观测性，保障了新一代业务和基础设施的持续创新。</p><p></p><h4>作者介绍：</h4><p></p><p></p><p>向阳，清华大学博士，云杉网络研发 VP，曾获网络测量领域国际顶会 ACM IMC 颁发的第一届 Community Contribution Award，现负责云原生可观测性产品 DeepFlow。产品基于 eBPF 等新技术帮助云原生应用快速实现零侵扰、全栈的可观测性，相关论文被通信领域国际顶会 ACM SIGCOMM 2023 主会录用。</p>",
    "publish_time": "2023-09-13 17:13:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从 Clickhouse 到 Apache Doris：有赞业务场景下性能测试与迁移验证",
    "url": "https://www.infoq.cn/article/LubhgmLOsRHDSFFd2r6t",
    "summary": "<p></p><blockquote>当前，电商运营的主要痛点不仅来自多变的市场和客户需求，也受困于碎片化用户触达等带来的竞争与挑战。为了深度挖掘用户价值、培养用户忠诚度、实现业绩增长，有赞为商家搭建了全方位 OLAP 分析系统，提供实时与离线分析报表、智能营销与人群圈选等 SaaS 服务。本文将详细介绍有赞从 Clickhouse 至 <a href=\"http://doris.apache.org/\">Apache Doris</a>\" 的迁移规划和性能对比测试实践，分享如何基于 Apache Doris 统一 OLAP 技术栈，并满足庞大数据体量下的实时分析与极速查询，最终有赞在多个场景下实现查询平均提速 200% 。</blockquote><p></p><p></p><p>作者：李闯 有赞 基础平台数据研发工程师</p><p></p><p>有赞是国内领先的电商 SaaS 服务商，目前拥有社交电商、新零售、美业、教育及有赞国际化五大业务体系，通过旗下的社交电商、门店管理、解决方案以及其他新零售 SaaS 软件产品，全面帮助商家解决在移动互联网时代遇到的推广获客、成交转化、客户留存、复购增长、分享裂变等问题，帮助每一位重视产品和服务的商家实现顾客资产私有化、互联网客群拓展、经营效率提升，最终助力商家成功。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/96fee97b1b7cf905e0eba681bf865227.png\" /></p><p></p><p>在面对商家与开发者的定制化服务需求的同时，为了能够更好地支持商家有效解决引流获客、分销体系等难题，有赞为商家搭建了 OLAP 分析系统，提供以下 SaaS 服务场景：</p><p></p><p>商家离线后台报表： 面向 B 端为商家提供 T+1 报表查询，对计算精度、查询性能及稳定性要求较高，同时会面临复杂查询场景。人群圈选与智能营销： 从私域触点、线下触点获取用户数据，结合常用社交平台中接入的用户数据，根据业务需求在客户数据平台（Customer Data Platform - 以下简称 CDP）、数据管理平台（ Data Management Platform -以下简称 DMP）、客户关系管理系统（Customer Relationship Management- 以下简称 CRM） 进行不同消费者的全方位画像分析。该场景会面临大量高频的数据实时更新，同时查询体量较大、QPS 较高，时常出现复杂 SQL 查询场景。商家实时分析报表： 面向 B 端为商家提供相关实时报表分析查询，该场景特点是 QPS 比较高，商家可以选择不同的维度组合进行查询，对实时性和稳定性要求高。天网日志分析系统： 为所有业务系统提供日志采集、消费、分析、存储、索引和查询的一站式日志服务。该场景写入吞吐高，需要达到每秒百万级别的数据写入；且查询频率低，涉及天网 TopN 日志查询，因此系统要求具备实时聚合以及模糊搜索能力。</p><p></p><p>随着业务数据体量逐渐庞大，业务对于时效性、联邦查询分析的需求也愈加迫切，现有组件在使用过程中对业务人员开发、运维人员维护都存在一定痛点，因此决定升级数据架构并基于 Apache Doris 来统一 OLAP 技术栈。本文将详细介绍早期架构的组成、 OLAP 系统运转流程、以及实际应用痛点，分享系统架构在迁移过程中的技术与调优经验。</p><p></p><h2>早期架构痛点</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/4d/4d1e503424b6cdd929a1bbcfbf45e358.png\" /></p><p></p><p>早期架构如图所示，数据主要来源于业务数据库 Binlog 与用户日志等原始数据，通过实时与离线两条链路分别对数据进行处理。其中原始数据首先导入至 Apache Kafka 与 NSQ 消息中间件，一部分会通过 Apache Flink 进行流处理计算并与存储在 HBase 中的维度明细表进行关联，另一部分数据会存储于 Apache Hive 与 HDFS 中作为离线数据，通过 Apache Spark 计算写入至 OLAP 引擎中。</p><p></p><p>有赞数据架构主要使用了以下三种 OLAP 引擎，各个组件根据业务场景的特点与需求为上游应用提供不同场景的查询与分析：</p><p></p><p>Apache Kylin： 基于 Apache Kylin 搭建商家离线报表后台，为商家提供 T+1 报表查询。目前后台已经具有超 500 万家的商家数量，对于部分体量较大的商家，其单点会员数能够达到千万级别、商品 SKU 达到数十万、平台构建 Cube 数量达 400+。Clickhouse： 基于 Clickhouse 进行人群圈选与 TopN 日志查询业务，其中人群圈选主要通过实时的明细查询来辅助用户行为数据分析。Apache Druid： 针对 B 端商家实时分析报表场景，基于 Druid 构建维度查询系统，为商家提供实时指标查询服务。</p><p></p><p>然而由于该架构组件过多、架构冗余等问题导致维养、开发、业务应用等方面带来了一系列的挑战，具体如下：</p><p></p><h3>Clickhouse ：查询性能不足</h3><p></p><p>针对部份 SaaS 场景的高并发高 QPS 查询场景，Clickhouse 的查询能力表现不够理想。由于 Clickhouse 组件本身设计的问题，无法支持多表或大表 Join 的查询场景，这就导致一旦出现关联查询场景，业务方需要重新寻找解决方案，使整体查询效率低下。</p><p></p><h3>Apache Druid ：数据修复处理难度大</h3><p></p><p>数据修复难度大： 当出现 Apache Flink 自身容错导致数据重复的情况，Druid 完全依赖写入侧进行幂等操作，由于自身不支持数据更新或删除，只能对数据进行全量替换，导致数据准确性低、修复难度大。数据一致性问题： 对于 Druid 而言，导入数据后需要构建完 Segment 才能响应查询结果。一旦上游 Flink 写入 Kafka 的过程中出现数据延迟，则无法按照预期时间写入 Druid 中，指标数据就会出现较大波动，数据一致性无法得到保障。数据修复链路过长、成本过高：为了解决部份临时数据修复问题，我们首先需要花费小时级时间将 Apache Kafka 数据备份至 HDFS上，在备份完成后还需要将数据重新导入 Druid 之后进行修复，整体修复的链路过长，投入的时间与研发成本会随之升高。</p><p></p><h3>Apache Kylin : T+1 时效性低</h3><p></p><p>Apache Kylin 在数据处理过程中采用了预计算的方式，通过在多维 Cube 构建过程中完成聚合计算，并生成 T+1 数据报表。对部分在夜间经营的商家而言，他们需要等待一天时间才能查看前一天的报表数据，这无法满足用户对于时效性的需求。</p><p></p><h3>整体架构：运维成本高、研发效能低、架构灵活度差</h3><p></p><p>研发成本高： 业务方需要学习每种组件（Clickhouse、Druid、Apache Kylin)的使用方式、并且查询 SQL 标准各异，这会使学习成本加大，并且在后期进行研发、监控、运维、周边生态工具等开发工作过程中，需要投入大量的人力与开发接入成本，降低开发效率。运维瓶颈： 在扩缩容期间业务方需要停写进行集群调整，且单次扩容需要将所有的库表都进行迁移，不仅无法保证运维时间成本，还会增加过高的人力成本。而目前有赞存在大量的扩容需求，现有架构的运维成本则成为系统的一大痛点。架构灵活度差： Apache Kylin 仅在维度和指标提前设定、表结构固定的场景下能够正常运行，一旦增加维度和指标则需要新建 Cube 并重刷历史数据；Clickhouse 在宽表补数时会出现需要重新全量导入数据，这些架构缺陷在业务运行过程中都会引发资源使用增加、运维成本增加、研发效能较低的问题。</p><p></p><h2>技术调研与收益成本评估</h2><p></p><p>基于上述架构痛点，我们对市面上的架构进行了调研与选型，希望选择一款能够简化当前复杂架构、统一 OLAP 技术栈的引擎。我们除了分析 OLAP 性能本身对于业务的帮助，还需要评估架构改造所带来的收益成本比，思考架构进行迁移和重构之后所带来的 ROI 是否符合预期。</p><p></p><p>对于收益而言，我们需要评估新架构引入后的性能是否如预期提升，将 Apache Doris 分别与 Clickhouse、Druid、Kylin 进行对比评估。</p><p></p><p>对于成本而言，我们首先会考虑在替换过程中，周边工具开发的成本，其中涉及监控、告警、上下游依赖平台等一系列工具的构建与研发；其次业务的迁移会涉及大量业务改造与协调，如何催动业务方进行改造、提供更丰富的改造工具、尽可能降低投入成本也是我们主要考虑的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/68b79899e7c233d2d59ac6c2a4f9d190.png\" /></p><p></p><p>经过一系列评估后，我们发现基于 Apache Doris 进行架构迭代，其不论是在业务赋能还是成本方面，都能够有效解决当前架构的痛点，极大程度地实现降本增效的目标，整体迭代的预期收益明显高于改造代价，因此我们决定基于 Apache Doris 构建统一实时数仓，具体评估分析如下：</p><p></p><p>查询性能优异： 解决了 Clickhouse 在高 QPS 查询与大表关联查询场景下的弊端，提供了优秀的并发查询能力。此外，在 Apache Doris 2.0 发布后，倒排索引功能支持任意维度快速检索、文本分词全文检索，在日志场景下的查询性能提升尤为明显。高效的数据更新： Apache Doris 的 Unique Key 支持大批量数据更新、小批量数据实时写入，覆盖我们 90 % 业务场景，其 Duplicate Key 与 Aggregate Key 模型还能够支持部分列更新，整体数据模型丰富，帮助提升写入与查询效率。保证数据正确性： Apache Doris 支持事务导入，Bitmap 功能支持精准去重，保证数据不丢不重；同时支持精准写入，保证数据基表与物化视图强一致性、副本数据强一致性。简单易用、开发成本低： Apache Doris 高度兼容 MySQL，使开发简单使用门槛降低，且 Doris 的迁移与扩缩容成本较低，在横向扩容等运维操作方面特别简单。其周边组件的接入与监控的接入皆相对简单，Doris 社区提供 Flink &amp; Doris Connector、Spark &amp; Doris Connector 等接入工具，并且监控模版能够直接取用，无需再开发。社区活跃度高： 在过往加入的开源社区中，Apache Doris 社区活跃度非常高，社区开发者多、迭代更新快，对于社区内的问题解答也十分积极，在开发过程给予了非常多的帮助。</p><p></p><h2>基于 Apache Doris 构建统一实时数仓</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/83847d2a58fed6cd97cd39029a9d10a9.png\" /></p><p></p><p>如上图所示，新架构将基于 Apache Doris 搭建统一实时数仓，替换原架构中的三个 OLAP 组件，解决由组件过多引起的接入成本高、资源使用增加、数据链路过长等问题，最终能够减轻业务方的负担、减少整体框架的硬件成本、实现引擎与技术栈统一等目标。</p><p></p><p>在有赞绝大多数应用场景中，原架构都存在数据重复、数据延迟需要修复的情况，引入 Apache Doris 之后，我们将利用其 Unique Key、Duplicate Key、Aggregate Key 模型功能实现高效的数据更新，保证写入效率，并且 Doris 架构具备弹性伸缩的能力，引入后能够极大程度地降低故障发生的概率以及出现故障时数据恢复的效率。</p><p></p><p>此外我们还将引入 Apache Doris 以下功能：</p><p></p><p>倒排索引： Apache Doris 2.0 版本的倒排索引功能优化天网日志分析系统，实现多维度快速检索，加速日志场景的查询分析性能。主键模型写时合并（Merge-on-Write）： Apache Doris 提供丰富的导入方式，可以将小批量数据实时导入 Doris 中，为后续上架门店业务提供实时报表查询，与原价构使用对比，Doris 能够极大程度提升导入时效性。</p><p></p><h2>从 Clickhouse 到 Apache Doris 的迁移经验</h2><p></p><p>在确定架构迁移之后，我们首先选择用 Apache Doris 来替换 Clickhouse 组件，主要由于在业务增长时 Clickhouse 查询性能瓶颈较大、集群扩缩容操作过于复杂等痛点使运维团队的工作量大幅增加，加之大表 Join 能力差、高 QPS 查询性能差等一系列问题无法满足业务方诉求，且 Clickhouse 功能与 Apache Doris 相似，业务方更便于迁移， 因此我们优先替换 Clickhouse 组件。</p><p></p><p>接下来，我们将分享 Doris 替换 Clickhouse 的迁移方案，架构迭代的整体节奏分为 SQL 语句改写实现自动导入（包含建表语句与查询语句的改写）、查询性能测试、稳定性测试、导入性能测试与优化，在结束一系列测试后最终进行整体业务数据的迁移。</p><p></p><h3>SQL 建表语句与查询语句改写</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a6198484a24477e00544095ed18b29b8.png\" /></p><p></p><p>目前，我们针对 Unique Key 模型与 Duplicate Key 模型制作了 SQL 建表语句改写工具，如上图所示，支持通过配置参数自动将 Clickhouse 建表语句转为 Doris 建表语句，该工具的主要功能具体如下：</p><p></p><p>字段类型映射： 由于 Doris 与 Clickhouse 字段不一致，存在一些特殊要求的转换，例如 Key 值类型 String 需要转为 Varchar 以及设置对应长度、分区字段 String 需要转为 Date V2 等；动态分区表的历史分区数确定： 因为部份表存在历史分区，需要在建表时指定分区数量，否则插入数据会出现 No Partition 异常；Buckets 数量确定： 虽然历史分区表可以进行统一配置，但是往往历史分区数据量不完全一致，因此我们根据历史分区的实际数据量推算出历史分区的分桶数，同时对于非分区表可以根据历史数据量设置 Properties 进行自动分桶配置；TTL 周期确定： 可以设定动态分区表的转换周期，设定保留时间后再转换；Unique 模型的 Sequence 设置： 在导入时可以指定 Sequence 列导入顺序，解决了导入顺序无法确定的问题，有效保证数据导入过程中的有序性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2e785af39b3c2a4ea5ac145e34aadc4c.png\" /></p><p></p><p>与建表语句改写工具类似，SQL 查询语句改写能够自动将 Clickhouse 查询语句转成 Doris 查询语句，主要为了双跑进行数据准确性和稳定性验证。在改写过程中，我们梳理了以下注意事项：</p><p></p><p>查询表名转换： 在 Clickhouse 与 Doris 建表过程中存在一定的映射规则，在进行双跑测试的过程中，我们可以直接根据映射规则直接进行转换。函数转换： 由于 Clickhouse 与 Doris 使用函数差异较大，需要根据 Doris 和 Clickhouse 的函数映射关系进行函数映射转换。其中我们遇到一些比较特殊的函数转换需要进行特别处理，例如 Clickhouse 中的 COUNTIF() 需要转换为 SUM（CASE WHEN _ THEN 1 ELSE 0) 以达到相同的效果， ORDER BY 与 GROUP BY 需要利用 Doris 开窗函数进行转化，此外 Clickhouse 利用 Array Join 进行列传行，对应 Doris 则需要利用 Explode 、 Lateral View 来展开；语法层面的不兼容： 由于 Clickhouse 不兼容 MySQL 协议而 Doris 高度兼容，因此在子查询中需要进行别名设置。特别是在人群圈选的业务场景中存在多个子查询，因此在售后转换的时候需要把对应子查询利用 sqlparse 进行递归，检查出所有的子查询进行设置。</p><p></p><h3>Apache Doris 与 Clickhouse 性能压测</h3><p></p><p>查询性能测试主要通过 Apache Doris 与原架构 Clickhouse 组件在三个核心业务场景（CDP、DMP、CRM）下的对比表现。我们选用了线上等比的集群规模，通过查询 SQL 性能对比、大表 Join 性能两方面进行对比压测，同时检测 Doris 在查询期间的 CPU 以及 内存损耗。接下来我们将详细介绍压测过程与具体性能数据对比。测试集群规模3 FE + 16 BE，BE单节点配置为（ 32C 128 G 7T SSD）。</p><p></p><p>核心场景下查询 SQL 性能对比</p><p></p><p>在进行查询 SQL 性能测试中，我们主要基于当前实际应用场景最多的三大系统进行查询，分别是 CDP、DMP、CRM 场景的对比。最终有效查询 SQL 16 条，线上场景下查询 SQL 的具体特点如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7e/7e2cdab3ae9316333c1cb39b812a9398.png\" /></p><p></p><p>如表格所示，我们将 Doris 与 Clickhouse 16 条 SQL 查询时间对比，其中有 10 条 SQL Doris 查询性能优于 Clickhouse。 此外我们将 Doris 与 Clickhouse 查询时间总和进一步对比，在对 Doris 表结构设计优化后，Doris 整体查询速度相比 Clickhouse 快 2-3 倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/9253167732b884374b6ceed07a414796.png\" /></p><p></p><p>大表 Join 查询性能测试</p><p></p><p>在关联查询测试中，以 CDP 场景下的相关数据表为基础，我们选用了不同数据量级的主表与维表数据，主表测试数据量分别为 40 亿的用户行为表、250 亿的用户额外属性表、960 亿的用户额外属性表；维表以 kdt_id + user_id 为基础，测试量级分别为 100 万、1000 万、5000 万、1 亿、5 亿、10 亿及 25 亿维表数据量。为了测试更加全面，关联查询测试分为完全关联与过滤关联两种测试，完全关联是将主表与维度表直接进行 Join，过滤关联是在相同主表量级关联中，增加了 WHERE 条件对指定的两个店铺 ID 进行过滤。</p><p></p><p>具体的查询测试表现如下：</p><p></p><p>全关联 40 亿： 在 40 亿主表完全关联查询中，Doris 查询性能均优于 Clickhouse，且随着维表数据量级增大，Doris 与 Clickhouse 查询耗时差距越大，其中 Doris 最高能够达到 5 倍性能提升；过滤指定店铺关联 40 亿： 在过滤条件关联查询中，主表按照 WHERE 条件过滤后的数据为 4100 万，相较于 Clickhouse，Doris 在维表数据量小的情况下能够达到 2-3 倍的性能提升，在维表数据量大的情况达到 10 倍以上的性能提升，其中当维度数据表超过 1 亿后，Doris 依旧可以稳定查询，而 Clickhouse 由于 OOM 情况导致查询失败。全关联 250 亿： 在 250 亿 50 字段宽表作为主表完全关联时，Doris 查询性能依旧优于 Clickhouse，Doris 在所有维表量级中均能跑出，而 Clickhouse 在超过 5000 万后出现 OOM 情况；与过滤指定店铺关联 250 亿： 在条件关联查询中，主表按照店铺 ID 过滤数据为 5.7 亿，Doris 的查询响应时间均达到了秒级，而 Clickhouse 最快响应时间也需要分钟级耗时，在数据量大的情况下更是无法跑出。全关联与过滤指定店铺关联 960 亿： 不论是主表关联查询还是条件关联查询，Doris 均可跑出且响应速度较快，Clickhouse 则在所有维表量级中无法跑出。</p><p></p><p>除响应性能外，我们还对于 Doris 的 CPU 与内存损耗进行检测，发现 Doris 在数百亿计大表关联查询的情况下集群负载依旧稳定。综上，Apache Doris 在绝大部份场景查询响应速度快于 Clickhosue ，特别是在大表 Join 场景下，Apache Doris 性能表现完全优于 Clickhouse。</p><p></p><h3>Clickhouse 线上流量回放稳定性测试</h3><p></p><p>在查询压测完成后，我们开始将 Doris 与 Clickhouse 线上双跑以进一步验证 Doris 的稳定性。具体步骤如下：</p><p></p><p>通过定时采集 Clickhouse 最近 1 分钟的查询状态为 QueryFinish 的有效查询信息。将查询信息上报至 Kafka，接着通过 Flink 消费 Kafka Topic 获取 Clickhouse 查询 SQL 并统计结果。在 Flink 中实现 UDF 将 Clickhouse 查询 SQL 转化为 Doris 查询 SQL，并由 JDBC 执行。获取执行结果与统计结果，与 Clcikhouse 执行信息进行对比最终存放至 RDS。最终通过对线上 Clickhouse 查询流量回放的统计，分析 Doris 查询性能与查询数据准确性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6ed8bd9abfb094d161403a742aae81ae.png\" /></p><p></p><h3>Apache Doris 数据导入性能测试与优化</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/31dfa30465bab04f8eef7843603330de.png\" /></p><p></p><p>数据导入性能测试是我们重要关注的环节之一，Apache Doris 本身对于实时数据和离线数据的导入提供了比较丰富的导入方式，实时数据的导入方式主要是通过 Apache Flink 将 NSQ 和 Apache Kafka 的数据实时通过 Stream Load 方式写入 Apache Doris 中。在离线数据中，Doris 提供了多种导入方式：</p><p></p><p>支持通过 Spark SQL 读取外部数据，通过 Stream Load 方式写入 Apache Doris；支持通过 Spark Load 方式，利用 Spark 集群资源将数据排序操作在 HDFS 中完成，再通过 Broker Load 将数据写入 Doris；支持 Doris Multi-Catalog 功能直接读取 外部数据源并通过 Insert Into 方式写入 Doris。</p><p></p><p>由于离线数据量较大，针对这类数据我们将几种数据导入方式进行了性能测试对比，通过明细数据的各个数据量级对比测试导入时间。 测试集群规模 3 FE + 16 BE，BE 单节点配置为（ 32C 128 G 7T SSD）测试结果：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/37/374f59f55b7f1f05ad64701aa3e05e19.png\" /></p><p></p><p>Spark Doris Connector 格式导入的并行度为 80，单批为 100 万，集群的负载情况如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e7/e744d223f3938382d382523baaacab86.png\" /></p><p></p><p>根据上方测试结果，我们进一步分析各种导入方式的优势与后续调优方案，希望以下的调优实践能够帮助到有类似需求的开发者们：</p><p></p><p>Doris Insert Into</p><p></p><p>Insert Into 方式能够提供快速导数性能，在用法上也相对简单，目前该方式的导入性能已经足够支持我们的业务需求。</p><p></p><p>Spark Doris Connector 支持阻塞写入</p><p></p><p>Spark Doris Connector 导入方式更具有通用性，能够解决大量数据导入的问题，导入性能相对稳定，在导入过程我们需要合理控制导入速率与导入并行度。考虑到我们的业务场景每天会涉及千亿级别的数据量并花费 5-6 个小时进行导入，对于这类大表数据的导入如果因为 BE 写入被拒绝导致失败，会造成下游数据产出延迟等问题。此外，在 2.0 版本中，类似 -235，-238 错误已经在 Apache Doris 内核层面解决，无需用户再手动处理此类问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/687ac737ab910cfb32e12c6366679193.png\" /></p><p></p><p>我们主要从控制写入速度入手，整体改造原理是通过指数退避写入的方式延迟阻塞，利用配置参数使大数据量出现导入异常时可以等待重试，不让任务失败。通过最大阻塞次数、单次最大阻塞时间、需要阻塞异常捕获关键词这三个参数来捕获阻塞异常情况，实现阻塞退避功能。最终在该设置下，我们的大表导入数据成功率达 95%以上。</p><p></p><p></p><blockquote>[1] 相关 PR： https://github.com/apache/doris-spark-connector/pull/117</blockquote><p></p><p></p><p>Spark Doris Connector 支持 Bitmap 数据导入</p><p></p><p>在阅读 Apache Doris 官方文档时，我们发现 Spark Load 的方式可以对 Bitmap 数据进行导入，同时能够将 Bitmap 数据计算放在 Spark 集群中进行计算。在业务实践中，我们使用 Spark Doris Connector 更为常用，于是开始探索通过 Spark Doris Connector 的方式实现 Bitmap 数据导入。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/70c48ec31541ea582cfbc9d082abe03f.png\" /></p><p></p><p>如上图所示，Bitmap 建表语句主要分为三个字段，其中最后一个字段是将 CASE_ID 进行 Bitmap 计算。在与社区成员沟通之后，提供一种设置 Doris Read Field 选项，写除 Bitmap 列外的其他列，同时在 Doris Write Field 中做映射处理。最终实现如上图所示方式通过 Spark Doris Connect 直接将 Apache Hive 明细数据导入 Apache Doris 的 Bitmap 数据中。</p><p></p><p>Spark Doris Connector CSV 格式导入优化</p><p></p><p>在我们的导入流程中，无论是 Spark Doris Connector 还是 Flink Doris Connector，最终都是利用 Stream Load 的方式进行导入，其导入文件 CSV 与 JSON 有两种导入格式且对于不同格式的选择，导入性能的损耗与速率也是不同的。</p><p></p><p>在优化前，我们进行了测试，以数十亿数据规模、26 个字段的业务表进行导入性能测试，发现 CSV 格式比 JSON 的导入速度快近 40% 且其内存消耗是更低的，这也是为什么 Apache Doris 官方推荐使用 CSV 格式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ea/ea3a679eeb0d6a2df31b3206c005319f.png\" /></p><p></p><p>其中值得注意的是使用 CSV 格式进行导入时，设置合理的字段分隔符和换行符对于 CSV Reader 识别效率是至关重要的，如果 BE 的 CSV Reader 对于字段中最后一个字符和分隔符的首字符相同时，则无法识别分隔符。</p><p></p><p>通过官方文档的提示，我们发现 Stream Load 中能够支持参数配置去除字段最外层的双引号，基于此我们决定在 Spark Doris Connector 写入阶段添加用户设置配置，在字段外层拼接双引号，保证不用选定特殊字符依然能够有效分隔，同时在 Stream Load 写入阶段添加去除最外层双引号的配置。通过两端配置，能够保证即使业务数据很复杂的情况下，也无需为字段符号的识别而烦恼，有效保证字段能够正常分割。</p><p></p><p></p><blockquote>[2] 相关 PR: https://github.com/apache/doris-spark-connector/pull/119</blockquote><p></p><p></p><p>Spark Load</p><p></p><p>Spark Load 导入方式的特点是基于 Spark 资源进行 Shuffle、排序等工作将文件输出在 HDFS 中，之后 BE 节点能够直接读取 HDFS 上的文件并按照 Doris 格式写入。基于这种方式，在测试过程中我们发现当数据量越大时导入速度越快、越能够节省 Doris 的集群资源，不会带来较大性能损耗。</p><p></p><p>由于 Spark Load 在临时修复数据场景中使用频繁，我们也基于测试进一步优化。通过官网文档与社区帮助下我们发现，Spark Load 阶段的导入速率主要由单次导入并发度和单次 BE 导入数据处理量两方面参数影响，且两个参数都与源文件大小、BE 节点密切相关。当控制其他变量的情况下，源文件越小，导入速度越慢，因此我们认为在 ETL 阶段充分利用 Spark 经营资源并且合理设置 Bucket 数量能够有效加速导入速率。</p><p></p><h2>未来规划与展望</h2><p></p><p>在整体测试环节中，基于 Apache Doris 2.0 正式版本的性能测试已经完成，我们对于 Doris 的查询性能表现是十分满意的。此外，对于导入性能，我们在测试时首先采用的是 Doris 2.0-Alpha 版本，发现在导入过程中存在偶发性 CPU 瓶颈的问题，例如当通过 Spark Doris Connector 的方式，Spark 使用资源和 Doris 导入数据 CPU 存在一定的瓶颈。同时，我们也将问题反馈给社区，在经过社区优化与 2.0-Beta 版本发布后，稳定性得到了改善。</p><p></p><p>目前，我们正在与 Clickhouse 线上双跑对 Doris 的稳定性进一步验证，同时我们正在对 Spark Doris Connector 导入方式的的进行性能优化、开发导入周边工具以完成组件替换等落地工作。后续在逐步完成 Clickhouse 的业务迁移后，基于 Clickhouse 的迁移经验，对未迁移的存量业务逐步完成 Druid、Kylin 两个组件的迁移，最终基于 Apache Doris 构建极速分析、实时统一的数据仓库。</p><p></p><p></p><blockquote>参考 GitHub PR：[1] Spark Doris Connector 支持阻塞写入https://github.com/apache/doris-spark-connector/pull/117[2] Spark Doris Connector CSV 格式导入优化<a href=\"https://github.com/apache/doris-spark-connector/pull/119\">https://github.com/apache/doris-spark-connector/pull/119</a>\"[3] Spark Load 创建 Hive 外表支持 Hive 版本设置<a href=\"https://github.com/apache/doris/pull/20622\">https://github.com/apache/doris/pull/20622</a>\"[4] Spark Load 系统环境变量获取优化<a href=\"https://github.com/apache/doris/pull/21837\">https://github.com/apache/doris/pull/21837</a>\"[5] HIve 外表属性在 Spark Load 不生效优化<a href=\"https://github.com/apache/doris/pull/21881\">https://github.com/apache/doris/pull/21881</a>\"</blockquote><p></p>",
    "publish_time": "2023-09-13 17:48:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]