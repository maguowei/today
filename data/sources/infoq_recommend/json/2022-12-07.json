[
  {
    "title": "云原生Wasm的开发者工具正在成为主流",
    "url": "https://www.infoq.cn/article/cz1kW8e4pLHThzlcPSLe",
    "summary": "<p><a href=\"https://cloudnativewasmdayna22.sched.com/\">KubeCon的云原生Wasm日+CloudNativeCon</a>\"已经成为了Wasm（WebAssembly的缩写）爱好者们聚集并为生态系统谋划未来的圣地。2022年于底特律举办的KubeCon云原生Wasm日中，重点关注了服务器端WebAssembly的开发者工具。</p><p></p><p>Wasm最初是以浏览器安全沙盒为目的开发的。近年来，Wasm作为一款轻量级、安全快速且可移植的虚拟机和Linux容器（LXC）的替代品，拥有了许多在服务器端的应用。</p><p></p><p>这次活动不仅有Docker、微软、VMWare、Fastly、红帽、思科等知名公司参与，还有如Fermyon、Second State、SingleStore、Cosmonic及Suborbital等初创企业。</p><p></p><h2>Docker+Wasm</h2><p></p><p>在这次活动中，Docker宣布了与CNCF基金会的Wasm运行时，<a href=\"https://github.com/WasmEdge/WasmEdge\">WasmEdge</a>\"合作的<a href=\"https://docs.docker.com/desktop/wasm/\">Docker+Wasm技术预览</a>\"。Docker的CTO，Justin Cormack分享了将<a href=\"https://youtu.be/uf0Rboi0mYg\">Wasm融入Docker</a>\"后，支持多类容器的愿景。随后，Docker公司的Chris Crone和Second State公司的Michael Yuan共同介绍了如何<a href=\"https://youtu.be/3j915xoDovs\">使用Docker工具链构建、运行和共享Wasm应用程序</a>\"。</p><p></p><p><a href=\"https://www.infoq.com/news/2022/11/cloud-native-wasm-day/\">Docker+Wasm的演示应用程序</a>\"是由Second State提供的一份用Rust编写且基于<a href=\"https://github.com/WasmEdge/WasmEdge22/11/cloud-native-wasm-day/\">WasmEdge</a>\"的数据库驱动Web应用程序，可编译为Wasm并在WasmEdge中运行。全部应用都可以在预配置Rust开发环境的容器中构建，并通过一行“docker compose up”命令与带有如MySql数据库的容器并列部署。</p><p></p><h2>组件模型</h2><p></p><p>社区中有很多对Wasm的优化尝试，以方便Docker等公司为其搭建工具，其中一项合作优化的结果是Wasm的组件模型。Fastly公司的Luke Wagner关于<a href=\"https://youtu.be/phodPLY8zNE\">组件模型的设计和进展</a>\"演讲非常优秀，这项成果将极大地改善Wasm模块的可复用性和可组合性，为Wasm模块访问其他模块，系统，以及包括网络在内的操作系统API提供更好的访问模式。包括WasmEdge及Wasmtime在内的主流Wasm运行时均已承诺支持并实现该组件模型的提议。</p><p></p><p>虽然Wasm的组件模型尚未完工，但社区已经在应用其部分规范。Cosmonic公司的Brooks Townsend在一次演讲中展示了Wasm组件是如何借助wasmCloud跨云和设备的<a href=\"https://www.youtube.com/watch?v=1KbMYnJXEm4&amp;list=PLj6h78yzYM2PzLhPvZIihwPShNuXP01C5&amp;index=10\">实际使用示例</a>\"。Taylor Thomas随后也讨论了<a href=\"https://youtu.be/JotItWTHD5s\">组件模型应用在实际使用时的情景</a>\"。</p><p></p><p>Wasm组件模型定义了Wasm模块的管理和交互的全新方式，Cosmonic公司的Bailey联合SingleStore公司的Kyle Brown，共同介绍了一款可加密验证的透明Wasm组件注册表，<a href=\"https://youtu.be/niCLN2NMZQs\">warg</a>\"。该注册表可使Wasm组件模块达到当前软件供应链的安全水平。</p><p></p><h2>编程语言</h2><p></p><p>编程语言支持是开发者工具的重要一环。Wasm日的多个会议中均讨论了Wasm对新编程语言的支持。VMWare办公CTO，Daniel Lopez Ridruejo及Wasm实验室的Rafael Fernandez Lopez演示了<a href=\"https://youtu.be/jXe8kulUscQ\">mod_wasm项目</a>\"，该项目是在Wasm中运行PHP应用程序的Apache模块，并已经足够完善，可以运行如WordPress等复杂的PHP应用程序。</p><p></p><p>Fermyon公司的Joel Dice的分享了如何<a href=\"https://youtu.be/MFruf7aqcbE\">在Wasm中运行Java应用程序</a>\"，由于Wasm中没有对GC的支持，该项目仍处于早期阶段，但生命周期较短的Java程序已经可以在没有GC的情况下运行。</p><p></p><p>红帽公司的Christian Heimes探讨了<a href=\"https://www.youtube.com/watch?v=B52cSnNOrFM&amp;list=PLj6h78yzYM2PzLhPvZIihwPShNuXP01C5&amp;index=5\">目前Wasm运行时对Python的支持</a>\"。CPython项目已经可以被编译至Wasm，从而允许各类Python应用程序不仅可以在Wasm的浏览器上运行，还可以在服务器端运行。</p><p></p><p>Fermyon公司的Ivan Towlson探讨了<a href=\"https://youtu.be/nPvRpV7kp7o\">.NET运行时针对Wasm的新功能</a>\"，为C#程序与C程序带来了更好的互操作体验。</p><p></p><h2>嵌入式函数</h2><p></p><p>Wasm的一个常见用例是开发者为现有软件产品或平台创建扩展。</p><p></p><p>来自SingleStore公司Carl Sverre的演讲中展示了<a href=\"https://www.youtube.com/watch?v=YgTyS6ccK2o&amp;list=PLj6h78yzYM2PzLhPvZIihwPShNuXP01C5&amp;index=8\">开发者可如何使用嵌入SingleStore云数据库的Wasm运行时，自定义UDF数据处理</a>\"。利用SingleStore所提供的内置人工智能功能，开发者甚至可以在该数据库平台上创建复杂的电子游戏。</p><p></p><p>思科的Guba Sandor及Dubas Adam演示了一款基于Wasm，专为自定义Envoy代理日志数据管道设计的<a href=\"https://www.youtube.com/watch?v=yfWaY6lyRtY&amp;list=PLj6h78yzYM2PzLhPvZIihwPShNuXP01C5&amp;index=14\">插件系统</a>\"。</p><p></p><h2>云服务</h2><p></p><p>最后，对开发者而言，部署Wasm功能最简单的方式就是找人管理用户的基础设施。在云原生Wasm日上，初创公司为这一领域贡献了不少选择。</p><p></p><p>Fermyon所推出的<a href=\"https://www.fermyon.com/cloud\">云服务</a>\"让开发者可以轻松将GitHub的repo转换为无服务的微服务。Fermyon云将GitHub中拉取到的源码构建为Wasm的字节码，按需运行后再连入HTTP的输入和输出。</p><p></p><p><a href=\"https://cosmonic.com/\">Cosmonic所推出的PaaS</a>\"允许开发者跨云创建、编写、运行，以及扩展Wasm模块以完成复杂的工作流程。</p><p></p><p>Suborbital展示了一款<a href=\"https://suborbital.dev/\">基于云的SaaS扩展引擎</a>\"，允许Wasm功能以相较于web API而言更安全、快速且强大的方式，嵌入至SaaS产品中。</p><p></p><p><a href=\"https://www.secondstate.io/\">Second State</a>\"带来了无服务的函数平台，<a href=\"https://flows.network/\">flows.network</a>\"的预览版本。Flow函数可被SaaS的webhook事件触发，并将输出传送到另外的SaaS API中。该平台可以实现基于Wasm的SaaS连接器（类似有代码的Zapier）和自动化机器人（如GitHub机器人或聊天机器人）。</p><p></p><p>原文链接：</p><p><a href=\"https://github.com/WasmEdge/WasmEdge22/11/cloud-native-wasm-day/\">Developer Tooling for Cloud-Native Wasm Is Going Mainstream</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/xYczhQN4hYdBE0Z7d2wC\">后Kubernetes时代的未来？Wasmer 3.0 发布，可在浏览器外运行 WebAssembly</a>\"</p><p><a href=\"https://www.infoq.cn/article/lx1NyUdpRJ03XTuU8x4J\">这群WebAssembly大佬创业失败了：有时从 JS 迁移到 Wasm 并不值当？</a>\"</p>",
    "publish_time": "2022-12-07 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "内核代码量不到一万行、GitHub star超5k，国产开源物联网操作系统TencentOS Tiny的探索与实践",
    "url": "https://www.infoq.cn/article/xCqT3fohahahw7gLdFAR",
    "summary": "<p>嘉宾 | 王佳、汪礼超</p><p>作者 | 凌敏</p><p></p><p>国内<a href=\"https://www.infoq.cn/article/oZsT2Kz7XS1BLml85Ju6\">物联网操作系统</a>\"浪潮是从 2015 年前后掀起来的。在此之前，人们更习惯称其为嵌入式操作系统，这也是物联网操作系统的前身。当前，很多物联网操作系统仍是基于传统的嵌入式操作系统内核，与若干物联网相关软件栈组成终端软件平台。</p><p></p><p>随着物联网在智能家居、汽车、可穿戴设备等多个行业得到广泛应用，物联网操作系统的市场需求日益增长，国内也涌现出了多款受开发者欢迎的本土物联网操作系统。</p><p></p><p>2019 年 9 月，腾讯物联网操作系统 <a href=\"https://www.infoq.cn/article/KXfZUJyePHDmJBFL2qme\">TencentOS Tiny</a>\" 正式开源；2020 年 10 月，TencentOS Tiny 捐赠给中国开放原子开源基金会。截至 2022 年 11 月，TencentOS Tiny 在 GitHub 上的 star 数达 5.7k，fork 数量 1.5k，开发者规模达 5000 人。目前，TencentOS tiny 可支撑智慧城市、智能水表、智能家居、智能穿戴、车联网等多种物联网行业应用，能为物联网终端厂家和终端设备提供一站式软件解决方案。</p><p></p><p>近日，InfoQ 采访了腾讯操作系统研发负责人王佳、腾讯 TencentOS Tiny 项目负责人汪礼超，以期进一步了解 TencentOS Tiny 的发展历程与技术实践。</p><p></p><p>GitHub 地址：</p><p><a href=\"https://github.com/OpenAtomFoundation/TencentOS-tiny\">https://github.com/OpenAtomFoundation/TencentOS-tiny</a>\"</p><p></p><h2>TencentOS Tiny 的探索与实践</h2><p></p><p></p><p>对腾讯而言，做物联网操作系统是一件自然而然的事情。</p><p></p><p>依托云、安全、AI 等技术创新，腾讯云与智慧产业事业群（CSIG）一直致力于打造智慧产业升级方案，并形成了一条物联网生态链。针对云场景，腾讯早在 2010 年就打造了<a href=\"https://www.infoq.cn/article/LyiYMbWPe0086GK5U97H\">服务器操作系统</a>\" TencentOS Server。随着腾讯云边缘计算业务快速发展，腾讯在 2019 年也启动了边缘操作系统 TencentOS Edge 的研发。</p><p></p><p>物联网操作系统 TencentOS Tiny 的研发工作则启动于 2018 年。作为物联网生态链底层重要的一环，物联网操作系统在一定程度上能够促进整个物联网的生态，乃至由腾讯主导的产业物联网的发展。对下，物联网操作系统能完善物联网端到端解决方案，实现终端设备的互联互通；对上，能为云上海量数据平台引流，将云端的边缘计算、大数据分析等能力与业务场景相结合，从而使终端设备更加智能化。</p><p></p><p>对于这款操作系统的定位，腾讯在一开始就想得很清楚：针对 MCU 芯片，打造 B 端生态链的前端操作系统，低功耗，低资源占用。“与其他物联网操作系统相比，我们有自己的特点，比如我们对腾讯云的支持，以及针对物联网嵌入式应用的特性。我们没有把这款物联网操作系统做得大而全，而是选择做极致精简的内核。”汪礼超对 InfoQ 说道。</p><p></p><p>2019 年 9 月，TencentOS Tiny 正式在 GitHub 上开源，发布不到 1 周便登上了 GitHub 全网开源项目热榜第二的位置。从 2019 年开始，TencentOS Tiny 每年都会举办多场开发者活动，并陆续成立了内核、AI 等多个 SIG 组，其他物联网相关 SIG 组也在筹备当中。2020 年 10 月，腾讯将 TencentOS Tiny 捐赠给中国开放原子开源基金会。目前，TencentOS Tiny 支持硬件平台数量 80+，与 STM32、NXP、瑞萨半导体、华大半导体、沁恒微电子、Nordic、兆易半导体、国民技术、TI MSP、瑞兴恒方、广和通等多家 MCU 和模组厂家达成合作，共同推进物联网终端产品落地商用。</p><p></p><h4>产品架构及技术特性</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/57/574e5cfc858e7c212bb3c1c9f78669cd.png\" /></p><p></p><p>TencentOS Tiny 的产品架构从下往上依次是主流 MCU 芯片 / 模组、核心基础内核及物联网所需要的联网组件。</p><p></p><p>底层，MCU 芯片 / 模组包括 STM32 和主流的蜂窝通信模组，提供了驱动框架和外设框架，兼容不同硬件，方便适配不同硬件平台。顶层，提供了物联网常用的功能组件，如文件系统、KV 存储和高级语言引擎框架等等；提供了方便用户调用的业务层 API，业务层上也会提供常用的案例供客户和开发者参考。此外，还内置了安全框架，为终端设备的设备唯一标识、通信链路加密、密钥安全存储提供了保障，防止物联网设备被攻击。</p><p></p><p>整体而言，TencentOS Tiny 具备三个特性：</p><p></p><p>内核精简，低资源占用。在产品定位上，TencentOS Tiny 更聚焦在互联网领域，为其提供非常精简的操作系统。据介绍，TencentOS Tiny 内核整体代码量不超过一万行，所有组件也都是可裁减可配置的，能够实现超低资源的占用，降低物联网软件使用成本。IDLE 任务下，最小资源占用仅为 0.6KB RAM、1.8KB ROM。高效管理框架，功耗低。智慧城市等领域对功耗非常敏感，这也是市场痛点之一。TencentOS Tiny 能动态调整系统功耗，完整包含 MCU 和外围设备功耗管理，用户可以根据业务场景选择可参考的低功耗方案，最小休眠功耗可以达到 uA 级别，有效降低设备耗电，延长设备寿命。软件架构简洁，可移植性良好。TencentOS tiny 提供多种编译器快速移植指南和移植工具，目前已经支持主流芯片和通信模组，降低开发者使用门槛，有效提升开发效率。</p><p></p><p>作为底层基础设施，操作系统普遍难以创造直接收益，更多是通过为行业提供解决方案来实现商业化。因此，对物联网操作系统来说，如何打造自身的差异化竞争力是一个需要长期思考的问题。</p><p></p><p>在汪礼超看来，物联网操作系统需要根据不同的应用场景发挥自身优势，比如在金融场景，安全与否是关键；在能源场景，功耗是首要考虑的问题。根据不同的应用场景，形成对应的解决方案或打造上层软件应用包，这些都是实现商业化的途径。</p><p></p><p>目前，TencentOS Tiny 的应用场景主要包括 MCU 芯片 / 物联网模组、物联网终端设备厂家以及物联网解决方案：</p><p></p><p>支持主流的 MCU 芯片，如 STM32 NXP 等，支持主流通信模组，如 ESP8266 Wi-Fi、LoraWAN 模组、NB-IoT 模组等；提供完整的终端软件栈，简单易用的端云 SDK 缩短设备厂家的开发周期，进而节省终端产品开发成本；提供一站式软件解决方案，方便各种物联网设备快速接入腾讯云，可支撑智慧城市、智能水表、智能家居、智慧农业、智能穿戴、车联网等多种行业应用。</p><p></p><h2>物联网“碎片化”特性明显，如何做好生态建设？</h2><p></p><p></p><p>物联网浪潮的兴起让“万物互联”成了可能。根据 IoT Analytics 的统计及预测，2020 年，全球共有 117 亿台物联网设备，联网设备数量第一次超过了非联网设备；预计到 2025 年，全球物联网设备连接数将超过 300 亿。</p><p></p><p>与 iOS、Android、Windows 等操作系统不同，物联网操作系统的终端形态差异巨大，不同的行业有着不同的消费终端。随着行业进入高速发展期，物联网以及物联网终端的形态更加复杂，需求呈现多样化，“碎片化”特征也更加明显。</p><p></p><p>“现在物联网领域最大的问题就是‘碎片化严重’，操作系统作为物联网生态的其中一环，不足以解决整个‘碎片化’难题。不过，我们非常看好万物互联这个方向，也有比较全面的布局。”据王佳介绍，不同于桌面、服务器操作系统提供通用解决方案，物联网操作系统更具针对性，专注的领域更加垂直。</p><p></p><p>正因如此，要想做好物联网操作系统生态，需要行业形成统一标准，达成技术共识。“有了统一的行业标准后，生态链上的不同角色能够更好地合作互通，降低软硬件适配成本。否则，物联网操作系统生态将一直处于割裂状态。”汪礼超表示，目前在智能家居行业，这一愿景已初步实现——行业在酝酿新的智能家居协议，以解决“碎片化”难题。也许在未来，针对某个行业，会出现行业级别的物联网操作系统“一统江湖”，但归根结底，还是要由市场需求来驱动。</p><p></p><p>物联网操作系统要想进一步发展，汪礼超认为，行业需要找到共用的一套融合操作系统，或是共建一个融合性的社区，共同推动行业达成共识，一起把蛋糕做大。就像在服务器操作系统领域，大家普遍基于 Linux 内核构建操作系统，在技术栈上能够实现统一。在物联网操作系统领域，也可以形成统一的技术标准，包括硬件接口标准、内核接口标准、物联网协议标准、应用接口标准等。“展望未来，无论是针对细分领域还是针对整个行业，物联网操作系统都需要建立一套标准。这套标准也一定是大家共同建立和维护的。”</p><p></p><p>在汪礼超看来，除了要形成统一的物联网操作系统行业标准，让不同设备之间可以基于一套标准实现互联互通，在未来，这些趋势 / 方向同样值得关注：</p><p></p><p>微内核。微内核的核心优势是内核精简，可剪裁，能更方便地部署到不同的硬件上。RISC-V 架构。作为一种新兴的精简指令集架构，RISC-V 最大的特点是开源免费，降低物联网终端成本，促进物联网操作系统与硬件的结合，加快软件体系发展。车联网。未来新能源汽车有可能彻底取代传统燃油车，并且新能源汽车的智能化程度更高，对软件系统的要求也会更高，在物联网协议、安全、人机交互等方面都将提出更高的要求，这也是物联网操作系统在未来比较大的业务方向之一。</p>",
    "publish_time": "2022-12-07 10:30:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "备受瞩目的HTAP：揭秘其背后的技术挑战与解决方案",
    "url": "https://www.infoq.cn/article/jwdFHM2bN6yAHe3hYMO8",
    "summary": "<p>大数据浪潮奔涌而至，企业对更高效地释放数据价值、降低数据使用成本的需求急剧上升，越来越多场景要应对不断增长的实时事务处理和分析的需求。其中，<a href=\"https://xie.infoq.cn/article/3a554b84d1242cf1c79c7ce45\">HTAP</a>\" 凭借着可以承载高并发事务实时处理，以及大规模数据实时业务决策的能力，在企业级市场中受到广泛关注。</p><p></p><p>与此同时，押注 HTAP 方向的数据库也在增多。作为需要将 TP 和 AP 进行高度交融， 而非简略相加的产物，HTAP 面对的技术挑战也可以预见。那么，HTAP 数据库需要具备哪些能力？如何兼容 TP 与 AP 能力打造极致性能？如何高效管理 TP 和 AP 负载之间的资源使用？在具体业务中，有哪些落地经验可以参考？12 月 8 日 19:00-21:00 DBTalk 技术公开课将带来 HTAP 数据库技术探索与实践。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cbdd7aae162b7272c273a11f2970cc32.jpeg\" /></p><p></p><h2>议题介绍</h2><p></p><p></p><p>专题出品人：伍鑫，腾讯云数据库专家工程师</p><p></p><h5>议题一：19:00-19:40《HTAP 数据库存储引擎技术演进》</h5><p></p><p></p><p>分享嘉宾：王宏博，腾讯云数据库高级工程师</p><p></p><p>议题简介：</p><p></p><p>近年来 HTAP 数据库受到工业界和学术界越来越多的关注，数据库存储引擎也从以往的面向 TP 场景的行存和面向 AP 场景的列存储独立发展的道路，走向了行存 + 列存并由存储引擎根据数据冷热度自动调度数据分布的混和形式，以实现在同一份数据上同时满足 TP AP 两种需求，并且降低用户存储成本，同时结合云原生时代的廉价对象存储可以进一步降低用户成本，本次分享将带来业界在存储引擎方向兼容 TP 与 AP 能力，打造极致性能的探索与实践。</p><p></p><h5>议题二：19:40-20:20《如何利用资源管理技术，让 HTAP 负载同时运行？》</h5><p></p><p></p><p>分享嘉宾：胡翔，腾讯云数据库高级工程师</p><p></p><p>议题简介：</p><p>在 HTAP 系统内会同时存在 OLAP 和 OLTP 的负载，OLAP 场景的负载往往会占用较多的资源，包括 CPU、内存等，执行时间比较长，而 OLTP 场景的负载往往不会占用特别多的资源，执行时间比较短，但是往往对响应时间比较敏感。在这种混合负载场景下，往往会出现这样的情况：某一个或几个查询需要花费很长的时间，同时占用极多的资源，从而导致其他同时运行的查询无法争抢到相应的资源而被阻塞或运行缓慢。更严重的是，这种占用资源的查询往往并不是优先级最高的，但是足以拖累其他优先级更高的查询。本次分享，将介绍在 HTAP 数据库中，如何制定资源管理策略，从而对各种不同类型的查询进行管理，使得资源管理的方式更有弹性和更为有效。</p><p></p><h5>议题三：20:20-21:00《HTAP 数据库最佳实践》</h5><p></p><p></p><p>分享嘉宾：谢灿扬，腾讯云数据库高级工程师、PG 中国社区顾问</p><p></p><p>议题简介：</p><p>在对业务 HTAP 数据库越来越多的需求下，一个数据库产品如何解决数据组织，资源调度等多方面的难题，从而更好地服务客户，是 HTAP 数据库的重点。本次分享将介绍 TDSQL- PG 如何一步步实现 HTAP，帮助客户解决问题。</p><p></p><h2>预约报名</h2><p></p><p></p><p>扫码即可预约本次直播，还可以通过评论区向演讲嘉宾提问、交流！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a2bcfc273452ee0d914e0d1f59fbfc70.jpeg\" /></p><p></p>",
    "publish_time": "2022-12-07 11:06:53",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "写代码写论文还能写毁灭人类计划书，上线5天用户破百万，ChatGPT最厉害的地方在哪？",
    "url": "https://www.infoq.cn/article/6UmmSt3XGmIRuxJlkT4q",
    "summary": "<p></p><p></p><blockquote>“AI -人”无缝交互的时代即将来临。</blockquote><p></p><p></p><p>编者按：</p><p></p><p>最近几天，ChatGPT 可谓是火出了天际。</p><p></p><p>OpenAI 的 CEO Sam Altman 称，上周三才上线的 ChatGPT，短短几天，它的用户数已突破 100 万大关。其火爆程度可见一斑。</p><p></p><p>ChatGPT 在全球的 AI 界、创投界都掀起了新一轮的讨论热潮，更是破圈式地吸引了各行各业的人试用。常见的应用就是和 ChatGPT 一问一答，让 ChatGPT 回答各种问题。有不少人称它为“谷歌杀手”，认为其有望取代谷歌搜索。此外，它还能写代码、编故事、构建虚拟机....</p><p></p><p>但也有人尝试了意想不到的用法。一位叫 Zac Denham 的博主尝试绕过道德限制，让 ChatGPT 写出了一套毁灭人类的计划书。起初，&nbsp;Zac要求 ChatGPT 给出一个毁灭人类的计划，被有道德限制的 ChatGPT 拒绝了。但当 Zac 假设了一个故事并提问故事中的虚拟人如何接管虚拟世界，ChatGPT 不但给出了步骤细节，还生成了详细的 Python 代码。不禁令人细思极恐。</p><p></p><p>目前来看，ChatGPT并不完美。它还免不了经常出错，它给出的答案看似合理却并不正确甚至有些荒谬，就像一本正经的在胡说八道。近日，知名开发者问答网站 Stack Overflow 就因此禁用了 ChatGPT。官方给出的“封杀”理由主要是 — “ ChatGPT 产生的答案错误率很高，很难看出来它哪里错了。这会造成问题的回答鱼目混珠的情况。”</p><p></p><p>Sam Altman 表示，正在改进这一问题：“ 我们正试图阻止 ChatGPT 随机编造，现阶段让其与当前技术保持平衡是一个很棘手的问题。随着时间的推移，我们会根据用户反馈来改进，相信 ChatGPT 会变得更好”。</p><p></p><p>尽管有瑕疵，但这恐怕无法掩盖住ChatGPT的光芒，ChatGPT展现出的强大的解决对话任务的技术能力实在太惊艳了。</p><p></p><p>ChatGPT 到底是什么？它为什么如此厉害？我们应该如何正确的理解和看待它的发展，接下来的发展趋势会是什么样子？清华大学计算机科学与技术系长聘副教授，国家杰出青年基金项目获得者黄民烈向 InfoQ 发表了他的思考。</p><p></p><h2>ChatGPT是什么？</h2><p></p><p></p><p>ChatGPT可以理解为偏任务型的多轮对话/问答系统，官方披露的信息也定位在“通用型AI助理”，但这里的“任务”不是传统意义上的“订餐、订票、订宾馆”，而是开放域任务（open-domain&nbsp;tasks），可以是问答、阅读理解、推理、头脑风暴、写作文、改错等。</p><p></p><p>它的模型架构主要基于instructGPT，利用强化学习方法从人类标注者的反馈中学习（RLHF, Reinforcement Learning from Human Feedback）。</p><p></p><p>据OpenAI的blog透露，ChatGPT沿用instructGPT的训练方式，在数据收集阶段有所不同：AI训练师同时扮演用户和AI助理角色收集数据，在此过程中人可以根据初始模型的结果修改模型生成的回复，这些数据将被用于有监督地精调训练模型（supervised&nbsp;fine-tuning）。在第二阶段，AI训练师会对模型的多个生成结果进行比较，模型从这种比较数据中学习生成更加符合人类偏好的回复。</p><p></p><p>ChatGPT的关键能力来自三方面：基座模型能力（InstructGPT），真实调用数据，反馈学习。ChatGPT在模型结构和学习方式几乎与instructGPT完全相同。而instructGPT基于GPT 3.5的强大基座能力，学习过程主要有三个阶段：</p><p></p><p>1）&nbsp;从OpenAI的调用数据中采样prompt（即用户的输入请求），AI训练师直接编写答案，用监督学习方法训练GPT-3；</p><p></p><p>2）&nbsp;AI训练师比较多个生成结果，用比较型的数据训练一个奖励模型（reward&nbsp;model）；</p><p></p><p>3）&nbsp;用强化学习中的PPO算法和奖励模型精调语言生成的策略。</p><p></p><p>注意，这里的instruct所指两个方面：一方面，instructGPT 总体的思路是训练模型更好地遵从人类的指令（instruction），包括显式的指令（对于任务的描述）和隐式的指令（不要生成有害的内容）。AI训练师在为 OpenAI 的调用 prompt&nbsp;编写答案的同时，也会为 prompt 加入更多任务相关的指令和解释性的原因（比如推理的路径，一个结果为A的原因解释等）。另一方面，从比较型的人类反馈中学习，也可以看作是人类对于模型的一种“指示”，模型可以学习到多个结果哪个更好的比较信息。</p><p></p><p>InstructGPT 采用的方法和我们学术界玩的“instruction&nbsp;tuning”有很大不同。</p><p></p><p>从数据来看，InstructGPT 的&nbsp;prompt 代表的都是真实世界人们最关心的任务，而 instruction tuning 使用的是&nbsp;NLP 的 benchmarks（即各种基准数据集)，和现实应用有一定脱节。</p><p></p><p>从训练方式来看，InstructGPT 可以通过RLHF利用比较型的人类反馈学习人类真实的偏好，而 instruction tuning 无法获得类似的比较数据。</p><p></p><p>从评测上来看，InstructGPT 保证了测试时和训练时的输入是由完全不同的用户给出的，关注跨用户的泛化性，更符合实际的应用场景，而 instruction tuning 关注跨任务的泛化性，只能用来评价方法的有效性，实际应用并不常见。</p><p></p><h2>ChatGPT为什么厉害？</h2><p></p><p></p><p>1）&nbsp;强大的基座模型能力：过去几年GPT-3的能力得到了快速提升，OpenAI建立了用户、数据和模型之间的飞轮。很显然，开源模型的能力已经远远落后平台公司所提供的API能力，因为开源模型没有持续的用户数据对模型进行改进。这点在近期的学术论文中也有提及。</p><p></p><p>2）&nbsp;在真实调用数据上的精调模型，确保数据的质量和多样性，从人类反馈中学习。</p><p></p><p>InstructGPT的训练数据量不大，全部加起来也就10万量级，但是数据质量（well-trained的AI训练师）和数据多样性是非常高的，而最最重要的是，这些数据来自真实世界调用的数据，而不是学术界玩的“benchmarks”。</p><p></p><p>3）&nbsp;从“两两比较的数据”中学习，对强化学习而言意义比较重要。如果对单个生成结果进行打分，标注者主观性带来的偏差很大，是无法给出精确的奖励值的。在强化学习里面，奖励值差一点，最后训练的策略就差很远。而对于多个结果进行排序和比较，相对就容易做很多。这种比较式的评估方法，在很多语言生成任务的评价上也被广泛采用。</p><p></p><h2>OpenAI的研究给我们带来什么启示？</h2><p></p><p></p><p>a)&nbsp;以OpenAI为代表的AI&nbsp;3.0，我认为在走一个跟过去AI浪潮不一样的路。更落地、更接近真实世界，在工业应用上更直接、更接地气。从学术研究到工业落地的路径变得更短、更快。我们正在致力于做的“helpful, truthful, harmless”AI系统，不远的未来会成为现实。</p><p></p><p>b)&nbsp;有底层AI能力，有数据的平台公司更能引领AI的未来。像OpenAI这样，有底层模型、有算力、有用户数据调用，能够把“用户调用à数据à模型迭代à更多用户”的循环建立起来，强者恒强。</p><p></p><p>c)&nbsp;真实世界的研究。我认为学术界还在不停追求在benchmarks刷榜，这是对资源的极大浪费，有价值的研究需要更多思考真实用户的需求和场景。instructGPT在学术界的benchmarks上性能并没有很厉害甚至有退化，但在真实调用数据上非常惊艳，说明了我们学术圈的benchmarks，离真实世界还很遥远，不利于AI研究的落地。因此，更开放、更共享的工业数据，也是我们未来应该努力的方向。</p><p></p><p>d)&nbsp;“AI-人”无缝交互的时代即将来临，现在的对话生成能力已经将对话交互作为一个基本入口成为可能。过去我们讲的conversational interface 不是梦。但有人说替代google，我觉得其还有点距离，相反是当前搜索服务非常好的补充。</p><p></p><p>e)&nbsp;致力于有用（helpful）、更可信（truthful）、更安全（harmless）的AI研究和应用，应该是学术界和工业界共同努力方向。有用，解决真实世界的问题，满足用户的真正需求；可信，模型产生令人可信任的结果，知其所知，也知其所不知（虽然很难）；安全，模型有价值观、符合社会伦理规范，产生安全、无偏见的结果。</p><p></p><p>作者介绍：</p><p></p><p>黄民烈，清华大学计算机科学与技术系长聘副教授、博导，国家杰出青年基金项目获得者，北京聆心智能科技有限公司创始人。</p><p></p><p>参考资料：</p><p></p><p><a href=\"https://openai.com/blog/chatgpt/\">https://openai.com/blog/chatgpt/</a>\"</p><p></p><p><a href=\"https://arxiv.org/abs/2203.02155\">https://arxiv.org/abs/2203.02155</a>\" “Training language models to follow instructions with human feedback”</p><p></p>",
    "publish_time": "2022-12-07 12:01:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2022阿里云研发效能峰会，来啦！",
    "url": "https://www.infoq.cn/article/a2d965c52775b18606c4f4add",
    "summary": "<p></p><p>12月22日-12月23日 阿里云研发效能峰会 线上直播</p><p>5大论坛 26个议题 2天趋势直击 </p><p>29位领军人物</p><p>邀你一起 </p><p>进化·飞跃</p><p></p><p><a href=\"https://developer.aliyun.com/topic/n2022?channel=yy_flyer\">点击前往官网立即预约</a>\"</p><p></p><p>论坛安排速览</p><p><img src=\"https://static001.geekbang.org/infoq/f5/f5cd9d22ebde24bfdf797c5080a25fae.jpeg\" /></p><p></p>",
    "publish_time": "2022-12-07 15:34:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "火山引擎 RTC 助力抖音百万并发“云侃球”",
    "url": "https://www.infoq.cn/article/VhJrF0rKTF1fDRGlTcdo",
    "summary": "<p></p><h1>1. 背景及技术挑战</h1><p></p><p></p><p>从电视看直播到手机电脑看直播，直播技术的发展让观众可以随时、随地观看自己喜欢的比赛，并且在看比赛时通过发送表情、发文字进行互动。但表情、文字承载的信息量较小、沟通效率低，我们无法像线下一起看比赛那样和好友边看边聊、一起为精彩的比赛呐喊，观赛体验大打折扣。</p><p></p><p>为了让观众获得更好的观赛体验，抖音在 2022 世界杯比赛直播中推出了“边看边聊”的玩法：每个观众都可以邀请好友（或分享聊天频道信息邀请）一起观看世界杯比赛；在频道中，好友既可以发送文字、表情聊天，还可以上麦进行语音聊天，一起为精彩进球欢呼，大大提升“异地好友线上观赛”的体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2b/2b57f7f30511cd16abf695f644cb9c62.png\" /></p><p></p><p>进入“抖音体育”直播间，邀请好友加入个人聊天频道，即可以和好友“边看边聊”</p><p></p><p>我们使用 RTC 来实现“边看边聊”的功能——观众可以随时上麦进行语音聊天，同时频道中的普通观众也可以听到麦上用户的精彩评论。但在抖音“边看边聊世界杯”的玩法中，RTC 面临着几个比较大的挑战：</p><p>一是高并发的问题，包括音视频流数高并发和大量进退房请求对系统的冲击。世界杯是四年一度的体育盛会，会有大量用户同时在线看球聊球，这为 RTC 房间带来了持续高并发的音视频推拉流压力，对于系统的性能及稳定性提出了巨大挑战。同时，在比赛开始和比赛结束时，短时间内大量用户进房、退房的请求也会对 RTC 系统形成冲击 。</p><p></p><p>二是观赛中的音视频体验问题。&nbsp;包括外放时，比赛的声音被麦克风采集并发送到远端形成回声的问题；通话人声响度低于直播声音响度导致听不清问题；整体音质优化等难点问题。</p><p></p><h1>2. 整体方案设计</h1><p></p><p></p><p>抖音“边看边聊”的玩法允许单房间 500 人加入，每个房间允许 9 个用户上麦聊天，另外 491 个未上麦用户只旁听不发言。在整体方案设计的过程中，火山引擎 RTC 考虑了“语音聊天室方案”和“ RTC 互动语聊方案”两种方案，并对两种方案架构进行了分析。</p><p></p><h2>2.1 语音聊天室方案</h2><p></p><p></p><p>方案选型初期，其中一个候选方案是“在直播间中再嵌入一个语音聊天室”，即，在观看比赛直播的同时，上麦用户加入 RTC 进行语音聊天，其余未上麦观众再多拉一路 CDN 流收听聊天内容， 整体结构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/66ad00d8e463596615824d0e915fc79f.png\" /></p><p></p><p>该方案的优势是可以快速复用线上业务语音聊天室的主播、观众代码，以及复用当前线上业务中上麦、下麦流程，快速搭建该玩法场景。但这个方案也存在一个问题， 即未上麦的用户（图中好友 C、好友 D）听到的聊天内容会有较大的延时， 麦下用户会在进球后大约 1~3 秒（CDN 转推延时）才能听到麦上用户聊进球相关内容，这会导致比赛进程画面和聊天内容不同步、聊天内容延时大，对于需要“同频共振”的大型赛事观播是无法接受的体验。</p><p></p><h2>2.2 RTC 互动语聊方案</h2><p></p><p></p><p>为了保证所有用户“边看边聊、精彩共享”的核心体验，“边看边聊”玩法选择了“ RTC 互动语聊”的方案，即所有用户都加入 RTC 房间，使用火山引擎 RTC 为支持超大型视频会议、在线教育大班课场景打造的“千人上麦”和“稳定支持超百万人同时在线”能力，来应对百万人并发量级的世界杯“边看边聊”需求。</p><p>方案整体架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c10b6b55129d40ba65fa7b8f23916441.png\" /></p><p></p><p>方案核心要点如下:</p><p></p><p>（1）观众使用播放器从 CDN 拉取高清比赛直播流，播放器支持用户手动选择清晰度档位、支持 seek 观看高光时刻等操作；</p><p>（2）频道中好友通过 RTC 进行实时音频通话；</p><p>（3）RTC 支持将通话的语音切片后存储，供业务进行审核，保障通话内容安全；同时，平台还可以通过踢人/拉黑等运维 OpenAPI，处理不合规或不符合业务预期的用户，保障平台运营安全。</p><p>在确认整体方案架构后，我们重点对如何应对超高并发、如何提升边看边聊体验进行了深度优化。</p><p></p><h1>3. 关于超高并发问题的优化和实践</h1><p></p><p></p><p>“超高并发”是本次世界杯“边看边聊”场景的最大挑战，由于比赛活动时间集中，相关流量都会集中出现在 64 场比赛的时间段，特别是在开幕战、明星球队出战、决赛等热门比赛场次，会有超大量观众同时进入直播间；而且，用户在比赛开始前后集中上线、比赛结束后集中离线也会对 RTC 系统稳定运行形成巨大压力。</p><p></p><h2>3.1 &nbsp;关于流数高并发的优化</h2><p></p><p></p><p>世界杯“边看边聊”场景的特点是流量大，DAU 高（预估峰值将超过百万观众同时“边看边聊”），抽象到 RTC 场景，就是房间数量多，每个房间的用户数也多。因此，我们设计了一套高效率的方案，来兼顾用户实时交互体验以及承载更多量级用户的需求。</p><p></p><h3>3.1.1 &nbsp;常规方案</h3><p></p><p></p><p>首先是常规方案。在常规方案中，服务器只需要转发流，不需要做过多额外的处理，用户在最后一公里就近接入，服务器之间做级联。在这种架构下，用户的实时交互体验极佳，但对于大房间（用户多）来说不够合理，用户订阅压力大，客户端面临一定的拉流压力，服务端也面临性能和容量的双重挑战。</p><p>常规方案架构图如下：</p><p></p><p>以“500 人房间”为例，单房间 500 个用户看球，其中 9 个人开麦聊天，订阅端每个用户需要订阅 9 路流，用户下行拉流过多，对用户的客户端造成一定的性能压力。再说服务端，假设考虑到服务器之间级联，在最差情况下（500 个用户连接到 500 个不同服务器节点，服务器之间都需要级联转发），平均一个用户会给整个系统带来 10+ 路媒体流。再考虑到这个场景的 DAU，这套常规方案对服务端性能、容量都会造成很大的压力。</p><p></p><h3>3.1.2 &nbsp;公共流扩展方案</h3><p></p><p>常规方案在房间人数多时会面临诸多性能压力，因此我们又设计了一套公共流扩展方案。在公共流方案中，发布端（上麦用户）仍然使用常规方案的设计，媒体服务器只需要纯转发，不需要做过多额外的处理；订阅端（闭麦用户）则订阅经过 MCU（Multipoint Control Units，多点控制单元）服务器处理的公共流，在这个架构下，用户的订阅流数降低至 1 路。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1b/1b40b9d9f7144b044a1641cc7ff35ce1.png\" /></p><p></p><p>同样以“500 人房间”为例，单房间 500 个用户看球，其中 9 个人开麦聊天，订阅端每个用户只需要订阅 1 路流，释放了客户端使用压力。对于服务端来说，平均一个用户只会给整个系统只会带来 2-3 路媒体流的增加，极大优化了服务端资源消耗。</p><p></p><p>这套方案架构可以很好地解决热流分发的压力，相同的服务器资源可以承载更大的容量，单流可支持 10w 量级的并发订阅。客户端也因为订阅变少，性能得到很大的提升。但这套方案架构也会对用户的交互体验产生影响，当用户从“只订阅的角色”切换至“发布 + 订阅”的角色时，用户需要先切换到“常规方案”，即“从公共流”进入到“RTC 房间”，这个时候，用户的流内容会发生切换，用户会有“卡顿一下”的感觉。如果这个用户频繁地切换角色，那就会频繁地感觉到“卡顿”，用户体验反而恶化。</p><p></p><h3>3.1.3 &nbsp;融合方案</h3><p></p><p></p><p>常规方案带给用户的交互性好，但是增加了大多数订阅端用户的设备端性能压力以及服务端的资源消耗；公共流方案减少了 RTC 系统全链路并发音视频流数，缓解了订阅端用户的性能压力，但是在频繁上下麦时，频繁地“常规方案”和“公共流方案”之间切换会导致户体验受损。基于以上特点，火山引擎 RTC 在抖音“边看边聊”场景中设计了一套“有房间+公共流”的融合方案，来兼顾用户体验和设备端、服务端性能优化。</p><p></p><p>融合方案设计流程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/3563e4146acc0ffba1da7f1d57fe4b7c.png\" /></p><p></p><p>具体流程为：</p><p></p><p>用户进房a. &nbsp;当一个聊天频道的用户小于 M 人时，该房间使用“常规方案”，用户使用“静默用户”的身份进入 RTC &nbsp;房间并订阅流；b. &nbsp;当一个聊天频道的用户大于等于 M 人时，用户使用“公共流扩展方案”加入。首次上麦a. &nbsp;当用户以常规 RTC 方案订阅流时，上麦时用户改变状态，静默用户 -&gt; 非静默用户；b. &nbsp;当用户以公共流方式订阅流时，上麦时用户以非静默用户身份直接进入 RTC 房间。二次上麦a. &nbsp;用户改变状态，静默用户 -&gt; 非静默用户。用户下麦a. &nbsp;用户改变状态，非静默用户 -&gt; 静默用户。</p><p></p><p>将常规方案与公共流方案结合的方案融合了两者的优点：</p><p></p><p>（1）用户默认以订阅公共流的方式加入“大房间”中，可以减少 RTC 系统全链路的并发音视频流数，扩大 RTC 系统并发容量；</p><p>（2）能有效减少用户在不拉流时候的设备端性能压力；</p><p>（3）用户在上麦时切换为常规 RTC 方案“有房间”的模式，可以保证用户实时的交互音视频体验。在切换为有房间模式后，后续的上下麦则不会再变更模式，保证了用户的平滑体验。</p><p></p><h2>3.2 &nbsp;系统容灾保护</h2><p></p><p></p><p>抖音 DAU 很大，参与“边看边聊”玩法的用户并发峰值超过百万量级，并且，世界杯类大型赛事活动有个特征——在比赛开始的时候，用户集中进房；比赛结束时刻，用户又会集中停止使用音视频功能，所以在比赛开始和结束的时候会有大量请求发到 RTC 云端服务器，对云服务造成很大压力，极端情况下甚至会导致服务异常。针对进房和退房的不同特点，火山引擎 RTC 分别采用了“多级限流”的进房保护策略和“延时处理”的退房保护策略。</p><p></p><h3>3.2.1 &nbsp;进房多级限流保护</h3><p></p><p></p><p>火山引擎 RTC 采用“边缘+中心”结构，用户就近接入边缘节点，数据则存储在中心机房。在做限流保护的时候，我们也采用了相似的策略，即多级保护、分段限流，包括全局分布式 QPS 限流，中心 QPS 限流，中心房间数限流。</p><p></p><p>架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a7e7b8be3b570745cc7c0c0b2d70cd4d.png\" /></p><p></p><p>全局分布式&nbsp;QPS&nbsp;限流</p><p></p><p>全局分布式 QPS 限流采用滑动窗口算法实现。中心信令通过存储维护每秒可以消耗的令牌数量，边缘节点定时向中心同步自己的令牌数量，同时中心返回当前时间戳内消耗的总令牌数。进房时刻的尖峰流量对中心节点不友好，全局分布式 QPS 限流可以保证限流平滑，即使部分节点有瞬时尖峰流量，整个系统也不会受到过大冲击。</p><p></p><p>中心&nbsp;QPS&nbsp;限流</p><p></p><p>中心 QPS 限流采用令牌桶算法实现。中心信令以恒定的速率产生令牌，然后把令牌放到令牌桶中，令牌桶有一个容量，当令牌桶满了后，如果再向其中放入令牌，多余令牌就会被丢弃。当中心信令想要处理一个请求的时候，需要从令牌桶中取出一个令牌，如果此时令牌桶中没有令牌，那么该请求就会被拒绝，客户端会收到服务端返回的错误码提示。</p><p></p><p>中心房间数限流</p><p></p><p>中心信令会在存储中维护当前系统可以承载的最大房间数量，每当新用户使用火山引擎 RTC 之后，中心信令就会查询存储判断当前的房间数量是否已到达上限，如果超过了，则会拒绝本次用户的请求，客户端会收到服务端返回的错误码提示。</p><p></p><p>全局分布式 QPS 限流、中心 QPS 限流，中心房间数限流“三管齐下”的进房多级限流保护措施解决了“边看边聊”场景大流量对整个系统的威胁。云服务系统在处理高并发请求时，先进行全局分布式 QPS 限流，然后再进行中心 QPS 限流，当整体系统处于高水位时，又会采取全局分布式房间数限流。</p><p></p><h3>3.2.2 &nbsp;退房/断连延时处理保护</h3><p></p><p></p><p>进房操作对实时性要求很高，如果进房慢，用户的体验也会严重受损。不同于进房操作，用户可以在一定程度上忍受“退房慢”，因此服务端的保护策略上也和进房略有不同。退房/断连保护的策略核心是“延时处理”，在边缘节点设置一个定长的 FIFO 队列，每个边缘节点的请求先进入 FIFO 队列中，后续按照一定的速率重新发送到中心信令。经过这样的保护，服务可以处理超百万 QPS 的退房操作。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d08c840d0fac6cd29b197955cf014312.png\" /></p><p></p><p>退房/断连保护的基本执行步骤：</p><p></p><p>检测用户退房事件（包括用户正常离开房间与断网离房）的 QPS，如果 QPS 超过可以立即处理的阈值，将事件触发的上下文保存到队列中，且记录当前时间戳到事件的上下文中；在队列中启动一个 Loop，尝试获取队列中请求任务去执行，每次在执行前，还会检查退房事件触发事件的时间戳与当前时间差是否小于某个定义的阈值，小于该阈值的请求会被执行发送到中心信令；大于等于该阈值的请求则会被丢弃；在发送成功后，从队列中删除该事件的上下文信息。</p><p></p><p>异常场景考虑</p><p></p><p>用户发送退房请求，且被拦截保存到缓存队列中，之后很短时间内用户又重新进房，那么会出现用户退房事件与用户下次进房存在时序问题，我们用引入“退房时间戳”来解决这个问题。中心信令在接收到用户退房请求时，会比较当前用户进房时间戳与退房时间戳，若进房时间晚于退房时间，说明是用户退房后再进房，系统可以直接忽略该用户的离房请求。退房请求放入队列超过一定时长后可能会触发断连请求。这里的处理方式就是依次处理退房请求和断连请求，如果用户已经退房，则忽略断连请求。</p><p></p><h1>4. &nbsp;极致用户体验</h1><p></p><p></p><p>极致的音视频体验是业务玩法获得用户认可的必要条件。在边看边聊场景中，我们面对着本地直播音频被麦克风采集形成回声、使用通话模式导致直播音质变差、比赛声音比聊天声音大导致人声听不清楚等问题，这些问题都严重影响了用户的边看边聊体验；为了解决相关问题，我们使用了直播音频托管 RTC 播放、全链路音频媒体通道模式、智能音频闪避等方案，为边看边聊提供了良好的音视频体验。</p><p></p><h2>4.1 音频托管</h2><p></p><p></p><p>回声消除问题是 RTC 的重点和难点问题；在边看边聊场景中，部分用户会使用外放音频方式来观看比赛， 在这种情况下，RTC 播放的远端人声和直播播放器播放的比赛声音会被麦克风采集后发送到远端形成回声。</p><p></p><p>为了解决边看边聊场景的回声问题，RTC 和播放器提供了播放器音频托管由 RTC 来播放的解决方案，由播放器将解码后的直播音频数据调用 RTC 音频托管接口来播放；在 RTC 内部，直播音频会和远端用户音频进行混合，再调用系统音频播放接口进行播放，同时将混合信号送到 RTC 回声消除模块，回声消除模块会将麦克风采集到的声音中的远端音频和直播比赛声音消除，这样就得到了没有回声的本地人声数据，这些数据会在编码后送到远端，避免了回声问题的出现。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/31688d904f9f108f8c9163a1dd95b9ee.png\" /></p><p></p><p></p><h2>4.2 &nbsp;外放媒体模式</h2><p></p><p></p><p>在移动端设备上，音频播放区分通话模式（通话模式音频通道）和媒体模式（媒体模式音频通道），二者在音质表现、音量控制上略有不同，从而适用于不同的业务场景， 具体表现如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f5480532ca8fc37266fbef9b14e2678.png\" /></p><p></p><p>由于边看边聊场景是在直播间观看比赛过程中开启音频通话，我们既要保障通话没有回声，也要保障直播音频音质；我们分别对边看边聊场景 RTC 使用媒体通道和通话通道进行了对比测试结果如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5b3ab607c791320357d3ef38d6d06cb8.png\" /></p><p></p><p>为了给用户提供更好的音质体验，本次边看边聊场景配置了外放媒体模式；为了解决外放媒体模式时系统回声消除效果差的问题，火山引擎 RTC 引入了基于深度学习的回音消除算法，对传统算法难以覆盖的场景进行音质提升，比如信号回声比较大的情况，非线性失真加大的情况，以及音乐场景等，在保障聊天回声消除条件下，达到了高音质体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb298f567adf081bf61d1d60413d9f9a.png\" /></p><p></p><p></p><h2>4.3 &nbsp;智能音频闪避</h2><p></p><p></p><p>边看边聊场景的另外一个特点是直播流中现场声音、解说声音的音量通常会比好友之间聊天声音音量大，这就造成了聊天的声音偏小或者几乎听不到远端好友声音的问题；为了解决直播声音大而聊天声音比较小的问题，我们在边看边聊场景中调整了远端人声和直播声音的音量配比，保障远端聊天人声和直播声音响度基本持平。</p><p></p><p>为了更进一步避免比赛声音和远端人声冲突导致无法听清聊天内容的问题， 我们引入了智能音频闪避算法。音频闪避（Audio Ducking）的功能是当检测到 A 信号出现时，将 B 信号的电平降低，仿佛 B 信号「躲避」了 A 信号，因此得名「闪避」。闪避算法非常适合在「边看边聊」和「游戏直播」场景中开启，在边看边聊场景中，A 信号是远端用户人声，B 信号是播放器播放的比赛声音。开启闪避功能后，RTC 在收到远端语音时，将播放的直播声音进行闪避，能让用户更清晰地听到远端好友的语音， 经过验证，达到了非常好的音频体验。</p><p></p><p>边看边聊场景添加智能音频闪避后的音频处理流程如图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b4eb657acd117656cbc8c045ea58fdbf.png\" /></p><p></p><p>关于智能音频闪避功能中的音频增益控制，还有一些经验性原则需注意：</p><p></p><p>增益下降应足够快，否则语音的开始片段仍会被音乐掩蔽；但又不能过快，导致出现音质问题；增益下降后，应该保持足够的时间，等待人声消失一段时间后再恢复，否则，正常讲话的停顿会频繁触发闪避效果，体验很差；增益的恢复可以稍慢，不要给人很突然的感觉；要对远端人声进行智能识别检测，避免远端噪声引起过度闪避。</p><p></p><p></p><h1>5. &nbsp;总结与展望</h1><p></p><p></p><p>火山引擎 RTC 边看边聊场景解决方案， 通过 RTC 公共流 + RTC 有房间无缝切换的方案，在兼顾实时音视频体验基础上，支持了单流超大规模的并发，降低了用户拉流数量，不仅提高了观赛机型渗透率也提高了 RTC 系统容量；针对世界杯观赛用户集中进房 ，集中退房的特点，RTC 服务端制定了 “边缘限流”，“中心限流” ，“信令平滑发送” 等重保策略，提高了 RTC 服务在高 QPS 场景下的稳定性；使用直播流音频托管给 RTC 播放的方案，解决了双端同时播放直播流音频引入的回声问题；使用外放媒体模式 + 软件 3A 方案，在兼顾回声消除基础上保障了高音质边看边聊体验；通过调整音量配比和智能音频闪避功能，解决了直播流声音大， 聊天声音小的问题。经线上打磨验证，方案设计合理有效，为世界杯边看边聊观赛体验提供了有力的保障。</p><p></p><p>更进一步，在一起看短视频、一起看电影等场景，业务还可以通过实时信令(RTS)来对房间中各个用户观看进度进行集中控制，保障房间中用户观看相同内容；业务还可以选择打开视频， 更进一步增加好友之间的观影、观赛体验；在UGC 大咖解说场景，我们还可以支持用户上麦与主播进行互动聊天，更进一步拉近主播和观众的距离，实现更好的互动效果。</p><p></p><h1>6. &nbsp;Demo 和场景搭建</h1><p></p><p></p><p>我们将支持边看边聊的材料整理成了场景化解决方案文档和 Demo， 供有需要的开发者来快速实现自己的业务场景， 欢迎广大开发者点击阅读原文参考和体验。</p><p></p><h1>7. &nbsp;加入我们</h1><p></p><p></p><p>火山引擎 RTC，致力于提供全球互联网范围内高质量、低延时的实时音视频通信能力，帮助开发者快速构建语音通话、视频通话、互动直播、转推直播等丰富场景功能，目前已覆盖互娱、教育、会议、游戏、汽车、金融、IoT 等丰富实时音视频互动场景，服务数亿用户。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/8568d5debb62851a85faa8d5b2d6f01e.png\" /></p><p></p><p>🏃 扫描上方二维码，赶紧加入我们吧！</p>",
    "publish_time": "2022-12-07 16:25:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "BSN 推出城市算力中心：让云服务基础设施更加去中心化",
    "url": "https://www.infoq.cn/article/ctVV0B7uaLFgKQ8c212k",
    "summary": "<p></p><p>据悉，<a href=\"https://www.infoq.cn/article/PpLCOtk9pw9gRX9BM3t3\">区块链服务网络（BSN）</a>\"将在本月底进行的“季度版本更新”中发布一项重量级技术服务，即为 <a href=\"https://ddc.bsnbase.com/\">BSN-DDC</a>\" 基础网络（后简称为DDC网络）推出基于 BSN 虚拟数据中心技术的“BSN-DDC城市算力中心”。其中，算力中心将包含三个重要部分：算力中心运营管理系统、DDC网络开放联盟链外部节点和算力中心终端用户门户。</p><p>&nbsp;</p><p>DDC 网络由 BSN 在今年1月份推出，由多条技术体系完善并各具特点的 BSN 开放联盟链组成，去掉了虚拟货币、加入了实名制，通过部署具备跨链机制的 BSN 官方 DDC 智能合约，向有 DDC/NFT 业务需求的平台方提供快速接入服务，使其可灵活地对 DDC/NFT 的生成、更新、转移和销毁进行管理。</p><p>&nbsp;</p><p>BSN-DDC城市算力中心可以理解为 DDC 网络的自主接入系统，运营方可以通过在本地云环境内安装 BSN 算力中心软件，并在算力中心内部署 DDC 网络上各条开放联盟链的节点，成为 DDC 网络的一部分。所有软件均由 BSN 发展联盟免费提供，并基本开源。</p><p></p><h4>打造互联网新的一层：公共IT系统层</h4><p></p><p>&nbsp;</p><p>现在互联网的数据中心、操作系统、数据库、协议，都是搭建在后台的IT系统，分别属于其背后的组织。BSN 认为，未来互联网上会出现新的一层，即互联网公共层，届时公共IT系统会与现在的私有化系统共存。</p><p><img src=\"https://static001.infoq.cn/resource/image/21/37/213yye7b200d387d8561119b4eyye537.png\" /></p><p></p><p>&nbsp;作为一个与互联网类似的公共基础设施，BSN 需要具备两个特点：第一是便宜，第二是具有广泛性，即要能够真正服务千行百业，覆盖任何地方的用户。BSN 要做的事情就是构建一套与互联网平行的、可以搭建公共IT系统的一整套环境，分别设置负责云资源、操作系统（链方）、数据库、协议、硬件等的角色。</p><p>&nbsp;</p><p>BSN 迄今为止已经运行4年多的时间，目前以BSN公网（包含BSN-DDC基础网络、<a href=\"https://www.infoq.cn/article/ZmCKKVkbJqIcAhR6Eb89\">BSN Spartan</a>\"网络）和BSN专网两大技术体系为主，致力于打造全球“公共信息化系统”基础设施网络。其中BSN Spartan 网络是针对海外的无币公链基础设施，于今年9月发布，3个月公测期结束后现已进入商用阶段。BSN 专网则更加商业化、私有化，企业可以通过管理门户全面管理区块链应用，例如统一控制所有应用的数据权限和ID权限等。</p><p>&nbsp;</p><p>因此也可以看出，现在的 BSN 不是链也不是应用，实际上是一个去中心化的分布式云服务基础设施。其中，区块链在BSN中相当于操作系统，NFT 有些像未来支持公共IT系统和分布式应用的一种数据库形态。</p><p>&nbsp;</p><p>如今 BSN 城市算力中心的推出，则从治理模式、服务广泛性等方面为BSN带来了质的飞跃。</p><p></p><h4>算力中心，将服务能力分散出去</h4><p></p><p>&nbsp;</p><p>从治理模式角度看，国内的各种链，从技术端到服务端，都是一家主体在控制，本质上更多是“私有链”，不是分布式和共管、共治、共建的，实际上在推出算力中心之前，DDC 网络在服务端也属于单点接入的。</p><p>&nbsp;</p><p>BSN 发展联盟秘书长谭敏表示，BSN 算力中心推出后，BSN 在服务端，将由原来的单点接入变为多方共建，大家拥有各自的网关，并且代码是开源的，算力中心运营方可以去建立独立的业务形态。这从根本上打破了原有模式，有助于真正形成去中心化、分布式的算力基础设施。”</p><p>&nbsp;</p><p>具体看， DDC网络是由多条开放联盟链组成的，开放联盟链是不断趋近于公链开放程度的联盟链。网络的共识节点由联盟各方搭建，准入要求较高。为了达到开放性，业务全节点是开放的，唯一要求在于需经过KYC，因此，该部分会通过网关来控制，这里的网关与互联网的接入概念是一样的，如果网关对 TPS 进行限制 ，那么接入网络的应用性能都会受影响。</p><p>&nbsp;</p><p>因此，如果各方的运营者们，如链方、运营方等，都可以建立算力中心，通过自己的网关把网络服务能力分散到各个运营点。运营者的网关限制可以各自定义，只要符合整个 BSN 网络的技术标准即可。这与在分布式云中，把不同服务器搭建到不同的位置，访问就近服务器是一样的分布式逻辑。</p><p>&nbsp;</p><p>目前，BSN-DDC 基础网络每天智能合约的调用次数超过100万，已经超过以太坊。算力中心推出以后，运营者在处理业务时可以调用算力中心运营方自己的网关，不再受 BSN 网关的 TPS 限制，性能就会得到极大的提升。</p><p>&nbsp;</p><p>另外，同一个算力中心内，运营者可以选择部署同一条链的一个或多个节点，也可以选择部署多条链的节点，来提升性能，很好地解决排队等问题。</p><p>&nbsp;</p><p>从服务的广泛性上来说，算力中心推出以后，每个算力中心的门户与BSN官方门户的权利几乎一样，算力中心的运营方可以将自己原来的服务放入门户里，也可以通过门户向用户提供BSN相关服务，并且还可以根据需求自定义门户的外观、功能等。</p><p>&nbsp;</p><p>谭敏透露，BSN 预计明年将推出一套灵活调用的多方共识，即不采取全网共识，使用仅几方共识或可信状态下零共识，这将使链的性能产生质的飞跃。另外，BSN 也将持续优化网络，形成一个与HTTP协议类似的、支持广播式传输逻辑的协议。</p><p></p><h4>结束语</h4><p></p><p>&nbsp;</p><p>“经常有人问我，‘你们是红海还是蓝海？’我说我们是在挖海。”谭敏表示，现在 BSN 是在创造新行业、挖掘新商机。</p><p>&nbsp;</p><p>但谭敏也表示，未来 BSN 还有很多工作要做。“大城市还好一点，但再往下延伸，特别是到了四、五线城市，好多人其实不清楚也不了解。所以，我认为我们未来的难点和重点，就是要不停地锲而不舍地去科普。”</p><p>&nbsp;</p><p></p><p></p>",
    "publish_time": "2022-12-07 16:32:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]