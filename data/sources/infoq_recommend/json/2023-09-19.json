[
  {
    "title": "亚马逊云科技推出基于生成式AI的临床文档工具HealthScribe预览版",
    "url": "https://www.infoq.cn/article/8SHsU6p460shd9drTEWg",
    "summary": "<p>最近，亚马逊云科技新推出了一项符合<a href=\"https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act\">HIPAA</a>\"标准的服务，名为<a href=\"https://aws.amazon.com/healthscribe/\">AWS HealthScribe</a>\"。该服务尚处于预览状态，它可以利用语音识别和生成式人工智能（基于<a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>\"）来生成临床文档。</p><p>&nbsp;</p><p>按照该公司的说法，AWS HealthScribe是<a href=\"https://aws.amazon.com/machine-learning/ml-use-cases/conversational-ai/\">会话式</a>\"和<a href=\"https://aws.amazon.com/generative-ai/\">生成式人工智能（AI）</a>\"的结合体，可以减轻编写临床文档的负担并改善咨询体验。借助这项服务，用户可以利用一整套的人工智能功能来加快临床应用中临床文档的编制。</p><p>&nbsp;</p><p>AWS HealthScribe为医疗卫生软件提供商提供了一个API，可以自动生成完整的记录及提取关键的详细信息（如医疗术语和药物），并根据医患之间的讨论创建可输入电子健康记录（EHR）系统的摘要。</p><p>&nbsp;</p><p>例如，在HealthScribe中创建的笔记可以通过人工智能进行补充，包括诸如就诊原因、当前病史、评估和随访等细节。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a9c0b5e197682706be65521de813caf8.png\" /></p><p>示例：医疗卫生软件开发人员可以使用AWS HealthScribe为用户提供的应用程序体验（图片来源：<a href=\"https://aws.amazon.com/blogs/industries/industries-introducing-aws-healthscribe/\">AWS for Industries博文</a>\"）</p><p>&nbsp;</p><p><a href=\"https://aws.amazon.com/blogs/industries/industries-introducing-aws-healthscribe/\">AWS for Industries博文</a>\"的作者是这样描述AWS HealthScribe的好处的：</p><p></p><blockquote>通过功能整合，AWS HealthScribe减少了训练、优化、集成单个的人工智能服务和构建自定义模型的需求，加快了实施速度。客户可以专注于为最终用户提供价值，而不必费力优化单个的AI组件。</blockquote><p></p><p>&nbsp;</p><p>另一方面，虽然该服务符合HIPAA，但公司必须签署一份称为商业伙伴附录的合同。对此，<a href=\"https://aws.amazon.com/compliance/hipaa-compliance/\">AWS的文档</a>\"做了详细说明，只有这样才能完全符合HIPAA。</p><p>&nbsp;</p><p>除了AWS，微软和谷歌也有像AWS HealthScribe这样的医疗卫生服务。例如，云服务<a href=\"https://www.microsoft.com/en-us/research/project/health-bot/\">Microsoft Healthcare Bot</a>\"就使得医疗卫生组织能够构建和部署可用于各种目的的会话代理，例如分诊和症状检查。还有<a href=\"https://cloud.google.com/healthcare-api\">Google Cloud Healthcare API</a>\"，该服务提供了一套基于Google Cloud Platform构建的专门用于医疗卫生领域的产品和服务。</p><p>&nbsp;</p><p><a href=\"https://twitter.com/Berci\">Bertalan Meskó</a>\"是医学未来学家协会（Medical Futurist Institute）的主任、哲学和医学博士，他在<a href=\"https://www.linkedin.com/posts/bertalanmesko_amazon-launches-generative-ai-based-clinical-activity-7090247186696802304-TeK2\">LinkedIn的一篇帖子</a>\"中评论道：</p><p></p><blockquote>看到科技巨头进军医疗卫生领域非常令人兴奋，我们都应该为此感到高兴，因为他们比医疗卫生/制药公司更擅长创造人们想要的技术。</blockquote><p></p><p>&nbsp;</p><p>此外，Batch首席执行官<a href=\"https://twitter.com/Virtualgoodz/status/1684892459682160640\">Simon Dawlat在推特上写到</a>\"：</p><p></p><blockquote>随着亚马逊加入微软/谷歌的竞争行列推出HealthScribe，基于人工智能的临床文档API淘金热正如火如荼地进行着——然而，与那些高度专注的公司（如@NablaTech）所提供的产品相比，所有FAANG的产品都显得有些尴尬。&nbsp;比赛开始了！</blockquote><p></p><p>&nbsp;</p><p><a href=\"https://www.nuance.com/healthcare.html\">Nuance</a>\"和<a href=\"https://www.cerner.com/\">Cerner Corporation</a>\"（Oracle）等公司也提供了其他一些类似的解决方案，前者为医疗卫生和客户互动提供了对话式人工智能，后者则是医疗卫生信息技术解决方案、服务、设备和硬件供应商。</p><p>&nbsp;</p><p>最后，AWS HealthScribe目前仅在美国东部（弗吉尼亚州北部）地区可用，客户可以<a href=\"https://pages.awscloud.com/GLOBAL-other-PT-AWS-HealthScribe-Preview-2023-reg.html\">填写表单完成注册</a>\"后访问该服务。了解定价细节可以查看<a href=\"https://aws.amazon.com/healthscribe/pricing/\">定价页面</a>\"，了解其他细节可以查阅<a href=\"https://docs.aws.amazon.com/transcribe/latest/dg/health-scribe.html\">文档</a>\"。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/08/aws-healthscribe-ai-preview/\">https://www.infoq.com/news/2023/08/aws-healthscribe-ai-preview/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/2SjM2FZqw7TB6SdxDaW7\">亚马逊云科技re:Inforce 2023中国站：企业如何提高数据、模型和应用安全？</a>\"</p><p><a href=\"https://www.infoq.cn/article/bHbaxQSIexCa63uzkJGl\">降本增效：Grab如何在亚马逊云科技上将Kafka消费者流量成本降到零</a>\"</p>",
    "publish_time": "2023-09-19 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "探寻互联网发展：如何利用“大模型+大数据”加速产业革新？ | 华为云联创营 MVP 专访",
    "url": "https://www.infoq.cn/article/8NZ6uCqWXg8W8OZqisGa",
    "summary": "<p>随着互联网行业的发展进入新阶段，互联网企业正在积极寻求外部增长机会，以推动业务的持续创新。在消费互联网时代，大多平台都是通过流量变现，然而当前互联网红利逐渐减少，“在保证流量基础上，降本增效的同时还要提高服务质量”成为了互联网平台角逐下半场的关键点，各大互联网平台向产业互联网转型已成为必然趋势。</p><p></p><p>更注重企业与产业的高质量发展的“产业互联网”，力求通过一切手段优化资源配置、降低交易成本、提高生产和服务效率。其中，大模型技术是推动消费互联网向产业互联网转型的关键技术之一。基于大模型，企业可以更加精准地预测用户需求，提供个性化的服务，并更好地优化生产和服务流程，提高生产效率和质量，成为了产业存量变革的来源，而企业也据此寻找到了第二增长曲线。可以说，大模型技术的发展将加速互联网产业升级。</p><p></p><p>大模型发展的前置条件是大数据技术的发展，超大参数规模的 AI 模型需要海量数据提供训练资源。行内人普遍认为，“大模型 + 大数据”的组合就像孪生双子，是推动世界向全面数字化目标迈进不可或缺的重要力量。在这样的背景下，很多企业技术先行者都正在积极努力，竭尽全力地将大模型 + 大数据的澎湃能量运用到生产实践中，实现真正的“降本增效、提升客户价值、改善用户体验”。在这些技术先行者中有这样一群人，他们有一个共同的身份——<a href=\"https://www.infoq.cn/article/ws2KFcwij5lcTV1v9WaU\">华为云联创营 MVP</a>\" （以下简称“ MVP”）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/54/54165e02e7301053d5107e3f497d3021.jpeg\" /></p><p></p><p>“MVP 计划” 于 2023 年 3 月正式发布，经过重重选拔，数十位来自行业头部企业的 MVP 成员脱颖而出。MVP 的定位是“产业数字化的实践者”、“创新技术的布道师”、“不断探索边界的实干家”，旨在从产业数字化的实际问题出发，携手共探促进更多行业解决方案的孵化，丰富行业技术生态，让数字化价值更普惠共享、更公平可及。近期，InfoQ 与其中 2 位 MVP 代表成员围绕“AI 大模型 + 大数据”主题进行了交流， 一起探索了 MVP 们背后的故事。</p><p></p><p></p><h2>一、37 手游技术总监陶学诚：“大模型技术创新需要行业共同探索”</h2><p></p><p></p><p>37 手游是三七互娱集团旗下专注于移动游戏运营的标杆品牌，累计运营超过 2000 款游戏。作为 37 手游技术总监，陶学诚重构了公司原有的数据系统，在公司事业群层面实施了数字化迭代，构建了完整的数据平台，同时积极打通内外部数据通道。目前，他正在带领 37 手游技术团队，以公司事业群数字化转型工作为基础，全面推动产业数字化进程。</p><p></p><p>37 手游是华为云联创营活动的“常客”，陶学诚通过第七期<a href=\"https://www.infoq.cn/article/HG6u2XkBQsRbHAcx8c07\">华为云</a>\"联创营 CTO 领航班了解到了“MVP 计划”。谈及从“了解”到成为该计划的一员，他第一时间想到的是——与华为云联创营共同探索、共同成长。他负责的团队之前主要面向公司内部，华为云联创营为其提供了与外界加强交流学习的机会。他希望通过这样的形式提升自己和团队在行业内的技术影响力，同时也能收获更多行业优秀实践反哺企业，帮助团队进一步提升。所以当该计划“号召各领域的‘价值专家’加入到计划中，与华为云联创营一起进行第三方公益技术沙龙布道、技术交流，积极总结个人实践经验，完成产业技术布道，为更多人、企业乃至产业发展赋能”时，他毫无犹豫地选择加入了该计划。</p><p></p><p>如今，AI 和大模型技术在手游行业的应用已经风生水起，陶学诚也在积极为行业大模型的构建做出自己的努力。目前他观察到，手游领域主要有三大 AI 应用场景——游戏资产的 AI 绘图生成、在游戏内容中构建可以与玩家进行自然对话的 NPC 或虚拟玩家、代码的 AI 辅助编程。此外，大模型在文案生成、营销策划、智能客服等层面也开始有实践应用，同时“打击黑产”工作中也可以利用 AI 技术对其进行行为识别。</p><p></p><p>37 手游作为游戏发行商，主要希望通过大模型技术来提升运营效率，并丰富游戏内容生态。在具体实践中，37 手游团队主要使用大模型来生成游戏发行运营过程中所需的大量宣传素材、优化数据分析管道。与此同时，团队还发现，大模型和其他 AI 技术在内容生态改善方面潜力十分突出，比如运营方可以使用 AI 技术制作虚拟玩家，帮助真实玩家加快拼局速度，改善持续游玩体验；又比如，基于 AI 的内容过滤系统则可以有效管制游戏内的对话交流平台，实时屏蔽恶意、负面的聊天内容。</p><p></p><p>大模型的应用与大数据是相辅相成的，37 手游之前就在陶学诚的主导下构建了自己的数据平台，获得了更快的数据分析能力，为将来的大模型应用打下了良好基础。随着数据规模不断扩大，模型训练会变得更加容易，效果也会更加令人满意。</p><p></p><p>目前陶学诚并没有计划带领团队自研底层大模型，而是选择了与行业优秀的大模型合作构建上层应用级私有模型。他认为，AI 与公有云的能力是高度绑定的，大部分公司的运维能力和基础设施更新速度都很难跟得上公有云迭代速度。正因如此，对于 37 手游这样的企业来说，基于行业优秀大模型展开研发可以有效降低门槛，使企业可以将主要精力集中在模型的行业应用，以解决企业运营中存在的实际问题。</p><p></p><p>他在采访中也提到，“目前大模型的应用还存在很多实际问题。例如，基于 AI 辅助生成的代码可能存在一些难以预期的 Bug，因此不太适用于同业务强挂钩、问题描述不清的复杂场景。整个行业对大模型的应用落地都处于探索阶段，需要参与其中的企业多尝试、多分享，才能推动行业在这一方面持续发展，解决现存的诸多挑战。”</p><p></p><p>从业者的观念转变是陶学诚眼中大模型实践应用的另一大障碍。在大量以编码为工作的技术团队中，编码能力被认为是一项很有壁垒和创意的工作，而当 AI 编码工具可以很快速的写出很优质代码的时候，传统的程序开发者往往对大模型的应用存在抵触心理。如何转变他们的思路，让他们能够在工作中自然而然地运用 AI 技术提升效率，是行业共同面对的难题。他表示，希望在未来能够基于自己手中在做的技术实践，去推动解决大模型在行业应用中出现的种种难题。他坚信：“随着上述问题在未来逐渐得到解决，AI 大模型技术在游戏产业中的潜力是不可估量的。”</p><p></p><p></p><h2>二、汽车之家 CTO 项碧波：“构建大模型技术与行业应用间的桥梁”</h2><p></p><p></p><p>项碧波在搜索引擎、商业广告、推荐系统、自然语言处理及数据挖掘领域具备丰富的经验。加入汽车之家后致力于打造企业的数据和技术能力，构建用户数据生态和经营数据生态，助力主机厂与经销商业务流程数字化，赋能汽车产业数字化转型。</p><p></p><p>作为汽车之家 CTO，项碧波需要为“一年一度”的 818 晚会做好技术支持工作，华为云在其中提供的一系列“高可用”能力，给他留下了深刻的印象。后来随着合作的深入，他多次参加华为云联创营的活动，华为云联创营过去为不同行业和领域的专家创造了很多分享机会，这也为他带来了不少新启发，让汽车之家获取到了很多可用资源。当他在今年 3 月了解到了“MVP 计划”，一下就被该计划驱动产业数字化发展的理念所击中，而他过去也一直希望通过自己的努力来推动技术发展，两者不谋而合，第一时间便加入到了计划中来。</p><p></p><p>一直以来，汽车之家的企业定位都是“沟通消费者与汽车厂商的门户与桥梁”。因此，项碧波带领的技术团队非常重视 AI 技术创新，并在日常经营中有着大量应用。例如，汽车之家的 AI 智能推荐可以为几千万用户提供 200 多个智能推荐场景，在数十亿内容中快速、精准地提供汽车资讯。团队还基于大语言模型和多模态内容理解等技术建立了多层级内容标签体系，提升推荐系统的精准度与质量。此外，汽车之家的智能客服、智能搜索服务应用了自然语言处理、机器学习等技术加快响应速度、解决用户问题，并运用大数据分析和 AI 预测能力的智能营销系统为广告投放创造了更高的转化渠道。</p><p></p><p>2022 年，汽车之家品牌代言人谷爱凌的“数字人“形象与汽车之家 AI 体验官宫玖羽相继上线。谈到这两位数字人，项碧波最自豪的是团队在行业现有技术的基础上做了很多突破。首先，数字人的背后都有着汽车之家大语言模型提供的智能化支撑，能够帮助用户解答很多专业汽车领域的问题；其次，汽车之家在语音识别与合成方面的积累也运用到了数字人上，甚至能够准确识别四川话、粤语等方言；同时，为解决用户设备性能参差不齐的问题，数字人还运用了云端 3D 渲染技术来为用户带来高清、流畅的一致体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9ca6f9466c72578686b8f2240b7ea4f1.jpeg\" /></p><p></p><p>全新打造的能源空间站是汽车之家 2022 年实现的另一大创新成果。空间站中的全息体验仓利用全息投影技术，可以让用户一次体验多款经过数字建模的车型，选购效率大大提升。同时大数据与 AI 智能语音的赋能确保了全息投影的车体细节准确，也为用户带来了更真实的互动体验。</p><p></p><p>在以上提到的多个实践过程中，汽车之家技术团队充分意识到了 AI 技术对业务增效与企业长期发展的重要意义，项碧波也非常重视“大模型”这样的行业创新，带领团队在这一领域投入了大量资源。汽车之家目前正在内测一款名为“仓颉”的自研行业大模型，该模型是汽车之家技术团队将大模型技术运用在汽车行业垂直领域的成果，主要解决用户选车决策时面临的问题。对此，项碧波介绍到：“未来用户无需通过传统的搜索、浏览、筛选过程来选择车型，只需直接告诉了仓颉自己的用车场景、家庭人口、日常通勤距离、成本预算等信息，模型就可以直接给出非常适合的选项。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cd432ad9a26b74830e65521d57cdc578.png\" /></p><p></p><p>据悉，仓颉大模型在海量通用数据学习的基础上，利用汽车之家 18 年来积累的亿级别之家自有的高质量汽车领域专业数据，通过数据的筛选和有效利用增强了汽车领域模型的理解能力，在汽车领域问答打分评测仓颉大语言模型优于 ChatGPT （提升 13%）。值得一提的是，该模型在汽车垂直领域训练中，平衡通用能力和汽车领域能力，减少在通用的能力遗忘，并对训练的效率进行了多方法提升，使得该模型在通用能力方面，与开源的同级别模型相比，性能显著提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b63dad4174cb709bf57e6086e241cbf6.png\" /></p><p></p><p>此外，汽车之家还开发了基于全链路智能营销的 AIGC 模型，覆盖了文案策划、创意视频内容定制等营销环节，可以帮助车企降本增效，带来更高的销售转化率。接下来，汽车之家还将进一步拓展大模型 + 大数据的应用场景，围绕具体的业务需求推动应用落地。项碧波认为，“汽车之家打造的大模型与相关产品相当于行业的一种基础设施，可以为 AI 产业和汽车行业应用之间建立起一座桥梁，降低 B 端和 C 端客户应用先进技术的门槛。”</p><p></p><p></p><h2>三、“互联网产业升级”是智能化时代的商业变革</h2><p></p><p></p><p>虽然以上两位 MVP 来自两个不同的行业，但他们对于大模型 + 大数据的行业应用却有着很多共同的观点。两位专家都高赞了大模型技术在垂直领域的应用潜力，并纷纷基于各自的业务实践，努力将这种潜力化为可衡量的价值。同时，两位技术专家都认可大数据与大模型应用“相辅相成”的关系，他们都认为——高水平的大数据底座是企业推进互联网产业化进程的前提，也是大模型能力落地的重要保障。</p><p></p><p>在这个背景下，企业不仅需要重视技术创新和应用，更需要在行业交流中持续吸收新知识、新成果，积极推动这些技术的业务落地和发展。但还需要注意的是，当前大模型正处于技术发展早期阶段，其在行业内的落地刚刚开始，无论是大模型研发厂商还是使用大模型的用户都缺乏足够的经验和实践。另一方面，大模型的产业应用潜力巨大，有希望充分调动现有海量数据的内在价值、简化工作流程、加强用户体验，但大模型在实际部署中遇到的很多问题和挑战也是难以预期的。而“MVP 计划”恰恰给了来自不同行业的企业技术贡献者一个推动行业“同交流、共使力”的机会，大家一起面对挑战，共同推进产业数字化发展。目前，MVP 计划第三期学员已启动招募工作，期待更多行业专家加入。</p><p></p><p>正如两位专家在访谈中所提到的那样，<a href=\"https://www.infoq.cn/article/FgogdjehRCRt9QjD4fMG\">华为云</a>\"在与伙伴的合作过程中展现的“开放共享、持续创新、一直与伙伴保持前沿的技术讨论”和“赋能千行百业的具体行动”是吸引各家企业加入华为云联创营的原因所在。就比如现在华为云的盘古大模型已迭代到了 3.0 版本，为大模型技术赋能千行百业按下了加速键。汽车之家 CTO 项碧波认为盘古大大降低了行业企业利用大模型技术改善业务的门槛；37 手游技术总监陶学诚也认为盘古这样的大模型未来将帮助越来越多的产业实现数智化重构。可以说，随着 AI 大模型、大数据等技术的发展，互联网与产业的融合将进一步加快。</p><p></p><p>众所周知，互联网产业升级不仅仅是技术的升级，更是一场深层次的商业变革，在这过程中，产业数字化需求端与数字技术供应侧，还存在着一条巨大的鸿沟，需要技术创新者和产业数字化深耕者不断双向奔赴。在这背景下，华为云将于 9 月 21 日，华为全联接大会 2023 期间，举办“华为云互联网产业峰会 &amp;AIGC 高峰论坛”，与互联网先锋企业、产业专家、技术大咖共探互联网产业升级之道，邀请各界同仁一起思想碰撞。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c26aa7a504e9e7250b0731e9d90b81d6.jpeg\" /></p><p></p>",
    "publish_time": "2023-09-19 08:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java并发Map的面试指南：线程安全数据结构的奥秘",
    "url": "https://www.infoq.cn/article/3c8eb933384aba7b39a2ff757",
    "summary": "<p></p><h2>简介</h2><p></p><p>在计算机软件开发的世界里，多线程编程是一个重要且令人兴奋的领域。然而，与其引人入胜的潜力相伴而来的是复杂性和挑战，其中之一就是处理共享数据。当多个线程同时访问和修改共享数据时，很容易出现各种问题，如竞态条件和数据不一致性。</p><p></p><p>本文将探讨如何在Java中有效地应对这些挑战，介绍一种强大的工具——并发Map，它能够帮助您管理多线程环境下的共享数据，确保数据的一致性和高性能。我们将深入了解Java中的并发Map实现，包括ConcurrentHashMap和ConcurrentSkipListMap，以及其他相关的知识点。无论您是初学者还是有经验的开发人员，都会在本文中找到有关并发编程的有用信息，以及如何在项目中应用这些知识的指导。让我们开始这个令人兴奋的多线程之旅吧！</p><p></p><h2>并发问题</h2><p></p><p>在深入了解并发Map之前，让我们首先探讨一下多线程编程中常见的问题。在多线程环境中，多个线程可以同时访问和修改共享数据，这可能导致以下问题：</p><p></p><h3>1. 竞态条件</h3><p></p><p>竞态条件是指多个线程试图同时访问和修改共享数据，而最终的结果取决于线程的执行顺序。这种不确定性可能导致不一致的结果，甚至是程序崩溃。</p><p></p><p><code lang=\"text\">class Counter {\n    private int value = 0;\n\n    public void increment() {\n        value++;\n    }\n\n    public int getValue() {\n        return value;\n    }\n}\n</code></p><p></p><p>在上面的示例中，如果两个线程同时调用increment方法，可能会导致计数器的值不正确。</p><p></p><h3>2. 数据不一致性</h3><p></p><p>在多线程环境中，数据的不一致性是一个常见问题。当一个线程修改了共享数据，其他线程可能不会立即看到这些修改，因为缓存和线程本地内存的存在。这可能导致线程之间看到不同版本的数据，从而引发错误。</p><p></p><h3>为什么需要并发Map？</h3><p></p><p>现在，您可能会想知道如何解决这些问题。这就是并发Map派上用场的地方。并发Map是一种数据结构，它专为多线程环境设计，提供了一种有效的方式来处理共享数据。它允许多个线程同时读取和修改数据，同时确保数据的一致性和线程安全性。</p><p></p><h2>Java并发Map的概述</h2><p></p><p>现在，让我们深入了解Java标准库中提供的不同并发Map实现，以及它们的特点和适用场景。</p><p></p><h3>1. ConcurrentHashMap</h3><p></p><p>ConcurrentHashMap 是Java标准库中最常用的并发Map实现之一。它使用分段锁（Segment）来实现高并发访问，每个分段锁只锁定一部分数据，从而降低了锁的争用。这使得多个线程可以同时读取不同部分的数据，提高了性能。</p><p></p><p><code lang=\"text\">ConcurrentMap map = new ConcurrentHashMap&lt;&gt;();\nmap.put(key, value);\nValueType result = map.get(key);\n</code></p><p></p><p>ConcurrentHashMap适用于大多数多线程应用程序，尤其是读多写少的情况。</p><p></p><h3>2. ConcurrentSkipListMap</h3><p></p><p>ConcurrentSkipListMap 是另一个有趣的并发Map实现，它基于跳表（Skip List）数据结构构建。它提供了有序的映射，而不仅仅是键值对的存储。这使得它在某些情况下成为更好的选择，例如需要按键排序的情况。</p><p></p><p><code lang=\"text\">ConcurrentMap map = new ConcurrentSkipListMap&lt;&gt;();\nmap.put(key, value);\nValueType result = map.get(key);\n</code></p><p></p><p>ConcurrentSkipListMap适用于需要有序映射的情况，它在一些特定应用中性能表现出色。</p><p></p><h3>3. 其他Java并发Map实现</h3><p></p><p>除了ConcurrentHashMap和ConcurrentSkipListMap之外，Java生态系统还提供了其他一些并发Map实现，例如Google Guava库中的ConcurrentMap实现，以及Java 8中对ConcurrentHashMap的增强功能。另外，还有一些第三方库，如Caffeine和Ehcache，提供了高性能的缓存和并发Map功能。</p><p></p><h2>ConcurrentHashMap详解</h2><p></p><p>现在，让我们深入研究ConcurrentHashMap，了解它的内部实现和线程安全机制。</p><p></p><h3>内部实现</h3><p></p><p>ConcurrentHashMap的内部实现基于哈希表和分段锁。它将数据分成多个段（Segment），每个段都是一个独立的哈希表，拥有自己的锁。这意味着在大多数情况下，不同段的数据可以被不同线程同时访问，从而提高了并发性能。</p><p></p><h3>常用操作</h3><p></p><p>ConcurrentHashMap支持许多常见的操作，包括put、get、remove等。下面是一些示例：</p><p></p><p><code lang=\"java\">ConcurrentMap map = new ConcurrentHashMap&lt;&gt;();\nmap.put(key, value);\nValueType result = map.get(key);\nmap.remove(key);\n</code></p><p></p><p>这些操作是线程安全的，多个线程可以同时调用它们而不会导致竞态条件。</p><p></p><h3>示例代码</h3><p></p><p>以下是一个简单的示例，演示如何在多线程环境中使用ConcurrentHashMap来管理共享数据：</p><p></p><p><code lang=\"text\">import java.util.concurrent.*;\n\npublic class ConcurrentMapExample {\n    public static void main(String[] args) {\n        ConcurrentMap map = new ConcurrentHashMap&lt;&gt;();\n\n        // 创建多个线程并发地增加计数器的值\n        int numThreads = 4;\n        ExecutorService executor = Executors.newFixedThreadPool(numThreads);\n\n        for (int i = 0; i &lt; numThreads; i++) {\n            executor.submit(() -&gt; {\n                for (int j = 0; j &lt; 1000; j++) {\n                    map.merge(\"key\", 1, Integer::sum);\n                }\n            });\n        }\n\n        executor.shutdown();\n        try {\n            executor.awaitTermination(1, TimeUnit.MINUTES);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n\n        System.out.println(\"Final Count: \" + map.get(\"key\")); // 应该是4000\n    }\n}\n</code></p><p></p><p>在上面的示例中，我们创建了一个ConcurrentHashMap来存储计数器的值，并使用多个线程并发地增加这个值。最终，我们可以得到正确的结果，而不需要显式的锁定或同步操作。</p><p></p><p>ConcurrentHashMap的强大之处在于它提供了高性能的并发操作，同时保持了数据的一致性和线程安全性。在多线程应用程序中，它是一个强大的工具，可用于管理共享数据。</p><p></p><h2>ConcurrentSkipListMap的用途</h2><p></p><p>在本节中，我们将探讨ConcurrentSkipListMap的独特之处以及在某些情况下为什么选择它。同时，我们将演示如何将有序映射与并发性结合使用。</p><p></p><h3>独特之处</h3><p></p><p>ConcurrentSkipListMap是基于跳表（Skip List）数据结构构建的，与传统的哈希表不同。它有以下特点：</p><p></p><p>有序性： ConcurrentSkipListMap中的元素是有序的，按键进行排序。这使得它非常适合需要按键顺序访问数据的场景。高并发性： 跳表的结构允许多个线程并发地访问和修改数据，而不需要像分段锁那样精细的锁定。动态性： ConcurrentSkipListMap具有自动调整大小的能力，因此它可以在数据量变化时保持高效性能。</p><p></p><h3>示例</h3><p></p><p>下面是一个示例，演示了如何使用ConcurrentSkipListMap来存储一组学生的分数，并按照分数从高到低进行排序：</p><p></p><p><code lang=\"text\">import java.util.concurrent.ConcurrentSkipListMap;\n\npublic class StudentScores {\n    public static void main(String[] args) {\n        ConcurrentSkipListMap scores = new ConcurrentSkipListMap&lt;&gt;();\n\n        scores.put(90, \"Alice\");\n        scores.put(80, \"Bob\");\n        scores.put(95, \"Charlie\");\n        scores.put(88, \"David\");\n\n        // 遍历并输出按分数排序的学生名单\n        scores.descendingMap().forEach((score, name) -&gt; {\n            System.out.println(name + \": \" + score);\n        });\n    }\n}\n</code></p><p></p><p>在上面的示例中，我们创建了一个ConcurrentSkipListMap来存储学生的分数和姓名，并使用descendingMap()方法按照分数从高到低遍历和输出学生名单。这展示了ConcurrentSkipListMap在需要有序映射的情况下的优势。</p><p></p><p>ConcurrentSkipListMap通常用于需要高并发性和有序性的场景，例如在线排行榜、事件调度器等。然而，它的性能可能会略低于ConcurrentHashMap，具体取决于使用情况和需求。</p><p></p><h2>其他Java并发Map实现</h2><p></p><p>除了Java标准库中的ConcurrentHashMap和ConcurrentSkipListMap之外，还有其他一些Java并发Map实现，它们提供了不同的特性和适用场景。</p><p></p><h3>1. Google Guava库中的ConcurrentMap</h3><p></p><p>Google Guava库提供了一个名为MapMaker的工具，用于创建高性能的并发Map。这个工具允许您配置各种选项，例如并发级别、过期时间和数据清理策略。这使得它非常适合需要自定义行为的场景。</p><p></p><p><code lang=\"text\">ConcurrentMap map = new MapMaker()\n    .concurrencyLevel(4)\n    .expireAfterWrite(10, TimeUnit.MINUTES)\n    .makeMap();\n</code></p><p></p><h3>2. Java 8中的ConcurrentHashMap增强功能</h3><p></p><p>Java 8引入了一些对ConcurrentHashMap的增强功能，包括更好的并发性能和更丰富的API。其中一个重要的改进是引入了compute和computeIfAbsent等方法，使得在并发环境中更容易进行复杂的操作。</p><p></p><p><code lang=\"text\">ConcurrentMap map = new ConcurrentHashMap&lt;&gt;();\n\nmap.compute(key, (k, v) -&gt; {\n    if (v == null) {\n        return initializeValue();\n    } else {\n        return modifyValue(v);\n    }\n});\n</code></p><p></p><p>这些增强功能使得ConcurrentHashMap更加强大和灵活，适用于各种多线程应用程序。</p><p></p><h3>3. 第三方并发Map库</h3><p></p><p>除了标准库和Guava之外，还有一些第三方库提供了高性能的并发Map实现，例如Caffeine和Ehcache。这些库通常专注于缓存和数据存储领域，并提供了丰富的功能和配置选项，以满足不同应用程序的需求。</p><p></p><h2>性能考虑</h2><p></p><p>在使用并发Map时，性能是一个关键考虑因素。以下是一些性能优化策略，可帮助您充分利用并发Map的潜力。</p><p></p><p>调整并发级别</p><p></p><p>大多数并发Map实现允许您调整并发级别，这决定了底层数据结构中的分段数量。较高的并发级别通常意味着更多的分段，从而减少了锁争用。但请注意，过高的并发级别可能会导致内存开销增加。在选择并发级别时，需要根据实际负载和硬件配置进行评估和测试。</p><p></p><p>选择合适的哈希函数</p><p></p><p>并发Map的性能与哈希函数的选择密切相关。好的哈希函数应该分散键的分布，以减少碰撞（多个键映射到同一个分段的情况）。通常，Java标准库中的并发Map会提供默认的哈希函数，但如果您的键具有特殊的分布特征，考虑自定义哈希函数可能会提高性能。</p><p></p><p>使用合适的数据结构</p><p></p><p>除了ConcurrentHashMap和ConcurrentSkipListMap之外，还有其他并发数据结构，如ConcurrentLinkedQueue和ConcurrentLinkedDeque，它们适用于不同的应用场景。选择合适的数据结构对于性能至关重要。例如，如果需要高效的队列操作，可以选择ConcurrentLinkedQueue。</p><p></p><p>性能测试和比较</p><p></p><p>在项目中使用并发Map之前，建议进行性能测试和比较，以确保所选的实现能够满足性能需求。可以使用基准测试工具来评估不同实现在不同工作负载下的性能表现，并根据测试结果做出明智的选择。</p><p></p><p>在多线程应用程序中，性能问题可能随着并发程度的增加而变得更加复杂，因此性能测试和调优是确保系统稳定性和高性能的关键步骤。</p><p></p><p>性能是多线程应用程序中的关键问题之一，了解并发Map的性能优化策略对于构建高性能的多线程应用程序至关重要。选择适当的并发Map实现、调整并发级别、选择良好的哈希函数以及进行性能测试都是确保应用程序能够充分利用多核处理器的重要步骤。</p><p></p><h2>分布式并发Map</h2><p></p><p>在分布式系统中，处理并发数据访问问题变得更加复杂。多个节点可能同时尝试访问和修改共享数据，而这些节点可能分布在不同的物理位置上。为了解决这个问题，可以使用分布式并发Map。</p><p></p><h3>分布式并发Map的概念</h3><p></p><p>分布式并发Map是一种数据结构，它允许多个节点在分布式环境中协同工作，共享和操作数据。它需要解决网络延迟、数据一致性和故障容忍等问题，以确保数据的可靠性和正确性。</p><p></p><h3>开源分布式数据存储系统</h3><p></p><p>有一些开源分布式数据存储系统可以用作分布式并发Map的基础，其中一些常见的包括：</p><p></p><p>Apache ZooKeeper： ZooKeeper是一个分布式协调服务，提供了分布式数据结构和锁。它可以用于管理共享配置、协调分布式任务和实现分布式并发Map。Redis： Redis是一个内存存储数据库，它支持复杂的数据结构，包括哈希表（Hash）和有序集合（Sorted Set），可以用于构建分布式并发Map。Apache Cassandra： Cassandra是一个高度可扩展的分布式数据库系统，它具有分布式Map的特性，可用于分布式数据存储和检索。</p><p></p><h3>分布式Map的挑战</h3><p></p><p>分布式并发Map面临一些挑战，包括：</p><p></p><p>一致性和可用性： 在分布式环境中，维护数据的一致性和可用性是一项艰巨的任务。分布式系统需要解决网络分区、故障恢复和数据同步等问题，以确保数据的正确性和可用性。性能： 分布式Map需要在不同节点之间传输数据，这可能会引入网络延迟。因此，在分布式环境中优化性能是一个重要的考虑因素。并发控制： 多个节点可能同时尝试访问和修改数据，需要实现适当的并发控制机制，以避免冲突和数据不一致性。</p><p></p><h3>结合分布式Map与其他并发数据结构</h3><p></p><p>在构建复杂的多线程应用程序时，通常需要将分布式Map与其他并发数据结构结合使用。例如，可以将分布式Map用于跨节点的数据共享，同时使用本地的ConcurrentHashMap等数据结构来处理节点内的并发操作。</p><p></p><p>在分布式系统中，设计和实现分布式Map需要深入了解分布式系统的原理和工具，以确保数据的一致性和可用性。同时，也需要考虑数据的分片和分布策略，以提高性能和扩展性。</p><p></p><h2>将并发Map与其他并发数据结构结合使用</h2><p></p><p>在多线程应用程序中，通常需要将并发Map与其他并发数据结构结合使用，以构建复杂的多线程应用程序并解决各种并发问题。以下是一些示例和最佳实践，说明如何将它们结合使用。</p><p></p><h3>1. 并发队列</h3><p></p><p>并发队列（Concurrent Queue）是一种常见的数据结构，用于在多线程环境中进行数据交换和协作。可以使用并发队列来实现生产者-消费者模式，从而有效地处理数据流。</p><p></p><p><code lang=\"text\">ConcurrentQueue queue = new ConcurrentLinkedQueue&lt;&gt;();\n\n// 生产者线程\nqueue.offer(item);\n\n// 消费者线程\nItem item = queue.poll();\n</code></p><p></p><h3>2. 信号量</h3><p></p><p>信号量是一种用于控制并发访问资源的机制。它可以用于限制同时访问某个资源的线程数量。</p><p></p><p><code lang=\"text\">Semaphore semaphore = new Semaphore(maxConcurrentThreads);\n\n// 线程尝试获取信号量\ntry {\n    semaphore.acquire();\n    // 执行受信号量保护的操作\n} catch (InterruptedException e) {\n    e.printStackTrace();\n} finally {\n    semaphore.release();\n}\n</code></p><p></p><h3>3. 读写锁</h3><p></p><p>读写锁是一种用于管理读写操作的锁机制，它允许多个线程同时读取数据，但只允许一个线程写入数据。</p><p></p><p><code lang=\"text\">ReadWriteLock lock = new ReentrantReadWriteLock();\n\n// 读取操作\nlock.readLock().lock();\ntry {\n    // 执行读取操作\n} finally {\n    lock.readLock().unlock();\n}\n\n// 写入操作\nlock.writeLock().lock();\ntry {\n    // 执行写入操作\n} finally {\n    lock.writeLock().unlock();\n}\n</code></p><p></p><h2>最佳实践和注意事项</h2><p></p><p>在多线程编程中，遵循最佳实践和注意事项是确保应用程序的稳定性和性能的关键。以下是一些关键的最佳实践和注意事项：</p><p></p><p>避免锁定整个Map： 尽量只锁定需要修改的部分数据，以减小锁的粒度，提高并发性能。例如，使用分段锁或读写锁来限制对特定部分数据的访问。考虑迭代器的安全性： 当在多线程环境中遍历并发Map时，需要确保迭代器的安全性。某些操作可能需要锁定整个Map来确保迭代器的正确性。避免空值： 注意处理并发Map中的空值。使用putIfAbsent等方法来确保值不为空。异常处理： 在多线程环境中，异常处理尤为重要。确保捕获和处理异常，以避免线程崩溃和数据不一致性。性能测试和调优： 在实际项目中，性能测试和调优是至关重要的步骤。根据实际需求进行性能测试，并根据测试结果进行必要的调整。文档和注释： 编写清晰的文档和注释，以便其他开发人员理解并发Map的使用方式和注意事项。线程安全编程： 线程安全编程是多线程应用程序的基础。确保您的代码符合线程安全原则，避免共享数据的直接访问，使用合适的同步机制来保护共享数据。异常情况处理： 考虑如何处理异常情况，例如死锁、超时和资源不足。实现适当的错误处理和回退策略。监控和日志记录： 添加监控和日志记录以跟踪应用程序的性能和行为。这可以帮助您及时发现问题并进行调整。并发安全性检查工具： 使用工具和库来辅助检查并发安全性问题，例如静态分析工具和代码审查。</p><p></p><p>最后，不要忘记线程安全编程的基本原则：最小化共享状态，最大化不可变性。尽量减少多个线程之间的共享数据，而是将数据不可变化或限制在需要同步的最小范围内。这将有助于减少竞态条件和数据不一致性的可能性。</p><p></p><h2>总结</h2><p></p><p>本文深入探讨了并发Map的概念、实现和性能优化策略。我们介绍了Java标准库中的ConcurrentHashMap和ConcurrentSkipListMap，以及其他Java并发Map实现和分布式并发Map的概念。我们还讨论了将并发Map与其他并发数据结构结合使用的最佳实践和注意事项。</p><p></p><p>在多线程应用程序中，正确使用并发Map可以帮助您管理共享数据，提高性能，并确保数据的一致性和线程安全性。同时，线程安全编程的良好实践是确保应用程序稳定性和可维护性的关键。希望本文对您在多线程编程中的工作有所帮助！</p><p></p><p></p><blockquote>更多内容请参考 <a href=\"https://www.infoq.cn/article/www.flydean.com\">www.flydean.com</a>\"最通俗的解读，最深刻的干货，最简洁的教程，众多你不知道的小技巧等你来发现！欢迎关注我的公众号:「程序那些事」,懂技术，更懂你！</blockquote><p></p>",
    "publish_time": "2023-09-19 09:51:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "用无代码搭建数据中台，竟然如此丝滑",
    "url": "https://www.infoq.cn/article/4a6d0c4bbbfdce748bcda9563",
    "summary": "<p></p><h1>需求背景</h1><p></p><p>企业飞速发展，各个业务部门快速扩张，企业数字化建设变得越来越重要。而开发一个一体化的数据资产管理平台、或者说数据中台系统，变得不可或缺。业务快速发展对企业带来的数据安全匮乏，数据治理成本高，数据口径难统一，数据质量问题多等一系列问题，也随着引入数据中台系统，变得迎刃而解。因为传统的数据中台又重又难用，使用成本高、开发运维成本也高，使用率低、价值不明显。而一个能够跟随自身业务管理灵活变更的轻量级数据中台，则具有更大优势。</p><p></p><h1>系统介绍</h1><p></p><p>要说数据中台用无代码平台构建可能大多数人不信，但smardaten确实有一点不容忽视，就是这个开发平台本身远远不止无代码开发。smardaten是一个以数据驱动的无代码平台，平台的前身就是大数据平台。现在把数据能力作为平台底层核心能力，包含了大多常见的数据处理能力。smardaten主要满足行业级复杂应用的开发，而不是通常的轻量级开发，由于自带大数据底座，数据层面可以减少大量的数据集成、数据清洗、数据治理、接口管理等开发工作，大大减少了业务系统的开发难度和设计难度。所以如果要构建数据资产管理、数据中台、数据集成处理平台等，几乎可以直接用他们工具，也可以通过无代码构建的方式新增应用场景。</p><p></p><p>本次我们做一个数据中台的搭建就使用了无代码平台smardaten。交付要求：6大数据模块、兼顾数据资产，数据服务，数据安全，数据集成，数据标准，质量报告等。</p><p></p><h1>配置说明</h1><p></p><p>因为数据中台中有很多模块是平台自带的，看下数据部分功能架构如图所示，功能比较齐全，满足大多数数据管理场景。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c27a973bc267a792ec24730901135395.jpeg\" /></p><p></p><p>在smardaten界面中，以上数据部分主要功能模块位置大概是这样的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0baeb2b1f7efa921bb83f59be36e4f99.png\" /></p><p></p><p>数据服务界面是这样的，包含内外部服务、服务目录管理和服务内容编排。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d7a37813c6f0a4796f365d0ed3fde2b4.png\" /></p><p></p><p>而当你通过无代码重新组装后，可能就变成了这样。通过添加业务流和逻辑最后支持将数据服务【上架】。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/2326feeeff97d3e2668990fd7d345f52.png\" /></p><p></p><p>所以总的来看，以上显示的功能基本上页面都是现成的，在数据中台组装构建时，需要什么功能就直接导入该页面，将其作为组件直接引用。剩下的功能需求（如表单、流程、仪表盘等）通过其他功能组件快速拖拽、组装即可，全程丝滑体验！</p><p></p><p>下面简单演示下如何引用数据模块和配置新增界面。</p><p></p><h2>1 菜单导航</h2><p></p><p>（1）首先我们通过平台首页上的创建应用，来新建一个空白的应用模板，后续的所有页面，都将在这个模板中一一实现。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/70/708f78b199cb821873bbaeebcdec2fc4.png\" /></p><p></p><p>创建应用（2）然后，我们使用的一键生成导航栏功能。也就是把一二级功能模块按照思维导图的方式记下来，然后一键生成导航菜单。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/686165896c80959cb6906f241ebd888c.png\" /></p><p></p><p>一键生成导航栏</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/1728b2bd5e732b9b835f385b57323048.png\" /></p><p></p><p>绘出导航栏具体样式</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/37/37376f1193d880a9868b08a3f37f06bb.png\" /></p><p></p><p>保存并生成对应页面通过框架设计，我们将整个数据中台系统，拆分成了六个大模块，数据资产，数据集成，数据服务，数据标准，数据标准和数据安全。并通过此功能，导航栏就已经自动生成了。当然也可以通过导航菜单一个个添加多级功能菜单。就像这样。、</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8e4f56023e9e7952e3ca56ebc35d398a.png\" /></p><p></p><h2>2 系统自带组件导入页面</h2><p></p><p></p><h3>（1）数据集成相关组件</h3><p></p><p>数据服务部分的组件，直接可用的包括【数据流】、【数据流编辑】和【调度监控】。其他隐含的系统组件的需要通过【系统组件】添加隐藏的组件路径（常用组件路径需要在平台付费获取）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1973b214b87b0e856dfc4738f2d7fea.png\" /></p><p></p><p>数据处理在平台中又叫做数据流，选择自带的【数据流】组件，可调用数据交换机主界面， 包括数据处理任务分类、数据流列表和控制流列表。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cd617ef13438403e48e9b2c383ea630f.png\" /></p><p></p><p>而具体的数据处理界面、也是数据集成的核心页面，添加【数据流编辑】组件，通过此功能完成数据采集、交换等任务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/01ea5f113141c0f48f3faa1e1e35438b.png\" /></p><p></p><h3>（2）数据服务相关组件</h3><p></p><p>数据服务部分的组件，直接可用的包括【服务搜索】、【服务主题】、【服务部门】和【服务管理】。其他隐含的系统组件的需要通过【系统组件】添加隐藏的组件路径（常用组件路径需要在平台付费获取）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/3135baef393147e23be10251d470fe7e.png\" /></p><p></p><p>（3）数据资产管理相关组件数据资产管理包括数据模型、数据源、数据关联关系、血缘关系、数据标签等页面组件，可完成多种形式功能组装、实现数据资产管理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b64da6fc757177a83cf7ec4a0ebf474.png\" /></p><p></p><h2>3 由系统组件路径添加页面</h2><p></p><p>除了已预置的功能组件，其他很多页面没有预置为组件，可以通过【系统组件】来添加隐藏的组件路径（常用组件路径需要在平台找到页面并复制路径）。</p><p></p><h3>（1）数据资产管理</h3><p></p><p>数据资产管理在平台中作为基础、且核心的能力，包含在平台多个模块中。包括数据源、数据连接器、数据图书馆等。例如添加【系统组件】、嵌入数据源组件路径。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f72f3b055b9bf47d86d330b94fed7141.png\" /></p><p></p><h3>（2）数据标准管理</h3><p></p><p>平台中数据标准包括字典标准和标准目录管理等。例如添加【系统组件】、嵌入标准目录组件路径。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/15ef68e6f00fd80a7115ee88e219593c.png\" /></p><p></p><h3>（3）数据质量管理</h3><p></p><p>数据质量可将资产中的指定字段，根据数据质量标准规则进行记录处理和输出。</p><p></p><p>例如添加【系统组件】、嵌入数据质量稽核配置组件路径。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/448977a9c827ae1f0cb88e98fad15ae3.png\" /></p><p></p><p>例如添加【系统组件】、嵌入数据质量监控组件路径。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a1/a126bd0dbdfb9ec68766de8236f7c39e.png\" /></p><p></p><p>例如添加【系统组件】、嵌入数据质量稽核配置组件路径。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/448977a9c827ae1f0cb88e98fad15ae3.png\" /></p><p></p><h3>（4）数据安全管理</h3><p></p><p>smardaten提供了多种数据加密方式，数据模糊化，AES128，AES192等，管理员可以通过此功能对密码进行加密，并设置秘钥有效时间，加强数据访问和操作的安全性。同时我们也可以自己设定数据的安全级别，来区分不同数据的重要程度。</p><p></p><p>例如添加【系统组件】、嵌入数据安全密钥管理组件路径。可以设置密钥类型、有效时间等，加强数据访问和操作的安全性</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/1190001a634ef0c91b9787a9cd14634b.png\" /></p><p></p><h2>4、自定义配置页面和业务流</h2><p></p><p>除了平台自带的组件页面和隐藏的功能页面，剩下的功能需求（如表单、流程、仪表盘等）通过其他无代码功能快速拖拽、组装即可，可以完成比较复杂的业务逻辑和交互。下面简单介绍几个与数据资产管理相关的页面配置流程。</p><p></p><h3>（1）数据资产管理</h3><p></p><p>左边菜单栏点击新增、增加目录节点，在目录下增加数据对象：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/30/3058eeb837d35a2b61031077294e8c70.png\" /></p><p></p><p>新增数据源，并选择数据库类型：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e840f825b353d05fdd09d62098356119.png\" /></p><p></p><p>实现数据目录管理页面：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a74795ef02ba445452b99667435899df.png\" /></p><p></p><p>输入响应地址和参数名称，点击确定跳转资产配置</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f5efafa8f459be24e6bbc3af577d6d7d.png\" /></p><p></p><p>在目录下绑定id字段</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/00/0073726ff5f993a64564c590b01c2704.png\" /></p><p></p><p>完成提交并保存，这样数据资产就配置完毕了。</p><p></p><h3>（2）数据服务申请</h3><p></p><p>在数据服务部分需要查看每个用户个人的申请信息。这个需要自定义创建页面，主要是构建申请表单、配置列表查看信息，用户可以查看不同目录下面的数据共享，可以提出订阅。申请字段包括资源名称、申请类型、表单ID、视图ID、审核状态等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5c/5c97274857ffa7999ca324a18c8d3689.png\" /></p><p></p><p>填报组件包括选择、文本、字典数据等多种方式，例如申请类型根据服务情况，设置为2种申请类型：接口申请和授权申请。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d953c7cd9ca044c9b16ff7a6e38adf28.png\" /></p><p></p><p>设置三种审核状态，已审核、待审批、已驳回等。这里的3个审核状态需要匹配当前表单的业务流审批状态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/9896fd8e74ee4ad6640cecd14dfd7485.png\" /></p><p></p><p>完全依靠拖拉拽完成业务流程设计。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab8bf9d3b9dc6e799c956b6e4ec34359.png\" /></p><p></p><p>表单配置完成后，当前申请信息在列表中的显示字段，直接勾选已有的所有字段，并选择展示顺序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e926a1857adbed01dd5f00b7faa2afaf.png\" /></p><p></p><h3>（3）元数据录入</h3><p></p><p>配置一个多字段的元数据录入，</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19ae5cac9517399da4dacf17476cf753.png\" /></p><p></p><p>支持对上报的元数据进行提交审核，只需配置相关业务逻辑按钮。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fa/fa78f3b89073ec89a072cef7fa9520ca.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/9297e8f117f27fdabe53a35b90ac086b.png\" /></p><p></p><h3>（4）数据资产大屏</h3><p></p><p>通过数据分析和可视化大屏，可以配置数据资产大屏，展示各类数据资产情况、数据标准管理成功、数据服务调用次数等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0c/0cf39fbc4c5810f36c3eae199a0ff78b.png\" /></p><p></p><h1>使用体验</h1><p></p><p>通过搭建数据中台系统，数据中台相对来说是数据处理要求比较高的应用，对业务逻辑也比较复杂，好在平台已基本具备主要功能模块直接使用，大大减少了上手难度，对于大多数人来说，只需当做数据资产管理工具直接上手即可。在一些复杂场景的实现上平台提供了完善的学习地图，视频讲解，和案例展示。对一些没有代码经验的小白十分友好。不用担心不会使用，所有初学者都能快速上手。同时还可以实现多人协助配置，让开发交付效率倍增。 smardaten兼具无代码构建、低代码开放集成、智能BI、数据中台等多元化能力，能够打造多行业、复杂的数字化应用，帮助客户轻松实现多种数字化需求。官网地址<a href=\"https://s3.smardaten.com/\">https://S3.smardaten.com/</a>\"</p><p><img src=\"https://static001.geekbang.org/infoq/16/1622e6ed4c3d58cdc7109e6b8edfd50a.png\" /></p><p></p><p></p>",
    "publish_time": "2023-09-19 07:47:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中关村科金技术副总裁张杰博士确认出席 FCon ，分享金融领域大模型的应用实践",
    "url": "https://www.infoq.cn/article/zpSVPq3WGrA1KrvMiPkP",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。中关村科金技术副总裁张杰博士将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5548?utm_source=infoqweb&amp;utm_medium=article\">金融领域大模型的应用实践</a>\"》主题分享，介绍大模型时代下的新型人机协同、大模型在金融领域应用的技术壁垒以及基于大模型的应用案例。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5548?utm_source=infoqweb&amp;utm_medium=article\">张杰博士</a>\"，天津大学本硕博学位，主要研究方向为知识工程、自然语言处理，曾出版《知识中台》《知识图谱》两部技术专著，发表学术论文十余篇，发明专利一百余项，主持或参与国家级课题八项，获第十届吴文俊人工智能技术发明一等奖。主持开发过推荐引擎、知识问答系统、客服机器人、大数据风控系统、行业知识图谱等多项商业系统，累计销售额数亿元。他在本次会议的演讲内容如下：</p><p></p><p>演讲：金融领域大模型的应用实践</p><p></p><p>随着人工智能技术的快速发展，大模型已经成为金融行业创新发展的重要驱动力。这不仅仅是金融行业的一项技术革新，更是推动金融业务升级和转型的重要引擎。基于大模型的应用层出不穷，势必为金融行业海量的数据和复杂的业务场景带来更优的处理方式。然而，金融行业有其独特的行业属性和监管要求，大模型在金融领域的应用过程中诸多的挑战亦是不可避免。</p><p></p><p>演讲提纲：</p><p></p><p>大模型时代下的新型人机协同大模型在金融领域应用的技术壁垒</p><p>○ 避免产生幻觉倾向的外挂知识库技术</p><p>○ 领先的多模态文档解析技术 </p><p>○ 基于大数据智能的思维链归纳技术 </p><p>○ 避免灾难性遗忘的领域模型训练技术</p><p>案例分享：基于领域大模型的企业智能知识库构建及其应用案例分享：基于领域大模型的理财师营销助手</p><p></p><p>你将获得：</p><p></p><p>○ 理解通用大模型和领域大模型的优劣势</p><p>○ 理解领域大模型的运行逻辑</p><p>○ 理解金融机构在大模型领域投入的方向</p><p>○ 理解领域大模型应用对金融业的作用</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-09-19 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "配置语言 KCL 正式成为CNCF 沙盒项目",
    "url": "https://www.infoq.cn/article/Xyu9ZU6BDIO3qNGudsMg",
    "summary": "<p>&nbsp;</p><p>9 月 12 日，&nbsp;配置语言 <a href=\"https://kcl-lang.io/\">KCL 项目</a>\"通过了云原生计算基金会（CNCF）技术监督委员会评定，正式成为 CNCF 沙箱项目。</p><p>&nbsp;</p><p>KCL 是一个<a href=\"https://github.com/kcl-lang/kcl\">开源的</a>\"基于约束的记录及函数语言，旨在通过成熟的编程语言技术和实践来改进大量繁杂配置比如云原生 Kubernetes 配置场景的编写等，致力于围绕配置的模块化、扩展性和稳定性，打造更简单的逻辑编写体验，构建更简单的自动化和生态集成路径。KCL在2022 年 5 月正式开源，并在今年6 月 成为 CNCF Landscape 项目。</p><p>&nbsp;</p><p>当前，在轻量级客户端云原生动态配置领域，配置语言及工具存在如下问题：</p><p>&nbsp;</p><p>维度爆炸：大多数静态配置如云原生领域的 Kubernetes YAML 配置需要为每个环境单独进行配置；在最糟糕的情况下，它可能引入涉及环境交叉链接的难以调试的错误，稳定性和扩展性都较差。配置漂移：对于不同环境的静态管理应用程序和基础设施配置的方式，往往没有标准的方式去管理这些动态的不同环境的配置，采用非标准化的方法比如脚本和胶水代码的拼盘，会导致复杂度呈指数增长，并导致配置漂移。认知负担：Kubernetes 等作为构建平台的平台技术手段在底层统一基础架构细节方面出色，但是缺乏更上层的应用软件交付抽象，对于普通开发者认知负担较高，影响了更上层应用开发者的软件交付体验。</p><p>&nbsp;</p><p>在云原生配置和自动化的特定问题域内，KCL 使用专用配置和策略语言用于编写和管理规模化复杂配置及策略。不同于混合编写范式、混合工程能力的高级通用语言，专用语言的核心逻辑是以收敛的有限的语法、语义集合解决领域问题近乎无限的变化和复杂性，将复杂配置和策略编写思路和方式沉淀到语言特性中。具体来说，KCL 具备以下能力：</p><p>&nbsp;</p><p>在代码层面提升配置语义验证的能力，比如 Schema 定义、字段可选/必选、类型、范围等配置检查校验能力；提供配置分块编写、组合和抽象的能力，比如结构定义、结构继承、约束定义和配置策略合并等；用现代编程语言的方式以编写代码的方式提升配置的灵活度，比如条件语句、循环、函数、包管理等特性提升配置重用的能力；提供完备的工具链支持，丰富的 IDE 插件、语言和生态工具链支持用以降低上手门槛，提升使用体验；通过包管理工具 和 OCI 注册表，使得配置以更简单的方式在不同团队/角色之间分享，传播和交付；提供高性能的编译器满足规模化配置场景诉求，比如满足由一份基线配置根据部署上下文生成不同环境、不同拓扑配置的渲染性能以及配置自动化修改性能诉求；通过多语言 SDK、KCL 语言插件等手段提升其自动化集成能力，在发挥配置及策略编写价值的同时显著降低 KCL 的学习成本。</p><p>&nbsp;</p><p>作为一种配置语言，KCL 为应用程序和平台开发人员/SRE 提供的最重要的功能是动态配置管理。通过代码抽象，构建以应用为中心的模型屏蔽复杂的基础设施和平台概念，为开发人员提供一个以应用程序为中心且易于理解的界面。</p><p>&nbsp;</p><p>KCL 支持与 Kubernetes Resource Model (KRM) 规范直接集成。无论是使用独立的 KCL 还是 KRM KCL 配置形式， KCL 可以与各种 CI/CD 和 GitOps 工具的集成。此外，KCL 还与 CNCF 其他众多生态项目进行了集成，比如 Helm、Kustomize、kpt 等，还在运行时提供 KCL Kubernetes Operator，以满足不同场景的配置管理需求等。</p><p>&nbsp;</p><p>&nbsp;</p><p>更多信息可以查看：</p><p>项目地址：<a href=\"https://github.com/kcl-lang/kcl\">https://github.com/kcl-lang/kcl</a>\"</p><p>项目官网：<a href=\"https://kcl-lang.io/\">https://kcl-lang.io</a>\"</p><p>KCL 2023路线规划：https://kcl-lang.io/docs/community/release-policy/roadmap</p><p></p>",
    "publish_time": "2023-09-19 12:14:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据处理的下一阶段，容器驱动的边缘计算",
    "url": "https://www.infoq.cn/article/lWBu7U1YHcDAL9PSEXV3",
    "summary": "<p>最近一段时间以来，边缘计算和容器正变得越来越流行，为我们日常生活中与数据处理相关的各种挑战提供了创新的解决方案。现在，这些技术已经渗透到了用途广泛的设备中，包括汽车、电话，甚至冰箱，为各种使用场景释放了新的可能性，使我们能够更有效地解决数据处理方面的挑战。在本文中，我们将探讨边缘计算和容器的结合点，这些技术的重要性以及与之相关的挑战。</p><p>&nbsp;</p><p></p><h2>边缘计算和容器的使用场景</h2><p></p><p>&nbsp;</p><p>我们有多个行业都可以从边缘计算和容器的使用中受益，包括工业物联网（Industrial Internet of Things，IIoT）、医疗保健、智能城市和零售行业。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec937e9b613bf614f2f820bf65a72692.jpeg\" /></p><p>&nbsp;</p><p>图1：边缘计算和容器的使用场景</p><p>&nbsp;</p><p>边缘计算无处不在，几乎所有的行业都会涉及到它。如下是一些比较成熟的行业：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44491b4a6acb798a6fef6ae41091a644.png\" /></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>边缘计算与容器的结合点</h2><p></p><p>&nbsp;</p><p>边缘计算和容器有多个共同点，包括它们支持分布式应用的能力以及对降低延迟的关注。容器特别适合边缘计算，因为它们非常轻量级，能够很容易地部署到远程的位置中。但是，在边缘计算环境中，使用容器也有一些挑战，比如有限的资源和安全方面的问题。</p><p>&nbsp;</p><p>边缘容器的收益包括：</p><p>&nbsp;</p><p>灵活性：边缘容器具有高度的可移植性，可以在各种边缘设备上运行，提供了部署的灵活性和敏捷性。可扩展性：容器具有高度的可扩展性，可以在多个边缘设备上实现复制、部署和管理，从而能够更容易地扩展应用和服务。在边缘计算环境中，这一点尤为重要，因为在这种环境中资源有限，传统的单体应用可能并不合适。安全性：容器为运行应用提供了一个安全的环境，并将它们与边缘设备上的其他进程实现了隔离。低延迟：通过在更接近源头的地方处理数据，容器有助于减少数据在设备和数据中心之间传输的时间。这在需要实时处理的应用中尤为重要，如IIoT或健康医疗中使用的应用。减少带宽：由于所有的流量均集中在云供应商的数据中心内，所以集中式的应用往往会导致高昂的网络费用。而边缘容器可以更靠近终端用户，允许对数据进行预处理和缓存，这有助于减少网络费用。成熟度：作为一种容器技术，<a href=\"https://www.docker.com/\">Docker</a>\"被认为是非常稳定的，并在生产环境中得到了广泛应用。此外，开发人员可以利用他们现有的知识和技能来使用Docker，这意味着测试边缘容器时不需要额外的培训。</p><p>&nbsp;</p><p>边缘容器的挑战包括：</p><p>&nbsp;</p><p>受限的资源：边缘设备通常资源有限，比如内存、处理能力和存储，这可能会影响边缘容器的性能。复杂性：边缘容器需要容器化和分布式计算方面的专业知识，这对一些组织来说是一种挑战。管理：跨多个边缘设备管理容器可能会很复杂和耗时，需要强大的容器编排解决方案。安全性：边缘设备通常位于远程和不安全的位置，这可能会使得它们易于遭受攻击。容器也会带来安全风险，比如容器逃逸（breakout）或容器镜像中的漏洞。</p><p>&nbsp;</p><p></p><h2>实现边缘计算和容器</h2><p></p><p></p><p>边缘计算和容器提供了很多的收益，组织很快就会为不用的业务场景采用这些技术。但是，成功实现这些技术需要仔细考虑多个关键因素。</p><p>&nbsp;</p><p></p><h3>选择正确的容器平台</h3><p></p><p>&nbsp;</p><p>在实现边缘计算和容器时，选择正确的容器平台（如Docker）非常重要。这些平台提供了一系列的特性和功能，如容器的编排和管理，这有助于简化边缘计算环境中容器的部署和管理。不过，由于边缘设备的资源容量问题，广泛使用的常见平台，如<a href=\"https://kubernetes.io/\">Kubernetes</a>\"和<a href=\"https://docs.openshift.com/\">OpenShift</a>\"，并不适合边缘计算。建议切换到兼容的替代品，它们通常是开源方案，如<a href=\"https://k3s.io/\">k3s</a>\"、<a href=\"https://kubeedge.io/en/\">KubeEdge</a>\"、<a href=\"https://microk8s.io/\">microk8s</a>\"或<a href=\"https://github.com/baetyl/baetyl\">Baetyl</a>\"。</p><p>&nbsp;</p><p></p><h3>部署策略</h3><p></p><p>&nbsp;</p><p>在实现边缘计算和容器时，应该考虑部署策略。根据具体的使用场景，组织可以选择采用混合云模式，即一些服务部署在云中，而另一些部署在边缘设备上。另外，容器可以直接部署在边缘设备上，这可以减少延迟并提升性能。</p><p>&nbsp;</p><p>一旦部署完成之后，管理边缘计算和容器可能很具挑战性，尤其是存在大量边缘设备的环境中。容器编排和管理平台（如Kubernetes）有助于简化边缘计算环境中容器的管理。这些平台提供了自动扩展、负载均衡和健康监控等特性，可以帮助确保容器的运行效率和效果。</p><p>&nbsp;</p><p>此外，监控容器的性能/状态能够确保在一些问题造成重大影响之前将其识别和解决掉。这包括监控容器的资源使用情况、网络流量和应用的性能，并使用日志和度量指标来解决问题。开源的工具通常是一个很好的起点，如<a href=\"https://opentelemetry.io/\">OpenTelemetry</a>\"和<a href=\"https://prometheus.io/\">Prometheus</a>\"。</p><p>&nbsp;</p><p>为边缘设备的故障做好计划，边缘设备可能会出现意外的故障，因此必须要通过实施冗余措施来规划这种情况，例如在不同的边缘设备上运行容器的多个实例或使用边缘到云的故障转移机制。</p><p>&nbsp;</p><p></p><h3>安全考量因素</h3><p></p><p>&nbsp;</p><p>在实现边缘计算和容器时，安全考量因素是非常重要的。边缘计算依赖于大量容易受到网络攻击的设备和网络，包括恶意软件、勒索软件和网络钓鱼攻击。如果没有适当的安全措施，这些设备和网络可能会被破坏，导致数据泄露和其他安全事件。如果某个边缘设备遭到了破坏，它可能感染整个网络。另外一个挑战是数据保护，特别是当涉及到敏感数据时，我们很难阻止对设备的物理访问。</p><p>&nbsp;</p><p>最后，边缘计算缺乏标准化，这会造成安全方面的挑战，因为这会使得在设备和网络间实现一致的安全措施变得更加困难。安全依然是使用边缘计算的主要挑战，它可能需要付出大量的努力来减轻风险。</p><p>&nbsp;</p><p></p><h2>边缘计算和容器的未来</h2><p></p><p>&nbsp;</p><p>随着人工智能（AI）和机器学习的使用以及新容器技术的发展，这些新兴的趋势会使得边缘计算和容器的未来更具吸引力。例如，边缘设备越来越多地配备了人工智能和机器学习能力，这有助于提升数据处理的准确性和速度。今天，我们已经有了<a href=\"https://www.arrow.com/en/research-and-events/articles/5-levels-of-autonomous-driving-technology\">自动驾驶的汽车</a>\"，可以<a href=\"https://www.engadget.com/2016-11-02-netatmo-presence-ai-security-camera.html\">区分猫/狗或人（小偷）</a>\"的智能相机，<a href=\"https://techcrunch.com/2022/08/12/trashbot-uses-ai-to-sort-recyclables/\">回收行业的自动分拣机</a>\"，甚至简单的手表，它能够分析健康数据并探测<a href=\"https://www.health.harvard.edu/heart-health/can-a-smart-watch-diagnose-a-heart-attack\">心脏病的发作</a>\"。所有的这些都是通过边缘计算加上AI实现的，在未来的几年中，我们日常生活中的使用场景将会迅速增加。</p><p>&nbsp;</p><p>同时，为了跟上这些新使用场景的步伐，新的容器技术也在开发中，比如<a href=\"https://webassembly.org/\">WebAssembly</a>\"，它有助于提高边缘计算环境中容器的性能和安全性。</p><p>&nbsp;</p><p>有项关于边缘计算市场的研究宣布，未来10年将会有20-30%的同比增长，这证实了该项技术的潜力。大型科技公司将会进行投资，以简化部署方案的实施，使每个行业均能使用这些方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/81/815227d2dc06f36612528e73d85a7cac.jpeg\" /></p><p></p><p>图2：美国边缘计算市场（<a href=\"http://grandviewresearch.com/\">数据源</a>\"）</p><p>&nbsp;</p><p>边缘计算和容器对企业和社会的影响将持续增长，为创新和效率提升带来新的机会。例如，在农业领域，我们会看到放置在田间的设备，以收集土壤湿度、温度和空气湿度等数据。然后，这些数据可以使用人工智能算法进行实时处理，以优化灌溉、肥料使用和病虫害管理，从而提高作物产量，减少对环境的影响。</p><p>&nbsp;</p><p>所有类型的设备均可以使用相关的传感器来优化能源使用，有些设备可以配置人工智能算法，分析来自传感器和其他来源的数据，以便在设备出现故障之前发现潜在的问题。这有助于减少停机时间和维护成本，并提升设备的整体性能。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>结论</h2><p></p><p>&nbsp;</p><p>虽然边缘计算和容器的结合点存在着一些挑战，但是创新和效率提升的机会是巨大的。随着越来越多的行业采用这些技术，考虑与其实施相关的挑战和机会是非常重要的。通过选择正确的容器平台和部署策略，并做出最佳的安全决策，企业可以成功实施边缘计算和容器，以推动商业价值的实现。</p><p>&nbsp;</p><p>边缘计算和容器的未来是很有吸引力的，新的技术和使用场景一直在不断出现。通过紧跟这些趋势，企业可以继续创新并推动其各自行业的价值。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://dzone.com/articles/the-edge-of-containers\">https://dzone.com/articles/the-edge-of-containers</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://www.infoq.cn/article/CishxTBMTqUf6YIm7Zvd\">锚定数据处理几大痛点，企业如何利用数据云平台释放数据全部价值？</a>\"</p><p><a href=\"https://www.infoq.cn/article/UDGzuR869oAB9iURUOYN\">秒级启动万个容器，如何实现容器镜像的“加速冲刺”？</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ==&amp;mid=2247504157&amp;idx=1&amp;sn=a9d4bf5386bc651586feb795638ca880&amp;chksm=eca7e859dbd0614f55c50c87dae769f72350799d72a2de6e00c0fbad3d73a6fbfb9e48bbff15&amp;scene=27#wechat_redirect\">自下而上学习容器</a>\"</p><p><a href=\"https://www.infoq.cn/article/U3uzsa3z03t92JB8p0iv\">开发容器：可重用的开发环境</a>\"</p>",
    "publish_time": "2023-09-19 12:54:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Dropbox 瘦身攻略：我们如何把JavaScript包缩小三分之一",
    "url": "https://www.infoq.cn/article/81hiCXm5osKRMxlThgKZ",
    "summary": "<p>不知道各位朋友是否还记得，上一次正打算点击网站上的按钮、结果页面突然变化导致你点上了错误的位置是什么时候。或者说，上一次你因为实在忍受不了缓慢的加载速度而愤然点叉又是在什么时候？</p><p>&nbsp;</p><p>这些问题在如今内容愈发丰富、交互度越来越高的应用场景中被无限放大。为了支持更复杂的功能，我们不得不编写出更多的前端代码，导致浏览器端需要接收、解析和执行的字节更多，最终性能自然变得更差。</p><p>&nbsp;</p><p>在Dropbox，我们深深了解此等糟糕体验是多么令人崩溃。所以过去一年来，我们的Web性能工程团队抽丝剥茧、将性能问题溯源到了一个常常被忽视的元素身上：模块捆绑器。</p><p>&nbsp;</p><p>米勒定律认为，人脑在任何给定的时间内只能容纳一定量的信息，所以大部分现代代码库（包括我们Dropbox的代码库）才会被拆分成一个个更小的模块。模块捆绑器负责把应用程序中的各类组件（例如JavaScript和CSS）合并成捆绑包，并在页面加载时由浏览器下载这些捆绑包。最常见的处理方式就是将捆绑包保存为最小JavaScript文件的形式，用以存放Web应用程序中的大部分逻辑。</p><p>&nbsp;</p><p>Dropbox模块捆绑器的首次迭代设计于2014年，当时以性能为先的模块捆绑方法才刚刚兴起（分别在2012年和2015年由Webpack和Rollup率先提出）。但毕竟年代久远，那时候的方案跟现代设计比起来还是太过简陋。我们的模块捆绑器并没多少性能优化，使用起来比较繁琐，既影响用户体验又会拖慢开发速度。</p><p>&nbsp;</p><p>随着捆绑器逐渐显露老态，我们决定面向未来做好性能优化、全面替换掉这位应当功成身退的老将。当前也是替换的最佳时机，因为我们正好在着手将页面迁移至Edison（我们的全新Web服务栈），统筹规划有望一箭双雕。替换之后，我们的静态资产管线也将迎来更现代的捆绑器，在架构层面让集成更为简单。</p><p></p><h2>现有架构</h2><p></p><p></p><p>虽然我们原本的捆绑器拥有相对较快的构建速度，但也存在着不少短板，包括捆绑包太过臃肿、工程师们感到难以维护等等。工程师们只能手动定义把哪些脚本跟包捆绑在一起，而且我们之前只简单提供页面渲染所需要的包，但几乎未做任何性能优化。随着时间推荐，这种粗糙的方案也带来了以下几大显著问题。</p><p>&nbsp;</p><p>问题一：捆绑代码有好几个版本</p><p>&nbsp;</p><p>直到不久前，我们还在使用名为Dropbox Web Server（DWS）的自定义Web架构。简单来讲，每个页面都由多个小页（pagelet，即页面中的子部分）组成，因此导致每个页面都有多个JS入口点，而各servlet也由后端处对应的控制器提供服务。虽然这种部署在多个团队同时处理同一页面时速度更快，但也往往导致pagelet指向不同的后端代码版本。这就要求DWS能支持在同一页面上交付不同版本的打包代码，而这经常会引发一致性问题（例如在同一页面上加载相同单例的多个实例）。我们向Edison的迁移将消除这种pagelet架构，从而更灵活地采取更符合行业标准的捆绑方案。</p><p>&nbsp;</p><p>问题二：需要手动分割代码</p><p>&nbsp;</p><p>所谓代码分割，就是把JS包分割成更小的块的过程，这样浏览器就能只加载当前页面所需要的代码库部分。例如，假设用户先访问dropbox.com/home，而后访问dropbox.com/recent，那么如果不进行代码分割，则浏览器会下载整个bundle.js，这无疑将显著减慢页面的初始导航速度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/dffb4dc08a558a0fa145db5c6bc43a97.png\" /></p><p></p><p>所有页面的全部代码均通过单一文件提供</p><p>&nbsp;</p><p>但在代码分割之后，浏览器只需要下载页面所需要的各个代码块。由于浏览器下载的代码量更少，所以dropbox.omc/home的初始导航速度将大大提升。此外，代码分割可以保证先加载关键脚本，而后再异步加载、解析和执行非关键脚本。共享代码片段也将被浏览器缓存下来，进一步减少用户在不同页面间移动时所需下载的JS代码量。所有这些，都将大大减少Web应用程序的加载时间。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/196cab4ecde4fe539fdc6ef3d11e237d.png\" /></p><p></p><p>仅下载页面所需的新代码块</p><p>&nbsp;</p><p>由于我们现在的捆绑器没有任何内置的代码分割工具，所以工程师只能手动对包做定义。具体来讲，我们的打包map是个6000多行的庞大字典，具体指定了哪些模块该放进哪个包中。</p><p>&nbsp;</p><p>可以想见，随着时间推移这样一套架构的维护工作将变得异常复杂。为了避免非优打包，我们强制执行了一套严格的测试（打包测试），但因为每次变更都可能打乱原本的模块排列，所以工程师们变得神经紧张、苦不堪言。</p><p>&nbsp;</p><p>这也导致我们的实际代码量比页面所需要多得多。例如，假定我们有以下包map：</p><p>&nbsp;</p><p><code lang=\"null\">{\n  \"pkg-a\": [\"a\", \"b\"],\n  \"pkg-c\": [\"c\", \"d\"],\n}</code></p><p>&nbsp;</p><p>如果页面依赖于模块a、b和c，则浏览器只须进行两次HTTP调用（分别获取pkg-a和pkg-b），而非对各模块各进行一次（共三次）调用。这虽然会减少HTTP调用的开销，但同时也会加载不必要的模块——在本示例中就是模块d。由于缺乏摇树优化，我们不但加载了不必要的代码，还加载了页面不需要的整个模块，因此会拖慢整体用户体验。</p><p>&nbsp;</p><p>问题三：缺少摇树优化</p><p>&nbsp;</p><p>摇树是一种包优化技术，能够消除未使用的代码来帮助捆绑包瘦身。假设我们的应用程序需要导入包含多个模块的第三方库，如果没有摇树优化，则实际加载的大部分捆绑代码其实都毫无用处。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/236162ea8430597ebad7f1889968f903.png\" /></p><p></p><p>无论是否实际使用，所有代码都会被捆绑进来</p><p>&nbsp;</p><p>通过摇树优化，我们可以分析代码的静态结构，并删除一切未被其他代码直接引用的代码。这样最终的捆绑包就能更加精简小巧。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/01776742478855f4a8916b443dcbec8e.png\" /></p><p></p><p>只捆绑要使用的代码</p><p>&nbsp;</p><p>因为我们之前的捆绑器不太完善，所以其中没有任何摇树功能。生成的包往往包含大量未使用代码，特别是来自第三方库的代码，这会导致页面加载无用内容、延长等待时间。此外，因为我们使用protobuf定义来实现从前端到后端的高效数据传输，所以在检测某些可观察性指标时往往要引入高达几MB的未使用代码！</p><p></p><h2>为何选择Rollup</h2><p></p><p></p><p>多年来我们其实考虑过不少解决方案，并最终把核心需求梳理了出来：我们真正需要的，就只有自动代码分割、摇树优化，以及可以进一步优化捆绑管线的可选插件。Rollup就是当前最成熟、也能灵活融入到我们现有构建管线的工具，于是最终成为我们的首选解决方案。</p><p>&nbsp;</p><p>另一个原因是：有助于降低工程开销。因为我们已经在使用Rollup捆绑我们的NPM模块，所以继续扩大Rollup的使用范畴肯定比再引入新工具要划算得多。此外，这也意味着跟其他捆绑器相比，我们已经在之前的运营中掌握了更多关于Rollup特性的工程专业知识，能有效降低用不下去的可能性。最后，我们还算了一笔账，发现跟深入集成Rollup相比，在原有模块捆绑器中重现Rollup的功能需要投入更多工程资源。</p><p></p><h2>不负众望的Rollup</h2><p></p><p></p><p>我们都知道，安全、分步推出模块捆绑器绝非易事，毕竟我们在期间需要同时可靠支持两种模块捆绑器（并生成两种对应的捆绑包）。我们主要关心的问题包括如何保证捆绑代码稳定、无bug，如何增加构建系统和CI的负载，还有怎样激励团队接受在其页面中使用Rollup捆绑包。</p><p>考虑到可靠性和可扩展性等问题，我们把发布过程分成了四个阶段。</p><p></p><p>开发者预览阶段：允许工程师在开发环境中选择加入Rollup捆绑包。这样我们就能让开发者尽早发现Rollup捆绑包引发的任何意外，借此推动行之有效的众包QA测试。认真收集相关信息后，我们将有充足的时间解决bug、适应范围变更。面向Dropbox员工的内部预览阶段将全面推广Rollup捆绑包，借此收集早期性能数据并进一步获取关于应用程序行为变化的实践反馈。通用阶段，即逐步向所有Dropbox用户（包括内部和外部用户）推出Rollup捆绑包。在此之前，我们已经对Rollup包做过彻底测试并确定其稳定性已经达到较高水平。维护阶段，强调解决项目中遗留的所有技术债，再通过迭代让Rollup进一步优化性能和开发者体验。我们意识到，如此规模的大体量项目将不可避免地积累下一些技术债，我们应计划在某个阶段将其解决，而不能假装债务不存在。</p><p>&nbsp;</p><p>为了有效支持各个阶段，我们混合使用了基于cookie的门控和内部功能门控系统。以往，Dropbox的大多数部署都纯粹借助我们的内部功能门控系统得以完成。但这一次，我们决定允许基于cookie的门控在Rollup和旧捆绑包间快速切换，从而加快调试速度。每个发布阶段都以交替形式分步推出，包括从1%、10%、25%，到50%乃至最终的100%。这让我们能够灵活地收集早期性能与稳定性结果，当发现问题时进行无缝回滚，同时尽可能降低对内、外部用户造成的影响。</p><p>&nbsp;</p><p>因为我们需要迁移大量页面，所以除了建立安全可靠的Rollup切换策略之外，还得激励页面所有者主动执行切换。由于我们的Web栈将配合Edison进行一波重大改造，所以这应该是个可以一箭双雕的绝佳时机。如果把Rollup塑造成Edison所支持的独特功能，那开发团队应该会更愿意同时接受Rollup和Edison，我们也能借此将Rollup的迁移策略跟Edison升级紧密绑定起来。</p><p>&nbsp;</p><p>Edison也有望借此提高自己的性能和开发速度。我们认为，将Edison与Rollup相结合，会在整个公司内产生强烈的转型协同效应。</p><p></p><h2>挑战与障碍</h2><p></p><p></p><p>我们早就做好了迎接意外挑战的准备，但事实证明将一种构建系统（Rollup）跟另一种构建系统（基于Bazel的原有基础设施）进行复杂对接，其挑战性要远远大于我们的任何想象。</p><p>&nbsp;</p><p>首先，我们发现同时运行两种不同模块捆绑器，所消耗的资源要远超我们的估计。Rollup的摇树算法虽然相当成熟，但仍需要将所有模块都先加载到内存中，之后生成分析关系并摇出代码所需的抽象语法树。此外，我们将Rollup集成到Bazel中的作法，限制了我们缓存中间构建结果的能力。也就是说，我们需要持续集成以重建并重新缩小每个构建上的全部Rollup块。这导致我们的持续集成构建因内存耗尽而超时，显著拖慢了部署节奏。</p><p>&nbsp;</p><p>我们还发现了Rollup摇树算法中的几个bug，这会导致摇树优化过于激进。值得庆幸的是问题不大，我们在开发者预览阶段就将其修复，所以最终用户并未受到影响。此外，我们发现旧版捆绑程序会提供来自第三方库的某些代码，而这些代码与JS严格模式并不兼容。一旦将这些代码提交给采用严格模式的新捆绑器，则会在浏览器中引发极为严重的运行时错误。这就要求我们对整个代码库、特别是与严格模式不兼容的补丁代码，开展一轮全面审计。</p><p>&nbsp;</p><p>最后，在Dropbox内部员工预览阶段，我们发现Rollup和旧版捆绑器之间的A/B遥测指标并未体现出符合预期的TTVC性能提升。我们最终意识到，这是因为Rollup生成的代码块比旧版捆绑器生成的代码块要多得多。尽管我们最初假设HTTP2的多路复用能消除大量代码块引发的性能下降，但事实证明代码块过多还是会导致浏览器耗费更长的时间来获取页面所需的各模块。再有，模块数量的增加也会拉低压缩效率，因为Zlib等压缩算法使用的是滑动窗口方法执行压缩，就是说单一大文件的压缩效率要明显好于多个小文件。</p><p></p><h2>最终结果</h2><p></p><p></p><p>在向全体Dropbox用户推出Rollup之后，我们发现新项目将JavaScript包缩小了约三分之一，JS脚本总量减少了15%，TTVC也实现了适度改进。我们还通过自动代码分割显著提高了前端开发速度，开发人员现在不必在每次变量时都手动调整捆绑包定义。最后，也可能是最重要的一点在于，我们完成了捆绑基础设施的现代化改造，削减了自2014年以来积累的大量技术债，显著减轻了未来的项目维护负担。</p><p>&nbsp;</p><p>除了令人眼前一亮的实践表现之外，Rollup项目还帮助我们发现了现有架构中的几个瓶颈：例如多个渲染会阻塞RPC，对第三方库的函数调用过多，以及浏览器加载模块依赖性map效率太低等。凭借Rollup丰富的插件生态系统，解决原有代码库中此类瓶颈正变得越来越简单。</p><p>总而言之，全面采用Rollup作为模块捆绑器不仅给性能和生产力带来立竿见影的提升，也将在未来帮助Dropbox实现更为显著的性能改进。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://dropbox-tech.translate.goog/frontend/how-we-reduced-the-size-of-our-javascript-bundles-by-33-percent?_x_tr_sl=en&amp;_x_tr_tl=zh-CN&amp;_x_tr_hl=zh-CN&amp;_x_tr_pto=sc\">https://dropbox-tech.translate.goog/frontend/how-we-reduced-the-size-of-our-javascript-bundles-by-33-percent</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/e976363d22c7ffd126d9b6eb1\">最佳的 18 个 JAVASCRIPT 前端开发框架和库</a>\"</p><p><a href=\"https://www.infoq.cn/article/2SyNfw6RkyTV4gkRavIQ\">新一波 JavaScript Web 框架</a>\"</p><p><a href=\"https://www.infoq.cn/article/GDc7cryCCPOhQS9FuAKh\">JavaScript 框架大战已结束，赢家只有一个</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247518116&amp;idx=1&amp;sn=74da7a01805480d6e0bb7f08c0f5ea13&amp;chksm=f95232e7ce25bbf16539c1ef807dfc132e9d72610dbe3d149dbda2db58f2f71163fb1ca6d21f&amp;scene=27#wechat_redirect\">跨过四个时代，JavaScript框架终于可以与原生应用SDK竞争了</a>\"</p>",
    "publish_time": "2023-09-19 12:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "和Node.js配置地狱说拜拜！Deno 零配置解决方案",
    "url": "https://www.infoq.cn/article/xzjZxV84u118R0GDTPii",
    "summary": "<p></p><p></p><blockquote>在使用Node.js进行构建时，配置问题常常会影响开发人员的工作效率。好在有了Deno，它的零配置、含“电池”设计让恼人麻烦就此讲拜拜。</blockquote><p></p><p>&nbsp;</p><p>在我们启动各种类型的Node repo时，root目录很快就会被配置文件塞满。例如，在最新版本的Next.js中，我们就有next.config.js、eslintrc.json、tsconfig.json和package.json。而在样式那边，还有postcss.config.js和tailwind.config.js。</p><p>&nbsp;</p><p>需要中间件不？加上middleware.ts。错误监控不能少吧？那就再上sentry.server.config.js、sentry.client.config.js和entry.edge.config.js。也别忘了我们的env文件、Git文件和Docker文件……</p><p>&nbsp;</p><p>于是还没等我们缓过神来，repo可能就已经变成了这个样子：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/99/99333fb94ee56d8dc22fff63865ecdc4.png\" /></p><p></p><p>所有软件都需要配置。毕竟我们终归要用某种方法来设置自己正在使用的项目、工具、插件和软件。但是，真有必要用30个文件来运行单一项目吗？我们怎么就掉进了这么恶心的配置泥潭？</p><p>&nbsp;</p><p>或者说，有没有办法能摆脱困境？</p><p>&nbsp;</p><p></p><h2>配置，但要用上智能默认项</h2><p></p><p></p><p>软件领域没有“银弹”——一切用户的需求都会略有不同。配置能让用户更灵活地根据实际用例，从软件当中汲取最大价值。</p><p>&nbsp;</p><p>但“想用软件？先搞配置”确实是种非常糟糕的用户体验。</p><p>&nbsp;</p><p>咱们以向现有Next.js项目中添加TypeScript为例，看看整个流程要怎么推进。首先，我们需要安装TypeScript和类型：</p><p>&nbsp;</p><p><code lang=\"null\">npm install --save-dev typescript @types/react @types/node</code></p><p>&nbsp;</p><p>之后需要创建自己的&nbsp;tsconfig.json:</p><p>&nbsp;</p><p><code lang=\"null\">touch tsconfig.json</code></p><p>&nbsp;</p><p>接下来，如果大家刚刚开始使用TypeScript、搞不清自己需要什么配置，那肯定要祭起开发者都知道的“秘密武器”——从Stack Overflow上扒一套配置：</p><p>&nbsp;</p><p><code lang=\"null\">{\n  \"compilerOptions\": {\n    \"target\": \"es5\",\n    \"lib\": [\"dom\", \"dom.iterable\", \"esnext\"],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"esModuleInterop\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"strict\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"noEmit\": true\n  },\n  \"include\": [\"next-env.d.ts\", \"**/*.ts\", \"**/*.tsx\"],\n  \"exclude\": [\"node_modules\"]\n}</code></p><p>&nbsp;</p><p>不想手动往项目里添加TypeScript支持？可以试试Deno，它原生支持TypeScript。</p><p>而这，还仅仅是添加了个TypeScript。</p><p>&nbsp;</p><p>高效软件可以通过更智能的默认项来预测用户想要达成的效果。这些“预设选项”能为大多数用户提供经过优化的体验，而且无需手动配置。所以说，先给套能用的配置模板，等确有必要时再要求用户调整才是王道。</p><p>&nbsp;</p><p>其实直接把软件配置页面甩到用户脸上，绝对不利于品牌的声誉、更有损客户信任。设想一下，如果我们第一次使用Gmail，看到的是这样一幅图景：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/12cf3c0e14d38b15189495ad53a6030d.png\" /></p><p></p><p>那鬼才想用你谷歌呢，还不如继续留在Hotmail这边。</p><p>&nbsp;</p><p>所以，智能默认项先行，有必要的时候再做配置调整。</p><p>&nbsp;</p><p></p><h2>这些配置文件到底是干啥的？</h2><p></p><p></p><p>咱们再回到之前的列表，这些文件都在那设置啥呢？</p><p>&nbsp;</p><p>Ignore文件&nbsp;(dockerignore,&nbsp;eslintignore,&nbsp;gitignore,&nbsp;prettierignore,&nbsp;styleignore): 它们的作用就是从操作中排除某些文件和目录。它们有助于保持环境清洁、让执行流程更高效。运行命令文件&nbsp;(eslintrc.json,&nbsp;lintstagedrc.json,&nbsp;nvmrc,&nbsp;nycrc,&nbsp;stylelintrc.json,&nbsp;prettierrc.json,&nbsp;swcrc): 运行命令（rc）配置文件负责指定某些命令在运行时的设置或参数，例如eslint、lint-staged等。包文件&nbsp;(package.json,&nbsp;yarn.lock): 这些文件提供的是关于自动化依赖项和脚本的重要信息，从而对项目环境进行统一管理。Next.js文件&nbsp;(middleware.ts,&nbsp;next-env.d.ts,&nbsp;Next.config.js,&nbsp;tsconfig.json): 这些文件管理Next.js应用程序的设置和配置。Docker&nbsp;(Dockerfile,&nbsp;Dockerfile.deploy,&nbsp;docker-compose.yml): 这些文件管理应用程序在容器内的自动部署和扩展配置。其他(editorconfig,&nbsp;happo.js,&nbsp;babel.config.js,&nbsp;playwright.config.ts,&nbsp;sentry.client.config.js,&nbsp;sentry.server.config.js,&nbsp;sentry.properties, ): 这些配置文件负责自定义并管理开发环境中的各方面设置，包括第三方工具和库。</p><p>&nbsp;</p><p>Next.js. Docker. Sentry. Happo. ESLint. npm. Yarn. Playwright. Babel. VSCode. SWC. Stylelint. Prettier. NVM. NYC. lint-staged. Git.</p><p>&nbsp;</p><p>这些工具并不深奥，属于是将Next.js应用程序部署到生产环境所需要的常规集合。但问题是，真需要30个文件那么多吗？</p><p></p><h2>JavaScript生态系统（一般来说）并不强制要求</h2><p></p><p></p><p>虽然如今Node.js主要用于构建网站和Web应用，但它最初的设计目标其实没那么强烈的倾向性，更多是用事件驱动架构来启用异步I/O。但随着Node的流行，JavaScript突然就占领了高地：跟浏览器/DOM、文件系统和Unix交互，构建系统、捆绑、转译，等等。</p><p>&nbsp;</p><p>JavaScript的广泛实用性，在npm注册表中超过200万个模块上得到了深切体现。为了发挥作用，JavaScript模块必须能够支持越来越多的框架、元框架、构建工具等，以便在任何情况下都能针对各种工作流程顺利接入不同项目。最直接的方法当然就是用更广泛的配置文件来保持模块的泛用能力，所以一大堆配置文件体现的其实是JavaScript需要跟多种框架、工具和技术栈配合使用的客观复杂性。</p><p>&nbsp;</p><p>随着越来越多的工具被添加到Node.js项目当中，配置文件不仅变得愈发繁琐，还会降低开发人员的工作效率。</p><p></p><h2>化繁为简</h2><p></p><p></p><p>软件是达成目标的手段，真正高效的软件绝不会“恶心”用户，而是帮助他们快速完成任务。</p><p>&nbsp;</p><p>Node.js最初的构建目标是作为异步I/O、由事件驱动的JavaScript运行时，当时的缔造者并没预料到它会对Web开发的日后变革发挥关键作用（目前，每三个新网页或Web应用中，就有一个用到Node）。但是，当开发人员使用Node构建新产品时，往往需要花费大量时间来整合自己需要的技术栈和工作流程——比如设置TypeScript、设置自己熟悉的测试框架和构建流程等。</p><p></p><p>那有没有一种办法，能让我们的Web构建成果立即投入生产？</p><p>&nbsp;</p><p>这就是Deno项目的意义所在。它是一种具有零配置加智能默认项的Web原生运行时，您可以在开发新项目时即刻享受到它带来的效率提升。它具备原生TypeScript支持能力，因此无需额外花时间做设置。Deno还附带一套强大的工具链，提供内置的格式化、linting质量检查、测试等功能，全部开箱即用。最后，Deno还使用与Web兼容的API，所以如果大家已经在构建Web应用，那上手Deno应该不会有任何难度。</p><p>&nbsp;</p><p>编程的意义就在于此：管理复杂性、化繁为简。所以，让我们携手Deno，告别恼人的配置步骤。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://deno.com/blog/node-config-hell\">https://deno.com/blog/node-config-hell</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/be78c75026bf0ede1253828fe\">Node.js 未来发展趋势</a>\"</p><p><a href=\"https://www.infoq.cn/article/juXB8EaoJrlLx4vB7ttD\">Node 之父着急宣布：Deno&nbsp;将迎来重大变革，更好地兼容</a>\"</p><p><a href=\"https://xie.infoq.cn/article/aad4610523c72781f0dd5b5b7\">Node&nbsp;版本控制</a>\"</p><p><a href=\"https://www.infoq.cn/article/9QU4eRfjNmNjidpjRkUI\">Node.js 20 正式发布</a>\"</p>",
    "publish_time": "2023-09-19 12:58:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Arm架构下性能分析与优化介绍",
    "url": "https://www.infoq.cn/article/qeL9feYxSmTS9jQdU5tb",
    "summary": "<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/7c/7e/7c1fcff3e899632c45c3f3d89454c07e.png\" /></p>",
    "publish_time": "2023-09-19 14:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "37Signals CTO：云支出减少60%，已节省100万美元",
    "url": "https://www.infoq.cn/article/HUO07yqKbvWdotq8E4Xw",
    "summary": "<p>运营项目管理平台 Basecamp 背后的 37Signals 公司首席技术官、Ruby On Rails 之父 David Heinemeier Hansson 最近发布了他主导的“下云”项目的最新信息，他表示下云计划已经为公司节省了 100 万美元。</p><p>&nbsp;</p><p>今年1月，37 Signals 宣布<a href=\"https://www.infoq.cn/article/0xb7kodt55mk8TB2mcgA\">把大量服务和依赖项从云端转移到内部硬件上</a>\"，借此削减费用。硬件采购大约花费了50万美元，并且均摊在未来5年里，也就是说每年该项成本为10万美元。考虑到一些不可预见的费用，Hansson之前<a href=\"https://world.hey.com/dhh/we-stand-to-save-7m-over-five-years-from-our-cloud-exit-53996caa\">估计</a>\"37Signals 将在五年内节省大约 700 万美元的服务器开支。</p><p>&nbsp;</p><p>现在，在9月16日的帖子中，Hansson写道，他认为该计划可以在五年内节省 1000 万美元。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/34/344a4b8d6481413d6162d362e0db03d6.jpeg\" /></p><p></p><p>&nbsp;</p><p>“我们在云上的支出已经减少了 60%…… 从大约每月 180,000 美元减少到不足 80,000 美元，” 他写道，但也指出这个数字没有包括 Amazon Web Services 的 Simple Storage Service 的费用。“按照这个速度计算，我们每年可以节省 100 万美元，并且预计在 9 月份之前还有另一笔大幅度的减少，之后的支出将在年底逐渐减少，” 他进一步说明。</p><p></p><p>David Heinemeier Hansson<a href=\"https://www.infoq.cn/article/0xb7kodt55mk8TB2mcgA\">此前透露</a>\"，37Signals 在2022 年的云上支出费用超 320 万美元，每月平均 26.67 万美元。为此，该公司将大量服务和依赖项从云端转移到内部硬件上。“经过深思熟虑、多次基准测试以及对 AMD 新 Zen4 芯片与第 4 代 NVMe 驱动器相结合带来的速度体验，我们几乎准备好向戴尔下达我们的巨额订单。”</p><p>&nbsp;</p><p>虽然要管理自己的硬件基础设施，但37Signals非常坚决的抛弃了<a href=\"https://www.infoq.cn/article/yfzmNASdI6Tsm1u8e20P\">业内主流的容器编排系统 K8s</a>\"方案，推出<a href=\"https://www.reddit.com/r/selfhosted/comments/11eziyl/introducing_mrsk_zerodowntime_deployments_on_bare/\">mrsk</a>\"作为K8s的替代方案。</p><p>&nbsp;</p><p>尽管他们自己购买硬件替代了公有云服务，但因为构建方案简单，所以37 Signals 运营团队的规模保持不变。</p><p>&nbsp;</p><p>减少了云支出，而运营人员成本也没有明显上升，在最新的帖子中，Hansson 表示：“仅从节省的资金与支出的资金进行基本比较，考虑到目前每月的节省额，我们在不到六个月内就会从大型采购中收回成本。”</p><p>&nbsp;</p><p>Hansson还认为，其他类似公司可以节省的资金比他管理的还要多。</p><p>&nbsp;</p><p>他写道：“通过查看其他软件公司未优化的云账单，我们的节省实际上可能与成本相比并不高。”他指出，据报道 Snapchat 在过去五年中在云上花费了 30 亿美元。</p><p>&nbsp;</p><p>Snap 最近也在采取措施降低支出，削减了向主要云提供商支付的费用。Snap首席财务官Derek Andersen在最近的年度投资者日上表示，该公司一直在努力降低基础设施成本，这是仅次于员工的第二大支出。</p><p>&nbsp;</p><p>“别告诉我他们实际有约十亿美元的潜在节省空间。”Hansson写道。</p>",
    "publish_time": "2023-09-19 14:28:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "LinkedIn开源Iris消息处理器，速度比旧系统快86.6倍",
    "url": "https://www.infoq.cn/article/AVYS6xdCzT8M0fXZ9jmq",
    "summary": "<p>LinkedIn<a href=\"https://engineering.linkedin.com/blog/2023/open-sourcing-iris-message-processor?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">开源“Iris消息处理器”</a>\"，该服务被用于增强其现有Iris事件升级管理系统（Escalation Management System）的性能和可靠性。iris-message-processor的处理速度有了显著提升，相比之前的处理器，在平均负载下速度快4.6倍，在高负载下速度快86.6倍。</p><p></p><p>“iris-message-processor”是之前的iris-sender的分布式替代方案，支持更高的并发处理和直接将消息发送给目标供应商。它可以进行水平伸缩，并且不使用数据库作为消息队列，因而减少了对数据库的依赖。</p><p></p><p>iris-message-processor可以处理大量的事件，经测试，它可以在不到10秒的时间内处理6000个事件，与之前的系统需要30分钟相比，进步巨大。即使在同时减少50%节点的情况下，新处理器也保持了高效的处理速度，可以在不到30秒的时间内完成集群再均衡，并在不到3秒的时间内处理升级事件，即使是在高于平均负载的情况下。</p><p></p><p>为了提升性能，LinkedIn将Iris升级事件拆分为桶，动态分配给Iris消息处理器集群中的不同节点，改进了并发处理和直接发送消息的能力。Iris进行了重新设计，以应对未来“10倍”流量增长，避免在未来因需求增加时才匆忙进行改造。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be8893d2f73e606dbca52e009b2c438e.png\" /></p><p></p><p>_新Iris系统的架构（<a href=\"https://engineering.linkedin.com/blog/2023/open-sourcing-iris-message-processor?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">来源</a>\"）_</p><p></p><p>新服务是用<a href=\"https://go.dev/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">Go</a>\"开发的。它不使用数据库作为消息队列，从而减轻了现有数据库系统的压力。新系统可以使用最终一致性数据库来存储结果消息进行审计跟踪，从而增强了可伸缩性。</p><p></p><p>LinkedIn已经使用iris-message-processor大约一年时间都没有发生中断，并始终保持每毫秒处理1000条消息的SLO。作为iris-sender的替代方案，iris-message-processor不仅保留了现有的Iris API，同时还提供了实质性的性能改进。</p><p></p><p>LinekedIn的工程师采用了各种措施来确保上线期间的稳定性，包括保持向后兼容性和进行增量式验证和测试。</p><p></p><p>LinekedIn已将<a href=\"https://github.com/linkedin/iris-message-processor?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">Iris -message-processor</a>\"开源，代码可在<a href=\"https://github.com/linkedin/iris?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">Iris</a>\"和<a href=\"https://github.com/linkedin/oncall?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">Oncall</a>\"代码库中找到。这个工具可以在LinkedIn的环境之外运行，可以完全取代其他现成的事件管理系统。</p><p></p><p>InfoQ采访了LinkedIn高级软件工程师<a href=\"https://www.linkedin.com/in/diego-cepeda-92142994/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUxMDUzNzIsImZpbGVHVUlEIjoiTDlrQkI4eW9RNENuUmJrSyIsImlhdCI6MTY5NTEwNTA3MiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTEyfQ.yZrcsZFeJ2UZjlxNzljM_MbS8lNaYsCZewx9tQkdsH4\">Diego Cepeda</a>\"，聊了聊关于iris-message-processor的话题，包括它的实现和开源。</p><p></p><p>InfoQ：引入iris-message-processor后，资源使用（CPU、内存、网络带宽）发生了什么变化，这对运营成本有怎样的影响？</p><p></p><p></p><blockquote>Diego Cepeda：实际上，资源利用率的变化可以忽略不计。LinkedIn的关键监控基础设施具有非常高的容错冗余水平，因为我们认为可靠的监控是可靠运营的基础。我们的Iris实例分布在不同的数据中心，每个数据中心都有足够的容量来处理整个站点的警报需求。不过，因为Iris足够高效，我们在每个数据中心只使用了3个实例，每个实例只配备了8个核心和32GB内存，这些足够了。每个数据中心有3个MySQL主机为Iris和iris-message-processor提供支持。值得注意的是，即使对于LinkedIn这样的规模，这样的配置也有点过剩，因为每个iris-message-processor实例平均使用不到5%的CPU和不到1%的内存。</blockquote><p></p><p></p><p>InfoQ：你是否可以提供一些投资回报率(ROI)指标来证明iris-message-processor与其他现成解决方案在成本效益方面的差异？</p><p></p><p></p><blockquote>Cepeda：我们没有与其他商业系统进行过比较或分析。不过，有大约6000名Iris内部用户积极待命，我们可以据此推测，使用现成的解决方案将是一笔相当大的投入。我们确实有时间维度的比较，例如持续维护Iris所需的工作量，我们估计大部分时间都花在帮助我们的开发人员熟悉系统和回答问题上。此外，我们没有专门为Iris投入人员资源，它只是监控基础设施团队负责处理的许多个服务中的一个。Iris已经为我们的整体投入带来了回报。对于选择使用Iris的组织来说，ROI可能会更大，因为它们只会产生硬件和维护成本，并且可以从Iris的开源中受益。</blockquote><p></p><p></p><p>InfoQ：你选择Go作为iris-message-processor的开发语言，这个决定背后的原因是什么？这个选择对系统的性能和可伸缩性有怎样的影响？</p><p></p><p></p><blockquote>Cepeda：我们选择Go是因为它能够快速开发Bug少、高度可伸缩的并发应用程序。Go的轻量级协程非常适合用来完成这项任务，因为管理并发任务变得很容易，避免了传统线程的复杂性。使用通道在程序之间进行通信增强了安全性和同步能力。此外，Go的标准库提供了并发、网络、分布式系统和测试用的包，减少了对第三方库的依赖，简化了开发。这些特性使得我们能够编写Bug更少的代码，支持高效的伸缩，并快速交付健壮的并发应用程序，如iris-message-processor。在过去的几年，我们一直在用Go语言逐步重写我们的大部分关键监控服务，并在开发速度、性能和可操作性方面获得显著的好处。一个典型的例子就是iris-message-processor，它在高负载下的性能比之前的版本要好几个数量级。</blockquote><p></p><p></p><p>InfoQ：你提到iris-message-processor已经在LinkedIn运行了大约一年时间都没有出现过中断，并且始终保持很好的SLO。你能分享系统在高压环境下测试的例子吗？它的表现如何？</p><p></p><p></p><blockquote>Cepeda：值得庆幸的是，LinkedIn的系统都是相对可靠的，所以我们很少对系统进行真正的压力测试。当然，对于庞大而复杂的系统，出现故障总是不可避免的。我们曾经遇到过一个DNS问题导致我们的一个生产数据中心中的大部分服务（包括iris-message-processor）无法访问时。这是我们所见过的最接近高压的场景，数百个服务同时出现数千个警报，Iris集群有三分之一的节点无法启动。值得庆幸的是，我们在设计iris-message-processor时就考虑到了这种场景。正如我们所计划的那样，集群可以丢弃不可达的节点，进行再均衡，并在不到60秒的时间内处理升级事件。以前的系统可能需要几十分钟才能完全解决这个问题，这会导致处理信息不及时，浪费了工程师发现问题和解决问题的宝贵时间。</blockquote><p></p><p></p><p>InfoQ：随着iris-message-processor的开源，你希望社区可以带来怎样的特新和改进？你计划如何管理开源项目，确保它们与目标和质量标准对齐？</p><p></p><p></p><blockquote>Cepeda：iris-message-processor的核心思想是可伸缩性和灵活性。我们意识到这个世界上不存在两个完全相同的环境，在设计和实现中我们注意到了这一点。例如，我们目前使用Twilio发送语音和短消息，但其他组织可能使用不同的供应商。我们没有对他们进行强制锁定，而是为消息供应商提供了一个可插拔的接口，任何想要集成到其他系统的组织都可以非常快速地编写出一个新的插件，并在对代码库进行最少变更的情况下启动并运行。这种模式也体现在我们对存储系统的选择上。iris-message-processor使用MySQL作为数据存储，当然也可以很容易地使用其他数据存储。所有与MySQL相关的代码都进行了抽象，这样就可以为不同的数据存储编写新的集成实现，而不需要修改代码的其他部分。这种设计使得拥有不同技术栈的组织更容易采用新系统。对于已经开源的Iris和Oncall来说，项目的管理无疑是一个挑战。在这方面，我们采用基于测试驱动的开发和测试。为了确保与质量标准对齐，我们希望新的贡献代码是可测试的，除此之外，代码在被接受之前会在本地开发环境进行验证。我们期待外部贡献者帮助我们把它变成一个更好、更容易使用的平台。</blockquote><p></p><p></p><p>声明：本文由InfoQ翻译，未经许可禁止转载。</p><p></p><p>查看英文原文：https://www.infoq.com/news/2023/09/linkedin-iris-message-processor/</p>",
    "publish_time": "2023-09-19 14:36:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AWS Lambda将废弃Go运行时",
    "url": "https://www.infoq.cn/article/fq9Iz84VK14gXMgo9Co1",
    "summary": "<p>AWS Lambda<a href=\"https://aws.amazon.com/blogs/compute/migrating-aws-lambda-functions-from-the-go1-x-runtime-to-the-custom-runtime-on-amazon-linux-2/\">宣布</a>\"将废弃go1.x运行时，仅在Amazon Linux 2运行时中支持Go。该项声明以及年底前需要迁移至定制的provided.al2运行时引起了Go社区的关注。</p><p>&nbsp;</p><p>根据<a href=\"https://aws.amazon.com/blogs/aws/update-on-amazon-linux-ami-end-of-life/\">Amazon Linux AMI的生命周期</a>\"，go1.x运行时将于12月31日废弃，开发人员应该在此之前迁移至<a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\">provided.al2</a>\"运行时。亚马逊云科技的高级解决方案架构师<a href=\"https://www.linkedin.com/in/micahwalter/\">Micah Walter</a>\"、<a href=\"https://www.linkedin.com/in/yanko-bolanos/\">Yanko Bolanos</a>\"和<a href=\"https://www.linkedin.com/in/ramesh-mathikumar-5118b816/\">Ramesh Mathikumar</a>\"解释了其中的收益：</p><p></p><blockquote>首先，它支持在AWS Graviton2处理器上运行Lambda函数，与在x86_64处理器上运行的函数相比，性价比最大可以提高34%。其次，它通过更小的部署包和更快的函数调用简化了实现。最后，该变更能够使Go与其他可编译为原生代码的语言保持一致，如Rust或C++。</blockquote><p></p><p></p><p>在名为<a href=\"https://www.wolfe.id.au/2023/08/09/rip-aws-go-lambda-runtime/\">RIP AWS Go Lambda Runtime</a>\"的文章中，Stax的工程主管<a href=\"https://www.linkedin.com/in/wolfeidau/\">Mark Wolfe</a>\"表达了不同的意见：</p><p></p><blockquote>我认为这是一件糟糕的事情，原因有如下几点：现有的Go Lambda函数没有自动迁移至新的自定义运行时的路径（……）。其次，这将从AWS Lambda控制台中移除Go1.x的名称，现在Go将会变成另一种运行时，而不是受支持的一等语言。</blockquote><p></p><p>&nbsp;</p><p>对于Node、Python或Java等其他语言，Lambda提供了原生运行时，而Go现在只能通过provided.al2运行时来使用，这降低了在AWS上Go作为serverless语言的可见性。</p><p>&nbsp;</p><p>甚至在AWS移除对Go的原生支持之前，就有开发人员认为，<a href=\"https://www.capitalone.com/tech/cloud/custom-runtimes-for-go-based-lambda-functions/\">转而使用自定义运行时有很多优势</a>\"，比如支持&nbsp;<a href=\"https://aws.amazon.com/blogs/aws/getting-started-with-using-your-favorite-operational-tools-on-aws-lambda-extensions-are-now-generally-available/\">Lambda扩展</a>\"、较新的Amazon Linux 2执行环境，以及能够统一运行时和处理器程序的代码。Wolfe评论到：</p><p>&nbsp;</p><p></p><blockquote>虽然自定义运行时提供了更好的性能和更新的操作系统，但是对于AWS Lambda上的许多Go开发人员来说，这一改变需要花费一定的精力。AWS提供的一些自动化辅助和验证有助于减少这一变更带来的冲突和问题。</blockquote><p></p><p>&nbsp;</p><p>在一个<a href=\"https://www.reddit.com/r/golang/comments/15pk1su/rip_aws_go_lambda_runtime/\">热门的Reddit帖子</a>\"中，有些Go开发人员强调了<a href=\"https://www.reddit.com/r/golang/comments/15pk1su/comment/jvy9wm6/?utm_source=share&amp;utm_medium=web2x&amp;context=3\">缺乏沟通</a>\"的问题，而另一些人则<a href=\"https://www.reddit.com/r/golang/comments/15pk1su/comment/jvyq64r/?utm_source=share&amp;utm_medium=web2x&amp;context=3\">支持这一变更</a>\"。NightVision的创始工程师、AWS Serverless Hero&nbsp;<a href=\"https://www.linkedin.com/in/aidansteele/\">Aidan Steele</a>\"则将关注的重点放在了<a href=\"https://awsteele.com/blog/2023/08/02/useful-flags-for-go-lambda-functions.html\">标记的收益</a>\"上：</p><p>&nbsp;</p><p></p><blockquote>我已经在使用更新的运行时了，但我也从博客文章中了解到，AWS SAM现在可以为更新的运行时构建Go Lambda函数了——不再需要Makefiles！我从BuildMethod: makefile切换到了BuildMethod: go1.x，发现我的Lambda包现在是原来的两倍大。这意味着冷启动和部署速度均会变慢。</blockquote><p></p><p>&nbsp;</p><p>为了减轻对开发人员的影响，公告介绍了如何将函数从go1.x运行时迁移到provided.al2运行时，以及如何对构建脚本或CI/CD配置进行修改。</p><p>&nbsp;</p><p>查看英文原文：<a href=\"https://www.infoq.com/news/2023/09/aws-lambda-go-deprecation/\">AWS Lambda Deprecates Go Runtime</a>\"</p>",
    "publish_time": "2023-09-19 14:40:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "开发者与大模型的双向奔赴，就在百度智能云千帆大模型平台应用开发挑战赛！",
    "url": "https://www.infoq.cn/article/JAE4rgf6s1cHNQoYZ93I",
    "summary": "<p>作为大模型时代的亲历者，我们一同见证了大模型技术从狂热到理性，从想法到实际落地的演变过程。时至今日，随着大模型应用开发热度越来越高，如何更快、更高效地把大模型应用落地到生产环境，以及如何将大模型与现有系统整合、改造，成为当下所有从业者关注的焦点。毋庸置疑的是，大模型将成为未来技术革新、生产力跃迁的关键力量。</p><p>&nbsp;</p><p>然而，作为一项新兴技术，业界对于大模型可能诞生的应用模式仍处于探索阶段，我们也深知大模型在对话、作画等应用之外，还有更多的想象空间。若想推动大模型在实体产业落地，切实为社会经济带来价值，无疑需要更多开发者的加入。与此同时，在大模型应用落地成为大势所趋的当下，业界也正在呼唤一批既懂行业技术又懂AI训练的综合型人才。如同李彦宏在2023百度云智大会上提到的：“卷大模型没有意义，卷应用机会更大”。</p><p>&nbsp;</p><p>现在，百度已经为所有大模型应用开发的创业者以及开发者，提供了综合演练平台——百度智能云千帆大模型平台2.0，其能够预置最多的大模型和数据集、有最丰富最全面的工具链、最佳算力效能、最完备的大模型安全方案。</p><p>&nbsp;</p><p>为了鼓励行业用户与开发者发挥想象力和创新精神，也为了进一步助力大模型的产业落地。<a href=\"https://mp.weixin.qq.com/s/CJXhC_eT-EhPU9f2L2QTSQ\">百度智能云千帆大模型社区</a>\"发起了【大模型应用开发挑战赛】！如果你期待为大模型应用落地贡献力量，如果你想了解前沿的大模型技术方案、落地案例；如果你希望得到百度大模型技术专家的辅导......这场【大模型应用开发挑战赛】或许能够满足你的所有期待！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7d/7d4b06f6da65ef0e5abbeaab6674d4ac.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/056e5e2e313f96731f260c7447d7e1a2.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc1b32262360ee79d8bec1b2259a519e.png\" /></p><p></p><p>报名通道现已开启！参赛选手可以基于百度智能云千帆大模型平台，开发任意一款应用，主题不限，使用场景不限！欢迎到<a href=\"https://cloud.baidu.com/qianfandev/bac?track=infoq\"> https://cloud.baidu.com/qianfandev/bac?track=infoq </a>\"，了解更多赛事信息。</p><p></p><p>参赛开发者请加入大赛交流群，及时获取大赛最新动态。</p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed3a88d489f2cc9e289358b56876d98e.png\" /></p><p></p>",
    "publish_time": "2023-09-19 14:45:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从单体到微服务的系统改造：采用事件驱动架构优化会员系统",
    "url": "https://www.infoq.cn/article/fOLaSc3lmxIMGfTBTvEI",
    "summary": "<p>&nbsp;</p><p>Baemin是一家快速增长的送餐服务公司，它已经成功地从单体架构过渡到了更灵活的基于微服务的系统。这一转变的基石是事件驱动架构的实现，这是<a href=\"http://tech.deliveryhero.com/building-event-based-architecture-for-member-system/\">Baemin架构之旅</a>\"的一个方面，正如最近的一篇博客文章所描述的那样。该架构的突出特点包括通过分层的事件订阅者细致地分离了关注点，以及战略性地使用事件存储来提高可靠性。Baemin利用通用的关系数据库管理系统（RDBMS）并采用<a href=\"https://microservices.io/patterns/data/transactional-outbox.html\">事务发件箱模式（Transactional Outbox Pattern）</a>\"，构建了一个可扩展且可靠的健壮框架。</p><p>&nbsp;</p><p>最初，Baemin努力克服单体架构的局限性，但随着公司规模的扩大，这种局限性变得越来越明显。系统无法处理激增的流量和订单，导致了大量的错误。向微服务的迁移于2019年11月完成，为下一阶段奠定了基础：采用事件驱动的架构。该架构对于实现各种微服务之间的松耦合、增强系统的弹性和灵活性至关重要。</p><p>&nbsp;</p><p>Baemin事件驱动架构的本质在于发布领域事件，而不是命令或请求。这种微妙但关键的区别可以减少不同系统之间的依赖关系。在传统的基于命令的系统中，一个服务的更改可能需要另一个服务也进行更改，从而导致紧密耦合的架构。通过关注领域事件，Baemin确保每个微服务独立运行，并订阅与其领域相关的事件。</p><p>&nbsp;</p><p>在Baemin的事件驱动架构中，事件和订阅者被组织为三层，以确保模块化和可扩展性。第一层，应用程序事件和第一订阅服务层，利用Spring框架的应用程序事件来管理内部、特定于领域的任务，并通过<a href=\"https://aws.amazon.com/sns/\">AWS简单通知服务（SNS）</a>\"发布这些事件。第二层，内部事件和第二订阅服务层，处理的任务虽不是领域的核心，但却是必不可少的。例如，在登录过程中，该层负责一些次要任务，例如从其他设备中注销。最后，第三层称为外部事件和第三订阅服务层，用于发布外部系统将使用的事件。这些事件是通用的，以避免创建与外部系统的依赖关系。这种分层的方法允许Baemin在保持松耦合架构的同时确保了每个微服务都能独立运行。</p><p>&nbsp;</p><p>事件驱动架构的挑战之一是确保事件发布的可靠性。Baemin通过引入事件存储系统来解决这一问题。该存储系统使用与其领域存储相同的关系数据库管理系统（RDBMS），从而确保了事务数据的一致性。事件存储充当缓冲区，捕获事件并确保它们能可靠地发布。</p><p>&nbsp;</p><p>该架构采用<a href=\"https://microservices.io/patterns/data/transactional-outbox.html\">事务发件箱模式（Transactional Outbox Pattern）</a>\"来确保数据一致性和可靠的事件发布。此模式对领域数据和事件数据使用相同的存储，从而实现可靠的事件发布机制。下图是其最终的设计，展示了不同类型的事件、事件存储系统和订阅者层之间的交互。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fceecdf3c371c6a7144335f39a3a5a51.png\" /></p><p></p><p>&nbsp;</p><p>尽管如此，最终的设计还是值得注意的，因为它通过不同层次的事件订阅者细致地分离了关注点。使用事件存储来确保可靠性以及关注松耦合的领域事件是其突出的特点。</p><p>&nbsp;</p><p>总而言之，Baemin的架构转型为解决规模问题的组织提供了丰富的见解。通过对事件订阅者进行分层，并利用由通用RDBMS支持的事件存储，Baemin精心打造了一个不仅满足其当前需求，而且非常适合未来的可扩展增强系统。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/09/delivery-hero-uses-eda/\">https://www.infoq.com/news/2023/09/delivery-hero-uses-eda/</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-09-19 15:06:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "行业老兵聊 To B 产品技术：To B 难，难不过做好软件",
    "url": "https://www.infoq.cn/article/llePoFGYerfzfWRZeMVi",
    "summary": "<p></p><p></p><p>To B 和 To C 这两个词是国内互联网行业的高频词汇，连普通老百姓也挂在嘴边，但尽管是两个洋文，到谷歌上搜却没几条相关的结果。</p><p></p><p>正如“互联网公司”这个词在海外找不到恰当的英语单词一样，To B 这个词也有点中国互联网特色的意思。海外的技术企业通常用“企业级软件（Enterprise Software）“来指代 To B 产品，且包括业内常常认为的不太像软件而更像所谓互联网产品的 Salesforce、Workday、ServiceNow 等 SaaS 服务。大概是中国互联网的成功主要体现在平台模式上，to 这个词隐约强调了一种交易或服务的商业模式，也成了 oriented 的简写。</p><p></p><p>阿里巴巴的 B2B 应该是国内业界最成功的 To B 业务之一，当然其业务不同于我们今天所说的云计算、钉钉、飞书、纷享销客等这类 To B 服务。如果在谷歌上搜索 B2B、B2C 的话，确实能精确命中我们期望的内容，但搜 To B 就不太行。</p><p></p><p>顺应大家习惯的概念，这里使用 To B 这个词，但讨论的主要是企业级 SaaS 产品。我在钉钉供职期间，常提醒团队不要认为“互联网”比起“软件”有多高大上，看目前市值最高的苹果和微软都不是传统意义的互联网公司。尤其在讨论“可以本地部署的 SaaS 产品”都不愿承认这玩意儿就是软件的时候，就有些大可不必了。做软件不是什么丢人的事，本文就是在探讨软件工程，敞亮一些。</p><p></p><p></p><h2>产品篇</h2><p></p><p></p><p></p><h4>正视 To B 产品的复杂度</h4><p></p><p></p><p>近些年来人人都在讨论 PLG，而把 To B 产品做成和 To C 产品一样易用的理念也几乎成为主流。一些 To B 产品的界面设计越来越趋近于 To C 产品，强调移动端体验，使用口语化和互联网语言的文案。</p><p></p><p>将 To B 产品的用户体验做成和 To C 产品一样低门槛和傻瓜化是应该的，但另一方面 To B 产品的系统复杂度是永远不可能和 To C 产品一个级别的。这就好比 Excel 尽管很易用，但仍然是极其复杂的，甚至可以直接用来开发三国杀游戏，所以这二者并不矛盾。</p><p></p><p>我常开玩笑说，To B 产品一个新功能的引入会导致至少九个以上的功能点，因为各种策略的组合，以及管理员的不同维度的汇总需求会自然地增加功能集合的复杂度。另一个玩笑是，To B 产品最终都会变成一个 BI 产品，因为老板总是希望横着看又纵着看，看完部门维度又看级别维度，说不一定弄成 Excel 文件后来做报表还方便些。</p><p></p><p>我见过很多产品经理将大量精力花在缩短用户链路和功能透出上，比如把二级入口升级成一级入口，但用户容易找得到，但导致了界面不简洁。或者通过并不严谨的数据模型来频繁调整界面元素，让用户觉得困扰。</p><p></p><p>我认为 To B 产品本来就该有一定的用户教育成本的，产品应该设计得易用，但不能为了简化链路而牺牲其他方面，期望用户拿到以后就像微信或淘宝一样是没有意义的。一个给员工使用的 To B 产品，即便没有相关培训，新员工一定会在同事或者主管的帮助下迅速学会产品，毕竟是吃饭用的家伙。这种学习是一次性的，对公司几乎没有成本，也不会因为单个员工的个体行为造成产品的用户流失。员工在工作中本就还需要大量其他的工具，所以这种“第二次就会了”的情景本身就是工作的日常。</p><p></p><p>To B 产品的复杂度既是用户的门槛，另一角度看也是展现自身价值和竞争力的舞台。如果觉得自己烧钱搞流量搞不过对手，但是有很强的产品技术能力，那么可能更适合做 To B。</p><p></p><p></p><h4>过度数据驱动</h4><p></p><p></p><p>国内的消费互联网是大数据的海洋，所以长期以来 To C 产品的设计很重视用户链路的相关数据，并基于此指导产品设计。这种产品理念也被顺理成章地应用于 To B 产品设计中，看链路漏斗，做 A/B 测试。</p><p></p><p>数据在产品设计中有重要参考价值，但如果过分迷信于数据，用力过猛，给 To B 用户带来的负面困扰也会远远大于 To C 产品。To C 产品思路通常认为高频功能应该放前面，但 To B 产品的用户角色是极其丰富多样的，对于某一角色的高频不代表对于另一角色就是高频。即便像 Word 这样的通用软件，“文件”菜单总是放在第一位，几十年来均如此，但“文件”菜单的使用频度却远远低于“编辑”菜单。</p><p></p><p>对于数据的分析和理解需要严谨，基于统计学，多个维度分析，避免一概而论，个体和共性混淆不清，更注意避免幸存者偏差。对于数据的理解不到位，将因果关系搞反，就成了“火车上的人居然都买到了票”的笑话。</p><p></p><p>产品经理是人，而我认为人在产品设计中的主观能动性仍然是最主要的因素。数据固然重要，但将决策交给数据和机器，则容易偏离产品设计的初心。有的产品甚至成了所谓的运营驱动，界面搞得很热闹，甚至还有广告位。对于产品团队 KPI 的设置，如果过于简单地下一些数据指标，很容易团队变形走样。产品像是一座高楼，是一个有机体，不是一根根烟囱，也不是一层层蛋糕。如何设计团队的 KPI 考验的是管理者的智慧，从而决定构建这做产品大厦的时候谁来当地基谁来当梁谁来当柱。</p><p></p><p>相对于 To C 产品，To B 产品的交互更稳定，升级变化更谨慎。按我前面的观点，使用 To B 产品就是要学习的，因此每一次变化都会有一定的成本。这不是说 To B 产品要万年不变，而是要深思熟虑，考虑回报和风险。反过来，同样的道理，如果有充足的理由要改产品，那么也不必过度担心用户找不到原来的功能。</p><p></p><p></p><h4>康威定理</h4><p></p><p></p><p>康威定理认为，产品必然是其（人员）组织沟通结构的缩影。有些产品的设计有明显的组织架构痕迹，甚至用户看一级菜单或者 tab 就能猜出其背后团队的组织架构。康威定理是对现状的总结，甚至在一些系统设计中（如微服务）被用于作为基础指导思想，但要警惕这只无形的手始终在影响产品的设计。</p><p></p><p>团队内部打架的情况不是新鲜事，当一级入口位置或顺序出现内部竞争时，康威定理带来的负面影响就会放大。如前面所说，To B 产品的价值决定了其本身就是复杂的，用户看到的部分是冰山露在水面上的部分，真正的核心是在水下。所有的软件系统从宏观和微观层面看都是分层的，或者说是以堆栈的形式存在，但团队的组织结构却又是树状的。如果层次定义不清晰，而是纯从用户的角度去划分产品模块，容易出现所谓烟囱式的产品设计，支离破碎。</p><p></p><p>国内互联网行业通常存在一种误区，认为所谓的架构设计是技术问题，不是产品经理关心的事，也很少听说有哪个公司或团队设立产品架构师这一岗位。产品中的用户账号系统，用技术语言叫目录服务，用产品语言叫通讯录，是每个 To B 产品的最核心系统之一，也是层次结构里躺在最底层的那一个系统。用户在写邮件的时候，从网盘选择一个文件作为附件，这个过程中邮件系统和网盘系统的交互是怎样一种依赖关系，不仅仅是个技术问题，也是一个产品架构问题。</p><p></p><p>创业团队一开始做出的东西都很精致，而规模上去以后就容易变粗糙。一个大型 To B 产品是否是个有机的整体，最终是否能成为一个好的作品，往往取决于其面对康威定理时候所表现的韧性。</p><p></p><p></p><h4>维度一致性</h4><p></p><p></p><p>看一个复杂 To B 产品是否有设计原则和设计模式，可以看其功能类似的实现和用户操作是否有基本的一致性。举个例子，用户导出一个文件，可能是异步操作等完成后发通知给用户，要考虑文件可能过大要切割，以及临时存储空间和清理的问题。一个有一定复杂度的 To B 产品往往由不同产品经理，甚至不同产品部门负责。产品不同系统的文件导出行为是否能做到一致，需要由统一的设计模式来约束，并有相应的 review 机制。</p><p></p><p>一些产品经理会根据部分用户的反馈立即做出产品调整，解决用户问题或痛点，但这种快速的反应往往缺乏深思熟虑，属于被动的见招拆招。如果一个产品长期以这种方式开发，那么迭代越快，导致的碎片化就越快。</p><p></p><p>我要求产品经理使用维度一致性去检查新的产品设计，即回答以下问题：</p><p></p><p>是否是在原有维度上扩展，还是引入了新的维度？如果引入新的维度，该维度上可能存在的元素是哪些？</p><p></p><p>看个例子，一个网盘里的文件一般通过可编辑、可读等属性实现 ACL 模型来管理文件权限，或者结合 RBAC 模型提供更丰富灵活的权限功能。如果某大客户提出一个“机密文件”的需求，在符合某些规则的情况下才能访问，那么这个功能的引入就是增加了一个新的维度。假如真的要考虑支持机密文件，就要检查该维度对现有各种产品的影响，比如是否有机密邮件、机密聊天、机密项目、机密会议等等，举一反三。尽管客户可能只提出了一个文件是否为机密类型的二元条件，但可能未来扩展成为高、中、低不同等级的机密，或者更细粒度。机密文件的本质是根据对象的属性进行权限判断，实际上已经属于 ABAC 模型的范畴，顺着下去还可以进一步泛化，比如根据创建时间来制定规则等等。</p><p></p><p>对于引入新维度的设计需要极其谨慎，因为新维度的出现必然影响现有的产品系统，甚至产生全局影响。产品架构师最好能维护一张大图，总结产品中各种维度的现状和未来可能的发展。</p><p></p><p></p><h4>竞品与定制化</h4><p></p><p></p><p>人人都知道要搞好竞品调研，而最通俗易懂的竞品利用方式，大家也都懂的。探讨一点反向的方式，产品经理设计一个新功能的时候，要多问一个问题，竞品有么？</p><p></p><p>对于竞品的分析和调研，要从正向和反向两方面去看。友商有自身的优势和劣势，做与不做的选择是什么。一个功能如果竞品没有，为什么？是友商没想到，没来得及做，还是人家就是不做？</p><p></p><p>面对大客户需求的时候，竞品现有的产品设计可以被借用来作为产品语言帮助描述需求，降低沟通难成本。另一方面，如果竞品也没有这样的功能，而自身也认为不合理的时候，可以考虑以此作为理由之一来婉拒客户的需求。</p><p></p><p>定制化需求应该是 To B 产品和技术团队最头疼的事之一。很多团队一开始总是立 flag 不做定制化，后来顶不住前线的压力，或者看重了某大客户的市场传播效应，最后硬着头皮上。定制化一定是亏本的，与 PLG 理念是冲突的。决定要做定制化就一定是在投资未来，要么该需求有朝一日成了多个客户的通用需求，要么财务报表上姑且当作市场推广费用记作运营成本。</p><p></p><p>常有人问我微软是怎么对待客户的定制化需求的。微软基本上没有定制化需求，包括自家用的 Windows 和 Office 也是和客户的版本一模一样，而且也不赶工期。微软的策略可以这样形容，如果一个需求不满足客户要求导致丢单，微软会去制定相应的产品研发计划来避免丢掉第四到第十个客户。为什么说第十个客户，因为需求需要有足够的通用性和足够大的客户群覆盖。为什么说是第四个客户而不是第二或第三个客户，是要保证产品计划的正常实施而不去为客户而赶工。</p><p></p><p></p><h2>技术篇</h2><p></p><p></p><p></p><h4>熵增与破窗效应</h4><p></p><p></p><p>To C 产品的用户角色相对单一，且同一用户用角色或类似用户画像覆盖了大量用户，这些用户通常具有相同的操作链路和使用习惯。当这种特性反映到工程实践中，一些技术团队选择聚焦在主链路的测试，甚至依赖线上真实用户的使用来验证功能，搞 Test in Production。</p><p></p><p>这种工程理念如果沿用到 To B 产品中很可能会出现水土不服的情况，迟早要因为功能 bug 被客户骂。由于 To B 产品角色复杂，客户和用户设置丰富，用户链路组合的复杂度是指数级增加的，而某一链路所覆盖的用户量又显著小于 To C 产品。有的功能可能还是季节性的，甚至是每年或者几年才用一次。</p><p></p><p>做 To B 产品的质量保证，自动化测试必不可少。即便手工测试外包再便宜，也不可能靠堆人来追赶指数级增长的复杂度和开发写 bug 的速度。自动化测试不是为了测试新的功能，而是保证添加新代码的时候已有的功能不被破坏，也就是回归测试。除此之外单元测试也会倒逼开发去设计更加抽象和优雅的接口，让代码质量和可复用性更高。</p><p></p><p>任何系统总是朝着无序的方向发展的，软件的熵增是质量下降的罪魁祸首，而技术团队的职责之一是通过持续的修护和维护来对抗软件的熵增。修复工作的周期越长，带来的成本就越高。软件的 bug 有明显的破窗效应，一个漏洞出现后不去修复，会立刻出现更多的漏洞，而且相互叠加和遮挡，最终引起故障。</p><p></p><p>几乎每个开发都懂得“面向失败（failure）设计”的大道理，但并非每个人都真正理解失败这个词的意思。代码里的 bug 与光缆被挖掘机挖断是两种本质截然不同的失败，前者是可控的而后者是不可预期的。光缆是别人挖的但 bug 是自己写的，能在上线前干掉的问题就不要留给客户。</p><p></p><p>每个技术团队都梦想着能达成持续集成持续交付（CI/CD），而且产品和业务团队也希望能快速迭代，恨不得每周都上新功能。但另一方很多团队又嫌写测试代码占用大量时间，不愿意投入，结果就是越想快团队越被动越狼狈，欲速不达。自动化测试是 CI/CD 的前提条件，整个 pipeline 要像无人值守的流水线一样有节奏的跑起来，像地铁时刻那样精准，才可能实现 CI/CD。一旦其中一个环节要人工参与，就不可能顺畅，一定卡壳。</p><p></p><p>我之前决定重写阿里邮箱的前端和桌面端代码，要求团队一开始就奔着 CI/CD 的目标去。团队最后达成了 90% 以上的自动化覆盖率，甚至包含了 UI 层代码，真正意义上地验证了磨刀不误砍柴工。我的前端团队负责人孟红伦（高级前端专家）还在 InfoQ 全球前端技术大会 GMTC 上分享了《CI/CD 在钉钉前端的实践》。</p><p></p><p></p><h4>技术信心</h4><p></p><p></p><p>苹果的产品用户体验好基本是公认的了。除了产品设计之外，所谓体验好的一大原因是觉得质量稳定，bug 很少。作为苹果用户，当看到一个错误消息的时候，一般会觉得是不是自己哪里操作错了，应该很少人会想到去找苹果客服问问怎么回事。反过来，某些产品使用中看到错误提示的时候，却可能趋向于怀疑该产品有 bug，而不是自己操作错了。</p><p></p><p>这种心理上的差异背后不仅仅影响产品本身在用户心中的形象和口碑。对于技术团队来说，面对外部反馈问题第一时间的反映，也会很大程度上影响团队的工作方式和生产力。当有人反馈说某个地方可能有 bug 的时候，开发工程师心里有没有咯噔一下，然后决定是否要去查问题，是不是赶紧得去查问题。</p><p></p><p>鉴于目前还没找到个好词，我个人把这一现象姑且称为技术信心。尽管网上都是各种关于程序员说自己代码没 bug 的段子，不同人的底气确实是有差别的。如果一个团队的工程师总是在查问题，一听到问题心里就咯噔，赶紧去看日志，那么可以想象这个团队的生产力不会太高。To B 产品的复杂度会放大这种差别，拉长了看甚至会影响团队的投入产出比。</p><p></p><p>在一个复杂系统中，“自证清白”是一种技术能力，也是一种合理的工程实践。我经常给团队讲需要我们需要搞一点 finger pointing technology，可以很快且自信说“不是我的问题，肯定是你上游（下游）系统的问题”。尤其在微服务大行其道的今天，快速排除可能性是高效的关键，也符合清晰定义系统接口和输入输出的原则。</p><p></p><p>做 To B 人人都讲客户第一，但很多技术团队有个误区，将帮客户查问题和数据修复这类服务式的行为当作客户第一的表现。团队里宣传某技术小哥哥帮客户查问题查到半夜，不知道有没有感动客户还是客户其实在背后骂，反正是感动了自己。真正的客户第一是把高质量的产品交到客户手里，而不是给客户制造问题再去解决问题。</p><p></p><p>一个团队的技术好不好？回答这个问题不是那么直接，角度和标准太多，也很难将不同领域的团队拿来做比较。如果要用一个简单的指标，技术信心也许可以说明一些问题。</p><p></p><p></p><h4>联调与日志</h4><p></p><p></p><p>联调几乎是工程师的日常，理所当然的工作内容，但我之前问了团队一个问题，这个词的英文是什么？大家讨论了一阵，排除了几个短语之外，最后说貌似是 debug together 最贴切。看起来是一个人自己不太好 debug，需要 together？这里有个问题，一个人的工作时间和方式对另一个人产生了依赖。而且这个不是系统或接口依赖问题，而是人之间的依赖问题。</p><p></p><p>联调的首要任务好像是把链路调通，然后貌似就成功了一半。但真实的情形是，代码成功的链路只有一种，出错的情况则可能是十八种。除了唯一成功的那种情况之外，异常和错误代码也是接口定义的核心组成部分，却容易被忽视。一些团队的系统由于种种原因，开发并不清楚自己要调用的接口会返回哪些错误，甚至也不知道自己提供的接口会给上游系统返回哪些错误。仿佛联调通了代码就算差不多写完了，或者写代码就等于 debug，一上来就开始联调。有些代码设计和接口定义，几乎抛弃了现代编程语言和框架所提供的丰富的错误处理支持，只有成功和错误两种状态，至于为什么错误，一定是要去看日志的，生生把 Java 写成了 C。这种过于简单的二元状态用来处理 To B 产品的复杂度就像在用鸟枪打怪兽，总觉得不够用。产品不出错还没感觉，一旦出错就全都是意外情况。</p><p></p><p>对联调的过度依赖暴露的是系统之间合约不清晰的问题，工程师不是基于规定好的接口定义去开发自己的那部分代码，而是通过全链路的表现来判断自己负责那部分的代码是否按照既定方式执行。这种开发方式一方面影响工程效率，互相拖后腿，另一方影响产品和系统质量。</p><p></p><p>我和团队说要把自己做成一个黑盒，即便你的兄弟团队可能就坐在你的隔壁，设计系统的时候要把他们当作外公司的人来看待。设计一个复杂系统，“通”是很简单的，而“堵”才是有技术含量和艺术性的，这也是为什么面向对象编程讲究封装和低耦合。</p><p></p><p>服务端开发的幸福之处是可以各种打日志（log），这种触手可及的方便甚至让一些工程师产生惰性，什么事都通过日志来解决，很少去思考怎么把系统接口设计成完整和自解释的。有开发甚至把 log 当 trace 用，在日志中完整地用英语描述了一遍代码的执行过程，就像打日志不要钱一样。</p><p></p><p>服务端的业务代码大多都跑在 Linux 上，但搞服务端开发并不需要去查 Linux 的日志，这是因为和 OS 的交互定义得足够清晰，而 OS 内部也有精心设计的层次关系。若回顾下历史，今天的很多 To B 的 SaaS 曾经都是单机软件或者部署在局域网内，而这些软件在开发工程师无法查阅系统日志时也照样稳定地运行。软件的云服务化并不意味着一定要拼命地打日志才能开发出好的 SaaS，而情况恰恰相反，如果一个系统要靠日志才能描述清楚其状态，那几乎可以说明这个系统设计得很糟糕。</p><p></p><p></p><h4>仍然是软件</h4><p></p><p></p><p>当 To B 业务的竞争日趋激烈，某些客户的数据安全及合规要求成为获客的门槛时，一些 SaaS 厂商为了拿下订单，开始将云服务改造成可以在专有云环境甚至客户本地环境部署。接到客户的单后，出身互联网大厂的技术负责人突然一拍大腿，原来我们是在做软件啊！</p><p></p><p>在阿里随便抓一个服务端 P7 都是世界顶尖的高可用高并发系统专家，设计开发的系统能抗住双十一那样的流量，但 To B 产品的技术挑战却不仅仅在这些方面。To B 产品要高确定性、高质量、高可靠，出现脏数据的危害在某些情况下甚至超过系统不可用。功能降级是高可用系统设计中的一种常用手段，在系统资源吃紧时候屏蔽一些功能，甚至牺牲一些用户，在 To C 上是可以接受的，但放到 To B 场景里面，这样的设计就要极其谨慎，因为 SaaS 服务的 SLA 所包含的条款可能不仅仅是可用性，搞不好还得赔钱。</p><p></p><p>相比 To C 互联网的架构，软件产品的架构往往更加注重高内聚和低耦合，而做到这两点是需要思考和投入的。系统既可能由开发工程师团队来运维，也要准备好可能交给第三方或者客户来运维。比如前面提到的日志，假如只有开发自己看得懂日志的内容，那这种部署的灵活性就很会很差。客户的环境可能缺少某种中间件，或者由于合规原因只能使用某一家厂商的产品。这些额外的需求和限制，是互联网 To C 产品中几乎不会遇到的问题，但这也是 To B 技术团队比拼技术实力的时候。一些架构设计上的考量和原则，完全可以去参考和借鉴一下经典软件系统的设计思想和实践。</p><p></p><p>Linux、MySQL、Kafka 等这些大家用来开发互联网系统的优秀产品，也都是一个个 To B 软件。SaaS 相对于 MySQL 来说是应用程序，而 MySQL 相对于 Linux 来说也是应用程序，开发 CRM 和开发 OS 的工程理念是一样的，因此从本质上讲并没有根本的差别。有的服务端技术小哥能迅速实现一个漂亮的微服务系统，接口定义得很整齐，却写不出一个好的 SDK 给其他人用，很可能是放不下心中的那朵云。经常有后端开发讨论单元化，其实单元化不就是软件化吗？无论软件是运行在客户端还是服务端，最本质的东西是一样的，也是几十年来不变的那些东西。</p><p></p><p></p><h2>团队篇</h2><p></p><p></p><p></p><h4>To B 产品经理</h4><p></p><p></p><p>无论是 To B 还是 To C，产品经理都是核心的发动机角色，产品经理犯一个错误可能把连带其他职能的整个综合团队带到沟里去。大学里是没有产品经理这门专业的，而如张小龙、周鸿祎等知名的产品经理的代表凑巧又都是技术出身的，所以业内总在讨论到底产品经理需要什么样的技能和素质。</p><p></p><p>都说 To C 的产品经理需要懂用户，洞悉人性，有很强的同理心，那 To B 的产品经理是不是翻译客户的需求就搞定了？我认为 To B 产品经理的同理心是最重要的素质之一，不仅要深刻理解的不同用户角色如何使用产品，还要理解同一角色的不同用户的认知和使用差异。同理心意味着社会阅历，也意味着不是越年轻越好。</p><p></p><p>产品经理的职责是用软件来描述真实的世界，所以抽象能力和逻辑思维是最核心的专业能力，而这并非开发写代码时候才需要的能力。产品经理和开发的区别只是开发用某种编程语言，而产品经理用接近人类自然语言的抽象语言来描述产品逻辑。这里说的是“接近”，因为在某些情况下自然语言并不是最理想的工具，需要用交互图、UML 图、甚至伪代码来描述。</p><p></p><p>我给经理提团队的要求是纵向能看到用户故事，横向能看到系统分层。我不希望 To B 产品经理只会用交互图来描述产品设计，而把图背后的一切通通丢给开发去搞定。我希望产品经理都是 3D 型的，而不是纸片型的。开放能力是 To B 产品的核心竞争力之一，理论上设计任何用户功能的时候要考虑对应的开发 API 的设计，所以开放接口的其实应该是产品设计文档的一部分，而不是技术设计文档的一部分。如果一个产品文档上有流程图和对象 OR 关系图，甚至还有开放接口的描述，那肯定可以说明文档作者是经过了深思熟虑的。</p><p></p><p>To B 产品的核心竞争力在于产品能力，而不是表层的用户交互。微软的产品开发速度是出了名的慢的，但几乎只用了半年多时间就能开发出 Teams，其原因是微软早就具备了 Teams 所需要的产品能力，无论是聊天、视频会议还是文档，微软一直都是领先者。Teams 只不过是之前 Office 365 众多套件产品的一个重新组合下的 App 而已，换句话就相当于是把 Skype for Business 的窗口拉大了，所以轻松超越 Slack 是必然的。做几个 App 是很简单的事，而难的是构建冰山在水面下看不到的产品能力，所以 To B 产品经理一定要搞清楚自己的产出是什么。</p><p></p><p>计算机不是神，代码不是仙丹，产品的流程和逻辑的设计都和代码都没有什么关系。曾经有一位产品经理设计了一个运营用的游戏，房间里可以支持最多一万人抢金币，而其中任何一个人在某一刻获得的金币会影响其他所有玩家手中获得的金币数，同时任何一位用户加入或者离开也会影响其他所有人，而游戏又要求所有用户的金币数实时刷新。我看了需求后问了一个问题，想象一下体育场里有一万个人，每人拿着一张卡且上面有个分数，有工作人员拿着喇叭指挥所有人到工作人员那里去登记并修改卡片上的分数，任何人一改其他人都要去改。我问说即便有很多的工作人员，要怎么设计这个活动才能避免里面的人不是一直在排队等待。这位产品经理回去想了一阵子后意识到这个设计是有问题的，就算计算机 CPU 飞快，硬盘和网络吞吐量巨大，这个游戏都玩不起来。作为产品经理，不要以为程序员有魔法能让什么都很快很流畅。把除代码之外的事都提前想好了，才是开发眼中靠谱的产品经理。</p><p></p><p></p><h4>技术驱动组织架构升级</h4><p></p><p></p><p>一个 To B 企业和团队的组织架构如何设计，这个问题层次有点高，以我的水平不敢造次胡乱瞎扯。在此尝试探讨一个常被提及的问题，产品和系统之间的依赖和相似性引起的协同问题，要怎样调整组织架构来解决。</p><p></p><p>如果 A 产品和 B 产品类似，或者 X 系统依赖了 Y 系统，很直观的做法是将负责这些产品和系统的团队放在一起，由同一位负责人来管理。但实际操作时会遇到问题，A 的一部分和 B 很类似，但 A 的另一部分又和 C 很类似。但处于现实原因又不可能将 ABC 三个团队放在一个主管下面，而且这样的问题还有传递性，环环相扣。如果单纯按照产品系统来进行组织设计，做组织架构调整时候可能会陷入一种永远调整不完，按下葫芦浮起瓢的尴尬境地。好不容易调整得差不多了，又到了下一次调整的时间，龙头动一下要很久才能把龙尾也摆直了，给组织带来了不必要的内耗。</p><p></p><p>我个人的观点是，组织架构的设计主要要看业务与人才的考量，而应通过技术手段来消除系统相似度和耦合度带来的困扰。To B 的产品和技术很复杂，系统之间的交互是图状模型，现实中很难做到完美的划分。既然如此，是不是可以换一种思路，把系统交互的因素降低到最低，尽可能消除部门墙。在设计产品和系统时，假如都能把自身的职责和接口定义清楚，那么不管上游下游系统是不是在一个部门内，至少从技术上就能做到不依赖组织架构。如前面提到的例子，调用你系统的团队可能在隔壁，也可能在另一个城市，甚至可能根本不是一个公司的。如果能尽量消除这种差异，那么做组织架构设计时就少了很多障碍，从而更好地帮助组织架构升级。</p><p></p><p>微软的所有产品把自身的开放 API 通过 Microsoft Graph 统一网关提供给开发者，以此来促进开发者生态的繁荣。而更重要的是，内部一方系统的交互也是通过这个平台完成的，遵从了一方即三方的原则。在这样的原则下，部门负责人可以更多聚焦在客户需求上，而不是把精力浪费在所谓的内部系统与外部系统的讨论中。微软还有一个很好的例子，用户要使用 Word 并不是一定要下载 Word 的 App，而可以通过 OneDrive、Teams、Outlook、SharePoint、Offce.com 等任何一个 App 和网页版，而这些部门也不用去争吵到底是谁的用户。海外一些 IT 公司做到了所有部门代码向全公司开放，甚至有的公司使用巨大的 mono repo，本质上也是把技术因素所带来的协同障碍降低到最低。开放不仅仅是个产品技术问题，也是一个理念和管理问题。</p><p></p><p>技术为业务服务，但如果技术即是业务，那么也不要让生产关系阻碍了生产力的发展。技术团队是否有话语权，是否能够聚焦打造产品和技术壁垒，也是组织架构设计的关键。</p><p></p><p></p><h4>长期主义与相对创新</h4><p></p><p></p><p>人说互联网唯快不破，也有张小龙突击开发微信的成功案例，这句话放在 To C 互联网上是真理，但放在 To B 产品里容易成为坑。To C 互联网要建设交易平台，要打造关系链，用户粘性是一切商业模式的基础，抢占先机至关重要。反观 To B 领域的 SaaS，这个逻辑是很不一样的。在 To B 里面我们已经看到了无数后起之秀的成功案例，有的后浪把前浪拍的死死的。这种现象的核心原因是 To B 产品通常靠产品能力取胜，而不一定在于产品上有多少用户。</p><p></p><p>To B 产品的数据属于客户而不是厂商，所以客户在决定是否使用和何时使用某产品时处于完全的主导地位，这就不是一个简单的快和慢的问题。一些 To B 的厂商故意给客户数据迁移设置产品和技术门槛，希望以此留住客户，我认为这是短视的表现。我同团队强调过，如果客户要来我们铺着红地毯欢迎，如果客户要走我们也铺着红地毯把客户送走。数据迁移能力本身就是产品能力和开放能力的表现之一，而一些客户在选择厂商时候本身就会考虑是否能安全地讲数据迁移走。在这个逻辑下，在某个产品功能上比对手快并不一定会带来明显的优势，甚至还会帮对手承担试错成本。</p><p></p><p>文章前面讲了那么多观点，但如果不给产品和研发足够的时间来把这些事情做好，都是白扯。从工程角度讲，任何产品都应该追求快速迭代，因为这关乎效率和成本，但前提一定是以高效的方式来迭代。假如一味追求上线时间，打乱开发节奏，甚至跳过一些必要的步骤，抢来的时间还不够被后面填坑浪费的。</p><p></p><p>既然选择做 To B，就选择了长期主义，看微软、AWS、SAP、Salesforce 都是几十年经营的成果。当下最耀眼的 Open AI，也是耕耘多年才开花结果的，不同于国内互联网行业普遍赚快钱的心理。微软的 Teams 虽然比 Slack 晚了几年才出来，但轻松超越，核心原因在于微软沉淀了多年的产品和技术能力，而这些都不是一朝一夕就能构建的。一位行业前辈说过，To B 产品一旦推出就要准备好支持客户二十年，否则就不要随意上线。做 To B，不要高估一年能做的事，也不要低估十年所能做的事。</p><p></p><p>IT 行业是一个必须持续创新的行业，不创新一定会死，但创新不是搞机会主义。To C 互联网常常讲试错，通过快速迭代来寻找机会点。To B 产品成功的关键是踏实做事，打造有匠心的产品。最近十年貌似并没有见到一个 To B 产品因为什么灵光一闪的 idea 的突然获得成功的，而那种 idea 也往往没有什么门槛，可以被对手轻松照搬和超越。那种觉得自己有一个前无古人后无来者的绝对创新想法的，往往是危险的浪费资源信号。</p><p></p><p>创新往往是朴素的，是相对的。由于企业基因和文化的差异，绩效评估方式的不同，一件在 A 公司做很容易的事，拿到 B 公司来可能就是登天的难度。如果一个人能在自身企业和组织的大框架下拓展一定的空间来完成一件在这个组织不容易做的事，我认为这就已经是巨大的创新了。</p><p></p><p>关于作者：</p><p></p><p>虞雷，微软 Azure Identity 首席总监、前阿里资深技术专家、钉钉产品技术总监、阿里邮箱总经理、前 Office 365 首席工程师。</p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651181536&amp;idx=1&amp;sn=bc061b95d8d644aff99cb035a35e4859&amp;chksm=bdb829b38acfa0a5f3244f1a1afef0eb51a780a8b2c29674455185124c17e4593c7886d853e7&amp;scene=21#wechat_redirect\">网易回应员工因 BUG 被 HR 威胁后轻生；阿里新 CEO：要让 85、90 后成为主力管理者；华为 Mate60 正面“刚赢”苹果？| Q 资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651181415&amp;idx=1&amp;sn=58bd52c1e2426a2b70a66c7240122ddd&amp;chksm=bdb829348acfa0227e56660bac1fc9849adb347cbb9a0c6cc2dab9158082d9c769ac4602aa01&amp;scene=21#wechat_redirect\">GitHub 变 Twitter？强“喂”新推荐算法引公愤，开发者从“编程乌托邦”被驱赶到了信息茧房</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651181313&amp;idx=1&amp;sn=e26b502c39d721c0c9628d6c995af0c7&amp;chksm=bdb829528acfa044b1e6e17aea3a0a50bb2790ce7085d7ac0ccd137d931204ff485b74e099ed&amp;scene=21#wechat_redirect\">小型开发者的生存之战：Unity 想要我们的全部收入！我们要破产了</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651181271&amp;idx=1&amp;sn=0c467f5ed0a2b7707ec8d8d9cd4c959d&amp;chksm=bdb828848acfa1927649405228a4711e092f85ca6aadd18af2d3c1fed14d39128af3560621f9&amp;scene=21#wechat_redirect\">40 多名直接下属、从不 1 对 1 沟通，老黄如此管理下的英伟达能在 AI 芯片领域称霸多久？</a>\"</p><p></p><p>内容推荐</p><p></p><p>《行知数字中国数字化转型案例集锦【第二期】》重磅发布，覆盖多个行业，对话一线专家，挖掘企业数字化的实践故事，揭秘数字化时代背景下如何重塑企业组织、技术与人才。扫描下方二维码，关注「InfoQ 数字化经纬」公众号，回复「行知数字中国」即可解锁全部内容。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/62/62cbd9dfdb49561de26eca79b82b8305.jpeg\" /></p><p></p>",
    "publish_time": "2023-09-19 15:31:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“高情商”的小冰框架，底层靠什么技术来支撑？",
    "url": "https://www.infoq.cn/article/KKqNuiCX5wTamdc2hubR",
    "summary": "<p></p><blockquote>小冰，作为最早诞生于微软公司内部的一款人工智能产品，从2014年亮相至今，已经成为了全球最受欢迎的虚拟形象之一。</blockquote><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b890c2888cc070d25c09b58c6b349e85.png\" /></p><p></p><p>小冰公司研发总监苏之阳博士</p><p></p><p>2013年，微软亚洲研究院成立了一个新的团队——小冰团队，该团队由当时担任微软亚洲研究院院长的沈向洋博士领导。这个团队的目标是研究下一代的人工智能交互方式。当时，人工智能技术刚开始崭露头角，语音助手、聊天机器人等人工智能产品层出不穷。但是，微软研究院的专家们认为，这些产品还没有完全达到自然、亲切的交互体验。</p><p>&nbsp;</p><p></p><h2>小冰发展史</h2><p></p><p>&nbsp;</p><p>为了让用户更好地与人工智能进行交互，小冰团队开始考虑将人工智能技术用于创建一个虚拟形象。这个形象可以与用户进行更自然、亲切的交互，同时也能更好地展示人工智能技术。</p><p>&nbsp;</p><p>在确定了小冰的初步设想后，小冰团队开始了紧锣密鼓的工作。他们首先开始设计小冰的性格、外貌等属性。小冰被设计成一个年轻的少女形象，有着活泼、开朗的性格。同时，小冰的外貌也十分可爱，备受欢迎。</p><p>&nbsp;</p><p>除了设计小冰的形象外，微软研究院的专家们还考虑了如何让小冰具备人工智能技术。当时，语音识别、自然语言处理等技术都已经有了一定的进展，但是要让一个虚拟形象与用户进行自然的交互还是有一定的难度。</p><p>&nbsp;</p><p>为了让小冰更好地与用户进行交互，微软研究院的专家们开始研究如何将深度学习等技术应用于小冰的交互中。同时，他们还考虑了如何让小冰能够进行自我学习、自我成长等操作。这样可以让小冰在不断的迭代中变得越来越聪明、越来越了解用户的习惯。</p><p>&nbsp;</p><p>经过一年多的研发，小冰于2014年5月29日正式发布第一版，并在中国、日本两地推出。推出后，小冰受到了很多用户的欢迎。为了让小冰更好地服务于用户，微软公司还不断地优化算法、提升模型精度，让小冰可以更好地与用户进行交互。</p><p>&nbsp;</p><p>在推出第一代&nbsp;小冰后，微软公司不断地迭代小冰的版本，推出了一系列新功能和新特性。例如2014年，小冰面向越来越多的第三方平台开放，能够跨平台地陪伴用户。2017&nbsp;年&nbsp;，小冰加快了在全球范围内的拓展速度，并在行业内率先将高级感官实际落地。2018 年，第六代小冰发布，微软首次披露小冰在全球已拥有6.6亿用户。这也是小冰历史上最大规模的一次全面升级，升级内容涉及到小冰情感计算框架的所有组成部分......</p><p>&nbsp;</p><p></p><h2>独立后的小冰，商业化路径更加清晰</h2><p></p><p>&nbsp;</p><p>2020年，对于小冰来说是一个全新的起点。</p><p>&nbsp;</p><p>2020年7月，微软宣布将人工智能业务小冰分拆为独立公司运营，由原微软（亚洲）互联网工程院常务副院长李笛为CEO。分拆后的新公司可继续使用并研发完整的小冰技术，微软保留对新公司的投资权益&nbsp;。这个决定意在加快小冰产品线的本土创新步伐，促进小冰商业生态环境的完善。&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>小冰离开微软独立后，人工智能技术正处于快速发展的阶段，各种人工智能技术应用和服务开始涌现。此时的小冰获得了更多的自主权和资源支持，可以更加专注于核心产品的研发。</p><p>&nbsp;</p><p>此前背靠微软这棵大树，小冰的商业化路径不算清晰，以至于脱离了微软“母体”的小冰在独立初期还是走了一小段弯路。</p><p>&nbsp;</p><p>但在ChatGPT等大语言模型爆火后，一直深耕 AI 数字人/对话聊天机器人技术的小冰有了大展拳脚的机会。</p><p>&nbsp;</p><p>小冰公司研发总监苏之阳博士在接受InfoQ采访中谈到了小冰的 To C 商业模式以及营收策略。苏之阳博士表示，小冰的一部分营收来自于与网络红人的合作业务。</p><p>&nbsp;</p><p>苏之阳博士进一步解释称，明星红人或内容创作者通常有陪伴粉丝、内容变现的需求。但人类精力有限，此时就可以借助人工智能完成上述工作。通过“克隆”自己，将之提供给粉丝，达到变现目的。平台与明星红人采用分成模式。“对那些想要克隆自己的人来说，克隆过程也很简单，最短只要提供三分钟数据，就能复刻自己的生物学特征。而在交互方面，本人会参与训练，确保克隆人更像自己或拥有自己的某项技能。”苏之阳说，为避免伦理问题，所有克隆人都要经过本人授权。同时，小冰严格限制了克隆人的使用场景，推出专属APP——X Eva，让用户能准确知道他所交互的对象是AI而非真人。这是一种C to C的新型模式，具有巨大的商业潜力。&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p></p><h2>“高情商”的小冰框架，底层靠什么技术来支撑？</h2><p></p><p>&nbsp;</p><p>一直以来，以小冰框架为底座的上层应用，包括克隆人”在内都以“高情商”著称。那么，所谓的高情商意味着什么？意味着它在聊天过程中很有趣？其实不止如此。情商高，在交互中主要体现为控制全程对话的能力。</p><p>&nbsp;</p><p>传统的智能语音助手不会去过度关注对话的全程，而更加关注对话中的每一个细节。它会把每一句话都优化得很好，甚至把对话形式分为：面向任务型的对话、面向知识型对话、无意义的闲聊。</p><p>&nbsp;</p><p>从大数据的分析来看，其实人与人的对话 / 人与人工智能的对话，就如同河流一般奔涌向前，任何一句看似无意义的闲聊，都可能在十几轮甚至几十轮的迭代之后产生一个非常重要的结果。小冰可以不停地去迭代，去改变对话的走向，去改变对话的长度。它关注的是整个对话的全局，而不是一城一地的得失，正如古人所言，“不谋全局，则不足以谋一隅”，这就是所谓的控制整个对话全程的能力。</p><p>&nbsp;</p><p>要控制全程的对话，所要处理的数据体量是非常庞大的。那么，小冰底层用到基础设施是哪些，效果又如何？</p><p>&nbsp;</p><p></p><h2>使用阿里云MongoDB托管服务，以支持增长的业务需求</h2><p></p><p>&nbsp;</p><p>苏之阳称，小冰框架最初使用的是MySQL数据库，后来研发数字人产品时，考虑到未来数字人数量会在百亿量级，不得不重新思考底层存储架构设计。</p><p>&nbsp;</p><p>在数据库选型过程中，小冰技术团队从产品需求出发，比较了多个候选存储方案的优缺点，最终选择了MongoDB数据库作为底层存储基建。存储选型主要考虑的因素有如下方面：第一，业务查询的复杂度，大多数数字人业务查询都是键值查询，不涉及复杂多表联查；第二，由于要支撑海量数字人信息，对于存储容量和性能要求很高，同时需要支持峰值流量来临时的动态弹性扩容；第三，研发效率要能跟上产品迭代的速度。</p><p>&nbsp;</p><p>小冰技术团队开始使用的是社区版本的MongoDB数据库，同时配有运维工程师。但在线上运维一段时间后遇到了一些挑战，苏之阳表示，“随着业务发展，当用户数和虚拟人数量快速增长时，社区版本的MongoDB数据库不能完全满足业务需求，比如线上流量激增时，数据库会出现一定程度的抖动，导致服务不稳定，这是当时遇到的棘手问题之一。MongoDB分片集群作为一个分布式系统，包含数十甚至数百个结点，运维成本不可忽视。考虑到MongoDB的性能优化，让专业的团队做专业的事是最优选择。”经过综合评估，小冰技术团队最终使用阿里云的MongoDB托管服务解决了运维的痛点。</p><p>&nbsp;</p><p>苏之阳还表示，“利用MongoDB分片集群的可扩展性，可以很好将业务层和存储层解耦，让研发同学更专注于应用开发而不必过分关注底层存储性能问题，节省了开发中间件处理分库分表的成本。此外，在用户支付场景，使用多文档事务的特性，同时更改用户的余额和订单，可以大幅提升研发效率，避免使用成本较高的异步消息补偿机制实现。小冰公司也高度重视用户的数据隐私与安全，阿里云MongoDB集成了云上数据备份和恢复的功能，让数据安全无忧。”</p><p>&nbsp;</p>",
    "publish_time": "2023-09-19 15:40:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "字节跳动 DevMind：深入浅出万人规模效能度量平台的构建与演进",
    "url": "https://www.infoq.cn/article/A9ZYAUcVY8p4aX0vw8sP",
    "summary": "<p></p><p>在当今企业降本增效、去肥增瘦的大环境下，大数据平台研发效能度量成为了重要的提升企业研发效率和产品质量的关键。本文介绍字节研发效能度量平台从 0 到 1 的完整演进过程，通过深入浅出工程实现过程中遇到的各类矛盾问题，帮助读者更好地应用技术解决方案，加深该领域的深入思考。</p><p></p><p></p><h2>什么是 DevMind？</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/43/4343045a2a7c2e399b77b0bb4f167760.png\" /></p><p></p><p>DevMind 是字节跳动的研发数字化产品与解决方案，目标是让研发各个层面的现状 &amp; 问题可视、可评价、可诊断，辅助决策和改进，实现“数据驱动研发效能提升”。其中，包含 3 个核心能力：</p><p></p><p>度量，量化现状：整 合所有研发大数据，并由领域专家将经验转化为研发指标、分析模型。洞察，诊断问题：对研发指标进行自动数据洞察，形成客观、准确的分析结论。决策，驱动问题解决：辅助业务负责人、TL、一线研发了解客观情况、问题、解法，在各个工作场景中科学决策（场景主要包括：研发改进、团队管理、研发过程风险识别等等）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0a/0ae9591326293b12ecf3379182b31594.png\" /></p><p></p><p>详细了解 DevMind 体系：<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1MjAzNjE4NA==&amp;mid=2247484742&amp;idx=1&amp;sn=ffe462867aef720b5addf7f505fec661&amp;scene=21#wechat_redirect\">构建效能提升的“导航仪”和“发动机”，实现从数据到价值的跃迁</a>\"</p><p></p><p>DevMind 的技术架构，如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c3/c3f6696049ee09702e810330a48be062.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ba/ba80cc1242fd1181c8353182fed08276.png\" /></p><p></p><p>本文将从业务工程、数据工程中的各自 3 大“矛盾”展开，详解一下，如何在面向万人规模的效能度量平台中，在“既要、又要、还要”的不可能需求下，进行工程能力的设计与实施，从而达成有限时间、有限人力条件下的业务目标。</p><p></p><p></p><h2>业务工程详解</h2><p></p><p></p><p>在业务工程领域有三大矛盾：</p><p></p><p>矛盾 1：数据领域、角色多样化与用户协作效率的矛盾。矛盾 2：平台领域专业性与用户非专业性的矛盾。矛盾 3：场景复杂性（数据规模，数据结构、算法复杂度，变更频率）与系统稳定性、性能的矛盾。</p><p></p><p>因此我们需要有 3 套方案逐一击破。</p><p></p><p></p><h3>2.1 激发生产力</h3><p></p><p></p><p>矛盾 1——数据领域、角色多样化与用户协作效率的矛盾。</p><p></p><p>研发效能链路漫长，涉及到多个领域。而每一个领域都有自身独特的领域知识门槛。为了更好的将这些门槛在数据可视化分析方式磨平，需要构建完备的数据体系，并且 Involve 这个数据链路中关键角色。在执行过程中其难点主要体现在 3 个方面：</p><p></p><p>1. 数据领域多：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c5/c570323688da0c698a1281d92eda8d99.png\" /></p><p></p><p>2. 用户角色多：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/50/50023e1f625d88e033141d071223a3b7.png\" /></p><p></p><p>3. 用户角色能力不同，协作存在障碍：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/51/5127bc91e8e3a28bc6e1998578c1f7d3.jpeg\" /></p><p></p><p>造成这个矛盾的直接原因是数据分析的专业化门槛，导致数据工程师和分析师供给不足，而未受专业培训的业务同学却有心无力。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e1/e1a6b76b4435932e968b1542250db45b.png\" /></p><p></p><p>其本质原因是数据产品专业性的僭妄，没有真正用同理心去感受普通用户的需求场景。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b0/b08374510305d9e02e1ade4acb58ae47.png\" /></p><p></p><p></p><h4>2.1.1 解决目标</h4><p></p><p></p><p>降低用户使用门槛，使得产品、运营、QA 等全角色都可以顺利使用。</p><p></p><p></p><h4>2.1.2 解决方案</h4><p></p><p></p><p>2.1.2.1 重塑流程</p><p></p><p>DevMind 重新设计了数据可视化分析的交互方式，将传统的“以选择数据源为起点”改为“以指标为核心”。让富有经验的数据分析师和领域专家专注于核心指标及数据分析模型的设计。让更多的业务参与者围绕指标完成数据探查，或是进行衍生指标的再生产。</p><p></p><p>这一交互形式的改变不仅释放了领域专家的生产力，更重要的是，以指标为核心的交互大幅降低了非专业用户的理解和使用门槛。此外，生产形态的改变，令 DevMind 指标中台不再是旁路的元信息管理工具，而可以深度地参与到整个生产链路之中。因此，这一改变有效避免了在指标中台中初次沉淀的数据资产因脱离后续业务活动而逐渐腐败过期的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ed/ed0c6f13fda17a2986de5a6fcf6c8e07.png\" /></p><p></p><p>2.1.2.2 抽象生产资料——元指标模型</p><p></p><p>核心思路：以“元指标 + 度量对象”二元形式对基础数据层进行高度封装，使其拥有完备业务含义且可独立执行。</p><p></p><p>元指标模型：从物质世界到数学的转换。将数据可视化配置中分解为设计业务查询逻辑和配置可视化表达。</p><p></p><p>复合元指标模型：带有丰富业务含义的代数矩阵。辅以算术运算符和逻辑运算符进而获得更高层次的业务表达。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/36/3651be4c932d8ccfe198b6149be84bd7.png\" /></p><p></p><p></p><h3>2.2 提升生产力</h3><p></p><p></p><p>矛盾 2——平台领域专业性与用户非专业性的矛盾。</p><p></p><p>即使作为官方效能度量平台，DevMind 也无法通过有限的人力应对近乎无限的业务需求。要想在将效能提升理念在全公司研发团队中推广，就需要让每一位业务线相关同学参与进来。由最理解业务的同学进行分析，更容易找到贴合业务实际的洞见。这里引入公民数据科学家的概念。公民数据科学家 (Citizen Data Scientist)，是指未受过高级数学和统计学正式培训的知识工作者，他们能够通过适配的应用产品从数据中提取高价值见解。</p><p></p><p>DevMind 所致力于的工作，就是不断降低数据分析的门槛，辅助公民数据科学家们更好的产出分析报告。</p><p></p><p></p><h4>2.2.1 解决目标</h4><p></p><p></p><p>丰富自动洞察算法工具库，优化用户分析效率，知其然知其所以然。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b5/b579715d3d1cd32a9b02553fab9dc692.png\" /></p><p></p><p></p><h4>2.2.2 解决方案</h4><p></p><p></p><p>2.2.2.1 Why——波动分析（比率指标）</p><p></p><p>比率指标难点：</p><p></p><p>数学层面：比率指标具有不可加性，无法直接进行维度分解，因此无法套用贡献度等经典分析方法。</p><p></p><p>业务层面: 比率指标的分子分母的维度并不严格对应，需要考虑至少三种场景，这也增加了分析复杂性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/54/54d63377a2ffefd09787071d209bf059.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f5/f51008ed0a155b6078f424586ed944fb.png\" /></p><p></p><p>2.2.2.2 How——潜力分析</p><p></p><p>需求背景：</p><p></p><p>基于波动分析可以有效定位造成异常波动的根因维度项，但有时这并非就是业务业务可以直接采纳的优化方向。因此需要有更好的算法，告诉业务方哪个方向有最大的优化潜力。</p><p></p><p>核心思路：</p><p></p><p>均值回归：假设维度分项数值都将围绕维度均值波动，波动趋近于均值的概率要高于背离均值的概率。</p><p></p><p>高占比项优先：指定维度的各维度项变化同一固定比率，则高占比维度项对大盘的影响更大。</p><p></p><p>维度稀疏度：基于稀疏度处理防止拥有较少维度分项的维度对大盘干扰，从而解决维度项数目对大盘。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/fb/fb8e4def293988251a38b94ba1cbc40d.png\" /></p><p></p><p></p><h3>2.3 量化生产力</h3><p></p><p></p><p>矛盾 3——场景复杂性（数据规模，数据结构、算法复杂度，变更频率）与系统稳定性、性能的矛盾。</p><p></p><p>DevMind 作为数据类产品，算力情况是研发侧最为重要的要素。因此希望构建一套综合性算力评估体系，能够对 DevMind 现状进行准确的定量刻画。该算力评估体系后续将应用于运维治理、新需求准入评审、技术改造定量考核等多个场景。</p><p></p><p></p><h4>2.3.1 解决目标</h4><p></p><p></p><p>打破黑盒 ，通过构建综合全面的算力评估体系，实现对平台整体现状进行准确的定量刻画。当前我们对算力情况完全黑盒，甚至对“算力”没有一个准确定义。</p><p></p><p>一个具体业务场景：每半年的绩效季。绩效季是用户访问 DevMind 高峰期，基于历史数据，我们可以知道用户访问量和渗透率。但是这个访问量背后会新增多少“算力”需求，当前数据库能够承载多少“算力”需求？这些问题都是未知的。</p><p></p><p>我们一直以来在做的，只能是尽可能做准备，然后祈祷不要出问题。You can’t manage what you don’t measure. 我们真正要做的就是对现状准确刻画，定量的描述“算力”的需求和库存。</p><p></p><p></p><h4>2.3.2 解决方案</h4><p></p><p></p><p>2.3.2.1 明确应用场景</p><p></p><p>稳定性运维：基于全场景算力消耗监控，指导日常运维工作。并为运营活动等场景的资源申请提供数据支撑。新需求准入评审：计算方法：{算力成本} = {功能点单次访问算力成本} × {功能点用户渗透率}业务约束：{平台总算力} ≥ {存量算力开销} + {新需求算力成本}技改收益定量评估：构建平台算力基准水位，为此后技术性能优化后的定量收益提供公允的依据。</p><p></p><p>2.3.2.2 建立算力评估模型</p><p></p><p>对“算力”度量来源于两个方面。第一个是生产者也就是数据库，算力对于数据库意思是在单位时间内能够完成的最大查询量。第二个是消费者也就是 DevMind 平台。消费者消耗的算力是每个功能点产生的数据查询量 乘以 每个功能点的单位时间 PV。当数据库生产的算力大于等于 DevMind 平台消费的算力时，我们就可以认为整体营收是合理的。</p><p></p><p>而在上述讨论中“算力”是一个抽象的概念，因此我们首先需要将“算力”实体化。</p><p></p><p>2.3.2.2.1 算力评估分类</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/2e/2e7cd1114dae0b111e9315d8fd3ce78e.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/90/902801e29f6fc1326561045eebc43819.png\" /></p><p></p><p>2.3.2.2.2 简化算力换算模型</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c1/c14c9392840881bcb6e02f47f6a18ebf.png\" /></p><p></p><p>算力定义：对于数据产品，算力是其完成业务目标所需要的物理成本开销。算力换算：通过「基准查询」作为生产端和消费端链接的媒介和基本单位，将“算力”这一抽象概念实体化。基准查询：选用产品形态中最通用且易于泛化的 Case，既可以在各业务场景中等价换算，也可由计算单元压测获得。消费端算力建模：模型分析对象是用户在指定时间窗口内对产品功能使用过程中产生的查询请求。用户侧，需要考虑不同类型用户的规模，以及访问频率和访问深度。产品侧，则考虑的是产品各个功能点对于算力消耗情况，以及产品不同用户空间的深度与规模。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d1/d150290e9620f23a5234a50f93fec55a.png\" /></p><p></p><p>2.3.2.2.3 生产端算力建模</p><p></p><p>定义：算力总量由服务架构计算层自身的物理特性和工程实现共同决定。算力总量的策略标准借鉴数据库 TPM(transactions per minute) 的概念，采用：「1 分钟最大可执行基准查询数」。物理特性：CPU、I/O、Memory、磁盘容量、机房 etc。工程特性：数据库类型及版本，索引选择、优化器实现、算法调优 etc。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/da/da65a956f168e9bd073b6617b7ec785a.png\" /></p><p></p><p>2.3.2.2.4 算力工程优化思路</p><p></p><p>基于 MECE 原则的算力优化分析：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8c/8c0a2f5312e327df29ea4d09cb92f873.png\" /></p><p></p><p>算力增加：查询引擎替换：OLAP &nbsp; &nbsp;HSAP查询模式升级：自建 SQL 优化器算力利用率提升：应用层利用率：精细化缓存、查询请求消重计算层利用率：基于时分复用前后台任务切换算力分配优化：数据生产阶段：预处理 &amp;&amp; 预计算数据消费阶段：集群间动态调度数据链路：离在线一体存算体系</p><p></p><p>2.3.2.2.5 动态算力调整：背压模式</p><p></p><p>在数据调用系统中存在三方：生产者 (Producer) 产生请求，通过管道 (Pipeline) 传输给消费者 (Consumer)。面对生产者生产速率超过消费者承载力时，可以衍生出三种策略：</p><p></p><p>缓存 Buffer：将多余的流量临时存储在管道内，再由消费者逐个执行。丢弃 Discard：当上游流量已经超过下游承载力，也超过缓存队列长度时，需要将超出部分的流量丢弃。控制 Control：降低消费者生产速度，直接从源头减少流量。</p><p></p><p>上述三种策略在工程实践中实际并不是孤立的，DevMind 通过自适应背压系统，实现了整体数据环路的动态控制。</p><p></p><p>背压（BackPressure）：通过负载探针检测消费者负载压力。当负载超过阈值后，将压力反向通知上游，使上游的生产者及管道可基于背压感知执行相应策略，从而实现自适应调节流量的目标。动态限流器：通过背压感知动态调节限流总量。触发限流时，基于在线配置按优先级放行请求。分布式队列：通过同步调用异步化，即可实现削峰，也可对队列窗口内请求执行精细化控制。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3d/3d8535db1b950ed8b95e23f923d8c07a.png\" /></p><p></p><p></p><h2>数据工程详解</h2><p></p><p></p><p>在数据工程领域也有三大矛盾：</p><p></p><p>矛盾 1：不同业务数据的差异化和标准化度量之间的矛盾（数据管理）。矛盾 2：查询规模、时效性和引擎物理性能极限的矛盾（数据应用）。矛盾 3：数据源的多样性、零散性和平台统一管理的矛盾（数据存储）。</p><p></p><p>因此我们仍用 3 套方案逐一击破。</p><p></p><p></p><h3>3.1 数据管理</h3><p></p><p></p><p>矛盾 1——不同业务数据的差异化和标准化度量之间的矛盾（数据管理）。</p><p></p><p>小到一个团队，大到一个部门，不同研发工具亦或是同一研发工具的不同使用方式都造成了数据的千差万别。</p><p></p><p></p><h4>3.1.1 解决目标</h4><p></p><p></p><p>打破传统 BI 平台空间隔离的固有属性，提供灵活的数据管理方式，最大化挖掘和复用数据价值。</p><p></p><p>我们可以带入如下的实际应用场景思考一下这些问题的解。对于场景一，首先是先找数据源，和需求平台沟通要数据，然后需要找 BI 平台去做落地。对于场景二，需要重复问题一的步骤，然后又会碰到新的问题，各业务数据标准不同，各自“小管”使用无问题，但是作为“大管”如何全局看整体？对于场景三，度量场景和范围不断被放大，又该如何解决这些类似的问题？</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/49/49995512d79c147e0b98326f54f40e5e.png\" /></p><p></p><p></p><h4>3.1.2 关键挑战</h4><p></p><p></p><p>跨业务复用能力，简化“沟通成本”，这一块往往代价是最大的。数据准确性刻画，“准”一般和“通用”是个悖论，需要权衡为了“准”所需要付出的成本和代价。层级关系表达，这一点尤其重要，而且传统的 BI 工具平台不会带有这个功能概念，常规做法是下沉到数据本身上，靠数据关系去实现。</p><p></p><p>带着场景输入，我们从工程角度看下以前和现在的区别。</p><p></p><p>过去：</p><p></p><p>数据接入：寻找需求数据源，依赖平台数据开放能力，复杂的 ETL 工作指标定义：从 0-1 讨论指标口径，定义计算逻辑绘制图表：传统可视化绘图工作，经过自定义格式，排版等琐碎重复工作，最后输出仪表盘，汇总报告，基于图表拼接出风格迥异的报告</p><p></p><p>现在：</p><p></p><p>总结为一句话：“站在巨人的肩膀上”。我们只需要挑选指标故事生成属于自己的洞察报告即可，其他什么都不用做。甚至，指标故事都不用挑选，随意切换度量对象，就可动态实时查看属于该对象的洞察报告。</p><p></p><p>数据接入：不需要去理解“OpenAPI”，“数仓”，不需要考虑工程化能力，做一些基础的映射配置，仅此而已指标定义：引入“专家”角色，“专家”可以是数据分析专家角色，可以是来自别人的经验沉淀绘制报告：指标故事、洞察报告的最佳形态平台已经自动化固化好了</p><p></p><p>达到这样的效果，如何做？归根结底就是建立合适的“映射关系”。</p><p></p><p>答案：数据管理地图 + 度量对象映射树</p><p></p><p></p><h4>3.1.3 解决方案</h4><p></p><p></p><p>3.1.3.1 建立数据管理地图</p><p></p><p>生产者角度建立数据和业务的第一层 Mapping。</p><p></p><p>实现方式：同一个核心数据模型，共享底表，两个要求，解决业务共性和差异性。通用字段：并不意味着同样叫“状态”的字段，所有业务方起的名字都一样，半自定义，字段业务含义通用。自定义字段：全自定义，字段业务含义完全自定义。效果：从数据生产上，尽可能把不同业务在同一场景下的数据集中到一个数据模型里。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f0/f08269b00be57a0fba0d66f6b2661038.png\" /></p><p></p><p>3.1.3.2 建立度量对象映射树</p><p></p><p>使用者角度建立数据和业务的第二层 Mapping。</p><p></p><p>通过一段自定义“SQL”片段，在同一个数据集上去解决单业务、多业务、父子业务之间的映射关系。通俗易懂点，就是需要个过滤规则去筛选数据。这里面涉及 SQL、数据集、组织三者之间的相互依赖关系：</p><p></p><p>单数据集与度量对象之间的关系业务特化 SQL 定义与度量对象之间的关系度量对象间的关系</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/dc/dcccb649253408d59edc694fdf43c572.png\" /></p><p></p><p></p><h3>3.2 数据应用</h3><p></p><p></p><p>矛盾 2——查询规模、时效性和引擎物理性能极限的矛盾（数据应用）。</p><p></p><p>大家见过的最长最复杂 SQL 长度有多夸张，比如在我们的项目中，单条 SQL 长度也就是字符数可以达到 1000W+，如果在这个复杂度的基础上再加上高 QPS 的要求呢。这样复杂的查询场景是如何形成的，从产品功能上归于组成洞察报告的三大要素：指标故事 + 洞察报告 + 度量对象，我们从三个维度看下复杂场景的来源：</p><p></p><p>定义：举个例子，复杂的聚合函数 + 复杂的逻辑表达式机制：对于接近顶层节点的“上卷”机制带来的豪华 SQL产品形态：单个格子，基线功能（全局基线、历史基线）；整体，支持层级树展开的产品形态</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4d/4d5a41f5c3d7b4ec6c75cfeb36a82a43.png\" /></p><p></p><p></p><h4>3.2.1 解决目标——“超跑理论”</h4><p></p><p></p><p>对于当前顶层业务节点的复杂查询，毫不夸张的说，单独拿一个指标出来都是个离线任务级别的计算。做个比喻：咱现有查询引擎现在是辆大巴车，载客多，但是不快，能到目的地；接下来比如我们用上了豪华查询引擎，就是超跑，速度贼快，但是载客少，一辆超跑拉一百个人基本翻车了。有一种方式是我们可以多搞几辆超跑同时运输，那我们还需要解决这样的问题，得告诉乘客们，你们这一波一万个人想都三秒同时到站，是不太现实的，当前技术下还没这种交通工具。</p><p></p><p>理想：我们希望找到一个完美的引擎，能够满足当前平台查询所有特点和要求。现实：很遗憾，没有绝对完美的引擎，不同的引擎一定在某一方面是做增强的。结果：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/17/17e52d6d40e73df80bf26b5a98501db1.png\" /></p><p></p><p></p><h4>3.2.2 解决方案</h4><p></p><p></p><p>曾经我们也尝试过，希望找到一颗银弹去解决燃眉之急，现实是仅仅靠一辆超跑“发动机”还不能够帮助我们将成千上万的用户又快又稳送达目的地。通过深度调研了业界流行的计算引擎以及技术选型的逻辑，基本概括为三大思路：</p><p></p><p>MPP 技术，大规模并行处理，分布式、并行化技术成熟，预期达到“亿级秒开”内存计算，数据量大无法加载预处理，Qube 化，存储换时间</p><p></p><p>引擎对比分析：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/78/785e4d310e008c019df089d8e759cd63.png\" /></p><p></p><p>引擎选择逻辑：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3e/3e0e4fc09465d11f76e4fa94f198d162.png\" /></p><p></p><p>历史原因：我们以关系型 MySQL 作为主要引擎，辅助 ClickHouse 解决大数据单表场景。引入新引擎面临的代价：语法适配和迁移改造成本，平台场景定位是 AD-Hoc 查询，自定义指标维度有几十万，由于 MySQL 太过强大和自由，导致切换任何另外一种查询引擎的成本非常昂贵，甚至都不可行。</p><p></p><p>3.2.2.1 换发动机（硬件能力）</p><p></p><p>寻求符合查询场景下的万能查询引擎</p><p></p><p>没有万能发动机，我们就尝试组合出一个万能发动机。这个时候不得不提字节的明星 HSAP 架构下的实时计算引擎 Krypton，OLAP 引擎比 MySQL 快百倍以上很正常，但是要应付我们如此复杂 SQL 的场景几乎没有，仅仅是一个语法兼容性就已经打败了很多组件。而恰好，MySQL 的语法兼容性正是 Krypton 的亮点之一：</p><p></p><p>云原生：存算分离架构，弹性扩展易用性：语法兼容性高性能：行列存储、向量化引擎、MPP、实时物化视图、异步执行</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3b/3b42812cbc5e01406ba341c53ac61957.png\" /></p><p></p><p>基于这些考虑，我们最终通过 MySQL + ClickHouse + Krypton 拼出了一个像样的“万能引擎”。</p><p></p><p>3.2.2.2 换策略（软件能力）</p><p></p><p>寻求有限硬件条件下的最佳策略组合</p><p></p><p>可以这么说，但凡和数据查询性能提升沾点边的优化策略方式，我们平台都有。在引擎上：不同的数据模型（明细表、笛卡尔表、Cube 表的合理性）、针对性的索引机制（比如查询特点优化联合索引顺序）、分库分表操作；在策略上：将大请求拆分、走异步限流削峰、数据预处理思路；在缓存上：分场景分功能模块下的长短缓存组合；在功能上：通过预刷（模拟人触发）、封板（提前算好存结果）等操作极限榨干引擎算力。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9e/9e472b6a362b1183055db7b6f626e45d.png\" /></p><p></p><p>3.2.2.3 加约束给预期（管理能力）</p><p></p><p>约束不合理行为（滥用、滥配置）+ 纠正错误预期（秒变秒出）</p><p></p><p>有时候，优化再多都赶不上用户“造”的能力，所以必须加强准入准出能力。</p><p></p><p>报告约束</p><p></p><p>举个写代码的例子，很多人一个方法写上千行，很典型的一个 bad case。类比到报告中，有些业务同学恨不得把所有的指标都塞到一个面板中，先不说引发查询性能问题，比如你是老板或者用户，看到满屏的包含各种主题的五花八门的数字，内心估计是崩溃的。还有一些特殊场景，比如复合指标故事，一个复合指标是几十个元指标的组合体，相当于指数级膨胀。</p><p></p><p>数据嵌套</p><p></p><p>部分用户养成了缺字段就是 Join 的习惯，引发底层数据模型 Join 的层级增多。或者是像是一些日志型数据集，本身数据量极其庞大，一旦查询周期过长，就会引起数据库扫行过多。</p><p></p><p>SQL 规范</p><p></p><p>在长度控制可以做控制，点击保存按钮那一刻给出一些建议提示，比如可以走其他更合适的机制解决。在语法上进行优化和检测，提示错误语法，甚至是给出缩减合并的建议，并能做一些基础的 SQL 漏洞注入检测等规则注入。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b3/b354170f2b7848768aec999bc57e4ae1.png\" /></p><p></p><p>在预期上，通过用户交流和访谈后发现个有意思的问题，用户并不是不能接受慢，而是需要给个心理暗示或者明示。比如进度条，转圈圈，或者更加直接粗暴“预计查询耗时”。因为不给任何预期，用户视角默认就觉的所有信息都是“秒级返回”，结果等了 10min，那肯定是要崩溃的。</p><p></p><p></p><h3>3.3 数据存储</h3><p></p><p></p><p>矛盾 3——数据源的多样性、零散性和平台统一管理的矛盾（数据存储）。</p><p></p><p>我们不产数据，我们只是数据的搬运工。这个问题估计做数据建设相关的工作同学都有共鸣，工作本身不难但是做好标准化管理很难。</p><p></p><p></p><h4>3.3.1 解决目标</h4><p></p><p></p><p>业务现状：业务数据开放能力参差不齐，每个业务或多或少做过一些数据建设。</p><p></p><p>技术现状：成熟的数仓建模理论且一般公司配备完善的数据中台能力。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/80/80e8de7beeef86bd0c00a43d6b18ce21.png\" /></p><p></p><p>对数据生产方来说，经营意识比较差，加上频繁被打扰，用户之间信息不共享，边际效益非常小；对数据消费者来说，沟通对接成本高昂。所以，不得不面临数据管理方式差、缺乏共建共享能力、对接成本高昂这些共性难题。</p><p></p><p></p><h4>3.3.2 解决方案</h4><p></p><p></p><p>3.3.2.1 建立数仓门户</p><p></p><p>从找人过渡到找系统</p><p></p><p>数据门户定义：利用前端自助建站平台，把杂乱的功能网址、文档秩序化的管理起来呈现给用户一个统一的入口。</p><p></p><p>生产侧：规范化标准建设，通过纯傻瓜式教学（数仓规范、命名、权限、质量、用户文档），然后将生产出来的资产表校验上架，汇总到一个大的数仓主题中。消费侧：有门户、有用户群、有清晰的用户文档、有分发答疑机制，用户消费数据无需再到处找不同的平台方，而是直接统一的入口。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/79/795bb6c23ad2522cc13fdbb0f9b108f8.png\" /></p><p></p><p>这套玩转起来后，会呈现一个滚雪球效应，消费者用的舒服了一是会驱动工具平台方按照这个标准实践，二是自己也可以变为生产者，把聚合后的表分享出来给其他消费者。</p><p></p><p>3.3.2.2 建设数据链路</p><p></p><p>标准化数据表到可用的数据模型。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/00/00d15732be07e151a880cfda8ead3466.png\" /></p><p></p><p>整体概括为数据“ETL”，经过批流集成、批流开发变成 DevMind 平台真正的数据资产表。数据集成核心就是不同存储类型之间的转换。数据开发核心就是写聚合逻辑，把数据加工成目标宽表。</p><p></p><p>工具能力本身，成熟的数据中台都会覆盖，在我们自己的项目中，更多的是聚焦在应用过程的管理和机制上，把数据中台能力用好才是难点。只有做好如下这些点，才能称之为“数据变为可用模型”的转换。</p><p></p><p>版本号：解决数据一致性以及快照问题调度时间：理想情况是满足 T+1，我们有成百上千个数据集，需要考虑队列优先级、任务优先级因素蓝绿部署：解决任务同步的“开天窗”问题，比如清除式写入任务带来的数据不准窗口期资源协调：资源不是无限的，同样需要考虑队列优先级、任务优先级因素任务依赖：合理的拓扑图和链路设置</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/75/75bba6dfe972f068def28389ac466a34.png\" /></p><p></p><p></p><h2>中长期演进方向</h2><p></p><p></p><p></p><h3>4.1 更开放、更智能</h3><p></p><p></p><p>开放性：类似 HiveUDF 生态流程化：DBT，解决 ETL 中的 T 场景智能化：当前火热的 AIGC 潮流，在具体的场景上能否做些实际落地可观测：数据全生命周期可视化管理，数据运维和排查上作用非常大</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ac/acf3304b50015dde9908a330a5287f99.png\" /></p><p></p><p></p><h3>4.2 隐私安全有保障</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/aa/aa8de5385b678c4d13cb219e14392b4e.png\" /></p><p></p><p>扩展阅读</p><p></p><p>DevMind 体系：<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1MjAzNjE4NA==&amp;mid=2247484742&amp;idx=1&amp;sn=ffe462867aef720b5addf7f505fec661&amp;scene=21#wechat_redirect\">构建效能提升的“导航仪”和“发动机”，实现从数据到价值的跃迁</a>\"DevMind 技术：<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651151610&amp;idx=1&amp;sn=0d6e77e851d9e5b10a68e238608a7cd0&amp;scene=21#wechat_redirect\">覆盖数万研发人员，字节跳动首次公开效能度量核心技术！</a>\"</p><p></p>",
    "publish_time": "2023-09-19 16:10:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023数字化转型发展大会暨首届数字原生大会在京召开",
    "url": "https://www.infoq.cn/article/1750334f257fd4637ec683b1f",
    "summary": "<p></p><h4>摘要</h4><p></p><p></p><blockquote>2023年9月13日-14日，由中国信息通信研究院（简称“中国信通院”）和中国通信标准化协会联合主办的“2023数字化转型发展大会暨首届数字原生大会”在北京召开。本届论坛以“数跨新阶 原生新纪”为主题，成立数字原生推进方阵（DNA），发布何宝宏博士新书《数字原生》，公布第二届“鼎新杯”数字化转型应用大赛案例结果，公布2023年上半年政企数字化转型最新评估结果。</blockquote><p></p><p></p><p>工业和信息化部信息通信发展司政策标准处处长陆洋、中国通信标准化协会理事长闻库、中国通信企业协会副会长兼秘书长赵中新、国务院国资委信息中心原主任王绪君、中国信通院院长余晓晖出席会议并致辞。开幕式由中国信通院云计算与大数据研究所所长何宝宏主持。</p><p><img src=\"https://static001.geekbang.org/infoq/e2/e25db07c380cf32087095e232dc4333f.jpeg\" /></p><p>工业和信息化部信息通信发展司政策标准处处长陆洋致辞</p><p>陆洋表示，数字经济已成为全球经济增长的新动能，当前，我国数字化转型发展顶层政策环境持续优化、基础设施水平加快提升、产业生态体系不断完善。未来建议一是提升数字基建质量，二是加快核心技术突破，三是强化标准规范建设。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/759788b175a2f9a24faac8476d1f97be.jpeg\" /></p><p>中国通信标准化协会理事长闻库致辞</p><p>闻库表示，我国数字经济规模正不断扩大，在国民经济中发挥作用。数字经济的顶层政策环境不断完善，技术创新活力不断并发，产业模式不断创新，转型标准体系不断完善。他建议，一是加速推进数字化政策的精准落地，二是大力促进多领域企业的交流与合作， 三是充分发挥标准的牵引作用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9eee8f29500f381633d9849c316574b8.jpeg\" /></p><p>中国通信企业协会副会长兼秘书长赵中新致辞</p><p>赵中新表示，数字化转型已经成为社会的主旋律，打造一批存在推广应用价值的数字化转型应用案例具有重要意义。应继续加大“鼎新杯”案例征集活动宣传推广，推动转型走深走实。展望未来，他认为一是持续推进信息通信行业赋能社会数字化转型发展，二是持续加强信息基础设施建设，三是强化赋能其他行业产供销协同。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/eb/ebd39d9fa6473f2c609bebcc8a095a6a.jpeg\" /></p><p>国务院国资委信息中心原主任王绪君致辞</p><p>王绪君表示，近年来国有企业持续开展数字化转型工作，数字化转型工作体制基本形成，新技术深化创新攻关，数字技术服务能力有序提升。他建议，一是持续优化政策环境，二是切实推动标准建设，三是持续打造行业数字化转型示范样本。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/ca/cae55db3a198399b7d48c7e555627d44.jpeg\" /></p><p>中国信息通信研究院院长余晓晖致辞</p><p>余晓晖表示，数字化转型与过去几十年已然发生的全球信息化浪潮一脉相承，都是信息通信技术驱动的范式变革，但又呈现全面互联、数据驱动、软件定义、平台支撑、全局智能等新的特征，而数字原生成为数字化的新阶段和重要方向。他提到，预计未来几年，数字化转型模式与价值将更为清晰，数字化转型的生产方式、业务形态、产业组织方式、商业模式、创新范式、技术架构都将发生转变。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a8bba377faaf98c35b11d9bd6556387.jpeg\" /></p><p>数字原生推进方阵（DNA）成立仪式</p><p>数字经济时代，一批具备“数字原生”基因的企业和组织应运而生，它们将数字思维、数字技术融入生产经营的各个环节，通过“数字原生”理念，重塑企业文化、组织架构、技术能力和经营模式，实现核心价值提升与商业模式创新。为促进数字技术与实体经济深度融合，让更多传统企业理解“数字原生”理念，中国信通院联合产业各方，正式发起成立“数字原生推进方阵（DNA）”，搭建数字原生企业对话平台，推动数字原生理念发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/73ea839c99c9fe45885754d47850989f.jpeg\" /></p><p>中国信息化百人会执委安筱鹏主旨演讲</p><p>中国信息化百人会执委安筱鹏发表题为《什么是数字原生?》的主旨演讲，剖析了数字原生出现的机理机制，并指出，数字原生企业的四个特征，即数字原生=客户运营商+数据运营商+进化型组织+长在云端。安筱鹏指出数字原生的关键在于构建基于数字技术底座的业务架构和组织架构，打造扁平化协同的进化型组织。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18bc76f7a15718d1c32565d643da242e.jpeg\" /></p><p>中国信通院云计算与大数据研究所所长何宝宏博士主题演讲</p><p>中国信通院云计算与大数据研究所所长何宝宏博士发表题为《转型的终点：数字原生》的主题演讲，从技术应用的生命周期，电力化转型与原生的视角切入，围绕软件、数据、算力、AI、Web3、元宇宙等技术和应用展开，深入分析了数字原生正在和即将带来的机遇和挑战，描绘了数字原生时代的场景，为更多企业和行业加快数字化转型，制定面向数字原生时代的战略规划提供了参考。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/7870c1aa081c1a9c1ac3a56d1a282b13.jpeg\" /></p><p>《数字原生》新书发布仪式</p><p>大会举行了何宝宏所长《数字原生》新书发布仪式。中国通信标准化协会理事长闻库，中译出版社社长乔卫兵、副总编辑刘永淳，中国信通院云计算与大数据研究所所长何宝宏共同出席新书发布环节。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c2/c2681d196de6a94265ffdc9da89fa540.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f70c0ba525471fccc5ff3c64099553b2.jpeg\" /></p><p>大会正式公布第二届“鼎新杯”数字化转型应用大赛标杆案例。本次“鼎新杯”数字化转型应用大赛以“数字扬帆，鼎新引领”为主题，设置了6大方向16个赛道，遴选和展示了一批极具代表性的数字化转型应用案例，为促进数字技术与业务融合创新，加速产业数字化发展，推动数字经济高质量发展注入了强劲动力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/00/002fccca5d17a060a3a6b5ee1f14a54d.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4fc35a9e30a7e04bed64fa66df0cfebf.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef7b7ab8b55400a459ef319551f0829a.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bbeec08c86121cc57b9fec2f14afa6c0.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/79f5d3e03a2be9706775b0f4c4815ef2.png\" /></p><p>大会发布了2023年上半年政企数字化转型IOMM最新评估结果，涵盖企业数字化转型、数字政府、云边端数字化三大类评估体系。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/6746842d7ab0a072e81e1f2d6cb40948.jpeg\" /></p><p>央国企高质量数字化发展赋能计划启动仪式</p><p>会上，中国信通院云计算与大数据研究所所长何宝宏、中国电信集团政企信息服务事业群处长高扬、联通数字科技有限公司战略客户服务部总经理王涛、金蝶软件（中国）有限公司副总裁张振坤、阿里云智能集团战略发展副总裁逢孝钢、华为云央国企行业总经理龚岳院、腾讯云能源行业副总经理吴丽颖、浪潮云企业云事业部副总经理练飞、百度智能云产业发展生态合作总监石乔共同启动“央国企高质量数字化发展赋能计划”，旨在为央国企高质量数字化转型提供有力支撑和实际落地指导，同时整合行业优质赋能资源，形成推动央国企数字化转型落地的有效方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c9/c9a606fb16c316c24e44cc63b48a8c7a.jpeg\" /></p><p>中国信通院云计算与大数据研究所副所长栗蔚发表演讲</p><p>会上，中国信通院云计算与大数据研究所副所长栗蔚发表题为《中国数字化转型数实融合IOMM综合指数（2023年）》的演讲。她表示随着国家战略的高度重视以及数字经济规模再创新高，数实融合成为数字经济发展的核心任务。中国信通院聚焦数实融合，深入研究不同行业、不同企业，以及典型行业的地域特征，得出数实融合IOMM综合指数，为进一步揭示数实融合发展趋势，评价我国重点行业及其代表性企业的数字化发展水平、转型成效提供了有益参考。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/77a4278a2f9a9778c5eba6caf537829f.jpeg\" /></p><p>中国信通院云计算与大数据研究所政企数字化转型部主任徐恩庆发表演讲</p><p>中国信通院云计算与大数据研究所政企数字化转型部主任徐恩庆带来《以IOMM方法论推动政企数字化转型》主题分享。徐恩庆指出，数字化转型正在成为推动经济高质量发展的重要途径，数实融合已成为国家重点战略之一，中国信通院深入研究数字化转型趋势及方法论，形成了覆盖企业整体转型、业务数字化、数字化平台、数字基础设施、数字政府、信创等的数字化转型方法论体系，并通过构建产业平台、标准评估等为推进我国数字化发展持续赋能。</p><p></p><h4>发布仪式</h4><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/80e8ad48a203e5594fa02af6346d398f.jpeg\" /></p><p>会上还发布了多项企业数字化、数字政府、政企信创最新研究成果，包括《流程挖掘行业发展报告（2023）》、《低代码无代码产业图谱》、《组装式发展白皮书》、《数字化供应链赋能产业链韧性协同发展（2023）》报告、《汽车金融行业数字化转型白皮书》、《一城一云服务城市高质量发展白皮书》、《政务大模型建设路径及评价体系研究报告》、《数字政府建设与发展研究报告（2023年）》、《音视频媒体处理平台技术能力要求》，启动了《企业流程治理研究报告（2023）》、组装式系列标准、SEPT信创标准化评估平台、信创ERP质效提升专项计划，并进行了数字政府签约仪式。</p><p></p><p></p><h4>先锋人物结果公布</h4><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/164da811347acfbd872e01157145a283.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9f/9fcd1c427947ee54ea5334406a4390e9.png\" /></p><p>大会公布了2023企业数字化发展共建共享平台（EDCC）先锋人物、2023数字政府建设赋能计划先锋人物。</p><p></p><p></p><h4>主题演讲环节</h4><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b3c84bd09bfa00bddc270524a38a04f.jpeg\" /></p><p>大会同时邀请了产业各方嘉宾发表精彩演讲。上午环节，招商局集团数字化中心技术管理处处长山金孝发表《数字化助力构建“两条曲线”，推动“三次创业”》主题演讲；中国电信集团有限公司全渠道运营中心总经理助理魏丫丫发表《指针：中国电信新零售数字化实践》主题演讲；华为云计算技术有限公司华为云城市云总经理赵永明发表《一城一云，共助城市数字化“智理”》主题演讲；中国联合网络通信集团有限公司数字化部总监杜宇发表《创新基因谋发展，转型升级谱新篇——中国联通数字化转型与创新实践》主题演讲；金蝶软件（中国）有限公司金蝶中国北京研发中心总经理汪东海发表《筑梦苍穹,智启未来》主题演讲。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7f/7fd21dfb5a6832f7a34e8e3944c4be45.jpeg\" /></p><p>大会下午环节，北京神州泰岳软件股份有限公司运营商BG产品副总裁兼数字化运营事业部总经理刘鹏发表《跃升数字化生产力，领航ICT运营管理新范式》主题演讲，长城汽车IDC产品总监张松发表《长城汽车采用PBC组装式架构搭建新一代业务中台》主题演讲，宿迁市大数据中心数据应用处处长周晓莉发表《宿迁市云数融合创新赋能数字政府建设》主题演讲，中国银行软件中心高级经理伊纯发表《银行数字化转型应用实践探索与讨论》主题演讲，中国移动云能力中心/中移（苏州）软件技术有限公司云原生负责人李莉发表《移动云星罗云原生底座，构建数字原生新基础设施》主题演讲，北京北投智慧城市科技有限公司总经理刘春凤发表《数据驱动的学习型智慧建筑大脑 ——北投大厦项目》主题演讲，重庆市两江协同创新区建设投资发展有限公司信息中心主任罗克发表《数字化变革下的协同与创新》主题演讲。</p><p></p><p>“2023数字化转型发展大会暨首届数字原生大会”历时两天，围绕数字化转型发展多个热点议题和前沿领域展开深入讨论，设立11个主题分论坛，包括：数字原生、云上数字政府、央国企信创与信创产业发展、大模型时代智慧政务发展新机遇、政务信创、信创ERP与经营管理数字化、数字化流程变革创新、企业数字化能力智慧运营、数字化安全生产与信创运维实践、低无代码与组装式技术应用、数字基础设施等，从不同角度、不同应用方向、不同技术特点为参会者呈现数字化转型发展相关的前沿思维、应用实践与技术创新情况。</p><p></p><p>大会咨询：</p><p></p><blockquote>中国信通院 云计算与大数据研究所政企数字化转型部车老师 18611139904（微信同号）&nbsp;chexin@caict.ac.cn</blockquote><p></p><p></p><p>业务咨询：</p><p></p><blockquote>中国信通院 云计算与大数据研究所&nbsp;政企数字化转型部董老师 18601280900（微信同号） dongenran@caict.ac.cn</blockquote><p></p>",
    "publish_time": "2023-09-19 16:15:27",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Mojo 正式发布！ Rust 能否与之匹敌？",
    "url": "https://www.infoq.cn/article/QXg26JLOUKWRro1xxo2U",
    "summary": "<p></p><p>9 月 7 日，Modular 公司宣布正式发布 Mojo：Mojo 现在已经开放本地下载——初步登陆 Linux 系统，并将很快提供 Mac 与 Windows 版本。据介绍，Mojo 最初的目标是比 Python 快 35000 倍，近日该团队表示，Mojo 将动态与静态语言的优点结合起来，一举将性能提升达 Python 的 68000 倍。那么未来的人工智能的语言，是 Rust 还是 Mojo ？张汉东从 Rust 和 Mojo 语言的特性、生态和其在 LLM 大模型时代的角色进行了剖析。</p><p></p><p></p><h2>编程语言是推动时代齿轮的抓手</h2><p></p><p></p><p>我从 2006 年入软件行业，截止今年我的职业生涯已经走过十七个年头。</p><p></p><p>这十七年我虽然没有什么光彩履历，但却很幸运，我还能在这个行业坚守，并能不断成长。同样很幸运，我经历了桌面软件没落， Web 2.0 崛起，以及移动互联网的兴盛，当下基础设施系统软件开始复兴的诸多历程。</p><p></p><p>这么多年我思考最多的两个问题就是：</p><p></p><p>编程语言对于程序员来说到底意味着什么？我们为什么要不断地学习新的编程语言？能不能学一门就行？</p><p></p><p>这两个问题的答案，取决于你如何看待编程语言。</p><p></p><p>在社区经常会听到一句话：“编程语言就是工具”。编程语言确实是工具，用于谋生的工具，用于编写软件的工具。在我看来，编程语言不仅仅是工具，更是思想的集合时代的缩影。编程语言的发展跟随计算机的发展一路走来，其中蕴含着推动时代变革的解决不同问题的思想。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/09/09425e1edee51e4bcdbb69515ee5e9a3\" /></p><p></p><p>编程语言背后都有共同的东西，比如计算机基础和其他领域知识。这些是可以在不同语言之间迁移的知识，只需要学一遍。但是编程语言的设计思想却是不同的，这就是编程语言吸引人的地方之一。就像同样都叫威士忌，可能会有不同的味道。</p><p></p><p>编程语言如何设计，一般都和它想要解决的问题有关。而它想解决的问题，通常都与语言创造者所处的时代和眼界有关。</p><p></p><p>C 语言的诞生，是为了解决操作系统快速交付的难题，背后是 70 年代操作系统的发展；Cpp 语言的诞生，是为了给 C 语言引入面向对象，提升开发效率，背后是 80 年代工业软件快速增长需求；Python / Java 语言的诞生，是为了让开发者专注于业务而非语言细节，背后是 90 年代日益增长的 Web 开发需求。</p><p></p><p>随着互联网的高速发展，2010 年编程语言领域迎来一个拐点，Rust 语言之父 Graydon 认为未来互联网应该是安全和性能并重，所以他集过去四十年众语言优势为一体，创造了 Rust 语言。到了 2022 年， Mojo 语言作者 Chris 认为 AI 基础设施生态的碎片化已经阻碍了 AI 的发展，所以他创造 Mojo 语言，想一统 AI 生态，解决碎片化问题，实现 All in One 理想。</p><p></p><p>回顾历史，我们看得出来。时代在不断变化，编程语言是推动时代齿轮的抓手。当新的时代到来时，有些语言是必须要学习的。让我们从 Rust 和 Mojo 语言的特性、生态和其在 LLM 大模型时代的角色来探索这两门语言的未来。</p><p></p><p></p><h2>Rust vs Mojo ：雄心与现状</h2><p></p><p></p><p></p><h3>Rust 语言</h3><p></p><p></p><p>Rust[1]&nbsp;&nbsp;语言之父 Graydon 带着内存安全和性能并重的设计初衷于 2009 年创立了 Rust 语言，幸运的是，这颗种子是种在了 Mozilla 这片开放的土壤中，在 2015 年结出了开放的花朵。</p><p></p><p>Rust 语言并不是要百分百地解决内存安全问题，而是消除过去五十年导致系统编程语言中 70% 安全 Bug 中的内存安全问题：</p><p></p><p>引用空指针。使用未初始化内存。释放后使用，即悬垂指针。缓冲区溢出，比如数组越界。非法释放已释放过或未分配的内存。并发场景下的数据竞争。</p><p></p><p>为了达成此目标，语言设计需要在六个原则中进行权衡：</p><p></p><p>可靠性。代码编译即正确。高性能。代码执行效率可以媲美 C/Cpp。支持性。为用户提供多方面支持，比如 IDE、用户友好的编译错误信息等。生产力。让开发更有效率，事半功倍。透明性。让用户对底层资源具有透明控制力。多样性。多个领域都可以用 Rust 。</p><p></p><p>Rust 语言是在这些原则中权衡的结果，客观情况无法做到同时满足这六大原则。所以，导致的问题就是学习曲线较其他语言更高，对学习和使用者的基础有一定的要求。</p><p></p><p>因为 Rust 语言面对的这个问题领域本身就非常复杂。从 Rust 语言架构层面来看，Rust 语言为了解决内存安全和高性能并重的问题，给出的方案其实非常简洁。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/af/af3fdd4d4a27da2422b05585ed686f3c\" /></p><p></p><p>首先，Rust 语言是一门编译语言。Rustc 是其编译前端，在编译过程中，通过精心设计的类型系统，通过对代码中类型的检查，来实现对内存安全进行管理，以及更好地优化代码。编译后端包括 LLVM 和 Cranelift，以及正在支持的 GCC 后端和 SPIR-V GPU IR。</p><p></p><p>其次，在类型系统之上，Rust 语言也提供了更高级的抽象范式，支持面向对象风格和函数式编程风格，甚至可以直接向写 C 那样遵循过程式范式。并且引入了很多现代化语言特性，比如 trait 和 enum ，允许开发者易于编写出更具可扩展性的系统。</p><p></p><p>从 2015 年 Rust 1.0 稳定版发布到写本文之时 （2023 年 9 月），Rust 已经发布了 72 个语义化版本，三个 Edition 版次（每三年发布一次的大版本）。在 Stackoverflow 的年度调研报告中，Rust 连续八年收获最受欢迎语言称号。</p><p></p><p>一门语言最重要的就是其生态。截止目前，crates.io 上面 crate 数量已经超 12 万，下载量已达到 393 亿次。虽然，Rust 学习曲线较高，但也没有阻碍生态的发展。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/21/21b496261528043f348f97277217a3b0\" /></p><p></p><p>并且其生态从 2020 年起，每年下载量以 1.7 倍速度增长。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/40/40adcc14f465d2fdd4981eb431833f95\" /></p><p></p><p>并且其生态基本能覆盖到 C/Cpp/Java/Go 等语言的应用领域。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/74/747515781727bb0cb51aedbae08e4bbf\" /></p><p></p><p>截止今年，Rust 语言已经证明了其在系统编程领域的优势。曾经流行的 Rewrite it in Rust 梗，已经变为了现实，目前能用 Rust 重写的基本都已经用 Rust 重写了。包括一些新的系统，Rust 也是第一选择。这里说的系统，目前是指基础设施领域的系统，包括 AI 基础设施。</p><p></p><p></p><h3>Mojo 语言</h3><p></p><p></p><p>据 Mojo 官方声称，Chris 在 2022 年创建 Modular 公司时，并未打算创造 Mojo 语言。他们在构建下一代推理引擎 Modular 时，发现整个技术栈的编程模型过于复杂，并且手动编写了大量的 MLIR，开发效率极低。因此，他们萌生了创建新的编程语言来统一整个技术栈的想法，Mojo 就诞生了。</p><p></p><p>Mojo 想要的是一种创新且可扩展的编程模型，能够针对在人工智能领域中普遍存在的加速器和其他异构系统进行编程。这意味 Mojo 要成为一种具有强大的编译时元编程能力、集成自适应编译技术、在整个编译流程中进行缓存以及其他现有语言不支持的功能的编程语言。</p><p></p><p>从 Mojo 语言架构层面来看，Mojo 如何解决这个问题：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f7/f71b240ed7379bb9e74bb2f3000cf857\" /></p><p></p><p>首先，Mojo 的语法兼容了 Python 语法。因为 AI 生态中 Python 库占据生态位，想要一统天下，必须坐拥 Python。曾经 Chris 在 Apple 就有过类似经历，Swift 可以与 ObjectiveC 的库混编，用了五年时间完成了语言之间的过渡。</p><p></p><p><code lang=\"python\">$ cat hello.🔥\ndef main():\n    print(\"hello world\")\n    for x in range(9, 0, -3):\n        print(x)\n$ mojo hello.🔥\nhello world\n9\n6\n3\n$\n</code></p><p></p><p>语法虽然与 Python 相似，但是 Mojo 的 def 定义中允许强类型检查。因为 Mojo 是 Python 的超集。</p><p></p><p><code lang=\"php\">struct MyPair:\n    var first: Int\n    var second: Int\n\n    # We use 'fn' instead of 'def' here - we'll explain that soon\n    fn __init__(inout self, first: Int, second: Int):\n        self.first = first\n        self.second = second\n\n    fn __lt__(self, rhs: MyPair) -&gt; Bool:\n        return self.first &lt; rhs.first or\n              (self.first == rhs.first and\n               self.second &lt; rhs.second)\n</code></p><p></p><p>看上去像不像 Rust 代码？更准确来说是披着 Python 皮的 Rust 。从这一点来看，Chris 也许也很喜欢 Rust 的设计，否则不会借鉴。</p><p></p><p>话说回来，Mojo 提供了 fn &nbsp;，相比 def 具有更加严格的检查，适合于系统编程。而 struct 的结构和内容是预先设置的，在程序运行时无法更改。与 Python 不同，你无法在运行过程中随意添加、删除或更改对象的属性。Mojo 不允许这样做。Mojo 支持 AOT 和 JIT 两种方式。</p><p></p><p>在 Mojo 语法之下是 MLIR。</p><p></p><p>MLIR，即 Multi-Level IR，是一种可扩展的中间表示（IR）格式，用于编译器设计。许多不同的编程语言和编译器将其源程序转换为 MLIR，因为 Mojo 提供了对 MLIR 功能的直接访问，这意味着 Mojo 程序可以享受到这些工具的好处。</p><p></p><p>Mojo 可以使用 MLIR 自定义类型。比如，使用 Mojo 的 struct 关键字来定义一个新的类型 OurBool：</p><p></p><p><code lang=\"cs\">struct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(inout self):\n        self.value = __mlir_op.`index.bool.constant`[\n            value : __mlir_attr.`false`,\n        ]()\n</code></p><p></p><p>一个布尔值可以表示 0 或 1，true 或 false。为了存储这个信息， OurBool 有一个单一成员，称为 value 。它的类型使用 MLIR 内置类型 i1 。实际上，在 Mojo 中可以使用任何 MLIR 类型，只需在类型名称前加上 __mlir_type 。</p><p></p><p>为了初始化底层的 i1 值，我们使用了来自 index 方言的 MLIR 算子，称为 index.bool.constant 。</p><p></p><p><code lang=\"bash\">let a = OurBool()\n# error: 'OurBool' does not implement the '__copyinit__' method\nlet b = a\n</code></p><p></p><p>创建一个 a ，然后将其赋值给 b，则会报错。因为 Mojo 语言中默认为 值语义，OurBool 并未实现 copyinit 方法，所以无法复制。</p><p></p><p><code lang=\"cs\">@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__() -&gt; Self:\n        return Self {\n            value: __mlir_op.`index.bool.constant`[\n                value : __mlir_attr.`false`,\n            ]()\n        }\n</code></p><p></p><p>通过为结构体增加装饰器 @register_passable(“trivial”) ，就可以复制其实例变量了。trivial 代表是“平凡的”或“平平无奇的”简单值，可以安全复制。这里有点类似于 Rust 语言的复制语义。</p><p></p><p>MLIR 是模块化和可扩展的。MLIR 由越来越多的“方言（Dialects）”组成。每个方言定义了算子（Operation）和优化：例如，“math”方言提供了诸如正弦和余弦等数学操作，“amdgpu”方言提供了针对 AMD 处理器的特定操作，等等。方言经过降级之后，Mojo 代码将被编译到指定平台的机器指令。</p><p></p><p>MLIR 的每个方言都可以互操作。这就是为什么说 MLIR 可以解锁异构计算的原因。随着新的、更快的处理器和架构的开发，新的 MLIR 方言被实现以生成适用于这些环境的最优代码。任何新的 MLIR 方言都可以无缝地转换为其他方言，因此随着更多方言的添加，所有现有的 MLIR 都变得更加强大。</p><p></p><p>利用 MLIR 这种特性，就实现了底层异构系统大统一。这就是 Mojo 解决问题的方式。</p><p></p><p></p><h2>Rust vs Mojo : 对立还是融合？</h2><p></p><p></p><p></p><h3>Mojo 官方观点</h3><p></p><p></p><p>七月份，Modular 官方博客发布标题为《未来的人工智能的语言，是 Rust 还是 Mojo ？》[2]&nbsp;的一篇文章。其中谈到 Rust 的语言特性，在 AI 领域相比于 Python 和 Cpp ，是一门更好的语言，这是一种认可。但是因为 Rust 语言是从零开始设计，其在 AI 领域的生态位还相当年轻，不如 Python 和 Cpp 。虽然生态中有一些 Rust 绑定库，比如 OpenCV-rust&nbsp;[3]&nbsp; 或者 libonnxruntime 的绑定 ort [4]&nbsp;，都是独立贡献者维护的，从 2019 年到现在进展不大。虽然现在也有更好的 Rust 实现，比如 &nbsp;tract-onnx&nbsp;[5]&nbsp;，但是缺乏贡献者和运营者，进展缓慢。并且声称绝大多数人工智能研究人员都是使用 Python，而且对学习 Rust 不感兴趣，因此很不可能在机器学习领域得到广泛应用。</p><p></p><p>反观 Mojo ，可以复用任何一个 Python 库。并且在语法上兼容 Python ，会受到广大人工智能研究人员的喜爱。除此之外，Mojo 也能简化当前 Python + Cpp 的麻烦，比如如果想加速代码，可能还得学习如何在 C++ 中使用 SIMD 指令集作为备选方案等。官方给出了一个用 Mojo 做快速均值模糊 (Box Blur) 的示例。其中用到了 MLIR 提供的 SIMD 功能和 自己编写的用于将表示地址的 Python 整数转换为具有给定数据类型的 Mojo 指针的功能（代码如下）。</p><p></p><p><code lang=\"javascript\">from DType import DType\nfrom Pointer import DTypePointer\n\nfn numpy_data_pointer(numpy_array: PythonObject) raises -&gt; DTypePointer[DType.uint32]:\n    return DTypePointer[DType.uint32](\n                __mlir_op.`pop.index_to_pointer`[\n                    _type:__mlir_type.`!pop.pointer&gt;&gt;`\n                ](\n                    SIMD[DType.index,1](numpy_array.__array_interface__['data'][0].__index__( \"DType.index,1\")).value\n                )\n            )\n\n</code></p><p></p><p>其中，pop 是 Modular 团队开发的 MLIR 方言。它并不是为了普通程序员需要理解这个语法，随着时间的推移，有用的东西将会被编译器工程师封装成一个漂亮的 API，供系统工程师和 Python 程序员（未来的 Mojo 程序员）在更高的层次上使用。但开发者仍然有能力定义自己的方言或使用 MLIR 生态系统中已经定义好的方言之一，这使得供应商可以轻松加速他们的硬件，例如 gpu 方言 [6]&nbsp;。</p><p></p><p>以上，是 Mojo 官方的观点。总结为一句话就是：Rust 很好，但其生态位不足；Mojo 才是 AI 的未来。</p><p></p><p></p><h3>Mojo 官方忽略的问题：大模型时代开启，资本推动与时间差</h3><p></p><p></p><p>Mojo 官方提供的观点论据都很足，很有道理。但是我认为官方忽视了一个重要问题：一门语言成熟所需的周期。</p><p></p><p>虽然 MLIR 功能强大极具潜力，但 Mojo 语言当前还是一个小火苗。它还需要很长时间来兑现它的承诺：安全、高性能、像 Python 一样简单易用。Rust 语言从发布到成熟，花了八年；Go 语言十三年；Swift 差不多也是八年。那么 Mojo 语言需要几年呢？</p><p></p><p>从上面官方给出的各种示例中发现，Mojo 标准库目前还未建立，如果用 Mojo 开发，还需要开发者懂 MLIR 各种方言，这学习曲线也许比 Rust 更高一个量级。难道这就是 Python 开发者喜欢的？</p><p></p><p></p><blockquote>“话说，过去八年 Rust 最受欢迎语言榜首投票中难道没有 Python 程序员？</blockquote><p></p><p></p><p>Mojo 语言目前只被用于其母公司产品 Modular 推理引擎的开发中。从小道消息处得知，有些 AI 公司也已经投入了 Mojo 的前期培训。</p><p></p><p>而 Rust 语言，2023 年之前确实在 AI 生态上进展缓慢，但今年大语言模型时代开启，资本大量涌入 AI 生态。在 Mojo 兑现承诺的这段时间差中，Rust 语言极有可能在 AI 生态中占据一席之地。</p><p></p><p>因为，据我观察，2023 年 AI 领域的一些独角兽已经开始采用 Rust 了。</p><p></p><p></p><h3>Rust 的 AI 生态位</h3><p></p><p></p><p>我们简单盘点一下当前 Rust 在 AI 领域的生态位。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cd/cd81d7075d79fc23278e4bebe073ad94\" /></p><p></p><p>AI 领域涉及模型训练、模型部署、到智能应用这一系列流程。在这整个流程过程中，都能看到 Rust 语言的影子。我们简单将其分为下面五类：</p><p></p><p>高性能数据分析深度学习框架及其依赖推理引擎开源大模型大模型应用相关基础设施</p><p></p><p></p><h4>高性能数据分析</h4><p></p><p></p><p>Polars在数据操作层面，每个人都喜欢 Pandas 的 API。它快速、简单且有据可查。但在生产方面，Pandas 有点棘手。Pandas 不能很好地扩展……没有多线程……它不是线程安全的……它不是内存效率。这一切都是 Rust 存在的理由。</p><p></p><p>Polars[7]&nbsp; 用 Rust 实现的新 Dataframe 库，具有方便的 Python 绑定。它试图做到以线程安全的方式进行读取、写入、过滤、应用函数、分组和合并。Polars 建立在 Apache Arrow 规范 [8]&nbsp; 的 安全 Arrow2 实现 [9]&nbsp; 之上 ，可实现高效的资源使用和处理性能。它还可以与 Arrow 生态系统中的其他工具无缝集成。</p><p></p><p>Polars 有两个优势：</p><p></p><p>它是性能杀手，参考 db-benchmark[10]&nbsp;。它的 API 非常简单。哪怕不懂 Rust 语法也能看懂该接口要做什么。</p><p></p><p>也有三个缺点：</p><p></p><p>构建 Dataframe API 很困难，Pandas 花了 12 年才达到 1.0.0，而 Polars 很年轻，所以目前还不够成熟。使用它的时候，不仅仅要熟悉 Polars API，还需要熟悉 Arrow API，因为很多繁重工作是 arrow 来完成的。编译时间太慢，可能需要 6 分钟左右。</p><p></p><p>Polars 现在主要由 Xomnia[11]&nbsp;公司赞助。Xomnia 是荷兰一家人工智能公司，在研究自动驾驶船只，被人称为水上特斯拉。</p><p></p><p>Linfa</p><p></p><p>Linfa&nbsp;[12]&nbsp; 是一组 Rust 高级库的集合，提供了常用的数据处理方法和机器学习算法。Linfa 对标 Python 上的 scikit-learn，专注于日常机器学习任务常用的预处理任务和经典机器学习算法，目前 Linfa 已经实现了 scikit-learn 中的全部算法，这些算法按算法类型组织在各子包中。</p><p></p><p>目前 Linfa 的中期 Roadmap[13]&nbsp; 距离与 Python 的 scikit-learn 目前可用的 ML 算法和预处理程序相媲美的实现的最终目标。</p><p></p><p>其他</p><p></p><p>nalgebra[14]&nbsp;，是 Rust 的通用线性代数库，和 Rapier 一起都是 ，Dimforge 开源组织[15]&nbsp;开发的。</p><p></p><p></p><h4>深度学习框架及其依赖</h4><p></p><p></p><p>HuggingFace 出品：Candle</p><p></p><p>candle[16]&nbsp;&nbsp;是 AI 独角兽 HuggingFace 出品的专注于性能（包括 GPU 支持）和易用性的 Rust 极简机器学习框架。</p><p></p><p>candle 框架的特点是：</p><p></p><p>语法简单，看起来和使用起来都像 PyTorch。多后端支持。优化的 CPU 后端，可选支持 x86 的 MKL 和 mac 的 AccelerateCUDA 后端以高效地在 GPU 上运行，通过 NCCL 实现多 GPU 分布。WASM 支持，允许在浏览器中运行模型。多模型支持。LLMs: LLaMA v1 和 v2，Falcon，StarCoder。Whisper（多语言支持）。Stable Diffusion。计算机视觉：DINOv2，EfficientNet，yolo-v3，yolo-v8。支持从 safetensors、npz、ggml 或 PyTorch 文件中加载模型支持在 CPU 上 Serverless 部署使用 llama.cpp 的量化类型来支持量化</p><p></p><p>Candle 的核心目标是实现无服务器推理。像 PyTorch 这样的完整机器学习框架非常庞大，这使得在集群上创建实例变得缓慢。Candle 允许部署轻量级二进制文件。</p><p></p><p>HuggingFace 其他 Rust 开源库：</p><p></p><p>safetensors[17]&nbsp; ，安全存储和分发张量（tensor），并且是高性能（零拷贝）。该库主要是为了消除默认情况下使用的 pickle 的需要，因为 pickle 是不安全的，有运行任意代码风险。tokenizers[18]&nbsp;.</p><p></p><p>新晋开源框架：Burn</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/48/482cfae26e81cf8ece4acb7acd4077d0\" /></p><p></p><p>Burn[19]&nbsp; 是一款开源的致力于成为全面的深度学习框架。它提供卓越的灵活性，并且使用 Rust 语言实现。目标是通过简化实验、训练和部署模型的过程，为研究人员和实践者提供服务。</p><p></p><p>Burn 的进展非常快，目前已经发布 0.9 版本。它的特点是：</p><p></p><p>可定制、直观且用户友好的神经网络模块。全面的训练工具，包括 metrics 、 logging 和 checkpointing。多功能的张量可插拔的后端工具箱：Torch[20]&nbsp; 后端，支持 CPU 和 GPUNdarray[21]&nbsp; 后端与 no_std 兼容性，确保了通用平台的适应性WebGPU[22]&nbsp; 后端，提供跨平台、包含浏览器的基于 GPU 的计算Candle[23]&nbsp; 后端。Autodiff[24]&nbsp; 自动微分后端。Dataset[25] 包含各种实用工具和资源的容器。Import[26]&nbsp; ，是用于导入一个简化预训练模型集成的包。</p><p></p><p>学习更多内容可以参考 Burn Book[27]&nbsp;。</p><p></p><p>社区也有第三方基于 Burn 实现了开源大模型：</p><p></p><p>stable-diffusion-burn[28]&nbsp;，将 Stable Diffusion v1.4 移植到 Burn 框架中。stable-diffusion-xl-burn[29]&nbsp;，将 stable diffusion xl 移植到 Rust 深度学习框架 burn 中。llama2-burn[30]&nbsp;，将 Meta 的大型语言模型 Llama2 移植到 Rust 深度学习框架 Burn 上。whisper-burn[31]，是使用 Rust 深度学习框架 Burn 实现的 OpenAI Whisper 转录模型的 Rust 版本。</p><p></p><p>其他框架</p><p></p><p>tch-rs[32]&nbsp;是 Pytorch 的 Cpp API 的 Rust 绑定，目前正在活跃维护中。tensorflow-rs[33]&nbsp;，是 Tensorflow 官方提供的 Rust &nbsp;绑定，目前正在活跃维护中。dfdx[34]，是一个强大的 crate，其中包含了类型中的形状。这样一来，编译器就可以立即检测到形状不匹配的问题，从而避免了很多麻烦。</p><p></p><p>潜力股：自动微分器 EnzymeAD Rust 前端</p><p></p><p>Enzyme[35]&nbsp;是 MIT 提出的自动微分框架，用于对可静态分析的 LLVM 和 MLIR 进行自动微分。当前，PyTorch、TensorFlow 等机器学习框架已经成为了人们开发的重要工具。计算反向传播、贝叶斯推理、不确定性量化和概率编程等算法的梯度时，我们需要把所有的代码以微分型写入框架内。这对于将机器学习引入新领域带来了问题：在物理模拟、游戏引擎、气候模型中，原领域组件不是由机器学习框架的特定领域语言（DSL）编写的。因此在将机器学习引入科学计算时，重写需求成为了一个挑战。</p><p></p><p>为了解决这一问题，现在的发展趋势包含构建新的 DSL，让重写过程变得简单，或者在编程时直接进行构建。这些方法可以让我们获得有效的梯度，但是仍然需要使用 DSL 或可微分的编程语言进行重写。为了方便开发者，来自 MIT 的研究者开源了 Enzyme。</p><p></p><p>目前，Enzyme 团队 fork Rust 语言项目开始实施 EnzymeAD Rust 前端&nbsp;[36]&nbsp; ，工作正在进行中。</p><p></p><p>依赖的基础库</p><p></p><p>pyo3[37]&nbsp;主要用于创建原生 Python 的扩展模块。PyO3 还支持从 Rust 二进制文件运行 Python 代码并与之交互，可以实现 Rust 与 Python 代码共存。因此，pyo3 是 Rust 和 AI 生态中的 Python 库交互必不可少的依赖库。目前 pyo3 维护非常活跃。</p><p></p><p>llm[38]&nbsp;，是一个用于处理大型语言模型的 Rust 库生态系统 - 它是基于快速高效的 GGML[39]&nbsp;机器学习库构建的。llm 由 ggml 张量库提供支持，旨在将 Rust 的稳健性和易用性带入大型语言模型的世界。目前，推理仅在 CPU 上进行，但后续希望通过备用后端在将来支持 GPU 推理。</p><p></p><p></p><h4>推理引擎</h4><p></p><p></p><p>tract：为嵌入式而生的推理引擎</p><p></p><p>Sonos[40]&nbsp; 是一款家庭智能音箱，该公司开源了一款 Rust 实现的推理引擎 tract[41]&nbsp;。tract 的设计是为了在小型嵌入式 CPU 上运行神经网络。</p><p></p><p>AI 大模型时代，算力是一个很大的问题。目前 AI 基本是被部署到云端，推理在云端完成：用户数据将被发送到云端，经过模型处理后，结果将被发送回终端用户的设备。有时候使用云服务并不是一个好的选择。自动驾驶汽车不能在进入隧道时停止行驶。当世界另一边的数据中心出现问题时，人们不应该被锁在家外面。而且，我们中的一些人只愿意与自己拥有的设备进行交互，而不是与那个神秘的云共享生活的一部分。所以，AI 芯片通常分为三个关键应用领域：云端训练、云端推理和边缘推理。</p><p></p><p>边缘推理场景下，大模型可以在消费级终端上面进行推理，包括 HuggingFace 开源的 candle 也是为了边缘计算。这背后有个大的目标就是万物大模型。</p><p></p><p>tract 架构背景</p><p></p><p>模型训练和推理是两个独立过程。训练模型是一项艰巨且复杂的任务，而推理则相对简单。模型设计和训练也是该领域大部分研究的重点。</p><p></p><p>在模型设计和训练过程中，机器学习团队注重预测的准确性。虽然整体计算预算是一个已知的限制条件，但目标是找出最佳的模型设计和训练过程，以获得最佳的准确性。</p><p></p><p>在推理过程中，效率至关重要。模型和硬件在这个阶段是固定的实体：问题是尽可能高效地使给定的模型在给定的硬件上运行。首先，要适应硬件，然后尽可能释放更多资源以供未来的发展使用。</p><p></p><p>一旦网络训练完成并冻结，训练的相关性就消失了。随之而来的是许多对模型设计和训练有用的抽象变得多余：当执行两个值的乘法时，CPU 并不太关心这个操作属于哪个高级神经网络概念，比如卷积或者归一化层。</p><p></p><p>当今 AI 生态中 ONNX （Open Neural Network eXchange）非常重要，ONNX 构建了一个开放的生态系统，它使人工智能开发人员在推进项目时选择合适的工具，不用被框架或者生态系统所束缚。这种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型，得不同的人工智能框架（如 Pytorch、MXNet）可以采用相同格式存储模型数据并交互。</p><p></p><p>但 ONNX 仍然非常注重模型设计和训练。从推理引擎实现的角度来看，它仍然包含许多冗余的运算符。2017 年由开源组织 Khronos Group 制定的 NNEF 神经网络交换标准则使用了一个更低级的表示，其中训练语义被抹去了。</p><p></p><p>NNEF 格式对于推理目的来说几乎是理想的，但该格式并不够主流，大多数软件集成商希望能够直接支持 ONNX 或 TensorFlow。所以，tract 引入了 tract-opl，它在语义上与 NNEF 非常接近：专注于简单操作，而不考虑 ONNX 和 TensorFlow 格式编码的高级训练特性。它被设计为一组 NNEF 扩展：如果模型不使用 NNEF 不包含的任何特性或运算符，tract 实际上可以将 tract-opl 序列化为纯 NNEF。这也意味着 tract 可以从 ONNX 和 TensorFlow 转换为 NNEF。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/81/81b7c843d7314acf7c932476d5e75375\" /></p><p></p><p>模型推理是计算密集型的任务。神经网络背后都会涉及到卷积和矩阵运算。tract 为了提供高性能和跨平台，利用 Rust 和 SIMD，以及内联汇编技术，来优化卷积和矩阵运算。比如自 2014 年至今移动 SoCs 最广泛使用的 CPU 架构 Cortex-A53，以及 苹果 M1 采用的 ARMv8 芯片，如果想充分利用这类芯片的性能，则需要汇编的加持。</p><p></p><p>目前 tract 还算是 Rust AI 生态中比较流行的推理引擎。该框架也处于积极维护中。</p><p></p><p></p><h4>开源大模型</h4><p></p><p></p><p>LLama2 Rust今年 7 月份，杨立昆在 X 上转发了来自 Sasha Rush 的开源大模型 LLama2 的纯 Rust 实现[42]&nbsp; 。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ae/aeab981f9d6311110a276510f8e95371\" /></p><p></p><p>而 Sasha Rush 是 HuggingFace 的工程师。看来 HuggingFace 内部对于 Rust 语言很是喜欢。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/2d/2dda03c9c99542cd97617e454ce83caf\" /></p><p></p><p>llama2.rs &nbsp;的目标是在 CPU 上进行推理，这样的好处就是，想要部署开源大模型的公司不必要专门去寻找包括 GPU 的机器了，也算是降本增效吧？</p><p></p><p>商业大模型Deepgram[43]&nbsp;是一家基础人工智能公司，提供语音转文本和语言理解能力，使数据能够被人类或机器读取和应用。Deepgram 是人类语音识别领域真正的专家。该服务使用先进的技术将音频文件无缝转换为文本。这家自然语言处理公司提供使用该服务转换电话、会议等的选项。所有这些都可以使公司的工作变得更加简单。去年年底完成 7200 万美元 B 轮融资。</p><p></p><p>去年 Deepgram 发布了一篇官方博客文章， 介绍了其平台为何使用 Rust 重写 [44]&nbsp;。Deepgram 的语音搜索 AI 大脑神经语音引擎 V4 版用 Rust 进行了重写，前三个版本都是 Python 实现的。</p><p></p><p>Rust 重写之后为他们解决了下列问题：</p><p></p><p>内存占用极大地降低了。可以放心地引入并发，解决了 CPU 和 GPU 的性能瓶颈。在这之前，因为音频识别领域需要靠 CPU 处理很多前置工作，比如解码之类，之前用 Python ，导致 CPU 的性能跟不上 GPU 而导致了性能瓶颈。用了 Rust 之后，可以放心地使用并发，并期待 GPU 成为瓶颈了。让开发人员专注于业务，而非把时间浪费在改 Bug 找 Bug 。</p><p></p><p></p><blockquote>“冷知识：GPU 没有成为瓶颈，意味着是对 GPU 的浪费。Deepgram 眼里的 AI 商业趋势：语言 AI 革命在一定程度上得益于一个并行趋势：深度学习在技术行业的普及。经过精细调整的深度神经网络，这些庞大而复杂的统计模型在数百万甚至数十亿个数据点上进行训练，展现出了一种近乎神奇、不合理的有效性，几乎可以完成任何它们被设计来完成的任务。例如，端到端深度学习（E2EDL）实现了几乎与人类准确度相当的语音转文本转录结果。适当配置的深度神经网络可以以几乎无限的规模运行，并且相比于人工转录员，提供更快速、更具成本效益的转录服务。目前，在优化深度神经网络方面需要投入相当大的人力，但在不久的将来，自学习神经网络将成为新常态。综合来看，这些核心技术是未来商业建立的基石。</blockquote><p></p><p></p><p></p><h4>大模型应用相关基础设施</h4><p></p><p></p><p>BlindAI : 快速且注重隐私的 Rust AI 部署解决方案</p><p></p><p>如今，大多数人工智能工具都没有隐私保护机制，因此当数据被发送给第三方进行分析时，数据就会暴露在恶意使用或潜在泄露的风险之中。比如使用 AI 语音助手时，音频录音经常被发送到云端进行分析，这样会导致对话内容暴露在外，被未经用户知情或同意的情况下进行泄露和未受控制的使用。尽管可以通过 TLS 安全地发送数据，但在其中一些利益相关者的环节中，仍然有可能被看到和暴露数据：租用机器的人工智能公司、云服务提供商或恶意内部人员。</p><p></p><p>BlindAI&nbsp;[45]&nbsp; 是一个利用安全隔离技术的 AI 部署解决方案，使远程托管的 AI 模型更加注重隐私保护。利用 &nbsp;tract[46]&nbsp; 项目作为推理引擎，在隔离环境中提供 ONNX 格式的 AI 模型服务。还使用 Rust SGX SDK[47]&nbsp;，在安全隔离环境中使用 Rust 语言。用户可以从云中的 AI 模型中受益，而无需向 AI 提供商或云提供商披露其明文数据。</p><p></p><p>bastionlab[48]&nbsp; 提供一个简单的隐私框架，用于数据科学协作，涵盖数据探索和人工智能训练。允许数据所有者和数据科学家可以在不暴露数据的情况下安全地合作，为那些过于冒险而不敢考虑的项目铺平道路。项目使用 Polars 🐻 进行数据探索，还使用了 Torch（tch-rs） 🔥，这是一个流行的用于 AI 训练的库。</p><p></p><p>其主公司 Mithril Security 目前处于 Pre Seed 融资，120 万欧元。</p><p></p><p></p><h4>向量数据库</h4><p></p><p></p><p>商业向量数据库 Pinecone向量数据库随着大语言模型时代的开启而迅速走上风口， Pinecone 则属于向量数据库行业内的独角兽。</p><p></p><p>Pinecone 虽然是闭源产品，但其在官方博客和 Rust 社区活动中都有相关的技术输出。就在去年年底，官方博客发表一篇文章《用 Rust 重写一个高性能的向量数据库》[49]&nbsp; ，其中记录了 Pinecone 从 Python + Cpp 到 Rust 重写的心路历程。</p><p></p><p></p><blockquote>“虽然向量数据库的概念多年来已被许多大型科技公司使用，但这些公司都建立了自己专有的深度学习 ANN 索引算法，用于提供新闻订阅、广告和推荐。这些基础设施和算法需要大量资源和开销，而大多数公司无法支持。Pinecone 解决方案通过严格的内存管理、高效的多线程和快速可靠的性能，填补了这个空白。这就是为什么需要专门的向量数据库。面对海量数据时，向量搜索的性能和相似度准确性，都是需要专门进行优化与平衡的，而非传统数据库简单地增加向量索引能解决的。</blockquote><p></p><p></p><p>Pinecone 在 Python + Cpp 的版本下，经常会遇到性能问题，但是却很难找到同时具备 Python 和 C++ 经验的开发人员来解决这些问题。所以 Pinecone 就用 Rust 重写了整个数据库。2023 年 4 月，Pinecone 拿到了 1 亿美元 B 轮融资。</p><p></p><p>虽然 Pinecone 用 Rust 重写了整个数据库，但并不意味着他们可以摆脱 Python ，毕竟 Python 是 AI 应用场景中占主导地位的语言 。就在前几天，Pinecone 工程师发文吐槽《 Python 的痛苦与诗意》[50]&nbsp;，并指明期待 Mojo 语言的到来。文中痛斥了 Python 项目的打包、测试、分发和测试工具生态系统，并使用 Poetry &nbsp;来管理 Pinecone Python 客户端，可以使用它来创建、更新和查询 Pinecone 向量数据库索引，因为他们认为它对 Pinecone 的内部维护人员、客户和社区贡献者提供了最多的好处。</p><p></p><p></p><h4>开源向量数据库</h4><p></p><p></p><p>Qdrant&nbsp;[51]&nbsp; 是一个向量数据库和向量相似度搜索引擎。Qdrant 是目前唯一一个纯 Rust 实现的开源向量数据库。</p><p></p><p>向量数据库作为大语言模型的「长期记忆」能力，当下很火。qdrant 目前融资 750 万美元种子轮。</p><p></p><p>传统数据库可以通过添加向量存储和向量搜索来提供向量数据库的功能，但是面对海量数据量，想要平衡向量搜索的准确度和性能，还需要专门的向量数据库。Qdrant （商业开源）和 Pinecone （商业闭源）就是专业的向量数据库。从 Qdrant 的实现看出，其在向量内存占用优化和向量海量搜索算法上下了不少功夫。内存占用优化使用 Product Quantization(乘积量化) 技术，使用 K-Means 聚类算法来平衡准确性和搜索性能。</p><p></p><p></p><h4>开源 AI Agent</h4><p></p><p></p><p>Chidor&nbsp;[52]&nbsp; &nbsp;也许是一个 LangChain 的替代品，同样可以方便的构建 AI Agent，主要优势是反应式编程。由 Rust 开发，能支持 Python、Nodejs 和 Rust 构建 Agent。它目前处于 alpha 阶段，尚未准备好投入生产使用。以下是它的一些特点：</p><p></p><p>从头开始构建代理运行时由 Rust 编写，开箱即支持 Python 和 Node.js构建可实际运行的代理LLM 缓存可最大限度地降低开发成本针对长时间运行的人工智能工作流进行了优化嵌入式代码解释器支持时间旅行调试</p><p></p><p>Chidori 是专注于 LLM+ 代码执行的具体操作方式，而不是提供特定的提示组合。其他框架没有关注这个领域，而这是一个重要的领域。Chidori 减少了构建长时间运行代理系统时的意外复杂性，这有助于开发人员构建成功的系统。</p><p></p><p>Chidori 是 火影忍者中卡卡西忍术的名称 ，它在日语中得名称是 Thousand Birds（千鸟），而千鸟是指一群鸟（或称为鸟群）以及由它们之间的互动产生的群体行为。千鸟是对长时间运行的代理人行为、它们内部执行的 LLM 单元以及由它们之间的互动产生的群体行为的一个很好的类比。</p><p></p><p>llm-chain[53]&nbsp;提供了一组 Rust crate，帮助开发者创建高级的 LLM 应用程序，如聊天机器人、代理等等。作为一个全面的 LLM-Ops 平台，对云端和本地托管的 LLM 都有强大的支持。还提供强大的支持，包括提示模板和多步骤链式提示的链接，使得在单个步骤中无法处理的复杂任务成为可能。还提供向量存储集成，使用户的模型能够轻松获得长期记忆和专业知识。允许开发者构建复杂的应用程序。</p><p></p><p>infino[54]，是 Rust 实现的一个可观测性平台，用于大规模存储指标和日志，并以更低的成本实现。可以集成到不同平台中，尤其是用于大模型相关基础设施和应用的可观测性。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>Rust 语言发布后，经过八年的发展，已经成为当下系统编程语言的最佳选择，目前常用于构建基础设施，包括 AI 基础设施。</p><p></p><p>Mojo 语言在 AI 领域极具潜力，但目前还未成熟，还需要很长时间来给开发者兑现承诺。</p><p></p><p>大模型时代开启，商业竞争激烈，资本推动下，Rust 将在 Mojo 成长的这段时间差内抢占一定比例的 AI 生态位。而 Mojo 目前唯一的应用很可能只是 Modular 推理引擎，这个状态和 Rust 早期与 Servo 浏览器内核共同演进的历史非常相似。</p><p></p><p>所以，短期内， Rust 和 Mojo 在各自适合的场景内逐渐发展。长期来看，Mojo 如果发展的好，就可以顺利地将 Python 生态过渡到 Mojo ，从而抢占一定的 AI 生态位。Mojo 还有一个更大的野心，就是也想成为通用语言。如果 Mojo 成熟到一定地步，那么会和 Rust 产生竞争，毕竟 Mojo 的语法相比 Rust 更好上手，但学习曲线不一定更低。在被 Rust 抢占的 AI 生态位，Mojo 也会与 Rust 进行交互融合。</p><p></p><p>以上就是我对于大模型时代编程语言的一些观点，不知道读者您有什么看法，欢迎留言讨论。</p><p></p><p></p><p></p><p>[1]Rust: <a href=\"https://rust-lang.org/\">https://rust-lang.org</a>\"</p><p>[2]《未来的人工智能的语言，是 Rust 还是 Mojo ？》: <a href=\"https://mojodojo.dev/blog/2023-07-17-rust-or-mojo-ai.html%5B3%5DOpenCV-rust:\">https://mojodojo.dev/blog/2023-07-17-rust-or-mojo-ai.html</a>\"</p><p><a href=\"https://mojodojo.dev/blog/2023-07-17-rust-or-mojo-ai.html%5B3%5DOpenCV-rust:\">[3]OpenCV-rust:</a>\" <a href=\"https://github.com/twistedfall/opencv-rust%5B4%5Dort:\">https://github.com/twistedfall/opencv-rust</a>\"</p><p><a href=\"https://github.com/twistedfall/opencv-rust%5B4%5Dort:\">[4]ort:</a>\" <a href=\"https://github.com/pykeio/ort%5B5%5Dtract-onnx:\">https://github.com/pykeio/ort</a>\"</p><p><a href=\"https://github.com/pykeio/ort%5B5%5Dtract-onnx:\">[5]tract-onnx:</a>\" <a href=\"https://github.com/sonos/tract%5B6%5Dgpu\">https://github.com/sonos/tract</a>\"</p><p><a href=\"https://github.com/sonos/tract%5B6%5Dgpu\">[6]gpu</a>\" 方言: <a href=\"https://mlir.llvm.org/docs/Dialects/GPU/%5B7%5DPolars:\">https://mlir.llvm.org/docs/Dialects/GPU/</a>\"</p><p><a href=\"https://mlir.llvm.org/docs/Dialects/GPU/%5B7%5DPolars:\">[7]Polars:</a>\" <a href=\"https://github.com/ritchie46/polars%5B8%5DApache\">https://github.com/ritchie46/polars</a>\"</p><p><a href=\"https://github.com/ritchie46/polars%5B8%5DApache\">[8]Apache</a>\" Arrow 规范: <a href=\"https://arrow.apache.org/docs/format/Columnar.html%5B9%5D%E5%AE%89%E5%85%A8\">https://arrow.apache.org/docs/format/Columnar.html</a>\"</p><p><a href=\"https://arrow.apache.org/docs/format/Columnar.html%5B9%5D%E5%AE%89%E5%85%A8\">[9]安全</a>\" Arrow2 实现: <a href=\"https://github.com/jorgecarleitao/arrow2%5B10%5Ddb-benchmark:\">https://github.com/jorgecarleitao/arrow2</a>\"</p><p><a href=\"https://github.com/jorgecarleitao/arrow2%5B10%5Ddb-benchmark:\">[10]db-benchmark:</a>\" <a href=\"https://h2oai.github.io/db-benchmark/%5B11%5DXomnia:\">https://h2oai.github.io/db-benchmark/</a>\"</p><p><a href=\"https://h2oai.github.io/db-benchmark/%5B11%5DXomnia:\">[11]Xomnia:</a>\" <a href=\"https://www.xomnia.com/%5B12%5DLinfa:\">https://www.xomnia.com/</a>\"</p><p><a href=\"https://www.xomnia.com/%5B12%5DLinfa:\">[12]Linfa:</a>\" <a href=\"https://github.com/rust-ml/linfa%5B13%5DLinfa\">https://github.com/rust-ml/linfa</a>\"</p><p><a href=\"https://github.com/rust-ml/linfa%5B13%5DLinfa\">[13]Linfa</a>\" 的中期 Roadmap: <a href=\"https://github.com/rust-ml/linfa/issues/7%5B14%5Dhttps://github.com/dimforge/nalgebra:\">https://github.com/rust-ml/linfa/issues/7</a>\"</p><p><a href=\"https://github.com/rust-ml/linfa/issues/7%5B14%5Dhttps://github.com/dimforge/nalgebra:\">[14]https://github.com/dimforge/nalgebra:</a>\" <a href=\"https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fdimforge%2Fnalgebra%5B15%5Dhttps://github.com/dimforge:\">https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fdimforge%2Fnalgebra</a>\"</p><p><a href=\"https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fdimforge%2Fnalgebra%5B15%5Dhttps://github.com/dimforge:\">[15]https://github.com/dimforge:</a>\" <a href=\"https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fdimforge%5B16%5Dcandle:\">https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fdimforge</a>\"</p><p><a href=\"https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fdimforge%5B16%5Dcandle:\">[16]candle:</a>\" <a href=\"https://github.com/huggingface/candle%5B17%5Dsafetensors:\">https://github.com/huggingface/candle</a>\"</p><p><a href=\"https://github.com/huggingface/candle%5B17%5Dsafetensors:\">[17]safetensors:</a>\" <a href=\"https://github.com/huggingface/safetensors%5B18%5Dtokenizers:\">https://github.com/huggingface/safetensors</a>\"</p><p><a href=\"https://github.com/huggingface/safetensors%5B18%5Dtokenizers:\">[18]tokenizers:</a>\" <a href=\"https://github.com/huggingface/tokenizers%5B19%5DBurn:\">https://github.com/huggingface/tokenizers</a>\"</p><p><a href=\"https://github.com/huggingface/tokenizers%5B19%5DBurn:\">[19]Burn:</a>\" <a href=\"https://github.com/burn-rs/burn%5B20%5DTorch:\">https://github.com/burn-rs/burn</a>\"</p><p><a href=\"https://github.com/burn-rs/burn%5B20%5DTorch:\">[20]Torch:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-tch%5B21%5DNdarray:\">https://github.com/burn-rs/burn/tree/main/burn-tch</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-tch%5B21%5DNdarray:\">[21]Ndarray:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-ndarray%5B22%5DWebGPU:\">https://github.com/burn-rs/burn/tree/main/burn-ndarray</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-ndarray%5B22%5DWebGPU:\">[22]WebGPU:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-wgpu%5B23%5DCandle:\">https://github.com/burn-rs/burn/tree/main/burn-wgpu</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-wgpu%5B23%5DCandle:\">[23]Candle:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-candle%5B24%5DAutodiff:\">https://github.com/burn-rs/burn/tree/main/burn-candle</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-candle%5B24%5DAutodiff:\">[24]Autodiff:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-autodiff%5B25%5DDataset:\">https://github.com/burn-rs/burn/tree/main/burn-autodiff</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-autodiff%5B25%5DDataset:\">[25]Dataset:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-dataset%5B26%5DImport:\">https://github.com/burn-rs/burn/tree/main/burn-dataset</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-dataset%5B26%5DImport:\">[26]Import:</a>\" <a href=\"https://github.com/burn-rs/burn/tree/main/burn-import%5B27%5DBurn\">https://github.com/burn-rs/burn/tree/main/burn-import</a>\"</p><p><a href=\"https://github.com/burn-rs/burn/tree/main/burn-import%5B27%5DBurn\">[27]Burn</a>\" Book: <a href=\"https://burn-rs.github.io/book/overview.html%5B28%5Dstable-diffusion-burn:\">https://burn-rs.github.io/book/overview.html</a>\"</p><p><a href=\"https://burn-rs.github.io/book/overview.html%5B28%5Dstable-diffusion-burn:\">[28]stable-diffusion-burn:</a>\" <a href=\"https://github.com/Gadersd/stable-diffusion-burn%5B29%5Dstable-diffusion-xl-burn:\">https://github.com/Gadersd/stable-diffusion-burn</a>\"</p><p><a href=\"https://github.com/Gadersd/stable-diffusion-burn%5B29%5Dstable-diffusion-xl-burn:\">[29]stable-diffusion-xl-burn:</a>\" <a href=\"https://github.com/Gadersd/stable-diffusion-xl-burn%5B30%5Dllama2-burn:\">https://github.com/Gadersd/stable-diffusion-xl-burn</a>\"</p><p><a href=\"https://github.com/Gadersd/stable-diffusion-xl-burn%5B30%5Dllama2-burn:\">[30]llama2-burn:</a>\" <a href=\"https://github.com/Gadersd/llama2-burn%5B31%5Dwhisper-burn:\">https://github.com/Gadersd/llama2-burn</a>\"</p><p><a href=\"https://github.com/Gadersd/llama2-burn%5B31%5Dwhisper-burn:\">[31]whisper-burn:</a>\" <a href=\"https://github.com/Gadersd/whisper-burn%5B32%5Dhttps://link.zhihu.com/?target=https%3A//github.com/LaurentMazare/tch-rs:\">https://github.com/Gadersd/whisper-burn</a>\"</p><p><a href=\"https://github.com/Gadersd/whisper-burn%5B32%5Dhttps://link.zhihu.com/?target=https%3A//github.com/LaurentMazare/tch-rs:\">[32]https://link.zhihu.com/?target=https%3A//github.com/LaurentMazare/tch-rs:</a>\" <a href=\"https://link.juejin.cn/?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2FLaurentMazare%2Ftch-rs%5B33%5Dtensorflow-rs:\">https://link.juejin.cn/?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fgithub.com%2FLaurentMazare%2Ftch-rs</a>\"</p><p><a href=\"https://link.juejin.cn/?target=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2FLaurentMazare%2Ftch-rs%5B33%5Dtensorflow-rs:\">[33]tensorflow-rs:</a>\" <a href=\"https://github.com/tensorflow/rust%5B34%5Ddfdx:\">https://github.com/tensorflow/rust</a>\"</p><p><a href=\"https://github.com/tensorflow/rust%5B34%5Ddfdx:\">[34]dfdx:</a>\" <a href=\"https://github.com/coreylowman/dfdx%5B35%5DEnzyme:\">https://github.com/coreylowman/dfdx</a>\"</p><p><a href=\"https://github.com/coreylowman/dfdx%5B35%5DEnzyme:\">[35]Enzyme:</a>\" <a href=\"https://github.com/EnzymeAD/Enzyme%5B36%5DEnzymeAD\">https://github.com/EnzymeAD/Enzyme</a>\"</p><p><a href=\"https://github.com/EnzymeAD/Enzyme%5B36%5DEnzymeAD\">[36]EnzymeAD</a>\" Rust 前端: <a href=\"https://github.com/EnzymeAD/rust%5B37%5Dpyo3:\">https://github.com/EnzymeAD/rust</a>\"</p><p><a href=\"https://github.com/EnzymeAD/rust%5B37%5Dpyo3:\">[37]pyo3:</a>\" <a href=\"https://github.com/PyO3/pyo3%5B38%5Dllm:\">https://github.com/PyO3/pyo3</a>\"</p><p><a href=\"https://github.com/PyO3/pyo3%5B38%5Dllm:\">[38]llm:</a>\" <a href=\"https://github.com/rustformers/llm%5B39%5DGGML:\">https://github.com/rustformers/llm</a>\"</p><p><a href=\"https://github.com/rustformers/llm%5B39%5DGGML:\">[39]GGML:</a>\" <a href=\"https://github.com/rustformers/llm/tree/main/crates/ggml%5B40%5DSonos:\">https://github.com/rustformers/llm/tree/main/crates/ggml</a>\"</p><p><a href=\"https://github.com/rustformers/llm/tree/main/crates/ggml%5B40%5DSonos:\">[40]Sonos:</a>\" <a href=\"https://tech-blog.sonos.com/about/%5B41%5Dtract:\">https://tech-blog.sonos.com/about/</a>\"</p><p><a href=\"https://tech-blog.sonos.com/about/%5B41%5Dtract:\">[41]tract:</a>\" <a href=\"https://github.com/sonos/tract%5B42%5DLLama2\">https://github.com/sonos/tract</a>\"</p><p><a href=\"https://github.com/sonos/tract%5B42%5DLLama2\">[42]LLama2</a>\" 的纯 Rust 实现: <a href=\"https://github.com/srush/llama2.rs%5B43%5DDeepgram:\">https://github.com/srush/llama2.rs</a>\"</p><p><a href=\"https://github.com/srush/llama2.rs%5B43%5DDeepgram:\">[43]Deepgram:</a>\" <a href=\"https://github.com/deepgram%5B44%5D%E4%BB%8B%E7%BB%8D%E4%BA%86%E5%85%B6%E5%B9%B3%E5%8F%B0%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8\">https://github.com/deepgram</a>\"</p><p><a href=\"https://github.com/deepgram%5B44%5D%E4%BB%8B%E7%BB%8D%E4%BA%86%E5%85%B6%E5%B9%B3%E5%8F%B0%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8\">[44]介绍了其平台为何使用</a>\" Rust 重写: <a href=\"https://deepgram.com/learn/why-deepgram-built-its-platform-in-rust%5B45%5DBlindAI:\">https://deepgram.com/learn/why-deepgram-built-its-platform-in-rust</a>\"</p><p><a href=\"https://deepgram.com/learn/why-deepgram-built-its-platform-in-rust%5B45%5DBlindAI:\">[45]BlindAI:</a>\" <a href=\"https://github.com/mithril-security/blindai%5B46%5Dtract:\">https://github.com/mithril-security/blindai</a>\"</p><p><a href=\"https://github.com/mithril-security/blindai%5B46%5Dtract:\">[46]tract:</a>\" <a href=\"https://github.com/sonos/tract%5B47%5DRust\">https://github.com/sonos/tract</a>\"</p><p><a href=\"https://github.com/sonos/tract%5B47%5DRust\">[47]Rust</a>\" SGX SDK: <a href=\"https://github.com/apache/incubator-teaclave-sgx-sdk%5B48%5Dbastionlab:\">https://github.com/apache/incubator-teaclave-sgx-sdk</a>\"</p><p><a href=\"https://github.com/apache/incubator-teaclave-sgx-sdk%5B48%5Dbastionlab:\">[48]bastionlab:</a>\" <a href=\"https://github.com/mithril-security/bastionlab%5B49%5D%E3%80%8A%E7%94%A8\">https://github.com/mithril-security/bastionlab</a>\"</p><p><a href=\"https://github.com/mithril-security/bastionlab%5B49%5D%E3%80%8A%E7%94%A8\">[49]《用</a>\" Rust 重写一个高性能的向量数据库》: <a href=\"https://www.pinecone.io/blog/rust-rewrite/%5B50%5D%E3%80%8A\">https://www.pinecone.io/blog/rust-rewrite/</a>\"</p><p><a href=\"https://www.pinecone.io/blog/rust-rewrite/%5B50%5D%E3%80%8A\">[50]《</a>\" Python 的痛苦与诗意》: <a href=\"https://www.pinecone.io/blog/pain-poetry-python/%5B51%5DQdrant:\">https://www.pinecone.io/blog/pain-poetry-python/</a>\"</p><p><a href=\"https://www.pinecone.io/blog/pain-poetry-python/%5B51%5DQdrant:\">[51]Qdrant:</a>\" <a href=\"https://qdrant.tech/%5B52%5DChidori:\">https://qdrant.tech/</a>\"</p><p><a href=\"https://qdrant.tech/%5B52%5DChidori:\">[52]Chidori:</a>\" <a href=\"https://github.com/ThousandBirdsInc/chidori%5B53%5Dllm-chain:\">https://github.com/ThousandBirdsInc/chidori</a>\"</p><p><a href=\"https://github.com/ThousandBirdsInc/chidori%5B53%5Dllm-chain:\">[53]llm-chain:</a>\" <a href=\"https://github.com/sobelio/llm-chain%5B54%5Dinfino:\">https://github.com/sobelio/llm-chain</a>\"</p><p><a href=\"https://github.com/sobelio/llm-chain%5B54%5Dinfino:\">[54]infino:</a>\" <a href=\"https://github.com/infinohq/infino\">https://github.com/infinohq/infino</a>\"</p><p></p>",
    "publish_time": "2023-09-19 17:09:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "存算分离实践：构建轻量、云中立的大数据平台",
    "url": "https://www.infoq.cn/article/jaBN4KkffQKLDEuT2HXq",
    "summary": "<p></p><blockquote>今天我们将分享社区用户多点DMALL&nbsp;的案例。多点DMALL 是亚洲领先的全渠道数字零售解决方案服务商，目前已与 380 家零售企业达成合作，覆盖 6 个国家和地区。面对 B 端客户日益增长的企业数据，存算一体的架构显得力不从心。计算资源冗余浪费、所依靠的 CDH 发行版技术栈复杂、部署运维困难及计算资源潮汐现象严重等问题，迫使多点启动架构升级的进程。同时，为满足 B 端客户多样化的需求，多点需要构建一个可以在多云环境下更具性价比、可复用的大数据底层基座和平台工具链。基于此，多点的大数据团队开始搭建存算分离的云原生大数据架构。本文深入剖析这次改造的架构设计与演进过程，分享多点DMALL 在此过程中的经验和挑战。值得一提的是，他们利用 JuiceFS 社区版实现了与 Ranger 组件进行权限的对接，希望此经验能为其他使用 JuiceFS 的企业提供参考。</blockquote><p></p><p></p><h2>一、存算一体架构下的痛点和挑战</h2><p></p><p></p><h3>1.1 架构原生存在的痛点</h3><p></p><p>存算一体架构带来的成本和运维挑战，是大部分企业在大数据发展中一定会面对的问题。</p><p></p><p>传统的 Hadoop 生态体系中，数据存储角色与计算角色通常会部署在相同的机器上，一个占据硬盘提供存储，一个利用 CPU 和内存做计算。为此，MapReduce&nbsp;和 Spark 也适应性的设计了多层级的数据本地化策略，即任务尽可能被分配到存储所需数据的对应节点上做计算，以减少中间数据交互产生的网络开销和额外的存储压力，提升整体的大数据应用效率。</p><p></p><p>可是，随着企业业务的发展，大数据存储量的增长速率与计算所需节点数量的增长速率很难保持一致。尤其是在“数据就是企业核心资产”的思想下，大量历史数据、冷数据的积累，导致企业数据存储量的增长诉求远远高于计算资源。最后企业只好不断新增机器存储更多数据，但大量计算资源得不到充分利用造成了闲置与浪费。</p><p><img src=\"https://static001.geekbang.org/infoq/49/4955676ca7ae1fe982a037d26b801361.png\" /></p><p>（同样是增加存储资源，存算一体架构下会闲置部分计算资源，存算分离则不会有这个问题）</p><p></p><p>此外，数据量的不断增长还带来了 HDFS NameNode 元数据压力、集群节点规模扩张受限等问题。这些问题也时时刻刻牵动着各个大数据团队紧绷的神经。</p><p></p><h3>1.2 多点DMALL 面临的挑战</h3><p></p><p>多点DMALL 的大数据体系在构建之初，也是采用传统&nbsp;Hadoop&nbsp;存算一体的技术栈。除了上述企业发展中架构原生带来的困境外，面对 To B 多样化的业务场景，多点DMALL 大数据团队面临更多场景化的挑战：</p><p>组件多技术栈复杂：之前主要依赖 CDH 发行版本，该套架构组件繁多，架构复杂，共包括11类服务（存储、计算、运维、监控、安全等），22 种角色类型。并且随着时间推移，很多新技术引入异常麻烦，需要考虑非常多兼容性问题。部署复杂 &amp; 运维困难：私有化部署、SaaS 服务模式一度给大数据团队带来了巨大的工作量，交付效率不高，包括网络规划、容量规划、公有云机型选择、漏洞修复和多环境日常维护等。计算资源潮汐现象严重：存算一体的架构下大数据集群和业务集群是相互独立的，资源使用有着不同的特点。大数据集群资源使用的高峰在凌晨，白天只有零散的即席查询占资源不多；业务集群的峰值在白天，晚上流量很少，这也是领域内老生常谈的“潮汐现象”，因此计算资源浪费和闲置一直没有彻底解决。</p><p></p><h2>二、存算分离的架构设计</h2><p></p><p>随着多点DMALL 全面 To B 转型，为越来越多的 B 端客户提供零售全渠道解决方案，需要具备在多云环境下提供更具性价比、可复用的大数据底层基座和平台工具链。多点DMALL 大数据团队结合已有经验和后续业务需求，设计搭建存算分离、轻量级、可扩展、云中立大数据集群架构。&nbsp;</p><p></p><p>而存算分离的第一步，便是要解决数据如何从 HDFS 集群上快速切换到云服务商存储服务的问题。</p><p></p><h3>2.1 小试牛刀：直接对接对象存储</h3><p></p><p>在架构升级探索期，能想到最直接的方案就是通过 API 对接云厂商的对象存储。</p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c81f492d93ee124e9f2b7b19ef57a24.png\" /></p><p>（原始计划：直接通过 API 对接云厂商的对象存储服务）</p><p></p><p>从架构图上看这逻辑非常简洁清晰。考虑到各大云厂商都提供了稳定的对象存储服务以及完善的API，直接加以利用应该会降低架构升级的难度。为了快速检验这一思路的可行性，我们首先选择了大数据平台上，与 HDFS 会产生交互的部分功能做切换，将其换成与对象存储进行交互的方式。</p><p></p><p>快速检验的结果是，这样的设计不仅没有达到预期，反而使大数据平台开发的复杂度成倍增加。</p><p></p><p>出现问题的核心点在于：</p><p>部分 B 端客户可能会选择自己信任/合作的云服务商，而选择的结果不可控。虽然底层都是 S3 的协议，为了构建技术壁垒，各大云服务商的对象存储 API 仍然存在一定差异。为了满足不同客户的不同云服务商需求，大数据平台工具链将需要适配开发多套代码，开发工作量巨大。</p><p><img src=\"https://static001.geekbang.org/infoq/94/94e4afd7902c175cecf0baf86bb0db8a.png\" /></p><p>（实际效果：需要使用多套 API 对接不同云厂商的对象存储服务）</p><p></p><p>经过验证，上述探索方案只能进行小型试点，无法支撑整个大数据架构的规模化调整，还需探寻新的解决方案。于是，JuiceFS 进入了我们的视线。</p><p></p><h3>2.2 JuiceFS：平滑过渡利器</h3><p></p><p>多点大数据团队很早便开始关注 JuiceFS了。在直接使用对象存储的方案宣告不可行之后，我们就一直在寻找能帮助大数据应用及引擎平滑切换到对象存储的方式。幸运的是，我们注意到了 JuiceFS 合伙人苏锐的一篇分享：<a href=\"http://mp.weixin.qq.com/s?__biz=Mzg5MjUyNjExMw==&amp;mid=2247487687&amp;idx=1&amp;sn=0cb8b390e531c7d0225f238304bc7be3&amp;chksm=c03d9b89f74a129f755cf83bdcda1e89a7ad8b2830b7f4eca24c19a7ce5d41e9485cfd064add&amp;scene=21#wechat_redirect\">从 Hadoop 到云原生， 大数据平台如何做存算分离</a>\"。而后经过不断探索与验证，我们意识到这就是一直在寻找的问题解决之道。</p><p><img src=\"https://static001.geekbang.org/infoq/9d/9db4d9f3034e09a6083cd688c47d941f.png\" /></p><p>（使用 JuiceFS 可以屏蔽底层不同云厂商的API）</p><p></p><p>采用 JuiceFS 的优势如下：</p><p>已对接市面主流公有云对象存储：为了将存储和计算剥离，对象存储是最佳选择，其本身是公有云最基础服务之一。JuiceFS 底层存储对接了市场上绝大部分云服务厂商提供的对象存储服务，可以帮助我们彻底剥离存储和计算资源，做到存算分离的效果。完美兼容 HDFS 协议，大数据引擎平滑切换：JuiceFS 提供了 Hadoop Java SDK，帮助所有使用传统的 HDFS API 的计算引擎和应用平滑切换，基本可以做到只需要修改相应的配置便可以直接执行，大大降低了新架构下引擎间调试适配的复杂性。独立元数据引擎，解决 NameNode 瓶颈问题：JuiceFS 的元数据存储在独立的存储引擎中，彻底解决了 NameNode 内存限制及单点问题。元数据引擎独立部署，对其单独的调优和运维也更加便利。没有元数据扩展的压力，集群扩张的限制也不再存在。提供 CSI 方式，支持云原生设计：在构建云原生架构的道路上，JuiceFS 提供的 Kubernetes CSI 驱动，让这个架构设计实现更加完善，在 K8s 上使用 JuiceFS 更加方便。</p><p></p><h3>2.3 最终架构设计</h3><p></p><p>以下是多点大数据最终的存算分离架构设计：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/07/0732a32569e01f5c8025f69154c6fd8f.png\" /></p><p></p><p>（多点大数据存算分离架构）&nbsp;</p><p></p><p>我们将整体架构逻辑分为以下几层：</p><p>工具层：最上层是多点大数据团队自研的 UniData 大数据平台工具链，提供完善的大数据开发治理能力，包括数据集成、数据开发、任务调度、数据资产等，实现了“用”。计算层：接下来是由 Kubernetes 管理的数据计算层，提供 Spark、Flink 等计算组件。这一层就是“存算分离”中的“算”。管控层：再下一层中，提供了除了数据计算外，元数据的存储、权限的管控、查询代理等功能，负责了架构中的“管”这一层。存储层：最后就是 JuiceFS 和各个云服务提供的对象存储，提供协议适配和加速能力，实现了“存”。在不断探索和尝试中，我们最终确定 JuiceFS 的引入和使用。JuiceFS 作为存储中间层，对下屏蔽了底层实际存储介质，隔离了不同的云环境，对上提供了统一的 HDFS API，保证了引擎执行和应用功能的一致性和稳定性，从而保障了集群整体对外服务的质量。</p><p></p><h2>三、JuiceFS 的深入运用实践</h2><p></p><p>新技术的引入总是伴随着折腾的过程。在探索和使用 JuiceFS 的过程中，多点DMALL 大数据团队不出意外地踩了一些坑，幸而最终都找到了较为合理的解决方案。在此将遇到的部分典型问题整理分享出来，希望给所有计划和正在使用 JuiceFS 的同学一些启发和帮助。</p><p></p><h3>3.1 添加基于 Ranger 的安全管控</h3><p></p><p>开源的 JuiceFS 项目中，Hadoop Java SDK 没有安全管控的功能。因此在选择使用 JuiceFS 时，安全成为我们最关注的问题。</p><p></p><p>通过对该模块代码的细致研究，参考 HDFS 的鉴权逻辑方案，我们在 JuiceFS 的 FileSystem 的实现类中，对每个API 的实现实际操作触发前都添加了权限拦截的处理。</p><p></p><p>“权限”一词的计算机语言内涵就是“实体+动作”，Ranger 的权限设计本质也是一样的。我们将拦截的对应操作（例如创建）和相关路径转化为Ranger HDFS模块所需鉴权的动作和实体，并与操作用户组合成RangerAccessRequest与 Ranger HDFS 模块打通进行鉴权。这个改动解决了 JuiceFS 在系统中“裸奔”的情况，为数据的安全做了一道防护。</p><p><img src=\"https://static001.geekbang.org/infoq/8a/8aaec2b3bf630c13363bcc03db5f05ac.png\" /></p><p>（多点大数据存算分离架构中的权限体系设计）</p><p></p><p>当然，从整体的权限体系设计来讲，考虑到 Ranger一直被人所诟病的 Ranger Admin 连接风暴和策略本地化等问题，我们设计增加了权限鉴权的代理层，来进行鉴权的分流、权限映射和缓存等。但这些架构上的优化不影响 JuiceFS 接入 Ranger 的权限管控的本质目标。除了正常的权限管控，对于可能存在的恶意使用我们也做了准备。考虑到 JuiceFS 开源代码的公开性，为了避免部分用户在了解到底层架构和引擎选择后，恶意破解调用以非法获取数据，我们对 JuiceFS 还做了额外的代码调整，包括修改核心参数的取值方式等。在保留和充分利用 JuiceFS 的核心功能前提下添加防护墙，提升整体的安全水平。</p><p></p><h3>3.2 Spark 的 Shuffle 数据处理</h3><p></p><p>在 Spark on K8s 的云原生设计中，Shuffle 数据的处理是需要重点关注的。相比于通过机器堆出来的 YARN 集群可以直接利用超大的本地磁盘存储 Shuffle 数据而言，试图避免依赖底层机器、存算分离设计下 K8s 上的任务只能另谋他路。</p><p></p><p>在看到 JuiceFS 提供的的 K8s CSI 驱动时，我们最初以此作为突破点。在设想中，可以利用 JuiceFS K8s CSI 驱动的 writeback 模式，Shuffle 数据先放置临时存储目录，超过阈值后载入远端对象存储中。这样的逻辑下，Spark 的Shuffle 数据就无需依赖本地机器磁盘大小，有海量对象存储作为最终存储介质，理论上不再担心执行压力和数据临时存储压力。</p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b8fa824a564320564d35a9c98718bf3.png\" /></p><p>（使用 JuiceFS CSI 驱动的 writeback 特性进行 Spark Shuffle 数据处理）</p><p></p><p>但经过实际验证，在进行on YARN 和 on K8s 的性能测试对比时发现，使用这个方案的实际效果是：慢得不止一点点。</p><p></p><p>以下为测试中最典型的一个Query结果：</p><p><img src=\"https://static001.geekbang.org/infoq/33/33753923f3c80526193d0471ffe652eb.png\" /></p><p>（性能测试中，一个典型 TPC-DS SQL 的执行时长对比结果）</p><p></p><p>在深入分析研究后，我们发现 Shuffle 场景本身会存在大量小文件及随机读操作，JuiceFS K8s CSI 并不适合这种场景，会产生较大的性能瓶颈。在与 JuiceFS 社区沟通探讨后，我们开始调研开源的 Remote Shuffle Service，将 CSI 的方式切换为利用独立的 Shuffle 服务，并根据测试最终选用了 Apache Celeborn（后简称Celeborn）支持这一场景，其整体性能表现跟 on YARN 差异不大。Apache Celeborn 自身支持分级存储能力，极大提升了各类实际负载适配能力，我们将 JuiceFS 作为内存/磁盘容量不足情况下最后的兜底 Shuffle 数据存储。</p><p></p><h3>3.3 Alpine 镜像问题</h3><p></p><p>上文提到，我们有一些大数据平台应用是直接通过 Hadoop Java SDK 与 HDFS 进行交互的。这些应用都是 Java 应用，在云原生的转换过程中发现以下报错：</p><p>initial-exec TLS resolves to dynamic definition in /tmp/libjfs-amd64.7.so</p><p></p><p>经过探索，并与 JuiceFS 社区沟通后，我们发现这个问题，是由于使用的基础镜像&nbsp;openjdk-Alpine 本身的 bug[1]，后来我们换成了eclipse-temurin 解决了问题。</p><p></p><h3>3.4 root 不会被设置为 Owner</h3><p></p><p>在 Celeborn 的使用中发现一个漏洞。Celeborn 会自动创建其存储 Shuffle 数据的 HDFS 目录，当该服务的启动用户是 root 时，自动创建的 HDFS 目录并没有被自动设置 Owner 为 root。在上面提到的我们模仿 HDFS 鉴权思路中，对于一些目录的操作会去校验是否是这个目录的 Owner。root 没有被设置，自然后续 Celeborn 很多针对这个 HDFS 目录的操作都会被权限拦截。</p><p></p><p>虽然我们可以通过切换 Celeborn 的启动用户，或者给他单独设置权限等方式绕过这个拦截。考虑到创建后设置Owner 是合理行为，而且除了 root 外的其他用户都会被正常设置，我们还是将这个疑问向 JuiceFS 社区提出来。感谢 JuiceFS 社区第一时间的响应和支持，很快就修复了这个小漏洞。</p><p></p><h3>3.5 数据缓存运用与 OOM 问题</h3><p></p><p>将 CSI 驱动切换成 Celeborn 后，我们又一次开始做 Spark on YARN 和 on K8s 的性能测试。对比中发现，相同的任务和资源，on K8s 的任务总是会报错 OOM。通过细致的 Spark 内存分析，并不断对比多个环境任务和差异点后，团队内一位同学发现了 JuiceFS 的数据缓存参数设置区别，深入挖掘，最终找到答案。</p><p></p><p>Spark 任务执行时，需要特别配置juicefs.cache-dir，不然 JuiceFS 就会默认将数据缓存放进内存中，从而对每一个 executor 多出好几百兆的额外内存占用。如果不做特殊配置，那就需要在 Spark 任务切换到on K8s的环境时，多配一些 off-heap 堆外内存，用以支持 JuiceFS 的额外数据缓存。</p><p></p><h3>3.6 数据缓存目录的权限</h3><p></p><p>在使用数据缓存目录（后简称 cache 目录）的应用中，我们还遇到了另一个问题。Spark on K8s 的 Jupyter 应用中我们使用 JuiceFS K8s CSI 驱动建立的 PVC，与使用 JuiceFS Hadoop Java SDK 挂载 cache 目录，当二者使用同一个目录，会产生权限冲突的问题，在 Spark 运行日志中出现 warn 日志无法落地/获取缓存数据。仔细跟踪后发现，是因为两条链路生成的缓存文件目录默认权限不同，相互修改权限最终导致了文件写入失败，这样相当于根本没利用上 JuiceFS 客户端缓存，每次都直接与对象存储交互，这样对 Spark 任务性能而言影响很大。</p><p></p><p>该问题在反馈给 JuiceFS 社区后，社区通过对 JuiceFS K8s CSI 驱动增加参数“cache-mode”进行了修复。</p><p></p><h3>3.7 TiKV &amp; Write Conflict</h3><p></p><p>在做容量规划的时候考虑到线上集群规模，TiKV 一开始就被我们选择为 JuiceFS 的元数据存储引擎。在我们对云原生架构开发测试的大部分时间内，TiKV 的表现一直很稳定，直到我们选择 Celeborn 作为独立 Shuffle 服务。</p><p></p><p>根据 Celeborn 的功能设计，在当本地磁盘存储 Shuffle 数据满时，将把数据下推到 HDFS 中（当然我们在这里利用 JuiceFS 让其实际下推到了对象存储）。但在具体测试时发现，多个 Celeborn 的 Worker 同时写一个 JuiceFS 的目录会出现 Write Conflict 问题并触发重试操作。重试操作会有次数极限，而且不断重试很明显降低了整体Shuffle 效率延长了任务执行时长，在很长一段时间内这个问题也困扰着我们。</p><p></p><p>最终，社区的另一位 JuiceFS 用户给出了方案。Write Conflict 的根本原因是所有的写文件都要修改父目录的更新时间，这个报错并非是因为写文件，而是修改同一个目录属性产生的异常。再进一步，产生 Write Conflict 的不是JuiceFS 管理的数据，而是元数据，也就是 TiKV 的锁问题。最终，考虑到除了 Shuffle 场景，这样高并发的修改同一个目录的属性并不常见，我们决定为 Celeborn 部署提供单独的 JuiceFS 的 Hadoop Java SDK，这个 SDK 是单独处理的，写数据不再更新父目录的属性。</p><p></p><h3>3.8 TiKV 垃圾回收机制</h3><p></p><p>加入 JuiceFS 社区群后，我们也时常关注群内其他企业使用的问题反馈，可以帮助我们在正式上线前覆盖更多的测试案例。TiKV 的垃圾回收机制问题就是其中一个。当看到群里有其他同学反馈后，我们快速分析了该问题发生的原因，并检查补充了部署策略。TiKV 的独立服务并不会自动触发垃圾回收机制，只有同步安装 TiDB 这个组件才会正常运转。而我们在元信息服务 TiKV 部署策略中会同步安装 TiDB，不会遇到这个问题。另外，JuiceFS 1.0.4 版本开始已经新增 TiKV gc worker 后台线程适时触发垃圾回收动作。</p><p></p><h3>3.9 HDFS 回收站文件无法清理</h3><p></p><p>当 HDFS 配置文件中开启了 HDFS 回收站功能（fs.trash.interval&nbsp;和&nbsp;fs.trash.checkpoint.interval），只要存活的客户端实例都会检查并触发回收站中文件清理工作。但是最开始我们测试发现，清理线程总是报错提示没有文件操作权限。跟 JuiceFS 社区沟通后发现，的确存在 bug 导致过期文件没法清理，并迅速提供了&nbsp;PR[2]&nbsp;&nbsp;修复。</p><p></p><h2>四、最终测试结果</h2><p></p><p>正如上文中提到的，我们在架构升级过程中多次在公司开发环境进行了 Spark on YARN 和 on K8s 的性能测试对比，分别执行多次 TPC-DS SQL。以下为最终的对比结果：</p><p><img src=\"https://static001.geekbang.org/infoq/29/298108f63c4dacac9da566864a2b6113.png\" /></p><p>（基于TPC-DS SQL的 on Yarn 和 on K8s 性能测试对比最终结果）</p><p></p><p>上述测试是通过大数据平台 UniData 配置任务进行数据计算对比，变量包含平台调度策略的调整、Spark 版本升级等。排除其他变量，深入分析时间差异后，我们得出以下结论：</p><p>Spark 任务基于 HDFS 的on YARN 执行时长与基于 JuiceFS 的 on K8s 执行时长基本持平，性能差异较小。JuiceFS 的数据缓存设计对数据查询存在明显的加速作用，同样的 SQL 在多次执行后，执行速度明显提升。JuiceFS 会占用部分内存，总体而言比基于 HDFS 的任务所需内存更多。</p><p></p><p>从上述测试结果来看，已经达到了我们新架构正式上线的要求。目前这套架构已在多个公有云环境中平稳运转，接下来我们会启动现有历史 CDH 存算一体集群下线，并升级为新的存算分离新架构的动作。另外，为进一步提升Spark 执行性能，我们也在积极开展引入向量化执行引擎框架 Gluten 的测试验证工作。</p><p></p><h2>五、小结</h2><p></p><p>在多点DMALL 从传统 Hadoop 存算一体到存算分离的升级过程中，JuiceFS 的出现填补了存储设计的空缺，推动了升级闭环。它对上保持了同样的 HDFS 协议，降低各个应用和引擎适配复杂度，对下完美对接各个云服务厂商提供的对象存储服务，提升了整体架构的升级效率。</p><p></p><p>经过整体向云原生的存算分离架构的升级，我们获得了多方面的收益：</p><p>节约成本：存算分离可以为企业客户节约大量硬件或云服务商的成本，从而提升客户满意度，这也推动了我们服务续约率的提升。技术扩展性好：我们之前使用 CDH 发行版进行组件的管理，因为引擎间版本限制，和重要组件升级带来的风险高等问题，客户有些技术升级诉求无法响应。存算分离后我们也摆脱了这个限制。现在，我们可以针对性地升级和调试单一组件，甚至在同一集群内进行AB测试，大大降低了升级风险。部署和运维效率提升：升级前我们的交付最快只能达到天级，这还不算前期的集群设计和准备工作。现在可以达到小时级，资源是按需使用的，随用随取，没有之前那些复杂的预投入，大数据平台一键拉起，释放了大量人力成本。我们很幸运在整体架构升级的过程中遇到了 JuiceFS 这个项目，也希望通过这篇实践分享能帮助到更多的企业更好的运用 JuiceFS。未来我们也会持续关注 JuiceFS 社区，持续为社区建设做出更多的贡献。</p><p></p><p>作者简介：</p><p>李铭，多点DMALL 数据平台团队高级研发工程师，目前负责公司大数据云原生架构设计与数据基座新特性研究；研究领域为大数据统一 SQL 网关、分布式文件存储、高性能计算、数据安全等。DataFun 技术社区年度星级志愿者，大数据开源社区爱好者，重点关注多个开源项目 Apache Kyuubi、JuiceFS、Apache Celeborn、Trino等在司内的适配和应用。</p><p>多点DMALL 成立于 2015 年，提供一站式全渠道数字零售解决方案 DMALL OS，目前已与 380 家零售企业、近 1000 家品牌达成合作，覆盖 6 个国家和地区。作为亚洲领先的全渠道数字零售解决方案服务商，多点DMALL 通过数字化解构重构零售产业，提供端到端的商业 SaaS服务。</p><p></p><p>引用链接</p><p>[1]&nbsp;openjdk-Alpine 本身的 bug:&nbsp;https://github.com/iegomez/mosquitto-go-auth/issues/20</p><p>[2]&nbsp;PR:&nbsp;https://github.com/juicedata/juicefs/pull/2512</p>",
    "publish_time": "2023-09-19 17:19:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023 数字化转型发展大会暨首届数字原生大会在京召开",
    "url": "https://www.infoq.cn/article/OZqybvKCTNF0a0fJbAey",
    "summary": "<p>2023 年 9 月 13 日 -14 日，由中国信息通信研究院（简称“中国信通院”）和中国通信标准化协会联合主办的“2023 <a href=\"https://www.infoq.cn/article/Digital-Transformation-Guide-1\">数字化转型</a>\"发展大会暨首届数字原生大会”在北京召开。本届论坛以“数跨新阶 原生新纪”为主题，成立<a href=\"https://www.infoq.cn/article/TggzC1PbadxdDozk0C7B\">数字原生</a>\"推进方阵（DNA），发布何宝宏博士新书《数字原生》，公布第二届“鼎新杯”数字化转型应用大赛案例结果，公布 2023 年上半年政企数字化转型最新评估结果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9cf695d9968b1da573252a8391d3f590.png\" /></p><p></p><p>工业和信息化部信息通信发展司政策标准处处长陆洋、中国通信标准化协会理事长闻库、中国通信企业协会副会长兼秘书长赵中新、国务院国资委信息中心原主任王绪君、中国信通院院长余晓晖出席会议并致辞。开幕式由中国信通院云计算与大数据研究所所长何宝宏主持。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/91/91a0bea52bc08aa6861ae9f1e362edfc.jpeg\" /></p><p>工业和信息化部信息通信发展司政策标准处处长陆洋致辞</p><p></p><p>陆洋表示，数字经济已成为全球经济增长的新动能，当前，我国数字化转型发展顶层政策环境持续优化、基础设施水平加快提升、产业生态体系不断完善。未来建议一是提升数字基建质量，二是加快核心技术突破，三是强化标准规范建设。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df3a193ca0536383f0c3162c6e8880ce.jpeg\" /></p><p>中国通信标准化协会理事长闻库致辞</p><p></p><p>闻库表示，我国数字经济规模正不断扩大，在国民经济中发挥作用。数字经济的顶层政策环境不断完善，技术创新活力不断并发，产业模式不断创新，转型标准体系不断完善。他建议，一是加速推进数字化政策的精准落地，二是大力促进多领域企业的交流与合作， 三是充分发挥标准的牵引作用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f0f4cc81b5545dd00f2ba03129ceb17.jpeg\" /></p><p>中国通信企业协会副会长兼秘书长赵中新致辞</p><p></p><p>赵中新表示，数字化转型已经成为社会的主旋律，打造一批存在推广应用价值的数字化转型应用案例具有重要意义。应继续加大“鼎新杯”案例征集活动宣传推广，推动转型走深走实。展望未来，他认为一是持续推进信息通信行业赋能社会数字化转型发展，二是持续加强信息基础设施建设，三是强化赋能其他行业产供销协同。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d752d46c6f99d432057a99dbc9eb18ca.jpeg\" /></p><p>国务院国资委信息中心原主任王绪君致辞</p><p></p><p>王绪君表示，近年来国有企业持续开展数字化转型工作，数字化转型工作体制基本形成，新技术深化创新攻关，数字技术服务能力有序提升。他建议，一是持续优化政策环境，二是切实推动标准建设，三是持续打造行业数字化转型示范样本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2501b2763c95b0fcae2699ac31a47f4.jpeg\" /></p><p>中国信息通信研究院院长余晓晖致辞</p><p></p><p>余晓晖表示，数字化转型与过去几十年已然发生的全球信息化浪潮一脉相承，都是信息通信技术驱动的范式变革，但又呈现全面互联、数据驱动、软件定义、平台支撑、全局智能等新的特征，而数字原生成为数字化的新阶段和重要方向。他提到，预计未来几年，数字化转型模式与价值将更为清晰，数字化转型的生产方式、业务形态、产业组织方式、商业模式、创新范式、技术架构都将发生转变。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f38009241e18688b462e2395c6d9b549.jpeg\" /></p><p>数字原生推进方阵（DNA）成立仪式</p><p></p><p>数字经济时代，一批具备“数字原生”基因的企业和组织应运而生，它们将数字思维、数字技术融入生产经营的各个环节，通过“数字原生”理念，重塑企业文化、组织架构、技术能力和经营模式，实现核心价值提升与商业模式创新。为促进数字技术与实体经济深度融合，让更多传统企业理解“数字原生”理念，中国信通院联合产业各方，正式发起成立“数字原生推进方阵（DNA）”，搭建数字原生企业对话平台，推动数字原生理念发展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2eeb8a4cbf0f04332d15a295ae24e3e1.jpeg\" /></p><p>中国信息化百人会执委安筱鹏主旨演讲</p><p></p><p>中国信息化百人会执委安筱鹏发表题为《什么是数字原生?》的主旨演讲，剖析了数字原生出现的机理机制，并指出，数字原生企业的四个特征，即数字原生 = 客户运营商 + 数据运营商 + 进化型组织 + 长在云端。安筱鹏指出数字原生的关键在于构建基于数字技术底座的业务架构和组织架构，打造扁平化协同的进化型组织。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/597f4129be5d051ce0f0e9e55effbe0d.jpeg\" /></p><p>中国信通院云计算与大数据研究所所长何宝宏博士主题演讲</p><p></p><p>中国信通院云计算与大数据研究所所长何宝宏博士发表题为《转型的终点：数字原生》的主题演讲，从技术应用的生命周期，电力化转型与原生的视角切入，围绕软件、数据、算力、AI、Web3、元宇宙等技术和应用展开，深入分析了数字原生正在和即将带来的机遇和挑战，描绘了数字原生时代的场景，为更多企业和行业加快数字化转型，制定面向数字原生时代的战略规划提供了参考。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df14e4271bd5d8ec118692aec195c873.jpeg\" /></p><p>《数字原生》新书发布仪式</p><p></p><p>大会举行了何宝宏所长《数字原生》新书发布仪式。中国通信标准化协会理事长闻库，中译出版社社长乔卫兵、副总编辑刘永淳，中国信通院云计算与大数据研究所所长何宝宏共同出席新书发布环节。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/695a55b3d64f19e938c76cc9e540ec96.jpeg\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/49a5ac9b1fe45cce41e8fe4548e2fd2f.jpeg\" /></p><p></p><p>大会正式公布第二届“鼎新杯”数字化转型应用大赛标杆案例。本次“鼎新杯”数字化转型应用大赛以“数字扬帆，鼎新引领”为主题，设置了 6 大方向 16 个赛道，遴选和展示了一批极具代表性的数字化转型应用案例，为促进数字技术与业务融合创新，加速产业数字化发展，推动数字经济高质量发展注入了强劲动力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/16147e6eaaacf1953257ad4b14dd712b.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f823de8eef9e40862aa528b60e09454f.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/606c5666ef281993b92f9eda9fe8ef96.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/64/64a9cbcf1d04bd793d4d63f640104db0.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e4fc8d79eda9423e83ea79d80c948947.png\" /></p><p></p><p>大会发布了 2023 年上半年政企数字化转型 IOMM 最新评估结果，涵盖企业数字化转型、数字政府、云边端数字化三大类评估体系。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1a/1a3395ccf66b3cbd8eb338942438d93f.jpeg\" /></p><p>央国企高质量数字化发展赋能计划启动仪式</p><p></p><p>会上，中国信通院云计算与大数据研究所所长何宝宏、中国电信集团政企信息服务事业群处长高扬、联通数字科技有限公司战略客户服务部总经理王涛、金蝶软件（中国）有限公司副总裁张振坤、阿里云智能集团战略发展副总裁逢孝钢、华为云央国企行业总经理龚岳院、腾讯云能源行业副总经理吴丽颖、浪潮云企业云事业部副总经理练飞、百度智能云产业发展生态合作总监石乔共同启动“央国企高质量数字化发展赋能计划”，旨在为央国企高质量数字化转型提供有力支撑和实际落地指导，同时整合行业优质赋能资源，形成推动央国企数字化转型落地的有效方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2ec30ddcc5e3d9f49aa8fe8c88892e43.jpeg\" /></p><p>中国信通院云计算与大数据研究所副所长栗蔚发表演讲</p><p></p><p>会上，中国信通院云计算与大数据研究所副所长栗蔚发表题为《中国数字化转型数实融合 IOMM 综合指数（2023 年）》的演讲。她表示随着国家战略的高度重视以及数字经济规模再创新高，数实融合成为数字经济发展的核心任务。中国信通院聚焦数实融合，深入研究不同行业、不同企业，以及典型行业的地域特征，得出数实融合 IOMM 综合指数，为进一步揭示数实融合发展趋势，评价我国重点行业及其代表性企业的数字化发展水平、转型成效提供了有益参考。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8ea119d12c260d84966f9c599c3de182.jpeg\" /></p><p>中国信通院云计算与大数据研究所政企数字化转型部主任徐恩庆发表演讲</p><p></p><p>中国信通院云计算与大数据研究所政企数字化转型部主任徐恩庆带来《以 IOMM 方法论推动政企数字化转型》主题分享。徐恩庆指出，数字化转型正在成为推动经济高质量发展的重要途径，数实融合已成为国家重点战略之一，中国信通院深入研究数字化转型趋势及方法论，形成了覆盖企业整体转型、业务数字化、数字化平台、数字基础设施、数字政府、信创等的数字化转型方法论体系，并通过构建产业平台、标准评估等为推进我国数字化发展持续赋能。</p><p></p><p>发布仪式</p><p><img src=\"https://static001.geekbang.org/infoq/2c/2ca7588bf8999373163e9062e97d50fa.jpeg\" /></p><p></p><p>会上还发布了多项企业数字化、数字政府、政企信创最新研究成果，包括《流程挖掘行业发展报告（2023）》、《低代码无代码产业图谱》、《组装式发展白皮书》、《数字化供应链赋能产业链韧性协同发展（2023）》报告、《汽车金融行业数字化转型白皮书》、《一城一云服务城市高质量发展白皮书》、《政务大模型建设路径及评价体系研究报告》、《数字政府建设与发展研究报告（2023 年）》、《音视频媒体处理平台技术能力要求》，启动了《企业流程治理研究报告（2023）》、组装式系列标准、SEPT 信创标准化评估平台、信创 ERP 质效提升专项计划，并进行了数字政府签约仪式。</p><p>先锋人物结果公布</p><p><img src=\"https://static001.geekbang.org/infoq/d0/d0510513d83815c524dad3bee761e437.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7ccef86e55ec46084dce3405e058bb36.png\" /></p><p>大会公布了 2023 企业数字化发展共建共享平台（EDCC）先锋人物、2023 数字政府建设赋能计划先锋人物。</p><p></p><p>主题演讲环节</p><p><img src=\"https://static001.geekbang.org/infoq/cd/cd059e475ea023ccb9d6905e94191d61.jpeg\" /></p><p></p><p>大会同时邀请了产业各方嘉宾发表精彩演讲。上午环节，招商局集团数字化中心技术管理处处长山金孝发表《数字化助力构建“两条曲线”，推动“三次创业”》主题演讲；中国电信集团有限公司全渠道运营中心总经理助理魏丫丫发表《指针：中国电信新零售数字化实践》主题演讲；华为云计算技术有限公司华为云城市云总经理赵永明发表《一城一云，共助城市数字化“智理”》主题演讲；中国联合网络通信集团有限公司数字化部总监杜宇发表《创新基因谋发展，转型升级谱新篇——中国联通数字化转型与创新实践》主题演讲；金蝶软件（中国）有限公司金蝶中国北京研发中心总经理汪东海发表《筑梦苍穹, 智启未来》主题演讲。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/3180c19b31834f76e8e0a43defa093be.jpeg\" /></p><p></p><p>大会下午环节，北京神州泰岳软件股份有限公司运营商 BG 产品副总裁兼数字化运营事业部总经理刘鹏发表《跃升数字化生产力，领航 ICT 运营管理新范式》主题演讲，长城汽车 IDC 产品总监张松发表《长城汽车采用 PBC 组装式架构搭建新一代业务中台》主题演讲，宿迁市大数据中心数据应用处处长周晓莉发表《宿迁市云数融合创新赋能数字政府建设》主题演讲，中国银行软件中心高级经理伊纯发表《银行数字化转型应用实践探索与讨论》主题演讲，中国移动云能力中心 / 中移（苏州）软件技术有限公司云原生负责人李莉发表《移动云星罗云原生底座，构建数字原生新基础设施》主题演讲，北京北投智慧城市科技有限公司总经理刘春凤发表《数据驱动的学习型智慧建筑大脑 ——北投大厦项目》主题演讲，重庆市两江协同创新区建设投资发展有限公司信息中心主任罗克发表《数字化变革下的协同与创新》主题演讲。</p><p></p><p>“2023 数字化转型发展大会暨首届数字原生大会”历时两天，围绕数字化转型发展多个热点议题和前沿领域展开深入讨论，设立 11 个主题分论坛，包括：数字原生、云上数字政府、央国企信创与信创产业发展、大模型时代智慧政务发展新机遇、政务信创、信创 ERP 与经营管理数字化、数字化流程变革创新、企业数字化能力智慧运营、数字化安全生产与信创运维实践、低无代码与组装式技术应用、数字基础设施等，从不同角度、不同应用方向、不同技术特点为参会者呈现数字化转型发展相关的前沿思维、应用实践与技术创新情况。&nbsp;&nbsp;</p>",
    "publish_time": "2023-09-19 17:52:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国数字化转型数实融合IOMM综合指数（2023年）解读",
    "url": "https://www.infoq.cn/article/7JyRlrx8m8rfYcfdPrX8",
    "summary": "<p>中国信通院聚焦数实融合，从宏观的经济与产业趋势洞察，到微观的业务与技术融合方法论总结实践，不断深化研究成果。继2021年发布“企业数字化转型发展双曲线”，2022年推出“企业数字化转型发展双象限”之后，在2023年进一步提出《中国数字化转型数实融合IOMM综合指数》，以评价我国重点行业及其代表性企业的数字化发展水平与转型成效。</p>",
    "publish_time": "2023-09-19 18:01:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国信通院发布《中国数字化转型数实融合IOMM综合指数（2023)》，展现重点行业与企业数字化发展特征",
    "url": "https://www.infoq.cn/article/6Nkph4VA1LXRfLFKAybZ",
    "summary": "<p>2023年9月13日-14日，由中国信息通信研究院（简称“中国信通院”）和中国通信标准化协会联合主办的“2023数字化转型发展大会暨首届数字原生大会”在北京召开。在14日上午的主论坛上，中国信通院云计算与大数据研究所副所长栗蔚对《<a href=\"https://www.infoq.cn/minibook/7JyRlrx8m8rfYcfdPrX8\">中国数字化转型数实融合IOMM综合指数（2023年）</a>\"》进行解读。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/37/37aad5bedcc916ac63907c57f950d690.jpeg\" /></p><p></p><p>栗蔚指出，随着数字经济在我国经济发展中所占比重及增速连年攀升，其作为经济“稳定器”与“加速器”的作用也愈加凸显。2022年我国数字经济规模达50.2万亿元，占GDP比重41.5%，并已连续11年显著高于同期GDP名义增速。而数字经济与实体经济的融合（简称“数实融合”）成为数字经济发展的核心任务，在国家战略性文件中被多次强调。</p><p></p><p>中国信通院聚焦数实融合，从宏观的经济与产业趋势洞察，到微观的业务与技术融合方法论总结实践，不断深化研究成果。继2021年发布“企业数字化转型发展双曲线”，2022年推出“企业数字化转型发展双象限”之后，在2023年进一步提出《中国数字化转型数实融合IOMM综合指数》，以评价我国重点行业及其代表性企业的数字化发展水平与转型成效，指数分为“企业数字化转型IOMM综合评价指数”（ 简称“企业指数”）和“行业数字化转型IOMM综合评价指数”（ 简称“行业指数”）两部分。</p><p></p><p>栗蔚表示，通过以上指数能更加直观的呈现出不同企业及相关重点行业的整体数字化发展水平。在企业指数中，三大运营商的新兴业务增速明显，2023上半年同比增长19.2%，同时，以招商局集团、五矿集团、南方电网、国家电网等为代表的大型央国企，通过转型升级来提升核心竞争力，实现自身能力跃迁与运营模式创新。在行业指数中，通信行业持续领跑，积极践行数字中国战略；金融行业稳中提速，强化风险管理，推动普惠金融；制造业发展步伐加快，柔性生产、无人工厂、数字孪生产线等场景落地；医疗行业提升迅速，医疗信息系统建设加快，技术与医疗服务的结合更加紧密。</p><p></p><p>栗蔚在演讲中进一步分析了通信、金融、汽车制造、医疗等典型行业在地域发展上的特点，并对过去一年中重点行业的主要赋能型企业进行了分析，以技术服务能力和行业赋能价值作为衡量维度，分别对处于“引领者”、“专注者”、“竞争者”及“创新者”象限中的赋能企业特点进行了总结。</p><p></p><p><a href=\"https://www.infoq.cn/minibook/7JyRlrx8m8rfYcfdPrX8\">点击链接</a>\"即可下载《中国数字化转型数实融合IOMM综合指数（2023年）解读》PPT全文</p>",
    "publish_time": "2023-09-19 18:02:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]