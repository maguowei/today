[
  {
    "title": "从HBase到TiDB：我们如何实现零停机在线数据迁移",
    "url": "https://www.infoq.cn/article/2qQNTBGeLLbyQFt9cm7H",
    "summary": "<p></p><blockquote>本文最初发布于<a href=\"https://medium.com/@Pinterest_Engineering?source=post_page-----43f0fb474b84--------------------------------\">Pinterest Engineering</a>\"技术博客，InfoQ经翻译授权发布。</blockquote><p></p><p></p><h2>迁移背景与动机</h2><p></p><p>在Pinterest，Hbase一直是我们最关键的存储后端之一，持续为众多线上存储服务提供支持，涵盖Zen（图数据库）、UMS（宽列数据存储）和Ixia（近实时二级索引服务）。HBase生态系统具备一系列突出优势，例如在大容量请求中保障行级强一致性、灵活的模式选项、低延迟数据访问、<a href=\"https://www.infoq.cn/topic/Hadoop-ten-years\">Hadoop</a>\"集成等，但也由于运营成本高、过于复杂和缺少二级索引/事务支持等问题，而明显无法满足未来三到五年内的客户实际需求。</p><p></p><p>在评估了十余种不同的存储后端选项，向三种入围方案导入影子流量（将生产流量异步复制至非生产环境）并开展深入性能评估之后，我们最终决定将TiDB选为这场统一存储服务角逐的胜出者。</p><p></p><p>如何将统一存储服务的支持职责顺利移交给<a href=\"https://www.infoq.cn/profile/E47A66A843E94F/publish\">TiDB</a>\"，无疑是一项需要数个季度才能完成的艰难挑战。我们需要将数据从HBase迁移至TiDB，重新设计并实现统一存储服务，将Ixia/Zen/UMS的API迁移至统一存储服务，再把各类离线作业由HBase/Hadoop生态系统迁移至TiSpark生态系统——而且整个过程中，现有业务的可用性和延迟SLA均不能受到影响。</p><p></p><p>在本篇博文中，我们将首先探讨不同数据迁移方法及其各自利弊。之后，我们再深入探究如何将数据从HBase迁移至TiDB，这也是Pinterest第一次以零停机方式迁移一个每秒14000次读取查询、400次写入查询的4&nbsp;TB大表。最后，我们将共同验证这套新方案能否实现99.999%的数据一致性，并了解如何衡量两个表之间的数据一致性。</p><p></p><h1>数据迁移策略</h1><p></p><p>一般来讲，零停机时间数据迁移的实施策略可以概括为以下几点：</p><p>假定已有数据库A，需要将数据迁移至数据库B，则首先开始对数据库A和B进行双重写入。将数据库A的转储数据导入至数据库B，并解决与实时写入间的冲突。对两套数据集进行验证。停止向数据库A写入。</p><p></p><p>当然，具体用例肯定各有不同，所以实际项目中往往会包含一些独特的挑战。</p><p></p><p>我们综合考量了各种数据迁移方法，并通过以下权衡筛选最适合我们需求的选项：</p><p>从服务向两个表（HBase和TiDB）执行双重写入（以同步/异步方式写入2个数据源），并在Lightning中使用TiDB后端模式进行数据摄取。</p><p></p><p>这种方式最简单也最易行。但TiDB后端模式提供的传输速率仅为每小时50 GB，因此只适合对较小的表进行数据迁移。</p><p></p><p>获取HBase表的快照转储，并将来自HBase&nbsp;cdc（变更数据捕捉）的数据流实时写入至kafka主题，而后使用lightning工具中的本地模式对该转储执行数据摄取。接下来，即可从服务层执行双重写入，并应用来自kafka主题的全部更新。</p><p></p><p>应用cdc更新时往往会引发复杂冲突，因此这种方法的实施难度较高。另外，我们此前负责捕捉HBase cdc的自制工具只能存储键，所以还需要额外的开发工作才能满足需求。</p><p></p><p>另一种替代方案，就是直接从cdc中读取键，并将存储在另一数据存储内。接下来，在面向两个表的双重写入启动后，我们从数据源（HBase）读取各键的最新值并写入TiDB。这种方法实施难度很低，不过一旦通过cdc存储各键的异步路径发生可用性故障，则可能引发更新丢失风险。</p><p></p><p>在评估了各项策略的利弊优劣之后，我们决定采取下面这种更加稳妥可靠的方法。</p><p></p><h2>迁移工作流</h2><p></p><p></p><h3>术语定义：</h3><p></p><p>客户端：与thrift服务对话的下游服务/库</p><p>服务：用于支持在线流量的thrift服务；在本次迁移用例中，服务指的是Ixia</p><p>MR Job：在map reduce框架上运行的应用程序</p><p>异步写入：服务向客户端返回OK响应，无需等待数据库响应</p><p>同步写入：仅在收到数据库响应后，服务才向客户端返回响应</p><p>双重写入：服务以同步或异步方式同时写入两个基础表</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/adc11aa8d11574d2509cf01a5d7cbb10.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff7aade57742bdafb323bd28594b09c7.png\" /></p><p></p><h3>实施细节</h3><p></p><p>由于HBase为无模式（schemaless），而TiDB使用严格模式，因此在着手迁移之前，需要先设计一个包含正确数据类型和索引的schema。对于我们这个4&nbsp;TB大小的表，HBase和TiDB&nbsp;schema之间为1：1映射，也就是说TiDB架构会通过map reduce作业来分析HBase行中的所有列和最大列大小，之后再分析查询以创建正确的索引。下面来看具体步骤：</p><p>我们使用hbasesnapshotmanager获取HBase快照，并将其以csv格式存储在S3内。各CSV行使用Base64编码，以解决特殊字符受限的问题。接下来，我们在本地模式下使用TiDB&nbsp;lightning对这一csv转储执行摄取，而后进行base64解码，并将行存储至TiDB内。摄取完成且TiDB表上线后，即可启动对TiDB的异步双写。异步双写能够既保障TiDB的SLA，又不影响服务SLA。虽然我们也为TiDB设置了监控和分页，但此时TiDB仍然以影子模式运行。使用map reduce作业对HBase和TiDB表执行快照保存。各行会首先被转换为一个通用对象，再以SequenceFiles的形式存储在S3内。我们使用MR Connector开发了一款自定义TiDB快照管理器，并在HBase中使用hbasesnapshotmanager。使用map reduce作业读取各sequencefiles，再将不匹配的行写回至S3。从S3中读取这些不匹配的行，从服务中读取其最新值（来自HBase），再将这些值写入至辅数据库（TiDB）。启用双重同步写入，同时向HBase和TiDB执行写入。运行步骤3、4、5中的协调作业，每天比较TiDB与HBase内的数据奇偶性，借此获取TiDB与HBase间数据不匹配的统计信息并进行协调。双重同步写入机制不具备回滚功能，无法解决对某一数据库的写入失败。因此必须定期运行协调作业，确保两个表间数据一致。继续保持对TiDB同步写入，同时对HBase启用异步写入。启用对TiDB的读取，此阶段中的服务SLA将完全取决于TiDB的可用性。我们将继续保持对HBase的异步写入，尽可能继续保持双方的数据一致性，以备发生回滚需求。彻底停止写入HBase，弃用HBase表。</p><p></p><h2>如何处理不一致问题</h2><p></p><p>由后端不可用导致的不一致</p><p>在Ixia服务层构建的双写框架无法回滚写入操作，这是为了防止因任一数据库不可用而导致局部故障。在这种情况下，就需要定期运行协调作业以保持HBase与TiDB双表同步。在修复此类不一致时，主数据库HBase为数据源，因此一旦出现HBase表写入失败、但TiDB表写入成功的情况，则协调过程会将这部分数据从TiDB中删除。</p><p></p><p>双重写入和协调过程中，因竞态条件引发的不一致</p><p>如果事件按以下顺序发生，则可能导致将旧数据写入TiDB：（1）协调作业从HBase表读取；（2）实时写入将数据同步写入至HBase，异步写入至TiDB；（3）协调作业将之前读取的值写入TiDB。</p><p>此类问题可以通过多次运行协调作业来解决，每次协调都能显著减少此类不一致数量。在实践中，对于支持每秒400次写入查询的4&nbsp;TB表，只需要运行一次协调作业即可在HBase与TiDB之间达成99.999%的一致性。这项一致性指标的验证源自对HBase和TiDB表转储值的二次比较。在逐行比较之后，我们发现两表的一致性为99.999%。</p><p></p><h2>成效</h2><p></p><p>我们看到，第99百分位处的读取延迟降低至三分之一到五分之一。在本用例中，第99百分位的查询延迟从500毫秒下降至60毫秒。实现了写后读取一致性，这也是我们希望通过替换Ixia达成的重要目标之一。迁移完成后，整个架构更简单、涉及的组件数量更少。这将极大改善对生产问题的调试流程。</p><p></p><h2>挑战与心得</h2><p></p><p></p><h3>内部TiDB部署</h3><p></p><p>由于我们没有使用TiUP（TiDB的一站式部署工具），所以Pinterest基础设施中的整个TiDB部署流程成了我们一次宝贵的学习经历。之所以没有选择TiUP，是因为它有很多功能都跟Pinterest内部系统相互重叠（例如部署系统、运营工具自动化服务、量化管道、TLS证书管理等），而综合二者间差异的成本会超出使用收益。</p><p></p><p>因此，我们决定继续维护自己的TiDB版本代码仓库和构建、发布与部署管道。集群的安全管理绝非易事、涉及大量细节，如果我们自己不努力探索，就只能把这些工作一股脑交给TiUP。</p><p></p><p>现在我们拥有了自己的TiDB平台，构建在Pinterest的AWS基础设施之上。我们可以在其中实现版本升级、实例类型升级和集群扩展等操作，且不会引发任何停机。</p><p></p><h3>数据摄取</h3><p></p><p>在数据摄取和协调过程中，我们也遇到了不少现实问题。感谢Pingcap在各个环节中提供的全力支持。我们也为TiDB代码库贡献了一些补丁，这些成果已经由上游社区完成了合并。</p><p></p><p>TiDB lightning 5.3.0版本不支持自动刷新TLS证书，而且由于缺少相关日志而难以调试。Pinterest的内部证书管理服务则每12小时刷新一次证书，所以期间总会发生一些失败的摄取操作，只能依靠pingcap来解决。好在证书自动刷新功能现已在TiDB 5.4.0版本中正式发布。Lightning的本地模式会在数据摄取阶段消耗大量资源，并影响到同一集群上运行的其他表的在线流量。为此，Pingcap与我们开展合作，对Placement Rules做出了短期和长期修复，因此支持在线流量的副本已经不会受到本地模式的影响。TiDB MR Connector需要进行可扩展性修复，才能把4&nbsp;TB表的快照保存时间控制在合理范围。此外，MR Connector的TLS也有改进空间，目前这些改进贡献已经完成了提交及合并。</p><p></p><p>在调优和修复之后，我们已经能够在约八小时之内摄取4&nbsp;TB数据，且每轮协调和验证运行只需要七小时左右。</p><p></p><h3>Ixia</h3><p></p><p>在本轮迁移中，我们的表由ixia负责支持。期间，我们在异步/同步双重写入和查询模式变更中遇到了几个可靠性问题。由于ixia本身的分布式系统架构非常复杂，导致thrift服务（ixia）极难进行调试。感兴趣的朋友请参阅我们的<a href=\"https://medium.com/pinterest-engineering/building-scalable-near-real-time-indexing-on-hbase-7b5eeb411888\">其他博文</a>\"以了解更多细节。</p><p></p><h2>鸣谢</h2><p></p><p>这里，我们要感谢Pinterest存储和缓存团队的各位前成员和现同事，谢谢大家在这场将最新NewSQL技术引入Pinterest存储堆栈的攻坚战中做出的卓越贡献。</p><p></p><p>我们还要感谢<a href=\"https://www.infoq.cn/profile/864AB02AC11ACA/publish\">Pingcap团队</a>\"为种种复杂问题提供的持续支持、联合调查和根本原因分析（RCA）。</p><p></p><p>最后，我们要感谢各位客户在此次大规模表迁移过程中表现出的耐心和支持。谢谢您的理解与配合！</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://medium.com/pinterest-engineering/online-data-migration-from-hbase-to-tidb-with-zero-downtime-43f0fb474b84\">https://medium.com/pinterest-engineering/online-data-migration-from-hbase-to-tidb-with-zero-downtime-43f0fb474b84</a>\"</p>",
    "publish_time": "2022-08-22 00:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]