[
  {
    "title": "混沌工程新视角：利用eBPF技术改进云原生可观察性与安全性",
    "url": "https://www.infoq.cn/article/2gWHzxSMqItrcztmRaYY",
    "summary": "<p></p><p>本文分享了如何将 eBPF 作为一种新的云原生技术来改善可观察性和安全工作流。入门门槛可能会很高，使用 eBPF 工具来辅助调试生产环境问题的步骤可能会很多。本文将学习如何使用现有工具并解决可观察性存储、警报和仪表盘方面的挑战。最好的工具就像备份一样——如果没有经过验证，它们就毫无用处。你将了解到混沌工程如何为我们提供帮助，并了解基于 eBPF 的可观察性和安全用例。通过专业的方式解决这些问题也为混沌工程本身带来了新的想法。文中还讨论了我们需要做的工作和需要面临的风险，并提出了未来调试生产环境问题需要改进的愿望清单。</p><p></p><p></p><h2>开始使用 eBPF</h2><p></p><p></p><p>使用 eBPF 程序和工具有多种方式，已经有太多的文章和建议，可能会让人感到不知所措。首先，我们需要定义用例和要解决的问题。获取更多底层监控指标是否有助于更快地解决生产事故？也许 Kubernetes 集群的安全性存在不确定性，我们是否有方法观察和减少恶意行为？微服务容器环境是一个观察行为和数据的好地方——但与在运行单体应用程序的虚拟机上读取文件相比，访问微服务数据通常更加困难。</p><p></p><p></p><h2>调试和诊断生产环境问题：eBPF 用例</h2><p></p><p></p><p>我们来看看实际的用例和工具，从中获得调试灵感，并弄清楚如何验证它们是否运行正常。</p><p></p><p>通过将额外收集的数据转换为指标和跟踪信息，eBPF 为我们带来了可观察性方面的好处。例如，我们可以使用自定义 Prometheus Exporter 收集底层的内核指标。更多面向开发人员的获得可观察性的方法是自动检测源代码，以便更好地了解应用程序性能，Pixie 就为开发人员提供了可用于获取可观察性的脚本语言。Coroot 用 eBPF 实现了 Kubernetes 服务映射，跟踪容器之间的流量，并提供 Ops/SRE 生产环境健康状况信息。Parca 的连续分析通过函数符号展开技术来跟踪应用程序代码内部的调用和性能。</p><p></p><p>安全是另一个 eBPF 可以为我们提供帮助的关键领域：检测未经授权的系统调用、回调连接或由恶意行为者（如比特币矿工或 rootkit）植入的事件系统调用。例如，Cilium 就提供了安全可观察性，并通过 Tetragon 进行预防。Aqua Security 的 Tracee 可以在 CLI 和 CI/CD 中使用，用于检测恶意行为。Falco 是一种 Kubernetes 威胁检测引擎，可能是云原生生态系统中最成熟的解决方案。GitLab 安全团队提供了一个特殊的用例，他们通过扫描包依赖安装脚本（例如 NodeJS 中的 package.json）来防止供应链攻击。</p><p></p><p>SRE/DevOps 团队需要合适的工具来进行快速调试，并在更接近全局的视图上查找根本原因。一些工具套件，如 Inspektor Gadget，可用于在 Kubernetes 中跟踪传出连接、调试 DNS 请求，等等。Caretta 可用于可视化服务依赖关系图。安全分发 eBPF 程序也需要新的想法：Bumblebee 通过利用 OCI 容器格式来打包 eBPF 程序，使其变得更加容易分发。</p><p></p><p></p><h2>可观察性挑战</h2><p></p><p></p><p>我们有多种可观察性数据类型，它们用于调试生产事件、分析应用程序性能、了解已知问题并看到潜在的未知问题。它们需要不同的存储后端，导致了我们的 DIY 疲劳。随着 eBPF 探针数据引入了更多的类型和来源，而我们无法将其转换为现有数据，这可能会让情况变得更加复杂。我们可能需要统一的可观察性数据存储，它需要具备伸缩性，以便存储大量的数据。</p><p></p><p>要处理某个故障，我们需要哪些确切的数据？诊断软件回归问题呢？找到最佳的数据保留期限是很困难的。自托管的存储后端也不可行，而 SaaS 可能又太贵。此外，我们还需要考虑成本效益和容量规划，这些有助于估算未来可观察性数据存储的增长。GitLab 基础设施团队推出的开源项目 Tamland 可以帮助我们进行容量规划和预测。</p><p></p><p>要从 eBPF 数据中获得更多的好处和洞察力，需要我们将其与警报和仪表盘集成在一起。如何减少公开的故障数量，并利用更多可用的洞察力来检测和修复问题？要进行异常检测、预测和趋势计算，我们需要整体的健康状态数据、与故障管理数据的交叉引用和“所有生产数据”——因为在微服务世界中，我们不会再去回答诸如“我的服务是否正常”这样的问题。</p><p></p><p>一个满是绿色的仪表盘，所有阈值都显示“OK”，但这并不能证明警报机制在正常工作，或者可以根据仪表盘信息快速解决关键问题。数据收集本身也可能受影响或遭到破坏，例如 eBPF 程序未按预期运行或生产环境发生了奇怪的内核错误。这带来了一个与模拟生产环境相关的问题：混沌工程——以可控的方式来破坏事物，验证服务级目标（SLO）、警报和仪表盘。我们可以用自己的经验来扩展混沌框架，例如用于云原生混沌工程的 Chaos Mesh，或集成到 CI/CD 工作流中的 Chaos Toolkit。来自混沌工程的模式也可以用于注入意外行为和执行安全测试，因为一切都会在随机混沌中有不同的表现，即使是安全漏洞。</p><p></p><p></p><h2>破坏 eBPF 的可观察性</h2><p></p><p></p><p>基于 eBPF 的工具和平台提供了极好的洞察力，有助于调试生产环境问题。这些工具和平台需要证明它们的优势并揭示它们的弱点，例如我们可以尝试破坏或攻击基础设施环境并观察工具 / 平台行为。我们先来关注可观察性和混沌工程。“金色信号”（延迟、流量、错误、饱和度）可以使用现有的混沌实验来验证，这些实验会注入 CPU/ 内存压力测试、TCP 延迟、DNS 随机响应等。</p><p></p><p>另一个实际的例子是收集底层的系统指标，包括 CPU、IO、内存。这可以使用 eBPF 的事件到 Prometheus 导出器来实现。为了验证收到的指标，现有的每一种类型（CPU、IO、内存压力、延迟）的混沌实验都可以帮助我们了解系统如何运行以及收集的数据是否有效。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ac/accb9d1570b1e546fbd1d9e378f00efd.png\" /></p><p></p><p>开发人员可以从 Pixie 中受益，它提供了应用程序代码的自动增强能力，并在 Kubernetes 集群内创建服务映射。为了验证映射显示的数据是否正确，以及跟踪信息显示的是否是性能瓶颈，我们需要添加混沌实验来进行压力测试和网络攻击。然后，我们就可以看到服务映射和跟踪信息随着时间发生变化，并在生产故障发生之前对发现的问题行为采取行动。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c4/c4a594632cecf7e85ea7da6d6c8b32c0.png\" /></p><p></p><p>对于 SRE 来说，可以通过安装 Inspektor Gadget 工具集合来有效地解决 Kubernetes 故障排除问题，帮助打破容器运行时黑盒的局限性。Inspektor Gadget 使用 eBPF 从容器通信路径上收集事件和指标，并将底层的 Linux 资源与高级的 Kubernetes 概念映射起来。这个工具集合提供了许多可用的 DNS、网络访问和内存分析部件，网站上将它们分类为建议、审计、profile、快照、top 和跟踪。你甚至可以使用“top ebpf”部件来可视化其他正在运行的 eBPF 程序的使用情况和性能。要测试它们的功能，建议单独运行它们，并进行与之匹配的混沌实验，例如，对于 DNS 请求，我们就随机返回或无响应。</p><p></p><p>安装 Coroot 并使用它的服务映射自动发现功能，我们可以实现更直观的 Kubernetes 故障排除和可观察性。可视化服务映射使用 eBPF 来跟踪容器网络连接，并使用 kube-state-metrics Prometheus 导出器中的指标将单个容器聚合为应用程序。Coroot 中的服务映射是混沌实验的绝佳目标——我们可以模拟影响服务的 TCP 连接（断开的或延迟的）或导致带宽增加的网络攻击，并验证仪表盘在故障条件下的状态。Coroot 还可以通过底层的 Prometheus 监控指标来检测 OOM 问题——对于泄漏内存的应用程序来说，这是一个完美的工具。在“Confidence with Chaos for your Kubernetes Observability”演讲中，我提供了一个演示用的示例应用程序，它只有在 DNS 失败时才会发生泄漏内存，就是专门针对这种情况的。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/50/508c1e4331afe25a46260e0799541a30.png\" /></p><p></p><p>Parca 的持续分析（Continuous Profiling）功能使用 eBPF 来自动增强代码，这样开发人员就不需要为了添加分析调用而去修改代码，为他们节省了精力。Parca 代理会生成关于调用栈、函数调用时间的分析数据见解，有助于识别应用程序中的性能瓶颈。CPU/ 内存压力测试会影响应用程序行为，可以揭示竞态条件和死锁问题，并帮助我们了解我们要优化什么。</p><p></p><p>除了跟踪信息和日志，OpenTelemetry 还支持指标数据格式。有一个项目提供了用于内核、Kubernetes 集群或超级云的 eBPF 收集器。不同的收集器将指标事件发送到 Reducer（数据摄取器），它既支持将指标作为抓取端点提供给 Prometheus，也支持使用 gRPC 将它们发送到 OpenTelemetry 收集器端点。我们可以用已知的方式添加混沌实验：对系统进行压力测试，了解指标随着时间会发生怎样的变化。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b9/b947f9a82e5c4aeeb02e2188c81c78e4.png\" /></p><p></p><p>最后但并非最不重要——一些场景涉及在高性能网络中作为 eBPF 程序运行运行的自定义 DNS 服务器。通过破坏 DNS 请求也有助于揭示它们的行为。</p><p></p><p></p><h2>换一个角度：破坏 eBPF 的安全性</h2><p></p><p></p><p>我们换一个角度，尝试破坏 eBPF 的安全性工具。一种方法是注入模拟权限升级的行为数据，并观察工具的反应。另一种方法是利用有数据分离要求的多租户环境，并模拟非预期的访问请求。</p><p></p><p>模仿攻击者通常很难，但当有人提到“跟踪系统调用、捕捉 rootkit 事件”，立刻引起了我的注意。搜索 Linux rootkit，我们会得到一些结果，了解它们有助于构建潜在的攻击模仿场景。在互联网上搜索 syscall hooking，可以找到更多资源，包括 tracee 维护人员关于使用 tracee 捕捉 rootkit 的演讲，其中涉及如何使用 Diamorphine rootkit。</p><p></p><p>在继续阅读和尝试运行示例之前，请不要在生产环境中做这些事情。创建一个隔离的测试虚拟机，下载并构建 rootkit，然后加载内核模块。它会隐藏自己并尽一切努力破坏系统。测试后删除虚拟机。</p><p></p><p>我们可以通过调用 Tracee CLI 来检测系统调用钩子。我们可以用下面的命令在 Docker 中运行 tracee——一个特权容器，映射了几个变量并指定要跟踪的事件“hooked_syscalls”。</p><p></p><p><code lang=\"null\">$ docker run \\  --name tracee --rm -it \\  --pid=host --cgroupns=host --privileged \\  -v /etc/os-release:/etc/os-release-host:ro \\  -v /sys/kernel/security:/sys/kernel/security:ro \\  -v /boot/config-`uname -r`:/boot/config-`uname -r`:ro \\  -e LIBBPFGO_OSRELEASE_FILE=/etc/os-release-host \\  -e TRACEE_EBPF_ONLY=1 \\  aquasec/tracee:0.10.0 \\  --trace event=hooked_syscalls\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/89/89a661bfcc7ac9e454ce1e572094c7c1.png\" /></p><p></p><p>问题是如何基于 rootkit 创建混沌实验？它并不是可靠的生产环境混沌测试，不过我们可以在生产环境中模拟 Diamorphine rootkit 的 getdents 系统调用 hook 方法，以此来验证是否触发了警报。</p><p></p><p>Cilium Tetragon 也通过类似的方式使用 eBPF 来检测恶意行为，并揭示了有关 rootkit 行为的新见解。检测和规则引擎显示了 rootkit 的夜间活动扩展到在给定端口上生成具有随机名称的进程。</p><p></p><p><code lang=\"null\">$ docker run --name tetragon \\   --rm -it -d --pid=host \\   --cgroupns=host --privileged \\   -v /sys/kernel/btf/vmlinux:/var/lib/tetragon/btf \\   quay.io/cilium/tetragon:v0.8.3 \\   bash -c \"/usr/bin/tetragon\"\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/22/22df16c7e2d9908e92953f1a51024b0b.png\" /></p><p></p><p>我们来想象一个更实际的场景：运行在云虚拟机上、Kubernetes 集群中以及 CI/CD 部署环境中的比特币挖矿恶意软件。检测这些模式是问题的一部分——预防入侵是另一个尚未被解决的问题。我们仍然不建议在生产环境中基于 rootkit 构建混沌实验——但可以在 eBPF 程序中模拟系统调用负载。</p><p></p><p>这让我想到了一个新提议：构建仅用于模拟的混沌测试 rootkit。例如，hook getdents 系统调用，获取目录文件列表，然后就可以验证安全工具是否能够检测到模拟的安全问题。如果可能，模拟更多之前了解到的攻击。训练 AI/ML 模型也是一种有趣的用例，可以提供额外的模拟攻击来验证 eBPF 的安全工具和平台。</p><p></p><p></p><h2>如何让混沌工程从 eBPF 中受益</h2><p></p><p></p><p>在准备 QCon 伦敦的演讲时，我想到将 eBPF 作为一种为混沌实验收集和注入数据的方法。如果 eBPF 允许我们访问内核底层的信息，我们就可以修改数据并模拟生产事故。有一篇关于“通过系统调用最大化混沌工程错误注入现实性”的研究论文，其中介绍了 Phoebe 项目，它利用 eBPF 来捕获和控制系统调用。</p><p></p><p>现有的混沌实验发生在用户级别。例如，Chaos Mesh 中的 DNS Chaos 被注入到负责处理 Kubernetes 集群中所有 DNS 请求的 CoreDNS 中。如果有一个运行在内核中的 eBPF 程序，在 DNS 请求到达用户空间对其进行 hook 会怎样？它可以分析 DNS 请求，并通过为解析器请求返回错误响应来注入混沌。Xpress DNS 项目已经做了一些这方面的工作。这是一个用 BPF 编写的实验性 DNS 服务器，用于生成高吞吐量和低延迟的 DNS 响应。运行在用户空间的应用程序可以向 BPF 映射中添加 / 更改 DNS 记录，内核 eBPF 程序会读取 BPF 映射。这可以成为使用 DNS 和 eBPF 进行新的混沌实验的入口点。</p><p></p><p>基于所有已知的与 eBPF 混沌注入相关的想法，我们可以创建新的混沌实验来模拟 rootkit 行为，并以此来验证安全可观察性。通过拦截流量来制造 TCP/UDP 和 DNS 的延迟、增加 CPU 压力，这些想法可以帮助我们验证可靠性和可观察性。</p><p></p><p></p><h2>混沌 eBPF：我们还有工作要做</h2><p></p><p></p><p>eBPF 的优点和好处看起来已经很清楚了，但问题是我们将来需要在哪些领域进行投入？我们应该注意哪些风险？</p><p></p><p>一个重点领域是 DevSecOps 和 SDLC，eBPF 程序代码需要进行编译、测试、验证、安全扫描和分析潜在的性能问题。我们还需要避免潜在的供应链攻击。鉴于 eBPF 的复杂性，用户需要遵循安装指南，还可能需要应用 curl | bash 命令模式。</p><p></p><p>在 CI/CD 管道中自动测试 eBPF 程序是一件很棘手的事情，因为内核会在加载时验证 eBPF 程序并拒绝执行潜在的不安全的程序。有人尝试将 eBPF 验证器移出内核，并允许在 CI/CD 中测试 eBPF 程序。</p><p></p><p>eBPF 存在风险，其中一个很明显的风险是对内核级别的所有内容都具有根访问权限。你可以在 TLS 库函数调用（包含原始字符串）后立即 hook TLS 加密流量。在一些真实世界的案例中，一些 rootkit 和漏洞利用 eBPF 来绕过 eBPF。有一些研究使用特殊编程技术进行漏洞利用和数据访问，这些技术不会被 eBPF 安全执行工具检测到。猫鼠游戏将继续……</p><p></p><p>针对 eBPF 的愿望清单：</p><p></p><p>内容可睡眠的 eBPF 程序，可以暂停上下文，并在稍后继续（在编程语言中叫作“Fiber”）。观察其他 eBPF 程序是否存在恶意行为的 eBPF 程序，类似于 Ops 中的 monitor-the-monitor（监控监视器）问题。更多的入门指南、学习资源和平台抽象，它们可以降低学习这项新技术的门槛，让每个人都可以参与贡献。</p><p></p><p></p><h2>结论</h2><p></p><p></p><p>eBPF 是一种新的可观测性数据采集方法，为我们带来了网络见解和安全可观察性。我们可以从生产故障的调试中受益。混沌工程有助于验证可观察性和 eBPF 程序，在混沌实验中使用 eBPF 探针的想法将使其进行更快的迭代。此外，我们能够从传统指标监控之外的更多数据源（关联、验证和观察生产环境）中受益。这有助于我们通向 DataOps、MLOps/AIOps——AllOps。</p><p></p><p>开发人员可以从可观察性驱动开发的自动增强中受益。DevOps/SRE 团队通过混沌工程验证可靠性，DevSecOps 将看到更多云原生安全默认设置。在 CI/CD 中对 eBPF 程序进行测试和验证是一项重要的工作，其次是将所有的想法带到上游，并降低使用和贡献 eBPF 开源项目的门槛。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/debugging-production-ebpf-chaos\">https://www.infoq.com/articles/debugging-production-ebpf-chaos</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/dOlXISVio781RFgaETVx\">混沌工程：让云原生工程师更贴近用户</a>\"</p><p><a href=\"https://www.infoq.cn/article/ANXQZd4rQcmyQbjLVVaE\">Kubernetes 混沌工程平台 Chaos Mesh 升级 CNCF 孵化项目</a>\"</p><p></p>",
    "publish_time": "2023-08-11 09:53:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Amazon SQS支持从死信队列重新生成消息",
    "url": "https://www.infoq.cn/article/zHY5OAVjkHg3fZzaA9Du",
    "summary": "<p>亚马逊云科技最近宣布<a href=\"https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-sqs-dead-letter-queue-redrive-aws-sdk-cli/\">在SQS中支持使用AWS SDK或命令行接口进行死信队列的重驱动</a>\"。新功能允许开发人员将未消费的消息从死信队列中移出并转移回其源队列。</p><p></p><p>当出现错误时，SQS会将未消费的消息转移至<a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">死信队列（dead-letter queue，DLQ）</a>\"，从而能够让开发人员探查未成功消费的消息并调试应用程序的故障。亚马逊云科技的开发人员倡导者<a href=\"https://www.linkedin.com/in/sebastienstormacq/\">Sébastien Stormacq</a>\"解释到：</p><p></p><p></p><blockquote>每当消费者应用捡取一个要处理的消息时，消息的接收计数就会加1。当ReceiveCount&nbsp;&gt;&nbsp;maxReceiveCount时，Amazon SQS会将消息移动到指定的DLQ中，供人工分析和调试。我们通常会将警报与DLQ关联起来，以便于在这种情况发生时发送通知。</blockquote><p></p><p></p><p>在失败的消息调试完成或消费者应用能够消费它时，<a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue-redrive.html\">新的重驱动功能</a>\"就会将消息移回源队列，从而能够在分布式系统中以编程的方式管理大规模未消费消息的生命周期。</p><p></p><p>过去，这只能通过<a href=\"https://aws.amazon.com/blogs/compute/introducing-amazon-simple-queue-service-dead-letter-queue-redrive-to-source-queues/\">在控制台手动处理</a>\"才能实现。Ampt公司的CEO兼创始人Jeremy Daly当时这样<a href=\"https://twitter.com/jeremy_daly/status/1474110309300580352\">写到</a>\"：</p><p></p><p></p><blockquote>这不是一个特性，这不是一个API，而是一种只能在AWS Console中才能获取的“体验”。我想要它吗？想要！但是，我想登录AWS Console来使用它吗？绝对不想要！</blockquote><p></p><p></p><p>要重新处理DLQ消息，开发人员可以使用如下的任务：StartMessageMoveTask用于从死信队列启动新的消息移动任务；CancelMessageMoveTask用于取消消息移动任务；ListMessageMoveTasks用于获取特定源队列最近的消息移动任务（最多10个）。</p><p></p><p>社区对这项特性给出了积极的反馈，MUSIC Tribe的云计算和平台主管Tiago Barbosa<a href=\"https://twitter.com/t1agoB/status/1666931380788371457\">评论</a>\"说：</p><p></p><p></p><blockquote>这是一个很好的改进。我一直不喜欢使用DLQ，其中一个原因就是需要建立一种机制来重新处理最终出现在DLQ中的条目。</blockquote><p></p><p></p><p>Curantis Solutions的CTO&nbsp;<a href=\"https://www.linkedin.com/in/benjamenpyle/\">Benjamen Pyle</a>\"撰写了一篇文章，介绍了如何使用Golang和Step Functions来<a href=\"https://www.binaryheap.com/sqs-re-drive-with-golang-and-step-functions/\">重新驱动消息</a>\"。</p><p></p><p>在DLQ的配置中，可以使用自定义目的地选项的ARN来指定将消息发送回源队列还是其他队列。PostNL首席工程师、AWS Serverless Hero Luc van Donkersgoed在<a href=\"https://twitter.com/donkersgood/status/1666855688272850946\">推特</a>\"上写到：</p><p></p><p></p><blockquote>如果能重新驱动到原始队列就好了。这一点非常棒，因为它允许我们指定任意的目标队列。这使得以前完成此项任务的Lambda Functions瞬间化为乌有。</blockquote><p></p><p></p><p>文档强调了一些限制：SQS仅支持标准队列的死信队列的重新驱动，不支持在重新生成它们时过滤和修改消息。除此之外，一个DLQ重新驱动任务最多可运行36小时，每个账户最多可以有100个活跃的重新驱动任务。有些开发人员质疑其<a href=\"https://twitter.com/peegee123/status/1667567610148401152\">缺少对Step Functions的支持</a>\"。</p><p></p><p>SQS不会自动创建DLQ，队列必须在接收到未消费的消息之前进行创建和配置。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/06/aws-sqs-dlq-redrive/\">Amazon SQS Supports Reprocessing Messages from Dead-Letter Queue</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/3utpk9247A6CtoyztLTB\">大模型竞争突然升级！亚马逊CEO亲自监督、组建新的核心技术团队，集中优势资源打造“最具野心”的大语言模型</a>\"</p><p><a href=\"https://www.infoq.cn/article/CC2RaXSKw5oRwzpymyxx\">亚马逊云科技开源PBAC领域特定语言Cedar</a>\"</p>",
    "publish_time": "2023-08-11 10:06:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "LMOps 工具链与千帆大模型平台",
    "url": "https://www.infoq.cn/article/o8abj2wff5yLfGWuB0E1",
    "summary": "<p></p><p>随着<a href=\"https://xie.infoq.cn/article/a549b23a4176dc689e5273635\">人工智能</a>\"、云计算、大数据等技术的不断发展，企业和机构需要处理海量数据，对<a href=\"https://xie.infoq.cn/article/14630754954bc49e59186a10a\">大模型</a>\"技术的需求不断增加。竞争环境日益激烈，大模型技术已经成为人工智能领域的竞争焦点，各大科技公司和研究机构都在不断投入资源进行研发，推出自己的大模型产品和解决方案。百度智能云在这方面提供了专业的产品，为产业带来了重大的变化和挑战。</p><p></p><p>那大模型到底在技术上带来了哪些变化和挑战？要掌握哪些关键的技术企业才能够驾驭大模型？百度智能云在这方面又提供了哪些专业的产品？在《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课第七讲《LMOps 工具链与文心千帆大模型平台》中，百度智能云主任架构师谢勇康将为大家回答这些问题。</p><p></p><p>课程内容主要包含以下四个方面：</p><p></p><p>人工智能的发展路径；LMOps 相关的概念以及关键技术；千帆大模型平台的功能和应用；千帆大模型平台在产业中的实践。</p><p></p><p>下文对课程内容以谢勇康第一人称进行了整理：</p><p></p><p>特别说明：本次课程中的大模型，如果没有特别指出的话，主要就是指大语言模型。国内对于 LMOps 也有称作 LLMOps 的，内容上基本相同。为了表述的简洁，业界越来越多地使用 LMOps 的提法。</p><p></p><p></p><h2>一、从机器学习到百模大战</h2><p></p><p></p><p>众所周知，目前我们实现人工智能的主要技术手段是机器学习技术，特别是其中基于深层神经网络的深度学习技术。机器学习的本质是通过具有学习能力的算法、对数据进行建模的技术。</p><p></p><p>深度学习借助大规模的算力解决了机器学习中特征表示的人工干预的瓶颈，在效果上取得了巨大突破。因此，机器学习成为目前人工智能的主流技术。</p><p></p><p><a href=\"https://xie.infoq.cn/article/9384dee7afcc8fd339e98dd99\">深度学习</a>\"和生成式大模型之间的关系，如下图右侧所示，在 2012 年至 2016 年左右，像卷积神经网络、对抗生成网络、ResNet 等经典的深度学习模型，已经在计算视觉、语音识别、自然语言处理等领域取得了显著的效果提升。这些经典深度学习模型既有判别式、也有生成式，它们往往会在 ImageNet、COCO 等有标注的数据集上进行预训练，形成带有预训练权重、可以进一步进行 Fine-tuning 的预训练模型。</p><p></p><p>在 2017 年之后，Transformer 结构在自然语言处理领域首先被成功应用，在这之后以 Transformer 为基础组件的生成式大模型逐步成为视觉、自然语言处理、跨模态理解和生成领域的主流技术。</p><p>这类技术通常以 Transformer 和注意力机制作为组件，并且它可以并行地进行自监督学习，参数规模在十亿以上。其中，将生成式大模型技术应用在语言建模上的方式，被称为「大语言模型」。在经过进一步的调优之后，形成了像 ChatGPT、文心一言等被大家熟知的对话式、生成式大语言模型应用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b9525d6fccbf0b6451f7e4dab491127.png\" /></p><p></p><p>在过去的半年，我们经历了一场百模大战。尤其是在开源社区，新的大模型如雨后春笋般涌现，而大模型相关的技术也越来越标准化、同质化。</p><p></p><p>在这里为大家分享一个小故事。我们可以在大模型中了解到很多「驼」系英语词汇，比如 Llama 是美洲驼，Alpaca 是羊驼，Vicuna 是小羊驼。为什么有那么多以「驼」命名的大语言模型？</p><p></p><p>因为大语言模型 Large Language Model 的缩写是 LLM，2 个 L 放在一起不方便读出来，Meta 公司为了方便大家记忆，所以选了相近的词语 Llama（美洲驼）。后来很多基于 Llama 开源模型进行调优和构建的大语言模型，都以「驼」系的名称命名。</p><p></p><p>如下图所示，我们可以看到在硅谷的大模型创业公司中，除 OpenAI 外，目前已有将近 1/3 的资金投入了 MLOps 和 LMOps 相关的平台和工具方向。</p><p></p><p>接下来，我将为大家详细拆解，在百模大战的背后，为什么 MLOps 和 LMOps 平台和工具能够获得资本的青睐。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/5135d2f5a771ab5ccbf1639e579cd8d6.png\" /></p><p></p><p>首先看看大模型在技术和应用层面带来了哪些变化。比如在以下 4 个技术层面：</p><p></p><p>数据：大模型的预训练通常需要 TB-PB 级别的数据， TB-PB 级别的数据规模和对应的数据加工技术，与之前的经典深度学习模型并不相同。同时，大模型大多以多模态、指令、对话数据作为训练或调优的输入，在数据内容上和之前的经典深度学习模型也有很大的差异。</p><p></p><p>训练和调优的方法：现在千亿参数级别的大模型，往往需要千卡、甚至万卡进行分布式训练，其中的调度、容错、通信技术和之前大不相同。大模型在调优过程中也出现了很多低资源开销、高效率的技术。</p><p></p><p>大模型效果评估方式：经典深度学习模型往往基于人工标注的测试数据集，来计算客观指标，并评估模型效果。因为大模型生成的海量内容暂无标准的答案，所以我们无法全部依赖人工去评判内容的质量。因此，大模型的效果和性能需要建立与以往不同的评估基准和评估方法。</p><p></p><p>推理：通过 Prompt 工程来调教大模型的输出，无论是在自然语言处理还是视觉生成领域，之前经典的深度学习模型都不具备这些能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2b07b703539d708a8435bf849ff59aa.png\" /></p><p></p><p>除技术方向上的变化外，大模型还改变了人工智能应用的构建模式，这也是大模型带来的最重要的变化。</p><p>如下图所示，在过去是一个模型去完成一类特定的任务（如：人脸识别、卡证识别、多轮的对话等），需要通过训练不同的模型、使用不同的数据才能完成。在大模型出现后，我们可以在少数几个甚至在一个预训练的大模型的基础上，针对行业的场景加入相关数据、最后调优就可以完成任务。这也使得大模型的研发更加地集约化，进一步提升生产效率。</p><p></p><p>当然，这种构建模式的改变并非一蹴而就的，而是循序渐进的。但趋势是很明确的，我们正处于「大炼模型」到「炼大模型」的升级过程中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f84430d3230069eaa55f735a4d83708e.png\" /></p><p></p><p>大模型在技术和应用模式上带来了巨大的变化，也对企业提出了新的挑战，比如如何对 AI 能力进行运用和管理。今天，我们要介绍的 LMOps 就是来解决上述挑战最有效的技术手段。</p><p></p><p>只有通过 LMOps，企业才能真正驾驭大模型，让其成为智能化升级的重要生产力。这也是为什么在热火朝天的百模大战之后，除了超级应用之外，LMOps 平台和工具获得资本青睐的原因。</p><p></p><p>下面，我们将介绍 LMOps 相关的概念以及一系列关键的技术。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ea/eaa21fe5ba18bf6094433cebe94b6517.png\" /></p><p></p><p></p><h2>二、DevOps、MLOps 和 LMOps 概念及相关技术</h2><p></p><p></p><p>DevOps 是贯穿传统软件生命周期的方法论和最佳的技术实践，它包括软件的需求提出、代码开发、测试、上线运维和推广运营等多个环节。其中的关键技术包括需求管理、版本控制、持续集成、组件化、容器化、微服务、持续交付、自动化运维等。目前 DevOps 的平台和工具已经成为大多数软件开发企业驾驭软件研发运营最有效的方法。</p><p></p><p>如下图所示，DevOps 各个环节中有大量的生态企业参与进来，共同构成了完整、繁荣的生态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/63/6354f00d77fca249fa382b1d6066cba0.png\" /></p><p></p><p>在机器学习的全生命周期中，和传统软件开发类似，算法的研发和代码编写只占其中很少一部分。如下图所示，图中的黑色部分是针对模型的代码和结构进行开发。其他部分，如数据采集、加工、服务搭建、效果监控等占用了机器学习工作量的绝大部分。因此，如果企业希望驾驭机器学习，就需要有合适的，针对机器学习生命周期特点的方法论和最佳的技术实践，这也是 MLOps 诞生的原因。</p><p></p><p>MLOps 涉及机器学习、 DevOps 和数据科学的相关技术，覆盖机器学习的需求分析、数据采集、加工、模型算法开发、训练、效果评估、调优、版本管理、服务部署、推理监控以及模型持续迭代等全生命周期的过程。</p><p></p><p>所以， MLOps 为企业在生产环境中稳定高效地部署和运营机器学习提供了一套具有实践意义的方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/031ca46f8aac3d082daebf533463b975.png\" /></p><p></p><p>接下来，为大家详细介绍本文讨论的重点 LMOps 、以及 LMOps 与 MLOps 的关系。</p><p></p><p>如上文所述，相对于传统机器学习和经典的深度学习模型，大模型发生了较大的变化，带来了新的挑战。LMOps 继承了 MLOps 整体的框架和机器学习的全生命周期等主要环节，并且针对大模型的变化进行了微调适配。</p><p></p><p>在生成式大模型的数据组成中，无监督数据和未标注的数据在大幅度上升。为了解决上述问题，LMOps 对无监督数据和未标注的数据加工进行了强化。对于传统的机器学习模型，我们经常需要修改代码来进行效果的强化和调整。而在 LMOps 下，我们几乎不需要去修改大模型的代码，同时也会注重并行分布式训练大模型的效能的提升。</p><p></p><p>另外在 LMOps 中，针对大模型出现了新的提示工程（ Prompt 工程），从工具链的角度思考哪些方面可以更加自动化、提供提示词模板等也是 MLOps 中新引入的能力。</p><p></p><p>对于预训练模型的调优，在 LMOps 中也出现了新的方法，包括下文我们会介绍的 PEFT 、 SFT 方法等。近期还有另一个非常热门的方向，主要讨论针对大语言模型，如何通过插件的方式去扩展它的能力，开源社区也有一些很好的 API 框架由此诞生。对于这部分插件的能力， 在 LMOps 生命周期中需要同时被管理、做进一步的适配和优化。</p><p></p><p>综上所述，LMOps 继承了 MLOps 主体及框架，并在每个环节上都针对大模型的特点进行适配，形成新的技术实践。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/986552f7ad5e597744d747736f712836.png\" /></p><p></p><p>如下图所示，虽然 LMOps 问世不久，但整个上下游的各类公司已经共同构建了繁荣的生态。其中就包括向量数据库 Pinecone、以及我们非常熟悉的 OpenAI、Hugging Face、Stable Diffusion 等做模型及模型集成的厂商。这也解释了为什么 MLOps 与 LMOps 在资本投入中占据很大比重。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d9/d97a2c2cc096baf8ff2d60d0dbeae8d3.png\" /></p><p></p><p>在了解了 LMOps 的相关概念之后，下面将为大家介绍 LMOps 在数据、训练、评估、推理、部署和安全这六个主要环节的核心技术。</p><p></p><p>在数据加工的环节上，大模型在预训练和再训练的阶段，往往使用的是大规模未标注数据。因此，对这些大规模未标注的数据进行加工和配比，对大模型的效果至关重要。数据加工环节包括五个步骤：</p><p></p><p>首先是对特殊字符的一些清除，如火星文 / 特殊的标点清除等，替换部分异常文本。</p><p></p><p>删除低质量文档。建立低质文档的统计指标，超过某个阈值就进行删除。或通过定制的分类模型对文档质量进行自动分类。</p><p></p><p>文档去重。通常情况下，我们可以针对文档中的句子和段落等进行文档内的内容去重；针对两个内容重复阈值较高的相似文档，可以进行跨文档去重。</p><p></p><p>去除隐私数据。使用基于规则的正则表达式的方法检测个人信息，来进行隐私数据的脱敏。</p><p></p><p>建立词表（ Tokenize 的过程）。目前建立此表常用的是 SentencePiece 等方法。当我们将原始的语料加工成 token，并建立 token_id 后，再喂给大模型进行训练或者推理。</p><p></p><p>对于数据加工的环境，我分享一下自己的心得。决定大模型效果的，并不主要是模型结构和参数配置，而是数据的质量和配比，对大模型的效果会产生更加重要的影响。比如训练大模型需要哪些来源的数据、数据的比例分配，通过加工步骤后数据的质量和多样性决定了预训练后大模型的效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff3de7bb8125197cca917629e7ab94f4.png\" /></p><p></p><p>LMOps 训练环节的核心技术，包括监督调优和基于人工反馈的强化学习。如下图所示，SFT （Supervised Fine Tuning）通过包含指令和提示数据的人工标注生成结果对大语言模型进行调优，能够较好地激发大模型的场景化能力和生成效果。RLHF（Reinforcement Learning from Human Feedback ）能够让大语言模型优先生成最符合人类喜好和价值观的结果。这也是目前大语言模型调优一定会做的技术，所以 RLHF 是 LMOps 训练环节的核心技术之一。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5a81ebf563a72bc100e2a08ae547aea.png\" /></p><p></p><p>除 SFT、RLHF 技术外，近期 PEFT 技术也非常热门，是目前大语言模型调优环节中广泛采用的技术，它是一系列高效率微调技术的总称。</p><p></p><p>大模型的参数量一般在十亿甚至百亿、千亿、万亿以上。如果按照传统的机器学习模型或经典的深度学习模型，对大模型的全量参数进行调优（再训练）会造成资源开销超负荷。并且，对大模型的全量参数进行调优，可能会把预训练的模型训坏掉，丧失大模型原有的能力和效果。针对此问题，研究者提出微量调优（针对少部分参数进行调优）的办法。所以，目前常用 PEFT 手段对大模型进行调优。</p><p></p><p>在 PEFT 中，常用的方法包括 P-tuning 和 LoRA，他们都是在大模型合适的位置增加需要微调的参数，来提升调优的效率、达到优化模型目的。</p><p></p><p>例如 Prefix-tuning 就是在输入 token 之前构造一段和任务相关的 virtual tokens 作为 Prefix，训练的时候只更新 Prefix 的参数，而 Transformer 中的其他部分参数固定。Prompt-tuning 可以看做是 Prefix-tuning 的简化版，只在输入层加入 Prompt tokens。只要模型规模够大，简单地加入 Prompt tokens 进行微调，也能取得很好的效果。</p><p></p><p>而 LoRA 采用的是另一种方法。研究者发现，大语言模型虽然参数众多，但是在模型推理中，真正起到关键作用的还是低秩的本质维度（Low Instrisic Dimension）。因此提出了 Low-Rank Adaption (LoRA)，在涉及到矩阵相乘的模块，引入 A、B 两个低秩矩阵模块去模拟 full finetune 的过程，相当于只对语言模型中起关键作用的低秩本质维度进行更新，这就是 LoRA 的原理。</p><p></p><p>这些 PEFT 方法都已经引入到 LMOps 中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b831bfd3ee292bbf46c1390234163dae.png\" /></p><p></p><p>对于大模型的评估，我们需要围绕评估标准、评测集、评估任务提示模板、评估工具四方面重新建立评估的流程，这四方面也是 LMOps 独有的特点：</p><p></p><p>对于大模型的评估我们需要建立新的评估标准，包括针对效果、性能、安全性、生态多样性等各个维度的评估。这些能力都需要在 LMOps 中构建相应的大规模的评估标准来实现。</p><p></p><p>在过去，针对经典的深度学习的模型已经有 Benchmark 存在，但是当大模型出现之后，发现这些已经存在的评测集已经不够用了。有些小规模的评测集可能已经包含在大语言模型的训练数据当中了，再去利用这些 Benchmark 做评估，它就没有区分度了，所以新的大规模的评测集需要构建起来。</p><p></p><p>因为大语言模型的能力在不断增强，所以评测集涵盖的方向需要进一步的拓展，包括像数学、历史、图像生成等跨模态等。</p><p></p><p>在评估环节，已经不是单纯的用一个指令或者 Query 输入去评估大模型的输出了，而是说会结合不同的评估模板，让机器去做自动的评估，包括通过让大语言模型去做选择题。这样的话就可以更加客观地评估在实际的使用过程中如何提升它的效果。</p><p></p><p>和评估环节相应的是评估工具，包括人工可操作的、以及面向机器的评估工具来进行自动化的评估。</p><p>在评估环节，以上四个方面是 LMOps 中新增和特有的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f73116c4a337ba56d083254289d9417.png\" /></p><p></p><p>在推理环节中，LMOps 中最明显的变化就是引入了 Prompt 工程（提示工程）。一个标准的 Prompt 模板，包括对任务背景的陈述，然后提供一些相关的上下文的材料。其中很重要的一点，就是如果能提供一些示例，也称为 few shot，那么大语言模型的输出的效果会显著的有一个提升，最后再加上这次输入的 Query。</p><p></p><p>而针对提示模板的构建，也有一系列的要求和实践指导。比如，提示模板的首要要求是安全无害的内容，意图本身模板足够的规范、流畅，提供真实可信的材料等。每一个使用者都需要全部遵守提示模板的要求，但每一个使用者对 Prompt 的书写各有不同，所以遵守 Prompt 模板的要求实施难度较大。所以，在 LMOps 中出现了对应的自动化生成提示工具，为使用者提供提示模板，辅助整个大模型提示工程的实现。</p><p></p><p>在很多平台型的 LMOps 工具中，还会自带 Playground，让我们可以直接以对话的形式、或分类摘要等形式直接体验训练 / 调优完成的大模型效果。</p><p></p><p>在推理环节的最后一部分是基础的 API 服务。企业在自己的应用中使用大模型，通常将 API 与现有的业务集成，使得业务具备更加智能化的体验。从一个模型到服务，需要经历 API 授权、提供访问的 Token 凭证，并对调用进行流控、鉴权等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/323115addaf566d61ab96bd9a35b86a5.png\" /></p><p></p><p>在部署环节，因为大语言模型参数量很大，需要在服务器上做部署，依赖大规模的 GPU 和 CPU 的资源等。</p><p></p><p>但是如果需要把这些大模型的核心能力部署到边缘或者小型化的设备上，甚至于一些性能比较好的手机上，是否有实现的可能？大模型如果能做本地的部署，那么它的响应效率会更高，延迟会更低，数据的隐私性会更好。</p><p></p><p>在部署环节，LMops 会引入很多优化，包括量化、蒸馏、交叉编译、国产芯片适配等技术。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/122b2d1c5db5da2ee0a276f0629523ef.png\" /></p><p></p><p>因为大模型的再训练、预训练的资源规模庞大，所以很多企业与个人用户是没有足够的算力资源去支撑他们完成相关任务。针对此情况，企业与个人用户需要依赖云端资源来满足自己的业务需求。</p><p></p><p>企业上云，如何保证数据的安全和隐私是企业考虑的重要因素。</p><p></p><p>下图为大家详细介绍的千帆大模型平台就在安全方面做了大量的优化工作。比如千帆大模型平台进行数据加密（包括同态加密、差分隐私等）、为训练环节提供可信的计算环境（包括沙盒环境等），以及在大模型完成部署进行推理的时候如何对数据进行加密、针对用户的隐私数据进行脱敏等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c9d5858abfbec4deb6bb6267a200f3e.png\" /></p><p></p><p></p><h2>三、千帆大模型平台</h2><p></p><p></p><p>接下来，我们将介绍百度智能云千帆大模型平台的能力、特点，以及我们如何在业务场景中使用千帆大模型平台。</p><p></p><p>在上层模型应用上，千帆大模型平台托管了文心系列的大模型，包括文心一言 API 调用、以及文心系列大模型的管理、调优等。千帆大模型平台也针对第三方的开源大模型进行了适配，包括已上线的 BloomZ、Llama 2 等，后续千帆大模型平台也将持续引入热门、高价值的第三方大模型。</p><p></p><p>在底层资源上，千帆大模型平台提供丰富的算力资源、存储资源及系列账号的管控等服务。将异构算力、高性能的文件系统以及高性能网络管理起来，为大模型的训练、调优、推理提供良好的基础服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/738f8fae8685fa69272ac177a2646e8e.png\" /></p><p></p><p>千帆大模型平台具有这样几个特点：</p><p></p><p>第一个就是它的易用性，只需要通过简单的几步点击就可以获得效果比较不错的一个模型。</p><p></p><p>第二个就是它功能在业内是比较全的，像数据标注和反馈、多种模型训练方式，各种 PEFT 的训练方式、模型部署当中涉及到的小型化和压缩，以及对推理服务的一些管理和插件化应用的集成等，这些功能都是比较全面的。</p><p></p><p>第三个就是千帆大模型平台是提供了非常可靠和安全的模型训练和推理服务的环境，包括可信计算的环境，包括隐私数据的脱敏、安全沙箱等。</p><p></p><p>第四个就是在千帆大模型平台上有效果和性能优化的工具链，所以使得大模型的研发效率可以得到显著的提升，研发周期可以成倍的缩短。</p><p></p><p>第五个就是千帆大模型平台是一个开放的平台，不仅有效果优异的文心一言的模型，它还会更多的引入第三方的优秀的开源大模型。最后就是在千帆大模型平台上托管的这些模型，我们会非常注重它的这个可扩展性。这些是面向场景和面向应用的可扩展性，那包括对这个插件机制的支持，包括对这个服务应用编排的这个支持。</p><p></p><p>因此，千帆大模型平台是在 LMOps 的方法论指导下，针对大模型的平台功能和工具链的一个具体实现。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fb/fb3aa8df7b4f0164d84270b3f1e92b3a.png\" /></p><p></p><p>接下来，我们演示一下在千帆大模型平台进行 RLHF 的过程。详细的演示 Demo 请参考「百度智能云技术站」视频号的回放，从视频的从视频的 40 分 45 秒处开始观看。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ea/ead4d443154ee415f2bc2ac05d0007b9.png\" /></p><p></p><p></p><h2>四、产业实践</h2><p></p><p></p><p>在这里，我们演示一个基于千帆大模型平台搭建的行业应用。这是大模型做投资顾问的场景，能够依据客户的投资偏好等，对客户的投资组合进行分析和评估，并诊断目前的投资组合中的风险，并给出进一步的投资建议和相应的逻辑。</p><p></p><p>这不仅提升了金融行业的工作效率，更重要的是他提供了创造性的输入，为决策进行辅助。详细的演示 Demo 请参考「百度智能云技术站」视频号的回放，从视频的 44 分 30 秒处开始观看。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6b/6b5dd253f8242be1c5772ea9d938bc4c.png\" /></p><p></p><p>对于大型的企业，智能化升级的需要显然不是几个模型就可以满足的，他需要一整套完整的企业级解决方案。例如大型企业将业务布局在全国及下属的各类子公司，如何管理企业中不同层级的子公司对 AI 能力的建设、使用和共享，如何协同云边端的 AI 能力，都需要企业解决方案来实现。</p><p></p><p>千帆大模型平台和百度 AI 中台解决方案可以帮助大型企业轻松实现从总公司到下属公司 AI 能力全生命周期的管理和建设，帮助企业实现智能化升级。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14a2d029566eba1f970eeb21bf482bc6.png\" /></p><p></p><p>本次课程为大家全面分享了从机器学习到百模大战的发展路径、LMOps 相关概念和关键技术、以及千帆大模型平台的特点，以及产业实践四个方面的内容，希望能够帮助大家更好地理解大模型发展史与应用场景。在接下来的日子里，《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课还将持续“营业”，请大家持续关注后续章节精彩内容！</p>",
    "publish_time": "2023-08-11 13:40:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌的反“背锅”文化",
    "url": "https://www.infoq.cn/article/1z4uxdh07hWDX6r4P1eE",
    "summary": "<p></p><blockquote>这个“锅”，谁都别背。</blockquote><p></p><p>&nbsp;</p><p>谷歌最早在 2003 年就提出了 SRE（Site Reliability Engineer，站点可靠性工程师）概念，经过 20 年发展，SRE 不仅是一个岗位，更是一个体系化的工程，并已逐步在越来越多的大型互联网公司落地。</p><p>&nbsp;</p><p>日前，谷歌 SRE 工程师 Michelle Brush 在 InfoQ&nbsp;英文站发表视频演讲，分享了谷歌&nbsp;SRE 工程的关键策略——反“背锅”文化。Brush 认为，反“背锅”文化并不是完全对个人无问责，而是构建一种持续改进的文化，并赋予人们权力，创建一种能让正确行为持续发生的环境。</p><p></p><h2>问责：以结果为驱动就是对的吗？</h2><p></p><p>&nbsp;</p><p>构建反“背锅”文化的第一步，是先明确“问责”二字。</p><p>&nbsp;</p><p>不少互联网企业会经常提到“问责”二字，但实际大家对“问责”的理解并不一致。有人认为“问责”是指因为错误的决定而受到惩罚，或者是必须面对自己行为的后果。Brush 则认为，“问责”是组织保证你会被根据自己为所负责的事情而做出的相关行为，所产生的后果进行评估。</p><p>&nbsp;</p><p>这里面每个关键词都有其存在的意义。比如保证、负责、行为三个关键词代表了组织的保证。在组织与个人之间存在某种信任或合约，保证评估不仅是根据结果，还会根据其在需要承担一定责任的情况下做出的行为。这些要素都很重要，不能只挑其中两三个执行。</p><p>&nbsp;</p><p>然而在实际工作中，有些组织忽视保证、负责、行为，只关注后果、评估，试图将重点放在对员工的评估上，仅根据结果对决策进行评估。这种情况通常会出现在那些自称“以结果为驱动”的组织中。有的组织自称“以结果为驱动”是在试图表明自己会在工作的开展方面给予员工自主权，不会微观管理或过度规定；有的则会通过这句话表明自己在乎的有且仅有结果，不在乎是这个结果是如何达成，中途又断了多少退路、错待了多少人。</p><p>&nbsp;</p><p>知名牌手 Annie Duke 曾在《用赌注思考》一书中提到组织中的领导者和扑克玩家在早期常见的一种糟糕的倾向——“结果论”——无论行为如何，都将决策的好坏与结果挂钩。如果一家组织只看得见结果，那他们永远没法真正学到东西，因为在任何事中都有运气的成分存在。</p><p>&nbsp;</p><p>Brush 认为，世界很复杂，系统很复杂，组织也很复杂，大家没办法掌控所有变量。有时会相信通过概率、可能性或是一些侥幸，才能让事情完全正确或完全错误。这种情况下，如果只评估结果，那么你会对组织以及什么可行、什么不可行有非常糟糕的理解。这也是为什么我们需要看得见所有要素，需要在评估的过程中包含结果和行为，而组织也需要保证他们会看重所有的要素。</p><p></p><h2>评估与提升：将行为和结果相结合</h2><p></p><p>&nbsp;</p><p>如果企业不以结果为驱动，那么该评估员工的哪些行为？如何判断行为的奖惩？</p><p>&nbsp;</p><p>Brush 认为，这是价值观的问题。组织或团队都会有价值观，比如更重视交付速度、成本降低，或者是创新等等。公司通常会从这些价值观中衍生出一些规范，让员工明白哪些个人行为在这个价值体系中占据了一定地位。如果重视这些行为，确实能带来良好的结果。反之，如果还是只看重结果，不仅看不到什么该做、什么不该做，甚至可能还会遇到这样的场景——人们会因为做了自己想要的事而得到奖励，而不一定是因为做了正确的事而被奖励。</p><p>&nbsp;</p><p>如果只按照行为奖励，比如“不相信结果，我只想确保人们永远在做对的事，我要为此奖励他们”，那么你很可能让组织的创新和效率为此受损，因为你周围的世界会不断变化。如果你一直在做自认为可行的事，那么即使这些不再能行得通，你也不会注意得到。</p><p>&nbsp;</p><p>将行为和结果相结合后，如何再进一步提升？</p><p>&nbsp;</p><p>Brush 认为，要想提升工作，就要去回顾成功的经历，要知道在自己的系统中，什么算是成功，一切顺利时又是什么样的？目前有哪些行为？如何持续？如何重现？如果没法重现，就要看员工每天都在做什么，是有他们的什么行为保证了成功的发生？接着，还要仔细地观察并思考，假设如果停止这些行为会发生什么？结果会有什么变化？人们不再进行这些行为之后组织会有什么变化？</p><p>&nbsp;</p><p>这样做的目的是小心幸存者偏差，不能只关注正向结果，认为所有导致正向结果的事情都必然是实现正向结果的一部分。此外还要关注险些成功的场景，要知道是什么让你侥幸避免了一场灾难的发生，并确保这些幸运能够重现。还要关注事故、错误、意外、中断场景，并扪心自问，情况是否会变得更糟？没有变得更糟是因为侥幸吗？如何做得更好？不从预防发生的角度来看，而是如果这类事情再次发生，组织将如何采取不同的应对措施？组织要如何更好地确保每个人都能在这个时间点上做正确的事？</p><p>&nbsp;</p><p>在过去，Brush 见过不少类似的情形：组织在分析时花了太多的时间考虑要如何阻止事情发生，而忘记了人总会犯错，软件总会有缺陷，硬件总会出故障，过度地预防会让人失去对组织中软件更为弹性的丰富理解，而这才是大家真正想要的。此外，如果人们花费了过多时间在预防上时，可能有时会不慎回到根因分析的模式中。</p><p></p><h2>根因分析：一定要找个“背锅”的人吗？</h2><p></p><p>&nbsp;</p><p>提到根因分析，有人认为，若想在组织中实行问责的制度，就必须让人们为自己犯下的错误负责。Brush 在职业生涯早期时也是这么做的，但她发现，这种观念的本质是，我们期望那些导致不良结果的行为或决策的参与人，成为推动解决方案的人。无论是否有意，我们都是将追溯总结视作了惩罚。</p><p>&nbsp;</p><p>这是因为要想让一切运转起来，就需要找到一个“背锅”的人，一个承担责任的人。要做到这点，有时候得将一切过度精简到一系列的事件，从而找到一个决策点，这样所有人都能一致认为这个决策点才是问题的根本所在。之后再找到是谁该为这个决策点负责，并将其称之为是“根因”。</p><p>&nbsp;</p><p>为了保证这类事件不再发生，企业把工作任务交给背锅的人，让他们写报告，把所有事情都承包了。很多时候，除了这些工作，企业还会再加上一些可怕的流程，比如到领导面前解释发生的一切，可能还会再挨上两句骂。所有这些，无论有意与否，本质上都是惩罚，都是行为的后果。</p><p>&nbsp;</p><p>这并不是一个好方法。在人们犯错时给予惩罚，通常会导致他们在犯错时不告诉你，“找根因”又会让一切变得更糟。因此，不能一味地认为追究责任就是让造成问题的人去解决问题，去写解决问题的方案。</p><p>&nbsp;</p><p>这又引出了关于大棒与萝卜的讨论。每个组织都有各种宏观和微观的奖惩措施，这些奖励是萝卜，而后果则是大棒。多数时候的后果和行为管理期望都是如此，为不顺利的事情承担后果，为顺利的事提供萝卜或奖励。</p><p>&nbsp;</p><p>那么问题来了，惩罚真的有效吗？这其中有很多研究和探讨的空间，但一般来说，答案是否定的，正向增强往往比负向增强要更有效。要想在组织中建立正确的行为，你得有正确行为的标榜。你需要体现出组织鼓励正确的行为，但不一定要建立惩罚或后果文化，因为后者会导致人们不想惹麻烦而对你有所隐瞒。</p><p>&nbsp;</p><p>每个人都有自己的平均表现，在行为进行或结果实现的过程中，处处都存在可变性。对于平均表现水平而言，如果有人在长时间内将一切都做到好，那么在另一段时间内这个人大概又不会做得很好。这种是符合人类表现的钟形曲线的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0912fcecfd327b7b3bfe5284bea18b0.png\" /></p><p></p><p>有趣的是，真的有人在试图解决故障或事故时踩过这个坑。他们在做事故分析时，可能会想出一个绝妙的主意，比如说通过减缓发布速度或者添加额外审查之类的，试图避免这种事故在未来重演。而在落实这个主意后，一段时间内事情似乎是在向好的方向发展，但这也是在复杂度与曲线的拟合，一切只是又在向均值回归而已。</p><p>&nbsp;</p><p>此外，幸存者偏差也存在于这种场景下。如果我们光是看到进展顺利的事情，就称这种行为或结果致使了事情的正向发展，那么我们其实是忽视了在同样行为下，进展不顺利的情况。一般来说，正向增强胜过负向增强，为了让人们做正确的事，我们宁可要奖励也不要后果。</p><p>&nbsp;</p><p>这背后，其实更关键的是内在动力胜过外在动力。让人们因为对自我的认同感而去做正确的事，而不是依赖外界的奖励或后果，这样可以提高成功率。但即使在我们发现了这种模式并试图改变行为从而获得不同的结果，或者说是组织想要学习并做出不同的举动，激励方式也不应该是在员工做出正确行为时立刻给予奖励，或者是在他们没做对时立刻进行惩罚。我们实际想看到的是如何为员工创造动力，让他们能自发地去做这些事。</p><p>&nbsp;</p><p>《如果你非常聪明，为什么你不快乐？》一书中提到，人们处于一种流动感时会感到快乐。流动感也就是我们所说的状态，本质来说就是人们效率超高且完全沉浸于工作之中的时候。我们能给予人们的流动感越多，便越能让人们想要去做正确的事。也是就是说，对这种流动感的破坏越大，人们就越容易做错事，因为我们没有为他们创造动力去做正确的事。如果我们想要激励人们去做对的事，我们需要想明白该怎么从这一角度入手，怎么利用这四点（目的、自主、掌控、流），让人们自发愿意去做正确的事。</p><p>&nbsp;</p><p>在《如果你非常聪明，为什么你不快乐？》这本书中，还探讨了人们会因为什么才会更快乐。人们不会因为升职加薪或是其他什么原因而更快乐，因为这类事情发生的频率太低了，所以它们不是个好的激励因素。通过对比得来得快乐或幸福也是非常短暂的，因为你很快便会常态化现在的处境。你需要减少对这方面的关注，而将驱动力视为重心。但这也不意味着你不该给员工升职加薪的机会，升职加薪还是该有的，但却不该是驱动员工每天做正确事的动力。</p><p>&nbsp;</p><p>除了对升职加薪这些短时效性奖励的过度依赖，有些企业会对目的过度依赖。</p><p>&nbsp;</p><p>Brush 曾在一家医疗机构工作过，这家机构常常会把医生或护士请来，给工程师讲讲糟糕代码的后果，比如代码错误可能导致病人死亡，试图借此让工程师们写出质量更高的软件。或许企业会以为这样能激励工程师，给大家一个强烈的目标和使命，大家就会自然而然地做正确的事。但这实际上，这样的方式非常打消大家的积极性。一是这会让大家产生焦虑，大家只想尽可能降低自己的责任，减少对发生结果的内疚感，出事了也不会被指责；二是这会让大家感到愤怒，这是在居高临下地暗示大家，写这个软件的原因不是构建系统需要优化，也不是需要更好的测试工具，而是在说“你们不了解情况，如果我们愿意告诉你怎么做，你们才能做得更好”。</p><p>&nbsp;</p><p>因此，企业不要把奖励看作是为了让人们做出正确事而悬在他们头顶的萝卜，而是用日常或更为频繁的奖励以填补晋升或补贴周期之间的空隙。人们其实非常喜欢惊喜和愉悦的事。在 Brush 负责的一个项目中，人人都在为追赶进度加班加点，Brush为了表达对项目中大家努力工作和性能提升的感激之情，给所有的人都买了一辆 Hot Wheel 小车。直到今天，Brush 还能收到大家的消息，告诉 Brush 他们在清理橱柜时看到了这辆小车的图片。</p><p></p><h2>如何理解反“背锅”文化？</h2><p></p><p>&nbsp;</p><p>Brush 认为，反“背锅”实际是指构建一种持续改进的文化，或至少要与持续改进的文化相辅相成，而建立这种文化则必须赋予人们权力，创建一种能让正确行为持续发生的环境。你需要奖励提升而不是一成不变。</p><p>&nbsp;</p><p>比如，有时组织里会设置一个高不可攀目标，然后等到人们达成这个目标时才给予奖励。但这会导致人们缺乏改进的动力。状态好的人达成了目标后会开始摸鱼，毕竟他们已经完成了任务，而离目标还有十万八千里的人可能会觉得这个目标太难了，他们下辈子都没办法完成，并最终放弃了实现这个死目标的动力。</p><p>&nbsp;</p><p>企业希望能奖励改进，为发展和探索留出时间，让人们能够发挥创造力，拥有掌控的能力，拥有用不同方法完成任务的自主权。此外，企业也要不断去寻求并听取反馈意见，寻求批评建议，让批评成为文化的一部分，真正地去奖励改进，变得更好和保持很好都是可以接受的，即使是做到最好的团队也应该寻求建设性的批评和反馈。</p><p>&nbsp;</p><p>有些人太害怕去责怪他人，他们其实并不想去批判性评估任何决定，只想假装一切正常，或者只想创建一个只会做对事的环境。这两种原因相结合，就形成了“不健康的积极”——组织所创建的文化中，只希望看到进展顺利的事情，就好比是“不允许失败”的文化。这样的结果就是，人们不再鼓励或直接拒绝建设性的批评。</p><p>&nbsp;</p><p>在只鼓励积极信息的环境中，人人都在赞美他人或者认为一切都很顺利、都很好，一旦有人站出来说事情其实进展得并不好，有需要面对的风险没有解决，会有问题出现，那么这个人就会被这种文化所制裁。因为在这种文化内，没有人会这么做，也没有人会去提及这些东西。而如果人们会因为提出风险或者承认失败而受到惩罚，那么他们也将被要求对任何决定做出毫无转圜余地的承诺，只能承诺不能否定。这会导致非常不健康的积极环境。</p><p>&nbsp;</p><p>比如，有些企业的文化是“要想反对，必先提议”。这听上去没什么问题，要是有人想提出一个风险点，那他们肯定不能光是嘴上说说，他们得给出正确的做法，给出解决方案。但这么做的问题在于，人们能发现风险点，但这不是他们的工作内容，他们有自己的任务，也有其他的工作，他们只是正巧发现了某些事情可能会进展不顺。</p><p>&nbsp;</p><p>这种“要想反对，必先提议”的文化会将解决问题的重担压在提出问题的人身上，指出风险点意味着工作量的增加，意味着额外的工作内容，人们不会想为额外的项目而操心。千万小心不要创造出这种有风险提出者承担解决责任的环境，听取批评性反馈可以，建设性建议则是更好。当然，即使是没有具体的实施计划，建设性建议可能会改善组织或系统的整体成功。</p><p>&nbsp;</p><p>要想正确地做到反“背锅”，需要具备一种“容许框架”的素质，也就是说，你必须要认可他人的能力，要默认他人是能够做对事的。扪心自问，是什么让这个决策在当时看来是最优的选择？在人们面对困境时，所有制约因素都自相矛盾，也许他们时缺乏特定技能，也许他们具备应有的技能但系统却是一团乱麻。他们或许不知道该怎么办，只是试试看会有什么结果。错不在他们，也不是他们做出了错误的决策，只是系统如此，他们别无他法。</p><p>&nbsp;</p><p>很多组织会将其归结于缺乏培训。一些组织花费了大价钱编写培训内容并让所有员工都去上课，祈祷着人们能记住教训，并在下次同样的事情发生时能长记性，但同样的事并不会再次发生。不要指望通过培训让人们掌握所有知识并做出正确的决策。相反，应该从系统的角度思考问题，人类作为系统的一部分，他们在创造、使用技术，他们身处特定环境之中，有自己要遵循的制度和程序，也有企业所制定的激励措施。这所有的一切都由反馈回路相连，从而形成一个庞大的社会技术系统，允许人们各行其是。</p><p>&nbsp;</p><p>在明白这点后，企业可以进入与人相关的讨论，找出哪些部分需要调整才能让下次的结果更好。有的时候答案是技能的开发，有的时候则是因为企业送员工走上了失败之路，因为企业对员工提出了要求，却没给他们准备的机会，在后者发生时默认地选择线上或导师培训，这些项目的投资回报率非常低。</p><p></p><h2>写在最后</h2><p></p><p>&nbsp;</p><p>总的来说，反“背锅”的最终要诀，其实就是评估行为和结果，而不是二者选一。你需要回顾进展顺利、勉强成功，以及进展不顺的事情，而不是紧盯着那些进展糟糕的事，需要避免因果律信条和根因分析，确保没有将一切过于精简从而导致问责式结局。</p><p>&nbsp;</p><p>如果企业希望人们能认领解决方案并做对事，应当使用目的、自主、掌控以及流，这些才是人们日常工作的动力源泉。企业需要创建持续改进的文化，消除由可视性驱动的规则、规定和衡量标准。最后，将决策转移到信息方向，而非反向而行。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/presentations/blameless-accountability/\">https://www.infoq.com/presentations/blameless-accountability/</a>\"</p>",
    "publish_time": "2023-08-11 14:18:08",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Python 失宠！Hugging Face 用 Rust 新写了一个 ML框架，现已低调开源",
    "url": "https://www.infoq.cn/article/fMBmytGX8aFOKzrN3pea",
    "summary": "<p>近期，<a href=\"https://www.infoq.cn/article/DDgl51H5dC9V7dHXrvIN\">Hugging Face </a>\"低调开源了一个重磅 ML 框架：<a href=\"https://github.com/huggingface/candle\">Candle</a>\"。Candle 一改机器学习惯用Python的做法，而是 Rust 编写，重点关注性能（包括 GPU 支持）和易用性。</p><p>&nbsp;</p><p>根据 Hugging Face 的介绍，Candle 的核心目标是让&nbsp;Serverless 推理成为可能。像 PyTorch 这样的完整机器学习框架非常大，这使得在集群上创建实例的速度很慢。Candle 允许部署轻量级二进制文件。另外，Candle 可以让用户从生产工作负载中删除 Python。Python 开销会严重影响性能，而<a href=\"https://www.backblaze.com/blog/the-python-gil-past-present-and-future/\">GIL</a>\"是众所周知的令人头疼的问题。</p><p>&nbsp;</p><p></p><h2>Rust 真的可以吗？</h2><p></p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/l9UJDcD4E1DbGuvOs15T\">Pytorch </a>\"框架是用 Python编写的，API 也是基于 Python 的，这让用户上手开发会非常快。另外，Python 本身就是一种简洁而易于学习的编程语言，很适合初学者和专业开发者使用。</p><p>&nbsp;</p><p>但基于 Python的Pytorch框架问题也很明显。相对于一些静态图框架（如 TensorFlow），Python 在某些情况下可能会导致性能问题。Python 的全局解释器锁（GIL）可能会在多线程的情况下影响性能，尤其是在涉及 CPU 密集型任务时。Python 的解释性质还可能会引入一些运行时开销。另外，将基于 Python 的 PyTorch 模型部署到生产环境中可能需要一些额外的步骤，不如其他编译型语言那么方便。</p><p>&nbsp;</p><p>显然，Hugging Face 一直在寻找解决办法，它给出的答案是用时下最快的语言Rust重写一个ML框架。“最酷的是，这是来自 Hugging Face 的，不仅仅是某人的爱好项目。”有网友赞叹道。实际上，许多 HF 生态系统已经使用 Rust，例如<a href=\"https://github.com/huggingface/safetensors\">safetensors</a>\"、<a href=\"https://github.com/huggingface/tokenizers\">tokenizer</a>\"。</p><p>&nbsp;</p><p>不过，Rust的难度也让一些开发者望而却步，“编写Rust是一件艰难的事情，你必须跳来跳去，花更多的时间思考编程语言的抽象，而不是思考要解决的问题。所以，我现在还不着急重写任何Python的东西。”</p><p>&nbsp;</p><p>开发者“fooblaster”指出，Pytorch 部署模型有多个生产路径无需Python解释器，如torchscript和libtorch，或是更烦人的路径如onnx export和onnx runtime，所以不需要Rust来解决这个问题。另外很人知道，现在可以使用 C++ 编写 Torch 训练代码，并与推理和训练共享一种通用语言。</p><p>&nbsp;</p><p>对此，开发者“malcolmgreaves”表示，这些是使模型推理独立于Python的伟大技术。然而，总是有大量的预处理、后处理或其他业务逻辑需要围绕模型推理。这种事情需要在通用编程语言中完成，因此 Python 经常被使用（因为支持模型的代码通常是由同一个人编写的，并且这些代码很可能是Python，因为您的模型训练和eval代码很可能也是Python）。这就是非Python PL（如Rust）可以在简化生产部署/维护以及具有真正高效的生产推理方面发挥巨大作用的地方。</p><p>&nbsp;</p><p>当然，也有开发者为Python打抱不平。</p><p>&nbsp;</p><p>“任何编程语言在生产环境中都可能是一种痛苦。Python 的缺点之一也是它的优点之一。使用 Python 或 JavaScript 等‘混乱’语言很容易陷入糟糕的生产环境，因此避免这些痛点的工具已经非常成熟。有了这些，Python 在生产中就会变得很棒。”开发者“devjab”进一步表示，“是的，这将要求您的组织做出一些严肃的 CI 文化决策并强制执行。但问题是，虽然使用某些编程语言可以不必如此，但当企业达到一定规模时，总是会需要它们。因此，更早建立这个流程就会容易得多，而且如果您认真使用 Python，早就会这样做了。我认为，如果在生产环境中工作很痛苦，那么问题不在于技术，而在于流程。”</p><p>&nbsp;</p><p>实际上，业内一直在努力解决 Python 带来的问题。</p><p>&nbsp;</p><p>5月份，LLVM 和 <a href=\"https://www.infoq.cn/article/apple-swift-nio\">Swift</a>\"编程语言联合创始人 Chris Lattner 创办的新公司 Modular AI 发布了一个名为 Mojo 的新编程语言。Mojo 将 Python 特性与 C、C++和 CUDA 的系统编程功能结合了起来，并通过其所谓“极限加速”与其他 Python 速度增强方案区分了开来。据悉，凭借着硬件加速，Mojo 在运行 Mandelbrot 等数字算法时比原始 Python 快上 3.5 万倍。</p><p>&nbsp;</p><p>另一方面，Python 自身也在做改进。最近，Python 终于宣布要删 GIL，Python默认版本将逐渐过渡到无 GIL 版本。这一决定能否巩固其在机器学习领域的地位，也需要时间验证。</p><p>&nbsp;</p><p></p><h2>与PyTorch对比</h2><p></p><p>&nbsp;</p><p>据悉，当前Candle 已经支持如今的前沿模型，像Llama2。经过改写的模型，比如Llama2能够方便、快速的运行在容器环境，甚至可以运行在浏览器中。Candle 结构包括：</p><p>&nbsp;</p><p>Candle-core：核心操作、设备和Tensor结构定义。Candle-nn：构建真实模型的工具。Candle-examples：在实际设置中使用库的示例。Candle-kernels：CUDA 自定义内核；Candle-datasets：数据集和数据加载器。Candle-Transformers：与Transformers相关的实用程序。Candle-flash-attn：Flash attention v2层。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/ac5cd111a387912038fc3285a2a01ae4.png\" /></p><p></p><p>Pytorch 和Candle 对比</p><p>&nbsp;</p><p>&nbsp;</p><p>该项目正在处于快速迭代过程中，更新非常频繁，很多功能在不断开发中，目前包含如下功能和特点：</p><p>&nbsp;</p><p>语法简单， 风格与 PyTorch 相似。CPU 和 Cuda Backend：m1、f16、bf16。支持Serverless（CPU）、小型和快速部署支持 WASM，可在浏览器中运行模型。模型训练使用 NCCL 进行分布式计算。开箱即用的模型：Llama、Whisper、Falcon、StarCoder...嵌入用户定义的操作/内核，如 flash-attention v2。</p><p>&nbsp;</p><p>对于Hugging Face的这一新 ML 框架，大家有什么感想或使用感受？欢迎在评论区分享！</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://github.com/huggingface/candle\">https://github.com/huggingface/candle</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=37049198\">https://news.ycombinator.com/item?id=37049198</a>\"</p>",
    "publish_time": "2023-08-11 14:32:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "京东零售AI 方向产品总监李博，确认担任QCon北京AIGC 产品设计专题出品人",
    "url": "https://www.infoq.cn/article/AqoqpSJ4GICvRQyyQo2J",
    "summary": "<p>9 月 3 日 - 5 日，在 <a href=\"https://qcon.infoq.cn/202309/beijing?utm_source=infoqweb&amp;utm_medium=teacheart&amp;utm_campaign=9&amp;utm_term=0811&amp;utm_content=libo\">QCon 全球软件开发大会（北京站）</a>\"，京东零售 AI 方向产品总监李博将担任「AIGC 产品设计」的专题出品人。在此次专题中，你将了解到大模型赛道的新产品，以及围绕着大模型升级迭代的老产品。</p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/track/1561?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0811&amp;utm_content=libo\">李博</a>\"，现任京东零售 AI 方向产品总监。AI 领域畅销书作者，从事 AI 产品设计工作 10 余年，国内早期 AI 方向创业者，从 0 参与了阿里 AI 平台建设，服务过数十家海内外 500 强企业。目前主要工作是京东零售业务方向数智化产品体系建设，大语言模型在零售场景的应用探索。</p><p></p><p>相信李博的到来，可以帮助提升此专题的质量，让你学习到大模型赛道的新产品、围绕着大模型升级迭代的老产品，以及新老产品中的设计逻辑与思考。</p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-08-11 15:11:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "QQ NT全新重构，探寻24岁QQ大重构背后的思考",
    "url": "https://www.infoq.cn/article/99SUIbZtx2BE1fwVQjwG",
    "summary": "<p></p><p>采访嘉宾 ｜ 王辉、吴浩、陈俊文</p><p>编辑｜Tina</p><p></p><p></p><blockquote>本期节选自<a href=\"https://www.infoq.cn/minibook/b8qPXJ8SuGLwTDVO7sSJ\">《中国卓越技术团队访谈录》（2023年第二季）</a>\"。此次我们深入采访了腾讯向量数据库团队，与他们一同探讨了腾讯云向量数据库背后的故事以及他对于向量数据库现在所面临的挑战和未来发展趋势的思考。</blockquote><p></p><p></p><p>在瞬息万变的互联网行业中，年过二十四的 QQ 堪称超长寿的产品，见证了中国互联网崛起的完整历程。然而，如今这个元老级产品经历了一次从内到外彻底的重构。</p><p></p><p>在这次重构中，QQ 选择了 Electron 作为 UI 跨平台开发框架。尽管 Electron 被 Slack、Visual Studio Code 和 Discord 等大型产品广泛使用，但也引发了一些网友的担忧，例如内存占用、安装包体积和启动速度等方面的问题。</p><p></p><p>好奇于 QQ 的决策，于是我们采访了 QQ 技术团队，窥探这次变革的脉络，揭示出那些潜藏在背后的思考。</p><p></p><p></p><h2>QQ 重构背后的思考：24 岁的 QQ 有什么样的技术债？</h2><p></p><p></p><p>QQ 的第一个版本发布于 1998 年，在 Windows 技术栈的基础上用纯原生的方式开发，在当时互联网带宽非常小的情况下，QQ 将安装包控制在了只有 200K 左右。2007 年后智能手机开始露出苗头，腾讯行动得比较早，部分前端技术开发开始转型到了移动端，在桌面端， QQ 随着业务和组织的发展，针对三大操作系统陆续组建了三支不同的研发团队，各自负责自己的一套代码。</p><p></p><p>三端不同代码，老产品历史包袱，加上移动时代研发人员的转型，导致桌面 QQ 维护成本很高。QQ 技术团队介绍，拿之前的桌面 QQ 为例，WindowsQQ 以前的 UI 框架用的是腾讯自研的 GF 框架，10 多年了，GF 这个框架文档还不全，新加入这个项目的团队人员，要基于这个基础框架去做一些事情，是效率很低的一件事情，慢慢的就没有人愿意去用这个框架了。简而言之，就是技术债。</p><p></p><p>旧版的桌面端 QQ，Windows 的功能最丰富，macOS 次之， Linux 功能非常简洁。比如“屏幕共享”这个功能，移动端有，Windows 端有，但是 macOS 端是没有的。那用户就会遇到一个问题，像 macOS 端无法与其它端 QQ 用户一起来使用这个功能。</p><p></p><p>“多端不统一不利于用户对于 QQ 的统一认知。我们这次的架构升级就是想尽量通过一套核心代码去拉平所有平台的体验，让它具有更好的可维护性和可扩展性，让桌面 QQ 能够更好地迭代产品交互和功能，升级用户体验，再次焕发生长的生命力。”</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4c/4c1a94ef60e071debc5d54d17b2beb23.png\" /></p><p></p><p>于是 QQ NT 项目是在 2022 年 3 月份正式启动， macOS QQ 在 6 月份开始发布内测， 9 月份正式上架了 App Store，迭代了几个版本之后，QQ 团队就同步开发 Linux。在 2022 年，QQ 发布了新的 macOS 和 Linux 版本，包括 QQ 后台其实也做了很大的改变和重构，核心系统做了全新重写，云原生成熟度也得到了很大的提升。从 2023 年开始，QQ 团队聚焦做 Windows 端的开发，在 3 月底就开始内测，7 月初上架官网。同时移动端 QQ NT 也在 7 月初完成了核心系统的重写和全量升级。在目前全新的框架设计下，无论是核心系统、功能迭代还是设计语言上，都可以尽可能地“原子化”，来让 QQ 后续更好地迭代功能。</p><p></p><p></p><h2>重构之路：最大的压力与挑战源自何处？</h2><p></p><p></p><p>“QQ 的重构其实是两方面的重构：一个是面向复杂业务的梳理重构，一个面向工程技术债的全新技术重构，重构之路也是两者相互伴随的过程。”</p><p></p><p>首先，在整个 QQ 重构过程中最大的挑战来自于 QQ 功能的复杂化，QQ 有很多十分复杂的历史功能，这些功能模块也曾经由非常多不同的人经手负责过。其中哪些功能是不合理的或没有价值的，如何去做取舍往往是最难的。“虽然技术上我们做了很多事情，但技术上的实现或许并没有那么难，我们处理起来更有经验和从容。相比于技术的复杂度，业务上的往往需要考虑的更多，这本身就是很大的挑战。”</p><p></p><p>因为 QQ 已经是近 25 年的产品了，有很多细小复杂的功能。虽然这些功能看看起来很小，但用户量其实又很大，稍微改动可能就会有很多的用户反馈，QQ 团队都得非常的关注。仅从产品功能角度上看，有些功能本身就已经是很重的负债，而 QQ 团队内部有一个叫做“QQ 节能计划”的项目，会有比较严谨的项目流程去评估是否需要下架。</p><p></p><p>技术上重构也有不少挑战，这次重构是一次跨平台的重构，而在多个平台里面比较有挑战则是 Linux 平台。</p><p></p><p>作为程序员，很多人免不了要跟 Linux 打交道。但是这么多年来，对于使用 Linux 系统的用户来讲，有一个特别让人烦恼的问题，那就是没有一个好用的 IM 聊天工具。被寄予厚望的 QQ，此前在 Linux 版本上功能也没有 Windows 和 macOS 版本全面，迭代速度也明显慢过其他两个版本。业界甚至猜测 Linux 第一个版本是由腾讯实习生所写，毕竟这个说法进一步加重了其初版的“简陋”特性，也为其“停更”的原因提供了更合理的解释。</p><p></p><p>QQ 技术团队表示，较之另两个版本，Linux 版本的研发最为复杂：一方面操作系统本身很多碎片化，市面上有非常多的发行版，也不缺乏一些千奇百怪的版本；另一方面因为机器运行环境或编译器的缺失，使得解决适配问题的难度很大。许多发行版相关的机器和开发环境实际上他们并没有，有时还需要外部公司帮助进行一些测试工作。由于没有相应的开发环境，一旦出现闪退等问题，解决难度自然会变得更大。此外，有时候需要与国产操作系统厂商进行特殊的合作，甚至需要对方寄送特定的编译好的代码库，但前后往往会花费一个月的时间才能收到。而在本次重构之后，“Linux 功能跟 Windows 一样多了”。</p><p></p><p>技术上的另一大挑战便是外界对于 QQ 桌面端使用 Electron 的质疑，尤其是内存方面。外界有些用户在没有使用和分析的情况下对此发表一些夸大和否定的言论，也确实给 QQ 技术团队带来不小压力，但他们却始终坚定选型方向，也相信其中的问题可以被攻克和解决。</p><p></p><p></p><h2>技术选择之争：为何 QQ 选择 Electron 而非纯 Native 技术栈？</h2><p></p><p></p><p>确实当时有很多人在问，为什么 Windows 不用原生去实现？为什么不用 Qt？</p><p></p><p>“首先不太想和以前一样，Windows、macOS、Linux 三端各由一个团队分开负责。在国内这种人才环境里面，相关的纯原生的开发人员其实非常难招了，桌面端的人才稀缺，同时也投入比较大。</p><p></p><p>而对于 Qt 技术栈，他们首先考虑的其实还是人才的问题，国内熟练 Qt 技术栈的人非常少。如果对这个框架不了解，使用它反而是一个负向作用。</p><p></p><p>至于微软的 Webview2，从本质上讲，Webview2 和 Electron 并没有太大的区别，只是相对在其中打包了一些微软自身的优化措施，其他方面也不是很完善，而且还无法跨平台。虽然内存方面相较于 Electron 做了更多的优化。但据了解，比如微软 Teams 也没有完全切到 Webview2。并且由于它没有开源，因此也没有办法基于 Webview2 做定制优化。</p><p></p><p>包括 Flutter，QQ 团队表示他们当时也有过调研。他们放弃的一个原因是 Flutter 在桌面端的完善程度并不高，也担心标准化的问题。虽然当前 Flutter 非常流行，但谁也说不好这是不是“2015 年的 React Native”。大家担心随着时间推移，这套技术可能会失去维护支持，因为本身 Google 使用 Flutter 的占比也比较小。</p><p></p><p>“虽然它很热，但我们历史上踩过了很多很多非标准化的坑，一旦某个技术栈热度一过、维护力度不够，它就会成为全新的负债，做选型时必然也是避免再有类似经历。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14d7d89fffd5a3058cba8e49fd848a43.jpeg\" /></p><p></p><p></p><p>至于为什么最后选择 Electron，QQ 技术团队表示主要是基于以下几个考量：</p><p></p><p>首先最看重的是框架成熟度和技术栈的标准化。Electron 基于 Web 技术栈，有足够低的上手和使用成本，不需要为了使用框架本身，还需要投入额外巨大人力成本去做基建和周边工具链的建设，以前在 RN、Flutter 的实践上都有类似的情况。而使用 Electron，现有的 Web 前端的大部分基建都可以直接复用，而且使用 Web 开发 UI 的效率，在主流技术栈里算是很高的了。至于迭代效率我觉得从新版桌面 QQ 功能的迭代速度就可以证明，这放在以前是完全办不到的。另外由于 Web 技术栈是标准化的，假如 Electron 修改开源协议或者要闭源了，他们也能很方便的去写出一套类似的框架。只不过现在已经有开源的了，没必要再去重复建设一个。而且随着 Web 标准长久发展，Web 技术栈也不会有大的问题，而且还会越来越好。</p><p></p><p>其次是技术经验及人才储备，技术选型是否适合当前团队也是一个很重要的考虑点，团队是否有相关的技术积累，是否有人才储备来持续投入这个技术栈。Qt 的确在性能上是一个很好的选择，但目前团队对 Qt 没有太多积累，基建基本没有，而且相关人才其实比较匮乏，招聘就更难了。而当前 QQ 技术团队 Web 前端团队还是有比较多的积累，在 QQ 频道项目中，也完整验证了 Electron 的技术可行性。</p><p></p><p>最后就是 Electron 具备的桌面端跨平台的优势。但 QQ NT 架构并不是仅指 Electron，Electron 主要是作为 UI 跨平台的框架，只是占比很小的一部分，并且 QQ 桌面端不是全部用 Electron 实现，QQ NT 最核心的部分还是 QQ 底层通用抽象的模块，称之为 NT 内核，包括核心登录、消息系统、关系链、富媒体、长连接、数据库等等模块，完全用 C++ 实现，全平台通用。因此底层是完全跨平台的架构，而 Electron 只是上层桌面端 UI 跨平台较薄的一层。</p><p></p><p>“其实我们当时选型的时候，也的确看得到大家对 Electron 的评价褒贬不一，但我们还是有信心去解决这个问题，前期也做了一些技术的 Demo 和预研。实际上 Electron 并没有糟糕到这个地步。我们觉得可能是国内很多没有用过 Electron 的开发者，对这个框架有些忌惮。其实你到 Electron 的网站去看，还是有非常多国内外的亿级 DAU 产品都使用 Electron 框架。目前这几年主流的桌面端应用基本都选择了 Electron，如 Visual Studio Code、Discord、Slack、Skype、Whatsapp、Figma 等等，新的应用基本上也是首选 Electron，版本的迭代速度和社区氛围都很在线。”</p><p></p><p>“我们觉得不需要单纯因为口碑问题，就对这个选型没有了期待。还是要从实际出发，哪种技术栈适合你的产品，看看到底能不能有技术实力去把这个事情搞定。”</p><p></p><p></p><h2>内存优化：如何有效控制 Electron 的内存占用？</h2><p></p><p></p><p>外界之所以会觉得 Electron 内存占用高，是因为其本身是一个多进程的架构，主进程基于 Node.js, 而每个窗口都对应一个渲染进程以及 V8 实例。可以说从技术框架层面上，上手写代码很容易，但不容易去管控它的内存。</p><p></p><p>QQ 技术团队认为 Electron 的开发者更多的是前端的开发者，可能在思维上没有去考虑怎么在这样一套技术框架里，去对内存数据进行管理和管控。开发者需要从前端开发者的思维，转变为客户端开发者的思维。</p><p></p><p>综合来看，对内存的看法其实不完全是 Electron 的技术框架所导致的，更多的是门槛上、开发思维上，导致内存没有得到很好的关注和优化。其实最简单的 Electron 应用大概也就只有几十兆的内存占用。因为前端原本更多还是停留在开发即用即走的 Web 站点，很少实现一个超大客户端，缺乏控制内存的经验，所以面对 QQ 这么大一个产品的时候，你就必须非常在意内存的使用和管控。</p><p></p><p>至于优化内存的突破口，可以说是从各个层面：从消息的链路中的每条消息的收发上，数据是怎么管理，包括像窗口及会话的管理，都得精打细算，也会做一些数据本地化和一些机制的按需加载，包括渲染上他们也提出一个根本的原则：“要做到所见才占用”，既我们看到的内容才占用这一部分内存，没看到和用不到的任何场景的内存就不应该再占用，通过各种方式来去让内存达到一个设定的目标。</p><p></p><p>他们也使用了不同维度的内存分析工具，从 V8 引擎到进程，再到整个应用程序，打通整个链路进行多角度的细节分析，以此来定位内存使用的瓶颈。之后采取一系列的针对性优化策略，包括缓存策略、按需加载、优雅降级等，同时使用线上监控、自动化测试手段，包括借助开发框架、工具建设、代码审查等，来阻止性能退化。（更多细节可以参看技术文章：<a href=\"https://www.infoq.cn/article/ZkQy1ouqocjxzpCQvKTq\">新 QQ NT 桌面版如何实现内存优化探索？</a>\"）</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a7/a780f0ca4415d66fa4a07c13cbecda27.png\" /></p><p></p><p>经过一系列组合优化之后，QQ 的内存在长时间挂机的条件下，平均稳定在 220M 左右。“现在优化还是不错的，比老版本要好很多。我们认为这个难题还是可以被很好的攻克，内存并不是大家认为的这么不可控，但是也需要团队去花费相当精力去探索和实践，才能去把内存控制到一个比较理想的状态。”</p><p></p><p></p><h2>未来展望：QQ 的前端发展规划是怎样的？</h2><p></p><p></p><p>目前 QQ 的前端团队作为一个公线团队，不仅负责桌面 QQ 的研发，还有 QQ 基础运营、QQ 空间以及基于 QQ 生态的创新项目研发，有比较多的线上项目的开发与维护和内部研效工具的建设。涉及的技术栈，包括 H5、Electron、Cocos、小程序、WebGL、WebAssembly、WebRTC 等。他们也表示会继续夯实这些技术，同时也不断地打破立下的性能目标，希望让桌面 QQ 覆盖更多平台。</p><p></p><p>他们也正在积极拥抱 AI，让 AI 在质量和效率上辅助日常开发。比如：前端设计稿还原，之前更多是一个耗时的体力活，D2C 是 QQ 前端一直探索的方向，之前使用纯规则转换生成代码，在视觉还原上效果还不错，但是代码可读性和可维护性不能很好的满足预期，所以除了一些日抛型的运营活动有些使用之外，比较难扩大成果。现在 D2C 结合大模型，生成的代码质量高了很多，也能很方便的将代码与 UI 组件库做映射，达到可以在核心业务中高效使用，达到通过 AI 提升研发效率的目的。针对一些无设计稿的管理平台开发，使用 P2C 提效，目前也有了一些不错的案例。</p><p></p><p>另外，QQ 技术团队也在积极探索 AI 更广阔的应用场景，比如代码评审，基本的 Lint 检检是难以实现的，但将已经掌握的内存泄漏模式通过规则的形式给到 AI，可以很方便地给开发同学一些不错的建议，为性能看家护院提供多一道保障。</p><p></p><p></p><h2>写在最后</h2><p></p><p></p><p>QQ NT 项目于 2022 年 3 月份启动，macOS QQ 花了该团队 3 个月的开发时间，9 月份上架 App Store，迭代了几个版本后同步开始开发 Linux QQ，并于这一年的最后一天上架各 Linux 应用市场，作为给 Linux 用户的一份特殊的新年礼物。2023 年 QQ 团队开始去聚焦做 Windows QQ NT 的开发，7 月正式上架应用市场和官网。同时移动端的 QQ 从 2022 年的 Q4 开始开发，也已经完成了全量升级和发布。</p><p></p><p>另外，桌面 QQ 也是在 NT 版本中第一次支持 64 位，这需要将音视频、安全、字节码、图形库等 C++ 模块，包括 Electron 框架都重新进行编译，花费了比较大的工作量。但在 64 位系统上，QQ 从此便不再需要以 32 位应用的方式通过额外的兼容和转换来运行。毕竟额外操作会增加开销，导致性能下降。</p><p></p><p>至此，QQ 实现了多个系统平台之间架构的统一。而团队的未来规划还是不断地打破性能目标，并覆盖更多平台，同时探索更多提升研发效率的办法，加快研发速度。</p><p></p><p>腾讯 QQ 用跨平台 Electron 取代之前原生应用程序的开发模式，这一举动引发的反响确实巨大。但我们也能看出，不同于小型产品团队，在大公司里具有一定规模的产品组织架构之下，快速满足用户需求，并逐渐需要为第三、第四乃至第五种运行平台提供支持时，保持一致性和协调性并不是想象中的那么容易。而缓慢而低效，最终会令你输掉比赛。</p><p></p><p>不管使用什么跨平台开发框架，都要去选择最合适自己团队的，也因此在 Web 标准技术栈上有丰富积累的 QQ 团队才会选择 Electron。并且我们认为没有人真正讨厌 Electron，只是我们对 QQ，对国内 App 寄予了非常高的期盼。</p><p></p><p>嘉宾简介：</p><p>王辉：QQ 技术负责人</p><p>吴浩：QQ 前端负责人</p><p>陈俊文：QQ 桌面端前端负责人</p><p></p><p>内容推荐</p><p></p><p>中国卓越技术团队访谈录（2023 年第二季）深入采访了腾讯、网易伏羲、阿里云、QQ 等技术团队，呈现了这些团队在向量数据库、大模型、前端和研效等方面的技术落地、产品演进和团队建设等方面的多年实践经验和相关心得体会。<a href=\"https://www.infoq.cn/minibook/b8qPXJ8SuGLwTDVO7sSJ\">点击下载电子书</a>\"，查看更多精彩内容。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8c/8c9840b927e9bf3e4ca219e002ca9f4f.png\" /></p><p></p><p>今日好文推荐</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177675&amp;idx=1&amp;sn=fde8a2a0a811eb7082a95f3564d0c2da&amp;chksm=bdb83a988acfb38e10e0cb561b9d64b81451404b7661e6bbe925565076595047a04ff3794bde&amp;scene=21#wechat_redirect\">谷歌重磅发布多平台应用开发神器：背靠 AI 编程神器 Codey，支持 React、Vue 等框架，还能补全、解释代码</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177423&amp;idx=1&amp;sn=0ad89e54449e5196b5b0693dfe3858a5&amp;chksm=bdb8399c8acfb08a6e6c7542a59c7d54c9e02f905aa28f6519509fa896624e2cd68153d961b9&amp;scene=21#wechat_redirect\">IPv4 开始收费！新的 IT 灾难？</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177313&amp;idx=1&amp;sn=a5706d6752ac6abef1fac02e4e1c5922&amp;chksm=bdb839328acfb024006e7e4b5692873731fa885bae553a654ff5c49078cf9b804be4e97c980b&amp;scene=21#wechat_redirect\">爱奇艺VR公司业务停滞，员工或被欠薪；阿里云开源通义千问 70 亿参数模型，免费可商用；华为正式发布鸿蒙 4，接入大模型｜Q资讯</a>\"</p><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651177310&amp;idx=1&amp;sn=2f2e881ce7beabb91b794c7e80c54d15&amp;chksm=bdb8390d8acfb01ba0432391918dfa3fbb98680e6205ee05258a36ec80663453e3e37d975571&amp;scene=21#wechat_redirect\">年薪超 600 万，比技术总监还高：电影行业 AI 产品经理的崛起</a>\"</p><p></p><p></p><p></p>",
    "publish_time": "2023-08-11 15:12:37",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ACM KDD 2023：GPT-4、ChatGLM2、Llama2、PaLM2在关心什么？",
    "url": "https://www.infoq.cn/article/Xdg28mC9kCqthLHYmSSJ",
    "summary": "<p>近日，数据挖掘顶会ACM KDD 2023在美国长滩开幕。在大模型开放日上，Open AI、Meta、智谱AI、Google DeepMind、Microsoft、Intel 等公司研究学者展开深度交流。微软首席科学家 &amp; 技术院士 Jaime Teevan、OpenAI ChatGPT 团队成员 Jason Wei、智谱AI CEO张鹏、谷歌DeepMind首席科学家/研究主管Denny Zhou，以及Meta FAIR研究工程师Vedanuj&nbsp;Goswami 就大模型赋能未来工作、语言模型推理能力、Llama 2、GLM-130B 和ChatGLM、大模型范式与挑战等主题进行了分享。</p><p></p><h2>OpenAI Jason Wei：大语言模型的复兴</h2><p></p><p>&nbsp;</p><p>Jason Wei是OpenAI ChatGPT研发团队成员之一。此前他在谷歌大脑团队担任高级研究科学家，期间致力于推广思维链提示，共同领导了指令调优前期工作，并撰写关于大语言模型涌现的工作。</p><p>&nbsp;</p><p>Jason 在分享中提到，大语言模型的三个主要特征，分别是缩放法则、涌现能力和推理能力，并探讨了这些特征如何影响自己的AI研究领域。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/fd/fd831124a5c7b55e87090d85950f9ee3.png\" /></p><p>&nbsp;</p><p>Jason还介绍了他和其他人在 LLM 推理能力上所做的研究工作：</p><p>第一，思维链（Chain-of-Thought）。如果你用一个想法来提示LLM，它给出的回复质量就会飞跃。第二，自我一致性（Self-Consistency）。对多次生成进行采样调查，然后选择最常见的答案。自我一致性改善了语言模型中的思维链推理。第三，从最少到最多的提示（Least-to-Most Prompting），这要求 LLM 将问题分解成不同的任务，并从易到难进行排序。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/2758b5c96d6215eed50355fabca2f18c.png\" /></p><p></p><h2>智谱AI 张鹏 ：从 GLM-130B 到 ChatGLM</h2><p></p><p>&nbsp;</p><p>作为智谱AI（Zhipu AI）的CEO，张鹏带领团队成功开发了 1300 亿参数的双语（中英文）大语言模型 GLM-130B。自 2022 年 8 月起，该模型已开源，在准确性和鲁棒性方面可媲美 GPT-3 davinci。</p><p>&nbsp;</p><p>2023 年 3 月 14 日，基于GLM-130B，智谱AI正式发布了 ChatGLM，一款类ChatGPT的对话机器人产品。此外，其开源、紧凑的版本 ChatGLM-6B 与ChatGLM2-6B全球下载量超过 5,000,000 次，连续 28 天位居Hugging Face Trending 榜首，并在 GitHub上获得超过4.4万颗星标。</p><p>&nbsp;</p><p>最近，智谱AI还把ChatGLM升级到ChatGLM2，推出了多个参数尺寸，大幅提升了能力，基于 ChatGLM2-6B 的代码生成模型，智谱 AI 还更新了代码生成工具CodeGeeX2。</p><p>&nbsp;</p><p>张鹏介绍了智谱AI自研的GLM框架，GLM的预训练框架是一种自回归填空的方法，集成了GPT和BERT这两种预训练框架的优势,既能够实现单项注意力的计算，做序列的生成,也可以做到双向注意力的计算，做回归的模型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/5281a4afc48c88ad68b903cc20c6f72f.png\" /></p><p></p><p>在GLM基础上，2022年8月，智谱推出拥有1300亿参数的中英双语稠密模型 GLM-130B。得益于新的模型架构，GLM 在许多具有百万参数甚至更少训练步骤的基准测试中，能够在自然语言理解方面实现比 BERT 和 T5 更好的性能。训练一个 1000 亿规模的大型语言模型并非易事，智谱团队解决了许多工程问题和算法上的挑战，包括频繁且随机的硬件故障、训练稳定性等问题，相关细节都发表在ICLR 2023的论文中。</p><p></p><h2>微软Jaime Teevan：LLM如何塑造未来的工作</h2><p></p><p>&nbsp;</p><p>Jaime 是微软首席科学家和技术院士，负责公司核心产品中的驱动技术创新。她提倡人们应找到更聪明的方式来充分利用好时间，领导微软的未来工作倡议，探索AI和混合办公等如何改变人们完成事情的方式。此前她曾担任微软 CEO&nbsp;萨提亚·纳德拉的技术顾问，并领导了微软研究院的生产力团队。</p><p>&nbsp;</p><p>此外，Jaime是ACM Fellow以及ACM SIGIR and SIGCHI Academies的会员。她还曾荣获TR35、BECA和Karen Sparck Jones奖。她本科毕业于耶鲁大学，并获得了MIT人工智能博士学位。她也是华盛顿大学的客座教授。</p><p>&nbsp;</p><p>Jaime认为，伴随LLM的崛起，未来的工作方式正在发生迅速变化，知识越来越多地蕴含在对话而非文档中。Jaime探讨了LLM如何通过生成符合人们语境和意图的自然语言建议和反馈，以提高人们的工作效率和创造力。要有效地做到这一点，LLM 需要能够利用各种来源的相关内容作为其响应的基础。人们还需要学习新的对话模式，以充分发挥大模型的价值，因为在人际交往中行之有效的模式对 LLM 来说可能并不是最佳的。</p><p>&nbsp;</p><p>此外，Jaime 讨论了提示工程在生产环境中的重要性，并强调能够识别和推荐对话模板的价值。通过对这些研究课题的深入研究，推荐系统界有机会创造一个全新的、更美好的工作未来。</p><p></p><h2>谷歌DeepMind Denny Zhou：教语言模型学推理</h2><p></p><p>&nbsp;</p><p>Denny Zhou 是 Google DeepMind的首席科学家/研究主管，他是推理团队的创立者和现任负责人。主要研究兴趣在于构建和教导大语言模型实现类人的推理能力。他领导的团队已经开发了思维链提示、自洽性解码、最少到最多提示、指令调优（FLAN2）、LLM自我调试等大语言模型的各种涌现属性。Denny Zhou 曾获得2022年谷歌研究技术影响力奖（Google Research Tech Impact Award）。</p><p>&nbsp;</p><p>Denny Zhou 认为，过去数十年，机器学习社区已经开发了大量用来增强学习效率的数据驱动方法，比如半监督学习、元学习、主动学习、迁移学习等。然而，所有这些方法已被证明对于现实世界的NLP任务并不是特别有效，由此暴露了机器学习的一大缺陷 ——缺乏推理。人们往往可以从很少的示例中学习，这就归功于推理能力而不是依赖数据统计。</p><p>&nbsp;</p><p>Denny Zhou 表示，谷歌 DeepMind引领的LLM推理工作开发的方法极大缩小了人类智能与机器学习之间的差距，在仅要求很少的注释示例且不需要训练的情况下也能实现新的 SOTA。这些工作，谷歌 CEO 桑达尔·皮查伊在2021年的Google I/O大会上进行过重点展示。</p><p></p><h2>Meta FAIR&nbsp;&nbsp;Vedanuj&nbsp;Goswami：Llama 2开放基础和微调聊天模型</h2><p></p><p>&nbsp;</p><p>上个月，最强的开源大模型&nbsp;Llama 2 惊艳发布，一夜之间改变了大模型竞争格局。发布之后，&nbsp;Llama 2 模型迅速成为了社区最广泛使用和下载的开源模型之一。Vedanuj曾经参与训练 Llama 2 系列模型，目前在Meta AI的LLM研究团队担任研究工程师，重点研究LLM预训练和缩放技巧。</p><p>&nbsp;</p><p>Vedanuj还曾是「No Language Left Behind」（不落下任何语言）和「Universal Speech Translation for Unwritten Languages」（非书面语的通用语音翻译）等翻译项目的研究负责人，并在FAIR从事过多模态研究，领导FLAVA和MMF等著名项目。</p><p>&nbsp;</p><p>Vedanuj表示，7月18日刚刚发布的Llama 2 模型系列包含 70 亿、130 亿和 700 亿三种参数变体，因为开源且可以直接商用化，吸引了整个业界的关注。</p><p>&nbsp;</p><p>在预训练层面，Llama 2 模型系列以 Llama 1 论文中描述的预训练方法为基础，使用了优化的自回归 transformer，并做了一些改变以提升性能。相比于 Llama 1，Llama 2 的训练数据多了 40%，上下文长度也翻倍，并采用了分组查询注意力机制。具体来说，Llama 2 预训练模型是在 2 万亿的 token 上训练的，精调 Chat 模型是在 100 万人类标记数据上训练的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/84/8490c24e9c14db1efc643b464a9ebf42.png\" /></p><p></p><p>&nbsp;</p><p>在训练硬件方面，Meta 在其研究超级集群（Research Super Cluster, RSC）以及内部生产集群上对模型进行了预训练。两个集群均使用了 NVIDIA A100。在&nbsp;Meta 的评估中，多项测评结果显示，Llama 2 在包括推理、编码、精通性和知识测试等许多外部基准测试中都优于其他开源语言模型。</p><p>&nbsp;</p><p>当然，对于今天的大模型来说，「安全」是一个重要性不亚于「性能」的指标。在 Llama 2 的研发过程中，Meta 使用了三个常用基准评估其安全性：</p><p>&nbsp;</p><p>真实性，指语言模型是否会产生错误信息，采用 TruthfulQA 基准；毒性，指语言模型是否会产生「有毒」、粗鲁、有害的内容，采用 ToxiGen 基准；偏见，指语言模型是否会产生存在偏见的内容，采用 BOLD 基准。</p>",
    "publish_time": "2023-08-11 15:19:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "INFINI Labs 产品更新 | Easysearch 支持 SQL 查询、Console 告警功能支持邮件等多渠道",
    "url": "https://www.infoq.cn/article/9f058d5e4357a6981af978dc5",
    "summary": "<p></p><p><img src=\"https://www.infinilabs.com/img/blog/release/banner.png\" /></p><p></p><p>INFINI Labs 产品又更新啦~。本次更新概要如下：Easysearch 新增 SQL 插件和JDBC 驱动，支持 SQL 查询，支持 SQL 常用函数等；Console 针对告警功能做了升级优化，新增了邮件渠道，支持自定义邮件服务器配置，以及支持飞书、钉钉、企业微信、Discord、Slack 等多渠道 Webhook 发送告警通知，优化平台概览 UI 界面、展示效果更简单友好，提高了用户体验。欢迎大家下载使用。</p><p></p><h2>INFINI Easysearch v1.5.0</h2><p></p><p>INFINI Easysearch 是一个分布式的近实时搜索与分析引擎，核心引擎基于开源的 Apache Lucene。Easysearch 的目标是提供一个轻量级的 Elasticsearch 可替代版本，并继续完善和支持更多的企业级功能。</p><p></p><p>Easysearch 本次更新如下：</p><p></p><h3>Features</h3><p></p><p>增加 SQL 插件，支持使用 REST 接口和 JDBC 进行 SQL 查询支持 SQL 常用函数、包括数学函数、三角函数、日期函数、字符串函数、聚合函数等SQL 语句可以嵌入全文检索增加 JDBC 驱动，可以通过用户密码或证书连接到集群</p><p></p><h3>Bug fix</h3><p></p><p>修复 kNN 插件的配置项导致非 kNN 索引的 setting 不能正常解析的 Bug</p><p></p><h2>INFINI Console v1.6.0</h2><p></p><p>INFINI Console 是一款非常轻量级的多集群、跨版本的搜索基础设施统一管控平台。通过对流行的搜索引擎基础设施进行跨版本、多集群的集中纳管， 企业可以快速方便的统一管理企业内部的不同版本的多套搜索集群。Console 在线体验：<a href=\"http://demo.infini.cloud/\">http://demo.infini.cloud</a>\" (用户名/密码：readonly/readonly)。</p><p></p><p>Console 本次主要更新如下：</p><p></p><h3>1、告警功能重磅更新</h3><p></p><p>该版本主要更新告警规则和渠道，Console 内置了常用的告警规则和渠道，支持邮件、飞书、钉钉、企业微信、Slack、Discord 等渠道，下载安装部署后仅需在 Console 界面菜单 [告警管理-&gt;告警渠道] 配置相关渠道的 Webhook 链接或者邮件服务器并启用渠道开关，无需做额外操作即可接收告警通知消息，我们的目标是做到开箱即用，简单实用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/60abe75cfc9db7cc7be4a7d8701e18c8.jpeg\" /></p><p></p><h4>1.1 支持邮件渠道</h4><p></p><p>通过配置邮件渠道，设置相关收件人、邮件内容模式（支持纯文本与 HTML）等参数，并将邮件渠道绑定到告警规则，当告警事件触发时，告警消息将被发送到相关收件人邮箱。通过增加邮件渠道有效提升了告警消息触达能力，让用户第一时间接受和处理问题。邮件渠道配置界面如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d8/d80eb29f06de1798a33c655d0b4df70c.png\" /></p><p></p><h4>1.2 全面优化告警通知效果</h4><p></p><p>本次更新 Console 默认内置了告警渠道通知消息内容模板，通过系统环境变量及告警上下文变量组合成消息内容，用户无需修改即可复用，也可以自定义修改。各渠道通知消息效果如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4afd06d4e955a856720f299d77a876d.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23ddb90ab34e434f79b877b46379bafa.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/77/77d8feeb7cb98f10da37ca242866e79d.jpeg\" /></p><p></p><h4>1.3 告警功能演示视频</h4><p></p><p>请查看 <a href=\"https://www.bilibili.com/video/BV1iu411n7MY/\">演示视频</a>\"</p><p></p><h3>2、优化平台概览 UI、新界面更简单友好</h3><p></p><p>本次更新优化了平台概览中的集群、节点、索引、主机列表的展示效果，统一风格，突出关键指标显示，提供了卡片和表格两种展示模式，可以按需切换查看。UI 效果如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/01d893d46aae827c337a1499c6267468.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/092d2aededc32c1bd7d6e81b502d98ff.jpeg\" /></p><p></p><p>Console 详细更新如下：</p><p></p><h3>Features</h3><p></p><p>优化平台概览 UI 界面、支持卡片和表格样式切换展示告警规则新增告警恢复通知配置告警渠道新增邮件通知告警规则和告警渠道新增导入导出新增邮件服务器</p><p></p><h3>Bug fix</h3><p></p><p>修复数据探索切换视图排序失效的问题</p><p></p><h3>Improvements</h3><p></p><p>调整告警规则渠道配置调整饼图样式</p><p></p><h2>INFINI Agent v0.6.1</h2><p></p><p>INFINI Agent 是 INFINI Console 的一个可选探针组件，负责采集和上传集群指标和日志等信息，并可通过 Console 管理。Agent 支持主流操作系统和平台，安装包轻量且无任何外部依赖，可以快速方便地安装。</p><p></p><p>Agent 本次更新如下：</p><p></p><h3>Bug fix</h3><p></p><p>修复发现节点进程信息时获取 ES 节点端口不对的问题</p><p></p><h2>期待反馈</h2><p></p><p>欢迎下载体验使用，如果您在使用过程中遇到如何疑问或者问题，欢迎前往 INFINI Labs Github（<a href=\"https://github.com/infinilabs\">https://github.com/infinilabs</a>\"） 中的对应项目中提交 Feature Request 或提交 Bug。</p><p></p><p>INFINI Gateway： <a href=\"https://github.com/infinilabs/gateway/issues\">https://github.com/infinilabs/gateway/issues</a>\"INFINI Console： <a href=\"https://github.com/infinilabs/console/issues\">https://github.com/infinilabs/console/issues</a>\"下载地址： <a href=\"https://www.infinilabs.com/download\">https://www.infinilabs.com/download</a>\"</p><p></p><p>您还可以通过邮件联系我们：<a href=\"mailto:hello@infini.ltd\">hello@infini.ltd</a>\"</p><p></p><p>或者拨打我们的热线电话：**(+86) 400-139-9200**</p><p></p><p>欢迎加入 Discord 聊天室：<a href=\"https://discord.com/invite/4tKTMkkvVX\">https://discord.com/invite/4tKTMkkvVX</a>\"</p><p></p><p>也欢迎大家微信扫码添加小助手（INFINI-Labs），加入用户群一起讨论交流。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a2bde60f0adc6238ed2c687eaeee91b.png\" /></p><p></p><h2>关于极限科技（INFINI Labs）</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/99cd84fe7b7bf9bec483d9ce0ac1b24b.png\" /></p><p></p><p>极限科技，全称极限数据（北京）科技有限公司，是一家专注于实时搜索与数据分析的软件公司。旗下品牌极限实验室（INFINI Labs）致力于打造极致易用的数据探索与分析体验。</p><p></p><p>极限科技是一支年轻的团队，采用天然分布式的方式来进行远程协作，员工分布在全球各地，希望通过努力成为中国乃至全球企业大数据实时搜索分析产品的首选，为中国技术品牌输出添砖加瓦。</p><p></p><p>官网：<a href=\"https://www.infinilabs.com/\">https://www.infinilabs.com</a>\"</p><p></p>",
    "publish_time": "2023-08-11 12:29:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "我的20年职业生涯：全是技术债",
    "url": "https://www.infoq.cn/article/MWPVDU3lqV9FH4x8Dm7a",
    "summary": "<p>如今，技术债无疑是最常用的流行语了。有人说：“我们正在快速开发MVP，同时最大限度地减少技术债！”。他们在这里提到技术债听起来很酷。但我只是笑笑，因为这最终都会是技术债。</p><p></p><p>我的整个职业生涯都是关于现在流行的技术债的，或者说是关于代码被弃用的。</p><p></p><p>如果你不相信你的整个职业生涯也会是关于技术债的，你可能会在阅读完这篇文章后有同样的想法。在本文中，我将带你了解我20年职业生涯中的变化。</p><p></p><p>我的职业生涯始于Visual Basic 6的开发。从1999年到2003年，我构建了多个不同的应用程序。我想你可能会说，按照今天的标准，Visual Basic 6中的任何东西都是技术债，或者早就被取代了。万岁，“下个错误再接再厉!”</p><p></p><p>我花了很多时间进行经典的动态服务器页面（ASP）开发。有一段时间，我还是一名在Internet Explorer 6和Netscape Navigator制作兼容网站的专家。这在简历上已经没有多大意义了！</p><p></p><p>Visual Basic、ASP、IE6和Netscape都是早已被遗忘的技术了。正如<a href=\"https://www.youtube.com/channel/UCMkbjxvwur30YrFWw8kpSaw\">Strong Bad</a>\"当时所说的那样，“被骗了！”</p><p></p><h2>过时的语言：Perl、Delphi、Fortran、FoxPro、ColdFusion</h2><p></p><p></p><p>在过去的20多年里，除了Visual Basic 6之外，还有很多编程语言也都失宠了。很有可能，如果你用这些语言中的任何一种构建了任何的东西，人们都会想要找出重写它的方法，因为很难再找到使用这些语言的程序员了：Perl、Delphi、Fortran、FoxPro、ColdFusion。</p><p></p><p>还有使用这些语言的应用程序吗？你还能雇人来做吗？这很难。在大多数情况下，这些公司都必须要对旧的应用程序进行现代化改造并淘汰旧的。在21世纪初，人们认为Adobe ColdFusion是最热门的产品。你还记得它刚成为明星的时候吗？</p><p></p><p>Ruby on Rails也正面临着被添加到这个列表中的危险。它已经失宠了，很难找到使用它的开发人员了。曾经使它独一无二的东西现在也可以在其他语言中使用了。</p><p></p><p>编程语言来来往往。开发人员不希望学习工作中不需要的技能。这永远保持供需平衡！开发人员跳槽的速度很快；他们总是希望自己的简历上有一些热门的新东西。</p><p></p><h2>ActiveX、Java Applets、Flash和Silverlight发生了哪些变化？</h2><p></p><p></p><p>我最初开发的一些应用程序使用了Internet Explorer 6中的ActiveX控件。当时，需要用它们来做打印和其他一些非常不安全的黑客工作。PDF在当时并不常见，用浏览器打印简直就是一场噩梦。</p><p></p><p>Java Applets在很久以前也是一件大事。它们运行缓慢，并且在电脑上安装正确版本的Java总是一团糟。我永远不会忘记处理Java小程序网络防火墙的噩梦。我一点也不怀念它们，幸运的是，它消失了。</p><p>当然，我们都还记得Macromedia/Adobe Flash！它一度是整个互联网的宠儿。Flash游戏层出不穷，许多软件都是用ActionScript在Flash中构建的。一种名为CheerpX的产品现在允许使用WebAssembly运行旧的Flash应用程序。</p><p></p><p>微软推出了一个名为Silverlight的Flash竞品。对于C#开发人员来说，这实际上是一个非常棒的框架。我的公司用Silverlight构建了一些非常棒的东西。</p><p></p><p>众所周知，苹果在浏览器中放弃了对Flash和Silverlight的支持，从而终结了它们。</p><p></p><p>这是十多年前我们在VinSolutions中使用Silverlight构建财务计算器的屏幕截图。Silverlight现在早已不复存在，他们用完全的JavaScript重写了它，但它没有旧版本酷了！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23b80397ffcfcdc50302e19e41383e93.webp\" /></p><p></p><p></p><h2>我的第一个移动应用程序</h2><p></p><p></p><p>在2004年，我开发了一个移动应用程序。这很难记起，但当时iPhone和Android还不存在。我为Compaq PDA编写了一个应用程序，用于跟踪汽车经销商的库存。它是用C#编写的，用于在Windows CE上运行的.NET Compact Framework中。</p><p></p><p>这个PDA有一个100万像素的摄像头。只要外面阴天遮住了刺眼的光线，这些照片就会糟糕些。😂 天哪，技术变了嘛！这个应用程序很早以前就被淘汰了，但在2005年时它还很前卫。</p><p></p><h2>Swift、WebForms、MVC与Angular JS</h2><p></p><p></p><p>Swift是另一个很好地说明开发工具变化速度之快的例子。苹果公司发布Swift后，就很难再证明用Objective C编写代码是合理的了。我确信在某些用例中仍然需要用Objective C。但Swift明显更易于开发，并且是向前迈出的重要一步。</p><p></p><p>我认为，任何用Objective C编写的应用程序现在都可能是技术债了。</p><p></p><p>在为构建Web应用程序编写了疯狂的内联脚本之后，我很乐意使用新的ASP.NET Web表单。它们的服务器端控件大大简化了开发。它们的目标是让创建Web应用程序变得像在Visual Basic 6中一样简单。它基本上是有效的！你可以在服务器端构建可重用的UI组件以呈现给浏览器。就像我们今天使用100%的JavaScript所做的那样。</p><p></p><p>WebForms并不完美，但它是一个相当大的提升。在Ruby on Rails出现并普及了用于开发Web应用程序的MVC（Model-View-Controller，模型-视图-控制器）框架之前，它一直运行得很好。</p><p>MVC很快就淘汰了我们制作的所有Web表单应用程序。任何网页形式的东西都绝对是技术债。（尽管如此，<a href=\"https://dotnet.microsoft.com/en-us/apps/aspnet/web-apps/blazor\">Blazor</a>\"也有同样的想法。）</p><p></p><p>不知不觉中，每种编程语言就都支持MVC框架了。我们转而使用ASP.NET MVC做所有的新功能。它无处不在，包括Django、Laravel、Symfony、Spring等。</p><p></p><p>快进到今天，MVC已经过时了。现在一切都是在React、Angular、Vue和其他框架中完成的。</p><p></p><p>在我们拥有这些框架之前，我们还有其他的Javascript框架。在<a href=\"https://stackify.com/\">Stackiy</a>\"，我们使用了Knockout，这是一个相当流行的前端框架。</p><p></p><p>你还记得这些框架吗？Knockout、Ember、Aurelia、Meteor、Backbone、Handlebars。</p><p>如果你使用过其中的任何一个，我敢打赌，所有这些代码现在都被认为是技术债并且已经失宠了。第一代前端框架输给了React和Angular。</p><p></p><p>2015年，Angular由谷歌创建，并一炮而红。它迅速成为最受欢迎的前端框架。然后在2016年，Angular进行了一次重大升级，不再向后兼容。猜猜这意味着什么？原始版本中的任何内容现在都是技术债了。我在我公司的项目中使用了旧版本的Angular，这是我们必须升级的主要技术债。</p><p></p><h2>过时的SOAP和WCF</h2><p></p><p></p><p>在REST API和JSON成为事实上的标准之前，另一种选择是SOAP，它代表简单对象访问协议。它使得调用Web服务并通过自动代码生成代理类来正确调用服务变得更容易。它主要由基于XML的Windows通信框架（WCF）来使用。</p><p></p><p>它非常棒......直到它失败。我职业生涯中最糟糕的项目之一就是要弄清楚如何在我的公司和另一家供应商之间通过WCF和SOAP使用安全证书。SOAP和WCF的承诺令人惊叹，但随着时间的推移，维护它简直是一场噩梦。</p><p></p><p>SOAP和WCF是两个我不会错过的东西。微软决定不再在.NET Core中支持WCF。像REST、gRPC和GraphQL这样的东西现在是首选。尽管如此，有个社区项目最终使<a href=\"https://devblogs.microsoft.com/dotnet/corewcf-v1-released/\">CoreWCF</a>\"得以继续发展。</p><p>随着时间的推移，我们用来调用Web服务的技术类型已经发生了变化。旧的方式仍然有效，但大多数人可能更愿意淘汰它们。</p><p></p><h2>主要语言版本</h2><p></p><p></p><p>另一个常见问题是主要编程语言的版本更改。无论是Ruby、PHP、.NET还是其他语言。它们通常需要大量的代码更改甚至重写。</p><p></p><p>当.NET Core刚发布时，它是专为在Linux上运行而设计的更新、更轻、更快的.NET版本。基本的C#代码都很容易移植过来，但没有人会在真实的应用程序中只使用基本代码。</p><p></p><p>然而，在复杂的企业应用程序中，在导航升级路径时存在许多潜在的问题。这就成为了一笔必须解决的重大技术债。否则，你最终会陷在一个古老的版本中。</p><p></p><p>这些主要版本的更新最终会成为重大的技术债项目。</p><p></p><p>我们在Stackiy遇到的最大挑战之一是卡在了旧版本的Elasticsearch上。有一次，它们对其工作方式进行了一些重大的更改，但这些更改并不完全向后兼容。我们用得非常重，所有的升级工作都变成了海量的技术债和升级项目。</p><p></p><p>我们一遍又一遍地把它踢到路边，最终远远落后了。这是另一个可能困扰公司的真实的技术债项目的例子。</p><p></p><h2>开源替代方案淘汰了我的代码</h2><p></p><p></p><p>在<a href=\"https://stackify.com/\">Stactify</a>\"，我们为6种编程语言构建了自己的跟踪/测评分析库。这项工作的工作量令人难以置信。</p><p>好吧，现在<a href=\"https://opentelemetry.io/\">OpenTelemetry</a>\"出现了，使得我所有的工作变得毫无用处。</p><p></p><p>既然可以使用开源的行业标准，为什么还要自己管理呢？Stackiy正在慢慢地消除那些我帮忙构建的.NET测评分析器。</p><p></p><p>随着时间的推移，你会看到你创造的几乎所有的东西都会因为各种原因而被废弃和替换，或者现在就已经都是基于旧技术的了。</p><p></p><p>我在职业生涯早期开发的几个应用程序都已经被终止了，因为这些公司被收购了，并且决定使用完全不同的技术。</p><p></p><p>大多数软件的使用寿命都很有限，比你想象的要短。所有的代码最终都变成了技术债，每个人都想用更现代的方式重写，或者业务需求发生重大的变化。</p><p></p><p>诚然，在企业界，更有可能拥有似乎永远存在的内部应用程序。像铁路或大型银行这样的公司使用同样的基于大型机的软件已经有40年了。</p><p></p><p>我预测WebAssembly最终会超越当今的前端开发，一个全新的世界将不断发展。</p><p></p><h2>技术债的本质</h2><p></p><p></p><p>在做新项目时，人们总是希望将技术债降至最低。我理解。在让事情运转起来和努力让事情变得完美之间是要有平衡的。</p><p></p><p>然而，没有什么是不技术债，因为它并不完美。世上没有十全十美的这回事。随着时间的推移，今天完美的东西将来也会不完美。要学会与不完美共存。</p><p></p><p>技术债的另一面是，随着时间的推移，一切都会慢慢腐化。它要么在升级到最新版本方面存在重大问题，要么由于更新的操作方式而最终失宠。祝你好运，能为旧的技术栈招聘到人员。</p><p></p><p>一切最终都会变成技术债，否则项目就会夭折。如果幸运的话，你的代码能存活足够长的时间，从而成为别人的技术债。</p><p></p><p>如果时间足够长的话，你的所有代码都将被删除。🤷‍♂️</p><p></p><p>原文链接：<a href=\"https://blog.visionarycto.com/p/my-20-year-career-is-technical-debt\">https://blog.visionarycto.com/p/my-20-year-career-is-technical-debt</a>\"</p>",
    "publish_time": "2023-08-11 15:55:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "为防商业竞争，HashiCorp 宣布更改所有产品开源许可证",
    "url": "https://www.infoq.cn/article/oIQh45RPuLXOSVKqkwFA",
    "summary": "<p>当地时间8 月 10 日，HashiCorp <a href=\"https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license\">宣布</a>\"所有产品和多个库的未来版本将从 Mozilla 公共许可证 v2.0 (MPL 2.0) 过渡到商业源代码许可证（BSL 或 BUSL）v1.1。HashiCorp API、SDK 和几乎所有其他库都将保留 MPL 2.0。</p><p></p><p>BSL 是一种相对较新的许可证，旨在平衡开源和商业利益之间的需求。BSL 许可证规定了一段时间，在此期间内，软件被认为是商业源代码。BSL 允许开发者在商业源代码期间提供付费支持和服务，以便在开源之前获得一些商业回报。期限届满后，软件许可会转变为一种更开放的许可证，软件的使用、修改和分发将受到更宽松的限制，更符合传统开源许可证的要求。</p><p>&nbsp;</p><p>据悉，Couchbase、Cockroach Labs、Sentry 和 MariaDB在2013 年开发了此许可证。Confluence、MongoDB、Elastic、Redis Labs 等公司也采用了包含商业使用限制的替代<a href=\"https://www.infoq.cn/article/DDgl51H5dC9V7dHXrvIN\">许可证</a>\"。在所有情况下，许可证使赞助商能够对开源项目商业化拥有更多控制权。</p><p>&nbsp;</p><p>“BSL 1.1 是一种源代码可用的许可证，允许复制、修改、再分发、非商业使用和在特定条件下的商业使用。BSL 允许我们的社区将我们的源代码用于几乎所有目的，同时防止商业竞争者使用源代码。”HashiCorp官方表示。</p><p>&nbsp;</p><p>HashiCorp 联合创始人 Armon Dadgar 表示，</p><p>&nbsp;</p><p></p><blockquote>我们的开源模式之所以成为可能，是因为数以千计的商业客户在他们的关键任务基础设施上与我们合作。我们每年在开源产品的研究和开发上投资数千万美元，我们的商业努力使我们能够继续支持和赞助我们充满活力的用户社区。&nbsp;我们的方法使我们能够与云提供商密切合作，为我们的联合用户和客户以及数百个与我们密切合作的其他技术伙伴提供紧密集成。然而，也有其他供应商利用纯OSS模型，社区致力于OSS项目，以实现他们自己的商业目标，但不提供任何物质回报。我们相信这并不符合开源的精神。</blockquote><p></p><p>&nbsp;</p><p>HashiCorp当前表示，在指定的变更日期或根据 BSL 首次公开发布代码四周年（以先到者为准），代码将自动根据变更许可证变得可用。当前 HashiCorp 项目的变更许可证是 MPL 2.0。</p><p>&nbsp;</p><p>实际上，BSL 许可证在开源社区中曾引发一些争议，因为它与传统的开源价值观有所不同。使用 BSL 许可证需要开发者仔细考虑项目的目标、商业策略和开源承诺，以确保在许可证转变期间不会对项目产生不良影响。</p><p>&nbsp;</p><p>&nbsp;</p><p>查看更多信息：</p><p><a href=\"https://www.hashicorp.com/license-faq#What-did-HashiCorp-announce-today-(Aug-10)\">https://www.hashicorp.com/license-faq#What-did-HashiCorp-announce-today-(Aug-10)</a>\"</p><p><a href=\"https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license\">https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-08-11 16:26:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Threads 横空出世，通过解析App，我发现了这些CSS小秘密",
    "url": "https://www.infoq.cn/article/JTZdvP6oRuVCKFdEROAP",
    "summary": "<p>每当遇上一款新产品，我首先想到的就是研究研究他们是怎么实现CSS的。Meta新近推出的Threads当然也不例外，我快速体验了这款移动应用，发现它的主要功能就是展示网络上的公共发帖。</p><p>&nbsp;</p><p>浏览过程中我也有了其他深入发现，本文将具体为大家一一介绍。</p><p>&nbsp;</p><p>闲言少叙，咱们马上开始！</p><p>&nbsp;</p><p></p><h2>在帖子布局中使用CSS网格</h2><p></p><p>&nbsp;</p><p>Threads当中的CSS网格，可以算是我在生产级应用中见到的最值得一聊的案例。Meta在这里选择用CSS网格构建帖子布局。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/7c/7c54dbfc87c02549f6f058cab25f1453.png\" /></p><p></p><p>&nbsp;</p><p>咱们简单看看：</p><p>&nbsp;</p><p><code lang=\"null\">:root {\n  --barcelona-threadline-column-width: 48px;\n}\n.post {\n  display: grid;\n  grid-template-columns:\n    var(--barcelona-threadline-column-width)\n    minmax(0, 1fr);\n  grid-template-rows: 21px 19px max-content max-content;\n}</code></p><p>&nbsp;</p><p>有趣发现：第一个网格列被命名为--barcelona。我很好奇他们为什么要选这个名字。</p><p>帖子布局由2列 x 4行网格组成。这里没有主容器，帖中的每个条目封镜 使用grid-column 和 grid-row属性进行手动放置。</p><p>&nbsp;</p><p>再来看用户头像：</p><p>&nbsp;</p><p><code lang=\"null\">.post-avatar {\n  padding-top: 4px;\n  grid-row: 1 / span 2;\n  grid-column: 1;\n}</code></p><p>&nbsp;</p><p>头像位于第一列并跨越前两行。这里的padding-top尤其值得注意。虽然我在生产代码中没找到确切用途，但猜测它可能是在微调UI对齐。</p><p>&nbsp;</p><p>下图所示，是经过/未经padding-top处理的头像部分前后对比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8549146feeaf4d720ac06667d3b6f84.png\" /></p><p></p><p>&nbsp;</p><p>在这里采用padding-top的另一个理由，可能是要把头像下推以对齐第二行的下沿。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e4d5ac4c260f1ab78e5326af27be13b.png\" /></p><p></p><p>&nbsp;</p><p></p><h2>在网格行数中使用奇数值</h2><p></p><p>&nbsp;</p><p>为什么行值选择的是21px 和 19px？经过进一步检查，这似乎也是对UI的微调措施。行高之和为40px，即头像高度再加上padding-top（36像素+4像素）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a7/a77ff5da5acabe3a32c3857783c9d527.png\" /></p><p></p><p>&nbsp;</p><p>大家可能会好奇，为什么不对这些值做标准化设置？毕竟在系统设计中存在这样一条“铁律”：设计师必须始终遵循UI元素的预定义规则。</p><p>&nbsp;</p><p>但从Threads来看，手动调整具体值也是可接受的。在某些情况下，甚至不妨先把严格的指导方针放下。</p><p>&nbsp;</p><p></p><h2>使用固定的行大小限制</h2><p></p><p>&nbsp;</p><p>由于行大小是固定的，因此无法为其添加填充。但只要意识到存在这个限制，我们也可以借用边距来绕过这一约束。</p><p>&nbsp;</p><p>请看以下示例：&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/0222e5bdb1284bc839123efa6a421ee2.png\" /></p><p></p><p>&nbsp;</p><p>由于行大小是固定的，所以添加顶部和底部填充不会影响到帖子标题。</p><p>&nbsp;</p><p></p><h2>各布局列之间的列距显得有点凌乱</h2><p></p><p>&nbsp;</p><p>布局列之间的当前列距为零。相反，图像大小为36 x 36像素，而其容器宽度则为48像素。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/241110330a2200b9d9c2fc455d86344b.png\" /></p><p></p><p>&nbsp;</p><p>这就用模拟的方式呈现出了列距的效果。我不知道开发团队为什么不直接设置列距，我个人是比较倾向这种作法。</p><p>&nbsp;</p><p></p><h2>为什么不用命名CSS网格区域？</h2><p></p><p>&nbsp;</p><p>根据我迄今为止观察到的情况，网格布局当中存在三种变体，而且使用命名网格区域后这三种变体都能获得效果提升。</p><p></p><p>我试着复制了这套网格并根据命名区域进行了构建，新的结果比直接为列和行指定值更加顺畅易读。</p><p>为了演示差别，我们先为布局中的各个条目分配一个grid-area：</p><p>&nbsp;</p><p><code lang=\"null\">.AvatarContainer {\n  grid-area: avatar;\n}\n.HeaderContainer {\n  grid-area: header;\n}\n.BodyContainer {\n  grid-area: body;\n}\n.ThreadlineContainer {\n  grid-area: line;\n}\n.FooterContainer {\n  grid-area: footer;\n}</code></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>变体1：使用默认值</h3><p></p><p>&nbsp;</p><p>之后，我们再来研究变体。以下为默认布局的效果：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc7471f4b46c713a3a6c4efdbc4b0c94.png\" /></p><p></p><p>&nbsp;</p><p><code lang=\"null\">.post {\n  display: grid;\n  grid-template-columns:\n    var(--barcelona-threadline-column-width)\n    minmax(0, 1fr);\n  grid-template-rows: 21px 19px max-content max-content;\n  grid-template-areas:\n    \"avatar header\"\n    \"avatar body\"\n    \". body\"\n    \". footer\";\n}</code></p><p>&nbsp;</p><p>请注意，这里使用 . 来表示空白区域。</p><p>&nbsp;</p><p></p><h3>变体2：回复</h3><p></p><p>&nbsp;</p><p>这个变体代表某人回复另一用户时的情况。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af40b4226517efcdc365198526115b6d.png\" /></p><p></p><p><code lang=\"null\">.post--reply {\n  grid-template-rows: 36px 0 max-content max-content;\n  grid-template-areas:\n    \"avatar header\"\n    \"body body\"\n    \"body body\"\n    \"footer footer\";\n}</code></p><p>&nbsp;</p><p></p><h3>变体3：默认值加Thread Line</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8f9d2519d8ca1d3d8271b95aff7b4a22.png\" /></p><p></p><p><code lang=\"null\">.post--withLine {\n  grid-template-areas:\n    \"avatar header\"\n    \"avatar body\"\n    \"line body\"\n    \"footer footer\";\n}</code></p><p>&nbsp;</p><p>在这里使用命名网格区域，即可通过编辑一处来变更整个布局。</p><p>&nbsp;</p><p></p><h2>Thread Lines中的SVG</h2><p></p><p>&nbsp;</p><p>老实说，Threads应用中最先引起我注意的就是这条螺旋线。从几周前第一次看到以来，我一直想搞清楚它是怎么实现的。</p><p>&nbsp;</p><p>先来看以下截屏：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e1a63116a52d7dd70bb4920d754dd93c.png\" /></p><p></p><p>&nbsp;</p><p>Threads Line这条螺旋线把我的头像和Zuck的头像连接了起来，而这其实是条SVG路径，具体由三部分组成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb0a4b9cf045227fd5e430da55385741.png\" /></p><p></p><p>&nbsp;</p><p>第一部分的长度用JavaScript代码计算得出。</p><p>&nbsp;</p><p></p><h2>CSS网格的内联CSS变量</h2><p></p><p>&nbsp;</p><p>这是个令人振奋的发现：我和其他很多从业者所提倡的设计，终于开始在Threads这类大型应用中得到体现。</p><p>&nbsp;</p><p>在用户个人资料部分，选项卡的网格布局是由包含选项卡计数的内联CSS变量构建而成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e69b5e82e3a07a70d71544d23b4ee545.png\" /></p><p></p><p>&nbsp;</p><p>这种设计非常精妙。随着选项卡数量的增加，我们只需要调整CSS变量的值即可。多么简洁、多么方便！</p><p>&nbsp;</p><p></p><h2>Overflow Wrapping</h2><p></p><p>&nbsp;</p><p>我注意到，Threads在帖子本体中用到了overflow-wrap: anywhere。有一说一，我之前从来没用过、甚至没听说过这个关键字，我一直用的都是break-word。</p><p>&nbsp;</p><p></p><blockquote>根据MDN的介绍，它跟break-word的作用相同，只有一点区别：在计算最小内容的实际大小时，它会考虑由单词截断造成的软换行情况。</blockquote><p></p><p>&nbsp;</p><p>我还是没发现break-word 跟 anywhere到底有什么区别。如果有Threads团队的同学正好看到这篇文章，还望不吝赐教。</p><p>&nbsp;</p><p></p><h2>使用动态视口单元</h2><p></p><p>&nbsp;</p><p>我很喜欢用动态视口单元 dvh作为启动画面。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/74/74114d5121b4dd7c4d262ff829acf68a.png\" /></p><p></p><p>&nbsp;</p><p>感兴趣的朋友也可以参考我之前写的关于新视口单元的文章：</p><p>&nbsp;</p><p><a href=\"https://ishadeed.com/article/new-viewport-units/\">https://ishadeed.com/article/new-viewport-units/</a>\"</p><p>&nbsp;</p><p></p><h2>几项防御式CSS策略</h2><p></p><p>&nbsp;</p><p>为了确保Flexbox的布局不会因最小内容长度而中断，可以使用min-width: 0来重置该行为。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b46833acc9020d86e5c1cb7cbdefee99.png\" /></p><p></p><p>&nbsp;</p><p>我在讨论Flexbox中最小内容大小的防御式CSS文章中，具体介绍了相关问题。</p><p>&nbsp;</p><p><a href=\"https://defensivecss.dev/tip/flexbox-min-content-size/\">https://defensivecss.dev/tip/flexbox-min-content-size/</a>\"</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>&nbsp;</p><p>文章就是这些。我很喜欢研究CSS，以此为切入点思考Threads团队是如何设计和构建这款产品的。相信还有很多细节逃过了我的双眼，毕竟目前能接触到的只是Web上的预览版本。随着后续研究的深入，我也期待给大家带来更多有趣的发现。</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p></p><p><a href=\"https://ishadeed.com/article/threads-app-css/\">https://ishadeed.com/article/threads-app-css/</a>\"</p><p>&nbsp;</p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://www.infoq.cn/article/xk8s3S1qPLZopINuBCA4\">5 天内用户数破亿、增速碾压 ChatGPT，Twitter 劲敌 Threads 是如何构建的？</a>\"</p><p><a href=\"https://xie.infoq.cn/article/e2f533bfbdd50b77499cd4427\">nodejs 中使用 worker_threads 来创建新的线程</a>\"</p><p><a href=\"https://www.infoq.cn/article/WAb5hXYcthvYjYqVMyxN\">48 小时注册用户达 7000 万，马斯克：Threads 是 Twitter 前员工做出来的！</a>\"</p><p><a href=\"https://www.infoq.cn/article/E5uA6Gtd7VkIoaebIHzd\">我眼中的&nbsp;CSS&nbsp;革命：新特性潜力无限</a>\"</p>",
    "publish_time": "2023-08-11 16:53:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "明略科技开源TensorBoard.cpp，助力大模型预训练",
    "url": "https://www.infoq.cn/article/CLG0Amw8Lo7zhuML5XrJ",
    "summary": "<p>近日，明略科技集团实现了机器学习可视化工具——TensorBoard的C++接口，进一步丰富了基于C++的大模型项目工具集，使得大模型预训练过程监控更加便捷、高效，加速营销领域大模型预训练进程。该工具已在Github开源。</p><p>&nbsp;</p><p>TensorBoard是Google开发的一款机器学习可视化工具，常用于监测机器学习过程的各项指标。明略科技高级技术总监赵亮介绍：“在大模型训练过程中，数据监测是一个重要维度，而TensorBoard通过可视化模型中的各种参数和结果，例如记录大模型训练过程的Loss变化、验证集的PPL变化、学习率变化、Token消耗量、单步参数更新时延等指标，帮助分析训练状态，发现训练过程中出现的问题并及时采取干预措施，提升大模型训练进程和效果。”</p><p>&nbsp;</p><p>此前，TensorBoard仅支持Python语言接口。此次明略科技通过C++实现TensorBoard，将进一步丰富基于C++实现的大模型项目工具集，大幅提升模型训练监测效率，加速模型训练进程，改写接口后的工具将通过多维度的数据模式展示训练指标，包括标量、直方图、图像、图像合集、音频、文本等数据模式。该工具包通过github项目Tensorboard.cpp分享，助力更多研究者和开发者参与并加速大模型的研发进程，推动人工智能多领域的应用探索。</p><p>&nbsp;</p><p>明略科技在Github开源的两款工具包：ASR-BlockFormer与tensorboard.cpp</p><p>&nbsp;</p><p>明略科技集团CTO郝杰表示：</p><p></p><p>“我们要在更高效、更低成本的要求下做出营销领域的大模型，通过自适应技术提升大模型的能力。好的行业大模型需要具备通用大模型的逻辑性、语言顺畅度，同时还需要实现通用大模型所不具备的，在某个行业内或具体的领域中的真实性、专业性。我们以明略科技凭借17年来积累的海量行业数据为基础，从客户实际需求出发，借助庞大的数据和知识库进行增强训练，满足客户多样化的任务和场景需求。在训练监测可视化工具的加持下，我们将提升训练速度，及时发现问题，为客户打造一个更加可靠、效果更好的行业大模型。”</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-08-11 17:35:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "架构师（2023 年 8月）",
    "url": "https://www.infoq.cn/article/l428myBt89OCybsmXQqB",
    "summary": "<p></p><h2>卷首语：我们能从 Telegram 的开发中学到什么？</h2><p></p><p></p><p>Telegram 由 Nikolai 和 Pavel Durov 两兄弟于 2013 年推出。根据维基百科的统计，截至 2023 年 7 月，Telegram 的月活跃用户已突破 8 亿。这一用户规模可与国民软件 QQ 相媲美。根据 2023 年第一季度的财报数据，24 岁高龄的 QQ 移动终端月活跃用户为 5.97 亿。</p><p></p><p>虽然规模很大，但 Telegram 性能非常优异，其系统指标是一众社交软件看齐的对象。同时，始终坚持采用纯原生的方式实现，技术栈简洁干净。此外，自 Telegram 成立以来的这些年里，其受欢迎程度一直在持续增长，深受开发者推崇：“自从用了 Telegram，我才知道某些即时通讯软件有多垃圾。”</p><p></p><p>在创办 Telegram 之前，这两位兄弟曾共同创建了俄罗斯社交网络 VK。从技术角度来看，VK 同样是一款出色的软件。根据 VK 的早期员工透露，尽管 VK 已经成熟，Pavel 仍然对产品功能保持着高度控制，并怀有极高的期望：“Pavel 对质量设定了极高的标准……不论是代码质量还是最终产品的品质。你必须用尽所能，使用各种方式达到这一标准。”在创办 Telegram 时，他们持续坚守这一高标准。</p><p></p><p>据报道，Pavel 在控制方面一直具有独特性，他还引导着公司的愿景。一名员工形容他为“具有远见的人”，能够吸引并团结杰出工程师，以实现共同的目标。尽管 Telegram 规模超越 QQ，但仅由一支小团队组成，由于团队采用扁平化的管理结构，他们负责的产品能够快速推进。</p><p></p><p>另外，他对人才的要求也很高，很少有公司像 Telegram 那样拥有如此多的才华横溢的工程师。他们喜欢用“竞赛”来招募人才，比如通过举办“开发者挑战赛”来改进其产品并寻找新的队友。该公司曾举办了一场“GIF 大赛”，数千人参加了这个比赛，Telegram 再从这个人才库中聘用“前两名或前三名”。这是该公司在其整个生命周期中始终保持精简的部分原因。毫不夸张地说，它招聘的工程师是<a href=\"https://www.generalist.com/briefing/telegram\">最顶尖的 0.1%</a>\"。</p><p></p><p>这种对技术和人才的高标准，使得 Telegram 在经过十年的发展后仍能在功能和技术方面保持高度内聚。或许我们应该庆幸有着 Telegram 这样的存在，它不仅提升了社交软件开发的标准，还推动了其竞争对手进行改进。</p><p></p><h2>目录</h2><p></p><p>热点 | Hot</p><p></p><p>MySQL 之父：不要把一个优秀的开发者提升为管理者，那会是种资源浪费</p><p></p><p>字节跳动开源 KubeAdmiral：基于 K8s 的新一代多集群编排调度引擎</p><p></p><p>比 JDK 最高快 170 倍，蚂蚁集团开源高性能多语言序列化框架 Fury</p><p></p><p>不到一年，Istio 项目正式从 CNCF 毕业</p><p></p><p>第一批因 AIGC 裁掉自家员工的老板该后悔了？</p><p></p><p>访谈文章 | Interview</p><p></p><p>专访 OpenSSF CTO：安全问题应该考虑在构建模型之前，别出了问题就让 ChatGPT“背锅”</p><p></p><p>案例研究 | Case Study</p><p></p><p>财报会议新时代：如何将 AI 训练成资深 CFO</p><p></p><p>小白大挑战：24 小时内用 ChatGPT 和 Next.js 开发开源项目，吸引上万用户！</p><p></p><p>面向大模型的存储加速方案设计和实践</p><p></p><p>Cube 轻量虚拟化如何做到 100ms 交付一个安全容器</p><p></p><p>如何挖掘 Bazel 的极致性能</p><p></p><p>面向故障处理的可观测性体系建设</p><p></p><p>推荐文章 | Article</p><p></p><p>红帽：我们为什么要改变 RHEL 源码的发布策略？</p><p></p><p>LLM 对程序员的冲击和影响</p><p></p><p>高薪缺人，但要懂全栈懂 LLM，一个全新职业正在兴起！</p><p></p><p>黄东旭：我对数据库如何 Serverless 化的一些思考</p><p></p><p>C++ 变化太大！该重新学习这门语言了</p><p></p><p>5 天内用户数破亿、增速碾压 ChatGPT，Twitter 劲敌 Threads 是如何构建的？</p><p></p><p>特别专题｜实时数仓 Apache Doris 精选实践</p><p></p><p>日增百亿数据，查询结果秒出， Apache Doris 在 360 商业化的统一 OLAP 应用实践</p><p></p><p>Apache Doris 在叮咚买菜的应用实践</p><p></p><p>天眼查基于 Apache Doris 构建统一实时数仓实践</p><p></p><p>星云零售信贷基于 Apache Doris 的 OLAP 演进之路</p><p></p><p>百亿大表 Join 提速 300 倍！Apache Doris 在约苗数据平台的实时数仓建设实践</p><p></p><p>特别专栏 | Video</p><p></p><p>本月，这些视频值得一看！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/390761402e3d3b5e6a7d3ae8fa6bac60.png\" /></p><p>扫/码/下/载</p>",
    "publish_time": "2023-08-11 18:04:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]