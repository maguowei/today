[
  {
    "title": "AIGC 生成代码正流行，对程序员是好还是坏？",
    "url": "https://www.infoq.cn/article/39DAkIPX97e716HJkPIa",
    "summary": "<p></p><blockquote>AIGC 最大的价值将表现为重塑生产力、重塑开发者工作流程。现在，AIGC生成的代码是否可以同于生产？AIGC会给开发领域带来什么变化？程序员该拒绝还是适应辅助编程工具？6月16日，WasmEdge 创始人和核心开发者、 Second State 联合创始人Michael Yuan 博士，品览联合创始人&amp;CTO、极客时间《AI 大模型应用开发实战营》讲师彭靖田，亚马逊云科技大中华区首席架构师邓明轩做客《极客有约》，一起聊了聊AIGC与程序员的那些事儿。本文根据直播内容整理，大家可以<a href=\"https://www.infoq.cn/video/D8fRLD8POS4Nq0PGHVrH\">点击查看完整视频</a>\"。</blockquote><p></p><p></p><h2>AIGC 生成的代码如何？</h2><p></p><p></p><p>吴少杰：我提前给三位老师准备了一个问题。作为管理者，我们可能会遇到一些与大语言模型生成代码相关的考虑。现在我将展示一段设置好的代码，邀请三位老师进行点评。</p><p></p><p>我给大模型的描述是：</p><p></p><p></p><blockquote>你是一个python工程师，写一个自动检查公司git代码库代码合规的脚本程序，功能包括：代码里是否包含数据库密码、调用pylint工具检查代码是否符合规范等，把不规范的代码文件信息发邮件给提交代码的程序员，并抄送他的直属领导。生成的结果如下：</blockquote><p></p><p></p><p><code lang=\"plain\">import os\nimport re\nimport subprocess\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n# 邮件相关配置\nSMTP_SERVER = 'smtp.example.com'\nSMTP_PORT = 587\nSMTP_USERNAME = 'your_username'\nSMTP_PASSWORD = 'your_password'\nSENDER_EMAIL = 'your_email@example.com'\nSUBJECT = '不规范的代码文件信息'\n\n# 检查数据库密码\ndef check_database_password(file_path):\n    with open(file_path, 'r') as file:\n        content = file.read()\n        if re.search(r'\\bpassword\\b', content, re.IGNORECASE):\n            return True\n    return False\n\n# 使用 PyLint 检查代码是否符合规范\ndef check_code_conformance(file_path):\n    command = f\"pylint {file_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    output = result.stdout\n    return output\n\n# 发送邮件通知\ndef send_email(file_path, recipient, cc):\n    msg = MIMEMultipart()\n    msg['From'] = SENDER_EMAIL\n    msg['To'] = recipient\n    msg['Cc'] = cc\n    msg['Subject'] = SUBJECT\n\n    body = f\"不规范的代码文件：{file_path}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    try:\n        smtp_server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        smtp_server.starttls()\n        smtp_server.login(SMTP_USERNAME, SMTP_PASSWORD)\n        smtp_server.sendmail(SENDER_EMAIL, [recipient, cc], msg.as_string())\n        smtp_server.quit()\n        print(\"邮件通知已发送\")\n    except Exception as e:\n        print(f\"发送邮件时发生错误：{str(e)}\")\n\n# 主函数\ndef main():\n    repo_path = '/path/to/your/git/repository'\n\n    for root, dirs, files in os.walk(repo_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n\n            # 检查数据库密码\n            if check_database_password(file_path):\n                send_email(file_path, RECIPIENT_EMAIL, CC_EMAIL)\n\n            # 使用 PyLint 检查代码是否符合规范\n            pylint_output = check_code_conformance(file_path)\n            if \"Your code has been rated at\" not in pylint_output:\n                send_email(file_path, RECIPIENT_EMAIL, CC_EMAIL)\n\nif __name__ == '__main__':\n    main()\n</code></p><p></p><p>邓明轩： 这个问题实际上是对大型语言模型的Prompt描述，我认为这也是当前程序员需要学习的一项技能，你必须学会如何与大型语言模型进行交互，你可以告诉它代码块是什么样的，请它简化或转化为一个类或打包到一个类中完成任务。因此，Prompt是一项非常重要的技能，我提醒大家要去学习，并在实际工作中不断提升，这将决定了后续使用这种基于大型语言模型生成代码工具的效率。Prompt写得越好，效率就会越高。</p><p></p><p>关于Prompt，吴老师使用的是中文，我个人认为，在当前的环境中，如果你的英文水平能够达到一定程度，最好开始使用一些英文的Prompt。我们必须面对这个现实，即大型语言模型目前无论是国内还是国外训练的，它从各种资源中获取的数据更多是英文的，因此大型语言模型对于理解英文的偏好和能力要强得多。当然，我们希望中文的表现越来越好，但这之前，我们应尽量适应这种工具的发展。</p><p></p><p>可能有人问：有了大语言模型，我们是否还需要学习英语？根据我的个人使用体验，虽然目前大型语言模型在翻译等方面的表现已经相当出色，但用英语直接与它进行交互会有获取更多信息的能力。因此，从短期来看，我仍然认为学习英语会有所助益，尽管对许多程序员来说英语一直是个难题。</p><p></p><p>就生成的这段代码而言，总体结构非常完整，代码看起来几乎可以运行，但一些配置项可能需要进一步编写。此外，它提到了SMTP。在很多年前分析SMTP协议时，我们需要了解各种配置和端口等信息，如果现在要我突然写这样的代码，可能要花很长的时间去检查各种细节，不过有了这些工具就可以帮助生成相关代码。</p><p></p><p>我之前与少杰讨论过这个话题：如果你的基础扎实，能力为2，那这个工具就是一个倍增器，可以增加40倍，变成80；但如果你的能力只有1，那么它只会增加40。**你的基础能力在使用这个工具时会被放大。**因此，我建议大家深入了解编程语言的本质以及底层技术等知识，你可能无需记住具体的字段或main结构，因为现在大型语言模型可以帮助生成。</p><p></p><p>彭靖田： 这个问题，我从两个角度来考虑。首先，大型模型有能力生成代码，但最终这段代码要能运行才会成为有价值的交付物。因此，我们通常会将生成的代码与开发人员编写的代码进行比较，包括代码是否适合进入生产环境。这其中的一个重要方面是Prompt的应用，比如当我们讨论一段代码是否适合在生产环境中使用时，九要考虑我们的Prompt是否会检查密码以明文形式存储。</p><p></p><p>现在，我们只进行了一轮的对话和询问，目前的输出结果已经足够好，开发人员能提供这样的代码也相当不错了。但这段代码要投入生产环境，通常第一反应可能是是否需要一个配置管理器或将配置保存为YAML文件？例如在端口号和配置项中出现的用户名和密码，生成的代码将密码作为明文字符串放置在代码文件中，这不是一个很好的做法。这个问题也在于在初始需求中没有给大语言模型足够详细的信息，但通过进行多轮对话，大型模型可以更好地解决。</p><p></p><p>关于一个优秀的大型模型或使用大型模型生成良好代码的最佳实践，我认为有两个关键点需要注意。第一是描述清楚需求细节，甚至可以将自己视为一名架构师，将大模型想象为拥有不同背景（例如前端、后端脚本等）的员工，作为架构师的你应该如何提出需求？第二是要多轮问询、逐步完善。我做过一个小实验，使用GPT-4完整地生成了一个开源项目，目标是进行完整的双语电子书翻译。这个项目的整体代码库可能有几千行，完全是由GPT-4生成的，但是经过了许多轮的对话。</p><p></p><p>站在架构师的角度，最初的问题可能不是具体的100行代码程序，而是让大模型理解你如何设计这段代码，例如需要分成哪几个模块、数据库密码检查是否只需要一个简单的函数就够了等，所有这些都可以通过架构设计来完成。总之，多轮对话、逐步完善并将自己视为架构师，用这种自上而下的设计让中间节点和叶子节点的代码变得更加友好。</p><p></p><p>邓明轩： 我完全同意彭老师提到的观点。作为开发者，我们应该将自己视为架构师来思考这个问题。对于这些编程工具、AI工具，我们可以称之为Copilot，辅助驾驶而非主驾驶。因此，我们真正使用的时候，不能期望只描述一个需求就可以迅速完成一个完整的软件。相反，它为我们提供了代码块，我们有责任将这些组织起来，包括项目的工程时间规划、项目管理等，只有通过自己的思考，我们才能更好地利用这个“副驾驶”。我们不能把方向盘交给它，让它代替我们驾驶，这是不合适的做法。</p><p></p><p>另外，彭老师提到我们的任务是检查安全性，而在生成的代码中又涉及密码等敏感信息，这引发了一个问题：当前的语言模型是基于Transformer架构的，而且大多数情况下是盲目生成的，它缺乏自省能力，如果直接向它提问，它会按照神经网络的方式逐字生成结果。因此，我们需要进行多轮对话。也就是说，当我向它提问并生成了一段文本后，我可以将这段文本反馈给它，询问是否符合要求或者提出其他的要求。这些都是与大型语言模型进行互动的良好实践。</p><p></p><p>Michael Yuan： 我很大程度上同意两位老师的观点，这里只提出一点：当运行一个Python脚本时，很可能是在本地环境下进行分析，在这种情况下，使用明文密码可能并不是一个问题。</p><p></p><p>那么，这就涉及到Prompt 工程师在做什么的问题。Prompt 工程师是否会最终消失？我觉得不太可能，因为Prompt 工程师是在应对需求并解决需求，就和人与人之间的对话一样，需要经过多轮交流。</p><p></p><p>现在，我们公司里大约一半的开发者都在使用Copilot，每个人每月支付大约10美元，因为有一些程序员非常依赖Copilot。他们之前是这样使用Copilot的：描述一个算法，然后让它生成相应的代码，而并不是像今天这个案例一样，已经将业务需求分割好后再描述一个具体场景。比如我需要对一个向量进行排序，这都是已知的算法，但我懒得找，所以让Copilot来帮忙，我写一个简短的提示，然后让它生成代码。这就是Copilot和GPT-3时代生成代码的方法。</p><p></p><p>今天，我们生成代码需要进行多轮对话。ChatGPT令人惊讶的地方在于能够跟随对话，我们可以继续提问，可以在对话中指代之前提到的内容，而它也能理解我在指代什么。当我提供描述并生成代码后，可以运行该代码并将结果反馈给它，告诉它哪里出错了，然后它可以继续生成下一轮代码，通过多次迭代，生成的代码会越来越符合预期。</p><p></p><p>我认识一个CTO，他有大约10年没有写过代码了，最近他想要创建一个Discord机器人，但对于如何使用Python处理他并不熟悉。于是，他向ChatGPT描述了想要实现的功能，然后ChatGPT生成了一段代码。初次生成的代码并不能正常工作，他就将错误信息反馈给了模型，然后重新生成了一段代码。经过几轮的交互，他花了大约半天的时间在一个不熟悉的领域中生成了大致能够运行的代码。我觉得这展示了ChatGPT的能力，尽管并不能完全理解需求，但通过不断交互就能够逐渐实现想要的结果。</p><p></p><p>这并不是说只需简单的描述，大语言模型就能完全理解。实际上很多时候，我们自己都并不清楚究竟想要什么，我们可能需要先看到一些代码，然后逐渐明确自己的需求。这个过程可能需要花费相当长的时间。尽管如此，我仍然觉得这是一项令人印象深刻的技术。</p><p></p><p>这也给编程工具的用户界面带来了很大的挑战。Copilot的使用非常简单，你只需在其中编写注释，然后让它将注释转化为相应的代码。这是一次性的代码生成过程，然而要实现更复杂的功能，就需要一个能够支持对话的用户界面了。</p><p></p><h2>生成代码的质量与安全问题</h2><p></p><p></p><p>吴少杰：AI 生成代码的质量高低与哪些因素有关？如何消除AI生成代存在的安全隐患？</p><p></p><p>邓明轩： 这里涉及两个问题：质量和安全性。质量与我们对Prompt工程的学习和掌握有关。而安全性则有两个方面需要考虑：首先是工具本身提供的安全辅助功能，另一方面是作为代码使用者，我们如何考虑安全问题。</p><p></p><p>我想提一下亚马逊云科技推出的Code Whisperer工具，它实际上对代码的安全性进行了检查和提示。当然，尽管可以使用工具来辅助我们更好、更快速地发现安全问题，但我们仍然需要对项目的代码负责。例如，我们需要考虑是否可能泄露敏感信息或者代码是否可能被错误使用，尤其是在涉及对外服务的方面，我们还需要考虑这些服务是否会对其他系统造成重大损害。</p><p></p><p>大型语言模型的多轮对话会受限于容量，可能只有几十KB，因此档面对复杂的项目，或者一个由10人或20人组成的团队时，其容量可能远远不够。此外，还有整个软件环境，包括软件设计和需求描述等方方面面的考虑。因此，作为程序员，我们需要对这个领域有全面了解，从头到尾清楚了解相关结构。在这种情况下，我们会将其中的细微问题拆分出来，与大型语言模型进行交互，一定程度上避免由于容量限制而无法处理复杂的逻辑跟踪。当然，这也涉及到安全性的问题。</p><p></p><p>回到质量问题，我认为质量同样是程序员的责任。在这方面，工具也对我们提出了一些要求，如果你的代码质量较高，那它将更容易理解，生成代码的可用性也会更高。最终代码质量的责任仍然在于代码的拥有者，也就是我们自己，我们需要真正对此负责。</p><p></p><p>Michael Yuan： 对于代码质量，有一些简单的方法可以采用，例如将代码与编译器进行链接，使生成的代码立即进行自动编译，并通过编译器提供的错误信息进行修复。此外，还可以与数据库等工具结合使用，自动扫描检测常见的问题。所以，代码质量并不是一个无法解决的科学问题，而是一个工程问题，我们可以通过各种工具的结合来不断接近最优解。</p><p></p><p>不过，目前对代码质量的控制问题可能会有一些争议。目前，从个人角度来评判代码质量的一个重要标准是可扩展性，也就是代码编写后是否容易修改。然而，现在有一些流派认为，未来尤其是对于前端框架而言，这种代码质量可能并不重要。</p><p></p><p>可以设想一个未来的场景，整个应用程序都是由大型模型生成的，不存在需要人为修改的问题，如果需求发生变化，只需重新生成代码即可。某些场景下，每次生成的代码都只使用一次，不存在扩展代码的问题。当然，现在可能还无法实现这一点，但这是一个可能出现的未来，如果成为现实，那么会对代码质量的评价产生重大影响。因此，我认为在架构上编写可扩展且优雅的代码并不重要，至少在某些场景中不重要。</p><p></p><p>我们再谈谈第二点，即安全问题。我认为，人类编写的代码在某种程度上也是相对安全的，否则就不会存在那么多问题，也不会有像 Rust 这样的语言出现。对于安全问题的解决，人们在编写代码时已经用了很多工具，例如 Rust 通过编译器解决内存安全问题、Java 通过运行时方法解决内存安全问题等。</p><p></p><p>多轮对话的有趣之处在于，生成的代码可以通过工具运行，然后将工具的结果反馈给模型，然后模型做进一步优化，这个过程可能比人类做得更好。</p><p></p><p>彭靖田： 我最后补充一个我个人非常认同的观点，即关于“我们今天应该如何解决 AI 生成代码的安全隐患？”这个问题没有太多可讨论的，因为这与 AIGC无关，代码就是代码，无论是谁生成的都存在安全隐患，所以我不想讨论这个问题。</p><p></p><p>我更关注生成代码的质量高低，也就是 AI 生成代码的问题。目前，有两种对此完全不同的观点。有一类观点认为，每个人都应该开发自己的大模型，因此出现了很多专注于大模型的创业公司。然而训练语料库是有限的，在 OpenAI 和其他国外大厂已经做了很多投入的情况下，是否值得从零开始开发一个大模型还需要探讨。我们需要知道的是，一些大模型并没有特别针对编写代码这一任务进行增强，特别是当我们要在生产环境中将其作为生产力工具替代选择时。</p><p></p><p>我们招聘一个研发工程师时为什么要写招聘要求？为什么要对研发工程师进行分类？因为他们花了很多时间进行自我训练，他们的训练来自各种类型的教程、框架手册、项目实践等。如果你的公司有自己的代码库，可能没有开源，或者你正在进行一个特定领域的项目，有许多开源框架和基础项目代码，那么是否应该进行 fine-tuning（微调）？是否可以提升模型的权重，使通过 fine-tuning 得到的模型能够更好地解决代码生成问题，而不是从零开始开发一个模型？我认为这可能是一个可以考虑的方向。</p><p></p><h2>关于AI辅助编程工具的使用建议</h2><p></p><p></p><p>吴少杰：三位老师平时会AI辅助编程工具来做些什么？是否可以提供一些使用建议？</p><p></p><p>Michael Yuan： 我们团队主要使用Copilot来生成代码，因为它与GitHub的IDE结合非常紧密，我认为这一点非常重要。如果你要在其他地方使用它，你就需要打开另一个用户界面（UI），这将增加工作量。然而，刚才提到的多轮对话生成应用并且使用人工工具进行检查，这是一个很好的方向。比如，我可以将它集成到问题跟踪（Issues）或讨论（Discussion）中。</p><p></p><p>关于代码质量本身，有很多工具可供选择。今天代码中的许多问题都源自所谓的供应链问题，这意味着你依赖于其他人的代码，如果其他人的代码出了问题，你的代码就会受到影响。另外，复制、粘贴的代码可能会在大型模型中带来更多问题。这段代码可能不是在NPM或Cargo的包管理器中引入的，而是来自Apache或其他许可证的，如果出了问题或者上游进行了修复，开发者可能并不知道，这种情况下就需要更深层次的代码检查器。</p><p></p><p>在AIGC中，这种代码检查器也特别有用，其实就是用同一个工具解决可以两个问题：一个是解决代码来源和许可证不兼容的问题；另一个是，如果上游代码出了问题，有工具可以跟踪并通知开发者，然后对下游代码进行相应的更改。</p><p></p><p>当然在编写代码后的测试是必不可少的。我们在用的Rust有很多工具选择，如快速测试（fast testing）。实际上，快速测试也是一种机器学习方法，它不断测试输入和输出边界，然后看代码是否出错。在这方面，我认为大型开源项目会提供这种专门的服务。我们的项目为什么加入CNCF，主要是因为CNCF与Google合作提供了这样的服务。但这需要大量的机器时间不断地进行编译和测试。</p><p></p><p>另外，每个项目的情况可能都不一样。我们使用的是Rust，因此我们有一套特定类型的工具。而对于使用Python的项目，可能要用其他不同的工具，因为它没有内存安全的问题，但有其他要解决的问题。</p><p></p><p>彭靖田： 我觉得目前最好用的工具还是AI辅助编程工具本身。大部分工作仍然是基于大模型进行的，开发框架的迁移还不够成熟。</p><p></p><p>我认为从安全的角度考虑是非常重要的。比如，对于我们要与Autodesk工业设计软件（如AuthCD和Rivet）竞争开发的SaaS产品，如果接入AI生成的代码，需要如何处理呢？实际上，传统的软件工程方法仍然非常有用。软件工程告诉我们需要进行环境划分、编写良好的测试用例、采用不同的测试方法，并在各个环境中有测试人员关注一些边界情况和使用场景。</p><p></p><p>举个例子，我们有个环节是将用户上传的CAD图纸通过计算机视觉和深度学习算法转换为三维模型。但建筑设计师绘制的CAD图纸也存在很多问题，他们没有编译器，也没有测试环境，只能依靠施工现场的人逐个解决。这其中有两类测试问题：一种是代码确实存在问题，这类问题可以通过软件工程的方法解决；还有一类问题更偏向于算法和领域场景，即输入数据本身就不够健壮和稳定，这种设计上的问题需要进行降级处理，捕捉异常。</p><p></p><p>邓明轩： 现在的大语言模型在架构上有多种路线，但是现在有一种观点认为，模型的架构本身并不重要，只要我们有充足的连接，即非线性的连接，并提供足够多的数据给模型进行训练，它就能够产生很好的效果。因为这些模型内嵌了大量人类知识，尽管不是全部但可以通过使用足够大的模型将文本输入其中，进而实现预期的效果。因此我个人认为，底层的算法结构可能并不会对上层的应用产生太大影响。</p><p></p><p>关于代码质量检查工具的使用，我同意两位老师提到的观点，即最好的评估方法就是运行它。现在普遍使用的是解释型语言和编译型语言。在这种情况下，解释型语言具有一定的优势，因为可以直接将生成的代码输入语言环境中进行解释和运行，捕捉可能的错误，然后返回给大语言模型。对于编译型语言如Rust，需要设置额外的运行时环境来进行编译和运行。总之，对于代码质量来说，“无论是白猫还是黑猫”，只要能够满足需求就是好的。</p><p></p><p>此外，现在许多与大语言模型相关的接口使用的都是Python，在神经网络和机器学习领域，Python是主流语言之一，具有动态解析和良好的语言支持。我认为，未来可能会进一步推动Python的发展，比如在大语言模型中生成一段代码后，直接在Python的运行环境中进行评估和调试，然后返回结果给大语言模型。</p><p></p><p>现在有许多与语言相关的LangChain或其他外围框架，这可能是未来的发展方向之一。通过额外的框架配合，可以在语言的运行环境中进行多轮对话，自动检查代码质量，无需人工干预，我们只需要描述要完成的任务，然后让系统开始运行即可。如果出现错误，系统会自动进行修改，并再次运行。</p><p></p><p>关于代码安全性，实际上在我了解CodeWhisper 时，也思考过版权问题。我们使用大语言模型生成的代码，但对于它的版权关系并不确定，比如它使用了什么开源许可证，如果你的代码在结构上与某个开源许可证中的代码完全一致，那根据过去的案例，你可能会被诉讼。这对于使用这些工具的程序员来说是一个挑战。</p><p></p><h2>零基础如何使用AIGC工具</h2><p></p><p></p><p>吴少杰：这里有个观众问题道，“零基础学员如何使用好这些工具，需要提升哪些点？比如大模型给出几段代码，但是我却不懂，很好的调试可以达到执行。”</p><p></p><p>邓明轩： 如果是零基础，那么了解一些编程概念是有必要的。如果有一些基础，只是不够深的话，那与大语言模型进行互动是一个不错的方法。</p><p></p><p>可以从简单的例子开始，比如我们之前写代码时常用的“Hello World”。现在你可以要求模型给你一个Rust版本的“Hello World”，虽然可能一开始看不懂，但可以直接运行这段代码，如果出现错误了就把错误信息反馈给模型，询问出了什么问题，这样的交互循环可以让你快速学习新技术。另外，还可以选择一段代码并要求模型解释它是做什么的。然后进一步问，比如要在其中添加一个循环或者一个条件分支该如何实现？总之，你可以慢慢学习，逐渐形成对编程的基本理解，关注在代码的运行逻辑上，而不是语法结构。</p><p></p><p>每种语言都不同，你并不需要详细研究每种语言的具体细节，只需要理解基本的概念，例如循环、分支，包括递归以及更复杂的数据结构等。关键时通过交互的方式逐步构建这些概念。我认为，有了这些工具之后，学习编程会更快速，而不是有些人担心的那样“程序员会失业，不需要学习了”，这实际上降低了编程和与计算机对话的门槛。</p><p></p><p>Michael Yuan： “有了更好的工具后，程序员就要失业了。”这种话我听了二十多年了。我还记得过去有种说法，“大家都不要学计算机了，因为都要印度人去做了，你们没有机会了。”但事实证明并非如此。我对大语言模型的理解是，它是程序员的工具，程序员会用它来辅助其他人工作，这就是我不同意OpenAI那篇报告的原因。</p><p></p><p>OpenAI的报告声称洗碗和刷房子不会失业，但实际上洗碗也会失业，因为程序员会有更多的时间开发出洗碗机器人，取代所有洗碗的人，所以程序员也是可以卷走其他人工作的。大模型可以大幅提高程序员的能力，有了大模型后可以很快地学习和理解。</p><p></p><p>回到之前的问题，之前计算机科学的书籍和教育对我们有很大的影响，我自己也写过几本书，写书的时候通常会从背景开始、介绍概念开始。有些语言容易有些就难，有些语言概念非常多，比如Rust概念很多，学习起来比较困难。但使用大模型后，它可以解释很多东西，还可以完全根据你的学习需求进行定制。</p><p></p><p>为什么很多人的编程学得不好？因为编程不是一种立竿见影的事情，你需要从一些没有意义的事情开始做起，比如写一个“Hello World”能有什么用？它解决不了实际问题。但就像我之前提到的，有了大模型后，我朋友花了半天时间写了一个 Discord 机器人，你也可以轻松地写出类似的东西。你可能并不完全理解为什么这样写，但你可以逐步改进，通过自己的努力学习快速掌握它的用法。</p><p></p><p>邓明轩： 我当年学习编程语言的时候，买了一本书，还要去机房上课，然后我们敲了一些代码却出现了错误，我们不知道什么意思，老师也不懂，甚至书上也没有解释。我们只能自己摸索和尝试。在那个年代，学习计算机编程是非常困难的。我不知道提问的观众是多大年纪，可能年龄较小。你们非常幸运，生活在这个时代，有机会接触到不断发展和提升的技术。我鼓励你们充分利用这项技术，因为未来可能会有各种惊人的可能性出现。</p><p></p><p>彭靖田： 之前，我将评估一个优秀开发者技能的维度分为四个部分：首先，编程和调试技能，也就是实际操作的能力，占比40%；其次，项目管理和协作能力，占据30%的权重，这意味着作为一个独立的开发者，你能够交付代码，并且能够与团队、需求方、测试方以及其他开发团队成员进行协作；接下来，算法和数据结构，占据20%的权重。计算机科学专业的人可能对数据结构和算法会有更深入的理解；最后的10%留给软件工程经验，也就是在实践中积累的最佳实践经验以及对优秀软件工程师的观察和学习。</p><p></p><p>但是现在，我有了不同的看法。关于那40%的技能，我会进一步抽象为大模型的应用实战能力。这意味着你有更多的最佳实践，而不是仅仅学习某种编程语言，浪费大量时间和精力去学习每一个知识点。正如这位提问者提到，前后学了QBasic、Basic、Java、C++、Python、Go和Java的派生版本等多种编程语言，这些语言都介绍了基本的数据类型，这可能会带来一些困扰和混乱。但实际上，数据类型并不是很重要。</p><p></p><p>一个没有学习过编程语言的人应该如何做？以前我们会从基础语法开始学起，但现在语法并不那么重要了，重要的是对编程语言的理解。比如理解编译型语言和解释型语言的区别，为什么自Python 3以来一直在强调类型标注？为什么一个本来应该非常灵活的语言现在越来越强调类型，并且推出了许多相关库？事实上，这就是所谓的Prompt实际上变成了与英语一样重要的语言技能。</p><p></p><p>还有20%我留给了人机和团队协作。现在我们可以与人工智能和大型模型进行协作，它们不会抱怨、不会说我们蠢，也不会在背后说坏话。如果你无法与这样的一个合作伙伴进行有效协作，那肯定是你自己的问题。</p><p></p><h2>这些工具还有哪些问题</h2><p></p><p></p><p>吴少杰：现在的辅助编程工具还存在哪些问题？提供厂商需要做哪些改进才能更好满足程序员的需要？</p><p></p><p>邓明轩： 目前工具的交互模式和发展都处于不断演进的过程中，还有很多空间可以探索。近一年来，我们经历了一个技术爆炸，整个行业也在思考这些工具应该具备怎样的形态。因此，过多地关注当前工具的局限性可能并不是非常有益。也许三五年后回头来看，我们会发现现在的情况非常有意思。</p><p></p><p>不同的厂商推出了自己的工具，但他们并不是要将这些工具作为盈利手段，而是通过这些新的交互模式将用户引入到某个平台上。对于如何将这种能力更多地扩展到用户手中，大家应该有相对一致的想法。我个人的体验是，我真的很希望有一个像《钢铁侠》中的贾维斯那样的助手，我可以用自然语言与他进行沟通，他能理解并帮助我完成许多任务。这种辅助性工具确实是我非常期待的东西。</p><p></p><p>Michael Yuan：我觉得投资人可能更加焦虑，因为他们可能会发现某个工具很好，但过几天就被OpenAI或其他公司干掉了，甚至有些简单的工具已经被别人复制了。目前工具方面有很多创新。例如OpenAI最近发布的Function Core可以直接从大模型生成数据结构，然后与核心函数结合。</p><p></p><p>工具创新的领域很多，甚至比底层大模型的创新和迭代速度都要快。为什么会这样呢？因为底层大模型需要耗费大量资金和时间进行训练，而现在正是一个百花齐放的时代，有大量不同的想法和方向。对我们来说，一个特别有用的工具是如何让多轮对话生成代码的界面更加友好，能够吸引更多人参与。以前的研发管理工具中并没有考虑到这一点。</p><p></p><p>吴少杰：三位老师观察，周围使用AI工具来生成代码的人多还是不用的比较多？各自团队的研发效率大概能提多少？ 20% 还是30%？</p><p></p><p>Michael Yuan： 实际上我们有自己的数据，但我认为还不足以进行直接比较。Twitter上有很多比较。实际上，一些经验丰富的人更不愿意使用AIGC，因为他们认为自己可以写得比AIGC更好。而那些比较初级的或者经验较少的程序员愿意使用AIGC，他们代码编写速度和完成度要比那些资深程序员高得多，这不是两倍的差距，而是几十倍的差距。当然，也可以说经验丰富的开发者写出来的代码更容易扩展。但是如果有了几十倍的差距，那么下次直接重写就好了。</p><p></p><p>邓明轩： 我观察到，小型初创公司在技术选型上会更快一些，因为他们可能更加灵活。对于一些大型企业，特别是传统企业，开发部门可能比较庞大、流程也比较复杂，目前找到合适的方法将这些纳入到整个软件工程流程中仍是一个问题，所以我认为还需要观察。</p><p></p><p>如果你的定位是写一些胶水代码或者试验性代码，那可以快速适应。另外，如果你所在的公司比较灵活，我建议大家去尝试。如果你有一个由5个或10个人组成的团队，并且你们的效率是巨大公司的30～40倍，那么你就可以与这些大型公司竞争了。</p><p></p><p>当然，对于传统的大型软件开发公司或大型IT部门来说，短期内将这些融入到开发流程中具有挑战性，大家可能还没有完全明白该怎么做。我们可以稍微放慢一些节奏，在比如半年或一年后，再来看看如何将这些纳入整个团队中。</p><p></p><p>彭靖田： 这是一个趋势性的事情。拿我们团队来说，我们目前没有专门的运维人员，只有一个后端同事兼任运维工作，如果是一个大公司就会需要一个团队。他需要管理华为、亚马逊云科技以及字节的云平台，每个云平台都有自己的Kubernets集群，这意味着他需要管理7～8个集群。一个云平台上可能会有多个环境，而且不同环境之间的命名空间和优先级可能也不同。七八年前刚接触Kubernets时，我就觉得这是一项非常复杂的工作，包括今天的Kubernets生态社区中，很多人就在做调度策略、HPA等各种工作。</p><p></p><p>但实际上，大多数团队只使用了容器云的一些基础功能，特别是在运维方面，大部分工作都可以自动化。比如GitLab现在强调DevSecOps。整个CI/CD流程实际上由许多脚本组成，以前这些脚本被认为是运维专家的特殊技能，但实际上并没有那么神秘，也没有那么多技巧，大模型可以解决这些问题。</p><p></p><p>我们那位后端同事使用了很多自动化技术，但还没有将GPT-4作为日常工具，最近两个月里他一直没有解决一个偶发性问题。我们一直使用较大的节点，最小的节点大约都有32个核心和近200GB内存的资源池。在这样的一个节点上，我们通常部署200-400个应用程序，而这些应用程序的请求资源可能只占用0.5个CPU核心和2GB内存，但这时候，他们的请求已经接近我们的资源上限。但它是一个流水线，不断地在消耗资源，以至于有的应用程序刚启动，另一批应用程序可能已经关闭了。</p><p></p><p>这里的问题在于，多云环境中的底层硬盘共享有一个阈值限制。当应用程序特别多时，这个默认阈值可能会被打破，导致应用程序无法启动。他发现应用程序无法启动，但CPU、内存资源都足够。因为多云在虚拟化存储时，借用了一部分硬盘来进行虚拟化存储，但实际上用不满。他无法解决这个问题，我获得权限后把那个节点上的日志原封不动地交给GPT-4，它告诉我只需重启一个命令，将阈值改为合适的值就可以了。起初，后端同事还不相信，他去Stack Overflow上查找，但结果各不相同。最后他按照指示执行了命令，问题解决了。</p><p></p><p>对我们这个同事来说，这个问题只是一种认知上的偏差，但提升的效率是非常显著的。对于那些1～3年里都在写SQL和编写CRUD业务逻辑的开发工程师来说，未来的代码效率也会大大提升。</p><p></p><h2>AIGC对程序员的影响</h2><p></p><p></p><p>吴少杰：当大规模使用AI辅助编程工具时，企业对程序员的要求会发生哪些变化？</p><p></p><p>邓明轩： 我认为，现在互联网降本增效和技术变革可能不处在同一个趋势上。技术变革主要受疫情和国际经济关系等因素的影响，个体很难改变，我们只能在这种情况下尽力发挥自己的价值，比如在降本增效的压力下如何展现自己的能力。</p><p></p><p>作为个体，我们应该如何适应这个新环境？首先，我们需要在心态上保持开放，接受这种新事物，迅速了解并进行实践。正如刚才提到的，实际上事情变得更容易了，而不是更难了。</p><p></p><p>另外，要学会学习的方法，这非常重要。未来，具体编程语言的语法结构等可能并不那么重要，重要的是如何具备良好的学习能力、快速适应新环境。我认为这是一项非常重要的技能，这次的技术变革就给我们提出了新的挑战。希望大家保持积极乐观，因为工具也带来了很多机会。</p><p></p><p>Michael Yuan： 程序员有一个优良传统，就是喜欢亲自动手去做，而不是空谈。在技术领域，那些只会谈论却无实际行动的人会让人感到厌烦。大型模型特别好的一点就是，它们可以让人立即动手尝试，我其实很喜欢这一点。</p><p></p><p>彭老师也讲他的整个项目都是通过AIGC完成的，包括项目的推广和市场营销都是由模型生成的。这个过程中有很多工具可以用，一旦尝试了这些，你就会知道这个东西的大致边界在哪里，因此也会有正确的预期。最难的一步可能是迈出第一步。</p><p></p><p>彭靖田：企业对程序员的要求会有哪些变化？实际上，我认为这个问题并不是从甲方的角度提出的，因为甲方对程序员的要求一直都是一样的：快速、高质量、低成本，就是要多快好省。从这个角度看，无论是程序员、开发人员还是内容生产者，作为提供服务的人，我们需要解决的问题就是如何更快、更好地完成工作。</p><p></p><p>我们和动物的区别在于我们懂得利用工具，而且我们的工具变得越来越巧妙、易用，我们也越来越擅长改造工具，使其更适合人类的需求。从这个视角来看，我们每个人都应该迅速拥抱这些新工具。想象一下，如果你的老板让你打车，你会站在路上等出租车吗？不会吧，你会使用滴滴、高德地图、美团打车，这个逻辑很简单明了。</p><p></p><p>嘉宾们的自我介绍：</p><p></p><p>吴少杰（特邀主持）：</p><p></p><p></p><blockquote>我之前在 58 集团、新浪微博负责过推荐算法相关的工作，现在在垂直行业做智能算法，给业务赋能。</blockquote><p></p><p></p><p>邓明轩：</p><p></p><p></p><blockquote>我已经从事编程工作23年了。我2000年毕业，陆续加入了IBM和苹果等公司。目前我在亚马逊担任首席架构师。个人而言，我对编程非常感兴趣。事实上，从我毕业至今的20多年间，我一直都在编写代码。尽管最近一两年由于工作中的事务性任务增多，我编写代码的机会相对较少，但我仍然对此感兴趣并愿意自己去写代码。在当前通用人工智能大语言模型的发展中，我注意到它对整个编程技术产生了巨大影响。我很高兴能够在有生之年经历这一切。</blockquote><p></p><p></p><p>彭靖田：</p><p></p><p></p><blockquote>我从中学开始接触编程，有大约15年左右的编程经验了，至今我一直在编程语言和算法一线。2015年，我在大三时开始接触创业，跟着浙大学长拿到了真格徐小平老师的天使轮融资，做了一个辅助诊疗的APP，是基于NLP和深度学习的AI系统。从加州大学访学结束后，我加入了华为2012实验室，从零到一参与了华为深度学习平台ModelArts 的设计和研发工作。之后我作为技术合伙人加入了才云科技（2020年被字节跳动收购后，整合为了字节火山云）。现在这家公司——品览是我的第三段创业经历，我是公司的联合创始人和CTO。我们致力于用AIGC赋能建筑设计，弯道超车AutoDesk的产品。除了创业外，我也非常喜欢开源。从上一轮深度学习的火热时期开始，我在很早期参与了Tensorflow项目的开源贡献，2017年与华为两位同事一起合作出版了国内首本深度剖析TensorFlow框架的热销书《深入理解TensorFlow》。在才云时期，我们与谷歌云团队共同发起了了一个Kubeflow的开源项目，它现在已经成为MLOps领域的事实标准之一。现在我积极拥抱大模型，也在极客时间开设了《AI大模型应用开发实战营》课程，欢迎订阅和交流。</blockquote><p></p><p></p><p>Michael Yuan：</p><p></p><p></p><blockquote>相比大家，我的编程起步比较晚，因为我是在博士毕业后才正式开始编程的。当时找不到工作，于是决定尝试寻找程序员的职位。我发现大厂不愿意雇佣没有实际编程经验的人，所以我决定从开源项目入手。那是很多年前，当时 Java 刚刚在服务端起步，所以我早期参与了 JBoss 这个开源项目，我们开发了当时也是现在最流行的 Java application server。后来我们将公司卖给了红帽软件公司。之后，我一直在开源社区中进行项目开发，也在公司工作过，还做过投资人，一直与开源社区和开发人员保持密切联系。期间，我出版了5本书。&nbsp;我目前有一个开源项目叫做WasmEdge，它是CNCF的一个项目。我们主要开发Rust和WebAssembly (Wasm) 在云原生与 AI 的应用框架与运行环境，大家可能会觉得Rust语言很受欢迎，但也有很多人认为它很难学。我们现在认为，使用大型语言模型来教授人们学习Rust是一件非常有趣的事情。另外，因为我从事研发管理工作已经很多年了，我发现使用大型语言模型，确实可以提高开发效率。</blockquote><p></p>",
    "publish_time": "2023-07-08 10:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁链 Web3 技术攻坚再进一步：推出基于零知识证明的可信计算架构",
    "url": "https://www.infoq.cn/article/FPHLp87Iko2aswqbDWe2",
    "summary": "<p>7 月 8 日，在 2023 WAIC 全球区块链产业高峰论坛上，蚂蚁链宣布隐私协作平台 AntChain FAIR 进行全新架构升级，引入零知识证明（ZKP）为核心的可验证计算技术，从可信数据流转拓展到计算过程、数据属性以及身份的可验证。据了解，这是 FAIR 平台继去年提升大规模计算能力后，一次针对核心架构的重要升级，也是蚂蚁链在 Web3 技术领域的持续突破。</p><p></p><p>零知识证明被业内看作是 Web3 下一个 5 年最关键的技术之一，它指的是证明者能够在不向验证者提供任何有用信息的状况下，使验证者相信某个论断是正确的。相较于隐私计算强调在保护数据隐私基础上进行数据分析和计算，零知识证明的特点在于“可验证”。</p><p></p><p>采用零知识证明架构的企业和机构，可以在隐私保护基础之上，确保数据计算的整个过程都是按照约定条件进行的、是正确且可被验证的，这将大大提高身份验证、AI 模型训练、数据流转、资产交易、链上链下协同等全流程可信度，加速 Web3从数字世界向更复杂的产业场景渗透。近年来，随着零知识证明在基础协议和硬件加速层面的高速发展，该技术已经逐渐能够处理诸如神经网络推理这类复杂的任务。</p><p></p><p>据了解，蚂蚁链在 Web3 技术领域全面自主可控，区块链、隐私计算等授权专利全球第一。作为国内最早布局 Web3 技术的厂商，蚂蚁链曾自研推出区块链及服务 BaaS、 区块链一体机、区块链通信网络 BTN、区块链存储引擎 LETUS、大规模开放架构 HOU、跨链 ODATS 等标志性产品，广泛覆盖 Web3 基础设施。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/33/17/339c52aff7f42845e982f02f96c58017.jpg\" /></p><p></p><p><a href=\"https://www.infoq.cn/news/5KJiUZKVdGQDHOczrupV\">蚂蚁</a>\"链资深技术专家魏长征表示：“隐私和可验证一直是数据和计算领域重要的话题，两者有相似之处也有本质不同。零知识证明具有高效的可验证特性，再加之可以保护特定数据不被验证方看见，因此在一定程度上满足了隐私和可验证的双重需求。”</p><p></p><p>零知识证明最关键的环节是“证明生成阶段”，即对数据、身份、计算过程等生成一个“证明”。据了解，<a href=\"https://www.infoq.cn/news/KvxWGt1PKhSaE1WSq5bf\">蚂蚁</a>\"链通过在软件算法优化，“证明生成”端到端加速了原有基线的 2.8 倍。</p><p></p><p>此外，零知识证明的上述特性，同样可以对计算和数据进行“压缩”，区块链典型应用就是将计算任务迁移到链下，利用零知识证明来证明链下发生的动作是可验证的，仅让证明结果在链上存储和验证。例如，以太坊等区块链平台使用基于零知识证明的叠卷技术（zkRollups），有效解决了链资源紧张的问题。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8d/bb/8d0c3ca2c864d5859d56db851f39dabb.jpg\" /></p><p></p><p>据了解，为降低零知识证明使用的门槛，<a href=\"https://www.infoq.cn/news/LHxY4zv2dPgJfWaJYxtC\">蚂蚁链 </a>\"FAIR 在可信计算架构中加入了 zkWASM——一种新型零知识证明虚拟机，实现指令级别的可证明特性，可以依托 WASM 多语言生态将多种前端语言编写的算法程序进行证明转化，而无需关心底层复杂的算法细节，提升开发应用的友好性。</p><p></p><p>FAIR 是蚂蚁链面向数据流转推出的隐私协作平台，其综合了区块链和隐私计算的优势，协作流程由智能合约驱动，数据流转由隐私计算引擎来解决，并通过区块链技术确权，登记和交易共识。可以解决数据流通过程中，数据共享与隐私保护之间的天然矛盾。目前，FAIR 已经在杭数交、贵数所等多家数据交易所得到应用。</p>",
    "publish_time": "2023-07-08 13:41:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "第四范式携大模型「式说」亮相WAIC，大模型落地进入加速时刻",
    "url": "https://www.infoq.cn/article/RuOlxrAbu26Juzbfb2Ib",
    "summary": "<p>7月6日，以“智联世界 生成未来”为主题的2023世界人工智能大会在上海正式拉开帷幕，本届大会以“生成式AI”为切入点，集中展示了大模型浪潮下的前沿科技创新和最佳产业实践。</p><p></p><p>第四范式携「式说」大模型亮相WAIC现场，这是「式说」发布以来的首次公开展示，更有第四范式「大模型之城」首秀，汇聚了大模型在金融、零售、房地产、航空、制造、司法等行业最具代表性的应用实践，受到广泛关注。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a6/af/a6d10dcf9fa96b6fa6496b52f02bfeaf.jpg\" /></p><p></p><h2>聚焦To B大模型，探索生成式AI重构企业软件机会</h2><p></p><p>与生成图片、生成海报、生成文案等大家所认知的AIGC不同，第四范式将大模型技术聚焦在企业软件领域，提出了AIGS（AI-Generated Software）的技术战略：以生成式AI重构企业软件。</p><p></p><p>用生成式AI重构企业软件，可以提升软件的用户体验及开发效率。由「式说」改造的企业软件，不管调用什么样的功能，都可以通过一段简单的对话获得结果，无需再层层点击，大幅提升体验。此外，软件的功能升级，也仅需新增或改写数据/接口规则，开发效率从「月级」提速到「天级」。</p><p></p><p>「式说」定位为ToB（企业级服务）领域的多模态大模型，拥有输入输出多模态、知识库、Copilot、思维链等核心技术，同时具备内容可信、成本可控、数据安全等“企业级”优势。首先，「式说」保证企业能私有化部署大模型，所需算力成本相对可控。其次，「式说」在学习行业知识后生成内容更加专业，并且在信息输出时定位原始出处，实现所有信息都“有据可查”。最后，「式说」能做到随着用户的反馈知错能改。为企业应用大模型进行业务创新提供数据安全、内容可信、成本可控的三大保障。</p><p></p><h2>“大模型之城”亮相 揭开大模型产业落地崭新成果</h2><p></p><p></p><p>「式说」大模型自发布以来即与国内上百家企业进行了大模型落地的相关探索，积累了数十家国内早期AIGC应用。「大模型之城」展示大模型结合不同行业后的真实实践，成为行业中不同岗位的助手，全面提升员工工作效率。</p><p></p><p>例如，在零售门店管理领域，门店管理通常涉及到员工管理、货品采购、库存控制、处理客诉等多个环节，店长需要在散落的系统中获取数据、完成管理任务，极为耗费时间。生成式AI改造后的门店管理，可直接通过向门店管理助手提问的形式，如「查看商品销量、值班人员状态、违规操作」，小助手就能从不同的系统中调用相关功能，直接给出所需信息，让店务管理更便捷。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/42/42c27fbbae3081ceed7fb3a0ebb41500.png\" /></p><p></p><p>在房产中介领域，房产经纪人要求专业化和规范化，比如说在二手房交易中，涉及购房资质审核、产权核验、付首付款与资金监管等20多个环节，经纪人的培养一直是该领域的难题。房产经纪大模型以「式说」做为能力底座，以我爱我家23年来积累的庞大知识库、规范的运营规则与沟通技巧作为高质量语料，训练针对特定行业及领域的垂直大模型。与ChatGPT相比，回答问题更加专业、可信，具备合格经纪人的基本能力。</p><p><img src=\"https://static001.geekbang.org/infoq/da/dae3e256e83106b343e45f308f319388.png\" /></p><p></p><p>（ChatGPT与房产经济大模型对比）</p><p></p><p>在工业设计领域，某商用飞机制造企业在用生成式AI改造工业设计软件，传统工业软件操作极为复杂，即便是数模查询、组装等基础功能，也需要记住具体参数，完成专业操作。有了基于生成式AI的“工业设计软件助手”后，只需向助手语音提问「帮我找类似的零件」，「给出这两个零件的装配方案」，AI可以自动从海量零件中找到合理的组装方案，所问即所得。</p><p></p><p>在制作运营领域，MOM系统（制造运营管理系统）被誉为智慧车间与数字化工厂的“大脑”，生成式AI强大的语义理解能力使人能够就制造运营核心业务与计算机高效对话，快速提供知识解答、数据及图文查询、报表生成等一系列信息及功能。此外，还能与工厂的终端设备（监控、摄像、门禁等）打通，兼职诸如车间安全、违规操作、设备状态维护等“工厂管理员”职能。</p><p></p><p>在司法领域，司法人员不仅要掌握相关的法律法规、司法解释，还要将这些灵活运用在法律分析、法律检索、法律写作、法律论证等具体实践上。生成式AI能在繁多的法律文件中快速定位并检索资料，还可以进行相应的法律文书书写、校对工作。此外，针对常见案件、速裁案件，可以协助提取审判的关键要素等。综合这些能力，都能帮助司法人员的工作更高效。</p><p>&nbsp;</p><p>更为重要的是，以上这些AI助手通过与员工的高频交流，记录了大量行为、思考等过往传统系统中无法捕捉的过程数据，通过不断思维链学习，逐步形成每个专业角色的“最佳实践攻略”，从而能为员工提供专业、个性化的辅导，提高企业整体数字化转型的效率。</p><p>&nbsp;</p><p>生成式AI浪潮正兴，产业融合发展进程不断加速，第四范式将不断打磨AI技术底座，持续为各个行业创新发展提供更多高价值的服务与应用，加快产业数字化转型进程。</p>",
    "publish_time": "2023-07-08 14:59:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯吴运声：聚焦技术底座、应用创新、助力科学计算，腾讯云MaaS能力再升级",
    "url": "https://www.infoq.cn/article/19rw6qa5K1tjavyRFifb",
    "summary": "<p>7月7日，世界人工智能大会期间，腾讯举办以“无限可能”为主题的论坛。腾讯云副总裁、腾讯云智能负责人、优图实验室负责人吴运声发表《腾讯云MaaS，激活产业发展新动能》主题演讲，分享了关于大模型行业应用观点，同时介绍了腾讯云行业大模型技术解决方案的最新进展。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d0fc3d782293fba1041dcf0f1c2313c4.png\" /></p><p>腾讯云副总裁、腾讯云智能负责人、优图实验室负责人吴运声</p><p></p><p>吴运声认为，产业场景已经成为大模型的最佳练兵场，但企业在应用大模型时，又会面临计算资源少、数据质量差、投入成本高、专业经验少等现实问题的挑战。</p><p>&nbsp;</p><p>对此，腾讯云不久前公布了行业大模型技术解决方案。该方案依托腾讯云TI平台打造行业大模型精选商店，为客户提供MaaS一站式服务，客户只需要加入自己独有的场景数据，就可以快速生成专属模型，结合实际业务场景需求，开发低成本、高可用的智能应用和服务。</p><p>&nbsp;</p><p>据吴运声介绍，继6月19日公布解决方案后，腾讯云MaaS平台能力敏捷迭代，在行业大模型技术底座、应用场景、助力科学计算等方向，均有所优化和提升。</p><p></p><p>在技术底座层面，腾讯不久前发布了自研星脉高性能计算网络、向量数据库，为大模型的行业应用提供更充沛的算力基础设施；在应用创新方面，腾讯云也将行业大模型的能力，应用到金融风控、交互翻译、数智人客服等场景中，提升智能应用效率；在科学计算方面，腾讯云始终坚持“AI for Science”，今年将大模型技术，应用在天文探索、甲骨文考释等科研场景。</p><p>&nbsp;</p><p>吴运声最后强调， AI大模型技术发展和产业探索，离不开产业链协同和生态共建，腾讯希望与行业伙伴携手，共同推进大模型在产业领域的创新和落地。</p><p>&nbsp;</p><p>以下为吴运声主题演讲全文：</p><p>&nbsp;</p><p>尊敬的各位嘉宾、媒体朋友们，大家下午好，欢迎来到腾讯论坛！我是腾讯的吴运声。</p><p>&nbsp;</p><p>作为国内规格最高、影响力最大的技术盛会，世界人工智能大会已经连续举办六届，很多新的技术趋势、创新方案、产业成果在这里发布，已经成为人工智能技术发展的风向标。回顾人工智能发展历史，这项技术自诞生之初，就始终伴随人们的期待与失望，呈螺旋式上升的发展趋势。今天，大语言模型驱动“智慧涌现”，又一次将人工智能技术推到了一个新的制高点。</p><p>&nbsp;</p><p>在过去这段时间里，我们发现大语言模型在智能问答、内容创作、智能决策、智能风控等很多业务场景，表现出令人惊艳的应用效果。尤其是广泛的产业场景，已经成为大模型的最佳练兵场。</p><p>&nbsp;</p><p>快速了解、应用大模型能力，是新一轮人工智能技术浪潮中，保持市场核心竞争力的关键。这也成为越来越多企业家共同思考的方向，同时也存在一些焦虑。</p><p>&nbsp;</p><p>实际上，大模型虽好，但用起来还是有很高的门槛。尤其对一些传统领域企业而言，通用大模型无法精准适配、达到降本增效的预期。企业需要的，是在实际场景中真正解决某个问题，而不是在100个场景中解决了70%-80%的问题。</p><p>&nbsp;</p><p>相比之下，训练和部署专属行业大模型，让“通才”变“专才”，对企业来说或许是一个可行路径，但又普遍存在计算资源少、数据质量差、投入成本高、专业经验少等现实挑战。此外，安全、合规，也是企业需要考虑的关键因素。</p><p>&nbsp;</p><p>针对这些问题，我们在6月19日推出了腾讯云MaaS一站式行业大模型解决方案：基于腾讯云TI平台，打造一站式行业大模型精选商店，TI平台内置多个行业高质量大模型，同时支持客户多模型训练任务，客户只需要加入自己独有的场景数据，就可以快速生成自己的“专属模型”，结合实际业务场景需求，开发低成本、高可用的智能应用和服务。</p><p>&nbsp;</p><p>目前，我们已经联合金融、文旅、政务、传媒、教育等十多个行业头部客户，共同打造了超过50个行业大模型解决方案。</p><p>&nbsp;</p><p>在金融领域，我们助力中金所技术公司，打造自有模型，降低技术开发与运营成本，支撑多部门业务。 针对资源、开发等需求，基于腾讯云TI平台能力，实现资源统一调度，按需扩缩容，降低开发门槛、成本，提高模型开发效率，支持投服部等部门的舆情数据分析场景，每日调用峰值高达1亿次，整体调用成功率超过99%。</p><p>&nbsp;</p><p>在文旅领域，我们联合一家线上旅游OTA公司，共同探索文旅行业大模型的应用。过去，传统智能客服需要人工进行对话配置：知识维护量大、耗时长，但运营人力有限、人力配置成本高，涉及订单等复杂业务场景，在无配置的情况下无法通过机器人闭环解决问题。精调后的客户专属模型，无需配置对话流程，即可实现端到端解决业务问题。提升任务完成率，降低对话构建成本。</p><p>&nbsp;</p><p>在传媒领域，我们联合央视总台，打造了传媒大模型方案。央视总台积累了海量的音视频、图片、文稿等信息资源，但人工编目低效且成本高，传统检索方式召回率低，无法满足时效性和高质量需求。通过腾讯云智能媒体AI中台的大模型能力，央视总台实现智能标签理解、泛化能力提升，以及人脸、图像、视频和自然语言描述的精准检索，提高了资源管理效率和准确性，推动了媒体内容生产与传播的智能化进程。</p><p>&nbsp;</p><p>腾讯云MaaS行业大模型精调解决方案，具备五大优势。基于腾讯深耕产业互联网的行业经验积累，我们在TI平台内置了高质量的行业大模型，同时我们提供完善的平台工具，成熟的流程方法，全面配套服务，领先的安全能力保障，可以为客户提供从模型选择、到落地部署的一站式服务。</p><p>&nbsp;</p><p>同时，在过去两周多时间里，我们也在不断优化和升级腾讯云MaaS平台能力，旨在为客户提供更普惠的行业大模型解决方案。在技术底座层面，我们前几天刚刚发布了自研星脉高性能计算网络、向量数据库，为大模型的行业应用提供更充沛的算力基础设施。在应用创新方面，我们也将行业大模型能力，应用到金融风控、交互翻译、数智人客服等场景中，极大提升了智能应用效率。在科学计算方向，AI for Science一直是我们关注的重要方向。今年，我们也将大模型技术，应用在天文探索、甲骨文考释等科研场景，取得了不错的成果。</p><p>&nbsp;</p><p>首先，行业大模型加持的金融风控解决方案，相比之前有了10倍效率提升。腾讯云风控大模型融合了腾讯过去20多年黑灰产对抗经验，和上千个真实业务场景，整体反欺诈效果比传统模式有20%左右的提升。企业可以基于prompt模式，迭代风控能力，从样本收集、模型训练到部署上线，实现全流程零人工参与，建模时间也从2周减少到仅需2天。</p><p>&nbsp;</p><p>同时，很多企业受限于积累的样本有限，或者是新品刚刚上线，面临“零样本”搭建风控体系，制定策略非常困难。我们也可以基于腾讯云风控大模型丰富的场景样本快速搭建能力，助力客户跳过“冷启动”过程。</p><p>&nbsp;</p><p>行业大模型在交互翻译场景，也有不错的提升效果。过去的同传技术，往往需要大量样本数据进行训练，尤其涉及一些特别专业领域的翻译，也需要人工加持进行调优，才能保障翻译效果。基于行业大模型技术，不再需要百万级的训练数据，使用小样本训练便也可以获得不错的翻译结果，让每一次交互翻译，都对下一句的翻译提升发挥实时作用，目前已经在多个垂直行业落地。大家看到今天会场，以及世界人工智能大会开幕式、主论坛的同传翻译，都是由腾讯同传进行的技术支持，我们已经连续6年为大会提供同传翻译服务。</p><p>&nbsp;</p><p>今年早些时候，我们推出了小样本数智人工厂，仅需少量数据、24小时即可复刻2D数字分身，大幅提升了生产效率。接下来，我们依托AI生成算法，提升3D形象复刻速度，推进生成式动作驱动，结合行业大模型能力，让企业获得更“个性化、专业、自然逼真”的数智员工，让“面对面”专业服务成为可能。这几天，我们在展台设置了小样本数智人体验，现场拍摄3分钟素材，24小时就可以领取专属数字分身。</p><p>&nbsp;</p><p>与此同时，我们也看到大模型在科学计算方面，也有非常惊艳的表现。在关注AI产业落地同时，AI for Science是我们一直关注的方向。早在2021年，我们将AI技术应用在天文领域，联合国家天文台、复旦大学计算机科学技术学院，发起“探星计划”，旨在通过人工智能技术，提升天文发现和科研的效率，已经帮助Fast发现30颗脉冲星。</p><p>&nbsp;</p><p>在今年，我们首次通过AI技术助力，从巡天观测数据中发现了2颗快速射电暴。相比脉冲星，快速射电暴因为发现时间晚、AI训练数据少、出现频率低，发现难度相比脉冲星要大很多。为此，团队通过全新设计的端到端AI算法，提升数据运算速度。引入多示例学习、注意力机制，同时结合腾讯云行业大模型辅助AI训练，通过自监督预训练&nbsp;+数据精调，提升模型精度，降低数据标注成本。</p><p>&nbsp;</p><p>在文化领域，我们与内外部多个部门携手合作，首次将文字检测、摹本生成、字形匹配等多种AI算法，综合应用于甲骨文研究。截至目前，已建立覆盖143万字的全球最大甲骨文单字数据库，提升甲骨内容提取、甲骨文识别与考释等效率，探索甲骨文研究的人机协同新模式。</p><p>&nbsp;</p><p>大模型训练，算力是基础。腾讯云在大模型算力方面拥有领先优势，今年早些时候，我们发布了面向大模型训练新一代HCC高性能计算集群，整体性能比过去提升了3倍。就在过去一周，腾讯云首次完整披露了自研星脉高性能计算网络，能提升40%的GPU利用率，节省30%~60%的模型训练成本，为AI大模型带来10倍通信性能提升。基于腾讯云新一代算力集群HCC，可支持10万卡的超大计算规模。</p><p>&nbsp;</p><p>最新发布的AI原生向量数据库最高支持10亿级向量检索规模，延迟控制在毫秒级，相比传统单机插件式数据库检索规模提升10倍，同时具备百万级每秒查询（QPS）的峰值能力。这些技术底座能力的持续提升，将为客户的模型训练提供强大的支持和动力。</p><p>&nbsp;</p><p>当然，AI大模型技术发展和产业探索，离不开产业链协同和生态共建，这也是腾讯在AI发展方面一直坚持的态度。为此，我们也在早些时候发布了腾讯云行业大模型共建计划。我们愿与行业伙伴携手，去探索无限的可能性。</p>",
    "publish_time": "2023-07-08 15:05:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "业内首份！中国信通院联合华院计算发布《认知智能技术和应用研究报告》",
    "url": "https://www.infoq.cn/article/qyFS4KHSRxpqwpgWiIKS",
    "summary": "<p>7月7日，由华院计算技术（上海）股份有限公司（以下简称“华院计算”）主办的2023世界人工智能大会“认知世界 智创未来”主题论坛在上海世博中心召开。</p><p><img src=\"https://static001.geekbang.org/infoq/ae/aebaf6bf643c28723a773e0a567616e4.png\" /></p><p>从2019年至今，今年是华院计算携手世界人工智能大会的第五年，多位海内外顶尖学者及产学研领域的专家代表相聚在此，从未来趋势到产业融合、共同探讨以认知智能为代表的AI前沿技术的最新研究成果、应用场景与商业化路线，从不同视角解读人工智能发展的技术路径与创新趋势。</p><p><img src=\"https://static001.geekbang.org/infoq/5e/5e4c300cf8f7c14f456ce9d260d95acc.png\" /></p><p>上海市经信委人工智能发展处 王志佳</p><p>上海市经信委人工智能发展处处长王志佳出席论坛并致辞：“面对生成式AI与大模型创新变革新浪潮，上海将充分利用超大城市的综合优势，以更开放的胸怀、更优质的服务、更宽大的力度汇聚全球创新资源，吸引国内外优质大模型团聚上海。由ChatGPT引发的大模型热潮,进一步推动了计算机的认知理解和推理能力，并拓展了认知智能的应用范畴。上海高度重视以生成式AI与大模型为代表的通用人工智能新风口，积极对标国内外最高水平，充分发挥城市数字化转型所带来的数据优势、场景优势，着力推动大模型创新发展。”</p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8e617ca325af0a794e52d4b79ee8ee0.png\" /></p><p>华院计算创始人、董事长宣晓华博士</p><p></p><p>华院计算创始人、董事长宣晓华在欢迎致辞中表示：“近几年华院一直大力持续投入认知智能研发，尤其在认知智能的自主算法创新、场景应用等方面，致力于将研究成果转化为现实生产力。而近期ChatGPT的热潮也推动了认知智能的研究，让大家对机器的理解和推理能力有了更高的期待。华院也在积极地尝试将大数据驱动的通用大模型和以知识驱动的面向垂直行业的小数据模型进行融合，以更好地解决实际应用问题。”</p><p></p><h2>先进算法和其高效实现带来巨大加速效果</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1c91f93376e4ff0b9cd9be5814a9552.png\" /></p><p>欧洲人文和自然科学院外籍院士、欧洲科学院院士，</p><p>上海交通大学自然科学研究院院长、上海交通大学重庆人工智能研究院院长金石教授</p><p></p><p>未来科学计算和人工智能发展到底是硬件还是软件的主导？这是一个公开问题，至今仍没有确切答案。上海交通大学的金石教授对此发表了名为《N-体问题：快速新算法，软件和应用》演讲。“在110个大规模问题中，43%的年均算法效率提升大于年均硬件能力提升；约13%的问题，算法提升大于硬件提升效率的十倍。“金石教授还表示，”基于牛顿第二定律 F=ma的微观模拟的N-体问题的难点在于O(N^2)的计算量。许多著名的算法，如上世纪十大算法中的快速傅里叶变换(FFT)、快速多极子(Fast Multipole Methods) 均是解决这一难题的算法。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4fe65e770cd764ce30e8ae8b05ff40b4.png\" /></p><p>澳大利亚国立大学计算机科学研究学院（RSCS）荣誉教授Marcus Hutter教授</p><p></p><p>现在有哪些人工智能技术和挑战实际上是我们正在面对的和未来要去面对的，包括未来人工智能的将会如何发展？通用人工智能（AGI）作为AI研究的终极目标。这个目标何时能实现？澳大利亚国立大学计算机科学研究学院（RSCS）荣誉教授Marcus Hutter教授分享了《The Near&amp;Far Future and Social Impact of AGI》的主题演讲，随着知识增强、跨模态、跨语言等技术路径激活了大模型的想象空间，目前人工智能发展已呈现出多技术融合、多学习方式共同演进的趋势，并逐渐实用化。尤其是生成式AI（AIGC）在数字内容与艺术创造领域的延申，不断孕育新的技术形态和价值模式，是AGI发展中的一个重要里程碑。但它并不代表AGI的完全实现，更不能代表AGI的理论体系和框架已经形成。AGI的理论体系和框架方面，目前仍处于不断发展和探索的阶段。AGI的理论涉及多个学科前沿，如计算机科学、语言学、神经科学、哲学等，需要跨学科整合和交流。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/080dcbacbeb7b4b5d04767c5810d9c55.png\" /></p><p></p><p>图灵奖获得者Manuel Blum教授及其夫人Lenore Blum教授</p><p></p><p>卡内基梅隆大学计算机科学学院名誉教授Manuel Blum教授及夫人前卡内基梅隆大学，计算机科学学院杰出教授Lenore Blum教授为大家做关于《The CTM approach to AGI》的演讲,这也是Blum教授夫妇连续第五年在华院计算的主题论坛上分享他们最新的研究成果。</p><p></p><p>今年，图灵奖获得者 Manuel Blum 教授及其夫人 Lenore Blum 教授发表了主题为《The CTM approach to AGI》的演讲，即如何通过意识图灵机实现通用人工智能，意识图灵机作为一种受神经科学启发的计算机架构，是在全局工作空间理论的基础上提出的一种研究意识的数学模型，能够帮助我们理解意识的产生机制。Blum教授夫妇表示ChatGPT其实使用的是GPT3的变体，GPT4是各种GPT3的智能组合，实际上他是由8个2200亿参数模型所组成。GPT4其实并不是一个巨大万亿参数模型，实际上它是由8个2200亿参数的GPT3模型巧妙组合在一起的组合体；而意识图灵机CTM则是1000万个GPT3的智能混合体。如何从8个GPT3的组合变成1000万个GPT组合，这听上去似乎有些天方夜谭的想法通过意识图灵机将有机会实现。</p><p></p><h2>认知智能引擎平台：增强大模型落地能力</h2><p></p><p></p><p>随着全球数字化进程的加快，人工智能技术与产业融合程度逐步加深，认知智能价值已得到产业初步认可，正逐渐步入发展快车道。结合华院计算多年来在认知智能方向探索与实践，联合中国信息通信研究院的的深入调研，双方共同发布了国内首份《认知智能技术和应用研究报告》，并围绕当前认知智能发展环境、技术链路、典型应用场景及未来发展方向等维度进行观点分享与技术产业分析。而大模型应用热潮又为认知智能发展注入一股新能量，进一步扩张认知智能应用潜力和价值边界，华院计算此次发布的认知智能引擎平台覆盖从底层算法平台到AI平台工具并赋能行业应用场景，这一“能思考、会判断”的认知智能技术正逐步应用于智能制造、数字政务、数字文旅和零售金融等诸多场景，增强大模型落地能力、不断释放产业应用价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/724ca716b8684c761739b57c456a9fe4.png\" /></p><p></p><p>复旦大学数学科学学院林伟教授</p><p></p><p>本次论坛由复旦大学数学科学学院教授林伟主持。华院计算致力于为人工智能通用算法前沿技术研究与技术突破贡献自己的力量，通过本次论坛举办，链接全球顶尖学者思想，汇聚认知智能前沿技术动态，进一步推动上海人工智能产业的加速发展，尤其在算法创新、场景赋能方面加快推进数字产业化，加强数字技术基础研究，为建设具有世界竞争力的人工智能产业集群注入新活力、增添新动能。</p>",
    "publish_time": "2023-07-08 15:27:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris 盲测性能十倍提升，更统一的多场景极速分析体验",
    "url": "https://www.infoq.cn/article/8jRoWP3WyLw6vOQigf2R",
    "summary": "<p>2023 年7月3日，Apache Doris 2.0-beta 版本正式发布。在 2.0-beta 版本中有超过 255 位贡献者为 Apache Doris 提交了超过 3500 个优化与修复，欢迎大家下载使用！</p><p></p><p>下载链接：https://doris.apache.org/download</p><p>GitHub 源码：https://github.com/apache/doris/tree/branch-2.0</p><p></p><p>在今年年初举办的 Doris Summit 年度峰会上，我们曾发布了 Apache Doris 的 2023 年 Roadmap 并提出了新的愿景：</p><p></p><p></p><blockquote>我们希望用户可以基于 Apache Doris 构建多种不同场景的数据分析服务、同时支撑在线与离线的业务负载、高吞吐的交互式分析与高并发的点查询；通过一套架构实现湖和仓的统一、在数据湖和多种异构存储之上提供无缝且极速的分析服务；也可通过对日志/文本等半结构化乃至非结构化的多模数据进行统一管理和分析、来满足更多样化数据分析的需求。这是我们希望 Apache Doris 能够带给用户的价值，不再让用户在多套系统之间权衡，仅通过一个系统解决绝大部分问题，降低复杂技术栈带来的开发、运维和使用成本，最大化提升生产力。</blockquote><p></p><p></p><p>面对海量数据的实时分析难题，这一愿景的实现无疑需要克服许多困难，尤其是在应对实际业务场景的真实诉求中更是遭遇了许多挑战：</p><p></p><p>如何保证上游数据实时高频写入的同时保证用户的查询稳定；如何在上游数据更新及表结构变更的同时保证在线服务的连续性；如何实现结构化与半结构化数据的统一存储与高效分析；如何同时应对点查询、报表分析、即席查询、ETL/ELT 等不同的查询负载且保证负载间相互隔离？如何保证复杂 SQL 语句执行的高效性、大查询的稳定性以及执行过程的可观测性？如何更便捷地集成与访问数据湖以及各种异构数据源？如何在大幅降低数据存储和计算资源成本的同时兼顾高性能查询？……</p><p></p><p>秉持着“将易用性留给用户、将复杂性留给自己”的原则，为了克服以上一系列挑战，从理论基础到工程实现、从理想业务场景到极端异常 Case、从内部测试通过到大规模生产可用，我们耗费了更多的时间与精力在功能的开发、验证、持续迭代与精益求精上。值得庆祝的是，在经过近半年的开发、测试与稳定性调优后，Apache Doris 终于迎来了 2.0-beta 版本的正式发布！而这一版本的成功发布也使得我们的愿景离现实更进一步！</p><p></p><h2>盲测性能 10 倍以上提升</h2><p></p><p></p><h3>全新的查询优化器</h3><p></p><p>高性能是 Apache Doris 不断追求的目标。过去一年在 Clickbench、TPC-H 等公开测试数据集上的优异表现，已经证明了其在执行层以及算子优化方面做到了业界领先，但从 Benchmark 到真实业务场景之间还存在一定距离：</p><p></p><p>Benchmark 更多是真实业务场景的抽象、提炼与简化，而现实场景往往可能面临更复杂的查询语句，这是测试所无法覆盖的；Benchmark 查询语句可枚举、可针对性进行调优，而真实业务场景的调优极度依赖工程师的心智成本、调优效率往往较为低下且过于消耗工程师人力；</p><p></p><p>基于此，我们着手研发了现代架构的全新查询优化器，并在 Apache Doris 2.0-beta 版本全面启用。全新查询优化器采取了更先进的 Cascades 框架、使用了更丰富的统计信息、实现了更智能化的自适应调优，在绝大多数场景无需任何调优和 SQL 改写即可实现极致的查询性能，同时对复杂 SQL 支持得更加完备、可完整支持 TPC-DS 全部 99 个 SQL。</p><p></p><p>我们对全新查询优化器的执行性能进行了盲测，仅以 TPC-H 22 个 SQL 为例 ，全新优化器在未进行任何手工调优和 SQL 改写的情况下 查询耗时，盲测性能提升了超过 10 倍！而在数十个 2.0 版本用户的真实业务场景中，绝大多数原始 SQL 执行效率得以极大提升，真正解决了人工调优的痛点！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e8/e8db39594f405f1b1266e62f360a2ede.png\" /></p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/query-acceleration/nereids</p><p></p><p>如何开启：SET enable_nereids_planner=true 在 Apache Doris 2.0-beta 版本中全新查询优化器已经默认开启，通过调用 Analyze 命令收集统计信息。</p><p></p><h3>自适应的 Pipeline 执行引擎</h3><p></p><p>过去 Apache Doris 的执行引擎是基于传统的火山模型构建，为了更好利用多机多核的并发能力，过去我们需要手动设置执行并发度（例如将 parallel_fragment_exec_instance_num 这一参数从默认值 1 手工设置为 8 或者 16），在存在大量查询任务时存在一系列问题：</p><p></p><p>大查询、小查询需要设置不同的instance 并发度，系统不能做到自适应调整；Instance 算子占用线程阻塞执行，大量查询任务将导致执行线程池被占满、无法响应后续请求，甚至出现逻辑死锁；Instance 线程间的调度依赖于系统调度机制，线程进行反复切换将产生额外的性能开销；在不同分析负载并存时，Instance 线程间可能出现 CPU 资源争抢的情况，可能导致大小查询、不同租户之间相互受影响；</p><p></p><p>针对以上问题，Apache Doris 2.0 引入了 Pipeline 执行模型作为查询执行引擎。在 Pipeline 执行引擎中，查询的执行是由数据来驱动控制流变化的， 各个查询执行过程之中的阻塞算子被拆分成不同 Pipeline，各个 Pipeline 能否获取执行线程调度执行取决于前置数据是否就绪，因此实现了以下效果：</p><p></p><p>Pipeline 执行模型通过阻塞逻辑将执行计划拆解成 Pipeline Task，将 Pipeline Task 分时调度到线程池中，实现了阻塞操作的异步化，解决了 Instance 长期占用单一线程的问题。可以采用不同的调度策略，实现 CPU 资源在大小查询间、不同租户间的分配，从而更加灵活地管理系统资源。Pipeline 执行模型还采用了数据池化技术，将单个数据分桶中的数据进行池化，从而解除分桶数对 Instance 数量的限制，提高 Apache Doris 对多核系统的利用能力，同时避免了线程频繁创建和销毁的问题。</p><p></p><p>通过 Pipeline 执行引擎，Apache Doris 在混合负载场景中的查询性能和稳定性都得以进一步提升。</p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/query-acceleration/pipeline-execution-engine</p><p></p><p>如何开启：Set enable_pipeline_engine = true 该功能在 Apache Doris 2.0 版本中将默认开启，BE 在进行查询执行时默认将 SQL 的执行模型转变 Pipeline 的执行方式。parallel_pipeline_task_num代表了 SQL 查询进行查询并发的 Pipeline Task 数目。Apache Doris 默认配置为0，此时 Apache Doris 会自动感知每个 BE 的 CPU 核数并把并发度设置为 CPU 核数的一半，用户也可以实际根据自己的实际情况进行调整。对于从老版本升级的用户，建议用户将该参数设置成老版本中parallel_fragment_exec_instance_num的值。</p><p></p><h2>查询稳定性进一步提升</h2><p></p><p></p><h3>多业务资源隔离</h3><p></p><p>随着用户规模的极速扩张，越来越多的用户将 Apache Doris 用于构建企业内部的统一分析平台。这一方面需要 Apache Doris 去承担更大规模的数据处理和分析，另一方面也需要 Apache Doris 同时去应对更多分析负载的挑战，而其中的关键在于如何保证不同负载能够在一个系统中稳定运行。</p><p></p><p>Apache Doris 2.0 版本中基于 Pipeline 执行引擎增加了 Workload 管理器 ，通过对 Workload 进行分组管理，以保证内存和 CPU 资源的精细化管控。</p><p></p><p>在过去版本中 Apache Doris 通过资源标签的方式进行了多租户资源隔离，可以通过节点资源划分来避免不同业务间的相互干扰，而 Workload Group 实现了更精细化的资源管控方式，通过将 Query 与 Workload Group 相关联，可以限制单个 Query 在 BE 节点上的 CPU 和内存资源的百分比，并可以配置开启资源组的内存软限制。当集群资源紧张时，将自动 Kill 组内占用内存最大的若干个查询任务以减缓集群压力。当集群资源空闲时，一旦 Workload Group 使用资源超过预设值时，多个 Workload 将共享集群可用空闲资源并自动突破阙值，继续使用系统内存以保证查询任务的稳定执行。</p><p></p><p><code lang=\"text\">create workload group if not exists etl_group\nproperties (\n    \"cpu_share\"=\"10\",\n    \"memory_limit\"=\"30%\",\n    \"max_concurrency\" = \"10\",\n    \"max_queue_size\" = \"20\",\n    \"queue_timeout\" = \"3000\"\n);\n</code></p><p></p><p>可以通过 Show 命令来查看创建的 Workload Group，例如：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df7e4616ca999fb9d07de1acce451657.png\" /></p><p></p><h3>作业排队</h3><p></p><p>同时在 Workload Group 中我们还引入了查询排队的功能，在创建 Workload Group 时可以设置最大查询数，超出最大并发的查询将会进行队列中等待执行。</p><p></p><p>max_concurrency 当前 Group允许的最大查询数，超过最大并发的查询到来时会进入排队逻辑；max_queue_size查询排队的长度，当队列满了之后，新来的查询会被拒绝；queue_timeout查询在队列中等待的时间，如果查询等待时间超过等待时间查询将会被拒绝，时间单位为毫秒；</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4cd3fe0a00ea80bc3067a6d1c079faad.png\" /></p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/admin-manual/workload-group/</p><p></p><h3>彻底告别 OOM</h3><p></p><p>在内存充足时内存管理通常对用户是无感的，但真实场景中往往面临着各式各样的极端 Case，这些都将为内存性能和稳定性带来挑战，尤其是在面临内存资源消耗巨大的复杂计算和大规模作业时，由于内存 OOM 导致查询失败甚至可能造成 BE 进程宕机。</p><p></p><p>因此我们逐渐统一内存数据结构、重构 MemTracker、开始支持查询内存软限，并引入进程内存超限后的 GC 机制，同时优化了高并发的查询性能等。在 2.0 版本中我们引入了全新的内存管理框架，通过有效的内存分配、统计、管控，在 Benchmark、压力测试和真实用户业务场景的反馈中，基本消除了内存热点以及 OOM 导致 BE 宕机的问题，即使发生 OOM 通常也可依据日志定位内存位置并针对性调优，从而让集群恢复稳定，对查询和导入的内存限制也更加灵活，在内存充足时让用户无需感知内存使用。</p><p></p><p>通过以上一系列优化，Apache Doris 2.0 版本在应对复杂计算以及大规模 ETL/ELT 操作时，内存资源得以有效控制，系统稳定性表现更上一个台阶。</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/Z5N-uZrFE3Qhn5zTyEDomQ</p><p></p><h2>高效稳定的数据写入</h2><p></p><p></p><h3>更高的实时数据写入效率</h3><p></p><p></p><h4>导入性能进一步提升</h4><p></p><p>聚焦于实时分析，我们在过去的几个版本中在不断增强实时分析能力，其中端到端的数据实时写入能力是优化的重要方向，在 Apache Doris 2.0 版本中，我们进一步强化了这一能力。通过 Memtable 不使用 Skiplist、并行下刷、单副本导入等优化，使得导入性能有了大幅提升：</p><p></p><p>使用 Stream Load 对 TPC-H 144G lineitem 表原始数据进行三副本导入 48 buckets 的 Duplicate 表，吞吐量提升 100%。使用 Stream Load 对 TPC-H 144G lineitem 表原始数据进行三副本导入 48 buckets 的 Unique Key 表，吞吐量提升 200%。使用 insert into select 对 TPC-H 144G lineitem 表进行导入 48 buckets 的 Duplicate 表，吞吐量提升 50%。使用 insert into select 对 TPC-H 144G lineitem 表进行导入 48 buckets 的 Unique Key 表，吞吐提升 150%。</p><p></p><h4>数据高频写入更稳定</h4><p></p><p>在高频数据写入过程中，小文件合并和写放大问题以及随之而来的磁盘 I/O和 CPU 资源开销是制约系统稳定性的关键，因此在 2.0 版本中我们引入了 Vertical Compaction 以及 Segment Compaction，用以彻底解决 Compaction 内存问题以及写入过程中的 Segment 文件过多问题，资源消耗降低 90%，速度提升 50%，内存占用仅为原先的 10%。</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/BqiMXRJ2sh4jxKdJyEgM4A</p><p></p><h4>数据表结构自动同步</h4><p></p><p>在过去版本中我们引入了毫秒级别的 Schema Change，而在最新版本 Flink-Doris-Connector 中，我们实现了从 MySQL 等关系型数据库到 Apache Doris 的一键整库同步。在实际测试中单个同步任务可以承载数千张表的实时并行写入，从此彻底告别过去繁琐复杂的同步流程，通过简单命令即可实现上游业务数据库的表结构及数据同步。同时当上游数据结构发生变更时，也可以自动捕获 Schema 变更并将 DDL 动态同步到 Doris 中，保证业务的无缝运行。</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/Ur4VpJtjByVL0qQNy_iQBw</p><p></p><h3>主键模型支持部分列更新</h3><p></p><p>在 Apache Doris 1.2 版本中我们引入了 Unique Key 模型的 Merg-on-Write 写时合并模式，在上游数据高频写入和更新的同时可以保证下游业务的高效稳定查询，实现了实时写入和极速查询的统一。 而 2.0 版本我们对 Unique Key 模型进行了全面增强。在功能上，支持了新的部分列更新能力，在上游多个源表同时写入时无需提前处理成宽表，直接通过部分列更新在写时完成 Join，大幅简化了宽表的写入流程。</p><p></p><p>在性能上，2.0 版本大幅增强了 Unique Key 模型 Merge-on-Write 的大数据量写入性能和并发写入能力，大数据量导入较 1.2 版本有超过 50% 的性能提升，高并发导入有超过 10 倍的性能提升，并通过高效的并发处理机制来彻底解决了 publish timeout(Error -3115) 问题，同时由于 Doris 2.0 高效的 Compaction 机制，也不会出现 too many versions (Error-235) 问题。这使得 Merge-on-Write 能够在更广泛的场景下替代 Merge-on-Read 实现，同时我们还利用部分列更新能力来降低 UPDATE 语句和 DELETE 语句的计算成本，整体性能提升约 50%。</p><p></p><p>部分列更新的使用示例（Stream Load）：</p><p></p><p>例如有表结构如下</p><p></p><p><code lang=\"text\">mysql&gt; desc user_profile;\n+------------------+-----------------+------+-------+---------+-------+\n| Field            | Type            | Null | Key   | Default | Extra |\n+------------------+-----------------+------+-------+---------+-------+\n| id               | INT             | Yes  | true  | NULL    |       |\n| name             | VARCHAR(10)     | Yes  | false | NULL    | NONE  |\n| age              | INT             | Yes  | false | NULL    | NONE  |\n| city             | VARCHAR(10)     | Yes  | false | NULL    | NONE  |\n| balance          | DECIMALV3(9, 0) | Yes  | false | NULL    | NONE  |\n| last_access_time | DATETIME        | Yes  | false | NULL    | NONE  |\n+------------------+-----------------+------+-------+---------+-------+\n</code></p><p></p><p>用户希望批量更新最近 10s 发生变化的用户的余额和访问时间，可以把数据组织在如下 csv 文件中</p><p></p><p><code lang=\"text\">1,500,2023-07-03 12:00:01\n3,23,2023-07-03 12:00:02\n18,9999999,2023-07-03 12:00:03\n</code></p><p></p><p>然后通过 Stream Load，增加 Header partial_columns:true，并指定要导入的列名即可完成更新</p><p></p><p><code lang=\"text\">curl  --location-trusted -u root: -H \"partial_columns:true\" -H \"column_separator:,\" -H \"columns:id,balance,last_access_time\" -T /tmp/test.csv http://127.0.0.1:48037/api/db1/user_profile/_stream_load\n</code></p><p></p><h2>极致弹性与存算分离</h2><p></p><p>过去 Apache Doris 凭借在易用性方面的诸多设计帮助用户大幅节约了计算与存储资源成本，而面向未来的云原生架构，我们已经走出了坚实的一步。</p><p></p><p>从降本增效的趋势出发，用户对于计算和存储资源的需求可以概括为以下几方面：</p><p></p><p>计算资源弹性：面对业务计算高峰时可以快速进行资源扩展提升效率，在计算低谷时可以快速缩容以降低成本；存储成本更低：面对海量数据可以引入更为廉价的存储介质以降低成本，同时存储与计算单独设置、相互不干预；业务负载隔离：不同的业务负载可以使用独立的计算资源，避免相互资源抢占；数据管控统一：统一 Catalog、统一管理数据，可以更加便捷地分析数据。</p><p></p><p>存算一体的架构在弹性需求不强的场景具有简单和易于维护的优势，但是在弹性需求较强的场景有一定的局限。而存算分离的架构本质是解决资源弹性的技术手段，在资源弹性方面有着更为明显的优势，但对于存储具有更高的稳定性要求，而存储的稳定性又会进一步影响到 OLAP 的稳定性以及业务的存续性，因此也引入了 Cache 管理、计算资源管理、垃圾数据回收等一系列机制。</p><p></p><p>而在与 Apache Doris 社区广大用户的交流中，我们发现用户对于存算分离的需求可以分为以下三类：</p><p></p><p>目前选择简单易用的存算一体架构，暂时没有资源弹性的需求；欠缺稳定的大规模存储，要求在 Apache Doris 原有基础上提供弹性、负载隔离以及低成本；有稳定的大规模存储，要求极致弹性架构、解决资源快速伸缩的问题，因此也需要更为彻底的存算分离架构；</p><p></p><p>为了满足前两类用户的需求，Apache Doris 2.0 版本中提供了可以兼容升级的存算分离方案：</p><p></p><p>第一种，计算节点。2.0 版本中我们引入了无状态的计算节点 Compute Node，专门用于数据湖分析。相对于原本存储计算一体的混合节点，Compute Node 不保存任何数据，在集群扩缩容时无需进行数据分片的负载均衡，因此在数据湖分析这种具有明显高峰的场景中可以灵活扩容、快速加入集群分摊计算压力。同时由于用户数据往往存储在 HDFS/S3 等远端存储中，执行查询时查询任务会优先调度到 Compute Node 执行，以避免内表与外表查询之间的计算资源抢占。</p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/advanced/compute_node</p><p></p><p>第二种，冷热分层。在存储方面，冷热数据数据往往面临不同频次的查询和响应速度要求，因此通常可以将冷数据存储在成本更低的存储介质中。在过去版本中 Apache Doris 支持对表分区进行生命周期管理，通过后台任务将热数据从 SSD 自动冷却到 HDD，但 HDD 上的数据是以多副本的方式存储的，并没有做到最大程度的成本节约，因此对于冷数据存储成本仍然有较大的优化空间。在 Apache Doris 2.0 版本中推出了<a href=\"https://mp.weixin.qq.com/s/mP3GfVxx8nvh0HseFZSV_A\">冷热数据分层功能</a>\" ，冷热数据分层功能使 Apache Doris 可以将冷数据下沉到存储成本更加低廉的对象存储中，同时冷数据在对象存储上的保存方式也从多副本变为单副本，存储成本进一步降至原先的三分之一，同时也减少了因存储附加的计算资源成本和网络开销成本。通过实际测算，存储成本最高可以降低超过 70%！</p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/advanced/cold_hot_separation</p><p></p><p>后续计算节点会支持查询冷数据和存储节点的数据，从而实现能兼容升级的存算分离方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2c/2c25fec393cfec06c330d4162a1ade73.png\" /></p><p></p><p>而为了满足第三类用户的需求，我们还将把 SelectDB Cloud 存算分离方案贡献回社区。这一方案在性能、功能成熟度、系统稳定性等方面经历了上百家企业生产环境的考验，后续功能合入的实际进展我们也将及时同步。</p><p></p><h2>10 倍以上性价比的日志分析方案</h2><p></p><p>从过去的实时报表和 Ad-hoc 等典型 OLAP 场景到 ELT/ETL、日志检索与分析等更多业务场景，Apache Doris 正在不断拓展应用场景的边界，而日志数据的统一存储与分析正是我们在 2.0 版本的重要突破。</p><p></p><p>过去业界典型的日志存储分析架构难以同时兼顾 高吞吐实时写入、低成本大规模存储与高性能文本检索分析，只能在某一方面或某几方面做权衡取舍。而在 Apache Doris 2.0 版本中，我们引入了全新倒排索引、以满足字符串类型的全文检索和普通数值/日期等类型的等值、范围检索，同时进一步优化倒排索引的查询性能、使其更加契合日志数据分析的场景需求，同时结合过去在大规模数据写入和低成本存储等方面的优势，实现了更高性价比的日志分析方案。</p><p></p><p>在相同硬件配置和数据集的测试表现上，Apache Doris 相对于 ElasticSearch 实现了日志数据写入速度提升 4 倍、存储空间降低 80%、查询性能提升 2 倍，再结合 Apache Doris 2.0 版本引入的冷热数据分层特性，整体性价比提升 10 倍以上。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5c/5c3534d907d8bbcf3ae5bc2a3a1bc512.png\" /></p><p></p><p>除了日志分析场景的优化以外，在复杂数据类型方面，我们增加了全新的数据类型 Map/Struct，包括支持以上类型的高效写入、存储、分析函数以及类型之间的相互嵌套，以更好满足多模态数据分析的支持。</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/WJXKyudW8CJPqlUiAro_KQ</p><p></p><h3>更全面、更高性能的数据湖分析能力</h3><p></p><p>在 Apache Doris 1.2 版本中，我们发布了 Multi-Catalog 功能，支持了多种异构数据源的元数据自动映射与同步，实现了数据湖的无缝对接。依赖 数据读取、执行引擎、查询优化器方面的诸多优化，在标准测试集场景下，Apache Doris 在湖上数据的查询性能，较 Presto/Trino 有 3-5 倍的提升。</p><p></p><p>在 2.0 版本中，我们进一步对数据湖分析能力进行了加强，不但支持了更多的数据源，同时针对用户的实际生产环境做了诸多优化，相较于 1.2 版本，能够在真实工作负载情况下显著提升性能。</p><p></p><p>更多数据源支持</p><p></p><p>支持 Hudi Copy-on-Write 表的 Snapshot Query 以及 Merge-on-Read 表的 Read Optimized Query，后续将支持 Incremental Query 和 Time Trival。参考文档：https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hudiJDBC Catalog 新增支持 Oceanbase，目前支持包括 MySQL、PostgreSQL、Oracle、SQLServer、Doris、Clickhouse、SAP HANA、Trino/Presto、Oceanbase 等近十种关系型数据库。参考文档：https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/jdbc</p><p></p><p>数据权限管控</p><p></p><p>支持通过 Apache Range 对 Hive Catalog 进行鉴权，可以无缝对接用户现有的权限系统。同时还支持可扩展的鉴权插件，为任意 Catalog 实现自定义的鉴权方式。 参考文档：https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/hive</p><p></p><p>性能进一步优化，最高提升数十倍</p><p></p><p>优化了大量小文件场景以及宽表场景的读取性能。通过小文件全量加载、小 IO 合并、数据预读等技术，显著降低远端存储的读取开销，在此类场景下，查询性能最高提升数十倍。优化了 ORC/Parquet 文件的读取性能，相较于 1.2 版本查询性能提升一倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9ef8312c5768d07de5bcfa110e026ab8.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/12d239529f003a5dfe3c8375d5795398.png\" /></p><p></p><p>支持湖上数据的本地文件缓存。可以利用本地磁盘缓存 HDFS 或对象存储等远端存储系统上的数据，通过缓存加速访问相同数据的查询。在命中本地文件缓存的情况下，通过 Apache Doris 查询湖上数据的性能可与 Apache Doris 内部表持平，该功能可以极大提升湖上热数据的查询性能。参考文档：https://doris.apache.org/zh-CN/docs/dev/lakehouse/filecache支持外表的统计信息收集。和 Apache Doris 内表一样，用户可以通过 Analyze 语句分析并收集指定外表的统计信息，结合 Nereids 全新查询优化器，能够更准确更智能地对复杂 SQL 进行查询计划的调优。以 TPC-H 标准测试数据集为例，无需手动改写 SQL 即可获得最优的查询计划并获得更好的性能表现。 参考文档：https://doris.apache.org/zh-CN/docs/dev/lakehouse/multi-catalog/优化了 JDBC Catalog 的数据写回性能。通过 PrepareStmt 和批量方式，用户通过 INSERT INTO 命令、通过 JDBC Catalog 将数据写回到 MySQL、Oracle 等关系型数据库的性能提升数十倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/493645b96c48bb315c7cf0eb94a8557d.png\" /></p><p></p><h3>高并发数据服务支持</h3><p></p><p>与复杂 SQL 和大规模 ETL 作业不同，在诸如银行交易流水单号查询、保险代理人保单查询、电商历史订单查询、快递运单号查询等 Data Serving 场景，会面临大量一线业务人员及 C 端用户基于主键 ID 检索整行数据的需求，在过去此类需求往往需要引入 Apache HBase 等 KV 系统来应对点查询、Redis 作为缓存层来分担高并发带来的系统压力。</p><p></p><p>对于基于列式存储引擎构建的 Apache Doris 而言，此类的点查询在数百列宽表上将会放大随机读取 IO，并且执行引擎对于此类简单 SQL 的解析、分发也将带来不必要的额外开销，往往需要更高效简洁的执行方式。因此在新版本中我们引入了全新的行列混合存储以及行级 Cache，使得单次读取整行数据时效率更高、大大减少磁盘访问次数，同时引入了点查询短路径优化、跳过执行引擎并直接使用快速高效的读路径来检索所需的数据，并引入了预处理语句复用执行 SQL 解析来减少 FE 开销。</p><p></p><p>通过以上一系列优化，Apache Doris 2.0 版本在并发能力上实现了数量级的提升！在标准 YCSB 基准测试中，单台 16 Core 64G 内存 4*1T 硬盘规格的云服务器上实现了单节点 30000 QPS 的并发表现，较过去版本点查询并发能力提升超 20 倍！基于以上能力，Apache Doris 可以更好应对高并发数据服务场景的需求，替代 HBase 在此类场景中的能力，减少复杂技术栈带来的维护成本以及数据的冗余存储。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/62cf67ecead1378223baf90c79d48bc0.png\" /></p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/query-acceleration/hight-concurrent-point-query</p><p></p><p>详细介绍：https://mp.weixin.qq.com/s/Ow77-kFMWXFxugFXjOPHhg</p><p></p><h2>CCR 跨集群数据同步</h2><p></p><p>为了满足用户多集群之间数据同步的需求，在过去需要定期通过 Backup/Restore 命令进行数据备份和恢复，操作较为复杂、数据同步时延高并且还需要中间存储。为了满足用户多集群的数据库表自动同步需求，在 2.0-beta 版本中我们增加了 CCR 跨集群数据同步的功能，能够在库/表级别将源集群的数据变更同步到目标集群、以提升在线服务的数据可用性并更好地实现了读写负载分离以及多机房备份。</p><p></p><h2>支持 Kubernetes 容器化部署</h2><p></p><p>在过去 Apache Doris 是基于 IP 通信的，在 K8s 环境部署时由于宿主机故障发生 Pod IP 漂移将导致集群不可用，在 2.0 版本中我们支持了 FQDN，使得 Apache Doris 可以在无需人工干预的情况下实现节点自愈，因此可以更好应对 K8s 环境部署以及灵活扩缩容。</p><p></p><p>参考文档：https://doris.apache.org/zh-CN/docs/dev/install/k8s-deploy/</p><p></p><h2>其他升级注意事项</h2><p></p><p>1.2-lts 可以滚动升级到 2.0-beta，2.0-alpha 可以停机升级到 2.0-beta；查询优化器开关默认开启 enable_nereids_planner=true；系统中移除了非向量化代码，所以参数将不再生效enable_vectorized_engine；新增参数 enable_single_replica_compaction；默认使用 datev2, datetimev2, decimalv3 来创建表，不支持 datev1，datetimev1， decimalv2 创建表；在 JDBC 和 Iceberg Catalog 中默认使用decimalv3；date type 新增 AGG_STATE；backend 表去掉 cluster 列；为了与 BI 工具更好兼容，在 show create table 时，将 datev2 和 datetimev2 显示为 date 和 datetime。在 BE 启动脚本中增加了 max_openfiles 和 swap 的检查，所以如果系统配置不合理，be 有可能会启动失败；禁止在 localhost 访问 FE 时无密码登录；当系统中存在 Multi-Catalog 时，查询 information schema 的数据默认只显示 internal catalog 的数据；限制了表达式树的深度，默认为 200；array string 返回值 单引号变双引号；对 Doris的进程名重命名为 DorisFE 和 DorisBE；</p><p></p><h2>踏上 2.0 之旅</h2><p></p><p>距离 Apache Doris 2.0-alpha 版本发布已经有一个半月之久，这一段时间内我们在加速核心功能特性开发的同时、还收获到了数百家企业对于新版本的切身体验与真实反馈，这些来自真实业务场景的反馈对于功能的打磨与进一步完善有着极大的帮助。因此 2.0-beta 版本无论在功能的完整度还是系统稳定性上，都已经具备了更佳的使用体验，欢迎所有对于 2.0 版本新特性有需求的用户部署升级。</p><p></p><p>如果您在调研、测试以及部署升级 2.0 版本的过程中有任何问题，<a href=\"https://wenjuan.feishu.cn/m/cfm?t=sF2FZOL1KXKi-m73g\">欢迎提交问卷信息</a>\"，届时将由社区核心贡献者提供 1-1 专项支持。我们也期待 2.0 版本为更多社区用户提供实时统一的分析体验，相信 Apache Doris 2.0 版本会成为您在实时分析场景中的最理想选择。</p>",
    "publish_time": "2023-07-08 15:51:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里云AI绘画创作大模型通义万相亮相，已开启定向邀测",
    "url": "https://www.infoq.cn/article/FRZJW3zPd1HSmw2MB2dm",
    "summary": "<p>7月7日，在2023世界人工智能大会上，阿里云宣布AI绘画创作大模型通义万相开启定向邀测，该模型可辅助人类进行图片创作，未来可应用于艺术设计、电商、游戏和文创等应用场景。阿里云智能集团CTO周靖人表示，这是阿里云大模型全面掌握多模态能力的关键一步，该能力将逐步向行业客户开放。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a09fb9c82ba946ed2a9ae8142b79e9f.png\" /></p><p></p><p>阿里云宣布AI绘画创作大模型通义万相开启定向邀测</p><p></p><p>据介绍，通义万相在语义理解和图片生成上均表现优秀，首批上线3大能力：在基础文生图功能中，可根据文字内容生成水彩、扁平插画、二次元、油画、中国画、3D卡通和素描等风格图像；在相似图片生成功能中，用户上传任意图片后，即可进行创意发散，生成内容、风格相似的AI画作；该模型还在业内率先支持图像风格迁移，用户上传原图和风格图，可自动把原图处理为指定的风格图。</p><p>&nbsp;</p><p>通义万相基于阿里研发的组合式生成模型Composer，其可通过对配色、布局、风格等图像设计元素进行拆解和组合，提供高度可控性和极大自由度的图像生成效果，仅需一个模型即可支持多类图像生成类任务能力，该研究成果已在国际AI顶会ICML&nbsp;2023上发表。</p><p>&nbsp;</p><p>阿里云在现场展示了该模型生成图片的效果，当输入“沉浸在全页灰度涂色的迷人世界中，有一只老虎在曼陀罗宁静的森林中，画面采用线条、笔画”后，系统能迅速生成线条细节丰富的老虎图片，如果继续使用风格迁移功能，还可随意切换图片风格。</p><p>&nbsp;</p><p>在去年的世界人工智能大会上，阿里云“通义”大模型系列首次亮相，在此之后，通义千问、通义听悟等模型和产品相继推出，通义万相的问世意味着阿里云在大模型领域已经具备处理或生成文本、语音和图片等模态的能力。目前，阿里云大模型正逐步走向千行百业，已有超过30万企业申请测试通义千问，通义听悟累计用户数达到36万。</p><p>&nbsp;</p><p>“多模态是大模型演进的必然路径，我们希望用不同模态的能力服务千行百业，帮助企业全面拥抱智能化时代。”周靖人表示。</p>",
    "publish_time": "2023-07-08 16:01:56",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从“少障碍”到“无障碍” ，蚂蚁 AI 专家畅谈人工智能助力科学探索的新范式",
    "url": "https://www.infoq.cn/article/t9685IWv74GeT7WIFIRe",
    "summary": "<p>7 月 8 日，2023 世界人工智能大会（WAIC）——科技无障碍论坛上，<a href=\"https://www.infoq.cn/news/FPHLp87Iko2aswqbDWe2\">蚂蚁集团</a>\"大安全事业群机器智能部副总经理李哲介绍蚂蚁集团在 AI 助力科技无障碍方面所做的努力，从创建“蓝马甲数字助老实验室”，到推出“划一划”和“挥一挥”服务视障群体。李哲倡议，希望更多科技公司加大 AI for Science 上的突破，真正做到从“少障碍”到“无障碍”。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/63/46/63eaf4ccebe3f7ee10700434ebe05246.jpg\" /></p><p>图：蚂蚁集团大安全事业群机器智能部副总经理李哲在WAIC现场分享</p><p>&nbsp;</p><p>助力弥合数字鸿沟、对产品不断进行无障碍改造，让弱势群体享受数字时代的发展红利是蚂蚁作为互联网公司的平台责任。2020 年起，在国家反诈中心、国家卫生健康委老龄健康司、工信部反诈中心指导下，由<a href=\"https://www.infoq.cn/news/LHxY4zv2dPgJfWaJYxtC\">蚂蚁集团</a>\"、浙江蚂蚁公益基金会发起，社会各界共同参与的蓝马甲公益行动正式启动，主要通过社区讲座、公益展览、大篷车下乡等方式，帮助老人解决智能设备“不敢用、不会用”的难题。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/6b/8b/6b9b7a876c070bed8c5875893d5c208b.jpg\" /></p><p>图：蓝马甲行动在WAIC上发布Hello老友亭功能介绍</p><p></p><p>在本届 WAIC 上，蓝马甲行动联合上海电信发布第一期“Hello 老友亭N大功能介绍”，用简洁明了和口语化的图文表达，拆解老友亭中五项实用功能的操作步骤及注意事项。蓝马甲计划将这份指南陆续投放在上海街头的 200 多座老友亭中，让叔叔阿姨在实际使用时，手边就能有一份“看得懂、用得上”的说明书。</p><p>&nbsp;</p><p>“数字鸿沟”不应成为“服务鸿沟”，蓝马甲行动着力在线下服务上做“加法”、在技术上做“减法”。2022 年 9 月，蓝马甲数字助老实验室应运而生，并不断进行升级，通过技术升级降低老年人使用智能手机等设备的门槛。</p><p>&nbsp;</p><p>“我们注意到针对一部分人群，比如视障人群、老年人群，他们使用像人脸识别、拼图验证码这类常见的身份识别产品，存在很大的困难”，于是，在李哲的带领下，蚂蚁 AI 团队研发了基于行为识别 AI 技术的创新产品——“划一划”和“挥一挥”，可以让这些群体在支付宝 APP 中像其他群体一样方便快捷地完成校验，“基于这项 AI 技术，我们也开发了视障群体独有的互动方式，用于支付宝的‘集五福’等活动中。目前，这些技术通过蓝马甲数字助老实验室不断进行扩散，通过 IIFAA 联盟生态和蚂蚁安全科技 ZOLOZ 品牌，去服务全球更多的人群。”</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/06/eb/066c0336fb3e737yy330139273c2f2eb.jpg\" /></p><p>图：蓝马甲数字助老实验室不断升级，推出更多AI无障碍产品</p><p>&nbsp;</p><p>蓝马甲数字助老实验室陆续又推出升级版 AI 欺诈叫醒产品、防骗码长辈模式等功能；通过不断优化支付宝长辈模式、暖洋洋热线、沉浸式体验课程，提升使用体验，助力老人共享数字新生活。未来，更多AI产品将通过这个实验室对外推出，服务好老年群体。</p><p>&nbsp;</p><p>“未来AI的革新，为无障碍带来 2 个阶段的改变。第一个阶段，大模型等AI新技术在图像处理、语音识别、自然语言理解等能力上带来的涌现能力，会让现有的很多信息无障碍产品的性能得到显著提升；第二个阶段，AI 和一些基础学科的交叉研究上带来的突破，才会真正给无障碍带来一些蜕变”， 李哲说，随着行业不断发展，人工智能将助力无障碍带来新一轮的生产力革新，“如果说现在AI的涌现能力更多的是让‘有障碍’变成‘少障碍’，我相信不远的将来，在 AI for Science 上的突破，才会真正做到&nbsp;‘少障碍’到‘无障碍’”。</p><p>&nbsp;</p><p>本场论坛汇聚了国内外研究机构和顶尖学者以及产业人士等，从不同视角分享了 AI 如何驱动科技无障碍创新、畅想数字时代的 <a href=\"https://www.infoq.cn/news/KvxWGt1PKhSaE1WSq5bf\">AI </a>\"无障碍新场景等方面的研究、应用和趋势，这些深入的讨论，也为共同探讨如何通过人工智能技术，打破生活中的障碍，让每个人都能平等享受科技便利提供创新研究方向。</p>",
    "publish_time": "2023-07-08 20:44:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]