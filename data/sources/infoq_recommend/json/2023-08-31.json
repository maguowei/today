[
  {
    "title": "GitHub工程师分享将 Copilot 引入工程实践的方法",
    "url": "https://www.infoq.cn/article/4wFGDTaXYE3ewP5RJwQx",
    "summary": "<p>GitHub工程师Albert Ziegler和John Berryman表示，不需要拥有机器学习或生成式AI博士学位就可以创建有效的基于LLM的应用程序，提示词工程是关键。他们还分享了他们<a href=\"https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/\">在开发GitHub Copilot过程中所积累的经验</a>\"。</p><p>&nbsp;</p><p>LLM的崛起为那些希望在应用程序中利用生成式AI的从业者创造了一个全新的领域。这个领域被称为<a href=\"https://en.wikipedia.org/wiki/Prompt_engineering\">提示词工程</a>\"，专注于如何指导LLM产生不属于其预训练部分内容的输出。人们可以通过提示词工程定义包含足够多上下文信息的提示词，让LLM产生可能最佳的输出。</p><p>&nbsp;</p><p>上下文信息存在于用户领域，并且应该与任务规范一起被包含在提示词中，而任务规范存在于不确定的文档领域，在那里，LLM只是一种可以预测下一个标记的预测器。如果这两个领域之间没有被正确映射，例如，没有在提示词中告知响应应该被作为“一个有用的IT专家”生成的内容返回，那么返回的响应可能会很一般。</p><p>&nbsp;</p><p>Ziegler和Berryman表示，对于Copilot来说，有用的上下文信息可能包括语言、文件路径、光标上方的文本、光标下方的文本、其他文件中的文本，等等。</p><p>&nbsp;</p><p></p><blockquote>用户领域和文档领域之间的转换正是提示词工程所覆盖的领域——由于我们已经在GitHub Copilot项目上工作了两年多，所以在这个过程中发现了一些模式。</blockquote><p></p><p>&nbsp;</p><p>总的来说，他们建议的方法是基于一系列步骤的。首先，你需要收集所有相关上下文（也就是上下文收集），可能包含所有的源文件。在大多数情况下，这些上下文信息的量将超出可用的LLM窗口，因此你需要通过将其分割成较小不重叠的块。接下来的两个阶段是找到一种自然的方式将上下文信息注入到LLM文档中，例如，对于Copilot来说就是使用代码注释，并根据其相关性确定要包含的片段的优先级。如果你有多个LLM模型可选择，那么另一个阶段是决定使用哪个模型进行推理。最后一步是定义一个停止标准，让LLM知道何时完成，例如，当输出换行符时。</p><p>&nbsp;</p><p>实现提示词工程有很多种方法。最近，<a href=\"https://www.infoq.com/news/2023/02/microsoft-lmops-tools/\">微软开源了LMOps工具包</a>\"，其中包含了<a href=\"https://arxiv.org/abs/2212.09611\">Promptist</a>\"（一种用于优化用户文本输入以生成图像的工具）和<a href=\"https://arxiv.org/abs/2212.06713\">结构化提示词</a>\"（一种用于在少量学习提示词中包含更多样本来生成文本的技术）。</p><p>&nbsp;</p><p>尽管我们可以推测LLM将发展到不再需要提示词工程的地步，但OpenAI工程师Sherwin Wu在上一次纽约QCon大会的“生产环境中的LLM”小组讨论会上指出，<a href=\"https://www.infoq.com/news/2023/06/qcon-ny-llm-panel/\">至少在未来五年内仍然可能需要它</a>\"。</p><p>&nbsp;</p><p>如果你对GitHub在提示词工程方面所采用的方法感兴趣，请不要错过这篇完整的文章，它涵盖了比本文更多的细节内容。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/copilot-prompt-engineering/\">https://www.infoq.com/news/2023/07/copilot-prompt-engineering/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/1TXzB56iFw4rB9rSwkp1\">Copilot Chat 推出公开测试版，GitHub：花费数月的单元测试成为过去</a>\"</p><p><a href=\"https://www.infoq.cn/article/ZRjMrDXcL4XPskS7CgzJ\">集成 GPT-4 的编程神器来了，GitHub 发布 Copilot X：编程 30 年，突然就不需要手敲代码了？</a>\"</p>",
    "publish_time": "2023-08-31 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "支持20+ App、100+业务场景，淘宝端计算平台架构设计与业务实践",
    "url": "https://www.infoq.cn/article/QgfW14qo7jsL8W3sK9uu",
    "summary": "<p>在搜索推荐等算法场景中，端侧基于用户行为进行实时推理，可以有效提升算法效果，但这需要在端侧进行高效的用户行为采集计算和模型推理。而且，端侧计算环境复杂，计算任务研发成本高，数据离散，数据理解缺失，这就要求我们需要进行端侧环境抽象隔离，才能实现低研发成本。</p><p></p><p>淘宝端计算平台基于自研的 MNN 推理引擎和深度优化的 Python 虚拟机，在端侧构建了完善的模型推理和特征计算环境，并构建了完善的发布监控等基础设施。目前淘宝端计算平台已服务了包含手机淘宝在内的 20+ App，支撑了百余个业务场景，并取得较好的算法和业务效果。</p><p></p><p>本文整理自字节跳动架构前端平台架构团队负责人黄丛宇（旁通）在 ArchSummit 2022 北京站演讲分享，主题为“淘宝端计算平台介绍”。</p><p></p><p>本次分享将从计算、数据、质量等多个维度，介绍淘宝端计算平台是如何搭建的以及端计算平台在淘宝业务中的实践，以期为你提供构建端计算平台的可复用经验。</p><p></p><p>以下是分享实录。</p><p></p><p></p><h2>⼀、整体架构</h2><p></p><p></p><p></p><h3>端计算平台面临的问题和挑战</h3><p></p><p></p><p>首先介绍淘宝端计算平台的整体架构。</p><p></p><p>端计算是相对于云计算的一个概念，相比云计算它有几个优势：第一，如果大家是基于云计算去做业务，在端上的交互要经过网络的传输，尤其是在移动端，2G 可能会是几十毫秒，甚至几百毫秒，但如果放到端上去做，只需要调用一个函数就能响应，是实时的。第二，在端侧做各种逻辑、计算的时候，数据存储不需要上云，数据存在用户的手机上，这样可以解决云计算中用户数据上云带来的隐私问题。如果数据上云存储，那么要传输的数据量会非常大，尤其对于手淘这种体量的 App 来说，它的传输量可能是 TB 级或者 PB 级的。第三，随着这两年手机的算力不断提高，移动端的计算能力是过剩的，而云端的计算资源捉襟见肘，所以把一些逻辑推到端上去做是有效利用资源的手段，可以节省云端的资源消耗和成本。</p><p></p><p>什么是端计算平台？在淘宝的环境下的定义，就是面向端侧、算法任务的一站式研发平台，包括基于 Python 的多端一致的计算容器和计算框架；一个端云数据通道，用来解决数据传输问题；以及一个统一的管理控制台，用于支撑整个任务的发布和管理。淘宝的端计算平台要解决以下几个问题：</p><p></p><p>统一高效的计算环境。因为我们有 iOS 端、安卓端，一个统一高效的计算环境可以屏蔽各种细节，这样我们只需要关注自己的算法逻辑，不用解决端侧功能上的细节问题。灵活的发布实验策略。这个策划也是跟算法强相关的，因为算法的研发思路或模式跟工程不太一样，算法需要做大量的实验，要在一个业务场景里面做很多 AB 实验，这对于整个发布的灵活性，以及发布策略的兼容性的要求非常高，而 App 端的发布次数远远不能满足要求，所以端计算平台一定要能解决灵活发布的问题。便捷的研发调试能⼒。因为算法要开发 Python 脚本，开发 Python 脚本需要研发调试的能力。</p><p></p><p>以上是整个端计算平台要解决的三大问题，后面会针对这三个问题详细讲解端计算平台各个部分的设计。</p><p></p><p></p><h3>淘宝端计算平台的整体架构</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c6/c6b17d443327840700373b1071869ae3.png\" /></p><p></p><p>上图是整个端计算平台逻辑，图左边是端侧相关的逻辑，最底下是基础能力，包括数据存储，是基于阿里内部一个优化版的 Sqlite 而不是源码做的。然后是 AliNNPython、MNN 引擎，MNN 引擎是我们端侧团队非常核心的深度学习推理引擎，它是在端上去做深度学习推理，基于 AliNNPython 做了一些优化和拆解，后面会详细讲解。</p><p></p><p>基础能力的封装层面向算法同学，提供更高层的能力的透出，方便他们去写脚本。这里面封装了各种 API，包括数据读写 API、模型推理 API、各种监控 API 等。端侧用户行为采集是整个端计算平台非常核心的一点，端计算平台是要能够基于用户的行为去做分析的。因为端上运行的脚本需要跟业务场景或上下文有关联，而用户行为是非常核心的关联点，我们这里会对用户行为做非常深入的处理。再上面一层我们封装成一个简单的框架，这个框架就是 PaaS 层，算法同学或者业务同学可以基于这个框架更方便地写脚本。在这之上，我们会去做一些任务的管理调度，这个是比较通用的功能，用于端上任务资源的调度。</p><p></p><p>上图右侧最下面是数据通道，用于端计算的任务和云端的数据传输。MNN 工作台提供完善的调试和研发的能力，是一个全功能的 RDE，不是一个简单的小工具，后面还会介绍。再上面是端计算的控制台，它、是一个网页的控制台，可以做一些任务管控、特征管控、任务发布、异常告警情。再往上一层是云端特征接入和管控，算法或者业务上传数据到云端让业务方使用，需要管控层进行管理。</p><p></p><p></p><h2>⼆、计算环境</h2><p></p><p></p><p></p><h3>计算环境构建：AliNNPython 与 MNN 引擎简介</h3><p></p><p></p><p>计算环境的 MNN 引擎和 AliNNPython 构成了整个端上的运行基础，可以跑 Python 脚本，可以跑深度模型。针对这两个引擎，我们有一些算法或者功能上的需求。首先是算法需要跨端、动态，屏蔽工程上的烦琐细节。其次，从工程上看，工程人员都会追求高效，但在手淘这种非常复杂的 App 里面很多资源是有严格限制的，比如内存、CPU，或者整个存储空间，我们要在受限的环境下尽量满足算法跨端的动态需求。MNN 引擎最近发布了 2.0 版本，它是在端侧的轻量级、高性能推理引擎，大家可以直接去官网，或者 GitHub 主页了解，这里不再赘述。</p><p></p><p>AliNNPython 是基于标准的 Python 虚拟机 CPython，做了一个面向端侧受限环境的优化版本虚拟机，基于 2.7 版本。我们主要做了两个工作，第一是对大小做精简。手淘对整个 App 包大小有比较严格的限制，我们花了大量的精力去解决包大小的限制问题，把包从原来的十几个 MB 压缩到了现在的 1.3MB。我们把编译部分拆到了云端，在端侧执行脚本的时候，直接基于编译好的 Bitcode 执行，也就说把任务发到云端之前，先把它编译成 Bitcode，再把 Bitcode 发布到端上执行，把整个执行效率和包大小控制在合理的范围内。我们还删掉了很多对端测算法任务没用的一些 Python 自带的标准库，删掉之后整个包大小就逐步减小了。</p><p></p><p>第二是对 PythonVM 的架构做了比较大的升级，支持线程级的 VM。Python 有一个全局锁，全局锁会导致整个线程是串行的，在 Python 里跑多线程是没法跑满多核的。我们解决了这个问题，把“锁”想办法给去掉了，去掉后在端上可以实现在多个核上并行跑多个 VM，这样整个 VM 的执行效率就会有比较大的提升，这个对在端侧做任务调度也有非常大的帮助。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a2/a2351dfdc19e5a70df8f58b1df516f45.png\" /></p><p></p><p></p><h2>三、研发运维</h2><p></p><p></p><p></p><h3>任务组织</h3><p></p><p></p><p>计算环境核心就是这两个引擎，它们可以支持在端上跑深度模型、算法的 Python 脚本。为了让脚本能顺畅地发布到端上，我们有一套任务组织逻辑。首先我们对算法任务有组织约束。每个业务场景、需求场景都有一个 Git 仓库来管理所有的端计算任务的代码。每个仓库里面有多个分支，每个分支对应不同的任务。区别是，我们的分支是完全独立的，任务对应的分支是不会合并的，因为可能他俩干的事情是完全不一样的，一合并可能就乱了。分支是用来区分任务，而不是用来做平时那种合并类需求的。每个任务有一个对应的标签，标记任务版本，就是 tag。有 tag 就可以知道当前代码对应的版本，这样会有个比较简单的版本的管理。</p><p></p><p>基于场景或基于团队这个角度去做基于 Git 的组织方式的管理，在每个端计算的内部也有一个相对比较固定的结构。每个任务里面包括以下三个部分：</p><p></p><p>脚本，就是 Python 算法脚本；资源，资源包括共享资源和独享资源；资源最核心的就是模型，在端上跑的深度模型通常是放在资源里面；配置，用来做变量的设置。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a4/a46db2a7ccd599aaeeedc02491d7757e.png\" /></p><p></p><p>任务的内部结构我们做了简单的固定化，大家在组织结构的时候可以用来参考。要说明的是，资源我们是分共享资源和独享资源的，这是为了解决下发问题。因为资源通常是模型，模型经常是兆级的，在手淘这种体量的 App 里下发兆级内容到用户手机上是件很麻烦的事情，而且会消耗大量的带宽，所以我们把共享资源先单独做下发。比如，有些任务可能下发的是一些独享资源，某个任务在某个版本或者某个用户里面才用，就只给他下发到某个用户的手机里，而不是一波全部下发，避免大量的资源浪费，这是我们在系统层面做的资源优化。</p><p></p><p></p><h3>研发流程</h3><p></p><p></p><p>把任务组织起来后，我们有一个对应的端计算任务研发的流程。当新的业务来了之后，要怎么去一步步地将任务开发上线。首先要做问题定义，这是比较通用的事请，在做任何事情前要先定义要做什么。接下来是埋点开发。这点比较特殊，因为我们的端计算平台是基于用户行为触发的，而用户行为是基于埋点采集的。这里要确认埋点是不是能够解决问题，或者是不是需要开发新的埋点。开发埋点很麻烦，因为涉及到 App 发版，所以需要慎重考虑。埋点问题解决后，就开始创建任务，在这之后就是后面的开发任务流程了。</p><p></p><p>开发任务可以基于 MNN 工作台做脚本编辑、真机调试和数据预览。任务开发完后就走整个的发布流程。接下来是在端上去做任务的执行。任务的执行的执行点通常会有两个分支，一个分支是数据采集，因为我们是基于用户行为去做计算，很多算法业务会把采集到的行为做计算，生成用户行为特征。用户行为特征会上传云端，或者给端侧的其他任务作为输入。这个地方会有一个分支，如果需要上传云端，就基于数据通道，把数据传到云端，在云端去做推理训练。如果在端上还有另外一个推理，或者训练任务，就在端上完成训练。把采集到的数据作为端上训练的一个输入，去做训练或者推理。另一个分支是模型压缩，针对云端训练出来的模型进行转换或压缩。在云端，算法通常用 TensorFlow 去做模型训练，训练出来的模型经过转换工具转成 MNN 模型，再经过压缩后下发到端上。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/87/87e05a880de010b9a707fdcb0228a145.png\" /></p><p></p><p></p><h3>任务发布</h3><p></p><p></p><p>接下来我们讲任务发布的逻辑。首先是发布策略，因为这是一个非常强的算法需求。当算法任务下来后，发任务可能会涉及发布到 App 的不同版本里，而不同版本采集的用户行为是不一样的。比如埋点，可能有的埋点在旧版本里没有，这就需要它能够区分不同的版本。我们需要定制策略来去解决这个问题，比如在某一个人群里面、或某几台设备里面，甚至某个用户的某台手机里面跑某一个任务。这样还有额外的好处，就是一旦能够把某个任务发到特定手机上之后，就可以有一个非常好用的线上 Beta 发布功能，算法同学可以非常方便地测试这个版本。比如说上线之前，在自己的手机上测，或者在同事手机上测。还有就是，如果你突然收到某个 crash，或者某个异常，但只在某几台手机产生，就可以把带调试信息的任务发到那台手机上，或者那个版本的 App 上去做信息采集，从而方便地调试端计算上的任务。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ad/ad4091d1e7cbc5f3824c9f1264da3c1f.png\" /></p><p></p><p>上图 Relase 部分是我们整个发布的技术设计。针对发布流程我们内部有回滚机制、有 Beta 机制，还有灰度机制。任务发布不管是端计算任务，还是普通服务端任务都会有些机制。下面将任务发到手机上流程。因为用户的手机不是实时在线，发布是有发布率的，我们基于推送把最新的配置信息先推送到用户手机上，类似发一个通知告诉用户的 App 端计算任务有更新了，用户的 App 在启动后会去拉对应的更新的内容。推送是不可能做到 100% 覆盖的，我们是基于淘宝的底层的基础设施去做的，覆盖率、推送率是由他们来决定，我们就是在上层做了个封装，相当于我们借助淘宝的推送通道完成发布机制的。我们基于推拉结合的方式，先把更新的信息推出去，之后 App 再来拉取。这样推送通道的实时性会好很多，不需要推太多东西。</p><p></p><p></p><h3>调试运维</h3><p></p><p></p><p>调试运维就是 MNN 工作台，这个是对外公开的，大家可以去官网下载。这个工作台是一个全功能案例，支持调试、代码编辑、日志、打点、断点。写端计算任务的时候，你可以基于这个 MNN 工作台把整个流程全部完成，可以在这个工作台完成编辑、调试和发布等功能，它还支持实时监测手机内存和 CPU 使用量。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/87/87eb2c73d3a94937f089f2afd2ad3525.png\" /></p><p></p><p>这个控制台可以比较全面地监控已经发布到线上的任务。比如任务的运行次数、成功率、失败率以及运行失败分析。如果有运行失败，它会显示总的失败次数，下面还会有对应的错误的日志，或者异常的堆栈。最右下角这张图展示了整个任务在端上的执行的情况。但我们不是全量展示，因为就手淘的体量全量展示对云端资源的消耗量太大。这个事情不是业务强相关的，所以我们做了采样，在你开发调试的过程这个功能还是比较方便的。任务上线之后，运维能够覆盖到各种场景，不会出现任务发到线上后就不知道它到底做了什么的问题了。</p><p></p><p></p><h2>四、计算框架</h2><p></p><p></p><p>前面三部分讲完之后，大家应该能感受到我们整个端计算平台已经能够完整地支持任务开发、调和发布。算法或业务可以基于端计算平台去做开发了。但是光有这个平台是不够的，这个东西业务方到底怎么用？怎么写脚本？怎么去拿用户行为？查到的数据怎么用？为了解决这些问题，我们就做了一个计算框架。这是搞技术的通用思路——再加一层，这个地方我们就加了一层。计算框架就是面向端侧用户行为计算能力的封装。为什么要做这个事情？因为端侧的用户行为非常复杂，同一个点击在不同的页面代表的含义是不一样的。用户的行为的关联很复杂，要根据业务去定义。算法拿到的是单点数据，这个单点的数据怎么跟业务场景做关联？也就是说，基于单点采集的数据缺少场景、关联性，我们要去解决这个问题。同时还有一个问题，算法在基于用户行为做解决方案的时候，需要知道用户在买了商品之前，到底干了什么事情，点了什么，滑动了什么，收藏了什么，这些东西要串成一条完整样本，这个样本非常复杂。这种现状和需求会导致三个问题：</p><p></p><p>现状和需求差异比较大，一个单点就能组合出非常复杂的样本。计算逻辑非常复杂。算法在做方案时要不断查各种数据库、各种数据表，然后去算某个点击到底在哪，过程非常烦琐。数据种类增多。比如用户的每次点击在端计算平台来看就是个点击，但在算法角度它是不同的点击，对应的是不同的属性。比如收藏和购买这两个都叫点击，但对算法来说肯定是不同的事情。整个的数据种类会随着算法不断更新和语义扩展，而不断增加。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/62/62c274cf89c21354a8cd44a9f17b0fdc.png\" /></p><p></p><p>为了解决这三个问题，我们做了下面三件事情。</p><p></p><p>数据标准。把端侧用户行为做定义（或者叫规范、范式），这样算法再去基于用户行为计算场景的时候，就不需要再去知道具体埋点的细节。计算框架。用来解决算法在脚本中各种问题，就是要定义各种 API、各种组件，帮助用户解决各种复杂的问题。管控平台。把面向复杂场景的管理平台做得更细化，更好地满足算法需求。</p><p></p><p></p><h3>数据标准</h3><p></p><p></p><p>我们把用户的行为基于时间、空间这两个概念构建成了一个流（或者树）。在空间上，比如用户在某个页面里面做了点击，之后到了另一个页面，这就形成了一个上下级的关系，这个上下级的关系是可以构建成一棵树的。这种关系就可以组合成一棵用户行为树。在时间维度，用户的所有行为都是基于时间单向递增的，这种单向递增就是一个非常完美的数据流。基于这两个概念可以推导出两件事，一个是任务触发，比如开发一个端计算任务，想基于用户的某个行为触发，但这个行为的触发是有上下文场景的，可能在详情页里触发也可能在首页触发，于是这棵行为树天生就带着场景信息。整个行为触发会有一个比较通用的模式，这样大家可以顺着通用模式去解决问题。第二个推导是，有了用户行为树 / 流之后，在端侧计算用户的行为时，它会把它转变成一个树 / 流的裁减 / 归并。把之前一些很复杂的数据库查询转变成非常规范的数据结构操作。如图右侧所示，用户在某个页面里面进去之后又出来到下级页面，会形成层级的关系。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e5/e52f4afeac1f5b3604c0f5e61193db1a.png\" /></p><p></p><p></p><h3>计算框架：流数据处理框架</h3><p></p><p></p><p>大家不要把这个框架和云端的流数据处理框架混淆，它是面向端侧行为数据流的，不是大家理解 Blink 那种数据流处理。这个框架做了两个核心的事情，一个是触发的配置，就是算法需要知道这个任务在什么地点触发，基于场景的上下游的关系去做配置。这个地方的配置就是长串的 Page_name。在这里面触发之后，用户行为的获取基于刚才的设计，把用户行为数据某一棵子树拿下来处理，这里我们做了比较简单的封装。先基于用户行为条件过滤一下，把整棵子树拿下来，拿下来之后再做一些转换。我这里边写的比较简单，就直接打印一条日志，最后得到的是一个用户的行为。这样再基于这些用户行为数据去进一步做算法处理，或者特征的计算等。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/96/96610d604aac9698e04930616d30af94.png\" /></p><p></p><p></p><h3>管控平台</h3><p></p><p></p><p>算法在端计算平台上有两大类事情，一类是去做深度模型的推理，深度模型的推理就是发一个模型到端上去做推理。另一类事情是，把采集上来的用户行为作为特征，作为模型的输入。我们针对特征层面做了更细化的管控。特征市场是为了帮算法做业务推广。比如有个特征做得很好，想给其他业务方用，这个时候在特征市场里就能够直接看到特征具体样子。</p><p></p><p>特征监控是针对在数据层面，可以看到一共算了多少数据，每个数据有预览，可以对数据的正确性、成功率、失败率等特征方面进行监控。特征发布跟任务发布是放到一起的，特征发布跟发布 App 或服务端变更差不多，有预发、线上、灰度流程，把这个流程走完了再去发步，以免系统崩溃。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b5/b5bae1478053a1f326905108a145b434.png\" /></p><p></p><p></p><h2>五、业务实践</h2><p></p><p></p><p>最后讲下业务实践。我选取了一个比较核心的场景——端上信息流推荐。端上信息流推荐就是淘宝首页，大家打开淘宝首页会刷新很多商品，它是基于你的各种喜好推荐出来的。比如你下拉了 50 个商品，在逛的过程中，可能会去点某个商品，也可能会长按某个商品点不感兴趣按钮。或者说对某个商品图片，你的停留时间比较长，点了这个商品，都说明你对这个商品的兴趣非常的浓厚；如果你点了那个不感兴趣，就是不喜欢这个商品。这里可以看到，人们的兴趣是不断变化的，以前的方式，一次下拉出来的 50 个商品就在你的手机里存着，我们没法去根据你的兴趣动态调整，这对整个推荐效率是有影响的。信息流的作用就是在端上做重排。用户在去下滑的过程中，会去动态的调整没有上屏的商品顺序，把你最感兴趣的商品放到最前面。也就是说，下面蹦出来的商品不是按之前的顺序，而是根据你感兴趣的程度动态提上来的，让你以最快的速度看到最感兴趣的商品。</p><p></p><p>端上信息流推荐解决了两个问题。一个是端上对用户行为感知是有延迟的。用户行为要基于各种通道上传到云端，可能几十秒，一两分钟就过去了。二是对用户行为感知是不全面的。比如滑动的快慢是很难上传到云端的，因为数据量太大了。除了端上重排，我们还做了智能请求。如果重排之后，后面这几十个商品都不是你感兴趣的，那么再怎么重排也没什么意义。这个时候就重新从云端基于你之前的各种新的兴趣点拉数据，拉下来之后再做重排，更好地匹配你的兴趣爱好。</p><p></p><p>下图右侧就整个系统的架构图。带来的业务效果就是，在去年大促时大概就这个场景的 GMV 提升了 10% 以上，以上是整个端计算平台在信息流上的使用场景介绍。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f1/f1d2b0835668102222c9a40537a1ae64.png\" /></p><p></p><p>整个端计算平台后面会朝更高效、更精细、更稳定的方向去发展，这是平台类产品共同的发展方向。我们后面也会进一步打磨整个平台的各种能力。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e9/e9446d89b3172cf10df3bcbb5e9f62d8.png\" /></p><p></p><p></p><h5>演讲嘉宾：</h5><p></p><p></p><p>黄丛宇（旁通），阿里巴巴技术专家。清华大学软件工程硕士，毕业后进入阿里巴巴，目前在淘宝端智能团队负责相关算法工程系统和端计算平台研发工作，主要负责服务端业务和平台系统的设计。</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/7813aaccafb98072f534029d9\">深入探析数智时代下的分布式系统架构设计</a>\"</p><p><a href=\"https://xie.infoq.cn/article/b581340c41fbfdf09c8e74e55\">解析架构设计：构建可靠、高效的软件系统</a>\"</p><p><a href=\"https://www.infoq.cn/article/1yc8IjNAFAOBy8Tw6xjk\">中台之上：业务架构设计</a>\"</p><p><a href=\"https://xie.infoq.cn/article/23477461449805031aefc4be2\">浅谈复杂业务系统的架构设计 | 京东云技术团队</a>\"</p>",
    "publish_time": "2023-08-31 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "WebAssembly 自我突破之路：如何构建一个跨编程语言的新生态？",
    "url": "https://www.infoq.cn/article/9B15Mt418sbeS2oTfA81",
    "summary": "<p>作为近几年最令业界感到兴奋的新兴技术之一，WebAssembly（缩写为 Wasm）已经拓展到浏览器之外，在嵌入式和云端都有了相当广泛的使用场景。随着 Wasm 不断地被各个语言及平台所集成，使用场景日益复杂、使用的开发者越来越多，新的问题也出现了。</p><p></p><p>一般而言，开发者在开发 Wasm App 的时候，往往会使用自己熟悉的编程语言做开发，比如 C、Rust、Java 或者 Go 等等，然后利用工具链将那些 C、Rust、Java 或者 Go 应用转换为一个 Wasm Module 并运行。这个过程在应用由单个 Wasm Module 组成的时候很流畅，不会遇到问题，但是当应用中需要包含多个由不同语言编写的 Wasm Module 时，就会出现多语言屏障、工具链一致性等棘手问题，给开发者增加不少负担。</p><p></p><p>为了解决多语言多模块互通问题，Wasm 社区推出了名为 WebAssembly Component Model 的新提案，目前这是 Wasm 社区最高优先级推进的一项工作。</p><p></p><p>在 9 月 3-5 日即将召开的<a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=caifang&amp;utm_campaign=10&amp;utm_term=0831&amp;utm_content=heliang\"> QCon 北京 2023</a>\" 上，Intel Web Platform Engineering 软件工程师、Wasm Micro Rumtime 项目主要贡献者<a href=\"https://qcon.infoq.cn/202309/beijing/presentation/5441?utm_source=infoqweb&amp;utm_medium=caifang&amp;utm_campaign=10&amp;utm_term=0831&amp;utm_content=heliang\">何良</a>\"将带来以《WebAssembly Component Model 构建一个跨语言的新生态》为主题的演讲分享。会前，InfoQ 对何良老师进行了专访，围绕 WebAssembly 技术演进和应用现状、WebAssembly Component Model 方案想要解决的问题和发展历程等话题展开探讨。</p><p></p><p>以下为访谈实录，经 InfoQ 编辑整理：</p><p></p><p>InfoQ：很荣幸有机会采访您，请您先介绍一下自己的从业经历。您是在什么时候、因为什么契机接触到 WebAssembly 的？又是如何加入 WebAssembly 社区的？</p><p></p><p>何良：我们是一个长期专注于 language runtime 的团队。早期从事 secure Java VM 的研发和优化，目前聚焦在 Node JS 和 WebAssembly language runtime 领域。我们在 Wasm 领域的项目叫做 wasm-micro-runtime。该项目启动于 2018 年，当时是 WebAssembly Post MVP。后来随着 2019 年 BytecodeAliance 成立，WAMR 项目贡献给了社区，也是最早的几款浏览器之外的 Wasm standalone runtime。除了满足通用的特性，针对嵌入式设备的特殊使用场景提供了很多定制化的特性。</p><p></p><h4>WebAssembly 现状与存在的问题</h4><p></p><p></p><p>InfoQ：WebAssembly 最初是为浏览器设计的，很多人对它的认识可能还是局限在浏览器，实际情况是什么样的？您认为 WebAssembly 当前处于什么样的发展阶段？</p><p></p><p>何良：WebAssembly 的使用场景已经扩展到浏览器之外，在嵌入式和云端都有广泛的使用场景。这主要得益于几个优势：由于 Wasm 运行在 sandbox 之内，天然在安全性上有优势。Wasm + WASI 的组合成为云上超轻量级的容器方案，这个方案比 docker 有更快的启动速度，更轻量的体积。另外，Wasm 语言中立的特性，可以让 Wasm 成为系统组件化（或者说插件化）的新思路，也就是将基础、稳定的底层功能以二进制形式发布，将需要快速变更、快速部署的业务逻辑代码以 Wasm 形式发布。</p><p></p><p>InfoQ：能否详细介绍一下目前 WebAssembly 在 Intel 的采用情况（包括主要在哪些业务场景下使用 WebAssembly、如何使用、带来了哪些收益或效果）？</p><p></p><p>何良：我们主要参与 Wasm spec 的制定和 language runtime 的开发。应用以嵌入式为主，也为客户提供云上方案的参考设计。</p><p></p><p>InfoQ：业界有声音认为 WebAssembly 的潜力尚未得到充分发挥，尤其在服务器端。您是否认同这一看法？在您看来，如果社区想让 WebAssembly 得到更广泛采用，有哪些问题亟待解决？</p><p></p><p>何良：是的。延续前面第二个问题的回答，Wasm 虽然有不少优点，但还没有得到足够的空间来施展。这里面的原因很多，我认为比较主要的包括：</p><p></p><p>工具链不丰富，不单指能够将某种开发者熟悉的语言转换为 Wasm module 的编译工具，还有调试工具和性能分析工具。</p><p></p><p>缺少有统治力的杀手级应用，Wasm 提供的能力更像是对现有能力的提高，而非一个解决问题的必需品，所以需要有一个有影响力的应用来做“吃螃蟹的人”。</p><p></p><p>接下来社区会重点推广的 Component Model 解决了多语言的问题，将有助于壮大 Wasm 的使用范围，将更多开发者卷入进来。</p><p></p><h4>社区最高优先级工作：WebAssembly Component Model</h4><p></p><p></p><p>InfoQ：您这次要分享的主题是“WebAssembly Component Model” ，那么，到底什么是 WebAssembly Component Model？原来的 WebAssembly Module 存在什么问题？为什么需要引入 Component Model？</p><p></p><p>何良：Component Model 是 WebAssembly 解决多语言多模块互通问题的方案。通常开发者在开发 Wasm App 的时候，并不会直接用“汇编语言”写。开发者往往使用自己熟悉的语言，比如 C、Rust、Java 或者 Go 等等开发，然后利用工具链将那些 C、Rust、Java 或者 Go 应用转换为一个 Wasm Module 并运行。</p><p></p><p>这个过程在应用由单个 Wasm Module 组成的时候很流畅，不会遇到问题，但是当应用中需要包含多个由不同语言编写的 Wasm Module 时，问题就出现了。</p><p></p><p>第一个问题是语言屏障。由于是不同的开发者用不同的语言来写 Wasm library，这里就有一个很自然的问题，如何才能让两个语言的不同类型系统互相理解，避免“鸡同鸭讲”的局面。比如 Java 要传一个 List 给 C，Rust 要传一个 Union 给 Golang。除去这些复合类型，还有一些语言特有的概念，比如 Optional、Pointer 等，不一定能在每个语言里找到对应的表示。</p><p></p><p>第二个问题是工具链的一致性。在 Wasm 中，两个模块的接口称为 import 和 export。import 的符号必须要和 export 的符号保持形式（比如函数签名）的一致才能成功对接。这就要求不同语言的工具链在转换 Wasm 的时候，对于逻辑上类似的类型概念要做出相同的转化，以保障在 Wasm 层面的对接能成功。也就是说，即使 Java 和 C 都说好了 List 如何用各自的语言表示，仍然需要两个工具链能够把各自的 List 的表示变化为相同的 Wasm 表示。</p><p></p><p>第三个问题是隔离性。如何划定主要几个 module 之间的内存范围？module 和 module dependencies 之间如果能共享线性内存，那么不同 module 的相同 dependencies 之间是否能共享线性内存？这些问题的答案似乎会随着使用场景的不同发生变化。</p><p></p><p>综上所述，当开发者需要从 Wasm Module 中分化出不同功能的 libraries，并希望用多种语言来实现复用和协作的时候，就会遇到多语言屏障。除此之外，Wasm 世界也迫切需要更强大的类型系统，来丰富 Core Wasm 的简单类型系统。</p><p></p><p>原有的 WebAssembly Module 虽然发展出了 Interface type 和 nanoprocess 的提案，但没法满足 WASI 在可移植接口上的要求。WASI 希望能同时具有模块化和基于能力模型的安全特性，这两个要求都指向了内聚，足够的内聚又引发了模块协作的问题。</p><p></p><p>InfoQ：社区是什么时候提出相关定义的？能否带我们回顾一下 WebAssembly Component Model 的发展历程（或发展路线图 RoadMap，有哪些关键节点）？目前 WebAssembly Component Model 已经是可用状态了吗？接下来的规划是怎样的？</p><p></p><p>何良：在 MVP 时代，社区已经在尝试解决多个 Wasm Module 的问题。</p><p></p><p>当时的重点是在运行时状态，也就是多个 Module Instance 在内存里的形态。比如怎么分配运行栈、怎么分配线性空间等等。主要的模型有两种，一种是类似 Linux 库之间的 shared-everyhing linking，一种是注重安全性的 shared-nothing linking。这个方向的 spec proposal 叫做 Module Linking。</p><p></p><p>另一个值得关注的 spec proposal 叫做 interface type。这个主要是为了解决 WASI 标准化。早期的 WASI 是用类 C 的语法来实现。由于早期开发者多为 Rust 和 C 的背景，类 C 的 WASI 也能工作得很好。</p><p></p><p>紧随其后，社区提出了 nanoprocess 的 vision。希望在保持内存独立的前提下， 每个 Module 都通过固定的“管道”和其他 module 协作通讯。</p><p></p><p>但随着 Wasm 被更多的开发者接受，多语言的问题逐渐变成了一个更大更迫切的问题。这里的多语言不只是从 C 增加到 Rust、C++ 这些系统语言，而且增加了 Java、Go、Koltin、Dart 等高级语言，甚至还有 JS、Python 等动态语言。再加上虚拟化、隔离等要求，社区就合并了 module linking 和 interface type，诞生了 Componet Model。</p><p></p><p>Component Model 现在是社区最高优先级推进的工作。随着 WASI Preview2 的不断推进，Componet Model 将在短时间内落地。Componet Model 本身已经具有相当的成熟度， 基本的工具链和 runtime 支持已经推出。Componet Model 正在开发的包括：增加 thread spawing 的支持、异步的支持等等。</p><p></p><p>InfoQ：WebAssembly Component Model 希望实现哪些设计目标？（有哪些关键的设计原则）</p><p></p><p>何良：主要包括这几点：遵守 Core Wasm 定义的框架；语言中立；支持 WASI 的可移植、虚拟化需求；实现隔离性（默认实现 shared-nothing linking 同时支持 shared-everything linking）；有利于浏览器支持。</p><p></p><p>InfoQ：WebAssembly Component Model 与现有的 WebAssembly 其他规范和提案，有什么区别和联系？</p><p></p><p>何良：WebAssembly Component Model 是早期两个 proposal （module linking 和 interface type）的合成和延续。不依赖 Wasm GC 的存在，不会破坏 Core Wasm 规范的设计。</p><p></p><p>InfoQ：您的演讲提纲中提到了“下一代 WASI”，如何理解 WebAssembly Component Model 与下一代 WASI 之间的关系？</p><p></p><p>何良：下一代 WASI 使用了 Component Model 提供的抽象类型系统，目的是利于更多语言实现 WASI。这也意味着为了运行使用新 WASI 的 Wasm APP， runtime 必须支持 Component Model。</p><p></p><p>Comonent Model 提供的隔离性和移植性有助于 Wasm library 实现复用和协作的目标。而 Component Model 引入的 resource、stream、异步等概念，有助于扩展 WASI 的能力。</p><p></p><p>InfoQ：在您看来，过去这两年 WebAssembly 在技术演进、社区建设、应用落地三大方面进展算快还是慢？有什么令人眼前一亮或影响比较大的里程碑事件吗？</p><p></p><p>何良：Wasm 过去在技术演进方面有很多进展。像 GC、Component Model、WASI-Thread、WASI-Socket 等重量级的提案出现了很多。在社区建设上，越来越多的开发者和落地方案在尝试、评估并使用 Wasm，这是我们乐于见到的。虽然在应用落地上还是没有出现有影响力、有话题度的案例，但我们希望培育好土壤，提供充足的阳光和养料，最后能结出硕果。</p><p></p><p>InfoQ：请您分享一下对 WebAssembly 未来发展的看法和期待？在 WebAssembly 社区重点投入的技术方向中，最令您感到兴奋的是哪一个方向？为什么？</p><p></p><p>何良：Wasm 是对很多现有技术的补充和替代。随着影响力慢慢扩大，它会得到更多的应用场景和实践。作为一门新兴的技术，Wasm 呈现出了学界和工业界对类型系统、模块化、容器化等多个领域的思考和实践总结，可以说是这些领域的集大成。</p><p></p><h5>采访嘉宾介绍</h5><p></p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/presentation/5441?utm_source=infoqweb&amp;utm_medium=caifang&amp;utm_campaign=10&amp;utm_term=0831&amp;utm_content=heliang\">何良</a>\"，Intel Web Platform Engineering 软件工程师，Wasm Micro Rumtime 项目主要贡献者。长期参与 WebAssembly 社区活动和推广。</p><p></p><p><a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=caifang&amp;utm_campaign=10&amp;utm_term=0831&amp;utm_content=heliang\">QCon 全球软件开发大会·北京站</a>\"，以「启航·AIGC 软件工程变革」为主题，将于 9 月 3 - 5 日于北京·富力万丽酒店正式开幕。此次大会策划了大前端融合提效、大模型应用落地、面向 AI 的存储、AIGC 浪潮下的研发效能提升、LLMOps、异构算力、微服务架构治理、业务安全技术、构建未来软件的编程语言、FinOps 等近 30 个精彩专题。会上，何良老师将围绕《WebAssembly Component Model 构建一个跨语言的新生态》主题做进一步分享，详细解读 WebAssembly Component Model 方案和下一代 WASI 的技术新动向，敬请期待~</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11fa662ed6d15860cbfbe57a20f03ccc.png\" /></p><p></p><p>还有更多精彩内容，尽在 QCon 全球软件开发大会·北京站， 9 月 3-5 日三天沉浸式学习，近 30 个场热门专题，130+ 名讲师现场分享。9 月 3 日上午，7 场<a href=\"https://qcon.infoq.cn/202309/beijing/track/1565?utm_source=infoqweb&amp;utm_medium=caifang&amp;utm_campaign=10&amp;utm_term=0831&amp;utm_content=heliang\">主题演讲</a>\"还将免费对外直播，想与 LangChain 作者 Harrison 交流？那就赶快扫码预约吧！</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1f/2f/1f88f66b16e5346a59299a8111c0d42f.jpg\" /></p><p></p><p>大会报名仅剩最后 3 天，咨询购票优惠信息可联系票务经理 18514549229（微信同手机号）。</p>",
    "publish_time": "2023-08-31 12:24:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "革命性软件定义存储 HBlock 极致易用背后的技术解析（第一期）",
    "url": "https://www.infoq.cn/article/bCwaTHw1hjf5sjU7GJFy",
    "summary": "<p>企业对于“数据驱动”这个词都不陌生，也都深知数据对于业务发展的重要影响。然而，如何保障传输数据的安全?如何避免意外场景下的数据丢失?如何合理利用老旧服务器?如何实现高效运维，有效节约人力和时间成本…</p>\n<p>是否存在一款存储产品能同时满足，使用者对成本、性能和安全的考量呢？天翼云重磅推出的存储资源盘活系统 HBlock，或许可以满足你的所有期待！</p>\n<p>InfoQ 联合天翼云策划了主题为《存储难题新解法，揭秘极致易用的 HBlock》的线上技术分享会，本期为第一期直播回放——《革命性软件定义存储 HBlock，极致易用背后的技术解析》。</p>\n<p>*直播回放中的入群抽奖互动截止到9月6日，欢迎扫码入群！如你对 HBlock 有更多想要了解的内容，可以评论区留言～</p>",
    "publish_time": "2023-08-31 14:32:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "台积电在美国投资400亿美元的芯片厂要建不下去了",
    "url": "https://www.infoq.cn/article/XcZYgQC2sQe9wBVYWgp5",
    "summary": "<p><a href=\"https://www.infoq.cn/article/Lw6O4deaOh1yRuC2jsLX\">台积电</a>\"曾承诺砸400亿美元去美国亚利桑那州兴建晶圆厂，这既是该州有史以来规模最大的一笔投资，也创下美国历史上最可观的外国投资纪录。但如今八个月时间过去，这座位于菲尼克斯的微芯片工厂仍然迟迟无法上线。</p><p>&nbsp;</p><p>目前，台积电方面已经将正式生产计划推迟至2025年，理由是缺乏熟练劳动力。台积电正努力为500名台湾工人快速办理签证。但与此同时，工会指责台积电虚构技能短缺问题，并以此为由雇用更便宜的外国劳动力。还有人称台积电工厂中存在安全隐患。</p><p>&nbsp;</p><p></p><h2>工人抱怨：管理混乱、没有安全保障</h2><p></p><p>&nbsp;</p><p>菲尼克斯半导体制造工厂（以下简称晶圆厂）是一项庞大的工程，占地1000英亩，具体包含两座晶圆厂的设施建设目标。预计施工将创造2.1万个建筑工作岗位，其中设施内劳动力估计约为4500人，同时将为当地各类供应商提供额外几千个工作岗位。</p><p>&nbsp;</p><p>但根据内部人士在接受外媒《卫报》采访时的说法，该工厂的建设进度因事故和误解等问题而受到阻碍。一位前现场主管称，现场施工的所有承包商都接受台积电两家附属公司的管理和指挥，分别是United Integrated Services（简称UIS）和Marketech International Corp，而工期延误主要源自管理层组织混乱和外来老板对美国安全规范/法规认识的缺乏。</p><p>&nbsp;</p><p>据报道，工作人员一旦表达任何反对意见，对方就会威胁“把你的工作交给别人”。“于是，没有工会的承包商发现根本就找不齐足够多的技术熟练人员。”由于担心总承包商雇主的报复，内部人士们纷纷要求匿名报道。</p><p>&nbsp;</p><p>这些匿名人士提到，随着现场工作的推进，所有工人都需要接受安全培训。但在实际施工过程中，他们从没见过有人真正执行安全计划、或者履行安全协议。</p><p>&nbsp;</p><p>“同一个小区域内存在着多家总承包商，他们说法不一、各行其是。没有人从中协调，每个人都在相互妨碍。人们把物料堆放得到处都是，最终导致一个个小项目进度缓慢。”</p><p>&nbsp;</p><p>他们还表示，总承包商会向他们指定须优先完成的任务，但具体任务内容每天都会改变。总承包商甚至会彻底改变主意，之前的规划全都不做数，因此施工方当然无法完成任务、延误问题也愈发严重。</p><p>&nbsp;</p><p>工人们坦言，“我们总是得把东西装起来、拆下去、再装起来、再拆下去，反反复复折腾个五、六次。这导致造价成本也飙升了五到六倍，毕竟拆除和重新安装就是要额外算钱。整个施工过程基本就是这样，每个决定似乎都是匆忙做出。他们没有给我们真正的设计图，拿到的只有一堆局部工程图，感觉就像是在边做边设计。”</p><p>&nbsp;</p><p>工人还批评了工地频繁组织人员疏散的情况，理由大多是误报。通讯问题也一直在延误工作进度。他们提到，由于地面泥泞且行进路线很长，进出工地的等待时间真的很难熬。而一旦遇到下雨，情况就更加糟糕。再加上承包商一会一个主意，最终让施工现场乱成了一锅粥。</p><p>&nbsp;</p><p>他们还提到，施工现场的移动式厕所太少，打扫不勤而且配备的卫生纸和肥皂都不够，这可能导致工人生病。遇到紧急安全情况时，工地要求大家别打911求助、而应打内部安全热线。但这些医疗服务总是需要很长时间才能做出回应。</p><p>&nbsp;</p><p>工人们得出的结论是，“我从来没见过这样的工作面。工作地点这么大，现场有这么多人，所以我们必须保证自己非常安全。要想获得成功、要想避免妨碍到其他人，你就必须制定出完美的施工计划，也就意味着一切都得放慢速度。我认为最好让那帮承包商赶紧离开，因为他们根本就不熟悉怎么搞建设。”</p><p>&nbsp;</p><p></p><h2>被“围攻”的台积电</h2><p></p><p>&nbsp;</p><p>工人和当地工会对台积电放出的劳动力水平不足等项目延迟理由提出了异议。Arizona Pipe Trades 469（亚利桑那州管道行业469工会）目前正在请愿，反对台积电申请500份签证来调动台湾省工人参与工厂建设。</p><p>&nbsp;</p><p>对此，台积电发言人强调，这些新的签证申请属于制程设备建设与安装流程中的一部分。“为确保工具安装这一关键阶段能顺利成功，半导体行业的常见做法就是从海外不同地点派出数量有限且经验丰富的人员，赶赴现场协助完成其中的重要操作步骤。这些经验丰富的人员对我们的供应商设备非常熟悉，并将在此阶段与我们强大的当地员工合作。”</p><p>&nbsp;</p><p>亚利桑那州建筑与建筑行业委员会主席Aaron Butler 则认为，台积电的声明会威胁到美国就业，并对台积电关于美国劳动力缺乏完成建设目标所需的经验和技能的说法提出了抗议。</p><p>&nbsp;</p><p>“把项目上的问题归咎于美国工人，既冒犯了美国工人又不符合事实。台积电明显是把工期延误的锅甩给美国工人，并打算以此为借口用更低的工资引进外国工人。”Butler说道。</p><p>&nbsp;</p><p>实际上，在今年6月，就有媒体报道称，菲尼克斯工厂现场一直受到错误、伤害和安全问题的困扰。台积电拒绝与当地工会签署项目劳工协议，决定通过非工会承包商管理大部分劳动力。工会报告称，在宣布取消电工们的项目奖金之后，大量中国台湾工人涌入工作地点接管了由工会支持的这些职位。</p><p>&nbsp;</p><p>另一位前雇员表示，2022年内工地上曾经出现过很多问题，包括工作时间过长、无法及时拿到工资，还有因工地上接触有害化学物质而造成健康问题等。</p><p>&nbsp;</p><p>“有些工人会一直在工字梁上喷洒防火化学涂料，而且不管下面有没有人正在吃午饭，他们就在上国不断喷洒。工地上几乎人人都在咳嗽，我觉得肯定跟这样的施工方式有关。而且在辞职一个月后，我的咳嗽就自己好了。”</p><p>&nbsp;</p><p>台积电并没有回应具体的安全投诉和问题，但公司发言人在邮件中表示，“台积电在我们所有设施的运营以及每个正在推进的建设项目（包括台积电亚利桑那州晶圆厂）当中，都坚定地致力于保障工作场所安全。我们会定期接受亚利桑那州安全与健康部（ADOSH）等组织根据现行安全标准开展的审查。台积电还根据州和国家公布的数字对安全记录进行内部审计。”</p><p>&nbsp;</p><p>“在亚利桑那州，我们的有记录安全事故率比全州平均数字低了近80%，造成工期延误的事故率更是低出近96%。”</p><p>&nbsp;</p><p></p><h2>张忠谋：成本超预期，美国建厂“注定失败”</h2><p></p><p>&nbsp;</p><p>台积电在华盛顿州卡马斯规划的另一家晶圆厂，在建设和施工过程中也遇到了类似的问题。尽管一期工程在1998年就首次上线，但后续由半导体专业承包商Wafertech负责的工厂增建计划一直未能成功。</p><p>&nbsp;</p><p>1996年，台积电不远万里来到美国华盛顿州的卡玛斯，与其他公司合资开设了第一家海外8英寸工厂WaferTech。2000年，台积电从其他三家合资公司手中收购了剩余股份，其持有的WaferTech股份从最早的57.23%升至近100%。</p><p>&nbsp;</p><p>根据台积电的计划，WaferTech第一座工厂会在1998年第二季度开始生产，当年底月产量达到1万片8英寸晶圆，次年底再提升到月产量3万片的满载产能。制程技术初期将以0.35微米为主，再朝0.25微米及更先进的0.18微米发展。但实际上，WaferTech直到1998年9月底才开始正式出货，到了2000年第二季度，WaferTech才转亏为盈，月产能勉强摸到了3万片的目标。</p><p>&nbsp;</p><p>这让台积电在其他国家尤其是发达国家办厂的态度谨慎了许多。去年，台积电创始人张忠谋就表示亚利桑那工厂很难找到足够的员工、建设成本超出了预期，在今年的一次活动上则直接称，美国重建国内芯片制造的努力“注定会失败”。</p><p>&nbsp;</p><p>一位前Wafertech员工称，该公司曾在一次全体员工会议上公开称美国员工都很懒。“我们感到震惊且愤怒。当时在全体员工会议上说我们很懒的是时任Wafertech总裁的Steve Tso。但高科技行业都知道这些流程的运行和操作有多严格，没有适当的程序就无法推进工作。说美国找不到能承担这部分工作的人才简直是无稽之谈。”</p><p>&nbsp;</p><p>Wafertech公司的当地代表并没有直接对以上言论发表置评，但在一封邮件中表示，Wafertech过去20年来一直是台积电大家庭中的成功一员。“内部员工会议的沟通内容应当保密，而且在我记忆中，Wafertech一直在强调每个人都应付出100%的努力以帮助整个集体取得成功。”</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.theguardian.com/business/2023/aug/28/phoenix-microchip-plant-biden-union-tsmc\">https://www.theguardian.com/business/2023/aug/28/phoenix-microchip-plant-biden-union-tsmc</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-08-31 14:44:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "一个潮流的终结？推出仅3年后，亚马逊宣布终止低代码Honeycode服务，前员工爆料：长期没有顾客！",
    "url": "https://www.infoq.cn/article/IddZNuobV3BLkWyAM3tZ",
    "summary": "<p></p><blockquote>从 2020 年低代码盛行以来，围绕低代码的争议从未停止过。如今，亚马逊宣布终止低代码 Honeycode 服务，是否预示着低代码热潮将终结？</blockquote><p></p><p></p><h2>亚马逊宣布终止低代码Honeycode服务</h2><p></p><p>&nbsp;</p><p>近日，亚马逊宣布将终止其低代码 Honeycode 服务，用户注册现已关闭，且客户的现有应用程序只能继续运行至 2024 年 2 月 29 日。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c4bd767a135c395597b46a4ba69eeb91.png\" /></p><p></p><p>Honeycode 于 2020 年 6 月发布 beta 版，作为“一项完全托管的服务，允许客户快速构建起强大的移动和 Web 应用程序，且无需编程过程”。</p><p>&nbsp;</p><p>据了解，Honeycode 应用程序的开发模型类似于电子表格，设计团队可能是基于这样的假设：对于想使用此类服务的高级业务用户来说，电子表格应该是种熟悉的操作载体。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d113c7f2b8cac1b18a5e5ece6328e0c1.png\" /></p><p></p><p>2022 年 4 月，该团队又公布了“下一代 Honeycode”，希望通过新的构建器用户界面和更好的图像支持能力进一步降低开发复杂性。但此时，Honecode 仍处于 beta 版阶段，其新模板涵盖的应用程序包括费用报告、库存系统、活动规划器、休假报告和反馈调查等。</p><p>&nbsp;</p><p>值得注意的是，这并不是 Honeycode 的完整形态，原本的不少功能反而不见了踪影。开发团队当时解释称，“在发布新的体验时，并没能提供与此前 Honeycode 经典体验相同的全部功能，这是为了尽快收集到用户的早期反馈”。</p><p>&nbsp;</p><p>亚马逊一位发言人证实了此次关闭，称“亚马逊 Honeycode&nbsp;beta 版将于 2024 年 2 月 29 日关闭，且从现在起不再接受新客户注册。我们正在帮助客户迁移至他们选定的工具，并在 2023 年 7 月 31 日之后不再向他们收取 Honeycode 使用费。”亚马逊建议那些收到影响的用户使用 Honeycode 的“导出数据”选项，并表示“我们将保留您的数据直到 2024 年 4 月 29 日。如果您不采取任何行动，您的数据将在 2024 年 4 月 30 日被删除。”</p><p>&nbsp;</p><p>亚马逊补充表示，Honeycode（RIP，2020-2024）的精神将在其其他产品中延续：“我们正在将亚马逊 Honeycode Beta 版的经验教训融入现有服务当中，并将继续致力于支持无/低代码服务，包括 Amazon SageMaker Canvas、AWS Amplify Studio 和 AWS AppFabric”。“亚马逊的 Day 1 文化使我们能够为客户快速进行实验和创新，其中部分原因就是我们为自己的服务设定了很高的标准，同时保持着强烈的自我批评精神，随时关注项目无法满足客户期望的现实迹象。我们非常感谢客户在测试期间分享的反馈意见。”</p><p></p><h2>一个早就存在的问题：Honeycode保留还是关停？</h2><p></p><p>&nbsp;</p><p>Honeycode 的关闭并非毫无征兆，甚至部分用户也早有预感。从社区论坛（包括主要文档）上的真实状况来看，Honeycode 最大的问题就是使用率有限，而这样惨淡经营的局面自然导致亚马逊必须做出一个艰难的决定：保留，还是关停？</p><p>&nbsp;</p><p>在 Honeycode 宣布正式关停之前，Honeycode 社区论坛出现过一篇名为“死或生”的帖子，用户&nbsp;Roma-d7d2 提到自己在过去的两天里热衷于从头开始构建一个应用程序，但他发现他使用的&nbsp;Honeycode 这个项目几年来一直处于测试阶段，最后一次更新是在去年 9 月，于是提出了“这个平台还活着，还是正在慢慢消亡”的疑问。</p><p>&nbsp;</p><p>在 Honeycode 社区论坛上，每个月只有少数新话题出现。到目前为止，连终止公告也只收到了 7 条评论。</p><p>&nbsp;</p><p>用户&nbsp;Eric0 表示：“感谢 Honeycode 团队的辛勤工作和支持，感谢您给用户一个迁移和备份数据的窗口。我公司的整个基础设施都是围绕 AWS Honeycode 构建的（考虑到 Hoeycode 的测试性质，事后看来这个选型有点愚蠢），因此我们需要很长时间才能迁移我们的服务和流程。我不能说我的团队对 AWS 的这一决定不感到失望，但还是表示理解。”</p><p>&nbsp;</p><p>至于关闭原因，外媒 Devclass 认为，Honeycode 的问题可能是过于关注可用性，而对功能丰富度重视不足——特别是在项目早期，它与其他服务和目录的集成度都很有限。此外，亚马逊在设计上似乎一直在倾向 IT 专家，而非真正对低代码工具更感兴趣的普通用户。</p><p>&nbsp;</p><p>Hacker News 上的一位评论者&nbsp;Honeycode_eng&nbsp;自称是“2017 年负责过 Honeycode 项目的工程师”，他表示：</p><p>&nbsp;</p><p></p><blockquote>我最初加入该项目是因为最优秀的人才蜂拥而至，最终为开发人员打造了一个前端构建器。我在那里工作时，愿景就在那里（允许人们使用电子表格技能构建应用程序），但执行却是一团糟：我们的工程师最感兴趣的是升职，所以这是超级政治性的。我记得每个团队都有自己的 redux 商店（包括一个用于导航栏、一个用于登录屏幕、一个用于主屏幕等）。它完全不起作用，但很多人得到了晋升。一直以来我们都没有一个顾客！&nbsp;如今，我对无代码这个概念非常怀疑，感觉开发的大众化趋势已经走进了死胡同。Honeycode 就像是计算机视觉中的「恐怖谷效应」，虽然看似真实有效，但却无法被应用于实际场景。Honeycode 既没有源代码控制、自定义 React 组件，也缺乏测试工具。</blockquote><p></p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/75/7537236812f06221794d9f6b72e02fe2.png\" /></p><p></p><h2>低代码软件开发是谎言吗？</h2><p></p><p>&nbsp;</p><p>从 2020 年低代码盛行以来，围绕低代码的争议从未停止过。有观点认为，低代码是 IT 革命，将“重塑整个中国软件的格局”，也有观点认为低代码是旧瓶装“新酒”，是炒作噱头而已。</p><p>&nbsp;</p><p>此前，博主&nbsp;Jay Little 曾在一篇名为《低代码软件开发是一个谎言》的文章中提到，有些低代码工具虽然能帮助一些非开发人员生成自定义逻辑和屏幕，但实际上并没有消除设计正确的数据结构、编写容错软件和验证最终软件的质量所固有的复杂性。由于许多相关编码员不具备这种专业知识，因此最终结果是一个脆弱的定制软件系统，需要团队未来成员不断进行救火。</p><p>&nbsp;</p><p>Jay Little 表示：</p><p>&nbsp;</p><p></p><blockquote>“像这样的工具并不便宜，而且它们倾向于构建许可证/计费，只要你使用通过该工具生成的软件，最终就需要付费。最重要的是，所有这些工具似乎都涉及接受某种程度的供应商锁定。因此，你在此类工具上投入的时间越多，它们对你的控制就越紧。&nbsp;在人工智能聊天机器人和低代码工具场景中，每个解决方案都承诺提供一条绕过非从业者所感知的复杂性的捷径。这就是陷阱的本质。从业者知道，代码的编写只是一个漫长过程中的最后一步，这个过程涉及大量的思考、讨论和规划。代码通常是最终结果，一旦你真正理解了手头的问题，生成代码就会相对容易。”</blockquote><p></p><p>&nbsp;</p><p>也有用户与 Jay Little 意见相左，用户 JimDabell 认为抱怨低代码开发工具的本质是，有些人认为代码是偶然的复杂性，而不是本质的复杂性。采用低代码工具可以帮助不会编程的用户做很多自己以前做不到的事情，从这个角度来看，低代码开发并不是谎言。但如果你想解决一个大而复杂的问题，即便不用考虑编程问题，也要管理大量的复杂性。这个时候，低代码工具并不是最佳选择，最好聘请专门的开发人员。</p><p>&nbsp;</p><p>“我见过一些人使用低代码工具构建非常复杂的项目，但在规模和复杂性达到一定程度后，使用低代码工具会带来更多麻烦。但这并不意味着低代码工具毫无用处——它们非常适合解决第一组问题，其中所需的代码只是偶然的复杂性。不应该仅仅因为人们在使用它们来解决不适合的问题时遇到麻烦，而放弃采用低代码工具。”JimDabell 评论道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://devclass.com/2023/08/30/muted-response-speaks-volumes-as-aws-scraps-low-code-honeycode/\">https://devclass.com/2023/08/30/muted-response-speaks-volumes-as-aws-scraps-low-code-honeycode/</a>\"</p><p><a href=\"https://honeycodecommunity.aws/t/dead-or-alive/28334\">https://honeycodecommunity.aws/t/dead-or-alive/28334</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=34008506\">https://news.ycombinator.com/item?id=34008506</a>\"</p><p><a href=\"https://jaylittle.com/post/view/2023/4/low-code-software-development-is-a-lie\">https://jaylittle.com/post/view/2023/4/low-code-software-development-is-a-lie</a>\"</p>",
    "publish_time": "2023-08-31 15:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "平安科技研发管理部副总工程师毛倩影，确认担任 FCon DevOps 在金融企业落地实践专题出品人",
    "url": "https://www.infoq.cn/article/Yh2rMGptZSN8fxpZWEsL",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。平安科技研发管理部副总工程师毛倩影将担任「<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=article\">DevOps 在金融企业落地实践</a>\"」的专题出品人。在此次专题中，你将学习到在金融行业落地 DevOps 的经验，为公司落地 DevOps 指明方向。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=article\">毛倩影</a>\"，深耕研发管埋领域 20 年＋，平安自研 DevOps 平台负责人，负责推动集团研效提升，BizDevops 业研一体及研发数字化转型等重大项目，并主导超过 10 家大型金融企业的 DevOps 项目合作。</p><p></p><p>相信毛倩影的到来，可以帮助提升此专题的质量，让你了解到，目前 DevOps 已经被越来越多的金融企业所采用，来支撑软件生产过程的数字化转型，以及金融行业落地 DevOps 的经验，帮助企业找到适合的 DevOps 实践之路。</p><p></p><p>除上述专题外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等专题进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：13269078023（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-08-31 16:06:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ECS倚天实例编程语言适配教程",
    "url": "https://www.infoq.cn/article/i6vDiLm13cNzaL6eeeA4",
    "summary": "<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/40/dc/40de9e7f03517df6a2ccfd10c4e0d2dc.jpeg\" /></p>",
    "publish_time": "2023-08-31 16:51:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]