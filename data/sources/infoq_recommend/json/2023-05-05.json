[
  {
    "title": "阿里、蚂蚁、酷家乐为你分享业务出海实践｜QCon",
    "url": "https://www.infoq.cn/article/hjUIzz29SzfH0QC3JjuP",
    "summary": "<p>技术出海的架构演进路线？面向海外复杂监管环境下的业务可扩展性如何设计？如何利用云基础设施建设全球化架构？等等一些关于出海的技术问题，是需要正在谋求海外发展的企业所关心的。</p><p>&nbsp;</p><p>为了给你提供更多可参考的案例，在 <a href=\"https://qcon.infoq.cn/2023/guangzhou/schedule\">QCon全球软件开发大会（广州）站</a>\"，InfoQ策划了【出海的思考】专题，以下为专题内容介绍：</p><p>&nbsp;</p><p>本专题我们邀请了蚂蚁安全科技技术负责人<a href=\"https://qcon.infoq.cn/2023/guangzhou/track/1510\">姚伟斌（文景）</a>\"担任专题出品人，为专题内容的质量进行把控，他现负责蚂蚁业务安全技术能力出海战役，曾任阿里云 CDN 技术负责人、Tengine 开源项目负责人。在本专题下，我们邀请了三位技术专题为你分享：</p><p>&nbsp;</p><p>首先，我们邀请了蚂蚁集团大安全技术部认证技术架构师张放，他将以《海外业务 1 年暴增 20 倍，ZOLOZ 是怎么做到的》为主题展开分享。据张放介绍，随着互联网金融在东亚、东南亚等地区的快速发展，如何对终端用户进行高效的 KYC 成为了困扰电子钱包等行业发展的一大痛点。蚂蚁大安全孵化出来的 ZOLOZ 可信身份平台应运而生，但是ZOLOZ 在实现技术出海的过程中，遇到一系列全新的技术挑战，例如，一国一策复杂多变的监管环境对于业务可扩展性的要求，统一业务架构与高度异构的部署环境之间的尖锐矛盾，高敏感数据如何在公有云 SaaS 内合规的流转、处理和存储等等。本次演讲，他将就这些问题展开细致的分享；</p><p>&nbsp;</p><p>其次，我们邀请了阿里云高级合规专家廖智杰，他拥有十六年信息安全从业经验。在阿里云负阿里云国际站云产品安全合规、数据隐私合规。拥有丰富的数据安全合规经验，帮助企业在云上构建更安全的体系，助力业务合规开展。他将以《企业出海合规及云上数据合规实践》为主题进行分享，通过他的分享，你可以了解到出海数据合规常见问题、以及了解云上数据合规实践；</p><p>&nbsp;</p><p>最后，我们也邀请了酷家乐云原生中间件资深技术专家<a href=\"https://qcon.infoq.cn/2023/guangzhou/presentation/5283\">王栋年</a>\"，他目前担任群核科技（酷家乐）中间件团队负责人，期间完成了多款中间件的开发建设以及升级换代。成功主导了多云架构、全球化基础架构等的设计以及落地实施。他将围绕出海面临的技术挑战，结合酷家乐海外业务拓展过程中遇到的问题，以酷家乐的技术和架构迭代为例，为大家分享如何通过演进式架构支撑业务的快速出海和稳定运行等。通过他的分享，你可以了解到多数据中心下的关键技术实现细节、以及如何利用云基础设施建设全球化架构。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/509f04f4c98f08886f5dc0a15ac69a2f.png\" /></p><p></p><p>&nbsp;</p><p>更多精彩议题持续上线中...</p><p>&nbsp;</p><p>活动推荐：</p><p>&nbsp;</p><p>在5月25-26日，QCon全球软件开发大会（广州）站即将落地，在此峰会上，共有十二个专题，近五十余场分享。其中包括稳定性即生命线、编程语言实战、DevOps vs 平台工程、AGI 与 AIGC 落地、下一代软件架构、数据驱动业务、出海的思考、云成本优化、现代数据架构、AIGC浪潮下的效能智能化、新型数据库、大前端技术探索。欢迎与你一起交流，更多详细信息可扫描下方海报二维码了解。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44286d1374db883a04a6acbda9c6bd2f.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-05-05 11:43:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国信通院金融数字化探索: 用技术搞定中小企业融资、绿色双碳、流量反欺诈等棘手问题",
    "url": "https://www.infoq.cn/article/wMayrlSP8IHzWBTdaROI",
    "summary": "<p>互联网连接了人，物联网连接了物。如今，金融机构的产品服务正沿着这两类庞大网络延伸到个人衣食住行以及企业经营的各个生态场景，以数字金融形态赋能实体经济的发展。</p><p></p><p>在最新一期的 InfoQ《<a href=\"https://www.infoq.cn/article/OuAe5PBxryakC5IliQru\">超级连麦. 数智大脑</a>\"》直播中，中国信通院泰尔终端实验室数字生态发展部主任王景尧博士介绍了中国信通院在金融数字化转型领域的探索，围绕金融机构如何利用物联网、大数据等数字技术实现金融产品和服务模式的创新，以及在这背后，金融机构基础 IT 能力如何进行迭代升级等内容进行了分享。</p><p></p><p>以下是分享全文，经 InfoQ 编辑整理（点击链接观看完整<a href=\"https://www.infoq.cn/video/rOARbiN1flkQTADUpBdc\">直播回放</a>\"）：</p><p></p><h3>通过物联网金融数字化平台解决中小企业融资问题</h3><p></p><p></p><p>我们首先介绍一些物联网金融相关的场景，这些场景主要是针对中小制造企业而设计的。</p><p></p><p>中小微企业融资，尤其是实体制造中小企业融资难、融资贵一直是金融行业的难点，也是国家非常关注的问题之一。对于这些企业来说，最大的问题是难以提供有效的抵押物来获得银行融资。</p><p></p><p>在这种情况下，我们通过大量使用物联网设备，可以帮助中小企业以无抵押的方式获得金融支持，从而帮助更多的制造业中小企业开展相关业务，促进企业发展。通过物联网设备监控中小企业的核心设备，如设备手环等，了解企业的开工率和生产经营状况。不同企业、不同行业的开工率反映了它们的生产经营情况，通过分析这些数据，可以了解企业的生产经营状况，降低银行贷款的金融风险。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/1184f69c2cf6e19bc4fc52c1ed23ec7e.png\" /></p><p></p><p>具体来说，在贷前，针对中小企业经营不透明、材料不规范的情况，可以通过设备监控了解中小企业的生产经营情况，用更低的成本找到可以授信或者放贷的中小企业客户；</p><p></p><p>在贷中，可以降低银行对企业的尽调和勘察成本，因为中小企业单笔贷款比较小，反复勘察给银行带来的成本压力比较大，而通过物联网手段就可以减少银行人员尽调的工作；</p><p></p><p>在贷后，还可以通过监控设备了解企业设备的残值以及设备的使用情况，帮助银行做更好的贷后资产管理和不良资产处置，从而提高银行在放贷过程中的投入产出比。</p><p></p><p>去年，中国信通院与平安银行合作成立了泰尔-LAMBDA 实验室，通过支持<a href=\"https://www.infoq.cn/article/qZm63L9zkiD57cpFMsv7\">金融场景</a>\"的应用，已经支持实体经济融资超过 6,500 亿。我们的方法就是通过设备数据的采集，包括交易数据和经营数据的流转，来实现两方面的工作。</p><p></p><p>一方面，可以对企业的经营生产行为进行预测，实时监控后期金融处置，包括抵押物设备残值和资产价格分析，从而降低贷款业务风险和银行开展成本，提高中小企业融资投入产出比。</p><p></p><p>另一方面，通过设备大量部署，采集生产经营数据，可以帮助企业提高生产效率，了解生产经营情况，帮助企业更好地增信并开展二次放贷业务，更好地利用银行的企业资源。</p><p></p><p>此外，我们还与平安银行联合发布了《物联网金融研究报告》，并成立了“星云开放联盟”，旨在促进金融发展，推动中小企业发展，寻找资源和客户合作，通过输出金融能力，支持中小企业获得更好的金融支持。</p><p></p><h3>通过绿色金融数字化平台助力双碳战略落地</h3><p></p><p></p><p><a href=\"https://www.infoq.cn/article/xlshY7xzPkVyq5rK4BtT\">绿色金融</a>\"是我们近期拓展的重点之一。随着国家发布一系列双碳政策，绿色金融也成为企业的创新重点之一。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b8c3966acdf81a877cf039b79af4ab1a.png\" /></p><p></p><p>对此，中国信通院推出了绿色金融和治理平台（中国信通院探索绿色监测技术和平台），通过部署物联网设备，可以帮助中小企业更好地管理能源使用，实现节能减排。</p><p></p><p>中小企业在使用电力时面临很大的成本压力，因此我们在平台上进行了峰谷电价的差别制定，并帮助中小企业预测其生产经营情况，以更好地利用碳价的电价波动来节约成本。在许多省份中，使用绿色能源可以获得绿色证书，这些证书又可以用于获得碳排放交易指标。这些指标是可以进行交易的。</p><p></p><p>我们看到，在整个系统中，一方面我们可以帮助企业利用碳价、电价的波动来节约成本；另一方面，还可以让更多的中小企业使用绿色能源。</p><p></p><p>绿色能源的供应受环境影响非常大。例如，太阳能和风能都受到天气条件的限制，特别是在没有风的情况下，风能就无法产生电能。因此，大型企业很难大量使用绿色能源。但是，由于中小企业的生产经营灵活性，它们可以提前预测其生产经营需求，并在使用绿色能源时更多地使用绿色能源。</p><p></p><p>这样做有两个好处：首先，绿色能源的价格相对较低。其次，使用绿色能源的同时也可以获得绿色证书，这些证书可以作为质押物，作为节能自愿减排的碳配额，帮助企业获得更多的质押物，从而实现更好的融资。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6d2355db2aabf9fd71f187822143f6cf.jpeg\" /></p><p></p><p>总而言之，我们的绿色金融探索包括两个方面：一是碳排放交易的回购和质押融资；二是通过碳排放监测系统帮助企业积攒碳资源，并让政府更好地了解本地碳排放情况，从而促进本地企业的发展。</p><p></p><h3>基于个人业务数字化平台赋能金融服务创新</h3><p></p><p></p><p>除了对公业务以外，中国信通院在个人业务方面也做了一些数字化创新。例如客户分层场景，<a href=\"https://www.infoq.cn/article/6YjkNdjohVMLBYeM9j6G\">银行</a>\"普遍存在的问题是大量存量客户如何进行运营。很多客户在办卡后可能不会长期使用银行卡，但他们仍然是银行宝贵的资源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be1be6216adad6721e1b52a8bc2f366e.png\" /></p><p></p><p>在当今互联网流量见顶的情况下，新客户拓展变得异常困难且成本高昂。因此，充分利用存量客户资源来提升效率，对金融机构而言是极具价值和意义的工作。</p><p></p><p>对此，我们构建了总行和省分行的整体架构。借助数据和分层能力，总行可以更好地识别有意愿或目标的客户，然后将客户拓展和相关工作交由分行来执行。这将有助于提升分行和地市银行的效率，因为本地许多客户是通过分行或本地银行的人员开拓的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c3/c3be40894c09a762b5ad02b31299d89c.png\" /></p><p></p><p>在有限的人力资源和时间下，如何找到最高效率的方法开拓目标客户，是所有银行都需要解决的问题。我们通过这套架构的设计和外部数据的引入，可帮助银行提升本地人员的效率，在单位时间内开拓更多客户，从而提高银行整体收入和投入产出比。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5dc746e1ff1aaf2c4c3a38013751486.png\" /></p><p></p><p>除此之外，我们还在移动互联网的个人业务方面进行了创新。中国信通院和中国互联网协会利用自身资源，在反欺诈、统一推送和移动 ID 服务方面做出了很多工作。目前，我们的工作已经覆盖了国内至少超过 15 亿的设备。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/250a4b6abe28af4f7ecfa6c5468e7652.png\" /></p><p></p><p>首先，金融流量反欺诈是所有行业面临的重要问题之一。今天的客户营销和数字经济的核心在于如何更好地利用数据要素来开源节流，数字化转型可以通过数字方式来开源，主要是通过在线上引入流量。然而，互联网流量中存在很多假量，这些流量背后并不是真人，这导致金融机构和银行在在线上活动中浪费大量资金，导致整个金融效率下降，并且被人恶意利用。因此，我们提供了不同的解决方案来帮助金融机构提高流量投放效率，避免流量欺诈。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/95c762ffbd4f8dbe4e10ac9ef2e2f66a.png\" /></p><p></p><p>这些解决方案包括对设备、IP 和用户行为进行验证，以识别整个流量中的假量。例如，行为验证可以检测到用户在短时间内在不同地点的多次点击等异常行为，我们就可以抓取这些行为并验证用户是否真实存在。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2e683e8534f80dd681cbe5ed50d1ea4c.png\" /></p><p></p><p>此外，我们还利用运营商的能力，通过网关取号的方式实现本机号码校验和设备一致性校验，从而更好地保护个人资金安全。在用户登录时，判断号码是否属于用户本人，以及设备是否为用户常用的手机设备，为金融机构提供更好的保护措施。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cfa0d1a4897e22fb48d20de95ef2abab.png\" /></p><p></p><p></p><p>自 2007 年苹果发布第一代 iPhone 以来，我国信息化智能终端已经发生了巨大变化，移动互联网的生态也随之改变。然而，在传统行业中，比如金融行业，大部分营销仍然采用短信或电话的方式，这种方式不仅不友好，而且无法实现很多复杂功能。因此，在移动互联网时代，如何更快地为用户提供服务，而不仅仅是通过电话触达，是所有银行都面临的重要问题。</p><p></p><p>在这个场景中，我们实现了许多功能，其中包括与三大运营商合作开发的“推必达”技术。这种技术的核心是使用短信通道发送 push 消息，通过 push 消息来拉起快应用。即使用户没有安装 APP，也可以享受到服务，这在许多场景下非常有价值。</p><p></p><p>例如，在<a href=\"https://www.infoq.cn/article/29uvshLBl6ZIeuxTdNOx\">保险</a>\"场景中，当我们在外地出险时，很难通过电话告诉勘验人员我们的出事地点。这时，使用定位方式告诉保险公司我们的位置信息更加方便。然而，在电话中上传位置信息并不容易。因此，我们通过“推必达”技术，使用下发的 push 消息和短信通道来实现这一功能。只需点击一下即可获得即点即用的服务，提高用户体验。</p><p></p><p>我们知道，金融行业本质上是一个服务业，如果我们能更好地为用户提供服务并提高用户体验，就可以帮助我们吸引更多的业务。当然，这个方案不仅适用于线上场景，许多线下场景也可以使用。</p><p></p><p>例如，在银行柜台，我们可以通过一条推送消息向我们的手机发送服务信息，从而避免排队等待的痛苦。同时，由于我们的消息是发送到注册号码的持有人，因此无法转发，这保证了点击的准确性和流程的安全性。这种创新不仅提高了我们的工作效率，而且大大增强了数据的保护。</p><p></p><h3>基础 IT 能力的数字化创新</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/58/58bfbb779a2c3a1219848e3a82a7f3bb.png\" /></p><p></p><p>虽然传统的数据<a href=\"https://www.infoq.cn/article/9FRdGfIdRe3OIoygTBrt\">中台</a>\"是许多企业采用的技术，但是这种架构相对臃肿。为了在业务快速部署方面提供更好的解决方案，我们提出了“微中台”的概念，它可以涵盖传统中台的技术模式或技术壁垒。特别是在分支机构和非总行等场景下，通过“微中台”架构，可以快速打通业务数据，为新业务的开展提供更好的模式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97282cebf6ef449af938eadbb4aca91b.png\" /></p><p></p><p>我们使用微中台集成外部数据资源，为金融行业的营销用户洞察、渠道优化、产品创新和运营提升提供更好的数据支持，以更好地了解客户画像。</p><p></p><p>金融行业是一个数据处理信息的行业，通过数据信息获得生产力，数字化转型的核心是提高数据使用效率。然而，基于传统中台架构，业务的上线时间和构建时间都较长，可能会对业务开展造成影响。在需要快速上线新业务的场景下，我们需要更新的 IT 架构来提高用户和业务的流畅性和敏捷性做到极致。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/8364d3d48baec6a675fef0de3bade561.png\" /></p><p></p><p>同时，我们还发现，金融机构所需的数据不仅是本地数据，还需要外部数据建模。因此，我们采用分布式数据库架构来打通本地数据和外部数据。数字化金融的目的是做更多的客户洞察、业务优化、宣传营销和竞品分析，这都需要大量的外部数据洞察。对于银行来说，了解客户和竞争对手的信息都是有限的，因此在进行营销和客户分析时，需要大量的外部资源。那么如何获取这些外部资源呢？</p><p></p><p>在许多场景下，获取外部数据不仅需要在短时间内获取数据，还需要在短时间内将其应用到业务中。例如，在进行广告投放时，需要在 200 毫秒内决定是否向用户投放广告并进行竞价。这样短的时间内，如果采用隐私计算等方式，则无法满足实时性要求。因此，我们通过分布式数据库架构将外部数据和本地数据集成，同时与山东数据交易所共同构建了卓信大数据联合建模中心。</p><p></p><p>通过这种分布式数据库，可以在短时间内实现数据交互和打通，满足高市值业务的需求，并更好地承接外部资源，实现数据资源的流转。对于一些非实质性的业务，比如授信业务、风控业务以及竞品分析业务，我们认为它们也可以在很大程度上帮助我们更灵活地实现这些业务。通过使用原生的 SQL 技术，我们可以实现数据的打通、融合和分析，甚至可以在 IT 架构的更底层完成这些功能，从而极大地提升了数据要素的价值和效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/27ca206a51ca1e0f2f715bac35bdf03a.png\" /></p><p></p><p>上图是卓信数据库的一个介绍。通过原生外部资源、原生隐私计算、原生数据安全和原生数据治理这四个原生工作，我们可以帮助金融机构和更小、更灵活的业务开展这些业务，甚至像分行或本地机构这样的面向本地的业务也可以更快地运转起来。</p><p></p><p>我们相信，这也是未来金融数字化转型的一个重要方向。</p><p></p><p>关注「InfoQ 数字化经纬」公众号，输入 “信通院金融”便可下载本次演讲的 PPT。&nbsp;我们将持续为您推送更多、更优质的数字化案例内容和线上线下活动。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/d3/da/d31d4e0e474feb853493fb404102b3da.png\" /></p><p></p>",
    "publish_time": "2023-05-05 12:11:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "CubeFS 在大数据和机器学习的探索和实践丨ArchSummit 峰会实录",
    "url": "https://www.infoq.cn/article/l6epCLrUwpGVr52UUOr7",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/66/6674afe25a3b56fd8fc397753968329e.png\" /></p><p></p><p>近日，AS 全球架构师峰会上海站圆满落地。会上，来自<a href=\"https://www.infoq.cn/article/BtUQsMtHu7mOc7lEqhK0\"> OPPO 安第斯智能云</a>\"的唐之享围绕云原生分布式存储 <a href=\"https://www.infoq.cn/article/xDMtUJC6EwCKhef6KKWK\">CubeFS</a>\" 在<a href=\"https://www.infoq.cn/article/rPLFkwPeX27aYbJlwEKa\">机器学习</a>\"和大数据的探索和实践这一主题展开了精彩分享，以下为本次分享的精华内容。</p><p></p><p>本次分享主要从以下四个方面进行：</p><p></p><p>CubeFS 的架构设计和关键产品特性；CubeFS 在机器学习领域的应用和实践，详细说明 OPPO 机器学习存储的演进过程和遇到的问题和挑战，以及如何基于 CubeFS 应对这些问题和挑战；CubeFS 在大数据的应用和实践；展望CubeFS的未来演进方向。</p><p></p><p></p><h2>CubeFS 简介</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43def96a629edd959b2fc39c1b85bde1.png\" /></p><p></p><p>CubeFS 是云原生计算基金会（CNCF）的新一代云原生开源存储产品，可以提供完整的文件和对象能力，目前处于孵化阶段，技术团队正在积极准备毕业相关事项。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/84/843b3df5ace594ab0eb5e44d08892bce.png\" /></p><p></p><p>CubeFS 主要分为资源管理模块、元数据子系统、数据子系统以及多协议客户端四个大的模块。其中资源管理模块（Master）负责管理数据节点和元数据节点的存活状态、创建和维护卷 (volume) 信息、元数据分片（metaPartition，简称mp）和数据分片(dataPartiton，简称dp）相关的创建和更新。Master 由多个节点构成，通过 Raft 保证服务的高可用。</p><p></p><p>这里涉及到一个概念——volume，这个是虚拟的逻辑概念，对于文件系统来说，volume 是一个挂载的文件系统；对于对象存储来说，volume 则是对应的一个 bucket。每个 volume 存储用户的数据以及对应数据的元数据，其中数据存储在数据子系统中，可以是多副本引擎的数据分片或者是纠删码引擎的条带中。</p><p></p><p>元数据保存在元数据节点的 metaPartition 中，元数据的分片是 CubeFS 的一个设计亮点。实际情况中 MetaNode 和 DataNode 可以同机部署，因为一个消耗内存资源，一个消耗是磁盘资源。</p><p></p><p>除了数据和元数据子系统之外，还有多协议的客户，客户端可以兼容 S3、HDFS 和 POSIX 协议。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e3f386d955792698f089cde5221469d.png\" /></p><p></p><p>CubeFS 的元数据设计是一个亮点，文件系统的元数据管理方式决定了系统的扩展性和稳定性。比较常见的元数据管理方式是静态子树，类似 HDFS 和 CephFS，CephFS 的单机模式元数据节点只能支持10亿到20亿的元数据，元数据节点会成为瓶颈，而集群模式容易出现热点目录，需要手动运维将热点目录进行拆分。除了静态子树的方式之外，还有 hash 分片的策略，但 hash 分片在扩容新节点时候会面临元数据的迁移，业务对元数据迁移有感知。CephFS 的动态子树是一个比较完善的方案，但由于其实现复杂度较高、稳定性不足，在生产环境很少使用。</p><p></p><p>元数据系统设计需要解决的关键问题是如何将庞大的元数据进行拆分，并且拆分后的元数据分片尽量均衡，由多个元数据节点共同存储和承担访问负载。唐之享在此详细介绍了 CubeFS 的元数据设计方案。用户数据存在 volume之中，每个 volume 对应多个 mp，每个 mp 负责一段范围的元数据，比如mp0负责[1-10000]，mp1负责[10001-20000]，mp2 负责[20001-正无穷]，这个正无穷指的最后一个 mp 的最大元数据没有上限，此处之所以这么设计，是因为最后 mp 支持分裂，当最后一个 mp 所在 MetaNode 节点内存使用率达到一个阈值，会把最后一个 mp 分裂成两个 mp，新的 mp 会根据内存权重分配到可用内存更多的 MetaNode 节点上，以此完成元数据的扩展，整个过程无需迁移任务数据，对业务无感。</p><p></p><p>元数据包含 inode 之外，也会保存 dentry 信息，dentry 记录是（parent_id ，name）到 inode 的索引，需要注意的是 dentry 会和其父目录的 inode 保存在同一个 mp，这样同一个父目录下的所有子文件存在一个分区，遍历该目录只需要访问一个元数据分区就可以获取数据，避免访问整个集群来获取数据。</p><p></p><p>MetaNode 之间通过 multi-raft 保证数据高可用和数据一致性，每个节点会有多个 mp，不同 MetaNode 上 mp 组成一个 raft-group 组，元数据都是保存的内存中，通过定期快照和 Raft 的 WAL日志保证高可靠性，具体而言，MetaNode 每五分钟做一次快照，五分钟间隔内有变动的元数据操作会先持久化 WAL 日志，节点故障或者重启后会通过快照+重放 WAL 日志的机制恢复所有的元数据。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/56c2cbd0e5b5ab4441298d39b63f75e0.png\" /></p><p></p><p>CubeFS 的数据子系统分为多副本引擎和纠删码引擎，多副本引擎支持两种协议，顺序写请求采用主从复制协议，这样可以优化 IO 吞吐；随机写采用 multi-raft 协议。大文件会采用分片存储，将大文件按照128KB拆分后并发写入到不同 dp 中。dp 由 normal extent 和 tiny extent 组成，大文件分片写入 normal extent 中，小文件采用聚合的方式写入一个 tiny extent 文件中，元数据会记录小文件在聚合文件内的偏移，这样可以有效减少 DataNode 维护的文件数目。</p><p></p><p>删除数据的空间回收是基于文件系统的 punch hole，可以避免空间回收需要的逻辑到物理映射，有效提升空间回收效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/48cf9b7aaa5e09bd97b7a0a1613a587b.png\" /></p><p></p><p>纠删码引擎提供低成本、高可靠的在线纠删码存储能力，数据写入直接在客户端编码写入存储节点，无须将数据先聚合到一个临时的多副本系统，然后再异步迁移到纠删码存储，可以避免数据多次迁移导致的流量浪费。元数据为服务基于Raft 做秒级切换保证一致性和可用性。后台服务定期做数据巡检、坏盘检修、数据均衡等任务来保证数据高可靠。不同模式的编码支持1、2、3AZ 的部署，多 AZ 部署方式支持 AZ 级别容灾。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d0c63d167aa7be6a39ed6fa31280ed1.png\" /></p><p></p><p>客户端支持 S3、POSIX 以及 HDFS 多种协议，通过一套系统做到完美融合，多协议之间共享一套元数据及数据，用户可以直接通过 S3 协议读取通过文件协议写入的数据，反之亦然。数据的统一存储可以提高数据的复用效率，一份数据多处访问，不同业务之间提供租户级别的隔离和租户级别的 QOS，可以最大化提升存储的利用率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/86a6c61579f7aed6a2fe6fb2c8d1e865.png\" /></p><p></p><p>总结一下，CubeFS 是开源的分布式存储产品。</p><p></p><p>提供多协议兼容 S3/POSIX/HDFS 等多种协议。支持纠删码和多副本引擎，用户可以根据实际情况选择合适的存储引擎。其优秀的水平扩展能力，可以帮忙用户快速构建 PB 甚至 EB 级别存储。元数据全内存缓存和多级缓存技术提供高性能存储。CubeFS 还支持多租户管理，可以提供细粒度的租户隔离策略，保证不同用户之间的数据安全和隔离。除此之外，CubeFS 还提供了基于 CSI 插件的快速部署方案，可以方便地在 Kubernetes 上使用 CubeFS。</p><p></p><p></p><h2>机器学习的应用</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bdb18a0075ee969755fcce2eeb7b4222.png\" /></p><p></p><p>OPPO 的机器学习存储主要分为四个阶段：</p><p></p><p>第一阶段使用 CephFS 作为存储；第二阶段使用 CubeFS 和 CephFS 混存；第三个阶段单独使用 CubeFS 存储；最后一个阶段使用 CubeFS 存储+多级缓存技术。</p><p></p><p>接下来具体看看每个阶段遇到的问题和挑战，以及如何应对这些挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4bdc302e224ddd53ca2e962de2bf41bb.png\" /></p><p></p><p>第一阶段使用 CephFS 存储集群学习的数据，这个阶段存储节点数目在150台，磁盘1500块左右。由于 MDS 采用的是主备模式，无法水平扩容，单个 MDS 承受10亿级别的元数据访问，节点负载过高导致 MDS 时延上升，训练的 IO 吞吐下降，大量 GPU 训练的利用率低。MDS 在稳定性方面也存在问题，用户频繁的超大目录遍历导致 oom，服务的恢复周期较长。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4b/4bdc302e224ddd53ca2e962de2bf41bb.png\" /></p><p></p><p>最直接的解决方案是分而治之，将大的 CephFS 集群拆分成6个小集群，保证每个集群规模控制500块盘以内，小集群模式在稳定性确实有提升，但是小集群模式并不是真的小而美。首先小集群模式的存储资源利用率不高，通常需要将存储水位控制在70%左右来应对突发的业务增长，其次面对百亿参数的大规模机器学习训练，小集群无法满足高 IO 的吞吐要求。这个阶段 CubeFS 技术团队也开始了灰度和验证 CubeFS。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/193d734caa1f7ba3e60efc5886eaba61.png\" /></p><p></p><p>总的来看，这一阶段机器学习存储的主要特点是海量小文件、超大目录热点目录以及访问时延敏感。在经过一段时间验证了 CubeFS 的稳定性、扩展性以及性能方面都能满足机器学习的存储需求之后，最终使用 CubeFS 作为统一存储。依靠 CubeFS 可扩展的元数据服务，元数据节点不会成为单点瓶颈，用户的元数据会均匀分配到不同 MetaNode 节点，由所有 MetaNode 节点共同承担，有效地解决热点目录的问题。最终将机器学习超过70亿的文件数量，总存储量超过30PB的数据全量存储在 CubeFS 中，SLA 也从3个9提升到4个9，元数据的访问时延从10ms降低到1ms，全年稳定运营无故障，为后续机器学习的高性能存储打下基础。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6e371d1686f7fd33bc94c07d8a7f256b.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/38b105d3218a1453c7bb686a3f442055.png\" /></p><p></p><p>第四阶段，进入混合云计算阶段，这个阶段对混合云弹性计算的需求主要是为了合理利用资源，降本增效。这一阶段会在 OPPO 私有云维护常态化的 GPU 算力水位，而应对突发的算力需求，采用公有云的 GPU 算力，通过这种混合云的弹性计算来节约计算成本。但是这也带来了一个挑战，由于公有云机房到私有云机房的专线时延是 2ms，导致公有云训练的时延比私有云的效果差两到三倍。</p><p></p><p>为了满足弹性计算的需求，CubeFS 技术团队提出了几种不同的解决方案：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c5aa00bbc5a4938a8e7d68b713ce5e65.png\" /></p><p></p><p>方案一：将数据存储在公有云的文件系统中，公有云的训练访问公有云的文件系统，以此来减少机房之间的时延。这种方案抛开昂贵的数据迁移代价不谈，还存在以下问题，数据是全量迁移还是部分迁移，如果全量迁移数据，公有云已经有全量的数据，无法做到弹性计算；如果是部分迁移，私有云的 CubeFS 和公有云的文件系统存在数据一致性需要解决；另外考虑终端用户的数据隐私安全问题，将数据保存公有云可能会产生数据安全合格风险。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/ccdd9f8879f5af09de54706679b24683.png\" /></p><p></p><p>方案二：在公有云部署一套 CubeFS 文件系统，该方案除了存在方案一的相关问题之外，由于GPU的云盘空间有限，还需要额外购买裸金属服务器来部署 CubeFS，增加存储成本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e5288e987952350906912d8ca9c72c0.png\" /></p><p></p><p>通过深入了解集群学习的训练过程的特点，发现大规模 AI 训练的 IO 有以下特点，每一轮迭代 epoch 会反复读取同一批数据，通常单次训练会跑上万轮。总的来说，AI 训练的 IO 特征就是在某个训练集的反复并且多次读取的一个过程。基于这个特征，利用 CubeFS 作为统一存储结合多级缓存的方案非常适合。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/060ca25f135cb1cd6551f38d3fbaa72a.png\" /></p><p></p><p>第一轮训练将数据从私有云 CubeFS 加载到公有云的缓存节点，客户端会缓存元数据 inode 和 dentry 信息，可以大量减少训练过程使用 fuse 客户端的 loopup 和 open 操作的元数据查询延时开销，并且元数据缓存可以指定缓存文件数量，最大可以支持千万级别的文件。GPU 的云盘（通常是1TB）可以作为数据缓存盘，通过指定缓存目录和配置 LRU 策略，无须申请额外资源就可以缓存数据。通过缓存加速策略，RESNET18 模型下 dataload worker分别是1和16的时候，整体性能提升了360%和114%，即使相比私有云的训练也有12-17的性能提升。</p><p></p><p></p><h2>大数据的应用</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6033d9f9d7c5c4270f98bfa25cc215d4.png\" /></p><p></p><p>大数据的存储过程也可以分为四个阶段：</p><p></p><p>第一阶段是 HDFS 存储，这一阶段主要面临的是存储成本和运维复杂高等问题；第二阶段是使用对象存储做降冷，来解决 HDFS 集群高成本的问题，但是由于对象存储不支持文件语义，在 list 等操作时候代价较高；第三阶段使用 CubeFS 来承接冷数据；第四阶段阶段是使用 CubeFS 作为统一存储。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/577bde15c220836ba261c2fd3cea8aae.png\" /></p><p></p><p>大数据最开始使用 HDFS 存储也面临了一些挑战。</p><p></p><p>首先是 HDFS 集群数目多，如下图所示，只是大数据业务的一部分 HDFS 集群，除了集群数量多之外，多个集群的存储空间资源紧张，需要集群间不断腾挪机器来满足日益增长的存储需求。并且&nbsp;HDFS 集群采用的是存算混合机型，这种机型单位存储成本高、能耗大，所以这一阶段面临的主要是存储成本过高、集群管理复杂的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c608dd9ad56541707f720028bf97ad0.png\" /></p><p></p><p>第二阶段主要采用对象储保存大数据的冷数据，这个阶段会将大数据的冷数据迁移到对象存储中，依靠对象存储的低成本优势来解决大数据业务面临的成本问题。但是对象存储承担大数据的冷数据有个天然的问题，就是不支持文件语义，业务的 list 和 rename 操作时候代价非常高昂，rename 操作需要先对数据做 copy 然后再删除旧的数据，整个过程代价极高。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d45ea135afb86d4c50e628d2cfa747aa.png\" /></p><p></p><p>第三阶段是基于 CubeFS 的来存储大数据冷数据，CubeFS 不仅能够提供低成本的存储，本身也支持文件语义。目前已经使用 CubeFS 存储超过100PB的大数据冷数据，整体存储成本比使用 HDFS 节约40%以上，即使比使用对象存储的成本也有所下降，并且整个降冷过程更快、更节约资源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9cf314eaf68eab690792fd94a7cdfe5d.png\" /></p><p></p><p>最后一个阶段是使用 CubeFS 作为统一存储，冷数据采用 CubeFS 低成本、高可靠的纠删码引擎，热数据采用 CubeFS 三副本引擎。CubeFS 统一存储可以支持更大 IO 并发需求，例如 Flink 的 check point 集群，需要定期将任务持久化到存储，会产生很多频繁大 IO 请求，小规模的 HDFS 集群需要靠扩容解决，导致集群整体存储利用率不高，存储成本增加，而使用 CubeFS 统一存储，可以提升整体存储利用率并且能够满足大 IO 的要求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/dea07cbe974c99cda11310bcc0c3f383.png\" /></p><p></p><p>大数据业务经历了这个四个阶段的存储演进，简单来说，大数据存储目前的需求就是以下几点。最核心的需求就是降本增效，这个也是目前很多公司的关键目标，在降本增效的同时需要保证系统的可用性、数据的可靠性以及运维的便捷性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/0819976bd1e015b7bf505547c4066be6.png\" /></p><p></p><p>关于 CubeFS 如何助力大数据降本，第一个策略是从数据冗余度出发， CubeFS 本身提供了弹性可变的副本机制，用户可以根据业务特性选择特定数量的副本数目。举个例子，大数据的 Shuffle 业务产生的是临时数据，这个业务场景很适合采用单副本存储来节约存储成本。</p><p></p><p>除了弹性副本之外还可以采用低成本的纠删码，不同冗余度的编码支持可配，用户可以根据对数据耐久度的需求来选择合适的编码，例如可以选择支持AZ级别容灾的编码，在降低数据冗余度的同时兼顾数据可靠性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e4/e43112cfc689d29b2c9dc2b55d2ef3f4.png\" /></p><p></p><p>除了软件层面的降本之外，CubeFS 技术团队还在硬件层面做了降本优化，这里主要是选择一些高密的存储服务器，高密存储服务器单位存储量的成本和功耗都更低，整体的存储成本也更低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/75193b13a94f9cd019fbc5de457c55ab.png\" /></p><p></p><p>除了节约存储成本之外，CubeFS 技术团队也特别关注大数据存储的性能，通过多级缓存技术可以在 Client 节点上同机部署 BlockCache 组件，在内存缓存元数据，利用本地磁盘来缓存数据，元数据和数据就近访问可以提升数据的读性能，当然由于本地磁盘容量有限，需要配置一定的缓存淘汰策略。</p><p></p><p>本地缓存之外还有全局缓存，如果业务对缓存容量需求更大，可以使用多副本 DataNode 作为缓存，例如利用 DataNode 作为全局缓存，相比本地缓存，全局缓存容量很大，并且副本数目可以调整。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1d31bb9bbfd48809b166f3947bbd0cb4.png\" /></p><p></p><p>除了利用多级缓存做优化之外，CubeFS 对小文件也有特定的优化，在前面机器学习的场景有提到过，机器学习主要通过缓存元数据的 inode 和 dentry 来优化读性能。其实多副本引擎的小文件会聚合到一个大文件中，小文件聚合会减少 DataNode 管理的文件数量。纠删码引擎写入小文件会采用填充的方式，这样小文件读取时候只访问第一块数据，可以避免跨 AZ 的读流量。顺便提一点，纠删码的读写采用 quorum 机制，RS(n,m) 的编码任意写 n+1份（这里+1还是加几可以配置）就成功，读任意 n 份就返回成功，这样可以有效避免长尾的时延问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d10b4541a52e38dd769b1cf4a657b929.png\" /></p><p></p><p>下面是一个大数据热数据在 CubeFS 的应用实例分享，传统大数据的 shuffle 任务中 map 和 shuffle-work 是同机部署，这样 shuffle-work 读写数据会抢占 CPU 资源，另外由于单机存储的空间有限，可能因为任务分配资源不均衡等问题导致任务失败。remote shuffle 是 OPPO 大数据团队开源的一个项目，将 shuffle-worker 与 map 解绑，在云上部署 shuffle-worker，使用分布式存储 CubeFS 存储 shuffle 过程中产生的临时文件。Shuffle 过程产生的是临时数据，即使数据丢失可以重新生成任务，对数据可靠性要求不高，更加关注成本；临时数据需要快速清理；另外 shuffle 对数据读写的吞吐量和性能要求较高，在多任务并行场景对读写带宽需求较大，测试过程中会发现经常能把网卡、磁盘打满导致机器负载整体能够达到 80%以上。</p><p></p><p>针对这些存储特点，CubeFS 提供以下的解决方案：</p><p></p><p>提供单副本存储，虽然会存在坏盘会导致数据丢失，但是就像上面所说， Shuffle 场景下产生的是临时数据，数据丢失后任务可以重做，代价就是任务时延增加，相比于正常情况下性能提升和成本降低，这是一个合理的权衡。</p><p></p><p>利用 CubeFS 的就近读写的能力，可以将 Shuffle-worker 与 CubeFS 的数据节点同机部署，这样 Shuffle-worker在读写数据的时候，就不需要经过网络，也不受网卡带宽的限制，直接从本机的 DataNode上读取数据，从而提高 Shuffle-worker 的数据读写性能。</p><p></p><p>提供异步删除的功能，将待清理的目录先 rename 到一个临时待删除的目录下，然后&nbsp; CubeFS 后台定时扫描，异常清理待删除目录。一次 rename 操作在 CubeFS 只需要跟后端交互两次，相比于之前串行的删除目录下所有文件，延时由 N 个ms降低到了稳定的 2ms 左右。使用 CubeFS 存储将 Shuffle 的时延减低20%，成本降低20%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/82b47a6caae5eebf723eb7952f6a4a2e.png\" /></p><p></p><p>总结来说，CubeFS 帮助大数据服务实现了快、稳、省的目标。有效提升数据的访问性能、提高存储服务的稳定性并大量地节约存储成本，降低总 TCO。</p><p></p><p></p><h2>未来演进</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e2/e2a5da4b19be55137bddaefa853c4fe1.png\" /></p><p></p><p>未来，CubeFS 将会在下面几个方面提供新的特性：</p><p></p><p>智能分层，未来会根据数据的特性和访问频度将数据分层存储，例如将热数据保存三副本的 SSD 中，而将不常用的冷数据存储在 HDD 或者直接存储到纠删码引擎，以实现更好的性能和资源利用率的提升。</p><p></p><p>多版本快照能力，机器学习存储过程可能存在多人同时修改文件的需求，对文件安全要求性很高，通过多版本快照能力记录文件变化的过程，可以帮忙用户快速恢复到特定版本混合云多云的支持，帮助用户充分利用不同云服务的优势和特性，实现多云的管理。除此之外还会在 GDS、数据加解密以及数据回收站等方面持续演进。</p><p></p><p>唐之享总结道，CubeFS 是开源的云原生分布式存储产品，高效、稳定、弹性；助力大数据与 AI 无限潜能，让大家存得放心、用得省心！同时呼吁大家参与到社区共建中，一起推动 CubeFS 的发展，为更多企业提供高性能、高可靠的分布式存储解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/16f33d338522a6b0b7a6d1f20f0bf6a7.png\" /></p><p></p>",
    "publish_time": "2023-05-05 13:45:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "用友 iuap 让企业数智化能力深入、让业务价值浅出",
    "url": "https://www.infoq.cn/article/dZ9fFqt9hq9kY4kUpXzb",
    "summary": "<p>本文转自 科技正能量 作者&nbsp;郑凯&nbsp;</p><p></p><p>IDC 近期的一份调研中提出：数字化转型在 2023 年开始进入到一个新的阶段：从数字化转型时代，进入到数字化业务时代。</p><p></p><p>在分水岭的两端，一个变化是：数字化转型时代是业务的数字化，而数字化业务时代则是数字的业务化。</p><p>这个潜移默化的改变背后，其实有一重很深的含义。</p><p>过去的数字化转型，是以信息技术建设为主导，尽管是支撑业务，但并非完全以业务为驱动，以业务为中心；而未来，所有的数字化都要从业务视角出发，所有的数字系统都要围绕业务，企业数智化平台当然也不外如是。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2b/2b35b75730c8730c8c3675cdc02edfd6.png\" /></p><p>从数智化 1.0 到数智化 2.0 时代，企业数智化底座面临全面升级，最大的变化就是要“懂业务”。</p><p></p><h2>继续基于 PaaS 核心能力做创新&nbsp;</h2><p></p><p></p><p>进入云时代以来，大型企业的数智化市场，就是得 PaaS 平台者得“天下”。</p><p></p><p>PaaS 平台是大型企业数智化服务的核心竞争高地，我们看到近几年来，&nbsp;SaaS 领域的厂商从应用向下延伸，开发出领域级的 PaaS；IaaS 大厂向上挤压，希望为服务创造更多的客户价值。</p><p></p><p>但时至今日，将这条路走通的似乎只有用友。</p><p></p><p>用友从 2006 年着手平台产品研发、沉淀技术能力，2010 年，用友 iuap 平台正式进入市场， 2014 年，iuap 推出了国内首款纯云原生 PaaS 平台。作为中国最早的 PaaS 平台探索，在近二十年的发展中，iuap 可谓是千锤百炼。</p><p></p><p>到 2020 年，基于 iuap 的底座能力，用友成功推出了 BIP 商业创新平台，一时间震惊业界；到 2022 年，iuap 平台通过能力的丰富化，成为从企业级走向社会级的数智化底座 。</p><p></p><p>如今，更进一步跃升，成为更懂业务、技术领先、体系完整的数平台，助力企业升级数智化底座。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5d293f7afbae9ff2d954872fce76b71e.png\" /></p><p></p><p>用友网络副总裁罗小江说，“这些年为企业提供服务的过程中，我们不断的思考，企业到底需要什么样的 PaaS 平台？我们认为，核心还是要解决企业业务的痛点，所以我们的平台构建始终围绕着业务的视角去做升级。”</p><p></p><p>这就是用友所说的，更懂业务的数智平台，真正意义上让业务有效地融合技术和数据能力，以及未来的 AI 能力，为平台上的业务进行赋能。Gartner 研究数据也显示，用友是全球应用平台软件 aPaaS 市场 TOP10 中唯一的中国厂商。</p><p></p><p>事实上，在企业端这种变化也在悄然发生。很多大企业都在探索，如何能够真正让 AI 和数据的能力与业务深度融合，来进一步提升企业运转的效率。</p><p></p><p>罗小江坦言，用友作为一家企业也感同身受，“最近我们做了内部的变革，很多机器人在被应用到场景里去，比如财务审批，通过 AI、通过自动化、通过数据挖掘的方式去做，也明显感觉到效率的提升。”</p><p></p><p>因此，为了把技术和数据真正意义上贯穿到业务场景里去，真正意义上推动从部门级到企业级到产业级的变革，懂业务的数智平台 iuap 通过帮助企业升级数智化底座，实现在社会化商业时代的创新发展。</p><p></p><h2>懂业务，才做得出“真中台”&nbsp;</h2><p></p><p></p><p>从业务的视角出发，用友 iuap 实现了多项应用架构的领先创新，比如支撑业务变化和精准运营的特征体系，支撑产业链及价值网业务协同的社会化商业组织与数据模型、重构企业精细管理的事项会计中台等等。</p><p>而关键的能力，两大中台实现了在业务上的协调统一：一方面，用友 iuap 业务中台把通用的企业服务功能提炼封装为可复用、可扩展、可运营的中台；另一方面，数据中台沉淀了众多先进管理思想和领先企业实践，预置了覆盖财务、人力、采购、制造、营销等 10 大领域 2400+业务模型和多个行业的主题包。两者结合，让企业的业务创新速度得到大幅提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8b21fb9a9ecaeb82d00122c2b951875.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1\" /></p><p></p><p>我们知道，这些年来，中台逐渐成为了企业数智化转型最为关注的技术方向。业界也充斥着各式各样的数据中台、技术中台，但业务中台却并不多。</p><p></p><p>其实大部分的数据中台，仍然处于做工具的阶段，基于数据做处理的能力是有的，但客户更关心最后一公里的问题，这最后一公里就不单纯是数据问题，而是与业务相关。</p><p></p><p>如罗小江所说，“很多数据中台不做主数据的处理，因为主数据要求对业务理解能力很高，比如物科主数据、客商主数据，内部财务科目的主数据，如果对业务不了解，就无法理解应用场景。但用友花了大量的精力在主数据上，并将我们对业务的理解融会贯通。”</p><p></p><p>很明显，更懂业务的用友 iuap 并不单纯是一个宣传口号，从数据中台在实际业务的表现上，和对业务的理解上，就可以推导出这个结果。</p><p></p><p>业务中台背后其实也有相似的逻辑存在，业界的一些所谓业务中台，甚至并非是厂家独立开发，而是通过生态伙伴来实现的，而用友的业务中台，则是基于客户实际业务场景里面积累所沉淀出来的。</p><p></p><p>是真中台，还是“伪中台”，真真假假似乎已经一目了然。</p><p></p><p>更何况，用友 iuap 不仅只有数据中台和业务中台，还有智能中台，并已经形成 RPA、VPA、智能大搜、规则引擎、知识图谱、AI 工作坊等多项智能化能力，并提供企业画像、人才画像、商机推荐、供应商推荐等智能服务，已经赋能用友 BIP 在财务、人力、采购、制造、营销等业务领域。在我们畅想的 AIGC 大时代到来前，用友已经提前做好了布局。数智化的“智”，终于要绽放了。</p><p></p><h2>让数智化深入、浅出&nbsp;</h2><p></p><p></p><p>所有的智能，都与数据价值息息相关。做好智能化的前提，一定是做好数据的业务化。</p><p></p><p>罗小江说，“数据的业务化可以真正服务于业务，并产生新的业务模式。要发挥数据价值，一定要往这个方向走。同时，数据业务化离不开数据底座，所以首先要有效的把数据治理好。所以我们的产品能力，主动去做数据治理，并为数据业务化打基础。”</p><p></p><p>其次，数据的业务化也与企业所处的环境相关，如果将外部数据与企业内部数据整合，就可以基于数据去找到新的业务机会。</p><p></p><p>由数据之始，融入业务之流，输出智能之果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f3875172b3454856b35be3816591e45.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1\" /></p><p></p><p>这其实，就是用友 iuap 融合三大中台为一体，并结合技术平台、低代码开发平台、连接集成平台，形成“三中台+三平台”完整企业数智化底座平台体系的本质。</p><p></p><p>用友 iuap 之所以能够成为更懂业务的数智平台，核心的原因有几个：</p><p></p><p>第一，软件领域是一个苦功夫，是需要慢性培育的市场，需要长期对客户需求的探索，投入和研究。用友，以过去 35 年的积累，深入企业业务流程，这是用户有资格谈“懂业务”的根本。</p><p></p><p>第二，投入 PaaS 平台建设更早，提炼的共性需求更多，所以用友 iuap 的客户适用性更强。因为软件本质就是解决问题，而不是卖产品，这个出发点决定了用友的数智平台是最适合企业用户的平台。</p><p></p><p>第三，对场景化的理解更深，虽然是底座，但本质上还是要支撑应用场景，而用友的客户五花八门，面对的场景千姿百态，这决定了用友 iuap 平台对场景的适配和支持更好，对业务的思考更深入。</p><p></p><p>除了平台能力体系，用友 iuap 还通过数智化工程体系、可持续运营体系，助力企业平台应用体系全面升级，让企业实现从工具链构建到运营，随需享受云计算、大数据、人工智能等新技术带来的便利及价值。</p><p></p><p>企业数智化是一个深入、浅出的过程，让数智化能力深入，让业务应用的价值浅出。不为了数智化而数智化，而是为了业务的高效和创新而数智化，我们从用友 iuap 的身上，也读出了这些“道理”。</p>",
    "publish_time": "2023-05-05 13:45:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "谷歌、OpenAI 都白干，开源才是终极赢家！谷歌内部文件泄露：欲借开源打败 OpenAI",
    "url": "https://www.infoq.cn/article/ebXXoCeJgkr3P2JQmeFz",
    "summary": "<p></p><p>“我们没有护城河，OpenAI也没有。”一名<a href=\"https://www.infoq.cn/article/hr0Vyhxtb701BXVt6ZwQ\">谷歌</a>\"内部研究员在一份文件的开头直接说道。近日，有位匿名人士将这份文件共享到了公共 Discord 服务器上，有外媒已经验证了这份文件的真实性。</p><p>&nbsp;</p><p>在我们都将注意力放到企业AI大战时，谷歌内部人员反而指出，最后的赢家不是谷歌和OpenAI，反而是开源模型。作者用各种示例证明了自己的观点，同时建议谷歌借力开源来赢下这场战争。大家可以先看下这份文件的内容，我们翻译后并在不改变原意的基础上做了整理。</p><p></p><h2>谷歌内部文件</h2><p></p><p>&nbsp;</p><p></p><h3>我们没有“护城河”</h3><p></p><p>&nbsp;</p><p>OpenAI也没有。</p><p>&nbsp;</p><p>我们一直在努力追赶<a href=\"https://www.infoq.cn/article/g9tuoTODP20N1lTzjsjw\">OpenAI</a>\"的步伐，但谁能率先跨越下一个里程碑？AI的下一步又该迈向哪里？</p><p>&nbsp;</p><p>真正令人不安的事实在于，我们双方可能都没有能力赢下这场军备竞赛。就在谷歌和OpenAI较劲的同时，第三股势力一直在闷声发大财。</p><p>&nbsp;</p><p>没错，我说的就是开源模型。明确地讲，他们在照搬我们的劳动成果，而且已然克服了开放AI面对几道重大关隘。下面仅举几例：</p><p>&nbsp;</p><p>大语言模型在手机上的运行：已经有人<a href=\"https://twitter.com/thiteanish/status/1635678053853536256\">在Pixel 6上运行起了基础模型</a>\"，每秒能处理5个token。可扩展的个人AI：大家花一个晚上，就能在自己的笔记本电脑上<a href=\"https://github.com/tloen/alpaca-lora\">微调出个性化AI</a>\"。负责任发布：与其说是“攻克”了这个难题，不如说是“回避”了问题。如今各种网站上充斥着大量艺术创作模型，文字生成模型同样所在多有。多模态：目前的多模态<a href=\"https://arxiv.org/pdf/2303.16199.pdf\">ScienceQA STOA</a>\"能在一小时之内完成训练。</p><p>&nbsp;</p><p>虽然我们的模型在质量方面仍略有优势，但双方差距正在以惊人的速度缩小。开源模型更快、可定制性更强、更私密，而且功能性也不落下风。他们可以用<a href=\"https://lmsys.org/blog/2023-03-30-vicuna/\">100美元外加13B参数</a>\"解决需求，而我们则需要面对千万美元的成本和540B参数。相较于长达数月的训练周期，他们的模型往往几个礼拜就能火热出炉。这对我们产生了深远的影响：</p><p>&nbsp;</p><p>我们没有什么秘方。我们最好的出路就是学习外部各方的思路并开展合作。谷歌应该优先考虑启用3P集成。当免费、不受限制的替代模型在质量上与专有模型相当时，人们肯定不会愿意继续付费。我们应该考虑真正的业务增值空间在哪里。巨型模型正在拖慢我们的脚步。从长远来看，最好的模型一定是那些能够快速迭代的模型。既然我们知道参数少于20B也能达到相当不错的效果，那就该主动尝试做一些更小的变体。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/abfabf60e4c1f626403aec82339f1b4e.png\" /></p><p></p><p></p><h3>发生什么事了？</h3><p></p><p>&nbsp;</p><p>3月初，随着<a href=\"https://www.infoq.cn/article/hFV2Af2BzyvAKRSzzymK\">Meta的LLaMA模型</a>\"被泄露，开源社区获得了首个真正具备一线“战斗力”的基础模型。虽然缺少说明、对话微调和RLHF（基于人类反馈的强化学习），但社区还是很快理解了这套模型的重要意义。</p><p>&nbsp;</p><p>随之而来的就是铺天盖地的创新涌现，而且每隔几天就会出现一波重大发展。仅仅一个月后，我们就走到了如今的局面，有了指令微调、量化、质量改进、人工评估、多模态、RLHF等变体，其中很多还互为依托。</p><p>&nbsp;</p><p>最重要的是，开源社区解决了真正的可扩展性问题，让普通人也有了在AI平台上一试身手的机会。训练和实验的门槛已经从研究机构的高精尖操作，下降成了一个人、一个晚上加一台高配笔记本电脑就能搞定的小探索。</p><p>&nbsp;</p><p>从很多方面来看，这样的现实都有其必然性。开源大语言模型的蹿红其实是紧跟着图像生成模型的爆发，社区也倾向于将目前阶段称作大语言模型的“Stable Diffusion时刻”。</p><p>&nbsp;</p><p>无论是图像生成还是文本生成，广泛的公众参与都是通过成本极低的微调机制来实现的，也就是所谓“低秩适应”（LoRA），同时辅以规模上的重大突破（图像合成中的latent diffusion，以及大语言模型的Chinchilla）。在这两方面，高质量模型的出现引发了每位个人和机构的关注和迭代尝试，最终形成了超越技术巨头的改进成果。</p><p>&nbsp;</p><p>这些贡献在图像生成领域至关重要，也让Stable Diffusion真正走上了与Dall-E不同的道路。前者基于开放的发展路线，使其获得了Dall-E所不具备的产品集成、市场、用户界面和创新加持。</p><p>&nbsp;</p><p>开放的效果也显而易见：与OpenAI的同类解决方案相比，开源模型的文化影响力迅速占据主导，后者则逐渐退出舞台的中心。目前还难以断言大语言模型会不会重复这样的故事，但总体来看，决定历史走向的基本要素是相同的。</p><p>&nbsp;</p><p></p><h3>我们错过了什么</h3><p></p><p>&nbsp;</p><p>推动开源近期一系列成功的创新举措，也直接攻克了一直困扰我们技术大厂的难题。更多关注他们的工作成果，将帮助我们避免重新发明轮子。</p><p>&nbsp;</p><p><a href=\"https://arxiv.org/abs/2106.09685\">LoRA</a>\"是一种非常强大的技术，能够将模型更新表示为低秩分解的形式，从而将更新矩阵的体量缩小至数千分之一。如此一来，我们就能以极低的成本和时间实现模型微调，从而在几个小时内在消费级硬件上打造出个性化语言模型。</p><p>&nbsp;</p><p>这绝对非同小可，也让我们真正走向以近实时方式整合新的、多样化知识的愿景。事实上，这项技术在谷歌内部并未得到充分的重视和运用，但却直接影响到了我们一些雄心勃勃的项目。</p><p>&nbsp;</p><p></p><blockquote>从零开始重新训练模型没有前途。</blockquote><p></p><p>&nbsp;</p><p>LoRA之所以如此高效，部分原因在于跟其他微调手段一样，它是一种可堆叠的方法。我们可以通过指令微调等实现改进，借此吸引其他贡献者提供的对话、推理或工具使用方式。虽然个别微调是低秩的，但其总和却不一定，模型的全秩更新需求将随着时间推移而逐渐累积。</p><p>&nbsp;</p><p>也就是说，随着新的、质量更高的数据集和任务的出现，模型能够始终维持较低的更新成本，而无需承担从零开始重新训练的开销。</p><p>&nbsp;</p><p>从零开始重新训练巨型模型不仅会丢弃预训练效果，还会丢弃以堆叠方式完成的任何迭代式改进。在开源世界中，这种改进将很快占据主导地位，并让重新训练的成本迅速提升至无法承担的程度。</p><p>&nbsp;</p><p>我们应当思考每个新的应用或场景是不是真的需要一套全新模型。如果真存在值得丢弃原有权重的重大架构改进，那也应该投资探索更积极的蒸馏形式，想办法让新模型尽可能多地保留上一代的功能。</p><p>&nbsp;</p><p></p><blockquote>如果我们能够在小模型上加快迭代速度，那么从长远来看，大模型恐怕将不再具备能力优势。</blockquote><p></p><p>&nbsp;</p><p>对于占据主流的模型规模，LoRA更新的生产成本可以控制到极低（约100美元）。也就是说，几乎任何人都能按照自己的想法实现模型微调，到时候一天之内的训练周期将成为常态。以这样的速度，微调的累积效应将很快帮助小模型克服体量上的劣势。</p><p>&nbsp;</p><p>事实上，从工程师的单人工时出发，这些小模型的改进速度大大超过了体量庞大的“同门兄长”，而且其中最出色的选手在相当程度上已经跟ChatGPT站在同一水平线上。另外，反复训练超大体量模型也不利于保护自然环境。</p><p>&nbsp;</p><p></p><blockquote>数据质量比数据规模更重要。</blockquote><p></p><p>&nbsp;</p><p>许多项目都开始在规模较小、但经过精心筛选的数据集上训练，希望借此节约时间。这代表着数据规模化法则其实具有一定的灵活性。此类数据集同样遵循“数据不在乎你怎么想”这个基本逻辑，并迅速成为外部社区的标准模型训练方式。</p><p>&nbsp;</p><p>这些数据集是使用合成方法（例如从现有模型中过滤出最佳响应）和其他项目中提取而来，这两种方式在谷歌内部都不太受重视。幸运的是，这些高质量数据集都是开源的，所以可以免费使用。</p><p></p><h3>直接与开源竞争将必然失败</h3><p></p><p>&nbsp;</p><p>按近的动向已经对我们的业务战略产生了直接且无法回避的影响。如果有了免费、高质量的替代品，谁还愿意花钱去用设定了请求上限的谷歌产品？</p><p>&nbsp;</p><p>而且这种差距是无法弥合的。现代互联网运行在开源基础之上绝非偶然，开源社区有着很多我们无法复制的显著优势。</p><p>&nbsp;</p><p></p><blockquote>相较于开源需要我们，我们更需要开源。</blockquote><p></p><p>&nbsp;</p><p>我们的技术机密只靠一项脆弱的协议来维持。谷歌研究人员随时都在跳槽前往其他企业，不妨假设我们知道的他们也都知道。而且只要职业生涯的规划渠道仍然开放，这种情况就无法改变。</p><p>&nbsp;</p><p>而随着大语言模型的前沿研究成本越来越低，保持技术竞争优势也变得愈发困难。世界各地的研究机构都在相互借鉴，以广度优先的方式探索解决方案空间，而单凭谷歌自己根本把握不住如此浩瀚的战场。我们当然可以在外部创新不断降低其价值的同时继续保守秘密，也可以敞开胸怀尝试相互学习。</p><p>&nbsp;</p><p></p><blockquote>企业受到的许可约束更强，个人则相对自由灵活。</blockquote><p></p><p>&nbsp;</p><p>目前开源社区的大部分创新都以Meta泄露的模型权重为基础。虽然随着开放模型质量的不断提升，这种情况肯定会发生变化，但开源阵营真正的优势在于“个人行为”。毕竟没有谁能够挨个打击和起诉个人用例，所以人家用不着等技术过气就能随时随地研究和探索。</p><p>&nbsp;</p><p></p><blockquote>作自己的客户，意味着更了解用例。</blockquote><p></p><p>&nbsp;</p><p>看看人们在图像生成空间中创建的模型，就会发现从动漫生成器到HDR景观，大家的创意简直源源不断。这些模型出自那些长期浸淫于特定流派和风格当中的人们，体现出了我们无法企及的知识深度和内心共鸣。</p><p>&nbsp;</p><p></p><h3>掌控生态系统：让开源为我们服务</h3><p></p><p>&nbsp;</p><p>矛盾的是，这一切背后最大的赢家反而是Meta自己。泄露的模型就是他们搞出来的，所以他们其实是获得了全世界的免费劳动力。由于大部分开源创新都发生在他们的架构之上，所以Meta当然可以直接把成果整合到自家产品当中。</p><p>&nbsp;</p><p>这种对生态系统的掌控力，再怎么强调都不为过。谷歌本身就已经在开源产品（例如Chrome和Android）中成功践行了这一原则。通过掌控创新发生的平台，谷歌巩固了自己作为思想主导和方向制定者的地位，获得了超越自身极限塑造宏大趋势的能力。</p><p>&nbsp;</p><p>而我们对自有模型约束得越严格，就相当于在为开源AI模型做市场宣传。谷歌和OpenAI都倾向于采取充满戒备感的发布模式，想要努力控制模型的使用方式。但这种控制力根本站不住脚，毕竟任何想用大语言模型的人们都可以直接选择免费开放的模型。</p><p>&nbsp;</p><p>谷歌应当让自己成为开源社区的领导者，通过更广泛的对话来塑造优势地位。没错，期间肯定涉及一些与以往习惯不符的环节，比如发布小型ULM变体的模型权重，也就是放弃我们对模型的某些控制。但这种妥协是不可避免的，我们没办法既想刺激创新、又要强行控制创新。</p><p>&nbsp;</p><p></p><h3>OpenAI怎么做并不重要</h3><p></p><p>&nbsp;</p><p>考虑到OpenAI目前的封闭政策，可能前面那些开源建议都让人很难接受。OpenAI都不愿意开放，我们谷歌凭什么这么做？但事实上，源源不断的技术挖角早就让封闭成了一个伪命题。如果无法阻断这条通路，那保密到底有没有意义真的不太好说。</p><p>&nbsp;</p><p>最后，OpenAI怎么做并不重要。毕竟他们只是谷歌的难兄难弟，在对待开源的态度上犯了类似的错误，保持优势的能力也没有明显更强。除非他们改变立场，否则开源AI模型可能、且最终必然会令其黯然失色。至少在这方面，我们可以先行一步。</p><p>&nbsp;</p><p></p><h2>关于开源模型的争论</h2><p></p><p>&nbsp;</p><p>谷歌内部研究员的观点引起了广大开发者对于开源模型和企业模型之间的讨论。</p><p></p><p>开发者“dahwolf”指出，当前的范例是人工智能的目的之一：用户访问并与之交互的产品。但这根本不是将来大众与人工智能互动的方式。人工智能将无缝集成到日常软件中：在 Office/Google 文档中、在操作系统级别 (Android)、在图形编辑器 (Adobe) 中，在主要网络平台上，如搜索、图像搜索、Youtube 等。</p><p>&nbsp;</p><p>他认为，由于谷歌和其他科技巨头继续控制着这些拥有十亿用户的平台，因此它们拥有人工智能的影响力，即便它们在能力上暂时落后。他们还将找到一种集成方式，让用户无需直接为该功能付费，因为它将通过其他方式收费：广告。</p><p>&nbsp;</p><p>“dahwolf”表示，OpenAI 面临生存风险，而不是谷歌。巨头们会迎头赶上，并将拥有覆盖面和补贴优势。它并没有就此结束。这种来自开源的所谓“竞争”将是免费劳动力。任何成功的想法都会在短时间内移植到 Google 的产品中。感谢开源！</p><p>&nbsp;</p><p>有网友指出，人工智能无处不在的问题就是需要一直都有非凡的计算能力，而这种计算能力需要花钱。因此，也有人指出，LLM 的运行成本越来越低，这对谷歌来说是一个胜利。OpenAI 的服务太贵了，无法靠广告资助。谷歌需要一种更便宜的技术来维持其广告支持的商业模式。</p><p>&nbsp;</p><p>对于开源模型，开发者“kelipso”表示，开源模式在过去几十年一直行之有效。对LLMs的担忧是开源（和学术界）无法做大公司正在做的事情，因为他们无法获得足够的计算资源。“这篇文章在讨论（我猜开源 ML 组正在展示）企业不需要用那些计算资源来铺平道路。OpenAI 或其他大公司能否通过某些模型、数据集、计算资源等在 AI 中获得最大收益，这仍然是一个悬而未决的问题。”</p><p>&nbsp;</p><p>不过，开发者“not2”回应称，没有一个“开源”AI 模型是经典意义上的开源。它们是免费的，但不是源代码；它们更接近可自由分发的编译二进制文件，其中编译器和原始输入尚未发布。一个真正的开源 AI 模型需要指定训练数据和从训练数据到模型的代码。</p><p>&nbsp;</p><p>“当然，让其他人获取这些信息、重新构建模型并验证是否获得了相同的结果成本非常高，也许我们真的不需要那样。但如果我们没有，那我认为我们需要用除‘开源’之外的其他术语来描述这些模型。你可以得到它、分享它，但你不知道它里面有什么。”not2 表示。</p><p>&nbsp;</p><p>未来，谷歌是否会选择开源策略，又能否借此打出不一样的“翻身仗”，我们拭目以待。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither\">https://www.semianalysis.com/p/google-we-have-no-moat-and-neither</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=35813322\">https://news.ycombinator.com/item?id=35813322</a>\"</p>",
    "publish_time": "2023-05-05 14:36:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "网易数帆低代码开发平台升级：加入智能大模型，可用自然语言描述快速开发应用",
    "url": "https://www.infoq.cn/article/k4GpYaGblS3RUzkeqIVE",
    "summary": "<p>4月25日，网易数帆在<a href=\"https://www.infoq.cn/article/gh6HUCBsJ32QuCUDaTOs\">低代码业务</a>\"战略发布会上推出CodeWave智能开发平台。平台以网易自研的智能大模型为底座，以低代码为开发工具，开发者只需编写少量代码，通过自然语言描述和可视化拖拉拽即可快速开发应用。</p><p>&nbsp;</p><p>网易公司CEO丁磊为发布会开场致辞指出，人工智能的价值已经被引导到了社会生产的第一线，这是一个生产力马上要倍速迸发的时代，也是企业做数字化转型的新时机。</p><p>&nbsp;</p><p>在此背景下，<a href=\"https://www.infoq.cn/article/fqygd7M5v6MlKUx3acIw\">网易数帆</a>\"将原有的低代码开发平台升级，发布CodeWave智能开发平台，以“智能大模型和全栈低代码”为核心，延续“低门槛，高上限”的特色，实现开发、测试、运维等软件生产全链路的智能化。</p><p>&nbsp;</p><p>网易副总裁、网易杭州研究院执行院长、网易数帆总经理汪源表示，软件供给水平被严重制约，导致软件成本非常高，而软件供给侧改革的核心关键变量就是智能大模型技术和全栈低代码技术。</p><p>&nbsp;</p><p>智能大模型，指的是参数量超过一亿的神经网络模型，可以实现类人水平的自我学习和自我演化，使其具备一定的自主选择和判断能力。网易数帆面向智能编程垂直领域推出大模型，并接入到智能开发平台中。开发者只需使用自然语言与CodeWave对话，即可生成低代码可视化编程语言代码。</p><p>&nbsp;</p><p>据介绍，CodeWave使用了网易预训练大模型及其在智能编程领域高效微调的成果。目前，基于大模型的智能辅助编程平台在网易公司内部试用。而在自然语言翻译成SQL的核心功能方面，网易自研模型效果超过了ChatGPT&nbsp;90%以上的水平。</p><p>&nbsp;</p><p>此外，该平台还提供智能检查和修复、智能补全等辅助工具，帮助完善编程成果，AI测试机器人也能自动完成低代码应用的测试，保证应用正常运行。</p><p>&nbsp;</p><p>网易数帆云原生及低代码产品线总经理<a href=\"https://www.infoq.cn/article/YioVaIhq4meev8iVMEW5\">陈谔</a>\"表示，AIGC虽然带来了软件生产的变革，但依然存在一些不变量。首先，人类的自然语言有天然的模糊性，这意味着自然语言不能用来表达和维护复杂的软件，我们依然需要编程语言。其次，无论是AI还是人类，在没有充分的上下文情况下，谁也写不出符合要求的软件。最后，软件开发是一个需要封装来屏蔽底层很多细节的过程。</p><p>&nbsp;</p><p>基于此，要让开发体验变得更好，网易数帆认为，第一是提供一门低代码编程语言，人人都能掌握，不用再学习Java、JS等语言；第二是要提供一个极致的标准化软件设计开发规范，大幅减少人类和AI开发软件时候需要掌握的上下文知识；最后是要提供高度模块化、服务化的封装，让开发者只需要关注业务逻辑。</p><p>&nbsp;</p><p>2020年，网易数帆正式对外发布低代码开发平台，以可视化为主要编程方式，用拖拉拽操作来替代写代码，统一前后端开发语言，抹平编程语言差异。平台采用自研NASL语言，具备丰富语言能力，内设丰富多样的通用组件和逻辑组件，支持互不影响的多人协作，在保证简单易操作的前提下，也能供开发者搭建企业级复杂应用。</p><p>&nbsp;</p><p>此外，网易数帆参与共建的AirCoder代码生成插件工具，提供上下文补全等多种代码生成模式，目前支持Java、Python、C/C++、Go等主流编程语言，兼容IntelliJ IDEA、PyCharm、VSCode、Eclipse等主流IDE，已经支持1000+网易技术人员在开发环境使用，AI生成代码量达到15%。</p><p>&nbsp;</p><p>在网易公司内部，低代码已经在游戏、电商、传媒等各个业务部门广泛使用，涉及人力资源与组织绩效管理、技术保障与业务增长等多个环节。以网易互娱为例，用低代码搭建运营活动管理系统，开发效率提升100%，可快速支撑多个私域流量转化。</p><p></p><p>根据网易数帆介绍，在金融行业，某大型国有银行已将专业开发和低代码开发分工，并多人协同开发，涉及台账管理、结算管理等复杂应用，提效降本达到60%，加强金融业务发展。</p><p>&nbsp;</p>",
    "publish_time": "2023-05-05 14:44:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]