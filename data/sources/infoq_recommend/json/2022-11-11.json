[
  {
    "title": "2022年中国开源创新大赛在乌镇正式启动",
    "url": "https://www.infoq.cn/article/xoG0SH2tyH1mXe7lF2C1",
    "summary": "<p>11月10日上午，在<a href=\"https://cn.wicinternet.org/\">2022年世界互联网大会乌镇峰会</a>\"数字经济分论坛上，2022中国互联网发展创新与投资大赛暨2022年中国开源创新大赛在乌镇正式启动。</p><p></p><p>本届大赛是在中央网信办信息化发展局指导下，由<a href=\"http://www.cidf.net/\">中国互联网发展基金会</a>\"、中国网络空间研究院、中国互联网投资基金联合主办的开源技术竞赛。</p><p></p><p>党的二十大报告提出，教育、科技、人才是全面建设社会主义现代化国家的基础性、战略性支撑。必须坚持科技是第一生产力、人才是第一资源、创新是第一动力，深入实施科教兴国战略、人才强国战略、创新驱动发展战略，开辟发展新领域新赛道，不断塑造发展新动能新优势。</p><p></p><p>开源理念坚持技术面前人人平等、科学知识自由传播，有助于促进我国高科技创新人才培养，有助于我国提升核心技术创新能力，早日实现自主发展，打破特定科技领域受制于人的局面。</p><p></p><p>中国工程院院士倪光南表示：“我国信息技术的快速崛起也得益于开放源代码，在软件领域通过支持开源、运用开源，大大缩小了我们与发达国家之间的差距。今后我们还应该进一步拥抱开源，积极参与，增加贡献，发扬开放、共享、协同、共赢的开源精神，汇天下英才，与世界协同创新。”</p><p></p><p>中国开源软件推进联盟名誉主席陆首群教授认为：“中国要更加重视开源的溢出效应，<a href=\"https://xie.infoq.cn/article/57af3d8a5e89dd76d7c737ac9\">开源创新</a>\"已成为数字化转型、智能化重构的基础，成为全球的一种创新和协同模式，符合创新国家建设的战略需求。”陆首群指出，“中国开源现在已经发展到第三个历史阶段，是在经济双循环基础上规范建设和改造我国的供应链并主要在促进产业链、供应链数字化的基础上，采用取代物料表格式样的开源代码，保障其安全的历史阶段。”</p><p></p><p>诚然，如诸多科技领域一样，发展离不开人才，创新更需要人才。倪光南认为，“在全面建设社会主义现代化国家新征程中，对开源人才的培养工作也提出了更高的要求。面临新形势新任务，中央网信办指导举办的2022年中国开源创新大赛意义格外重大。”</p><p></p><p>据悉，本次大赛以“开源创新&nbsp;共建生态”为主题，设立了人工智能、操作系统、<a href=\"https://www.infoq.cn/article/AMHObrpVe29vbUpQoky3\">Risc-V</a>\"、<a href=\"https://xie.infoq.cn/article/3491e72fc39cc02438f6e61b1\">开源供应链</a>\"等十余个专业技术赛道，邀请到业内五十余位一线技术专家参与评选，整个赛程接近半年时间，面向全社会开放，企事业单位、创业团队、开源社区、大学生和个人开发者均可报名参赛。</p><p></p><p>“我们希望能够将本次开源创新大赛办成一场公开、公正、公平、公益的赛事，通过这样的赛事来为社会培养和选拔开源领域的青年才俊，为国家储备专业技术人才。”中国互联网发展基金会有关负责同志表示，“本届大赛得到了国内开源企业、开源社区、高校和科研机构的大力支持，多家开源社区共建大赛官网，可以直接覆盖国内上千万的开源爱好者。接下来上海、深圳、杭州、成都等多地开源社区还将参与到大赛的宣传和推广中来。”</p><p></p><p>2022年中国开源创新大赛已于11月10日正式上线，现邀请全国的开源爱好者，开源技术人士关注赛事官网，积极报名参赛。本届大赛由北京长风信息技术产业联盟承办，报名截止日期为2023年1月9日。报名完成后，随即进入初赛阶段。2023年4月底，大赛的决赛将如约而至。</p><p></p><p>大赛将为从事开源技术研发工作者提供与同行交流切磋、展现自身才能的机会。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/99/992ede35bffb8b436cbd1e0a451ad92e.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/38cf214a456b6db3abb14a2c9f43e048.png\" /></p><p></p>",
    "publish_time": "2022-11-11 00:11:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "下一代 NLP 模型将走向何方？",
    "url": "https://www.infoq.cn/article/LBhYJYr63GysNIKZCgzI",
    "summary": "<p>嘉宾 | 相洋</p><p>作者 | 凌敏</p><p></p><p>作为人工智能的重要子领域之一，NLP （自然语言处理）技术近年来备受关注，在工业界也得到了广泛应用。此前有专家分析称，NLP 即将迎来“黄金十年”，各领域对 NLP 的需求会大幅度上升，并且对 NLP 质量也将提出更高的要求。</p><p></p><p>本期<a href=\"https://www.infoq.cn/video/C0IXAd7dDLXDy2dV1F0V\">“InfoQ 极客有约”与“OpenI 启智社区”联合推出的系列直播栏目</a>\"邀请到了鹏城实验室网络智能部云计算所副所长、OpenI 启智社区 NLP 开源算法库负责人相洋，他将为我们解读 NLP 技术的整体发展，并一同探索下一代 NLP 模型将走向何方。</p><p></p><p></p><p></p><p></p><h2>NLP 迎来“黄金十年”</h2><p></p><p></p><h4>InfoQ：前几年有专家判断说 NLP 将迎来“黄金十年”，现在仍是 NLP 的黄金时代吗？</h4><p></p><p></p><p>相洋：我认为是的。现在的大模型相当于从 2017 年底、2018 年初才刚开始火起来，到现在满打满算也就四五年的时间。无论是模型的纵深，比如从小及大；还是模型的横向，比如从 NLP 的通用领域，扩散到其他的一些垂直领域，甚至扩展到其他的行业，如视觉、生物信息等等，这个过程还远远没有结束，应该还有很大的发展空间。因此，现在肯定还是处于一个吃红利的时代。</p><p></p><h4>InfoQ：近几年，NLP 的整体发展情况如何？有哪些值得关注的技术突破？</h4><p></p><p></p><p>相洋：在 NLP 里面，现在最热门的是预训练技术，也就是大模型技术。一方面，从 2017 年底的 <a href=\"https://www.infoq.cn/article/wt-KaTfcsAv9E7exzIkF\">Transformer</a>\"，到 2018 年、2019 年的 <a href=\"https://www.infoq.cn/article/IWIURYBlyV1vG58H8JO5\">BERT</a>\"，以及 <a href=\"https://www.infoq.cn/article/cTX3kDQHY2Ky6gdHcGHa\">GPT</a>\" 等模型出现之后，带来了很多传统的机器学习、深度学习模型所达不到的 SOTA。另一方面，应该也是在 2018 年左右，图神经网络技术的兴起给 NLP 带来了一些新的突破，比如大家可以把一个知识图谱用图的形式去表示出来，再建模，并跟一些主流的 NLP 模型结合，进一步去增强模型对于知识的表达能力和融合能力，这也是一个大的突破。</p><p></p><p>此外，NLP 领域也向其他领域进行了迁移，比如在生物信息领域，谷歌提出来的蛋白质折叠系列研究取得了非常好的成果。现在很多人也在想，是不是能用一些类似于 语言的形式表达这样的数据，用 NLP 的方法去建模，也有很多人在做各种各样的尝试。</p><p></p><h4>InfoQ：近几年 NLP 在医学领域的整体应用还是比较广泛的，根据您的观察也是如此吗？除了医学领域，NLP 在哪些领域还应用的比较多？</h4><p></p><p></p><p>相洋：我在 2017 年下半年去美国德克萨斯州的时候，发现在那里，NLP 跟医疗是紧密相关的。但是在那时，NLP 的应用还不是特别广，那会也正好赶上了 Transformer 这一系列模型的兴起，大家才进一步去用 NLP 的方法在大规模数据上做实验。前期基本上都是一些比较小的实验，比如对电子病历的理解，以及基于病人的历史信息去预测他将来得病的可能性或概率。后来随着大模型的提出，我开始跟他们一起去做流行病、传染病相关的工作，并且用了一些自然语言处理的技术，最后发现结果还挺好，比过去使用传统方法取得的结果还要好。</p><p></p><p>这几年我在一线工作，也参与到了一些医学领域的相关研究中，眼见着 NLP 在医学领域开始应用得越来越广泛了，取得的成果也越来越多，在 Nature， Science 也有很多相关研究文章。</p><p></p><p>除了医学领域，NLP 技术也被广泛应用在计算机视觉领域，比如 Transformer 现在也被用来编码图像，前两年甚至有一种说法叫做“Transformer 一统世界”，就是说，无论在文本，还是影像、语音，都可以基于 Transformer的一些架构去建模，并且最终的效果也还不错。目前来看，人工智能最主流的两个领域就是 CV 跟 NLP，一些 NLP 技术通过跟不同的领域融合之后，在 CV 领域也取得了很好的成果。</p><p></p><h4>InfoQ：今年 9 月 7 日，鹏城实验室正式发布了“鹏程·丝路 2.0”翻译平台，新发布的 2.0 版本主要解决了哪些痛点问题？在技术上有哪些突破？</h4><p></p><p></p><p>相洋：大概在去年五六月份，我们发布了“鹏程·丝路&nbsp;1.0”，主要的模型是 Bilingual Word Model，就是双语语言模型。打个比方，如果我们要进行中文、俄语之间的翻译，这个模型只能解决中俄的翻译；如果要进行中法的翻译，就需要另外一个模型。也就是说，如果我们有一百种语言需要翻译，那么就需要一百个模型。一方面，这需要有足够的 GPU 去存储这些模型，并且势必会对相关的训练和部署带来很大的困难；另一方面，我们针对语种做定制化研究、深入优化的时候，也会有很多的问题。</p><p></p><p>所以，我们在近几年国际上比较热门的研究方式的基础上，提出了一套多语言的机器翻译——一个模型可以去解决 N 个语种。这个模型的训练和部署的困难度整体有所降低，另外，这些语系里面的不同的语言之间是有关联的。</p><p></p><p>我们通过构建这种多语言的语言模型，能够通过一些特殊的技术，把语言之间的关系建立起来。这就解决了我们之前遇到的低资源语种的问题。低资源语种可能跟其他的某个语种有些关联性，我们通过这样的多语言模型就能建立这种关联，用高资源语种去促进低资源语种的学习，这也是 2.0 模型提出的最大的一个改进。</p><p></p><p>还有一些其他的技术，比如之前我们训练过一些单语种的模型，如盘古模型，我们在这个模型的基础上进一步增强了翻译系统的单语言理解能力。比如加入了英文、中文等很多单语种，在单语种的加持之上，对于机器翻译的编码阶段或解码阶段的语言理解就会更强。还有非常重要的一点就是，现在国际上很多做机器翻译的研究都是以英文为中心语言，而我们做的是以中文为中心语言，解决的是中文到其他语言以及其他语言到中文的翻译。</p><p></p><p>为了解决这个问题，我们需要有一套独立的评测数据集，我们与外文局等第三方的语言专家一起建立了一套标准的测试集。这个测试集现在已经涵盖了 30 几个语种，我们在这些语种上与专家一起做了人机协同的优化，才达到了现在这个效果。</p><p></p><h2>模型参数越大就越好吗？</h2><p></p><p></p><h4>InfoQ：为什么最近几年预训练大模型这么火？它主要能解决什么问题？</h4><p></p><p></p><p>相洋：在深度学习刚提出来的时候，传统的信号处理在计算机视觉上面表现比较好。后来，NLP 领域出了一个新的技术叫做 Word Embedding，就是词向量。以一个句子为例，我们把这个句子看作一个一维向量，每个单词再用一个词向量来表示，这个句子就变成了一个二维的表示。这个二维的表示可以像一张图像一样，我们用图像的卷积神经网络等一些方法去处理它，也取得了很好的效果。</p><p></p><p>大概在 2013 年，提出了词向量的概念。那么，词向量是怎么训练出来的？它就是通过一些大规模的文本数据进行无监督的训练，然后让词向量的模型去学习到一些模式。比如，单词 A 周围跟它贡献频率最高的是哪些单词，这样就大概能形成一种语言模型。那时我们称之为浅层的预训练，后来随着软硬件技术的不断发展，算力规模也越来越大，浅层的技术才慢慢演化成了深层的技术。</p><p></p><p>举个很简单的例子，我们用词向量表示 Apple 这个词，可能就只有一个向量来表示它，有时这个向量表示的是水果，有时表示的是苹果公司。甚至还有一些其他的表意，可能都含在这一个词向量里边了。</p><p></p><p>而深层的预训练技术能解决什么问题？就是我们对上下文，通过更多的神经网络层去建模，这样就可以建出更加丰富的语义信息。相当于这个算法对自然语言的单词、句子、段落，出现的场景会理解得更加深一点。简单来讲就是，当模型足够大，足够深，又读过特别多的训练数据之后，能力自然就提升上来了，能力提升之后，再去做一些其他的事情可能就更加得心应手一点。</p><p></p><h4>InfoQ：对于预训练大模型来说，是参数越大就越好吗？</h4><p></p><p></p><p>相洋：这是一个争议很大的问题。我们最初见到的一些模型是几万个参数，后来就到了几亿、几十亿、百亿、千亿，还有可能上万亿。目前从事实来说，的确是模型越大数据越多，且质量越好，带来的性能是越高的。但是我个人认为，这个提升曲线可能会有一个瓶颈期，到了瓶颈或者平台期的时候，它的上升速度可能就会缓慢，或者说基本就达到稳定了。就目前而言，可能我们还没有到达平台期。所以说，“模型参数越大越好”这个说法在一定程度上是成立的。</p><p></p><h4>InfoQ：判断一个大模型是否优秀，需要关注哪几方面？</h4><p></p><p></p><p>相洋：要看实际表现，用大模型得出来的任务效果好，我们就可以认为这个模型是个好模型。一般来说，我们可能认为一个预训练模型影响的不只是一个任务，而是一系列任务。比如，我们训练了一个 BERT，如果只在 A 任务上的效果好，就不能认为它是一个特别成功的模型。如果它在 ABCDE 多个任务上的泛化性，或者是提升能力都特别好，我们才认为它算是比较好的模型。</p><p></p><p>我认为参数不是问题，当机器无论是在存储还是计算能力都足够强的时候，大模型也可以变成小模型。</p><p></p><p>此外，现在很多 AI 模型在可解释能力上是有所欠缺的，也就是说，人工智能模型是一个黑盒子，我们并不清楚它内部做了一些什么魔法，我们输进去一个内容，模型就会输出来一个内容，但是我们并不知道它是怎么得来的。现在我们可能会更理性、更辩证地去看待大模型，比如会考虑到它的可解释能力，如果它有一定的解释能力，我就认为这个模型是一个好模型，这是一个评价标准。</p><p></p><p>另一个评价模型优劣的标准是，它是否容易受噪声的攻击。比如，我加进去一些有噪声的样本，原来模型学到的东西立马就变了，从原来预测为 1，变成预测为 0 了，这就涉及到模型的鲁棒性问题。如果一个模型不易被这样的噪声数据，或者是其他因素影响的话，我们认为它就是一个好的模型。</p><p></p><h4>InfoQ：如何评价这几年预训练大模型的整体发展？从发展历程上来看，预训练大模型经过了哪些重要阶段，有没有一些超出您预期的进展或成果？</h4><p></p><p></p><p>相洋：第一阶段是前面提到的浅层的预训练模型阶段，大概是从 2013 年开始，到 2017 年下半年 Transformer 问世，这也可以看做是大规模预训练模型的起步阶段。</p><p></p><p>第二阶段，模型开始野蛮生长。从一开始是几百兆的规模，发展到百亿、千亿、万亿，再后来发展成了要用一些其他的技术，比如并不是传统的稠密模型，而是新的稀疏模型，它的连接并不是全连接的，可能会通过一些混合专家系统(MoE)技术去实现更加大型的、超过万亿或者是十万亿的模型。</p><p></p><p>横向来讲，大概分为两类，一类叫自编码，包括像 BERT 这一系列的模型。这类模型在训练过程中把一个单词盖住，用周围其他的单词去预测这个单词，重建这个句子。另一类叫做自回归，以 GPT 为代表，这类模型在训练过程中会把最后一个单词空出来，用前面这些单词去预测这个单词，这样循环迭代，从而把一个句子预测出来。</p><p></p><p>整体而言，大模型的发展是超出我的预期的。本来预训练大模型能取得成功已经超乎大家预期了，没想到在深度学习出现在这么多年之后，针对原来的技术又突然产生了一些革命性的变革，或者是一些颠覆性的成果。比如，原来的一些任务预测的准确率只能到 30%-40%，现在一下提到了 70%-80%，这是一个特别颠覆的成果，已经给我们带来了很多意想不到的收获。</p><p></p><p>随着大模型技术纵向、横向的扩展和更新，也有一些新的研究方向延伸出来，并取得了不小的关注。比如，一些新的大模型更加倾向于小样本或者是 0 样本学习，等等。</p><p></p><p>从传统意义上来讲，深度学习相对于传统的统计机器学习，数据要多一点，几十条、几百条的数据可能训练不出一个特别好的深度学习模型，几千条或者几万条、几十万条的数据才能得到比较好的效果。有了预训练模型之后，我们会发现，它所需要的数据量变少了。</p><p></p><p>此外，还有一个新技术叫做 Prompt，即提示。具体来说，在训练完大模型之后，应用的时候加一个提示，能更加有利于模型去做一些小的任务，并进一步减少模型训练所需要的数据量。这个技术是我这两年看到的，算是在大模型之后比较有突破的技术，也算是带领大模型实现了一个飞跃。</p><p></p><h4>InfoQ：预训练大模型从技术向产业落地的路径是什么？</h4><p></p><p></p><p>相洋：据我了解，百度、阿里、腾讯、科大讯飞等公司都已经在用大模型了，我认为产业落地不是一个问题。比如百度文心大模型，比百度之前的搜索效率提高了 40% 以上，带来了很多效能上的提升，说明至少在这些互联网公司中，大模型已经开始落地了，而且落地的效果还挺好。</p><p></p><p>作为企业来讲，想要用好大模型，首先，要确定需求到底是什么，你的需求是通过一个简单的模型就能解决，还是需要这样的大模型才能解决。企业需要去平衡一下大模型的能力到底是不是你需要的，它的投入产出比是否合适，性价比高不高，如果没有问题就可以去用。</p><p></p><p>另外，企业也要评估一下自己到底有没有这样的实力，比如，有很多大模型的参数量很大，模型也很大，要是部署到自己的机器上，或者部署到某些云上的时候，需要有很好的资源条件才能用起来。因此，企业在采用大模型之前也需要评估，是否值得投入大量的硬件设备。</p><p></p><h2>面向未来的 NLP 模型</h2><p></p><p></p><h4>InfoQ：下一代 NLP 模型的发展趋势是什么样的？在未来有哪些值得关注的方向？</h4><p></p><p></p><p>相洋：第一，传统意义上，大模型需要大量的数据、算力，训练很久才能训练出来。那么，我们如何才能更高效地去训练大模型？这个问题肯定是我们在未来需要解决的。如果在未来，我们能够在有限的资源下，快速地对大模型进行更新迭代，这对于人工智能产业将大大有利。</p><p></p><p>第二，就是模型的可解释性。这一直是机器学习模型，包括现在的深度学习、大模型遇到的一个难点，将来我们如何通过一些知识融入，包括因果推理等等，去进一步增强模型的解释性，可能也是一个非常热门的方向。</p><p></p><p>第三，最近我看到很多社区、微信群里在使用百度文心做的文本生成图像工具，输入一段文字，它就可以根据这些词去生成一幅图画。这是一个多模态的问题，从文本到图像的生成过程。这里面还体现了一个问题，即可控文本生成，也就是说，生成的内容是经过一些控制去生成的，而不是随意生成的。现在大模型生成文本的过程很多时候是不可控的，它可能生成的是一句话，但这句话只能保证语法和流畅度，它在逻辑或是常识性等方面是比较缺乏的。我们怎么通过技术去让这个模型生成的句子、图像更加可信、可靠？这也未来需要关注的一个方向。</p><p></p><p>第四，我认为 NLP 以及大模型能在更多领域发挥作用，比如现在比较热门的元宇宙、VR、AR 等方面，能结合现在的一些可视化元素，让虚拟的场景、虚拟的人以及虚拟的社区，更加具有人性化。有自然语言做加持，人与人之间的沟通可能在元宇宙中更加通畅。</p><p></p><h4>InfoQ：在未来，NLP 技术在元宇宙中是否会有一些具体的落地应用？</h4><p></p><p></p><p>相洋：我也一直在思考这个问题。现在很多人在讲元宇宙虚拟主播的概念，虚拟主播除了外形看起来比较科幻，还是需要具备一些与人类交互的能力。这就涉及到人机交互非常重要的一环，即怎么去理解人类的语言，然后生成人类的语言，表达自己的感情，甚至是发出自己的声音。这些可能都跟 NLP 技术相关。</p><p></p><h2>如何才能快速入门 NLP？</h2><p></p><p></p><h4>InfoQ：有用户提问，现在 NLP 方向的就业前景怎么样？</h4><p></p><p></p><p>相洋：NLP 方向的就业前景还是可以的，现在很多大厂比如 BAT、华为、字节跳动、科大讯飞等等，还有一些初创企业，都需要大量的 NLP 人才。而且当你掌握了一些 NLP 技术之后，你也可以从事与 NLP 相关的其他工作，比如推荐等等。如果大家对这个方向感兴趣，希望大家能够进一步增强自身的实力，把 NLP 的技术掌握到极致，这样将来找工作也是没有问题的。</p><p></p><h4>InfoQ：小白如何才能快速入门 NLP？有哪些书籍推荐吗？</h4><p></p><p></p><p>相洋：第一，大家需要先去了解到底什么是 NLP，NLP 能干什么。可以去网上看专家讲解、视频报告，也可以与自己周围的老师、同学、前辈多交流一下。先把 NLP 概念以及要做得事情搞明白，明确自己是不是对 NLP 这项技术，或者对 NLP 其中的某几项技术感兴趣。</p><p></p><p>第二，当你明确自己确实想往 NLP 方向发展的时候，就要针对自己关注的一项技术去找网上的公开课，比如斯坦福等学校有很多专家课专门讲自然语言处理、深度学习、机器学习等等，在掌握一些基础知识后，还要看一些 NLP 相关的论文，或是论文解读、相关文章，等等，这能够帮助大家更快地去理解一些 NLP 的知识。</p><p></p><p>第三，动脑和动手需要结合，不能只去看理论。希望大家可以从社区，比如启智社区、GitHub 等等，找一些比较有代表性的算法以及模型库，自己拿过来跑一跑，针对自己感兴趣的问题，可以尝试调改，把理论学习和实践相结合，才是一个比较完整有效的入门 NLP 的方式。</p><p></p><p>书籍方面，我最初接触 NLP 的时候，用的是李航老师写的《统计学习方法》。此外，还有一本复旦大学邱锡鹏老师写的《神经网络与深度学习》，这本书相对比较新，里面涉及到了很多神经网络、深度学习以及自然语言处理比较新的技术，包括预训练技术，等等。还有一本比较传统的书籍叫做《Deep learning》，自然语言处理离不开机器学习、深度学习，在这本书里，除了理论，还重点讲了一些自然语言处理例子，大家可以做参考。从理论到实践，循序渐进去掌握这些知识。</p><p></p><h4>InfoQ：有用户提问，NLP 算法工程师的核心竞争力是什么？</h4><p></p><p></p><p>相洋：是 NLP 算法底层的一些算法理解能力，而并不是应用能力。当你能够把 NLP 的一些底层原理、底层算法掌握好，融会贯通之后，才能够对模型的改进、使用、集成等更加得心应手，我认为这才是 NLP 算法工程师的核心竞争力。</p><p></p><h2>开源是现代 AI 软件发展的必然趋势</h2><p></p><p></p><h4>InfoQ：您是如何理解开源，以及开源的价值？</h4><p></p><p></p><p>相洋：在<a href=\"https://www.infoq.cn/article/J66xNisfoG4V1cCyfmYw\">开源</a>\"领域我还是个入门者，过去没有太多考虑开源的问题，近几年，鹏城实验室在开源方面做了很多工作，包括 OpenI 的理事长就是鹏城实验室主任高文院士，我们也会更多地参与到开源工作中，比如我们专门有一支团队去从事 OpenI 启智社区的建设，还有运维等工作。</p><p></p><p>我现在对开源的理解可能还是比较浅显的，这也是我个人的一些见解。我认为，开源可能是现代 AI 软件的大势所趋，它不同于过去那种自顶向上的模式，它是自底向下的，我们从大众出发，从开发者出发，从用户出发去做一些事情，从需求入手去把开源的产品不断地更新、迭代出来。这种模式对于开发者来讲，可能成本会更低；对于企业来讲也能够快速地对新产品进行迭代；对于用户来讲，也可以快速地去得到他想要的一些功能或产品，用户也可以通过开源社区，把自己的需求直接提给原创的开发者，进而开发者可以快速地解决用户的需求。</p><p></p><p>总体来讲，现在这种开源的模式能够进一步增强 AI，或者是机器学习产品的更新和迭代。</p><p></p><h4>InfoQ：OpenI 社区最吸引您的地方是什么？未来将会在哪些方面助力开源？</h4><p></p><p></p><p>相洋：我之前一直在用 GitHub，回国之后发现，我自己以及我的团队都在抢计算资源，但是在 OpenI 社区，它们提供了很多免费的资源，我们很容易就能抢到英伟达的 A100，或者昇腾 910 这样高性能的显卡，这是比较吸引我的点之一。</p><p></p><p>另一方面，OpenI 为了方便我们应用这些显卡，建立了一套流水线式的程序训练，还有数据上传的模式，这一套工具能够方便我们更好地去利用这些资源，让我们更快地进行模型的训练和部署。</p><p></p><p>开源项目方面，目前我们有一个课题专门做开源算法库，并贡献一些 NLP 相关的算法库到 OpenI 社区中，也有强化学习、图神经网络等技术方向的开源项目。此外，我们云计算所也承担了一些 OpenI 启智社区相关的建设和运营工作，我们也会协助社区做更多的推广和开源活动，来进一步提高社区的影响力。</p><p></p><p>现在，OpenI 启智社区已经接入了中国算力网的算力。除了鹏城云脑，中国算力网还连入了全国其他地方的智算、超算的一些集群、机器，我们都可以使用。相当于我们帮助社区把它的算力网的算力底座给建设好了，这也是对社区的一个非常大的贡献。</p><p></p><h4>InfoQ：接下来您在科研方面有哪些规划，对于想加入鹏城实验室的网友，有哪些寄语？</h4><p></p><p></p><p>相洋：我们这个的团队是做自然语言处理的，主要和预训练模型相关，包括怎么去更高效地训练模型，怎么做模型的压缩、剪枝、蒸馏，还有模型的推理能力、解释能力等等。</p><p></p><p>此外还有很多自然语言处理方向的应用，比如机器翻译、情感分析、医学 NLP 等等。这几个技术领域我们在 OpenI 启智社区也有开源算法，大家可以去搜索 <a href=\"https://git.openi.org.cn/PCLNLP\">https://git.openi.org.cn/PCLNLP</a>\"，可以找到我们团队在 OpenI 上面的主页，里面也有很多开源的算法库。</p><p></p><p>对鹏城实验室感兴趣的小伙伴，我想说的是，鹏城实验室具有非常好的资源，比如在这里可以跟许多院士、学术界的权威专家一起讨论问题，交流合作。另外，我们跟华为、腾讯、百度等头部企业有深度的合作，比如联合实验室等等，也会经常跟这些企业的专家们进行联合开发、联合攻关。我认为这是非常难得的机会，我们能够与学术大佬在一起交流合作的同时，也能跟企业的前辈、专家们一同开发、一同学习。这是鹏城实验室所具备的一些特有的优势，也希望想获得更多机会、更多机遇的小伙伴们能够关注鹏城实验室。</p>",
    "publish_time": "2022-11-11 08:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Kafka 3.3使用KRaft共识协议替代ZooKeeper",
    "url": "https://www.infoq.cn/article/j1jm5Qehr1jiEQubY0ot",
    "summary": "<p>Apache软件基金会<a href=\"https://www.mail-archive.com/announce@apache.org/msg07620.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">发布</a>\"了包含许多新特性和改进的Kafka 3.3.1。这是第一个标志着可以在生产环境中使用<a href=\"https://developer.confluent.io/learn/kraft/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">KRaft</a>\"（Kafka Raft）共识协议的版本。在几年的开发过程中，它先是在Kafka 2.8早期访问版本中发布，然后又在Kafka 3.0预览版本中发布。</p><p></p><p>KRaft是一种共识协议，可以直接在Kafka中管理元数据。元数据的管理被整合到了<a href=\"https://kafka.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">Kafka</a>\"当中，而不需要使用像<a href=\"https://zookeeper.apache.org/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">ZooKeeper</a>\"这样的第三方工具，这大大简化了Kafka的架构。这种新的KRaft模式提高了分区的可伸缩性和弹性，同时简化了Kafka的部署，现在可以不依赖ZooKeeper单独部署Kafka了。</p><p></p><p>KRaft使用了<a href=\"https://raft.github.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">Raft</a>\"共识算法的一种基于事件的变体，因此得名。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/10/apache-kafka-kraft/en/resources/2event-driven-consensus-internals-1666684453761.png\" /></p><p></p><p>随KRaft引入的新的仲裁控制器确保元数据在整个仲裁中可以被准确复制。活动控制器将元数据存储在事件源日志主题中，仲裁中的其他控制器对活动控制器创建的事件做出响应。事件日志定期进行快照，确保日志不会无限增长。与基于ZooKeeper的控制器不同，如果出现了问题，仲裁控制器不需要从ZooKeeper加载状态，因为集群的内部状态已经分布在元数据主题中。这大大减少了不可用时间窗口，缩短了系统最坏情况恢复时间。</p><p></p><p>下图显示了使用新的仲裁控制器比使用ZooKeeper更快地关闭具有200万个分区的Kafka集群。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/10/apache-kafka-kraft/en/resources/1timed-shutdown-1666684453761.jpg\" /></p><p></p><p>新的KRaft共识算法和仲裁控制器使得Kafka集群可以扩展到数百万个分区，不仅提升了稳定性，让Kafka变得更容易监控、管理和支持，而且让整个系统可以有一个单一的安全模型，使控制器故障转移接近瞬时。</p><p></p><p>Kafka社区计划在下一个版本（3.4）中弃用ZooKeeper，然后在4.0版本中完全删除它。</p><p></p><p>此外，Kafka 3.3还提供了其他一些新特性，比如添加了与元数据日志处理错误相关的指标，允许用户为其他用户创建委托令牌，以及严格统一的粘性分区器，以缩短分区时间。</p><p></p><p>对于Kafka Streams，这个版本增加了源/接收器指标，如消费/生产吞吐量、暂停/恢复拓扑，并集成了KStream transform()和process()方法。Kafka Connect增加了对源连接器的<a href=\"https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">精确一次</a>\"语义支持。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/10/apache-kafka-kraft/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjgxMjk0MjIsImZpbGVHVUlEIjoiVnpnWTNVck9FeXdJT3RVSCIsImlhdCI6MTY2ODEyOTEyMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.wSHkfjFZViJesOxgPpH2s_F32DBlypcdpMnW604pbc0\">https://www.infoq.com/news/2022/10/apache-kafka-kraft/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/ElNtSM5ISobpMB8fMC0j\">使用 Strimzi 将&nbsp;Kafka&nbsp;和 Debezium 迁移到 Kubernetes</a>\"</p><p><a href=\"https://xie.infoq.cn/article/eebc47cc789df57c040f6a5c2\">Flink 读写多套 Kerberos 认证的&nbsp;Kafka&nbsp;方案</a>\"</p><p></p>",
    "publish_time": "2022-11-11 09:21:59",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“天猫双11”背后的流量治理技术与标准实践",
    "url": "https://www.infoq.cn/article/rxMB5sk9Ix0oBRLiDzb4",
    "summary": "<p></p><p>作者 | 赵奕豪 (宿何)：Sentinel &amp; OpenSergo 开源项目负责人</p><p></p><p>一年一度的天猫双 11 正在火热进行中，大家在疯狂买买买的过程中一定会有疑问：如何保障微服务在双十一的超级峰值下也能如丝般顺滑稳定？这背后的技术原理是怎样的，有没有一些最佳实践与标准？这篇文章就为大家介绍如何结合 Sentinel 与 OpenSergo 玩转双十一背后的流量治理技术与标准。</p><p></p><h2>OpenSergo 是什么？</h2><p></p><p>业界微服务治理存在概念不统一、配置形式不统一、能力不统一、多框架统一管控较为复杂等问题。比如我们希望对某个接口配置熔断，在 Hystrix 中可能需要使用 HystrixCommand 中的配置项进行配置，在 Sentinel 中可能需要通过 Sentinel 动态规则的方式进行配置，在 Istio 中可能又是另一套配置方式。不同框架治理配置方式的不一致使得微服务统一治理管控的复杂度相当高。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/57ea0d698be407fe66793c1cbd8f44cd.png\" /></p><p></p><p>基于以上背景，由 阿里云、bilibili、中国移动、SphereEx 等企业及 Kratos、CloudWeGo、ShardingSphere、Database Mesh、Spring Cloud Alibaba、Dubbo 等社区共同发起的 OpenSergo 项目 应运而生。OpenSergo 是开放通用的，覆盖微服务及上下游关联组件的微服务治理项目，从微服务的角度出发，涵盖流量治理、服务容错与自愈、服务元信息治理、安全治理等关键治理领域，提供一系列的治理能力与标准、生态适配与最佳实践，支持 Java, Go, Rust 等多语言生态。OpenSergo 的最大特点就是以统一的一套配置 /DSL/ 协议定义服务治理规则，面向多语言异构化架构，覆盖微服务框架及上下游关联组件。无论微服务的语言是 Java, Go, Node.js 还是其它语言，无论是标准微服务还是 Mesh 接入，从网关到微服务调用，再到微服务对数据库 / 缓存的访问，开发者都可以通过同一套 OpenSergo CRD 标准配置进行统一的治理管控，而无需关注各框架、语言的差异点，降低异构化、全链路微服务治理管控的复杂度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e15b0bfbcdadc5d41d9a414dde655f17.png\" /></p><p></p><p>OpenSergo 涵盖的微服务治理关键领域：</p><p>流量治理与服务容错：流量路由、流量染色、全链路灰度、流量防护与自愈（流量控制、服务熔断、容错防抖）微服务视角的数据库与缓存治理：端侧连接池治理、读写流量路由、SQL 流控等服务元信息与服务发现</p><p>OpenSergo 提供 Java、Go 等多语言的 SDK，各个框架生态可以非常方便地通过 OpenSergo SDK 来对接 OpenSergo 标准配置，接入到 OpenSergo 生态中，通过 OpenSergo 控制平面 (Control Plane) 统一管理服务治理规则。</p><p></p><h2>为什么需要流量防护与容错？</h2><p></p><p>微服务的稳定性一直是开发者非常关注的话题。随着业务从单体架构向分布式架构演进以及部署方式的变化，服务之间的依赖关系变得越来越复杂，业务系统也面临着巨大的高可用挑战。大家可能都经历过以下的场景：</p><p>演唱会抢票瞬间洪峰流量导致系统超出最大负载，load 飙高，用户无法正常下单；在线选课时同一时刻提交选课的请求过多，系统无法响应；页面服务中某一块内容访问很慢，一直加载不出来，导致整个页面都卡住，无法正常操作</p><p>影响微服务可用性的因素有非常多，而这些不稳定的场景可能会导致严重后果。我们从微服务流量的视角来看，可以粗略分为两类常见的运行时场景：</p><p>服务自身流量超过承载能力导致不可用。比如激增流量、批量任务投递导致服务负载飙高，无法正常处理请求。服务因依赖其他不可用服务，导致自身连环不可用。比如我们的服务可能依赖好几个第三方服务，假设某个支付服务出现异常，调用非常慢，而调用端又没有有效地进行预防与处理，则调用端的线程池会被占满，影响服务自身正常运转。在分布式系统中，调用关系是网状的、错综复杂的，某个服务出现故障可能会导致级联反应，导致整个链路不可用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aac193675d654a35e216365f345e567d.png\" /></p><p></p><p>在遇到这些微服务运行时稳定性的问题时，我们应该如何解决呢？针对这些不稳定的场景，阿里巴巴开源的 Sentinel 提供全方位的流量防护能力，以流量为切入点，从流量控制、并发控制、不稳定服务熔断、热点防护、系统自适应过载保护等多个维度来帮助保障服务的稳定性，覆盖微服务框架、云原生网关、Service Mesh 等几大场景，原生支持 Java、Go、C++、Rust 等多种语言的异构微服务架构。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/629a438174ef9f04d91f55337a660261.png\" /></p><p></p><p>Sentinel 底层基于精心设计的高性能毫秒级滑动窗口统计结构来实现百万 QPS 流量的精确统计，结合上层各个流量治理策略模块的组合来实现对不同维度的流量进行治理，同时支持灵活的扩展定制机制。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3be203b2444bb757fcd85aebbd7bc252.png\" /></p><p></p><p>Sentinel 在阿里巴巴内部承载非常多的服务可用性与容错的场景，保障了近十年天猫双 11 流量峰值的稳定。在阿里云上，我们也在 MSE 微服务引擎产品中提供全方位的流量防护与治理能力，帮助大量企业保障微服务的稳定性。</p><p></p><h2>OpenSergo 流量防护与容错标准</h2><p></p><p>在 OpenSergo 中，我们结合 Sentinel 等框架的场景实践对流量防护与容错抽出标准 CRD。一个容错治理规则 (FaultToleranceRule) 由以下三部分组成：</p><p>Target: 针对什么样的请求。可以通过通用的 resourceKey（Sentinel 中即为资源名的概念）来标识，也可以用细化的规则来标识（如具有某个特定 HTTP header 的请求）Strategy: 容错或控制策略，如流控、熔断、并发控制、自适应过载保护、离群实例摘除等FallbackAction: 触发后的 fallback 行为，如返回某个错误或状态码</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14b794a4ec588a86ae6dab71446ee12d.png\" /></p><p></p><p>无论是 Java 还是 Go 还是 Mesh 服务，无论是 HTTP 请求还是 RPC 调用，还是数据库 SQL 访问，我们都可以用这统一的容错治理规则 CRD 来给微服务架构中的每一环配置容错治理，来保障我们服务链路的稳定性。只要微服务框架适配了 OpenSergo，即可通过统一 CRD 的方式来进行流量防护等治理。</p><p>以下 YAML CR 示例定义的规则针对 path 为 /foo 的 HTTP 请求（用资源名标识）配置了一条流控策略，全局不超过 10 QPS。当策略触发时，被拒绝的请求将根据配置的 fallback 返回 429 状态码，返回信息为 Blocked by Sentinel，同时返回 header 中增加一个 header，key 为 X-Sentinel-Limit, value 为 foo。</p><p></p><p>apiVersion: fault-tolerance.opensergo.io/v1alpha1</p><p>kind: RateLimitStrategy</p><p>metadata:</p><p>  name: rate-limit-foo</p><p>spec:</p><p>  metricType: RequestAmount</p><p>  limitMode: Global</p><p>  threshold: 10</p><p>  statDuration: \"1s\"</p><p>---</p><p>apiVersion: fault-tolerance.opensergo.io/v1alpha1</p><p>kind: HttpRequestFallbackAction</p><p>metadata:</p><p>  name: fallback-foo</p><p>spec:</p><p>  behavior: ReturnProvidedResponse</p><p>  behaviorDesc:</p><p>    # 触发策略控制后，HTTP 请求返回 429 状态码，同时携带指定的内容和 header.</p><p>    responseStatusCode: 429</p><p>    responseContentBody: \"Blocked by Sentinel\"</p><p>    responseAdditionalHeaders:</p><p>      - key: X-Sentinel-Limit</p><p>        value: \"foo\"</p><p>---</p><p>apiVersion: fault-tolerance.opensergo.io/v1alpha1</p><p>kind: FaultToleranceRule</p><p>metadata:</p><p>  name: my-rule</p><p>  namespace: prod</p><p>  labels:</p><p>    app: my-app</p><p>spec:</p><p>  selector:</p><p>    app: foo-app # 规则配置生效的服务名</p><p>  targets:</p><p>    - targetResourceName: '/foo'</p><p>  strategies: </p><p>    - name: rate-limit-foo</p><p>  fallbackAction: fallback-foo</p><p></p><h2>Sentinel 原生支持 OpenSergo 流量防护与容错标准</h2><p></p><p>Sentinel 2.0 品牌将升级为流量治理，并作为 OpenSergo 流量治理的标准实现。Sentinel 目前已原生支持 OpenSergo 流量防护与容错 spec（流控、匀速排队、熔断、并发控制等规则），结合 Sentinel 提供的各框架的适配模块，让 Dubbo, Spring Cloud Alibaba, gRPC 等 20+ 框架能够无缝接入到 OpenSergo 生态中，用统一的 CRD 来配置流控、异常熔断等治理规则。只要微服务框架适配了 Sentinel，即可通过统一 CRD 的方式来进行流量治理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/abcda3ca4c463796e88b23f83926b1a4.png\" /></p><p></p><p>Sentinel 社区近期已提供 OpenSergo spec 初版适配，欢迎社区一起参与完善：</p><p>Sentinel Java OpenSergo data-source: https://github.com/alibaba/Sentinel/tree/master/sentinel-extension/sentinel-datasource-opensergoSentinel Go OpenSergo data-source (work-in-progress): https://github.com/alibaba/sentinel-golang/pull/489</p><p>Sentinel 利用 OpenSergo 多语言 SDK 来订阅指定应用的流量防护规则，结合 Sentinel 数据源扩展机制，来实现 OpenSergo 流量防护与容错 spec 的整合模块。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/733c0934fb1d2267c44a1713e8640e83.png\" /></p><p></p><p>下面以 Java 社区为例，我们来演示一下如何在 Sentinel 接入 OpenSergo 数据源并通过 OpenSergo Control Plane 动态配置流控规则。目前 OpenSergo Control Plane 支持部署在 Kubernetes 集群中，通过 OpenSergo CRD 来管理规则。</p><p>第一步：首先我们先在 Kubernetes 集群中安装 OpenSergo CRD，并在 Kubernetes 中部署 OpenSergo Control Plane 实例。具体步骤可以参考 OpenSergo Control Plane 项目的文档。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a95ed97f089215657941f1475fc893f1.png\" /></p><p></p><p>第二步：假定我们是一个 Spring Boot Web 项目，已接入 Sentinel。我们在项目中引入 sentinel-datasource-opensergo 数据源模块：</p><p></p><p></p><p>    com.alibaba.csp</p><p>    sentinel-datasource-opensergo</p><p>    <!-- 对应 Sentinel 1.8.6 版本 --></p><p>    0.1.0-beta</p><p></p><p></p><p>第三步：在项目合适的位置（如 Spring 初始化 hook 或 Sentinel InitFunc 中）中创建并注册 Sentinel OpenSergo 数据源：</p><p></p><p>// 传入 OpenSergo Control Plane 的 endpoint，以及希望监听的应用名.</p><p>// 在我们的例子中，假定应用名为 foo-app</p><p>OpenSergoDataSourceGroup openSergo = new OpenSergoDataSourceGroup(\"localhost\", 10246, \"default\", \"foo-app\");</p><p>// 初始化 OpenSergo 数据源.</p><p>openSergo.start();</p><p>// 订阅 OpenSergo 流控规则，并注册数据源到 Sentinel 流控规则数据源中.</p><p>FlowRuleManager.register2Property(openSergo.subscribeFlowRules());</p><p></p><p>第四步：启动应用，访问项目中的 Web 接口，在没有配置规则的情况下应该一直返回正常结果。接下来我们按 OpenSergo CRD 的方式针对 /foo 这个 Web 接口配置一个 QPS=2 的流控规则：</p><p></p><p>apiVersion: fault-tolerance.opensergo.io/v1alpha1</p><p>kind: RateLimitStrategy</p><p>metadata:</p><p>  name: rate-limit-foo</p><p>  labels:</p><p>    app: foo-app</p><p>spec:</p><p>  metricType: RequestAmount</p><p>  limitMode: Local</p><p>  threshold: 2</p><p>  statDurationSeconds: 1</p><p>---</p><p>apiVersion: fault-tolerance.opensergo.io/v1alpha1</p><p>kind: FaultToleranceRule</p><p>metadata:</p><p>  name: my-opensergo-rule-1</p><p>  labels:</p><p>    app: foo-app</p><p>spec:</p><p>  targets:</p><p>    # 这里对应 Sentinel 的资源名，根据实际情况填写</p><p>    - targetResourceName: '/foo'</p><p>  strategies:</p><p>    - name: rate-limit-foo</p><p>      kind: RateLimitStrategy</p><p></p><p>我们将这个配置保存为 YAML 文件，然后通过 kubectl apply 到集群中。我们查看 Sentinel 日志目录（默认目录为 ~/logs/csp）下的 sentinel-record.log，可以看到规则已经成功下发到 Sentinel 侧。</p><p><code lang=\"null\">2022-10-26 14:26:59.390 INFO Subscribing OpenSergo base fault-tolerance rules for target 2022-10-26 14:26:59.391 INFO Subscribing OpenSergo config for target: SubscribeKey{namespace='default', app='foo-app', kind=RATE_LIMIT_STRATEGY}2022-10-26&nbsp;14:27:59.552&nbsp;INFO&nbsp;[FlowRuleManager]&nbsp;Flow&nbsp;rules&nbsp;received:&nbsp;{/foo=[FlowRule{resource=/foo,&nbsp;limitApp=default,&nbsp;grade=1,&nbsp;count=2.0,&nbsp;strategy=0,&nbsp;refResource=null,&nbsp;controlBehavior=0,&nbsp;warmUpPeriodSec=10,&nbsp;maxQueueingTimeMs=500,&nbsp;clusterMode=false,&nbsp;clusterConfig=null,&nbsp;controller=com.alibaba.csp.sentinel.slots.block.flow.controller.DefaultController@4bbadef7}]}</code></p><p>同时我们再去连续触发 /foo 这个 Web 接口，可以看到，同一秒前两次请求是正常通过的，后面的请求会被拒绝，拒绝效果为默认的 429 返回。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3a/3a43960224214423e5e8a8d0f66c191c.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24defb44bdd6c70ec0c7da53a63d2ee9.png\" /></p><p></p><p>后续 Sentinel 2.0 也会支持通过 OpenSergo FallbackAction CRD 来动态配置 fallback 行为，比如指定返回值与返回状态码等，无需代码编写逻辑。</p><p></p><p></p><h2>展望</h2><p></p><p>流量防护与容错是微服务流量治理中的重要的一环，同时 OpenSergo 还提供更广范围、更多场景的微服务治理标准与最佳实践，包括流量路由、流量染色、微服务视角的数据库治理、日志治理等一系列的微服务治理能力与场景。服务治理是微服务改造深入到一定阶段之后的必经之路，是将微服务做稳做好的关键。同时我们也在与 CloudWeGo、Kratos、Spring Cloud Alibaba、Dubbo、ShardingSphere、Database Mesh 等社区共同建设 OpenSergo 微服务治理标准，将企业与社区中微服务治理的场景与最佳实践共同提取成标准规范，也欢迎更多社区与企业一起参与 OpenSergo 微服务治理标准的共建。</p><p>OpenSergo 社区现在处于高速发展阶段，从微服务治理标准定义，到 Control Plane 的实现，再到 Java/Go/C++/Rust 等多语言 SDK 与治理功能的实现，再到各个微服务生态的整合与落地，都还有大量的演进工作，欢迎社区一起参与标准完善与代码贡献。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/4527d83b1f79682ecbaa3848777b11f9.png\" /></p><p></p><p>OpenSergo 开源贡献小组正在火热招募贡献者。如果您有时间，有热情，有意愿，欢迎联系社区加入开源贡献小组，一起共同完善 OpenSergo 和 Sentinel，一起主导微服务治理技术与标准演进。Now let's start hacking!</p><p></p><p>欢迎关注 OpenSergo 社区微信公众号，了解微服务治理社区最新动态：OpenSergo（可以附公众号引流跳转）</p><p></p><p>相关链接：</p><p>Sentinel 项目官网：https://sentinelguard.io/zh-cn/OpenSergo 项目官网：https://opensergo.io/zh-cn/MSE Sentinel 流量治理：https://help.aliyun.com/document_detail/420631.html</p>",
    "publish_time": "2022-11-11 13:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]