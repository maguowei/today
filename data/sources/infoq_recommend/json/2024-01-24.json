[
  {
    "title": "亚马逊云科技将关闭 Aurora Serverless v1 服务",
    "url": "https://www.infoq.cn/article/yF7s2buZfbjX61eBEhs0",
    "summary": "<p>最近，亚马逊云科技向正在运行 Amazon Aurora 的现有客户发出通知，将停止对无服务器 v1 的支持，并计划在一年内关闭该服务。新的 Aurora Serverless v2 没有零伸缩特性，这引发了社区对潜在的使用成本上升以及 AWS 平台上缺失“真正”无服务器关系型数据库的担忧。</p><p></p><p>于 2018 年夏天推出的 Aurora Serverless v1 不支持多 AZ 集群，没有集成最新的 MySQL 和 PostgreSQL 主要版本。但是，在没有活动流量的情况下，集群规模可以自动缩小到零。亚马逊云科技在给现有客户的电子邮件中写道：</p><p></p><p></p><blockquote>从 2024 年 12 月 31 日起，Amazon Aurora 将不再支持 Serverless v1。根据 Aurora 的版本政策，我们将提前 12 个月通知您升级数据库集群。Aurora 支持两个版本的 Serverless。我们只是宣布结束对 Serverless v1 的支持，但对 Aurora Serverless v2 的支持仍然继续。我们建议您在 2024 年 12 月 31 日之前自行主动将运行 Amazon Aurora Serverless v1 的数据库升级到 Amazon Aurora Serverless v2。</blockquote><p></p><p></p><p>JPMorgan Chase＆Co 云解决方案架构负责人 Ganesh Swaminathan 在评论中表示：</p><p></p><p></p><blockquote>再见了，一个可以自动伸缩到零的关系数据库。你好，翻倍的账单（或更多）。</blockquote><p></p><p></p><p>虽然 Aurora Serverless v2 通过引入副本、逻辑复制和全局数据库等特性大大缩小了与预配置 Aurora 之间的差距，但它无法伸缩到零。零伸缩特性对于开发和测试数据库来说非常重要，因为这些数据库可能会经历较长时间的不活动。Aurora Serverless v2 要求至少 0.5 个 Aurora 容量单位（ACU），这将导致不间断的使用成本。在 Reddit 的一个热门的帖子中，用户 zmose 写道：</p><p></p><p></p><blockquote>我对 Aurora Serverless V2 无法伸缩到 0 ACU 感到非常失望。你知道，“无服务器”现在意味着什么吗？我认为是每月至少约 50 美元的成本。</blockquote><p></p><p></p><p>大多数开发人员都在强调小规模部署的价格差异，用户 SteveTabernacle2 则评论道：</p><p></p><p></p><blockquote>这是原则问题。亚马逊云科技以前极力强调“零伸缩”，可以查看 2020 年之前的任意一个无服务器 re:Invent 演讲（可能在演讲的前 5 分钟内被提到）。但新一代的“无服务器”（Aurora v2、Elasticache、OpenSearch）不应该被称为无服务器。</blockquote><p></p><p></p><p>一些开发人员认为，期待已久的 RDS Data API v2 的发布表明这家云供应商正在努力弥合这两个版本之间的差距。亚马逊云科技首席数据库专家解决方案架构师 Tim Gustafson 解释了如何利用蓝 / 绿部署的优势，以最短的停机时间从 Aurora Serverless v1 升级到 v2：</p><p></p><p></p><blockquote>Aurora Serverless v1 有一条新的升级路径，让你可以从 Amazon Serverless v1 数据库迁移到一个预配置的 Aurora 集群，只需要 30 秒的故障转移时间，类似于你将 Aurora 读副本升级为新的写副本时会发生的情况。然后，你可以利用新的蓝 / 绿部署特性将数据库升级到 Aurora Serverless v2 支持的版本，并将无服务器实例添加到集群中。</blockquote><p></p><p></p><p>亚马逊云科技很少会淘汰服务，但独立顾问和 AWS Serverless Hero Yan Cui 指出：</p><p></p><p>不幸的是，Aurora Serverless v1 正在退出……这是罕见的亚马逊云科技淘汰服务的事件（你看，SimpleDB 都还在！）。</p><p></p><p>由于 Aurora Serverless v1 和 Aurora Serverless v2 支持不同版本的 MySQL 和 PostgreSQL 引擎，Gustafson 警告说：</p><p></p><p></p><blockquote>在迁移到 Amazon Serverless v2 之前，我们需要将数据库升级到 Amazon Serverless v2 支持的版本……进行主要版本升级需要做一些计划，并需要进行适当的测试。</blockquote><p></p><p></p><p>除了向 Aurora Serverless v1 现有用户发送电子邮件外，亚马逊云科技并未发布任何官方公告或分享有关该服务退役的详细路线图。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/aurora-serverless-v1-retirement/\">https://www.infoq.com/news/2024/01/aurora-serverless-v1-retirement/</a>\"</p><p></p><p></p><p></p><p></p>",
    "publish_time": "2024-01-24 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "纯向量数据库和向量插件都有局限，那未来发展有其他方向吗？",
    "url": "https://www.infoq.cn/article/Cj0RxJAR1ICQd0WXrVKW",
    "summary": "<p>导读：向量数据库的争议差不多一年了，但我们一直缺少一篇能透彻讲解向量数据库相关问题的文章，这导致在这个领域的讨论一直没有得到充分的澄清。在这篇文章中，我们将深入剖析向量数据库核心技术的争议点，解释其优势和局限性，为读者提供全面而清晰的了解。本文作者的原标题是《向量数据库路在何方？结合 RAG 的发展谈谈它的未来》。</p><p></p><p>数据库网红教授 Andy Pavlo 于 2024 年 1 月 4 日他的博客发表了 2023 年度数据库报告，正文开始就提到了向量数据库的兴起。对于所有数据库从业人员来说，都知道 2023 年是向量数据库的大年，这从 2023 年 3 月英伟达的黄仁勋在 GTC 大会上点名向量数据库开始，到 2023 年 4 月一系列向量数据库的巨额融资都可以感受出来。</p><p></p><p>截至 2023 第三季度，向量数据库的火热都是以它作为大语言模型的外挂记忆体作为场景定义而出现的，从 2023 年第四季度开始，这个场景的称谓，被更多的接纳为 RAG（Retrieval-Augmented Generation）——基于检索增强的内容生成， 并且 RAG 这个称谓逐渐流行，甚至已经有说法认为 2024 年是 RAG 元年。</p><p></p><p>因此在这里，我们结合 Andy 在博客中的观点，结合 2023 年以来 RAG 的出现和兴起，以及发展中出现的问题，给出我们对向量数据库未来发展的判断。</p><p></p><p></p><h2>RAG 的兴起与向量数据库</h2><p></p><p></p><p>RAG 一开始就致力于解决大语言模型（LLM）本身存在的一些问题。当 LLM 刚刚火热起来时，就涌现了对其如何处理个人或企业内部的私有数据进行回答的需求。如果一切都仅凭 LLM 本身，这会导致三个问题：</p><p></p><p>私有数据产生的任何变更，如果都送到 LLM 训练的话，那么如何保证实时性？如果任何数据都需要通过训练或者微调才能被回答出来，那如何解决这种成本太高的问题？LLM 的上下文 Token 数有限， 如果用户的数据量大到超过了 Token 数限制，如何让 LLM 回答问题？</p><p></p><p>针对这些问题，RAG 的做法是：把私有数据先用工具切分好，然后通过一个 Embedding 模型转成为向量保存到向量数据库。回答问题的时候，先把问题也转化为一条向量，再用该向量去数据库内进行 Top K 相似度比对，然后把返回的结果拼接成提示词，交给 LLM 回答，就可以满足对私有数据提问的需求。</p><p></p><p>RAG 的工作流程如下图：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/88/88b0721df5ae75de99732d3aa6ce517d.png\" /></p><p></p><p>由于这种搭载向量数据库的做法直接解决了上面三个痛点，因此它一出现，就引起了众多注意，这就是 2023 年上半年向量数据库火热的主要缘起。</p><p></p><p>在这个生态中，核心组件包含三个部分：LLM 负责最后问题的摘要和润色；向量数据库保存数据；还有 LangChain 和 llama_index 这样的中间件负责其他的部分，主要包括：把文档切分好转化为向量，连接向量数据库和 LLM 以及拼接提示词。</p><p></p><p>由于单纯的向量搜索技术门槛有限，这一时刻也引发了对向量数据库的广泛讨论：</p><p></p><p>简单使用，拿 faiss 这样的索引库就可以嵌套在 RAG 中了，还容易部署，向量数据库不过就是把 faiss 包装成一个 Server 而已，那向量数据库的门槛在哪里？即便有了云原生的向量数据库，降低使用门槛和使用成本，那直接给成熟数据库添加向量能力，也不是难以做到的事情，为什么还需要一款向量数据库呢？类似的产品比如 pg_vector，它可以轻松的让 PostgreSQL 具备向量搜索能力，而且其他的数据库实现向量搜索能力，也不过就是一两个月的工作量，只是增加一种数据类型而已。随着 LLM 自己的 Token 上限增加，比如 Stream LLM 都可以达到百万 Token，只要 LLM 自身能力强，那么也不需要向量数据库了，RAG 本身只是 LLM 能力增强之前的临时解决方案。</p><p></p><p>这些问题似是而非，而支持向量数据库的一方却只是围绕着“向量索引的性能”、“对大规模向量查询的支持能力”以及“向量数据库的易用程度”来进行辩护，却未真正提供强有力的观点。</p><p></p><p>另一方面，随着 RAG 在更多场景中的应用，一些问题逐渐显露出来：</p><p></p><p>向量无法表达准确信息。在神经网络中，我们使用一个多维向量表征一段内容，比如一个词、一段文字、一张图片、一段声音、一段视频等。然而，这种表征仅代表了语义的关联，一个词可以转化为一条向量，一篇文章也可以转化为一条向量，因此，要精确捕获具体某个词，向量是无法做到的。而在实际应用中，对信息的准确提取占据绝大多数场景，例如：如何让 LLM 根据使用手册精确回答问题；如何让 LLM 精确回答合同里的内容；如何让 LLM 对研报内容做精准摘要，等等。由于任何文本都可以表示为向量，那么文本与向量之间如何进行映射呢？这种映射关系的不便维护导致很多 RAG 仅仅停留在个人使用阶段，而无法在企业真正使用起来。</p><p></p><p>因此，由于 RAG 在使用过程中的效果不佳，而导致这些不佳的原因又不是向量数据库自身能解决的，因此针对向量数据库的技术讨论，逐渐迷失在 RAG 本身的循环反复之中。</p><p></p><p>让我们抽丝剥茧，来探讨 RAG 究竟需要什么。先抛论点在前：在我们看来，RAG 本质上是企业搜索引擎在 LLM 时代的进化。</p><p></p><p></p><h2>RAG 和搜索引擎</h2><p></p><p></p><p>搜索引擎是最早的人工智能之一，站在使用者的角度，搜索引擎与 LLM 大模型类似：由爬虫抓取的互联网数据经过处理后建立倒排索引，对于企业搜索引擎来说，就是连接企业内部的数据源，对它们建立好索引。用户的查询请求会被转化为若干关键词，然后由倒排索引返回 Top K 个粗筛结果。这 Top K 个粗筛结果根据一些相对固定的因子进行排序，比如 Web 搜索引擎中的 PageRank 或者企业搜索引擎中的文档词频统计信息如 TF/IDF 等。粗筛之后还需要对返回结果作精排。精排通常需要基于机器学习模型，模型根据用户以往基于查询和返回结果的点击日志训练得到。对于个性化搜索来说，还要加上用户的以往点击和搜索偏好等。精排的结果仍然是原始数据，但由于它是经过多轮从粗筛到精排乃至重排序后的结果， 可以尽可能地把用户最希望看到的结果放到最前。</p><p></p><p>而 LLM 大模型是一种生成式模型，它返回的并不是 Top K 记录，而是根据用户的提问生成的最终答案。当用户的提问意图是问答类型时，LLM 返回的结果就可以类比为对搜索引擎返回的 Top K 个结果的总结，从而避免用户再回到原始数据中去发现，极大提升了搜索体验。</p><p></p><p>当然 LLM 还可以处理其他类型的用户提问，比如逻辑推理、代码生成，跨模态内容生成等，跟 RAG 密切相关的主要就是问答。如果把 LLM 直接应用到企业内搜索，一样需要解决传统企业搜索引擎已经解决过的问题：企业内部数据如何访问，企业最新实时数据如何访问，如何根据权限返回用户权限内的内容，等等。</p><p></p><p>基于以上 RAG 架构的 LLM 与搜索引擎在使用上非常相似，但是它们有两个核心区别：其一 是向量数据库和倒排索引，其二 是 RAG 的最后一步必须要由 LLM 来根据 Top K 个返回文本生成最终答案。</p><p></p><p>搜索引擎采用的倒排索引，是基于分词后的结果，倒排索引会根据文章内的不同词的统计信息建立词与包含这些词的文档间的映射关系。相应的查询则是通过把问题变为关键词，再从索引中获取结果。倒排索引天然具备精确的特点，但它仅仅是关键词的映射，很难具备强语义关系，这跟向量数据库正相反：利用向量可以很轻松的对一段话查询容易到语义接近的结果，因为这段话可以表征为一个或者多个向量，只需要找出与查询向量最接近的 Top K 即可。</p><p></p><p>再来看其二，搜索引擎通过倒排索引得到结果之后，经历了从粗筛到精排乃至重排序的过程，最后展现给用户的实际是经过排序后的 Top K 个原始文本，用户仍然需要从这些 Top K 个文本中获取真正的答案。LLM 则是直接给出最终答案。在 RAG 架构中，这些最终答案是根据包含了语义关系的向量搜索返回的 Top K 结果生成的。可以说，RAG 架构的 LLM 的核心能力是从用户的查询结果中生成摘要，通过限定输入的方式减少非 RAG 架构的 LLM 可能产生的幻觉现象。此时，查询返回的 Top K 个结果就成了参考文献，用户只需查看 LLM 生成的最终答案，如果不确定答案再到这 Top K 个结果中查看原文验证。</p><p></p><p>因此，RAG 架构的 LLM，更符合企业内部检索的需求，RAG 其实就是 LLM 时代由企业搜索引擎进化而来。我们来看几个例子：</p><p></p><p>Vespa：开源多年的搜索引擎。Vespa 的历史可以追溯到上世纪九十年代。Yahoo 于 2003 年收购的搜索引擎 Overture 就是 Vespa 的前身。2023 年 10 月，Vespa 从 Yahoo 独立出来并获得 Blossom 的 3100 万美元融资。它的重要融资项目就是一个基于互联网数据向 C 端用户提供高质量问答服务的大型 RAG。百川：百川近期开放了基于 RAG 的 Turbo API 。其中后台就采用了增强 RAG 效果的技术。包括查询扩展、多路召回、排序在内，无一不是搜索引擎强大技术能力的体现。而百川的前身搜狗就是一个互联网搜索引擎，搜索引擎转型提供 RAG 可以说是水到渠成。perplexity.ai：面向 C 端最成功的 RAG 应用。本质也是搜索引擎的进化，只是比搜索引擎多做了以下几件事：用 LLM 重新理解用户的提问，并解析为更清晰的搜索语句；调用搜索引擎，比如 Google、Bing 的 API，创建了自己的索引库，来构建特定领域的索引库，保证搜索质量；采用自研排序算法对所有的搜索结果重排序，筛选出数量不等、质量不错的网页；用 LLM 来阅读排序返回网页中的内容，并输出与问题相关的答案。</p><p></p><h2>RAG 核心需求</h2><p></p><p></p><p>根据我们对各行业 RAG 落地效果的总结，再结合其他不同行业的 RAG 实施经验，对如何提升 RAG 的效果，已经逐步形成体系。这些经验和体系，总结下来，可以归纳为三大核心需求：</p><p></p><p>用更好的 embedding 模型获取更好的向量，在把文字送进数据库之前，需要对文字做足够好的预加工，文字分片切也需要做得足够好，才能确保获得的向量是恰当的。多路召回。只有多路召回，才能既保证基于语义的查询结果，也能保证企业场景必备的精确检索，没有多路召回的 RAG 在大多数企业场景下会失败。排序（cross-attentional re-ranking）。所谓“跨注意力”指的是多路召回的结果还需要经过良好的重排序（fused ranking）才能把最适当的结果交给 LLM。</p><p></p><p>这三大核心需求中除了第一点是在数据库以外完成，其余两点都可以在数据库内部完成，这本质上是综合了向量搜索和全文搜索的更高级搜索引擎——传统的数据库无法服务好 RAG ，是因为缺乏以下这些搜索引擎必备的组件：</p><p></p><p>高效率的全文索引；多样化的排序手段；无处不在的自然语言处理。</p><p></p><p>举个例子，PostgreSQL 是一款 OLTP 数据库，OLTP 的核心设计目标是确保数据写入的 ACID，而这跟向量和全文搜索都不相关。尽管 PostgreSQL 有全文搜索的功能，而且已经存在十多年了，为何至今企业仍然采用 Elasticsearch 而不是 PostgreSQL 进行全文搜索呢？这是因为 PostgreSQL 的全文搜索只适合小数据规模的简易搜索，而一款能够服务好 RAG 的数据库需要胜任各种数据规模，进行可定制的相关度排序，尤其还需要与向量进行多路召回的融合排序，这些都是 PostgreSQL 没有办法胜任的。</p><p></p><p>所以，纯向量数据库固然无法满足 RAG 的要求，而实现了向量搜索能力的传统数据库，同样无法满足 RAG 的要求。</p><p></p><p>那么，用一个搜索引擎比如 Elasticsearch 来服务 RAG，是否就足够了呢？它有多路召回（同时提供全文检索和向量搜索），也有基于 RRF 算法的融合排序。然而，即便如此，这仍然不够。</p><p></p><p>在企业内部会面临各种数据源，我们设想这样一个简单的场景：根据权限来查找出符合要求的答案，然后将这些答案交给 LLM 返回。假定权限是在一张表中，我们需要把这个表的字段同步到 Elasticsearch 的 collection，这意味着三件事：</p><p></p><p>即便是这样简单的需求也要引入高成本的 ETL。原始权限表的数据不能及时体现到 LLM 返回的内容。引入不必要的数据膨胀。权限过滤仅仅是一个例子。如果完全依赖 ETL 解决来自不同数据源的其他各种查询，那相当于保存一张带有全部过滤字段的宽表。除了上述的系统维护和数据更新问题之外，这也是一种不必要的数据膨胀。在大多数情况下，企业数字化系统架构内只有离线场景的数据仓库才有引入宽表的必要。</p><p></p><p>因此，一个良好的企业级 RAG 需要一个以完整功能的数据库为核心，辅助以各类工具，包括良好的文字切分和 embedding 模型，更好的融合排序模型特别是针对垂直业务或者不同企业内部的各种排序模型和查询扩展，才能达到满意的效果。</p><p></p><p>Elasticsearch 在十多年前就是企业搜索引擎，它相比数据库的一个显著区别是它没有执行引擎组件：收到搜索请求以后，直接从倒排索引获取数据后排序返回，所有工作都在一个线程中完成。而执行引擎的一条查询请求进来之后会被编译成计算图，然后以流水线的方式在计算图流转，引擎会根据可用资源的数量决定计算图每个节点使用的并行度。缺乏执行引擎在应用层面的表现就是无法为企业内的各类精确信息查找提供足够的访问能力。</p><p></p><p></p><h2>RAG 的未来</h2><p></p><p></p><p>前面提到的关于向量数据库本身的似是而非的问题，我们已经解答完毕。还有一些问题，虽然并不重要，但它们在一段时间内确实也影响了很多人的判断，在这里一并回答：</p><p></p><p>过于追求模型的上下文 Token 数没有实际意义。RAG 通过检索缩小用户提问所需要的上下文窗口，是解决上下文 Token 数限制的最佳方案。在 2023 年 11 月 6 日 OpenAI 推出的 GPT4 Turbo，本质就是一个 RAG （根据一些出错日志信息， OpenAI 可能是采用了类似 Qdrant 的向量数据库， 由于 OpenAI 主要面临个人的简单场景，因此只用基于向量数据库的 RAG， 也能基本满足要求）。前期很火的项目 Stream LLM 是一个号称解决了上下文 Token 数限制的 LLM 的例子：它几乎可以被认为拥有无限 Token，但并没有真正解决推理过程中上下文 KV 缓存容量有限带来的瓶颈，只是借助滑动窗口的概念仅保留最近的注意力。换言之，就算我们把一整本书都放进 LLM 里提问，它也只能记得清最近的内容，之前的就很模糊了。因此，我们不能说 LLM 的能力强，就可以扔掉 RAG，更何况企业场景的数据，根本无法用 Token 数衡量，百万量级的 Token，对应的文本也不过几个 MB 而已，而企业级数据达到 GB 甚至 TB 级也轻而易举。即使解决了海量数据检索的问题，我们仍然需要考虑如何处理精确信息查询，比如权限等等。因此，RAG 将永远存在，它不是一个临时解决方案。当前 RAG 最大的瓶颈其实是 LLM 本身，用 LLM 支撑业务的过程中的最大痛点从来都不是上下文的处理能力不足，而是在上下文有限的情况下依然控制不了 LLM 的胡说八道（幻觉）。因此，随着 LLM 能力的增强，RAG 的未来，将如下图所示：由 LLM 搭配一个专用的数据库，承担起企业各类数智化在线业务的门户级需求。企业会把各类数据都交给这个数据库——我们定义为 AI 原生数据库，其应用架构图如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/77/7771869e3944d05f40eacdbf62de73f9.png\" /></p><p></p><p>适合用向量表达的数据，转化为向量保存下来。不适合变为向量的数据（如各类结构化和半结构化数据），以原方式保存。查询也不是用一条向量去查出相似的多条向量，而是在一条查询语句中包含多种查询条件：有向量搜索（也包含多向量搜索）、全文检索、对各类结构化数据的查询，还有针对多种搜索后的多路召回。</p><p></p><p>以上，也正是我们开发全新的 AI 原生数据库 Infinity 的初衷，它已于 2023 年冬至前开源（<a href=\"https://github.com/infiniflow/infinity/\">https://github.com/infiniflow/infinity/</a>\"），具备多路召回，融合排序，也提供结构化数据查询能力。</p><p></p><p>让我们再回到开始，回顾 Andy 的 2023 年数据库年报。在年报中，Andy 和 Weaviate 的 CTO 讨论了向量数据库未来可能朝两个方向发展。</p><p></p><p>方向一，向量数据库支持传统 DBMS 的功能，支撑运营性质的负载（operational workloads，通常即为在线负载）；方向二，向量数据库充当一个索引数据库的角色，类似 Elastic 或者 Vespa ，它与主数据库协同工作。Infinity 其实是这两条路线的结合：相比传统数据库，它是一个更高级的搜索引擎，具备 RAG 所需要的一切能力；相比 Elasticsearch 这样的搜索引擎，它具备数据库的能力，拥有数据库必备的执行引擎，可以支撑企业内部对于在线业务的各类访问需求。</p><p></p><p>因此，我们把它看作是为 AI 配套的第三代数据库基础设施：</p><p></p><p>第一代，以统计模型和数据挖掘为基础，基础设施的配套代表就是搜索引擎。因此配套的企业 AI 应用架构，往往是以 Elasticsearch，再加上 MySQL 等数据库共同支撑业务体系。第二代，深度学习催生了向量搜索的需求，也催生了向量数据库这个品类。由于向量数据库仅仅提供单一的向量搜索能力，在构建企业信息系统时，还需要搭配各类数据库、数据仓库等，形成所谓的 AI 中台。第三代，AI 进化到大模型之后解锁了更多场景。因此，它需要的不再是一款单纯的向量数据库，而是能够同时提供向量搜索、全文搜索和结构化数据检索，可以支撑大模型对于复杂数据的获取需求，能够配合大模型共同支撑起企业门户业务需求的基础软件产品。</p><p></p><p></p><h5>作者简介：</h5><p></p><p></p><p>张颖峰，英飞流（InfiniFlow）创始人，连续创业者，先后负责 7 年搜索引擎内核研发，5 年数据库内核研发，10 年云计算基础架构和大数据架构研发，10 年人工智能核心算法研发，包括两代广告和推荐引擎，计算机视觉和自然语言处理。先后主导并参与三家大型企业数字化转型，支撑过日活千万，日均两亿搜索动态请求的 C 端互联网业务。</p>",
    "publish_time": "2024-01-24 10:38:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "浪潮信息与英特尔联合发布全球首个全液冷冷板服务器参考设计",
    "url": "https://www.infoq.cn/article/quj9NEBSi8Bg3SGwMqBb",
    "summary": "<p>近日，浪潮信息与英特尔联合发布全球首个全液冷冷板服务器参考设计，并面向业界开放，为全球液冷产业链上下游提供极具价值的参考样板，推动先进全液冷冷板解决方案在全球数据中心的大规模部署应用，实现数据中心更加绿色低碳可持续发展。</p><p></p><h2>算力基础设施迈向高质量发展，绿色算力大势所趋</h2><p></p><p></p><p>在数字化与智能化的浪潮中，算力的重要性愈发凸显，是数字经济增长的重要底座。工信部统计显示，我国算力产业规模快速增长，年增长率近30%，算力规模排名全球第二。截至2022年年底，我国算力总规模达到180百亿亿次浮点运算/秒，存力总规模超过1000EB（1万亿GB）。随着当前大模型迎来爆发期，算力产业也迎来了前所未有的发展机遇，并逐渐从行业引导阶段走到了高质量发展阶段。</p><p></p><p>据介绍，高质量的算力基础设施具备以下四个特点：</p><p></p><p>多元泛在：随着人工智能、大数据等技术的快速发展，通用算力、智能算力等需要协同发展，形成多元异构的算力供给体系。如何实现这种多元泛在的算力是高质量发展的关键。智能敏捷：像水和电一样，算力应成为基础资源，并根据需求进行弹性调度。这需要建立多层次的算力调度体系，使算力更加泛在化，实现算力资源和需求的精准对接。普惠赋能：算力应与千行百业深度融合，扩大算力应用需求，为实体经济提供有力支持。绿色安全：随着算力需求的持续增长，绿色和低碳化变得越来越重要。提高算力设施的能源效率和低碳水平，以及构建安全的供应链，是算力发展的重要特征。</p><p></p><p>其中，绿色算力是衡量计算系统从部件到整机到上层应用全堆栈的计算能效，主要是通过对服务器系统的升级、改造，使用创新的绿色算力设计方案，智能部件管理技术助力节能减碳，创新性整机系统设计，液冷技术理念成熟，整机柜设计得到广泛认可等措施来实现。</p><p></p><p>在“双碳”目标和政策扶持的双重背景下，绿色算力作为数字经济领域的新兴力量，正逐渐成为推动可持续发展的重要驱动力。政策方面，工信部等六部门联合印发的《算力基础设施高质量发展行动计划》中明确提出了提高能源利用效率和算力能效水平的要求，同时将相关技术和产品赋能行业绿色转型作为重要任务。业界方面，众多数据中心和企业都在积极探索节能技术和产品，推动绿色算力的应用。此外，能源侧也在积极与可再生能源和绿电等模式搭建起算力绿色转型的路径。</p><p></p><p>中国信通院产业与规划研究所副总工程师王青认为，绿色算力的发展不仅是行业内部问题，更是国家践行双碳战略、节能降碳的重要体现。首先，发展绿色算力是推动算力供给高质量发展的必然选择。在持续高耗能的形势下，绿色算力是多元异构泛在算力的必然选择。其次，发展绿色算力有助于降低算力成本，加强数据、算力和经济的协同发展，是促进数字经济绿色转型和可持续发展的重要路径。</p><p></p><p>“发展绿色算力需要多方面的支撑，其中两大关键点尤为重要。”浪潮信息服务器产品线总经理赵帅表示，一方面，需要围绕算力的全生命周期进行先进绿色技术的创新。这不仅局限于算力本身，在关注算力输出设施的能源消耗外，还要从总体数据中心的减排出发进行创新，包括一体化电源、一体化数据中心，绿电使用、数据中心能耗管理，服务器和数据中心一体化运维等方面，以有效降低数据中心的总体碳排放。此外，还要关注算力全生命周期的碳排放问题，包括生产过程、运输过程以及算力的回收和再利用等方面，需要对整个生命周期的碳排放进行有效的计算、管理和监控，并最终实现有效的利用。</p><p></p><p>另一方面，“对于绿色算力发展而言，不可评估就不可改进，不可评估就不可提升”。目前，绿色节能技术缺乏行业标准，且现有的绿色算力评估体系多聚焦单一维度，对算力的能效评估缺乏深入研究，限制了绿色算力的应用广度和效果。因此，我们要建立多维度的绿色标准和评价体系，来推进绿色计算产业化发展。</p><p></p><p>当前，在全球范围内，绿色算力的发展已经成为一种趋势。无论是国家层面还是企业层面，都在积极探索和实践相关的政策和措施。未来，随着技术的不断进步和应用场景的不断拓展，绿色算力将会在可持续发展和数字经济中发挥更加重要的作用。</p><p></p><h2>液冷技术已成数据中心“必选项”</h2><p></p><p></p><p>液冷技术作为绿色计算的重要组成部分，正逐渐成为数据中心领域的研究热点和未来的发展趋势。</p><p></p><p>据了解，根据冷却液和服务器接触换热方式的不同，液冷技术可分为直接液冷和间接液冷，其中间接液冷以冷板式液冷技术为主，直接液冷以浸没式液冷技术为主。相比于其他液冷散热模式，冷板式液冷在对数据中心与服务器架构的改造程度、产业链成熟度、部件更换运维便捷性、初期投资等方面均有显著优势。</p><p></p><p>IDC数据显示，冷板式液冷已经成为液冷数据中心的主流，在中国液冷服务器市场中的占比达到90%。随着AIGC时代的到来，对CPU、AIPU、内存、存储等各类IT资源的部署密度提出更高的需求，传统风冷制冷模式在换热性能及能耗优化方面逐步受限，全液冷冷板技术将成为大规模、高密度数据中心特别是智算中心的必然选择。</p><p></p><p>从全球范围来看，越来越多的企业和机构开始认识到液冷技术的优势和应用潜力。美国、欧洲等地已经率先开展液冷技术的研发和应用，不少大型数据中心和企业纷纷采用液冷技术来提高散热效率和降低能耗。在中国，“东数西算”等国家级战略也积极推动液冷技术在数据中心领域的应用和发展，许多头部互联网企业和三大基础运营商都在积极推动液冷技术的项目试点。</p><p></p><p>有数据预测，中国液冷数据中心市场规模预计由 2019 年的 260.9 亿元增长至 2025 年的 1283.2 亿元，中国液冷数据中心基础设施市场规模预计由 2019 年的 64.7 亿元增长至 2025 年的 245.3 亿元。</p><p></p><p>不过由于液冷产业标准不完善，产业链上各个企业技术路径多种多样、产品规格千差万别，产品质量良莠不齐，各液冷模块无法兼容，让用户难以选择，获取成本和使用门槛高，液冷产业发展面临阻碍。</p><p></p><p>此次浪潮信息与英特尔联合发布全球首个全液冷冷板服务器参考设计，一方面是基于双方在液冷技术领域的创新能力，从系统级解决方案出发，以模块化解耦设计思维，兼容不同厂商和型号的计算设备，探索高能效、易于运维、使用更安全，兼容性更出色的全液冷冷板服务器设计；另一方面，通过设计开放，对产业探索全液冷冷板解决方案，提供新思路，加速全液冷冷板技术的普及和规模化应用，推动产业化进程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef1c9c9006eb918515a264b2e8f6c94f.png\" /></p><p></p><p>基于该参考设计，浪潮信息推出全液冷冷板服务器，实现了对CPU、高功耗内存、NVMe 硬盘、OCP 网卡、电源、PCIe 转接卡和光模块等服务器主要发热部件的冷板全液冷覆盖，PUE降至1.05以下，并通过“去空调化”节省30%以上的数据中心空间，充分满足数据中心的高密度部署需求。</p><p></p><p>针对不同部件的结构、材质、工作温度等差异化需求，全液冷冷板服务器创新设计了诸多系统部件级液冷解决方案，包含PSU、内存、硬盘、扩展模块等，灵活性更高，用户可以按需选择。其中，首创的内存枕木散热器液冷方案结合了传统风冷散热和冷板散热的优势，比现有的管路（Tubing）内存液冷方案，更加易于组装和维护，且通用性更强。</p>",
    "publish_time": "2024-01-24 10:44:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "代码人生攻略：程序员们如何为自己编织一份明朗未来？｜ 年度技术盘点与展望",
    "url": "https://www.infoq.cn/article/NIYfevVbiaJBhotWbdcd",
    "summary": "<p>2023，没有一个打工人避得开 AI。程序员这个行业也不例外。1月22日晚8点，InfoQ策划了重磅直播，邀请到了章文嵩（后端）、周爱民（架构、前端）、李博源（大数据）、陶建辉（后端偏数据库方向）四位重量级大咖，通过分享各自的职业心得和技术洞察，帮助我们更好地为未来的发展做好准备。</p>\n<p>直播亮点：</p>\n<p>什么技能能让大家更好的面对挑战，建立自己的职业壁垒？</p>\n<p>前端、后端、架构等，哪个职业最容易受到 AIGC 的影响？我们该如何应对？</p>\n<p>企业如何建设 IT 队伍，才能跟上时代的发展？</p>\n<p>“成本中心、降本增效”的形势下，企业如何做投入才能利于软件行业发展？</p>\n<p>互联网企业、传统企业，到底需要一个什么样的 CTO？</p>",
    "publish_time": "2024-01-24 12:05:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "微软战略AI产品发布一周就翻车！网友：跟ChatGPT Plus 比，简直就是垃圾",
    "url": "https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4",
    "summary": "<p>1月16日，微软重磅推出了针对个人用户的订阅服务Copilot Pro，每月20美元，Microsoft 365个人版/家庭版用户就能在Word、Excel、PPT等Office全家桶中直接用上GPT-4。</p><p>&nbsp;</p><p>在发布一周多后，Copilot Pro也迎来了用户的第一波反馈。结果显示，Copilot Pro 性能似乎配不上这么高的价格。</p><p></p><h2>“简直就是垃圾”</h2><p></p><p>&nbsp;</p><p>“目前为止，就非常平庸。我还没有真正找到它的良好需求。虽然它总结当天电子邮件/团队聊天的能力很酷，不过我在生产中用不到。”网友“Bowlen000”说道。</p><p>&nbsp;</p><p>一位取消了订阅的用户表示，“因为GitHub Copilot建议至少在 PowerShell 中使用无用的代码块，即使其新的会​​话 VS 代码扩展也相当不准确，并且带来的解决方案会产生比解决实际问题更多的错误。”</p><p>&nbsp;</p><p>“我不想再使用它了。我曾尝试让它为我撰写电子邮件，结果发现我更喜欢我自己写的。”有网友说道。</p><p>&nbsp;</p><p>当然也有人表示已经接受了它，“作为我的助手，它大大加快了我的工作流程。”还有网友表示，“我正在等待它将 Excel 转换为PPT，这样我就不必......为什么高管如此喜欢PPT？”</p><p>&nbsp;</p><p>Webcafe AI的开发者 Corbin对比了Copilot Pro和ChatGPT&nbsp;Plus&nbsp;，对于同一个表格类任务，ChatGPT无法直接导出到工作表格中。而在文本生成方面，两者没有特别大的差异。</p><p><img src=\"https://static001.infoq.cn/resource/image/3f/e8/3ff5afc067e52a7a66f59126be079de8.png\" /></p><p></p><p>这其实也可以看作Copilot Pro的优势之一：与 Microsoft 365 数据和工作环境的无缝集成。但 Corbin补充称，如果需要类似功能，ChatGPT的用户可以添加插件。</p><p>&nbsp;</p><p>“我有一份时常面向客户的工作，需要从一个会议跳到另一个会议。如果它能将这些转变为‘即将行动’添加到 Planner 或类似计划表中，那将是天赐之物。对我来说，工作中最困难的部分就是保持事情井井有条，并且不要忘记跟进。”有网友说道。</p><p>&nbsp;</p><p>可见，大家对Copilot Pro目前最大的期待就是切实提高工作效率。</p><p>&nbsp;</p><p>同样在编程方面，Copilot Pro的口碑也并不好。</p><p>&nbsp;</p><p>有网友表示，“每当我问同一个技术问题时，我都会得到同样的 SEO 错误答案。每当我使用 GitHub Copilot 时，我都会收到带有不存在的 cmdlet 和参数的脚本。它似乎没有任何技术能力……”</p><p>&nbsp;</p><p>另一位网友Erik表示，“我认为Copilot Pro广泛使用的用例是管理咨询。所有这些咨询公司每年都让商学院应届毕业生制作Excel表格和PowerPoint演示文稿，并收取数百万美元的费用。他们所有的”建议’都是‘循环利用’的。我和妻子都曾在大型跨国公司工作，并注意到麦肯锡向我们两家公司出售同样的‘数字化转型’工具……除了颜色、logo和自定制品牌/口号/名称外，其他几乎相同。”</p><p>&nbsp;</p><p>“对于所有 IT 类事物，我们正在测试Copilot Pro有哪些好处，以便可以将其提供给客户，但到目前为止我还没有真正看到好处。”Bowlen000说道。</p><p>&nbsp;</p><p>还有网友“MichaelBoyens”指出了两者在上下文窗口大小的不同：“自推出之日起我就一直在使用 Copilot Pro，我发现 Copilot 中的上下文非常小，即 2000 个字符，而 GPT 中则为 32k token。我认为这很重要，因为如果我在 ChatGPT 中使用经过训练的 GPT 和一个大的上下文窗口，我可以获得比 Copilot 更‘智能’的答案。Copilot 中还没有存在任何意义的 GPT，尽管 Copilot Bulider 即将到来，这也许能将使事情变得更加有趣！”</p><p>&nbsp;</p><p>网友NotKoreanXD在推特上抱怨，在使用Copilot pro 中出现了运行非常慢的情况：当你在Edge浏览器上让它搜索任何内容，比如天气、Office 365 后，它开始显着减慢，令牌速度甚至小于 1令牌/秒。</p><p>&nbsp;</p><p>对此，微软广告和网络服务主管Mikhail Parakhin <a href=\"https://twitter.com/MParakhin/status/1748046106968391844\">告诉苦苦挣扎的用户</a>\"，服务器端“看起来不错”，并暗示浏览器可能有问题。</p><p>&nbsp;</p><p>更有网友直接指出，“与 ChatGPT Plus 相比，Copilot Pro 简直就是垃圾。两者差远了。”值得注意的是 ChatGPT Plus 也是每月20美元。</p><p>&nbsp;</p><p>另外，Copilot Pro的这个定价并不便宜。微软自己的M365 个人版每月才 6.99 美元，Copilot Pro 的定价几乎是其三倍，可以说是相当高了。</p><p>&nbsp;</p><p>“这看起来更像是 Adob​​e 会选择的一个奇怪定价，而不是微软会选择的……尤其是对于一个还没免费版本更有用的产品来说，而且还没有人真正迷上它。”Threxx说道。也有网友表示，如果必须一年支付240美元，那么大多数人不会在没有明确需求的情况下使用Copilot Pro。</p><p>&nbsp;</p><p></p><h2>AI 功能没有带来明显收入</h2><p></p><p>&nbsp;</p><p>尽管作为一项新服务，出现这样的问题并不意外，但获得这样的“差评”不免让微软感到尴尬。微软需要人工智能成为不可或缺的生产力工具，以证明其投资的合理性。但其在AI应用方面的投入目前并没有换来相应的回报，比如搜索。</p><p>&nbsp;</p><p>微软于 2023 年 2 月<a href=\"https://www.theregister.com/2023/02/07/microsoft_bing_ai/\">推出了</a>\"由 OpenAI 驱动的 Bing 聊天机器人，当时其在所有平台的全球搜索市场份额为 2.81%。尽管 Bing 每个月都在一点点增长，但根据 StatCounter 的数据，截止去年12月，Bing 的使用率仅为 3.37%。</p><p>&nbsp;</p><p>这些数字与竞争对手谷歌相比相形见绌，谷歌在 2023 年初占据所有平台全球搜索市场 93.37% 的份额，尽管到去年 12 月这一数字已下滑至 91.62%。仅在桌面领域，Bing 从 8.18% 略有上升至 10.53%；谷歌从85.64%下降到81.71%。在移动领域，Bing 全年仍低于 1%，而 Google 则拥有全球 95% 以上的市场份额。</p><p>&nbsp;</p><p>也就是说，到目前为止，所有这些大肆宣传的人工智能功能几乎没有增加微软在全球搜索市场的份额。尽管微软在推出 Bing 对话助手后吸引了一些新用户但数量并不多。谷歌仍然是目前互联网搜索的老大。</p><p>&nbsp;</p><p>有分析师表示，微软正在以两种方式承担人工智能建设成本：一是为自己的产品提供动力，例如企业每月支付 30 美元的Copilot人工智能助手；二是为使用 Azure 云计算服务创建人工智能产品的公司提供服务。</p><p>&nbsp;</p><p>但在收益回笼之前，微软还要进行更大规模的投资，比如微软建立了新的数据中心来支持人工智能，还要从英伟达等公司那里购买芯片，各种支出都在增长。</p><p><img src=\"https://static001.infoq.cn/resource/image/90/00/90477b8273dfcf6712610bdd0f4c2900.png\" /></p><p></p><p>虽然 GitHub Copilot 在很大程度上取得了成功，但性能问题和成本问题表明微软仍有很长的路要走。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://www.reddit.com/r/sysadmin/comments/19dj0dv/copilot_feedback_nearly_1_week_in/\">https://www.reddit.com/r/sysadmin/comments/19dj0dv/copilot_feedback_nearly_1_week_in/</a>\"</p><p><a href=\"https://www.youtube.com/watch?v=5mhEu-U1yxs\">https://www.youtube.com/watch?v=5mhEu-U1yxs</a>\"</p><p><a href=\"https://www.theregister.com/2024/01/23/microsoft_copilot_pro/?td=rt-3a\">https://www.theregister.com/2024/01/23/microsoft_copilot_pro/?td=rt-3a</a>\"</p><p><a href=\"https://www.theregister.com/2024/01/18/bing_ai_search/\">https://www.theregister.com/2024/01/18/bing_ai_search/</a>\"</p><p><a href=\"https://www.reuters.com/technology/ai-lesson-microsoft-google-spend-money-make-money-2023-07-25/\">https://www.reuters.com/technology/ai-lesson-microsoft-google-spend-money-make-money-2023-07-25/</a>\"</p>",
    "publish_time": "2024-01-24 14:48:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache 顶级项目 MXNet 退役！大神李沐创办、亚马逊首选深度学习框架如何从大厂“宠儿”到落入“冷宫”？",
    "url": "https://www.infoq.cn/article/UYqTqTU9iloqhITdaHIz",
    "summary": "<p>&nbsp;</p><p>近日，InfoQ从Apache官网获悉，知名深度学习技术专家李沐的开源学习框架项目MXNet已被移至Apache Attic，原因是这个项目不活跃。</p><p>&nbsp;</p><p>Apache Attic（阿帕奇阁楼）是Apache软件基金会的一个项目，为已终止的Apache项目提供解决方案。Attic项目成立于2008年11月。已退休的项目也可以被保留。</p><p></p><h2>从Apache孵化器毕业后不久，就进入“退役”阶段</h2><p></p><p>&nbsp;</p><p>Apache MXNet 是一款开源的轻量级深度学习框架，支持多种语言，包括 Python、Scala、R、Julia、Perl 等等，这使得它可以被各种人群使用，包括开发人员和数据科学家。其核心代码是用C++编写的，但它提供了一个灵活的界面，允许用户使用他们喜欢的语言编写代码。MXNet于2022年9月从Apache 孵化器毕业成为顶级项目。</p><p>&nbsp;</p><p>MXNet 的另一个重要特性是它支持混合符号编程和命令式编程，从而能最大限度提高效率和生产力。其核心是一个动态的依赖调度，它能够自动并行符号和命令的操作。它还具备一个图形优化层，使得符号命令执行速度更快，内存使用更高效。它也是一款便携框架，能够扩展到多个 GPU 和多台机器。</p><p>&nbsp;</p><p>MXNet支持超大规模云厂商AWS和Azure。2016年，亚马逊选择MXNet作为AWS的深度学习框架，亚马逊首席技术官 Werner Vogels亲自撰文解释并进行了大力推广。经过多年的发展，MXNet得到了英特尔、Dato、百度、微软、Wolfram Research以及卡耐基梅隆大学、MIT、华盛顿大学和香港科技大学等研究机构的支持。</p><p>&nbsp;</p><p>但MXNet的发展并非一路高歌猛进，在最近几年MXNet逐渐走向没落。</p><p>&nbsp;</p><p>早在去年6月，MXNet项目核心成员Sheng Zha就曾给社区发了封声明坦陈了项目面临的艰难处境，声明中称，“自 2022 年底以来，代码开发大部分已停止，社区参与度放缓。尽管生成式人工智能和相关技术蓬勃发展，深度学习框架和分布式训练等空间解决方案，但该项目目前的定位还不够维持项目的增长，特别是考虑到开源深度学习框架空间的开发。为此，社区需要做出一些选择，包括通过找到关键的大型项目的路径继续维护该项目；讨论 MXNet 可以转向哪些替代位置；退休到Apache Attic中。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/39/3964837fe52507be85b4d0869457db81.jpeg\" /></p><p></p><p>&nbsp;遗憾的是，社区最终选择了最后一条路。去年9月，MXNet正式进入Apache Attic，自此退休。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/9f/9fb656291f1b22075caf9d15d0594bc6.jpeg\" /></p><p></p><p>&nbsp;10月份，Apache还给社区发出“警告”通知，表示项目进入“只读“阶段。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/e9/e9debc448fd8c1b667d16f25cb137f98.jpeg\" /></p><p></p><p>&nbsp;到了11月17日，项目又再次发出邮件，提醒用户MXNet已经成功移入Attic。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/83/83432e801c9436cc664e871cde7ffde4.png\" /></p><p></p><p>&nbsp;但值得一提的是，退休一个项目并不是关闭一切那么简单，因为现有用户需要保留对自己开发工作必要信息的访问权。也就是既需要让现有用户知道该项目正在退役，又要保留获取必要信息以进行自身开发的机会。具体来说，Apache Attic机制的目的是：不影响用户、对代码库提供受限制的监督、在没有项目管理委员会（PMC）的情况下对活动用户列表进行监督；但它将不再重建社区、修正 bug、发布新版本。</p><p>&nbsp;</p><p>一个项目可以通过两种机制进入 Attic：管理项目管理委员会（PMC）决定要该项目迁移进入 Attic，或者 Apache 软件基金会的董事会解散 PMC 并选择迁移该项目。</p><p>&nbsp;</p><p>而什么样的项目应该进入 Attic ？“PMC 无法召集 3 票进行版本发布、没有积极的提交者或无法履行其向董事会报告的项目都是 Attic 的良好候选者”。</p><p>&nbsp;</p><p>据悉，MXNet社区共有875 名贡献者、87 名提交者和 51 名 PMC 成员。许多社区成员在 MXNet 之外继续发挥重要作用生成式人工智能和深度学习系统空间。</p><p></p><h2>MXNet的发展历史</h2><p></p><p>&nbsp;</p><p>MXNet由DMLC（Distributed ‘Deep’ Machine Learning Community）打造，DMLC的大部分成员目前大部分是华人，以陈天奇、李沐、解浚源等为代表，创建了这个世界上目前排名第四的深度学习框架：MXNet。</p><p>&nbsp;</p><p>作为Apache MXNet的作者，李沐本科就读于上海交大，博士毕业于美国卡内基梅隆大学，并先后在港科大、伯克利担任教职，2016年加入亚马逊成为首席科学家。</p><p>&nbsp;</p><p>2014年在NIPS上的相遇之后，陈天奇与李沐等人决定组建DMLC分布式深度机器学习社区联盟（Distributied Deep Machine Learning Community），号召大家一起合作开发MXNet，分别发挥各自的特长，共同解决机器学习门槛高的问题。</p><p>&nbsp;</p><p>该联盟前期成员主要来自于<a href=\"https://github.com/dmlc/cxxnet\">CXXNet</a>\"团队, 核心成员十人左右。其中张铮老师指导的 Minerva 团队主要负责后端引擎，陈天奇进行接口设计，李沐负责分布式。</p><p>&nbsp;</p><p>经过了一年的发展，2015年9月，MXNet的初代版本就已经开源出来并在<a href=\"https://github.com/apache/mxnet\">GitHub</a>\"上发布。在一众技术大佬背书下，项目开源就受到了极大的关注。在李沐等人的不断努力下，MXNet在多个机器学习和计算机视觉竞赛中取得了优异的成绩。与此同时，MXNet持续增加了对新颖算法的支持，并逐渐被学术界和工业界广泛采用。</p><p>&nbsp;</p><p>2016年，越来越多的开源贡献者参与到了开发及维护 MXNet 的工作中，对 MXNet 进行了大量的扩展、优化和修复工作，截止2016年11月，该项目已经拥有200多位贡献者。也是这一年，MXNet的发展迎来了重要的里程碑。</p><p>&nbsp;</p><p>同年年底，亚马逊宣布将MXNet作为官方使用的深度学习框架，这一决定让外界哗然。因为当时的深度学习市场上已经有了<a href=\"https://www.infoworld.com/article/3127397/review-tensorflow-shines-a-light-on-deep-learning.html\">TensorFlow</a>\"、Theano、Torch 或 Caffe 等知名度较高的深度学习框架，MXNet 作为新兴的后起之秀，为何会被亚马逊这样的超级巨头选中？</p><p>&nbsp;</p><p>在接受媒体采访时，时任亚马逊CTO的&nbsp;Werner Vogels给出了答案。Vogels表示之所以选择MXNet作为深度学习框架，是因为它的扩展性和运行性能比几乎任何其他产品都要好开发，此外，技术人员还可以在该框架中使用多种语言，包括Python、C++、R、Scala、Julia、Matlab 和 JavaScript。</p><p>&nbsp;</p><p>Vogels 发布了使用 Inception v3 图像分析算法的 MXNet 训练吞吐量基准，并声称通过在多个 GPU 上运行该算法获得的加速是高度线性的。在 128 个 GPU 上，MXNet 的执行速度比单个 GPU 快 109 倍。</p><p>&nbsp;</p><p>同时，AWS还表示将会通过增加源代码贡献、改进文档以及支持来自其它框架的可视化、开发以及迁移工具，为实现MXNet成功的长远目标做出贡献，AWS的大力驰援进一步推动了MXNet的发展和推广。</p><p>&nbsp;</p><p>2017年1月23日，MXNet被Apache软件基金会接纳，成为其孵化项目之一。这一举措为MXNet的开发和社区生态注入了新的活力，吸引了众多大公司的支持和贡献，包括亚马逊、微软、英特尔等。到2017年底， MXNet 贡献力量的人员达到 400 多名。</p><p>&nbsp;</p><p>2017年，<a href=\"https://link.zhihu.com/?target=https%3A//github.com/apache/incubator-mxnet/releases\">MXNet</a>\"&nbsp;0.11是MXNet正式加入Apache以后的第一个版本。这次最大的改进是加入了动态图接口Gluon，Gluon同时支持灵活的动态图和高效的静态图，既保留动态图的易用性，也具有静态图的高性能。随后，在MXNet 1.0版本中，引入了动态图计算和TensorFlow兼容接口等重要更新，进一步提升了MXNet的性能和灵活性。这使得MXNet在与其他深度学习框架的竞争中更具优势。</p><p>&nbsp;</p><p>在随后的几年中，MXNet持续发展和壮大。它推出了多项新功能和技术创新，包括混合精度训练、模型量化、自动混合推理等。这些创新使得MXNet在处理大规模数据集和复杂模型时更加高效。此外，MXNet还提供了更加灵活且直观的接口，方便用户使用。</p><p>&nbsp;</p><p>2022 年 9 月，MXNet项目作为顶级项目从孵化器毕业。</p><p>&nbsp;</p><p>MXNet的一路发展和李沐个人的职业选择深度绑定。2019 年，InfoQ 有幸对李沐进行了一次专访，了解到了他从学术界到工业界的精彩故事。</p><p></p><h2>硅谷华人 AI 大神李沐的开挂人生</h2><p></p><p></p><p>上海交通大学 ACM 班， 取名源于国际科学教育计算机组织——美国计算机学会（Association of Computing Machinery），ACM 是世界上第一个、也是最有影响的计算机组织，计算机领域最高奖——图灵奖是由该组织设立和颁发的。</p><p>&nbsp;</p><p>李沐的大学本科，就是在这样一个被寄予厚望的“特色班”里度过的。</p><p>&nbsp;</p><p>有意思的是，他当时只是对数学很感兴趣，高中的时候虽然参加过一些数学竞赛，可几乎没有接触过计算机。进到了这么一个培养计算机科学家的班里，他自己也有些拿不准：到底能不能搞定这个复杂的机器？</p><p>&nbsp;</p><p>ACM 班最大的特点在于注重实践，即使是在本科阶段，学生也需要手写大量的代码来练习，至少也是其他院校同院系实践量的四五倍。此外，ACM 班非常注重数学能力，这也给了李沐发挥所长的机会。</p><p>&nbsp;</p><p>不过，可不要觉得李沐的生活就此陷入了无边无际的“代码海洋”之中，“学霸”的生活并不枯燥，反而还充满了不少乐趣。据李沐的室友透露：那时的李沐不仅学习成绩好，还是篮球场上的一把好手，更是电竞场上的“大神级”人物。</p><p>&nbsp;</p><p>然而不久之后，戏剧性的一幕出现了：虽然踌躇满志进入了 ACM 班，并且报名参加了计算机竞赛，但是李沐最终因为训练的成绩不合格被刷了下来。</p><p>&nbsp;</p><p>这个时候，李沐遇到了他的导师：吕宝粮教授，也正是从那时起，李沐正式推开了人工智能的大门。</p><p></p><h3>初识AI</h3><p></p><p>&nbsp;</p><p>吕宝粮，上海交通大学计算机系教授、博士生导师，IEEE 高级会员。主要研究方向有：仿脑计算机理论与模型；机器学习理论与应用；神经网络理论与应用；生物特征识别；自然语言处理；脑与计算机界面。</p><p>&nbsp;</p><p>李沐第一次接触人工智能，就是被吕宝粮教授的一个案例震撼到了。</p><p>&nbsp;</p><p>“当时教授展示了一张他们实验室人员的合照，并用人脸识别系统把每一个人的名字都对应显示了出来，当时我们就被这个 demo 吸引住了，也就这样选择加入了这个实验室。”</p><p>&nbsp;</p><p>李沐的人工智能之路也就由此正式开始了。“本科阶段还有时间打球、打游戏，后来真的是一年比一年累，回想起来，在 ACM 班的经历就像是一个过渡期。”</p><p>&nbsp;</p><p>“过渡期”之后，李沐没有去找工作，而是选择到香港科技大学继续深造。李沐的选择是受到了 ACM 班定位的影响——培养计算机科学家，ACM 班也更鼓励人才去从事学术研究。</p><p>&nbsp;</p><p>“除此之外，当时的大环境也是影响我选择的一个原因。那是大概 2008 年左右，虽然当时计算机已经成了热门专业，但是当时的 IT 公司并没有那么厉害，尤其在 AI 方面的技术并没有那么强。”</p><p>&nbsp;</p><p>在港科大又修炼了一年多的内功之后，李沐开始了“初入江湖”的旅程。</p><p>&nbsp;</p><p>在正式进入工业界之前，2007年夏天，李沐已经在微软亚洲研究院“小试牛刀”了三个月左右真正开始与工业界亲密接触，是2011年4月，那一年李沐进入了百度。</p><p>&nbsp;</p><p>在百度，李沐的主要工作是广告预测，利用广告数据有效地训练一个模型，使得该模型能够准确的预测用户会不会点某个广告。</p><p>&nbsp;</p><p>在百度的一年时间里，李沐积累了大量的经验，他也坦言，这些经验是在学校里学不到的：“学校的实验环境里，也许只能用到几块 GPU，跑几个 G 的数据。而在企业里，你面对的是几个 T 的数据量和上百台机器，在这种情况下，你不得不去考虑如何做分布式，或者如何让代码和系统变得更快、更简洁。”</p><p>&nbsp;</p><p>2013年，在百度工作了一年后，李沐再次踏上了求学之路。“在百度的经历让我学到了很多，也更清楚地发现了自己的短板。但是当时国内做系统方向的人比较少，所以我还是选择了去美国继续攻读博士。”</p><p></p><h3>从百度到亚马逊， MXNet和李沐深深绑定</h3><p></p><p>&nbsp;</p><p>2012至2017年间，李沐在 CMU攻读博士学位，在读博期间，他也在百度担任首席架构师一职，也正是在这期间他牵头创办了MXNet项目，并伴随着项目的发展选择了不同的职业道路。</p><p>&nbsp;</p><p>2017年，MXNet项目被亚马逊选中，李沐在博士毕业后也开始了他在亚马逊的职业之旅。</p><p>&nbsp;</p><p>李沐初到亚马逊时，人工智能部门刚刚成立，正是需要深度学习框架来支持的时候，李沐的经验正好派上了用场。</p><p>&nbsp;</p><p>除了 MXNet，李沐也参与了一些别的项目。“框架只是一个工具，更重要的是要知道如何用好工具来服务客户。”分布式系统出身的李沐感慨道：“分布式系统最难的不是如何让每个机器计算的多快，而是机器之间的通讯问题，如何减少通讯提升交互效率，这对系统来说也是至关重要的。”</p><p>&nbsp;</p><p>已经有过多年工作经验的李沐在面对项目的时候会比刚毕业时考虑得更多。他需要考虑项目组之间的交互问题，如他所言：亚马逊也像是一个巨大的分布式系统，部门与部门之间的交互就像机器之间的通讯一样。但是人之间的联系并不像机器那么好优化，一旦协作不畅，很容易使项目陷入瓶颈。这些都是李沐之前不曾考虑过的问题，也是他需要努力的方向。</p><p>&nbsp;</p><p>2022年年底，大模型爆火，许多AI领域大佬也都纷纷从原公司离职自己创业，李沐也成为了这场浩浩荡荡AI创业潮的一员。</p><p></p><h3>从亚马逊离职，MXNet走向迟暮</h3><p></p><p></p><p>2023年3月，据媒体曝出，李沐已从亚马逊离职，加入了导师 Alex Smola 的创业项目，并且有消息称新公司融资情况良好。</p><p>&nbsp;</p><p>同年2月，“参数服务器之父” Alex Smol 教授从亚马逊云科技（AWS）离职，创办了一家名为 Boson.ai 的人工智能公司。同月，Alex Smol 教授在领英宣布成立了新公司并担任 CEO。“</p><p>&nbsp;</p><p>关于 Boson.ai，目前网上的资料还很少。Boson.ai 的官网还在建设中，目前仅显示：适用于所有人的大模型，并配文：“我们正在做一件大事。敬请期待！”</p><p>&nbsp;</p><p>虽然李沐并没有在官宣加入Boson.ai的消息，但他已经出现在了 Boson.ai 的 GitHub 项目主页中，据悉，Boson.ai 的 GitHub 仓库页面由李沐亲自编写。&nbsp;&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6e455dae0f8391526d3166cb557c3875.jpeg\" /></p><p></p><p>李沐的悄然离职，也映衬着MXNet的陨落。</p><p></p><h2>李沐解释没落原因：保技术，舍推广</h2><p></p><p>&nbsp;</p><p>MXNet的没落多少让人有些意外。知乎上有篇热帖也讨论了“MXNet明明很好，为何没有人用”的话题。值得一提的是，在留言回复中，我们发现了李沐的回答。</p><p>&nbsp;</p><p>李沐表示：“简单来说就是我们没有足够的人手能够在短时间内同时技术上做出足够的深度而且大规模推广，所以我们前期是舍推广保技术。”</p><p>&nbsp;</p><p>MXNet虽然是一个非常好的框架，但是由于背景不够，开发人员忙于版本迭代却忽视了文档的重要性，导致新用户难以上手，导致 MXNet 的用户群体一直无法扩大，错过了关键的竞争窗口，在与 TensorFlow，Pytorch 的竞争中一直处于绝对下风。</p><p>&nbsp;</p><p>对于李沐保技术舍推广才导致项目没落的解释网友却有着不同的观点。在这篇帖子下方，有网友经过一番调研，分析了造成MXNet火不起来的几点原因：</p><p>&nbsp;</p><p>推广太强，而对应的效果可能还没发挥出来，毕竟有着Amazon、Nvidia 以及<a href=\"https://www.zhihu.com/search?q=%E4%BD%99%E5%87%AF&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A132137595%7D\">余凯</a>\"等一众大佬站台不能算推广不够强；社区人员经历严重不足，同时又有点避重就轻。像 TensorFlow 核心集中于 C++ 和 Python 的支持，核心人员重点解决主要问题，其他的主要依赖于强大的社区贡献；而 MXNet 却重点宣扬多语言支持，把核心成员本来就较少的精力分散到各语言去了，造成的结果就是看着样样有，但是样样不精；框架本身属于开始容易，入门难：简单跑个样例还好，但是文档缺乏和混乱问题很严重；实验效果上有差距，部分模型跑不出<a href=\"https://www.zhihu.com/search?q=state-of-art&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A132137595%7D\">state-of-art</a>\"的效果，这个是致命伤；其他问题还包括比如 TensorFlow 吸引力太强，部分实验室已经绑定在以往用的比较称手的框架上，这一问题可能是无解的。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.zhihu.com/question/52498007/answer/132137595\">https://www.zhihu.com/question/52498007/answer/132137595</a>\"</p><p><a href=\"https://www.infoworld.com/article/3144025/why-amazon-picked-mxnet-for-deep-learning.html\">https://www.infoworld.com/article/3144025/why-amazon-picked-MXNet-for-deep-learning.html</a>\"</p><p><a href=\"https://lists.apache.org/thread/otf22vzk99hn1h9558s5sq4h757ccptk\">https://lists.apache.org/thread/otf22vzk99hn1h9558s5sq4h757ccptk</a>\"</p><p><a href=\"https://lists.apache.org/thread/7oz6mglh8ck9xtzs2oopl168vvknyw30\">https://lists.apache.org/thread/7oz6mglh8ck9xtzs2oopl168vvknyw30</a>\"</p>",
    "publish_time": "2024-01-24 15:28:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]