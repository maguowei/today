[
  {
    "title": "有状态自动扩展系统的设计模式提议",
    "url": "https://www.infoq.cn/article/9ivZNYc9J4Hrs7fPKH7y",
    "summary": "<p></p><h2>介绍</h2><p></p><p></p><p>面对软件工程界对隔离的需求趋势以及日益增长的可扩展性需求，自动扩展的有状态系统（多指数据库）逐渐复杂化，甚至有时可行性也会成为挑战。因此，多数公司为了让此类系统能适应最高的负载需求而选择了过度配置，但这也带来了另一个问题，过度的资源配置所需求的成本非常之高且无法保证系统的可靠性，激增的需求数量或 DOS 攻击都能轻易击溃系统负载的承受能力。本文中将会更深入地挖掘自动扩展有状态系统时所面临的挑战，并为解决这些挑战给出一个结合已有方法与新手段的设计方案提议。</p><p></p><h2>回顾</h2><p></p><p></p><p>回顾软件工程的发展史，我们能清楚看到软件的构建与用户的限制和期望方面的几个重要里程碑。从<a href=\"https://en.wikipedia.org/wiki/Mainframe_computer\">大型机</a>\"与大型服务器的集中式方法开始，过渡到桌面应用程序阶段中<a href=\"https://en.wikipedia.org/wiki/Client%E2%80%93server_model\">客户-服务器形式</a>\"的出现，再到网络应用变革的诸多阶段，从大型单体应用到现代<a href=\"https://en.wikipedia.org/wiki/Microservices\">微服务</a>\"的发展。</p><p></p><p>纵观历史，隔离的趋势非常明显。其中垂直隔离是指按关注点或上下文将系统分割，通常为数据库与应用程序的隔离，或者用户界面与业务及服务层的隔离。另一种则是水平隔离，即通过增加节点的配置以支持不断增长的需求，这一过程也可以借助&nbsp;<a href=\"https://kubernetes.io/docs/concepts/overview/\">Kubernetes</a>\"&nbsp;等工具的帮助进行自动扩展。</p><p></p><p>这种对隔离的渴求致使了<a href=\"https://en.wikipedia.org/wiki/Shared-nothing_architecture\">无共享架构</a>\"等方式的出现，即将应用程序构建为本身不具备状态的形式，也就是我们所了解的无状态应用程序，从而简化扩展的难度。听起来很棒，但工程师们很快便发现很少能有不具备任何状态的，“真正无状态”的应用程序。</p><p></p><p>退而求其次的方法是将应用程序的一部分（通常为服务或微服务）构建为无状态形式，但即便如此，这部分程序依旧依赖于数据库等有状态的系统以维持状态。这便是本文的主题，我们将讨论这个软件工程师间的共同挑战：如何在现代应用中有效地自动扩展有状态系统？</p><p></p><h2>适用用例</h2><p></p><p></p><p>本文不适用于在网页服务器中保持状态的有状态系统。</p><p></p><p>这种方式为软件工程师们提供了一个可以自行搭建数据库的基础设计方案，切实地解决了单节点上存储系统的问题，并将系统转变为具有自主自动扩展能力的分布式系统。以一个线上商城的微服务架构为例，见图一。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/4image001-1674490178804.jpeg\" /></p><p>图一：线上商城用例示范</p><p></p><p>假设出于某些需求的原因，在项目中选择&nbsp;<a href=\"http://rocksdb.org/\">RocksDB</a>\"&nbsp;是相较 Redis 而言更好的键值存储引擎，但麻烦的是，RocksDB 作为存储引擎只能原封不动地部署在单个节点上。假设现在要做另一个对可扩展性需求很高的全球性新系统，那么这篇文章将会是个很好的起点，这里你能找到如何将 RocksDB 这样的单节点存储引擎改造为一个分布式可自动伸缩的应用。</p><p></p><p>当然，我们这里所说的 RocksDB 只是一个例子，这种设计模式对其他任何的存储引擎或工具均可通用，无论是用于文本索引的&nbsp;<a href=\"https://lucene.apache.org/\">Apache Lucene</a>\"，还是不具备任何引擎的存内存储，不区分任何的存储引擎、语言、数据类型或结构。此外，本文中所分享的设计也可以为 Mongo、Redis、Postgres 数据库提供自动伸缩的功能。</p><p></p><h2>有状态系统的定义</h2><p></p><p></p><p>有状态的系统是指必须处理状态的系统。在现代网页应用中，这项任务通常由数据库承担，不过其他如网页服务器也可以通过将用户会话存储在网页服务器内存中做到。</p><p>状态管理在网站中的典型用例是用户的购物车（见图一）。在多个 HTTP 请求之间保留购物车内容，确保在用户结束购物结账付款时，购物车内商品内容和数量的准确无误。这些数据就是被存储在了有状态系统中，而在我们图一的用例中则是 Mongo DB 集群、Redis 集群、Postgres 集群。</p><p></p><h2>自动伸缩：问题所在</h2><p></p><p></p><p>我们在面对自动伸缩的有状态系统时，我们往往会思考以下这些问题：什么时候应该开始自动伸缩？有什么契机？应该如何进行伸缩？要如何移动数据？如何保证节点的一致性？以下将会是本文中讨论的重点：</p><p></p><h3>一致性</h3><p></p><p></p><p>任何拥有状态的系统都需要保证集群下一个有效状态的一致性。这个领域中相关的研究有很多，比如分别由&nbsp;<a href=\"https://www.mongodb.com/home\">MongoDB</a>\"、<a href=\"https://redis.io/\">Redis</a>\"、和 Postgres 所用的一致性算法&nbsp;<a href=\"https://www.infoq.com/news/2014/08/ark-mongodb/\">Ark</a>\"、<a href=\"https://github.com/RedisLabs/redisraft\">Raft</a>\"，以及&nbsp;<a href=\"https://www.enterprisedb.com/docs/pgd/latest/bdr/\">BDR</a>\"。这些均是在图一中线上商城示例所用的数据库。</p><p></p><p>在软件应用中，每当集群必须统一存储记录的下一个值时都会有对一致性的要求，这也是数据库实现中最出名的用例。本文中将涵盖一些能让 Raft 在选择新领导者的一致性上更加智能的建议。</p><p></p><h3>自动伸缩</h3><p></p><p></p><p>在有状态系统中，虽然越来越多的产品提供自动伸缩功能，比如云供应商所提供的数据库实例管理，但在实践中，公司内部的类似方案实现似乎总是困难重重，可能存在的原因如下：</p><p>缺乏透明度：主流云供应商如&nbsp;<a href=\"https://aws.amazon.com/\">AWS</a>\"&nbsp;及&nbsp;<a href=\"https://azure.microsoft.com/en-us/\">Azure</a>\"&nbsp;的自动伸缩功能似乎配置起来很容易，但具体的实现细节却无人知晓。了解该功能的运作原理对拥有大型数据集的关键场景至关重要。数据的迁移时机、策略等等这些问题都应该在产品详情页有明确的说明。不存在的自动扩展公用模式。自动扩展有状态系统方面至今为止仍无公开可用的模板。</p><p></p><h3>数据迁移的滞后</h3><p></p><p></p><p>常见的系统扩展方式是在集群中新增节点，但对于有状态的集群来说，同步其他节点已有的数据是需要时间的，而在某些情况下，这一过程的数据量可能是海量的。</p><p></p><p>还是以我们的线上商城为例（图一），假设组织的业务涉及多个地理区域，那么数据量将会高达数十亿。在这种规模下，一个清晰且高效的数据同步和迁移方式是非常重要的。</p><p></p><h3>快慢需求的增长</h3><p></p><p></p><p>以下这两种情况中，都需要更高的能力：1. 稳定、中长期逐步增加的需求。在先前的线上商城示例中，可以是一段时间内消费者数量的持续增长。2. 无法预测需求量激增，可能带来服务不可用的风险，如系统遭受 DoS 攻击时。</p><p></p><h3>封闭的解决方案</h3><p></p><p></p><p>为解决上述这些问题，我们需要更多公开可用的设计模式，盲目相信云供应线的解决方案会奏效不是个好主意。即使云供应商的方案确实有效，我们也很快会发现自己已经和这个特定供应商绑定了，这很不理想。</p><p></p><h2>愿景</h2><p></p><p></p><p>作者欲公开提出一种通用且可复用的，客户化自动扩展有状态系统的手段，以最少的配置或运维干预，在单集群内从单节点自动向上（垂直）或向下（水平）扩展至成百上千的节点。本文中所提出的解决方案在现阶段仅仅存在于理论之中，仍需实施和测试。</p><p></p><h2>核心原则</h2><p></p><p></p><h3>数据类型不可知</h3><p></p><p></p><p>这类设计不应局限于任何具体的数据类型，也就是说，同样的解决方案应能处理 JSON 对象、序列化数据、流数据、二进制大对象（Blob），以及其他任何的数据类型。</p><p></p><p>各司其职，负责写入新状态的集群领导者（leader）只执行写操作，不进行读取；负责读取副本的也只应执行读操作而不进行写入。</p><p></p><h3>让代理成为集群的一部分</h3><p></p><p></p><p>有状态集群中必须要有一个不提供读写的代理实现，让集群以这个代理为节点缓慢地同步集群中的数据，并最终在需要时准备提供读或写的请求。</p><p></p><h3>借助平均响应时间触发自动扩展</h3><p></p><p></p><p>云供应商所提供的大多数自动扩展方式均是利用了 CPU 和内存阈值，但这种方式并不能带来最佳的用户体验。在某些情况下，终端用户不一定能感知到系统资源的紧张，即使是占用了99%的 CPU，请求也可能及时交付。</p><p></p><p>将平均响应时间作为主要的触发条件可以改变扩展的契机，以用户角度看待系统性能。</p><p></p><h3>分片标签的优先级</h3><p></p><p>将每个对象或记录贴上分片 ID 的标签可以规避系统高压时的成本，每当需要分片时可以直接利用已有标签而无需外界干预。</p><p></p><h2>模式的设计</h2><p></p><p></p><p>文中所提议的自动扩展有状态系统包含三个不同角色，集群中这些角色各司其职。值得注意的是，作者所提议的设计与系统在单节点的运行能力与生产目的息息相关，这些角色不一定要在自己的进程或节点中运行，但对于测试、POC，甚至是部分 MVP 而言，这点还是很重要的。</p><p></p><p>话不多说，让我们看看这些角色各自的责任吧。</p><p></p><h3>写入角色（领导者）</h3><p></p><p></p><p>写入者或领导者是负责处理写操作的角色，该角色会将新状态写入自身存储并将复制后的数据传给“副本读取”角色。每个分片只有一个写入角色，关于分片的详细解释请见后文。写入角色是一致性的领导者，负责所有的写操作执行，且不承担任何读操作的执行。</p><p></p><h3>副本读取</h3><p></p><p></p><p>副本读取角色承担所有的读请求，包含来自写入角色（领导者）的一份复制数据，但如果写入角色和读取角色均在同一节点或进程中运行，则二者可以共享同一存储。在一致性协议选举新的领导者时，副本读取角色负责打开一个与领导者相连的多路通信管道，并在任何一个节点关闭或连接被网络分区中断之前一直保持管道的开放。如此，通过避免连接打开和关闭的开销，有效加快了角色与节点之间的通信速度。</p><p></p><h3>负载管理角色</h3><p></p><p></p><p>负载管理角色充当了<a href=\"https://whatismyipaddress.com/gateway\">网关</a>\"与<a href=\"https://en.wikipedia.org/wiki/Load_balancing_%28computing%29\">负载平衡器</a>\"的角色，负责将写请求发送给领导者，将读请求发送给副本读取角色。负载管理角色同时也是一个可接受上千入站请求的<a href=\"https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7\">背压</a>\"机制，将目标（对于副本读取或写入者）的并行线程数量限制在可配置的范围内，从而保证了对这些角色的压力控制。这一功能是受<a href=\"https://dzone.com/articles/understanding-tomcat-nio\">阿帕奇的 Tomcat Nio 连接器</a>\"的启发，此外，该功能也对集群负载激增或 DOS 攻击的防护至关重要，在这类情况下，负载管理角色将会吸收压力，确保读写角色的安全与稳定请求流的接收。</p><p></p><p>负载管理角色也负责将写请求路由到正确的分片，并在需要聚合时查询请求分发至每个分片中。在将结果返回客户端之前，该角色还会对结果进行聚合及排序，从而减少副本读取角色的工作量。负载管理角色解决问题的方式与<a href=\"https://www.programmingbrain.com/2022/11/what-is-database-proxy.html\">数据库代理</a>\"类似，不同点在于该角色并未以外部插件的形式存在，而是作为集群中的一部分。</p><p></p><p>负载管理角色可以有一个或多个实例，而每当实例数量到达 CPU 或内存的可配置上限时，则会另外配置一个新的负载管理角色，生成负载管理集群，并在该集群内拥有自己独立的一致性机制。</p><p></p><h2>高层设计</h2><p></p><p></p><p>这个设计模式中各个角色之间的基本互动形式可见图二中表示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/4image003-1674490178804.jpeg\" /></p><p>图二：解决方案的基本设计</p><p></p><h3>为什么选择Raft？</h3><p></p><p></p><p>作为一款名声在外且饱经考验的一致性校验算法，<a href=\"https://raft.github.io/\">Raft</a>\"&nbsp;远比&nbsp;<a href=\"https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html\">Paxos</a>\"&nbsp;要简单，但性能却有时能与后者相媲美，正如在<a href=\"https://emptysqua.re/blog/paxos-vs-raft/\">论文评论：Raft vs. Paxos</a>\"中所阐述的。</p><p></p><p>对于作者所提出的策略而言，Raft 只能同时拥有一个领导者是非常重要的。</p><p></p><h3>智能 Raft</h3><p></p><p></p><p>作者提议通过修改 Raft 协议以提升集群的整体性能，让 Raft 能意识到节点之间的不同并选举“更大”的可用节点作为领导者。这点对自动扩展的写操作尤为重要，因为同时只能有一个领导者，那么增强其能力最有效的方式就是提供一个“更大”的领导者，从而触发一个新的选举。Raft 要能识别并选举这个“更大”的节点作为领导者。</p><p></p><p>另一种方式是通过修改 Raft 使其能接受到一个“切换”指令，从而让集群将领导者切换至这个“更大”的节点。</p><p></p><p>第二种方式要更好，不仅对协议的修改要更小，还可以将领导者的切换与逻辑解耦。</p><p></p><p>这里所说的“更大”，是指 CPU、内存、存储技术（<a href=\"https://en.wikipedia.org/wiki/Solid-state_drive\">SSD</a>\"），或者其他资源大小，完全取决于有状态集群的需求目的。如果集群需要服务于复杂计算，那么大概会需要“更大”的 CPU 资源，如果集群需要服务于请求，则应该是会需要“更大”的内存资源和更好的存储技术。</p><p></p><h3>自动扩展策略</h3><p></p><p></p><p>作者将可扩展性的不同阶段以“马赫”命名，“马赫”本意为超过音速的物体移动速度，而在本文中，各个马赫阶段则喻指集群节点的数量。</p><p></p><p>注：“马赫”一词仅在本文中使用，并非是行业中专有名词。</p><p></p><h3>可配置的扩展触发器</h3><p></p><p></p><p>清楚自动扩展或缩减的时机很重要，在系统承受巨大压力时选择扩展并不是个明智的选择，这也是为什么负载管理角色所提供的背压十分关键。</p><p></p><p>作者将重点探讨随时间逐渐增加的需求这一情景。为此，有两种基本的配置可用于自动扩展的触发，其中的负载管理角色都需要能够识别触发自动扩展的时机并向操作人员发出通知。操作人员可以为人或软件系统，后者更好。</p><p></p><p>可配置的触发器如下。</p><p></p><h4>1. 借助平均响应时间的阈值</h4><p></p><p></p><p>负载管理角色的任务之一是监测请求的平均响应时间。当请求的平均响应时间到达某个特定的阈值后，则触发扩展的请求。需要向上配置扩展举例：上一个60分钟内，每个请求平均响应时间为3秒需要向下配置扩展举例：上一个60分钟内，每个请求平均响应时间小于0.5秒。</p><p></p><h3>2. 借助超时的阈值</h3><p></p><p></p><p>在自动扩展信号发送之前，超时的阈值是特定时间段内可能超时的请求所占百分比。需要向上配置扩展举例：上一个5分钟内，大于1%的请求超时需要向下配置扩展举例：不建议使用超时比例作为阈值，出于安全考量，集群中任何级别的超时都不建议用作向下扩展的阈值。</p><p></p><p>再回到图一的例子中，假设我们目前已经用可自动扩展的 RocksDB 替代了 Redis，这个全新可自动扩展的 RocksDB 集群将无需人类操作者或管理员的干预，自动根据是否超过阈值进行扩展。</p><p>在继续阅读本文之前，请注意：- 下文中每个马赫阶段所举的例子大多都注重于提升读取能力，以下文中所述的工作负载隔离，侧面提升写入能力。在马赫 IV 阶段之后会有专门章节表述目标写入能力的扩展。- 本文中的“节点”意指集群中的参与者或运行中的某个进程，不一定代表不同的硬件。- 创建分片之前应先配置需要分配的副本总数。- 任何配置下都可以启动集群，马赫 IV 中所述的是生产所用的最低推荐配置。- 写入、副本读取，以及负载管理这三个角色的实现均为模块化，且常部署在节点之上。举例来说，当某个节点被标记为“写入角色”时，节点虽然仅启用写入角色模块，但也包含副本读取和负载管理的未启用模块，允许该节点在后续按需切换角色。</p><p></p><h2>马赫 I</h2><p></p><p></p><p>在集群或系统的初始状态下，所以角色均在同一节点上启用，代表小型用例或测试管道等场景，类似图三中的示例。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/4image005-1674491096782.jpeg\" /></p><p>图三：单节点部署——马赫 I</p><p></p><p>在马赫 I阶段，所有组件均作为单一进程部署在同一个几点之上，这个单一的节点负责管理所有的读写请求。用例：主要推荐用于测试场景，不适合于生产环境。</p><p></p><h3>一致性与副本</h3><p></p><p></p><p>在马赫 I阶段，组件在内存模块内通信，因此不需要一致性或副本。</p><p></p><h2>马赫 II</h2><p></p><p></p><p>在马赫 II 阶段，开始部署包含双节点的集群。扩展出的第二个节点一般都是负载管理角色，确保能为响应请求的节点提供背压保护，并允许新节点开始同步数据。</p><p>拓扑结构如图四所示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/5image007-1674491096782.jpeg\" /></p><p>图四：双节点部署——马赫 II</p><p></p><h3>一致性与副本</h3><p></p><p></p><p>在马赫 II 阶段，无需考虑一致性，因为少于三个节点时是不可能建立 Raft 一致性的。</p><p></p><p>部署在节点二上的副本读取模块会复制与负载管理角色用时运行在节点二上的副本读取角色。需要注意的是，部署了负载管理角色的节点二上的副本读取角色不提供请求，这一设计背后的意图是为时时保持一个“几乎完全”同步的节点，该节点可以以额外的副本读取或领导者节点身份快速加入操作，如作者在马赫 III 阶段所述。</p><p></p><p>用例：可被用于可靠性不是非常重要或低运营成本的场景中。</p><p></p><h2>马赫 III</h2><p></p><p></p><p>在马赫 III 中，集群中又新增一节点，目前一共有三个节点。</p><p></p><p>新节点永远会以新的负载管理角色进入集群，客户端将被重新定向至新的负载管理，而在马赫 II 中的负载管理角色则会切换为副本读取角色。</p><p></p><p>马赫 III 的场景可见图五。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/3image009-1674491096782.jpeg\" /></p><p>图五：马赫 III 中的三节点部署</p><p></p><h3>一致性与复制</h3><p></p><p></p><p>三节点时暂时无需考虑一致性，因为即使是拥有三个节点，但只有节点一和二在主动服务于请求。</p><p></p><p>用例：通过将读写操作分割在不同节点之上，性能已经得到了提升。但如果某个节点故障而没有第二个副本读取节点，那么写入节点将被迫在新节点被分配之前负责处理读取请求。</p><p></p><h3>复原策略</h3><p></p><p></p><p>领导者故障：集群回到马赫 II 阶段的拓扑结构，节点二回到处理读写操作的阶段，直到新节点加入集群。节点二（副本读取）故障：领导者或写入角色开始处理读取请求，直到新节点加入集群。节点三（负载处理）故障：节点二将从副本读取角色切换为负载管理，节点一开始执行写入和读取操作。</p><p></p><p>以上三种情况都会向运营商发送一个信号，提示需要用新节点替换故障节点，新节点永远会以负载管理角色加入集群。</p><p></p><h2>马赫 IV</h2><p></p><p></p><p>在这一阶段，集群中有四个节点，其中一个是第二副本读取角色，部署情况如图六所示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/3image011-1674492416274.jpeg\" /></p><p>图六：马赫 IV 中的四节点部署</p><p></p><p>用例：用于生产时工作负载、良好性能，以及故障节点的优秀响应时间所需的最低配置。</p><p></p><h3>一致性与复制</h3><p></p><p></p><p>马赫 IV 阶段第一次引入一致性，但此时仍未进行选举。节点一将维持领导者角色，不浪费时间切换至新领导者，专注于写入操作。Raft 实现的扩展对这一安排至关重要。此外，如果领导者故障，则节点二或节点三成为新领导者，并回到马赫 III 的拓扑结构，这一决定由负载管理角色做出。</p><p></p><h3>复原策略</h3><p></p><p></p><p>领导者故障：Raft 的协议不允许在集群中只有两个节点时进行选举，因此负载管理会随机在包含运行中副本读取的节点二和节点三之间随机挑选一个领导者。</p><p></p><p>对马赫 IV 来说，副本与节点管理节点的复原策略与马赫 III 相同，可在创建分片之前将新的副本读取节点添加至集群中以达到可配置的最大数量。</p><p></p><h2>马赫 V、VI……</h2><p></p><p></p><p>新的副本读取可不断被添加至集群，直到到达可配置的上线从而触发分片。需要注意的是，副本读取的添加意味着新负载管理角色的添加，从而取代上一个负载管理角色；上一个负载管理会加入 Raft 的一致性并开始服务于读取请求。</p><p></p><p>用例：随着集群规模的增加，可靠性也在增加，单个节点的故障将不再像小型部署一样影响重大。</p><p></p><h3>复原策略</h3><p></p><p></p><p>领导者故障：Raft 在可用的副本读取中选举一个新的领导者，新领导者将告知负载管理角色选举结果。副本读取故障：新节点应需加入集群。负载管理故障：上一个被加入集群的副本读取角色会接手负载管理角色的责任，且在新节点被配置之前本身不再服务于请求读取，同理，新节点会以负载管理角色加入集群。</p><p></p><h2>读取密集型与写入密集型的场景</h2><p></p><p></p><p>在马赫 IV 之前的阶段中，考虑到可靠系统的最低可复制配置，这套自动扩展的方案并未将负载的特性纳入考虑范围。在后续的阶段中，系统自动扩展的方式将发生变化，将通常情况下的负载区分为读取密集型（80%及以上的读取操作）、写入密集型（80%及以上的写入操作），或均衡型（其他情况）。虽然可能不包含一些特殊情况，但这个方案是客户化的模式，可以按需适应特殊场景。我们的目标只是为解决多数用例而非全部。</p><p></p><h3>扩展读取密集型场景</h3><p></p><p></p><p>随着新节点按照马赫 I 至 VII 阶段的模式不断被加入集群，我们需要一个最简单的策略，能够代表七个节点（一个读取管理、一个写入领导者、五个副本读取），并让集群开始以分片标签的形式创建已有数据的分片，并将传入的请求负载划分到新创建的分片之中。</p><p></p><p>在进去两个分片的运行之前，我们需要新增一个节点，达到八节点才能支持图七中所介绍的拓扑结构。</p><p>使用用例: 需求不断增长的大规模场景。</p><p></p><p>图七所展示的两个分片的设置情况，两个分片各自包含一个领导者和两个副本读取角色，并在负载管理节点中还有额外的一个副本。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/3image013-1674491679528.jpeg\" /></p><p>图七：两个分片的拓扑结构示例</p><p></p><p>当每个分片中都配置了足够的节点，我们可以进一步扩展至三到四个分片。举例来说，如果分片二扩展至七个节点，那么接下来新增的节点将被分至第二个分片之中。</p><p></p><p>每个负载管理角色只持有一个分片的副本，负载管理角色可在需要时在该分片上处理写入或读取操作，但若想充分满足客户需求，负载管理需要从所有分片中进行读和写。</p><p></p><h3>扩展写入密集型场景</h3><p></p><p></p><p>对于写入操作为重点或写入操作存在合理退化的情况中，存在两种扩展集群写能力的方式：</p><p>提供一个更大的领导者第一种方式是提供比当前领导者更大的节点，“更大”可以指内存、CPU、读写存储速度等等。这一节点最初将按管理配置为负载管理，并在与当前领导者完成同步（如前文各个马赫阶段所述）之前都会作为负载管理角色存在。一旦完成同步，负载管理将开始新的选举，成为 Raft 一致性的新领导者并与旧领导者互换。集群中的每个节点都有从1到5的等级规划，其中1代表最低的等级规划即最少的资源，5则是最高的即较大的资源数量。分片一旦纵向扩展到达极限（节点等级5），则开始<a href=\"https://www.geeksforgeeks.org/what-is-sharding/\">分片</a>\"。新的领导者被配置为副本只读，从同步上一任领导者的分片开始，逐渐成为仅负责当前分片的领导者本身，新增的节点等级（1至5）必须可配置。</p><p></p><p>这与前文中所描述的读取密集型分片触发方式是不同的。读取密集型分片的触发是基于水平规模（节点数），而写入密集型场景的分片触发则是基于垂直规模的，换句话说，领导者的规模到达了最大量（5）。</p><p></p><p>也可以在配置新分片时指定等级，比如让两个领导者都是等级3，但两个等级3会组成一个“等级6”的写入能力集群，不过这也应该是可配置的。在这种情况下，以前的等级5写入节点将被在两个分片上的两个等级3的节点所取代。</p><p></p><h3>分片策略的标签优先级</h3><p></p><p></p><p>问题点：将信息清晰有效地划分至分片并不容易，尤其是在数据量庞大或数据复杂时。</p><p></p><p>简单的解决方案：每当一条记录或对象被存储在集群中时，就给其分配一个1至1000的随机桶 ID，并确保在大规模时，每个桶都有类似数量的对象分配在其中，以均衡分片。</p><p></p><p>在需要分片时，桶 ID 可用于判断对象所属的分片。举例来说，假设桶的总数只有1000，那么在两个分片中，第一个分片中可以仅包含第1至500个桶，第二个分片中则仅被分配第501至1000的桶。</p><p></p><p>每当需要新的分片时，都重复这个桶的划分过程。因此在我们的例子中，分片的最大数量为1000，远超大多数情况下的正常值。</p><p></p><h2>结论及未来展望</h2><p></p><p></p><p>作者不奢望能通过这篇文章解决有状态系统自动扩展的所有问题，只是提供了一个基于作者职业生涯中所使用过的技术模板和模式的方案，并将其标准化，作为有状态自动扩展实现的基础。这些涉及可能不会完美适配必须对所有甚至全部分片执行大部分读取操作的场景，在这些情况下，建议采用其他更好的分片桶定义，从而尝试在一个或尽量少的分片中分配所有需要的数据。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/design-proposal-autoscaling-stateful-systems/\">https://www.infoq.com/articles/design-proposal-autoscaling-stateful-systems/</a>\"</p>",
    "publish_time": "2023-03-28 09:08:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java近期新闻：新JEP、GraalVM 23早期访问构建、Infinispan、Mojarra、Micrometer Metrics",
    "url": "https://www.infoq.cn/article/F6cg96zpRMMi9uAq1zyU",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p></p><p>上周，JEP 440（<a href=\"https://openjdk.org/jeps/440\">记录模式</a>\"）已从JEP Draft 8300541<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007470.html\">提升</a>\"到Candidate状态。该JEP最终确定了这一特性，并针对前2轮<a href=\"https://openjdk.java.net/jeps/12\">预览</a>\"的反馈做了增强。这两轮预览分别是在JDK 20中发布的JEP 432（<a href=\"https://openjdk.org/jeps/432\">记录模式第2次预览</a>\"）和在JDK 19中发布的JEP 405（<a href=\"https://openjdk.org/jeps/405\">记录模式预览</a>\"）。该特性为这门语言添加了记录模式，用于解构记录值。记录模式可以与类型模式搭配使用，为“强大的声明式、可组合数据导航和处理形式”提供支持。最近，类型模式被扩展应用于switch 的选择标记：JEP 420（<a href=\"https://openjdk.java.net/jeps/420\">switch模式匹配第2次预览</a>\"，在JDK 18中交付）和JEP 406（<a href=\"https://openjdk.java.net/jeps/406\">switch模式匹配预览</a>\"，在JDK 17中交付）。JEP 432最重要的变化是不再支持在增强for语句头中使用记录模式。</p><p>&nbsp;</p><p>类似地，JEP 441（<a href=\"https://openjdk.org/jeps/441\">switch模式匹配</a>\"）已经从JEP Draft 8300542<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007471.html\">提升</a>\"到Candidate状态。该JEP最终确定了这一特性，并针对前4轮的预览反馈做了增强：JEP 433（<a href=\"https://openjdk.org/jeps/433\">switch模式匹配第4次预览</a>\"），在JDK 20中交付；JEP 427（<a href=\"https://openjdk.org/jeps/427\">switch模式匹配第3次预览</a>\"），在JDK 19中交付；JEP 420（<a href=\"https://openjdk.java.net/jeps/420\">switch模式匹配第2次预览</a>\"），在JDK 18中交付；JEP 406（<a href=\"https://openjdk.java.net/jeps/406\">switch模式匹配开关预览</a>\"），在JDK 17中交付。该特性通过在switch表达式和语句中支持模式匹配来增强语言。</p><p>&nbsp;</p><p>JEP 442（<a href=\"https://openjdk.org/jeps/442\">外部函数和内存API第3次预览</a>\"）已经从JJEP Draft 8301625<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007473.html\">提升</a>\"到Candidate状态。这个JEP基于之前的反馈做了改进：JEP 434（<a href=\"https://openjdk.org/jeps/434\">外部函数和内存API第2次预览</a>\"），在JDK 20中交付；JEP 424（<a href=\"https://openjdk.org/jeps/424\">外部函数和内存API预览</a>\"），在JDK 19中交付；JEP 419（<a href=\"https://openjdk.org/jeps/419\">外部函数和内存API第2轮孵化</a>\"），在JDK 18中交付；JEP 412（<a href=\"https://openjdk.org/jeps/412\">外部函数和内存API第1轮孵化</a>\"），在JDK 17中交付。该特性为Java应用程序提供了一个可以与Java运行时之外的代码和数据进行互操作的API，让它们可以高效地调用外部函数以及安全地访问不受JVM管理的外部内存。JEP 434的更新包括：在Arena接口中集中管理原生段（native segments）的生命周期；使用一个新元素解引用地址布局，增强布局路径；删除VaList类。</p><p>&nbsp;</p><p>JEP Draft 8303683（<a href=\"https://openjdk.org/jeps/8303683\">虚拟线程</a>\"）是由<a href=\"https://inside.java/u/RonPressler/\">Ron Pressler</a>\"（Oracle Loom项目架构师和技术主管）和<a href=\"https://inside.java/u/AlanBateman/\">Alan Bateman</a>\"（Oracle Java平台组架构师）于上周提交的。该JEP建议根据前2轮预览的反馈最终确定这一特性：JEP 436（<a href=\"https://openjdk.org/jeps/436\">虚拟线程第2次预览</a>\"），在JDK 20中交付；JEP 425（<a href=\"https://openjdk.org/jeps/425\">虚拟线程预览</a>\"），在JDK 19中交付。该特性为Java平台提供了虚拟线程。这种轻量级的线程极大地减少了编写、维护和观察高吞吐量并发应用程序的工作量。与JEP 436相比，其最重要的变化是虚拟线程现在完全支持<a href=\"https://openjdk.org/jeps/8303683#Thread-local-variables\">线程局部变量</a>\"，取消了不使用这些变量的选项。要了解更多关于JEP 425的细节，可以阅读<a href=\"https://www.infoq.com/news/2022/05/virtual-threads-for-jdk19/\">InfoQ的新闻报道</a>\"及观看<a href=\"https://www.linkedin.com/in/jos%C3%A9-paumard-2458ba5/\">José Paumard</a>\"（Oracle Java平台组Java开发大使）提供的JEP Café<a href=\"https://inside.java/2022/06/08/jepcafe11/\">截屏视频</a>\"。</p><p>&nbsp;</p><p>JEP Draft 8304400（<a href=\"https://openjdk.org/jeps/8304400\">启动多文件源代码程序</a>\"）也是由Pressler提交的。该JEP建议增强Java启动器，让它可以执行以一个或多个Java源代码文件形式提供的应用程序。这样就可以推迟全面的项目设置，使得从小型应用程序到大型应用程序的过渡更加平滑。</p><p>&nbsp;</p><p></p><h4>JDK 20</h4><p></p><p></p><p>JDK 20仍处于<a href=\"https://openjdk.java.net/jeps/3#rc\">发布候选</a>\"阶段，GA版本预计将于2023年3月21日发布。<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B36\">Build 36</a>\"仍然是JDK 20<a href=\"https://jdk.java.net/20/\">早期访问构建</a>\"的当前构建。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/20/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JDK 21</h4><p></p><p></p><p>JDK 21的<a href=\"https://jdk.java.net/21/\">早期访问构建</a>\"<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B14\">Build 14</a>\"也于上周发布，其中包括来自Build 13的<a href=\"https://github.com/openjdk/jdk/compare/jdk-21%2B13...jdk-21%2B14\">更新</a>\"，该更新修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2021%20and%20%22resolved%20in%20build%22%20%3D%20b14%20order%20by%20component%2C%20subcomponent\">问题</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"和<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\"，我们鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告Bug。</p><p>&nbsp;</p><p></p><h4>GraalVM</h4><p></p><p></p><p>Oracle实验室<a href=\"https://twitter.com/graalvm/status/1636747570377523200?cxt=HHwWgICw9cnP8rYtAAAA\">发布</a>\"了GraalVM 23.0.0的最新早期访问开发构建。其新特性包括：对<a href=\"https://docs.oracle.com/en/graalvm/enterprise/20/docs/reference-manual/native-image/Resources/\">Native Image Bundles</a>\"的初始支持；经过改进的Linux上AWT支持；原生镜像推荐。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/graalvm/graalvm-ce-dev-builds/releases/tag/23.0.0-dev-20230317_0549\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Spring Framework</h4><p></p><p></p><p>Spring Tools 4.18.0<a href=\"https://spring.io/blog/2023/03/15/spring-tools-4-18-0-released\">发布</a>\"，新特性包括：经过升级的Eclipse 2023-03 IDE；经过改进的新一代Spring Data存储库查询方法内容辅助；修复了导致VSCode中常规Java内容辅助停止工作的问题；修复m2e资源文件（如application.properties ）不向目标文件夹复制的问题。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/spring-projects/sts4/releases/tag/4.18.0.RELEASE\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Quarkus</h4><p></p><p></p><p><a href=\"https://quarkus.io/blog/quarkus-3-0-0-alpha6-released/\">Quarkus 3.0.0的第6个Alpha版本</a>\"提供了2个新特性：通过将quarkus.datasource.jdbc.telemetry 属性设置为true来启用OpenTelemetry for JDBC；CredentialsProviders接口现在支持MongoDB连接。该版本还进行了依赖项升级，包括：SnakeYaml 2.0、Maven Compiler Plugin 3.11.0、Maven OpenRewrite Maven Plugin 4.41.0、SmallRye Common 2.1.0和JBoss Threads 3.5.0.Final。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.0.0.Alpha6\">更新日志</a>\"。</p><p>&nbsp;</p><p></p><h4>Hibernate</h4><p></p><p></p><p><a href=\"https://in.relation.to/2023/03/17/orm-62-cr4/\">Hibernate ORM 6.2的第4个候选版本</a>\"根据Java社区的反馈提供了33个Bug修复和28个改进。预计这将是最终版本发布之前的最后一个候选版本。</p><p>&nbsp;</p><p></p><h4>Micrometer</h4><p></p><p></p><p><a href=\"https://github.com/micrometer-metrics/micrometer/releases/tag/v1.11.0-M2\">Micrometer Metrics 1.11.0的第2个里程碑版本</a>\"提供了一些新特性，包括：一个新指标jvm.threads.started ，用于报告JVM中活动应用程序线程的总数；一个新的ElasticSearch端点\\_index\\_template，用于创建索引模板；将GC名称添加到jvm.gc.pause指标；在基于OSGi的Java运行时上支持Micrometer 库。</p><p>&nbsp;</p><p>类似地，Micrometer Tracing 1.1.0的第2个里程碑版本也提供了一些新特性，包括：<a href=\"https://spring.io/projects/spring-cloud-sleuth\">Spring Cloud Sleuth</a>\"注解的等效物；依赖项升级到Micrometer 1.11.0-M2和OpenTelemetry 1.24.0。</p><p>&nbsp;</p><p></p><h4>Infinispan</h4><p></p><p></p><p>Infinispan 14.0.7.Final<a href=\"https://infinispan.org/blog/2023/03/13/infinispan-14\">发布</a>\"，支持Spring Framework 6和Spring Boot 3。它提供了一些值得注意的Bug修复，包括：MetricsCollector类中的NullPointerException；JSON解析器不能正确报告错误位置；Redis序列化协议（RESP）端点不能解析超过数据包大小的请求；并发访问<a href=\"https://spring.io/projects/spring-session\">Spring Session</a>\"集成会导致会话属性丢失。</p><p>&nbsp;</p><p></p><h4>Piranha</h4><p></p><p></p><p><a href=\"https://piranha.cloud/\">Piranha</a>\"23.3.0<a href=\"https://github.com/piranhacloud/piranha/releases/tag/v23.3.0\">发布</a>\"，显著的变化包括：升级<a href=\"https://codeql.github.com/\">CodeQL</a>\"工作流；为DefaultAnnotationManager类添加JUnit测试；修复当端点应用程序仍处于部署过程中时报RuntimeException的问题。要了解关于这个版本的更多细节，请查阅<a href=\"https://javadoc.io/doc/cloud.piranha/project/latest/index.html\">文档</a>\"和<a href=\"https://github.com/piranhacloud/piranha/issues?q=is%3Aissue+-label%3Awontfix+milestone%3A23.3.0+is%3Aclosed\">问题跟踪系统</a>\"。</p><p>&nbsp;</p><p></p><h4>Reactor项目</h4><p></p><p></p><p><a href=\"https://github.com/reactor/reactor/blob/main/README.md\">Reactor</a>\" 2022.0.5是该项目的<a href=\"https://github.com/reactor/reactor/releases/tag/2022.0.5\">第5个维护版本</a>\"，依赖项升级到reactor-core 3.5.4、reactor-addons 3.5.1、reactor-netty 1.1.5、reactor-kafka 1.3.17和reactor-kotlin-extensions1.2.2。</p><p>&nbsp;</p><p></p><h4>Eclipse Mojarra</h4><p></p><p></p><p>Eclipse Mojarra 4.0.2<a href=\"https://twitter.com/OmniFishEE/status/1636829803721392128?cxt=HHwWgICwiZiCmLctAAAA\">发布</a>\"，带来了一些显著的变化，包括：清理MockServletContext类，删除未使用的方法并添加@Override注解；清理ParseXMLTestCase类，删除未使用的方法、变量和注释掉的代码；确保@FacesConfig注解中的version()方法不会返回null；修复了在更新数据表分页标题中的按钮时报NumberFormatException的问题。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/eclipse-ee4j/mojarra/releases/tag/4.0.2-RELEASE\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Apache软件基金会</h4><p></p><p></p><p>Apache Groovy 4.0.10<a href=\"https://www.mail-archive.com/announce@apache.org/msg08029.html\">发布</a>\"，带来了一些值得注意的Bug修复和改进，包括：来自GroovyScriptEngine类的令人困惑的错误消息；局部变量值未丢弃时的内存泄漏；@Builder注解在JDK 16上不起作用；MissingPropertyException截断嵌套类的类名。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12352914\">发布说明</a>\"。</p><p>&nbsp;</p><p>类似地，<a href=\"https://www.mail-archive.com/announce@apache.org/msg08028.html\">Apache Groovy 3.0.16</a>\"也带来了一些值得注意的Bug修复，包括：无法在JRE 16+的闭包或Lambda表达式上从BiPredicate接口调用方法；使用@CompileStatic注解会混淆静态导入的实例和方法；IllegalAccessException会使用JDK 17和Groovy 3.0.9的默认接口方法。该版本还支持JDK 16。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12352913\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JHipster</h4><p></p><p></p><p>JHipster团队<a href=\"https://twitter.com/pascalgrimaud/status/1635700948583448582?cxt=HHwWjICw8ZXWlrMtAAAA\">发布</a>\"了JHipster Lite 0.29.0，带来了新特性和功能增强，包括：根据用户反馈删除JHipsterModulePackageJson类的依赖；删除当Cassandra数据库应用程序中正在测试的活动ApplicationContext会话超过四个时的警告消息；新的Redis依赖项和配置。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.29.0\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JReleaser</h4><p></p><p></p><p><a href=\"https://jreleaser.org/\">JReleaser</a>\" 1.5.1（一个简化项目发布的Java实用工具）<a href=\"https://andresalmiray.com/jreleaser-1-5-1-has-been-released/\">发布</a>\"，带来了一些值得注意的修复，包括：添加<a href=\"https://jreleaser.org/guide/latest/reference/assemble/native-image.html\">Native Image</a>\"汇编程序实用工具中缺少的graalVMNativeImage属性；<a href=\"https://jreleaser.org/guide/latest/reference/assemble/java-archive.html\">Java Archive</a>\"实用工具为JAVA_OPTS环境变量生成的错误格式；改进执行外部命令时的错误处理。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/jreleaser/jreleaser/releases/tag/v1.5.1\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JobRunr</h4><p></p><p></p><p>JobRunr 6.1.2<a href=\"https://github.com/jobrunr/jobrunr/releases/tag/v6.1.2\">发布</a>\"，主要是修复了两个Bug：当使用MySQL并将useServerPrepStmts属性设置为true时，元数据更新失败，并导致最终关闭；<a href=\"https://www.jobrunr.io/en/documentation/configuration/quarkus/\">JobRunr Quarkus扩展</a>\"中JobRunrDocumentDBStorageProviderProducer类未使用正确配置的问题。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/java-news-roundup-mar13-2023/\">https://www.infoq.com/news/2023/03/java-news-roundup-mar13-2023/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/SZXNxA7DaBzCddCNAUxG\">Java 20 发布，新特性一览：Amber、Loom 和 Panama 项目</a>\"</p><p><a href=\"https://www.infoq.cn/article/P1vXcLlwewcK5XQQIQdN\">Java 近期新闻：JDK 21 序列集合、JDK 20 向量 API、Gen ZGC、Hilla 2.0</a>\"</p>",
    "publish_time": "2023-03-28 09:22:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "超大模型工程应用难？快手给出短视频场景下的实战指南",
    "url": "https://www.infoq.cn/article/jmGdd6N1vgC64KJiKxyp",
    "summary": "<p>号称性能吊打<a href=\"https://www.infoq.cn/video/So2yItKrdYsDZpEG7DjS\"> ChatGPT </a>\"的 GPT-4 近日又一次引爆关注。&nbsp;据OpenAI介绍，当任务的复杂性达到阈值时，二者就会显现差异。它的发布是一件新鲜事，但其背后的多模态大模型技术其实已经发展多年。如今，大模型工程应用的能力成为很多企业关注的重点，也是以 ChatGPT 为代表的预训练大模型广受关注的原因。</p><p></p><p>目前，<a href=\"https://s.geekbang.org/search/c=0/k=%E5%A4%A7%E6%A8%A1%E5%9E%8B/t=\">大模型</a>\"从自然语言处理已经扩展到多媒体视觉、多模态等多领域。近日，在英伟达 GTC 2023 大会上，快手的技术专家张胜卓、韩青长、李杰以多模态超大模型在快手短视频场景下的落地为例，分享了多模态超大模型落地过程中的难点、技术解决方案和性能收益。InfoQ对分享内容进行梳理，以飨读者。</p><p></p><p>主流训练大模型的方法主要会从算法模型架构、分布式并行加速、内存和计算优化三个层面来比较。确定适当的算法和模型架构是大模型训练的第一步。</p><p></p><p><a href=\"https://www.infoq.cn/article/Nou4kpZa2AU5dadx61z6\">快手</a>\"多模态超大模型，采用的是类 T5 的 Encoder + 多 Decoder 架构，通过 Encoder 接受图像、文本多模态语料，学习特征提取能力；通过 Decoder 输出上下文 Embedding 向量，为下游任务提供用户特征信息。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bd0f589f215e68e971b168788df97043.png\" /></p><p></p><p>数据规模和模型容量一直以来是决定深度学习发展的关键指标。条件计算的引入实现了可以在不增加计算量的情况下，增加模型参考量。稀疏门控 MoE 是超大型神经网络的条件计算方法之一。</p><p></p><p>快手多模态超大模型采用的是门控神经网络稀疏 MoE 架构，这种架构实现了模型容量超过 1000 倍的改进，在 GPU 集群的计算效率损失很小，可以实现多场景多任务统一建模的形式。门控网络让快手可以通过对短视频、商业化等不同业务场景构建不同的专家组来选择最合适的专家来处理数据。稀疏门控 MoE 架构吸收信息的能力受限于参数的数量。所以，快手通过扩展专家数量，达到增加模型参数量、提升模型效果指标的目的。目前，快手该模型的专家数量近 200 个，模型参数量达 100B（千亿）。</p><p></p><p>快手希望多模态大模型可以帮助公司构建通用的理解能力，在推荐、搜索、广告等核心业务上取得一些业务收益。那么，多模态超大模型工程应用为什么这么难？</p><p></p><h1>多模态超大模型工程应用，难在哪？</h1><p></p><p>快手技术团队发现，训练时间漫长、推理效率过低、部署相对复杂是多模态超大模型工程应用的三大拦路虎。</p><p></p><h4>1. 训练时间漫长</h4><p></p><p>2017 年 Transformer 结构提出后，深度学习模型参数突破了 1 亿。2021 年末，Google 发布的 Switch Transformer，首次将模型规模提升至 1.6 万亿。与超高速增长的模型参数量相反，GPU 显存增长有限，有数据显示每 18 个月仅翻了 1.7 倍。模型参数量的增长和硬件的显存的增长之间的差距越来越大。但是，超大的计算量和超大的参数量是训练 AI 大模型训练的必经之路。在千亿模型上训练就需要 1.5TB 显存，但快手当时使用的最新硬件 A100 只有 80GB。</p><p></p><p>显存不足只是一方面。越大的模型参数就需要越大的规模训练样本和越长的训练时间。就算有算力的增速，不断扩大的训练数据集耗费的训练时间依然是一般业务团队难以承受的漫长。漫长的训练时间带来的是成本不可控的增长。</p><p></p><h4>2. 推理效率过低</h4><p></p><p>大模型的高效推理是实现大模型工程应用落地的关键所在。相对训练环节，推理环节在计算精度、算力消耗量等方面的要求较低，但显存增长速度的限制同样会出现在推理环节。推理速度受限于通信延迟和硬件内存带宽，所以不仅要在保证低延迟的前提下，尽可能节省计算资源，还要尽可能使用现有显存满足推理的需求，提升推理效率。可现实情况是，100B 模型约需 400GB 显存，但快手技术团队当时使用的主流推理卡 A10 显存仅 24GB。</p><p></p><h4>3. 部署相对复杂</h4><p></p><p>部署决定了多模态超大模型能否成功工程应用。常见的大模型部署难点有参数量过大、计算反馈慢等问题。大模型参数复杂，但在实际应用中却不需要那么复杂的参数和计算。一般企业会对大模型进行精剪、蒸馏、压缩，减少大模型的冗余。快手在部署环节也遇到了多模态数据预处理复杂和 CPU/GPU 负载不均衡的问题。</p><p></p><p>关关难过关关过，快手技术团队是怎么逐步解决这些问题的？</p><p></p><h1>快手多模态超大模型落地技术方案</h1><p></p><p></p><h4>1. 混合并行训练</h4><p></p><p>“训练是我们大模型应用的第一步，也是耗时最久、难度考验最大的一部分。”快手大模型训练和优化专家张胜卓表示。为了优化训练环节，快手采取了混合并行训练的方式，通过并行策略的选择和加速技术的提升来提升训练质量和效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/36ed8c6bd5eac5bf8947f2af040332b2.png\" /></p><p></p><p>目前主流的并行策略包括数据并行技术、张量并行技术、流水并行技术和专家并行技术。快手技术团队在实施混合并行训练时，也发现一些需要考虑的问题。</p><p></p><p>第一，数据并行技术可以将数据分成不同的样本，用不同的设备同时做计算，让每个设备都有一个完整的模型。这样带来的问题是，单个设备上模型可以加载，但是受到显存的制约。所以在多个机器之间并行的时候，快手技术团队通过 AllReduce 来实现多机 / 多卡之间的梯度同步。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b6694a3715a6bcc3ff0f1fc61c737033.png\" /></p><p></p><p>第二，在更新模型的时候，卡与卡之间激活传输产生的通讯开销几乎是一个参数量级别，所以如果把模型做水平方向的拆分，子模型同时并行在多张设备上，就需要考虑层与层之间做计算时的依赖关系（激活开销）。</p><p></p><p>第三，流水并行，指的是在垂直面上切分，把不同的层切成不同子模块，并行在不同的卡上。流水并行是在模型上做并行，并行开销在不同设备上的边界上。常见的流水并行切分策略，一种是通过参数量，保证参数在不同的卡上面相对来讲比较平衡；另一种是基于规则，把不同的 Transformer Layer，按 Transformer Layer 来切分。前者容易忽略不同层算力和计算需求不一样，后者在相同规则的 layer 之间，算力是平行的。</p><p></p><p>但是这里又会出现新的问题，由于在复杂模型场景下，layer 的种类非常多，用简单规则很难匹配，粒度太粗、规则太简单，很难覆盖快手的真实场景。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/59c2c1dc15b366dd7c5a6c6f06721af7.png\" /></p><p></p><p>所以，快手技术团队使用了成本模型的方式，把显存和计算作为两个主要的约束做一些平衡，同时考虑显存不会 OOM（显存不足）的场景下，怎么能够把计算做到相对平均，在流水线上面能够没有显著的计算瓶颈。这样做的好处在于，可以达到更细颗粒度的控制。快手技术团队通过这个成本模型来实现自动化的并行切分，相对于简单的规则匹配，性能显著提升了 5%-15%。</p><p></p><p>第四，专家并行是针对于 MoE 架构才有的，不同的 Token 会经过 gate 网络进入不同的专家网络上面输出，再到不同的设备上面。所以在不同的设备上面还有通讯，它的通讯量跟激活值 + 专家数有关系。</p><p></p><p>基于此，快手制定了如下的并行策略。</p><p></p><p>对于千亿模型，由于硬件条件使用的训练环境是单机 8 卡 A100，通过 NVLink 来高速互联。所以快手会优先考虑，把并行成本最高的这部分专家并行放在单机多卡之间。</p><p></p><p>另外，把张量并行和数据并行放在单机多卡，应用于其他层上，这样单机内部通过高速互联可以形成专家的并行、张量和数据的混合并行。另外，快手技术团队在不同的机器之间为了降低通讯量，使用了流水并行，相当于把不同的层切分成不同的 stage，构建这样一个流水线。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/686f5f217af74d48003eb894505609a5.png\" /></p><p></p><p>除了并行策略，快手还从加速技术方面降低整体训练耗时。</p><p></p><p>一方面，快手使用了零冗余优化器，在非 MoE 层上面使用了 ZeRO—2，把 optimizer states 数据和 gradients 数据做了一些零冗余的切分。在不增加通信量的情况下，可以降低 GPU 的显存占用。在 MoE 层禁用了 ZeRO，避免掉跨机之间做通讯的情况。</p><p></p><p>另一方面，快手做了一些 Activation Checkpoint 的细粒度控制。“比如我们在 MoE 层 E 前向的时候会产生一次 ALL2ALL 的通讯，我们得到 MoE 层的输出，如果这时候在反向的时候再做重计算的话，它会重新触发一次 ALL2ALL 的通讯，实际上显存的占用和通讯成本的考量来看，ALL2ALL 成本会更大。所以我们会把 MoE 的细粒度检测点关掉，只在不需要 ALL2ALL 的层上面做一些 Activation Checkpoint。”另外，快手技术团队也集成了一些 ALL2ALL 通信优化库，优化通信性能。</p><p></p><p>此外，快手技术团队还采用了一些通用的优化技术，比如使用了 PowerSGD 梯度压缩算法，可以来提升多机之间的线性扩展比；集成了 FlashAttention 的优化，除了降低显存之外，也提升了计算性能；在 GPUDirect RDMA 等方面也使用了一些技术。</p><p></p><h4>2. 优化显存和实现高效推理</h4><p></p><p>快手多模态大模型的推理优化基于自研的推理引擎开发的。据悉，之所以选择自研推理引擎，一方面是因为在 DeepSpeed 上推理的成本有点高，而且难以集成量化、手写算子等自定义优化；另一方面，由于多模态大模型是一个非标准的网络，还有可能涉及到多卡推理，目前的 TensorRT 生态对多卡推理支持的并不太好。所以，快手自研发了一套推理引擎来作为 TensorRT 的补充方案。</p><p></p><p>快手的这套推理引擎有着高性能、高易用性和支持多卡推理的特点。</p><p></p><p>高性能方面，这套推理引擎支持一些常见的优化的手段，包括内存池的复用、MoE 和一些常见算子的融合、通过 Transformer 里变长序列的计算减少 padding 混合精度，以及 FP16 和 INT8 的混合精度。</p><p>高易用性方面，这套引擎提供了 Operator 和 Tensor 的抽象分装。Operator 实现了像类似 TensorRT 的一个 plugin，用户只需要自定义一些接口就可以；在 C++ 里提供了类似 PyTorch 的一个 API，开发者可以像在 PyTorch 里一样把这些 OP 搭建成一个模型，还可以通过参数来修改网络的结构，实现新 OP 和网络的成本比较低。Tensor 实现了一些新建、拷贝、Delete 等接口的便利性。</p><p></p><p>支持多卡推理方面，通过 Pipeline 并行、Tensor 并行和专家并行实现。</p><p></p><p>大模态大模型如何选择？</p><p></p><p>据韩青长介绍，一个千亿模型在 FP32 精度下大概要占 400GB 左右的显存。快手主要的推理机器是两卡 A10，加起来也只有 48GB 的显存，所以就需要模型压缩技术。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf75d984d76647119f9136b6a6d6b362.png\" /></p><p></p><p>主流的模型压缩技术主要有模型剪枝、知识蒸馏、结构搜索和权重量化等。快手技术团队对比这些压缩技术发现，在迭代周期上模型剪枝大概需要 2 个 base 的训练周期，知识蒸馏需要 1 个 base 的训练周期，结构搜索是大于 2 个 base 训练周期，而权重量化是小于 1 个 base 训练周期，仅在这一项上权重量化优势比较明显。另外，权重量化在 Train From Pretrained 和 Train From Scratch 上效果都会比较好，基于以上这些信息的考量快手选择权重量化作为压缩手段。权重量化对于模型精度几乎是无损的。</p><p></p><p>量化效果如何保证？</p><p></p><p>快手采用的是 QAT 量化训练，通过在训练的时候插入一些伪量化的算子来模拟量化，学习一些量化的 scale，并把这些 scale 传给推理端，推理的时候只需要根据 QAT 学习到的 scale 执行量化的卷积算子以及反量化的计算就可以了。</p><p></p><p>此外，针对 Transformer 类大模型的权重量化策略，快手通过 QAT 量化训练提升效果，并结合推理算子融合规则插入量化算子。</p><p></p><p>第一，使用 QAT 量化训练的时候来提升训练效果。一是对称和非对称的量化方式搭配；二是选择多种策略选择最优的量化截断区间，来减少量化损失。第二，结合推理算子融合规则插入量化算子，比如已知在推理时 bias 会被融合进一些别的算子中，那么在 QAT 的时候就不去对 bias 进行量化，这样就能够提升模型推理速度，同时保证模型效果。</p><p></p><p>最终，快手通过 INT8 量化，将模型参数从 400GB 减小到 100GB。虽然压缩后的 100GB 距离目标的 48GB 还很远，但是已初见成效。</p><p></p><p>为了进一步实现目标，快手基于模型结构设计提出了权重选择性加载的策略，一方面通过对同一模型，在不同场景下只加载对应 group 的 MoE 权重，节省显存；另一方面优化模型加载机制，轮流为每个 op 加载权重，避免将所有权重拷贝到 host memory，节约实例内存。</p><p></p><p>层层手段下，快手成功将单场景部署的模型参数减小到了 40GB 以下。但是，主流推理卡（A10 24GB 显存）还是放不下，快手技术团队还需要继续想办法。</p><p></p><p>多卡推理或许是解决的办法。快手依照各部分的计算时间和参数量设计模型切分策略，选择流水线并行的方式，将模型和数据进行切分，以流水线形式计算，提高 GPU 利用率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/8761bc4c2edf9d1eaf32686873a0549a.png\" /></p><p></p><p>整个模型结构分为两部分，前部是 Feature Extractor，计算时间长、参数量小；后部是 Encoder，包含 MoE 的结构，计算时间短、但参数量大。快手的切分策略是在 Encoder 的靠前位置切一刀，将两部分别放在 GPU0 和 GPU1 上。从上图的 timeline 可以看到，两张卡的任务分配并不够均匀，明显 GPU1 的利用率比较低。因此，快手提出了一种改进的“切分策略二”——将 Feature Extractor 和 Encoder 各自平均分成两半，整个模型分成四部分，分别依次放在 GPU0/GPU1、GPU0/GPU1 上，实现任务分配更加均匀，提高两张卡的利用率。虽然这样做会增加一些通讯开销，但由于通讯量比较小，开销在可控范围内。</p><p></p><p>至此，显存的问题终于得到了解决。</p><p></p><p>计算的角度，如何优化推理呢？快手的答案是 MoE 实现优化和算子融合。</p><p></p><p>模型中的特殊结构主要是 MoE，快手技术团队首先按照 DeepSpeed 的方式对 MoE 进行了实现，但发现了两个可改进点：</p><p></p><p>第一，padding 引起冗余计算，计算量和存储量都与 capacity 参数成正比，capacity 参数是在实际计算过程中根据输入求出来的一个参数，这个 capacity 参数对于某些输入可能会很大，这样的情况下会浪费非常多的计算量和存储空间。</p><p></p><p>第二，DeepSpeed 方案是串行执行 expert 计算的，需要依次启动多个 kernel，GPU 利用率比较低。快手技术团队在测试的时候发现，这个实现下的 MoE 计算占比大约 70%—80%，而且非常容易 OOM（显存不足）。</p><p></p><p>快手技术团队着手优化 MoE 部分的实现，优化这两个问题。</p><p></p><p>第一，将所有 Token 进行重排，不依赖 Padding，避免无效计算。在实测中他们发现，对于一些样例输入计算量能够节省到原来的几分之一甚至几十分之一。第二，在一个 Grouped GEMM kernel 中并行完成所有 Expert 的计算，提高 SM 利用率。下图中的 Expert0 对应有 3 个 Token，Expert1 对应有 2 个 Token，Expert2 对应有 1 个 Token，其实这可以算是三个矩阵乘，并且三个矩阵乘的计算规模不一致，而 Grouped GEMM 就是专门解决这样问题的，它可以用来同时计算 N 个规模不一致的 GEMM。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bddf52d4e2f61b3a794fa2f38babd91.png\" /></p><p></p><p>算子融合不仅可以节省一些访存量，还可以减少 kernel launch 的时间。快手技术团队从 MoE 算子融合和通用算子融合两方面来考虑算子融合的事情。</p><p></p><p>由于 MoE 部分使用了 INT8 量化，所以在两次 Grouped GEMM 前后及中间会有很多算子，如 Permute、Quantize、Dequantize、GELU、Unpermute。快手技术团队最终确定的方案是通用一些 Transformer 常用的算子融合，比如 LayerNorm、QKV GEMM 融合、Bias 和其它一些残差以及 LayerNorm 的融合，将这些连续的 element-wise 算子全都合成一个，整个 MoE 部分的 kernel 数量可以减少到 5 个。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/6783d2edfd1ff1d1581fb3bc7e64b443.png\" /></p><p></p><p>上图就是 MoE 优化后的一个实验结果，对比的对象分别是 DeepSpeed 的实现和 Grouped GEMM 的实现。“这里强调一下，这里并不是说直接调用 DeepSpeed，而是说在我们的自研推理引擎中按照 DeepSpeed 思路实现了一个方案。主要是 DeepSpeed 不支持 INT8，而且在两卡 A10 上是跑不起来的。这里的环境是两卡 A10，均为 INT8 精度。”可以看到，Grouped GEMM 在 Latency 上会取得 7—21 倍的加速比，显存占用可以降低 30%-40%，为大模型的推理上线创造了条件。</p><p></p><h4>3. 优化模型部署</h4><p></p><p>前文提到，部署决定了多模态超大模型能否成功工程应用。快手在部署环节也遇到了多模态数据预处理复杂和 CPU/GPU 负载不均衡的问题。</p><p></p><p>快手技术团队在部署时发现，为了追求最优性能，需要从 Python 训练代码迁移到 C++ 的线上服务代码，在迁移过程中容易出现各种差异，尤其是 CV 方面的预处理，如 Python 的 PIL 库等。为了解决图像预处理时 Python 库和 openCV 存在的 diff 以及加速期预处理，快手技术团队使用 SIMD 和 CUDA 分别实现了一套图像预处理库。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb2fb592ce79f4734ebaf9e49999410b.png\" /></p><p></p><p>以计算引起 diff 的 Resize 操作为例，Resize 主要操作是通过不同的差值算法对图像进行缩放，主要操作是一个重采样的过程，对于目标图像上的一个点选取其源头中的采样区间，通过与不同的差值算法相乘、相加得到，这种乘、加操作非常适合CPU SIMD 和 GPU CUDA 进行加速，CUDA 主要利用其现有模型进行加速，而SIMD 采用了 AVX 并行指令集，将 RGB 通道的数据存入寄存器中，与差值系数进行并行的乘、加操作，实现线上结果与 Python 结果一致。</p><p></p><p>解决预处理 diff 之后，快手技术团队对部署方式进行了优化。</p><p></p><p>对于多模态的数据输入，快手技术团队在预处理阶段进行了并行处理，预处理之后得到的特征数据经过 AutoBatching 尽量凑到最大的 Batch size 数据送入 GPU 进行模型推理，以最大化利用 GPU 的性能，推理结果送入后处理队列等待处理。整个过程中 CPU 和 GPU 都能同时满负荷运行。</p><p></p><p>在线上大规模部署时，为了满足最佳的 GPU 的吞吐，使单次推理的 Batch size 尽量大，将预处理和 Batching 使用的 CPU 机器单独部署，使用更少的节点进行 Batching 操作，以聚合更大的 Batch size，增大流量低峰期的 GPU 数据输入 Batch size，尽量压榨 GPU 性能，配合线上的动态扩缩容机制能够大大提供流量低峰期的资源利用率。</p><p></p><h4>4. 整体收益</h4><p></p><p>在训练层面，快手技术团队基于 DeepSpeed 做了一些深度优化，集成 Megatron、Xformers、Tutel 等高性能库，最终可以实现大模型高效训练。在业务收益方面，相对于快手原本的 10B-T5 模型，100B-MoE 参数量扩大了 10 倍，样本扩大 16 倍，最终只增加了 75% 的训练卡时，在业务指标获得了显著提升。</p><p></p><p>在优化部署模型后，相比于第一个版本上线的 10B—T5 模型，快手 100B—MoE 模型上线之后的模型指标大大提升，在性能上模型变大 10 倍的情况下，在 Batch size 等于 16 的情况下折算到单卡 T4 依旧有不错的加速比，线上实际部署时机器资源在相同的流量情况下节省了将近 60%。</p><p></p><h1>多模态超大模型工程应用，AI 商业化吹响号角？</h1><p></p><p>据悉，不仅仅是技术层面上探索出了大模型从训练到落地的技术实现道路，快手多模态超大模型技术在快手公司内部的推荐、搜索、直播、商业化、电商等多个场景落地，以较低的资源成本取得了显著的业务收益。</p><p></p><p>在 ChatGPT 和 GPT-4 带动下，AIGC 大火。就连 Adobe 都推出了名为“萤火虫”（Firefly）的创意生成式 AI，正式杀入 AIGC 商业化赛道。可以预见，随着 AI 技术的进一步发展，大模型以及多模态模型的商业化应用将进一步加速。我们也期待看到更多多模态大模型工程应用案例的出现。</p>",
    "publish_time": "2023-03-28 09:49:53",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "KMM 技术在移动 App 开发中的探索与实践",
    "url": "https://www.infoq.cn/article/RTmb9YuI0ZfFqRzqZCfT",
    "summary": "<p></p><blockquote>本文整理自百度资深研发工程师袁晗光在<a href=\"https://qcon.infoq.cn/2023/beijing\">QCon 2022 北京站</a>\"的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2023/beijing/presentation/4532\">KMM 技术在移动 App 开发中的探索与实践</a>\"”。</blockquote><p></p><p>&nbsp;</p><p>提升效率永远是软件研发要追求的目标，让代码实现跨平台运行似平就是永恒的主题。当前本该一样的业务逻辑双端需要各实现一遍，最后不仅体验上有着细微的差别，QA 也需要双端各验证一遍；以及由于 UI 代码没有很好的和业务逻辑代码解耦合，导致业务逻辑代码复用困难等，这些影响人效的瓶颈该如何突破?</p><p>&nbsp;</p><p>为什么偏爱 KMM 技术？如何减少为不同平台编写和维护相同业务逻辑代码所花费的时间，同时又能保留 NA 编程的录活性和优势。</p><p>&nbsp;</p><p></p><h1>问题背景</h1><p></p><p></p><p>编码成本高，需降本增效 。</p><p>相同的业务逻辑双端需用不同的语言各自实现一遍 。双端业务逻辑的实现完全一致还很难做到。同样的业务逻辑仅靠口头上或文档上方案对齐导致最终编码实现上有着不小的差别。双端体验上有着不小的差别，数据上产生的不一致性导致不便解释其合理性（交互体验、收益回顾）。双端业务逻辑实现方式不一致导致修改点不能完全对齐、同步 。</p><p>&nbsp;</p><p>后期升级维护、测试成本都较高 。</p><p>UI 代码没有很好的和业务逻辑代码解耦合，造成业务逻辑代码复用困难，不方便做单元测试，组件间的循环依赖增多。业务逻辑变更需要拉上双端的研发都对齐一遍，然后各自编码实现一遍 。同样的业务逻辑双端都需要测试验证一遍 。</p><p>&nbsp;</p><p></p><h1>1. 概述</h1><p></p><p></p><h2>1.1 KMM 基本原理 – 简介</h2><p></p><p></p><p>我对跨平台产生了兴趣，大约在Swift语言刚推出不久，约17年左右，曾在一款创新型的APP上（简单搜索App）进行过一个小实验。那时，我们在做创新类APP方面很活跃，我做了一个类似于云控的模块，云控模块估计在许多APP上都存在。按照以往的做法是iOS 和 Android 双端各出一个RD来开发这个模块，过程中通过设计文档对齐技术方案，但我这次的实验重点在于先用Swift编写了这个云控模块，然后还是同一个人去通过kotlin 来开发这个模块使得其能运行在Android平台上，因为大家都知道这两种语言的语法越来越接近，而不像早期的Objective-C和Java那样，语法格式和其他方面的差异较大。所以在Swift版本开发、测试完成之后，开始逐行翻译每一行Swift代码，用Kotlin代码逐一对应地写出来。最后我发现效果还是不错，在使用Swift编写代码的过程中踩过的坑或遇到的问题，在直接翻译到Kotlin上后这些问题基本上都消失了。只需少量的工作和测试就能在Android上运行，而且问题很少运行得非常好。</p><p>&nbsp;</p><p>因此，我有一个想法就是如果Swfit可以直接跨平台那就太好了。这也是我对语言跨平台最初的兴趣。当然，这与我早年是C++程序员也有关，因为众所周知 C++是可以跨平台的。</p><p>&nbsp;</p><p>如果Swift语言是否能够直接在Android平台上运行就不用再翻译一遍了，两三年前我在偶然的机会下发现了与Kotlin相关的代码和文档，看到了一个名为Kotlin Native的技术。当时官方还没有发布KMM插件，Kotlin Multiplatform Mobile，其中 iOS 端基于 Kotlin Native（简称KN）技术，Android端基于 Kotlin JVM，利用 KMM 技术可以使用 Kotlin 语言技术栈在iOS 和Android应用程序之间共享通用代码，只在必要时编写特定于平台的代码，用来构建统一的业务逻辑代码。</p><p>&nbsp;</p><p></p><h2>1.2 同类跨平台框架对比 – KMM的优势</h2><p></p><p></p><p>无需内置多套引擎(runtime)，包体积增量更少 。对于 Android 开发者无需多学习一套编程语言和编程思想，门槛更低 。基于双端标准组件输出，审核被拒风险较小（iOS）。更强的互操作性， 支持与本地编程语言的双向互操作，可以直接使用现有库，避免了众多基础组件的重复建设。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/675507bd5c6f3fa56f58443af312ad9d.png\" /></p><p></p><p></p><h1>2. KMM 开发环境介绍</h1><p></p><p></p><h2>2.1 Android Studio &amp; Xcode 环境配置</h2><p></p><p></p><p>安装工具： Android Studio（建议官方最新版）Kotlin 插件Kotlin Multiplatform Mobile 插件Xcode（12.5 版本及以上）JDK（8 及以上）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/03c3639115d3e6b1697595dab41d7de7.png\" /></p><p></p><p></p><h3>2.1.2 创建跨平台App</h3><p></p><p></p><p>步骤：</p><p>① 在Android Studio中，选择文件|新建|新建项目。</p><p>② 在项目模板列表中选择Kotlin Multiplatform App/Library。</p><p>③ 为应用程序指定名称。</p><p>④ 保留应用程序和共享文件夹的默认名称，并在iOS 框架分发选项列表中选择常规框架。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/58/587c872b1f1a1f726b9d2f6a09ceacb8.png\" /></p><p></p><p></p><h3>2.1.2 项目结构</h3><p></p><p></p><p>三个部分组成：</p><p>① 共享模块：包含Android和iOS应用程序的核心应用程序逻辑：类、函数等。构建到AAR&amp; Framework中 ，使用Gradle作为构建系统（commonMain、androidMain、iosMain ）。</p><p>② iOS应用程序中的Xcode项目。</p><p>③ Android应用程序中的Kotlin模块。</p><p>&nbsp;</p><p>如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/66441c9336f17a0de42868868305f771.png\" /></p><p></p><p>我们可以看到，这个项目主要分为三个模块。首先，我们先看一下名为“shared”的模块，它位于最下面。其次是iOSApp模块，再上面是AndroidApp模块。为什么会有这三个模块呢？因为在我们的核心目标即双端共享代码。什么是共享代码呢？让同一份代码能在Android &amp; iOS 上运行，那怎么实现这个目标呢？简单来说把全部代码分为两个部分，其中一部分就是与平台无关的代码。啥是平台无关的代码呢 ？比如我们要编写一个检验电子邮件的算法，我们认为这个算法的代码是平台无关的，因为输入就是一个字符串，里面的实现就是根据指定的规则来判断这个输入字符串的合法性，期间不涉及任何平台相关特性的访问，比如系统 API 的访问。但是光有与平台无关的代码是不够的，一旦涉及到与平台相关的访问，例如获取设备版本号或硬件信息等，我们就需要独立于平台去完成它们，这些独立于平台的代码在哪实现？这就有了AndroidApp和iOSApp这两个模块。在“shared”模块中，我们包含了核心应用程序逻辑，例如类、函数，并使用Gradle作为构建系统。</p><p>&nbsp;</p><p></p><h2>2.2 特定于平台的API 和实现</h2><p></p><p></p><p>expect/actual 机制 ：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc9c091e964efa2f34879dad4d8aeb04.png\" /></p><p></p><p>KMM 里 expect/actual 机制是非常重要的，因为它提供了一种语法技术来解决平台相关的问题。举例来说，当我们需要在业务逻辑中使用设备的 model 号时，我们需要编写特定平台的代码才能实现。这时，expect 就相当于声明一个协议，它定义了我们期望得到的接口或数据，之后各个平台都需要独立地实现这个协议来满足业务需要。</p><p>&nbsp;</p><p>可能会有人认为这样写了两遍，每个平台都写一遍不如让各个平台自己去写。但是，这种方式只需要建设一次，就像一些基础库一样只需要做一次就能在后面被大家共用。这种共用甚至不限于公司界，整个业界都可以共用一组基础库。</p><p>&nbsp;</p><p>基本流程如下： 在commonMain目录下建立一个expect类或Top-Level方法 , 类似创建一个协议声明。分别在androidMain和iosMain目录中，创建与expect声明（类名、方法名、参数类型及名称）完全一致的实现，否则编译器会报错。</p><p>&nbsp;</p><p></p><h3>2.2.1 如何引用已有组件 - Android</h3><p></p><p></p><p>在使用expect机制后我们解决了平台相关的代码问题。但要充分发挥KMM技术的优势，还有一个重要的点，那就是能否复用已有的组件。因为，如果需要再开发一个新的网络库、数据库或多线程相关的组件成本是很高的，新开发的组件稳定性也是有待考验的 。</p><p>&nbsp;</p><p>KMM技术的一个很重要优势之一就是可以轻松地引用原生平台上已有的组件。例如，我们可以在androidMain后的闭包中按照Gradle规范添加依赖项，然后在androidMain目录下的Kotlin代码中调用依赖库中的类或方法。在这里，我们举了一个自定义组件的例子，引用安卓平台上已有的组件和android 原生开发没啥本质上的区别，关键是如何引用 iOS 上已有的组件呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/533fed47db54d8c8417cfe7f18b0c791.png\" /></p><p></p><p></p><h3>2.2.2 如何引用已有组件 - iOS</h3><p></p><p>&nbsp;</p><p>同样，举个例子如果我想在kotlin代码中判断当前系统是 iPad 还是 iPhone，类似这样的操作该如何实现呢？</p><p>&nbsp;</p><p>为了解决这个问题，我们需要使用官方提供的工具 cinterop，它可以扫描 Apple Framework 并根据 .h 文件获取可调用的类、方法、变量、常量以及它们对应的类型，最终生成 klib 文件。如果需要使用 Swift 的方法，需要添加 @objc 前缀。</p><p>&nbsp;</p><p>要使用 cinterop，需要了解相关的头文件或声明。首先，cinterop 会下载构建依赖的苹果原生 framework，例如 CFNetwork、CoreData、Foundation 等，然后编写 def 文件并配置 cinterop 的编译和链接。最后，使用脚本生成可用于 KMM 的 API。cinterop 工具将生成中间文件 Klib，它包含了扫描到的所有的 framework。经过扫描之后，每个 framework 都会对应一个 Kotlin 可以识别的 Klib 文件，这样 Kotlin 就可以识别这些 iOS 原生组件了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/7362bd53e815c7e850e8fb11cd100600.png\" /></p><p></p><p></p><h3>2.2.3 如何引用已有组件 - EasyBox KMM Gradle 插件</h3><p></p><p></p><p>由于『百度 APP』工程使用 EasyBox 管理依赖及 iOS 工程结构，而 KMM 官方只能够支持 CocoaPods 或上面介绍的直接引用 Framework 的形式实现 Kotlin 与 Objective-C 的交互，如需兼容 EasyBox 组件，在不使用自定义插件的情况下只能使用常规模式（非 CocoaPods）引入依赖库。</p><p>&nbsp;</p><p>但官方提供的Framework引用配置比较复杂，根据官方文档的说明，如果不使用 CocoaPods 引入依赖库的模式， build.gradle.kts 文件内的配置项相当复杂，每次新增依赖时需要做大量的 cinterop 配置，对于新入门 KMM 的开发人员，非常不友好，影响开发效率，甚至破坏工程配置环境 。为简化 KMM 引用 EasyBox 组件的配置流程，做好二进制版本控制，并避免造成代码库文件冗余，我们参考 KMM 官方的 CocoaPods 插件实现，开发了 EasyBox KMM 插件，以便在 KMM 工程中，充分利用现有的各类组件，避免重复建设的问题 ，大家可以结合自己的组件管理工具或源码管理工具来使用。</p><p>&nbsp;</p><p>EasyBox KMM Gradle 插件实现了的主要功能：</p><p>根据配置，自动拉取 Box 仓库（二进制源）中的 Framework 。自动生成并配置 cinterop 所需要的 def 文件，让 KMM 模块能够识别并扫描 Framework 中对外公开的类、方法、常量等元素 。生成带有 cinterop 产物的 klib，并支持发布到 Maven 仓库中，在不具备 cinterop 环境的设备上无需再次进行 Framework 下载和扫描即可复用 cinterop 后的能力 。</p><p>&nbsp;</p><p>EasyBox KMM Gradle 插件的使用：</p><p>有EasyBox KMM Gradle插件，使用过程就相对简单了。首先声明 easybox-kmm 插件，然后在easybox闭包中添加所有依赖的组件，如图所示2个步骤。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23345b273c42f766b970b6170a850b7f.png\" /></p><p></p><p></p><h2>2.3 在已有的工程中集成 KMM</h2><p></p><p></p><p>接下来，我们来看下如何将KMM的产物集成到现有的工程中。由于 KMM 模块在 iOS 平台是以 Framework 的形式产出的 ，所以 在xcode 里面我们可以像使用其他Framework一样引入它，在使用 Framework 中，它的所有协议都是Objective-C格式的，我们可以在我们的工程中像使用其他的API一样使用它。同样 KMM 模块在 Android上是以 AAR 的形式产出的，如下图输出的api 之间的对应关系，名字一致，参数也基本上一致。因此，在已有工程中集成KMM 的产物的便利性也是KMM的一个优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6017ebfcbd06d2adcea388a68952d641.png\" /></p><p></p><p>&nbsp;</p><p>重点技术</p><p>&nbsp;</p><p>基础流程已经介绍完毕，现在让我们来看看重点技术。其中最主要的技术还是多线程处理，它涉及到协程、状态共享、不可变性（frozen）和原子类（Atomic）等。接下来，第二个重点是基础库的建设，主要包括网络库和数据库。</p><p>&nbsp;</p><p></p><h2>3.1 多线程 &amp; 内存管理</h2><p></p><p></p><p>多线程并发开App开发中一直都是个重点和难点，也是我们在实际编码工作中容易出错的地方，这点在KMM 中的开发中也是。所以技术选型是关键，官方提供了多种方式支持多线程，我们也对比了几种方式的优缺点。首先是协程，虽然协程的概念上类似于线程语法简单且概念新颖，但我们当时认为它对Kotlin Native的支持不够成熟和完善，虽然在安卓平台上它仍然使用JVM，但我们最终还是没有选择协程方式去继续探索。</p><p>&nbsp;</p><p>接下下来看第三方库，CoroutineWorker对Kotlin协程进行了封装，但迭代较少，不是很稳定。Reaktive采用RxJava的实现思想，Native（iOS和macOS）底层采用Kotlin Native实现，具有多种功能，但框架相对较重。</p><p>&nbsp;</p><p>最终我们选择的是利用expect/actual加Block方法，Android端可以利用线程池，而iOS端可以使用GCD自行实现。优势是使用了各自平台已有比较成熟的多线程方案，更稳定，运行效率更高 。</p><p>我们认为这是风险最低的方案，因为新技术前期稳定性是首要考虑的因素。因为在原生开发中，iOS的GCD多线程方案已经运行了很多多年，我们对这些多线程的属性和用法也都非常熟悉，这也是我们选择它的一个关键原因，当然不足之处是需要编写一定量平台差异化的代码，但这些都是一次性的基建。</p><p>&nbsp;</p><p></p><h3>3.1.1 模型</h3><p></p><p></p><p>这是我们最终选择的一个与多线程相关的模型，即Common代码。通常情况下，如果我们要执行一个与多线程相关的异步任务，我们将其分发到两种任务队列里面去，串行队列和并行队列。先聊下串行异步队列，将一系列小的任务放入后台串行队列中执行是非常常见的，这对于平台类的应用开发这通常就足够了，因为这些任务比较小单独开一个线程去执行又显得较重，但直接扔进主线程多了又会卡顿UI，有时各任务间又有一定的实效性需要保障。另一方面，如果我们有耗时较高的任务，我们需要将其放入独立的异步线程中执行。我们通常把需要异步执行的任务封装为一个个代码块，然后把它扔到合适的异步任务队列里面去执行。因此，可以看到我们的通用代码分为两条线路，iOS上通过iOSMain的GCD实现，通过dispatch实现。而在Android上，我们则通过BackgroudTaskUtils来实现。在实际应用中，我们可以直接用自己APP中常用多线程组件来替换它，但对外接口上应该还是大同小异的，common 层声明了对外输出的接口层，抹平了双端接口的差异化，再向下就是各个平台独立于平台的实现了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/26/26c26d4228697a906f0957c4dc708d0b.png\" /></p><p></p><p></p><h3>3.1.2 多线程间共享状态的规则</h3><p></p><p>&nbsp;</p><p>我们做移动APP开发的同学可能对线程和对象之间的关系可能还没太多的概念，因此在对象和线程关联方面可能没有形成一个习惯。KN目前主要还是采用了传统的legacy内存管理方式，虽然从1.8开始也开发了一种新的叫做New Memory的内存管理方式，但在本文中我们将重点介绍lex的内存管理方式，先介绍两个规则。</p><p>&nbsp;</p><p>规则1：可变状态仅单个线程可见 。</p><p>状态，就是我们平时所说的变量或者说是属性，可以简单的理解为每个新生成的变量都是和当前线程关联的 ，离开这个线程到别的线程去这个变量就是不可见的，直接访问的话会产生crash 异常。</p><p>&nbsp;</p><p>规则2：不可变状态才多线程可见 。</p><p>&nbsp;</p><p>如果我们的应用程序永远只有一个线程，那所有的状态都是可变的就够了，但这是不现实的，所以如果希望一个状态能被夸线程访问或修改那它应该是不变的 , 这样多个线程才可以安全的访问它.</p><p>&nbsp;</p><p></p><h3>3.1.3 什么是不变和冻结状态 ？</h3><p></p><p>&nbsp;</p><p>KN 定义一个新的运行期状态, 称为冻结(Frozen)。一个对象的任何实例都可以冻结. KN 运行期对所有类添加了一个扩展函数&nbsp;Freeze() . 调用它将会冻结一个对象, 并递归的冻结这个对象引用的所有对象. 如果一个对象被冻结, 那么你不能改变它的状态的任何部分. 尝试这样做会导致一个运行期crash。</p><p>&nbsp;</p><p>被 freeze 的实体，其所有属性都不可被修改 。被const修饰的常量默认都是被冻结的状态。在一个对象上执行 freeze()会变成 frozen 状态，该对象所引用的其他对象都会变成frozen状态（含容器）。freeze() 操作是不可逆的，如果需要获得非 frozen 的对象，只能将原先被froze 的对象进行深拷贝，成为一个新的对象 。</p><p>&nbsp;</p><p>当一个对象被freeze后，其子对象也会被冻结。换句话说，一旦一个对象被冻结，它引用的所有子对象、数组列表等也将被冻结。当运行期需要检查一个对象是否可以被其他线程访问, 它只需要检查对象是否被冻结。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/16974f042a55b7498a8b6194d3132a20.png\" /></p><p></p><p></p><h3>3.1.3 Object单例</h3><p></p><p>&nbsp;</p><p>Object单例： 默认在创建后就会被 freeze，对所有线程可见，但如果修改其内部引用，将会导致报错。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c30227d61d0f113a46eade471650216.png\" /></p><p></p><p></p><h3>3.1.4 Top-level 属性</h3><p></p><p>Top level 属性：默认仅对主线程可见，且是可变的，从其他线程访问将引发异常。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/da/dafa6e5136e4af8d3ffecde1ae7e3a18.png\" /></p><p></p><p>注：Top Level 的 val 变量被认为是 const ,默认也是 frozen 状态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/506be29c17ea6eff050495d9fa6ec4da.png\" /></p><p></p><p>&nbsp;</p><p>注意事项：Top-level 属性必须在主线程初始化！</p><p>&nbsp;</p><p></p><h3>3.1.5 两种注解来标注 Top level 属性</h3><p></p><p>&nbsp;</p><p>① @SharedImmutable：该注解可以让Top Level属性全局范围可见，但是被 freeze 之后无法修改。（此注解只能用于val属性）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef2a86e0a9965cbabff665b2ad3daafc.png\" /></p><p></p><p>&nbsp;</p><p>② @ThreadLocal：给每个线程提供独立的可变副本。（此注解可用于 val、var 属性）。</p><p>如果需要在不同的线程中对其进行修改，用 @ThreadLocal 对其进行注解，这将允许它是可变的，并为每个线程提供其状态的副本，如何让多线程共享修改同一个对象的属性，将在后面提到。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee30eeb30341b70a49d9353223696309.png\" /></p><p></p><p></p><h3>3.1.6 如何实现可变性？</h3><p></p><p></p><p>前面我们讨论清楚了如何实现状态的多线程可见，但实际项目中仅可见是不够的，@ThreadLocal 只用来修饰top level属性，且只是为每个线程提供一份独立的副本，那如何让多线程能修改共享的属性呢？即可变性。</p><p>&nbsp;</p><p>① Atomics（原子类）</p><p>② Thread-isolated&nbsp;states（线程隔离状态）</p><p>③ Low-level capabilities (略)</p><p>&nbsp;</p><p></p><h3>3.1.7 Atomic （Stately库）</h3><p></p><p></p><p>为了能够让多线程同时读取、修改一个冻结的状态中的值， KN 提供了一组Atomic类库。</p><p>1、对简单数值使用AtomicInt/AtomicLong：持有共享的 Int/Long允许多个线程读写。示例中SomeState是一个全局的object,在KN中它默认是冻结的，左边的 count++ 会产生异常，而右边的 increment() 是可以在任何线程中正常工作的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2ed05f82d4acfcf8d0ce12a7bf0b9362.png\" /></p><p></p><p>&nbsp;</p><p>2、AtomicReference ： 持有一个对象实例允许多个线程读写，持有的对象必须是frozen状态的。</p><p>&nbsp;</p><p>示例：</p><p><code lang=\"null\">data class SomeData(val i: Int)</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f77f8a9b3e2a36f4de515835bf6c36a.png\" /></p><p></p><p></p><h3>3.1.8 实际应用便利性技巧</h3><p></p><p></p><p>采用Atomic封装的状态，取值、赋值的过程较繁琐 ，可以通过对外暴露一个普通的属性，然后修改其get/set 方法来实现Atomic封装 。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2409940915749db074f178bebbf1c8c.png\" /></p><p></p><p></p><h3>3.1.9 AtomicReference 的内存泄露</h3><p></p><p>&nbsp;</p><p>循环强引用 &amp; 内存泄露是我们经常遇到的一个问题，在KN中也不例外，比如当我们定义了一个 Kotlin 对象时，这个对象需要在 iOS 的 ViewController 中使用，对它有了第一个持有引用。由于在 ViewController 中会持有这个kotlin 对象，同时为了使用的便利性AtomicReference对象也会引用这个 ViewController对象而产生循环引用，这时候我们应该在&nbsp;AtomicReference 中使用一个可为 null 的类型, 并在使用完毕时, 明确的设置为 null. 这样就可以这个循环引用导致的内存泄露了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9d8789db35b93b1d59d1185225a03b2.png\" /></p><p></p><p>接下来是性能问题 。和一个标准的可变状态相比，访问并修改AtomicReference内引用的值是比较耗时 的，尤其是在频繁对引用的对象进行修改操作的情况下会对性能造成更大的影响，主要原因是跨线程导致，出现这种情况的时候不再建议使用该类实现，如果需要实现 MutableList 或 HashMap 时，该如何操作？我们引入线程隔离状态 。</p><p></p><h3>3.1.10 线程隔离状态</h3><p></p><p></p><p>隔离状态：将可变状态隔离到单个线程，并允许其他线程与该状态通信。比如一个列表对象，需要经常进行增删改查操作，用AtomicReference<!--?-->封装会导较大的致性能问题。这时，我们可以通过 Iso 提供的相关功能来解决这个问题。</p><p>&nbsp;</p><p>IsoArrayDeque 的用法 ： 创建一个对线程具有独占访问权限的工作队列，并创建一个仅存在于该线程中的可变状态， 其他线程通过调度工作队列上的工作与可变线程进行通信。</p><p>&nbsp;</p><p>AtomicReference&gt; 和 IsoArrayDeque 该用谁 ？</p><p>① 对于单个的值，AtomicReference 是更好的选择。</p><p>② 对于值的集合，使用线程隔离状态是更好的选择， 主要的性能损失实际上是跨线程。</p><p>&nbsp;</p><p></p><h2>3.2 基础库建设 ， 网络、数据库 …</h2><p></p><p></p><p>最后是基础库的建设，要发挥新技术的优势建设强大的基础库是关键。为此，我们充分利用『百度 APP』沉淀的各类组件和能力， 避免了重复建设。 双端接口差异化抹平，使用起来也更方便，更有利于以保证双端逻辑代码编写的一致性 。</p><p>&nbsp;</p><p>例如 SQLDelight可以很方便的实现跨平台操作数据库的增、删、改、查，但是引入新库往往又会增加应用包体积，这些都是我们在做技术选型的时候要优先考虑的点，我们也可以采用 expect/actual 机制来定义接口协议，并用原生的实现来解决这个问题。我们也采用各自平台原生的实现（GCD）来解决多线程问题；对于我们必用的KV存储，底层使用了SwanKV和MMKV；Json解析库使用了Serialization来实现，我们也正在优化Json解析库的包体积。更底层的StringUtls和Log等都需要一起建设，尽管这些建设都是一次性的，但要做到业界共享目前还是有困难的，因为各公司的底层基础库还是有较大的差异，需要桥接到上面就需要各个团队开发自己的底层实现。所以，在实际更复杂的业务场景中，需要一套丰富、稳定、可靠的「跨端」基础能力也有不少的工作量，但随之逐步的完善KMM的核心效率优势就逐渐发挥出来了。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89a5c750ec7e0f5f8b58fdad2d2fd8b9.png\" /></p><p></p><p>&nbsp;</p><p>落地情况</p><p>关于技术的应用落地，我们其实从 2021 年开始探索。最初我们落地了一个创新类的 APP，但目前该 APP 已经停止服务。接着，我们又成功地在百度 APP 中应用了该技术，大字版也于去年的 5 月份推出该技术。目前，我们已经在百度 APP 中应用了几个重要的业务场景，包括首页、一级 Tab 等。由于一些业务逻辑的共用，这些场景在运营中显得尤为重要。</p><p>&nbsp;</p><p>就落地情况而言，我们发现在我们正在开发的这个新业务中，UI 交互占比较大。目前的数据显示，其占比大约为 33% 左右，因此我们认为这个领域还有很大的发展空间。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/13/13eb67ab6a7ff9435750b4446f0d0d4b.png\" /></p><p></p><p>&nbsp;</p><p></p><h1>嘉宾介绍</h1><p></p><p>&nbsp;</p><p>袁晗光，百度资深研发工程师。软件开发 16 余年，2013 年入职百度，目前负责百度 App 和其他多个创新类 App 客户端功能迭代与架构方向，长期从事研发质量效率的平衡探索。</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ==&amp;mid=2650847427&amp;idx=1&amp;sn=83dc2c1868e29dd24824f1aa90fbeb40&amp;chksm=80b76c5db7c0e54b4135f0d3e1d01271579a9272a64f4df8e1ab1f79cd5664f5377c3b48fad2&amp;scene=27#wechat_redirect\">KMM 、 Compose 、 Flutter “神仙打架”？了解下现状</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ==&amp;mid=2650847146&amp;idx=1&amp;sn=4454989511dc4f1cfc1e1f1c5ab41093&amp;chksm=80b76334b7c0ea228793be6f3e9f7b6435f5a2a2c7feb184e2d24381f582ce7884ab696056c1&amp;scene=27#wechat_redirect\">KMM跨平台开发入门，看这一篇就够了~</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651116150&amp;idx=4&amp;sn=7cd0fe571494ceae161f4ccc008f9f59&amp;chksm=bdb92a258acea333aa9c4f807067d74ea8f95e3446ccbb47b53d3aee552c49ff6a43bf379833&amp;scene=27#wechat_redirect\">跨平台框架新进展：Weex2.0、Waft、KMM ｜GMTC</a>\"</p><p><a href=\"https://www.infoq.cn/article/tTiZu8E4MNXzJvd18YI2\">热门方向 Top4：大前端监控、移动端性能与效率优化、团队可持续发展、低代码</a>\"</p>",
    "publish_time": "2023-03-28 10:09:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "我被 React 劫持了，很痛苦又离不开",
    "url": "https://www.infoq.cn/article/CZKMjHaxbf1Z7xcSzisX",
    "summary": "<p></p><h2>前言</h2><p></p><p></p><p>如果几年前我写的这篇文章，可能会被视为亵渎。但如今越来越多的人表达了对 React 的不满，我终于可以表达我的观点了。坦率地说，我不太喜欢 React，大多数人也是这样的想法，即使你现在还没有任何不满情绪。</p><p></p><p>我已经使用 React 很长时间了，但实际上我更喜欢其他替代方案。尽管我是一名“React 开发人员”，过去用它，现在用它，可能将来还会用它，我似乎无法完全摆脱它。也许你认为我会在一段时间后不再关注，但一旦看到其他选项，我就会感到困惑，不知道为什么自己仍然继续使用 React。</p><p></p><p>当 React 刚出现时，一切都显得如此优雅。组件代码注重 行为局部性。DOM 是内联的，就在事件处理程序旁边。代码看起来是模块化的，很整洁。</p><p></p><p>类组件</p><p></p><p><code lang=\"null\">class MyComponent extends React.Component {  constructor() {    super();    this.setState({ num: 0 });  }  handleClick = () =&gt;     this.setState({ num: this.state.num + 1});  render() {    return (      <button>        Clicked {this.state.num} times!      </button>    )  }}\n</code></p><p></p><p>随着教程的发布、著作的出版以及大师的诞生，React 以前所未有的速度建立起了自己的帝国。它提供了无与伦比的开发体验，但更好的是，容易理解。最初尝试 React 的是活跃分子、思想影响者以及我们中最为书呆子的人。这个库很快就流行起来了，这也不难理解，因为它的语法非常简单。</p><p></p><p>于是我们都加入了进来。然后，我们有了钩子（Hooks）！万岁！说实话，它们一开始让人感到有些陌生。但那种清新的感觉！React 变得更加优雅了！它使输入更快、迭代更快、阅读更容易。React 是正确的选择。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/bd/bd0e48a5c426c4eb92dd0172ad7b8eef.png\" /></p><p></p><p>函数组件</p><p></p><p><code lang=\"null\">const MyComponent = () =&gt; {  const [num, setNum] = useState(0);  const handleClick = () =&gt; setNum(num + 1);  return (    <button>      Clicked {num} times!    </button>  )}\n</code></p><p></p><p>几年过去了，我们开始注意到，有许多其他可选项出现了。每个月都会有新的名称出现。当时我们嗤之以鼻，认为这些新潮项目永远不会流行起来！我们认为 React 已经经过战斗考验，完美地满足了我们的需求，其他库没有留下任何空间。</p><p></p><p>然后人们开始说话过于积极，过于一致。感觉每个人都对他们的新东西感到满意，对旧东西感到不满意。但那些只是过眼云烟的潮流！从 2015 年发布到 2019 年的迭代版本，没有什么能像 React 那样经得起时间的考验，而且 React 仍在不断发展。</p><p></p><p>你知道的，React 感觉不再那么稳定了。我是说，或许它是最好的选择，但谁知道呢？我以为我们已经完成了迭代，但还是不断地被一些新变化所吸引。也许还有更好的方法，只是我还没看到！看看也无妨。</p><p></p><p></p><h2>React 与 Hooks</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0a/0ad44de855d53a7b8eb101bb75ed9f93.png\" /></p><p></p><p>尽管 Hooks 并不是 React 本身的必要组成部分，你仍然可以使用类组件，但 Hooks 已经成为了 React 生态系统中不可或缺的一部分。本文所提到的 React 模型指的就是“React Hooks”。</p><p></p><p>最近，我注意到人们对 React 生态系统向 Hooks 的平稳转换感到惊讶，因为现在每个人都一致认为 Hooks 带来了许多好处。然而，我还记得不久前在这个问题上曾有过激烈的争论。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/13/1399353fff97039cb753971afdc257e9.png\" /></p><p></p><p>我不是非要和那些反对 React Hooks 的人站在一起，但我确实认为他们的担忧是有一定道理的。React Hooks 存在于一个由类组件控制的环境中，为了实现这种转换，React 必须与类代码完全兼容，而事实也确实如此。这种兼容性，再加上 Hooks 所带来的可组合性和可读性的提升，使得整个行业采用 Hooks 的速度超出了预期，这也是 Hooks 带来的两项重要改进。</p><p></p><p>我坚信，“可组合性优于继承”，虽然不是所有人都同意这种观点，但我认为通过函数共享行为是对类继承的巨大改进。至于可读性，虽然代码的长度可能与易读性没有直接关系（例如 Regex、代码高尔夫），但 React Hooks 提高了代码的“行为局部性”。</p><p></p><p>这意味着，Hooks 可以减少查看那些比较小的组件的次数。事件监听器、状态转换和渲染输出等细节让人应接不暇，而 React Hooks 对此进行了改进。我发现，函数组件写起来更快，读起来也更容易。</p><p></p><p></p><h2>可读性与复杂性</h2><p></p><p></p><p>但乍看起来，可读性本身与复杂性并没有直接的联系（至少直觉上是这样）。Hooks 通过本地化行为降低了复杂性，但必要的抽象又增加了复杂性。</p><p></p><p>我经常想起 Amos 那句断章取义的话。</p><p></p><p></p><blockquote>或者更确切地说，那个说法半真半假，它掩盖了这样一个事实：当你把某件事情变得简单时，其实是把复杂性转移到了别处。Amos（简单就是个谎言）</blockquote><p></p><p></p><p>当我们抽象复杂的系统时，不是在消除复杂性，而是在转移它。在我们的例子中，复杂系统不是 前端开发，而是 React。</p><p></p><p>Hooks 改变了我们的心理模型，它让我们考虑状态转换和同步，而不是生命周期。或者，至少它的目的是这样的。</p><p></p><p><code lang=\"null\">componentDidMount → useEffect(func, [])componentWillUnmount → useEffect(() =&gt; func, [])componentDidUpdate → useEffect(func, [props])\n</code></p><p></p><p>这一举措有一些性能上的损失——这个问题可以通过 Hooks useMemo和useCallback得到缓解。我并不是说在 Hooks 出现之前，React 中不存在记忆化（memoization）。它是存在的（React.memo()）。我是说，由于本地化行为的改进，我们现在必须记住状态初始化和状态转换。</p><p></p><p>社区中经常会有关于 React 记忆化的讨论，而且与其他框架相比，这类讨论更多。在所有框架中，值缓存都很重要，但是 Hooks 将很多决策留给了组件作者，而不是核心库。</p><p></p><p>我们稍后会详细介绍。但在此之前，我想花点时间讨论一下 心理模型。</p><p></p><p>不管是 React 的文档，还是 YouTube 上的视频，都经常提到这个模型，这也是实际上正在发生的事。或者说，至少存在一种更符合实际行为的心理模型，我认为这一点很重要。</p><p></p><p></p><h2>更好的心理模型</h2><p></p><p></p><p>在关于 React 的讨论中，经常提及“VDOM”这个术语。然而，Dan Abramov 似乎不喜欢它。我赞同他的看法，因为 VDOM 并不是 React 的决策因素，而是其结果。这一点在讨论直接差异时也很容易理解。</p><p></p><p>因此，我们应该将重点放在如何保持 React 组件的“纯净性”上，而不是关注 VDOM。一想到组件有状态，这个术语似乎立马就不合时宜了。因为状态似乎与纯函数的概念不符——对于给定的一组输入，无论调用多少次，无论以何种方式调用，输出都应该相同。</p><p></p><p>纯函数</p><p></p><p><code lang=\"null\">// 纯函数const getPlusOne = (num) =&gt; num + 1;// 非纯函数const getDateNow = () =&gt; Date.now();\n</code></p><p></p><p>关键在于理解 React 中的‍‍状态并不存储在组件中。</p><p></p><p>状态是另一种输入。</p><p></p><p>在 React 中，调用useState是另一种接收输入的方式。状态存在于 React VDOM/ 状态树中。组件有严格的调用顺序，useState将从提供的栈中弹出输入。</p><p></p><p>state &amp; props = inputs</p><p></p><p><code lang=\"null\">const Component = ({ color }) =&gt; {  const [num1] = useState(0); // receive next state argument  const [num2] = useState(0); // receive next state argument  const [num3] = useState(0); // receive next state argument  return </code></p><div><code lang=\"null\">{num1} + {num2}</code></div><code lang=\"null\">}\n</code><p></p><p></p><p>state 和props 都是一种输入。调用setState是向 React 内部发信号，而不是直接更改。</p><p></p><p>这些信号将依次更新其组件 状态栈 并重新运行组件。给这个组件提供新的输入，它将产生一个特定的输出。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ff/ffe5ce14f8701f97c4a482e9b983346f.png\" /></p><p></p><p>React 组件对 React 来说也可能是个黑盒，因为其内部行为是不可见的。相反，我们可以将组件本身视为反应式对象，而不是单个状态块。这也是为什么人们认为 React 的反应模型不是“细粒度”的原因。</p><p></p><p>因此，React 需要一种方法来避免在每次更新时重写整个 DOM。因此，它会在新的更新上运行一个差异对比过程，以判断哪个 DOM 节点需要更新。也许没有不同之处。也许全不相同。不检查就没法知道。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/29/29ceb0755a26d9d4914206992f0123b9.png\" /></p><p></p><p>这是 React 渲染器和调和器之间的关系。这是“纯组件”行为。状态更新和 DOM 更新之间缺少直接的联系。</p><p></p><p>在 React 中，组件是真实的，而 DOM 不是。也许这就是为什么 React 对于 非 Web 渲染器 来说是一个很好的选择。我们可以使用 React 来绘制 UI 和更新，但把更新应用到 UI 的过程替换掉。</p><p></p><p>作为一名 Web 开发人员，这算不上是一个多大的好处。</p><p></p><p></p><h2>发现陷阱</h2><p></p><p></p><p>作为一名 React 新手，你最先遇到的障碍会是类似这样的东西。</p><p></p><p>死循环</p><p></p><p><code lang=\"null\">function MyComponent() {  const [num, setNumber] = useState(42);  // 死循环  setNumber(n =&gt; n + 1);  return </code></p><div><code lang=\"null\">{num}</code></div><code lang=\"null\">}\n</code><p></p><p></p><p>试图在组件的顶层进行状态更新将导致无限循环。状态更新会重新运行组件。这并不意味着 DOM 更新，但确实意味着另一个状态更新，状态更新将再次触发重新运行，重新运行将再次触发状态更新，再次触发重新运行，就这样不断进行下去。</p><p></p><p>可能你很快就会发现这个 Bug。像这样的无限循环并不难发现。</p><p></p><p>当你开始使用 React Context 并在父组件中发出更新信号时，情况会变得更加复杂，因为会出现级联渲染。这意味着一些组件可能需要重新挂载，导致状态更新被延迟数秒钟。这种情况很常见，值得单独写一篇文章来深入讨论如何修复 React 中的这个问题。不过在本文中，我并不打算探讨这个问题。</p><p></p><p></p><p></p><p>我们将继续讨论作为反应式对象而不是状态存在的组件。这种模式有一些后果。</p><p></p><p>模拟表单组件</p><p></p><p><code lang=\"null\">const MyForm = () =&gt; {  const [text1, setText1] = useState('');  const [text2, setText2] = useState('');  const [text3, setText3] = useState('');  return </code></p><form><code lang=\"null\">    <input type=\"text\" value=\"{text1}\" /> setText1(e.currentTarget.value)} /&gt;    <input type=\"text\" value=\"{text2}\" /> setText2(e.currentTarget.value)} /&gt;    <input type=\"text\" value=\"{text3}\" /> setText3(e.currentTarget.value)} /&gt;  ;}\n</code></form><p></p><p></p><p>为了让你感到轻松一些，我对这个组件进行了大幅简化。然而，任何使用过 React 表单的人都知道，它们通常会更加复杂。</p><p></p><p>我曾经看到一些表单组件达到了 300 多行的代码量。这些组件涉及到状态转换、验证和错误视图等多个方面。但是，很多特性都是表单本身固有的，而不是 React 独有的。只是，React 往往会使事情变得更加复杂。</p><p></p><p>需要记住的是，组件是反应式的，而不是状态本身。当使用 受控输入 时，每次按键输入都会导致“重新渲染”，这意味着不管是否触及状态，都可能运行状态计算代码。</p><p></p><p></p><blockquote>VDOM 可以解决所有问题！</blockquote><p></p><p></p><p>这似乎是一种流行的 VDOM 误解。VDOM 的作用是避免不必要的 DOM 更新，而不是避免状态计算。</p><p></p><p>组件是一个函数，每次需要检查更新时，它‍都会重新运行。虽然可能没有触及 DOM，但代码仍然会被执行，而实际上它并不需要被执行。</p><p></p><p>设想有下面这样一个组件。</p><p></p><p>自定义输入封装器</p><p></p><p><code lang=\"null\">const MyInput = ({ label, value, onInput, isError, errorText }) =&gt; {  const labelText = label ? toTitleCase(label) : 'Text Input';  return &lt;&gt;    <label>      <span>{labelText}</span>      <input value=\"{value}\" />    </label>    {isError &amp;&amp; </code></p><div><code lang=\"null\">{errorText}</code></div><code lang=\"null\">}  &lt;&gt;;}\n</code><p></p><p></p><p>我认为这个例子更加贴切。对于提供的标签输入，我们决定将它们转换为“首字母大写（Title Case）”。</p><p></p><p>目前还好。我决定不对其进行记忆化，因为这个计算看起来很简单。但如果情况发生变化呢？</p><p></p><p>如果 toTitleCase 变得越来越复杂呢？也许，随着时间的推移，我们会不断添加新功能，创建出终极的 Title Caser™️！</p><p></p><p>有新的自定义输入的表单</p><p></p><p><code lang=\"null\">const MyForm = () =&gt; {  const [text1, setText1] = useState('');  const [text2, setText2] = useState('');  const [text3, setText3] = useState('');  return </code></p><form><code lang=\"null\">     setText1(e.currentTarget.value)} /&gt;     setText2(e.currentTarget.value)} /&gt;     setText3(e.currentTarget.value)} /&gt;  ;}\n</code></form><p></p><p></p><p>现在，每次按键时，每个组件都会重新运行toTitleCase。useState使得整个表单组件可以对任何状态变化做出反应！</p><p></p><p>噢，不！</p><p></p><p>我是说，这有问题吗？浏览器的速度非常快。硬件的速度也非常快。也许这不是问题。</p><p></p><p>好吧，直到它成了问题。</p><p></p><p>在不同的地方逐步增加计算不会造成太大的伤害。但一直这样做下去，最终就会变得缓慢。现在，你必须面对这样一个问题：性能问题的源头不止一个——哪都可能导致性能问题。解决这个问题所要做的工作会远远超出你的想象。</p><p></p><p></p><blockquote>你是不是忘了 useMemo ？</blockquote><p></p><p></p><p>啊，是的。那……</p><p></p><p></p><h2>记忆化</h2><p></p><p></p><p>在这个问题上，我无疑希望人们达成了共识。然而，有一篇支持记忆化的文章，就会有一篇反对记忆化的文章。</p><p></p><p>要知道，记忆化有性能成本。</p><p></p><p></p><blockquote>Dan Abramov 反复指出，记忆化仍然会产生比较 props 的成本，并且在许多情况下，记忆化检查永远无法阻止重新渲染，因为组件总会接收新的 props。例如，可以看下 Dan 的推特讨论。—— Mark Erikson（React 渲染行为（大部分）完整指南）</blockquote><p></p><p></p><p>讨论提到了 React.memo()，这是 React 中一种略有不同的记忆化形式。</p><p></p><p><code lang=\"null\">const MyInputMemoized = React.memo(MyInput);\n</code></p><p></p><p>记忆化整个组件可以使渲染级联不再检查它的子组件。把这个作为默认设置似乎很合理，但 React 团队似乎认为，比较 props 的性能成本超过了大规模渲染级联的平均性能成本。</p><p></p><p>我觉得那可能是错的。Mark 似乎同意这个观点。</p><p></p><p>这也使得排版看起来更加丑陋。我见过的大多数代码库都倾向于避免使用React.memo()，除非非常确定它可以显著提高性能。</p><p></p><p>另一个反对记忆化的论据是，当父代码编写不正确时，React.memo()很容易失效。</p><p></p><p>模拟的 React.memo() 示例</p><p></p><p><code lang=\"null\">// 记忆化以防止重新渲染const Child = React.memo(({ user }) =&gt; </code></p><div><code lang=\"null\">{user.name}</code></div><code lang=\"null\">);function Parent2() {  const user = { name: 'John' };  // 无论如何重新渲染  return ;}\n</code><p></p><p></p><p>我们用速度最快的方式（浅相等）比较 props。每次重新渲染看起来都像一个新的 prop。由于重新渲染很常见，所以我们需要注意这一点。</p><p></p><p>这里，组件是反应式“原语”，我们可以通过 在组件中移动状态 来修复一些记忆化问题。</p><p></p><p>在创建一个产品时，我并不是很喜欢这种讨论。</p><p></p><p></p><blockquote>是的，我说的是 useMemo()，不是 React.memo() 。</blockquote><p></p><p></p><p>对于 useMemo()，我们也有同样的性能考量。现在，成本变成了比较“依赖关系”，而不是 props。</p><p></p><p>模拟的记忆化示例</p><p></p><p><code lang=\"null\">const value = useMemo(() =&gt; {  const items = dataList    .map(item =&gt; [item, placeMap.get(item)])    .filter(([item, place]) =&gt; itemSet.has(place));  return pickItem(items, randomizer);}, [dataList, placeMap, itemSet, pickItem, randomizer]);\n</code></p><p></p><p>不要在上面的代码上花太多时间。那只是为了演示。</p><p></p><p>但是你有没有注意到，事情有些怪异？有两个单独的状态变换。一个是列表操作，另一个是在结果数据上调用某个函数。</p><p></p><p>我们不小心记忆化了过多内容！如果randomizer &nbsp;变了会发生什么？重新运行整个函数？！我们应该写成下面这样。</p><p></p><p>恰当的记忆化</p><p></p><p><code lang=\"null\">const items = useMemo(() =&gt; {  return dataList    .map(item =&gt; [item, placeMap.get(item)])    .filter(([item, place]) =&gt; itemSet.has(place))}, [dataList, placeMap, itemSet]);const value = useMemo(() =&gt; {  return pickItem(items, randomizer)}, [items, pickItem, randomizer]);\n</code></p><p></p><p>现在，我们的价值更明确了。对randomizer的更改不会重新运行.map和.filter，只会重新运行pickItem调用。</p><p></p><p>得救了！……是吧？</p><p></p><p>当涉及到列表操作时，我通常会倾向于自动记忆数据，但我并不确定这是否是一个合理的做法。这种记忆化的最大问题在于它会让代码变得更加 复杂难懂，可能被称为“代码气味”。</p><p></p><p>尽管记忆化有时可能会有所帮助，但我们必须同时注意组件的使用和组成，才能明确它是否适用于特定的情况。</p><p></p><p>记忆化的复杂性并不是 React 所独有的，但我们需要手动处理它的频率远高于必要的次数。</p><p></p><p>记忆化确实是一种解决问题的方法，但很难确定何时以及何处使用它，这可能会令人沮丧。它的有效性设计得真的很差。</p><p></p><p></p><h2>教学方法</h2><p></p><p></p><p>这是我喜欢关注的。多年来，我一直把研究编程教学方法当成一种爱好。我一直在关注一个问题：</p><p></p><p></p><blockquote>如何最有效地传达编程概念？</blockquote><p></p><p></p><p>我想我还没有答案，但我知道你的做法正相反。</p><p></p><p>在传统的 React 教学中，它被描述为一个简单的组件系统，状态连接到 UI 并在随时间更新。然而，我曾有幸教授一些不太熟悉框架、React 或编码的人。我发现，React 并不简单，而模糊的教材使得它更难学。</p><p></p><p>对于那些使用 React 时间较长的人来说，我们所讨论的概念和心理模型可能不再需要解释。然而，对于大多数人来说，这些概念并不那么容易理解。</p><p></p><p>例如，组件在状态更新时会重新渲染，但这并不是显而易见的。状态的使用并没有对应的名称，那么它是如何保存的呢？</p><p></p><p>实际上，状态保存在 VDOM 栈上，这也解释了为什么组件的顺序很重要。状态是组件的输入，而状态突变是向树发送信号，由树再次调用函数来比较输出差异。你是否理解了这个过程呢？</p><p></p><p>随着时间的推移，你是否也注意到了这些问题呢？也许你读了一篇文章，看了一段视频，或者你比我聪明很多。</p><p></p><p>与其他同时代的替代方案相比，状态更新的复杂性在 React 开发中经常成为一个障碍。然而，这些必备的概念却往往是被作为 高级主题 来教授的。</p><p></p><p>在这方面，我希望新的 React 文档 会做出改变。我也希望人们能够认识到，有很多初学者更喜欢通过视频来获取信息，而不是冗长的教程。</p><p></p><p></p><h2>修复 React</h2><p></p><p></p><p>我想重新探讨下这个表单。</p><p></p><p>这种痛苦由来已久，以至于表单最佳实践出现了一些变化。不受控输入现在大行其道。</p><p></p><p>组件更新源于状态更新。受控输入会强制对每个表单交互进行状态更新。如果只是为了让表单可以做任何事，那么只需更新提交和验证步骤即可。</p><p></p><p>这种模式在 Formik 和 react-hook form 等表单库中得到了推广。我们可以将</p><p></p><p>纯 React 表单</p><p></p><p><code lang=\"null\">const [firstName, setFirstName] = useState('');const onSubmit = data =&gt; console.log(data);return </code></p><form><code lang=\"null\">  <input name=\"firstName\" value=\"{firstName}\" /> setFirstName(e.currentTarget.value)}   /&gt;\n</code></form><p></p><p></p><p>转换成 react-hook-form 表单</p><p></p><p><code lang=\"null\">const { control, handleSubmit } = useForm({  defaultValues: { firstName: '' }});const onSubmit = data =&gt; console.log(data);return </code></p><form><code lang=\"null\">   <input />}  /&gt;\n</code></form><p></p><p></p><p>是的，这增加了一些复杂性，但是我们帮助解决了状态更新对组件影响过大的问题。</p><p></p><p>然而，这引出了一个有趣的问题。当审视 React 的生态系统时，我们会发现很多库都是为了修复 React 的缺点而存在的。</p><p></p><p>当你看到一个库号称速度提升 100 倍并改进了功效学设计时，它们所做的就是避开 React。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a0/a0fd8fe9e4c2947f1246821e5839c169.png\" /></p><p></p><p>郑重声明，我并不反对这个观点。看着 UI 渲染器生态系统如此不知疲倦地工作，一边使用它，又一边躲着它真是太有意思了。</p><p></p><p>关于状态讨论，有几个朋友加入我们，包括 react-redux、 @xstate/react、Zustand、Jotai、Recoil 等。</p><p></p><p>状态讨论一般会让人感到沮丧，因为它们通常会掩盖某种形式的 React Context。我们必须按照 React 的规则来触发 UI 更新，所以前面提到的所有库都有某种形式的级联渲染效果。</p><p></p><p>React 组件不能直接共享状态。因为状态存在于树上，我们只能间接地访问这棵树，所以我们必须爬上爬下，而不是从一个树枝跳到另一个树枝。在这个过程中，我们会接触到本不需要的东西。</p><p></p><p>Jotai 示例</p><p></p><p><code lang=\"null\">const countAtom = atom(0);const doubleCountAtom = atom(get =&gt; get(countAtom) * 2);const MyComponent = () =&gt; {  const [count, setCount] = useAtom(countAtom);  const doubleCount = useAtomValue(doubleCountAtom);  return <button> setCount(count + 1)}&gt;    {count} x 2 = {doubleCountAtom}  </button>;}\n</code></p><p></p><p>我们已经使用 Jotai 巧妙地设置了一些派生状态，但将其插入 React 意味着我们回到了基于组件的反应性。</p><p></p><p>你可以为 React 添加“细粒度”的反应式系统，而且不需要做太多修改。</p><p></p><p>那需要框架级的集成。</p><p></p><p></p><h2>细粒度反应</h2><p></p><p></p><p>与框架集成的细粒度反应会是什么样子？可能是像 Solid.js 这样的东西。</p><p></p><p>solid.js 示例</p><p></p><p><code lang=\"null\">function Counter() {  const [count, setCount] = createSignal(0);  setInterval(() =&gt; setCount(count() + 1), 1000);  return </code></p><div><code lang=\"null\">Count: {count()}</code></div><code lang=\"null\">;}\n</code><p></p><p></p><p>在讨论 React 时提到 Solid 很有趣，因为它的 API 看起来与 React 非常像。主要的不同是我们不需要像这样在useEffect中封装代码。</p><p></p><p>在 React 中，这类代码会导致一个严重的 Bug，即每秒创建一个新的setInterval调用。</p><p></p><p>对于非基于组件的反应式框架，组件之间的区别就消失了。它们在设置和生成 UI 时非常有用。在应用程序生命周期中，状态才是真正重要的。</p><p></p><p>Preact、Vue、Angular、Marko、Solid 和 Slvelte 等框架都采用了某种形式的细粒度反应。它们称之为信号、存储或可观察量。语义差异可能很重要，但我将把这个概念称为 信号。</p><p></p><p>reactive 数据存储（信号）</p><p></p><p><code lang=\"null\">const [headerEl, divEl, spanEl] = getEls();const nameSignal = signal('John');nameSignal.subscribe(name =&gt; headerEl.textContent = `Name: ${name}`);nameSignal.subscribe(name =&gt; divEl.textContent = name);nameSignal.subscribe(name =&gt; spanEl.textContent = `\"${name}\"`);// 在应用程序中的某个地方nameSignal.set('Jane')\n</code></p><p></p><p>这个例子包含了信号——可以感知自身“订阅者”的状态块。当我们改变这个状态值时，信号将通过传递进来的函数“通知”它的订阅者有一个更新。</p><p></p><p>在执行更新之前，我们不需要参照最高状态树来比较 UI 输出差异。我们可以直接将状态连接到 UI 更改。</p><p></p><p>信号也可以通知其他信号。计算状态机仍然存在，只是具备了非常好的功效学设计。</p><p></p><p>使用反应式原语，你可以在一个小时内构建出自己的框架，并且代码比使用其他反应式模型要好得多。</p><p></p><p>reactive mini-framework</p><p></p><p><code lang=\"null\">const num1 = signal(0), num2 = signal(0);const total = computed(() =&gt; num1.value + num2.value);const inputEl1 = create('input').bind('value', num1);const inputEl2 = create('input').bind('value', num2);const outputEl = create('input').bind('textContent', total);get('body').append(inputEl1, ' + ', inputEl2, ' = ', outputEl);\n</code></p><p></p><p>就像 Rustaceans 反对任何没有内存安全的新语言一样，我反对任何没有信号的新框架。</p><p></p><p>我们在 WASM 战争中也发现了类似的战斗。Yew 是最突出的 Rust 前端框架，但它依赖于类似 React 的方法。它的性能仅略优于 React，而基于信号的 Rust 框架（如 Leptos 和 Sycamore）超过了 Angular 和 Slvelte。</p><p></p><p></p><h2>问题总结</h2><p></p><p></p><p>虽说如此，但我认为只看框架基准测试是不够的。</p><p></p><p>React 的功效学设计很差。</p><p></p><p>使用 React 比使用 Slvelte 更容易搞砸。当然，超优化的 React 只比其他框架差一点点，但我不写超优化的代码。因此，在实践中，我看到的 React 代码往往每个文件都有十几个性能问题，出于理智，我们忽略了这些问题。</p><p></p><p>React 在发布时非常棒！但现在，我们有了更好的选择；客观情况基本就是这样。虽然随着时间的推移会有所改进，但我不认为 React 会从根本上改变它的工作机制，让它再次变得可以忍受。</p><p></p><p>那么我们为什么还在使用 React 呢？</p><p></p><p>1.经过实战检验</p><p>a.大公司已经证明可以把它应用于生产。</p><p>b.当你看到使用特定技术的产品取得了成功时，就更容易做出决定。</p><p></p><p>2.生态成熟</p><p>a.技术上讲，是这样的，但是有一半的生态系统要么是作为普通库的 React 封装器而存在，要么是作为 React 缓解包而存在。</p><p>b.非凡的反应模型通常意味着插入 React 之外的第三方库更容易。</p><p></p><p>3.劳动力更充足</p><p>a.这一点似乎无法反驳。如果你想要一份工作，最好的选择就是 React。如果你想雇佣员工，最好的选择也是 React。</p><p>b.虽然我认为教授其他框架通常更容易，但只有当你有足够的时间和能力来培训工程师时，这才有意义。</p><p></p><p>4.还在演进</p><p>a.当“解决方案”就在眼前时，要做出改变很难。</p><p>b.几乎“全栈”都在演进，但每一个新产品都是作为 React 所有弊病的解决方案而出现。</p><p></p><p>5.很难离开</p><p>a.预期收益抵不上迁移成本。</p><p>b.它的反应模型非常独特，以至于迁移到另一个框架需要花费大量的时间，无法立即获得明显的改进。</p><p></p><p>我现在的工作是 React 开发。我的下一份工作还将是 React 开发。再下一份也可能是。</p><p></p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://emnudge.dev/blog/react-hostage\">https://emnudge.dev/blog/react-hostage</a>\"</p><p></p><p></p><h2>欢迎参与讨论</h2><p></p><p></p><p>3 月 17 日，React 正式发布了全新的 React 官方文档，官网的首选框架建议是 Next.js 或 Remixjs ，这引发了社区的积极讨论，你对此有何看法，欢迎留言。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d6/d6c3170c4c30a8c00ea05b27f77d34a3.png\" /></p><p></p><h5> 相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/3e10ee935ffd1b23b1ecd8842\">看透 react 源码之感受 react 的进化</a>\"</p><p><a href=\"https://xie.infoq.cn/article/9d8a9c2ca82a82fdb098ee31b\">前端开发框架 React 技术如何与小程序结合，进行页面构建</a>\"</p><p><a href=\"https://xie.infoq.cn/article/7c17044e7d4827b37bfd698cb\">React 源码分析 1-jsx 转换及 React.createElement</a>\"</p><p><a href=\"https://xie.infoq.cn/article/958fbe57ff88cdba990d99739\">手写一个 react，看透 react 运行机制</a>\"</p>",
    "publish_time": "2023-03-28 10:10:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "WebAssembly在工业领域的巨大机遇",
    "url": "https://www.infoq.cn/article/pskeeKXTSbmQa2cauwBh",
    "summary": "<p>WebAssembly 是一种新的编码方式，一种虚拟的指令集，具有紧凑的二进制格式，可以以接近原生的性能运行，并被多种语言诸如 C / C ++，Rust，Go，Kotlin等作为编译目标。</p><p></p><p>在工业领域，WebAssembly已经获得学术界和一些大型企业的强烈兴趣，在过去的2年中已经涌现一些相关前沿性研究报告与开源项目。</p><p></p><p>在2022年由美国卡耐基梅隆大学举办的一场名为“WebAssembly Research Day”的学术会议中，西门子研究人员作了一个主题为“An end-to-end toolchain for evaluating WebAssembly runtimes for CPS-IoT Use cases”的报告。西门子团队介绍了当前工业界最新的编程语言WebAssembly技术在以下方面对他们的巨大意义：</p><p></p><p>网络物理控制（ Cyber Physical Control）：传统上，电气控制工程师、计算机软件工程师和通信工程师三组人马分开工作，相对独立。工业化与信息化的两化深度融合在工业控制系统内集成计算、通信和存储等功能，使用WebAssembly技术支持计算和控制在网络中自由分布流动，是网络物理控制的极佳的支撑技术，也是电气控制、软件、通信工程师们共同的工具。</p><p></p><p>广泛的设备支持（Range of supported devices）：WebAssembly的轻量化设计可以支持其在500K内存以内的嵌入式设备上轻松工作，也能很好利用512M内存的网关和16G内存的工业PC，同时还可以在云端服务器上运行。WebAssembly程序的跨平台特性，让计算负载可以按需灵活部署。</p><p></p><p>兼容巨大存量的软件（Legacy Software Support）：像西门子这样的大型公司会具有基于C/C++的海量存量软件，这些软件源码都可以被重新编译到WebAssembly，部署到各种现代的硬件平台上。工业化与信息化的两化深度融合就是要把用在IT的先进技术应用到OT领域。WebAssembly正是这一有力的工具，将在IT领域开发出的应用，如人工智能、机器学习、计算机视觉、数据处理、数据存储、数据流处理、以及复杂模型的控制系统等无缝移植到工业领域里。</p><p></p><p>高性能和一致性（Performance and consistency）：西门子对业界主要的开源WebAssembly引擎进行了科学全面的测试，确认WebAssembly技术在程序运行与计算性能上的优势，其中由英特尔、小米、蚂蚁、亚马逊等公司主导开发WebAssembly Micro Runtime（WAMR）表现了令人满意的性能。</p><p></p><h2>WebAssembly是什么？</h2><p></p><p></p><p>WebAssembly最初由谷歌、火狐、微软等浏览器厂商推动，用于为用户提供更高执行性能的Web浏览器程序。国际标准组织W3C于2017年推出WebAssembly第一版规范草稿，时至今日WebAssembly的发展已经取得令人瞩目的成就，在我们每一个人的电脑、手机、机顶盒中都有WebAssembly技术在背后支持。同时WebAssembly也超越了浏览器的领域，在云计算、可信计算、网格计算、边缘计算、IoT以及区块链等众多领域取得的非凡的发展。</p><p>&nbsp;</p><p>作为 W3C WebAssembly Community Group中的一项开放标准，WebAssembly 是为下列目标而诞生的：</p><p></p><p>快速、高效、可移植——通过利用常见的硬件能力，同一份WebAssembly字节码程序在不同平台上能够以接近本地的速度运行。</p><p></p><p>可读、可调试——WebAssembly 是一门低阶语言，但是它确实有一种人类可读的文本格式（其标准的最终版本即将颁布），这样可以通过手工来写代码，看代码以及调试代码。</p><p></p><p>安全——WebAssembly 被限制运行在一个安全的沙箱执行环境中。像其他网络代码一样，它遵循浏览器的同源策略和授权策略。</p><p></p><p>支持作为多种编程语言的编译目标。目前支持较成熟的语言有C、C++、Rust、TinyGo、AssemblyScript，业界也在努力让更多语言如Kotlin, TypeScript、Java可以编译到WebAssembly。中国大湾区数字经济研究院的基础软件中心也在以WebAssembly为编译目标设计全新的编程语言。</p><p>&nbsp;</p><p>字节码联盟（BytecodeAlliance）是推动WebAssembly技术发展的一个最活跃业界合作非盈利组织，合作成员包含英特尔、西门子、微软、Fastly、Mozilla，谷歌、亚马逊等企业。字节码联盟提供开源的 WebAssembly引擎（Runtime）实现，WebAssembly系统编程接口（WebAssembly System Interface-WASI)，WebAssembly模块接口工具（WIT），组件模型等工具和组件生态的开源项目。字节码联盟的开源地址是https://github.com/bytecodealliance/，它提供两个主要的WebAssembly引擎开源项目wasmtime和WAMR。</p><p>&nbsp;</p><p>简而言之对于工业等众多领域而言，WebAssembly 的巨大意义在于它提供了一条途径，以使得以各种语言编写的代码都可以以接近原生的速度在WebAssembly引擎上运行。由此WebAssembly被全美计算机协会编程语言特别兴趣组评为2021年度“Programming Languages Software Award”。</p><p></p><p></p><h2>为什么是WebAssembly？</h2><p></p><p></p><p>和Java与.NET的不同？WebAssembly是一种同时具有字节码格式和文本格式的计算机程序语言。字节码格式带来了跨平台的关键能力，Java是由Sun最早引入字节码格式的语言，其class字节码格式已经帮助Java语言取得了巨大成功。微软也为.NET引入了公共语言运行时 (CLR)的字节码文件格式，成为Windows平台主流的程序运行格式。那在工业界为什么是WebAssembly呢？</p><p></p><p>首先，WebAssembly是W3C组织下定义的开放标准，W3C所创建的标准定义流程保证了万维网（Web）领域互联互通与向后兼容，WebAssembly的演进是先有规范后有实现，具有充分的向后兼容支持，不会发生剧烈甚至颠覆性的改变。其所有的相关编译器、引擎、工具等都是完全开源的方式在组织开发。在WebAssembly技术领域，不存在像甲骨文公司和微软公司对Java和.NET从商标、专利、版权、技术路线等全方位的独家控制。这些特点，对于需要有长时期支持的工业系统来说尤其重要。</p><p></p><p>其次，支持多种编程语言，尤其是C/C++。虽然Java，Kotlin和Scala等语言也都支持编译到JVM字节码格式，但与之相比，WebAssembly是唯一支持C/C++、Rust的字节码技术。在工业领域，需要能够重用巨大存量的C/C++库和软件，Rust语言因为其内存安全特性，也备受工业界关注。</p><p></p><p>第三点，内置的隔离能力。Java和.NET设计初衷并不包含模块之间的隔离能力。WebAssembly的设计，让在同一个进程内或者在嵌入式无用户状态环境中执行多个WebAssembly程序模块，保证模块之间具有强内存隔离性。同时任何一个WebAssembly程序如发生了非法内存访问，将被WebAssembly引擎所捕获，不会影响到其他程序的执行。这个特点让WebAssembly在CDN领域作为微容器，在单个进程内创建许多的WebAssembly实例，达到极高的吞吐能力。对于工业领域众多嵌入式设备，这个能力可以帮助分离应用程序和固件的开发，支持嵌入式上的应用后装载和动态更新，甚至为支持第三方应用开发提供了可能。</p><p></p><p>第四点，超轻量型与高执行性能。参考开源项目WebAssembly Micro Runtime，其执行一个WebAssembly代码所需要的资源可以在100K内存之内，同时通过预编译技术，WebAssembly程序执行速度可以接近甚至超过GCC原生编译程序执行的速度。同时WebAssembly支持硬件加速计算能力如SIMD，支持多线程充分利用现代CPU多核计算能力，让WebAssembly轻松执行Tensorflow这样机器学习应用类型的密集计算。</p><p></p><p>最后，跨平台能力。WebAssembly和Java和.NET一样具有跨平台能力，同一份二进制文件可以运行在不同CPU架构、不同操作系统的环境上。工业领域存在如此多的不同设备类型，我们可以说没有比工业领域更欢迎这个关键能力了。</p><p></p><p>最后我们借用英特尔WebAssembly技术专家在2021 Automotive Linux Summit上主题为“The Cool Features Of WebAssembly Micro Runtime For IoT And Embedded ”汇报中的如下一页，来做为总结。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b0ffc537500cef734b4a7f908cd615d6.png\" /></p><p></p><p></p><h2>WebAssembly在工业领域的展望</h2><p></p><p></p><p>卡耐基梅隆研究人员汇报了与德国博世公司合作的主题为“Giving the Cloud an Edge with WASM”的研究项目，介绍了目前工业制造中的一些痛点（如下图）：</p><p></p><p>每一个工艺阶段都包含若干FPGA/PLC执行固定内部循环控制</p><p></p><p>高层的控制在车间/工厂层级运行，运行在许多不同的操作系统上</p><p></p><p>设计工作一般在一个非生产现场的办公或实验室环境中进行</p><p></p><p>补丁和升级经常都需要手工、现场执行，既低效又容易出错</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/4902815e17851322f1cf624aa644adfe.png\" /></p><p></p><p>云计算发展今天已经证明大规模计算调度的技术成熟性，工厂实际上也包含海量的、规格各异的计算单元，WebAssembly 可以帮助在工厂内部实现计算资源的统一调度。下图中所示，WebAssembly (WA) 用于运行各种的工业应用，可以按照时延、资源等需要在嵌入式设备、工业PC和车间/工厂服务器之间灵活部署与调度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c92586885eef232b829da3d1c8f08eb.png\" /></p><p></p><p>由此他们开发的银线平台（Silverline Platform），可以将大部分的工业应用程序编译成WebAssembly，使用WebAssembly Micro Runtime（WAMR）开源项目作为运行引擎，提供一个统一的管理调度平台。这个系统帮助充分利用已有的硬件计算资源，实现更先进的业务柔性和管理性。</p><p>&nbsp;</p><p>Digital Twin Assembly (dtasm)是由西门子开发的一个支持数字孪生、模块式组装的开源项目，也使用了WebAssembly模块作为仿真程序的目标格式。每个仿真单元在过程的每一个时间单元格中使用其输入数据执行计算，将结果输出到其输出变量中，作为其当前时间格的输出。该项目定义仿真模块的输入和输出接口定义和二进制接口标准，这样可以由很多个基于WebAssembly程序模块的仿真单元可以灵活组装起来，形成链式调用，构建一个复杂的数字孪生系统。</p><p>&nbsp;</p><p>在中国尽管还没有许多在工业企业中被应用的公开报告，已经有不少企业在物联网相关领域使用WebAssembly。小米的工程师在2021 Wasm Open Day活动中报告了“WASM &amp; WAMR在AIOT中的实践”，介绍了小米Vela物联网操作系统中对WebAssembly的支持与应用。阿里巴巴工程师在题为“Waft：基于WebAssembly的AIoT应用框架实践”的汇报 中介绍了基于WebAssembly设计的物联网应用开发框架，提供更加平滑的用户体验。</p><p>&nbsp;</p><p>在开发者工具领域，浏览器内的WebAssembly技术已经让一些传统的桌面工具迁移到浏览器上，例如Autodesk公司已经将传统的AUTOCAD软件迁移值浏览器。国内企业三维家自研基于WebAssembly的图形引擎，并且在基于云的建模内核之上，融入大数据、AI人工智能等前沿技术，打造出面向制造业、高AI化、一体化的工业软件。</p><p>&nbsp;</p><p>在不久的将来，我们可以期望WebAssembly在广泛的工业领域成为一个关键性的支撑技术。其中最具有想象的三个主要的领域：</p><p></p><p>自动化与机器人控制应用：传统PLC等编程语言的能力已不足以满足现代以数控机床、视觉控制、自动巡航、工业机器人等为代表的复杂控制场景需要， WebAssembly可以让更多的编程语言进入控制应用开发，引入更多的语言生态资源。</p><p></p><p>人工智能的应用：基于WASI-NN（神经元网络）系统接口，WebAssembly程序将可以在工业场景的各种不同计算单元中使用人工智能的算法</p><p></p><p>云边端的统一的集中管理调度平台（Centralized Management Platform）：使用统一的编程调试软件（Engineering System）及管理调度平台，易于部署、实施、管理、监控和维护。</p><p></p><h2>如何开始使用WebAssembly？</h2><p></p><p></p><p>现在正是工业领域企业开始使用WebAssembly构建解决方案的合适时机。下面提供了一些可用于了解、使用WebAssembly的参考链接：</p><p></p><p>https://developer.mozilla.org/zh-CN/docs/WebAssembly：由Mozilla开发者网络提供的WebAssembly基础介绍</p><p></p><p>https://github.com/WebAssembly/proposals：WebAssembly规范提案状态</p><p></p><p>https://github.com/WebAssembly/WASI: WebAssembly system interface规范提案</p><p></p><p>https://github.com/bytecodealliance/wasm-micro-runtime：WebAssembly Micro Runtime开源项目</p><p></p><p></p><p>作者简介：</p><p></p><p>魏东，于1991年毕业于清华大学电机工程系，获工程学士学位；并分别与2001年和2004年于美国新泽西理工学院（New Jersey Institute of Technology）获得电气工程的硕士和博士学位。在美国博士毕业后，魏东博士长期从事工业自动化领域的理论及应用研究，取得了一系列在国际上有影响的独创性成果。他是cyber-resilient ICS模型的创立者之一，其研究成果的原创性得到学术界和工业界的认可。魏东于2004年五月至2019年五月在西门子（美国）研究院任科学家，从事工业自动化技术的研究与开发，主要的研究课题包括工业4.0、可编程逻辑控制器（PLC）、运动控制、工业网络通信及安全。魏东博士于2020年入选宁波市镇海区“雄镇英才”高层次人才创业项目计划，并于9月来宁波创立浙江清捷智能（Tsing-Jet）科技有限公司。2022年入选宁波市“甬江引才”计划。目前聚焦于开发基于５G与边缘计算技术的产品和系统去解决工业互联网在工厂侧的“最后一公里”问题。</p><p>&nbsp;</p><p>侯明鹏，男，研究员，博士。就职于北京机科国创轻量化科学研究院有限公司。主要从事先进制造、增材制造和特种设备控制系统的开发和研究工作。获得授权发明专利3项；发表论文5篇；授权软件著作权12项；发布团体标准2项；先后主持和参加863、04专项、重大专项、北京市科技计划等纵向课题10余项。获得省部级奖三项：（1）中国机械工业科学技术奖特等奖（2）中国机械工业科学技术奖一等奖（3）北京市科学技术奖。</p><p></p><p>参考资料：</p><p></p><p>1.https://developer.mozilla.org/zh-CN/docs/WebAssembly/Concepts</p><p>2.https://www.cs.cmu.edu/~wasm/wasm-research-day-2022.html</p><p>3.slides, An end-to-end toolchain for evaluating WebAssembly runtimes for CPS-IoT Use cases, Siemens</p><p>4.slides, Giving the Cloud an Edge with WASM, Carnegie Mellon University, Bosch</p><p>5.https://github.com/siemens/dtasm, Digital Twin Assembly, Siemens</p><p>6.https://ex.chinadaily.com.cn/exchange/partners/82/rss/channel/cn/columns/sz8srm/stories/WS62e4f86aa3101c3ee7ae186f.html</p><p>7.http://www.sigplan.org/Awards/Software/</p><p>8.https://2021.international.conference.modelica.org/proceedings/papers/Modelica2021session6A_paper3.pdf</p><p>9.https://idea.edu.cn/dii.html</p><p>10.https://github.com/bytecodealliance/wasm-micro-runtime</p><p>11.https://ossalsjp21.sched.com/event/peeL/the-cool-features-of-webassembly-micro-runtime-wamr-for-iot-and-embedded-xin-wang-intel</p><p>12.https://github.com/WebAssembly/WASI/issues/443</p><p></p><p></p><blockquote>原文转载自“CAA会员服务”微信公众号，链接：https://mp.weixin.qq.com/s/Lmmt3_R6-Wkh-JzBapSgJw</blockquote><p></p>",
    "publish_time": "2023-03-28 10:55:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度何俊杰：AI创作时代，AIGC就是“马良”的“神笔”",
    "url": "https://www.infoq.cn/article/wgZQsRQNIFKLQ3cJ3AUw",
    "summary": "<p>写新闻、画插画、剪视频.......随着生成式AI的爆发，内容创作领域的变革正在按下加速键。3月28日，百度集团资深副总裁、百度移动生态事业群组（MEG）总经理何俊杰在“2023百度内容生态共生大会”上表示，“AI创作时代”到来，创作者将迎来三重新的机会，包括新的流量风口、新的内容生产力革命、新的多元变现蓝海，而百度将与广大MCN机构和创作者一起拥抱“AI创作时代”，让AI技术赋能内容创作、分发和变现的全流程。</p><p>&nbsp;</p><p>此前不久，百度自研生成式AI正式发布，已有超过650家企业接入，AIGC+阶段正在呼啸而来。而百度移动生态处在这场变革的最前线，“听得见炮火，看得见市场”，何俊杰表示，百度移动生态要与广大创作者一起，攀上风口，抓住机会，让所有创作者、合作伙伴都乘上开往“AI创作时代”的大船，一个都不能少！</p><p>&nbsp;</p><p>而机会之一就是“新的流量增长风口”，何俊杰指出，生成式AI的发展，是全新的计算范式带来的新机会。机会落到互联网行业，这将是一次代际变革，就像蒸汽时代向电气时代的变革，PC时代向移动互联网时代的跨越。“每一次这样的变革，都会诞生新的增长风口。”百度作为中国人工智能市场长期增长的最佳代表，正站在浪潮之巅。据悉，生成式AI与百度搜索整合，将引领搜索体验的代际变革。除了提供更好的搜索和答案，还会提供全新的交互和聊天体验，以及独特的生成内容，极大地丰富内容生态和供给，吸引更多用户，并有机会形成新的流量入口。</p><p>&nbsp;</p><p>其次，新生产力革命正在爆发。“如今，AIGC就是创作者手中的那支神笔。”何俊杰说。在AIGC的帮助下，创作的效率将大大提高,创作的门槛将会无限降低，创作的想象力持续延伸。&nbsp;“未来，AIGC工具将会成为创作者最得力的搭档、最核心的标配，而且使用门槛会比鼠标键盘还要低。”何俊杰表示。去年万象大会，百度推出创作者AI助理团，目前已有超过45万百度创作者使用，产出了超700万篇内容，累计分发量超过200亿。今年万象大会上，百度移动生态还将发布更多更好的生产力工具，让AI人人可用。</p><p>&nbsp;</p><p>第三，新的多元变现蓝海也在开启。内容变现一直是困扰众多创作者和MCN的核心难题，而百度内容生态拥有百度搜索这样的超级流量入口，是用户离交易最近的决策入口，也是创作者多元变现的绝佳场域。百度搜索每天响应来自100多个国家、超60亿次的搜索请求，每天有超9000万人在百度搜索商品，催生出了包括教育、数码、汽车、母婴育儿、美食、大健康等特色领域在内的一批极具潜力和商业价值的特色领域。因此，从创作到变现，百度内容生态也有自己独有的解法。何俊杰表示，百度要与创作者们一起挖掘内容消费场景的“聚宝盆”，还要用AI修一条内容变现的“高速路”。接下来，百度移动生态将会全面整合生成式AI，推出内容创作的一站式AI创作平台、AI笔记等系列AIGC产品。</p><p>&nbsp;</p><p>而对于创作者们担心被AIGC取代的焦虑，何俊杰直言，人类历史上的每一次科技变革都给出了答案。造纸术让纸张替代了竹简和绢帛，却没有剥夺书生写诗作文的机会，而是进一步促进了知识的普惠化；汽车普及的时候，聪明的马车夫也不会因此失业，而是率先取得了驾照。因此，AI创作时代之于创作者，是蛋糕变大了而不是变小了，是机会更多了而不是更少了。它不是危机，而是生机。AI的每一次创新与进化也都是为了提高社会生产力，为人类带来更多的自由与可能。</p><p>&nbsp;</p><p>据悉，会上，百度集团副总裁、移动生态商业体系负责人王凤阳，百度副总裁赵强，百度MEG内容生态副总经理宋健也分别从商业和内容两个层面，向到场的近100家MCN分享了百度内容生态未来的规划和愿景，2023年，百度内容生态将从流量、技术、商业三个维度，引领创作者拥抱AI创作时代，帮助创作者创造风口、把握风口。</p>",
    "publish_time": "2023-03-28 13:22:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯大规模云原生技术实践案例集",
    "url": "https://www.infoq.cn/article/Is534v4mBvUmgC1uJEdc",
    "summary": "<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/f9/8e/f9924e14289757686d940513a85bdd8e.jpg\" /><br />\n经过多年磨砺与创新，腾讯内部海量自研业务已实现全面上云。近三年来，腾讯的自研业务上云规模 <span class=\"orange\"><strong>已经突破 5000 万核，累计节省成本超过 30 亿。</strong></span></p>\n<p>包括 QQ、微信、腾讯视频、王者荣耀等在内的 <span class=\"orange\"><strong>腾讯内部业务，和腾讯云百万级外部客户一样基于公有云的模式来开发运营</strong></span>，腾讯全面开启业务云端生长新时代。</p>\n<p>“这是腾讯自研上云战略的一个里程碑。”腾讯集团高级执行副总裁、云与智慧产业事业群CEO 汤道生表示：“把腾讯内部海量业务搬上云端，不仅帮助腾讯构建面向未来的技术架构和研发文化，推动科技成为公司业务发展和产品创新的动力与支撑，也全面锤炼了腾讯云的产品、技术和综合服务能力，这些能力将加快推动产业的数字化升级，助力实体经济全面发展。”</p>\n<p>大部分业务都是在保持高速增长的过程中上云。比如，QQ 是腾讯首个全面上云的内部业务，把如此庞大和复杂的业务搬上云端，技术团队实现了对用户零感知，被外界称为“开着飞机换引擎”。</p>\n<p>同时，腾讯云也为新兴业务的高速发展提供有力支撑。以视频号为例，借助腾讯云的弹性扩容能力，视频号稳健支撑诸如西城男孩、周杰伦、崔健等明星的大型线上演唱会活动；得益于对象存储 COS 和腾讯云直播服务，视频号在春节等特殊时段抗住了超平时 3 倍以上业务高峰。</p>\n<p>腾讯会议凭借生于云、长于云的大规模实践，现在已经成为中国最受欢迎的云视频会议产品，依托业界领先的实时音视频产品 TRTC，腾讯会议可以有效保障数亿用户在复杂网络环境中流畅清晰的视频会议体验。</p>\n<p>腾讯自研业务上云，打造出了 <span class=\"orange\"><strong>国内最大规模的云原生实践</strong></span>。</p>\n<p>三年来，数千万核的自研业务上云规模，推动腾讯云的自研产品能力不断优化，多项产品性能达到业界领先水平，也推动腾讯云在全球的基础设施不断完善。</p>\n<p>腾讯自研上云 <span class=\"orange\"><strong>明确基于云原生来构建面向未来的技术架构</strong></span>。例如，通过容器和微服务等技术，腾讯构建了统一的技术底座和算力调度平台，有效促进公司内部技术团队的协作与创新。</p>\n<p>目前，腾讯云的 TKE 平台拥有国内最大规模的 Kubernetes 集群，以及最为领先的在离线混部技术，腾讯上云打造了国内最大规模的云原生实践。</p>\n<p>为了向开发者更好的介绍腾讯自研业务、外部客户如何通过云原生技术产品支撑业务发展的，特别推出<strong><span class=\"orange\">《腾讯大规模云原生技术实践案例集》</span></strong>，包括 QQ、腾讯会议、腾讯广告、和平精英、腾讯文档、作业帮、中国南方电网、小红书、知乎、Unity、斗鱼、微盟等十多个海量产品和大规模场景的云原生技术实践。希望在给业界带去参考的同时，能够一起推动国内大规模场景下云原生技术实践的有效落地。</p>",
    "publish_time": "2023-03-28 14:15:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大数据上云存算分离演进思考与实践",
    "url": "https://www.infoq.cn/article/de0971c840628b7b467a110dc",
    "summary": "<p>作者：汤祯捷  阿里云智能计算平台团队</p><p></p><p></p><blockquote>存算分离、数据湖、在离线混部，这些名词越来越多的出现在各行各业数字化转型的关键活动中。本文仅从大数据产品商业化从业者的视角来探讨与分析大数据领域的存算分离演进过程，核心价值，与相关所产生的蓬勃技术生态。内容来自阿里云计算平台大数据技术商业化思考与实践，与大家共同探讨。</blockquote><p></p><p></p><p></p><h1>一、起源：存算分离不是新架构</h1><p></p><p></p><p>数据架构起源：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4a1d43e1253c209bc64c9933afaf68c7.png\" /></p><p></p><p>以上三张图分别代表过去三十年大规模数据架构演进过程。</p><p></p><p>第一张图是最早的Shared-Disk架构主要是通过独立定制网络NAS于存储架构来实现第一代的存算分离架构，性能好但成本较高, 可扩展性较差(ScaleUp)。其中StorageAreaNetwork作为网络存储体系结构，采用网状通道（Fibre Channel ，简称FC，区别与Fiber Channel光纤通道）技术，通过FC交换机连接存储阵列和服务器主机，建立专用于数据存储的区域网络。第二张图是过去十几年Shared-Nothing架构大行其道，主要用于分布式数据仓库/大数据应用场景上，其特点在于性能好且性价比高，有一定的可扩展性(ScaleOut)。可以通过普通硬件实现类似高成本数据存储阵列所实现的效果。过去由于网络带宽的限制，我们习惯性的把计算和存储偶合在一起，以减少网络传输的压力。传统Hadoop大数据架构是MapReduce和HDFS，是用优先本地IO后网络传输，是存算一体的一种形态。但由于这样的架构，导致在资源利用率，异构工作负载等方面存在明显短板。第三图是为了解决存算一体架构问题而诞生的新一代存算分离架构Storage Disaggregation。其背景与趋势我在这里做一下阐述。</p><p></p><p>背景一：高速网络全面普及。以太网传输协议和带宽能力已不再是IO瓶颈。</p><p></p><p>因为网络的高速发展，以及大数据计算框架对 IO 的优化，使得数据本地化已经不再重要，存储和计算分离的架构才是未来。高速以太网Ethernet吞吐量大幅提升而成本和部署灵活性相比FC和IB有大幅度改善，足以应对从当年的千兆迈入10GB，25GB，到现在100GB甚至200GB/s带宽时代。另外无阻塞转发网络：比如FaceBook采用了CLOS网络拓扑，实现了分解式的网络，网络不会成为性能瓶颈，同时提供了灵活的组网能力。</p><p></p><p>背景二：云计算兴起，所带来低成本可扩展的网络存储方案</p><p></p><p>在 AWS 等公有云上，基于新网络架构的块存储/对象存储逐步取代了单机的本地存储，使得公有云上的计算和存储耦合架构更加不合理。针对公有云设计的大数据分析服务，一开始就是采用了计算和存储分离的架构（直接使用 S3 作为存储）。这给产品带来了非常大的灵活性，按需创建和自动弹性伸缩的 Spark 集群是一大卖点（还能最大限度使用 Spot 节点来大大降低计算成本），非常受客户欢迎。AWS 上的先驱也是使用 S3 作为大数据的存储，他们针对 Hive 等做了很多改造才能稳定使用 （不是开源的S3mper）。</p><p></p><p>趋势一：云计算AWS首先提出基于在公共云架构下云原生存算分离数据产品。</p><p></p><p>基于存算分离的云原生数据库诞生。数据库2014年AWS首次推出了共享存储的Aurora，阿里云在2017年推出了云原生数据库PolarDB。华为云在2020年推出了GaussDB for MySQL，华为存储也在2021年针对企业自建数据中心，推出OceanData分布式数据库存算分离方案。</p><p></p><p>趋势二：大数据从数据本地化Data Locality到DisAgg</p><p></p><p>Data Locality在大数据领域发展的前期更加重要，主要是因为分布式计算跨服务器的带宽瓶颈（2012年之前，跨核心交换机的带宽收敛比基本在1:10）,所以超大规模大数据计算的调度器对数据和计算locality匹配，需要特别优化。因为存储具有搬移困难的特点，2012年之前，做大规模计算的核心词是“Locality”：让计算尽量靠近数据以提升效率。当时一个公认的模型是：构建一个足够大的资源池，把数据和计算融合在里面发挥规模效应。</p><p></p><p>2013 年Facebook做了一个这方面的研究，观察在关闭 Hadoop 的数据本地化优化的情况下，对性能究竟有多少影响。实测表明，对计算任务的整体影响在 2%以内，对数据的本地读优化已经不那么重要了（仅限于性能，不包括成本）。后来 Facebook 就逐渐往Spark计算和存储分离的架构迁移，也对所用的大数据软件做了些调整以适应这种新的架构，他们在2015年的 Apache Spark &amp; AI Summit 上做了主题为  详细的分享，他们把这种架构称为 Storage DisAggregation。</p><p></p><p>https://www.databricks.com/session/taking-advantage-of-a-disaggregated-storage-and-compute-architecture</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/25c14eceaceafb161b7867f12073d79e.png\" /></p><p>Figure 1: Spark Disagg</p><p></p><p></p><h1>二、定义：当前存算分离能力思考</h1><p></p><p></p><p>随着新一代的存算分离StorageDisagg架构逐步普及的当下, 如何定义存算分离架构下的能力标准成为了焦点？</p><p></p><p>例如，如何确保以通用存算分离标准实现可扩展可运维的下一代大数据架构？如何确保存算分离架构的ROI为正？是我们一直在思考和探索的问题答案。围绕这存算分离具体应用的场景实践，我们大致可以得到以下7个重要定义指标用于定义存算分离能力标准。</p><p></p><p>系统态计算节点集群与存储节点集群分离, 分别独立支持弹性扩缩容(存储/计算资源)物理集群节点。计算层无状态，支持热升级/存储层数据无需重分布。用户态用户可分拆配置计算资源与存储资源的成本计费，精细化成本核算优化。用户可分拆配置计算资源与存储资源的独立弹性扩缩容。公共云以分时Quota定额方式体现。存储服务化。其形态有开放共享存储形态和存储开放服务形态。非独占的统一开放存储系统服务。多源数据存放在共享存储服务, 资源优化利用。提供统一存储StorageService, 有外部引擎访问接口。性能与稳定性更优,支持可访问/可查询/可分析/默认高可用。多种计算引擎查询访问此统一存储系统服务。计算引擎可访问多种统一存储系统服务。统一计算资源管控,细粒度资源单位拆分，按照细资源单元做弹性扩缩容。</p><p></p><p>以上的7个重要定义能力指标在公共云与非公共云形态下都适用。</p><p></p><p></p><h1>三、价值：存算分离的业务价值分析—业务问题/业务场景/业务价值</h1><p></p><p></p><p>存算分离架构已在互联网广泛使用，当前已在政企金融等行业越来越讨论到。很多企业将大数据上云与存算分离当作数字化转型中架构升级的必要路径。无论是内外部客户与同行都把存算分离当作救命稻草与灵丹妙药。以为存算分离架构升级就能解决当前所遇到所有技术资源瓶颈与业务滞涨。这样的理解是有问题的，其核心原因是在于缺少对存算分离背后的核心价值量化评估。</p><p></p><p>很多人说存算分离的需求很明确，存算分离技术方案就有价值且值得做的。笔者要说是需求与价值并不完全等同。需求是价值的源头, 价值是需求的最终呈现方式。在实际业务实践中，往往需求方和业务决策方不是同一方。存算分离需求清晰明确后，我们需要思考如何存算分离改造后可量化的价值到底在哪里。存算分离是技术手段，那核心目的或者价值到底是reduce cost or involution fast？</p><p></p><p></p><h2>3.1 为什么要做存算分离？</h2><p></p><p></p><p>无论是所有的技术架构升级与业务改造, 为什么要做存算分离, 其根本逻辑就是一本投入产出价值的经济帐。</p><p></p><p>源于网络带宽的能力提升与成本下降，云计算技术带动了面向网络存储方案的低成本普及。此为技术突破准备。而海量数据的激增（数据多、算不动）促使企业从数据使用与数据管理走向数据运营与分析，逐步到面临数据应用三大挑战: 成本高、存不下；效率低、流不动；自动化差、管不好。此为问题需求。越来越多的企业发现相较于之前DataLocality的存算一体架构，新一代存算分离架构能够提供在性能不下降的情况更低成本的大数据方案。此为架构升级。</p><p></p><p>总体上看，对存算分离的诉求以大数据资源效率+稳定性提升是首要目的，此外还包含着业务效率提升诉求(技术创新/业务增效)。业务优先的客户对存算分离的诉求在于降低成本与业务稳定性提升。技术与业务并重的客户对存算分离的诉求在于成本优化与业务增效升级。从当前计算平台云原生存算分离业务实践来看，主要包括以下几个方面的核心价值：</p><p></p><p>资源效率优化资源利用率提高：计算与存储资源解耦，资源使用成本优化。作为底层的资源平台，基础IT环境的资源总是有限的，站在业务的角度是往往是存储先于计算达到瓶颈，到达时间点是不一样的。由于计算和存储的耦合设计，无论扩计算还是扩存储，都在会造成资源的浪费；异构计算的资源负载混部：在统一存储平台提供面向异构计算的工作资源负载下的多维度查询分析服务。在线与离线计算共用计算和存储资源。解决资源波峰波谷问题，实现资源动态削峰填谷存储降本: 存储利用率+冷热分层。支持基于分布式存储系统上的多层存储(热存储/标准存储/冷存储等)。举例来说，存储降本优化主要依赖于归档与冷存储占比大小。如冷存储占比40+%,存储成本大致下降20+%。</p><p></p><p>系统稳定性提升SRE可靠性提升:&nbsp;业务稳定性-计算集群与存储集群分别高可用。运维易用提升：独立热升级-支持原地升级、滚动升级、补丁升级，升级时间短。原先耦合造成扩容不便：计算和存储耦合在一起的典型问题，例如每次扩容都需要考虑数据的迁移，给本来简单的扩容工作带来很多风险和不可控因素。</p><p></p><p>技术创新/业务增效创新数据应用：基于统一存储平台之上对多源异构数据的深度分析数据价值的挖掘，大数据+AI的整合应用，以提升数据协同应用效率。硬件利旧降本:&nbsp;从运维的角度来讲，降低服务器的款型是降低运维难度和工作量的有效手段。随着业务复杂度的增加和新业务线上的加快，对服务器新机型与资源配比的要求也会随之增加。降低服务器的款型很难做到。私有化存算分离改造目的是有两个, 通过存算分离来更顺滑的兼容更多硬件款型, 支持不同计算引擎间混部以降低硬件成本。行业趋势：行业大数据改造升级。随着反全球化浪潮的加剧，政企/金融行业信创自建大数据升级改造的需求越来越多，如何实现基于自研和信创大数据体系下的存算分离架构成为客户普遍诉求</p><p></p><p></p><h2>3.2&nbsp;存算分离架构改造的成本核算</h2><p></p><p></p><p>在关注存算分离的价值优势的同时, 我们无法回避的就是存算分离改造所来的各种额外成本。大数据平台从来就不是一个简单的数据产品，而是一个完整的围绕大数据的生态体系。大数据存算分离架构改造的可行性评估尤为重要。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cfc8bd4e8ec9466fd9dbca9a2a7aa71.png\" /></p><p>Figure 2: 存算分离的价值与成本</p><p></p><p>门槛一：为什么大数据存算分离架构先出现在公共云场景下？</p><p></p><p>大数据存算分离架构首先出现在公共云是有其一定确定性的。</p><p></p><p>原因1）云架构上孵化基于高速网络的低成本+高性价比分布式存储(文件系统/对象存储)。毕竟如果没有高速以太网的普及与更廉价且高可用的存储方案,大数据存算一体一定还是主流。以上两点技术升级，最先都落在云计算场景上。实现了理论上无限的分布式存储扩展，默认高可用。</p><p></p><p>原因2）云计算IaaS架构提供自动化基础运维能力。基本解决存储计算分离架构升级所带来运维复杂性问题(屏蔽网络，安全，稳定性风险，搭建周期长等问题)。</p><p></p><p>原因3）云计算架构支持多租户资源共用机制(网络/存储作为基础设施共用), 以降低支撑单用户的成本。使用用户越多越边际成本越低。</p><p></p><p>云计算厂商所提供的基础平台能力基本上解决了运维稳定性风险，改造周期长，资源使用成本的问题。将整体存算分离改造的复杂度明显降低，可行性提升。三大问题的妥善解决（从技术到成本的核算后），让工程师更多尝试在公共云标准IaaS环境下不断改进大数据存算分离的技术方案，无论是数据湖存储加速还是缓存加速等等。</p><p></p><p>门槛二: 存算分离独立改造的技术债</p><p></p><p>大数据存算分离本身是非常复杂的技术架构改造，特别是本地架构搭建时。其中牵涉到网络改造(带宽/网卡/交换机)，硬件适配(CPU，SSD磁盘)。从网络开销到本地磁盘IOPS的性能测试等等。虽然有公共云大数据存算分离的架构实践可以参考，但因为由于其与私有化技术差异较大，对大数据架构和系统运维提出了极高的要求。当前业内也有相关的商业化公司提供独立部署大数据存算分离的产品方案，其效果有待考量。</p><p></p><p>读写IO性能损耗从新架构到新硬件的适配网络传输的稳定性风险技术改造的不确定性风险架构改造周期长，过渡期运维复杂度</p><p></p><p>门槛三：私有化大数据存算分离改造的额外成本</p><p></p><p>1）新硬件采购成本</p><p></p><p>私有化自持大数据存算分离架构需要定制相关硬件，包括网络设备部署，专线，存储设配等。在加上超高网络带宽改造、超低延迟支撑，整体的采购成本是非常高的（网络部署和使用成本、供电成本)。</p><p></p><p>2）运维复杂度的明显提升</p><p></p><p>之前提到, 存算分离本身是个异常复杂的架构改造工作。之后从改造开始到后续运维的很长一段时间内, 整体系统的稳定性问题和风险是会明显上升的。云计算厂商通过整体IaaS的统一架构布局，屏蔽了相关问题。但私有化部署场景下依然存在。</p><p></p><p>3）大多数硬件无法利旧</p><p></p><p>其可行性存疑，因为真正的存算分离架构对硬件(服务器/网络/交换机)都有比较明确的定制优化需求，常规通用的服务器往往无法满足存算分离的架构要求</p><p></p><p>4）满足监管合规要求</p><p></p><p>在金融行业等强监管行业往往应国家监管合规要求，导致机房物理隔离，网络隔离，其本身给存算分离改造造成了很多困难。这样将产生新的额外成本负担。</p><p></p><p></p><h2>3.3 在哪些数据密集型行业场景下存算分离解决什么业务问题？</h2><p></p><p></p><p>大数据存算分离本身作为一个创新技术解决方案。更多的需要选择合适的数据业务应用场景是支撑好业务价值转换。</p><p></p><p>在互联网行业的蓬勃发展一直有赖于海量多源数据不断的增长, 属于典型的数据密集型行业，通过数字化方式来分析挖掘来实现数据的价值。其对多源线上数据的探查/分析/挖掘有赖于存算分离架构上所构建的多源数据汇聚（统一存储数据湖）和实时离线混合分析能力（支持Hive/Spark/Presto/ES等各类计算引擎）。另外因为互联网行业对大数据SRE稳定性要求较高，存算分离改造后的业务SRE稳定性明显有提升。因此在互联网及泛互联网行业存算分离的需求是清晰而明确的。</p><p></p><p>相比互联网存算分离应用的遍地开花，在非互联网行业对数据密集型场景的应用挖掘需求日趋强烈，因为行业不同，其对大数据架构的主要诉求差异大。其中数据密集型的智能制造（汽车）与消费科技行业因其对各端数据的强烈需求，对处理数据的资源利用率要求高，伴随着存算分离应用增大。而金融行业正处在全面数字化改造的进程中，该业内普遍共识是依托于大数据存算分离等新技术来加速金融科技数字化转型升级。另外金融与互联网的互相借鉴，让金融行业中各类应用数据极速增长，产生了对资源成本优化与运维稳定性的典型诉求。而政企行业大多属于非数据密集型行业（除GA，运营商），对存算分离技术改造需求不算强烈。但随着政府国务院印发的全国一体化政务大数据体系建设指南中的体现，相信很快存算分离架构也将成为政企行业标准。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bb9708b7e70c2604bb81aa88b81b91a.png\" /></p><p>引用自：国务院办公厅关于印发全国一体化政务大数据体系建设指南的通知</p><p></p><p></p><h2>3.4&nbsp;典型客户存算分离需求分析</h2><p></p><p></p><p>以非互联网的金融行业为例，客户典型考虑存算分离为业务提供的整体价值（降本是必要非充分条件）。其中还裹挟着其他非功能性需求，包括监管合规，容灾备份，高可用等。因此实现广大的行业大数据存算分离改造，是非常复杂且庞杂的系统化工程。</p><p></p><p>案例一：某互联网金融公司</p><p></p><p>背景：存算一体架构。离线数仓存储增长261%，计算增长200+%，受限于存算一体，存储和计算容量调整是同比例变化的，对于计算存在一定的浪费。</p><p>主要需求：</p><p>成本高，存不下年年存储100%增长，计算资源增长不大，问题在于如何实现更低成本的存算分离架构改造。稳定性：离线数仓环境，由于存算一体部署共用磁盘资源，当磁盘io被计算资源占满后，导致dn服务不能正常发送心跳而失联，造成hdfs集群不稳定。计算资源效率提升离线数仓每天晚批（0-9）点计算资源不足，而公司内联机业务服务器资源相对空闲，不能充分利用计算资源，存在资源浪费。考虑通过在离线混部来降低成本。金融监管要求不能上公共云, 要求线下存算分离方案灾备与双活</p><p></p><p>存算分离后，容灾备份主要依赖于统一存储的高可用能力与元数据定期备份机制。</p><p></p><p></p><h1>四、内在：大数据存算分离统一能力拆解</h1><p></p><p></p><p></p><h2>4.1 存算分离骨架：计算与存储集群物理分离改造</h2><p></p><p></p><p>可以实现计算和存储资源的单独扩容，然后原本分散的数据实现集中存储，打造统一数据湖。</p><p></p><p>计算存储的分离后，可便利实现计算层产品版本的灵活管理，存储部分求稳，要保持存储层版本稳定。计算部分求快，可以通过数据沙盒和容器技术，实现不同算力模型的快速交付，各部分独立升级互不影响。从湖仓一体到批流一体背后所体现的诉求都可以解决计算与存储不同资源的有效利用，资源错配问题。</p><p></p><p></p><h2>4.2 存算分离底座：存储服务化与异构存储应用</h2><p></p><p></p><p>独立部署HDFS及存在的问题：</p><p></p><p>HDFS独立部署，计算节点可以通过本地IO访问本地DataNode, 或者通过网络传输读取HDFS上的数据。但是HDFS独立部署会存在如下问题：</p><p></p><p>HDFS云上部署成本相对过高(ECS集群)，运维成本高。高可用方案三副本占用存储空间大。HDFS的NameNode 只能 Scale-up（垂直扩展：只能通过提高机器的配置实现扩展，无法通过增加机器数量实现扩展）。文件数量超过5亿只能做 Federation（联邦）,增加了运维管理的成本，也影响使用效率。NameNode只有双机互备方案，高可用性不足。Java GC会影响系统可用性。根据实际运维经验，一般在 3 亿文件以内，运维 HDFS 还是比较轻松的。超过3 亿文件以后运维的复杂度就会明显提升，峰值可能就在 5 亿文件左右，就达到单集群的天花板了(NameNode瓶颈)。文件量更多，需要引入 HDFS 的 Federation 联邦的机制，但是它就增加了很多的运维和管理的成本。</p><p></p><p>存算分离作用：</p><p>存算分离的架构需要对存储底座做选型。传统大数据存储文件系统HDFS默认采用三副本，整体成本更高。HDFS适合存储大文件，更适合Hadoop大数据的结构化/半结构化场景。相比之下，对象存储有以下优势：</p><p></p><p>一）存储优化，采用EC纠删方法，可以实现更高的利用率，低成本且高可用。二）对象存储是服务化的，开箱即用免运维。不用做任何的部署监控运维这些工作，特别省事儿。三）弹性伸缩，企业可以按量付费，不用考虑任何的容量规划，开一个对象存储的 bucket ，有多少数据写多少数据，不用担心写满。</p><p></p><p>另外，随着对象存储的使用更加普遍的趋势, 有越来越多技术方案（例如阿里云JindoCache/开源Alluxio等）来支持海量中小文件处理。这对大数据/非结构化数据分析场景较有用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/29/294f7a3b5e0615f19bc46907890eb664.png\" /></p><p>Figure 4: 各种数据存储模式</p><p></p><p>云端存算分离已成熟:&nbsp;基于对象存储之上的文件系统映射能力应用广泛（JindoFS&amp;JuiceFS/虚拟文件系统Alluxio）。对象存储在并发访问的支持、前端应用的调用等方面，由于接口间接，默认高可用，以及采用成本可控的分布式架构，相比块存储和NAS存储有很大优势，对象存储因此是云端存算分离架构的主流存储底座。</p><p></p><p>云下存算分离探索时:&nbsp;如果没有成熟且低成本的支持存算分离架构的独立存储解决方案。存算分离私有化都无从谈起。线下自持存储集群的成本普遍要高于借助云网络架构来搭建的分布式对象存储方案。</p><p></p><p></p><h2>4.3&nbsp;存算分离算力: 资源池化与统一调度</h2><p></p><p></p><p>大数据资源管理与存储的架构，逐渐从HDFS与YARN紧耦合，演进到存储与资源管理的解耦的形态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c8/c84907727be11532583f01a765b78ff9.png\" /></p><p>Figure 5: Hadoop的资源与存储分离架构</p><p></p><p>通过统一算力资源池实现资源统筹调度，对资源细粒度的优化管理与调度很有帮助</p><p>可以将离线计算与其它在线计算任务进行资源混部达到峰谷互补的效果，有助于提升服务器资源利用率和管理运维效率。可以根据业务优先级来为不同业务计算任务分配对应资源，确保资源间不存在资源抢占。在业务高峰期，以极度弹性扩缩容模式调用算力资源。</p><p></p><p></p><h2>4.4 存算分离下的异构计算：负载混部协同计算和IO之间动态平衡</h2><p></p><p></p><p>存算分离作用：</p><p>彻底解决多种计算引擎（数据仓库+数据湖）工作负载混部与协同的问题。</p><p>在离线混部：支持离线计算与实时计算资源的Seamless切换</p><p>湖仓一体混部离线实时一体混部流批一体混部大数据云上云下混部: 数据存储保留在本地，机器学习等计算资源部署在公有云，既考虑了安全性，又实现了计算的敏捷大数据+AI一体</p><p>从计算引擎层面看, 异构计算混部协同一直是业内共识的提升资源利用率的重要手段之一。</p><p></p><p></p><h2>4.5&nbsp;大数据智能运维: DataOps+MLOps</h2><p></p><p></p><p>存算分离作用：</p><p>热升级：支持原地升级、滚动升级、补丁升级，升级时间缩短弹性伸缩：支持客户自助不停服，弹性扩缩容计算节点，存储节点可按照半年周期扩容。故障解耦：存储故障和计算节点故障解耦，方便故障定位和恢复。信创解耦：计算/存储节点信创适配主要由云厂商(例如阿里云ECS/K8S/存储产品架构)解决，不需要客户自行适配。</p><p></p><p></p><h1>五、应用：大数据存算分离之上的数据湖仓</h1><p></p><p></p><p></p><h2>5.1 数据湖的定义</h2><p></p><p></p><p>数据湖本身的定义相对比较宽泛，当前所指的是狭义层面的数据湖定义。</p><p>统一集中分布式存储系统，包含物理与逻辑数据湖允许统一存放各类原始的结构化数据/半结构化数据/非结构化数据数据湖存储支持服务化能力</p><p>从这个定义来看，把HDFS定义数据湖并不是准确的提法。HDFS主要支持相对结构化数据存储。</p><p></p><p></p><h2>5.2&nbsp;存算分离与数据湖的关系</h2><p></p><p></p><p>关于存算分离与数据湖的关系，主要是要讲清楚现代数据湖的架构设计所需的构成因素。其中包括以下内容：</p><p>存算分离将单体框架分解为同类最佳框架无缝对接跨小型和大型文件/对象数据的性能支持横向可扩展的软件定义云原生解决方案</p><p>由此可以看出存算分离技术属于构建现代数据湖的必要非充分条件。大数据存算分离改造是实现数据湖的第一步。</p><p></p><p></p><h2>5.3 数据湖与数据仓库的对比与关系</h2><p></p><p></p><p>有了数据湖，是不是就不需要数据仓库了？在业务实践过程中发现，数据湖与数据仓库更多的是相辅相成的关系。我们可以看到数据湖与数据仓库虽然应用场景不同，但明显有互相融合的趋势。</p><p></p><p>首先，以下内容数据湖与数据仓库的能力对比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa53186fc347414cbe924bf84bbfc817.png\" /></p><p>Figure 6: 数据湖与数据仓库的对比</p><p></p><p>从以上能力对比可以看到, 数据湖与数据仓库产品设计目标有差异的，数据湖面向更多元更丰富数据生态，数据仓库面向更高效稳定更安全可靠的数据分析场景。</p><p></p><p>其次，我们来看一下数据湖与数据仓库的相互融合的特性。</p><p></p><p>从数据湖到湖仓一体，以数据湖为基石，在湖扩展数据仓库的部分能力,走向LakeHouse。</p><p></p><p>从数据仓库到仓湖一体，以数仓为核心，扩展数据湖的开放性和可扩展性，支持非结构化等原生数据的处理等。</p><p></p><p></p><h1>六、实践；数据湖存算分离实践公共云方案分析</h1><p></p><p></p><p>从以上多个层面的分析可以看到，源于云计算IaaS兴起的存算分离架构。当前存算分离已在公共云场景下被广泛使用，其原因也已在上文所提及到。在此我们来分享基于阿里云公共云架构下的数据湖仓存算分离方案最佳实践。</p><p></p><p></p><h2>6.1 DataLake数据湖存算分离参考架构</h2><p></p><p></p><p>数据湖存算分离的参考架构可以被分解为三个大层面，分别是存储层，数据加速层和计算层。其架构优化路径也是从Botton Top的层次来实现。以下根据数据湖存储/数据湖存算中间件/统一元数据管理/LakeHouse湖格式等多个维度来体系化阐述数据湖的存算分离架构实现参考架构。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/03a1031ebebab803ececae6fdaa0d309.png\" /></p><p>Figure 7: DataLake数据湖存算分离参考架构</p><p></p><p></p><h2>6.2 数据湖分层存储：对象存储OSS服务化</h2><p></p><p></p><p>对象存储在并发访问的支持、前端应用的调用等方面，由于接口间接，默认高可用，以及采用成本可控的分布式架构，相比HDFS文件系统，块存储和NAS存储有很大优势，对象存储因此是云端存算分离架构的主流存储底座。以阿里云OSS为例，建立统一数据湖存储。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ca/ca51dd84d9b7478f8c604d4f6c7038ee.png\" /></p><p>Figure 8: OSS数据湖</p><p></p><p>存算分离优先存储方案-云端对象存储服务化</p><p></p><p>对比企业自建存储的费时费力，阿里云云上对象存储是个性价比更高且行之有效的方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/72/72b62a129416032521bb72316f8dd3d9.png\" /></p><p>Figure 9: 企业存储与对象存储OSS的对比</p><p></p><p>加上基于对象存储之上的分布式文件系统加速映射能力应用广泛（JindoFS&amp;JuiceFS/虚拟文件系统Alluxio）。</p><p></p><p>优势一：OSS冷热分层存储，支持智能存储模式。</p><p>依托OSS分层存储能力，优化存储利用率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/07/07397a7f6235b69723a9448049845178.png\" /></p><p>Figure 10: OSS分层存储</p><p></p><p>以上存储优化方案（30%热数据-缓存集群，全量OSS: 30%热数据+70%冷数据存储的情况），整体存储预估下降20%。如在不使用缓存集群机制的情况下, 客户全量数据可都存放在OSS分层存储内，相比可再减少存储成本10-20%。OSS网络带宽要求较高(100GB/s), 需要对齐本地IOPS的数据传输规模。</p><p></p><p>优势二：面向大数据场景的OSS-HDFS云存储产品，支持HDFS一键上云。</p><p></p><p>是在阿里云上把 JindoFS 全托管服务化的部署形态，开箱即用，无须运维。&nbsp;是在 OSS 上提供了全托管的 HDFS 元数据服务，数据仍然存储在 OSS 上。该服务二进制兼容开源 HDFS，功能全面对齐，实现 HDFS 数据无缝迁移。适用场景与限制Hive/Spark 大数据分析、ETL 计算HBase、Flink 替代 HDFSHDFS 重度依赖，上云平迁AI 生态，充分 POSIX 支持</p><p></p><p></p><h2>6.3 数据湖元数据管理DataLakeFormation</h2><p></p><p></p><p>无论是从客户视角来看，还是从存算分离架构的需求来看，都需要在基于统一存储之上提供独立元数据管理体系，来支持多种计算引擎的元数据信息，来管理数据湖上结构化与非结构化的数据存储。在过去的业务实践中，有不止一次金融客户愿意采用类似Aliyun DataLakeFormation的数据湖元数据管理产品，用来补全数据湖存算分离架构的最后一块拼图。但是由于投入与人力受限，很难做到完善成熟的产品商业化输出。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c250d7ac4afbaa96f48a299e1339ac3.png\" /></p><p>Figure 11: DataLakeFormation元数据管理</p><p></p><p></p><h2>6.4 数据湖分布式存储加速层</h2><p></p><p></p><p>在存算分离架构下，计算集群与存储集群的分离会造成网络间数据交换的成本激增。在此情况下，存算中间层就变得尤为重要。存算中间层是要解决统一数据访问+通用存储计算加速的核心需求上。</p><p></p><p>以阿里云数据湖存储加速产品JindoData为例：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/31/31a3150ade010b02c30a98166ffd616d.png\" /></p><p>Figure 12: JindoData数据湖存储加速产品</p><p></p><p>JindoData作为阿里云领先的数据湖优化产品系列。支持分布式数据编排与文件系统体系建设。针对原先的JindoFS架构整体升级，将两个模式(Block模式与Cache模式)加以拆分，一分为二，分别定义为JindoFS分布式文件存储系统与JindoCache缓存加速引擎系统，新升级的JindoFS专注打造下一代数据湖存储文件系统，而分布式缓存加速的功能则交给 JindoCahce加速系统，两者松耦合同时也紧密协作。新的 JindoFS 作为统一文件系统，将 HDFS 兼容和功能对齐作为核心目标，把 HDFS 重度用户和头部用户的上云平迁作为核心考虑要素，并着重解决云原生数据湖场景跨产品打通访问的痛点。目前新版本 JindoFS 已经完成服务化部署嵌入到 OSS 产品，推出全托管 OSS-HDFS 服务，并上线开服使用。本文系统阐述我们为什么把 JindoFS 打造成为下一代HDFS，而且是云时代更好的云上HDFS。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e534cad64ef20e27415b378cd55abbb1.png\" /></p><p>Figure 13: JindoCache缓存加速组件</p><p></p><p></p><h2>6.5 LakeHouse湖格式TableFormat</h2><p></p><p></p><p>Lakehouse = 云上统一对象存储 + 湖格式 + 统一湖管理平台</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/851767a546600a3f73ace5ba2b9a38e6.png\" /></p><p>Figure 14: LakeHouse湖仓一体</p><p></p><p>开源湖格式TableFormat: Hudi / Iceberg / DeltaLake﻿</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2065bc1fdcc008fbb04b7e4e9f4ccbd.png\" /></p><p>Figure 15: LakeHouse湖仓一体-湖格式</p><p></p><p>阿里云开源大数据团队的LakeHouse实践应用</p><p></p><p>Flink+Iceberg技术日趋成熟Flink CDC实时入湖，ODS层的实时化Flink CDC分库分表合并，增量+全量支持批流一体的写入和读取Spark+Hudi/DeltaLake实践经验丰富。DeltaLake2.0值得研究应用创新方案: Flink+TableStore面向流计算的湖格式当前湖格式能力对比分析</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e6cedb33edff23432a8960c8ed4b5d8c.png\" /></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cbd8a0bf6bfc3f893e8eda02f32836ee.png\" /></p><p></p><p></p><h2>6.6 统一资源调度 — 从YARN到K8s</h2><p></p><p></p><p>随着数据湖存算分离不断深入, 围绕基于云原生架构下来建立统一容器化资源调度系统成为数据湖存算分离发展的必要组件。为大数据与AI一体化架构提供统一资源池化与在离线混部的基础支撑。</p><p></p><p>全局架构统一：多种资源管理集群整合到统一调度架构，多业务资源池升级到统一资源池架构。全局调度打通：在不同业务之间实现在离线混部，支持在一个机房内，应用可跨多个k8s集群部署。全局额度打通：多业务之间应用额度可全局管理，确保全局范围内不超额，同时可实现额度相互共享与转移。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/4590463b058394363eaee4c38cfcd45d.png\" /></p><p>Figure 16: 统一资源调度参考</p><p></p><p></p><h2>6.7 数据湖存算分离公共云用例</h2><p></p><p></p><p>数据湖存算分离的公共云应用实践主要是对外面向用户的提供更开放与更灵活的大数据分析能力, 支撑不同形态计算引擎能力与开放存储多源且不同结构的数据。以下数据湖存算分离产品体系是基于阿里云计算平台多年经验所沉淀的大数据应用最佳实践，来打造的数据湖湖仓一体的最佳实践。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/485fcbef80b157a9b0f36ad5467521c1.png\" /></p><p>Figure 17: 数据湖存算分离用例</p><p></p><p>数据湖存算分离方案优势</p><p></p><p>围绕数据湖体系建立完备的标准云上大数据存算分离产品体系。数据湖存储加速套件JindoData(分布式文件系统+缓存加速)。支持OSS对象存储服务化。统一元数据管理DLF对齐AWS Glue，支持数据分层存储的全生命周期管理。[开源] EMR弹性底座升级-太昊平台。极致弹性伸缩，多硬件适配，异构计算混部，数据开发平台对接。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ced5da2d37c686b7a119de9bff70f549.png\" /></p><p>Figure 18: EMR 2.0 分业务场景的开源大数据集群类型</p><p></p><p>[开源] EMR Doctor大数据医生。[自研] ODPS-MaxCompute/Hologres基于OSS数据湖上的即席查询分析。EMR+DataWorks统一数据开发平台，EMR+全托管Notebook。</p><p></p><p></p><h1>七、对比：数据湖与ServerlessDW存算分离方案对比</h1><p></p><p></p><p></p><h2>7.1&nbsp;ServerlessDW的存算分离架构特征</h2><p></p><p></p><p>ServerlessDataWarehouse云数据仓库产品是面向云计算发展方向的解决极致扩展性和弹性伸缩，自动化运维(内置高可用和容错能力)的高性价比方案(Cloud Ground Up)。云数据仓库产品的存算分离架构应用需求与数据湖的开放存算分离应用有所不同。因为云数据仓库的无服务器化/免运维特性, 用户直接以Serverless服务化方式来使用数据分析工作, 对基础架构无感。其存算分离更多是对内解决海量数据增长后，如何对系统持续提升各类资源利用率的需求。例如阿里云云原生数据仓库ODPS-MaxCompute存算分离架构下可以在各种业务场景和负载条件下可以实现计算水位90%+、存储水位80%+。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1d922a4edeb8b99e7e79b0bf6737c082.png\" /></p><p>Figure 19: 云端数据仓库的不同产品架构分类</p><p></p><p>对比当前业内最典型的ServerlessCloudDW产品形态：</p><p>ODPS-MaxCompute、Google BigQuery、Azure SQL Data Warehouse有三个重点的应用内容。</p><p>提供数仓存储能力(弹性扩展)，内置存算分离架构支持离线ELT，ServerlessELT近实时分析，支持大规模复杂聚合分析</p><p></p><p></p><h2>7.2&nbsp;ServerlessDW的优缺点 — ODPS-MaxCompute</h2><p></p><p></p><p></p><h3>优点</h3><p></p><p></p><p>1）全托管模式，节省运营/运维费用</p><p>云数据仓库可以让将麻烦的管理任务外包给必须满足服务等级协议的云服务商。这样可以节省运营费用，并使企业内部团队专注于增长计划。</p><p></p><p>2）SLA保证，比本地数据仓库更好的正常运行时间</p><p>云服务商有义务满足服务等级协议 (SLA)，并通过可顺畅扩缩的可靠云基础架构提供更好的正常运行时间。本地数据仓库具有规模和资源限制，可能会影响性能。</p><p></p><p>3）可灵活弹性扩缩</p><p>云数据仓库具有弹性，因此可以根据企业的业务需求变化顺畅扩容或缩减。</p><p></p><p>4）按资源需求付费，价格灵活，成本效益高</p><p>借助云，可以采用灵活的价格模式，选择根据用量付费或使用更可预测的统一费率方案。有些服务商按吞吐量或每节点每小时收费，有些服务商则对一定量的资源按固定价格收费。不管采用哪些模式，都可以避免为每周 7 天、每天 24 小时不间断运行的本地数据仓库支付巨额费用，无论是否有在使用资源。</p><p></p><p>5）近实时分析</p><p>云数据仓库支持流式数据，允许近实时查询数据，以便快速做出明智的业务决策。</p><p></p><p>6）机器学习和 AI 计划</p><p>客户可以快速发掘机器学习的潜力并应用于相关使用场景，以预测业务成果。</p><p></p><p></p><h3>缺点</h3><p></p><p></p><p>1）自研数仓依赖生态开放，缺少与第三方大数据生态产品整合对接</p><p></p><p>2）ServerlessDW仍有部分运维工作需用户参与，对用户自运维能力有一定要求。</p><p></p><p></p><h2>7.3&nbsp;云原生数据仓库ODPS-MaxCompute的存算分离优势</h2><p></p><p></p><p>ServerlessDW对比开源存算分离的核心优势</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a33af717f51142dd5546c5863443c853.png\" /></p><p></p><p>MaxCompute产品优势</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da46debf384b785ddff807ffa019b621.png\" /></p><p></p><p>﻿﻿高安全与稳定性实时离线一体架构演进增量计算-流式Tunnel写入增量计算-Schema Evolution增量计算-增量更新ACID2.0 --进行中MCQA-MaxCompute ServiceMode查询加速优化MaxCompute开放生态扩展能力 - StorageService开放存储</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/2535d141bc60e626cfe75daded08062d.png\" /></p><p></p><p>﻿﻿ODPS仓湖一体-MCQA+DLF+OSS / Hologres+DLF+OSS即席查询分析能力</p><p></p><p></p><h2>7.4&nbsp;云原生数仓与开源数据湖的能力对比</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a282e4d301d7f5c955e65edb69576281.png\" /></p><p></p><p>从以上的产品架构对比可以作为参考，无法完全一一对应。但从侧面可以看到，大数据存算分离技术领域已逐步走向成熟和融合的趋势。无论是数据湖还是数据仓库架构技术。EMR2.0云原生数据湖发展如火如荼，逐渐成为行业开源标准。而ODPS-MaxCompute云原生数仓正在向下一代云原生存算分离数仓演进。从存算分离1.0的物理存算分离，到存储分离2.0的仓湖一体，再到正在进行中存算分离3.0-统一调度/资源混部等架构进化。后续将专项技术产品深度分享。</p><p></p><p></p><h1>八、总结：大数据存算分离方案总结</h1><p></p><p></p><p>云+大数据+存算分离一直是当今数据领域的热点话题。在计算正向轻量化/容器化/AI融合方向发展的当下，计算存储分离架构演进随着云计算的普及已成为事实标准。大数据存储分离后，无论是使用云计算的低成本对象存储还是用企业级的存储底座，替代原来的原生大数据存储架构，把各种云原生的先进技术(云端高性价比存储方案/高性能网络/服务化数据管理)加之云端存算加速技术带入到大数据系统里面来，将大数据架构升级全新的领域，支持包括高可靠、智能运维、高利用率、多协议融合等能力，再加之更加弹性扩缩容且精细化的资源管理体系。更好地释放新一代大数据体系的核心价值。</p><p></p><p>以下是部分大数据存算分离的整体化思考与总括，与大家共同探讨。</p><p></p><p>大数据存算分离因其复杂度高，运维难度大。其整体方案可行性需与业务价值挂钩。大数据上云与存算分离紧密相关，有了云计算，才让大数据存算分离真正走向商业化。大数据存算分离改造，公共云架构已日趋成熟，成为行业标准。专有云大数据存算分离因其本地化架构相对复杂，在持续演进与优化中。无论是湖仓一体，流批一体，实时离线一体，都需要找到精准的数据应用场景才能有价值。否则皆是空谈。大数据存算分离的技术核心主要来自于统一存储(分层存储)/统一资源调度(混部)/在线离线计算引擎协同的建设。其他皆是围绕其展开的。大数据存算分离的产品表现形态各有差异，不能一概而论。开放生态的数据湖存算分离与Serverless稳定高效的云数据仓库存算分离，其产品能力各有千秋。随着数据湖与数据仓库技术融合趋势凸显，数据湖仓的技术整合会越来越被关注。</p><p></p><p></p><h2>参考阅读</h2><p></p><p></p><p>[01] https://mp.weixin.qq.com/s/d7MC-9kyawOUMr-PVSFWXg﻿</p><p>[02] https://www.sohu.com/a/587783648_355140﻿﻿</p><p>[03] https://www.stitchdata.com/resources/google-bigquery/﻿</p><p>[04] ﻿﻿https://www.serverless.com/blog/things-consider-building-serverless-data-warehouse﻿</p><p>[05] https://xie.infoq.cn/article/24f184b066c50a6d15c23f3ff﻿</p><p>[06] https://cloud.google.com/learn/what-is-a-data-warehouse</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44edc3723fccd7abc1f8217902f3faf9.png\" /></p><p></p>",
    "publish_time": "2023-03-28 11:13:31",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "什么才是开发者友好的HTAP？",
    "url": "https://www.infoq.cn/article/wpmZIQL4kUv73fW24vzq",
    "summary": "<p>OLTP 和 OLAP 技术栈曾因大数据系统规模问题分道扬镳。但是随数字化技术落地到千行百业，复杂的业务场景产生海量数据，即使是相对成熟的 OLTP 产品也无法满足业务对于实时分析的需求，于是 OLAP 技术快速发展，至 Gartner 正式提出了<a href=\"https://xie.infoq.cn/article/77c4366f13b3fcba53bdce7c4\"> HTAP</a>\" 这一概念，到如今呈现出 HTAP 理念大火的局面。</p><p></p><p>虽然 HTAP 概念已经普及很长时间，但是业内对于 HTAP 的理解不同，国内几款代表性的 HTAP 数据库技术路线也不尽相同。开发者应该如何理解 HTAP？什么才是开发者友好的 HTAP？想要了解什么才是开发者友好的 HTAP，不如先来看看开发者们对于<a href=\"https://s.geekbang.org/search/c=0/k=%E6%95%B0%E6%8D%AE%E5%BA%93/t=\">数据库</a>\"的偏好。</p><p></p><h1>开发者需要怎样的数据库？</h1><p></p><p>在 <a href=\"https://www.oceanbase.com/devcon2023?utm_source=infoq&amp;utm_medium=pc&amp;utm_campaign=dc2023&amp;utm_term=homebanner\">OceanBase 开发者大会</a>\"上，OceanBase CTO 杨传辉表示，开发者的首要需求是稳定可靠。“或许开发者第一反应是代码要好，写起来比较简单，但是如果你去追问，一定会得到‘首先不能有故障’的答案。”其次是多云原生。在多云、混合云的大趋势下，云原生数据库如今更能满足开发者需求。沃趣科技创始人兼 CEO 陈栋也表示：“多数据库选择、多云部署趋势特别明显。”</p><p></p><p>再次是一体化。目前国内数据库有集中式和分布式两大技术路线，每条技术路线各有优劣，虽然分布式数据库目前还未占据主导地位，但其对于业务场景的适配能力得到了普遍认同。整体来看，国内企业整体对于分布式数据库的需求依然旺盛，开发者对于分布式数据库的热情不减。在数据库融合的大趋势下，杨传辉判断，国内集中式和分布式融合的需求是存在的，开发者也就自然需要关注集中式分布式数据库融合的问题。“最早的 OceanBase 的一体化架构叫做‘集中式分布式一体化’，当时我们认为 DBA 更加熟悉集中式这个说法。不过，市场品牌的负责人建议修改成 ‘<a href=\"https://xie.infoq.cn/article/35f36a65c8515581f7b1e6bc8\">单机分布式一体化</a>\"’，这样会更加形象直观，能够更好地表达 OceanBase 的技术特点，开发者也更容易理解。很多开发者来自使用分布式数据库的公司，不仅希望分布式简单，还希望不会因为数据库能力不足而做很多业务的改造。OceanBase 收到了很多融合的需求。”他谈道。</p><p></p><p>最后是用户体验，降低技术产品的使用门槛是保证用户体验重要的一环。“从入门到劝退、还是从入门到精通，其实就是用户体验应该解决的问题。”杨传辉表示。</p><p></p><p>dbaplus 社群联合创始人杨建荣近期在社区内做了一项开发者对于分布式数据库需求的调研，结果显示，开发者对于分布式数据库选型依次比较看重稳定性、成本（硬件成本和研发接入成本）、易用性和对主流技术栈的兼容。PostgreSQL 中文社区主席张文升观察到，业务越来越复杂，企业开始关注在不同细分场景里是否有更好用的数据库功能或数据库，导致企业采用越来越多的数据库可能成为一种常态。</p><p></p><p>但是从开发者或者企业的角度来说，采用的数据库越来越多并不是一件好事。技术人员对不同数据库的学习成本居高不下，企业对于多种类数据库全生命周期的管理也是一个难题。不完全统计显示，国内数据库公司超过 300 家，技术产品各有千秋。所以，企业选用的数据库产品固定在少数几家或许才是未来数据库在企业内部的常态。沃趣科技创始人兼 CEO 陈栋表示：“我们对接的头部券商基本上都会说希望数据库类型最终控制在 10 种以内。dbaplus 社群联合创始人杨建荣判断，这个数量可能还会缩减至 5-6 种。</p><p></p><p>如今我们可以看到的是，稳定可靠、简单易用、多云原生、可扩展、用户体验佳、计费方式更加弹性的数据库产品将更受开发者欢迎。HTAP 作为数据库融合的方案之一，广受关注的原因之一就是简化了企业对于数据库的使用。</p><p></p><h1>什么才是开发者友好的 HTAP？</h1><p></p><p>如今，OLTP 技术已经比较成熟，但是成本也比较高，所以很多企业开始把 OLTP 数据库慢慢转向 OLAP 对外提供服务。新一代云原生 OLAP 正在替代传统大数据项目，让大数据项目朝着更简单、更好用、高性能的方向发展。</p><p></p><p>HTAP 是数据库融合的一种方案，把事务处理和分析处理都集中在一个系统中对外提供服务，目前已经成为 OLTP 方向数据库正在追求的一个重要方向。国内的 OceanBase、PolarDB、TDSQL-H、TiDB 等都实现了 HTAP。在过去很长一段时间内，似乎是否支持 HTAP 成为数据库是否“先进”的隐形指标。</p><p></p><p>不过在杨传辉看来，如今业内对于 HATP 甚至 AP 这个词都没有明确的定义。他将 HTAP 分成三类：第一类做并集，TP 也行，AP 也行；第二类做交集，本身的 TP 能力和 AP 能力单独去看都是比较弱的，交集在一起能去覆盖一些场景；第三类就是 OceanBase 自然衍生的“OLTP plus”思路，可以理解为是 OLTP 加上 OLAP 形成 HTAP 这个系统应用的核心。</p><p></p><p>在 dbaplus 社群联合创始人杨建荣看来，HTAP 的底层设计还是读写分离，需求也是读写分离，更多需要解决读的场景下的问题。在他看来，原生对于对 HTAP 要求会更高一点，具备 TP 的能力，从 AP 的场景入手，到 TP 的切换会更容易。</p><p></p><p>“（现在出现）很多不同的路径，从 OceanBase 的角度来讲，在一个单机上甚至上在某一个页面上进行优化，有一部分列存，有一部分行存。怎么调它都有成本，你怎么搬过来搬过去都有成本。怎么去找到平衡点，我认为是最重要的。”中达金桥资深数据库专家卢东明表示。</p><p></p><p>在陈栋看来，\"Oracle 也是这样的思路，把 TP 做到极致，同时拥有部分 AP 能力、离线能力。OceanBase 后续的 roadmap 非常巧妙，拿出一个副本做列存，既升节空间，又做到数据的一致性，不需要单独设计一个表格，我十分期待这个功能\"，HTAP 最终是为了解决混合场景的问题，解决这个问题可以用多表查询、大表关联的方式来实现，也可以用列寸的方式实现，还可以纯粹依赖硬件红利。“因为数据库运行是平衡的系统，从计算到网络到存储到 IO 到数据库内核，就看哪个地方最高性价比可以解决问题。”</p><p></p><p>仅从运维的角度来看，杨传辉认为开发者友好的 HTAP，“一定是让开发者觉得越来越简单，比如最好是在公有云上都不用怎么去部署。”</p><p></p><p>杨传辉对于 OceanBase 在走的 HTAP 路线非常坚定。他表示，“从开发者视角来看，HTAP 只有从 TP 开始才有可能成功，TP 的门槛很高，而且 TP 会涉及到核心场景，如果核心场景做好了的话，往 AP 是自然延伸，相当于由高打低，如果 AP 做好了，本身 AP 的重要程度、核心程度远远低于 TP，开发者不会因为选了 AP 而选 TP，但是可能因为选了 某个数据库 的 TP 也选了 他的 AP。”</p><p></p><p>“用心做好最难的、用户最需要的事情”的商业理念应用到技术里依然有效，这个层面上，Oracle 和 OceanBase 的思路相同，都是希望将 OLTP 能力做到极致，再抽取打磨出部分 OLAP 的能力。国内大量复杂的查询业务和高并发频现的运营商业务场景，为打磨数据库尤其是 HTAP 提供了土壤。杨传辉谈认为，“如果满足不了开发者对应用的核心 OLTP 能力，那种 HTAP 不是真正的 HTAP。”</p>",
    "publish_time": "2023-03-28 14:55:21",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT 出现重大 Bug、7天还未完全修复， OpenAI 直指 Redis 开源库错误导致",
    "url": "https://www.infoq.cn/article/uZFepbwFzA9uoHEiRLQR",
    "summary": "<p>&nbsp;</p><p>近日，许多 ChatGPT 用户称，自己在使用 ChatGPT时看到其他人的聊天查询列出现在了自己的历史记录中。</p><p>&nbsp;</p><p>“这个应用程序正在向我显示其他人的聊天记录和内容。我没有输入任何这些提示或问题。”有推特网友称。这意味着，完全陌生的人可以使用不同的帐户查看别人的历史记录，而无需执行任何不同的操作。另外还有一些用户称自己看不到完整的聊天记录，但是可以看到对话标题。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/5634960981b20d9da0fbd6b9a2408dbd.png\" /></p><p></p><p>&nbsp;</p><p>该问题最早是在3月20日发现。不久后，OpenAI 将 ChatGPT 下线并调查问题，但没有立即提供导致中断的详细信息。</p><p>&nbsp;</p><p>OpenAI首席执行官Sam Altman 在3月23日在推特上致歉，“由于开源库中的错误，我们在 ChatGPT 中遇到了一个重大问题，现在已经发布了修复程序，我们刚刚完成验证。一小部分用户能够看到其他用户对话历史的标题。我们对此感到很抱歉。”</p><p>&nbsp;</p><p>次日，OpenAI 正式发布声明解释了该问题发生的原因。OpenAI 称这个错误是由Redis 开源库中的一个错误导致的。如果两个用户差不多同时活跃，那么新创建对话的第一条消息也可能在另一个用户的聊天记录中可见。</p><p>&nbsp;</p><p>另外，这个Bug 可能导致了 1.2% ChatGPT Plus用户的支付相关信息在无意中被泄露。一些用户可以看到另一个活跃用户的姓名、电子邮件地址、支付地址、信用卡号码的后四位数字(仅限)和信用卡到期日期。OpenAI 强调，完整的信用卡号码在任何时候都没有被曝光。</p><p>&nbsp;</p><p>在声明中，OpenAI 表示“该错误现已修补”。但根据软件安全公司Sonatype的说法，尽管 Redis 在 4.5.3 版本和一些反向移植中发布了修复程序，但测试人员仍然能够重现该问题，因此认定其还未修复Bug。</p><p>&nbsp;</p><p></p><h2>到底发生了什么？</h2><p></p><p>&nbsp;</p><p>根据 OpenAI 的说法，这个错误是在 Redis 客户端开源库 redis-py 中发现的。OpenAI 发现该错误后联系了 Redis 维护者并提供了一个补丁来解决这个问题。</p><p>&nbsp;</p><p>OpenAI 在服务器中使用 Redis 缓存用户信息，因此不需要为每个请求检查数据库。&nbsp;OpenAI 使用 Redis Cluster 将此负载分布到多个 Redis 实例上，并使用 redis-py 库来连接 Python 服务器中的 Redis ，该服务器使用Asyncio运行。这个库在服务器和集群之间维护一个共享连接池，并在完成后回收连接以用于处理另一个请求。</p><p>&nbsp;</p><p>当使用Asyncio时，redis-py的请求和响应表现为两个队列：调用者将请求推送到传入队列，然后从传出队列弹出响应，然后将连接返回到池中。</p><p>&nbsp;</p><p>如果被推送到传入队列后请求被取消，但在响应从传出队列弹出之前可以看到一个Bug：连接因此损坏，并且为无关请求退出队列的下一个响应可以接收到留在连接中的数据。</p><p>&nbsp;</p><p>在大多数情况下，这会导致不可恢复的服务器错误，用户将不得不再次尝试进行请求。&nbsp;但在某些情况下，损坏的数据恰好与请求者期望的数据类型相匹配，因此从缓存中返回的数据看起来是有效的，即使它属于另一个用户。</p><p>&nbsp;</p><p>在太平洋时间 3 月 20 日星期一凌晨 1 点，OpenAI 团队无意中对服务器进行了更改，导致 Redis 请求取消数量激增，也使得每个连接返回错误数据的概率很小。</p><p>&nbsp;</p><p>OpenAI 表示，此错误仅出现在 Redis Cluster 的 Asyncio redis-py 客户端中，现已修复。事故发生后，OpenAI 为改进系统采取了以下措施：</p><p>&nbsp;</p><p>对潜在Bug进行了大规模测试和修复。添加了冗余检查，以确保 Redis 缓存返回的数据与请求用户匹配。以编程方式检查了日志，以确保所有消息仅对正确的用户可用。关联了多个数据源来准确识别受影响的用户，以便通知相关用户。改进了日志记录，用以识别何时发生并完全确认它已停止。提高了 Redis 集群的稳健性和规模，减少在极端负载情况下出现连接错误的可能性。</p><p>&nbsp;</p><p>官方声称修复Bug后，安全研究员 Gal Nagli 在<a href=\"https://twitter.com/naglinagli/status/1639343866313601024\">推特上补充称</a>\"，每当用户登录 ChatGPT，OpenAI 的应用程序都会从​​服务器获取用户的帐户上下文，如电子邮件、名称、图像和 accessToken，如下图所示：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/16bc82943eefa70a840b2bd0857b0471.png\" /></p><p></p><p>&nbsp;</p><p>Nagli 表示，“在高级视图中，该漏洞非常简单，如果设法强制负载均衡器将请求缓存在特定路径上，我们将能够从缓存的响应中读取受害者的敏感数据。在这种情况下，漏洞不会直接发生。为了让漏洞发挥作用，我们需要让 CF-Cache-Status 响应以确认缓存的‘HIT’，这意味着它缓存了数据，并将提供给跨同一区域的下一个请求。我们收到‘动态’响应，不会缓存数据。”</p><p>&nbsp;</p><p>Nagli 称，要实现这一点，首先要创建一个特别设计的链接，将.CSS资源附加到 \"chat.openai[.]com/api/auth/session/\"中，并诱骗受害者点击链接，这会让包含accessToken字符串的JSON对象被缓存在Cloudflare的CDN中。对CSS资源(CF-Cache-Status头值设置为HIT)的缓存响应然后被攻击者滥用，以获取目标的JSON Web Token (JWT)凭据并接管帐户。</p><p>&nbsp;</p><p>Nagli说道，OpenAI负责任地在披露后两小时内修复了这个Bug，表明了问题的严重性。</p><p>&nbsp;</p><p>Sonatype认为，这背后是Redis 的并发竞争问题。 Redis 团队也是对AsyncIO 竞争条件（#2624、#2579）进行了紧急修复，但问题并没有完全解决。</p><p>&nbsp;</p><p></p><h2>开源软件担责吗？</h2><p></p><p>&nbsp;</p><p>对于OpenAI 给出的解释，有开发者表示，“将责任推给开源可不是个好主意。”</p><p>&nbsp;</p><p>“将闭源产品的错误归咎于开源库是不公平的。MIT 许可的依赖项明确表示没有任何保证。毕竟，在 ChatGPT 施加压力之前，该错误并未引起注意，而且是 ChatGPT 未能在其发布前的 QA 测试中排除该错误。”网友“abujazar”说道。</p><p>&nbsp;</p><p>abujazar认为，考虑到 OpenAI 本身在尽可能地封闭源代码，他们在声明的第一行中提到开源软件是他们中断的原因似乎有些不合适。“我认为开源作为生命的开场白，而对 Redis 团队的致谢出现在最后一行并不是巧合。许多软件工程以外的人可能会将此解读为‘开源导致 OpenAI 崩溃’。”</p><p>&nbsp;</p><p>注：OpenAI 在声明的最后写道：Redis 开源维护者是出色的合作者，他们迅速解决了错误并推出了补丁。Redis 和其他开源软件在我们的研究工作中发挥着至关重要的作用。它们的重要性不可低估——如果没有 Redis，我们将无法扩展 ChatGPT。我们致力于不断支持和贡献 Redis 社区。</p><p>&nbsp;</p><p>“如果 OpenAI 试图让 Redis 承担潜在的法律责任，我会非常愤怒。”有网友表示。</p><p>&nbsp;</p><p>网友“YPPH”表示，“如果有人要求 ChatGPT 生成一些代码，然后不假思索地将其复制并粘贴到他们的项目中，我想知道 OpenAI 会如何看待这种说法：该错误是 ChatGPT 生成的错误代码造成的。”</p><p>&nbsp;</p><p>不过也有一些网友表示，OpenAI 并没有责怪任何人，他们只是客观地表明了是那个库中的一个错误导致了问题。</p><p>&nbsp;</p><p>开发者“random_cynic”表示，“讽刺的是，这种疯狂的言论对开源开发者的形象造成了比OpenAI 或任何新闻稿更大的伤害。提到导致Bug的开源库是很重要的，因为许多其他使用它的应用程序可能也会发生这种情况。这基本上是开源的要点之一。</p><p>&nbsp;</p><p></p><h2>OpenAI，道阻且长</h2><p></p><p>&nbsp;</p><p>这次事件也引发了其他用户吐槽此前遇到的Bug。“这让我想起了我遇到的第一个 Bug：通过 yahoo messenger 向自己发送一个  标签，你会随机得到一个从其他人和它的目标用户发回给你的消息对话。”网友“<a href=\"https://twitter.com/hackerfantastic/status/1637829796909416448\">Greg Linares (Mantis)</a>\"”表示。</p><p>&nbsp;</p><p>“我有那个错误的变体，它在 0x45 (iirc) 的协议处理程序中允许用户注入格式错误的字符，并且会从消息流中泄漏，发生一次就为其他用户发送一条消息。”有网友表示，当其使用提示写出一些 React 代码时，已经发生了几次这样的错误。“它一直在提示超时，然后突然间我看到了其他人的提示。它们每次都不一样。”</p><p>&nbsp;</p><p>Cyber​​haven 首席执行官 Howard Ting 表示，“数据从本地迁移到云端，下一个重大转变将是将数据迁移到这些生成的应用程序中。”&nbsp;这个转变中的数据安全问题也越来越引起人们的重视。</p><p>&nbsp;</p><p>员工越来越多地将敏感的业务数据和隐私信息提交给 ChatGPT，比如有高管将公司的 2023 年战略文件剪切并粘贴到 ChatGPT 中，并要求它创建一个 PowerPoint 幻灯片。但OpenAI是否会将这些数据整合到模型中还没有明确的表示。</p><p>&nbsp;</p><p>在最近的一份报告中，数据安全服务 Cyber​​haven 检测到并阻止了其客户公司 160 万名员工中 4.2% 的人将数据输入 ChatGPT 的请求，因为存在泄露机密信息、客户数据、源代码或监管信息的风险。目前，摩根大通已经<a href=\"https://www.cnn.com/2023/02/22/tech/jpmorgan-chatgpt-employees/index.html\">限制员工使用 ChatGPT</a>\"，亚马逊、微软和沃尔玛也已<a href=\"https://www.businessinsider.com/walmart-warns-workers-dont-share-sensitive-information-chatgpt-generative-ai-2023-2\">向员工发出警告</a>\"，要求员工谨慎使用生成式 AI 服务。</p><p>&nbsp;</p><p>但上述问题并没有影响 OpenAI 的发展。不久前，OpenAI <a href=\"https://openai.com/blog/chatgpt-plugins\">宣布正式上线</a>\"了以安全为核心的 ChatGPT 插件系统，ChatGPT 可以连接到第三方应用程序，与开发人员定义的 API 进行交互，从而增强自己的功能并允许其执行范围广泛的操作。</p><p>&nbsp;</p><p>随着 OpenAI 生态的不断发展和扩大，OpenAI 必须做好应用安全运行的各方面工作。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://news.ycombinator.com/item?id=35267569#35270165\">https://news.ycombinator.com/item?id=35267569#35270165</a>\"</p><p><a href=\"https://blog.sonatype.com/openai-data-leak-and-redis-race-condition-vulnerability-that-remains-unfixed\">https://blog.sonatype.com/openai-data-leak-and-redis-race-condition-vulnerability-that-remains-unfixed</a>\"</p><p><a href=\"https://www.bleepingcomputer.com/news/security/openai-chatgpt-payment-data-leak-caused-by-open-source-bug/\">https://www.bleepingcomputer.com/news/security/openai-chatgpt-payment-data-leak-caused-by-open-source-bug/</a>\"</p><p><a href=\"https://openai.com/blog/march-20-chatgpt-outage#technical-details\">https://openai.com/blog/march-20-chatgpt-outage#technical-details</a>\"</p><p><a href=\"https://www.darkreading.com/risk/employees-feeding-sensitive-business-data-chatgpt-raising-security-fears\">https://www.darkreading.com/risk/employees-feeding-sensitive-business-data-chatgpt-raising-security-fears</a>\"</p>",
    "publish_time": "2023-03-28 15:27:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "提升字节规模化效能的平台化思路 ｜InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/IMEjWQl3zS66Eb6uCOXL",
    "summary": "<p>大规模微服务实践条件下，字节是如何通过平台工程协同数万研发人员的？字节内部开发者平台是如何演进的？传统意义上的运维岗位，未来会被平台替代掉？平台工程也会危及 DevOps 相关联的其它岗位吗？</p>",
    "publish_time": "2023-03-28 15:54:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "马斯克对 Twitter 估值仅 200 亿美元，接管半年较收购价腰斩，网友：什么都不做，赔钱最慢",
    "url": "https://www.infoq.cn/article/Sktl2hsqUBygTlDGRzOI",
    "summary": "<p>近日，根据Platformer和The Information的报道，埃隆·马斯克为Twitter估值为 200 亿美元，不及他当时收购该网站支付的 440 亿美元的一半。</p><p></p><h2>收购Twitter不足半年，市值跌去一半</h2><p></p><p>&nbsp;</p><p>这位亿万富豪去年10月刚刚以440亿美元<a href=\"https://www.infoq.cn/article/LxFCa6vY353SjMTw8qpL\">收购了社交媒体Twitter</a>\"，并将其转为私营。</p><p>&nbsp;</p><p>在给Twitter员工的邮件中，马斯克警告称Twitter公司的财务状况仍不稳定，距离资金耗尽一度只有四个月时间。</p><p>&nbsp;</p><p>伊隆·马斯克在上周五发给公司员工的一封电子邮件中写道，Twitter目前的价值约为200亿美元，已经远低于去年10月他出手收购时的440亿美元。</p><p>&nbsp;</p><p>这封发给员工的邮件是要公布一项新的股权补偿计划。<a href=\"https://www.infoq.cn/article/GV2p2VQyx4G2yoQmqJPZ\">马斯克</a>\"借此向员工发出警告，称Twitter的财务状况仍不稳定，距离资金耗尽一度只有四个月时间。他表示为了避免<a href=\"https://www.infoq.cn/article/GV2p2VQyx4G2yoQmqJPZ\">破产</a>\"和精简运营，公司必须开展“彻底变革”，包括大规模裁员和削减成本。</p><p>&nbsp;</p><p>马斯克写道，“Twitter正在迅速重组”，并表示希望将其逆向塑造成“一家初创公司”。</p><p>&nbsp;</p><p>随着马斯克对公司的全面改革，Twitter的价值也在一路走低。去年10月，马斯克将Twitter转回私营，意味着其不再有对外披露财务状况的义务。但这位亿万富豪公开承认，在他接手之后广告商纷纷逃离，导致Twitter公司收入受损。他甚至暗示Twitter有破产的风险。</p><p>&nbsp;</p><p>马斯克做出的200亿美元估值，意味着<a href=\"https://www.infoq.cn/article/5LYIHLWuPVU8btDrfBut\">Twitter</a>\"的价值仅略高于Snapchat的母公司Snap。Snapchat近期同样饱受广告业务不景气的困扰，预期收入也将持续下滑。Snap目前的市值约为180亿美元，日活用户约3.75亿；而Twitter公司在转为私营之前的最新日活用户为2.378亿。</p><p>&nbsp;</p><p>马斯克没有回应置评请求，媒体发给Twitter通讯部门的邮件也被退回，还加上了“便便”表情符号。</p><p>&nbsp;</p><p>根据马斯克在邮件中提出的最新股权补偿计划，Twitter员工将获得X Corporation的股票，也就是马斯克用来收购Twitter的控股公司。具体股权奖励将根据200亿美元的Twitter估值来核算。马斯克还在邮件中提到，他相信Twitter的价值有朝一日将达到2500亿美元。</p><p>&nbsp;</p><p>邮件补充称，Twitter计划允许员工每六个月出售一次股票，这种方式与马斯克管理的航天企业SpaceX类似。马斯克认为，放开私有股交易既能让保证员工的“股权流动性，也不会像上市企业那样引发股价混乱和诉讼问题。”</p><p></p><h2>广告商大批解约，Twitter 盈利遥遥无期</h2><p></p><p>&nbsp;</p><p>虽然马斯克上任后对Twitter进行了大刀阔斧地“改革”，但最终取得的效果却远不及人们的预期。</p><p>&nbsp;</p><p>新的 200 亿美元估值可能反映了由于马斯克一系列激进的手段，导致公司将面对一些新的挑战，例如新的带有验证订阅的 Twitter Blue 导致了一波假账户浪潮，以及“大赦”政策使 Twitter 的一些最差的用户仍然活跃在平台上。</p><p>&nbsp;</p><p>马斯克完成收购后，不少广告商对马斯克领导的 Twitter 提出了担忧。《华尔街日报》报道说，许多广告商担心他提出的缩减内容审核和与潜在利益冲突的计划。</p><p>&nbsp;</p><p>对于这笔交易，当时许多分析师认为，鉴于 2022 年美国许多科技股的价值下跌以及 Twitter 目前难以吸引用户和发展壮大趋势放缓，马斯克为该公司支付的价格太高了。在一次财报电话会议上，马斯克也承认，Twitter 是“一种长期萎靡不振的资产，但具有令人难以置信的潜力，尽管显然我自己和其他投资者现在为 Twitter 付出了过高的代价”。&nbsp;</p><p>&nbsp;</p><p>Twitter的情况仍然不乐观。Vox最近的一份报告表明，Twitter失去了一些最大的广告商，在被马斯克收购之前，Twitter 上排名前 1000 的广告商中有超过一半不再在该平台上展示广告。这显然没有帮助公司解决一些财务问题，包括巨额债务和几笔据称欠房东、咨询公司、私人飞机公司和许多其他公司的未付账单。富达去年也将其在该公司的股份从 5347 万美元削减至 2346 万美元，而Twitter 的收入下滑12 月同比增长约 40%。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.nytimes.com/2023/03/26/technology/elon-musk-twitter-value.html\">https://www.nytimes.com/2023/03/26/technology/elon-musk-twitter-value.html</a>\"</p>",
    "publish_time": "2023-03-28 16:17:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "杭银消金基于 Apache Doris 的统一数据查询网关改造实践",
    "url": "https://www.infoq.cn/article/EOUASNMVTwgDrTVq9kNh",
    "summary": "<p></p><blockquote>随着业务量快速增长，数据规模的不断扩大，杭银消金早期的大数据平台在应对实时性更强、复杂度更高的的业务需求时存在瓶颈。为了更好的应对未来的数据规模增长，杭银消金于 2022 年 10 月正式引入 Apache Doris 1.2 对现有的风控数据集市进行了升级改造，利用 Multi Catalog 功能统一了 ES、Hive、GP 等数据源出口，实现了联邦查询，为未来统一数据查询网关奠定了基础；同时，基于 <a href=\"https://github.com/apache/doris\">Apache Doris</a>\" 高性能、简单易用、部署成本低等诸多优势，也使得各大业务场景的查询分析响应实现了从分钟级到秒级的跨越。</blockquote><p></p><p></p><p>作者｜杭银消金大数据团队 周其进、唐海定、 姚锦权</p><p></p><p>杭银消费金融股份有限公司，成立于 2015 年 12 月，是杭州银行牵头组建的浙江省首家持牌消费金融公司，经过这几年的发展，在 2022 年底资产规模突破 400 亿，服务客户数超千万。公司秉承“数字普惠金融”初心，坚持服务传统金融覆盖不充分的、具有消费信贷需求的客户群体，以“数据、场景、风控、技术”为核心，依托大数据、人工智能、云计算等互联网科技，为全国消费者提供专业、高效、便捷、可信赖的金融服务。</p><p></p><h1>业务需求</h1><p></p><p>杭银消金业务模式是线上业务结合线下业务的双引擎驱动模式。为更好的服务用户，运用数据驱动实现精细化管理，基于当前业务模式衍生出了四大类的业务数据需求：</p><p></p><p>预警类：实现业务流量监控，主要是对信贷流程的用户数量与金额进行实时监控，出现问题自动告警。分析类：支持查询统计与临时取数，对信贷各环节进行分析，对审批、授信、支用等环节的用户数量与额度情况查询分析。看板类：打造业务实时驾驶舱与 T+1 业务看板，提供内部管理层与运营部门使用，更好辅助管理进行决策。建模类：支持多维模型变量的建模，通过算法模型回溯用户的金融表现，提升审批、授信、支用等环节的模型能力。</p><p></p><h1>数据架构 1.0</h1><p></p><p>为满足以上需求，我们采用 Greenplum + CDH 融合的架构体系创建了大数据平台 1.0 ，如下图所示，大数据平台的数据源均来自于业务系统，我们可以从数据源的 3 个流向出发，了解大数据平台的组成及分工：</p><p></p><p>业务系统的核心系统数据通过 CloudCanal 实时同步进入 Greenplum 数仓进行数据实时分析，为 BI 报表，数据大屏等应用提供服务，部分数据进入风控集市 Hive 中，提供查询分析和建模服务。业务系统的实时数据推送到 Kafka 消息队列，经 Flink 实时消费写入 ES，通过风控变量提供数据服务，而 ES 中的部分数据也可以流入 Hive 中，进行相关分析处理。业务系统的风控数据会落在 MongoDB，经过离线同步进入风控集市 Hive，Hive 数仓支撑了查询平台和建模平台，提供风控分析和建模服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fcc1e849745c05296d02bfca3ffc2dd5.png\" /></p><p></p><p>我们将 ES 和 Hive 共同组成了风控数据集市，从上述介绍也可知，四大类的业务需求基本都是由风控数据集市来满足的，因此我们后续的改造升级主要基于风控数据集市来进行。在这之前，我们先了解一下风控数据集市 1.0 是如何来运转的。</p><p></p><p>风控数据集市 1.0</p><p></p><p>风控数据集市原有架构是基于 CDH 搭建的，由实时写入和离线统计分析两部分组成，整个架构包含了 ES、Hive、Greenplum 等核心组件，风控数据集市的数据源主要有三种：通过 Greenplum 数仓同步的业务系统数据、通过 MongoDB 同步的风控决策数据，以及通过 ES 写入的实时风控变量数据。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/793a25d0ba51493406210100d0215483.png\" /></p><p></p><p>实时流数据： 采用了 Kafka + Flink + ES 的实时流处理方式，利用 Flink 对 Kafka 的实时数据进行清洗，实时写入ES，并对部分结果进行汇总计算，通过接口提供给风控决策使用。</p><p></p><p>离线风控数据： 采用基于 CDH 的方案实现，通过 Sqoop 离线同步核心数仓 GP 上的数据，结合实时数据与落在 MongoDB 上的三方数据，经数据清洗后统一汇总到 Hive 数仓进行日常的跑批与查询分析。</p><p></p><p>需求满足情况：</p><p></p><p>在大数据平台 1.0 的的支持下，我们的业务需求得到了初步的实现：</p><p></p><p>预警类：基于 ES + Hive 的外表查询，实现了实时业务流量监控；分析类：基于 Hive 实现数据查询分析和临时取数；看板类：基于 Tableau +Hive 搭建了业务管理驾驶舱以及T+1 业务看板；建模类：基于 Spark+Hive 实现了多维模型变量的建模分析；</p><p></p><p>受限于 Hive 的执行效率，以上需求均在分钟级别返回结果，仅可以满足我们最基本的诉求，而面对秒级甚至毫秒级的分析场景，Hive 则稍显吃力。</p><p></p><p>存在的问题：</p><p></p><p>单表宽度过大，影响查询性能。风控数据集市的下游业务主要以规则引擎与实时风控服务为主，因规则引擎的特殊性，公司在数据变量衍生方面资源投入较多，某些维度上的衍生变量会达到几千甚至上万的规模，这将导致 Hive 中存储的数据表字段非常多，部分经常使用的大宽表字段数量甚至超过上千，过宽的大宽表非常影响实际使用中查询性能。数据规模庞大，维护成本高。 目前 Hive 上的风控数据集市已经有存量数据在百 T 以上，面对如此庞大的数据规模，使用外表的方式进行维护成本非常高，数据的接入也成为一大难题。接口服务不稳定。 由风控数据集市离线跑批产生的变量指标还兼顾为其他业务应用提供数据服务的职责，目前 Hive 离线跑批后的结果会定时推送到 ES 集群（每天更新的数据集比较庞大，接口调用具有时效性），推送时会因为 IO 过高触发 ES 集群的 GC 抖动，导致接口服务不稳定。</p><p></p><p>除此之外，风控分析师与建模人员一般通过 Hive &amp; Spark 方式进行数据分析建模，这导致随着业务规模的进一步增大，T+1 跑批与日常分析的效率越来越低，风控数据集市改造升级的需求越发强烈。</p><p></p><h1>技术选型</h1><p></p><p></p><p>基于业务对架构提出的更高要求，我们期望引入一款强劲的 OLAP 引擎来改善架构，因此我们于 2022 年 9 月份对 ClickHouse 和 Apache Doris 进行了调研，调研中发现 Apache Doris 具有高性能、简单易用、实现成本低等诸多优势，而且 Apache Doris 1.2 版本非常符合我们的诉求，原因如下：</p><p></p><p>宽表查询性能优异：从官方公布的测试结果来看，1.2 Preview 版本在 SSB-Flat 宽表场景上相对 1.1.3 版本整体性能提升了近 4 倍、相对于 0.15.0 版本性能提升了近 10 倍，在 TPC-H 多表关联场景上较 1.1.3 版本上有近 3 倍的提升、较 0.15.0 版本性能提升了 11 倍以上，多个场景性能得到飞跃性提升。</p><p></p><p>便捷的数据接入框架以及联邦数据分析能力： Apache Doris 1.2 版本推出的 Multi Catalog 功能可以构建完善可扩展的数据源连接框架，便于快速接入多类数据源，提供基于各种异构数据源的联邦查询和写入能力。 目前 Multi-Catalog 已经支持了 Hive、Iceberg、Hudi 等数据湖以及 MySQL、Elasticsearch、Greenplum 等数据库，全面覆盖了我们现有的组件栈，基于此能力有希望通过 Apache Doris 来打造统一数据查询网关。</p><p></p><p>生态丰富： 支持 Spark Doris Connector、Flink Doris Connector，方便离线与实时数据的处理，缩短了数据处理链路耗费的时间。</p><p></p><p>社区活跃： Apache Doris 社区非常活跃，响应迅速，并且 SelectDB 为社区提供了一支专职的工程师团队，为用户提供技术支持服务。</p><p></p><h1>数据架构 2.0</h1><p></p><p></p><p>风控数据集市 2.0</p><p></p><p>基于对 Apache Doris 的初步的了解与验证，22 年 10 月在社区的支持下我们正式引入 Apache Doris 1.2.0 Preview 版本作为风控数据集市的核心组件，Apache Doris 的 Multi Catalog 功能助力大数据平台统一了 ES、Hive、Greenplum 等数据源出口，通过 Hive Catalog 和 ES Catalog 实现了对 Hive &amp; ES 等多数据源的联邦查询，并且支持 Spark-Doris-Connector，可以实现数据 Hive 与 Doris 的双向流动，与现有建模分析体系完美集成，在短期内实现了性能的快速提升。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b5/b520a3463d6f83be31b1170be0fe28f2.png\" /></p><p></p><p>大数据平台 2.0</p><p></p><p>风控数据集市调整优化之后，大数据平台架构也相应的发生了变化，如下图所示，仅通过 Doris 一个组件即可为数据服务、分析平台、建模平台提供数据服务。</p><p></p><p>在最初进行联调适配的时候，Doris 社区和 SelectDB 支持团队针对我们提出的问题和疑惑一直保持高效的反馈效率，给于积极的帮助和支持，快速帮助我们解决在生产上遇到的问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a190bb7fbf76b4ace493a24875941ed.png\" /></p><p></p><p>需求实现情况：</p><p></p><p>在大数据平台 2.0 的加持下，业务需求实现的方式也发生了变更，主要变化如下所示</p><p></p><p>预警类：基于 ES Catalog+ Doris 实现了对实时数据的查询分析。在架构 1.0 中，实时数据落在 ES 集群上，通过 Hive 外表进行查询分析，查询结果以分钟级别返回；而在 Doris 1.2 集成之后， 使用 ES Catalog 访问 ES，可以实现对 ES 数据秒级统计分析。分析类：基于 Hive Catalog + Doris 实现了对现有风控数据集市的快速查询。目前 Hive 数据集市存量表在两万张左右，如果通过直接创建 Hive 外部表的方式，表结构映射关系的维护难度与数据同步成本使这一方式几乎不可能实现。而 Doris 1.2 的 Multi Catalog 功能则完美解决了这个问题，只需要创建一个 Hive Catalog，就能对现有风控数据集市进行查询分析，既能提升查询性能，还减少了日常查询分析对跑批任务的资源影响。看板类：基于 Tableau + Doris 聚合展示业务实时驾驶舱和 T+1 业务看板，最初使用 Hive 时，报表查询需要几分钟才能返回结果，而 Apache Doris 则是秒级甚至是毫秒级的响应速度。建模类：基于 Spark+Doris 进行聚合建模。利用 Doris1.2 的 Spark-Doris-Connector功 能，实现了 Hive 与 Doris 数据双向同步，满足了 Spark 建模平台的功能复用。同时增加了 Doris 数据源，基础数据查询分析的效率得到了明显提升，建模分析能力的也得到了增强。</p><p></p><p>在 Apache Doris 引入之后，以上四个业务场景的查询耗时基本都实现了从分钟级到秒级响应的跨越，性能提升十分巨大。</p><p></p><p>生产环境集群监控</p><p></p><p>为了快速验证新版本的效果，我们在生产环境上搭建了两个集群，目前生产集群的配置是 4 个 FE + 8个 BE，单个节点是配置为 64 核+ 256G+4T，备用集群为 4 个 FE + 4 个 BE 的配置，单个节点配置保持一致。</p><p></p><p>集群监控如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/5341804ee54efff6bef73097f6f5caab.png\" /></p><p></p><p>可以看出，Apache Doris 1.2 的查询效率非常高，原计划至少要上 10 个节点，而在实际使用下来，我们发现当前主要使用的场景均是以 Catalog 的方式查询，因此集群规模可以相对较小就可以快速上线，也不会破坏当前的系统架构，兼容性非常好。</p><p></p><h2>数据集成方案</h2><p></p><p>前段时间，Apache Doris 1.2.2 版本已经发布，为了更好的支撑应用服务，我们使用 Apache Doris 1.2.2 与 DolphinScheduler 3.1.4 调度器、SeaTunnel 2.1.3 数据同步平台等开源软件实现了集成，以便于数据定时从 Hive 抽取到 Doris 中。整体的数据集成方案如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90d1d4bc7ec35721de793f84bdd41445.png\" /></p><p></p><p>在当前的硬件配置下，数据同步采用的是 DolphinScheduler 的 Shell 脚本模式，定时调起 SeaTunnel 的脚本，数据同步任务的配置文件如下：</p><p></p><p><code lang=\"sql\"> env{\n  spark.app.name = \"hive2doris-template\"\n  spark.executor.instances = 10\n  spark.executor.cores = 5\n  spark.executor.memory = \"20g\"\n}\nspark {\n  spark.sql.catalogImplementation = \"hive\"\n}\nsource {\n  hive {\n    pre_sql = \"select * from ods.demo_tbl where dt='2023-03-09'\"\n    result_table_name = \"ods_demo_tbl\"\n  }\n}\n \ntransform {\n}\n \nsink {\n  doris {\n      fenodes = \"192.168.0.10:8030,192.168.0.11:8030,192.168.0.12:8030,192.168.0.13:8030\"\n      user = root\n      password = \"XXX\"\n      database = ods\n      table = ods_demo_tbl\n      batch_size = 500000\n      max_retries = 1\n      interval = 10000\n      doris.column_separator = \"\\t\"\n    }\n}\n</code></p><p></p><p>该方案成功实施后，资源占用、计算内存占用有了明显的降低，查询性能、导入性能有了大幅提升：</p><p></p><p>存储成本降低</p><p></p><p>使用前：Hive 原始表包含 500 个字段，单个分区数据量为 1.5 亿/天，在 HDFS 上占用约 810G 的空间。</p><p></p><p>使用后：我们通过 SeaTunnel 调起 Spark on YARN 的方式进行数据同步，可以在 40 分钟左右完成数据同步，同步后数据占用 270G 空间，存储资源仅占之前的 1/3。</p><p></p><p>计算内存占用降低，性能提升显著</p><p></p><p>使用前：上述表在 Hive 上进行 Group By 时，占用 YARN 资源 720 核 1.44T 内存，需要 162 秒才可返回结果；</p><p></p><p>使用后：</p><p></p><p>通过 Doris 调用 Hive Catalog 进行聚合查询，在设置 set exec_mem_limit=16G 情况下用时 58.531 秒，查询耗时较之前减少了近 2/3；在同等条件下，在 Doris 中执行相同的的操作可以在 0.828 秒就能返回查询结果，性能增幅巨大。</p><p></p><p>具体效果如下：</p><p></p><p>（1）Hive 查询语句，用时 162 秒。</p><p></p><p><code lang=\"sql\">select count(*),product_no   FROM ods.demo_tbl where dt='2023-03-09'\ngroup by product_no;\n</code></p><p></p><p>（2）Doris 上 Hive Catalog 查询语句，用时 58.531 秒。</p><p></p><p><code lang=\"sql\">set exec_mem_limit=16G；\nselect count(*),product_no   FROM hive.ods.demo_tbl where dt='2023-03-09'\ngroup by product_no;\n</code></p><p></p><p>（3）Doris 上本地表查询语句，仅用时0.828秒。</p><p></p><p><code lang=\"sql\">select count(*),product_no   FROM ods.demo_tbl where dt='2023-03-09'\ngroup by product_no;\n</code></p><p></p><p>导入性能提升</p><p></p><p>使用前：Hive 原始表包含 40 个字段，单个分区数据量 11 亿/天，在 HDFS 上占用约 806G 的空间</p><p></p><p>使用后：通过 SeaTunnel 调起 Spark on YARN 方式进行数据同步，可以在 11 分钟左右完成数据同步，即 1 分钟同步约一亿条数据，同步后占用 378G 空间。</p><p></p><p>可以看出，在数据导入性能的提升的同时，资源也有了较大的节省，主要得益于对以下几个参数进行了调整：</p><p></p><p>push_write_mbytes_per_sec：BE 磁盘写入限速，300M</p><p></p><p>push_worker_count_high_priority: 同时执行的 push 任务个数，15</p><p></p><p>push_worker_count_normal_priority: 同时执行的 push 任务个数，15</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6f/6f8c6d8b9075fc0fa851f538947edfe0.png\" /></p><p></p><h2>架构收益</h2><p></p><p></p><p>（1）统一数据源出口，查询效率显著提升</p><p></p><p>风控数据集市采用的是异构存储的方式来存储数据，Apache Doris 的 Multi Catalog 功能成功统一了 ES、Hive、GP 等数据源出口，实现了联邦查询。 同时，Doris 本身具有存储能力，可支持其他数据源中的数据通过外表插入内容的方式快速进行数据同步，真正实现了数据门户。此外，Apache Doris 可支持聚合查询，在向量化引擎的加持下，查询效率得到显著提升。</p><p></p><p>（2） Hive 任务拆分，提升集群资源利用率</p><p></p><p>我们将原有的 Hive 跑批任务跟日常的查询统计进行了隔离，以提升集群资源的利用效率。目前 YARN 集群上的任务数量是几千的规模，跑批任务占比约 60%，临时查询分析占比 40%，由于资源限制导致日常跑批任务经常会因为资源等待而延误，临时分析也因资源未及时分配而导致任务无法完成。当部署了 Doris 1.2 之后，对资源进行了划分，完全摆脱 YARN 集群的资源限制，跑批与日常的查询统计均有了明显的改善，基本可以在秒级得到分析结果，同时也减轻了数据分析师的工作压力，提升了用户对平台的满意度。</p><p></p><p>（3）提升了数据接口的稳定性，数据写入性能大幅提升</p><p></p><p>之前数据接口是基于 ES 集群的，当进行大批量离线数据推送时会导致 ES 集群的 GC 抖动，影响了接口稳定性，经过调整之后，我们将接口服务的数据集存储在 Doris 上，Doris 节点并未出现抖动，实现数据快速写入，成功提升了接口的稳定性，同时 Doris 查询在数据写入时影响较小，数据写入性能较之前也有了非常大的提升，千万级别的数据可在十分钟内推送成功。</p><p></p><p>（4）Doris 生态丰富，迁移方便成本较低。</p><p></p><p>Spark-Doris-Connector 在过渡期为我们减轻了不少的压力，当数据在 Hive 与 Doris 共存时，部分 Doris 分析结果通过 Spark 回写到 Hive 非常方便，当 Spark 调用 Doris 时只需要进行简单改造就能完成原有脚本的复用，迁移方便、成本较低。</p><p></p><p>（5）支持横向热部署，集群扩容、运维简单。</p><p></p><p>Apache Doris 支持横向热部署，集群扩容方便，节点重启可以在在秒级实现，可实现无缝对接，减少了该过程对业务的影响； 在架构 1.0 中，当 Hive 集群与 GP 集群需要扩容更新时，配置修改后一般需要较长时间集群才可恢复，用户感知比较明显。而 Doris 很好的解决了这个问题，实现用户无感知扩容，也降低了集群运维的投入。</p><p></p><h1>未来与展望</h1><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/12798d25e141cfcb0a53efe8ccd959ab.png\" /></p><p></p><p>当前在架构 2.0 中的 Doris 集群在大数据平台中的角色更倾向于查询优化，大部分数据还集中维护在 Hive 集群上，未来我们计划在升级架构 3.0 的时候，完成以下改造：</p><p></p><p>实时全量数据接入：利用 Flink 将所有的实时数据直接接入 Doris，不再经过 ES 存储；数据集数据完整性：利用 Doris 构建实时数据集市的原始层，利用 FlinkCDC 等同步工具将业务库 MySQL与决策过程中产生的 MongoDB 数据实时同步到 Doris，最大限度将现有数据都接入 Doris 的统一平台，保证数据集数据完整性。离线跑批任务迁移：将现有 Hive&amp;Spark 中大部分跑批任务迁移至 Doris，提升跑批效率；统一查询分析出口：将所有的查询分析统一集中到 Doris，完全统一数据出口，实现统一数据查询网关，使数据的管理更加规范化；强化集群稳定扩容：引入可视化运维管理工具对集群进行维护和管理，使 Doris 集群能够更加稳定支撑业务扩展。</p><p></p><h1>总结与致谢</h1><p></p><p></p><p>Apache Doris1.2 是社区在版本迭代中的重大升级，借助 Multi Catalog 等优异功能能让 Doris 在 Hadoop 相关的大数据体系中快速落地，实现联邦查询；同时可以将日常跑批与统计分析进行解耦，有效提升大数据平台的的查询性能。</p><p></p><p>作为第一批 Apache Doris1.2 的用户，我们深感荣幸，同时也十分感谢 Doris 团队的全力配合和付出，可以让 Apache Doris 快速落地、上线生产，并为后续的迭代优化提供了可能。</p><p></p><p>Apache Doris 1.2 值得大力推荐，希望大家都能从中受益，祝愿 Apache Doris 生态越来越繁荣，越来越好！</p>",
    "publish_time": "2023-03-28 16:24:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]