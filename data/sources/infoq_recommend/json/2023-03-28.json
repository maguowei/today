[
  {
    "title": "有状态自动扩展系统的设计模式提议",
    "url": "https://www.infoq.cn/article/9ivZNYc9J4Hrs7fPKH7y",
    "summary": "<p></p><h2>介绍</h2><p></p><p></p><p>面对软件工程界对隔离的需求趋势以及日益增长的可扩展性需求，自动扩展的有状态系统（多指数据库）逐渐复杂化，甚至有时可行性也会成为挑战。因此，多数公司为了让此类系统能适应最高的负载需求而选择了过度配置，但这也带来了另一个问题，过度的资源配置所需求的成本非常之高且无法保证系统的可靠性，激增的需求数量或 DOS 攻击都能轻易击溃系统负载的承受能力。本文中将会更深入地挖掘自动扩展有状态系统时所面临的挑战，并为解决这些挑战给出一个结合已有方法与新手段的设计方案提议。</p><p></p><h2>回顾</h2><p></p><p></p><p>回顾软件工程的发展史，我们能清楚看到软件的构建与用户的限制和期望方面的几个重要里程碑。从<a href=\"https://en.wikipedia.org/wiki/Mainframe_computer\">大型机</a>\"与大型服务器的集中式方法开始，过渡到桌面应用程序阶段中<a href=\"https://en.wikipedia.org/wiki/Client%E2%80%93server_model\">客户-服务器形式</a>\"的出现，再到网络应用变革的诸多阶段，从大型单体应用到现代<a href=\"https://en.wikipedia.org/wiki/Microservices\">微服务</a>\"的发展。</p><p></p><p>纵观历史，隔离的趋势非常明显。其中垂直隔离是指按关注点或上下文将系统分割，通常为数据库与应用程序的隔离，或者用户界面与业务及服务层的隔离。另一种则是水平隔离，即通过增加节点的配置以支持不断增长的需求，这一过程也可以借助&nbsp;<a href=\"https://kubernetes.io/docs/concepts/overview/\">Kubernetes</a>\"&nbsp;等工具的帮助进行自动扩展。</p><p></p><p>这种对隔离的渴求致使了<a href=\"https://en.wikipedia.org/wiki/Shared-nothing_architecture\">无共享架构</a>\"等方式的出现，即将应用程序构建为本身不具备状态的形式，也就是我们所了解的无状态应用程序，从而简化扩展的难度。听起来很棒，但工程师们很快便发现很少能有不具备任何状态的，“真正无状态”的应用程序。</p><p></p><p>退而求其次的方法是将应用程序的一部分（通常为服务或微服务）构建为无状态形式，但即便如此，这部分程序依旧依赖于数据库等有状态的系统以维持状态。这便是本文的主题，我们将讨论这个软件工程师间的共同挑战：如何在现代应用中有效地自动扩展有状态系统？</p><p></p><h2>适用用例</h2><p></p><p></p><p>本文不适用于在网页服务器中保持状态的有状态系统。</p><p></p><p>这种方式为软件工程师们提供了一个可以自行搭建数据库的基础设计方案，切实地解决了单节点上存储系统的问题，并将系统转变为具有自主自动扩展能力的分布式系统。以一个线上商城的微服务架构为例，见图一。</p><p></p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/4image001-1674490178804.jpeg\" /></p><p>图一：线上商城用例示范</p><p></p><p>假设出于某些需求的原因，在项目中选择&nbsp;<a href=\"http://rocksdb.org/\">RocksDB</a>\"&nbsp;是相较 Redis 而言更好的键值存储引擎，但麻烦的是，RocksDB 作为存储引擎只能原封不动地部署在单个节点上。假设现在要做另一个对可扩展性需求很高的全球性新系统，那么这篇文章将会是个很好的起点，这里你能找到如何将 RocksDB 这样的单节点存储引擎改造为一个分布式可自动伸缩的应用。</p><p></p><p>当然，我们这里所说的 RocksDB 只是一个例子，这种设计模式对其他任何的存储引擎或工具均可通用，无论是用于文本索引的&nbsp;<a href=\"https://lucene.apache.org/\">Apache Lucene</a>\"，还是不具备任何引擎的存内存储，不区分任何的存储引擎、语言、数据类型或结构。此外，本文中所分享的设计也可以为 Mongo、Redis、Postgres 数据库提供自动伸缩的功能。</p><p></p><h2>有状态系统的定义</h2><p></p><p></p><p>有状态的系统是指必须处理状态的系统。在现代网页应用中，这项任务通常由数据库承担，不过其他如网页服务器也可以通过将用户会话存储在网页服务器内存中做到。</p><p>状态管理在网站中的典型用例是用户的购物车（见图一）。在多个 HTTP 请求之间保留购物车内容，确保在用户结束购物结账付款时，购物车内商品内容和数量的准确无误。这些数据就是被存储在了有状态系统中，而在我们图一的用例中则是 Mongo DB 集群、Redis 集群、Postgres 集群。</p><p></p><h2>自动伸缩：问题所在</h2><p></p><p></p><p>我们在面对自动伸缩的有状态系统时，我们往往会思考以下这些问题：什么时候应该开始自动伸缩？有什么契机？应该如何进行伸缩？要如何移动数据？如何保证节点的一致性？以下将会是本文中讨论的重点：</p><p></p><h3>一致性</h3><p></p><p></p><p>任何拥有状态的系统都需要保证集群下一个有效状态的一致性。这个领域中相关的研究有很多，比如分别由&nbsp;<a href=\"https://www.mongodb.com/home\">MongoDB</a>\"、<a href=\"https://redis.io/\">Redis</a>\"、和 Postgres 所用的一致性算法&nbsp;<a href=\"https://www.infoq.com/news/2014/08/ark-mongodb/\">Ark</a>\"、<a href=\"https://github.com/RedisLabs/redisraft\">Raft</a>\"，以及&nbsp;<a href=\"https://www.enterprisedb.com/docs/pgd/latest/bdr/\">BDR</a>\"。这些均是在图一中线上商城示例所用的数据库。</p><p></p><p>在软件应用中，每当集群必须统一存储记录的下一个值时都会有对一致性的要求，这也是数据库实现中最出名的用例。本文中将涵盖一些能让 Raft 在选择新领导者的一致性上更加智能的建议。</p><p></p><h3>自动伸缩</h3><p></p><p></p><p>在有状态系统中，虽然越来越多的产品提供自动伸缩功能，比如云供应商所提供的数据库实例管理，但在实践中，公司内部的类似方案实现似乎总是困难重重，可能存在的原因如下：</p><p>缺乏透明度：主流云供应商如&nbsp;<a href=\"https://aws.amazon.com/\">AWS</a>\"&nbsp;及&nbsp;<a href=\"https://azure.microsoft.com/en-us/\">Azure</a>\"&nbsp;的自动伸缩功能似乎配置起来很容易，但具体的实现细节却无人知晓。了解该功能的运作原理对拥有大型数据集的关键场景至关重要。数据的迁移时机、策略等等这些问题都应该在产品详情页有明确的说明。不存在的自动扩展公用模式。自动扩展有状态系统方面至今为止仍无公开可用的模板。</p><p></p><h3>数据迁移的滞后</h3><p></p><p></p><p>常见的系统扩展方式是在集群中新增节点，但对于有状态的集群来说，同步其他节点已有的数据是需要时间的，而在某些情况下，这一过程的数据量可能是海量的。</p><p></p><p>还是以我们的线上商城为例（图一），假设组织的业务涉及多个地理区域，那么数据量将会高达数十亿。在这种规模下，一个清晰且高效的数据同步和迁移方式是非常重要的。</p><p></p><h3>快慢需求的增长</h3><p></p><p></p><p>以下这两种情况中，都需要更高的能力：1. 稳定、中长期逐步增加的需求。在先前的线上商城示例中，可以是一段时间内消费者数量的持续增长。2. 无法预测需求量激增，可能带来服务不可用的风险，如系统遭受 DoS 攻击时。</p><p></p><h3>封闭的解决方案</h3><p></p><p></p><p>为解决上述这些问题，我们需要更多公开可用的设计模式，盲目相信云供应线的解决方案会奏效不是个好主意。即使云供应商的方案确实有效，我们也很快会发现自己已经和这个特定供应商绑定了，这很不理想。</p><p></p><h2>愿景</h2><p></p><p></p><p>作者欲公开提出一种通用且可复用的，客户化自动扩展有状态系统的手段，以最少的配置或运维干预，在单集群内从单节点自动向上（垂直）或向下（水平）扩展至成百上千的节点。本文中所提出的解决方案在现阶段仅仅存在于理论之中，仍需实施和测试。</p><p></p><h2>核心原则</h2><p></p><p></p><h3>数据类型不可知</h3><p></p><p></p><p>这类设计不应局限于任何具体的数据类型，也就是说，同样的解决方案应能处理 JSON 对象、序列化数据、流数据、二进制大对象（Blob），以及其他任何的数据类型。</p><p></p><p>各司其职，负责写入新状态的集群领导者（leader）只执行写操作，不进行读取；负责读取副本的也只应执行读操作而不进行写入。</p><p></p><h3>让代理成为集群的一部分</h3><p></p><p></p><p>有状态集群中必须要有一个不提供读写的代理实现，让集群以这个代理为节点缓慢地同步集群中的数据，并最终在需要时准备提供读或写的请求。</p><p></p><h3>借助平均响应时间触发自动扩展</h3><p></p><p></p><p>云供应商所提供的大多数自动扩展方式均是利用了 CPU 和内存阈值，但这种方式并不能带来最佳的用户体验。在某些情况下，终端用户不一定能感知到系统资源的紧张，即使是占用了99%的 CPU，请求也可能及时交付。</p><p></p><p>将平均响应时间作为主要的触发条件可以改变扩展的契机，以用户角度看待系统性能。</p><p></p><h3>分片标签的优先级</h3><p></p><p>将每个对象或记录贴上分片 ID 的标签可以规避系统高压时的成本，每当需要分片时可以直接利用已有标签而无需外界干预。</p><p></p><h2>模式的设计</h2><p></p><p></p><p>文中所提议的自动扩展有状态系统包含三个不同角色，集群中这些角色各司其职。值得注意的是，作者所提议的设计与系统在单节点的运行能力与生产目的息息相关，这些角色不一定要在自己的进程或节点中运行，但对于测试、POC，甚至是部分 MVP 而言，这点还是很重要的。</p><p></p><p>话不多说，让我们看看这些角色各自的责任吧。</p><p></p><h3>写入角色（领导者）</h3><p></p><p></p><p>写入者或领导者是负责处理写操作的角色，该角色会将新状态写入自身存储并将复制后的数据传给“副本读取”角色。每个分片只有一个写入角色，关于分片的详细解释请见后文。写入角色是一致性的领导者，负责所有的写操作执行，且不承担任何读操作的执行。</p><p></p><h3>副本读取</h3><p></p><p></p><p>副本读取角色承担所有的读请求，包含来自写入角色（领导者）的一份复制数据，但如果写入角色和读取角色均在同一节点或进程中运行，则二者可以共享同一存储。在一致性协议选举新的领导者时，副本读取角色负责打开一个与领导者相连的多路通信管道，并在任何一个节点关闭或连接被网络分区中断之前一直保持管道的开放。如此，通过避免连接打开和关闭的开销，有效加快了角色与节点之间的通信速度。</p><p></p><h3>负载管理角色</h3><p></p><p></p><p>负载管理角色充当了<a href=\"https://whatismyipaddress.com/gateway\">网关</a>\"与<a href=\"https://en.wikipedia.org/wiki/Load_balancing_%28computing%29\">负载平衡器</a>\"的角色，负责将写请求发送给领导者，将读请求发送给副本读取角色。负载管理角色同时也是一个可接受上千入站请求的<a href=\"https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7\">背压</a>\"机制，将目标（对于副本读取或写入者）的并行线程数量限制在可配置的范围内，从而保证了对这些角色的压力控制。这一功能是受<a href=\"https://dzone.com/articles/understanding-tomcat-nio\">阿帕奇的 Tomcat Nio 连接器</a>\"的启发，此外，该功能也对集群负载激增或 DOS 攻击的防护至关重要，在这类情况下，负载管理角色将会吸收压力，确保读写角色的安全与稳定请求流的接收。</p><p></p><p>负载管理角色也负责将写请求路由到正确的分片，并在需要聚合时查询请求分发至每个分片中。在将结果返回客户端之前，该角色还会对结果进行聚合及排序，从而减少副本读取角色的工作量。负载管理角色解决问题的方式与<a href=\"https://www.programmingbrain.com/2022/11/what-is-database-proxy.html\">数据库代理</a>\"类似，不同点在于该角色并未以外部插件的形式存在，而是作为集群中的一部分。</p><p></p><p>负载管理角色可以有一个或多个实例，而每当实例数量到达 CPU 或内存的可配置上限时，则会另外配置一个新的负载管理角色，生成负载管理集群，并在该集群内拥有自己独立的一致性机制。</p><p></p><h2>高层设计</h2><p></p><p></p><p>这个设计模式中各个角色之间的基本互动形式可见图二中表示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/4image003-1674490178804.jpeg\" /></p><p>图二：解决方案的基本设计</p><p></p><h3>为什么选择Raft？</h3><p></p><p></p><p>作为一款名声在外且饱经考验的一致性校验算法，<a href=\"https://raft.github.io/\">Raft</a>\"&nbsp;远比&nbsp;<a href=\"https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html\">Paxos</a>\"&nbsp;要简单，但性能却有时能与后者相媲美，正如在<a href=\"https://emptysqua.re/blog/paxos-vs-raft/\">论文评论：Raft vs. Paxos</a>\"中所阐述的。</p><p></p><p>对于作者所提出的策略而言，Raft 只能同时拥有一个领导者是非常重要的。</p><p></p><h3>智能 Raft</h3><p></p><p></p><p>作者提议通过修改 Raft 协议以提升集群的整体性能，让 Raft 能意识到节点之间的不同并选举“更大”的可用节点作为领导者。这点对自动扩展的写操作尤为重要，因为同时只能有一个领导者，那么增强其能力最有效的方式就是提供一个“更大”的领导者，从而触发一个新的选举。Raft 要能识别并选举这个“更大”的节点作为领导者。</p><p></p><p>另一种方式是通过修改 Raft 使其能接受到一个“切换”指令，从而让集群将领导者切换至这个“更大”的节点。</p><p></p><p>第二种方式要更好，不仅对协议的修改要更小，还可以将领导者的切换与逻辑解耦。</p><p></p><p>这里所说的“更大”，是指 CPU、内存、存储技术（<a href=\"https://en.wikipedia.org/wiki/Solid-state_drive\">SSD</a>\"），或者其他资源大小，完全取决于有状态集群的需求目的。如果集群需要服务于复杂计算，那么大概会需要“更大”的 CPU 资源，如果集群需要服务于请求，则应该是会需要“更大”的内存资源和更好的存储技术。</p><p></p><h3>自动扩展策略</h3><p></p><p></p><p>作者将可扩展性的不同阶段以“马赫”命名，“马赫”本意为超过音速的物体移动速度，而在本文中，各个马赫阶段则喻指集群节点的数量。</p><p></p><p>注：“马赫”一词仅在本文中使用，并非是行业中专有名词。</p><p></p><h3>可配置的扩展触发器</h3><p></p><p></p><p>清楚自动扩展或缩减的时机很重要，在系统承受巨大压力时选择扩展并不是个明智的选择，这也是为什么负载管理角色所提供的背压十分关键。</p><p></p><p>作者将重点探讨随时间逐渐增加的需求这一情景。为此，有两种基本的配置可用于自动扩展的触发，其中的负载管理角色都需要能够识别触发自动扩展的时机并向操作人员发出通知。操作人员可以为人或软件系统，后者更好。</p><p></p><p>可配置的触发器如下。</p><p></p><h4>1. 借助平均响应时间的阈值</h4><p></p><p></p><p>负载管理角色的任务之一是监测请求的平均响应时间。当请求的平均响应时间到达某个特定的阈值后，则触发扩展的请求。需要向上配置扩展举例：上一个60分钟内，每个请求平均响应时间为3秒需要向下配置扩展举例：上一个60分钟内，每个请求平均响应时间小于0.5秒。</p><p></p><h3>2. 借助超时的阈值</h3><p></p><p></p><p>在自动扩展信号发送之前，超时的阈值是特定时间段内可能超时的请求所占百分比。需要向上配置扩展举例：上一个5分钟内，大于1%的请求超时需要向下配置扩展举例：不建议使用超时比例作为阈值，出于安全考量，集群中任何级别的超时都不建议用作向下扩展的阈值。</p><p></p><p>再回到图一的例子中，假设我们目前已经用可自动扩展的 RocksDB 替代了 Redis，这个全新可自动扩展的 RocksDB 集群将无需人类操作者或管理员的干预，自动根据是否超过阈值进行扩展。</p><p>在继续阅读本文之前，请注意：- 下文中每个马赫阶段所举的例子大多都注重于提升读取能力，以下文中所述的工作负载隔离，侧面提升写入能力。在马赫 IV 阶段之后会有专门章节表述目标写入能力的扩展。- 本文中的“节点”意指集群中的参与者或运行中的某个进程，不一定代表不同的硬件。- 创建分片之前应先配置需要分配的副本总数。- 任何配置下都可以启动集群，马赫 IV 中所述的是生产所用的最低推荐配置。- 写入、副本读取，以及负载管理这三个角色的实现均为模块化，且常部署在节点之上。举例来说，当某个节点被标记为“写入角色”时，节点虽然仅启用写入角色模块，但也包含副本读取和负载管理的未启用模块，允许该节点在后续按需切换角色。</p><p></p><h2>马赫 I</h2><p></p><p></p><p>在集群或系统的初始状态下，所以角色均在同一节点上启用，代表小型用例或测试管道等场景，类似图三中的示例。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/4image005-1674491096782.jpeg\" /></p><p>图三：单节点部署——马赫 I</p><p></p><p>在马赫 I阶段，所有组件均作为单一进程部署在同一个几点之上，这个单一的节点负责管理所有的读写请求。用例：主要推荐用于测试场景，不适合于生产环境。</p><p></p><h3>一致性与副本</h3><p></p><p></p><p>在马赫 I阶段，组件在内存模块内通信，因此不需要一致性或副本。</p><p></p><h2>马赫 II</h2><p></p><p></p><p>在马赫 II 阶段，开始部署包含双节点的集群。扩展出的第二个节点一般都是负载管理角色，确保能为响应请求的节点提供背压保护，并允许新节点开始同步数据。</p><p>拓扑结构如图四所示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/5image007-1674491096782.jpeg\" /></p><p>图四：双节点部署——马赫 II</p><p></p><h3>一致性与副本</h3><p></p><p></p><p>在马赫 II 阶段，无需考虑一致性，因为少于三个节点时是不可能建立 Raft 一致性的。</p><p></p><p>部署在节点二上的副本读取模块会复制与负载管理角色用时运行在节点二上的副本读取角色。需要注意的是，部署了负载管理角色的节点二上的副本读取角色不提供请求，这一设计背后的意图是为时时保持一个“几乎完全”同步的节点，该节点可以以额外的副本读取或领导者节点身份快速加入操作，如作者在马赫 III 阶段所述。</p><p></p><p>用例：可被用于可靠性不是非常重要或低运营成本的场景中。</p><p></p><h2>马赫 III</h2><p></p><p></p><p>在马赫 III 中，集群中又新增一节点，目前一共有三个节点。</p><p></p><p>新节点永远会以新的负载管理角色进入集群，客户端将被重新定向至新的负载管理，而在马赫 II 中的负载管理角色则会切换为副本读取角色。</p><p></p><p>马赫 III 的场景可见图五。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/3image009-1674491096782.jpeg\" /></p><p>图五：马赫 III 中的三节点部署</p><p></p><h3>一致性与复制</h3><p></p><p></p><p>三节点时暂时无需考虑一致性，因为即使是拥有三个节点，但只有节点一和二在主动服务于请求。</p><p></p><p>用例：通过将读写操作分割在不同节点之上，性能已经得到了提升。但如果某个节点故障而没有第二个副本读取节点，那么写入节点将被迫在新节点被分配之前负责处理读取请求。</p><p></p><h3>复原策略</h3><p></p><p></p><p>领导者故障：集群回到马赫 II 阶段的拓扑结构，节点二回到处理读写操作的阶段，直到新节点加入集群。节点二（副本读取）故障：领导者或写入角色开始处理读取请求，直到新节点加入集群。节点三（负载处理）故障：节点二将从副本读取角色切换为负载管理，节点一开始执行写入和读取操作。</p><p></p><p>以上三种情况都会向运营商发送一个信号，提示需要用新节点替换故障节点，新节点永远会以负载管理角色加入集群。</p><p></p><h2>马赫 IV</h2><p></p><p></p><p>在这一阶段，集群中有四个节点，其中一个是第二副本读取角色，部署情况如图六所示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/3image011-1674492416274.jpeg\" /></p><p>图六：马赫 IV 中的四节点部署</p><p></p><p>用例：用于生产时工作负载、良好性能，以及故障节点的优秀响应时间所需的最低配置。</p><p></p><h3>一致性与复制</h3><p></p><p></p><p>马赫 IV 阶段第一次引入一致性，但此时仍未进行选举。节点一将维持领导者角色，不浪费时间切换至新领导者，专注于写入操作。Raft 实现的扩展对这一安排至关重要。此外，如果领导者故障，则节点二或节点三成为新领导者，并回到马赫 III 的拓扑结构，这一决定由负载管理角色做出。</p><p></p><h3>复原策略</h3><p></p><p></p><p>领导者故障：Raft 的协议不允许在集群中只有两个节点时进行选举，因此负载管理会随机在包含运行中副本读取的节点二和节点三之间随机挑选一个领导者。</p><p></p><p>对马赫 IV 来说，副本与节点管理节点的复原策略与马赫 III 相同，可在创建分片之前将新的副本读取节点添加至集群中以达到可配置的最大数量。</p><p></p><h2>马赫 V、VI……</h2><p></p><p></p><p>新的副本读取可不断被添加至集群，直到到达可配置的上线从而触发分片。需要注意的是，副本读取的添加意味着新负载管理角色的添加，从而取代上一个负载管理角色；上一个负载管理会加入 Raft 的一致性并开始服务于读取请求。</p><p></p><p>用例：随着集群规模的增加，可靠性也在增加，单个节点的故障将不再像小型部署一样影响重大。</p><p></p><h3>复原策略</h3><p></p><p></p><p>领导者故障：Raft 在可用的副本读取中选举一个新的领导者，新领导者将告知负载管理角色选举结果。副本读取故障：新节点应需加入集群。负载管理故障：上一个被加入集群的副本读取角色会接手负载管理角色的责任，且在新节点被配置之前本身不再服务于请求读取，同理，新节点会以负载管理角色加入集群。</p><p></p><h2>读取密集型与写入密集型的场景</h2><p></p><p></p><p>在马赫 IV 之前的阶段中，考虑到可靠系统的最低可复制配置，这套自动扩展的方案并未将负载的特性纳入考虑范围。在后续的阶段中，系统自动扩展的方式将发生变化，将通常情况下的负载区分为读取密集型（80%及以上的读取操作）、写入密集型（80%及以上的写入操作），或均衡型（其他情况）。虽然可能不包含一些特殊情况，但这个方案是客户化的模式，可以按需适应特殊场景。我们的目标只是为解决多数用例而非全部。</p><p></p><h3>扩展读取密集型场景</h3><p></p><p></p><p>随着新节点按照马赫 I 至 VII 阶段的模式不断被加入集群，我们需要一个最简单的策略，能够代表七个节点（一个读取管理、一个写入领导者、五个副本读取），并让集群开始以分片标签的形式创建已有数据的分片，并将传入的请求负载划分到新创建的分片之中。</p><p></p><p>在进去两个分片的运行之前，我们需要新增一个节点，达到八节点才能支持图七中所介绍的拓扑结构。</p><p>使用用例: 需求不断增长的大规模场景。</p><p></p><p>图七所展示的两个分片的设置情况，两个分片各自包含一个领导者和两个副本读取角色，并在负载管理节点中还有额外的一个副本。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/design-proposal-autoscaling-stateful-systems/en/resources/3image013-1674491679528.jpeg\" /></p><p>图七：两个分片的拓扑结构示例</p><p></p><p>当每个分片中都配置了足够的节点，我们可以进一步扩展至三到四个分片。举例来说，如果分片二扩展至七个节点，那么接下来新增的节点将被分至第二个分片之中。</p><p></p><p>每个负载管理角色只持有一个分片的副本，负载管理角色可在需要时在该分片上处理写入或读取操作，但若想充分满足客户需求，负载管理需要从所有分片中进行读和写。</p><p></p><h3>扩展写入密集型场景</h3><p></p><p></p><p>对于写入操作为重点或写入操作存在合理退化的情况中，存在两种扩展集群写能力的方式：</p><p>提供一个更大的领导者第一种方式是提供比当前领导者更大的节点，“更大”可以指内存、CPU、读写存储速度等等。这一节点最初将按管理配置为负载管理，并在与当前领导者完成同步（如前文各个马赫阶段所述）之前都会作为负载管理角色存在。一旦完成同步，负载管理将开始新的选举，成为 Raft 一致性的新领导者并与旧领导者互换。集群中的每个节点都有从1到5的等级规划，其中1代表最低的等级规划即最少的资源，5则是最高的即较大的资源数量。分片一旦纵向扩展到达极限（节点等级5），则开始<a href=\"https://www.geeksforgeeks.org/what-is-sharding/\">分片</a>\"。新的领导者被配置为副本只读，从同步上一任领导者的分片开始，逐渐成为仅负责当前分片的领导者本身，新增的节点等级（1至5）必须可配置。</p><p></p><p>这与前文中所描述的读取密集型分片触发方式是不同的。读取密集型分片的触发是基于水平规模（节点数），而写入密集型场景的分片触发则是基于垂直规模的，换句话说，领导者的规模到达了最大量（5）。</p><p></p><p>也可以在配置新分片时指定等级，比如让两个领导者都是等级3，但两个等级3会组成一个“等级6”的写入能力集群，不过这也应该是可配置的。在这种情况下，以前的等级5写入节点将被在两个分片上的两个等级3的节点所取代。</p><p></p><h3>分片策略的标签优先级</h3><p></p><p></p><p>问题点：将信息清晰有效地划分至分片并不容易，尤其是在数据量庞大或数据复杂时。</p><p></p><p>简单的解决方案：每当一条记录或对象被存储在集群中时，就给其分配一个1至1000的随机桶 ID，并确保在大规模时，每个桶都有类似数量的对象分配在其中，以均衡分片。</p><p></p><p>在需要分片时，桶 ID 可用于判断对象所属的分片。举例来说，假设桶的总数只有1000，那么在两个分片中，第一个分片中可以仅包含第1至500个桶，第二个分片中则仅被分配第501至1000的桶。</p><p></p><p>每当需要新的分片时，都重复这个桶的划分过程。因此在我们的例子中，分片的最大数量为1000，远超大多数情况下的正常值。</p><p></p><h2>结论及未来展望</h2><p></p><p></p><p>作者不奢望能通过这篇文章解决有状态系统自动扩展的所有问题，只是提供了一个基于作者职业生涯中所使用过的技术模板和模式的方案，并将其标准化，作为有状态自动扩展实现的基础。这些涉及可能不会完美适配必须对所有甚至全部分片执行大部分读取操作的场景，在这些情况下，建议采用其他更好的分片桶定义，从而尝试在一个或尽量少的分片中分配所有需要的数据。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/design-proposal-autoscaling-stateful-systems/\">https://www.infoq.com/articles/design-proposal-autoscaling-stateful-systems/</a>\"</p>",
    "publish_time": "2023-03-28 09:08:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java近期新闻：新JEP、GraalVM 23早期访问构建、Infinispan、Mojarra、Micrometer Metrics",
    "url": "https://www.infoq.cn/article/F6cg96zpRMMi9uAq1zyU",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p></p><p>上周，JEP 440（<a href=\"https://openjdk.org/jeps/440\">记录模式</a>\"）已从JEP Draft 8300541<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007470.html\">提升</a>\"到Candidate状态。该JEP最终确定了这一特性，并针对前2轮<a href=\"https://openjdk.java.net/jeps/12\">预览</a>\"的反馈做了增强。这两轮预览分别是在JDK 20中发布的JEP 432（<a href=\"https://openjdk.org/jeps/432\">记录模式第2次预览</a>\"）和在JDK 19中发布的JEP 405（<a href=\"https://openjdk.org/jeps/405\">记录模式预览</a>\"）。该特性为这门语言添加了记录模式，用于解构记录值。记录模式可以与类型模式搭配使用，为“强大的声明式、可组合数据导航和处理形式”提供支持。最近，类型模式被扩展应用于switch 的选择标记：JEP 420（<a href=\"https://openjdk.java.net/jeps/420\">switch模式匹配第2次预览</a>\"，在JDK 18中交付）和JEP 406（<a href=\"https://openjdk.java.net/jeps/406\">switch模式匹配预览</a>\"，在JDK 17中交付）。JEP 432最重要的变化是不再支持在增强for语句头中使用记录模式。</p><p>&nbsp;</p><p>类似地，JEP 441（<a href=\"https://openjdk.org/jeps/441\">switch模式匹配</a>\"）已经从JEP Draft 8300542<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007471.html\">提升</a>\"到Candidate状态。该JEP最终确定了这一特性，并针对前4轮的预览反馈做了增强：JEP 433（<a href=\"https://openjdk.org/jeps/433\">switch模式匹配第4次预览</a>\"），在JDK 20中交付；JEP 427（<a href=\"https://openjdk.org/jeps/427\">switch模式匹配第3次预览</a>\"），在JDK 19中交付；JEP 420（<a href=\"https://openjdk.java.net/jeps/420\">switch模式匹配第2次预览</a>\"），在JDK 18中交付；JEP 406（<a href=\"https://openjdk.java.net/jeps/406\">switch模式匹配开关预览</a>\"），在JDK 17中交付。该特性通过在switch表达式和语句中支持模式匹配来增强语言。</p><p>&nbsp;</p><p>JEP 442（<a href=\"https://openjdk.org/jeps/442\">外部函数和内存API第3次预览</a>\"）已经从JJEP Draft 8301625<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-March/007473.html\">提升</a>\"到Candidate状态。这个JEP基于之前的反馈做了改进：JEP 434（<a href=\"https://openjdk.org/jeps/434\">外部函数和内存API第2次预览</a>\"），在JDK 20中交付；JEP 424（<a href=\"https://openjdk.org/jeps/424\">外部函数和内存API预览</a>\"），在JDK 19中交付；JEP 419（<a href=\"https://openjdk.org/jeps/419\">外部函数和内存API第2轮孵化</a>\"），在JDK 18中交付；JEP 412（<a href=\"https://openjdk.org/jeps/412\">外部函数和内存API第1轮孵化</a>\"），在JDK 17中交付。该特性为Java应用程序提供了一个可以与Java运行时之外的代码和数据进行互操作的API，让它们可以高效地调用外部函数以及安全地访问不受JVM管理的外部内存。JEP 434的更新包括：在Arena接口中集中管理原生段（native segments）的生命周期；使用一个新元素解引用地址布局，增强布局路径；删除VaList类。</p><p>&nbsp;</p><p>JEP Draft 8303683（<a href=\"https://openjdk.org/jeps/8303683\">虚拟线程</a>\"）是由<a href=\"https://inside.java/u/RonPressler/\">Ron Pressler</a>\"（Oracle Loom项目架构师和技术主管）和<a href=\"https://inside.java/u/AlanBateman/\">Alan Bateman</a>\"（Oracle Java平台组架构师）于上周提交的。该JEP建议根据前2轮预览的反馈最终确定这一特性：JEP 436（<a href=\"https://openjdk.org/jeps/436\">虚拟线程第2次预览</a>\"），在JDK 20中交付；JEP 425（<a href=\"https://openjdk.org/jeps/425\">虚拟线程预览</a>\"），在JDK 19中交付。该特性为Java平台提供了虚拟线程。这种轻量级的线程极大地减少了编写、维护和观察高吞吐量并发应用程序的工作量。与JEP 436相比，其最重要的变化是虚拟线程现在完全支持<a href=\"https://openjdk.org/jeps/8303683#Thread-local-variables\">线程局部变量</a>\"，取消了不使用这些变量的选项。要了解更多关于JEP 425的细节，可以阅读<a href=\"https://www.infoq.com/news/2022/05/virtual-threads-for-jdk19/\">InfoQ的新闻报道</a>\"及观看<a href=\"https://www.linkedin.com/in/jos%C3%A9-paumard-2458ba5/\">José Paumard</a>\"（Oracle Java平台组Java开发大使）提供的JEP Café<a href=\"https://inside.java/2022/06/08/jepcafe11/\">截屏视频</a>\"。</p><p>&nbsp;</p><p>JEP Draft 8304400（<a href=\"https://openjdk.org/jeps/8304400\">启动多文件源代码程序</a>\"）也是由Pressler提交的。该JEP建议增强Java启动器，让它可以执行以一个或多个Java源代码文件形式提供的应用程序。这样就可以推迟全面的项目设置，使得从小型应用程序到大型应用程序的过渡更加平滑。</p><p>&nbsp;</p><p></p><h4>JDK 20</h4><p></p><p></p><p>JDK 20仍处于<a href=\"https://openjdk.java.net/jeps/3#rc\">发布候选</a>\"阶段，GA版本预计将于2023年3月21日发布。<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-20%2B36\">Build 36</a>\"仍然是JDK 20<a href=\"https://jdk.java.net/20/\">早期访问构建</a>\"的当前构建。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/20/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JDK 21</h4><p></p><p></p><p>JDK 21的<a href=\"https://jdk.java.net/21/\">早期访问构建</a>\"<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B14\">Build 14</a>\"也于上周发布，其中包括来自Build 13的<a href=\"https://github.com/openjdk/jdk/compare/jdk-21%2B13...jdk-21%2B14\">更新</a>\"，该更新修复了各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2021%20and%20%22resolved%20in%20build%22%20%3D%20b14%20order%20by%20component%2C%20subcomponent\">问题</a>\"。要了解关于这个版本的更多细节，请查看<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.java.net/projects/jdk/20/\">JDK 20</a>\"和<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\"，我们鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java Bug数据库</a>\"报告Bug。</p><p>&nbsp;</p><p></p><h4>GraalVM</h4><p></p><p></p><p>Oracle实验室<a href=\"https://twitter.com/graalvm/status/1636747570377523200?cxt=HHwWgICw9cnP8rYtAAAA\">发布</a>\"了GraalVM 23.0.0的最新早期访问开发构建。其新特性包括：对<a href=\"https://docs.oracle.com/en/graalvm/enterprise/20/docs/reference-manual/native-image/Resources/\">Native Image Bundles</a>\"的初始支持；经过改进的Linux上AWT支持；原生镜像推荐。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/graalvm/graalvm-ce-dev-builds/releases/tag/23.0.0-dev-20230317_0549\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Spring Framework</h4><p></p><p></p><p>Spring Tools 4.18.0<a href=\"https://spring.io/blog/2023/03/15/spring-tools-4-18-0-released\">发布</a>\"，新特性包括：经过升级的Eclipse 2023-03 IDE；经过改进的新一代Spring Data存储库查询方法内容辅助；修复了导致VSCode中常规Java内容辅助停止工作的问题；修复m2e资源文件（如application.properties ）不向目标文件夹复制的问题。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/spring-projects/sts4/releases/tag/4.18.0.RELEASE\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Quarkus</h4><p></p><p></p><p><a href=\"https://quarkus.io/blog/quarkus-3-0-0-alpha6-released/\">Quarkus 3.0.0的第6个Alpha版本</a>\"提供了2个新特性：通过将quarkus.datasource.jdbc.telemetry 属性设置为true来启用OpenTelemetry for JDBC；CredentialsProviders接口现在支持MongoDB连接。该版本还进行了依赖项升级，包括：SnakeYaml 2.0、Maven Compiler Plugin 3.11.0、Maven OpenRewrite Maven Plugin 4.41.0、SmallRye Common 2.1.0和JBoss Threads 3.5.0.Final。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.0.0.Alpha6\">更新日志</a>\"。</p><p>&nbsp;</p><p></p><h4>Hibernate</h4><p></p><p></p><p><a href=\"https://in.relation.to/2023/03/17/orm-62-cr4/\">Hibernate ORM 6.2的第4个候选版本</a>\"根据Java社区的反馈提供了33个Bug修复和28个改进。预计这将是最终版本发布之前的最后一个候选版本。</p><p>&nbsp;</p><p></p><h4>Micrometer</h4><p></p><p></p><p><a href=\"https://github.com/micrometer-metrics/micrometer/releases/tag/v1.11.0-M2\">Micrometer Metrics 1.11.0的第2个里程碑版本</a>\"提供了一些新特性，包括：一个新指标jvm.threads.started ，用于报告JVM中活动应用程序线程的总数；一个新的ElasticSearch端点\\_index\\_template，用于创建索引模板；将GC名称添加到jvm.gc.pause指标；在基于OSGi的Java运行时上支持Micrometer 库。</p><p>&nbsp;</p><p>类似地，Micrometer Tracing 1.1.0的第2个里程碑版本也提供了一些新特性，包括：<a href=\"https://spring.io/projects/spring-cloud-sleuth\">Spring Cloud Sleuth</a>\"注解的等效物；依赖项升级到Micrometer 1.11.0-M2和OpenTelemetry 1.24.0。</p><p>&nbsp;</p><p></p><h4>Infinispan</h4><p></p><p></p><p>Infinispan 14.0.7.Final<a href=\"https://infinispan.org/blog/2023/03/13/infinispan-14\">发布</a>\"，支持Spring Framework 6和Spring Boot 3。它提供了一些值得注意的Bug修复，包括：MetricsCollector类中的NullPointerException；JSON解析器不能正确报告错误位置；Redis序列化协议（RESP）端点不能解析超过数据包大小的请求；并发访问<a href=\"https://spring.io/projects/spring-session\">Spring Session</a>\"集成会导致会话属性丢失。</p><p>&nbsp;</p><p></p><h4>Piranha</h4><p></p><p></p><p><a href=\"https://piranha.cloud/\">Piranha</a>\"23.3.0<a href=\"https://github.com/piranhacloud/piranha/releases/tag/v23.3.0\">发布</a>\"，显著的变化包括：升级<a href=\"https://codeql.github.com/\">CodeQL</a>\"工作流；为DefaultAnnotationManager类添加JUnit测试；修复当端点应用程序仍处于部署过程中时报RuntimeException的问题。要了解关于这个版本的更多细节，请查阅<a href=\"https://javadoc.io/doc/cloud.piranha/project/latest/index.html\">文档</a>\"和<a href=\"https://github.com/piranhacloud/piranha/issues?q=is%3Aissue+-label%3Awontfix+milestone%3A23.3.0+is%3Aclosed\">问题跟踪系统</a>\"。</p><p>&nbsp;</p><p></p><h4>Reactor项目</h4><p></p><p></p><p><a href=\"https://github.com/reactor/reactor/blob/main/README.md\">Reactor</a>\" 2022.0.5是该项目的<a href=\"https://github.com/reactor/reactor/releases/tag/2022.0.5\">第5个维护版本</a>\"，依赖项升级到reactor-core 3.5.4、reactor-addons 3.5.1、reactor-netty 1.1.5、reactor-kafka 1.3.17和reactor-kotlin-extensions1.2.2。</p><p>&nbsp;</p><p></p><h4>Eclipse Mojarra</h4><p></p><p></p><p>Eclipse Mojarra 4.0.2<a href=\"https://twitter.com/OmniFishEE/status/1636829803721392128?cxt=HHwWgICwiZiCmLctAAAA\">发布</a>\"，带来了一些显著的变化，包括：清理MockServletContext类，删除未使用的方法并添加@Override注解；清理ParseXMLTestCase类，删除未使用的方法、变量和注释掉的代码；确保@FacesConfig注解中的version()方法不会返回null；修复了在更新数据表分页标题中的按钮时报NumberFormatException的问题。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/eclipse-ee4j/mojarra/releases/tag/4.0.2-RELEASE\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>Apache软件基金会</h4><p></p><p></p><p>Apache Groovy 4.0.10<a href=\"https://www.mail-archive.com/announce@apache.org/msg08029.html\">发布</a>\"，带来了一些值得注意的Bug修复和改进，包括：来自GroovyScriptEngine类的令人困惑的错误消息；局部变量值未丢弃时的内存泄漏；@Builder注解在JDK 16上不起作用；MissingPropertyException截断嵌套类的类名。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12352914\">发布说明</a>\"。</p><p>&nbsp;</p><p>类似地，<a href=\"https://www.mail-archive.com/announce@apache.org/msg08028.html\">Apache Groovy 3.0.16</a>\"也带来了一些值得注意的Bug修复，包括：无法在JRE 16+的闭包或Lambda表达式上从BiPredicate接口调用方法；使用@CompileStatic注解会混淆静态导入的实例和方法；IllegalAccessException会使用JDK 17和Groovy 3.0.9的默认接口方法。该版本还支持JDK 16。要了解关于这个版本的更多细节，请查看<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12352913\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JHipster</h4><p></p><p></p><p>JHipster团队<a href=\"https://twitter.com/pascalgrimaud/status/1635700948583448582?cxt=HHwWjICw8ZXWlrMtAAAA\">发布</a>\"了JHipster Lite 0.29.0，带来了新特性和功能增强，包括：根据用户反馈删除JHipsterModulePackageJson类的依赖；删除当Cassandra数据库应用程序中正在测试的活动ApplicationContext会话超过四个时的警告消息；新的Redis依赖项和配置。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.29.0\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JReleaser</h4><p></p><p></p><p><a href=\"https://jreleaser.org/\">JReleaser</a>\" 1.5.1（一个简化项目发布的Java实用工具）<a href=\"https://andresalmiray.com/jreleaser-1-5-1-has-been-released/\">发布</a>\"，带来了一些值得注意的修复，包括：添加<a href=\"https://jreleaser.org/guide/latest/reference/assemble/native-image.html\">Native Image</a>\"汇编程序实用工具中缺少的graalVMNativeImage属性；<a href=\"https://jreleaser.org/guide/latest/reference/assemble/java-archive.html\">Java Archive</a>\"实用工具为JAVA_OPTS环境变量生成的错误格式；改进执行外部命令时的错误处理。要了解关于这个版本的更多细节，请查看<a href=\"https://github.com/jreleaser/jreleaser/releases/tag/v1.5.1\">发布说明</a>\"。</p><p>&nbsp;</p><p></p><h4>JobRunr</h4><p></p><p></p><p>JobRunr 6.1.2<a href=\"https://github.com/jobrunr/jobrunr/releases/tag/v6.1.2\">发布</a>\"，主要是修复了两个Bug：当使用MySQL并将useServerPrepStmts属性设置为true时，元数据更新失败，并导致最终关闭；<a href=\"https://www.jobrunr.io/en/documentation/configuration/quarkus/\">JobRunr Quarkus扩展</a>\"中JobRunrDocumentDBStorageProviderProducer类未使用正确配置的问题。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/java-news-roundup-mar13-2023/\">https://www.infoq.com/news/2023/03/java-news-roundup-mar13-2023/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/SZXNxA7DaBzCddCNAUxG\">Java 20 发布，新特性一览：Amber、Loom 和 Panama 项目</a>\"</p><p><a href=\"https://www.infoq.cn/article/P1vXcLlwewcK5XQQIQdN\">Java 近期新闻：JDK 21 序列集合、JDK 20 向量 API、Gen ZGC、Hilla 2.0</a>\"</p>",
    "publish_time": "2023-03-28 09:22:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "超大模型工程应用难？快手给出短视频场景下的实战指南",
    "url": "https://www.infoq.cn/article/jmGdd6N1vgC64KJiKxyp",
    "summary": "<p>号称性能吊打<a href=\"https://www.infoq.cn/video/So2yItKrdYsDZpEG7DjS\"> ChatGPT </a>\"的 GPT-4 近日又一次引爆关注。&nbsp;据OpenAI介绍，当任务的复杂性达到阈值时，二者就会显现差异。它的发布是一件新鲜事，但其背后的多模态大模型技术其实已经发展多年。如今，大模型工程应用的能力成为很多企业关注的重点，也是以 ChatGPT 为代表的预训练大模型广受关注的原因。</p><p></p><p>目前，<a href=\"https://s.geekbang.org/search/c=0/k=%E5%A4%A7%E6%A8%A1%E5%9E%8B/t=\">大模型</a>\"从自然语言处理已经扩展到多媒体视觉、多模态等多领域。近日，在英伟达 GTC 2023 大会上，快手的技术专家张胜卓、韩青长、李杰以多模态超大模型在快手短视频场景下的落地为例，分享了多模态超大模型落地过程中的难点、技术解决方案和性能收益。InfoQ对分享内容进行梳理，以飨读者。</p><p></p><p>主流训练大模型的方法主要会从算法模型架构、分布式并行加速、内存和计算优化三个层面来比较。确定适当的算法和模型架构是大模型训练的第一步。</p><p></p><p><a href=\"https://www.infoq.cn/article/Nou4kpZa2AU5dadx61z6\">快手</a>\"多模态超大模型，采用的是类 T5 的 Encoder + 多 Decoder 架构，通过 Encoder 接受图像、文本多模态语料，学习特征提取能力；通过 Decoder 输出上下文 Embedding 向量，为下游任务提供用户特征信息。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bd0f589f215e68e971b168788df97043.png\" /></p><p></p><p>数据规模和模型容量一直以来是决定深度学习发展的关键指标。条件计算的引入实现了可以在不增加计算量的情况下，增加模型参考量。稀疏门控 MoE 是超大型神经网络的条件计算方法之一。</p><p></p><p>快手多模态超大模型采用的是门控神经网络稀疏 MoE 架构，这种架构实现了模型容量超过 1000 倍的改进，在 GPU 集群的计算效率损失很小，可以实现多场景多任务统一建模的形式。门控网络让快手可以通过对短视频、商业化等不同业务场景构建不同的专家组来选择最合适的专家来处理数据。稀疏门控 MoE 架构吸收信息的能力受限于参数的数量。所以，快手通过扩展专家数量，达到增加模型参数量、提升模型效果指标的目的。目前，快手该模型的专家数量近 200 个，模型参数量达 100B（千亿）。</p><p></p><p>快手希望多模态大模型可以帮助公司构建通用的理解能力，在推荐、搜索、广告等核心业务上取得一些业务收益。那么，多模态超大模型工程应用为什么这么难？</p><p></p><h1>多模态超大模型工程应用，难在哪？</h1><p></p><p>快手技术团队发现，训练时间漫长、推理效率过低、部署相对复杂是多模态超大模型工程应用的三大拦路虎。</p><p></p><h4>1. 训练时间漫长</h4><p></p><p>2017 年 Transformer 结构提出后，深度学习模型参数突破了 1 亿。2021 年末，Google 发布的 Switch Transformer，首次将模型规模提升至 1.6 万亿。与超高速增长的模型参数量相反，GPU 显存增长有限，有数据显示每 18 个月仅翻了 1.7 倍。模型参数量的增长和硬件的显存的增长之间的差距越来越大。但是，超大的计算量和超大的参数量是训练 AI 大模型训练的必经之路。在千亿模型上训练就需要 1.5TB 显存，但快手当时使用的最新硬件 A100 只有 80GB。</p><p></p><p>显存不足只是一方面。越大的模型参数就需要越大的规模训练样本和越长的训练时间。就算有算力的增速，不断扩大的训练数据集耗费的训练时间依然是一般业务团队难以承受的漫长。漫长的训练时间带来的是成本不可控的增长。</p><p></p><h4>2. 推理效率过低</h4><p></p><p>大模型的高效推理是实现大模型工程应用落地的关键所在。相对训练环节，推理环节在计算精度、算力消耗量等方面的要求较低，但显存增长速度的限制同样会出现在推理环节。推理速度受限于通信延迟和硬件内存带宽，所以不仅要在保证低延迟的前提下，尽可能节省计算资源，还要尽可能使用现有显存满足推理的需求，提升推理效率。可现实情况是，100B 模型约需 400GB 显存，但快手技术团队当时使用的主流推理卡 A10 显存仅 24GB。</p><p></p><h4>3. 部署相对复杂</h4><p></p><p>部署决定了多模态超大模型能否成功工程应用。常见的大模型部署难点有参数量过大、计算反馈慢等问题。大模型参数复杂，但在实际应用中却不需要那么复杂的参数和计算。一般企业会对大模型进行精剪、蒸馏、压缩，减少大模型的冗余。快手在部署环节也遇到了多模态数据预处理复杂和 CPU/GPU 负载不均衡的问题。</p><p></p><p>关关难过关关过，快手技术团队是怎么逐步解决这些问题的？</p><p></p><h1>快手多模态超大模型落地技术方案</h1><p></p><p></p><h4>1. 混合并行训练</h4><p></p><p>“训练是我们大模型应用的第一步，也是耗时最久、难度考验最大的一部分。”快手大模型训练和优化专家张胜卓表示。为了优化训练环节，快手采取了混合并行训练的方式，通过并行策略的选择和加速技术的提升来提升训练质量和效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/36ed8c6bd5eac5bf8947f2af040332b2.png\" /></p><p></p><p>目前主流的并行策略包括数据并行技术、张量并行技术、流水并行技术和专家并行技术。快手技术团队在实施混合并行训练时，也发现一些需要考虑的问题。</p><p></p><p>第一，数据并行技术可以将数据分成不同的样本，用不同的设备同时做计算，让每个设备都有一个完整的模型。这样带来的问题是，单个设备上模型可以加载，但是受到显存的制约。所以在多个机器之间并行的时候，快手技术团队通过 AllReduce 来实现多机 / 多卡之间的梯度同步。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b6/b6694a3715a6bcc3ff0f1fc61c737033.png\" /></p><p></p><p>第二，在更新模型的时候，卡与卡之间激活传输产生的通讯开销几乎是一个参数量级别，所以如果把模型做水平方向的拆分，子模型同时并行在多张设备上，就需要考虑层与层之间做计算时的依赖关系（激活开销）。</p><p></p><p>第三，流水并行，指的是在垂直面上切分，把不同的层切成不同子模块，并行在不同的卡上。流水并行是在模型上做并行，并行开销在不同设备上的边界上。常见的流水并行切分策略，一种是通过参数量，保证参数在不同的卡上面相对来讲比较平衡；另一种是基于规则，把不同的 Transformer Layer，按 Transformer Layer 来切分。前者容易忽略不同层算力和计算需求不一样，后者在相同规则的 layer 之间，算力是平行的。</p><p></p><p>但是这里又会出现新的问题，由于在复杂模型场景下，layer 的种类非常多，用简单规则很难匹配，粒度太粗、规则太简单，很难覆盖快手的真实场景。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/59/59c2c1dc15b366dd7c5a6c6f06721af7.png\" /></p><p></p><p>所以，快手技术团队使用了成本模型的方式，把显存和计算作为两个主要的约束做一些平衡，同时考虑显存不会 OOM（显存不足）的场景下，怎么能够把计算做到相对平均，在流水线上面能够没有显著的计算瓶颈。这样做的好处在于，可以达到更细颗粒度的控制。快手技术团队通过这个成本模型来实现自动化的并行切分，相对于简单的规则匹配，性能显著提升了 5%-15%。</p><p></p><p>第四，专家并行是针对于 MoE 架构才有的，不同的 Token 会经过 gate 网络进入不同的专家网络上面输出，再到不同的设备上面。所以在不同的设备上面还有通讯，它的通讯量跟激活值 + 专家数有关系。</p><p></p><p>基于此，快手制定了如下的并行策略。</p><p></p><p>对于千亿模型，由于硬件条件使用的训练环境是单机 8 卡 A100，通过 NVLink 来高速互联。所以快手会优先考虑，把并行成本最高的这部分专家并行放在单机多卡之间。</p><p></p><p>另外，把张量并行和数据并行放在单机多卡，应用于其他层上，这样单机内部通过高速互联可以形成专家的并行、张量和数据的混合并行。另外，快手技术团队在不同的机器之间为了降低通讯量，使用了流水并行，相当于把不同的层切分成不同的 stage，构建这样一个流水线。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/686f5f217af74d48003eb894505609a5.png\" /></p><p></p><p>除了并行策略，快手还从加速技术方面降低整体训练耗时。</p><p></p><p>一方面，快手使用了零冗余优化器，在非 MoE 层上面使用了 ZeRO—2，把 optimizer states 数据和 gradients 数据做了一些零冗余的切分。在不增加通信量的情况下，可以降低 GPU 的显存占用。在 MoE 层禁用了 ZeRO，避免掉跨机之间做通讯的情况。</p><p></p><p>另一方面，快手做了一些 Activation Checkpoint 的细粒度控制。“比如我们在 MoE 层 E 前向的时候会产生一次 ALL2ALL 的通讯，我们得到 MoE 层的输出，如果这时候在反向的时候再做重计算的话，它会重新触发一次 ALL2ALL 的通讯，实际上显存的占用和通讯成本的考量来看，ALL2ALL 成本会更大。所以我们会把 MoE 的细粒度检测点关掉，只在不需要 ALL2ALL 的层上面做一些 Activation Checkpoint。”另外，快手技术团队也集成了一些 ALL2ALL 通信优化库，优化通信性能。</p><p></p><p>此外，快手技术团队还采用了一些通用的优化技术，比如使用了 PowerSGD 梯度压缩算法，可以来提升多机之间的线性扩展比；集成了 FlashAttention 的优化，除了降低显存之外，也提升了计算性能；在 GPUDirect RDMA 等方面也使用了一些技术。</p><p></p><h4>2. 优化显存和实现高效推理</h4><p></p><p>快手多模态大模型的推理优化基于自研的推理引擎开发的。据悉，之所以选择自研推理引擎，一方面是因为在 DeepSpeed 上推理的成本有点高，而且难以集成量化、手写算子等自定义优化；另一方面，由于多模态大模型是一个非标准的网络，还有可能涉及到多卡推理，目前的 TensorRT 生态对多卡推理支持的并不太好。所以，快手自研发了一套推理引擎来作为 TensorRT 的补充方案。</p><p></p><p>快手的这套推理引擎有着高性能、高易用性和支持多卡推理的特点。</p><p></p><p>高性能方面，这套推理引擎支持一些常见的优化的手段，包括内存池的复用、MoE 和一些常见算子的融合、通过 Transformer 里变长序列的计算减少 padding 混合精度，以及 FP16 和 INT8 的混合精度。</p><p>高易用性方面，这套引擎提供了 Operator 和 Tensor 的抽象分装。Operator 实现了像类似 TensorRT 的一个 plugin，用户只需要自定义一些接口就可以；在 C++ 里提供了类似 PyTorch 的一个 API，开发者可以像在 PyTorch 里一样把这些 OP 搭建成一个模型，还可以通过参数来修改网络的结构，实现新 OP 和网络的成本比较低。Tensor 实现了一些新建、拷贝、Delete 等接口的便利性。</p><p></p><p>支持多卡推理方面，通过 Pipeline 并行、Tensor 并行和专家并行实现。</p><p></p><p>大模态大模型如何选择？</p><p></p><p>据韩青长介绍，一个千亿模型在 FP32 精度下大概要占 400GB 左右的显存。快手主要的推理机器是两卡 A10，加起来也只有 48GB 的显存，所以就需要模型压缩技术。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf75d984d76647119f9136b6a6d6b362.png\" /></p><p></p><p>主流的模型压缩技术主要有模型剪枝、知识蒸馏、结构搜索和权重量化等。快手技术团队对比这些压缩技术发现，在迭代周期上模型剪枝大概需要 2 个 base 的训练周期，知识蒸馏需要 1 个 base 的训练周期，结构搜索是大于 2 个 base 训练周期，而权重量化是小于 1 个 base 训练周期，仅在这一项上权重量化优势比较明显。另外，权重量化在 Train From Pretrained 和 Train From Scratch 上效果都会比较好，基于以上这些信息的考量快手选择权重量化作为压缩手段。权重量化对于模型精度几乎是无损的。</p><p></p><p>量化效果如何保证？</p><p></p><p>快手采用的是 QAT 量化训练，通过在训练的时候插入一些伪量化的算子来模拟量化，学习一些量化的 scale，并把这些 scale 传给推理端，推理的时候只需要根据 QAT 学习到的 scale 执行量化的卷积算子以及反量化的计算就可以了。</p><p></p><p>此外，针对 Transformer 类大模型的权重量化策略，快手通过 QAT 量化训练提升效果，并结合推理算子融合规则插入量化算子。</p><p></p><p>第一，使用 QAT 量化训练的时候来提升训练效果。一是对称和非对称的量化方式搭配；二是选择多种策略选择最优的量化截断区间，来减少量化损失。第二，结合推理算子融合规则插入量化算子，比如已知在推理时 bias 会被融合进一些别的算子中，那么在 QAT 的时候就不去对 bias 进行量化，这样就能够提升模型推理速度，同时保证模型效果。</p><p></p><p>最终，快手通过 INT8 量化，将模型参数从 400GB 减小到 100GB。虽然压缩后的 100GB 距离目标的 48GB 还很远，但是已初见成效。</p><p></p><p>为了进一步实现目标，快手基于模型结构设计提出了权重选择性加载的策略，一方面通过对同一模型，在不同场景下只加载对应 group 的 MoE 权重，节省显存；另一方面优化模型加载机制，轮流为每个 op 加载权重，避免将所有权重拷贝到 host memory，节约实例内存。</p><p></p><p>层层手段下，快手成功将单场景部署的模型参数减小到了 40GB 以下。但是，主流推理卡（A10 24GB 显存）还是放不下，快手技术团队还需要继续想办法。</p><p></p><p>多卡推理或许是解决的办法。快手依照各部分的计算时间和参数量设计模型切分策略，选择流水线并行的方式，将模型和数据进行切分，以流水线形式计算，提高 GPU 利用率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/8761bc4c2edf9d1eaf32686873a0549a.png\" /></p><p></p><p>整个模型结构分为两部分，前部是 Feature Extractor，计算时间长、参数量小；后部是 Encoder，包含 MoE 的结构，计算时间短、但参数量大。快手的切分策略是在 Encoder 的靠前位置切一刀，将两部分别放在 GPU0 和 GPU1 上。从上图的 timeline 可以看到，两张卡的任务分配并不够均匀，明显 GPU1 的利用率比较低。因此，快手提出了一种改进的“切分策略二”——将 Feature Extractor 和 Encoder 各自平均分成两半，整个模型分成四部分，分别依次放在 GPU0/GPU1、GPU0/GPU1 上，实现任务分配更加均匀，提高两张卡的利用率。虽然这样做会增加一些通讯开销，但由于通讯量比较小，开销在可控范围内。</p><p></p><p>至此，显存的问题终于得到了解决。</p><p></p><p>计算的角度，如何优化推理呢？快手的答案是 MoE 实现优化和算子融合。</p><p></p><p>模型中的特殊结构主要是 MoE，快手技术团队首先按照 DeepSpeed 的方式对 MoE 进行了实现，但发现了两个可改进点：</p><p></p><p>第一，padding 引起冗余计算，计算量和存储量都与 capacity 参数成正比，capacity 参数是在实际计算过程中根据输入求出来的一个参数，这个 capacity 参数对于某些输入可能会很大，这样的情况下会浪费非常多的计算量和存储空间。</p><p></p><p>第二，DeepSpeed 方案是串行执行 expert 计算的，需要依次启动多个 kernel，GPU 利用率比较低。快手技术团队在测试的时候发现，这个实现下的 MoE 计算占比大约 70%—80%，而且非常容易 OOM（显存不足）。</p><p></p><p>快手技术团队着手优化 MoE 部分的实现，优化这两个问题。</p><p></p><p>第一，将所有 Token 进行重排，不依赖 Padding，避免无效计算。在实测中他们发现，对于一些样例输入计算量能够节省到原来的几分之一甚至几十分之一。第二，在一个 Grouped GEMM kernel 中并行完成所有 Expert 的计算，提高 SM 利用率。下图中的 Expert0 对应有 3 个 Token，Expert1 对应有 2 个 Token，Expert2 对应有 1 个 Token，其实这可以算是三个矩阵乘，并且三个矩阵乘的计算规模不一致，而 Grouped GEMM 就是专门解决这样问题的，它可以用来同时计算 N 个规模不一致的 GEMM。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bddf52d4e2f61b3a794fa2f38babd91.png\" /></p><p></p><p>算子融合不仅可以节省一些访存量，还可以减少 kernel launch 的时间。快手技术团队从 MoE 算子融合和通用算子融合两方面来考虑算子融合的事情。</p><p></p><p>由于 MoE 部分使用了 INT8 量化，所以在两次 Grouped GEMM 前后及中间会有很多算子，如 Permute、Quantize、Dequantize、GELU、Unpermute。快手技术团队最终确定的方案是通用一些 Transformer 常用的算子融合，比如 LayerNorm、QKV GEMM 融合、Bias 和其它一些残差以及 LayerNorm 的融合，将这些连续的 element-wise 算子全都合成一个，整个 MoE 部分的 kernel 数量可以减少到 5 个。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/6783d2edfd1ff1d1581fb3bc7e64b443.png\" /></p><p></p><p>上图就是 MoE 优化后的一个实验结果，对比的对象分别是 DeepSpeed 的实现和 Grouped GEMM 的实现。“这里强调一下，这里并不是说直接调用 DeepSpeed，而是说在我们的自研推理引擎中按照 DeepSpeed 思路实现了一个方案。主要是 DeepSpeed 不支持 INT8，而且在两卡 A10 上是跑不起来的。这里的环境是两卡 A10，均为 INT8 精度。”可以看到，Grouped GEMM 在 Latency 上会取得 7—21 倍的加速比，显存占用可以降低 30%-40%，为大模型的推理上线创造了条件。</p><p></p><h4>3. 优化模型部署</h4><p></p><p>前文提到，部署决定了多模态超大模型能否成功工程应用。快手在部署环节也遇到了多模态数据预处理复杂和 CPU/GPU 负载不均衡的问题。</p><p></p><p>快手技术团队在部署时发现，为了追求最优性能，需要从 Python 训练代码迁移到 C++ 的线上服务代码，在迁移过程中容易出现各种差异，尤其是 CV 方面的预处理，如 Python 的 PIL 库等。为了解决图像预处理时 Python 库和 openCV 存在的 diff 以及加速期预处理，快手技术团队使用 SIMD 和 CUDA 分别实现了一套图像预处理库。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb2fb592ce79f4734ebaf9e49999410b.png\" /></p><p></p><p>以计算引起 diff 的 Resize 操作为例，Resize 主要操作是通过不同的差值算法对图像进行缩放，主要操作是一个重采样的过程，对于目标图像上的一个点选取其源头中的采样区间，通过与不同的差值算法相乘、相加得到，这种乘、加操作非常适合CPU SIMD 和 GPU CUDA 进行加速，CUDA 主要利用其现有模型进行加速，而SIMD 采用了 AVX 并行指令集，将 RGB 通道的数据存入寄存器中，与差值系数进行并行的乘、加操作，实现线上结果与 Python 结果一致。</p><p></p><p>解决预处理 diff 之后，快手技术团队对部署方式进行了优化。</p><p></p><p>对于多模态的数据输入，快手技术团队在预处理阶段进行了并行处理，预处理之后得到的特征数据经过 AutoBatching 尽量凑到最大的 Batch size 数据送入 GPU 进行模型推理，以最大化利用 GPU 的性能，推理结果送入后处理队列等待处理。整个过程中 CPU 和 GPU 都能同时满负荷运行。</p><p></p><p>在线上大规模部署时，为了满足最佳的 GPU 的吞吐，使单次推理的 Batch size 尽量大，将预处理和 Batching 使用的 CPU 机器单独部署，使用更少的节点进行 Batching 操作，以聚合更大的 Batch size，增大流量低峰期的 GPU 数据输入 Batch size，尽量压榨 GPU 性能，配合线上的动态扩缩容机制能够大大提供流量低峰期的资源利用率。</p><p></p><h4>4. 整体收益</h4><p></p><p>在训练层面，快手技术团队基于 DeepSpeed 做了一些深度优化，集成 Megatron、Xformers、Tutel 等高性能库，最终可以实现大模型高效训练。在业务收益方面，相对于快手原本的 10B-T5 模型，100B-MoE 参数量扩大了 10 倍，样本扩大 16 倍，最终只增加了 75% 的训练卡时，在业务指标获得了显著提升。</p><p></p><p>在优化部署模型后，相比于第一个版本上线的 10B—T5 模型，快手 100B—MoE 模型上线之后的模型指标大大提升，在性能上模型变大 10 倍的情况下，在 Batch size 等于 16 的情况下折算到单卡 T4 依旧有不错的加速比，线上实际部署时机器资源在相同的流量情况下节省了将近 60%。</p><p></p><h1>多模态超大模型工程应用，AI 商业化吹响号角？</h1><p></p><p>据悉，不仅仅是技术层面上探索出了大模型从训练到落地的技术实现道路，快手多模态超大模型技术在快手公司内部的推荐、搜索、直播、商业化、电商等多个场景落地，以较低的资源成本取得了显著的业务收益。</p><p></p><p>在 ChatGPT 和 GPT-4 带动下，AIGC 大火。就连 Adobe 都推出了名为“萤火虫”（Firefly）的创意生成式 AI，正式杀入 AIGC 商业化赛道。可以预见，随着 AI 技术的进一步发展，大模型以及多模态模型的商业化应用将进一步加速。我们也期待看到更多多模态大模型工程应用案例的出现。</p>",
    "publish_time": "2023-03-28 09:49:53",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "KMM 技术在移动 App 开发中的探索与实践",
    "url": "https://www.infoq.cn/article/RTmb9YuI0ZfFqRzqZCfT",
    "summary": "<p></p><blockquote>本文整理自百度资深研发工程师袁晗光在<a href=\"https://qcon.infoq.cn/2023/beijing\">QCon 2022 北京站</a>\"的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2023/beijing/presentation/4532\">KMM 技术在移动 App 开发中的探索与实践</a>\"”。</blockquote><p></p><p>&nbsp;</p><p>提升效率永远是软件研发要追求的目标，让代码实现跨平台运行似平就是永恒的主题。当前本该一样的业务逻辑双端需要各实现一遍，最后不仅体验上有着细微的差别，QA 也需要双端各验证一遍；以及由于 UI 代码没有很好的和业务逻辑代码解耦合，导致业务逻辑代码复用困难等，这些影响人效的瓶颈该如何突破?</p><p>&nbsp;</p><p>为什么偏爱 KMM 技术？如何减少为不同平台编写和维护相同业务逻辑代码所花费的时间，同时又能保留 NA 编程的录活性和优势。</p><p>&nbsp;</p><p></p><h1>问题背景</h1><p></p><p></p><p>编码成本高，需降本增效 。</p><p>相同的业务逻辑双端需用不同的语言各自实现一遍 。双端业务逻辑的实现完全一致还很难做到。同样的业务逻辑仅靠口头上或文档上方案对齐导致最终编码实现上有着不小的差别。双端体验上有着不小的差别，数据上产生的不一致性导致不便解释其合理性（交互体验、收益回顾）。双端业务逻辑实现方式不一致导致修改点不能完全对齐、同步 。</p><p>&nbsp;</p><p>后期升级维护、测试成本都较高 。</p><p>UI 代码没有很好的和业务逻辑代码解耦合，造成业务逻辑代码复用困难，不方便做单元测试，组件间的循环依赖增多。业务逻辑变更需要拉上双端的研发都对齐一遍，然后各自编码实现一遍 。同样的业务逻辑双端都需要测试验证一遍 。</p><p>&nbsp;</p><p></p><h1>1. 概述</h1><p></p><p></p><h2>1.1 KMM 基本原理 – 简介</h2><p></p><p></p><p>我对跨平台产生了兴趣，大约在Swift语言刚推出不久，约17年左右，曾在一款创新型的APP上（简单搜索App）进行过一个小实验。那时，我们在做创新类APP方面很活跃，我做了一个类似于云控的模块，云控模块估计在许多APP上都存在。按照以往的做法是iOS 和 Android 双端各出一个RD来开发这个模块，过程中通过设计文档对齐技术方案，但我这次的实验重点在于先用Swift编写了这个云控模块，然后还是同一个人去通过kotlin 来开发这个模块使得其能运行在Android平台上，因为大家都知道这两种语言的语法越来越接近，而不像早期的Objective-C和Java那样，语法格式和其他方面的差异较大。所以在Swift版本开发、测试完成之后，开始逐行翻译每一行Swift代码，用Kotlin代码逐一对应地写出来。最后我发现效果还是不错，在使用Swift编写代码的过程中踩过的坑或遇到的问题，在直接翻译到Kotlin上后这些问题基本上都消失了。只需少量的工作和测试就能在Android上运行，而且问题很少运行得非常好。</p><p>&nbsp;</p><p>因此，我有一个想法就是如果Swfit可以直接跨平台那就太好了。这也是我对语言跨平台最初的兴趣。当然，这与我早年是C++程序员也有关，因为众所周知 C++是可以跨平台的。</p><p>&nbsp;</p><p>如果Swift语言是否能够直接在Android平台上运行就不用再翻译一遍了，两三年前我在偶然的机会下发现了与Kotlin相关的代码和文档，看到了一个名为Kotlin Native的技术。当时官方还没有发布KMM插件，Kotlin Multiplatform Mobile，其中 iOS 端基于 Kotlin Native（简称KN）技术，Android端基于 Kotlin JVM，利用 KMM 技术可以使用 Kotlin 语言技术栈在iOS 和Android应用程序之间共享通用代码，只在必要时编写特定于平台的代码，用来构建统一的业务逻辑代码。</p><p>&nbsp;</p><p></p><h2>1.2 同类跨平台框架对比 – KMM的优势</h2><p></p><p></p><p>无需内置多套引擎(runtime)，包体积增量更少 。对于 Android 开发者无需多学习一套编程语言和编程思想，门槛更低 。基于双端标准组件输出，审核被拒风险较小（iOS）。更强的互操作性， 支持与本地编程语言的双向互操作，可以直接使用现有库，避免了众多基础组件的重复建设。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/67/675507bd5c6f3fa56f58443af312ad9d.png\" /></p><p></p><p></p><h1>2. KMM 开发环境介绍</h1><p></p><p></p><h2>2.1 Android Studio &amp; Xcode 环境配置</h2><p></p><p></p><p>安装工具： Android Studio（建议官方最新版）Kotlin 插件Kotlin Multiplatform Mobile 插件Xcode（12.5 版本及以上）JDK（8 及以上）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/03c3639115d3e6b1697595dab41d7de7.png\" /></p><p></p><p></p><h3>2.1.2 创建跨平台App</h3><p></p><p></p><p>步骤：</p><p>① 在Android Studio中，选择文件|新建|新建项目。</p><p>② 在项目模板列表中选择Kotlin Multiplatform App/Library。</p><p>③ 为应用程序指定名称。</p><p>④ 保留应用程序和共享文件夹的默认名称，并在iOS 框架分发选项列表中选择常规框架。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/58/587c872b1f1a1f726b9d2f6a09ceacb8.png\" /></p><p></p><p></p><h3>2.1.2 项目结构</h3><p></p><p></p><p>三个部分组成：</p><p>① 共享模块：包含Android和iOS应用程序的核心应用程序逻辑：类、函数等。构建到AAR&amp; Framework中 ，使用Gradle作为构建系统（commonMain、androidMain、iosMain ）。</p><p>② iOS应用程序中的Xcode项目。</p><p>③ Android应用程序中的Kotlin模块。</p><p>&nbsp;</p><p>如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/66441c9336f17a0de42868868305f771.png\" /></p><p></p><p>我们可以看到，这个项目主要分为三个模块。首先，我们先看一下名为“shared”的模块，它位于最下面。其次是iOSApp模块，再上面是AndroidApp模块。为什么会有这三个模块呢？因为在我们的核心目标即双端共享代码。什么是共享代码呢？让同一份代码能在Android &amp; iOS 上运行，那怎么实现这个目标呢？简单来说把全部代码分为两个部分，其中一部分就是与平台无关的代码。啥是平台无关的代码呢 ？比如我们要编写一个检验电子邮件的算法，我们认为这个算法的代码是平台无关的，因为输入就是一个字符串，里面的实现就是根据指定的规则来判断这个输入字符串的合法性，期间不涉及任何平台相关特性的访问，比如系统 API 的访问。但是光有与平台无关的代码是不够的，一旦涉及到与平台相关的访问，例如获取设备版本号或硬件信息等，我们就需要独立于平台去完成它们，这些独立于平台的代码在哪实现？这就有了AndroidApp和iOSApp这两个模块。在“shared”模块中，我们包含了核心应用程序逻辑，例如类、函数，并使用Gradle作为构建系统。</p><p>&nbsp;</p><p></p><h2>2.2 特定于平台的API 和实现</h2><p></p><p></p><p>expect/actual 机制 ：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc9c091e964efa2f34879dad4d8aeb04.png\" /></p><p></p><p>KMM 里 expect/actual 机制是非常重要的，因为它提供了一种语法技术来解决平台相关的问题。举例来说，当我们需要在业务逻辑中使用设备的 model 号时，我们需要编写特定平台的代码才能实现。这时，expect 就相当于声明一个协议，它定义了我们期望得到的接口或数据，之后各个平台都需要独立地实现这个协议来满足业务需要。</p><p>&nbsp;</p><p>可能会有人认为这样写了两遍，每个平台都写一遍不如让各个平台自己去写。但是，这种方式只需要建设一次，就像一些基础库一样只需要做一次就能在后面被大家共用。这种共用甚至不限于公司界，整个业界都可以共用一组基础库。</p><p>&nbsp;</p><p>基本流程如下： 在commonMain目录下建立一个expect类或Top-Level方法 , 类似创建一个协议声明。分别在androidMain和iosMain目录中，创建与expect声明（类名、方法名、参数类型及名称）完全一致的实现，否则编译器会报错。</p><p>&nbsp;</p><p></p><h3>2.2.1 如何引用已有组件 - Android</h3><p></p><p></p><p>在使用expect机制后我们解决了平台相关的代码问题。但要充分发挥KMM技术的优势，还有一个重要的点，那就是能否复用已有的组件。因为，如果需要再开发一个新的网络库、数据库或多线程相关的组件成本是很高的，新开发的组件稳定性也是有待考验的 。</p><p>&nbsp;</p><p>KMM技术的一个很重要优势之一就是可以轻松地引用原生平台上已有的组件。例如，我们可以在androidMain后的闭包中按照Gradle规范添加依赖项，然后在androidMain目录下的Kotlin代码中调用依赖库中的类或方法。在这里，我们举了一个自定义组件的例子，引用安卓平台上已有的组件和android 原生开发没啥本质上的区别，关键是如何引用 iOS 上已有的组件呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/533fed47db54d8c8417cfe7f18b0c791.png\" /></p><p></p><p></p><h3>2.2.2 如何引用已有组件 - iOS</h3><p></p><p>&nbsp;</p><p>同样，举个例子如果我想在kotlin代码中判断当前系统是 iPad 还是 iPhone，类似这样的操作该如何实现呢？</p><p>&nbsp;</p><p>为了解决这个问题，我们需要使用官方提供的工具 cinterop，它可以扫描 Apple Framework 并根据 .h 文件获取可调用的类、方法、变量、常量以及它们对应的类型，最终生成 klib 文件。如果需要使用 Swift 的方法，需要添加 @objc 前缀。</p><p>&nbsp;</p><p>要使用 cinterop，需要了解相关的头文件或声明。首先，cinterop 会下载构建依赖的苹果原生 framework，例如 CFNetwork、CoreData、Foundation 等，然后编写 def 文件并配置 cinterop 的编译和链接。最后，使用脚本生成可用于 KMM 的 API。cinterop 工具将生成中间文件 Klib，它包含了扫描到的所有的 framework。经过扫描之后，每个 framework 都会对应一个 Kotlin 可以识别的 Klib 文件，这样 Kotlin 就可以识别这些 iOS 原生组件了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/7362bd53e815c7e850e8fb11cd100600.png\" /></p><p></p><p></p><h3>2.2.3 如何引用已有组件 - EasyBox KMM Gradle 插件</h3><p></p><p></p><p>由于『百度 APP』工程使用 EasyBox 管理依赖及 iOS 工程结构，而 KMM 官方只能够支持 CocoaPods 或上面介绍的直接引用 Framework 的形式实现 Kotlin 与 Objective-C 的交互，如需兼容 EasyBox 组件，在不使用自定义插件的情况下只能使用常规模式（非 CocoaPods）引入依赖库。</p><p>&nbsp;</p><p>但官方提供的Framework引用配置比较复杂，根据官方文档的说明，如果不使用 CocoaPods 引入依赖库的模式， build.gradle.kts 文件内的配置项相当复杂，每次新增依赖时需要做大量的 cinterop 配置，对于新入门 KMM 的开发人员，非常不友好，影响开发效率，甚至破坏工程配置环境 。为简化 KMM 引用 EasyBox 组件的配置流程，做好二进制版本控制，并避免造成代码库文件冗余，我们参考 KMM 官方的 CocoaPods 插件实现，开发了 EasyBox KMM 插件，以便在 KMM 工程中，充分利用现有的各类组件，避免重复建设的问题 ，大家可以结合自己的组件管理工具或源码管理工具来使用。</p><p>&nbsp;</p><p>EasyBox KMM Gradle 插件实现了的主要功能：</p><p>根据配置，自动拉取 Box 仓库（二进制源）中的 Framework 。自动生成并配置 cinterop 所需要的 def 文件，让 KMM 模块能够识别并扫描 Framework 中对外公开的类、方法、常量等元素 。生成带有 cinterop 产物的 klib，并支持发布到 Maven 仓库中，在不具备 cinterop 环境的设备上无需再次进行 Framework 下载和扫描即可复用 cinterop 后的能力 。</p><p>&nbsp;</p><p>EasyBox KMM Gradle 插件的使用：</p><p>有EasyBox KMM Gradle插件，使用过程就相对简单了。首先声明 easybox-kmm 插件，然后在easybox闭包中添加所有依赖的组件，如图所示2个步骤。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23345b273c42f766b970b6170a850b7f.png\" /></p><p></p><p></p><h2>2.3 在已有的工程中集成 KMM</h2><p></p><p></p><p>接下来，我们来看下如何将KMM的产物集成到现有的工程中。由于 KMM 模块在 iOS 平台是以 Framework 的形式产出的 ，所以 在xcode 里面我们可以像使用其他Framework一样引入它，在使用 Framework 中，它的所有协议都是Objective-C格式的，我们可以在我们的工程中像使用其他的API一样使用它。同样 KMM 模块在 Android上是以 AAR 的形式产出的，如下图输出的api 之间的对应关系，名字一致，参数也基本上一致。因此，在已有工程中集成KMM 的产物的便利性也是KMM的一个优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6017ebfcbd06d2adcea388a68952d641.png\" /></p><p></p><p>&nbsp;</p><p>重点技术</p><p>&nbsp;</p><p>基础流程已经介绍完毕，现在让我们来看看重点技术。其中最主要的技术还是多线程处理，它涉及到协程、状态共享、不可变性（frozen）和原子类（Atomic）等。接下来，第二个重点是基础库的建设，主要包括网络库和数据库。</p><p>&nbsp;</p><p></p><h2>3.1 多线程 &amp; 内存管理</h2><p></p><p></p><p>多线程并发开App开发中一直都是个重点和难点，也是我们在实际编码工作中容易出错的地方，这点在KMM 中的开发中也是。所以技术选型是关键，官方提供了多种方式支持多线程，我们也对比了几种方式的优缺点。首先是协程，虽然协程的概念上类似于线程语法简单且概念新颖，但我们当时认为它对Kotlin Native的支持不够成熟和完善，虽然在安卓平台上它仍然使用JVM，但我们最终还是没有选择协程方式去继续探索。</p><p>&nbsp;</p><p>接下下来看第三方库，CoroutineWorker对Kotlin协程进行了封装，但迭代较少，不是很稳定。Reaktive采用RxJava的实现思想，Native（iOS和macOS）底层采用Kotlin Native实现，具有多种功能，但框架相对较重。</p><p>&nbsp;</p><p>最终我们选择的是利用expect/actual加Block方法，Android端可以利用线程池，而iOS端可以使用GCD自行实现。优势是使用了各自平台已有比较成熟的多线程方案，更稳定，运行效率更高 。</p><p>我们认为这是风险最低的方案，因为新技术前期稳定性是首要考虑的因素。因为在原生开发中，iOS的GCD多线程方案已经运行了很多多年，我们对这些多线程的属性和用法也都非常熟悉，这也是我们选择它的一个关键原因，当然不足之处是需要编写一定量平台差异化的代码，但这些都是一次性的基建。</p><p>&nbsp;</p><p></p><h3>3.1.1 模型</h3><p></p><p></p><p>这是我们最终选择的一个与多线程相关的模型，即Common代码。通常情况下，如果我们要执行一个与多线程相关的异步任务，我们将其分发到两种任务队列里面去，串行队列和并行队列。先聊下串行异步队列，将一系列小的任务放入后台串行队列中执行是非常常见的，这对于平台类的应用开发这通常就足够了，因为这些任务比较小单独开一个线程去执行又显得较重，但直接扔进主线程多了又会卡顿UI，有时各任务间又有一定的实效性需要保障。另一方面，如果我们有耗时较高的任务，我们需要将其放入独立的异步线程中执行。我们通常把需要异步执行的任务封装为一个个代码块，然后把它扔到合适的异步任务队列里面去执行。因此，可以看到我们的通用代码分为两条线路，iOS上通过iOSMain的GCD实现，通过dispatch实现。而在Android上，我们则通过BackgroudTaskUtils来实现。在实际应用中，我们可以直接用自己APP中常用多线程组件来替换它，但对外接口上应该还是大同小异的，common 层声明了对外输出的接口层，抹平了双端接口的差异化，再向下就是各个平台独立于平台的实现了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/26/26c26d4228697a906f0957c4dc708d0b.png\" /></p><p></p><p></p><h3>3.1.2 多线程间共享状态的规则</h3><p></p><p>&nbsp;</p><p>我们做移动APP开发的同学可能对线程和对象之间的关系可能还没太多的概念，因此在对象和线程关联方面可能没有形成一个习惯。KN目前主要还是采用了传统的legacy内存管理方式，虽然从1.8开始也开发了一种新的叫做New Memory的内存管理方式，但在本文中我们将重点介绍lex的内存管理方式，先介绍两个规则。</p><p>&nbsp;</p><p>规则1：可变状态仅单个线程可见 。</p><p>状态，就是我们平时所说的变量或者说是属性，可以简单的理解为每个新生成的变量都是和当前线程关联的 ，离开这个线程到别的线程去这个变量就是不可见的，直接访问的话会产生crash 异常。</p><p>&nbsp;</p><p>规则2：不可变状态才多线程可见 。</p><p>&nbsp;</p><p>如果我们的应用程序永远只有一个线程，那所有的状态都是可变的就够了，但这是不现实的，所以如果希望一个状态能被夸线程访问或修改那它应该是不变的 , 这样多个线程才可以安全的访问它.</p><p>&nbsp;</p><p></p><h3>3.1.3 什么是不变和冻结状态 ？</h3><p></p><p>&nbsp;</p><p>KN 定义一个新的运行期状态, 称为冻结(Frozen)。一个对象的任何实例都可以冻结. KN 运行期对所有类添加了一个扩展函数&nbsp;Freeze() . 调用它将会冻结一个对象, 并递归的冻结这个对象引用的所有对象. 如果一个对象被冻结, 那么你不能改变它的状态的任何部分. 尝试这样做会导致一个运行期crash。</p><p>&nbsp;</p><p>被 freeze 的实体，其所有属性都不可被修改 。被const修饰的常量默认都是被冻结的状态。在一个对象上执行 freeze()会变成 frozen 状态，该对象所引用的其他对象都会变成frozen状态（含容器）。freeze() 操作是不可逆的，如果需要获得非 frozen 的对象，只能将原先被froze 的对象进行深拷贝，成为一个新的对象 。</p><p>&nbsp;</p><p>当一个对象被freeze后，其子对象也会被冻结。换句话说，一旦一个对象被冻结，它引用的所有子对象、数组列表等也将被冻结。当运行期需要检查一个对象是否可以被其他线程访问, 它只需要检查对象是否被冻结。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/16/16974f042a55b7498a8b6194d3132a20.png\" /></p><p></p><p></p><h3>3.1.3 Object单例</h3><p></p><p>&nbsp;</p><p>Object单例： 默认在创建后就会被 freeze，对所有线程可见，但如果修改其内部引用，将会导致报错。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c30227d61d0f113a46eade471650216.png\" /></p><p></p><p></p><h3>3.1.4 Top-level 属性</h3><p></p><p>Top level 属性：默认仅对主线程可见，且是可变的，从其他线程访问将引发异常。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/da/dafa6e5136e4af8d3ffecde1ae7e3a18.png\" /></p><p></p><p>注：Top Level 的 val 变量被认为是 const ,默认也是 frozen 状态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/50/506be29c17ea6eff050495d9fa6ec4da.png\" /></p><p></p><p>&nbsp;</p><p>注意事项：Top-level 属性必须在主线程初始化！</p><p>&nbsp;</p><p></p><h3>3.1.5 两种注解来标注 Top level 属性</h3><p></p><p>&nbsp;</p><p>① @SharedImmutable：该注解可以让Top Level属性全局范围可见，但是被 freeze 之后无法修改。（此注解只能用于val属性）</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef2a86e0a9965cbabff665b2ad3daafc.png\" /></p><p></p><p>&nbsp;</p><p>② @ThreadLocal：给每个线程提供独立的可变副本。（此注解可用于 val、var 属性）。</p><p>如果需要在不同的线程中对其进行修改，用 @ThreadLocal 对其进行注解，这将允许它是可变的，并为每个线程提供其状态的副本，如何让多线程共享修改同一个对象的属性，将在后面提到。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/ee30eeb30341b70a49d9353223696309.png\" /></p><p></p><p></p><h3>3.1.6 如何实现可变性？</h3><p></p><p></p><p>前面我们讨论清楚了如何实现状态的多线程可见，但实际项目中仅可见是不够的，@ThreadLocal 只用来修饰top level属性，且只是为每个线程提供一份独立的副本，那如何让多线程能修改共享的属性呢？即可变性。</p><p>&nbsp;</p><p>① Atomics（原子类）</p><p>② Thread-isolated&nbsp;states（线程隔离状态）</p><p>③ Low-level capabilities (略)</p><p>&nbsp;</p><p></p><h3>3.1.7 Atomic （Stately库）</h3><p></p><p></p><p>为了能够让多线程同时读取、修改一个冻结的状态中的值， KN 提供了一组Atomic类库。</p><p>1、对简单数值使用AtomicInt/AtomicLong：持有共享的 Int/Long允许多个线程读写。示例中SomeState是一个全局的object,在KN中它默认是冻结的，左边的 count++ 会产生异常，而右边的 increment() 是可以在任何线程中正常工作的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2e/2ed05f82d4acfcf8d0ce12a7bf0b9362.png\" /></p><p></p><p>&nbsp;</p><p>2、AtomicReference ： 持有一个对象实例允许多个线程读写，持有的对象必须是frozen状态的。</p><p>&nbsp;</p><p>示例：</p><p><code lang=\"null\">data class SomeData(val i: Int)</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f77f8a9b3e2a36f4de515835bf6c36a.png\" /></p><p></p><p></p><h3>3.1.8 实际应用便利性技巧</h3><p></p><p></p><p>采用Atomic封装的状态，取值、赋值的过程较繁琐 ，可以通过对外暴露一个普通的属性，然后修改其get/set 方法来实现Atomic封装 。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2409940915749db074f178bebbf1c8c.png\" /></p><p></p><p></p><h3>3.1.9 AtomicReference 的内存泄露</h3><p></p><p>&nbsp;</p><p>循环强引用 &amp; 内存泄露是我们经常遇到的一个问题，在KN中也不例外，比如当我们定义了一个 Kotlin 对象时，这个对象需要在 iOS 的 ViewController 中使用，对它有了第一个持有引用。由于在 ViewController 中会持有这个kotlin 对象，同时为了使用的便利性AtomicReference对象也会引用这个 ViewController对象而产生循环引用，这时候我们应该在&nbsp;AtomicReference 中使用一个可为 null 的类型, 并在使用完毕时, 明确的设置为 null. 这样就可以这个循环引用导致的内存泄露了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9d8789db35b93b1d59d1185225a03b2.png\" /></p><p></p><p>接下来是性能问题 。和一个标准的可变状态相比，访问并修改AtomicReference内引用的值是比较耗时 的，尤其是在频繁对引用的对象进行修改操作的情况下会对性能造成更大的影响，主要原因是跨线程导致，出现这种情况的时候不再建议使用该类实现，如果需要实现 MutableList 或 HashMap 时，该如何操作？我们引入线程隔离状态 。</p><p></p><h3>3.1.10 线程隔离状态</h3><p></p><p></p><p>隔离状态：将可变状态隔离到单个线程，并允许其他线程与该状态通信。比如一个列表对象，需要经常进行增删改查操作，用AtomicReference<!--?-->封装会导较大的致性能问题。这时，我们可以通过 Iso 提供的相关功能来解决这个问题。</p><p>&nbsp;</p><p>IsoArrayDeque 的用法 ： 创建一个对线程具有独占访问权限的工作队列，并创建一个仅存在于该线程中的可变状态， 其他线程通过调度工作队列上的工作与可变线程进行通信。</p><p>&nbsp;</p><p>AtomicReference&gt; 和 IsoArrayDeque 该用谁 ？</p><p>① 对于单个的值，AtomicReference 是更好的选择。</p><p>② 对于值的集合，使用线程隔离状态是更好的选择， 主要的性能损失实际上是跨线程。</p><p>&nbsp;</p><p></p><h2>3.2 基础库建设 ， 网络、数据库 …</h2><p></p><p></p><p>最后是基础库的建设，要发挥新技术的优势建设强大的基础库是关键。为此，我们充分利用『百度 APP』沉淀的各类组件和能力， 避免了重复建设。 双端接口差异化抹平，使用起来也更方便，更有利于以保证双端逻辑代码编写的一致性 。</p><p>&nbsp;</p><p>例如 SQLDelight可以很方便的实现跨平台操作数据库的增、删、改、查，但是引入新库往往又会增加应用包体积，这些都是我们在做技术选型的时候要优先考虑的点，我们也可以采用 expect/actual 机制来定义接口协议，并用原生的实现来解决这个问题。我们也采用各自平台原生的实现（GCD）来解决多线程问题；对于我们必用的KV存储，底层使用了SwanKV和MMKV；Json解析库使用了Serialization来实现，我们也正在优化Json解析库的包体积。更底层的StringUtls和Log等都需要一起建设，尽管这些建设都是一次性的，但要做到业界共享目前还是有困难的，因为各公司的底层基础库还是有较大的差异，需要桥接到上面就需要各个团队开发自己的底层实现。所以，在实际更复杂的业务场景中，需要一套丰富、稳定、可靠的「跨端」基础能力也有不少的工作量，但随之逐步的完善KMM的核心效率优势就逐渐发挥出来了。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89a5c750ec7e0f5f8b58fdad2d2fd8b9.png\" /></p><p></p><p>&nbsp;</p><p>落地情况</p><p>关于技术的应用落地，我们其实从 2021 年开始探索。最初我们落地了一个创新类的 APP，但目前该 APP 已经停止服务。接着，我们又成功地在百度 APP 中应用了该技术，大字版也于去年的 5 月份推出该技术。目前，我们已经在百度 APP 中应用了几个重要的业务场景，包括首页、一级 Tab 等。由于一些业务逻辑的共用，这些场景在运营中显得尤为重要。</p><p>&nbsp;</p><p>就落地情况而言，我们发现在我们正在开发的这个新业务中，UI 交互占比较大。目前的数据显示，其占比大约为 33% 左右，因此我们认为这个领域还有很大的发展空间。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/13/13eb67ab6a7ff9435750b4446f0d0d4b.png\" /></p><p></p><p>&nbsp;</p><p></p><h1>嘉宾介绍</h1><p></p><p>&nbsp;</p><p>袁晗光，百度资深研发工程师。软件开发 16 余年，2013 年入职百度，目前负责百度 App 和其他多个创新类 App 客户端功能迭代与架构方向，长期从事研发质量效率的平衡探索。</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ==&amp;mid=2650847427&amp;idx=1&amp;sn=83dc2c1868e29dd24824f1aa90fbeb40&amp;chksm=80b76c5db7c0e54b4135f0d3e1d01271579a9272a64f4df8e1ab1f79cd5664f5377c3b48fad2&amp;scene=27#wechat_redirect\">KMM 、 Compose 、 Flutter “神仙打架”？了解下现状</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ==&amp;mid=2650847146&amp;idx=1&amp;sn=4454989511dc4f1cfc1e1f1c5ab41093&amp;chksm=80b76334b7c0ea228793be6f3e9f7b6435f5a2a2c7feb184e2d24381f582ce7884ab696056c1&amp;scene=27#wechat_redirect\">KMM跨平台开发入门，看这一篇就够了~</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651116150&amp;idx=4&amp;sn=7cd0fe571494ceae161f4ccc008f9f59&amp;chksm=bdb92a258acea333aa9c4f807067d74ea8f95e3446ccbb47b53d3aee552c49ff6a43bf379833&amp;scene=27#wechat_redirect\">跨平台框架新进展：Weex2.0、Waft、KMM ｜GMTC</a>\"</p><p><a href=\"https://www.infoq.cn/article/tTiZu8E4MNXzJvd18YI2\">热门方向 Top4：大前端监控、移动端性能与效率优化、团队可持续发展、低代码</a>\"</p>",
    "publish_time": "2023-03-28 10:09:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "我被 React 劫持了，很痛苦又离不开",
    "url": "https://www.infoq.cn/article/CZKMjHaxbf1Z7xcSzisX",
    "summary": "<p></p><h2>前言</h2><p></p><p></p><p>如果几年前我写的这篇文章，可能会被视为亵渎。但如今越来越多的人表达了对 React 的不满，我终于可以表达我的观点了。坦率地说，我不太喜欢 React，大多数人也是这样的想法，即使你现在还没有任何不满情绪。</p><p></p><p>我已经使用 React 很长时间了，但实际上我更喜欢其他替代方案。尽管我是一名“React 开发人员”，过去用它，现在用它，可能将来还会用它，我似乎无法完全摆脱它。也许你认为我会在一段时间后不再关注，但一旦看到其他选项，我就会感到困惑，不知道为什么自己仍然继续使用 React。</p><p></p><p>当 React 刚出现时，一切都显得如此优雅。组件代码注重 行为局部性。DOM 是内联的，就在事件处理程序旁边。代码看起来是模块化的，很整洁。</p><p></p><p>类组件</p><p></p><p><code lang=\"null\">class MyComponent extends React.Component {  constructor() {    super();    this.setState({ num: 0 });  }  handleClick = () =&gt;     this.setState({ num: this.state.num + 1});  render() {    return (      <button>        Clicked {this.state.num} times!      </button>    )  }}\n</code></p><p></p><p>随着教程的发布、著作的出版以及大师的诞生，React 以前所未有的速度建立起了自己的帝国。它提供了无与伦比的开发体验，但更好的是，容易理解。最初尝试 React 的是活跃分子、思想影响者以及我们中最为书呆子的人。这个库很快就流行起来了，这也不难理解，因为它的语法非常简单。</p><p></p><p>于是我们都加入了进来。然后，我们有了钩子（Hooks）！万岁！说实话，它们一开始让人感到有些陌生。但那种清新的感觉！React 变得更加优雅了！它使输入更快、迭代更快、阅读更容易。React 是正确的选择。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/bd/bd0e48a5c426c4eb92dd0172ad7b8eef.png\" /></p><p></p><p>函数组件</p><p></p><p><code lang=\"null\">const MyComponent = () =&gt; {  const [num, setNum] = useState(0);  const handleClick = () =&gt; setNum(num + 1);  return (    <button>      Clicked {num} times!    </button>  )}\n</code></p><p></p><p>几年过去了，我们开始注意到，有许多其他可选项出现了。每个月都会有新的名称出现。当时我们嗤之以鼻，认为这些新潮项目永远不会流行起来！我们认为 React 已经经过战斗考验，完美地满足了我们的需求，其他库没有留下任何空间。</p><p></p><p>然后人们开始说话过于积极，过于一致。感觉每个人都对他们的新东西感到满意，对旧东西感到不满意。但那些只是过眼云烟的潮流！从 2015 年发布到 2019 年的迭代版本，没有什么能像 React 那样经得起时间的考验，而且 React 仍在不断发展。</p><p></p><p>你知道的，React 感觉不再那么稳定了。我是说，或许它是最好的选择，但谁知道呢？我以为我们已经完成了迭代，但还是不断地被一些新变化所吸引。也许还有更好的方法，只是我还没看到！看看也无妨。</p><p></p><p></p><h2>React 与 Hooks</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0a/0ad44de855d53a7b8eb101bb75ed9f93.png\" /></p><p></p><p>尽管 Hooks 并不是 React 本身的必要组成部分，你仍然可以使用类组件，但 Hooks 已经成为了 React 生态系统中不可或缺的一部分。本文所提到的 React 模型指的就是“React Hooks”。</p><p></p><p>最近，我注意到人们对 React 生态系统向 Hooks 的平稳转换感到惊讶，因为现在每个人都一致认为 Hooks 带来了许多好处。然而，我还记得不久前在这个问题上曾有过激烈的争论。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/13/1399353fff97039cb753971afdc257e9.png\" /></p><p></p><p>我不是非要和那些反对 React Hooks 的人站在一起，但我确实认为他们的担忧是有一定道理的。React Hooks 存在于一个由类组件控制的环境中，为了实现这种转换，React 必须与类代码完全兼容，而事实也确实如此。这种兼容性，再加上 Hooks 所带来的可组合性和可读性的提升，使得整个行业采用 Hooks 的速度超出了预期，这也是 Hooks 带来的两项重要改进。</p><p></p><p>我坚信，“可组合性优于继承”，虽然不是所有人都同意这种观点，但我认为通过函数共享行为是对类继承的巨大改进。至于可读性，虽然代码的长度可能与易读性没有直接关系（例如 Regex、代码高尔夫），但 React Hooks 提高了代码的“行为局部性”。</p><p></p><p>这意味着，Hooks 可以减少查看那些比较小的组件的次数。事件监听器、状态转换和渲染输出等细节让人应接不暇，而 React Hooks 对此进行了改进。我发现，函数组件写起来更快，读起来也更容易。</p><p></p><p></p><h2>可读性与复杂性</h2><p></p><p></p><p>但乍看起来，可读性本身与复杂性并没有直接的联系（至少直觉上是这样）。Hooks 通过本地化行为降低了复杂性，但必要的抽象又增加了复杂性。</p><p></p><p>我经常想起 Amos 那句断章取义的话。</p><p></p><p></p><blockquote>或者更确切地说，那个说法半真半假，它掩盖了这样一个事实：当你把某件事情变得简单时，其实是把复杂性转移到了别处。Amos（简单就是个谎言）</blockquote><p></p><p></p><p>当我们抽象复杂的系统时，不是在消除复杂性，而是在转移它。在我们的例子中，复杂系统不是 前端开发，而是 React。</p><p></p><p>Hooks 改变了我们的心理模型，它让我们考虑状态转换和同步，而不是生命周期。或者，至少它的目的是这样的。</p><p></p><p><code lang=\"null\">componentDidMount → useEffect(func, [])componentWillUnmount → useEffect(() =&gt; func, [])componentDidUpdate → useEffect(func, [props])\n</code></p><p></p><p>这一举措有一些性能上的损失——这个问题可以通过 Hooks useMemo和useCallback得到缓解。我并不是说在 Hooks 出现之前，React 中不存在记忆化（memoization）。它是存在的（React.memo()）。我是说，由于本地化行为的改进，我们现在必须记住状态初始化和状态转换。</p><p></p><p>社区中经常会有关于 React 记忆化的讨论，而且与其他框架相比，这类讨论更多。在所有框架中，值缓存都很重要，但是 Hooks 将很多决策留给了组件作者，而不是核心库。</p><p></p><p>我们稍后会详细介绍。但在此之前，我想花点时间讨论一下 心理模型。</p><p></p><p>不管是 React 的文档，还是 YouTube 上的视频，都经常提到这个模型，这也是实际上正在发生的事。或者说，至少存在一种更符合实际行为的心理模型，我认为这一点很重要。</p><p></p><p></p><h2>更好的心理模型</h2><p></p><p></p><p>在关于 React 的讨论中，经常提及“VDOM”这个术语。然而，Dan Abramov 似乎不喜欢它。我赞同他的看法，因为 VDOM 并不是 React 的决策因素，而是其结果。这一点在讨论直接差异时也很容易理解。</p><p></p><p>因此，我们应该将重点放在如何保持 React 组件的“纯净性”上，而不是关注 VDOM。一想到组件有状态，这个术语似乎立马就不合时宜了。因为状态似乎与纯函数的概念不符——对于给定的一组输入，无论调用多少次，无论以何种方式调用，输出都应该相同。</p><p></p><p>纯函数</p><p></p><p><code lang=\"null\">// 纯函数const getPlusOne = (num) =&gt; num + 1;// 非纯函数const getDateNow = () =&gt; Date.now();\n</code></p><p></p><p>关键在于理解 React 中的‍‍状态并不存储在组件中。</p><p></p><p>状态是另一种输入。</p><p></p><p>在 React 中，调用useState是另一种接收输入的方式。状态存在于 React VDOM/ 状态树中。组件有严格的调用顺序，useState将从提供的栈中弹出输入。</p><p></p><p>state &amp; props = inputs</p><p></p><p><code lang=\"null\">const Component = ({ color }) =&gt; {  const [num1] = useState(0); // receive next state argument  const [num2] = useState(0); // receive next state argument  const [num3] = useState(0); // receive next state argument  return </code></p><div><code lang=\"null\">{num1} + {num2}</code></div><code lang=\"null\">}\n</code><p></p><p></p><p>state 和props 都是一种输入。调用setState是向 React 内部发信号，而不是直接更改。</p><p></p><p>这些信号将依次更新其组件 状态栈 并重新运行组件。给这个组件提供新的输入，它将产生一个特定的输出。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ff/ffe5ce14f8701f97c4a482e9b983346f.png\" /></p><p></p><p>React 组件对 React 来说也可能是个黑盒，因为其内部行为是不可见的。相反，我们可以将组件本身视为反应式对象，而不是单个状态块。这也是为什么人们认为 React 的反应模型不是“细粒度”的原因。</p><p></p><p>因此，React 需要一种方法来避免在每次更新时重写整个 DOM。因此，它会在新的更新上运行一个差异对比过程，以判断哪个 DOM 节点需要更新。也许没有不同之处。也许全不相同。不检查就没法知道。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/29/29ceb0755a26d9d4914206992f0123b9.png\" /></p><p></p><p>这是 React 渲染器和调和器之间的关系。这是“纯组件”行为。状态更新和 DOM 更新之间缺少直接的联系。</p><p></p><p>在 React 中，组件是真实的，而 DOM 不是。也许这就是为什么 React 对于 非 Web 渲染器 来说是一个很好的选择。我们可以使用 React 来绘制 UI 和更新，但把更新应用到 UI 的过程替换掉。</p><p></p><p>作为一名 Web 开发人员，这算不上是一个多大的好处。</p><p></p><p></p><h2>发现陷阱</h2><p></p><p></p><p>作为一名 React 新手，你最先遇到的障碍会是类似这样的东西。</p><p></p><p>死循环</p><p></p><p><code lang=\"null\">function MyComponent() {  const [num, setNumber] = useState(42);  // 死循环  setNumber(n =&gt; n + 1);  return </code></p><div><code lang=\"null\">{num}</code></div><code lang=\"null\">}\n</code><p></p><p></p><p>试图在组件的顶层进行状态更新将导致无限循环。状态更新会重新运行组件。这并不意味着 DOM 更新，但确实意味着另一个状态更新，状态更新将再次触发重新运行，重新运行将再次触发状态更新，再次触发重新运行，就这样不断进行下去。</p><p></p><p>可能你很快就会发现这个 Bug。像这样的无限循环并不难发现。</p><p></p><p>当你开始使用 React Context 并在父组件中发出更新信号时，情况会变得更加复杂，因为会出现级联渲染。这意味着一些组件可能需要重新挂载，导致状态更新被延迟数秒钟。这种情况很常见，值得单独写一篇文章来深入讨论如何修复 React 中的这个问题。不过在本文中，我并不打算探讨这个问题。</p><p></p><p></p><p></p><p>我们将继续讨论作为反应式对象而不是状态存在的组件。这种模式有一些后果。</p><p></p><p>模拟表单组件</p><p></p><p><code lang=\"null\">const MyForm = () =&gt; {  const [text1, setText1] = useState('');  const [text2, setText2] = useState('');  const [text3, setText3] = useState('');  return </code></p><form><code lang=\"null\">    <input type=\"text\" value=\"{text1}\" /> setText1(e.currentTarget.value)} /&gt;    <input type=\"text\" value=\"{text2}\" /> setText2(e.currentTarget.value)} /&gt;    <input type=\"text\" value=\"{text3}\" /> setText3(e.currentTarget.value)} /&gt;  ;}\n</code></form><p></p><p></p><p>为了让你感到轻松一些，我对这个组件进行了大幅简化。然而，任何使用过 React 表单的人都知道，它们通常会更加复杂。</p><p></p><p>我曾经看到一些表单组件达到了 300 多行的代码量。这些组件涉及到状态转换、验证和错误视图等多个方面。但是，很多特性都是表单本身固有的，而不是 React 独有的。只是，React 往往会使事情变得更加复杂。</p><p></p><p>需要记住的是，组件是反应式的，而不是状态本身。当使用 受控输入 时，每次按键输入都会导致“重新渲染”，这意味着不管是否触及状态，都可能运行状态计算代码。</p><p></p><p></p><blockquote>VDOM 可以解决所有问题！</blockquote><p></p><p></p><p>这似乎是一种流行的 VDOM 误解。VDOM 的作用是避免不必要的 DOM 更新，而不是避免状态计算。</p><p></p><p>组件是一个函数，每次需要检查更新时，它‍都会重新运行。虽然可能没有触及 DOM，但代码仍然会被执行，而实际上它并不需要被执行。</p><p></p><p>设想有下面这样一个组件。</p><p></p><p>自定义输入封装器</p><p></p><p><code lang=\"null\">const MyInput = ({ label, value, onInput, isError, errorText }) =&gt; {  const labelText = label ? toTitleCase(label) : 'Text Input';  return &lt;&gt;    <label>      <span>{labelText}</span>      <input value=\"{value}\" />    </label>    {isError &amp;&amp; </code></p><div><code lang=\"null\">{errorText}</code></div><code lang=\"null\">}  &lt;&gt;;}\n</code><p></p><p></p><p>我认为这个例子更加贴切。对于提供的标签输入，我们决定将它们转换为“首字母大写（Title Case）”。</p><p></p><p>目前还好。我决定不对其进行记忆化，因为这个计算看起来很简单。但如果情况发生变化呢？</p><p></p><p>如果 toTitleCase 变得越来越复杂呢？也许，随着时间的推移，我们会不断添加新功能，创建出终极的 Title Caser™️！</p><p></p><p>有新的自定义输入的表单</p><p></p><p><code lang=\"null\">const MyForm = () =&gt; {  const [text1, setText1] = useState('');  const [text2, setText2] = useState('');  const [text3, setText3] = useState('');  return </code></p><form><code lang=\"null\">     setText1(e.currentTarget.value)} /&gt;     setText2(e.currentTarget.value)} /&gt;     setText3(e.currentTarget.value)} /&gt;  ;}\n</code></form><p></p><p></p><p>现在，每次按键时，每个组件都会重新运行toTitleCase。useState使得整个表单组件可以对任何状态变化做出反应！</p><p></p><p>噢，不！</p><p></p><p>我是说，这有问题吗？浏览器的速度非常快。硬件的速度也非常快。也许这不是问题。</p><p></p><p>好吧，直到它成了问题。</p><p></p><p>在不同的地方逐步增加计算不会造成太大的伤害。但一直这样做下去，最终就会变得缓慢。现在，你必须面对这样一个问题：性能问题的源头不止一个——哪都可能导致性能问题。解决这个问题所要做的工作会远远超出你的想象。</p><p></p><p></p><blockquote>你是不是忘了 useMemo ？</blockquote><p></p><p></p><p>啊，是的。那……</p><p></p><p></p><h2>记忆化</h2><p></p><p></p><p>在这个问题上，我无疑希望人们达成了共识。然而，有一篇支持记忆化的文章，就会有一篇反对记忆化的文章。</p><p></p><p>要知道，记忆化有性能成本。</p><p></p><p></p><blockquote>Dan Abramov 反复指出，记忆化仍然会产生比较 props 的成本，并且在许多情况下，记忆化检查永远无法阻止重新渲染，因为组件总会接收新的 props。例如，可以看下 Dan 的推特讨论。—— Mark Erikson（React 渲染行为（大部分）完整指南）</blockquote><p></p><p></p><p>讨论提到了 React.memo()，这是 React 中一种略有不同的记忆化形式。</p><p></p><p><code lang=\"null\">const MyInputMemoized = React.memo(MyInput);\n</code></p><p></p><p>记忆化整个组件可以使渲染级联不再检查它的子组件。把这个作为默认设置似乎很合理，但 React 团队似乎认为，比较 props 的性能成本超过了大规模渲染级联的平均性能成本。</p><p></p><p>我觉得那可能是错的。Mark 似乎同意这个观点。</p><p></p><p>这也使得排版看起来更加丑陋。我见过的大多数代码库都倾向于避免使用React.memo()，除非非常确定它可以显著提高性能。</p><p></p><p>另一个反对记忆化的论据是，当父代码编写不正确时，React.memo()很容易失效。</p><p></p><p>模拟的 React.memo() 示例</p><p></p><p><code lang=\"null\">// 记忆化以防止重新渲染const Child = React.memo(({ user }) =&gt; </code></p><div><code lang=\"null\">{user.name}</code></div><code lang=\"null\">);function Parent2() {  const user = { name: 'John' };  // 无论如何重新渲染  return ;}\n</code><p></p><p></p><p>我们用速度最快的方式（浅相等）比较 props。每次重新渲染看起来都像一个新的 prop。由于重新渲染很常见，所以我们需要注意这一点。</p><p></p><p>这里，组件是反应式“原语”，我们可以通过 在组件中移动状态 来修复一些记忆化问题。</p><p></p><p>在创建一个产品时，我并不是很喜欢这种讨论。</p><p></p><p></p><blockquote>是的，我说的是 useMemo()，不是 React.memo() 。</blockquote><p></p><p></p><p>对于 useMemo()，我们也有同样的性能考量。现在，成本变成了比较“依赖关系”，而不是 props。</p><p></p><p>模拟的记忆化示例</p><p></p><p><code lang=\"null\">const value = useMemo(() =&gt; {  const items = dataList    .map(item =&gt; [item, placeMap.get(item)])    .filter(([item, place]) =&gt; itemSet.has(place));  return pickItem(items, randomizer);}, [dataList, placeMap, itemSet, pickItem, randomizer]);\n</code></p><p></p><p>不要在上面的代码上花太多时间。那只是为了演示。</p><p></p><p>但是你有没有注意到，事情有些怪异？有两个单独的状态变换。一个是列表操作，另一个是在结果数据上调用某个函数。</p><p></p><p>我们不小心记忆化了过多内容！如果randomizer &nbsp;变了会发生什么？重新运行整个函数？！我们应该写成下面这样。</p><p></p><p>恰当的记忆化</p><p></p><p><code lang=\"null\">const items = useMemo(() =&gt; {  return dataList    .map(item =&gt; [item, placeMap.get(item)])    .filter(([item, place]) =&gt; itemSet.has(place))}, [dataList, placeMap, itemSet]);const value = useMemo(() =&gt; {  return pickItem(items, randomizer)}, [items, pickItem, randomizer]);\n</code></p><p></p><p>现在，我们的价值更明确了。对randomizer的更改不会重新运行.map和.filter，只会重新运行pickItem调用。</p><p></p><p>得救了！……是吧？</p><p></p><p>当涉及到列表操作时，我通常会倾向于自动记忆数据，但我并不确定这是否是一个合理的做法。这种记忆化的最大问题在于它会让代码变得更加 复杂难懂，可能被称为“代码气味”。</p><p></p><p>尽管记忆化有时可能会有所帮助，但我们必须同时注意组件的使用和组成，才能明确它是否适用于特定的情况。</p><p></p><p>记忆化的复杂性并不是 React 所独有的，但我们需要手动处理它的频率远高于必要的次数。</p><p></p><p>记忆化确实是一种解决问题的方法，但很难确定何时以及何处使用它，这可能会令人沮丧。它的有效性设计得真的很差。</p><p></p><p></p><h2>教学方法</h2><p></p><p></p><p>这是我喜欢关注的。多年来，我一直把研究编程教学方法当成一种爱好。我一直在关注一个问题：</p><p></p><p></p><blockquote>如何最有效地传达编程概念？</blockquote><p></p><p></p><p>我想我还没有答案，但我知道你的做法正相反。</p><p></p><p>在传统的 React 教学中，它被描述为一个简单的组件系统，状态连接到 UI 并在随时间更新。然而，我曾有幸教授一些不太熟悉框架、React 或编码的人。我发现，React 并不简单，而模糊的教材使得它更难学。</p><p></p><p>对于那些使用 React 时间较长的人来说，我们所讨论的概念和心理模型可能不再需要解释。然而，对于大多数人来说，这些概念并不那么容易理解。</p><p></p><p>例如，组件在状态更新时会重新渲染，但这并不是显而易见的。状态的使用并没有对应的名称，那么它是如何保存的呢？</p><p></p><p>实际上，状态保存在 VDOM 栈上，这也解释了为什么组件的顺序很重要。状态是组件的输入，而状态突变是向树发送信号，由树再次调用函数来比较输出差异。你是否理解了这个过程呢？</p><p></p><p>随着时间的推移，你是否也注意到了这些问题呢？也许你读了一篇文章，看了一段视频，或者你比我聪明很多。</p><p></p><p>与其他同时代的替代方案相比，状态更新的复杂性在 React 开发中经常成为一个障碍。然而，这些必备的概念却往往是被作为 高级主题 来教授的。</p><p></p><p>在这方面，我希望新的 React 文档 会做出改变。我也希望人们能够认识到，有很多初学者更喜欢通过视频来获取信息，而不是冗长的教程。</p><p></p><p></p><h2>修复 React</h2><p></p><p></p><p>我想重新探讨下这个表单。</p><p></p><p>这种痛苦由来已久，以至于表单最佳实践出现了一些变化。不受控输入现在大行其道。</p><p></p><p>组件更新源于状态更新。受控输入会强制对每个表单交互进行状态更新。如果只是为了让表单可以做任何事，那么只需更新提交和验证步骤即可。</p><p></p><p>这种模式在 Formik 和 react-hook form 等表单库中得到了推广。我们可以将</p><p></p><p>纯 React 表单</p><p></p><p><code lang=\"null\">const [firstName, setFirstName] = useState('');const onSubmit = data =&gt; console.log(data);return </code></p><form><code lang=\"null\">  <input name=\"firstName\" value=\"{firstName}\" /> setFirstName(e.currentTarget.value)}   /&gt;\n</code></form><p></p><p></p><p>转换成 react-hook-form 表单</p><p></p><p><code lang=\"null\">const { control, handleSubmit } = useForm({  defaultValues: { firstName: '' }});const onSubmit = data =&gt; console.log(data);return </code></p><form><code lang=\"null\">   <input />}  /&gt;\n</code></form><p></p><p></p><p>是的，这增加了一些复杂性，但是我们帮助解决了状态更新对组件影响过大的问题。</p><p></p><p>然而，这引出了一个有趣的问题。当审视 React 的生态系统时，我们会发现很多库都是为了修复 React 的缺点而存在的。</p><p></p><p>当你看到一个库号称速度提升 100 倍并改进了功效学设计时，它们所做的就是避开 React。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a0/a0fd8fe9e4c2947f1246821e5839c169.png\" /></p><p></p><p>郑重声明，我并不反对这个观点。看着 UI 渲染器生态系统如此不知疲倦地工作，一边使用它，又一边躲着它真是太有意思了。</p><p></p><p>关于状态讨论，有几个朋友加入我们，包括 react-redux、 @xstate/react、Zustand、Jotai、Recoil 等。</p><p></p><p>状态讨论一般会让人感到沮丧，因为它们通常会掩盖某种形式的 React Context。我们必须按照 React 的规则来触发 UI 更新，所以前面提到的所有库都有某种形式的级联渲染效果。</p><p></p><p>React 组件不能直接共享状态。因为状态存在于树上，我们只能间接地访问这棵树，所以我们必须爬上爬下，而不是从一个树枝跳到另一个树枝。在这个过程中，我们会接触到本不需要的东西。</p><p></p><p>Jotai 示例</p><p></p><p><code lang=\"null\">const countAtom = atom(0);const doubleCountAtom = atom(get =&gt; get(countAtom) * 2);const MyComponent = () =&gt; {  const [count, setCount] = useAtom(countAtom);  const doubleCount = useAtomValue(doubleCountAtom);  return <button> setCount(count + 1)}&gt;    {count} x 2 = {doubleCountAtom}  </button>;}\n</code></p><p></p><p>我们已经使用 Jotai 巧妙地设置了一些派生状态，但将其插入 React 意味着我们回到了基于组件的反应性。</p><p></p><p>你可以为 React 添加“细粒度”的反应式系统，而且不需要做太多修改。</p><p></p><p>那需要框架级的集成。</p><p></p><p></p><h2>细粒度反应</h2><p></p><p></p><p>与框架集成的细粒度反应会是什么样子？可能是像 Solid.js 这样的东西。</p><p></p><p>solid.js 示例</p><p></p><p><code lang=\"null\">function Counter() {  const [count, setCount] = createSignal(0);  setInterval(() =&gt; setCount(count() + 1), 1000);  return </code></p><div><code lang=\"null\">Count: {count()}</code></div><code lang=\"null\">;}\n</code><p></p><p></p><p>在讨论 React 时提到 Solid 很有趣，因为它的 API 看起来与 React 非常像。主要的不同是我们不需要像这样在useEffect中封装代码。</p><p></p><p>在 React 中，这类代码会导致一个严重的 Bug，即每秒创建一个新的setInterval调用。</p><p></p><p>对于非基于组件的反应式框架，组件之间的区别就消失了。它们在设置和生成 UI 时非常有用。在应用程序生命周期中，状态才是真正重要的。</p><p></p><p>Preact、Vue、Angular、Marko、Solid 和 Slvelte 等框架都采用了某种形式的细粒度反应。它们称之为信号、存储或可观察量。语义差异可能很重要，但我将把这个概念称为 信号。</p><p></p><p>reactive 数据存储（信号）</p><p></p><p><code lang=\"null\">const [headerEl, divEl, spanEl] = getEls();const nameSignal = signal('John');nameSignal.subscribe(name =&gt; headerEl.textContent = `Name: ${name}`);nameSignal.subscribe(name =&gt; divEl.textContent = name);nameSignal.subscribe(name =&gt; spanEl.textContent = `\"${name}\"`);// 在应用程序中的某个地方nameSignal.set('Jane')\n</code></p><p></p><p>这个例子包含了信号——可以感知自身“订阅者”的状态块。当我们改变这个状态值时，信号将通过传递进来的函数“通知”它的订阅者有一个更新。</p><p></p><p>在执行更新之前，我们不需要参照最高状态树来比较 UI 输出差异。我们可以直接将状态连接到 UI 更改。</p><p></p><p>信号也可以通知其他信号。计算状态机仍然存在，只是具备了非常好的功效学设计。</p><p></p><p>使用反应式原语，你可以在一个小时内构建出自己的框架，并且代码比使用其他反应式模型要好得多。</p><p></p><p>reactive mini-framework</p><p></p><p><code lang=\"null\">const num1 = signal(0), num2 = signal(0);const total = computed(() =&gt; num1.value + num2.value);const inputEl1 = create('input').bind('value', num1);const inputEl2 = create('input').bind('value', num2);const outputEl = create('input').bind('textContent', total);get('body').append(inputEl1, ' + ', inputEl2, ' = ', outputEl);\n</code></p><p></p><p>就像 Rustaceans 反对任何没有内存安全的新语言一样，我反对任何没有信号的新框架。</p><p></p><p>我们在 WASM 战争中也发现了类似的战斗。Yew 是最突出的 Rust 前端框架，但它依赖于类似 React 的方法。它的性能仅略优于 React，而基于信号的 Rust 框架（如 Leptos 和 Sycamore）超过了 Angular 和 Slvelte。</p><p></p><p></p><h2>问题总结</h2><p></p><p></p><p>虽说如此，但我认为只看框架基准测试是不够的。</p><p></p><p>React 的功效学设计很差。</p><p></p><p>使用 React 比使用 Slvelte 更容易搞砸。当然，超优化的 React 只比其他框架差一点点，但我不写超优化的代码。因此，在实践中，我看到的 React 代码往往每个文件都有十几个性能问题，出于理智，我们忽略了这些问题。</p><p></p><p>React 在发布时非常棒！但现在，我们有了更好的选择；客观情况基本就是这样。虽然随着时间的推移会有所改进，但我不认为 React 会从根本上改变它的工作机制，让它再次变得可以忍受。</p><p></p><p>那么我们为什么还在使用 React 呢？</p><p></p><p>1.经过实战检验</p><p>a.大公司已经证明可以把它应用于生产。</p><p>b.当你看到使用特定技术的产品取得了成功时，就更容易做出决定。</p><p></p><p>2.生态成熟</p><p>a.技术上讲，是这样的，但是有一半的生态系统要么是作为普通库的 React 封装器而存在，要么是作为 React 缓解包而存在。</p><p>b.非凡的反应模型通常意味着插入 React 之外的第三方库更容易。</p><p></p><p>3.劳动力更充足</p><p>a.这一点似乎无法反驳。如果你想要一份工作，最好的选择就是 React。如果你想雇佣员工，最好的选择也是 React。</p><p>b.虽然我认为教授其他框架通常更容易，但只有当你有足够的时间和能力来培训工程师时，这才有意义。</p><p></p><p>4.还在演进</p><p>a.当“解决方案”就在眼前时，要做出改变很难。</p><p>b.几乎“全栈”都在演进，但每一个新产品都是作为 React 所有弊病的解决方案而出现。</p><p></p><p>5.很难离开</p><p>a.预期收益抵不上迁移成本。</p><p>b.它的反应模型非常独特，以至于迁移到另一个框架需要花费大量的时间，无法立即获得明显的改进。</p><p></p><p>我现在的工作是 React 开发。我的下一份工作还将是 React 开发。再下一份也可能是。</p><p></p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://emnudge.dev/blog/react-hostage\">https://emnudge.dev/blog/react-hostage</a>\"</p><p></p><p></p><h2>欢迎参与讨论</h2><p></p><p></p><p>3 月 17 日，React 正式发布了全新的 React 官方文档，官网的首选框架建议是 Next.js 或 Remixjs ，这引发了社区的积极讨论，你对此有何看法，欢迎留言。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d6/d6c3170c4c30a8c00ea05b27f77d34a3.png\" /></p><p></p><h5> 相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/3e10ee935ffd1b23b1ecd8842\">看透 react 源码之感受 react 的进化</a>\"</p><p><a href=\"https://xie.infoq.cn/article/9d8a9c2ca82a82fdb098ee31b\">前端开发框架 React 技术如何与小程序结合，进行页面构建</a>\"</p><p><a href=\"https://xie.infoq.cn/article/7c17044e7d4827b37bfd698cb\">React 源码分析 1-jsx 转换及 React.createElement</a>\"</p><p><a href=\"https://xie.infoq.cn/article/958fbe57ff88cdba990d99739\">手写一个 react，看透 react 运行机制</a>\"</p>",
    "publish_time": "2023-03-28 10:10:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "WebAssembly在工业领域的巨大机遇",
    "url": "https://www.infoq.cn/article/pskeeKXTSbmQa2cauwBh",
    "summary": "<p>WebAssembly 是一种新的编码方式，一种虚拟的指令集，具有紧凑的二进制格式，可以以接近原生的性能运行，并被多种语言诸如 C / C ++，Rust，Go，Kotlin等作为编译目标。</p><p></p><p>在工业领域，WebAssembly已经获得学术界和一些大型企业的强烈兴趣，在过去的2年中已经涌现一些相关前沿性研究报告与开源项目。</p><p></p><p>在2022年由美国卡耐基梅隆大学举办的一场名为“WebAssembly Research Day”的学术会议中，西门子研究人员作了一个主题为“An end-to-end toolchain for evaluating WebAssembly runtimes for CPS-IoT Use cases”的报告。西门子团队介绍了当前工业界最新的编程语言WebAssembly技术在以下方面对他们的巨大意义：</p><p></p><p>网络物理控制（ Cyber Physical Control）：传统上，电气控制工程师、计算机软件工程师和通信工程师三组人马分开工作，相对独立。工业化与信息化的两化深度融合在工业控制系统内集成计算、通信和存储等功能，使用WebAssembly技术支持计算和控制在网络中自由分布流动，是网络物理控制的极佳的支撑技术，也是电气控制、软件、通信工程师们共同的工具。</p><p></p><p>广泛的设备支持（Range of supported devices）：WebAssembly的轻量化设计可以支持其在500K内存以内的嵌入式设备上轻松工作，也能很好利用512M内存的网关和16G内存的工业PC，同时还可以在云端服务器上运行。WebAssembly程序的跨平台特性，让计算负载可以按需灵活部署。</p><p></p><p>兼容巨大存量的软件（Legacy Software Support）：像西门子这样的大型公司会具有基于C/C++的海量存量软件，这些软件源码都可以被重新编译到WebAssembly，部署到各种现代的硬件平台上。工业化与信息化的两化深度融合就是要把用在IT的先进技术应用到OT领域。WebAssembly正是这一有力的工具，将在IT领域开发出的应用，如人工智能、机器学习、计算机视觉、数据处理、数据存储、数据流处理、以及复杂模型的控制系统等无缝移植到工业领域里。</p><p></p><p>高性能和一致性（Performance and consistency）：西门子对业界主要的开源WebAssembly引擎进行了科学全面的测试，确认WebAssembly技术在程序运行与计算性能上的优势，其中由英特尔、小米、蚂蚁、亚马逊等公司主导开发WebAssembly Micro Runtime（WAMR）表现了令人满意的性能。</p><p></p><h2>WebAssembly是什么？</h2><p></p><p></p><p>WebAssembly最初由谷歌、火狐、微软等浏览器厂商推动，用于为用户提供更高执行性能的Web浏览器程序。国际标准组织W3C于2017年推出WebAssembly第一版规范草稿，时至今日WebAssembly的发展已经取得令人瞩目的成就，在我们每一个人的电脑、手机、机顶盒中都有WebAssembly技术在背后支持。同时WebAssembly也超越了浏览器的领域，在云计算、可信计算、网格计算、边缘计算、IoT以及区块链等众多领域取得的非凡的发展。</p><p>&nbsp;</p><p>作为 W3C WebAssembly Community Group中的一项开放标准，WebAssembly 是为下列目标而诞生的：</p><p></p><p>快速、高效、可移植——通过利用常见的硬件能力，同一份WebAssembly字节码程序在不同平台上能够以接近本地的速度运行。</p><p></p><p>可读、可调试——WebAssembly 是一门低阶语言，但是它确实有一种人类可读的文本格式（其标准的最终版本即将颁布），这样可以通过手工来写代码，看代码以及调试代码。</p><p></p><p>安全——WebAssembly 被限制运行在一个安全的沙箱执行环境中。像其他网络代码一样，它遵循浏览器的同源策略和授权策略。</p><p></p><p>支持作为多种编程语言的编译目标。目前支持较成熟的语言有C、C++、Rust、TinyGo、AssemblyScript，业界也在努力让更多语言如Kotlin, TypeScript、Java可以编译到WebAssembly。中国大湾区数字经济研究院的基础软件中心也在以WebAssembly为编译目标设计全新的编程语言。</p><p>&nbsp;</p><p>字节码联盟（BytecodeAlliance）是推动WebAssembly技术发展的一个最活跃业界合作非盈利组织，合作成员包含英特尔、西门子、微软、Fastly、Mozilla，谷歌、亚马逊等企业。字节码联盟提供开源的 WebAssembly引擎（Runtime）实现，WebAssembly系统编程接口（WebAssembly System Interface-WASI)，WebAssembly模块接口工具（WIT），组件模型等工具和组件生态的开源项目。字节码联盟的开源地址是https://github.com/bytecodealliance/，它提供两个主要的WebAssembly引擎开源项目wasmtime和WAMR。</p><p>&nbsp;</p><p>简而言之对于工业等众多领域而言，WebAssembly 的巨大意义在于它提供了一条途径，以使得以各种语言编写的代码都可以以接近原生的速度在WebAssembly引擎上运行。由此WebAssembly被全美计算机协会编程语言特别兴趣组评为2021年度“Programming Languages Software Award”。</p><p></p><p></p><h2>为什么是WebAssembly？</h2><p></p><p></p><p>和Java与.NET的不同？WebAssembly是一种同时具有字节码格式和文本格式的计算机程序语言。字节码格式带来了跨平台的关键能力，Java是由Sun最早引入字节码格式的语言，其class字节码格式已经帮助Java语言取得了巨大成功。微软也为.NET引入了公共语言运行时 (CLR)的字节码文件格式，成为Windows平台主流的程序运行格式。那在工业界为什么是WebAssembly呢？</p><p></p><p>首先，WebAssembly是W3C组织下定义的开放标准，W3C所创建的标准定义流程保证了万维网（Web）领域互联互通与向后兼容，WebAssembly的演进是先有规范后有实现，具有充分的向后兼容支持，不会发生剧烈甚至颠覆性的改变。其所有的相关编译器、引擎、工具等都是完全开源的方式在组织开发。在WebAssembly技术领域，不存在像甲骨文公司和微软公司对Java和.NET从商标、专利、版权、技术路线等全方位的独家控制。这些特点，对于需要有长时期支持的工业系统来说尤其重要。</p><p></p><p>其次，支持多种编程语言，尤其是C/C++。虽然Java，Kotlin和Scala等语言也都支持编译到JVM字节码格式，但与之相比，WebAssembly是唯一支持C/C++、Rust的字节码技术。在工业领域，需要能够重用巨大存量的C/C++库和软件，Rust语言因为其内存安全特性，也备受工业界关注。</p><p></p><p>第三点，内置的隔离能力。Java和.NET设计初衷并不包含模块之间的隔离能力。WebAssembly的设计，让在同一个进程内或者在嵌入式无用户状态环境中执行多个WebAssembly程序模块，保证模块之间具有强内存隔离性。同时任何一个WebAssembly程序如发生了非法内存访问，将被WebAssembly引擎所捕获，不会影响到其他程序的执行。这个特点让WebAssembly在CDN领域作为微容器，在单个进程内创建许多的WebAssembly实例，达到极高的吞吐能力。对于工业领域众多嵌入式设备，这个能力可以帮助分离应用程序和固件的开发，支持嵌入式上的应用后装载和动态更新，甚至为支持第三方应用开发提供了可能。</p><p></p><p>第四点，超轻量型与高执行性能。参考开源项目WebAssembly Micro Runtime，其执行一个WebAssembly代码所需要的资源可以在100K内存之内，同时通过预编译技术，WebAssembly程序执行速度可以接近甚至超过GCC原生编译程序执行的速度。同时WebAssembly支持硬件加速计算能力如SIMD，支持多线程充分利用现代CPU多核计算能力，让WebAssembly轻松执行Tensorflow这样机器学习应用类型的密集计算。</p><p></p><p>最后，跨平台能力。WebAssembly和Java和.NET一样具有跨平台能力，同一份二进制文件可以运行在不同CPU架构、不同操作系统的环境上。工业领域存在如此多的不同设备类型，我们可以说没有比工业领域更欢迎这个关键能力了。</p><p></p><p>最后我们借用英特尔WebAssembly技术专家在2021 Automotive Linux Summit上主题为“The Cool Features Of WebAssembly Micro Runtime For IoT And Embedded ”汇报中的如下一页，来做为总结。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b0ffc537500cef734b4a7f908cd615d6.png\" /></p><p></p><p></p><h2>WebAssembly在工业领域的展望</h2><p></p><p></p><p>卡耐基梅隆研究人员汇报了与德国博世公司合作的主题为“Giving the Cloud an Edge with WASM”的研究项目，介绍了目前工业制造中的一些痛点（如下图）：</p><p></p><p>每一个工艺阶段都包含若干FPGA/PLC执行固定内部循环控制</p><p></p><p>高层的控制在车间/工厂层级运行，运行在许多不同的操作系统上</p><p></p><p>设计工作一般在一个非生产现场的办公或实验室环境中进行</p><p></p><p>补丁和升级经常都需要手工、现场执行，既低效又容易出错</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/4902815e17851322f1cf624aa644adfe.png\" /></p><p></p><p>云计算发展今天已经证明大规模计算调度的技术成熟性，工厂实际上也包含海量的、规格各异的计算单元，WebAssembly 可以帮助在工厂内部实现计算资源的统一调度。下图中所示，WebAssembly (WA) 用于运行各种的工业应用，可以按照时延、资源等需要在嵌入式设备、工业PC和车间/工厂服务器之间灵活部署与调度。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c92586885eef232b829da3d1c8f08eb.png\" /></p><p></p><p>由此他们开发的银线平台（Silverline Platform），可以将大部分的工业应用程序编译成WebAssembly，使用WebAssembly Micro Runtime（WAMR）开源项目作为运行引擎，提供一个统一的管理调度平台。这个系统帮助充分利用已有的硬件计算资源，实现更先进的业务柔性和管理性。</p><p>&nbsp;</p><p>Digital Twin Assembly (dtasm)是由西门子开发的一个支持数字孪生、模块式组装的开源项目，也使用了WebAssembly模块作为仿真程序的目标格式。每个仿真单元在过程的每一个时间单元格中使用其输入数据执行计算，将结果输出到其输出变量中，作为其当前时间格的输出。该项目定义仿真模块的输入和输出接口定义和二进制接口标准，这样可以由很多个基于WebAssembly程序模块的仿真单元可以灵活组装起来，形成链式调用，构建一个复杂的数字孪生系统。</p><p>&nbsp;</p><p>在中国尽管还没有许多在工业企业中被应用的公开报告，已经有不少企业在物联网相关领域使用WebAssembly。小米的工程师在2021 Wasm Open Day活动中报告了“WASM &amp; WAMR在AIOT中的实践”，介绍了小米Vela物联网操作系统中对WebAssembly的支持与应用。阿里巴巴工程师在题为“Waft：基于WebAssembly的AIoT应用框架实践”的汇报 中介绍了基于WebAssembly设计的物联网应用开发框架，提供更加平滑的用户体验。</p><p>&nbsp;</p><p>在开发者工具领域，浏览器内的WebAssembly技术已经让一些传统的桌面工具迁移到浏览器上，例如Autodesk公司已经将传统的AUTOCAD软件迁移值浏览器。国内企业三维家自研基于WebAssembly的图形引擎，并且在基于云的建模内核之上，融入大数据、AI人工智能等前沿技术，打造出面向制造业、高AI化、一体化的工业软件。</p><p>&nbsp;</p><p>在不久的将来，我们可以期望WebAssembly在广泛的工业领域成为一个关键性的支撑技术。其中最具有想象的三个主要的领域：</p><p></p><p>自动化与机器人控制应用：传统PLC等编程语言的能力已不足以满足现代以数控机床、视觉控制、自动巡航、工业机器人等为代表的复杂控制场景需要， WebAssembly可以让更多的编程语言进入控制应用开发，引入更多的语言生态资源。</p><p></p><p>人工智能的应用：基于WASI-NN（神经元网络）系统接口，WebAssembly程序将可以在工业场景的各种不同计算单元中使用人工智能的算法</p><p></p><p>云边端的统一的集中管理调度平台（Centralized Management Platform）：使用统一的编程调试软件（Engineering System）及管理调度平台，易于部署、实施、管理、监控和维护。</p><p></p><h2>如何开始使用WebAssembly？</h2><p></p><p></p><p>现在正是工业领域企业开始使用WebAssembly构建解决方案的合适时机。下面提供了一些可用于了解、使用WebAssembly的参考链接：</p><p></p><p>https://developer.mozilla.org/zh-CN/docs/WebAssembly：由Mozilla开发者网络提供的WebAssembly基础介绍</p><p></p><p>https://github.com/WebAssembly/proposals：WebAssembly规范提案状态</p><p></p><p>https://github.com/WebAssembly/WASI: WebAssembly system interface规范提案</p><p></p><p>https://github.com/bytecodealliance/wasm-micro-runtime：WebAssembly Micro Runtime开源项目</p><p></p><p></p><p>作者简介：</p><p></p><p>魏东，于1991年毕业于清华大学电机工程系，获工程学士学位；并分别与2001年和2004年于美国新泽西理工学院（New Jersey Institute of Technology）获得电气工程的硕士和博士学位。在美国博士毕业后，魏东博士长期从事工业自动化领域的理论及应用研究，取得了一系列在国际上有影响的独创性成果。他是cyber-resilient ICS模型的创立者之一，其研究成果的原创性得到学术界和工业界的认可。魏东于2004年五月至2019年五月在西门子（美国）研究院任科学家，从事工业自动化技术的研究与开发，主要的研究课题包括工业4.0、可编程逻辑控制器（PLC）、运动控制、工业网络通信及安全。魏东博士于2020年入选宁波市镇海区“雄镇英才”高层次人才创业项目计划，并于9月来宁波创立浙江清捷智能（Tsing-Jet）科技有限公司。2022年入选宁波市“甬江引才”计划。目前聚焦于开发基于５G与边缘计算技术的产品和系统去解决工业互联网在工厂侧的“最后一公里”问题。</p><p>&nbsp;</p><p>侯明鹏，男，研究员，博士。就职于北京机科国创轻量化科学研究院有限公司。主要从事先进制造、增材制造和特种设备控制系统的开发和研究工作。获得授权发明专利3项；发表论文5篇；授权软件著作权12项；发布团体标准2项；先后主持和参加863、04专项、重大专项、北京市科技计划等纵向课题10余项。获得省部级奖三项：（1）中国机械工业科学技术奖特等奖（2）中国机械工业科学技术奖一等奖（3）北京市科学技术奖。</p><p></p><p>参考资料：</p><p></p><p>1.https://developer.mozilla.org/zh-CN/docs/WebAssembly/Concepts</p><p>2.https://www.cs.cmu.edu/~wasm/wasm-research-day-2022.html</p><p>3.slides, An end-to-end toolchain for evaluating WebAssembly runtimes for CPS-IoT Use cases, Siemens</p><p>4.slides, Giving the Cloud an Edge with WASM, Carnegie Mellon University, Bosch</p><p>5.https://github.com/siemens/dtasm, Digital Twin Assembly, Siemens</p><p>6.https://ex.chinadaily.com.cn/exchange/partners/82/rss/channel/cn/columns/sz8srm/stories/WS62e4f86aa3101c3ee7ae186f.html</p><p>7.http://www.sigplan.org/Awards/Software/</p><p>8.https://2021.international.conference.modelica.org/proceedings/papers/Modelica2021session6A_paper3.pdf</p><p>9.https://idea.edu.cn/dii.html</p><p>10.https://github.com/bytecodealliance/wasm-micro-runtime</p><p>11.https://ossalsjp21.sched.com/event/peeL/the-cool-features-of-webassembly-micro-runtime-wamr-for-iot-and-embedded-xin-wang-intel</p><p>12.https://github.com/WebAssembly/WASI/issues/443</p><p></p><p></p><blockquote>原文转载自“CAA会员服务”微信公众号，链接：https://mp.weixin.qq.com/s/Lmmt3_R6-Wkh-JzBapSgJw</blockquote><p></p>",
    "publish_time": "2023-03-28 10:55:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百度何俊杰：AI创作时代，AIGC就是“马良”的“神笔”",
    "url": "https://www.infoq.cn/article/wgZQsRQNIFKLQ3cJ3AUw",
    "summary": "<p>写新闻、画插画、剪视频.......随着生成式AI的爆发，内容创作领域的变革正在按下加速键。3月28日，百度集团资深副总裁、百度移动生态事业群组（MEG）总经理何俊杰在“2023百度内容生态共生大会”上表示，“AI创作时代”到来，创作者将迎来三重新的机会，包括新的流量风口、新的内容生产力革命、新的多元变现蓝海，而百度将与广大MCN机构和创作者一起拥抱“AI创作时代”，让AI技术赋能内容创作、分发和变现的全流程。</p><p>&nbsp;</p><p>此前不久，百度自研生成式AI正式发布，已有超过650家企业接入，AIGC+阶段正在呼啸而来。而百度移动生态处在这场变革的最前线，“听得见炮火，看得见市场”，何俊杰表示，百度移动生态要与广大创作者一起，攀上风口，抓住机会，让所有创作者、合作伙伴都乘上开往“AI创作时代”的大船，一个都不能少！</p><p>&nbsp;</p><p>而机会之一就是“新的流量增长风口”，何俊杰指出，生成式AI的发展，是全新的计算范式带来的新机会。机会落到互联网行业，这将是一次代际变革，就像蒸汽时代向电气时代的变革，PC时代向移动互联网时代的跨越。“每一次这样的变革，都会诞生新的增长风口。”百度作为中国人工智能市场长期增长的最佳代表，正站在浪潮之巅。据悉，生成式AI与百度搜索整合，将引领搜索体验的代际变革。除了提供更好的搜索和答案，还会提供全新的交互和聊天体验，以及独特的生成内容，极大地丰富内容生态和供给，吸引更多用户，并有机会形成新的流量入口。</p><p>&nbsp;</p><p>其次，新生产力革命正在爆发。“如今，AIGC就是创作者手中的那支神笔。”何俊杰说。在AIGC的帮助下，创作的效率将大大提高,创作的门槛将会无限降低，创作的想象力持续延伸。&nbsp;“未来，AIGC工具将会成为创作者最得力的搭档、最核心的标配，而且使用门槛会比鼠标键盘还要低。”何俊杰表示。去年万象大会，百度推出创作者AI助理团，目前已有超过45万百度创作者使用，产出了超700万篇内容，累计分发量超过200亿。今年万象大会上，百度移动生态还将发布更多更好的生产力工具，让AI人人可用。</p><p>&nbsp;</p><p>第三，新的多元变现蓝海也在开启。内容变现一直是困扰众多创作者和MCN的核心难题，而百度内容生态拥有百度搜索这样的超级流量入口，是用户离交易最近的决策入口，也是创作者多元变现的绝佳场域。百度搜索每天响应来自100多个国家、超60亿次的搜索请求，每天有超9000万人在百度搜索商品，催生出了包括教育、数码、汽车、母婴育儿、美食、大健康等特色领域在内的一批极具潜力和商业价值的特色领域。因此，从创作到变现，百度内容生态也有自己独有的解法。何俊杰表示，百度要与创作者们一起挖掘内容消费场景的“聚宝盆”，还要用AI修一条内容变现的“高速路”。接下来，百度移动生态将会全面整合生成式AI，推出内容创作的一站式AI创作平台、AI笔记等系列AIGC产品。</p><p>&nbsp;</p><p>而对于创作者们担心被AIGC取代的焦虑，何俊杰直言，人类历史上的每一次科技变革都给出了答案。造纸术让纸张替代了竹简和绢帛，却没有剥夺书生写诗作文的机会，而是进一步促进了知识的普惠化；汽车普及的时候，聪明的马车夫也不会因此失业，而是率先取得了驾照。因此，AI创作时代之于创作者，是蛋糕变大了而不是变小了，是机会更多了而不是更少了。它不是危机，而是生机。AI的每一次创新与进化也都是为了提高社会生产力，为人类带来更多的自由与可能。</p><p>&nbsp;</p><p>据悉，会上，百度集团副总裁、移动生态商业体系负责人王凤阳，百度副总裁赵强，百度MEG内容生态副总经理宋健也分别从商业和内容两个层面，向到场的近100家MCN分享了百度内容生态未来的规划和愿景，2023年，百度内容生态将从流量、技术、商业三个维度，引领创作者拥抱AI创作时代，帮助创作者创造风口、把握风口。</p>",
    "publish_time": "2023-03-28 13:22:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯大规模云原生技术实践案例集",
    "url": "https://www.infoq.cn/article/Is534v4mBvUmgC1uJEdc",
    "summary": "<p><img alt=\"\" src=\"https://static001.infoq.cn/resource/image/f9/8e/f9924e14289757686d940513a85bdd8e.jpg\" /><br />\n经过多年磨砺与创新，腾讯内部海量自研业务已实现全面上云。近三年来，腾讯的自研业务上云规模 <span class=\"orange\"><strong>已经突破 5000 万核，累计节省成本超过 30 亿。</strong></span></p>\n<p>包括 QQ、微信、腾讯视频、王者荣耀等在内的 <span class=\"orange\"><strong>腾讯内部业务，和腾讯云百万级外部客户一样基于公有云的模式来开发运营</strong></span>，腾讯全面开启业务云端生长新时代。</p>\n<p>“这是腾讯自研上云战略的一个里程碑。”腾讯集团高级执行副总裁、云与智慧产业事业群CEO 汤道生表示：“把腾讯内部海量业务搬上云端，不仅帮助腾讯构建面向未来的技术架构和研发文化，推动科技成为公司业务发展和产品创新的动力与支撑，也全面锤炼了腾讯云的产品、技术和综合服务能力，这些能力将加快推动产业的数字化升级，助力实体经济全面发展。”</p>\n<p>大部分业务都是在保持高速增长的过程中上云。比如，QQ 是腾讯首个全面上云的内部业务，把如此庞大和复杂的业务搬上云端，技术团队实现了对用户零感知，被外界称为“开着飞机换引擎”。</p>\n<p>同时，腾讯云也为新兴业务的高速发展提供有力支撑。以视频号为例，借助腾讯云的弹性扩容能力，视频号稳健支撑诸如西城男孩、周杰伦、崔健等明星的大型线上演唱会活动；得益于对象存储 COS 和腾讯云直播服务，视频号在春节等特殊时段抗住了超平时 3 倍以上业务高峰。</p>\n<p>腾讯会议凭借生于云、长于云的大规模实践，现在已经成为中国最受欢迎的云视频会议产品，依托业界领先的实时音视频产品 TRTC，腾讯会议可以有效保障数亿用户在复杂网络环境中流畅清晰的视频会议体验。</p>\n<p>腾讯自研业务上云，打造出了 <span class=\"orange\"><strong>国内最大规模的云原生实践</strong></span>。</p>\n<p>三年来，数千万核的自研业务上云规模，推动腾讯云的自研产品能力不断优化，多项产品性能达到业界领先水平，也推动腾讯云在全球的基础设施不断完善。</p>\n<p>腾讯自研上云 <span class=\"orange\"><strong>明确基于云原生来构建面向未来的技术架构</strong></span>。例如，通过容器和微服务等技术，腾讯构建了统一的技术底座和算力调度平台，有效促进公司内部技术团队的协作与创新。</p>\n<p>目前，腾讯云的 TKE 平台拥有国内最大规模的 Kubernetes 集群，以及最为领先的在离线混部技术，腾讯上云打造了国内最大规模的云原生实践。</p>\n<p>为了向开发者更好的介绍腾讯自研业务、外部客户如何通过云原生技术产品支撑业务发展的，特别推出<strong><span class=\"orange\">《腾讯大规模云原生技术实践案例集》</span></strong>，包括 QQ、腾讯会议、腾讯广告、和平精英、腾讯文档、作业帮、中国南方电网、小红书、知乎、Unity、斗鱼、微盟等十多个海量产品和大规模场景的云原生技术实践。希望在给业界带去参考的同时，能够一起推动国内大规模场景下云原生技术实践的有效落地。</p>",
    "publish_time": "2023-03-28 14:15:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]