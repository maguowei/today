[
  {
    "title": "在Java中如何加快大型集合的处理速度",
    "url": "https://www.infoq.cn/article/pQ7VDmaij1aCT9TD0R06",
    "summary": "<p>本文讨论了Java Collections Framework背后的目的、Java集合的工作原理，以及开发人员和程序员如何最大限度地利用Java集合。</p><p></p><h2>什么是Java集合</h2><p></p><p>尽管Java已经过了25岁生日，仍然是当今最受欢迎的编程语言之一。超过100万个网站通过某种形式在使用Java，超过<a href=\"https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/\">三分之一</a>\"的软件开发人员的工具箱中有Java。</p><p></p><p>Java在它的整个生命历程中经历了重大的演变。一个早期的进步出现在1998年，当时Java引入了Collections Framework（Java Collection Framework，JCF），简化了操作Java对象的任务。JCF为集合提供了标准化的接口和通用方法，减少了编程工作，并提升了Java程序的运行速度。</p><p></p><p>理解Java集合和Java Collections Framework之间的区别是至关重要的。Java集合只是表示一组Java对象的数据结构。开发人员可以像处理其他数据类型一样处理集合，执行搜索或操作集合内容等常见任务。</p><p></p><p>Set接口（java.util.Set）就是Java集合的一个例子。Set是一种集合，不允许出现重复元素，也不以任何特定的顺序存储元素。Set接口继承了Collection（java.util.Collection）的方法，并且只包含这些方法。</p><p></p><p>除了集合之外，还有队列（java.util.Queue)和Map（java.util.Map）。Map并不是真正意义上的集合，因为它们没有<a href=\"https://howtodoinjava.com/interview-questions/useful-java-collection-interview-questions/\">继承集合接口</a>\"，但开发人员可以像操作集合一样操作Map。集合、队列、列表和Map都有后代，比如排序集合（java.util.SortedSet）和可导航Map（java.util.NavigableMap）。</p><p></p><p>在使用集合时，开发人员需要熟悉和理解一些特定的集合相关术语。</p><p></p><p>可修改与不可修改——正如这些术语表面上所表明的，不同的集合可能支持也可能不支持修改操作。可变集合与不可变集合——不可变集合在创建后不能被修改。虽然在某些情况下，不可修改的集合仍然可能由于其他代码的访问而发生变化，但不可变集合会阻止这种变更。不可变集合是指能够保证Collection对象中不会有任何变更的集合，而不可修改的集合是指不允许“add”或“clear”等修改操作的集合。固定大小与可变大小——这些术语仅与集合的大小有关，与集合是可修改还是可变无关。随机访问与顺序访问——如果一个集合允许为每一个元素建立索引，那么它就是可随机访问的。在顺序访问集合中，必须通过所有前面的元素到达指定的元素。顺序访问集合更容易扩展，但搜索时间更长。初学者可能会难以理解不可修改集合和不可变集合之间的区别。不可修改集合不一定是不可变的。实际上，不可修改集合通常是可修改集合的包装器，其他代码仍然可以访问和修改被包装的可修改集合。通常需要使用集合一些时间才能在一定程度上理解不可修改集合和不可变集合。</p><p></p><p>例如，我们将创建一个可修改的按市值排名前五的加密货币列表。你可以使用java.util.Collections.unmodifiableList()方法创建底层可修改列表的不可修改版本。你仍然可以修改底层列表，它只是被包装成不可修改列表，但你不能直接修改不可修改的版本。</p><p></p><p><code lang=\"java\">import java.util.*;\npublic class UnmodifiableCryptoListExample {  \n    public static void main(String[] args) {  \n\n        List cryptoList = new ArrayList&lt;&gt;();  \n        Collections.addAll(cryptoList, \"BTC\", \"ETH\", \"USDT\", \"USDC\", \"BNB\");  \n        List unmodifiableCryptoList = Collections.unmodifiableList(cryptoList);  \n        System.out.println(\"Unmodifiable crypto List: \" + unmodifiableCryptoList);  \n\n        // 尝试在可修改列表中再添加一种加密货币，并显示在不可修改列表中\n        cryptoList.add(\"BUSD\");\n        System.out.println(\"New unmodifiable crypto List with new element:\" + unmodifiableCryptoList);\n\n        // 尝试添加并显示一个额外的加密货币到不可修改列表中——unmodifiableCryptoList.add将抛出一个未捕获的异常，println代码将无法被执行\n        unmodifiableCryptoList.add(\"XRP\");\n        System.out.println(\"New unmodifiable crypto List with new element:\" + unmodifiableCryptoList);\n\n        }  \n}\n</code></p><p></p><p>在运行代码时，你将看到对底层可修改列表添加的内容显示为对不可修改列表的修改。</p><p></p><p>但这与你创建了一个不可变列表并试图修改底层列表不同。有许多种方法可以基于现有的<a href=\"https://springframework.guru/using-immutablelist-in-java/\">可修改列表创建不可变列表</a>\"，下面我们使用List.copyOf()方法创建了一个不可变列表。</p><p></p><p><code lang=\"java\">import java.util.*;\npublic class UnmodifiableCryptoListExample {  \n    public static void main(String[] args) {  \n\n        List cryptoList = new ArrayList&lt;&gt;();  \n        Collections.addAll(cryptoList, \"BTC\", \"ETH\", \"USDT\", \"USDC\", \"BNB\");\n        List immutableCryptoList = List.copyOf(cryptoList);\n        System.out.println(\"Underlying crypto list:\" + cryptoList)\n        System.out.println(\"Immutable crypto ist: \" + immutableCryptoList);  \n\n        // 尝试添加更多的加密货币到可修改列表，但不可变列表并没有显示变化\n        cryptoList.add(\"BUSD\");\n        System.out.println(\"New underlying list:\" + cryptoList);\n        System.out.println(\"New immutable crypto List:\" + immutableCryptoList);\n\n        // 尝试添加并显示一个新的加密货币到不可修改的列表中\n        immutableCryptoList.add(\"XRP\");\n        System.out.println(\"New unmodifiable crypto List with new element:\" + immutableCryptoList);\n\n        }  \n}\n</code></p><p></p><p>修改底层的列表后，不可变列表不显示变更。尝试修改不可变列表会直接导致抛出UnsupportedOperationException。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/4figure-1-1661256736453.jpg\" /></p><p></p><h2>集合与Java Collections Framework的关系</h2><p></p><p>在引入JCF之前，开发人员可以使用几个特殊的类，即Array、Vector和Hashtable。但这些类有很大的局限性，除了缺乏公共接口之外，它们还难以扩展。</p><p></p><p>JCF提供了一个用于处理集合的通用架构。集合接口包含了几个不同的组件。</p><p></p><p>公共接口——主要集合类型的表示，包括集合、列表和Map；实现——集合接口的特定实现，从通用的到特殊的再到抽象的。此外，还有一些与旧的Array、Vector和Hashtable类相关的遗留实现；算法——用于操作集合的静态方法；基础结构——对各种集合接口的底层支持。与之前相比，JCF为开发人员提供了许多好处。值得注意的是，JCF降低了开发人员对自己编写数据结构的需求，从而提高了Java编程的效率。</p><p></p><p>但是，JCF也从根本上改变了开发人员使用API的方式。JCF通过提供一组新的公共接口来处理不同的API，简化了开发人员学习、设计和实现API的过程。此外，API的互操作性也大大提升了。<a href=\"https://www.infoq.com/news/2022/02/eclipse-collections-11-0-0/\">Eclipse Collections就是一个例子</a>\"，它是一个完全兼容不同Java集合类型的开源Java集合库。</p><p></p><p>由于JCF提供了更容易重用代码的结构，从而进一步提升了开发效率。其结果就是开发时间缩短了，程序质量也得到了提升。</p><p></p><p>JCF有一个定义良好的接口层次结构。java.util.Collection扩展了超接口Iterable，Collection有许<a href=\"https://www.scientecheasy.com/2020/09/collection-hierarchy-in-java.html/\">多子接口和子类</a>\"，如下所示。</p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/2figure-2-1661256736453.jpg\" /></p><p></p><p>如前所述，集合是唯一性对象的无序容器，而列表是可能包含重复项的有序集合。你可以在列表中的任何位置添加元素，但其他部分仍然保留了顺序。</p><p></p><p>队列也是集合，元素被添加到一端，并在另一端被删除。也就是说，它是一种先进先出（FIFO）接口。Deque（双端队列）允许从任意一端添加或删除元素。</p><p></p><h2>使用Java集合的方法</h2><p></p><p>JCF中的每一个接口，包括java.util.Collection，都提供了特定的方法用于访问和操作集合的各个元素。集合提供的常用的方法有：</p><p></p><p>size()——返回集合中元素的个数；add(Collection element) / remove(Collection object)——这些方法用于修改集合的内容。需要注意的是，当集合中有重复元素时，移除只会影响元素的单个实例；equals(Collection object)——比较对象与集合是否等价；clear()——删除集合中的所有元素。每个子接口也可以有其他方法。例如，尽管Set接口只包含来自Collection接口的方法，但List接口包含了许多用于访问特定列表元素的方法。get(int index)——返回指定索引位置的元素；set(int index, element)——设置指定索引位置的元素；remove(int,index)——移除指定索引位置的元素。</p><p></p><h2>Java集合的性能</h2><p></p><p>随着集合元素数量的增长，它们可能会出现明显的性能问题。事实证明，<a href=\"https://www.researchgate.net/publication/313820944_Empirical_Study_of_Usage_and_Performance_of_Java_Collections\">集合类型的选择</a>\"和集合的相关设计也会极大地影响集合的性能。</p><p></p><p>随着需要处理的数据量不断增加，Java引入了新的处理集合的方法来提升整体性能。在2014年发布的Java 8引入了Streams——旨在简化和提高批量处理对象的速度。自从推出以来，Streams已经有了许多<a href=\"https://www.baeldung.com/java-8-streams-introduction\">改进</a>\"。</p><p></p><p>需要注意的是，流本身并不是数据结构，而是“对流中的元素进行函数式操作（例如对集合进行map-reduce转换）的类。”</p><p></p><p>Streams使用方法管道来处理从数据源（如集合）接收到的数据。Streams的每一个方法要么是一个中间方法（返回可以进一步处理的流），要么是一个终端方法（在此之后不可能进行其他流处理）。管道中的中间方法是惰性的，也就是说，它们只在必要时才进行求值。</p><p></p><p>并行执行和<a href=\"https://www.geeksforgeeks.org/what-is-java-parallel-streams/\">串行执行</a>\"都存在于流中。默认情况下，流是串行的。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/1figure-3-1661263465544.jpg\" /></p><p></p><h2>通过并行处理来提升性能</h2><p></p><p>在Java中处理大型集合可能很麻烦。虽然Streams简化了大型集合的处理和编码工作，但并不总是能保证性能上的提升。事实上，程序员经常发现使用Streams反而会<a href=\"https://blogs.oracle.com/javamagazine/post/java-parallel-streams-performance-benchmark\">减慢处理速度</a>\"。</p><p></p><p>众所周知，网站用户只会等待几秒钟的加载时间，然后他们就会离开。因此，为了提供最好的用户体验并<a href=\"https://www.getweave.com/reputation-management/\">维护开发人员</a>\"提供高质量产品的声誉，开发人员必须考虑如何优化大型数据集合的处理。虽然并行处理并不总能保证提高速度，但至少是有希望的。</p><p></p><p>并行处理，即将处理任务分解为更小的块并同时执行它们，提供了一种在处理大型集合时减少处理开销的方法。但是，即使并行流处理简化了代码编写，也会<a href=\"https://www.infoq.com/presentations/parallel-java-se-8/\">导致性能下降</a>\"。本质上，多线程管理开销会抵消并行运行线程所带来的好处。</p><p></p><p>因为集合不是线程安全的，并行处理可能会导致线程干扰或内存不一致（当并行线程看不到其他线程所做的修改，对相同的数据有不同的视图时）。Collections Framework试图通过使用同步包装器在并行处理期间防止线程不一致。虽然包装器可以让集合变成线程安全的，从而实现更高效的并行处理，但它可能会产生不良的性能影响。具体来说，同步可能会导致线程争用，从而导致线程执行得更慢或停止执行。</p><p></p><p>Java有一个用于集合的元素并行处理函数Collection.parallelstream。默认的串行处理和并行处理之间的一个显著区别是，串行处理时总是相同的执行和输出顺序在并行处理时可能会有不同。</p><p></p><p>因此，在处理顺序不影响最终输出的场景中，并行处理会特别有效。但是，在一个线程的状态可能会影响另一个线程状态的场景中，并行处理可能会有问题。</p><p></p><p>我们来考虑一个简单的示例，在这个示例中，我们为包含1000个客户创建了一个应收账款列表。我们想要知道这些客户中有多少人的应收账款超过25000美元。我们可以按照串行或并行的处理方式检查这个列表。</p><p><code lang=\"java\">import java.util.Random;\nimport java.util.ArrayList;\nimport java.util.List;\n\nclass Customer {\n\n    static int customernumber;\n    static int receivables;\n\n    Customer(int customernumber, int receivables) {\n        this.customernumber = customernumber;\n        this.receivables = receivables;\n    }\n\n    public int getCustomernumber() {\n        return customernumber;\n    }\n\n    public void setCustomernumber(int customernumber) {\n        this.customernumber = customernumber;\n    }\n\n    public int getReceivables() {\n        return receivables;\n    }\n\n    public void setReceivables() {\n        this.receivables = receivables;\n    }\n}\n\npublic class ParallelStreamTest {\n\n    public static void main( String args[] ) {\n\n        Random receivable = new Random();\n\n        int upperbound = 1000000;\n   \n            List &lt; Customer &gt; custlist = new ArrayList &lt; Customer &gt; ();\n\n                for (int i = 0; i &lt; upperbound; i++) {\n    \n                    int custnumber = i + 1;\n                    int custreceivable = receivable.nextInt(upperbound);\n                    custlist.add(new Customer(custnumber, custreceivable));\n                 \n }\n\nlong t1 = System.currentTimeMillis();\n\n                System.out.println(\"Sequential Stream count: \" + custlist.stream().filter(c -&gt;\nc.getReceivables() &gt; 25000).count());\n\n                long t2 = System.currentTimeMillis();\n\n                System.out.println(\"Sequential Stream Time taken:\" + (t2 - t1));\n\n               t1 = System.currentTimeMillis();\n\n                System.out.println(\"Parallel Stream count: \" + custlist.parallelStream().filter(c -&gt;\nc.getReceivables() &gt; 25000).count());\n\n                 t2 = System.currentTimeMillis();\n\n                 System.out.println(\"Parallel Stream Time taken:\" + (t2 - t1));\n\n    }\n\n}\n</code></p><p></p><p>代码执行结果表明，在处理数据集合时，<a href=\"https://www.tutorialspoint.com/compile_java_online.php\">并行处理</a>\"可能会提升性能：</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/1figure-4-1661263465544.jpg\" /></p><p>但需要注意的是，每次执行代码时，你可能获得不同的结果。在某些情况下，串行处理仍然优于并行处理。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/1figure-5-1661263465544.jpg\" /></p><p>在本例中，我们使用Java的原生进程来分割数据和分配线程。</p><p></p><p>不幸的是，对于上述两种情况，Java的原生并行处理并不总是比串行处理更快。实际上，经常会更慢。</p><p></p><p>例如，并行处理对于链表没有什么用。虽然ArrayList很容易被分割成并行处理，但LinkedList却不是这样的。TreeMap和HashSet介于两者之间。</p><p></p><p>Oracle的<a href=\"https://developer.ibm.com/articles/j-java-streams-5-brian-goetz/\">NQ模型</a>\"是决定是否使用并行处理的一种方法。在NQ模型中，N表示需要处理的数据元素数量，Q表示每个数据元素所需的计算量。在NQ模型中，计算N和Q的乘积，数值越大，说明并行处理提高性能的可能性越大。</p><p></p><p>在使用NQ模型时，N和Q之间存在反比关系，即每个元素所需的计算量越高，并行处理的数据集就越小。经验法则是，对于较低的计算需求，包含10000个元素的数据集是使用并行处理的基线。</p><p></p><p>除此之外，还有其他更高级的方法来优化Java集合中的并行处理。例如，高级开发人员可以调整集合中数据元素的分区，以最大化并行处理性能。还有一些<a href=\"https://www.infoq.com/articles/Refactoring-to-Eclipse-Collections/\">第三方的JCF插件</a>\"和替代品可以提升性能。但是，初学者和中级开发人员应该重点了解哪些操作可以从Java的原生并行处理特性中受益。</p><p></p><h2>结论</h2><p></p><p>在大数据世界里，想要创建高性能的网页和应用程序，必须找到改进大量数据处理的方法。Java提供了内置的集合处理特性帮助开发人员改进数据处理，包括Collections Framework和原生并行处理功能。开发人员需要熟悉如何使用这些特性，并了解可以时候可以使用原生特性，什么时候应该使用并行处理。</p><p></p><p>作者简介：</p><p>Nahla Davies是一名软件开发人员和技术作家。在全职从事技术写作之前，她曾在一家体验式品牌企业担任首席程序员，该组织的客户包括三星、时代华纳、Netflix和索尼。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/java-collections-streams/\">https://www.infoq.com/articles/java-collections-streams/</a>\"</p>",
    "publish_time": "2022-09-23 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "端到端语音识别应用基于前缀树的热词技术",
    "url": "https://www.infoq.cn/article/jw9fXWXWShx407VsfFLc",
    "summary": "<p>文 |&nbsp;王伟戌、王强强</p><p></p><h2>背景</h2><p></p><p>在深度学习火爆的今天，大规模数据下训练的大规模模型在线上任务中日益常见。随着大模型效果的提升， 随之带来了一些使用上的不便。通常情况下，大模型需要基于大量语料、文本训练，迭代周期长。且对于特定场景下词语在训练语料中出现次数不多，常常拟合不好。本文介绍的是关键词即特定场景语料，在序列到序列任务中通过构建状态转移自动机的方法改善最终效果的方案。</p><p></p><h2>生成模型即生成模型解码</h2><p></p><p>序列到序列模型常用于机器翻译、语音识别等任务。其架构提出于2014年[1]，包含两个核心组件：编码器、解码器。本文中略去这种模型的训练过程，对该模型在使用过程中解码这一过程进行介绍：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44501d391e0286d2e87fda6556c5d964.png\" /></p><p></p><p>通过这个图我们不难发现，每个时刻的生成结果不仅于输入时刻序列有关，还与输出序列相关，一个简单的想法，将每个时刻置信度最高的结果存下来，做为下一个时刻输入，但这样很容易产生问题：在一个时刻缺乏全局视野，即某一个时刻最优不代表是全局最优的结果。而将所有结果都记录下来，这将会是指数级增长的数据量。因此一种beam&nbsp;search的方法被提出用来解决这一问题，我们用一个形象的例子来讲述beam&nbsp;search这一过程。</p><p></p><p>假设我们得到了一串拼音序列：</p><p></p><p></p><p></p><p>我们如何知道这个拼音序列代表什么意思呢？</p><p></p><p>如下图所示，我们展现了一个通过简单概率模型产生的文本，在第零个时刻的5个候选（为了展示方便，这里省略了编号为④的候选），在第一个时刻各产生了三个延伸在这15个候选中，通过语言模型概率选取了top5保留，剩余的舍弃掉，以达到缩减搜索空间的目的。通过这样的方法，每一个时刻不保留全部结果只保留top&nbsp;N，最终将指数级增长的搜索空间变为平方级增长的搜索空间。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d244c370ab86fe0955bdb02c4cde187.png\" /></p><p></p><h2>传统解码应用问题与改进</h2><p></p><p></p><p>这种方法与全部状态保存的方案相比牺牲了准确度以换取时间，具有一定局限性。对于通常情况，每个时刻top N能覆盖当前时刻90%以上的情况，但是这种方法在面对关键词检测、风控词语检测等任务会产生两个问题：</p><p></p><p>1、待检测关键字在日常语料中较少出现，传统beam&nbsp;search非常容易漏召回。</p><p>2、时效性强，经常有实时插入的新检测词语，要即时生效。</p><p></p><p>对于第一个问题，一种直观的思想是通过标注数据，弱标注数据等重新训练模型，但这显然迭代周期长且迭代预期不稳定。</p><p></p><p>Google在2018年论文中[2]，提出一种设计方案调整beam&nbsp;search的结果，即不重新训练模型，仅在解码时通过追加模型进行重新打分来改善对小众语料的拟合。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23d46a02ee6e500cd493ffd20dd5f793.png\" /></p><p></p><p>以上图中识别系统设计为例，除去传统声学识别模块还增加了上下文模块，这个模块举例了几个功能，包括标点，语言模型打分，文本归一。其中语言模型的使用如下图，用beam&nbsp;search对中间结果进行追加打分，更新beam&nbsp;search中top&nbsp;N，并将当前结果作为下一时刻解码的输入。这种方式的优点是，在不更新语音识别模型的情况下，也可以通过添加不同语境的语言模型影响beam&nbsp;search过程中top&nbsp;N选取，从而达到改善结果的目的。缺点便是显著增加了计算量；尽管语言模型计算量常常不高，但beam&nbsp;search过程中每个候选结果都要多次经过语言模型，次数过多，耗时上升比较明显。</p><p></p><p>为了改进计算效率，经典wfst解码[3]方案重新在seq2seq模型中使用[4]，这种解码方案注意到了我们是在一个序列过程中重打分，不需在每一个时刻对从开始至当前所有文本进行重新计算，我们将语言模型生成新的概率转移自动机[5]，解码时，维护当前beam&nbsp;search过程中top&nbsp;N于图2 状态。如下图为cat(音标：kæt)这个单词的状态转移图，当声学识别模块产出c对应的k的音其转移到状态1，而当a对应的æ产生时，不在需要从状态0计算k&nbsp;æ一起的概率，而是计算当前状态1的后继状态中是否有æ。没有则计算回退到初始状态的概率。这样对于在序列生成中的计算，只需要记录其处于图中状态，在新的识别结果产生时对当前状态计算可行的转移状态即可。与普通语言模型相比，相当于省去了从状态0至当前状态的重复计算，复杂度大为降低。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2b/ea/2bf96775ab639319d9a545403586yyea.png\" /></p><p></p><p>这种浅融合的方案很好的解决了训练语料不均衡的问题，缺点是不能实时对图进行修改，且缺乏对特定词的加权，为此我们引入了前缀自动机来对这一过程改进。</p><p></p><h2>基于前缀自动机的解码加权方案</h2><p></p><p></p><p>前缀自动机，是一种经典算法，常应用于多模式串匹配。如果我们有一个字典，对于输入文本想检测是否命中字典中词语，这便是一个多模式串匹配任务。对于这类任务，一个显然的方案是遍历全部字典，但这样复杂度太高。</p><p></p><p>对此我们开始优化，一种方式是优化字典结构，即字典中字符有公共部分的比如teach和teacher都在字典中，那如果teach不匹配了，teacher这个单词也不用匹配了。将顺序的字典变为前缀树的存储方式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11cf4a955148c22fc694e082cf3f2abb.png\" /></p><p></p><p>与此同时，我们继续在这种情况下优化，如果一个词前缀为另一个词的子串，如图 6 中she和her，检测字符串sher中含有前缀树中多个词，当she在状态4匹配成功后我们知道he也是待匹配串中的，因此我们不需要跳回到状态1，而是从h开始匹配，而是直接跳转至状态9，从r开始匹配即可。便是前缀自动机的核心思想。</p><p></p><p>这种跳转关系构建方法是一种递归过程。用一句话来概括，一层一层的构建，如果我的父节点的跳转状态的子节点中有与我相同的，那就是我的跳转状态，否则我的跳转状态就是根结点。按照这个思路，我们将上图的点按遍历顺序重新标号以便于理解。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/171deb41f77222f7a750db263e9f251a.png\" /></p><p></p><p>有了这个前缀转移关系，我们便能高效的处理热词构建及查询，一个带有前缀自动机的解码流程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0ad25d1d659f08dcda62506265b0463.png\" /></p><p></p><p>即对于每个beam&nbsp;search过程我们不仅维护beam&nbsp;search过程中结果，同时维护其处于前缀自动机状态；此状态便于维护，仅存储状态指针即可。从性能上看，执行时的额外计算量及内存使用量都可以认为是常数增加。</p><p></p><h2>前缀自动机实时增加新词方案</h2><p></p><p></p><p>前一段中我们介绍了通过前缀自动机的解码方案，但这一过程依然不能很好解决“实时”这一要求，如果我希望实时向解码过程中添加热词，需要怎么改进？</p><p></p><p>回顾上一节的内容，我们了解到前缀自动机构建应分为两步，即前缀树的构建与状态转移的构建。其中，前缀树是算法正确性的保证，而状态转移可以大幅优化时间。同时，状态转移需要层次遍历整棵前缀树，这意味转移状态的构建不能随前缀树形态更改而自动更改，而必须全量重新构建。</p><p></p><p>当我们插入一个新词，由于前缀树的特性，可以在字符长度的复杂度将该词插入前缀树，但是构建新的转移状态需要遍历所有节点，如果每次插入新词都要重新访问整棵树全部节点，这种复杂度是难以接受的。比较起来，损失一些转移状态等价于将部分词的查询复杂度变大，对比遍历全部词典的复杂度这种损失是可以接受的。</p><p></p><p>根据上述想法，我们将整体查询变为两棵树，一棵为带转移状态的前缀自动机，另一棵为普通字典树，当新词插入时我们在普通字典树插入，当普通字典树规模大于一规定阈值后我们将他们合并，并在合并后的树上构建转移状态。对于一次查询，复杂度从词长变为小字典树规模，但能够节省构建转移状态遍历全文的时间。</p><p></p><h2>方案效果</h2><p></p><p></p><p>我在语音识别系统中应用了这种解码方案，并通过两方面指标评估该模块效果，一方面通过标注带关键词语音数据集，评估关键词准召。在下表中，beam search代表普通方案，ac automation代表前缀自动机加权解码方案，发现在识别结果中对关键词召回相对提升4.6%。另外，我们对比在普通语音识别数据集上字错误率（CER），由于对特殊词提升了权重使整体准确率有一定的下降，但整体损失可以接受，低于对关键词召回的收益。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/69e148f4a4183977429c670935c567c7.png\" /></p><p></p><h2>总结</h2><p></p><p></p><p>本文主要基于seq2seq类模型，通过追加状态转移自动机来减少模型在面对专有领域语料时的识别准确率；同时可以使语料实时生效，并将该工作的实时性流程构建方法加以介绍。对于语音转录文本中的关键词检测，常常局限于语音模型不能实时调整，很难识别出新词，通过这种方案可以做到秒级别新词添加，显著改变这一困扰。</p><p></p><p>参考文献：</p><p></p><p>[1] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" Advances in neural information processing systems 27 (2014).</p><p></p><p>[2] Zhao, Ding, et al. \"Shallow-Fusion End-to-End Contextual Biasing.\" Interspeech. 2019.</p><p></p><p>[3] Hori T, Hori C, Minami Y, et al. Efficient WFST-based one-pass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition[J]. IEEE Transactions on audio, speech, and language processing, 2007, 15(4): 1352-1365.</p><p></p><p>[4] Williams I, Kannan A, Aleksic P S, et al. Contextual Speech Recognition in End-to-end Neural Network Systems Using Beam Search[C]//Interspeech. 2018: 2227-2231.</p><p></p><p>[5] Hori T, Nakamura A. Speech recognition algorithms using weighted finite-state transducers[J]. Synthesis Lectures on Speech and Audio Processing, 2013, 9(1): 1-162.</p><p></p><p>[6] <a href=\"https://blog.csdn.net/weixin_53360179/article/details/119718426\">https://blog.csdn.net/weixin_53360179/article/details/119718426</a>\"</p>",
    "publish_time": "2022-09-23 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "更好的中英文混合语音识别系统",
    "url": "https://www.infoq.cn/article/FI5XMrlruQx4HVANqyOh",
    "summary": "<p>（文/商迎新 王伟戌 王强强）&nbsp;&nbsp;&nbsp;&nbsp;</p><p></p><h2>研究背景&nbsp;</h2><p></p><p></p><p>语音作为人与人交流的直接媒介，承载着人们日常生活中的大部分信息来源。基于近年来通信技术与物联网的发展，各式各样的语音助手、智能家具等软硬件层出不穷，人机交互技术的发展及人们对其需求日益攀升。语音识别技术在人机交互上扮演着重要的角色，任何因其导致的识别错误都可能在人机交互系统中的各个模块上传播，并最终导致交互失败。因此针对语音识别的研究具有重要的学术价值和应用价值。</p><p></p><p>混合语言现象常常出现在能够流利使用多种语言的群体中。英文作为全球的通用语言，就时常以各种形式与其他语言混合在一起。然而现有的大多数最先进的语音识别系统都专注于单语种语音识别，即它们一次只能处理一种语言，这样的系统无法识别中英混合语言的语音。随着语音技术开始渗透到人类生活的方方面面，混合语言的现象受到越来越多的关注。因此，开发用于中英文混合语言的自动语音识别(CSSR)系统尤为重要。</p><p></p><p></p><h1>行业现状&nbsp;</h1><p></p><p></p><p>中英文混合语音识别算法属于多语言语音识别领域。但与常规多语言语音识别不同，常规多语言语音识别仅针对一句话中出现一种语言，而混合语言语音识别则是指同一句话中说话人会在两种语言间切换使用。尽管语言学家对混合语言现象已经研究了长达半个多世纪，随着近年来语音技术的不断突破，对混合语言语音识别的研究近二十年才被人们重视。针对中英文混合语音识别也是近十多年来才开始研究。</p><p></p><p>其技术难点主要表现为：嵌入语受主体语影响形成的非母语口音现象严重、不同语言音素构成之间的差异给混合声学建模带来巨大困难、带标注的混合语音训练数据极其稀缺。传统语音框架基于单一语种基础建模单元，如汉语是基于拼音的声母韵母、英语则是英文的音素，这种技术架构对指定语种的语言学知识依赖较大，难以扩展到多语种识别。</p><p></p><p>由于不同语言之间的声学单元相互独立，且声学属性不同，常规基于声学单元建模的&nbsp;DNN-HMM 语音识别模型无法很好的建模不同语言之间声学属性的联系。而端到端模型无需对于声学单元建模，转而采用字符建模，模糊了建模单元与声学属性之间的关联。并且由于端到端模型能够考虑帧的上下文信息，可以有效建模语言转换点的声学属性。因此最近几年的研究[1][2]偏向于采用端到端方式搭建混合语言语音识别系统。</p><p></p><p>基于深度学习的端到端模型灵活且复杂，相较于传统语音识别，融合多任务学习也能够提升模型性能。考虑到混合语言语音识别系统的特有属性，有学者提出可以鉴于 LID 模型[3][4]能够判别语言之间的差异性，以进行分类。中英文混杂识别联合语种识别受到越来越多的关注，在识别文本内容的同时进行语言分类，以增强对不同语言的分辨能力。本文从[5][6]获取灵感，在端到端网络模型基础之上添加语种信息进行联合训练，期望增强模型对不同语言的识别以及判别能力。</p><p></p><p></p><h2>作业帮实践&nbsp;</h2><p></p><p></p><h2>3.1&nbsp;实验细节介绍</h2><p></p><p></p><p>为了便于后续讨论，首先介绍我们实验所采用的数据集、模型训练建模单元以及最终评价指标。</p><p></p><p>（1）我们的训练语料为作业帮标注的约1000小时的老师上课的英文授课混合数据集，并在其中随机取30小时作为训练开发集，6.7小时作为测试集，其余数据作为训练集。</p><p></p><p>（2）中文训练最小单元以汉字建模，但英文字母只有26个，因此以子词[7]替代字母作为英文的建模单元。从声学层面上，中文字符对应的音频长度远大于英文字母对应的长度，因此采用子词能够增大英文建模单元所对应的声学时长，从而减小中英文建模之间的差异，增强模型训练鲁棒性。</p><p></p><p>（2）在语音识别系统的评估中，若单元为字符，则该错误率称为字符错误率 (Character Error Rate, CER)。若单元为单词，则称为词错误率 &nbsp;(Word Error Rate, WER)。中英文混合语音识别系统由于包含两种语言，则中文部分计算 CER，英文部分计算WER，称为混合错误率（Mix Error Rate, MER）。</p><p></p><p></p><h2>3.2&nbsp;基线识别模型</h2><p></p><p></p><p>混合语言语音识别本质上还是语音识别任务，为了方便说明结果，我们的基线识别系统采用语音识别框架Wenet[8]，Wenet网络结构设计借鉴Espnet的joint loss框架，这一框架采取Conformer Encoder + CTC/attention loss，利用帧级别的CTC loss和label级别attention-based auto-regression loss联合训练整个网络。这一框架是目前语音领域比较流行的框架之一。我们的实验也在其网络模型基础之上进行改进并对比实验效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/98825b29f174f384e28ccebe89066325.png\" /></p><p></p><p></p><h2>3.3&nbsp;语种信息联合训练</h2><p></p><p></p><p>将语种信息加入中英文混合语言语音识别的网络模型训练，设X为声学模型的特征序列，Y为模型的预测输出的中文字符或者英文字词，Z&nbsp;为语种预测的输出类别 ，则最终的模型损失函数由之前的公式(1)增加为公式(2)。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8c/e0/8c0a87b713eb6f8d6bcyy86a2bee29e0.png\" /></p><p></p><p></p><p></p><h3>3.3.1&nbsp;帧级别语种loss联合训练</h3><p></p><p></p><p>在混合语言语音识别中，不同语言的具有相似发音的单元很有可能被识别错误，因此我们考虑对识别系统加入语言识别。整体框架如下图所示，Wenet编码器由基于Attention的解码器、CTC 和 LID 模块共享。它将输入序列 X 转换为高维特征 H。基于Attention的解码器和 CTC 生成输出序列 Y，而 LID模块则输出每一帧的语种ID。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/279d64ea5cbd7f64603284affc6f7e70.png\" /></p><p></p><p></p><p>我们预先将数据集进行帧级别语种对齐，在实验中，我们利用的是之前训练过的中英文混合数据得到的chain模型，得到中英文混合数据的帧级别LID-label，在训练过程中，将语种信息的预测和label的交叉墒作为语种的损失函数进行联合训练，最终目标函数为公式（2）所示。</p><p></p><p></p><h3>3.3.2&nbsp;token级别语种loss联合训练</h3><p></p><p></p><p>考虑到帧级别模型语种信息获取需要提前对中英文混合数据进行对齐，如果对齐不准，最终错误会累积。虽然模型在解码时，可以推断出语种信息，但是在模型训练中未加入此信息，因此在这直接进行token级别的语种信息联合训练，网络框架如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a01b2d31e947cd0ade6fb18c2d7c2588.png\" /></p><p></p><p>在这里，我们探讨了两种 LID 的loss添加方式，一种是与语音识别任务共享相同的注意力模型（LID-shared），另一种是单独学习一个独立的注意力模型（LID-indep）。两种方法都使用与等式 (2) 中相同的目标函数。</p><p></p><h3>3.3.3语种信息联合训练实验总结</h3><p></p><p></p><p>1、3.3.2介绍过，token级别的语种信息添加有两种方式，即是否与识别系统共用Attention，为了更有效率的对比两种方式的优劣性，我们先在小数据集上进行对比，在1000小时训练集中随机抽取70小时作为训练集，进行实验对比，效果如下表</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23261f25f6236a226df00d8c7dcb12c2.jpeg\" /></p><p></p><p>由以上表格可以看出，LID-indep效果较好。因此后续添加token级别的语种信息时，都采用LID-indep方式&nbsp;，并将系数设置为0.2。&nbsp;&nbsp;</p><p></p><p>2、利用全部数据进行训练,为了方便进行记录，将帧级别添加语种信息训练模型称为LID-frame。为了验证加入语种信息训练正确，我们还统计了LID的识别准确度T-LID-ACC。以免中英文占比分布不均匀，这里统计的是F1 score 。实验结果如下表：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6074baea7ef3007790503bdf253b380a.jpeg\" /></p><p></p><p>从以上表格来看，LID-indep在测试集上效果要优于基线模型，相对提升约1.76%，并且语种判别准确度也达到了98%，证明了添加token级别语种信息联合训练的有效性；</p><p></p><p>同时LID-frame效果要比基线系统差，分析原因可能有以下几个方面：（1）帧级别的对齐label构建存在误差，传统混合语言语音chain模型的识别效果较差，会造成错误累积；（2）在训练过程中，Wenet会对数据进行语速增强，即随机对音频进行(0.9,1.0,1.1)倍变速，由于帧级别的label需要提前对齐，因此训练过程中只能关闭掉数据增强部分，模型鲁棒性变差，因此效果要低于基线模型。</p><p></p><h2>3.4&nbsp;混合语言错误类型分析</h2><p></p><p>在验证添加语种信息训练的有效性基础之上，我们进行识别case分析，统计错误率分类占比，确定下一步优化策略。下表格是中英文错误单独统计的MER。</p><p><img src=\"https://static001.geekbang.org/infoq/1c/1ce354192a09b027128bc0a32057929c.jpeg\" /></p><p>由上表格可以看出，混合语言语音识别中，无论是中文还是英文，均是替换错误占比较高，因此我们统计下替换错误的类型如下表（其中E代表英文，C代表中文，SE →E则代表英文单词替换为英文单词）。</p><p></p><p></p><p>经过细致的错误分类，我们可以看出，不同语种之间的相互替换占比较低，替换错误主要发生在同类语种之间，尤其是英文单词替换为英文单词的占比较高为7.8%，接下来我们通过添加语言模型辅助，期望降低英文同类语言的替换错误。</p><p></p><h2>3.5&nbsp;语言模型增强</h2><p></p><p>WeNet中的LM支持方案，其中语言模型需要自己构建，依靠ctc wfst search生成n-best，wfst search为依靠传统解码图的传统解码方式。需要注意的是由于我们的建模单元是汉字加英文子词，因此构建的词典L是汉字映射为词语，英文子词映射为英文单词，英文词典的构建需要和训练时分词方式保持一致。T为端到端训练时的建模单元 ，G为语言模型即将n-gram的语言模型转为WFST形式的表示，最终构建TLG进行解码。以下是在LID-indep模型基础之上添加语言模型的效果。</p><p></p><p>由上图表明，我们添加TLG实验的有效性，在测试集上错误率相对下降约2.71%，同时计算下测试集的替换错误如下表</p><p></p><p>添加TLG同时也降低了中英文同类替换错误，由之前的3.729下降到3.388，0.268下降到0.223，分别相对下降约9.14%&nbsp;和&nbsp;2%，降低了混合语言测试集的替换错误。进一步验证了添加语言模型的有效性。&nbsp;&nbsp;</p><p>&nbsp;</p><p></p><h2>3.6最终实验对比</h2><p></p><p>在模型训练期间，词典是由训练文本产生，因此训练文本和开发集中几乎不含有,在实际业务场景中是不合理的，因此在后续我们添加了约50小时的带有 的数据，来增强模型鲁棒性，实验步骤和前面介绍的一致。最终实验结果如下表所示。</p><p></p><p>另外，考虑到在实际的业务场景中，我们几乎不关注感叹词的识别情况，也不在意&lt;他她它&gt;的对错与否，因此将参考文本和识别结果中的感叹词过滤，统一&lt;他她它&gt;，重新将基线模型效果和最终实验效果进行测试，结果如下表所示</p><p></p><p>最终将感叹词去掉后，和基线模型相比较，混合错误率相对降低约6.96%，中文错误率相对降低约6.41%，英文错误率相对降低约8.24%。&nbsp;</p><p>&nbsp;</p><p></p><h1>总结与展望&nbsp;</h1><p></p><p>在优化中英文混合语言识中，我们通过三个方面来提升中英文混合的识别效果。</p><p></p><p>第一是模型训练层面，在Wenet的基础之上，我们对比了不同语种信息加入方式的优劣性，并从中选出最适合匹配基线模型的方式，测试集效果提升相对约1.76%；</p><p></p><p>第二是数据方面，为了更贴合实际业务场景，很多未在模型训练词典中的词可以识别出来，因此我们加入了部分数据，进一步提升识别系统的可用性，相对提升约3.1%；</p><p></p><p>第三是考虑到中英文语言文本的连贯性，进一步通过语言模型来增强混合语言语音识别模型，构建TLG，进一步相对提升约2.5%&nbsp;。最终相对基线模型提升约7.8%。</p><p></p><p>最终在实际应用方面，我们去除了感叹词和将&lt;他她它&gt;进行统一后，对比测试基线模型效果，从整体对比，混合错误率相对降低约6.96%，中文错误率相对降低约6.41%，英文错误率相对降低约8.24%。</p><p></p><p>同时，实验也还有很多不足之处，后续会考虑从不同训练方式层面来提升中英文混合语言语音识别模型的识别效果。例如训练模型参数调优、预训练[9]以及无监督[10]的方式生成大量混合数据文本等。</p><p></p><p>【参考文献】</p><p>[1] Shan, Changhao, &nbsp;et &nbsp;al. &nbsp;\"Investigating &nbsp;end-to-end &nbsp;speech &nbsp;recognition &nbsp;for &nbsp;mandarin-english code-switching.\" in Proc. of the 2019 IEEE International Conference on Acoustics, Speech and &nbsp;Signal Processing (ICASSP). IEEE, 2019</p><p>[2] Emre Yılmaz, Samuel Cohen, Xianghu Yue, David van Leeuwen, and Haizhou Li, “Multi-graph decoding for code-switching ASR,” in Twentieth Annual Conference of the International Speech Communication Association &nbsp;(INTERSPEECH). ISCA, 2019, pp. 3750–3754.</p><p>[3] Chan, Joyce YC, et al. \"Detection of language boundary in code-switching utterances by bi-phone probabilities.\" in Proc. of the 2004 International Symposium on Chinese Spoken Language Processing. IEEE, 2004.</p><p>[4] Weiner, Jochen, et al. \"Integration of language identification into a recognition system for spoken &nbsp;conversations &nbsp;containing &nbsp;code-switches.\" Spoken &nbsp;Language &nbsp;Technologies &nbsp;for &nbsp;Under-Resourced Languages. 2012. &nbsp;</p><p>[5] Zeng Z, Khassanov Y, Pham V T, et al. On the end-to-end solution to mandarin-english code-switching speech recognition[J]. arXiv preprint arXiv:1811.00241, 2018.</p><p>[6] Luo N, Jiang D, Zhao S, et al. Towards end-to-end code-switching speech recognition[J]. arXiv preprint arXiv:1810.13091, 2018.</p><p>[7] A. Zeyer, K. Irie, R. Schlu ̈ter, and H. Ney, “Improved training of end-to-end attention models for speech recognition,” arXiv preprint arXiv:1805.03294, 2018.</p><p>[8] Yao Z, Wu D, Wang X, et al. Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit[J]. arXiv preprint arXiv:2102.01547, 2021.</p><p>[9] Xinyuan Zhou, Emre Yilmaz, Yanhua Long, Yijie Li and Haizhou Li. &nbsp;\"Multi-</p><p>Encoder-Decoder Transformer for Codeswitching Speech Recognition.\" in Proc. of the 21st Annual &nbsp;Conference &nbsp;of &nbsp;the &nbsp;International &nbsp;Speech &nbsp;Communication &nbsp;Association &nbsp;(INTERSPEECH). ISCA, 2020.</p><p>[10] Guo, Pengcheng, et al. \"Study of semi-supervised approaches to improving english-mandarin code-switching speech recognition.\" arXiv preprint arXiv:1806.06200 (2018).</p>",
    "publish_time": "2022-09-23 09:02:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "语音评测技术在古诗背诵场景中的应用",
    "url": "https://www.infoq.cn/article/3ZFao6WKxQO8qTTjBqIF",
    "summary": "<p>(文 / 作业帮语音技术团队&nbsp;王洲 王强强)</p><p></p><p>口语环节正在语言类教育课程中获得更多重视，一对一的师生交流和指导是提高口语水平最有效的方式，但该方式很难满足数量众多的口语学习者需求。得益于计算机技术和语音评测技术的突飞猛进，计算机辅助语言学习（Computer Assisted Language Learning）技术应运而生，各种基于人工智能技术的口语评测方案相继落地。包括朗读，背诵，复述，自由表达等多种方式，它为学生提供了额外的学习机会和充足的学习材料，能够辅助或者替代教师指导学生进行更有针对性的发音练习，指出学生的发音错误，提供有效的诊断反馈信息，并评估学生的整体发音水平，从而切实提高学生的口语学习效率和口语水平。</p><p></p><h2>语音评测技术简介</h2><p></p><p></p><h3>2.1 评价指标</h3><p></p><p></p><p>发音评测维度包括发音的准确率，流畅度，完整度，韵律，语调等。</p><p></p><p>准确度：体现用户的发音水平。</p><p>流利度：体现用户朗读流畅程度，和语速、停顿次数相关。</p><p>完整度：体现用户发音正确的单词占比。</p><p>单词得分：句子中每个单词的得分。</p><p>句子得分：段落中每个句子的得分。</p><p>总分：评测整体得分，综合上面分数获得，准确度对总分影响最大。</p><p></p><p>口语好坏的评价是主观的，人工专家根据自身的专业知识及经验，在各个维度按照优、良、中、差、劣等级进行打分，不同的专家会得出不同的结果。评价机器口语评测系统是否靠谱，一般采用相关系数（Pearson correlation coefficient），一致性（kappa coefficient）等指标来衡量打分系统的性能。与专家打分越接近，系统越靠谱。但有多靠谱，需要有参照物，通常会用平均的人与人之间的相关性来作为参照，目前的人机相关性、一致性已经超过了人与人之间的平均相关性，一致性。语音评测技术已被普遍使用在中英文的口语评测和定级中。</p><p></p><h3>2.2 技术框架</h3><p></p><p></p><p>语音评测目前的主流方案是基于隐马尔科夫-深度神经网络（Hidden Markov Model，HMM，Deep Neural Networks, DNN）模型获得语音后验概率，与评测文本强制对齐（force alignment）后，使用GOP（Goodness of Pronunciation）方法进行打分，流程如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/86e0f58962fdbf2861699cf809fd409d.png\" /></p><p>&nbsp;</p><p>1）声学特征提取</p><p></p><p>语音信号在一帧（frame）观察窗口内平稳，将切分后的时域片段信号转换到频域，获得MFCC（13维或40维），Fbank（80维）等特征，一帧是语音序列的最小单元。在GMM（Gaussian Mixture Model）模型中，使用MFCC特征，是因为GMM的协方差估计使用了对角矩阵，输入特征需要保证每一帧的元素之间相互独立。DNN模型中，特征可以是MFCC或Fbank特征；</p><p></p><p>2）HMM-GMM无监督聚类</p><p></p><p>该过程用来获取帧标签（Frame-wise label）进行后面的DNN训练。HMM对语音的时序约束、依赖进行建模，GMM属于生成模型，对HMM的观察概率（属于各个音素的概率）进行建模。语音的标注通常是单词序列，每一帧对应哪个音素提前是不知道的。GMM通过相似度聚类，同一个音素的相邻各帧归为同一类，获得音素的duration、边界信息。目前语音信号中对齐算法主流依旧是HMM-GMM，其它算法ctc，attention对齐都有各自的问题，ctc的尖峰，attention的时序无约束问题。</p><p></p><p>3）DNN判别学习</p><p></p><p>GMM聚类效果尚可，但GMM对音素建模能力有限，无法准确的表征语音内部复杂的结构。通过DNN取代了GMM 来进行HMM观察概率的输出，可以大大提高准确率。DNN模型的最后一层使用softmax进行软分类，交叉熵loss进行训练，DNN的输出称为后验概率 (phonetic posteriorgrams，PPG)。</p><p></p><p>用一个例子来说明DNN的输出，假设有5帧语音特征, 发音内容为“er_3 duo_0”，取[sil,er_3,d,uo_0,uo_3]PPG如下：</p><p></p><p>在t=0第一帧，er_3的概率0.921272最大。</p><p>&nbsp;</p><p>4）评测文本构建hmm解码图，约束文本时序关系：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85d97be15c1d5dcde0ce7d60efd17924.png\" /></p><p>&nbsp;</p><p>对于上面的评测文本“er_3 duo_0”, 我们要评价每个音素的发音正确性，也要保证时序性，如果用户读的是“d uo_0 er_3”每个音都对，但时序是错误的，为了约束这种时序关系，我们定义了如上的HMM结构。状态0到1，1到2，2到3， er_3，d，uo_0 每个音至少发生一次，状态1，2，3上面每个都有自旋，表示该音素可以重复发生。</p><p></p><p>5）Viterbi解码获得强制对齐结果</p><p></p><p>首先我们将PPG，表示成状态转移图，如下图所示，有六个状态，状态之间的弧表示各个音素及音素的后验概率，0粗线圆圈代表开始状态，5双圆圈代表结束状态。如果音素个数为C，帧数为T，则弧路径个数为CT。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b71eb9b7915b1dd0693830a47f64d87e.png\" /></p><p>&nbsp;</p><p>DNN的后验概率转移图，通过评测文本HMM解码图的约束，在CT条路径中，只剩下构成“er_3d uo_0”的路径及相应的score。使用Viterbi算法我们可以高效获得score最大路径。上图中Viterbi最优路径[er_3, er_3,d, uo_0, uo_0] ，也就是强制对齐结果。</p><p></p><p>6）GOP准确度打分,Averaged Frame-level Posteriors[1]：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/621189b1215fc355ada5caee33fa986b.png\" /></p><p></p><p>P(St|Ot)是DNN模型softmax的输出,Ot是t时刻语音帧，St是force alignment后t时刻的对齐音素。ts,te是音素的开始和结尾。</p><p></p><p>GOP(er_3)=(0.92+0.95)/2</p><p>GOP(d) =0.99</p><p>GOP(duo_0)=(0.65+0.80)/2</p><p></p><p>&nbsp;</p><p>7）整体打分</p><p></p><p>获得音素的准确度打分，离最终的单词，句子，整体打分还有距离。评测任务最后都会接一个打分模型，使用音素的GOP分数，流利度等特征拟合到专家分数。2021年腾讯提出一种将声学模型中的深层特征[2]，而不是GOP特征传递到打分模块中，该模块基于注意力（attention）机制，考虑句子中不同粒度之间的关系，获得了更优的整体评分。</p><p></p><p>作业帮目前基于特征提取工程，收集了包括GOP分数，元辅音，词性，声调，发音时长，流利度等多维度信息，通过神经网络模型强拟合能力，预测单词，句子的分数结果，整体打分效果跟人工打分取得了较高的一致性。</p><p></p><h2>DNN声学模型的改进</h2><p></p><p></p><h3>3.1 conformer模型</h3><p></p><p></p><p>Conformer是Google在2020年提出的语音识别模型，基于Transformer改进而来，主要的改进点在于Transformer在提取长序列依赖的时候更有效，而卷积则擅长提取局部特征，因此将卷积应用于Transformer的Encoder层。Conformer的Encoder模型由多个encoder_layer叠加而成,每个encoder_layer由feed-forward ，multi-head self-attention，convolution，feed-forward组成。</p><p></p><p>在流式评测任务中，随着用户的朗读，需要实时快速的返回评测结果，直接使用conformer模型进行流式评测，会遇到以下问题：</p><p></p><p>1）由于attention机制，每一帧数据需要看到全部数据做weight计算，随着语音时长的增加，计算复杂度和内存储存开销会越来越大；</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6091f4fc84de9e8aef67e19459f984a5.png\" /></p><p></p><p>2）为了提高帧的识别准确率，每帧会向前看几帧数据，随着encoder_layer的增加，向前看数据量就会累加，这会造成很大的延迟；</p><p></p><h3>3.2基于块（chunk）的mask流式解决方案[3]</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/6107e1c8da3743f7c1c2d4b74f59f16e.png\" /></p><p></p><p>如上图所示，相邻三帧为一个chunk，通过mask掉一些数据，attention作用在当前chunk和前一个chunk中，当前chunk中的每一帧对右边的视野依赖最多两帧，平均时延为一帧时长，在每个encoder_layer层，对历史数据的依赖最多五帧。通过mask机制牺牲掉一些attention带来的收益，换取流式低延迟作为妥协。</p><p>conformer中的卷积模块也使用了因果卷积，避免右边视野的累积。通过以上的改进，conformer胜任了流式评测任务，时延在300ms左右。</p><p></p><p></p><h2>古文背诵应用中的特殊处理</h2><p></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24264c8b56fa69eabdcbc0f62712fbef.png\" /></p><p></p><p>如上图所示，学生在手机应用中选择古文进行背诵，屏幕上就会实时显示背诵情况，正确染成黑色，有发音错误，染成红色。背诵后给出整体的评分报告，以此帮助学生检查和进行背诵练习。</p><p></p><p>多分支评测:古文背诵中，评测文本一般为一首诗，或古文的一段，包含了多句话。评测文本的解码图需要以句子为单位，支持句子的重复读（家长领读，小孩跟读），跳读（可以往前跳，也可以往后跳读）。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a6/a6f84fdeceb3a58e7dd6178e69b805ef.png\" /></p><p></p><p>为了关注句子级别的构图关系，上图做了简化，将音素改为字，去掉字的跳读，重复读等弧。0为开始状态，1是结束状态，到达结束状态可以继续回到开始状态，开始下一句的背诵，每句背诵前面会有一个句子标识“$0”,“$1”等，通过该标识，我们可以知道当前用户读到了哪一句，方便染色。</p><p></p><p>上面的构图方式在一些诗句背诵中遇到问题，当诗句中存在重复诗句，或开头相似的句子。当回到开始状态0时，由于每条路径都是等概率的。会出现如下的现象：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b1a898c01cc173cae0362da1b5ed07d3.png\" /></p><p></p><p>在t+1 时刻读完了第二句，应该开始第三句“$3鱼”的染色，但由于等概率，及顺序问题，仍然会走$2路径，当t+5时刻读到“东”，路径会突然跳转到“$3鱼戏莲叶东”。用户的体验就是，读完第二句一直不染色，读到“东”，立马第三句整句染色，影响体验。</p><p></p><p>解决方案：如下图所示，每一句读完添加到下一句的弧，例如状态7到8添加空边，第一句读完可以走第二句路径，或者回到起始状态，但添加惩罚（比如0.5）。这样就可以保证古文多句的顺序读，但又保留了足够的自由度，允许重复读，跳读。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/436193f251359ddf2b87c2dceca8d95e.png\" /></p><p></p><p></p><h2>评测引擎本地化</h2><p></p><p></p><p>评测服务在部署阶段遇到了如下问题：</p><p></p><p>1）课堂场景应用评测技术会带来超高并发，直播课场景，老师下发题目后学生几乎是同一时刻做答，并发量瞬间达到几万，要保证用户在短时间内尽快拿到打分结果，不能通过消息队列异步处理，只能提前为每个连接准备资源，需要大量的服务器资源支撑。</p><p></p><p>2）网络传输有一定的时延，对于需要做实时染色的评测服务，会感觉到染色时延，体验较差。</p><p></p><p>3）语音评测需要建立长链接，用户的网络有波动，但是语音评测服务要实时返回结果，无法缓存，会造成一定程度的评测失败。</p><p></p><p>为了解决这些问题，我们考虑开发本地评测方案，相比云端，本地评测具备以下优势：低延迟：节省了网络请求的延迟，优化了用户体验。安全性：更好的保护用户隐私数据。更稳定：消除了网络波动对服务的影响。节约云端资源：根据不同的本地算力，与云端推理结合，从而降低对云端算力的压力。</p><p></p><p>作业帮语音评测技术经过数轮迭代，在保证效果的前提下，conformer模型通过裁剪以及量化等处理，压缩到 10M 以内，极大降低了端的算力要求。</p><p></p><p>作业帮用户体量比较大，部分用户使用的手机配置较低，难以进行端上计算，为了给所有用户带来好的体验，我们采用了端云一体解决方案，如下图所示。为了结合“端”和“云”的优势，在评测时，服务判断端上算力，对于较高算力的设备，评测是在“端”上完成；对于较低算力的设备，评测是在“云”上完成。在“端”的解决方案中，能提供PC、手机、平板电脑、学习笔等多类型的硬件设备；跨平台支持iOS，安卓、Windows、鸿蒙、Linux等操作系统。</p><p></p><p>使用端云一体方案后，延时从原来的200ms 降低到 50ms 以内，用户体验明显提升。同时释放了大量服务端资源，服务端资源占用降低为原有的 20%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3eb7b4a3434e09cdf4709165d8c1b8e2.png\" /></p><p></p><p></p><h2>技术总结展望</h2><p></p><p></p><p>语音评测技术和语音识别任务非常类似，近些年都获得了快速发展，语音识别中各种端到端算法不仅简化了训练流程，同时降低了整体错误率。评测技术也从原来的HMM-GMM升级到HMM-DNN,准确率大幅提升。但在语音评测中GMM依旧是非常核心，必不可少的算法模块，通过GMM良好的聚类能力，音素的边界信息被提取出来。但能否通过神经网络直接获得对齐信息，大家有在各种尝试，interspeech2021会议论中 Teytaut通过ctc与Auto-Encoder的结合获得了优于GMM的对齐效果[4]，ICASSP2022会议论文中字节跳动提出了基于双向注意力神经网络强制对齐（ForcedAlignment）模型 NeuFA[5]。生成模型Auto-Encoder，Variational Auto-Encoder，以及attention机制都有不错的对齐潜力，语音评测技术如果能借鉴这些方法实现完全的端到端训练，准确度有机会进一步提高。&nbsp;</p><p>&nbsp;</p><p>References：：</p><p>[1]A NewDNN-based High Quality Pronunciation Evaluation for</p><p>Computer-AidedLanguage Learning (CALL)</p><p>[2]Deepfeature transfer learning for automatic pronunciation assessment</p><p>[3]DevelopingReal-time Streaming Transformer Transducer for Speech Recognition onLarge-scale Dataset</p><p>[4]Phoneme-to-audio alignment withrecurrent neural networks for speaking and singing voice</p><p>[5]NeuFA: Neural Network Based End-to-EndForced Alignment with Bidirectional Attention Mechanism</p>",
    "publish_time": "2022-09-23 09:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《架构师成长计划》异构计算数据中心“芯”变革",
    "url": "https://www.infoq.cn/article/tV9I4XUiJcSOEumAP5y1",
    "summary": "<p>国际学术期刊Science/AAAS和英特尔在全球首次联袂推出第一季《架构师成长计划》以来，吸引了无数架构师踊跃参与，获得业内广泛赞誉。为持续助力架构师把握数智机遇，构建未来，第二季《架构师成长计划》全新升级，强势归来！业内顶尖架构师大咖齐聚，为架构师群体量身打造系统成长课程，带来涵盖云游戏、云原生、联邦学习、生信大数据、网络智能化、算力网络、云网融合等多个热门话题的前沿技术及案例实践。</p>\n<p>以数据中心、人工智能等为代表的高性能计算类应用的发展，驱动算力需求不断攀升。然而，当前单一的计算架构已经无法处理复杂多样的数据类型。异构计算既能提高算力和性能，降低成本及功耗，又具备多类型任务的处理能力，凭借其强大优势，成为公认的突破算力瓶颈的关键技术方向。但实际运用中，异构计算在技术、互连和软件等方面仍面临着技术难题，如何寻求解决之道，释放异构计算优势，助力业务变革？</p>\n<p>英特尔《架构师成长计划》第六期精彩上线！邀请快手异构计算中心AI&amp;HPC负责人钟辉、英特尔云与行业解决方案事业部互联网行业技术总监高明、极客帮科技创始人&amp;CEO霍太稳三位业内专家，共同探讨异构计算的优势和未来前景，倾力分享异构计算的前沿应用场景、架构设计及硬软件方案，并通过客户实际应用案例详细阐述其如何助力企业化解算力瓶颈、实现更佳的并发、吞吐、延时与经济性，为您提供极具价值的现实参考！期待您的参与！点击链接观看完整课程https://bizwebcast.intel.cn/eventstart.aspx?eid=325&amp;tc=6om8hxw514&amp;frm=InfoQ</p>\n<p><strong>精彩亮点</strong></p>\n<ul>\n<li>什么是异构计算？异构计算最能解决的市场问题和技术场景是什么？</li>\n<li>异构计算在实际运用中面临哪些痛点和挑战？解决之道又是什么？</li>\n<li>日活3.5亿的快手，如何通过异构计算化解算力瓶颈？</li>\n<li>在异构计算领域，英特尔又有哪些技术探索和应用案例？</li>\n</ul>\n<p><strong>议程</strong></p>\n<ul>\n<li>圆桌讨论：异构计算 数据中心“芯”变革</li>\n</ul>\n<p>钟辉快手异构计算中心AI&amp;HPC负责人</p>\n<p>高明英特尔云与行业解决方案事业部互联网行业技术总监</p>\n<p>霍太稳极客邦科技创始人、CEO</p>\n<ul>\n<li>课程分享：智能升级 异构创新</li>\n</ul>\n<p>钟辉快手异构计算中心AI&amp;HPC负责人</p>\n<ul>\n<li>课程分享：异构计算 数据中心“芯”变革</li>\n</ul>\n<p>高明英特尔云与行业解决方案事业部互联网行业技术总监</p>",
    "publish_time": "2022-09-23 11:07:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]