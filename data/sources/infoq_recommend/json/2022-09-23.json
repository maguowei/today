[
  {
    "title": "在Java中如何加快大型集合的处理速度",
    "url": "https://www.infoq.cn/article/pQ7VDmaij1aCT9TD0R06",
    "summary": "<p>本文讨论了Java Collections Framework背后的目的、Java集合的工作原理，以及开发人员和程序员如何最大限度地利用Java集合。</p><p></p><h2>什么是Java集合</h2><p></p><p>尽管Java已经过了25岁生日，仍然是当今最受欢迎的编程语言之一。超过100万个网站通过某种形式在使用Java，超过<a href=\"https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/\">三分之一</a>\"的软件开发人员的工具箱中有Java。</p><p></p><p>Java在它的整个生命历程中经历了重大的演变。一个早期的进步出现在1998年，当时Java引入了Collections Framework（Java Collection Framework，JCF），简化了操作Java对象的任务。JCF为集合提供了标准化的接口和通用方法，减少了编程工作，并提升了Java程序的运行速度。</p><p></p><p>理解Java集合和Java Collections Framework之间的区别是至关重要的。Java集合只是表示一组Java对象的数据结构。开发人员可以像处理其他数据类型一样处理集合，执行搜索或操作集合内容等常见任务。</p><p></p><p>Set接口（java.util.Set）就是Java集合的一个例子。Set是一种集合，不允许出现重复元素，也不以任何特定的顺序存储元素。Set接口继承了Collection（java.util.Collection）的方法，并且只包含这些方法。</p><p></p><p>除了集合之外，还有队列（java.util.Queue)和Map（java.util.Map）。Map并不是真正意义上的集合，因为它们没有<a href=\"https://howtodoinjava.com/interview-questions/useful-java-collection-interview-questions/\">继承集合接口</a>\"，但开发人员可以像操作集合一样操作Map。集合、队列、列表和Map都有后代，比如排序集合（java.util.SortedSet）和可导航Map（java.util.NavigableMap）。</p><p></p><p>在使用集合时，开发人员需要熟悉和理解一些特定的集合相关术语。</p><p></p><p>可修改与不可修改——正如这些术语表面上所表明的，不同的集合可能支持也可能不支持修改操作。可变集合与不可变集合——不可变集合在创建后不能被修改。虽然在某些情况下，不可修改的集合仍然可能由于其他代码的访问而发生变化，但不可变集合会阻止这种变更。不可变集合是指能够保证Collection对象中不会有任何变更的集合，而不可修改的集合是指不允许“add”或“clear”等修改操作的集合。固定大小与可变大小——这些术语仅与集合的大小有关，与集合是可修改还是可变无关。随机访问与顺序访问——如果一个集合允许为每一个元素建立索引，那么它就是可随机访问的。在顺序访问集合中，必须通过所有前面的元素到达指定的元素。顺序访问集合更容易扩展，但搜索时间更长。初学者可能会难以理解不可修改集合和不可变集合之间的区别。不可修改集合不一定是不可变的。实际上，不可修改集合通常是可修改集合的包装器，其他代码仍然可以访问和修改被包装的可修改集合。通常需要使用集合一些时间才能在一定程度上理解不可修改集合和不可变集合。</p><p></p><p>例如，我们将创建一个可修改的按市值排名前五的加密货币列表。你可以使用java.util.Collections.unmodifiableList()方法创建底层可修改列表的不可修改版本。你仍然可以修改底层列表，它只是被包装成不可修改列表，但你不能直接修改不可修改的版本。</p><p></p><p><code lang=\"java\">import java.util.*;\npublic class UnmodifiableCryptoListExample {  \n    public static void main(String[] args) {  \n\n        List cryptoList = new ArrayList&lt;&gt;();  \n        Collections.addAll(cryptoList, \"BTC\", \"ETH\", \"USDT\", \"USDC\", \"BNB\");  \n        List unmodifiableCryptoList = Collections.unmodifiableList(cryptoList);  \n        System.out.println(\"Unmodifiable crypto List: \" + unmodifiableCryptoList);  \n\n        // 尝试在可修改列表中再添加一种加密货币，并显示在不可修改列表中\n        cryptoList.add(\"BUSD\");\n        System.out.println(\"New unmodifiable crypto List with new element:\" + unmodifiableCryptoList);\n\n        // 尝试添加并显示一个额外的加密货币到不可修改列表中——unmodifiableCryptoList.add将抛出一个未捕获的异常，println代码将无法被执行\n        unmodifiableCryptoList.add(\"XRP\");\n        System.out.println(\"New unmodifiable crypto List with new element:\" + unmodifiableCryptoList);\n\n        }  \n}\n</code></p><p></p><p>在运行代码时，你将看到对底层可修改列表添加的内容显示为对不可修改列表的修改。</p><p></p><p>但这与你创建了一个不可变列表并试图修改底层列表不同。有许多种方法可以基于现有的<a href=\"https://springframework.guru/using-immutablelist-in-java/\">可修改列表创建不可变列表</a>\"，下面我们使用List.copyOf()方法创建了一个不可变列表。</p><p></p><p><code lang=\"java\">import java.util.*;\npublic class UnmodifiableCryptoListExample {  \n    public static void main(String[] args) {  \n\n        List cryptoList = new ArrayList&lt;&gt;();  \n        Collections.addAll(cryptoList, \"BTC\", \"ETH\", \"USDT\", \"USDC\", \"BNB\");\n        List immutableCryptoList = List.copyOf(cryptoList);\n        System.out.println(\"Underlying crypto list:\" + cryptoList)\n        System.out.println(\"Immutable crypto ist: \" + immutableCryptoList);  \n\n        // 尝试添加更多的加密货币到可修改列表，但不可变列表并没有显示变化\n        cryptoList.add(\"BUSD\");\n        System.out.println(\"New underlying list:\" + cryptoList);\n        System.out.println(\"New immutable crypto List:\" + immutableCryptoList);\n\n        // 尝试添加并显示一个新的加密货币到不可修改的列表中\n        immutableCryptoList.add(\"XRP\");\n        System.out.println(\"New unmodifiable crypto List with new element:\" + immutableCryptoList);\n\n        }  \n}\n</code></p><p></p><p>修改底层的列表后，不可变列表不显示变更。尝试修改不可变列表会直接导致抛出UnsupportedOperationException。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/4figure-1-1661256736453.jpg\" /></p><p></p><h2>集合与Java Collections Framework的关系</h2><p></p><p>在引入JCF之前，开发人员可以使用几个特殊的类，即Array、Vector和Hashtable。但这些类有很大的局限性，除了缺乏公共接口之外，它们还难以扩展。</p><p></p><p>JCF提供了一个用于处理集合的通用架构。集合接口包含了几个不同的组件。</p><p></p><p>公共接口——主要集合类型的表示，包括集合、列表和Map；实现——集合接口的特定实现，从通用的到特殊的再到抽象的。此外，还有一些与旧的Array、Vector和Hashtable类相关的遗留实现；算法——用于操作集合的静态方法；基础结构——对各种集合接口的底层支持。与之前相比，JCF为开发人员提供了许多好处。值得注意的是，JCF降低了开发人员对自己编写数据结构的需求，从而提高了Java编程的效率。</p><p></p><p>但是，JCF也从根本上改变了开发人员使用API的方式。JCF通过提供一组新的公共接口来处理不同的API，简化了开发人员学习、设计和实现API的过程。此外，API的互操作性也大大提升了。<a href=\"https://www.infoq.com/news/2022/02/eclipse-collections-11-0-0/\">Eclipse Collections就是一个例子</a>\"，它是一个完全兼容不同Java集合类型的开源Java集合库。</p><p></p><p>由于JCF提供了更容易重用代码的结构，从而进一步提升了开发效率。其结果就是开发时间缩短了，程序质量也得到了提升。</p><p></p><p>JCF有一个定义良好的接口层次结构。java.util.Collection扩展了超接口Iterable，Collection有许<a href=\"https://www.scientecheasy.com/2020/09/collection-hierarchy-in-java.html/\">多子接口和子类</a>\"，如下所示。</p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/2figure-2-1661256736453.jpg\" /></p><p></p><p>如前所述，集合是唯一性对象的无序容器，而列表是可能包含重复项的有序集合。你可以在列表中的任何位置添加元素，但其他部分仍然保留了顺序。</p><p></p><p>队列也是集合，元素被添加到一端，并在另一端被删除。也就是说，它是一种先进先出（FIFO）接口。Deque（双端队列）允许从任意一端添加或删除元素。</p><p></p><h2>使用Java集合的方法</h2><p></p><p>JCF中的每一个接口，包括java.util.Collection，都提供了特定的方法用于访问和操作集合的各个元素。集合提供的常用的方法有：</p><p></p><p>size()——返回集合中元素的个数；add(Collection element) / remove(Collection object)——这些方法用于修改集合的内容。需要注意的是，当集合中有重复元素时，移除只会影响元素的单个实例；equals(Collection object)——比较对象与集合是否等价；clear()——删除集合中的所有元素。每个子接口也可以有其他方法。例如，尽管Set接口只包含来自Collection接口的方法，但List接口包含了许多用于访问特定列表元素的方法。get(int index)——返回指定索引位置的元素；set(int index, element)——设置指定索引位置的元素；remove(int,index)——移除指定索引位置的元素。</p><p></p><h2>Java集合的性能</h2><p></p><p>随着集合元素数量的增长，它们可能会出现明显的性能问题。事实证明，<a href=\"https://www.researchgate.net/publication/313820944_Empirical_Study_of_Usage_and_Performance_of_Java_Collections\">集合类型的选择</a>\"和集合的相关设计也会极大地影响集合的性能。</p><p></p><p>随着需要处理的数据量不断增加，Java引入了新的处理集合的方法来提升整体性能。在2014年发布的Java 8引入了Streams——旨在简化和提高批量处理对象的速度。自从推出以来，Streams已经有了许多<a href=\"https://www.baeldung.com/java-8-streams-introduction\">改进</a>\"。</p><p></p><p>需要注意的是，流本身并不是数据结构，而是“对流中的元素进行函数式操作（例如对集合进行map-reduce转换）的类。”</p><p></p><p>Streams使用方法管道来处理从数据源（如集合）接收到的数据。Streams的每一个方法要么是一个中间方法（返回可以进一步处理的流），要么是一个终端方法（在此之后不可能进行其他流处理）。管道中的中间方法是惰性的，也就是说，它们只在必要时才进行求值。</p><p></p><p>并行执行和<a href=\"https://www.geeksforgeeks.org/what-is-java-parallel-streams/\">串行执行</a>\"都存在于流中。默认情况下，流是串行的。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/1figure-3-1661263465544.jpg\" /></p><p></p><h2>通过并行处理来提升性能</h2><p></p><p>在Java中处理大型集合可能很麻烦。虽然Streams简化了大型集合的处理和编码工作，但并不总是能保证性能上的提升。事实上，程序员经常发现使用Streams反而会<a href=\"https://blogs.oracle.com/javamagazine/post/java-parallel-streams-performance-benchmark\">减慢处理速度</a>\"。</p><p></p><p>众所周知，网站用户只会等待几秒钟的加载时间，然后他们就会离开。因此，为了提供最好的用户体验并<a href=\"https://www.getweave.com/reputation-management/\">维护开发人员</a>\"提供高质量产品的声誉，开发人员必须考虑如何优化大型数据集合的处理。虽然并行处理并不总能保证提高速度，但至少是有希望的。</p><p></p><p>并行处理，即将处理任务分解为更小的块并同时执行它们，提供了一种在处理大型集合时减少处理开销的方法。但是，即使并行流处理简化了代码编写，也会<a href=\"https://www.infoq.com/presentations/parallel-java-se-8/\">导致性能下降</a>\"。本质上，多线程管理开销会抵消并行运行线程所带来的好处。</p><p></p><p>因为集合不是线程安全的，并行处理可能会导致线程干扰或内存不一致（当并行线程看不到其他线程所做的修改，对相同的数据有不同的视图时）。Collections Framework试图通过使用同步包装器在并行处理期间防止线程不一致。虽然包装器可以让集合变成线程安全的，从而实现更高效的并行处理，但它可能会产生不良的性能影响。具体来说，同步可能会导致线程争用，从而导致线程执行得更慢或停止执行。</p><p></p><p>Java有一个用于集合的元素并行处理函数Collection.parallelstream。默认的串行处理和并行处理之间的一个显著区别是，串行处理时总是相同的执行和输出顺序在并行处理时可能会有不同。</p><p></p><p>因此，在处理顺序不影响最终输出的场景中，并行处理会特别有效。但是，在一个线程的状态可能会影响另一个线程状态的场景中，并行处理可能会有问题。</p><p></p><p>我们来考虑一个简单的示例，在这个示例中，我们为包含1000个客户创建了一个应收账款列表。我们想要知道这些客户中有多少人的应收账款超过25000美元。我们可以按照串行或并行的处理方式检查这个列表。</p><p><code lang=\"java\">import java.util.Random;\nimport java.util.ArrayList;\nimport java.util.List;\n\nclass Customer {\n\n    static int customernumber;\n    static int receivables;\n\n    Customer(int customernumber, int receivables) {\n        this.customernumber = customernumber;\n        this.receivables = receivables;\n    }\n\n    public int getCustomernumber() {\n        return customernumber;\n    }\n\n    public void setCustomernumber(int customernumber) {\n        this.customernumber = customernumber;\n    }\n\n    public int getReceivables() {\n        return receivables;\n    }\n\n    public void setReceivables() {\n        this.receivables = receivables;\n    }\n}\n\npublic class ParallelStreamTest {\n\n    public static void main( String args[] ) {\n\n        Random receivable = new Random();\n\n        int upperbound = 1000000;\n   \n            List &lt; Customer &gt; custlist = new ArrayList &lt; Customer &gt; ();\n\n                for (int i = 0; i &lt; upperbound; i++) {\n    \n                    int custnumber = i + 1;\n                    int custreceivable = receivable.nextInt(upperbound);\n                    custlist.add(new Customer(custnumber, custreceivable));\n                 \n }\n\nlong t1 = System.currentTimeMillis();\n\n                System.out.println(\"Sequential Stream count: \" + custlist.stream().filter(c -&gt;\nc.getReceivables() &gt; 25000).count());\n\n                long t2 = System.currentTimeMillis();\n\n                System.out.println(\"Sequential Stream Time taken:\" + (t2 - t1));\n\n               t1 = System.currentTimeMillis();\n\n                System.out.println(\"Parallel Stream count: \" + custlist.parallelStream().filter(c -&gt;\nc.getReceivables() &gt; 25000).count());\n\n                 t2 = System.currentTimeMillis();\n\n                 System.out.println(\"Parallel Stream Time taken:\" + (t2 - t1));\n\n    }\n\n}\n</code></p><p></p><p>代码执行结果表明，在处理数据集合时，<a href=\"https://www.tutorialspoint.com/compile_java_online.php\">并行处理</a>\"可能会提升性能：</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/1figure-4-1661263465544.jpg\" /></p><p>但需要注意的是，每次执行代码时，你可能获得不同的结果。在某些情况下，串行处理仍然优于并行处理。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/java-collections-streams/en/resources/1figure-5-1661263465544.jpg\" /></p><p>在本例中，我们使用Java的原生进程来分割数据和分配线程。</p><p></p><p>不幸的是，对于上述两种情况，Java的原生并行处理并不总是比串行处理更快。实际上，经常会更慢。</p><p></p><p>例如，并行处理对于链表没有什么用。虽然ArrayList很容易被分割成并行处理，但LinkedList却不是这样的。TreeMap和HashSet介于两者之间。</p><p></p><p>Oracle的<a href=\"https://developer.ibm.com/articles/j-java-streams-5-brian-goetz/\">NQ模型</a>\"是决定是否使用并行处理的一种方法。在NQ模型中，N表示需要处理的数据元素数量，Q表示每个数据元素所需的计算量。在NQ模型中，计算N和Q的乘积，数值越大，说明并行处理提高性能的可能性越大。</p><p></p><p>在使用NQ模型时，N和Q之间存在反比关系，即每个元素所需的计算量越高，并行处理的数据集就越小。经验法则是，对于较低的计算需求，包含10000个元素的数据集是使用并行处理的基线。</p><p></p><p>除此之外，还有其他更高级的方法来优化Java集合中的并行处理。例如，高级开发人员可以调整集合中数据元素的分区，以最大化并行处理性能。还有一些<a href=\"https://www.infoq.com/articles/Refactoring-to-Eclipse-Collections/\">第三方的JCF插件</a>\"和替代品可以提升性能。但是，初学者和中级开发人员应该重点了解哪些操作可以从Java的原生并行处理特性中受益。</p><p></p><h2>结论</h2><p></p><p>在大数据世界里，想要创建高性能的网页和应用程序，必须找到改进大量数据处理的方法。Java提供了内置的集合处理特性帮助开发人员改进数据处理，包括Collections Framework和原生并行处理功能。开发人员需要熟悉如何使用这些特性，并了解可以时候可以使用原生特性，什么时候应该使用并行处理。</p><p></p><p>作者简介：</p><p>Nahla Davies是一名软件开发人员和技术作家。在全职从事技术写作之前，她曾在一家体验式品牌企业担任首席程序员，该组织的客户包括三星、时代华纳、Netflix和索尼。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/java-collections-streams/\">https://www.infoq.com/articles/java-collections-streams/</a>\"</p>",
    "publish_time": "2022-09-23 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "端到端语音识别应用基于前缀树的热词技术",
    "url": "https://www.infoq.cn/article/jw9fXWXWShx407VsfFLc",
    "summary": "<p>文 |&nbsp;王伟戌、王强强</p><p></p><h2>背景</h2><p></p><p>在深度学习火爆的今天，大规模数据下训练的大规模模型在线上任务中日益常见。随着大模型效果的提升， 随之带来了一些使用上的不便。通常情况下，大模型需要基于大量语料、文本训练，迭代周期长。且对于特定场景下词语在训练语料中出现次数不多，常常拟合不好。本文介绍的是关键词即特定场景语料，在序列到序列任务中通过构建状态转移自动机的方法改善最终效果的方案。</p><p></p><h2>生成模型即生成模型解码</h2><p></p><p>序列到序列模型常用于机器翻译、语音识别等任务。其架构提出于2014年[1]，包含两个核心组件：编码器、解码器。本文中略去这种模型的训练过程，对该模型在使用过程中解码这一过程进行介绍：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44501d391e0286d2e87fda6556c5d964.png\" /></p><p></p><p>通过这个图我们不难发现，每个时刻的生成结果不仅于输入时刻序列有关，还与输出序列相关，一个简单的想法，将每个时刻置信度最高的结果存下来，做为下一个时刻输入，但这样很容易产生问题：在一个时刻缺乏全局视野，即某一个时刻最优不代表是全局最优的结果。而将所有结果都记录下来，这将会是指数级增长的数据量。因此一种beam&nbsp;search的方法被提出用来解决这一问题，我们用一个形象的例子来讲述beam&nbsp;search这一过程。</p><p></p><p>假设我们得到了一串拼音序列：</p><p></p><p></p><p></p><p>我们如何知道这个拼音序列代表什么意思呢？</p><p></p><p>如下图所示，我们展现了一个通过简单概率模型产生的文本，在第零个时刻的5个候选（为了展示方便，这里省略了编号为④的候选），在第一个时刻各产生了三个延伸在这15个候选中，通过语言模型概率选取了top5保留，剩余的舍弃掉，以达到缩减搜索空间的目的。通过这样的方法，每一个时刻不保留全部结果只保留top&nbsp;N，最终将指数级增长的搜索空间变为平方级增长的搜索空间。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d244c370ab86fe0955bdb02c4cde187.png\" /></p><p></p><h2>传统解码应用问题与改进</h2><p></p><p></p><p>这种方法与全部状态保存的方案相比牺牲了准确度以换取时间，具有一定局限性。对于通常情况，每个时刻top N能覆盖当前时刻90%以上的情况，但是这种方法在面对关键词检测、风控词语检测等任务会产生两个问题：</p><p></p><p>1、待检测关键字在日常语料中较少出现，传统beam&nbsp;search非常容易漏召回。</p><p>2、时效性强，经常有实时插入的新检测词语，要即时生效。</p><p></p><p>对于第一个问题，一种直观的思想是通过标注数据，弱标注数据等重新训练模型，但这显然迭代周期长且迭代预期不稳定。</p><p></p><p>Google在2018年论文中[2]，提出一种设计方案调整beam&nbsp;search的结果，即不重新训练模型，仅在解码时通过追加模型进行重新打分来改善对小众语料的拟合。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23d46a02ee6e500cd493ffd20dd5f793.png\" /></p><p></p><p>以上图中识别系统设计为例，除去传统声学识别模块还增加了上下文模块，这个模块举例了几个功能，包括标点，语言模型打分，文本归一。其中语言模型的使用如下图，用beam&nbsp;search对中间结果进行追加打分，更新beam&nbsp;search中top&nbsp;N，并将当前结果作为下一时刻解码的输入。这种方式的优点是，在不更新语音识别模型的情况下，也可以通过添加不同语境的语言模型影响beam&nbsp;search过程中top&nbsp;N选取，从而达到改善结果的目的。缺点便是显著增加了计算量；尽管语言模型计算量常常不高，但beam&nbsp;search过程中每个候选结果都要多次经过语言模型，次数过多，耗时上升比较明显。</p><p></p><p>为了改进计算效率，经典wfst解码[3]方案重新在seq2seq模型中使用[4]，这种解码方案注意到了我们是在一个序列过程中重打分，不需在每一个时刻对从开始至当前所有文本进行重新计算，我们将语言模型生成新的概率转移自动机[5]，解码时，维护当前beam&nbsp;search过程中top&nbsp;N于图2 状态。如下图为cat(音标：kæt)这个单词的状态转移图，当声学识别模块产出c对应的k的音其转移到状态1，而当a对应的æ产生时，不在需要从状态0计算k&nbsp;æ一起的概率，而是计算当前状态1的后继状态中是否有æ。没有则计算回退到初始状态的概率。这样对于在序列生成中的计算，只需要记录其处于图中状态，在新的识别结果产生时对当前状态计算可行的转移状态即可。与普通语言模型相比，相当于省去了从状态0至当前状态的重复计算，复杂度大为降低。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2b/ea/2bf96775ab639319d9a545403586yyea.png\" /></p><p></p><p>这种浅融合的方案很好的解决了训练语料不均衡的问题，缺点是不能实时对图进行修改，且缺乏对特定词的加权，为此我们引入了前缀自动机来对这一过程改进。</p><p></p><h2>基于前缀自动机的解码加权方案</h2><p></p><p></p><p>前缀自动机，是一种经典算法，常应用于多模式串匹配。如果我们有一个字典，对于输入文本想检测是否命中字典中词语，这便是一个多模式串匹配任务。对于这类任务，一个显然的方案是遍历全部字典，但这样复杂度太高。</p><p></p><p>对此我们开始优化，一种方式是优化字典结构，即字典中字符有公共部分的比如teach和teacher都在字典中，那如果teach不匹配了，teacher这个单词也不用匹配了。将顺序的字典变为前缀树的存储方式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11cf4a955148c22fc694e082cf3f2abb.png\" /></p><p></p><p>与此同时，我们继续在这种情况下优化，如果一个词前缀为另一个词的子串，如图 6 中she和her，检测字符串sher中含有前缀树中多个词，当she在状态4匹配成功后我们知道he也是待匹配串中的，因此我们不需要跳回到状态1，而是从h开始匹配，而是直接跳转至状态9，从r开始匹配即可。便是前缀自动机的核心思想。</p><p></p><p>这种跳转关系构建方法是一种递归过程。用一句话来概括，一层一层的构建，如果我的父节点的跳转状态的子节点中有与我相同的，那就是我的跳转状态，否则我的跳转状态就是根结点。按照这个思路，我们将上图的点按遍历顺序重新标号以便于理解。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/171deb41f77222f7a750db263e9f251a.png\" /></p><p></p><p>有了这个前缀转移关系，我们便能高效的处理热词构建及查询，一个带有前缀自动机的解码流程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0ad25d1d659f08dcda62506265b0463.png\" /></p><p></p><p>即对于每个beam&nbsp;search过程我们不仅维护beam&nbsp;search过程中结果，同时维护其处于前缀自动机状态；此状态便于维护，仅存储状态指针即可。从性能上看，执行时的额外计算量及内存使用量都可以认为是常数增加。</p><p></p><h2>前缀自动机实时增加新词方案</h2><p></p><p></p><p>前一段中我们介绍了通过前缀自动机的解码方案，但这一过程依然不能很好解决“实时”这一要求，如果我希望实时向解码过程中添加热词，需要怎么改进？</p><p></p><p>回顾上一节的内容，我们了解到前缀自动机构建应分为两步，即前缀树的构建与状态转移的构建。其中，前缀树是算法正确性的保证，而状态转移可以大幅优化时间。同时，状态转移需要层次遍历整棵前缀树，这意味转移状态的构建不能随前缀树形态更改而自动更改，而必须全量重新构建。</p><p></p><p>当我们插入一个新词，由于前缀树的特性，可以在字符长度的复杂度将该词插入前缀树，但是构建新的转移状态需要遍历所有节点，如果每次插入新词都要重新访问整棵树全部节点，这种复杂度是难以接受的。比较起来，损失一些转移状态等价于将部分词的查询复杂度变大，对比遍历全部词典的复杂度这种损失是可以接受的。</p><p></p><p>根据上述想法，我们将整体查询变为两棵树，一棵为带转移状态的前缀自动机，另一棵为普通字典树，当新词插入时我们在普通字典树插入，当普通字典树规模大于一规定阈值后我们将他们合并，并在合并后的树上构建转移状态。对于一次查询，复杂度从词长变为小字典树规模，但能够节省构建转移状态遍历全文的时间。</p><p></p><h2>方案效果</h2><p></p><p></p><p>我在语音识别系统中应用了这种解码方案，并通过两方面指标评估该模块效果，一方面通过标注带关键词语音数据集，评估关键词准召。在下表中，beam search代表普通方案，ac automation代表前缀自动机加权解码方案，发现在识别结果中对关键词召回相对提升4.6%。另外，我们对比在普通语音识别数据集上字错误率（CER），由于对特殊词提升了权重使整体准确率有一定的下降，但整体损失可以接受，低于对关键词召回的收益。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/69e148f4a4183977429c670935c567c7.png\" /></p><p></p><h2>总结</h2><p></p><p></p><p>本文主要基于seq2seq类模型，通过追加状态转移自动机来减少模型在面对专有领域语料时的识别准确率；同时可以使语料实时生效，并将该工作的实时性流程构建方法加以介绍。对于语音转录文本中的关键词检测，常常局限于语音模型不能实时调整，很难识别出新词，通过这种方案可以做到秒级别新词添加，显著改变这一困扰。</p><p></p><p>参考文献：</p><p></p><p>[1] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" Advances in neural information processing systems 27 (2014).</p><p></p><p>[2] Zhao, Ding, et al. \"Shallow-Fusion End-to-End Contextual Biasing.\" Interspeech. 2019.</p><p></p><p>[3] Hori T, Hori C, Minami Y, et al. Efficient WFST-based one-pass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition[J]. IEEE Transactions on audio, speech, and language processing, 2007, 15(4): 1352-1365.</p><p></p><p>[4] Williams I, Kannan A, Aleksic P S, et al. Contextual Speech Recognition in End-to-end Neural Network Systems Using Beam Search[C]//Interspeech. 2018: 2227-2231.</p><p></p><p>[5] Hori T, Nakamura A. Speech recognition algorithms using weighted finite-state transducers[J]. Synthesis Lectures on Speech and Audio Processing, 2013, 9(1): 1-162.</p><p></p><p>[6] <a href=\"https://blog.csdn.net/weixin_53360179/article/details/119718426\">https://blog.csdn.net/weixin_53360179/article/details/119718426</a>\"</p>",
    "publish_time": "2022-09-23 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "更好的中英文混合语音识别系统",
    "url": "https://www.infoq.cn/article/FI5XMrlruQx4HVANqyOh",
    "summary": "<p>（文/商迎新 王伟戌 王强强）&nbsp;&nbsp;&nbsp;&nbsp;</p><p></p><h2>研究背景&nbsp;</h2><p></p><p></p><p>语音作为人与人交流的直接媒介，承载着人们日常生活中的大部分信息来源。基于近年来通信技术与物联网的发展，各式各样的语音助手、智能家具等软硬件层出不穷，人机交互技术的发展及人们对其需求日益攀升。语音识别技术在人机交互上扮演着重要的角色，任何因其导致的识别错误都可能在人机交互系统中的各个模块上传播，并最终导致交互失败。因此针对语音识别的研究具有重要的学术价值和应用价值。</p><p></p><p>混合语言现象常常出现在能够流利使用多种语言的群体中。英文作为全球的通用语言，就时常以各种形式与其他语言混合在一起。然而现有的大多数最先进的语音识别系统都专注于单语种语音识别，即它们一次只能处理一种语言，这样的系统无法识别中英混合语言的语音。随着语音技术开始渗透到人类生活的方方面面，混合语言的现象受到越来越多的关注。因此，开发用于中英文混合语言的自动语音识别(CSSR)系统尤为重要。</p><p></p><p></p><h1>行业现状&nbsp;</h1><p></p><p></p><p>中英文混合语音识别算法属于多语言语音识别领域。但与常规多语言语音识别不同，常规多语言语音识别仅针对一句话中出现一种语言，而混合语言语音识别则是指同一句话中说话人会在两种语言间切换使用。尽管语言学家对混合语言现象已经研究了长达半个多世纪，随着近年来语音技术的不断突破，对混合语言语音识别的研究近二十年才被人们重视。针对中英文混合语音识别也是近十多年来才开始研究。</p><p></p><p>其技术难点主要表现为：嵌入语受主体语影响形成的非母语口音现象严重、不同语言音素构成之间的差异给混合声学建模带来巨大困难、带标注的混合语音训练数据极其稀缺。传统语音框架基于单一语种基础建模单元，如汉语是基于拼音的声母韵母、英语则是英文的音素，这种技术架构对指定语种的语言学知识依赖较大，难以扩展到多语种识别。</p><p></p><p>由于不同语言之间的声学单元相互独立，且声学属性不同，常规基于声学单元建模的&nbsp;DNN-HMM 语音识别模型无法很好的建模不同语言之间声学属性的联系。而端到端模型无需对于声学单元建模，转而采用字符建模，模糊了建模单元与声学属性之间的关联。并且由于端到端模型能够考虑帧的上下文信息，可以有效建模语言转换点的声学属性。因此最近几年的研究[1][2]偏向于采用端到端方式搭建混合语言语音识别系统。</p><p></p><p>基于深度学习的端到端模型灵活且复杂，相较于传统语音识别，融合多任务学习也能够提升模型性能。考虑到混合语言语音识别系统的特有属性，有学者提出可以鉴于 LID 模型[3][4]能够判别语言之间的差异性，以进行分类。中英文混杂识别联合语种识别受到越来越多的关注，在识别文本内容的同时进行语言分类，以增强对不同语言的分辨能力。本文从[5][6]获取灵感，在端到端网络模型基础之上添加语种信息进行联合训练，期望增强模型对不同语言的识别以及判别能力。</p><p></p><p></p><h2>作业帮实践&nbsp;</h2><p></p><p></p><h2>3.1&nbsp;实验细节介绍</h2><p></p><p></p><p>为了便于后续讨论，首先介绍我们实验所采用的数据集、模型训练建模单元以及最终评价指标。</p><p></p><p>（1）我们的训练语料为作业帮标注的约1000小时的老师上课的英文授课混合数据集，并在其中随机取30小时作为训练开发集，6.7小时作为测试集，其余数据作为训练集。</p><p></p><p>（2）中文训练最小单元以汉字建模，但英文字母只有26个，因此以子词[7]替代字母作为英文的建模单元。从声学层面上，中文字符对应的音频长度远大于英文字母对应的长度，因此采用子词能够增大英文建模单元所对应的声学时长，从而减小中英文建模之间的差异，增强模型训练鲁棒性。</p><p></p><p>（2）在语音识别系统的评估中，若单元为字符，则该错误率称为字符错误率 (Character Error Rate, CER)。若单元为单词，则称为词错误率 &nbsp;(Word Error Rate, WER)。中英文混合语音识别系统由于包含两种语言，则中文部分计算 CER，英文部分计算WER，称为混合错误率（Mix Error Rate, MER）。</p><p></p><p></p><h2>3.2&nbsp;基线识别模型</h2><p></p><p></p><p>混合语言语音识别本质上还是语音识别任务，为了方便说明结果，我们的基线识别系统采用语音识别框架Wenet[8]，Wenet网络结构设计借鉴Espnet的joint loss框架，这一框架采取Conformer Encoder + CTC/attention loss，利用帧级别的CTC loss和label级别attention-based auto-regression loss联合训练整个网络。这一框架是目前语音领域比较流行的框架之一。我们的实验也在其网络模型基础之上进行改进并对比实验效果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/98825b29f174f384e28ccebe89066325.png\" /></p><p></p><p></p><h2>3.3&nbsp;语种信息联合训练</h2><p></p><p></p><p>将语种信息加入中英文混合语言语音识别的网络模型训练，设X为声学模型的特征序列，Y为模型的预测输出的中文字符或者英文字词，Z&nbsp;为语种预测的输出类别 ，则最终的模型损失函数由之前的公式(1)增加为公式(2)。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8c/e0/8c0a87b713eb6f8d6bcyy86a2bee29e0.png\" /></p><p></p><p></p><p></p><h3>3.3.1&nbsp;帧级别语种loss联合训练</h3><p></p><p></p><p>在混合语言语音识别中，不同语言的具有相似发音的单元很有可能被识别错误，因此我们考虑对识别系统加入语言识别。整体框架如下图所示，Wenet编码器由基于Attention的解码器、CTC 和 LID 模块共享。它将输入序列 X 转换为高维特征 H。基于Attention的解码器和 CTC 生成输出序列 Y，而 LID模块则输出每一帧的语种ID。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/27/279d64ea5cbd7f64603284affc6f7e70.png\" /></p><p></p><p></p><p>我们预先将数据集进行帧级别语种对齐，在实验中，我们利用的是之前训练过的中英文混合数据得到的chain模型，得到中英文混合数据的帧级别LID-label，在训练过程中，将语种信息的预测和label的交叉墒作为语种的损失函数进行联合训练，最终目标函数为公式（2）所示。</p><p></p><p></p><h3>3.3.2&nbsp;token级别语种loss联合训练</h3><p></p><p></p><p>考虑到帧级别模型语种信息获取需要提前对中英文混合数据进行对齐，如果对齐不准，最终错误会累积。虽然模型在解码时，可以推断出语种信息，但是在模型训练中未加入此信息，因此在这直接进行token级别的语种信息联合训练，网络框架如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a01b2d31e947cd0ade6fb18c2d7c2588.png\" /></p><p></p><p>在这里，我们探讨了两种 LID 的loss添加方式，一种是与语音识别任务共享相同的注意力模型（LID-shared），另一种是单独学习一个独立的注意力模型（LID-indep）。两种方法都使用与等式 (2) 中相同的目标函数。</p><p></p><h3>3.3.3语种信息联合训练实验总结</h3><p></p><p></p><p>1、3.3.2介绍过，token级别的语种信息添加有两种方式，即是否与识别系统共用Attention，为了更有效率的对比两种方式的优劣性，我们先在小数据集上进行对比，在1000小时训练集中随机抽取70小时作为训练集，进行实验对比，效果如下表</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/23261f25f6236a226df00d8c7dcb12c2.jpeg\" /></p><p></p><p>由以上表格可以看出，LID-indep效果较好。因此后续添加token级别的语种信息时，都采用LID-indep方式&nbsp;，并将系数设置为0.2。&nbsp;&nbsp;</p><p></p><p>2、利用全部数据进行训练,为了方便进行记录，将帧级别添加语种信息训练模型称为LID-frame。为了验证加入语种信息训练正确，我们还统计了LID的识别准确度T-LID-ACC。以免中英文占比分布不均匀，这里统计的是F1 score 。实验结果如下表：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6074baea7ef3007790503bdf253b380a.jpeg\" /></p><p></p><p>从以上表格来看，LID-indep在测试集上效果要优于基线模型，相对提升约1.76%，并且语种判别准确度也达到了98%，证明了添加token级别语种信息联合训练的有效性；</p><p></p><p>同时LID-frame效果要比基线系统差，分析原因可能有以下几个方面：（1）帧级别的对齐label构建存在误差，传统混合语言语音chain模型的识别效果较差，会造成错误累积；（2）在训练过程中，Wenet会对数据进行语速增强，即随机对音频进行(0.9,1.0,1.1)倍变速，由于帧级别的label需要提前对齐，因此训练过程中只能关闭掉数据增强部分，模型鲁棒性变差，因此效果要低于基线模型。</p><p></p><h2>3.4&nbsp;混合语言错误类型分析</h2><p></p><p>在验证添加语种信息训练的有效性基础之上，我们进行识别case分析，统计错误率分类占比，确定下一步优化策略。下表格是中英文错误单独统计的MER。</p><p><img src=\"https://static001.geekbang.org/infoq/1c/1ce354192a09b027128bc0a32057929c.jpeg\" /></p><p>由上表格可以看出，混合语言语音识别中，无论是中文还是英文，均是替换错误占比较高，因此我们统计下替换错误的类型如下表（其中E代表英文，C代表中文，SE →E则代表英文单词替换为英文单词）。</p><p></p><p></p><p>经过细致的错误分类，我们可以看出，不同语种之间的相互替换占比较低，替换错误主要发生在同类语种之间，尤其是英文单词替换为英文单词的占比较高为7.8%，接下来我们通过添加语言模型辅助，期望降低英文同类语言的替换错误。</p><p></p><h2>3.5&nbsp;语言模型增强</h2><p></p><p>WeNet中的LM支持方案，其中语言模型需要自己构建，依靠ctc wfst search生成n-best，wfst search为依靠传统解码图的传统解码方式。需要注意的是由于我们的建模单元是汉字加英文子词，因此构建的词典L是汉字映射为词语，英文子词映射为英文单词，英文词典的构建需要和训练时分词方式保持一致。T为端到端训练时的建模单元 ，G为语言模型即将n-gram的语言模型转为WFST形式的表示，最终构建TLG进行解码。以下是在LID-indep模型基础之上添加语言模型的效果。</p><p></p><p>由上图表明，我们添加TLG实验的有效性，在测试集上错误率相对下降约2.71%，同时计算下测试集的替换错误如下表</p><p></p><p>添加TLG同时也降低了中英文同类替换错误，由之前的3.729下降到3.388，0.268下降到0.223，分别相对下降约9.14%&nbsp;和&nbsp;2%，降低了混合语言测试集的替换错误。进一步验证了添加语言模型的有效性。&nbsp;&nbsp;</p><p>&nbsp;</p><p></p><h2>3.6最终实验对比</h2><p></p><p>在模型训练期间，词典是由训练文本产生，因此训练文本和开发集中几乎不含有,在实际业务场景中是不合理的，因此在后续我们添加了约50小时的带有 的数据，来增强模型鲁棒性，实验步骤和前面介绍的一致。最终实验结果如下表所示。</p><p></p><p>另外，考虑到在实际的业务场景中，我们几乎不关注感叹词的识别情况，也不在意&lt;他她它&gt;的对错与否，因此将参考文本和识别结果中的感叹词过滤，统一&lt;他她它&gt;，重新将基线模型效果和最终实验效果进行测试，结果如下表所示</p><p></p><p>最终将感叹词去掉后，和基线模型相比较，混合错误率相对降低约6.96%，中文错误率相对降低约6.41%，英文错误率相对降低约8.24%。&nbsp;</p><p>&nbsp;</p><p></p><h1>总结与展望&nbsp;</h1><p></p><p>在优化中英文混合语言识中，我们通过三个方面来提升中英文混合的识别效果。</p><p></p><p>第一是模型训练层面，在Wenet的基础之上，我们对比了不同语种信息加入方式的优劣性，并从中选出最适合匹配基线模型的方式，测试集效果提升相对约1.76%；</p><p></p><p>第二是数据方面，为了更贴合实际业务场景，很多未在模型训练词典中的词可以识别出来，因此我们加入了部分数据，进一步提升识别系统的可用性，相对提升约3.1%；</p><p></p><p>第三是考虑到中英文语言文本的连贯性，进一步通过语言模型来增强混合语言语音识别模型，构建TLG，进一步相对提升约2.5%&nbsp;。最终相对基线模型提升约7.8%。</p><p></p><p>最终在实际应用方面，我们去除了感叹词和将&lt;他她它&gt;进行统一后，对比测试基线模型效果，从整体对比，混合错误率相对降低约6.96%，中文错误率相对降低约6.41%，英文错误率相对降低约8.24%。</p><p></p><p>同时，实验也还有很多不足之处，后续会考虑从不同训练方式层面来提升中英文混合语言语音识别模型的识别效果。例如训练模型参数调优、预训练[9]以及无监督[10]的方式生成大量混合数据文本等。</p><p></p><p>【参考文献】</p><p>[1] Shan, Changhao, &nbsp;et &nbsp;al. &nbsp;\"Investigating &nbsp;end-to-end &nbsp;speech &nbsp;recognition &nbsp;for &nbsp;mandarin-english code-switching.\" in Proc. of the 2019 IEEE International Conference on Acoustics, Speech and &nbsp;Signal Processing (ICASSP). IEEE, 2019</p><p>[2] Emre Yılmaz, Samuel Cohen, Xianghu Yue, David van Leeuwen, and Haizhou Li, “Multi-graph decoding for code-switching ASR,” in Twentieth Annual Conference of the International Speech Communication Association &nbsp;(INTERSPEECH). ISCA, 2019, pp. 3750–3754.</p><p>[3] Chan, Joyce YC, et al. \"Detection of language boundary in code-switching utterances by bi-phone probabilities.\" in Proc. of the 2004 International Symposium on Chinese Spoken Language Processing. IEEE, 2004.</p><p>[4] Weiner, Jochen, et al. \"Integration of language identification into a recognition system for spoken &nbsp;conversations &nbsp;containing &nbsp;code-switches.\" Spoken &nbsp;Language &nbsp;Technologies &nbsp;for &nbsp;Under-Resourced Languages. 2012. &nbsp;</p><p>[5] Zeng Z, Khassanov Y, Pham V T, et al. On the end-to-end solution to mandarin-english code-switching speech recognition[J]. arXiv preprint arXiv:1811.00241, 2018.</p><p>[6] Luo N, Jiang D, Zhao S, et al. Towards end-to-end code-switching speech recognition[J]. arXiv preprint arXiv:1810.13091, 2018.</p><p>[7] A. Zeyer, K. Irie, R. Schlu ̈ter, and H. Ney, “Improved training of end-to-end attention models for speech recognition,” arXiv preprint arXiv:1805.03294, 2018.</p><p>[8] Yao Z, Wu D, Wang X, et al. Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit[J]. arXiv preprint arXiv:2102.01547, 2021.</p><p>[9] Xinyuan Zhou, Emre Yilmaz, Yanhua Long, Yijie Li and Haizhou Li. &nbsp;\"Multi-</p><p>Encoder-Decoder Transformer for Codeswitching Speech Recognition.\" in Proc. of the 21st Annual &nbsp;Conference &nbsp;of &nbsp;the &nbsp;International &nbsp;Speech &nbsp;Communication &nbsp;Association &nbsp;(INTERSPEECH). ISCA, 2020.</p><p>[10] Guo, Pengcheng, et al. \"Study of semi-supervised approaches to improving english-mandarin code-switching speech recognition.\" arXiv preprint arXiv:1806.06200 (2018).</p>",
    "publish_time": "2022-09-23 09:02:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "语音评测技术在古诗背诵场景中的应用",
    "url": "https://www.infoq.cn/article/3ZFao6WKxQO8qTTjBqIF",
    "summary": "<p>(文 / 作业帮语音技术团队&nbsp;王洲 王强强)</p><p></p><p>口语环节正在语言类教育课程中获得更多重视，一对一的师生交流和指导是提高口语水平最有效的方式，但该方式很难满足数量众多的口语学习者需求。得益于计算机技术和语音评测技术的突飞猛进，计算机辅助语言学习（Computer Assisted Language Learning）技术应运而生，各种基于人工智能技术的口语评测方案相继落地。包括朗读，背诵，复述，自由表达等多种方式，它为学生提供了额外的学习机会和充足的学习材料，能够辅助或者替代教师指导学生进行更有针对性的发音练习，指出学生的发音错误，提供有效的诊断反馈信息，并评估学生的整体发音水平，从而切实提高学生的口语学习效率和口语水平。</p><p></p><h2>语音评测技术简介</h2><p></p><p></p><h3>2.1 评价指标</h3><p></p><p></p><p>发音评测维度包括发音的准确率，流畅度，完整度，韵律，语调等。</p><p></p><p>准确度：体现用户的发音水平。</p><p>流利度：体现用户朗读流畅程度，和语速、停顿次数相关。</p><p>完整度：体现用户发音正确的单词占比。</p><p>单词得分：句子中每个单词的得分。</p><p>句子得分：段落中每个句子的得分。</p><p>总分：评测整体得分，综合上面分数获得，准确度对总分影响最大。</p><p></p><p>口语好坏的评价是主观的，人工专家根据自身的专业知识及经验，在各个维度按照优、良、中、差、劣等级进行打分，不同的专家会得出不同的结果。评价机器口语评测系统是否靠谱，一般采用相关系数（Pearson correlation coefficient），一致性（kappa coefficient）等指标来衡量打分系统的性能。与专家打分越接近，系统越靠谱。但有多靠谱，需要有参照物，通常会用平均的人与人之间的相关性来作为参照，目前的人机相关性、一致性已经超过了人与人之间的平均相关性，一致性。语音评测技术已被普遍使用在中英文的口语评测和定级中。</p><p></p><h3>2.2 技术框架</h3><p></p><p></p><p>语音评测目前的主流方案是基于隐马尔科夫-深度神经网络（Hidden Markov Model，HMM，Deep Neural Networks, DNN）模型获得语音后验概率，与评测文本强制对齐（force alignment）后，使用GOP（Goodness of Pronunciation）方法进行打分，流程如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/86e0f58962fdbf2861699cf809fd409d.png\" /></p><p>&nbsp;</p><p>1）声学特征提取</p><p></p><p>语音信号在一帧（frame）观察窗口内平稳，将切分后的时域片段信号转换到频域，获得MFCC（13维或40维），Fbank（80维）等特征，一帧是语音序列的最小单元。在GMM（Gaussian Mixture Model）模型中，使用MFCC特征，是因为GMM的协方差估计使用了对角矩阵，输入特征需要保证每一帧的元素之间相互独立。DNN模型中，特征可以是MFCC或Fbank特征；</p><p></p><p>2）HMM-GMM无监督聚类</p><p></p><p>该过程用来获取帧标签（Frame-wise label）进行后面的DNN训练。HMM对语音的时序约束、依赖进行建模，GMM属于生成模型，对HMM的观察概率（属于各个音素的概率）进行建模。语音的标注通常是单词序列，每一帧对应哪个音素提前是不知道的。GMM通过相似度聚类，同一个音素的相邻各帧归为同一类，获得音素的duration、边界信息。目前语音信号中对齐算法主流依旧是HMM-GMM，其它算法ctc，attention对齐都有各自的问题，ctc的尖峰，attention的时序无约束问题。</p><p></p><p>3）DNN判别学习</p><p></p><p>GMM聚类效果尚可，但GMM对音素建模能力有限，无法准确的表征语音内部复杂的结构。通过DNN取代了GMM 来进行HMM观察概率的输出，可以大大提高准确率。DNN模型的最后一层使用softmax进行软分类，交叉熵loss进行训练，DNN的输出称为后验概率 (phonetic posteriorgrams，PPG)。</p><p></p><p>用一个例子来说明DNN的输出，假设有5帧语音特征, 发音内容为“er_3 duo_0”，取[sil,er_3,d,uo_0,uo_3]PPG如下：</p><p></p><p>在t=0第一帧，er_3的概率0.921272最大。</p><p>&nbsp;</p><p>4）评测文本构建hmm解码图，约束文本时序关系：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85d97be15c1d5dcde0ce7d60efd17924.png\" /></p><p>&nbsp;</p><p>对于上面的评测文本“er_3 duo_0”, 我们要评价每个音素的发音正确性，也要保证时序性，如果用户读的是“d uo_0 er_3”每个音都对，但时序是错误的，为了约束这种时序关系，我们定义了如上的HMM结构。状态0到1，1到2，2到3， er_3，d，uo_0 每个音至少发生一次，状态1，2，3上面每个都有自旋，表示该音素可以重复发生。</p><p></p><p>5）Viterbi解码获得强制对齐结果</p><p></p><p>首先我们将PPG，表示成状态转移图，如下图所示，有六个状态，状态之间的弧表示各个音素及音素的后验概率，0粗线圆圈代表开始状态，5双圆圈代表结束状态。如果音素个数为C，帧数为T，则弧路径个数为CT。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b71eb9b7915b1dd0693830a47f64d87e.png\" /></p><p>&nbsp;</p><p>DNN的后验概率转移图，通过评测文本HMM解码图的约束，在CT条路径中，只剩下构成“er_3d uo_0”的路径及相应的score。使用Viterbi算法我们可以高效获得score最大路径。上图中Viterbi最优路径[er_3, er_3,d, uo_0, uo_0] ，也就是强制对齐结果。</p><p></p><p>6）GOP准确度打分,Averaged Frame-level Posteriors[1]：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/621189b1215fc355ada5caee33fa986b.png\" /></p><p></p><p>P(St|Ot)是DNN模型softmax的输出,Ot是t时刻语音帧，St是force alignment后t时刻的对齐音素。ts,te是音素的开始和结尾。</p><p></p><p>GOP(er_3)=(0.92+0.95)/2</p><p>GOP(d) =0.99</p><p>GOP(duo_0)=(0.65+0.80)/2</p><p></p><p>&nbsp;</p><p>7）整体打分</p><p></p><p>获得音素的准确度打分，离最终的单词，句子，整体打分还有距离。评测任务最后都会接一个打分模型，使用音素的GOP分数，流利度等特征拟合到专家分数。2021年腾讯提出一种将声学模型中的深层特征[2]，而不是GOP特征传递到打分模块中，该模块基于注意力（attention）机制，考虑句子中不同粒度之间的关系，获得了更优的整体评分。</p><p></p><p>作业帮目前基于特征提取工程，收集了包括GOP分数，元辅音，词性，声调，发音时长，流利度等多维度信息，通过神经网络模型强拟合能力，预测单词，句子的分数结果，整体打分效果跟人工打分取得了较高的一致性。</p><p></p><h2>DNN声学模型的改进</h2><p></p><p></p><h3>3.1 conformer模型</h3><p></p><p></p><p>Conformer是Google在2020年提出的语音识别模型，基于Transformer改进而来，主要的改进点在于Transformer在提取长序列依赖的时候更有效，而卷积则擅长提取局部特征，因此将卷积应用于Transformer的Encoder层。Conformer的Encoder模型由多个encoder_layer叠加而成,每个encoder_layer由feed-forward ，multi-head self-attention，convolution，feed-forward组成。</p><p></p><p>在流式评测任务中，随着用户的朗读，需要实时快速的返回评测结果，直接使用conformer模型进行流式评测，会遇到以下问题：</p><p></p><p>1）由于attention机制，每一帧数据需要看到全部数据做weight计算，随着语音时长的增加，计算复杂度和内存储存开销会越来越大；</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/60/6091f4fc84de9e8aef67e19459f984a5.png\" /></p><p></p><p>2）为了提高帧的识别准确率，每帧会向前看几帧数据，随着encoder_layer的增加，向前看数据量就会累加，这会造成很大的延迟；</p><p></p><h3>3.2基于块（chunk）的mask流式解决方案[3]</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/6107e1c8da3743f7c1c2d4b74f59f16e.png\" /></p><p></p><p>如上图所示，相邻三帧为一个chunk，通过mask掉一些数据，attention作用在当前chunk和前一个chunk中，当前chunk中的每一帧对右边的视野依赖最多两帧，平均时延为一帧时长，在每个encoder_layer层，对历史数据的依赖最多五帧。通过mask机制牺牲掉一些attention带来的收益，换取流式低延迟作为妥协。</p><p>conformer中的卷积模块也使用了因果卷积，避免右边视野的累积。通过以上的改进，conformer胜任了流式评测任务，时延在300ms左右。</p><p></p><p></p><h2>古文背诵应用中的特殊处理</h2><p></p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24264c8b56fa69eabdcbc0f62712fbef.png\" /></p><p></p><p>如上图所示，学生在手机应用中选择古文进行背诵，屏幕上就会实时显示背诵情况，正确染成黑色，有发音错误，染成红色。背诵后给出整体的评分报告，以此帮助学生检查和进行背诵练习。</p><p></p><p>多分支评测:古文背诵中，评测文本一般为一首诗，或古文的一段，包含了多句话。评测文本的解码图需要以句子为单位，支持句子的重复读（家长领读，小孩跟读），跳读（可以往前跳，也可以往后跳读）。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a6/a6f84fdeceb3a58e7dd6178e69b805ef.png\" /></p><p></p><p>为了关注句子级别的构图关系，上图做了简化，将音素改为字，去掉字的跳读，重复读等弧。0为开始状态，1是结束状态，到达结束状态可以继续回到开始状态，开始下一句的背诵，每句背诵前面会有一个句子标识“$0”,“$1”等，通过该标识，我们可以知道当前用户读到了哪一句，方便染色。</p><p></p><p>上面的构图方式在一些诗句背诵中遇到问题，当诗句中存在重复诗句，或开头相似的句子。当回到开始状态0时，由于每条路径都是等概率的。会出现如下的现象：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b1a898c01cc173cae0362da1b5ed07d3.png\" /></p><p></p><p>在t+1 时刻读完了第二句，应该开始第三句“$3鱼”的染色，但由于等概率，及顺序问题，仍然会走$2路径，当t+5时刻读到“东”，路径会突然跳转到“$3鱼戏莲叶东”。用户的体验就是，读完第二句一直不染色，读到“东”，立马第三句整句染色，影响体验。</p><p></p><p>解决方案：如下图所示，每一句读完添加到下一句的弧，例如状态7到8添加空边，第一句读完可以走第二句路径，或者回到起始状态，但添加惩罚（比如0.5）。这样就可以保证古文多句的顺序读，但又保留了足够的自由度，允许重复读，跳读。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/436193f251359ddf2b87c2dceca8d95e.png\" /></p><p></p><p></p><h2>评测引擎本地化</h2><p></p><p></p><p>评测服务在部署阶段遇到了如下问题：</p><p></p><p>1）课堂场景应用评测技术会带来超高并发，直播课场景，老师下发题目后学生几乎是同一时刻做答，并发量瞬间达到几万，要保证用户在短时间内尽快拿到打分结果，不能通过消息队列异步处理，只能提前为每个连接准备资源，需要大量的服务器资源支撑。</p><p></p><p>2）网络传输有一定的时延，对于需要做实时染色的评测服务，会感觉到染色时延，体验较差。</p><p></p><p>3）语音评测需要建立长链接，用户的网络有波动，但是语音评测服务要实时返回结果，无法缓存，会造成一定程度的评测失败。</p><p></p><p>为了解决这些问题，我们考虑开发本地评测方案，相比云端，本地评测具备以下优势：低延迟：节省了网络请求的延迟，优化了用户体验。安全性：更好的保护用户隐私数据。更稳定：消除了网络波动对服务的影响。节约云端资源：根据不同的本地算力，与云端推理结合，从而降低对云端算力的压力。</p><p></p><p>作业帮语音评测技术经过数轮迭代，在保证效果的前提下，conformer模型通过裁剪以及量化等处理，压缩到 10M 以内，极大降低了端的算力要求。</p><p></p><p>作业帮用户体量比较大，部分用户使用的手机配置较低，难以进行端上计算，为了给所有用户带来好的体验，我们采用了端云一体解决方案，如下图所示。为了结合“端”和“云”的优势，在评测时，服务判断端上算力，对于较高算力的设备，评测是在“端”上完成；对于较低算力的设备，评测是在“云”上完成。在“端”的解决方案中，能提供PC、手机、平板电脑、学习笔等多类型的硬件设备；跨平台支持iOS，安卓、Windows、鸿蒙、Linux等操作系统。</p><p></p><p>使用端云一体方案后，延时从原来的200ms 降低到 50ms 以内，用户体验明显提升。同时释放了大量服务端资源，服务端资源占用降低为原有的 20%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3eb7b4a3434e09cdf4709165d8c1b8e2.png\" /></p><p></p><p></p><h2>技术总结展望</h2><p></p><p></p><p>语音评测技术和语音识别任务非常类似，近些年都获得了快速发展，语音识别中各种端到端算法不仅简化了训练流程，同时降低了整体错误率。评测技术也从原来的HMM-GMM升级到HMM-DNN,准确率大幅提升。但在语音评测中GMM依旧是非常核心，必不可少的算法模块，通过GMM良好的聚类能力，音素的边界信息被提取出来。但能否通过神经网络直接获得对齐信息，大家有在各种尝试，interspeech2021会议论中 Teytaut通过ctc与Auto-Encoder的结合获得了优于GMM的对齐效果[4]，ICASSP2022会议论文中字节跳动提出了基于双向注意力神经网络强制对齐（ForcedAlignment）模型 NeuFA[5]。生成模型Auto-Encoder，Variational Auto-Encoder，以及attention机制都有不错的对齐潜力，语音评测技术如果能借鉴这些方法实现完全的端到端训练，准确度有机会进一步提高。&nbsp;</p><p>&nbsp;</p><p>References：：</p><p>[1]A NewDNN-based High Quality Pronunciation Evaluation for</p><p>Computer-AidedLanguage Learning (CALL)</p><p>[2]Deepfeature transfer learning for automatic pronunciation assessment</p><p>[3]DevelopingReal-time Streaming Transformer Transducer for Speech Recognition onLarge-scale Dataset</p><p>[4]Phoneme-to-audio alignment withrecurrent neural networks for speaking and singing voice</p><p>[5]NeuFA: Neural Network Based End-to-EndForced Alignment with Bidirectional Attention Mechanism</p>",
    "publish_time": "2022-09-23 09:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《架构师成长计划》异构计算数据中心“芯”变革",
    "url": "https://www.infoq.cn/article/tV9I4XUiJcSOEumAP5y1",
    "summary": "<p>国际学术期刊Science/AAAS和英特尔在全球首次联袂推出第一季《架构师成长计划》以来，吸引了无数架构师踊跃参与，获得业内广泛赞誉。为持续助力架构师把握数智机遇，构建未来，第二季《架构师成长计划》全新升级，强势归来！业内顶尖架构师大咖齐聚，为架构师群体量身打造系统成长课程，带来涵盖云游戏、云原生、联邦学习、生信大数据、网络智能化、算力网络、云网融合等多个热门话题的前沿技术及案例实践。</p>\n<p>以数据中心、人工智能等为代表的高性能计算类应用的发展，驱动算力需求不断攀升。然而，当前单一的计算架构已经无法处理复杂多样的数据类型。异构计算既能提高算力和性能，降低成本及功耗，又具备多类型任务的处理能力，凭借其强大优势，成为公认的突破算力瓶颈的关键技术方向。但实际运用中，异构计算在技术、互连和软件等方面仍面临着技术难题，如何寻求解决之道，释放异构计算优势，助力业务变革？</p>\n<p>英特尔《架构师成长计划》第六期精彩上线！邀请快手异构计算中心AI&amp;HPC负责人钟辉、英特尔云与行业解决方案事业部互联网行业技术总监高明、极客帮科技创始人&amp;CEO霍太稳三位业内专家，共同探讨异构计算的优势和未来前景，倾力分享异构计算的前沿应用场景、架构设计及硬软件方案，并通过客户实际应用案例详细阐述其如何助力企业化解算力瓶颈、实现更佳的并发、吞吐、延时与经济性，为您提供极具价值的现实参考！期待您的参与！点击链接观看完整课程https://bizwebcast.intel.cn/eventstart.aspx?eid=325&amp;tc=6om8hxw514&amp;frm=InfoQ</p>\n<p><strong>精彩亮点</strong></p>\n<ul>\n<li>什么是异构计算？异构计算最能解决的市场问题和技术场景是什么？</li>\n<li>异构计算在实际运用中面临哪些痛点和挑战？解决之道又是什么？</li>\n<li>日活3.5亿的快手，如何通过异构计算化解算力瓶颈？</li>\n<li>在异构计算领域，英特尔又有哪些技术探索和应用案例？</li>\n</ul>\n<p><strong>议程</strong></p>\n<ul>\n<li>圆桌讨论：异构计算 数据中心“芯”变革</li>\n</ul>\n<p>钟辉快手异构计算中心AI&amp;HPC负责人</p>\n<p>高明英特尔云与行业解决方案事业部互联网行业技术总监</p>\n<p>霍太稳极客邦科技创始人、CEO</p>\n<ul>\n<li>课程分享：智能升级 异构创新</li>\n</ul>\n<p>钟辉快手异构计算中心AI&amp;HPC负责人</p>\n<ul>\n<li>课程分享：异构计算 数据中心“芯”变革</li>\n</ul>\n<p>高明英特尔云与行业解决方案事业部互联网行业技术总监</p>",
    "publish_time": "2022-09-23 11:07:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "24年了，终于有人发现curl的这个Bug了",
    "url": "https://www.infoq.cn/article/43a6Rhf0Dx70W41xHrrK",
    "summary": "<p></p><blockquote>这是一个关于 cookie、互联网代码和 CVE（通用漏洞披露）的故事。</blockquote><p></p><p></p><p>本文最初发布于Daniel Stenberg的<a href=\"https://daniel.haxx.se/blog/2022/09/05/a-bug-that-was-23-years-old-or-not/\">个人博客</a>\"。</p><p></p><p><a href=\"https://xie.infoq.cn/article/2d0f6e690046516ac2edf7d26\">curl</a>\" 作者 Daniel Stenberg 近日在个人博客分享了一个存在 23.9 年的 curl 漏洞。curl 是常用的命令行工具，用来请求 Web 服务器，于 1997 年首次发行。</p><p></p><p>据 Stenberg 透露，这个漏洞是在 curl 发布后的第 201 天引入的，但是直到第 8930 天，漏洞才修复好。一个持续了 23.9 年的漏洞背后有着怎样的故事？</p><p></p><p>一切还得从 1998 年说起。</p><p></p><h2>curl 4.9 与 cookie</h2><p></p><p></p><p>1998 年 10 月，Stenberg 带领团队推出了 curl 4.9 版本。当时，听过或用过 curl 的人还少得可怜。几个月之后，curl 官网才宣布新版本的下载量达到了 300。那时，无论从何种意义上讲， curl 都还很小众。</p><p></p><p>curl 4.9 作为第一个带有 “cookie 引擎” 的版本，可以接收 HTTP cookie、解析、识别，并在后续的请求中把 cookie 正确地返回。在 curl 中，处理 cookie 的大部分代码都是 Stenberg 编写的。</p><p></p><p>那会，<a href=\"https://www.infoq.cn/article/VXv0V8xDeLxfv277iZ1Y\">cookie </a>\"还没有明确的规范，仅有的一份描述 cookie 工作原理的规范，是一份由Netscape 管理的文档cookie_spec （感兴趣的同学可以戳链接查看文档副本：<a href=\"https://curl.se/rfc/cookie_spec.html\">https://curl.se/rfc/cookie_spec.html</a>\"）。这份文档并不完善，有不少信息需要通过查看其它客户端才能了解到。</p><p></p><p>Stenberg 在实现处理 cookie 的代码时，就是参考了这份文档以及当时浏览器的大致处理方式。</p><p></p><p>此后十年，IETF（互联网工程任务组）一直在努力创建 cookie 规范，但均以失败而告终。这些早期 cookie 规范的创建者可能觉得，他们创建了标准，世界就会情不自禁地遵守它们，但事实并非如此。cookie 的特殊之处在于，有很多不同的作者、代码库和网站实现了它。因此，很难从根本上改变它们的工作方式。</p><p></p><p>直到 2011 年，cookie RFC 正式发布了，它记录并解释了 cookie 实际的使用方式，这可以说是真正意义上的 cookie 规范。Stenberg 本人也参与了规范的制定过程，并在其中阐述了自己的观点和意见。对于这份规范的内容，虽然 Stenberg 并不完全赞同，但与此前的各种 cookie 规范相比，cookie RFC 的确是一个巨大的进步。</p><p></p><h2>cookie 双重语法带来的挑战</h2><p></p><p></p><p>一开始，新的 cookie 规范并没有给 Stenberg 造成困扰，但很快，规范的特殊编写方式让 Stenberg 倍感头疼：它针对服务器如何发送 cookie 提供了一种字段语法，而针对客户端应该接受什么样的 cookie 提供了另一种语法。也就是说，同样的 cookie，需要两种语法。</p><p></p><p>这有两个很直接的缺点：</p><p></p><p>规范很难阅读。你很容易就停留在其中一种语法上，以为那就是适合自己用例的，但却没有意识到角色描述是错误的。定义如何发送 cookie 的语法其实并不重要，因为如何接收和处理 cookie 都是由客户端决定的。现有的大型 cookie 解析器（浏览器）有一定程度的自由决定自己接受什么，所以没人注意，也没人关心服务器是否严格遵守了规范中的语法。与此同时，cookie 规范也在持续更新。从几年前开始，IETF 就一直在修订和更新 2011 年的 cookie 规范，计划将世界上一些已实际投入使用的 cookie 扩展添加到规范中。这项 cookie 规范更新工作被称为 6265bis。</p><p></p><p>curl 也同步进行更新，以确保符合 RFC 6265bis 草案版本的规定。</p><p></p><p>但是，双重语法仍然是 cookie 规范文档中悬而未决的问题。</p><p></p><p>随着时间的推移，cookie 的发展变得缓慢。在过去的几十年里，HTTP 规范也就更新了有限的几次，但值得一提的是，HTTP 服务器实现已经实施了更严格的解析策略：</p><p></p><p></p><blockquote>如果传入的 HTTP 请求看上去“非法”或格式不正确，那么 HTTP 服务器就会提前拒绝，把它们挡在门外。对于请求中的控制代码尤其如此。如果你试图将一个包含控制代码（这里的控制代码指的是介于 1 到 31 之间的字节值，不包括 9，9 是TAB）的请求发送到一个相当新的 HTTP 服务器，那么服务器很可能会拒绝，并返回 400 响应代码。从 2016 年 12 月发布的 2.4.25 版本开始，HTTP 服务器 Apache httpd 就默认启用了此行为。最新版本的 Nginx 似乎也是这样做的。</blockquote><p></p><p></p><p>如果是现在设计 cookie，那么肯定会有所不同。</p><p></p><p>设置 cookie 的网站把 cookie 发送到客户端，对于其发送的每个 cookie，它都会设置多个属性。尤其是当需要客户端发回 cookie时，它会设置匹配参数。</p><p></p><p>在 cookie 的这些参数中，其中有一个是 domain，客户端发送 cookie 时要匹配它。服务器www.example.com可以设置 cookie 的有效范围为整个example.com域，这时，客户端在访问second.example.com 时也会发送 cookie。也就是说，服务器可以将 cookie 设置为适用于“兄弟站点”。</p><p></p><p>值得一提的是，1998 年添加到 curl 中的 Cookie 代码在接受内容方面相当自由，当然，多年来也经过了不少调整和完善，不过它始终与现实世界的网站保持了兼容。对于那部分代码，Stenberg 修改的主要动力始终是为了使 curl 的 Cookie 处理方式与其他已有的使用 cookie的代理保持基本一致，并可以互操作。</p><p></p><h2>curl 的 Bug 详情与修复方案</h2><p></p><p></p><p>2022 年 6 月底，Stenberg 收到了一份报告，报告怀疑 curl 中存在安全问题。正是这份报告促使 curl 发布了 <a href=\"https://curl.se/docs/CVE-2022-35252.html\">CVE-2022-35252</a>\"。</p><p></p><p>事实证明，源于 1998 年的旧 cookie 代码，会接受包含控制代码的 cookie。控制代码可以是名称或内容的一部分，如果用户启用了“cookie 引擎”，那么 curl 就会存储那些 cookie，并在后续的请求中将它们发送回来。</p><p></p><p>例如，curl 会接受下面这样的 cookie：</p><p></p><p><code lang=\"plain\">Set-cookie: name^a=content^b; domain=.example.com</code></p><p></p><p>^a 和 ^b 表示控制代码。由于域可以将 cookie 标记为适用于其他主机，、所以发送到域中所有主机的请求都会包含这个 cookie。</p><p></p><p>当 curl 将类似那样的一个 cookie 发送到 <a href=\"https://xie.infoq.cn/article/8176e4fe774aee69cd57a6275\">HTTP</a>\" 服务器时，它的外发请求中会包含下面这样一个 header 字段：</p><p></p><p><code lang=\"plain\">cookie: name^a=content^b</code></p><p></p><p>对此，Apache httpd 及其他服务器的默认配置都会返回 400。一个脚本或应用程序在收到这样的 cookie 后，如果后续的请求中还继续发送 cookie，就会遭到拒绝。</p><p></p><p>Stenberg 复盘后发现，cookie 规范 RFC 6265 5.2 节确实说了，客户端应该丢弃包含控制代码的 cookie，但这部分对用户来说理解起来比较难，需要对文档有深入的研究才能发现。此外，规范并没有提及“控制代码”或是字节值范围。</p><p></p><p>Stenberg 认为，要弄清楚主流浏览器是怎么做的还是比较容易的，因为它们的源代码很容易获得。事实证明，Chrome 和 Firefox 都已经忽略了包含以下任何字节的传入 cookie：</p><p></p><p><code lang=\"plain\">%01-%08 / %0b-%0c / %0e-%1f / %7f</code></p><p></p><p>其中不包含 %09（TAB）和 %0a / %0d（行结束符）。</p><p></p><p>Bug 修复方面，Stenberg 表示，curl 的修复补丁处理方式非常简单：拒绝包含一个或多个禁用字节值的 cookie 字段。Stenberg 认为，这种修改基本是没有风险的。</p><p></p><h2>写在最后</h2><p></p><p></p><p>推算起来，有漏洞的代码从 curl 4.9 版本开始就一直存在，curl 7.85.0 版本才完成修复。整个历程有 8729 天（23.9年）。也就是说，这个 Bug 是在项目发布的第 201 天引入的，到第 8930 天才修复。</p><p></p><p>Stenberg 认为，代码在发布时是没什么问题的，并且在用户的使用过程中，也基本没有产生什么问题。它的问题出在，HTTP 服务开始拒绝可能的恶意 HTTP 请求时。如此一来，这段代码就变成了一种拒绝服务，这或多或少会带来一些副作用。</p><p></p><p>或许，这个 Bug 诞生于 RFC 6265 发布之时。或许，它诞生于 HTTP 服务器开始拒绝这些请求时。不管怎样，这个 Bug 创造了一个新的项目记录：它是第四个被发现之前存在了 8000 多天的 Bug。</p>",
    "publish_time": "2022-09-23 14:08:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "绿色制造：AIoT的潜力应用场景",
    "url": "https://www.infoq.cn/article/a9ChVY8DGJRbXqTHqjah",
    "summary": "<p>虽然现在我们谈到<a href=\"https://www.infoq.cn/article/7vTfCLYoKzuIZTjdLGk0\">AIoT</a>\"在工业场景的应用时，主要还是聚焦质量检测、安全生产管理、设备预测性维护、现场排产优化等等。但是，由于绿色制造与产线和设备运行息息相关，因此也在成为AIoT的潜力应用场景。</p><p></p><p>尤其是在“双碳”战略背景下，绿色制造成了一个硬指标。工信部在《<a href=\"http://www.gov.cn/zhengce/zhengceku/2021-12/03/content_5655701.htm\">“十四五”工业绿色发展规划</a>\"》中提出，到2025年，要实现我国规模以上工业单位增加值能耗降低13.5%，大宗工业固废综合利用率达到57%，主要再生资源回收利用量达到4.8亿吨，单位工业增加值用水量降低16%，重点行业主要污染物排放强度降低10%。</p><p></p><p>这一连串数字反馈到具体的工业生产场景，将变成一个又一个具体的节能减排任务，成为工业企业绿色制造战略的重要组成部分。</p><p></p><p>但是，在过去很长一段时间里，绝大多数工业企业的第一任务是生产规模扩张和业务快速发展，工业污染的控制方式主要还是“先污染后治理”。简单理解，就是污染的处理会等到生产末端环节才进行，生产过程中的能耗和排放往往会被忽视。</p><p></p><p>这种粗放的“填坑”方式，显然已经很难满足目前绿色制造的标准和要求，大量的能源浪费、污染排放实际上在前端的生产环节就已经发生，并且不可逆转。所以，从20世纪80年代开始，以“防范为主”的污染控制策略就慢慢成了国际主流，它主张的是以节能、降耗、减污为目标，以管理和技术为手段，实施工业生产全过程污染控制，使污染物的产生量最少化。</p><p></p><p>其中，实现生产全过程的污染控制，意味着要掌握从产品研发、规划、设计、生产到运营管理全流程中与污染相关的信息，并针对性采取措施。比如哪些环节的能耗最高、什么时间段最高、如何据此进行绿色排产，哪些环节是比较大的排污节点、需要使用多少废气废水废渣的处理原料才能满足绿色制造标准等等......</p><p></p><p>也就是说，绿色制造的背后需要一系列的能源管理机制，涉及能源管理与各个业务模块间的协调运作。在这个过程中，如果只是散点、滞后的收集，很难形成全局视角，更谈不上指导生产，设备的连接、实时的监测和分析是一些基本前提，而这就是AIoT等相关技术，在绿色制造场景中有的放矢的原因。</p><p></p><h1>减少不必要的能源损耗和原料浪费</h1><p></p><p></p><p><a href=\"https://www.keduotech.com/sy\">苏州科舵</a>\"工程技术有限公司（以下简称“苏州科舵”）是一家环保和制药设备的工程设计与制造的企业，目前他们正在把三废处理过程中对污染气体、液体和固态颗粒物的有机处理技术融入到设备的工艺设计中，并且结合AIoT让绿色制造的理念贯穿于制造全过程。</p><p></p><p>日前，苏州科舵数智工业部总监兼首席信息官许宏在接受InfoQ记者采访时表示，AIoT是在万物互联的基础上与<a href=\"https://xie.infoq.cn/article/5760ea35b9df78e05d1c6bf57\">人工智能</a>\"技术进一步形成连接关系。“通过传感感知、微控制器等<a href=\"https://xie.infoq.cn/article/1c77ccf6da77b17423bcbc660\">物联网</a>\"等技术相互连接形成网络，工业场景中的客观物理世界可以形成大量信息的数据化，在这基础上引入人工智能的意义，是依靠计算机帮助加快运算，并且它可以模仿人脑的思维，以此来找出数据间的相关性或趋势，进而产生探索新价值点的可能性，得到新增长的生产关系与发展机遇。”</p><p></p><p>而在<a href=\"https://xie.infoq.cn/article/4ca3d6b8a17839cc9c9977459\">绿色制造</a>\"过程中，具体来说有两个核心目标：一是节能，包括通过资源的有效利用、资源的再利用、节水省料，从而减缓对资源的消耗；二是减排，包括减少废气、废水、废渣的产生和排放，减少生产活动对环境的伤害。</p><p></p><p>针对节能目标，许宏指出，AIoT的一个关键价值就是实现能源平衡调度。比如，通过实时采集燃料气、电力、水等能源介质的投入和消耗数据，并结合生产数据进行相关性分析，可以获取二者的线性关系，其中包括在产出较低时设备耗能是否异常超出指标，产出较高的时候设备耗能是否控制在合理水平内等等，从而及时对其中的无效生产运行做出调节，提高生产资料的综合利用率，优化能源调度。</p><p></p><p>针对减排目标，过去企业大多是通过加装处理装备，把排放物的指标控制在环保水平以内。拿有机工业废水处理来说，其中主要包含了高盐成分、高氮氧化物等等，在排放之前，需要往里面加入吸附材料等处理材料，把废水分解成符合环保标准的可排放水。</p><p></p><p>虽然从结果来看确实达到了减排的目的，但是许宏表示，很多企业对这些添加吸附材料的用量、利用率等信息并不关注，通常是每隔一段时间就往里添加新的原料，并没有考虑是否已经吸附饱满、能不能循环利用等等，而且根据人的经验，企业经常会多备库存，这同样会带来不必要的损耗和浪费。</p><p></p><p>“而通过在关键环节进行数据采集和智能分析，企业就可以知道这些原料的吸附率实时的趋势，帮助企业判断什么时候要添加、需要添加多少，如果没有吸附饱满，可以进入哪个环节进行二次循环利用。”</p><p></p><p>许宏告诉InfoQ，“我们曾经对一家日排放有机废水600吨的企业做过测算，他们在没有使用AIoT之前，整套设备的运行成本是一年四千万左右，通过生产资料的优化，他们把很多原料投入的利用率提升了25%-32%，最后的设备运行成本节省了一千万左右。”</p><p></p><h1>解决连接、安全和模型的问题</h1><p></p><p></p><p>虽然这种以“防范为主”的污染控制策略在80年代就开始被提出，但是践行和推广这个理念并不容易，尤其是对于很多老厂来说，过去的工业产线设计即便考虑了能源管理，也只是从成本视角出发，其中涉及的标准和规范，与绿色制造的目标并不完全一致，数据采集的关键节点也会有所偏差。</p><p></p><p>换句话说，要实现以绿色制造为目标的<a href=\"https://xie.infoq.cn/article/518a56a36d42fc059b0ca5ec4\">能源管理</a>\"，需要采集更多维度的数据。</p><p></p><p>而在这个过程中，许宏提到一个难点：过去制造企业的传感器鲜少布置在一些高温的极端环境，现在，为了采集热能热值数据，这变成了不得不做的工作。</p><p></p><p>“其中会涉及一些跨学科的知识，比如为了适应高温环境，可能就要用电磁学的方式进行感知感测。这对于苏州科舵这样的设备的工程设计企业来说，就会提出新的要求，包括相关技术是否达到了可行性的标准等等。”许宏举例。</p><p></p><p>不过，他也强调，这并不是企业利用AIoT实现绿色制造的主要挑战。“比较大的一个挑战是，这些废水、废气的处理设备都是分立的，需要接入到企业原来的生产<a href=\"https://www.infoq.cn/news/AkzC5erKpwNFTELuPAGQ\">场景</a>\"中才能真正实现所谓的全工艺过程管控优化，这是比较难的。”</p><p></p><p>针对这个问题，苏州科舵提出的办法是用<a href=\"https://xie.infoq.cn/article/e4f936f259dae1d7bf39b05e8\">边缘计算</a>\"的方式，把机器、设备等固定资产IOT化链路到网络中，并入原始生产厂商的设备中；此外，与工业企业现场的设备连接，则采用传统以太网或者Wi-Fi。</p><p></p><p>也就是说，苏州科舵把废污处理设备设计成了一个边缘计算设备，其中内置了网关、计算、算法模型等能力。但这又会演化出两个新的问题：</p><p></p><p>首先，一旦涉及网络，就有一个无法回避的问题——安全。许宏谈到，在苏州科舵帮助企业部署废污设备时，他们经常会对其中的网络安全提出担忧。但在他看来，这种担忧往往是因为认知不足而放大了不成立的风险，而解决这个问题最关键的是时间，需要通过让新技术带来价值，让顾客分享到收益，才会消除企业的顾虑。</p><p></p><p>其次，边缘设备中的模型必须随着设备的持续运行与采集，根据实时的数据情况不断优化，并且对照旧模型持续迭代。</p><p></p><p>“比如一个管道，过去企业采集的数据都是静态的，里面的压力是多少就是多少。但是，现在企业除了要动态采集管道中液体的压力数据，可能还要采集它的流速、材质（是粘稠的还是稀薄的）等等，然后才能把这些物理属性转变为动态变化的数字属性。并且，随着产线的运行，其中涉及的参数要素、算法模型也要不断做出调整，才能满足生产现场的需要。”许宏举例。</p><p></p><p>对此，他还强调，这背后要求企业能够拉通技术、生产、设备等各个部门的人员，整合各个层面的领域知识，才能界定其中的各种关键参数和要素，比如，流速在什么范围内是正常值、超出多少需要预警、达到多少需要立即采取措施——只有让相关人员参与到这些细节确认中来，才能让<a href=\"https://xie.infoq.cn/article/c78637e0d3db5513ac232d71c\">算法模型</a>\"越来越精细化。</p><p></p><h1>信息化是AIoT应用的先决条件</h1><p></p><p></p><p>除此之外，要让“AIoT助力企业实现绿色制造”这个课题成立，还有一些先决条件。许宏认为，其中，最重要的就是必须要健全企业的信息化、网络化和<a href=\"https://www.infoq.cn/article/21Jp2xK1AlTE0gYGBxVJ\">数据基础</a>\"，所有信息要能够互联互通，所有的流程要可以被标准化，并且由<a href=\"https://www.infoq.cn/article/YcvRvZaOoez9sKJ92jHR\">数据驱动</a>\"。</p><p></p><p>“所谓的标准就是各个作业环节要连接起来，如果企业的信息还是割裂的，或者信息交互还需要通过人传人，这就不是标准化的信息驱动方式。在这种情况下，要去落地AIoT是不现实的。”许宏表示。</p><p></p><p>拿能源管理场景举例：假设生产过程中的某种能源介质原料投入出现指标异常，而后台人员在拿到数据后还是通过手工单据的方式记录数据，或者用口头的方式通知生产现场人员进行处理，那么，造成的后果——一方面是现场人员拿到的不是当下数据，采取的应对举措可能会不准确或者滞后；另一方面，很多数据掌握在后台人员手中无法被有效留存，很难形成管理闭环。</p><p></p><p>“也就是说，数据要被多维度并且实时呈现，在数据背后还要有人做出判断和决策，这些决策都是通过数据去驱动的。只有这样一环扣一环，系统和人才能各自发挥最大作用。否则，人就只是信息的制造者、传递者，而不是信息的使用者和受益者。”许宏表示。</p><p></p><p>当然，这里还有另一个前提，即人在这个过程中拿的数据分析结果是准确的，否则就会出现决策方向偏差。对此，许宏强调，企业中的所有信息应该放在一个所谓的“数据网格”中，在其中，企业必须定义清楚元数据，确定数据的类型、数据的业务归属、相关数据归类等等，这是人工智能得出准确分析结果的基础和前提。</p><p></p><p>“所以，AIoT应用不是上一个系统就能解决的事情，它的投资建设周期是比较长的。从信息、数据、数字到智能的这个基本逻辑是贯穿于其中的。对于企业来说，任何盲从行为，都将是在浪费有效变革活动的时间与资金。”许宏强调。</p><p></p><p>抛开绿色制造场景，想清楚企业的现实情况和真实需求同样重要。许宏举了另一个场景案例——<a href=\"https://www.infoq.cn/article/MDuweEQy7Qr1FNHFXzfN\">AI质量检测</a>\"。</p><p></p><p>“比如说企业的生产产量很大，产线运转速度非常快，如果依赖人工检测或者后期抽检，一是效率不高，二是出错率比较大，这时候就可以考虑通过AI质检替代人工。但是，如果企业的生产规模很小，产线流转也没那么快，可能用一两个人就能看过来了，那这时候投入那么多钱上AI质检系统就是不划算的。”许宏表示，“也就是说，很多AI应用的场景，一定是在企业产量要达到一定规模，技术的投入和应用才有意义、才有经济性。企业应该从自身经营的角度，进行全盘的考量和统筹。”</p><p></p><p>在许宏看来，AIoT的“种子”刚刚种下，一切刚刚开始，企业只有脚踏实地从基础做起、从自身出发，才能从中汲取能量和价值。“未来，AIoT的价值一定不会局限于企业本身，它会贯穿于垂直产业链，通过生态共创和共同增长，将有助于个体企业在AIoT技术应用场景有效实现的关键助力者。而在此之前，需要企业先保持开放的心态，从原本的独享价值收益转为共享价值收益，只有开放方可共生。”</p>",
    "publish_time": "2022-09-23 14:20:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "人人都用Bootstrap的年代过去了，如今我很难向开发者们推荐Bootstrap 5",
    "url": "https://www.infoq.cn/article/RIEpRiQSDkOoNSLXdzjI",
    "summary": "<p></p><h2>Bootstrap 5和Tailwind相比简直就是噩梦</h2><p></p><p>&nbsp;</p><p>我最近开始为开发人员编写小教程。我的目标是向初级开发人员展示高级开发人员在编写代码时是如何思考的。例如，高级开发人员如何开始理解他们原先不了解的文档或新框架，以及他们在出错时（也就是著名的“代码坏味道”）如何解决问题。</p><p>&nbsp;</p><p>我在Frontend Mentor网站上找到了一些示例项目，并写了各种各样的代码。例如，第一个项目是一个产品卡片组件，我决定使用普通的CSS和Tailwind以及Bootstrap来编写解决方案。</p><p>&nbsp;</p><p>这不仅对读者来说很有趣，对我来说也很有趣。我想看看Tailwind和Bootstrap在过去几年里发展得怎样。我知道Tailwind的开发者体验会很棒，但我并不知道原来使用Bootstrap 5会如此痛苦。它很糟糕，所以我写了这篇文章。</p><p>&nbsp;</p><p>在本文中，你将看到历史（主要是关于Bootstrap）、想法（关于开发体验）和代码片段。让我们先从历史部分讲起吧！</p><p>&nbsp;</p><p></p><h2>人人都用Bootstrap的年代</h2><p></p><p>&nbsp;</p><p>多年前，Bootstrap无处不在。在2016年，你几乎找不到一个没有使用Bootstrap的网站。CSS Flexbox已经有了，但CSS Grid还不为人所知，也没有得到很好的支持。我记得在2016年中，我曾与一位开发人员谈起CSS Grid，我向他讲述了它的强大功能，但他的回答令人感到震惊——他还以为我说的是HTML表格元素，可见他对CSS Grid一无所知。那时，所有人都依赖Bootstrap 3，他们都还在使用jQuery。</p><p>&nbsp;</p><p>除了Bootstrap，还有其他CSS框架。第二个最著名框架的是Foundation。除了电子邮件模板，我使用Foundation的机会并不多。也许它比Bootstrap更好或更差或不同或相同，事实上，我真的不知道。</p><p>&nbsp;</p><p>他们声称（现在在他们的GitHub页面上仍然如此）自己是“世界上最先进的响应式前端框架”。但从他们的官方文档和GitHub代码库来看，我觉得它已经死了。我不想在历史方面花太多时间，只是想在继续其他话题之前补充一些东西。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/dd/27/dd1bb8ee7aa2d373b306783ae4744727.png\" /></p><p></p><p>bonnegueule.fr——2016年使用Bootstrap编写的一个法国网站（来源：waybackmachine.com）</p><p>&nbsp;</p><p>首先，Bootstrap是由Twitter的工程师开发的，他们做得很好。Twitter仍然在使用Bootstrap，还有许多网站也是。大多数时候，即使你不是一个经验丰富的开发人员，也可以很容易地判断一个网站是否使用了Bootstrap。但在我看来，这并不总是好事情，因为它会让所有网站看起来都一个样。</p><p>&nbsp;</p><p>对于一些网站来说，这可能是一个优势，因为用户不需要重新熟悉网站。例如，他们知道链接或提交按钮是什么样子的。但是，在某些时候，你的网站失去了个性，依赖它就变成了一个劣势。它的第二个优点是核心开发团队决定在Bootstrap 5中移除jQuery。我不知道这个决定是好是坏。今天，jQuery没有其他前端框架那么受欢迎，开发人员也越来越少使用它，所以在某种程度上，这么做是有意义的。</p><p>&nbsp;</p><p>然而，移除jQuery增加了核心开发团队的工作量，并且失去了对旧浏览器的一些兼容性。</p><p>&nbsp;</p><p>最后要说声“感谢”。Bootstrap核心开发团队在网页可访问性方面做得非常出色，尤其是Bootstrap 4。他们引入了许多与aria属性相关的概念，并提供了人们可以使用的具体示例和代码示例。在我看来，Bootstrap 4很棒！遗憾的是，Bootstrap 5不是。</p><p>&nbsp;</p><p></p><h2>Tailwind重塑编写CSS的方式</h2><p></p><p>&nbsp;</p><p>与许多人一样，我也是因为Tailwind才知道“实用优先CSS（Utility-first CSS）”这个词。在此之前，我一直在尝试使用BEM。我不知道你是怎么想的，但我一直对BEM有种迷惘的感觉。我了解修饰符的概念，但是当我需要使用块或元素时，会感到有点困惑。</p><p>&nbsp;</p><p>理论上，按照这种方式构建CSS似乎是一个合理的想法。然而，它往往会污染HTML的可读性。这里有一篇文章（<a href=\"https://medium.com/@jescalan/bem-is-terrible-f421495d093a\">https://medium.com/@jescalan/bem-is-terrible-f421495d093a</a>\"）介绍了这些概念。</p><p>&nbsp;</p><p>让我们回到Tailwind上来。为什么说在开发“实用优先”CSS框架时Tailwind向前迈出了一大步？如果你读过前面我分享的那个文章资源，那么你应该已经知道答案了。Tailwind不像Bootstrap那样依赖组件且一次应用多种样式，相反，它添加只做一件事的类。换句话说，你编写的类可以进行任意组合——一个类用于填充，另一个类用于控制字体大小，等等。</p><p>&nbsp;</p><p>我们来举个例子。</p><p>&nbsp;</p><p>下面是使用Bootstrap编写的卡片组件。</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><div class=\"card\" style=\"width: 18rem;\"><code lang=\"null\">\n  <img alt=\"...\" class=\"card-img-top\" src=\"https://www.infoq.cn/article/...\" />\n  <div class=\"card-body\">\n    <h5 class=\"card-title\">Card title</h5>\n    <p class=\"card-text\">Some quick example text to build on the card title and make up the bulk of the card's content.</p>\n    <a class=\"btn btn-primary\" href=\"https://www.infoq.cn/article/RIEpRiQSDkOoNSLXdzjI#\">Go somewhere</a>\n  </div>\n</code></div><p></p><p>&nbsp;</p><p>结果看起来像这样：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/98/75/98ce858d3d8661e4372c393f378e4475.png\" /></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/98/75/98ce858d3d8661e4372c393f378e4475.png\" /></p><p></p><p>下面是使用Tailwind编写的卡片组件。</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><figure class=\"md:flex bg-slate-100 rounded-xl p-8 md:p-0 dark:bg-slate-800\"><code lang=\"null\">\n  <img alt=\"\" class=\"w-24 h-24 md:w-48 md:h-auto md:rounded-none rounded-full mx-auto\" height=\"512\" src=\"https://www.infoq.cn/sarah-dayan.jpg\" width=\"384\" />\n  <div class=\"pt-6 md:p-8 text-center md:text-left space-y-4\">\n    <blockquote>\n      <p class=\"text-lg font-medium\">\n        “Tailwind CSS is the only framework that I've seen scale\n        on large teams. It’s easy to customize, adapts to any design,\n        and the build size is tiny.”\n      </p>\n    </blockquote>\n    <figcaption class=\"font-medium\">\n      <div class=\"text-sky-500 dark:text-sky-400\">\n        Sarah Dayan\n      </div>\n      <div class=\"text-slate-700 dark:text-slate-500\">\n        Staff Engineer, Algolia\n      </div>\n    </figcaption>\n  </div>\n</code></figure><p></p><p>&nbsp;</p><p>结果看起来像这样。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/d2/0b/d200bbbe41b3e5c7d1b99d39642d490b.png\" /></p><p></p><p>这两张卡片看起来不一样，但我希望你们看看代码。在使用Bootstrap时，卡片组件可以在大小和颜色方面有所不同，但最终，它们或多或少看起来是一样的。还记得我在本文开头说的“即使你不是一个经验丰富的开发人员，也可以很容易地判断一个网站是否使用了Bootstrap”吗？</p><p>&nbsp;</p><p>在使用Tailwind时，你几乎可以把自定义样式添加到任何东西上。你可能会问，这重要吗？首先，因为它允许你按照你想要的方式设计网站，这就非常棒了，尤其是对设计师来说！其次，它可以帮助开发人员尽可能接近你想要的模型，并且不会破坏HTML标记和CSS之间的任何责任关系。</p><p>&nbsp;</p><p>有了Tailwind，你就不再需要依赖BEM了。对我来说，这真是个好消息！</p><p>&nbsp;</p><p></p><h2>开发者体验和文档</h2><p></p><p>&nbsp;</p><p></p><h4>Tailwind——设置和使用都方便</h4><p></p><p>&nbsp;</p><p>那是我第一次使用Tailwind。我的意思是，我之前就知道这个项目，并且已经阅读了文档，但还没有机会使用它。对于这个项目，我决定不使用CLI或PostCSS。</p><p>&nbsp;</p><p>我决定使用CDN。通过阅读文档得知，生产环境的网站不适合这么做，但如果是出于试验的目的是可以的。不过我还是惊讶地发现，即使使用了CDN，仍然可以很简单地添加自定义颜色和字体。</p><p>&nbsp;</p><p>相比之下，Bootstrap API则是一团糟。它的文档和示例代码都不容易看懂，尤其是如果你还不知道有版本5。</p><p>&nbsp;</p><p>我是这样在项目中设置Tailwind的：</p><p>&nbsp;</p><p><code lang=\"null\">\n  </code></p><p>&nbsp;</p><p>在上面的代码示例中，我在tailwind.config对象中添加了新的颜色和新的字体。</p><p>&nbsp;</p><p>然后，我在代码中使用它：</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p class=\"font-montserrat font-medium text-xs tracking-[.5em] uppercase\ntext-stormGray mb-3\"><code lang=\"null\">\n  Perfume\n</code></p><code lang=\"null\">\n<h1 class=\"font-fraunces text-3xl font-bold text-ebonyClay mb-4\">\n  Gabrielle Essence Eau de Parfum\n</h1>\n<p class=\"font-montserrat font-medium text-sm text-stormGray mb-6\">\n  A floral, solar and voluptuous interpretation composed by Olivier Polge,\n  Perfumer-Creator for the House of CHANEL.\n</p></code><p></p><p>&nbsp;</p><p>看，这很容易！我不需要写自定义CSS或Sass代码，我只需要调用正确的类就可以了。它真的非常简单，但却很强大！</p><p>&nbsp;</p><p></p><h4>Tailwind——多么好的文档</h4><p></p><p>&nbsp;</p><p>这就引出了Tailwind的第二个优势——文档。我是一名开发人员（我想你也是），大多数时候，当我们忙于一个项目时，根本没有时间阅读整个文档。我们需要可以直接使用或重用的简单但具体的示例。</p><p>&nbsp;</p><p>我们想看看Tailwind是否是一个对的工具，我们想很快得出结论。在过去的几年里，我对开发者体验（简称DX）越来越感兴趣。以下是我在网上找到的一个有关DX的定义：</p><p>&nbsp;</p><p></p><blockquote>开发者体验是指与API或开发工具相关的每一次交互。—— 来源：<a href=\"https://everydeveloper.com/developer-experience/\">https://everydeveloper.com/developer-experience/</a>\"</blockquote><p></p><p>&nbsp;</p><p>关于交互，不要只考虑代码注释、变量名或遵循约定，我们还要考虑文档，以及使用库或框架的容易程度。</p><p>&nbsp;</p><p>例如，我认为Symfony和Angular的开发者体验就很棒。它们提供了教程、优秀的文档和代码示例。但还有更多的东西！当你开始使用它们时，你也会学到其他概念。例如，Symfony文档解释了实体和存储库模式的概念。Angular告诉我们什么是Promise和Observable。</p><p>&nbsp;</p><p>相反，React的文档就算不上出色。它们都没有解释如何编写有效的测试，甚至关于Hook都没有提供足够的信息。大多数时候，当我与开发人员谈及React Hook时，他们并不了解它们的工作原理。所以说，DX很重要，比你想象的更重要。它可以让一个项目成功，也可以让它失败。有了Tailwind，我一个下午就能完成这个项目。文档应该易于阅读和使用，框架也是。那么，Bootstrap 5怎么样？</p><p>&nbsp;</p><p></p><h4>Bootstrap——组件和Utility的混合</h4><p></p><p>&nbsp;</p><p>当我在写这篇文章时，我不确定我是想先讨论Utility API还是组件和Utility之间的模糊边界。</p><p>&nbsp;</p><p>退一步来看，我认为这两个问题是相关的，从组件和Utility开始讨论可能会更有意义。</p><p>&nbsp;</p><p>Bootstrap从一开始都是关于组件，有表单组件、轮播组件（即使你不应该使用它）、面包屑组件、模态组件，等等。这些是（还是曾经是？）Bootstrap最大的优势。</p><p>&nbsp;</p><p>如果我想要一个标题呢？我在“Content”部分的“Typography”页面可以找到关于heading、display和lead类的信息。</p><p>&nbsp;</p><p>这太棒了！但是“Utilities”部分的“Text”页面呢，尤其是“Font size”部分呢？这是另一种改变字体大小的方式。那么我应该用哪一种呢？我可以把它们结合起来使用吗？框架想让我怎么做？这些都是我在编写代码时问自己的问题。</p><p>&nbsp;</p><p>Bootstrap给我的印象是它位于两把椅子之间，不知道该坐哪一边。一方面，他们希望你坚持使用Bootstrap及其组件库，因为核心开发团队投入了大量时间和精力。另一方面，他们引入了Utility及其API，但文档里又没有给出很多示例。</p><p>&nbsp;</p><p></p><h4>Bootstrap Utility API——理解它需要一些时间</h4><p></p><p>&nbsp;</p><p>几天前，我正在为产品组件编写Bootstrap解决方案，并创建了不使用Bootstrap Utility的类。</p><p>&nbsp;</p><p>我想给一个元素添加90%的宽度。我查看了文档，在“Utilities”部分找到了“Sizing”页面。默认宽度为25%、50%、75%和100%。我先创建了一个叫作w-90的自定义宽度，表示90%，但我知道有些地方出错了。</p><p>&nbsp;</p><p>这又是代码的坏味道！我快速查看了Utility API页面，并认为已经理解了。我必须在项目中安装Sass和Bootstrap才能使用这个API。</p><p>&nbsp;</p><p>我安装好了，然后跳到“Customize”部分的“Sass”页面，发现我可以做出一个选择——要么导入Bootstrap所有的东西，但不能修改Utility，要么手动导入你需要的东西。</p><p>&nbsp;</p><p><code lang=\"null\">// Custom.scss\n// Option B: Include parts of Bootstrap\n\n\n// 1. Include functions first (so you can manipulate colors, SVGs, calc, etc)\n@import \"../node_modules/bootstrap/scss/functions\";\n\n\n// 2. Include any default variable overrides here\n\n\n// 3. Include remainder of required Bootstrap stylesheets\n@import \"../node_modules/bootstrap/scss/variables\";\n\n\n// 4. Include any default map overrides here\n\n\n// 5. Include remainder of required parts\n@import \"../node_modules/bootstrap/scss/maps\";\n@import \"../node_modules/bootstrap/scss/mixins\";\n@import \"../node_modules/bootstrap/scss/root\";\n\n\n// 6. Optionally include any other parts as needed\n@import \"../node_modules/bootstrap/scss/utilities\";\n@import \"../node_modules/bootstrap/scss/reboot\";\n@import \"../node_modules/bootstrap/scss/type\";\n@import \"../node_modules/bootstrap/scss/images\";\n@import \"../node_modules/bootstrap/scss/containers\";\n@import \"../node_modules/bootstrap/scss/grid\";\n@import \"../node_modules/bootstrap/scss/helpers\";\n\n\n// 7. Optionally include utilities API last to generate classes based on the Sass map in `_utilities.scss`\n@import \"../node_modules/bootstrap/scss/utilities/api\";\n\n\n// 8. Add additional custom code here</code></p><p>&nbsp;</p><p>然后我修改了Utility（下面将向你展示是如何修改的），并将Sass编译为CSS。乍一看似乎没问题，但我很快发现按钮没有了样式。原来我忘记导入一些东西了，看看第19行到26行就知道了。</p><p>&nbsp;</p><p>注释写着“根据需要导入其他部分”，但我是从Bootstrap 5开始的，我不知道哪些是需要的，哪些是不需要的。还记得我讲的DX吗？对我来说，这就是糟糕的DX！因为即使是简单的复制和粘贴，你也找不到一个能用的例子。这很令人沮丧，也不会给人留下什么好印象。</p><p>&nbsp;</p><p>下面是我修改的东西。</p><p>&nbsp;</p><p><code lang=\"null\">// Include any default variable overrides here (though functions won't be available)\n// Custom variables\n$viridian: hsl(158, 36%, 37%);\n$dawn-pink: hsl(30, 38%, 92%);\n$white: hsl(0, 0%, 100%);\n$ebony-clay: hsl(212, 21%, 14%);\n$storm-gray: hsl(228, 12%, 48%);\n\n\n$font-montserrat: 'Montserrat', sans-serif;\n$font-fraunces: 'Fraunces', serif;\n\n\n// Configuration\n@import \"../node_modules/bootstrap/scss/functions\";\n\n\n\n\n// 2. Include any default variable overrides here\n$body-bg: $dawn-pink;\n$font-weight-normal: 500;\n\n\n\n\n@import \"../node_modules/bootstrap/scss/variables\";\n\n\n// 4. Include any default map overrides here\n$custom-colors: (\n  \"viridian\": $viridian,\n  \"dawn-pink\": $dawn-pink,\n  \"white\": $white,\n  \"ebony-clay\": $ebony-clay,\n  \"storm-gray\": $storm-gray\n);\n\n\n// Merge the maps\n$theme-colors: map-merge($theme-colors, $custom-colors);\n\n\n\n\n@import \"../node_modules/bootstrap/scss/maps\";\n@import \"../node_modules/bootstrap/scss/mixins\";\n@import \"../node_modules/bootstrap/scss/utilities\";\n\n\n// Layout &amp; components\n@import \"../node_modules/bootstrap/scss/root\";\n@import \"../node_modules/bootstrap/scss/reboot\";\n@import \"../node_modules/bootstrap/scss/type\";\n@import \"../node_modules/bootstrap/scss/images\";\n@import \"../node_modules/bootstrap/scss/containers\";\n@import \"../node_modules/bootstrap/scss/grid\";\n@import \"../node_modules/bootstrap/scss/buttons\";\n\n\n\n\n// Import we don't need\n@import \"../node_modules/bootstrap/scss/tables\";\n@import \"../node_modules/bootstrap/scss/forms\";\n@import \"../node_modules/bootstrap/scss/transitions\";\n@import \"../node_modules/bootstrap/scss/dropdown\";\n@import \"../node_modules/bootstrap/scss/button-group\";\n@import \"../node_modules/bootstrap/scss/nav\";\n@import \"../node_modules/bootstrap/scss/navbar\";\n@import \"../node_modules/bootstrap/scss/card\";\n@import \"../node_modules/bootstrap/scss/accordion\";\n@import \"../node_modules/bootstrap/scss/breadcrumb\";\n@import \"../node_modules/bootstrap/scss/pagination\";\n@import \"../node_modules/bootstrap/scss/badge\";\n@import \"../node_modules/bootstrap/scss/alert\";\n@import \"../node_modules/bootstrap/scss/progress\";\n@import \"../node_modules/bootstrap/scss/list-group\";\n@import \"../node_modules/bootstrap/scss/close\";\n@import \"../node_modules/bootstrap/scss/toasts\";\n@import \"../node_modules/bootstrap/scss/modal\";\n@import \"../node_modules/bootstrap/scss/tooltip\";\n@import \"../node_modules/bootstrap/scss/popover\";\n@import \"../node_modules/bootstrap/scss/carousel\";\n@import \"../node_modules/bootstrap/scss/spinners\";\n@import \"../node_modules/bootstrap/scss/offcanvas\";\n@import \"../node_modules/bootstrap/scss/placeholders\";\n\n\n// Helpers\n@import \"../node_modules/bootstrap/scss/helpers\";\n\n\n\n\n$utilities: map-merge(\n  $utilities,\n  (\n    \"width\": map-merge(\n      map-get($utilities, \"width\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"width\"), \"values\"),\n          (90: 90%),\n        ),\n      ),\n    ),\n    \"max-width\": map-merge(\n      map-get($utilities, \"max-width\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"max-width\"), \"values\"),\n          (600: 680px),\n        ),\n      ),\n    ),\n    \"font-family\": map-merge(\n      map-get($utilities, \"font-family\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"font-family\"), \"values\"),\n          (\n            montserrat: $font-montserrat,\n            fraunces: $font-fraunces\n          ),\n        ),\n      ),\n    ),\n    \"letter-spacing\": (\n      property: letter-spacing,\n      class: lt,\n      responsive: true,\n      values: (\n        0: 0px,\n        5: 5px,\n        10: 10px\n      )\n    )\n  )\n);\n\n\n// Utilities\n@import \"../node_modules/bootstrap/scss/utilities/api\";\n\n\n.max-w {\n  max-width: 300px;\n}</code></p><p>&nbsp;</p><p>我导入了所有东西，甚至是我（还）不需要的部分，然后我加了一个注释告诉自己（未来的自己）我不需要它们。</p><p>&nbsp;</p><p>我多么希望能有一个具体的例子。最后，我终于能够添加新的Utility了。是的，这确实有点乏味。在下面的两个小节中，你将看到几个例子，关于如何在Bootstrap和Tailwind中导入和使用自定义字体以及如何添加新的宽度。我想有些人很希望能够看到具体的例子！</p><p>&nbsp;</p><p></p><h2>如何导入自定义字体</h2><p></p><p>&nbsp;</p><p></p><h4>Tailwind</h4><p></p><p>&nbsp;</p><p>首先，导入字体。我使用的是谷歌字体。</p><p>&nbsp;</p><p><code lang=\"null\">\n\n</code></p><p>&nbsp;</p><p>然后，从CDN导入Tailwind并更新tailwind.config对象。我添加了备用字体和新的颜色。</p><p>&nbsp;</p><p><code lang=\"null\">  \n  </code></p><p>&nbsp;</p><p>最后，使用它！</p><p>&nbsp;</p><p><code lang=\"null\"><span class=\"text-white font-montserrat font-bold text-sm\">\n  Add to Cart\n</span></code></p><p>&nbsp;</p><p>很简单，对吧？</p><p>&nbsp;</p><p></p><h4>Bootstrap</h4><p></p><p>&nbsp;</p><p>通过npm安装Saas。</p><p>&nbsp;</p><p><code lang=\"null\">{\n  \"dependencies\": {\n    \"bootstrap\": \"^5.2.0\",\n    \"sass\": \"^1.53.0\"\n  },\n  \"scripts\": {\n    \"sass\": \"node_modules/.bin/sass scss/main.scss css/main.css --watch\"\n  }\n}</code></p><p>&nbsp;</p><p>通过Sass添加Bootstrap并导入Bootstrap所有的东西。</p><p>&nbsp;</p><p>然后，你就可以去掉你不想要的部分。</p><p>&nbsp;</p><p>在第9行和第10行之间，我定义了Sass变量。我在第102行和第113行之间修改了font-family Utility。</p><p>&nbsp;</p><p><code lang=\"null\">// Include any default variable overrides here (though functions won't be available)\n// Custom variables\n$viridian: hsl(158, 36%, 37%);\n$dawn-pink: hsl(30, 38%, 92%);\n$white: hsl(0, 0%, 100%);\n$ebony-clay: hsl(212, 21%, 14%);\n$storm-gray: hsl(228, 12%, 48%);\n\n\n$font-montserrat: 'Montserrat', sans-serif;\n$font-fraunces: 'Fraunces', serif;\n\n\n// Configuration\n@import \"../node_modules/bootstrap/scss/functions\";\n\n\n\n\n// 2. Include any default variable overrides here\n$body-bg: $dawn-pink;\n$font-weight-normal: 500;\n\n\n\n\n@import \"../node_modules/bootstrap/scss/variables\";\n\n\n// 4. Include any default map overrides here\n$custom-colors: (\n  \"viridian\": $viridian,\n  \"dawn-pink\": $dawn-pink,\n  \"white\": $white,\n  \"ebony-clay\": $ebony-clay,\n  \"storm-gray\": $storm-gray\n);\n\n\n// Merge the maps\n$theme-colors: map-merge($theme-colors, $custom-colors);\n\n\n\n\n@import \"../node_modules/bootstrap/scss/maps\";\n@import \"../node_modules/bootstrap/scss/mixins\";\n@import \"../node_modules/bootstrap/scss/utilities\";\n\n\n// Layout &amp; components\n@import \"../node_modules/bootstrap/scss/root\";\n@import \"../node_modules/bootstrap/scss/reboot\";\n@import \"../node_modules/bootstrap/scss/type\";\n@import \"../node_modules/bootstrap/scss/images\";\n@import \"../node_modules/bootstrap/scss/containers\";\n@import \"../node_modules/bootstrap/scss/grid\";\n@import \"../node_modules/bootstrap/scss/buttons\";\n\n\n\n\n// Import we don't need\nimport \"../node_modules/bootstrap/scss/tables\";\n@import \"../node_modules/bootstrap/scss/forms\";\n@import \"../node_modules/bootstrap/scss/transitions\";\n@import \"../node_modules/bootstrap/scss/dropdown\";\n@import \"../node_modules/bootstrap/scss/button-group\";\n@import \"../node_modules/bootstrap/scss/nav\";\n@import \"../node_modules/bootstrap/scss/navbar\";\n@import \"../node_modules/bootstrap/scss/card\";\n@import \"../node_modules/bootstrap/scss/accordion\";\n@import \"../node_modules/bootstrap/scss/breadcrumb\";\n@import \"../node_modules/bootstrap/scss/pagination\";\n@import \"../node_modules/bootstrap/scss/badge\";\n@import \"../node_modules/bootstrap/scss/alert\";\n@import \"../node_modules/bootstrap/scss/progress\";\n@import \"../node_modules/bootstrap/scss/list-group\";\n@import \"../node_modules/bootstrap/scss/close\";\n@import \"../node_modules/bootstrap/scss/toasts\";\n@import \"../node_modules/bootstrap/scss/modal\";\n@import \"../node_modules/bootstrap/scss/tooltip\";\n@import \"../node_modules/bootstrap/scss/popover\";\n@import \"../node_modules/bootstrap/scss/carousel\";\n@import \"../node_modules/bootstrap/scss/spinners\";\n@import \"../node_modules/bootstrap/scss/offcanvas\";\n@import \"../node_modules/bootstrap/scss/placeholders\";\n\n\n// Helpers\n@import \"../node_modules/bootstrap/scss/helpers\";\n\n\n\n\n$utilities: map-merge(\n  $utilities,\n  (\n    \"width\": map-merge(\n      map-get($utilities, \"width\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"width\"), \"values\"),\n          (90: 90%),\n        ),\n      ),\n    ),\n    \"max-width\": map-merge(\n      map-get($utilities, \"max-width\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"max-width\"), \"values\"),\n          (600: 680px),\n        ),\n      ),\n    ),\n    \"font-family\": map-merge(\n      map-get($utilities, \"font-family\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"font-family\"), \"values\"),\n          (\n            montserrat: $font-montserrat,\n            fraunces: $font-fraunces\n          ),\n        ),\n      ),\n    ),\n    \"letter-spacing\": (\n      property: letter-spacing,\n      class: lt,\n      responsive: true,\n      values: (\n        0: 0px,\n        5: 5px,\n        10: 10px\n      )\n    )\n  )\n);\n\n\n// Utilities\n@import \"../node_modules/bootstrap/scss/utilities/api\";\n\n\n.max-w {\n  max-width: 300px;\n}</code></p><p>&nbsp;</p><p>然后，我在HTML文件中使用它，如下所示：</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p class=\"text-uppercase text-storm-gray font-montserrat fw-normal lt-5\"><code lang=\"null\">Perfume</code></p><p></p><p>&nbsp;</p><p></p><h2>如何添加letter-spacing属性</h2><p></p><p>&nbsp;</p><p></p><h4>Tailwind</h4><p></p><p>&nbsp;</p><p>用下面的代码导入Tailwind：</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p>&nbsp;</p><p>使用自定义的Tailwind类，如tracking-[.5em]。</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p class=\"font-montserrat font-medium text-xs tracking-[.5em] uppercase text-stormGray mb-3\"><code lang=\"null\">Perfume</code></p><p></p><p>&nbsp;</p><p>就是这么简单！</p><p>&nbsp;</p><p></p><h4>Bootstrap</h4><p></p><p>&nbsp;</p><p>与上面一样，我通过Npm安装Bootstrap和Sass。</p><p>&nbsp;</p><p>然后，我修改了Utility。请看第34行。我创建了一个新的Utility，它使用了lt类，有三个值。</p><p>&nbsp;</p><p><code lang=\"null\">$utilities: map-merge(\n  $utilities,\n  (\n    \"width\": map-merge(\n      map-get($utilities, \"width\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"width\"), \"values\"),\n          (90: 90%),\n        ),\n      ),\n    ),\n    \"max-width\": map-merge(\n      map-get($utilities, \"max-width\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"max-width\"), \"values\"),\n          (600: 680px),\n        ),\n      ),\n    ),\n    \"font-family\": map-merge(\n      map-get($utilities, \"font-family\"),\n      (\n        values: map-merge(\n          map-get(map-get($utilities, \"font-family\"), \"values\"),\n          (\n            montserrat: $font-montserrat,\n            fraunces: $font-fraunces\n          ),\n        ),\n      ),\n    ),\n    \"letter-spacing\": (\n      property: letter-spacing,\n      class: lt,\n      responsive: true,\n      values: (\n        0: 0px,\n        5: 5px,\n        10: 10px\n      )\n    )\n  )\n);</code></p><p>&nbsp;</p><p>最后是使用它：</p><p>&nbsp;</p><p><code lang=\"null\"></code></p><p class=\"text-uppercase text-storm-gray font-montserrat fw-normal lt-5\"><code lang=\"null\">Perfume</code></p><p></p><p>&nbsp;</p><p>这是一篇很长的文章，比我刚开始写的时候预期的要长。在文章结束之前，我想告诉你们一些关于Bootstrap的事情。Bootstrap 3和Bootstrap 4会好一些，但Bootstrap 5不是很好，特别是在文档和开发者体验方面。</p><p>&nbsp;</p><p>我感觉Bootstrap正在衰落。它还没有消亡，但核心开发团队似乎迷失了愿景，不知道要去到哪里。也许我是错的，但毕竟Bootstrap在React和全域SPA之前就已经出现了。</p><p>&nbsp;</p><p>它在过去、现在都可以与全栈式框架结合使用，但不包括React，至少在我看来是这样。也许在几年后，我们会重新回归，但在今天，我似乎很难向初级或高级开发人员推荐Bootstrap。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p><a href=\"https://betterprogramming.pub/the-rise-and-fall-of-bootstrap-68d4cd703666\">https://betterprogramming.pub/the-rise-and-fall-of-bootstrap-68d4cd703666</a>\"</p>",
    "publish_time": "2022-09-23 14:21:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "用于VS Code的Edge开发工具扩展太棒了，因此我放弃了 Chrome",
    "url": "https://www.infoq.cn/article/XTw4I4heyyHZSWHNoh7E",
    "summary": "<p>我已经使用Visual Studio Code好几年了，一直用得很是愉快，但是<a href=\"https://marketplace.visualstudio.com/items?itemName=ms-edgedevtools.vscode-edge-devtools\">Microsoft Edge Tools的VS Code扩展</a>\"将Web开发工具带到了另一个层次。这个扩展足以令人做出了改变，我让Edge取代Chrome成了日常用于网页开发工作的浏览器。VS Code的早期版本并没有对运行和调试应用程序提供任何开箱即用的支持；它是一个可扩展的代码编辑器，而不是集成开发环境。微软和社区开发的扩展添加了用户所需的功能和各种语言/框架支持。多年来，我一直将针对<a href=\"https://github.com/microsoft/vscode-chrome-debug\">Chrome的VS Code Debugger</a>\"扩展(现在已经弃用了)与Angular和React一起使用来做开发，虽然它用起来也还行，但也有一些问题使它算不得真正优秀的用户体验。经过多年的发展，VS Code包含了越来越多对JavaScript和浏览器调试的内置支持，这改善了体验，并进一步推动了Web开发者采用这一多功能编辑器。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/cb/4b/cb6e52b690f0b24c3848845cb073144b.png\" /></p><p></p><p>不久前，我发现了针对Visual Studio Code的Microsoft Edge Tools。这个扩展更紧密地集成了Microsoft的Edge浏览器和VS Code，提供了它们两个之间更加无缝的开发和调试体验，并进一步模糊了功能强大的代码编辑器和IDE之间的界限。如果你习惯了Chrome的开发工具，Edge的工具几乎是一样的。Edge甚至还有一些额外的功能，如流行的浅色和深色主题，包括Solarized、Monokai等。虽然我在安装这个扩展之前已经是Edge的一个用户，但大多数Chrome用户会发现Edge用起来很熟悉很直观，不会对他们的正常工作流程产生干扰。因为基于Chrome的浏览器都使用<a href=\"https://en.wikipedia.org/wiki/Blink_(browser_engine)\">Blink</a>\"浏览器引擎，所以你可以确信JavaScript的支持是相同的，页面内容在Edge中的呈现与在Chrome中的呈现是相同的。如果你还没有安装Edge和VS Code，可以访问他们的主页获取你的操作系统对应的最新版本：</p><p></p><p><a href=\"https://www.microsoft.com/en-us/edge\">microsoft.com/en-us/edge</a>\"<a href=\"https://code.visualstudio.com/download\">code.visualstudio.com/download</a>\"</p><p></p><p>注意：如果你使用的是Linux，请安装microsoft针对Edge和VS Code提供的.deb包、.rpm包或tar.gz，而不是Flatpak或Snap版本。对于开发人员来说，这些往往更难处理，因为权限问题让获取扩展、附加调试器和访问文件都很头疼。</p><p>&nbsp;</p><p>下面介绍我如何在我的机器上针对一般的Web开发配置VS Code和Edge工具扩展。我用<a href=\"https://pop.system76.com/\">Pop!_OS</a>\" 22.04，但这也适用于Ubuntu和其他基于Debian的发行版。我也在Windows上使用了类似的配置，但我没有在macOS上测试过。打开VS Code，在Extensions下搜索“Microsoft Edge Tools for VS Code”并安装它：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/45/a1/451de23a74fb7d6b8f34693274cd9fa1.png\" /></p><p></p><p>这个扩展最酷的功能之一是能够在VS Code中完全调试静态内容，零配置，没有其他依赖。</p><p>为了验证这一点，我使用Vue.js制作了一个小应用程序，并通过右键单击根文件“index.html”并选择“Open with Edge”来启动Edge工具扩展的调试会话。我可以设置断点，单步调试代码，查看网络流量，inspect 数据和页面元素，修改CSS等等等等，所有这些在VS Code中都有！</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/11/99/1129b950d643334903077a208d0de099.png\" /></p><p></p><p>这是一个很好的特性——一个好的使用场景可能是从StackBlitz、CodePen等里面取出一些可运行的示例代码，并在本地机器上运行/调试它，在集成到一个更大的项目之前对它的代码片段进行试验。然而，大多数开发人员都想要一个更加可配置的设置，因为这种快速调试配置非常有限。我在使用一个特定的项目时立即遇到了一个问题，在本地运行应用程序时我无法配置一个第三方认证服务(我把<a href=\"https://stackoverflow.com/questions/73196704/edge-dev-tools-for-vs-code-gives-err-unsafe-redirect-error-during-authenticati\">这个问题发布到了StackOverflow上</a>\"，有兴趣的话可以看一下)。Edge Tools扩展使用了一个带有“<a href=\"https://en.wikipedia.org/wiki/File_URI_scheme\">file://</a>\"”模式的url，而认证服务配置不允许将这样的url设置为回调。我需要有一个有模有样运行着的本地主机Web服务器，以便我的应用程序会有一个像“<a href=\"http://localhost:port\">http://localhost:port</a>\"”的url，这才是被允许的认证服务。为了做到这一点，我安装了<a href=\"https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer\">VS Code的Live Server扩展</a>\"。这是一个轻量级的、易于使用的本地开发Web服务器，也可以在VS Code的Extensions中找到它并进行安装：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/45/a5/45b1ae5b10c7ec02c67bf7d65e6539a5.png\" /></p><p></p><p>然而，Live Server扩展并不是Edge Tools所必需的。如果你正在使用Angular、React、Vue.js等框架，你可能正在使用Node.js和/或各种CLI工具作为你的开发栈的一部分来构建和服务应用程序。VS Code内置的Node.js支持和Edge tools扩展能很好地集成以这种方式配置的项目，下面的例子仍然适用于不使用Live Server的项目。</p><p>&nbsp;</p><p>为了进一步配置调试设置，建议在VS Code的<a href=\"https://code.visualstudio.com/docs/editor/debugging#_launchjson-attributes\">“launch.json”文件</a>\"中添加一些调试配置。这允许对任意数量的所需配置进行简单的一键调试，并且在与团队成员共享运行和调试配置时特别有用。如果“launch.json”文件不存在，Edge 工具扩展通过自动添加它们，创建一个“launch.json”文件，使这变得很容易。点击左边面板上“Extension”的图标，然后点击“Generate launch.json”按钮：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/61/6a/61338d9fcbb3fe68ea53104518cac06a.png\" /></p><p></p><p>如果你的项目中已经有了“launch.json”文件，点击“Configure launch.json”按钮将额外的配置添加到此文件。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/bd/e6/bd348123dd48f54c9d46cd30dce72ae6.png\" /></p><p></p><p>这将更新“launch.json”文件并将几个选项添加到”Run and Debug”菜单。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0a/3f/0a4c4433012d69cdee432e1c0a67ef3f.png\" /></p><p></p><p>“launch.json”还不十分完整——必须指定项目的url。我还设置了一个附加的Edge运行时参数，并设置了一些项目文件夹，这些文件夹在调试过程中应该使用“skipFiles”属性忽略：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/06/28/063856540a17d87234dc3626e89a5d28.png\" /></p><p></p><p>在VS Code中打开浏览器窗口、浏览器开发工具和代码编辑器是一个很酷的功能，但我需要更大的屏幕空间来运行和调试应用。幸运的是，Edge tools扩展也可以用于独立运行的Edge浏览器实例。我发现这让我拥有了使用浏览器及其内置开发工具和代码编辑环境的“传统”网页开发的所有优势，但由于Edge和VS Code之间的紧密集成，体验还得到了增强。如果希望如此，请打开Edge Tools扩展设置，并确保选中“<a href=\"https://docs.microsoft.com/en-us/microsoft-edge/visual-studio-code/microsoft-edge-devtools-extension#opening-an-external-browser-window-headless-mode\">Headless</a>\"”模式:</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a6/yy/a65da3ca409837c628907d074e1b8fyy.png\" /></p><p></p><p>通过右键单击根目录“index.html”&gt;Open with Live Server，或者点击VS Code的状态栏中的“Go Live”来启动Live服务器。如果你正在使用另一个工具或框架来构建和服务你的应用，请像往常一样启动它(例如：ng serve)。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ea/e7/ea84d1790f21cfdebafc438cf38155e7.png\" /></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/64/c4/648c534028cc5d2cd69e4337dd8c2ec4.png\" /></p><p></p><p>选择debug 菜单中的 “Launch Edge and attach DevTools” 调试配置，启动它：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/82/ba/828fbfdced77770cfd3c39bc356d5fba.png\" /></p><p></p><p>将会在启动一个附加VS Code调试器的Edge实例，并且仍然可以使用所有的调试功能。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e6/4e/e6f4431a64b65732e217561e0f8e514e.png\" /></p><p></p><p>没有必要在VS Code中打开Edge Dev Tools选项卡——关闭它不会影响当前的调试会话。它可以通过调试工具栏菜单&gt;“Open Browser Devtools”重新打开。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/0f/79/0f3b1d7bbf23dda1c2ec77abccb46379.png\" /></p><p></p><p>我个人喜欢在VS Code中集成Edge开发工具窗口，我经常在我的开发工作流程中使用这两种工具。我发现Edge控制台可以作为内置调试控制台的一个很好的补充。在调试时，我经常交替使用不同的窗口配置。有时我喜欢像这样使用内置控制台：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/dd/a8/dd1d51f2cbce139e1ddbe5709d38e1a8.png\" /></p><p></p><p>有时我使用这样的窗口配置，显示Edge Dev Tools的“Network”选项卡。在下面的例子中，我可以在我的应用程序中检查流媒体服务的当前Web套接字连接，并在VS Code中查看单独的消息：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/2c/3a/2c8d723fed1c117b922cdb02ed6e683a.png\" /></p><p></p><p>为了获得更大的灵活性，还可以从正在运行的Edge实例中打开开发工具并像往常一样使用。事实上，VS Code中集成的Edge开发工具窗口实际上是常规浏览器开发工具的屏幕截图。我使用的另一个典型配置是这样的，我只用VS Code打开各种代码文件，通过外部Edge开发工具进行大部分调试。在这个配置中，一切仍然按照预期工作；在VS Code或浏览器开发工具窗口中可以设置断点，但如果需要，我可以只使用浏览器进行所有测试和调试。下图是我在Windows的机器上的配置——Edge和VS Code在不同平台上的UI几乎是一样的，这对于使用多种操作系统的开发者来说可是太棒了：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/dd/1c/dd0355b9c9b56e2d45f6aaf02499e41c.png\" /></p><p></p><p>另一个不可忽视的特性是Edge Tools扩展提供的内联代码分析。我发现这是对ESLint的一个很好的补充。这个功能完全集成到“问题”标签；下面展示了该扩展如何显示错误和问题：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/25/f6/2584e1eb6138bc2d7c8b19c23550cff6.png\" /></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8c/06/8cfb3f590d5fcf8ae6fb1e6ecf19c706.png\" /></p><p></p><p>对于尽可能有效地使用屏幕空间，我的最后一个建议是停靠调试工具栏。我发现当设置为“浮动”时，真的没有一个好地方放它。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ba/c6/baf2f653432b0ba73a2bcb44142855c6.png\" /></p><p></p><p>点击File &gt; Settings &gt; Features &gt; Debug &gt; Tool Bar Location打开VS Code 设置，设置为“docked”。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/4b/b9/4b9cf186b7731f1f900f9990f6a922b9.png\" /></p><p></p><p>通过这个设置，调试工具栏停靠在左边的面板上，只有在VS Code中选择“Run and Debug”时才可见。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b7/1e/b7a1575d4a324cf4d39eccf24d786d1e.png\" /></p><p></p><p>我还安装了<a href=\"https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-statusbar-debugger\">StatusBar Debugger extension</a>\"，顾名思义，它将调试控件添加到VS Code状态栏。我喜欢在VS Code中的“Run and Debug”面板不活跃时，在UI上补充不会形成干扰的调试控件，算是给调试快捷键点个赞吧。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/d6/91/d60fe1756cf5a8fbbecf4030bebed391.png\" /></p><p></p><p>从使用情况来看，VS Code扩展的Edge Tools已被证明非常通用，我已经改为使用它作为我日常工作的主要Web开发环境。我发现的唯一的主要缺陷是无法从VS Code的Edge开发工具选项卡访问<a href=\"https://microsoftedge.microsoft.com/addons/detail/vuejs-devtools/olofadcdnkkjdfgjcmjaadnlehnnihnl\">Vue.js Devtools Edge扩展</a>\"。我能想象得到，其他类似的扩展（比如<a href=\"https://microsoftedge.microsoft.com/addons/detail/react-developer-tools/gpphkfbcpidddadnkolkpfckpihlkkil\">React Developer Tools</a>\"）也存在这样的情况，虽然我没有实际去验证过这一点。但是我发现这并没有给我带来多大的不便，因为在标准的Edge开发工具窗口中Vue.js扩展仍然是完全可用的。尽管存在这个问题，我仍然强烈建议所有网页开发人员都尝试一下。</p><p>&nbsp;</p><p>下面是一些关于配置和使用Microsoft Edge Tools VS Code扩展材料链接:</p><p>&nbsp;</p><p><a href=\"https://marketplace.visualstudio.com/items?itemName=ms-edgedevtools.vscode-edge-devtools\">VS Code Marketplace: Microsoft Edge Dev Tools VS Code Extension</a>\"<a href=\"https://docs.microsoft.com/en-us/microsoft-edge/visual-studio-code/microsoft-edge-devtools-extension\">Microsoft Docs: Microsoft Edge DevTools Extension documentation</a>\"<a href=\"https://github.com/microsoft/vscode-edge-devtools\">GitHub: Microsoft Edge Dev Tools for VS Code repository</a>\"<a href=\"https://medium.com/young-coder/microsoft-edge-deepens-its-integration-with-vs-code-f96767666aae\">Medium: Microsoft Edge Deepens Its Integration with VS Code</a>\"</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p><a href=\"https://mfcallahan.blog/2022/08/26/the-microsoft-edge-dev-tools-extension-for-vs-code-is-so-awesome-that-im-ditching-chrome-for-web-development\">The Microsoft Edge Dev Tools extension for VS Code is so awesome that I’m ditching Chrome for web&nbsp;development</a>\"</p><p>&nbsp;</p><p>译者介绍：</p><p>&nbsp;</p><p>冬雨，小小技术宅一枚，现从事研发过程改进及质量改进方面的工作，关注研发、测试、软件工程、敏捷、DevOps、云计算、人工智能等领域，非常乐意将国外新鲜的IT资讯和深度技术文章翻译分享给大家，已翻译出版《深入敏捷测试》、《持续交付实战》。</p>",
    "publish_time": "2022-09-23 14:57:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "算力新生态，透视异构计算的机会和挑战",
    "url": "https://www.infoq.cn/article/ewPi5CH4SYcejee1JSc7",
    "summary": "<p>算力助推经济增长，成为数字经济发展新引擎。今年 4 月，由 IDC、浪潮信息和清华大学全球产业研究院联合推出的《2021—2022 全球计算力指数评估报告》显示，计算力指数平均每提高 1 点，数字经济和 GDP 将分别增长 3.5‰和 1.8‰。中国信通院发布的《中国算力发展指数白皮书》表明，在算力中每投入 1 元，将带动 3-4 元经济产出。算力发展指数每提高 1 点，GDP 增长约 1293 亿元。</p><p></p><p>虽然算力变得愈加重要，但是其发展却面临供需矛盾问题。一方面，对算力的需求增长迅猛。无论是企业数字化转型，还是智能终端消费和移动数据流量消费规模的不断扩大，都在持续释放算力需求；另一方面，传统的单一计算架构面临性能和功耗瓶颈，无法满足日益高涨的算力需求。简言之，算力遇到瓶颈，并且成为摆在企业和行业面前的难题。</p><p></p><h2>异构计算 脱颖而出</h2><p></p><p></p><p>如何解决算力瓶颈问题，业界提出了各种解决思路和方法，其中，异构计算从中逐渐脱颖而出，备受企业和行业期待。</p><p></p><p><a href=\"https://www.infoq.cn/article/YMLmY5xt8zz0qwfGQ8sd\">异构计算</a>\"（Heterogeneous Computing），主要指不同类型的指令集和体系架构的计算单元组成的系统的计算方式，在云数据中心、边缘计算场景等有着广泛应用。</p><p></p><p>它的兴起从表面上看与功耗瓶颈有关，但从更深层看，则与工作负载密切相关。虽然通用 CPU 拥有广泛应用，但是经过 30 多年的发展，通过提升 CPU 时钟频率和内核数量来提高计算能力的传统方式遇到散热和能耗瓶颈。</p><p></p><p>更重要的是，自 2020 年以来，受疫情影响，远程办公、线上学习、居家娱乐等应用进一步发展，刺激了对大数据、云计算、人工智能等技术的多样化需求，各行各业数字化转型加速。高性能计算、云计算和虚拟化、大数据分析等一系列的应用场景都会带来非常复杂的工作负载，这背后需要强大的算力支持。</p><p></p><p>英特尔行业解决方案事业部互联网行业技术总监高明表示，异构计算是受工作负载驱动的。在数据量越来越大的今天，人们需要采用多种异构计算单元加速数据处理，来获得更高的吞吐、更低的延时，并付出更低的成本。</p><p></p><p>相比传统的单一计算架构，异构计算不仅可以提高算力和性能，降低功耗和成本，而且还具备多类型任务的处理能力，发展潜力巨大。具体而言，异构计算能充分发挥 CPU/GPU 在通用计算上的灵活性，及时响应数据处理需求，搭配上 FPGA/ASIC 等特殊能力，来充分发挥协处理器的效能，根据特定需求合理地分配计算资源。并且，由于目前神经网络算法和与之对应的计算架构层出不穷，如果采用不断更新 ASIC 架构的方式，最终下沉到用户和企业身上，就会导致使用成本和替换成本过高，而异构计算成本更低，在产业落地上有更大优势。</p><p></p><p>结合异构计算的优势，高明总结了六大应用场景：第一类，HPC 高性能计算场景，包括汽车和航空航天建模仿真、电子自动化设计与验证、生命科学等；第二类是人工智能场景，不管是深度学习训练，还是深度学习推理，都需进行大量矩阵运算，尤其是大规模互联网应用场景，比如推荐、广告、搜索等；第三类是物联网与边缘计算场景，由于海量数据要在边缘或云端进行处理，其中在线推理任务需要大量的边缘和云端算力进行加速；第四类是 5G 和通信场景。虽然部分网络功能以软件 NFV 形式运行于 CPU 上，但是仍然有部分算法需要异构加速器（如 FPGA 或 ASIC）进行加速；第五类是多媒体处理和云游戏场景。在高清视频转码、视频图像渲染、图像超分辨率等场景，为获得高吞吐和低延时，异构算力不可或缺；最后一类是云计算，为了让云计算平台可以提供更高的性能、更低的成本，以及满足对基础设施管理的需求，云计算企业逐渐在云中部署更多的异构加速器来加速计算、网络和存储。</p><p></p><p>为推进异构计算的落地，业界出现了 CPU+GPU、CPU+FPGA 和<a href=\"https://www.infoq.cn/article/Ni5RxDomW4KVyNwSaJWK\"> SVMS 架构</a>\"等。CPU+GPU 架构是充分使用 GPU 和 CPU 两者的计算能力，有效提高计算处理性能，降低处理能耗。CPU+FPGA 架构则利用了 FPGA 灵活的可编程性、按需定制和大规模并发延迟低的特点，它在提高 CPU 计算能力的同时，降低了服务器功耗。SVMS 架构则是由英特尔提出的，该公司在 2018 年提出<a href=\"https://www.infoq.cn/article/TwX9PMgOZrc5IWIaXGXz\"> XPU 愿景</a>\"：使用多种计算架构充分满足复杂计算需求。具体来说，是由标量 (Scalar)、矢量 (Vector)、矩阵 (Matrix)、空间 (Spatial) 组成 SVMS 架构，可以进行多种异构处理器组合，从而实现高性能处理多种负载。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/29/2925c7d15686a79131f29d67bfab8764.png\" /></p><p></p><p></p><h2>提升性能，降本增效，快手如何落地异构计算</h2><p></p><p></p><p>无论是 CPU+GPU，还是 CPU+FPGA，异构计算只有在实际业务场景中得到落地，才能体现其真正的价值。作为一个超 3 亿日活的短视频 App，快手的异构计算实践颇具代表性。</p><p></p><p>据了解，快手的推荐系统在大规模复杂业务中面临巨大性能挑战。作为短视频内容平台，内容生产、内容理解、内容分发、内容消费、用户互动这些环节，构成了大规模的复杂业务，对算力产生更多元的需求。为破解算力瓶颈难题，快手推出了可实现异构计算的 LaoFe NDP（Latency oriented Fpga engine for Near Data Processing）架构，加速不同场景的计算，并在英特尔硬件上得到最优的性能执行。</p><p></p><p>以推荐业务场景为例，它需要根据用户画像推荐用户感兴趣的内容。首先，从海量信息中选择与用户特征相关的结果，再通过“排序”划分内容的优先级别。如何保证任务在这个过程中高效、准确地完成？参数服务器至关重要，因为它负责存储、处理海量数据特征以及排序模型参数。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b5/b51dae32d8c89693944a4c0fd63eef57.png\" /></p><p>快手推荐系统采用计算与存储分离的架构模式</p><p></p><p>为应对海量数据冲击，快手的推荐系统采用计算与存储分离的架构模式。参数服务器属于存储型服务，该服务要保存和实时更新上亿规模的用户画像、数十亿规模的短视频特征、以及千亿规模的排序模型参数。并且，它不仅受限于容量和带宽，而且还要支撑每秒数亿次的 KV 请求，这会消耗大量 CPU 资源，成为其性能的主要瓶颈。</p><p></p><p>要解决这个问题，最佳方案是采用异构计算，使用不同计算设备处理不同负载。通过使用英特尔® 至强® 可扩展处理器、英特尔® Agilex™ FPGA 和英特尔® 傲腾™ 持久内存，借助软硬一体化、领域专用加速器设计，使快手的 LaoFe NDP 近数据架构在计算体系结构上实现创新，从而做到网络、存储、计算三重加速，为各个业务系统提供低延迟、高并发、高吞吐、低总体拥有成本的基础资源。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/47/4754c7450622302a7a084b40a4b890e5.png\" /></p><p>快手 LaoFe NDP 异构计算架构</p><p></p><p>在笔者看来，网络、存储和计算的三重加速真正体现出异构计算带来的价值。</p><p></p><p>在网络层面，LaoFe NDP 架构将 CPU 收发网络数据操作，卸载到 FPGA 上。Client 发送的请求包直接发送给 FPGA。相比 gRPC 基于 TCP/IP（网络协议栈），功能过于复杂，性能和延时方案无法保证。而使用基于 FPGA 实现了一套 SD-RDMA 协议，通过应用层添加字段的方式，保证了类似 gRPC 的可靠性传输，大幅降低了请求延时。</p><p></p><p>在存储层面，LaoFe NDP 架构将 CPU 存储操作也卸载到 FPGA 上。为了最大程度发挥 FPGA 的能力，快手基于通用 KV 存储场景定制了一套易于 FPGA 访问的 KV（Key-Value）引擎。同时，其支持 SSD/ 英特尔® 傲腾™ 持久内存 /DRAM 内存、基于 hash 的 Key-Value 存储引擎，能够有效加速存储性能。通过实战检验，使用 KV 查表的吞吐相比 CPU 方案提升 5 倍以上。</p><p></p><p>在计算方面，LaoFe NDP 的计算加速仰仗 FPGA 作为领域专用处理，可以更有效地并行处理数据，提供更高效的内存层次结构与定制化的执行单元，从而支持机器学习、深度学习和大数据等场景。英特尔® FPGA 具备富于弹性的可编程硬件能力，延时低且可精确控制，单位算力功耗低、片上内存大，适合于快手延时要求高、批处理比较小、并发性和重复性强的应用场景。</p><p></p><p>快手 LaoFe NDP 架构在英特尔软硬件优化下，最终实现了：一、系统吞吐显著提升，延时显著降低，参数服务器的吞吐性能提升了 5-6 倍，整体请求延时降低了 70%-80%，提供更好的用户体验；二、更好地控制 TCO，FPGA 的强大性能提供远超传统方案的吞吐能力，仅需部署少量的服务器就能满足特性的性能指标要求，替代比可达到 1：5，有效降低 TCO；三、降低性能抖动，基于 CPU 的软件方案常因需要进行高频率更新而出现性能抖动，而通过 FPGA 来处理负载，能大幅减少性能抖动。</p><p></p><h2>异构计算的困境</h2><p></p><p></p><p>通过快手的异构计算实践可以发现，异构计算在未来有很大的发展潜力和空间。不过，企业在采用异构计算前，还需认识到异构计算所存在的技术难题：</p><p></p><p>一是异构计算产品需要面对不同的系统架构、指令集和编程模型，需要降低多样计算带给软件开发者的难度；二是异构计算芯片产品除了要在芯片设计层面实现突破之外，还需要解决在芯片制造和封装过程中不同结构之间的适配和升级问题；三是异构计算要实现性能的多样性合一，使其同时满足人工智能训练、推理、图像视频处理等各种不同的需求。</p><p></p><p>尤其是异构计算带来的硬件复杂性，对编程人员提出了严苛挑战。不同开发框架之间的性能表现、兼容性，以及学习成本一直是影响开发效率的主要因素之一，复杂的开发环境、无法同步更新的框架导致开发者要花费不少精力去自行解决问题。这些都依赖于生态链的建设。标准的制定与推广，语言、编译器、框架、运行库等的支持，都不是易事。</p><p></p><p>虽然这事不简单，但是厂商已有所行动，推出各种解决方案，其中，<a href=\"https://www.infoq.cn/article/GGPuU7v0EcQC6I1Zty2T\">英特尔的 oneAPI </a>\"值得一提。作为统一的软件编程架构，oneAPI 支持多种异构计算单元，不仅有英特尔硬件，而且还包括其他厂商的硬件。同时，它提供开放、统一的编程语言 DPC++。并且，oneAPI 还提供基于 API 的高性能库，能在多种异构平台上运行并提供极高的性能，其中很多库将开源，为进一步扩展增加新功能提供可能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/47/4787c181fdbe767a882baf0715b3cbc3.png\" /></p><p></p><p>如今，oneAPI 正在被越来越多的独立软件提供商、操作系统供应商、终端用户和学术界采用，它提供的跨架构的兼容性，也大大地提高了开发人员的生产力和创新能力。</p><p></p><h2>写在最后</h2><p></p><p></p><p>以前，是以计算为中心，指令控制流驱动计算；未来，将以数据为中心，数据流驱动计算。在进入以数据为中心的时代后，CPU、GPU、FPGA 等不再像以往一样可以独当一面，传统的通用架构已远不能满足当下的需求。唯有多种架构之间的组合方能应对处理海量、密集数据的工作负载需求。</p><p></p><p>如今，行业正转向以异构为基础的技术新生态，异构计算成为新的全球竞争点。纵观业界，主流芯片供应商都在大力布局异构计算，试图构建更完整的生态。</p><p></p><p>异构计算将越来越精细地拆分特性不同、要求不同的工作负载，然后逐渐统一化、标准化。未来，异构计算会根据不同的场景、数据种类和处理的延时以及带宽要求进行设计。在这个新的发展趋势下，除 CPU 和 GPU 外，将有更多种类的“PU”出现。英特尔的 XPU 战略在这样的背景和趋势下优势愈发显著。其不断完善的产品线横跨 CPU、GPU、FPGA 和 IPU 等领域，且秉持“软件优先”理念，通过 oneAPI 面向异构计算提供统一的、可扩展的编程模型，软硬并进。此外，在全新的 IDM 2.0 战略引领下，英特尔正在架构和制程方面加速迭代演进，携手合作伙伴，更好地应对未来海量、多变的异构计算需求。</p><p></p><p>想了解更多关于异构计算的知识？扫描下方海报二维码，观看英特尔联合国际学术期刊《科学》共同推出的“架构师成长计划”第六期《异构计算 数据中心“芯”变革》精彩回放，了解更多技术干货。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/119e547cbda62601856ee39880bd14cf.jpeg\" /></p><p></p>",
    "publish_time": "2022-09-23 15:04:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Serverless 遇到 FinOps: Economical Serverless",
    "url": "https://www.infoq.cn/article/ckIbtIoFXN4wHycfJT8b",
    "summary": "<p></p><blockquote>Key Takeaways:1. 尽管 Serverless 的迅猛发展吸引了广泛深入的关注，Serverless 函数总成本的事先估计仍缺乏有效的理论指导。本文基于 FunctionGraph 在 Serverless 领域的 FinOps 探索和实践，提出业界首个 Serverless 函数总成本估计模型；2. 根据对成本模型的关键因素分析，提出五大类函数运行成本的优化方法；同时，为更好地帮助用户实现降本增效，华为云首次提出透明、高效、一键式的 “用户函数成本研究中心”。</blockquote><p></p><p></p><p></p><h2>问题引言</h2><p></p><p></p><p>Serverless 精确到毫秒级的按用付费模式使得用户不再需要为资源的空闲时间付费。然而，对于给定的某个应用函数，由于影响其计费成本的因素并不唯一，使得用户对函数运行期间的总计费进行精确的事先估计变成了一项困难的工作。</p><p></p><p>以传统云资源的周期性租赁模式为例，通过周期数乘以周期单价，用户可以很容易地估计出租赁期间的总费用，形成清晰的心理账户预期，即使在云平台采用阶梯定价或价格歧视策略的情形下，计算租赁总成本也不是一件难事。</p><p></p><p>但在 Serverless 场景中，事先估计函数总成本仍缺乏有效的理论指导。一方面，影响函数计费的关键因素不唯一，如包括函数内存规格、单实例并发度、函数执行时长等；另一方面，函数调用流量的波动通常具有随机性和非平稳性，使得基于流量的“按用计费”具有较大的不确定性。</p><p></p><p>当然，寻找函数计费的理论指导主要是为用户评估函数总成本提供一种有效依据，但更加重要地，如何进一步利用估计模型，帮助用户优化应用函数及其配置选择，进而显著降低用户函数总成本，是 Serverless 领域中，FinOps 亟待回答的问题。</p><p></p><p>FinOps 聚焦云上资源管理和成本优化，通过有机链接技术、业务、和财务专业人士，来优化用户、企业、组织的云资源成本，提高云上业务的投入 - 产出比[1]。本文结合华为云 FunctionGraph 在 Serverless 领域的 FinOps 探索和实践，剖析 Serverless 场景下的函数计费模式和关键影响因素，介绍一种对函数运行期间总计费进行事先估计的模型框架 ; 更重要地，该模型为帮助用户优化函数运行总成本、提升用户云上 Serverless 资源管理效能，实现经济型 (Economical) Serverless 提供有效依据。</p><p></p><h2>一、名词解释与背景知识</h2><p></p><p></p><p>首先对表 1 所列的几个概念做简要说明。</p><p></p><p>表 1：Serverless 函数常见名词</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c4/c41f935d8f4cc18c833b43ea1f1b39ae.png\" /></p><p></p><p>内存规格 (Memory)：内存规格也即函数规格、函数实例规格，表示 Serverless 平台为函数的单个实例所分配的资源大小，一般表示为函数可使用的内存大小，由用户指定；实例可使用的 CPU 份额与内存大小成正比。Serverless 云平台通常提供多种规格供用户选择，以 FunctionGraph 为例，用户可选 15 种函数规格，如图 1 所示。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f8/f8a6d9ad92cfb6b4fc1485a0d4f2891a.png\" /></p><p></p><p>图 1：FunctionGraph 提供多种函数内存规格</p><p></p><p>函数执行时延 (Function Execution Time): 这里指完成一次调用请求响应的过程中，函数本身执行所消耗的时间，主要由函数代码逻辑决定。一般地，对于 CPU 密集型的函数，增大函数资源规格（内存 -CPU Share），可以显著降低函数执行时延。但对于消耗大部分时间在网络 IO 等操作上的函数，增大资源规格对执行时延的改善则非常有限。</p><p></p><p>单实例最大并发度 (Maximum Requests per Instance)：函数的单个实例可以同时处理的最大请求数，主要适用于函数执行过程中有显著时间在等待下游服务返回的场景，如访问数据库操作或磁盘 IO 等。对于相同的流量负载，提高函数的单实例并发度可以降低按量实例个数，为用户节省计费，同时，也可以降低函数调用请求的冷启动比例。</p><p></p><p>单函数最大实例数 (Maximum Instances per Function)：指同一函数同一时刻下同时运行的实例数上限。对用户来说，最大实例数可以防止异常流量洪峰下或函数发生故障时由于云平台的过度扩容而导致的费用失控；对云平台来说，最大实例数可以防止异常情况下平台资源被部分函数耗光，从而保障不同函数间的性能隔离。</p><p></p><h2>二、函数计费与成本模型</h2><p></p><p></p><p>单实例视角下的函数计费估计模型，可参考[2]。在真实生产环境中，除异步函数外，Serverless 云平台通常采用 FCFS（First Come First Serve）的方式响应调用请求，对于函数流量的潮汐波动，平台通过自动扩缩容实例进行自适应，系统中运行的并发实例数随时间的变化，可以由一个分段常线性函数完全刻画，如图 2 所示。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d6/d614c910e56edabb017178e0aedfd1ca.png\" /></p><p></p><p>图 2：函数并发实例数随扩缩容过程的变化</p><p></p><p>尽管不同 Serverless 云厂商之间的计费方法存在差异，函数计费一般主要包括两部分：对函数所使用资源的计费以及对请求次数的计费，表示如下：</p><p></p><p></p><p></p><p>其中，表示对资源使用的计费，单位为 GB- 秒（GB-second），表示对调用次数的计费。</p><p></p><p>为方便计算 TotalCost，用表示函数的资源规格，单位为 GB, 例如，对于 128MB 规格的函数，其; c 表示该函数的单实例并发数，μ&nbsp;表示函数的平均执行时延，单位为毫秒 ; 并用 α&nbsp;(0&lt;α&lt;1)&nbsp;表示 Serverless 平台的调用链路性能，在最理想的情况下，该指标为 1，表示在当前 Serverless 平台上，该函数响应单个请求的端到端时延等于函数执行时延 μ&nbsp;本身，不同 Serverless 平台的 α&nbsp;值可能略有不同，但通常在 0.9 以上。给定上述指标，可以得到单实例在理想状况下的请求处理能力, 即理论上每秒可以响应的调用次数为：</p><p></p><p></p><p>因此，单实例的实际请求处理能力则为：</p><p></p><p></p><p></p><p>我们以一个月作为估计周期。假设一个月内，函数共经历了 n 次扩、缩容，形成了 n&nbsp;个常线性子区间（如图 2 所示）。先考察单个子区间内的计费成本模型，总成本模型则为各个连续子区间的加和。</p><p></p><p>在时间窗口内，假设函数调用次数为，则该时间窗内的并发实例数为：</p><p></p><p></p><p></p><p>对应的资源计费部分则可表示为：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8a/8ad3896e2d0030080f562b6abcdb4d2a.png\" /></p><p></p><p>其中，表示每 GB- 秒的资源的计费单价。现在，记第 i 个子区间为, 则一个月内的总成本模型可以估计为：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6e/6ea89e51d078d4127d42259c9a452d37.png\" /></p><p></p><p>其中，表示每次调用的计费单价，</p><p></p><p>表示函数该月总流量，为云平台提供的月度免费计量时间，为月度免费计量调用次数。</p><p></p><p>在上式中，单实例并发度 c 和函数规格可以认为在用户配置之后属于常数；α 属于平台侧参数，也可视作常数；对于函数执行时延&nbsp;μ，实际中通常会由于冷热启动差异、网络抖动、调用请求入参等的不同而波动，且考虑到 Serverless 计费是精确到毫秒级别的，因此严格意义上不能被视作为常数。 不过，作为估计模型，这里暂且假定 μ&nbsp;也为常数，综上，总成本模型可以表示为：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e7/e737d8a33cd458065f1f08d64a8d8e57.png\" /></p><p></p><p>后半部分代表云平台提供的免计费总量，与函数调用流量以及函数配置无关。</p><p></p><h2>三、成本优化方法讨论</h2><p></p><p></p><p>有了函数成本的估计模型，就可以对影响用户成本的关键因素进行讨论。在估计式 (1) 中，忽略云平台提供的免计费总量，函数月度总成本的结构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a6/a6fe347465ba679599ad879085adc0cc.png\" /></p><p></p><p>Point 1: 优化函数代码逻辑本身，降低函数执行时延</p><p></p><p>对于同样的函数流量负载，更低的执行时延 &nbsp;可以为用户节省更多计费成本。在用户业务逻辑允许的前提下，不断优化函数代码、提高函数执行效率是软件工程本身天然的诉求，但在 Serverless 场景下，这一点显得更为迫切。</p><p></p><p>具体地，考虑采用 Python、Nodejs 等轻量化编程语言，减少函数初始化配置中的非必要项，将连接其它服务如数据库等的操作尽量移到函数执行入口之前的初始化阶段完成，简化代码逻辑等。</p><p></p><p>另外，为帮助用户掌握函数运行情况，FunctionGraph 为应用函数提供深度可视化的可观测能力，支持丰富的观测指标配置，包括调用次数、错误次数、运行时延等，如图 3 所示的函数运行时间监控示例。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/22/229b3dc6cb04af52b2748804f2122503.png\" /></p><p></p><p>图 3: FunctionGraph 函数运行时间监控示例</p><p></p><p>Point 2: 优化函数代码包、依赖包、镜像大小</p><p></p><p>当函数调用触发冷启动的时候，从计费角度看，冷启动时延包含在执行时延 μ 中一起计费，而冷启动中有相当比例的时延消耗在云平台从第三方存储服务（如华为云对象存储服务 OBS）中下载用户的代码包、依赖包，或从镜像仓库服务中拉取用户应用镜像，如图 4 所示。尽管为了优化冷启动性能，目前大部分云平台均会采用各类缓存机制，对用户代码和镜像进行预缓存，但实例启动中消耗在用户代码加载上的时延仍然十分显著。因此，应尽可能优化函数代码包大小，包括对依赖包、镜像等进行瘦身，进而降低计费时长。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/11/11c8ea3de6e1fb1aeaf79d4f3b46cdd8.png\" /></p><p></p><p>图 4：冷热启动下的计费时长及优化点</p><p></p><p>Point 3: 编写功能聚焦的轻量化函数</p><p></p><p>在 Serverless 编程框架下，尽可能将函数编写为轻量型的、功能聚焦的程序代码，即“functions should be small and purpose-built”[3]；让“一个函数只做一件事”，一方面，功能单一的函数，运行时延也更容易针对性地进行优化；另一方面，当一个函数内同时实现多个功能的时候，大概率会以所有功能都在性能上同时做出妥协为结果，最终提高了函数运行期间总计费。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/04/041d1ed61a1ad1f27e29fb60d9a5ad23.png\" /></p><p></p><p>图 5：华为云 FunctionGraph 函数流示例</p><p></p><p>若应用函数的确需要提供多个功能，可以考虑将大函数分解为多个小函数，然后通过函数编排的方式实现整体逻辑, 如图 5 所示的 FunctionGraph 函数流功能。大函数分解也是 Serverless 计算中用户处理超时（timeout）等异常场景的最佳实践之一[4]。</p><p></p><p>Point 4: 业务模型支持的前提下，采用单实例多并发</p><p></p><p>从公式（2）的函数成本结构中可以看出，在用户业务模型支持的前提下，配置一定的单实例并发度 ，可以有效降低函数月度总成本；若用户不进行配置，云平台默认值通常为 1，即单个实例同一时刻只能处理一个请求；因此，在函数被并发调用的情形下，平台会启动多个实例进行响应，从而增大了计费实例数目，如图 6 所示；同时，采用单实例多并发，也能改善调用请求处于等待状态的尾时延。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/35/35e66dee105eebb44b585739aca79ce8.png\" /></p><p></p><p>图 6：单实例并发度：计费时长视角和实例数视角</p><p></p><p>当然，单实例并发度并非越高越好，例如，过高的并发度设置会使得函数实例内多线程之间的资源竞争加剧（e.g., CPU contention），导致函数响应性能恶化，影响用户应用的 QoS 指标等。同时，如本文在背景知识中所提，并非所有的应用函数都适合设置单实例多并发。单实例多并发主要适用于函数执行过程中有相当比例的时延消耗在等待下游服务返回的场景，这类场景下，实例资源如 CPU 等有显著比例处于空闲等待状态，如访问数据库、消息队列等中间件、或磁盘 IO、网络 IO 等。单实例多并发也需要用户在函数代码中对错误捕获（e.g., 考虑请求级别的错误捕获粒度）和全局共享变量的线程安全（e.g., 加锁保护）问题进行适配。</p><p></p><p>Point 5: 函数资源规格的选择需考虑对执行时延的影响</p><p></p><p>最后讨论函数资源规格的选择问题。从公式（2）明显可以看出，更大规格的实例内存 对应更高的计费成本。但内存规格的选择，需要同时考虑对函数执行时延 &nbsp;的影响。从用户函数的角度看，函数执行时延除了由代码本身的业务逻辑决定之外，还受实例运行时可使用资源大小的影响。更大的实例规格，对应更大的可使用内存和更多的 CPU 份额，从而可能显著改善高内存占用型或 CPU 密集型函数的执行性能，降低执行时延；当然，这种改善也存在上限，超过某个资源规格后，资源的增加对降低函数执行时延的效果几乎可以忽略，如图 7 中虚线所表示的过程。上述事实表明，对于给定的用户函数，为降低总计费成本，需要配置合理的实例规格，使得·μ&nbsp;尽可能取得最小值，如图 7 中实线所表示的过程。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f5/f5d871a9b44af1d2a7671e4c2bc8e1c7.png\" /></p><p></p><p>图 7：函数规格的选择需同时考虑对成本和执行时延的影响</p><p></p><p>例如，考虑实例规格的初始配置为（例如从最小规格开始，i.e., 128MB）, 经测试该规格下函数执行时延为，则可以得到基线，然后逐步增大资源规格，测试对应执行时延，直到某一组</p><p></p><p>出现，使得：</p><p></p><p>此时表明，资源增大对计费成本的边际提升已经超过了对执行时延的边际改善，因此，从成本的角度看，此时的为帕累托最优解，即最佳规格，对应执行时延为。</p><p></p><p>最后，图 8 对上述几个决定函数成本的关键因素做了一个总结，其中，箭头方向表示元素之间的直接影响，“+”号代表成正比，“-”代表成反比。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f8/f89649fd6c29d1a5b718ac1c72fd9a1f.png\" /></p><p></p><p>图 8：函数计费成本的关键因素分析</p><p></p><p></p><h2>四、Serverless 函数成本研究中心</h2><p></p><p></p><p>为用户降本增效，是 FunctionGraph 的核心理念。尽管前文分析的五种函数成本优化手段是站在用户视角下的讨论，但我们认为这些问题远不是只属于用户需要考虑的范围；相反地，FunctionGraph 在持续探索如何最大限度地帮助用户在 Serverless 领域实现最佳的 FinOps 效果，让用户能够真正享受到 Economical Serverless 的福利；例如，在实例级别的深度可视化、可观测性前提下，帮助用户实现函数 FinOps 全流程的自动化，为用户提供透明、高效、一键式的函数资源管理和成本优化服务。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/19/190efbfaa8161320725b453a09be395d.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/68/68bc4c64d24c15ab3c81b8adaa37d7e7.png\" /></p><p></p><p>图 9. 在线式资源消耗感知与规格动态推荐</p><p></p><p>为此，基于内部实践，FunctionGraph 将于近期推出“用户函数成本研究中心 – Cost Analysis and Optimization Center”, 为用户提供包括离线式函数最佳配置调优（offline power tuning）、在线式资源消耗感知与规格动态推荐（online resource recommendation， 如图 9 所示）、预测性函数弹性预览（predictive auto-scaling preview）等在内的多个重量级特性服务，最大限度降低用户实现函数 FinOps 的技术门槛，为用户业务开发、Serverless 化改造等提供极致便捷性。</p><p></p><h2>五、总结与展望</h2><p></p><p></p><p>本文主要讨论了 Serverless 计算场景下的 FinOps 问题，给出了业界首个用户函数总成本估计模型，并根据该模型，为用户优化应用函数、提升 Serverless 资源管理效能、降低总成本提供理论参考和实践依据。</p><p></p><p>一项新兴技术领域的兴起，首先需要回答的问题是“Why &amp; Value”, &nbsp;FunctionGraph 作为华为元戎加持的下一代 Serverless 函数计算与编排服务，结合 FinOps 等技术理念，持续为用户提供经济型 Serverless 服务。后续我们将分享更多围绕通用全场景 Serverless 的前沿理论及其案例实践，回馈社区，包括 FunctionGraph 在微服务 Serverless 化上的实践经验等。</p><p></p><p>作者介绍：</p><p></p><p>历川：华为云 Serverless 研发专家</p><p></p><p>平山：华为云中间件 Serverless 负责人</p><p></p><p>冯嘉：华为云中间件首席专家</p><p></p><p>参考资料：</p><p></p><p>[1] What is FinOps:</p><p></p><p><a href=\"https://www.finops.org/introduction/what-is-finops/\">https://www.finops.org/introduction/what-is-finops/</a>\"</p><p></p><p>[2] Running Lambda Functions Faster and Cheaper:</p><p></p><p><a href=\"https://levelup.gitconnected.com/running-lambda-functions-faster-and-cheaper-416260fbc375?gi=4370e4c57684\">https://levelup.gitconnected.com/running-lambda-functions-faster-and-cheaper-416260fbc375?gi=4370e4c57684</a>\"</p><p></p><p>[3] AWS Lambda Cost Optimizations Strategies That Work.</p><p></p><p><a href=\"https://dashbird.io/blog/aws-lambda-cost-optimization-strategies/\">https://dashbird.io/blog/aws-lambda-cost-optimization-strategies/</a>\"</p><p></p><p>[4] Timeout Best Practices.</p><p></p><p><a href=\"https://lumigo.io/learn/aws-lambda-timeout-best-practices/\">https://lumigo.io/learn/aws-lambda-timeout-best-practices/</a>\"</p>",
    "publish_time": "2022-09-23 16:22:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "英伟达入局AI大模型“战场”：发布两项全新大型语言模型，推动AI和数字生物的发展",
    "url": "https://www.infoq.cn/article/GxD5gkqc9WdmV2cLl5h5",
    "summary": "<p>当地时间9月20日，NVIDIA宣布发布两项全新大型语言模型（LLM）云 AI 服务——<a href=\"https://www.nvidia.cn/gpu-cloud/nemo-llm-service/\">NVIDIA NeMo 大型语言模型服务</a>\"和 <a href=\"https://www.nvidia.cn/gpu-cloud/bionemo/\">NVIDIA BioNeMo LLM 服务</a>\"。使开发者能够轻松调整 LLM 并部署定制的 AI 应用程序，其可用于内容生成、文本摘要、聊天机器人、代码开发，以及蛋白质结构和生物分子特性预测等。</p><p></p><p>NVIDIA NeMo LLM 服可提供一条快速路径，以便自定义和使用在多个框架上训练的大型语言模型。开发者可以在私有云和公有云上使用 NeMo LLM 部署企业 AI 应用，还可以通过云 API 体验 Megatron 530B（大型语言模型之一），也可以通过 LLM 服务开展实验。</p><p></p><p>NVIDIA BioNeMo 服务是云应用程序编程接口（API），它可以将 LLM 用例扩展到语言以外的科学应用中，加快制药和生物技术公司的药物研发速度。</p><p></p><p>NVIDIA 创始人兼首席执行官黄仁勋表示： “大型语言模型具有改变每个行业的潜力。通过对基础模型进行调整，可将 LLM 的力量带给数百万的开发者，让他们无需重新构建庞大的模型，即可创建各种语言服务并推动科学发现。”</p><p></p><h2>NeMo LLM 通过提示学习提高准确性，加快部署速度</h2><p></p><p></p><p>借助 NeMo LLM 服务，开发者可以使用自己的训练数据定制基础模型——从 30 亿个参数到世界上最大的 LLM 之一 Megatron 530B。与从头开始训练模型所需的数周或数月相比，该过程只需几分钟到几小时。</p><p>&nbsp;</p><p>提示学习是使用一种名为 P-tuning 的技术来定制模型，使开发者只需使用几百个示例就能快速定制最初需要使用数十亿个数据点训练的基础模型。定制流程生成任务专属的提示标记，这些标记会与基础模型相结合，为特定用例提供更高的准确性和更相关的响应。</p><p>&nbsp;</p><p>开发者可以使用同一模型定制多个用例并生成许多不同的 Prompt Token。游乐场功能提供可以轻松实验并与模型互动的无代码选项，进一步提高了 LLM 对行业特定用例的有效性和可及性。</p><p>&nbsp;</p><p>一旦准备好部署，经过调整的模型即可在云实例、本地系统上或通过 API 运行。</p><p></p><h2>BioNeMo LLM 使研究人员能够充分利用大规模模型的力量</h2><p></p><p></p><p><a href=\"https://blogs.nvidia.com/blog/2022/09/20/bionemo-large-language-models-drug-discovery/\">BioNeMo LLM 服务</a>\"包括两个用于化学和生物学应用的新 BioNeMo 语言模型。该服务提供蛋白质、DNA 和生物化学数据方面的支持，帮助研究人员发现生物序列中的模式和洞察。</p><p>&nbsp;</p><p>BioNeMo 使研究人员能够利用包含数十亿个参数的模型来扩大研究范围。这些大型模型可以储存更多关于蛋白质结构、基因间进化关系的信息，甚至可以生成用于治疗的新型生物分子。</p><p></p><p>除了调整基础模型之外，LLM 服务还提供通过云 API 使用现成和定制模型的选项。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>这让开发者可以访问包括 Megatron 530B 在内的各种预训练 LLM，以及使用 <a href=\"https://developer.nvidia.com/zh-cn/blog/?search_posts_filter=NeMo+Megatron\">NVIDIA NeMo Megatron</a>\" 框架创建的 T5 和 GPT-3 模型。NVIDIA NeMo Megatron 框架现已进入公测阶段，可支持各种应用和多语言服务需求。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>汽车、计算、教育、医疗、电信等行业的领导者正在使用 NeMo Megatron 为客户提供中文、英语、韩语、瑞典语等语言的服务。</p>",
    "publish_time": "2022-09-23 16:35:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI宣布开源多语言语音识别系统Whisper，英文识别能力接近人类水平",
    "url": "https://www.infoq.cn/article/Rsk9FJE3Nk4U1Yc3KhKO",
    "summary": "<p></p><p>9月21日，<a href=\"https://openai.com/blog/whisper/\">OpenAI</a>\"宣布，已经训练并开源了一个名为 Whisper 的神经网络，它在英语语音识别方面接近人类水平的鲁棒性和准确性。</p><p></p><p>Whisper 是一个自动语音识别 (ASR) 系统，它使用从网络上收集的 680,000 小时多语言和多任务监督数据进行训练。使用如此庞大且多样化的数据集可以提高对口音、背景噪音和技术语言的鲁棒性。此外，它还支持多种语言的转录，以及将这些语言翻译成英语。</p><p></p><p>OpenAI开源了<a href=\"https://github.com/openai/whisper/blob/main/model-card.md\">模型</a>\"和推理<a href=\"https://github.com/openai/whisper\">代码</a>\"，以作为构建有用应用程序和进一步研究稳健语音处理的基础。</p><p></p><p>查看论文：<a href=\"https://cdn.openai.com/papers/whisper.pdf\">https://cdn.openai.com/papers/whisper.pdf</a>\"</p><p>开源代码：<a href=\"https://github.com/openai/whisper\">https://github.com/openai/whisper</a>\"</p><p>查看模型卡：<a href=\"https://github.com/openai/whisper/blob/main/model-card.md\">https://github.com/openai/whisper/blob/main/model-card.md</a>\"</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a4/7c/a40526a0448af8c6173820f33045997c.jpeg\" /></p><p></p><p></p><p>Whisper 架构是一种简单的端到端方法，实现为编码器-解码器 Transformer。输入音频被分成 30 秒的块，转换成 log-Mel 频谱图，然后传递到编码器。解码器被训练来预测相应的文本标题，并与特殊标记混合，这些标记指导单个模型执行诸如语言识别、短语级时间戳、多语言语音转录和英语语音翻译等任务。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/bd/99/bd73b013e26a73b3080974aa4851be99.jpeg\" /></p><p></p><p></p><p>其他现有的方法经常使用更小、更紧密配对的音频-文本训练数据集，或使用广泛但无监督的音频预训练。因为Whisper是在一个庞大而多样的数据集上训练的，没有针对任何特定数据进行微调，所以它无法击败专门研究LibriSpeech性能的模型，这是语音识别领域一个著名的竞争基准。然而，当我们在许多不同的数据集上测量Whisper的零样本性能时，我们发现它比那些模型更健壮，并且错误率降低了 50%。</p><p></p><p>Whisper的音频数据集中大约有三分之一是非英语的，它被轮流分配任务，将原始语言转录或翻译成英语。并且优于 CoVoST2 到英语翻译零样本的监督 SOTA。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/1c/2c/1cc6c2749f9faea14beb973480639a2c.png\" /></p><p></p><p></p><p>Whisper的高精度和易用性能够让开发者将语音界面添加到更广泛的应用程序中。</p>",
    "publish_time": "2022-09-23 17:20:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]