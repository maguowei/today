[
  {
    "title": "Netflix基于Redis、Kafka和Elasticsearch构建高吞吐优先队列Timesone",
    "url": "https://www.infoq.cn/article/JC8GL4g4OHL1WFISs4g1",
    "summary": "<p>最近，Netflix公布了它是<a href=\"https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjYwNTcxNzksImZpbGVHVUlEIjoiem5hSTQwQ3QzcVVtUlhDdiIsImlhdCI6MTY2NjA1Njg3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.ZCmK0IQLsF3M-6uDdePoMyHU7WRbsNA8kcqarqfIFnY\">如何构建Timestone</a>\"的——一个高吞吐、低延迟的优先队列系统。Netflix使用Redis、Apache Kafka、Apache Flink和Elasticsearch等开源组件来构建这个队列系统。Netflix的工程师们表示，他们之所以要构建Timestone，是因为他们无法找到满足其所有要求的现成解决方案。</p><p></p><p>其中一个需求是不需要在消费者端进行任何锁定或协调的情况下将某些工作项标记为不可并行。这一需求意味着在属于同一工作集的前一个项目完成之前，Timestone不应该发送消息。Timestone引入了“独占队列（Exclusive Queue）”的概念来实现这一目的。</p><p></p><p>Netflix的软件工程师Kostas Christidis解释了独占队列的工作原理。</p><p></p><p></p><blockquote>独占队列被创建后将与用户定义的独占键相关联——例如，“project”。所有发布到该队列的消息都必须在其元数据中携带此键。例如，带有\"project=foo\"的消息将被接收到独占队列中，不包含该键的消息将不会进入独占队列。在这个例子中，与独占键对应的值是“foo”，也就是消息的独占值。独占队列的约定是，在任何时间点，每个独占值最多只能有一个消费者。因此，如果我们示例中以“project-”为前缀的独占队列中有两个消息的键值对为“project=foo”，并且其中一个消息已经分配给了一个消费者，那么另一个消息就不能退出队列。</blockquote><p></p><p></p><p>下图描绘了这个示例。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/10/netflix-timestone-priority-queue/en/resources/1Netflix-Timestone-Exclusive-Queues-1664933775499.png\" /></p><p></p><p>当worker_2发出出队列调用时，会收到msg_2而不是msg_1，即使msg_1具有更高的优先级</p><p></p><p>来源：https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f</p><p></p><p>另一个需求是，在任何给定的时间，一条消息只能分配给一个消费者。这很重要，因为Cosmos种的工作负载往往是资源密集型的，并且可能扇出数千个动作，这个需求的目标之一便是减少资源浪费。这个需求排除了最终一致性解决方案，这意味着Netflix的工程师想要的是队列级别的<a href=\"https://jepsen.io/consistency/models/linearizable?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjYwNTcxNzksImZpbGVHVUlEIjoiem5hSTQwQ3QzcVVtUlhDdiIsImlhdCI6MTY2NjA1Njg3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.ZCmK0IQLsF3M-6uDdePoMyHU7WRbsNA8kcqarqfIFnY\">线性一致性</a>\"。</p><p></p><p>Netflix工程师通过为每条消息维护一个消息状态来实现这一需求。当生产者将消息入队时，消息将被设置为“Pending”或“Invisible”状态，这取决于消息的超时设置（可选）。当消费者将挂起的消息从队列中取出时，它将获得该消息的独占租约，Timestone将该消息设置为“Running”状态。在这个阶段，生产者可以将消息标记为“Completed”或“Cancelled”。每条消息最多可以尝试有限的取出次数，然后Timestone将其标记为“Errored”状态。下图说明了所有可能的状态转换。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/10/netflix-timestone-priority-queue/en/resources/1Netflix-Timestone-Message-States-1664933775499.png\" /></p><p></p><p>来源：https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f</p><p></p><p>Timestone服务器提供了一个基于gRPC的接口。所有API操作都在队列作用域内。所有修改状态的API操作都是幂等的。记录系统是一个Redis集群。在将响应发送回服务器之前，Redis会将每个写请求持久化到事务日志中。在Redis内部使用了一个按优先级排序的排序集代表每个队列。消息和队列配置以散列值的方式存储。</p><p></p><p>Christidis提到了Netflix工程师如何用Redis实现原子性：</p><p></p><p></p><blockquote>几乎所有Timestone和Redis之间的交互都写在Lua脚本中。在大多数Lua脚本中，我们倾向于更新大量的数据结构。由于Redis保证每个脚本都是原子执行的，所以成功执行脚本意味着可以保证系统处于一致的（在ACID意义上）状态。</blockquote><p></p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2022/10/netflix-timestone-priority-queue/en/resources/1Netflix-Timestone-System-Architecture-1664933775498.png\" /></p><p></p><p></p><p>来源：https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f</p><p></p><p>为了实现可观察性，Timestone捕获关于传入消息及其状态间转换的信息，并将其保存在Elasticsearch的两个二级索引中。当Timtstone服务器从Redis获得写入响应时，它将其转换为发送到Kafka集群的事件。有两个分别对应Timestone两个索引的Flink作业，消费来自相应Kafka主题的事件，并更新Elasticsearch中的索引。</p><p></p><p>Netflix创建Timestone是为了满足其媒体编码平台Cosmos的需求。Timestone还支持<a href=\"https://conductor.netflix.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjYwNTcxNzksImZpbGVHVUlEIjoiem5hSTQwQ3QzcVVtUlhDdiIsImlhdCI6MTY2NjA1Njg3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.ZCmK0IQLsF3M-6uDdePoMyHU7WRbsNA8kcqarqfIFnY\">Conductor</a>\"——Netflix的通用工作流编排引擎，作为大规模数据管道的调度器。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/10/netflix-timestone-priority-queue/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjYwNTcxNzksImZpbGVHVUlEIjoiem5hSTQwQ3QzcVVtUlhDdiIsImlhdCI6MTY2NjA1Njg3OSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4NTA5NTIwOX0.ZCmK0IQLsF3M-6uDdePoMyHU7WRbsNA8kcqarqfIFnY\">Netflix Builds a Custom High-Throughput Priority Queue Backed by Redis, Kafka and Elasticsearch</a>\"</p>",
    "publish_time": "2022-10-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从 “B 站 713 事故”看高可用系统的治理",
    "url": "https://www.infoq.cn/article/DDyq0CVQMTlTq8wPBNFl",
    "summary": "<p>随着数字化经济的不断发展，云计算成为了各行各业进行数字化转型的重要基础设施，越来越多的企业选择上云。然而，随着云上业务的规模与复杂度日趋増长，对云上的运维、安全和管理也提出了新的挑战。</p><p>&nbsp;</p><p>为了让企业以及相关IT和业务团队更好地紧跟技术变革，利用好云技术、释放云能力使能业务发展，据悉，此前在华为上海研究所举办的华为云联创营 • 云上综合治理班携手 bilibili，并邀请到诸多业内技术专家，围绕着智能运维中的算法落地、bilibili＂713 事故”后优化改进实践、叮咚买菜基础技术应对疫情保障的经验、华为云 SRE 运维体系和企业云上容灾备份实践等内容，共同探讨安全可靠确定性的云上治理之路。</p><p>&nbsp;</p><p>之前受开发者广泛关注的 bilibili 713 事故 bilibili 在线 SRE 负责人武安闯在华为云联创营 • 云上综合治理班中做了详细的事故解析和技术优化实践分享，为企业制定高可用业务系统的治理方案提供了许多启发。</p><p></p><h2>一、bilibili 713 事故：耗时近 3H 才全面解决问题</h2><p></p><p>&nbsp;</p><p>2022 年 7 月 23 日，一篇名为《2021.07.13 我们是这样崩的》文章在 bilibili 社媒发布之后，迅速引起业界广泛关注，这篇文章发布之后，在技术圈掀起了一段讨论热潮，很多读者觉得意犹未尽，期待 bilibili 继续剖析＂713 事故”之后如何执行优化落地。在本次华为云联创营 • 云上综合治理班中武安闯就给大家了一个回复：</p><p>&nbsp;</p><p>bilibili 713 事故的时间线是这样的，2021 年 7 月 13 日 22:52，B 站无法使用，大量客户反馈内部大量服务、域名接入层不可用报警，在 22:57，bilibili Oncall SRE 便发现是 SLB 故障，但因为 SRE 团队核心成员在 VPN 公司内网时也受影响，23:17 才陆续进入内网系统正式解决问题。23:23 bilibili APP 推荐、APP 播放、评论&amp;弹幕拉取、动态、追番、影视等多活业务读取恢复，当晚多活机房 SLB 容量过载，后流量下降，重启后恢复，但直播移动端首页接口因为没配置多机房调度，导致当晚没有及时恢复。</p><p>&nbsp;</p><p>紧接着，bilibili SRE 团队开始进行三次 Lua 层面的变更回滚，但直到 23:55 依旧没有恢复。于是团队在 01:00 开始新建 SLB 集群、配置初始化，进行四层 LB 配置与公网线路配置，CDN 开始切换回源流量，直到 01:40，核心业务切换到了 SLB 新集群，业务全部恢复。</p><p>&nbsp;</p><p>为了彻底消除了风险，第二天上午，bilibili SRE 团队开始复现问题，定位到 3 个主要原因：</p><p>多活基架能力不足；切量强依赖 CDN 运维；业务多活元信息缺乏平台管理。</p><p>&nbsp;</p><p>之后 bilibili 针对这些问题在多活基架能力建设和多活管控能力提升两方面进行了优化。</p><p>&nbsp;</p><p>从多活基架能力建设方面，bilibili 优化了多活基础组件的支持能力，如数据层同步组件优化、接入层支持基于用户分片，让业务的多活接入成本更低；重新梳理各机房在多活架构下的定位，梳理 Czone、Gzone、Rzone 业务域；推动不支持多活的核心业务和已实现多活但架构不规范的业务改造优化。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/a6/0f/a6dce232620aacbbf966fed58278520f.png\" /></p><p></p><p>从多活管控能力提升方面，统一管控所有多活业务的元信息、路由规则，联动其他平台，成为多活的元数据中心；支持多活接入层规则编排、数据层编排、预案编排、流量编排等，接入流程实现自动化和可视化；对接 CDN、存储等组件，实现了一键全链路切量，提升效率和准确率。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/72/f9/72352e1199860071126344a282f0bcf9.png\" /></p><p></p><p>值得一提的是，目前 bilibili 支持在多活切量时的前置能力预检，比如容量预检、延迟预检、限流预检、隔离预检等，而且还实现了切量中风险巡检和多活流量、业务/应用 SLO、Trace 链路等核心指标的可观测。</p><p><img src=\"https://static001.infoq.cn/resource/image/96/9c/96d52073dee006b4664e4843ca03ce9c.png\" /></p><p>图：风险预检</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/dc/c2/dc87cc39c8cf27306d3e99522a4c37c2.png\" /></p><p>图：多活切量可观测</p><p></p><p>在经历了以上一系列优化后，多活相关故障解决效率大大提升。比如“713 事故”时，如果业务 A 故障，那就要首先切量到多活机房，SRE 跟研发沟通后确认需要切域名 A+URL A，然后要告知 CDN 运维进行切量，全程至少需要半小时。而在优化后，SRE 与研发确认切量的业务、组件、流量比例后便可以实施动作，执行全程仅需 3-5 分钟。目前 bilibili 多活业务全部接入、生产已演练 80 多次，整体运行都非常稳定。</p><p></p><h2>二、稳健的多活架构的本质是“高可用架构”</h2><p></p><p>&nbsp;</p><p>事实上，类似于“bilibili 713 事故”的类似事故并不少见，这也是为什么上个月复盘文章发出就引发众多开发者关注的原因。构建稳定的多活架构说到底就是高可用架构构建问题，高可用正如华为云多活高可用解决方案架构师郑学强在本次华为云联创营 • 云上综合治理班中所说的那样，“当前环境下，企业对业务连续性 (BCM) 的追求，促使应用向高可用架构演进。”</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/a1/24/a1b160ba469418d026a8a62743c5ce24.png\" /></p><p></p><p>在企业需求和行业发展要求的推动下，华为云一直在努力完善多活高可用解决方案。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/8d/7d/8d2197fc5cc48aabf253285eb789c57d.png\" /></p><p></p><p>这个方案能够实现异地多活、单元化，并且支持流量自治，非常适用于对容灾可靠性要求极高、业务对时延敏感且要求数据分区化的企业：</p><p>用户访问流量：通过 DNS 的 GSLB 特性实现业务访问流量控制，并在网关层进行流量分区管理和纠错；网关层：进行流量路由标记，流量染色，并通过染色结果精准调度；数据层：数据禁写保护，避免脏数据，数据双向同步，单元化数据要通过网关判断纠错，数据层 SDK 进行多层数据禁写；容灾恢复切换：通过多活切流，并按照单元化的流量分配，切流过程汇总数据禁写保护，保证数据一致性；容灾演练：客户自行通过人工/脚本方式进行演练或基于第三方软件进行容灾演练。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/0b/11/0bd1344aa43537b8d7773508619f3b11.png\" /></p><p></p><p>该方案有两个核心技术亮点，首先，华为独有的探活仲裁管理面 DCG，以及配合切换的 SDK，多层配合实现业务双活。在公网接入层中，DNS 中配置双 AZ IP 的两条 A 记录，DNS 域名解析返回 IP1&amp;IP2，P1、IP2 顺序随机，50% 用户 IP1/IP2，50% 用户 IP2/IP1。在端侧使用 URLConnection 或 OKHttp 调用 Http 接口，并设置超时为 8-10 秒；端侧域名解析获得两个 IP，缓存在本地，在 TTL（通常 300 秒）时间内使用缓存；端侧默认使用第一个 IP 访问；如果第一个 IP 访问超时，底层会自动尝试第二个 IP（RFC3484)。在负载均衡层中，支持 ELB 双 AZ 转发、SLB 双 AZ 转发。如果 SLB 节点故障，ELB 通过健康检查会自动摘除故障 SLB 节点；如果 App 节点故障，SLB 通过健康检查会自动摘除故障 APP 节点。而在业务层，实例无状态，可横向扩容；多个实例负荷分担方式工作，任何一个故障，不影响业务，同时业务读写 AZ1 的数据层。如果 AZ1 中数据层发生故障，则可由 DBMonitor 自动切换到 AZ2 中的数据。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/77/37/77591db3197a548cc2ecf1e5f78eea37.png\" /></p><p></p><p>其次，两地三中心——应用跨 AZ 双活、跨 Region 容灾。从系统架构方面来看，容灾区域可双活或者单 AZ 部署，根据业务进行评估。MAS 容灾管控负责管理端到端同步任务，数据同步关系建立、展现、切换，同时负责监控主备 Region 状态，还提供主备 Region 容灾切换能力，用户在切换时可根据告警手动切换。 在容灾切换停止主 Region 业务（入口流量、定时任务等）后，MAS 则停止主-备数据同步任务，并启动备-主数据同步任务，用户修改 DNS 配置完成容灾切换。</p><p>&nbsp;</p><p>在跨 Region 容灾切换方面，华为云提供了切换编排能力，常见的切换动作流：主 Region 数据库设置只读（可选）、停止跨 Region 数据库容灾同步任务、容灾 Region 的数据源升主、容灾 Region 应用启动/扩容、DNS 切换到容灾 Regin、启动容灾 Region 的应用侧定时任务。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/19/f9/195b8fab77d87c4200f8b63e97db21f9.png\" /></p><p></p><p>当生产站点因为不可抗力因素或因设备故障导致应用在短时间内无法恢复时，这样的容灾方案可以很好的解决运维事故，以满足 SRE 需求。</p><p>&nbsp;</p><p>除此之外，典型的双活架构日常非故障时可以将流量负载分担到不同的分区，减少并发压力，像 bilibili 这种自媒体视频平台很容易因为突如其来的热点而产生突发性高并发流量，华为云提供的这种多活高可用解决方案还可以很好地应对这种难题。</p><p>&nbsp;</p><p>从华为云多活高可用的实践方案中我们可以看出，华为云在高可用方面的表现非常优秀。像“华为终端云服务迁云+双活”，其业务增长年复合率已经超过了 30%。从 2013 年至今，该架构可靠性已实现 4 个 9，运维成本降低了 30%，服务类型已经超过 30 种，弹性伸缩效率达到 1k/min,安全能力以达到 T 级。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/d6/24/d6bb2dyy0a2af0e091e4edb295028924.png\" /></p><p></p><p>在消费者中国区云化双活项目中，截至 2018 年 Q2，其所有业务流量就已由公有云环境承载。目前系统运行正常，华为精选、协议服务、A/B 测试平台等新业务也已在公有云高可用部署上线，另外借助此方案已经完成了中国区公有云双 AZ 故障的应急演练，涉及已云化的 12 个业务，演练结果均符合预期，有效地确保了公有云双 AZ 的可靠性。</p><p></p><h2>三、写在最后</h2><p></p><p>&nbsp;</p><p>系统可用性是通过可用性指标来进行衡量的，我们说的“高可用”则是指这个系统 99.99% 的时间都是可用的，这也就意味着一年中的不可用时间只占 53 分钟，在这个飞速发展的数字化时代，高并发流量突发、设备突发故障或其他不可预知的情况为企业提出了挑战，“4 个 9”真的不是轻易可以做到的。</p><p>&nbsp;</p><p>面对企业上云之后带来业务规模与复杂度的日趋增长，华为云基于和伙伴的协同实践构建了一套完整的面向云上应用的立体化治理系统，通过融合 AOM、APM，提供云应用基础设施层、应用层、业务层的运维能力，并对各类资源可实现多维度实时监控，通过应用与资源关联分析技术，实现问题快速诊断和修复，保障云上应用持续稳定运行。</p><p>&nbsp;</p><p>如今上云已经成为企业战略部署的一部分，上云这件事对于企业来说已经是个既定命题，而如何构建高可用系统架构成为企业业务上云后新的关注点之一，通过一系列的自动化手段实现业务的高可用是目前所有企业常用的解决方案，华为云一直在持续加强技术研发和创新，为智能世界构筑云底座而不断展开技术探索，全面助力千行百业的数字化转型。</p><p></p><p>关于华为云与 bilibili 的更多技术，请关注↓</p><p><img src=\"https://static001.infoq.cn/resource/image/97/d9/97yy2f5643604dc1c70c6be9eab868d9.jpeg\" /></p><p></p>",
    "publish_time": "2022-10-20 12:43:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]