[
  {
    "title": "加速数字化转型：深度解析API成熟度模型",
    "url": "https://www.infoq.cn/article/IeNEgLeqmfLxzdjYeg4x",
    "summary": "<p>如果数字化转型得当，可以影响一个组织的各个方面。然而，API成熟度成为数字化转型需要解决的一个常见的问题。API是推动业务增长的桥梁，但随着API被广泛使用，可能会出现API蔓延。在解决日常业务问题的过程中，没有计划和管理的API激增会导致API蔓延。API蔓延说明有大量的API正在被创建，部署API的分布式基础设施也在发生物理扩散。</p><p></p><p>公司看到他们的API正以前所未有的速度在全球范围内蔓延。对于希望在分布式基础设施之间保持高质量和卓越体验的组织来说，API蔓延给他们带来了一个独特的挑战。</p><p></p><p>管理大规模的API需要自上而下的监督，它还需要一种实用的方法，并从旨在统一API的API程序计划开始。程序应该将API打包成产品或服务来推动API的采用，并促进对其整个生命周期的管理。问题在于，创建一个可行的程序来管理API成熟度是一个缓慢的过程。</p><p></p><p>本文将为构建成熟的API计划提供一个框架，这个框架使用了一个可以促进API驱动业务演化的四层API程序成熟度模型。</p><p></p><h2>什么是API成熟度模型</h2><p></p><p></p><p>关于API的成熟度和生命周期，我们可以用两个阶段来描述：API成熟度和API程序成熟度。</p><p></p><p>API成熟度与设计和开发相关，遵循的过程与软件开发成熟度一致。API成熟度确保API符合公认的API规范，比如REST。在讨论API成熟度时，你讨论的是为特定应用程序或目的而创建的一组API。</p><p></p><p>API程序成熟度主要是从整个公司层面来看的，即公司为满足各种业务目标而积累的大量API。对于API程序成熟度来说，将API构建成统一的服务是有必要的。API程序成熟度模型为通过简化API来促进业务创新提供了基准。</p><p></p><h2>API程序成熟度模型</h2><p></p><p></p><p>API程序成熟度从技术和业务的角度评估API的非功能性指标。API的技术指标包括性能、安全性、体验和可伸缩性。API的业务指标与间接影响时间和成本的流程和生产力的改进有关。</p><p></p><p>与其他业务流程一样，API程序也应该从小处开始，然后逐渐演化。API程序的结构必须能够遵循持续的改进周期。指标应该随着API程序从较低成熟度级别过渡到较高成熟度级别而得到改进。</p><p></p><p>在开始你的API成熟度模型之旅之前，你必须首先将API视为一种工具。然后，在模型的演进过程中，随着达到更高的成熟度级别，你需要基于API为日常业务所带来的能力将其视为组件、模型或生态系统。</p><p></p><h2>API程序成熟度的四个级别</h2><p></p><p></p><p>如果你将API程序成熟度视为企业数字化转型整体方法的一部分，API程序可以分为四个成熟度级别：</p><p></p><h4>级别1：“API的暗黑时代”——API作为数据采集工具</h4><p></p><p></p><p>从历史上看，构建API是为了方便数据采集，Salesforce和Amazon的早期API就是最好的例子。这类API的主要目的就是用于标准化跨多个业务应用程序的数据共享。</p><p></p><p>API程序成熟度的第一个级别是为可以提供单一事实来源的数据采集创建标准化的数据访问接口。这些API按照不同的业务功能分类。例如，你可以有分别用来访问财务、销售、员工和客户数据的API。</p><p></p><p>当你建立了API设计和架构最佳实践，你的组织就达到了API程序成熟度级别1。一些最佳实践包括：</p><p>在设计API时要考虑到集成便利性和可重用性；所有的API都保持一致的接口；在设计中结合版本控制，同时支持多个客户端；确保API的可伸缩性，适应不断变化的用户需求。</p><p></p><p>这些API相对简单，不需要高级的可编程功能。级别1也可以定义为一种相对不成熟的、手动的API部署方法。手动部署的个体API不支持API生命周期管理，技术的重点放在了将API构建为独立的工具上。</p><p></p><h4>级别2：“API的复兴时代”——API作为流程集成组件</h4><p></p><p></p><p>回顾API的发展历史，从2000年代开始，当它们开始被用作连接器来集成不同的系统时，迎来了它们的复兴时代。单点登录（SSO）就是一个典型的例子。SSO时一种被广泛使用的API集成工具，用于对用户进行身份验证，让用户能够安全访问多个应用程序和第三方服务。</p><p></p><p>当你的组织达到API程序成熟度级别2时，你的API程序将使用基于组件的方法。应用程序被分解为单独的组件，每个组件都可以独立于应用程序的其他部分进行开发和测试，然后再集成成一个完整的应用程序。这种方法降低了复杂性，易于维护，并提高了可伸缩性。</p><p></p><p>API将作为集成不同业务和特定领域流程的组件捆绑在一起。这些API包简化了运营和工作流，将多个部门连接起来，甚至可以集成与外部合作伙伴的工作流与交互。</p><p></p><p>当你达到级别2时，你的组织就迈出了将API应用于业务的第一步。在级别2，API被视为组件，为你提供了标准化和可重用的API目录。级别2通过CI/CD（持续集成/持续交付）管道实现标准化和自动化，改进了开发周期，从而推进了API开发和生命周期管理。</p><p></p><h4>级别3：“API的启蒙时代”——API作为统一体验的平台</h4><p></p><p></p><p>在API复兴时代，API被视为组件，以此来简化集成和提升可重用性。级别3是API的启蒙时代，它更进一步，让API变得对用户更加友好和有价值。</p><p></p><p>当你达到级别3，API就不再被视为改进业务工作流的组件或独立的工具。这一级别关注的是构建API套件，通过创建互连的体验来驱动更好的工作流程。API组件的作用在于，API提供者可以在设计和构建API时对应用程序进行分解。而API套件的作用在于，API提供者可以对功能进行分组，让API消费者可以与它们集成，从而获得更好的体验。</p><p></p><p>例如，物流公司依靠卡车和送货车车队来维持业务的连续性。它可以使用API套件来监控和管理其车队的各个方面。在级别3，你需要一个精心设计的包含多个API的API套件来监控卡车、绘制路线、提供性能分析，等等。</p><p></p><p>在级别3，API是用户体验（UX）的关键。API套件成为面向用户的应用程序的支柱。在上面的卡车车队示例中，公司用于车队管理的前端软件依赖于API来驱动最终的用户体验，因此API套件成为为整个软件包提供接口的后端平台。</p><p></p><p>当你达到了级别3，API程序将起着至关重要的作用，因为API套件成了任务关键型的服务。在这个阶段，API消费者做了大量的投入，API的可靠性和成熟度就变得非常重要。处于级别3的API程序都达到了一定程度的技术成熟度，包括：</p><p>部署：API套件进行批量部署，与API生命周期阶段和版本控制紧密结合。性能：API支持云原生环境，可以获得更好的可伸缩性和弹性工作负载。安全性：启用多层安全机制，确保严格的认证和授权过程。自动化：CI/CD管道是完全自动化的，包含了严格的API测试。体验：自助式API门户有助于开发人员快速上手。</p><p></p><h4>级别4：“API的自由化时代”——API作为业务转型的生态系统</h4><p></p><p></p><p>当你的组织达到了API程序成熟度级别4时，你将拥有完全外部化的API。API演进的最后阶段更多地是由业务需求而不是技术驱动的。在这个级别，你可能已经有了一个运行良好的技术栈，并且正在推动内部和合作伙伴采用API，因为API带来了很多业务价值。下一个合乎逻辑的进程是通过货币化将这种价值外部化。</p><p></p><p>在级别4，你将采用一种新的方法——API即产品。在这个级别，你可以通过服务订阅模型向客户提供API。API即产品可以作为独立服务或补充服务来提供，具体根据公司的业务性质来决定。无论采用哪种方式，API都紧密集成到了你的产品、营销和销售中，因此每个人都可以通过协作来推动这种新兴的价值流。</p><p></p><p>在级别4，API程序成为业务增长的引擎。是否已经达到级别4的一些指标包括：</p><p></p><p>API治理</p><p></p><p>你有一个专门的API产品管理小组。这个小组确保所有API都是基于一组预定义的规则开发的。它还定义了API生命周期进展策略，确保API具备架构和安全合规性。</p><p></p><p>API可观测性</p><p></p><p>你的团队不仅能够对API进行监视，还能够捕获API业务逻辑的内部状态，收集与性能有关的数据。</p><p></p><p>API生态系统</p><p></p><p>你建立了一个API社区，让开发人员和消费者有交换意见和寻求支持的地方。API论坛进一步增强了API生态系统，推动API的采用。</p><p></p><h2>API程序改进周期</h2><p></p><p></p><p>这个世界上没有完美的API程序。任何一个API治理框架都必须进行定期审计，确定API程序的当前成熟度级别。</p><p></p><p>无论处于哪一个API成熟度级别，采用DevOps方法并通过持续的小Sprint来提高API成熟度都是必要的。要采用DevOps方法，需要在组织范围内建立共识，实现更敏捷、更快的小增量改进周期。</p><p></p><p>理想的API程序改进周期包括五个阶段。</p><p></p><h4>评估和探索</h4><p></p><p></p><p>第一个阶段是在技术和业务层面评估API程序的当前状态，并探索改进它的可能性。当然，技术成熟度要先于业务成熟度，应该是级别1和级别2的核心关注点。</p><p></p><p>在探索需要改进的领域时，可以设定一些小目标，而不是试图从一个级别直接跳到下一个级别。你可以在内部定义子级别，改进API程序的某个特定方面，例如部署自动化、安全性或可伸缩性。</p><p></p><h4>设计与建议</h4><p></p><p></p><p>第二个阶段是改进周期中最关键的决策点。在这个阶段，你需要梳理来自不同利益相关者的技术规范和业务目标。然后，你可以建议对底层API管理技术栈做出修改，这些修改应该是当前改进周期的一部分。</p><p></p><h4>构建与实现</h4><p></p><p></p><p>第三个阶段是改进周期的实现阶段。这个阶段包含了基于建议的开发和配置增强。</p><p></p><h4>测试与监控</h4><p></p><p></p><p>在第四个阶段，你开始通过测试来驱动API。在这个阶段，你通过监控重要的性能和改进指标来判断API改进周期的总体有效性。这个阶段可能会很长，因为你需要在第三阶段和第四阶段之间来回转换，直到可以从指标上看到可度量的改进。</p><p></p><h4>启动新的API程序</h4><p></p><p></p><p>一旦完成了测试和监控阶段，并且看到了真正的改进，就来到了最后一个阶段——生产部署。在这个阶段，你将新的API程序产品化，并启动和运行它们。</p><p></p><h2>如何提升你的API程序成熟度</h2><p></p><p></p><p>这里给出的不同级别的API程序成熟度提供了一条带有逻辑里程碑的清晰路径，帮助你的组织从低级别API实现过渡到高级API实现，但这里还有一个更重要的挑战需要你去解决。</p><p></p><p>API程序象征着组织范围内采用其他API的氛围。将API的发展定位成推动业务增长的主要引擎之一，这是一个理想的愿景。然而，要让API程序取得成功，必须将其构建成横向的跨部门和团队的功能。</p><p></p><p>在大多数企业中，每一个部门都需要其他部门的清晰度和可见性，这也是为什么需要大量工作来执行治理和标准化的原因之一。缺乏可见性会增加创建重复API的可能性。</p><p></p><p>孤立的API团队可能会带来一些挑战，例如缺乏沟通、理解和可见性。彼此孤立的团队为建立API开发集成策略带来了挑战。此外，每个团队可能有不同的优先级，导致整个过程出现延迟和错误。此外，团队彼此孤立会导致协作和知识共享的机会变少，而这些本来可用于提高正在开发的API的质量。</p><p></p><p>横向的API程序功能跨越了这种部门间的孤岛，有助于确保API的统一治理和标准化。</p><p></p><p>以下是一些你可以用来应对持续改进周期挑战的规则。</p><p></p><h4>由外而内建立共识</h4><p></p><p></p><p>由外而内的方法需要对业务工作流进行分析，围绕它们设计出正确的数字体验。与由内而外的方法相比，由外而内的方法在捕获各种利益相关者的期望方面要有效得多。</p><p></p><h4>自上而下的文化转变</h4><p></p><p></p><p>寻求推动全公司文化转变的最佳实践是一个极具争议的话题。由于成功的API程序需要水平对齐，因此采用自上而下而不是自下而上的方法可以降低出现API蔓延的可能性。</p><p></p><p>自上而下的API开发方法有几个优势，例如缩短发布时间、缩短开发周期和更容易维护。它还为API架构提供了更大程度的清晰度，使得开发人员能够更容易地知道他们必须做些什么以及他们在整个项目中的职责。此外，自上而下的方法可以减少与API安全性、可靠性和文档相关的工作量。</p><p></p><h4>战略观点</h4><p></p><p></p><p>API程序成熟度的初始级别是将API视为技术工具包的一部分。记住，这是一种短期的战术视角。随着API程序的不断发展，必须不断努力构建战略愿景，让API程序带来可通过业务级KPI来度量的价值。</p><p></p><p>达到最高的API程序成熟度级别需要时间和精力，同时还要管理利益相关者的期望。但不管怎样，开发成熟的API程序将为加速业务创新和增长带来新的机会。</p><p></p><p>作者简介：</p><p>Darshan Shivashankar是APIwiz的创始人兼首席执行官。APIwiz是一个低代码API运营平台，旨在通过简化API生命周期管理来提高生产力和实现大规模治理。他有12年帮助企业进行数字化转型的经验。他曾为电信、医疗保健和金融行业的大型企业领导和实现API程序。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/api-maturity-model/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTAxNzA5MzUsImZpbGVHVUlEIjoiTVlneHNCOTVDTlU1TGx2NSIsImlhdCI6MTY5MDE3MDYzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.GnN_eCmUH_ihtMltPOgnFRsI72aQPdHIExmo9K95EHs\">https://www.infoq.com/articles/api-maturity-model/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/shlKO1bTHS2tCFiFpNe1\">API 设计评审已死，API 设计评审万岁</a>\"</p><p><a href=\"https://www.infoq.cn/article/mDlgpAcKK4V3Qr4r9d8E\">API 网关和负载均衡器，到底怎么选？</a>\"</p>",
    "publish_time": "2023-07-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于鲲鹏+昇腾 华鲲振宇重磅发布算力创新产品及解决方案",
    "url": "https://www.infoq.cn/article/Nk9DyErUiAA5uH80qUU0",
    "summary": "<p></p><p>2023年7月26日，<a href=\"https://www.infoq.cn/article/iCuurCMJCvofpfylbGpE\">华鲲振宇</a>\"在北京举办产品发布会，以“中华鲲鹏振兴寰宇，昇腾万里智领未来”为主题，分享基于“鲲鹏+昇腾”的算力创新，发布全新一代算力基础设施与解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec4766737610417cbcf9f7a141a8576e.png\" /></p><p></p><p>此次发布会汇聚产业精英、客户和生态伙伴，共同探讨满足数字时代多样性算力需求的未来发展方向。北京市朝阳区人民政府党组成员、副区长舒毕磊、华为计算业务总裁田华、华鲲振宇总裁万诤等领导出席大会并致辞。</p><p></p><h4>深入场景发布四大“天”系产品 全栈创新敢于亮剑</h4><p></p><p></p><p>擎天之力铸坚实算力底座——“天擎”系列高规格通用计算平台</p><p></p><p>随着应用与场景驱动需求激增，对<a href=\"https://xie.infoq.cn/article/372faa42c86908abae0360a08\">算力基础设施</a>\"提出了更多差异化的需求，本次大会，华鲲振宇正式揭开基于鲲鹏创新架构算力产品的神秘面纱，华鲲振宇研发总经理赵彦钧携华为鲲鹏计算业务副总裁马银川正式发布“天擎”系列高规格通用计算产品，最大支持16块NVMe SSD，创新蝶形IO模组设计，最多可支持11个PCIE 标准插槽，规格提升了37%；率先支持9张 PCIe X8的 NPU/GPU加速卡，或两张全高全长双宽的训练卡，整机AI计算性能提升15%以上，各项参数直指鲲鹏最高规格，为千行百业提供高密性能的算力应用平台，已规模服务于运营商、互联网、金融等行业，满足客户在不同部署场景、不同算力、不同存力需求下的计算任务，让每一次计算都能心有所想、算有所成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52d019f2cd0760e857efad515f809a84.png\" /></p><p></p><p>深耕智算助力大模型普惠AI——“天智”系列高密AI算力平台</p><p></p><p>伴随大模型带来的生成式AI突破，人工智能技术正在跨越重要的历史拐点，华鲲振宇也早已有了筹谋，基于昇腾AI计算，在多个典型应用场景进行了深入创新和升级，积极拥抱人工智能，在大模型训练场景下，推出基于昇腾的“天智”系列高密AI算力服务器——新一代训推一体服务器AT800 A2，底层技术的突破为大模型的训练推理提供极致的算力输出，助推国内大模型应用广泛落地，进一步普惠AI。基于天智训练服务器，华鲲振宇已为“蓉城 • 夔（kuí）牛”短临预测模型提供稳定“算力底座”，以成都智算中心300P算力为基础，实现精细化的气象预报服务。备受关注的第31届世界大学生夏季运动会（成都大运会）即将开赛，\"夔牛\"可作为赛场的一匹“黑马”，助力大运会期间的天气预测，把前沿的“黑科技”运用到大运会“智慧气象”服务中，让AI大模型应用于具体场景。</p><p></p><p>以恒致远令融合算力触手可及——“天恒”超融合移动平台解决方案</p><p></p><p>在<a href=\"https://xie.infoq.cn/article/376502f80af0b1d2c672a8e96\">边缘计算</a>\"场景中，客户对计算设备部署的便利性、可移动性以及异构算力融合方面提出了更高的要求。华鲲振宇产品总经理李锐携手华为昇腾计算业务副总裁刘鑫发布业界首创鲲鹏+昇腾“天恒”超融合移动平台，应对应急救援、海上勘测、地质勘探和航拍合成等各种极端恶劣的环境场景， 具备高可靠性和稳定性，轻松完成信号处理、全景视频融合、IoT数据采集与处理、目标检测等功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fbb31cb233ae282c4203a34f6bfad7c.png\" /></p><p></p><p>技术创新让数据中心极致节能——“天极”系列液冷解决方案</p><p></p><p>在国家双碳战略背景下，数据中心算力规模目前持续高速增长，能耗随之急剧扩张，以应用液冷技术为代表的绿色数据中心，成为新基建、“东数西算”的算力底座，如何降低数据中心的能耗成为整个行业技术创新的焦点。华鲲振宇超前洞察，携手华为集群计算业务副总裁王振华发布双液冷解决方案——天极HC8000冷板式液冷和天极HC6000浸没式液冷，挑战PUE小于1.1，HC6000更是基于鲲鹏+昇腾全球首发的浸没式液冷服务器，采用国产原生冷却液，具有极致节能、极稳运行、轻松运维等极致优势，为关键业务场景提供绿色低碳的强劲算力。</p>",
    "publish_time": "2023-07-27 09:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型刷新教育赛道，网易有道发布国内首个教育大模型“子曰”",
    "url": "https://www.infoq.cn/article/RybdOoGtK9NiYTsqmEgx",
    "summary": "<p>7月26日，教育科技公司<a href=\"https://www.infoq.cn/article/5LaZorFzj54vi6YV5xLf\">网易有道</a>\"在“powered by 子曰”教育大模型应用成果发布会上，推出了国内首个教育领域垂直大模型“子曰”，并发布了基于“子曰”大模型研发的六大创新应用——“LLM翻译”、“虚拟人口语教练”、“AI作文指导”、“语法精讲”、“AI Box”以及“文档问答”。</p><p>&nbsp;</p><p>网易有道CEO周枫表示：“一个好的技术有没有价值、能不能发挥巨大的作用，很多时候关键在场景和应用的选择以及细节的打磨。通过软件、硬件、AI技术的结合，做出精品是我们现在做的事”。</p><p></p><h2>有道发布教育大模型“子曰”，六大创新应用成果正式落地</h2><p></p><p>&nbsp;</p><p>大模型的出现能给教育带来的最大机会是什么？</p><p>&nbsp;</p><p>周枫在发布会上表示：“我认为，是助力因材施教”。据介绍，之所以叫‘子曰’，是因为孔子是我国的教育先贤，又是因材施教教育理念的奠基者。‘夫子教人，各因其材’，我们希望子曰大模型可以朝着这样的教育理想去做。”</p><p>&nbsp;</p><p>周枫向在场观众分享了大模型“因材施教”的三大优势。首先，大模型能为学生提供个性化的分析和指导；其次，大模型能够实现引导式学习，与教师一样，提出问题并引导学生自行探索答案；最后，大模型具备全科知识整合能力。通过连接多模态知识库、跨学科整合知识内容，大模型能随时满足学生的动态需求，帮助孩子培养更综合的能力。</p><p>&nbsp;</p><p>相比于通用大模型，“子曰”大模型从一开始就定位为是一个“场景为先”的教育垂类大模型。它能够作为基座模型支持诸多下游任务，向所有下游场景提供语义理解、知识表达等基础能力。基于此，有道研发团队在“子曰”大模型的基础上，为不同学习场景设计了定制化的模型，以实现模型与场景的高度契合。</p><p>&nbsp;</p><p>“我们的目标很明确，就是以实际的教育场景驱动，用技术创新助力教育创新。”周枫表示，希望技术和产品的深度融合，可以根据学⽣不同能⼒和需求，提供因人而异的个性化教学。</p><p></p><p>在发布会上，网易有道展示了“子曰”大模型在多个场景中的应用成果，包括“LLM翻译”、“虚拟人口语教练”、“AI作文指导”、“语法精讲”、“AI Box”以及“文档问答”。</p><p>&nbsp;</p><p>其中，最为瞩目的是大模型时代的英语口语练习神器——虚拟人口语教练Hi Echo。发布会现场，有道词典业务负责人与 Echo 进行了多组随机对话。Echo 能迅速理解场景和上下文，并给出迅速反馈，发音也很地道，重音、弱读、升降调等细节处理得非常到位。在对话过程中，Echo能够像真人老师一样循循善诱，启发式进行对话引导，还能进行实时反馈。对话结束后，Echo会从发音、语法等维度给予建议和润色，能有效解决长期困扰英语口语学习者无话可说、不知从何说起、害怕说错等问题。</p><p>&nbsp;</p><p>“中国人在说英语时往往面临开口难、不敢说、不知道该从何说起的困境，其中的关键就在于缺乏语言环境。” 有道词典业务负责人表示，Echo恰恰能为用户带来这种真正贴合实际的“语境”，帮助他们更好地练习英语口语。</p><p>&nbsp;</p><p>此外，“子曰”大模型还覆盖了多种学习场景。例如，在写英语作业时，学生们不仅有解决具体问题的需求，还需要学会举一反三。“子曰”大模型赋能的“语法精讲”功能可以为学生提供针对性的解题思路和方法，还能推荐同类型的考题，帮助学生触类旁通，真正理解考纲中的考点。</p><p>&nbsp;</p><p>“AI作文指导”应用不仅具备“作文批改”功能，还具备“作文指导”功能。据介绍，该应用旨在解决“学生不会写”和“老师没时间改”的问题。针对学生在写作、前、中后过程中面临的题目主旨难确定、写作素材匮乏等难题，该应用都能够给予指导，帮助学生“下笔如有神”。批改环节中，AI作文指导还会从表达、结构、内容深度、情感丰富度四大维度全面提供改进建议。</p><p></p><h2>场景拉动，“AI+教育”技术沉淀与创新&nbsp;</h2><p></p><p>&nbsp;</p><p>会上，周枫多次强调“场景拉动”的重要性。他表示，“子曰”大模型在教育行业的应用，不仅可以帮助学生更好地学习，也可以帮助老师更好地教学，借此实现因材施教的教育理想。</p><p>&nbsp;</p><p>通过深入调研和分析用户在不同场景下的需求，网易有道成功利用大模型的力量，在教育领域打造了诸如虚拟人口语教练、语法精讲、AI 写作指导等丰富的解决方案。这一策略不仅体现了网易有道对教育场景的深入理解，还为用户提供了更加个性化和高效的学习体验。</p><p>&nbsp;</p><p>早在2008年，有道就推出自主研发的国内首家统计机器翻译线上引擎。经过15年技术迭代，有道神经网络翻译（NMT）已经进化成行业领先的“最强大脑”。根据QUESTMOBILE最新数据，到目前有道词典月活用户已经超过1亿，是国内词典翻译市场的第一名。</p><p>&nbsp;</p><p>从2016年开始，有道协同构建AI基础能力，同步组建语言、视觉、声音等团队，目前积累了有道神经网络翻译（NMT）、计算机视觉、智能语音AI技术、高性能计算(HPC）四大底层技术能力。</p><p>&nbsp;</p><p>自2017年，有道就与主流技术<a href=\"https://www.infoq.cn/article/voRBHu6lzxRuNplYwD0P\">Transformer</a>\"“双向奔赴”，将AI能力统一在大模型之下，并尤其重视在端侧的落地应用。有道词典笔2代2019年首次搭载离线Transformer NMT。2022年，有道词典笔P5中搭载了自研的离线ASR，也已升级为Transformer技术。技术的持续赋能奠定了有道学习硬件在行业内的领先地位。</p><p>&nbsp;</p><p>底层技术不断革新的同时，有道还在不断研发细分场景下的“黑科技”。如虚拟人口语教练实现语音识别能力、虚拟人驱动技术和内容生成和对话能力等多项技术能力的突破。&nbsp;例如，虚拟人口语教练在语音识别能力方面进行了巨大的革新，它支持多语种的流式低延迟语音识别技术，让Echo在中式英语、英语、中英混合等场景下游刃有余；声学降噪、回声消除、自动语音检测、自动断句等技术，则让它像一个真正的倾听者和交流者，不仅能判断用户说话的起始，还能让用户随时打断，智能触发后续流程。</p><p>&nbsp;</p><p>在AI虚拟人的驱动方面，有道基于自主研发的情感识别算法和实时渲染驱动引擎，对播放的语音数据进行深度分析，实时驱动虚拟人的面部表情和语音同步的口型变化，使虚拟人能够贴近真人，以更加自然和生动的方式与用户一对一交互，从而显著提升对话的真实感和用户体验。</p>",
    "publish_time": "2023-07-27 10:49:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型趋势下，企业数据体系有何挑战？有何新机遇？",
    "url": "https://www.infoq.cn/article/0UqI1dHapss161qeXyr3",
    "summary": "<p></p><p>随着<a href=\"https://www.infoq.cn/article/kWKCoO36iYG4RmoGyatN\">大模型</a>\"的兴起，企业数据体系的构建和管理面临着新的挑战和机遇。对于企业来说，如何以最低的成本、最安全的方式，最快速地调整和优化数据体系以支持大模型的训练和应用，成为了一个重要的议题。在刚刚结束的 ArchSummit 全球架构师峰会 2023（深圳站）中，数巅科技创始人、CEO 何昌华博士在主论坛上围绕“大模型趋势下的企业数据体系思考”展开了主题分享，在演讲中，他为大家介绍了他在大模型时代数据处理领域的发展趋势洞察，并简明扼要地为大家介绍了虚拟化技术、物化视图等关键技术与前沿数据理念，现场的架构师们获得了有关未来企业数据架构构建的关键洞察和有效指导，以应对大模型快速发展趋势下的大数据环境。</p><p>&nbsp;</p><p>为了更深度地了解大模型与企业数据治理领域的发展，在演讲结束后，InfoQ 对何博士进行了视频专访，以下是视频专访的全部内容，为方便读者查看，视频下方也附上了文字内容。</p><p>&nbsp;</p><p></p><p></p><p>采访记者：InfoQ 资深编辑鲁冬雪</p><p>采访嘉宾：数巅科技创始人、CEO 何昌华博士</p><p>嘉宾简介：斯坦福大学 PhD，数巅科技创始人、CEO。之前曾经在蚂蚁集团担任计算智能部门负责人，计算存储首席架构师；2017 年之前在硅谷任职于 Google，Airbnb 等互联网公司。在过往职业生涯中，何昌华主导开发过实时智能决策系统、金融级的分布式图数据库、新一代分布式计算引擎、下一代逻辑数仓、新一代搜索引擎架构等。</p><p>&nbsp;</p><p></p><p>InfoQ：您今天的演讲围绕《大模型趋势下的企业数据体系思考》展开的分享，您在演讲中提到了数据治理、人工智能领域的技术发展速度非常快，您能用三个关键词来总结下数据治理与人工智能领域未来发展的主要趋势是什么吗？</p><p>&nbsp;</p><p>何昌华：第一个词是“规模化”。很明显规模化意味着数据、模型的规模，这些都是在极度膨胀，同时也意味着应用场景其实是越来越多，也要呈规模化的发展。</p><p>&nbsp;</p><p>第二个词是“自动化”。以往通过人工来加工各类数据、做各类模型，这个过程会慢慢地被越来越多的自动化流程所取代，因为这样整个流程才能真正地实现自动化，没有人工的干预并且整个系统能够自动往前演进。</p><p>&nbsp;</p><p>还有一个词是“融合”，或者叫“协同”。当前大家可以看到各类来源的数据，会把它融合在一起，成为一个叫做所谓超融合的数据体系。对模型而言，大模型基本上是具备了以前很多个小模型在不同场景下的一些通用能力，而“大模型”和“数据”又应该协同起来。</p><p>&nbsp;</p><p></p><p>InfoQ：在您看来，人工智能和大数据领域最重要的技术挑战是什么？目前是否有比较好的解决方案？</p><p>&nbsp;</p><p>何昌华：我觉得这两个领域有区分，但又是紧密联系在一起的。现在面临的一个非常核心的问题就是如何获得“高质量的数据”，事实上大家可以看到 ChatGPT，它之所以能够实现这样的能力，是因为它完全是在工程上，尤其是在数据策略上做到了极致，但其实本质上它并没有一个理论上的突破。他们有非常多的人，不仅仅是收集到公共数据，他还会去给数据做各种各样的分析、标注以及挑选合适的数据，最后做出了这样的一个模型。而反观我们有很多大模型的公司，事实上都是处在一个相对比较缺乏数据的阶段，尤其缺乏高质量的数据，这是一个非常大的挑战。</p><p>&nbsp;</p><p>另外一个就是“整个计算的能力”，随着模型的规模越来越大，需要的算力越来越高。算力本身的转化，需要大量的设备与金钱来解决问题。这个问题其实还有一些解决方案，至少我们有能力去做大规模的集群，或者去打造一种专用的芯片，能让这个计算的密度变高，这个问题还是容易被解决的。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您在 2022 年创立了数巅科技，在创业之前在蚂蚁集团工作，想问一下您的创业初衷是什么？</p><p>&nbsp;</p><p>何昌华：我在蚂蚁集团的时候就觉得数据智能这个事情在金融行业是非常有可能落地的。因为前一个巨大的场景就是搜索推荐广告（互联网场景）。通过在蚂蚁集团多年的观察，我发现“智能数据治理”不仅只服务于金融行业，它同时能够降低数据智能在很多其他行业的门槛。我们现在讲产业互联网的升级，或者讲企业数字化，它是真的可以帮到各行各业的，所以我们创立了数巅科技。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：数巅科技是一家怎样的公司呢？公司愿景是怎样的？</p><p>&nbsp;</p><p>何昌华：数巅科技的愿景是让数据智能像水电一样简单，让每个人都能方便地使用数据智能技术。这个愿景看似很高端，但我们的初心是让数据智能成为真正人人可用、触手可及的一项能力。</p><p>&nbsp;</p><p>数巅科技目前已经拥有近百人的工程师团队，我们是技术驱动、产品驱动的一家公司，基本上大家都是在雕磨技术。我们的技术目前在世界上还没有完全对标的竞争对手，我们不断创新，打造出独特的技术。我们的团队全力打造这项技术，让它成为一项领先于市场的创新型技术。</p><p>&nbsp;</p><p>数巅科技目前有两个核心产品。第一个产品是最底层的<a href=\"https://www.infoq.cn/article/Re4O8YyDbACbrsOh3R7l\">数据引擎</a>\"，它基于数据虚拟化技术打造，具有通用的数据服务功能。这个引擎提供了虚拟的数据语义，使用户无需关注技术细节，像使用水龙头一样方便直接使用数据。同时，这个产品还采用了智能化和自动化的技术，性能比原来的产品更优秀。这个产品已经在实际场景中得到了应用和打磨，现在在多家企业得到了应用。</p><p>&nbsp;</p><p>第二个产品我们叫大模型的智能助手，它基于现有大模型的技术，充分利用企业内部的数据资产，帮助用户充分协同大模型的能力，从而让生产一个大模型的应用，在企业内部变得非常简单。这个产品可以帮助企业进行智能的 BI 分析，用户可以直接说出指标和看板。同时，这个产品也提供了智能问答功能，可以结合企业内部的规章制度，回答用户提出的问题。这个产品已经在不同的场景下得到了应用和验证，表现出了良好的性能和效果，深受用户喜爱。</p><p>&nbsp;</p><p>我们也做了智能的运营，你提一个问题，它就可以告诉你一个运营方案。我们也做了这样的一些智能的问答，它可以结合你企业内部的规章制度、企业内部的很多数据，回答用户提出的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：据悉，公司定位为“大模型应用和企业数据的协同者”，为何会将此作为公司定位？</p><p>&nbsp;</p><p>何昌华：我们也是一直在思考这个问题，回到公司的愿景，我们还是希望能够让数据智能这个事情，能够很容易的让所有人能够用到。尤其（在）大模型出来的时候，我们就发现大模型它完完全全地降低了模型的门槛。以前我可能要花很大的精力针对一个场景去研发一个模型，当你发现有这样一个大语言模型，它的能力覆盖足够多的通用的场景。</p><p>&nbsp;</p><p>第二个就是我们又发现在企业内部，它拥有大量的数据，甚至很多企业已经有了很比较成熟的数据体系。但是那样的一套数据体系，它以前是完全没有考虑过大模型这样的一个角色的，它那个数据体系里面基本上在做数据分析的一些东西。</p><p>&nbsp;</p><p>所以我们觉得在这两个之间有一个巨大的鸿沟。并且从我们在技术上的理解来看，这两者也必须充分协同，才能够真正地做到企业业务的智能决策，所以我们才把自己定位在这样一个地方，并且我们能够抽象出一个通用的引擎来做这件事情。</p><p>&nbsp;</p><p>InfoQ：听起来其实也不是大模型应用了，我觉得定位更像是“大模型与企业数据的协同者”。</p><p>何昌华：是的，我觉得我们不能算一个大模型的应用。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：刚才您也聊到，咱们现在目前在研发的核心产品是数据虚拟化引擎和大模型的应用框架。我们先来了解一下数据虚拟化引擎，您觉得它的主要的功能和优势是什么？</p><p>&nbsp;</p><p>何昌华：我觉得它最核心的功能上的优势之一，即用户可见的功能。它的确将大数据本身的复杂度全都屏蔽在用户之外了。在大数据过去 20 年的发展中，其实主要是因为数据量太大难以计算，因此才发展出一整套包括批处理、流处理以及各类数据加工，最后再回到现在所谓的一个 <a href=\"https://xie.infoq.cn/article/77ec0d231d36c963a8e6d1630\">OLAP 引擎</a>\"供大家消费。这条链路很长，我们能够看到很多中小企业，根本没有能力搭起这样一条链路。用户去消费数据时，他需要理解前面是什么样的数据和引擎，其理解的成本也很高。因此我们想用数据虚拟化引擎给用户真的提供一个纯数据视角。他作为一个业务开发者或一个业务人员，其所看到的就是一片数据。至于此数据你究竟是用什么引擎处理、存储位置、其计算的内容、他都不用关注，我觉得这是数据虚拟化引擎提供的最核心的一个能力。</p><p>&nbsp;</p><p>我们后面需要把它的性能做到极致，这种情况下，我们也会做很多针对数据消费的语言，我们叫 SQL，针对 SQL 语言的优化包括自动的物化、包括加速等这样的能力。这个引擎可以帮助分析你的数据体系里有多少数据是重复的？有多少计算是现在冗余的？你又如何优化现有的体系？这是能够显著地给企业降本增效的。同时，企业如果用我们的引擎来访问它的数据，它事实上又能够得到更高效的数据访问、更容易地在未来跟大模型来集成。这些都是用户在实际使用过程中会看到的一些优势。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：数据虚拟化引擎的技术特点有什么？刚才您也提到了 SQL，是不是在识别分析优化能力上，它的表现还是很出色的？</p><p>&nbsp;</p><p>何昌华：我觉得这个引擎虚拟化是一个抽象的概念。它非常核心的能力是智能化，智能化就包含我们能够智能地对 SQL 做很多优化，我们能够智能地预测出来。譬如说可能哪一些中间表是需要预先加工出来的，这个技术叫做物化视图。这样下一个 SQL 执行的时候就会更加高效、更快，同时我们也能够做全局智能的编排。SQL 执行下来后，我应该怎么样分解这个任务、怎样去执行它、包括我内部的存储的数据该去有怎样的索引、如何跟外部交互，这些现在均在一个智能引擎的控制之下。这应该也算一条较创新的路。</p><p>&nbsp;</p><p>当然从产品模式上来说，这个智能化的过程又是从产品上开放出接口，让用户完可以完全地、白盒化地控制它，因为事实上很多用户还是希望对中间的数据能有很多手动的控制，与一些自动、手动的处理。在这种情况下，我们完全在产品上会开放出给他们调节的机会。但是整个引擎本身是基于一个智能化的能力的。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>InfoQ：我们还是聚焦到技术上，您觉得数据虚拟化引擎是如何帮助公司实现数据的高效处理和分析？</p><p>&nbsp;</p><p>何昌华：我觉得这一层可以看作是实现了一个逻辑的湖仓一体。现在业界也有这样的一些概念出来，即所谓的逻辑数仓，大家都在基于这个概念在做产品，但我觉得我们其实没有100%的和这个概念匹配，概念上我们相当于是建立起来一个逻辑的数仓，或者叫逻辑的数据集市。</p><p>&nbsp;</p><p>这个逻辑化的体系，其实它是完全针对数据语义的。用户在使用的过程中，譬如说我们给其提供了一个虚拟宽表。现在就是做大数据的从业者，大家都会谈到大数据宽表的概念。数据宽表是非常难以加工，即你要用大量的计算存储才能把宽表加工出来。我们提供一个逻辑宽表很简单，但用户针对逻辑宽表来消费数据时，我们会根据其消费找到对应的物理数据，然后对 SQL 做各类优化处理。</p><p>包括我前置的可能会做一些预先的物化，从而能够让此 SQL 更加高速。这是我们整个链路上很核心的两个能力。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：“数据虚拟化引擎”在整个大模型技术的发展中扮演了一个什么角色？</p><p>&nbsp;</p><p>何昌华：这样的一个数据虚拟化的引擎在我们分析的范围以内，它是最适合于大模型去迭代并做好业务决策协同者的角色的。不容讳言在整个大模型的生态里面，大模型本身它肯定是最重要的能力。大模型的能力越强，肯定越易做出一个很正确的业务决策。但我们也发现大模型的技术本身可能无法处理如此大量的结构化的数据并做精确的计算后来辅助决策，它更多是提供一个逻辑思考的能力。当然这个逻辑思考的能力，需要有语言把它训练出来。本质上，它是在根据企业数据在去做决策的时候提供的一个逻辑思考能力。</p><p>&nbsp;</p><p>关于企业数据本身，它应该提供计算、存储等一些多样化的能力。所以我们认为数据虚拟化引擎是最好的、能够去跟大模型对接的数据工具之一，它能够帮助企业把你的把数据资产完全统一管起来，并且能够加速计算，同时能够跟大模型做好非常充分的交互。</p><p>&nbsp;</p><p>InfoQ：所以您觉得“数据虚拟化引擎”是一个很好的与大模型技术对接的一个工具？</p><p>何昌华：是的，我觉得两者相辅相成。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：关于数巅的另外一个产品“大模型应用框架”的主要特点和优势是什么？它又是如何支持大模型训练和部署的？ &nbsp;</p><p>&nbsp;</p><p>何昌华：大模型的应用框架，本质上我们是想打造一套自己往前能够自我迭代演进的大模型的系统。就像刚刚说的，对大模型而言，我们更多的是用一些开源的大模型做一些微调，我们并没有真正去做一个基础的大模型。但是我们认为一个大模型要在企业内部要成为一个智能的、数字的人，或者帮助做智能角色的助手的角色的话，它一定是需要能自我迭代的。否则的话这个模型很快就会过时，因此它需要能自我迭代。自我迭代的过程在企业内部如何建立起来？以及包括自我迭代需要跟很多业务场景落地，可能需要跟业务场景上的系统打通，它就可能需要连接上各类数据，也包括如何驱动各种自动化的工具来实现这样的业务决策。其中有很多工作，我们这个框架希望能把此类工作沉淀下来后结合我们的大模型以及一个数据虚拟化引擎，这样企业就有了一套完整的，相当于是大模型能够落地，并且能够往前演进，在各个场景下能够快速落地的一套解决方案。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：“大模型应用框架”在大模型应用中有没有优秀实践可以分享一下？</p><p>何昌华：目前已有若干实际案例，这也是我们觉得这样的框架很有意义的原因。我们发现在这个下面，与客户沟通的过程中当我们听到他们有某一诉求时，用这样一套框架去搭配另一个。这种业务场景落地很迅速，基本均以周记，即几周时间你所需的业务场景就可以快速落地了。</p><p>&nbsp;</p><p>其中有若干例子，比如有客户会说我有一堆指标资产，你能否接入它们？同时我拿自然语言说一句话，你就能够把对应的资产给我找到，这个情况下我们就用虚拟化引擎去对接其资产是很容易的，然后去部署一个我们微调过的、开源的大模型，跟着这套框架很快就打造出来这样一个体系，我们叫做智能的指标体系。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您如何看待人工智能和大数据领域的安全和隐私问题？数巅科技有什么措施来保护这些信息？</p><p>&nbsp;</p><p>何昌华：这是一个很好的问题。包括安全隐私类、伦理类问题，这都是现在大家非常担心的，我觉得完全可以理解。</p><p>&nbsp;</p><p>我对这个问题的看法是，首先这肯定是我们需要正视的一个问题，包括我们遇到很多企业就确实存在它们很多数据不能跟公有数据一起去被大模型所训练（的情况），这也是为什么我们觉得有一个企业内部的独立的数据系统可以更好地去管控。</p><p>&nbsp;</p><p>同时包括大模型训练的很多数据，就算在很多数据里面，它也需要做好一些安全性的控制。包括让大模型不要学到很多负面的东西，就像教小孩一样，此类一些区别性的对待，我觉得这都是很关键的一些举措。在我们数巅，我们希望在企业内部能够把数据跟通用大模型的能力隔离开。</p><p>&nbsp;</p><p>大家可以看到，譬如在我们的数据虚拟化引擎里，我们会对数据的权限控制得很严格。我们可以到“行、列”的级别都在控制数据访问权限。当我们去调用大模型的时候，我们可以决定调什么大模型，以及把什么样的数据传给它，这都是有一个非常好的总控的。所以这一块我们是非常关注的一个问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：确实是，目前感觉数巅在安全隐私这块也关注的较多。那您觉得除了安全问题以外，大模型在未来的应用中还有哪些风险和挑战？</p><p>&nbsp;</p><p>何昌华：具体来说的话，模型展现出了很强的能力。因为我们可能在一些金融行业，应该说是在对安全要求比较严格的一些领域里面，我们发现大模型可能除了大家经常说的错觉，有时它也会出一些错误的结果，此类问题之外，可能最需要解决的是可控性的问题。大模型目前的输出结果，因为跟你的上下文相关，所以有时你会发现即便是同样的一个问题，你问它两次，它会给出你不同的答案。我觉得在普通的对话中其实关系不大，但当我要做一个很 critical 的业务决策的时候，这种情况我是不能允许的。我问同一个问题，你肯定应该给出的是同样的一个建议。</p><p>&nbsp;</p><p>在我们数巅，我们也在想办法去解决这类问题。事实上我们发现让大模型把这个执行计划分解的越细，对微小的执行计划我们的可控程度越高，我们能够逐步达到一个相对比较可控的利用大模型智能的一个输出。这个反正也在探索的过程中，但这个我也觉得会是在大模型未来发展过程中，它真正会有非常广泛化的应用。打个比喻，像我们评价人一样，要让我觉得一个人靠谱，肯定我每次问他一个问题，他告诉我的是同一个正确答案，而不是说每次都不一样。我觉得大概可以这样来比喻。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：随着技术及企业业务的发展，您觉得未来企业在数据处理方面将面临的最大问题是什么？数巅科技是否已经在准备应对方案？</p><p>&nbsp;</p><p>何昌华：现在企业经过了这么多年的发展，往往他都有了自己的一套系统，或者甚至是不止一套。我们能够看到很多企业从最开始的数据仓库，到基于大数据的解决方案，到后来的实时，到后来的在线分析。在很多企业内部，它是有很多这样的系统一路发展过来，他们面临的很大问题就是这些系统都是割裂的，从而导致存储是冗余的，导致很难操作一些系统。在这样一些企业内部，他们肯定会希望把所有的东西能够统一地融合到同一套体系里去。</p><p>&nbsp;</p><p>所以，数巅的“数据虚拟化引擎”也是希望为企业提供这样一个选择。当然这个融合的方式可以有多种，可以是数据中台的方案，也可以是其他引擎方案。从大模型角度出发，从怎么去更好地支持未来的数据智能的角度，这是我们相对思考的多一些的内容。</p><p>&nbsp;</p><p>对于我们的产品而言，当我把“数据虚拟化引擎”架在企业的很多套体系的上层的时候，你可以去访问到它所有的数据，对于计算速度较慢的数据，引擎会将其预先拖到自己的缓存中，从而加快计算速度；对于计算速度较快的数据，引擎会直接将请求发送给数据源。有一些它计算得非常快的，我就直接会把请求发给他，在我们内部都有这样的一套自动判断的机制。</p><p>&nbsp;</p><p>所以本质上数据虚拟化引擎，相当于是建立起了这样的一个用户访问数据的一个桥梁。只不过以往大家谈到数据虚拟化这一层就会做得非常的薄。那个时候有一个概念叫 data federation，就是数据联盟、联邦查询、数据联邦，但是那个你做得太薄以后，你就很难确保它能够实现高效的访问。我们相当于是把这个再往前又迈了一步，做的有自己的缓存策略，有自己的智能优化的手段，有自己智能的内核在那里控制，从而让它做到极为高效地来解决这个问题。如果数据虚拟化引擎在一个企业内部能够将其规模充分利用起来，那它可以帮助企业做数据诊断的工作。但同时它到第二个阶段，直接用我们的引擎去访问数据的时候，它也可以帮企业统一它所有的数据体系，提供一个统一的、给业务数据的一个视图。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您理想中企业数据治理的未来是怎样的？数巅科技的下一步技术迭代方向是什么？您是否有信心达到“理想中”的未来？信心来源于哪里？</p><p>&nbsp;</p><p>何昌华：肯定是有信心的，正是因为觉得这个理想有实现的可能才会去尝试。我们的理想是在一个企业内部，通过我们的产品能够充分地把数据能够管好、用好，能够跟大模型深度的协同起来，为企业提供智能的业务决策能力。</p><p>&nbsp;</p><p>我觉得这里面是一个很大的体系，在这其中，我们可能会在数据方面提供更多的价值。有一些公司可能是在做纯粹的大模型，这两者是互补与协作的一个关系。</p><p>&nbsp;</p><p>在数据这一块，我们从最开始一直有一个比较朴素的理想，我们那时候在讲数据自动驾驶这件事，就是说我们认为在一个企业内部，当用户、业务要用数据的时候，他只应该关注这个数据究竟是什么意思？比如说我要用每个人的消费额度，或者我要用一个什么样的数，这个东西我觉得是他应该关注的，其他的都应该是自动把它解决掉的，我们把它叫“数据的自动驾驶”。像今天也有一位嘉宾老师分享到了，比如说自动驾驶从 L1 到 L5，以前的数据我觉得有点相当于是 L1、L2 这样的，或者最多到 L3 这样的。我们可能有一些引擎的能力越来越强，但是用户还是需要去关注所有的事情，我们希望做到的就是用户完全不感知底下的东西，像“我坐上车只要报一个目的地，这个车就会自动开到那”。我们目前在不停地摸索这个过程。</p>",
    "publish_time": "2023-07-27 11:09:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "8年了，Transformer注意力机制一直有Bug？",
    "url": "https://www.infoq.cn/article/tu8SRD5u1ecTQ40lvajH",
    "summary": "<p></p><blockquote>注意力机制是一个构建网络的思路，也是 Transformer 模型的核心。</blockquote><p></p><p>&nbsp;</p><p>注意力是人类认知功能的重要组成部分，指人的心理活动对外界一定事物的指向和集中。在大模型中也有注意力机制。比如神经网络语言模型面对一个由 n 个单词组成的句子时，不同位置的单词，重要性是不一样的。因此，需要让模型“注意”到那些相对更加重要的单词，这种方式称之为注意力机制，也称作 Attention 机制。</p><p>&nbsp;</p><p>简单来说，<a href=\"https://time.geekbang.org/column/article/461691\">注意力机制</a>\"要做的事情就是：找到最重要的关键内容。它对网络中的输入（或者中间层）的不同位置，给予了不同的注意力或者权重，然后再通过学习，网络就可以逐渐知道哪些是重点，哪些是可以舍弃的内容了。</p><p>&nbsp;</p><p>2017 年，谷歌发布《Attention is All You Need》，这也是关于注意力机制最经典的论文。在这篇论文中，谷歌提出了一个新型神经网络架构——Transformer，它完全基于注意力机制，省去了循环层和卷积层。并在 2014 年 WMT 英德翻译任务中达到 28.4 BLEU，比现有的最佳结果(包括集成部分)提高了 2 个 BLEU 以上。结果表明，Transformer 可以很好地将其推广到其他任务，不论是大量还是有限的训练数据。</p><p>&nbsp;</p><p>2018 年，OpenAI 发布了 GPT，谷歌发布了 BERT，两大模型都是基于 Transformer 结构构建的——GPT 是基于 Transformer Decoder 结构和无监督预训练方法实现的生成式预训练语言模型，BERT 是基于 Transformer Encoder 结构的预训练语言模型。</p><p>&nbsp;</p><p>随着近几年 AIGC 技术的爆火以及 ChatGPT 成为现象级应用，Transformer 正以井喷的势头快速发展，基于 Transformer 架构打造的模型数量激增，并将&nbsp;Transformer 的热度推上了新的高峰。</p><p>&nbsp;</p><p>然而近日，Eppo 初创公司的工程师 Evan Miller 在 Twitter 上表示，他发现注意力机制有一个存在了 8 年的 Bug，所有 Transformer 模型（GPT、LLaMA 等）都会受到影响。虽然研究人员上个月隔离了该错误，但他们忽视了一个简单的解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab995ca58d8aa47f479353322887e562.png\" /></p><p></p><p>Miller 在一篇博客中详细阐述了该 Bug，并提出了修复方案。Miller 表示，他在跟 Pytorch 和 biblatex 的一连串交锋中败下阵来。虽然如此，他还是想向大家解释当前这代 AI 模型如何在关键位置出现差一错误，并导致每一个 Transformer 模型都难以压缩和部署。</p><p>&nbsp;</p><p>以下是 Mille 的博客原文，经编译。</p><p></p><h2>这个Bug有多重要？</h2><p></p><p>&nbsp;</p><p>大家发现这个公式中的差一错误了吗？</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/80ad4e6dc46ce5f6f143bc12e24416ec.png\" /></p><p></p><p>首先，我们来聊聊为什么这个差一错误这么重要。ChatGPT的效果不是挺好吗，老兄，你在闹什么呢？其实我第一次察觉有问题，是在认真阅读关于量化的研究论文时。大语言模型的探索者们正尝试将其塞进Mac Mini、Raspberry Pi，甚至是越了狱的家用智能恒温器。</p><p>&nbsp;</p><p>但AI从业者都知道，如今限制最大的因素就是RAM容量。无论是在云端还是边缘，RAM占用量越小、能做的事情就越多。大语言模型拥有几十亿的权重参数，如果我们能想办法帮这些参数成功瘦身，那完全可以写出更好的诗歌、生成更好的综述文章，或者加快人类末日到来的脚步——具体要看大家怎么应用了。</p><p>&nbsp;</p><p>RAM负责存储信息。这好像是废话，但大家先别急。信息属于负对数概率，代表我们需要用多少个bit来存储事物。如果数字流具备高度可预测性，例如总是处于有限的范围之内，那么我们要使用的存储bit量就可以更少。而如果一串数字毫无可预测性，包括偶尔甚至会出现极其巨大的数字，那我们就需要更多位二进制数字对其做编码。</p><p>&nbsp;</p><p>而这正是大语言模型的真实情况。出于种种或明确、或含糊的原因，Transformer模型会包含这些异常权重，用本不必存在的臃肿体态来应对那些发生概率极低、极低的情况。但奇怪的是，似乎没人在意这件事：不对吧，这类罕见异常值与我们之前所认为的、关于构建良好神经网络的一切知识都背道而驰。关于这些异常值，人们已经写过不少论文，也在制定各种“瘦身”方案用更少的0和1对其做编码。顺带一提，直接使用比例和偏差整量化会导致性能大幅下降，所以还得想点更好的办法。</p><p>&nbsp;</p><p>目前关于这个问题的最佳分析，来自高通AI研究团队发表的论文《量化Transformers：通过让注意力头什么都不做来消除离群值》（<a href=\"https://arxiv.org/abs/2306.12929\">https://arxiv.org/abs/2306.12929</a>\"）。作者将这些离群值追溯到了注意力机制中的Softmax函数，之前没人认为这个看似无辜的指数器居然拥有如此野蛮的峰度。其实研究人员马上就要发现其中的差一错误了，但此刻他们可能是在意大利度假，否则怎么就没人回复我发过去的邮件提议呢？于是乎，我只能用最传统的方式向国际学术界发出呼吁。</p><p>&nbsp;</p><p>如果大家读过链接中的论文，请直接忽略他们给出的建议。我知道这话讲得不客气，但裁剪之后的Softmax带有一个旋转的零梯度，而他们的门控注意力建议虽然可行，却要靠引入数百万个新参数来解决一个实质上仅是增量失败的问题。这里明显有个简单且直观的解决方案，但从我接触过的相关讨论来看，似乎没人想到要尝试。</p><p>&nbsp;</p><p>好吧，关于这个愚蠢的错误，我已经介绍得够多了。现在咱们来看看Softmax函数，还有它在注意力机制里惹出了怎样的麻烦。</p><p></p><h2>Softmax惹出了什么麻烦？</h2><p></p><p>&nbsp;</p><p>要想明确解释这个Bug，大家先得真正理解注意力机制是干什么的。大多数数值Bug源自程序员把方程给弄错了。但如果错误不出在代码上、而是出在数学上时，我们就必须先搞清当前方程的来源和它的预期效果，然后才能加以修复。</p><p>&nbsp;</p><p>为此，我不得不认真阅读了大约50篇arXiV文章才逐渐理清思路。关于这个问题的讨论，我们先从所谓“输入嵌入”入手，这是一个浮点向量，表示输入字符串中的一个单词。</p><p>&nbsp;</p><p>这个向量似乎正随着模型的逐年发展而越来越长。例如，Meta最近的LlaMA 2模型使用的嵌入向量长度为3204，其半精度浮点计算结果为6&nbsp;KB以上。是的，这仅仅对应词汇表中的一个单词，而词汇表通常要包含3到5万个单词条目。</p><p>&nbsp;</p><p>现在，如果大家跟我一样是对内存极为吝惜的C程序员，那肯定会考虑：为什么这帮AI蠢货非要用6&nbsp;KB空间来表达这种双字节信息？如果总词汇量不超过216=65384，那我们只需要16个bit就能表示一个条目了，对吧？</p><p>&nbsp;</p><p>确实，人家Transformer其实也是这么干的：它将输入向量转换成相同大小的输出向量，而最终的6&nbsp;KB输出向量会对预测当前token之后的下一token所需要的全部内容进行编码。</p><p>&nbsp;</p><p>Transformer每个层的工作，其实就是把信息添加到原始单字向量当中。这就是残差连接的意义所在：整个注意力机制只是向原始的两个字节的信息添加补充材料，通过分析更多上下文来证明当前文本中的“pupil”是指某位学生、而不该直译为瞳孔。把注意力机制重复个几十次，模型就掌握了英语及其承载的一切广泛内容。</p><p>&nbsp;</p><p>现在，Transformer会将输出向量乘以一个矩形矩阵，再将生成的词汇长度向量填充到Softmax当中，最后把这些指数输出视为下一个token概率。这确有合理性，但大家都知道其中仍有问题。一切广受尊重的模型都承认这种输出概率在所偏差，需要在具体实现中通过采样机制来隐藏Softmax过度代表低概率的事实。</p><p>&nbsp;</p><p>引入采样确实可行。毕竟输出步骤中的Softmax将为我们提供词汇表内每个单词的梯度，在没有更好结果可用的情况下，确实可以先遵循拿来主义。</p><p>&nbsp;</p><p>但我想说的是，就没必要非把不相干的东西搅和在一起。Transformer的输出Softmax与注意力机制的内部Softmax用途完全不同，所以我们最好别跟后者硬靠，或者至少该对分母做点科学调整⛱️。</p><p>&nbsp;</p><p></p><h4>Softmax到底是什么？</h4><p></p><p>&nbsp;</p><p>那么，Softmax到底是什么？它源自统计力学，代表一种根据能级预测状态分布的方法：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43dd5aea2f3e5637e92082b59acab232.png\" /></p><p></p><p>在掌握了它之后，经济学家们意识到，如果人们的线性效用函数中的噪声项恰好遵循Gumbel分布，那么某人选择某个条目的概率就将与效用输入的指数成正比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85c5388f521b8a5418f5411f3cc13d71.png\" /></p><p></p><p>这就为Softmax在多项逻辑函数中赋予了生命力。我在读研究生时曾亲手推导过这个Hessian矩阵，想通过编码用线性固定效应将其运行在GPU上。据我所知，之前和之后都没人蠢到做这样的尝试。说了这么多，就是想告诉大家我跟Softmax函数还真就有那么点缘分。</p><p>&nbsp;</p><p>Softmax其实是一种“作弊码”，能够将实数值映射到总和为1的概率上。它在物理学中的效果相当好，在经济学上有点虚；而在机器学习领域，它已经成为离散选择当中的有效手段。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/1972df915a29a550958d7fe680b8ca7b.png\" /></p><p>这也是Softmax的核心机制：它要求在各种可行方案间做出选择，包括粒子选择能级状态、也包括消费者选择心仪的汽车。也就是说，如果一个Softmax机制根本不打算做选择，那就应该做出修改，否则Softmax一定会在处理真实数据时发生扭曲。</p><p>&nbsp;</p><p>回到大语言模型这个话题上，此类扭曲的一种实际表现，就是对非语义token（比如逗号）进行重点加权，而这就变成了那些难以压缩的离群值，致使模型很难被部署在边缘场景当中。高通AI研究院的报告发现，大语言模型中有超过97%的异常激活发生在空白和标点位置。</p><p></p><h4>具体是怎么出错的？</h4><p></p><p>&nbsp;</p><p>下面我们深入研究一下Softmax在注意力机制中如何起效，看看是哪里出了问题。仍然是这个等式：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c7bc0adac294169e395e8f4b6be6f73.png\" /></p><p></p><p>&nbsp;分解一下：在仅解码器模型中（即ChatGPT之后的所有模型），Q、K和V都来自同一输入序列。虽然它们会在期间被以不同方式投影，所以彼此间并不相同，但在每一层中，它们都始于相同的已注释（已添加）嵌入向量。</p><p>&nbsp;</p><p>现在，𝑄𝐾𝑇在寻找不同位置上token（嵌入）向量之间的相关性，这实际上会构建一个相关性（点积按1/√𝑑‾‾缩放）值的方形矩阵，其中每行和每列对应一个token位置。方形矩阵中的每一行都经过Softmax的处理，得出概率以作为V矩阵中各值向量的混合函数。混合概率后的V矩阵将被添加至输入向量中，再被传递至神经网络内做进一步处理。</p><p>&nbsp;</p><p>多头注意力会在每个层中都经历这个过程，完成多次处理。它基本上就是将嵌入向量划分成几个部分，每个头使用整个微量中的信息来注释输入向量中的一个（不重叠）部分。简单来讲，就是Head 1向Segment 1添加信息，Head 2向Segment 2添加信息，依此类推。</p><p>&nbsp;</p><p>但使用Softmax的问题在于，即使没有什么信息可以添加到输出向量当中，它也会迫使各注意力头进行注释。所以在离散选择中使用Softmax效果拔群，但在可选注释（即输入到加法中）则不太理想。其中，多头注意力又会进一步加剧问题，这是因为专用头往往比通用头更想要“通过”，反而带来了不必要的噪声。</p><p>&nbsp;</p><p>说到这里，是不是该直接让Softmax下课走人？当然不至于，它在大多数情况下都运行良好，唯一的小Bug也只是让注意力头没法“乖乖闭嘴”。所以我提出了一个很小也很直观的调整建议，自从注意力机制在2014年被发明出来，这个办法就一直在每个人的眼皮子底下。</p><p>大家准备好了吗？</p><p></p><h2>修复方案</h2><p></p><p>&nbsp;</p><p>现在，让我们有请睽违已久、经过改造的Softmax Super-Mod闪亮登场：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a3/a363e918e17944b2f361779937523bf9.png\" /></p><p></p><p>感觉也没什么了不起？是的，我只是在分母处加了个1。这就是让整个向量朝着零挪动了一点点，而这将在注意力之后的归一化过程中得到补偿。</p><p>&nbsp;</p><p>修改后的主要区别在于负极限，当x中的条目明显小于零且模型试图回避一次注释时，其表现将与原始Softmax的行为有所不同。先来看原始Softmax：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/533ada43e23552d15cf59b539ac28e36.png\" /></p><p></p><p>以下则是经过改进的新Softmax 1：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d523b13f3dbb74472b20c7bc63a45de.png\" /></p><p></p><p>&nbsp;</p><p>原始Softmax将始终发出相同的总权重；Softmax 1看似区别不大，但在负半轴中设有一处“逃生通道”。再次强调，这里的关键在于数学问题，而非数值问题。进一步提高精度并不能拯救Softmax，所有transformers均受此影响。</p><p>&nbsp;</p><p>Softmax 1还有其他一些特性。因为它的导数是正值，所以我们始终拥有非零梯度；它的和在0到1之间，因此输出不会失控。该函数还具备以下属性，即输出向量中的相对值不变：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5eb604d00133a9589147d739854cbf62.png\" /></p><p></p><p>&nbsp;</p><p>最初我本想把这个函数命名为Ghostmax，因为这里的x中有个额外的零值条目（即exp(0)=1），而V矩阵中有一个会衰减结果的零向量。但别担心，这些不是重点。</p><p></p><p>虽然Softmax 1看似平平无奇，但我有99.44%的把握相信它能解决离群反馈循环的量化问题。</p><p>我们将这种改进机制称为QuietAttention，因为它允许注意力头保持安静：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a05dcbe43e6ca6187b68b19de87a39c2.png\" /></p><p></p><p>大家很快就可以测试起来了。只要在每个输入上下文中添加一个零向量作为前缀，并确保所选择的神经网络不会添加任何偏差（包括位置编码），那么零向量应该就能原封不动通过，并在每个后续的Softmax分母中都加上一个单位。这样，也就不用再纠结什么梯度代码了。</p><p>&nbsp;</p><p>但这种方法要求重新训练模型，所以请先别急着在Raspberry Pi上尝试。希望大家能分享关于权重峰度和激活无穷范数多轮运行后的实际影响，我想把这些数字整理成表格，供arXiV上发表的新论文使用。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.evanmiller.org/attention-is-off-by-one.html\">https://www.evanmiller.org/attention-is-off-by-one.html</a>\"</p>",
    "publish_time": "2023-07-27 11:13:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "关于 To B 业务增长，我们是这么思考的｜ArchSummit 闭门会",
    "url": "https://www.infoq.cn/article/kyQ3ioi751hAmSmJ1XYR",
    "summary": "<p>在经济不佳的当前环境，ToB&nbsp;成长和创新的重要性日益突出。在这样的背景下，<a href=\"https://archsummit.infoq.cn/202307/shenzhen/\">ArchSummit架构师峰会（深圳站）</a>\"举行了一场专门针对&nbsp;ToB&nbsp;增长的技术交流会。在此活动中，我们探讨了各个发展阶的&nbsp;ToB&nbsp;企业所面临的特定增长难题，如何找寻独特的创新解决思路和策略，包括市场定位、产品定制和客户成功管理等。以下为精彩分享摘要～</p><p></p><h4>精彩分享1</h4><p></p><p>首先，对于产品启动阶段，特别是在建立信任方面，我们需要自己刷脸去找客户，或者依靠销售团队与客户直接沟通，以打出标杆并建立信心。其次，产品的品类和市场也影响着SLG（Sales-led&nbsp;Growth）和PLG（Product-led&nbsp;Growth）的属性。SLG更适用于需要多方协同的产品，而PLG则适用于小规模、单一职能的产品。第三点是，随着参与角色和决策链的增多，需要更多的沟通，而小客户则更容易通过PLG方式使用产品。最后，针对协同类产品，虽然它们可能很有用，但可能局限在少数人使用，因此PLG成分可能更高，而相较之下，对于小客户来说，不需要进行过多的沟通。</p><p></p><h4>精彩分享2</h4><p></p><p>我曾创办了一家公司，我们专注于社交媒体舆情分析，并使用自然语言处理（NLP）技术。我们的ToB业务就是通过搜索引擎，将信息整合成报告并尝试在大公司中进行销售，初期我们甚至提供免费报告，逐渐地客户认识到报告的价值，开始为之付费。经过一段时间的技术提升，我们成功将原先需要5个小时才能完成的报告，最后缩短到30分钟，最后甚至5分钟。</p><p></p><p>当时的CEO告诉我，虽然我们可以做一个咨询公司，但他更希望我们能成为一个SaaS模型的公司，后来，我们做到了，成为全球最好的舆情分析公司之一。但是，我要说的是，在SaaS模型中，你必须确保你的用户稳固，否则你的SaaS模型就不能持续下去。我们成功地稳定了市场，成为客户情报调查的市场领先者。我们的SaaS模型已经消化了所有的信息，所有的信息都存储在我们的数据库里。只需要按一个按钮，各种报告、报表就可以出来。</p><p></p><p>但是，我并不认为这样的成功故事可以在中国复现。因为中国的市场与美国不同，中国的客户期望得到全套的专业服务，而非仅仅是一个平台。这种全面性的需求远超过我们在美国的最好客户的支付水平。</p><p></p><h4>精彩分享3</h4><p></p><p>JIRA市值曾达到1000亿美元，现在已降至几百亿，但我们必须承认其影响力。有一个值得我们思考的地方：JIRA在其产品的发展中，很少有直接的竞争对手。我们观察到JIRA将许多SLG的工作推给其代理商来做，尽管看起来公司并没有参与其中，但实际上有许多代理商在做相关的工作。这也与其创始人的风格有关，他们更喜欢简单、标准化的产品，不愿意花费大量的精力在复杂的事物上</p><p></p><p>PLG和SLG都是需要做的，公司可以选择偏重于其中一个。如果我们希望实现更大的增长，可能就需要自己管理相关的成本和方式。对于大型企业，他们通常会选择通过代理商来进行销售。在项目管理这个领域，因为不容易清晰地定义价值，所以更倾向于使用代理商。</p><p></p><p>总的来说，我认为PLG在任何阶段都可以实施，但现在还没有找到完全依靠它就可以支撑收入的产品。SLG在早期启动时对自己有帮助，而且看起来无法避免做大客户，特别是在有多角色协作、多角色使用的情况下。</p><p></p><h4>精彩分享4</h4><p></p><p>我认为目前金融行业在技术和交付能力上还存在许多问题。尽管我们已经在努力奋斗了20多年，但金融客户的生产系统和核心生产系统中仍有90%以上的部分使用过时的技术，这使得推动金融行业的现代化进程变得缓慢且困难。</p><p></p><p>要解决金融行业的现代化和增长问题，除了技术的持续提升外，企业还需要重新定位市场，寻找更有效的增长模式。当然我们也面临着金融行业发展中的几个关键问题。</p><p></p><p>首先，我们渴望在产品中实现授权收费，并获得收入，但目前还没有实现这一目标。其次，我们的企业和产品定位存在问题，导致增长困难。在解决增长问题时，我们尝试着从大的市场转向小的市场，但在实施中遇到了困难。后来，我们的老板借鉴了制造业的概念，希望我们能像制造螺丝钉一样，做一项特定的事情并覆盖全球。因此，我们决定将银网云平台、银管平台和运维平台进行虚拟化，并尝试借鉴他人的成功经验。</p><p></p><p>我们的发展经验表明，核心的产品定位对于组织和技术的变革非常重要，虽然并不是唯一解决办法，但是在应对增长难题时至关重要。我们曾经在云龙初期带领团队开发了类似飞书和钉钉的产品，但后来发现难以推广。后来逐渐精简产品，保留了企业网盘功能，并将其重新包装为政务云盘。这一举措取得了成功，在广东省范围内取得了广泛应用，并有望未来一到两年覆盖整个广东省。</p><p></p><h4>精彩分享5</h4><p></p><p>定位非常重要，我们企业从初创走到现在，虽然我们还未进入成熟期，但是已经非常突出了。首先，我们公司的定位是商用清洁机器人领域第一名。我注意到机器人行业在早期，大多数产品都是展示型的。虽然它们能动，但其实并不能真正实现日常工作的需要。</p><p></p><p>我们是第一个从展示机器人中走出来的公司，我们决定将产品做好，然后，我们开始遇到了各种实际问题，比如环境复杂，机器人不能识别和处理各种不可预见的障碍，甚至像吸管和纸片这样的小东西都会给我们的机器人带来困扰。</p><p></p><p>我们公司在初创期选择了一个非常有意思的技能，即通过算法大幅提高清洁效率。我们优化过的机器人一小时能清理&nbsp;800&nbsp;至&nbsp;1200&nbsp;平方米，这远远超过人的效率。</p><p></p><p>当我们进入成长期，我们的机器人已经可以在工业场景中连续工作几天。但随着扩大规模，我们发现维护成本也等比例上升。尤其是当客户增加了机器人的使用量后，故障率上升，但经过我们的女里，这些问题都被解决了。</p><p></p><p>最后，我们发现这种成本降低并不是由于我们的机器人替代了人工，而是因为机器人的协助。我们的系统能够在物业经理巡逻时，发现需要清洁的房间并报告，然后物业经理就可以继续他们的工作，不需要再跟进。这就导致了一个惊人的结果，原来需要三个物业经理的地方，现在只需要一个，大大节省了成本。</p><p></p><p>我们也思考过我们的机器人系统是否应以SaaS模式出售。当我们的机器人销售出去后，我们是否应该将服务以SaaS模式随机器人一并销售。最后，我们决定不采用SaaS模式。</p><p></p><p>我们的视角是，既然硬件已经销售出去，我们应该在硬件内增加增值服务，客户会很容易接受并愿意支付。买固定资产的决策非常简单。我评估硬件的价值，再考虑包含的增值服务，我们可以接受某种程度的议价决策。采购和&nbsp;IT&nbsp;部门可以快速做出决策，因为他们知道设备可以立即使用。</p><p></p><p>但是，如果我们让业务部门评估SaaS的决策，他们会要求技术部门一起评估，需要考虑许多复杂的因素。我们的机器人销售出去时，我们提供增值服务的方式是，你可以支付额外的费用来开通这个服务。我们不会单独售卖SaaS服务。</p><p></p><p>总的来说，定位非常重要。如果你在一个领域是第一，就有很大的说服力。我们不想与整个行业竞争，我们只是抱着我们的第一名产品出发，然后直接销售。因此，我们认为在初创期和成长期，我们需要找到一个定位。当产品成熟，并普遍被使用时，如果客户不使用我们的产品，反而是他们的问题。</p><p></p><h4>精彩分享6</h4><p></p><p>就部署还是&nbsp;SaaS&nbsp;会增长得更快而言，我是这样考虑的。在讨论企业服务的软件时，我们会把它们分成三类：一是与核心产品有关的，即与核心生产系统有关的；二是辅助生产系统的；三是平常工作、生活以及工作流程协同的工具。对于这三类，越接近外圈的，越类似于传统的财务系统和CIM系统。例如，我们的产品就在这个最外面的圈子。我们不直接帮助客户创造收益，而是间接地提供帮助。而像虚拟化这种技术，就不同了，它至少处于第一或第二个圈子，与生产系统有直接关系。但是总体来说，无论在中国还是全球，越大的企业就会有越多的合规、安全和数据等问题需要处理。我觉得大部分的这个软件产品，特别是像现在我觉得就售部署的场景要求都会很高，特别是大做中大客，在中国做小客基本上做不出来了。</p><p></p><h4>精彩分享7</h4><p></p><p>关于处理用户需求成本太高的问题，我们采取的价格区分策略，这样的处理方式是基于实际情况的需求。比如，国内的一些公司等向我们咨询是否支持某项服务时，我们坦然回答不支持。而当海外分销商咨询同样的问题，我们不能直接说不行，因为他们有大量的潜在客户，他们是全球医药行业的头部。</p><p></p><p>这里逻辑是，一旦我们满足了他们的需求，其他的医疗机构也会效仿，因此我们必须满足他们。我们会采取所有必要的措施来支持他们。而在中国，要实现如此高的要求，特别是在大中型客户中，对小客户的要求基本无法满足。然而，我们为提供服务的理由很简单，因为我们能以三倍的价格出售我们的产品。</p><p></p><p></p><h4>精彩分享8</h4><p></p><p>现在我们的大环境确实面临挑战，我认为整合是一个很好的方式，无论是在行业竞争中还是产品推广中。举个例子，我在前端实施中使用了代理商的推广策略。虽然有更便宜的选项，但我选择了代理商的产品，因为他们提供了更多的资源和空间，能够将产品推广得更广更远。这类似于政务网盘的概念，通过合作伙伴的推广，整合各方面的资源，让产品得到倍数的放大效应。在我们的行业中，我认为这是一个关键的点，通过整合资源来实现产品的成功推广。</p><p></p><h4>精彩分享9</h4><p></p><p>在我的观点中，如果客户对我们所提供的服务的核心竞争力不感兴趣，那说明我们的服务对他们来说并不是必需的。即使我们在产品和服务方面做得再好，在客户看来，这也是我们应有的水准。</p><p></p><p>在我们的内部，我们都会有营造一个故事。我们会告诉产品经理们这个故事，让用户了解我们的理念和方法。这也是我们项目成功的关键。我们要能根据客户的需求，给他们提供合适的产品和服务。</p><p></p><p>另外，我们做产品时，经常会被客户的需求所驱动。尽管有时候这些需求看似不合理，但我们还是会尽力满足他们。在这个过程中，我们也面临着一些问题。例如，我们需要判断一个项目的性质，我们需要为客户提供哪种服务。</p><p></p><p>最后，搞定大客户相当重要，在我们的经验中，我们发现在金融行业有很大的增长。我们有大约700个目标客户，目前已经有一百多个。我们的主要问题是如何解决客户的选择性问题。在过去，他们可能会选择华为、华商，但现在我们也有我们的优势，我们有我们的产品定位，我们用全公司之力把头部银行等一些大型企业纳入我们的服务范围。一旦客户对我们没有选择性的顾虑，剩下的就是销售操作、服务以及产品的可靠性等问题。这些都不是主要问题。</p><p></p><h4>精彩分享10</h4><p></p><p>在我的经验中，了解并满足客户的需求至关重要。一次，我向一位客户解释了我们产品的各项优势，他提出了一些关于安全稳定性的问题。我很奇怪，这些都是我们在过去一年解决过的问题，然后，我向他展示了我们最新的解决方案。但是后来发现，他们并不买账。最后我才了解到要真正满足他的需求，我了解到他在向上级汇报时的需求和困扰。</p><p></p><p>客户跟我关系比较好，坦白告诉我，恰恰是我们我们的产品稳定性强，五六年不需要更换，这让他在第二年没有什么可以向老板汇报的。于是，给了他新的解决方案。从这个案例中，我明白了要成功推销&nbsp;To&nbsp;B&nbsp;产品，我需要站在客户的立场上，解决他们的关键问题。这些问题可能并不会直接向销售人员或解决方案提供者表达出来，但是一旦找到并解决了这些问题，客户就很容易被搞定。</p><p></p><h4>精彩分享11</h4><p></p><p>在我看来，我认为想要在国产领域的立足，除了研发之外，确保我们的产品在目录上有位置至关重要，没有目录位置，没人会来帮我们。这是门槛，而且是硬门槛。举个例子，如果我们有300人研发预算，如果这还不够，我认为我们可以削减50个人，然后把这些资源投入到进入目录，这是我们必须要做的。一旦进入目录，我们可以从一些边缘的创新项目开始，比如销售方面的改进，使我们的应用一体化交付，这是好的，没人会否认在目录里的合规性。</p><p></p><h4>精彩分享12</h4><p></p><p>To&nbsp;B发展很困难，有一个关键原因就是定位，这其中就包含了&nbsp;SLG&nbsp;还是&nbsp;PLG，其实在这个AI行当里面还真需要&nbsp;SLG，并不是个&nbsp;PLG&nbsp;可以搞定的。CEO、CTO都必须亲自走出去，与客户进行深入交流，推进产品。</p><p></p><p>我们也曾尝试了多个方向，但真正能够稳定带来现金收入的成功案例并不多。当然也有例外，也有客户主动找过来的。例如，某市公安局就对我们的技术有强烈需求，他们需要能够在海量人群中快速找到可疑人员的技术。我们在这个领域取得突破后，客户对我们的产品非常满意，这使得我们的业绩大幅增长。这就是一个PLG案例。</p><p></p><h4>活动推荐：</h4><p></p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">FCon全球金融科技大会（2023·上海站）</a>\"是极客邦科技旗下 InfoQ 中国团队推出的面向金融行业高端技术管理者、技术专家的会议，50%参会者拥有 8 年及以上工作经验。</p><p></p><p>FCon 聚焦当前金融行业遇到的问题，围绕金融企业在数字化转型过程中的痛点，例如数据治理，智能化、数字化风控，数字化投研，数字化营销，IT 技术能力等方向，邀请国内外金融企业，来分享人工智能、区块链、大模型、大数据、数字货币等新一代信息技术实践话题，帮助听众解决技术和业务上的问题，提升技术能力。欢迎大家报名参会，<a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">详细信息可点击这里查看</a>\"</p>",
    "publish_time": "2023-07-27 11:17:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于鲲鹏+昇腾，华鲲振宇重磅发布算力创新产品及解决方案",
    "url": "https://www.infoq.cn/article/Nk9DyErUiAA5uH80qUU0",
    "summary": "<p></p><p>2023年7月26日，<a href=\"https://www.infoq.cn/article/iCuurCMJCvofpfylbGpE\">华鲲振宇</a>\"在北京举办产品发布会，以“中华鲲鹏振兴寰宇，昇腾万里智领未来”为主题，分享基于“鲲鹏+昇腾”的算力创新，发布全新一代算力基础设施与解决方案。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/14/dd/14e0b4653e4c39665318e1e3ea64f4dd.jpg\" /></p><p></p><p>此次发布会汇聚产业精英、客户和生态伙伴，共同探讨满足数字时代多样性算力需求的未来发展方向。北京市朝阳区人民政府党组成员、副区长舒毕磊、华为计算产品线副总裁田华、华鲲振宇总裁万诤等领导出席大会并致辞。</p><p></p><h4>深入场景发布四大“天”系产品 全栈创新敢于亮剑</h4><p></p><p></p><p>擎天之力铸坚实算力底座——“天擎”系列高规格通用计算平台</p><p></p><p>随着应用与场景驱动需求激增，对<a href=\"https://xie.infoq.cn/article/372faa42c86908abae0360a08\">算力基础设施</a>\"提出了更多差异化的需求，本次大会，华鲲振宇正式揭开基于鲲鹏创新架构算力产品的神秘面纱，华鲲振宇研发总经理赵彦钧携华为鲲鹏计算业务副总裁马银川正式发布“天擎”系列高规格通用计算产品，最大支持16块NVMe SSD，创新蝶形IO模组设计，最多可支持11个PCIE 标准插槽，规格提升了37%；率先支持9张 PCIe X8的 NPU/GPU加速卡，或两张全高全长双宽的训练卡，整机AI计算性能提升15%以上，各项参数直指鲲鹏最高规格，为千行百业提供高密性能的算力应用平台，已规模服务于运营商、互联网、金融等行业，满足客户在不同部署场景、不同算力、不同存力需求下的计算任务，让每一次计算都能心有所想、算有所成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52d019f2cd0760e857efad515f809a84.png\" /></p><p></p><p>深耕智算助力大模型普惠AI——“天智”系列高密AI算力平台</p><p></p><p>伴随大模型带来的生成式AI突破，人工智能技术正在跨越重要的历史拐点，华鲲振宇也早已有了筹谋，基于昇腾AI计算，在多个典型应用场景进行了深入创新和升级，积极拥抱人工智能，在大模型训练场景下，推出基于昇腾的“天智”系列高密AI算力服务器——新一代训推一体服务器AT800 A2，底层技术的突破为大模型的训练推理提供极致的算力输出，助推国内大模型应用广泛落地，进一步普惠AI。基于天智训练服务器，华鲲振宇已为“蓉城 • 夔（kuí）牛”短临预测模型提供稳定“算力底座”，以成都智算中心300P算力为基础，实现精细化的气象预报服务。备受关注的第31届世界大学生夏季运动会（成都大运会）即将开赛，\"夔牛\"可作为赛场的一匹“黑马”，助力大运会期间的天气预测，把前沿的“黑科技”运用到大运会“智慧气象”服务中，让AI大模型应用于具体场景。</p><p></p><p>以恒致远令融合算力触手可及——“天恒”超融合移动平台解决方案</p><p></p><p>在<a href=\"https://xie.infoq.cn/article/376502f80af0b1d2c672a8e96\">边缘计算</a>\"场景中，客户对计算设备部署的便利性、可移动性以及异构算力融合方面提出了更高的要求。华鲲振宇产品总经理李锐携手华为昇腾计算业务副总裁刘鑫发布业界首创鲲鹏+昇腾“天恒”超融合移动平台，应对应急救援、海上勘测、地质勘探和航拍合成等各种极端恶劣的环境场景， 具备高可靠性和稳定性，轻松完成信号处理、全景视频融合、IoT数据采集与处理、目标检测等功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fbb31cb233ae282c4203a34f6bfad7c.png\" /></p><p></p><p>技术创新让数据中心极致节能——“天极”系列液冷解决方案</p><p></p><p>在国家双碳战略背景下，数据中心算力规模目前持续高速增长，能耗随之急剧扩张，以应用液冷技术为代表的绿色数据中心，成为新基建、“东数西算”的算力底座，如何降低数据中心的能耗成为整个行业技术创新的焦点。华鲲振宇超前洞察，携手华为集群计算业务副总裁王振华发布双液冷解决方案——天极HC8000冷板式液冷和天极HC6000浸没式液冷，挑战PUE小于1.1，HC6000更是基于鲲鹏+昇腾全球首发的浸没式液冷服务器，采用国产原生冷却液，具有极致节能、极稳运行、轻松运维等极致优势，为关键业务场景提供绿色低碳的强劲算力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/519c99276fb37bf7582bdb30fd01b21f.png\" /></p><p></p><p></p><h4>携手伙伴共襄成长 合力向上共建繁荣</h4><p></p><p></p><p>赋予算力时代价值的，并不单纯是算力本身，而是产业上下游的合作伙伴，在本次产品发布会上，华为中国区运营商业务计算解决方案总经理张立鹏、华鲲振宇副总裁宋璇携手20+伙伴共同发布基于华鲲振宇产品的解决方案，持续深化合作，发挥各自优势，深入场景，充分释放出“鲲鹏+昇腾”澎湃算力，给客户持续提供更具价值的解决方案与服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d402fc25cec335efd6b75782a2d5e32c.png\" /></p><p></p><p>此外，华为中国区政企部件销售部总经理李国涛和华鲲振宇高级副总裁于巍波为10+产品合作伙伴正式授牌，感谢一直以来相伴同行。面对风高浪急的国际环境，树木相依偎而生长，星辰因辉映而璀璨，华鲲振宇秉承合作共赢的理念，与关键部件供应伙伴签署长期战略合作协议，与产业伙伴携手共建多样性计算的安全、可靠、连续供应链，最大化保障客户权益，共建繁荣生态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf923c3e65e5ee907932a3d0f46782b7.png\" /></p><p></p><h4>结语</h4><p></p><p></p><p>在计算产业蓬勃发展，技术创新方兴未艾的浪潮中，华鲲振宇此次发布会的成功举办，将为数字经济的快速发展提供强有力支持，并满足市场对差异化、高效能、低碳绿色计算产品的需求。作为鲲鹏+昇腾计算产业的重要参与者，华鲲振宇将坚持技术创新和产品研发，不断推出更优质、更高效能的计算产品和解决方案，为客户和伙伴创造更大的价值，给世界多一种算力选择。</p>",
    "publish_time": "2023-07-27 09:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大数据在大模型趋势下的“新姿态”：大模型与企业数据充分协同",
    "url": "https://www.infoq.cn/article/PV95VcMEDcilhOrNMpPl",
    "summary": "<p></p><p>随着大数据、人工智能和云计算等技术的不断发展，大模型成为了企业数据体系中不可或缺的一部分。大模型趋势下，企业数据体系面临着新的挑战和机遇。比如，大模型的训练需要大量的数据，而数据的收集、清洗和处理需要耗费大量时间和人力。同时，大模型的训练需要高性能计算资源，这需要企业进行大量的投资，而且大模型的训练和推理需要强大的算法和计算能力，这进一步增加了技术难度和成本。</p><p></p><p>然而，大模型趋势也为企业数据体系带来了新的机遇。企业需要加强数据治理、数据存储、数据安全、数据整合、数据分析和挖掘以及业务应用等方面的能力，以应对大模型趋势带来的挑战和机遇，实现数字化转型。为此，在数据治理领域有多年实践经验的何昌华博士在刚刚结束的 ArchSummit 全球架构师峰会2023（深圳站）中就《大模型趋势下的企业数据体系思考》展开了分享，他从“大模型的火爆引发数据处理进入新次元”等行业背景、“大模型时代的数据处理新需求及传统数据架构的桎梏”、“大模型时代的企业数据处理发展趋势”、“企业数据架构演变的前瞻展望”四个方面展开了分享，输出了众多精彩观点。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44be3aed51d41c905c5d486232575b45.jpeg\" /></p><p></p><p>何昌华博士，斯坦福大学 PhD，数巅科技创始人、CEO。之前曾经在蚂蚁集团担任计算智能部门负责人，计算存储首席架构师；2017 年之前在硅谷任职于 Google，Airbnb 等互联网公司。在过往职业生涯中，何昌华主导开发过实时智能决策系统、金融级的分布式图数据库、新一代分布式计算引擎、下一代逻辑数仓、新一代搜索引擎架构等。</p><p>何博士从最开始用 MATLAB 写一些很简单的神经网络，到在 Google 的时候接触深度学习并在业务上有越来越多的应用，到后来蚂蚁时期做大规模搜索推荐的一些模型以及图学习的一些工作，经历了神经网络发展的完整过程。</p><p></p><p></p><h2>一、大模型在企业落地过程中，对“数据体系”有三大需求</h2><p></p><p></p><p>何昌华创建的数巅科技愿景是“让数据智能像水电一样简单”，希望数据智能可以让大家真正在企业内部采用，这本质上是非常高门槛的一件事情。过去半年大模型的飞速发展给行业非常大的震撼，数巅科技近期在大模型上也做了一些事情，探索怎么能让大模型跟企业数据充分的协同起来，这样能够真正释放大模型的潜力，能够释放大数据的价值。</p><p></p><p>很多人说，是不是大模型无穷的发展下去有足够大的规模，企业就可以把所有的数据灌进去，目前主流的数据计算存储体系以后就不再需要了。数巅科技却不是这样认为的——大模型不能取代数据计算存储体系的作用，其实大模型是需要跟企业的数据做深度协同才能够真正地做出一个好的决策。因为这些数据（尤其是结构化数据）的规模远超大模型，并且数据驱动的决策里面往往需要精确的计算。</p><p></p><p>为了让大家更明了地了解企业需求，在演讲中，何博士为大家介绍了大模型在企业内部落地的过程，引用了一个知名投资机构 A16Z 的图示，“事实上现在业界很多的框架也都是这种模式。大家可以看到只有在右边一个小的框里面是大模型，整条链路上其实其它的内容占了非常大的部分。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a3057f390f0a203cbeadfdb867df4136.png\" /></p><p>图：大模型应用一般流程</p><p></p><p>流程的第一部分就是数据的加载和接入。我们都知道，大模型的构建肯定是要用到数据的，大模型可以接到大量各种各样的数据，但是对企业数据体系提出了一个非常重要的需求，就是希望数据是一个统一的高质量资产，可能是多模态的数据、有多种格式，也可能是需要有清晰的血缘定义，要求非常严格。</p><p>第二部分是编排框架。其实在大模型刚出来的时候大家还没有想到这条路，但是后来大家看到 OpenAI 发布了他们的 PLUGINS，它可以调用你的函数。通过大模型给出一些建议以后，调用 PLUGINS Function 真正实现它的功能，在这里最重要的是究竟有多少数据分析、模型甚至是深度的归因分析这样的自动化工具。总之，所有的这一切都是走向企业智能化应用的必经之路。</p><p></p><p>此外，我们还可以看到，大模型跟数据的交互非常频繁。何博士举了一个例子：“当企业要做一个决策，希望通过一个清晰的报表看一下过去三个月的交易额的变化来决定业务未来如何调整，但是真正的大模型可能比人做出更好的决策，它可能看的是 10 个指标，然后做对比进行反复的迭代，最后找到最优指标。在这种情况下，大家可以看到它的计算模式对计算量的需求是有数量级的增长的。”</p><p></p><p>由这个流程我们可以判断，统一的数据资产、自动化工具以及高效的计算能力，这三个需求是大模型在企业内部会碰到的最核心的三个需求。</p><p></p><p>面对这三个核心需求，现有的数据体系目前在这方面遇到很多问题，包括数据中台在内的各种方案都不好解决。计算机科学里面有一句话：当你发现一个问题很难解决的时候，你可以加入一层 indirection，所以数巅科技决定用数据虚拟化的技术来解决这个问题。其实数据虚拟化并不是一个新概念，但数巅科技的做法相较于目前行业还是有比较大的区别和创新，我们可以看下数巅科技的“智能数据虚拟化技术”的架构图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c59005c48b436382637a2a0591498320.png\" /></p><p>图：智能数据虚拟化技术</p><p></p><p>按照从下到上的顺序，我们先看第一层是现在主流的一些数据体系，有各种各样的引擎在系统内计算存储数据。第二层数据连接层，相当于是定义了一个虚拟的能够连接到外部物理层的数据连接，在这一层虚拟化之后，事实上“我的数据”就是数据本身，它跟数据源就被解耦开了，虽然这种情况现在有很多引擎也可以做到，比如 Presto，data federation 工具等，但数巅科技认为在这一层之上还应该有更高层次的数据虚拟化。</p><p></p><p>譬如说企业在上面建立的一些数据虚拟表，这个通常是数据建模的过程，用户会直接消费这些数据表。同时在更上层，企业可以根据业务模型来建一些更上层的虚拟表，一层一层这样起来，大家可以看到整个建设过程其实是完全一个数据的语义，它不涉及到任何系统的改动或者任何中间表的加工。</p><p></p><p>这个概念及架构看似非常简单，但是整套体系最核心的问题就是计算能力。大家知道大数据整个历史之所以发展起来，就是因为数据量太大了，需要有强大的计算能力，需要有中间结果，需要不停地一层一层加工等等。在数巅科技的虚拟化引擎之中，所有这些技术上的实现细节都是通过智能化的手段来完成，譬如自动的生成物化视图，自动 check 数据的血源，非常高效。中间包括虚拟表组织、自动的物化血源以及自动优化这一整套技术功能，我们把它叫做一个数据虚拟化的引擎。因为一般企业已经有数据仓库或者数据湖，或者至少有一些数据源，所以基于企业现有的数据架构，数据虚拟化引擎的位置是在数据源与数据消费者之间。它会给消费者提供一个虚拟视图的数据去消费，消费者只需要关注数据究竟是什么业务语义，而不需要关注数据的“具体位置”、“用什么引擎”、“该如何取用”。与此同时，用户也包含一些与深度学习及大模型紧密连接的操作行为，当大模型进行数据调用的时候，通过虚拟化引擎的接口调用数据会更加的高效。</p><p></p><p></p><h2>二、“数据虚拟化引擎”将切实解决企业业务需求</h2><p></p><p></p><p>企业需求已经明确了，那如何解决这些需求呢？数巅科技给出了解决方案——数据虚拟化技术。面对企业的三个业务需求，数巅科技也分别从三个维度上提出自己的解决方案。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f95e886c37cc71d6ce27036e04c0ea32.png\" /></p><p>图：数据虚拟化技术解决三大需求</p><p></p><p>第一，统一数据资产。用虚拟表的语义很容易接入一个数据资产，定义一个虚拟表，map 外部数据源之后就形成了一份虚拟数据，其它工作都由虚拟化引擎内部执行完成。这其中，虚拟化引擎内置了对外部数据的格式转化，加速以及预处理的过程。</p><p></p><p>当用户在使用虚拟表数据的时候，如果需要新增表，或者衍生出新的数据资产，可以非常快速地新增虚拟表。虚拟化引擎由于是自动化的分析与执行，因此所有的数据血源都是可被追踪的，一旦有完整或部分的 SQL 来访问相同数据的时候，虚拟化引擎就会把它合并掉，资产被定义的 SQL 语义所唯一决定，由此不会存在有二义性的资产。</p><p></p><p>第二，实现数据驱动的自动化工具。在 OpenAI 的大模型里面，可以对接很多 PLUGINS，包括后面发布的支持函数的功能，它可以去帮你订票，可以去做跟第三方应用的对接，这些都属于一个大的生态。当我们聚焦到在企业内部做决策的场景时，自动化工具往往需要的是数据驱动，虚拟化引擎能够给它提供强大的能力。</p><p></p><p>举一个智能运营的场景案例，比如说用户要找到新信用卡发行的运营客群，大模型根据自身知识告诉你“日均交易金额按一定的阶段性区间来分成”这个信号可能非常有用，它就会到数据系统里面去找。在这个过程中，我们会发现，在特征宽表里面没有这样一个特征，但是有另外一个特征叫“日均交易金额”，按照常规流程我们就需要请数据工程师帮忙加工出“日均交易金额的分成”这个新特征，加工完了他还得合并到一个表，再把这个表推到线上服务系统，由我的模型这个工具来访问它才能够 work。</p><p></p><p>但在虚拟化引擎体系里面这个动作可以自动化完成，快速得到新特征，做出更好的决策。整个数据可以自动根据大模型的需求灵活扩充和加工，实现数据驱动自动化的关键一步。如果整条链路自动化，不需要人工加工新特征，才能真正实现大模型效果。</p><p></p><p>第三，高效的计算能力。能不能满足企业需求，最核心的要义就是“足够高效的计算”，包括预计算。通过虚拟表来访问底下的数据，能不能够做到与以前人工加工同样甚至更好的效果。关于这一点，数巅科技有两个核心探索：</p><p></p><p>智能加速，分析 SQL 能够做物化视图的优化</p><p></p><p>物化视图其实在数据库里是一个老课题，但因为数据库涉及到事务性，这一块在数据库里没有体现革命性的变化，然而在大数据分析这个领域，它其实是可以具备革命性的。物化视图技术本质上解决了人工去做 ETL 的过程。</p><p></p><p>数巅科技目前正在分阶段解决这个问题，目前是数据清洗通过人工加工、 手工创建 DWD，到之后自动生成物化视图这种混合模式，但随着阶段的推进，之后做到完全的自动化效果会非常的好。对用户而言，直接消费的主体就是一个虚拟的宽表，不需要感知底层是实时数据还是批量数据，存在哪个引擎，用什么样的存储来处理，所以对用户来说他的体感非常好。</p><p></p><p>真正好用、高效的“计算存储底盘”</p><p></p><p>其实像 data fabric 等虚拟化的概念都是停留在了第一步，没有能够再往前继续走一步，而数巅科技觉得它的体系里面缺少了非常重要的一环——真正的存储。存储在这里是一个高效缓存，有 KV、图等多种格式。另外存储性能也是很关键的能力，但是我们要把存储用 SSD 做到跟内存几乎一样的性能。</p><p></p><p>此外，何博士表示，除了以上三个方面，计算引擎的优化也非常重要——数巅科技认为引擎最核心的能力应该就是 join，他们会把外部的数据做一些预计算，形成中间表放到高效存储，同时会有索引在高效存储里面建立起来。另外如果需要，也会直接去读取外部的数据。在这种情况下，引擎的能力就是企业能够快速地拿到外部数据以后把它 join 成为最后的结果，这个也是数巅科技目前最强大的一个能力。比如有些引擎擅长于做批处理、增量处理，便可以完全复用他们的能力去调用它。</p><p></p><p>据相关数据统计，经过以上这个完整的“技术栈的智能优化”闭环，与主流产品相比，数据查询的性能可以提升 10 倍以上。</p><p></p><p></p><h2>三、大模型与企业数据的“终极未来”：充分协同</h2><p></p><p></p><p>过去大数据和 AI 一直面临着一个问题，那就是虽然具有强大的能力，而且也很容易演示，但是在各种场景下能否被广泛地使用一直是一个挑战。何博士认为，这是判断大语言模型是否是这次工业革命的颠覆者的一个关键判断条件，只有当大语言模型能够在各种场景下被广泛应用，才能真正展现出其潜力和价值。</p><p></p><p>这就意味着，在企业部署大模型之后，构建可以自我演进的大模型框架是一个关键课题，自我迭代的大模型应用框架可以帮助企业根据自己的数据体系来构建大模型应用，让企业数据与大模型充分协同后发挥出最大价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1dd28a44244da31ed31808838b46eb98.png\" /></p><p>图：自我迭代的企业大模型应用框架</p><p></p><p>在大模型与企业数据充分协同这一方面，数巅科技也做了许多探索——自我迭代的企业大模型应用框架。该框架可以接入各种大模型，将用户问题分解为一系列任务并分配执行。执行任务需要使用数据，虚拟化引擎统一管理数据资产，实现了高效计算，执行过程中，进行评估并反馈结果给用户，反馈包括人工标注微调的样本，虚拟化引擎管理数据传回给大模型进行调试。在这个体系中，大模型起到逻辑推理的作用，这是其更稀缺的能力之一。另外值得一提的是，在面对“大模型充当的知识库功能”这一企业诉求中，企业可以完全利用这套数据体系来组织，对比其他方式，它更实时，而且可以说，只要能够跟大模型的逻辑能力交互起来，这个体系就能为企业提供更好的服务。</p><p></p><p>然而，观察当下所有的大模型应用场景，很多人都会疑虑“大模型会不会吃掉一切”，何博士给出的观点非常值得大家思考，“关于大模型的未来，尤其是在企业内部，大模型跟企业数据同时都是需要的，我们要做的事情是让这两者能够充分的协同起来，能够真正在广泛的业务场景下帮助实现智能决策。”</p><p></p><p>目前数巅科技的愿景就是完美实现“大模型与企业数据的充分协同”，正如何博士在演讲后接受 InfoQ 专访时所说的那样，“我希望企业可以通过我们的产品能够充分地把数据能够管好、用好，能够跟大模型深度地协同起来，为企业提供智能的业务决策能力。”</p><p></p><p>以下附何博士在演讲后接受 InfoQ 专访的视频实录，大家可以一起再去深度了解下何博士对于“大模型与企业数据治理领域发展”的观察：</p><p></p><p></p><p></p>",
    "publish_time": "2023-07-27 14:06:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "⻜桨⼤模型推理部署⾼性能优化",
    "url": "https://www.infoq.cn/article/kaWtcvbEK9FRQbxqUdI6",
    "summary": "<p>在《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课第六讲中，百度飞桨架构师邓凯鹏分享了飞桨大模型分布式训练技术。</p>\n<p>首先他对大模型推理做了一个简介，接着具体分析了其需求与难点，之后他详解了大语言模型推理部署高性能优化，即是分析其应用场景。在分析完大模型推理的需求、难点、应用场景后，他又讲解了多模态大模型高性能优化，并在最后进行了总结与展望。</p>\n<p>邓凯鹏先生在此次公开课中有重点并循序渐进地对于大模型推理做了认真细致的讲解，希望本期视频可以为各位答疑解惑、拓展知识。</p>",
    "publish_time": "2023-07-27 15:09:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "企业资深独立 Rust 咨询师张汉东，确认担任 QCon 北京构建未来软件的编程语言专题出品人",
    "url": "https://www.infoq.cn/article/GpCabxjXueW9zZr92d8S",
    "summary": "<p>9 月 3 日 - 5 日，在<a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0727\"> QCon 全球软件开发大会（北京站）</a>\"，企业资深独立 Rust 咨询师张汉东将担任「构建未来软件的编程语言」的专题出品人。在此次专题中，你将了解到具有前瞻性的四个问题：Python 能否承担下一个时代的发展重任；Mojo 语言的横空出世，对 AI 研发生态产生什么影响；Rust 语言会如何应对软硬协同时代的新挑战；WebAssembly 会发挥什么重要作用。</p><p></p><p>张汉东是十六年软件行业从业者，现为企业资深独立 Rust 咨询师，为企业内部落地 Rust 提供解决方案。Rust 中文社区布道者，RustChinaConf 发起人之一，著有《Rust 编程之道》。</p><p></p><p>相信张汉东的到来，可以帮助提升此专题的质量，让你充分了解到未来软件的几种编程语言，以及这些编程语言会产生的影响、面临的挑战和发挥的作用，指明编程语言未来发展趋势。</p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！ 现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-07-27 15:10:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Web开放性或遭重大打击！谷歌四名工程师推出WEI方案，可让广告拦截变成历史",
    "url": "https://www.infoq.cn/article/MZUDtZ5OuqwGqeGpMIRd",
    "summary": "<p>最近，谷歌提出了一项名为“Web环境完整性（WEI）”的提案，由其四名工程师撰写。这是一种确定浏览器是否可信的方法，有助于防范欺诈和其他不良行为。</p><p>&nbsp;</p><p>谷歌在 Chrome 中进行原型设计，目前看起来它已经被推送到了 Chromium 中。另外，苹果去年已经开发和部署了一个极其相似的系统，现已集成到 MacOS 13、iOS 16 和 Safari 中。</p><p>&nbsp;</p><p>但互联网社区中也有人担心，这正是大家最不愿看到的“网络的终结”：“这是对<a href=\"https://www.infoq.cn/article/VUqNUlYvuv6fgpjbjCk0\">开放Web的重大打击之一</a>\"，可能为少数科技巨头主导的数字体系铺平了道路。”“这几乎让广告拦截变成了不可能的事情”......</p><p>&nbsp;</p><p></p><h2>提案背景</h2><p></p><p>&nbsp;</p><p>这项提案于今年4月以代码形式出现，并在5月份正式公布，但并未引起技术社区的过多关注。上周五被定为在拟规范草案并更新后，才引起了关注Chromium开源项目Blink渲染引擎的人们的在意。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/9hwuj1vixx*UWAMzFW0K\">谷歌工程师们</a>\"将WEI描述为浏览器客户端通过第三方（例如Google Play）同服务器建立信任的一种方式，由该第三方提供可证明客户端环境完整性的令牌。</p><p>&nbsp;</p><p>简单来说，WEI为浏览器提供了一种方法，能够证明其正按网站运营商的预期工作、没有受到操纵。举例来说，如果大家拥有一个Web页面游戏网站并想要确定玩家没有作弊，那就可以使用WEI来确定接入的客户端纯粹、合法，且没有运行任何作弊代码。</p><p>&nbsp;</p><p>对于那些不希望自动机器人跑来发帖或点赞的网站，WEI也能帮上大忙——所有参与活动必须通过可接受且未更改的浏览器来完成。对于想要通过浏览器发布内容和广告的经营者来说，肯定不希望自己的努力最后都被发给了机器人。</p><p>&nbsp;</p><p>这也标志着整个Web将走向新的时代：网站只接受经过授权的、正式发布的浏览器。</p><p>&nbsp;</p><p>而由于Chromium不仅是谷歌Chrome浏览器的基础，更是微软Edge、Brave等众多其他浏览器的基础，因此一旦WEI得到部署和采用，很可能对网络产生广泛的影响。</p><p>&nbsp;</p><p>规范草案解释道，“Web环境完整性API允许用户代理向证明者申请裁量，该裁量可用于验证Web环境的完整性。这些裁量将通过管线传送给依赖方，并在期间验证其真实性。Web环境完整性机制最适合检测各类欺诈性Web环境。”</p><p>&nbsp;</p><p>从解释“Web环境”的待办事项相关链接来看，这项提案目前仍未公布太多细节。</p><p>&nbsp;</p><p>该API的既定目标是解决网络上长期存在的各种问题：社交媒体操纵与造假；机器人检测；在应用程序中滥用WebView；批量网络劫持与账户创建；在网络游戏中作弊；入侵设备；以及密码猜测尝试。</p><p>&nbsp;</p><p>然而，这里的“滥用”并没有明确定义。因此，尽管规范作者提到目标是“提供一种对抗性强、且长期可持续的反滥用解决方案”时，但其实很难确定其要打击哪些对象。</p><p>&nbsp;</p><p></p><h2>概念并不新鲜</h2><p></p><p>&nbsp;</p><p>在网络交互当中建立信任的思路并不新鲜。Android和iOS生态系统都已存在用于验证原生应用的类似API。行业内也出现了类似的提案——例如PrivacyPass、Trust Token API以及UserConfidenceScore等。WEI的前身最初于2022年4月被提出，当时就引发了关于设计后果的担忧和质疑。</p><p>&nbsp;</p><p>总体来讲，如果人们不信任建立该技术的实体，那么为Web客户端建立信任机制就会变得更加困难。</p><p>&nbsp;</p><p>WEI于4月底在W3C反欺诈社区小组中接受了审议，并作为浏览器功能开发的正常迭代过程被部分发布到网络之上。</p><p>&nbsp;</p><p>尽管该规范尚处于设计阶段，但上周很快有批评意见出现，开始占领WEI的GitHub repo甚至针对提案作者进行谩骂。谷歌开发者的回应是，将评论权调整为仅向此前为该repo做过贡献的用户开放，同时发布了一份行为准则文件、提醒人们保持文明。</p><p>&nbsp;</p><p>反馈意见中的担忧包括：可能违反欧盟数据规则；所有网络交互都须接受认证——但谷歌明确否认了这一点；新浏览器难以继续发展；人们对谷歌普遍不信任；担心Web领域出现DRM（数字版权管理）；拦截能力可能存在限制等等。</p><p>&nbsp;</p><p>有15年全栈开发经验的Alex Ivanovs就直言谷歌此举就是想让广告拦截变为“不可能”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/37/37b67467c110ce6836e934555eb05e48.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>而更多人认为这不仅仅是广告拦截的问题，这也意味着“竞争”的结束。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/75fe51d132ff93b6e154d747a4be8621.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>仅可在 Chrome、Safari 或 Edge 上浏览互联网（无需修改或扩展），意味着不允许浏览器的竞争。只能在 macOS、Windows、Android 或 iOS 上浏览互联网，意味着不允许操作系统领域的竞争。只有谷歌才能抓取互联网，意味着搜索引擎也没有了竞争。</blockquote><p></p><p>&nbsp;</p><p></p><h2>对垄断的恐惧</h2><p></p><p>&nbsp;</p><p>作为iOS设备越狱工具Cydia的开发者，Jay Freeman（又名「Saurik」）在一篇帖子中，将该提案描述成基于广告的商业模式之下“Web必将踏上的一条不归路”。</p><p>&nbsp;</p><p>在一封采访邮件中，Freeman表示很长一段时间以来，Web最初采取的开放标准、可供任何人构建兼容浏览器的立场早已被打破。如今的软件正变得越来越复杂。</p><p>&nbsp;</p><p>在他看来，由于被塞进了越来越多的功能以满足Web内容发布者的期待，其实只有少数几款浏览器能跟得上时代变化。</p><p>&nbsp;</p><p>“如果网站开始要求「用户证明自己使用的是这小部分值得依赖的浏览器之一，即未对其原始行为做出修改，而后才愿意向真实用户展示其广告」，那么未来市面上恐怕再难觅浏览器新秀的踪影。”</p><p>&nbsp;</p><p>Freeman还认为，WEI的问题绝不仅限于阻碍浏览器市场的正常竞争。</p><p>&nbsp;</p><p>他强调，“我觉得这里还有更大的利害关系——比如剥夺了大家对计算机的控制权。这项功能要想成立，唯一的原因也许就是大多数人都在计算机上采用到DRM技术，比如Arm TrustZone和英特尔SGX。”</p><p>&nbsp;</p><p>“马斯克现在希望每个人只使用官方Twitter应用跟他的服务对接；Reddit近期也在朝着类似的方向发展：向应用程序公开可信计算原语，意味着可以确保仅官方客户端才能正常访问网站。如果谷歌也加入这波潮流甚至予以推动，那么我相信这不仅是对开放Web的重大打击之一，甚至也是对我们迄今为止所看到的、对运行通用计算机的基本自由的严重侵犯——自此之后，我们将无法信任运行在「不可信」操作系统上的浏览器。”</p><p>&nbsp;</p><p>Freeman补充称，“我当然相信谷歌至少在其提出的用例上是诚实的……只是这样的倾向性令人感到不安：广告发布者希望自己基于广告的商业模式继续起效，所以才要求用户只能使用匹配这个前提的受信浏览器。所以这个规范的本质，似乎更像是要求用户自己向内容发布者证明，他们没有运行任何广告拦截器。”</p><p>&nbsp;</p><p></p><h2>提案是否会被搁置或丢弃？</h2><p></p><p>&nbsp;</p><p>本周一，Mozilla公司Web平台高级首席工程师Brian Grinstead也对该提议表达了反对。</p><p>&nbsp;</p><p>“Mozilla公司反对这项提议，因为它违背了我们对于Web原则和发展愿景的初衷。……检测欺诈和无效流量确实是个颇具挑战的问题，我们也有意参与并帮助解决。但是，这项提案没有解释它要如何在列出的用例上取得实质性进展，而且在实际采用后还会带来明显的缺点。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9d935441305efe84fa85ef5937c04d1.jpeg\" /></p><p></p><p>&nbsp;</p><p>在熟悉浏览器技术开发的群体中，微软Edge合作伙伴产品经理、谷歌前高级工程师Alex Russell也在Mastodon上发帖，敦促人们在WEI充分发展成熟前先不要急于做出判断。</p><p>&nbsp;</p><p>Russell指出，“特别是在早期设计阶段，很多想法都大有问题。但没关系，API设计就是需要经历这样的问题空间，而探索其潜力的最佳方法不是推断最坏的情况，而是展示工作内容并证明其至少拥有哪些价值。”</p><p>&nbsp;</p><p>前谷歌工程师、现供职于Tailscale的Chris Palmer上周在另一篇Mastodon帖子上，认为这项提案是个坏主意。</p><p>&nbsp;</p><p>他写道，“远程认证严重冲击了激励措施。如果把客户变成了自己的敌人，那你离失败也就不远了。而如果一套框架把广告发布者的客户变成了自己的敌人，那这东西就应该被扔进垃圾堆里。”</p><p>&nbsp;</p><p>自由技术顾问Ondřej&nbsp;Pokorný同样在Mastodon上表达了类似的观点。“整个「隐私沙箱」和其他想要取代「合法」第三方用例的提案，最大的问题就在于其中提出的各种API把浏览器从用户代理变成了双重代理。这虽然符合广告商和其他企业者的利益，但却往往与用户利益存在冲突。”</p><p>&nbsp;</p><p>Palmer补充称，“最好的结果，就是谷歌明天一早就直接撤回这项提议。其中已经没有任何调整和修复的余地，唯一的选择是丢弃然后道歉。”</p><p>&nbsp;</p><p>面对大量的负面讨论，谷歌工程师给大家初步的回应是： WEI 的目标是提供设备可信的信号，让网络更加私密和安全，并且还留有一个余地，通过保留措施防止平台级别的锁定。比如在一定比例的情况下，例如 5% 或 10%，WEI 证明会被故意省略，看起来像用户选择退出WEI或设备不支持WEI一样。最后他们还强调了会“将继续公开设计、讨论和辩论”，但反对在个人页面讨论问题，随后他们锁定了该Github页面并从个人页面中删除了此提案。</p><p>&nbsp;</p><p>显然，他们的回复作用有限，网友点评：谷歌这相当于在说“我们是好人，相信我！”</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theregister.com/2023/07/25/google_web_environment_integrity/?td=rt-3a\">https://www.theregister.com/2023/07/25/google_web_environment_integrity/?td=rt-3a</a>\"</p><p><a href=\"https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd\">https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd</a>\"</p><p><a href=\"https://github.com/mozilla/standards-positions/issues/852\">https://github.com/mozilla/standards-positions/issues/852</a>\"</p><p><a href=\"https://httptoolkit.com/blog/apple-private-access-tokens-attestation/\">https://httptoolkit.com/blog/apple-private-access-tokens-attestation/</a>\"</p><p><a href=\"https://github.com/RupertBenWiser/Web-Environment-Integrity/issues/28\">https://github.com/RupertBenWiser/Web-Environment-Integrity/issues/28</a>\"</p><p><a href=\"https://beehaw.org/post/6820219\">https://beehaw.org/post/6820219</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=36875226\">https://news.ycombinator.com/item?id=36875226</a>\"</p>",
    "publish_time": "2023-07-27 15:26:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《产业数字人才研究与发展报告（2023）》",
    "url": "https://www.infoq.cn/article/rlOCrXvCZvy11RZ3F1UU",
    "summary": "<p>随着新一轮科技革命和产业变革的深入发展，数字经济正在成为重塑全球经济结构、改变全球竞争格局的关键力量，全球主要经济体均希望通过数字化转型建立更具包容性、竞争力和创新性的新型经济结构。《全球数字经济白皮书 2022》显示，2021 年全球 47 个主要经济体数字经济增加值占 GDP 比重为 45%，毫无疑问，数字经济与实体经济的深度融合，将是未来经济发展的重要动力。而数字人才作为数字经济的核心要素，对推动数字经济高质量发展的作用至关重要。随着各产业数字化转型进入更深的阶段，大量数字化、智能化的岗位相继涌现，相关行业对数字人才的需求与日俱增，人才短缺已经成为制约数字经济发展的重要因素。中国信息通信研究院发布的《中国数字经济发展报告（2022）》显示，2021 年我国数字经济规模达 45.5 万亿元，占 GDP 比重达 39.8%。人瑞人才和德勤中国通过估算发现，当前数字人才总体缺口在 2500 万至 3000 万左右，且缺口仍在持续扩大。</p><p></p><p>我们深知，任何形式的组织变革与科技的运用其成败的根本依然在于人。人是发现问题的主观能动者，是保障组织任务和目标落地的决定因素。我们调研了众多的数字化转型企业，转型收效甚微或失败的比比皆是，究其原因主要有：1. 对数字化转型理解不深，盲目表面地转；2. 只侧重技术的运用，片面地转；3. 看到问题解决问题，缺乏系统性地转；4. 转型过程中，人才和组织机制保障跟不上。</p><p></p><p>在此背景下，人瑞人才与德勤中国携手合作，共同撰写《产业数字人才研究与发展报告（2023）》，希望通过此次研究，观察中国产业数字化进程现状，发掘企业数字化转型中的关键问题，分析数字人才现状与发展趋势，并给出具有针对性的数字人才发展问题解决方案。本报告研究对象涉及政府、企业、求职者及高校多级主体，采用公开政策研究、头部招聘平台数据采集与分析、第三方报告案头研究、企业深度访谈、调查问卷等方法获取企业数字化转型需求、问题及数字人才信息。这其中包括由公司决策者、业务主管、员工和 HR 填答的近 2500 份调查问卷，与 11 个不同行业高管交流得到的近 100 份访谈资料，以及公开的招聘数据信息，报告力求从客观的角度分析、发现各类企业数字化转型中遇到的问题，总结归纳好的经验方法，并针对困扰绝大部分企业的“数字人才不足与培养”问题提出一些针对性的解决方案，以供企业参考，期望能助力更多的企业成功实现数字化转型。</p><p></p><p>本报告第一部分概述中国企业数字化发展的背景与趋势。该部分在明确数字经济内涵的基础上概述了数字人才缺口的总体特征，并细化分析企业组织结构和人才管理体系的现状、挑战，以及应对策略。报告第二、三部分则根据德勤中国在数字化产业和产业数字化的行业经验进一步细化研究，分 11 个不同行业描述企业在人才方面遇到的机遇与挑战。该部分在展示细分行业人才供需情况的基础上，通过胜任力模型给出目标人才具备的能力特征，描绘人才微观画像以指明人才培养方向。报告第四部分将视角转向企业发展，人瑞人才从企业数字转型战略出发，分析数字化时代的产品研发策略和项目管理，并聚焦数字化时代的组织模式与人力资源管理策略，提出“以任务为中心”的组织形态、“基于人才领先”的人力资源战略、“企业内外相结合”的人才供应链体系，以及多元化的用工模式，为数字化转型过程中人才管理与培养问题提供了全面的解决方案，对长期困扰企业的“要不要转型、怎么转型、转型过程中需要注意什么，以及如何更有效地实现转型”等问题给出了归纳性意见及参考思路，对处于数字化转型进程中的企业予以启发。</p><p></p><p>数字化转型并非简简单单地将数字化技术叠加运用在企业管理中，一个企业要实现数字化转型，需要对企业组织架构、业务模式、人才结构、管理体系、企业文化等方方面面做系统性的转化。人瑞人才过去十几年服务中国各个行业著名企业，沉淀了大量专业服务经验并对行业的业务拥有深入理解与研究，德勤中国在各个行业的市场动态、业务发展趋势、企业管理模式等方面拥有深刻洞察与丰富的咨询服务经验，双方合作共同完成的《产业数字人才研究与发展报告（2023）》是国内首次对 11 个重点产业的数字人才发展的全面梳理与分析，对各行业企业的数字化转型和人才管理具有重要的参考价值。在此我们也希望借本书抛砖引玉，引发更多针对数字人才发展的讨论，并共同推进中国数字经济深入发展。</p><p><img src=\"https://static001.geekbang.org/infoq/3d/3dcfa9b13d886182faea3e38a5020150.png\" /></p><p>扫/码/下/载</p>",
    "publish_time": "2023-07-27 15:44:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯云数库技术实践精选集 2022年版",
    "url": "https://www.infoq.cn/article/SeMVeacdwmuhvY7wQzz6",
    "summary": "<p>在过去一年中，来自腾讯云的技术专家，通过 DBTalk 技术公开课、QCon 全球软件开发大会、ArchSummit 全球架构师峰会以及 DIVE 全球基础软件创新大会，分享了腾讯云数据库的技术探索与不同场景下的落地实践。</p><p></p><p>《腾讯云数据库技术实践精选集》2022 版，收录了全年 40 余场分享的内容，也涵盖了优秀企业数据库应用的成功案例，相信一定能给数据库领域的各位同仁带去一些思考和启发。</p><p></p><h3>精选集内容：</h3><p></p><p>36 位数据库技术专家演讲实录涵盖多业务场景数据库应用案例全方位分享国产数据库建设思考</p><p></p><p>展望未来，国产数据库的“进化”之路仍任重道远，愿与君携手，百尺竿头，更进一步！</p><p><img src=\"https://static001.geekbang.org/infoq/35/3556f6bfcadd377a448effae3da0cb79.png\" /></p><p>扫/码/下/载</p>",
    "publish_time": "2023-07-27 16:49:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "财报会议新时代：如何将AI训练成资深CFO",
    "url": "https://www.infoq.cn/article/MPINGBSC8woTh558i7Fq",
    "summary": "<p></p><blockquote>这是由&nbsp;AI&nbsp;平台公司&nbsp;Lepton&nbsp;和著名&nbsp;LLM&nbsp;工具平台&nbsp;LangChain&nbsp;合著的，关于如何通过微调的方式，来实现更加“聪明”的大模型的实践文章。InfoQ经Lepton授权后翻译，如需转载译文，请联系InfoQ申请转载授权。原文链接：<a href=\"https://mp.weixin.qq.com/s/RPgc-56EH8ti0dnCmp0pBQ\">https://mp.weixin.qq.com/s/RPgc-56EH8ti0dnCmp0pBQ</a>\"</blockquote><p></p><p>&nbsp;</p><p></p><h2>简介</h2><p></p><p></p><p>各位有没有参加过财报电话会议，特别是向CFO们抛出一个个尖锐问题？曾几何时，这是只有摩根大通、高盛和摩根士丹利这帮高端投资者才具有的特权。</p><p></p><p>但随着大语言模型的普及，加上适当的技术调教，如今情况已大不相同。如果嫌内容太长，大家也可以直接访问此处的演示（<a href=\"https://earningsage.lepton.run/\">https://earningsage.lepton.run/</a>\"），其中数据来自苹果2023年第二季度财报电话会议上的内容。</p><p></p><p></p><blockquote>提问：您觉得生成式AI对苹果产生了哪些影响？财报机器人：谢谢您的提问。如您所知，我们多年来一直在产品中使用AI技术。我们用它让产品更智能、更个性，并创造出新的、令人惊叹的使用体验。有些朋友可能一直觉得苹果只在自身产品上投资，但其实我们也在投资AI。我们为此设立了庞大的专项团队，也对所取得的进展感到非常兴奋。而且我们坚信，AI应该面向所有人开放，我们也在致力于解决人工智能的可解释性问题。我们正努力攻克难关，打造出透明、负责任且用于公益的AI方案，给所有人带来巨大收益。</blockquote><p></p><p></p><p>下面，我们将一同走近尖端技术与财务智能的结合领域，深入探讨AI技术的应用与变革过程，了解它究竟是怎么学会像资深CFO一样侃侃而谈的。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/65/65f3ad6709c81bd049490c4111992a36.png\" /></p><p></p><p></p><h2>&nbsp;&nbsp;&nbsp;</h2><p></p><p></p><h2>问题陈述</h2><p></p><p></p><p>首先，我们需要将上述挑战拆分成一个抽象概念，希望能帮助大家从工程层面理解我们面临的问题。简而言之，这个问题可以转换成：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d57baf117aea54e9d0d321d9de43596e.png\" /></p><p></p><p></p><p>将原始财报会议记录、文本生成模型（可能主要是OpenAI&nbsp;ChatGPT&nbsp;3.5）和工具集（Python、Lanchain、Chroma等）有机结合起来，从而模仿CFO的言谈举止。</p><p></p><h2>关于流程的思想实验</h2><p></p><p></p><h3>从OpenAI起步</h3><p></p><p></p><p>我们先使用来自OpenAI的ChatGPT&nbsp;3.5和Langchain检索QA链，这目前基本是有意构建AI应用程序的开发者们的标准操作流程。现在，我们的解决方案如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/905e611bda4646808f3bad52ac80c479.png\" /></p><p></p><p></p><p>不出所料，ChatGPT&nbsp;3.5面对这些简单问题表现得相当从容。比如，我们可以问它“这次财报电话会议涵盖了哪些内容？”从原型设计角度看，开源工具已经相当成熟。所以我们的首个产品版本很快就能构建完成。但面对更复杂的问题，ChatGPT却只能乖乖投降。</p><p></p><p>以下是ChatGPT&nbsp;3.5回复的完整问题列表：</p><p>本次财报电话会议讨论了哪些内容？(<a href=\"https://smith.langchain.com/public/dbac4723-5a7d-4c63-914d-90f651c2ad22/r\">https://smith.langchain.com/public/dbac4723-5a7d-4c63-914d-90f651c2ad22/r</a>\")每位iPhone用户持有的苹果数量，拥有多大的增长潜力？(<a href=\"https://smith.langchain.com/public/ecc37b09-8528-49be-b694-5f4d73258c80/r\">https://smith.langchain.com/public/ecc37b09-8528-49be-b694-5f4d73258c80/r</a>\")苹果公司在AI领域有哪些未来战略？&nbsp;(<a href=\"https://smith.langchain.com/public/e3d0b2da-b1cc-4e38-963c-d4607dc7278f/r\">https://smith.langchain.com/public/e3d0b2da-b1cc-4e38-963c-d4607dc7278f/r</a>\")目前市场对于Apple&nbsp;Pay&nbsp;Later作何反馈？(<a href=\"https://smith.langchain.com/public/a62fb0ac-91f4-4581-91f5-5266a5871d3a/r\">https://smith.langchain.com/public/a62fb0ac-91f4-4581-91f5-5266a5871d3a/r</a>\")</p><p>&nbsp;</p><p></p><h3>之后，我又试了试初版Vicuna</h3><p></p><p></p><p>我是无意间听朋友说起Vicuna的。身为开源开发者（我在Jupyter&nbsp;Lab工作），我决定亲自试试。这是款开源聊天机器人，基于ShareGPT收集到的用户共享对话对LlaMa进行微调而成。到这一步，我们的解决方案如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/935eacc3454c18a0eb2d73abea12bed4.png\" /></p><p></p><p></p><p>这里最棘手的部分，在于该产品的首个版本是基于Langchain构建的，而Langchain最初又是基于OpenAI的API构建的。跟其他提示工程框架类似，在不同模型间往来迁移时总要面对很多兼容性问题。比如其他模型可能不提供同样的嵌入API端点，或者其tiktoken库不支持某些模型等。</p><p></p><p>考虑到这个问题，Lepton.AI（https://www.lepton.ai/）的工程团队找到了一种方法，能让模型与原始OpenAI的API端点相兼容，这就让大语言模型应用的模型切换变得更容易。现在，模型服务能够轻松改变环境，使用户得以简单完成模型切换。例如，由：&nbsp;</p><p></p><p><code lang=\"null\">OPENAI_API_BASE=https://api.openai.com/v1\nOPENAI_API_KEY=YOUR_OPEN_API_KEY</code></p><p></p><p>切换为：</p><p></p><p><code lang=\"null\">OPENAI_API_BASE=YOUR_DEPLOYMENT_URL\nOPENAI_API_KEY=YOUR_LEPTON_AI_API_KEY</code></p><p></p><p>乍看之下，结果已经相当可靠，但我们还得找到能准确评估输出结果的办法。这时候就要请出LangSmith&nbsp;(<a href=\"https://smith.langchain.com/\">https://smith.langchain.com/</a>\")了。它能帮助我们添加四行代码来轻松更改环境变量，并接手解决余下的所有工作。</p><p></p><h2>事实证明，微调模型性能更好</h2><p></p><p></p><p>尽管原版模型不会像ChatGPT&nbsp;3.5那么快放弃，但在语言表达方面仍然没有CFO那个“范儿”。换言之，它的谈话方式给不了我参加顶级金融人才云集的财报电话会议那种感觉。</p><p></p><p>因此，受&nbsp;llama&nbsp;微调模型Vicuna的启发，我决定用部分财报会议数据微调一个领域模型。通过从记录中收集到的数据，我成功提取到大量会议记录。之后使用TUNA（一种面向数据和模型的增强服务），我顺利构建起更熟悉财报会议背景的模型。到这里，我们的解决方案已经成了下面的样子：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/09/0909fbc309603df2a112d16c85d22680.png\" /></p><p></p><p></p><p>以下是问题列表对应的部分查询结果：</p><p>本次财报电话会议涵盖哪些内容？(<a href=\"https://smith.langchain.com/public/80e7f6b6-8ea7-41b9-b875-55781b1c1df8/r\">https://smith.langchain.com/public/80e7f6b6-8ea7-41b9-b875-55781b1c1df8/r</a>\")每位iPhone用户所持有的苹果设备数量，还有多大的潜在增长空间？&nbsp;(<a href=\"https://smith.langchain.com/public/7f14895a-d6e5-45b9-98bf-bc128954de45/r\">https://smith.langchain.com/public/7f14895a-d6e5-45b9-98bf-bc128954de45/r</a>\")苹果公司在AI领域有哪些未来战略？(<a href=\"https://smith.langchain.com/public/a2a6d24b-fd46-46f4-9850-b414f0b716f0/r\">https://smith.langchain.com/public/a2a6d24b-fd46-46f4-9850-b414f0b716f0/r</a>\")目前市场对于Apple&nbsp;Pay&nbsp;Later作何反馈？（<a href=\"https://smith.langchain.com/public/34555f2f-79a6-4532-bb0e-09727100c2a8/r\">https://smith.langchain.com/public/34555f2f-79a6-4532-bb0e-09727100c2a8/r</a>\")</p><p></p><p>同样的，我对代码做出的唯一调整就是OPENAI_API_BASE，其他均可保持原样。在LangSmith的帮助下，我得以快速比较输出结果，并将其分享给其他感兴趣的朋友。</p><p></p><h2>总结</h2><p></p><p></p><p>总体来看，将数据与大语言模型技术（包括数据增强和微调）相结合，标志着AI应用开发迎来了一个关键里程碑。通过将大量多样性数据集同大语言模型对接起来，我们释放出了前所未有的潜力，让AI系统能够生成更准确、具备上下文感知且输出连贯顺畅的结果。数据与大语言模型间的协同作用不仅增强了AI应用的整体性能，同时也为更多创新和发现开辟出新的可能性。</p><p></p><p>随着不断完善和扩展对这种动态关系的理解，我们正踏上一段新的旅程，用数据驱动见解和高级语言模型的融合重新定义更多可能性，并有望最终步与卓越AI新时代、彻底改变我们的交互方式与技术手段。未来就在前方，我们正满怀信心地迎接AI超越预期、成为进步路上不可或缺的全新资产类型这一伟大愿景。</p><p></p><p>关于本文提到的工具，LangSmith和LeptonAI目前仍处于内测阶段，大家可以注册候补名单、申请亲自体验。另外，也欢迎大家给我写邮件（uz@lepton.ai），期待听到大家的感受和意见。</p><p></p><p>相关链接：</p><p>LangSmith——用于调试、测试、评估和监控大语言模型应用的统一平台&nbsp;(<a href=\"https://smith.langchain.com/\">https://smith.langchain.com/</a>\")。Lepton.AI——以简单方式构建AI方案&nbsp;(<a href=\"http://lepton.ai/\">http://lepton.ai</a>\")。本文示例代码——欢迎大家随意使用(<a href=\"https://github.com/bobmayuze/Earning-Sage/tree/main\">https://github.com/bobmayuze/Earning-Sage/tree/main</a>\")。</p>",
    "publish_time": "2023-07-27 17:45:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "关于Node.js调试，你需要了解的一切",
    "url": "https://www.infoq.cn/article/HtebZgQ4aKVItoM0b0vM",
    "summary": "<p>Node.js 是一种颇具人气的 JavaScript 运行时，与谷歌 Chrome 浏览器一样采用同款 V8 引擎。</p><p></p><p>Node.js 具备跨平台属性，目前已经成为服务器端 Web 应用程序开发、工具构建和命令行应用程序等领域的主流选项。</p><p></p><p>但体验过 Node.js 的朋友往往发现，一旦编写代码并尝试运行，往往难以轻松处理深藏其中的问题。幸运的时候，代码崩溃还能显示明确的错误信息；但如果运气不好，应用程序仍能勉强运行，只是结果与开发者预期相去甚远。</p><p></p><p></p><h2>什么是调试？</h2><p></p><p></p><p>所谓调试，就是修复软件缺陷的艺术。修复 bug 并不高深，大多数问题其实就是由字符错录或代码行里的小问题引发，但查找 bug 却是无缘艰难。开发人员往往得花上大量时间才能抽丝剥茧、厘清问题的根源。</p><p></p><p>以下几种方法能帮助大家有效规避错误：</p><p></p><p>使用高质量的代码编辑器，应具备行编号、彩色编码、代码校验、自动补全、括号匹配、参数提示等功能。使用 Git 等源代码控制系统以管理代理修订工作。这些工具能帮助开发者检查更新，定位 bug 出现的方式、时间和位置。采用 bug 跟踪系统，例如 Jira、FogBugz 以及 Bugzilla 等。它们能向开发者报告 bug、高亮显示重复项、记录重现步骤、确定 bug 严重性、计算优先级、分配开发人员、记录讨论线索并跟踪修复进度。使用测试驱动开发（TDD）方法。TDD 是一种开发过程，鼓励开发者在编写函数之前先用编码测试该函数的运行效果。尝试使用代码解释或结对编程等方法同其他开发者携手合作，对方提供的全新视角能帮助我们发现自己遗漏的问题。</p><p></p><p>但没有哪种解决方案能够直接消除所有错误，而且任何一种编程语言都免不了出现以下几种错误类型。</p><p></p><p></p><h2>语法错误</h2><p></p><p></p><p>如果代码内容未遵循某些语言规则，就会触发错误。常见的语法错误包括拼写错误或缺少括号等。</p><p></p><p>VS Code 等优秀代码编辑器能帮助大家在实际运行代码之前，预先检查各种常见的 Node.js 问题：</p><p></p><p>将有效和无效语句标记为彩色形式；自动补全函数和变量名称；高亮显示匹配的括号；自动缩进代码块；为函数、属性和方法提供参数提示；检测无法访问的代码；重构混乱函数。</p><p></p><p>可以使用 ESLint 等代码检查器寻找各种语法问题，或者不符合正常编码风格的情况。使用以下命令，即可将 ESLint 安装为全局 Node.js 模块：</p><p></p><p><code lang=\"nginx\">npm i eslint -g\n</code></p><p></p><p>而后通过命令行检查 JavaScript 文件：</p><p></p><p><code lang=\"css\">eslint code.js\n</code></p><p></p><p>ESLint for VS Code 扩展程序的效果更好，能在我们输入的同时对代码内容做验证：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/00/0095efc1c9b27e2600eaf21f38f07385.png\" /></p><p></p><p></p><h2>逻辑错误</h2><p></p><p></p><p>逻辑错误意味着我们的代码可以运行，但却无法达成预期的效果。例如，用户无法使用有效凭证正常登录；报告中的统计信息不正确；用户数据未被保存至数据库等。引发逻辑错误的原因多种多样，包括：</p><p></p><p>使用了不正确的变量名称；使用了不正确的条件，例如应该是 if(x&gt;5) 而非 if(x&lt;5)；使用了无效的函数、参数或算法。</p><p></p><p>我们往往需要分步执行代码，并在过程当中检查特定的运行状态点。</p><p></p><p></p><h2>运行时错误</h2><p></p><p></p><p>运行时错误主要影响的是应用程序的执行过程。代码执行可能并不出错，但也随时可能被无效的用户输入而意外触发。例如：</p><p></p><p>尝试将某个值除以零；访问目前已不存在的数组项或数据库记录；在不具备适当访问权限的情况下，尝试写入文件；不正确的异步函数实现会引发“内存溢出”崩溃。</p><p></p><p>众所周知，运行时错误往往很难重现，所以保持良好的日志记录习惯至关重要。</p><p></p><p></p><h2>Node.js 调试中的环境变量</h2><p></p><p></p><p>主机操作系统中的环境变量负责控制 Node.js 应用程序的具体设置。最常见的环境变量是 NODE_ENV，一般在调试时被设定为 development、在 production 过程中则被设定为 production。</p><p></p><p>大家可以在 Linux/macOS 上这样设置环境变量：</p><p></p><p><code lang=\"ini\">NODE_ENV=development\n</code></p><p></p><p>在 Windows（旧版 DOS）命令行中这样设置：</p><p></p><p><code lang=\"bash\">set NODE_ENV=development\n</code></p><p></p><p>在 Windows Powershell 上则是这样设置：</p><p></p><p><code lang=\"ruby\">$env:NODE_ENV=\"development\"\n</code></p><p></p><p>应用程序可以检测环境设置，并在必要时启用调试消息，例如：</p><p></p><p><code lang=\"javascript\">// running in development mode?\nconst DEVMODE = (process.env.NODE_ENV === 'development');\nif (DEVMODE) {\n  console.log('application started in development mode');\n}\n</code></p><p></p><p>NODE_DEBUG 需要使用 Node.js util.debuglog 来启用调试消息（后文中的 Node.js util.debuglog 部分将具体介绍）。另外，请注意检查主模块和框架的说明文档，了解更多日志记录选项。</p><p></p><p></p><h2>使用 Node.js 命令行选项进行调试</h2><p></p><p></p><p>在启动应用程序时，您可以将命令行选项传递给 node 或 nodemon 运行时。其中最有用的选项之一当数—trace-warnings，它会在无法解析或拒绝 promise 时输出栈跟踪信息：</p><p></p><p><code lang=\"css\">node --trace-warnings index.js\n</code></p><p></p><p>其他选项包括：</p><p></p><p>–enable-source-maps: 使用 TypeScript 等转译器时，启用源映射–throw-deprecation: 在使用已被弃用的功能时，抛出错误–inspect: 激活 V8 检查器（具体请参阅后文中的 Node.js V8 检查器部分）</p><p></p><p></p><h2>使用控制台日志进行调试</h2><p></p><p></p><p>最简单的应用程序调试方法，就是在执行期间将值输出至控制台：</p><p></p><p><code lang=\"javascript\">console.log(`myVariable: ${ myVariable }`);\n</code></p><p></p><p>有些开发者坚持认为 console.log() 这东西没有意义，因为代码本身一直在不断变更，而且还有更好的调试选项可用。话虽没错，但大家还是会经常用到 console.log()，而且任何能提高编程效率的工具都有价值。控制台日志就是这样一种快速且实用的选项，能帮助大家切实找到并修复 bug。</p><p></p><p>当然，除了标准的 console.log() 之外，大家也可以考虑其他几个选项：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/40/401d7814a6974ae5d50bd6d5a4728790.png\" /></p><p></p><p>console.log() 支持以逗号分隔各值的列表，例如：</p><p></p><p><code lang=\"javascript\">let x = 123;\nconsole.log('x:', x);\n// x: 123\n</code></p><p></p><p>ES6 的解构赋值能以更简洁的方式提供类似输出：</p><p></p><p><code lang=\"javascript\">console.log({ x });\n// { x: 123 }\n</code></p><p></p><p>util.inspect 能够格式化对象以方便阅读，而 console.dir() 能会帮助大家完成其他费时费力的工作：</p><p></p><p><code lang=\"css\">console.dir(myObject, { depth: null, color: true });\n</code></p><p></p><p></p><h2>Node.js util.debuglog</h2><p></p><p></p><p>Node.js 的标准 util 模块提供 debuglog 方法，能够按特定条件将日志消息写入 STDERR：</p><p></p><p><code lang=\"javascript\">const util = require('util');\nconst debuglog = util.debuglog('myapp');\n\ndebuglog('myapp debug message [%d]', 123);\n</code></p><p></p><p>当大家将 NODE_DEBUG 环境变量设置为 myapp 或通配符形式（例如或*my*）时，控制台会显示以下调试消息：</p><p></p><p><code lang=\"nginx\">MYAPP 4321: myapp debug message [123]\n</code></p><p></p><p>其中 4321 代表 Node.js 的进程 ID。</p><p></p><p></p><h2>使用日志模块进行调试</h2><p></p><p></p><p>Node.js 支持各种第三方日志记录模块，我们可以根据需求具体选择消息传递级别、详细程度、排序、文件输出、分析、报告等：</p><p></p><p>cabinloglevelmorgan (Express.js 中间件)pinosignalestoryboardtracerwinston</p><p></p><p></p><h2>使用 Node.js V8 检查器进行调试</h2><p></p><p></p><p>Node.js 是围绕 V9 JS 引擎构建的打包器。V8 引擎中包含自己的检查器和调试客户端，这里就从检查参数起步（注意，不要将其与后文中「使用 Chrome 调试 Node.js 应用程序」中提到的—inspect 标志混淆）：</p><p></p><p><code lang=\"css\">node inspect index.js</code></p><p></p><p>调试器会在第一行暂停，并显示以下 debug 提示：</p><p></p><p><code lang=\"ruby\">$ node inspect index.js\n&lt; Debugger listening on ws://127.0.0.1:9229/b9b6639c-bbca-4f1d-99f9-d81928c8167c\n&lt; For help, see: https://nodejs.org/en/docs/inspector\n&lt;\nconnecting to 127.0.0.1:9229 ... ok\n&lt; Debugger attached.\n&lt;\nBreak on start in index.js:4\n  2\n  3 const\n&gt; 4   port = (process.argv[2] || process.env.PORT || 3000),\n  5   http = require('http');\n  6</code></p><p></p><p>输入 help 可查看命令列表。大家可以使用以下步骤逐步跑通应用程序：</p><p></p><p>cont 或 c: 继续执行next 或 n: 运行下一条命令step 或 s: 单步执行被调用函数out 或 o: 跳出被调用函数并返回其调用者pause: 暂停运行代码</p><p></p><p>还可以：</p><p></p><p>使用 watch(‘x’) 查看变量值；使用 setBreakpoint()/sb() 命令设置断点（也可以在代码中插入 debugger; 语句）；restart 重启脚本；.exit 退出调试器（请注意开头的. 句点）。</p><p></p><p>整个操作过程似乎不太方便，确实如此。所以除非实在没有其他方法，否则尽量不要使用内置的调试客户端。</p><p></p><p></p><h2>使用 Chrome调试 Node.js 应用</h2><p></p><p></p><p>使用—inspect 标志启动 Node.js V8 检查器：</p><p></p><p><code lang=\"css\">node --inspect index.js\n</code></p><p></p><p>（nodemon 也支持此标志。）</p><p></p><p>此命令会在 127.0.0.1:9229 端口上启动侦听调试器：</p><p></p><p><code lang=\"nginx\">Debugger listening on ws://127.0.0.1:9229/4b0c9bad-9a25-499e-94ff-87c90afda461\n</code></p><p></p><p>如果大家在其他设备或 Docker 容器上运行 Node.js 应用，请确保端口 9229 可以访问，具体使用以下命令授予远程访问权限：</p><p></p><p><code lang=\"nginx\">node --inspect=0.0.0.0:9229 index.js\n</code></p><p></p><p>与—inspect 不同，我们可以使用—inspect-brk 停止对首条语句的处理，以便逐步分步执行。</p><p></p><p>打开 Chrome 网络浏览器（或者其他基于 Chromium 内核的浏览器），并在地址栏中输入 chrome://inspect：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4c/4c64b1b9d74d4d434c6d3cfa7247b6f3.png\" /></p><p></p><p>几秒后，您的 Node.js 应用就会显示为 Remote Target。如果仍未找到，请选中 Discover network targets，而后单击 Configure 按钮为运行应用的设备添加 IP 地址和端口。</p><p></p><p>单击目标的 inspect 链接以启动 DevTools。对于熟悉在浏览器上调试客户端应用的朋友，整个操作流程应该非常顺畅。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/38/389f0356b4b687d819979ff26b0d1eed.png\" /></p><p></p><p>要直接从 DevTools 加载、编辑和保存文件，请打开 Sources 窗格，单击 + Add folder to workspace 向工作区添加文件夹。之后选择 Node.js 文件的位置，而后单击 Agree。现在，我们可以从左侧窗格或按 Ctrl | Cmd + P 并输入文件名。</p><p></p><p>单击任何行号以设置断点（显示为蓝色标记）：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/67/67ba25127bf278cb63cac2fae1db4b0f.png\" /></p><p></p><p>这里的 breakpoint 断点，负责指定调试器应在何处暂停处理。我们可以借此检查程序状态，包括局部和全局变量。您可以定义任意数量的断点，或向代码中添加调试器语句，这些语句会在调试器开始运行时停止处理。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f9/f9520ae5c75a7e803ce6cdc5793ba0e9.png\" /></p><p></p><p>右侧面板显示以下内容：</p><p></p><p>Watch 窗格中，您可以通过单击 + 图标以输入变量名称并监视变量Breakpoint 窗格中，您可以查看、启用和禁用断点Scope 窗格中，您可以检查所有变量Call Stack 窗格中，您可以查看达到此点前所调用的所有函数</p><p></p><p>Paused on breakpoint“在断点处暂停”上方，会出现一行图标。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/37/376c49567e1ebb2f2031e573d94b6d3d.png\" /></p><p></p><p>从左至右，各图标分别对应以下操作：</p><p></p><p>resume execution: 继续处理至下一断点step over: 执行下一条命令，但停留在当前函数内；不跳转至命令所调用的任何其他函数step into: 执行下一条命令，并跳转至命令所调用的任何其他函数step out: 继续处理至函数末尾，而后返回至调用命令step: 与 step into 类似，但不会跳转至 async 函数中deactivate all breakpoints：禁用所有断点pause on exceptions: 当发生错误时，停止处理</p><p></p><p></p><h2>在 Chrome 中设置条件断点</h2><p></p><p></p><p>假设我们有一个运行 1000 次迭代的循环，但真正需要关注的是最后一次迭代的状态：</p><p></p><p><code lang=\"javascript\">for (let i = 0; i &lt; 1000; i++) {\n  // set breakpoint here?\n}\n</code></p><p></p><p>这里我们当然无需对着 resume 单击 999 次，而是右键单击该行并选择 Add conditional breakpoint 添加条件断点，而后输入条件，例如 i=999：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/86/86fb2baefe219f8f3f438900a70ce001.png\" /></p><p></p><p>条件断点会显示为黄色，而非蓝色。</p><p></p><p></p><h2>在 Chrome 中设置日志点</h2><p></p><p></p><p>日志点为 console.log()，不涉及任何代码！执行此代码时会输出一条表达式，但与断点不同的是，处理过程不会暂停。要添加日志点，先右键单击任意行，选择 Add log point 添加日志点，而后输入表达式，例如’loop counter I’,i。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f2/f290e5f59c166d155720dc47be44f8c7.png\" /></p><p></p><p></p><h2>使用 VS Code调试 Node.js 应用</h2><p></p><p></p><p>VS Code 支持 Node.js，而且提供内置调试客户端。在本地系统上运行 Node.js 应用时无需任何配置。只要打开启动脚本（一般为 index.js），激活 Run and Debug 窗格，点击 Run and Debug Node.js 按钮，再选择相应的 Node.js 环境。之后单击任意行即可激活断点。</p><p></p><p>如果您正在运行 Web 应用程序，可在任意浏览器中打开，VS Code 会在遇到断点或 debugger 语句时停止执行：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/69/69d950baf8ded2ca48605284d84e58bd.png\" /></p><p></p><p>VS Code 调试方法与 Chrome DevTools 中的 Variables、Watch、Call stack 和 Breakpoints 窗格类似。其中 Loaded Scripts 窗格会显示应用程序所加载的各脚本，也包括 Node.js 的内部脚本。</p><p></p><p>操作图标工具栏提供以下功能：</p><p></p><p>resume execution: 继续处理至下一断点step over: 执行下一条命令，但停留在当前函数之内；不跳转至命令调用的任何函数step into: 执行下一条命令，并跳转至它调用的任何其他函数step out: 继续处理至函数末尾，而后返回至调用命令restart：重新启动应用程序和调试器stop：停止应用程序和调试器</p><p></p><p>与 Chrome DevTools 类似，我们可以右键单击任意行来添加：</p><p></p><p>标准断点在指定条件下停止程序的条件断点，例如 x&gt;3计算花括号中表达式的日志点，例如 URL:{ reg.url }</p><p></p><p>关于更多信息，请参阅在 VS Code 中调试（<a href=\"https://code.visualstudio.com/docs/introvideos/debugging%EF%BC%89%E3%80%82\">https://code.visualstudio.com/docs/introvideos/debugging）。</a>\"</p><p></p><p></p><h2>VS Code 高级调试配置</h2><p></p><p></p><p>如果希望在另一台设备或虚拟机上调试代码，或者需要使用其他替代启动选项（例如 nodemon），我们可能须进一步调整 VS Code 配置。</p><p></p><p>编辑器将启动配置存储在项目中隐藏的.vscode 文件夹内的 launch.json 文件。要生成此文件，请点击 Run and Debug 窗格上方的 create a launch.json file 创建文件，而后选择 Node.js 环境。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/24/24492a8059ae21bda8e84500565a70df.png\" /></p><p></p><p>可以使用 Add Configuration 按钮，将任意数量的配置设置对象添加至”configurations”: [] 数组当中。VS Code 能够：</p><p></p><p>Launch 启动 Node.js 进程本身，或者Attach 附加至调试 Web Socket 服务器，该服务器可能运行在远程计算机或 Docker 容器中。</p><p></p><p>以上截屏所示，为 nodemon 的启动配置。其中 Add Configuration 按钮提供 nodemon 选项，我们可以编辑其中的 program 属性以指向入口脚本 (${workspaceFolder}/index.js)。</p><p></p><p>保存 launch.json，而后在 Run and Debug 窗格上方的下拉菜单中选择 nodemon，接着单击绿色的运行图标：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/78/78bf4c4eb6bc1a560c282dc67ba2390d.png\" /></p><p></p><p>nodemon 会启动我们的应用程序，之后即可正常编辑代码并设置断点或日志点。</p><p></p><p>关于更多信息，请参阅 VS Code 启动配置（<a href=\"https://code.visualstudio.com/docs/editor/debugging#_launch-configurations%EF%BC%89%E3%80%82\">https://code.visualstudio.com/docs/editor/debugging#_launch-configurations）。</a>\"</p><p></p><p>VS Code 可以调试任何 Node.js 应用程序，而善用以下扩展能让调试过程更轻松：</p><p></p><p>Remote - Containers: 接入运行在 Docker 容器中的应用Remote - SSH: 接入远程服务器上运行的应用Remote - WSL: 接入运行在 Windows 上 Linux in WSL 中的应用</p><p></p><p></p><h2>Node.js 的其他调试选项</h2><p></p><p></p><p>参考 Node.js 调试指南：<a href=\"https://nodejs.org/en/docs/guides/debugging-getting-started/\">https://nodejs.org/en/docs/guides/debugging-getting-started/</a>\"</p><p></p><p>主要为 Visual Studio、JetBrains、WebStorm、Gitpod 和 Eclipse 等 IDE 和编辑器提供调试建议。</p><p></p><p>ndb 提供更好的调试体验，同时具备强大功能，例如附加至子进程和能够限制文件访问的脚本黑盒。</p><p></p><p>IBM report-toolkit for Node.jshttps://github.com/ibm/report-toolkit</p><p></p><p>在 node 运行时使用 --experimental-report 选项，即可分析数据输出。</p><p></p><p>最后，LogRocket 和 Sentry.io 等商业服务可以与客户端和服务器上的实时 Web 应用程序相集成，帮助用户记录真实发生的错误。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>过去十年以来，JavaScript 和 Node.js 的调试已经变得愈发轻松。我们可以用各种实用工具定位问题，使用 console.log() 快速查找 bug。如果面对更复杂的问题，Chrome DevTools 或者 VS Code 可能是更合适的选项。熟悉掌握这些工具将帮助大家编写出更健壮的代码，同时显著缩短在 bug 修复上投入的时间和精力。</p><p></p><p></p><h5>原文链接：</h5><p></p><p></p><p><a href=\"https://blog.openreplay.com/an-introduction-to-debugging-in-nodejs/\">https://blog.openreplay.com/an-introduction-to-debugging-in-nodejs/</a>\"</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/be78c75026bf0ede1253828fe\">Node.js 未来发展趋势</a>\"</p><p><a href=\"https://www.infoq.cn/article/juXB8EaoJrlLx4vB7ttD\">Node 之父着急宣布：Deno&nbsp;将迎来重大变革，更好地兼容</a>\"</p><p><a href=\"https://xie.infoq.cn/article/aad4610523c72781f0dd5b5b7\">Node&nbsp;版本控制</a>\"</p><p><a href=\"https://www.infoq.cn/article/9QU4eRfjNmNjidpjRkUI\">Node.js 20 正式发布</a>\"</p><p></p>",
    "publish_time": "2023-07-27 17:49:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "财付通基础支付平台高可用保障体系演进之路",
    "url": "https://www.infoq.cn/article/0o5aTpM9L7k7XikKEpIf",
    "summary": "<p></p><p>中国已成为全球移动支付第一大市场，在移动支付用户规模、交易规模和渗透率等方面大幅领先。在线买买买已成常态，特别是在2020年后受疫情的影响，线上经济更是发挥了重要作用，外卖叫餐、寻医问药、网购商品等都属于线上经济的范畴，在线支付是线上经济不可缺少的组成部分。</p><p>&nbsp;</p><p>移动支付如同人们生活中的水和电一样，微信支付作为国民级支付工具，支付系统的可用性非常重要。财付通基础支付团队对系统架构不断升级演进，总结了一套保障系统高可用的方法，以解决不同阶段系统面临的可用性问题。</p><p>&nbsp;</p><p>本文整理自财付通高级开发工程师曾明福的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2022/guangzhou/presentation/4794\">微信支付后台系统高可用保障体系演进之路</a>\"”。分享主要分为三部分：一、面向 PC 支付架构；二、移动支付架构；三、思考 &amp; 演进。</p><p>&nbsp;</p><p></p><h1>面向 PC 支付架构</h1><p></p><p></p><h2>基础支付平台演进历史</h2><p></p><p></p><p>基础支付平台演进历史主要分为三个阶段：</p><p>&nbsp;</p><p>移动支付 1.0阶段及其可用性保障措施：OMP、熔断、降级、限流移动支付 2.0阶段及其可用性保障措施：健康度平台、公告、自动切换移动支付 3.0阶段及其可用性保障措施：条带化、快慢分离、Cache 化本地查询</p><p>&nbsp;</p><p></p><h2>支付系统的三个重要角色</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d722f6078fd41a85f40fd4c81b2877f.png\" /></p><p></p><p>支付系统涉及三个重要的角色：用户、商户和平台，为便于理解，我们先简单介绍一下这三个角色之间他们的一些重要的交互。在此以在线购物为例。首先，用户在商户的系统里下单，同时商户到我们的支付平台里做下单操作。下单之后会拉起收银台，平台有支付查询交互功能，展示订单、用户相关的信息。用户输入密码之后，支付完成。之后平台会给商户发通知，商户进行发货操作。</p><p>&nbsp;</p><p></p><h2>财付通基础支付平台及其发展历程</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b5/b5a16eae04cc3461ffa249f1c890278b.png\" /></p><p></p><p>这是我们平台的全景图，最上面是微信支付和QQ钱包两大品牌，底层大概分为如图几大系统。上层是接入网关，负责所有业务的接入，同时会做路由的分发、过载保护机制等等。下面两层是订单和清算体系，在底层是分布式的账户系统和银行接入系统，旁边是贯穿整个交易流程的风控和反洗钱系统，为大家提供安全保障。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/93d65d7ccd0234a070d4be9e0ad7fba2.png\" /></p><p></p><p>我们再聊一下这个平台的发展的历程。</p><p>&nbsp;</p><p>财付通成立于2005年，在2005年到2013年这一阶段，主要处于PC时代，有一点通、快捷支付等等，这套体系的建立，为即将到来的移动支付打下了夯实的基础。</p><p>&nbsp;</p><p>移动支付始于2011年，以央行发布第一批支付牌照为起点，财付通是第一批获得支付牌照的。2013年微信支付发布了第一个版本，从此拉开了财付通在移动支付上的建设之路。实际上移动支付跟PC确实有很大的区别，碰到节日或者抢购的活动，流量峰值一下就过来了。我们面临的大问题，是系统要做重构、要做优化，要大大提升我们的性能。</p><p>&nbsp;</p><p>2014年我们重构了整个订单系统，采取了像Cache化、异步化等等措施。在2015年到2016年间，线下支付的场景逐步丰富，人们慢慢进入了无现金的生活阶段，用户对可用的要求变得非常高，我们提出了同城多活架构。</p><p>&nbsp;</p><p>2016到2017年业务发展量级已经非常大了，我们在安全、在灾备又做了更长远的考虑，提出两地四中心、跨城多活核架构。2017到2018年，为了更好地提升用户体验，做了跨城多活条带化化的访问。2019年之后，我们持续进行可用优化，保障整个体系在发生故障时保持有效。</p><p>&nbsp;</p><p></p><h2>架构演变之路</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f2d4b788bec11a9c9773a6230b16f4db.png\" /></p><p></p><p>讲完历程，再看一下我们的架构是如何演变来支撑业务体量的。</p><p>&nbsp;</p><p>从2015年开始，我们归纳为五代架构。初代架构是一个非常基础的架构，是一个集中式的系统，在这个体系下，基本上是单台的数据服务，应用是无状态的，所有的状态全部都在DB里面，这种体系下，没有很强的容灾诉求。</p><p>&nbsp;</p><p>随着业务发展，我们渐渐发现这种架构有很多的弊端，往二代架构发展，将集中式的系统拆成了几大子系统，做了垂直纬度的拆分。</p><p>&nbsp;</p><p>再到同城要做多活系统之后，就要水平拆分了，即我们的三代系统。我们在这一时期建立了分布式的账户系统，提供了分布式的能力。</p><p>&nbsp;</p><p>在第四代系统之后有几大诉求，就是要做到支持快速的扩容，还要做一些标准化、快速的扩容和容灾等等，我们提出一个Set化的架构，就像集装箱单元一样，能在你需要的可用区域里面快速进行部署。</p><p>&nbsp;</p><p>再到第五代，系统的容量已经不是核心关键的问题，可能更关注是可运维、可维护、快速伸缩等等。以下是基础支付平台可用性保障全局视图。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/388d9b49e60dae2ca9efd3858ed00860.png\" /></p><p></p><p></p><h1>移动支付架构</h1><p></p><p></p><p>接下来我们从刚刚讲的移动支付的三个阶段挑选一些技术来跟大家分享。首先我们看1.0阶段。</p><p></p><h2>1.0时代</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c409c5ef3beacba2b888ec89e732e41d.png\" /></p><p></p><p>1.0时代我们面临的问题是什么？系统的架构还不足以支撑业务快速发展，我们想快速扩容，但是做不到。</p><p>&nbsp;</p><p>我们的思想是先扛住再优化，措施有熔断，有降级，有限流，这些措施怎么去优化？OMP。什么叫OMP？就是在一台机器上面完成支付所有的流程，或者支付的关键流程。因为各个服务之间穿插着调用，再加上网络的开销，是不好评估的。OMP就相当于在最前面有个路由总线，进总线之后，路由到哪个机器，整个机器就完成支付的关键能力。如此支付尽可能减少了依赖，去除了网状的调用，极大削减网络的流量，同时，如果哪台机器出现故障可直接摘除。另外，服务的配比可以做到最优，每个单机都很好评估，很好调整，更重要的一点是有了对容量评估的基准。之后扩容时，直接用当前单机数乘一个系数即可，大大简化了流程。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4e/4e89b2947615d266a697f4663983c5c6.png\" /></p><p></p><p>&nbsp;熔断、降级、接口限流是经常使用的措施。熔断的主要应用场景有三个：访问非关键资源、访问具备冗余链路资源（异步）、保护下游服务（特别是无自我保护能力服务）。它有几个状态：</p><p>Closed：熔断器关闭状态，调用失败次数积累，到了阈值则启动熔断机制；</p><p></p><p>Open：熔断器打开状态，此时对下游的调用都直接返回错误；同时设计了一个时钟选项，默认达到了一定时间则进入半熔断状态；</p><p></p><p>Half-Open：半熔断状态，允许定量的服务请求，如果调用都成功（或一定比例）则认为恢复了，关闭熔断器，否则重新回到熔断器打开状态；</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7b/7be8ad1a2266170bac10817e9502cd0a.png\" /></p><p></p><p>接口限流从部署的阶段来看有单机限流和全局限流。常见的限流算法有计数器算法、漏桶算法（leaky bucket）、令牌桶算法（token bucket）。在实现接口限流时要注意两大问题流量突发问题（临界值问题） 和流量不均匀问题。</p><p>&nbsp;</p><p></p><h2>2.0时代</h2><p></p><p></p><p>来到2.0的时代，线下的场景已经逐步丰富，人们已经开始步入无现金的生活，用户对线下支付是否可用更敏感了。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/15acbd4233e39b6940aefa047d1e8a8b.png\" /></p><p></p><p>这个是我们2.0同城多活的架构。我们分层去看，第一层是接入层，是无状态的，我们直接把CGI接入云平台，实现自动化的运维即可。</p><p>&nbsp;</p><p>第二层是银行接入，我们也可以做多专线银行渠道，进行健康度检测，如果A专线有问题，转走B专线。</p><p>&nbsp;</p><p>第三层是订单，这也是最复杂的一层，因为涉及到很多资源的基层服务都在这一层。订单系统多Set要解决Set之间弱关联甚至无关联的问题。如果在Set1这一层有故障，用户重试的时候可以直接切换到对应的Set里面完成交易。</p><p>&nbsp;</p><p>最后是账户层，通过DB部署实现核心账户多Set，这个阶段我们的实现还是人工的切换。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f2/f249e27cde2d7b3562f3a49e5dca78bd.png\" /></p><p></p><p>接下来我们看一下健康度平台。左侧是实时流程，我们分为网关订单和银行接入账户系统，在这一层，各层上报自己的健康度，健康度平台采集逐层的健康度，通过健康度平台做策略分析，最终产生动作，负反馈到实时链路。这样做的好处，首先是基于主调方的健康度，能做到整个平台有一个负反馈的机制，对实时流程影响相对比较小，能做到平台化和业务解耦。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/14c6282b90d1724f8d2cadaafe515784.png\" /></p><p></p><p>这里再看一下，多个Set是怎么切换的？当发现订单Set1这一层出现故障的时候，两个Set各自上报健康度，健康度平台根据阈值做流控仲裁判断，判断Set1出现了故障，需要做切换处理，下发路由把Set1屏蔽掉，把流量全部摘除，并成功切换到Set2，这是全链路自动切换的视图。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a5526cea85ec3d5bfa57ecd941038d7c.png\" /></p><p></p><p>比如，网关这一层是无状态，有问题可以直接检测健康度之后进行切换，如果接入专线有故障，可以在专线这一层屏蔽，如果是银行流水有故障，可以通过银行的多Set去屏蔽，通过这样的一套体系来保证全链路的多Set是多效的。</p><p></p><h2>3.0时代</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a633d958a19e94ac99bc92fa6fee33b8.png\" /></p><p></p><p>进入3.0阶段，业务有了飞跃的发展，微信支付已经像人们生活中的水电一样了。我们要考虑，一些自然灾害、地震等其他情况，可能会对整个中心造成比较严重的损害。在这个时候，要怎么做才能保障我们系统的可用？</p><p>&nbsp;</p><p>我们当时提出一个跨城的多活系统方案。跨城国标是有些要求的，要求两个城市之间的距离不能太短，但满足距离上的要求，却会给系统带来一个极大的挑战。那就是网络的延迟，以深圳到上海为例，一个ping包40毫秒，也就是说一个请求来回，什么也不做在网络上就已经可能消耗40毫秒了。比如深圳的中心如果真发生故障了，要往上海切，就需要考虑原来在深圳的账户数据上海是不是都已经拿到了，在深圳已经完成的交易在上海是不是都已经拿到了，没有拿到的话缺多少，怎么去解决。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5eebbdf6d7b626718c30382e4f99278d.png\" /></p><p></p><p>当时，我们把一个城市多活的系统变成两城市的时候看到各个城市之间有大量的流量穿插，这个时候发现系统没有原来健壮了。</p><p>&nbsp;</p><p>我们怎么去解决？第一，做条带化的访问，条带化的一个关键点就是在账户一层路由要对齐，就是把这些用户分在两城市的时候，应该要尽量做到按用户的归属层次做接入。举个例子，深圳的用户就应该从深圳接入进来，通过路由总线，在深圳的订单Set和深圳的账户体系去完成整个交互。要做这一层的梳理，做好账户体系和订单这一层的规划，通过最外面层的路由，把用户路由到正确的位置。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/71/71fa374f4d94d30249187745a4a1badd.png\" /></p><p></p><p>你可能会问，我原来的业务这么复杂，都能梳理清楚吗？都能满足你的要求吗？确实满足不了的时候我们怎么做？这个时候要做快慢的分离。一个用户归属在城市2，但是又从城市1接入了，就要在最外面这一层，通过转发代理把它路由到对应的账户中心去完成交易。同时，这条链路是独立的，不会影响原来的快的链路，保证原来99%以上的请求都走快链路，将慢链路的影响降到最低。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a6/a617970f001b02c8b56fad1a2dab83dc.png\" /></p><p></p><p>上图为读写分离，实现数据的一致访问。首先用户这一层的数据有个特点，读多写少，变更不是很频繁。这一类数据，主DB建在其中的一个城市，只在这里更新，通过用户的Cache完成城市数据的建立，通过消息的总线把所有可能变更的数据往其他中心的Cache里做快速准实时的数据同步，同时通过DB的异步复制链路，去保障其他中心DB数据一致，最终通过对帐体系保障各个Cache数据一致。通过这样的方式，即使在跨城的体系下都能访问到本地的资源，以提升性能。</p><p></p><h1>思考 &amp; 演进</h1><p></p><p></p><p>前面就是通过几个阶段介绍了我们的体系，在这个演进的过程中，我们也做了很多的思考，总结如下：</p><p>系统架构不断演进：虚拟化、云化、云原生目标始终一致：快速稳定支持业务发展核心思想不变：腾讯海量服务之道演进： 建立云原生下的可用性保障体系</p><p></p><p></p><h5>相关阅读：</h5><p></p><p></p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzIxMzEzMjM5NQ==&amp;mid=2651029772&amp;idx=1&amp;sn=698c87e7da11fc2f923c183ec231b78b&amp;chksm=8c4c5408bb3bdd1e9b093f0ec8272ed7672d9ae2c40eaea829bcb5eae2652b6294c650b9e11f&amp;scene=27#wechat_redirect\">解密支付平台建设资金底线防火墙的杀手级设计方案</a>\"</p><p><a href=\"https://www.infoq.cn/article/hCo7QLgJe2yoRkgoIG02\">在数据规模重压、多维度查询需求之下，支付平台的迁移之旅</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzUzMTA2NTU2Ng==&amp;mid=2247526151&amp;idx=3&amp;sn=316301b0dacc2e0e77beeff56e704789&amp;chksm=fa4a18b6cd3d91a0e465828b97abcbe3555a692985c19734dd0b7636db09585d0d68062a3108&amp;scene=27#wechat_redirect\">支付运营平台架构设计</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247500872&amp;idx=1&amp;sn=bc195401c06ad743889563b654361f24&amp;chksm=e8d43b8adfa3b29cc76b492f9fbdeaf5343cc79f4c0f127bfce86efa0dfab46a2fe5b9df48ee&amp;scene=27#wechat_redirect\">支付平台架构技术实现之终端安全</a>\"</p>",
    "publish_time": "2023-07-27 17:52:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java Web应用开发案例｜使用AJAX实现省市区三级联动效果",
    "url": "https://www.infoq.cn/article/0d9f1e13e2f78101a75712c3d",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/d2/d2779a8599f2acf079ec39ad5237d9cc.png\" /></p><p></p><p></p><h1>01、案例：省市区三级联动</h1><p></p><p>省市区三级联动，在位置查找时非常有用，如查找景点、查找酒店等操作。</p><p>项目操作步骤如下</p><p>(1) 定义省、市、区三个实体对象。</p><p><code lang=\"null\">public class Province{\nprivate String id;\nprivate String name;\nprivate List &lt; City&gt; cityList;\n}\npublic class City{\nprivate String id;\nprivate String name;\nprivate List &lt; Country&gt; countryList;\n}\npublic class Country {\nprivate String id;\nprivate String name;\n}\n</code></p><p>(2) 新建city.jsp，显示省市区信息。</p><p><code lang=\"null\">&lt; table border =\"1\"&gt;\n&lt; tr &gt;&lt; td&gt;省级 :\n&lt; select name =\"shengSelect\"onchange = \"getShiList()\";\nid=\"shengID”onclick = getShengName()\"&gt;\n<option value=\"nulISelect\">-- 请选择--</option>\n\nS{sheng.id}\"&gt;\n&lt; option value =${sheng.name}\n\n\n\n&lt; td&gt;市级: \"getQvList()\"&gt;\n&lt; select name =\"shiSelect” id=\"shiID\"onchange&lt; option&gt; --请选择 --区县: &lt; select name =\"gvSelect\"id =\"gvID”onclick = \"getQvName()\"&gt;-- 请选择-- &lt; option &gt;\n\n\n</code></p><p>(3) 访问&nbsp;<a href=\"http://localhost:8080/CityThree/%EF%BC%8C\">http://localhost:8080/CityThree/，</a>\"&nbsp;web.xml中的欢迎页指向GetShengSvl。</p><p><code lang=\"null\">\n&lt; welcome - file&gt; GetShengSvl \n</code></p><p>(4) 新建控制器GetShengSvl。</p><p><code lang=\"null\">@WebServlet(\"/GetShengSvl\"\npublic class GetShengSvl extends HttpServlet{\n   \n</code></p><p>读取所有省的数据,并转向 city.jsp 。</p><p><code lang=\"null\">public void doPost(HttpServletRequest request\nHttpServletResponse response)\nthrows ServletException，IOException{\n   \n (TestBiz testBiz = new TestBiz();\nList &lt; Province&gt; arrSheng = testBiz.getAllSheng();\nif(arrSheng != null){\n   \nrequest.setAttribute(\"arrSheng\"，arrSheng);\nrequest.getRequestDispatcher(\"/WEB - INF/main/city.jsp”)\nforward(request，response)\n}\n</code></p><p>(5) 参见city.jsp，所有省的数据是使用JSTL填充的。</p><p><code lang=\"null\">\n<option value=\"s(sheng.id\">Ssheng.name\n</option>\n\n</code></p><p>(6) 在city.jsp页选择省的数据，会触发下拉框的onchange()事件。</p><p><code lang=\"null\">function getShiList()\nif($(\"# shengID\").val() != \"nullSelect\"){\n   \n$ .getJSON(\"GetShiSvl\",{\n   sheng: (\"# shengID\").val()},\nfunction callback(data)\n$(\"# shiID\").empty();\nS (data).each( function(i){\n   \nS(\"<option value=\"”\">\"\n+ data[i].name +\"</option>\").appendTo(\"# shiID\");;\n}）；\n$ (\"# shiID\").change();  //触发#shiID的onChange 事件\n\nelse{\n     \nS (\"# shiID\").empty();\n$ (\"# qvID\").empty();\n$(\"&lt; option &gt;-- 请选择 -- \").appendTo(\"# shiID\");\n$(\"<option>--请选择 -- </option>\").appendTo(\"#qvID\");\n}\n}\n</code></p><p>(7) 新建控制器GetShiSvl，根据省的id返回该省下的所有市数据。</p><p><code lang=\"null\">public void doPost(HttpServletRequest request, HttpServletResponse response）\nthrows ServletException， IOException{\n   \nString shengID = request.getParameter(\"sheng”);\nTestBiz testBiz = new TestBiz();\nList &lt; City&gt; cityList = testBiz.getShiBySheng(shengID)；\nJSONArray jsonArray = JSONArray.fromObject(cityList);\nresponse.setCharacterEncoding(\"utf - 8\")；\nresponse.getWriter().print(jsonArray.toString())\n}\n</code></p><p>(8) 选择市，会触发下拉框的onchange()事件。</p><p><code lang=\"null\">function getOvList(){\n   \n {\n   mathJaxContainer[0]} (\"# shiID\").val(),\nsheng:$(\"# shengID\").val())，function callback(data){\n   \n(\"# qvID\").empty();\nS (data).each(function(i){\n   \n$(\"<option value=\" + datalil.id + \">\"\n+ data[i].name +\"</option>\").appendTo(\"# qvID\");;\n});\n$(\"# qvID\").click();\n</code></p><p>(9) 新建控制器GetQvSvl，根据市的id返回该市下的所有区县数据。</p><p><code lang=\"null\">@WebServlet(\"/GetQvSvl\") \npublic class GetQvSvl extends HttpServlet{\n   \npublic void service(HttpServletRequest request,HttpServletResponse response)\nthrow ServletException,IOException{\n   \nString provinceID = request.getParamenter(\"sheng\");\nString CityID = request.getParamenter(\"shi\");\nTestBiz biz = new TestBiz();\nListqvList = biz.getCountrys(provinceID,CityID);\nJSONArray jsonArray = JSONArray.fromObject(qvList);\nresponse.setCharracterEncoding(\"UTF-8\");\nresponse.GetWriter().print(jsonArray.toString())\n}\n}\n</code></p><p>(10) 选择省，根据省的id返回该省下的所有市数据，JSON格式如下：</p><p><code lang=\"null\">[(\"id\":\"tj01\",\"name\":\"和平区\"),\n[\"id\":\"tj02\",\"name\":\"河东区\"},\n[\"id\":\"tj03\",\"name\":\"河西区\"},\n{\n   \"id\":\"tj04\",\"name\":\"南开区\"},\n{\n   \"id\":\"tj05\",\"name\":\"河北区\"),\n{\n   \"id\":\"tj06\",\"name\"\"红桥区\"),\n{\n   \"id\":\"tj07\",\"name\":\"塘沽区\"),\n\"id\":\"tj08\",\"name\":\"汉沽区\"),\n{\n   \"id\":\"tj0g\"\"name\":\"大港区\"),\n{\n   \"id\":\"tj10\",\"name\":\"东丽区\"),\n\"d\":\"tj11\",\"name\":\"西青区\"),\n[\"id\"\"tj12\",\"name\":\"津南区\"],\n\"id\":\"tj13\",\"name\":\"北辰区\"),\n\"d\":\"tj14\",\"name\":\"武清区\"),\n\"d\".\"tj15”\"name\":\"宝抵区\"}]\n</code></p><p>(11) 选择市，根据市的id返回的该市下的所有区县数据，JSON格式如下：</p><p><code lang=\"null\">[(\"id\":\"tj16\",\"name\":\"宁河县\"),\n\"id\":\"tj17\",\"name\":\"静海县\"},\n\"id\":\"tj18\",\"name\":\"蓟县\"}]</code></p><p></p>",
    "publish_time": "2023-07-27 11:32:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT居然攻破了图灵测试，基准测试也不靠谱了？",
    "url": "https://www.infoq.cn/article/9v2Kjbi4Xt7qj4C2BDkM",
    "summary": "<p></p><blockquote>大语言模型拥有出色的人类语言模拟能力，但科学家们对其推理表现仍存在分歧。</blockquote><p></p><p>&nbsp;</p><p>7月25日，《Nature》在一篇文章中称，ChatGPT已经攻破了图灵测试，是时候要启用其他新的方法来评估人工智能技术了。</p><p>&nbsp;</p><p>世界上最强的人工智能（AI）系统能够通过严格的考试、写出令人信服的论文、顺畅参与聊天，甚至很多人已经无法分辨AI与人在语言表达上有何分别。还有什么是它们做不到的吗？当然有，而且是些非常简单的问题。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/32/32c797adcf9990de95d5f21609695ec6.png\" /></p><p></p><p>&nbsp;</p><p>屏幕上排列着一系列色彩鲜艳的图形，面对这类视觉逻辑测试，大多数人都能快速找出答案。但作为聊天机器人ChatGPT和搜索引擎Bing背后的技术之光、当前AI的最高杰作，GPT-4却明显力不从心。今年5月的一项研究显示，GPT-4在其中一类模式测试中的正确率只有三分之一，而在另一类测试中正确率更是可怜的3%。</p><p>&nbsp;</p><p>逻辑谜题背后的研究团队，希望测试能给AI系统提供更好的基准，帮助解决关于GPT-4等大语言模型的固有短板。总结来讲：在语言类测试中，大语言模型轻易就完成了曾经被视为具有里程碑意义的智能壮举；但在视觉逻辑测试中，它们的表现则相当孱弱、存在明显的盲点，无法根据抽象概念做出推理。</p><p>&nbsp;</p><p>新墨西哥州圣达菲研究所的计算机科学家Melanie Mitchell表示，“AI领域的从业者们正努力解决对大语言模型系统的评估难题。”为此，她的团队整理出了这套逻辑问题集。</p><p>&nbsp;</p><p>过去两、三年里，大语言模型已经在跨多任务能力上全面碾压以往的AI系统。它们的工作原理并不复杂：根据训练时接触过的数十亿在线句子，总结出各单词之间的统计相关性，而后面对给定的输入文本生成合理的下一单词。对于基于大语言模型构建的聊天机器人，则额外再添加一个元素：由人类训练师提供广泛反馈，因此微调机器人的响应方式。</p><p>&nbsp;</p><p>值得注意的是，在这类海量人类语言库上训练而成的、类似于自动补全性质的算法，成功表现出了广泛的问题解决能力。虽然原有AI系统也许在特定某项任务上能够击败大语言模型，但前者必须接受与特定问题相关的数量训练，且这种能力无法快速从一项任务转移至其他任务。</p><p>&nbsp;</p><p>哈佛大学认知科学家Tomer Ullman表示，从广义上讲，这两大阵营的研究人员对于大语言模型的底层实现原理抱有完全相反的观点。有些人将算法的成就归结于真正的推理或理解能力，但其他人（包括Ullman本人和前文中的Mitchell等研究人员）则仍持谨慎态度。</p><p>&nbsp;</p><p>Ullamn认为，“这场辩论的双方都才华横溢、水平很高。”产生分歧的根本原因，在于缺乏确凿的证据来支持其各自观点。“毕竟还没有盖革计数器那样稳定可靠的智能检测器，能明确给出有智能或者无智能的答案。”</p><p>&nbsp;</p><p>讨论双方的研究人员表示，依靠逻辑问题等测试提示人类与AI系统间的能力差异，应该是向正确方向迈出的重要一步。纽约大学认知计算科学家Brenden Lake表示，此类基准测试还有助于揭示当今机器学习系统所缺失的能力，同时厘清人类智能到底由哪些要素组成。</p><p>&nbsp;</p><p>另外，这种对大语言模型及基准能力研究的测试，还具有其他现实意义。Mitchell指出，如果想把大语言模型应用到医学、法律等现实场景当中，首先必须明确其能力边界在哪里。“我们得搞清楚它能做什么、做不了什么，之后才能判断如何安全加以运用。”</p><p></p><h2>图灵测试已经过时了吗？</h2><p></p><p>在机器智能测试领域，最著名的方案一直是图灵测试。该测试由英国数学家兼计算机先驱阿兰·图灵于 1950 年提出，当时的计算机尚处于起步阶段。图灵提出了一种所谓“模仿游戏”的评估方法，在该场景中，人类裁判分别与隐藏在屏幕之后的计算机和人类进行简短的文本对话，看其能否据此准确识别出机器和人。图灵认为，这应该能回答“机器有没有思考能力”的问题。</p><p>&nbsp;</p><p>Mitchell指出，图灵并没有详细说明该场景的大量细节，因此缺乏确切规则可供遵循。来自谷歌的软件工程师François Chollet认为，“图灵测试并不是能在机器上实际运行的具体测试——而更多只是种思想实验。”</p><p>&nbsp;</p><p>但这种用语言来检测机器是否具备思考能力的观点，已经在技术领域根深蒂固。几十年来，商人兼慈善家Hugh Loebner长期资助一年一度的图灵测试活动，也就是“Loebner奖”。但计算机科学家Rob Wortham表示，这项活动在2019年之后就停止了，因为随着Loebner本人过世、活动经费也无以为继。Wortham是英国人工智能与行为模拟研究学会的联席主任，该学会自2014年开始就代表Loebner主办这场竞赛。他解释道，大语言模型现在基本具备了骗过人类的能力，所以Loebner奖在大语言模型全面起飞前夕被迫停办颇有种黑色幽默的意味。</p><p>&nbsp;</p><p>其他研究人员也认为，GPT-4等大语言模型已经基本具备了通过图灵测试的能力。至少在简短的对话中，多数人恐怕很难分辨谁是人、谁是大模型。今年5月，以色列特拉维夫AI21实验室的研究人员报告称，有超过150万人参与过基于图灵测试的在线游戏。用户将参与到两分钟的聊天当中，面对的要么是另一位用户、要么是根据研究人员提示伪装成真人的大语言模型。玩家正确识别出机器人的概率只有60%，已经跟完全乱猜差不多了。</p><p>&nbsp;</p><p>但比较熟悉大语言模型的研究者还是能从种种细节中分辨出聊天机器人。Chollet指出，他发现只要利用系统的已知弱点，就能轻松检测出谁是大语言模型。“如果让我自己接受测试，判断到底是不是在跟大语言模型聊天，那我肯定能得出正确答案。”</p><p>&nbsp;</p><p>而其中的关键，就是让大语言模型走出自己的舒适区。他的诀窍就是向大语言模型提出与常见训练场景不同的差异化场景。在多数情况下，大语言模型都是在根据训练数据输出可能性最高的单词，而并非真的按照新场景给出正确答案。</p><p>&nbsp;</p><p>而且，Chollet等人对于这种基于欺骗性能的测试方法持怀疑态度。“这明显就是为了欺骗人类裁判而存在”，这样的测试只会鼓励开发者向AI灌输更多伪装技巧，并不能激发出更多有用或者有趣的功能。</p><p></p><h2>基准测试也不靠谱</h2><p></p><p>研究人员经常会用评估特定能力（例如语言能力、常识推理和数学能力）的基准测试对AI系统做出评估，各技术团队也越来越多采用那些专为人类设计的学术和专业考试。</p><p>&nbsp;</p><p>今年3月GPT-4刚刚发布时，来自加州旧金山的OpenAI公司就在一系列专为机器设计的基准测试上评估了新模型的性能，内容包括阅读理解、数学和编码。据OpenAI报告，GPT-4在大多数测试中表现出色。他们还为GPT-4设置了约30项考试，包括：面向美国高中生的各科考试，即先修课程（Advanced Placement）；评估美国医生临床知识的考试；以及美国研究生选拔过程中使用的标准测试（GRE）。在统一律师考试（美国有多个州在律师资格考试中包含此项考试）中，GPT-4的得分成功跻身前10%。</p><p></p><h2>AI系统性能——结果摘录</h2><p></p><p></p><p>资料来源: OpenAI/参考文献4</p><p>&nbsp;</p><p>*这里的排名百分位，为达到该分数的人类考生在全体受试者中的所在位置。</p><p>&nbsp;</p><p>Mitchell承认，“不少语言模型在这些基准测试中都表现良好。但多数情况下，这并不足以证明它们在一般能力上超越了人类，而是基准本身存在局限。”研究人员提出了有力的质疑，即因为模型接受了大量文本素材的训练，所以很可能已经在训练数据中见过类似的问题。这种情况下得出的基准测试结论被称为“污染”，显然不足以采信。</p><p>&nbsp;</p><p>OpenAI公司表示，他们通过在问题和训练数据中查找相似字符串的方式查验过这一点。在删除相似字符串之前和之后对大语言模型做测试，其性能几乎没有变化。这表明极高的得分跟污染无关，但仍有部分研究人员质疑测试是否足够严格。</p><p>&nbsp;</p><p>Sam Bowman是纽约大学的语言技术科学家，同时也在旧金山AI公司Anthropic工作。他警告称，千万不要简单把GPT-4的考试成绩视为“见过类似问题”的结果，进而否定GPT-4的能力。在他看来，“污染的说法确实让情况变得有点复杂，但我认为这并没有真正影响大局。”</p><p>&nbsp;</p><p>研究人员还指出，大语言模型拿下考试高分的能力本身也比较脆弱，恐怕无法被转化成在现实世界中做出正确判断的能力。Mitchell认为，只要稍微调整一下考试题目，就有可能导致大模型无法通过。例如，她从ChatGPT通过的工商管理硕士考试中选出一个问题并稍加改动，人类可以轻松根据变化调整答案，但ChatGPT却惨遭失败。</p><p>&nbsp;</p><p>在解读基准测试含义时，还有另一个更深层次的问题。对人类来说，在这些考试里拿下高分一般都代表其具备较强的智力水平——其实智力水平本身也是个模糊概念，主要反映在一系列任务中表现出的能适应不同环境的能力。换言之，在考试中拿高分证明此人拥有较好的认知能力，而且出色掌握了某些抽象概念。但对大语言模型来说，情况则并非如此。Mitchell强调，大模型的判断方式跟人类非常不同，“在多数情况下，AI系统并不是在以人类熟悉的方式做推理。”</p><p>&nbsp;</p><p>这可能是因为大语言模型只能从语言当中学习经验；由于缺少与现实世界连接的通道，它们无法像人那样体验语言跟物体、属性和情感之间的联系。Lake指出，“很明显，它们理解单词的方式跟人类不一样。”在他看来，目前的证据表明大语言模型“可以在不真正理解自己在说什么的情况下，非常流利地使用语言。”</p><p>&nbsp;</p><p>另一方面，大语言模型也表现出一些人类所不具备的能力，例如理解人类写下的几乎每个单词之间的联系。Mitchell表示，这可能代表模型是在依靠语言或者其他指标的某些特征来解决问题，而用不着掌握更广泛的推理能力。</p><p>&nbsp;</p><p>OpenAI公司研究员Nick Ryder也认同这一判断，表示AI在单一测试中的性能表现并不足以像证明人类受试者那样证明其普遍能力。“我觉得大家不该把人类得分跟大语言模型的得分做直接比较”，OpenAI公布的得分“并不是在描述大语言模型具备类人能力或者类人推理水平，而单纯是展示这些模型在执行这些任务时的表现。”</p><p>&nbsp;</p><p>在传统机器基准测试和人类专业考试之外，研究人员还对大语言模型做出更广泛的探讨。今年3月，微软研究院的Sébastien Bubeck及其同事就放出了题为《通用人工智能的火花：GPT-4早期实验》的预发表版本，在行业内引起热议。他们使用GPT-4的早期版本记录下一系列令人惊讶的功能，而且其中很多功能与语言并没有直接或明确的联系。其中一个值得注意的亮点，就是它能通过用于评估心理学理论的测试。心理学理论是人类的一种核心能力，用于预测和推理他人的心理状态。他们在文章中写道，“鉴于GPT-4在功能上的广度和深度，我们有理由相信它已经代表着通用人工智能（AGI）系统的早期（但尚不完美）版本。”</p><p>&nbsp;</p><p>但Bubeck本人随后也做了澄清，强调“GPT-4肯定不会像人那样思考，而且对于展现出的任何功能，它都有着自己独特的、与人类不同的实现方式。”</p><p>&nbsp;</p><p>Mitchell认为，尽管这份报告表述得相当激进，但却并没有对大语言模型的能力做出系统性探讨。“这更像种人类学研究。”Ullman也表示要想证明机器能掌握心理学理论，至少要给出与之对应的潜在认知过程证据，而不能简单依据机器输出了跟人类相同的答案就粗暴断言。</p><p>&nbsp;</p><p>AI研究人员们认为，要想摸清大语言模型的优势和短板，还需要展开更广泛、更严格的审查。而色彩逻辑问题可能正是其中的重要一环。</p><p>&nbsp;</p><p></p><h2>新鲜谜题</h2><p></p><p>2019年，就在大语言模型全面爆发之前，Chollet在网上发布了专门为AI系统整理的一套新型逻辑测试集，名为抽象与推理语料库（ARC）。解答者将面对一段视觉演示，其中几个正方形网格会转变成另外一种模式，再由其指示下一网格该如何变化来证明自己已经理解了变化规则。Chollet表示，“这测试的是我们适应以往从未见过的事物的能力”，他认为这种探寻规律的能力才是智能的本质。</p><p>&nbsp;</p><p>Lake认为，ARC把握住了“人类智能的标志”：从日常知识中进行抽象，并将其应用于以往从未见过的问题。</p><p>&nbsp;</p><p>Chollet在2020年组织了一场ARC机器人竞赛，当时大语言模型还没有获得广泛关注。最终获胜的AI系统经过了专门训练，善于解决ARC这类任务。但跟大语言模型不同，它并不具备通用功能，而且也只答对了21%的问题。相比之下，人类正确解决ARC问题的比例为80%7。多个研究团队目前正使用ARC来测试大语言模型的能力，也没有任何一种能接近人类的表现。</p><p>&nbsp;</p><p>Mitchell和她的同事在ARC的启发下又开发出一套新的谜题（称为ConceptARC），主要区别有两点。ConceptARC的难度更低：Mitchell团队希望让基准测试反映出机器功能的进步，哪怕只是一点点改进。第二是，该团队选择了特定概念来做测试，之后围绕每个概念创建一系列与主题相关的谜题变体。</p><p>&nbsp;</p><p>例如，为了测试相同性这个概念，一道题要求解题者将具有相同形状的对象保持不动，另一道题则要求将同形状对象沿一条轴对齐。这样做的目的，是减少AI系统在未掌握概念的情况下通过测试的几率。</p><p></p><h2>性能不佳代表着什么？</h2><p></p><p>研究人员将ConceptARC任务发布给了GPT-4和招募来的400名受试人员。人类在所有概念组上的平均得分为91%（得分最高的一组为97%）；GPT-4得分最高的一组为33%，在其余概念组中的得分均不超过30%。</p><p>&nbsp;</p><p>Mitchell指出，“我们证明了机器仍然达不到人类的智力水平。但令人惊讶的是，尽管从未接受过相关问题的训练，但它还是能够解决其中一些问题。”</p><p>&nbsp;</p><p>该团队还测试了在Chollet竞赛中胜出的机器人，这些机器人并不属于大语言模型那种通用能力系统，而是专门针对ARC等视觉问题训练而成。总体而言，它们的性能比GPT-4更好，但还是不如人类，其中最佳概念组得分为77%，但在大多数概念组中得分低于60%1。</p><p>&nbsp;</p><p>不过Bowman认为，GPT-4通不过ConceptARC的训练，并不证明它缺乏潜在的抽象推理能力。在他看来，ConceptARC与GPT-4之间存在偏差，毕竟这是一种视觉测试。“即使这些模型真的很擅长这种概念推理，也不大可能在初次参与此类测试时就拿下高分。”</p><p>&nbsp;</p><p>测试方式的限制，也可能是GPT-4表现不佳的影响因素。大语言模型的公开版本只能接受文本输入，因此研究人员提交了用于描述图像的数字数组。（例如，空白像素可能用0表示，彩色广场则可能用相应的数字表示。）相比之下，人类受试者能够直接看到图像。Mitchell也承认，“我们是在拿纯语言系统跟人类做比较，而人类拥有高度发达的视觉系统，所以这样的比较恐怕并不完全公平。”</p><p>&nbsp;</p><p>OpenAI已经建立了GPT-4的“多模态”版本，能够直接接受图像输入。Mitchell团队正在等待该技术的正式公开，这样就能再做一轮ConceptARC。但她认为多模态GPT-4的成绩也好不了多少，“我觉得这些系统仍然不具备能与人类比肩的抽象概念和推理能力。”</p><p>&nbsp;</p><p>麻省理工学院的计算认知科学家Sam Acquaviva也赞同这一判断，“否则就太让人震惊了。”他还提到，另一组研究人员已经在1D-ARC基准上测试了GPT-4，且模式限制为单行而非网格8。这应该能消除一定的不公平问题，但Acquaviva看到虽然GPT-4的性能有所提高，但同样不足以证明大语言模型具备可靠的规则理解和推理能力。</p><p></p><h2>推理论证</h2><p></p><p>Bowman还提到其他一些实验，综合结果来看，大语言模型至少已经掌握了推理抽象概念的基本能力。在其中一个案例中，哈佛大学计算机科学家Kenneth Li和他的同事采用了黑白棋的数字版本，由对弈双方将黑色和白色棋子放进8&nbsp;x 8的网格当中。他们希望借此评估大语言模型到底是依赖记住的语言统计关系来生成文本，还是真的能像人类一样为现象构建内部表征。</p><p>&nbsp;</p><p>在向大语言模型提交人类选手的操作训练集之后，AI很快就掌握了为下一步棋选择正确策略的能力。研究人员认为，这表明大语言模型甚至能够理解棋盘上的态势，并结合当前特征给出棋步建议，这明显突破了文本形式的束缚。</p><p>&nbsp;</p><p>Bowman承认，大语言模型的推理能力总体上可谓是“参差不齐”，而且达不到人类推理的高度。但他认为这种推理能力确实存在，而且似乎会随着模型规模的增长而提升。也就是说，未来的大语言模型会表现得越来越好。“这些系统没有我们期待中那么可靠或者通用，而且在某些特定的抽象推理方面完全搞不清状况。但我认为，它们的基本推理能力确实客观存在。”</p><p>&nbsp;</p><p>Bowman和Mitchell等研究人员还一致认为，如何更好地测试大语言模型抽象推理及其他智能指标的方法，仍然是个悬而未决的问题。斯坦福大学认知科学家Michael Frank认为不可能存在单一某种包罗万象的测试能够全面取代图灵测试。相反，他认为研究人员需要设计大量测试来量化各类系统的优势和短板。“这些智能体都很棒，只是在诸多方面仍有缺陷，所以最重要的就是对此开展系统性探索。”</p><p>&nbsp;</p><p>Wortham则向刚刚接触AI系统的朋友们提出建议，希望尽量远离那种对拟人化的执念。“我们总想把任何表现出智能的东西理解成人，这真的很没必要。”</p><p>&nbsp;</p><p>“这甚至可说是种诅咒，意味着除了人类自己，我们无法想象其他表现出明确目标导向的智能形式。我们总是一厢情愿地认为，它这么做的深层思维方式跟自己一样。”</p><p>&nbsp;</p><p>参考文献：</p><p></p><p>Moskvichev, A., Odouard, V. V. &amp; Mitchell, M. Preprint at&nbsp;<a href=\"https://arxiv.org/abs/2305.07141\">https://arxiv.org/abs/2305.07141</a>\"&nbsp;(2023).Turing, A. M.&nbsp;Mind&nbsp;LIX, 433–460 (1950).<a href=\"https://doi.org/10.1093%2Fmind%2FLIX.236.433\">Article</a>\"&nbsp;<a href=\"http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Mind&amp;doi=10.1093%2Fmind%2FLIX.236.433&amp;volume=59&amp;pages=433-460&amp;publication_year=1950&amp;author=Turing%2CA.%20M.\">Google Scholar</a>\"&nbsp;Jannai, D., Meron, A., Lenz, B., Levine, Y. &amp; Shoham, Y. Preprint at&nbsp;<a href=\"https://arxiv.org/abs/2305.20010\">https://arxiv.org/abs/2305.20010</a>\"&nbsp;(2023).OpenAI. Preprint at&nbsp;<a href=\"https://arxiv.org/abs/2303.08774\">https://arxiv.org/abs/2303.08774</a>\"&nbsp;(2023).Bubeck, S.&nbsp;et al.&nbsp;Preprint at&nbsp;<a href=\"https://arxiv.org/abs/2303.12712\">https://arxiv.org/abs/2303.12712</a>\"&nbsp;(2023).Chollet, F. Preprint at&nbsp;<a href=\"https://arxiv.org/abs/1911.01547\">https://arxiv.org/abs/1911.01547</a>\"&nbsp;(2019).Johnson, A., Vong, W. K., Lake, B. M. &amp; Gureckis, T. M. Preprint at&nbsp;<a href=\"https://arxiv.org/abs/2103.05823\">https://arxiv.org/abs/2103.05823</a>\"&nbsp;(2021).Xu, Y., Li, W., Vaezipoor, P., Sanner. S. &amp; Khalil, E. B. Preprint at&nbsp;<a href=\"https://arxiv.org/abs/2305.18354\">https://arxiv.org/abs/2305.18354</a>\"&nbsp;(2023).Li, K.&nbsp;et al.&nbsp;Proc. Eleventh Int. Conf. Learn. Represent.&nbsp;<a href=\"https://openreview.net/forum?id=DeG07_TcZvT\">https://openreview.net/forum?id=DeG07_TcZvT</a>\"&nbsp;(2023).</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.nature.com/articles/d41586-023-02361-7\">https://www.nature.com/articles/d41586-023-02361-7</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2023-07-27 18:07:14",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "打造中国版“Snowflake”，经济低迷时期技术创业型公司如何乘风破浪？",
    "url": "https://www.infoq.cn/article/AN33kAfP4y35xBeb6M22",
    "summary": "<p>在当下科技创业热潮中，多云及一体化的数据平台提供商云器科技备受瞩目。成立于2021年的云器科技，在近期宣布完成连续两轮总计数亿元人民币的融资，并举办新产品发布会的消息获得了广泛的关注。</p>\n<p>这家初创公司选择在如此低迷的经济环境中乘风破浪，其背后秘诀何在？他们又是如何看待技术创新与商业成功的关系？在全球竞争激烈的市场上，云器科技如何保持核心竞争力？一体化趋势下，云器自创的Single-Engine 理念有何独特之处？本期 C 位面对面将与大家一同探寻云器科技的创业之路、技术创新的策略以及对中国数据平台发展的独到见解。</p>",
    "publish_time": "2023-07-27 18:25:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]