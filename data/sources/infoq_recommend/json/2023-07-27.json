[
  {
    "title": "加速数字化转型：深度解析API成熟度模型",
    "url": "https://www.infoq.cn/article/IeNEgLeqmfLxzdjYeg4x",
    "summary": "<p>如果数字化转型得当，可以影响一个组织的各个方面。然而，API成熟度成为数字化转型需要解决的一个常见的问题。API是推动业务增长的桥梁，但随着API被广泛使用，可能会出现API蔓延。在解决日常业务问题的过程中，没有计划和管理的API激增会导致API蔓延。API蔓延说明有大量的API正在被创建，部署API的分布式基础设施也在发生物理扩散。</p><p></p><p>公司看到他们的API正以前所未有的速度在全球范围内蔓延。对于希望在分布式基础设施之间保持高质量和卓越体验的组织来说，API蔓延给他们带来了一个独特的挑战。</p><p></p><p>管理大规模的API需要自上而下的监督，它还需要一种实用的方法，并从旨在统一API的API程序计划开始。程序应该将API打包成产品或服务来推动API的采用，并促进对其整个生命周期的管理。问题在于，创建一个可行的程序来管理API成熟度是一个缓慢的过程。</p><p></p><p>本文将为构建成熟的API计划提供一个框架，这个框架使用了一个可以促进API驱动业务演化的四层API程序成熟度模型。</p><p></p><h2>什么是API成熟度模型</h2><p></p><p></p><p>关于API的成熟度和生命周期，我们可以用两个阶段来描述：API成熟度和API程序成熟度。</p><p></p><p>API成熟度与设计和开发相关，遵循的过程与软件开发成熟度一致。API成熟度确保API符合公认的API规范，比如REST。在讨论API成熟度时，你讨论的是为特定应用程序或目的而创建的一组API。</p><p></p><p>API程序成熟度主要是从整个公司层面来看的，即公司为满足各种业务目标而积累的大量API。对于API程序成熟度来说，将API构建成统一的服务是有必要的。API程序成熟度模型为通过简化API来促进业务创新提供了基准。</p><p></p><h2>API程序成熟度模型</h2><p></p><p></p><p>API程序成熟度从技术和业务的角度评估API的非功能性指标。API的技术指标包括性能、安全性、体验和可伸缩性。API的业务指标与间接影响时间和成本的流程和生产力的改进有关。</p><p></p><p>与其他业务流程一样，API程序也应该从小处开始，然后逐渐演化。API程序的结构必须能够遵循持续的改进周期。指标应该随着API程序从较低成熟度级别过渡到较高成熟度级别而得到改进。</p><p></p><p>在开始你的API成熟度模型之旅之前，你必须首先将API视为一种工具。然后，在模型的演进过程中，随着达到更高的成熟度级别，你需要基于API为日常业务所带来的能力将其视为组件、模型或生态系统。</p><p></p><h2>API程序成熟度的四个级别</h2><p></p><p></p><p>如果你将API程序成熟度视为企业数字化转型整体方法的一部分，API程序可以分为四个成熟度级别：</p><p></p><h4>级别1：“API的暗黑时代”——API作为数据采集工具</h4><p></p><p></p><p>从历史上看，构建API是为了方便数据采集，Salesforce和Amazon的早期API就是最好的例子。这类API的主要目的就是用于标准化跨多个业务应用程序的数据共享。</p><p></p><p>API程序成熟度的第一个级别是为可以提供单一事实来源的数据采集创建标准化的数据访问接口。这些API按照不同的业务功能分类。例如，你可以有分别用来访问财务、销售、员工和客户数据的API。</p><p></p><p>当你建立了API设计和架构最佳实践，你的组织就达到了API程序成熟度级别1。一些最佳实践包括：</p><p>在设计API时要考虑到集成便利性和可重用性；所有的API都保持一致的接口；在设计中结合版本控制，同时支持多个客户端；确保API的可伸缩性，适应不断变化的用户需求。</p><p></p><p>这些API相对简单，不需要高级的可编程功能。级别1也可以定义为一种相对不成熟的、手动的API部署方法。手动部署的个体API不支持API生命周期管理，技术的重点放在了将API构建为独立的工具上。</p><p></p><h4>级别2：“API的复兴时代”——API作为流程集成组件</h4><p></p><p></p><p>回顾API的发展历史，从2000年代开始，当它们开始被用作连接器来集成不同的系统时，迎来了它们的复兴时代。单点登录（SSO）就是一个典型的例子。SSO时一种被广泛使用的API集成工具，用于对用户进行身份验证，让用户能够安全访问多个应用程序和第三方服务。</p><p></p><p>当你的组织达到API程序成熟度级别2时，你的API程序将使用基于组件的方法。应用程序被分解为单独的组件，每个组件都可以独立于应用程序的其他部分进行开发和测试，然后再集成成一个完整的应用程序。这种方法降低了复杂性，易于维护，并提高了可伸缩性。</p><p></p><p>API将作为集成不同业务和特定领域流程的组件捆绑在一起。这些API包简化了运营和工作流，将多个部门连接起来，甚至可以集成与外部合作伙伴的工作流与交互。</p><p></p><p>当你达到级别2时，你的组织就迈出了将API应用于业务的第一步。在级别2，API被视为组件，为你提供了标准化和可重用的API目录。级别2通过CI/CD（持续集成/持续交付）管道实现标准化和自动化，改进了开发周期，从而推进了API开发和生命周期管理。</p><p></p><h4>级别3：“API的启蒙时代”——API作为统一体验的平台</h4><p></p><p></p><p>在API复兴时代，API被视为组件，以此来简化集成和提升可重用性。级别3是API的启蒙时代，它更进一步，让API变得对用户更加友好和有价值。</p><p></p><p>当你达到级别3，API就不再被视为改进业务工作流的组件或独立的工具。这一级别关注的是构建API套件，通过创建互连的体验来驱动更好的工作流程。API组件的作用在于，API提供者可以在设计和构建API时对应用程序进行分解。而API套件的作用在于，API提供者可以对功能进行分组，让API消费者可以与它们集成，从而获得更好的体验。</p><p></p><p>例如，物流公司依靠卡车和送货车车队来维持业务的连续性。它可以使用API套件来监控和管理其车队的各个方面。在级别3，你需要一个精心设计的包含多个API的API套件来监控卡车、绘制路线、提供性能分析，等等。</p><p></p><p>在级别3，API是用户体验（UX）的关键。API套件成为面向用户的应用程序的支柱。在上面的卡车车队示例中，公司用于车队管理的前端软件依赖于API来驱动最终的用户体验，因此API套件成为为整个软件包提供接口的后端平台。</p><p></p><p>当你达到了级别3，API程序将起着至关重要的作用，因为API套件成了任务关键型的服务。在这个阶段，API消费者做了大量的投入，API的可靠性和成熟度就变得非常重要。处于级别3的API程序都达到了一定程度的技术成熟度，包括：</p><p>部署：API套件进行批量部署，与API生命周期阶段和版本控制紧密结合。性能：API支持云原生环境，可以获得更好的可伸缩性和弹性工作负载。安全性：启用多层安全机制，确保严格的认证和授权过程。自动化：CI/CD管道是完全自动化的，包含了严格的API测试。体验：自助式API门户有助于开发人员快速上手。</p><p></p><h4>级别4：“API的自由化时代”——API作为业务转型的生态系统</h4><p></p><p></p><p>当你的组织达到了API程序成熟度级别4时，你将拥有完全外部化的API。API演进的最后阶段更多地是由业务需求而不是技术驱动的。在这个级别，你可能已经有了一个运行良好的技术栈，并且正在推动内部和合作伙伴采用API，因为API带来了很多业务价值。下一个合乎逻辑的进程是通过货币化将这种价值外部化。</p><p></p><p>在级别4，你将采用一种新的方法——API即产品。在这个级别，你可以通过服务订阅模型向客户提供API。API即产品可以作为独立服务或补充服务来提供，具体根据公司的业务性质来决定。无论采用哪种方式，API都紧密集成到了你的产品、营销和销售中，因此每个人都可以通过协作来推动这种新兴的价值流。</p><p></p><p>在级别4，API程序成为业务增长的引擎。是否已经达到级别4的一些指标包括：</p><p></p><p>API治理</p><p></p><p>你有一个专门的API产品管理小组。这个小组确保所有API都是基于一组预定义的规则开发的。它还定义了API生命周期进展策略，确保API具备架构和安全合规性。</p><p></p><p>API可观测性</p><p></p><p>你的团队不仅能够对API进行监视，还能够捕获API业务逻辑的内部状态，收集与性能有关的数据。</p><p></p><p>API生态系统</p><p></p><p>你建立了一个API社区，让开发人员和消费者有交换意见和寻求支持的地方。API论坛进一步增强了API生态系统，推动API的采用。</p><p></p><h2>API程序改进周期</h2><p></p><p></p><p>这个世界上没有完美的API程序。任何一个API治理框架都必须进行定期审计，确定API程序的当前成熟度级别。</p><p></p><p>无论处于哪一个API成熟度级别，采用DevOps方法并通过持续的小Sprint来提高API成熟度都是必要的。要采用DevOps方法，需要在组织范围内建立共识，实现更敏捷、更快的小增量改进周期。</p><p></p><p>理想的API程序改进周期包括五个阶段。</p><p></p><h4>评估和探索</h4><p></p><p></p><p>第一个阶段是在技术和业务层面评估API程序的当前状态，并探索改进它的可能性。当然，技术成熟度要先于业务成熟度，应该是级别1和级别2的核心关注点。</p><p></p><p>在探索需要改进的领域时，可以设定一些小目标，而不是试图从一个级别直接跳到下一个级别。你可以在内部定义子级别，改进API程序的某个特定方面，例如部署自动化、安全性或可伸缩性。</p><p></p><h4>设计与建议</h4><p></p><p></p><p>第二个阶段是改进周期中最关键的决策点。在这个阶段，你需要梳理来自不同利益相关者的技术规范和业务目标。然后，你可以建议对底层API管理技术栈做出修改，这些修改应该是当前改进周期的一部分。</p><p></p><h4>构建与实现</h4><p></p><p></p><p>第三个阶段是改进周期的实现阶段。这个阶段包含了基于建议的开发和配置增强。</p><p></p><h4>测试与监控</h4><p></p><p></p><p>在第四个阶段，你开始通过测试来驱动API。在这个阶段，你通过监控重要的性能和改进指标来判断API改进周期的总体有效性。这个阶段可能会很长，因为你需要在第三阶段和第四阶段之间来回转换，直到可以从指标上看到可度量的改进。</p><p></p><h4>启动新的API程序</h4><p></p><p></p><p>一旦完成了测试和监控阶段，并且看到了真正的改进，就来到了最后一个阶段——生产部署。在这个阶段，你将新的API程序产品化，并启动和运行它们。</p><p></p><h2>如何提升你的API程序成熟度</h2><p></p><p></p><p>这里给出的不同级别的API程序成熟度提供了一条带有逻辑里程碑的清晰路径，帮助你的组织从低级别API实现过渡到高级API实现，但这里还有一个更重要的挑战需要你去解决。</p><p></p><p>API程序象征着组织范围内采用其他API的氛围。将API的发展定位成推动业务增长的主要引擎之一，这是一个理想的愿景。然而，要让API程序取得成功，必须将其构建成横向的跨部门和团队的功能。</p><p></p><p>在大多数企业中，每一个部门都需要其他部门的清晰度和可见性，这也是为什么需要大量工作来执行治理和标准化的原因之一。缺乏可见性会增加创建重复API的可能性。</p><p></p><p>孤立的API团队可能会带来一些挑战，例如缺乏沟通、理解和可见性。彼此孤立的团队为建立API开发集成策略带来了挑战。此外，每个团队可能有不同的优先级，导致整个过程出现延迟和错误。此外，团队彼此孤立会导致协作和知识共享的机会变少，而这些本来可用于提高正在开发的API的质量。</p><p></p><p>横向的API程序功能跨越了这种部门间的孤岛，有助于确保API的统一治理和标准化。</p><p></p><p>以下是一些你可以用来应对持续改进周期挑战的规则。</p><p></p><h4>由外而内建立共识</h4><p></p><p></p><p>由外而内的方法需要对业务工作流进行分析，围绕它们设计出正确的数字体验。与由内而外的方法相比，由外而内的方法在捕获各种利益相关者的期望方面要有效得多。</p><p></p><h4>自上而下的文化转变</h4><p></p><p></p><p>寻求推动全公司文化转变的最佳实践是一个极具争议的话题。由于成功的API程序需要水平对齐，因此采用自上而下而不是自下而上的方法可以降低出现API蔓延的可能性。</p><p></p><p>自上而下的API开发方法有几个优势，例如缩短发布时间、缩短开发周期和更容易维护。它还为API架构提供了更大程度的清晰度，使得开发人员能够更容易地知道他们必须做些什么以及他们在整个项目中的职责。此外，自上而下的方法可以减少与API安全性、可靠性和文档相关的工作量。</p><p></p><h4>战略观点</h4><p></p><p></p><p>API程序成熟度的初始级别是将API视为技术工具包的一部分。记住，这是一种短期的战术视角。随着API程序的不断发展，必须不断努力构建战略愿景，让API程序带来可通过业务级KPI来度量的价值。</p><p></p><p>达到最高的API程序成熟度级别需要时间和精力，同时还要管理利益相关者的期望。但不管怎样，开发成熟的API程序将为加速业务创新和增长带来新的机会。</p><p></p><p>作者简介：</p><p>Darshan Shivashankar是APIwiz的创始人兼首席执行官。APIwiz是一个低代码API运营平台，旨在通过简化API生命周期管理来提高生产力和实现大规模治理。他有12年帮助企业进行数字化转型的经验。他曾为电信、医疗保健和金融行业的大型企业领导和实现API程序。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/api-maturity-model/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTAxNzA5MzUsImZpbGVHVUlEIjoiTVlneHNCOTVDTlU1TGx2NSIsImlhdCI6MTY5MDE3MDYzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.GnN_eCmUH_ihtMltPOgnFRsI72aQPdHIExmo9K95EHs\">https://www.infoq.com/articles/api-maturity-model/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/shlKO1bTHS2tCFiFpNe1\">API 设计评审已死，API 设计评审万岁</a>\"</p><p><a href=\"https://www.infoq.cn/article/mDlgpAcKK4V3Qr4r9d8E\">API 网关和负载均衡器，到底怎么选？</a>\"</p>",
    "publish_time": "2023-07-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于鲲鹏+昇腾 华鲲振宇重磅发布算力创新产品及解决方案",
    "url": "https://www.infoq.cn/article/Nk9DyErUiAA5uH80qUU0",
    "summary": "<p></p><p>2023年7月26日，<a href=\"https://www.infoq.cn/article/iCuurCMJCvofpfylbGpE\">华鲲振宇</a>\"在北京举办产品发布会，以“中华鲲鹏振兴寰宇，昇腾万里智领未来”为主题，分享基于“鲲鹏+昇腾”的算力创新，发布全新一代算力基础设施与解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec4766737610417cbcf9f7a141a8576e.png\" /></p><p></p><p>此次发布会汇聚产业精英、客户和生态伙伴，共同探讨满足数字时代多样性算力需求的未来发展方向。北京市朝阳区人民政府党组成员、副区长舒毕磊、华为计算业务总裁田华、华鲲振宇总裁万诤等领导出席大会并致辞。</p><p></p><h4>深入场景发布四大“天”系产品 全栈创新敢于亮剑</h4><p></p><p></p><p>擎天之力铸坚实算力底座——“天擎”系列高规格通用计算平台</p><p></p><p>随着应用与场景驱动需求激增，对<a href=\"https://xie.infoq.cn/article/372faa42c86908abae0360a08\">算力基础设施</a>\"提出了更多差异化的需求，本次大会，华鲲振宇正式揭开基于鲲鹏创新架构算力产品的神秘面纱，华鲲振宇研发总经理赵彦钧携华为鲲鹏计算业务副总裁马银川正式发布“天擎”系列高规格通用计算产品，最大支持16块NVMe SSD，创新蝶形IO模组设计，最多可支持11个PCIE 标准插槽，规格提升了37%；率先支持9张 PCIe X8的 NPU/GPU加速卡，或两张全高全长双宽的训练卡，整机AI计算性能提升15%以上，各项参数直指鲲鹏最高规格，为千行百业提供高密性能的算力应用平台，已规模服务于运营商、互联网、金融等行业，满足客户在不同部署场景、不同算力、不同存力需求下的计算任务，让每一次计算都能心有所想、算有所成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52d019f2cd0760e857efad515f809a84.png\" /></p><p></p><p>深耕智算助力大模型普惠AI——“天智”系列高密AI算力平台</p><p></p><p>伴随大模型带来的生成式AI突破，人工智能技术正在跨越重要的历史拐点，华鲲振宇也早已有了筹谋，基于昇腾AI计算，在多个典型应用场景进行了深入创新和升级，积极拥抱人工智能，在大模型训练场景下，推出基于昇腾的“天智”系列高密AI算力服务器——新一代训推一体服务器AT800 A2，底层技术的突破为大模型的训练推理提供极致的算力输出，助推国内大模型应用广泛落地，进一步普惠AI。基于天智训练服务器，华鲲振宇已为“蓉城 • 夔（kuí）牛”短临预测模型提供稳定“算力底座”，以成都智算中心300P算力为基础，实现精细化的气象预报服务。备受关注的第31届世界大学生夏季运动会（成都大运会）即将开赛，\"夔牛\"可作为赛场的一匹“黑马”，助力大运会期间的天气预测，把前沿的“黑科技”运用到大运会“智慧气象”服务中，让AI大模型应用于具体场景。</p><p></p><p>以恒致远令融合算力触手可及——“天恒”超融合移动平台解决方案</p><p></p><p>在<a href=\"https://xie.infoq.cn/article/376502f80af0b1d2c672a8e96\">边缘计算</a>\"场景中，客户对计算设备部署的便利性、可移动性以及异构算力融合方面提出了更高的要求。华鲲振宇产品总经理李锐携手华为昇腾计算业务副总裁刘鑫发布业界首创鲲鹏+昇腾“天恒”超融合移动平台，应对应急救援、海上勘测、地质勘探和航拍合成等各种极端恶劣的环境场景， 具备高可靠性和稳定性，轻松完成信号处理、全景视频融合、IoT数据采集与处理、目标检测等功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fbb31cb233ae282c4203a34f6bfad7c.png\" /></p><p></p><p>技术创新让数据中心极致节能——“天极”系列液冷解决方案</p><p></p><p>在国家双碳战略背景下，数据中心算力规模目前持续高速增长，能耗随之急剧扩张，以应用液冷技术为代表的绿色数据中心，成为新基建、“东数西算”的算力底座，如何降低数据中心的能耗成为整个行业技术创新的焦点。华鲲振宇超前洞察，携手华为集群计算业务副总裁王振华发布双液冷解决方案——天极HC8000冷板式液冷和天极HC6000浸没式液冷，挑战PUE小于1.1，HC6000更是基于鲲鹏+昇腾全球首发的浸没式液冷服务器，采用国产原生冷却液，具有极致节能、极稳运行、轻松运维等极致优势，为关键业务场景提供绿色低碳的强劲算力。</p>",
    "publish_time": "2023-07-27 09:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型刷新教育赛道，网易有道发布国内首个教育大模型“子曰”",
    "url": "https://www.infoq.cn/article/RybdOoGtK9NiYTsqmEgx",
    "summary": "<p>7月26日，教育科技公司<a href=\"https://www.infoq.cn/article/5LaZorFzj54vi6YV5xLf\">网易有道</a>\"在“powered by 子曰”教育大模型应用成果发布会上，推出了国内首个教育领域垂直大模型“子曰”，并发布了基于“子曰”大模型研发的六大创新应用——“LLM翻译”、“虚拟人口语教练”、“AI作文指导”、“语法精讲”、“AI Box”以及“文档问答”。</p><p>&nbsp;</p><p>网易有道CEO周枫表示：“一个好的技术有没有价值、能不能发挥巨大的作用，很多时候关键在场景和应用的选择以及细节的打磨。通过软件、硬件、AI技术的结合，做出精品是我们现在做的事”。</p><p></p><h2>有道发布教育大模型“子曰”，六大创新应用成果正式落地</h2><p></p><p>&nbsp;</p><p>大模型的出现能给教育带来的最大机会是什么？</p><p>&nbsp;</p><p>周枫在发布会上表示：“我认为，是助力因材施教”。据介绍，之所以叫‘子曰’，是因为孔子是我国的教育先贤，又是因材施教教育理念的奠基者。‘夫子教人，各因其材’，我们希望子曰大模型可以朝着这样的教育理想去做。”</p><p>&nbsp;</p><p>周枫向在场观众分享了大模型“因材施教”的三大优势。首先，大模型能为学生提供个性化的分析和指导；其次，大模型能够实现引导式学习，与教师一样，提出问题并引导学生自行探索答案；最后，大模型具备全科知识整合能力。通过连接多模态知识库、跨学科整合知识内容，大模型能随时满足学生的动态需求，帮助孩子培养更综合的能力。</p><p>&nbsp;</p><p>相比于通用大模型，“子曰”大模型从一开始就定位为是一个“场景为先”的教育垂类大模型。它能够作为基座模型支持诸多下游任务，向所有下游场景提供语义理解、知识表达等基础能力。基于此，有道研发团队在“子曰”大模型的基础上，为不同学习场景设计了定制化的模型，以实现模型与场景的高度契合。</p><p>&nbsp;</p><p>“我们的目标很明确，就是以实际的教育场景驱动，用技术创新助力教育创新。”周枫表示，希望技术和产品的深度融合，可以根据学⽣不同能⼒和需求，提供因人而异的个性化教学。</p><p></p><p>在发布会上，网易有道展示了“子曰”大模型在多个场景中的应用成果，包括“LLM翻译”、“虚拟人口语教练”、“AI作文指导”、“语法精讲”、“AI Box”以及“文档问答”。</p><p>&nbsp;</p><p>其中，最为瞩目的是大模型时代的英语口语练习神器——虚拟人口语教练Hi Echo。发布会现场，有道词典业务负责人与 Echo 进行了多组随机对话。Echo 能迅速理解场景和上下文，并给出迅速反馈，发音也很地道，重音、弱读、升降调等细节处理得非常到位。在对话过程中，Echo能够像真人老师一样循循善诱，启发式进行对话引导，还能进行实时反馈。对话结束后，Echo会从发音、语法等维度给予建议和润色，能有效解决长期困扰英语口语学习者无话可说、不知从何说起、害怕说错等问题。</p><p>&nbsp;</p><p>“中国人在说英语时往往面临开口难、不敢说、不知道该从何说起的困境，其中的关键就在于缺乏语言环境。” 有道词典业务负责人表示，Echo恰恰能为用户带来这种真正贴合实际的“语境”，帮助他们更好地练习英语口语。</p><p>&nbsp;</p><p>此外，“子曰”大模型还覆盖了多种学习场景。例如，在写英语作业时，学生们不仅有解决具体问题的需求，还需要学会举一反三。“子曰”大模型赋能的“语法精讲”功能可以为学生提供针对性的解题思路和方法，还能推荐同类型的考题，帮助学生触类旁通，真正理解考纲中的考点。</p><p>&nbsp;</p><p>“AI作文指导”应用不仅具备“作文批改”功能，还具备“作文指导”功能。据介绍，该应用旨在解决“学生不会写”和“老师没时间改”的问题。针对学生在写作、前、中后过程中面临的题目主旨难确定、写作素材匮乏等难题，该应用都能够给予指导，帮助学生“下笔如有神”。批改环节中，AI作文指导还会从表达、结构、内容深度、情感丰富度四大维度全面提供改进建议。</p><p></p><h2>场景拉动，“AI+教育”技术沉淀与创新&nbsp;</h2><p></p><p>&nbsp;</p><p>会上，周枫多次强调“场景拉动”的重要性。他表示，“子曰”大模型在教育行业的应用，不仅可以帮助学生更好地学习，也可以帮助老师更好地教学，借此实现因材施教的教育理想。</p><p>&nbsp;</p><p>通过深入调研和分析用户在不同场景下的需求，网易有道成功利用大模型的力量，在教育领域打造了诸如虚拟人口语教练、语法精讲、AI 写作指导等丰富的解决方案。这一策略不仅体现了网易有道对教育场景的深入理解，还为用户提供了更加个性化和高效的学习体验。</p><p>&nbsp;</p><p>早在2008年，有道就推出自主研发的国内首家统计机器翻译线上引擎。经过15年技术迭代，有道神经网络翻译（NMT）已经进化成行业领先的“最强大脑”。根据QUESTMOBILE最新数据，到目前有道词典月活用户已经超过1亿，是国内词典翻译市场的第一名。</p><p>&nbsp;</p><p>从2016年开始，有道协同构建AI基础能力，同步组建语言、视觉、声音等团队，目前积累了有道神经网络翻译（NMT）、计算机视觉、智能语音AI技术、高性能计算(HPC）四大底层技术能力。</p><p>&nbsp;</p><p>自2017年，有道就与主流技术<a href=\"https://www.infoq.cn/article/voRBHu6lzxRuNplYwD0P\">Transformer</a>\"“双向奔赴”，将AI能力统一在大模型之下，并尤其重视在端侧的落地应用。有道词典笔2代2019年首次搭载离线Transformer NMT。2022年，有道词典笔P5中搭载了自研的离线ASR，也已升级为Transformer技术。技术的持续赋能奠定了有道学习硬件在行业内的领先地位。</p><p>&nbsp;</p><p>底层技术不断革新的同时，有道还在不断研发细分场景下的“黑科技”。如虚拟人口语教练实现语音识别能力、虚拟人驱动技术和内容生成和对话能力等多项技术能力的突破。&nbsp;例如，虚拟人口语教练在语音识别能力方面进行了巨大的革新，它支持多语种的流式低延迟语音识别技术，让Echo在中式英语、英语、中英混合等场景下游刃有余；声学降噪、回声消除、自动语音检测、自动断句等技术，则让它像一个真正的倾听者和交流者，不仅能判断用户说话的起始，还能让用户随时打断，智能触发后续流程。</p><p>&nbsp;</p><p>在AI虚拟人的驱动方面，有道基于自主研发的情感识别算法和实时渲染驱动引擎，对播放的语音数据进行深度分析，实时驱动虚拟人的面部表情和语音同步的口型变化，使虚拟人能够贴近真人，以更加自然和生动的方式与用户一对一交互，从而显著提升对话的真实感和用户体验。</p>",
    "publish_time": "2023-07-27 10:49:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型趋势下，企业数据体系有何挑战？有何新机遇？",
    "url": "https://www.infoq.cn/article/0UqI1dHapss161qeXyr3",
    "summary": "<p></p><p>随着<a href=\"https://www.infoq.cn/article/kWKCoO36iYG4RmoGyatN\">大模型</a>\"的兴起，企业数据体系的构建和管理面临着新的挑战和机遇。对于企业来说，如何以最低的成本、最安全的方式，最快速地调整和优化数据体系以支持大模型的训练和应用，成为了一个重要的议题。在刚刚结束的 ArchSummit 全球架构师峰会 2023（深圳站）中，数巅科技创始人、CEO 何昌华博士在主论坛上围绕“大模型趋势下的企业数据体系思考”展开了主题分享，在演讲中，他为大家介绍了他在大模型时代数据处理领域的发展趋势洞察，并简明扼要地为大家介绍了虚拟化技术、物化视图等关键技术与前沿数据理念，现场的架构师们获得了有关未来企业数据架构构建的关键洞察和有效指导，以应对大模型快速发展趋势下的大数据环境。</p><p>&nbsp;</p><p>为了更深度地了解大模型与企业数据治理领域的发展，在演讲结束后，InfoQ 对何博士进行了视频专访，以下是视频专访的全部内容，为方便读者查看，视频下方也附上了文字内容。</p><p>&nbsp;</p><p></p><p></p><p>采访记者：InfoQ 资深编辑鲁冬雪</p><p>采访嘉宾：数巅科技创始人、CEO 何昌华博士</p><p>嘉宾简介：斯坦福大学 PhD，数巅科技创始人、CEO。之前曾经在蚂蚁集团担任计算智能部门负责人，计算存储首席架构师；2017 年之前在硅谷任职于 Google，Airbnb 等互联网公司。在过往职业生涯中，何昌华主导开发过实时智能决策系统、金融级的分布式图数据库、新一代分布式计算引擎、下一代逻辑数仓、新一代搜索引擎架构等。</p><p>&nbsp;</p><p></p><p>InfoQ：您今天的演讲围绕《大模型趋势下的企业数据体系思考》展开的分享，您在演讲中提到了数据治理、人工智能领域的技术发展速度非常快，您能用三个关键词来总结下数据治理与人工智能领域未来发展的主要趋势是什么吗？</p><p>&nbsp;</p><p>何昌华：第一个词是“规模化”。很明显规模化意味着数据、模型的规模，这些都是在极度膨胀，同时也意味着应用场景其实是越来越多，也要呈规模化的发展。</p><p>&nbsp;</p><p>第二个词是“自动化”。以往通过人工来加工各类数据、做各类模型，这个过程会慢慢地被越来越多的自动化流程所取代，因为这样整个流程才能真正地实现自动化，没有人工的干预并且整个系统能够自动往前演进。</p><p>&nbsp;</p><p>还有一个词是“融合”，或者叫“协同”。当前大家可以看到各类来源的数据，会把它融合在一起，成为一个叫做所谓超融合的数据体系。对模型而言，大模型基本上是具备了以前很多个小模型在不同场景下的一些通用能力，而“大模型”和“数据”又应该协同起来。</p><p>&nbsp;</p><p></p><p>InfoQ：在您看来，人工智能和大数据领域最重要的技术挑战是什么？目前是否有比较好的解决方案？</p><p>&nbsp;</p><p>何昌华：我觉得这两个领域有区分，但又是紧密联系在一起的。现在面临的一个非常核心的问题就是如何获得“高质量的数据”，事实上大家可以看到 ChatGPT，它之所以能够实现这样的能力，是因为它完全是在工程上，尤其是在数据策略上做到了极致，但其实本质上它并没有一个理论上的突破。他们有非常多的人，不仅仅是收集到公共数据，他还会去给数据做各种各样的分析、标注以及挑选合适的数据，最后做出了这样的一个模型。而反观我们有很多大模型的公司，事实上都是处在一个相对比较缺乏数据的阶段，尤其缺乏高质量的数据，这是一个非常大的挑战。</p><p>&nbsp;</p><p>另外一个就是“整个计算的能力”，随着模型的规模越来越大，需要的算力越来越高。算力本身的转化，需要大量的设备与金钱来解决问题。这个问题其实还有一些解决方案，至少我们有能力去做大规模的集群，或者去打造一种专用的芯片，能让这个计算的密度变高，这个问题还是容易被解决的。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您在 2022 年创立了数巅科技，在创业之前在蚂蚁集团工作，想问一下您的创业初衷是什么？</p><p>&nbsp;</p><p>何昌华：我在蚂蚁集团的时候就觉得数据智能这个事情在金融行业是非常有可能落地的。因为前一个巨大的场景就是搜索推荐广告（互联网场景）。通过在蚂蚁集团多年的观察，我发现“智能数据治理”不仅只服务于金融行业，它同时能够降低数据智能在很多其他行业的门槛。我们现在讲产业互联网的升级，或者讲企业数字化，它是真的可以帮到各行各业的，所以我们创立了数巅科技。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：数巅科技是一家怎样的公司呢？公司愿景是怎样的？</p><p>&nbsp;</p><p>何昌华：数巅科技的愿景是让数据智能像水电一样简单，让每个人都能方便地使用数据智能技术。这个愿景看似很高端，但我们的初心是让数据智能成为真正人人可用、触手可及的一项能力。</p><p>&nbsp;</p><p>数巅科技目前已经拥有近百人的工程师团队，我们是技术驱动、产品驱动的一家公司，基本上大家都是在雕磨技术。我们的技术目前在世界上还没有完全对标的竞争对手，我们不断创新，打造出独特的技术。我们的团队全力打造这项技术，让它成为一项领先于市场的创新型技术。</p><p>&nbsp;</p><p>数巅科技目前有两个核心产品。第一个产品是最底层的<a href=\"https://www.infoq.cn/article/Re4O8YyDbACbrsOh3R7l\">数据引擎</a>\"，它基于数据虚拟化技术打造，具有通用的数据服务功能。这个引擎提供了虚拟的数据语义，使用户无需关注技术细节，像使用水龙头一样方便直接使用数据。同时，这个产品还采用了智能化和自动化的技术，性能比原来的产品更优秀。这个产品已经在实际场景中得到了应用和打磨，现在在多家企业得到了应用。</p><p>&nbsp;</p><p>第二个产品我们叫大模型的智能助手，它基于现有大模型的技术，充分利用企业内部的数据资产，帮助用户充分协同大模型的能力，从而让生产一个大模型的应用，在企业内部变得非常简单。这个产品可以帮助企业进行智能的 BI 分析，用户可以直接说出指标和看板。同时，这个产品也提供了智能问答功能，可以结合企业内部的规章制度，回答用户提出的问题。这个产品已经在不同的场景下得到了应用和验证，表现出了良好的性能和效果，深受用户喜爱。</p><p>&nbsp;</p><p>我们也做了智能的运营，你提一个问题，它就可以告诉你一个运营方案。我们也做了这样的一些智能的问答，它可以结合你企业内部的规章制度、企业内部的很多数据，回答用户提出的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：据悉，公司定位为“大模型应用和企业数据的协同者”，为何会将此作为公司定位？</p><p>&nbsp;</p><p>何昌华：我们也是一直在思考这个问题，回到公司的愿景，我们还是希望能够让数据智能这个事情，能够很容易的让所有人能够用到。尤其（在）大模型出来的时候，我们就发现大模型它完完全全地降低了模型的门槛。以前我可能要花很大的精力针对一个场景去研发一个模型，当你发现有这样一个大语言模型，它的能力覆盖足够多的通用的场景。</p><p>&nbsp;</p><p>第二个就是我们又发现在企业内部，它拥有大量的数据，甚至很多企业已经有了很比较成熟的数据体系。但是那样的一套数据体系，它以前是完全没有考虑过大模型这样的一个角色的，它那个数据体系里面基本上在做数据分析的一些东西。</p><p>&nbsp;</p><p>所以我们觉得在这两个之间有一个巨大的鸿沟。并且从我们在技术上的理解来看，这两者也必须充分协同，才能够真正地做到企业业务的智能决策，所以我们才把自己定位在这样一个地方，并且我们能够抽象出一个通用的引擎来做这件事情。</p><p>&nbsp;</p><p>InfoQ：听起来其实也不是大模型应用了，我觉得定位更像是“大模型与企业数据的协同者”。</p><p>何昌华：是的，我觉得我们不能算一个大模型的应用。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：刚才您也聊到，咱们现在目前在研发的核心产品是数据虚拟化引擎和大模型的应用框架。我们先来了解一下数据虚拟化引擎，您觉得它的主要的功能和优势是什么？</p><p>&nbsp;</p><p>何昌华：我觉得它最核心的功能上的优势之一，即用户可见的功能。它的确将大数据本身的复杂度全都屏蔽在用户之外了。在大数据过去 20 年的发展中，其实主要是因为数据量太大难以计算，因此才发展出一整套包括批处理、流处理以及各类数据加工，最后再回到现在所谓的一个 <a href=\"https://xie.infoq.cn/article/77ec0d231d36c963a8e6d1630\">OLAP 引擎</a>\"供大家消费。这条链路很长，我们能够看到很多中小企业，根本没有能力搭起这样一条链路。用户去消费数据时，他需要理解前面是什么样的数据和引擎，其理解的成本也很高。因此我们想用数据虚拟化引擎给用户真的提供一个纯数据视角。他作为一个业务开发者或一个业务人员，其所看到的就是一片数据。至于此数据你究竟是用什么引擎处理、存储位置、其计算的内容、他都不用关注，我觉得这是数据虚拟化引擎提供的最核心的一个能力。</p><p>&nbsp;</p><p>我们后面需要把它的性能做到极致，这种情况下，我们也会做很多针对数据消费的语言，我们叫 SQL，针对 SQL 语言的优化包括自动的物化、包括加速等这样的能力。这个引擎可以帮助分析你的数据体系里有多少数据是重复的？有多少计算是现在冗余的？你又如何优化现有的体系？这是能够显著地给企业降本增效的。同时，企业如果用我们的引擎来访问它的数据，它事实上又能够得到更高效的数据访问、更容易地在未来跟大模型来集成。这些都是用户在实际使用过程中会看到的一些优势。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：数据虚拟化引擎的技术特点有什么？刚才您也提到了 SQL，是不是在识别分析优化能力上，它的表现还是很出色的？</p><p>&nbsp;</p><p>何昌华：我觉得这个引擎虚拟化是一个抽象的概念。它非常核心的能力是智能化，智能化就包含我们能够智能地对 SQL 做很多优化，我们能够智能地预测出来。譬如说可能哪一些中间表是需要预先加工出来的，这个技术叫做物化视图。这样下一个 SQL 执行的时候就会更加高效、更快，同时我们也能够做全局智能的编排。SQL 执行下来后，我应该怎么样分解这个任务、怎样去执行它、包括我内部的存储的数据该去有怎样的索引、如何跟外部交互，这些现在均在一个智能引擎的控制之下。这应该也算一条较创新的路。</p><p>&nbsp;</p><p>当然从产品模式上来说，这个智能化的过程又是从产品上开放出接口，让用户完可以完全地、白盒化地控制它，因为事实上很多用户还是希望对中间的数据能有很多手动的控制，与一些自动、手动的处理。在这种情况下，我们完全在产品上会开放出给他们调节的机会。但是整个引擎本身是基于一个智能化的能力的。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>InfoQ：我们还是聚焦到技术上，您觉得数据虚拟化引擎是如何帮助公司实现数据的高效处理和分析？</p><p>&nbsp;</p><p>何昌华：我觉得这一层可以看作是实现了一个逻辑的湖仓一体。现在业界也有这样的一些概念出来，即所谓的逻辑数仓，大家都在基于这个概念在做产品，但我觉得我们其实没有100%的和这个概念匹配，概念上我们相当于是建立起来一个逻辑的数仓，或者叫逻辑的数据集市。</p><p>&nbsp;</p><p>这个逻辑化的体系，其实它是完全针对数据语义的。用户在使用的过程中，譬如说我们给其提供了一个虚拟宽表。现在就是做大数据的从业者，大家都会谈到大数据宽表的概念。数据宽表是非常难以加工，即你要用大量的计算存储才能把宽表加工出来。我们提供一个逻辑宽表很简单，但用户针对逻辑宽表来消费数据时，我们会根据其消费找到对应的物理数据，然后对 SQL 做各类优化处理。</p><p>包括我前置的可能会做一些预先的物化，从而能够让此 SQL 更加高速。这是我们整个链路上很核心的两个能力。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：“数据虚拟化引擎”在整个大模型技术的发展中扮演了一个什么角色？</p><p>&nbsp;</p><p>何昌华：这样的一个数据虚拟化的引擎在我们分析的范围以内，它是最适合于大模型去迭代并做好业务决策协同者的角色的。不容讳言在整个大模型的生态里面，大模型本身它肯定是最重要的能力。大模型的能力越强，肯定越易做出一个很正确的业务决策。但我们也发现大模型的技术本身可能无法处理如此大量的结构化的数据并做精确的计算后来辅助决策，它更多是提供一个逻辑思考的能力。当然这个逻辑思考的能力，需要有语言把它训练出来。本质上，它是在根据企业数据在去做决策的时候提供的一个逻辑思考能力。</p><p>&nbsp;</p><p>关于企业数据本身，它应该提供计算、存储等一些多样化的能力。所以我们认为数据虚拟化引擎是最好的、能够去跟大模型对接的数据工具之一，它能够帮助企业把你的把数据资产完全统一管起来，并且能够加速计算，同时能够跟大模型做好非常充分的交互。</p><p>&nbsp;</p><p>InfoQ：所以您觉得“数据虚拟化引擎”是一个很好的与大模型技术对接的一个工具？</p><p>何昌华：是的，我觉得两者相辅相成。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：关于数巅的另外一个产品“大模型应用框架”的主要特点和优势是什么？它又是如何支持大模型训练和部署的？ &nbsp;</p><p>&nbsp;</p><p>何昌华：大模型的应用框架，本质上我们是想打造一套自己往前能够自我迭代演进的大模型的系统。就像刚刚说的，对大模型而言，我们更多的是用一些开源的大模型做一些微调，我们并没有真正去做一个基础的大模型。但是我们认为一个大模型要在企业内部要成为一个智能的、数字的人，或者帮助做智能角色的助手的角色的话，它一定是需要能自我迭代的。否则的话这个模型很快就会过时，因此它需要能自我迭代。自我迭代的过程在企业内部如何建立起来？以及包括自我迭代需要跟很多业务场景落地，可能需要跟业务场景上的系统打通，它就可能需要连接上各类数据，也包括如何驱动各种自动化的工具来实现这样的业务决策。其中有很多工作，我们这个框架希望能把此类工作沉淀下来后结合我们的大模型以及一个数据虚拟化引擎，这样企业就有了一套完整的，相当于是大模型能够落地，并且能够往前演进，在各个场景下能够快速落地的一套解决方案。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：“大模型应用框架”在大模型应用中有没有优秀实践可以分享一下？</p><p>何昌华：目前已有若干实际案例，这也是我们觉得这样的框架很有意义的原因。我们发现在这个下面，与客户沟通的过程中当我们听到他们有某一诉求时，用这样一套框架去搭配另一个。这种业务场景落地很迅速，基本均以周记，即几周时间你所需的业务场景就可以快速落地了。</p><p>&nbsp;</p><p>其中有若干例子，比如有客户会说我有一堆指标资产，你能否接入它们？同时我拿自然语言说一句话，你就能够把对应的资产给我找到，这个情况下我们就用虚拟化引擎去对接其资产是很容易的，然后去部署一个我们微调过的、开源的大模型，跟着这套框架很快就打造出来这样一个体系，我们叫做智能的指标体系。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您如何看待人工智能和大数据领域的安全和隐私问题？数巅科技有什么措施来保护这些信息？</p><p>&nbsp;</p><p>何昌华：这是一个很好的问题。包括安全隐私类、伦理类问题，这都是现在大家非常担心的，我觉得完全可以理解。</p><p>&nbsp;</p><p>我对这个问题的看法是，首先这肯定是我们需要正视的一个问题，包括我们遇到很多企业就确实存在它们很多数据不能跟公有数据一起去被大模型所训练（的情况），这也是为什么我们觉得有一个企业内部的独立的数据系统可以更好地去管控。</p><p>&nbsp;</p><p>同时包括大模型训练的很多数据，就算在很多数据里面，它也需要做好一些安全性的控制。包括让大模型不要学到很多负面的东西，就像教小孩一样，此类一些区别性的对待，我觉得这都是很关键的一些举措。在我们数巅，我们希望在企业内部能够把数据跟通用大模型的能力隔离开。</p><p>&nbsp;</p><p>大家可以看到，譬如在我们的数据虚拟化引擎里，我们会对数据的权限控制得很严格。我们可以到“行、列”的级别都在控制数据访问权限。当我们去调用大模型的时候，我们可以决定调什么大模型，以及把什么样的数据传给它，这都是有一个非常好的总控的。所以这一块我们是非常关注的一个问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：确实是，目前感觉数巅在安全隐私这块也关注的较多。那您觉得除了安全问题以外，大模型在未来的应用中还有哪些风险和挑战？</p><p>&nbsp;</p><p>何昌华：具体来说的话，模型展现出了很强的能力。因为我们可能在一些金融行业，应该说是在对安全要求比较严格的一些领域里面，我们发现大模型可能除了大家经常说的错觉，有时它也会出一些错误的结果，此类问题之外，可能最需要解决的是可控性的问题。大模型目前的输出结果，因为跟你的上下文相关，所以有时你会发现即便是同样的一个问题，你问它两次，它会给出你不同的答案。我觉得在普通的对话中其实关系不大，但当我要做一个很 critical 的业务决策的时候，这种情况我是不能允许的。我问同一个问题，你肯定应该给出的是同样的一个建议。</p><p>&nbsp;</p><p>在我们数巅，我们也在想办法去解决这类问题。事实上我们发现让大模型把这个执行计划分解的越细，对微小的执行计划我们的可控程度越高，我们能够逐步达到一个相对比较可控的利用大模型智能的一个输出。这个反正也在探索的过程中，但这个我也觉得会是在大模型未来发展过程中，它真正会有非常广泛化的应用。打个比喻，像我们评价人一样，要让我觉得一个人靠谱，肯定我每次问他一个问题，他告诉我的是同一个正确答案，而不是说每次都不一样。我觉得大概可以这样来比喻。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：随着技术及企业业务的发展，您觉得未来企业在数据处理方面将面临的最大问题是什么？数巅科技是否已经在准备应对方案？</p><p>&nbsp;</p><p>何昌华：现在企业经过了这么多年的发展，往往他都有了自己的一套系统，或者甚至是不止一套。我们能够看到很多企业从最开始的数据仓库，到基于大数据的解决方案，到后来的实时，到后来的在线分析。在很多企业内部，它是有很多这样的系统一路发展过来，他们面临的很大问题就是这些系统都是割裂的，从而导致存储是冗余的，导致很难操作一些系统。在这样一些企业内部，他们肯定会希望把所有的东西能够统一地融合到同一套体系里去。</p><p>&nbsp;</p><p>所以，数巅的“数据虚拟化引擎”也是希望为企业提供这样一个选择。当然这个融合的方式可以有多种，可以是数据中台的方案，也可以是其他引擎方案。从大模型角度出发，从怎么去更好地支持未来的数据智能的角度，这是我们相对思考的多一些的内容。</p><p>&nbsp;</p><p>对于我们的产品而言，当我把“数据虚拟化引擎”架在企业的很多套体系的上层的时候，你可以去访问到它所有的数据，对于计算速度较慢的数据，引擎会将其预先拖到自己的缓存中，从而加快计算速度；对于计算速度较快的数据，引擎会直接将请求发送给数据源。有一些它计算得非常快的，我就直接会把请求发给他，在我们内部都有这样的一套自动判断的机制。</p><p>&nbsp;</p><p>所以本质上数据虚拟化引擎，相当于是建立起了这样的一个用户访问数据的一个桥梁。只不过以往大家谈到数据虚拟化这一层就会做得非常的薄。那个时候有一个概念叫 data federation，就是数据联盟、联邦查询、数据联邦，但是那个你做得太薄以后，你就很难确保它能够实现高效的访问。我们相当于是把这个再往前又迈了一步，做的有自己的缓存策略，有自己的智能优化的手段，有自己智能的内核在那里控制，从而让它做到极为高效地来解决这个问题。如果数据虚拟化引擎在一个企业内部能够将其规模充分利用起来，那它可以帮助企业做数据诊断的工作。但同时它到第二个阶段，直接用我们的引擎去访问数据的时候，它也可以帮企业统一它所有的数据体系，提供一个统一的、给业务数据的一个视图。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您理想中企业数据治理的未来是怎样的？数巅科技的下一步技术迭代方向是什么？您是否有信心达到“理想中”的未来？信心来源于哪里？</p><p>&nbsp;</p><p>何昌华：肯定是有信心的，正是因为觉得这个理想有实现的可能才会去尝试。我们的理想是在一个企业内部，通过我们的产品能够充分地把数据能够管好、用好，能够跟大模型深度的协同起来，为企业提供智能的业务决策能力。</p><p>&nbsp;</p><p>我觉得这里面是一个很大的体系，在这其中，我们可能会在数据方面提供更多的价值。有一些公司可能是在做纯粹的大模型，这两者是互补与协作的一个关系。</p><p>&nbsp;</p><p>在数据这一块，我们从最开始一直有一个比较朴素的理想，我们那时候在讲数据自动驾驶这件事，就是说我们认为在一个企业内部，当用户、业务要用数据的时候，他只应该关注这个数据究竟是什么意思？比如说我要用每个人的消费额度，或者我要用一个什么样的数，这个东西我觉得是他应该关注的，其他的都应该是自动把它解决掉的，我们把它叫“数据的自动驾驶”。像今天也有一位嘉宾老师分享到了，比如说自动驾驶从 L1 到 L5，以前的数据我觉得有点相当于是 L1、L2 这样的，或者最多到 L3 这样的。我们可能有一些引擎的能力越来越强，但是用户还是需要去关注所有的事情，我们希望做到的就是用户完全不感知底下的东西，像“我坐上车只要报一个目的地，这个车就会自动开到那”。我们目前在不停地摸索这个过程。</p>",
    "publish_time": "2023-07-27 11:09:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "8年了，Transformer注意力机制一直有Bug？",
    "url": "https://www.infoq.cn/article/tu8SRD5u1ecTQ40lvajH",
    "summary": "<p></p><blockquote>注意力机制是一个构建网络的思路，也是 Transformer 模型的核心。</blockquote><p></p><p>&nbsp;</p><p>注意力是人类认知功能的重要组成部分，指人的心理活动对外界一定事物的指向和集中。在大模型中也有注意力机制。比如神经网络语言模型面对一个由 n 个单词组成的句子时，不同位置的单词，重要性是不一样的。因此，需要让模型“注意”到那些相对更加重要的单词，这种方式称之为注意力机制，也称作 Attention 机制。</p><p>&nbsp;</p><p>简单来说，<a href=\"https://time.geekbang.org/column/article/461691\">注意力机制</a>\"要做的事情就是：找到最重要的关键内容。它对网络中的输入（或者中间层）的不同位置，给予了不同的注意力或者权重，然后再通过学习，网络就可以逐渐知道哪些是重点，哪些是可以舍弃的内容了。</p><p>&nbsp;</p><p>2017 年，谷歌发布《Attention is All You Need》，这也是关于注意力机制最经典的论文。在这篇论文中，谷歌提出了一个新型神经网络架构——Transformer，它完全基于注意力机制，省去了循环层和卷积层。并在 2014 年 WMT 英德翻译任务中达到 28.4 BLEU，比现有的最佳结果(包括集成部分)提高了 2 个 BLEU 以上。结果表明，Transformer 可以很好地将其推广到其他任务，不论是大量还是有限的训练数据。</p><p>&nbsp;</p><p>2018 年，OpenAI 发布了 GPT，谷歌发布了 BERT，两大模型都是基于 Transformer 结构构建的——GPT 是基于 Transformer Decoder 结构和无监督预训练方法实现的生成式预训练语言模型，BERT 是基于 Transformer Encoder 结构的预训练语言模型。</p><p>&nbsp;</p><p>随着近几年 AIGC 技术的爆火以及 ChatGPT 成为现象级应用，Transformer 正以井喷的势头快速发展，基于 Transformer 架构打造的模型数量激增，并将&nbsp;Transformer 的热度推上了新的高峰。</p><p>&nbsp;</p><p>然而近日，Eppo 初创公司的工程师 Evan Miller 在 Twitter 上表示，他发现注意力机制有一个存在了 8 年的 Bug，所有 Transformer 模型（GPT、LLaMA 等）都会受到影响。虽然研究人员上个月隔离了该错误，但他们忽视了一个简单的解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab995ca58d8aa47f479353322887e562.png\" /></p><p></p><p>Miller 在一篇博客中详细阐述了该 Bug，并提出了修复方案。Miller 表示，他在跟 Pytorch 和 biblatex 的一连串交锋中败下阵来。虽然如此，他还是想向大家解释当前这代 AI 模型如何在关键位置出现差一错误，并导致每一个 Transformer 模型都难以压缩和部署。</p><p>&nbsp;</p><p>以下是 Mille 的博客原文，经编译。</p><p></p><h2>这个Bug有多重要？</h2><p></p><p>&nbsp;</p><p>大家发现这个公式中的差一错误了吗？</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/80ad4e6dc46ce5f6f143bc12e24416ec.png\" /></p><p></p><p>首先，我们来聊聊为什么这个差一错误这么重要。ChatGPT的效果不是挺好吗，老兄，你在闹什么呢？其实我第一次察觉有问题，是在认真阅读关于量化的研究论文时。大语言模型的探索者们正尝试将其塞进Mac Mini、Raspberry Pi，甚至是越了狱的家用智能恒温器。</p><p>&nbsp;</p><p>但AI从业者都知道，如今限制最大的因素就是RAM容量。无论是在云端还是边缘，RAM占用量越小、能做的事情就越多。大语言模型拥有几十亿的权重参数，如果我们能想办法帮这些参数成功瘦身，那完全可以写出更好的诗歌、生成更好的综述文章，或者加快人类末日到来的脚步——具体要看大家怎么应用了。</p><p>&nbsp;</p><p>RAM负责存储信息。这好像是废话，但大家先别急。信息属于负对数概率，代表我们需要用多少个bit来存储事物。如果数字流具备高度可预测性，例如总是处于有限的范围之内，那么我们要使用的存储bit量就可以更少。而如果一串数字毫无可预测性，包括偶尔甚至会出现极其巨大的数字，那我们就需要更多位二进制数字对其做编码。</p><p>&nbsp;</p><p>而这正是大语言模型的真实情况。出于种种或明确、或含糊的原因，Transformer模型会包含这些异常权重，用本不必存在的臃肿体态来应对那些发生概率极低、极低的情况。但奇怪的是，似乎没人在意这件事：不对吧，这类罕见异常值与我们之前所认为的、关于构建良好神经网络的一切知识都背道而驰。关于这些异常值，人们已经写过不少论文，也在制定各种“瘦身”方案用更少的0和1对其做编码。顺带一提，直接使用比例和偏差整量化会导致性能大幅下降，所以还得想点更好的办法。</p><p>&nbsp;</p><p>目前关于这个问题的最佳分析，来自高通AI研究团队发表的论文《量化Transformers：通过让注意力头什么都不做来消除离群值》（<a href=\"https://arxiv.org/abs/2306.12929\">https://arxiv.org/abs/2306.12929</a>\"）。作者将这些离群值追溯到了注意力机制中的Softmax函数，之前没人认为这个看似无辜的指数器居然拥有如此野蛮的峰度。其实研究人员马上就要发现其中的差一错误了，但此刻他们可能是在意大利度假，否则怎么就没人回复我发过去的邮件提议呢？于是乎，我只能用最传统的方式向国际学术界发出呼吁。</p><p>&nbsp;</p><p>如果大家读过链接中的论文，请直接忽略他们给出的建议。我知道这话讲得不客气，但裁剪之后的Softmax带有一个旋转的零梯度，而他们的门控注意力建议虽然可行，却要靠引入数百万个新参数来解决一个实质上仅是增量失败的问题。这里明显有个简单且直观的解决方案，但从我接触过的相关讨论来看，似乎没人想到要尝试。</p><p>&nbsp;</p><p>好吧，关于这个愚蠢的错误，我已经介绍得够多了。现在咱们来看看Softmax函数，还有它在注意力机制里惹出了怎样的麻烦。</p><p></p><h2>Softmax惹出了什么麻烦？</h2><p></p><p>&nbsp;</p><p>要想明确解释这个Bug，大家先得真正理解注意力机制是干什么的。大多数数值Bug源自程序员把方程给弄错了。但如果错误不出在代码上、而是出在数学上时，我们就必须先搞清当前方程的来源和它的预期效果，然后才能加以修复。</p><p>&nbsp;</p><p>为此，我不得不认真阅读了大约50篇arXiV文章才逐渐理清思路。关于这个问题的讨论，我们先从所谓“输入嵌入”入手，这是一个浮点向量，表示输入字符串中的一个单词。</p><p>&nbsp;</p><p>这个向量似乎正随着模型的逐年发展而越来越长。例如，Meta最近的LlaMA 2模型使用的嵌入向量长度为3204，其半精度浮点计算结果为6&nbsp;KB以上。是的，这仅仅对应词汇表中的一个单词，而词汇表通常要包含3到5万个单词条目。</p><p>&nbsp;</p><p>现在，如果大家跟我一样是对内存极为吝惜的C程序员，那肯定会考虑：为什么这帮AI蠢货非要用6&nbsp;KB空间来表达这种双字节信息？如果总词汇量不超过216=65384，那我们只需要16个bit就能表示一个条目了，对吧？</p><p>&nbsp;</p><p>确实，人家Transformer其实也是这么干的：它将输入向量转换成相同大小的输出向量，而最终的6&nbsp;KB输出向量会对预测当前token之后的下一token所需要的全部内容进行编码。</p><p>&nbsp;</p><p>Transformer每个层的工作，其实就是把信息添加到原始单字向量当中。这就是残差连接的意义所在：整个注意力机制只是向原始的两个字节的信息添加补充材料，通过分析更多上下文来证明当前文本中的“pupil”是指某位学生、而不该直译为瞳孔。把注意力机制重复个几十次，模型就掌握了英语及其承载的一切广泛内容。</p><p>&nbsp;</p><p>现在，Transformer会将输出向量乘以一个矩形矩阵，再将生成的词汇长度向量填充到Softmax当中，最后把这些指数输出视为下一个token概率。这确有合理性，但大家都知道其中仍有问题。一切广受尊重的模型都承认这种输出概率在所偏差，需要在具体实现中通过采样机制来隐藏Softmax过度代表低概率的事实。</p><p>&nbsp;</p><p>引入采样确实可行。毕竟输出步骤中的Softmax将为我们提供词汇表内每个单词的梯度，在没有更好结果可用的情况下，确实可以先遵循拿来主义。</p><p>&nbsp;</p><p>但我想说的是，就没必要非把不相干的东西搅和在一起。Transformer的输出Softmax与注意力机制的内部Softmax用途完全不同，所以我们最好别跟后者硬靠，或者至少该对分母做点科学调整⛱️。</p><p>&nbsp;</p><p></p><h4>Softmax到底是什么？</h4><p></p><p>&nbsp;</p><p>那么，Softmax到底是什么？它源自统计力学，代表一种根据能级预测状态分布的方法：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43dd5aea2f3e5637e92082b59acab232.png\" /></p><p></p><p>在掌握了它之后，经济学家们意识到，如果人们的线性效用函数中的噪声项恰好遵循Gumbel分布，那么某人选择某个条目的概率就将与效用输入的指数成正比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85c5388f521b8a5418f5411f3cc13d71.png\" /></p><p></p><p>这就为Softmax在多项逻辑函数中赋予了生命力。我在读研究生时曾亲手推导过这个Hessian矩阵，想通过编码用线性固定效应将其运行在GPU上。据我所知，之前和之后都没人蠢到做这样的尝试。说了这么多，就是想告诉大家我跟Softmax函数还真就有那么点缘分。</p><p>&nbsp;</p><p>Softmax其实是一种“作弊码”，能够将实数值映射到总和为1的概率上。它在物理学中的效果相当好，在经济学上有点虚；而在机器学习领域，它已经成为离散选择当中的有效手段。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/1972df915a29a550958d7fe680b8ca7b.png\" /></p><p>这也是Softmax的核心机制：它要求在各种可行方案间做出选择，包括粒子选择能级状态、也包括消费者选择心仪的汽车。也就是说，如果一个Softmax机制根本不打算做选择，那就应该做出修改，否则Softmax一定会在处理真实数据时发生扭曲。</p><p>&nbsp;</p><p>回到大语言模型这个话题上，此类扭曲的一种实际表现，就是对非语义token（比如逗号）进行重点加权，而这就变成了那些难以压缩的离群值，致使模型很难被部署在边缘场景当中。高通AI研究院的报告发现，大语言模型中有超过97%的异常激活发生在空白和标点位置。</p><p></p><h4>具体是怎么出错的？</h4><p></p><p>&nbsp;</p><p>下面我们深入研究一下Softmax在注意力机制中如何起效，看看是哪里出了问题。仍然是这个等式：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c7bc0adac294169e395e8f4b6be6f73.png\" /></p><p></p><p>&nbsp;分解一下：在仅解码器模型中（即ChatGPT之后的所有模型），Q、K和V都来自同一输入序列。虽然它们会在期间被以不同方式投影，所以彼此间并不相同，但在每一层中，它们都始于相同的已注释（已添加）嵌入向量。</p><p>&nbsp;</p><p>现在，𝑄𝐾𝑇在寻找不同位置上token（嵌入）向量之间的相关性，这实际上会构建一个相关性（点积按1/√𝑑‾‾缩放）值的方形矩阵，其中每行和每列对应一个token位置。方形矩阵中的每一行都经过Softmax的处理，得出概率以作为V矩阵中各值向量的混合函数。混合概率后的V矩阵将被添加至输入向量中，再被传递至神经网络内做进一步处理。</p><p>&nbsp;</p><p>多头注意力会在每个层中都经历这个过程，完成多次处理。它基本上就是将嵌入向量划分成几个部分，每个头使用整个微量中的信息来注释输入向量中的一个（不重叠）部分。简单来讲，就是Head 1向Segment 1添加信息，Head 2向Segment 2添加信息，依此类推。</p><p>&nbsp;</p><p>但使用Softmax的问题在于，即使没有什么信息可以添加到输出向量当中，它也会迫使各注意力头进行注释。所以在离散选择中使用Softmax效果拔群，但在可选注释（即输入到加法中）则不太理想。其中，多头注意力又会进一步加剧问题，这是因为专用头往往比通用头更想要“通过”，反而带来了不必要的噪声。</p><p>&nbsp;</p><p>说到这里，是不是该直接让Softmax下课走人？当然不至于，它在大多数情况下都运行良好，唯一的小Bug也只是让注意力头没法“乖乖闭嘴”。所以我提出了一个很小也很直观的调整建议，自从注意力机制在2014年被发明出来，这个办法就一直在每个人的眼皮子底下。</p><p>大家准备好了吗？</p><p></p><h2>修复方案</h2><p></p><p>&nbsp;</p><p>现在，让我们有请睽违已久、经过改造的Softmax Super-Mod闪亮登场：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a3/a363e918e17944b2f361779937523bf9.png\" /></p><p></p><p>感觉也没什么了不起？是的，我只是在分母处加了个1。这就是让整个向量朝着零挪动了一点点，而这将在注意力之后的归一化过程中得到补偿。</p><p>&nbsp;</p><p>修改后的主要区别在于负极限，当x中的条目明显小于零且模型试图回避一次注释时，其表现将与原始Softmax的行为有所不同。先来看原始Softmax：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/533ada43e23552d15cf59b539ac28e36.png\" /></p><p></p><p>以下则是经过改进的新Softmax 1：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d523b13f3dbb74472b20c7bc63a45de.png\" /></p><p></p><p>&nbsp;</p><p>原始Softmax将始终发出相同的总权重；Softmax 1看似区别不大，但在负半轴中设有一处“逃生通道”。再次强调，这里的关键在于数学问题，而非数值问题。进一步提高精度并不能拯救Softmax，所有transformers均受此影响。</p><p>&nbsp;</p><p>Softmax 1还有其他一些特性。因为它的导数是正值，所以我们始终拥有非零梯度；它的和在0到1之间，因此输出不会失控。该函数还具备以下属性，即输出向量中的相对值不变：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5eb604d00133a9589147d739854cbf62.png\" /></p><p></p><p>&nbsp;</p><p>最初我本想把这个函数命名为Ghostmax，因为这里的x中有个额外的零值条目（即exp(0)=1），而V矩阵中有一个会衰减结果的零向量。但别担心，这些不是重点。</p><p></p><p>虽然Softmax 1看似平平无奇，但我有99.44%的把握相信它能解决离群反馈循环的量化问题。</p><p>我们将这种改进机制称为QuietAttention，因为它允许注意力头保持安静：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a05dcbe43e6ca6187b68b19de87a39c2.png\" /></p><p></p><p>大家很快就可以测试起来了。只要在每个输入上下文中添加一个零向量作为前缀，并确保所选择的神经网络不会添加任何偏差（包括位置编码），那么零向量应该就能原封不动通过，并在每个后续的Softmax分母中都加上一个单位。这样，也就不用再纠结什么梯度代码了。</p><p>&nbsp;</p><p>但这种方法要求重新训练模型，所以请先别急着在Raspberry Pi上尝试。希望大家能分享关于权重峰度和激活无穷范数多轮运行后的实际影响，我想把这些数字整理成表格，供arXiV上发表的新论文使用。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.evanmiller.org/attention-is-off-by-one.html\">https://www.evanmiller.org/attention-is-off-by-one.html</a>\"</p>",
    "publish_time": "2023-07-27 11:13:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "关于 To B 业务增长，我们是这么思考的｜ArchSummit 闭门会",
    "url": "https://www.infoq.cn/article/kyQ3ioi751hAmSmJ1XYR",
    "summary": "<p>在经济不佳的当前环境，ToB&nbsp;成长和创新的重要性日益突出。在这样的背景下，<a href=\"https://archsummit.infoq.cn/202307/shenzhen/\">ArchSummit架构师峰会（深圳站）</a>\"举行了一场专门针对&nbsp;ToB&nbsp;增长的技术交流会。在此活动中，我们探讨了各个发展阶的&nbsp;ToB&nbsp;企业所面临的特定增长难题，如何找寻独特的创新解决思路和策略，包括市场定位、产品定制和客户成功管理等。以下为精彩分享摘要～</p><p></p><h4>精彩分享1</h4><p></p><p>首先，对于产品启动阶段，特别是在建立信任方面，我们需要自己刷脸去找客户，或者依靠销售团队与客户直接沟通，以打出标杆并建立信心。其次，产品的品类和市场也影响着SLG（Sales-led&nbsp;Growth）和PLG（Product-led&nbsp;Growth）的属性。SLG更适用于需要多方协同的产品，而PLG则适用于小规模、单一职能的产品。第三点是，随着参与角色和决策链的增多，需要更多的沟通，而小客户则更容易通过PLG方式使用产品。最后，针对协同类产品，虽然它们可能很有用，但可能局限在少数人使用，因此PLG成分可能更高，而相较之下，对于小客户来说，不需要进行过多的沟通。</p><p></p><h4>精彩分享2</h4><p></p><p>我曾创办了一家公司，我们专注于社交媒体舆情分析，并使用自然语言处理（NLP）技术。我们的ToB业务就是通过搜索引擎，将信息整合成报告并尝试在大公司中进行销售，初期我们甚至提供免费报告，逐渐地客户认识到报告的价值，开始为之付费。经过一段时间的技术提升，我们成功将原先需要5个小时才能完成的报告，最后缩短到30分钟，最后甚至5分钟。</p><p></p><p>当时的CEO告诉我，虽然我们可以做一个咨询公司，但他更希望我们能成为一个SaaS模型的公司，后来，我们做到了，成为全球最好的舆情分析公司之一。但是，我要说的是，在SaaS模型中，你必须确保你的用户稳固，否则你的SaaS模型就不能持续下去。我们成功地稳定了市场，成为客户情报调查的市场领先者。我们的SaaS模型已经消化了所有的信息，所有的信息都存储在我们的数据库里。只需要按一个按钮，各种报告、报表就可以出来。</p><p></p><p>但是，我并不认为这样的成功故事可以在中国复现。因为中国的市场与美国不同，中国的客户期望得到全套的专业服务，而非仅仅是一个平台。这种全面性的需求远超过我们在美国的最好客户的支付水平。</p><p></p><h4>精彩分享3</h4><p></p><p>JIRA市值曾达到1000亿美元，现在已降至几百亿，但我们必须承认其影响力。有一个值得我们思考的地方：JIRA在其产品的发展中，很少有直接的竞争对手。我们观察到JIRA将许多SLG的工作推给其代理商来做，尽管看起来公司并没有参与其中，但实际上有许多代理商在做相关的工作。这也与其创始人的风格有关，他们更喜欢简单、标准化的产品，不愿意花费大量的精力在复杂的事物上</p><p></p><p>PLG和SLG都是需要做的，公司可以选择偏重于其中一个。如果我们希望实现更大的增长，可能就需要自己管理相关的成本和方式。对于大型企业，他们通常会选择通过代理商来进行销售。在项目管理这个领域，因为不容易清晰地定义价值，所以更倾向于使用代理商。</p><p></p><p>总的来说，我认为PLG在任何阶段都可以实施，但现在还没有找到完全依靠它就可以支撑收入的产品。SLG在早期启动时对自己有帮助，而且看起来无法避免做大客户，特别是在有多角色协作、多角色使用的情况下。</p><p></p><h4>精彩分享4</h4><p></p><p>我认为目前金融行业在技术和交付能力上还存在许多问题。尽管我们已经在努力奋斗了20多年，但金融客户的生产系统和核心生产系统中仍有90%以上的部分使用过时的技术，这使得推动金融行业的现代化进程变得缓慢且困难。</p><p></p><p>要解决金融行业的现代化和增长问题，除了技术的持续提升外，企业还需要重新定位市场，寻找更有效的增长模式。当然我们也面临着金融行业发展中的几个关键问题。</p><p></p><p>首先，我们渴望在产品中实现授权收费，并获得收入，但目前还没有实现这一目标。其次，我们的企业和产品定位存在问题，导致增长困难。在解决增长问题时，我们尝试着从大的市场转向小的市场，但在实施中遇到了困难。后来，我们的老板借鉴了制造业的概念，希望我们能像制造螺丝钉一样，做一项特定的事情并覆盖全球。因此，我们决定将银网云平台、银管平台和运维平台进行虚拟化，并尝试借鉴他人的成功经验。</p><p></p><p>我们的发展经验表明，核心的产品定位对于组织和技术的变革非常重要，虽然并不是唯一解决办法，但是在应对增长难题时至关重要。我们曾经在云龙初期带领团队开发了类似飞书和钉钉的产品，但后来发现难以推广。后来逐渐精简产品，保留了企业网盘功能，并将其重新包装为政务云盘。这一举措取得了成功，在广东省范围内取得了广泛应用，并有望未来一到两年覆盖整个广东省。</p><p></p><h4>精彩分享5</h4><p></p><p>定位非常重要，我们企业从初创走到现在，虽然我们还未进入成熟期，但是已经非常突出了。首先，我们公司的定位是商用清洁机器人领域第一名。我注意到机器人行业在早期，大多数产品都是展示型的。虽然它们能动，但其实并不能真正实现日常工作的需要。</p><p></p><p>我们是第一个从展示机器人中走出来的公司，我们决定将产品做好，然后，我们开始遇到了各种实际问题，比如环境复杂，机器人不能识别和处理各种不可预见的障碍，甚至像吸管和纸片这样的小东西都会给我们的机器人带来困扰。</p><p></p><p>我们公司在初创期选择了一个非常有意思的技能，即通过算法大幅提高清洁效率。我们优化过的机器人一小时能清理&nbsp;800&nbsp;至&nbsp;1200&nbsp;平方米，这远远超过人的效率。</p><p></p><p>当我们进入成长期，我们的机器人已经可以在工业场景中连续工作几天。但随着扩大规模，我们发现维护成本也等比例上升。尤其是当客户增加了机器人的使用量后，故障率上升，但经过我们的女里，这些问题都被解决了。</p><p></p><p>最后，我们发现这种成本降低并不是由于我们的机器人替代了人工，而是因为机器人的协助。我们的系统能够在物业经理巡逻时，发现需要清洁的房间并报告，然后物业经理就可以继续他们的工作，不需要再跟进。这就导致了一个惊人的结果，原来需要三个物业经理的地方，现在只需要一个，大大节省了成本。</p><p></p><p>我们也思考过我们的机器人系统是否应以SaaS模式出售。当我们的机器人销售出去后，我们是否应该将服务以SaaS模式随机器人一并销售。最后，我们决定不采用SaaS模式。</p><p></p><p>我们的视角是，既然硬件已经销售出去，我们应该在硬件内增加增值服务，客户会很容易接受并愿意支付。买固定资产的决策非常简单。我评估硬件的价值，再考虑包含的增值服务，我们可以接受某种程度的议价决策。采购和&nbsp;IT&nbsp;部门可以快速做出决策，因为他们知道设备可以立即使用。</p><p></p><p>但是，如果我们让业务部门评估SaaS的决策，他们会要求技术部门一起评估，需要考虑许多复杂的因素。我们的机器人销售出去时，我们提供增值服务的方式是，你可以支付额外的费用来开通这个服务。我们不会单独售卖SaaS服务。</p><p></p><p>总的来说，定位非常重要。如果你在一个领域是第一，就有很大的说服力。我们不想与整个行业竞争，我们只是抱着我们的第一名产品出发，然后直接销售。因此，我们认为在初创期和成长期，我们需要找到一个定位。当产品成熟，并普遍被使用时，如果客户不使用我们的产品，反而是他们的问题。</p><p></p><h4>精彩分享6</h4><p></p><p>就部署还是&nbsp;SaaS&nbsp;会增长得更快而言，我是这样考虑的。在讨论企业服务的软件时，我们会把它们分成三类：一是与核心产品有关的，即与核心生产系统有关的；二是辅助生产系统的；三是平常工作、生活以及工作流程协同的工具。对于这三类，越接近外圈的，越类似于传统的财务系统和CIM系统。例如，我们的产品就在这个最外面的圈子。我们不直接帮助客户创造收益，而是间接地提供帮助。而像虚拟化这种技术，就不同了，它至少处于第一或第二个圈子，与生产系统有直接关系。但是总体来说，无论在中国还是全球，越大的企业就会有越多的合规、安全和数据等问题需要处理。我觉得大部分的这个软件产品，特别是像现在我觉得就售部署的场景要求都会很高，特别是大做中大客，在中国做小客基本上做不出来了。</p><p></p><h4>精彩分享7</h4><p></p><p>关于处理用户需求成本太高的问题，我们采取的价格区分策略，这样的处理方式是基于实际情况的需求。比如，国内的一些公司等向我们咨询是否支持某项服务时，我们坦然回答不支持。而当海外分销商咨询同样的问题，我们不能直接说不行，因为他们有大量的潜在客户，他们是全球医药行业的头部。</p><p></p><p>这里逻辑是，一旦我们满足了他们的需求，其他的医疗机构也会效仿，因此我们必须满足他们。我们会采取所有必要的措施来支持他们。而在中国，要实现如此高的要求，特别是在大中型客户中，对小客户的要求基本无法满足。然而，我们为提供服务的理由很简单，因为我们能以三倍的价格出售我们的产品。</p><p></p><p></p><h4>精彩分享8</h4><p></p><p>现在我们的大环境确实面临挑战，我认为整合是一个很好的方式，无论是在行业竞争中还是产品推广中。举个例子，我在前端实施中使用了代理商的推广策略。虽然有更便宜的选项，但我选择了代理商的产品，因为他们提供了更多的资源和空间，能够将产品推广得更广更远。这类似于政务网盘的概念，通过合作伙伴的推广，整合各方面的资源，让产品得到倍数的放大效应。在我们的行业中，我认为这是一个关键的点，通过整合资源来实现产品的成功推广。</p><p></p><h4>精彩分享9</h4><p></p><p>在我的观点中，如果客户对我们所提供的服务的核心竞争力不感兴趣，那说明我们的服务对他们来说并不是必需的。即使我们在产品和服务方面做得再好，在客户看来，这也是我们应有的水准。</p><p></p><p>在我们的内部，我们都会有营造一个故事。我们会告诉产品经理们这个故事，让用户了解我们的理念和方法。这也是我们项目成功的关键。我们要能根据客户的需求，给他们提供合适的产品和服务。</p><p></p><p>另外，我们做产品时，经常会被客户的需求所驱动。尽管有时候这些需求看似不合理，但我们还是会尽力满足他们。在这个过程中，我们也面临着一些问题。例如，我们需要判断一个项目的性质，我们需要为客户提供哪种服务。</p><p></p><p>最后，搞定大客户相当重要，在我们的经验中，我们发现在金融行业有很大的增长。我们有大约700个目标客户，目前已经有一百多个。我们的主要问题是如何解决客户的选择性问题。在过去，他们可能会选择华为、华商，但现在我们也有我们的优势，我们有我们的产品定位，我们用全公司之力把头部银行等一些大型企业纳入我们的服务范围。一旦客户对我们没有选择性的顾虑，剩下的就是销售操作、服务以及产品的可靠性等问题。这些都不是主要问题。</p><p></p><h4>精彩分享10</h4><p></p><p>在我的经验中，了解并满足客户的需求至关重要。一次，我向一位客户解释了我们产品的各项优势，他提出了一些关于安全稳定性的问题。我很奇怪，这些都是我们在过去一年解决过的问题，然后，我向他展示了我们最新的解决方案。但是后来发现，他们并不买账。最后我才了解到要真正满足他的需求，我了解到他在向上级汇报时的需求和困扰。</p><p></p><p>客户跟我关系比较好，坦白告诉我，恰恰是我们我们的产品稳定性强，五六年不需要更换，这让他在第二年没有什么可以向老板汇报的。于是，给了他新的解决方案。从这个案例中，我明白了要成功推销&nbsp;To&nbsp;B&nbsp;产品，我需要站在客户的立场上，解决他们的关键问题。这些问题可能并不会直接向销售人员或解决方案提供者表达出来，但是一旦找到并解决了这些问题，客户就很容易被搞定。</p><p></p><h4>精彩分享11</h4><p></p><p>在我看来，我认为想要在国产领域的立足，除了研发之外，确保我们的产品在目录上有位置至关重要，没有目录位置，没人会来帮我们。这是门槛，而且是硬门槛。举个例子，如果我们有300人研发预算，如果这还不够，我认为我们可以削减50个人，然后把这些资源投入到进入目录，这是我们必须要做的。一旦进入目录，我们可以从一些边缘的创新项目开始，比如销售方面的改进，使我们的应用一体化交付，这是好的，没人会否认在目录里的合规性。</p><p></p><h4>精彩分享12</h4><p></p><p>To&nbsp;B发展很困难，有一个关键原因就是定位，这其中就包含了&nbsp;SLG&nbsp;还是&nbsp;PLG，其实在这个AI行当里面还真需要&nbsp;SLG，并不是个&nbsp;PLG&nbsp;可以搞定的。CEO、CTO都必须亲自走出去，与客户进行深入交流，推进产品。</p><p></p><p>我们也曾尝试了多个方向，但真正能够稳定带来现金收入的成功案例并不多。当然也有例外，也有客户主动找过来的。例如，某市公安局就对我们的技术有强烈需求，他们需要能够在海量人群中快速找到可疑人员的技术。我们在这个领域取得突破后，客户对我们的产品非常满意，这使得我们的业绩大幅增长。这就是一个PLG案例。</p><p></p><h4>活动推荐：</h4><p></p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">FCon全球金融科技大会（2023·上海站）</a>\"是极客邦科技旗下 InfoQ 中国团队推出的面向金融行业高端技术管理者、技术专家的会议，50%参会者拥有 8 年及以上工作经验。</p><p></p><p>FCon 聚焦当前金融行业遇到的问题，围绕金融企业在数字化转型过程中的痛点，例如数据治理，智能化、数字化风控，数字化投研，数字化营销，IT 技术能力等方向，邀请国内外金融企业，来分享人工智能、区块链、大模型、大数据、数字货币等新一代信息技术实践话题，帮助听众解决技术和业务上的问题，提升技术能力。欢迎大家报名参会，<a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">详细信息可点击这里查看</a>\"</p>",
    "publish_time": "2023-07-27 11:17:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于鲲鹏+昇腾，华鲲振宇重磅发布算力创新产品及解决方案",
    "url": "https://www.infoq.cn/article/Nk9DyErUiAA5uH80qUU0",
    "summary": "<p></p><p>2023年7月26日，<a href=\"https://www.infoq.cn/article/iCuurCMJCvofpfylbGpE\">华鲲振宇</a>\"在北京举办产品发布会，以“中华鲲鹏振兴寰宇，昇腾万里智领未来”为主题，分享基于“鲲鹏+昇腾”的算力创新，发布全新一代算力基础设施与解决方案。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/14/dd/14e0b4653e4c39665318e1e3ea64f4dd.jpg\" /></p><p></p><p>此次发布会汇聚产业精英、客户和生态伙伴，共同探讨满足数字时代多样性算力需求的未来发展方向。北京市朝阳区人民政府党组成员、副区长舒毕磊、华为计算产品线副总裁田华、华鲲振宇总裁万诤等领导出席大会并致辞。</p><p></p><h4>深入场景发布四大“天”系产品 全栈创新敢于亮剑</h4><p></p><p></p><p>擎天之力铸坚实算力底座——“天擎”系列高规格通用计算平台</p><p></p><p>随着应用与场景驱动需求激增，对<a href=\"https://xie.infoq.cn/article/372faa42c86908abae0360a08\">算力基础设施</a>\"提出了更多差异化的需求，本次大会，华鲲振宇正式揭开基于鲲鹏创新架构算力产品的神秘面纱，华鲲振宇研发总经理赵彦钧携华为鲲鹏计算业务副总裁马银川正式发布“天擎”系列高规格通用计算产品，最大支持16块NVMe SSD，创新蝶形IO模组设计，最多可支持11个PCIE 标准插槽，规格提升了37%；率先支持9张 PCIe X8的 NPU/GPU加速卡，或两张全高全长双宽的训练卡，整机AI计算性能提升15%以上，各项参数直指鲲鹏最高规格，为千行百业提供高密性能的算力应用平台，已规模服务于运营商、互联网、金融等行业，满足客户在不同部署场景、不同算力、不同存力需求下的计算任务，让每一次计算都能心有所想、算有所成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52d019f2cd0760e857efad515f809a84.png\" /></p><p></p><p>深耕智算助力大模型普惠AI——“天智”系列高密AI算力平台</p><p></p><p>伴随大模型带来的生成式AI突破，人工智能技术正在跨越重要的历史拐点，华鲲振宇也早已有了筹谋，基于昇腾AI计算，在多个典型应用场景进行了深入创新和升级，积极拥抱人工智能，在大模型训练场景下，推出基于昇腾的“天智”系列高密AI算力服务器——新一代训推一体服务器AT800 A2，底层技术的突破为大模型的训练推理提供极致的算力输出，助推国内大模型应用广泛落地，进一步普惠AI。基于天智训练服务器，华鲲振宇已为“蓉城 • 夔（kuí）牛”短临预测模型提供稳定“算力底座”，以成都智算中心300P算力为基础，实现精细化的气象预报服务。备受关注的第31届世界大学生夏季运动会（成都大运会）即将开赛，\"夔牛\"可作为赛场的一匹“黑马”，助力大运会期间的天气预测，把前沿的“黑科技”运用到大运会“智慧气象”服务中，让AI大模型应用于具体场景。</p><p></p><p>以恒致远令融合算力触手可及——“天恒”超融合移动平台解决方案</p><p></p><p>在<a href=\"https://xie.infoq.cn/article/376502f80af0b1d2c672a8e96\">边缘计算</a>\"场景中，客户对计算设备部署的便利性、可移动性以及异构算力融合方面提出了更高的要求。华鲲振宇产品总经理李锐携手华为昇腾计算业务副总裁刘鑫发布业界首创鲲鹏+昇腾“天恒”超融合移动平台，应对应急救援、海上勘测、地质勘探和航拍合成等各种极端恶劣的环境场景， 具备高可靠性和稳定性，轻松完成信号处理、全景视频融合、IoT数据采集与处理、目标检测等功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fbb31cb233ae282c4203a34f6bfad7c.png\" /></p><p></p><p>技术创新让数据中心极致节能——“天极”系列液冷解决方案</p><p></p><p>在国家双碳战略背景下，数据中心算力规模目前持续高速增长，能耗随之急剧扩张，以应用液冷技术为代表的绿色数据中心，成为新基建、“东数西算”的算力底座，如何降低数据中心的能耗成为整个行业技术创新的焦点。华鲲振宇超前洞察，携手华为集群计算业务副总裁王振华发布双液冷解决方案——天极HC8000冷板式液冷和天极HC6000浸没式液冷，挑战PUE小于1.1，HC6000更是基于鲲鹏+昇腾全球首发的浸没式液冷服务器，采用国产原生冷却液，具有极致节能、极稳运行、轻松运维等极致优势，为关键业务场景提供绿色低碳的强劲算力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/519c99276fb37bf7582bdb30fd01b21f.png\" /></p><p></p><p></p><h4>携手伙伴共襄成长 合力向上共建繁荣</h4><p></p><p></p><p>赋予算力时代价值的，并不单纯是算力本身，而是产业上下游的合作伙伴，在本次产品发布会上，华为中国区运营商业务计算解决方案总经理张立鹏、华鲲振宇副总裁宋璇携手20+伙伴共同发布基于华鲲振宇产品的解决方案，持续深化合作，发挥各自优势，深入场景，充分释放出“鲲鹏+昇腾”澎湃算力，给客户持续提供更具价值的解决方案与服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d402fc25cec335efd6b75782a2d5e32c.png\" /></p><p></p><p>此外，华为中国区政企部件销售部总经理李国涛和华鲲振宇高级副总裁于巍波为10+产品合作伙伴正式授牌，感谢一直以来相伴同行。面对风高浪急的国际环境，树木相依偎而生长，星辰因辉映而璀璨，华鲲振宇秉承合作共赢的理念，与关键部件供应伙伴签署长期战略合作协议，与产业伙伴携手共建多样性计算的安全、可靠、连续供应链，最大化保障客户权益，共建繁荣生态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf923c3e65e5ee907932a3d0f46782b7.png\" /></p><p></p><h4>结语</h4><p></p><p></p><p>在计算产业蓬勃发展，技术创新方兴未艾的浪潮中，华鲲振宇此次发布会的成功举办，将为数字经济的快速发展提供强有力支持，并满足市场对差异化、高效能、低碳绿色计算产品的需求。作为鲲鹏+昇腾计算产业的重要参与者，华鲲振宇将坚持技术创新和产品研发，不断推出更优质、更高效能的计算产品和解决方案，为客户和伙伴创造更大的价值，给世界多一种算力选择。</p>",
    "publish_time": "2023-07-27 09:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大数据在大模型趋势下的“新姿态”：大模型与企业数据充分协同",
    "url": "https://www.infoq.cn/article/PV95VcMEDcilhOrNMpPl",
    "summary": "<p></p><p>随着大数据、人工智能和云计算等技术的不断发展，大模型成为了企业数据体系中不可或缺的一部分。大模型趋势下，企业数据体系面临着新的挑战和机遇。比如，大模型的训练需要大量的数据，而数据的收集、清洗和处理需要耗费大量时间和人力。同时，大模型的训练需要高性能计算资源，这需要企业进行大量的投资，而且大模型的训练和推理需要强大的算法和计算能力，这进一步增加了技术难度和成本。</p><p></p><p>然而，大模型趋势也为企业数据体系带来了新的机遇。企业需要加强数据治理、数据存储、数据安全、数据整合、数据分析和挖掘以及业务应用等方面的能力，以应对大模型趋势带来的挑战和机遇，实现数字化转型。为此，在数据治理领域有多年实践经验的何昌华博士在刚刚结束的 ArchSummit 全球架构师峰会2023（深圳站）中就《大模型趋势下的企业数据体系思考》展开了分享，他从“大模型的火爆引发数据处理进入新次元”等行业背景、“大模型时代的数据处理新需求及传统数据架构的桎梏”、“大模型时代的企业数据处理发展趋势”、“企业数据架构演变的前瞻展望”四个方面展开了分享，输出了众多精彩观点。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/44/44be3aed51d41c905c5d486232575b45.jpeg\" /></p><p></p><p>何昌华博士，斯坦福大学 PhD，数巅科技创始人、CEO。之前曾经在蚂蚁集团担任计算智能部门负责人，计算存储首席架构师；2017 年之前在硅谷任职于 Google，Airbnb 等互联网公司。在过往职业生涯中，何昌华主导开发过实时智能决策系统、金融级的分布式图数据库、新一代分布式计算引擎、下一代逻辑数仓、新一代搜索引擎架构等。</p><p>何博士从最开始用 MATLAB 写一些很简单的神经网络，到在 Google 的时候接触深度学习并在业务上有越来越多的应用，到后来蚂蚁时期做大规模搜索推荐的一些模型以及图学习的一些工作，经历了神经网络发展的完整过程。</p><p></p><p></p><h2>一、大模型在企业落地过程中，对“数据体系”有三大需求</h2><p></p><p></p><p>何昌华创建的数巅科技愿景是“让数据智能像水电一样简单”，希望数据智能可以让大家真正在企业内部采用，这本质上是非常高门槛的一件事情。过去半年大模型的飞速发展给行业非常大的震撼，数巅科技近期在大模型上也做了一些事情，探索怎么能让大模型跟企业数据充分的协同起来，这样能够真正释放大模型的潜力，能够释放大数据的价值。</p><p></p><p>很多人说，是不是大模型无穷的发展下去有足够大的规模，企业就可以把所有的数据灌进去，目前主流的数据计算存储体系以后就不再需要了。数巅科技却不是这样认为的——大模型不能取代数据计算存储体系的作用，其实大模型是需要跟企业的数据做深度协同才能够真正地做出一个好的决策。因为这些数据（尤其是结构化数据）的规模远超大模型，并且数据驱动的决策里面往往需要精确的计算。</p><p></p><p>为了让大家更明了地了解企业需求，在演讲中，何博士为大家介绍了大模型在企业内部落地的过程，引用了一个知名投资机构 A16Z 的图示，“事实上现在业界很多的框架也都是这种模式。大家可以看到只有在右边一个小的框里面是大模型，整条链路上其实其它的内容占了非常大的部分。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a3/a3057f390f0a203cbeadfdb867df4136.png\" /></p><p>图：大模型应用一般流程</p><p></p><p>流程的第一部分就是数据的加载和接入。我们都知道，大模型的构建肯定是要用到数据的，大模型可以接到大量各种各样的数据，但是对企业数据体系提出了一个非常重要的需求，就是希望数据是一个统一的高质量资产，可能是多模态的数据、有多种格式，也可能是需要有清晰的血缘定义，要求非常严格。</p><p>第二部分是编排框架。其实在大模型刚出来的时候大家还没有想到这条路，但是后来大家看到 OpenAI 发布了他们的 PLUGINS，它可以调用你的函数。通过大模型给出一些建议以后，调用 PLUGINS Function 真正实现它的功能，在这里最重要的是究竟有多少数据分析、模型甚至是深度的归因分析这样的自动化工具。总之，所有的这一切都是走向企业智能化应用的必经之路。</p><p></p><p>此外，我们还可以看到，大模型跟数据的交互非常频繁。何博士举了一个例子：“当企业要做一个决策，希望通过一个清晰的报表看一下过去三个月的交易额的变化来决定业务未来如何调整，但是真正的大模型可能比人做出更好的决策，它可能看的是 10 个指标，然后做对比进行反复的迭代，最后找到最优指标。在这种情况下，大家可以看到它的计算模式对计算量的需求是有数量级的增长的。”</p><p></p><p>由这个流程我们可以判断，统一的数据资产、自动化工具以及高效的计算能力，这三个需求是大模型在企业内部会碰到的最核心的三个需求。</p><p></p><p>面对这三个核心需求，现有的数据体系目前在这方面遇到很多问题，包括数据中台在内的各种方案都不好解决。计算机科学里面有一句话：当你发现一个问题很难解决的时候，你可以加入一层 indirection，所以数巅科技决定用数据虚拟化的技术来解决这个问题。其实数据虚拟化并不是一个新概念，但数巅科技的做法相较于目前行业还是有比较大的区别和创新，我们可以看下数巅科技的“智能数据虚拟化技术”的架构图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c59005c48b436382637a2a0591498320.png\" /></p><p>图：智能数据虚拟化技术</p><p></p><p>按照从下到上的顺序，我们先看第一层是现在主流的一些数据体系，有各种各样的引擎在系统内计算存储数据。第二层数据连接层，相当于是定义了一个虚拟的能够连接到外部物理层的数据连接，在这一层虚拟化之后，事实上“我的数据”就是数据本身，它跟数据源就被解耦开了，虽然这种情况现在有很多引擎也可以做到，比如 Presto，data federation 工具等，但数巅科技认为在这一层之上还应该有更高层次的数据虚拟化。</p><p></p><p>譬如说企业在上面建立的一些数据虚拟表，这个通常是数据建模的过程，用户会直接消费这些数据表。同时在更上层，企业可以根据业务模型来建一些更上层的虚拟表，一层一层这样起来，大家可以看到整个建设过程其实是完全一个数据的语义，它不涉及到任何系统的改动或者任何中间表的加工。</p><p></p><p>这个概念及架构看似非常简单，但是整套体系最核心的问题就是计算能力。大家知道大数据整个历史之所以发展起来，就是因为数据量太大了，需要有强大的计算能力，需要有中间结果，需要不停地一层一层加工等等。在数巅科技的虚拟化引擎之中，所有这些技术上的实现细节都是通过智能化的手段来完成，譬如自动的生成物化视图，自动 check 数据的血源，非常高效。中间包括虚拟表组织、自动的物化血源以及自动优化这一整套技术功能，我们把它叫做一个数据虚拟化的引擎。因为一般企业已经有数据仓库或者数据湖，或者至少有一些数据源，所以基于企业现有的数据架构，数据虚拟化引擎的位置是在数据源与数据消费者之间。它会给消费者提供一个虚拟视图的数据去消费，消费者只需要关注数据究竟是什么业务语义，而不需要关注数据的“具体位置”、“用什么引擎”、“该如何取用”。与此同时，用户也包含一些与深度学习及大模型紧密连接的操作行为，当大模型进行数据调用的时候，通过虚拟化引擎的接口调用数据会更加的高效。</p><p></p><p></p><h2>二、“数据虚拟化引擎”将切实解决企业业务需求</h2><p></p><p></p><p>企业需求已经明确了，那如何解决这些需求呢？数巅科技给出了解决方案——数据虚拟化技术。面对企业的三个业务需求，数巅科技也分别从三个维度上提出自己的解决方案。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f95e886c37cc71d6ce27036e04c0ea32.png\" /></p><p>图：数据虚拟化技术解决三大需求</p><p></p><p>第一，统一数据资产。用虚拟表的语义很容易接入一个数据资产，定义一个虚拟表，map 外部数据源之后就形成了一份虚拟数据，其它工作都由虚拟化引擎内部执行完成。这其中，虚拟化引擎内置了对外部数据的格式转化，加速以及预处理的过程。</p><p></p><p>当用户在使用虚拟表数据的时候，如果需要新增表，或者衍生出新的数据资产，可以非常快速地新增虚拟表。虚拟化引擎由于是自动化的分析与执行，因此所有的数据血源都是可被追踪的，一旦有完整或部分的 SQL 来访问相同数据的时候，虚拟化引擎就会把它合并掉，资产被定义的 SQL 语义所唯一决定，由此不会存在有二义性的资产。</p><p></p><p>第二，实现数据驱动的自动化工具。在 OpenAI 的大模型里面，可以对接很多 PLUGINS，包括后面发布的支持函数的功能，它可以去帮你订票，可以去做跟第三方应用的对接，这些都属于一个大的生态。当我们聚焦到在企业内部做决策的场景时，自动化工具往往需要的是数据驱动，虚拟化引擎能够给它提供强大的能力。</p><p></p><p>举一个智能运营的场景案例，比如说用户要找到新信用卡发行的运营客群，大模型根据自身知识告诉你“日均交易金额按一定的阶段性区间来分成”这个信号可能非常有用，它就会到数据系统里面去找。在这个过程中，我们会发现，在特征宽表里面没有这样一个特征，但是有另外一个特征叫“日均交易金额”，按照常规流程我们就需要请数据工程师帮忙加工出“日均交易金额的分成”这个新特征，加工完了他还得合并到一个表，再把这个表推到线上服务系统，由我的模型这个工具来访问它才能够 work。</p><p></p><p>但在虚拟化引擎体系里面这个动作可以自动化完成，快速得到新特征，做出更好的决策。整个数据可以自动根据大模型的需求灵活扩充和加工，实现数据驱动自动化的关键一步。如果整条链路自动化，不需要人工加工新特征，才能真正实现大模型效果。</p><p></p><p>第三，高效的计算能力。能不能满足企业需求，最核心的要义就是“足够高效的计算”，包括预计算。通过虚拟表来访问底下的数据，能不能够做到与以前人工加工同样甚至更好的效果。关于这一点，数巅科技有两个核心探索：</p><p></p><p>智能加速，分析 SQL 能够做物化视图的优化</p><p></p><p>物化视图其实在数据库里是一个老课题，但因为数据库涉及到事务性，这一块在数据库里没有体现革命性的变化，然而在大数据分析这个领域，它其实是可以具备革命性的。物化视图技术本质上解决了人工去做 ETL 的过程。</p><p></p><p>数巅科技目前正在分阶段解决这个问题，目前是数据清洗通过人工加工、 手工创建 DWD，到之后自动生成物化视图这种混合模式，但随着阶段的推进，之后做到完全的自动化效果会非常的好。对用户而言，直接消费的主体就是一个虚拟的宽表，不需要感知底层是实时数据还是批量数据，存在哪个引擎，用什么样的存储来处理，所以对用户来说他的体感非常好。</p><p></p><p>真正好用、高效的“计算存储底盘”</p><p></p><p>其实像 data fabric 等虚拟化的概念都是停留在了第一步，没有能够再往前继续走一步，而数巅科技觉得它的体系里面缺少了非常重要的一环——真正的存储。存储在这里是一个高效缓存，有 KV、图等多种格式。另外存储性能也是很关键的能力，但是我们要把存储用 SSD 做到跟内存几乎一样的性能。</p><p></p><p>此外，何博士表示，除了以上三个方面，计算引擎的优化也非常重要——数巅科技认为引擎最核心的能力应该就是 join，他们会把外部的数据做一些预计算，形成中间表放到高效存储，同时会有索引在高效存储里面建立起来。另外如果需要，也会直接去读取外部的数据。在这种情况下，引擎的能力就是企业能够快速地拿到外部数据以后把它 join 成为最后的结果，这个也是数巅科技目前最强大的一个能力。比如有些引擎擅长于做批处理、增量处理，便可以完全复用他们的能力去调用它。</p><p></p><p>据相关数据统计，经过以上这个完整的“技术栈的智能优化”闭环，与主流产品相比，数据查询的性能可以提升 10 倍以上。</p><p></p><p></p><h2>三、大模型与企业数据的“终极未来”：充分协同</h2><p></p><p></p><p>过去大数据和 AI 一直面临着一个问题，那就是虽然具有强大的能力，而且也很容易演示，但是在各种场景下能否被广泛地使用一直是一个挑战。何博士认为，这是判断大语言模型是否是这次工业革命的颠覆者的一个关键判断条件，只有当大语言模型能够在各种场景下被广泛应用，才能真正展现出其潜力和价值。</p><p></p><p>这就意味着，在企业部署大模型之后，构建可以自我演进的大模型框架是一个关键课题，自我迭代的大模型应用框架可以帮助企业根据自己的数据体系来构建大模型应用，让企业数据与大模型充分协同后发挥出最大价值。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1dd28a44244da31ed31808838b46eb98.png\" /></p><p>图：自我迭代的企业大模型应用框架</p><p></p><p>在大模型与企业数据充分协同这一方面，数巅科技也做了许多探索——自我迭代的企业大模型应用框架。该框架可以接入各种大模型，将用户问题分解为一系列任务并分配执行。执行任务需要使用数据，虚拟化引擎统一管理数据资产，实现了高效计算，执行过程中，进行评估并反馈结果给用户，反馈包括人工标注微调的样本，虚拟化引擎管理数据传回给大模型进行调试。在这个体系中，大模型起到逻辑推理的作用，这是其更稀缺的能力之一。另外值得一提的是，在面对“大模型充当的知识库功能”这一企业诉求中，企业可以完全利用这套数据体系来组织，对比其他方式，它更实时，而且可以说，只要能够跟大模型的逻辑能力交互起来，这个体系就能为企业提供更好的服务。</p><p></p><p>然而，观察当下所有的大模型应用场景，很多人都会疑虑“大模型会不会吃掉一切”，何博士给出的观点非常值得大家思考，“关于大模型的未来，尤其是在企业内部，大模型跟企业数据同时都是需要的，我们要做的事情是让这两者能够充分的协同起来，能够真正在广泛的业务场景下帮助实现智能决策。”</p><p></p><p>目前数巅科技的愿景就是完美实现“大模型与企业数据的充分协同”，正如何博士在演讲后接受 InfoQ 专访时所说的那样，“我希望企业可以通过我们的产品能够充分地把数据能够管好、用好，能够跟大模型深度地协同起来，为企业提供智能的业务决策能力。”</p><p></p><p>以下附何博士在演讲后接受 InfoQ 专访的视频实录，大家可以一起再去深度了解下何博士对于“大模型与企业数据治理领域发展”的观察：</p><p></p><p></p><p></p>",
    "publish_time": "2023-07-27 14:06:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "⻜桨⼤模型推理部署⾼性能优化",
    "url": "https://www.infoq.cn/article/kaWtcvbEK9FRQbxqUdI6",
    "summary": "<p>在《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课第六讲中，百度飞桨架构师邓凯鹏分享了飞桨大模型分布式训练技术。</p>\n<p>首先他对大模型推理做了一个简介，接着具体分析了其需求与难点，之后他详解了大语言模型推理部署高性能优化，即是分析其应用场景。在分析完大模型推理的需求、难点、应用场景后，他又讲解了多模态大模型高性能优化，并在最后进行了总结与展望。</p>\n<p>邓凯鹏先生在此次公开课中有重点并循序渐进地对于大模型推理做了认真细致的讲解，希望本期视频可以为各位答疑解惑、拓展知识。</p>",
    "publish_time": "2023-07-27 15:09:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "企业资深独立 Rust 咨询师张汉东，确认担任 QCon 北京构建未来软件的编程语言专题出品人",
    "url": "https://www.infoq.cn/article/GpCabxjXueW9zZr92d8S",
    "summary": "<p>9 月 3 日 - 5 日，在<a href=\"https://qcon.infoq.cn/202309/beijing/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=0727\"> QCon 全球软件开发大会（北京站）</a>\"，企业资深独立 Rust 咨询师张汉东将担任「构建未来软件的编程语言」的专题出品人。在此次专题中，你将了解到具有前瞻性的四个问题：Python 能否承担下一个时代的发展重任；Mojo 语言的横空出世，对 AI 研发生态产生什么影响；Rust 语言会如何应对软硬协同时代的新挑战；WebAssembly 会发挥什么重要作用。</p><p></p><p>张汉东是十六年软件行业从业者，现为企业资深独立 Rust 咨询师，为企业内部落地 Rust 提供解决方案。Rust 中文社区布道者，RustChinaConf 发起人之一，著有《Rust 编程之道》。</p><p></p><p>相信张汉东的到来，可以帮助提升此专题的质量，让你充分了解到未来软件的几种编程语言，以及这些编程语言会产生的影响、面临的挑战和发挥的作用，指明编程语言未来发展趋势。</p><p></p><p>除上述专题外，QCon 北京还将围绕<a href=\"https://qcon.infoq.cn/202309/beijing/track/1553?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">异构计算</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1554?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">向量数据库</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1556?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">FinOps&nbsp;落地</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1558?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">业务安全技术</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1557?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;BI&nbsp;到&nbsp;BI+AI，新计算范式下的大数据平台</a>\"、<a href=\"https://qcon.infoq.cn/202309/beijing/track/1559?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9\">从&nbsp;MLOps&nbsp;到&nbsp;LLMOps</a>\" 等进行分享。</p><p></p><p>近 100 名讲师、近 30 个精彩专题、8 种交流活动，QCon 北京 2023，相约 9 月！ 现在购票，享 9 折特惠，立省 ¥880！咨询购票请联系 18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33cbbbf20baa8b2a18db4f0681f159aa.jpeg\" /></p><p></p>",
    "publish_time": "2023-07-27 15:10:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Web开放性或遭重大打击！谷歌四名工程师推出WEI方案，可让广告拦截变成历史",
    "url": "https://www.infoq.cn/article/MZUDtZ5OuqwGqeGpMIRd",
    "summary": "<p>最近，谷歌提出了一项名为“Web环境完整性（WEI）”的提案，由其四名工程师撰写。这是一种确定浏览器是否可信的方法，有助于防范欺诈和其他不良行为。</p><p>&nbsp;</p><p>谷歌在 Chrome 中进行原型设计，目前看起来它已经被推送到了 Chromium 中。另外，苹果去年已经开发和部署了一个极其相似的系统，现已集成到 MacOS 13、iOS 16 和 Safari 中。</p><p>&nbsp;</p><p>但互联网社区中也有人担心，这正是大家最不愿看到的“网络的终结”：“这是对<a href=\"https://www.infoq.cn/article/VUqNUlYvuv6fgpjbjCk0\">开放Web的重大打击之一</a>\"，可能为少数科技巨头主导的数字体系铺平了道路。”“这几乎让广告拦截变成了不可能的事情”......</p><p>&nbsp;</p><p></p><h2>提案背景</h2><p></p><p>&nbsp;</p><p>这项提案于今年4月以代码形式出现，并在5月份正式公布，但并未引起技术社区的过多关注。上周五被定为在拟规范草案并更新后，才引起了关注Chromium开源项目Blink渲染引擎的人们的在意。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.cn/article/9hwuj1vixx*UWAMzFW0K\">谷歌工程师们</a>\"将WEI描述为浏览器客户端通过第三方（例如Google Play）同服务器建立信任的一种方式，由该第三方提供可证明客户端环境完整性的令牌。</p><p>&nbsp;</p><p>简单来说，WEI为浏览器提供了一种方法，能够证明其正按网站运营商的预期工作、没有受到操纵。举例来说，如果大家拥有一个Web页面游戏网站并想要确定玩家没有作弊，那就可以使用WEI来确定接入的客户端纯粹、合法，且没有运行任何作弊代码。</p><p>&nbsp;</p><p>对于那些不希望自动机器人跑来发帖或点赞的网站，WEI也能帮上大忙——所有参与活动必须通过可接受且未更改的浏览器来完成。对于想要通过浏览器发布内容和广告的经营者来说，肯定不希望自己的努力最后都被发给了机器人。</p><p>&nbsp;</p><p>这也标志着整个Web将走向新的时代：网站只接受经过授权的、正式发布的浏览器。</p><p>&nbsp;</p><p>而由于Chromium不仅是谷歌Chrome浏览器的基础，更是微软Edge、Brave等众多其他浏览器的基础，因此一旦WEI得到部署和采用，很可能对网络产生广泛的影响。</p><p>&nbsp;</p><p>规范草案解释道，“Web环境完整性API允许用户代理向证明者申请裁量，该裁量可用于验证Web环境的完整性。这些裁量将通过管线传送给依赖方，并在期间验证其真实性。Web环境完整性机制最适合检测各类欺诈性Web环境。”</p><p>&nbsp;</p><p>从解释“Web环境”的待办事项相关链接来看，这项提案目前仍未公布太多细节。</p><p>&nbsp;</p><p>该API的既定目标是解决网络上长期存在的各种问题：社交媒体操纵与造假；机器人检测；在应用程序中滥用WebView；批量网络劫持与账户创建；在网络游戏中作弊；入侵设备；以及密码猜测尝试。</p><p>&nbsp;</p><p>然而，这里的“滥用”并没有明确定义。因此，尽管规范作者提到目标是“提供一种对抗性强、且长期可持续的反滥用解决方案”时，但其实很难确定其要打击哪些对象。</p><p>&nbsp;</p><p></p><h2>概念并不新鲜</h2><p></p><p>&nbsp;</p><p>在网络交互当中建立信任的思路并不新鲜。Android和iOS生态系统都已存在用于验证原生应用的类似API。行业内也出现了类似的提案——例如PrivacyPass、Trust Token API以及UserConfidenceScore等。WEI的前身最初于2022年4月被提出，当时就引发了关于设计后果的担忧和质疑。</p><p>&nbsp;</p><p>总体来讲，如果人们不信任建立该技术的实体，那么为Web客户端建立信任机制就会变得更加困难。</p><p>&nbsp;</p><p>WEI于4月底在W3C反欺诈社区小组中接受了审议，并作为浏览器功能开发的正常迭代过程被部分发布到网络之上。</p><p>&nbsp;</p><p>尽管该规范尚处于设计阶段，但上周很快有批评意见出现，开始占领WEI的GitHub repo甚至针对提案作者进行谩骂。谷歌开发者的回应是，将评论权调整为仅向此前为该repo做过贡献的用户开放，同时发布了一份行为准则文件、提醒人们保持文明。</p><p>&nbsp;</p><p>反馈意见中的担忧包括：可能违反欧盟数据规则；所有网络交互都须接受认证——但谷歌明确否认了这一点；新浏览器难以继续发展；人们对谷歌普遍不信任；担心Web领域出现DRM（数字版权管理）；拦截能力可能存在限制等等。</p><p>&nbsp;</p><p>有15年全栈开发经验的Alex Ivanovs就直言谷歌此举就是想让广告拦截变为“不可能”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/37/37b67467c110ce6836e934555eb05e48.jpeg\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>而更多人认为这不仅仅是广告拦截的问题，这也意味着“竞争”的结束。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/75fe51d132ff93b6e154d747a4be8621.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>仅可在 Chrome、Safari 或 Edge 上浏览互联网（无需修改或扩展），意味着不允许浏览器的竞争。只能在 macOS、Windows、Android 或 iOS 上浏览互联网，意味着不允许操作系统领域的竞争。只有谷歌才能抓取互联网，意味着搜索引擎也没有了竞争。</blockquote><p></p><p>&nbsp;</p><p></p><h2>对垄断的恐惧</h2><p></p><p>&nbsp;</p><p>作为iOS设备越狱工具Cydia的开发者，Jay Freeman（又名「Saurik」）在一篇帖子中，将该提案描述成基于广告的商业模式之下“Web必将踏上的一条不归路”。</p><p>&nbsp;</p><p>在一封采访邮件中，Freeman表示很长一段时间以来，Web最初采取的开放标准、可供任何人构建兼容浏览器的立场早已被打破。如今的软件正变得越来越复杂。</p><p>&nbsp;</p><p>在他看来，由于被塞进了越来越多的功能以满足Web内容发布者的期待，其实只有少数几款浏览器能跟得上时代变化。</p><p>&nbsp;</p><p>“如果网站开始要求「用户证明自己使用的是这小部分值得依赖的浏览器之一，即未对其原始行为做出修改，而后才愿意向真实用户展示其广告」，那么未来市面上恐怕再难觅浏览器新秀的踪影。”</p><p>&nbsp;</p><p>Freeman还认为，WEI的问题绝不仅限于阻碍浏览器市场的正常竞争。</p><p>&nbsp;</p><p>他强调，“我觉得这里还有更大的利害关系——比如剥夺了大家对计算机的控制权。这项功能要想成立，唯一的原因也许就是大多数人都在计算机上采用到DRM技术，比如Arm TrustZone和英特尔SGX。”</p><p>&nbsp;</p><p>“马斯克现在希望每个人只使用官方Twitter应用跟他的服务对接；Reddit近期也在朝着类似的方向发展：向应用程序公开可信计算原语，意味着可以确保仅官方客户端才能正常访问网站。如果谷歌也加入这波潮流甚至予以推动，那么我相信这不仅是对开放Web的重大打击之一，甚至也是对我们迄今为止所看到的、对运行通用计算机的基本自由的严重侵犯——自此之后，我们将无法信任运行在「不可信」操作系统上的浏览器。”</p><p>&nbsp;</p><p>Freeman补充称，“我当然相信谷歌至少在其提出的用例上是诚实的……只是这样的倾向性令人感到不安：广告发布者希望自己基于广告的商业模式继续起效，所以才要求用户只能使用匹配这个前提的受信浏览器。所以这个规范的本质，似乎更像是要求用户自己向内容发布者证明，他们没有运行任何广告拦截器。”</p><p>&nbsp;</p><p></p><h2>提案是否会被搁置或丢弃？</h2><p></p><p>&nbsp;</p><p>本周一，Mozilla公司Web平台高级首席工程师Brian Grinstead也对该提议表达了反对。</p><p>&nbsp;</p><p>“Mozilla公司反对这项提议，因为它违背了我们对于Web原则和发展愿景的初衷。……检测欺诈和无效流量确实是个颇具挑战的问题，我们也有意参与并帮助解决。但是，这项提案没有解释它要如何在列出的用例上取得实质性进展，而且在实际采用后还会带来明显的缺点。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9d935441305efe84fa85ef5937c04d1.jpeg\" /></p><p></p><p>&nbsp;</p><p>在熟悉浏览器技术开发的群体中，微软Edge合作伙伴产品经理、谷歌前高级工程师Alex Russell也在Mastodon上发帖，敦促人们在WEI充分发展成熟前先不要急于做出判断。</p><p>&nbsp;</p><p>Russell指出，“特别是在早期设计阶段，很多想法都大有问题。但没关系，API设计就是需要经历这样的问题空间，而探索其潜力的最佳方法不是推断最坏的情况，而是展示工作内容并证明其至少拥有哪些价值。”</p><p>&nbsp;</p><p>前谷歌工程师、现供职于Tailscale的Chris Palmer上周在另一篇Mastodon帖子上，认为这项提案是个坏主意。</p><p>&nbsp;</p><p>他写道，“远程认证严重冲击了激励措施。如果把客户变成了自己的敌人，那你离失败也就不远了。而如果一套框架把广告发布者的客户变成了自己的敌人，那这东西就应该被扔进垃圾堆里。”</p><p>&nbsp;</p><p>自由技术顾问Ondřej&nbsp;Pokorný同样在Mastodon上表达了类似的观点。“整个「隐私沙箱」和其他想要取代「合法」第三方用例的提案，最大的问题就在于其中提出的各种API把浏览器从用户代理变成了双重代理。这虽然符合广告商和其他企业者的利益，但却往往与用户利益存在冲突。”</p><p>&nbsp;</p><p>Palmer补充称，“最好的结果，就是谷歌明天一早就直接撤回这项提议。其中已经没有任何调整和修复的余地，唯一的选择是丢弃然后道歉。”</p><p>&nbsp;</p><p>面对大量的负面讨论，谷歌工程师给大家初步的回应是： WEI 的目标是提供设备可信的信号，让网络更加私密和安全，并且还留有一个余地，通过保留措施防止平台级别的锁定。比如在一定比例的情况下，例如 5% 或 10%，WEI 证明会被故意省略，看起来像用户选择退出WEI或设备不支持WEI一样。最后他们还强调了会“将继续公开设计、讨论和辩论”，但反对在个人页面讨论问题，随后他们锁定了该Github页面并从个人页面中删除了此提案。</p><p>&nbsp;</p><p>显然，他们的回复作用有限，网友点评：谷歌这相当于在说“我们是好人，相信我！”</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theregister.com/2023/07/25/google_web_environment_integrity/?td=rt-3a\">https://www.theregister.com/2023/07/25/google_web_environment_integrity/?td=rt-3a</a>\"</p><p><a href=\"https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd\">https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd</a>\"</p><p><a href=\"https://github.com/mozilla/standards-positions/issues/852\">https://github.com/mozilla/standards-positions/issues/852</a>\"</p><p><a href=\"https://httptoolkit.com/blog/apple-private-access-tokens-attestation/\">https://httptoolkit.com/blog/apple-private-access-tokens-attestation/</a>\"</p><p><a href=\"https://github.com/RupertBenWiser/Web-Environment-Integrity/issues/28\">https://github.com/RupertBenWiser/Web-Environment-Integrity/issues/28</a>\"</p><p><a href=\"https://beehaw.org/post/6820219\">https://beehaw.org/post/6820219</a>\"</p><p><a href=\"https://news.ycombinator.com/item?id=36875226\">https://news.ycombinator.com/item?id=36875226</a>\"</p>",
    "publish_time": "2023-07-27 15:26:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "《产业数字人才研究与发展报告（2023）》",
    "url": "https://www.infoq.cn/article/rlOCrXvCZvy11RZ3F1UU",
    "summary": "<p>随着新一轮科技革命和产业变革的深入发展，数字经济正在成为重塑全球经济结构、改变全球竞争格局的关键力量，全球主要经济体均希望通过数字化转型建立更具包容性、竞争力和创新性的新型经济结构。《全球数字经济白皮书 2022》显示，2021 年全球 47 个主要经济体数字经济增加值占 GDP 比重为 45%，毫无疑问，数字经济与实体经济的深度融合，将是未来经济发展的重要动力。而数字人才作为数字经济的核心要素，对推动数字经济高质量发展的作用至关重要。随着各产业数字化转型进入更深的阶段，大量数字化、智能化的岗位相继涌现，相关行业对数字人才的需求与日俱增，人才短缺已经成为制约数字经济发展的重要因素。中国信息通信研究院发布的《中国数字经济发展报告（2022）》显示，2021 年我国数字经济规模达 45.5 万亿元，占 GDP 比重达 39.8%。人瑞人才和德勤中国通过估算发现，当前数字人才总体缺口在 2500 万至 3000 万左右，且缺口仍在持续扩大。</p><p></p><p>我们深知，任何形式的组织变革与科技的运用其成败的根本依然在于人。人是发现问题的主观能动者，是保障组织任务和目标落地的决定因素。我们调研了众多的数字化转型企业，转型收效甚微或失败的比比皆是，究其原因主要有：1. 对数字化转型理解不深，盲目表面地转；2. 只侧重技术的运用，片面地转；3. 看到问题解决问题，缺乏系统性地转；4. 转型过程中，人才和组织机制保障跟不上。</p><p></p><p>在此背景下，人瑞人才与德勤中国携手合作，共同撰写《产业数字人才研究与发展报告（2023）》，希望通过此次研究，观察中国产业数字化进程现状，发掘企业数字化转型中的关键问题，分析数字人才现状与发展趋势，并给出具有针对性的数字人才发展问题解决方案。本报告研究对象涉及政府、企业、求职者及高校多级主体，采用公开政策研究、头部招聘平台数据采集与分析、第三方报告案头研究、企业深度访谈、调查问卷等方法获取企业数字化转型需求、问题及数字人才信息。这其中包括由公司决策者、业务主管、员工和 HR 填答的近 2500 份调查问卷，与 11 个不同行业高管交流得到的近 100 份访谈资料，以及公开的招聘数据信息，报告力求从客观的角度分析、发现各类企业数字化转型中遇到的问题，总结归纳好的经验方法，并针对困扰绝大部分企业的“数字人才不足与培养”问题提出一些针对性的解决方案，以供企业参考，期望能助力更多的企业成功实现数字化转型。</p><p></p><p>本报告第一部分概述中国企业数字化发展的背景与趋势。该部分在明确数字经济内涵的基础上概述了数字人才缺口的总体特征，并细化分析企业组织结构和人才管理体系的现状、挑战，以及应对策略。报告第二、三部分则根据德勤中国在数字化产业和产业数字化的行业经验进一步细化研究，分 11 个不同行业描述企业在人才方面遇到的机遇与挑战。该部分在展示细分行业人才供需情况的基础上，通过胜任力模型给出目标人才具备的能力特征，描绘人才微观画像以指明人才培养方向。报告第四部分将视角转向企业发展，人瑞人才从企业数字转型战略出发，分析数字化时代的产品研发策略和项目管理，并聚焦数字化时代的组织模式与人力资源管理策略，提出“以任务为中心”的组织形态、“基于人才领先”的人力资源战略、“企业内外相结合”的人才供应链体系，以及多元化的用工模式，为数字化转型过程中人才管理与培养问题提供了全面的解决方案，对长期困扰企业的“要不要转型、怎么转型、转型过程中需要注意什么，以及如何更有效地实现转型”等问题给出了归纳性意见及参考思路，对处于数字化转型进程中的企业予以启发。</p><p></p><p>数字化转型并非简简单单地将数字化技术叠加运用在企业管理中，一个企业要实现数字化转型，需要对企业组织架构、业务模式、人才结构、管理体系、企业文化等方方面面做系统性的转化。人瑞人才过去十几年服务中国各个行业著名企业，沉淀了大量专业服务经验并对行业的业务拥有深入理解与研究，德勤中国在各个行业的市场动态、业务发展趋势、企业管理模式等方面拥有深刻洞察与丰富的咨询服务经验，双方合作共同完成的《产业数字人才研究与发展报告（2023）》是国内首次对 11 个重点产业的数字人才发展的全面梳理与分析，对各行业企业的数字化转型和人才管理具有重要的参考价值。在此我们也希望借本书抛砖引玉，引发更多针对数字人才发展的讨论，并共同推进中国数字经济深入发展。</p><p><img src=\"https://static001.geekbang.org/infoq/3d/3dcfa9b13d886182faea3e38a5020150.png\" /></p><p>扫/码/下/载</p>",
    "publish_time": "2023-07-27 15:44:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]