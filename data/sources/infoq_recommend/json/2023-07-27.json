[
  {
    "title": "加速数字化转型：深度解析API成熟度模型",
    "url": "https://www.infoq.cn/article/IeNEgLeqmfLxzdjYeg4x",
    "summary": "<p>如果数字化转型得当，可以影响一个组织的各个方面。然而，API成熟度成为数字化转型需要解决的一个常见的问题。API是推动业务增长的桥梁，但随着API被广泛使用，可能会出现API蔓延。在解决日常业务问题的过程中，没有计划和管理的API激增会导致API蔓延。API蔓延说明有大量的API正在被创建，部署API的分布式基础设施也在发生物理扩散。</p><p></p><p>公司看到他们的API正以前所未有的速度在全球范围内蔓延。对于希望在分布式基础设施之间保持高质量和卓越体验的组织来说，API蔓延给他们带来了一个独特的挑战。</p><p></p><p>管理大规模的API需要自上而下的监督，它还需要一种实用的方法，并从旨在统一API的API程序计划开始。程序应该将API打包成产品或服务来推动API的采用，并促进对其整个生命周期的管理。问题在于，创建一个可行的程序来管理API成熟度是一个缓慢的过程。</p><p></p><p>本文将为构建成熟的API计划提供一个框架，这个框架使用了一个可以促进API驱动业务演化的四层API程序成熟度模型。</p><p></p><h2>什么是API成熟度模型</h2><p></p><p></p><p>关于API的成熟度和生命周期，我们可以用两个阶段来描述：API成熟度和API程序成熟度。</p><p></p><p>API成熟度与设计和开发相关，遵循的过程与软件开发成熟度一致。API成熟度确保API符合公认的API规范，比如REST。在讨论API成熟度时，你讨论的是为特定应用程序或目的而创建的一组API。</p><p></p><p>API程序成熟度主要是从整个公司层面来看的，即公司为满足各种业务目标而积累的大量API。对于API程序成熟度来说，将API构建成统一的服务是有必要的。API程序成熟度模型为通过简化API来促进业务创新提供了基准。</p><p></p><h2>API程序成熟度模型</h2><p></p><p></p><p>API程序成熟度从技术和业务的角度评估API的非功能性指标。API的技术指标包括性能、安全性、体验和可伸缩性。API的业务指标与间接影响时间和成本的流程和生产力的改进有关。</p><p></p><p>与其他业务流程一样，API程序也应该从小处开始，然后逐渐演化。API程序的结构必须能够遵循持续的改进周期。指标应该随着API程序从较低成熟度级别过渡到较高成熟度级别而得到改进。</p><p></p><p>在开始你的API成熟度模型之旅之前，你必须首先将API视为一种工具。然后，在模型的演进过程中，随着达到更高的成熟度级别，你需要基于API为日常业务所带来的能力将其视为组件、模型或生态系统。</p><p></p><h2>API程序成熟度的四个级别</h2><p></p><p></p><p>如果你将API程序成熟度视为企业数字化转型整体方法的一部分，API程序可以分为四个成熟度级别：</p><p></p><h4>级别1：“API的暗黑时代”——API作为数据采集工具</h4><p></p><p></p><p>从历史上看，构建API是为了方便数据采集，Salesforce和Amazon的早期API就是最好的例子。这类API的主要目的就是用于标准化跨多个业务应用程序的数据共享。</p><p></p><p>API程序成熟度的第一个级别是为可以提供单一事实来源的数据采集创建标准化的数据访问接口。这些API按照不同的业务功能分类。例如，你可以有分别用来访问财务、销售、员工和客户数据的API。</p><p></p><p>当你建立了API设计和架构最佳实践，你的组织就达到了API程序成熟度级别1。一些最佳实践包括：</p><p>在设计API时要考虑到集成便利性和可重用性；所有的API都保持一致的接口；在设计中结合版本控制，同时支持多个客户端；确保API的可伸缩性，适应不断变化的用户需求。</p><p></p><p>这些API相对简单，不需要高级的可编程功能。级别1也可以定义为一种相对不成熟的、手动的API部署方法。手动部署的个体API不支持API生命周期管理，技术的重点放在了将API构建为独立的工具上。</p><p></p><h4>级别2：“API的复兴时代”——API作为流程集成组件</h4><p></p><p></p><p>回顾API的发展历史，从2000年代开始，当它们开始被用作连接器来集成不同的系统时，迎来了它们的复兴时代。单点登录（SSO）就是一个典型的例子。SSO时一种被广泛使用的API集成工具，用于对用户进行身份验证，让用户能够安全访问多个应用程序和第三方服务。</p><p></p><p>当你的组织达到API程序成熟度级别2时，你的API程序将使用基于组件的方法。应用程序被分解为单独的组件，每个组件都可以独立于应用程序的其他部分进行开发和测试，然后再集成成一个完整的应用程序。这种方法降低了复杂性，易于维护，并提高了可伸缩性。</p><p></p><p>API将作为集成不同业务和特定领域流程的组件捆绑在一起。这些API包简化了运营和工作流，将多个部门连接起来，甚至可以集成与外部合作伙伴的工作流与交互。</p><p></p><p>当你达到级别2时，你的组织就迈出了将API应用于业务的第一步。在级别2，API被视为组件，为你提供了标准化和可重用的API目录。级别2通过CI/CD（持续集成/持续交付）管道实现标准化和自动化，改进了开发周期，从而推进了API开发和生命周期管理。</p><p></p><h4>级别3：“API的启蒙时代”——API作为统一体验的平台</h4><p></p><p></p><p>在API复兴时代，API被视为组件，以此来简化集成和提升可重用性。级别3是API的启蒙时代，它更进一步，让API变得对用户更加友好和有价值。</p><p></p><p>当你达到级别3，API就不再被视为改进业务工作流的组件或独立的工具。这一级别关注的是构建API套件，通过创建互连的体验来驱动更好的工作流程。API组件的作用在于，API提供者可以在设计和构建API时对应用程序进行分解。而API套件的作用在于，API提供者可以对功能进行分组，让API消费者可以与它们集成，从而获得更好的体验。</p><p></p><p>例如，物流公司依靠卡车和送货车车队来维持业务的连续性。它可以使用API套件来监控和管理其车队的各个方面。在级别3，你需要一个精心设计的包含多个API的API套件来监控卡车、绘制路线、提供性能分析，等等。</p><p></p><p>在级别3，API是用户体验（UX）的关键。API套件成为面向用户的应用程序的支柱。在上面的卡车车队示例中，公司用于车队管理的前端软件依赖于API来驱动最终的用户体验，因此API套件成为为整个软件包提供接口的后端平台。</p><p></p><p>当你达到了级别3，API程序将起着至关重要的作用，因为API套件成了任务关键型的服务。在这个阶段，API消费者做了大量的投入，API的可靠性和成熟度就变得非常重要。处于级别3的API程序都达到了一定程度的技术成熟度，包括：</p><p>部署：API套件进行批量部署，与API生命周期阶段和版本控制紧密结合。性能：API支持云原生环境，可以获得更好的可伸缩性和弹性工作负载。安全性：启用多层安全机制，确保严格的认证和授权过程。自动化：CI/CD管道是完全自动化的，包含了严格的API测试。体验：自助式API门户有助于开发人员快速上手。</p><p></p><h4>级别4：“API的自由化时代”——API作为业务转型的生态系统</h4><p></p><p></p><p>当你的组织达到了API程序成熟度级别4时，你将拥有完全外部化的API。API演进的最后阶段更多地是由业务需求而不是技术驱动的。在这个级别，你可能已经有了一个运行良好的技术栈，并且正在推动内部和合作伙伴采用API，因为API带来了很多业务价值。下一个合乎逻辑的进程是通过货币化将这种价值外部化。</p><p></p><p>在级别4，你将采用一种新的方法——API即产品。在这个级别，你可以通过服务订阅模型向客户提供API。API即产品可以作为独立服务或补充服务来提供，具体根据公司的业务性质来决定。无论采用哪种方式，API都紧密集成到了你的产品、营销和销售中，因此每个人都可以通过协作来推动这种新兴的价值流。</p><p></p><p>在级别4，API程序成为业务增长的引擎。是否已经达到级别4的一些指标包括：</p><p></p><p>API治理</p><p></p><p>你有一个专门的API产品管理小组。这个小组确保所有API都是基于一组预定义的规则开发的。它还定义了API生命周期进展策略，确保API具备架构和安全合规性。</p><p></p><p>API可观测性</p><p></p><p>你的团队不仅能够对API进行监视，还能够捕获API业务逻辑的内部状态，收集与性能有关的数据。</p><p></p><p>API生态系统</p><p></p><p>你建立了一个API社区，让开发人员和消费者有交换意见和寻求支持的地方。API论坛进一步增强了API生态系统，推动API的采用。</p><p></p><h2>API程序改进周期</h2><p></p><p></p><p>这个世界上没有完美的API程序。任何一个API治理框架都必须进行定期审计，确定API程序的当前成熟度级别。</p><p></p><p>无论处于哪一个API成熟度级别，采用DevOps方法并通过持续的小Sprint来提高API成熟度都是必要的。要采用DevOps方法，需要在组织范围内建立共识，实现更敏捷、更快的小增量改进周期。</p><p></p><p>理想的API程序改进周期包括五个阶段。</p><p></p><h4>评估和探索</h4><p></p><p></p><p>第一个阶段是在技术和业务层面评估API程序的当前状态，并探索改进它的可能性。当然，技术成熟度要先于业务成熟度，应该是级别1和级别2的核心关注点。</p><p></p><p>在探索需要改进的领域时，可以设定一些小目标，而不是试图从一个级别直接跳到下一个级别。你可以在内部定义子级别，改进API程序的某个特定方面，例如部署自动化、安全性或可伸缩性。</p><p></p><h4>设计与建议</h4><p></p><p></p><p>第二个阶段是改进周期中最关键的决策点。在这个阶段，你需要梳理来自不同利益相关者的技术规范和业务目标。然后，你可以建议对底层API管理技术栈做出修改，这些修改应该是当前改进周期的一部分。</p><p></p><h4>构建与实现</h4><p></p><p></p><p>第三个阶段是改进周期的实现阶段。这个阶段包含了基于建议的开发和配置增强。</p><p></p><h4>测试与监控</h4><p></p><p></p><p>在第四个阶段，你开始通过测试来驱动API。在这个阶段，你通过监控重要的性能和改进指标来判断API改进周期的总体有效性。这个阶段可能会很长，因为你需要在第三阶段和第四阶段之间来回转换，直到可以从指标上看到可度量的改进。</p><p></p><h4>启动新的API程序</h4><p></p><p></p><p>一旦完成了测试和监控阶段，并且看到了真正的改进，就来到了最后一个阶段——生产部署。在这个阶段，你将新的API程序产品化，并启动和运行它们。</p><p></p><h2>如何提升你的API程序成熟度</h2><p></p><p></p><p>这里给出的不同级别的API程序成熟度提供了一条带有逻辑里程碑的清晰路径，帮助你的组织从低级别API实现过渡到高级API实现，但这里还有一个更重要的挑战需要你去解决。</p><p></p><p>API程序象征着组织范围内采用其他API的氛围。将API的发展定位成推动业务增长的主要引擎之一，这是一个理想的愿景。然而，要让API程序取得成功，必须将其构建成横向的跨部门和团队的功能。</p><p></p><p>在大多数企业中，每一个部门都需要其他部门的清晰度和可见性，这也是为什么需要大量工作来执行治理和标准化的原因之一。缺乏可见性会增加创建重复API的可能性。</p><p></p><p>孤立的API团队可能会带来一些挑战，例如缺乏沟通、理解和可见性。彼此孤立的团队为建立API开发集成策略带来了挑战。此外，每个团队可能有不同的优先级，导致整个过程出现延迟和错误。此外，团队彼此孤立会导致协作和知识共享的机会变少，而这些本来可用于提高正在开发的API的质量。</p><p></p><p>横向的API程序功能跨越了这种部门间的孤岛，有助于确保API的统一治理和标准化。</p><p></p><p>以下是一些你可以用来应对持续改进周期挑战的规则。</p><p></p><h4>由外而内建立共识</h4><p></p><p></p><p>由外而内的方法需要对业务工作流进行分析，围绕它们设计出正确的数字体验。与由内而外的方法相比，由外而内的方法在捕获各种利益相关者的期望方面要有效得多。</p><p></p><h4>自上而下的文化转变</h4><p></p><p></p><p>寻求推动全公司文化转变的最佳实践是一个极具争议的话题。由于成功的API程序需要水平对齐，因此采用自上而下而不是自下而上的方法可以降低出现API蔓延的可能性。</p><p></p><p>自上而下的API开发方法有几个优势，例如缩短发布时间、缩短开发周期和更容易维护。它还为API架构提供了更大程度的清晰度，使得开发人员能够更容易地知道他们必须做些什么以及他们在整个项目中的职责。此外，自上而下的方法可以减少与API安全性、可靠性和文档相关的工作量。</p><p></p><h4>战略观点</h4><p></p><p></p><p>API程序成熟度的初始级别是将API视为技术工具包的一部分。记住，这是一种短期的战术视角。随着API程序的不断发展，必须不断努力构建战略愿景，让API程序带来可通过业务级KPI来度量的价值。</p><p></p><p>达到最高的API程序成熟度级别需要时间和精力，同时还要管理利益相关者的期望。但不管怎样，开发成熟的API程序将为加速业务创新和增长带来新的机会。</p><p></p><p>作者简介：</p><p>Darshan Shivashankar是APIwiz的创始人兼首席执行官。APIwiz是一个低代码API运营平台，旨在通过简化API生命周期管理来提高生产力和实现大规模治理。他有12年帮助企业进行数字化转型的经验。他曾为电信、医疗保健和金融行业的大型企业领导和实现API程序。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/api-maturity-model/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTAxNzA5MzUsImZpbGVHVUlEIjoiTVlneHNCOTVDTlU1TGx2NSIsImlhdCI6MTY5MDE3MDYzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.GnN_eCmUH_ihtMltPOgnFRsI72aQPdHIExmo9K95EHs\">https://www.infoq.com/articles/api-maturity-model/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/shlKO1bTHS2tCFiFpNe1\">API 设计评审已死，API 设计评审万岁</a>\"</p><p><a href=\"https://www.infoq.cn/article/mDlgpAcKK4V3Qr4r9d8E\">API 网关和负载均衡器，到底怎么选？</a>\"</p>",
    "publish_time": "2023-07-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于鲲鹏+昇腾 华鲲振宇重磅发布算力创新产品及解决方案",
    "url": "https://www.infoq.cn/article/Nk9DyErUiAA5uH80qUU0",
    "summary": "<p></p><p>2023年7月26日，<a href=\"https://www.infoq.cn/article/iCuurCMJCvofpfylbGpE\">华鲲振宇</a>\"在北京举办产品发布会，以“中华鲲鹏振兴寰宇，昇腾万里智领未来”为主题，分享基于“鲲鹏+昇腾”的算力创新，发布全新一代算力基础设施与解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec4766737610417cbcf9f7a141a8576e.png\" /></p><p></p><p>此次发布会汇聚产业精英、客户和生态伙伴，共同探讨满足数字时代多样性算力需求的未来发展方向。北京市朝阳区人民政府党组成员、副区长舒毕磊、华为计算业务总裁田华、华鲲振宇总裁万诤等领导出席大会并致辞。</p><p></p><h4>深入场景发布四大“天”系产品 全栈创新敢于亮剑</h4><p></p><p></p><p>擎天之力铸坚实算力底座——“天擎”系列高规格通用计算平台</p><p></p><p>随着应用与场景驱动需求激增，对<a href=\"https://xie.infoq.cn/article/372faa42c86908abae0360a08\">算力基础设施</a>\"提出了更多差异化的需求，本次大会，华鲲振宇正式揭开基于鲲鹏创新架构算力产品的神秘面纱，华鲲振宇研发总经理赵彦钧携华为鲲鹏计算业务副总裁马银川正式发布“天擎”系列高规格通用计算产品，最大支持16块NVMe SSD，创新蝶形IO模组设计，最多可支持11个PCIE 标准插槽，规格提升了37%；率先支持9张 PCIe X8的 NPU/GPU加速卡，或两张全高全长双宽的训练卡，整机AI计算性能提升15%以上，各项参数直指鲲鹏最高规格，为千行百业提供高密性能的算力应用平台，已规模服务于运营商、互联网、金融等行业，满足客户在不同部署场景、不同算力、不同存力需求下的计算任务，让每一次计算都能心有所想、算有所成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52d019f2cd0760e857efad515f809a84.png\" /></p><p></p><p>深耕智算助力大模型普惠AI——“天智”系列高密AI算力平台</p><p></p><p>伴随大模型带来的生成式AI突破，人工智能技术正在跨越重要的历史拐点，华鲲振宇也早已有了筹谋，基于昇腾AI计算，在多个典型应用场景进行了深入创新和升级，积极拥抱人工智能，在大模型训练场景下，推出基于昇腾的“天智”系列高密AI算力服务器——新一代训推一体服务器AT800 A2，底层技术的突破为大模型的训练推理提供极致的算力输出，助推国内大模型应用广泛落地，进一步普惠AI。基于天智训练服务器，华鲲振宇已为“蓉城 • 夔（kuí）牛”短临预测模型提供稳定“算力底座”，以成都智算中心300P算力为基础，实现精细化的气象预报服务。备受关注的第31届世界大学生夏季运动会（成都大运会）即将开赛，\"夔牛\"可作为赛场的一匹“黑马”，助力大运会期间的天气预测，把前沿的“黑科技”运用到大运会“智慧气象”服务中，让AI大模型应用于具体场景。</p><p></p><p>以恒致远令融合算力触手可及——“天恒”超融合移动平台解决方案</p><p></p><p>在<a href=\"https://xie.infoq.cn/article/376502f80af0b1d2c672a8e96\">边缘计算</a>\"场景中，客户对计算设备部署的便利性、可移动性以及异构算力融合方面提出了更高的要求。华鲲振宇产品总经理李锐携手华为昇腾计算业务副总裁刘鑫发布业界首创鲲鹏+昇腾“天恒”超融合移动平台，应对应急救援、海上勘测、地质勘探和航拍合成等各种极端恶劣的环境场景， 具备高可靠性和稳定性，轻松完成信号处理、全景视频融合、IoT数据采集与处理、目标检测等功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fbb31cb233ae282c4203a34f6bfad7c.png\" /></p><p></p><p>技术创新让数据中心极致节能——“天极”系列液冷解决方案</p><p></p><p>在国家双碳战略背景下，数据中心算力规模目前持续高速增长，能耗随之急剧扩张，以应用液冷技术为代表的绿色数据中心，成为新基建、“东数西算”的算力底座，如何降低数据中心的能耗成为整个行业技术创新的焦点。华鲲振宇超前洞察，携手华为集群计算业务副总裁王振华发布双液冷解决方案——天极HC8000冷板式液冷和天极HC6000浸没式液冷，挑战PUE小于1.1，HC6000更是基于鲲鹏+昇腾全球首发的浸没式液冷服务器，采用国产原生冷却液，具有极致节能、极稳运行、轻松运维等极致优势，为关键业务场景提供绿色低碳的强劲算力。</p>",
    "publish_time": "2023-07-27 09:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型刷新教育赛道，网易有道发布国内首个教育大模型“子曰”",
    "url": "https://www.infoq.cn/article/RybdOoGtK9NiYTsqmEgx",
    "summary": "<p>7月26日，教育科技公司<a href=\"https://www.infoq.cn/article/5LaZorFzj54vi6YV5xLf\">网易有道</a>\"在“powered by 子曰”教育大模型应用成果发布会上，推出了国内首个教育领域垂直大模型“子曰”，并发布了基于“子曰”大模型研发的六大创新应用——“LLM翻译”、“虚拟人口语教练”、“AI作文指导”、“语法精讲”、“AI Box”以及“文档问答”。</p><p>&nbsp;</p><p>网易有道CEO周枫表示：“一个好的技术有没有价值、能不能发挥巨大的作用，很多时候关键在场景和应用的选择以及细节的打磨。通过软件、硬件、AI技术的结合，做出精品是我们现在做的事”。</p><p></p><h2>有道发布教育大模型“子曰”，六大创新应用成果正式落地</h2><p></p><p>&nbsp;</p><p>大模型的出现能给教育带来的最大机会是什么？</p><p>&nbsp;</p><p>周枫在发布会上表示：“我认为，是助力因材施教”。据介绍，之所以叫‘子曰’，是因为孔子是我国的教育先贤，又是因材施教教育理念的奠基者。‘夫子教人，各因其材’，我们希望子曰大模型可以朝着这样的教育理想去做。”</p><p>&nbsp;</p><p>周枫向在场观众分享了大模型“因材施教”的三大优势。首先，大模型能为学生提供个性化的分析和指导；其次，大模型能够实现引导式学习，与教师一样，提出问题并引导学生自行探索答案；最后，大模型具备全科知识整合能力。通过连接多模态知识库、跨学科整合知识内容，大模型能随时满足学生的动态需求，帮助孩子培养更综合的能力。</p><p>&nbsp;</p><p>相比于通用大模型，“子曰”大模型从一开始就定位为是一个“场景为先”的教育垂类大模型。它能够作为基座模型支持诸多下游任务，向所有下游场景提供语义理解、知识表达等基础能力。基于此，有道研发团队在“子曰”大模型的基础上，为不同学习场景设计了定制化的模型，以实现模型与场景的高度契合。</p><p>&nbsp;</p><p>“我们的目标很明确，就是以实际的教育场景驱动，用技术创新助力教育创新。”周枫表示，希望技术和产品的深度融合，可以根据学⽣不同能⼒和需求，提供因人而异的个性化教学。</p><p></p><p>在发布会上，网易有道展示了“子曰”大模型在多个场景中的应用成果，包括“LLM翻译”、“虚拟人口语教练”、“AI作文指导”、“语法精讲”、“AI Box”以及“文档问答”。</p><p>&nbsp;</p><p>其中，最为瞩目的是大模型时代的英语口语练习神器——虚拟人口语教练Hi Echo。发布会现场，有道词典业务负责人与 Echo 进行了多组随机对话。Echo 能迅速理解场景和上下文，并给出迅速反馈，发音也很地道，重音、弱读、升降调等细节处理得非常到位。在对话过程中，Echo能够像真人老师一样循循善诱，启发式进行对话引导，还能进行实时反馈。对话结束后，Echo会从发音、语法等维度给予建议和润色，能有效解决长期困扰英语口语学习者无话可说、不知从何说起、害怕说错等问题。</p><p>&nbsp;</p><p>“中国人在说英语时往往面临开口难、不敢说、不知道该从何说起的困境，其中的关键就在于缺乏语言环境。” 有道词典业务负责人表示，Echo恰恰能为用户带来这种真正贴合实际的“语境”，帮助他们更好地练习英语口语。</p><p>&nbsp;</p><p>此外，“子曰”大模型还覆盖了多种学习场景。例如，在写英语作业时，学生们不仅有解决具体问题的需求，还需要学会举一反三。“子曰”大模型赋能的“语法精讲”功能可以为学生提供针对性的解题思路和方法，还能推荐同类型的考题，帮助学生触类旁通，真正理解考纲中的考点。</p><p>&nbsp;</p><p>“AI作文指导”应用不仅具备“作文批改”功能，还具备“作文指导”功能。据介绍，该应用旨在解决“学生不会写”和“老师没时间改”的问题。针对学生在写作、前、中后过程中面临的题目主旨难确定、写作素材匮乏等难题，该应用都能够给予指导，帮助学生“下笔如有神”。批改环节中，AI作文指导还会从表达、结构、内容深度、情感丰富度四大维度全面提供改进建议。</p><p></p><h2>场景拉动，“AI+教育”技术沉淀与创新&nbsp;</h2><p></p><p>&nbsp;</p><p>会上，周枫多次强调“场景拉动”的重要性。他表示，“子曰”大模型在教育行业的应用，不仅可以帮助学生更好地学习，也可以帮助老师更好地教学，借此实现因材施教的教育理想。</p><p>&nbsp;</p><p>通过深入调研和分析用户在不同场景下的需求，网易有道成功利用大模型的力量，在教育领域打造了诸如虚拟人口语教练、语法精讲、AI 写作指导等丰富的解决方案。这一策略不仅体现了网易有道对教育场景的深入理解，还为用户提供了更加个性化和高效的学习体验。</p><p>&nbsp;</p><p>早在2008年，有道就推出自主研发的国内首家统计机器翻译线上引擎。经过15年技术迭代，有道神经网络翻译（NMT）已经进化成行业领先的“最强大脑”。根据QUESTMOBILE最新数据，到目前有道词典月活用户已经超过1亿，是国内词典翻译市场的第一名。</p><p>&nbsp;</p><p>从2016年开始，有道协同构建AI基础能力，同步组建语言、视觉、声音等团队，目前积累了有道神经网络翻译（NMT）、计算机视觉、智能语音AI技术、高性能计算(HPC）四大底层技术能力。</p><p>&nbsp;</p><p>自2017年，有道就与主流技术<a href=\"https://www.infoq.cn/article/voRBHu6lzxRuNplYwD0P\">Transformer</a>\"“双向奔赴”，将AI能力统一在大模型之下，并尤其重视在端侧的落地应用。有道词典笔2代2019年首次搭载离线Transformer NMT。2022年，有道词典笔P5中搭载了自研的离线ASR，也已升级为Transformer技术。技术的持续赋能奠定了有道学习硬件在行业内的领先地位。</p><p>&nbsp;</p><p>底层技术不断革新的同时，有道还在不断研发细分场景下的“黑科技”。如虚拟人口语教练实现语音识别能力、虚拟人驱动技术和内容生成和对话能力等多项技术能力的突破。&nbsp;例如，虚拟人口语教练在语音识别能力方面进行了巨大的革新，它支持多语种的流式低延迟语音识别技术，让Echo在中式英语、英语、中英混合等场景下游刃有余；声学降噪、回声消除、自动语音检测、自动断句等技术，则让它像一个真正的倾听者和交流者，不仅能判断用户说话的起始，还能让用户随时打断，智能触发后续流程。</p><p>&nbsp;</p><p>在AI虚拟人的驱动方面，有道基于自主研发的情感识别算法和实时渲染驱动引擎，对播放的语音数据进行深度分析，实时驱动虚拟人的面部表情和语音同步的口型变化，使虚拟人能够贴近真人，以更加自然和生动的方式与用户一对一交互，从而显著提升对话的真实感和用户体验。</p>",
    "publish_time": "2023-07-27 10:49:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型趋势下，企业数据体系有何挑战？有何新机遇？",
    "url": "https://www.infoq.cn/article/0UqI1dHapss161qeXyr3",
    "summary": "<p></p><p>随着<a href=\"https://www.infoq.cn/article/kWKCoO36iYG4RmoGyatN\">大模型</a>\"的兴起，企业数据体系的构建和管理面临着新的挑战和机遇。对于企业来说，如何以最低的成本、最安全的方式，最快速地调整和优化数据体系以支持大模型的训练和应用，成为了一个重要的议题。在刚刚结束的 ArchSummit 全球架构师峰会 2023（深圳站）中，数巅科技创始人、CEO 何昌华博士在主论坛上围绕“大模型趋势下的企业数据体系思考”展开了主题分享，在演讲中，他为大家介绍了他在大模型时代数据处理领域的发展趋势洞察，并简明扼要地为大家介绍了虚拟化技术、物化视图等关键技术与前沿数据理念，现场的架构师们获得了有关未来企业数据架构构建的关键洞察和有效指导，以应对大模型快速发展趋势下的大数据环境。</p><p>&nbsp;</p><p>为了更深度地了解大模型与企业数据治理领域的发展，在演讲结束后，InfoQ 对何博士进行了视频专访，以下是视频专访的全部内容，为方便读者查看，视频下方也附上了文字内容。</p><p>&nbsp;</p><p></p><p></p><p>采访记者：InfoQ 资深编辑鲁冬雪</p><p>采访嘉宾：数巅科技创始人、CEO 何昌华博士</p><p>嘉宾简介：斯坦福大学 PhD，数巅科技创始人、CEO。之前曾经在蚂蚁集团担任计算智能部门负责人，计算存储首席架构师；2017 年之前在硅谷任职于 Google，Airbnb 等互联网公司。在过往职业生涯中，何昌华主导开发过实时智能决策系统、金融级的分布式图数据库、新一代分布式计算引擎、下一代逻辑数仓、新一代搜索引擎架构等。</p><p>&nbsp;</p><p></p><p>InfoQ：您今天的演讲围绕《大模型趋势下的企业数据体系思考》展开的分享，您在演讲中提到了数据治理、人工智能领域的技术发展速度非常快，您能用三个关键词来总结下数据治理与人工智能领域未来发展的主要趋势是什么吗？</p><p>&nbsp;</p><p>何昌华：第一个词是“规模化”。很明显规模化意味着数据、模型的规模，这些都是在极度膨胀，同时也意味着应用场景其实是越来越多，也要呈规模化的发展。</p><p>&nbsp;</p><p>第二个词是“自动化”。以往通过人工来加工各类数据、做各类模型，这个过程会慢慢地被越来越多的自动化流程所取代，因为这样整个流程才能真正地实现自动化，没有人工的干预并且整个系统能够自动往前演进。</p><p>&nbsp;</p><p>还有一个词是“融合”，或者叫“协同”。当前大家可以看到各类来源的数据，会把它融合在一起，成为一个叫做所谓超融合的数据体系。对模型而言，大模型基本上是具备了以前很多个小模型在不同场景下的一些通用能力，而“大模型”和“数据”又应该协同起来。</p><p>&nbsp;</p><p></p><p>InfoQ：在您看来，人工智能和大数据领域最重要的技术挑战是什么？目前是否有比较好的解决方案？</p><p>&nbsp;</p><p>何昌华：我觉得这两个领域有区分，但又是紧密联系在一起的。现在面临的一个非常核心的问题就是如何获得“高质量的数据”，事实上大家可以看到 ChatGPT，它之所以能够实现这样的能力，是因为它完全是在工程上，尤其是在数据策略上做到了极致，但其实本质上它并没有一个理论上的突破。他们有非常多的人，不仅仅是收集到公共数据，他还会去给数据做各种各样的分析、标注以及挑选合适的数据，最后做出了这样的一个模型。而反观我们有很多大模型的公司，事实上都是处在一个相对比较缺乏数据的阶段，尤其缺乏高质量的数据，这是一个非常大的挑战。</p><p>&nbsp;</p><p>另外一个就是“整个计算的能力”，随着模型的规模越来越大，需要的算力越来越高。算力本身的转化，需要大量的设备与金钱来解决问题。这个问题其实还有一些解决方案，至少我们有能力去做大规模的集群，或者去打造一种专用的芯片，能让这个计算的密度变高，这个问题还是容易被解决的。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您在 2022 年创立了数巅科技，在创业之前在蚂蚁集团工作，想问一下您的创业初衷是什么？</p><p>&nbsp;</p><p>何昌华：我在蚂蚁集团的时候就觉得数据智能这个事情在金融行业是非常有可能落地的。因为前一个巨大的场景就是搜索推荐广告（互联网场景）。通过在蚂蚁集团多年的观察，我发现“智能数据治理”不仅只服务于金融行业，它同时能够降低数据智能在很多其他行业的门槛。我们现在讲产业互联网的升级，或者讲企业数字化，它是真的可以帮到各行各业的，所以我们创立了数巅科技。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：数巅科技是一家怎样的公司呢？公司愿景是怎样的？</p><p>&nbsp;</p><p>何昌华：数巅科技的愿景是让数据智能像水电一样简单，让每个人都能方便地使用数据智能技术。这个愿景看似很高端，但我们的初心是让数据智能成为真正人人可用、触手可及的一项能力。</p><p>&nbsp;</p><p>数巅科技目前已经拥有近百人的工程师团队，我们是技术驱动、产品驱动的一家公司，基本上大家都是在雕磨技术。我们的技术目前在世界上还没有完全对标的竞争对手，我们不断创新，打造出独特的技术。我们的团队全力打造这项技术，让它成为一项领先于市场的创新型技术。</p><p>&nbsp;</p><p>数巅科技目前有两个核心产品。第一个产品是最底层的<a href=\"https://www.infoq.cn/article/Re4O8YyDbACbrsOh3R7l\">数据引擎</a>\"，它基于数据虚拟化技术打造，具有通用的数据服务功能。这个引擎提供了虚拟的数据语义，使用户无需关注技术细节，像使用水龙头一样方便直接使用数据。同时，这个产品还采用了智能化和自动化的技术，性能比原来的产品更优秀。这个产品已经在实际场景中得到了应用和打磨，现在在多家企业得到了应用。</p><p>&nbsp;</p><p>第二个产品我们叫大模型的智能助手，它基于现有大模型的技术，充分利用企业内部的数据资产，帮助用户充分协同大模型的能力，从而让生产一个大模型的应用，在企业内部变得非常简单。这个产品可以帮助企业进行智能的 BI 分析，用户可以直接说出指标和看板。同时，这个产品也提供了智能问答功能，可以结合企业内部的规章制度，回答用户提出的问题。这个产品已经在不同的场景下得到了应用和验证，表现出了良好的性能和效果，深受用户喜爱。</p><p>&nbsp;</p><p>我们也做了智能的运营，你提一个问题，它就可以告诉你一个运营方案。我们也做了这样的一些智能的问答，它可以结合你企业内部的规章制度、企业内部的很多数据，回答用户提出的问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：据悉，公司定位为“大模型应用和企业数据的协同者”，为何会将此作为公司定位？</p><p>&nbsp;</p><p>何昌华：我们也是一直在思考这个问题，回到公司的愿景，我们还是希望能够让数据智能这个事情，能够很容易的让所有人能够用到。尤其（在）大模型出来的时候，我们就发现大模型它完完全全地降低了模型的门槛。以前我可能要花很大的精力针对一个场景去研发一个模型，当你发现有这样一个大语言模型，它的能力覆盖足够多的通用的场景。</p><p>&nbsp;</p><p>第二个就是我们又发现在企业内部，它拥有大量的数据，甚至很多企业已经有了很比较成熟的数据体系。但是那样的一套数据体系，它以前是完全没有考虑过大模型这样的一个角色的，它那个数据体系里面基本上在做数据分析的一些东西。</p><p>&nbsp;</p><p>所以我们觉得在这两个之间有一个巨大的鸿沟。并且从我们在技术上的理解来看，这两者也必须充分协同，才能够真正地做到企业业务的智能决策，所以我们才把自己定位在这样一个地方，并且我们能够抽象出一个通用的引擎来做这件事情。</p><p>&nbsp;</p><p>InfoQ：听起来其实也不是大模型应用了，我觉得定位更像是“大模型与企业数据的协同者”。</p><p>何昌华：是的，我觉得我们不能算一个大模型的应用。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：刚才您也聊到，咱们现在目前在研发的核心产品是数据虚拟化引擎和大模型的应用框架。我们先来了解一下数据虚拟化引擎，您觉得它的主要的功能和优势是什么？</p><p>&nbsp;</p><p>何昌华：我觉得它最核心的功能上的优势之一，即用户可见的功能。它的确将大数据本身的复杂度全都屏蔽在用户之外了。在大数据过去 20 年的发展中，其实主要是因为数据量太大难以计算，因此才发展出一整套包括批处理、流处理以及各类数据加工，最后再回到现在所谓的一个 <a href=\"https://xie.infoq.cn/article/77ec0d231d36c963a8e6d1630\">OLAP 引擎</a>\"供大家消费。这条链路很长，我们能够看到很多中小企业，根本没有能力搭起这样一条链路。用户去消费数据时，他需要理解前面是什么样的数据和引擎，其理解的成本也很高。因此我们想用数据虚拟化引擎给用户真的提供一个纯数据视角。他作为一个业务开发者或一个业务人员，其所看到的就是一片数据。至于此数据你究竟是用什么引擎处理、存储位置、其计算的内容、他都不用关注，我觉得这是数据虚拟化引擎提供的最核心的一个能力。</p><p>&nbsp;</p><p>我们后面需要把它的性能做到极致，这种情况下，我们也会做很多针对数据消费的语言，我们叫 SQL，针对 SQL 语言的优化包括自动的物化、包括加速等这样的能力。这个引擎可以帮助分析你的数据体系里有多少数据是重复的？有多少计算是现在冗余的？你又如何优化现有的体系？这是能够显著地给企业降本增效的。同时，企业如果用我们的引擎来访问它的数据，它事实上又能够得到更高效的数据访问、更容易地在未来跟大模型来集成。这些都是用户在实际使用过程中会看到的一些优势。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：数据虚拟化引擎的技术特点有什么？刚才您也提到了 SQL，是不是在识别分析优化能力上，它的表现还是很出色的？</p><p>&nbsp;</p><p>何昌华：我觉得这个引擎虚拟化是一个抽象的概念。它非常核心的能力是智能化，智能化就包含我们能够智能地对 SQL 做很多优化，我们能够智能地预测出来。譬如说可能哪一些中间表是需要预先加工出来的，这个技术叫做物化视图。这样下一个 SQL 执行的时候就会更加高效、更快，同时我们也能够做全局智能的编排。SQL 执行下来后，我应该怎么样分解这个任务、怎样去执行它、包括我内部的存储的数据该去有怎样的索引、如何跟外部交互，这些现在均在一个智能引擎的控制之下。这应该也算一条较创新的路。</p><p>&nbsp;</p><p>当然从产品模式上来说，这个智能化的过程又是从产品上开放出接口，让用户完可以完全地、白盒化地控制它，因为事实上很多用户还是希望对中间的数据能有很多手动的控制，与一些自动、手动的处理。在这种情况下，我们完全在产品上会开放出给他们调节的机会。但是整个引擎本身是基于一个智能化的能力的。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>InfoQ：我们还是聚焦到技术上，您觉得数据虚拟化引擎是如何帮助公司实现数据的高效处理和分析？</p><p>&nbsp;</p><p>何昌华：我觉得这一层可以看作是实现了一个逻辑的湖仓一体。现在业界也有这样的一些概念出来，即所谓的逻辑数仓，大家都在基于这个概念在做产品，但我觉得我们其实没有100%的和这个概念匹配，概念上我们相当于是建立起来一个逻辑的数仓，或者叫逻辑的数据集市。</p><p>&nbsp;</p><p>这个逻辑化的体系，其实它是完全针对数据语义的。用户在使用的过程中，譬如说我们给其提供了一个虚拟宽表。现在就是做大数据的从业者，大家都会谈到大数据宽表的概念。数据宽表是非常难以加工，即你要用大量的计算存储才能把宽表加工出来。我们提供一个逻辑宽表很简单，但用户针对逻辑宽表来消费数据时，我们会根据其消费找到对应的物理数据，然后对 SQL 做各类优化处理。</p><p>包括我前置的可能会做一些预先的物化，从而能够让此 SQL 更加高速。这是我们整个链路上很核心的两个能力。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：“数据虚拟化引擎”在整个大模型技术的发展中扮演了一个什么角色？</p><p>&nbsp;</p><p>何昌华：这样的一个数据虚拟化的引擎在我们分析的范围以内，它是最适合于大模型去迭代并做好业务决策协同者的角色的。不容讳言在整个大模型的生态里面，大模型本身它肯定是最重要的能力。大模型的能力越强，肯定越易做出一个很正确的业务决策。但我们也发现大模型的技术本身可能无法处理如此大量的结构化的数据并做精确的计算后来辅助决策，它更多是提供一个逻辑思考的能力。当然这个逻辑思考的能力，需要有语言把它训练出来。本质上，它是在根据企业数据在去做决策的时候提供的一个逻辑思考能力。</p><p>&nbsp;</p><p>关于企业数据本身，它应该提供计算、存储等一些多样化的能力。所以我们认为数据虚拟化引擎是最好的、能够去跟大模型对接的数据工具之一，它能够帮助企业把你的把数据资产完全统一管起来，并且能够加速计算，同时能够跟大模型做好非常充分的交互。</p><p>&nbsp;</p><p>InfoQ：所以您觉得“数据虚拟化引擎”是一个很好的与大模型技术对接的一个工具？</p><p>何昌华：是的，我觉得两者相辅相成。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：关于数巅的另外一个产品“大模型应用框架”的主要特点和优势是什么？它又是如何支持大模型训练和部署的？ &nbsp;</p><p>&nbsp;</p><p>何昌华：大模型的应用框架，本质上我们是想打造一套自己往前能够自我迭代演进的大模型的系统。就像刚刚说的，对大模型而言，我们更多的是用一些开源的大模型做一些微调，我们并没有真正去做一个基础的大模型。但是我们认为一个大模型要在企业内部要成为一个智能的、数字的人，或者帮助做智能角色的助手的角色的话，它一定是需要能自我迭代的。否则的话这个模型很快就会过时，因此它需要能自我迭代。自我迭代的过程在企业内部如何建立起来？以及包括自我迭代需要跟很多业务场景落地，可能需要跟业务场景上的系统打通，它就可能需要连接上各类数据，也包括如何驱动各种自动化的工具来实现这样的业务决策。其中有很多工作，我们这个框架希望能把此类工作沉淀下来后结合我们的大模型以及一个数据虚拟化引擎，这样企业就有了一套完整的，相当于是大模型能够落地，并且能够往前演进，在各个场景下能够快速落地的一套解决方案。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：“大模型应用框架”在大模型应用中有没有优秀实践可以分享一下？</p><p>何昌华：目前已有若干实际案例，这也是我们觉得这样的框架很有意义的原因。我们发现在这个下面，与客户沟通的过程中当我们听到他们有某一诉求时，用这样一套框架去搭配另一个。这种业务场景落地很迅速，基本均以周记，即几周时间你所需的业务场景就可以快速落地了。</p><p>&nbsp;</p><p>其中有若干例子，比如有客户会说我有一堆指标资产，你能否接入它们？同时我拿自然语言说一句话，你就能够把对应的资产给我找到，这个情况下我们就用虚拟化引擎去对接其资产是很容易的，然后去部署一个我们微调过的、开源的大模型，跟着这套框架很快就打造出来这样一个体系，我们叫做智能的指标体系。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您如何看待人工智能和大数据领域的安全和隐私问题？数巅科技有什么措施来保护这些信息？</p><p>&nbsp;</p><p>何昌华：这是一个很好的问题。包括安全隐私类、伦理类问题，这都是现在大家非常担心的，我觉得完全可以理解。</p><p>&nbsp;</p><p>我对这个问题的看法是，首先这肯定是我们需要正视的一个问题，包括我们遇到很多企业就确实存在它们很多数据不能跟公有数据一起去被大模型所训练（的情况），这也是为什么我们觉得有一个企业内部的独立的数据系统可以更好地去管控。</p><p>&nbsp;</p><p>同时包括大模型训练的很多数据，就算在很多数据里面，它也需要做好一些安全性的控制。包括让大模型不要学到很多负面的东西，就像教小孩一样，此类一些区别性的对待，我觉得这都是很关键的一些举措。在我们数巅，我们希望在企业内部能够把数据跟通用大模型的能力隔离开。</p><p>&nbsp;</p><p>大家可以看到，譬如在我们的数据虚拟化引擎里，我们会对数据的权限控制得很严格。我们可以到“行、列”的级别都在控制数据访问权限。当我们去调用大模型的时候，我们可以决定调什么大模型，以及把什么样的数据传给它，这都是有一个非常好的总控的。所以这一块我们是非常关注的一个问题。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：确实是，目前感觉数巅在安全隐私这块也关注的较多。那您觉得除了安全问题以外，大模型在未来的应用中还有哪些风险和挑战？</p><p>&nbsp;</p><p>何昌华：具体来说的话，模型展现出了很强的能力。因为我们可能在一些金融行业，应该说是在对安全要求比较严格的一些领域里面，我们发现大模型可能除了大家经常说的错觉，有时它也会出一些错误的结果，此类问题之外，可能最需要解决的是可控性的问题。大模型目前的输出结果，因为跟你的上下文相关，所以有时你会发现即便是同样的一个问题，你问它两次，它会给出你不同的答案。我觉得在普通的对话中其实关系不大，但当我要做一个很 critical 的业务决策的时候，这种情况我是不能允许的。我问同一个问题，你肯定应该给出的是同样的一个建议。</p><p>&nbsp;</p><p>在我们数巅，我们也在想办法去解决这类问题。事实上我们发现让大模型把这个执行计划分解的越细，对微小的执行计划我们的可控程度越高，我们能够逐步达到一个相对比较可控的利用大模型智能的一个输出。这个反正也在探索的过程中，但这个我也觉得会是在大模型未来发展过程中，它真正会有非常广泛化的应用。打个比喻，像我们评价人一样，要让我觉得一个人靠谱，肯定我每次问他一个问题，他告诉我的是同一个正确答案，而不是说每次都不一样。我觉得大概可以这样来比喻。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：随着技术及企业业务的发展，您觉得未来企业在数据处理方面将面临的最大问题是什么？数巅科技是否已经在准备应对方案？</p><p>&nbsp;</p><p>何昌华：现在企业经过了这么多年的发展，往往他都有了自己的一套系统，或者甚至是不止一套。我们能够看到很多企业从最开始的数据仓库，到基于大数据的解决方案，到后来的实时，到后来的在线分析。在很多企业内部，它是有很多这样的系统一路发展过来，他们面临的很大问题就是这些系统都是割裂的，从而导致存储是冗余的，导致很难操作一些系统。在这样一些企业内部，他们肯定会希望把所有的东西能够统一地融合到同一套体系里去。</p><p>&nbsp;</p><p>所以，数巅的“数据虚拟化引擎”也是希望为企业提供这样一个选择。当然这个融合的方式可以有多种，可以是数据中台的方案，也可以是其他引擎方案。从大模型角度出发，从怎么去更好地支持未来的数据智能的角度，这是我们相对思考的多一些的内容。</p><p>&nbsp;</p><p>对于我们的产品而言，当我把“数据虚拟化引擎”架在企业的很多套体系的上层的时候，你可以去访问到它所有的数据，对于计算速度较慢的数据，引擎会将其预先拖到自己的缓存中，从而加快计算速度；对于计算速度较快的数据，引擎会直接将请求发送给数据源。有一些它计算得非常快的，我就直接会把请求发给他，在我们内部都有这样的一套自动判断的机制。</p><p>&nbsp;</p><p>所以本质上数据虚拟化引擎，相当于是建立起了这样的一个用户访问数据的一个桥梁。只不过以往大家谈到数据虚拟化这一层就会做得非常的薄。那个时候有一个概念叫 data federation，就是数据联盟、联邦查询、数据联邦，但是那个你做得太薄以后，你就很难确保它能够实现高效的访问。我们相当于是把这个再往前又迈了一步，做的有自己的缓存策略，有自己的智能优化的手段，有自己智能的内核在那里控制，从而让它做到极为高效地来解决这个问题。如果数据虚拟化引擎在一个企业内部能够将其规模充分利用起来，那它可以帮助企业做数据诊断的工作。但同时它到第二个阶段，直接用我们的引擎去访问数据的时候，它也可以帮企业统一它所有的数据体系，提供一个统一的、给业务数据的一个视图。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：您理想中企业数据治理的未来是怎样的？数巅科技的下一步技术迭代方向是什么？您是否有信心达到“理想中”的未来？信心来源于哪里？</p><p>&nbsp;</p><p>何昌华：肯定是有信心的，正是因为觉得这个理想有实现的可能才会去尝试。我们的理想是在一个企业内部，通过我们的产品能够充分地把数据能够管好、用好，能够跟大模型深度的协同起来，为企业提供智能的业务决策能力。</p><p>&nbsp;</p><p>我觉得这里面是一个很大的体系，在这其中，我们可能会在数据方面提供更多的价值。有一些公司可能是在做纯粹的大模型，这两者是互补与协作的一个关系。</p><p>&nbsp;</p><p>在数据这一块，我们从最开始一直有一个比较朴素的理想，我们那时候在讲数据自动驾驶这件事，就是说我们认为在一个企业内部，当用户、业务要用数据的时候，他只应该关注这个数据究竟是什么意思？比如说我要用每个人的消费额度，或者我要用一个什么样的数，这个东西我觉得是他应该关注的，其他的都应该是自动把它解决掉的，我们把它叫“数据的自动驾驶”。像今天也有一位嘉宾老师分享到了，比如说自动驾驶从 L1 到 L5，以前的数据我觉得有点相当于是 L1、L2 这样的，或者最多到 L3 这样的。我们可能有一些引擎的能力越来越强，但是用户还是需要去关注所有的事情，我们希望做到的就是用户完全不感知底下的东西，像“我坐上车只要报一个目的地，这个车就会自动开到那”。我们目前在不停地摸索这个过程。</p>",
    "publish_time": "2023-07-27 11:09:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "8年了，Transformer注意力机制一直有Bug？",
    "url": "https://www.infoq.cn/article/tu8SRD5u1ecTQ40lvajH",
    "summary": "<p></p><blockquote>注意力机制是一个构建网络的思路，也是 Transformer 模型的核心。</blockquote><p></p><p>&nbsp;</p><p>注意力是人类认知功能的重要组成部分，指人的心理活动对外界一定事物的指向和集中。在大模型中也有注意力机制。比如神经网络语言模型面对一个由 n 个单词组成的句子时，不同位置的单词，重要性是不一样的。因此，需要让模型“注意”到那些相对更加重要的单词，这种方式称之为注意力机制，也称作 Attention 机制。</p><p>&nbsp;</p><p>简单来说，<a href=\"https://time.geekbang.org/column/article/461691\">注意力机制</a>\"要做的事情就是：找到最重要的关键内容。它对网络中的输入（或者中间层）的不同位置，给予了不同的注意力或者权重，然后再通过学习，网络就可以逐渐知道哪些是重点，哪些是可以舍弃的内容了。</p><p>&nbsp;</p><p>2017 年，谷歌发布《Attention is All You Need》，这也是关于注意力机制最经典的论文。在这篇论文中，谷歌提出了一个新型神经网络架构——Transformer，它完全基于注意力机制，省去了循环层和卷积层。并在 2014 年 WMT 英德翻译任务中达到 28.4 BLEU，比现有的最佳结果(包括集成部分)提高了 2 个 BLEU 以上。结果表明，Transformer 可以很好地将其推广到其他任务，不论是大量还是有限的训练数据。</p><p>&nbsp;</p><p>2018 年，OpenAI 发布了 GPT，谷歌发布了 BERT，两大模型都是基于 Transformer 结构构建的——GPT 是基于 Transformer Decoder 结构和无监督预训练方法实现的生成式预训练语言模型，BERT 是基于 Transformer Encoder 结构的预训练语言模型。</p><p>&nbsp;</p><p>随着近几年 AIGC 技术的爆火以及 ChatGPT 成为现象级应用，Transformer 正以井喷的势头快速发展，基于 Transformer 架构打造的模型数量激增，并将&nbsp;Transformer 的热度推上了新的高峰。</p><p>&nbsp;</p><p>然而近日，Eppo 初创公司的工程师 Evan Miller 在 Twitter 上表示，他发现注意力机制有一个存在了 8 年的 Bug，所有 Transformer 模型（GPT、LLaMA 等）都会受到影响。虽然研究人员上个月隔离了该错误，但他们忽视了一个简单的解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab995ca58d8aa47f479353322887e562.png\" /></p><p></p><p>Miller 在一篇博客中详细阐述了该 Bug，并提出了修复方案。Miller 表示，他在跟 Pytorch 和 biblatex 的一连串交锋中败下阵来。虽然如此，他还是想向大家解释当前这代 AI 模型如何在关键位置出现差一错误，并导致每一个 Transformer 模型都难以压缩和部署。</p><p>&nbsp;</p><p>以下是 Mille 的博客原文，经编译。</p><p></p><h2>这个Bug有多重要？</h2><p></p><p>&nbsp;</p><p>大家发现这个公式中的差一错误了吗？</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/80ad4e6dc46ce5f6f143bc12e24416ec.png\" /></p><p></p><p>首先，我们来聊聊为什么这个差一错误这么重要。ChatGPT的效果不是挺好吗，老兄，你在闹什么呢？其实我第一次察觉有问题，是在认真阅读关于量化的研究论文时。大语言模型的探索者们正尝试将其塞进Mac Mini、Raspberry Pi，甚至是越了狱的家用智能恒温器。</p><p>&nbsp;</p><p>但AI从业者都知道，如今限制最大的因素就是RAM容量。无论是在云端还是边缘，RAM占用量越小、能做的事情就越多。大语言模型拥有几十亿的权重参数，如果我们能想办法帮这些参数成功瘦身，那完全可以写出更好的诗歌、生成更好的综述文章，或者加快人类末日到来的脚步——具体要看大家怎么应用了。</p><p>&nbsp;</p><p>RAM负责存储信息。这好像是废话，但大家先别急。信息属于负对数概率，代表我们需要用多少个bit来存储事物。如果数字流具备高度可预测性，例如总是处于有限的范围之内，那么我们要使用的存储bit量就可以更少。而如果一串数字毫无可预测性，包括偶尔甚至会出现极其巨大的数字，那我们就需要更多位二进制数字对其做编码。</p><p>&nbsp;</p><p>而这正是大语言模型的真实情况。出于种种或明确、或含糊的原因，Transformer模型会包含这些异常权重，用本不必存在的臃肿体态来应对那些发生概率极低、极低的情况。但奇怪的是，似乎没人在意这件事：不对吧，这类罕见异常值与我们之前所认为的、关于构建良好神经网络的一切知识都背道而驰。关于这些异常值，人们已经写过不少论文，也在制定各种“瘦身”方案用更少的0和1对其做编码。顺带一提，直接使用比例和偏差整量化会导致性能大幅下降，所以还得想点更好的办法。</p><p>&nbsp;</p><p>目前关于这个问题的最佳分析，来自高通AI研究团队发表的论文《量化Transformers：通过让注意力头什么都不做来消除离群值》（<a href=\"https://arxiv.org/abs/2306.12929\">https://arxiv.org/abs/2306.12929</a>\"）。作者将这些离群值追溯到了注意力机制中的Softmax函数，之前没人认为这个看似无辜的指数器居然拥有如此野蛮的峰度。其实研究人员马上就要发现其中的差一错误了，但此刻他们可能是在意大利度假，否则怎么就没人回复我发过去的邮件提议呢？于是乎，我只能用最传统的方式向国际学术界发出呼吁。</p><p>&nbsp;</p><p>如果大家读过链接中的论文，请直接忽略他们给出的建议。我知道这话讲得不客气，但裁剪之后的Softmax带有一个旋转的零梯度，而他们的门控注意力建议虽然可行，却要靠引入数百万个新参数来解决一个实质上仅是增量失败的问题。这里明显有个简单且直观的解决方案，但从我接触过的相关讨论来看，似乎没人想到要尝试。</p><p>&nbsp;</p><p>好吧，关于这个愚蠢的错误，我已经介绍得够多了。现在咱们来看看Softmax函数，还有它在注意力机制里惹出了怎样的麻烦。</p><p></p><h2>Softmax惹出了什么麻烦？</h2><p></p><p>&nbsp;</p><p>要想明确解释这个Bug，大家先得真正理解注意力机制是干什么的。大多数数值Bug源自程序员把方程给弄错了。但如果错误不出在代码上、而是出在数学上时，我们就必须先搞清当前方程的来源和它的预期效果，然后才能加以修复。</p><p>&nbsp;</p><p>为此，我不得不认真阅读了大约50篇arXiV文章才逐渐理清思路。关于这个问题的讨论，我们先从所谓“输入嵌入”入手，这是一个浮点向量，表示输入字符串中的一个单词。</p><p>&nbsp;</p><p>这个向量似乎正随着模型的逐年发展而越来越长。例如，Meta最近的LlaMA 2模型使用的嵌入向量长度为3204，其半精度浮点计算结果为6&nbsp;KB以上。是的，这仅仅对应词汇表中的一个单词，而词汇表通常要包含3到5万个单词条目。</p><p>&nbsp;</p><p>现在，如果大家跟我一样是对内存极为吝惜的C程序员，那肯定会考虑：为什么这帮AI蠢货非要用6&nbsp;KB空间来表达这种双字节信息？如果总词汇量不超过216=65384，那我们只需要16个bit就能表示一个条目了，对吧？</p><p>&nbsp;</p><p>确实，人家Transformer其实也是这么干的：它将输入向量转换成相同大小的输出向量，而最终的6&nbsp;KB输出向量会对预测当前token之后的下一token所需要的全部内容进行编码。</p><p>&nbsp;</p><p>Transformer每个层的工作，其实就是把信息添加到原始单字向量当中。这就是残差连接的意义所在：整个注意力机制只是向原始的两个字节的信息添加补充材料，通过分析更多上下文来证明当前文本中的“pupil”是指某位学生、而不该直译为瞳孔。把注意力机制重复个几十次，模型就掌握了英语及其承载的一切广泛内容。</p><p>&nbsp;</p><p>现在，Transformer会将输出向量乘以一个矩形矩阵，再将生成的词汇长度向量填充到Softmax当中，最后把这些指数输出视为下一个token概率。这确有合理性，但大家都知道其中仍有问题。一切广受尊重的模型都承认这种输出概率在所偏差，需要在具体实现中通过采样机制来隐藏Softmax过度代表低概率的事实。</p><p>&nbsp;</p><p>引入采样确实可行。毕竟输出步骤中的Softmax将为我们提供词汇表内每个单词的梯度，在没有更好结果可用的情况下，确实可以先遵循拿来主义。</p><p>&nbsp;</p><p>但我想说的是，就没必要非把不相干的东西搅和在一起。Transformer的输出Softmax与注意力机制的内部Softmax用途完全不同，所以我们最好别跟后者硬靠，或者至少该对分母做点科学调整⛱️。</p><p>&nbsp;</p><p></p><h4>Softmax到底是什么？</h4><p></p><p>&nbsp;</p><p>那么，Softmax到底是什么？它源自统计力学，代表一种根据能级预测状态分布的方法：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/43dd5aea2f3e5637e92082b59acab232.png\" /></p><p></p><p>在掌握了它之后，经济学家们意识到，如果人们的线性效用函数中的噪声项恰好遵循Gumbel分布，那么某人选择某个条目的概率就将与效用输入的指数成正比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85c5388f521b8a5418f5411f3cc13d71.png\" /></p><p></p><p>这就为Softmax在多项逻辑函数中赋予了生命力。我在读研究生时曾亲手推导过这个Hessian矩阵，想通过编码用线性固定效应将其运行在GPU上。据我所知，之前和之后都没人蠢到做这样的尝试。说了这么多，就是想告诉大家我跟Softmax函数还真就有那么点缘分。</p><p>&nbsp;</p><p>Softmax其实是一种“作弊码”，能够将实数值映射到总和为1的概率上。它在物理学中的效果相当好，在经济学上有点虚；而在机器学习领域，它已经成为离散选择当中的有效手段。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/1972df915a29a550958d7fe680b8ca7b.png\" /></p><p>这也是Softmax的核心机制：它要求在各种可行方案间做出选择，包括粒子选择能级状态、也包括消费者选择心仪的汽车。也就是说，如果一个Softmax机制根本不打算做选择，那就应该做出修改，否则Softmax一定会在处理真实数据时发生扭曲。</p><p>&nbsp;</p><p>回到大语言模型这个话题上，此类扭曲的一种实际表现，就是对非语义token（比如逗号）进行重点加权，而这就变成了那些难以压缩的离群值，致使模型很难被部署在边缘场景当中。高通AI研究院的报告发现，大语言模型中有超过97%的异常激活发生在空白和标点位置。</p><p></p><h4>具体是怎么出错的？</h4><p></p><p>&nbsp;</p><p>下面我们深入研究一下Softmax在注意力机制中如何起效，看看是哪里出了问题。仍然是这个等式：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1c/1c7bc0adac294169e395e8f4b6be6f73.png\" /></p><p></p><p>&nbsp;分解一下：在仅解码器模型中（即ChatGPT之后的所有模型），Q、K和V都来自同一输入序列。虽然它们会在期间被以不同方式投影，所以彼此间并不相同，但在每一层中，它们都始于相同的已注释（已添加）嵌入向量。</p><p>&nbsp;</p><p>现在，𝑄𝐾𝑇在寻找不同位置上token（嵌入）向量之间的相关性，这实际上会构建一个相关性（点积按1/√𝑑‾‾缩放）值的方形矩阵，其中每行和每列对应一个token位置。方形矩阵中的每一行都经过Softmax的处理，得出概率以作为V矩阵中各值向量的混合函数。混合概率后的V矩阵将被添加至输入向量中，再被传递至神经网络内做进一步处理。</p><p>&nbsp;</p><p>多头注意力会在每个层中都经历这个过程，完成多次处理。它基本上就是将嵌入向量划分成几个部分，每个头使用整个微量中的信息来注释输入向量中的一个（不重叠）部分。简单来讲，就是Head 1向Segment 1添加信息，Head 2向Segment 2添加信息，依此类推。</p><p>&nbsp;</p><p>但使用Softmax的问题在于，即使没有什么信息可以添加到输出向量当中，它也会迫使各注意力头进行注释。所以在离散选择中使用Softmax效果拔群，但在可选注释（即输入到加法中）则不太理想。其中，多头注意力又会进一步加剧问题，这是因为专用头往往比通用头更想要“通过”，反而带来了不必要的噪声。</p><p>&nbsp;</p><p>说到这里，是不是该直接让Softmax下课走人？当然不至于，它在大多数情况下都运行良好，唯一的小Bug也只是让注意力头没法“乖乖闭嘴”。所以我提出了一个很小也很直观的调整建议，自从注意力机制在2014年被发明出来，这个办法就一直在每个人的眼皮子底下。</p><p>大家准备好了吗？</p><p></p><h2>修复方案</h2><p></p><p>&nbsp;</p><p>现在，让我们有请睽违已久、经过改造的Softmax Super-Mod闪亮登场：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/a3/a363e918e17944b2f361779937523bf9.png\" /></p><p></p><p>感觉也没什么了不起？是的，我只是在分母处加了个1。这就是让整个向量朝着零挪动了一点点，而这将在注意力之后的归一化过程中得到补偿。</p><p>&nbsp;</p><p>修改后的主要区别在于负极限，当x中的条目明显小于零且模型试图回避一次注释时，其表现将与原始Softmax的行为有所不同。先来看原始Softmax：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/533ada43e23552d15cf59b539ac28e36.png\" /></p><p></p><p>以下则是经过改进的新Softmax 1：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d523b13f3dbb74472b20c7bc63a45de.png\" /></p><p></p><p>&nbsp;</p><p>原始Softmax将始终发出相同的总权重；Softmax 1看似区别不大，但在负半轴中设有一处“逃生通道”。再次强调，这里的关键在于数学问题，而非数值问题。进一步提高精度并不能拯救Softmax，所有transformers均受此影响。</p><p>&nbsp;</p><p>Softmax 1还有其他一些特性。因为它的导数是正值，所以我们始终拥有非零梯度；它的和在0到1之间，因此输出不会失控。该函数还具备以下属性，即输出向量中的相对值不变：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5e/5eb604d00133a9589147d739854cbf62.png\" /></p><p></p><p>&nbsp;</p><p>最初我本想把这个函数命名为Ghostmax，因为这里的x中有个额外的零值条目（即exp(0)=1），而V矩阵中有一个会衰减结果的零向量。但别担心，这些不是重点。</p><p></p><p>虽然Softmax 1看似平平无奇，但我有99.44%的把握相信它能解决离群反馈循环的量化问题。</p><p>我们将这种改进机制称为QuietAttention，因为它允许注意力头保持安静：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a0/a05dcbe43e6ca6187b68b19de87a39c2.png\" /></p><p></p><p>大家很快就可以测试起来了。只要在每个输入上下文中添加一个零向量作为前缀，并确保所选择的神经网络不会添加任何偏差（包括位置编码），那么零向量应该就能原封不动通过，并在每个后续的Softmax分母中都加上一个单位。这样，也就不用再纠结什么梯度代码了。</p><p>&nbsp;</p><p>但这种方法要求重新训练模型，所以请先别急着在Raspberry Pi上尝试。希望大家能分享关于权重峰度和激活无穷范数多轮运行后的实际影响，我想把这些数字整理成表格，供arXiV上发表的新论文使用。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.evanmiller.org/attention-is-off-by-one.html\">https://www.evanmiller.org/attention-is-off-by-one.html</a>\"</p>",
    "publish_time": "2023-07-27 11:13:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "关于 To B 业务增长，我们是这么思考的｜ArchSummit 闭门会",
    "url": "https://www.infoq.cn/article/kyQ3ioi751hAmSmJ1XYR",
    "summary": "<p>在经济不佳的当前环境，ToB&nbsp;成长和创新的重要性日益突出。在这样的背景下，<a href=\"https://archsummit.infoq.cn/202307/shenzhen/\">ArchSummit架构师峰会（深圳站）</a>\"举行了一场专门针对&nbsp;ToB&nbsp;增长的技术交流会。在此活动中，我们探讨了各个发展阶的&nbsp;ToB&nbsp;企业所面临的特定增长难题，如何找寻独特的创新解决思路和策略，包括市场定位、产品定制和客户成功管理等。以下为精彩分享摘要～</p><p></p><h4>精彩分享1</h4><p></p><p>首先，对于产品启动阶段，特别是在建立信任方面，我们需要自己刷脸去找客户，或者依靠销售团队与客户直接沟通，以打出标杆并建立信心。其次，产品的品类和市场也影响着SLG（Sales-led&nbsp;Growth）和PLG（Product-led&nbsp;Growth）的属性。SLG更适用于需要多方协同的产品，而PLG则适用于小规模、单一职能的产品。第三点是，随着参与角色和决策链的增多，需要更多的沟通，而小客户则更容易通过PLG方式使用产品。最后，针对协同类产品，虽然它们可能很有用，但可能局限在少数人使用，因此PLG成分可能更高，而相较之下，对于小客户来说，不需要进行过多的沟通。</p><p></p><h4>精彩分享2</h4><p></p><p>我曾创办了一家公司，我们专注于社交媒体舆情分析，并使用自然语言处理（NLP）技术。我们的ToB业务就是通过搜索引擎，将信息整合成报告并尝试在大公司中进行销售，初期我们甚至提供免费报告，逐渐地客户认识到报告的价值，开始为之付费。经过一段时间的技术提升，我们成功将原先需要5个小时才能完成的报告，最后缩短到30分钟，最后甚至5分钟。</p><p></p><p>当时的CEO告诉我，虽然我们可以做一个咨询公司，但他更希望我们能成为一个SaaS模型的公司，后来，我们做到了，成为全球最好的舆情分析公司之一。但是，我要说的是，在SaaS模型中，你必须确保你的用户稳固，否则你的SaaS模型就不能持续下去。我们成功地稳定了市场，成为客户情报调查的市场领先者。我们的SaaS模型已经消化了所有的信息，所有的信息都存储在我们的数据库里。只需要按一个按钮，各种报告、报表就可以出来。</p><p></p><p>但是，我并不认为这样的成功故事可以在中国复现。因为中国的市场与美国不同，中国的客户期望得到全套的专业服务，而非仅仅是一个平台。这种全面性的需求远超过我们在美国的最好客户的支付水平。</p><p></p><h4>精彩分享3</h4><p></p><p>JIRA市值曾达到1000亿美元，现在已降至几百亿，但我们必须承认其影响力。有一个值得我们思考的地方：JIRA在其产品的发展中，很少有直接的竞争对手。我们观察到JIRA将许多SLG的工作推给其代理商来做，尽管看起来公司并没有参与其中，但实际上有许多代理商在做相关的工作。这也与其创始人的风格有关，他们更喜欢简单、标准化的产品，不愿意花费大量的精力在复杂的事物上</p><p></p><p>PLG和SLG都是需要做的，公司可以选择偏重于其中一个。如果我们希望实现更大的增长，可能就需要自己管理相关的成本和方式。对于大型企业，他们通常会选择通过代理商来进行销售。在项目管理这个领域，因为不容易清晰地定义价值，所以更倾向于使用代理商。</p><p></p><p>总的来说，我认为PLG在任何阶段都可以实施，但现在还没有找到完全依靠它就可以支撑收入的产品。SLG在早期启动时对自己有帮助，而且看起来无法避免做大客户，特别是在有多角色协作、多角色使用的情况下。</p><p></p><h4>精彩分享4</h4><p></p><p>我认为目前金融行业在技术和交付能力上还存在许多问题。尽管我们已经在努力奋斗了20多年，但金融客户的生产系统和核心生产系统中仍有90%以上的部分使用过时的技术，这使得推动金融行业的现代化进程变得缓慢且困难。</p><p></p><p>要解决金融行业的现代化和增长问题，除了技术的持续提升外，企业还需要重新定位市场，寻找更有效的增长模式。当然我们也面临着金融行业发展中的几个关键问题。</p><p></p><p>首先，我们渴望在产品中实现授权收费，并获得收入，但目前还没有实现这一目标。其次，我们的企业和产品定位存在问题，导致增长困难。在解决增长问题时，我们尝试着从大的市场转向小的市场，但在实施中遇到了困难。后来，我们的老板借鉴了制造业的概念，希望我们能像制造螺丝钉一样，做一项特定的事情并覆盖全球。因此，我们决定将银网云平台、银管平台和运维平台进行虚拟化，并尝试借鉴他人的成功经验。</p><p></p><p>我们的发展经验表明，核心的产品定位对于组织和技术的变革非常重要，虽然并不是唯一解决办法，但是在应对增长难题时至关重要。我们曾经在云龙初期带领团队开发了类似飞书和钉钉的产品，但后来发现难以推广。后来逐渐精简产品，保留了企业网盘功能，并将其重新包装为政务云盘。这一举措取得了成功，在广东省范围内取得了广泛应用，并有望未来一到两年覆盖整个广东省。</p><p></p><h4>精彩分享5</h4><p></p><p>定位非常重要，我们企业从初创走到现在，虽然我们还未进入成熟期，但是已经非常突出了。首先，我们公司的定位是商用清洁机器人领域第一名。我注意到机器人行业在早期，大多数产品都是展示型的。虽然它们能动，但其实并不能真正实现日常工作的需要。</p><p></p><p>我们是第一个从展示机器人中走出来的公司，我们决定将产品做好，然后，我们开始遇到了各种实际问题，比如环境复杂，机器人不能识别和处理各种不可预见的障碍，甚至像吸管和纸片这样的小东西都会给我们的机器人带来困扰。</p><p></p><p>我们公司在初创期选择了一个非常有意思的技能，即通过算法大幅提高清洁效率。我们优化过的机器人一小时能清理&nbsp;800&nbsp;至&nbsp;1200&nbsp;平方米，这远远超过人的效率。</p><p></p><p>当我们进入成长期，我们的机器人已经可以在工业场景中连续工作几天。但随着扩大规模，我们发现维护成本也等比例上升。尤其是当客户增加了机器人的使用量后，故障率上升，但经过我们的女里，这些问题都被解决了。</p><p></p><p>最后，我们发现这种成本降低并不是由于我们的机器人替代了人工，而是因为机器人的协助。我们的系统能够在物业经理巡逻时，发现需要清洁的房间并报告，然后物业经理就可以继续他们的工作，不需要再跟进。这就导致了一个惊人的结果，原来需要三个物业经理的地方，现在只需要一个，大大节省了成本。</p><p></p><p>我们也思考过我们的机器人系统是否应以SaaS模式出售。当我们的机器人销售出去后，我们是否应该将服务以SaaS模式随机器人一并销售。最后，我们决定不采用SaaS模式。</p><p></p><p>我们的视角是，既然硬件已经销售出去，我们应该在硬件内增加增值服务，客户会很容易接受并愿意支付。买固定资产的决策非常简单。我评估硬件的价值，再考虑包含的增值服务，我们可以接受某种程度的议价决策。采购和&nbsp;IT&nbsp;部门可以快速做出决策，因为他们知道设备可以立即使用。</p><p></p><p>但是，如果我们让业务部门评估SaaS的决策，他们会要求技术部门一起评估，需要考虑许多复杂的因素。我们的机器人销售出去时，我们提供增值服务的方式是，你可以支付额外的费用来开通这个服务。我们不会单独售卖SaaS服务。</p><p></p><p>总的来说，定位非常重要。如果你在一个领域是第一，就有很大的说服力。我们不想与整个行业竞争，我们只是抱着我们的第一名产品出发，然后直接销售。因此，我们认为在初创期和成长期，我们需要找到一个定位。当产品成熟，并普遍被使用时，如果客户不使用我们的产品，反而是他们的问题。</p><p></p><h4>精彩分享6</h4><p></p><p>就部署还是&nbsp;SaaS&nbsp;会增长得更快而言，我是这样考虑的。在讨论企业服务的软件时，我们会把它们分成三类：一是与核心产品有关的，即与核心生产系统有关的；二是辅助生产系统的；三是平常工作、生活以及工作流程协同的工具。对于这三类，越接近外圈的，越类似于传统的财务系统和CIM系统。例如，我们的产品就在这个最外面的圈子。我们不直接帮助客户创造收益，而是间接地提供帮助。而像虚拟化这种技术，就不同了，它至少处于第一或第二个圈子，与生产系统有直接关系。但是总体来说，无论在中国还是全球，越大的企业就会有越多的合规、安全和数据等问题需要处理。我觉得大部分的这个软件产品，特别是像现在我觉得就售部署的场景要求都会很高，特别是大做中大客，在中国做小客基本上做不出来了。</p><p></p><h4>精彩分享7</h4><p></p><p>关于处理用户需求成本太高的问题，我们采取的价格区分策略，这样的处理方式是基于实际情况的需求。比如，国内的一些公司等向我们咨询是否支持某项服务时，我们坦然回答不支持。而当海外分销商咨询同样的问题，我们不能直接说不行，因为他们有大量的潜在客户，他们是全球医药行业的头部。</p><p></p><p>这里逻辑是，一旦我们满足了他们的需求，其他的医疗机构也会效仿，因此我们必须满足他们。我们会采取所有必要的措施来支持他们。而在中国，要实现如此高的要求，特别是在大中型客户中，对小客户的要求基本无法满足。然而，我们为提供服务的理由很简单，因为我们能以三倍的价格出售我们的产品。</p><p></p><p></p><h4>精彩分享8</h4><p></p><p>现在我们的大环境确实面临挑战，我认为整合是一个很好的方式，无论是在行业竞争中还是产品推广中。举个例子，我在前端实施中使用了代理商的推广策略。虽然有更便宜的选项，但我选择了代理商的产品，因为他们提供了更多的资源和空间，能够将产品推广得更广更远。这类似于政务网盘的概念，通过合作伙伴的推广，整合各方面的资源，让产品得到倍数的放大效应。在我们的行业中，我认为这是一个关键的点，通过整合资源来实现产品的成功推广。</p><p></p><h4>精彩分享9</h4><p></p><p>在我的观点中，如果客户对我们所提供的服务的核心竞争力不感兴趣，那说明我们的服务对他们来说并不是必需的。即使我们在产品和服务方面做得再好，在客户看来，这也是我们应有的水准。</p><p></p><p>在我们的内部，我们都会有营造一个故事。我们会告诉产品经理们这个故事，让用户了解我们的理念和方法。这也是我们项目成功的关键。我们要能根据客户的需求，给他们提供合适的产品和服务。</p><p></p><p>另外，我们做产品时，经常会被客户的需求所驱动。尽管有时候这些需求看似不合理，但我们还是会尽力满足他们。在这个过程中，我们也面临着一些问题。例如，我们需要判断一个项目的性质，我们需要为客户提供哪种服务。</p><p></p><p>最后，搞定大客户相当重要，在我们的经验中，我们发现在金融行业有很大的增长。我们有大约700个目标客户，目前已经有一百多个。我们的主要问题是如何解决客户的选择性问题。在过去，他们可能会选择华为、华商，但现在我们也有我们的优势，我们有我们的产品定位，我们用全公司之力把头部银行等一些大型企业纳入我们的服务范围。一旦客户对我们没有选择性的顾虑，剩下的就是销售操作、服务以及产品的可靠性等问题。这些都不是主要问题。</p><p></p><h4>精彩分享10</h4><p></p><p>在我的经验中，了解并满足客户的需求至关重要。一次，我向一位客户解释了我们产品的各项优势，他提出了一些关于安全稳定性的问题。我很奇怪，这些都是我们在过去一年解决过的问题，然后，我向他展示了我们最新的解决方案。但是后来发现，他们并不买账。最后我才了解到要真正满足他的需求，我了解到他在向上级汇报时的需求和困扰。</p><p></p><p>客户跟我关系比较好，坦白告诉我，恰恰是我们我们的产品稳定性强，五六年不需要更换，这让他在第二年没有什么可以向老板汇报的。于是，给了他新的解决方案。从这个案例中，我明白了要成功推销&nbsp;To&nbsp;B&nbsp;产品，我需要站在客户的立场上，解决他们的关键问题。这些问题可能并不会直接向销售人员或解决方案提供者表达出来，但是一旦找到并解决了这些问题，客户就很容易被搞定。</p><p></p><h4>精彩分享11</h4><p></p><p>在我看来，我认为想要在国产领域的立足，除了研发之外，确保我们的产品在目录上有位置至关重要，没有目录位置，没人会来帮我们。这是门槛，而且是硬门槛。举个例子，如果我们有300人研发预算，如果这还不够，我认为我们可以削减50个人，然后把这些资源投入到进入目录，这是我们必须要做的。一旦进入目录，我们可以从一些边缘的创新项目开始，比如销售方面的改进，使我们的应用一体化交付，这是好的，没人会否认在目录里的合规性。</p><p></p><h4>精彩分享12</h4><p></p><p>To&nbsp;B发展很困难，有一个关键原因就是定位，这其中就包含了&nbsp;SLG&nbsp;还是&nbsp;PLG，其实在这个AI行当里面还真需要&nbsp;SLG，并不是个&nbsp;PLG&nbsp;可以搞定的。CEO、CTO都必须亲自走出去，与客户进行深入交流，推进产品。</p><p></p><p>我们也曾尝试了多个方向，但真正能够稳定带来现金收入的成功案例并不多。当然也有例外，也有客户主动找过来的。例如，某市公安局就对我们的技术有强烈需求，他们需要能够在海量人群中快速找到可疑人员的技术。我们在这个领域取得突破后，客户对我们的产品非常满意，这使得我们的业绩大幅增长。这就是一个PLG案例。</p><p></p><h4>活动推荐：</h4><p></p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">FCon全球金融科技大会（2023·上海站）</a>\"是极客邦科技旗下 InfoQ 中国团队推出的面向金融行业高端技术管理者、技术专家的会议，50%参会者拥有 8 年及以上工作经验。</p><p></p><p>FCon 聚焦当前金融行业遇到的问题，围绕金融企业在数字化转型过程中的痛点，例如数据治理，智能化、数字化风控，数字化投研，数字化营销，IT 技术能力等方向，邀请国内外金融企业，来分享人工智能、区块链、大模型、大数据、数字货币等新一代信息技术实践话题，帮助听众解决技术和业务上的问题，提升技术能力。欢迎大家报名参会，<a href=\"https://fcon.infoq.cn/2023/shanghai?utm_source=infoq&amp;utm_medium=conference\">详细信息可点击这里查看</a>\"</p>",
    "publish_time": "2023-07-27 11:17:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于鲲鹏+昇腾，华鲲振宇重磅发布算力创新产品及解决方案",
    "url": "https://www.infoq.cn/article/Nk9DyErUiAA5uH80qUU0",
    "summary": "<p></p><p>2023年7月26日，<a href=\"https://www.infoq.cn/article/iCuurCMJCvofpfylbGpE\">华鲲振宇</a>\"在北京举办产品发布会，以“中华鲲鹏振兴寰宇，昇腾万里智领未来”为主题，分享基于“鲲鹏+昇腾”的算力创新，发布全新一代算力基础设施与解决方案。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/14/dd/14e0b4653e4c39665318e1e3ea64f4dd.jpg\" /></p><p></p><p>此次发布会汇聚产业精英、客户和生态伙伴，共同探讨满足数字时代多样性算力需求的未来发展方向。北京市朝阳区人民政府党组成员、副区长舒毕磊、华为计算产品线副总裁田华、华鲲振宇总裁万诤等领导出席大会并致辞。</p><p></p><h4>深入场景发布四大“天”系产品 全栈创新敢于亮剑</h4><p></p><p></p><p>擎天之力铸坚实算力底座——“天擎”系列高规格通用计算平台</p><p></p><p>随着应用与场景驱动需求激增，对<a href=\"https://xie.infoq.cn/article/372faa42c86908abae0360a08\">算力基础设施</a>\"提出了更多差异化的需求，本次大会，华鲲振宇正式揭开基于鲲鹏创新架构算力产品的神秘面纱，华鲲振宇研发总经理赵彦钧携华为鲲鹏计算业务副总裁马银川正式发布“天擎”系列高规格通用计算产品，最大支持16块NVMe SSD，创新蝶形IO模组设计，最多可支持11个PCIE 标准插槽，规格提升了37%；率先支持9张 PCIe X8的 NPU/GPU加速卡，或两张全高全长双宽的训练卡，整机AI计算性能提升15%以上，各项参数直指鲲鹏最高规格，为千行百业提供高密性能的算力应用平台，已规模服务于运营商、互联网、金融等行业，满足客户在不同部署场景、不同算力、不同存力需求下的计算任务，让每一次计算都能心有所想、算有所成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/52/52d019f2cd0760e857efad515f809a84.png\" /></p><p></p><p>深耕智算助力大模型普惠AI——“天智”系列高密AI算力平台</p><p></p><p>伴随大模型带来的生成式AI突破，人工智能技术正在跨越重要的历史拐点，华鲲振宇也早已有了筹谋，基于昇腾AI计算，在多个典型应用场景进行了深入创新和升级，积极拥抱人工智能，在大模型训练场景下，推出基于昇腾的“天智”系列高密AI算力服务器——新一代训推一体服务器AT800 A2，底层技术的突破为大模型的训练推理提供极致的算力输出，助推国内大模型应用广泛落地，进一步普惠AI。基于天智训练服务器，华鲲振宇已为“蓉城 • 夔（kuí）牛”短临预测模型提供稳定“算力底座”，以成都智算中心300P算力为基础，实现精细化的气象预报服务。备受关注的第31届世界大学生夏季运动会（成都大运会）即将开赛，\"夔牛\"可作为赛场的一匹“黑马”，助力大运会期间的天气预测，把前沿的“黑科技”运用到大运会“智慧气象”服务中，让AI大模型应用于具体场景。</p><p></p><p>以恒致远令融合算力触手可及——“天恒”超融合移动平台解决方案</p><p></p><p>在<a href=\"https://xie.infoq.cn/article/376502f80af0b1d2c672a8e96\">边缘计算</a>\"场景中，客户对计算设备部署的便利性、可移动性以及异构算力融合方面提出了更高的要求。华鲲振宇产品总经理李锐携手华为昇腾计算业务副总裁刘鑫发布业界首创鲲鹏+昇腾“天恒”超融合移动平台，应对应急救援、海上勘测、地质勘探和航拍合成等各种极端恶劣的环境场景， 具备高可靠性和稳定性，轻松完成信号处理、全景视频融合、IoT数据采集与处理、目标检测等功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2fbb31cb233ae282c4203a34f6bfad7c.png\" /></p><p></p><p>技术创新让数据中心极致节能——“天极”系列液冷解决方案</p><p></p><p>在国家双碳战略背景下，数据中心算力规模目前持续高速增长，能耗随之急剧扩张，以应用液冷技术为代表的绿色数据中心，成为新基建、“东数西算”的算力底座，如何降低数据中心的能耗成为整个行业技术创新的焦点。华鲲振宇超前洞察，携手华为集群计算业务副总裁王振华发布双液冷解决方案——天极HC8000冷板式液冷和天极HC6000浸没式液冷，挑战PUE小于1.1，HC6000更是基于鲲鹏+昇腾全球首发的浸没式液冷服务器，采用国产原生冷却液，具有极致节能、极稳运行、轻松运维等极致优势，为关键业务场景提供绿色低碳的强劲算力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/519c99276fb37bf7582bdb30fd01b21f.png\" /></p><p></p><p></p><h4>携手伙伴共襄成长 合力向上共建繁荣</h4><p></p><p></p><p>赋予算力时代价值的，并不单纯是算力本身，而是产业上下游的合作伙伴，在本次产品发布会上，华为中国区运营商业务计算解决方案总经理张立鹏、华鲲振宇副总裁宋璇携手20+伙伴共同发布基于华鲲振宇产品的解决方案，持续深化合作，发挥各自优势，深入场景，充分释放出“鲲鹏+昇腾”澎湃算力，给客户持续提供更具价值的解决方案与服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d402fc25cec335efd6b75782a2d5e32c.png\" /></p><p></p><p>此外，华为中国区政企部件销售部总经理李国涛和华鲲振宇高级副总裁于巍波为10+产品合作伙伴正式授牌，感谢一直以来相伴同行。面对风高浪急的国际环境，树木相依偎而生长，星辰因辉映而璀璨，华鲲振宇秉承合作共赢的理念，与关键部件供应伙伴签署长期战略合作协议，与产业伙伴携手共建多样性计算的安全、可靠、连续供应链，最大化保障客户权益，共建繁荣生态。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf923c3e65e5ee907932a3d0f46782b7.png\" /></p><p></p><h4>结语</h4><p></p><p></p><p>在计算产业蓬勃发展，技术创新方兴未艾的浪潮中，华鲲振宇此次发布会的成功举办，将为数字经济的快速发展提供强有力支持，并满足市场对差异化、高效能、低碳绿色计算产品的需求。作为鲲鹏+昇腾计算产业的重要参与者，华鲲振宇将坚持技术创新和产品研发，不断推出更优质、更高效能的计算产品和解决方案，为客户和伙伴创造更大的价值，给世界多一种算力选择。</p>",
    "publish_time": "2023-07-27 09:57:23",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]