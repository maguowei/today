[
  {
    "title": "Docker+Wasm第2个技术预览版发布，新增3个运行时引擎支持",
    "url": "https://www.infoq.cn/article/1WdmawdEDTcLLLdwq7tg",
    "summary": "<p>Docker<a href=\"https://www.docker.com/blog/announcing-dockerwasm-technical-preview-2/\">宣布</a>\"了Docker+Wasm的第2个技术预览版，旨在简化Wasm工作负载的运行，并扩展运行时支持，包括<a href=\"https://www.infoq.com/news/2022/11/Fermyon-cloud-webassembly/\">Fermyon的spin</a>\"，<a href=\"https://deislabs.io/tags/slight/\">Deislabs的slight</a>\"和<a href=\"https://www.infoq.com/news/2020/10/bytecode-alliance-one-year/\">Bytecode Alliance的wasmtime</a>\"运行时引擎。</p><p>&nbsp;</p><p>Docker+Wasm中新增支持的3个Wasm引擎使其支持的运行时总数达到了4个，其中包括在<a href=\"https://www.infoq.com/news/2022/11/docker-webassembly/\">Docker+Wasm第1个技术预览版</a>\"中已经支持的WasmEdge。它们都基于runwasi库。<a href=\"https://www.infoq.com/news/2023/02/containerd-wasi/\">该库最近加入了containerd 项目</a>\"。</p><p>&nbsp;</p><p>runwasi是一个Rust库，它支持运行通过containerd管理的wasm工作负载。因此，除了containerd最初支持的Linux容器之外，它还为新的容器类型创建了一个有效的抽象。顾名思义，runwasi是基于WASI的。WASI是WebAssembly的模块化系统接口，为Wasm运行时提供了一个公共平台。也就是说，如果一个程序被编译为目标WASI，那么它就可以在任何符合WASI标准的运行时上运行。</p><p>&nbsp;</p><p>通常，Wasm容器只包含一个编译好的Wasm字节码文件，而且不需要任何额外的二进制库，这使得容器小很多。这也意味着Wasm容器的启动速度通常比Linux容器更快，而且可移植性更好。例如，正如WasmEdge联合创始人<a href=\"https://twitter.com/juntao/status/1620547819546480640?s=20\">Michael Yuan在Twitter上所说的那样</a>\"，Linux上“最小”的Python容器镜像超过40MB，而<a href=\"https://twitter.com/juntao/status/1620547819546480640\">对应的Wasm容器镜像不到7MB</a>\"。</p><p>&nbsp;</p><p>作为containerd直接支持的Wasm容器，要在Docker Desktop的最新版本中尝试Docker+Wasm的第2个技术预览版，唯一需要做的事情是启用开发 &gt; Settings &gt; Features下的“Use containerd”选项。</p><p>&nbsp;</p><p>使用wasmtime运行Wasm容器，可以执行以下命令：</p><p><code lang=\"shell\">$ docker run --rm --runtime=io.containerd.wasmtime.v1 \n--platform=wasi/wasm secondstate/rust-example-hello:latest</code></p><p>&nbsp;</p><p>正因为如此，借助Docker Compose或其他编排平台（如Kubernetes），Wasm容器可以与Linux容器并行运行。此外，通过在OCI容器中嵌入Wasm运行时，Docker Desktop还可以将Wasm应用程序打包到OCI容器中，从而可以通过DockerHub等容器注册中心来共享Wasm应用程序。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/docker-wasm-containers-preview-2/\">https://www.infoq.com/news/2023/03/docker-wasm-containers-preview-2/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/XNAGHzotjohTSqoWDE9S\">部署太慢，我们用 Warm Docker 容器将速度提高了 5 倍</a>\"</p><p><a href=\"https://www.infoq.cn/article/8lGiLHiAfbyateIF9Blr\">Docker正在淘汰开源组织，CTO硬刚开发者，网友：想赚钱可以，但沟通方式烂透了</a>\"</p>",
    "publish_time": "2023-04-10 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "透明代码大页：让数据库也能用上 2MB 大页！",
    "url": "https://www.infoq.cn/article/QPQ8TugsDmbkraE7Wfe8",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p>大页技术是操作系统中优化内存访问延迟的一种技术，其优化原理与 CPU TLB 硬件有直接关系，而其优化效果不仅受 CPU TLB 硬件影响，还需要看应用访存特点。只考虑 Arm 和 x86 两种平台，已知的大页技术包括透明大页、hugetlbfs、16k 和 64k 全局大页。在合适的场景，大页技术可以提升应用性能达 10% 以上，尤其是针对当前云上应用逐年增长的内存使用趋势，使用大页技术是其中重要的提升“性能-成本”比例的优化手段。</p><p></p><p>透明大页（Transparent Huge Pages，THP）从 2011 年开始在 Linux 内核中已经支持起来，其通过一次性分配 2M 页填充进程页表，避免多次缺页开销，更深层次从硬件角度优化了 TLB 缺失开销，在最好情况下，对应用的优化效果达到 10% 左右。除以上优点外，透明大页（主要供堆栈使用）使用过度也会导致严重的内存碎片化、内存膨胀和内存利用率低等问题，这就是当前透明大页没有在数据库中使用的核心原因，只能感叹“卿本巧技，奈何有坑”。</p><p></p><p>代码大页在透明大页的基础上，将支持扩展到可执行二进制文件，包括进程二进制文件本身、共享库等可执行数据。与透明大页相比，由于代码大页仅将占比较低且有限的可执行文件页部分转换为大页，从根本上避开了内存碎片以及内存不足的问题。与此同时，由于代码类数据和普通堆栈数据访问热度对整体性能影响不同（主要指代码数据或堆栈数据访问缺页一次的性能影响），导致代码类数据使用大页所提升的性能远大于同样分量的透明大页。所以推广和完善代码大页相比透明大页更加简单和容易。</p><p></p><p>本文主要介绍我们的代码大页方案以及一些实验阶段性能测试。为了方便阅读，在这里简单归纳了一下 Linux 系统中大页的支持现状和和必要的数据库相关背景。</p><p></p><h2>大页现状</h2><p></p><p></p><p>当前 Linux 内核支持的大页包括 THP 和 hugetlb，其页大小分别是：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c3/36/c370b2d637116e0b6c6da03ec6c37736.png\" /></p><p></p><p>不考虑其他架构，在 x86 和 Arm 架构中，提到 THP，我们可以一股脑地认为其大小就是 2MB，当前内核暂时还不支持 1GB THP，技术上实现没有什么问题，社区隐隐约约也有人曾经发过相关补丁。回到这里，Arm64 相比 x86，hugtlb 多两种大页支持，主要是 contiguous bit，该特性主要是针对 TLB entry 的优化（连续的 16 个 PTE/PMB，若其上的 PFN 也是连续的，cont bit 会将其使用的 16 个 TLB entry 优化仅占一个 TLB entry）。这里的 64KB 和 32MB 的由来就是 16*4KB 和 16*2MB。</p><p></p><p>另外，在全局页粒度的支持上，Arm64 也比 x86 更会玩，提供的 16KB&nbsp;（CONFIG_16KB 表示）和 64KB（CONFIG_64KB表示）两种选择，这两种页相比 4KB 页，在 TLB 和 cache 上都有明显的优化相关，从而优化宏观指标，当然也并非所有 benchmark 表现出性能提升，例如在 SPECjvm2008 和 stream 中，我们就发现有多项指标在使用 CONFIG_64KB 的时候有较严重的性能下降。除以上问题，还想再啰嗦一下，CONFIG_64KB 中，THP 的大小着实有点“吓人”，有 512MB。</p><p></p><p>因此，大伙对 16KB、64KB 还是又爱又恨。</p><p></p><h2>Mysql、PostgreSQL 和 OceanBase</h2><p></p><p></p><p>站在内存管理的角度，我们仅仅关心 Mysql、PostgreSQL 这些数据库用了多少内存、页缓存占多少、匿名页占多少以及代码段还有 iTLB/dTLB miss 到底高不高。当然还随便想知道 THP 有没有优化，下面几个是简单归纳的几点我们关系的数据库特点：</p><p></p><h4>Mysql</h4><p></p><p></p><p>Mysql 是一个多线程模式的数据库，其代码段大小一般 18M 左右。THP 不敏感，打开 THP，大约仅有不到 3% 的性能提升。跨 NUMA 敏感，本地虚拟机 32 核验证跨 NUMA 抖动在 5~7% 左右。</p><p></p><h4>PostgreSQL</h4><p></p><p></p><p>多进程模型，代码段大小大约 10M 左右。应用 iTLB-load-misses 较高，大约 1.41% 左右。</p><p></p><h4>OceanBase</h4><p></p><p></p><p>多线程模型，代码段大小打印 200M~280M。一般独占单机使用，性能验证过程中并发数要求高：128、1000、1500。THP 本地验证不敏感。</p><p></p><p>这些数据库大约至少有两个共同点：代码段大、iTLB Miss 高。本文也是基于这两个特征进行的优化，当然代码大页优化目标也不局限于这三种数据库。</p><p></p><h2>代码大页</h2><p></p><p></p><p>接着前面数据库背景介绍，这里直接开始代码大页方案。</p><p></p><p>代码大页大致实现分为：整理结构、大致实现、填充功能简介还有最后的代码大页性能评估（包括 Mysql 和 PostgreSQL），最后是我们专门为解决 x86 平台设计的自适应功能。</p><p></p><h3>整体结构与实现</h3><p></p><p></p><p>基于透明大页异步整合大页（主要指 khugepaged 内核线程）的框架：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/51362a6959188b61da0b9656dc994bb0.png\" /></p><p></p><p>上图所展示的代码大页方案主要包括三个部分：</p><p></p><p>（1）映射首地址对齐（蓝色高亮）：这个部分主要是在 elf binary 和 DSO 建立映射的过程中，优先考虑分配 2M 对齐的虚拟地址空间，便于映射到 2M 大页。</p><p></p><p>（2）异步 khugepaged 扫描整合以及加速（橙色高亮）：与 THP 相似，单独设计用户态接口 hugetext_enabled 控制。复用 khugepaged 整合 2M 大页。此外，由于 hugetext 与 THP 共用 khugepaged，在 THP=always 时，也能整合部分符合条件的代码大页；关于加速部分，我们解决了在 THP 使能的场景中，代码段整合慢的问题，这也是我们改进 READ_ONLY_THP_FOR_FS 带来的挑战之一。</p><p></p><p>（3）DSO写回退（紫色高亮）：对于 DSO 所建立的映射，内核屏蔽了 MAP_DENYWRITE，导致用户态可以写打开共享库文件（尽管一旦对该共享库写，进程多数情况会 core dump）。针对这种情况，在检测到该共享库存在写者时，对其 pagecache 进行清空；DSO 为什么有这么多顾虑可以跳转：</p><p>https://developer.aliyun.com/article/863760</p><p></p><p>注意：首地址 2M 对齐的本意是 mmap addr = mmap pgoff (mod 2M)，由于 elf binary 和 DSO 的可执行 LOAD 段的 pgoff 一般为 0，这里为了叙述方便，我们简称地址 2M 对齐。</p><p></p><p>另外，在开发和测试过程中，我们的实现方案解决了 file THP 相关的三个 bug 的补丁，Linux 社区已经合入，glibc 社区也合入了一个 p_align 修复补丁。</p><p></p><p>除以上部分，我们目前已经完成的自适应代码大页已经完成，主要解决 x86 平台使用代码大页过多的问题。详细见“自适应处理”一节。</p><p></p><h4>代码段填充功能</h4><p></p><p></p><p>前面，我们已经描述代码大页方案支持动态链接库（DSO）和二进制文件本身，对于 libhugetlbfs 方案，其仅仅支持二进制文件本身的大页转换，虽转换比较完全，但是它无法同时让 DSO 使用大页。显而易见，与 libhugetlbfs 相比，我们提供的 hugetext_enabled 方式更加完整，性能优势更大。填充功能主要是我们为弥补在某些场景中，其大量的热点发生在代码段的尾部，libhugetlbfs 天生可以做到，所以我们也不得不解决这个问题。</p><p></p><p>言归正传，回到代码段填充。首先用一张图来表示代码段填充具体是什么。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/197aa59b8e2813be499d798b0f6f2138.png\" /></p><p></p><p>大多数应用并不能完全保证其代码段都会 2M 对齐，如上图所示，一个应用第一个 r-xp 有 3.2M，其中 2M 我们的 hugetext_enabled 本身即支持，后面的 1.2M 我们不得不向后填充这个 vma，保证其映射大小 2M 对齐，其填充的内容其实就来自于下一个 r--p 的数据（r-xp 和 r--p 在 ELF 磁盘中是两个相邻的 PT_LOAD 段，具体可以通过 readelf 观察）。</p><p></p><p>当然我们在填充过程中会判断填充后大小不会超过原文件大小和不会与下一个 r--p 属性的 vma 重叠。下面是我们新引入的代码大页填充使能开关，例如将 0x1000 写入 hugetext_pad_threshold，表示需填充内容超过 4k 时填充功能才会使能。</p><p></p><p><code lang=\"null\">/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</code></p><p></p><p>最后再夹带一点私货，以上代码段填充主要是在不重新编译应用程序的情况的一种内核方式，还有一种方式是在应用的链接脚本中加入“. = ALIGN(0x200000)”，将代码段直接按照 2M 对齐，如此不需要填充或进行填充更加安全。例如以一个简单的测试程序 align.out 为例。设置链接脚本后 Section 出现对齐：</p><p></p><p><code lang=\"null\">$&nbsp;readelf&nbsp;-l&nbsp;align.out\n\nElf&nbsp;file&nbsp;type&nbsp;is&nbsp;EXEC&nbsp;(Executable&nbsp;file)\nEntry&nbsp;point&nbsp;0x400520\nThere&nbsp;are&nbsp;9&nbsp;program&nbsp;headers,&nbsp;starting&nbsp;at&nbsp;offset&nbsp;64\n\nProgram&nbsp;Headers:\n&nbsp;&nbsp;Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Offset&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VirtAddr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PhysAddr\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FileSiz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MemSiz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Flags&nbsp;&nbsp;Align\n&nbsp;&nbsp;PHDR&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000040&nbsp;0x0000000000400040&nbsp;0x0000000000400040\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x00000000000001f8&nbsp;0x00000000000001f8&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x8\n&nbsp;&nbsp;INTERP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000238&nbsp;0x0000000000400238&nbsp;0x0000000000400238\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x000000000000001b&nbsp;0x000000000000001b&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x1\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Requesting&nbsp;program&nbsp;interpreter:&nbsp;/lib/ld-linux-aarch64.so.1]\n&nbsp;&nbsp;LOAD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000000&nbsp;0x0000000000400000&nbsp;0x0000000000400000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000200194&nbsp;0x0000000000200194&nbsp;&nbsp;R&nbsp;E&nbsp;&nbsp;&nbsp;&nbsp;0x200000\n&nbsp;&nbsp;LOAD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000400000&nbsp;0x0000000000a00000&nbsp;0x0000000000a00000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000010040&nbsp;0x0000000000200008&nbsp;&nbsp;RW&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x200000\n&nbsp;&nbsp;DYNAMIC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000400010&nbsp;0x0000000000a00010&nbsp;0x0000000000a00010\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x00000000000001d0&nbsp;0x00000000000001d0&nbsp;&nbsp;RW&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x8\n&nbsp;&nbsp;NOTE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000254&nbsp;0x0000000000400254&nbsp;0x0000000000400254\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000044&nbsp;0x0000000000000044&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x4\n&nbsp;&nbsp;GNU_EH_FRAME&nbsp;&nbsp;&nbsp;0x000000000020004c&nbsp;0x000000000060004c&nbsp;0x000000000060004c\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x000000000000004c&nbsp;0x000000000000004c&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x4\n&nbsp;&nbsp;GNU_STACK&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000000&nbsp;0x0000000000000000&nbsp;0x0000000000000000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000000&nbsp;0x0000000000000000&nbsp;&nbsp;RW&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x10\n&nbsp;&nbsp;GNU_RELRO&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000400000&nbsp;0x0000000000a00000&nbsp;0x0000000000a00000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000010000&nbsp;0x0000000000010000&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x1\n\n&nbsp;Section&nbsp;to&nbsp;Segment&nbsp;mapping:</code></p><p></p><p>从上面 readelf 文件输出可以看出：两个 PT_LOAD的offset 已经按照 2M 对齐，并且两个 PT_LOAD 完全分布到不同的 2M 页中，如此，若进行填充，完全不需要用后一个 PT_LOAD 数据。测试方法：导出默认并修改默认的lds文件，在代码和数据段前加上\". = ALIGN(0x200000);\"即可。备注：“gcc -Wl,--verbose”可查看默认的链接脚本。另外，在编译过程中需要加上“-z max-page-size=0x200000”选项，否则对齐会被约束为 4K 或 64K，如下：</p><p></p><p><code lang=\"null\">$&nbsp;gcc&nbsp;main.c&nbsp;test.c&nbsp;-o&nbsp;align.out&nbsp;-Wl,-T&nbsp;ld-align.lds&nbsp;-z&nbsp;max-page-size=0x200000</code></p><p></p><p>链接脚本加 hugetext_pad_threshold 结合使用，可以让应用对代码大页使用更加友好。</p><p></p><h3>快速试用</h3><p></p><p></p><p>为了方便业务使用，我们扩展了两种打开方式：启动参数和 sysfs 接口。</p><p></p><p>启动参数</p><p></p><p>打开：hugetext=1 or 2 or 3</p><p>关闭：缺省即为关闭</p><p></p><p>sysfs 接口</p><p></p><p>仅打开二进制和动态库大页：echo 1 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>仅打开可执行匿名大页：echo 2 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>打开以上两类大页：echo 3 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>关闭：echo 0 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>查看/proc//smaps中FilePmdMapped字段可确定是否使用了代码大页。</p><p>扫描进程代码大页使用数量（单位 KB）：</p><p>cat /proc//smaps | grep FilePmdMapped | awk '{sum+=$2}END{print\"Sum= \",sum}'</p><p></p><p>padding 接口</p><p></p><p>/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</p><p>echo [0~2097151] &gt; &nbsp;/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</p><p>当二进制文件末尾剩余 text 段由于不足 2M 而无法使用大页，当剩余 text 大小超过 hugetext_pad_threshold 值，可将其填充为 2M text，保证可使用上大页。hugetext_pad_threshold=0，表示填充功能关闭，该接口依赖 hugetext_enabled 接口。</p><p>建议一般情况写 4096 即可：echo 4096 &gt; &nbsp;/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</p><p>当然，如果想完全回退代码大页对应用的影响，可以采用下面的回退方式：</p><p>在打开 hugetext_enabled 后，若关闭 hugetext_enabled 并且完全消除 hugetext_enabled 影响，可以下面几种方式：</p><p></p><p>清理整个系统的page cache：echo 3 &gt; /proc/sys/vm/drop_caches清理单个文件的page cache：vmtouch -e //target</p><p></p><p>清理遗留大页：echo 1 &gt; /sys/kernel/debug/split_huge_pages</p><p></p><h2>代码大页性能评估</h2><p></p><p></p><h3>Mysql 性能评估</h3><p></p><p></p><p>为了充分挖掘代码大页的收益，我们针对不同的平台，包括 x86、Arm，分别对 mysql、python 以及 jvm 等应用进行了测试，由于数据太多、混乱，我们挑选出 mysql 上的测试数据，测试数据包括 TPS、QPS 以及 iTLB miss 数据。</p><p></p><p>本文数据的测试环境：</p><p></p><p>5.10（alios 5.10-002）mysql 版本：MariaDB-10.3.28虚拟机配置：32 核、128G 内存物理机配置：打开透明大页</p><p></p><p>由于在 Arm 平台和 x86 平台测试的数据指标和测试方法相同，所以我们挑选出 Arm 篇对这些数据进行了较详细的描述和分析，在 x86 篇中，仅仅简单描述数据统计图中代码大页与 4k 代码页的性能差异。</p><p></p><h4>Arm 篇</h4><p></p><p></p><p>下面展示在 Arm 平台上，代码大页对 mysql 的性能提升。</p><p></p><p>测试过程中，mysql 并发数分别是 1、8（25%）、16（50%）、32（100%）。测试结果与普通的 4K 代码页数据对比如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4ad78e7de7d55875d60f3ac0b6eda4b1.png\" /></p><p></p><p>上图 TPS 对比可以看出：代码大页的性能始终高于普通的代码页。关于这张图中其他的结论，还有：</p><p></p><p>并发数为 1 时，外在的影响因素最小，此时，代码大页相比普通代码页，性能提升大约在 6.9%。并发数 8、16 基本可以保证没有 CPU 的竞争，代码大页的性能提升大约也在 6.5% 以上。并发数32时，由于总核数为 32，存在于其他应用竞争 CPU，所以 TPS 较低于前面的测试结果。但是代码大页的鲁棒性更好，此时相比普通代码页，性能提升大约在 11% 左右。</p><p></p><p>另外，还有 RT 的对比，如下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/7914daa67115e3003115c3f15ac4bbee.png\" /></p><p></p><p>RT 的数据主要是前 95% 的请求的最大响应时间（95th percentile）。代码大页在 RT 上的表现与图 1 一致，大约提升在 5.7%~11.4% 之间。</p><p></p><p>最后的展示的数据是微架构数据，我们在分析代码大页对性能的影响过程中，主要观察 iTLB miss，如图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8bf6012f49e9071c87fa5fc769a62315.png\" /></p><p></p><p>图 5（a）和图 5（b）都展示的是 iTLB 的数据，但是考虑到不同读者的偏好，我们在测试的过程中 iTLB miss 和 iTLB MPKI 也一并记录。如这两图所示：</p><p></p><p>mysql 使用代码大页后，iTLB miss 大约下降了 10 倍左右，数值大小从原来的 1% 下降到 0.08% 左右。在 iTLB MPKI 上，大约下降了 6 倍左右。</p><p></p><p>经过上述数据的对比，以及我们在实验阶段进行的 THP（这里主要指 anon THP）数据对比， 在 mysql 场景下，大致可以得出一个简单的公式：</p><p></p><p>收益：代码大页 &gt; anon THP &gt; 4k</p><p></p><h4>x86 篇</h4><p></p><p></p><p>与上一小节相同，这里分别对代码大页和 4k 代码页进行 TPS、RT 对比、iTLB miss 对比。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/225ce1c8e522b3deb89ea6d3aa952c03.png\" /></p><p></p><p>由于 TLB 硬件的差异，代码大页在 x86 和 Arm 上性能提升存在差异，根据图 6（a、b）中的测试数据，代码大页在 x86 上，对 TPS 的提升大致在 3%~5 之间，在 RT 上大致有 5%~7.5% 的收益。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b87adeb7571e174bbdd373d5af5b8f7c.png\" /></p><p></p><p>在图 7 中，可以看出：</p><p></p><p>从 iTLB miss 的数据看，代码大页相比 4k 代码页大约下降了 1.5~2.3 倍左右。从 iTLB MPKI 的数据看，代码大页相比4k代码页大约下降了1.6~2.3倍之间，与iTLB miss效果相似。</p><p></p><p>到这里，我们结束对代码大页性能数据的展示。关于在物理机的测试，感兴趣的读者可以自行在&nbsp; alios 5.10-002 上进行测试。</p><p></p><h3>PostgreSQL 性能评估</h3><p></p><p></p><p>代码大页相比 libhugetlbfs 方案，不仅仅解决了使用 libhugetlbfs 后无法使用 perf 观察热点问题，这里在 PostgreSQL 上 padding 功能派上了作用，最终代码大页性能提升较 libhugetlbfs 多 2% 左右。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04e57f21f771cbc03ce614243873eb27.png\" /></p><p></p><p>上述测试数据主要在 Arm 环境中。</p><p></p><h3>小结</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/95e54e99754ada46ad5d8b075af73fe1.png\" /></p><p></p><p>注：以上主要为 Arm 平台测试数据。符号 x 可以认为该应用没有采用该技术，或优化效果几乎是 0。</p><p>最后归纳一下，归根结底，代码大页对性能的提升也是采用的内存领域常用方法：热点+大页。</p><p></p><h2>自适应代码大页</h2><p></p><p></p><p>自适应代码大页在代码大页的基础上，考虑到 x86 平台上 2MB iTLB entry 不足的问题，进行的“热点采集+大页整合”的大页使用数量限制策略，该特性基本解决了代码大页在某些特大代码段上的负优化问题。</p><p></p><p>云上JVM类应用大约占比 60% 以上，这类应用在阿里内部大约可发现所使用的 code cache 大约有 150M~400M 情况，这类 code cache 是比较特殊的匿名页，属于 JIT 热点代码缓存在此处，属于特殊的代码数据。对于 Arm 平台，其 TLB 硬件社区并未区域 2M 或 4k 的页表项，而对于 x86 平台，其 TLB 命中有页大小要求，例如：4k 页只能使用 4k TLB entry、2M 页使用 2M TLB entry。这种设计要求应用本身使用的页表不会超过硬件支持，否则易出现严重的 TLB 未命中情况，导致性能骤降。</p><p></p><p>当前我们对云上常用的 x86 平台情况进行了一个调研，主要包括以下三类服务器在役：</p><p></p><p></p><p></p><p>从上面的数据可以看出，当前在 x86 平台，基本无法使用代码大页，2M code TLB 资源使用率基本为 0。下面我们以蚂蚁的 Java 的业务为例说明由于 TLB 资源匮乏导致的性能问题。在蚂蚁的 Java 业务总通过 hugetext 让 code cache 使用大页，出现性能回退：iTLB miss 上升 16% 左右，CPU 利用率上升 10% 左右。其原因可以确定在于 code cache 大约 150M，需要覆盖 70 多个 2M iTLB entry，而当前蚂蚁环境使用的机器基本都是 Intel 机器，以 skylake 为例，仅仅 82M iTLB entry，造成 2M iTLB entry 竞争激烈。</p><p></p><p>下表为测试的代码大页负优化数据：</p><p></p><p></p><p></p><p>上面的数据表示在打开 hugetext 后，icache_64b.iftag_stall 反而上升了 16% 左右，另外在 STLB 命中的比例上升了 0.48%，造成的结果就是 CPU 利用率上升 10% 左右。</p><p></p><p>将上面描述的问题可以简单总结为以下：</p><p></p><p>（1）Intel 机器普遍存在大页 iTLB entry 数量较少的问题，大多数服务器集中在 8~16 之间。当前 Arm 平台由于不区分 4k 和 2M iTLB，尚未发现类似问题。</p><p>（2）诸如此类，其他业务，如 flink、容器场景，代码大页使用过多造成上述问题。</p><p></p><p>以 Intel skylake 为例，iTLB entry 信息：</p><p></p><p></p><p></p><p>另外，也发现 STLB 命中后，替换 L1 iTLB entry 的惩罚较重，大约占 22 cycles。因此需要避免代码在 STLB 命中。简而言之，在 Intel 系列普遍存在 2M iTLB entry 有限的问题，是代码大页在此类平台性能提升不明显或存在负优化的核心原因。</p><p></p><p>针对上面描述的问题，自适应代码大页采用限制大页使用数量的方式，将较热点的数据整合为大页。在使用时，仅需在用户态设置自适应大页的数量接口。自适应代码大页处理流程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/089dda4ca29dc66c06eb99a578b93a9f.png\" /></p><p></p><p>使用该方案，可以解决代码大页在 x86 平台上负优化的问题，可以应用到 JVM 类应用和 oceanbase 数据库上。</p><p></p><p>自适应代码大页使用接口：/sys/kernel/mm/transparent_hugepage/khugepaged/max_nr_hugetext，只要为该接口设置一个值，就可以约束单个应用使用的最多代码大页数量，且不会影响 THP 的正常逻辑。</p><p></p><h2>除了大页，代码段优化还有什么</h2><p></p><p></p><p>代码段的优化除了大页的方案，业内还包括 PGO/autoFDO、LTO 等。</p><p></p><p>按代码段优化时间，可以将当前在编译器和链接器所做优化，简单分为：post-link optimization、link-time optimizations 以及比较混合的 BOLT optimization，如下。这些优化都是对应用代码段布局的优化。</p><p></p><p>feedback-driven optimizations (FDO) or called profile-guided optimizations (PGO)（一般用 PGO 统称）sample-based profilinginstrumentation-based profiling （基本都是线下训练）hardware-event sampling e.g. LBR (e.g. SampleFDO, AutoFDO)profile-guided function reordering algorithmPettis-Hansen (PH) heuristic (weight dynamic call-graph, instrumentation-based profiling)Call-Chain Clustering (C3) heuristic (weight and directed call-graph, hardware-event sampling or stack traces sampling)link-time optimizations (LTO, [27, 28])only depend on function reordering algorithm (PH heuristic, C3 heuristic)post-link optimization (e.g. Ispike Spike [5], Etch [6], FDPR [7])BOLT optimizations （大杂烩）</p><p>（不需要看懂，只要知道有 autoFDO 和 BOLT 就行）</p><p>所以这个东西干嘛用的？举一个简单例子，实验课上老师给大家一段排序的代码，让大家进行优化，将排序的时间开学降低 10 倍，这个时候就可以用上面的工具，再加上简单的 perf 使用，而不需要看具体的代码实现。</p><p></p><h2>代码大页支持产品</h2><p></p><p></p><p>当前，代码大页已经支持龙蜥操作系统（Anolis OS）和阿里云 ECS 支持，参考链接如下：</p><p></p><p>龙蜥操作系统（Anolis OS）：https://gitee.com/anolis/cloud-kernel</p><p></p><p>龙蜥操作系统 4.19 和 5.10 最新版本默认打开 CONFIG_HUGETEXT（代码大页的主要配置）。“代码大页性能评估”一节的数据可以在龙蜥操作系统 5.10 内核复现，使用镜像可选择：</p><p></p><p></p><p></p><blockquote>镜像缺省 sudo 用户为 anuser，对应登录密码是 anolisos。链接：https://docs.openanolis.cn/products/anolis/rnotes/anolis-8.8.html</blockquote><p></p><p></p><p>阿里云 ECS</p><p></p><p>若使用环境为阿里云 ECS，可参考：</p><p>https://help.aliyun.com/document_detail/462660.html&nbsp;使用代码大页。</p><p></p><h2>后续展望</h2><p></p><p></p><p>最后头脑风暴一下，代码大页也许还可以：</p><p></p><p>代码只读文件系统，专门用于放置 lib 库。64kB、32MB 代码大页，接近特殊场景 2MB 代码大页无法使用的问题。多种代码大页支持。靶向代码大页。</p><p></p><p>从 iTLB miss 角度看，这些也许只能算是功能完善，并没有太大的性能优化。</p><p></p><p>参考：</p><p>阿里云 ECS上使用代码大页说明：</p><p>https://help.aliyun.com/document_detail/462660.html</p><p>将 mysql 可执行文件的代码段和数据段加载到大页上：</p><p>https://jira.mariadb.org/browse/MDEV-24051</p><p>社区讨论：</p><p>https://sourceware.org/pipermail/libc-alpha/2021-February/122334.html</p><p>如何将自己的代码段和数据段映射到大页：hugepage_text.cc</p><p>MAP_DENYWRITE：被 Linux 内核屏蔽的 flag</p><p>https://developer.aliyun.com/article/863760</p>",
    "publish_time": "2023-04-10 10:28:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "GPT-4预示着前端开发的终结？你准备好面对无法预测的技术挑战了吗？",
    "url": "https://www.infoq.cn/article/hyRP71z0tdRzvc5ZUtUS",
    "summary": "<p>过去几个月来，我跟很多刚刚步入前端开发领域的朋友们交流过，大家都对步步紧逼的 AI 感到焦虑。他们看到 GPT-4 等工具带来的令人印象深刻的演示，担心自己学成 HTML/CSS/JS 知识之日，就是岗位消失之时。</p><p></p><p>这类情绪绝非个例，Twitter 上早已是哀鸿一片。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/fe/fea8ad278e62afc60d427de1bf937828.png\" /></p><p></p><p></p><blockquote>澄清声明我先向大家承认，这个话题跟我有切身关联：我的工作就是在网上教学员开发软件。如果开发者岗位逐渐消失，那我的饭碗也就完了。但我向大家保证，我在本文中表达的所有观点都出于自己的真实判断。欢迎大家自行斟酌这些观点是否存在偏见。</blockquote><p></p><p></p><p></p><h2>“又”要被替代了</h2><p></p><p></p><p>CSS 语言最初发布于 1996，随 IE 3 一同推出。两年之内，首款“无代码”网站构建器 Homestead 就跟大家见面了。</p><p></p><p>Homestead 能帮助人们在无需编写代码的情况下，轻松构建起自定义网页：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6f/6f0e36a9c76ddfd8a4c808acf2a40ded.png\" /></p><p></p><p>也就是说，从 Web 开发这类岗位刚刚出现时，就有人担心会被某些新技术所淘汰。2000 年代的威胁源头是 WordPress，2010 年代是 Webflow，2020 年代初则是“无代码”工具。</p><p></p><p>其实从某种意义上讲，Web 开发者确实已经过时了。如今，本地面包小店、牙医诊所或者艺术家如果需要一个网站，那大概率不会花几万美元聘请开发者从零开始做设计。他们可以直接前往 SquareSpace，找到自己喜欢的模板，然后每月花 20 美元来运营自己的业务门户。</p><p></p><p>但就算这样，Web 开发者也仍然存在。</p><p></p><p>上星期，OpenAI 公布了 GPT-4，期间有一段令人印象深刻的演示：GPT-4 能够将手绘的网站草图转化成功能齐全的网站，甚至包括一些 JS 来连接“Reveal Punchline”按钮。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/33/332ed0806644b61308e274d5fd235683.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/82/823e7f1d22ef70f74f16c0e3c652c115.png\" /></p><p></p><p>这功能确实很酷，在原型设计方面有很大的潜力……但我们还是得明确一点：几十年来，我们的 Web 开发者负责构建的从来不是这类页面。演示中的 HTML 文档跟目前前端开发者编写的各种代码，可以说是天渊之别。</p><p></p><p></p><h2>展望未来</h2><p></p><p></p><p>到目前为止，我个人看到的大部分演示都非常有限：要么是一个简单的 HTML 页面，要么是 JS 函数。总之，就是那种开发者可以随手搞定的东西。</p><p></p><p>但这一切还仅仅只是开始！如果 AI 继续保持目前的发展速度，再过几年就能构建起完整的应用程序了，对吧？</p><p></p><p>我当然不是什么大语言模型专家。但对于 GPT-4，我完全能够理解它的宏观工作原理。</p><p></p><p>从本质上讲，大语言模型是一种超级强大的文本预测器。给定一个提示，它们就能使用机器学习来尝试找到最可能符合提示要求的字符集。</p><p></p><p>OpenAI 等厂商花费了大量的时间和精力来调整模型，借此改善输出质量。一大群人工标者对模型的输出进行“评级”，借此引导模型不断学习和进化。</p><p></p><p>如果大家体验过 CHatGPT 或者 Bing 的 AI 搜索等工具，就会发现这些回答大概有个 80% 的正确率，但 AI 却表现得像是有 100% 的把握。</p><p></p><p>大语言模型无法验证或者说测试自己的假设，无法确定自己的表达内容是否为真。它们还是在玩概率游戏，猜测给出的字符应该会跟用户在提示中输入的字符相匹配。</p><p></p><p>有时候，AI 甚至会给出非常荒谬的回答。OpenAI 团队将其称为“幻觉”。</p><p></p><p>随着技术进步，预计某些粗糙的问题会被一一克服。但从根本上讲，AI 或多或少还是会存在准确度问题。毕竟 AI 工具没有任何机制能够对自己的响应做出客观验证。</p><p></p><p>因此，准确率可以不断提高，但永远达不到完美。而趋于完美，正是 AI 真正取代 Web 开发者的前提。如果它本身不是程序员，那就无法判断哪些部分是对的、哪些是错的。AI 自己发现不了“幻觉”。</p><p></p><p>但大家别急，我也认真看了 GPT-4 的演示，AI 似乎能够自我纠正！只要复制 / 粘贴错误消息，它就能找到并解决问题。</p><p></p><p>但是……并不是所有幻觉都会引发异常。比如说，我最近用 GPT-4 在 React 上生成了一个〈Modal〉组件，虽然输出非常好，但还是存在一些可访问性错误。创建应用程序的人可能注意不到这些问题，不过最终用户肯定会发现！</p><p></p><p>还有代码中的安全漏洞呢？一旦引发可怕的错误，该由谁来承担责任？</p><p></p><p>还有一点：生成一个 50 行的 HTML 文档，跟输出生产就绪的 Web 应用程序之间根本就不是一回事。像博客这样的小型 JS 应用就有大约 6.5 万行代码，总计 900 多个文件。这还不包含写作文本，单是 JavaScript 和 TypeScript 的体量。</p><p></p><p>也就是说，即使准确率达到 95%，在这样的应用规模下也将很难调试。这相当于一位开发者用几个月时间构建一个巨大的项目，但却从未做过任何代码测试，直到 100% 完成后才加以验证。这不是生产力提升，而是前所未有的噩梦。</p><p></p><p>AI 不是什么神迹和魔法，它的质量直接取决于训练数据。而代码片段在互联网上随处可见，往往高度通用。相比之下，每个代码库都是独一无二的，网上鲜有大型开源代码库。那么，AI 要如何学会构建能够实际应用的大型项目？</p><p></p><p>按现在的发展速度看，很快非开发者就可以直接跟聊天机器人对接，轻松启动一个小型独立项目。也就是说，如今的 Webflow 等工具会被 AI 所替代，而且效果更好、功能更强，这绝对是件大好事！</p><p></p><p>但要让科技大厂们放弃自己的开发队伍，全面选择 AI 工程师？那还有很长的路要走，前面提到的这些潜在问题将严重阻碍这种想象在现实中落地。</p><p></p><p></p><h2>增强，而非替</h2><p></p><p></p><p>说了这么多，我就明确给出自己的态度吧：我其实非常看好 AI 技术 😅。</p><p></p><p>我认为最有可能的情况，是 GPT-4 这类工具被整合进开发者工具当中，用来扩大熟练开发者的生产能力。</p><p></p><p>木匠并没有被电动工具所替代，会计师没有被电子表格替代，摄影师没有被数码相机 / 智能手机替代，我认为开发者也不会被大语言模型替代。</p><p></p><p>我当然不确定未来的开发者职位总量会不会有所下降；毕竟如果每位开发者的效率都显著提高，那岗位方面的整体需求应该减少才对。</p><p></p><p>但也不一定。就目前来讲，软件开发者仍处于供不应求的状态。在我工作过的每家公司，大家都有太多想做的事情，只因开发者数量有限而被迫作罢。</p><p></p><p>那如果开发者的工作效率瞬间提升 2 倍，结果会如何？无疑是修复更多 bug、发布更多功能、赚取更多利润。我们可以开发的东西太多了，根本不存在开发者过剩一说。</p><p></p><p>我反倒认为，大语言模型的普及会增加开发者的职位总量。</p><p></p><p>目前，很多企业根本就不雇用软件开发者。我曾在一家名叫 Konrad Group 的公司工作，这是一家专为客户构建 Web 应用程序的企业，客户名单里不乏家喻户晓的大厂。但因为开发成本太高，他们发现与其招聘固定开发人员，还不如把这活儿外包出去。</p><p></p><p>这些财富五百强公司会根据当前的软件开发成本做核算。这里我们假定一个场景：如果他们需要 4 名开发人员，平均年薪 15 万美元，那每年就是 60 万美元。在这种情况下，他们不如向外包机构支付 50 万美元来搞定这方面需求。但如果大语言模型真能提高开发者的工作效率，他们也许只雇 2 名年薪 15 万美元的开发者就够了。突然之间，原本自己没法做的项目现在可以做了！</p><p></p><p>我要澄清一下：我不是经济学家，以上案例安全就是随口一说。我不知道未来的情况会不会朝这个方向发展。但我想提醒大家，大语言模型的爆发并不一定会摧毁我们的工作和生活。没人知道未来会向哪里去，而我实在受不了人们言之凿凿地把前景描述得愁云惨淡。</p><p></p><p></p><h2>网上不乏与我相同的观点</h2><p></p><p></p><p>Aaron Blaise 是一位经验丰富的动画师兼插画家。他在迪士尼工作了近 20 年，曾为《美女与野兽》（1991）、《阿拉丁》（1992）、《风中奇缘》（1995）等经典影片做出贡献。</p><p></p><p>几周之前，他在 YouTube 上发布了一段视频，题为《迪士尼动画师对 AI 动画的反应》（<a href=\"https://www.youtube.com/watch?v=xm7BwEsdVbQ%EF%BC%89%E3%80%82%E4%BB%96%E7%9A%84%E8%A7%82%E7%82%B9%E8%B7%9F%E6%88%91%E5%BE%88%E5%83%8F%EF%BC%8C%E5%B9%B6%E4%B8%8D%E8%A7%89%E5%BE%97\">https://www.youtube.com/watch?v=xm7BwEsdVbQ）。他的观点跟我很像，并不觉得</a>\" AI 工具是种威胁，而认为它将提高动画师的生产力并创造出更多动画岗位。</p><p></p><p>来自几十个行业的艺术家和知识工作者们都在讨论这方面话题，毕竟很多人担心自己的工作会被 GPT-4、DALL-E 2 和 Midjourney 等 AI 工具所吞噬。</p><p></p><p>GPT-4 能通过模拟律师考试吗？可以，而且得分在人类应试者里能排进前 10%。所以律师们也慌了。</p><p></p><p>我个人认为，大多数专业人士都能找到将 AI 技术整合进工作流程的办法，借此提高自己的生产力和价值。一部分任务可能被全面委托给 AI，但只是部分任务、而非整个工作。</p><p></p><p>那如果我是错的，事实证明大语言模型可以全面取代软件开发者，又该怎么办？如果真是这样，那我觉得也不用太担心了，到那时候绝大多数知识工作者都会被“清洗”。</p><p></p><p>这样一来，我们就算是转行、转专业也没有用，AI 海啸之下将没有安全的高地。所以与其去赌未来会不会朝着最糟糕的方向去，倒不如专注于自己热衷的工作、感兴趣的问题、擅长的方向。</p><p></p><p></p><h2>前端开发与其他工程学科</h2><p></p><p></p><p>网上有帮家伙，一直在暗示前端开发特别容易被 AI 替代，还建议开发者再精进一步，往后端或数据工程那边努力。</p><p></p><p>但我的观点恰恰相反。我觉得开发者群体根本就不用担心被淘汰，如果非得说哪部分群体更危险，我倒认为是后端。</p><p></p><p>OpenAI 的 GPT-4 直播展示了两段与代码相关的内容：</p><p></p><p>“Joke website”前端；基于 Python 的 Discord 机器人。</p><p></p><p>在这两个项目中，Python 代码对我来说似乎更接近生产。我最近用 Node.js 构建了一个 Discord 机器人，跟 GPT-4 生成的代码确实差不多。</p><p></p><p>相比之下，AI 为那个笑话网站生成的基础 HTML 文档，就跟我每天编写的前端代码间相隔十万八千里。</p><p></p><p>这可能有点以偏概全，但过去十年来，我发现很多复杂性要素已经从服务器转向了客户端。</p><p></p><p>Monolithic Express 应用程序已经转化为无服务器函数的集合，而我们的前端也从超链接数字文档发展为成熟的桌面质量应用程序。</p><p></p><p>再有，前端是用户与之交互的产品部分。企业当然希望自己的产品能体现出定制化、独特性和根据品牌形象精心设计等特点。与之对应，后端则完全不可见，所以通用后端要比通用前端更让人易于接受。</p><p></p><p></p><h2>用大语言模型辅助学习</h2><p></p><p></p><p>我听有些人说，ChatGPT 能帮助大家快速学习技术性技能。如果各位对教程中的某些内容感到困惑，不妨问问神奇的 AI。</p><p></p><p>这对我来说是个非常有趣的用例。从本质上讲，ChatGPT 就像你的结对程序员，能帮助你了解自己不熟悉的事物。你可以向它提出具体问题，并获取具体答案。</p><p></p><p>当然，使用过程中也要当心。利用 AI 工具来学习，也有正确的方式与错误的方式之分。</p><p></p><p>所谓错误的方式，就是把 AI 当作 GPS 导航。当我们打算开车前往某地时，就会把地址输入 GPS，然后不假思索地按它的指示前进。等到了目的地附近时我会把导航关掉，整个过程不做任何思考和评判。结果就是，我的方向感完全萎缩了，现在如果没有合成语音的指示，我根本不知道该往哪里走。😬</p><p></p><p>与之相对，我们最好把大语言模型当成“被告”，而自己则扮演“陪审团成员”。</p><p></p><p>你要认真听它在说什么，但不可全盘相信。保持怀疑的态度，以批判的心态考量一字一句。</p><p></p><p>不可盲目复制 / 粘贴 ChatGPT 生成的代码，而是逐行检查并保证你理解了其中内容。有不清楚的地方要提醒它做出澄清，并配合权威来源（例如官方文档）仔细检查看似可疑的部分。请记住，大语言模型 100% 自信，但却并非 100% 准确。</p><p></p><p>通过这种方式，相信大语言模型能够创造出前所未有的价值。😄</p><p></p><p></p><h2>给各位年轻开发者的忠告</h2><p></p><p></p><p>我之所以要写下这篇文章，就是想鼓励各位正在学习 Web 开发、并为大语言模型的爆发式进步压得喘不过气的朋友们。很多人彻底失去了信心，认为随着 AI 技术的继续成熟，自己耗费时间和精力掌握的所有技能将被无情淘汰。</p><p></p><p>我不敢保证未来一定不会这样。但我对 AI 能够给工作方式带来的影响仍抱怀疑。早在 2007 年，我就开始研究 HTML/CSS/JS，多年以来情况已经发生了很大变化。但开发者们仍然凭借强大的适应性，与技术一同进步、携手为网络世界塑造新的样貌。</p><p></p><p>到目前为止，我还没看到有切实迹象表明前端开发的工作处于危险之中。我也想象过未受任何专业训练的非开发者，在不了解 Web 技术的情况下会开发出怎样的 Web 应用程序。太多的因素会导致其无法运行了，所以即使 GPT 未来继续保持迭代，大家也没必要过于焦虑。这，又何尝不是另一种“幻觉”。</p><p></p><p>再次强调，我的观点很可能是错的，毕竟我可没有能预见未来的水晶球🔮。但未来总有不确定性，没准明天太阳就爆炸了，谁知道呢。但我真的很难相信 Web 开发者会被迅速淘汰，反而更担心潜在开发者会因为这种莫须有的可能性而放弃学习。</p><p></p><p>总之，但愿不要五年之后软件开发者需求更旺之时，大家才感觉后悔、反思当下的自己为什么要停止追求梦想。请继续努力吧，各位同学！❤️</p><p></p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.joshwcomeau.com/blog/the-end-of-frontend-development/\">https://www.joshwcomeau.com/blog/the-end-of-frontend-development/</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/7c2b26c5bb69b6acbc6dfe7df\">GPT-4：不了不了，这些我还做不到</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ebf7dab20cbf79517f2fc46c5\">3 分钟快速了解 GPT-4</a>\"</p><p><a href=\"https://www.infoq.cn/article/tDCUgv7rCH0yX40wdFVG\">对话 OpenAI Greg Brockman：GPT-4 并不完美，但人类也一样</a>\"</p><p><a href=\"https://www.infoq.cn/article/HFSPasQ7SXZ9QzdFXhGO\">GPT-4 重磅发布，吊打 ChatGPT！编程能力牛到让我睡不着：10 秒做出一个网站，1 分钟开发一个游戏</a>\"</p>",
    "publish_time": "2023-04-10 12:43:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "备受Vue、Angular和React青睐的Signals演进史",
    "url": "https://www.infoq.cn/article/qvL7WDtjURgbx7ICvhy6",
    "summary": "<p>最近，在前端领域，围绕着“Signals”这个词，有一些热烈的讨论。不管是<a href=\"https://preactjs.com/guide/v10/signals/\">Preact</a>\"还是<a href=\"https://github.com/angular/angular/discussions/49090\">Angular</a>\"，似乎都在讨论该话题。</p><p>&nbsp;</p><p>但它们并不是什么新东西。如果我们将其追溯到上个世纪60年代的研究，那么这就更算不上新鲜的事物了。它的基础采用了与第一个<a href=\"https://www.historyofinformation.com/detail.php?id=5478\">电子表格</a>\"和硬件描述语言（如Verilog和VHDL）相同的模型。</p><p>&nbsp;</p><p>即便是在JavaScript中，从声明式JavaScript框架诞生开始，我们就拥有这种理念了。随着时间的推移，它们有了不同的名字，并且在这些年里不断流行了起来。现在，它又重新出现了，这是一个很好的时机，我们可以对它是什么以及为何需要它进行更多的介绍。</p><p>&nbsp;</p><p></p><blockquote>免责声明：我是SolidJS的作者。本文从我的角度介绍了演进的过程。尽管文中没有提及，但是<a href=\"https://csmith111.gitbooks.io/functional-reactive-programming-with-elm/content/section5/Signals.html\">Elm Signals</a>\"、<a href=\"https://emberjs.com/\">Ember的计算属性</a>\"和<a href=\"https://www.meteor.com/\">Meteor</a>\"都是很值得称道的。&nbsp;如果你还不清楚Signals是什么以及它是如何运行的，请参阅我的这篇对<a href=\"https://dev.to/ryansolid/a-hands-on-introduction-to-fine-grained-reactivity-3ndf\">细粒度反应性（Fine-Grained Reactivity）</a>\"的介绍。</blockquote><p></p><p></p><h2>起初的蛮荒时代</h2><p></p><p>&nbsp;</p><p>有时候，我们会惊讶地发现，很多参与者在完全相同的时间形成了类似的方案。在声明式JavaScript框架的起步阶段，有三个方案在三个月内陆续发布，它们分别是<a href=\"https://knockoutjs.com/\">Knockout.js</a>\"（2010年7月）、<a href=\"https://backbonejs.org/\">Backbone.js</a>\"（2010年10月）和<a href=\"https://angularjs.org/\">Angular.js</a>\"（2010年10月）。</p><p>&nbsp;</p><p>Angular的脏值检查、Backbone的模型驱动重渲染以及Knockout的细粒度更新，虽然它们彼此间有些差异，但是最终都成为了我们今天管理state和更新DOM的基础。</p><p>&nbsp;</p><p>Knockout.js对本文的主题特别重要，因为它们的细粒度更新是建立在所谓的“Signals”的基础之上的。他们最初引入了两个概念，分别为observable（状态）和computed（副作用），但是在接下来的几年中，他们在前端语言中引入了第三个概念pureComputed（衍生状态）。</p><p>&nbsp;</p><p><code lang=\"null\">const count = ko.observable(0);\n\n\nconst doubleCount = ko.pureComputed(() =&gt; count() * 2);\n\n\n// 每当doubleCount更新时，打印日志记录\nko.computed(() =&gt; console.log(doubleCount()))</code></p><p></p><h2>狂野时代</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/32b35aeffee1a8b40d6b045782f3ecfa.jpeg\" /></p><p></p><p>&nbsp;</p><p>在这个时代，服务器端开发的MVC和过去几年从jQuery中学到的模式进行了融合，形成了新的模式。其中，最常见的一个模式叫做数据绑定，Angular.js和Knockout.js都具有该模式，不过实现方式略有不同。</p><p>&nbsp;</p><p>数据绑定的概念是，state（状态）应该被关联（attached）到view tree（视图树）的一个特定部分上。借助这种方式，能够实现的一种强大功能叫做双向绑定。所以，我们可以让状态更新DOM，反过来，DOM事件会自动更新状态，所有的这一切均是以一种简单的声明方式实现的。</p><p>&nbsp;</p><p>但是，滥用这种力量最终会作茧自缚。我们构建应用的时候，对其缺乏足够深入的了解。在Angular中，如果不知道什么内容发生变化，就会对整个树进行脏值检查，而向上传播会导致它多次发生。在Knockout中，很难跟踪变化的路径，因为你会在DOM上走来走去，出现循环也是司空见惯的。</p><p>&nbsp;</p><p>当<a href=\"https://reactjs.org/\">React</a>\"出现的时候，我们已经准备好逃离这一切了，对我个人来说，是<a href=\"https://youtu.be/nYkdrAPrdcw\">Jing Chen的演讲</a>\"，让我稳住了阵脚。</p><p>&nbsp;</p><p></p><p></p><p></p><h2>自由时刻</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/65/64/65dedbcfee801653a4cf9269da27ef64.gif\" /></p><p></p><p></p><p>接下来，就是对React的采用。有些人依然喜欢反应式模型，因为React对状态管理没有自己的偏好，所以完全可以将两者结合起来。</p><p>&nbsp;</p><p><a href=\"https://mobx.js.org/\">Mobservable</a>\"（2015）就是这样的方案。但是，相对于与React的集成，它还带来了一些新的内容。它强调一致性和顺畅（glitch-free）的传播。也就是说，对于任何给定的变更，系统的每个部分仅运行一次，而且以适当的顺序同步运行。</p><p>&nbsp;</p><p>为了实现这一点，它使用了一种推-拉（push-pull）混合的系统来替换先前方案中基于推送的反应性。变更的通知会被推送出去，但是衍生状态的执行会推迟到读取它的地方。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0badd6d7848beacb47a93ed5882a0a33.png\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>为了更好地理解Mobservable的原始方式，请参阅Michel Westrate的“<a href=\"https://hackernoon.com/becoming-fully-reactive-an-in-depth-explanation-of-mobservable-55995262a254\">Becoming Fully Reactive: An in Depth Explanation of Mobservable</a>\"”一文。</blockquote><p></p><p>&nbsp;</p><p>虽然在很大程度上，这个细节会被React重新渲染读取变更的组件所掩盖，但是，这是使系统实现可调试和一致性的关键步骤。在接下来的几年里，随着算法的不断完善，我们会看到一种趋势，那就是<a href=\"https://dev.to/modderme123/super-charging-fine-grained-reactive-performance-47ph\">更多基于拉取的语义</a>\"。</p><p>&nbsp;</p><p></p><h2>征服泄露的观察者</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff5538449eb88881857e58a6dbe90c91.jpeg\" /></p><p></p><p>细粒度反应性是<a href=\"https://en.wikipedia.org/wiki/Observer_pattern\">四人组（Gang of Four）观察者模式</a>\"的变种。虽然观察者模式是一个强大的同步模式，但是它也有一个典型的问题。一个Signal会保持对所有订阅者的强引用，所以长期存活的Signal会保留所有的订阅，除非进行手动处置。</p><p>&nbsp;</p><p>这种记录方式在大量使用时会变得很复杂，尤其是在涉及嵌套的时候。在处理分支逻辑和树的时候嵌套很常见的，就像在构建UI视图时的那样。</p><p>&nbsp;</p><p>有一个鲜为人知的库，叫做<a href=\"https://github.com/adamhaile/S\">S.js</a>\"（2013）提供了答案。S是独立于其他大多数解决方案而开发的，它更直接地以数字电路作为模型，所有的状态变化都在时钟周期内进行。S将其状态基元称为“Signals”。尽管它不是第一个使用该名字的，但它是我们今天使用该术语的起源。</p><p>&nbsp;</p><p>更为重要的是，它引入了反应式所有权的概念。所有者会收集所有的子反应式作用域，并在所有者处置（disposal）自身或重新执行时，管理子反应式作用域的处置。反应式图会从一个根所有者开始，然后每个节点均作为它所拥有的后代。这个所有者模式不仅对处置过程很有用处，而且在反应式图中，建立了一种提供者/消费者（Provider/Consumer）上下文的机制。</p><p>&nbsp;</p><p></p><h2>调度</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a62b776042547cc725d010588aa9a2a.png\" /></p><p></p><p></p><p><a href=\"https://vuejs.org/\">Vue</a>\"（2014）也为我们今天的发展做出了巨大的贡献。除了在优化一致一致性方面与MobX的节奏保持一致之外，Vue从一开始就将细粒度反应性作为其核心。</p><p>&nbsp;</p><p>虽然Vue和React都使用了虚拟DOM，但是Vue的反应性得到了最好的支持，这意味着它是与框架一起研发的，首先是作为内部机制，为其Options API提供支持，在过去的几年中，它成为了Composition API（2020）的前沿和核心。</p><p>&nbsp;</p><p>Vue将推送/拉取向前推进了一步，能够调度任务何时会完成。默认情况下，Vue会收集所有的变更，但是下一个微任务在处理作用（effect）队列之前不会处理它们。</p><p>&nbsp;</p><p>然而，这种调度也可以用来做其他的事情，比如keep-alive和Suspense。甚至像<a href=\"https://github.com/ryansolid/solid-sierpinski-triangle-demo\">并发渲染</a>\"这样的功能也可以用这种方式来实现，从而充分体现了如何同时利用基于推送和拉取的方式能够达成的最佳效果。</p><p>&nbsp;</p><p></p><h2>编译</h2><p></p><p>&nbsp;</p><p>在2019年，<a href=\"https://svelte.dev/blog/svelte-3-rethinking-reactivity\">Svelte 3</a>\"向我们展示了利用编译器能够完成多少事情。实际上，他们将反应性完全编译掉了。在这过程中，也会有一些权衡，Svelte向我们展示了编译器如何抹平人类工程学方面的欠缺。这将会成为一种趋势。</p><p>&nbsp;</p><p>反应式语言（如状态、衍生状态、作用）不仅向我们描述了用户界面等同步系统所需的所有内容，而且它是可分析的。我们可以精确地知道都发生了哪些变更以及它们发生在什么地方。可追溯性的潜力是很深远的。</p><p>&nbsp;</p><p>来自<a href=\"https://preactjs.com/\">Preact</a>\"团队的Marvin Hagemeister<a href=\"https://twitter.com/marvinhagemeist/status/1625428781199421440\">在Twitter这样说到</a>\"，“我认为这是基于Signals的方式优于钩子（hook）的主要原因之一。它能够使我们添加更多的调试洞察力，这是钩子所无法实现的，比如准确地显示一个状态发生变更的原因。”</p><p>&nbsp;</p><p>如果能够在编译时知道这一切，我们就可以交付更少的JavaScript代码。对于代码的加载，我们会有更高的自由度。这就是<a href=\"https://www.builder.io/blog/hydration-is-pure-overhead\">Qwik</a>\"和<a href=\"https://dev.to/ryansolid/what-has-the-marko-team-been-doing-all-these-years-1cf6\">Marko</a>\"的可恢复性的基础。</p><p>&nbsp;</p><p></p><h2>面向未来的Signals</h2><p></p><p>&nbsp;</p><p>Angular团队的成员Pawel Kozlowski则<a href=\"https://twitter.com/pkozlowski_os/status/1573774557995044864\">认为</a>\"：</p><p>&nbsp;</p><p></p><blockquote>“Signals是新的VDOM。&nbsp;人们对它的兴趣正在爆发：很多人正在尝试一些新东西。这将使我们能够探索该领域，尝试不同的策略，对其增进了解和优化。&nbsp;虽然现在不知道最终结果是什么，但是这种集体探索是很好的！”</blockquote><p></p><p>&nbsp;</p><p>鉴于这项技术已经非常古老，说还有很多东西需要探索，这可能会令人感到惊讶。但是，这里的原因在于，它是一种对解决方案进行建模的方式，而不是一种具体的方案。它所提供的是一种描述状态同步的语言，与要让它执行的副作用完全无关。</p><p>&nbsp;</p><p>因此，它能够被Vue、Solid、Preact、Qwik和Angular采用似乎并不足为奇。我们已经看到它进入了Rust的Leptos和Sycamore，表明DOM上的WASM<a href=\"https://twitter.com/RyanCarniato/status/1580347110611816448\">不一定会慢</a>\"。React甚至考虑在底层使用它。</p><p>&nbsp;</p><p>来自React核心团队的Andrew Clark<a href=\"https://twitter.com/acdlite/status/1626590880126889984\">表示</a>\"：</p><p>&nbsp;</p><p></p><blockquote>“我们可能会在React中添加一个类似Signals的基元，但我并不认为这是一个编写UI代码的好方法。它对性能来说是很好的。但我更喜欢React的模式，在这种模式下，你每次都会假装重新创建所有的内容。我们的计划是使用一个编译器来实现与之相当的性能”。</blockquote><p></p><p>&nbsp;</p><p>也许这是一种合适的方式，因为React的虚拟DOM始终只是一个实现细节。</p><p>&nbsp;</p><p>Signals和反应性语言似乎是一个交汇点。但是，这在JavaScript诞生之初却并不那么明显。也许这是因为JavaScript并不是最好的语言。我甚至可以说，长期以来，我们在前端框架设计中感受到的很多痛苦都是语言本身的问题。</p><p>&nbsp;</p><p>无论这一切的结局如何，到目前为止，都是一次相当不错的旅程。有这么多人关注Signals，我迫不及待地想知道我们的下一步会是什么。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://dev.to/this-is-learning/the-evolution-of-signals-in-javascript-8ob\">https://dev.to/this-is-learning/the-evolution-of-signals-in-javascript-8ob</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/958fbe57ff88cdba990d99739\">手写一个 react，看透 react 运行机制</a>\"</p><p><a href=\"https://xie.infoq.cn/article/3e10ee935ffd1b23b1ecd8842\">看透 react 源码之感受 react 的进化</a>\"</p><p><a href=\"https://xie.infoq.cn/article/7baec545b8202471064494a69\">2023 重学 Angular</a>\"</p><p><a href=\"https://xie.infoq.cn/article/118bc90c00b5b7b4faf3cd774\">初识 VUE 响应式原理</a>\"</p>",
    "publish_time": "2023-04-10 12:44:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]