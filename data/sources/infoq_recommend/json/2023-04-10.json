[
  {
    "title": "Docker+Wasm第2个技术预览版发布，新增3个运行时引擎支持",
    "url": "https://www.infoq.cn/article/1WdmawdEDTcLLLdwq7tg",
    "summary": "<p>Docker<a href=\"https://www.docker.com/blog/announcing-dockerwasm-technical-preview-2/\">宣布</a>\"了Docker+Wasm的第2个技术预览版，旨在简化Wasm工作负载的运行，并扩展运行时支持，包括<a href=\"https://www.infoq.com/news/2022/11/Fermyon-cloud-webassembly/\">Fermyon的spin</a>\"，<a href=\"https://deislabs.io/tags/slight/\">Deislabs的slight</a>\"和<a href=\"https://www.infoq.com/news/2020/10/bytecode-alliance-one-year/\">Bytecode Alliance的wasmtime</a>\"运行时引擎。</p><p>&nbsp;</p><p>Docker+Wasm中新增支持的3个Wasm引擎使其支持的运行时总数达到了4个，其中包括在<a href=\"https://www.infoq.com/news/2022/11/docker-webassembly/\">Docker+Wasm第1个技术预览版</a>\"中已经支持的WasmEdge。它们都基于runwasi库。<a href=\"https://www.infoq.com/news/2023/02/containerd-wasi/\">该库最近加入了containerd 项目</a>\"。</p><p>&nbsp;</p><p>runwasi是一个Rust库，它支持运行通过containerd管理的wasm工作负载。因此，除了containerd最初支持的Linux容器之外，它还为新的容器类型创建了一个有效的抽象。顾名思义，runwasi是基于WASI的。WASI是WebAssembly的模块化系统接口，为Wasm运行时提供了一个公共平台。也就是说，如果一个程序被编译为目标WASI，那么它就可以在任何符合WASI标准的运行时上运行。</p><p>&nbsp;</p><p>通常，Wasm容器只包含一个编译好的Wasm字节码文件，而且不需要任何额外的二进制库，这使得容器小很多。这也意味着Wasm容器的启动速度通常比Linux容器更快，而且可移植性更好。例如，正如WasmEdge联合创始人<a href=\"https://twitter.com/juntao/status/1620547819546480640?s=20\">Michael Yuan在Twitter上所说的那样</a>\"，Linux上“最小”的Python容器镜像超过40MB，而<a href=\"https://twitter.com/juntao/status/1620547819546480640\">对应的Wasm容器镜像不到7MB</a>\"。</p><p>&nbsp;</p><p>作为containerd直接支持的Wasm容器，要在Docker Desktop的最新版本中尝试Docker+Wasm的第2个技术预览版，唯一需要做的事情是启用开发 &gt; Settings &gt; Features下的“Use containerd”选项。</p><p>&nbsp;</p><p>使用wasmtime运行Wasm容器，可以执行以下命令：</p><p><code lang=\"shell\">$ docker run --rm --runtime=io.containerd.wasmtime.v1 \n--platform=wasi/wasm secondstate/rust-example-hello:latest</code></p><p>&nbsp;</p><p>正因为如此，借助Docker Compose或其他编排平台（如Kubernetes），Wasm容器可以与Linux容器并行运行。此外，通过在OCI容器中嵌入Wasm运行时，Docker Desktop还可以将Wasm应用程序打包到OCI容器中，从而可以通过DockerHub等容器注册中心来共享Wasm应用程序。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/03/docker-wasm-containers-preview-2/\">https://www.infoq.com/news/2023/03/docker-wasm-containers-preview-2/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/XNAGHzotjohTSqoWDE9S\">部署太慢，我们用 Warm Docker 容器将速度提高了 5 倍</a>\"</p><p><a href=\"https://www.infoq.cn/article/8lGiLHiAfbyateIF9Blr\">Docker正在淘汰开源组织，CTO硬刚开发者，网友：想赚钱可以，但沟通方式烂透了</a>\"</p>",
    "publish_time": "2023-04-10 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "透明代码大页：让数据库也能用上 2MB 大页！",
    "url": "https://www.infoq.cn/article/QPQ8TugsDmbkraE7Wfe8",
    "summary": "<p></p><h2>背景</h2><p></p><p></p><p>大页技术是操作系统中优化内存访问延迟的一种技术，其优化原理与 CPU TLB 硬件有直接关系，而其优化效果不仅受 CPU TLB 硬件影响，还需要看应用访存特点。只考虑 Arm 和 x86 两种平台，已知的大页技术包括透明大页、hugetlbfs、16k 和 64k 全局大页。在合适的场景，大页技术可以提升应用性能达 10% 以上，尤其是针对当前云上应用逐年增长的内存使用趋势，使用大页技术是其中重要的提升“性能-成本”比例的优化手段。</p><p></p><p>透明大页（Transparent Huge Pages，THP）从 2011 年开始在 Linux 内核中已经支持起来，其通过一次性分配 2M 页填充进程页表，避免多次缺页开销，更深层次从硬件角度优化了 TLB 缺失开销，在最好情况下，对应用的优化效果达到 10% 左右。除以上优点外，透明大页（主要供堆栈使用）使用过度也会导致严重的内存碎片化、内存膨胀和内存利用率低等问题，这就是当前透明大页没有在数据库中使用的核心原因，只能感叹“卿本巧技，奈何有坑”。</p><p></p><p>代码大页在透明大页的基础上，将支持扩展到可执行二进制文件，包括进程二进制文件本身、共享库等可执行数据。与透明大页相比，由于代码大页仅将占比较低且有限的可执行文件页部分转换为大页，从根本上避开了内存碎片以及内存不足的问题。与此同时，由于代码类数据和普通堆栈数据访问热度对整体性能影响不同（主要指代码数据或堆栈数据访问缺页一次的性能影响），导致代码类数据使用大页所提升的性能远大于同样分量的透明大页。所以推广和完善代码大页相比透明大页更加简单和容易。</p><p></p><p>本文主要介绍我们的代码大页方案以及一些实验阶段性能测试。为了方便阅读，在这里简单归纳了一下 Linux 系统中大页的支持现状和和必要的数据库相关背景。</p><p></p><h2>大页现状</h2><p></p><p></p><p>当前 Linux 内核支持的大页包括 THP 和 hugetlb，其页大小分别是：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c3/36/c370b2d637116e0b6c6da03ec6c37736.png\" /></p><p></p><p>不考虑其他架构，在 x86 和 Arm 架构中，提到 THP，我们可以一股脑地认为其大小就是 2MB，当前内核暂时还不支持 1GB THP，技术上实现没有什么问题，社区隐隐约约也有人曾经发过相关补丁。回到这里，Arm64 相比 x86，hugtlb 多两种大页支持，主要是 contiguous bit，该特性主要是针对 TLB entry 的优化（连续的 16 个 PTE/PMB，若其上的 PFN 也是连续的，cont bit 会将其使用的 16 个 TLB entry 优化仅占一个 TLB entry）。这里的 64KB 和 32MB 的由来就是 16*4KB 和 16*2MB。</p><p></p><p>另外，在全局页粒度的支持上，Arm64 也比 x86 更会玩，提供的 16KB&nbsp;（CONFIG_16KB 表示）和 64KB（CONFIG_64KB表示）两种选择，这两种页相比 4KB 页，在 TLB 和 cache 上都有明显的优化相关，从而优化宏观指标，当然也并非所有 benchmark 表现出性能提升，例如在 SPECjvm2008 和 stream 中，我们就发现有多项指标在使用 CONFIG_64KB 的时候有较严重的性能下降。除以上问题，还想再啰嗦一下，CONFIG_64KB 中，THP 的大小着实有点“吓人”，有 512MB。</p><p></p><p>因此，大伙对 16KB、64KB 还是又爱又恨。</p><p></p><h2>Mysql、PostgreSQL 和 OceanBase</h2><p></p><p></p><p>站在内存管理的角度，我们仅仅关心 Mysql、PostgreSQL 这些数据库用了多少内存、页缓存占多少、匿名页占多少以及代码段还有 iTLB/dTLB miss 到底高不高。当然还随便想知道 THP 有没有优化，下面几个是简单归纳的几点我们关系的数据库特点：</p><p></p><h4>Mysql</h4><p></p><p></p><p>Mysql 是一个多线程模式的数据库，其代码段大小一般 18M 左右。THP 不敏感，打开 THP，大约仅有不到 3% 的性能提升。跨 NUMA 敏感，本地虚拟机 32 核验证跨 NUMA 抖动在 5~7% 左右。</p><p></p><h4>PostgreSQL</h4><p></p><p></p><p>多进程模型，代码段大小大约 10M 左右。应用 iTLB-load-misses 较高，大约 1.41% 左右。</p><p></p><h4>OceanBase</h4><p></p><p></p><p>多线程模型，代码段大小打印 200M~280M。一般独占单机使用，性能验证过程中并发数要求高：128、1000、1500。THP 本地验证不敏感。</p><p></p><p>这些数据库大约至少有两个共同点：代码段大、iTLB Miss 高。本文也是基于这两个特征进行的优化，当然代码大页优化目标也不局限于这三种数据库。</p><p></p><h2>代码大页</h2><p></p><p></p><p>接着前面数据库背景介绍，这里直接开始代码大页方案。</p><p></p><p>代码大页大致实现分为：整理结构、大致实现、填充功能简介还有最后的代码大页性能评估（包括 Mysql 和 PostgreSQL），最后是我们专门为解决 x86 平台设计的自适应功能。</p><p></p><h3>整体结构与实现</h3><p></p><p></p><p>基于透明大页异步整合大页（主要指 khugepaged 内核线程）的框架：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/51362a6959188b61da0b9656dc994bb0.png\" /></p><p></p><p>上图所展示的代码大页方案主要包括三个部分：</p><p></p><p>（1）映射首地址对齐（蓝色高亮）：这个部分主要是在 elf binary 和 DSO 建立映射的过程中，优先考虑分配 2M 对齐的虚拟地址空间，便于映射到 2M 大页。</p><p></p><p>（2）异步 khugepaged 扫描整合以及加速（橙色高亮）：与 THP 相似，单独设计用户态接口 hugetext_enabled 控制。复用 khugepaged 整合 2M 大页。此外，由于 hugetext 与 THP 共用 khugepaged，在 THP=always 时，也能整合部分符合条件的代码大页；关于加速部分，我们解决了在 THP 使能的场景中，代码段整合慢的问题，这也是我们改进 READ_ONLY_THP_FOR_FS 带来的挑战之一。</p><p></p><p>（3）DSO写回退（紫色高亮）：对于 DSO 所建立的映射，内核屏蔽了 MAP_DENYWRITE，导致用户态可以写打开共享库文件（尽管一旦对该共享库写，进程多数情况会 core dump）。针对这种情况，在检测到该共享库存在写者时，对其 pagecache 进行清空；DSO 为什么有这么多顾虑可以跳转：</p><p>https://developer.aliyun.com/article/863760</p><p></p><p>注意：首地址 2M 对齐的本意是 mmap addr = mmap pgoff (mod 2M)，由于 elf binary 和 DSO 的可执行 LOAD 段的 pgoff 一般为 0，这里为了叙述方便，我们简称地址 2M 对齐。</p><p></p><p>另外，在开发和测试过程中，我们的实现方案解决了 file THP 相关的三个 bug 的补丁，Linux 社区已经合入，glibc 社区也合入了一个 p_align 修复补丁。</p><p></p><p>除以上部分，我们目前已经完成的自适应代码大页已经完成，主要解决 x86 平台使用代码大页过多的问题。详细见“自适应处理”一节。</p><p></p><h4>代码段填充功能</h4><p></p><p></p><p>前面，我们已经描述代码大页方案支持动态链接库（DSO）和二进制文件本身，对于 libhugetlbfs 方案，其仅仅支持二进制文件本身的大页转换，虽转换比较完全，但是它无法同时让 DSO 使用大页。显而易见，与 libhugetlbfs 相比，我们提供的 hugetext_enabled 方式更加完整，性能优势更大。填充功能主要是我们为弥补在某些场景中，其大量的热点发生在代码段的尾部，libhugetlbfs 天生可以做到，所以我们也不得不解决这个问题。</p><p></p><p>言归正传，回到代码段填充。首先用一张图来表示代码段填充具体是什么。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/197aa59b8e2813be499d798b0f6f2138.png\" /></p><p></p><p>大多数应用并不能完全保证其代码段都会 2M 对齐，如上图所示，一个应用第一个 r-xp 有 3.2M，其中 2M 我们的 hugetext_enabled 本身即支持，后面的 1.2M 我们不得不向后填充这个 vma，保证其映射大小 2M 对齐，其填充的内容其实就来自于下一个 r--p 的数据（r-xp 和 r--p 在 ELF 磁盘中是两个相邻的 PT_LOAD 段，具体可以通过 readelf 观察）。</p><p></p><p>当然我们在填充过程中会判断填充后大小不会超过原文件大小和不会与下一个 r--p 属性的 vma 重叠。下面是我们新引入的代码大页填充使能开关，例如将 0x1000 写入 hugetext_pad_threshold，表示需填充内容超过 4k 时填充功能才会使能。</p><p></p><p><code lang=\"null\">/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</code></p><p></p><p>最后再夹带一点私货，以上代码段填充主要是在不重新编译应用程序的情况的一种内核方式，还有一种方式是在应用的链接脚本中加入“. = ALIGN(0x200000)”，将代码段直接按照 2M 对齐，如此不需要填充或进行填充更加安全。例如以一个简单的测试程序 align.out 为例。设置链接脚本后 Section 出现对齐：</p><p></p><p><code lang=\"null\">$&nbsp;readelf&nbsp;-l&nbsp;align.out\n\nElf&nbsp;file&nbsp;type&nbsp;is&nbsp;EXEC&nbsp;(Executable&nbsp;file)\nEntry&nbsp;point&nbsp;0x400520\nThere&nbsp;are&nbsp;9&nbsp;program&nbsp;headers,&nbsp;starting&nbsp;at&nbsp;offset&nbsp;64\n\nProgram&nbsp;Headers:\n&nbsp;&nbsp;Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Offset&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VirtAddr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PhysAddr\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FileSiz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MemSiz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Flags&nbsp;&nbsp;Align\n&nbsp;&nbsp;PHDR&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000040&nbsp;0x0000000000400040&nbsp;0x0000000000400040\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x00000000000001f8&nbsp;0x00000000000001f8&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x8\n&nbsp;&nbsp;INTERP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000238&nbsp;0x0000000000400238&nbsp;0x0000000000400238\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x000000000000001b&nbsp;0x000000000000001b&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x1\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Requesting&nbsp;program&nbsp;interpreter:&nbsp;/lib/ld-linux-aarch64.so.1]\n&nbsp;&nbsp;LOAD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000000&nbsp;0x0000000000400000&nbsp;0x0000000000400000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000200194&nbsp;0x0000000000200194&nbsp;&nbsp;R&nbsp;E&nbsp;&nbsp;&nbsp;&nbsp;0x200000\n&nbsp;&nbsp;LOAD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000400000&nbsp;0x0000000000a00000&nbsp;0x0000000000a00000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000010040&nbsp;0x0000000000200008&nbsp;&nbsp;RW&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x200000\n&nbsp;&nbsp;DYNAMIC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000400010&nbsp;0x0000000000a00010&nbsp;0x0000000000a00010\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x00000000000001d0&nbsp;0x00000000000001d0&nbsp;&nbsp;RW&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x8\n&nbsp;&nbsp;NOTE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000254&nbsp;0x0000000000400254&nbsp;0x0000000000400254\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000044&nbsp;0x0000000000000044&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x4\n&nbsp;&nbsp;GNU_EH_FRAME&nbsp;&nbsp;&nbsp;0x000000000020004c&nbsp;0x000000000060004c&nbsp;0x000000000060004c\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x000000000000004c&nbsp;0x000000000000004c&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x4\n&nbsp;&nbsp;GNU_STACK&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000000&nbsp;0x0000000000000000&nbsp;0x0000000000000000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000000000&nbsp;0x0000000000000000&nbsp;&nbsp;RW&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x10\n&nbsp;&nbsp;GNU_RELRO&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000400000&nbsp;0x0000000000a00000&nbsp;0x0000000000a00000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x0000000000010000&nbsp;0x0000000000010000&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0x1\n\n&nbsp;Section&nbsp;to&nbsp;Segment&nbsp;mapping:</code></p><p></p><p>从上面 readelf 文件输出可以看出：两个 PT_LOAD的offset 已经按照 2M 对齐，并且两个 PT_LOAD 完全分布到不同的 2M 页中，如此，若进行填充，完全不需要用后一个 PT_LOAD 数据。测试方法：导出默认并修改默认的lds文件，在代码和数据段前加上\". = ALIGN(0x200000);\"即可。备注：“gcc -Wl,--verbose”可查看默认的链接脚本。另外，在编译过程中需要加上“-z max-page-size=0x200000”选项，否则对齐会被约束为 4K 或 64K，如下：</p><p></p><p><code lang=\"null\">$&nbsp;gcc&nbsp;main.c&nbsp;test.c&nbsp;-o&nbsp;align.out&nbsp;-Wl,-T&nbsp;ld-align.lds&nbsp;-z&nbsp;max-page-size=0x200000</code></p><p></p><p>链接脚本加 hugetext_pad_threshold 结合使用，可以让应用对代码大页使用更加友好。</p><p></p><h3>快速试用</h3><p></p><p></p><p>为了方便业务使用，我们扩展了两种打开方式：启动参数和 sysfs 接口。</p><p></p><p>启动参数</p><p></p><p>打开：hugetext=1 or 2 or 3</p><p>关闭：缺省即为关闭</p><p></p><p>sysfs 接口</p><p></p><p>仅打开二进制和动态库大页：echo 1 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>仅打开可执行匿名大页：echo 2 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>打开以上两类大页：echo 3 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>关闭：echo 0 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled</p><p>查看/proc//smaps中FilePmdMapped字段可确定是否使用了代码大页。</p><p>扫描进程代码大页使用数量（单位 KB）：</p><p>cat /proc//smaps | grep FilePmdMapped | awk '{sum+=$2}END{print\"Sum= \",sum}'</p><p></p><p>padding 接口</p><p></p><p>/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</p><p>echo [0~2097151] &gt; &nbsp;/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</p><p>当二进制文件末尾剩余 text 段由于不足 2M 而无法使用大页，当剩余 text 大小超过 hugetext_pad_threshold 值，可将其填充为 2M text，保证可使用上大页。hugetext_pad_threshold=0，表示填充功能关闭，该接口依赖 hugetext_enabled 接口。</p><p>建议一般情况写 4096 即可：echo 4096 &gt; &nbsp;/sys/kernel/mm/transparent_hugepage/hugetext_pad_threshold</p><p>当然，如果想完全回退代码大页对应用的影响，可以采用下面的回退方式：</p><p>在打开 hugetext_enabled 后，若关闭 hugetext_enabled 并且完全消除 hugetext_enabled 影响，可以下面几种方式：</p><p></p><p>清理整个系统的page cache：echo 3 &gt; /proc/sys/vm/drop_caches清理单个文件的page cache：vmtouch -e //target</p><p></p><p>清理遗留大页：echo 1 &gt; /sys/kernel/debug/split_huge_pages</p><p></p><h2>代码大页性能评估</h2><p></p><p></p><h3>Mysql 性能评估</h3><p></p><p></p><p>为了充分挖掘代码大页的收益，我们针对不同的平台，包括 x86、Arm，分别对 mysql、python 以及 jvm 等应用进行了测试，由于数据太多、混乱，我们挑选出 mysql 上的测试数据，测试数据包括 TPS、QPS 以及 iTLB miss 数据。</p><p></p><p>本文数据的测试环境：</p><p></p><p>5.10（alios 5.10-002）mysql 版本：MariaDB-10.3.28虚拟机配置：32 核、128G 内存物理机配置：打开透明大页</p><p></p><p>由于在 Arm 平台和 x86 平台测试的数据指标和测试方法相同，所以我们挑选出 Arm 篇对这些数据进行了较详细的描述和分析，在 x86 篇中，仅仅简单描述数据统计图中代码大页与 4k 代码页的性能差异。</p><p></p><h4>Arm 篇</h4><p></p><p></p><p>下面展示在 Arm 平台上，代码大页对 mysql 的性能提升。</p><p></p><p>测试过程中，mysql 并发数分别是 1、8（25%）、16（50%）、32（100%）。测试结果与普通的 4K 代码页数据对比如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4a/4ad78e7de7d55875d60f3ac0b6eda4b1.png\" /></p><p></p><p>上图 TPS 对比可以看出：代码大页的性能始终高于普通的代码页。关于这张图中其他的结论，还有：</p><p></p><p>并发数为 1 时，外在的影响因素最小，此时，代码大页相比普通代码页，性能提升大约在 6.9%。并发数 8、16 基本可以保证没有 CPU 的竞争，代码大页的性能提升大约也在 6.5% 以上。并发数32时，由于总核数为 32，存在于其他应用竞争 CPU，所以 TPS 较低于前面的测试结果。但是代码大页的鲁棒性更好，此时相比普通代码页，性能提升大约在 11% 左右。</p><p></p><p>另外，还有 RT 的对比，如下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/7914daa67115e3003115c3f15ac4bbee.png\" /></p><p></p><p>RT 的数据主要是前 95% 的请求的最大响应时间（95th percentile）。代码大页在 RT 上的表现与图 1 一致，大约提升在 5.7%~11.4% 之间。</p><p></p><p>最后的展示的数据是微架构数据，我们在分析代码大页对性能的影响过程中，主要观察 iTLB miss，如图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8b/8bf6012f49e9071c87fa5fc769a62315.png\" /></p><p></p><p>图 5（a）和图 5（b）都展示的是 iTLB 的数据，但是考虑到不同读者的偏好，我们在测试的过程中 iTLB miss 和 iTLB MPKI 也一并记录。如这两图所示：</p><p></p><p>mysql 使用代码大页后，iTLB miss 大约下降了 10 倍左右，数值大小从原来的 1% 下降到 0.08% 左右。在 iTLB MPKI 上，大约下降了 6 倍左右。</p><p></p><p>经过上述数据的对比，以及我们在实验阶段进行的 THP（这里主要指 anon THP）数据对比， 在 mysql 场景下，大致可以得出一个简单的公式：</p><p></p><p>收益：代码大页 &gt; anon THP &gt; 4k</p><p></p><h4>x86 篇</h4><p></p><p></p><p>与上一小节相同，这里分别对代码大页和 4k 代码页进行 TPS、RT 对比、iTLB miss 对比。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/22/225ce1c8e522b3deb89ea6d3aa952c03.png\" /></p><p></p><p>由于 TLB 硬件的差异，代码大页在 x86 和 Arm 上性能提升存在差异，根据图 6（a、b）中的测试数据，代码大页在 x86 上，对 TPS 的提升大致在 3%~5 之间，在 RT 上大致有 5%~7.5% 的收益。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b87adeb7571e174bbdd373d5af5b8f7c.png\" /></p><p></p><p>在图 7 中，可以看出：</p><p></p><p>从 iTLB miss 的数据看，代码大页相比 4k 代码页大约下降了 1.5~2.3 倍左右。从 iTLB MPKI 的数据看，代码大页相比4k代码页大约下降了1.6~2.3倍之间，与iTLB miss效果相似。</p><p></p><p>到这里，我们结束对代码大页性能数据的展示。关于在物理机的测试，感兴趣的读者可以自行在&nbsp; alios 5.10-002 上进行测试。</p><p></p><h3>PostgreSQL 性能评估</h3><p></p><p></p><p>代码大页相比 libhugetlbfs 方案，不仅仅解决了使用 libhugetlbfs 后无法使用 perf 观察热点问题，这里在 PostgreSQL 上 padding 功能派上了作用，最终代码大页性能提升较 libhugetlbfs 多 2% 左右。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04e57f21f771cbc03ce614243873eb27.png\" /></p><p></p><p>上述测试数据主要在 Arm 环境中。</p><p></p><h3>小结</h3><p></p><p><img src=\"https://static001.geekbang.org/infoq/95/95e54e99754ada46ad5d8b075af73fe1.png\" /></p><p></p><p>注：以上主要为 Arm 平台测试数据。符号 x 可以认为该应用没有采用该技术，或优化效果几乎是 0。</p><p>最后归纳一下，归根结底，代码大页对性能的提升也是采用的内存领域常用方法：热点+大页。</p><p></p><h2>自适应代码大页</h2><p></p><p></p><p>自适应代码大页在代码大页的基础上，考虑到 x86 平台上 2MB iTLB entry 不足的问题，进行的“热点采集+大页整合”的大页使用数量限制策略，该特性基本解决了代码大页在某些特大代码段上的负优化问题。</p><p></p><p>云上JVM类应用大约占比 60% 以上，这类应用在阿里内部大约可发现所使用的 code cache 大约有 150M~400M 情况，这类 code cache 是比较特殊的匿名页，属于 JIT 热点代码缓存在此处，属于特殊的代码数据。对于 Arm 平台，其 TLB 硬件社区并未区域 2M 或 4k 的页表项，而对于 x86 平台，其 TLB 命中有页大小要求，例如：4k 页只能使用 4k TLB entry、2M 页使用 2M TLB entry。这种设计要求应用本身使用的页表不会超过硬件支持，否则易出现严重的 TLB 未命中情况，导致性能骤降。</p><p></p><p>当前我们对云上常用的 x86 平台情况进行了一个调研，主要包括以下三类服务器在役：</p><p></p><p></p><p></p><p>从上面的数据可以看出，当前在 x86 平台，基本无法使用代码大页，2M code TLB 资源使用率基本为 0。下面我们以蚂蚁的 Java 的业务为例说明由于 TLB 资源匮乏导致的性能问题。在蚂蚁的 Java 业务总通过 hugetext 让 code cache 使用大页，出现性能回退：iTLB miss 上升 16% 左右，CPU 利用率上升 10% 左右。其原因可以确定在于 code cache 大约 150M，需要覆盖 70 多个 2M iTLB entry，而当前蚂蚁环境使用的机器基本都是 Intel 机器，以 skylake 为例，仅仅 82M iTLB entry，造成 2M iTLB entry 竞争激烈。</p><p></p><p>下表为测试的代码大页负优化数据：</p><p></p><p></p><p></p><p>上面的数据表示在打开 hugetext 后，icache_64b.iftag_stall 反而上升了 16% 左右，另外在 STLB 命中的比例上升了 0.48%，造成的结果就是 CPU 利用率上升 10% 左右。</p><p></p><p>将上面描述的问题可以简单总结为以下：</p><p></p><p>（1）Intel 机器普遍存在大页 iTLB entry 数量较少的问题，大多数服务器集中在 8~16 之间。当前 Arm 平台由于不区分 4k 和 2M iTLB，尚未发现类似问题。</p><p>（2）诸如此类，其他业务，如 flink、容器场景，代码大页使用过多造成上述问题。</p><p></p><p>以 Intel skylake 为例，iTLB entry 信息：</p><p></p><p></p><p></p><p>另外，也发现 STLB 命中后，替换 L1 iTLB entry 的惩罚较重，大约占 22 cycles。因此需要避免代码在 STLB 命中。简而言之，在 Intel 系列普遍存在 2M iTLB entry 有限的问题，是代码大页在此类平台性能提升不明显或存在负优化的核心原因。</p><p></p><p>针对上面描述的问题，自适应代码大页采用限制大页使用数量的方式，将较热点的数据整合为大页。在使用时，仅需在用户态设置自适应大页的数量接口。自适应代码大页处理流程如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/089dda4ca29dc66c06eb99a578b93a9f.png\" /></p><p></p><p>使用该方案，可以解决代码大页在 x86 平台上负优化的问题，可以应用到 JVM 类应用和 oceanbase 数据库上。</p><p></p><p>自适应代码大页使用接口：/sys/kernel/mm/transparent_hugepage/khugepaged/max_nr_hugetext，只要为该接口设置一个值，就可以约束单个应用使用的最多代码大页数量，且不会影响 THP 的正常逻辑。</p><p></p><h2>除了大页，代码段优化还有什么</h2><p></p><p></p><p>代码段的优化除了大页的方案，业内还包括 PGO/autoFDO、LTO 等。</p><p></p><p>按代码段优化时间，可以将当前在编译器和链接器所做优化，简单分为：post-link optimization、link-time optimizations 以及比较混合的 BOLT optimization，如下。这些优化都是对应用代码段布局的优化。</p><p></p><p>feedback-driven optimizations (FDO) or called profile-guided optimizations (PGO)（一般用 PGO 统称）sample-based profilinginstrumentation-based profiling （基本都是线下训练）hardware-event sampling e.g. LBR (e.g. SampleFDO, AutoFDO)profile-guided function reordering algorithmPettis-Hansen (PH) heuristic (weight dynamic call-graph, instrumentation-based profiling)Call-Chain Clustering (C3) heuristic (weight and directed call-graph, hardware-event sampling or stack traces sampling)link-time optimizations (LTO, [27, 28])only depend on function reordering algorithm (PH heuristic, C3 heuristic)post-link optimization (e.g. Ispike Spike [5], Etch [6], FDPR [7])BOLT optimizations （大杂烩）</p><p>（不需要看懂，只要知道有 autoFDO 和 BOLT 就行）</p><p>所以这个东西干嘛用的？举一个简单例子，实验课上老师给大家一段排序的代码，让大家进行优化，将排序的时间开学降低 10 倍，这个时候就可以用上面的工具，再加上简单的 perf 使用，而不需要看具体的代码实现。</p><p></p><h2>代码大页支持产品</h2><p></p><p></p><p>当前，代码大页已经支持龙蜥操作系统（Anolis OS）和阿里云 ECS 支持，参考链接如下：</p><p></p><p>龙蜥操作系统（Anolis OS）：https://gitee.com/anolis/cloud-kernel</p><p></p><p>龙蜥操作系统 4.19 和 5.10 最新版本默认打开 CONFIG_HUGETEXT（代码大页的主要配置）。“代码大页性能评估”一节的数据可以在龙蜥操作系统 5.10 内核复现，使用镜像可选择：</p><p></p><p></p><p></p><blockquote>镜像缺省 sudo 用户为 anuser，对应登录密码是 anolisos。链接：https://docs.openanolis.cn/products/anolis/rnotes/anolis-8.8.html</blockquote><p></p><p></p><p>阿里云 ECS</p><p></p><p>若使用环境为阿里云 ECS，可参考：</p><p>https://help.aliyun.com/document_detail/462660.html&nbsp;使用代码大页。</p><p></p><h2>后续展望</h2><p></p><p></p><p>最后头脑风暴一下，代码大页也许还可以：</p><p></p><p>代码只读文件系统，专门用于放置 lib 库。64kB、32MB 代码大页，接近特殊场景 2MB 代码大页无法使用的问题。多种代码大页支持。靶向代码大页。</p><p></p><p>从 iTLB miss 角度看，这些也许只能算是功能完善，并没有太大的性能优化。</p><p></p><p>参考：</p><p>阿里云 ECS上使用代码大页说明：</p><p>https://help.aliyun.com/document_detail/462660.html</p><p>将 mysql 可执行文件的代码段和数据段加载到大页上：</p><p>https://jira.mariadb.org/browse/MDEV-24051</p><p>社区讨论：</p><p>https://sourceware.org/pipermail/libc-alpha/2021-February/122334.html</p><p>如何将自己的代码段和数据段映射到大页：hugepage_text.cc</p><p>MAP_DENYWRITE：被 Linux 内核屏蔽的 flag</p><p>https://developer.aliyun.com/article/863760</p>",
    "publish_time": "2023-04-10 10:28:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "GPT-4预示着前端开发的终结？你准备好面对无法预测的技术挑战了吗？",
    "url": "https://www.infoq.cn/article/hyRP71z0tdRzvc5ZUtUS",
    "summary": "<p>过去几个月来，我跟很多刚刚步入前端开发领域的朋友们交流过，大家都对步步紧逼的 AI 感到焦虑。他们看到 GPT-4 等工具带来的令人印象深刻的演示，担心自己学成 HTML/CSS/JS 知识之日，就是岗位消失之时。</p><p></p><p>这类情绪绝非个例，Twitter 上早已是哀鸿一片。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/fe/fea8ad278e62afc60d427de1bf937828.png\" /></p><p></p><p></p><blockquote>澄清声明我先向大家承认，这个话题跟我有切身关联：我的工作就是在网上教学员开发软件。如果开发者岗位逐渐消失，那我的饭碗也就完了。但我向大家保证，我在本文中表达的所有观点都出于自己的真实判断。欢迎大家自行斟酌这些观点是否存在偏见。</blockquote><p></p><p></p><p></p><h2>“又”要被替代了</h2><p></p><p></p><p>CSS 语言最初发布于 1996，随 IE 3 一同推出。两年之内，首款“无代码”网站构建器 Homestead 就跟大家见面了。</p><p></p><p>Homestead 能帮助人们在无需编写代码的情况下，轻松构建起自定义网页：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6f/6f0e36a9c76ddfd8a4c808acf2a40ded.png\" /></p><p></p><p>也就是说，从 Web 开发这类岗位刚刚出现时，就有人担心会被某些新技术所淘汰。2000 年代的威胁源头是 WordPress，2010 年代是 Webflow，2020 年代初则是“无代码”工具。</p><p></p><p>其实从某种意义上讲，Web 开发者确实已经过时了。如今，本地面包小店、牙医诊所或者艺术家如果需要一个网站，那大概率不会花几万美元聘请开发者从零开始做设计。他们可以直接前往 SquareSpace，找到自己喜欢的模板，然后每月花 20 美元来运营自己的业务门户。</p><p></p><p>但就算这样，Web 开发者也仍然存在。</p><p></p><p>上星期，OpenAI 公布了 GPT-4，期间有一段令人印象深刻的演示：GPT-4 能够将手绘的网站草图转化成功能齐全的网站，甚至包括一些 JS 来连接“Reveal Punchline”按钮。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/33/332ed0806644b61308e274d5fd235683.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/82/823e7f1d22ef70f74f16c0e3c652c115.png\" /></p><p></p><p>这功能确实很酷，在原型设计方面有很大的潜力……但我们还是得明确一点：几十年来，我们的 Web 开发者负责构建的从来不是这类页面。演示中的 HTML 文档跟目前前端开发者编写的各种代码，可以说是天渊之别。</p><p></p><p></p><h2>展望未来</h2><p></p><p></p><p>到目前为止，我个人看到的大部分演示都非常有限：要么是一个简单的 HTML 页面，要么是 JS 函数。总之，就是那种开发者可以随手搞定的东西。</p><p></p><p>但这一切还仅仅只是开始！如果 AI 继续保持目前的发展速度，再过几年就能构建起完整的应用程序了，对吧？</p><p></p><p>我当然不是什么大语言模型专家。但对于 GPT-4，我完全能够理解它的宏观工作原理。</p><p></p><p>从本质上讲，大语言模型是一种超级强大的文本预测器。给定一个提示，它们就能使用机器学习来尝试找到最可能符合提示要求的字符集。</p><p></p><p>OpenAI 等厂商花费了大量的时间和精力来调整模型，借此改善输出质量。一大群人工标者对模型的输出进行“评级”，借此引导模型不断学习和进化。</p><p></p><p>如果大家体验过 CHatGPT 或者 Bing 的 AI 搜索等工具，就会发现这些回答大概有个 80% 的正确率，但 AI 却表现得像是有 100% 的把握。</p><p></p><p>大语言模型无法验证或者说测试自己的假设，无法确定自己的表达内容是否为真。它们还是在玩概率游戏，猜测给出的字符应该会跟用户在提示中输入的字符相匹配。</p><p></p><p>有时候，AI 甚至会给出非常荒谬的回答。OpenAI 团队将其称为“幻觉”。</p><p></p><p>随着技术进步，预计某些粗糙的问题会被一一克服。但从根本上讲，AI 或多或少还是会存在准确度问题。毕竟 AI 工具没有任何机制能够对自己的响应做出客观验证。</p><p></p><p>因此，准确率可以不断提高，但永远达不到完美。而趋于完美，正是 AI 真正取代 Web 开发者的前提。如果它本身不是程序员，那就无法判断哪些部分是对的、哪些是错的。AI 自己发现不了“幻觉”。</p><p></p><p>但大家别急，我也认真看了 GPT-4 的演示，AI 似乎能够自我纠正！只要复制 / 粘贴错误消息，它就能找到并解决问题。</p><p></p><p>但是……并不是所有幻觉都会引发异常。比如说，我最近用 GPT-4 在 React 上生成了一个〈Modal〉组件，虽然输出非常好，但还是存在一些可访问性错误。创建应用程序的人可能注意不到这些问题，不过最终用户肯定会发现！</p><p></p><p>还有代码中的安全漏洞呢？一旦引发可怕的错误，该由谁来承担责任？</p><p></p><p>还有一点：生成一个 50 行的 HTML 文档，跟输出生产就绪的 Web 应用程序之间根本就不是一回事。像博客这样的小型 JS 应用就有大约 6.5 万行代码，总计 900 多个文件。这还不包含写作文本，单是 JavaScript 和 TypeScript 的体量。</p><p></p><p>也就是说，即使准确率达到 95%，在这样的应用规模下也将很难调试。这相当于一位开发者用几个月时间构建一个巨大的项目，但却从未做过任何代码测试，直到 100% 完成后才加以验证。这不是生产力提升，而是前所未有的噩梦。</p><p></p><p>AI 不是什么神迹和魔法，它的质量直接取决于训练数据。而代码片段在互联网上随处可见，往往高度通用。相比之下，每个代码库都是独一无二的，网上鲜有大型开源代码库。那么，AI 要如何学会构建能够实际应用的大型项目？</p><p></p><p>按现在的发展速度看，很快非开发者就可以直接跟聊天机器人对接，轻松启动一个小型独立项目。也就是说，如今的 Webflow 等工具会被 AI 所替代，而且效果更好、功能更强，这绝对是件大好事！</p><p></p><p>但要让科技大厂们放弃自己的开发队伍，全面选择 AI 工程师？那还有很长的路要走，前面提到的这些潜在问题将严重阻碍这种想象在现实中落地。</p><p></p><p></p><h2>增强，而非替</h2><p></p><p></p><p>说了这么多，我就明确给出自己的态度吧：我其实非常看好 AI 技术 😅。</p><p></p><p>我认为最有可能的情况，是 GPT-4 这类工具被整合进开发者工具当中，用来扩大熟练开发者的生产能力。</p><p></p><p>木匠并没有被电动工具所替代，会计师没有被电子表格替代，摄影师没有被数码相机 / 智能手机替代，我认为开发者也不会被大语言模型替代。</p><p></p><p>我当然不确定未来的开发者职位总量会不会有所下降；毕竟如果每位开发者的效率都显著提高，那岗位方面的整体需求应该减少才对。</p><p></p><p>但也不一定。就目前来讲，软件开发者仍处于供不应求的状态。在我工作过的每家公司，大家都有太多想做的事情，只因开发者数量有限而被迫作罢。</p><p></p><p>那如果开发者的工作效率瞬间提升 2 倍，结果会如何？无疑是修复更多 bug、发布更多功能、赚取更多利润。我们可以开发的东西太多了，根本不存在开发者过剩一说。</p><p></p><p>我反倒认为，大语言模型的普及会增加开发者的职位总量。</p><p></p><p>目前，很多企业根本就不雇用软件开发者。我曾在一家名叫 Konrad Group 的公司工作，这是一家专为客户构建 Web 应用程序的企业，客户名单里不乏家喻户晓的大厂。但因为开发成本太高，他们发现与其招聘固定开发人员，还不如把这活儿外包出去。</p><p></p><p>这些财富五百强公司会根据当前的软件开发成本做核算。这里我们假定一个场景：如果他们需要 4 名开发人员，平均年薪 15 万美元，那每年就是 60 万美元。在这种情况下，他们不如向外包机构支付 50 万美元来搞定这方面需求。但如果大语言模型真能提高开发者的工作效率，他们也许只雇 2 名年薪 15 万美元的开发者就够了。突然之间，原本自己没法做的项目现在可以做了！</p><p></p><p>我要澄清一下：我不是经济学家，以上案例安全就是随口一说。我不知道未来的情况会不会朝这个方向发展。但我想提醒大家，大语言模型的爆发并不一定会摧毁我们的工作和生活。没人知道未来会向哪里去，而我实在受不了人们言之凿凿地把前景描述得愁云惨淡。</p><p></p><p></p><h2>网上不乏与我相同的观点</h2><p></p><p></p><p>Aaron Blaise 是一位经验丰富的动画师兼插画家。他在迪士尼工作了近 20 年，曾为《美女与野兽》（1991）、《阿拉丁》（1992）、《风中奇缘》（1995）等经典影片做出贡献。</p><p></p><p>几周之前，他在 YouTube 上发布了一段视频，题为《迪士尼动画师对 AI 动画的反应》（<a href=\"https://www.youtube.com/watch?v=xm7BwEsdVbQ%EF%BC%89%E3%80%82%E4%BB%96%E7%9A%84%E8%A7%82%E7%82%B9%E8%B7%9F%E6%88%91%E5%BE%88%E5%83%8F%EF%BC%8C%E5%B9%B6%E4%B8%8D%E8%A7%89%E5%BE%97\">https://www.youtube.com/watch?v=xm7BwEsdVbQ）。他的观点跟我很像，并不觉得</a>\" AI 工具是种威胁，而认为它将提高动画师的生产力并创造出更多动画岗位。</p><p></p><p>来自几十个行业的艺术家和知识工作者们都在讨论这方面话题，毕竟很多人担心自己的工作会被 GPT-4、DALL-E 2 和 Midjourney 等 AI 工具所吞噬。</p><p></p><p>GPT-4 能通过模拟律师考试吗？可以，而且得分在人类应试者里能排进前 10%。所以律师们也慌了。</p><p></p><p>我个人认为，大多数专业人士都能找到将 AI 技术整合进工作流程的办法，借此提高自己的生产力和价值。一部分任务可能被全面委托给 AI，但只是部分任务、而非整个工作。</p><p></p><p>那如果我是错的，事实证明大语言模型可以全面取代软件开发者，又该怎么办？如果真是这样，那我觉得也不用太担心了，到那时候绝大多数知识工作者都会被“清洗”。</p><p></p><p>这样一来，我们就算是转行、转专业也没有用，AI 海啸之下将没有安全的高地。所以与其去赌未来会不会朝着最糟糕的方向去，倒不如专注于自己热衷的工作、感兴趣的问题、擅长的方向。</p><p></p><p></p><h2>前端开发与其他工程学科</h2><p></p><p></p><p>网上有帮家伙，一直在暗示前端开发特别容易被 AI 替代，还建议开发者再精进一步，往后端或数据工程那边努力。</p><p></p><p>但我的观点恰恰相反。我觉得开发者群体根本就不用担心被淘汰，如果非得说哪部分群体更危险，我倒认为是后端。</p><p></p><p>OpenAI 的 GPT-4 直播展示了两段与代码相关的内容：</p><p></p><p>“Joke website”前端；基于 Python 的 Discord 机器人。</p><p></p><p>在这两个项目中，Python 代码对我来说似乎更接近生产。我最近用 Node.js 构建了一个 Discord 机器人，跟 GPT-4 生成的代码确实差不多。</p><p></p><p>相比之下，AI 为那个笑话网站生成的基础 HTML 文档，就跟我每天编写的前端代码间相隔十万八千里。</p><p></p><p>这可能有点以偏概全，但过去十年来，我发现很多复杂性要素已经从服务器转向了客户端。</p><p></p><p>Monolithic Express 应用程序已经转化为无服务器函数的集合，而我们的前端也从超链接数字文档发展为成熟的桌面质量应用程序。</p><p></p><p>再有，前端是用户与之交互的产品部分。企业当然希望自己的产品能体现出定制化、独特性和根据品牌形象精心设计等特点。与之对应，后端则完全不可见，所以通用后端要比通用前端更让人易于接受。</p><p></p><p></p><h2>用大语言模型辅助学习</h2><p></p><p></p><p>我听有些人说，ChatGPT 能帮助大家快速学习技术性技能。如果各位对教程中的某些内容感到困惑，不妨问问神奇的 AI。</p><p></p><p>这对我来说是个非常有趣的用例。从本质上讲，ChatGPT 就像你的结对程序员，能帮助你了解自己不熟悉的事物。你可以向它提出具体问题，并获取具体答案。</p><p></p><p>当然，使用过程中也要当心。利用 AI 工具来学习，也有正确的方式与错误的方式之分。</p><p></p><p>所谓错误的方式，就是把 AI 当作 GPS 导航。当我们打算开车前往某地时，就会把地址输入 GPS，然后不假思索地按它的指示前进。等到了目的地附近时我会把导航关掉，整个过程不做任何思考和评判。结果就是，我的方向感完全萎缩了，现在如果没有合成语音的指示，我根本不知道该往哪里走。😬</p><p></p><p>与之相对，我们最好把大语言模型当成“被告”，而自己则扮演“陪审团成员”。</p><p></p><p>你要认真听它在说什么，但不可全盘相信。保持怀疑的态度，以批判的心态考量一字一句。</p><p></p><p>不可盲目复制 / 粘贴 ChatGPT 生成的代码，而是逐行检查并保证你理解了其中内容。有不清楚的地方要提醒它做出澄清，并配合权威来源（例如官方文档）仔细检查看似可疑的部分。请记住，大语言模型 100% 自信，但却并非 100% 准确。</p><p></p><p>通过这种方式，相信大语言模型能够创造出前所未有的价值。😄</p><p></p><p></p><h2>给各位年轻开发者的忠告</h2><p></p><p></p><p>我之所以要写下这篇文章，就是想鼓励各位正在学习 Web 开发、并为大语言模型的爆发式进步压得喘不过气的朋友们。很多人彻底失去了信心，认为随着 AI 技术的继续成熟，自己耗费时间和精力掌握的所有技能将被无情淘汰。</p><p></p><p>我不敢保证未来一定不会这样。但我对 AI 能够给工作方式带来的影响仍抱怀疑。早在 2007 年，我就开始研究 HTML/CSS/JS，多年以来情况已经发生了很大变化。但开发者们仍然凭借强大的适应性，与技术一同进步、携手为网络世界塑造新的样貌。</p><p></p><p>到目前为止，我还没看到有切实迹象表明前端开发的工作处于危险之中。我也想象过未受任何专业训练的非开发者，在不了解 Web 技术的情况下会开发出怎样的 Web 应用程序。太多的因素会导致其无法运行了，所以即使 GPT 未来继续保持迭代，大家也没必要过于焦虑。这，又何尝不是另一种“幻觉”。</p><p></p><p>再次强调，我的观点很可能是错的，毕竟我可没有能预见未来的水晶球🔮。但未来总有不确定性，没准明天太阳就爆炸了，谁知道呢。但我真的很难相信 Web 开发者会被迅速淘汰，反而更担心潜在开发者会因为这种莫须有的可能性而放弃学习。</p><p></p><p>总之，但愿不要五年之后软件开发者需求更旺之时，大家才感觉后悔、反思当下的自己为什么要停止追求梦想。请继续努力吧，各位同学！❤️</p><p></p><p></p><p>原文链接：</p><p></p><p><a href=\"https://www.joshwcomeau.com/blog/the-end-of-frontend-development/\">https://www.joshwcomeau.com/blog/the-end-of-frontend-development/</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/7c2b26c5bb69b6acbc6dfe7df\">GPT-4：不了不了，这些我还做不到</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ebf7dab20cbf79517f2fc46c5\">3 分钟快速了解 GPT-4</a>\"</p><p><a href=\"https://www.infoq.cn/article/tDCUgv7rCH0yX40wdFVG\">对话 OpenAI Greg Brockman：GPT-4 并不完美，但人类也一样</a>\"</p><p><a href=\"https://www.infoq.cn/article/HFSPasQ7SXZ9QzdFXhGO\">GPT-4 重磅发布，吊打 ChatGPT！编程能力牛到让我睡不着：10 秒做出一个网站，1 分钟开发一个游戏</a>\"</p>",
    "publish_time": "2023-04-10 12:43:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "备受Vue、Angular和React青睐的Signals演进史",
    "url": "https://www.infoq.cn/article/qvL7WDtjURgbx7ICvhy6",
    "summary": "<p>最近，在前端领域，围绕着“Signals”这个词，有一些热烈的讨论。不管是<a href=\"https://preactjs.com/guide/v10/signals/\">Preact</a>\"还是<a href=\"https://github.com/angular/angular/discussions/49090\">Angular</a>\"，似乎都在讨论该话题。</p><p>&nbsp;</p><p>但它们并不是什么新东西。如果我们将其追溯到上个世纪60年代的研究，那么这就更算不上新鲜的事物了。它的基础采用了与第一个<a href=\"https://www.historyofinformation.com/detail.php?id=5478\">电子表格</a>\"和硬件描述语言（如Verilog和VHDL）相同的模型。</p><p>&nbsp;</p><p>即便是在JavaScript中，从声明式JavaScript框架诞生开始，我们就拥有这种理念了。随着时间的推移，它们有了不同的名字，并且在这些年里不断流行了起来。现在，它又重新出现了，这是一个很好的时机，我们可以对它是什么以及为何需要它进行更多的介绍。</p><p>&nbsp;</p><p></p><blockquote>免责声明：我是SolidJS的作者。本文从我的角度介绍了演进的过程。尽管文中没有提及，但是<a href=\"https://csmith111.gitbooks.io/functional-reactive-programming-with-elm/content/section5/Signals.html\">Elm Signals</a>\"、<a href=\"https://emberjs.com/\">Ember的计算属性</a>\"和<a href=\"https://www.meteor.com/\">Meteor</a>\"都是很值得称道的。&nbsp;如果你还不清楚Signals是什么以及它是如何运行的，请参阅我的这篇对<a href=\"https://dev.to/ryansolid/a-hands-on-introduction-to-fine-grained-reactivity-3ndf\">细粒度反应性（Fine-Grained Reactivity）</a>\"的介绍。</blockquote><p></p><p></p><h2>起初的蛮荒时代</h2><p></p><p>&nbsp;</p><p>有时候，我们会惊讶地发现，很多参与者在完全相同的时间形成了类似的方案。在声明式JavaScript框架的起步阶段，有三个方案在三个月内陆续发布，它们分别是<a href=\"https://knockoutjs.com/\">Knockout.js</a>\"（2010年7月）、<a href=\"https://backbonejs.org/\">Backbone.js</a>\"（2010年10月）和<a href=\"https://angularjs.org/\">Angular.js</a>\"（2010年10月）。</p><p>&nbsp;</p><p>Angular的脏值检查、Backbone的模型驱动重渲染以及Knockout的细粒度更新，虽然它们彼此间有些差异，但是最终都成为了我们今天管理state和更新DOM的基础。</p><p>&nbsp;</p><p>Knockout.js对本文的主题特别重要，因为它们的细粒度更新是建立在所谓的“Signals”的基础之上的。他们最初引入了两个概念，分别为observable（状态）和computed（副作用），但是在接下来的几年中，他们在前端语言中引入了第三个概念pureComputed（衍生状态）。</p><p>&nbsp;</p><p><code lang=\"null\">const count = ko.observable(0);\n\n\nconst doubleCount = ko.pureComputed(() =&gt; count() * 2);\n\n\n// 每当doubleCount更新时，打印日志记录\nko.computed(() =&gt; console.log(doubleCount()))</code></p><p></p><h2>狂野时代</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/32/32b35aeffee1a8b40d6b045782f3ecfa.jpeg\" /></p><p></p><p>&nbsp;</p><p>在这个时代，服务器端开发的MVC和过去几年从jQuery中学到的模式进行了融合，形成了新的模式。其中，最常见的一个模式叫做数据绑定，Angular.js和Knockout.js都具有该模式，不过实现方式略有不同。</p><p>&nbsp;</p><p>数据绑定的概念是，state（状态）应该被关联（attached）到view tree（视图树）的一个特定部分上。借助这种方式，能够实现的一种强大功能叫做双向绑定。所以，我们可以让状态更新DOM，反过来，DOM事件会自动更新状态，所有的这一切均是以一种简单的声明方式实现的。</p><p>&nbsp;</p><p>但是，滥用这种力量最终会作茧自缚。我们构建应用的时候，对其缺乏足够深入的了解。在Angular中，如果不知道什么内容发生变化，就会对整个树进行脏值检查，而向上传播会导致它多次发生。在Knockout中，很难跟踪变化的路径，因为你会在DOM上走来走去，出现循环也是司空见惯的。</p><p>&nbsp;</p><p>当<a href=\"https://reactjs.org/\">React</a>\"出现的时候，我们已经准备好逃离这一切了，对我个人来说，是<a href=\"https://youtu.be/nYkdrAPrdcw\">Jing Chen的演讲</a>\"，让我稳住了阵脚。</p><p>&nbsp;</p><p></p><p></p><p></p><h2>自由时刻</h2><p></p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/65/64/65dedbcfee801653a4cf9269da27ef64.gif\" /></p><p></p><p></p><p>接下来，就是对React的采用。有些人依然喜欢反应式模型，因为React对状态管理没有自己的偏好，所以完全可以将两者结合起来。</p><p>&nbsp;</p><p><a href=\"https://mobx.js.org/\">Mobservable</a>\"（2015）就是这样的方案。但是，相对于与React的集成，它还带来了一些新的内容。它强调一致性和顺畅（glitch-free）的传播。也就是说，对于任何给定的变更，系统的每个部分仅运行一次，而且以适当的顺序同步运行。</p><p>&nbsp;</p><p>为了实现这一点，它使用了一种推-拉（push-pull）混合的系统来替换先前方案中基于推送的反应性。变更的通知会被推送出去，但是衍生状态的执行会推迟到读取它的地方。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0badd6d7848beacb47a93ed5882a0a33.png\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>为了更好地理解Mobservable的原始方式，请参阅Michel Westrate的“<a href=\"https://hackernoon.com/becoming-fully-reactive-an-in-depth-explanation-of-mobservable-55995262a254\">Becoming Fully Reactive: An in Depth Explanation of Mobservable</a>\"”一文。</blockquote><p></p><p>&nbsp;</p><p>虽然在很大程度上，这个细节会被React重新渲染读取变更的组件所掩盖，但是，这是使系统实现可调试和一致性的关键步骤。在接下来的几年里，随着算法的不断完善，我们会看到一种趋势，那就是<a href=\"https://dev.to/modderme123/super-charging-fine-grained-reactive-performance-47ph\">更多基于拉取的语义</a>\"。</p><p>&nbsp;</p><p></p><h2>征服泄露的观察者</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff5538449eb88881857e58a6dbe90c91.jpeg\" /></p><p></p><p>细粒度反应性是<a href=\"https://en.wikipedia.org/wiki/Observer_pattern\">四人组（Gang of Four）观察者模式</a>\"的变种。虽然观察者模式是一个强大的同步模式，但是它也有一个典型的问题。一个Signal会保持对所有订阅者的强引用，所以长期存活的Signal会保留所有的订阅，除非进行手动处置。</p><p>&nbsp;</p><p>这种记录方式在大量使用时会变得很复杂，尤其是在涉及嵌套的时候。在处理分支逻辑和树的时候嵌套很常见的，就像在构建UI视图时的那样。</p><p>&nbsp;</p><p>有一个鲜为人知的库，叫做<a href=\"https://github.com/adamhaile/S\">S.js</a>\"（2013）提供了答案。S是独立于其他大多数解决方案而开发的，它更直接地以数字电路作为模型，所有的状态变化都在时钟周期内进行。S将其状态基元称为“Signals”。尽管它不是第一个使用该名字的，但它是我们今天使用该术语的起源。</p><p>&nbsp;</p><p>更为重要的是，它引入了反应式所有权的概念。所有者会收集所有的子反应式作用域，并在所有者处置（disposal）自身或重新执行时，管理子反应式作用域的处置。反应式图会从一个根所有者开始，然后每个节点均作为它所拥有的后代。这个所有者模式不仅对处置过程很有用处，而且在反应式图中，建立了一种提供者/消费者（Provider/Consumer）上下文的机制。</p><p>&nbsp;</p><p></p><h2>调度</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a62b776042547cc725d010588aa9a2a.png\" /></p><p></p><p></p><p><a href=\"https://vuejs.org/\">Vue</a>\"（2014）也为我们今天的发展做出了巨大的贡献。除了在优化一致一致性方面与MobX的节奏保持一致之外，Vue从一开始就将细粒度反应性作为其核心。</p><p>&nbsp;</p><p>虽然Vue和React都使用了虚拟DOM，但是Vue的反应性得到了最好的支持，这意味着它是与框架一起研发的，首先是作为内部机制，为其Options API提供支持，在过去的几年中，它成为了Composition API（2020）的前沿和核心。</p><p>&nbsp;</p><p>Vue将推送/拉取向前推进了一步，能够调度任务何时会完成。默认情况下，Vue会收集所有的变更，但是下一个微任务在处理作用（effect）队列之前不会处理它们。</p><p>&nbsp;</p><p>然而，这种调度也可以用来做其他的事情，比如keep-alive和Suspense。甚至像<a href=\"https://github.com/ryansolid/solid-sierpinski-triangle-demo\">并发渲染</a>\"这样的功能也可以用这种方式来实现，从而充分体现了如何同时利用基于推送和拉取的方式能够达成的最佳效果。</p><p>&nbsp;</p><p></p><h2>编译</h2><p></p><p>&nbsp;</p><p>在2019年，<a href=\"https://svelte.dev/blog/svelte-3-rethinking-reactivity\">Svelte 3</a>\"向我们展示了利用编译器能够完成多少事情。实际上，他们将反应性完全编译掉了。在这过程中，也会有一些权衡，Svelte向我们展示了编译器如何抹平人类工程学方面的欠缺。这将会成为一种趋势。</p><p>&nbsp;</p><p>反应式语言（如状态、衍生状态、作用）不仅向我们描述了用户界面等同步系统所需的所有内容，而且它是可分析的。我们可以精确地知道都发生了哪些变更以及它们发生在什么地方。可追溯性的潜力是很深远的。</p><p>&nbsp;</p><p>来自<a href=\"https://preactjs.com/\">Preact</a>\"团队的Marvin Hagemeister<a href=\"https://twitter.com/marvinhagemeist/status/1625428781199421440\">在Twitter这样说到</a>\"，“我认为这是基于Signals的方式优于钩子（hook）的主要原因之一。它能够使我们添加更多的调试洞察力，这是钩子所无法实现的，比如准确地显示一个状态发生变更的原因。”</p><p>&nbsp;</p><p>如果能够在编译时知道这一切，我们就可以交付更少的JavaScript代码。对于代码的加载，我们会有更高的自由度。这就是<a href=\"https://www.builder.io/blog/hydration-is-pure-overhead\">Qwik</a>\"和<a href=\"https://dev.to/ryansolid/what-has-the-marko-team-been-doing-all-these-years-1cf6\">Marko</a>\"的可恢复性的基础。</p><p>&nbsp;</p><p></p><h2>面向未来的Signals</h2><p></p><p>&nbsp;</p><p>Angular团队的成员Pawel Kozlowski则<a href=\"https://twitter.com/pkozlowski_os/status/1573774557995044864\">认为</a>\"：</p><p>&nbsp;</p><p></p><blockquote>“Signals是新的VDOM。&nbsp;人们对它的兴趣正在爆发：很多人正在尝试一些新东西。这将使我们能够探索该领域，尝试不同的策略，对其增进了解和优化。&nbsp;虽然现在不知道最终结果是什么，但是这种集体探索是很好的！”</blockquote><p></p><p>&nbsp;</p><p>鉴于这项技术已经非常古老，说还有很多东西需要探索，这可能会令人感到惊讶。但是，这里的原因在于，它是一种对解决方案进行建模的方式，而不是一种具体的方案。它所提供的是一种描述状态同步的语言，与要让它执行的副作用完全无关。</p><p>&nbsp;</p><p>因此，它能够被Vue、Solid、Preact、Qwik和Angular采用似乎并不足为奇。我们已经看到它进入了Rust的Leptos和Sycamore，表明DOM上的WASM<a href=\"https://twitter.com/RyanCarniato/status/1580347110611816448\">不一定会慢</a>\"。React甚至考虑在底层使用它。</p><p>&nbsp;</p><p>来自React核心团队的Andrew Clark<a href=\"https://twitter.com/acdlite/status/1626590880126889984\">表示</a>\"：</p><p>&nbsp;</p><p></p><blockquote>“我们可能会在React中添加一个类似Signals的基元，但我并不认为这是一个编写UI代码的好方法。它对性能来说是很好的。但我更喜欢React的模式，在这种模式下，你每次都会假装重新创建所有的内容。我们的计划是使用一个编译器来实现与之相当的性能”。</blockquote><p></p><p>&nbsp;</p><p>也许这是一种合适的方式，因为React的虚拟DOM始终只是一个实现细节。</p><p>&nbsp;</p><p>Signals和反应性语言似乎是一个交汇点。但是，这在JavaScript诞生之初却并不那么明显。也许这是因为JavaScript并不是最好的语言。我甚至可以说，长期以来，我们在前端框架设计中感受到的很多痛苦都是语言本身的问题。</p><p>&nbsp;</p><p>无论这一切的结局如何，到目前为止，都是一次相当不错的旅程。有这么多人关注Signals，我迫不及待地想知道我们的下一步会是什么。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://dev.to/this-is-learning/the-evolution-of-signals-in-javascript-8ob\">https://dev.to/this-is-learning/the-evolution-of-signals-in-javascript-8ob</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/958fbe57ff88cdba990d99739\">手写一个 react，看透 react 运行机制</a>\"</p><p><a href=\"https://xie.infoq.cn/article/3e10ee935ffd1b23b1ecd8842\">看透 react 源码之感受 react 的进化</a>\"</p><p><a href=\"https://xie.infoq.cn/article/7baec545b8202471064494a69\">2023 重学 Angular</a>\"</p><p><a href=\"https://xie.infoq.cn/article/118bc90c00b5b7b4faf3cd774\">初识 VUE 响应式原理</a>\"</p>",
    "publish_time": "2023-04-10 12:44:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "在全球掀起史诗级的狂欢，ChatGPT真正颠覆了什么 ？| 深度",
    "url": "https://www.infoq.cn/article/iEkbUrxDh6c7svEbepKj",
    "summary": "<p></p><p>自去年11月底正式发布以来，OpenAI  最新的 AI 聊天机器人 ChatGPT 火出天际，成为现象级应用，在全网话题度狂飙。</p><p></p><p>瑞银发布的研究报告称， ChatGPT 推出后，今年 1 月的月活跃用户估计已达 1 亿，成为历史上用户增长最快的消费应用。</p><p></p><p>达摩院基础视觉负责人赵德丽在接受 InfoQ 采访时表示，传统搜索引擎只会寻找已经存在的信息，而 ChatGPT 的应用实现了从信息的搜索到信息的创造的范式转变。清华大学计算机科学与技术系长聘副教授黄民烈认为，“ ChatGPT 宣示着无缝人机交互时代的来临。过去conversation as a service （caas）还停留在纸面，但今天无论是开放域聊天，还是通用任务助理（ ChatGPT ）都表明这一概念已经走向现实”。短期来看， ChatGPT 有望成为传统信息检索的强有力辅助工具；长期来看，它可能发展成为 AI 系统级的综合服务平台。</p><p></p><p>自 ChatGPT 走红后，全球互联网大厂、创业公司纷纷加码布局，一场关于 ChatGPT 的军备竞赛已然拉开。谷歌加急推出了 ChatGPT 的竞争对手——人工智能聊天机器人 Bard Bard；微软已经宣布推出了由 OpenAI  提供技术支持的最新版必应搜索引擎。在国内，百度已经推出对标 ChatGPT 的产品——文心一言，多位科技大佬宣布在大模型领域创业。</p><p></p><p>让全网沸腾的 ChatGPT 到底有什么魔力？ ChatGPT 具有哪些颠覆性的创新？其落地和商业化应用的前景几何？对于科技界来说， ChatGPT 的出现到底会带来哪些改变？ ChatGPT 为什么是 OpenAI  最先做出来？爆红之下，有多少泡沫？</p><p></p><p>我们试图找到这些问题的答案。于是，InfoQ 发起了一场《极客有约》特别栏目<a href=\"https://www.infoq.cn/video/TXoCTL6MCpZWm2BKaesc\">《极客圆桌派：狂飙的 ChatGPT 》</a>\"，我们邀请了多位 AI 领域的资深技术专家一起共同探讨 ChatGPT 的现在和未来。</p><p></p><p>以下为本次直播的精华内容，经编辑：</p><p></p><h2>嘉宾介绍</h2><p></p><p></p><p></p><blockquote>主持人Mingke：大家好，我是 Mingke。我是《人工智障》系列的作者。这个系列主要是对使用基于统计的机器学习方法来发展 AI 技术时所存在的局限性进行批判。随着 ChatGPT 的出现，很多人问我之前的观点是否仍然成立，今天我们会讨论这些问题。今天我的角色是 prompt engineer，而我们的三位嘉宾则是 content generation。首先介绍圆桌第一位嘉宾张晴晴博士，她是 Magic Data 的创始人和CEO，曾任中国科学院声学研究所副研究员，是对话式 AI 先行者和 Data-Centric MLOps 引领者。她所在的机构非常注重数据和机器学习，是该领域的领导者。</blockquote><p></p><p></p><p>张晴晴： 大家晚上好！我是 Magic Data 的创始人张晴晴。我很高兴能看到像 OpenAI  这样一家专注于对话式 AI 的公司推出了 ChatGPT ，它在很大程度上颠覆了我之前的预期，让我看到对话式 AI 已经取得了超乎想象的进展。我相信在未来，通过更多的数据驱动，我们能够将 ChatGPT 这一技术不断升级，实现更高水平的人工智能。</p><p></p><p>Magic Data 成立7年，一直在为对话式 AI 提供深层次的数据支持。经过6年的时间积累，我们现在已拥有20万小时的对话数据，这些数据是通过模拟采集的方式生成的。最近，越来越多的公司和客户开始关注这一领域，特别是数据的底层支撑。 我很高兴能在这次直播中与大家分享有关 ChatGPT 和对话式 AI 数据的理念。</p><p></p><p></p><blockquote>主持人Mingke：接下来介绍今天的第二位嘉宾祝海林，Kyligence 技术合伙人 / 资深数据架构师、新一代开源编程语言 Byzer 的作者，拥有 13+ 年技术开发经验。一直专注于 Data + AI 融合方向，致力于帮助工程师们从根本上提高数据平台落地和 AI 工程化的效率。</blockquote><p></p><p></p><p>祝海林： Byzer 作为 Data &amp; AI 低代码开发平台，也是 ChatGPT 的受益者，我们同样是致力于构建基础设施的企业。 众所周知， ChatGPT 的基础设施是非常重要。OpenAI  有大量的基础设施积累，还得到了微软等公司的支持。这其中涉及数据采集、处理和人类反馈的标注，另外还需要模型训练，管理各种 GPU，将其作为服务提供出去。 我们给流程引入了 MLOps 的概念，即一套将数据和 AI 结合起来的非常复杂的流水线。Byzer 公司的目标就是让这件事情变得非常简单，希望我们都能成为 ChatGPT 的受益者。</p><p></p><p></p><blockquote>主持人Mingke：最后一位嘉宾是我的老朋友鲍捷博士，文因互联董事长及创始人，爱荷华州立大学博士，伦斯勒理工学院博士后，麻省理工学院分布式信息组访问研究员。现任 W3C 顾问委员会委员、中国中文信息学会语言与知识计算专业委员会委员、中国人工智能学会心智计算专委会委员、金融知识图谱工作组主席、中文开放知识图谱联盟 (OpenKG) 发起人之一，Data Intelligence 杂志编委。</blockquote><p></p><p></p><p>鲍捷： 我的自我介绍开头都是一句话：“我是一名程序员”，从 15 岁开始写程序，我的梦想是直到生命的尽头都能坚持不懈地写程序。我一直在文因互联这家公司工作，文因互联的愿景是创造互联世界的记忆。我们一直致力于知识工程的研究，希望能够将全世界的知识以统一的表达方式呈现，最终为每个人配备一个智能助理。</p><p></p><p>我们从十多年前开始探索了无数应用，主要集中在 ToB 领域。特别是在金融领域，我们做了很多关于知识建模的工作，如科创板和北交所的自动化审核系统，让机器能够自动理解大量文本。</p><p></p><p>我们在这方面已经取得了很大进展，但是当新的框架出现时，我们突然感到了危机感。从“危”的角度说，过去的10年甚至20年的工作好像都没有多大意义了；从“机”的角度说，过去用了十年的时间将成本降低了一个数量级，实现了工程化，但再往下发展就非常困难了。我们曾经以为已经极限压榨了技术成本，但突然有一天出现了新的框架，让我们可以将成本再次降低一个甚至两个数量级。以前许多无法实现的应用现在都可以实现了，新的商业模式和机会也出现了。 这是，我们感觉非常幸福的地方。</p><p></p><p>我也是人工智能工程师，曾经写过一篇文章《扒一扒聊天机器人》，在里面我讲述了基于神经网络的机器人是不可能实现的。还写过一篇名为《搞砸人工智能的十种方法》的文章，讲的是大部分基于经验主义的方法都不可行。但是今天，我需要回头修正之前的一些观点。</p><p></p><h2>如何看待基于统计的机器学习方法来进行对话和语言实践？</h2><p></p><p></p><p></p><blockquote>主持人Mingke：正如鲍捷老师所讲，我和他之前对使用基于统计的机器学习方法来进行对话和语言实践是持怀疑态度的。请各位分享一下你们在这个问题上的观点。</blockquote><p></p><p></p><p>张晴晴：我认为，掌握数据以及数据的处理方式是人工智能未来能够实现更快发展的关键。 人工智能无法摆脱统计的影响，这是一个关键点。</p><p></p><p>目前 AI 的未来发展方向还难以预测，但我认为要么它会以统计学为基础，要么在统计学的基础上结合领域知识。</p><p></p><p>在过去，我们可能并没有完全依赖统计学，而是采用物理学或数学的逻辑进行研究。早期我们使用的高斯-马尔可夫模型更具有物理学意义，但这种可解释性的逻辑体系为什么效果会赶不上一个不可解释的“黑盒子”呢？这是因为黑盒子这套体系可能更像人类大脑的连接突触，所以这种模式再加上大数据的耦合之后，会有非常突出的效果。因此在未来的趋势中，我更认同数据驱动这条路。</p><p></p><p>ChatGPT 面对某些高难任务时，在数据量持续增加前期识别效果并没有变得更好，但当数据积累到一定量时就会突然出现陡峭的提升，这种陡峭的提升可以理解成人类的“顿悟”。 另外 Transfomer 在数据处理方面与我们以前所了解的 BERT 有很大的不同，它将理解和生成相结合。对于某些任务来说，数据处理环节可能会更趋向于生成逻辑，这意味着我们可以将对话的呈现形式更多地输入机器，而无需在中间进行各种解析，如词性标注、知识图谱等。Transfomer 越来越多地将中间环节剥离，希望更简单、更直接地完成端到端处理。</p><p></p><p>祝海林： 我的答案是半信半疑。我曾经参与过医疗对话机器人的开发，因为我们拥有大量医疗问答数据，所以我认为在没有大的技术突破之前，这个领域的发展已经到达顶峰。例如知识图谱领域需要抽取实体和标签，几乎所有的互联网和相关文本类公司都在做这样的工作，都面临着相同的瓶颈。</p><p></p><p>我之前与 ChatGPT 交互的经历让我意识到这种困境的解决方法。我特意让 ChatGPT 分析用户宝宝的问题，比如一些症状，然后让 ChatGPT 从这段文字中抽取所有的症状词汇，但不包括疾病词汇。</p><p>ChatGPT 非常精准地完成了这项任务，这让我认识到这种技术的威力，从而改变了我对机器学习的看法。这种技术能够实现降维打击，不需要大量的知识积累和人力堆叠。 通过输入一些自然语言，它可以完成一系列交互任务。比如，我们以前编写交互是使用编程语言，但现在我们可以直接写一段话并输入给 ChatGPT，它就能帮我们完成工作。总之，我认为 ChatGPT 的出现真的让我感到震惊，但我也发现背后的技术并没有那么神秘。</p><p></p><p>AI 难以突破的原因之一是因为我们一直在教导机器如何完成任务，并且要求将每个任务分解成很多步骤，这些步骤需要程序员或研发人员去教导机器。这种方法导致了瓶颈的出现。但当我们直接将问题交给机器端到端地解决时，我们发现机器的学习能力超乎我们的想象。目前来看使用数据驱动的端到端方法将是主要趋势。</p><p></p><p>鲍捷：   举个例子，比如说我去日本旅游，虽然我不懂日语，但基本上可以从日文报纸中猜出大概意思。因为日语中有50%的符号是汉字，因此我可以通过这些符号来推测我看不懂的符号的含义。</p><p></p><p>ChatGPT 就像一只聪明的鹦鹉，可以像这样来读懂并翻译我们不认识的语言。但是 ChatGPT  目前还做不好四则运算。你可以尝试问它各种算术题，就会发现它会进行数字内插，这是错误的。它很明显是在把这些数字本身变成一个符号、一组符号，然后按照一定规则进行处理。而我们现实中，无论是真正的语言构造，还是数学、财务规则、法律合规等，这些所有的知识结构都是递归文法。所以我认为这个运算仅仅只有 ChatGPT 是搞不定的。</p><p></p><p></p><blockquote>主持人Mingke：如果一个模拟近似于某个实体，但本质并非该实体，我们该如何加以区分？例如，对于人工智能这个概念，我们需要思考一下是否有必要对其进行准确的定义？ </blockquote><p></p><p></p><p>张晴晴： 我认为不需要。因为我认为我不需要去搞清楚一个人、机器或者动物的思维机理，我相信行动比思考更重要。我会在我能力范围内去了解一个人的历史来预测其未来可能的行动，而不是仅仅关注他们的思考过程。我们会经常思考要做某件事情，但是这并不代表我们真的会去实施，这是两个不同的概念。</p><p></p><p>我们必须承认人的认知能力是有限的。施一公老师提到一个观点：人对世界的认知受到自身构造的限制。我们是由蛋白质组成，我们的感官体验，如嗅觉、触觉和味觉，都源自于身体中的这些蛋白质。由于蛋白质数量有限，我对那些超出我能感知的部分的理解也有限。但这是否意味着那些东西不存在呢？就像我们说暗物质是存在的，但是我无法感知它。如果有机会，我当然会尝试用任何方法去接触到这些我感知不到的部分。只要我能够基于我感知到的部分获取所有信息并获得反馈，就可以在当前这种三维空间中进行学习和探索，但作为一个人，我无法跳到五维空间中去。</p><p></p><p>祝海林： 我更关注的是如何更好地利用人工智能技术，将其应用和扩大化，而不是一味地去追求其终极形态。只有这样，我们才能不断推动人工智能技术的发展，让其越来越接近我们所期望的形态。我们会继续探讨人工智能的发展，但是如果我们认为某种技术非常出色，我们一定要利用它并将其推广应用。</p><p></p><h2>ChatGPT最重要的特征是什么？</h2><p></p><p></p><blockquote>主持人Mingke：ChatGPT 对于我们这些从业者来说，其最重要的特征是什么？如果要挑选两个最重要的特征（最亮眼的能力），你认为是什么？</blockquote><p></p><p></p><p>Mingke ：以我的角度来看，ChatGPT 最亮眼的地方在于它的世界模型和生成能力。在世界模型方面，我们可以借助 ChatGPT 的常识推理器来实现更加智能的知识推导和应用，这可以帮助我们更好地发掘 ChatGPT 在专业领域的应用潜力。在生成能力方面，ChatGPT 可以通过巧妙的方式解决个性化模板的困难，从而实现更加智能、个性化的应答能力，这可以帮助我们更好地应对不同场景和需求的挑战。</p><p></p><p>祝海林：我认为最大的优点是它形成了无数模板，这些模板本质上都是一些“套路”。 人类在掌握一些知识后，学习的实际上就是这些“套路”。我认为 ChatGPT 具备“套路”生成能力，它生成的东西不仅可以解决你的问题，甚至还能指导你完成尚未实现的任务。</p><p></p><p>第二个亮点是它具有多轮对话能力，它可以通过“in context learning”学习。 在工作中，从员工的角度来看，你需要学会如何与 ChatGPT 建立联系，以便更快、更准确地获取信息。</p><p></p><p>从公司层面考虑，在 ChatGPT 诞生后我们可以在三个方面努力。 首先，我们可以帮助更好地构建 ChatGPT ，例如 Magic Data 公司提供更好的匹配数据，我们这边提供更好的基础设施，可以更好地训练大型模型。其次，我们可以利用大模型去做一些事情，比如我们公司做了一个指标中台，你只需描述一下需要的数据，系统会自动算出指标值。以前我们需要通过 SQL 来实现这个功能，但即使是 SQL 也有很多人不会用。现在我们可以利用 ChatGPT 写代码，它可以自动生成 SQL 语句，甚至可以用自然语言来描述复杂需求。程序员们将面向自然语言编程，这也是他们的终极梦想。</p><p></p><p>鲍捷： 从技术角度来看，我体会到了两个方面的突破。去年的知识计算专委会上大家曾询问过有关大模型和类似 BERT 的技术问题。当时我们还觉得这些技术并没有太大的用处。我们主要用它来确定数据集的极限。例如，我用 BERT 算出一个数据集的准确度是 91％，那我优化到 90% 就不再优化了，因为进一步的优化不会有太大的提升。通常我不会将其应用到用户终端系统中，因为成本非常高，而且迭代速度非常慢。</p><p></p><p>在过去几个月内我突然意识到了这个东西的价值。这种价值与两个新的计算范式有关，即提示学习方法和强化学习方法。无论是提示学习还是强化学习，我认为其基本范式仍然是如何更有效、更低成本地将人类知识注入到机器中。这种结构化的知识是人类大脑中拥有的，我们需要以一种低成本的方式将其转化为机器可理解的表示形式。最终的“佐料”虽然很少，但它可以改变整个配方的味道。这个“佐料”是人类知识，而不是机器数据。当我们最后加入了这一丁点催化剂后，化学反应突然加速了。这使得我们能够在客户服务中做许多以前根本不可能做到的事情。</p><p></p><p>在 2016 年我们尝试了智能投顾，但在 2018 年放弃了这个方向。因为你永远无法深刻地理解客户，也无法真正实现科学的投资建议。但现在有了 ChatGPT，你至少可以做两件以前根本做不到的事情。首先你可以以非常低的成本与客户长期陪伴，其次现在可以自动化撰写资产配置说明书。 通过用更低的成本获取用户的数据，我们可以利用技术自动化生成各种报告和对话，并在一定规则的约束下生成各种观点、整合数据和创建更友好的用户交互方式，大大提高用户的粘性和数据量。这种方法不仅适用于获客、风控和投研等领域，未来还有很大的探索空间。</p><p></p><p>ChatGPT 最终可能成为你生命中一个至关重要的工具，甚至会“劫持”你的人生。 有人估计一个人一生可以创作 5000 万个单词。这意味着如果你拥有大约 5000 万个标记，你几乎可以让用户感受到你比他们的父母还了解他们的需求，从而提供高度个性化的服务。我们之前的个性化服务主要通过标签分类实现，但这种方式的数据维度太少，无法充分理解个人。未来，这种技术在工程化方面将变得更加复杂，由许多因素综合而成才能发挥作用。这种技术将每个人都放进信息茧房，提供量身定制的服务，无论好坏，可能都会让我们感到不适。</p><p></p><p>有一个很重要的讨论，就是未来人类社会可能向左走或者向右走？向左走就是人工智能统治一切，建立“暴政”。而另外一种则是更加分权的数据，建立数据市场经济和数据资产市场经济，通常称之为 Web 3。过去有人提出了“拉动”的概念，即每个人都应该拥有一个属于自己的个人数据仓库。但当时的最大问题是普通人无法建立这样的数据仓库，因为成本非常高。现在，我们是否可以通过新的文本处理技术来降低个人数据仓库的建立成本呢？我认为这是非常有可能的。</p><p></p><p>张晴晴：我觉得 ChatGPT 有很多值得我反思的地方。第一个让我反思的点是“信息茧房”，我很难理解为什么像 ChatGPT 这种以数据驱动的系统更有优势。我坚信以数据为中心的理念，当然算法和算力同样重要，我认为三者缺一不可。我感觉大家好像在集体忽视这个点，即OpenAI 坚持做到极致的事情，就是让数据变得更有效率。我坚信数据做到极致可以成为一个奇点。</p><p></p><p>从另一个维度来看，行业似乎更多地关注视觉数据。例如无人驾驶、辅助驾驶和安防都是视觉数据，因此视觉数据应该占据我们主要处理的数据市场。但 ChatGPT 告诉我们对话数据也是非常重要的。 视觉只能传达有限的信息，而人类信息的传递靠的是语言。语言对于我们的认知和教育有着很深的影响，因此我在这次对话式数据的探索中深刻认识到了语言的重要性。 以上就是我自己深有感触的两个方面。</p><p></p><h2>未来会有多少个大模型？</h2><p></p><p></p><p></p><blockquote>主持人Mingke：作为人类知识的表达方式，ChatGPT 的应用领域是非常广泛的。最近我们经常听到大家要做 ChatGPT 这样的东西，所以我问一个问题：你认为未来会有多少个大模型？</blockquote><p></p><p></p><p>张晴晴： 在谈到大模型时,我们需要明确其定义。在 ChatGPT 中数据是分层的。如果我们将那些使用网上扒取海量数据灌入模型的方法定义为大模型，我认为全世界可能没有多少企业或国家能够支撑这种非常庞大的母体模型。在一些行业领域中，我个人倾向于采用联邦学习方法。在一定的阶段内，我们无法将所有东西都融合到一个通用人工智能的微领域模型中。</p><p></p><p>领域模型是不是大语言模型取决于具体情况。如果你正在开发针对整个金融行业或客服行业的领域模型，那么这是大模型。但如果你将这个模型应用于你的企业，那么它可能不能被视为大模型。</p><p></p><p>祝海林： 我同意。我认为将模型分领域是不正确的。一个真正的大模型必须包含三个方面的内容。首先它必须具有通识，也就是人类的基础经验和知识。 只有学习了这些通用知识，才能进一步学习特定领域的知识。其次，需要用领域特定的数据去训练模型，来让模型学习领域知识。最后，需要决定如何表达模型的结果，这可能需要基于人类反馈的方法。</p><p></p><p>将领域模型直接与大型模型相结合可能是不可行的，因为这些领域特定的知识和经验可能与大型模型所学的通用知识并不兼容。 举个例子，我们以前的做法相当于是直接把专业知识灌输给一个不具备相关背景的人，就像把金融领域的知识灌输给一个三岁的孩子一样。但 ChatGPT 告诉我们，我们需要先构建一个大模型，以便让它具备良好的知识能力。接下来我们需要使用 finetune 技术来支持多任务。金融领域的专业知识和经验可能与大型模型所学的知识不一致，因此需要将它们分开处理。使用领域模型进行 finetune 的目的就是让模型能够学习特定领域的知识，并将其与通用知识结合起来，从而使模型更加准确和精确。</p><p></p><p>另外我们还需要让机器学会如何正确表达，这是通过数据挖掘人类做出的选择来实现的。对于同一个问题，机器可能会产生多种回答，但只有符合人类认知的答案才是正确的。因此在开发机器学习算法时，我们需要更加关注人类认知。</p><p></p><p>我认为领域模型是一项生态技术。尽管趋势是使用通用的、具有优秀知识结构的大模型，但它可能仍然无法解决所有问题。 在这个过程中，我们可以使用通用模型来翻译领域专业术语，并添加一些偏好。对于金融领域，我们可能需要与非常资深的客户打交道，并需要使用非常专业的语言进行表达。因此，我认为领域模型对于不同的机构和公司都是一个未来的机会。</p><p></p><p>在生态系统中，领域模型和大模型应该不是同一个拥有者，很多创业的机会可能就会出现在基于大模型的领域模型中。大公司和小公司都会存在，而未来大公司很可能提供基于层次的大型模型， 至少在接下来的一两年里，他们可能会提供这种服务。而对于一些创业公司来说，他们可以基于这些大型模型，为自己的领域提供应用。</p><p></p><p>鲍捷：首先，我们需要将理论应用到实践中。在当前的阶段，特别是在中国和中文环境下，以及我们所关注的 ToB 服务领域内，我认为短期内通用语言模型的商业化前景不大。因此，真正能够应用的语言模型仍然是在特定领域内使用的模型。然而这并不是终点。也许在五年或十年后，一个通用的基础设施生态系统可能会出现，但目前我对此持悲观态度。对于这样一个组织能否出现，我认为在短期内是不太可能的，因为提供者必须是中立和开放的主体，而不是商业公司。</p><p></p><p>在这个具体的语境下来说，我们应该避免将商业生命建立在无法验证的假设之上。因此我们应该采用更加相对简化的方法，以明确的任务为基础，在有限的数据和边界内，用较小的规模启动业务闭环。我们可以利用 3 个月、6 个月或者一年的中转周期来逐步撬动资源，不断优化我们的系统。然后，当系统真正成长起来或者我们的思想突然有了突破时，我们会积极地拥抱这些变化。</p><p></p><p>短期内，我们无法拥有与英文版 ChatGPT 相媲美的基础数据，比如中文百科数据。 虽然维基百科数据的数量可能不是最多的，但是其质量是最高的。目前，中文没有一个百科网站能够与维基百科相媲美。另外，像 Reddit 等在线论坛这样的公开数据集我们也是欠缺的。 如果我们仍然像过去一样，每家互联网公司都将自己的数据保密，那么每家公司所构建的大模型只能基于其自身的数据，这样构建的人类知识和常识肯定是不够全面的。因此，我们需要构建一个基于开放数据的生态系统，这需要很长时间来实现。</p><p></p><p>我想再补充一下关于语言模型的“neutral”的看法，它是指语言本身并不具有倾向性。因此，在面向消费者的应用中，尤其是在头部互联网应用中，这一特点可能成立的。但是，在特定领域中，如医疗、法律、金融等领域，这一特点的作用可能不会像在面向消费者的领域中那样。虽然语言模型可以进行跨语言学习和知识转移，但它并不能为客户最终买单的核心问题提供直接解决方案。这有点像小学生，三年级学生就可以开始写作文了，但是如果你想通过律师或医生的资格考试呢？</p><p></p><p>祝海林： 在构建大型语言模型时，我们不能仅基于纯中文数据进行训练，而需要将西方或欧美的数据纳入其中。对人类来说，多语言可能是困难的问题，但在大型语言模型中这并不是难题。目前我们可能面临的困难是，我们对英语的掌握能力还有所欠缺，虽然这些数据是公开的，但我们需要对其进行有效的清洗和加工。在这方面，我们与欧美的一些公司相比仍有很大差距。 但我们可以通过努力来弥补这个差距，因此比起创造数据，我们可以更快地利用国外的数据。我相信通过几年的积累，我们将能够充分利用这些数据。</p><p></p><h2>ChatGPT 这样的大模型，能否在国内出现和应用？</h2><p></p><p></p><p></p><blockquote>主持人Mingke：从创业者的角度出发，像 ChatGPT 这样的大模型是否能够在国内应用？目前有很多人想做类似的事情，你认为他们能够成功吗？如果想要实现这些目标，他们需要克服哪些挑战？</blockquote><p></p><p></p><p> 张晴晴： 我认为这是一定能够做到的。我觉得目前全球最有机会做到的可能只有中美两国。但是在中国，目前我们还需要不断探索。中国面临的挑战主要来自于两个方面：金融体系和法律体系。</p><p></p><p>祝海林： 换个角度来看，我认为中国人一个不太好的地方是，从领导层面开始，很多人往往会有一种大力出奇迹或者依靠个人英雄主义的思维方式。 他们会认为只要雇用一些人就能够完成一件事情，但实际上这种思维方式是错误的。我认为核心问题在于人才密度。如果我们想要实现或超越其他国家，我们需要关注什么？ 根据数据显示，在人工智能领域的顶尖人才中，有 59% 在美国工作，而只有 11% 在中国。 尽管中国已成为全球第二大经济体，但与美国相比，中国的人才密度仍然存在 6 倍左右的差距。这是第一个问题。</p><p></p><p>第二个问题是，从 ChatGPT 的角度来看，中国在人工智能领域的差距正在扩大。中国的顶级 AI 人才中，有 29% 在中国获得本科学位，但有 56% 在美国学习和生活。这意味着中美两国在 AI 人才方面的差距非常大。 最近有很多人想在国内做 ChatGPT ，他们开始寻找资本。相对来说资本比较容易找到，因为这种模式已经得到验证。但最大的问题是他们很难找到合适的人才。</p><p></p><p>此外，OpenAI 背后有很多公司的支持，但目前我还没有看到中国的公司能够形成合作力量。每家大公司都想做一件事情，每个人都想开一家创业公司。因此我持有一种稍微悲观的态度。虽然这些公司都能够做出成果，但是我认为效果会差很多，它们最终都会成为领域模型。我认为这个差距至少在未来 3～5 年内会一直存在，即使我们能够做出相似的产品或技术，但在效果上这个差距可能是 2%、10%或者20%，甚至有可能这个差距会持续扩大。</p><p></p><p>鲍捷：我想聊一下在国内落地的问题。因为中美两国的人工智能落地方式是不同的。在美国，AI 主要通过大型互联网公司落地，他们会做一些增量型产品。但是在过去几年中，中国人工智能的应用基本上都是围绕着社会治理展开的。</p><p></p><p>如果 ChatGPT 这条路这次真的成功了，那么它未来十年的应用路线会是什么呢？我认为仍然是社会治理。 所以我们应该围绕社会治理建立一种什么样的技术架构呢？我想这种技术肯定会提升在各个领域中的治理能力，特别是对于我们现在最关心的金融服务领域来说，它的应用也将非常强大。这种需求是关于 Surveillance Compliance （监控合规）的，未来这种需求只会增加而不会减少。</p><p></p><p>一方面，这种需求将逐渐渗透到每个企业的内部的治理，即数字化转型。这将带来许多新的应用，因此未来可能不仅仅是 MLOps。随着语言处理能力的提高，我们不仅仅是在传统的结构化数据上进行机器学习，更重要的是在数据湖上进行湖商一体化，并实时生成和构造知识。这就是从 MLOps 发展到 KBOps（Knowledge-Based Operations）。另一方面，这种语言处理能力的提升会带来全新的办公套件和工具的全面升级。 在未来的十年里，我们今天所使用的办公软件可能会变得面目全非，大量的机器人将在工作流程的每一个环节上帮助提升工作效率。</p><p></p><p>围绕这个大前提，商业化的方向就变得比较清晰了。它涉及如何帮助人们理解文档、实现商业智能和数据分析。另外还有大量内容的自动化生成， 例如各种报告、研究报告、公告、文件等内部流转的文件。这个领域的市场潜力非常巨大，可能会增长到万亿级别，特别是在社会治理和监管这两个领域。最终这个生意的本质在于领域，而不是技术。</p><p></p><p>我还有另外一个观点，就是这个世界上压根就不应该有太多人工智能公司，就好像世界上绝大多数公司都不是操作系统公司，也都不是所谓的浏览器公司，更多的是扎根场景服务的公司。</p><p></p><h2>ChatGPT 出现后，是否会改变公司的路线图及人力规划</h2><p></p><p></p><p></p><blockquote>主持人Mingke：ChatGPT 出现后，你会如何改变公司的路线图？如何对投资人和团队讲解？你是否考虑过哪些人需要招聘或辞退，业务上需要抓住或放弃哪些东西？</blockquote><p></p><p></p><p>祝海林： ChatGPT 技术的应用可以分为不同层次，如构建 ChatGPT 的公司、开发大模型的公司、将大模型应用于实际场景的公司以及像我们公司一样为 ChatGPT 提供基础设施的公司。</p><p></p><p>我们可以根据自身情况进行调整。对于 Byzer 而言就是帮助大家更好地构建大模型。因为我们认为构建大模型的流程可以被验证和标准化，例如数据处理、构建、人工标注和反馈等环节，这套流程可以固化下来，只需要不断更新数据处理的逻辑即可。我们可能会朝这个方向努力。</p><p></p><p>对于人员招募而言，我们需要那些能快速跟进并选择使用 ChatGPT 的人。因为作为一名技术人员，如果你告诉我你还不知道 ChatGPT，或者你不知道它的作用，那么从我的角度来看你可能不太适合这个职位。人员结构方面，那些能够快速获取知识的人仍然是非常有竞争力的，因为他们不仅知道如何快速获取信息，而且还知道如何利用这些信息。我不认为这种技术会取代一些低端岗位。这是我与大家看法不同的地方。相反，我认为这种技术将帮助他们更好地完成工作。当然，前提是你必须接受这种技术并充分利用它。</p><p></p><p>鲍捷：一些我们原本以为需要三到五年才会发生的事情，已经出现了，所以我们体调整了优先级。本质上我们的工作可以分为三个方面：帮助人们“抄作业”、“查作业”和“写作业”。这三个方面对应的工具分别是 Word、Excel 和 PowerPoint。</p><p></p><p>在过去的6年中，我们向金融机构提供的所有服务本质上都是以上述三个工具为核心的。 例如，如果你需要撰写报告，那么 Word 是必不可少的；如果你需要进行风控，那么 Excel 是必须的；如果你需要完成私募投资的调研报告，那么 PowerPoint 是必需的。因此我们一直在开发各种各样的机器人来帮助我们在这个领域中工作。</p><p></p><p>根据我们以往的路线图，我们最初花费了大量的时间和精力来开发人工智能的 Word。然而在最近的两年中，我们开始着手开发人工智能 Excel。尽管我们最初认为人工智能写作可能要到 3～5 年后才会普及，但现在我们已经发现，我们需要马上开始积极推进这项工作，我非常有信心在中国金融领域中做到最出色的表现。</p><p></p><p>我们也在尝试将这种能力拓展到其他领域，比如最近我们开始在医疗和航空领域尝试帮助医生和航空工程师撰写材料。虽然这些还只是初步尝试，但我相信随着我们公司的成熟，这种通用能力也有可能被孵化出来。不过需要强调的是，因为有创新者的窘境，因此这种能力可能不会以“文因互联”为主体。</p><p></p><p>张晴晴： 总的来说我们的方向并没有发生太大的变化。我们一直坚持在这个方向上，只是这次让大家更清晰地了解我们的价值观。唯一可能会发生一些比较大的变化的是我们对于标注员的要求。我们认为未来的标注员要么是能够非常平衡地工作，要么是专家级别的人才。 我们找到了 100 个标注工程师，其中只有两个人成功通过考试。我们给这些人一些考题，通过这些考题来了解这个人的能力和专业技能，从而确定他是否合格。</p><p></p><p>像 ChatGPT 这样的模型是基于 GPT-3 发展而来，但实际上 GPT-3 主要依赖于从互联网上扒取的各种非监督数据，而 ChatGPT 引入了上千个人做出的问答数据。因此从这个角度来看，我认为我们需要继续积累更多的数据储备，以便更好地支持像 ChatGPT 这样的模型。 例如，如果你尝试过同 ChatGPT 进行多轮交互，你会发现在进行了大约 8～10 轮交互后，它也会开始胡说八道。这是因为随着轮次的增加，模型需要更多的数据来支撑，但是我们的数据储备量还没有达到能够支持这么多轮交互的水平。**当然，这也可能与话题转换有关，我们需要从辩证的角度来看待这个问题。</p><p></p><h2>“谷歌时刻”到来</h2><p></p><p></p><p></p><blockquote>主持人Mingke：从创投的角度来看，我们现在面临的情况是否是“网景时刻”？</blockquote><p></p><p></p><p>鲍捷：我认为这并不是“网景时刻”，而是“谷歌时刻”。 在 1998 年谷歌成立之前，有很多搜索引擎公司，但现在大家都不记得它们的名字了。谷歌做对了一件事情，就是将人类反馈纳入了搜索算法中。</p><p></p><p>今天我想再次强调，针对所有正在从事认知智能领域的公司而言，如果在三年内不能跟上技术进展和转型，那么三年后这些公司都将面临破产的风险。</p><p></p><p>但我认为，当前的时刻并不会导致像谷歌这样的应用成为主流，因为无论是在中国还是在美国，人工智能的应用依然主要是面向企业（ToB）的，而不是面向消费者（ToC）。从盈利的角度来看，ToB 占 80%，而 ToC 只占 20%。在 ToB 领域中很难出现像谷歌这样的巨头。相反，可能会出现数百家公司在不同的领域上进行应用落地的局面。</p><p></p><p>祝海林： 我基本认同。ToC 确实很容易实现大一统。 但在 ToB 领域，每个客户都可能有一些奇怪的需求或者特殊情况，很难说一个公司能够满足所有人的需求。但我始终坚信，通用的大型模型再加上一些其他有趣的功能，可以让公司更好地满足客户需求。</p><p></p><p></p><blockquote>主持人Mingke：作为一名创业者，当我们的企业都已经发展到足够大的程度，可以进行投资时，如果从投资人的角度出发，你会选择投资怎样的公司？你会扶持怎样的初创企业？</blockquote><p></p><p></p><p>祝海林：如果从盈利的角度出发，我现在会选择投资基于大型模型的应用型公司。ToC、ToB都会投。</p><p>我认为像这种自然语言技术，它的趋势之一是多模态转换。 这意味着我们可以轻松地在自然语言、视频和语音之间进行相互转换。这个领域有很多商机和应用，可以被重新颠覆或重写。如果从赚钱的角度考虑，我会投资类似的公司，以实现新的商业模式。例如颠覆原有的招聘流程或公关文案相关领域的公司。</p><p></p><p>如果从更广阔的视角考虑，我可能会投资一些基础软件公司。这些公司可能做基础设施或分布式实现，即使它们的发展周期可能会很长。我认为这两个方面是相辅相成的。我们不能完全跟随热点。这也是业内人士经常批评的一点，即你不能只关注表面的东西，你必须要有坚实的基础。就像 OpenAI 一样，确实需要很多人共同出资才能将其建立起来。</p><p></p><p>张晴晴： 我们是一家数据公司，Magic Data 这条路我认为是不错的。但数据行业有其自身的特点和差异化表现形式，需要专业的行业专家来处理。 因此，我不相信任何一家数据公司可以包揽全部市场份额，因为数据领域的复杂性需要不同领域的专业人才才能胜任。</p><p></p><p>我认为未来的趋势是各个公司会朝着不同的领域专业化发展，形成各自的专业壁垒，使得其他公司难以进入。 例如，在对话式、无人驾驶和工业等领域，会存在专门从事这种数据处理的公司，这种形态已经开始出现。虽然目前行业仍在不断演进，但我相信未来会逐步形成这种裂变趋势。</p><p></p><p>从长期来看，我会选择投资一家类似于脑机接口的公司。我从事数据方面的工作，虽然这涉及到一些人权问题，但我认为脑机接口是一种潜在的数据采集方式。</p><p></p><p>鲍捷： 如果我在中国或美国寻找不同的投资目标，那么在中国，我会投资一家提供“Consulting as a Service”服务，并结合硬件设备使用的公司。</p><p></p><p>我们在中国市场的探索中发现，中国的 ToB 服务与美国市场相比，“SaaS”的第一个 S 和最后一个 S 的顺序是颠倒的。 在美国它是“Software as a Service”，但在中国它是“Service as a Software”。因此美国那种 SaaS 在中国的 ToB 领域里，在可预见的未来是不可能落地的。我们发现，这两个社会或两种经济形态的底层运行规律是不同的，这也是为什么它们存在差异的原因。</p><p></p><p>因此在这种情况下，美国环境下成长起来的软件，在中国市场很难生存。要想在中国落地，最终还是以服务而非软件为主，这是客户最核心的买单因素。 在中国，想要扩大 B 端业务规模，唯有具备高效交付和服务能力。许多人认为人工智能公司正在颠覆软件公司，但事实上人工智能颠覆的是服务公司，是让传统服务公司的效率得到极大提升。这个行业有能力创造真正的正面社会价值，并让生意不断扩大。</p><p></p><p>如果我想进行投资，应该是一家底层公司，提供数字化转型的能力。可能是一个小团队，他们可能会使用传统的方式来工作，效率比较低，但他们在这个行业中已经有了几十年的经验，非常了解这个行业。我会投这样的公司，最终把他们纳入到生态当中。</p><p></p><h2>如果国内不能有效建立起大模型，将有哪些影响？</h2><p></p><p></p><p></p><blockquote>主持人Mingke：如果我们不能在国内有效地建立起大型模型，从长远来看可能会对经济体造成哪些影响？</blockquote><p></p><p></p><p>祝海林： 从宏观的角度来看，如果我们不能在 AI 领域跟其他国家保持同步的话，我们可能会一直处于代差的状态。如果我们不能追赶先进的技术，那么持有先发优势的国家会越来越强大，可能会领先我们数代甚至更多，这类似于光刻机领域的情况。</p><p></p><p>其次，AI 代表着一种生产力，这意味着我们整个社会的效率会受到影响。如果我们的人才没有掌握先进的 AI 技术，他们的工作效率可能会更低，而其他国家可能会更高效地运转，这将导致我们的社会运转效率低下。因此，我们需要重视发展 AI 技术，否则可能会面临不可想象的问题。</p><p></p><p>张晴晴： 我们国家已经不再单纯追求国民生产总值，而是要关注人均产值，这强调了我们要降低成本、提高效率，发挥每个人的最大价值。现在某些行业可能会处于被动状态，因为他们可能已经缺乏对研发的投入。当 ChatGPT 出现时，这种落后感可能会更加严重。 在这种情况下，你是投资还是不投资呢？这是一个左右为难的问题，但如果你问我的话，我会勇敢地投资，因为只有这样才能实现更好的发展。</p><p></p><h2>ChatGPT 本身就是 AGI？</h2><p></p><p></p><p></p><blockquote>主持人Mingke：通常我们使用 “AGI ”这个术语来描述真正的智能。最近似乎大家都认为，ChatGPT 本身就是 AGI。过去我们可能有一个固有的想法，即必须先实现真正的智能，然后才能让它适用于每个行业。从这个角度来看，AGI 是否已经实现了，或者 AGI 的概念是否需要重新界定，以使其适应现有的技术水平呢？在你看来，AGI是否必须要先具备真正的智能？</blockquote><p></p><p></p><p>鲍捷：每当人工智能话题被谈及时，总有一些人会提到 AGI，这让我感到有些不安。 这个问题就像意识何时在生物体中开始一样，是一个无法回答的问题，也许也无需回答。就像真正的爱情一样，无法定义，无法划定范围。对于我们来说，这个问题在可预见的未来并没有多大意义，它只是一个定义问题。任何人都可以有自己的观点。</p><p></p><p>张晴晴： 人工智能的未来发展趋势，可以从人类自身的发展历程中得到启示。我们常说优秀的人才应该具备 T 型特质，即在某个领域内成为专家，同时也具备多样性的能力。在这种定义中，我们首先需要做到的是拥有一项专业技能，形成自己的规律体系，去思考领域内的逻辑体系。当我们掌握了这种逻辑体系后，我们往往比那些没有这种体系的人更容易举一反三。当我们面对其他领域时，我们也会更快地理解它们。我发现，这种特点不仅适用于我，对于其他专家也是如此。</p><p></p><p>因此我们可以说，人类成为专家的过程，就是不断学习知识、吸收数据的过程。当我们积累了足够多的知识，融会贯通后，我们就会具备跨领域的能力。ChatGPT 也是这样一个形态，它在处理某些数据方面达到了极致，从而具备了横跨多领域的能力。人工智能的发展也是这样一个演进过程，不一定需要先有智能的部分。</p><p></p><p>祝海林：我认为现在像 ChatGPT 这样的技术已经是真正的人工智能了。 虽然我们可能认为它只是学习了一些表面知识，就像鹦鹉学舌一样。但实际上它所学习的远比我们所了解的要多得多。许多研究者也许会为此写很多篇论文，从不同的角度解释它，但我们可能并不了解所有这些。</p><p></p><p>我相信 ChatGPT 走在了正确的道路上，因为它与人类非常相似。 它需要学习通识，需要能够承受挑战，需要接受教育，需要学习如何表达自己，并通过人类的反馈来改善自己。而且，如果它的回答错误，你可以指出并纠正它。它甚至可以理解你的偏好，例如：“我的妻子永远是对的”。在我看来，这个过程已经非常接近人类，虽然它仍然有许多缺陷，但这些缺陷已经不是主要问题。</p><p></p><h2>“human-like AI” VS “human-level” AI</h2><p></p><p></p><p></p><blockquote>主持人Mingke：最近经常被提及的一个概念是“human-like AI ”和“human-level AI” ，用来代替过去人工智能追求的目标。然而，人类会犯错，如果我们造一个类似人的 AI，它也会犯错，这是可以接受的吗？如果你们更偏向 human-like AI 的路线，你们是否支持开发这样的项目，即使这些 AI 也会像人一样犯错？</blockquote><p></p><p></p><p>祝海林： 只有在犯错、胡说八道的时候，AI 才会展现出它的潜力。有时候，它可能会说出一些我们认为毫无意义的话语，但我们暂时无法判断其正确性。这也意味着 AI 有可能比人类更加先进。</p><p></p><p>张晴晴： 就像男孩子和女孩子谈恋爱一样，每个人都有自己的个性和缺点，但这并不妨碍他们相爱并共同成长。同样，人工智能也应该被允许犯错和有缺点，这样才能真正成为像人类一样的存在。我们不能一味地追求完美和无错，因为这不符合现实。</p><p></p><p>鲍捷：有时候，一些美好的事物并不是新的，而是在某一时刻以出人意料的方式重新组合而成。 比如瓦特发明蒸汽机时，他并没有发明神奇的新东西，他只是通过新的机械构造，提高了蒸汽机的效率。</p><p></p><p>其实 ChatGPT 里的元素并不是全新的东西，ChatGPT 就是将这些技术有机地组合在一起了。虽然我没有看过 ChatGPT 的核心代码，但我坚信它一定不是纯粹的语言模型，其中一定涵盖了许多工程性的元素。这种计算机体系架构的能力以及大规模数据的操作能力，可能才是真正的“奥秘”。这些关键因素往往不会在论文中公开，而是需要进行深入的工程研发。我们不能仅仅通过抄袭来获得这些关键技术，因为抄袭的产品是没有灵魂的。因此，我们必须自己开发出一款产品，但这款产品必须以更加简单的方式为基础，尽早推出 MVP，并不断迭代和循环改进。</p><p></p><p>大规模语言模型的重要性等同于操作系统和浏览器内核。对应的技术复杂性也是等价的。维护这种模型可能需要上千人的团队长期维护。这个操作系统之上会形成一个庞大的生态系统，这可能会带来软件革命。</p><p></p><p>程序等于算法加上数据结构的概念可能会被改写，因为数据可能不再仅仅是我们理解的传统意义上的数据，而是更加丰富的结构化数据。大规模语言模型本身就是一种数据，但更好的称呼应该是“知识”。</p><p></p><p>算法的意义可能也会变得更加广泛，不再仅仅是我们以前理解的底层代码，而是更多机器生成的源代码或模块，这需要更多的人去引导和提示。未来的提示学习工程师可能会比软件工程师多。在未来，程序=知识+提示，这可能是一种全新的范式，未来的每个人可能都是在做提示学习。</p>",
    "publish_time": "2023-04-10 13:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "基于 Vue 和 Canvas，轻舟低代码 Web 端可视化编辑器设计解析 | 低代码技术内幕",
    "url": "https://www.infoq.cn/article/ssRCU5cgGPGxJNHH2TRU",
    "summary": "<p></p><blockquote>自 2020 年来，网易数帆探索可视化低代码编程已两年有余，打造了轻舟低代码平台用于企业应用开发。然而，不少编程技术人员对这一领域还比较陌生。我们开设《低代码技术内幕》专栏，旨在讨论低代码编程领域中的困难、问题，以及高效的解决方案。本文为第二篇，结合我们的产品研发经验解读打造 web 端可视化代码编辑器需要权衡的因素以及技术实现的要点。专栏第一篇：<a href=\"https://www.infoq.cn/article/Us10TOa418u2xwfZ19Gp\">低代码编程及其市场机遇剖析 | 低代码技术内幕&nbsp;&nbsp;</a>\"</blockquote><p></p><p></p><p>轻舟低代码平台是一款基于云服务的 web 端产品，面向零基础或者有一定编程基础的用户。用户不需要额外安装软件，就可以在任何有浏览器的电脑上编写和发布应用。可视化代码编辑器是轻舟低代码平台的重要组成部分，用户通过可视化界面开发应用。然而，在 web 端构建一个拥有良好体验的可视化编程工具是一个很大的挑战：良好的体验要求良好的视觉效果、交互、性能；而视觉效果越花哨，交互越复杂，性能也就越低，过低的性能（卡顿）会影响体验，但如果单纯为了性能，减配视觉效果和交互，又会显得简陋，所以可视化代码编辑器需要掌握好这个平衡。</p><p></p><p>为了完成这个挑战，我们在 canvas 上模拟实现了浏览器的事件、容器、布局等功能。另外，为了兼顾团队本身的技术栈（Vue）和项目的可维护性，我们最终使用了与 Vue 框架结合，通过 Vue 模板来控制 canvas 渲染的方案。</p><p></p><p>下面我们从渲染、交互、数据与视图三个方面来介绍。其中渲染部分主要考虑了性能问题，交互部分介绍了如何模拟浏览器的事件机制，数据与视图部分说明了如何与支持双向绑定特性的 Vue 框架结合。</p><p></p><h2>高性能渲染</h2><p></p><p></p><p>低代码可视化编辑器保留了控制流的设计，所以在整体结构上类似于传统的流程图。但其与流程图有两个明显的区别：</p><p></p><p>流程图的节点相对简单且布局自由，而轻舟低代码的可视化代码编辑器的节点多且复杂（超一个量级）且布局严格。这种复杂性来自于编程语言本身：我们的低代码编辑器的结点相当于编程语言中的语句（statement），如循环语句 for、条件语句 if 等，这些语句内部又包含很多条表达式（expression），如算数运算、函数调用等；表达式可以嵌套组合，因此每个语句结点可以异常复杂。流程图的交互操作简单，而可视化代码编辑器的交互复杂。我们提供给用户的交互粒度是表达式级别的，它包括了鼠标悬停、点击、拖拽和键盘输入等，我们需要处理好语句中嵌套的、表达式中嵌套的每一个表达式，这些交互的复杂性都是流程图所不具有的。</p><p></p><p>为了处理好这些问题，我们调研了主流的渲染方式。目前浏览器提供的主流渲染方式有以下三种：</p><p></p><p>HTML + SVG + CSS    这是一种成熟的渲染方式，它提供的 API 中包含事件以及对于内部绘制对象操作的方法。但这种渲染方式有很多历史包袱，要保持不同浏览器的渲染一致性比较困难。其次由于它要使用 DOM 来操作节点，会比下面提到的 2D canvas 要慢。并且这种方式只适合于布局相对稳定的整体交互，因为布局变化会触发 DOM 重排，而频繁的 DOM 重排会成为性能瓶颈。另外这种方式在高分辨率的屏幕上有时无法做到抗锯齿，渲染效果无法保证。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/cb/06/cb234b1b4f49d4f50ee520e323481506.jpg\" /></p><p>2D Canvas    2D canvas 是一种高性能的相对高度抽象的立即绘制模式的 API，其渲染依赖于一个 Frame 内执行的指令。但是 API 中并不包含事件以及对于内部绘制对象操作的方法，需要额外设计框架实现。</p><p><img src=\"https://static001.infoq.cn/resource/image/20/65/2045e4734d314030a90b76755b071265.jpg\" /></p><p>WebGL    WebGL 是一种更为底层的渲染技术，它通过基于 OpenGL ES 的 API 控制着色器渲染 2D 或 3D 场景，性能比 2D canvas 更强，但是技术门槛较高。它提供的 API 中也不包含事件以及对于内部绘制对象操作的方法，需要额外引入框架来实现。</p><p></p><p>在考虑到团队技术栈和开发周期的同时，为追求更好的用户体验，我们选择了使用 2D canvas API，并实现了一个自研的框架 JFlow。JFlow 框架通过模拟浏览器的事件系统以及布局系统，可以无缝嵌入到我们的 Vue 工程下，让我们的前端工程师能够快速迭代业务需求。</p><p></p><h2>细粒度交互</h2><p></p><p></p><p>为了支持表达式级别的细粒度交互，我们在 JFlow 框架中实现了一整套可交互的基础设施，我们的交互设计是基于浏览器内部基本的事件，没有超出浏览器给出的事件范围。HTML 中 <canvas> 的事件都是从顶层触发的，如果要在画布内部，针对每个绘图单元来实现交互，得从 <canvas> 上的原始事件出发，通过构建画布内部的多级坐标系统，定位到具体的绘图单元（捕获），结合浏览器事件本身和定位到的绘图单元层级，再模拟事件抛出（冒泡）的过程，在画布内模拟出浏览器的原生事件系统，通过结合前端事件基础知识，从而实现业务中的交互设计。下面我们从定位、状态、事件三点来介绍：</canvas></canvas></p><p></p><p>在什么定位下、在什么状态下、在什么浏览器事件下才能触发什么交互？交互是单一确定的，还是个像事件列表那样有优先级？会冒泡到父对象的交互中处理吗？</p><p></p><p>定位    定位的基础在于坐标系统，canvas 的坐标系  是固定的，而 canvas 内部对象的坐标系  也是相对固定的，坐标系  与  之间存在以下关系：</p><p></p><p>$$节点的位置由具体的节点布局算法来确定，算法基于内部对象坐标系来计算。坐标系  上的节点及内部节点均设计为中心对称，即节点在其父级坐标系的坐标为图形中心，这种设计方便设置行列对齐。节点内部的子坐标系以图形中心 为原点，若子坐标为 ，父坐标为 ，则内部的父子坐标系存在如下关系：$$节点内部绘图单元的位置，由节点上具体布局算法来确定，算法基于父级坐标系来计算。状态    判断状态的最常见的方法是碰撞检测，鼠标交互的实现仅需要判断交互点与具体几何图形的关系，即接触到图形或未接触到图形。通过捕捉这个状态的变化，来判断当前正在交互的对象。计算是判断的意思？</p><p></p><p>事件    Canvas 内部本身不存在事件，要模拟内部事件，需要从顶层分析 canvas 的原始事件，模拟浏览器事件捕获和冒泡的过程来实现。在有了定位的坐标系和逐层的状态判断之后，就能够从顶层的坐标计算出当前交互的对象，并向上抛出事件。</p><p></p><p>通过构建这些基础设施，我们模拟了事件机制并获得了对内部绘制对象操作的方法，但是 JFlow 作为语言的基础渲染框架，还需要增强对抽象语法树（abstract syntax trees，AST）的互操作能力，见下一小节。</p><p></p><h2>数据与视图</h2><p></p><p></p><p>我们上面的设计相当于在 canvas 上加了 DOM 的部分能力，使其能够更方便地接收事件和操作内部绘图对象。但是直接操作 DOM 的缺点在于视图操作和业务逻辑会耦合在了一起，在我们的场景下就是在操作 AST 的同时，还要同时操作渲染对象。为了解耦，以及减少业务代码出错造成的渲染问题，我们需要引入 MVC 或者 MVVM 之类的框架。</p><p></p><p>我们选择了与现有的一些 MVVM 的框架结合来解决这个问题。但是在结合之前，我们还需要补齐一些能力。</p><p></p><p>数据对象与渲染节点：在浏览器内部，布局是由 CSS 来控制的，而在 JFlow 内部，布局由布局算法来确定，布局算法建立了数据和布局之间的关系。在编辑器场景下，顶层的布局需要根据 AST 来绘制出基于控制流的固定布局，为了控制 AST 上节点的位置，需要根据具体 AST 上的节点来查找到具体的渲染节点。JFlow 通过引入 WeakMap 来解决这个问题：在节点被添加时，建立从 AST 节点到具体渲染节点的映射关系。数据改变与渲染变更：MVVM 框架的模板会响应数据的变化，数据变化会引起视图上的变化。JFlow 通过 vue plugin 提供了响应数据变化渲染的能力，内部通过调用 scheduleRender 方法，在浏览器每次重绘之前触发一次性的渲染。</p><p></p><p>现在我们可以引入 MVVM 框架了：</p><p></p><h3>数据流向</h3><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b4/e6/b40748a981ac448a632462f65e899be6.png\" /></p><p></p><p></p><p>虚线左侧是 AST 的使用部分，右侧是框架控制的部分，原始数据决定了布局算法，布局算法控制渲染节点的位置，原始数据节点经过布局节点对应的模板，来控制渲染节点，框架再通过事件通知原始数据或模板更新，进而更新渲染节点。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/75/fb/7557cb22a223aecb988c76536392f5fb.png\" /></p><p></p><p></p><p>上图更具体说明了整体数据流向，Vue 起到了 MVVM 的作用。原始数据更新布局，相当于浏览器里面的重排。</p><p></p><h3>一个例子</h3><p></p><p>假设在某种语言里有：</p><p></p><p><code lang=\"javascript\">class A extends B mixin C, D {}\nclass B implements E {}\n\nclass C {}\nclass D {}\ninterface E {}\n</code></p><p></p><p>这样的代码，我们将用 jFlow 来实现如下的展示效果：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/26/04/2697d03a9ee53e696bc184799270e004.gif\" /></p><p></p><p></p><p>首先，我们构建描述 A、B、C、D、E 这几个类型之间的关系的 JSON：</p><p></p><p><code lang=\"javascript\">[\n    {\n        \"name\": \"A\" ,\n        \"extends\": \"B\",\n        \"mixins\": [\"C\", \"D\"]\n    },\n    {\n        \"name\": \"B\" ,\n        \"implements\": \"E\"\n    },\n    {\n        \"name\": \"C\" \n    },\n    {\n        \"name\": \"D\" \n    },\n    {\n        \"name\": \"E\" \n    }\n]\n</code></p><p></p><p>然后，我们根据视图和数据的关系来构建总布局。</p><p></p><p>总布局由布局节点和布局组成：</p><p></p><p><code lang=\"javascript\">// DemoLayout.js\nclass VirtualNode {\n    constructor(source) {\n        this.type = 'VirtualNode';\n        this.source = source;\n    }\n\n    makeLink(callback) {\n        const {\n            extends: ext, mixins, implements: impl\n        } = this.source;\n        if(ext) {\n            callback({\n                from: ext,\n                to: this.source.name,\n                part: 'extends',\n            })\n        }\n\n        if(mixins) {\n            mixins.forEach(t =&gt; {\n                callback({\n                    from: t,\n                    to: this.source.name,\n                    part: 'mixins',\n                    fontSize: '24px',\n                    lineDash: [5, 2]\n                })\n            })\n        }\n        if(impl) {\n            callback({\n                from: impl,\n                to: this.source.name,\n                part: 'implements',\n            })\n        }\n    }\n}\n\nclass DemoLayout {\n    constructor(source) {\n        this.static = false;\n        // 布局实例必须包含节点(flowStack)和边界(flowLinkStack)\n        this.flowStack = [];\n        this.flowLinkStack = [];\n        const nodeMap = {};\n        const nodes = source.map(s =&gt; {\n            const node = new VirtualNode(s);\n            nodeMap[s.name] = node\n            return node;\n        });\n        nodes.forEach(node =&gt; {\n            this.flowStack.push({\n                type: node.type,\n                source: node.source,\n                layoutNode: node,\n            })\n            node.makeLink((configs) =&gt; {\n                const fromNode = nodeMap[configs.from];\n                const toNode = nodeMap[configs.to];\n                if(!fromNode) return;\n                if(!toNode) return;\n                \n                this.flowLinkStack.push({\n                    ...configs,\n                    from: fromNode,\n                    to: toNode\n                })\n            })\n        });\n        this.erNodes = nodes;\n    }\n\n    reflow(jflow) {\n        const nodes = this.erNodes;\n        nodes.forEach((node, idx) =&gt; {\n            // 计算 节点位置\n            const renderNode = jflow.getRenderNodeBySource(node.source) \n            renderNode.anchor = [-idx * 220, (idx % 2) * 80];\n        });\n    }\n\n}\n\nexport default DemoLayout;\n</code></p><p></p><p>接着，在全局引入 vue 插件</p><p></p><p><code lang=\"javascript\">import Vue from 'vue'\nimport { JFlowVuePlugin } from '@joskii/jflow';\nimport App from './App.vue'\n\nVue.config.productionTip = false\nVue.use(JFlowVuePlugin);\nnew Vue({\n  render: h =&gt; h(App),\n}).$mount('#app')\n</code></p><p></p><p>其中导入的 App.vue 代码如下：</p><p></p><p><code lang=\"javascript\">\n    \n        \n            \n        \n        \n            \n            \n        \n    \n\n\n</code></p><p></p><p>其中引入的 virtual-node.vue 代码如下：</p><p></p><p><code lang=\"javascript\">\n    \n        \n        \n    \n\n\n</code></p><p></p><p>运行这些代码，即可得到本小节开头的效果展示图。</p><p></p><h2>JFlow 在低代码平台中的效果</h2><p></p><p></p><p>我们用同一个 JFlow 框架，搭建了轻舟低代码平台的可视化编程引擎、流程图引擎、实体-联系图（ER 图）引擎，下面我们通过一组截图，分别展示其效果。</p><p></p><p>可视化编程：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/08/7a/08a17d6e5245593a06db916f2c53ba7a.png\" /></p><p></p><p></p><p>流程图：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/7b/36/7bcf3a7945cbb59014122866d1c43236.png\" /></p><p></p><p></p><p>实体-关系图：</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/c8/de/c8d16df8cddd5bccd9e4ef1d1c6bedde.jpeg\" /></p><p></p><p></p><h2>结论</h2><p></p><p></p><p>我们在框架设计实现中一直在寻求用户体验、可拓展性、性能、学习梯度、团队合作之间的平衡。这些框架设计可能跟具体特性不太一样，只有在缺失的时候才会被注意到，但是这些设计为我们的团队和产品带来了深远的影响和改变。</p><p></p><p>作为框架的设计和实现者，我们很开心轻舟低代码可视化编程能够为零基础或者有一定基础的开发者，带来良好的编程体验。未来，我们会继续探索可视化编程的可能性。</p><p></p><p>作者简介：</p><p></p><p>网易数帆编程语言实验室，负责轻舟低代码平台核心编程能力的设计，包括类型系统、语义语法、声明式编程、可视化交互等 NASL 的语言设计，Language Server、可视化引擎等，以及后续演进方案的规划和预研，旨在创造低门槛高上限的低代码开发体验。</p>",
    "publish_time": "2023-04-10 15:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]