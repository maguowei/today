[
  {
    "title": "领域驱动探索：开启架构现代化转型之路",
    "url": "https://www.infoq.cn/article/ZmNiV7F2973R19QVTYkO",
    "summary": "<p>项目的成功始于强有力的探索能力。对于构建新数字产品和服务的团队来说，这通常包括在编写代码行之前进行的用户研究、需求收集和待办事项的创建。但是，如果你的项目是对错综复杂的旧遗留系统进行现代化改造或是将所有工作负载迁移到云上，该怎么办呢？在发布一款新产品时，你如何才能满怀信心地启动项目呢？</p><p></p><p>本文提供了一种指导方法，通过领域驱动探索（Domain-Driven Discovery，DDD）启动下一个架构现代化项目。为了说明这一点，我们将使用我的一个客户为例，这是一家医疗用品供应公司，它正在将其所有核心系统迁移到云上，并且需要创建一个未来状态架构和实现这一目标的计划。</p><p></p><p>我们分解了关键步骤和常见的视觉效果，可以让你的团队有信心使用<a href=\"https://martinfowler.com/bliki/DomainDrivenDesign.html\">领域驱动设计（Domain-Driven Design）</a>\"来创建未来状态架构。你可以将本文作为参考来对单个系统或组合系统的架构进行现代化改造。</p><p></p><h2>从探索开始</h2><p></p><p></p><p>曾经有一段时间，我们以周期为两周的Sprint 0开始新的敏捷项目，然后直接开始编写解决方案。不幸的是，团队后来经常发现他们把时间和金钱浪费在了“把错误的事情改对”上。受设计思维（Design Thinking）和<a href=\"https://www.svpg.com/dual-track-agile/\">双轨敏捷（Dual-Track Agile）</a>\"以及像<a href=\"https://www.mobiusloop.com/\">Mobius</a>\"这样框架的影响，我们集体开阔了视野，认识到了简短探索对产品运营的重要性。</p><p></p><p>但在过去的五年里，对于各种规模的客户，从初创企业到财富100强，我们都已将领域驱动设计应用于它们的架构现代化项目中了。这些经验，再加上对团队培训的投资，帮助我们学会了如何运作有时间限制的DDD项目，从而增加了我们架构现代化项目的成功机会。</p><p></p><p>本文通过四个集成步骤来为你的团队提供架构现代化项目的蓝图：</p><p>框定问题——明确你要解决的问题、受影响的人、期望的结果和解决方案的约束。分析当前状态——探索现有的业务流程和系统架构，以建立改进基线。探索未来状态——基于有界上下文设计现代化架构，设定战略优先级，评估选项并为未来状态创建解决方案设计。创建路线图——创建一个计划，随着时间的推移对架构进行现代化改造，并使其与期望的工作流或结果保持一致。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure1-large-1683727412214.jpg\" /></p><p></p><p>图1——域驱动探索概述</p><p></p><h2>步骤0：战略或战术DDD</h2><p></p><p></p><p>在开始之前，确定你的工作等级：</p><p>战略DDD——示例：使用DDD对组合系统进行现代化改造。将应用程序迁移到云上，并为复杂的遗留系统创建现代架构。战术DDD——示例：使用DDD对单个系统进行现代化改造，例如重新构建Web应用程序或构建新产品。</p><p></p><p>根据我们的经验，一个小团队可以在4-6周内完成战术DDD的探索。战略DDD的探索通常需要同一团队8-12周的时间才能完成。</p><p></p><h3>探索团队</h3><p></p><p></p><p></p><p></p><h3>探索时间表</h3><p></p><p></p><p></p><p></p><p>我们发现，具有时间限制的探索创造了一种专注的紧迫感，可以帮助团队规避分析瘫痪。因为DDD使用两周的增量，所以它非常适合现有敏捷团队的日程安排。在开始之前，组建合适的团队，制定时间表，然后开始吧！</p><p></p><h2>步骤1：框定问题</h2><p></p><p></p><p>在架构现代化项目中，经常有很多口头上提到要使用的现代技术，如微服务、无服务器（serverless）、Kubernetes或服务网格等。人们往往会掩盖他们试图要解决的问题和他们希望实现的结果。但这就是我们应该开始的地方。</p><p></p><p>以两到三个小时的研讨会开始，围绕一个共享的Miro板和一系列练习，要求团队作为一个集体来澄清：</p><p>问题——我们要解决什么问题？人——谁是受影响的人？成果——成功后会是什么样的？约束——我们需要考虑哪些约束条件？</p><p></p><p>对齐这些问题，找到答案至关重要。</p><p></p><p>对于我们的医疗用品供应客户来说，问题的框定对于定义其核心系统提高弹性和可维护性的成功结果特别有帮助。只需迁移到云上并在多个区域中运行即可解决弹性问题。但是在这一步中，团队开始明白他们还需要提高长期的可维护性，因为他们复杂的架构已经有机地发展了15年。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure2-large-1683727412214.jpg\" /></p><p></p><p>图2——问题描述、涉众、成果和约束</p><p></p><p>将你的架构现代化与成果相结合，可以为所有相关人员阐明项目背后的“原因”。 它还建立了改进基线，并鼓励设定目标，这样你就可以随着时间的推移来衡量进展了。</p><p></p><h2>步骤2：分析现状</h2><p></p><p></p><p>框定问题后，你就可以进入第2步了。在这一步中，你需要同时关注两件事：业务流程和系统架构。</p><p>我们建议使用<a href=\"https://github.com/ddd-crew/eventstorming-glossary-cheat-sheet\">事件风暴</a>\"研讨会来阐明与作用系统相关的业务流程。首先选择要关注的主要流程或体验，例如新客户注册。接下来，协同识别该端到端流程中的每个事件。重要的是关注它当前的运作方式，而不是它将来应如何运作。然后确定对流程至关重要的事件子集，并标记这些关键事件。根据经验，仅使用关键事件，你应该就能向外行描述端到端的流程了。</p><p></p><p>对于战术DDD项目，一个单独的研讨会和一些后续的讨论通常就足够了。对于战略DDD项目，你可能需要举办多个专注于不同用户和流程的研讨会。我建议在第一周时，从一个研讨会开始，然后根据需要再安排其他研讨会。对于战略DDD，你不需要详尽地列出整个组织中的每个事件，只需列出足以让你自信地划分与作用系统组合相关的有限上下文即可。</p><p></p><p>在这些研讨会中，你应该将一种共同的语言与视觉效果配对使用，以使每个人对现今事物的运作方式有一个共同的看法。</p><p></p><p>例如，这是我们医疗用品客户的事件风暴。图3显示了围绕初始客户注册的事件。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure3-large-1683727412214.jpg\" /></p><p></p><p>图3——初始客户注册的详细信息</p><p></p><p>图4缩小显示了整个新的客户旅程，其中一些细节被模糊掉了。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure4-large-1683727412210.jpg\" /></p><p></p><p>图4——新客户旅程的事件风暴</p><p></p><p>同时，你应该深入研究当前状态的架构。我们<a href=\"https://c4model.com/\">喜欢C4模型</a>\"，因为它们很简单，建议从<a href=\"https://c4model.com/#SystemContextDiagram\">某个上下文（C1）关系图</a>\"开始，将作用系统放在中间，这样你就可以看到包括集成在内的整个生态系统了。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure5-large-1683727412214.jpg\" /></p><p></p><p>图5——当前状态的上下文关系图（C1）</p><p></p><p>接下来，我们将深入到创建容器（C2）关系图，并阐明不同的组件，例如Web应用程序、后端服务、数据库和消息传递。与事件风暴一样，这通常是在前两周的研讨会中完成，并根据需要进行后续的调整。尽可能准确地做到这一点至关重要，这样未来的状态建议才能以现实为基础。</p><p></p><p>对于我们的医疗用品供应客户来说，C2关系图说明了他们核心系统的复杂性，我们必须要在他们的迁移计划和长期架构现代化中考虑到这一点。图6显示了不同角色是如何与连接到订单管理系统的门户进行交互的。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure6-large-1683727412214.jpg\" /></p><p></p><p>图6——客户与连接到订单系统的门户进行交互</p><p></p><p>图7缩小显示了生态系统中的所有组件以及它们之间的连接方式。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure7-large-1683727412214.jpg\" /></p><p></p><p>图7——当前状态的容器关系图（C2）显示了当前系统的复杂性</p><p></p><p>至少，我们为每个项目创建了事件风暴、上下文关系图（C1）和容器关系图（C2）。根据项目类型的不同，我们可能需要用额外的视觉效果来对这些进行补充。</p><p></p><h2>步骤3：探索未来状态</h2><p></p><p></p><p>现在你已经对当前状态有了深入的了解，你可以继续执行第3步，根据与业务模型一致的<a href=\"https://martinfowler.com/bliki/BoundedContext.html\">有界上下文</a>\"来创建未来状态。这些有界上下文将指导团队进行架构现代化，并确保技术架构以业务的运作方式为基础。在探索有界上下文时，在业务流程和系统的作用域内寻找<a href=\"https://medium.com/clarityhub/low-coupling-high-cohesion-3610e35ac4a6\">高内聚和低耦合</a>\"的区域。</p><p></p><p>为此，我们可以克隆事件风暴，然后使用红色标记来识别潜在的有界上下文。图8显示了医疗用品供应公司的情况。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure8-large-1683727412214.jpg\" /></p><p></p><p>图8——带有草稿上下文的事件风暴</p><p></p><p>在决定要在哪里划定界限时，关键事件通常会提供线索，但这并不是标准公式。如有疑问，请从较少的上下文开始，然后根据反馈进行调整。在战术DDD项目中，我们通常会发现10个或更少的面向业务的上下文。对于战略DDD项目，我们发现的业务上下文可能是这个数字的两倍或三倍。记住要使用领域专家同意的术语来命名面向业务的上下文，并添加支持性的技术上下文，例如共享服务和分析。</p><p></p><h3>使用消息流细化有界上下文</h3><p></p><p></p><p>接下来，我们将创建一个带有消息流的有界上下文关系图，该关系图阐明了如何在上下文之间发送消息。图9显示了客户和订单上下文的详细视图:</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure9-large-1683727412214.jpg\" /></p><p></p><p>图9——客户和订单上下文的详细信息</p><p></p><p>图10缩小显示了组织中所有上下文的鸟瞰图，并用带有编号的消息来说明新客户旅程的步骤。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure10-large-1683727412214.jpg\" /></p><p></p><p>图10——带有消息流的有界上下文关系图</p><p></p><p>DDD的核心是一个为期一天的研讨会，用于验证步骤2和步骤3的结果。将这个研讨会安排在项目的中途，并确保邀请领域专家、技术专家和主要利益相关者参与。</p><p></p><p>在此研讨会中，会发现团队带领参与者了解有界上下文和消息流，一步一个脚印地工作，并与领域专家实时澄清术语。团队将会添加并删除一些箭头。我们应该期待有关命名和事物是如何运作的激烈辩论。在领域专家努力保持一致的同时，技术人员应该将各个点连接起来，确认整个系统是组合在一起的。</p><p></p><p>在我们的医疗用品供应客户的研讨会上，他们的领域专家建议将收入周期管理（Revenue Cycle Management）拆分到自己的上下文中，而不是将其与付款人（Payer）合并。随后的讨论帮助大家理解了这两种情况之间的差异。我们实时地重新调整消息，最重要的是，整个团队在共享语言和对消息如何在上下文之间流动方面的理解保持了一致。</p><p></p><h3>在解决方案之前先探索选择</h3><p></p><p></p><p>架构现代化项目通常从基于企业愿景或战略的总体思路开始的，但从A到B的方法不止一种，因此有很多选择可供探索。在确定解决方案之前，请先花点时间探索下选择。我们选择的审查广度和分析深度取决于项目的类型和范围以及客户文化，但我们应该为这项工作安排时间。</p><p></p><p>例如，我们可能会评估是Amazon Web Services（AWS）还是Microsoft Azure更适合我们，或者评估使用云管理的数据库是否比使用在虚拟服务器上运行的数据库更具优势。我们还可以考虑用多种方法来重新构建当前系统和数据库，以便更好地使它们，并与未来状态的有界上下文保持一致。</p><p></p><p>我总是坚持先有选择再有解决方案，因为这会迫使团队通过多种方式的思考来解决问题。结果通常是每种方法的最佳方面的某种组合，这通常会产生更好的结果。</p><p></p><h3>从战略上对有界上下文进行分类</h3><p></p><p></p><p>在DDD中，我们喜欢使用<a href=\"https://github.com/ddd-crew/core-domain-charts\">核心领域图表</a>\"来清楚地确定哪些上下文是最重要的战略差异化因素，从而使我们能够相应地调整投资。在这个过程中，我们经常让执行领导参与，以揭示战略洞察力，例如：</p><p>我们应该在什么时候构建自定义软件，而不是从市场供应商那里购买？未来我们应该在哪些方面增加或减少投资？根据我们的业务战略。我们的上下文将如何随着时间的推移而演进？我们是否期望基于我们的商业模式或战略出现新的上下文？我们目前对人员和系统的投资如何与我们的有界上下文相适应？</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure11-large-1683727412215.jpg\" /></p><p></p><p>图11——上下文的战略性分类</p><p></p><p>要回答这些问题，请先从模型复杂性和业务差异化的尺度建立今天的基线。在研讨会上，获取新的有界上下文并将它们依次拉入到关系图中，同时调整它们在x轴和y轴上的相对位置。首先要询问领导者，“你认为你的竞争优势在哪些？”并将这些上下文拉入到<a href=\"https://medium.com/nick-tune-tech-strategy-blog/core-domain-patterns-941f89446af5\">关系图的核心（Core）部分</a>\"上。接下来，问下：“哪些上下文对我们来说不是唯一的？”并将它们拉入到关系图的通用（Generic）部分中。其余上下文属于关系图的支持（Supporting）部分。它们是业务必需品，但提供的投资回报（ROI）有限。添加新上下文时继续调整关系图。最后，引入实验（Experimental）部分来作为一个可能颠覆公司商业模式或行业的大赌注。</p><p></p><p>如图11所示，我们客户的付款人（Payer）上下文是他们的“秘密武器”，也是最大的市场差异化，其次是他们的客户（Customer）上下文。另一方面，收入周期管理（Revenue Cycle Management）很复杂，但并不是他们所独有的。这种洞察力揭示了用供应商的解决方案替换他们自定义的解决方案来提高其核心系统可维护性的机会。</p><p></p><p>这个研讨会应该是一个民主的过程，其中有很多关于每个上下文在这张关系图上的位置的反复讨论。完成后，我们就有了一个独特的视觉效果，可以战略性地对架构的构建块进行分类：我们的有界上下文。根据我的经验，这通常是主管们“理解”并开始欣赏DDD带来清晰度的时候。团队也能从中受益，因为他们了解了构建自定义解决方案的意义所在（核心领域），以及商用现成（COTS）解决方案的意义所在（支持和通用领域）。该研讨会可以帮助我们避免将错误的东西构建成正确的！</p><p></p><h3>使未来状态架构与有界上下文保持一致</h3><p></p><p></p><p>确定了上下文并对其进行了战略性分类后，我们就拥有了与业务模型保持一致的未来状态架构构建块。从这里，我们就可以开始将选择转化为解决方案设计了。</p><p></p><p>在探索过程中，此时所涉及的设计主题高度依赖于我们正在处理的现代化项目的类型。对于像云迁移这样的战略DDD项目，我们将关注云基础功能、网络和安全边界以及工作负载的部署。对于战术DDD项目，比如创建一个新的Web应用程序，我们需要关注微服务、API和数据模型。</p><p></p><p>对于我们的医疗用品供应客户，设计的重点是：1）将应用程序迁移到云上；2）将其应用程序重构为与上下文保持一致的微服务，包括数据库、事件消息传递、API网关和可部署组件等。我们使用容器（C2）关系图来可视化未来状态的样子以及它与当前状态的不同之处。图12突出展示了客户和订单上下文相关的组件。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure12-large-1683727412214.jpg\" /></p><p></p><p>图12——客户和订单上下文相关的组件</p><p></p><p>图13缩小展示了未来状态架构的所有组件，这些组件与有界上下文保持一致，其中包括跨多个上下文的共享组件，如联邦API网关、消息总线和数据仓库。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure13-large-1683727412214.jpg\" /></p><p></p><p>图13——未来状态架构的容器图（C2）</p><p></p><p>本身没有用于创建和改进这些设计的研讨会。我们经常在多个小组会议中研究它们。技术负责人通常通过与提供输入和反馈的客户和专家密切合作来创建这些关系图。我们建议将所有的关系图保存在一个虚拟板上，这样每个人都可以异步地查看它们，并随着设计的演变对其进行评论。我们的团队还应该研究未来设计所考虑的技术能力和限制。</p><p></p><p>到目前为止，我们一直关注支持业务领域的有界上下文。但是支持组织基础运营的技术服务呢？如何将它们建模为有界上下文的呢？随着时间的推移，我们已经确定了几个应该添加用以支持面向业务的上下文：</p><p>共享服务上下文——面向业务上下文使用的通用DevOps工具、网络、日志记录、消息传递、监视和其他服务。安全服务上下文——身份、身份验证和其他安全工具。分析上下文——数据仓库、分析、数据转换、机器学习、报告和其他数据工具审计上下文——审计日志记录和合规性工具</p><p></p><p>有时将网络分离到单独的上下文中是有意义的，有时我们可以简单地将其包含在共享服务中，这取决于我们的基础架构。</p><p></p><p>设计活动可以很容易地耗完我们分配给它时间，所以一定要将这一步限制在几周内。这段时间足够用于创建未来状态的参考架构，它可以作为起点。我们可以在实施过程中处理剩余的设计决策。目标是到达一个“足够好”的位置，然后继续创建路线图。</p><p></p><h2>步骤4：制定路线图</h2><p></p><p></p><p>实施前的最后一步可以帮助我们定义创建未来状态架构所需的工作、顺序和时间表。我们建议使用一个简单的列表来表示灵活的时间范围（现在-Now、下一个-Next和以后-Later），而工作流或结果行则表示对工作进行分组的方式，如图14所示。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure14-large-1683727412214.jpg\" /></p><p></p><p>图14——带有建议的未来状态路线图</p><p></p><p>对于我们的医疗用品供应客户，我们首先确定了一个重要的里程碑，需要在“现在”阶段结束时完成：他们的所有应用程序都需要“准备好迁移”，他们的云基础必须准备好安全地承载工作负载。就此含义达成一致，并迅速制定路线图的初。锚定这一里程碑有助于帮他们确定在“现在”阶段必须完成的事情，以及可以等到“下一个”和“以后”再做的事情。图15显示了路线图上的一些初始项目。</p><p></p><p><img src=\"https://imgopt.infoq.com/articles/architecture-modernization-domain-driven-discovery/en/resources/1figure15-large-1683727412214.jpg\" /></p><p></p><p>图15——路线图中“现在”阶段的详细信息</p><p></p><p>有了路线图草案之后，将其社交化以在审查会议和工作会议期间从利益相关者那里获得反馈。 我们通常在探索阶段结束前两到三周开始制定路线图，这样在向执行涉众展示路线图之前，我们有足够的时间来收集和迭代工作团队的反馈。</p><p></p><h2>阐明架构的演进步骤</h2><p></p><p></p><p>DDD的最后一个阶段是创建架构从当前状态到未来状态演进的清晰视图。我们应该在制定路线图时并行地进行这项工作，因为技术负责人将创建代表架构在“现在”和“下一个”里程碑上的临时状态视觉效果图。团队应该讨论每个可视化效果图，以澄清每个演化步骤中所需的更改，并在工作时修改路线图。</p><p>当人们转向领域驱动设计时，我们经常看到<a href=\"https://learn.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer\">反腐模式</a>\"。这是一个软件层，它将新的基于上下文的有界架构与现有架构隔离开，在我们发展到未来状态时充当有用的桥梁。虽然这一软件生命周期是短暂的，但在我们完成过渡之前，它都可以存在。设计、编码、测试和部署该软件的额外工作为我们提供了从当前状态过渡到未来状态的时间，并减少了一次与重写所有系统相关的风险。</p><p></p><h2>从探索到交付</h2><p></p><p></p><p>在短期内，我们的团队可以使用DDD来：</p><p>明确我们需要解决的问题，受影响的人，期望的结果和解决方案的约束。分析我们当前的业务流程和系统架构。确定有界上下文和消息流。从战略上对上下文进行分类，使投资和决策保持一致。设计一个与有界上下文保持一致的新的未来状态架构。创建一个路线图，使当前到未来的状态与业务价值保持一致。可视化架构的演进步骤。</p><p></p><p>探索后，我们应该将结果提炼成一个可操作的计划，其中包括（但不限于）：</p><p>基于路线图构建史诗产品的待办事项。为“现在“阶段制定发布计划。设置具有访问权限的开发环境。对齐团队规范和会议节奏。进行额外的研究和设计工作，以完善架构。</p><p></p><p>使用共享的数字白板来完成这项工作可以提供清晰度和透明度，并为以后加入该项目的任何人提供了更简单的入职培训。我目前正在为DDD中的所有步骤制作一个Miro模板，并将在发布后更新本文的链接。</p><p></p><h2>开始使用DDD</h2><p></p><p></p><p>项目的成功始于强有力的探索能力。在开始另一个架构现代化项目之前，请考虑使用我们的四步DDD。这种协作方法专为跨职能团队而设计，能以透明的方式将战略与架构结合起来，同时为领导者和技术人员构建了一种共享语言。在短期内，你即可获得清晰的洞察力来指导你的项目，这将有助于节省时间、精力和金钱。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/architecture-modernization-domain-driven-discovery/\">https://www.infoq.com/articles/architecture-modernization-domain-driven-discovery/</a>\"</p>",
    "publish_time": "2023-07-13 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "不到一年，Istio项目正式从 CNCF 毕业",
    "url": "https://www.infoq.cn/article/akP8x4df0wUtPZeGKaPh",
    "summary": "<p>7 月12日，Istio项目正式从 CNCF 毕业。Tetrate 创始人Varun Talwar在<a href=\"https://tetrate.io/blog/istio-service-mesh-graduates-cncf/\">博客</a>\"中写道，Istio 现在是最快毕业的 CNCF 项目。Tetrate 由 Istio 创始团队成员发起，旨在促进和扩大服务网格的采用，自成立以来一直是 Istio 的主要贡献者。</p><p>&nbsp;</p><p>Varun Talwar表示，“这一具有里程碑意义的时刻代表了 Istio 作为云原生领域重要组成部分的成长和成熟，为部署最广泛的服务网格开启了激动人心的新篇章。Kubernetes 是第一个在 2018 年获得毕业资格的项目。如今，距离 Istio 作为孵化项目进入 CNCF 还不到一年，Istio 就毕业了，这是 CNCF 历史上最快的项目。”</p><p></p><h2>Istio 的发展历程</h2><p></p><p>&nbsp;</p><p>Istio 项目始于 2016 年，最初由 Google、IBM 以及构建 Envoy 代理的 Lyft 团队共同开发。2018 年 6 月，Istio 发布了 1.0 版本，2019 年，Istio 成为整个 GitHub 中增长速度第四快的开源项目。目前，已经有 190 多家公司承诺使用 Istio，其中 20 多家供应商为其 Kubernetes 平台提供托管的 Istio 产品或插件。</p><p>&nbsp;</p><p>2022年4月，谷歌提议将 Istio <a href=\"https://www.infoq.cn/article/tfJtw1pWmbEIFNSxDK8q\">捐赠给 CNCF</a>\"，该想法得到了社区众多开发者的关注和支持。经过近半年的尽职调查，9 月 28 日，CNCF正式宣布技术监督委员会(TOC) 已投票用过将Istio作为 CNCF 孵化项目。</p><p>&nbsp;</p><p>根据此前介绍，当前 Istio 社区重要的贡献来自：</p><p>&nbsp;</p><p>技术公司和云计算厂商，包括红帽、思科、VMware、英特尔、华为、腾讯、阿里巴巴和 DaoCloud为将 Istio 解决方案推向市场而成立的公司，包括 Tetrate、Aspen Mesh 和 Solo.ioIstio 的终端用户，包括 Auto Trader UK、Salesforce、SAP 和 Yahoo!</p><p></p><h2>Istio 毕业对用户意味着什么？</h2><p></p><p>&nbsp;</p><p>对于那些已经将 Istio 用作支持数千次部署的基础设施核心部分的人来说，CNCF 毕业是对他们将 Istio 视为现代应用程序网络的关键组件的愿景的验证。对于那些希望实现基础设施现代化的人来说，Istio 的毕业地位是一个强烈的信号，表明它是在生产中扩展关键应用程序的经过验证的强大选择。&nbsp;</p><p>&nbsp;</p><p>​​对于用户来说，Istio 的毕业状态具有以下几个含义和优势：</p><p>&nbsp;</p><p>稳定、成熟。潜在用户可以对该项目的稳定性充满信心，因为知道它已经满足 CNCF 严格的毕业标准。安全。Istio在及时发布安全公告以及业界最重要的安全思想领导者的战略指导方面拥有悠久而稳健的记录。生产准备就绪。毕业状态向用户保证 Istio 具有在生产环境中使用所需的功能、可扩展性和稳健性。采用和生态系统。 毕业项目在云原生生态系统中获得了广泛采用。它们被各种规模和行业的组织广泛认可和使用。Istio 的用户受益于其他采用者的经验。分级项目的广泛采用还培育了一个充满活力的工具、扩展和集成生态系统，可以进一步增强其功能。CNCF 支持和治理。毕业项目也受益于 CNCF 的支持和治理。CNCF 为协作和社区参与提供资源、指导和框架。用户可以对该项目的长期可持续性和发展路线图充满信心，因为它是由致力于推进云原生技术的可信组织支持的。社区和企业支持。社区提供的集体知识、经验和支持提供了广泛的用户群、对文档、论坛和用户组等资源的访问以及在故障排除和解决问题方面的潜在帮助的好处。Istio 生态系统还享有许多供应商（包括 Tetrate）提供的企业支持，为那些需要保证在需要时获得专家支持的组织提供支持。</p><p>&nbsp;</p>",
    "publish_time": "2023-07-13 11:12:18",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "理论→实践：金融科技与 AI 如何有效融合？ | ArchSummit 深圳×汇丰科技",
    "url": "https://www.infoq.cn/article/P0i2zyh1Ym0FVx8IsTIL",
    "summary": "<p>在当今数字化迅猛发展的时代，AI 技术正在深刻地推动金融科技的前进。AI 以其卓越的数据处理能力和决策制定功能，正在为金融科技带来一场革命。目前金融行业很多组织都在探索 AI 的应用，比如：</p><p></p><p>在风控 - 信用决策架构中，AI 是如何赋能 credit decisioning 的？</p><p></p><p>AI 是如何被应用在隐私安全有极高要求的银行业数据平台的？</p><p></p><p>火爆的<a href=\"https://xie.infoq.cn/article/51f18e0fc95aedcd6505d0e94\">生成式人工智能</a>\"（GenAI）是如何赋能金融行业的？</p><p></p><p>量子密钥分发（QKD）技术是如何与 AI 结合，以帮助银行业实现更安全、高效和智能的通信和信息处理的呢？</p><p></p><p>……</p><p></p><p>这些问题，汇丰科技都有了自己的探索，7 月 21 日下午，汇丰科技中国区代理总经理、工商金融资讯科技总监马国栋作为 ArchSummit 全球架构师峰会（深圳站）的《汇丰科技在 AI 时代下的创新与探索》解决方案专场的出品人，召集了汇丰科技的 5 位技术专家来到现场，分别围绕风控、数据平台、生成式人工智能（GenAI）、量子密钥分发等方面的 AI 实践经验展开分享。</p><p></p><p></p><h2>推荐议题 1：《GenAI 简介及其在银行的应用》</h2><p></p><p></p><p>演讲嘉宾：汇丰科技 AI 算法研究专家 - 李紫源</p><p></p><p>演讲内容：</p><p></p><p>生成式人工智能（GenAI）是一种能够生成新数据、图像和文本的机器学习技术。这种技术已经在许多领域实现了突破性进展，包括自然语言生成、图像和音频合成等。在银行业，GenAI 的应用也在不断地扩展，包括智能客服、私有知识库问答、Copilot 及 API Discovery 等。然而，GenAI 在银行业的广泛应用也带来了一些挑战和风险。这些风险包括数据隐私和安全等问题。因此，在采用 GenAI 时，银行需要结合审核和质量控制措施，以确保生成的内容符合预期的质量、监管和安全标准。</p><p></p><p>为了有效地应用 GenAI，可以按需采用如下路径，包括 Training from scratch、Finetune、Embedding、0-shot or Few-shot 等。通过这些路径，银行可以为其业务和客户提供更好的服务和体验。本次分享便将深度解读 GenAI 及其在银行业的应用，以下为他的演讲提纲：</p><p></p><p>1. GenAI 的介绍</p><p></p><p>LLM 的发展历程LLM 的分类及原理LLM 的特性，待解决问题GenAI 对行业带来的机会及冲击整体的情况软件工程以及银行</p><p></p><p>2. GenAI 的采纳路径</p><p></p><p>Training from scratchFinetuneEmbedding0-shot or Few-shot</p><p></p><p>3. GenAI 在银行的应用</p><p></p><p>智能客服企业私有知识库问答 (risk &amp; compliance, legal contract)CopilotAPI Discovery</p><p></p><p>推荐理由：此演讲将介绍 GenAI 的基本概念、发展历程，以及大语言模型、机会冲击、采纳路径和在银行领域的应用。通过这场演讲，您将深入了解 GenAI 如何为银行业带来智能客服、企业私有知识库问答、Copilot 和 API Discovery 等诸多应用场景。话不多说，来和演讲嘉宾共同探讨生成式人工智能为银行业带来的无限可能，立即点击<a href=\"https://www.infoq.cn/form/?id=1704&amp;utm_source=gzh&amp;sign=iq_64af71308eb93\">此链接</a>\"报名演讲！</p><p></p><p></p><h2>推荐议题 2：《汇丰数据科学家平台的探索与实践》</h2><p></p><p></p><p>演讲嘉宾：汇丰科技数据科学家平台交付经理 - 巴晨骁</p><p></p><p>演讲内容：</p><p></p><p>作为全球规模最大的银行和金融服务机构之一，汇丰始终致力于实施国际化策略，为客户提供国际化的机遇和视野。这一策略推动了汇丰银行业务的持续发展，但同时也带来了合规方面的巨大挑战。在当前大数据、生成式 AI 以及<a href=\"https://xie.infoq.cn/article/78d194d595b75f4f447e10187\">混合云技术</a>\"快速发展的背景下，作为数据平台部门的工程师，我们应如何在确保合规要求得到满足的前提下，让数据和人工智能更便捷、安全地被使用，以进一步支持汇丰银行的国际化策略呢？本议题将为大家进行分享，演讲提纲如下：</p><p></p><p>1. 背景：</p><p></p><p>混合云与数据监管生成式 AI 与模型合规</p><p></p><p>2. 属于汇丰的数据科学家工作平台</p><p></p><p>属于汇丰的数据科学家工作平台统一的数据访问监管流程基于 Kubernetes 的混合云全球部署架构基于数据虚拟化的多平台跨区域的数据访问</p><p></p><p>3. 数据科学家平台与生成式 AI 平台的集成</p><p></p><p>数据科学家平台与生成式 AI 平台集成的设计与实践生成式 AI 的监管模型</p><p></p><p>推荐理由：在数字化时代，数据科学家平台是汇丰银行的重要战略之一。该演讲将分享汇丰数据科学家平台在混合云环境下的设计与实践，探讨如何应用数据虚拟化解决跨区域多平台的实时数据访问，并深入了解金融机构对于生成式 AI 的思考及使用方式。这场演讲为大家带来的汇丰银行在数据科学家平台的实践经验和思考，将对大家在“金融机构在数字化时代的应对策略”方面的思考多有助益，所以赶快点击<a href=\"https://www.infoq.cn/form/?id=1704&amp;utm_source=gzh&amp;sign=iq_64af71308eb93\">此链接</a>\"报名演讲吧！</p><p></p><p></p><h2>推荐议题 3：《基于供应链金融贸易数据的信用风险实践解读》</h2><p></p><p></p><p>演讲嘉宾：汇丰科技中国全球贸易和应收账款金融技术总监 - 杜小飞、汇丰科技香港全球贸易和应收账款金融客户平台副总监 - 霍利锋</p><p></p><p>演讲内容：综合营运资金融资方案为核心买家的供应商提供从装运前到装运后的全流程融资方案，令其可凭借经由核心买家确认的订单，向汇丰申请融资。与传统的商业贷款不同，该装运前融资依托供应链出运后融资，将融资节点提前至出运前即可起始。装运前融资计划与装运后供应链解决方案在系统和操作流程层面无缝联接，到期自动还款，极大地简化了整个流程。汇丰通过买卖双方的历史贸易数据将信贷决策数字化，并将对财务的依赖减少到几个关键检查点，该信用决策模型使供应商参与到信贷审批的准备时间从 1 个月以上大幅缩短至 2 周。本次分享便将为大家详细剖析整个技术实践过程。以下为演讲提纲：</p><p></p><p>应用业务场景 - 综合营运资本融资方案</p><p></p><p>基于历史供应链交易数据的信用决策模型 - 记分牌（含 AI 相关技术分享）</p><p></p><p>信用决策架构</p><p></p><p>推荐理由：随着 AI 技术的不断发展，汇丰银行在综合营运资金融资场景的信用决策模型方面取得了重要进展。演讲将深入解读这一模型，让听众了解汇丰银行针对中小微供应商的贷款业务实施、数字化授信决策流程以及基于贸易数据的信用决策模型。通过这些内容，听众将了解到如何利用 AI 技术提高融资效率、降低风险，以及如何在竞争激烈的市场中更好地服务客户。这场演讲将为听众带来有关 AI 技术在风控领域应用的深刻思考和实用建议，值得一听，立即点击<a href=\"https://www.infoq.cn/form/?id=1704&amp;utm_source=gzh&amp;sign=iq_64af71308eb93\">此链接</a>\"报名吧！</p><p></p><h2>推荐议题 4：《量子密钥分发：量子计算时代的安全密钥分发实践解读》</h2><p></p><p></p><p>演讲嘉宾：汇丰科技量子计算研究资深专家 - 朱兵</p><p></p><p>演讲内容：</p><p>本次演讲着重介绍在银行环境中进行的量子密钥分发（Quantum Key Distribution，简称 QKD）实验试点，展示其在显著提升密码安全性方面的潜力。将深入探讨 QKD 的基本原理，并解释其如何解决传统加密方法在量子计算时代面临的威胁。通过此次试点，汇丰银行旨在探索将 QKD 整合到现有基础设施的可行性，最终保护敏感客户数据，并强化应对网络威胁的防御能力。此外，演讲嘉宾也将讨论量子密码分发对安全银行业的未来影响。以下为演讲提纲：</p><p></p><p>1. 引言</p><p></p><p>简要概述密码安全在银行业中的重要性介绍量子密钥分发（QKD）在银行业进行实验试点的动机</p><p></p><p>2. 理解量子密钥分发</p><p></p><p>与 QKD 相关的量子力学基本原理利用量子信道进行密钥分发量子态和测量QKD 相对于传统加密方法的安全优势</p><p></p><p>3. 实验试点设置</p><p></p><p>银行实验设置概述将 QKD 与现有银行基础设施集成实施过程中的挑战和注意事项试点中采用的性能评估指标</p><p></p><p>4. 对安全银行业的影响</p><p></p><p>针对网络威胁的增强保护对客户数据安全和隐私的影响与其他安全技术的潜在整合</p><p></p><p>5. 未来方向和结论</p><p></p><p>QKD 在银行业的潜在可扩展性和商业可行性总结和建议</p><p></p><p>推荐理由：在 经典数字技术（如 AI）日新月异的当下，量子密钥分发（QKD）作为量子计算时代的安全密钥分发实践，正逐渐成为关注的焦点。演讲将深入探讨 QKD 技术的原理和应用，为大家提供对量子安全通信的深入理解。通过了解 QKD 技术在银行领域的试点和在提升安全性方面的潜力，大家将更好地了解量子技术在金融行业的应用前景。最后，演讲将展望 QKD 技术在银行业的未来发展方向，以及在提高安全性方面的商业可行性和潜在整合。对于对量子计算和安全性感兴趣的听众来说，这场演讲将带来宝贵的洞察和建议，立即点击<a href=\"https://www.infoq.cn/form/?id=1704&amp;utm_source=gzh&amp;sign=iq_64af71308eb93\">此链接</a>\"报名演讲收听吧！</p><p></p><p>我们相信，这些主题已经引发了你的兴趣与关注。在这个充满机遇和挑战的时代，InfoQ 和汇丰科技与您同在，共同探索 AI 技术在金融科技领域的应用和发展。立即抓住这次难得的机会，报名参加 ArchSummit 全球架构师峰会（深圳站）的《汇丰科技在 AI 时代下的创新与探索》专场，报名链接见<a href=\"https://www.infoq.cn/form/?id=1704&amp;utm_source=gzh&amp;sign=iq_64af71308eb93\">此链接</a>\"，席位有限，先到先得！</p><p></p><p>7 月 21 日下午，深圳·博林天瑞喜来登酒店，我们在《汇丰科技在 AI 时代下的创新与探索》专场不见不散！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1d47040e593ea1c5e42cc1d90d29163.jpeg\" /></p><p></p>",
    "publish_time": "2023-07-13 00:13:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "甲骨文火上浇油、SUSE投入1000万美元，多方“围剿”红帽：“红帽负担不起？那我们来！”",
    "url": "https://www.infoq.cn/article/YSrph60My5GlkWuti1X7",
    "summary": "<p></p><blockquote>甲骨文对红帽开启了嘲讽模式，而SUSE则直接投入1000万美元创建RHEL分支。</blockquote><p></p><p>&nbsp;</p><p>6 月 21 日，<a href=\"https://mp.weixin.qq.com/s/Irj8dTBWhI9ruRTCPqQKpw\">红帽软件宣布</a>\"将 CentOS Stream 的源码发布到 git.centos.org，而 Red Hat Enterprise Linux（简称 RHEL）的源代码则面向红帽软件订阅用户以及合作伙伴开放，订阅用户可以通过 Red Hat Customer Portal 访问。</p><p>&nbsp;</p><p>虽然没有明确表明，但这意味着红帽将限制第三方对 RHEL 源代码的访问，并将会阻止Alma Linux、Rocky Linux、Euro&nbsp;Linux和Oracle Linux等下游项目的后续代码改进。</p><p>&nbsp;</p><p>上月底<a href=\"https://www.theregister.com/2023/06/28/rocky_linux_rhel_ripples/\">曾有文章</a>\"提到：</p><p>&nbsp;</p><p></p><blockquote>GPL仅要求红帽为需要向其提供二进制文件的各方提供源代码，对其他各方均无源码提供责任。现在这部分红帽客户仍可获取源代码，因此红帽方面并未违反GPL。换言之，GPL并不足以把全体客户从红帽的合同条款中解放出来：如果愿意，付费客户可以二次分发源代码；但同样，红帽也可以通过终止合同来回应此类行为，且100%符合GPL要求。</blockquote><p></p><p>&nbsp;</p><p>根据各方媒体对当前情况的分析，红帽唯一的义务就是向收取RHEL二进制文件的付费客户提供源代码。也就是说红帽会对个人用户提供免费的 RHEL，只是开发者只可在最多 16 个系统上使用。但是对于IT企业，以后这些企业要么花钱买服务，要么就自己花钱养团队。</p><p>&nbsp;</p><p>红帽更改源码发布策略的决定，发生在CentOS（与RHEL绑定的非商业Linux版本）于2020年过渡至与RHEL有一定距离的CentOS Stream项目之后。这一举措在开源社区以及社群中均引发了负面反响。</p><p></p><p>更多阅读：<a href=\"https://www.infoq.cn/video/leVHm81oCJkYMaIKFSQG\">红帽：我们为什么要改变 RHEL 源码的发布策略？｜InfoQ《极客有约》</a>\"</p><p>&nbsp;</p><p>6 月 26 日，红帽核心平台副总裁 Mike McGrath 再次出面做了回应，解释称因为下游企业没有对现有代码增加价值或进行任何修改，而红帽则不同，“我们不是简单地拿来上游软件包并进行重建。在红帽，成千上万的开发者花费时间编写代码，实现新功能、修复漏洞、集成不同的软件包，然后长期提供支持服务。”</p><p>&nbsp;</p><p>意外的是，这事儿居然还没完，现在，甲骨文和SUSE都开始介入红帽开源 Linux 代码的混乱之中了。</p><p>&nbsp;</p><p></p><h2>甲骨文火上浇油，红帽源码风波再起</h2><p></p><p>&nbsp;</p><p>本周一，甲骨文公司首席企业架构师Edward Screven和Oracle Linux开发主管Wim Coekaerts纷纷发声，指责IBM试图以牺牲开源社区利益的方式获取利润。</p><p>&nbsp;</p><p>Screven和Coekaerts援引红帽核心平台副总裁Mike McGrath的声明，表示免费的RHEL发行版已经无法持续，因为红帽需要向人们支付RHEL的开发费用，花出去的都是真金白银。“有意思，IBM不继续公开RHEL源代码的理由，是他们需要向工程师支付费用？”</p><p>&nbsp;</p><p>“这可太奇怪了。毕竟IBM在2019年才以340亿美元收购了红帽，而之前红帽一直是家成功的独立开源公司，多年来一直公开发布RHEL源代码，从没听说过工程师开不起工资的问题。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/76/76915bc49e03d53314c302714e9cb74e.png\" /></p><p></p><p>&nbsp;</p><p>这对甲骨文二人组声称IBM“实际上扼杀了它作为RHEL免费替代品的地位”。“红帽是不是负担不起？如果负担不起，我们很乐意挑起这副重担！”</p><p>&nbsp;</p><p>他们认为此番操作，相当于蓝色巨人旗下的红帽朝着CentOS的继任者Alma Linux和Rocky Linux发动了攻击。他们推测，“也许这就是此番风波的真正答案：消除竞争对手。竞争对手越少，IBM赚取营收的机会就更大。”</p><p>&nbsp;</p><p>甲骨文早在2006年就推出了现在的Oracle Linux，计划是提供与RHEL相兼容的Linux发行版，在避免Linux社区分裂的同时为客户和独立系统供应商（ISV）维护一套通用平台。</p><p>&nbsp;</p><p>由于目前无法确定能否与RHEL继续保持兼容性，Screven和Coekaerts称Oracle Linux将在9.2版本之前维持兼容。在此之后，甲骨文将与客户及独立系统供应商合作以解决可能出现的各种问题。</p><p>&nbsp;</p><p></p><h2>SUSE投入1000万美元创建RHEL分支</h2><p></p><p>&nbsp;</p><p>随后一天，欧洲 Linux 巨头SUSE 也宣布它将 fork 公开可用的 Red Hat Enterprise Linux (RHEL)，并将开发和维护与 RHEL 兼容的发行版，所有人都可以不受限制地使用。而且 SUSE 计划向该项目投资超过 1000 万美元。</p><p>&nbsp;</p><p>SUSE 首席执行官 Dirk-Peter van Leeuwen 表示：“几十年来，协作和共同成功一直是我们开源社区的基石。我们有责任捍卫这些价值观。这项投资将在未来几年保持创新的流动，确保客户和社区等不会受到供应商锁定，并在明天和今天都有真正的选择。”</p><p>&nbsp;</p><p>根据LinkedIn资料，Dirk-Peter 3个月前开始在SUSE担任首席执行官，之前在红帽工作了18年，并担任红帽高级副总裁，可能对 Red Hat Linux 了如指掌。而SUSE企业本身已经存在了30年，在维护自己的企业内核方面拥有丰富的经验。因此，有人认为SUSE的这一举动会对IBM/Red Hat的RHEL构成可信的威胁，终将会对生态造成比较大的影响。</p><p>&nbsp;</p><p>SUSE 计划将该项目贡献给开源基金会，该基金会将提供对替代源代码的持续免费访问。</p><p>&nbsp;</p><p>SUSE 不是孤军奋战。Rocky Linux创始人 Gregory Kurtzer 表示：“CIQ 通过建立志同道合的公司、组织和个人的广泛联盟，为我们的合作伙伴、客户和社区带来稳定。SUSE 体现了我们的核心开源原则和精神；CIQ 很高兴与 SUSE 合作推进开放企业 Linux 标准。”</p><p>&nbsp;</p><p>&nbsp;Kurtzer 补充道：“红帽打开了潘多拉魔盒。企业 Linux 社区需要的是标准化、稳定性和一致性。多年来，这就是 RHEL。现在，替代品的大门已经敞开。”</p><p>&nbsp;</p><p></p><h2>技术的变化</h2><p></p><p>&nbsp;</p><p>在这场开源混战中，甲骨文指出IBM只是想求财而已，其订阅服务有违GPL许可，“虽然甲骨文和IBM的Linux发行版相互兼容，但我们对于开源管理者的责任以及在GPLv2下运营的理解可谓截然不同。”这对甲骨文搭档还借此鼓励更多用户为Oracle Linux贡献代码，甚至表示甲骨文会提供潜在的工作机会。</p><p>&nbsp;</p><p>“甲骨文始终向所有人免费提供Oracle Linux的二进制文件与源代码。我们没有任何订阅协议会阻止订户重新分发Oracle Linux。另一方面，IBM的订阅协议则规定，利用订阅服务行使GPLv2权利的行为将属于违约。”</p><p>&nbsp;</p><p>开源运动的创始人之一Bruce Perens认为IBM和红帽在故意利用GPL的漏洞：“GPL要求将变更以源代码的形式共享。如果协议是现在才编写的，那指的‘共享’肯定是通过网络公开。但问题是协议内容诞生在磁带数据时代，所以受约束方只需要向收取程序二进制版本的人提供源代码。”</p><p>&nbsp;</p><p>软件自由保护协会（SFC）的政策研究员Bradley Kuhn撰写文章，具体介绍了关于RHEL的过渡故事，并表示这场看似对企业客户的争夺很可能会损害整个开源社区。Kuhn同时对甲骨文做出的GPL条款解释提出了质疑，称GPLv2也并不是要求向所有人都免费提供二进制文件和源代码。</p><p>&nbsp;</p><p>“甲骨文暗示称，GPL的意思是要求把所有源代码都开放给公众”，但实际上IBM和红帽做出的，只须向二进制版本接收者或者申请获取发行版源代码者提供完整相应源码（CCS）的解读才是正确的。</p><p>&nbsp;</p><p>Kuhn解释道，“虽然不向全体公众开放相应源代码似乎有违开源精神，但其本身确实不会触犯GPL的条款内容。”</p><p>&nbsp;</p><p>“遗憾的是，IBM和甲骨文两家主营专有软件的公司之间大打口水战，只会分散人们对于RHEL商业模式这个核心问题的关注度。”</p><p>&nbsp;</p><p>&nbsp;“比方说，RHEL合同要求其客户同意 BSA式审计。就是说在开展客户审计时，只要IBM/红帽发现客户使用GPL软件制作了哪怕是合法的额外副本，就有权永久撤销对方的RHEL服务。”</p><p>&nbsp;</p><p>“所以我们把这种商业模式总结为：‘想要行使GPL规定的权利？不好意思，你的订阅费白交了。’&nbsp;关于RHEL的这种商业模式到底符不符合GPL条款，一直是个引发激烈争论的问题。人们意见不一，但除了红帽之外，没人觉得这种商业模式真正符合GPL和自由开源软件精神。”</p><p>&nbsp;</p><p>Perens表示，Kuhn认为IBM当然有权追求这种商业模式，但他本人对此难以认同。“Bradley也许应该把这事诉诸司法。”</p><p>&nbsp;</p><p>Kuhn还提到，自己目前还不确定该怎样表达立场。毕竟还不清楚IBM的商业模式是否与GPL相兼容，“但即使兼容，恐怕也已经相当逼近边界了。”在他看来，“我们不该放任事情继续发展。”</p><p>&nbsp;</p><p>因此这件事最终可能对簿公堂，只是不清楚软件自由保护协会或者开源社区那边的人们愿不愿意选择这样的处理方式。IBM明显是身经百战，不惧掺和任何漫长且昂贵的诉讼。Kuhn觉得应该还有更好的问题解决办法。</p><p>&nbsp;</p><p>“IBM/红帽应该立即主动停止这种做法。”</p><p>&nbsp;</p><p>“长期以来，红帽一直以超越甲骨文的道德制高点而自豪。甲骨文的整个商业模式，向来是以激进的专有许可在客户当中营造恐惧感为核心。但让我难过的是，现在RHEL的商业模式似乎也越来越倾向于甲骨文的路子。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theregister.com/2023/07/10/oracle_ibm_rhel_code/\">https://www.theregister.com/2023/07/10/oracle_ibm_rhel_code/</a>\"</p><p><a href=\"https://www.zdnet.com/article/suse-to-fork-red-hat-enterprise-linux/\">https://www.zdnet.com/article/suse-to-fork-red-hat-enterprise-linux/</a>\"</p>",
    "publish_time": "2023-07-13 14:06:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "马斯克宣布成立xAI并亲自领导！华裔明星团队成员吸睛，推特、特斯拉为其撑腰",
    "url": "https://www.infoq.cn/article/BjJt1kkDd6e0EczzbRpJ",
    "summary": "<p>当地时间7 月 13 日，马斯克在推特上正式宣布成立xAI公司“去了解现实”。根据官网介绍，xAI 的目标是了解宇宙的真实本质。这一消息迅速吸引了大家的关注。</p><p></p><h2>“全明星团队”首亮相</h2><p></p><p>&nbsp;</p><p>在官网有限的信息中，xAI 主要列出了自己的创始团队名单，并大概介绍了这些人之前的贡献：</p><p>&nbsp;</p><p></p><blockquote>我们的团队由<a href=\"https://pitchhub.36kr.com/project/2179869795830913\">特斯拉</a>\"和SpaceX的CEO埃隆·马斯克领导。我们曾在DeepMind、OpenAI、谷歌研究、微软研究、特斯拉以及多伦多大学工作过。我们共同贡献了该领域中最广泛使用的一些技术方法，特别是<a href=\"https://arxiv.org/abs/1412.6980\">Adam 优化器</a>\"、<a href=\"https://arxiv.org/abs/1502.03167\">批量归一化</a>\"、<a href=\"https://arxiv.org/abs/1607.06450\">层归一化和</a>\"<a href=\"https://arxiv.org/abs/1312.6199\">对抗性</a>\"样本的发现。我们还引入了创新技术和分析，如<a href=\"https://arxiv.org/abs/1901.02860\">Transformer-XL</a>\"、<a href=\"https://arxiv.org/abs/2205.12615\">Autoformalization</a>\"、<a href=\"https://arxiv.org/abs/2203.08913\">Memorizing Transformer</a>\",、<a href=\"https://arxiv.org/abs/1907.04164\">Batch Size Scaling</a>\"和<a href=\"http://arxiv.org/abs/2203.03466\">μTransfer</a>\"。我们曾参与并领导了该领域的一些重大突破发展，包括<a href=\"https://www.nature.com/articles/s41586-019-1724-z\">AlphaStar</a>\"、<a href=\"https://www.science.org/doi/10.1126/science.abq1158\">AlphaCode</a>\"、<a href=\"https://arxiv.org/pdf/1409.4842.pdf\">Inception</a>\"、<a href=\"https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\">Minerva</a>\"、<a href=\"https://platform.openai.com/docs/models/gpt-3-5\">GPT-3.5</a>\"和<a href=\"https://openai.com/research/gpt-4\">GPT-4</a>\"。</blockquote><p></p><p>&nbsp;</p><p>马斯克从谷歌DeepMind挖来了三名核心成员：Igor Babuschkin、Manuel Kroiss和Toby Pohlen。</p><p>&nbsp;</p><p>Igor 是马斯克看好的大将之一，曾为DeepMind AlphaStar研发团队核心成员，AlphaStar学会了玩《星际争霸II》，在所有三场比赛中都被评为大师级。Igor 还曾跳槽至OpenAI工作一年半，曾为ChatGPT等聊天机器人提供驱动力的机器学习模型。随后 Igor 再次回到了DeepMind。据报道，从今年2月起，马斯克就和Igor频繁接触，今年三月，Igor 加入推特担任高级工程总监。</p><p>&nbsp;</p><p>Manuel Kroiss是在DeepMind就职了六年的软件工程师，在此之前是谷歌的方案工程师；Ross Nordeen曾担任特斯拉超级计算和机器学习团队的技术项目经理；Kyle Kosic曾担任OpenAI技术研发团队成员；Christian Szegedy 曾在谷歌工作近5年，担任研究科学家；<a href=\"https://jimmylba.github.io/\">Jimmy Ba</a>\" 则之前在多伦多大学担任助理教授，也是加拿大 CIFAR AI主席。他在个人主页上表示自己的长期研究目标是构建具有类似人类效率和适应性的、可以解决问题的通用机器。</p><p>&nbsp;</p><p>不过，在这份名单中，备受国内媒体关注的是占了三分之一的华人成员。</p><p>&nbsp;</p><p><a href=\"https://www.163.com/dy/article/H513M91I051193U6.html\">Greg Yang</a>\" 出生于湖南，初中开始前往美国学习，本科与硕士皆毕业于哈佛大学数学系。2018年Greg Yang获得本科生数学领域最高荣誉Morgan&nbsp;Prize，随后加入微软担任高级研究员。</p><p>&nbsp;</p><p>在本科就读期间，Greg Yang曾休学一年半，成为全职电子舞曲制作人和DJ，并在该过程中深度接触到了AI。这期间他明确了自己致力于实现通用人工智能的目标：能够制造一个比自己还要聪明的东西，并淘汰掉他自己，是一个“intellectually”极其激动的事情。而数学是现实的底层逻辑。</p><p>&nbsp;</p><p>作为 xAI 的联合创始人，他在推特上表示，开发大型神经网络的“万物理论”将是将人工智能提升到下一个水平的核心。相反，这种人工智能将使每个人以以前无法想象的方式理解我们的数学宇宙。“Math for Al and Al for math！”</p><p>&nbsp;</p><p>另外两位华人成员张国栋、戴子航均是在国内本科毕业后，前往美国就读。<a href=\"http://www.cs.toronto.edu/~gdzhang/\">张国栋</a>\"本科毕业于浙江大学，2015年获得中国大学生数学建模竞赛（CUMCM）一等奖，之后进入多伦多大学攻读机器学习博士学位，2022年毕业后加入DeepMind。而戴子航本科毕业于清华大学，2020年博士毕业于卡内基梅隆大学计算机系，在2020年加入谷歌。</p><p>&nbsp;</p><p>最后一位华人成员<a href=\"https://yuhuaiwu.github.io/\">Yuhuai (Tony) Wu</a>\" 2021年博士毕业于多伦多大学，曾就职于谷歌的研发小组N2Formal，该组织主要负责研发能自己推理的“AI数学家”。</p><p>&nbsp;</p><p>有趣的是，Jimmy Ba是Tony Wu在多伦多大学的恩师之一，而Tony Wu在谷歌N2Formal的领头人就是Christian Szegedy，三人一起加入了xAI。</p><p>&nbsp;</p><p>目前，xAI 正在继续招聘员工。在提交表单中，有一项是填入当前雇主，或许也暗示着马斯克还将继续从AI 大厂里挖人。</p><p>&nbsp;</p><p>Nvidia 人工智能研究员 Linxi “Jim” Fan 表示，该团队是一支“全明星创始团队”。“人才密度给我留下了深刻的印象——他们写的论文太多了，数不胜数，”他<a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7084949359460560897/\">在 LinkedIn 的一篇帖子中写道</a>\"。</p><p>&nbsp;</p><p></p><h2>马斯克要讲什么故事？</h2><p></p><p>&nbsp;</p><p>骨干团队已经就绪，那马斯克要讲什么样的故事呢？</p><p>&nbsp;</p><p>“最基本的未知问题是什么？一旦你知道该问的问题，答案往往是容易的部分。正如我的英雄Douglas Adams所说。” Adams是英国广播剧作家、音乐家，尤其以《银河系漫游指南》系列作品出名。</p><p>&nbsp;</p><p>马斯克将公司成立日期“2023 年 7 月 12 日”标红，并在旁边加上一行文字：7+12+23=42。“42 是生命、宇宙和万物的终极问题的答案”，马斯克表示。“42”这个数字是在《银河系漫游指南》系列作品中，主角Arthur Dent和他的朋友们追求宇宙的真正含义时，一组超级智能生物构建的超级电脑告诉他们的答案。</p><p>&nbsp;</p><p>然而，“42”这个答案本身并没有解释宇宙的真正含义，而是一个非常模糊和不可理解的答案，只是突出了小说表达中荒诞的特点。而马斯克也并没有给出更多的解释，一切还是推向了带有“科幻”色彩的一面。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/92f4e00b3fb76e452316a08faeb5ac69.png\" /></p><p></p><p>“AI 可以有意识吗？”有推特网友问道。马斯克也没有正面回答，“我经常想，当我们从一个细胞发展到35万亿个细胞时，意识是从哪里开始的。如果标准模型是正确的，那么假设不存在有知觉的外星人，那么夸克和轻子是从开始后大约138亿年变得‘有意识’的。顺便说一下，外星人在哪里！？”</p><p>&nbsp;</p><p>马斯克随后开玩笑说，“我一直告诉人们我是外星人，但没有人相信我。”</p><p>&nbsp;</p><p>外媒推测，也许在早期阶段，xAI 的言论主要是为了吸引人才。对于新进入者，最紧迫的问题是证明它可以吸引具有有竞争力的研究人员，即使是马斯克这样有很大声誉和雄厚财力支持的企业。</p><p></p><h2>背后是Twitter 和特斯拉的支持？</h2><p></p><p>&nbsp;</p><p>与许多其他新的人工智能项目一样，马斯克的动机是出于对 ChatGPT 迅速崛起的担忧，或许还有一些 FOMO 情绪。马斯克在 2015 年参与创立了 OpenAI ，但三年后，由于<a href=\"https://www.semafor.com/article/03/24/2023/the-secret-history-of-elon-musk-sam-altman-and-openai\">据称</a>\"未能完全控制OpenAI，他与当时的非营利组织断绝了关系，这一事实或许加剧了马斯克的不满。该公司于 2019 年成为一家营利性企业。</p><p>&nbsp;</p><p>马斯克也加入了那些警告人工智能可能对人类和人类构成生存威胁的行列。这变相巩固了微软和谷歌等巨头的力量。但目前在xAI 唯一有从事人工智能风险工作历史的人是唯一的指定顾 Dan Hendrycks，他是非营利性人工智能安全中心的主任，并协调了技术领导者最近关于人工智能可能构成生存威胁的公开声明。</p><p>&nbsp;</p><p>马斯克习惯于大胆下注，但 xAI 的目标几乎没有透露多少。ChatGPT 及其竞争对手（例如 Google 的 Bard）都建立在深度学习的基础上，而 OpenAI 的首席执行官 Sam Altman 曾表示，需要全新的想法来超越现有系统。</p><p>&nbsp;</p><p>但人工智能最近的进展大部分来自于使现有系统变得更大，并向其投入更多的计算能力和数据。</p><p>现阶段，xAI 似乎缺乏与 OpenAI、微软和谷歌相匹配所需的云计算能力，但马斯克拥有一些可以利用的重要资源。</p><p>&nbsp;</p><p>官网显示，xAI 公司将独立于 X Corp，但会与 X (Twitter)、特斯拉和其他公司密切合作。Twitter平台上的对话数据非常适合训练像ChatGPT背后那样的大型语言模型，而特斯拉现在设计了自己的专用AI芯片，并且在为AI构建大型计算集群方面拥有丰富的经验，这可以用来提升xAI的云计算能力。特斯拉还在建造一个人形机器人，这个项目将来也可能会得到 xAI 的帮助。</p><p>&nbsp;</p><p>马斯克为自己设定的宏伟目标：挑战现有的人工智能巨头并保护人类免受有害人工智能的侵害。但目前这样的团队似乎还不足以帮助其实现目标。许多人工智能研究人员似乎认为，这个目标的实现需要更大的透明度和协作，而不是一个孤独的天才和一小群明星。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://x.ai/\">https://x.ai/</a>\"</p><p><a href=\"https://twitter.com/elonmusk/with_replies\">https://twitter.com/elonmusk/with_replies</a>\"</p><p><a href=\"https://www.wired.com/story/fast-forward-elon-musks-xai-chatgpt-hallucinating/\">https://www.wired.com/story/fast-forward-elon-musks-xai-chatgpt-hallucinating/</a>\"</p><p><a href=\"https://www.163.com/dy/article/H513M91I051193U6.html\">https://www.163.com/dy/article/H513M91I051193U6.html</a>\"</p>",
    "publish_time": "2023-07-13 14:13:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“ AI 框架”与“ AI 中台”;在大模型训练实践中如何发挥作用？",
    "url": "https://www.infoq.cn/article/6ErEzdyB8u9riX7CA3Pz",
    "summary": "<p>大模型已经成为了各个应用领域的标配，而支持<a href=\"https://xie.infoq.cn/article/f65552c0d5e5451a2cbf7fb69\">大模型</a>\"训练和部署的 AI 基础设施也变得越来越重要。</p><p></p><p>为了保障大模型的成功落地，需要构建起全栈的 AI 基础设施，包含 AI IaaS 和 AI PaaS，其中 AI IaaS 提供海量的算力和资源调度、任务管理的能力，解决资源效能的问题。AI PaaS 为大模型提供并行策略和优化过的环境，覆盖训练的全生命周期，解决开发效率的问题。</p><p></p><p>为了让大家更全面地了解 <a href=\"https://xie.infoq.cn/article/d7f06df6500a9f00ecd610058\">AI </a>\"技术的前沿发展趋势及技术实践，<a href=\"https://xie.infoq.cn/article/2a11bdd378c723de1fd3ae28b\">百度</a>\"智能云团队特推出《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课，该课程围绕“云：AI 算力构建”、“智：AI 框架和 AI 中台”、“实践：大模型训练实践”三大主题展开，由多位专业大咖倾情打造，揭秘核心技术，直击行业痛点。</p><p></p><p>过去一个月，第一模块“云：AI 算力构建”的四讲课程已上线直播，为大家陆续剖析了《大规模 AI 高性能网络的设计和实践》、《GPU 容器虚拟化新能力发布和全场景实践》、《面向大模型的存储加速方案设计和实践》、《向量检索在⼤模型应⽤场景的设计和实践》。接下来的一个多月，我们将继续围绕“AI 框架和 AI 中台”、“大模型训练实践”两大模块进行深度探索：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/88/88cfa2d5a920a88d94727868a8c6f4e2.jpeg\" /></p><p></p><p>第五讲 《飞桨大模型分布式训练技术》：</p><p></p><p>了解飞桨核心分布式训练技术以及在大模型训练场景中的应用；</p><p></p><p>了解如何根据实际场景选择合适的飞桨分布式训练技术。</p><p></p><p>&nbsp;第六讲《飞桨大模型推理部署高性能优化 》：</p><p></p><p>了解大模型推理的需求、难点、应用场景；</p><p></p><p>了解大模型推理加速的常用方法、加速原理和关键技术。</p><p></p><p>&nbsp;第七讲《大模型 LLMOps 工具链与文心千帆大模型平台》：</p><p></p><p>了解大模型技术发展趋势；</p><p></p><p>了解大模型训练推理的主要环节及挑战；</p><p></p><p>了解什么是企业级大模型 LLMOps 工具链和基础设施；</p><p></p><p>了解百度 AI 大底座中的文心千帆大模型平台的核心技术。</p><p></p><p>&nbsp;第八讲《百度百舸平台的大模型训练最佳实践》：</p><p></p><p>了解不同大模型对基础设施资源选型和规划的要求；</p><p></p><p>了解在百度百舸平台中如何进行环境搭建和使用；</p><p></p><p>了解训练过程中典型故障处理方法和性能调优技巧。</p><p></p><p>第五讲课程《飞桨大模型分布式训练技术》上线时间为 2023 年 7 月 19 日 19:30，目前课程报名通道现已开启，立即点击<a href=\"https://www.infoq.cn/form/?id=1701&amp;utm_source=5&amp;sign=iq_64ae48c549ac0\">此链接</a>\"进行报名，还有机会抽奖赢取周边大礼哦！</p>",
    "publish_time": "2023-07-13 14:24:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "向量检索在大模型应用场景的技术和实践",
    "url": "https://www.infoq.cn/article/2JOG7j7QB3XlDF2BMvIo",
    "summary": "<p>在《大模型时代的 AI 基础设施——百度 AI 大底座》系列云智公开课第四讲中，百度智能云高级架构师李莅分享了向量检索在大模型应用场景的技术和实践。</p>\n<p>首先，他介绍了向量检索的的应用领域，从传统的视频搜索、语音识别等领域到如今的大模型应用都离不开向量检索；其次，详细讲解了向量检索技术，包括IVF-PQ算法、图算法和HNSW算法；之后，介绍向量检索工程的相关落地实践；最后，展望未来，百度云将结合百度自研算法，推出专用型向量数据库，来更好地支撑云上大模型业务和相关应用。</p>",
    "publish_time": "2023-07-13 15:47:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI大模型竞争白热化，算力优化才是“超车点”？",
    "url": "https://www.infoq.cn/article/FRXOJ4dDWCK34mzO9EDM",
    "summary": "<p>嘉宾 | 蒋晓维博士、戴金权</p><p>采访 | 凌敏、李冬梅</p><p>作者 | 凌敏</p><p></p><p>算力是驱动人工智能产业发展的核心动力。在数据、算法和算力三大人工智能要素中，算力是将数据和算法真正通过硬件执行的基础单元，并将数据、算法转化为最终的生产力。</p><p>&nbsp;</p><p>随着 AI 技术的高速发展，以及 AI&nbsp;大模型的广泛应用，AI 算力需求正在快速增加，大概每隔 3-4 个月就会增加一倍。如今，对 AI 任务所需算力总量的度量单位已经进入 PD 时代（PetaFlops/s-day），即用每秒千万亿次的计算机完整运行一天消耗的算力总量作为度量单位。比如，特斯拉 FSD 全自动驾驶系统的融合感知模型训练消耗的算力当量是 500 个 PD。</p><p>&nbsp;</p><p>可以看到，在 AI 大模型时代，AI 领域的“军备竞赛”正从过去算法和数据层面的竞争，转变为底层算力的竞争。机遇的背后，如何破解算力困局、实现算力优化，也是整个行业需要解决的课题。近日，InfoQ 采访了大禹智芯联合创始人/CTO、IEEE 国际顶会 HPCA 名人堂成员蒋晓维博士，英特尔院士、大数据技术全球 CTO 戴金权，以期探索 AI 大模型时代下的算力困局破解路径，寻求算力优化最优解。</p><p></p><h2>AI 大模型时代，算力需求大爆发</h2><p></p><p>&nbsp;</p><p>作为 AI 的重要子领域，机器学习的发展最早可以追溯至 20 世纪 50 年代。2012 年，AlexNet 首次引起广泛关注，使得机器学习分支深度学习的热度呈指数级上升。在传统的机器学习和深度学习技术中，算力作为底层基础设施扮演着至关重要的角色，不断推动上层技术迭代创新。使得这些传统技术在图像识别、图像分类、自然语言处理、广告推荐、自动驾驶和图像生成等领域愈加成熟，并在实践中得到了广泛应用。</p><p>&nbsp;</p><p>在 AI 领域，大家关注的焦点主要包括各种各样的数据集，以及诸如 Caffe、TensorFlow、PyTorch 等深度学习框架，还有像 Horovod 这样的分布式训练框架。与此同时，底层芯片技术也在不断演进发展。最早企业使用 CPU 进行训练；随后，GPU/GPGPU（通用 GPU）成为训练和推理的标准设备；再到后来开始出现一些专用的 AI 芯片，比如谷歌的 TPU 芯片，以及国内的寒武纪等等。</p><p>&nbsp;</p><p>2022 年，AIGC 技术迎来应用大爆发，从 OpenAI 文本生成图像系统 Dall-E2 到 AI 绘画神器 Stable Diffusion，AIGC 迅速火成“顶流”。</p><p>&nbsp;</p><p>戴金权表示，AIGC 技术主要涵盖两类模型：一类是像 Stable Diffusion 这样的扩散模型，它可以生成图片、音频、视频等等；另一类是大语言模型，从语言模型角度来生成文本、对话等等。这两种模型的需求不一样，扩散模型更多是对计算的需求更高一些，而大语言模型更多是要求内存的带宽和大小能够支撑。很多时候一个比较大的大语言模型，是无法放到一张显卡上同时运行的，可能需要更大的内存支持。</p><p>&nbsp;</p><p>“从英特尔的角度来看，我们需要对不同的计算、内存、Transformer 注意力机制算子的要求，以及对模型进行压缩，不管是稀疏化还是低精度等等，通过多样化技术对它进行更好的支持。多模态是一个非常重要的方向，最终大模型追求的是这个模型不仅可以追求处理文本，还可以处理图片、视频等，不再是一个单一的算子，而是很多算子在模型里同时存在，如何来提供这样的支持，都是一些技术上的挑战。”戴金权说道。</p><p>&nbsp;</p><p>2022 年 11 月，ChatGPT 横空出世，成功掀起了 AI 大模型热潮。随后，国内外陆续发布了多款 AI 大模型。</p><p>&nbsp;</p><p>蒋晓维认为，这一波大语言模型热潮与之前的机器学习和深度学习创新相比，确实存在诸多不同，并不断刷新大家的认知。“从 AlexNet、CNN+LSTM、VGG、ResNet，再到后来的 GAN 和最近的 Diffusion Model，以及 AIGC 领域的 Bert、GPT 等，这些模型领域的不断迭代创新已经持续至少 9 年了。ChatGPT 的出现实际上是过去 9 年各种技术栈有机结合后的一个积累和突破的过程。”</p><p>&nbsp;</p><p>参数规模方面，GPT-3 的参数规模是 1750 亿。近日，“天才黑客”乔治·霍兹在接受采访时透露，GPT-4 参数高达 1.76 万亿，是 GPT-3 的 10 倍。算力需求方面，有数据显示，GPT-3 的整个完整训练需要 3.14E11（TFLOPS）的每秒浮点运算量。OpenAI 首席执行官 Sam Altman 曾在接受采访时指出，GTP-4 需要的计算量为 GTP-3 的 10 倍；GTP-5 需要的计算量为 GTP-3 的 200-400 倍。</p><p>&nbsp;</p><p>大模型的背后离不开庞大算力的支撑，这种支撑通常来自于硬件与软件两方面。以英特尔为例，戴金权在接受采访时表示，从算力角度来看，英特尔支持生成式 AI 的计算主要做两方面工作：</p><p>&nbsp;</p><p>一是在硬件层面。得益于英特尔的XPU战略，比如一个笔记本电脑也可以有一个强大的 XPU 平台，有 CPU、集成显卡、独立显卡，下一代还将有 VPU，利用不同的加速来对生成式 AI 进行运算的支撑。在数据中心端也是如此，第四代英特尔至强可扩展处理器内置的矩阵运算加速器（英特尔 AMX），还有英特尔数据中心 GPU Ponte Vecchio（PVC）、Gaudi 系列专用 AI 加速器。二是在软件层面，利用软件的技术将硬件的计算能力提供出来，包括与 TensorFlow、PyTorch、Hybrid Bonding 等开源软件进行了广泛合作，与 OpenAI 合作的 AI 编译器 Triton，以及和微软合作优化的做大规模分布式训练的软件栈 DeepSpeed 等等。</p><p></p><h2>如何破解 AI 算力困局？</h2><p></p><p>&nbsp;</p><p>庞大的算力需求也意味着需要高昂的训练成本。根据英伟达的数据，GPT-3 需要使用 1024 颗 A100 芯片训练长达一个月的时间，总成本约为 460 万美元。而 GPT-4 的训练成本大约在 1 亿美元左右，GPT-5 的成本会更高。</p><p>&nbsp;</p><p>毫无疑问，AI 大模型的训练是一个“非常昂贵的过程”。所以也有观点认为，算力成本是限制 AI 大模型和生成式 AI 发展的因素之一。</p><p>&nbsp;</p><p>“除了在软件、模型和算法层面进行多维度的优化之外，CPU通用计算领域的发展历程可以为大模型算力领域的成本优化提供一些借鉴意义”。蒋晓维提到。在CPU通用计算领域，提升算力存在两种模型，分别是“Scale up”（水平方向上扩展）和“Scale out”（垂直方向上扩展）。“Scale up”是指通过各种方式将一台机器扩展到像小型机甚至大型机的规模，而“Scale out”是指通过由 CPU、内存、存储等商业化部件构建单台服务器，通过复制这些机器，并将这些机器以高性能的数据中心网络互联起来，再结合一些系统层面的技术将其构建成类似小型机的解决方案。传统的小型机是“Scale up”的经典案例，以单路和双路x86服务器构建的数据中心则是“Scale out”的代表。</p><p>&nbsp;</p><p>从“Scale up”到“Scale out”是通用计算领域经历的一种发展过程。在国外，谷歌是一个早期的代表案例，而在国内，阿里是最著名的代表。阿里有一个著名的故事叫做“去 IOE”，即摒弃 IBM 的小型机、Oracle 的数据库以及 EMC 的存储，通过商用化的 x86 服务器构建“Scale out”的数据中心。</p><p>&nbsp;</p><p>蒋晓维认为，这或许是大型模型和 GPU 算力领域未来可能要走的路线。“目前我们仍然在走走’Scale up’这条路线，单 GPU 服务器越做越大、也越做越贵。而‘Scale out’的方式，我认为应该是维持一个最基本的小单元，可能包含 CPU 、GPU 和高性能互联网卡，不同的芯片器件可以由不同的厂家提供。英伟达的 Grace-Hopper superchip 目前是这种基本单元的代表方案。通过分布式方式和高性能、高效的网络将计算单元互联起来是一种降低成本的可能途径。现如今，数据中心的网络延迟已经达到了亚微秒级别，甚至是纳秒级别，完全具备了将计算单元高效互联的能力。这是从‘Scale up’方式逐渐演变到‘Scale out’方式的一个维度。我们可以借鉴通用计算领域先前的一些经验。”</p><p>&nbsp;</p><p>此外，通过软件来承担一些高可用功能，如容错等，以及寻找第二供应商，都是降低成本的关键手段。</p><p></p><h3>构建分布式算力</h3><p></p><p>&nbsp;</p><p>在降低算力成本之外，如何更好地利用算力、提升算力的效率也是业界亟待解决的问题。而如何将计算能力分布式化、构建分布式计算能力，正是算力优化的前提。</p><p>&nbsp;</p><p>在过去，大家对 AI 芯片领域的关注点主要集中在推理方面，但现在大模型使得人们更关注分布式训练，尤其是分布式训练集群的构建。因为单张卡无法满足需求，所以需要构建分布式训练集群，通过高效的互联将大量 GPU 连接起来。</p><p>&nbsp;</p><p>除了提升单个 GPU 芯片的能力之外，另一个核心问题是如何高效地将 GPU 单卡构建成分布式训练能力。这是当前大模型算力构建过程中一个非常核心的领域和技术。这需要超级计算网络的能力和高性能网络，以高效地互联单个节点的 GPU 计算单元，并且还需要更高效的 CPU 与 GPU 协同能力。最近发布的英伟达的 DGX GH200 正是这些技术的巅峰体现。</p><p>&nbsp;</p><p>蒋晓维认为，英伟达不仅仅是一家 GPU 算力公司，同时也是一家高性能网络和 CPU 公司。“我们可以看下英伟达的核心技术。首先，它在芯片功能方面往往是采用最先进的制程技术，同时需要在最先进的制程支持下达到单 die 面积以及功耗和散热的极限。因此，对于芯片设计领域以及制程的各个环节，都有非常高的要求。我认为这是第一个基础，就是芯片设计领域，包括先进的制程技术，高计算能力的单卡芯片。在此基础上，我们再构建多机多卡的训练，将高效的单卡互联起来。这就需要高性能网络的能力，通过这种高性能网络能力实现单卡性能的‘线性’理想状况，同时在扩展性方面也有很高的基本要求。”</p><p>&nbsp;</p><p>在过去的几十年中，英伟达曾涉足 x86 芯片组领域，并且在退出该业务后一直致力于 ARM CPU 的研发。目前，英伟达已经推出了基于 ARM 架构的 Grace 芯片产品，并通过 NvLink C2C 能力在最近发布的 Grace Hopper 超级芯片中实现了高速高效的 GPU 和 CPU 之间的互联。通过 NvLink 技术实现多个 CPU 芯片之间的互联，以实现双路甚至多路 CPU 架构。除此之外，在完成对 Mellanox 的收购之后，英伟达在高性能网络领域的 Infiniband、RDMA、GDR 等技术也充分支持了多 GPU 服务器节点直接的互联，为“Scale out”的部署奠定了基础。</p><p>&nbsp;</p><p>此外，英特尔和 AMD 也在同时在CPU、GPU和高性能网络互联技术领域具备强大能力。在 CPU 领域，英特尔和 AMD 都是行业领导者。在网络领域，英特尔拥有自己的 Mount Evans（IPU），而 AMD 在收购 Pansando 后在 DPU 领域也获得了强大实力。在带内-带间互联方面，英特尔通过 QPI 或 UPI 等技术实现了 CPU 的多插槽互连能力。同时，它还有像 CXL 这样的技术，可以实现加速器与 CPU 或内存与 CPU 之间的高效互连，以及自身功能所拥有的 EMIB（2.5D 封装技术），实现芯片之间的互联。而 AMD 则拥有 Hyper Transport 以及基于此的 Infinity Fabric 等核心技术，可以实现带内-带间芯片之间的高效互连。所有这些技术都为构建分布式算力提供了必要的基础。</p><p>&nbsp;</p><p>目前，英伟达的 DGX GH200 产品已经达到了极致水平，其拥有 1.8 万个 CPU 核心、256 个 GPU 和 144T 内存，它们之间通过各种高速互联技术有机地结合在一起。这种模式已经对分布式训练框架和模式产生了重大影响。接下来的问题是，如何支持这种设备类型的操作系统？如何支持如此大规模的设备内存？这些都是未来技术发展的方向和挑战。</p><p></p><h3>算力优化探索与实践</h3><p></p><p>&nbsp;</p><p>在具体的算力优化探索与实践中，蒋晓维表示，作为一家 DPU 公司，大禹智芯关注的是分布式集群算力模型领域的优化，主要集中在从单机单卡到多机规模的优化。</p><p>&nbsp;</p><p>在分布式训练场景中，尤其是训练大型模型如 GPT 时，通常需要使用成千上万个 GPU。在这个过程中，大禹智芯将算力或芯片执行的计算分为两个维度：</p><p>&nbsp;</p><p>第一个维度是纯计算，即模型的前向传播和反向传播过程，主要在GPU上完成。另一个维度是耗费大量算力但不是 GPU 算力的部分，即训练中的梯度下降过程，在分布式 GPU 中，需要对参数进行全局约简操作，以获得最终的全局约简结果。可以将这部分称为训练中的 I/O 部分，它主要消耗芯片的网络资源而不是 GPU 算力。这部分也是大禹智芯产品关注的焦点。&nbsp;</p><p>&nbsp;</p><p>在大型模型训练中，当达到 2000 个 GPU 时，I/O 部分和计算部分的比例已经达到 1:1。随着 GPU 数量超过 2000，I/O 部分所花费的时间和算力可能会超过计算部分。因此，大禹智芯专注在分布式训练中优化 I/O 部分，利用核心网络技术能力来进行优化。</p><p>&nbsp;</p><p>“在算力优化方面，我们有几个核心技术：首先是我们支持高度灵活且可编程的硬件零拥塞控制技术，用于取代传统以太网上的 RoCE v2 协议。传统协议在流量控制方面比较简单单一，存在一些问题。我们的技术提供了更灵活和可编程的解决方案，解决了这些问题。第二，我们支持超低延迟特性。第三，我们支持用于分布式训练中的 MPI 消息传递这种集体通信。通过对各个维度进行大量硬件优化，并结合 RDMA 和 MPI，在训练过程中实现与 InfiniBand 相当的性能。这些是我们在从单机单卡到分布式训练的过程中进行的算力网络优化工作。”蒋晓维介绍道。</p><p>&nbsp;</p><p>据了解，目前在构建 GPU 算力网络方面，大多数公司仍选择使用 InfiniBand 网卡和交换机，其中主要使用两种核心技术：一种是 RDMA（远程直接内存访问） 技术，通过 GPUDirect RDMA 来消除 CPU 在 I/O 层面上的控制角色，从而降低整个训练过程中的 I/O 消耗。另一种技术是 SHARP（Scalable Hierarchical Aggregation and Reduction Protocol），这也是 Mellanox 的核心技术，通过 SHARP 技术来减少在分布式算力过程中对网络带宽的消耗。</p><p>&nbsp;</p><p>目前，大多数公司在构建算力网络时仍基于英伟达的解决方案。然而，一些头部互联网公司已经开始在以太网上构建 GPU 算力网络，不再完全依赖 InfiniBand 网络。在这种情况下，一个核心问题是找到一个能够完全替代 InfiniBand 上 RDMA 的技术。</p><p>&nbsp;</p><p>英伟达除了在 InfiniBand 上有 RDMA 技术之外，也有以太网上的 RDMA 技术，称为 RoCE v2。然而，在许多头部互联网公司应用中，这种技术仍然存在一些问题，所以国际国内一些头部互联网公司已经开始研发自己的技术，用以取代 RoCE v2 以太网上的 RDMA，并通过自研的方式实现更可靠的运行。他们能够在有丢包的网络环境中稳定地运行 RDMA，并将这项技术应用于 GPU 训练集群中，这是一些行业内领先公司具备的核心能力。</p><p>&nbsp;</p><p>“对于大禹智芯来说，我们的工作完全基于这些头部公司的实践和技术趋势。我们也在致力于开发类似的产品，因为我们相信这些头部公司的核心技术往往只局限于他们自身的部署。但是，我们认为更广泛的公司可能并不具备这样的能力。像大禹智芯这样的第三方芯片公司的价值就在于通过通用化的技术，为更广泛的场景提供支持，并通过更普适的方式将这些技术落地。”蒋晓维说道。</p><p></p><h2>写在最后：软件算法设计的多样化亦是关键</h2><p></p><p>&nbsp;</p><p>在分布式算力构建方面，蒋晓维认为，构建分布式算力网络需要与芯片领域紧密结合，并且在每个单元上都需要应用先进的制程技术，以支持最大规模的带宽。未来，需要重点考虑两方面：</p><p>&nbsp;</p><p>首先是芯片产业的发展。这涉及到各种芯片 IP，例如 SerDes、PCIE 控制器等核心 IP，还有 EDA 工具和先进制程技术。这些都是构建各种算力的基本单元能力。其次是国内的各种 xPU 公司。目前，国内的 xPU 公司仍处于早期阶段，各自为政，发展还比较零散。</p><p>&nbsp;</p><p>“在国内，要在相对较短的时间内集合整个产业的力量共同实现目标，而不是通过一家公司逐步发展各个领域的能力，可能需要采取某种方式来结合产业力量共同实现目标。在这方面，我认为有一个关键技术是芯片领域的芯片模块化（Chiplet）技术，这是一项非常有潜力的技术。通过芯片模块化，我们可以通过成本较低的封装技术将不同的芯片模块集成在一颗芯片上，从而实现让每个领域的专业公司专注于其擅长的事情。另外，芯片模块化本身还是一个相对较新的概念，例如芯片模块化的标准化组织 UCIe 也刚刚成立不久。因此，在这个领域，国内与国外之间肯定存在差距，但差距并不是特别大，仍然有迎头赶上的机会。”蒋晓维总结道。</p><p>&nbsp;</p><p>展望未来，戴金权希望可以做到“AI 无所不在”，不管是在本地端、云端还是边缘端。从这个角度来看，从小尺寸设备扩展到大规模数据中心的 XPU 架构，是一个非常重要的、且能够支持未来 AIGC 技术无所不在的需求的趋势。从软件的角度来看，现在的大模型基本上是以 Transformer 架构作为基础构件，目前业界正在做大量的研究工作，探索 Transformer 架构对内存的需求，包括内存带宽、内存容量以及计算需求如何进行更好的加速。从发展的眼光来看，至少 Transformer 这样的大模型可能会有更大的尺寸，包括输入上下文的扩展，将来可能是今天的几倍、几十倍甚至更高。这必然会对软件算法的设计，比如低精度、低比特、压缩、稀疏化，包括注意力机制设计等有不同的需求。</p><p>&nbsp;</p><p>“所以，软件算法设计的多样化，是我们认为未来有助于满足 AIGC 和大语言模型的算力需求的重要组成部分。这些需求可能会进一步引导我们未来的训练、推理，以及芯片的架构等。此外，大模型还在快速发展当中，可能在更长的时间段，比如十年、几十年的时间里有很多的发展，有不同算法级别的发展，以及在不同场景适配的发展，这些都会对 AI 芯片，包括对所有计算的芯片、计算的能力带来深远的影响。”戴金权总结道。</p><p></p><h4>采访嘉宾</h4><p></p><p>&nbsp;</p><p>蒋晓维博士，大禹智芯联合创始人/CTO，入选 HPCA 名人堂，曾供职英特尔、阿里、谷歌。是英特尔首颗超低功耗处理器 Quark D1000 首席架构师、Edison SoC 芯片架构师。在阿里工作期间，为国内首颗 x86 CPU 的特性定制化工作带领人、阿里智能网卡团队创建人、阿里云倚天 710 Arm CPU 的 IO 子系统首席架构师。曾担任谷歌智能网卡团队技术负责人，带领团队进行 IPU 在谷歌云的研发部署工作。</p><p>&nbsp;</p><p>戴金权，英特尔院士、大数据技术全球 CTO。负责领导英特尔全球（位于硅谷和上海）的工程团队在高级大数据分析、分布式机器学习和深度学习上的研发工作，以及和全球领先的研究机构（如 UC Berkeley AMPLab、RISELab 等）的技术合作。Apache Spark 项目的创始 committer 和项目管理委员会（PMC）委员，Apache MXNet 项目导师，BigDL 和 Analytics Zoo 项目创始人。</p><p></p><h4>AIGC&nbsp;课程推荐</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/3953d202a8310824d0865106bcbc2c55.png\" /></p><p></p><p>极客时间《AI 大模型应用开发&nbsp;·&nbsp;实战营》首发，作者是深耕&nbsp;AI&nbsp;领域多年的技术专家彭靖田。</p><p>&nbsp;</p><p>课程&nbsp;8&nbsp;周全程直播授课，覆盖硬件选型、大模型理论、&nbsp;LangChain&nbsp;开发框架剖析和落地实践，全程紧贴实际生产环境。现在市面上流行实用的，比如&nbsp;AutoGPT&nbsp;数字员工、翻译助手、智能销售顾问等经典项目，课程中都会带你动手实践。</p><p>&nbsp;</p><p>不容错过的历史机遇，帮助你掌握利用大模型开发相关的&nbsp;AI&nbsp;应用的能力！</p>",
    "publish_time": "2023-07-13 15:53:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "七部门公布《生成式人工智能服务管理暂行办法》：鼓励采用安全可信的芯片、软件、工具、算力和数据资源",
    "url": "https://www.infoq.cn/article/l9d6WALH3ufAvBIDCKJr",
    "summary": "<p>据“网信中国”消息，近日，国家网信办联合国家发展改革委、教育部、科技部、工业和信息化部、公安部、广电总局公布《生成式人工智能服务管理暂行办法》（以下简称“办法”）。</p><p>&nbsp;</p><p>《办法》提出，鼓励生成式人工智能技术在各行业、各领域的创新应用，生成积极健康、向上向善的优质内容，探索优化应用场景，构建应用生态体系。鼓励生成式人工智能算法、框架、芯片及配套软件平台等基础技术的自主创新，平等互利开展国际交流与合作，参与生成式人工智能相关国际规则制定。</p><p>&nbsp;</p><p>提供者发现违法内容的，应当及时采取停止生成、停止传输、消除等处置措施，采取模型优化训练等措施进行整改，并向有关主管部门报告。提供者发现使用者利用生成式人工智能服务从事违法活动的，应当依法依约采取警示、限制功能、暂停或者终止向其提供服务等处置措施，保存有关记录，并向有关主管部门报告。</p><p>&nbsp;</p><p>全文如下：</p><p></p><h4>生成式人工智能服务管理暂行办法</h4><p></p><p></p><h4>第一章&nbsp; 总　则</h4><p></p><p></p><p>第一条&nbsp; 为了促进生成式人工智能健康发展和规范应用，维护国家安全和社会公共利益，保护公民、法人和其他组织的合法权益，根据《中华人民共和国网络安全法》、《中华人民共和国数据安全法》、《中华人民共和国个人信息保护法》、《中华人民共和国科学技术进步法》等法律、行政法规，制定本办法。</p><p>&nbsp;</p><p>第二条&nbsp; 利用生成式人工智能技术向中华人民共和国境内公众提供生成文本、图片、音频、视频等内容的服务（以下称生成式人工智能服务），适用本办法。</p><p>国家对利用生成式人工智能服务从事新闻出版、影视制作、文艺创作等活动另有规定的，从其规定。</p><p>&nbsp;</p><p>行业组织、企业、教育和科研机构、公共文化机构、有关专业机构等研发、应用生成式人工智能技术，未向境内公众提供生成式人工智能服务的，不适用本办法的规定。</p><p>&nbsp;</p><p>第三条&nbsp;&nbsp;国家坚持发展和安全并重、促进创新和依法治理相结合的原则，采取有效措施鼓励生成式人工智能创新发展，对生成式人工智能服务实行包容审慎和分类分级监管。</p><p>&nbsp;</p><p>第四条&nbsp; 提供和使用生成式人工智能服务，应当遵守法律、行政法规，尊重社会公德和伦理道德，遵守以下规定：</p><p>&nbsp;</p><p>（一）坚持社会主义核心价值观，不得生成煽动颠覆国家政权、推翻社会主义制度，危害国家安全和利益、损害国家形象，煽动分裂国家、破坏国家统一和社会稳定，宣扬恐怖主义、极端主义，宣扬民族仇恨、民族歧视，暴力、淫秽色情，以及虚假有害信息等法律、行政法规禁止的内容；</p><p>（二）在算法设计、训练数据选择、模型生成和优化、提供服务等过程中，采取有效措施防止产生民族、信仰、国别、地域、性别、年龄、职业、健康等歧视；</p><p>（三）尊重知识产权、商业道德，保守商业秘密，不得利用算法、数据、平台等优势，实施垄断和不正当竞争行为；</p><p>（四）尊重他人合法权益，不得危害他人身心健康，不得侵害他人肖像权、名誉权、荣誉权、隐私权和个人信息权益；</p><p>（五）基于服务类型特点，采取有效措施，提升生成式人工智能服务的透明度，提高生成内容的准确性和可靠性。</p><p></p><h4>第二章&nbsp; 技术发展与治理</h4><p></p><p>&nbsp;</p><p>第五条&nbsp; 鼓励生成式人工智能技术在各行业、各领域的创新应用，生成积极健康、向上向善的优质内容，探索优化应用场景，构建应用生态体系。</p><p>&nbsp;</p><p>支持行业组织、企业、教育和科研机构、公共文化机构、有关专业机构等在生成式人工智能技术创新、数据资源建设、转化应用、风险防范等方面开展协作。</p><p>&nbsp;</p><p>第六条&nbsp; 鼓励生成式人工智能算法、框架、芯片及配套软件平台等基础技术的自主创新，平等互利开展国际交流与合作，参与生成式人工智能相关国际规则制定。</p><p>&nbsp;</p><p>推动生成式人工智能基础设施和公共训练数据资源平台建设。促进算力资源协同共享，提升算力资源利用效能。推动公共数据分类分级有序开放，扩展高质量的公共训练数据资源。鼓励采用安全可信的芯片、软件、工具、算力和数据资源。</p><p>&nbsp;</p><p>第七条&nbsp; 生成式人工智能服务提供者（以下称提供者）应当依法开展预训练、优化训练等训练数据处理活动，遵守以下规定：</p><p>&nbsp;</p><p>（一）使用具有合法来源的数据和基础模型；</p><p>（二）涉及知识产权的，不得侵害他人依法享有的知识产权；</p><p>（三）涉及个人信息的，应当取得个人同意或者符合法律、行政法规规定的其他情形；</p><p>（四）采取有效措施提高训练数据质量，增强训练数据的真实性、准确性、客观性、多样性；</p><p>（五）《中华人民共和国网络安全法》、《中华人民共和国数据安全法》、《中华人民共和国个人信息保护法》等法律、行政法规的其他有关规定和有关主管部门的相关监管要求。</p><p>&nbsp;</p><p>第八条&nbsp; 在生成式人工智能技术研发过程中进行数据标注的，提供者应当制定符合本办法要求的清晰、具体、可操作的标注规则；开展数据标注质量评估，抽样核验标注内容的准确性；对标注人员进行必要培训，提升尊法守法意识，监督指导标注人员规范开展标注工作。</p><p>&nbsp;</p><p></p><h4>第三章&nbsp; 服务规范</h4><p></p><p>&nbsp;</p><p>第九条&nbsp; 提供者应当依法承担网络信息内容生产者责任，履行网络信息安全义务。涉及个人信息的，依法承担个人信息处理者责任，履行个人信息保护义务。</p><p>&nbsp;</p><p>提供者应当与注册其服务的生成式人工智能服务使用者（以下称使用者）签订服务协议，明确双方权利义务。</p><p>&nbsp;</p><p>第十条&nbsp; 提供者应当明确并公开其服务的适用人群、场合、用途，指导使用者科学理性认识和依法使用生成式人工智能技术，采取有效措施防范未成年人用户过度依赖或者沉迷生成式人工智能服务。</p><p>&nbsp;</p><p>第十一条&nbsp; 提供者对使用者的输入信息和使用记录应当依法履行保护义务，不得收集非必要个人信息，不得非法留存能够识别使用者身份的输入信息和使用记录，不得非法向他人提供使用者的输入信息和使用记录。</p><p>&nbsp;</p><p>提供者应当依法及时受理和处理个人关于查阅、复制、更正、补充、删除其个人信息等的请求。</p><p>&nbsp;</p><p>第十二条&nbsp; 提供者应当按照《互联网信息服务深度合成管理规定》对图片、视频等生成内容进行标识。</p><p>第十三条&nbsp; 提供者应当在其服务过程中，提供安全、稳定、持续的服务，保障用户正常使用。</p><p>&nbsp;</p><p>第十四条&nbsp; 提供者发现违法内容的，应当及时采取停止生成、停止传输、消除等处置措施，采取模型优化训练等措施进行整改，并向有关主管部门报告。</p><p>&nbsp;</p><p>提供者发现使用者利用生成式人工智能服务从事违法活动的，应当依法依约采取警示、限制功能、暂停或者终止向其提供服务等处置措施，保存有关记录，并向有关主管部门报告。</p><p>&nbsp;</p><p>第十五条&nbsp; 提供者应当建立健全投诉、举报机制，设置便捷的投诉、举报入口，公布处理流程和反馈时限，及时受理、处理公众投诉举报并反馈处理结果。</p><p></p><h4>第四章&nbsp; 监督检查和法律责任</h4><p></p><p>&nbsp;</p><p>第十六条&nbsp; 网信、发展改革、教育、科技、工业和信息化、公安、广播电视、新闻出版等部门，依据各自职责依法加强对生成式人工智能服务的管理。</p><p>&nbsp;</p><p>国家有关主管部门针对生成式人工智能技术特点及其在有关行业和领域的服务应用，完善与创新发展相适应的科学监管方式，制定相应的分类分级监管规则或者指引。</p><p>&nbsp;</p><p>第十七条&nbsp; 提供具有舆论属性或者社会动员能力的生成式人工智能服务的，应当按照国家有关规定开展安全评估，并按照《互联网信息服务算法推荐管理规定》履行算法备案和变更、注销备案手续。</p><p>&nbsp;</p><p>第十八条&nbsp; 使用者发现生成式人工智能服务不符合法律、行政法规和本办法规定的，有权向有关主管部门投诉、举报。</p><p>&nbsp;</p><p>第十九条&nbsp; 有关主管部门依据职责对生成式人工智能服务开展监督检查，提供者应当依法予以配合，按要求对训练数据来源、规模、类型、标注规则、算法机制机理等予以说明，并提供必要的技术、数据等支持和协助。</p><p>&nbsp;</p><p>参与生成式人工智能服务安全评估和监督检查的相关机构和人员对在履行职责中知悉的国家秘密、商业秘密、个人隐私和个人信息应当依法予以保密，不得泄露或者非法向他人提供。</p><p>&nbsp;</p><p>第二十条&nbsp; 对来源于中华人民共和国境外向境内提供生成式人工智能服务不符合法律、行政法规和本办法规定的，国家网信部门应当通知有关机构采取技术措施和其他必要措施予以处置。</p><p>&nbsp;</p><p>第二十一条&nbsp; 提供者违反本办法规定的，由有关主管部门依照《中华人民共和国网络安全法》、《中华人民共和国数据安全法》、《中华人民共和国个人信息保护法》、《中华人民共和国科学技术进步法》等法律、行政法规的规定予以处罚；法律、行政法规没有规定的，由有关主管部门依据职责予以警告、通报批评，责令限期改正；拒不改正或者情节严重的，责令暂停提供相关服务。</p><p>&nbsp;</p><p>构成违反治安管理行为的，依法给予治安管理处罚；构成犯罪的，依法追究刑事责任。</p><p></p><h4>第五章&nbsp; 附　则</h4><p></p><p>&nbsp;</p><p>第二十二条&nbsp; 本办法下列用语的含义是：</p><p>&nbsp;</p><p>（一）生成式人工智能技术，是指具有文本、图片、音频、视频等内容生成能力的模型及相关技术。</p><p>（二）生成式人工智能服务提供者，是指利用生成式人工智能技术提供生成式人工智能服务（包括通过提供可编程接口等方式提供生成式人工智能服务）的组织、个人。</p><p>（三）生成式人工智能服务使用者，是指使用生成式人工智能服务生成内容的组织、个人。</p><p>&nbsp;</p><p>第二十三条&nbsp; 法律、行政法规规定提供生成式人工智能服务应当取得相关行政许可的，提供者应当依法取得许可。</p><p>&nbsp;</p><p>外商投资生成式人工智能服务，应当符合外商投资相关法律、行政法规的规定。</p><p>&nbsp;</p><p>第二十四条&nbsp; 本办法自2023年8月15日起施行。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://mp.weixin.qq.com/s/TztAWbEJmA3qBCiLgvZMyg\">https://mp.weixin.qq.com/s/TztAWbEJmA3qBCiLgvZMyg</a>\"</p>",
    "publish_time": "2023-07-13 17:31:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OB Cloud 上新，4.1 版本支持云上单机分布式一体化能力",
    "url": "https://www.infoq.cn/article/8r98IXBPrha7JRleT2XL",
    "summary": "<p>近日，OB&nbsp;Cloud&nbsp;正式上线&nbsp;OceanBase&nbsp;4.1&nbsp;新版本，支持企业在云上使用最新的&nbsp;<a href=\"https://xie.infoq.cn/article/c632f61b2c045f0c07d5ad8b5\">OceanBase</a>\"&nbsp;单机分布式一体化架构能力。去年 8 月 OceanBase&nbsp;宣布公有云服务全球开服，伴随 4.1 版本推出，OceanBase公有云正式命名为 OB&nbsp;Cloud 云数据库。</p><p></p><p>在本次推出的 4.1 版本中，性能较&nbsp;4.0 版本进一步提升，在内核能力上对<a href=\"https://xie.infoq.cn/article/29f71cabed8259f12a246d0d0\">分布式事务</a>\"和存储进行了大量优化。同时为了更好地支撑不同客户规模和多元业务场景，OB&nbsp;Cloud&nbsp;还推出了针对开发者及中小场景使用的租户共享规格，以及企业生产环境所需的集群独享规格，全系列规格支持客户云上从小到大的业务诉求。</p><p></p><p>具体来说，4.1 版本进行了多项能力升级：</p><p></p><p>内核能力增强：故障恢复能力再升级，当系统故障时依然可持续提供服务能力。少数派节点发生宕机时能够快速无损自动切换，支持更多场景达到 RTO&lt;8 秒的自动恢复故障指标。</p><p></p><p>性能大幅提升：用户期待的小规格场景下，OLTP 及 OLAP 性能显著提升。经实际测试，sysbench&nbsp;综合读写能力相比上一版本提升&nbsp;40%；TPC-H&nbsp;100G&nbsp;场景性能提升&nbsp;17%，TPC-DS&nbsp;100G&nbsp;场景性能提升&nbsp;15%。</p><p></p><p>兼容性增强：Oracle&nbsp;兼容性进一步增强，提供&nbsp;Oracle&nbsp;跨库读写能力，便于用户数据访问与分析的同时，支持数据库的跨库实时查询及写入。同时，提供全面兼容&nbsp;MySQL&nbsp;Binlog&nbsp;协议的能力，帮助用户更方便接入下游数据生态，更全面地兼容&nbsp;MySQL&nbsp;8.0&nbsp;版本。</p><p></p><p>4.1 版本的支持企业在云上使用单机分布式一体化能力，不论大小企业，一次选择满足全生命周期诉求。</p><p></p><p>OB&nbsp;Cloud&nbsp;构建在&nbsp;AWS、阿里云、腾讯云等全球主流公有云基础设施上，不仅具备降本增效、多级弹性、混合云等关键优势，还可以通过多租户、数据高压缩、HTAP&nbsp;等能力，帮助企业大幅降低数据库运维成本，平稳应对流量洪峰、解决高可用难题，突破 MySQL 的分析瓶颈，并降低大体量数据在分库分表时的运维成本。</p><p></p><p>OB&nbsp;Cloud 同时推出企业“节”计划，每个企业最高可享受价值 6.8 万元的支持礼包，旨在帮助不同规模的企业以更低门槛使用云数据库服务，“节”约数据库资源与运维成本，提升运营效率，实现降本增效。</p><p></p><p>目前，众多企业客户已通过 OB&nbsp;Cloud 实现数据库上云。海底捞通过 OB&nbsp;Cloud 打造新一代<a href=\"https://xie.infoq.cn/article/26e8bcf19c65b02101bf61a17\">分布式</a>\"业务系统，实现数据库整体成本节省 50%，降本增效成效显著。理想汽车借助 OB&nbsp;Cloud 打造全球领先的制造系统，并实现车云业务的跨云异地多活，产线连续性和业务稳定性得到全面保障。国潮品牌泡泡玛特凭借 OB&nbsp;Cloud 打造新一代分布式抽盒机系统，扩缩容时间降低 90% ，轻松应对高于日常流量百倍的“抽盒”高峰。</p><p></p><p>此外，包括新零售行业的知名企业二维火、客如云，互联网行业的携程、高德、菜鸟、GCash 及从事跨境服务的纵腾集团和洋葱集团等，都已通过 OB&nbsp;Cloud 实现数据库上云。</p><p></p><p>经过 13 年的发展，OceanBase 从原生分布式走向单机分布式一体化，从金融走向千行百业，从线下走向云端，OB&nbsp;Cloud 正在不断降低大家使用数据库的门槛，助力更多企业以更低的成本使用分布式数据库。</p>",
    "publish_time": "2023-07-13 17:53:49",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "MaaS赛道再添选手，京东推出言犀大模型：把大模型做实，把供应链做透",
    "url": "https://www.infoq.cn/article/UUvJEUI66giFqpxajpuY",
    "summary": "<p>7月13日，2023京东全球科技探索者大会暨京东云峰会在北京举行，全面推出京东言犀大模型、言犀AI开发计算平台、升级支撑大模型落地行业的产品及解决方案，服务千行百业拥抱产业智能。</p><p></p><p>源于产业、服务产业。言犀大模型融合了70%的通用数据与30%京东数智供应链原生数据，具有“更高产业属性、更强泛化能力、更多安全保障”的优势，致力于面向知识密集型、任务型产业场景，解决真实产业问题。</p><p></p><p>“从产业端切入大模型，如同从北坡攀爬珠峰，有更波澜壮阔的风景，有巨大的探索价值。”京东集团CEO许冉首次对外展示了京东的技术追求：成本、效率、体验、可信、普惠、突破。她表示，京东的技术发展史，就是一部供应链技术的发展史、技术驱动的发展史、产业降本增效的发展史。大模型的出现，为京东帮助产业实现价值倍增，增添了新的可能性。</p><p></p><p>大模型依托供应链深入产业</p><p></p><p>京东集团技术委员会主席、京东云事业部总裁曹鹏在会议上发表了题为《突破供应链价值 跨越产业智能》的演讲。他表示，对于京东而言，大模型不是目标，而是工具，目的在于真正创造出产业价值。因此，秉承着把人工智能技术与实体产业结合起来的理念，京东要做产业大模型而不是通用大模型。基于京东深耕多模态研发经验，集合京东零售、物流、健康等丰富产业场景，曹鹏宣布京东正式发布产业大模型“言犀”，内置70%标准语料和30%独有产业数据。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/37/2b/37c1f39b49ceac9663615681e82b362b.png\" /></p><p></p><p>“单一的模型是没有办法直接用起来的”，曹鹏表示，京东为“言犀”大模型准备了相应的配套开发工具——言犀AI开发计算平台，内置京东在开发大模型时积累的能力，预计8月会正式上线发布。他表示，言犀AI开发计算平台可以用“创建目标”“上传数据”“创建管道”“模型训练”“部署和发布”五步将通用大模型转化为专业的产业大模型。在平台中，除了京东自己的“言犀”大模型以外，还内置了其他开源大模型框架。</p><p></p><p>京东的经营理念是“成本”“效率”“体验”，而“言犀”大模型的应用不仅可以帮助用户提升体验，还可以帮助京东在零售、物流、健康、金融等领域降低成本、提升效率。此外，“做好大模型还需要有基础技术支持”。曹鹏表示，为了更好地应用“言犀”大模型，京东还对技术基座产品做了优化，为大模型设计了极致性能数字基础设施如京东云云舰3.0、京东云云海2.0以及京东云京刚3.0。</p><p></p><p>“我们的技术理念源于产业，也服务于产业。”曹鹏如是总结。“孤举者难起，众行者易趋”，整个产业智能时代的到来，不是光靠京东一家企业，而是靠无数行业中的同仁一起不断努力。</p><p></p><p>言犀大模型的技术优势</p><p></p><p>数智供应链训练产业大模型，大模型依托供应链深入产业。</p><p></p><p>优势一：尖端技术突破</p><p>7年深耕多模态模型研发：自2017年，京东云就布局人机交互、多模态智能等AI前瞻技术，在多模态领域的26项国际赛事中夺魁，并在2023年斩获中国人工智能最高奖吴文俊奖个人、团体两项殊荣。首创将领域知识注入大模型：基于领域知识注入的模型K-PLUG，采用推理低延时策略，推理速度提升6.2倍，模型部署成本降低90%。</p><p></p><p>优势二：产业原生数据</p><p>30%数智供应链原生数据：数智供应链长链路、复杂协同的原生数据，更适合产业大模型训练，沉淀30%原生数据，每年数百亿优质动态交互数据回流。围绕这些场景训练的大模型更适合产业应用。</p><p></p><p>优势三：新型算力加持</p><p>超大规模计算集群“天琴α”：2021年，京东落地重庆全国首个基于SuperPOD架构的超大规模计算集群——天琴α，算力总规模达到135TFLOPS（每秒浮点运算次数），推理提速6.2倍，推理成本节省90%。</p><p></p><p>构建大模型“训练营”和“弹药库” 打通技术落地每一步</p><p></p><p>京东探索研究院院长何晓冬在会议上介绍，言犀大模型支持语言、语音、视觉、多模态等，70%通用域数据，结合30%的行业数据，实现基座模型+垂直领域模型的精调，可服务于零售、金融、教育、政务等领域。他还透露，计划在2024年上半年全面开放大模型能力，向产业输出定制化模型。此外，京东也在探索从语言大模型走向多模态数字人交互，而具身智能也将是京东探索研究院未来重点探索方向。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/65/c6/65b0ff815636f8ac8448dc96ca3902c6.png\" /></p><p></p><p>何晓东列举了大模型的应用方向：已写30亿字营销文案，上万个直播脚本；数智人，应用于电商直播；艺术创作等。他认为，未来通往AGI的终局，多模态智能是必经之路。如果想打造一个像人一样聪明的AGI，就需要向人学习，通过多模态方式来进行知识获取、知识融合，这是让机器走向AGI的必经之路；未来人与机器交互时，必然是通过语言、视觉、语音进行沟通的，所以机器也必须能够理解相应的信息，否则机器就无法服务人类。</p><p></p><p>大模型在京东的实践</p><p></p><p>言犀大模型以“三步走”深入产业，2023年7月，产业原生，推出大模型；2023年下半年，内部实践，产品融合，锤炼迭代；2024年上半年，服务产业，全面开放，大模型能力向外部严肃商业场景开放。</p><p></p><p>数智供应链，服务超千万商品SKU、5000万工业品SKU 、800多万家活跃企业客户（其中世界500强企业超90%、全国专精特新中小企业近70%）、全国2000多条产业带。这个链路更长、场景更复杂、数据更丰富的京东内部场景，是大模型绝佳的“训练场”。</p><p></p><p>言犀AI开发计算平台：实现大模型下的模型即服务，高效、低成本构建AI产品。AIGC商品内容营销平台：每套图成本降低90%，制作周期从7天缩短到半天。多模态数字人：5分钟采集生成高仿真数字人，入驻4000+品牌直播间。AI增长营销平台：操作效率提升40倍，活动生产效率提高上百倍。健康助手及辅助诊疗：涵盖超千种疾病专业性服务，20种评价标准保障医疗安全。京东物流超脑：自动生成全局最优供应链解决方案。</p><p></p><p>在MaaS（Model as a Service，模型即服务）赛道越来越激烈的环境下，各大厂都聚焦在大模型的研发和应用场景探索，我们希望看到的是真的能实现业务增长，效率提升的现象，对各个细分行业有实际的价值。</p><p></p><p>【活动推荐】</p><p></p><p>在7月21-22日深圳举办的 <a href=\"https://archsummit.infoq.cn/2023/shenzhen/schedule\">ArchSummit 全球架构师峰会</a>\"上，我们也邀请了国内的企业，例如出门问问、科大讯飞、腾讯、顺丰科技、趣丸科技等企业来演讲，分享各自企业在大模型上的进展，欢迎你来交流。期待与你线下交流！咨询购票请联系 18514549229（微信同手机号）</p><p><img src=\"https://static001.infoq.cn/resource/image/c7/d2/c7e682d386f62f3685fcc149d3b612d2.jpg\" /></p><p></p>",
    "publish_time": "2023-07-13 18:02:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]