[
  {
    "title": "Java近期新闻：JHipster 8.1、Piranha Cloud 23.12、Open Liberty 23.0.0.12和多个版本的Tomcat",
    "url": "https://www.infoq.cn/article/xWykuUX5CkMaRdYXCMj4",
    "summary": "<p>本期的Java综述包括OpenJDK的早期访问版本、Open Liberty 23.0.0.12、Infinispan 15.0.0-Dev06、JHipster 8.1.0、Piranha 23.12.0和Apache Tomcat的多个版本（11.0.0-M15、10.1.17、9.0.84和8.5.97），以及首次登场亮相的Payara虚拟会议。</p><p>&nbsp;</p><p></p><h4>JDK 23</h4><p></p><p>JDK 23的早期访问构建版本的<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-23%2B2\">Build 2</a>\"发布，它是对Build 1的<a href=\"https://github.com/openjdk/jdk/compare/jdk-23%2B1...jdk-23%2B2\">更新</a>\"，其中包括对各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2023%20and%20%22resolved%20in%20build%22%20%3D%20b02%20order%20by%20component%2C%20subcomponent\">问题</a>\"的修复。关于这个版本的更多细节可以在<a href=\"https://jdk.java.net/23/release-notes\">发布说明</a>\"中找到。</p><p>&nbsp;</p><p></p><h4>JDK 22</h4><p></p><p>JDK 22的早期访问构建版本的<a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-22%2B28\">Build 28</a>\"发布，它是对Build 27的<a href=\"https://github.com/openjdk/jdk/compare/jdk-22%2B27...jdk-22%2B28\">更新</a>\"，其中包括对各种<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2022%20and%20%22resolved%20in%20build%22%20%3D%20b28%20order%20by%20component%2C%20subcomponent\">问题</a>\"的修复。关于这个版本的更多细节可以在<a href=\"https://jdk.java.net/22/release-notes\">发布说明</a>\"中找到。</p><p>&nbsp;</p><p>对于<a href=\"https://openjdk.org/projects/jdk/23/\">JDK 23</a>\"和<a href=\"https://openjdk.org/projects/jdk/22/\">JDK 22</a>\"，鼓励开发人员通过<a href=\"https://bugreport.java.com/bugreport/\">Java缺陷数据库</a>\"报告缺陷。</p><p>&nbsp;</p><p></p><h4>Spring Framework</h4><p></p><p><a href=\"https://spring.io/projects/spring-framework\">Spring Framework</a>\"的6.1.2和6.0.15版本<a href=\"https://spring.io/blog/2023/12/14/spring-framework-6-0-15-and-6-1-2-available-now\">发布</a>\"，提供了缺陷修复、文档改进、依赖性升级和新特性，例如，在TargetSource接口中，将isStatic()和releaseTarget()声明为默认方法；改进@RegisterReflectionForBinding注解，用于显式处理枚举；解决在ConcurrentReferenceHashMap中出现竞态条件的问题。这些版本可以分别在即将发布的<a href=\"https://spring.io/projects/spring-boot\">Spring Boot</a>\"&nbsp;3.2.1和3.1.7中使用。关于这些版本的更多细节，请参阅<a href=\"https://github.com/spring-projects/spring-framework/releases/tag/v6.1.2\">6.1.2版本</a>\"和<a href=\"https://github.com/spring-projects/spring-framework/releases/tag/v6.0.15\">6.0.15版本</a>\"的发布说明。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-data\">Spring Data</a>\"的2023.1.1和2023.0.7版本<a href=\"https://spring.io/blog/2023/12/15/spring-data-2023-1-1-and-2023-0-7-available\">发布</a>\"，提供了缺陷修复，并对相应的子项目进行了升级，比如：Spring Data Commons 3.2.1和3.1.7；Spring Data MongoDB 4.2.1和4.1.7；Spring Data Elasticsearch 5.2.1和5.1.7，以及Spring Data Neo4j 7.2.1和7.1.7。这些版本可能分别被即将发布的Spring Boot 3.2.1和3.1.7版本所使用。</p><p>&nbsp;</p><p></p><h4>Open Liberty</h4><p></p><p>IBM<a href=\"https://openliberty.io/blog/2023/12/12/23.0.0.12.html\">发布</a>\"了<a href=\"https://openliberty.io/\">Open Liberty</a>\"的23.0.0.12版本，包括如下特性：支持MicroProfile 6.1；升级至<a href=\"https://github.com/OpenLiberty/ci.maven/releases/tag/liberty-maven-3.10\">Liberty Maven plug-in 3.10</a>\"、<a href=\"https://github.com/OpenLiberty/ci.gradle/releases/tag/liberty-gradle-plugin-3.8\">Liberty Gradle plug-in 3.8</a>\"以及面向Eclipse IDE、IntelliJ IDEA和Visual Studio Code的Liberty Tools 23.0.12；解决<a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-44487\">CVE-2023-44487</a>\"，这是一个Tomcat实现的HTTP/2易于受到<a href=\"https://www.securityweek.com/rapid-reset-zero-day-exploited-to-launch-largest-ddos-attacks-in-history/\">快速重置攻击</a>\"的漏洞，这样会造成<a href=\"https://www.mail-archive.com/announce@apache.org/msg08557.html\">拒绝服务</a>\"，通常表现为OutOfMemoryError。</p><p></p><h4>Quarkus</h4><p></p><p><a href=\"https://quarkus.io/\">Quarkus</a>\"&nbsp;3.6.3<a href=\"https://quarkus.io/blog/quarkus-3-6-3-released/\">发布</a>\"，解决了如下问题：在3.6.2版本的回归中，ConfigDiagnostic类产生的NullPointerException；禁用Keycloak的<a href=\"https://quarkus.io/guides/dev-services\">Dev Services</a>\"所导致的启动错误；当Quarkus试图匹配未知的配置文件时，会抛出NullPointerException。关于该版本的更多细节，请参阅<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.6.3\">变更日志</a>\"。</p><p></p><h4>Micronaut</h4><p></p><p>Micronaut基金会<a href=\"https://micronaut.io/2023/12/15/micronaut-framework-4-2-2-released/\">发布</a>\"了<a href=\"https://micronaut.io/\">Micronaut框架</a>\"的4.2.2版本，其中包含了<a href=\"https://github.com/micronaut-projects/micronaut-core/releases/v4.2.2\">Micronaut Core 4.2.2</a>\"，以及对<a href=\"https://micronaut-projects.github.io/micronaut-aws/latest/guide/\">Micronaut AWS</a>\"和<a href=\"https://micronaut-projects.github.io/micronaut-cache/latest/guide/\">Micronaut Cache</a>\"模块的更新。关于该版本的更多细节，请参阅<a href=\"https://github.com/micronaut-projects/micronaut-platform/releases/tag/v4.2.2\">发布说明</a>\"。</p><p></p><h4>Helidon</h4><p></p><p><a href=\"https://helidon.io/\">Helidon</a>\"发布了<a href=\"https://github.com/helidon-io/helidon/releases/tag/2.6.5\">2.6.5版本</a>\"，包含如下特性：依赖性升级；在OciExtension类中支持供应商专门的注入点；纠正文档中如何设置OpenAPI生成器的错误。关于该版本的更多细节，请参阅<a href=\"https://github.com/helidon-io/helidon/blob/2.6.5/CHANGELOG.md\">变更日志</a>\"。</p><p></p><h4>Grails</h4><p></p><p>Grails基金会发布了<a href=\"https://grails.org/\">Grails框架</a>\"的6.1.1版本，其中包含了缺陷修复、依赖性升级和一些值得注意的变更，例如，通过为每个测试使用不同的模板名称来解决测试凌乱的问题；将Grails更新到Groovy 3.0.19并与之兼容；提供了SnakeYAML BOM。关于该版本的更多细节，请参阅<a href=\"https://github.com/grails/grails-core/releases/tag/v6.1.1\">发布说明</a>\"。</p><p></p><h4>Infinispan</h4><p></p><p><a href=\"https://infinispan.org/\">Infinispan</a>\"&nbsp;15.0.0的<a href=\"https://github.com/infinispan/infinispan/releases/tag/15.0.0.Dev06\">第六个开发版本</a>\"有一些显著的变化，比如，重新引入了对JCache的支持，因为它的CDI切面对javax命名空间的依赖是可选的，这使得不需要CDI就可以实现JCache；解决JGroupsTransport类中定义的getmemberspphysicaladdresses()方法抛出IllegalArgumentException的问题；在尝试注册度量指标之前，进行检查以确保MetricsRegistry接口是启用的。关于该版本的更多细节，请参阅<a href=\"https://github.com/infinispan/infinispan/compare/15.0.0.Dev05...15.0.0.Dev06\">变更日志</a>\"。</p><p></p><h4>Micrometer</h4><p></p><p><a href=\"https://github.com/micrometer-metrics/micrometer/blob/main/README.md\">Micrometer Metrics</a>\"&nbsp;的1.12.1和1.11.7版本都提供了依赖性升级和值得注意的变更，包括：新的ModifiedClassPathClassLoader类，从而能够与Spring Boot版本同步；修复了缺陷，即在第一个步骤关闭完成之前，就关闭步骤注册表，将会导致重复发布数据。关于这两个版本的更多细节，请参阅<a href=\"https://github.com/micrometer-metrics/micrometer/releases/tag/v1.12.1\">1.12.1版本</a>\"和<a href=\"https://github.com/micrometer-metrics/micrometer/releases/tag/v1.11.7\">1.11.7版本</a>\"的发布说明。</p><p>&nbsp;</p><p>类似的，<a href=\"https://github.com/micrometer-metrics/tracing/blob/main/README.md\">Micrometer Tracing</a>\"的1.2.1和1.1.8版本都提供了依赖性升级，并解决了在Observation接口中的内部接口Event中，getWallTime()方法的默认值返回0的问题，该方法在上传span时会导致后端失败。关于这两个版本的更多细节，请参阅<a href=\"https://github.com/micrometer-metrics/tracing/releases/tag/v1.2.1\">1.2.1版本</a>\"和<a href=\"https://github.com/micrometer-metrics/tracing/releases/tag/v1.1.8\">1.1.8版本</a>\"的发布说明。</p><p></p><h4>Eclipse Vert.x</h4><p></p><p>Eclipse&nbsp;<a href=\"https://vertx.io/\">Vert.x</a>\"的4.5.1版本<a href=\"https://vertx.io/blog/eclipse-vert-x-4-5-1/\">发布</a>\"，带了一些值得关注的变化，包括：由于JDK 22的变更，在解析PostgreSQL时间戳时，从Locale.ROOT切换到了Local.US；解决了当HTTP/1.1缺少主机头信息时，ForwardedParser类中出现NullPointerException的问题；新的@JsonGen注解，它将替换@DataObject注解以触发转换器的生成。关于该版本的更多细节，请参阅<a href=\"https://github.com/vert-x3/wiki/wiki/4.5.1-Release-Notes\">发布说明</a>\"以及对<a href=\"https://github.com/vert-x3/wiki/wiki/4.5.1-Deprecations-and-breaking-changes\">废弃功能和破坏性变更</a>\"的介绍。</p><p></p><h4>JHipster</h4><p></p><p><a href=\"https://www.jhipster.tech/\">JHipster</a>\"&nbsp;8.1.0版本<a href=\"https://twitter.com/deepu105/status/1734199476527464455\">发布</a>\"，包含了缺陷和依赖性升级，并增加了新特性，例如，使用会话端点元数据进行OAuth注销；重构&nbsp;CustomClaimConverter类中的授权头信息。关于这个版本的更多细节，请参阅JHipster 8.0的<a href=\"https://www.jhipster.tech/2023/12/11/jhipster-release-8.1.0.html\">发布说明</a>\"和InfoQ关于JHipster 8.0的<a href=\"https://www.infoq.com/news/2023/12/jhipster-version8-release/\">新闻</a>\"。</p><p></p><h4>Project Reactor</h4><p></p><p><a href=\"https://github.com/reactor/reactor/blob/main/README.md\">Project Reactor</a>\"&nbsp;2023.0.1是第一个维护版本，提供了对reactor-core 3.6.1、reactor-netty 1.1.14和reactor-pool 1.0.4的依赖性升级。在2023.0.1版本中，reactor-kafka 1.3.22、reactor-addons 3.5.1和reactor-kotlin-extensions 1.2.2制品没有变化。关于该版本的更多细节，请参阅<a href=\"https://github.com/reactor/reactor/compare/2023.0.0...2023.0.1\">变更日志</a>\"。</p><p>&nbsp;</p><p>与之类似，Project Reactor 2022.0.14（<a href=\"https://github.com/reactor/reactor/releases/tag/2022.0.14\">第14个维护版本</a>\"）提供了对reactor-core 3.5.13、reactor-netty 1.1.14和reactor-pool 1.0.4的依赖性升级。在2022.0.14版本中，reactor-kafka 1.3.22、reactor-addons 3.5.1和reactor-kotlin-extensions1.2.2制品没有变化。关于该版本的更多细节，请参阅<a href=\"https://github.com/reactor/reactor/compare/2022.0.13...2022.0.14\">变更日志</a>\"。</p><p></p><h4>Apache软件基金会</h4><p></p><p><a href=\"https://tomcat.apache.org/\">Apache Tomcat</a>\"的<a href=\"https://www.mail-archive.com/announce@apache.org/msg08753.html\">11.0.0-M15</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg08752.html\">10.1.17</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg08754.html\">9.0.84</a>\"和<a href=\"https://www.mail-archive.com/announce@apache.org/msg08751.html\">8.5.97</a>\"版本都修复了缺陷，并提供了值得注意的变更，例如，当容器的生命周期操作正在进行时，容器的后台进程不再执行；修正了<a href=\"http://www.webdav.org/\">WebDAV</a>\"响应会出现意料之外的XML转义的问题；在HTTP请求处理时，如果发生读取超时的话，使用<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/408\">HTTP 408</a>\"状态码“Request Timeout”而不是<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\">HTTP 400</a>\"状态码“Bad Request”。关于这些版本的更多细节，请参阅<a href=\"http://tomcat.apache.org/tomcat-11.0-doc/changelog.html\">11.0.0-M15版本</a>\"、<a href=\"http://tomcat.apache.org/tomcat-10.1-doc/changelog.html\">10.1.17版本</a>\"、<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/changelog.html\">9.0.84版本</a>\"和<a href=\"https://tomcat.apache.org/tomcat-8.5-doc/changelog.html\">8.5.97版本</a>\"的变更日志。</p><p>&nbsp;</p><p><a href=\"https://maven.apache.org/\">Apache Maven</a>\"&nbsp;4.0.0的<a href=\"https://www.mail-archive.com/announce@apache.org/msg08756.html\">第9个alpha版本</a>\"提供了一些值得注意的变化，例如，依赖升级到Maven Resolver 2.0.0-alpha-3；提供了多线程的map/reduce算法来并行解析冗长的reactor模型；当需要注入bean而会话作用域还不可用时，@SessionScoped注解现在将创建代理来包装bean。关于该版本的更多细节，请参阅<a href=\"https://maven.apache.org/docs/4.0.0-alpha-9/release-notes.html\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://camel.apache.org/\">Apache Camel</a>\"的<a href=\"https://camel.apache.org/blog/2023/12/RELEASE-3.21.3/\">3.21.3</a>\"和<a href=\"https://camel.apache.org/blog/2023/12/RELEASE-3.20.9/\">3.20.9</a>\"版本都对依赖性进行了升级，并修复了一些值得关注的缺陷，例如，在通过multipart启动大文件上传时出现的OutOfMemoryError；EndpointDslMojo类中定义的addHeaderNameMethod()方法生成错误的头信息名；<a href=\"https://camel.apache.org/camel-k/2.1.x/index.html\">Apache Camel K</a>\"的Kubernetes secret配置没有按照预期方式运行。关于这两个版本的更多细节，请参阅<a href=\"https://camel.apache.org/releases/release-3.21.3/\">3.21.3版本</a>\"和<a href=\"https://camel.apache.org/releases/release-3.20.9/\">3.20.9版本</a>\"的发布说明。</p><p></p><h4>Piranha</h4><p></p><p><a href=\"https://piranha.cloud/\">Piranha</a>\"发布<a href=\"https://github.com/piranhacloud/piranha/releases/tag/v23.12.0\">23.12.0版本</a>\"，提供了值得关注的变更，包括：在Payara Web Profile中支持CRaC；将Docker文件更新到JDK 21；将依赖升级到Spring Boot 3.1.6。关于此版本的更多细节，请参阅其<a href=\"https://javadoc.io/doc/cloud.piranha/project/latest/index.html\">文档</a>\"和<a href=\"https://github.com/piranhacloud/piranha/issues?q=is%3Aissue+-label%3Awontfix+milestone%3A23.12.0+is%3Aclosed\">问题跟踪器</a>\"。</p><p></p><h4>OpenXava</h4><p></p><p><a href=\"https://openxava.org/\">OpenXava</a>\"&nbsp;7.2.1<a href=\"https://openxava.org/blog/openxava-7.2.1-released\">发布</a>\"，提供了依赖性升级和值得关注的缺陷修复，例如，XSTL依赖所引发的远程代码执行漏洞；在@Coordinates中使用@OnChange&nbsp;action无法按照预期方式运行；忽略掉了@Tree注解中所定义的idProperties属性。关于此版本的更多细节，请参阅<a href=\"https://github.com/openxava/openxava/releases/tag/7.2.1\">发布说明</a>\"。</p><p></p><h4>Payara虚拟会议</h4><p></p><p>首届<a href=\"https://www.crowdcast.io/c/virtualpayaraconference\">Payara虚拟会议</a>\"举行，该会议为期一天，由顶级行业分析师、Java Champions和Jakarta EE专家参加。与会者还从Payara首席执行官<a href=\"https://www.linkedin.com/in/smillidge/\">Steve Millidge</a>\"那里了解了更多关于Payara Platform 2024路线图的信息。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/12/java-news-roundup-dec11-2023/\">&nbsp;Java News Roundup: JHipster 8.1, Piranha Cloud 23.12, Open Liberty 23.0.0.12, Tomcat Releases</a>\"</p>",
    "publish_time": "2024-01-04 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "以 Git 为数据源、具备版本控制的数据源 Dolt 新增了 PostgreSQL 风味",
    "url": "https://www.infoq.cn/article/4862sxkDtGxglAojD3Bd",
    "summary": "<p><a href=\"https://www.dolthub.com/blog/2023-11-01-announcing-doltgresql/\">DoltgreSQL</a>\" 以版本控制数据库 Dolt 为基础构建，为 Postgre 数据库模式及数据提供类 Git 的日志、差异、分支及合并功能。</p><p>&nbsp;</p><p>Dolt 作为 SQL 数据库，允许用户像是 Git 代码库一样进行克隆、fork、分支及合并。通过 Dolt，应用程序开发者可以为用户创建分支，合并工作流，比如发送 pull 请求修复数据中的错误。同理，Dolt 可以通过数据库分支、变更应用，在暂存环境中测试，并最终部署回生产环境的这种简单模型修改生产数据库。</p><p>&nbsp;</p><p>Dolt 从创建之初就采用了 MySQL 的语法和<a href=\"https://docs.dolthub.com/cli-reference/git-comparison\">面向命令行的范式</a>\"，Git 用户对此必然不会感到陌生。</p><p>&nbsp;</p><p>DoltgreSQL 专注于数据库服务体验，提供可定制且易于部署的服务器。此外，该公司表示这款数据库不提供命令行支持，以更好地与 PostgreSQL 的一般用户体验保持一致。</p><p>&nbsp;</p><p></p><blockquote>DoltgreSQL 的工作原理是模拟 PostgreSQL 服务器，将接收到的命令转化为 AST 后提供给底层的 Dolt 服务器。如此一来便能实现快速的启动和运行，同时还可利用 Dolt 已提供的能力和功能。</blockquote><p></p><p>&nbsp;</p><p>这种在 Dolt 基础之上的构建新功能的优势在于可借助后者的稳定性和可靠性，减少开发的范围和工作量。</p><p>&nbsp;</p><p>DoltHub 称他们研究了不同的方式，其中包括编写外来数据包装器、构建全新 PostgreSQL 存储后端，甚至是 fork PostgreSQL 本身。这些方式中有些存在太大的局限性，有些（如 fork PostgreSQL）则需要数年之久的开发时间。</p><p>&nbsp;</p><p>至于负面方面，这种仿 Git 方法的缺点在于其无法运行实际的 PostgreSQL 二进制文件。正如前文所述，DoltgreSQL 是将 PostgreSQL 语法转换为 AST 表示法，并在 Dolt 层中运行。</p><p>&nbsp;</p><p>在完成安装 DoltgreSQL 之后，用户可以使用 psql 命令行客户端连接到数据库。若要查询数据库状态，则可运行这行语句：</p><p><code lang=\"java\">select * from dolt_status;</code></p><p>这行语句会列出所有现存表，并指定这些表为新表或暂存表等等。若要将一个表添加到暂存区域，则可运行这行语句：</p><p><code lang=\"null\">call dolt_add('my_table_name');</code></p><p>若要提交变更，则运行：</p><p><code lang=\"java\">call dolt_commit('-m', 'updated schema');</code></p><p>而&nbsp;select * from dolt_log;语句则是等同于 git log。</p><p>&nbsp;</p><p>Doltgres 仍处于试验阶段且存在一些<a href=\"https://github.com/dolthub/doltgresql#limitations\">限制情况</a>\"，其中包括不支持 DoltHub 和 DoltLab、没有身份验证或用户管理、对 SSL 连接的支持有限、不支持复制、群集等。</p><p>&nbsp;</p><p>虽然 Dolt 的“数据版 Git”这一价值主张听起来很有吸引力，但数据库专家 <a href=\"http://www.jandrewrogers.com/about/\">J. Andrew Rogers</a>\"&nbsp;在 Hacker News 上指出，这一目标与<a href=\"https://news.ycombinator.com/item?id=31852067\">多版本并发控制（MVCC）几十年来的尝试并无二致</a>\"，而且还存在几个重要缺点。Dolt 首席执行官 Tim Sehn 强调，<a href=\"https://docs.dolthub.com/sql-reference/benchmarks/latency\">与原生 MySQL 在 sysbench 基准的运行相对比</a>\"，Dolt 仅比 MySQL 略慢一点。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/11/DoltgreSQL-git-for-data-postgres/\">Git-for-Data, Version-Controlled Database Dolt Gets PostgreSQL-Flavor</a>\"</p>",
    "publish_time": "2024-01-04 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Redis之父亲自上手用大模型撸代码：通晓古今的白痴队友，将来可以取代99%程序员",
    "url": "https://www.infoq.cn/article/s2f71m8cQVqRUEdlLegx",
    "summary": "<p></p><p></p><blockquote>Redis 创始人 antirez&nbsp;写下了自己2024年的第一篇博文，他从一名普通程序员的角度谈了谈对大语言模型的感受，虽然他的成就并不普通。他在文章里犀利评价Google引擎已经成为垃圾的海洋，并客观评价了现在的AIGC能力：愚蠢但通晓古今。&nbsp;通过长期使用，他认为现阶段的生成式AI只会让已经很强的程序员变得更强。目前大多数编程任务都是在重复工作，根本不需要大模型有太高的推理水平，大模型很适合那些“用完就扔”的程序。我们对antirez&nbsp;的博文进行了翻译，并在不改变作者原意基础上进行了一些删减。</blockquote><p></p><p>&nbsp;&nbsp;</p><p>自从ChatGPT横空出世以来，包括后面以本地方式运行的各种大模型，生成式AI已然得到了广泛应用。我个人的目的一方面是想依靠大模型提高编码能力，另外还希望把宝贵的精力从繁琐且价值有限的工作中解放出来。相信很多朋友也像我一样，花费了无数时间搜索没什么启发性的技术文档、被迫学习各种过于复杂的API、编写过短时间内就沦为垃圾的程序。工作不该是这样的，开发也不该是这样的。现如今，Google引擎已经成了垃圾的海洋，我们得费尽心思才能在其中找到一点有用的内容。</p><p>&nbsp;</p><p>另外，我本人并不是编程新手。哪怕不借助任何外部资源，我也能够编写代码，甚至可以说具备一定开发水平。只是随着时间推移，我开始越来越多地用大模型协助编写高级代码：Python代码最多，但在C语言中则应用较少。</p><p>&nbsp;</p><p>大语言模型最让我印象深刻的一点，就是我能准确意识到何时可以使用、而哪些情况下盲目使用只会拖慢进度。我还发现，大模型其实很像维基百科和YouTube上的各种视频课程：对于有意愿、有能力、更自律的使用者来说效果拔群，但对本就业务能力不足的朋友来说则边际收益递减。所以我很担心，至少在现阶段，生成式AI只会让已经很强的程序员变得更强。</p><p>&nbsp;</p><p>下面让我们一步步开始讨论。</p><p>&nbsp;</p><p></p><h1>大语言模型：全知全能还是鹦鹉学舌？</h1><p></p><p>&nbsp;</p><p>机器学习新浪潮中最令人忧心的现象之一，就是AI专家对于大模型的认知还相当有限。我们虽然发明了神经网络，但在实质上发明的仅仅是一种自动优化神经网络参数的算法。硬件已经能够训练出越来越大的模型，并使用提取自待处理数据（先验素材）的统计知识，再通过大量迭代试验排除错误、逼近正确答案。必须承认，大模型确实要比以往其他架构效果更好。但总体来讲，神经网络本身仍然极不透明。</p><p>&nbsp;</p><p>由于无法解释大模型为何具备某些新兴能力，预计科学家们的态度将更趋谨慎。但在另一个极端上，也有不少人都严重低估了大语言模型，认为它们只不过是某种更先进的马尔可夫链，最多只能重现在训练集中见到过的有限变化。但大量事实证据表明，这种大模型只是在“鹦鹉学舌”的理论根本站不住脚。</p><p>&nbsp;</p><p>也有不少热心群众觉得大语言模型获得了某种实际上不存在的超自然力量。没那么玄乎，大模型最多只能对自己在训练期间接触过的数据表示空间进行插值，而这并不是什么新鲜成果。而且哪怕单论插值，其能力也相当有限（但足以超出人类预期，甚至带来惊喜）。如果能够更进一步，在接触过的所有代码围成的空间当中进行连续插值，那么大模型哪怕无法创造出真正新奇的事物，也足以取代99%的程序员。</p><p>&nbsp;</p><p>好在现实没这么夸张，我们开发者们仍有生存的空间。大语言模型确实能编写出自己没有原样接触到的程序形式，也表现出通过融合训练集内不同出现频率的思路来引导开发方向的初步能力。只是这种能力目前还存在很大的局限性，而种种微妙的推理任务总会令大语言模型遭遇灾难性的失败。但必须承认，大语言模型已经代表着AI技术从诞生至今最伟大的成就，这一点应该成为所有讨论的前提。</p><p>&nbsp;</p><p></p><h1>既愚蠢，却又通晓古今</h1><p></p><p>&nbsp;</p><p>此言不假：大语言模型最多只能进行最基本的推理，这种推理还不够准确，很多时候充满了事实层面的幻觉和捏造。但它们同样拥有着渊博的知识。</p><p>&nbsp;</p><p>以编程领域及其他能够获取高质量数据的场景为例，大模型就像那种通晓古今的愚蠢学者。与这样的合作伙伴进行结对编程并不明智（当然，在我看来哪怕是跟人做结对编程也不明智）：它们往往会抛出荒谬的想法，而我们则需要在开发中不断努力强调自己的思路。</p><p>&nbsp;</p><p>但反过来，如果把这个博学的傻瓜当成可支配的工具、由它提出问题以作为我们激发灵感的素材，那么效果将完全不同。目前的大模型还无法引领人类跨越知识的鸿沟，但如果我们想解决某个自己不太熟悉的问题，它们往往可以帮助我们从一无所知快速前进到具备完全自学能力的程度。</p><p>&nbsp;</p><p>在编程领域，之前二、三十年间的程序员们可能对大模型的这种能力评价不高。毕竟那时候我们只需要掌握几种编程语言、特定的经典算法和那十来套基础库，余下的就纯粹是自我表达、发挥才智、运用专业知识和设计技能的部分了。只要拥有这种能力，我们就是当之无愧的专业程序员，具备了解决一切难题的潜质。</p><p>&nbsp;</p><p>但随着时间推移，各种框架、编程语言和库开始轮番上阵，爆发式的增长令开发难度激增，也给程序员的日常工作带来了既无必要、又不合理的诸多困扰。在这样的现实和背景之下，大模型这样一位通晓古今的白痴队友就成了最宝贵的前进指引。</p><p>&nbsp;</p><p>举个例子：我自己的机器学习实验在整整一年间都是靠Keras完成的。后来出于种种原因，我转而使用PyTorch。当时我已经学习了什么叫嵌入和残差网络，但我实在不想逐字逐句去研究PyTorch文档（当初我在学Keras时就是这么硬啃下来的，如果能有ChatGPT肯定可以帮我回避很多痛苦的回忆）。如今有了大语言模型，我可以非常轻松地编写出使用Torch的Python代码，唯一的前提就是对想要组合的模型拥有清晰的思路、同时能够提出正确的问题。</p><p>&nbsp;</p><p></p><h1>用案例说话</h1><p></p><p>&nbsp;</p><p>请注意，我这里说的可不是那些简单的需求，比如“X类是怎么实现Y的？”如果只是这类场景，那大语言模型的作用其实相当有限，甚至可以说跟搜索引擎和技术论坛区别不大。相反，复杂模型能做到的要多得多，包括那些短短几年前我们还无法想象的功能。</p><p>&nbsp;</p><p>现在我可以告诉GPT-4：“看看，这是我在PyTorch实现的神经网络模型。这些是我设置的批任务。我想调整张量大小，保证批函数与神经网络的输入相兼容，同时想以这种特定方式来表示。你能告诉我需要怎样的代码进行重写吗？”提示完成之后，GPT-4就会编写代码，而我要做的就是在Python CLI中测试张量结果的维度是否满足需求、数据布局是否正确。</p><p>&nbsp;</p><p>再来看另一个例子。前段时间，我需要为某些基于ESP32的设备开发BLE客户端。经过一番研究，我发现多平台蓝牙编程绑定大多无法直接使用，而解决方案非常简单，使用macOS的本机API在Objective C中编写代码即可。这就要求我同时处理两个问题：学习Objective C那繁琐的BLE API，适应种种毫无意义的模式（我属于那种极简主义者，而Objective C的BLE&nbsp;API绝对是“优秀设计”的典型反例）；同时学会如何用Objective C编程。我上次用它编程还是在十年之前，如今早就忘了事件循环、内在管理等技术细节。</p><p>&nbsp;</p><p>最终结果就是以下代码，虽然不够优雅简洁，但至少能够正常起效。在大模型的帮助下，我只用了很短时间就完成了开发，这在以往根本就无法想象：</p><p>&nbsp;</p><p><a href=\"https://github.com/antirez/freakwan/blob/main/osx-bte-cli/SerialBTE.m\">https://github.com/antirez/freakwan/blob/main/osx-bte-cli/SerialBTE.m</a>\"</p><p>&nbsp;</p><p>这些代码主要由ChatGPT生成，而我的工作就是把自己想做、但不太确定要怎么实现的要求粘贴进去。如此一来，大模型就能向我做出解释，包括问题的实质是什么、应当如何解决。</p><p>&nbsp;</p><p>的确，大模型并没有实际编写多少代码，但却帮助我显著加快了开发速度。如果没有ChatGPT，我能不能把项目做下来？当然也行，但最重要的并不是我要额外投入多少时间，而是我可能干脆就放弃了：毕竟这么麻烦的事情，已经不值得我浪费精力。</p><p>&nbsp;</p><p>在我看来，这才是真正决定性的因素。如果没有大模型，我在衡量工作量和收益之后压根不会编写这样一个程序。大模型甚至还帮我完成了一项比程序本身更重要的调整：在项目中，我修改了linenoise（我使用的行编辑库）以使其能在多路复用中生效。</p><p>&nbsp;</p><p></p><h1>即抛型程序</h1><p></p><p>&nbsp;</p><p>像前文提到的这类案例还有很多，这里就不再过多重复了，毕竟类似的故事基本都是一样的套路和效果。在日常工作中，我还经常面临另一类问题，就是想要快速获得某些可以验证的成果。在这种情况下，同样可以使用大模型来提升探索效率。</p><p>&nbsp;</p><p>对于此类场景，我往往会让大模型负责编写所有代码。例如，当我需要编写某些即抛型程序时，比如下面这个：</p><p><a href=\"https://github.com/antirez/simple-language-model/blob/main/plot.py\">https://github.com/antirez/simple-language-model/blob/main/plot.py</a>\"</p><p>&nbsp;</p><p>我想要对小型神经网络学习过程中的损失曲线进行可视化，因此向GPT-4展示了PyTorch程序生成的CSV文件格式，然后提出如果我在命令行内指定多个CSV文件，希望能对不同实验所验证的损失曲线进行比较。而以上链接就是GPT-4生成的结果，前后只用了短短30秒。</p><p>&nbsp;</p><p>同样的，我还需要一个程序来读取AirBnB CSV报告，并按月份和年份对各处公寓进行分组。之后，结合清洁费用以及单次预订的住宿天数，由它来统计一年中不同月份的平均租金价格。这款程序对我来说确实有用，但编写过程又极其无聊：因为里面根本没什么新奇有趣的功能。于是乎，我选取了一部分CSV文件并粘贴进GPT-4当中，之后描述了一下希望大模型解决的问题。输出的程序一次运行成功。但我们自己得正确理解具体的数据分组方式，否则会感觉这些数据既分散又无序。</p><p>&nbsp;</p><p>通过简单的推理，我认为大模型绝对不是简单从接触过的训练素材中照搬来的解决方案。没错，GPT-4在训练期间肯定观察到过类似的程序，只是这些程序所对应的具体分组要求跟我的提示有所不同，特别是要求分组成特定格式的CSV文件。所以在我看来，大模型应该能在一定程度上对训练集中不同程序描述的空间进行插值。</p><p>&nbsp;</p><p>让我自己浪费时间编写这类简单程序实在是不太明智。事实证明大模型可以承接此类任务，帮助我将精力集中在真正重要的工作上，这无疑变相提高了我的代码生产效率。</p><p>&nbsp;</p><p></p><h1>大模型搞不定的典型任务：系统编程</h1><p></p><p>&nbsp;</p><p>虽然我的大模型编程尝试取得了不小的成功，但在使用C语言编写程序时，我发现大模型更多只能作为便携的文档记录助手。我本人是系统编程方面的专家，在这类用例中，大模型由于缺乏复杂的推理能力而几乎帮不上什么忙。相信各位朋友也有类似的感受。</p><p>&nbsp;</p><p>下面我们一起来看这段实验性的提示词：</p><p>&nbsp;</p><p>“为bloom过滤器生成一条优雅、短小且有效的C语言实现。应重点关注哈希函数处理，然后用高质量C语言进行编写。另须考虑，这条实现的大小应可存储10万个元素，误报概率不得超过5%。添加的元素是以null结尾的字符串。“</p><p>&nbsp;</p><p>GPT-4给出的答案说不上好。Bloom过滤器其实相当普遍，涉及的数据结构也不特殊。但很明显，编写一个像样的bloom过滤器需要更强大的抽象能力：例如找到一种有效方法对同一字符串进行N次哈希处理，并确保各哈希值得到充分的去相关处理。如果换个思路，明确要求GPT-4修改哈希函数，使其产生N个去相关输出，那么它给出的解决方案就靠谱多了。如果它能自己发现这个思路，就会以不同的方式编写bloom过滤器，使用单个哈希函数一次设置K个bits。</p><p>&nbsp;</p><p>事实就是，GPT-4能够独立编写出适当也更加通用的哈希函数，但在编写bloom过滤器这类更大的项目时，它却未能表现出良好的推理能力，而是给出了两个不同但却高度相似的哈希函数。</p><p>&nbsp;</p><p>总而言之，当前大语言模型的推理能力仍然孱弱，再加上关于这个问题的资源可能比较稀少，甚至存在大量低质量资源，于是导致其给出的结果不尽如人意。而且这绝不是孤立的案例，我还多次尝试在算法或系统编程当中使用大模型，结果也非常差。哪怕是下调对推理能力的预期，它也没法重现Python编程环境中的代码生成水平。</p><p>&nbsp;</p><p>但与此同时，GPT-4能够反编译它所输出的函数（需要通过单独的会话），也能准确理解这样做的意义，因此，大模型在系统编程场景下还是具有一定作用的，只是非常有限。</p><p>&nbsp;</p><p>另一个有趣且令人期待的点，是在上述情况下，较小模型和较大模型间的表现有着显著差异。</p><p>&nbsp;</p><p>虽然Mixtral是一套适合多种用途的优秀模型，但考虑到大模型本就孱弱的推理能力，目前能够总结出的规律明显是体量越大、效果越好。另外，本地模型deepseek-coder设置为4 bits量化精度，因为本地设备的内存不足以在更高的精度上运行模型。哪怕如此，凭借340亿参数，它在同一问题上的推理能力还是更强一些。</p><p>&nbsp;</p><p>在尝试中，我给出了关于问题的解决线索，而模型则正确得出了答案、确定了引发问题的真正根源，并最终给出了行之有效的替代方案。这类应用在任何文档、书籍或者Google搜索中都没有直接答案。</p><p>&nbsp;</p><p>无论是从原始插值的角度、还是其他思路来看，模型都已经掌握了某种形式的推理能力。也只有借助这份推理能力，AI才能找到问题的根源并发现潜在的解决方案。所以我觉得没必要再争论了，大语言模型对于程序员们确实具备积极的辅助意义。</p><p>&nbsp;</p><p>但与此同时，过去几个月间的使用体验表明，在系统编程领域、特别是对于经验丰富的程序员们，大模型几乎无法给出任何可以拿来就用的解决方案。</p><p>&nbsp;</p><p>我目前负责的ggufflib项目要求编写一个读取和写入GGUF格式文件的库，也就是llama.cpp加载量化模型的格式。最初，为了理解量化编码的工作原理，我尝试使用过ChatGPT，但最后还是决定对llama.cpp的代码进行逆向工程——这样速度还更快些。</p><p>&nbsp;</p><p>理想中的大语言模型应该能根据接触到的数据编码“结构”声明和解码函数，还原出关于数据格式的说明文档，借此帮助系统程序员理解设计思路。可虽然llama.cpp的函数不大，完全可以塞进GPT-4的上下文窗口，但输出的结论却毫无意义。</p><p>&nbsp;</p><p>对于这类情况，我们就只能像最传统的程序员那样：掏出纸和笔，一行行阅读代码，查看解码器提取的bits在哪里注册。</p><p>&nbsp;</p><p></p><h1>正确看待大语言模型</h1><p></p><p>&nbsp;</p><p>虽然怀着深深的遗憾，但我不得不承认：目前大多数编程任务都是在以略有不同的形式重复着相同的工作，根本不需要太高的推理水平。而大语言模型在这方面表现出色，只是仍然受到上下文规模的硬性约束。</p><p>&nbsp;</p><p>而这也应当引起我们程序员的思考：这样的程序，真值得我们花费时间和精力动手编写吗？没错，这活能给我们带来相当丰厚的报酬，但如果大语言模型逐渐接手了这部分任务，那么五年、最多不超过十年，就会有很多程序员同行丢掉饭碗。</p><p>&nbsp;</p><p>再有，大语言模型到底具不具备一定程度的推理能力，还是说仍然是在鹦鹉学舌、只是学得更加惟妙惟肖？我认为某些情况下它们确实具备推理能力，也就是掌握了符号学家们所说的“能指”概念，即实质上并不存在的意义。</p><p>&nbsp;</p><p>相信每一位跟大模型经常打交道的朋友，都能在理解它们局限性的同时，感受到其中体现出的推理之力：它们对以往接触过的内容的融合能力，远远超出了随机输出单词的范畴。尽管其学习过程主要是在预训练阶段完成，但在预测下一个token时，大模型还是会根据目标建立起某种形式的抽象模型。这个模型虽然还很脆弱、不够完备和完美，但通过实际观察，我们会意识到这种能力的客观存在。正所谓耳听为虚、眼见为实，哪怕可能挑战数学专业的确定性原理、与最伟大的技术专家观点相背，我也仍然对大模型表现出的认知水平抱有信心。</p><p>&nbsp;</p><p>最后，希望大家能够积极拥抱大模型，尝试用它解决编程中的各种问题。向大模型提出正确问题将成为一项基础性的开发技能，而且演练的次数越多，AI就越是能更好地改进工作效果。哪怕不考虑AI因素，这种明确清晰的问题描述能力也有助于我们更好地跟他人沟通。毕竟大语言模型并不是唯一跟不上我们思维过程的会话对象。相信大家也有体会，很多程序员虽然在自己的特定领域里非常出色，但沟通能力却很差，这也成为限制其职业发展的瓶颈。</p><p>&nbsp;</p><p>现如今的Google引擎已经稀烂，所以哪怕是从浓缩和提炼文本内容的角度，大模型也肯定具备巨大的现实意义。就个人而言，我也将继续使用大模型、了解大模型。我向来不喜欢学习晦涩的通信协议细节，也不愿跟那些炫技式的复杂库编写方法打交道。对我来说，这些只是白白浪费时间和精力的“垃圾知识”。感谢大语言模型，把我从这些曾经的泥潭当中解救出来。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"http://antirez.com/news/140\">http://antirez.com/news/140</a>\"</p><p>&nbsp;</p>",
    "publish_time": "2024-01-04 14:05:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]