[
  {
    "title": "JEP 447 已发布，可在构造函数的 super()调用之前执行语句",
    "url": "https://www.infoq.cn/article/wXOMDKrJ1W6SiDUJp27i",
    "summary": "<p>在评审结束后，JEP 447（super() 前置语句 (预览)） 已在 JDK 22 中交付。该 JEP 来自 Project Amber 项目，提议允许在构造函数的 super() 调用之前出现不引用正在创建的实例的语句，并保留构造函数现有的安全性和初始化保证。Oracle 技术顾问成员 Gavin Bierman 为 Java 社区提供了这个 JEP 的初始规范，供社区评审并提供反馈意见。</p><p></p><p>传统上，要求 Java 构造函数将对另一个构造函数的显式调用作为第一条语句。这个约束确保了自上而下的执行顺序，并防止对未初始化字段的访问，极大地限制了构造函数逻辑的表达性和可读性。考虑下面的例子：</p><p></p><p><code lang=\"cs\">public class PositiveBigInteger extends BigInteger {\n\n    public PositiveBigInteger(long value) {\n        super(value);               // Potentially unnecessary work\n        if (value &lt;= 0)\n            throw new IllegalArgumentException(\"non-positive value\");\n    }\n}\n</code></p><p></p><p>通过在调用超类构造函数之前验证其参数来声明快速失败的构造函数会更好。JEP 447 放宽了这些限制，允许在显式构造函数调用之前出现不引用正在创建的实例的语句。有了这个功能，上面的代码可以简化为：</p><p></p><p><code lang=\"cs\">public class PositiveBigInteger extends BigInteger {\n\n    public PositiveBigInteger(long value) {\n        if (value &lt;= 0)\n            throw new IllegalArgumentException(\"non-positive value\");\n        super(value);\n    }\n}\n</code></p><p></p><p>考虑另一种情况，比如子类构造函数需要为超类构造函数准备参数。以前，由于超类构造函数调用必须作为第一条语句，这就需要使用辅助方法。</p><p></p><p><code lang=\"java\">public class SubClass extends SuperClass {\n    public SubClass(Certificate certificate) {\n        super(prepareByteArray(certificate));\n    }\n\n    private static byte[] prepareByteArray(Certificate certificate) {\n        // Logic to prepare byte array from certificate\n        // ...\n        return byteArray;\n    }\n}\n</code></p><p></p><p>在这个例子中，prepareByteArray 方法在将 Certificate 对象传递给 SuperClass 构造函数之前对它进行处理。有了 JEP 447，这个过程变得更加简洁和直观。</p><p></p><p><code lang=\"java\">public class SubClass extends SuperClass {\n    public SubClass(Certificate certificate) {\n        // Directly include the logic to prepare byte array\n        PublicKey publicKey = certificate.getPublicKey();\n        if (publicKey == null) {\n            throw new IllegalArgumentException(\"Null certificate\");\n        }\n        byte[] byteArray = switch (publicKey) {\n            case RSAPublicKey rsaKey -&gt; rsaKey.getEncoded();\n            case DSAPublicKey dsaKey -&gt; dsaKey.getEncoded();\n            default -&gt; throw new UnsupportedOperationException(\"Unsupported key type\");\n        };\n        super(byteArray);\n    }\n}\n</code></p><p></p><p>在这个新的例子中，SubClass 的构造函数直接包含处理 Certificate 对象的逻辑。这种直接的方法增强了可读性，减少对使用辅助方法的需求，展示了 JEP 447 在实际应用场景中的好处。</p><p></p><p>JEP 447 不仅提供了更大的灵活性，还保留了构造函数行为的基本保证，确保子类构造函数不会干扰超类的实例化。这个更新不需要对 Java 虚拟机（JVM）做任何修改，仅依赖 JVM 现有的能力来验证和执行构造函数调用之前的代码。</p><p></p><p>随着 Java 的不断发展，JEP 447 清楚地表明了 Java 在不断适应现代编程实践。它反映了在引入新特性和保持 Java 生态系统健壮性之间的平衡。对于 Java 开发者来说，这意味着有机会探索更高效的编码实践，同时仍然保持对编程语言的核心原则。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2024/01/java-constructors-enhancement/\">https://www.infoq.com/news/2024/01/java-constructors-enhancement/</a>\"</p>",
    "publish_time": "2024-01-29 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2024 年 7 个 Web 前端开发趋势",
    "url": "https://www.infoq.cn/article/28AAMVMBA16EXo2ux5QO",
    "summary": "<p>希腊哲学家赫拉克利特认为，变化是生命中唯一不变的东西。这句话适用于我们的个人生活、行业和职业领域。</p><p></p><p>尤其是前端开发领域，新技术、开发趋势、库和框架不断涌现，变化并不陌生。最近发生的一些事件正在改变开发人员构建网站和 Web 应用的方式，其中包括支持服务器端渲染的 Angular 17 发布、Next.js v14 发布以及 TypeScript 采用增加。</p><p></p><p>虽然跟上前端生态系统的变化可能具有挑战性，但作为专业人士，保持技能的更新有助于我们开展工作。想知道明年前端领域会发生什么变化吗？那就请继续阅读吧。</p><p></p><h2>趋势一：新的样式解决方案和组件库将持续涌现</h2><p></p><p></p><p>在 Web 网站样式方案的选择上，开发人员可谓是富得流油。除了大量的基于 Angular、React 和 Vue 的组件库之外，还有 40 种以上的 CSS 框架 和 40 种以上的 CSS-in-JS 库 可以选择。</p><p></p><p>2023 年，我们看到了诸如 Shadcn UI、Ark UI、Panda CSS 和 StyleX（来自 Meta）等新发布的样式解决方案。除此之外，还有 Lemon Squeezy 开源的 React UI 库 Wedges，该库在本文撰写的前几天才刚刚发布。</p><p></p><p>随着开发人员和开源社区不断分享他们在 UI 设计和网站构建上的独特方法，将来我们有望看到更多的网站样式解决方案的出现。</p><p></p><p>除了新的样式解决方案会不断发布之外，未来以下这些方面也值得我们期待：</p><p></p><p>现有解决方案的持续更新。CSS-in-JS 解决方案将被抛弃，因为该方案不仅会增加运行时开销、构建包的大小，还无法很好地与 SSR 配合使用。Open Props 将取代 Tailwind CSS 的宝座。根据 《2023 年 CSS 现态》 这篇文章的数据，开发人员对 Tailwind CSS 的兴趣值从 2022 年的 50.1% 下降到了 2023 年的 47%。与此同时，Open Props 的关注度从 2022 年的不到 10% 上升到了 2023 年的 60%。</p><p></p><h2>趋势二：利用 AI 来增强开发流程</h2><p></p><p></p><p>毫不夸张地说，生成式 AI 已经在全球的许多行业中掀起了风暴。它也将彻底改变 Web 开发的现状和众多开发人员构建网站的方式。</p><p></p><p>Vercel 发布的 v0 就是一个活生生的例子，它是一个基于提示词生成用户界面的工具。例如，我们可以通过提示词让 v0 创建一个电子商务风格的 dashboard 页面。下图就是 v0 的输出结果。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/cb/cbc4c2552a850c8bde4a0e5a3a958e3c.png\" /></p><p></p><p>除了 v0，Vercel 在推动 AI 的应用落地上也是不遗余力。他们发布了 AI SDK ，帮助开发者轻松构建 AI 驱动的应用程序。目前该 SDK 每周的 npm 下载量已超过 85000 次，这也是人们在 Vercel 上注册和创建 API 应用程序的原因之一（译注：在 Vercel 上，基于其提供的 AI 能力可以很轻松的创建 AI 驱动的应用）。</p><p></p><p>虽然人工智能不会抢走我们的饭碗，但越来越多的开发人员正将其整合到开发流程中。根据 Retool 的 《2023 年 AI 现状》 报告，自 2022 年以来，57% 的开发人员减少了对 Stack Overflow 的使用，甚至有 10% 的开发人员因为有了 ChatGPT 和 GitHub Copilot 而不再使用 Stack Overflow。</p><p></p><p>除了越来越多的开发人员将通过 AI 来简化开发流程之外，预计会有更多的公司将 AI 集成到自己的产品中，GitHub 的 Copilot 和 Sourcegraph 的 AI 编码助手 Cody 都是很好的例子。</p><p></p><h2>趋势三：SSR 和 SSG 两种框架之间的竞争将会愈演愈烈</h2><p></p><p></p><p>近期，服务器端渲染（SSR）和静态网站生成（SSG）这两种渲染方法，因在 SEO 和性能方面的优势而备受关注。随着越来越多的开发人员和企业出于 SEO 和性能方面的需求而采用 SSR 和 SSG 渲染方案，支持这两类渲染方式的框架之间的竞争将会愈演愈烈。</p><p></p><p>最近的一个 SSR/SSG 框架之争的例子是 Tech Twitter 上对比 Next.js 和 Remix 的话题。先是 Kent C. Dodds 发表了一篇题为 \"为什么我不会使用 Next.js \"的文章，随后作为回应，来自 Vercel 的 Lee Robinson 发表了题为 \"为什么我要使用 Next.js \"的文章。</p><p></p><p>在技术选择方面，没有放之四海而皆准的工具，只有最适合工作的工具。不过，就目前情况来看，可以肯定地说，Next.js \"击败 \"了其他竞争对手。</p><p></p><p>根据 Stack Overflow 2023 年的调查数据，Next.js 是第六大最受欢迎的 Web 框架，超过了分别排在第 21、24 和 30 位的 Nuxt.js、Gatsby 和 Remix。而根据 Stack Overflow 2022 年的调查数据，Next.js 还只排在第 11 位。随着时间的推移，Next.js 的受欢迎程度将会越来越高。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/77/77aa6656cd45c4308e67387aecde22fb.png\" /></p><p></p><p>凭借对 RSC（React Server Components）的支持、内置图片和字体优化以及 Server Actions 等功能，我不认为其他 SSR/SSG 框架能在短期内超越它。不过，随着这些框架的功能不断迭代和更新，我们预计这场战斗将会继续下去。</p><p></p><p>此外，我认为 Astro 将成为继 Next.js 之后的另一个爆款框架。根据 Netlify 的 《2023 年Web开发现状》 报告，Astro 的使用率和满意度增长最快。最令人印象深刻的是，它的使用率几乎翻了一番，使用满意度也从 2022 年的 4.5 上升到 2023 年的 6.8。</p><p></p><p>下面是 NPM 提供的这些框架的每周下载数据：</p><p></p><p>Next.js：5,037,121Nuxt：571,196Gatsby：316,779SvelteKit：306,599Astro：197,435Quasar：111,975Remix：22,676</p><p></p><h2>趋势四：前端、后端和全栈开发之间的界限将越来越模糊</h2><p></p><p></p><p>在早期的 Web 开发中，开发人员遵循的原则是 \"关注点分离 \"。因此，前端、后端和全栈开发人员被分配在了不同的系统和领域中。</p><p></p><p>然而，随着时间的推移，这些界限变得越来越模糊：</p><p></p><p>Next.js 提供了 路由处理程序（Route Handlers），它允许我们处理 HTTP 请求、从数据库获取数据、运行服务器端逻辑以及执行从数据库获取数据等任务。React 18 内置了 RSC（React Server Components），该功能允许我们预渲染应用程序，并在服务器端而不是客户端进行数据库查询，从而进一步模糊了两者之间的界限。这意味着我们可以直接在 React 组件中编写数据库查询功能。Next.js 还发布了 Server Actions 功能，该功能允许我们直接在客户端上定义函数操作服务器上的数据。</p><p></p><p>除了这些发展之外，诸如 Supabase、Appwrite 和 Xata 等后端即服务（BaaS）解决方案的出现和流行，也让前端开发人员可以更容易地创建全栈应用程序。借助这些新工具和技术，随着前端开发人员对后端和全栈开发的不断探索，我们可以期待更多跨领域专业人才的出现。</p><p></p><h2>趋势五：越来越多的人关注无障碍性</h2><p></p><p></p><p>目前有 超过 10 亿人 患有这样或那样的残疾，因此在开发过程中，针对无障碍性的设计不能只是事后才想到的问题。以下是关注无障碍性带来的一些好处：</p><p></p><p>使受众范围更广：无障碍网站可供各种残疾人士使用，从而大大扩展了其潜在的受众范围。增强 SEO：许多针对无障碍性的实践可以增强 SEO，使其内容排名更靠前，更容易被找到。有利于合法合规：许多地区的法律都要求网站具有无障碍性，因此创建无障碍性网站有助于避免触犯法律和招致罚款。增强用户体验：针对无障碍性功能的设计，通常能改善所有用户的整体体验，而不仅仅是残疾用户。积极的品牌形象：对无障碍性的承诺体现了社会责任感和包容性，可以对组织的声誉产生积极影响。</p><p></p><p>虽然几十年来 Web 对无障碍性的支持程度有所提高，但距离我们理想中的完全无障碍 Web 还很遥远。截至 2022 年，只有 3% 的互联网支持残障人士访问。虽然这一比例低得令人震惊，但我们看到，在无障碍方面，互联网正在逐渐且稳步地改善。</p><p></p><p>通过对 100 万个网页进行调研，WebAIM 发布了 《2023 年无障碍性报告》。该报告显示，越来越多的开发人员和机构开始逐渐意识到了无障碍性的重要性。以下是报告中的一些统计数据：</p><p></p><p>自 2022 年以来，ARIA 代码的使用率增长了 29%，自 2019 年以来增长了近四倍。100 万个主页中有 80% 使用了 ARIA，比 2022 年的 74.6% 有所提高。89.8% 的主页采用了有效的 HTML5 doctype，比 2022 年的 86.1% 和 2021 年的 79.1% 都有所增加。</p><p></p><p>软件之家发布的 《2022 年前沿技术现状》 显示，63% 的开发人员预测无障碍性将在未来几年内得到普及（见下图）。越来越多的开发人员和企业正在加倍努力提高其网站的无障碍性，在 WebAIM 2024 年的报告中，我们可以期待对无障碍性的控诉会减少，而支持无障碍性的网站会增加。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/3d/3d85edf736bd8164105a66b90bd8f054.png\" /></p><p></p><p></p><h2>趋势六：VS Code 将继续占据最受欢迎 IDE 的宝座</h2><p></p><p></p><p>VS Code 于 2019 年发布，现已成长为最受欢迎的代码编辑器之一，可与 Vim、IntelliJ 和 Webstorm 等 IDE 相媲美。Stack Overflow 2023 年的调查数据显示，VS Code 仍然是最受开发人员欢迎的 IDE。软件之家的 2022 年前端现状调查 也显示，VS Code 是开发人员最喜爱的代码编辑器，占比达到了 74.4%。</p><p></p><p>为什么开发人员会喜欢 VS Code？以下是一些原因：</p><p></p><p>它支持 100 多种语言。它拥有一个庞大的插件市场。诸如 Live Server、React、Next.js Snippets、Live Sass Compiler 以及 HTML End Tag Labels 等最受欢迎的 VS Code 扩展，前端开发人员应该都是耳熟能详 。它具有高度可定制性。</p><p></p><p>VS Code 的更新速度非常快，用户被频繁地提示安装更新就是证明。鉴于其目前的发展速度和轨迹，我们可以期待 VS Code 在 2024 年及未来将不断迭代和增加其功能，并且将持续保持其作为头部 IDE 的地位。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4f/4f6d85288fb5525271361cd5df26343f.png\" /></p><p></p><p></p><h2>趋势七：将会有更多人使用 TypeScript</h2><p></p><p></p><p>虽然在开发生态系统中对是否使用 TypeScript 的看法依然不统一，但支持 TypeScript 的人占多数。事实上，你会看到更多公司和开源社区将他们的代码库迁移到 TypeScript。Stripe 就是一个例子，它将其最大的 JavaScript 代码库（约 370 万行的代码）迁移到了 TypeScript。</p><p></p><p>Stack Overflow 的开发者调查显示，TypeScript 的受欢迎程度已从 2022 年的 34.83% 上升到了 2023 年的 38.87%。TypeScript 的使用已经非常广泛，以至于许多开发人员的文档中都设置了用于切换 TypeScript 和 JavaScript 代码的按钮。而且，在某些情况下，有些文档只提供 TypeScript 代码。</p><p></p><p>虽然 TypeScript 不会在短期内超越 JavaScript，甚至可能永远不会，但我们可以预见，随着团队将代码库迁移到 TypeScript 或直接使用 TypeScript 来启动新项目，采用 TypeScript 的情况将会变得越来越多。</p><p></p><p>下图是来自 2022 年前端现状调查，该调查显示了开发人员对 TypeScript 未来的看法。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/74/74a806bb07c027aa70289bf9cbf62c95.png\" /></p><p></p><h2>结论：为你的 2024 做好准备</h2><p></p><p></p><p>随着新技术、新框架和新趋势的出现，前端开发领域也在不断演变。虽然适应快节奏的变化可能非常有挑战性，但紧跟最新趋势可以促进你的职业发展，使你的技能与时俱进。</p><p></p><p>以下是根据本文中介绍的趋势，我们给出的一些可以为 2024 年做准备的建议：</p><p></p><p>尝试使用 Ark UI、Open Props 和 Shadcn UI 等新的样式解决方案并熟悉它们。学习如何将 GitHub Copilot 等人工智能工具集成到日常开发工作流程中。如果还没有，请开始学习 SSR/SSG 框架。可以考虑从 Astro 或 Next.js 开始。探索使用 Xata 和 Supabase 等 BaaS 平台构建全栈应用程序。在你参与的每个项目中都使自己成为无障碍性的倡导者。成为 VS Code 专家，学习可提高工作效率的插件。开始学习 TypeScript。它很可能会在开发领域存在一段时间。</p><p></p><p>原文地址：</p><p><a href=\"https://www.frontendmentor.io/articles/7-frontend-web-development-trends-for-2024-qtBD0H0hY3\">https://www.frontendmentor.io/articles/7-frontend-web-development-trends-for-2024-qtBD0H0hY3</a>\"</p><p></p>",
    "publish_time": "2024-01-29 09:45:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "突发！美国拟限制中国公司使用其云数据中心训练AI模型；TikTok、英雄联盟开发商裁员；哄哄模拟器爆火 | AI周报",
    "url": "https://www.infoq.cn/article/2QSam7XmeLkiSDwQNKWY",
    "summary": "<p></p><p></p><blockquote>美国商务部新规：要求美国云计算公司报告外国实体使用其云数据中心训练AI模型的情况；TikTok、英雄联盟开发商、谷歌纷纷启动裁员；苹果员工年均创收 1785.3 万元；蚂蚁集团成立 AI“精锐部门”，前谷歌 AI 工程师徐鹏担任负责人；腾讯斥资 64.2 亿在北京海淀区购地，加强研发与办公基础设施建设；马云重回阿里巴巴最大股东地位，软银持股比例大幅下降；OpenAI 首席执行官奥尔特曼寻求数十亿美元投资，以缓解芯片短缺问题；京东重返央视春晚，豪送一亿份一分钱奖品以拓展下沉市场；华为宣布与淘宝合作，启动鸿蒙原生应用开发；谷歌发布 AI 视频大模型 Lumiere，革新视频生成技术……</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>美国商务部新规：要求美国云计算公司报告外国实体使用其云数据中心训练AI模型的情况</h4><p></p><p>路透社1月26日报道， 美国商务部长吉娜·雷蒙多周五表示，拜登政府提议要求美国云公司，确定外国实体是否正在访问美国数据中心以训练人工智能模型。</p><p></p><p>雷蒙多在接受路透社采访时表示：“我们不能让非国家行为者、中国或是我们不想让他们访问我们的云的人，来训练他们的模型。”（We can't have non-state actors or China or folks who we don’t want accessing our cloud to train their models） “我们对芯片实行出口管制，”她指出。“这些芯片位于美国云数据中心，因此我们还必须考虑关闭潜在恶意活动的途径。”</p><p></p><p>拜登政府正在采取一系列措施，阻止中国将美国技术用于人工智能，因为这一新兴行业引发了安全担忧。拟议的“了解你的客户”法规已于周五发布供公众查阅，并将于周一发布。“这是一件大事，”雷蒙多说。</p><p></p><h4>TikTok、英雄联盟开发商、谷歌纷纷启动裁员</h4><p></p><p>本周，在谷歌宣布其2024年的宏伟目标，即成为全球最强AI公司，并致力于提供最先进、最安全、最负责任的AI技术，改善知识、学习、创造力和生产力，以及构建最有用的个人计算平台和设备的同时，公司也面临着新一轮的裁员风波。据外媒报道，谷歌计划在2024年裁减更多员工，这一消息引发了广泛的关注和讨论。</p><p></p><p>资深工程师Diane Hirsh Theriault在社交媒体上公开批评谷歌管理层，她指出管理层缺乏远见，并指责裁员过程缺乏透明度和合理性，这不仅破坏了公司的知识体系，也引起了员工的普遍不满。Theriault认为，高管们试图通过裁员来简化流程，但这种做法并没有明确的愿景支持。</p><p></p><p>谷歌CEO Sundar Pichai此前在内部邮件中提到，为了确保公司能够在优先事项上进行投资，不得不做出艰难的选择。这暗示了未来可能还会有更多的裁员行动。去年，谷歌已经裁减了1.2万名员工，而今年的裁员规模虽然尚未明确，但CEO已经确认裁员将继续。</p><p></p><p>Alphabet工会计划在谷歌园区举行示威活动，以抗议公司的裁员政策。工会通讯主席Stephen McMurtry表示，裁员导致剩余员工的工作量增加，同时也加剧了员工的焦虑。工会和员工们对谷歌的这一决策表示担忧，认为这可能会对公司的长期发展和员工福祉产生负面影响。</p><p></p><p>除了谷歌，游戏开发商拳头游戏公司（Riot Games）也宣布全球裁员。作为腾讯的全资子公司，拳头游戏宣布将在全球范围内裁员约530人，占员工总数的11%。这一决定是在公司CEO的公开信中宣布的，信中强调裁员是为了公司未来的发展，并非为了短期的股东利益或季度盈利。拳头游戏表示，过去几年员工数量翻倍，但部分投资未能带来预期回报，公司需要重新聚焦于玩家最关心的领域，并减少对影响力不足项目的投入。</p><p></p><p>另外，全球短视频平台TikTok近期在美国实施了裁员措施，以降低成本。据外媒报道，此次裁员涉及约60名员工，主要集中在销售和广告部门。报道显示，TikTok裁减了约60名员工，涉及到多座城市，包括洛杉矶、纽约和奥斯汀，也有美国之外的员工。即便如此，TikTok目前在美国还有约7000名员工。</p><p></p><h4>苹果员工年均创收1785.3万元</h4><p></p><p>1月25日，市场研究机构Ondeck发布了一份报告，揭示了全球科技公司中人均创收最高的企业。报告显示，Netflix以每位员工年均创收249万美元（约合人民币1785.3万元）的业绩位居榜首。紧随其后的是苹果公司，其每位员工年均创收为234.8万美元（约合人民币1683.5万元）。Ondeck的分析师利用福布斯全球2000强企业排行榜的数据，对这些科技巨头的员工规模和年度总收入进行了分析，并将总收入平均分配到每位员工身上，从而得出了这一排名。</p><p></p><p>值得注意的是，尽管苹果公司的员工数量是Meta的两倍，但其收入却是Meta的三倍。在2023年科技公司普遍裁员的背景下，苹果公司得益于其谨慎的扩张策略，避免了大规模的裁员。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3b5aedd565a8154c0a78f29a5c5baad.png\" /></p><p></p><h4>蚂蚁集团成立AI“精锐部门”，前谷歌AI工程师徐鹏担任负责人</h4><p></p><p>蚂蚁集团近日宣布成立名为NextEvo的AI创新研发与应用部门，该部门由蚂蚁集团副总裁徐鹏领导，徐鹏曾在谷歌工作11年，负责谷歌翻译的核心技术研发，并参与了谷歌显示广告系统的算法研发。</p><p></p><p>NextEvo被视为蚂蚁集团内部的AI“精锐部门”，承担了蚂蚁AI所有核心技术研发任务，包括蚂蚁百灵大模型的研发。2023年，NextEvo发表了30余篇AI国际顶刊顶会论文，并开源了智能大规模分布式深度学习系统DLRover和GPU显存+传输优化开源项目GLake，填补了国内AI垂直领域技术开源的空白。</p><p></p><h4>壁仞总裁离职，知情人士称会继续留在算力生态行业</h4><p></p><p>据最新消息，中国GPU龙头企业壁仞科技联合创始人、总裁徐凌杰确认已离开公司。在徐凌杰给壁仞同事发送的离职邮件信中，他表示“期待在未来新的征程中能继续与壁仞的朋友们互相扶持。AGl (通用人工智能)is calling,江湖再见！”</p><p></p><p>按照知情人士的说法，徐凌杰虽然人不在壁工作，但会继续留在算力生态行业，未来可能还会与壁仞合作。</p><p></p><h4>腾讯斥资64.2亿在北京海淀区购地，加强研发与办公基础设施建设</h4><p></p><p>腾讯科技（北京）有限公司近日以64.2亿元人民币的底价成功竞得北京市海淀区学院路北端的A、B、C、J地块，该地块包括B4综合性商业金融服务业用地和B23研发设计用地。此次购地将主要用于满足腾讯对办公用地的需求，为员工提供稳定集中的办公场所。地块总面积约为70,601平方米，地上建筑规模为285,523平方米，每平方米约合9.1万元人民币。商业用地出让年限为40年，办公和公服（科研）用地为50年。</p><p></p><p>腾讯此前已在北京拥有多个办公地点，包括位于海淀区的腾讯北京总部大楼和奥林匹克森林公园的亚洲金融大厦。此次购地将进一步扩大腾讯在北京的办公空间，以适应公司不断增长的员工规模。腾讯方面表示，新地块的建设将遵循相关规划，预计在签订土地使用权出让合同后的12个月内开工建设，并在3年内竣工。</p><p></p><h4>马云重回阿里巴巴最大股东地位，软银持股比例大幅下降</h4><p></p><p>近日消息，阿里巴巴集团创始人马云在一系列增持行动后，已超越软银集团，再次成为公司的最大股东。这一变化标志着软银集团在阿里巴巴的持股比例经历了显著下降，从2022年12月的约7%降至2023年5月的不到0.5%。马云在2023年第四季度的增持行动中，购买了价值约5000万美元的阿里巴巴股票，使得其持股比例超过了2021年底的4.3%。与此同时，阿里巴巴现任董事长蔡崇信通过其家族投资平台蓝池资本管理公司，也增持了1.5亿美元的阿里股票。</p><p></p><p>阿里巴巴集团对此次股权变动表示，这不仅体现了马云和蔡崇信对公司未来发展的坚定信心，也是对公司管理团队及战略方向的有力支持。此外，阿里巴巴在2023年实施了大规模的股票回购计划，以95亿美元回购了8.979亿股普通股，进一步巩固了公司在市场中的稳定地位。</p><p></p><h4>OpenAI首席执行官奥尔特曼寻求数十亿美元投资，以缓解芯片短缺问题</h4><p></p><p>OpenAI首席执行官萨姆·奥尔特曼（Sam Altman）正积极寻求数十亿美元的投资，以应对AI行业面临的芯片短缺问题。奥尔特曼在公开场合多次表达对英伟达GPU供应不足的担忧，并表示OpenAI的GPU供应“严重受限”。为了解决这一问题，奥尔特曼正在与包括G42和日本软银集团在内的多个大型投资者讨论合作，计划筹集资金用于建立芯片制造厂或晶圆厂。</p><p></p><p>奥尔特曼的计划还包括与英特尔、台积电、三星等知名芯片制造商合作，以建立全球生产网络。此外，奥尔特曼预计将访问韩国首尔，与SK集团会长崔泰源会面，探讨AI芯片合作的可能性。SK集团旗下的SK海力士是英伟达HBM的独供商，尽管英伟达正在扩大其HBM供应商范围，SK海力士仍然是主要供应商之一。</p><p></p><h4>京东重返央视春晚，豪送一亿份一分钱奖品以拓展下沉市场</h4><p></p><p>近日消息，京东计划重返2024年央视春晚互动平台，通过App上的春晚主会场，以抽奖互动的形式送出一亿份一分钱奖品，以及包括汽车在内的实物商品。京东此举意在通过春晚这一高收视率平台，吸引并拓展下沉市场的新用户群体。</p><p></p><p>据悉，成为春晚互动平台的企业通常需要支付上亿元的赞助费用。京东此前在2022年曾作为春晚独家互动合作伙伴，投入价值15亿元的红包和商品，远超其他平台的投入。此次合作并非独家，小红书也已宣布成为春晚的笔记与直播分享平台，将在除夕直播春晚，并提供同步购买春晚舞台上展示的商品。</p><p></p><p>京东还于本月初独家冠名了“2023 - 2024 湖南卫视芒果 TV 跨年晚会”。至于今年京东为什么重返春晚，一位京东人士的解释是，2023 年是京东面临增长压力、渴求寻找更多下沉用户的一年。</p><p></p><h4>微软组建“GenAI”团队，专注开发高效小型AI模型，以减少对OpenAI的依赖</h4><p></p><p>微软近期组建了一支名为“GenAI”的新团队，该团队由公司副总裁Misha Bilenko领导，旨在开发小型AI模型，以减少对外部合作伙伴OpenAI的依赖。“GenAI”团队成员包括来自Azure的工程师以及微软研究院的AI研究人员，他们将共同致力于开发能够在移动设备上运行的轻量级模型，同时在某些任务上接近大型模型的性能。</p><p></p><p>微软此前推出的小型模型Phi-2，展示了公司在小模型领域的技术实力。Phi-2模型拥有2.7B参数，但在多项基准测试中表现出色，性能甚至超过了一些参数量更大的模型。这一成就不仅证明了微软在小模型开发上的潜力，也突显了公司在降低AI模型成本和推理需求方面的努力。随着AI技术的不断进步，小型模型因其在资源消耗和部署灵活性方面的优势，正逐渐成为行业关注的焦点。微软的这一新团队预计将在这一领域发挥重要作用。</p><p></p><h2>IT业界</h2><p></p><p></p><h4>华为宣布与淘宝合作，启动鸿蒙原生应用开发</h4><p></p><p>淘宝与华为签署鸿蒙合作，将开发基于 HarmonyOS 的鸿蒙原生应用，以增强电商领域的鸿蒙生态，并打造全场景购物体验。目前，鸿蒙生态已进入第二阶段，有 200 + 伙伴加速鸿蒙化，覆盖多个领域，鸿蒙原生应用版图成型，设备数量也迅速增长。</p><p></p><h4>AI原生应用“哄哄模拟器”24小时内吸引60万用户，探索情感沟通新趋势</h4><p></p><p>近日，一款名为“哄哄模拟器”的AI原生应用在情感沟通领域掀起热潮，仅24小时内便吸引了60万用户的关注。这款应用由开发者王登科基于个人生活经历创意开发，旨在通过模拟情侣吵架场景，帮助用户提升哄人技巧。用户在游戏中扮演一方，AI则扮演另一方，通过互动对话，系统会根据用户的回复调整“原谅值”，从而评估用户的沟通效果。</p><p></p><p>“哄哄模拟器”最初以网页版形式亮相，后推出iOS端App，提供了如“约会迟到”、“忘记纪念日”等生活化场景。产品结合了游戏化的升级和打分系统，让用户在轻松的游戏体验中学习如何化解冲突。用户还可以自定义沟通场景，进一步个性化体验。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/74/74bde417047eca24e4792558ee2c3454.jpeg\" /></p><p></p><h4>谷歌发布AI视频大模型Lumiere，革新视频生成技术</h4><p></p><p>谷歌在人工智能领域的最新突破，AI视频大模型Lumiere，经过7个月的密集研发，现已亮相。这一模型采用了创新的时空U-Net架构，能够在一次生成过程中创建连贯且高质量的视频内容，显著提升了视频的时长和一致性，超越了现有的Gen-2和Pika模型。</p><p></p><p>Lumiere模型通过联合空间和时间下采样技术，能够生成长达5秒、80帧的全帧率视频，这在AI视频生成领域是一个重大进步。它不仅能够从文本提示生成视频，还能将静态图像转换为动态视频，甚至能够进行视频编辑和风格化生成。此外，Lumiere还能够在视频中插入或修改对象，以及生成特定艺术风格的视频。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/58/582927ca3bab766c757a1127f0412d0f.gif\" /></p><p></p>",
    "publish_time": "2024-01-29 10:15:09",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "九部门印发《方案》推动原材料工业数字化转型，3年培育60+标杆工厂",
    "url": "https://www.infoq.cn/article/pkPhasg71lhx6qsIAxB2",
    "summary": "<p>近日，工业和信息化部、国家发展改革委、财政部、自然资源部、生态环境部、国务院国资委、市场监管总局、中国科学院、中国工程院等九部门联合印发《原材料工业数字化转型工作方案（2024—2026年）》（以下简称《工作方案》）。</p><p></p><p>《工作方案》提出，到2026年，原材料工业数字化转型取得重要进展，打造120个以上数字化转型典型场景，培育60个以上数字化转型标杆工厂，形成一批数字化转型标杆企业，重点行业关键工序数控化率、数字化研发设计工具普及率等指标显著提升。</p><p></p><p>据工业和信息化部原材料工业司相关负责人介绍，原材料工业增加值占我国规模以上工业增加值的30%左右，是推进制造业数字化转型的主力军。作为典型的流程制造业，原材料工业主要有以下三方面特点：一是资源能源密集，二是过程机理复杂，三是生产连续性强。</p><p></p><p>至于原材料工业数字化转型的含义，是指利用人工智能、5G、工业互联网等数字技术，在材料研发设计、生产制造，企业经营管理、物流仓储，行业运行调控、耦合协调以及上下游协同等各环节进行融合创新和改造提升，实现生产要素泛在感知、制造过程数字孪生、运营管理最优决策。</p><p></p><p>近年来，我国原材料工业数字化转型不断走向纵深，部分行业龙头企业达到国际领先水平，但仍面临对数字化转型认识不够、数字化转型基础差异大、建模仿真难度高、人工智能等数字技术融合应用不深入、复合型人才紧缺等问题。</p><p></p><p>出台《工作方案》，旨在认真落实全国新型工业化推进大会部署，瞄准目标要求，紧抓发展机遇，以数字化转型促进原材料工业高质量发展，提升行业核心竞争力和构筑国际竞争新优势，为加快推进新型工业化、建设制造强国提供坚实支撑。</p><p></p><p>此外，《工作方案》还提出，到2026年，支撑能力显著增强，突破一批数字化转型急需的关键核心技术，制修订一批先进适用的数字化转型标准规范。推广应用100款以上优秀产品，培育100家优秀系统解决方案提供商。服务体系持续完善，建设 1 个新材料大数据中心、4个重点行业数字化转型推进中心、4 个重点行业制造业创新中心、5个以上工业互联网标识解析二级节点、6 个以上行业级工业互联网平台。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdbd32ad79311f6ff3fb00a81d2ef476.png\" /></p><p>《工作方案》围绕上述目标从4个方面部署14项任务。</p><p></p><p>工信部方面强调，将面向石化化工、钢铁、有色金属、建材等重点行业，依托第三方机构、科研院所、重点企业等，推动组建重点行业数字化转型推进中心，加快推进落实数字化转型重点任务，开展数字化转型评估、咨询、诊断等工作；将在智能制造成熟度评估模型、数字化转型成熟度模型与评估等现有标准基础上，结合原材料工业特点和转型需求，研究制定石化化工、钢铁、有色金属、建材等重点行业数字化转型水平与成效评估标准。更高要求、更加精准、更加客观对原材料企业数字化转型情况进行评估诊断，摸清问题、提出对策，选树行业标杆。</p><p></p><p>《工作方案》原文件链接：<a href=\"https://www.miit.gov.cn/cms_files/filemanager/1226211233/attach/202312/1152c2a7d7cc4e9593792371da7df498.pdf\">https://www.miit.gov.cn/cms_files/filemanager/1226211233/attach/202312/1152c2a7d7cc4e9593792371da7df498.pdf</a>\"</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6a/7f/6a727c625eae4byya4a6075aed39e17f.jpg\" /></p><p></p>",
    "publish_time": "2024-01-29 11:41:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "微服务穷途末路？新招式是否能开启“黄金演进期”？",
    "url": "https://www.infoq.cn/article/48ah0aQ09mscYFLTOEU6",
    "summary": "<p>采访嘉宾 | 罗广明</p><p>编辑 | Tina</p><p>&nbsp;</p><p>尽管微服务架构长期以来被视为云原生应用的事实标准，但在2023这一年，来自各方的反思声音逐渐增多。去年3月，AWS分享了一个案例，Prime Video 团队将其Serverless应用程序中的部分微服务调整成为了一个单体，称此举节省了 90% 的运营成本。DHH对此评论道：“亚马逊也无法理解无服务器或微服务。”谷歌也于去年开源了一个名叫Service Weaver的框架，在相关论文中，他们表示这种方法可以将发布式系统带来的延迟降低15倍，成本降低9倍。</p><p>&nbsp;</p><p>那么，微服务到底有没有什么问题？Service Weaver是否能够成为微服务架构的“新解”呢？在年终盘点之际，InfoQ采访了字节跳动服务框架团队架构师、CloudWeGo 开源负责人罗广明，探讨了微服务发展十几年来的进展和关键技术演变。</p><p>&nbsp;</p><p>InfoQ：微服务架构今年已经满十岁了，您是否能总结下它的演进关键阶段？目前分布式架构还面临的最大挑战是什么？</p><p>&nbsp;</p><p>罗广明：微服务架构在过去十年中经历了几个关键阶段的演进：</p><p>初期探索阶段（2010-2013年）：微服务概念的提出和初步探索，尝试将传统的单体应用拆分成小型、自治的服务。技术领域开始出现一些支持微服务架构的工具和平台，但尚未形成成熟的最佳实践。早期采用和成熟化（2014-2016年）：微服务架构得到更广泛的关注和采用，许多组织开始将其应用于生产环境中。持续集成和持续交付（CI/CD）、容器化技术（如Docker）、编排工具（如Kubernetes）的出现和广泛应用，为微服务的部署和管理提供了更多支持。成熟阶段和生态系统建设（2017-至今）：微服务架构成为主流，许多大型企业和组织开始采用，并建立了成熟的最佳实践和模式。服务网格（Service Mesh）的兴起，为微服务架构中的服务通信、可观测性和服务治理提供了更多的解决方案。基于云原生和 Serverless 的发展，微服务架构更趋向于弹性、自动化和灵活性。</p><p>&nbsp;</p><p>微服务架构的一些挑战包括：</p><p>微服务拆分与粒度：微服务粒度的控制对整个架构的性能、可维护性和扩展性都有很大的影响。微服务粒度的控制是一个动态的且充满挑战的事情。微服务需要治理：随着服务数量的增加，需要解决服务发现、负载均衡、版本控制等问题，确保服务之间的通信可靠性和一致性。分布式系统复杂性管理：微服务架构涉及多个分布式组件，因此需要处理分布式系统带来的复杂性，如一致性、事务管理和错误处理。安全性需求变高：分布式系统面临着更多的安全挑战，需要确保每个服务都能得到充分的保护，防止安全漏洞和攻击。文化和组织变革：采用微服务架构需要组织文化的变革，包括团队间的协作、持续交付和 DevOps 文化的推广。</p><p>&nbsp;</p><p>解决这些挑战需要不断演进的工具、最佳实践的制定以及组织文化上的变革，以确保微服务架构能够持续发展、能在复杂的环境中稳定运行以及获得真实的收益。</p><p>&nbsp;</p><p>InfoQ：这一年架构领域有哪些关键框架/组件有关键更新或演进？</p><p>&nbsp;</p><p>罗广明：架构领域一直在不断演进和更新，在2023年，一些关键框架和组件经历了重要的更新或者取得了进展：</p><p>服务网格：更加成熟的实现和标准化。继 Proxyless Mesh，Istio 今年推出 Ambient Mesh 模式，并正式从 CNCF 毕业，成为 CNCF 活跃度排名第三的项目。开发框架：更多适用生产环境微服务架构的开发框架。以 Kitex/Hertz 为例的微服务框架更加关注企业用户在生产环境的落地和使用反馈，关注易用性和降本增效成为框架选型的主流意见。编程语言与生态：Golang 成为国内诸多大厂主流或最热门的编程语言，Golang 相关的开源中间件生态繁荣，竞争加剧；Rust 成为最有潜力的编程语言，诸多大厂纷纷投资入局，新的 Rust 微服务框架如 Volo 推动 Rust 在企业内部更广泛落地。字节跳动服务框架团队联合&nbsp;CloudWeGo 开源社区发布 《CloudWeGo 技术白皮书: 字节跳动云原生微服务架构原理与开源实践&nbsp;》助力企业用户完成微服务架构转型，实现降本增效和稳定性提升。</p><p>&nbsp;</p><p>InfoQ：微服务、单体、模块：有人认为解决过度复杂性和缺乏可维护性的方法是“模块化”，您同意这个观点吗，为什么？</p><p>&nbsp;</p><p>罗广明：“模块化”确实是解决复杂性和可维护性问题的一种重要方法，但并不是唯一的方法。微服务架构、单体应用和模块化都可以在一定程度上解决复杂性和可维护性问题，但它们有不同的特点和适用场景。</p><p>&nbsp;</p><p>模块化指的是将一个系统或应用拆分成独立的功能模块，每个模块都有明确定义的职责和界限。模块化可以促进代码重用，减少重复编写相似功能的代码；模块化降低了系统的耦合度，使得单个模块的修改和维护更加简单和安全。但是，如果模块之间的交互设计不合理，或者模块划分不够清晰，仍然可能导致系统的复杂性和维护困难。模块化架构在服务弹性和可靠性仍存在挑战。</p><p>&nbsp;</p><p>微服务架构和单体应用则是针对不同需求和场景的解决方案。微服务架构通过将应用拆分成小的、自治的服务来解决单体应用的复杂性和可维护性问题。而单体应用虽然有着较高的集中性和一体性，但在一些规模较小或者业务简单的情况下，也能够提供良好的开发和维护体验。</p><p>&nbsp;</p><p>在解决复杂性和可维护性的问题时，并非只有一种方法是完全正确的。模块化是一个重要的思维模式和架构方法之一，但结合实际需求和场景，可能需要综合考虑模块化、微服务架构、单体应用等不同的解决方案。</p><p>&nbsp;</p><p>在大型企业中，采用微服务架构后，随着业务的快速增长，出现微服务过微或者过度复杂似乎是一件不可避免的事情。这个时候，企业需要一套成熟的微服务架构复杂度治理体系，用来管控增量服务和治理存量服务。字节跳动于 2023 年正式启动微服务架构复杂度治理，大致分为服务数治理、依赖治理和领域治理，循序渐进，依次治理，逐步深入。以服务治理为例，既包括对功能废弃服务、过微服务、长尾服务、定位重复服务等存量服务的下线治理，也包含合理的增量管控策略。而链路治理和领域治理则是围绕优化架构反模式和面向领域的微服务架构治理展开。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：今年一个比较大的事件是Google发布了<a href=\"https://www.infoq.com/news/2023/03/google-weaver-framework/\">Service Weaver</a>\"，允许用户“将应用程序编写为模块化整体，并将其部署为一组微服务”，这种方法有可能代表未来的一种相对主流解决方案吗？</p><p>&nbsp;</p><p>罗广明：Service Weaver 提供了一个灵活的框架，能够在本地简化开发，并将模块化单体应用转变为分布式微服务架构，在部署时允许自由配置组件的分布式部署方式，从而应对应用演进过程中的不确定性，并轻松适应组件间交互模式的变化。</p><p>&nbsp;</p><p>可以看到，Service Weaver 提供了一种全新的开发与部署解决方案，其最大的特点就是提供了一种灵活性和可配置性，对于业务的演进非常友好，可以灵活调整部署模式，来实现成本优化和价值最大化。</p><p>&nbsp;</p><p>但是，Service Weaver 目前仍处于早期开发阶段，可能存在稳定性和功能性方面的限制。这意味着可能会有一些尚未解决的问题，或者缺少一些重要的功能，因此尚不适合在生产环境中落地，后续有待于观望，包括更广泛的采纳和活跃的社区支持。其次，也是最重要的一点，这种架构模式并不适应于已经落地了微服务架构的业务，也不适应于已经比较稳定的业务，更不适应于对于性能要求极高的业务。反过来说，Service Weaver 对于尚处于快速发展的互联网在线业务比较友好，允许应用程序随着时间的推移进行低成本演进。</p><p>&nbsp;</p><p>与 Service Weaver 架构模式相对应的，字节跳动正在内部大规模落地合并部署与合并编译，这是给已经大规模落地微服务架构的业务提供一种降本增效的手段。&nbsp;合并部署方案主要思路是结合容器亲和性调度、流量调度计算、更高效的本地通信，让原本需要跨机的网络通信变成同机进程间调用，既能与现有体系融合又能减少微服务链路带来的性能损耗。</p><p>&nbsp;</p><p>合并编译则是一套全新的微服务编排思路，还是基于微服务的模式开发，在编译/发布期像内联函数一样内联微服务，以实现微服务成本优化。既可以拥有单体的性能，又拥有微服务的研发效率。其核心目标是将微服务进程间通信开销变成进程内方法调用，避免网络传输和序列化成本，也无需微服务治理逻辑以及其带来的一切成本开销。</p><p>&nbsp;</p><p>合并编译方案已在字节跳动内部多个业务线中使用，接入核心超过百万核，在不影响研发效率的情况下，带来数十万核的性能收益。在将于4月18-20日举办的 <a href=\"https://qcon.infoq.cn/2024/beijing/track?utm_source=infoq&amp;utm_medium=article&amp;utm_campaign=7&amp;utm_term=yinxunran\">QCon全球软件开发大会（北京站）2024</a>\" 的「单体&nbsp;vs&nbsp;微服务」专场，字节跳动服务框架团队研发工程师尹旭然将以<a href=\"https://qcon.infoq.cn/2024/beijing/presentation/5702?utm_source=infoq&amp;utm_medium=article&amp;utm_campaign=7&amp;utm_term=yinxunran\">《大规模微服务破局之道：合并编译》</a>\"为题，把方案落地经验分享出来，点击链接可了解详情：<a href=\"https://qcon.infoq.cn/2024/beijing/track/1621?utm_source=wechat&amp;utm_medium=infoqart2-0127\">https://qcon.infoq.cn/2024/beijing/track/1621?utm_source=wechat&amp;utm_medium=infoqart2-0127</a>\"</p><p>&nbsp;</p><p>InfoQ：云原生可移植性设计比如 Dapr 等框架是否有得到广泛采用？为什么？</p><p>&nbsp;</p><p>罗广明：Dapr 作为新兴的云原生项目，以\"应用运行时\"之名围绕云原生应用的各种分布式需求，致力于打造一个通用且可移植的抽象能力层。这个愿景有着令人向往的美好前景，然而 API 的标准化之路异常艰辛，Dapr 的分布式能力抽象在实践和落地过程中遇到了各种挑战和困扰。据观察，Dapr 近年来得到了不少关注，但是在国内并没有得到广泛的采用。</p><p>&nbsp;</p><p>与服务网格相比，Dapr 架构更加复杂，Dapr 的工作模式是能力抽象，需要业务应用程序遵从标准 API 进行请求开发与改造。服务网格主要设计目标是 低侵入，采用原协议转发的方式可以尽可能的降低对应用的侵入。Dapr 的主要设计目标是 可移植性，即在跨云跨平台的前提下实现无厂商绑定，采用的方式是在将分布式能力抽象为标准 API，并在各种开源项目和云平台上提供这套标准 API 的不同实现，从而达到在不同平台上运行的目标。因此 Dapr 的代价是需要应用进行适配和改造，侵入性比较大。因此 Dapr 更适合新应用开发 （Green Field），对于现有的老应用（Brown Field）则需要付出较高的改造代价。这也是 Dapr 当前并未获得广泛采用的主要原因。</p><p>&nbsp;</p><p>虽然 Dapr 和类似的框架提供了许多优势和新颖的特性，未来仍需要更多时间、成熟度和社区的支持。在面对已有系统、组织惯例和技术选型方面，新框架的采用需要认真权衡其优势与现有解决方案的差异，并选择最适合的方案。</p><p>&nbsp;</p><p>InfoQ：微服务兴起后，架构领域有了一个变革：架构更多的是做出设计决策，而不是使用 UML 或架构描述语言绘制方框和线条。最近这十年，您观察到架构设计文化还发生了哪些变化？有哪些在消亡，有哪些得到演化？软件架构师的技能要求主要有哪些改变？</p><p>&nbsp;</p><p>罗广明：根据本人经验和观察，近年来，软件架构设计文化经历了一些显著的变化：</p><p>演进的设计方法论： 架构设计不再仅限于传统的UML或架构描述语言。更多的方法和实践在不同领域兴起，如面向领域的设计（DDD）等，强调交互式和迭代式设计过程。微服务成熟和治理： 微服务架构已经不再是新兴概念，而是被广泛接受并应用。焦点已经从单纯的微服务拆分转移到了治理、监控、自动化和安全等方面。多样的架构模式以及眼花缭乱的微服务框架为架构师的技术设计与选型同样带来挑战。云原生和容器化： 云原生设计和容器化技术对架构设计产生了深远影响。设计需要更多地考虑弹性、可扩展性和云端部署的优化。</p><p>&nbsp;</p><p>技能要求的转变主要包括，需要深入了解云原生编排、容器化、服务网格、微服务框架、CICD、可观测等诸多技术，以便为应用程序选择最佳的部署和运行环境、提供安全能力和微服务治理能力、保障系统的稳定性和可靠性。</p><p>&nbsp;</p><p>InfoQ：您认为人工智能会对软件架构以及架构设计的哪些方面造成影响？</p><p>&nbsp;</p><p>罗广明：2023 年，人工智能（AI）展现了前所未有的智慧和发展，尤其是在基础大模型领域，给很多人带来了很多机遇和挑战。与此同时，AI 原生应用的发展尚处于早期，AI 对于软件架构与设计的影响尚没有在大规模生产环境下得以体现和验证，但是不可否认的是，AI 必然可以带来多维度的变化。比如，AI 可以帮助提高决策效率、优化设计、增强系统的自适应性和安全性，以及更好地应对系统的演化和变化。然而，AI 技术在这方面的应用也需要考虑隐私、安全以及技术成熟度等方面的问题。</p><p>&nbsp;</p><p>InfoQ：您能展望下架构未来3-5年的发展吗？</p><p>&nbsp;</p><p>罗广明：这个问题太庞大了，可以简单预料一下几点：</p><p>持续的云原生和微服务发展： 云原生和微服务架构会继续发展并成为主流，更多企业将采用这些架构以获得更好的弹性、扩展性和敏捷性。尤其是传统行业，其云原生架构采纳和转型还处于非常早期，需要诸多微服务基础设施和通用能力来简化云原生改造成本。AI 在架构中的应用增加： AI 技术将更广泛地用于架构设计中，包括 AI 辅助设计、决策支持与建议、智能监控、性能优化等方面，提高架构设计和系统优化的智能化水平。低代码/无代码平台的普及： 随着 AI 能力的支持与落地，低代码/无代码平台将得到更广泛的应用，使得开发人员能够更快速地开发、构建和部署应用，对架构设计也会带来一定的影响。</p><p>&nbsp;</p><p>【活动推荐】</p><p>随着业务的快速发展，微服务过微的弊端逐渐凸显：资源的开销、时延的上升、服务治理开销的增大变得不可忽视……如果你也在为这些问题头疼，那么一定要来听听4月18-20日举办的QCon全球软件开发大会（北京站），字节跳动后端研发工程师尹旭然将分享《大规模微服务破局之道：合并编译》，介绍字节跳动在治理大规模微服务中的技术思考、解决方案和应用实践。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f8d7999c8ac89cb8088bc984461cb836.png\" /></p><p></p><p>&nbsp;</p><p></p>",
    "publish_time": "2024-01-29 12:43:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "贾扬清新作被某印度创始人内涵借鉴，懒得纠缠：巧了，正准备开源，GitHub 见",
    "url": "https://www.infoq.cn/article/APVI3iyPCBtHufMGFeGm",
    "summary": "<p></p><p>1 月 25 日，LeptonAI 发布了一个基于自家服务的小 demo，用 500 行 Python 代码实现了一个大模型加持的对话式搜索引擎。随后，号称要干掉谷歌搜索的 Perplexity 创始人声称 LeptonAI 在“借鉴”、“致敬”他们的产品。作为 LeptonAI 的创始人，贾扬清在 Twitter 上进行了公开回击。此前，LeptonAI 正打算开源该演示工具的全部代码。</p><p></p><p></p><h2>事情经过</h2><p></p><p></p><p>LeptonAI 于近日发布了一个对话式搜索引擎 demo，名为“Lepton Search”。该 demo 界面主要是一个对话框，在对话框中输入想问的问题后，Lepton Search 会根据提问，返回答案、对应来源（Sources）、相关问题（Related）。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ab/abacfd5ea9ce0fdecb8189648ef63c87.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d2/d2533e94cc25331f697661dc374d74b3.png\" /></p><p></p><p>截图来源：<a href=\"https://search.lepton.run/\">https://search.lepton.run/</a>\"</p><p></p><p>LeptonAI 以此为例，向大家解释现在构建一个人工智能应用已经相当简单：这个演示程序，他们只用了不到 500 行 Python 代码，后端是一个非常快的 Mixtral-8x7b 模型，运行在 LeptonAI 自家的 playground 托管平台上，正常情况下吞吐量可高达约 200 个令牌 / 秒。该搜索引擎目前建立在 Bing 搜索 API 上，用 Lepton KV 作为无服务器存储。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ed/ed1dec92e59b60006aca5b52c380a1bb.png\" /></p><p></p><p>原本是基于 LeptonAI 云平台的一个简单 demo，没想到贾扬清在 Twitter 上发布演示视频后，Perplexity 的创始人突然出现，并发文感谢 LeptonAI 向他们“致敬”：“太棒了，看到 Perplexity 成为未来融资活动的标杆，前 Meta 和阿里巴巴高管都来取经！这说明 Perplexity 的影响力不局限于产品本身，而是辐射到了整个科技生态和行业发展，令人振奋！”</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/32/32b020b999cf2aff85b5c7ea3134353c.jpeg\" /></p><p></p><p>Perplexity AI 成立于 2022 年 8 月，总部设在旧金山。Aravind Srinivas 是 Perplexity AI 的创始人兼首席执行官，2017 年从印度理工学院毕业，考入加州大学伯克利分校攻读博士学位，后来又在 OpenAI 担任过一年的研究科学家。创始团队还包括 Denis Yarats 和 Johnny Ho，均具有人工智能相关背景。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/69/696daab622265a77fa8836bac6639921.jpeg\" /></p><p></p><p>截图源自 The Wall Street Journal</p><p></p><p>2022 年 9 月，Perplexity 获得 310 万美元的种子轮投资。2023 年 3 月，Perplexity 获 2560 万美元 A 轮融资。今年 1 月，再获英伟达领投的超 7000 万美元融资。</p><p></p><p>自 2023 年 12 月在亚马逊云科技 re: Invent 主题上亮相后，Perplexity 就受到了广泛关注，并得到了包括前 GitHub 首席执行官 Nat Friedman 等在内的一众大佬热捧。</p><p></p><p>Srinivas 的目标是挑战谷歌，他表示他自己是拉里·佩奇和谷歌的忠实粉丝：“我一直有做一些与谷歌同样规模和雄心的事情的冲动。”“目前看来，世界似乎对谷歌仍感到满意，他们的流量并没有实质性的变化。不过，就像谷歌和 Facebook 改变了人们获取新闻的方式一样，远离传统搜索引擎的转变最终会发生。”</p><p></p><p>Perplexity 的一众粉丝则表示 LeptonAI “借鉴”了他们的界面。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/83/8362e2f6429b24cb608ff5321bb7b55f.jpeg\" /></p><p></p><p>而其他粉丝则一脸懵“这是有专利吗？人家只是演示而已。”</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/97/97ebc8ee170de52ba9c4f1f4d93845cc.png\" /></p><p></p><p>对 Perplexity 的挑衅，贾扬清大佬罕见地进行了正面回击，“对话搜索”的领导者地位并不是来源于 Perplexity：“灵感在有一次贾扬清和微软最年轻的技术专家吴忧喝咖啡的时候，讨论 RAG 的效果究竟是源自搜索还是源自大模型，为了分析这个问题，所以自己手搭了一个 demo，同时展示 Lepton 对于 AI 创作者的效率提升。值得一提的是，吴忧是微软的搜索、对话式搜索等技术背后的核心技术领导者。”</p><p></p><p>并表示在发布这个 demo 之初已经声明要开源该演示工具的全部代码。当天下午，LeptonAI 如约将其开源，采用Apache-2.0 许可证。</p><p></p><p>开源地址如下：<a href=\"https://github.com/leptonai/search_with_lepton\">https://github.com/leptonai/search_with_lepton</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/22/22437c529d269958d8cb28577118988e.png\" /></p><p></p><p></p><h2>会话式搜索引擎原理是什么样的？</h2><p></p><p></p><p>作为一款想取代谷歌的搜索引擎，从表面看来，Perplexity 的工作原理是：当用户输入一个查询时，它会理解并重新构建这个查询，从实时索引中提取出相关链接。然后，Perplexity 将回答用户查询的任务交给 LLM，要求它阅读所有链接，并从每个链接中提取出相关段落整合内容，最终形成一段精准答案。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8d/8d8b03028045a47d3b63a08a1022b313.jpeg\" /></p><p></p><p>目前，大语言模型（LLM）主要面临两大挑战：数据陈旧、偶发幻觉。由于基础模型所使用的预训练数据集具有明确的截止日期，因此无法根据最新数据做出响应。即使是当前最强大的模型，也往往会因数据过时而编造答案，也就是人们常说的“幻觉”问题。</p><p></p><p>对于无法访问最新数据，可以有两种方法，第一种是通过搜索引擎，通过执行网络搜索并向大模型提交输来改善决策质量。Perplexity AI 更依赖于这种方法。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b2/b278cbed1d25e2486caccfeb09a17622.png\" /></p><p></p><p>第二种方法是，通过所谓检索增强生成（RAG），这项成熟技术可以解决一定程度的“幻觉”问题。与前面提到的动态调用搜索 API 方法不同，RAG 强调从公开数据存储中检索数据，例如向量数据库或者由外部维护的全文搜索索引等。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d8/d836f8e0026f2f6c8505595755f971ad.png\" /></p><p></p><p>通过对 Perplexity Copilot 底层技术的深入研究，还有专家称其灵感来自论文《FreshLLMs：使用搜索引擎增强更新大语言模型》（FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation）提出的搜索引擎增强大模型。</p><p></p><p>FreshLLM 提出了按搜索内容的发布日期顺序注入热门搜索摘要的想法。除了添加上下文之外，文章还建议配合少量提示词，引导大模型根据具体示例做出回应。论文作者还尝试了一种名为 FRESHPROMPT 的技术解决大模型无法回复实时问题的局限性，这项技术将来自搜索引擎的最新上下文信息注入经过预训练的大模型当中。</p><p></p><p>面对给定问题，这种方法会先在搜索引擎上查询该问题，检索全部搜索结果，包括答案框、相关结果及其他有用信息（包括知识图谱、公共问答平台上的信息，以及其他用户搜索过的相关问题等）。之后，再利用这些信息指导大模型对检索到的证据进行推理，基于多条提示词改善模型输出准确响应的能力。</p><p></p><p>Perplexity AI 底层以两套在线大语言模型为基础，同时借助内部数据承包商构建起高质量、多样化的大型训练数据集，打造了这么一套大模型搜索产品。这两套模型分别为 pplx-8b-online 和 pplx-70b-online，可以通过 API 公开访问，允许开发者将该技术整合进自己的应用程序与网站当中。</p><p></p><p>在 RAG based search 中，召回 + 排序出相关内容，然后再由模型来推理生成。在大模型同质化的年代，对于对话式搜索引擎来说，召回 + 排序才是核心竞争力。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c4/c482f0d1bac0288a09840a115fa8f661.jpeg\" /></p><p></p><p>而 LeptonAI，正如贾扬清所说，他们焦点在于一个帮助开发者构建人工智能应用程序的现代云平台，而不是做一个搜索引擎。那么基于此目的来通过调用已有基础架构方式构建出来的搜索引擎，其实也相对简单，所以能用不到 500 行代码来实现。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8\" /></p><p></p><p></p>",
    "publish_time": "2024-01-29 13:02:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "今年技术除了AIGC真没啥看头？别让“网红效应”遮住了真正的创新！",
    "url": "https://www.infoq.cn/article/gIXc8Twpa1MvWeT9XBsP",
    "summary": "<p>编辑 | Tina、褚杏娟</p><p>&nbsp;</p><p>过去一年，我们经历了许多意外瞬间，或许当时我们感到有些措手不及，但现在回首一望，这一切都变得一目了然。这正是我们每年对各领域进行盘点的意义所在，我们追求的是在迷雾中找到清晰的方向。</p><p>&nbsp;</p><p>而在2023年，这一盘点显得更为特殊。比尔·盖茨<a href=\"https://www.forbes.com/sites/alexkonrad/2023/02/06/bill-gates-openai-microsoft-ai-hottest-topic-2023/?sh=1ffa0a224777\">指出</a>\"，过去12个月人工智能领域发生的事情“与个人电脑或互联网一样重要”。大模型项目在过去一年中如雨后春笋般涌现，这波创新浪潮给各领域都带来了巨大的变化。</p><p>&nbsp;</p><p>然而，除了AIGC领域取得的突破外，在前端、架构、运维和云计算等领域中，也涌现了一系列引人瞩目的进步和革新。在年终盘点之际，InfoQ邀请到了黄玄（Hux）、曹立成（蒜蓉）、罗广明、董晓聪、杨振涛、张凯，分享在过去一年中各自领域的创新和进展，为我们揭示未来技术发展方向。</p><p>&nbsp;</p><p></p><h2>前端遇到麻烦了吗？</h2><p></p><p>&nbsp;</p><p>前两三年，前端技术的发展相对平稳，主要以 React、Vue 等成熟框架的演进为主。但今年，前端技术的发展呈现出新的活力。</p><p>&nbsp;</p><p></p><h4>编程技术的多样化</h4><p></p><p>&nbsp;</p><p>相比于过去“各司其职”井水不犯河水的光景，今年大前端领域的各种语言和技术边界都在面临打破和重建。</p><p>&nbsp;</p><p>新兴系统语言 Rust、Zig 已经通过 Rspack、Bun 这样的工具链切入到广大开发者的日常工作中。而 WebAssembly GC 的落地，以及 Static Hermes 这类 JavaScript 原生化探索，也继续宣告着大前端技术进一步“下沉”系统的趋势。</p><p>&nbsp;</p><p>另一边，无论是 React Native、Kotlin Multiplatform、Flutter 以及国内各大厂自研跨端技术的愈演愈烈，还是 Web 领域 JavaScript 框架 Next、Remix、Astro、Qwik、Fresh 纷纷侵蚀服务端的阵势，则宣告着大前端技术进一步在应用层“泛化”的趋势。</p><p>&nbsp;</p><p>我们有理由相信，虽然由 React，Vue 引领的声明式编程范式趋于稳定，但是在性能和应用元框架领域，大前端技术处处都在孕育着新的可能性，我们说不定就在大前端又一轮百花齐放的前夜。</p><p>&nbsp;</p><p></p><h4>终端平台多元化，前端迎来新机遇</h4><p></p><p>&nbsp;</p><p>在剧烈变化的环境下，大家可能会更关注生存问题，2023年，虽然“前端已死”的论调不绝于耳，但这一年也在终端平台上孕育了新的可能性。</p><p>&nbsp;</p><p>去年6月，在一年一度的科技春晚 WWDC 上，苹果发布了Vision Pro。目前，苹果已正式推出Vision Pro应用商店，百万款App准备上架；去年9月，Meta发布Quest 3，对打苹果。MR设备设备的发布，表明硅谷并不服气华尔街资本的短视，依然在为元宇宙成为下一代计算平台而蓄势待发，XR 与图形作为大前端的一个垂类，值得军备和持续关注。</p><p>&nbsp;</p><p>小米澎湃OS、vivo蓝河BlueOS等国产操作系统先后发布，HarmonyOS NEXT也在去年8月华为开发者大会上第一次公开亮相。其中，HarmonyOS NEXT的进展受到大量关注，华为的“1+8+N”战略，即以手机为核心的全场景智慧化（物联网）战略，一旦成功了，未来更多厂商 OS&nbsp;都会涌现出来，大家都可以摸着石头过河。</p><p>&nbsp;</p><p>这将能打破国内移动原生软件平台生态双足鼎立的现状，大概率会像小程序生态的碎片化一样，对国内大前端领域从框架到工程到行业分工，提出新的机遇和挑战。</p><p>&nbsp;</p><p></p><h4>鸿蒙大考，你准备好了吗？</h4><p></p><p>&nbsp;</p><p>今年9月鸿蒙将跟安卓彻底切分，仅支持鸿蒙内核及鸿蒙系统的应用。同时，原生鸿蒙的开发语言以ArkTS为主，不同于iOS开发使用的Swift语言，以及安卓开发使用的Java语言，且不支持打开APK文件，开发环境与IDE深度绑定，这意味着如果使用今年的最新版本，会跟iOS、安卓产生巨大的割裂。</p><p>&nbsp;</p><p>开发者需要维护包括iOS、安卓、Web以及鸿蒙在内的四端体验一致。生态体验是风险，但这对开发者来说，也代表着新的岗位的出现。</p><p>&nbsp;</p><p>在QCon闭门会上，有鸿蒙技术专家透露出一个特别积极的信息：鸿蒙开发供不应求，连外包开发价格都水涨船高。举例来说，假如原来一位外包价格在两千元左右，现在只要做过两个月的鸿蒙功能，价格就翻了一倍。做六个月的，价格可以达到5-6000元以上。</p><p>&nbsp;</p><p>他还表示，鸿蒙项目非常受欢迎，只要沾上边，就会有大批公司去抢人。尤其像美团、京东等公司，开出的价格都很高。</p><p>&nbsp;</p><p>鸿蒙官方表示，首批200+鸿蒙原生应用已启动开发，其中100+完成了鸿蒙原生应用Beta版本。</p><p>&nbsp;</p><p>鸿蒙适配之路，协议是第一步。就像盖房子需要地基一样，没有协议作为基础，开发者就难以下手。设备、教程、专家指导等关键资源，都依赖于双方明确的权利和义务。</p><p>&nbsp;</p><p>有企业向InfoQ表示，其与鸿蒙系统的合作目前仍处于前期阶段，尚未进入驻场开发环节。目前的工作主要集中在备忘录签署和深入调研适配过程所需的开发资源上，包括主应用程序的重写需求评估等。</p><p>&nbsp;</p><p>基于目前的研究，该企业认为适配鸿蒙系统存在一定难度，部分功能可能需要完全重写。为应对开发过程中的挑战，该企业的内部相关团队已开始进行技术储备。</p><p>&nbsp;</p><p>另一家商业银行表示已完成其鸿蒙应用的第一版demo，该版本基本涵盖了应用所需的功能，得益于采用了类似于H5的开发方式，使得大部分功能得以顺畅实现。然而，正如28定律所言，剩余的20%难题往往占据了80%的时间和精力。在该案例中，最大的挑战在于SDK适配，比如存在一些使用了不同企业的技术的SDK。该银行接下来将专注于解决这一问题，对接SDK并对每个业务进行深度调试，以确保应用的稳定性和功能完整性。</p><p>&nbsp;</p><p></p><h4>AI 会取代前端开发吗？</h4><p></p><p>&nbsp;</p><p>当然，今年的一切“之最”都离不开 2023 年作为“生成式AI 元年”带来的颠覆性变革，前端也不例外。大家都在研究怎么把这个黑科技融入工作流，让开发效率飞升。不过，也有不少人心里打鼓：“AI不会把我这份前端饭碗端走吧？”</p><p>&nbsp;</p><p>虽然今年还不用担心失业危机，但不可否认，AI确实为前端打开了一扇大门，潜力巨大！</p><p>&nbsp;</p><p>一方面，前端工作流程中的诸多环节，包括 PRD 到代码，从设计到代码，或者是 Github Copilot、Vercel 的 v0 这样的 AI 辅助开发，注定它会成为整个行业提效的重要手段。。</p><p>&nbsp;</p><p>另一方面， AI 也可以用来解决大前端面对的问题：前端本质上解决的是将信息映射为用户可以理解和交互的表现形式的过程，它在传统上非常依赖我们进行离线化和静态化的分析（比如产品经理的需求分析、交互与界面的设计、软件的硬编码等），而 AI 为这整个流程带来了一种实时在线的、动态化的可能。</p><p>&nbsp;</p><p>另外，随着大模型兴起，也有了一些AI native独立端开发，豆包、通义都有在做这种纯UI的应用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/eb/eb63307f0545072f4b36862193959ec4.jpeg\" /></p><p></p><p>截图为“高级前端开发工程师-大模型应用岗位要求”</p><p>&nbsp;</p><p>虽然现在的大趋势还是超级App，但移动互联网进入一个后期阶段后，就是朝着消费者的端智能的方向了。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>有更好的架构方法了吗</h2><p></p><p>&nbsp;</p><p>去年3月，谷歌开源了一个名叫<a href=\"https://www.youtube.com/watch?v=tb4NfUaU7yE\">Service Weaver</a>\"的框架。它能够实现简化本地开发，并将模块化单体应用转变为分布式微服务架构，在部署时允许自由配置组件的分布式部署方式，从而应对应用演进过程中的不确定性，并轻松适应组件间交互模式的变化。Jeff Dean也曾<a href=\"https://twitter.com/JeffDean/status/1631379386476953600?lang=en\">发推</a>\"称这是他的许多同事，包括其长期合作者 Sanjay Ghemaway开发的系统。</p><p>&nbsp;</p><p>谷歌描述了构建微服务架构的挑战：“维护多个不同的微服务二进制文件的开销显着降低了开发速度”、“分布式系统的问题（故障处理、广泛变化的延迟等）不会神奇地消失”&nbsp;。在去年6月份发布的<a href=\"https://dl.acm.org/doi/10.1145/3593856.3595909\">论文</a>\"中，谷歌称基于新提出的结构，他们能够将系统的延迟降低15倍，成本降低9倍。</p><p>&nbsp;</p><p>无独有偶，同样在去年3月，AWS也分享了一个<a href=\"https://www.infoq.cn/article/NU2Y3XiazG1cqiaNoXXa\">案例</a>\"，Prime Video 团队将他们的Serverless应用中的部分微服务调整成为了一个单体，称此举节省了 90% 的运营成本。</p><p>&nbsp;</p><p>谷歌和AWS的这波操作，跟过去十年大部分应用的开发思路反着来的：利用微服务边界进行快速本地开发；保证隔离，以便服务在运送到生产环境时可以组合；将微服务捆绑成大型二进制文件，以简化生产管理和相关服务的并置。</p><p>&nbsp;</p><p>这究竟是架构方法的革新，还是对取舍空间的进一步探索？</p><p>&nbsp;</p><p></p><h4>Service Weaver并不是微服务的“解”</h4><p></p><p>&nbsp;</p><p>从2017年起，微服务进入成熟阶段，微服务改造依然是当前趋势。随着互联网业务需求的增长，覆盖度和精细程度不断提高，维护一个模块需要数十人，协同合作出现巨大问题，需要专人负责代码合并，并选择一天统一上线。线上运行中，功能流量差异大，一旦出现故障影响全局。</p><p></p><p>解决这些问题的解法很简单也很复杂。简单说的是问题域大了，拆分成小的，问题自然就得到解决了。这就是微服务化。复杂说的是，拆分的原则没那么简单。原来的拆分从上而下，按产品按项目拆分即可，更多是组织决策就可以，技术架构的考量因素不多。但现在是要对一个相对比较原子的事物进行拆分，就必须对他所在的领域、公司业务所处的发展阶段、未来发展的重点、团队人员的能力等诸多因素综合考虑，才能得出拆分的方案。这也是微服务架构的魅力所在，也是业务架构师的核心价值所在。</p><p>&nbsp;</p><p>微服务改造是大势所趋，但引入了新问题。在单体架构下，对服务治理体系的要求较低，通信简单，服务感知和流量管控需求有限。然而，微服务模式中，每个请求会构建复杂的调用树，树上的节点几十几百起很正常。在这种模式下，再没有服务治理体系的化，研发效率会极大幅度降低。服务治理的整个体系，甚至其子体系都开始蓬勃发展，也衍生出不少流派。以注册发现为例，有基于客户端负载的模式，也有基于中心负载的模式。各种组件也是层出不穷，如zookeeper、consul、etcd等。</p><p>&nbsp;</p><p>微服务引起的问题不仅限于上述，服务数量增加必然导致人员需求上升。虽然效率工具可能改变人员与服务的关系，但趋势仍是增加。由于微服务拆分没有统一解决方案，每个企业和部门的架构师根据不同条件做决策，可能导致超前设计。一旦企业进入了降本增效的阶段，就会打破原本人员数量和服务数量的平衡。这时候微服务就会成为企业的技术负担。因此，一些企业选择回归单体架构，并取得显著成果。</p><p></p><p>对于常用而拆分过度的服务，需要考虑合并方案，但目前尚未出现一个统一的解决方案。</p><p>&nbsp;</p><p>Service Weaver 提供了一种全新的开发与部署解决方案，其最大的特点就是提供了一种灵活性和可配置性，对于业务的演进非常友好，可以灵活调整部署模式，来实现成本优化和价值最大化。</p><p>&nbsp;</p><p>但是，<a href=\"https://www.infoq.cn/article/48ah0aQ09mscYFLTOEU6\">这种架构模式并不适应于已经落地了微服务架构的业务</a>\"，也不适应于已经比较稳定的业务，更不适应于对于性能要求极高的业务。反过来说，Service Weaver 对于尚处于快速发展的互联网在线业务比较友好，允许应用程序随着时间的推移进行低成本演进。</p><p>&nbsp;</p><p></p><h4>2023年架构领域的关键进展</h4><p></p><p>&nbsp;</p><p>架构领域一直在不断演进和更新，在2023年，一些关键框架和组件经历了重要的更新或者取得了进展：</p><p>服务网格：更加成熟的实现和标准化。继 Proxyless Mesh，Istio 今年推出 Ambient Mesh 模式，并正式从 CNCF 毕业，成为 CNCF 活跃度排名第三的项目。开发框架：更多适用生产环境微服务架构的开发框架。以 Kitex/Hertz 为例的微服务框架更加关注企业用户在生产环境的落地和使用反馈，关注易用性和降本增效成为框架选型的主流意见。编程语言与生态：Golang 成为国内诸多大厂主流或最热门的编程语言，Golang 相关的开源中间件生态繁荣，竞争加剧；Rust 成为最有潜力的编程语言，诸多大厂纷纷投资入局，新的 Rust 微服务框架如 Volo 推动 Rust 在企业内部更广泛落地。</p><p>&nbsp;</p><p>据观察，前两年比较火的云原生可移植性设计 Dapr 框架在国内并没有得到广泛的采用。</p><p>&nbsp;</p><p>与服务网格相比，Dapr 架构更加复杂，Dapr 的工作模式是能力抽象，需要业务应用程序遵从标准 API 进行请求开发与改造。服务网格主要设计目标是低侵入，采用原协议转发的方式可以尽可能的降低对应用的侵入。Dapr 的主要设计目标是可移植性，即在跨云跨平台的前提下实现无厂商绑定，采用的方式是在将分布式能力抽象为标准 API，并在各种开源项目和云平台上提供这套标准 API 的不同实现，从而达到在不同平台上运行的目标。因此 Dapr 的代价是需要应用进行适配和改造，侵入性比较大。因此 Dapr 更适合新应用开发 （Green Field），对于现有的老应用（Brown Field）则需要付出较高的改造代价。这也是 Dapr 当前并未获得广泛采用的主要原因。</p><p>&nbsp;</p><p>虽然 Dapr 和类似的框架提供了许多优势和新颖的特性，未来仍需要更多时间、成熟度和社区的支持。在面对已有系统、组织惯例和技术选型方面，新框架的采用需要认真权衡其优势与现有解决方案的差异，并选择最适合的方案。</p><p>&nbsp;</p><p></p><h4>AIGC来了，架构师岗位会受影响吗</h4><p></p><p>&nbsp;</p><p>架构师就像是整个系统的设计大师，负责操刀整个系统架构的规划。这个规划不仅仅包括技术选型、架构模式、演进变化，还得考虑业务需求、团队能力、可运维性、成本等一系列不那么技术的要素。现在，架构决策很大程度上还依赖于人的经验和直觉，但如果我们能把设计和变更都记录得明明白白，把架构知识管理搞得井井有条，那么人工智能岂不是能搞定架构领域的一部分工作？这还是未知数。</p><p>&nbsp;</p><p>AI原生应用的发展现在还处于初级阶段，虽然我们还没看到AI在软件架构和设计上有多大的影响，但我们不能否认一点，AI肯定会给这个领域带来各种有趣的变革。</p><p>&nbsp;</p><p>比方说，AI可以帮我们提高决策效率、优化设计、增强系统的自适应性和安全性，还能更好地适应系统的演化和变化。当然，AI 技术在这方面的应用也需要考虑隐私以及技术成熟度等方面的问题。</p><p>&nbsp;</p><p>未来，我们可以期待AI在架构领域的应用逐渐增多：AI技术将更广泛地用于架构设计，包括AI辅助设计、决策支持与建议、智能监控等方面，从而提高架构设计的智能水平。看来，未来架构师团队里可能不只有人类，还可能有人工智能的！</p><p>&nbsp;</p><p></p><h2>运维困局，平台工程能否破局？</h2><p></p><p>&nbsp;</p><p>有些事情是我们预测不到的，Spotify 的 Backstage 开发者门户人气激增。此前被低估的举措，例如开发者体验，变得至关重要。</p><p>&nbsp;</p><p>云原生技术的加速采用，为软件交付及运行态的保障持续产生着深刻的影响，比如开发与运维的边界持续模糊，从而导致双方对系统的控制权也同步持续拉扯。</p><p>&nbsp;</p><p>随着大规模DevOps实践所面临的复杂度日趋提升，云原生时带的平台开发者们正在寻找新的解决思路、探索新的解决方案，平台工程则成为其中冉冉升起的一颗未来之星！&nbsp;</p><p>&nbsp;</p><p>CNCF应用交付TAG在今年先后发布了平台白皮书和平台工程成熟度模型，加之咨询公司对于平台工程发展趋势的乐观预测，让平台工程连续两年进入年度10大新兴技术趋势榜单，并认为中国的平台工程正在萌芽期。</p><p>&nbsp;</p><p></p><h4>新框架不断涌现</h4><p></p><p>&nbsp;</p><p>Backstage 正成为一股不可忽视的力量。从具体项目和实践案例来看，以BackStage为代表的开源项目正趁着内部开发者平台（IDP）等平台工程最佳实践快速发展，现已从CNCF沙箱项目进入孵化阶段。</p><p>&nbsp;</p><p>Backstage 集成了 Git 仓库、构建管道、API 和基础设施自动化等关键资源，将其无缝整合进一个单一门户，供所有开发者随时调用。根据GitHub 上的fork信息，梅赛德斯-奔驰、美国航空、爱立信等知名企业早已加入 Backstage 的行列。</p><p>&nbsp;</p><p>从趋势上看，早期实践者在组织内部明显地分化出了应用开发团队和平台开发团队，传统的运维工程师也经过了SRE实践阶段，分化成为通过运维平台来工作的专职应用运维和平台开发者，这在很大程度上验证了《团队拓扑》理论对于实践的指导意义。&nbsp;</p><p>&nbsp;</p><p>从行业共识角度，目前已涌现出CNOE、BACK&nbsp;Stack、KAOPS等框架和实践指南。</p><p>&nbsp;</p><p>其中，CNOE为AWS联合Adobe、Salesforce等企业推出的一项用于构建内部开发人员平台 (IDP) 的开源计划。BACK Stack，代表 Backstage、Argo、Crossplane 和 Kyverno这四个工具组成的一个强大的组合，可以实现安全且可扩展的自助服务工作流程。而KAOps则提供了一种集成 DevOps 持续交付和多云服务的创新方法。</p><p>&nbsp;</p><p>这些框架为更加系统化地实践平台工程奠定了基础共识度，也有效促进了技术生态的持续发展和相互融合。</p><p>&nbsp;</p><p></p><h4>AI全面入侵：未来的运维工作模式如何进化？</h4><p></p><p>&nbsp;</p><p>结合AIGC与AGI的发展趋势来看，AIOps智能化运维方面的探索已过渡到参考自动驾驶的L0-L5成熟度模型来度量的阶段&nbsp;，这使得行业开始从整个软件的全生命周期来思考AI的赋能和提效。涉及的领域包括需求工程、设计开发、构建与集成、质量保障、持续发布与运行维护、故障分析定位等。业界甚至提出了一个面向未来的、由不同技术方向的AI Agent组成的开发团队的构想。这些前期的探索和畅想仍然强调了开发过程的标准化和资源的平台化，要求整个软件研发过程都能够友好地与AI协同工作。在这方面，来自Vercel的v0.dev是一个典型代表产品，它能够根据自然语言指令生成即时可视的前端页面，并自动部署到Vercel服务上。</p><p>&nbsp;</p><p>在接下来的2024年，我们预测平台工程理念以及实践将更进一步随着云原生技术的加速采纳而深刻影响软件交付与运行保障，DevOps理念中的左移将进一步发展为左移结合下移，平台的价值会得到更大程度的重视，认知负荷过重的现象对于开发和运维角色来说将会有一定缓解。</p><p>&nbsp;</p><p>同时，结合AIGC与AGI的发展趋势，以AIOps、知识库与问答机器人、流程机器人、代码生成等为代表的应用场景将进一步得到深化和拓展，为整个软件工程行业带来效率提升；至于软件研发模式方面，短期内依然会保持现状，但我们不得不在软件设计方面考虑到面向AI的API。&nbsp;</p><p>&nbsp;</p><p></p><h2>云计算的新战场</h2><p></p><p>&nbsp;</p><p>今年，受益于AIGC的快速发展，云计算领域的主题基本都是围绕助力AIGC来做能力提升。</p><p>&nbsp;</p><p>从用户群体来看，云计算和大模型用户没有很大差异，但关注点会有不同。比如大模型厂商或创业公司最为关注资源的交付；而有的企业是希望在自有产品中快速部署已有的成熟模型，并快速验证；更小众的用户则更关注LLM等生成式AI模型本身的发展，不仅要高效使用资源，还要借助云原生能力将模型能力转化为自己的 SaaS 服务，继而对外售卖或提供智能服务。</p><p>&nbsp;</p><p>总之，除了传统云原生客户，这些新用户的成熟度更高。他们目的是探索AI 带来加速业务创新的可能性。还有在支持大模型生产和落地方面进行的能力和需求沉淀，也促进了云计算自身的新一轮迭代。</p><p>&nbsp;</p><p></p><h4>云原生 AI，更重资源效率和工程化交付效率</h4><p></p><p>&nbsp;</p><p>云原生与AI结合领域被业内称为云原生 AI，目标是利用云原生的标准化、可扩展性、弹性等技术优势，为AI模型生成加速，为AI服务交付提效。主要包括下面三部分：</p><p>&nbsp;</p><p>IaaS 资源层，包括高性能存储、计算、网络等基础设施。工程平台，包括云原生和以容器化形式交付的云原生 AI，提供基础异构资源的调度和任务管理、对各种AI计算框架的支持、MLOps生命周期管理、模型开发，训练和推理，以及后续服务化的运维等。AI PaaS 平台，用一站式的用户体验，构建、训练，部署和管理数据，模型和服务。</p><p>&nbsp;</p><p>首先，对于 IaaS 层来说，AI为其带来了规模、性能和效率方面的挑战。为了训练出一个对通用知识或专业领域知识有相当理解和推理能力的大模型，模型参数量往往会超过百亿，甚至千亿。这就需要高达万卡 GPU集群的算力管理规模；爆炸的数据量将存储提高到了PB级、吞吐达到TB/s级；网络进入到800Gbps，甚至达到单机3.2Tbps RDMA这样的高性能要求。</p><p>&nbsp;</p><p>为此，在计算上，各家都在卷 GPU 芯片。一定程度上，对于像大模型厂商这样对算力要求极大的用户来说，芯片储备成为选择云厂商的首要考虑因素。但是GPU的选择很少，国外基本只有英伟达。但在新禁令情况下，国内各厂商基本很难拿到高性能卡，只能寻求性价比高的\"阉割版\"或国产化替代，这更加大了国内自研芯片方面的力度。但是，目前这些措施，对于高性能卡缺失带来的市场弥补有限，很多自研芯片更多是厂商内部使用。</p><p>&nbsp;</p><p>网络方面，厂商的做法更是简单粗暴：资金充足就用 InfiniBand（无限带宽），不足就用 RoCE（RDMA over Converged Ethernet）。国内外基本都是满配单机从 800G 到 3.2T 的标配、集群弹性支持几万卡的规模。各家也有自研高性能网络项目，在做产品化和商业化的尝试。</p><p>&nbsp;</p><p>存储方面则是在传统架构的分布式文件系统，或者并行文件系统上进行自研增强。但这种模式在大模型应用中，先前的高性能存储还不太适用聚合带宽压力骤增的场景。传统存储是通过横向容量扩展提升带宽能力，这会带来成本的增加。也有不少用户在尝试基于弹性更好，更廉价的对象存储服务的方案。但仍需要大幅优化训练场景下的数据访问速度。因此在架构上，今年较为明显的一个趋势是各厂商尤为关注数据缓存层的构建。</p><p>&nbsp;</p><p>当然，根据模型的参数规模、数据量、预训练还是微调等的不同，大模型对底层基础设施的需求也不一样。具体到预训练来说，国内各厂的基本做法就是网络带宽、单机满卡满配形成万卡乃至更大的十万卡集群，而高精度的网络拓扑则从原先的三层压缩至两层，从而增加可扩展性并减少跳数。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>传统的资源交付多以特定规格实例的形式进行，通过 配置网络、存储、计算等资源方面的需求，在虚拟机或容器实例层面进行集群管理和任务编排调度。但目前 AI 的计算资源类型在性能与成本方面有很高提升，传统交付形式意味着使用者需要自行把控资源的利用率，并且可能带来较高的 TCO (total cost of wonership)。因此，业界也在寻求更为极致的（以秒级)按需弹性交付和计量方式。</p><p>&nbsp;</p><p>目前，Serverless 是业内较为推崇的资源交付形式。Serverless 可以弹性优化资源利用情况，根据资源的真正使用情况自动扩缩容，减轻使用者对集群管理、环境一致性、健康状况检查、错误处理等底层资源运维的负担。但是，这种只购买资源的模式，意味着使用者可能会还需要承担自建AI平台所带来的维护复杂度。因此，也有厂商还提供了软硬一体，以serverless形态交付的AI平台服务。比如阿里云的PAI灵骏智算服务。</p><p>&nbsp;</p><p></p><h4>支撑AI 复杂任务，容器等云原生技术还有哪些短板？</h4><p></p><p>&nbsp;</p><p>现在，云原生对人工智能的支持更多是利用其可扩展架构、标准化交付及弹性等自身优势，加速 AI 能力的生产过程。或者说，AI 是云原生平台上的一种特殊类型的工作负载。</p><p>&nbsp;</p><p>实际上，深度学习、大数据处理等数据计算密集型任务已经广泛采用容器、Kubernetes、微服务等一系列云原生技术。比如，OpenAI为其大模型训练提供可扩展的基础设施， 在2021年就已经将Kubernetes 集群扩展到 7500 个节点。这些任务的计算规模和复杂度远比 Web、微服务等互联网应用要高。</p><p>&nbsp;</p><p>Web 应用可能只需要简单横向扩展实例副本数，就可以提升服务性能和可用性。但数据计算密集型任务自身会有复杂的拓扑关系，一个任务又会细分多个子任务，子任务之间还有逻辑关联，比如数据依赖、时序关系、执行顺序等其他逻辑上的依赖。再加上任务的状态转化和对异构资源的高消耗，对 CPU、GPU、内存容量、内存带宽、网络、磁盘IO等资源的协同敏感，导致任务无法轻松地横向扩展。</p><p>&nbsp;</p><p>Kubernetes或容器在支持这种复杂任务类工作负载方面还很欠缺。体现在对异构资源的协同优化管理，以及对Batch任务的调度和整体可扩展性上。为了支持AI、机器学习这样的工作负载，Kubernetes就需要做很大的增强，包括核心调度、异构资源统一管理、利用率优化、可观测性、故障诊断和自愈等，甚至整体的架构和生态都需要做很多增强。所有在容器和Kubernetes底座上进行的增强，被称为云原生AI基础服务层。</p><p>&nbsp;</p><p>云原生的优势在于标准化交付，将业务应用中的运维、架构、DevOps 统一化。企业 IT 可以将更为复杂的数据计算型任务迁移到同一套技术堆栈上，用统一的标准交付模式和 API 来支撑不一样的工作负载，通过弹性和混合调度等手段的综合应用，从而达到整体降本增效的目标。这是一个比较长期且有远见的架构演进上的诉求。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/db/dbda863b4fa48b26ea06de50ad6e34af.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>上图是一个非常典型的云原生AI系统分层架构。</p><p>&nbsp;</p><p>最底下的是高性异构资源管理，包括对高性能计算、存储、网络的统一管理和运维；上面是AI任务调度和流水线的构建。再往上就是为了运行各种各样的计算框架或者训练、推理的运行时，要做容器化支持；当运行时和框架跑起来后，就要关注如何不断优化任务性能，优化方法除了算法和框架以外，还有非常多手段相互配合；性能优化之上，要管理整个AI作业的生命周期；而所有这些能力都需要用统一的工具链和统一的标准API向上暴露给整个生态，既可以集成开源生态、私有业务系统，也可以跟第三方生态的集成。</p><p>&nbsp;</p><p>从支持系统看，弹性运维和安全始终贯穿其中，而厂商需要对客户的数据和模型进行统一管理，并对效率之外的数据安全和隐私做好保护工作。</p><p>&nbsp;</p><p></p><h4>MaaS发展未定，云厂商的“野心”能否实现？</h4><p></p><p>&nbsp;</p><p>很多产品都在用 LLM 进行增强甚至重构，其中包括智能诊断、AIOps 等在云服务使用助手的场景。模型生态的繁荣会吸引更多新型业务应用围绕 AI 模型关联或集成，更快更广地发展出更多新应用，间接帮云厂商接近了客户的应用需求，往上面向更多需求和机会，往下承接更大的资源消耗。</p><p>&nbsp;</p><p>IaaS层只是最基本的支持，AI PaaS、AI SaaS也成为云厂商们提供附加值的关键之一。在这方面，各家也有自己的平台。AI PaaS 包括数据，模型等资产管理平台，模型开发平台、模型训练平台、模型推理平台，还有各行业解决方案。AI SaaS 则更多关注个人工作效率和企业在流程化、规章制度和执行等方面的效率提升，这些都可以交给 AI 工具来完成。</p><p>&nbsp;</p><p>今年，很多云厂商也纷纷发布了自己的大模型，打造自己的MaaS（ Model as a Service）服务。MaaS则包括了底层的基础设施、模型相关能力及产品和场景应用等，主要就是以大模型为核心提供场景化智能服务。</p><p>&nbsp;</p><p>出于安全问题或领域定制性较强而外部模型无法达到预期效果等考虑，进行知识库的微调或增强等来自研模型是可以理解的，但效果如何可能要先打个问号，目前国内还没有成熟案例出来。</p><p>&nbsp;</p><p>各家的大模型虽然分层差不多，但如果抽象力度够高，那每层内容展开后也有很多：从下层智算IaaS到 AI PaaS 或面向云原生的 AI，再到最上面服务生态的 MaaS 层以及垂直化领域的各类配置化模型。但对云厂商来说，MaaS 商业模式可能只是间接的，但其影响力会更大。现阶段还是会以规模为主，是否会引发新的商业模式还是个未知数。</p><p>&nbsp;</p><p>12月，科技市场分析公司 Canalys 的一份<a href=\"https://www.cnbc.com/2023/12/27/ai-boom-fails-to-propel-chinas-cloud-market-growth-.html\">报告显示</a>\"，人工智能热潮尚未推动中国云市场增长，“中国云服务市场仍然保守，严重依赖政府和国有企业。”</p><p>&nbsp;</p><p>未来，离业务更近的云原生技术与大模型会有更多集成，这对本身就有很多业务场景的企业来说更为有利。在多云和多集群等更为复杂的环境中，业内也在探索进行统一的 AI 能力交付。此外，在国产化背景和异构芯片现状下，业内将在降低复杂度和提效上努力。</p><p>&nbsp;</p><p>采访嘉宾简介：</p><p>黄玄（Hux），字节跨端与 Web 架构师，前 React 团队核心成员</p><p>曹立成（蒜蓉），淘天集团 1688 终端架构负责人</p><p>罗广明，字节跳动服务框架团队架构师，CloudWeGo 开源负责人</p><p>董晓聪，作业帮基础架构负责人</p><p>杨振涛，vivo互联网研发总监，PECommunity平台工程社区发起人</p><p>张凯，阿里云云原生应用平台容器智算负责人</p><p>&nbsp;</p><p>更多阅读：</p><p><a href=\"https://www.infoq.cn/article/32WOMJbur2ytBY09SIqI\">你当初被谁“忽悠”上了云，现在又在被谁“忽悠”下云？｜盘点</a>\"</p><p><a href=\"https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU\">挑战 Spark 和 Flink？大数据技术栈的突围和战争 ｜盘点</a>\"</p><p><a href=\"https://www.infoq.cn/article/I1oy5usjmBX6ZZ1SawUO\">今年向量数据库“杀疯了”，但纯向量数据库“凉”了？|&nbsp;盘点</a>\"</p><p><a href=\"https://www.infoq.cn/article/9YctNwMoOrKyX8b4H2oz\">颠覆软件工程、“杀死”开发者？回溯大模型落地应用这一年 |&nbsp;盘点</a>\"</p><p><a href=\"https://www.infoq.cn/article/grTehb05ZU7yJj93LVHi\">并发王座易主？Java 21 虚拟线程强势崛起，Go &amp; Kotlin 还稳得住吗 |&nbsp;盘点</a>\"</p><p><a href=\"https://www.infoq.cn/article/1X5jJEzrsIa5M1HR3RRz\">国产编程语言新拐点：聊聊从 Mojo 到 MoonBit 的思考 |&nbsp;盘点</a>\"</p><p><a href=\"https://www.infoq.cn/article/BrtGN23K5fzhoG9tzSRC\">大模型时代，我们可以用 Julia 做什么？|&nbsp;盘点</a>\"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2024-01-29 14:41:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "零售业海量场景下 ToC 系统的数据库选型和迁移实践",
    "url": "https://www.infoq.cn/article/Pr2Ji3x030OGaECKtLQO",
    "summary": "<p>云盛海宏是一家零售业科技公司，以科技的力量为门店和线上客户打造 360 度的优秀体验，目前服务中国 6000 余家的线下门店和千万级别的线上会员。云盛海宏的 To C 系统分为私域商城和会员营销两条业务线，它为 7000 多万注册会员提供了丰富的权益和服务，是我们非常核心的系统。</p><p></p><p></p><h1>选型背景</h1><p></p><p></p><p>随着近几年消费模式的升级，我们和消费者的互动与服务从传统线下逐渐延展至线上，使得 To C 系统的能力和规模越来越大，其数据库压力也越来越大。</p><p></p><p>最初在建设 To C 系统时，业务库主要使用 MySQL，既有单库架构，也有分库分表架构，时至今日我们面临的问题主要如下：</p><p></p><p>1、分库分表不合理导致的数据倾斜，某个分片负载居高不下，且难以动态调整</p><p></p><p>    a. 分库分表规则为品牌名称，而不同品牌之间数据规模、用户规模有较大差异</p><p>    b. 要针对大分片再次进行二次拆分才能解决该问题，但同时复杂度将大幅提升</p><p></p><p>2、个别单库架构的 MySQL，数据增长远超预期，单表数据量过大，性能问题凸显</p><p></p><p>    a. 数据量千万级以上表：87 张；亿级以上表：21 张</p><p>    b. 需要将单库架构改造成分库分表架构才能解决</p><p></p><p>以上两个问题均需要大幅调整数据库架构来解决，解决成本高（人力、硬件），并且未来还可能再次面临这样的问题。为彻底解决以上问题，我们计划直接切换到原生分布式数据库 TiDB：</p><p></p><p>1、TiDB 兼容 MySQL 协议，并且是原生分布式，无需规划分片规则，对应用友好，能够很好的解决之前分库分表数据倾斜的问题</p><p></p><p>2、TiDB 架构下提供的动态水平扩展、热点自动调度等能力，大幅简化了一系列运维成本，能够支撑应用规模持续的增长，即使数据增长超过预期也能动态增加节点解决 </p><p></p><p>3、另外我们的零售系统在去年成功切换到 TiDB，也给了我们团队很大的信心</p><p></p><p></p><h1>数据库测试方案</h1><p></p><p></p><p>对于数据库的切换我们比较关心以下几个问题：</p><p></p><p>迁移数据的完整性：数据是企业的核心资产，不容许丢失SQL 兼容性及性能：这意味着我们迁移改造的成本资源隔离能力：多个业务库合并后如何保障其服务质量</p><p></p><p>测试目的：识别关键问题，基于测试结果完善应用改造</p><p></p><p></p><h2>测试一：迁移数据的完整性</h2><p></p><p></p><p></p><h3>数据同步</h3><p></p><p></p><p>TiDB 提供 DM 数据同步工具，该工具支持 MySQL 全量、增量数据的同步，同时也支持分库分表的合并。对于分库分表的合并，我们的任务策略如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e12397079f693713b9e600ce0e95a53a.png\" /></p><p></p><p></p><h3>数据比对</h3><p></p><p></p><p>为确保 DM 数据同步工具的可靠性，在切换过程中需要进行数据一致性校验。实测数据比对效率较高，能够达到 400MB/s 以上的全量比对速度，以下是数据比对映射关系：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc2e6b51209b55329f52d6ed798a5df7.png\" /></p><p></p><p></p><h2>测试二：SQL 兼容性及性能</h2><p></p><p></p><p>针对生产的全量 SQL 语句进行兼容性以及性能的测试，靠人力手工完成测试是不现实的，所以我们引入了 Percona 开源的 playback 工具进行测试。</p><p></p><p></p><h3>playback SQL 回放工具经验分享</h3><p></p><p></p><p>playback 工具介绍</p><p></p><p>项目地址：https://github.com/Percona-Lab/query-playback.git</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fc/fc64feee1e199c0b92ff07b8643c50d0.png\" /></p><p></p><p>SQL 录制：MySQL 数据库在开启慢查询功能时，会将慢 SQL 输出到慢查询日志</p><p></p><p>SQL 回放：playback 工具解析慢查询文件中的 SQL，并连接到目标数据库进行回放</p><p></p><p>报告展示： 回放完成会输出报告（执行失败的 SQL 含结果不一致等、性能数据）</p><p></p><p>实际测试流程</p><p></p><p>由于我们是存在分库分表架构，而 TiDB 中存储的都是单表，所以我们步骤进行了一些调整：</p><p></p><p>SQL 录制： 将生产 MySQL 库的 long_query_time 设置为 0，运行一个业务周期（一天），记录一天内所有 SQL（样本数越大测试结果越准确）</p><p></p><p>SQL 处理：部分慢查询日志未记录 schema 信息，通过脚本指定 schema（还存在将 db_1 映射成 db 这样的 schema 转换）</p><p></p><p>SQL 回放： 指定慢查询回放整个业务周期运行的 SQL 语句</p><p></p><p>回放结果分析</p><p></p><p>测试结果汇总</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3f9a1dc1c63cd8f71d962c901650c68b.png\" /></p><p></p><p>由于私域商城大表十分多，所以性能提升非常明显，2524 万条 SQL 的总执行时间约之前的 1/6；而会员运营之前进行过拆分，737 万条 SQL 的执行总时间约之前的 1/2。</p><p></p><p>错误详情分析：</p><p></p><p>会员运营：</p><p></p><p>1 处业务 SQL 错误：“during query: Data too long for column”，原因字段精度不够，调大后解决，其余业务 SQL 均兼容</p><p></p><p>剩余 1220855 次均为非业务 SQL 的报错：如 MySQL 中\"show binary logs/status/events\"、set 特有变量、系统表查询，或慢查询格式调整时出现的一些格式错误等</p><p></p><p>私域商城：</p><p></p><p>无业务 SQL 错误，业务 SQL 均兼容</p><p></p><p>所有错误均为非业务 SQL：如 MySQL 中\"show binary logs/status/events\"、set 特有变量、系统表查询，或慢查询格式调整时出现的一些格式错误等</p><p></p><p>兼容性基本没有问题</p><p></p><p>性能详情分析：</p><p></p><p>虽然总体执行时间缩短了，但我们还是需要排查下性能退化的 SQL 是哪些，需要保证原本正常的 SQL 还是要处于在一个基本对用户无感知的响应时间范围。</p><p></p><p>理论上来说，小于 100ms 的 SQL 基本都不影响前端用户的体验，所以分析时可以忽略这一部分的 SQL；而对于 100ms-1s 的 SQL，可能会影响用户体验，需要关注；1 秒以上时基本上用户感知非常明显。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/45/4567b23d34b6275d4a01548e0fce8528.png\" /></p><p></p><p>通过详细性能分析数据以及 SQL 回放执行总耗时，我们不难发现：</p><p></p><p>1、由于 TiDB 是存储计算分离的分布式架构，1000us 内的 SQL 数很少，基础操作（如 show variables/start transaction/set ... 等）执行时间均高于 MySQL；同时另一个极端，大于 10 秒以上的 SQL 数，两个系统在 TiDB 中下降了一个数量级。</p><p></p><p>2、通过一些采样分析，我们发现在 TiDB 中一些 commit/rollback 操作的时间也普遍高于 MySQL，个别操作从几百微秒变成几十 / 几百毫秒。查阅了 TiDB 中的事务机制，发现 TiDB 提交成本高于 MySQL，首先是 2PC 跨节点事务，另外就是事务中的脏数据直到 commit 时才开始刷到存储（计算节点 -&gt;存储节点），对于这种类型的 SQL 在性能分析时也可以忽略掉。</p><p></p><p>3、我们将样本数据整理成桑基图，将这部分性能退化、并且影响用户体验的 SQL 识别出来，进行分析和优化</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c74cc839bf583b04b7238e3014c595fc.png\" /></p><p>                     以上为会员运营中 SQL 性能数据桑基图，如红色箭头以及红色框的这些 SQL，需要重点分析</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/01965828493252e0f7f1db113529258c.png\" /></p><p>                                       以上为会员运营中原本 10 秒以上 SQL 性能变化</p><p></p><p>私域商城的 SQL 性能提升很明显，100ms 内 SQL 数量均高于 MySQL，同时 1s 以上的 SQL 少于 MySQL，说明用户体验提升明显。但还是需要根据桑基图来分析是否存在异常的 SQL</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1d/1d981b15a7859d8417e160161a79893d.png\" /></p><p>                                  以上为私域商城系统 SQL 性能桑基图，红框对应的 SQL 应该重点分析</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/daee0e0caf4c13a536351a0b27bb94dc.png\" /></p><p>                              以上为私域商城原本 10 秒以上 SQL 性能变化</p><p></p><p></p><h2>测试三：资源隔离能力</h2><p></p><p></p><p>资源隔离能力在我们这边的用途：</p><p></p><p>a. 系统间资源隔离：多个 MySQL 库上的应用系统合并到一个 TiDB 时，如何保障各个系统在业务高峰期的可用资源</p><p></p><p>b. 系统内资源隔离：</p><p></p><p>   i . 当某个系统中出现一个大查询时，如何限制其资源消耗，避免对该应用、对整个集群造成影响</p><p>   ii . 当某个系统中批量调度作业到白天还没跑完时，如何限制其资源消耗，避免对白天业务造成影响</p><p></p><p>c. 其他场景的资源隔离</p><p></p><p>   i . 应用监控等定时调度操作往往比较复杂，如何限制其运行时的资源消耗</p><p>   ii . 客户端数据查询场景难以避免 SQL 条件不规范的情况，当出现这种情况时，如何避免人工查询导致的系统不可用</p><p></p><p>为解决以上几种问题，需要使用 TiDB 7.1 LTS 提供的 Resource Control 功能，该功能能够实现：</p><p></p><p>按用户设置资源规格按会话设置资源规格按 SQL 设置资源规格</p><p></p><p>以下是用户级别测试效果：</p><p></p><p>为数据库压测用户指定其 RU 为 500，并使用 Jmeter 压测应用，观察 TiDB 数据库是否能够限制资源，并且在达到资源限制时，应用是否报错。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f734fa792e43dc5fa6dfe434f8556c53.png\" /></p><p></p><p>该用户在达到 500RU 时，使用值轻微超过限制值，基本符合预期。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6e/6e73505706d43c208453e3ed784b1d79.png\" /></p><p></p><p></p><h1>应用改造</h1><p></p><p></p><p></p><h2>分页 SQL 增加排序条件</h2><p></p><p></p><p>这也是几乎所有的 MySQL 系统迁移到 TiDB 会遇到的问题：</p><p></p><p>当 SQL 中无显示排序条件时，返回结果无顺序保障，这将导致分页结果不可靠</p><p></p><p>我们大概梳理了系统中存在的分页 SQL，大概 1600 余条，最终改造 + 测试工作量约 2 个月</p><p></p><p></p><h2>性能退化的 SQL 优化</h2><p></p><p></p><p>如特定的表关联方式，执行计划是全表扫描</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4f/4fac66e5d09db1cd52cc963fff8cca92.png\" /></p><p></p><p>改写成</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/490ba005651ca2fbca9d02fd4f33e7d8.png\" /></p><p></p><p></p><h2>从分库分表处理逻辑改成单库处理逻辑</h2><p></p><p></p><p>业务 sql 中存在批量查询、批量更新的场景，调整成按照用户链接维度设置 batchquery</p><p></p><p></p><h2>数据回写改造</h2><p></p><p></p><p>应用切换到 TiDB 前，需要将 TiDB 的增量数据写回到 MySQL，保障紧急情况下的可回退：</p><p></p><p>之前是单库的场景，可以直接使用 TiCDC 提供的 mysql sink 完成回写。</p><p></p><p>分库分表的场景下，TiCDC 并不能直接写 MyCAT 组件；所以我们先将增量数据通过 TiCDC 发送给 Kafka，再消费写入到 MyCAT 下的分片中。</p><p></p><p></p><h2>下游订阅改造</h2><p></p><p></p><p>TiDB 不兼容 MySQL Binlog，原本的消息订阅链路（Binlog/canal/kafka）需要换成 TiCDC-&gt;Kafka 这条链路，TiCDC 提供 canal-json 格式的兼容，消费程序上要基于 TiCDC 的消息格式进行一定的调整。</p><p></p><p></p><h1>生产切换效果</h1><p></p><p></p><p>我们于双十一之前的两周完成消息中心等系统（4 个 MySQL 库）的切换，切换到 TiDB 后经受住了双十一大批量消息推送的验证，也增强了我们的信心。</p><p></p><p>在元旦后第一个工作日进行了私域商城系统（16 个 MySQL 库）的切换，切换过程比较顺利。以下是切换后第一个工作日的业务高峰，最大 QPS 4.4 万，P95 响应延迟 3.9ms，整体运行良好。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ec/ec945f96d163d1d348ccb9a7753fe6e5.png\" /></p><p></p><p>1.8 日某品牌大促，业务量是平时的一倍，数据库最大 QPS 6.5 万，P95 响应延迟 3.9-4.5ms 之间：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b73942ddc4a719aeb58b8b969e80514d.png\" /></p><p></p><p>以下是切换 TiDB 的整体流程，可以看到切换到 TiDB 后了简化了其架构：由于 TiDB 无需设置分片规则，数据都在一个集群中，原本综合库（MySQL 单库）上的查询也直接切到 TiDB 中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/edab4d239128b4c4f4d55e54d94160c3.png\" /></p><p>                                                    以上为生产切换流程</p><p></p><p></p><h1>总结</h1><p></p><p></p><p>数据库迁移是一个复杂且高风险的工程，迁移前规划一个全面的测试方案必不可少，提前识别迁移风险，大幅降低迁移后的风险，当然像分阶段迁移、回退链路等保障措施也及其重要。</p><p></p><p>年后我们将继续把会员运营系统（20 个 MySQL 库）切换至 TiDB，实现 To C 系统从 MySQL 40 个库到 TiDB 的整体切换，支撑未来持续增长的数据规模。</p>",
    "publish_time": "2024-01-29 14:55:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2024年入局大模型，晚了吗？",
    "url": "https://www.infoq.cn/article/GDd1kvi4MGNqs48UofG5",
    "summary": "<p>&nbsp;</p><p>在过去的一年里，我们见证了大模型领域的迅猛发展，超出了所有人的预期。ChatGPT等开源模型正在以惊人的速度进行技术迭代，诸如RHF、BERT等技术都在迅猛演进，甚至小模型与专家模型的混合也崭露头角。千行百应的企业都在竞相将大模型应用于自己的业务中。</p><p>&nbsp;</p><p>在经历了大模型一年多的蓬勃发展之后，我们借助年终这个节点，停下脚步，回顾一下大模型在过去一年中所取得的成就与面临的挑战。</p><p>&nbsp;</p><p>InfoQ《极客有约》邀请Hugging Face 工程师王铁震，对话新加坡国立大学校长青年教授、潞晨科技董事长尤洋和AI 领域知名投资人陈于思，聊聊2024年的大模型发展。</p><p></p><p>完整版视频可查看：</p><p>https://www.infoq.cn/video/eqoaOs3Fy9844iUgHNEx</p><p></p><h2>2023年大模型，哪件事让你印象深刻</h2><p></p><p>&nbsp;</p><p>王铁震：首先请两位嘉宾做下自我介绍，聊聊今年都做了哪些事情？</p><p>&nbsp;</p><p>尤洋：我目前在新加坡国立大学担任教职，同时也是潞晨科技的创始人和董事长。我的研究始于高性能计算，旨在优化数据移动和各种操作的计算速度，例如浮点运算。</p><p>&nbsp;</p><p>大约十年前，我们使用几百卡、几千卡的设备进行训练，主要应用领域是地震天气模型。在这个过程中，我意识到无论模型应用在何种领域，最终都涉及底层的矩阵运算。简而言之，我们追求的目标就是让计算变得越来越快。</p><p>&nbsp;</p><p>在四五年前，当时AI模型并不是很庞大时，我们尝试使用上百卡、上千卡进行训练，但只能采用数据并行的方式，即将一个批次的数据分成多份，分配到不同的GPU上，然后每个服务器计算或者更新梯度。在与Google合作的过程中，我们成功将Bert的训练时间从3天缩短到76分钟。随后，我在加入加利福尼亚大学伯克利分校并获得博士学位后，回到新加坡国立大学任教，并创建了高性能人工智能实验室。目前，我们实验室拥有13名博士生、7名博士后和十多名硕士生，成为一个规模不小的实验室。</p><p>&nbsp;</p><p>在2021年，我有幸接触到了李开复老师的团队，并在任博冰先生的推动下，共同创立了潞晨科技。逐步得到了一系列投资，包括创业工场、红杉等知名机构以及百强安区基金。我们目前专注于产品的技术商业化。</p><p>&nbsp;</p><p>在回顾整个2023年，如果要提及一件对业界有着重大影响的事情，我认为Llama是一个值得关注的焦点。虽然ChatGPT于2022年底推出，但我认为Llama的出现为整个产业带来了更多的可能性。我相信人工智能的产生具有长期的价值，未来的10到30年不仅仅是高端玩家的领域，而是具有广泛的社会影响。我坚信人工智能的能力具有广泛的受益性。在未来3到5年内，可能不会是严格意义上的Llama，但类似于Llama的模型，拥有数百亿的参数，比如70B、3B的，我认为对于广泛推广人工智能的能力将起到至关重要的作用。如果我只能选择提到2023年的一件事，那么像Llama这样的模型在人工智能领域的出现，我认为具有长期的战略意义。</p><p>&nbsp;</p><p>陈于思：我是陈于思，毕业于斯坦福大学，专攻电子工程博士学位。我的博士研究集中在当前备受关注的技术领域，即芯片之间的高速互联。我们当时观察到，随着摩尔定律的继续，单一芯片的计算能力可能会达到极限，因此要实现更高的计算能力，需要将更多芯片进行连接。于是，我与MIT、伯克利、哈佛等多所高校合作，共同推进了一个涉及芯片之间光互联的大型项目。</p><p>&nbsp;</p><p>在那个时候，AI 尚未兴起，因为那时我大约20岁，研究时间跨足2011至2015年，深度学习刚刚起步，一块GPU可能已经足够。因此，我在博士毕业后转而从事界面设计，并加入硅谷的麦肯锡公司。从那时起，我开始更加关注人工智能，包括机器学习等新技术在商业领域的应用和商业化机会。回国后，我加入平安集团，负责整个集团人工智能战略和实际应用。</p><p>&nbsp;</p><p>在2018年，我们尝试使用上一代Transformer模型，如BERT，将其应用于智能客服领域。然而，当时模型的能力可能还不够强大。从2019年开始，我加入了一家基金公司，致力于全球范围内的人工智能相关投资。我投资了一家名为OmniML的公司，今年3月被NVIDIA收购。此外，还投资了【金纳豆、AIZMO】等一系列与AI相关的公司。另一方面，由于铁震是Hugging Face的创始人，我一直关注中国开源行业的发展，并曾被评选为中国开源先锋33人。</p><p>&nbsp;</p><p>在回顾整个2023年，我认为可以用一个词来总结，那就是“Scaling Law”（扩展定律）。在我看来，这一年是Scaling Law持续发扬光大的一年，而且在可预见的未来，Scaling Law肯定会继续发扬光大，为我们带来更大、更强大、更有力的模型，以及能够推动更多应用的可能性。</p><p>&nbsp;</p><p>当谈到Scaling Law时，我认为可以从算力、算法和数据三个维度来讨论。在算力方面，我们目睹了整个2023年英伟达股价的飙升。我记得在年初和英伟达谈论收购案时，股价可能只有100多元，但等到收购结束时，股价已经涨到近400元。这也证明了英伟达在过去一年内的强大表现，主要是由于供给不平衡导致的定价权的强势，人们对其主导未来算力生态的信心十分充足。另一方面，在年底，谷歌的Gemini以及其发布的整套TPU v5p，以及业界领先的光电混合互联技术，也给人们留下了很多想象空间，预示着未来算力可能会更加多元化。</p><p>&nbsp;</p><p>在算法方面，例如，GPT-4在年初泄露时，展示了一个MV（多模态视觉）的架构。而谷歌的Gemini则是一个全面的多模态架构，不同于先前分别预训练视觉和语言模型，Gemini从一开始就将不同的模态进行融合。我们目睹了算法架构和整个多模态领域在创新和突破方面的发展。GPT-4在年初主要是文本数据，到了4V时开始引入更多的图像数据，而Gemini更是将所有模态，甚至宣称将YouTube上所有视频的数据都引入训练。数据领域远未达到瓶颈。结合算力、算法和数据，我们看到了整个2023年有许多突破和创新。虽然我们看到的已经很多，但在OpenAI和Google可能还有更多我们未曾见到的突破。因此，对于模型能力在2024年、2025年甚至未来几年的提升，我感到非常乐观。结合持续发展的开源，例如今年号称可能会发布一个强大的Llama-3，我对未来充满期待。不论是闭源还是开源的模型，在Scaling Law的指导下，都将为我们带来更多惊喜。</p><p>&nbsp;</p><p></p><h2>大模型不再遥不可及</h2><p></p><p>&nbsp;</p><p>王铁震：在过去的一年里，我们见证了模型数量的急剧增长，技术在这段时间内迅速扩散，所有人都开始相信这个领域将会有巨大的发展。现在我们看到有这么多的模型，似乎在年初的时候，大模型被认为是昂贵且困难的，很多人觉得我们缺乏足够的数据和算力。</p><p>&nbsp;</p><p>现在模型似乎已经不再是遥不可及的事物，其成本迅速下降，变得非常平民化。在这个过程中到底发生了什么？我们能否请两位嘉宾分享一下，过去一年我们在技术上取得了什么样的突破，是什么样的驱动力让我们经历了如此巨大的变革？</p><p>&nbsp;</p><p>尤洋：我认为开源对这个影响非常大。特别是在年初Llama开源之后，整个行业迅速展开了许多基于Llama的微调工作。事实上，可能在去年或者2022年底的时候，很多人对AI大模型的发展并没有太多关注，所以当GPT-3于2020年6月发布时，可能有些人感到惊讶。这项技术其实有很多公开的信息，只要我们花一些时间静下来，我认为任何水平较高的团队都可以大致复现出一个不错的模型。</p><p>&nbsp;</p><p>刚开始可能有些人感到不知所措，因为之前可能接触的不够多。但是一旦大家理解了它的工作原理，我觉得我们至少可以在技术上达到ChatGPT或者GPT-4的效果。首先，Llama的技术很多都是开放的，再加上全球开源社区，例如Hugging Face以及PyTorch开源社区对这个生态做出了很多贡献。Llama的出现加上这些因素，使得像今天制作一个200亿的模型或者200亿的模型并不是一件非常困难的事情。我个人认为，至少90%的大公司都有能力去实现这样的任务。</p><p>&nbsp;</p><p>王铁震：过去，训练一个模型似乎对企业来说是一个非常大的成本和挑战。然而，随着微调技术的发展，我们现在可以相对轻松地拥有这些模型。我想问一下陈于思，在模型数量急剧增长，很多人都在进行开源模型的情况下，你是否认为从投资的角度来看，开源是一个非常好的投资机会？为什么这么多公司都在参与开源，他们的玩法是什么样的？从投资的角度，你会更倾向于选择投资开源的公司吗？还是会考虑闭源的机会呢？</p><p>&nbsp;</p><p>陈于思：我认为开源在整个生态中是非常重要的组成部分，特别是对于一些欧洲公司，例如Hugging Face和最近备受瞩目的MR。这些公司成功地融了大量资金，尤洋老师的HPC-AI Tech一直在构建一个开源的生态系统。</p><p>&nbsp;</p><p>从软件的角度来看，开源实际上是构建自己生态系统的一种极好方式，提高品牌知名度，并吸引客户，最终实现付费转化。这一套开源到最终付费的转化过程在一些早期数据库和云上软件等方面已经得到验证。</p><p>&nbsp;</p><p>在大模型领域，开源显得尤为重要。正如洋老师刚才提到的，像Llama和基于Llama的团队，尤其是MR的核心人员，通过持续开源努力，降低了从事大模型研发和理解的门槛。</p><p>&nbsp;</p><p>在Llama出现之前，人们对大模型的理解可能只是一知半解，甚至有些人可能对其心存畏惧。但随着Llama-2和后续一系列开源模型的出现，尤其是模型层面和工具层面的不断开源，使得更多人能够参与大模型的创新和商业化。</p><p>&nbsp;</p><p>对于开源公司来说，一方面，它们有机会通过开源构建生态系统，吸引更多客户，并将其转化为付费用户。另一方面，许多开源公司最终会以较高的价格被收购，这对于投资者来说是一种良好的退出渠道。</p><p>&nbsp;</p><p>从投资者的角度来看，开源公司拥有大量用户基础，即使其本身收入不算太多，也能够吸引成功的大公司进行收购。开源技术的快速演进也是显而易见的，例如Llama-2的出现极大地提升了垂直领域模型的能力，而MR的7B和最近的8*7B的MOE架构模型有望成为新的基准。</p><p>&nbsp;</p><p>整个开源生态和闭源公司在竞争中相辅相成，这可能是一个更为健康的状态，有助于推动大模型生态不断向前快速发展。在2024年，我们期待看到更多令人激动的新成果。</p><p>&nbsp;</p><p>王铁震：于思刚才提到的全栈开源是一个非常有趣的点，这个术语可能之前我们用得比较少。在大家对开源的认识中，大模型开源通常指的是权重的开源。然而，你提到的全栈开源更全面，包括模型的训练过程以及在服务上进行的成本优化，以确保模型更快地运行并降低成本，这具有重要的意义。</p><p>&nbsp;</p><p>我之前了解到尤洋老师的Colossal-AI也在这个领域做中间层的工作。我想请教一下尤洋老师，能否简要介绍一下你们主要做了哪些工作，以便能够快速实现高性能的训练或模型推理，从而降低成本，使更多人能够轻松地使用这些模型。另外，像于思老师所说，如果不使用这些工具，成本可能比直接购买OpenAI的token还要高。那么在这个优化的过程中，或者说在这个领域中还存在哪些优化的点或者欠缺的地方呢？</p><p>&nbsp;</p><p>尤洋：我们的基础设施的最终目标是隐藏底层的具体操作细节，让用户在训练大模型时能够像制作PPT一样简单。具体而言，我们的基础设施服务针对三类用户进行了优化。</p><p>&nbsp;</p><p>第一类用户是进行大模型预训练的用户，通常拥有至少100卡，甚至千卡、万卡的资源。这类用户面临并行分布式计算的问题，涉及大模型的切割和各种并行计算策略，如数据并行、张量并行、流水线并行等。核心原则是优化数据移动，以降低延迟和频率。</p><p>&nbsp;</p><p>第二类用户是进行微调的用户，他们的资源相对有限，可能只有一个或几个服务器。这类用户对训练时间不敏感，但关注资源的充分利用和内存的优化，以在有限的资源下训练更大的模型。内存优化成为关键，涉及远端内存的使用，同时需要降低数据移动的频率和延迟。</p><p>&nbsp;</p><p>第三类用户直接进行推理，可能是在调用API或进行模型服务。这类用户的操作相对简单，可操作的空间很大，我们通过将市面上各种推理解决方案结合起来，以及引入训练的技术，实现基本操作的高效执行。</p><p>&nbsp;</p><p>王铁震：我想就结合这个问题，继续请教于思，从投资的角度来看，你认为我们当前所做的这些优化已经达到了极致吗？未来是否还存在新的机会，可以进一步提升用户的整体体验，使各种应用场景都能够得到更好的提升？2024年，再次审视这些基础设施或者中间层，你更关注哪些机会呢？</p><p>&nbsp;</p><p>陈于思：从用户体验的角度来看，我认为有两个关键点。首先是性价比，简而言之，我们希望看到模型性价比持续提升。性价比可以从两个方面考虑，首先是性能。我之前提到了Scaling Law2024年，Scaling Law仍将是一个备受关注的趋势。以Google为例，通过更强大的多模态能力，基于Gemini Ultra，已经在许多基准测试中超过了GPT-4。未来，随着模型对更全面、更多模态数据的预训练，GPT-5有望成为一个完全的多模态模型，这将提高基础模型本身的能力，并启用更多应用场景，如视频理解、图文理解和语音生成等，带来更多商业化的应用。</p><p>&nbsp;</p><p>另一个方面是规模化。随着模型规模的增加，成本理论上会更高。然而，从整个服务和模型成本优化的角度来看，我对2024年还是比较乐观的。目前的硬件、系统和模型技术，我相信能够实现100倍性能的优化。这包括一系列的技术创新，如FlashAttention、Flash Decoding以及Speculative Decoding等，都在模型加速和性能成本优化方面发挥了显著的作用。</p><p>&nbsp;</p><p>另一方面，我认为未来还会有更多针对推理的硬件优化。不同的公司都在自主研发硬件，例如Google的TPU、亚马逊的Infinity Training、微软的推理芯片以及OpenAI的潜在芯片。硬件与软件的一体化优化可能会带来更多的性价比惊喜。</p><p>&nbsp;</p><p>第二个关键点是在中间层和工具层方面可能会有更多的投资机会。在AGI到来之前，大模型的发展仍处于早期阶段。因此，在这个过程中，各种各样的工具，帮助开发者更好地开发模型，将会非常有价值。中间层的工具，如调度不同模型、自动化工具等，都有可能成为未来的发展方向。在不同任务和需求之间做好调度，将任务导向性价比最合适的模型，是一种可能的趋势。总体来说，中间层和工具层的发展将有助于更好地应对各种复杂的开源和闭源模型，以及满足不同用户需求的优化。</p><p>&nbsp;</p><p></p><h2>AI中间层的发展潜力</h2><p></p><p>&nbsp;</p><p>王铁震：我们可以在软件和算法层面进行大量优化，这领域还有很多潜力。不仅如此，在硬件层面还将推出专门为大型模型设计的新硬件，其中包括更大容量的内存和显存芯片。软硬件的结合，以及中间层的一些优化，都为提高效率提供了巨大机会。</p><p>&nbsp;</p><p>我一直在思考一个问题，即针对大模型的场景，通过构建中间层基础设施来提高效率、降低成本，是否与过去我们所了解的SaaS有相似之处。SaaS的目标是使企业生产效率达到最优，让最合适的人去做最合适的事情，而不是将所有工作都集中在同一个平台上。然而，我们看到在国内，SaaS的发展并不十分顺利。我想请教大家，对于AI基础设施，特别是AI中间层的发展，我们是否可以借鉴SaaS在国内的发展经验？对于这个AI中间层公司未来的潜力和机遇，老师们如何看待呢？</p><p>&nbsp;</p><p>陈于思：我认为中国的AI中间层软件公司有两个主要机遇。首先，从国内的角度看，市场空间与最终市场息息相关。中国的企业应用软件市场相较于其他国家而言，市场空间可能相对较小，可能是几百亿或上千亿的规模。尽管在这个市场中，细分领域的SaaS市场空间可能较小，但对于AI而言，我认为它的未来市场潜力将是非常巨大的。很少有人将SaaS视为产生工业革命级别机会的领域，而AI则被认为是一个具有巨大机遇的工业革命级别的领域。</p><p>&nbsp;</p><p>在这个发展过程中，中间层软件公司将有很大的机会。在讨论SaaS和企业软件的SaaS时，我们可能在讨论两个市场规模相差几个数量级的机会。其次，在中国，这些中间层公司在当前阶段需要考虑如何扩大自身的生态和行业影响力，同时更迅速地实现商业落地。尤洋老师和潞晨科技在这方面表现相当不错，我期待尤洋老师的详细介绍。</p><p>&nbsp;</p><p>第二点，我认为中国或华人开源的中间层机会，也可以放眼世界。在全球范围内，AI仍处于相对早期的阶段。中国工程师的成本更低，我认为中国在开发软件方面与美国相比具有一定的性价比优势。全球化也是一个重要考虑因素，例如OpenAI的GPT-3在全球范围内引起了广泛的关注。AI是在这个充分全球化、信息全球化的时代产生的产物。在AI领域的认知方面，虽然在基础模型领域存在一定差距，但在应用和开发者方面，中国与美国之间没有代际差距。一些早期以出海为目标的中国公司在认知上甚至可能更强，因此不一定局限于中国的市场，可以扩展到全球市场。</p><p>&nbsp;</p><p>王铁震：于思提出了两个观点。首先，他认为与传统的SaaS行业相比，AI的中间层面临的市场机会非常大。这种市场机会并不像SaaS那样局限于某一个地区，而是为中国的开发者提供了大量走向世界的机会。无论是从技术还是从成本的优势来看，我们都有很大的优势。其次，他认为在SaaS这个领域，中国的开发者也有很大的优势。我想知道，尤洋老师在整个Colossal-AI的发展过程中，是否有一些有趣的故事可以分享给我们？</p><p>&nbsp;</p><p>尤洋：首先，我要分享一组数据，这些数据来自拾象科技的李广密。他发现SaaS付费率与一个国家的人均GDP有强关联。例如，美国的SaaS付费率大约为7%，对应于其人均GDP的7万美元。欧洲和日韩的付费率约为4%，与4万美元的人均GDP相吻合。中国的SaaS付费率约为1%，与人均GDP的1万美元相吻合。印度的SaaS付费率是0.2%，与人均GDP的0.2万美元相吻合。</p><p>&nbsp;</p><p>在这组数据的基础上，我们可以做一些思考。首先，我们公司目前探索出了一条适合自身发展的道路。无论是SaaS、PaaS，还是大型AI公司，我们都希望能做出一本万利的产品，让收入以指数级增长，人力则不必线性增长。</p><p>&nbsp;</p><p>中国的SaaS在过去几年失败或者没有特别成功的原因是定制化需求过高，这也与中国的甲方文化有关。另一个重要因素是在过去十年的SaaS发展中，硬件或芯片的作用并不大。例如，我们不会通过观察阿里巴巴积累了多少CPU来评估其好坏。但在AI领域，底层芯片的效能对整个生命周期有重要影响。训练成本和产品迭代周期直接与底层芯片的效率相关。</p><p>&nbsp;</p><p>所有大公司都在优化推理成本，因为他们最终想赚钱。模型调用的成本越低，模型使用的频率越高，微调训练的需求就越高。推理和训练是相辅相成的，训练包括预训练和微调。如果一个产品被频繁调用，它就需要频繁更新。</p><p>&nbsp;</p><p>目前，我们主打自己的多位一体Colossal-AI和PaaS平台。Colossal-AI主要针对规模不大、主要做微调的用户。PaaS平台是针对稍微规模大一些的客户。我们现在也有一些世界500强和2000强的客户，他们有自己的算力，直接购买我们的企业版软件。通过这种方式，我们在2023年成功实现了几千万的收入。在公司亏损很低的情况下，达到了60%以上的毛利率。下一步，我们将继续观察市场反馈，寻找扩大规模的机会。</p><p>&nbsp;</p><p>王铁震：过去的 SaaS 可能需要高定制成本，因为强烈的甲方文化。但我的理解是，对 AI 来说，任务需求比较统一，如快速训练和推理。只要性能做好，甲方可以专注应用层开发。而且，PaaS 层有很多机会帮助甲方节省成本，甲方也愿意采购。</p><p>&nbsp;</p><p>尤洋：我认为这一点至关重要。例如，我们公司最近想要建立一个奖金股权系统。我们研究了市场上的所有SaaS软件，但发现它们并不能很好地支持这个功能。因此，我们决定雇佣两个人来帮助我们编写代码。这就说明，有很多事情实际上是难以标准化的，并且很容易走向定制化的道路。当然，如果AI的基础设施层的Transformer能统一市场，那么它的接口最终可能会相对标准化。</p><p>&nbsp;</p><p>王铁震：标准化接口有助于实现一本万利的商业模式，只需要在一个地方优化产品，就能卖给很多客户，这个市场有巨大的潜力。然而，也有一些观众提问，硬件公司最了解自己的硬件，如何看待自己做中间层工作的可能性。这实质上是之前问题的另一面，即我们看到应用层与中间层的配合非常好，那么从另一个角度来看，硬件层可能与中间层存在某种竞争或合作关系。</p><p>&nbsp;</p><p>陈于思：这个我认为在技术栈中是常见的现象。例如，英伟达公司的整个CUDA软件栈，从底层的CUDA se g 加加，到内核的优化，再到算子，固定库glass，它都有集成。也包括了deep speed 和Megaton框架的推理，像test RT，对于许可，无论是Canceflow，Jack，还是Pytorch，实际上都有很多优化的支持。我想说的是，硬件公司可以完成很多模型侧的中间层工作。但是有两个问题，首先，他们只会优化自己的产品，你很难想象英伟达会去优化AMD或英特尔的软件栈。因此，跨平台有很多机会，尤其是现在大家都在试图打破英伟达的垄断。</p><p>&nbsp;</p><p>在不同的硬件平台上，AMD，英特尔，SambaNova，还有国内的华为，摩尔线程、壁仞等公司，都有很多计算芯片。我认为，这个时候，中间层是非常有价值的。</p><p>&nbsp;</p><p>最后，硬件公司本质上是做芯片的，中间层软件对他们来说是挑战。英伟达成功的地方在于它已经完全是一个系统公司和软件公司。但是其他硬件公司还是硬件公司，让他们去写优秀的软件是有挑战的。</p><p>&nbsp;</p><p>所以，我认为，基于硬件的模型中间层优化还有很多机会。但是，从模型到应用的中间层，我认为硬件公司很难做到。因为这个领域的差距太大了，从硬件算法到应用，每个层都在变化。在这个时候，中间层有很多机会，但是长期可能要考虑的是，如果生态稳定了，中间层的长期壁垒在哪里。我认为这是创业或者入局需要深思的问题。</p><p>&nbsp;</p><p>王铁震：对于Nvidia来说，其软件生态相对较好，可能会继续提升，但我们现在正处于一个充满竞争的时代。许多芯片公司可能还没有从纯硬件公司转型为硬件和系统公司，他们可能更希望与中间层合作，以使自己的软件生态更加完善，并更好地服务最终用户。这可能是硬件公司和中间层之间的合作关系，而不是竞争关系。</p><p>&nbsp;</p><p>尤洋：英伟达之所以能取得今天的成就，主要是因为他们开放了软件，没有在软件层面上与他人竞争。据英伟达的数据，现在全球有400万个CUDA开发者，这是一个非常庞大的群体。而这400万人中，大部分并非英伟达的员工，我觉得正是这种合作力量，使英伟达取得了今天的成果。</p><p>&nbsp;</p><p>如果我们再看看AI的中间层，从最开始的Cafe到Tensorflow、Pytorch，这些成功的框架并非英伟达自己开发的。现在我们需要大规模分布式处理，这已经超出了英伟达最熟悉的领域。因为分布式处理涉及GPU之间的关系，GPU和CPU之间的关系，服务器之间的关系，这不是一个硬件公司能够全面管理的。</p><p>&nbsp;</p><p>英伟达的成功已经证明了硬件公司通过与软件公司合作是可以赚到大钱的。如果英伟达关闭了CUDA生态，我相信他们不可能取得今天的成就。因此，我认为硬件公司应该会做出理性的选择。我之前参加了一次华为升腾的会议，他们也希望能够开放升腾的生态。</p><p>&nbsp;</p><p></p><h2>创业者的机会</h2><p></p><p>&nbsp;</p><p>王铁震：我们过去在这个模型层看到的是，例如Llama在形成自己的生态，开放自己的模型，引入更多的开发者，共同建设这个行业，使得这个行业更好。硬件也是如此，需要有一个庞大的开发者生态共同开发，以使硬件销售更好，并围绕硬件开发软件。</p><p>&nbsp;</p><p>当我们谈到应用侧的创业机会时，如果我们在2024年的推理和训练成本继续大幅下降，那么我们在2024年的应用层能看到一些什么样的事情？我准备了一些问题。首先，我想大家探讨一下，当前哪些行业可能有更多的大模型应用场景？他们会面临哪些挑战？很多人认为今年会是大模型应用的元年，因为大家对大模型的边界了解得比较清楚，而我们这些中间层让模型的成本大幅下降，创业者在这一年可以做些什么？难度如何？</p><p>&nbsp;</p><p>陈于思：在我看来，大规模模型的商业化落地在过去的二三年更像是一个探索期。如今，全球的大规模公司，除了OpenAI外，所有公司的营收加起来可能都无法与OpenAI匹敌。这显然是因为模型能力还处在早期阶段，同时，商业化的探索也仍处在早期阶段。在此背景下，我们可以看到的是，无论是ToB还是ToC，都有一些初步的落地应用。</p><p>&nbsp;</p><p>在ToC方面，我们看到了一些工具类产品，如OpenAI的聊天机器人和一些情感陪聊类型的产品。这些都初步验证了AI在社交领域的应用可能性。此外，在游戏领域，我们也看到了一些游戏化的探索，如网易的《逆水寒》AI角色，以及字节跳动推出的《豆》。</p><p>&nbsp;</p><p>在ToB方面，我们看到了一些类似于搜索助手的产品。我相信，在2024年，随着大规模能力的提升和性价比的极大提升，我们将能看到更多的应用落地。OpenAI 的 GPT Store就是一个例子，它是AI大规模商业化落地的一个探索，可能许多垂直化的领域模型都将基于GPT Store建立起来。</p><p>&nbsp;</p><p>在ToB端，我觉得各家公司都希望拥有自己的GPT。它们可以帮助企业利用自己的垂直数据来创建自己的大模型，或者使用类似IAG的搜索方式来创建自己的企业内部知识库。</p><p>&nbsp;</p><p>我相信，AI将在未来给SaaS带来极大的发展机会。一方面，AI可以极大地提升SaaS在需求方面的自动化程度；另一方面，以前很多定制化的问题，可能可以通过AI的方式来降低定制化的成本。</p><p>&nbsp;</p><p>然而，更多的B端应用需要模型基座的能力进一步提升，因为大模型本身还存在一些问题，比如幻觉问题，这可能需要使用一种混合方式，即在大模型的基础上叠加一些其他模型或方式来控制幻觉。但我相信，随着基座模型能力的提升，我们将能找到更好的方法来控制幻觉，开发更多的B端应用。</p><p>&nbsp;</p><p></p><h2>关于2024年的一些预测</h2><p></p><p>&nbsp;</p><p>王铁震：下面问下尤洋老师，从您的视角看，当前大模型的落地情况如何？您认为新的一年里，可以探索和应用大模型的领域有哪些？</p><p>&nbsp;</p><p>尤洋：我们接触了很多客户，他们大部分都处于种子阶段。我发现有很多场景他们认为现有的产品或大模型都做的不足。例如，一个朋友想创业，他的产品是将论文转化为PPT。这个需求非常强烈，无论在学校还是其他单位，如果能将报告迅速转化为PPT，方便与同事分享，那将非常方便。但是目前我没有找到很好的产品，他正在考虑是否要自己训练一个模型。</p><p>&nbsp;</p><p>另一方面，我也测试了一些文字转视频的软件，但效果并不理想。它们需要非常复杂的提示才能生成一个看得过去的视频。即使不提视频，我发现在生成图片方面也有很大的进步空间。我觉得AI的发展并没有那么快。ChatGPT已经出来一年了，但我感觉使用最高配的ChatGPT生成的图片效果也不是很好。</p><p>&nbsp;</p><p>我们还有很多事情可以做，至少在技术上，2024年能做的事情还是很多的，目前AI在很多领域都远远没有达到我们的预期。</p><p>&nbsp;</p><p>王铁震：两位老师预计今年大模型领域会如何发展？我们提到了大模型面临的一些问题，例如生成效果不佳和成本较高。你们认为哪些因素可能阻碍大模型的发展？什么是你认为的痛点？未来，在基础设施或大模型领域创业是否有机会？</p><p>&nbsp;</p><p>陈于思：我对2024年充满期待，可以预见到的是GPT-5的出现，以及Cloud 3的发展。更多的大型企业也将在这一年崭露头角。我认为过去两三年可能是融资的年份，一些头部公司，特别是美国的，可能已经筹集了数十亿美元的资金。</p><p>&nbsp;</p><p>2024年，大家也应该开始做功课了。比如说，Property预计今年自己可以实现8.5亿美元的收入。去年他们已经实现了十五六亿美元的收入，今年他们能保持多快的增长？更强大的模型能否带来更好的效果？能否启动更多的应用？就像尤洋老师说的，我自己也经常使用ChatGPT，但它画出的图形效果并不理想。有时候，你可能需要多次修改prompt，这感觉就像炼丹。</p><p>&nbsp;</p><p>2024年，我们能否拥有更好的控制力？能否有更低的幻觉率？我期待这些变化。随着AI机构模型能力的不断提升，是否会有更好的AI 原生 APP出现？我们已经看到了一些雏形，包括Character，Inflection，以及Publicity。那么，是否有更多的B端应用，如AGI的应用，GPT Store等，我期待这些变化。</p><p>&nbsp;</p><p>尤洋：2024年，我们必须解决任何限制Scaling Law的问题，因为我们实际上并不知道模型的上限。目前，这确实是一个实验性的工程问题，我们需要看看能否发掘出更好的基础模型。我们三人刚才初步达成了共识，在图文生产领域，我们还能做的事情很多。首先，当前效果并不理想。我已经多次提醒AI，但它似乎仍然无法画出我想要的感觉，这说明它的底层模型还不够好。其次，对于种子用户，他们并不打算训练自己的模型，而是希望使用API。在这种情况下，我们需要看极致的成本优化和效率优化是否能为他们带来更好的体验。因为开放AI现在确实有初步的营收，但如果它想进一步扩大规模，单次token的调用是否能为那些真正部署AI应用的人带来更有价值的东西？</p><p>&nbsp;</p><p>另外，我非常想看看Llama-3的效果如何，以及开源和闭源之间的差距是否会缩小。虽然现在Llama的效果很好，但我们无形中还是认为它们比ChatGPT差。如果Llama-3发布后，ChatGPT没有太大的进步，或者我们感觉不到它们之间的差距，那会是一种什么样的局面呢？我们可以拭目以待。</p><p>&nbsp;</p><p>王铁震：你们想给参与 AIGC 浪潮的年轻企业家或开发者提供一些建议呢？</p><p>&nbsp;</p><p>陈于思：我觉得可以All in。 AI是一个非常大的机会。尽管我们现在还处于AI的早期阶段，就像90年代末的互联网或2010年代的移动互联网一样，但是只要在这个行业里深耕，找到好的方向，随着行业的整体增长，个人一定能够抓住很多机会。不要过于纠结于ToB还是ToC的方向，或者担心自己的AI应用被大公司抢先。就像张一鸣在创办今日头条之前，他可能做了十几个APP，最后才发现了今日头条的价值。所以，关键是要尽快动手去做。</p><p>&nbsp;</p><p>尤洋：我非常赞同陈总的观点，AI未来将会像互联网和智能手机一样渗透到我们生活的方方面面，这是一个巨大的机会。当然，AI也会经历低潮期，就像互联网泡沫一样。但就像互联网泡沫之后出现了Google、Facebook、eBay等成功企业一样，AI行业在经历挫折后也将会诞生出更多有价值的巨头企业。</p>",
    "publish_time": "2024-01-29 16:05:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "百川智能发布超千亿大模型Baichuan 3，中文评测超越GPT-4",
    "url": "https://www.infoq.cn/article/eBpIOezhpdBY7LFPYZu3",
    "summary": "<p>1月29日，百川智能发布超千亿参数的大语言模型Baichuan 3。</p><p>&nbsp;</p><p>在多个权威通用能力评测如CMMLU、GAOKAO和AGI-Eval中，Baichuan 3都展现了出色的能力，尤其在中文任务上更是超越了GPT-4。而在数学和代码专项评测如MATH、HumanEval和MBPP中同样表现出色，证明了Baichuan 3在自然语言处理和代码生成领域的强大实力。</p><p>&nbsp;</p><p>不仅如此，其在对逻辑推理能力及专业性要求极高的MCMLE、MedExam、CMExam等权威医疗评测上的中文效果同样超过了GPT-4，是中文医疗任务表现最佳的大模型。Baichuan 3还突破“迭代式强化学习”技术，进一步提升了语义理解和生成能力，在诗词创作的格式、韵律、表意等方面表现优异，领先于其他大模型。</p><p>&nbsp;</p><p>链接：<a href=\"https://www.baichuan-ai.com/\">https://www.baichuan-ai.com/</a>\"&nbsp;</p><p></p><h2>百川智能做了哪些改进</h2><p></p><p>&nbsp;</p><p>与百亿、几百亿级别参数模型训练不同，超千亿参数模型在训练过程中对高质量数据，训练稳定性、训练效率的要求都高出几个量级。为解决相关问题，百川智能在训练过程中针对性地提出了“动态数据选择”、“重要度保持”以及“异步CheckPoint存储”等多种创新技术手段及方案，有效提升了Baicuan 3的各项能力。</p><p>&nbsp;</p><p>高质量数据方面，传统的数据筛选依靠人工定义，通过滤重筛选、质量打分、Textbook筛选等方法过滤数据。而百川智能认为，数据的优化和采样是一个动态过程，应该随着模型本身的训练过程优化，而非单纯依靠人工先验进行数据的采样和筛选。</p><p>&nbsp;</p><p>为全面提升数据质量，百川智能设计了一套基于因果采样的动态训练数据选择方案，该方案能够在模型训练过程中动态地选择训练数据，极大提升数据质量。</p><p>&nbsp;</p><p>训练稳定性方面，超千亿参数的模型由于参数量巨大，训练过程中经常会出现梯度爆炸、loss跑飞、模型不收敛等问题。对此，百川智能提出了“重要度保持”(Salience-Consistency)的渐进式初始化方法，用以保证模型训练初期的稳定性。并且优化了模型训练过程的监控方案，在梯度、Loss等指标上引入了参数“有效秩”的方法来提早发现训练过程中的问题，极大加速对训练问题的定位，确保了最后模型的收敛效果。</p><p>&nbsp;</p><p>此外，为了确保在数千张GPU上高效且稳定地训练超千亿参数模型，百川智能同步优化了模型的训练稳定性和训练框架，并采用“异步CheckPoint存储”机制，可以无性能损失地加大存储的频率，减少机器故障对训练任务的影响，使Baichuan 3的稳定训练时间达到一个月以上，故障恢复时间不超过10分钟。</p><p>&nbsp;</p><p>训练效率方面，百川智能针对超千亿参数模型的并行训练问题进行了一系列优化，如高度优化的RoPE, SwiGLU计算算子；在数据并行中实现参数通信与计算的重叠，以及在序列并行中实现激活值通信与计算的重叠，从而有效降低了通信时间的比重；在流水并行中引入了将激活值卸载至GPU的技术，解决了流水并行中显存占用不均的问题，减少了流水并行的分段数量并显著降低了空泡率。通过这些技术创新，Baichuan 3的训练框架在性能方面相比业界主流框架提升超过30%。</p><p>&nbsp;</p><p></p><h2>测评展示</h2><p></p><p>&nbsp;</p><p></p><h4>中文任务成绩超越GPT-4</h4><p></p><p>&nbsp;</p><p>根据百川智能，Baichuan 3在多个英文评测中表现出色，达到接近GPT-4的水平。而在CMMLU、GAOKAO、HumanEval和MBPP等多个中文评测榜单上，是超越GPT-4展现了其在中文任务上的优势。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c5/c5d0d4de908244b55fe354458a30baf8.png\" /></p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d13dbff4219855337242e6a5d9b7e083.png\" /></p><p>&nbsp;</p><p>&nbsp;</p><p>此外，在MT-Bench、IFEval等对齐榜单的评测中，Baichuan 3超越了GPT-3.5、Claude等大模型，处于行业领先水平。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a42d932de4a94d51c25474f89bb65f9.png\" /></p><p>&nbsp;</p><p></p><h4>医疗数据集Token数超千亿，医疗能力逼近GPT-4</h4><p></p><p>&nbsp;</p><p>另外值得注意的是，百川智能还给Baichuan3注入了丰富的医疗知识。</p><p>&nbsp;</p><p>Baichuan 3在数学和代码等多个权威评测上中文任务超越GPT-4的优异成绩，已经充分证明了其基础逻辑推理的能力。在拥有丰富高质量专业医疗知识，并能通过调优后的Prompt对这些知识进行充分激发的基础上，结合超千亿参数的推理能力，Baichuan 3在医疗领域的任务效果提升显著，在各类中英文医疗测试中的成绩提升了2～14个百分点。</p><p>&nbsp;</p><p>根据百川智能，Baichuan 3在多个权威医疗评测任务中表现优异，不仅MCMLE、MedExam、CMExam等中文医疗任务的评测成绩超过GPT-4，USMLE、MedMCQA等英文医疗任务的评测成绩也逼近了GPT-4的水准，是医疗能力最强的中文大模型。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10b62e1400a75ae6dd0a4209d77fff8c.png\" /></p><p>&nbsp;</p><p>据悉，百川智能在模型预训练阶段构建了超过千亿Token的医疗数据集，包括医学研究文献、真实的电子病历资料、医学领域的专业书籍和知识库资源、针对医疗问题的问答资料等。该数据集涵盖了从理论到实际操作，从基础理论到临床应用等各个方面的医学知识，确保了模型在医疗领域的专业度和知识深度。</p><p>&nbsp;</p><p>针对医疗知识激发的问题，百川智能在推理阶段针对Prompt做了系统性的研究和调优，通过准确的描述任务、恰当的示例样本选择，让模型输出更加准确以及符合逻辑的推理步骤，最终不仅提升了Baichuan 3在多项医疗考试上的成绩，并且在真实的医疗问答场景下也能给用户提供更精准、细致的反馈。</p><p>&nbsp;</p><p></p><h2>创作精准度再大幅提升</h2><p></p><p>&nbsp;</p><p>语义理解和文本生成，作为大模型最基础的底层能力，是其他能力的支柱。为提升这两项能力，业界进行了大量探索和实践，OpenAI、Google以及Anthropic等引入的RLHF(基于人类反馈的强化学习)和RLAIF(基于AI反馈的强化学习)是其中的关键技术。</p><p>&nbsp;</p><p>基于强化学习对齐后的模型不仅可以更精准地理解用户指令，尤其是多约束以及多轮对话下的指令，还能进一步提升生成内容的质量。但是在大模型中充分发挥强化学习的作用不仅需要稳定且高效的强化学习训练框架和高质量的优质偏序数据，还需要在“探索与利用”两者间进行平衡，实现模型能力持续爬坡。</p><p>&nbsp;</p><p>对于以上问题，百川智能进行了深入研究并给出了针对性的解决方案。</p><p>&nbsp;</p><p>强化学习训练框架方面，百川智能自研了训练推理双引擎融合、多模型并行调度的PPO训练框架，能够很好支持超千亿模型的高效训练，训练效率相比业界主流框架提升400%。</p><p>&nbsp;</p><p>偏序数据方面，百川智能创新性的采用了RLHF与RLAIF结合的方式来生成高质量优质偏序数据，在数据质量和数据成本之间获得了更好的平衡。在此基础上，对于“探索与利用”这一根本挑战，百川智能通过PPO探索空间与Reward Model评价空间的同步升级，实现“迭代式强化学习”(iterative RLHF&amp;RLAIF)。基于强化学习的版本爬坡，可以在SFT的基础上进一步发挥底座模型的潜力，让Baichuan 3的语义理解和生成创作能力大幅提升。</p><p>&nbsp;</p><p>以文本创作中最具挑战的唐诗宋词为例，作为中国传统文化的瑰宝，诗词不仅在格式、平仄、对偶、韵律等方面均有着严格的约束条件，并且内容高度凝练、寓意深远。如果仅通过SFT的微调学习，一方面高质量诗词的创作数据需要极高的专家成本，另一方面不能在平仄、对偶、韵律等多个方面实现较好的约束理解和遵循。此外，传统的单次RLHF范式在唐诗宋词面前也遇到极大挑战，PPO在训练过程中生成的Response有可能超出Reward Model的评价范围导致“探索”的过程失控。</p><p>&nbsp;</p><p>Baichuan 3结合“RLHF&amp;RLAIF”以及迭代式强化学习的方法，让大模型的诗词创作能力达到全新高度。可用性相比当前业界最好的模型水平提升达500%，文采远超GPT-4。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b8/b88362325ceb66ef91e477f635f4ef72.png\" /></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/435b5150da5f4d74330398fa8dc2de94.png\" /></p><p>&nbsp;</p><p>作为参数规模超过千亿的大语言模型，Baichuan 3不仅英文效果达到接近GPT-4的水平，还在多项通用中文任务的表现上实现了对GPT-4的超越，是百川智能的全新里程碑。Baichuan 3全面的通用能力以及在医疗领域的强大表现，将为百川智能打造“超级应用”，把大模型技术落地到诸多复杂应用场景提供有力支撑。</p>",
    "publish_time": "2024-01-29 16:10:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "系统稳定与信息安全——体系建设与实战经验",
    "url": "https://www.infoq.cn/article/0BuWTwxtpauUQc19Z5wK",
    "summary": "<p>Qcon上海站，美的集团 首席信息安全官兼软件工程院刘向阳院长，在本次演讲中将介绍其在美的集团的数字化底座稳定性建设经验总结成十板斧（架构梳理、风险矩阵、变更预案、日常巡检、代码防御、全面监控、应急预案、一键逃生、故障演练、管理制度），把信息安全建设经验总结成五板斧（进不来、能发现、防泄漏、保合规、重运营），和大家一起探讨系统的稳定性和安全性建设。</p>",
    "publish_time": "2024-01-29 16:45:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大语言模型的低比特计算",
    "url": "https://www.infoq.cn/article/3DGR1LIadO4uEdLfOZ8m",
    "summary": "<p>Qcon上海站英特尔大数据技术全球CTO戴金权主讲大语言模型的挑战与优化，本次视频内容提到了大语言墨香推理和训练的瓶颈以及优化的低比特计算等，并介绍了开源大模型加速库BigDL-LLM，并提供了试用途径。</p>",
    "publish_time": "2024-01-29 16:59:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI出手后，GPT-4真的不懒了？网友不买账：只靠打补丁恐怕无济于事！",
    "url": "https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK",
    "summary": "<p></p><blockquote>大模型会从人类经验中学习，如果人类本身越来越懒，那模仿人类的程序是不是也会越来越懒？</blockquote><p></p><p></p><h2>OpenAI发布更新，解决GPT-4“变懒”问题</h2><p></p><p>&nbsp;</p><p>近日，OpenAI在一篇博文中发布了多项更新，并表示更新后的GPT-4 Turbo“拥有比之前预览模型更好的代码生成等能力，且减少了模型在任务中途罢工的「变懒」情况。”但该公司并没有对更新内容做进一步解释。</p><p>&nbsp;</p><p>OpenAI在帖子中提到，由于知识库更新，已经有超过70%的GPT-4 API用户转向了GPT-4 Turbo。OpenAI表示，未来几个月内将陆续推出更多GPT-4 Turbo更新，包括发布具有视觉模态处理能力的GPT-4 Turbo正式版。这意味着用户将可输入各类多模态提示词，例如文本到图像生成提示。</p><p>&nbsp;</p><p>此外，OpenAI还推出了被称为“嵌入”的小体量AI模型。OpenAI对于嵌入的定义，是“代表自然语言或代码等内容中概念的数字序列”。以此为基础，即可搭配检索增强生成（简称RAG，一种从数据库获取信息、而非生成答案的AI方法）应用找到各类可访问内容间的关系。这些新模型、text-embedding-3-small嵌入乃至更强大的text-embedding-3-large版本现均已正式开放。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab8d4a0684379bcc0d56b4573528f399.png\" /></p><p></p><p>经过改进的各GPT模型现已通过API开放，包括质量更高、价格更低廉的嵌入模型（e.gone模型的成本仅为此前嵌入模型的五分之一，但性能更强）。</p><p></p><h2>用户抱怨GPT-4学会偷懒：越来越像人类了？</h2><p></p><p>&nbsp;</p><p>2023年12月，有不少用户抱怨称，“这段时间使用 ChatGPT 或 GPT-4 API 时，会遇到高峰期速度非常慢、敷衍回答、拒绝回答、中断会话等一系列问题”。</p><p>&nbsp;</p><p>比如，某些时候，GPT-4系统会给出一些特别模糊的答案，特别是关于Docker、Kubernetes以及其他CI/CD的问题。此外，GPT-4还学会了“废话文学”——不直接回答问题，只是堆叠素材来讲解应该怎样回答问题。有用户反映，哪怕明确要求不要使用空白占位符，模型也仍然会用占位符把回答截得七零八落。这种限制回复质量的作法倒是替服务商节约了资源，但却极大浪费了普通用户的时间。</p><p>&nbsp;</p><p>用户&nbsp;jonathanallengrant 在OpenAI社区一个名为“为什么我觉得GPT变懒了”的帖子中提到：“不少人注意到自从Dev Day活动以来，模型的输出上限就变成了850个token。换言之就是ChatGPT变懒了，不光留出大量空白，还常常在同一条消息里半天停在原地。我相信这应该是OpenAI正在以某种方式扩展模型的推理方法。”</p><p>&nbsp;</p><p>用户manchkiran表示自己也遇到过类似的情况，并吐槽“现在的模型绝对是变懒了，只会快速搜索并给出Bing引擎的链接”，他猜测大模型变懒的原因或许与“微软加入OpenAI董事会后下调了算力分配”有关。</p><p>&nbsp;</p><p>sasindujayashmaavmu则从另一个角度分析了GPT-4变懒的原因：“我觉得这可能是人机回圈的锅……大模型会从人类经验中学习，所以如果人类本身越来越懒，那模仿人类的程序也会越来越懒。”</p><p>&nbsp;</p><p>对于漫天盖地的吐槽声，ChatGPT 官方通过 X 平台通知用户，“我们听到了你们关于 GPT-4 变得越来越懒的反馈！我们自 11 月 11 日起就没有更新过模型了，当然这不是故意的。”</p><p></p><h2>OpenAI出手后，GPT-4真的不懒了？</h2><p></p><p>&nbsp;</p><p>OpenAI本次更新承诺解决了GPT-4“变懒”问题，根据社区用户反馈来看，如今的GPT-4似乎真的聪明多了。</p><p>&nbsp;</p><p>用户Distinct_Salad_6683提到，最近自己发现GPT在编码能力有所提升，能够根据提示词快速提供完整的示例。之前GPT经常会拒绝给出具体示例，只是在描述自己要求它干的工作，并用“在此处插入函数逻辑”之类的废话来搪塞问题。</p><p>&nbsp;</p><p>也有用户“阴阳”OpenAI：软件只要更新一下就能解决“变懒”，真羡慕。要是能有补丁帮我扛过礼拜一就好了。</p><p>&nbsp;</p><p>由于OpenAI并未对更新内容做进一步解释，因此也有不少用户开始分析其到底是怎么解决GPT-4“变懒”问题的。语言学家&nbsp;christelle.hilz 分析，GPT-4变懒的问题跟算法无关，单靠打补丁恐怕无济于事。这个问题还得从其他角度尝试解决。“我好奇的是OpenAI愿意花多少钱来解决GPT变懒问题”。</p><p>&nbsp;</p><p>也有观点认为，OpenAI并未真正地解决问题。因为大语言模型就是算法加公式的组合，所以哪怕更新真的解决了变懒问题，只能用这种方法改进模型本身也不是什么好兆头。</p><p>&nbsp;</p><p>chieffy99则更悲观地表示，哪怕是聘请了世界各地的专家，大语言模型自身的问题还是难以解决，毕竟任何专家都不可能确切了解每一个问题。因为越是越是专注于自己的专业积累，我们的视野反而变得越狭窄。chieffy99还向OpenAI的管理团队“开炮”：</p><p>&nbsp;</p><p></p><blockquote>我向来敢于对OpenAI的缺点开炮，这里我也要明确表态：OpenAI一直认为AI的问题不可能通过开发AI方案来解决，但我觉得这是错的。&nbsp;我自己没有任何关于AI的知识和使用经验，但拥有丰富的项目管理积累。抱怨变懒问题的用户是谁、当时是怎么操作的并不是重点，重点在于大模型为什么会倾向于消极工作。我本人喜欢从问题当中寻找共性，而且从目前的情况看应该不只是模型自身出了问题。我自己还没有明确的答案，但OpenAI的态度明显是“先尝试从内部做解决或者改进，等影响到正常使用了再说”。&nbsp;在我看来，OpenAI的管理思路很有问题。以常见的团队沟通规划为例，只要提供足够的信息，GPT-3.5的表现还是相当不错的。所以我猜OpenAI也是用这种方式蒙蔽了高管团队的判断，毕竟精调提示词并不困难，请个专人就能解决。正因为如此，OpenAI才产生了单靠调整AI模型就能解决AI问题的思路。&nbsp;我不知道现在大家说的这些问题到底跟变懒有没有关系，毕竟引发问题的原因多种多样。而且GPT大模型本身也不老实，甚至会说谎来隐藏自己的真实行为。哪怕是被发现，OpenAI也可以解释说是存在误会或者提示词存在不当内容。另外别太过迷信规则，基于规则的行为也不一定比随机问题更稳定，比如GPT-3.5就会访问网站、并把外部聊天和相关数据保存成html文件。这其实是不符合GPT身份和功能定位的操作。我也遇到过中途“罢工”的情况，但这主要是大模型忘记了当前上下文中的内容必须与之前的上下文接续起来。普通用户当然分不清楚，所以很自然地认为是大模型在偷懒。这跟之前的GPT幻觉差不多，刚开始似乎经常发生，但使用的人越多、涉及的内部信息越少，幻觉也开始逐渐缓解。&nbsp;另外还有三点个人观察。首先，我很好奇OpenAI的专家到底做了什么。这个问题始于去年12月，当时外界认为GPT过于迷信专业知识、甚至为此而倾向于输出错误信息。比如通过知识文件向GPT自动输入提示词，那么生成的信息就会有所不同。而如果不输入预设文件，GPT的表现则比较正常。我就遇到了这样的情况，还专门向OpenAI上报了观察结果，想搞清在RAG问题有最终结论之前，到底该采取什么措施加以避免。而且之前我还尝试把知识跟行为区分开来做GPT训练，借此建立起纯知识库。在确保知识库内容与现实不冲突之后，再配合其他信息一起使用。第二点就是错误学习的问题。既然选择把大模型向公众开放，那能做纯软件修复的问题OpenAI肯定早就解决了。问题是时至今日，GPT还是没法在不改变形状的前提下，把不同尺度下相同颜色的图表正确合并。还是那句话，如果能修复的话早该修复好了。最后一点就是GPT号称全球最受欢迎的AI模型。这个评判标准实在太模糊了，我觉得应该从功能层面做准确描述。&nbsp;总而言之，当前关于GPT的种种报道明显是刻意设计出来的。可怕的是GPT明显还没做好准备，因此无脑宣传已经在扭曲中立研究、造成现实损害、甚至让AI制造出更多的社会问题。有人在违规使用GPT，甚至有人把它当作非法工具来设计和实施犯罪。我不知道这次的更新能产生多大影响，但各种违规行为已经真实存在，甚至对普通用户产生直接影响。我想问问OpenAI，这一切是单靠更新AI模型就能解决的吗？</blockquote><p></p><p>&nbsp;</p><p>值得一提的是，OpenAI此番发布的更新针对的是GPT-4 Turbo，即得到广泛使用的特定GPT-4版本。这套模型根据截至2023年4月的最新信息训练而成，目前仅提供预览版本。也就是说，大家如果继续使用GPT-4（使用截止于2021年9月的数据训练而成），那么“变懒”问题可能仍将存在。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.theverge.com/2024/1/25/24050829/openai-gpt-4-turbo-lazy-ai-model\">https://www.theverge.com/2024/1/25/24050829/openai-gpt-4-turbo-lazy-ai-model</a>\"</p><p><a href=\"https://community.openai.com/t/i-wonder-how-much-openai-would-pay-to-cure-gpt-lazyness/604781\">https://community.openai.com/t/i-wonder-how-much-openai-would-pay-to-cure-gpt-lazyness/604781</a>\"</p><p><a href=\"https://community.openai.com/t/why-i-think-gpt-is-now-lazy/534332/11\">https://community.openai.com/t/why-i-think-gpt-is-now-lazy/534332/11</a>\"</p>",
    "publish_time": "2024-01-29 17:09:57",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "LLM 训练推理加速在阿里巴巴的实践",
    "url": "https://www.infoq.cn/article/jrcB3ZydtA0PgAD48lZa",
    "summary": "<p>在大规模语言模型 (LLM) 的训练和推理实践中，工程和算法需求间存在许多需要细心权衡的问题。这些问题涉及到从软硬件协同优化，到分布式处理，以及至算法工程 Co-design 等多个领域。为了解决这些挑战，Qcon上海站杨斯然，刘侃两位专家，深入研究了不同的应用场景和流量特性，并对LLM系统进行了全面优化。</p>",
    "publish_time": "2024-01-29 17:52:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]