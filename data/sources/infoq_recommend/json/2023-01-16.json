[
  {
    "title": "“实用主义”主导的数字化趋势下，云底座被撼动了吗？",
    "url": "https://www.infoq.cn/article/yzpZpUaq7A8GEkXLOLgS",
    "summary": "<p>活下去，是这两年每一个企业的主线任务。无论是马斯克大肆“整顿”Twitter，还是互联网大厂们的裁员风波，背后无一不是企业家们焦虑情绪的体现。而为了活下去，降本增效的优先级变得越来越高。</p><p></p><p>在疫情之初，很多企业都在通过加速数字化转型应对市场环境的动荡变化，一手抓降本增效，另一手也还在继续谋求突破式创新。但是，如我们在《<a href=\"https://www.infoq.cn/article/BXWSliYk7FK3EfGSbRtS\">InfoQ&nbsp;年度技术盘点与展望</a>\"》的数字化解读中观察到的，与过往热衷于创新事物的现象相比，如今企业在数字化转型越来越强调“实用主义”，在有限的资源条件下，企业开始“集中力量办大事”，也更看重技术的投入和回报。</p><p></p><p>拿云计算来说，虽然一直被视为是企业数字化转型的底座，但是在巨大的财务和运营压力下，不少企业也开始对云“下手”了，一场“下云潮”悄然而至。</p><p></p><p>根据&nbsp;Wanclouds&nbsp;在《2022&nbsp;年下半年云计算成本和优化展望》的<a href=\"https://www.infoq.cn/news/mMtEqAXuvF7Weml1GROL\">最新报告</a>\"中显示，81%&nbsp;的&nbsp;IT&nbsp;管理者表示，随着成本飙升和市场下行，他们的最高管理层已经指示他们要减少或不承担额外的云支出。根据调研结果，39%&nbsp;的人已经决定将大量的云消耗和高性能工作负载迁移或留在本地，还有&nbsp;29%&nbsp;的人表示在&nbsp;2022&nbsp;年上半年由于价格贵而更换了公有云厂商。</p><p></p><p>此外，IBM在2022年发布的《IBM企业转型指数：云现状》中也反映了这一趋势：80%的企业已经考虑或正在考虑将已经部署到公有云上的工作负载迁回私有的基础设施。</p><p></p><p>所以，在“实用主义”主导的数字化趋势下，<a href=\"https://www.infoq.cn/article/psWB79HDzjsKislYsE0Y\">云计算</a>\"这个“底座”要被撼动了吗？</p><p></p><h2>“下云潮”针对的主要是公有云</h2><p></p><p></p><p>事实上，从目前来看，“下云潮”主要针对的还是公有云。据IBM观察，对于那些不愿放在公有云的负载，企业更愿意选择放在私有云、主机以及主机以外的其它本地设备。</p><p></p><p>“这里有很多原因：首先是性能的问题，如何让公有云的性能达到本地设备的性能值，并且减少延迟，这很重要；其次，已经上了公有云的业务，安全和合规性怎么保证；此外，企业还要避免供应商锁定。”IBM&nbsp;副总裁、IBM&nbsp;中国总经理缪可延在日前接受媒体采访时表示，“过去大家采用云计算的一个重要出发点就是不喜欢集中式管理，不喜欢被锁定。但是，如今他们突然发现，公有云也会锁定，而锁定则会给企业带来成本的增加，这里的成本很可能与私有设施不相上下，甚至更多。”</p><p></p><p>也就是说，技术创新对于企业而言并非不再重要，只不过在经济“寒冬”这样的特殊环境下，企业必须在技术价值和技术投入之间找到一个相对的平衡点。在过去经济蓬勃发展的时候，很多企业为了创新可以不计成本，但随着盈利性降低，成本投入的大小则决定着企业业务是否可持续。</p><p></p><p>“有一段时间，我们经常听到企业纷纷在说自己每年要上几朵云，其实回过头来看，当时大家都是为了拥有而拥有。随着如今大环境的变化，成本压力、不确定性增加，我们发现，企业确实变得更加理智和冷静。他们开始越来越多地关注在数字化转型和上云过程中，哪些与自己的核心业务价值挂钩。”IBM大中华区科技事业部客户成功管理部总经理朱辉强调。</p><p></p><p>当然，在朱辉看来，企业在数字化转型过程中日趋理性，并不意味着一定要极端地抛弃云。根据IBM&nbsp;商业价值研究院一项名为《混合云平台的优势》调研显示，采用全面<a href=\"https://xie.infoq.cn/article/bd7390a2e7768a5e084709970\">混合多云</a>\"平台技术及运营模式所实现的价值，是采用单一平台、单一云厂商所能实现价值的&nbsp;2.5倍。</p><p></p><p>换言之，混合多云或将成为企业平衡云计算投入和成本的那个关键“砝码”。具体而言，IBM对2023年企业云转型给出了如下预测：</p><p></p><p>第一，越来越多企业会开始采用“主机+云”的模式，二者不再是排他的选择题，通过战略性地将云与主机结合，企业可以获得更优的创新、速度和安全优势；</p><p></p><p>第二，企业将实施全面安全策略，构建安全的生态系统，通过全面了解混合云环境中（本地、公共云或私有云、边缘）的数据，为未来的威胁（如量子安全挑战）做好准备；</p><p></p><p>第三，企业将更加重视合规，尤其是金融服务等高度受监管的行业，以及需要处理客户个人信息的企业，将更加专注于处理最关键的任务工作负载，确保数据受到保护；</p><p></p><p>第四，可持续性成为一个重要课题，企业将更加关注提高整个IT运营的能源效率，同时又不牺牲安全性或性能。</p><p></p><h2>混合多云带来复杂架构和云原生应用管理挑战</h2><p></p><p></p><p>混合云作为未来云计算的主流形态，这似乎已经不是什么新鲜观点。而云计算厂商的混合云赛道之争，也早已打响。对于IBM来说，2018年大笔一挥宣布豪掷340亿美元收购<a href=\"https://www.infoq.cn/article/SRcxh_0GMRgRwBlHMRdi\">红帽</a>\"，正是其加速布局混合云战略的决心。</p><p></p><p>但是，由于混合多云架构带来的管理复杂性，不少企业在具体落地过程中也面临全新的挑战。《IBM企业转型指数：云现状》报告指出，虽然77%的受访企业采用了混合云方法，但只有不到1/4的企业能够全面管理其混合云环境。</p><p></p><p>不仅如此，随着更多云原生应用的出现，企业的开发和运维压力也上到新的台阶。</p><p></p><p>以国内某大型股份制商业银行为例，在过去几年中，该银行构建了一套自主可控的全栈式云平台以支持分布式、云原生、微服务等技术，其应用也在进行云原生化改造和上云。在其分布式、容器化、&nbsp;Kubernetes&nbsp;环境下，有上百个服务和上千个实例在运行，运维团队急需理解整个系统当中微服务应用间的相互调用关系，应用开发团队则需要及时发现、定位和解决快速迭代和发布新版本的各种问题。</p><p></p><p>为了应对这一系列挑战，据了解，该银行基于IBM&nbsp;Instana自动化运营监控能力，实现了在传统和云原生环境下不同技术栈问题的自动发现和监控，通过关联全面的相关信息，为快速定位故障提供了<a href=\"https://www.infoq.cn/article/V5R45xKtTVizcIeg6fFS\">可观测</a>\"性。</p><p></p><p>“不管是开发还是运维人员都可以通过IBM&nbsp;Instana了解某个系统和应用的运行状况，及时发现问题。”朱辉介绍，“也就是说，即使是运维团队，也可以在不修改任何应用代码、不改变应用部署的前提下，进行实时监控，并且自动关联和分析应用的端到端调用链，使得动态底层架构之间的关系一目了然。”</p><p></p><p>IBM大中华区科技事业部技术销售总经理陈国豪告诉记者，IBM&nbsp;Instana可以把发现应用运行问题的时间缩短到几分钟或者几秒钟，并且能够快速解决。而相较之下，过去企业在遇到停机或者应用性能问题时，往往至少要花费至少一小时才能发现问题所在，尤其是在上云之后，基于复杂的后台和网络，这个时间可能还要更长。</p><p></p><p>他举了另一个例子：用友网络iUAP技术平台在其生产环境中有600多个微服务，同时支持十几条产品线，在部署了容器化之后会产生大约3000多个微服务实例，需要通过上千个节点支撑微服务的运行。过去，用友使用的是自己产品做实时观测，而通过IBM&nbsp;Instana，定位一个应用故障点或性能瓶颈点的时间从40分钟缩短到4分钟，效率提高了10倍以上。</p><p></p><p>所以，在IBM看来，随着IT架构变的越来越复杂，以及更多云原生应用的引入，企业需要引入更智能的方式协助——混合多云和AI将是一对强CP。</p><p></p><h2>只有企业成功技术才有价值</h2><p></p><p></p><p>当然，放到整个数字化转型的大语境下，<a href=\"https://www.infoq.cn/article/rggHjzaBfCVPV5hxTF7H\">AI</a>\"的价值绝不止于应用开发和运维场景。在各行各业众多具体的业务场景中，AI也大有可为。</p><p></p><p>比如，在全球&nbsp;20&nbsp;多个国家拥有&nbsp;9&nbsp;家研发基地、240&nbsp;多个工厂的全球汽车零部件供应商延锋国际汽车技术有限公司（以下简称“延锋汽车”）。过去，他们需要通过人工根据经验把通用订单转为内部订单，每个工厂每天需要两名工作人员花&nbsp;150&nbsp;分钟进行手工分类。即使在这样的人工投入下，仍伴随&nbsp;15%&nbsp;的分类错误，这给延锋汽车带来成本和效率的双重挑战。</p><p></p><p>为此，延锋汽车利用 IBM&nbsp;Watson&nbsp;Discovery&nbsp;的自然语言学习能力构建&nbsp;AI&nbsp;模型，从&nbsp;1.8&nbsp;亿历史数据、200&nbsp;多种排列组合、结构化数据和非结构化文本混合数据中，学习通用订单对应的内部订单背后蕴藏的规则，进而实现了运营自动化，整个流程不仅无需人工操作，并且订单分类正确率从&nbsp;85%&nbsp;提升到了97%，大大减少了返工时间。</p><p></p><p>再比如在水利行业，上海水利科技基于数据和AI提出了两个应用场景——构建智慧大脑提高工作效率和构建水利枢纽工程安全模型。</p><p></p><p>众所周知，水利建设是一个庞大的工程，需要遵守严格的流程，并且需要严谨的科学理论做支撑。而各种类型的文档及文本数据涉及到很多业领域，使得信息查询耗时耗力。尤其当工程人员在工地上时，特别困难。通过使用&nbsp;IBM&nbsp;Cloud&nbsp;Pak&nbsp;for&nbsp;Data、IBM&nbsp;Watson&nbsp;Discovery&nbsp;Cartridge&nbsp;解决方案所提供的智能文本搜索、数据科学和机器学习预测技术，上海水利科技为工程人员提供了统一平台，在一个整体视图中搜索与访问所需文档信息。通过简单关键字输入，就可以实现从众多非结构化文档（如&nbsp;PDF&nbsp;和图片）中快速定位信息。利用机器学习模型辅助文档分类和标注，为文档提供建议。借助<a href=\"https://xie.infoq.cn/article/6e8c851e2cebf0c0eed1bb4c8\">自然语音处理</a>\"（NLP）技术对非结构化数据进行分析，从文本中提取有效信息，帮助工程人员高效获取知识，辅助水利工程建设智慧决策。</p><p></p><p>同时，安全是工程建设的核心要素，水利工程需要依托实景三维模型和有限元计算模型作为数据模型资产，对大坝及其围堰结构进行分析，通过采集和管理水利工程的多维监测数据。基于&nbsp;IBM&nbsp;Cloud&nbsp;Pak&nbsp;for&nbsp;Data，运用数据科学和机器学习算法，在传统土木工程模型的基础上构建水利枢纽工程安全模型，实现数据模型优化和在线推演预测，实现结构变形预测及异常监测预警等实时应用，从而实现工程安全分析预警、综合决策等上层业务。</p><p></p><p>“其实技术都是手段，只有客户成功了，技术才有存在的价值。”朱辉强调。而可以预见，在“实用主义”主导的数字化趋势下，这种所谓“价值”将越来越重要。</p>",
    "publish_time": "2023-01-16 12:43:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "报告下载 | DQMIS高端闭门论坛成果报告——《2022第六届数据质量管理国际峰会关于数据要素发展几点看法和建议》",
    "url": "https://www.infoq.cn/article/1baec64a35106c7fdbdf2e27e",
    "summary": "<p></p><p>1月10日，DQMIS 2022第六届数据质量管理国际峰会圆满闭幕。其中本次峰会设置的高端闭门论坛在1月7日成功举办，汇聚众多专业学者与资深专家针对行业中的热点问题进行深度研讨，获得业界的高度关注。闭门论坛经过学者专家们的精华分享与充分讨论，最终形成浓缩了行业真知灼见的研讨成果——《2022第六届数据质量管理国际峰会关于数据要素发展几点看法和建议》，并由DQMIS峰会组委会秘书长谭海华先生代表本次闭门论坛与会嘉宾在峰会主论坛进行发表。</p><p></p><p>数据要素无疑是当下数据产业的关键词。近日，国家出台的“数据二十条”强调从数据产权、流通交易、收益分配、安全治理等方面构建数据基础制度，指导数据要素市场秩序建设，夯实数据要素市场发展基石。受益于法规标准持续完善以及生态持续扩容，数据要素产业迎来快速发展期。围绕数据要素的数据基础制度构建，行业的现状与挑战、数据安全与隐私计算、数据流通与开放共享等关键问题都引发数据行业从业者的重点关注。</p><p></p><p>为了全面探讨数据要素面临的问题，峰会邀请来自不同领域的近40位嘉宾出席闭门会议，重点探讨数据要素流通的现状与挑战，数据交易合规及数据流通全周期管理，数据要素流通及治理中的数据安全与隐私计算，数据质量管理及数据资产评估与数据要素流通，企业、政府数据开放、共享与数据增值，数字经济、数据要素与人才培养等20个热点话题，与会专家们针对议题分别表达了各自独到的观点。会议讨论最终归纳出关于数据要素行业发展的8个重要观点与建议，纳入本届闭门专题成果报告，包括：</p><p></p><p>1、国家“数据二十条”政策解读，对于数据要素市场的趋势影响</p><p>2、粤港澳的数据要素流通实践为行业带来参考</p><p>3、数据要素流通与数据交易所目前面临的问题与建议</p><p>4、数据安全、隐私计算与数据要素流通及数字经济发展的的影响与建议</p><p>5、数据资产评估的挑战与发展趋势</p><p>6、数据共享趋势与挑战及建议</p><p>7、企业数据资产运营的现状与挑战</p><p>8、数据人才培养观点汇总</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3db361829a433ff94700df90a2197c4e.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6b/6bcd122bc117442e6c65b45a88de1376.png\" /></p><p></p><p>扫描二维码获取闭门论坛成果报告下载地址</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/564a725f7c297ea4f73a44287c8b235e.png\" /></p><p></p><p>再次感谢本次与会专家的无私分享，同时感谢本届闭门论坛独家冠名赞助合作伙伴华矩科技对本活动的大力支持。希望通过与会专家们的探讨成果，可以为行业发展带来新思路新启发。</p><p></p><p>报告鸣谢</p><p>谭海华&nbsp; &nbsp; &nbsp;峰会组委会秘书长，华矩科技创始人及董事长CEO</p><p>时&nbsp; 静&nbsp; &nbsp; &nbsp;机械工业出版社计算机分社社长</p><p>陈艳艳&nbsp; &nbsp; &nbsp;交通运输部城市公共交通智能化交通运输行业重点实验室主任、北京工业大学城市交通学院院长</p><p>卢&nbsp; &nbsp;勇&nbsp; &nbsp; &nbsp;上海数据交易所副总经理</p><p>汤寒林&nbsp; &nbsp; &nbsp;华东江苏大数据交易中心总经理、江苏省大数据交易和流通工程实验室主任</p><p>张旭东&nbsp; &nbsp; &nbsp;华南（广东）国际数据交易有限公司总经理助理</p><p>任&nbsp; &nbsp;頲&nbsp; &nbsp; &nbsp;北京大学汇丰商学院副院长</p><p>周&nbsp; &nbsp;胜&nbsp; &nbsp; &nbsp;澳门亚太IT协会理事长</p><p>谢振东&nbsp; &nbsp; &nbsp;广州市公共交通集团有限公司大数据总监、广州羊城通有限公司董事长</p><p>李映华&nbsp; &nbsp; &nbsp;金域医学副总裁兼信息管理中心总经理，广州医科大学教授</p><p>左银康&nbsp; &nbsp; &nbsp;国信证券股份有限公司数据治理负责人（经理）</p><p>车春雷&nbsp; &nbsp; &nbsp;中国建设银行总行数据管理部处长</p><p>陈&nbsp; &nbsp;彬&nbsp; &nbsp; &nbsp;中国南方电网有限责任公司数字化部数字化管理经理</p><p>张&nbsp; &nbsp;兴&nbsp; &nbsp; &nbsp;大数据协同安全技术国家工程实验室副主任、中电长城网际系统应用有限公司副总经理</p><p>傅毅明&nbsp; &nbsp;&nbsp;&nbsp;大数据分析与应用技术国家工程实验室副主任兼网信智能中心主任</p><p>周纪海&nbsp;&nbsp; &nbsp;&nbsp;汇丰科技证券服务技术部门DevSecOps负责人</p><p>朱伟华&nbsp;&nbsp; &nbsp;&nbsp;中国汽车工业协会北斗应用分会秘书长，国家车联网产品质量检测检验中心专家委员，杭州市数据质量研究院院长</p><p>黄文彬&nbsp; &nbsp; &nbsp;北京大学信息化与信息管理研究中心主任</p><p>曹建军&nbsp; &nbsp; &nbsp;国防科技大学副研究员</p><p>郑慧娟&nbsp; &nbsp; &nbsp;广东财经大学财政税务学院副院长、副研究员，硕士生导师</p><p>潘&nbsp; &nbsp;蓉&nbsp; &nbsp; &nbsp;英国标准协会亚太首席数据治理标准专家</p><p>刘&nbsp; &nbsp;骥&nbsp; &nbsp; &nbsp;北京大成（珠海）律师事务所高级合伙人</p><p>杨&nbsp; &nbsp;春&nbsp; &nbsp; &nbsp;南方电网能源发展研究院领军专业技术专家、能源数字经济创新团队首席研究员</p><p>陈&nbsp; &nbsp;喆&nbsp; &nbsp; &nbsp;广东联合电子服务股份有限公司首席信息官</p><p>梁轶涛&nbsp;&nbsp; &nbsp;&nbsp;广东联合电子服务股份有限公司高级工程师</p><p>陈立节&nbsp;&nbsp; &nbsp;&nbsp;毕马威数据治理咨询全国主管合伙人</p><p>王青兰&nbsp; &nbsp; &nbsp;深圳市北鹏前沿科技法律研究院理事、副院长</p><p>王&nbsp; &nbsp;斌&nbsp; &nbsp; &nbsp;机械工业出版社计算机分社数字化转型产品项目负责人</p><p>晋&nbsp; &nbsp;彤&nbsp; &nbsp; &nbsp;云润大数据研究院院长</p><p>彭向阳&nbsp; &nbsp;&nbsp; 中国战略与管理研究会副秘书长、 《战略与管理》杂志《数字经济》 专刊负责人、深圳市大数据产业发展促进会创会（驻会）副会长</p><p>刘顺海&nbsp; &nbsp; &nbsp;大数据分析与应用技术国家工程实验室网信智能中心技术委员、国家物联网标识管理公共服务平台特聘专家</p><p>李&nbsp; &nbsp;翔&nbsp;&nbsp; &nbsp;&nbsp;中国软件软件交付中心副总经理、公司研究院网信工程交付部副主任</p><p>覃文延&nbsp;&nbsp; &nbsp;&nbsp;加拿大北美区块链基金会主席、加拿大数字资产交易所创始人</p><p>钱&nbsp; &nbsp;兵&nbsp; &nbsp; &nbsp;中国电信研究院AI 研发中心研发总监</p><p>谢&nbsp; &nbsp;艳&nbsp; &nbsp;&nbsp;《北大创新评论》执行主编、产经评论家</p><p>&nbsp;</p><p>更多峰会资讯</p><p></p><p>预约演讲资料下载</p><p><img src=\"https://static001.geekbang.org/infoq/e5/e5d86355c215b0c8a6ad51b15a855702.png\" /></p><p></p><p>预约峰会回放</p><p><img src=\"https://static001.geekbang.org/infoq/c5/c5465eda7137882e24e557a3a01a593b.png\" /></p><p></p><p>关注更多峰会信息</p><p><img src=\"https://static001.geekbang.org/infoq/d3/d33866ab897337d145c266b17cc2eb2b.png\" /></p><p></p><p>组委会DQPro小秘</p><p>扫描小秘微信号，加入DQMIS交流群</p><p><img src=\"https://static001.geekbang.org/infoq/2b/2b7a7f769c5b34d005262037485c4c55.png\" /></p><p></p><p>组织机构</p><p><img src=\"https://static001.geekbang.org/infoq/f1/f153a0cba71edb6d97a176d4b140e3f5.png\" /></p><p></p><p>更多峰会信息请关注“数据质量管理智库”公众号或峰会官网<a href=\"http://www.dqmis.com/2022\">http://www.dqmis.com/2022</a>\"&nbsp;</p><p>&nbsp;</p><p>DQpro数据质量管理智库以“数据质量”为研究主题的大数据行业分享智库。通过聚集中国数据治理与数据质量领域的研究专家、技术人才、学术大咖、行业代表等，建立大数据质量技术研讨圈层，分享国际前沿理念、实践案例、技术干货及权威观点等，提高我国对数据质量的认知水平和实操能力，更好地推动中国数据质量技术与大数据生态环境优化发展。</p><p>&nbsp;</p><p>&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-01-16 14:03:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "在混合云下，我们将Kubernetes与Fluid结合后性能提升了30%",
    "url": "https://www.infoq.cn/article/WAuJUZKYQDEDTGWBvYJF",
    "summary": "<p></p><p>作者 | 温芳 360 系统部数据开发高级工程师</p><p></p><p>一年前，360 系统部开始研究云舟项目——打破传统存算一体结构、保持近实时的弹性，云原生计算存储分离类似 Snowflake 的 DaaS 数仓平台，并支撑公司日益增多的机器学习任务 。</p><p></p><p>我们遇到的第一个挑战就是线下存储如何与云上的计算资源适配，数据依然存储到云下的 PoleFS 存储中，无法对接云上的 Serverless 弹性容器实例。我们倾向于使用 serverless 容器，因为它简单易用、极致弹性、最优成本、按需付费；但同时 Serverless 容器平台是黑盒系统，只支持公有云存储（NAS，对象存储）无法访问我们的线下数据。</p><p></p><p>第二个挑战是成本和性能的问题。即使退一步放弃云上的 Serverless 容器实例，改成使用云服务器访问（增加运维复杂度）绕过数据访问的问题，但是专线成本、GPU 利用率不高和训练速度慢，依然是无法绕过的问题。</p><p></p><p>这时，我们的目光投入到了 CNCF 旗下 Fluid 开源项目上，一款兼容 Kubernetes 的分布式数据集编排和加速引擎。经过很多调研，我们发现 Fluid 非常适用于处理如机器学习等数据密集型任务：Fluid 通过管理和调度 Runtime 实现数据集的可见性、弹性伸缩、数据迁移。同时，Fluid 还可以解决云端存储适配问题、降低专线成本，同时提高 GPU 的利用率。更难能可贵的是，Fluid 支持开源的 AlluxioRuntime 也满足了可以使用我司内部定制版 Alluxio 的要求。</p><p></p><p>因此，我们决定将 Fluid 作为云舟的数据编排层和数据管理层。接下来，本文将重点介绍我们是如何通过 Fluid 优化混合云场景下的机器学习任务数据访问。</p><p></p><h3>机器学习上云痛点</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/28/289684e9f93c682a56274a4a161a8aca.png\" /></p><p></p><p>PoleFs 是 360 自研的存储解决方案，但是直接从云上计算直接访问 PoleFS 中存在多种问题，包括与云端 Serverless 容器实例的兼容性问题，由于混合云存储架构中的高数据访问延迟导致的 GPU 资源浪费，较高的专线成本，以及单点存储链路问题。</p><p></p><p>第一，访问无法适配：PoleFs 作为 360 的自研存储，无法对接云上的 Serverless 容器实例。比如阿里云的 ECI 只支持自身存储（OSS，CPFS，NAS），但无法与 PoleFs 对接。这样需要访问 PoleFs 的计算任务就无法利用到阿里云的 Serverless 容器按需弹性伸缩，免运维等能力。</p><p></p><p>第二，GPU 资源浪费：即使退一步放弃云上的 Serverless 容器实例（ECI），改成使用云服务器访问，可以绕过数据访问的问题。现有混合云场景存储分离架构导致数据访问延时高。机器学习训练的过程中不断的从 PoleFS 拉取数据，造成网络宽带高、IO 成为了瓶颈，大大降低了 GPU 的利用率，导致使用云上资源成本反而更高。</p><p></p><p>第三，专线昂贵：Kubernetes 缺乏感知数据缓存的能力。对于多次访问的数据集，性能上并没有提升，一次次的拉取数据带来额外的网络专线开销。</p><p></p><p>第四，存储单点：PoleFs 成为数据并发访问的瓶颈点。（PoleFS 是我们部门自研的分布式文件系统）。云上和云下大量的机器学习训练造成的 IO 压力比较大， 访问 PoleFs 链路成为了单点链路，一旦 PoleFS 带宽出现瓶颈或者 PloeFS 不响应则会影响所有机器学习任务。</p><p></p><h3>选择 Fluid 的原因</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/65/65e5cd4195cb02fb84d13e2330ea9fbd.png\" /></p><p></p><p>我们使用 Fluid 对优化云上计算资源访问 PoleFS，Fluid 的引入为整个项目带来了很多的优势：</p><p></p><p>应用无侵入，存储无修改实现数据接入：通过引入 Fluid，业务方无需修改应用，使用简单；对下存储团队无需修改 PoleFS 就可以实现云上计算对接 PoleFS。数据本地性调度作业，加入 Fluid 作为缓存之后，机器学习任务只需要访问 pvc 去进行数据训练，Fluid 根据数据对任务进行调度，尽可能达到了数据本地读，减少了 IO 压力和网络压力。数据预加载功能。fluid 具有数据预加载能力，可以在任务训练之前将所需要的数据拉取到缓存中，这样大大加快了训练速度，提高了 GPU 的利用率。数据复用。多个任务可以共享数据缓存，避免了同一份数据拉取多次带来的网络消耗。</p><p></p><h3>如何落地实践</h3><p></p><p></p><h4>安装部署</h4><p></p><p></p><p>在阿里云上可以直接通过安装云原生 AI 套件完成 Fluid 的安装，如果在自己的数据中心使用，也可以按照如下安装方式：</p><p></p><p>1. 首先，需要创建 namespace，fluid-system；</p><p></p><p><code lang=\"sql\">kubectl create ns fluid-system\n</code></p><p></p><p>2. 然后，从 Fluid 官网 [<a href=\"https://fluid-cloudnative.github.io/\">https://fluid-cloudnative.github.io/</a>\"] 下载 Fluid 最新版本；</p><p></p><p>3. 下载完后，就可以通过 helm 进行安装；</p><p></p><p><code lang=\"sql\">helm install fluid fluid-.tgz\n</code></p><p></p><p>4. 然后，可以检验一下安装是否成功</p><p></p><p><code lang=\"cs\">kubectl get po -n fluid-system\n</code></p><p></p><p></p><h4>使用方法</h4><p></p><p></p><p>首先，创建一个 pvc polefs，底层挂载存储的是 PoleFS。</p><p></p><p><code lang=\"properties\">￥kubectl get pv\nNAME      CAPACITY   ACCESS MODES   RECLAIM POLICY       STORAGECLASS         STATUS\npolefs    30000Gi    RWX            Retain               polefs-storageclass  Bound\n</code></p><p></p><p>然后，创建 dataset，通过 mountPoint 指向 polefs-acc pvc, 此处为了满足业务要求，我们直接挂载到了根目录。由于 Fluid 支持 PVC 协议，我们可以很容易的实现 PoleFS 和云的对接。</p><p></p><p><code lang=\"properties\">apiVersion: data.fluid.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: polefs-acc\nspec:\n  mounts:\n    - mountPoint: pvc://polefs\n      name: /\n  accessModes:\n    - ReadWriteMany\n</code></p><p></p><p>之后，创建 AlluxioRuntime，我们测试了 Alluxio 使用 SSD 和内存时性能对比，发现差别不大，所以我们最终选用了 SSD 做 alluxio 的缓存介质。</p><p></p><p><code lang=\"properties\">apiVersion: data.fluid.io/v1alpha1\nkind: AlluxioRuntime\nmetadata:\n  name: polefs-acc\nspec:\n  replicas: 3\n  data:\n    replicas: 1\n  master:\n    jvmOptions:\n      - -Xms20G\n      - -Xmx20G\n      - -XX:+UseG1GC\n  tieredstore:\n    levels:\n      - mediumtype: SSD\n        path: /ssd        \n        quota: 50Gi\n        high: \"0.99\"\n</code></p><p></p><p>Fluid 在拉起 alluxio 之后，会创建一个 pvc：</p><p></p><p><code lang=\"properties\">kubectl get pv,pvc -n wz\n\nNAME                              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS\npersistentvolume/wz-polefs-acc  100Gi      RWX            Retain           Bound    \n\nNAME                                 STATUS   VOLUME            CAPACITY   ACCESS MODES  \npersistentvolumeclaim/polefs-acc   Bound    wz-polefs-acc    100Gi      RWX\n</code></p><p></p><p>当看到 pvc 状态为 Bound 时，就可以直接使用了。我们的机器学习伙伴可以使用 pvc polefs-acc 访问到自己的数据。</p><p></p><p>之后，为了提升机器学习任务训练的速度，我们会先对数据进行预加载。</p><p></p><p>接下来，创建 dataload 对数据进行预加载。</p><p></p><p><code lang=\"properties\">apiVersion: data.fluid.io/v1alpha1\nkind: DataLoad\nmetadata:\n  name: polefs-acc\nspec:\n  dataset:\n    name: polefs-acc\n    namespace: wz\n  target:\n   - path: /data/test/images\n</code></p><p></p><p>数据预加载完成后，就可以方便的使用云上 ECS 和 ECI 资源，灵活的进行机器学习任务训练啦。</p><p></p><h4>遇到的问题</h4><p></p><p></p><p>我们司内对 alluxio 进行了定制改造，在刚刚引入 fluid 时，我们发现通过 helm 安装时，fluid 还没有支持设置自己版本的 alluxio 镜像，我们及时将我们的需求反馈给社区，社区很给力，很快就帮我们实现了这个功能。</p><p></p><p>由于我们的平台会有新的业务接入，我们的一个 fluid+alluxio 集群会跑很多业务的作业。刚开始 fluid 只支持静态挂载 dataset，一旦集群启动好就无法挂载别的路径，这样势必会影响我们新业务、新集群的接入，因此我们开发了自动挂载的功能，并提交给了 Fluid 社区。</p><p></p><p>为了平滑迁移用户作业，我们想在不改动用户作业的情况下将机器学习作业迁移到 Kubernetes 上，这时由于我们的 pole-fs 已经挂载了一层目录，导致我们再用 alluxio 挂载时会多一层目录。后来，我们通过在 dataset 设置挂载点根路径特性实现了将 pole-fs 的 pvc 挂载到了 alluxio 的根上，这样解决了我们平滑迁移的问题。</p><p></p><h3>实践结果：30% 的性能提升</h3><p></p><p></p><p>我们对直接 IDC 服务器访问 PoleFS 训练和加入 Fluid 作为缓存在云上训练进行了性能对比测试，该训练场景的小文件偏多、有近百万张图片。下面是性能对比图：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/28/28cac0fd3ad4cc4462dae486daab228c.png\" /></p><p></p><p>测试得出，加入 Fluid+360 优化过的 alluxio 对数据进行预热，在云上运行机器学习任务即使和在自建数据中心通过 PoleFS 直接运行机器学习任务相比，在执行效率上还有大约 30% 的提升，同时也提高了 GPU 的利用率，也减少了专线的费用开销。</p><p></p><p>通过使用 Fluid，我们可以简单安心地使用云上计算资源构建混合云平台下的机器学习平台。</p><p></p><h3>哪些经验可以分享</h3><p></p><p></p><p>根据实践，我们总结了以下五个方面的经验供大家参考。</p><p></p><p>选择合适的缓存节点: 使用 AlluxioRuntime 通过缓存获得更好的数据访问性能，在实际生产中我们发现并非所有节点都来做缓存性能就比较好。原因最开始云上缓存选择了不同 ECS 类型，而有些 ECS 的磁盘和网络 IO 性能不是很好，这个时候需要我们能够把缓存节点调度到同种类型的 ECS 上。Fluid 支持 dataset 的可调度性，也就是说，可以通过指定 dataset 的 nodeAffinity 来进行数据集缓存节点的调度，从而保证缓存节点可高效的提供缓存服务。指定 Master 调度策略：alluxio 是典型的主从架构，master 作为集群的大脑，负责服务所有用户请求并管理元数据，master 的稳定性至关重要，而宿主机的磁盘、内存、网络是影响 master 稳定性的一个重要因素，因此我们通过配置 master 的 nodeselector 选择比较好的宿主机部署 master，以进一步保证 master 的稳定性。定时数据预热: 在进行训练前的一个重要的步骤是进行数据的预热，Fluid 提供原生支持丰富策略的数据预热，可在训练前将训练文件的元数据和数据缓存到本地，可大大加速训练速度。只读场景给 dataset 设置 readOnly：我们的训练数据集是只读的，并且在训练期间不会发生变化，此时可以通过将 dataset 设置为 readOnly，Fluid 自动开启各级缓存，包括 FUSE 层和 Alluxio 元数据缓存，这样可以避免额外开销，减少非必须的调用链路。比如避免不必要的元数据交互。而需要写 checkpoint 的，则创建一个支持读写的 dataset，应用同时挂载两个 dataset。自动扩缩容：我们通过我们的麒麟平台在高峰期是将作业调度到 fluid 管理的 alluxio 集群，在高峰期 alluxio 的缓存容量可达 95% 以上，而在低谷期时几乎没有作业，这时我们利用了 fluid 的自动扩缩容的功能，我们配置扩缩容策略，在高峰时检测到 alluxio 缓存容量达到 90% 时扩容。低谷期时我们的 alluxio 集群基本上没有作业在运行，这时开始缩容到默认节点数。自动扩缩容功使我们更加灵活的使用通过分布式缓存提升数据访问加速能力，并且大大的节省了成本，提高了资源使用率。</p><p></p><p></p><h3>总结</h3><p></p><p></p><p>我们混合云器学习平台在引入 Fluid 之后，简化了云上数据访问复杂度，同时性能和成本上也达到了比较满意的效果。而且我们也看到了云上弹性资源使用自有存储在成本，效率上都有不错的收益，也打消我们之前对于混合云场景下利用云上资源扩展机器学习平台的算力受制于线下存储的顾虑。</p><p></p><p>现在 Fluid 已经成熟，在刚开始引入 Fluid 的时候，我们遇到了很多的问题，也发现了 Fluid 的一些 bug，感谢 Fluid 社区及时的修复以及对我们的需求的及时支持。感谢车漾、徐之浩和顾荣等几位老师的对我们的指导和帮助。</p><p></p><p>未来，360 团队会加大在 Fluid 开源社区的投入，也希望更多感兴趣的人加进来，一起共建 Fluid 社区。期待 Fluid 成为一流的开源产品。</p>",
    "publish_time": "2023-01-16 14:36:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "服务网格发展5年：复杂性问题悬而未决，社区依然“热闹”非凡",
    "url": "https://www.infoq.cn/article/6ZxKuL63J96B5DjKKibz",
    "summary": "<p>Service Mesh（服务网格）的概念由 Buoyant CEO William Morgan 首次提出。2017 年 4 月该公司发布了第一个 Service Mesh 产品 Linkerd。当时在同一时间发表的文章《<a href=\"https://linkerd.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/\">What’s a service mesh？And why do I need one?</a>\"》也被公认是 Service Mesh 的权威定义。</p><p>&nbsp;</p><p></p><blockquote>“A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the service mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.”翻译：Service Mesh 是一个处理服务通讯的专门的基础设施层。它的职责是在由云原生应用组成服务的复杂拓扑结构下进行可靠的请求传送。在实践中，它是一组和应用服务部署在一起的轻量级的网络代理，对应用服务透明。</blockquote><p></p><p>&nbsp;</p><p></p><h2>为什么需要 Service Mesh？</h2><p></p><p>&nbsp;</p><p>Service Mesh 诞生的背景主要有两点：首先，微服务架构模式逐渐流行，开发者将多个服务组合在一起来构建应用程序；其次，企业已经使用了云原生平台技术，例如容器（Docker）、编排器（Kubernetes）和网关等。</p><p>&nbsp;</p><p>Service Mesh 模式试图解决的问题包括：</p><p>&nbsp;</p><p>无需将特定语言的通信库编译到单个服务中来处理服务发现、路由和application-level (Layer 7) 非功能性通信需求。外部化服务通信配置，包括外部服务的网络位置、安全凭证和服务质量目标。为其他服务提供被动和主动监控。在整个分布式系统中，执行分散策略。提供可观察性默认值并标准化相关数据的收集，如启用请求日志记录、配置分布式跟踪、收集指标等。</p><p>&nbsp;</p><p>Phil Calçado在<a href=\"https://philcalcado.com/2017/08/03/pattern_service_mesh.html\">Pattern：Service Mesh</a>\" 一文中讲述了服务通信演变的过程：</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/f1/f181c3e424f72680ce42d8259432e0c6.png\" /></p><p></p><p>&nbsp;</p><p>最初，流量管理和控制能力（比如图例中的熔断、服务发现）是和业务逻辑耦合在一起，即便以引用包的方式被调用，依然解决不了异构系统无法重用的问题。流控功能和业务耦合相当不美好，于是出现了提供这些功能的公共库和框架。但这些库通常比较复杂，无论是学习使用，与业务系统整合、维护都会带来很大的成本。为避免花费太多时间开发和维护这些通用库，人们希望流量控制能力可以下沉到网络通讯栈的层面，但几乎无法实现。于是另一种思路出现，就是将这些功能独立成一个代理，由它先接管业务服务的流量，处理完成后再转发给业务服务本身，这就是 Sidecar 模式。为统一管理 Sidecar，该模式进一步进化，形成网络拓扑，增加了控制平面，演变成 Service Mesh（最后的网格图中，绿色代表业务服务，蓝色代表 sidecar 服务）。</p><p>&nbsp;</p><p>从上图可以看出，Service Mesh 就是 Sidecar 的网络拓扑形态，Mesh 这个词也由此而来。Service Mesh 的出现主要带来了以下两方面的变革：</p><p>&nbsp;</p><p>解决了微服务框架中的服务流量管理的痛点，使开发人员专注于业务本身；将服务通信及相关管控功能从业务程序中分离并下层到基础设施层，使其和业务系统完全解耦。</p><p>&nbsp;</p><p></p><h2>生态发展</h2><p></p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdb2f831e631a234964997970a2df0d5.png\" /></p><p></p><p>&nbsp;</p><p>2013 年，Airbnb 发布了SmartStack ，为新兴的“微服务”架构提供了进程外服务发现机制（使用HAProxy ）。当然，许多以企业在此之前都进行过此类技术的研究。从 2000 年代初开始，Google 就在开发 Stubby&nbsp;RPC 框架，后来演变为 gRPC 以及Google Frontend (GFE) 和 Global Software Load Balancer (GSLB)，在Istio中依然可以看到这些产品的特征。2010 年代初期，Twitter 开始开发基于 Scala 的 Finagle，Linkerd 由此衍生而来。</p><p>&nbsp;</p><p>2014 年底，Netflix 发布了一整套基于 JVM 的实用程序， 包括Prana，这是一个“sidecar”进程，允许任何语言编写的应用程序服务通过 HTTP 与库的独立实例进行通信。2016 年，NGINX 团队开始谈论“ Fabric 模型”，它与服务网格非常相似，但需要使用他们的商业 NGINX Plus 产品来实现。此外，Linkerd v0.2 于 2016 年 2 月发布，尽管该团队后来才开始将其称为Service Mesh。&nbsp;&nbsp;</p><p>&nbsp;</p><p>Service Mesh历史上的其他重要事件还包括 2017 年 5 月发布的Istio、2018 年 7 月的Linkerd 2.0、2018 年 11 月的Consul Connect 和Gloo Mesh 、2019 年 5 月的服务网格接口 (SMI) 以及同年9月发布的 Maesh（现称为 Traefik Mesh ） 和 Kuma。&nbsp; &nbsp; &nbsp;</p><p>&nbsp;</p><p>其他企业的产品，如HashiCorp的Consul等，也都是从上述技术中汲取灵感。</p><p>&nbsp;</p><p></p><h2>实践现状</h2><p></p><p>&nbsp;</p><p>很多企业都是多协议、多语言栈的，他们选择使用 Service Mesh 来解决复杂的服务治理问题。在之前的一些实践取得正反馈后，Service Mesh 使用范围也在扩大。如今的Service Mesh 不再局限于 RPC，开始向对象存储、加解密、MySQL、Redis 等领域深入。</p><p>&nbsp;</p><p>但总体看，如今Service Mesh 落地还是遇到了大的技术挑战，远没有达到企业理想的使用状态。有一定研发能力的企业使用传统治理模式也可以做得不错，这时就不会选择完全换成 Mesh 架构，只会在一些新的、没有历史负担的业务上试用。</p><p>&nbsp;</p><p>本质上，Service Mesh 只是转移了复杂度，当业务发展到一定规模后，复杂度问题就会再次显现。sidecar 模式很适用于逻辑复杂的场景，如路由、治理，灵活且对业务无入侵。但在大规模场景下，其复杂度就上来了，性能优势不再明显，资源占用也变得不可忽略。可以说，sidecar 模式天生在大规模场景应用中就有一定的局限性。</p><p>&nbsp;</p><p>为解决这个问题，今年九月，Istio 推出了 Sidecarless 的 Ambient Mesh。Ambient 是将 Istio 分成两个不同的层次：安全覆盖层（四层治理）和 七层处理层（七层治理）。但在网易数帆云原生平台负责人冯常健看来，四层治理模式将复杂度降到了 Node 级别，但可能只有对网格安全能力感兴趣的企业会尝试，而七层治理模式本质上还是独立的应用层代理，链路也并未减少。因此，对于该模式的应用，业内更多还是持观望态度。</p><p>&nbsp;</p><p>现在，Service Mesh社区还在统一的方向演化。比如今年4月谷歌声明将 Istio 捐赠给 CNCF，9 月份 Istio 正式成为 CNCF 孵化项目。这一事件使 CNCF 社区的确定性更强，也消除了前些年大家对社区治理、法规等方面的顾虑。</p><p>&nbsp;</p><p>但在网关层面，社区基本还是分为 NGINX 和 Envoy 两派：Kong 、APISIX 等基于 NGINX，网易、阿里云等更多应用 Envoy 技术栈。有人认为 NGINX 及其生态已经比较成熟了，但随着 Kubernetes Gateway API 的成熟，今年社区推出了 Envoy Gateway 组件，新一轮网关标准定义的争论再次掀起。</p><p>&nbsp;</p><p>Kubernetes Gateway API 对标的是 Ingress API。Ingress 的 API 解决流量从集群外导入集群内的问题，但表达能力较弱，使用场景有限，因此社区推出了 Kubernetes Gateway API，希望其提供更高级的网络能力。</p><p>&nbsp;</p><p>Kubernetes Gateway API 直接促进了 Envoy Gateway 项目的发展。Envoy Gateway 进而统一了网关的控制面 API。原先网关控制面是通过 xDS 控制数据面，现在更多会基于 Kubernetes Gateway API。</p><p>&nbsp;</p><p>实际上，现在各个企业都在从不同的方向尝试对 Service Mesh 进行完善和补充。虽然社区有了各种开源产品，但业内还没有形成像 Kubernetes 这样的事实标准。当有这样的一个事实标准出来后，Service Mesh 才会迎来自己的爆发。这与容器的发展轨迹是类似的。</p><p>&nbsp;</p><p>Service Mesh 也在寻找更适合的落地方式。现在，业内有尝试不再将 Service Mesh 作为一个独立的产品，而是将其与 Serverless 结合。Serverless 不让用户去关心服务器，Service Mesh 不让用户关心服务治理，如果将服务治理的 Service Mesh 容器内置到 Serverless 平台里面，企业提交一个业务的容器进项后也会拥有 Serverless 的能力。</p>",
    "publish_time": "2023-01-16 14:48:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "事件驱动架构要避开的5个陷阱",
    "url": "https://www.infoq.cn/article/7Wbo3Ivcv5rvosyL10TO",
    "summary": "<p>事件驱动架构非常强大，非常适合用在分布式微服务环境中。事件驱动架构提供了解耦的架构、更容易实现的可伸缩性和更高程度的弹性。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e500d7de4630d14e7c81faaaecd76255.png\" /></p><p></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>请求应答（客户端和服务器）与事件流（发布和订阅）</p><p></p><p>但是，与请求和应答类型的架构相比，正确使用事件驱动架构要困难得多。</p><p></p><p>在过去的几年里，我们一直在逐步将我们不断增长的微服务（目前有 2300 个）从请求和应答模式迁移到事件驱动架构。下面是 Wix 工程师在实验事件驱动架构时遇到的 5 个陷阱。</p><p></p><p>这些陷阱让我们遭遇了生产事故，给我们带来了巨大痛苦，我们不得不进行重写，还得面对陡峭的学习曲线。对于每一个陷阱，我都提供了已经在 Wix 使用的经过实战验证的解决方案。</p><p></p><h2>1. 写入数据库再触发事件（非原子操作）</h2><p></p><p>我们以一个简单的电子商务流程为例（我们将在本文中使用这个示例）。</p><p></p><p>在支付处理完成后，应该更新商品库存，表示为客户保留商品。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d41a91ec84d7257249aabef575b7c3be.png\" /></p><p></p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p>写入数据库和产生事件是非原子操作</p><p></p><p>问题在于，将支付完成状态写入数据库，然后向 Kafka（或其他消息代理）生成“支付完成”事件并不是一个原子操作。在某些情况下，可能只有其中一个动作执行成功。</p><p></p><p>例如，数据库不可用或 Kafka 不可用可能会导致分布式系统不同部分之间的数据不一致。在这种情况下，库存可能与实际订单不一致。</p><p></p><h3>原子性补救 1——Greyhound 弹性生产者</h3><p></p><p>有几种方法可以缓解这个问题。在 Wix，我们使用了两种方式。第一种是使用我们自己的消息平台Greyhound（https://github.com/wix/greyhound#greyhound），我们通过弹性生产者确保事件最终被写入 Kafka。这种缓解措施的缺点是最终可能会导致对下游事件的无序处理。Greyhound</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f4a15e0ba83edd67888e2be5d0bb3abe.png\" /></p><p></p><p>Greyhound 生产者回退到 S3，一个将消息恢复到 Kafka 的专用服务</p><p>&nbsp;</p><p></p><h3>原子性补救 2——Debezium Kafka 源连接器</h3><p></p><p>第二种确保数据库更新动作和 Kafka 生成动作都发生并且数据保持一致的方法是使用 Debezium Kafka 连接器。Debezium 连接器可以自动捕获数据库中发生的变更事件（CDC），并将它们生成到 Kafka 主题中。</p><p></p><p>使用 Debezium 数据库连接器和 Kafka Connect 结合使用可以保证事件最终被生成到 Kafka。此外，还可以保持事件的顺序。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5a/5a4119f314bb55870fbedf3292b11aeb.png\" /></p><p></p><p></p><p>Debezium 连接器确保变更事件最终与数据库保持一致</p><p></p><p>需要注意的是，Debezium 也支持其他事件流平台，如 Apache Pulsar。</p><p></p><p></p><h2>2. 事件溯源无处不在</h2><p></p><p>在事件溯源模式中，服务不是在业务操作时更新实体的状态，而是将事件保存到数据库中。服务通过重放事件来重建实体的状态。</p><p></p><p>这些事件也被发布到事件总线上，其他服务也可以在其他数据库上创建物化视图，这些数据库通过重放事件优化查询。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1f/1fed3bdbfd32c40bbcb08e39c7548434.png\" /></p><p></p><p>事件溯源——将变更事件持久化到事件存储中，通过重放事件重建状态</p><p></p><p>虽然这种模式有一定的优点（可靠的审计日志、实现“时间旅行”——能够在任何时间点获取实体的状态，并在相同的数据上构建多个视图），但到目前为止，它比更新保持在数据库中的实体状态的 CRUD 服务要复杂得多。</p><p></p><h3>&nbsp;事件溯源的缺点</h3><p></p><p>复杂性——为了确保读取性能不受重放事件的影响，必须不时地获取实体状态快照，以减少性能损失。这增加了系统的复杂性，因为后台进程可能会出问题。当后台进程出问题时，数据可能是过时的。最重要的是，拥有两个数据副本意味着它们可能会不同步。雪花属性——与 CRUD ORM 解决方案不同，事件溯源很难创建通用库和框架来简化开发并全局解决适合每一个应用场景的快照和读取优化。只支持最终一致性（不适合写后读的场景）。</p><p></p><h3>事件溯源替代方案——CRUD + CDC</h3><p></p><p>利用简单的 CRUD 和向下游发布数据库变更事件（例如创建查询优化的物化视图）可以降低复杂性，增加灵活性，并仍然可以在特定用例中实现命令查询责任隔离（CQRS）。</p><p></p><p>对于大多数场景，服务可以公开一个简单的读取端点，这个端点从数据库获取实体的当前状态。随着规模的扩大，需要更复杂的查询，这个时候可以使用额外发布的变更事件来创建专门为复杂查询定制的物化视图。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b4f59ff25f15aff2dc5f821c2c7febde.png\" /></p><p></p><p>CRUD——简单地读取数据库 + 用于外部物化视图的 CDC</p><p></p><p>为了避免将数据库变更作为契约暴露给其他服务，并在它们之间创建耦合，服务可以读取 CDC 主题并生成变更事件的“官方”API，类似于在事件溯源模式中创建的事件流。</p><p></p><h2>3. 无上下文传播</h2><p></p><p>切换到事件驱动架构意味着开发人员、DevOps 和 SRE 在调试产品问题和跟踪用户请求方面可能存在更大的困难。</p><p></p><p>与请求和应答模型不同，事件驱动架构没有可跟踪的 HTTP/RPC 请求链。调试代码变得更加困难，因为事件处理代码分散在服务代码中，无法通过简单地单击对象或模块的函数定义进行跟踪。</p><p></p><p>我们仍然以本文中使用的电子商务流程为例。订单服务必须使用多个来自 3 个不同主题的事件，这些事件都与同一个用户操作（在网商购买商品）相关。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6d70e27c8d956cefcec2192c579a89ff.png\" /></p><p></p><p>完全事件驱动的微服务很难跟踪请求流</p><p></p><p>其他服务也使用来自一个或多个主题的多个事件。我们假设某些商品的库存水平是不正确的，这个时候，调查所有相关订单事件的处理就变得至关重要。否则，我们需要花很长时间查看各个服务的日志，并尝试手动将不同的证据片段连接在一起。</p><p></p><h3>自动上下文传播</h3><p></p><p>自动为所有事件添加请求上下文使得过滤与用户请求相关的事件变得非常简单。在我们的示例中，添加了 2 个事件标头——requesttid 和 userId。这两个标识都能极大地帮助我们进行问题诊断。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d0/d04f064ba6225ee2cc23f75e728fc13b.png\" /></p><p></p><p>为每个事件自动附加用户请求上下文，便于跟踪和调试</p><p></p><p>在 Wix，当事件被生成和消费时，Greyhound 会自动传播用户请求上下文。此外，我们还可以在日志中找到请求上下文，这样就可以针对特定的用户请求过滤日志。</p><p></p><h2>4. 发布包含大消息体的事件</h2><p></p><p>在处理包含大消息体的事件（大于 5MB，例如图像识别、视频分析等）时，人们可能会倾向于将它们发布到 Kafka（或 Pulsar），但这可能会大大增加延迟、降低吞吐量并增加内存压力（特别是当不使用分层存储时 0。</p><p></p><p>幸运的是，有几种方法可以克服这个问题，包括压缩、将消息体拆分为块、将消息体放入对象存储并只在流式平台中传递引用。</p><p></p><h3>大消息体补救措施 1——压缩</h3><p></p><p>Kafka 和 Pulsar 都支持压缩消息体。你可以尝试几种压缩算法（lz4、snappy 等），找到最适合你的压缩算法。如果消息体比较大（最多 5MB）， 50% 的压缩率可以帮你保持消息代理集群良好的性能。</p><p>Kafka 级别的压缩通常比应用程序级别的更好，因为消息体可以批量压缩，从而提高压缩比。</p><p></p><h3>大消息体补救措施 2——分块</h3><p></p><p>减少代理压力和覆盖消息大小限制的另一种方法是将消息分割为块。</p><p></p><p>分块是 Pulsar 的内置功能（有一些限制），但对于 Kafka 来说，分块必须发生在应用程序级别。</p><p></p><p>如何在应用程序级实现分块的示例可以在这里（https://medium.com/wix-engineering/chunks-producer-consumer-f97a834df00d）和这里（https://medium.com/workday-engineering/large-message-handling-with-kafka-chunking-vs-external-store-33b0fc4ccf14）找到。基本前提是生产者发送带有额外元数据的数据块，帮助消费者重新组装它们。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85d872f7638f92fb98c29b6d5ae9e9c6.png\" /></p><p></p><p>生产者将数据分成块，消费者将其组装复原</p><p></p><p>这两种示例方法的不同之处在于它们如何组装数据块。第一个示例将数据块保存在某个持久存储中，当所有数据块都生成后，消费者一次性获取所有数据块。第二个示例让消费者在所有数据块到达后在主题分区中向后查找第一个数据块。</p><p></p><h3>大消息体补救措施 3——使用对象存储的引用</h3><p></p><p>最后一种方法是简单地将消息体内容存储在对象存储中（如 S3），并将对象的引用（通常是 URL）作为事件的消息体。这些对象存储允许在不影响第一个字节延迟的情况下持久化任何所需的大小。</p><p></p><p>在生成链接之前，需要确保消息体内容已经完全上传到对象存储中，否则消费者需要不断重试，直到可以开始下载它。</p><p></p><h2>5. 不处理重复事件</h2><p></p><p>大多数消息代理和事件流平台默认保证至少一次传递，这意味着一些事件可能出现重复，或者可能会被处理两次（或多次）。</p><p></p><p>确保重复事件的副作用只发生一次叫作幂等性。</p><p></p><p>我们继续以本文中一直使用的电子商务流程为例。由于一些处理错误导致需要进行重复处理，记录到库存数据库中的已购买商品的库存量下降得可能比实际的要多一些。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2f/2f8dca1827fe937c342f6dfe5cbec132.png\" /></p><p></p><p>消费者多次处理导致库存变得不正确</p><p></p><p>其他副作用包括多次调用第三方 API（在我们的示例中，这可能意味着对相同的事件和商品两次调用降低库存数量的服务）。</p><p></p><h3>等幂补救——revisionId（版本控制）</h3><p></p><p>在需要等幂处理的情况下，可以考虑使用乐观锁技术。在发生更新之前需要先读取存储实体的当前 revisionId（或版本），如果有多方尝试同时更新实体（同时增加版本），那么第二个尝试更新的一方将失败，因为版本与之前读取的不匹配。</p><p></p><p>在对重复事件进行幂等处理时，revisionId 必须是唯一的，并且是事件本身的一部分，这样可以确保两个事件不共享相同的 id，并且针对同一 revisionId 的第二次更新将（静默地）失败。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d580177133156acd14e9f31772db388.png\" /></p><p></p><p>为每个事件附加 transactionId，避免重复处理</p><p></p><p>特别是在使用 Kafka 时，有可能配置精确一次语义，但由于某些故障，数据库更新仍然可能出现重复。在这种情况下，txnId 可以是主题 - 分区 - 偏移量三元组，这样可以保证 id 是唯一的。</p><p></p><h2>总结</h2><p></p><p>迁移到事件驱动架构可以是一个循序渐进的过程，这样可以减少与之相关的风险，比如调试困难和心里层面的复杂性。微服务架构允许为每个不同的服务灵活地选择模式。HTTP/RPC 端点可以作为事件处理的一部分进行调用，反之亦然。</p><p></p><p>作为这种渐进迁移的结果，我强烈建议采用 CDC 模式，将其作为一种既能确保数据一致性（陷阱 1）又能避免与完全成熟的事件溯源相关的复杂性和风险（陷阱 2）的方法。CDC 模式仍然允许将请求和应答模式与事件处理模式结合在一起。</p><p></p><p>解决陷阱 3（在事件流中传播用户请求上下文）将大大提高快速查找生产事故根源的能力。</p><p></p><p>陷阱 4 和陷阱 5 的补救措施是针对具体场景的——陷阱 4 的消息体非常大，而陷阱 5 的副作用不是幂等的。如果没有必要，就不需要做出这些变更。尽管作为最佳实践，可以使用压缩（陷阱 4）和事务 ID（陷阱 5）。</p><p></p><h5>原文链接：</h5><p></p><p>https://natansil.medium.com/event-driven-architecture-5-pitfalls-to-avoid-b3ebf885bdb1</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/d1f44702cf0c8c2d146d7e88a\">一文读懂什么是 Web3 架构</a>\"</p><p><a href=\"https://xie.infoq.cn/article/166f934380b81621d6949f6c8\">深度剖析 Apache EventMesh 云原生分布式事件驱动架构</a>\"</p><p><a href=\"https://xie.infoq.cn/article/53be702d83bca45d4d80e81c6\">事件总线 + 函数计算构建云上最佳事件驱动架构应用</a>\"</p><p><a href=\"https://xie.infoq.cn/article/b8b721ecea37635c46ae24d20\">企业级业务架构设计：方法论与实践 学习笔记</a>\"</p>",
    "publish_time": "2023-01-16 14:52:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "五年后，谷歌还在全力以赴发展 Kotlin",
    "url": "https://www.infoq.cn/article/Rk14ZrMSF2gyAwCrTNr6",
    "summary": "<p>自 2017 年谷歌 I/O 全球开发者大会上，谷歌首次宣布将 Kotlin（JetBrains 开发的 Java 虚拟机静态类型语言）作为编写安卓应用的一流语言，至今已超过五年。自那以后，谷歌又进一步将 Kotlin 列为 2019 年度编写安卓应用的首选语言，尽管许多开发者依然在使用 Java，但是 Kotlin 很快就成为了谷歌移动操作系统中构建应用的默认方式。追溯到 2018 年，谷歌和 JetBrains 联手创建了 Kotlin 基金会。</p><p></p><p>TechCrunch 编辑 Frederic Lardinois 与谷歌公司的 Kotlin 产品经理 James Ward 坐在一起，讨论了该语言在安卓生态系统及其他领域的角色，以及该公司对它的未来计划。</p><p></p><p>谷歌希望随着时间的推移，所有的安卓开发者都能改用 Kotlin，这不足为奇。Ward 表示：“在安卓上还有很多 Java 应用。我们知道，相比 Java，开发者更满意 Kotlin。我们知道，他们的工作更有效率，应用的质量也更好，因此，我们的首要任务就是，让更多的人将他们的代码库转移到 Kotlin 上面来。Kotlin 与 Java 的互操作性让人们能够逐渐地把代码库迁移到上面来，如果能够实现一切都是 Kotlin，那就太好了。”</p><p></p><p>然而，谷歌并未完全实现这一目标，部分是因为 Java 的生态系统太过庞大，所以它仍然具有巨大的吸引力。因为 Kotlin 与 Java 具有互操作性，所以开发者可以将这两者组合使用，并对库进行匹配，但是要想从 Kotlin 中得到全部的好处，开发者就必须待在 Kotlin 的生态系统中。虽然所有人都在关注 Kotlin，但是有一点很重要，那就是安卓平台的内核和 API 依然基于 Java。目前已经有了使用 Kotlin 编写的安卓库，但是很明显，这仅仅是整个平台的一小部分。</p><p></p><p></p><p></p><p></p><p>但是，Kotlin 现在也不仅仅局限于安卓。现在，服务器端的 Kotlin 在谷歌已经非常普及了，到目前为止，Kotlin 代码在谷歌内部代码库里已经有 850 万行了。谷歌表示，这一数字，每年都会增加一倍。</p><p></p><p>谷歌和 JetBrains 最近几年来都在致力于重新编写 Kotlin 编译器。新的编译器保证速度更快，接口更好，IDE 可以改善更好的代码提示和静态代码分析，目前已经进入测试阶段，明年有望发布。因为该公司在此项目上投入了大量的资金，同时并行维护两个编译器，这就使得语言自身的开发速度放缓了一些。</p><p></p><p>Ward 说：“我们故意放缓了语言的变化速度，因为我们有两个编译器在并行进行，一旦我们发布了新的编译器，并且所有人都开始使用它，我们就可以在新的语言功能上投入更多的资源。”</p><p></p><p>在这些新功能中，有上下文接收器，它目前仍然在标志后面。这些将允许开发人员将参数传递给某个函数，比如，只要一次写入与数据库相连的代码和值，就可以在每次需要重新连接时，都会再次使用相同的上下文。</p><p></p><p>至于 Kotlin 基金会，值得注意的是，直到现在，只有谷歌和 JetBrains 是基金会的成员，这两家公司利用该基金会来协调他们对 Kotlin 的投资。但 Ward 解释说，这两家公司都希望通过新的成员来扩大该基金会。他说：“我们有一个扩展计划，这是发展 Kotlin 生态系统的核心：发展 Kotlin 基金会，而不仅仅是作为创始成员的两家公司。”他指出，这两家公司还没有理由将基金会纳入 Linux 基金会这样的组织的保护伞之下，部分原因是，因为如果只有两名成员的话，会显得有些过分，但是随着时间的推移，谷歌和 JetBrains 带来更多的成员，这种情况可能会改变。</p><p></p><h5>作者简介：</h5><p></p><p>Frederic Lardinois，2012 年在 TechCrunch 任编辑。之前曾创立 SiliconFilter，并为 ReadWriteWeb（现为 ReadWrite）撰稿。</p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://techcrunch.com/2022/08/18/five-years-later-google-is-still-all-in-on-kotlin/\">https://techcrunch.com/2022/08/18/five-years-later-google-is-still-all-in-on-kotlin/</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/942bb329c38cd72544c860f41\">浅谈 Kotlin 编程 01. 初识 Kotlin 和入门示例</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ba7cceea4b506026b9cb0d42f\">从 HelloWorld 看 Java 与 Kotlin</a>\"</p><p><a href=\"https://www.infoq.cn/article/aTY2ysTOjaEwUv9Hzls6\">Meta 将百万行代码从 Java 移植到 Kotlin</a>\"</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzAxODcyNjEzNQ==&amp;mid=2247571124&amp;idx=1&amp;sn=93bb6d6dc0678677eb89f03fbc256824&amp;chksm=9bd27b2caca5f23a436467fe7b5f6c3809a9bba3d7ccd4091bc2c70ac714814e7092709758a5&amp;scene=27#wechat_redirect\">又一巨头从 Java 迁移到 Kotlin ！</a>\"</p>",
    "publish_time": "2023-01-16 15:24:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "React 18：新玩具、新陷阱以及新可能性",
    "url": "https://www.infoq.cn/article/pi8g04Fpma4KmMD9hDwp",
    "summary": "<p></p><p><img src=\"https://static001.geekbang.org/infoq/c6/c6487a38e36bcf1578c5d69f136dfeeb.png\" /></p><p></p><p>&nbsp;图源<a href=\"https://unsplash.com/@lautaroandreani\">Lautaro Andreani</a>\"&nbsp;@ Unsplash&nbsp;</p><p></p><p>坦白地说，我最近也没怎么用过React，只用过Vanilla React（我在另一篇文章里总结过<a href=\"https://blog.bitsrc.io/next-js-13-what-do-the-new-bleeding-edge-features-actually-do-d3e5fd418563\">版本13的复杂性</a>\"），以及<a href=\"https://astro.build/\">Astro</a>\"&nbsp;+&nbsp;<a href=\"https://preactjs.com/\">Preact</a>\"的组合工具。别误会，React依旧很赞，但多数情况下，你大概会觉得React可行性在很大程度上会取决于你愿意投入多少时间学习它的怪癖，以及你愿意写多少代码来对抗黑客。</p><p>&nbsp;</p><p>但React 18（在我写这篇文章时是18.2.0）为弥补这一差距迈出了巨大一步，提供了许多开箱即用的新功能，如并发渲染、过渡（Transitions）和悬停（Suspense），以及一些锦上添花的变化。</p><p>&nbsp;</p><p>那么代价是什么呢？更多“神奇”的抽象。并不是所有人都吃这一套，但就结果而言，我们或许可以考虑在下一个项目中跳过“功能齐全”框架，并用React 18取而代之，让react-query成为我们数据获取或缓存的解决方案。</p><p>&nbsp;</p><p>那究竟是什么说服了我呢？容我慢慢道来。</p><p>&nbsp;</p><p></p><h2>并发渲染</h2><p></p><p>&nbsp;</p><p>突击问答：JavaScript是单线程的吗？</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/6247c1d2904347f62d9c489591f8ba69.png\" /></p><p></p><p>&nbsp;</p><p>JavaScript本身是单线程的，初始代码不会等DOM树完成立刻执行，但其他基于浏览器Web接口的，如Ajax请求、渲染、事件触发等却不是单线程。React的开发者或许已经对这种独立地从不同组件中获取数据并遭遇竞赛条件的情况驾轻就熟了。</p><p>&nbsp;</p><p>要想应对这种情况，我们需要求助并发。并发让React具备并行性，且有能力在响应性方面与本地设备UI相匹配。</p><p>&nbsp;</p><p>怎么做到这一点？要回答这个问题，让我们先看看React幕后的工作原理。</p><p>&nbsp;</p><p>React的核心设计是维护一个虚拟或影子DOM，渲染DOM树的副本，其中每一个独立的节点都代表一个React元素。在对UI做更新后，React都会递归更新两个树之间的差异，并将累计的变更传递到渲染通道。</p><p>&nbsp;</p><p>在React 16中引入了一套新算法来完成这段流程，也就是<a href=\"https://github.com/acdlite/react-fiber-architecture\">React Fiber</a>\"，取代了原先基于堆栈的算法。所有React元素或者说是组件都是一个Fiber，每个Fiber的子和兄弟都可以延迟渲染，React通过对这些Fiber的延迟渲染实现数量级更高、效果更好的UI响应。具体观感对比可见<a href=\"https://claudiopro.github.io/react-fiber-vs-stack-demo/fiber.html\">这里</a>\"。</p><p>&nbsp;</p><p>React 17以此为基础构建，而React 18则为这套流程带来了更多可控性。</p><p>&nbsp;</p><p>React 18所做的是在所有子树被评估之前暂停DOM树之后的差异化渲染传递。最终结果？每个渲染通道现在都可以中断。React可以有选择地在后台更新部分UI，暂停、恢复或者放弃正在进行的渲染过程，同时保证UI不会崩溃，不会掉帧，或帧数时间一致（如，60 FPS的UI应该需要16.67毫秒来渲染每一帧）。</p><p>&nbsp;</p><p></p><blockquote>💡 随着 React 18 加入 React Native，移动设备的游戏规则将彻底改变。</blockquote><p></p><p>&nbsp;</p><p>React 18功能背后的核心概念是并发渲染，其中包括悬念、流式HTML、过渡API，等等。每次这些新功能都是并发式的，用户不用具体了解其背后的机制原理。</p><p>&nbsp;</p><p></p><h2>悬停</h2><p></p><p>&nbsp;</p><p>悬停（Suspense）最早出现在React 16.6.0中，但也只能用于动态导入React.lazy，如：</p><p>&nbsp;</p><p><code lang=\"null\">const CustomButton = React.lazy(() =&gt; import(‘./components/CustomButton’));</code></p><p>&nbsp;</p><p>在React 18中，悬停有了新的扩展，应用也更加普遍。你是否有遇到过组件树还没有完成数据获取，什么都显示不出来的情况？在能够给出真正的数据之前，指定一个默认的、非阻塞的加载状态展示给用户。</p><p>&nbsp;</p><p><code lang=\"null\">}&gt;\n  \n</code></p><p>&nbsp;</p><p>这样能够提升用户体验的方式的原因有：</p><p>&nbsp;</p><p>1. 用户不用等待所有数据获取完毕后才能看到东西；</p><p>2. 用户会看到一个加载按钮，动态骨架，或者仅仅是一个</p><p>加载中</p>之类的即时反馈，告诉用户程序正在运行，应用程序并没有崩溃；<p></p><p>3. 用户不用等待所有交互元素或组件完成水合（hydration），就能开始交互。还没加载完？没问题，用户完全可以先点点看、，或者里的数据。</p><p>&nbsp;</p><p>与此同时，开发者体验也得到了改善。在构建应用程序或是在使用Next.js和Hydrogen类似的元框架时，开发者们可以参考React新定义的，规范的“加载状态”。另外，如果你已经知道要怎么在Vanilla JavaScript中写try-catch模块，那你应该如何使用悬停边界。</p><p>&nbsp;</p><p>悬停会捕捉“悬停状态”的组件，而不是错误。比如在数据、代码缺失之类的情况中，给出“嘿我还没准备好所有东西”的信息。抛出的错误会触发最近的catch模块，无论其中有多少组件，最邻近的都会捕获其下第一个暂停组件，并展示其回退UI。</p><p>&nbsp;</p><p>悬停的边界再加上React编程模型中的“加载状态”概念，让UI的设计更加精细化。不过，当你将其与过渡API相结合，以指定组件渲染的“优先级”时，那么这一功能将会更加强大。</p><p>&nbsp;</p><p></p><h2>过渡API</h2><p></p><p>&nbsp;</p><p>我应该还没有提过我最喜欢的React自定义hook？</p><p>&nbsp;</p><p>在多个产品的发行中，这个简单的hook都为我带来了非常好的服务体验，我认为它对于我写的任何用户输入组件来说都是无价的。</p><p>&nbsp;</p><p><code lang=\"null\">   /* 只有在用户停止打字的几毫秒延迟后，才会设置变量 */\nfunction useDebounce(value, delay) {\n  const [debouncedValue, setDebouncedValue] = useState(value);\n  useEffect(() =&gt; {\n      /* 1. 延迟数毫秒后的新防抖值 */\n      const handler = setTimeout(() =&gt; {\n        setDebouncedValue(value);\n      }, delay);\n      /* 2. 如果变量值在延迟的毫秒内有变动，则防抖值保持不变 */\n      return () =&gt; {\n        clearTimeout(handler);\n      };\n    },[value, delay]); \n\n  return debouncedValue;\n}</code></p><p>&nbsp;</p><p>功能背后的想法很简单，在用户搜索栏中输入或下拉列表选择过滤器时，你不会想在每次按键输入时都对下拉列表更新（甚至是调用API搜索）。这个hook可以节流调用或者说“防抖”，确保服务器不会崩溃。</p><p>&nbsp;</p><p>但缺点也很明显，那就是感知滞后。本质上这个功能是引入任意延迟，以UI响应性为代价，确保应用程序的内部结构不被破坏。</p><p>&nbsp;</p><p>在React 18中，并发性支持一种更直观的方法：接收新状态后可以自如地打断计算及其渲染，以提高响应性和稳定性。</p><p>&nbsp;</p><p>新的过渡API支持进一步微调，将状态更新划分为像是前文中SearchField例子中的打字、点击、高亮和更新查询文本的紧急状态（Urgent），以及例子中更新实际展示列表的，可以暂缓直到数据准备好的过渡（Transition）更新。过渡是可以随时中断，且不会阻碍用户输入的，让应用程序保持更高的响应速度。</p><p>&nbsp;</p><p><code lang=\"null\">import { startTransition } from 'react';\n\n// UI updates are Urgent\nsetSearchFieldValue(input);\n\n// State updates are Transitions\nstartTransition(() =&gt; {\n  setSearchQuery(input);\n});</code></p><p>&nbsp;</p><p>你可能也猜到了，这段代码在悬停边界上效果更好，也避免了明显的UI问题：如果你在过渡期间悬停，React实际只是在展示旧状态和旧数据，而不是用回退内容替代已经在界面上展示的内容。新的渲染将被延迟直到有数据加载完毕。</p><p>&nbsp;</p><p>悬停、过渡以及流式SSR，并发React到底对用户体验和开发者体验有多少改善呢？</p><p>&nbsp;</p><p></p><h2>服务器组件</h2><p></p><p>这是React 18中的又一个重要的新功能，能够让网页构建工作变得更简单，更容易。唯一的问题就是……它仍然不够稳定，只能通过Next.js 13等元框架使用。</p><p>&nbsp;</p><p>React服务器组件（RSC）实际只是在服务器上渲染的组件，而不是客户端。</p><p>&nbsp;</p><p>那又有什么影响呢？很多，这里给出一个太长不看版：</p><p>&nbsp;</p><p>在使用RSC时，完全不会向客户端发送任何JavaScript。光是考虑这点就很强了，你再也不用担心发送庞大的客户端库（比如GraphQL客户端就是个常见的例子），影响产品的程序包大小及首字节时间（Time-to-First-Byte）。你可以直接在其中运行数据获取操作，如数据库查询、API、微服务交互等，随后直接通过&nbsp;props 将结果数据返回给客户端组件（如传统React组件）。这些查询的速度会是倍数级增长，因为通常来说服务器都会比客户端快上非常多，客户端与服务器之间的通信一般也只用于UI，而不是数据。RSC和悬停相辅相成。我们可以在服务器上获取数据，并将渲染好的UI单元流式递增地传递到客户侧。同时，RSC也不会在重新加载或获取时丢失客户端的状态，确保用户体验和开发者体验的一致性。你不能像是用useState/useEffect一样用hook，就像不能像onClick()一样用事件监听器，访问画布或剪贴板的浏览器API，或者像CSS-in-JS的引擎一样用emotion或styled-components。你可以在服务器和客户端之间共享代码，从而更容易确保类型安全。</p><p>&nbsp;</p><p>现在，网页开发变得更加容易，可以混搭服务器和客户端组件，根据是否需要在较小的软件包上运行，或需要更丰富的用户互动性，有选择地在二者之间跳转。帮你构建灵活且多功能混合的应用程序，适应不断变化的技术或业务需求。</p><p>&nbsp;</p><p></p><h2>自动批处理：看不见的性能优化</h2><p></p><p>&nbsp;</p><p>React在幕后的渲染流程就是：一次状态更新=一次新的渲染。你可能不知道的是，React如何通过将多个状态更新集中到一个渲染通道，以达到优化效果的。当然，既然状态更新=重新渲染，你会想尽量减少这种情况的。</p><p>&nbsp;</p><p>在React 17以及更低的版本中，这种情况只会出现在事件监听器中。任何在React管理之外的事件处理程序都不会被批处理，当然也包括Promise.then()里的、await之后的，以及setTimeout之内的东西。因此，你大概会遇到多次意料之外的重新渲染，这是因为其背后的批处理是基于调用堆栈的，而Promise（或回调）= 首次浏览器事件之外的多个新调用堆栈 = 多次批处理 = 多个渲染过程。</p><p>&nbsp;</p><p>那有什么变化呢？好吧，React现在变聪明了，会将所有状态更新排序成一个事件循环，以确保尽量减少重新渲染。但这点你并不用去考虑或选择，因为这些在React 18中是自动发生的。</p><p>&nbsp;</p><p><code lang=\"null\">function App() {\n  const [data, setData] = useState([]);\n  const [isLoading, setIsLoading] = useState(true);\n\n  function handleClick() {\n    fetch('/api').then((response) =&gt; {\n      setData(response.json()); // In React 17 this causes re-render #1\n      setIsLoading(false); // In React 17 this causes re-render #2\n      // In React 18 the first and only re-render is triggered here, AFTER the 2 state updates\n    });\n  }\n\n  return (\n    </code></p><div><code lang=\"null\">\n      <button> Get Data </button>\n      <div> {JSON.stringify(data)} </div>\n    </code></div><code lang=\"null\">\n  );\n}</code><p></p><p>&nbsp;</p><p></p><h3>对Async/Await的原生支持：usehook介绍</h3><p></p><p>&nbsp;</p><p>好消息！好消息！React终于接受了大部分数据操作都是异步的现实，并在React 18中新增了对其的原生支持。那对开发者体验来说意味着什么呢？可以分为两部分：</p><p>服务器组件不能也不需要使用hook，因为它们是无状态的，async/await可以使用任何Promise。客户端组件却不是异步的，并且不能用await来解包Promise值。React 18为此提供了一个全新的usehook。</p><p>&nbsp;</p><p>这个usehook（顺带一提，我不是很喜欢这个名字）是唯一可以被条件调用的React hook，而且是可以在任何地方调用的，即使是在循环之中。以后，<a href=\"https://github.com/acdlite/rfcs/blob/first-class-promises/text/0000-first-class-support-for-promises.md#other-usable-types\">React也将包含对Context等其他值的解包支持</a>\"。</p><p>&nbsp;</p><p>那要怎么用use呢？</p><p>&nbsp;</p><p><code lang=\"null\">import { experimental_use as use, Suspense } from 'react';\n\nconst getData = fetch(\"/api/posts\").then((res) =&gt; res.json());\nconst Posts = () =&gt; {\n  const data = use(getData);\n  return </code></p><div><code lang=\"null\"> { JSON.stringify(data) } </code></div><code lang=\"null\">\n};\n\nfunction App() {\n  return (\n    <div>\n       }&gt;\n        \n      \n    </div>\n  );\n}</code><p></p><p>&nbsp;</p><p>是的，非常简单，但也非常容易翻车。举例来说，你可能会遇到这种情况：</p><p>&nbsp;</p><p><code lang=\"null\">import { experimental_use as use, Suspense } from 'react';\n\n// 哈，你刚刚触发了一个无限加载 \nconst PostsWhoops = () =&gt; {\n  // 因为这个最后总是会回到一个新的引用\n  const data = use(fetch(\"/api/posts\").then(res) =&gt; res.json())); \n  return </code></p><div><code lang=\"null\"> { JSON.stringify(data) } </code></div><code lang=\"null\">\n};\n\n// 正确方法\nconst getData = fetch(\"/api/posts\").then((res) =&gt; res.json());\nconst Posts = () =&gt; {\n  const data = use(getData);\n  return <div> { JSON.stringify(data) } </div>\n};\n\n// ...\n}</code><p></p><p>&nbsp;</p><p>为什么会这样？</p><p>&nbsp;</p><p>假设一种情况，hook解包了一个出于各种原因（网络速度或数据错误）还没完成加载的Promise。那么，这种在悬停边界的使用将被悬停，但由于组件的工作方式和vanilla JS中的异步或等待不同，它不会在故障点恢复执行，而是会在问题解决后重新渲染组件，并在下一次渲染中解包Promise的真实值，也就是非未定义值。</p><p>&nbsp;</p><p>然而，这也就意味着每次对Promise的引用都是全新的引用，这一过程会重复执行，也就是为什么会触发例子中的无限渲染循环。</p><p>&nbsp;</p><p>为避免这种情况，我们应该把use和<a href=\"https://github.com/reactwg/react-18/discussions/25\">即将发布的Cache API</a>\"一起使用，用于自动记忆打包好的函数结果。Next.js 13中实现了自动缓存和清理缓存，甚至可以按路由字段而不是像上面例子中一样按请求实现，以作为新的API扩展&nbsp;fetch。</p><p>&nbsp;</p><p>这就是真相了。React目前对服务器和客户端的异步代码都有完全的原生支持，确保对其余JavaScript的完全兼容。</p><p>&nbsp;</p><p></p><h2>如何更新？</h2><p></p><p>&nbsp;</p><p>你可能已经用上React 18了！无论是CRA、Vite还是Next.js通过npx的启动模板，都已经在使用React 18.2.0了。</p><p>&nbsp;</p><p>但如果你想把React 17及以下的版本升级，那还需要注意以下几点。</p><p>&nbsp;</p><p></p><h3>1. 替换为createRoot</h3><p></p><p>&nbsp;</p><p>根管理换成了一个新的API，且不再支持ReactDOM.render，取而代之的是createRoot。随着createRoot而来的还有新的并发渲染器，以启动所有新奇的新功能。替换之前的应用不会中断，但会和React 17一样运行，无法获得React 18的任何优势。</p><p>&nbsp;</p><p><code lang=\"null\">// React 17\nimport { render } from 'react-dom';\nconst container = document.getElementById('app');\nrender(, container);\n\n// React 18\nimport { createRoot } from 'react-dom/client';\nconst container = document.getElementById('app');\nconst root = createRoot(container); // createRoot(container!) if you use TypeScript\nroot.render();</code></p><p>&nbsp;</p><p></p><h3>2. 替换为hydrateRoot</h3><p></p><p>&nbsp;</p><p>同样，对于SSR来说，ReactDOM.hydrate也没有了，取而代之的是hydrateRoot。如果你不想换，那React 18 会和React 17的行为一样：</p><p>&nbsp;</p><p><code lang=\"null\">// React 17\nimport { hydrate } from 'react-dom';\nconst container = document.getElementById('app');\nhydrate(, container);\n\n// React 18\nimport { hydrateRoot } from 'react-dom/client';\nconst container = document.getElementById('app');\nconst root = hydrateRoot(container, );</code></p><p>&nbsp;</p><p></p><h3>3. 没有渲染回调了</h3><p></p><p>如果你的应用程序在用回调（callback）作为渲染函数的第三个参数，并且还想保留的话，就必须用useEffect替代，旧方法会破坏悬停。</p><p>&nbsp;</p><p><code lang=\"null\">// React 17\nconst container = document.getElementById('app');\nrender(, container, () =&gt; {\n  console.log('rendered');\n});\n\n// React 18\nfunction AppWithCallbackAfterRender() {\n  useEffect(() =&gt; {\n    console.log('rendered');\n  });\n\n  return \n}\n\nconst container = document.getElementById('app');\nconst root = createRoot(container);\nroot.render();</code></p><p>&nbsp;</p><p></p><h3>4. 严格模式</h3><p></p><p>React 18中的一大性能提升就在于并发，但它也要求组件能与可复用的状态兼容。为了实现并发，我们需要能够中断正在进行的渲染，同时复用旧的状态以保持UI一致性。</p><p>&nbsp;</p><p>为了消除反模式，React 18的严格模式将通过两次调用功能组件、初始化器以及更新器，模拟效果被多次加载和销毁，具体过程如下：</p><p>第一步：安装组件（Layout影响代码运行，Effect影响代码运行）第二步：React模拟组件隐藏及卸除效果（Layout影响清理代码运行+Effect影响清理代码运行）第三步：React模拟组件以旧的状态重新安装（返回第一步）</p><p>&nbsp;</p><p>为了展示React在保持<a href=\"https://en.wikipedia.org/wiki/Pure_function\">纯组件</a>\"理念中与并发相关的代码错误，可以参考这个例子：</p><p>&nbsp;</p><p><code lang=\"null\">setTodos(prevTodos =&gt; {\n  prevTodos.push(createTodo());\n});</code></p><p>&nbsp;</p><p>例子中的函数直接修改了数据状态，因此是一个不纯的函数。在严格模式中，React会调用两次Updater函数，也就是说同一个Todo会被添加两次，可以非常明显地看到错误问题。</p><p>&nbsp;</p><p>正确的解决方法是：替换数据，不要直接改变状态。</p><p>&nbsp;</p><p><code lang=\"null\">setTodos(prevTodos =&gt; {\n  return […prevTodos, createTodo()];\n});</code></p><p>&nbsp;</p><p>如果你的组件、初始化器和更新器都是幂等的，那这种仅存在于开发模式，不上生产的双重渲染不会破坏代码。事件处理程序因为不是纯函数，所以不受新严格模式的影响。</p><p>&nbsp;</p><p></p><h3>5. 关于TypeScript</h3><p></p><p>如果你在用TypeScript（强烈推荐），那还需要更新类型定义（@types/react以及@types/react-dom）到最新版本。除此之外，新版本还要求明确列出children项：</p><p><code lang=\"null\">interface MyButtonProps {\n  color: string;\n  children?: React.ReactNode;\n}</code></p><p>&nbsp;</p><p></p><h3>6. 不再支持IE浏览器</h3><p></p><p>虽然目前代码还在，估计直到React 19都不会删，但如果你必须要支持IE的话，建议保持React 17版本不要升级。</p><p>&nbsp;</p><p></p><h2>未来的日子</h2><p></p><p>&nbsp;</p><p>React 18是向着正确的前进方向迈出的一大步，是预示着更美好的webdev生态系统。但如果你对React的奇思妙想和抽象不太满意，那你大概是不会喜欢这个包含诸多超赞的新功能，但同时也有更多神奇抽象的版本。</p><p>&nbsp;</p><p>React 18.2.01的开发者，目前的工作流程应该大致是这样的：</p><p>默认情况下，数据操作、鉴权、以及任何后端代码等组件渲染都是在服务器上进行的。在需要互动性时，选择性地添加客户端组件（useState/useEffect/DOM API），流式传输结果。</p><p>&nbsp;</p><p>更快的页面加载速度，更小的JavaScript程序包，更短的可交互时间（TTI），全是为了更好的用户体验和开发体验。React的下一步是什么？以目前来看，我觉得会是自动记忆的编译器，激动人心的时刻即将到来！</p><p>&nbsp;</p><p></p><h5>原文链接：</h5><p></p><p><a href=\"https://blog.bitsrc.io/whats-new-in-react-js-v18-new-toys-new-footguns-new-possibilities-baa0bb6ee863\">https://blog.bitsrc.io/whats-new-in-react-js-v18-new-toys-new-footguns-new-possibilities-baa0bb6ee863</a>\"</p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"https://xie.infoq.cn/article/4c7b8a8ff13679254e4ec33d7\">React 源码分析 1-jsx 转换及 React.createElement</a>\"</p><p></p><p><a href=\"https://xie.infoq.cn/article/af5f8cd40e98e968650f8c190\">手写一个 react，看透 react 运行机制</a>\"</p><p></p><p><a href=\"https://xie.infoq.cn/article/5b9a8e6677d6a88d68383966a\">看透 react 源码之感受 react 的进化</a>\"</p><p></p><p><a href=\"https://xie.infoq.cn/article/72b93a8bda67ad0753f164983\">深度分析 React 源码中的合成事件</a>\"</p>",
    "publish_time": "2023-01-16 15:27:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]