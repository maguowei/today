[
  {
    "title": "苹果公司开源机器学习框架MLX，针对Silicon芯片进行了优化",
    "url": "https://www.infoq.cn/article/2ARZuWc3L5UFT4aeu622",
    "summary": "<p>苹果公司的机器学习框架<a href=\"https://github.com/ml-explore/mlx?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ\">MLX</a>\"结合了开发者熟悉的API、可组合的函数转换和惰性计算，部分灵感源于NumPy和PyTorch，并针对苹果的Silicon进行了优化。该框架使用Python和C++实现，旨在为在苹果Silicon上训练和部署机器学习模型提供用户友好且高效的解决方案。</p><p></p><p>根据苹果公司的说法，MLX是由机器学习研究人员为机器学习研究人员设计的，并基于MIT发布许可，可以很容易地被扩展和改进。它支持转换语言模型训练、使用Mistral进行大规模文本生成、使用Stable Diffusion进行图像生成以及使用Whisper进行语音识别。</p><p></p><p>MLX提供了受NumPy启发的底层Python API和一个完整的与之密切对应的C++ API。此外，它还提供了一个高级API，可用于根据PyTorch API创建更复杂的模型。</p><p></p><p>该框架支持自动微分、自动向量化和计算图优化，可组合的函数使得构建复杂数组转换变得更加容易。MLX还支持惰性计算，这意味着它可以只在必要时才计算数组，以提高计算效率。同样，计算图是动态构建的，因此修改函数参数并不会触发缓慢的编译过程。</p><p></p><p>MLX的一个独有的特性是使用了苹果Silicon的<a href=\"https://ml-explore.github.io/mlx/build/html/unified_memory.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ\">统一内存</a>\"，这让它有别于其他的ML框架。这意味着数组位于共享内存中，可以在CPU或GPU上执行数组操作，无需在内存之间传输数据。例如，在创建一个数组时，你不需要指定位置，因为它位于统一内存中，而在执行操作时可以选择在CPU或GPU上执行转换：</p><p><code lang=\"text\">a = mx.random.normal((100,))\n\nb = mx.random.normal((100,))\n\nmx.add(a, b, stream=mx.cpu)\n\nmx.add(a, b, stream=mx.gpu)</code></p><p></p><p>MLX可在任意的苹果Silicon CPU上运行，包括M1，并可以利用集成的GPU，因此研究人员可以选择最适合其需求的硬件。</p><p></p><p>MLX的代码库中包含了一些针对不同模型的示例，包括BERT、Llama、Mistral、Stable Diffusion等。每个示例都在requirements.txt文件中列出所需的依赖项，并提供了现成的CLI工具。例如，要使用Stable Diffusion生成图像，首先安装所有必需的依赖项，然后运行txt2image.py命令：</p><p>pip install -r requirements.txt</p><p>python txt2image.py \"A photo of an astronaut riding a horse on Mars.\" --n_images 4 --n_rows 2</p><p></p><p>苹果尚未公开发布基准测试，因此我们目前不知道它与<a href=\"https://developer.apple.com/metal/pytorch/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ\">PyTorch/MPS</a>\"或Georgi Gerganov的<a href=\"https://github.com/ggerganov/llama.cpp?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ\">Llama.cpp</a>\"相比表现如何。</p><p></p><p>不过，Stable Diffusion示例中包含了使用PyTorch和MLX运行UNet的性能比较。MLX在批次大小为16时的吞吐量比PyTorch高约40%，最佳批次大小大15%左右。</p><p></p><p>然而，PyTorch在较小的批次大小时表现更好，批次大小为1时吞吐量高约50%，批次大小为4时高约10%。根据苹果公司的说法，PyTorch在这些情况下的优势要归因于在模型还没有被加载到内存中且PyTorch的MPS图内核未被缓存时的编译速度。</p><p></p><p>如果你有兴趣体验MLX，请参阅其<a href=\"https://ml-explore.github.io/mlx/build/html/quick_start.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ\">快速入门指南</a>\"或<a href=\"https://ml-explore.github.io/mlx/build/html/install.html#?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ\">完整文档</a>\"。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/12/apple-silicon-machine-learning/\">https://www.infoq.com/news/2023/12/apple-silicon-machine-learning/</a>\"</p>",
    "publish_time": "2024-01-10 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]