[
  {
    "title": "Android Studio Giraffe 稳定版亮相，带来IntelliJ 2022.3及全新IDE外观",
    "url": "https://www.infoq.cn/article/hkqHm2GKjCFUggQ7urfA",
    "summary": "<p>&nbsp;<a href=\"https://developer.android.com/studio\">Android Studio Giraffe</a>\"现已<a href=\"https://developer.android.com/studio\">稳定</a>\"，其引入了新的IntelliJ 2022.3、新的IDE外观和感受、改进的Live Edit、Compose动画预览，等等。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.com/news/2013/05/Android-Studio/\">自2013年首次发布之来</a>\"的十年里，Android Studio仍然是安卓开发的IDE。其最新版本在不同领域引入了许多更改，包括IDE增强、编码效率提升和构建系统改进等。</p><p>&nbsp;</p><p>Android Studio Giraffe采用了一个新的可选择IDE外观和风格，旨在降低视觉复杂性。它致力于简化对最常用功能的访问，同时使较复杂的功能在需要时更易于访问，但在正常使用中不那么突出。此外，它还提供了一个新的主题，使IDE的视觉效果看起来更加现代：</p><p>&nbsp;</p><p></p><blockquote>随着Giraffe的发布，我们已经开始采用新的UI，并对Android Studio进行了一些特定的更改，例如优化Android的默认主工具栏和工具窗口配置，以及刷新我们的风格图标。</blockquote><p></p><p>&nbsp;</p><p>新的IDE还包括一个更新的设备资源管理器，它可以检查任何连接设备的文件和进程，包括复制或删除文件、终止进程或将调试器附加到正在运行的进程上的可能性。</p><p>&nbsp;</p><p>在代码效率方面，Android Studio Giraffe提供了在可组合项中预览UI更改的可能性，而无需将应用程序重新部署到模拟器或物理设备上。该功能可以通过设置/编辑器/实时编辑（Settings/Editor/Live Edit）启用，并且需要Android Gradle Plugin（AGP）8.1或更高版本以及Jetpack Compose Runtime 1.3.0或更高版本的支持。</p><p>&nbsp;</p><p>与预览功能相关的是，Compose动画预览已经支持了许多其他Compose API，包括<a href=\"https://developer.android.com/jetpack/compose/animation/value-based#animate-as-state\">animate*AsState</a>\"、<a href=\"https://developer.android.com/jetpack/compose/animation/composables-modifiers#crossfade\">CrossFade</a>\"、<a href=\"https://developer.android.com/jetpack/compose/animation/value-based#rememberinfinitetransition\">rememberInfiniteTransition</a>\"和<a href=\"https://developer.android.com/jetpack/compose/animation/composables-modifiers#animatedcontent\">AnimatedContent</a>\"。动画可以播放、暂停、滑动等等。</p><p>&nbsp;</p><p>提高代码效率的最后一个帮助来自新的Android SDK升级助手。</p><p>&nbsp;</p><p></p><blockquote>新的Android SDK升级助手可以让你直接在IDE中查看升级targetSdkVersion或应用程序所针对的API级别所需的步骤。</blockquote><p></p><p>&nbsp;</p><p>该助手将显示与你选择的升级选项相关的所有信息，因此你无需要再单独浏览这些信息，并且能够突出显示每个迁移步骤的主要突破性更改。</p><p>&nbsp;</p><p>说到构建系统，你现在可以在<a href=\"https://android-developers.googleblog.com/2023/04/kotlin-dsl-is-now-default-for-new-gradle-builds.html\">Gradle构建脚本中使用Kotlin DSL</a>\"，利用它的编译时检查，可将所有项目代码整合到一种语言下。</p><p>&nbsp;</p><p></p><blockquote>此外，我们还添加了对基于TOML的Gradle Version Catalogs的实验性支持，该功能允许你在一个中心位置管理依赖项，并在模块或项目之间共享这些依赖项。</blockquote><p></p><p>&nbsp;</p><p>最后需要说明的是，Android Studio Giraffe可以在Gradle同步时显示依赖项下载信息。这将能帮助你检测存储库配置中的低效率问题。</p><p>&nbsp;</p><p>Android Studio Giraffe的内容远不止这些。如果你对完整的细节感兴趣，请不要错过官方公告。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/android-studio-giraffe-stable/\">https://www.infoq.com/news/2023/07/android-studio-giraffe-stable/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/twFsuxo4IQi5FIqe3OKA\">移动端性能挖掘：字节跳动iOS与安卓性能归因实践</a>\"</p><p><a href=\"https://www.infoq.cn/article/MuATtLq6R00gEssCOwzj\">Lyft如何检测生产中安卓的内存泄漏</a>\"</p>",
    "publish_time": "2023-09-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何利用播放器节省 20% 点播成本？",
    "url": "https://www.infoq.cn/article/Rx45QcxHI4zZCfMR5r8J",
    "summary": "<p>点播成本节省的点其实涉及诸多内容，例如 CDN、<a href=\"https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT\">转码</a>\"、存储等，而利用播放器降本却是很多客户比较陌生的部分。火山引擎基于内部支撑抖音集团相关业务的实践，播放器恰恰是成本优化中最重要和最为依赖的部分。</p><p></p><p><a href=\"https://www.infoq.cn/article/r3J9F7CtFJyljwQWwl79\">火山引擎</a>\"的视频团队做了份数据统计，在一个很经典的视频业务中，我们在 2022 年至 2023 年大约 1 年半的时间里，针对这个业务进行了 33 次成本优化点，其中 13 次是播放器主导的优化，其余的有 12 次也是需要播放器强配合的优化，也就是说在这个业务里，75% 的成本优化是直接或间接由播放器参与，可见客户端对成本优化的关键作用。</p><p></p><p>最终我们在很多实践中也发现，通过播放器的优化可以为点播业务节省 20% 甚至更多的成本，本篇内容便将聚焦在播放器层面如何节省成本这一主题上。</p><p></p><h2>一、点播成本构成</h2><p></p><p></p><p>在视频点播的成本构成中，有很明显的二八原则：</p><p><img src=\"https://static001.geekbang.org/infoq/b7/b7a1d2eb46169bedf7939dacf12f6cbf.png\" /></p><p>CDN 带宽成本占绝对的大头，80% 都是带宽成本；其次是存储和转码成本，二者占不到 20%；额外还有一些其他的周边的成本，比如日志处理的数据成本、AI 处理的成本。</p><p></p><p>我们可以将成本的优化理解成“置换”，在点播的成本优化中，就存在 2 种“置换关系”：</p><p></p><p>第 1 种置换关系是“成本项之间的置换”，指的是「带宽-转码-存储」之间的置换。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a0b4cad5199d28bdfb5ae239aa7174c.png\" /></p><p></p><p>上图是 H.264 升级到 H.265 编码格式的例子，265 的压缩率相对比 264 要优 20%-40%，所以带宽、存储上 265 是大幅度减少；但是 265 的计算复杂度要复杂很多，所以转码成本大幅度升高。而这个图不是一个等边三角形，带宽成本要远大于转码和存储成本，所以这个置换是非常划算的。</p><p></p><p>第 2 种置换是“成本和体验的置换”，我们一般说是“跷跷板效应：</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/69b5518c52156b7633a99b6ecf8ab462.png\" /></p><p></p><p>例如：</p><p>我们增大缓存时长，对应体验上「卡顿率」就会降低，但是成本会增加；抖音小视频 feed 流场景，我们做预加载，这时候首屏感会更顺滑，但对应的成本是增加的；降低码率，那么体验上感到清晰度变差了，而成本就是减少的；跷跷板中间支点是技术，我们通常是希望固定体验、降低成本，依靠技术来支撑。</p><p></p><p>所以我们总在说降成本，那降的到底是什么呢？我们这里用一个很简单的乘法公式来表示：</p><p><img src=\"https://static001.geekbang.org/infoq/43/43bd4980ca1f18f0f87f459cf0ecdac8.png\" /></p><p></p><p>在过去，“单价”是非常明显的因素，大家往往选择在采购环节尽量的压低单价； 而“用量”上通常会被认为是无法改变的业务因素。但“用量”实际上是包含 2 类，一类是正常用量，确实是比较难改变的业务因素，但另一类是“浪费”，是可以被优化的。所以如何识别出浪费、降低浪费，是播放器降本的关键点。</p><p></p><p>那么造成浪费的因素有哪些呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef6af0158d68eca672870783bb48158b.png\" /></p><p></p><p>例如在视频播放过程中，会包括“已播放的数据”，和“未播放但已经缓存的数据”，如果用户中途离开播放，那其中“已缓存的数据”都是浪费了。所以我们定义“浪费”是“已经缓存了、但不需要的字节数”。</p><p></p><p>从理想上来说，没有浪费是最好的；但往往业务中，浪费是非常大的，大于30%是很常见的。常见的可能带来的浪费包括了：</p><p>未播放离开向后拖拽切换档位清晰度溢出（举例：很小的手机屏幕播放4K的内容，肉眼感知不到清晰度的区别）</p><p></p><p></p><h2>二、播放器的成本优化方法</h2><p></p><p></p><p>针对上述的浪费我们进行了如下的具体优化方法：</p><p></p><h4>（一）缓存的浪费</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d28d3667236f7e52cebf1c043b11535.png\" /></p><p></p><p>承接上图的播放器缓存示意图，如果用户播放过程中离开了，那么深灰色是浪费部分。很容易就想到我们减少深灰色的部分的大小，比如把播放水位降低 1/3（也就是图中浅黄色的部分减少掉），不去缓存，那么浪费就明显的减少了。</p><p></p><p>这个就是静态水位的思路，通过减少缓存水位来减少浪费。</p><p></p><p>但是，静态水位是很难抉择的，水位大了浪费多，但是水位太小了，卡顿就会明显的增加。</p><p></p><p>这里有个马太效应，从原理上，缓存的本质是为了对抗网络的抖动的。网络稳定好时，只需要很少的缓存就足够了，但是网络好时缓存会填充的很快，大部分时间都是饱和的。反之，波动大的网络，需要更多的水位，但总的上限也有限，无法提供有效的缓存。</p><p></p><p>为此我们实现了的动态水位算法，我们根据一些因素来动态的决策缓存水位的大小：</p><p>探测用户的网络速度和稳定性，对稳定性高、速度快的，我们减少缓存；对网络速度差、稳定性差的网络，就增大缓存，这样在网络抖动时就能够有更大的缓存空间使用；根据用户的播放行为，通过数据分析道，视频观看的前期，用户离开的比例会更高，观看的后期，离开的比例就会降低，所以前期的缓存水位小一些，后期的缓存水位大一些；一些其他的因素，但目的是在每次播放时决策出一个尽量合理的缓存水位，来平衡卡顿和浪费。</p><p></p><p>决定了缓存水位大小之后，还有个细节点就是 Range 请求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a92548397903c21bd2ec05e397ec127d.png\" /></p><p></p><p>Range 是 http 协议的一个请求头，默认是“0-请求”，表示请求完整文件。</p><p></p><p>左侧的图示意，如果是单独发一个“0-请求”，那么 CDN 服务端就会持续的返回整个文件，如果在中途断开，从服务端视角来说，这些数据已经发送过去了，无论客户端是否需要，都已经计费了，就构成了浪费。</p><p></p><p>在上图，我们分成 3 段来发 Range 请求，中途断开时，是可以停止掉最后一段，那么浪费就大幅度减少了。同样，静态的 Range 是很难抉择的，Range拆分的太细会引起卡顿的提升；Range 过大了成本节省的效果又不够了。</p><p></p><p>这里我们引入目标水位的概念，就是刚刚讲的动态水位算法所决策出来的水位大小。</p><p></p><p>播放器 Range 请求的应遵循两个原则：</p><p>1. 将当前视频尽快缓存到目标水位。</p><p>2. 控制 Range 拆分的大小，避免太小的 Range 拆分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/177d1c20a6f1f2475729dd93415e31b5.png\" /></p><p></p><p>上图是动态水位算法+动态 Range 拆分的效果示意图：</p><p></p><p>横轴代表时间线。纵轴上图是视频下载的大小，蓝色块代表一个 Range 请求；下图是缓存的大小，橙色的折线表示缓存随着视频文件下载和播放时间的波动情况，横着的虚线是目标水位。</p><p></p><p>我们从左到右，分析下目标水位和 Range 的关系：</p><p>第 1 条竖红线，决策出来第一条目标水位1，是启播水位，启播时的 Range 会略大于后面的 2 个 Range；第 2 条竖红线，是判断出一次水位提升，有可能是检测到网络波动，会提高目标水位到水位 2，同时做一次略大的 Range 请求来达到目标水位；第 3 条竖红线，是再次提升目标水位，到水位 3，有可能是因为观看时长增加到阈值，判断离开概率较小，所以保持高水位；后续的播放，在目标水位3随着时间波动，Range大小也会稳定些。</p><p></p><p>从最终效果上看，在任意一个时间点离开，都能够保障相对合理的浪费。</p><p></p><p>我们在不同业务上实践了很多次动态水位+动态range的AB实验，在体验指标持平或更优的前提下，带宽降低8%。</p><p></p><h4>（二）预加载的浪费</h4><p></p><p></p><p>在类似于抖音这种 feed 流下滑的场景，会提前加载好下面的视频，能够使滑动更顺畅，我们 叫“零首帧”效果，里面作用最大的就是预加载。</p><p></p><p>一般的预加载是固定几个视频，每个视频固定的大小。为了得到更好的预加载效果，会尽量多、尽量大的做预加载，也就构成了浪费。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fe/fe95e01501613611343b42477de82da9.png\" /></p><p></p><p>我们做的“精准预加载策略”，在“时机、大小、个数”上做精细化的优化：</p><p>时机上：对预加载也进行切片，这样可以区分出来一部分是紧急的，其他是不紧急的。比如图里，标记P0的是要最优先下载的，然后可以做预加载，预加载标记 P1 的部分，然后是当前视频的缓存水位，之后可以选择是否要预加载 P3 的部分。大小上：每个视频也会结合视频的长度、头大小、码率等因素计算出来需要预加载的大小个数上：按照 feed list 中的优先级依次预加载后续 N 个视频（动态计算），也会结合用户本身的行为（比如快速滑动）来动态决策。</p><p></p><p>AB 实验证明，能够提升预加载利用率 5%，对应流量成本降低约 0.8%。</p><p></p><h4>（三）清晰度的浪费</h4><p></p><p></p><p>现在的主干场景是在移动端看视频，大家都会有启播选档的策略，就是在播放启动时，决定所需要的清晰度，一般是跟随网速、码率来决策的。</p><p><img src=\"\" /></p><p>经常大家面临的场景是，在竖屏里播放横屏视频时，实际上在很窄的一个空间里进行播放，这个时候，如果依然使用完整的清晰度，那么肉眼是看不出来的清晰的。而且，通常情况下小窗播放时用户的主要关注度也并不是画面清晰度，所以就产生了实际上的清晰度浪费。</p><p></p><p>我们对应的解决策略叫“窄屏低清”，就是识别出来显示区域很窄时，播放低清晰度的视频（比如360P），当需要横屏时，再快速的切换为正常的清晰度。这里如果是 mp4 格式播放，需要转码也做些配合，支持 mp4 的帧对齐和平滑切换。</p><p></p><p>在很多应用中都是很常见的，也有常见的小窗播放，多个业务的 AB 实验都能有 3% 以上的成本收益；</p><p>另外清晰度上还有个很棒的能力，是客户端超分。随着客户端超分能力的优化，现在很大一部分机型在客户端向上超分一个档位是完全没问题的，耗电可以忽略。</p><p></p><p>对应节省成本的策略是“降档超分”，就是分发的清晰度向下降一档，然后再通过客户端超分降主观清晰度补回来。在国内当前的机型条件下，大部分业务能够有 6～8% 左右的成本收益。</p><p></p><h4>（四）异常流量的浪费</h4><p></p><p></p><p>我们根据「播放器日志是否可以识别」、「是否是正常流量」把流量分成了 4 类。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/355b42ef6dc4ecbae6514c8c12ecd54c.png\" /></p><p>在非常多的业务中会发现第三种情况：流量有异常浪费，比如有部分视频码率过高，可能是没转码，或者转码模版用错了。我们开始时会认为“这些都是很明显的失误，业务层小心点不就行了么？”，但后来我们做成了单独的异常流量分析模块。我们跟业务尝试分析原因，发现业务总是复杂的：</p><p>业务场景很复杂，包括短视频、长视频、主页视频、广告视频等；研发的迭代通常会带来些历史问题；并不是所有的人员都需要持续的感知成本，只要有一个环节漏掉了，那么就可能会造成很大浪费。</p><p></p><p>这里还有个问题点，如果是体验问题或者 bug，总会有用户保障，来及时发现。但成本问题，用户基本是无法发现的，发现时就比较晚了。</p><p></p><p>我们是通过端到端的日志分析来发现和避免这些浪费的。原理很简单：</p><p>在客户端对日志染色；cdn 日志里记录的，区分是否是播放器产生的、是否是我们点播的域名；对两头的日志进行比对和分析。</p><p></p><p>不仅如此，这里还有个副产物，是通过这些日志分析，识别到业务真实是被盗链了，然后做盗链的治理。</p><p></p><h2>三、数据挖掘成本优化空间</h2><p></p><p></p><p>以上是<a href=\"https://www.infoq.cn/news/iSSNhwt4qU2WuSQ6U3AM\">火山引擎</a>\"是实际业务服务过程中探索出的优化方案，但优化是不是有上限的，优化到什么水平可以达到成本和体验的平衡，更多的能力是通过数据能力持续的挖掘出来的。</p><p></p><p>先从结果上来看，我们成本优化后通常会有2个报告：</p><p>1、AB 实验报告，里面会分析对 QoE 的体验影响多少，对成本优化的影响多少，比如图中的人均播放时长增加多少，成本降低多少；做成本的 AB 实验，依赖一个工具——客户端成本指标。</p><p>2、价值回溯文档，用于核算真实收益有多少，一般发生在完整上量之后，比如1个月或2个月后。关键结果叫“万分钟播放成本”，这个对应的依赖的工具是“成本评估公式”。</p><p></p><h4>（一）客户端成本指标</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f94e68e1e1c0828594268e411a16b7ca.png\" /></p><p></p><p>这张图从左往右是视频点播的数据流向。想要建设好成本埋点，有2个难点：</p><p></p><p>1、成本拟合。因为真实的计费数据是左侧 CDN 的计费日志，在右侧的客户端侧实际上是没有成本数据的，所以我们需要把数据缓存层的对成本的埋点尽量的拟合，使之尽量的对应到 CDN 的计费日志。这个过程是非常艰难的，我们通过了大量的离线校验。</p><p></p><p>2、提升可解释率。业务动作比较复杂（播放、预加载、拖拽、重播等等），举个例子，重复播放，播放层是记录2遍播放时长的，但是因为有缓存，真实的网络请求只有1遍。我们想要两份数据尽量对齐、可解释，就需要涵盖住尽量所有的业务场景。</p><p></p><p>我们当前达到了“可解释率达到 95%”，也就是说比如服务端 CDN 产生了 100Gbps 的带宽，客户端的日志能够拟合解释清楚 95%。</p><p></p><p>虽然还不到 100%，但日常来做成本优化、成本归因已经足够了。</p><p></p><p>下图是成本指标进入 AB 实验后的结果：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3f5aa2bf270087ee3e07d826556bc41.png\" /></p><p>图：核心指标</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/36c765358635fac3fb3cc769b770652d.png\" /></p><p>图：归因指标</p><p></p><p>成本数据进入AB实验有什么用呢？有2  点：</p><p></p><p>1、快速判断客户端的成本变化结果。大部分成本优化的能力都是伴随着策略的，不同策略有不同的结果置换关系，我们需要通过实验来确定效果。假设没有客户端的成本数据的话，我们就需要用不同的 CDN 域名来实验，这是很低效的，并且域名带宽的波动也会引起成本的波动。而在客户端成本指标进入了 AB 实验之后，大部分场景都直接看报表数字就可以了。</p><p></p><p>2、机制上可以防蜕化。业务的产品经理、分析师等角色也日常会关注到实验数据的，当成本数据也进入实验后，这些角色也可以关注到成本的变化，这样就能够防退化了。举个例子，版本升级时，只要经历了 AB 实验，就很难有成本退化的问题。</p><p></p><h4>（二）成本评估公式</h4><p></p><p></p><p>“成本评估公式” ，本质是一种单位成本的衡量方法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3faace5c96677c360f3369d5060bc21c.png\" /></p><p></p><p>我们叫“万分钟播放成本”，分子是点播的IT成本，分母是点播视频消费时长。</p><p></p><p>从技术侧来看，分子是“CDN、存储、转码等各种成本的加和”，分子是播放的时长。</p><p></p><p>这个公式很简单，但为什么要这么做呢？</p><p></p><p>因为涉及到成本优化，就会跟采购、财务团队打交道，采购、财务看到的都是每月的账单，业务用量每个月都在上下波动，导致账单每个月也都在波动。万分钟播放成本是单位成本，就可以刨除掉业务用量的影响因素，来衡量成本是否真的优化了。</p><p></p><p>拿其中的万分钟 CDN 成本来举例：</p><p><img src=\"https://static001.geekbang.org/infoq/61/614883436e1bc01a0737693d6285b3cb.png\" /></p><p></p><p>万分钟 CDN 成本的影响因子会涉及到价格、码率、浪费率、带宽流量比。</p><p></p><p>举一个真实的例子：有个客户反馈成本增加了，但是客户自己的业务用量在波动，不太好判断是什么情况。我们拆解分析万分钟 CDN 成本的具体影响因子，就发现了万分钟 CDN 成本确实是涨了 11%，主因是“码率”涨了 8%，“浪费率”增加了 5%。</p><p></p><h2>四、总结和展望</h2><p></p><p></p><h4>（一）建标准</h4><p></p><p></p><p>在服务业务的过程中，大家经常会面临一个问题，还能再降多少？极限是多少？</p><p></p><p>这些问题是很难回答的，因为每个业务的场景都不同，举例缓存浪费中，每个业务的客户中断离开的模型可能都不一样，那么建设统一的标准就很难了。</p><p></p><p>火山引擎目前通过 3 种方式来建设标准：</p><p>通过排名获取标杆：将类似场景的业务进行排名，对齐当前技术做的最好的，可以作为一种标准；离线的实验来模拟：我们做了成本的自动化测试平台，设计测试case，测试出来不同的参数的成本结果是多少，最后总结分析出来极限是多少；通过“理论公式”来推算“标准” ：举例通过“视频播放时长、中途离开比例”的关系，然后推算出理论的优化空间有多少。</p><p></p><h4>（二）做顾问</h4><p></p><p></p><p>面对的业务越来越多，降本的能力也越来越多时，就会遇到效率问题：功能这么多，应该用哪些？每个业务的场景也不一样，那么策略参数应该怎么配置呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/029a51f266a6efd2695de7e2beb1d610.png\" /></p><p>万分钟播放成本分析和策略推荐</p><p></p><p>解决方法是做顾问。如上图所示，其是我们的一个万分钟 CDN 成本与理想万分钟成本的一个差异分析表，我们给计算出了对应的差异，然后再给出可以补足差异的策略或功能推荐。</p><p></p><p>当然，这个表只是一个总结概览，更多的内容我们会整理成“顾问服务报告”，把各个点的差异、业务分析、解决方法与业务逐一的讨论分析。总之，万分钟播放成本是一个非常简单、容易落地、价值很大的工具，大家计算下万分钟播放成本，如有调优的诉求，非常欢迎来与火山引擎交流。火山引擎视频点播<a href=\"https://www.volcengine.com/product/vod\">https://www.volcengine.com/product/vod</a>\"。</p><p></p><p></p><p></p><h4>本文作者简介</h4><p></p><p>赵春波，火山引擎视频点播产品负责人。十余年视频相关研发和产品经验。目前主要负责火山引擎视频点播的产品工作，支撑抖音、西瓜等业务的点播基础技术、体验优化和成本优化等，并将这些技术能力沉淀到火山引擎，来服务更多的行业客户。</p>",
    "publish_time": "2023-09-20 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“芯经济”崛起，英特尔加速AI抢位战",
    "url": "https://www.infoq.cn/article/g43AaWaexclQ0QVlyelb",
    "summary": "<p>&nbsp;当地时间9月19日，英特尔On技术创新大会（Intel Innovation） 2023在美国加州圣何塞拉开帷幕，InfoQ有幸受邀参会并从现场发回报道。英特尔On技术创新大会是一场面向开发者的大会，更是一个了解过去一年英特尔在不同方向（包括端侧计算、数据中心、边缘计算和云计算）取得的最新进展的好方法。来自全球的上千位开发者、英特尔合作伙伴和客户亲临这场盛会，以期全面了解英特尔将以怎样的新策略、新产品应对生成式AI大爆发带来的机遇和挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90e9c724d5763d9bf01039c278203882.jpeg\" /></p><p>图：InfoQ记者拍摄于现场</p><p>&nbsp;</p><p>AI毫无疑问是贯穿大会首日主题演讲全程的重要关键词。围绕“让AI无处不在”这一主题，英特尔CEO帕特·基辛格在主题演讲中展示了英特尔如何在其各种硬件产品中加入AI能力，并通过开放、多架构的软件解决方案推动AI应用的普及。</p><p></p><h2>AI推动“芯经济”崛起</h2><p></p><p></p><p>基辛格在主题演讲开场表示：“AI代表着新时代的到来。AI正在催生全球增长的新时代，在新时代中，算力起着更为重要的作用，让所有人迎来更美好的未来。对开发者而言，这将带来巨大的社会和商业机遇，以创造更多可能，为世界上的重大挑战打造解决方案，并造福地球上每一个人。”</p><p>&nbsp;</p><p>基辛格提到，如今芯片形成了规模达5740亿美元的行业，并驱动着全球约8万亿美元的技术经济。世界对计算的需求呈指数级增长，而且这种需求与芯片的面积、成本和功耗成反比。简而言之，这就是摩尔定律。随后他提出了“芯经济”（Siliconomy）这一概念：“芯经济”是一个由可持续、开放、安全的算力需求所驱动的经济增长新时代。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d7c9452e6c69668e55c4ad38685fd8f0.png\" /></p><p></p><p>&nbsp;更充足、更强大、更具性价比的处理能力，是经济增长的关键组成。而人工智能代表着计算的新时代，促进了“芯经济”的崛起。基辛格表示，五大超级技术力量——计算、连接、基础设施、人工智能、传感和感知，由“芯经济”推动。随后他分享了一系列AI相关的软硬件产品方案新进展和技术路线图。</p><p></p><h2>英特尔开发者云全面上线</h2><p></p><p></p><p>基辛格宣布英特尔开发者云平台自今日起全面上线。英特尔开发者云平台能够帮助开发者利用最新的英特尔软硬件创新来进行AI开发（包括用于深度学习的英特尔Gaudi2加速器），并授权他们使用英特尔最新的硬件平台，如即将在未来几周内上线的第五代英特尔至强可扩展处理器（代号为 Emerald Rapids），以及将在12月14日上线的英特尔数据中心GPU Max系列1100和1550。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0bcc3821726c9b97a5348dbb7022851e.jpeg\" /></p><p></p><p>&nbsp;在使用英特尔开发者云平台时，开发者可以构建、测试并优化AI以及HPC应用程序，他们还可以运行从小规模到大规模的AI训练、模型优化和推理工作负载，以实现高性能和高效率。</p><p>&nbsp;</p><p>英特尔开发者云平台建立在支持多架构、多厂商硬件的oneAPI开放编程模型基础之上，为开发者提供硬件选择，并摆脱了专有编程模型，以支持加速计算、代码重用和满足可移植性需求。</p><p>&nbsp;</p><p>据InfoQ了解，已经有不少客户基于英特尔开发者云构建自己的AI应用，埃森哲是其中之一。</p><p>&nbsp;</p><p>此外，基辛格还在会上发布了英特尔发行版OpenVINO工具套件2023.1版，并表示Arm也将参与到OpenVINO工作中。OpenVINO是英特尔的AI推理和部署运行工具套件，在客户端和边缘平台上为开发人员提供了优质选择。该版本包括针对跨操作系统和各种不同云解决方案的集成而优化的预训练模型，包括多个生成式AI模型，例如Meta的Llama 2模型。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/ba/ba11e2415bddb323043cad33640fbc95.jpeg\" /></p><p></p><p>为了更好地将AI扩展到边缘侧，英特尔还将推出Strata项目以及边缘原生软件平台。其中边缘原生软件平台平台将于2024年推出，提供模块化构件、优质服务和产品支持。这是一种横向扩展智能边缘（intelligent edge）和混合人工智能（hybrid AI）所需基础设施的方式，并将英特尔和第三方的垂直应用程序整合在一个生态系统内。该解决方案将使开发人员能够构建、部署、运行、管理、连接和保护分布式边缘基础设施和应用程序。</p><p></p><h2>AI芯片路线图、下一代至强处理器亮相</h2><p></p><p>&nbsp;</p><p>自今年7月英特尔发布 Gaudi 2 训练加速器以来，其性能表现一直备受关注。最近的 MLPerf AI 推理<a href=\"https://www.intel.com/content/www/us/en/newsroom/news/intel-shows-strong-ai-inference-performance.html\">性能测试结果</a>\"进一步证明了 Gaudi 2性能在市场上的竞争力，其是目前市场上满足 AI 计算需求的唯一可行替代方案。基辛格披露了一台完全基于英特尔至强处理器和 4000 个英特尔 Gaudi2 AI 硬件加速器构建的大型 AI 超级计算机。这台AI超级计算机将跻身全球TOP15超算，而AI独角兽企业Stability AI 是其主要客户。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/42/42b3a7746acd443bdf203f4b69438fd6.jpeg\" /></p><p></p><p>作为第四代英特尔至强处理器的早期采用者，阿里巴巴也被邀请来为英特尔站台。阿里云首席技术官周靖人阐述了阿里巴巴如何将内置AI加速器的第四代英特尔至强可扩展处理器用于其生成式AI和大语言模型，即“阿里云通义千问大模型”。周靖人表示，英特尔技术“大幅缩短了模型响应时间，平均加速可达3倍”。</p><p>&nbsp;</p><p>面向AI计算，基辛格亮出了英特尔最新的三代AI芯片路线图，其中采用5nm制程的Gaudi 3将于2024年推出，再下一代AI芯片代号为Falcon Shores，计划于2025年推出。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/923adbae77bd67b27b20adf7b09001b8.png\" /></p><p></p><p>其中Gaudi 3的算力将达到Gaudi 2的两倍，网络带宽、HBM容量将达到Gaudi 2的1.5倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/9028195e64b90828118cac67c62bef38.jpeg\" /></p><p></p><p>基辛格还在演讲中预览了下一代英特尔至强处理器，并透露第五代英特尔至强处理器将于12月14日发布，届时，将在相同的功耗下为全球数据中心提高性能和存储速度。此外，具备高能效的能效核（E-core）处理器Sierra Forest将于2024年上半年上市。与第四代至强相比，拥有288核的该处理器预计将使机架密度提升2.5倍，每瓦性能提高2.4倍。紧随Sierra Forest发布的是具备高性能的性能核（P-core）处理器Granite Rapids，与第四代至强相比，其AI性能预计将提高2到3倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/874ac283e432995661e9d836ba2550b7.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af67cd6efa8cbc8c2abd6284ecd32cbf.jpeg\" /></p><p></p><p>展望2025年，代号为Clearwater Forest的下一代至强能效核处理器将基于Intel 18A制程节点制造。</p><p></p><h2>迈向AI PC新时代</h2><p></p><p>&nbsp;</p><p>为了让AI的使用更加普及化，英特尔也将目光放到了个人PC上。基辛格在演讲中表示：“AI将通过云与PC的紧密协作，进而从根本上改变、重塑和重构PC体验，释放人们的生产力和创造力。我们正迈向AI PC的新时代。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/21de39187af217d96624706d2fe0cd76.jpeg\" /></p><p></p><p>这一全新的PC体验，即将在接下来推出的产品代号为Meteor Lake的英特尔酷睿Ultra处理器上得到展现。该处理器配备英特尔首款集成的神经网络处理器（NPU），用于在PC上带来高能效的AI加速和本地推理体验。基辛格透露，酷睿Ultra将在12月14日发布。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b4/b401df224216ec1e3f752224e64dbb4b.png\" /></p><p>酷睿Ultra处理器</p><p>&nbsp;</p><p>基辛格将酷睿Ultra处理器称作英特尔客户端处理器路线图的一个转折点：该款处理器是首个采用Foveros封装技术的客户端芯粒设计。除了NPU以及Intel 4制程节点在性能功耗比上的重大进步外，这款处理器还通过集成英特尔锐炫显卡，带来独立显卡级别的性能。</p><p>&nbsp;</p><p>基辛格展示了全新AI PC的众多使用场景，并邀请宏碁首席运营官高树国上台介绍了搭载酷睿Ultra处理器的宏碁笔记本电脑。高树国表示：“我们与英特尔团队合作，通过OpenVINO工具包共同开发了一套宏碁AI库，以充分利用英特尔酷睿Ultra平台，还共同开发了AI库，最终将这款产品带给用户。”</p><p>&nbsp;</p><p>AI创业公司Rewind AI也来到现场，演示在断网的情况下，由英特尔OpenVINO驱动在PC本地运行大语言模型，与AI聊天机器人进行实时问答。</p><p></p><h2>制程、封装最新进展</h2><p></p><p></p><p>在2022年英特尔投资者大会上，英特尔公布了接下来几年间的制程发展规划。按照英特尔的计划，未来的四年间，英特尔将跨过五个制程节点。此后一年间，这个计划的进展颇受关注。</p><p>&nbsp;</p><p>在今天的主题演讲中，基辛格表示，英特尔的“四年五个制程节点”计划进展顺利，Intel 7已经实现大规模量产，Intel 4已经生产准备就绪，Intel 3也在按计划推进中，目标是2023年年底。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5be4db4865e0fbd06c9547916cf21d1a.jpeg\" /></p><p></p><p>基辛格还展示了基于Intel 20A制程节点打造的英特尔Arrow Lake处理器的首批测试芯片。他表示，Arrow Lake将于2024年面向客户端市场推出。Intel 20A将是首个应用PowerVia背面供电技术和新型全环绕栅极晶体管RibbonFET的制程节点。同样将采用这两项技术的Intel 18A制程节点也在按计划推进中，将于2024年下半年生产准备就绪。</p><p>&nbsp;</p><p>此前英特尔已经官宣 Intel 18A进程的多项进展。今年4月，英特尔代工服务事业部（IFS）和Arm宣布签署协议，旨在使芯片设计公司能够利用Intel 18A制程工艺来开发低功耗计算系统级芯片（SoC）。今年7月，爱立信宣布与英特尔达成战略合作协议，将采用英特尔18A制程和制造技术为爱立信的下一代5G基础设施优化提供支持。</p><p>&nbsp;</p><p>除制程外，英特尔向前推进摩尔定律的另一路径是使用新材料和新封装技术，如玻璃基板（glass substrates）。这是英特尔刚于本周宣布的一项突破。玻璃基板将于2020年代后期推出，继续增加单个封装内的晶体管数量，助力满足AI等数据密集型高性能工作负载的需求，并在2030年后继续推进摩尔定律。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/58/5885cfc3b010843f050f867823486802.png\" /></p><p>基辛格展示玻璃基板材料</p><p>&nbsp;</p><p>英特尔还展示了基于通用芯粒高速互连开放规范（UCIe）的测试芯片封装。基辛格表示，摩尔定律的下一波浪潮将由多芯粒封装技术所推动，如果开放标准能够解决IP集成的障碍，它将很快变成现实。发起于去年的UCIe标准将让来自不同厂商的芯粒能够协同工作，从而以新型芯片设计满足不同AI工作负载的扩展需求。目前，UCIe开放标准已经得到了超过120家公司的支持。</p><p>&nbsp;</p><p>该测试芯片集成了基于Intel 3制程节点的英特尔UCIe IP芯粒，和基于TSMC N3E制程节点的Synopsys UCIe IP芯粒。这些芯粒通过EMIB（嵌入式多芯片互连桥接）先进封装技术互连在一起。英特尔代工服务（Intel Foundry Services）、TSMC和Synopsys携手推动UCIe的发展，体现了三者支持基于开放标准的芯粒生态系统的承诺。</p><p></p><h2>先进计算和前沿研究</h2><p></p><p></p><p>除了大众关注更多的软硬件进展，英特尔研究院还在诸多前沿技术方向上展开探索，包括神经拟态计算、量子计算等。在主题演讲的最后，基辛格也对这些前沿研究方向的最新进展做了介绍。</p><p>&nbsp;</p><p>基于Loihi 2第二代研究芯片和开源Lava软件框架，英特尔研究院正在推动神经拟态计算的发展。Loihi 2是性能业界领先的神经拟态研究芯片，基于Intel 4制程节点开发，每个芯片最多可包含100万个神经元。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/25/255bb82d22d783dbe72d20de51839d98.jpeg\" /></p><p></p><p>Loihi 2还具有可扩展性，8芯片Loihi 2开发板Kapoho Point，可通过堆叠满足大规模工作负载的需求。英特尔还提供开源、模块化、可扩展的Lava软件框架，助力神经拟态应用的开发。</p><p>&nbsp;</p><p>量子计算方面，今年6月，英特尔发布了包含12个硅自旋量子比特（silicon spin qubit）的全新量子芯片Tunnel Falls，继续探索量子实用性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a54d095dc1771a56c9cf0d46369b418f.jpeg\" /></p><p></p><p>在英特尔的晶圆厂里，Tunnel Falls是在300毫米的硅晶圆上生产的，利用了英特尔领先的晶体管工业化制造能力，如极紫外光刻技术（EUV），以及栅极和接触层加工技术。</p><p>&nbsp;</p><p>虽然这次在主题演讲中并未提及，但也有部分开发者对于英特尔在RISC-V方向的动态十分关注。英特尔近几年一直在投资 RISC-V，去年英特尔加入了全球开放硬件标准组织 RISC-V International，该组织成员包括阿里云、谷歌、IBM、Nvidia、三星等；旗下的 Mobileye 还推出了基于 RISC-V 的 EyeQ Ultra 芯片。但今年1月份，英特尔停止Intel Pathfinder for RISC-V项目，引发了RISC-V社区对于英特尔是否会继续投入的担忧。根据InfoQ获知的最新消息，英特尔只是停止了一个针对RISC-V 的前期探路项目，并没有停止对RISC-V 的支持，而是转而继续正式支持RISC-V，相关研究工作英特尔中国团队将会重点投入。</p><p></p><h2>写在最后</h2><p></p><p></p><p>在生成式AI这波大潮之下，更多人只关注到了GPU的成功和抢手程度，但GPU可能并这波浪潮唯一的受益者。随着大模型热潮爆发，未来将有海量潜在的AI应用需求，业内有观点认为，未来运行大模型所消耗的计算量（即推理的算力规模），将超过用于训练模型的计算量。从硬件层面来看，这意味着 AI 研究的重点将转向如何降低推理成本，而这恰恰是英特尔的优势所在。在本次活动现场与英特尔高层的交流中，我们也听到了类似的观点。</p><p></p><p>训练并非生成式AI的全部，我们可以看到，英特尔正试图在AI工作流的各个环节全面发力，从训练到Fine-tuning，再到部署和推理，英特尔都有软硬件层面对应的产品和技术布局。这些布局能否帮助英特尔在生成式AI浪潮下继续取得成功，归根结底还是在于其能否很好地匹配客户需求、给客户带来足够的商业价值，这也是英特尔当前仍然面临的挑战，让我们一起拭目以待。</p>",
    "publish_time": "2023-09-20 09:33:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Java 近期新闻：Spring AI, Spring Modulith 1.0, Testcontainers桌面版",
    "url": "https://www.infoq.cn/article/Z9COLI6mFFhjJGskgUrS",
    "summary": "<p></p><h4>OpenJDK</h4><p></p><p>Oracle 的 Loom 项目架构师兼技术负责人 <a href=\"https://inside.java/u/RonPressler/\">Ron Pressler</a>\" <a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-August/008061.html\">提出</a>\"了 JEP 草案 8307341，《<a href=\"https://openjdk.org/jeps/8307341\">准备限制 JNI 的使用</a>\"》，提议限制使用本质上不安全的 Java 本地接口（JNI）及外部函数与内存（FFM） API 中的受限方法，后者预计于 JDK 22 中成为最终功能。从 JDK 22 开始，除非 FFM 用户在命令行中启用了不安全的本地访问，这项策略将使 Java 运行时显示使用 JNI 的相关警告。预计在 JDK 22 之后的版本中， JNI 的使用将导致异常抛出而非警告。</p><p>&nbsp;</p><p>JDK 回归测试工具 jtreg 的版本 7.3.1 已<a href=\"https://mail.openjdk.org/pipermail/jdk-dev/2023-August/008060.html\">发布</a>\"并准备集成至 JDK 中， 修复了 jtreg</p><p>版本 7.3 中会导致无法在 Windows 上正确设置默认环境变量的回归问题。关于该版本的更多详情，请参见<a href=\"https://github.com/openjdk/jtreg/blob/master/CHANGELOG.md\">发表说明</a>\"。</p><p></p><h4>JDK 21</h4><p></p><p><a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-21%2B35\">Build 35</a>\"&nbsp;仍是 JDK 21 <a href=\"https://jdk.java.net/20/\">早期访问构建</a>\"的当前版本。关于该版本的更多详情，请参见<a href=\"https://jdk.java.net/21/release-notes\">发布说明</a>\"。</p><p></p><h4>JDK 22</h4><p></p><p>JDK 22 <a href=\"https://jdk.java.net/22/\">早期访问构建</a>\" 的 <a href=\"https://github.com/openjdk/jdk/releases/tag/jdk-22%2B12\">Build 12</a>\"&nbsp;也已于上周发布，主要提供对 Build 11 的<a href=\"https://github.com/openjdk/jdk/compare/jdk-22%2B11...jdk-22%2B12\">更新</a>\"及多项<a href=\"https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2022%20and%20%22resolved%20in%20build%22%20%3D%20b12%20order%20by%20component%2C%20subcomponent\">问题</a>\"的修复。关于该版本的更多详情，请参见<a href=\"https://jdk.java.net/22/release-notes\">发布说明</a>\"。</p><p>&nbsp;</p><p>鼓励开发者们将 <a href=\"https://openjdk.org/projects/jdk/22/\">JDK 22</a>\"&nbsp;及&nbsp;<a href=\"https://openjdk.java.net/projects/jdk/21/\">JDK 21</a>\" 中的问题反馈至 <a href=\"https://bugreport.java.com/bugreport/\">Java Bug 数据库</a>\"。</p><p></p><h4>Jakarta EE</h4><p></p><p>Eclipse 基金会的 Jakarta EE 开发倡导者 <a href=\"https://se.linkedin.com/in/ivargrimstad\">Ivar Grimstad</a>\" 在每周的 Hashtag Jakarta EE&nbsp;<a href=\"https://www.agilejava.eu/\">博客</a>\"中给出了添加&nbsp;<a href=\"https://jakarta.ee/specifications/data/\">Jakarta Data</a>\"、<a href=\"https://jakarta.ee/specifications/mvc/\">Jakarta MVC</a>\"&nbsp;及 <a href=\"https://jakarta.ee/specifications/nosql/\">Jakarta NoSQL</a>\"规范至 Jakarta EE 11 平台的<a href=\"https://www.agilejava.eu/2023/08/20/hashtag-jakarta-ee-190/\">投票结果</a>\"：只有 Jakarta Data 这一项规范通过。</p><p>&nbsp;</p><p>对引入 Jakarta MVC 决定投反对票或弃权的部分评论如下：</p><p>&nbsp;</p><p></p><blockquote>这项规范成熟且目前已有人采用，但在强制使用前还应有更多供应商侧的采用。正如其他人所说的，这项规范可以被独立添加到任何配置文件中，在不限制任何人的使用的同时，还能创造更多的需求从而被添加到未来版本中，或者是给下次的版本发布计划提供一个更新的理由。我鼓励这项工作并希望它能得到进一步发展，我期望见到它最终被平台采用的一天。我认为这是对平台的一项有趣补充，它已经被添加到 GlassGish 中并且还是开箱可用的。但我们也有一些顾虑，其中就有 Jakarta MVC 是基于 Jakarta REST，而 Jakarta EE 中目前的 MVC 框架是基于 Jakarta Servlet 这一事实。以 REST 为基础的新 API 则又让事情更加混乱，毕竟 Jakarta EE 的核心就是“HTTP 处理 API”。我们希望 Jakarta Servlet 和 Jakarta REST 之间能先搭建一个共同基础，再将任何构建于 Jakarta REST 的东西接入平台。</blockquote><p></p><p>&nbsp;</p><p>对引入 Jakarta NoSQL 决定投反对票或弃权的部分评论如下：</p><p>&nbsp;</p><p></p><blockquote>目前的架构设计相比 Jakarta 平台的发布计划似乎需要更为频繁的更新，这让将其暂时排除在平台内给出了有利的论据。另外一项要求则可能是要先将 Jakarta Data 和 Jakarta Config 引入。总体来说，支持 NoSQL 是个好主意，所以这个决定在未来可能会变化。这项规范很有用并且在不远的将来也应被引入，但不是现在，它在 EE 11 时间框架内的成熟度也不明确。与供应商的 API 或运行时相比，（NoSQL）没有实际的功能，甚至可能恰恰相反：不使用专属 API 就没法使用 NoSQL 后端，在我看来这和目标南辕北辙。唯一的好处是它能在10到15行代码内完成，所以我觉得这不能让维护的负担名正言顺。</blockquote><p></p><p>&nbsp;</p><p></p><h4>BellSoft</h4><p></p><p>BellSoft 已为 Liberica JDK 17 及 OpenJDK 11 的下游发行版本提供了<a href=\"https://bell-sw.com/blog/invalid-cen-header-fixed-in-the-latest-jdk-11-and-17-updates/\">补丁发布</a>\"，其中包括对 JDK-8313765“<a href=\"https://bugs.openjdk.org/browse/JDK-8313765\">无效 CEN 头（无效 zip64 额外数据字段大小）</a>\"”这一关键问题修复，该回归问题中通过多个三方工具打开 APK、ZIP 或 JAR 文件会导致 ZipException 抛出。这一问题在 JDK-8302483“<a href=\"https://bugs.openjdk.org/browse/JDK-8302483\">改善 ZIP64 额外字段验证</a>\"”中出现，在打开 ZIP 文件时需要对 ZIP64 额外字段提供附加验证。</p><p>&nbsp;</p><p>BellSoft 同样<a href=\"https://bell-sw.com/blog/bellsoft-releases-alpaquita-containers-for-spring-boot-applications/\">推出</a>\"了基于 <a href=\"https://bell-sw.com/alpaquita-linux/\">Alpaquita Linux</a>\"且<a href=\"https://bell-sw.com/alpaquita-containers-for-spring-boot-apps/\">专为 Spring Boot 应用程序设计的 Alpaquita 容器</a>\"，前者是一款为 Java 编程语言和 <a href=\"https://bell-sw.com/libericajdk/\">Liberica JDK</a>\" 量身定做、以 <a href=\"https://www.alpinelinux.org/\">Alpine Linux</a>\" 为基础的操作系统。Alpaquita 容器于2022年9月首次<a href=\"https://bell-sw.com/blog/bellsoft-introduces-alpaquita-linux/\">推出</a>\"，其灵感来源是带有 Spring Boot 应用程序的小型容器可节省云资源这一<a href=\"https://bell-sw.com/blog/spring-container-size-why-it-is-important/\">发现</a>\"。</p><p></p><h4>Spring 框架</h4><p></p><p><a href=\"https://spring.io/projects/spring-boot\">Spring Boot</a>\"&nbsp;3.2.0 的<a href=\"https://spring.io/blog/2023/08/24/spring-boot-3-2-0-m2-available-now\">第二里程碑版本的发布</a>\"提供问题修复、文档优化，依赖升级以及新功能，其中包括：使用 jOOQ 功能判断 SQL 方言；替换已弃用的 TaskSchedulerBuilder类的新增&nbsp;ThreadPoolTaskSchedulerBuilder&nbsp;类；新增&nbsp;SimpleAsyncTaskExecutorBuilder&nbsp;类用于构建&nbsp;SimpleAsyncTaskExecutor&nbsp;类的实例。关于该版本的更多详情，请参见<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v3.2.0-M2\">发布说明</a>\"。</p><p>&nbsp;</p><p>Spring Boot 的 <a href=\"https://spring.io/blog/2023/08/24/spring-boot-3-1-3-available-now\">3.1.3</a>\"、<a href=\"https://spring.io/blog/2023/08/24/spring-boot-3-0-10-available-now\">3.0.10</a>\"&nbsp;及&nbsp;<a href=\"https://spring.io/blog/2023/08/24/spring-boot-2-7-15-available-now\">2.7.15</a>\"&nbsp;版本，均提供文档优化、依赖升级及重要问题修复，其中包括：记录检测到的非 XML 格式且带有查询参数的配置 URL；JobLauncherApplicationRunner 类的一个实例即使未执行任何任务也会返回成功退出码；添加 RabbitMQ <a href=\"https://docs.vmware.com/en/VMware-RabbitMQ-for-Tanzu-Application-Service/2.2/tanzu-rmq/GUID-smoke-tests.html\">冒烟测试</a>\"所缺的一个测试。关于这些版本的更多详情，请参见<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v3.1.3\">3.1.3 版本</a>\"、<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v3.0.10\">3.0.10 版本</a>\"及<a href=\"https://github.com/spring-projects/spring-boot/releases/tag/v2.7.15\">2.7.15 版本</a>\"的发布说明。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-modulith\">Spring Modulith</a>\" 1.0 版本 <a href=\"https://spring.io/blog/2023/08/21/spring-modulith-1-0-ga-released\">发布</a>\"，主要提供：移除 Scenario 类的实验性声明；移除 BOM 中 Spring Modulith Events 的父 POM；升级至 Spring Asciidoctor Backends 0.0.7 版本及 jMolecules 2023.1.0 版本。关于该版本的更多详情，请参见<a href=\"https://github.com/spring-projects/spring-modulith/releases/tag/1.0.0\">发布说明</a>\"。InfoQ 将跟进更为详细的报道。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-authorization-server\">Spring 授权服务器</a>\"&nbsp;1.1.2 版本<a href=\"https://spring.io/blog/2023/08/22/spring-authorization-server-1-1-2-available-now\">发布</a>\"，提供依赖升级及重要问题修复，其中包括：新增长度验证以避免无效 usercode 导致的 HTTP 500 内部服务器错误；示例测试套组 demo-authorizationserver 未能在构建流程中被一并执行；自定义表单登录类 DefaultErrorController 在抛出 NullPointerException 时缺失错误信息属性。关于该版本的更多详情，请参见<a href=\"https://github.com/spring-projects/spring-authorization-server/releases/tag/1.1.2\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://spring.io/projects/spring-batch\">Spring Batch</a>\"&nbsp;的 5.1.0-M2、5.0.3 及 4.3.9 版本现已<a href=\"https://spring.io/blog/2023/08/24/spring-batch-5-1-0-m2-5-0-3-and-4-3-9-available-now\">发布</a>\"，提供问题修复、文档优化及功能增强：在 Jackson2ExecutionContextStringSerializer 类的可信列表中新增 Java 的 ConcurrentHashMap 及 Date类；替换 mock() 方法中的 mock(Class classToMock) 方法以实现自动检测需要模拟的类或接口。5.1.0-M2 版本中新增功能包括：支持批量插入，MongoItemWriter 类中新增访问器，以便于扩展。关于这些版本的更多详情，请参见&nbsp;<a href=\"https://github.com/spring-projects/spring-batch/releases/tag/v5.1.0-M2\">5.1.0-M2 版本</a>\"、<a href=\"https://github.com/spring-projects/spring-batch/releases/tag/v5.0.3\">5.0.3 版本</a>\"及 <a href=\"https://github.com/spring-projects/spring-batch/releases/tag/4.3.9\">4.3.9 版本</a>\"的发布说明。</p><p>&nbsp;</p><p>于上周 SpringOne 大会中<a href=\"https://springone.io/sessions/intelligent-beans-with-spring-ai\">推出</a>\"的 <a href=\"https://github.com/spring-projects-experimental/spring-ai/blob/main/README.md\">Spring AI</a>\"，据称是“用于开发 AI 应用程序的 Spring 友好 API 和抽象”，开发者们可通过这段由 VMware 公司 Spring 开发倡导者 <a href=\"https://www.linkedin.com/in/joshlong/\">Josh Long</a>\" 及 高级资深工程师 <a href=\"https://www.linkedin.com/in/marklpollack/\">Mark Pollack</a>\" 带来的油管<a href=\"https://www.youtube.com/watch?v=0P8UU5vkvI8\">视频</a>\"，或是 <a href=\"https://twitter.com/asirselvasingh/status/1694435497010192502\">ACME 健身商城</a>\"应用程序了解更多详情。InfoQ 将跟进更为详细的新闻报道。</p><p></p><h4>AtomicJar</h4><p></p><p>“为数据库、消息代理、网络浏览器或任何可运行于 Docker 容器内的东西提供可抛式轻量级实例的开源框架”的 <a href=\"https://testcontainers.com/\">Testcontainers</a>\" 的制造商 <a href=\"https://www.atomicjar.com/\">AtomicJar</a>\"，<a href=\"https://www.atomicjar.com/2023/08/announcing-testcontainers-desktop-free-application/\">推出</a>\"了全新&nbsp;<a href=\"https://testcontainers.com/desktop/\">Testcontainers 桌面版</a>\"应用程序，面向 Java 社区免费使用。该版本所提供的功能包括：允许开发人员设置固定端口，以提升调试体验，连接至运行中容器，并可冻结容器避免其在调试过程中关闭。该应用还允许开发者轻松切换本地容器运行时，避免在通过 OrbStack、Colima、Rancher Desktop 或 Podman 运行 Testcontainers 时再操作 testcontainers.properties 文件。InfoQ 将跟进更为详细的新闻报道。</p><p>&nbsp;</p><p><a href=\"https://github.com/testcontainers/testcontainers-java/blob/main/README.md\">Testcontainers for Java</a>\"&nbsp;1.19.0 版本同样于上周<a href=\"https://github.com/testcontainers/testcontainers-java/releases/tag/1.19.0\">发布</a>\"，提供重大变更，其中包括：Wait 类中新增 forListeningPort(port) 方法便于检查特定端口；默认使用 SelinuxContext.SHARED 枚举； 新增 ClickHouseContainer 类实现，支持 withUsername()、withPassword()、withDatabaseName()&nbsp;及 withUrlParam() 方法。</p><p></p><h4>Open Liberty</h4><p></p><p>IBM 已<a href=\"https://openliberty.io/blog/2023/08/22/23.0.0.8.html\">发布</a>\"<a href=\"https://openliberty.io/\">Open Liberty</a>\"&nbsp;23.0.0.8 版本，主要提供：支持 OpenID Connect 客户端<a href=\"https://datatracker.ietf.org/doc/html/rfc7636\">保护授权代码授权</a>\"（PKCE），可用于避免授权码拦截攻击；修复 <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-38737\">CVE-2023-38737</a>\" 漏洞，该漏洞可导致攻击者向 Open Liberty 22.0.0.13 版本至 23.0.0.7 版本发送特制请求，导致服务器占用内存资源从而导致拒绝服务；确保 featureUtility installFeature  命令执行后会安装足够数量的功能，该命令在更新前无法保证正常工作。</p><p></p><h4>Quarkus</h4><p></p><p>红帽<a href=\"https://quarkus.io/blog/quarkus-3-3-0-released/\">发布</a>\"了 <a href=\"https://quarkus.io/\">Quarkus</a>\"&nbsp;的 3.3.0 版本，提供重要变更，其中包括：优化 <a href=\"https://quarkus.io/extensions/io.quarkus/quarkus-opentelemetry\">OpenTelemetry</a>\" 扩展；新增&nbsp;<a href=\"https://quarkus.io/extensions/io.quarkus/quarkus-smallrye-reactive-messaging-pulsar\">SmallRye Reactive Messaging Pulsar</a>\"&nbsp;扩展；可在&nbsp;<a href=\"https://quarkus.io/guides/rest-client-reactive\">REST Client Reactive</a>\" 扩展中自定义 Jackson ObjectMapper 类的功能。值得注意的是，从这一版本开始，版本名称中的 .Final 后缀将被移除，这种版本控制方式已经过时。关于该版本的更多详情，请参见<a href=\"https://github.com/quarkusio/quarkus/releases/tag/3.3.0\">发布说明</a>\"。</p><p></p><h4>MicroProfile</h4><p></p><p>在前往 MicroProfile 6.1 版本的路上，MicroProfile 工作组交付了 <a href=\"https://github.com/eclipse/microprofile-metrics/blob/main/README.adoc\">MicroProfile Metrics</a>\"&nbsp;5.1规范的<a href=\"https://github.com/eclipse/microprofile-metrics/releases/tag/5.1.0-RC1\">首个发布版本候选</a>\"，提供的重要变更包括：引入 MicroProfile 配置属性，可自定义直方图和定时器指标追踪和输出的百分比、直方图桶的统计数据；@RegistryScope 注解变更为 Qualifier；新增 mp.metrics.defaultAppName 属性用于要求标签组的统一，这一问题曾在多应用程序服务器实现中造成问题。关于该版本的更多详情，请参见<a href=\"https://download.eclipse.org/microprofile/microprofile-metrics-5.1.0-RC1/microprofile-metrics-spec-5.1.0-RC1.html#release_notes_5_1\">发布说明</a>\"。</p><p>&nbsp;</p><p>同样，<a href=\"https://github.com/eclipse/microprofile-telemetry/blob/main/README.adoc\">MicroProfile Telemetry</a>\"&nbsp;1.1 规范的<a href=\"https://github.com/eclipse/microprofile-telemetry/releases/tag/1.1-RC2\">第二发布版本候选</a>\"也已发布，提供对 <a href=\"https://opentelemetry.io/docs/instrumentation/java/\">OpenTelemetry Java</a>\"&nbsp;1.29.0 版本的依赖升级；明确 Span&nbsp;和 Baggage bean 在当前 span 或 baggage 变更时的行为；不依赖时间戳的测试实现。关于该版本的更多详情，请参见<a href=\"https://download.eclipse.org/microprofile/microprofile-telemetry-1.1-RC2/tracing/microprofile-telemetry-tracing-spec-1.1-RC2.html#_release_notes\">发布说明</a>\"。</p><p></p><h4>Micronaut</h4><p></p><p>Micronaut 基金会已交付 <a href=\"https://micronaut.io/\">Micronaut 框架</a>\"的<a href=\"https://micronaut.io/2023/08/24/micronaut-framework-4-0-5-released/\">第五维护版本</a>\"&nbsp;4.0.5，提供模块更新如下：<a href=\"https://micronaut-projects.github.io/micronaut-cassandra/latest/guide/\">Micronaut Cassandra</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-microstream/latest/guide/\">Micronaut MicroStream</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-security/latest/guide/\">Micronaut Security</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-liquibase/latest/guide/\">Micronaut Liquibase</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-flyway/latest/guide/\">Micronaut Flyway</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-gcp/latest/guide/\">Micronaut GCP</a>\"、<a href=\"https://micronaut-projects.github.io/micronaut-aws/latest/guide/\">Micronaut AWS</a>\"&nbsp;以及 <a href=\"https://micronaut-projects.github.io/micronaut-servlet/latest/guide/\">Micronaut Servlet</a>\"。关于该版本的更多详情，请参见<a href=\"https://github.com/micronaut-projects/micronaut-platform/releases/tag/v4.0.5\">发布说明</a>\"。</p><p>&nbsp;</p><p>用于 <a href=\"https://www.jhipster.tech/\">JHipster</a>\"&nbsp;的 <a href=\"https://github.com/jhipster/generator-jhipster-micronaut\">Micronaut Blueprint</a>\"&nbsp;2.0.0 版本也于上周<a href=\"https://micronaut.io/2023/08/24/micronaut-jhipster-2-0-0/\">发布</a>\"。基于 JHipster 最新稳定版本7.9.3 版本，该蓝图可为单体或微服务形式 JHipster 应用程序生成基于 Micronaut 框架 3.10.1 版本的后端服务器。</p><p></p><h4>阿帕奇软件基金会</h4><p></p><p><a href=\"https://groovy-lang.org/\">阿帕奇 Groovy</a>\"&nbsp;5.0.0 版本的<a href=\"https://www.mail-archive.com/announce@apache.org/msg08440.html\">首个 alpha 发布</a>\"提供众多问题修复、依赖升级、优化及新功能，其中包括：DefaultGroovyMethods 类中新方法 asChecked() ，用于优化对 checkedCollection()、checkedList()、checkedMap() 等 Java Collections 类中的支持；新增 @OperatorRename 注解，用于优化 AST 转换； 增加对 JEP 445，<a href=\"https://openjdk.org/jeps/445\">未命名类和实例 main 方法（预览版）</a>\"的初始支持。关于该版本的更多详情，请参见<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12351227\">发布说明</a>\"。</p><p>&nbsp;</p><p>阿帕奇 Groovy <a href=\"https://www.mail-archive.com/announce@apache.org/msg08435.html\">4.0.14</a>\"&nbsp;及 <a href=\"https://www.mail-archive.com/announce@apache.org/msg08436.html\">3.0.19</a>\" 版本同样提供问题修复、依赖升级及优化，其中包括：DefaultGroovyMethods 类中定义的 collectEntries() 方法支持空参数；支持静态类型检查时元组闭包参数类型推断。关于该版本的更多详情，请参见&nbsp;<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12353386\">4.0.14 版本</a>\"及 <a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12353387\">3.0.19 版本</a>\"的发布说明。</p><p>&nbsp;</p><p>最后，阿帕奇 Groovy 2.5.23版本的<a href=\"https://www.mail-archive.com/announce@apache.org/msg08437.html\">发布</a>\"，提供两处问题修复：优化 Closure 类中变量解析的行为；执行 Groovy 脚本时抛出 NoSuchMethodError。关于该版本的更多详情，请参见<a href=\"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12318123&amp;version=12353077\">发布说明</a>\"。</p><p>&nbsp;</p><p><a href=\"https://tomcat.apache.org/\">阿帕奇 Tomcat</a>\"的&nbsp;<a href=\"https://www.mail-archive.com/announce@apache.org/msg08449.html\">11.0.0-M11</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg08451.html\">10.1.13</a>\"、<a href=\"https://www.mail-archive.com/announce@apache.org/msg08450.html\">9.0.80</a>\"&nbsp;及 <a href=\"https://www.mail-archive.com/announce@apache.org/msg08453.html\">8.5.93</a>\"版本于上周发布，四个版本均提供重要变更，其中包括：修复阿帕奇 Tomcat 的表单验证功能导致 URL 重定向至不受信任网站的 <a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-41080\">CVE-2023-41080</a>\" 漏洞；应用程序或库同时设置了非 HTTP 500 内部服务器错误码及 jakarta.servlet.error.exception 请求属性时，在处理错误页面时不会再使用默认 HTTP 500，而使用程序提供的错误码。11.0.0-M11 版本同样包含对 HTTP 参数处理的更新，以与 <a href=\"https://jakarta.ee/specifications/servlet/6.1/\">Jakarta Servlet 6.1</a>\" API 的 ServletRequest 接口方法定义变更保持一致。关于这些版本的更多详情，请参见&nbsp;<a href=\"http://tomcat.apache.org/tomcat-11.0-doc/changelog.html\">11.0.0-M11 版本</a>\"、<a href=\"http://tomcat.apache.org/tomcat-10.1-doc/changelog.html\">10.1.13 版本</a>\"、<a href=\"https://tomcat.apache.org/tomcat-9.0-doc/changelog.html\">9.0.80 版本</a>\"及<a href=\"https://tomcat.apache.org/tomcat-8.5-doc/changelog.html\">8.5.93 版本</a>\"的发布说明。</p><p></p><h4>Grails</h4><p></p><p>Grails 基金会<a href=\"https://grails.org/blog/2023-08-25-introducing-grails-spring-security-core-6.html\">推出</a>\"了 <a href=\"https://github.com/grails/grails-spring-security-core/blob/6.0.x/README.md\">Grails Spring Security Core 插件</a>\"的 6.0.0 版本，该版本具有更高安全性、支持 Spring Security 5.8.6 版本、 Grails 6.0.0 版本兼容、优化了命令行界面、升级依赖关系，并改进了文档导航。</p><p></p><h4>JHipster</h4><p></p><p><a href=\"https://www.jhipster.tech/jhipster-lite/\">JHipster Lite</a>\" 的&nbsp;<a href=\"https://twitter.com/pascalgrimaud/status/1693565243396325500\">0.41.0</a>\"&nbsp;版本已经发布，主要提供问题修复、依赖升级及优化，其中包括：使用 JHipster&nbsp;的@ExcludeFromGeneratedCodeCoverage 注解替换 Java 的 @Generated 注解；移除 OAuth2Configuration 类中的 password() 方法；使用应用程序配置文件中的配置项执行集成测试。关于该版本的更多详情，请参见<a href=\"https://github.com/jhipster/jhipster-lite/releases/tag/v0.41.0\">发布说明</a>\"。</p><p></p><h4>Eclipse Vert.x</h4><p></p><p>Eclipse Vert.x 团队已为用于分析工作负载的实时分布式数据存储，<a href=\"https://docs.pinot.apache.org/users/clients/java\">阿帕奇 Pinot Java 客户端</a>\"<a href=\"https://vertx.io/blog/soc-vertx-pinot-client/\">推出</a>\"了全新的 <a href=\"https://github.com/reactiverse/pinot-client/blob/main/README.md\">Pinot 客户端</a>\"。这一新客户端可为 Eclipse Vert.x 应用程序查询阿帕奇 Pinot 服务器提供便利的 API。</p><p></p><h4>Yupiik</h4><p></p><p><a href=\"https://www.yupiik.io/fusion/\">Yupiik Fusion</a>\" 的 1.0.6 版本现已<a href=\"https://www.yupiik.io/blog/release-yupiik-fusion-1.0.6.html\">发布</a>\"，提供重要变更，其中包括：支持超过 255 行的嵌套式表格；可在 PartialResponse 类中自定义 JsonRpcHandler 类的 RESPONSE_HEADERS 字段；支持OffsetDateTime、ZoneOffset&nbsp;及 <a href=\"https://download.java.net/java/early_access/jdk21/docs/api/java.base/java/time/LocalDate.html\">LocalDate</a>\" 作为JSON-RPC端点的根参数。关于该版本的更多信息，请参见<a href=\"https://github.com/yupiik/fusion/releases/tag/fusion-1.0.6\">发布说明</a>\"。</p><p></p><h4>SpringOne</h4><p></p><p><a href=\"https://springone.io/\">SpringOne</a>\"&nbsp;及 VMware Explore 大会上周于内华达州拉斯维加斯的威尼斯人会展中心举行，<a href=\"https://springone.io/sessions\">大会</a>\"内容受众为应用程序开发人员、平台操作员、DevOps、SRE 及应用程序架构师。涉及的 Spring 技术包括：Spring 应用程序的平台及工具、Spring 框架、Spring Boot、Spring Security、Spring Cloud、Spring Data/Stream 以及 Spring 社区。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/08/java-news-roundup-aug21-2023/\">Java News Roundup: Introducing Spring AI, Spring Modulith 1.0, Testcontainers Desktop</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/QB87kOkWjf6jXxODkd9T\">Java 21：最新进展一览</a>\"</p><p><a href=\"https://www.infoq.cn/article/wti8XjbwvZdWSr79kOc6\">Java 近期新闻：单查询加载、GraalVM、GlassFish、JReleaser、Quarkus、Micronaut</a>\"</p>",
    "publish_time": "2023-09-20 09:59:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "平安人寿科技总监魏政刚确认出席 FCon ，分享 AIGC 及大模型在保险销售领域的应用",
    "url": "https://www.infoq.cn/article/jglWMMBydt4xsU4XYE7L",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。平安人寿科技总监魏政刚将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5528?utm_source=infoqweb&amp;utm_medium=article\">AIGC 及大模型在保险销售领域的应用</a>\"》主题分享，介绍人工智能在金融行业里应用实践，AIGC、大模型技术落地中面临的挑战、举措及经验教训以及保险知识体系的构造。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5528?utm_source=infoqweb&amp;utm_medium=article\">魏政刚</a>\"，数字时代的思考者和践行者，热衷于通过数字化转型与创新，发掘内、外部业务机会，在企业发展的不同阶 段，创新、赋能企业可持续性发展。擅长团队组建、管理咨询、信息化规划、数字化战略与运营、解决方案 设计、以及数字科技（人工智能、大数据、云计算、交互、分析等）与商业价值的优化整合。在二十多年的职业生涯中，在全球范围内服务过金融、生产制造、医疗健康、生命科学、信息通信技术（ICT）、消费品、 自然资源、物流配送等行业。他在本次会议的演讲内容如下：</p><p></p><p>演讲：AIGC 及大模型在保险销售领域的应用</p><p></p><p>把握科技进步趋势，深入探索人工智能的行业应用，对 AIGC 及大模型在保险全价值链所起到的作用前瞻性的精准把握，形成了以智能拜访助手、AI 云面试、AI 跟拍、AI 画报、AI 形象制作、平安体验中心等一系列 AI 产品为基础的产品矩阵，有力赋能一线销售队伍，匹配保险客户需求，不断向价值最大化进步。</p><p></p><p>演讲提纲：</p><p></p><p>AIGC 及大模型与销售技术、保险知识的结合文字、图像、视频的转换数字人及代理人个人 IP 的打造在合规上的应用</p><p></p><p>你将获得：</p><p></p><p>○ 人工智能在金融行业里应用实践</p><p>○ AIGC、大模型技术落地中面临的挑战、举措及经验教训</p><p>○ 保险知识体系的构造等</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-09-20 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "德邦基金CTO李鑫：中小金融机构数字化规划要聚焦重点、有所取舍",
    "url": "https://www.infoq.cn/article/Su6bfESLE0kA7g9waE7X",
    "summary": "<p>数字化转型是一个长期工程，其中充满各种阻力和不确定性，这如今已经是业界共识。但是，不同体量的企业在面对这一系列挑战时，其棘手程度并不相同。对<a href=\"https://www.infoq.cn/theme/200\">中小型企业</a>\"而言，由于可投入的人力、资金等资源有限，在做数字化规划时，不能直接照抄大型公司的“作业”。</p><p>&nbsp;</p><p>“大型公司由于资源更为充足，因此有更多的余量来进行全面、长远的规划。然而，中小型企业更倾向于看到短期的成效，数字化转型的重点会有所不同。在进行规划时，也需要更加聚焦。只有聚焦，才能用有限的资源更快地产生成效。”在&nbsp;9&nbsp;月&nbsp;14&nbsp;日的&nbsp;InfoQ《超级连麦·数智大脑》直播中，InfoQ&nbsp;与德邦基金CTO<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576\">李鑫</a>\"深入探讨了中小金融机构数字化转型背后的思路和路径。</p><p>&nbsp;</p><p>李鑫介绍，德邦基金的数字化转型是一个长周期的规划，目的是通过拉长转型周期来稀释成本。在这个长期过程中，战略聚焦和持续性成为非常重要的两个方面，不仅要面对各种不确定性问题，还需要平衡成本、短期诉求、长期战略之间的矛盾。</p><p>&nbsp;</p><p>以下是对话全文（经&nbsp;InfoQ&nbsp;进行不改变原意的编辑整理）：</p><p></p><h5>InfoQ：您是如何理解数字化转型的？</h5><p></p><p></p><p>李鑫：在我看来，数字化转型的定义因行业而异。以金融企业为例，我认为数字化转型可以定义为：通过全面信息化来获取金融业务各个维度的过程数据及结果数据，并对这些数据进行联接、加工、汇总，从而在数字世界中构建金融业务的<a href=\"https://www.infoq.cn/article/N2gWCU0NhYWjmyxwwy2R\">数字孪生</a>\"，实现对金融业务的深刻洞察，进而辅助企业作出更优化的经营决策并落地，最终实现降本增效、并以极低成本规模化创设稳定绩优的金融产品，同时为客户提供更优质的服务。</p><p>&nbsp;</p><p>其中有几个关键词值得注意：首先是“全面信息化”，这是实现数字化转型的前提。只有通过全面信息化，企业才能收集到足够的数据进行后续分析和挖掘。</p><p>&nbsp;</p><p>其次是“数字孪生”，这个词最早来源于工业领域，这里指利用数据在虚拟数字世界中构建企业的一个全数字化投影模型。在这个轻量级的数字世界里，企业可以利用这个虚拟模型快速进行各种模拟、演化和分析，从而实现全面的自我洞察。</p><p>&nbsp;</p><p>基于这些洞察，企业能做出更优的决策，并对业务场景进行针对性的优化及创新。而业务场景的优化创新，必然涉及信息系统的改造，从而进一步推动信息化的升级，这样就形成了一个数字化转型的闭环体系。</p><p></p><h5>InfoQ：&nbsp;德邦基金作为一家中小金融企业，其数字化转型过程有什么差异化特点？有哪些需要注意的问题？</h5><p></p><p></p><p>李鑫：数字化转型是一个长期过程，其中充满了各种挑战和困难。因此，我们需要用长期的思维去看待这件事，同时也要保持聚焦。这个过程不仅是自上而下的，也是自下而上的。</p><p>&nbsp;</p><p>比如，在德邦基金的数字化转型规划中，我们是以十年为周期来进行规划的。由于我们是一家<a href=\"https://www.infoq.cn/article/1zd5MLr5iQOBzgAaGpOm\">中小型基金</a>\"公司，成本投入上无法与大公司相比。因此，我们会通过拉长转型周期来稀释成本。</p><p></p><p>在这个长周期中，战略聚焦和持续性成为非常重要的两个方面。我们不仅要面对各种问题和挑战，还需要平衡成本、短期诉求、长期战略之间的矛盾。</p><p>&nbsp;</p><p>对于大型公司，由于资源更为充足，因此他们有更多的余量来进行全面、长远的规划。然而，中小型公司由于成本限制，更倾向于看到短期的成效。因此，从中小公司和大公司的角度看，数字化转型的重点会有所不同。</p><p>&nbsp;</p><p>在进行规划时，中小型公司需要更加聚焦。因为只有聚焦，才能用有限的资源更快地产生成效，从而给公司一个阶段性的产出（交代）。这也涉及到技术资源和业务诉求的平衡，必须有所取舍。</p><p>&nbsp;</p><p>举例来说，现在基础资源的云化和服务化是一个主要的趋势，不同规模的公司在这方面有不同的做法。大型公司由于业务复杂，需求多样，会同时关注IaaS（基础设施即服务）层和PaaS（平台即服务）层的能力建设。而对于中小公司来说，如果走大公司的路线，所需的资金和技术投入会非常大，风险也会很高。</p><p></p><p>因此，我们更倾向于聚焦，例如重点关注IaaS这一层能力建设，而忽略PaaS这一层。同时，我们也会更倾向采购成熟、稳定、开箱即用的商用产品，而非自行从头搭建，从而缩短落地周期，降低总体建设成本。</p><p></p><h5>InfoQ：从底层来看，传统IT架构显然已经无法支撑全新的技术应用，德邦基金自身在技术架构层面遇到过哪些挑战？</h5><p></p><p></p><p>李鑫：技术的本质是为业务服务，随着业务的不断发展，原有的传统IT架构可能会逐渐显得不合时宜，也需要进行调整。</p><p>&nbsp;</p><p>数字化转型的核心是利用数字进行数字洞察和数字赋能，因此需要有足够多的充分覆盖各个业务领域的过程数据和结果数据，这也是前面所说的全面信息化的意义：通过足够多的信息系统来获取各类数据。而大量信息系统的存在，必然对企业的IT运维工作产生巨大的压力。因此，在整个数字化转型过程中，我们面临的首要挑战是如何保障大量信息系统的线上稳定运维的需求。</p><p>&nbsp;</p><p>回想早期，上线一个信息系统的时候，要采购独立的物理服务器，机房部署后再安装系统、部署各种中间件等，上线周期基本上以月为单位。这种效率完全无法满足现在业务发展及数字化转型的要求。所以，这几年基础资源云化的模式越来越普遍，金融企业目前越来越多的采用云服务，由于监管的要求，金融企业使用公有云的比例还是偏低，主要使用的是私有云或者行业云。在这个大背景下，我们也在朝云计算方向转型，打造我们自身的云运维技术体系。</p><p>&nbsp;</p><p>随着互联网基金销售业务的快速发展，用户量也在迅速增加。用户量的增加对直销系统的并发稳定性也提出了新的挑战，以往单体架构为主的销售系统已经不再适用，我们的在线销售系统也已经转向分布式架构。</p><p>&nbsp;</p><p>此外，由于数据量的增加，传统的关系型数据仓库已经难以为继。我们也引入分布式数据库来解决数据量快速膨胀的问题。</p><p></p><h5>InfoQ：有没有印象中特别深刻的挑战或困难？</h5><p></p><p></p><p>李鑫：数字化转型涉及大量数据的采集、存储、分析，需要有一个很好的数据底座来支撑。数据底座的核心是统一数仓，一个模型完善、治理良好的数仓，是进行数字化转型的关键所在，它对构建数字孪生至关重要，也是实现数字洞察和数字赋能的前提。数据底座的构建对我们这类非技术型的中小金融公司挑战还是非常大的。所以我们一开始采取的策略是\"借力\"，基于监管报送的需求，我们通过引入专业的技术供应商，构建了我们第一代的核心数仓，初步建立起完整的数据模型。在此基础上，我们建立了自己的数据服务和开发团队，通过消化吸收，将数仓运维能力和模型设计能力完全转化为我们自己的能力，并进一步将数仓升级为分布式架构，同时自行构建了数据网关和数据中台，从而完成了完全自我掌控的数据底座的建设。</p><p>&nbsp;</p><p>此外，从我个人的经验来看，金融企业在建立面向互联网C端的销售和营销应用时，也会面临严峻的挑战。互联网应用的特点是流量波动大，而根据金融监管要求，金融行业对于在线交易的用户身份、资金的合规性的检测必须非常严格，交易过程中涉及大量复杂繁琐的<a href=\"https://www.infoq.cn/article/ixcNvwClOzTSal48vR6k\">风控检测</a>\"及用户告知操作，异步处理这类常见的电商解决方案在金融场景中很难直接照搬使用，必须在合规性、易用性和资源投入之间找到一个平衡点。金融系统一旦出现故障，后果非常严重。因此，在方案选择上，考虑稳定性的同时，还需要考虑合规和监管方面的一系列要求，这对架构设计的挑战非常大。</p><p></p><h5>InfoQ：德邦基金在数字化转型方面，有哪些具体成果吗？</h5><p></p><p></p><p>李鑫：我们公司针对基金业务发展实施了一个“3+1”的数字化转型规划。“3”代表基金公司三大核心业务方面的数字化能力建设，它们分别是基金产品运营\"自动化\"，基金销售\"一体化\"，以及基金投研\"数智化\"，而“+1”则代表着IT自身的能力进化。</p><p>&nbsp;</p><p>基金产品运营\"自动化\"</p><p>&nbsp;</p><p>基金产品的运营是基金公司的基础业务，包括产品发行、审批、以及日常的估值清算和份额登记等环节，涉及的业务链条长、相关联的系统多、操作也很琐碎。在实际的数字化建设过程中，通过引入RPA等自动化技术，我们大幅降低了人工操作比例，提高了自动化程度，这不仅降低了人为操作的风险，还减少了劳动成本投入。实施后效果非常显著，单单RPA一项技术的引入，去年就为公司节省了一万多个小时的人力投入。</p><p>&nbsp;</p><p>基金销售\"一体化\"</p><p>&nbsp;</p><p>基金销售的一大特点是代销渠道众多，为了能够以大规模、低成本的方式进行营销，并且确保在不同渠道中用户能有统一的体验。就需要从公司视角出发，构建一体化的销售及营销平台，对外做统一的用户增长体系和客户画像体系，对内做统一的销售管理。正是基于这个理念，我们以CRM系统为核心，规划并逐步落地德邦基金销售业务一体化赋能平台，通过这套平台，我们对内解决销售管理问题；对外聚焦“对客管理”和“对客服务”，进而构建公司级的“用户增长体系”，实现一套平台，多端赋能。</p><p>&nbsp;</p><p>基金投研\"数智化\"</p><p>&nbsp;</p><p>在投资研究方面，为了让研究成果可衡量，过程可管理，我们打造了专业的投研一体化平台，通过系统化来提高投研效率，从而让研究工作更标准，更规范。同时引入大数据技术和AI技术以辅助研究人员开展研究工作，让其可以更高效地处理大量的市场、行情、舆情等数据，从而更快、更准确地把握市场行情和动态。系统化和数据智能化能力的打造不仅提高了投研的规范化和标准化，原来依赖个人单兵作战的投研方式通过系统和工具的赋能，也实现了向团队协同作战方式转变。</p><p></p><h5>InfoQ：德邦基金在数字化转型人才的培养上是如何考虑的？</h5><p></p><p></p><p>李鑫：数字化转型目前是一个热门话题，虽然大家都在讨论，相关案例也很多，但真正理解并能成功落地实施的企业和人才还是相对少数。同时，信息过多反而容易形成干扰，让企业难以筛选出贴合自身实际的参考方案。</p><p>&nbsp;</p><p>从我的个人经验看，成功的数字化转型需要两个方面的努力。首先，\"他山之石,可以攻玉\"，数字化转型是一个复杂的体系性工作，各行业都已经做了很多有益的探索。我们需要不断学习和借鉴同行业和跨行业的成功案例。不仅仅金融行业，工业制造和互联网行业在数字化转型方面也有比较丰富的实践经验，可以作为我们的有益参考。</p><p>&nbsp;</p><p>其次，理解业务是数字化转型的核心。如果你对企业业务不了解，那么进行的数字化转型规划将缺乏根基，难以有效地推动业务发展。</p><p>&nbsp;</p><p>综合来看，成功的数字化转型需要负责操盘的人员既有全面丰富的数字化方面的专业知识，又要深刻理解企业业务。只有这样，才能制定出切实有效的数字化转型规划，并在实施落地过程中不断纠偏，以保证大方向的正确。所以，在这两个方向加强人才培养在企业数字化转型中是至关重要的。</p><p></p><h5>InfoQ：在FCon大会《创新的金融科技应用》专题上，会邀请哪些重磅专家、围绕哪些话题进行分享？</h5><p></p><p>&nbsp;</p><p>李鑫：根据规划，我们主要聚焦几个关键领域。首先是人机协同的金融投研，特别是人工智能和数据挖掘在此领域的应用；其次是智能投顾和智能营销，包括精准营销和沉浸式社区构建；第三是通过科技辅助高质量的金融产品创新和优化产品运营。</p><p>&nbsp;</p><p>当然我们也关注全面的金融合规，包括数据合规、行为合规，以及前沿科技在金融安全和防洗钱方面的应用。此外，数据安全、隐私保护和可信的数字身份也是我们的关注重点。在新兴技术方面，我们正在探索Web&nbsp;3.0，以及区块链和NFT如何能助力金融创新的话题。</p><p></p><h4>关于 FCon</h4><p></p><p></p><p>首届<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">FCon全球金融科技大会</a>\"将于 11 月 19-20 日在上海举办。大会将围绕金融领域数字化转型挑战探索、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等 10+专题进行交流。</p><p></p><p>目前大会邀请了汇丰科技中国区的代理总经理马国栋、度小满金融数据智能部总经理杨青先、蚂蚁集团副总裁 &amp; 首席技术安全官韦韬博士、恒生聚源总经理吴震操担任大会联席主席。更多嘉宾仍在邀请中......</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">点击链接</a>\"即可查看全部演讲专题。</p><p></p><p>目前 <a href=\"https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">5 折 优惠购票</a>\"，仅限前 100 人，咨询购票可联系：17310043226（微信同手机号）。</p>",
    "publish_time": "2023-09-20 13:56:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023-09-20：用go语言，保证一定是n*n的正方形，实现从里到外转圈打印的功能 如果n是奇数，中心点唯一，比如 a b c d e f g h i e是中心点，依次打印 : e f i h ...",
    "url": "https://www.infoq.cn/article/32dbb711f8386a57feaf5f200",
    "summary": "<p>2023-09-20：用go语言，保证一定是n*n的正方形，实现从里到外转圈打印的功能</p><p></p><p>如果n是奇数，中心点唯一，比如</p><p></p><p>a b c</p><p></p><p>d e f</p><p></p><p>g h i</p><p></p><p>e是中心点，依次打印 : e f i h g d a b c</p><p></p><p>如果n是偶数，中心点为最里层2*2的右下点</p><p></p><p>比如</p><p></p><p>a b c d e f</p><p></p><p>g h i j k l</p><p></p><p>m n o p q r</p><p></p><p>s t u v w x</p><p></p><p>y z 0 1 2 3</p><p></p><p>4 5 6 7 8 9</p><p></p><p>最里层是</p><p></p><p>o p</p><p></p><p>u v</p><p></p><p>v是中心点，依次打印 : v u o p q w ....</p><p></p><p>来自<a href=\"https://b23.tv/Zho7gh0\">左程云</a>\"。</p><p></p><p>答案2023-09-20：</p><p></p><h1>大体步骤如下：</h1><p></p><p>1.定义一个函数print，接收一个二维字节切片m作为参数。2.获取二维切片m的长度n。3.设置四个变量a, b, c, d为(n-1)/2, (n-1)/2, n/2, n/2，分别表示每一层的起始点和终止点。4.使用循环，从最外层到最内层逐层打印。4.a.在每一层中，调用函数loop打印当前层的内容。5.在循环结束后，打印换行符。</p><p></p><p>函数loop的过程如下：1.判断如果a和c相等，表示只有一个元素，直接打印该元素并返回。2.对于其他情况，依次打印当前层的四个边。2.a. 从起始点的下一行开始，按列打印边界元素，即从上到下。2.b. 从终止点的左侧列开始，按行打印边界元素，即从右到左。2.c. 从终止点的上一行开始，按列打印边界元素，即从下到上。2.d. 从起始点的右侧列开始，按行打印边界元素，即从左到右。</p><p></p><p>在主函数main中，定义了几个测试用例，分别为不同大小的二维字节切片m，然后调用print函数进行打印。</p><p></p><p>总的时间复杂度为O(n^2)，其中n为输入二维切片m的大小。</p><p></p><p>总的额外空间复杂度为O(1)，没有使用额外空间。</p><p></p><h1>go完整代码如下：</h1><p></p><p><code lang=\"go\">package main\n\nimport \"fmt\"\n\nfunc print(m [][]byte) {\n  n := len(m)\n  for a, b, c, d := (n-1)/2, (n-1)/2, n/2, n/2; a &gt;= 0; a, b, c, d = a-1, b-1, c+1, d+1 {\n    loop(m, a, b, c, d)\n  }\n  fmt.Println()\n}\n\nfunc loop(m [][]byte, a, b, c, d int) {\n  if a == c {\n    fmt.Printf(\"%c \", m[a][b])\n  } else {\n    for row := a + 1; row &lt;= c; row++ {\n      fmt.Printf(\"%c \", m[row][d])\n    }\n    for col := d - 1; col &gt;= b; col-- {\n      fmt.Printf(\"%c \", m[c][col])\n    }\n    for row := c - 1; row &gt;= a; row-- {\n      fmt.Printf(\"%c \", m[row][b])\n    }\n    for col := b + 1; col &lt;= d; col++ {\n      fmt.Printf(\"%c \", m[a][col])\n    }\n  }\n}\n\nfunc main() {\n  map1 := [][]byte{{'a'}}\n  print(map1)\n\n  map2 := [][]byte{{'a', 'b'}, {'c', 'd'}}\n  print(map2)\n\n  map3 := [][]byte{{'a', 'b', 'c'}, {'d', 'e', 'f'}, {'g', 'h', 'i'}}\n  print(map3)\n\n  map4 := [][]byte{{'a', 'b', 'c', 'd'}, {'e', 'f', 'g', 'h'}, {'i', 'j', 'k', 'l'}, {'m', 'n', 'o', 'p'}}\n  print(map4)\n\n  map5 := [][]byte{{'a', 'b', 'c', 'd', 'e'}, {'f', 'g', 'h', 'i', 'j'}, {'k', 'l', 'm', 'n', 'o'}, {'p', 'q', 'r', 's', 't'}, {'u', 'v', 'w', 'x', 'y'}}\n  print(map5)\n\n  map6 := [][]byte{{'a', 'b', 'c', 'd', 'e', 'f'}, {'g', 'h', 'i', 'j', 'k', 'l'}, {'m', 'n', 'o', 'p', 'q', 'r'}, {'s', 't', 'u', 'v', 'w', 'x'}, {'y', 'z', '0', '1', '2', '3'}, {'4', '5', '6', '7', '8', '9'}}\n  print(map6)\n}\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0f/0fa3602affd14f6d396706c646d8d4e1.png\" /></p><p></p><h1>rust完整代码如下：</h1><p></p><p><code lang=\"rust\">fn print(m: &amp;[Vec]) {\n    let n = m.len() as i32;\n    let mut a = (n - 1) / 2;\n    let mut b = (n - 1) / 2;\n    let mut c = n / 2;\n    let mut d = n / 2;\n    while a &gt;= 0 {\n        loop2(&amp;m, a, b, c, d);\n        a -= 1;\n        b -= 1;\n        c += 1;\n        d += 1;\n    }\n    println!();\n}\n\nfn loop2(m: &amp;[Vec], a: i32, b: i32, c: i32, d: i32) {\n    if a == c {\n        print!(\"{} \", m[a as usize][b as usize]);\n    } else {\n        for row in a + 1..=c {\n            print!(\"{} \", m[row as usize][d as usize]);\n        }\n        for col in (b..=d - 1).rev() {\n            print!(\"{} \", m[c as usize][col as usize]);\n        }\n        for row in (a..=c - 1).rev() {\n            print!(\"{} \", m[row as usize][b as usize]);\n        }\n        for col in b + 1..=d {\n            print!(\"{} \", m[a as usize][col as usize]);\n        }\n    }\n}\n\nfn main() {\n    let map1: Vec&gt; = vec![vec!['a']];\n    print(&amp;map1);\n\n    let map2: Vec&gt; = vec![vec!['a', 'b'], vec!['c', 'd']];\n    print(&amp;map2);\n\n    let map3: Vec&gt; = vec![\n        vec!['a', 'b', 'c'],\n        vec!['d', 'e', 'f'],\n        vec!['g', 'h', 'i'],\n    ];\n    print(&amp;map3);\n\n    let map4: Vec&gt; = vec![\n        vec!['a', 'b', 'c', 'd'],\n        vec!['e', 'f', 'g', 'h'],\n        vec!['i', 'j', 'k', 'l'],\n        vec!['m', 'n', 'o', 'p'],\n    ];\n    print(&amp;map4);\n\n    let map5: Vec&gt; = vec![\n        vec!['a', 'b', 'c', 'd', 'e'],\n        vec!['f', 'g', 'h', 'i', 'j'],\n        vec!['k', 'l', 'm', 'n', 'o'],\n        vec!['p', 'q', 'r', 's', 't'],\n        vec!['u', 'v', 'w', 'x', 'y'],\n    ];\n    print(&amp;map5);\n\n    let map6: Vec&gt; = vec![\n        vec!['a', 'b', 'c', 'd', 'e', 'f'],\n        vec!['g', 'h', 'i', 'j', 'k', 'l'],\n        vec!['m', 'n', 'o', 'p', 'q', 'r'],\n        vec!['s', 't', 'u', 'v', 'w', 'x'],\n        vec!['y', 'z', '0', '1', '2', '3'],\n        vec!['4', '5', '6', '7', '8', '9'],\n    ];\n    print(&amp;map6);\n}\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bbf9b024dc29593bfed48b633034536e.png\" /></p><p></p><h1>c++完整代码如下：</h1><p></p><p><code lang=\"cpp\">#include\n#include\n\nusing namespace std;\n\nvoid loop(vector&gt; m, int a, int b, int c, int d);\n\nvoid print(vector&gt; m) {\n    int n = m.size();\n    for (int a = (n - 1) / 2, b = (n - 1) / 2, c = n / 2, d = n / 2; a &gt;= 0; a--, b--, c++, d++) {\n        loop(m, a, b, c, d);\n    }\n    cout &lt;&lt; endl;\n}\n\nvoid loop(vector&gt; m, int a, int b, int c, int d) {\n    if (a == c) {\n        cout &lt;&lt; m[a][b] &lt;&lt; \" \";\n    }\n    else {\n        for (int row = a + 1; row &lt;= c; row++) {\n            cout &lt;&lt; m[row][d] &lt;&lt; \" \";\n        }\n        for (int col = d - 1; col &gt;= b; col--) {\n            cout &lt;&lt; m[c][col] &lt;&lt; \" \";\n        }\n        for (int row = c - 1; row &gt;= a; row--) {\n            cout &lt;&lt; m[row][b] &lt;&lt; \" \";\n        }\n        for (int col = b + 1; col &lt;= d; col++) {\n            cout &lt;&lt; m[a][col] &lt;&lt; \" \";\n        }\n    }\n}\n\nint main() {\n    vector&gt; map1 = { {'a'} };\n    print(map1);\n\n    vector&gt; map2 = { {'a', 'b'}, {'c', 'd'} };\n    print(map2);\n\n    vector&gt; map3 = { {'a', 'b', 'c'}, {'d', 'e', 'f'}, {'g', 'h', 'i'} };\n    print(map3);\n\n    vector&gt; map4 = { {'a', 'b', 'c', 'd'}, {'e', 'f', 'g', 'h'}, {'i', 'j', 'k', 'l'}, {'m', 'n', 'o', 'p'} };\n    print(map4);\n\n    vector&gt; map5 = { {'a', 'b', 'c', 'd', 'e'},\n                                 {'f', 'g', 'h', 'i', 'j'},\n                                 {'k', 'l', 'm', 'n', 'o'},\n                                 {'p', 'q', 'r', 's', 't'},\n                                 {'u', 'v', 'w', 'x', 'y'} };\n    print(map5);\n\n    vector&gt; map6 = { {'a', 'b', 'c', 'd', 'e', 'f'},\n                                 {'g', 'h', 'i', 'j', 'k', 'l'},\n                                 {'m', 'n', 'o', 'p', 'q', 'r'},\n                                 {'s', 't', 'u', 'v', 'w', 'x'},\n                                 {'y', 'z', '0', '1', '2', '3'},\n                                 {'4', '5', '6', '7', '8', '9'} };\n    print(map6);\n\n    return 0;\n}\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/62/62f820532e31f163fbd3700c9e620359.png\" /></p><p></p><h1>c完整代码如下：</h1><p></p><p><code lang=\"c\">#include \n\n\nvoid loop(char** m, int a, int b, int c, int d);\n\nvoid print(char** m, int n) {\n    for (int a = (n - 1) / 2, b = (n - 1) / 2, c = n / 2, d = n / 2; a &gt;= 0; a--, b--, c++, d++) {\n        loop(m, a, b, c, d);\n    }\n    printf(\"\\n\");\n}\n\nvoid loop(char** m, int a, int b, int c, int d) {\n    if (a == c) {\n        printf(\"%c \", m[a][b]);\n    }\n    else {\n        for (int row = a + 1; row &lt;= c; row++) {\n            printf(\"%c \", m[row][d]);\n        }\n        for (int col = d - 1; col &gt;= b; col--) {\n            printf(\"%c \", m[c][col]);\n        }\n        for (int row = c - 1; row &gt;= a; row--) {\n            printf(\"%c \", m[row][b]);\n        }\n        for (int col = b + 1; col &lt;= d; col++) {\n            printf(\"%c \", m[a][col]);\n        }\n    }\n}\n\nint main() {\n    char* map1[] = { \"a\" };\n    int n1 = sizeof(map1) / sizeof(char*);\n    print(map1, n1);\n\n    char* map2[] = { \"ab\", \"cd\" };\n    int n2 = sizeof(map2) / sizeof(char*);\n    print(map2, n2);\n\n    char* map3[] = { \"abc\", \"def\", \"ghi\" };\n    int n3 = sizeof(map3) / sizeof(char*);\n    print(map3, n3);\n\n    char* map4[] = { \"abcd\", \"efgh\", \"ijkl\", \"mnop\" };\n    int n4 = sizeof(map4) / sizeof(char*);\n    print(map4, n4);\n\n    char* map5[] = { \"abcde\", \"fghij\", \"klmno\", \"pqrst\", \"uvwxy\" };\n    int n5 = sizeof(map5) / sizeof(char*);\n    print(map5, n5);\n\n    char* map6[] = { \"abcdef\", \"ghijkl\", \"mnopqr\", \"stuvwx\", \"yz0123\", \"456789\" };\n    int n6 = sizeof(map6) / sizeof(char*);\n    print(map6, n6);\n\n    return 0;\n}\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/512ae91bae89a2df4dc8401dce6b8bc5.png\" /></p><p></p>",
    "publish_time": "2023-09-20 10:56:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI放大招“对打”谷歌Gemini：全力筹备多模态大模型，并发布新指令语言模型",
    "url": "https://www.infoq.cn/article/g4L1Y5SPGGUiX1BB4l2t",
    "summary": "<p></p><blockquote>这场大模型时代的较量，谁能笑到最后？</blockquote><p></p><p>&nbsp;</p><p>自去年年底通过 ChatGPT 惊艳全球以来，OpenAI 一直保持着惊人的产品发布速度，通过迅如闪电的“组合拳”保持该公司在 AI 领域建立的统治地位与领导者形象。</p><p>&nbsp;</p><p>但其他科技巨头绝不可能坐视 OpenAI 一家独大。谷歌已经公布大语言模型 Gemini，计划于今年秋季首次与广大用户见面，且有报道称该模型已经在接受指定企业客户的测试。从目前的情况看，谷歌有可能后来居上、实现反超。</p><p>&nbsp;</p><p>面对挑战，OpenAI 连续放大招，除了发布新指令语言模型 GPT-3.5-turbo-instruct，还计划推出多模态大模型 GPT-Vision 与 Gobi。据一位未公开身份的知情人士透露，OpenAI 在积极将多模态功能（类似于 Gemini 将要提供的功能）纳入 GPT-4。</p><p></p><h2>新语言模型InstructGPT-3.5</h2><p></p><p>&nbsp;</p><p>近日，OpenAI 推出 GPT-3.5-turbo-instruct，这是一款新的指令语言模型，效率可以与聊天优化的 GPT-3.5 Turbo 模型相媲美。</p><p>&nbsp;</p><p>指令模型属于大语言模型的一种，会在使用一大量数据进行预训练之后，再通过人类反馈（RLHF）做进一步完善。在此过程中，会由人类负责评估模型根据用户提示词生成的输出，对结果做改进以达成目标效果，再将更新后的素材用于进一步训练。</p><p>&nbsp;</p><p>因此，指令模型能够更好地理解并响应人类的查询预期，减少错误并缓解有害内容的传播。从OpenAI 的测试结果来看，尽管体量仅为后者的百分之一，但人们明显更喜欢拥有 13 亿参数的 InstructGPT 模型，而非拥有 1750 亿参数的 GPT 模型。</p><p>&nbsp;</p><p>据了解，GPT-3.5-turbo-instruct 的成本与性能同其他具有 4K 上下文窗口的 GPT-3.5 模型相同，使用的训练数据截止于 2021 年 9 月。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/80/809370ea78382a5c5d269aafb39437aa.png\" /></p><p></p><p>GPT-3.5-turbo-instruct 将取代一系列现有 Instruct 模型，外加 text-ada-001、text-babbage-001 和 text-curie-001。这三款 text-davinci 模型将于 2024 年 1 月 4 日正式停用。</p><p>&nbsp;</p><p>OpenAI 表示，GPT-3.5-turbo-instruct 的训练方式与之前的其他 Instruct 模型类似。该公司并未提供新 Instruct 模型的细节或基准，而是参考了 2022 年 1 月发布的 InstructGPT，即 GPT-3.5 模型的实现基础。</p><p>&nbsp;</p><p>OpenAI 称，GPT-4 拥有超越 GPT-3.5 的复杂指令遵循能力，生成的结果也比 GPT-3.5 质量更高；但 GPT-3.5 也有自己的独特优势，例如速度更快且运行成本更低。GPT-3.5-turbo-instruct 并非聊天模型，这一点与原始 GPT-3.5 有所区别。具体来讲，与之前的聊天应用模型不同，GPT-3.5-turbo-instruct 主要针对直接问答或文本补全进行优化。</p><p>&nbsp;</p><p>速度方面，OpenAI 称 GPT-3.5-turbo-instruct 速度与 GPT-3.5-turbo 基本相当。</p><p>&nbsp;</p><p>下图为 OpenAI 设计的 Instruct 指令模型与 Chat 聊天模型之间的区别。这种固有差异自然会对提示词的具体编写产生影响。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11e71f7bd469918f32333d59db603e8b.webp\" /></p><p></p><p>OpenAI 负责开发者关系的Logan Kilpatrick称，这套新的指令模型属于向 GPT-3.5-turbo 迁移当中的过渡性产物。他表示其并不属于“长期解决方案”。已经在使用微调模型的用户，需要根据新的模型版本做重新微调。目前微调功能只适用于 GPT-3.5，GPT-4 的微调选项计划于今年晚些时候发布。</p><p></p><h2>多模态大模型GPT-Vision与Gobi</h2><p></p><p>&nbsp;</p><p>除了 GPT-3.5-turbo-instruct，OpenAI 近日还计划发布多模态大模型 GPT-Vision，以及一个代号为“Gobi”的更强大的多模态大模型。</p><p>&nbsp;</p><p>据悉，GPT-Vision 在 3 月份的 GPT-4 发布期间首次预览，是 OpenAI 融合文本和视觉领域的雄心勃勃的尝试。虽然该功能最初实际用例仅限于 Be My Eyes 公司，这家公司通过其移动应用帮助视力障碍或失明用户进行日常活动。</p><p>&nbsp;</p><p>GPT-Vision 有潜力重新定义创意内容生成的界限。想象一下使用简单的文本提示生成独特的艺术品、徽标或模因。或者考虑一下对有视觉障碍的用户的好处，他们可以通过自然语言查询与视觉内容交互并理解视觉内容。该技术还有望彻底改变视觉学习和教育，使用户能够通过视觉示例学习新概念。</p><p>&nbsp;</p><p>如今，OpenAI 正准备将这项名为 GPT-Vision 的功能开放给更广泛的市场受众。</p><p>&nbsp;</p><p>此外，据 The Information 报道，OpenAI 即将发布代号为“Gobi”的下一代多模态大语言模型，希望借此击败谷歌并继续保持市场领先地位。目前，Gobi 的训练还没有开始，有评论认为其有机会成为 GPT-5。</p><p>&nbsp;</p><p>报道称，OpenAI 之所以耗费大量时间来推出 Gobi，主要是担心新的视觉功能会被坏人利用，例如通过自动解决验证码来冒充人类，或者通过人脸识别追踪人们。但现在，OpenAI 的工程师们似乎想到办法来缓解这个安全问题了。</p><p></p><h2>OpenAI CEO：GPT-5尚未出现，计划将多模态功能纳入GPT-4</h2><p></p><p>&nbsp;</p><p>据了解，多模态大语言模型的本质是一种先进 AI 系统，能够理解和处理多种数据形式，包括文本和图像。与主要处理文本内容的传统语言模型不同，多模态大语言模型能够同时对文本加视觉类内容进行分析和生成。</p><p>&nbsp;</p><p>也就是说，这类模型可以解释图像、理解上下文并生成包含文本和视觉输入的响应结果。多模态大模型还拥有极高的通用性，适用于从自然语言理解到图像解释的诸多应用，借此提供更广泛的信息处理能力。</p><p>&nbsp;</p><p>报道指出，“这些模型能够处理图像和文本，例如通过查看用户绘制的网站外观草图来生成网站构建代码，或者根据文本分析结果输出可视化图表。如此一来，普通用户也能快速理解内容含义，不必再向拥有技术背景的工程师们求助。”</p><p>&nbsp;</p><p>OpenAI 首席执行官 Sam Altman 在最近的采访中表示，尽管 GPT-5 尚未出现，但他们正计划对 GPT-4 进行各种增强。而开放多模态支持功能，也许就是这项计划的一部分。</p><p>&nbsp;</p><p>在上周接受《连线》采访时，谷歌 CEO 桑达尔·皮查伊表达了他对于谷歌当前 AI 江湖地位的信心，强调其仍掌握着技术领先优势、并在创新与责任方面求取平衡的审慎战略。他也对 OpenAI&nbsp;ChatGPT 的深远意义表示认可，称赞其拥有良好的产品-市场契合度、让用户对 AI 技术做好了准备。但他同时强调，谷歌在产品信任和负责态度方面会采取更加谨慎的立场。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://the-decoder.com/openai-releases-new-language-model-instructgpt-3-5/\">https://the-decoder.com/openai-releases-new-language-model-instructgpt-3-5/</a>\"</p><p><a href=\"https://www.theinformation.com/articles/openai-hustles-to-beat-google-to-launch-multimodal-llm\">https://www.theinformation.com/articles/openai-hustles-to-beat-google-to-launch-multimodal-llm</a>\"</p><p><a href=\"https://aibeat.co/openai-multimodal-llm-gpt-vision-google/\">https://aibeat.co/openai-multimodal-llm-gpt-vision-google/</a>\"</p>",
    "publish_time": "2023-09-20 14:56:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "五个很实用的IDEA使用技巧",
    "url": "https://www.infoq.cn/article/5499702f00f8f4eafbbe30897",
    "summary": "<p>日常开发中，相信广大 Java 开发者都使用过 IntelliJ IDEA 作为开发工具，IntelliJ IDEA 是一款优秀的 Java 集成开发环境，它提供了许多强大的功能和快捷键，可以帮助开发者提高编码效率和质量。除了一些常见的技巧，如自动导包、智能补全、重构工具等。IntelliJ IDEA 还有一些不为人知的技巧，可能很多开发者都没有注意到或者使用过。本文就在为你介绍博主常用的五个 IntelliJ IDEA 使用技巧，希望能够给你带来一些工作效率上的提升。本文内容大纲如下：</p><p>注释对齐快速创建包目录/文件夹快付复制类代码生成 Java 类多实例启动分支比较</p><p></p><blockquote>注意：本文的示例截图以及操作演示都是基于 IntelliJ IDEA 2023.2 版本而来。过低版本可能不尽相同，大家注意。</blockquote><p></p><p></p><h1>注释对齐</h1><p></p><p>日常开发中，相信大家在添加注释时都知道使用快捷键 ctrl + / ，不过大家可能会遇见下面这种情况，</p><p><code lang=\"null\">&nbsp;&nbsp;&nbsp;&nbsp;public&nbsp;DiamondJumpContext(List&nbsp;diamondJumpTypes)&nbsp;{\n//添加注释\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(DiamondJumpType&nbsp;diamondJumpType&nbsp;:&nbsp;diamondJumpTypes)&nbsp;{\n//&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;map.put(diamondJumpType.getType(),&nbsp;diamondJumpType);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;&nbsp;&nbsp;}\n</code></p><p>可以看到，使用快捷键 ctrl + / 添加注释代码或者注释已有代码时，默认是在首行位置，无法与下方 for 循环代码自动对齐，造成了视觉上的不美观，那如何解决这个问题嘞？其实解决方法就藏在 IDEA 里，我来告诉大家如何设置。</p><p>打开 IDEA 设置 Editor -&gt; Code Style -&gt; Java -&gt; Code Generation，找到下方的 Comment code 区域， 取消 Line comment at first column 的默认勾选，将 Add a space at line comment start 以及 Enforce on reformat 打上勾就可以了。如下图所示，</p><p><img src=\"https://static001.geekbang.org/infoq/0d/0dd3bfda16836213eb7e20ef04bf193b.png\" /></p><p>然后我们重新给上面的示例代码打赏注释时，效果就是下面这样了，</p><p><code lang=\"null\">&nbsp;&nbsp;&nbsp;&nbsp;public&nbsp;DiamondJumpContext(List&nbsp;diamondJumpTypes)&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;添加注释\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(DiamondJumpType&nbsp;diamondJumpType&nbsp;:&nbsp;diamondJumpTypes)&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;map.put(diamondJumpType.getType(),&nbsp;diamondJumpType);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;&nbsp;&nbsp;}\n</code></p><p>大家可以很明显的看到，此时添加的注释默认是跟当前代码对齐，并且自动留出一个空格，代码的可阅读性有了很大提升。</p><p></p><h1>快速创建包目录/文件夹</h1><p></p><p></p><h2>创建包目录</h2><p></p><p>日常开发时，当大家想新建 Java 类时，考虑到项目的包目录划分可能需要新建一个包目录，那么大家是如何操作的嘞？是先创建好包目录后，在创建 Java 类吗？其实在 IDEA 中创建 Java 类时，是可以直接顺带创建包目录的，具体操作如下。</p><p>在创建 Java 类时，直接写出包目录名称通过 . 拼接即可，如下图新建 Java 类时填上 controller.TestController，</p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef3530ee5941224deb7f46af30998f3a.png\" /></p><p>效果如下，</p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0c403f98a60aa45077e189e1663df3f.png\" /></p><p>可以看到，包目录自动创建成功。</p><p></p><h2>创建文件夹</h2><p></p><p>快速创建文件夹的方式跟快速创建包目录差不多，只不过将 . 拼接改为 / 即可，如下图新建文件时填上 template/test.csv，</p><p><img src=\"https://static001.geekbang.org/infoq/d8/d86dab088d6c7ff3768bb09373a2fdf1.png\" /></p><p>效果如下，</p><p><img src=\"https://static001.geekbang.org/infoq/0b/0be8c4790fa0cec6538f30072364d535.png\" /></p><p>可以看到，文件夹自动创建成功。</p><p></p><h1>快付复制类代码生成 Java 类</h1><p></p><p>日常中有个场景，博主经常在上网浏览网上的技术资料，看到感兴趣的文章时，想要复制文章中的 demo 代码在本地跑一遍验证一下。于是就有了这个快付复制代码生成 Java 类的需求。起初博主老老实实通过常规操作在本地手动创建 Java 类，然后拷贝 demo 代码进行验证。后来了解到其实在 IDEA 中是有快付复制类代码生成 Java 类的功能，这里分享给大家。</p><p>假如当前我们想要快速复制下方的 Java 类代码到 IDEA 项目中，那改如何操作嘞？</p><p><code lang=\"null\">/**\n&nbsp;*&nbsp;金刚位跳转策略配置\n&nbsp;*/\n@Component\npublic&nbsp;class&nbsp;DiamondJumpContext&nbsp;{\n\n&nbsp;&nbsp;&nbsp;&nbsp;private&nbsp;final&nbsp;Map&nbsp;map&nbsp;=&nbsp;new&nbsp;HashMap&lt;&gt;();\n\n&nbsp;&nbsp;&nbsp;&nbsp;/**\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;由spring自动注入DiamondJumpType子类\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;@param&nbsp;diamondJumpTypes&nbsp;金刚位跳转类型集合\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/\n&nbsp;&nbsp;&nbsp;&nbsp;public&nbsp;DiamondJumpContext(List&nbsp;diamondJumpTypes)&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(DiamondJumpType&nbsp;diamondJumpType&nbsp;:&nbsp;diamondJumpTypes)&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;map.put(diamondJumpType.getType(),&nbsp;diamondJumpType);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;&nbsp;&nbsp;}\n\n&nbsp;&nbsp;&nbsp;&nbsp;public&nbsp;DiamondJumpType&nbsp;getInstance(Integer&nbsp;jumpType)&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;map.get(jumpType);\n&nbsp;&nbsp;&nbsp;&nbsp;}\n}\n</code></p><p>其实操作是很简单的，我们可以通过全选复制上述 Java 类代码，然后打开 IDEA，鼠标选中指定的包目录后通过 ctrl + v 粘连即可。</p><p><img src=\"https://static001.geekbang.org/infoq/bf/bf3b52f0bc22d574e331f586334cc401.png\" /></p><p>效果如下，</p><p><img src=\"https://static001.geekbang.org/infoq/83/83cf9d1b72334c9d5be0797f41e7f8e5.png\" /></p><p>可以看到，我们已经将 Java 类代码快速复制到本地文件中。</p><p></p><h1>多实例启动</h1><p></p><p>日常本地开发微服务项目时，博主想要验证一下网关的负载均衡以及感知服务上下线能力时，需要用到多实例启动。</p><p>那么什么是多实例启动嘞？简单说就是能在本地同时启动多个同一服务。打个比方项目中有一个 MobileApplication 服务，我现在想要同时启动两个 MobileApplication 服务，通过不同端口区分，该如何操作嘞？IDEA 中其实是支持这个操作的。我将给大家介绍如何启用多实例功能。</p><p>打开服务配置，点击 Modify options 选项，</p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9196f4115055df7d0ff48615cf97cd7.png\" /></p><p>勾选打开 Operation System -&gt; Alow multiple instance ，继续勾选打开 Java -&gt; Program arguments，然后在 Program arguments 输入框中指定端口，在 Spring 项目中通过 --server.port=9999，如下图（注意每次启动实例时，这里的端口不能相同），</p><p><img src=\"https://static001.geekbang.org/infoq/fb/fbdacfe8a2e4dba8f1e8d85079fa680f.png\" /></p><p>效果图如下，</p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d9ab4b73519e2ecc8f7c376e2ba6ec0.png\" /></p><p>可以看到，这里多实例启动的功能就 OK 了，在 Modify options 选项中还有许多常用功能，对于日常开发都是很多帮助的，比如 Java -&gt; add VM options 可以用来添加 jvm 启动参数等。</p><p></p><h1>分支比较</h1><p></p><p>日常开发中，当项目上线时，博主经常会使用这个功能，用于比较当前新功能分支与 master 分支的代码差异。</p><p>假如新功能分支上线前经历了多个人员参与迭代以及冲突修复，作为项目主程的你需要在项目上线前进行代码 review，以确保代码质量，那该如何操作嘞？ 其实 IDEA 已经贴心的为我们做好了一切，这里我将给大家介绍远程分支比较功能。</p><p>右键项目根目录，找到 Git -&gt; Compare with Branch 选项，点击打开，</p><p><img src=\"https://static001.geekbang.org/infoq/32/32a13e41b18e9fbbf801cabd12b8bc02.png\" /></p><p>此时 IDEA 会显示项目的所有分支列表，我们从中找到 master 分支，点击即可，</p><p><img src=\"https://static001.geekbang.org/infoq/9b/9bb09d0aafc6a3e15591f70cb15172f4.png\" /></p><p>最后 IDEA 会显示所有的文件差异在项目左侧栏目，我们点击某个文件即可查看某个具体差异。</p><p><img src=\"https://static001.geekbang.org/infoq/9e/9e2f270d92d2a287949a89339a7990c0.png\" /></p><p>效果如下，</p><p><img src=\"https://static001.geekbang.org/infoq/a9/a99be26bdcd62fc731a44810d034d5e7.png\" /></p><p>可以看到，通过 IDEA 提供的分支比较功能，为我们进行代码 review，提供了非常便利的操作。</p><p></p><h1>总结</h1><p></p><p>其实本文所讲解的五个实用技巧不光在 IDEA 里可以使用，在 PhpStorm、PyCharm 中都是通用的，欢迎大家在浏览完本文后实践体验下。最后希望本文在日常工作中能够帮助到大家，感谢阅读。</p><p></p><blockquote>关注公众号【waynblog】每周分享技术干货、开源项目、实战经验、国外优质文章翻译等，您的关注将是我的更新动力！</blockquote><p></p>",
    "publish_time": "2023-09-20 16:13:41",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "李宁集团：用数字化贯穿 59 个产品研发关键节点",
    "url": "https://www.infoq.cn/article/b42NKDlBrsSg6HicEBKo",
    "summary": "<p>随着行业红利期的结束，在当下的数字化时代，企业要想持续茁壮成长，必须以更为精细和高效的方式经营。</p>\n<p>本期《行知数字中国》走进李宁，我们将看到，李宁如何运用数字技术提高门店运营和库存管理的效率、改善产品研发的生命周期，每一双鞋诞生的背后都有哪些科技力量？</p>\n<p>相关文章延展阅读：<a href=\"https://www.infoq.cn/article/GcdCeRTEHlT9tUYDN55A\">《线上线下一盘棋，品牌经销一盘棋 | 李宁公司的数字化转型实践》</a><br />\n如果您对《行知数字中国》栏目或者更多企业数字化转型案例感兴趣，欢迎关注「<strong>InfoQ 数字化经纬</strong>」公众号，我们将持续为您推送更多、更优质的数字化案例内容和线上线下活动。</p>",
    "publish_time": "2023-09-20 17:02:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "线上线下一盘棋，品牌经销一盘棋 | 李宁公司的数字化转型实践",
    "url": "https://www.infoq.cn/article/GcdCeRTEHlT9tUYDN55A",
    "summary": "<p>当谈及中国体育品牌的崛起时，李宁无疑是一个备受瞩目的名字。在三十多年的发展过程中，李宁曾经在国内外市场创造过数次高光时刻，但也经历了多次挑战和低谷时刻。而这些时刻其实也是转型的契机，是重新思考、再出发的机会。</p><p></p><p>比如早在 2012 年，李宁就展开了一场<a href=\"https://www.infoq.cn/article/cS1UEOxZRAZFVDkx8dkH\">零售</a>\"革命，将注意力从传统批发模式转向终端消费者的需求和体验。进入数字时代，李宁更是积极主动地转型，它不仅要做一个体育品牌，更把目标放在“源自中国并被世界认可的，具有时尚性的专业运动品牌”。数字化转型成为这一变革的驱动力之一。</p><p></p><p><a href=\"https://www.infoq.cn/video/b42NKDlBrsSg6HicEBKo\">本期《行知数字中国》走进李宁</a>\"，我们将看到，李宁如何运用数字技术，不仅提高了门店运营和库存管理的效率，还改善了产品研发的生命周期。我们也将深入了解到，李宁数字化门店是如何将线上和线下融为一体，实现对消费者的精准画像和个性化营销。</p><p></p><h2>数字化门店，支撑零售运营的重要“桥梁”</h2><p></p><p></p><p><a href=\"https://www.infoq.cn/article/efSUzgZjSP2opNfwgQfl\">数字化门店</a>\"在李宁的数字化转型中扮演了连接线上与线下、实现精细化管理的重要桥梁作用。</p><p></p><p>发展到今天，李宁的数字化门店逐渐实现了线上与线下全域会员数据的深度整合，以便准确勾画消费者群体的画像，从而实现个性化的精准营销。通过门店数字化技术，记录消费者整体逛店热力动线轨迹，通过数据分析更合理优化门店陈列布局，提高全店商品透出频次，增加门店销售机会。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/ea/a8/eac1a50a938ab53183df062465e78da8.jpg\" /></p><p></p><p>据统计，目前李宁全渠道数字化门店超过 1300 家，通过全渠道运营带来额外约 5% 增长，会员数量增长达到 1000 万。</p><p></p><h4>门店运营：人货场 + 运营效率</h4><p></p><p></p><p>“数字化门店并不是表面上装许多设备去让客户体验，其实是我们在尝试感知客户需求，再反过来服务于门店运营的方方面面。”对于数字化门店的布局，李宁集团信息技术系统总经理李飞认为，重点不在于“秀科技肌肉”，而是让“门店运营”这件事做到更极致。</p><p></p><p>人、货、场本是零售行业中众所周知的三要素，但在李飞看来，数字化门店的精细化运营还需要加上一个关键因素——运营效率，可以理解为“门店的盈利能力”。</p><p></p><p>具体而言，数字化门店首先要将运营系统化，实现“人、货、场”的标准化。通过将员工的任务和流程固化在系统中，确保门店的员工清晰地知道每天需要做什么以及怎么做，比如货物陈列、POP（广告陈设）、库存、私域运营等等，这些日常标准动作都可以在李宁内部的零售运营平台移动端查看。</p><p></p><p>而门店的盈利能力则需要数据分析支持零售运营来提供一定支持。一方面，零售运营平台会给门店店长、城市经理和区域经理提供门店盈利能力的相关分析数据，包括关键性的零售指标和数据（连带率、成交率等）。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/fc/3b/fc2f0379af849aa01ec2453bd40eb03b.jpg\" /></p><p></p><p>另一方面，从门店的整体管理维度看，李宁数字化门店的整体运营数据（流水达成、单店成长、库存等等）也会被进一步收集和汇总到“城市零售大脑”，相关管理人员能够及时了解区域门店整体的零售运营情况，随时发现问题并进行改进。</p><p></p><p>这一系列能力的根基在于李宁数字化团队运用云计算、大数据、人工智能等前沿技术，根据公司的业务特性，量身定制并开发的一套智能系统。</p><p></p><p>李飞进一步举例道，系统内有一个重要的功能叫做“门店零售归因”，是一个用于评估门店经营状况的诊断工具。对于有经验的管理者，他们可以从自己的经验和数据中判断门店的问题。为了让更多的人能够具备这样的分析判断能力，“门店零售归因”还可以将经验沉淀下来，帮助每个门店和城市经理从多个维度分析门店的经营表现，找出问题的原因以及改进的方法。</p><p></p><p>“我们把这叫‘有意志的生意’，它记录了一些经营思维、销售方法和标准，这些对于管理者的要求都在这个系统中得以清晰勾画，可以实现经验共享，让大家从中借鉴。”李飞表示，系统还能下发任务给门店，指导门店采取措施去改进经营。</p><p></p><p>综上所述，李宁数字化门店不仅是为实现“人、货、场”的标准化，提高门店快速布置的效率。更进一步地，其通过多个系统和工具，用以支持门店的运营管理、数据分析和改进，并加强管理决策的效率和可靠性。</p><p></p><p></p><h4>线上线下一盘棋，品牌经销一盘棋</h4><p></p><p></p><p>此外，李宁数字化门店的能力并不局限于直营门店，还向经销商门店开放。</p><p></p><p>李宁早在 2015 年就开展线上和线下的全渠道运营，在这个过程中，李飞直言公司所面临最大的挑战主要有两方面，分别是“线上线下一盘棋”以及“品牌经销一盘棋”。“为什么说棋呢？棋要考虑战略布局、战术布局。”李飞解释称，“做全渠道不是说发了一个新渠道，给你一盘货自己去卖，而是如何让这一盘货在这些渠道发挥加起来的效益能更高。”</p><p></p><p>也就是说，如果仅仅从 IT 的维度看，只要“打通系统”就可以。但是，从业务的角度去考虑，就要想明白怎么样打通才是有效的。</p><p></p><p>李飞还提到，线上线下主要是内部逐步拉通的过程，而品牌与经销商的合作与拉通，带来的挑战会更复杂。李宁与数千家经销商打通系统的工程量特别大，也非常耗时，经销商有自己的系统，一些综合性的批发商也用不一样的系统，这其中涉及到库存、商品、业务、单据、财务结算、绩效评估等维度的打通。</p><p></p><p>因此，李飞认为全渠道运营很重要的一点是全面理清业务流程，必须要理解不同部门之间的关系和业务流向，再形成系统化的业务逻辑。如果只解决部分业务问题而不考虑整体，容易导致全渠道运营的过程中出现问题，比如数据流或订单流的中断。</p><p></p><p>目前，李宁已经打通了品牌与几千家线下经销商门店之间的数据竖井，使得经销商门店也可以受益于这些数据和分析支持，从而提高业务效率和运营效率。</p><p></p><p></p><h4>仓储物流数字化</h4><p></p><p></p><p>数字化门店高效运营的背后亦离不开仓储物流的快速响应。近些年李宁利用数字化技术来优化门店商品的供应链管理，包括商品的设计、生产、运输、库存管理、以及送货到门店的流程。</p><p></p><p>近两年，李宁进行了物流仓储的调整，要将以前分散的地区仓合并成五个大的中心仓，并且这些大仓都做了数字化改造，实现自动化管理。</p><p></p><p>其次，李宁落地了智能排单能力，从仓库到门店的送货计划是系统根据多种因素计算而来，这需要一套复杂的系统来支持。</p><p></p><p>据李飞介绍，李宁使用智能物流系统来帮助计算并指导货物的送货路线。系统会将订单数据、发货优先级和货物到达时间等信息进行实时计算，然后将指令传达给 PLS 系统（生产物流作业管理系统），从而将货物发送到相应的门店。从工厂到门店的这一整个物流链路，比如出货，铺货，中间的调、补、配等等，都必须联动起来。</p><p></p><p>“我们的目标是最大限度地提高铺货和补货的效率，以确保门店始终能供应上所需的商品。”</p><p></p><p>2022 年起，李宁的物流作业已全面实现自动化，目前华东、华北、华中以及华南的物流仓均已上线零拣自动化方案，年内零拣综合效率提升 80% 以上，存储效率提升达100%。</p><p></p><p></p><h2>用数字化贯穿 59 个产品研发关键节点</h2><p></p><p></p><p>面对瞬息万变的消费市场，李宁还通过数字化手段来提高产品研发的效率，使得产品研发更加匹配市场的需求。</p><p></p><p>从创意原型、开发、材料、产品成型，成本核算到供应链下单的商品全生命周期中，李宁切分出了 59 个关键节点，并把这些节点进行系统性管理。</p><p></p><p>一款鞋从概念到订货，一般需要提前 18 个月去做规划。为加快速度，李宁在产品研发设计环节使用了 3D 技术进行设计建模，系统还能根据不同材料的选择，自动测算成本，此举不但能大幅提升研发效率，也极大地缩短了产品迭代的时间。</p><p></p><p>与此同时，针对一些“快反订单”，内部也可以通过 3D 建模和系统测算直接下单，快速响应市场需求的同时，也极大地降低了成本。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/6b/82/6bd5yyd3cb6f7d7c65992995e571ea82.jpg\" /></p><p></p><p></p><p>而为了更深入地了解用户的需求，李宁不仅需要依赖员工的业务专业知识，还需要充分利用数据分析。比如，设计师们常常通过深入市场分析和对运动的深刻理解来汲取创意灵感，其中许多设计师本身就是运动领域的专家。他们需要积极与专业运动员沟通，了解他们的需求，同时也走访消费者，以收集市场反馈。</p><p></p><p>但除了紧密关注市场趋势和消费者反馈，相关研发系统还会基于历史数据、市场趋势和消费者偏好等信息为产品迭代、销售目标等提供研判和预测。</p><p></p><p></p><h2>数字化要注意“新旧”的稳步交替，价值要从长远看</h2><p></p><p></p><p>回溯李宁的数字化历程，在李飞看来，过程中所面临的最大挑战之一是如何平衡传统销售渠道和新兴数字化渠道的发展。也就是说，不仅需要保持传统销售渠道的稳定，还需要积极拓展新兴数字化渠道，如电商平台、社交媒体等，以满足消费者日益多元化的购物需求。</p><p></p><p>从技术的维度看，数字化的迭代也不是将所有旧有系统全部翻新，而是需要让新建系统和旧有底座能够相互关联，通过一个集中的交互平台将这些系统连接起来，以确保业务流程的连贯性和通畅性。</p><p></p><p>除此之外，李飞提到另一大挑战是如何对公司内部管理的数字化升级，包括管理流程、信息化系统、数据分析等方面。李宁需要加大投入来建设数字化基础设施，并培养数字化人才，以确保数字化转型的顺利进行。</p><p>但数字化建设通常需要很长时间，并且成果可能不会立即显现。</p><p></p><p>那么，在这个过程中如何平衡数字化投入产出比？李飞认为，核心是数字化团队要与业务部门达成一致，双方基于公司发展战略和重点任务确定投入和目标，再进行排兵布阵。</p><p></p><p>首先，对于投入产出比，李宁会设定明确的项目目标和 KPI，例如成本、效率、质量等，并在项目进行的过程中持续监测和追踪进展，以确保数字化项目能达到预期结果。但数字化投资并没有固定比例，也是基于业务战略和目标来决定，根据具体需求进行分配。</p><p></p><p>李飞表示，数字化的价值不仅包括一些量化的指标，还包括对公司战略、愿景和长远目标等无形的影响。</p><p></p><p></p><h2>下一阶段五大数字化重点</h2><p></p><p></p><p>“数字化这个事，我们‘永远在路上’。但是，也‘一切皆有可能’。”李飞感慨道。在下一阶段，李宁公司的数字化重点主要集中在以下几个关键领域：</p><p></p><p>第一，继续深耕数字化营销，通过数据分析和精准营销，提高品牌曝光度和销售转化率。</p><p></p><p>第二，持续推进数字化供应链建设，进一步优化成本、交付时间和质量，推进数字化供应链建设，提高供应链的响应速度和效率。</p><p></p><p>第三，强化数字化商品管理，计划利用机器学习和人工智能等技术，建立更多的模型，以帮助业务团队进行商品预测、分配和运营，实现自动化的商品管理，以更好地满足市场需求。</p><p></p><p>第四，在零售领域，特别是门店数字化，将持续更新以提供更多工具和支持，帮助店员和店长更好地开展业务和销售工作，从而促进业务增长。</p><p></p><p>此外，下一阶段李宁的相关底层能力也将持续加强，包括数据治理、数据服务和组织能力的提升，加强产品团队和研发团队之间的合作，确保业务对数字化需求有更好的理解，并将这些需求转化为产品。</p><p></p><p></p>",
    "publish_time": "2023-09-20 17:03:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "大模型时代，企业如何打造 AI 数据基础设施？",
    "url": "https://www.infoq.cn/article/oZkw8G2uXvv2oVNJ7XFm",
    "summary": "<p>9 月 19 日，AI 基础设施公司 Fabarta 在北京举办首届产品与用户大会，本次大会以“数联世界 智见未来”为主题，汇聚前沿技术与商业洞见，来自学术界、投资界、以及 Fabarta 的客户、合作伙伴、各行业大咖共同分享&nbsp; AI 赋能业务新范式的多种路径，探讨大模型时代企业如何打造 AI 数据基础设施，迈入真正的全面智能化时代。</p><p></p><h2>构建面向智能应用的数据基础设施</h2><p></p><p>&nbsp;</p><p>当前 AIGC 技术快速发展，企业面临数字化转型和智能化升级的历史机遇。Fabarta 创始人兼 CEO 高雪峰在“数联世界、智见未来”的主题演讲中指出，构建面向智能应用的数据基础设施，是助力企业实现决策智能化的关键所在。</p><p>&nbsp;</p><p>Fabarta 构筑了以数据为中心的 AIGC 落地架构范式，围绕着数据以及数据之间的关系（图和向量融合）所代表的企业数据，可以帮助大模型进行推理增强，减少模型的幻觉，实现数据的安全可控，并为其提供长效记忆能力。同时，也可以为其本地数据的 fine-tune 及预训练提供结构化的数据集。此外，作为本地知识库可以不断地进行知识的沉淀，提供可解释的智能，并为 AI Agents 打造核心的数据引擎。</p><p>&nbsp;</p><p>Fabarta 创始人兼 CEO 高雪峰表示，Fabarta 不止将数据和数据之间显性、确定的关系记录在数据引擎中，通过向量的距离计算，还可以找到数据之间的丰富的隐含关系，成为了真正 AI 时代可以自生长的有机数据记忆体。&nbsp;</p><p>&nbsp;</p><p>高雪峰强调，Fabarta 致力于打造的是这个时代创新的数据基础设施，希望 Fabarta 的产品，可以帮助万千企业打造核心的面向智能应用的数据基础设施，从而更好，更方便，更加放心地将包括生成式人工智能技术在内的智能技术应用到企业决策智能领域，最终带来业务价值的巨大飞跃。抓住机遇，以数据为核心，构建支持智能化的架构，将是产业变革和发展的关键路径。</p><p>&nbsp;</p><p>Fabarta 联合创始人兼 CTO&nbsp; 杨成虎在演讲中深入探讨了企业智能发展的新引擎——多模态智能引擎 ArcNeural。他强调，在强人工智能的背景下，企业知识数据与大型模型之间的互补性，企业私有数据的潜在价值，以及将私有知识与智能应用紧密结合的必要性。</p><p>&nbsp;</p><p>杨成虎还详细分享了 ArcNeural 存查算一体的数据管理能力，并结合大模型技术实现智能化、可追溯、可解释的数据洞察能力。ArcNeural 架构面向云原生，并同时支持图与向量的同表管理。此外，在数据计算分析能力基础之上，利用大模型的智能化生成多模态查询计划，以此支持严谨的企业级多模态数据洞察。&nbsp; 最后，他还现场展示了多模态引擎 ArcNeural 结合 ArcFabric、ArcPilot，利用一体两翼范式在 Fabarta 企业内实施的一个 Copilot 系统--Arc42，Arc42 集成了代码、文档、组织关系等多模态数据，协同大模型显著提升企业效能。他表示：“计算&amp;存储是过去，推理&amp;记忆才是未来”。</p><p>&nbsp;</p><p>Fabarta 联合创始人兼 CPO 仲光庆分享了产品矩阵的设计思路和出发点。Fabarta 经历了从 DB for AI 到大模型时代 AI 基础设施的演进过程。他表示：“当前大模型时代，企业面临从数据、模型到业务落地的全链路挑战，构建融合图数据库和向量数据库的新型数据基础设施，实现数据治理、知识管理和模型记忆，是应对大模型时代的关键。通过低代码平台加速场景化应用落地，企业更关注数据资产与业务价值的产出是智能化新范式，Fabarta 打造的‘一体两翼’产品矩阵，以多模态引擎为基础，数据与 AI 平台为双翼，为企业提供全链路支持，助力企业实现决策智能化”。</p><p></p><h2>AI 时代数据基础设施的学术研究与行业思考</h2><p></p><p>&nbsp;</p><p>会上，Fabarta 科学顾问，国家杰青和优青基金获得者，北京理工大学特聘教授、大数据研究院院长袁野做了“AI 时代数据基础设施新范式的研究与应用”的学术分享。</p><p>&nbsp;</p><p>袁教授围绕 AI 视角、数据视角和案例分析三个角度展开讲解，他认为，在 AI 视角，连接主义是模型实现基础，符号系统是高层能力特征，两种基础范式的结合是人工智能发展的必经之路，可以通过图知识增强大模型。 在数据视角，神经符号化的融合方式实现一体化数据管理。通过三个案例分析 —多模态图谱查询、视频数据分析和神经符号数据库，给出 AI 时代数据基础设施新范式的案例参考。</p><p>&nbsp;</p><p>随着 AI 时代的技术演进，数据基础设施也在持续更新和迭代中。中国信通院云计算与大数据研究所大数据与区块链部副主任马鹏玮指出，AI 和数据基础设施正在双向奔赴，AI 赋能数据基础设施升级，数据基础设施助力 AI 产业发展。</p><p>&nbsp;</p><p>马鹏玮认为，AI 赋能数据基础设施升级，通过数据存算+AI，诞生自治数据库；通过数据管理+AI，诞生智能数据管理工具；通过数据分析+AI ：诞生增强型数据分析工具。数据基础设施助力 AI 产业，通过隐私计算、防篡改数据库、全密态数据库和多模态数据编织实现“汇数”，通过向量数据库实现“炼模”，通过图、向量和大模型相结合的技术，实现“用模”。</p><p></p><h2>Fabarta如何助力企业实现数智化转型？</h2><p></p><p>&nbsp;</p><p>当前，Fabarta 已经通过“一体两翼”产品矩阵帮助金融、制造、医疗等多个行业客户解决业务中的问题，并且成功实施落地。</p><p>&nbsp;</p><p>泰康养老作为保险行业头部企业，利用 Fabarta 产品进行数据资产项目落地。 泰康养老数据基建部总经理吴坚平提到，金融保险行业数据资产管理工作中，数据量级爆炸性增长、数据类型多种多样、数据质量需求、数据合规性要求严，数字化建设过程中对于数据资产工具的需求越来越高，亟需智能化工具和产品可以解决以上痛点和问题。</p><p>&nbsp;</p><p>通过 Fabarta ArcFabric 产品以及图和 AI 的能力，泰康养老已经构建从数据到资产、从资产到服务的全链路。在数据资源纳管阶段，实现元数据主动采集，利用 ArcGraph 进行血缘分析；利用 AI 技术实现元数据智能补齐、资产智能分类，构建数据资产大图；结合资产运营、资产评估，通过对话式数据看板等方式提供一站式数据资产服务。泰康养老已实现数据资产管理、数据治理与数据研发的协同以及治研一体的管理落地，未来会与 Fabarta 进行数据编织领域更深入的探索和落地。</p><p>&nbsp;</p><p>大树科技是专注于智能企服及供应链数字技术的高新技术企业，利用产业链场景数据丰富小微客户画像，通过智能风控手段对客户实现精准的风险分层，实现风险可控的同时为特定产业内的上下游小微客户提供便利的普惠金融服务。大树科技风控和大数据负责人申宇峰带来“图解复杂，洞察价值 图智能助力业务创新”的分享。申宇峰指出，图数据库和图算法的应用已成为金融风控的新趋势，可以通过多跳复杂关系发现隐藏在孤立统计信息背后的关系和价值。</p><p>&nbsp;</p><p>申宇峰表示，大树科技利用 ArcGraph 和 ArcPilot 产品，结合海量企业数据和交易数据，构建企业图谱和交易图谱，开展企业图谱业务分析、交易流水分析以及团伙欺诈分析，及时发现关联风险从而有效避免损失。在落地实践中，可以利用 ArcPilot 的图计算能力、灵活可配置画布功能以及行业模版提升图应用开发速度，沉淀图分析场景数据资产。大树科技期待与 Fabarta 进行更多合作和共创，从图智能扩展到大模型与图融合等更多领域。</p><p>&nbsp;</p><p>亨通数科是一个面向全球工业装备提供从平台建设、数据应用到知识服务的全栈式产品服务商。亨通数科产品总监董晓健带来 “新一代设备全生命周期管理 — 多模态智能引擎，红海突围的利器”的分享。</p><p>&nbsp;</p><p>董晓健指出，“在产品开发中，需要深挖市场需求和设备管理痛点。亨通数科利用 Fabarta ArcNeural 多模态智能引擎，打造新一代设备全生命周期管理软件。多模态智能引擎提供图、向量和大模型融合能力，可以为设备管理从工单录入与生成、设备故障与维修推荐、设备维修知识总结、设备技改方向选择等各环节提供支撑；同时可以利用向量代表的模糊性知识到图代表的确定性知识的转换，实现知识迁移和衍生，已经沉淀的知识可以在不同工厂复制，也可以在不同行业衍生。”</p>",
    "publish_time": "2023-09-20 17:17:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "当我想要构建一款 LLM 应用时：关于技术栈、省钱和游戏规则",
    "url": "https://www.infoq.cn/article/vlQAK2YR51pYvHxTaFwI",
    "summary": "<p></p><p>“虽然在 2012 年到 2015 年间，深度学习在图像识别领域获得了巨大成功，但要达到通用人工智能，还需要时间。”这是读研究生时刘小洋的老教授告诉他的。</p><p></p><p>自然语言理解是走向通用人工智能的关键，但当时整个行业没能迈出这一步。机器学习框架 TensorFlow 曾一度名声大噪，“当年很多人都学习 Tensorflow，但是我从头到尾没有学过，因为我的一些朋友特别是学术圈的朋友认为，它已经是过去式了，不会再有什么前途。”</p><p></p><p>如今，刘小洋已经是哥伦比亚大学电子工程系研究员。这么多年过去，时间终于给出如何走向通用人工智能的答案：ChatGPT。ChatGPT 的意外突破让包括刘小洋在内的学者们和企业里的研发人员看到了希望。“我不想落后，”刘小洋随后便投入了自己的开源模型创建中，而他的这句话也道出了众人的心声：是的，没人想在这场竞赛中落后。</p><p></p><h3>让人感受到了“温度”</h3><p></p><p></p><p>真正影响深远的技术突破并不出现在 ChatGPT 引领的这次浪潮中，而是在五年前。2017 年，谷歌发表了划时代的论文：Attention Is All You Need，创新性地提出了神经网络架构 Transformer，Transformer 后来成了许多模型的主导架构，包括我们熟知的 GPT。</p><p></p><p>OpenAI 基于 Transformer 从 GPT-1 开始做起，但直到 GPT-3，普通大众才看到了 GPT 的强大能力。OpenAI 的关键突破在于 GPT-3 让人机对话变得更有“温度”，人们开始将人工智能应用看作有情感、有理解能力的“生命体”。</p><p></p><p>具体来看，ChatGPT 的学习流程主要分三个步骤：第一步，训练监督调优模型，主要收集数据、训练有监督的策略模型；第二步，训练奖励模型，人类标注者对监督调优模型的输出打分，这个分数反映了被选定人类标注者的偏好，这个偏好多数情况下是符合人类共同认知的；第三步，使用近端策略优化模型微调监督调优模型。这其中的关键在于用人类反馈来强化学习不断提升效果，最后让用户感觉“就像在跟人对话”。</p><p></p><p>这种优化后的对话体验促进了人与计算机交互方式的发展，即从 GUI（Graphical User Interface，图形用户界面）变为了 LUI（Language User Interface，语言用户界面），用户可以用自然语言表达需要，而不需要记住特定的命令或点击特定的图标。</p><p></p><p>交互方式的改变意味着用户习惯的改变，进而可能导致与 IT 有关的各行各业都或多或少受到冲击。对开发技术栈的一个显著影响就是，应用将以某个庞大的通用模型为基础设施。就如李彦宏所说，人工智能时代，IT 技术栈发生的根本性改变是从原来的芯片、操作系统和应用三层架构，变成了芯片、框架、模型、应用四层架构。LLM（Large Language Model，大型语言模型）成为了人工智能时代的操作系统，所有应用都将基于 LLM 开发。</p><p></p><p>具体来看，之前的 NLP（Natural Language Processing，自然语言处理） 技术栈相对较浅，假设需要对一段文本进行词向量表示，要先将这段文本转化为词向量，然后将向量数据输入到模型中处理，最后模型输出结果。整个过程可以看作是由输入端到输出端的多个阶段组成，一个小团队就可以完成架构搭建。虽然这种技术栈有助于保持一致性，但对于 LLM 来说，这种方式能力差强人意，还不够“性感”。</p><p></p><p>对于新的 LLM 应用技术栈，目前流传较广、也较为全面的是硅谷风投公司 Andreessen Horowitz 发布的下面这张图，图中涵盖了数据管道、API 插件、存储、LLMOps、日志等方方面面。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4e/4eba0de5b044c20681ce9f62b7c36c65.png\" /></p><p></p><p>而 LLM 应用理念的基本组件有四个：数据源、数据工程、LLM 和应用程序。其中，数据源层是管道的起点，负责协调从各种来源获取大量数据；数据工程层专注 NLP 数据的实时处理；LLM 层是核心，包括各种微调方法等；应用层则面向个人用户，提供咨询、程序开发等不同的服务。</p><p></p><h3>LLM 应用开发，更简单</h3><p></p><p></p><p>目前，业内将 LLM 分为两类：通用模型和行业模型。通用模型面向各种通用功能设计，通常由资源丰富的大公司主导，如 OpenAI、Google 和 Meta。但在电商、客服和辅助教学等特定行业场景下，通用模型并不完全适用，同时个人或小型企业在业务规模较小时也没有足够的资源支撑通用模型，这时的另一个可选项就是用特定场景数据对通用模型微调得到的行业模型。</p><p></p><p>在这种分类影响下，LLM 应用主要有两种：基于通用模型构建的应用和基于特定行业模型的应用。</p><p></p><p>对于前者，开发者在通用模型的支持下，直接调用 API（Application Programming Interface，应用程序编程接口）就能够构建很多应用。许多企业会选择直接购买通用模型提供商发布的 API，如 OpenAI 提供的 GPT-4 接口，并在上面进行 UI（User Interface，用户界面）开发。开发过程中，开发者输入自然语言就可以实现开发操作，这个过程更像是一种交流，而不是僵硬的执行。</p><p></p><p>与此同时，企业还会开发很多功能不同的插件集成到应用中，让应用变得更加丰富和灵活。插件模式在国内得到了广泛应用，无论是要开发一个新的应用，还是嵌入到现有的像 Slack、微信等平台，插件模式都具有极强的渗透力。</p><p></p><p>这种使用 API 的开发模式会让 LLM 应用在未来很长一段时间里趋于标准化。不过，这种模式主要依赖大公司发布通用模型进行更新和迭代，企业用户虽然可以更方便地使用通用模型，但很容易跟不上大公司的迭代速度。</p><p></p><p>对于后者，企业可以通过微调拥有自己的行业模型，进而构建相应的应用。模型微调具备少数据、少参数、强任务的泛化能力，具体实现方法很多，不少企业开发者都在尝试。</p><p></p><p>据悉，30B 参数以上规模的 LLM 比较适合用来构建应用：能力足够强，显存需求最低但还有扩展空间。开发者可以根据场景特点做取舍，比如金融领域的量化表达应用不必是 32 位或 64 位浮点数，8 位就足够好，但不能再降低否则性能会大幅下降。内存方面，8 位浮点数至少要 10G 左右，移动端还可以考虑用通信换存储。</p><p></p><p>目前，中美 LLM 应用的软件技术栈存在一些区别。美国更倾向于水平分层的结构，即某些公司如 OpenAI 位于通用模型的层级上方，并给下游公司提供接口，下游公司再将服务提供给最终用户。</p><p></p><p>相比之下，国内更多采用垂直分层的结构。国内企业目前在生成式人工智能技术上处于相对劣势的位置，如果做不出高度先进的通用模型，那就会将精力放在寻找大量合适的应用场景上，这些企业能够自行训练开源的通用模型得到自己的行业模型，并直接服务于特定的垂直领域。</p><p></p><p>事实上，业内人士普遍认为，国内的机会就在于行业模型。行业模型拥有行业数据优势，只需要修改开源通用模型代码、投喂特定的行业数据，并服务于已有的用户来优化体验，就可以达到降本增效的目的。因此，紧密贴合具体应用场景的行业模型，像金融、医疗、教育等在数据方面有独特优势的 LLM 项目备受风投们青睐。</p><p></p><p>“当前比较流行的通用模型 ChatGPT 和 Llama2 已经做得很好，OpenAI、Meta 早期投入了巨额 GPU 算力成本、训练了海量数据，也提供了比较好的产品体验，如今大量用户涌入又提供了新的数据帮助企业优化模型，已经形成了‘强者恒强’的趋势。”Chainfir Capital CEO 田大超表示。另外，基于通用模型的应用还存在大量技术不成熟的地方，风投们认为过早投入这一领域的风险太高，很难形成规模效应。</p><p></p><h3>构建 LLM 应用的省钱之道</h3><p></p><p></p><p>都知道做 LLM“烧钱”，但到底能“烧”多少？我们可以姑且看下 Bloomberg 训练出来的金融大语言模型 BloombergGPT。</p><p></p><p>BloombergGPT 参数规模高达 500 亿，使用了包含 3630 亿 token 的金融领域数据集以及 3450 亿 token 的通用数据集。虽然测试中这个模型在金融方面表现不俗，但 BloombergGPT 有着密集的计算需求，使用了大约 130 万 GPU 小时进行训练，以亚马逊云科技 2.3 美元的费率计算，每次训练成本已经接近惊人的 300 万美元了。可以说，一般企业承担不起这样的费用。</p><p></p><p>开源是现在大家降本的基本解法。比如与 BloombergGPT 相比，同为开源金融模型的 FinGPT 通过专注于顶级开源 LLMs 的轻量级改编，提供更容易访问的解决方案，可以让训练成本大幅下降，每次训练费用不到 300 美元，成本下降了 1 万倍。</p><p></p><p>在训练 LLM 的时候，业内也会通过给 LLM“瘦身”的方式降低成本。比如对于 7B、13B 的 Llama 2，开发者首先可以做一定的限制，如将其调整为 Int8 类型以减小模型尺寸。接下来，开发者可以对模型进行低复杂度微调，具体做法是将原本线性的 QKV（Query，Key，Value）层设计简化成更为精简的结构，即将权重矩阵分解成多个小矩阵，从而大幅减小 LLM 的规模。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/11/11712838a06264b8bd32ebd1b4e564a3.png\" /></p><p></p><p>示意图，来源：<a href=\"https://arxiv.org/pdf/2205.05638.pdf\">https://arxiv.org/pdf/2205.05638.pdf</a>\"</p><p></p><p>刘小洋团队研发的开源金融模型 FinGPT 就是通过使用 LoRA（Low-Rank Adaptation of Large Language Models），将可训练参数数量从 61.7 亿减少到仅有 367 万。整体下来，该模型在显存方面的需求从 38G 降低到了大约 13G，而且微调所需的时间也被显著缩短，通常在 8 个 GPU 小时内就可以完成，有时甚至只需要 6 个小时，而费用则保持在 1000 元以下，甚至低至 600 元。与此同时，FinGPT 的性能提升了 30%。</p><p></p><p>以上数据显示出了企业在利用开源大型模型进行微调时可以拥有的强大成本优势。在进行 LLM 微调时，通常需要直接调整模型的权重，而那些提供基础模型 API 的企业则更多是提供接口服务，如果购买模型的权重，费用将会高得多，并且可能需要签署保密协议，预计花费可达在 2000 万人民币左右。</p><p></p><p>另外一项不可忽视的成本就是人力。田大超以这段时间爆火的妙鸭相机为例道，这样一款产品的开发周期实际上很短，起初团队也就7个人左右，就是他们内部被称为“AIGC破壁行动小组”的一个以张月光为leader的小团队，也就用了6个月左右的时间就做出来了。这样一款 LLM 应用总投入成本大概是小几百万人民币，其中人工成本占大头，其次是租用算力的成本和调用 API 的成本。</p><p></p><h3>LLM 应用，终归还是生态游戏</h3><p></p><p></p><p>目前生成式人工智能领域的上下游关系已经逐渐建立起来了：上游是芯片厂商，如英伟达；中游是通用级别的大模型公司，如 OpenAI；中下游是细分领域的大模型厂商，如 Bloomberg；下游就是一些 LLM 应用，如 Midjourney。普通用户最能感知到的就是面向 C 端的各种 LLM 应用，比如 AI 绘画工具 Midjourney、Stable Diffusion，AI 音频生成工具 Forever Voice 等。</p><p></p><p>一方面，现在 LLM 应用的研发方式决定了“生态”的重要性。比如，使用 OpenAI GPT 模型的用户自成一个生态，使用开源 Llama 模型的用户又自成另一个生态，最后开发者选择哪一种研发方式本质上就是选择进入哪个生态。</p><p></p><p>另一方面，以搜索为例，将 ChatBot 整合到搜索引擎这样的大型平台中，需要足够庞大的搜索引擎用户基础，这样才能够迅速解决可能出现的问题。开发者必须具备全网范围内思考的能力，了解全球用户的搜索习惯，对信息检索有充分的理解。然而，并非所有人都能做到这一点。因此，“整合”游戏说到底还是生态游戏。</p><p></p><p>LLM 应用的本质还是商业产品，营收模式还是以用户付费购买服务为主，比如妙鸭相机要付费后才给用户改变形象后的照片，这与美图秀秀收会员费类似。这种模式与过去各种应用的商业逻辑是一样的，即用户流量为王。</p><p></p><p>用户在谁家，谁才能笑到最后。就像刘小洋说的，在 IT 行业，有许多第一批实践者最后成为炮灰的案例，能否走到最后取决于用户选择哪家公司、用户对什么产品感兴趣。广大用户构成最终市场，这个市场进而支撑起大模型的更新迭代。</p><p></p><p>当前，行业模型发展的关键已经不是模型本身的能力如何，而是它们在某一领域的专业知识、专业数据的积累，专业能力多强，它们未来的壁垒就有多高。</p><p></p><p>未来短期内，通用模型和各领域的行业模型赛道，最后可能分别只有一两家企业能够脱颖而出。而长期看，做得好的通用模型可能会创造巨头公司。同时，随着通用模型的专业度越来越高，通用模型会掌握大多数垂直领域的专业知识，不排除未来行业模型被通用模型替代的可能。</p><p></p><h3>结束语</h3><p></p><p></p><p>LLM 应用刚起步不久，如果要长期发展下去，每个环节都面临着不同的问题：在算力方面，电子芯片的能耗太高，70% 的能耗被用在散热上，这造成了巨大的能源浪费，行业急需出现效能更高的芯片；在算法方面，人工智能发展到一定阶段后可能会面临数学领域最高深问题，算法也需要不断提高和优化；在数据方面，隐私、数据所有权等问题亟待解决，尤其随着人工智能的快速发展，政府、机构、公司和个人都极其关心自己的敏感数据是否会泄露，数据处理变得十分重要。</p><p></p><p>可以看出，LLM 应用行业未来需要努力的地方还有很多，但这也是留给后来者的机会。现在处在技术前沿、风光无限的企业未来未必一定成功。如今竞赛才刚刚开始，入局的开发者们如何能在这条路上一直走下去，还需要时间给出答案。</p><p></p><p>采访嘉宾简介：</p><p></p><p>刘小洋，哥伦比亚大学电子工程系研究员，伦斯勒理工学院计算机系讲师，开源项目 FinGPT、FinRL 和 ElegantRL 的主创</p><p></p><p>田大超，Chainfir Capital CEO</p><p></p><p></p>",
    "publish_time": "2023-09-20 17:23:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "200亿参数大模型书生·浦语在阿里云魔搭开源首发，免费可商用",
    "url": "https://www.infoq.cn/article/8jXJojQydaR7GLQJYPLM",
    "summary": "<p>9月20日，上海人工智能实验室等机构发布书生·浦语大模型（InternLM）200亿参数版本InternLM-20B，并在阿里云魔搭社区（ModelScope）开源首发、免费商用。书生·浦语大模型体系与魔搭社区建立重磅生态合作，以开源开放促进中国大模型生态繁荣。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/69/69587579d4c30f767bb574ac3c214465.png\" /></p><p></p><p>书生·浦语（InternLM）大语言模型由上海人工智能实验室联合多家机构共同推出。今年6月，InternLM千亿参数（104B）语言大模型首次发布，现已经历多轮升级；7月，上海人工智能实验室开源书生·浦语70亿参数的轻量级版本InternLM-7B，且在业内率先开源贯穿数据、预训练、微调、部署和评测的全链条工具体系。InternLM-7B也已上线魔搭社区。</p><p>&nbsp;</p><p>本次发布的InternLM-20B是一款中量级大模型，性能先进且应用便捷，模型基于2.3T Tokens预训练语料从头训练，相较于InternLM-7B，其理解能力、推理能力、数学能力、编程能力等都有显著提升。</p><p>&nbsp;</p><p>相比于此前国内陆续开源的7B和13B规格模型，20B量级模型具备更强大的综合能力，复杂推理和反思能力尤为突出，能为实际应用场景提供更有力的性能支持；同时，20B量级模型可在单卡上进行推理，经过低比特量化后，可运行在单块消费级GPU上，因而在实际应用中更为便捷。</p><p>&nbsp;</p><p>具体而言，InternLM-20B拥有几大优势：</p><p></p><p>优异的综合性能。不仅全面领先相近量级的开源模型，且以不足1/3的参数量，测评成绩达到了Llama2-70B的水平。强大的工具调用能力。支持数十类插件，上万个API功能，还具备代码解释和反思修正能力，为智能体（Agent）的构建提供了良好的技术基础。更长的语境。实现了对长文理解、长文生成和超长对话的有效支持，同时支持 16K 语境长度。更安全的价值对齐。在研发训练的过程中，研究团队进行了基于SFT和RLHF两阶段价值对齐，并通过专家红队的对抗训练大幅提高其安全性。</p><p>&nbsp;</p><p>目前，书生·浦语开源工具链也已全新升级，形成更完善的体系，包括预训练框架InternLM-Train、低成本微调框架XTuner、部署推理框架 LMDeploy、评测框架OpenCompass 以及面向场景应用的智能体框架Lagent。书生·浦语工具链将和开源数据平台OpenDataLab构成强大的开源工具及数据体系，为业界提供全链条研发与应用支持。</p><p>&nbsp;</p><p>魔搭社区开设了书生·浦语“模型品牌馆”专页，聚合书生·浦语系列所有模型及体验接口，便于开发者一站式查询、下载、使用书生模型；魔搭公众号则推出了最佳实践教程，提前跑通模型的部署、推理和微调流程，供开发者参考。</p><p>&nbsp;</p><p>相关链接：</p><p>书生·浦语-20B：</p><p><a href=\"https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-20b\">https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-20b</a>\"</p><p>书生·浦语-对话-20B：</p><p><a href=\"https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-20b-chat\">https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-20b-chat</a>\"</p><p>魔搭社区最佳实践：</p><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;mid=2247486560&amp;idx=1&amp;sn=28517fb276b9d87b7f75d432a8a3dc2f&amp;chksm=c15e8813f62901053216926c3d9b18d63ee825ea9e28fbeeec2a9e92360b1d3286263a6ae574%23rd\">https://mp.weixin.qq.com/s?__biz=MzkxNTM5NTg2OA==&amp;mid=2247486560&amp;idx=1&amp;sn=28517fb276b9d87b7f75d432a8a3dc2f&amp;chksm=c15e8813f62901053216926c3d9b18d63ee825ea9e28fbeeec2a9e92360b1d3286263a6ae574#rd</a>\"</p>",
    "publish_time": "2023-09-20 17:57:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "腾讯安全发布云原生安全数据湖",
    "url": "https://www.infoq.cn/article/ffVDN4YNTXAk4O5TfFDi",
    "summary": "<p>安全日志记录着企业服务器、云基础设施、应用程序等的全部执行过程，对日志数据进行追溯分析，可以准确清晰地了解企业IT设施状况、排查安全隐患、检索故障源头等。</p><p>&nbsp;</p><p>随着企业规模增长、数字化程度加深和安全设备的增加，安全日志数量呈指数级增长，而企业现行的日志处理平台在应对今天动辄PB级别的安全日志时，要么是需要支付非常高昂的授权费和专业人力成本，要么是性能瓶颈无法支撑PB级别体量的日志处理。</p><p>&nbsp;</p><p>如何采用一套新的架构，能支撑企业在PB级别的数据里实现秒级查询，同时成本又可控？</p><p>&nbsp;</p><p>9月20日，腾讯安全发布全新一代云原生安全数据湖，专注海量日志数据分析，助力企业构建一体化云原生数据湖平台，迈向主动安全。据悉，在同等数据规模下，该产品的硬件成本仅为同类开源软件的1/10，此外在查询性能特别是聚合查询性能方面有了成倍的提升，能实现PB级日志的秒级查询。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/70/c4/707ae6255b52ea0ac57b5be24bf882c4.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>“目前企业的日志处理平台普通使用开源大数据ELK等组件快速搭建，以收集告警数据为主，分析数据不全，而且执行长周期数据查询需要分钟级甚至小时级等待，而网络安全是实时对抗的，任何以‘小时’为单位的数据分析产品肯定都不适用。”腾讯安全大数据实验室高级研究员杨浚宇表示。</p><p>&nbsp;</p><p>两年前，腾讯安全在服务客户过程中发现，客户普遍反应遇到日志存储成本攀升、查询效率低下的问题，因此腾讯安全大数据实验室基于多年的大数据分析处理能力，前后花费两年时间自主研发了一款面向云原生的安全数据湖产品。</p><p>&nbsp;</p><p>腾讯云原生安全数据湖是基于云原生的自研数据分析平台，利用日志数据无需修改、大量字段重复、有时间戳等特性进行了几大创新：</p><p>&nbsp;</p><p>架构领先：MPP架构，采用Rust语言开发，针对日志及安全场景进行专项优化极致降本：使用列存储实现极致压缩比，无索引架构避免索引开销一体化引擎：通过原子能力实现数据处理、查询、存储、分析一体化插件式扩展：通过SQL/SPL语句支持不同分析场景，支持“插件式”扩容易运维：面向云原生架构实现存算分离、读写分离、从而实现一键弹性扩容，故障秒级切换</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/db/d5/db94516d1f84826d423d7aa97c6587d5.png\" /></p><p></p><p></p><p></p><p>依托上述技术创新，腾讯云原生安全数据湖实现了极致的压缩比和数据处理效率，能将企业的安全运营存储成本降低90%；在底层架构上面向云原生设计，支撑多实例、多用户，能根据企业的实际需求实现弹性扩容。</p><p>&nbsp;</p><p>此外，腾讯云原生安全数据湖支持泛安全数据接入、加工、存储、分析、告警、可视化等服务，还具备“插件化”应用开发能力，企业用户可根据需求定制上层应用，并通过平台+APP+合作伙伴构建完整的日志应用生态体系，全面赋能各类安全场景。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/f0/a6/f0c15fcyy1e91e2d82e074b1d67743a6.png\" /></p><p></p><p>目前，该数据湖已经集成在腾讯安全SOC+产品下，为企业安全运营管理提供基座。未来，腾讯安全还会对外提供独立产品，助力企业构建云原生数据湖平台。</p><p>&nbsp;</p><p>面向智能化时代，安全运营与管理是企业的安全免疫中枢系统，而安全大数据的智能分析能力将成为企业迈向智能安全的基础。腾讯安全的安全运营产品矩阵始终围绕一件事情，就是如何为客户创造价值。未来，腾讯安全将持续开放技术原子能力，把腾讯领先的技术融合在企业现有的安全能力中，为千行百业的安全实践注入数字安全免疫力。</p>",
    "publish_time": "2023-09-20 18:00:40",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]