[
  {
    "title": "Android Studio Giraffe 稳定版亮相，带来IntelliJ 2022.3及全新IDE外观",
    "url": "https://www.infoq.cn/article/hkqHm2GKjCFUggQ7urfA",
    "summary": "<p>&nbsp;<a href=\"https://developer.android.com/studio\">Android Studio Giraffe</a>\"现已<a href=\"https://developer.android.com/studio\">稳定</a>\"，其引入了新的IntelliJ 2022.3、新的IDE外观和感受、改进的Live Edit、Compose动画预览，等等。</p><p>&nbsp;</p><p><a href=\"https://www.infoq.com/news/2013/05/Android-Studio/\">自2013年首次发布之来</a>\"的十年里，Android Studio仍然是安卓开发的IDE。其最新版本在不同领域引入了许多更改，包括IDE增强、编码效率提升和构建系统改进等。</p><p>&nbsp;</p><p>Android Studio Giraffe采用了一个新的可选择IDE外观和风格，旨在降低视觉复杂性。它致力于简化对最常用功能的访问，同时使较复杂的功能在需要时更易于访问，但在正常使用中不那么突出。此外，它还提供了一个新的主题，使IDE的视觉效果看起来更加现代：</p><p>&nbsp;</p><p></p><blockquote>随着Giraffe的发布，我们已经开始采用新的UI，并对Android Studio进行了一些特定的更改，例如优化Android的默认主工具栏和工具窗口配置，以及刷新我们的风格图标。</blockquote><p></p><p>&nbsp;</p><p>新的IDE还包括一个更新的设备资源管理器，它可以检查任何连接设备的文件和进程，包括复制或删除文件、终止进程或将调试器附加到正在运行的进程上的可能性。</p><p>&nbsp;</p><p>在代码效率方面，Android Studio Giraffe提供了在可组合项中预览UI更改的可能性，而无需将应用程序重新部署到模拟器或物理设备上。该功能可以通过设置/编辑器/实时编辑（Settings/Editor/Live Edit）启用，并且需要Android Gradle Plugin（AGP）8.1或更高版本以及Jetpack Compose Runtime 1.3.0或更高版本的支持。</p><p>&nbsp;</p><p>与预览功能相关的是，Compose动画预览已经支持了许多其他Compose API，包括<a href=\"https://developer.android.com/jetpack/compose/animation/value-based#animate-as-state\">animate*AsState</a>\"、<a href=\"https://developer.android.com/jetpack/compose/animation/composables-modifiers#crossfade\">CrossFade</a>\"、<a href=\"https://developer.android.com/jetpack/compose/animation/value-based#rememberinfinitetransition\">rememberInfiniteTransition</a>\"和<a href=\"https://developer.android.com/jetpack/compose/animation/composables-modifiers#animatedcontent\">AnimatedContent</a>\"。动画可以播放、暂停、滑动等等。</p><p>&nbsp;</p><p>提高代码效率的最后一个帮助来自新的Android SDK升级助手。</p><p>&nbsp;</p><p></p><blockquote>新的Android SDK升级助手可以让你直接在IDE中查看升级targetSdkVersion或应用程序所针对的API级别所需的步骤。</blockquote><p></p><p>&nbsp;</p><p>该助手将显示与你选择的升级选项相关的所有信息，因此你无需要再单独浏览这些信息，并且能够突出显示每个迁移步骤的主要突破性更改。</p><p>&nbsp;</p><p>说到构建系统，你现在可以在<a href=\"https://android-developers.googleblog.com/2023/04/kotlin-dsl-is-now-default-for-new-gradle-builds.html\">Gradle构建脚本中使用Kotlin DSL</a>\"，利用它的编译时检查，可将所有项目代码整合到一种语言下。</p><p>&nbsp;</p><p></p><blockquote>此外，我们还添加了对基于TOML的Gradle Version Catalogs的实验性支持，该功能允许你在一个中心位置管理依赖项，并在模块或项目之间共享这些依赖项。</blockquote><p></p><p>&nbsp;</p><p>最后需要说明的是，Android Studio Giraffe可以在Gradle同步时显示依赖项下载信息。这将能帮助你检测存储库配置中的低效率问题。</p><p>&nbsp;</p><p>Android Studio Giraffe的内容远不止这些。如果你对完整的细节感兴趣，请不要错过官方公告。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/android-studio-giraffe-stable/\">https://www.infoq.com/news/2023/07/android-studio-giraffe-stable/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/twFsuxo4IQi5FIqe3OKA\">移动端性能挖掘：字节跳动iOS与安卓性能归因实践</a>\"</p><p><a href=\"https://www.infoq.cn/article/MuATtLq6R00gEssCOwzj\">Lyft如何检测生产中安卓的内存泄漏</a>\"</p>",
    "publish_time": "2023-09-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何利用播放器节省 20% 点播成本？",
    "url": "https://www.infoq.cn/article/Rx45QcxHI4zZCfMR5r8J",
    "summary": "<p>点播成本节省的点其实涉及诸多内容，例如 CDN、<a href=\"https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT\">转码</a>\"、存储等，而利用播放器降本却是很多客户比较陌生的部分。火山引擎基于内部支撑抖音集团相关业务的实践，播放器恰恰是成本优化中最重要和最为依赖的部分。</p><p></p><p><a href=\"https://www.infoq.cn/article/r3J9F7CtFJyljwQWwl79\">火山引擎</a>\"的视频团队做了份数据统计，在一个很经典的视频业务中，我们在 2022 年至 2023 年大约 1 年半的时间里，针对这个业务进行了 33 次成本优化点，其中 13 次是播放器主导的优化，其余的有 12 次也是需要播放器强配合的优化，也就是说在这个业务里，75% 的成本优化是直接或间接由播放器参与，可见客户端对成本优化的关键作用。</p><p></p><p>最终我们在很多实践中也发现，通过播放器的优化可以为点播业务节省 20% 甚至更多的成本，本篇内容便将聚焦在播放器层面如何节省成本这一主题上。</p><p></p><h2>一、点播成本构成</h2><p></p><p></p><p>在视频点播的成本构成中，有很明显的二八原则：</p><p><img src=\"https://static001.geekbang.org/infoq/b7/b7a1d2eb46169bedf7939dacf12f6cbf.png\" /></p><p>CDN 带宽成本占绝对的大头，80% 都是带宽成本；其次是存储和转码成本，二者占不到 20%；额外还有一些其他的周边的成本，比如日志处理的数据成本、AI 处理的成本。</p><p></p><p>我们可以将成本的优化理解成“置换”，在点播的成本优化中，就存在 2 种“置换关系”：</p><p></p><p>第 1 种置换关系是“成本项之间的置换”，指的是「带宽-转码-存储」之间的置换。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a0b4cad5199d28bdfb5ae239aa7174c.png\" /></p><p></p><p>上图是 H.264 升级到 H.265 编码格式的例子，265 的压缩率相对比 264 要优 20%-40%，所以带宽、存储上 265 是大幅度减少；但是 265 的计算复杂度要复杂很多，所以转码成本大幅度升高。而这个图不是一个等边三角形，带宽成本要远大于转码和存储成本，所以这个置换是非常划算的。</p><p></p><p>第 2 种置换是“成本和体验的置换”，我们一般说是“跷跷板效应：</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/69/69b5518c52156b7633a99b6ecf8ab462.png\" /></p><p></p><p>例如：</p><p>我们增大缓存时长，对应体验上「卡顿率」就会降低，但是成本会增加；抖音小视频 feed 流场景，我们做预加载，这时候首屏感会更顺滑，但对应的成本是增加的；降低码率，那么体验上感到清晰度变差了，而成本就是减少的；跷跷板中间支点是技术，我们通常是希望固定体验、降低成本，依靠技术来支撑。</p><p></p><p>所以我们总在说降成本，那降的到底是什么呢？我们这里用一个很简单的乘法公式来表示：</p><p><img src=\"https://static001.geekbang.org/infoq/43/43bd4980ca1f18f0f87f459cf0ecdac8.png\" /></p><p></p><p>在过去，“单价”是非常明显的因素，大家往往选择在采购环节尽量的压低单价； 而“用量”上通常会被认为是无法改变的业务因素。但“用量”实际上是包含 2 类，一类是正常用量，确实是比较难改变的业务因素，但另一类是“浪费”，是可以被优化的。所以如何识别出浪费、降低浪费，是播放器降本的关键点。</p><p></p><p>那么造成浪费的因素有哪些呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef6af0158d68eca672870783bb48158b.png\" /></p><p></p><p>例如在视频播放过程中，会包括“已播放的数据”，和“未播放但已经缓存的数据”，如果用户中途离开播放，那其中“已缓存的数据”都是浪费了。所以我们定义“浪费”是“已经缓存了、但不需要的字节数”。</p><p></p><p>从理想上来说，没有浪费是最好的；但往往业务中，浪费是非常大的，大于30%是很常见的。常见的可能带来的浪费包括了：</p><p>未播放离开向后拖拽切换档位清晰度溢出（举例：很小的手机屏幕播放4K的内容，肉眼感知不到清晰度的区别）</p><p></p><p></p><h2>二、播放器的成本优化方法</h2><p></p><p></p><p>针对上述的浪费我们进行了如下的具体优化方法：</p><p></p><h4>（一）缓存的浪费</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d28d3667236f7e52cebf1c043b11535.png\" /></p><p></p><p>承接上图的播放器缓存示意图，如果用户播放过程中离开了，那么深灰色是浪费部分。很容易就想到我们减少深灰色的部分的大小，比如把播放水位降低 1/3（也就是图中浅黄色的部分减少掉），不去缓存，那么浪费就明显的减少了。</p><p></p><p>这个就是静态水位的思路，通过减少缓存水位来减少浪费。</p><p></p><p>但是，静态水位是很难抉择的，水位大了浪费多，但是水位太小了，卡顿就会明显的增加。</p><p></p><p>这里有个马太效应，从原理上，缓存的本质是为了对抗网络的抖动的。网络稳定好时，只需要很少的缓存就足够了，但是网络好时缓存会填充的很快，大部分时间都是饱和的。反之，波动大的网络，需要更多的水位，但总的上限也有限，无法提供有效的缓存。</p><p></p><p>为此我们实现了的动态水位算法，我们根据一些因素来动态的决策缓存水位的大小：</p><p>探测用户的网络速度和稳定性，对稳定性高、速度快的，我们减少缓存；对网络速度差、稳定性差的网络，就增大缓存，这样在网络抖动时就能够有更大的缓存空间使用；根据用户的播放行为，通过数据分析道，视频观看的前期，用户离开的比例会更高，观看的后期，离开的比例就会降低，所以前期的缓存水位小一些，后期的缓存水位大一些；一些其他的因素，但目的是在每次播放时决策出一个尽量合理的缓存水位，来平衡卡顿和浪费。</p><p></p><p>决定了缓存水位大小之后，还有个细节点就是 Range 请求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a9/a92548397903c21bd2ec05e397ec127d.png\" /></p><p></p><p>Range 是 http 协议的一个请求头，默认是“0-请求”，表示请求完整文件。</p><p></p><p>左侧的图示意，如果是单独发一个“0-请求”，那么 CDN 服务端就会持续的返回整个文件，如果在中途断开，从服务端视角来说，这些数据已经发送过去了，无论客户端是否需要，都已经计费了，就构成了浪费。</p><p></p><p>在上图，我们分成 3 段来发 Range 请求，中途断开时，是可以停止掉最后一段，那么浪费就大幅度减少了。同样，静态的 Range 是很难抉择的，Range拆分的太细会引起卡顿的提升；Range 过大了成本节省的效果又不够了。</p><p></p><p>这里我们引入目标水位的概念，就是刚刚讲的动态水位算法所决策出来的水位大小。</p><p></p><p>播放器 Range 请求的应遵循两个原则：</p><p>1. 将当前视频尽快缓存到目标水位。</p><p>2. 控制 Range 拆分的大小，避免太小的 Range 拆分。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/17/177d1c20a6f1f2475729dd93415e31b5.png\" /></p><p></p><p>上图是动态水位算法+动态 Range 拆分的效果示意图：</p><p></p><p>横轴代表时间线。纵轴上图是视频下载的大小，蓝色块代表一个 Range 请求；下图是缓存的大小，橙色的折线表示缓存随着视频文件下载和播放时间的波动情况，横着的虚线是目标水位。</p><p></p><p>我们从左到右，分析下目标水位和 Range 的关系：</p><p>第 1 条竖红线，决策出来第一条目标水位1，是启播水位，启播时的 Range 会略大于后面的 2 个 Range；第 2 条竖红线，是判断出一次水位提升，有可能是检测到网络波动，会提高目标水位到水位 2，同时做一次略大的 Range 请求来达到目标水位；第 3 条竖红线，是再次提升目标水位，到水位 3，有可能是因为观看时长增加到阈值，判断离开概率较小，所以保持高水位；后续的播放，在目标水位3随着时间波动，Range大小也会稳定些。</p><p></p><p>从最终效果上看，在任意一个时间点离开，都能够保障相对合理的浪费。</p><p></p><p>我们在不同业务上实践了很多次动态水位+动态range的AB实验，在体验指标持平或更优的前提下，带宽降低8%。</p><p></p><h4>（二）预加载的浪费</h4><p></p><p></p><p>在类似于抖音这种 feed 流下滑的场景，会提前加载好下面的视频，能够使滑动更顺畅，我们 叫“零首帧”效果，里面作用最大的就是预加载。</p><p></p><p>一般的预加载是固定几个视频，每个视频固定的大小。为了得到更好的预加载效果，会尽量多、尽量大的做预加载，也就构成了浪费。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fe/fe95e01501613611343b42477de82da9.png\" /></p><p></p><p>我们做的“精准预加载策略”，在“时机、大小、个数”上做精细化的优化：</p><p>时机上：对预加载也进行切片，这样可以区分出来一部分是紧急的，其他是不紧急的。比如图里，标记P0的是要最优先下载的，然后可以做预加载，预加载标记 P1 的部分，然后是当前视频的缓存水位，之后可以选择是否要预加载 P3 的部分。大小上：每个视频也会结合视频的长度、头大小、码率等因素计算出来需要预加载的大小个数上：按照 feed list 中的优先级依次预加载后续 N 个视频（动态计算），也会结合用户本身的行为（比如快速滑动）来动态决策。</p><p></p><p>AB 实验证明，能够提升预加载利用率 5%，对应流量成本降低约 0.8%。</p><p></p><h4>（三）清晰度的浪费</h4><p></p><p></p><p>现在的主干场景是在移动端看视频，大家都会有启播选档的策略，就是在播放启动时，决定所需要的清晰度，一般是跟随网速、码率来决策的。</p><p><img src=\"\" /></p><p>经常大家面临的场景是，在竖屏里播放横屏视频时，实际上在很窄的一个空间里进行播放，这个时候，如果依然使用完整的清晰度，那么肉眼是看不出来的清晰的。而且，通常情况下小窗播放时用户的主要关注度也并不是画面清晰度，所以就产生了实际上的清晰度浪费。</p><p></p><p>我们对应的解决策略叫“窄屏低清”，就是识别出来显示区域很窄时，播放低清晰度的视频（比如360P），当需要横屏时，再快速的切换为正常的清晰度。这里如果是 mp4 格式播放，需要转码也做些配合，支持 mp4 的帧对齐和平滑切换。</p><p></p><p>在很多应用中都是很常见的，也有常见的小窗播放，多个业务的 AB 实验都能有 3% 以上的成本收益；</p><p>另外清晰度上还有个很棒的能力，是客户端超分。随着客户端超分能力的优化，现在很大一部分机型在客户端向上超分一个档位是完全没问题的，耗电可以忽略。</p><p></p><p>对应节省成本的策略是“降档超分”，就是分发的清晰度向下降一档，然后再通过客户端超分降主观清晰度补回来。在国内当前的机型条件下，大部分业务能够有 6～8% 左右的成本收益。</p><p></p><h4>（四）异常流量的浪费</h4><p></p><p></p><p>我们根据「播放器日志是否可以识别」、「是否是正常流量」把流量分成了 4 类。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/355b42ef6dc4ecbae6514c8c12ecd54c.png\" /></p><p>在非常多的业务中会发现第三种情况：流量有异常浪费，比如有部分视频码率过高，可能是没转码，或者转码模版用错了。我们开始时会认为“这些都是很明显的失误，业务层小心点不就行了么？”，但后来我们做成了单独的异常流量分析模块。我们跟业务尝试分析原因，发现业务总是复杂的：</p><p>业务场景很复杂，包括短视频、长视频、主页视频、广告视频等；研发的迭代通常会带来些历史问题；并不是所有的人员都需要持续的感知成本，只要有一个环节漏掉了，那么就可能会造成很大浪费。</p><p></p><p>这里还有个问题点，如果是体验问题或者 bug，总会有用户保障，来及时发现。但成本问题，用户基本是无法发现的，发现时就比较晚了。</p><p></p><p>我们是通过端到端的日志分析来发现和避免这些浪费的。原理很简单：</p><p>在客户端对日志染色；cdn 日志里记录的，区分是否是播放器产生的、是否是我们点播的域名；对两头的日志进行比对和分析。</p><p></p><p>不仅如此，这里还有个副产物，是通过这些日志分析，识别到业务真实是被盗链了，然后做盗链的治理。</p><p></p><h2>三、数据挖掘成本优化空间</h2><p></p><p></p><p>以上是<a href=\"https://www.infoq.cn/news/iSSNhwt4qU2WuSQ6U3AM\">火山引擎</a>\"是实际业务服务过程中探索出的优化方案，但优化是不是有上限的，优化到什么水平可以达到成本和体验的平衡，更多的能力是通过数据能力持续的挖掘出来的。</p><p></p><p>先从结果上来看，我们成本优化后通常会有2个报告：</p><p>1、AB 实验报告，里面会分析对 QoE 的体验影响多少，对成本优化的影响多少，比如图中的人均播放时长增加多少，成本降低多少；做成本的 AB 实验，依赖一个工具——客户端成本指标。</p><p>2、价值回溯文档，用于核算真实收益有多少，一般发生在完整上量之后，比如1个月或2个月后。关键结果叫“万分钟播放成本”，这个对应的依赖的工具是“成本评估公式”。</p><p></p><h4>（一）客户端成本指标</h4><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f9/f94e68e1e1c0828594268e411a16b7ca.png\" /></p><p></p><p>这张图从左往右是视频点播的数据流向。想要建设好成本埋点，有2个难点：</p><p></p><p>1、成本拟合。因为真实的计费数据是左侧 CDN 的计费日志，在右侧的客户端侧实际上是没有成本数据的，所以我们需要把数据缓存层的对成本的埋点尽量的拟合，使之尽量的对应到 CDN 的计费日志。这个过程是非常艰难的，我们通过了大量的离线校验。</p><p></p><p>2、提升可解释率。业务动作比较复杂（播放、预加载、拖拽、重播等等），举个例子，重复播放，播放层是记录2遍播放时长的，但是因为有缓存，真实的网络请求只有1遍。我们想要两份数据尽量对齐、可解释，就需要涵盖住尽量所有的业务场景。</p><p></p><p>我们当前达到了“可解释率达到 95%”，也就是说比如服务端 CDN 产生了 100Gbps 的带宽，客户端的日志能够拟合解释清楚 95%。</p><p></p><p>虽然还不到 100%，但日常来做成本优化、成本归因已经足够了。</p><p></p><p>下图是成本指标进入 AB 实验后的结果：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e3f5aa2bf270087ee3e07d826556bc41.png\" /></p><p>图：核心指标</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/36c765358635fac3fb3cc769b770652d.png\" /></p><p>图：归因指标</p><p></p><p>成本数据进入AB实验有什么用呢？有2  点：</p><p></p><p>1、快速判断客户端的成本变化结果。大部分成本优化的能力都是伴随着策略的，不同策略有不同的结果置换关系，我们需要通过实验来确定效果。假设没有客户端的成本数据的话，我们就需要用不同的 CDN 域名来实验，这是很低效的，并且域名带宽的波动也会引起成本的波动。而在客户端成本指标进入了 AB 实验之后，大部分场景都直接看报表数字就可以了。</p><p></p><p>2、机制上可以防蜕化。业务的产品经理、分析师等角色也日常会关注到实验数据的，当成本数据也进入实验后，这些角色也可以关注到成本的变化，这样就能够防退化了。举个例子，版本升级时，只要经历了 AB 实验，就很难有成本退化的问题。</p><p></p><h4>（二）成本评估公式</h4><p></p><p></p><p>“成本评估公式” ，本质是一种单位成本的衡量方法。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3f/3faace5c96677c360f3369d5060bc21c.png\" /></p><p></p><p>我们叫“万分钟播放成本”，分子是点播的IT成本，分母是点播视频消费时长。</p><p></p><p>从技术侧来看，分子是“CDN、存储、转码等各种成本的加和”，分子是播放的时长。</p><p></p><p>这个公式很简单，但为什么要这么做呢？</p><p></p><p>因为涉及到成本优化，就会跟采购、财务团队打交道，采购、财务看到的都是每月的账单，业务用量每个月都在上下波动，导致账单每个月也都在波动。万分钟播放成本是单位成本，就可以刨除掉业务用量的影响因素，来衡量成本是否真的优化了。</p><p></p><p>拿其中的万分钟 CDN 成本来举例：</p><p><img src=\"https://static001.geekbang.org/infoq/61/614883436e1bc01a0737693d6285b3cb.png\" /></p><p></p><p>万分钟 CDN 成本的影响因子会涉及到价格、码率、浪费率、带宽流量比。</p><p></p><p>举一个真实的例子：有个客户反馈成本增加了，但是客户自己的业务用量在波动，不太好判断是什么情况。我们拆解分析万分钟 CDN 成本的具体影响因子，就发现了万分钟 CDN 成本确实是涨了 11%，主因是“码率”涨了 8%，“浪费率”增加了 5%。</p><p></p><h2>四、总结和展望</h2><p></p><p></p><h4>（一）建标准</h4><p></p><p></p><p>在服务业务的过程中，大家经常会面临一个问题，还能再降多少？极限是多少？</p><p></p><p>这些问题是很难回答的，因为每个业务的场景都不同，举例缓存浪费中，每个业务的客户中断离开的模型可能都不一样，那么建设统一的标准就很难了。</p><p></p><p>火山引擎目前通过 3 种方式来建设标准：</p><p>通过排名获取标杆：将类似场景的业务进行排名，对齐当前技术做的最好的，可以作为一种标准；离线的实验来模拟：我们做了成本的自动化测试平台，设计测试case，测试出来不同的参数的成本结果是多少，最后总结分析出来极限是多少；通过“理论公式”来推算“标准” ：举例通过“视频播放时长、中途离开比例”的关系，然后推算出理论的优化空间有多少。</p><p></p><h4>（二）做顾问</h4><p></p><p></p><p>面对的业务越来越多，降本的能力也越来越多时，就会遇到效率问题：功能这么多，应该用哪些？每个业务的场景也不一样，那么策略参数应该怎么配置呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/029a51f266a6efd2695de7e2beb1d610.png\" /></p><p>万分钟播放成本分析和策略推荐</p><p></p><p>解决方法是做顾问。如上图所示，其是我们的一个万分钟 CDN 成本与理想万分钟成本的一个差异分析表，我们给计算出了对应的差异，然后再给出可以补足差异的策略或功能推荐。</p><p></p><p>当然，这个表只是一个总结概览，更多的内容我们会整理成“顾问服务报告”，把各个点的差异、业务分析、解决方法与业务逐一的讨论分析。总之，万分钟播放成本是一个非常简单、容易落地、价值很大的工具，大家计算下万分钟播放成本，如有调优的诉求，非常欢迎来与火山引擎交流。火山引擎视频点播<a href=\"https://www.volcengine.com/product/vod\">https://www.volcengine.com/product/vod</a>\"。</p><p></p><p></p><p></p><h4>本文作者简介</h4><p></p><p>赵春波，火山引擎视频点播产品负责人。十余年视频相关研发和产品经验。目前主要负责火山引擎视频点播的产品工作，支撑抖音、西瓜等业务的点播基础技术、体验优化和成本优化等，并将这些技术能力沉淀到火山引擎，来服务更多的行业客户。</p>",
    "publish_time": "2023-09-20 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“芯经济”崛起，英特尔加速AI抢位战",
    "url": "https://www.infoq.cn/article/g43AaWaexclQ0QVlyelb",
    "summary": "<p>&nbsp;当地时间9月19日，英特尔On技术创新大会（Intel Innovation） 2023在美国加州圣何塞拉开帷幕，InfoQ有幸受邀参会并从现场发回报道。英特尔On技术创新大会是一场面向开发者的大会，更是一个了解过去一年英特尔在不同方向（包括端侧计算、数据中心、边缘计算和云计算）取得的最新进展的好方法。来自全球的上千位开发者、英特尔合作伙伴和客户亲临这场盛会，以期全面了解英特尔将以怎样的新策略、新产品应对生成式AI大爆发带来的机遇和挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/90e9c724d5763d9bf01039c278203882.jpeg\" /></p><p>图：InfoQ记者拍摄于现场</p><p>&nbsp;</p><p>AI毫无疑问是贯穿大会首日主题演讲全程的重要关键词。围绕“让AI无处不在”这一主题，英特尔CEO帕特·基辛格在主题演讲中展示了英特尔如何在其各种硬件产品中加入AI能力，并通过开放、多架构的软件解决方案推动AI应用的普及。</p><p></p><h2>AI推动“芯经济”崛起</h2><p></p><p></p><p>基辛格在主题演讲开场表示：“AI代表着新时代的到来。AI正在催生全球增长的新时代，在新时代中，算力起着更为重要的作用，让所有人迎来更美好的未来。对开发者而言，这将带来巨大的社会和商业机遇，以创造更多可能，为世界上的重大挑战打造解决方案，并造福地球上每一个人。”</p><p>&nbsp;</p><p>基辛格提到，如今芯片形成了规模达5740亿美元的行业，并驱动着全球约8万亿美元的技术经济。世界对计算的需求呈指数级增长，而且这种需求与芯片的面积、成本和功耗成反比。简而言之，这就是摩尔定律。随后他提出了“芯经济”（Siliconomy）这一概念：“芯经济”是一个由可持续、开放、安全的算力需求所驱动的经济增长新时代。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d7/d7c9452e6c69668e55c4ad38685fd8f0.png\" /></p><p></p><p>&nbsp;更充足、更强大、更具性价比的处理能力，是经济增长的关键组成。而人工智能代表着计算的新时代，促进了“芯经济”的崛起。基辛格表示，五大超级技术力量——计算、连接、基础设施、人工智能、传感和感知，由“芯经济”推动。随后他分享了一系列AI相关的软硬件产品方案新进展和技术路线图。</p><p></p><h2>英特尔开发者云全面上线</h2><p></p><p></p><p>基辛格宣布英特尔开发者云平台自今日起全面上线。英特尔开发者云平台能够帮助开发者利用最新的英特尔软硬件创新来进行AI开发（包括用于深度学习的英特尔Gaudi2加速器），并授权他们使用英特尔最新的硬件平台，如即将在未来几周内上线的第五代英特尔至强可扩展处理器（代号为 Emerald Rapids），以及将在12月14日上线的英特尔数据中心GPU Max系列1100和1550。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0bcc3821726c9b97a5348dbb7022851e.jpeg\" /></p><p></p><p>&nbsp;在使用英特尔开发者云平台时，开发者可以构建、测试并优化AI以及HPC应用程序，他们还可以运行从小规模到大规模的AI训练、模型优化和推理工作负载，以实现高性能和高效率。</p><p>&nbsp;</p><p>英特尔开发者云平台建立在支持多架构、多厂商硬件的oneAPI开放编程模型基础之上，为开发者提供硬件选择，并摆脱了专有编程模型，以支持加速计算、代码重用和满足可移植性需求。</p><p>&nbsp;</p><p>据InfoQ了解，已经有不少客户基于英特尔开发者云构建自己的AI应用，埃森哲是其中之一。</p><p>&nbsp;</p><p>此外，基辛格还在会上发布了英特尔发行版OpenVINO工具套件2023.1版，并表示Arm也将参与到OpenVINO工作中。OpenVINO是英特尔的AI推理和部署运行工具套件，在客户端和边缘平台上为开发人员提供了优质选择。该版本包括针对跨操作系统和各种不同云解决方案的集成而优化的预训练模型，包括多个生成式AI模型，例如Meta的Llama 2模型。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/ba/ba11e2415bddb323043cad33640fbc95.jpeg\" /></p><p></p><p>为了更好地将AI扩展到边缘侧，英特尔还将推出Strata项目以及边缘原生软件平台。其中边缘原生软件平台平台将于2024年推出，提供模块化构件、优质服务和产品支持。这是一种横向扩展智能边缘（intelligent edge）和混合人工智能（hybrid AI）所需基础设施的方式，并将英特尔和第三方的垂直应用程序整合在一个生态系统内。该解决方案将使开发人员能够构建、部署、运行、管理、连接和保护分布式边缘基础设施和应用程序。</p><p></p><h2>AI芯片路线图、下一代至强处理器亮相</h2><p></p><p>&nbsp;</p><p>自今年7月英特尔发布 Gaudi 2 训练加速器以来，其性能表现一直备受关注。最近的 MLPerf AI 推理<a href=\"https://www.intel.com/content/www/us/en/newsroom/news/intel-shows-strong-ai-inference-performance.html\">性能测试结果</a>\"进一步证明了 Gaudi 2性能在市场上的竞争力，其是目前市场上满足 AI 计算需求的唯一可行替代方案。基辛格披露了一台完全基于英特尔至强处理器和 4000 个英特尔 Gaudi2 AI 硬件加速器构建的大型 AI 超级计算机。这台AI超级计算机将跻身全球TOP15超算，而AI独角兽企业Stability AI 是其主要客户。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/42/42b3a7746acd443bdf203f4b69438fd6.jpeg\" /></p><p></p><p>作为第四代英特尔至强处理器的早期采用者，阿里巴巴也被邀请来为英特尔站台。阿里云首席技术官周靖人阐述了阿里巴巴如何将内置AI加速器的第四代英特尔至强可扩展处理器用于其生成式AI和大语言模型，即“阿里云通义千问大模型”。周靖人表示，英特尔技术“大幅缩短了模型响应时间，平均加速可达3倍”。</p><p>&nbsp;</p><p>面向AI计算，基辛格亮出了英特尔最新的三代AI芯片路线图，其中采用5nm制程的Gaudi 3将于2024年推出，再下一代AI芯片代号为Falcon Shores，计划于2025年推出。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/923adbae77bd67b27b20adf7b09001b8.png\" /></p><p></p><p>其中Gaudi 3的算力将达到Gaudi 2的两倍，网络带宽、HBM容量将达到Gaudi 2的1.5倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/9028195e64b90828118cac67c62bef38.jpeg\" /></p><p></p><p>基辛格还在演讲中预览了下一代英特尔至强处理器，并透露第五代英特尔至强处理器将于12月14日发布，届时，将在相同的功耗下为全球数据中心提高性能和存储速度。此外，具备高能效的能效核（E-core）处理器Sierra Forest将于2024年上半年上市。与第四代至强相比，拥有288核的该处理器预计将使机架密度提升2.5倍，每瓦性能提高2.4倍。紧随Sierra Forest发布的是具备高性能的性能核（P-core）处理器Granite Rapids，与第四代至强相比，其AI性能预计将提高2到3倍。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/874ac283e432995661e9d836ba2550b7.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/af67cd6efa8cbc8c2abd6284ecd32cbf.jpeg\" /></p><p></p><p>展望2025年，代号为Clearwater Forest的下一代至强能效核处理器将基于Intel 18A制程节点制造。</p><p></p><h2>迈向AI PC新时代</h2><p></p><p>&nbsp;</p><p>为了让AI的使用更加普及化，英特尔也将目光放到了个人PC上。基辛格在演讲中表示：“AI将通过云与PC的紧密协作，进而从根本上改变、重塑和重构PC体验，释放人们的生产力和创造力。我们正迈向AI PC的新时代。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/21de39187af217d96624706d2fe0cd76.jpeg\" /></p><p></p><p>这一全新的PC体验，即将在接下来推出的产品代号为Meteor Lake的英特尔酷睿Ultra处理器上得到展现。该处理器配备英特尔首款集成的神经网络处理器（NPU），用于在PC上带来高能效的AI加速和本地推理体验。基辛格透露，酷睿Ultra将在12月14日发布。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/b4/b401df224216ec1e3f752224e64dbb4b.png\" /></p><p>酷睿Ultra处理器</p><p>&nbsp;</p><p>基辛格将酷睿Ultra处理器称作英特尔客户端处理器路线图的一个转折点：该款处理器是首个采用Foveros封装技术的客户端芯粒设计。除了NPU以及Intel 4制程节点在性能功耗比上的重大进步外，这款处理器还通过集成英特尔锐炫显卡，带来独立显卡级别的性能。</p><p>&nbsp;</p><p>基辛格展示了全新AI PC的众多使用场景，并邀请宏碁首席运营官高树国上台介绍了搭载酷睿Ultra处理器的宏碁笔记本电脑。高树国表示：“我们与英特尔团队合作，通过OpenVINO工具包共同开发了一套宏碁AI库，以充分利用英特尔酷睿Ultra平台，还共同开发了AI库，最终将这款产品带给用户。”</p><p>&nbsp;</p><p>AI创业公司Rewind AI也来到现场，演示在断网的情况下，由英特尔OpenVINO驱动在PC本地运行大语言模型，与AI聊天机器人进行实时问答。</p><p></p><h2>制程、封装最新进展</h2><p></p><p></p><p>在2022年英特尔投资者大会上，英特尔公布了接下来几年间的制程发展规划。按照英特尔的计划，未来的四年间，英特尔将跨过五个制程节点。此后一年间，这个计划的进展颇受关注。</p><p>&nbsp;</p><p>在今天的主题演讲中，基辛格表示，英特尔的“四年五个制程节点”计划进展顺利，Intel 7已经实现大规模量产，Intel 4已经生产准备就绪，Intel 3也在按计划推进中，目标是2023年年底。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5be4db4865e0fbd06c9547916cf21d1a.jpeg\" /></p><p></p><p>基辛格还展示了基于Intel 20A制程节点打造的英特尔Arrow Lake处理器的首批测试芯片。他表示，Arrow Lake将于2024年面向客户端市场推出。Intel 20A将是首个应用PowerVia背面供电技术和新型全环绕栅极晶体管RibbonFET的制程节点。同样将采用这两项技术的Intel 18A制程节点也在按计划推进中，将于2024年下半年生产准备就绪。</p><p>&nbsp;</p><p>此前英特尔已经官宣 Intel 18A进程的多项进展。今年4月，英特尔代工服务事业部（IFS）和Arm宣布签署协议，旨在使芯片设计公司能够利用Intel 18A制程工艺来开发低功耗计算系统级芯片（SoC）。今年7月，爱立信宣布与英特尔达成战略合作协议，将采用英特尔18A制程和制造技术为爱立信的下一代5G基础设施优化提供支持。</p><p>&nbsp;</p><p>除制程外，英特尔向前推进摩尔定律的另一路径是使用新材料和新封装技术，如玻璃基板（glass substrates）。这是英特尔刚于本周宣布的一项突破。玻璃基板将于2020年代后期推出，继续增加单个封装内的晶体管数量，助力满足AI等数据密集型高性能工作负载的需求，并在2030年后继续推进摩尔定律。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/58/5885cfc3b010843f050f867823486802.png\" /></p><p>基辛格展示玻璃基板材料</p><p>&nbsp;</p><p>英特尔还展示了基于通用芯粒高速互连开放规范（UCIe）的测试芯片封装。基辛格表示，摩尔定律的下一波浪潮将由多芯粒封装技术所推动，如果开放标准能够解决IP集成的障碍，它将很快变成现实。发起于去年的UCIe标准将让来自不同厂商的芯粒能够协同工作，从而以新型芯片设计满足不同AI工作负载的扩展需求。目前，UCIe开放标准已经得到了超过120家公司的支持。</p><p>&nbsp;</p><p>该测试芯片集成了基于Intel 3制程节点的英特尔UCIe IP芯粒，和基于TSMC N3E制程节点的Synopsys UCIe IP芯粒。这些芯粒通过EMIB（嵌入式多芯片互连桥接）先进封装技术互连在一起。英特尔代工服务（Intel Foundry Services）、TSMC和Synopsys携手推动UCIe的发展，体现了三者支持基于开放标准的芯粒生态系统的承诺。</p><p></p><h2>先进计算和前沿研究</h2><p></p><p></p><p>除了大众关注更多的软硬件进展，英特尔研究院还在诸多前沿技术方向上展开探索，包括神经拟态计算、量子计算等。在主题演讲的最后，基辛格也对这些前沿研究方向的最新进展做了介绍。</p><p>&nbsp;</p><p>基于Loihi 2第二代研究芯片和开源Lava软件框架，英特尔研究院正在推动神经拟态计算的发展。Loihi 2是性能业界领先的神经拟态研究芯片，基于Intel 4制程节点开发，每个芯片最多可包含100万个神经元。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/25/255bb82d22d783dbe72d20de51839d98.jpeg\" /></p><p></p><p>Loihi 2还具有可扩展性，8芯片Loihi 2开发板Kapoho Point，可通过堆叠满足大规模工作负载的需求。英特尔还提供开源、模块化、可扩展的Lava软件框架，助力神经拟态应用的开发。</p><p>&nbsp;</p><p>量子计算方面，今年6月，英特尔发布了包含12个硅自旋量子比特（silicon spin qubit）的全新量子芯片Tunnel Falls，继续探索量子实用性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a54d095dc1771a56c9cf0d46369b418f.jpeg\" /></p><p></p><p>在英特尔的晶圆厂里，Tunnel Falls是在300毫米的硅晶圆上生产的，利用了英特尔领先的晶体管工业化制造能力，如极紫外光刻技术（EUV），以及栅极和接触层加工技术。</p><p>&nbsp;</p><p>虽然这次在主题演讲中并未提及，但也有部分开发者对于英特尔在RISC-V方向的动态十分关注。英特尔近几年一直在投资 RISC-V，去年英特尔加入了全球开放硬件标准组织 RISC-V International，该组织成员包括阿里云、谷歌、IBM、Nvidia、三星等；旗下的 Mobileye 还推出了基于 RISC-V 的 EyeQ Ultra 芯片。但今年1月份，英特尔停止Intel Pathfinder for RISC-V项目，引发了RISC-V社区对于英特尔是否会继续投入的担忧。根据InfoQ获知的最新消息，英特尔只是停止了一个针对RISC-V 的前期探路项目，并没有停止对RISC-V 的支持，而是转而继续正式支持RISC-V，相关研究工作英特尔中国团队将会重点投入。</p><p></p><h2>写在最后</h2><p></p><p></p><p>在生成式AI这波大潮之下，更多人只关注到了GPU的成功和抢手程度，但GPU可能并这波浪潮唯一的受益者。随着大模型热潮爆发，未来将有海量潜在的AI应用需求，业内有观点认为，未来运行大模型所消耗的计算量（即推理的算力规模），将超过用于训练模型的计算量。从硬件层面来看，这意味着 AI 研究的重点将转向如何降低推理成本，而这恰恰是英特尔的优势所在。在本次活动现场与英特尔高层的交流中，我们也听到了类似的观点。</p><p></p><p>训练并非生成式AI的全部，我们可以看到，英特尔正试图在AI工作流的各个环节全面发力，从训练到Fine-tuning，再到部署和推理，英特尔都有软硬件层面对应的产品和技术布局。这些布局能否帮助英特尔在生成式AI浪潮下继续取得成功，归根结底还是在于其能否很好地匹配客户需求、给客户带来足够的商业价值，这也是英特尔当前仍然面临的挑战，让我们一起拭目以待。</p>",
    "publish_time": "2023-09-20 09:33:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]