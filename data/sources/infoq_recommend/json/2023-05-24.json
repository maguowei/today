[
  {
    "title": "亚马逊云科技 Lambda引入响应有效负载流",
    "url": "https://www.infoq.cn/article/YgsLlr5LXF1yvoZfkm2i",
    "summary": "<p>亚马逊云科技最近宣布，AWS Lambda函数可以<a href=\"https://aws.amazon.com/about-aws/whats-new/2023/04/aws-lambda-response-payload-streaming/?utm_source=newsletter&amp;utm_medium=email&amp;utm_content=offbynone&amp;utm_campaign=Off-by-none%3A%20Issue%20%23229\">将响应有效负载以流的方式逐步传回客户端</a>\"。新特性提高了Web和移动应用程序的性能，目前在Node.js 14.x及以上版本的运行时以及自定义运行时中可用。</p><p>&nbsp;</p><p>响应流帮助开发人员将响应从他们的函数以流的方式传输给他们的用户，而不必等待整个响应完成。对于这项<a href=\"https://stackoverflow.com/questions/39381139/how-to-stream-aws-lambda-response-in-node\">期待已久的特性</a>\"，可以使用<a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html\">Lambda函数URL</a>\"和SDK来调用其API，但目前还无法使用API网关或应用程序负载均衡器（ALB）对响应有效负载进行流式处理。对于它的优势，首席开发大使<a href=\"https://twitter.com/julian_wood\">Julian Wood</a>\"是这样<a href=\"https://aws.amazon.com/blogs/compute/introducing-aws-lambda-response-streaming\">说</a>\"的：</p><p></p><p></p><blockquote>在传统的请求-响应模型中，在将响应返回给客户端之前，需要完全生成和缓存响应。客户端等待响应生成会降低TTFB（Time To First Byte）性能。Web应用程序对TTFB和页面加载性能尤其敏感。</blockquote><p></p><p>&nbsp;</p><p>响应流是为图像、视频、大型文档或数据库结果等大型有效负载而设计的，它需要用streamifyResponse()装饰器封装Lambda函数处理程序，如下所示：</p><p></p><p><code lang=\"null\">exports.handler = awslambda.streamifyResponse(\nasync (event, responseStream, context) =&gt; {\n    responseStream.setContentType(\"text/plain\");\n    responseStream.write(\"Hello, world!\");\n    responseStream.end();\n}\n);</code></p><p>&nbsp;</p><p>其中，新的responseStream对象提供了一个流对象，函数可以向该对象写入数据并立即发送到客户端。开发人员可以选择设置响应的Content-Type报头来传递关于流的额外信息。Datadog工程团队负责人<a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7051209952417206274?updateEntityUrn=urn%3Ali%3Afs_feedUpdate%3A%28V2%2Curn%3Ali%3Aactivity%3A7051209952417206274%29\">AJ Stuyvenberg</a>\"<a href=\"https://dev.to/aws-builders/introducing-streaming-response-from-aws-lambda-511f\">测试了这一新功能</a>\"。他评论道：</p><p></p><p></p><blockquote>这次发布可能看起来微不足道，但它解锁了几个关键的用例——对于希望利用服务器端渲染并减少第一字节时间的Next.js和React开发人员来说尤其如此。</blockquote><p></p><p>&nbsp;</p><p>新特性目前支持的最大响应大小为20MB（软限制），流函数的最大带宽吞吐量限制为16Mbps（2MB/s）。目前，响应流仅在Node.js SDK中原生提供，不过开发人员可以<a href=\"https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html#runtimes-custom-response-streaming\">在支持其他编程语言的自定义Lambda运行时中实现它</a>\"。虽然有些开发人员关注的是<a href=\"https://twitter.com/ashishpandeyone/status/1644480011276091392\">它的局限性</a>\"，但Ampt首席执行官兼联合创始人<a href=\"https://www.linkedin.com/in/jeremydaly\">Jeremy Daly</a>\"在其新闻通讯中写道：</p><p></p><p></p><blockquote>该特性不仅让Lambda函数可以将响应流逐步返回给客户端以减少TTFB，而且还允许开发人员超过标准的6MB负载限制。</blockquote><p></p><p>&nbsp;</p><p>云咨询顾问、亚马逊云科技无服务器英雄<a href=\"https://www.linkedin.com/in/theburningmonk/\">Yan Cui</a>\"写了一篇文章，演示了如何使用新的流式响应<a href=\"https://lumigo.io/blog/return-large-objects-with-aws-lambdas-new-streaming-response/\">返回大型对象</a>\"，而无需将结果存储在S3中。</p><p>&nbsp;</p><p>Lambda的响应流并不是云计算领域中第一个可用的选项，Vercel最近就在Node.js（Lambda）和Edge运行时中提供了<a href=\"https://vercel.com/blog/streaming-for-serverless-node-js-and-edge-runtimes-with-vercel-functions\">HTTP响应流支持</a>\"。</p><p>&nbsp;</p><p>亚马逊云科技在<a href=\"https://serverlessland.com/patterns\">无服务器模式集合</a>\"中发布了Lambda流应用程序示例。这些应用程序支持用AWS SAM来构建和部署资源。流式响应增加了<a href=\"https://aws.amazon.com/lambda/pricing/\">Lambda的网络传输成本</a>\"，不过，除了每个请求的前6MB之外，亚马逊云科技免费套餐现在还包含每月100GiB的HTTP响应流。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/04/aws-lambda-response-streaming/\">https://www.infoq.com/news/2023/04/aws-lambda-response-streaming/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/OzPQdde0UPv9O5Cs9xAu\">亚马逊云科技开源 Mountpoint for Amazon S3，通过挂载点技术简化 Amazon S3 对象存储的使用</a>\"</p><p><a href=\"https://www.infoq.cn/article/EvMkADbFYlbaphLm8ywY\">容器与无服务器，是竞争对手还是队友？</a>\"</p>",
    "publish_time": "2023-05-24 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "老牌网关NGINX的云原生新故事",
    "url": "https://www.infoq.cn/article/eBRp9AervUEZCbikdTeD",
    "summary": "<p></p><p>云几乎给每项基础设施都带来了变革，网关也不例外。由于各企业技术栈、性能需求、成本预算等不同，企业找到适合自己的网关产品也非易事。另一方面，虽然 NGINX 及其生态已经相对成熟，但随着 Kubernetes Gateway API 的推出，新一轮网关标准定义的争论再次掀起。</p><p>&nbsp;</p><p>我们在 4 月份邀请了网关领域的资深专家，每周一晚 8 点做客《极客有约》直播间，与大家一起聊聊云原生网关的现状与发展趋势。4 月 17 日，我们邀请到了 NGINX 资深架构师易久平，与大家一起聊聊老牌网关NGINX如何讲好自己云原生时代的故事。以下内容根据直播内容整理，<a href=\"https://www.infoq.cn/video/RVjO8JuqI6EbA0D7Dirw\">点击链接可直接观看回放内容</a>\"。</p><p>&nbsp;</p><p>关键收益：</p><p>&nbsp;</p><p>API 网关发展历史及云原生时代的网关特点当前网关技术路线有哪些企业如何做网关选型NGINX 产品演化策略及如何云原生网关需求</p><p>&nbsp;</p><p></p><h2>云原生对API 网关的要求</h2><p></p><p>&nbsp;</p><p>InfoQ：云原生时代的到来，对 API 网关领域带来了哪些改变？</p><p>&nbsp;</p><p>易久平：既然我们要讲云原生网关，一定要理解云原生到底是什么。此外，我们可以看下整个API网关的发展历程，以及它变化的驱动力到底是什么。</p><p>&nbsp;</p><p>CNCF对云原生的定义中提到了几个关键的点。首先，强调应用环境的动态性，像公有云、私有云、混合云等新型的动态环境已成为大多数应用的首选；另外，强调在跨多云部署应用时具备非云平台绑定的属性；此外，还强调了弹性扩展、基于自动化手段快速部署和拉起等方面的重要性。</p><p>&nbsp;</p><p>在整个互联网应用大量崛起和企业都在做数字化转型的两大背景下，应用的数量不断增多，复杂性也在变大，因此需要更敏捷地支撑和响应这些变化。云原生技术的发展主要就是在解决这类问题。</p><p>&nbsp;</p><p>谈到API网关，我们也知道，API网关概念并没有一致的定义。我也去查阅了关于API网关整个发展历程的资料。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/1a/1aa173e35e0de7cac058e1c3f25dabc1.png\" /></p><p>&nbsp;</p><p>最早是1995年的时候，大家谈的是硬负载。那个时候互联网刚刚兴起，应用大都是一些单体的Web应用，可能只要有个硬件做负载均衡就好了。</p><p>&nbsp;</p><p>2000年左右，有一部分小型互联网公司发展起来，这时大家意识到硬负载的成本还是比较高的，因为一些公司希望快速验证业务场景但不投入那么大。这个时候，有些公司，比如NGINX等，实现了软负载，主要目的是为了降低成本。这两种负载主要还是基础设施工程师、网络运维或者系统运维在维护。</p><p>&nbsp;</p><p>到了2005年左右，互联网应用快速发展，应用越来越多。这个时候，应用交付过程会涉及很多团队，比如交付部署应用要跟安全团队、网络团队或运维团队交流，沟通成本比较高。后来有了一个新的概念，叫应用交付控制器，就是在负载均衡的基础上叠加一些安全能力、SSL卸载、性能优化、做连接多路复用等，其中包括流量监控，主要是为了解决应用快速交付问题。当时也有一个技术背景：应用已经开始做前后端分离了。结果就是，相比前两个阶段，应用的流量请求数越来越少，但是请求频率会更高。对后端来讲，这个时候它的流量形态已经发生变化，可能有更多的请求到后端，有更多SSL卸载的需求。</p><p>&nbsp;</p><p>2010年以后，我们开始讲API网关的概念了。这个时候有大量的企业已经开始做架构转型，比如开始做SOA架构。做SOA架构的时候，系统之间需要去沟通，需要通过ESB这样的组件串联系统。另外，各类单体应用需要对外提供服务能力。这个时候，API经济已经开始发展。对于企业来讲，API变成一种资产，也是一种商业模式，需要有统一对外的API入口来管理分散在各个部门的能力，并把这些能力用一个通用的方式暴露出来，这就是API网关的第一个阶段，可以称之为1.0阶段。这以后，网关角色更加偏向于开发或者开发运维。</p><p>&nbsp;</p><p>2015年左右，API网关进入2.0阶段，此时微服务大规模兴起。当时移动互联网崛起，应用数量越来越多，迭代也越来越频繁，这导致大家希望网关由平台运维或者运维团队搭建，然后交给开发人员快速部署、上线自己的API。在这个阶段，API网关多了一些能力，比如跟微服务相关的服务治理、服务发现、认证鉴权、限流限速、灰度发布等。像 Spring&nbsp;Cloud Gateway就属于这一类网关，Kubernetes&nbsp;Ingress &nbsp;Controller严格来讲也属于这一类，他们只是部署场景不一样。</p><p>&nbsp;</p><p>从网关的发展过程可以看出，其演进过程都是跟随着应用技术架构变化发展的，到现在的云原生技术阶段，网关本身需要融入云原生体系，同时作为生态的一部分，需要服务基于云原生技术实现的应用服务。</p><p>&nbsp;</p><p>最后，我们可以推断和总结出，云原生时代的API网关除了常规网关需要具备的安全能力、流量调度或控制能力外，还需要具备以下特性：</p><p>&nbsp;</p><p>1） 容器化：支持容器化部署，可部署在容器集群外、集群入口、集群内，作为容器集群入口网关需要实现Ingress、Gateway API模型规范；</p><p>2） 微服务：支持容器集群的服务发现，服务好容器集群内的微服务；</p><p>3） 服务网格：支持容器集群边缘部署，成为服务网格的出入代理网关；</p><p>4） 弹性扩展：基于容器的弹性伸缩；</p><p>5） 动态应用环境：支持多云部署，实现云平台无关性；</p><p>6） 声明式API：使用声明式接口完成配置运维，并可集成到CI/CD流水线实现自动化；</p><p>7） 可观测性：可被云原生监控体系集成，实现日志、指标、链路监控；</p><p>8） 多角色：DevOps、NetOps、SecOps、AppDev等不同用户角色可基于K8s实现协作，提供自服务能力。</p><p>&nbsp;</p><p></p><h2>如何做网关技术选型</h2><p></p><p>&nbsp;</p><p>InfoQ：开发者做网关的选型时，主要考虑哪些因素？</p><p>&nbsp;</p><p>易久平：从网关选型角度，我的观点是没有一种网关可以覆盖所有的场景，因为不同规模的企业组织架构，不同的应用业务架构，不同的应用技术架构，都会对API网关的侧重点有所不同。但是，我们可以总结出几个关键的通用点：</p><p></p><p>企业架构：企业内的组织架构决定了应用系统的架构，API网关的范围是全企业，还是部门；技术架构：API网关部署的位置与应用场景会直接影响技术选型，位置是在容器集群外、集群边缘或集群内？网关仅服务单K8s集群、还是跨多K8s集群、或者跨K8s集群与老的微服务集群？是否需要API管理，对外输出统一的API文档和开发者门户？业务场景：业务应用场景对不同的应用协议、性能、安全性等有所区别；性能：API网关代理的后端服务级别及业务特性是什么？性能是否能满足您的需求？可伸缩性：API网关是否具备横向伸缩或纵向伸缩的能力？安全性：API网关需要具备哪些安全能力：零信任（访问控制、认证鉴权、TLS/mTLS、审计日志）、WAAP（WAF、Bot防护、DDoS缓解、API安全）；用户角色：谁负责建设和运维，谁负责使用？成本：总的投入成本是多少，是完全基于开源自建，还是购买企业级API网关？</p><p>&nbsp;</p><p>InfoQ：目前的云原生网关市场上有哪些代表性技术路线？</p><p>&nbsp;</p><p>易久平：我认为，技术路线取决于场景。</p><p>&nbsp;</p><p>有的企业可能云原生技术栈发展比较快，已经全面 Kubernetes化了，有的企业只是部分 Kubernetes化，甚至有部分企业只是把 Kubernetes集群当成当虚拟机场景。不同情况对网关的需求完全不一样。</p><p>&nbsp;</p><p>全面 Kubernetes化的企业，可能只需要关注 Kubernetes集群的流量入口就好了，这时把网关放在 Kubernetes集群入口的位置，用 Ingress Controller或者 Gateway API就可以实现。有的企业可能没有容器化，或者只是容器化但没有到 Kubernetes集群部署阶段，这种可能还在依赖Spring&nbsp;Cloud Gateway 直接部署。</p><p>&nbsp;</p><p>另外，很多时候还需要分层级，因为不同层级的网关处理的场景也不一样的，比如边缘网关需要实现跨集群的负载均衡等。</p><p>&nbsp;</p><p>InfoQ：可以说，现在各种网关是在各自场景里发挥作用？</p><p>&nbsp;</p><p>易久平：它的能力是差不多的，无非就是分流、调度、流量控制、安全防护、流量监控这些，基础能力大部分还是通用的。</p><p>&nbsp;</p><p>InfoQ：底层能力一样，差异性体现在哪些方面？</p><p>&nbsp;</p><p>易久平：使用场景不同。比如放在边缘位置的话，一定不会做像协议转换那种比较重的动作，否则可能对性能有很大影响。具体场景还是要根据实际情况分析，然后才能构建出符合自己企业特点，或者符合自身长期技术计划规划的网关架构。</p><p>&nbsp;</p><p></p><h2>NGINX 的发展和完善</h2><p></p><p>&nbsp;</p><p>InfoQ：回到NGINX自身，QUIC 第一份规范草案提交给 IETF 五年后，NGINX才选择支持 QUIC 协议。在选择是否支持某协议时的考量是什么？</p><p>&nbsp;</p><p>易久平：<a href=\"https://www.infoq.cn/article/gV9MuaQixrQUoSu7su46\">QUIC</a>\"协议最早是在2012年时由谷歌发起的，当时只是支持 Chrome 浏览器，它的优点是基于UDP协议，UDP协议无需建立连接，也就没有连接的开销问题，通讯效率会更高。它天然支持TLS，安全性也比较高，这是QUIC协议的优点。</p><p>&nbsp;</p><p>但是，QUIC 协议的整个规范制订过程比较漫长。2012年谷歌发起后，2015年才开始提交到IETF做规范化，第一个正式草案在2018年才提出来，正式版本是2021年才发布。QUIC协议的整个标准化过程花了八九年的时间。在上述阶段中，如果适配各种草案的版本，相当于只是实现了一个半成品。</p><p>&nbsp;</p><p>从NGINX角度讲，<a href=\"https://www.infoq.cn/article/q00BxoNTXC1Qgobjyzs6\">NGINX</a>\"在全球有十亿的部署，作为一个通用性的 Web Server，在实现QUIC协议过程中会考虑各种稳定性因素。NGINX当时有个针对QUIC协议的独立开发分支，是在2020年3月份创建的，但实际上到了2023年2月份才提出正式的预览版。可以看到，从开发到正式的预览版就已经跨了三年的时间。</p><p>&nbsp;</p><p>从NGINX支持QUIC的开发分支提交历史上可以看到，开发工程师们做了很多代码的变更，可见开发工作量还是很大的，这其中主要的原因有技术层面也有其他层面的因素，主要有：</p><p>&nbsp;</p><p>1） QUIC协议的规范一直没有定稿，开发工程师需要不断的适配新的规范草案版本，对于一个通用的Web服务器/负载均衡/API网关产品，需要做大量的客户端兼容性测试才能正式发布；</p><p>&nbsp;</p><p>2） 从技术架构上，NGINX 的多进程架构与QUIC基于UDP协议的无连接有一定的冲突。在处理TCP连接时，每个连接会被分配到一个工作进程，但是基于UDP的QUIC在Linux内核层面不会建立端口到进程的映射关系，NGINX的解决方案是通过实现了一个eBPF扩展来集成SO_RESUSEPORT（允许将多个进程关联到一个端口），将Connection ID映射到首先处理的工作进程上（QUIC在五元组的基础上支持Connection ID用于逻辑上保持连接的不中断）；</p><p>&nbsp;</p><p>3） 为了支持各种协议相同的处理能力，例如：长连接处理等，还做了一些NGINX内核优化；</p><p>&nbsp;</p><p>4） QUIC天然就支持TLS 1.3，NGINX默认使用OpenSSL库，但是OpenSSL的老版本并不支持QUIC，所以NGINX只能选择BoringSSL库，及OpenSSL的quictls分支版本；</p><p>&nbsp;</p><p>这几个事情做下来花费的工作量还是很大的，也解释了为什么花那么长时间还没有正式版本支持QUIC。不过，大概在5月左右，公司计划正式发布QUIC支持的版本。</p><p>&nbsp;</p><p>InfoQ：可以看出，背后其实也有很多工作一直在做。</p><p>&nbsp;</p><p>易久平：并没有那么简单。这也正面体现出，稳定性、可靠性、安全性对于一个网关组件的重要程度，大家在发布新版本时候是比较谨慎的。</p><p>&nbsp;</p><p>InfoQ：NGINX+插件机制带来了扩展性，这种方式的局限是什么？&nbsp;</p><p>&nbsp;</p><p>易久平：插件机制不只是在网关层面，在其他的应用里面都有很多的插件。这种模式的核心思想就是实现了一个稳定可扩展的基础平台，一些可有可无或者创新性的功能通过插件的方式扩展，既保证了基础平台的稳定性和健壮性，又保证了一定的扩展性。</p><p>&nbsp;</p><p>这对网关来讲也是一样的。比如我们可以通过NGINX技术框架实现一些底层的内核级优化，比如连接、事件机制、安全机制等。但这些都是底层能力，不能解决所有问题。有了这种扩展机制后，它的扩展就不一定受制于官方支持，只要具备这种开发能力的人都可以去做，只是扩展难度不同的问题。</p><p>&nbsp;</p><p>NGINX目前最主流的扩展机制有两种，一种就是基于C语言直接写扩展模块，这种方式可以做更多内核级的优化，当然它的开发难度也更大，需要同时做静态编译、考虑平台兼容性等。</p><p>&nbsp;</p><p>另外一种比较流行的是基于Lua的扩展机制。Lua机制是由 OpenResty&nbsp;扩展出来的，他当时把Lua扩展模块开源出来了。大部分情况都不需要做底层优化，只是需要做流量控制和监控。Lua的开发难度要低，而且支持动态加载，因此解决了不少扩展问题。</p><p>&nbsp;</p><p>除了Lua以外，NGINX官方也支持JavaScript的扩展机制。JavaScript 机制本质上跟Lua机制相似，能力上也基本上跟Lua扩展机制持平，或者能做得更多。它是由官方实现的，考虑得会更全面。另外，JavaScript 语言的掌握群体要大于Lua，前端的开发者大都会写JavaScript，这也是NGINX选择用JavaScript作为扩展语言的主要原因之一。</p><p>&nbsp;</p><p>InfoQ：Lua 脚本维护难的问题有解决办法吗？</p><p>&nbsp;</p><p>易久平：灵活性跟复杂性永远是一个矛盾体。所有使用动态脚本方式扩展功能都会面临着灵活性与管理复杂性的平衡问题，例如：基于脚本的规则引擎、工作流引擎、低代码平台等。NGINX无论是基于Lua，还是基于NJS扩展也都会面临同样的问题，如何做配置管理、版本控制、回归测试等。在K8s这种强调动态性的云原生环境中，更是无法采用传统的文件复制方式维护。但是，好在云原生环境中也强调Everything as Code的思想，包括NGINX的conf、Lua、NJS等都可以当成源码放入Git代码仓库中，通过运用GitOps思想，以及Kubernetes原生的运维配置风格（如：声明式API、YAML、ConfigMap），把脚本变更纳入CI/CD流水线中，再完善回归测试用例和脚本，也能有效管理起来。</p><p>&nbsp;</p><p>InfoQ：语言生态对网关项目有什么影响吗？</p><p>&nbsp;</p><p>易久平：一般来说，每种开发语言都有各自的天然优势领域和劣势领域，在语言选择上我们总是需要从所需要开发的技术组件功能与非功能特点，以及对云原生环境的融入与适应能力出发。</p><p>&nbsp;</p><p>我个人不是某一个语言的忠实粉丝，因为存在即有价值，但是网关类组件作为数据通路，一定是要求高性能、高安全性、高可靠性为首要考虑因素，其次是可扩展性、动态性和敏捷性，最后还要考虑团队的技术栈偏好。</p><p>&nbsp;</p><p>从目前流行的网关组件技术栈来看，NGINX核心采用C语言、Envoy核心采用C++、Cloudflare采用了Rust、Tyk和Traefik采用Go、Spring Cloud Gateway则采用了Java、Ocelot采用了.net，可见在语言选择上并没有一家独大。但是细看这几个网关可以简单总结其语言选择逻辑：</p><p>&nbsp;</p><p>1） 微服务框架主导类网关，一般由微服务框架所采用语言决定，例如：Spring Cloud Gateway、Ocelot，更关注于微服务框架的融合能力；</p><p>2） PaaS平台或容器平台主导类网关，一般采用Go语言，因为Kubernetes大量使用Go，更关注于平台技术栈一致；</p><p>3） 通用型网关，能部署在各种位置的软负载、反向代理、数据中心边缘网关等组件，则通常采用C、C++、Rust，更关注于性能、安全性、可靠性等。</p><p>&nbsp;</p><p>InfoQ：我们分出一些时间来回答下观众的问题。第一个是说到了 NGINX配置难的问题。</p><p>&nbsp;</p><p>易久平：&nbsp;“为什么我觉得NGINX配置好麻烦？”NGINX发展确实好多年了，要考虑不同的场景，官方模块大概有89个，配置指令将近八百多条，这些配置指令都有不同的上下文，这也是为什么这位观众觉得麻烦了一点。但也可以看出，NGINX的功能非常多，除了这些官方模块，还有开源模块，扩展的模块功能更多。从这个层面上来讲，灵活性与复杂性永远是并存的。</p><p>&nbsp;</p><p>所谓难度，大家如果掌握的好了，也并没有那么难，这是我个人的观点。我们也有一些通用的配置最佳实践，大家可以参考我们的官方Blog。</p><p>&nbsp;</p><p>InfoQ：有开发者认为“NGINX的多进程架构不太好，进程间共享资源没有多线程方便。”您如何看待这个评价？</p><p>&nbsp;</p><p>易久平：所有事情都是有利有弊的。从NGINX的整个发展历程看，我认为，它的益大于弊，因为正是多进程架构的设计，才造就了现在很好的性能。它可以充分利用CPU多核能力达到更高的吞吐量、做到更低的延时，这是它的一个亮点。早期，如果没有多线程模型，就会造成一定的开销问题。</p><p>&nbsp;</p><p>但我们其实也很早就支持了单工作进程下面做多线程，实现了“熊掌鱼翅二者兼得”，既利用了多进程架构去充分利用CPU多核处理能力，同时在单工作进程下面也实现了多线程，避免请求无限等待、死锁等问题。</p><p>&nbsp;</p><p>本质上，我认为这不是核心问题，主要还是取决于整个实现过程中的功能架构设计，以及实现的代码的质量问题。</p><p>&nbsp;</p><p>InfoQ：还有一个观众问到：目前的云原生网关产品有什么不一样？</p><p>&nbsp;</p><p>易久平：我个人觉得“云原生网关”是新造出来的一个概念，我更认同API网关2.0的概念。云原生网关更多指的是 Kubernetes入口的网关，其现在主流实现有三种形态：一是以NGINX或者NGINX衍生品为底座的一类网关，它们都实现了Ingress API规范，作为容器 Kubernetes集群入口或者出口的角色存在；二是基于Envoy去实现Envoy&nbsp;Proxy；三是自研的产品。但本质上，他们都实现的是 Kubernetes的规范，比如Ingress 或Gateway&nbsp;API。</p><p>&nbsp;</p><p>NGINX 的云原生策略</p><p>&nbsp;</p><p>InfoQ：去年发布的 NGINX Kubernetes Gateway 与Kubernetes 发布的 Gateway API 有什么差异吗？</p><p>&nbsp;</p><p>易久平：最早的时候， Kubernetes社区还没有Ingress概念，使用NodePort直接对外发布，但它只能做四层，也没办法拿到源地址。随着 NodePort&nbsp;越来越多，大家发现在 Kubernetes环境也需要一个API网关的角色，所以才有了Ingress。Ingress本质上是一种规范，但是完整度并不高，解决的业务场景也不全面。作为一个API网关，它的能力是缺失的。</p><p>&nbsp;</p><p>这也是为什么当时大家在实现的时候都在定义自己的CRD，自己扩展Ingress模型实现更多生产级的能力。NGINX也一样，做了很多能力补充。这段时间可以认为是Gateway 1.0阶段。</p><p>&nbsp;</p><p>Ingress一方面是能力不足，另一方面是没有区分多角色，导致边界不清晰。发现这个问题后，Istio 建立了一套Istio Ingress Gateway规范，它引入了Gateway这个名词，与之前的Gateway概念有了衔接。Istio&nbsp;Ingress Gateway 实现了多角色和更多的模型。这个背景下，社区干脆制定了一套新的规范，所谓的2.0规范，即Gateway &nbsp;API规范，这个规范解决了刚才提到的 Ingress的两个问题：多角色协同和能力拓展。2.0规范支持不同的角色定义不同的资源对象，解决了边界的问题。同时，在吸取了很多生产级的API&nbsp;Gateway能力后，它把一些标准API网关的能力扩展出来。</p><p>&nbsp;</p><p>当然，规范还是规范，只是定了一个框架，具体怎么实现、实现得好不好，还是由各个厂商去做。NGINX 去年发起了NGINX &nbsp;Kubernetes Gateway 产品，实现了Gateway &nbsp;API 规范，严格来讲，它实现的是控制面的规范，数据面还是NGINX来做。</p><p>&nbsp;</p><p>InfoQ：现在整个行业还没有形成一个完全统一的规范？</p><p>&nbsp;</p><p>易久平：这是大家希望努力得到的结果，就是希望这个规范会不断成熟、不断迭代。通过这个规范，厂商的能力会越来越强，网关用户的切换成本会更低，因为所有的配置都是基于标准规范配置的，今天可以用基于NGINX的云原生网关，明天也可以改成其他的，因为他的配置不用改。</p><p>&nbsp;</p><p>当一个技术开始百花齐放的时候，一定会出现一个规范，来降低使用者的迁移成本。</p><p>&nbsp;</p><p>InfoQ：云原生这块，&nbsp;NGINX 最近发布了 NGINXaaS for Azure 主要是在做云上迁移这块，为什么关注迁移这块的内容？网关迁移上有哪些难点吗？</p><p>&nbsp;</p><p>易久平：我是这样想的，因为所有的开源厂商最终都要盈利的，因为他也要养活自己的团队。NGINX一个独立的厂商，也希望通过云平台提供的应用市场来扩大我们的销售渠道。从数据中心、混合云、多云、分布式云的发展，各类公有云平台已经成为非常流行应用运行环境，这也是云原生技术快速发展的主要原因，作为技术组件类产品能跟云厂商集成，依托云平台的应用市场和渠道，可加速我们的商业产品推广。云租户可快速按需拉起NGINX实例，就像其他中间件一样，例如：Redis、Kafka等。这种模式主要有两个问题：</p><p>&nbsp;</p><p>1） 有部分厂商锁定的嫌疑，尽管比完全由云厂商自己提供的网关组件要小，但是对于跨云平台的技术迁移还是有些许成本；</p><p>&nbsp;</p><p>2） 如何与现有的CI/CD流水线集成，让基于云应用市场的组件能融入现有技术体系。</p><p>&nbsp;</p><p>InfoQ：整体看，NGINX 在云原生网关方面的战略是什么？</p><p>&nbsp;</p><p>易久平：NGINX在去年的时候已经把整个企业的云原生策略重新梳理过了，总结就是：拥抱开源，拥抱云原生。</p><p>首先是拥抱开源。拥抱开源会有几个动作，第一是开源更多的功能，包括数据面的功能和控制面的功能。在控制面上，我们开源了NGINX Kubernetes &nbsp;Gateway、NGINX Agent；数据面上，我们开源了DNS解析能力及QUIC支持能力等。当然还有一些更多的开源的策略还在内部讨论中，大家可以持续关注。</p><p>&nbsp;</p><p>另一个策略就是多场景覆盖。基于云原生的多场景覆盖，我们提出了几点，第一是连接应用，这一点体现了原来NGINX强势的地方，就是作为ADC或软负载去实现连接应用的能力。第二是连接API，作为API网关来使用。第三是连接 Kubernetes。我们在 Kubernetes的体系里也实现了不少能力，有Ingress Controller实现，有Kubernetes Gateway API的Gateway实现，也有Service&nbsp;Mesh的实现。</p><p>&nbsp;</p><p>基于此，我们还可以叠加安全防护的能力，比如我们有自己的安全防护模块，可以做七层DDoS的防御、做机器人防御、做API安全等，让整个云原生的安全防护左移，把整个安全防护策略放到 CI/CD，DevSecOps 环节里，这部分能力NGINX在持续迭代演进。</p><p>&nbsp;</p><p>InfoQ：与其它项目相比，您认为NGINX 分别有哪些技术上的优势和不足？</p><p>&nbsp;</p><p>易久平：网关系统里最重要的是数据面能力。底层能力决定了上层能力，底层能力不好，上层的那些业务抽象也不一定好。从这点来看，我觉得NGINX团队还是有很强的工匠精神，就像我们在实现QUIC协议上，做了很多很细致的调优。有了这种精神，它才会持续地去打造更好的数据面能力，通过底层能力再衍生到对云原生体系的支持，例如：可观测的集成、安全的集成、规范的实现等，都是从底层能力衍生出来的上层应用。“底层能力+部署环境+应用场景”就得出了不同的方案，这是NGINX在持续做的。</p><p>&nbsp;</p><p>InfoQ：中国本地客户的需求是什么？与国外开发者相比有什么差异吗？</p><p>&nbsp;</p><p>易久平：我个人觉得，网关的角色从核心能力上来讲，国内国外并没有差异。它的核心能力包括性能、安全性、稳定性、协议支持、限流、限速等各种标准能力，我认为这些是通用的。但是对于国内客户来讲，可能更多会看是否有中文文档、是否有国内的技术支持团队、是否有本地化的技术服务、是否支持国内的云平台、是否支持国产的操作系统和服务器等，这些是国内企业比较关注的点，其实是基于核心能力衍生出来的一些本地化能力。</p><p>&nbsp;</p><p>InfoQ：企业如何选择适合自己的网关产品？</p><p>&nbsp;</p><p>易久平：&nbsp;从个人的实践经验来说，一般会从“理清概念，识别需求，分步实现”的三部曲开启网关产品的选型和实践，理清概念对于理解产品的核心设计理念和想解决的关键问题有一个清晰的认识，识别需求则是排定自身需要解决的关键问题。对于网关类产品来说，建议按照以下几步进行：</p><p>&nbsp;</p><p>1) 理清概念</p><p></p><p>网关的核心价值是建立客户端与服务端的桥梁，那么从5W2H的方式来分析理解一下网关的概念：</p><p>&nbsp;</p><p>What：什么是API网关/流量网关/业务网关/微服务网关/云原生网关/BFF，跟ADC或负载均衡有啥区别？我需要用网关解决什么问题？When：什么时候需要增加网关层，什么时候需要修改网关配置？Where：网关部署在什么位置？一般来说网关部署在某一个“体系”的边缘，如：数据中心入口、可用区入口、VPC入口、企业/部门入口、微服务集群入口、K8s集群入口；Why：我为什么需要增加这层网关，带来什么价值和缺陷？Who：网关由谁来建设，谁来运维，谁来操作使用？How：怎么使用网关？自动化配置，还是手动配置？网关自身的高可用、安全性如何保障？如何构建网关，是自建还是用云产品？How Many：我们需要部署几层网关？每个网关集群的性能指标是什么？打算投入多少成本？</p><p>&nbsp;</p><p>2) 架构梳理</p><p>&nbsp;</p><p>一定要了解自身的企业组织架构和应用技术架构：</p><p>&nbsp;</p><p>企业组织架构：康威定律中说企业的组织架构决定应用架构，网关往往位于各组织的边界，用于衔接不同的组织。应用技术架构：我们需要理清自身应用的主要业务场景与流量场景，及使用了哪些应用协议等。例如：视频直播类应用与电商平台类应用的流量特征肯定有所区别。此外，应用部署的环境是什么、是否为传统微服务架构（Spring Cloud），有无使用K8s集群，有无使用多K8s集群，多K8s集群是主备/双活/多活高可用，有无历史系统需要兼容集成，是否有统一的PaaS平台等都是重点考虑的内容。</p><p>&nbsp;</p><p>3) 场景梳理</p><p>&nbsp;</p><p>网关衔接了应用与客户端，通常来说网关的核心价值点有：</p><p>&nbsp;</p><p>隐藏后端应用的内部技术架构，对外提供量身定制的API，降低复杂度并加速应用发布；为应用提供统一的基础设施层，把应用通用的横切面非功能都卸载到这一层；通过统一的流量监控简化应用故障的排查；具体有哪些场景大类呢？常见的有：应用协议：四层协议（TCP、UDP），七层协议（HTTP/1、HTTP/2、HTTP/3、HTTPS、WebSocket、gRPC、Dubbo、SOAP、MQTT、HLS、RTMP等）流量路由：基于请求上下文的条件路由、蓝绿部署、灰度发布、A/B测试、负载均衡、会话保持；流量控制：限流限速、限并发、限带宽、请求/响应改写、请求重定向、协议转换、集群级限流限速流量安全：访问控制、认证鉴权、TLS/mTLS、审计日志、WAF、Bot防御、DDoS缓解、API安全、CORS；服务治理：自动服务发现、熔断降级、主动健康检查；服务优化：响应缓存、响应压缩流量可视：指标监控、日志监控、链路监控、安全监控API管理：API生命周期、版本控制、安全策略、流量策略、API文档、开发者门户高可用：主备、多活高可用，网关有状态集群（配置同步、会话同步、内存计数器同步）；高性能：网络吞吐、请求吞吐、响应延时、基于加密卡的SSL卸载加速、基于DPDK的内核优化；可扩展：足够的扩展点、基于脚本的插件扩展能力；自动化运维：声明式API、动态配置加载、自动化监控采集、容器化部署；国产化支持：国产服务器/操作系统的支持、国密算法等；</p><p>&nbsp;</p><p>4) 选型对比</p><p>&nbsp;</p><p>结合自身需求优先级，选定可选网关产品项，并做对比打分选择。</p><p>&nbsp;</p><p>5) 分步上线</p><p>&nbsp;</p><p>网关的上线通常需要非常谨慎，一不小心就是巨大的生产事故，除了需要在测试、预发环境做深入测试外，还需要考虑网关自身的灰度上线，流量灰度引流到新的网关，网关功能逐步开启等。</p><p>&nbsp;</p><p>InfoQ：大家要考虑的点还是很多的。</p><p>&nbsp;</p><p>易久平：这也体现出了网关的重要性，你必须得谨慎。虽然大家在讲云原生，但我理解并不是所有企业都已经完全切换到了所谓的云原生体系里面去，还有一部分历史的系统要兼容，还有多集群要考虑，甚至得考虑多部门的协同，这些问题也是避免不了的。</p><p>&nbsp;</p><p>InfoQ：NGINX有采取什么措施来降低开发者使用门槛吗？</p><p>&nbsp;</p><p>易久平：NGINX近期开源了一个Agent项目，其可作为NGINX的控制面工具使用，通过Agent管理NGINX实例，可降低开发者门槛。另外，在Kubernetes集群里，通过实现Ingress&nbsp;与Gateway&nbsp;API规范，让网关的配置方式符合云原生标准，开发或运维人员可通过Kubernetes原生方式管理和配置网关。底层的实现逻辑是，Ingress&nbsp;Controller充当了网关控制面的角色，其与Kubernetes&nbsp;API Server对接，自动监听配置变化，并将配置转换成NGINX原生配置，可能通过配置文件生成方式，也可能调用NGINX实例API动态修改。Gateway&nbsp;API或Service&nbsp;Mesh本质都是实现了不同的控制面，并与NGINX数据面配合简化NGINX的使用难度。由于这些项目都是开源的，开发者可基于此实现更多自定义功能。</p><p>&nbsp;</p><p></p><h2>未来的网关竞争</h2><p></p><p>&nbsp;</p><p>InfoQ：您如何看待未来网关市场的竞争？</p><p>&nbsp;</p><p>易久平：网关的竞争我想从来就没有停止过，这块市场很大，玩家也很多。最近几年企业都在做数字化转型，大量的应用与API出现，同时应用的技术架构也变的越来越复杂，对于各类网关的需求非常大。我们可以参考IDC在2022年10月发布的《IDC Market Glance: Integration and API Management, 2022》，其中提到API管理与集成总潜在市场将以15.8%的复合年增长率增长，到2026年将达到159亿美元的规模。正是因为有很大的市场前景，才有这么多玩家在里面努力去打下自己的一片天地。</p><p>&nbsp;</p><p>除了市场规模，Garner在2022年发表了一份市场报告《Market Guide for API Gateways》，站在企业组织架构及市场分类的角度来定义网关的层级，最外面一层叫边缘网关，再下面一层叫企业网关，企业网关下面有部门级的网关，部门级的网关下面有微服务的网关，还有内嵌网关等。如下图所示：</p><p><img src=\"https://static001.geekbang.org/infoq/71/71e2311bd6c98244714cb5ecd0a00da1.png\" /></p><p>在不同层级的网关市场，有不同的企业在主导。像边缘网关，可能像F5或是传统的ADC厂商是比较强的，这类网关对性能、稳定性、安全性要求都比较高。但是像企业级的网关，或者部门级的网关，强调的是整个API生命周期管理、API编排与聚合等这类能力，可能其他的一些厂商在发力。还有基于API网关2.0阶段的微服务架构入口这类网关，又有不同的厂商在里面角逐。</p><p>&nbsp;</p><p>总体来看，我觉得Gartner市场角度的网关分类还算比较合理，基本上通过层级把各个厂商区分出来，只是可能有的厂商并不只在某个纬度发力，而是跨边界的。但大家总体来说，只是各自擅长的领域会有所不同。</p><p>&nbsp;</p><p>InfoQ：NGINX未来有哪些计划来增加自身竞争力？</p><p>&nbsp;</p><p>易久平：我们官网上有一张图，从CDN的内容缓存和接入网关开始，再到前面说的API网关，再到Service Mesh的Sidecar，你会发现，NGINX的产品线很完整，使用场景很多，覆盖面已经很广。</p><p>&nbsp;</p><p>以下是产品分类：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a5555374667a1cb0af178b133f85bee.png\" /></p><p>&nbsp;</p><p>&nbsp;</p><p>以下是使用场景：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c0/c02674118f566d28d4866bfb2edf8e9f.png\" /></p><p>&nbsp;</p><p>&nbsp;</p><p>在云原生提体系下我们主要做三件事：</p><p>&nbsp;</p><p>第一，帮助K8s建立起高性能、高安全、高可观测性的网络连接能力，包含南北向与东西向。东西向通过Service&nbsp;Mesh去解决，南北向通过Ingress Controller 或者&nbsp;Gateway &nbsp;API 解决。那么，安全体现在哪？所有有NGINX的地方都可以部署NGINX相关的安全模块，其基于零信任与WAAP体系实现较为全面的安全能力。</p><p>&nbsp;</p><p>第二，帮助K8s安全且高性能的管理集群内与集群外的API。不管是在&nbsp;Kubernetes集群里面还是外面，都能做好高性能的管理，就是在安全高性能的网络问题解决后，再提供API管理的能力。</p><p>&nbsp;</p><p>第三，帮助K8s提升系统可靠性与韧性，实现跨集群或跨云的伸缩。</p><p>&nbsp;</p><p>总体来讲，我们的整体战略很明确：拥抱开源，拥抱云原生。</p><p>&nbsp;</p>",
    "publish_time": "2023-05-24 09:36:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "上百+应用运维面临挑战，开源“走不通”｜广州银行信用卡中心的自动化运维实践",
    "url": "https://www.infoq.cn/article/xqiKcXDU2QhwmkCRy6c4",
    "summary": "<p>采访嘉宾｜陈晓岚，广州银行信用卡中心运维与信息安全技术负责人</p><p></p><p>在全国地方城商行中，<a href=\"http://www.gzcb.com.cn/\">广州银行</a>\"是全国第二家成立信用卡中心的城商行。</p><p></p><p>因为信用卡业务的特殊性，往往会促使商业银行在信用卡方面的科技应用进度条要快于其它业务。比如，由于信用卡没有网点，这就使得它更早实现线上化运营；再比如，信用卡业务与个人征信、消费分期、移动金融等都有着强关联，加上在审批、风控、运营等方面的高要求，使得信用卡中心的数字化、自动化步伐也迈得更快。</p><p></p><p>广州银行自2011年开始发行信用卡；2016年，专门成立了信用卡专营机构，即广州银行信用卡中心。而自广州银行信用卡中心成立以来，<a href=\"https://www.infoq.cn/article/lk8j300YC47WfAc9H497\">金融科技</a>\"便在信用卡业务的各个环节扮演着关键角色。</p><p></p><p>举例来说，在信用卡风险管理场景，通过引入大数据和AI技术，广州银行建立了一套精细化的风控体系。在营销展业环节可以了解客户属性，进行风险前置；同时，在催收环节，也可以实现智能催收，基于标签提升催收效率，并持续优化催收策略。</p><p></p><p>值得关注的是，作为零售业务，广州银行信用卡中心面对的是数量众多的零售客户。随着近年来信用卡业务的持续增长，以及产品类型的逐渐增加，要确保精细化且高效的管理和运营，对底层系统稳定性、灵活性、扩展性要求越来越高，传统IT系统运维的问题也逐步暴露出来。</p><p></p><p>在这样的背景下，广州银行信用卡中心在去年上线了<a href=\"https://xie.infoq.cn/article/d5d680d04e4b171153335f62c\">自动化运维</a>\"平台，进行IT运维升级。围绕这一平台构建的初衷、实践历程以及背后的思考逻辑，InfoQ采访了该项目的产品规划和技术统筹实施负责人——广州银行信用卡中心运维与信息安全技术负责人陈晓岚。</p><p></p><h3>涉及多个应用系统，发布效率和质量面临巨大挑战</h3><p></p><p></p><p>虽然广州银行立足于广州，但是其业务范围辐射整个珠三角，除此之外，在南京也设有分行。这意味着，广州银行不仅要跟本地银行竞争，还要跟全国性的国有大行和股份制银行竞争。如何在这样的激烈竞争中赢得更多的市场份额，作为城商行必须找到具有特色和个性化的突破点。</p><p></p><p>据陈晓岚介绍，广州银行信用卡中心的业务主要面向C端客户，需要对客群进行更精细化的管理和运营。</p><p>“一方面，既需要我们能够把更优质的客户筛选出来；另一方面，也要能够对个人在贷前、贷中、贷后全流程的风险进行管控，包括贷前的获客，贷中的客群分类、产品推荐、风险分级，以及贷后的回款追踪等等。在这个过程中，涉及一系列精细化的运营动作，这些动作需要对应的系统去做支撑。”</p><p></p><p>陈晓岚告诉InfoQ记者，过去，广州银行信用卡中心大大小小的应用系统较多，包括不同的类型，面向不同的业务模式服务。并且，消费类信用业务需要根据市场环境、监管要求频繁做调整。</p><p></p><p>在自动化运维平台上线之前，广州银行信用卡中心的大部分应用系统发布都靠人工部署，一旦涉及变更的系统数量比较多，其中的工作量和耗时可以想象，应用的整体发布效率面临巨大挑战。</p><p></p><p>另一个挑战来自于发布质量。周期性的任务执行，比如脚本和配置等等，如果不进行集中、系统化的统一纳管，可能造成系统对人的依赖性很高，如果出现人员交接，或者需要对突发问题进行排查，效率比较低，系统稳定性也可能会受到冲击。</p><p></p><p>此外，系统非常依赖运维管理的完善性——比如，发布版本包命名管理规范；以及发布过程需要有效的回溯、审计措施，需要及时对投产日上线发布成功与否进行有效的统计等等。这一系列的管理如果存在问题，将影响应用的发布质量。</p><p></p><p>“繁琐复杂的运维工作，将会使得人力无法释放，可能导致每一次投产，投入的运维时间成本和人力投入非常大。”陈晓岚强调，“这就是我们去建设自动化运维平台的一些导火索。换句话说，前台业务的变化对后台的要求越来越高，我们必须不断适应业务场景的需求，持续提升开发运维效率。”</p><p></p><h3>尝试基于开源做自动化发布编排，不断尝试调整适应</h3><p></p><p></p><p>系统的稳定性、灵活性、扩展性以及响应速度是保障业务高可用性的重要前提。陈晓岚说，这些要素就是广州银行在做技术选型的过程中非常看重的部分。</p><p></p><p>也正因为如此，广州银行信用卡中心区别于传统的商业银行信息系统建设思路——强调开发和管理流程建设，而弱化运维工作的优先级。“我们认为，即便是后端运维，一旦出错，对生产环境的维护效率也会带来巨大影响。所以，我们一直把前端开发和后端的运维、流程管理视为一个密不可分的整体，在每个环节都会持续去做迭代优化，而不是更侧重哪一部分。”陈晓岚表示。</p><p></p><p>事实上，早在此次自动化运维平台上线之前，广州银行信用卡中心就在自动化运维方面做了大量探索和研究工作。时间最早可以追溯到2018年，陈晓岚及其运维团队就参考了当时互联网公司自动化发布的模式，基于<a href=\"https://www.infoq.cn/minibook/EA3IfyXsCLTzSzJNc10z\">开源</a>\"技术去做自动化的发布编排。</p><p></p><p>陈晓岚向InfoQ介绍，最开始团队之所以选择使用开源工具，主要原因来自几个层面：第一，开源的方式可以快速部署和落地，帮助前端业务更快去做模式的探索和实验；第二，开源技术比较灵活，不涉及太多的选型，有一定的试错空间，如果部署后发现不可行，就能立即更换，不会浪费太多资源；第三，希望基于开源，对内部的技术人员有一些能力方面的提升。</p><p></p><p>然而，银行和互联网公司的业务特点毕竟有着明显差别，开源模式需要结合实际情况不断调整适应。比如，互联网公司大部分应用都部署在<a href=\"https://www.infoq.cn/article/69a7zS7vfptq4azqGkq5\">公有云</a>\"上，在基础平台层面基本不需要做管理，因此不存在管理方面的挑战；再比如，互联网公司对安全性的要求没有银行那么高，物理环境也相对简单。</p><p></p><p>而银行的物理环境、技术栈远远要比它们复杂得多，通过开源自研的自动化发布生产线上线之后，能够对接的系统有限，根本无法做到平台化，并且随着系统越来越多，问题就越来越明显。</p><p></p><p>“所以，我们引进外部自动化运维平台，把所有的应用系统发布全部放到平台上统一进行管理，就是希望应用发布同时兼顾稳定和效率，实现标准流程化、应用发布自动化的目标。同时，也希望能够基于这些商业化平台的案例实践探索，更好地解决我们面临的一系列问题。”陈晓岚指出。</p><p></p><h3>从事前、事中到事后落实技术管控和流程管理，确保系统稳定性</h3><p></p><p></p><p>那么，广州银行信用卡中心是如何结合自身情况对自动化运维平台的技术路线和产品进行选型的？陈晓岚表示，当时团队主要围绕以下四个方面做了着重考量：</p><p></p><p>第一，由于场景很复杂，所以这个平台必须在比较多的企业场景有过实践和验证；</p><p></p><p>第二，它必须能够满足多种技术栈的发布需求，包括蓝绿发布、灰度发布和滚动发布等等；</p><p></p><p>第三，这个平台需要具备一定的操作便利度，比如能够通过低代码支持我们快速实现编排，当有系统新增的时候，也能快速进行操作；</p><p></p><p>第四，这个平台本身的源代码能够一定程度上做开放，因为其业务调整比较频繁，这样的话便与做一些二次开发。</p><p></p><p>从这几个角度出发，最终广州银行信用卡中心通过打造应用发布自动化能力建设，启用应用发布中心（如下图）。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/4c/2b/4ce3d2317fedf5e1be48b662934c392b.png\" /></p><p></p><p>在陈晓岚看来，自动化运维项目的上线和实施，非常核心的一个点在于，不能为了自动化而做自动化，而是企业在其中确实有比较大的痛点。</p><p></p><p>如前面介绍，结合系统多而且复杂的现状，避免因手工操作影响效率的情况。为了简化多技术栈的问题，同时分担运维人员的工作压力，引入自动化运维平台。“因此，我们的规划其实也是围绕这些点展开的。先明确目标，然后分步骤落实。在这个过程中除了技术之外，还会涉及一些配套的管理模式的优化，需要从管理效率上做到整体提升。”</p><p></p><p>值得注意的是，广州银行信用卡中心建设的是一个“平台”而非“工具”，这个平台目前主要包括了配置中心、应用发布、作业平台、标准运维等主要的部分。“现在，我们所有的开发运维工作基本上都是以它为基础去实现的，包括生产的发布、生产维护、版本发布，以及版本回退等等，基本上相当于一个底座的角色，基于整体的底座，后续还会有一系列的功能拓展。”陈晓岚表示。</p><p></p><p>既然是“底座”，系统运行的稳定性必是首要前提。对此，广州银行信用卡中心的核心思路是技术管控和流程管理并重，根据时间条线，把技术管控和流程管理分为事前、事中、事后三个关键步骤，并且层层落实：</p><p></p><p>首先，事前要重预防，这跟基础架构的整体设计、系统研发的高质量保障等等有很大的关系。拿基础架构来说，如果最开始没做好，后面肯定就会有很多修修补补的动作，需要人的介入，从而引发后续一系列的连锁反应。陈晓岚表示，如果这个环节能够做好，基本上就解决了70%的稳定性问题；</p><p></p><p>其次，事中要重监控，持续关注隐患发生的可能性，并在发生之后快速进行处理和修复，这个环节对稳定性的影响占20%。通过自动化运维平台，能在这个过程中快速实现重启和切换，减少人工操作，让业务稳定性更高；</p><p></p><p>最后，事后要重闭环，也就是说在问题发生之后，要跟踪复盘，形成闭环机制，找出根本原因是什么。</p><p></p><h3>管理和技术的“坑”都踩过，平台已成为业务创新的扎实“后盾”</h3><p></p><p></p><p>回顾整个项目的实施落地历程，陈晓岚坦言，自己和团队也踩过不少坑，其中的难点主要来自两个方面：</p><p></p><p>一方面与管理相关，因为项目涉及的团队和人员角色比较多，需要多方协同，期间涉及很多沟通协调问题；另一方面与技术相关，此前的系统应用发布顺序不一样，并且很多都归属不同的安全区域，要把它们统一放到平台上，会涉及很多技术方面的调试和任务拆解。</p><p></p><p>陈晓岚强调，“当然，其中技术其实是相对可控的，比较好解决。但‘人’的问题是比较棘手的。针对这个问题，管理层面的规范非常重要，需要通过固定的流程，用比较长的时间慢慢改变人的习惯。 我们自己也在这一方面花费了比较多的时间，包括做一些技术上的适应，以及管理上协调。”</p><p></p><p>举例来说，针对人的问题，广州银行信用卡中心在内部成立了一个大项目组，把关联的核心人员都拉进来，主要涉及软件工程管理及业务务团队中的关键角色。</p><p></p><p>通过这种方式，职责更加明确，团队之间可以更好地进行横向和向上的沟通。“其实很多时候大家并不是不想配合你做事，而是在很多事情不明确的状况下，他很难理解具体的目的和意图，导致项目推进困难。而有了项目组，就可以拉齐大家的意识，分工更加明确，项目推进起来就会顺畅很多。”</p><p></p><p>如今基于自动化运维平台，广州银行信用卡中心完成了多套业务系统的应用发布，支撑近百次发布任务，几百+应用节点，接入主机数几百+，大幅提升了生产上线交付效率，实现有质量的应用敏捷交付。</p><p></p><p>陈晓岚认为，最直接的两点改变，一是应用整体发布流程更规范，二是发布和运维效率提升比较明显。比如，可以实现一键重启自动化安装、自动化回退等等，并且在处理故障的过程中，速度也更快。以前端移动展业系统为例，此前整个流程全部走完，一次大概需要一个小时，现在通过自动化运维平台，5分钟左右就能搞定。</p><p></p><p>反馈到业务层面，如此高效的应用发布意义重大。“首先，快速的应用交付，可以帮助业务快速迭代、优化，包括在这个过程中，如果稳定性可以保障，那么也能给业务人员更多的信心，更大胆、更高频地去做创新；其次，当系统出现异常，我们可以快速通过一键回退解决，这中间可以减少很多故障时间，减少对业务的次生伤害。”陈晓岚说。</p><p></p><p>换言之，如今广州银行信用卡中心的自动化运维平台，已然成为其前端业务快速试错和创新的扎实“后盾”，是其<a href=\"https://www.infoq.cn/theme/192\">数字化</a>\"转型过程中不可或缺的一块基石。</p><p></p><p>参考链接：</p><p><a href=\"https://mp.weixin.qq.com/s/FG57squ5JKw1591vCB9k0A\">https://mp.weixin.qq.com/s/FG57squ5JKw1591vCB9k0A</a>\"</p><p><a href=\"https://www.fddnet.cn/2021/jiaodian_0616/784.html\">https://www.fddnet.cn/2021/jiaodian_0616/784.html</a>\"</p>",
    "publish_time": "2023-05-24 09:38:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数字农业先锋：温氏股份的数字化转型案例研究｜《行知数字中国》完整版",
    "url": "https://www.infoq.cn/article/PtWLsoZYiPyYc5A015d5",
    "summary": "<p>走进温氏股份，了解数字化时代的农业智能化探索与实践。</p>\n<p>虽然坐落于云浮新兴这个当时还是一个小乡镇的偏远地区，但温氏始终遵循“用科学 办实事”的精神，与时代同频共振。在数字化成为企业“必修课”的今天，温氏早已打下 30 多年的信息化基础，从上世纪 90 年代开始使用计算机记账，到后来又成为最早涉足数字化转型的农业企业。</p>\n<p>让人尤为好奇的是，作为一个农业企业，驱动温氏敢为人先落地信息化、数字化的原因是什么？在这个过程中，温氏又是如何思考并实践数字化的？本期视频带你深入了解。</p>\n<p>精剪版观看：<a href=\"https://www.infoq.cn/video/1mdmikVizJFiI3WS5oij?utm_source=album_info&amp;utm_medium=articlel\">《从8000元起步到年产值超800亿，藏在郊县里的农牧数字化探索者》</a></p>",
    "publish_time": "2023-05-24 11:32:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]