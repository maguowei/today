[
  {
    "title": "突破调试瓶颈：Uno平台的VS Code扩展支持调试移动应用程序",
    "url": "https://www.infoq.cn/article/vUTZv9GP8T8P4eLWaGBH",
    "summary": "<p><a href=\"https://platform.uno/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Uno</a>\"，一个用C#和XAML构建原生移动、桌面和WebAssembly应用程序的框架，<a href=\"https://platform.uno/blog/announcing-net-mobile-debugging-in-vs-code-mobile-development-in-vs-code-with-uno-platform-or-net-maui/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">发布了新版</a>\"的Visual Studio Code扩展。新版本增加了对构建移动应用程序以及直接在Visual Studio Code中调试应用程序的支持。</p><p></p><p>使用Uno的.NET开发人员现在可以直接在Visual Studio Code中构建、运行和调试移动应用程序。它支持Visual Studio Code可用的所有调试特性，例如设置断点、条件断点或在异常时中断。这个扩展还支持<a href=\"https://code.visualstudio.com/docs/editor/debugging#_logpoints?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">打印消息</a>\"，无需进入调试器和检查变量。开发人员可以对自己的代码使用这些特性，也可以通过<a href=\"https://github.com/dotnet/sourcelink?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">SourceLink</a>\"或<a href=\"https://github.com/dotnet/roslyn/issues/12625?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">嵌入PDB文件</a>\"的方式对第三方代码使用这些特性。</p><p></p><p>由于Uno应用程序和.NET MAUI应用程序是在同一个.NET基础上构建的，因此开发人员也可以使用<a href=\"https://platform.uno/vs-code/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Uno扩展</a>\"来构建和调试.NET MAUI项目。</p><p></p><p>除了调试之外，该扩展还为XAML提供了代码补全和热重载功能，C#的热重载功能预计将在即将发布的版本中推出。这些特性可用于<a href=\"https://platform.uno/docs/articles/getting-started/requirements.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Uno支持的所有类型的项目</a>\"。此外，如果开发人员使用的操作系统不支持他们正在构建的目标，他们可以<a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">连接到Visual Studio Code的远程实例</a>\"并在那里运行项目。例如，Windows或Linux开发人员可以调试运行在远程macOS机器上的iOS或Mac应用程序。</p><p></p><p>调试移动应用程序是社区中呼声较高的一个特性，而社区也热情地迎接调试移动应用程序的特性发布公告。在推特上，<a href=\"https://twitter.com/UnoPlatform/status/1653457840600764417?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">发布公告推文</a>\"收到了200多个点赞和正面评论。微软高级内容开发者<a href=\"https://twitter.com/alvinashcraft/status/1653463300024201218?s=20&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Alvin Ashcraft说</a>\"：“这太棒了！我等不及要试用它了。”</p><p></p><p><a href=\"https://www.reddit.com/r/dotnet/comments/135wmbc/full_net_mobile_debugging_in_vs_code_with_either/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Reddit的.NET社区</a>\"也表达了同样的观点。<a href=\"https://www.reddit.com/user/pinedax?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">用户pinedax分享</a>\"了他们使用这个扩展的经验：“它也可以在Linux上运行。”</p><p></p><p>对这个Uno扩展的积极反响反映在它在<a href=\"https://marketplace.visualstudio.com/items?itemName=unoplatform.vscode&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Visual Studio Marketplace</a>\"的评分上，该扩展的下载超过了10000次。</p><p></p><p>Uno是开源的，其Visual Studio Code扩展是免费的，但是是闭源的。Uno团队呼吁开发人员帮助测试其新功能，并<a href=\"https://github.com/unoplatform/uno/discussions?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">在GitHub上分享反馈</a>\"。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/uno-platform-debugging/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">https://www.infoq.com/news/2023/07/uno-platform-debugging/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/zi0OdBgVJuoVsqc2S4rB\">VS Code有多么不安全：一个扩展就可能导致公司GitHub中的所有代码被擦除？</a>\"</p><p><a href=\"https://www.infoq.cn/article/4zm3T5aeOKzej15X9YKR\">另一种“推翻”&nbsp;VS Code 的尝试：JetBrains Fleet 现开放公测</a>\"</p>",
    "publish_time": "2023-07-17 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "架构师特刊：天工开物 AIGC",
    "url": "https://www.infoq.cn/article/CTbOrxaOBwlDXwCYw6ae",
    "summary": "<p></p><h2>卷首语</h2><p></p><p>史学家把十六、十七世纪的中国称为“天崩地解”的时代，社会经济关系、工农业内部结构发生变化，科学技术快速发展，因此，民间诞生了《天工开物》、《本草纲目》这样的奇书。三百余年过去了，我们再一次来到了科学技术重构社会经济关系的拐点——AIGC，这个最有想象力的词正在以巨大的影响力，渗透到各行各业。在这样的背景下，为了给大家带来经得住“拷问”的高质量内容，InfoQ 发起了《天工开物 AIGC》架构师特刊。</p><p></p><p>在内容设计方面，我们决定从“水面上”、“水面下”两个维度着手。在“水面上”的部分，我们整理 AIGC 领域近期的热点新闻和测评，比如 ChatGPT 进入车载系统的消息、Google 等大厂对 AIGC 辅助开发的态度，希望借此最大限度地保证你的知情权。或许当下所处的行业、所任职的岗位并未受到 AIGC 太猛烈的冲击，但中立的、实时的内容报道，可以保证你时刻观测 AIGC 的最新行业进展，在变革到来之际早做准备。</p><p></p><p>在“水面下”的部分，我们更注重弥合信息差，击破信息墙。近期有很多企业家在组织企业团队做调研，也有技术专家、产品专家远赴硅谷走近 OpenAI，建立了对 AIGC 更加深度的认知。我们借 Prompt 工程师的发展前景、AIGC 辅助编程实践、AIGC 大模型算力优化、AIGC 重构智能客服等几个维度，联系相关企业、负责人，做了深度访谈，希望有条理地、尽可能地把行业第一手信息同步给更多的读者。</p><p></p><p>从实际访谈、调查结果来看，各行业对 AIGC 的理解和实践是超乎想象的，从去年 12 月，InfoQ 于国内第一次报道 ChatGPT 爆火至今，短短七个月，很多研发团队已经积累了成熟的 Prompt 方法论，在在线设计、在线文档、金融等领域做出了诸多产品研发尝试，甚至研发团队已经非常习惯 Copilot 的辅助。与过往的元宇宙、Web3 相比，AIGC 的影响更为深远，变革更为迅速。动作最为迅速的企业，其 AIGC 应用已经发布上线；Prompt 工程师岗位已经完成 JD 设计，并面向社会开始招聘。</p><p></p><p>此外，我们特别发起了四期直播，针对以上话题，邀请了行业十余位有实操经验的技术高管、骨干圆桌讨论，超过三十万人在 InfoQ 视频号观看了这些直播。直播的回放我们一并附在本期架构师特刊中，以飨读者。</p><p></p><p>这期特刊包含的内容很多，但我们不想给大家制造焦虑。信息的透明与同步，带来的应该是真正的平和。我们希望在这个信息爆炸的时代，去伪存真，通过经过调查的内容，给你带来足够的信息增量，有足够的参考价值。帮助你更便捷地享受这一波科技红利，更大程度地促成技术普惠。</p><p></p><h2>目录</h2><p></p><p>热点 | Hot</p><p></p><p>用 AIGC 写 2023 高考语文作文，结果如何？</p><p></p><p>程序员越“老”就越看不上 AI 辅助编程工具？Stack Overflow 2023 开发者调查 AI 特别报告</p><p></p><p>ChatGPT 正式进入车载系统：奔驰首测 AI 语音助手，可进行复杂对话</p><p></p><p>谷歌警告自家员工：不要使用 Bard 生成的代码</p><p></p><p>微软也搞起了开源小模型！利用 OpenAI 的 ChatGPT 和 GPT-4 训练，实力碾压当前最强开源模型</p><p></p><p>AIGC 领域最大收购：Databricks 花费 13 亿美元买下只有 15 名研发的小公司！</p><p></p><p>专题访谈 | Interview</p><p></p><p>实访用人单位：Prompt 工程师真是低门槛“香饽饽”？</p><p></p><p>神器还是垃圾？那些用 AIGC 编程的人，实践得怎么样了</p><p></p><p>AI 大模型竞争白热化，算力优化才是“超车点”？</p><p></p><p>AIGC 重构智能客服，能否淘到大模型时代的第一桶金？</p><p></p><p>要想用好 GPT，我们必须跑得比“黑客”更快</p><p></p><p>探索大模型智能：众安保险基于 AIGC 的应用实践</p><p></p><p>推荐文章 | Article</p><p></p><p>GitHub Copilot：做出一个划时代的产品，只需要 6 个人</p><p></p><p>大语言模型进化之谜：涌现现象的挑战与争议</p><p></p><p>编程已死，AI 当立？教授公开“唱反调”：AI 还帮不了程序员</p><p></p><p>LangChain：2023 年最潮大语言模型 Web 开发框架</p><p></p><p>扫码下载</p><p><img src=\"https://static001.geekbang.org/infoq/98/98e4e92be2603b6c09b1b8e4a363555c.png\" /></p><p></p>",
    "publish_time": "2023-07-17 09:51:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据库行业的新篇章：国产化发展潜力几何？",
    "url": "https://www.infoq.cn/article/Au8A0GLJcHPNJQp3Vizx",
    "summary": "<p>近年来，全球范围内创新型数据库企业和产品不断涌现，我国数据库产业和生态日益繁荣，正在进入高质量发展期。《数据库发展研究报告（2023 年）》显示，2022 年我国<a href=\"https://xie.infoq.cn/article/9b1b88ee2bf5190a4f2e7489c\">公有云</a>\"数据库市场规模首次过半，预计 2023 年公有云市场占比将进一步扩大达到 59.8%。腾讯云数据库总经理王义成表示，这个预测结果非常合理：“云数据库分为公有云和私有云两种形式。私有云是建立在云厂商基础上的，例如中国建设银行、中国银联、数字广东和浙江政务等企业都是使用腾讯云、阿里巴巴和华为等云厂商提供的私有云进行业务孵化。公有云市场仍然在持续增长，但纯数据库软件市场将会逐渐稳定甚至逐步下降。”</p><p></p><p>当下，国内企业对云数据库的关注度有所增加，譬如快手和美团等大型企业逐渐放弃自建的 IDC 部署转向公有云。在金融和政务领域，一些创新型的企业会选择公有云或行业云来满足其业务需求，而对于核心业务要求稳定性和安全性的企业则相对较难改变现有的机房部署方式。总体而言，云数据库的发展将继续增长，特别是公有云市场，随着云的便捷性和弹性优势的日益突出，越来越多的行业和企业将选择将其数据迁移到云上。</p><p></p><p>在这种趋势下，2023 年 7 月 4-5 日，由中国信息通信研究院、中国通信标准化协会指导，中国通信标准化协会大数据技术标准推进委员会（CCSA TC601）与 InfoQ 联合主办了“2023 可信数据库发展大会”，腾讯云数据库总经理王义成在本届大会上发表了《腾讯云数据库 TDSQL 助力金融业核心系统国产化转型》的主题演讲分享，并在会后接受了 InfoQ 的专访。</p><p></p><p></p><h2>一、数据库国产化进程显著提升</h2><p></p><p></p><p>“十四五”规划提出“加快数字化发展”的总体布局，数据库作为金融信息系统的关键基础设施，其行业景气度持续提高，数据库国产化趋势愈发明显，上百家金融业试点单位在数据库国产化的进程中，进一步增强信心，向 50% 国产化率大步迈进。</p><p></p><p>如今大批量的应用其实都是基于 Oracle 构建的，在国产化替代过程中，厂商就要提供很多在驱动、内核、资源、架构等层面与 Oracle 兼容的能力，构建全面的 Oracle 兼容解决方案，从而实现迁移、平滑的双轨运行以及互相切换等。</p><p></p><p>关于如何更好地从Oracle手里“接棒”，王义成在接受 InfoQ 的采访中表示，“国产数据库与 Oracle 在数据库核心能力上差距其实不大，目前国产数据库可能更多专注于自身，但是客户看待数据库是一体化的视角。以排查处理故障为例，数据库厂商应该跳出自己，去沉淀全套故障排查和高可靠的白屏化能力，这样双方的人力和效率都能有大幅提升。数据库绝非是数据库本身，需要考虑如何从功能、架构、优化、运维等全方位角度帮助用户使用好数据库，这样厂商才能走得远。”</p><p></p><p></p><h2>二、腾讯云数据库 TDSQL 在金融行业展现国产数据库实力</h2><p></p><p></p><p>当今，在以数据要素驱动、数据价值实现为核心的数字化转型大潮中，企业如逆水行舟，不进则退，如果不跟上数字化转型的步伐，终将会被用户抛弃、被竞争对手超越、被市场边缘化，以致最终出局。</p><p></p><p>落实传统数据库转型、布局智能化建设可以说是近年来金融行业的两大工作重点，腾讯云数据库一直在赋能金融行业的核心数据库替换方面“开疆辟土”，其数据库产品<a href=\"https://xie.infoq.cn/article/dcc9581dc2f67b7c70da1bc05\"> TDSQL</a>\" 在推动数据库技术实现安全可控的道路上取得了开拓式创新，已累计为 3000+ 的政企和金融机构提供数据库的公有云及私有云服务，客户覆盖银行、保险、证券、互联网金融、计费、第三方支付、物联网、互联网 +、政务等诸多领域，为国产化数据库的发展和应用积累了宝贵经验。</p><p></p><p>王义成在演讲中表示，从技术角度，金融行业核心下移数据库选型大致有两条路线。其一是，历经 30 多年全球海量场景淬炼的开源方案，其数据库核心引擎基于 B+ 树的数据结构，对磁盘亲和，并深度优化 MySQL、PostgreSQL 内核，具备研发分布式化、金融级高可用、一致性、智能运维等能力；其二多是采用新技术进行研发的 NewSQL 路线，其数据库核心引擎基于 LSM-Tree 数据结构，对内存亲和，计算层和存储层完全分离。</p><p></p><p>要知道，金融行业里的每个机构发展状况都各不相同，在数据库的选型方面也存在差异，需要针对具体应用场景对数据库能力的需求和侧重，选择适合自身的数据库产品：比如互联网银行、互联网保险在线类业务由于在扩容方面的要求比较高，会更偏向于敏态扩容方案或弹性方案；又如，一些银行传统业务会选择基于相对稳定的内核构建的分布式解决方案来做业务支撑；再如，网上交易核心、渠道核心业务会更倾向于 NewSQL 方式。</p><p></p><p>针对金融行业这种传统行业在数据库选型中的变化与差异，腾讯云数据库提供了相应的三种解决方案：</p><p></p><p>针对核心业务系统替换，提供极具竞争力的产品和定价来支撑金融、政务、保险、资管、零售这类核心业务系统的替换，在底层采用基于比较稳定的内核构建的分布式数据库架构来支撑。</p><p></p><p>依靠标准化产品的被兼容、拓展和多态部署能力，去拓展更多行业场景（如能源、交通、制造等），助力更多行业的数字化转型。</p><p></p><p>拥抱云原生，解耦存储和计算，无缝嵌入云原生开发工具，规模化满足新应用的底层数据需求。</p><p></p><p>TDSQL 作为腾讯云多年持续投入研发的数据库产品，它基于分布式架构，无论是资源还是功能都能提供良好的扩展性，通过软硬结合的方式支持读写分离、秒杀、红包、全球同服等超高性能场景，能够确保多副本架构下数据强一致性，避免故障后出现集群数据错乱和丢失；同时，通过数据库防火墙、透明加密、自动脱敏等保障企业级安全性，减少用户误操作或黑客入侵带来的安全风险；在金融级高可用方面，还具备跨区容灾、同城双活、故障自动修复等特点；此外，TDSQL 还通过智能 DBA、自助化运营管理后台等配套设施来提供便携的运维。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/55/a9/559019990060a133e753a95a471755a9.png\" /></p><p></p><p>性能：针对不同的 SQL 语句，通过基于代价和规则的判断来提高执行效率，同时优化复制机制，确保数据链路在从逻辑到物理复制过程中不受网络拖累；</p><p></p><p>稳定性：TDSQL 的 HA 模块经过长时间的调优，能够应对各种场景下的稳定性切换；</p><p></p><p>数据安全和隐私保护：在事前阶段通过配置规则、安全组、白名单、SQL 防火墙来保障数据安全性，在事中阶段通过 SQL 拦截、SQL 黑名单等措施，防止大规模脱库和攻击行为；在事后阶段，支持对事件进行溯源追溯；</p><p></p><p>数据的脱敏和加密：支持链路层加密、存储层加密和针对特定列项的脱敏加密，在演练环境中还可对指定库进行脱敏操作从而保护隐私数据。</p><p></p><p>腾讯云 TDSQL 是国产数据库艰辛爬坡的一个缩影。大家看到的，是国产数据库在各行各业的大量投产以及市场份额的逐步提升，大家看不到的，是背后无数的打磨与优化。</p><p></p><p>三年前，腾讯云 TDSQL 的兼容性还不够完善，经历了在驱动、语法、性能层面的不断打磨和攻坚，目前已经可以独当一面。比如某头部保险公司使用 TDSQL 全面替换 Oracle，目前已在线上平稳运行 24 个月以上，部署了数万核心；又如，阳光保险 OA 系统凭借 TDSQL 的高度 Oracle 兼容能力，实现了业务短期迁移上线，生产环境更新表结构效率提升 50%；再如基于 TDSQL-C 的查询能力，能够打破 CPU 单核限制，实现整体的性能提升。</p><p></p><p>除了国内市场的数据库国产化替换，中国数字技术在出海方面也取得了不错的成绩：实现金融服务闭环，海量数据、高频交易、网银业务是印尼银行新项目面临的巨大挑战，TDSQL 以专业的可靠性、安全性和体验为印尼银行的数字银行核心系统赋能，将 TDSQL 纳入其核心系统后，2022 年印尼银行每天可以处理 200 万笔交易和 150,000 笔贷款支付，TDSQL 的高性能也为印尼银行在一年内获得 2000 万用户和 3500 万账户的业务爆发式增长提供了有力支持。</p><p></p><p>TDSQL 多次入围 Gartner 全球数据库魔力象限，其 OLTP 能力获得了国内第一的好成绩，是如今金融行业核心系统替换过程中的优选数据库。在前段时间的 TPC-C 打榜中，TDSQL 性能达到每分钟 8.14 亿笔交易（tpmC），打破了世界记录，同时也构建了业界最大的分布式集群，平均时延等领先其它厂商；在性价比方面，通过软件优化降低了硬件成本，通过公有云模式降低了服务成本，仅用行业三分之一的单位成本，就扛住了更大规模的并发，实现了超大规模集群性能稳定性，在 8 个小时持续压测过程中，tpmC 波动率一直处于 0.2% 以下，远低于标准的 2%，刷新了全球数据库极限，这充分证明了 TDSQL 承载金融核心场景的能力，这既给国产数据库的研发增强了信心，也给国产数据库的使用者增强了信心。</p><p></p><p></p><h2>三、国产数据库发展需要全行业共同努力</h2><p></p><p></p><p>《数据库发展研究报告（2023 年）》显示，中国数据库厂商有 150 家，中国市场虽然很大，但让每家厂商都能存活下来却异常艰难，想要在群雄逐鹿的中国数据库市场跻身前列，交付成本和升级成本是两大亟需解决的事情。得益于云厂商升级过程全部依赖于原厂能力这一天生优势，相较于传统独立的数据供应商，云厂商的升级成本很低。而在交付成本方面，构建生态是非常关键的因素，有多少合作伙伴愿意在上游做适配在下游拥抱数据库做数据支撑非常关键。</p><p></p><p>据此，腾讯云认为绑定合作生态是非常重要的，未来腾讯云将打造以 TDSQL 为中心，上下游产业协调生态圈，抓住国产化软件替换的大潮，在操作系统、芯片及整机厂商、中间件厂商做兼容性适配，在集成商、认证服务商，共同打造 TDSQL 服务生态环境，促进数据库技术的快速演进。</p><p></p><p>腾讯云 TDSQL 通过其稳定性、功能丰富和易用性等优势，在国产化数据库的竞争中，为客户提供了可靠的数据库解决方案。比如第七次全国人口普查从线下搬到了线上，数据库需要承载全国十多亿人口数据，并要求在 15 天内完成数据的高速入库汇聚，支持海量数据的分析，无论是数据量还是并发度，靠传统集中式数据库无法解决这一难题。TDSQL 基于多年打磨的 HTAP 混合负载能力，在 2020 年底完整支撑了第七次人口普查海量数据高速入库和海量数据的多维统计分析。此外，TDSQL 还参与了首家运营商核心系统数据库分布式国产化改造项目，TDSQL 团队利用兼容工具进行数据迁移，并逐步提高产品语法的兼容性，最终在项目中贡献了 98% 的兼容性，对于剩下的 2%，TDSQL 团队与开发商合作进行了改动，收获了客户的认可。</p><p></p><p>目前我们已经观察到，当前企业对于国产数据库的选型能力正在逐步提升，不同行业的企业已经具备了一定的判断力和自主选择性。拿金融行业来说，无论是大型机构还是中小型企业，都具备了较为成熟的选型能力，其中一些大型机构更倾向于自主制定选型策略。而中小型机构则会参考其他同类机构的选型经验，以降低选型风险。王义成在采访中的回答也印证了这一观点：“在其他行业中，大企业通常会参照其他大企业的做法，选择被广泛认可的数据库，以优化数据处理和提升业务效率。例如，当某行业的一家领先企业选用了某款数据库产品后，其他企业也会倾向于选择相同的数据库，以保障数据处理的稳定性和安全性。”</p><p></p><p>但不管怎么说，云数据库国产化已经走上了高速发展的阶段。从企业应用层面来看，整个国产化替换都已经到了一个关键时期，厂商单打独斗的力量总是有限的，只有各数据库厂商团结上下游的力量，共同应对技术瓶颈，才能实现国产数据库的技术突破。</p><p></p><p></p><h2>四、国产数据库的下一个竞技场在哪？</h2><p></p><p></p><p>随着 5G、云计算、大数据等的跨越式发展，数据库技术不断演进。王义成在接受采访时表示，在当前的数据库发展中，有两个比较明确的方向——分布式数据库和云原生数据库，这两个方向已经得到广泛应用，并取得了成功的发展；另外，在非关系型数据库领域，出现了多种类型的数据库，如图数据库、向量数据库、键值型内存数据库和文档型数据库等，都正在“齐头并进”式发展。</p><p></p><p>其中，向量数据库是王义成比较看好的，也是当今较为热门的数据库技术之一，它在大模型应用和人工智能应用中具备一定的优势。尽管向量数据库存在发展机会，但目前它仍然处于初级阶段，而且还需要注意的是，向量数据库并不是一个可以独立主导所有数据的数据库，它更多是一个与关系型数据库和 AP 数据库相结合，提供贴近 AI 场景的实时存储功能解决方案。</p><p></p><p>目前<a href=\"https://www.infoq.cn/article/ElrMobhxqr1JJIF5rXUK\">向量数据库</a>\"技术仍需各厂商进一步探索，以在具体的垂直领域或应用场景中获得更大的爆发大，但随着大模型技术的日益火热，相信向量数据库的发展也将进入快车道，成为各厂商竞技的新赛场。</p><p></p><p>对于腾讯云来说，其正在进行向量数据库的自研工作，并于近日发布了国内首个 AI Native 向量数据库 VectorDB，可支持 10 亿级向量检索规模，延迟控制在毫秒级。相比传统单机插件式数据库检索规模提升 10 倍，同时具备百万级 QPS 峰值能力。针对大模型场景，在接入层、计算层、存储层实现了全面 AI 化，使得企业接入大模型的效率提升 10 倍。腾讯云已经在内部使用向量数据库来支持音乐搜索和其他业务应用，并计划在今年 8 月开始面向公有云客户推出公测，大家可以期待一下。</p>",
    "publish_time": "2023-07-17 09:53:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023中国企业数字化人才发展白皮书",
    "url": "https://www.infoq.cn/article/7HbjMqJvZvBlvOk0Qf0J",
    "summary": "<p></p><h1>目录</h1><p></p><p> PART 01 数字化产业与人才融合发展呈现平台化生态化特征</p><p></p><p>经济的落脚点是产业，产业的落脚点是企业，企业发展的落脚点是人才，人才发展的落脚点是人才服务尤其是人才发展服务。</p><p></p><p>产业集群数字化转型彰显了数字经济的平台化生态化特征。数字化人才在这个生态中呈网络状分布，与产业生态相匹配的数字人才发展服务生态更好地促进了产业与人才的融合发展。</p><p></p><p>PART 02 技术降低学习门槛发展需要创新探索</p><p></p><p>数字技术的成熟和普惠应用降低了学习门槛。基层人才也能快速开发相关应用。但是领导团队的思维制约了一批企业的数字化。更广泛地实现企业领导团队的数字化转型，让企业行动起来，开展各种创新探索，可以更快地发展数字化人才。</p><p></p><p>PART 03 平台赋能是数字化人才发展主旋律</p><p></p><p>平台赋能成为企业内部人才发展的趋势，平台型企业更以其率先行动促进了企业外部平台赋能的生态体系形成。在此背景下，人才发展的自主性和自驱性也被调动起来。平台赋能成为数字化人才发展的主旋律，数字化人才发展不是难题。</p><p></p><p>PART 04 不同生态位企业 数字化人才发展重点与路径</p><p></p><p>共性底座企业承担着数字经济筑底的责任，在前瞻性领导人才和尖端科技人才发展上做了充分布局，平台型企业以应用技术人才为核心向两头延伸，咨询服务企业正在大规模扩充人才队伍，数字应用企业也已经形成了丰富的标杆案例。更多的企业需要拥抱平台赋能的数字人才发展生态，实现更广泛地企业数字化转型。</p><p></p><p>扫码下载</p><p><img src=\"https://static001.geekbang.org/infoq/89/89dfb68f34fca0b8973d8b1571600d6d.png\" /></p><p></p>",
    "publish_time": "2023-07-17 10:04:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "最新进展：Serverless 冷启动优化技术",
    "url": "https://www.infoq.cn/article/2KNoj9SI8cGGM1em4Ud3",
    "summary": "<p>作者：华为云Serverless团队</p><p></p><h1>问题背景</h1><p></p><p></p><p><a href=\"https://www.infoq.cn/article/6SUNgEE6BX1xtDkRXHME\">Serverless计算</a>\"也称服务器无感知计算或函数计算，是近年来一种新兴的云计算编程模式。其致力于大幅简化云业务开发流程，使得应用开发者从繁杂的服务器运维工作中解放出来（例如自动伸缩、日志和监控等）。借助Serverless计算，开发者仅需上传业务代码并进行简单的资源配置便可实现服务的快速构建部署，云服务商则按照函数服务调用量和实际资源使用收费，从而帮助用户实现业务的快速交付 (fast built &amp; Relia. Deliv.)和低成本运行。</p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a1bf7c5f10a12d8199a499058ff8b9f.png\" /></p><p></p><p>在当前阶段，业界公认的Serverless计算的准确定义应该为“FaaS+BaaS”，即Function-as-a-Service 同 Backend-as-a-service的组合。Serverless计算改变了用户使用云的方式，为了实现细粒度的资源供给和高度弹性的扩缩容能力，Serverless计算提供了无状态函数的编程抽象，即将用户的应用程序构建为多个无状态的函数集合，并通过中间存储、BaaS等业务组件交互，继而构建完整的云应用。</p><p>&nbsp;</p><p>然而，Serverless计算的无状态函数编程在带来高度弹性和灵活性的同时，也导致了不可避免的冷启动问题。由于函数通常在执行完请求后被释放，当请求到达时，如果没有可用实例则需要从零开始启动新的实例处理请求（即<a href=\"https://www.infoq.cn/article/mI0lpQuvXi7BPCQ7oU9u\">冷启动</a>\"）。当冷启动调用发生时，Serverless平台需要执行实例调度、镜像分发、实例创建、资源配置、运行环境初始化以及代码加载等一系列操作，这一过程引发的时延通常可达请求实际执行时间的数倍。相对于冷启动调用，热调用（即请求到达时有可用实例）的准备时间可以控制在亚毫秒级。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18addb3a53ad6dbdf39404196c9da1bd.png\" /></p><p></p><p>&nbsp;在特定领域例如AI推理场景，冷启动调用导致的高时延问题则更为突出，例如，使用TensorFlow框架的启动以及读取和加载模型可能需要消耗数秒或数十秒。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/09/09e9723e875ec84cd0981d4b3052cc15.png\" /></p><p></p><p>&nbsp;因此，如何缓解Serverless函数的冷启动问题，改善函数性能是当前Serverless领域面临的主要挑战之一。</p><p>&nbsp;</p><p>从研究思路上看，目前工业界和学术界主要从两个方面入手解决冷启动问题：</p><p>&nbsp;</p><p>加快实例启动速度：当冷启动调用发生时，通过加速实例的初始化过程来减少启动时延。降低冷启动发生率：通过函数预热、复用或实例共享等方法提高实例的利用效率，减少冷启动调用的发生。</p><p></p><h1>业界最新进展</h1><p></p><p></p><p>在本篇文章中，我们首先从提高实例启动速度方面介绍下业界的前沿进展。</p><p></p><h2>提高实例启动速度</h2><p></p><p>&nbsp;</p><p>当冷启动发生时，Serverless平台内部实例的初始化过程可以划分为准备和加载两个阶段。其中，准备阶段主要包括控制面决策调度/镜像获取、Runtime运行时初始化、应用数据/代码传输几个部分。而加载阶段位于实例内部，包括用户应用框架和代码的初始化过程。在工业界和学术界公开的研究成果中，针对实例启动过程中的每个阶段都有大量的技术手段和优化方法。如下图所示，经过优化，实例冷启动的准备阶段和加载阶段时间可被极大地缩短。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c77d7f5ead9558519000935b0ce9537b.png\" /></p><p></p><p>下面列举了一些近年来发表在计算机系统领域知名会议的相关工作，主要可以分为五个方面：&nbsp;</p><p>&nbsp;</p><p>调度优化/镜像快速分发/本地池化：例如利用文件级镜像缓存复用机制和并行化镜像构建技术，加快容器构建速度 FAST-BUILD [MSST'19]；阿里云基于树结构的跨节点快速镜像分发 FaasNet [ATC'21]；AWS基于Block和分布式多级缓存的镜像分发策略 [Arxiv'23]；Pod池+特化实例跳过镜像传输 [华为云FunctionGraph]。其中，快速镜像分发依赖于VM节点的上/下行网络带宽，Pod池特化技术则是典型的以空间换时间的做法。轻量级虚拟化/安全容器：例如针对传统容器Docker的精简优化工作SOCK [ATC'21]；更侧重安全性的轻量级虚拟化技术（Kata Containers, gVisor等)；基于安全容器的进一步的精简优化工作 (Catalyzer [ASPLOS'20], REAP[ASPLOS'21])。通过裁剪优化，安全容器的启动时延最快可以被压缩至亚毫秒级。数据共享/跨节点传输优化：例如基于RDMA共享内存减少跨节点启动过程的数据拷贝 RemoteFork [OSDI'23]；或者利用本地代码缓存跳过代码传输 [华为FunctionGraph, 字节ByteFaaS等]。基于RDMA技术的跨节点数据传输时延可降低至微妙级。用户代码精简/快速加载：例如针对Java语言的JVM（Java Virtual Machine）运行时优化技术 [FunctionGraph]；以及针对Python运行时库的裁剪优化工作FaasLight [arxiv'23]。通过特定的优化，JVM启动时间可由数秒降低至数十毫秒，而Python代码的启动加载时延可降低约1/3。其它非容器运行时技术：例如WASM（即WebAssembly）技术以及针对WASM的内存隔离方面的优化工作Faasm [ATC'20]。相比容器化技术，直接以进程和线程方式组织运行函数，可在保证低开销函数运行的同时具备高度灵活性。&nbsp;</p><p>&nbsp;</p><p>接下来，针对上述5个方面的优化工作进行详细介绍。</p><p></p><h3>镜像压缩/快速分发/本地池化</h3><p></p><p>&nbsp;</p><p>快速请求/实例调度。</p><p>&nbsp;</p><p>在Serverless场景下，由于函数的体积更小、动态性更高，Serverless平台在负载高峰时也会面临较大压力。因此主流的Serverless服务商通常采用“大小流区分的负载均衡+本地优先调度”的设计来实现请求的快速分发，降低调度时延。即在流量较小的时候，采用工作负载聚合的方式来分发请求，减少集群资源使用；在流量较大的时候，通过一致性hash或轮询等方法将请求快速映射到后端节点，从而降低排队时延。这种“自顶向下”的调度设计简单，易于实现，但是在超大规模的函数并发场景下仍可能面临顶层控制器的性能瓶颈问题，因此，根据实际场景架构适当地探索“自下而上”的调度框架设计将有助于缓解该问题。</p><p>&nbsp;</p><p>此外，也有相关研究人员提出可将Serverless集群划分为多个小型的worker池，每个worker池由一个单独的semi-global子调度器管理，降低各个调度器的压力，从而改善调度时延&nbsp;[1]。</p><p>&nbsp;</p><p>镜像压缩与快速分发。</p><p>&nbsp;</p><p>在Serverless平台中，用户的函数镜像通常被存储在cluster-level的镜像仓库，节点（通常指虚拟机Virtual Machine, 简称VM）创建实例时首先需要将镜像文件通过网络传输到对应的节点（1个100MB的镜像文件从传输到容器创建完成需要耗费约20秒 [阿里云，Borg]）。在这个过程中，影响镜像传输时延的因素主要为镜像体积和节点网络带宽。因此，Serverless平台通常会采用镜像压缩技术缩小镜像体积，例如开源工具fastfreeze [2]。除此之外，为了解决节点网络带宽受限导致的拉取镜像时间过长的问题，阿里云的FaasNet [29]工作提出了一种基于树状网络的跨节点镜像分发策略，即通过根节点的VM将镜像下发给子节点（通常为2），子节点获取到镜像后以同样方式扩展到更多的节点，从而充分利用节点的上/下行带宽实现短时间内快速扩容大量实例的目的。</p><p>&nbsp;</p><p>此外，在节点中设立镜像缓存也是一种有效的方法，可以在一定程度上减少镜像传输操作，例如AWS Lambda采用了一种基于Block的镜像切分和缓存方法 [30]，相比于基于Layer的镜像构建方法，基于Block的镜像切分粒度更细，可以极大地提高镜像复用度，同时这些镜像Block被存储在由本地缓存，分布式缓存和S3组成的三级缓存系统中，从而实现快速镜像分发和实例创建速度。</p><p>&nbsp;</p><p>Pod池与实例特化。</p><p>&nbsp;</p><p>类似于本地镜像缓存的概念，在集群中预留包含函数运行时基础镜像的实例（即具备用户代码运行环境的热容器）也是一种常用的“以时间换空间”的做法 [3]。这种方法在集群中建立Pod池，预先启动一定数量的具有不同资源规格和语言运行时的实例（例如Java, Nodejs, Python等）。当冷启动调用发生时，Serverless平台可以快速从Pod池中取出所需要的实例进行特化（挂载代码文件并加载执行），从而跳过镜像获取和运行时环境启动阶段，实现快速创建实例的目的。</p><p>&nbsp;</p><p>生产环境下的数据测算显示，Node.js实例特化时的启动时间仅需15ms。然而，在Pod池中预留实例会占用大量集群资源，增加云供应商的服务成本。如何权衡用户函数性能和Pod池预留成本也是该类方法所面临的一个问题。</p><p>&nbsp;</p><p></p><h3>轻量级虚拟化/安全容器加速</h3><p></p><p>&nbsp;</p><p>传统容器加速。</p><p>&nbsp;</p><p>相比于传统的VM动辄数十秒至数分钟的创建时间，容器的启动速度和运行时开销更小，因此Serverless平台通常使用容器技术来隔离用户的函数实例，例如Docker [4]。然而在考虑函数的实际运行性能时，Docker之类的传统容器设计包含了很多不必要的运行开销，因此并不适合直接作为函数沙盒环境。为此，SOCK工作 [5]基于Linux container的设计分析了Docker文件系统和网络模块的性能瓶颈，通过裁剪不必要的启动项并对网络模块进行特定优化，大幅降低了Docker容器的内核扩展开销（超过18倍）。</p><p>&nbsp;</p><p>兼顾安全性的轻量化容器或microVM。</p><p>&nbsp;</p><p>在非Serverless的云场景中，VM的强安全隔离性使其在云租户的服务部署中扮演着重要角色。在Serverless场景中，租户函数间的安全隔离性也同样重要。为了弥补传统容器的安全性不足的问题，研究学者尝试将VM的高安全性同容器技术进行融合，例如Intel和HyperHQ团队主导的Kata Containers项目 [6]，Kata Containers的本质是一个精简后的轻量级虚拟机，在其上面运行着用户的容器进程。为了减少暴露给容器进程的攻击面，Kata Containers基于传统的虚拟化技术通过硬件虚拟出一个microVM，而microVM里则运行了一个裁剪后的 Linux 内核来实现进程间的强隔离（注：裁剪即屏蔽掉非必须的系统调用接口）。AWS Lambda的Firecracker [7]也采用了类似microVM的技术路线来实现多租户隔离性和函数性能间的权衡。</p><p>&nbsp;</p><p>除此之外，gVisor [8]则是谷歌推出的轻量级安全容器项目。gVisor采用了另一条技术路线，其核心原理是给容器进程配置一个类似“Guest Kernel”的极小的“独立内核”（即守护进程），通过拦截容器进程的系统调用来向宿主机OS发起有限的、可控的系统调用，从而提高系统安全性。由于这个独立内核运行在用户态，免去了大量地址空间转换和内核态切换的操作，因此具有较小的性能开销和更好的灵活性，同时又达到了将容器和宿主机OS（Operation System）隔离的目的。</p><p>&nbsp;</p><p>无论是Kata Containers类容器技术，还是gVisor安全容器技术，它们实现安全容器的方法在本质上是殊途同归的。这两种技术都采用了“系统分层”的设计思想，即向容器进程分配了一个独立的“操作系统内核”，从而避免了让租户的容器直接共享宿主机的内核。这样，容器进程的攻击面就从整个宿主机内核变成了一个极小的、独立的、以容器为单位的内核，从而有效解决了容器进程发生“逃逸”或者夺取整个宿主机控制权限的问题。</p><p>&nbsp;</p><p>针对安全容器的进一步加速。</p><p>&nbsp;</p><p>尽管gVisor之类的安全容器极大地弥补了传统容器的安全隔离性不足的问题，然而，安全性提高的同时也带来了额外的性能损失，例如拦截调用会增大启动和运行开销。为此，Catalyzer [9]基于快照技术和状态重用技术针对gVisor安全容器的启动过程进行了进一步优化，大幅缩短了gVisor的启动时间。Catalyzer的设计思想是“避免从零开始启动”，即通过检查点和快照恢复机制跳过启动过程关键路径上的初始化（零初始化），从而实现快速启动的目的。此外，Catalyzer还提出了一个新的操作系统原语 sfork技术，通过直接重用正在运行的沙箱实例的状态来进一步减少启动延迟。评估表明，Catalyzer在最佳情况下可将gVisor的启动延迟降低至&lt;1ms。</p><p>&nbsp;</p><p>值得注意的是，尽管Catalyzer实现了“用户角度”的快速启动，但是其并不能保证内部所有的系统状态都为最新可用状态，为此，Catalyzer采用了按需加载机制作为补救措施，即通过在后续运行过程中逐步恢复用户级内存状态和系统状态来最终恢复函数性能。该领域的另一项研究工作REAP [10]发现Catalyzer的设计机制带来的开销并不是可忽略的，运行过程中频繁的系统缺页在最坏的情况下会导致时延提高接近一倍。为此，REAP提出了一种页面预加载机制，通过识别函数运行过程中所需的必要工作集并提前载入内存，从而显著改善了Catalyzer的缺页开销问题。</p><p>&nbsp;</p><p>注：该部分涉及的一些术语可以参考扩展知识部分进行了解。</p><p>&nbsp;</p><p></p><h3>数据共享/传输优化</h3><p></p><p>&nbsp;</p><p>基于RDMA的跨节点分发。</p><p>&nbsp;</p><p>在Serverless平台内部，借助网络进行跨节点数据、代码或者状态的传输也是一个较为耗时的操作，尤其是在大量实例并发创建的场景中，远程容器初始化的频繁需求进一步加剧了这种现象。为了解决该问题，MITOSIS [11]工作在支持RDMA的操作系统中实现了一个新的操作系统原语，即remote fork，remote fork允许不同节点的操作系统之间通过RDMA技术进行内存共享映射，从而实现快速远程内存读取和跨节点容器间的数据传输。MITOSIS可以大幅降低跨节点的实例初始化时间，从单个实例跨节点复制10,000 个新实例的时间小于1秒。然而，MITOSIS的实现涉及对操作系统内核代码的大量修改，同时还依赖于RDMA硬件的支持，这些都使得它在实际部署中会面临着挑战。</p><p>&nbsp;</p><p>除此之外，新型网络设备例如DPU硬件等也提供了基于RDMA的高速跨节点互联的机制，即通过内存映射和共享来加快网络数据传输速度，这些硬件都可以被探索用于改善Serverless平台跨节点数据传输的效率。</p><p>&nbsp;</p><p></p><h3>代码加载延迟优化</h3><p></p><p>前面章节提到的加速技术都集中在系统层和框架层，在应用层，容器运行时环境的启动和函数代码的加载也在冷启动延迟中占据了相当的比例，尤其是对于机器学习等应用，运行库的加载和框架的启动时间可达数秒 [12]。值得注意的是，即便调度时延和容器初始化时间再短，长的用户侧启动时延仍然会使得冷启动问题变得棘手。</p><p>&nbsp;</p><p>为此，Serverless平台通常会针对函数的运行时进行优化，例如华为的FunctionGraph内部针对JVM的优化技术可将简单Java应用的初始化时间降低至数十毫秒。在学术研究领域，也有针对Python函数运行时库的裁剪优化工作例如FaasLight [13]，FaasLight的核心思想是“在函数启动过程中尽可能地只加载少量运行必需的代码”。为了实现这一目标，FaaSLight构建了函数级调用图来识别和筛选与应用程序功能相关的代码，并对非强相关的代码（即可选代码）进行按需加载，避免必要代码识别不准确导致应用失败。实验结果显示FaaSLight可最高降低78%的Python代码加载延迟，平均延迟降低约28%。</p><p>&nbsp;</p><p></p><h3>非容器化运行时技术</h3><p></p><p>&nbsp;</p><p>WASM运行时。</p><p>&nbsp;</p><p>WSAM（即WebAssembly）[14] 技术也是近年来一种新兴的编程运行时，可以用在云场景中代替部分容器运行时来部署用户函数。WSAM的运行原理类似于JVM，可直接运行在宿主机OS上，其支持多种语言编码，用户函数代码在运行前需要编译成WASM运行时，继而实现跨平台运行。WASM同样提供了高性能沙盒环境，并具备高灵活性和一定的安全性。</p><p>&nbsp;</p><p>尽管WASM有诸多优势，但是也存在着诸如内存隔离性弱这样的问题，因此并不能完全替代现有的容器技术，而是在大多数情况下与Docker容器技术互相配合使用。据悉，基于WASM的函数计算平台目前在国内著名的云计算厂商内部都有布局，有的平台已经推出了初步的商用版本。</p><p>&nbsp;</p><p>在研究领域，也有针对WASM的内存隔离性开展优化的工作，例如Faasm [15]。Faasm提出了一个新的隔离抽象“faaslets”，并基于WebAssembly的software-default isolation (SFI) 机制实现了函数间的内存隔离。此外，在faaslets的设计中，来自不同租户的多个函数可共享相同的内存地址空间，这样就可以避免函数间通信时大量的数据交换所导致的性能开销。faaslets还可以与Linux cgroups机制配合使用，继而实现例如CPU, 网络资源等资源的隔离。这些优化措施可以使得Faasm获得接近2倍的性能加速同时降低内存使用10倍以上。</p><p>&nbsp;</p><p>值得注意的是，上述的这些技术手段，无论是容器技术、VM技术还是WASM技术，它们本质上都是以进程方式运行在服务器内部，因此都可以借助快照技术（例如Linux CRIU [16]）实现更快的系统状态保存和恢复，从而进一步加快初始化速度。</p><p>&nbsp;</p><p>Unikernal技术。</p><p>&nbsp;</p><p>从前面列举的一些技术的特征可以看到，函数运行环境的启动速度和安全性之间总是存在一个折中，即为了改善安全性必须添加一些中间层来增强系统的控制能力，然而中间层的引入势必会降低系统启动速度。那么是否存在能够同时满足快速启动和高安全性的技术方案呢？答案就是Unikernel [17]。Unikernel顾名思义，是指定制化的操作系统内核。它的核心思想是将“用户程序和运行必需的库文件统一编译成为一个特殊的、单地址空间的内核镜像”。由于只有一个操作系统进程（这里不再有用户态和内核态的区分），同时编译时去掉了不必要的运行库，所以Unikernel运行速度可以非常快。如果将Unikernel加载到VM运行，也可以实现高安全性和隔离性。</p><p>&nbsp;</p><p>但是Unikernel的缺点也十分明显，即灵活性差。由于Unikernel 是与某种语言紧密相关的，所以一个Unikernel只能用一种语言编写，如果用户程序有所变动，则需要重新编译Unikernel，之后加载并重启VM。同时，Unikernel的编码门槛也很高，这与Serverless计算的简化编程的设计初衷相悖，也是其难以被广泛应用的原因之一。</p><p>&nbsp;</p><p></p><h4>扩展知识介绍：</h4><p></p><p>&nbsp;</p><p>传统裸金属、VM和容器技术的区别 （在体系结构中所处的层级图、用户态和内核态概念）</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/039a0cf978767a8e9513d1a43b139e18.png\" /></p><p></p><p>&nbsp;</p><p>在计算机的体系结构中，各种物理设备例如CPU处理器、磁盘、网卡等首先组成计算机的物理硬件，也成为“裸金属服务器”。裸金属服务器的运行需要操作系统的支持，比如我们熟知的Linux操作系统。借助操作系统就可以编写运行各种软件例如网页服务器、数据库等来提供外部服务。</p><p>&nbsp;</p><p>在云计算场景中，通过虚拟机共享资源是早期云服务商的主要做法。借助虚拟化技术例如Microsoft hyper-V、Citrix&nbsp;Xen等，可以将裸金属服务器虚拟为多个单独的VM，VM间共享裸金属服务器（也称宿主机）的物理资源，但是在每个VM内对租户来说是独占资源的（例如CPU，内存和磁盘）。实现这一目的的手段就是借助Hypervisor（即虚拟机监视器）来管理划分各个VM使用的物理资源，实现高安全性和隔离性。Hypervisor的内部比较复杂，涉及到系统调用、物理地址划分等操作，这里不再展开。</p><p>&nbsp;</p><p>在Hypervisor划分出来的每个VM内部都可以运行一个独立的操作系统（也称Guest OS），相比于VM，宿主机的操作系统则称为Host OS。Guest OS对于租户来讲是一个类似本机的操作系统，包含运行程序所需要的各种系统调用库和可执行文件，具体的层级划分如左侧子图所示。</p><p>&nbsp;</p><p>容器则是相比VM更为轻量级的虚拟化技术，例如Linux Container（LXC）和基于LXC实现的Docker等技术（右侧子图所示）。容器的本质是一个用户态进程，因此它可以直接运行在Host OS上，我们称之为Container Engine。Container Engine可以创建出多个容器给租户使用，并通过Linux cgroups等机制实现namespace和租户资源（例如CPU和内存）的隔离。不同于VM的运作方式，容器内部并没有Guest OS，而是复用底层的宿主机的操作系统，通过创建文件系统和加载用户程序运行所需要的库来实现运作。因此，相比VM来讲，容器的运行方式更加轻便灵活，但是安全性较差（例如某个租户的程序可能导致宿主机OS被攻击，继而使得其他租户的程序也无法运行）。为了解决这个问题，早期的云厂商做法通常是将容器部署在VM中，从而实现安全性和灵活性的折中取舍。</p><p>&nbsp;</p><p>注：操作系统通常运行在内核态，权限最高，而租户程序则运行在用户态。用户态程序执行系统调用需要切换至内核态，执行完后再切换回用户态。</p><p>&nbsp;</p><p>VM、容器和Unikernel对比 （灵活性，启动速度和隔离性）</p><p>&nbsp;</p><p>前面我们提到，Serverless平台中函数运行的技术路线有VM，容器和Unikernal几种或是它们的组合。无论哪种方式，基本都在围绕技术方案的灵活性（Flexibility），启动速度（Startup Latency）和隔离性（Isolation Level）进行权衡。</p><p>&nbsp;</p><p>下图总结并列出了这几种虚拟化方案的对比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ce4ee2d65da8ac7e7dcd854a978d3f89.png\" /></p><p></p><p>&nbsp;可以看到，VM具有高隔离性，灵活性较好，但是启动速度最慢。</p><p>&nbsp;</p><p>而传统容器以用户态进程运行，具有高度灵活性，较快的启动速度，但是隔离性较差。</p><p>安全容器是针对传统容器进行了裁剪和进一步控制，具有较高的隔离性，但是启动速度和灵活性有所降低。</p><p>&nbsp;</p><p>WASM技术类似JVM的设计理念，编译完成后即可运行，因此具有更快的启动速度和较高的灵活性，但是隔离性较差。</p><p>&nbsp;</p><p>Unikernel采用了定制化OS和单地址空间，因此可以兼顾快的启动速度和高隔离性，但是灵活性很差。</p><p>&nbsp;</p><p>不难发现，这三个要素的关系非常类似于分布式系统的CAP原理，即一致性 (Consistency)，可用性(Availability) 和分区容忍性 (Partitiontolerance)在分布式系统中最多只能同时实现两点，不可能三者兼顾。</p><p>&nbsp;</p><p>下表展示了各虚拟化技术的启动延迟等指标的具体对比：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cae984e39c26fd9736634db3c94cdde.png\" /></p><p></p><p>注：表格数据引用出处&nbsp;The serverless computing survey [31]。</p><p>&nbsp;</p><p></p><h3>小结</h3><p></p><p>在实际场景中，不同的函数计算厂商往往有自己的虚拟化技术方案，往往根据自家产品的业务线，技术积累或者客户群体需求进行综合考虑。在出于商业意图或者个人需要搭建函数计算平台进行研究时，可以根据安全性需求制定自己的技术路线。例如个人或者高校团体研究可以归类为非安全性场景，基于目前的\"VM+Kuberneters (k8s)+Docker+runtime优化\"技术栈就可以实现一个可用版本，其中VM可以提供资源强隔离性，基于多个VM节点组建k8s集群，函数计算框架例如OpenWhisk或OpenFaaS可以依托k8s进行部署。尽管同一个VM内的多租户共享会导致面临一定的安全隐患，但对于实验环境来说是足够的。</p><p>&nbsp;</p><p>在注重租户间隔离性和强调安全的场景下，AWS Lambda采用的“裸金属+Hypervisor+microVM+容器”技术路线通过microVM实现强隔离性，并且VM内一般只允许部署同一个租户的函数来提高安全性。而谷歌采用的“裸金属+Hypervisor+VM+gVisor”则是另一种技术方案，Hypervisor和VM负责提供硬件虚拟化及资源强隔离性，安全性则由gVisor提供。其中，Firecracker和gVisor均已开源。</p><p>&nbsp;</p><p>讨论：VM在每种技术方案都属于必需品的地位，但是这样势必会导致高开销的存在。那么能不能采用“裸金属+轻量级Hypervisor+Host OS+安全容器”或者直接“裸金属+Host OS+安全容器”的技术方案减少中间层的存在呢？其实这种方式并非不可以，比如剑桥大学就提出了CHERI [18]指令集扩展从而实现在硬件层面进行内存隔离，通过卸载掉虚拟化软件的部分职责，使得Hypervisor“变薄”。尽管这项工作只基于模拟器进行了实现，但是也验证了未来更轻量化的Serverless计算底座的可行性。&nbsp;</p><p></p><h2>降低冷启动发生率</h2><p></p><p></p><p>在文章的上半部分，我们主要介绍了如何加速函数实例初始化过程的一些技术方案，包括通过优化调度和镜像分发策略，采用更轻量级的虚拟化技术，或者借助RDMA硬件改善跨节点数据传输等方法，尽管这些方法已经可以将实例运行时环境的初始化的时间压缩至数十毫秒甚至是数毫秒，然而用户侧的延迟却仍然存在，例如程序状态的恢复，变量或者配置文件的重新初始化，相关库和框架的启动。具体来讲，在机器学习应用中，TensorFlow框架的启动过程往往需要花费数秒，即使实例运行时环境的启动时间再短，应用整体的冷启动时延对用户而言依然是无法接受的（注：通常大于200ms的时延可被用户察觉）。</p><p>&nbsp;</p><p>在这种情况下，可以从另一个角度入手解决冷启动问题，即降低冷启动调用的发生率。例如，通过缓存完整的函数实例，请求到达时可以快速恢复并处理请求，从而实现近乎零的初始化时延（例如Docker unpause操作时延小于0.5ms）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e899ac6cf60c3e57dfeed95b5655b4e.png\" /></p><p></p><p>&nbsp;降低冷启动发生率的相关研究可以分为如下几个方面：</p><p>&nbsp;</p><p>实例保活/实例预留：例如基于Time-to-Live的keepalive保活机制 [AWS Lambda, OpenWhisk]；或者通过并发配置接口预留一定数量的实例&nbsp;[AWS Labmda等]；这些方法原理简单，易于实现，但是在面对负载变化时缓存效率较低。基于负载特征学习的动态缓存：例如基于请求到达间隔预测的动态缓存方案 Serverless in the Wild [ASPLOS'20]；学习长短期负载变化特征的动态缓存方案 INFless [ASPLOS'22]；基于优先级的可替换缓存策略FaasCache [ATC'21]；面向异构服务器集群的低成本缓存方案 IceBreaker [ASPLOS'22]。这些动态缓存方案根据负载特征学习决定实例缓存数量或时长，从而在降低冷启动调用率的同时改善缓存资源消耗。优化请求分发提高命中率：例如兼顾节点负载和本地化执行的请求调度算法 CH-RLU [HPDC'22]。通过权衡节点负载压力和缓存实例的命中率来对请求的分发规则进行优化设计，避免节点负载过高导致性能下降，同时兼顾冷启动率。改善并发/实例共享或复用：例如允许同一函数工作流的多个函数共享Sandbox环境 SAND [ATC'18]；使用进程或线程编排多个函数到单个实例中运行 Faastlane [ATC'21]；提高实例并发处理能力减少实例创建 Fifer [Middle'20]; 允许租户复用其它函数的空闲实例减少冷启动时间 Pagurus [ATC'22]。这些实例共享或者复用技术可以同缓存方案结合使用，降低冷启动带来的性能影响。</p><p>&nbsp;</p><p>接下来，针对上述4个方面的研究工作进行详细介绍。</p><p>&nbsp;</p><p></p><h3>实例保活/实例预留</h3><p></p><p>&nbsp;</p><p>实例并发设置和预留。</p><p>&nbsp;</p><p>商用的Serverless平台例如AWS Lambda [19]、华为云FunctionGraph [20]都内置了Time-to-live实例保活机制即keepalive方法。其原理是将执行完请求的实例内存中保活一段时间再释放。如果保活期间有新的请求到达，则可以立即执行并重置剩余的保活时间。除此之外，云服务器通常会向用户提供实例并发设置的接口，用户在部署函数后可以设置预留一定数量的实例（需额外支付费用），从而降低由于突发负载导致的冷启动调用率。然而，这类静态的缓存方法易于实现，但是实例预留数量或保活时长难以根据负载变化进行实时调整，从而导致性能下降或者资源超分浪费。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0877eec57a9a08d3d36e2dae1e227a4.png\" /></p><p></p><p></p><h3>&nbsp;</h3><p></p><p></p><h3>基于负载特征学习的动态缓存</h3><p></p><p>&nbsp;</p><p>基于直方图统计的动态实例缓存。</p><p>&nbsp;</p><p>微软的研究团队通过分析Azure的生产环境trace，发现请求的到达间隔普遍分布在秒级、数分钟乃至数小时，基于Time-to-live的静态保活策略会导致大量的资源浪费。因此，作者提出了基于混合直方图的策略来统计函数请求的调用间隔，并根据这些信息动态的释放或提前拉取函数实例，从而在减少冷启动调用率的同时减少Serverless平台内部的缓存资源浪费 [21]。然而，这套设计方案只针对0-1的函数扩缩场景进行了评估，因为它只控制实例的缓存时长，而无法感知实例数量的变化，如何将这套机制运用在1-多的实例扩缩场景中还面临着一些挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bdfba5c42bbd75f029bf02f2856e6ae3.png\" /></p><p></p><p>基于长短期直方图的动态实例缓存。</p><p>&nbsp;</p><p>直方图策略通过生成函数请求到达间隔的分布来决定缓存实例的何时创建和缓存时长，然而，在初期历史统计数据不足或者负载发生短期突变的场景下，得到的直方图分布往往会不具有代表性，从而导致缓存策略的实际效果下降。为了解决该问题，INFless [22]提出了一个长短期结合的混合直方图策略来改善预测精度，通过分别统计长期的直方图分布特征和短期的直方图分布特征，来共同决定缓存实例的创建时间点和保活时长，从而改善缓存效率（冷启动率/缓存资源成本）。</p><p>&nbsp;</p><p>基于优先级的可替换缓存策略。</p><p>&nbsp;</p><p>无论是Time-to-live的keepalive还是基于直方图的动态缓存方案，都是倾向于减少缓存资源的消耗，从而降低函数服务提供商的运营成本。然而，FaasCache [23]的作者认为函数计算集群的内存资源与其与闲置，不如用来缓存更多的函数实例，从而最大程度地降低冷启动调用发生率。为此，FaasCache借鉴了CPU缓存的替换算法，通过函数的最近访问时间，函数内存消耗以及冷启动时长为每个实例定义优先级。请求执行结束后的函数实例都会被长期缓存在节点中，仅当节点无可用资源时才驱逐较低优先级的实例来缓存新的函数。此外，FaasCache还设计了一个反馈控制机制来调整缓存容量的大小，通过权衡冷启动率和缓存资源成本来实现较好的缓存效率。</p><p>&nbsp;</p><p>面向异构集群的低成本缓存策略。</p><p>&nbsp;</p><p>云服务供应商通常会提供多种具有不同CPU型号的服务器给租户使用，这些VM的处理能力和价格有所差异。IceBreaker [24]针对异构函数计算集群设计了一个成本优先的缓存策略，其核心思想是“可以使用价格更为便宜的低端服务器缓存冷启动开销较低的函数，从而节省总体成本”。为了实现该目的，IceBreaker基于傅里叶变化预测函数的负载到达率，并结合实例的使用率和节点加速前亲和性来决定将函数缓存在高端服务器，低端服务器或者不进行缓存，从而以损失部分函数性能的代价降低整体缓存消耗的资源成本。</p><p>&nbsp;</p><p></p><h3>优化请求分发提高缓存命中率</h3><p></p><p>&nbsp;</p><p>此外，还有一些工作通过优化或重新设计Serverless平台的请求分发策略来改善函数实例的命中率，从而改善缓存效率。在Serverless平台内部，前端组件收到请求后，通常以hash或负载均衡的策略分发到后端节点进行处理。将同一函数的请求分发到相对固定的节点可以有效地提高节点内实例的使用率（即缓存命中率），这也称为缓存的本地性原理。但是，由于函数负载中通常存在大量的热点函数，当热点函数聚集在后端节点时，则可能导致节点资源使负载压力升高，从而引发性能下降。</p><p>&nbsp;</p><p>因此，CH-RLU [25]工作提出了一个负载感知的请求调度策略，作者基于一致性Hash的分发规则在调度过程中考虑后端节点的压力，通过将高负载的函数分发到不同的节点来避免局部热点， 从而缓解资源竞争导致的函数性能下降问题。可以看到，优化负载分发的研究工作可以同现有的缓存方案结合使用，通过协同设计可以进一步改善Serverless平台的缓存效率。</p><p></p><h3>改善并发/实例共享或复用</h3><p></p><p>&nbsp;</p><p>多函数共享Sandbox。</p><p>&nbsp;</p><p>SAND [26]工作在函数工作流场景下提出了一种多函数共享实例Sandbox的方案，它允许同一个函数工作流内的不同函数共享同样的Sandbox环境，从而减少请求执行时创建的函数实例数量，这将有助于减少请求调用的冷启动发生率。然而，这在实际场景中也有很多局限性，例如租户的多个函数可能由不同的语言运行时编写，这些函数往往无法从实例共享中获益。此外，多个函数共享Sandbox会增加实例的体积，而工作流中的函数可能被多条路径所调用，在函数扩容时则会导致不必要的资源浪费。</p><p>&nbsp;</p><p>进/线程混合的函数编排。</p><p>&nbsp;</p><p>传统的函数实例以容器为单位，每个函数运行在一个单独的容器实例内部，容器间的数据传输需要借助网络或第三方存储实现，以AWS S3为例，通过S3进行函数间数据交换会导致较高时延。为了改善函数实例间的通信效率，Faastlane [27]工作提出了一个新的函数编排组织方式，即将工作流中的不同函数以进程或线程的形式在容器实例中执行，同时采用IPC进程间通信来提高函数交互效率。对于包含敏感数据操作的函数工作流，Faastlane使用英特尔内存保护密钥 (MPK) 提供轻量级线程级隔离域。在Apache OpenWhisk 上的评测结果表明，Faastlane可以将函数工作流的处理时间降低15倍，并将函数间的交互延迟减少99.95%。</p><p>&nbsp;</p><p>提高实例并发处理能力。</p><p>&nbsp;</p><p>在Serverless平台中，函数实例通常被配置较少的资源容量，例如内存以128MB为单位递增，而CPU的配额同内存大小成比例。当请求到达后，平台通常以“一对一”的请求-实例映射方式处理请求。低的实例并发处理能力可能导致Serverless平台在面临突发负载时创建出大量的实例，不仅降低函数的服务性能，也会增加系统调度压力。为了解决该问题，Fifer [12]工作提出了一种支持多并发处理的函数实例运行方案，通过为实例分配多个独立的CPU处理线程来提高实例的吞吐。同时，通过计算实例的排队延迟，并发处理数和未来请求到达率来提前预置对应数量的实例，从而减少冷启动调用的产生。</p><p>&nbsp;</p><p>允许租户间实例抢占/复用。</p><p>&nbsp;</p><p>通常Serverless平台中的租户函数运行在单独的实例中，不同函数间的实例有独立的资源配置和运行环境。由于不同租户的函数可能具有相同的运行时语言环境，而租户的函数实例也不一定在同一时间都处于忙碌状态，因此，Pagurus [28]工作另辟蹊径，设计了一个类似“寄居蟹”的租户间实例复用机制，其核心思想是“当A租户函数的冷启动发生时，可以迅速抢占B租户的空闲实例的运行时环境，从而减少冷启动的时延”，通过实例抢占和复用，从而避免冷启动发生时从零启动的长时延问题。同时，可以看到该做法同Pod池特化技术有异曲同工之妙。</p><p></p><h3>小结</h3><p></p><p>&nbsp;</p><p>可以看到，改善实例的并发能力或是通过实例共享/复用都是为了提高已有实例的利用效率，而缓存的方案则是尽可能利用少的资源来预置更多的实例，从而降低冷启动调用的产生。上述这些方法从不同的角度缓解Serverless函数冷启动的问题，可以结合使用来改善缓存效率。</p><p>&nbsp;</p><p>在主动缓存的方案中，由于突发流量或者负载的无规律性，基于负载预测来进行缓存还存在较大难度。在实际场景中，通过主动预测+优先级缓存调整来实现缓存方案将更有潜力。受限于各家函数计算平台内部的架构，函数共享实例或者抢占复用的实施成本较高，因此开发人员通常会选择更加易于实现的缓存方案例如预热池来改善服务质量。</p><p></p><h1>总结</h1><p></p><p></p><p>Serverless的无状态设计赋予了函数计算高度弹性化的扩展能力，然而也带来了难以避免的冷启动问题。消除Serverless函数的冷启动开销还是从降低函数冷启动率和加速实例启动过程两个角度综合入手。</p><p>&nbsp;</p><p>对于冷启动开销比较大的函数，在函数计算框架的设计机制中进行优化，尽量避免冷启动发生；当冷启动发生时，采用一系列启动加速技术来缩短整个过程进行补救。在Serverless平台的内部，冷启动的管理在实践中可以做进一步精细的划分，例如针对VIP大客户、针对有规律的负载，或是针对冷启动开销小的函数，通过分类做定制化、有目的的管理可以进一步改善系统效率。</p><p>&nbsp;</p><p>在未来，基于WASM+进程fork的技术可能会有助于彻底消除冷启动问题，但是考虑到实际可用性和安全性等问题，短期内的结果可能也是WASM和Container-based函数并存的折中方案，互相取长补短，因此在当前阶段研究冷启动问题还是有较大价值的。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考文献</p><p>&nbsp;</p><p>[1] Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mohammed Danish Shaikh, Shivaram Venkataraman, Aditya Akella. Archipelago: A Scalable Low-Latency Serverless Platform. arXiv:1911.09849, 2019.</p><p>[2] Fastfreeze. <a href=\"https://github.com/twosigma/fastfreeze.\">https://github.com/twosigma/fastfreeze.</a>\" 2023.</p><p>[3] 刘方明, 李林峰, 王磊. 华为Serverless核心技术与实践[M]. 北京: 电子工业出版社, 2021.11.</p><p>[4] Docker. <a href=\"https://www.docker.com/\">https://www.docker.com/.</a>\" 2023.</p><p>[5] Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau: SOCK: Rapid Task Provisioning with Serverless-Optimized Containers. USENIX Annual Technical Conference 2018: 57-70.</p><p>[6] Kata Containers. <a href=\"https://katacontainers.io/\">https://katacontainers.io/.</a>\" 2023.</p><p>[7] Alexandru Agache, Marc Brooker, Alexandra Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, Diana-Maria Popa: Firecracker: Lightweight Virtualization for Serverless Applications. NSDI 2020: 419-434.</p><p>[8] Open-sourcing gVisor, a sandboxed contaienr runtime. <a href=\"https://github.com/google/gvisor.\">https://github.com/google/gvisor.</a>\" 2023.</p><p>[9] Dong Du, Tianyi Yu, Yubin Xia, Binyu Zang, Guanglu Yan, Chenggang Qin, Qixuan Wu, Haibo Chen: Catalyzer: Sub-millisecond Startup for Serverless Computing with Initialization-less Booting. ASPLOS 2020: 467-481.</p><p>[10] Dmitrii Ustiugov, Plamen Petrov, Marios Kogias, Edouard Bugnion, Boris Grot: Benchmarking, analysis, and optimization of serverless function snapshots. ASPLOS 2021: 559-572.</p><p>[11] Xingda Wei, Fangming Lu, Tianxia Wang, Jinyu Gu, Yuhan Yang, Rong Chen, Haibo Chen. No Provisioned Concurrency: Fast RDMA-codesigned Remote Fork for Serverless Computing. OSDI 2023.</p><p>[12] Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan Chidambaram Nachiappan, Mahmut Taylan Kandemir, Chita R. Das. Fifer: Tackling Resource Underutilization in the Serverless Era. Middleware 2020: 280-295.</p><p>[13] Xuanzhe Liu, Jinfeng Wen, Zhenpeng Chen, Ding Li, Junkai Chen, Yi Liu, Haoyu Wang, Xin Jin. FaaSLight: General Application-Level Cold-Start Latency Optimization for Function-as-a-Service in Serverless Computing. arXiv:2207.08175. 2022.&nbsp;</p><p>[14] WebAssembly. <a href=\"https://webassembly.org./\">https://webassembly.org.</a>\" 2023.</p><p>[15] Simon Shillaker, Peter R. Pietzuch: Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing. USENIX Annual Technical Conference 2020: 419-433.</p><p>[16] Linux CRIU, stands for checkpoint and resotre in userspace. <a href=\"https://github.com/checkpoint-restore/criu.\">https://github.com/checkpoint-restore/criu.</a>\" 2023.</p><p>[17] Anil Madhavapeddy, Richard Mortier, Charalampos Rotsos, David J. Scott, Balraj Singh, Thomas Gazagnaire, Steven Smith, Steven Hand, Jon Crowcroft: Unikernels: library operating systems for the cloud. ASPLOS 2013: 461-472.</p><p>[18] Robert N. M. Watson, Jonathan Woodruff, Peter G. Neumann, Simon W. Moore, Jonathan Anderson, David Chisnall, Nirav H. Dave, Brooks Davis, Khilan Gudka, Ben Laurie, Steven J. Murdoch, Robert M. Norton, Michael Roe, Stacey D. Son, Munraj Vadera: CHERI: A Hybrid Capability-System Architecture for Scalable Software Compartmentalization. IEEE Symposium on Security and Privacy 2015: 20-37.&nbsp;</p><p>[19] AWS Lambda. <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/.</a>\" 2023.</p><p>[20] 华为云FunctionGraph. <a href=\"https://www.huaweicloud.com/product/functiongraph.html.\">https://www.huaweicloud.com/product/functiongraph.html.</a>\" 2023.</p><p>[21] Mohammad Shahrad, Rodrigo Fonseca, Iñigo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, Ricardo Bianchini: Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider. USENIX Annual Technical Conference 2020: 205-218.</p><p>[22] Yanan Yang, Laiping Zhao, Yiming Li, Huanyu Zhang, Jie Li, Mingyang Zhao, Xingzhen Chen, Keqiu Li: INFless: a native serverless system for low-latency, high-throughput inference. ASPLOS 2022: 768-781.</p><p>[23] Alexander Fuerst, Prateek Sharma: FaasCache: keeping serverless computing alive with greedy-dual caching. ASPLOS 2021: 386-400.</p><p>[24] Rohan Basu Roy, Tirthak Patel, Devesh Tiwari: IceBreaker: warming serverless functions better with heterogeneity. ASPLOS 2022: 753-767.</p><p>[25]Alexander Fuerst, Prateek Sharma: Locality-aware Load-Balancing For Serverless Clusters. HPDC 2022: 227-239.</p><p>[26] Istemi Ekin Akkus, Ruichuan Chen, Ivica Rimac, Manuel Stein, Klaus Satzke, Andre Beck, Paarijaat Aditya, Volker Hilt: SAND: Towards High-Performance Serverless Computing. USENIX Annual Technical Conference 2018: 923-935.</p><p>[27] Swaroop Kotni, Ajay Nayak, Vinod Ganapathy, Arkaprava Basu: Faastlane: Accelerating Function-as-a-Service Workflows. USENIX Annual Technical Conference 2021: 805-820.</p><p>[28] Zijun Li, Quan Chen, Minyi Guo: Pagurus: Eliminating Cold Startup in Serverless Computing with Inter-Action Container Sharing. USENIX Annual Technical Conference 2022.</p><p>[29] Ao Wang, Shuai Chang, Huangshi Tian, Hongqi Wang, Haoran Yang, Huiba Li, Rui Du, Yue Cheng: FaaSNet: Scalable and Fast Provisioning of Custom Serverless Container Runtimes at Alibaba Cloud Function Compute. USENIX Annual Technical Conference 2021: 443-457.</p><p>[30] Marc Brooker, Mike Danilov, Chris Greenwood, Phil Piwonka: On-demand Container Loading in AWS Lambda. CoRR abs/2305.13162 (2023).</p><p>[31] Zijun Li, Linsong Guo, Jiagan Cheng, Quan Chen, Bingsheng He, Minyi Guo: The Serverless Computing Survey: A Technical Primer for Design Architecture. ACM Comput. Surv. 54(10s): 220:1-220:34 (2022).</p><p>&nbsp;</p>",
    "publish_time": "2023-07-17 10:40:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "企业数字化转型技术发展趋势研究报告",
    "url": "https://www.infoq.cn/article/tjvf64LLOGW69mfut9Uq",
    "summary": "<p>为了更好地引导企业数字化建设，高效推进数字化升级，帮助用户更好地获取数字化产品技术支持和应用服务，本报告从企业数字化转型技术的发展趋势、技术应用保障体系、技术转型路径需求、构建转型技术发展体系做了系统性论述，提出未来技术发展的五大趋势，并对每一项技术进行了深入详细的探讨，提出了应用解决方案和实现数字化转型的路径建议，为我国产业数字化转型提供有力支撑。</p>\n<blockquote>\n<p>本报告版权属于中国信息通信研究院和北京元年科技股份有限公司，并受法律保护。转载、摘编或利用其它方式使用本报告文字或者观点，应注明“来源：中国信息通信研究院和北京元年科技股份有限公司”。违反上述声明者，编者将追究其相关法律责任。</p>\n</blockquote>",
    "publish_time": "2023-07-17 11:11:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "比 JDK 最高快 170 倍，蚂蚁集团开源高性能多语言序列化框架 Fury",
    "url": "https://www.infoq.cn/article/qYslJsfjkXXjNlQPWuA2",
    "summary": "<p>Fury 是一个基于 JIT 动态编译和零拷贝的多语言序列化框架，支持Java/Python/Golang/JavaScript/C++ 等语言，提供全自动的对象多语言 / 跨语言序列化能力，和相比 JDK 最高 170 倍的性能。</p><p></p><p>代码主仓库的 GitHub 地址为：<a href=\"https://github.com/alipay/fury\">https://github.com/alipay/fury</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/95/95d1a9d1f8225559fa1ae8da44b072ee.png\" /></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>序列化是系统通信的基础组件，在大数据、AI 框架和云原生等分布式系统中广泛使用。当对象需要跨进程、跨语言、跨节点传输、持久化、状态读写、复制时，都需要进行序列化，其性能和易用性影响运行效率和开发效率。</p><p></p><p>静态序列化框架 protobuf/flatbuffer/thrift 由于不支持对象引用和多态、需要提前生成代码等原因，无法作为领域对象直接面向应用进行跨语言开发。而动态序列化框架 JDK 序列化 /Kryo/Fst/Hessian/Pickle 等，尽管提供了易用性和动态性，但不支持跨语言，且性能存在显著不足，并不能满足高吞吐、低延迟和大规模数据传输场景需求。</p><p></p><p>因此，我们开发了一个新的多语言序列化框架 Fury，并正式在 Github 开源。通过一套高度优化的序列化基础原语，结合 JIT 动态编译和 Zero-Copy 等技术，同时满足了性能、功能和易用性的需求，实现了任意对象自动跨语言序列化，并提供极致的性能。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9a/9ac77edab4d330b1dffee7b034360868.jpeg\" /></p><p></p><h2>Fury 简介</h2><p></p><p></p><p>Fury 是一个基于 JIT 动态编译和零拷贝的多语言序列化框架，提供极致的性能和易用性：</p><p></p><p>支持主流编程语言 Java/ Python/ C++/ Golang/ JavaScript，其它语言可轻易扩展；统一的多语言序列化核心能力：高度优化的序列化原语；Zero-Copy 序列化 支持，支持 Out of band 序列化协议，支持堆外内存读写；基于 JIT 动态编&nbsp;技术在运行时异步多线程自动生成序列化代码优化性能，增加方法内联、代码缓存和消除死代码，减少虚方法调用 / 条件分支 /Hash 查找 / 元数据写入 / 内存读写等，提供相比别的序列化框架最高 170 倍的性能；多协议支持：兼顾动态序列化的灵活性和易用性，以及静态序列化的跨语言能力。Java 序列化：无缝替代 JDK/Kryo/Hessian，无需修改任何代码，但提供最高 170x 的性能，可以大幅提升高性能场景 RPC 调用、数据传输和对象持久化效率；100% 兼容 JDK 序列化，原生支持 JDK 自定义序列化方法 writeObject/ readObject/ writeReplace/ readResolve/ readObjectNoData跨语言对象图序列化：多语言 / 跨语言自动序列化任意对象，无需创建 IDL 文件、手动编译 schema 生成代码以及将对象转换为中间格式；多语言 / 跨语言自动序列化共享引用和循环引用，不需要关心数据重复或者递归错误；支持对象类型多态，多个子类型对象可以同时被序列化；行存序列化：提供缓存友好的二进制随机访问行存格式，支持跳过序列化和部分序列化，适合高性能计算和大规模数据传输场景；支持和 Arrow 列存自动互转 ;</p><p></p><h2>序列化核心能力</h2><p></p><p></p><p>尽管不同的场景对序列化有需求，但序列化的底层操作都是类似的。因此 Fury 定义和实现了一套序列化的基础能力，基于这套能力能够快速构建不同的多语言序列化协议，并通过编译加速等优化具备高性能。同时针对一种协议在基础能力上的性能优化，也能够让所有的序列化协议都受益。</p><p></p><h3>序列化原语</h3><p></p><p></p><p>序列化涉及的常见操作主要包括：</p><p></p><p>bitmap 位操作整数编解码整数压缩字符串创建 * 拷贝优化字符串编码：ASCII/UTF8/UTF16内存拷贝优化数组拷贝压缩优化元数据编码 &amp; 压缩 &amp; 缓存</p><p></p><p>Fury 针对这些操作在每种语言内部都做了大量的优化，结合 SIMD 指令和语言高级特性，将性能推到极致，从而方便不同协议使用。</p><p></p><h3>零拷贝序列化</h3><p></p><p></p><p>在大规模数据传输场景，一个对象图内部往往有多个 binary buffer，而序列化框架在序列化过程当中会把这些数据写入一个中间 buffer，引入多次耗时内存拷贝。Fury 借鉴了 pickle5、ray 以及 arrow 的零拷贝设计，实现了一套 Out-Of-Band 序列化协议，能够把一个对象图当中的所有 binary buffer 直接抓取出来，避免掉这些 buffer 的中间拷贝，将序列化期间的内存拷贝开销降低到 0。</p><p></p><p>下图是 Fury 关闭引用支持时 Zero-Copy 的大致序列化过程。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9f/9fc343c6bb0bcf06c8e279e8463b6c1b.png\" /></p><p></p><p>目前 Fury 内置了以下类型的 Zero-Copy 支持：</p><p></p><p>Java：所有基本类型数组、ByteBuffer、ArrowRecordBatch、VectorSchemaRootPython：array 模块的所有 array、numpy 数组、pyarrow.Table、pyarrow.RecordBatchGolang：byte slice</p><p></p><p>用户也可以基于 Fury 的接口扩展新的零拷贝类型。</p><p></p><h3>JIT 动态编译加速</h3><p></p><p></p><p>对于要序列化的自定义类型对象，其中通常包含大量类型信息，Fury利用这些类型信息在运行时直接生成高效的序列化代码，将大量运行时的操作在动态编译阶段完成，从而增加方法内联和代码缓存，减少虚方法调用 / 条件分支 /Hash 查找 / 元数据写入 / 内存读写 等，最终大幅加速了序列化性能。</p><p></p><p>对于 Java 语言，Fury 实现了一套运行时代码生成框架，定义了一套序列化逻辑的算子表达式 IR，在运行时基于对象类型的泛型信息进行类型推断，然后构建一颗描述序列化代码逻辑的表达式树，根据表达式树生成高效的 Java 代码，再在运行时通过 Janino 编译成字节码，再加载到用户的 ClassLoader 里面或者 Fury 创建的 ClassLoader 里面，最终通过 Java JIT 编译成高效的汇编代码。</p><p></p><p>由于 JVM JIT 会跳过大方法编译和内联，Fury 也实现了一套优化器，将大方法递归拆分成小方法，这样就保证了 Fury 生成的所有代码都可以被编译和内联，压榨 JVM 的性能到极致。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/10/100abd1fa3ae527be867ff6a3848a567.png\" /></p><p></p><p>同时 Fury 也支持异步多线程动态编译，将不同序列化器的代码生成任务提交到线程池执行，在编译完成之前使用解释模式执行，从而保证不会出现序列化毛刺，不需要提前预热所有类型的序列化。</p><p></p><p>Python 和 JavaScript 场景也是采用的类似代码生成方式，这样的生成方式开发门槛低，更容易排查问题。</p><p></p><p>由于序列化需要密切操作每种编程语言的对象，而编程语言并没有暴露内存模型的低阶 API，通过 Native 方法调用存在较大开销，因此我们并不能通过 LLVM 构建一个统一的序列化器 JIT 框架，而是需要在每种语言内部结合语言特性实现特定的代码生成框架以及序列化器构建逻辑。</p><p></p><h3>静态代码生成</h3><p></p><p></p><p>尽管 JIT 编译能够大幅提升序列化效率，并且在运行时能够根据数据的统计分布重新生成更优的序列化代码，但 C++/Rust 等语言不支持反射，没有虚拟机，也没有提供内存模型的低阶 API，因此我们无法针对这类语言通过 JIT 动态编译生成序列化代码。</p><p></p><p>对于此类场景，Fury 正在实现一套 AOT 静态代码生成框架，在编译时根据对象的 schema 提前生成序列化代码，然后使用生成的代码进行自动序列化。对于 Rust，未来也会通过 Rust 的 macro 在编译时生成代码，提供更好的易用性。</p><p></p><h3>缓存优化</h3><p></p><p></p><p>在序列化自定义类型时，会把字段进行重排序，保证相同接口类型的字段依次序列化，增加缓存命中的概率，同时也促进了 CPU 指令缓存，实现了更加高效的序列化。对于基本类型字段将写入顺序按照字节字段大小降序排列，这样如果开始地址是对齐的，随后的读写都会发生在内存地址对齐的位置，CPU 执行起来更加高效。</p><p></p><h2>多协议设计与实现</h2><p></p><p></p><p>基于 Fury 提供的多语言序列化核心能力，我们在这之上构建了三种序列化协议，分别适用于不同的场景：</p><p></p><p>Java 序列化：适合纯 Java 序列化场景，提供最高百倍以上的性能提升；跨语言对象图序列化：适合面向应用的多语言编程，以及高性能跨语言序列化；行存序列化：适合分布式计算引擎如 Spark/Flink/Dories/Velox/ 样本流处理框架 / 特征存储等；</p><p></p><p>后续我们也会针对一些核心场景添加新的协议，用户也可以基于 Fury 的序列化能力构建自己的协议。</p><p></p><h3>Java 序列化</h3><p></p><p></p><p>由于 Java 在大数据、云原生、微服务和企业级应用的广泛使用，对 Java 序列化的性能优化可以大幅降低系统延迟，提升吞吐率，降低服务器成本。</p><p></p><p>因此 Fury 针对 Java 序列化进行了大量极致性能优化，我们的实现具备以下能力：</p><p></p><p>极致性能：通过利用 Java 对象的类型和泛型信息，结合 JIT 编译、Unsafe 低阶操作，Fury 相比 JDK 最高有 170 倍的性能提升，相比 Kryo/Hessian 最高有 50~100 倍的性能提升。100% JDK 序列化 API 兼容性：支持了所有 JDK 自定义序列化方法 writeObject/readObject/ writeReplace/ readResolve/readObjectNoData 的语义，保证任意场景替换 JDK 序列化的正确性。而已有的 Java 序列化框架如 Kryo/Hessian 在这些场景，都存在一定的正确性问题类型前后兼容：在反序列化端和序列化端 Class Schema 不一致时，仍然可以正确反序列化，支持应用独立升级部署，独立增删字段。并且我们对元数据进行了极致的压缩和共享，类型兼容模式相比类型强一致模式做到了几乎没有任何性能损失。元数据共享：在某个上下文 (TCP 连接) 下多次序列化之间共享元数据(类名称、字段名称、Final 字段类型信息等)，这些信息会在该上下文下第一次序列化时发送到对端，对端可以根据该类型信息重建相同的反序列化器，后续序列化可以避免传输元数据，减小网络流量压力，同时也自动支持类型前后兼容。零拷贝支持：支持 Out of band 零拷贝和堆外内存读写。</p><p></p><h3>跨语言对象图序列化</h3><p></p><p></p><p>跨语言对象图序列化主要用于对动态性和易用性有更高要求的场景。尽管 Protobuf/Flatbuffer 等框架提供了多语言序列化能力，但仍然存在一些不足：</p><p></p><p>需要提前编写 IDL 并静态编译生成代码，不具备足够的动态性和灵活性；生成的类不符合面向对象设计也无法给类添加行为，并不能作为领域对象 直接用于多语言应用开发。不支持子类序列化。面向对象编程的主要特点是通过接口调用子类方法。这类模式也无法得到很好的支持。尽管 Flatbuffer 提供了 Union，Protobuf 提供了 OneOf/Any 特性，这类特性需要在序列化和反序列化时判断对象的类型，不符合面向对象编程的设计。不支持循环和共享引用，需要针对领域对象重新定义一套 IDL 并自己实现引用解析，然后在每种语言里面编写代码实现领域对象和协议对象之间的相互转换，如果对象图嵌套层数较深，则需要编写更多的代码。</p><p></p><p>结合以上几点，Fury 实现了一套跨语言的对象图序列化协议：</p><p></p><p>多语言 / 跨语言 自动序列化任意对象：在序列化和反序列化端定义两个 Class，即可自动将一种语言的对象自动序列化为另一种语言的对象，无需创建 IDL 文件、编译 schema 生成代码以及手写转换代码；多语言 / 跨语言自动序列化共享引用和循环引用；支持对象类型多态，符合面向对象编程范式，多个子类型对象可以同时被自动反序列化，无需用户手动处理；同时我们在这套协议上面也支持了 Out of band 零拷贝；</p><p></p><p>自动跨语言序列化示例：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ed/ed811261733f7241e811bcb45c7289c7.png\" /></p><p></p><h3>行存序列化</h3><p></p><p></p><p>对于高性能计算和大规模数据传输场景，数据序列化和传输往往是整个系统的性能瓶颈。如果用户只需要读取部分数据，或者根据对象某个字段进行过滤，反序列化整个数据将带来额外开销。因此 Fury 也提供了一套二进制数据结构，在二进制数据上直读直写，避开序列化。</p><p></p><p>Apache arrow 是一个成熟的列存格式，支持二进制读写。但列存并不能满足所有场景需求，在线链路和流式计算场景的数据天然就是行存结构，同时列式计算引擎内部在涉及到数据变更和 Hash/Join/Aggregation 操作时，也会使用到行存结构。</p><p></p><p>而行存并没有一个统一标准实现，计算引擎如 Spark/Flink/Doris/Velox 等都定义了一套行存格式，这些格式不支持跨语言，且只能被自己引擎内部使用，无法用于其它框架。尽管 Flatbuffer 能够支持按需反序列化，但需要静态编译 Schema IDL 和管理 offset，无法满足复杂场景的动态性和易用性需求。</p><p></p><p>因此 Fury 在早期借鉴了 spark tungsten 和 apache arrow 格式，实现了一套可以随机访问的二进制行存结构，目前实现了 Java/Python/C++ 版本，实现了在二进制数据上面直读直写，避免掉了所有序列化开销。</p><p></p><p>下图是 Fury Row Format 的二进制格式：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/81/81aec151ae40ab60d441c1314f7424f2.png\" /></p><p></p><p>该格式密集存储，数据对齐，缓存友好，读写更快。由于避免了反序列化，能够减少 Java GC 压力。同时降低 Python 开销，同时由于 Python 的动态性，Fury 的数据结构实现了 _getattr__/getitem/slice/ 和其它特殊方法，保证了行为跟 python dataclass/list/object 的一致性，用户没有任何感知。</p><p></p><h2>性能对比</h2><p></p><p></p><p>这里给出部分 Java 序列化性能数据，其中标题包含 compatible 的图表是支持类型前后兼容下的性能数据，标题不包含 compatible 的图表是不支持类型前后兼容下的性能数据。为了公平起见，所有测试 Fury 关闭了零拷贝特性。</p><p></p><p>更多 benchmark 数据请参考 Fury Github 官方文档：<a href=\"https://github.com/alipay/fury/tree/main/docs/benchmarks\">https://github.com/alipay/fury/tree/main/docs/benchmarks</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9a/9ac77edab4d330b1dffee7b034360868.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7c/7c39dcfb78c21a8382d8a12424363d71.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a5/a57a4ed01c9d4f3f6651681565ad50c3.jpeg\" /></p><p></p><h2>未来规划</h2><p></p><p></p><p>元数据压缩和自动共享跨语言序列化支持类型前后兼容静态代码生成框架，用于提前生成 c++/golang/rust 代码C++/Rust 支持跨语言对象图序列化Golang/Rust/JavaScript 支持行存兼容 ProtoBuffer 生态，支持根据 Proto IDL 自动生成 Fury 序列化代码新的协议实现：AI 特征存储，知识图谱序列化持续改进我们的序列化基础原语，提供更高性能实现标准化协议，提供二进制兼容性文档和易用性改进</p><p></p><h2>加入我们</h2><p></p><p></p><p>我们致力于将 Fury 打造为一个开放中立、追求极致与创新的社区项目，后续的研发与讨论等工作都会在社区以开源透明的方式进行。欢迎任何形式的参与，包括但不限于提问、代码贡献、技术讨论等。非常期待收到大家的想法和反馈，一起参与到项目的建设中来，推动项目向前发展，打造最先进的序列化框架。</p><p></p><p>代码主仓库的 GitHub 地址为：<a href=\"https://github.com/alipay/fury\">https://github.com/alipay/fury</a>\"</p><p></p><p>官方网站：<a href=\"https://furyio.org/\">https://furyio.org</a>\"</p><p></p><p>欢迎各种 Issue、PR、Discussion。</p><p></p><p>也欢迎直接加入下面的官方交流群，和我们一起交流。</p><p></p><p>微信公众号：高性能序列化框架 Fury</p><p></p><p>钉钉交流群：钉钉群号 (36170003000)</p><p></p><h4>作者简介</h4><p></p><p></p><p>杨朝坤，蚂蚁集团技术专家，Fury 框架作者。2018 年加入蚂蚁集团，先后从事流计算框架、在线学习框架、科学计算框架和 Ray 等分布式计算框架开发，对批计算、流计算、Tensor 计算、高性能计算、AI 框架、张量编译等有深入的理解。</p><p></p><p></p><h4>活动推荐</h4><p></p><p></p><p>以「启航·AIGC 软件工程变革」为主题的 QCon 全球软件开发大会·北京站将于 9 月 3-5 日在北京•富力万丽酒店举办，此次大会策划了构建未来软件的编程语言、大模型应用落地、面向 AI 的存储、AIGC 浪潮下的研发效能提升、大前端融合提效、LLMOps、异构算力、微服务架构治理、业务安全技术、FinOps 等近 30 个精彩专题。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/db/db1963da1a756b9a6b0c80b0b5123868.jpeg\" /></p><p></p><p>现在购票即可享受 8 折优惠，立减 ¥1760。咨询购票可联系票务经理 18514549229（微信同手机号）。<a href=\"https://qcon.infoq.cn/202309/beijing?utm_source=infoq&amp;utm_medium=arti&amp;utm_campaign=8&amp;utm_term=0717&amp;utm_content=mubai\">点击查看专题详情</a>\"，期待与各位开发者现场交流。</p><p></p>",
    "publish_time": "2023-07-17 11:44:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "探访海底捞门店数字化：减员不是目的，创造价值才是",
    "url": "https://www.infoq.cn/article/s3VTceqbD8pAulSx5rUu",
    "summary": "<p>众所周知，服务是海底捞火锅门店的一大特色；但少有人知的是，海底捞的后厨同样有着许多令人眼前一亮的“隐藏技能”。</p><p></p><p>严格的净污分区，每个关键操作位都配有高清摄像头识别员工操作规范，如果出现任何问题可以快速进行食品安全追溯；自主研发的私人订制自动配锅机，通过结合大数据和云端记录，可以满足“千人千味”的锅底专享需求；配菜区每盒菜品盒子上配备RFID芯片，只要放在智能菜品台上，就能清晰显示菜品日期、重量、供应商、以及，是否符合安全规范等信息，如果是临期或过期食品将会有显眼的颜色标识进行提示......</p><p></p><p>在全国数千家海底捞门店中，诸如此类的数字化创新场景数不胜数。而这背后，是300多款应用系统的支撑——其中，超过100应用是<a href=\"https://xie.infoq.cn/article/b478fb34061ab7dcaebce0357\">海底捞</a>\"在短短1年内构建的，另外200多个跨部门的应用系统也已经实现了统一管理和打通。对于海底捞实现进一步精细化管理，这是具有里程碑意义的关键一步。</p><p></p><h2>系统割裂，开发需求多且频繁</h2><p></p><p></p><p>海底捞的数字化，最早要从2011年前后说起。当时，海底捞门店已经开始使用平板电脑点菜，并完成了点餐、收银、排号、订餐等系统的上线。经过10多年来的发展，海底捞业务规模持续扩大，门店数量增加，现场及系统管理复杂度都大大提升。</p><p></p><p>在这个过程中，海底捞上线了菜品创新提报、内部培训认证、内部招聘和投诉、安全打分、员工身份核验、差旅管理等上百个系统。但这些系统分散在各处，数据之间互不相通，交互难度大，这一方面使得管理者难以实时掌握全面的信息，同时，对于员工而言，日常工作中也要在不同系统间来回切换，工作效率较低。</p><p></p><p>以海底捞独创的“四色卡管理模式”为例，其中红卡代表服务、蓝卡代表卫生、黄卡代表菜品出品、绿卡代表食品安全。总部和门店经理每天会针对四个维度的各个细节，比如顾客满意度、投诉情况、卫生安全、设备等进行检查和评分，从而提升门店现场的管理水平。</p><p></p><p>此前，海底捞使用内部开发的小程序作为四色卡管理工具，这意味着，该工具的用户数量超过了10万，几乎每一位员工每一天都要打开和使用该工具。但与此同时，为了处理不同工作，他们还可能同时要打开多个平台或应用进行来回切换，这不仅体验差，效率也不高。</p><p></p><p>另一个挑战来自于技术部门对业务侧需求的响应效率。</p><p></p><p>海底捞信息科技部PMO颉鹏伟告诉InfoQ记者，自主开发的四色卡管理小程序最大的弊端就是不够灵活，“我们的技术团队面对的是全国1000多家门店，如果业务一线频繁地提出需求，或者即便只有10%的门店在同一时间提出了新的需求，我们都很难做到快速高效的响应。”</p><p></p><h2>在飞书上找到了问题“解法”</h2><p></p><p></p><p>据了解，为了解决系统割裂和<a href=\"https://www.infoq.cn/article/BkVptJA3JSK5cf14RN3e\">数据烟囱</a>\"的问题，海底捞在2022年上线了飞书，并基于集成平台把大量多而散的应用系统都“搬”到了飞书上，使得内部10多万员工都可以在线同频。</p><p></p><p>“也就是说，我们把各种典型的业务场景模型化后，通过工具的手段都落到了系统上。对于员工来说，他们在手机端工作台入口就可以触达各种应用，从而完成日常的各项工作。”颉鹏伟补充说，“并且，在手机界面上还会把审批、员工身份核验等常用应用呈现出来，方便大家使用。”据他介绍，所有相关应用的排序都是经过细致调研呈现的，而这也是海底捞使用飞书进行精细化管理的一个体现。</p><p></p><p>除此之外，对于类似于“审批”这样的高频使用且对时效性要求比较高的应用，海底捞还进行了模块定制化，将“待审批”、“待处理”功能单独拉出来显示在前端页面，让员工可以快速触达。</p><p></p><p>作为一个面向内部使用的核心工具，海底捞同样把“千人千面”的理念融入在了<a href=\"https://xie.infoq.cn/article/ddc70d9de98ef0af47b0034cc\">飞书</a>\"上。举个例子，对于海底捞员工来说，飞书工作台的使用是高度私人定制的——不同部门、不同员工看到的界面是差异化的。比如，管理者需要处理审批工作，一线员工则不需要，他们关心的可能是计件工资，因此，海底捞就会把审批模块在一线员工账号上去掉，而在更显眼和便捷的位置呈现计件工资模块。</p><p></p><p>另一方面，针对业务需求快速响应的挑战，海底捞同样在飞书上找到了“解法”。</p><p></p><p>今年初，飞书发布了以“多维表格”、“飞书应用引擎”、“飞书集成平台”组成的“业务三件套”。其中，“飞书集成平台”帮助海底捞搞定了上述的系统集成问题，而另外两项功能，则使得海底捞的技术人员的效率大大提升，甚至是解放出来。</p><p></p><p>“多维表格”是一个一线业务员工也能自主进行应用搭建的功能，基于业务模版能够实现1分钟快速搭建，1小时内进行应用调试。而“飞书应用引擎”面向的则是业务相对更复杂、数据量更大的专业系统应用，基于<a href=\"https://www.infoq.cn/article/UdGTsgRmdWEUxMxV3Aii\">低代码</a>\"能力，能帮助技术人员大幅降低开发成本，快速搭建业务系统。</p><p></p><p>据颉鹏伟介绍，在上线飞书后短短1年的时间里，海底捞已经通过“多维表格”和“飞书应用引擎”构建了100多个系统应用。</p><p></p><h4>对「人」的数字化管理</h4><p></p><p></p><p>海底捞的业务重心聚焦一线门店。在经营管理过程中，店经理就是离“战场”最近，且拥有极大影响力的岗位角色。对店经理的管理，将直接决定海底捞门店经营甚至是业务经营效益。</p><p></p><p>因此，在基于飞书开发的上百个系统应用中，与店经理管理相关的应用不在少数。</p><p></p><p>以“店经理任职资格分管理”应用为例，这是海底捞借助“多维表格”把自身的管理经验和方法落在系统上的成果。基于该应用，海底捞总部实现了对店经理进行量化考核，在应用上进行考核记录不但可以线上留痕、有据可依，同时还能够实时反馈给当事人。如果考核结果低于标准线，店经理就要从岗位下来重新进行学习和提升。</p><p></p><p>与此同时，海底捞还通过“多维表格”实现了店经理工作流程管理线上化和规范化。在海底捞，有一套完整的制度和流程规定店经理的日、周、月，以及季度和年度的工作流程。过去，所有流程的执行情况散落在各个门店和不同区域。从公司角度而言，很难全面地掌握和确认流程的被执行程度和效果。</p><p></p><p>“事实上，有的流程没有被很好执行并不一定是因为大家不愿意做，而是某个流程不合理，这时候，我们是需要相关数据做支撑的。”颉鹏伟指出，出于这一原因，海底捞把店经理的相关工作流程都基于“多维表格”转到了线上，并且可以以自动提醒的方式提前把工作流程推送给店经理，为其当日的工作开展和复盘、总结提供“备忘”。</p><p></p><p>对于店经理而言，这些操作都是在统一入口、一键操作、瞬间完成。而当所有的数据汇总在一起，呈现给总部管理者的就是多维度的大盘数据，可以用来判断门店管理情况，店经理工作状态和趋势，同时评估流程的合理性。</p><p></p><h4>对「物」的数字化管理</h4><p></p><p></p><p>如果说对人的管理关乎门店经营效率，那么对物的管理，对菜品质量、食品安全的把控，则是餐饮门店经营的“生死线”。</p><p></p><p>在海底捞，对菜品质量的把控分两个方面：一是在前置环节进行流程标准、出品的把控；二是在问题暴露时，快速应对和解决。据了解，海底捞门店的库管，每天都要对食材进行验货，一旦发现产品缺陷，就要联系供应链退回，并加紧运送新货，保障门店正常运转。</p><p></p><p>但食材在送到消费者口中之前，要从供货源经过运输到达门店收货，期间还要进入冷藏库保存，有的食材还要进行餐前解冻。如果食材出现缺陷，那么原因有可能发生在任何一个环节，问题往往以散点的方式暴露出来，追溯的过程也非常漫长。</p><p></p><p>过去，库管会第一时间在工作沟通群中跟采购反馈问题，但海底捞全国有上千家门店，几十家门店会被划分到一个聊天沟通群中，同时要求库管在工作沟通群之外，还需要在海底捞自己的系统上再提交，然后由门店店长将隔天的数据统一提交给总部。总部将数据导出之后，会按照职责划分，再将各门店各产品具体缺陷信息提交给负责的采购，由采购再将该事件处理的结果录入到系统中。整个流程要耗时3-4天，而在这个过程中如果没能及时追溯到原因，相同问题还会持续出现。</p><p></p><p>为了解决这个问题，海底捞使用飞书应用引擎，搭建了一套“菜品缺陷提报”系统。通过这套系统，海底捞门店最快&nbsp;2&nbsp;小时就可以闭环处理单个菜品问题提，报流程节省了&nbsp;40%的人力，提效&nbsp;25%以上。</p><p></p><p>“菜品缺陷问题的解决和优化它并不是线性的流程，在过程中它可能经历问题退回、纠纷、重复排查等等，所以，用一个报表或简单工具是很难解决这种复杂多变问题的，它需要一个能够支持灵活流程的工具。”颉鹏伟表示。</p><p></p><h2>减员不是目的，创造价值才是</h2><p></p><p></p><p>用魔法打败魔法，用技术降低技术的门槛，对此，海底捞的技术研发团队和一线员工都在从中受益。但是，新的问题也可能随之出现——比如，当没有技术背景的人员也可以自己搭建系统应用，那么应用的稳定性、可靠性如何保障？</p><p></p><p>颉鹏伟向InfoQ解释，多维表格类技术的使用是为了让大家可以根据业务场景放心大胆地做创新，但是，在海底捞内部，所有应用的创建也都要经过严格的流程申请和检验，而不是放任自流。比如，创建的应用会先做试点，这期间如果不适用就会把它阻断在摇篮里，有任何问题都可以调整、优化，然后再进行推广；再比如，在应用正式上线后，也并不是一下子推向所有人，而是针对性的，面向有需要的角色推送。</p><p></p><p>“目前来看，我们在这一年里基于飞书应用的搭建工具的使用体验和稳定性还是比较好的。前提是我们在应用推广的收口上，有相应的规则在做把控，并不是10万多员工都可以创造应用，也不是把创建的应用都像大水漫灌似地推给每一位员工。”</p><p></p><p>与此同时，如果从硬币的另一面再来看效率提升这件事，颉鹏伟还进一步强调，“海底捞的提效并不以减员为目的，而是为了让有限的资源放到更有价值的方面。假设原本20个人可能产生&nbsp;500&nbsp;万的价值，现在20&nbsp;个人可能就可以产生1000&nbsp;万的价值。”</p><p></p><p>比如，用技术赋能技术团队，他们就得以聚焦更复杂的业务系统开发中，不断提升用户体验；而用技术赋能业务，则使得门店更多人力能被释放到前台现场的顾客服务中，不断提升客户体验。如此种种，恰恰就是海底捞做数字化的理念核心。</p>",
    "publish_time": "2023-07-17 12:32:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]