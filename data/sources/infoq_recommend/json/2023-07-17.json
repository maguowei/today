[
  {
    "title": "突破调试瓶颈：Uno平台的VS Code扩展支持调试移动应用程序",
    "url": "https://www.infoq.cn/article/vUTZv9GP8T8P4eLWaGBH",
    "summary": "<p><a href=\"https://platform.uno/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Uno</a>\"，一个用C#和XAML构建原生移动、桌面和WebAssembly应用程序的框架，<a href=\"https://platform.uno/blog/announcing-net-mobile-debugging-in-vs-code-mobile-development-in-vs-code-with-uno-platform-or-net-maui/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">发布了新版</a>\"的Visual Studio Code扩展。新版本增加了对构建移动应用程序以及直接在Visual Studio Code中调试应用程序的支持。</p><p></p><p>使用Uno的.NET开发人员现在可以直接在Visual Studio Code中构建、运行和调试移动应用程序。它支持Visual Studio Code可用的所有调试特性，例如设置断点、条件断点或在异常时中断。这个扩展还支持<a href=\"https://code.visualstudio.com/docs/editor/debugging#_logpoints?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">打印消息</a>\"，无需进入调试器和检查变量。开发人员可以对自己的代码使用这些特性，也可以通过<a href=\"https://github.com/dotnet/sourcelink?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">SourceLink</a>\"或<a href=\"https://github.com/dotnet/roslyn/issues/12625?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">嵌入PDB文件</a>\"的方式对第三方代码使用这些特性。</p><p></p><p>由于Uno应用程序和.NET MAUI应用程序是在同一个.NET基础上构建的，因此开发人员也可以使用<a href=\"https://platform.uno/vs-code/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Uno扩展</a>\"来构建和调试.NET MAUI项目。</p><p></p><p>除了调试之外，该扩展还为XAML提供了代码补全和热重载功能，C#的热重载功能预计将在即将发布的版本中推出。这些特性可用于<a href=\"https://platform.uno/docs/articles/getting-started/requirements.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Uno支持的所有类型的项目</a>\"。此外，如果开发人员使用的操作系统不支持他们正在构建的目标，他们可以<a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">连接到Visual Studio Code的远程实例</a>\"并在那里运行项目。例如，Windows或Linux开发人员可以调试运行在远程macOS机器上的iOS或Mac应用程序。</p><p></p><p>调试移动应用程序是社区中呼声较高的一个特性，而社区也热情地迎接调试移动应用程序的特性发布公告。在推特上，<a href=\"https://twitter.com/UnoPlatform/status/1653457840600764417?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">发布公告推文</a>\"收到了200多个点赞和正面评论。微软高级内容开发者<a href=\"https://twitter.com/alvinashcraft/status/1653463300024201218?s=20&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Alvin Ashcraft说</a>\"：“这太棒了！我等不及要试用它了。”</p><p></p><p><a href=\"https://www.reddit.com/r/dotnet/comments/135wmbc/full_net_mobile_debugging_in_vs_code_with_either/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Reddit的.NET社区</a>\"也表达了同样的观点。<a href=\"https://www.reddit.com/user/pinedax?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">用户pinedax分享</a>\"了他们使用这个扩展的经验：“它也可以在Linux上运行。”</p><p></p><p>对这个Uno扩展的积极反响反映在它在<a href=\"https://marketplace.visualstudio.com/items?itemName=unoplatform.vscode&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">Visual Studio Marketplace</a>\"的评分上，该扩展的下载超过了10000次。</p><p></p><p>Uno是开源的，其Visual Studio Code扩展是免费的，但是是闭源的。Uno团队呼吁开发人员帮助测试其新功能，并<a href=\"https://github.com/unoplatform/uno/discussions?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">在GitHub上分享反馈</a>\"。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/07/uno-platform-debugging/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2ODkzMDIyMDksImZpbGVHVUlEIjoidk1xSmc5c052c29zNmhoTiIsImlhdCI6MTY4OTMwMTkwOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.-Wr1O6rd5girE4QJhSSLSwyGQw64JefaUvIuzAvUeyc\">https://www.infoq.com/news/2023/07/uno-platform-debugging/</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/zi0OdBgVJuoVsqc2S4rB\">VS Code有多么不安全：一个扩展就可能导致公司GitHub中的所有代码被擦除？</a>\"</p><p><a href=\"https://www.infoq.cn/article/4zm3T5aeOKzej15X9YKR\">另一种“推翻”&nbsp;VS Code 的尝试：JetBrains Fleet 现开放公测</a>\"</p>",
    "publish_time": "2023-07-17 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "架构师特刊：天工开物 AIGC",
    "url": "https://www.infoq.cn/article/CTbOrxaOBwlDXwCYw6ae",
    "summary": "<p></p><h2>卷首语</h2><p></p><p>史学家把十六、十七世纪的中国称为“天崩地解”的时代，社会经济关系、工农业内部结构发生变化，科学技术快速发展，因此，民间诞生了《天工开物》、《本草纲目》这样的奇书。三百余年过去了，我们再一次来到了科学技术重构社会经济关系的拐点——AIGC，这个最有想象力的词正在以巨大的影响力，渗透到各行各业。在这样的背景下，为了给大家带来经得住“拷问”的高质量内容，InfoQ 发起了《天工开物 AIGC》架构师特刊。</p><p></p><p>在内容设计方面，我们决定从“水面上”、“水面下”两个维度着手。在“水面上”的部分，我们整理 AIGC 领域近期的热点新闻和测评，比如 ChatGPT 进入车载系统的消息、Google 等大厂对 AIGC 辅助开发的态度，希望借此最大限度地保证你的知情权。或许当下所处的行业、所任职的岗位并未受到 AIGC 太猛烈的冲击，但中立的、实时的内容报道，可以保证你时刻观测 AIGC 的最新行业进展，在变革到来之际早做准备。</p><p></p><p>在“水面下”的部分，我们更注重弥合信息差，击破信息墙。近期有很多企业家在组织企业团队做调研，也有技术专家、产品专家远赴硅谷走近 OpenAI，建立了对 AIGC 更加深度的认知。我们借 Prompt 工程师的发展前景、AIGC 辅助编程实践、AIGC 大模型算力优化、AIGC 重构智能客服等几个维度，联系相关企业、负责人，做了深度访谈，希望有条理地、尽可能地把行业第一手信息同步给更多的读者。</p><p></p><p>从实际访谈、调查结果来看，各行业对 AIGC 的理解和实践是超乎想象的，从去年 12 月，InfoQ 于国内第一次报道 ChatGPT 爆火至今，短短七个月，很多研发团队已经积累了成熟的 Prompt 方法论，在在线设计、在线文档、金融等领域做出了诸多产品研发尝试，甚至研发团队已经非常习惯 Copilot 的辅助。与过往的元宇宙、Web3 相比，AIGC 的影响更为深远，变革更为迅速。动作最为迅速的企业，其 AIGC 应用已经发布上线；Prompt 工程师岗位已经完成 JD 设计，并面向社会开始招聘。</p><p></p><p>此外，我们特别发起了四期直播，针对以上话题，邀请了行业十余位有实操经验的技术高管、骨干圆桌讨论，超过三十万人在 InfoQ 视频号观看了这些直播。直播的回放我们一并附在本期架构师特刊中，以飨读者。</p><p></p><p>这期特刊包含的内容很多，但我们不想给大家制造焦虑。信息的透明与同步，带来的应该是真正的平和。我们希望在这个信息爆炸的时代，去伪存真，通过经过调查的内容，给你带来足够的信息增量，有足够的参考价值。帮助你更便捷地享受这一波科技红利，更大程度地促成技术普惠。</p><p></p><h2>目录</h2><p></p><p>热点 | Hot</p><p></p><p>用 AIGC 写 2023 高考语文作文，结果如何？</p><p></p><p>程序员越“老”就越看不上 AI 辅助编程工具？Stack Overflow 2023 开发者调查 AI 特别报告</p><p></p><p>ChatGPT 正式进入车载系统：奔驰首测 AI 语音助手，可进行复杂对话</p><p></p><p>谷歌警告自家员工：不要使用 Bard 生成的代码</p><p></p><p>微软也搞起了开源小模型！利用 OpenAI 的 ChatGPT 和 GPT-4 训练，实力碾压当前最强开源模型</p><p></p><p>AIGC 领域最大收购：Databricks 花费 13 亿美元买下只有 15 名研发的小公司！</p><p></p><p>专题访谈 | Interview</p><p></p><p>实访用人单位：Prompt 工程师真是低门槛“香饽饽”？</p><p></p><p>神器还是垃圾？那些用 AIGC 编程的人，实践得怎么样了</p><p></p><p>AI 大模型竞争白热化，算力优化才是“超车点”？</p><p></p><p>AIGC 重构智能客服，能否淘到大模型时代的第一桶金？</p><p></p><p>要想用好 GPT，我们必须跑得比“黑客”更快</p><p></p><p>探索大模型智能：众安保险基于 AIGC 的应用实践</p><p></p><p>推荐文章 | Article</p><p></p><p>GitHub Copilot：做出一个划时代的产品，只需要 6 个人</p><p></p><p>大语言模型进化之谜：涌现现象的挑战与争议</p><p></p><p>编程已死，AI 当立？教授公开“唱反调”：AI 还帮不了程序员</p><p></p><p>LangChain：2023 年最潮大语言模型 Web 开发框架</p><p></p><p>扫码下载</p><p><img src=\"https://static001.geekbang.org/infoq/98/98e4e92be2603b6c09b1b8e4a363555c.png\" /></p><p></p>",
    "publish_time": "2023-07-17 09:51:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据库行业的新篇章：国产化发展潜力几何？",
    "url": "https://www.infoq.cn/article/Au8A0GLJcHPNJQp3Vizx",
    "summary": "<p>近年来，全球范围内创新型数据库企业和产品不断涌现，我国数据库产业和生态日益繁荣，正在进入高质量发展期。《数据库发展研究报告（2023 年）》显示，2022 年我国<a href=\"https://xie.infoq.cn/article/9b1b88ee2bf5190a4f2e7489c\">公有云</a>\"数据库市场规模首次过半，预计 2023 年公有云市场占比将进一步扩大达到 59.8%。腾讯云数据库总经理王义成表示，这个预测结果非常合理：“云数据库分为公有云和私有云两种形式。私有云是建立在云厂商基础上的，例如中国建设银行、中国银联、数字广东和浙江政务等企业都是使用腾讯云、阿里巴巴和华为等云厂商提供的私有云进行业务孵化。公有云市场仍然在持续增长，但纯数据库软件市场将会逐渐稳定甚至逐步下降。”</p><p></p><p>当下，国内企业对云数据库的关注度有所增加，譬如快手和美团等大型企业逐渐放弃自建的 IDC 部署转向公有云。在金融和政务领域，一些创新型的企业会选择公有云或行业云来满足其业务需求，而对于核心业务要求稳定性和安全性的企业则相对较难改变现有的机房部署方式。总体而言，云数据库的发展将继续增长，特别是公有云市场，随着云的便捷性和弹性优势的日益突出，越来越多的行业和企业将选择将其数据迁移到云上。</p><p></p><p>在这种趋势下，2023 年 7 月 4-5 日，由中国信息通信研究院、中国通信标准化协会指导，中国通信标准化协会大数据技术标准推进委员会（CCSA TC601）与 InfoQ 联合主办了“2023 可信数据库发展大会”，腾讯云数据库总经理王义成在本届大会上发表了《腾讯云数据库 TDSQL 助力金融业核心系统国产化转型》的主题演讲分享，并在会后接受了 InfoQ 的专访。</p><p></p><p></p><h2>一、数据库国产化进程显著提升</h2><p></p><p></p><p>“十四五”规划提出“加快数字化发展”的总体布局，数据库作为金融信息系统的关键基础设施，其行业景气度持续提高，数据库国产化趋势愈发明显，上百家金融业试点单位在数据库国产化的进程中，进一步增强信心，向 50% 国产化率大步迈进。</p><p></p><p>如今大批量的应用其实都是基于 Oracle 构建的，在国产化替代过程中，厂商就要提供很多在驱动、内核、资源、架构等层面与 Oracle 兼容的能力，构建全面的 Oracle 兼容解决方案，从而实现迁移、平滑的双轨运行以及互相切换等。</p><p></p><p>关于如何更好地从Oracle手里“接棒”，王义成在接受 InfoQ 的采访中表示，“国产数据库与 Oracle 在数据库核心能力上差距其实不大，目前国产数据库可能更多专注于自身，但是客户看待数据库是一体化的视角。以排查处理故障为例，数据库厂商应该跳出自己，去沉淀全套故障排查和高可靠的白屏化能力，这样双方的人力和效率都能有大幅提升。数据库绝非是数据库本身，需要考虑如何从功能、架构、优化、运维等全方位角度帮助用户使用好数据库，这样厂商才能走得远。”</p><p></p><p></p><h2>二、腾讯云数据库 TDSQL 在金融行业展现国产数据库实力</h2><p></p><p></p><p>当今，在以数据要素驱动、数据价值实现为核心的数字化转型大潮中，企业如逆水行舟，不进则退，如果不跟上数字化转型的步伐，终将会被用户抛弃、被竞争对手超越、被市场边缘化，以致最终出局。</p><p></p><p>落实传统数据库转型、布局智能化建设可以说是近年来金融行业的两大工作重点，腾讯云数据库一直在赋能金融行业的核心数据库替换方面“开疆辟土”，其数据库产品<a href=\"https://xie.infoq.cn/article/dcc9581dc2f67b7c70da1bc05\"> TDSQL</a>\" 在推动数据库技术实现安全可控的道路上取得了开拓式创新，已累计为 3000+ 的政企和金融机构提供数据库的公有云及私有云服务，客户覆盖银行、保险、证券、互联网金融、计费、第三方支付、物联网、互联网 +、政务等诸多领域，为国产化数据库的发展和应用积累了宝贵经验。</p><p></p><p>王义成在演讲中表示，从技术角度，金融行业核心下移数据库选型大致有两条路线。其一是，历经 30 多年全球海量场景淬炼的开源方案，其数据库核心引擎基于 B+ 树的数据结构，对磁盘亲和，并深度优化 MySQL、PostgreSQL 内核，具备研发分布式化、金融级高可用、一致性、智能运维等能力；其二多是采用新技术进行研发的 NewSQL 路线，其数据库核心引擎基于 LSM-Tree 数据结构，对内存亲和，计算层和存储层完全分离。</p><p></p><p>要知道，金融行业里的每个机构发展状况都各不相同，在数据库的选型方面也存在差异，需要针对具体应用场景对数据库能力的需求和侧重，选择适合自身的数据库产品：比如互联网银行、互联网保险在线类业务由于在扩容方面的要求比较高，会更偏向于敏态扩容方案或弹性方案；又如，一些银行传统业务会选择基于相对稳定的内核构建的分布式解决方案来做业务支撑；再如，网上交易核心、渠道核心业务会更倾向于 NewSQL 方式。</p><p></p><p>针对金融行业这种传统行业在数据库选型中的变化与差异，腾讯云数据库提供了相应的三种解决方案：</p><p></p><p>针对核心业务系统替换，提供极具竞争力的产品和定价来支撑金融、政务、保险、资管、零售这类核心业务系统的替换，在底层采用基于比较稳定的内核构建的分布式数据库架构来支撑。</p><p></p><p>依靠标准化产品的被兼容、拓展和多态部署能力，去拓展更多行业场景（如能源、交通、制造等），助力更多行业的数字化转型。</p><p></p><p>拥抱云原生，解耦存储和计算，无缝嵌入云原生开发工具，规模化满足新应用的底层数据需求。</p><p></p><p>TDSQL 作为腾讯云多年持续投入研发的数据库产品，它基于分布式架构，无论是资源还是功能都能提供良好的扩展性，通过软硬结合的方式支持读写分离、秒杀、红包、全球同服等超高性能场景，能够确保多副本架构下数据强一致性，避免故障后出现集群数据错乱和丢失；同时，通过数据库防火墙、透明加密、自动脱敏等保障企业级安全性，减少用户误操作或黑客入侵带来的安全风险；在金融级高可用方面，还具备跨区容灾、同城双活、故障自动修复等特点；此外，TDSQL 还通过智能 DBA、自助化运营管理后台等配套设施来提供便携的运维。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/55/a9/559019990060a133e753a95a471755a9.png\" /></p><p></p><p>性能：针对不同的 SQL 语句，通过基于代价和规则的判断来提高执行效率，同时优化复制机制，确保数据链路在从逻辑到物理复制过程中不受网络拖累；</p><p></p><p>稳定性：TDSQL 的 HA 模块经过长时间的调优，能够应对各种场景下的稳定性切换；</p><p></p><p>数据安全和隐私保护：在事前阶段通过配置规则、安全组、白名单、SQL 防火墙来保障数据安全性，在事中阶段通过 SQL 拦截、SQL 黑名单等措施，防止大规模脱库和攻击行为；在事后阶段，支持对事件进行溯源追溯；</p><p></p><p>数据的脱敏和加密：支持链路层加密、存储层加密和针对特定列项的脱敏加密，在演练环境中还可对指定库进行脱敏操作从而保护隐私数据。</p><p></p><p>腾讯云 TDSQL 是国产数据库艰辛爬坡的一个缩影。大家看到的，是国产数据库在各行各业的大量投产以及市场份额的逐步提升，大家看不到的，是背后无数的打磨与优化。</p><p></p><p>三年前，腾讯云 TDSQL 的兼容性还不够完善，经历了在驱动、语法、性能层面的不断打磨和攻坚，目前已经可以独当一面。比如某头部保险公司使用 TDSQL 全面替换 Oracle，目前已在线上平稳运行 24 个月以上，部署了数万核心；又如，阳光保险 OA 系统凭借 TDSQL 的高度 Oracle 兼容能力，实现了业务短期迁移上线，生产环境更新表结构效率提升 50%；再如基于 TDSQL-C 的查询能力，能够打破 CPU 单核限制，实现整体的性能提升。</p><p></p><p>除了国内市场的数据库国产化替换，中国数字技术在出海方面也取得了不错的成绩：实现金融服务闭环，海量数据、高频交易、网银业务是印尼银行新项目面临的巨大挑战，TDSQL 以专业的可靠性、安全性和体验为印尼银行的数字银行核心系统赋能，将 TDSQL 纳入其核心系统后，2022 年印尼银行每天可以处理 200 万笔交易和 150,000 笔贷款支付，TDSQL 的高性能也为印尼银行在一年内获得 2000 万用户和 3500 万账户的业务爆发式增长提供了有力支持。</p><p></p><p>TDSQL 多次入围 Gartner 全球数据库魔力象限，其 OLTP 能力获得了国内第一的好成绩，是如今金融行业核心系统替换过程中的优选数据库。在前段时间的 TPC-C 打榜中，TDSQL 性能达到每分钟 8.14 亿笔交易（tpmC），打破了世界记录，同时也构建了业界最大的分布式集群，平均时延等领先其它厂商；在性价比方面，通过软件优化降低了硬件成本，通过公有云模式降低了服务成本，仅用行业三分之一的单位成本，就扛住了更大规模的并发，实现了超大规模集群性能稳定性，在 8 个小时持续压测过程中，tpmC 波动率一直处于 0.2% 以下，远低于标准的 2%，刷新了全球数据库极限，这充分证明了 TDSQL 承载金融核心场景的能力，这既给国产数据库的研发增强了信心，也给国产数据库的使用者增强了信心。</p><p></p><p></p><h2>三、国产数据库发展需要全行业共同努力</h2><p></p><p></p><p>《数据库发展研究报告（2023 年）》显示，中国数据库厂商有 150 家，中国市场虽然很大，但让每家厂商都能存活下来却异常艰难，想要在群雄逐鹿的中国数据库市场跻身前列，交付成本和升级成本是两大亟需解决的事情。得益于云厂商升级过程全部依赖于原厂能力这一天生优势，相较于传统独立的数据供应商，云厂商的升级成本很低。而在交付成本方面，构建生态是非常关键的因素，有多少合作伙伴愿意在上游做适配在下游拥抱数据库做数据支撑非常关键。</p><p></p><p>据此，腾讯云认为绑定合作生态是非常重要的，未来腾讯云将打造以 TDSQL 为中心，上下游产业协调生态圈，抓住国产化软件替换的大潮，在操作系统、芯片及整机厂商、中间件厂商做兼容性适配，在集成商、认证服务商，共同打造 TDSQL 服务生态环境，促进数据库技术的快速演进。</p><p></p><p>腾讯云 TDSQL 通过其稳定性、功能丰富和易用性等优势，在国产化数据库的竞争中，为客户提供了可靠的数据库解决方案。比如第七次全国人口普查从线下搬到了线上，数据库需要承载全国十多亿人口数据，并要求在 15 天内完成数据的高速入库汇聚，支持海量数据的分析，无论是数据量还是并发度，靠传统集中式数据库无法解决这一难题。TDSQL 基于多年打磨的 HTAP 混合负载能力，在 2020 年底完整支撑了第七次人口普查海量数据高速入库和海量数据的多维统计分析。此外，TDSQL 还参与了首家运营商核心系统数据库分布式国产化改造项目，TDSQL 团队利用兼容工具进行数据迁移，并逐步提高产品语法的兼容性，最终在项目中贡献了 98% 的兼容性，对于剩下的 2%，TDSQL 团队与开发商合作进行了改动，收获了客户的认可。</p><p></p><p>目前我们已经观察到，当前企业对于国产数据库的选型能力正在逐步提升，不同行业的企业已经具备了一定的判断力和自主选择性。拿金融行业来说，无论是大型机构还是中小型企业，都具备了较为成熟的选型能力，其中一些大型机构更倾向于自主制定选型策略。而中小型机构则会参考其他同类机构的选型经验，以降低选型风险。王义成在采访中的回答也印证了这一观点：“在其他行业中，大企业通常会参照其他大企业的做法，选择被广泛认可的数据库，以优化数据处理和提升业务效率。例如，当某行业的一家领先企业选用了某款数据库产品后，其他企业也会倾向于选择相同的数据库，以保障数据处理的稳定性和安全性。”</p><p></p><p>但不管怎么说，云数据库国产化已经走上了高速发展的阶段。从企业应用层面来看，整个国产化替换都已经到了一个关键时期，厂商单打独斗的力量总是有限的，只有各数据库厂商团结上下游的力量，共同应对技术瓶颈，才能实现国产数据库的技术突破。</p><p></p><p></p><h2>四、国产数据库的下一个竞技场在哪？</h2><p></p><p></p><p>随着 5G、云计算、大数据等的跨越式发展，数据库技术不断演进。王义成在接受采访时表示，在当前的数据库发展中，有两个比较明确的方向——分布式数据库和云原生数据库，这两个方向已经得到广泛应用，并取得了成功的发展；另外，在非关系型数据库领域，出现了多种类型的数据库，如图数据库、向量数据库、键值型内存数据库和文档型数据库等，都正在“齐头并进”式发展。</p><p></p><p>其中，向量数据库是王义成比较看好的，也是当今较为热门的数据库技术之一，它在大模型应用和人工智能应用中具备一定的优势。尽管向量数据库存在发展机会，但目前它仍然处于初级阶段，而且还需要注意的是，向量数据库并不是一个可以独立主导所有数据的数据库，它更多是一个与关系型数据库和 AP 数据库相结合，提供贴近 AI 场景的实时存储功能解决方案。</p><p></p><p>目前<a href=\"https://www.infoq.cn/article/ElrMobhxqr1JJIF5rXUK\">向量数据库</a>\"技术仍需各厂商进一步探索，以在具体的垂直领域或应用场景中获得更大的爆发大，但随着大模型技术的日益火热，相信向量数据库的发展也将进入快车道，成为各厂商竞技的新赛场。</p><p></p><p>对于腾讯云来说，其正在进行向量数据库的自研工作，并于近日发布了国内首个 AI Native 向量数据库 VectorDB，可支持 10 亿级向量检索规模，延迟控制在毫秒级。相比传统单机插件式数据库检索规模提升 10 倍，同时具备百万级 QPS 峰值能力。针对大模型场景，在接入层、计算层、存储层实现了全面 AI 化，使得企业接入大模型的效率提升 10 倍。腾讯云已经在内部使用向量数据库来支持音乐搜索和其他业务应用，并计划在今年 8 月开始面向公有云客户推出公测，大家可以期待一下。</p>",
    "publish_time": "2023-07-17 09:53:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023中国企业数字化人才发展白皮书",
    "url": "https://www.infoq.cn/article/7HbjMqJvZvBlvOk0Qf0J",
    "summary": "<p></p><h1>目录</h1><p></p><p> PART 01 数字化产业与人才融合发展呈现平台化生态化特征</p><p></p><p>经济的落脚点是产业，产业的落脚点是企业，企业发展的落脚点是人才，人才发展的落脚点是人才服务尤其是人才发展服务。</p><p></p><p>产业集群数字化转型彰显了数字经济的平台化生态化特征。数字化人才在这个生态中呈网络状分布，与产业生态相匹配的数字人才发展服务生态更好地促进了产业与人才的融合发展。</p><p></p><p>PART 02 技术降低学习门槛发展需要创新探索</p><p></p><p>数字技术的成熟和普惠应用降低了学习门槛。基层人才也能快速开发相关应用。但是领导团队的思维制约了一批企业的数字化。更广泛地实现企业领导团队的数字化转型，让企业行动起来，开展各种创新探索，可以更快地发展数字化人才。</p><p></p><p>PART 03 平台赋能是数字化人才发展主旋律</p><p></p><p>平台赋能成为企业内部人才发展的趋势，平台型企业更以其率先行动促进了企业外部平台赋能的生态体系形成。在此背景下，人才发展的自主性和自驱性也被调动起来。平台赋能成为数字化人才发展的主旋律，数字化人才发展不是难题。</p><p></p><p>PART 04 不同生态位企业 数字化人才发展重点与路径</p><p></p><p>共性底座企业承担着数字经济筑底的责任，在前瞻性领导人才和尖端科技人才发展上做了充分布局，平台型企业以应用技术人才为核心向两头延伸，咨询服务企业正在大规模扩充人才队伍，数字应用企业也已经形成了丰富的标杆案例。更多的企业需要拥抱平台赋能的数字人才发展生态，实现更广泛地企业数字化转型。</p><p></p><p>扫码下载</p><p><img src=\"https://static001.geekbang.org/infoq/89/89dfb68f34fca0b8973d8b1571600d6d.png\" /></p><p></p>",
    "publish_time": "2023-07-17 10:04:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "最新进展：Serverless 冷启动优化技术",
    "url": "https://www.infoq.cn/article/2KNoj9SI8cGGM1em4Ud3",
    "summary": "<p>作者：华为云Serverless团队</p><p></p><h1>问题背景</h1><p></p><p></p><p><a href=\"https://www.infoq.cn/article/6SUNgEE6BX1xtDkRXHME\">Serverless计算</a>\"也称服务器无感知计算或函数计算，是近年来一种新兴的云计算编程模式。其致力于大幅简化云业务开发流程，使得应用开发者从繁杂的服务器运维工作中解放出来（例如自动伸缩、日志和监控等）。借助Serverless计算，开发者仅需上传业务代码并进行简单的资源配置便可实现服务的快速构建部署，云服务商则按照函数服务调用量和实际资源使用收费，从而帮助用户实现业务的快速交付 (fast built &amp; Relia. Deliv.)和低成本运行。</p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a1bf7c5f10a12d8199a499058ff8b9f.png\" /></p><p></p><p>在当前阶段，业界公认的Serverless计算的准确定义应该为“FaaS+BaaS”，即Function-as-a-Service 同 Backend-as-a-service的组合。Serverless计算改变了用户使用云的方式，为了实现细粒度的资源供给和高度弹性的扩缩容能力，Serverless计算提供了无状态函数的编程抽象，即将用户的应用程序构建为多个无状态的函数集合，并通过中间存储、BaaS等业务组件交互，继而构建完整的云应用。</p><p>&nbsp;</p><p>然而，Serverless计算的无状态函数编程在带来高度弹性和灵活性的同时，也导致了不可避免的冷启动问题。由于函数通常在执行完请求后被释放，当请求到达时，如果没有可用实例则需要从零开始启动新的实例处理请求（即<a href=\"https://www.infoq.cn/article/mI0lpQuvXi7BPCQ7oU9u\">冷启动</a>\"）。当冷启动调用发生时，Serverless平台需要执行实例调度、镜像分发、实例创建、资源配置、运行环境初始化以及代码加载等一系列操作，这一过程引发的时延通常可达请求实际执行时间的数倍。相对于冷启动调用，热调用（即请求到达时有可用实例）的准备时间可以控制在亚毫秒级。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18addb3a53ad6dbdf39404196c9da1bd.png\" /></p><p></p><p>&nbsp;在特定领域例如AI推理场景，冷启动调用导致的高时延问题则更为突出，例如，使用TensorFlow框架的启动以及读取和加载模型可能需要消耗数秒或数十秒。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/09/09e9723e875ec84cd0981d4b3052cc15.png\" /></p><p></p><p>&nbsp;因此，如何缓解Serverless函数的冷启动问题，改善函数性能是当前Serverless领域面临的主要挑战之一。</p><p>&nbsp;</p><p>从研究思路上看，目前工业界和学术界主要从两个方面入手解决冷启动问题：</p><p>&nbsp;</p><p>加快实例启动速度：当冷启动调用发生时，通过加速实例的初始化过程来减少启动时延。降低冷启动发生率：通过函数预热、复用或实例共享等方法提高实例的利用效率，减少冷启动调用的发生。</p><p></p><h1>业界最新进展</h1><p></p><p></p><p>在本篇文章中，我们首先从提高实例启动速度方面介绍下业界的前沿进展。</p><p></p><h2>提高实例启动速度</h2><p></p><p>&nbsp;</p><p>当冷启动发生时，Serverless平台内部实例的初始化过程可以划分为准备和加载两个阶段。其中，准备阶段主要包括控制面决策调度/镜像获取、Runtime运行时初始化、应用数据/代码传输几个部分。而加载阶段位于实例内部，包括用户应用框架和代码的初始化过程。在工业界和学术界公开的研究成果中，针对实例启动过程中的每个阶段都有大量的技术手段和优化方法。如下图所示，经过优化，实例冷启动的准备阶段和加载阶段时间可被极大地缩短。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c77d7f5ead9558519000935b0ce9537b.png\" /></p><p></p><p>下面列举了一些近年来发表在计算机系统领域知名会议的相关工作，主要可以分为五个方面：&nbsp;</p><p>&nbsp;</p><p>调度优化/镜像快速分发/本地池化：例如利用文件级镜像缓存复用机制和并行化镜像构建技术，加快容器构建速度 FAST-BUILD [MSST'19]；阿里云基于树结构的跨节点快速镜像分发 FaasNet [ATC'21]；AWS基于Block和分布式多级缓存的镜像分发策略 [Arxiv'23]；Pod池+特化实例跳过镜像传输 [华为云FunctionGraph]。其中，快速镜像分发依赖于VM节点的上/下行网络带宽，Pod池特化技术则是典型的以空间换时间的做法。轻量级虚拟化/安全容器：例如针对传统容器Docker的精简优化工作SOCK [ATC'21]；更侧重安全性的轻量级虚拟化技术（Kata Containers, gVisor等)；基于安全容器的进一步的精简优化工作 (Catalyzer [ASPLOS'20], REAP[ASPLOS'21])。通过裁剪优化，安全容器的启动时延最快可以被压缩至亚毫秒级。数据共享/跨节点传输优化：例如基于RDMA共享内存减少跨节点启动过程的数据拷贝 RemoteFork [OSDI'23]；或者利用本地代码缓存跳过代码传输 [华为FunctionGraph, 字节ByteFaaS等]。基于RDMA技术的跨节点数据传输时延可降低至微妙级。用户代码精简/快速加载：例如针对Java语言的JVM（Java Virtual Machine）运行时优化技术 [FunctionGraph]；以及针对Python运行时库的裁剪优化工作FaasLight [arxiv'23]。通过特定的优化，JVM启动时间可由数秒降低至数十毫秒，而Python代码的启动加载时延可降低约1/3。其它非容器运行时技术：例如WASM（即WebAssembly）技术以及针对WASM的内存隔离方面的优化工作Faasm [ATC'20]。相比容器化技术，直接以进程和线程方式组织运行函数，可在保证低开销函数运行的同时具备高度灵活性。&nbsp;</p><p>&nbsp;</p><p>接下来，针对上述5个方面的优化工作进行详细介绍。</p><p></p><h3>镜像压缩/快速分发/本地池化</h3><p></p><p>&nbsp;</p><p>快速请求/实例调度。</p><p>&nbsp;</p><p>在Serverless场景下，由于函数的体积更小、动态性更高，Serverless平台在负载高峰时也会面临较大压力。因此主流的Serverless服务商通常采用“大小流区分的负载均衡+本地优先调度”的设计来实现请求的快速分发，降低调度时延。即在流量较小的时候，采用工作负载聚合的方式来分发请求，减少集群资源使用；在流量较大的时候，通过一致性hash或轮询等方法将请求快速映射到后端节点，从而降低排队时延。这种“自顶向下”的调度设计简单，易于实现，但是在超大规模的函数并发场景下仍可能面临顶层控制器的性能瓶颈问题，因此，根据实际场景架构适当地探索“自下而上”的调度框架设计将有助于缓解该问题。</p><p>&nbsp;</p><p>此外，也有相关研究人员提出可将Serverless集群划分为多个小型的worker池，每个worker池由一个单独的semi-global子调度器管理，降低各个调度器的压力，从而改善调度时延&nbsp;[1]。</p><p>&nbsp;</p><p>镜像压缩与快速分发。</p><p>&nbsp;</p><p>在Serverless平台中，用户的函数镜像通常被存储在cluster-level的镜像仓库，节点（通常指虚拟机Virtual Machine, 简称VM）创建实例时首先需要将镜像文件通过网络传输到对应的节点（1个100MB的镜像文件从传输到容器创建完成需要耗费约20秒 [阿里云，Borg]）。在这个过程中，影响镜像传输时延的因素主要为镜像体积和节点网络带宽。因此，Serverless平台通常会采用镜像压缩技术缩小镜像体积，例如开源工具fastfreeze [2]。除此之外，为了解决节点网络带宽受限导致的拉取镜像时间过长的问题，阿里云的FaasNet [29]工作提出了一种基于树状网络的跨节点镜像分发策略，即通过根节点的VM将镜像下发给子节点（通常为2），子节点获取到镜像后以同样方式扩展到更多的节点，从而充分利用节点的上/下行带宽实现短时间内快速扩容大量实例的目的。</p><p>&nbsp;</p><p>此外，在节点中设立镜像缓存也是一种有效的方法，可以在一定程度上减少镜像传输操作，例如AWS Lambda采用了一种基于Block的镜像切分和缓存方法 [30]，相比于基于Layer的镜像构建方法，基于Block的镜像切分粒度更细，可以极大地提高镜像复用度，同时这些镜像Block被存储在由本地缓存，分布式缓存和S3组成的三级缓存系统中，从而实现快速镜像分发和实例创建速度。</p><p>&nbsp;</p><p>Pod池与实例特化。</p><p>&nbsp;</p><p>类似于本地镜像缓存的概念，在集群中预留包含函数运行时基础镜像的实例（即具备用户代码运行环境的热容器）也是一种常用的“以时间换空间”的做法 [3]。这种方法在集群中建立Pod池，预先启动一定数量的具有不同资源规格和语言运行时的实例（例如Java, Nodejs, Python等）。当冷启动调用发生时，Serverless平台可以快速从Pod池中取出所需要的实例进行特化（挂载代码文件并加载执行），从而跳过镜像获取和运行时环境启动阶段，实现快速创建实例的目的。</p><p>&nbsp;</p><p>生产环境下的数据测算显示，Node.js实例特化时的启动时间仅需15ms。然而，在Pod池中预留实例会占用大量集群资源，增加云供应商的服务成本。如何权衡用户函数性能和Pod池预留成本也是该类方法所面临的一个问题。</p><p>&nbsp;</p><p></p><h3>轻量级虚拟化/安全容器加速</h3><p></p><p>&nbsp;</p><p>传统容器加速。</p><p>&nbsp;</p><p>相比于传统的VM动辄数十秒至数分钟的创建时间，容器的启动速度和运行时开销更小，因此Serverless平台通常使用容器技术来隔离用户的函数实例，例如Docker [4]。然而在考虑函数的实际运行性能时，Docker之类的传统容器设计包含了很多不必要的运行开销，因此并不适合直接作为函数沙盒环境。为此，SOCK工作 [5]基于Linux container的设计分析了Docker文件系统和网络模块的性能瓶颈，通过裁剪不必要的启动项并对网络模块进行特定优化，大幅降低了Docker容器的内核扩展开销（超过18倍）。</p><p>&nbsp;</p><p>兼顾安全性的轻量化容器或microVM。</p><p>&nbsp;</p><p>在非Serverless的云场景中，VM的强安全隔离性使其在云租户的服务部署中扮演着重要角色。在Serverless场景中，租户函数间的安全隔离性也同样重要。为了弥补传统容器的安全性不足的问题，研究学者尝试将VM的高安全性同容器技术进行融合，例如Intel和HyperHQ团队主导的Kata Containers项目 [6]，Kata Containers的本质是一个精简后的轻量级虚拟机，在其上面运行着用户的容器进程。为了减少暴露给容器进程的攻击面，Kata Containers基于传统的虚拟化技术通过硬件虚拟出一个microVM，而microVM里则运行了一个裁剪后的 Linux 内核来实现进程间的强隔离（注：裁剪即屏蔽掉非必须的系统调用接口）。AWS Lambda的Firecracker [7]也采用了类似microVM的技术路线来实现多租户隔离性和函数性能间的权衡。</p><p>&nbsp;</p><p>除此之外，gVisor [8]则是谷歌推出的轻量级安全容器项目。gVisor采用了另一条技术路线，其核心原理是给容器进程配置一个类似“Guest Kernel”的极小的“独立内核”（即守护进程），通过拦截容器进程的系统调用来向宿主机OS发起有限的、可控的系统调用，从而提高系统安全性。由于这个独立内核运行在用户态，免去了大量地址空间转换和内核态切换的操作，因此具有较小的性能开销和更好的灵活性，同时又达到了将容器和宿主机OS（Operation System）隔离的目的。</p><p>&nbsp;</p><p>无论是Kata Containers类容器技术，还是gVisor安全容器技术，它们实现安全容器的方法在本质上是殊途同归的。这两种技术都采用了“系统分层”的设计思想，即向容器进程分配了一个独立的“操作系统内核”，从而避免了让租户的容器直接共享宿主机的内核。这样，容器进程的攻击面就从整个宿主机内核变成了一个极小的、独立的、以容器为单位的内核，从而有效解决了容器进程发生“逃逸”或者夺取整个宿主机控制权限的问题。</p><p>&nbsp;</p><p>针对安全容器的进一步加速。</p><p>&nbsp;</p><p>尽管gVisor之类的安全容器极大地弥补了传统容器的安全隔离性不足的问题，然而，安全性提高的同时也带来了额外的性能损失，例如拦截调用会增大启动和运行开销。为此，Catalyzer [9]基于快照技术和状态重用技术针对gVisor安全容器的启动过程进行了进一步优化，大幅缩短了gVisor的启动时间。Catalyzer的设计思想是“避免从零开始启动”，即通过检查点和快照恢复机制跳过启动过程关键路径上的初始化（零初始化），从而实现快速启动的目的。此外，Catalyzer还提出了一个新的操作系统原语 sfork技术，通过直接重用正在运行的沙箱实例的状态来进一步减少启动延迟。评估表明，Catalyzer在最佳情况下可将gVisor的启动延迟降低至&lt;1ms。</p><p>&nbsp;</p><p>值得注意的是，尽管Catalyzer实现了“用户角度”的快速启动，但是其并不能保证内部所有的系统状态都为最新可用状态，为此，Catalyzer采用了按需加载机制作为补救措施，即通过在后续运行过程中逐步恢复用户级内存状态和系统状态来最终恢复函数性能。该领域的另一项研究工作REAP [10]发现Catalyzer的设计机制带来的开销并不是可忽略的，运行过程中频繁的系统缺页在最坏的情况下会导致时延提高接近一倍。为此，REAP提出了一种页面预加载机制，通过识别函数运行过程中所需的必要工作集并提前载入内存，从而显著改善了Catalyzer的缺页开销问题。</p><p>&nbsp;</p><p>注：该部分涉及的一些术语可以参考扩展知识部分进行了解。</p><p>&nbsp;</p><p></p><h3>数据共享/传输优化</h3><p></p><p>&nbsp;</p><p>基于RDMA的跨节点分发。</p><p>&nbsp;</p><p>在Serverless平台内部，借助网络进行跨节点数据、代码或者状态的传输也是一个较为耗时的操作，尤其是在大量实例并发创建的场景中，远程容器初始化的频繁需求进一步加剧了这种现象。为了解决该问题，MITOSIS [11]工作在支持RDMA的操作系统中实现了一个新的操作系统原语，即remote fork，remote fork允许不同节点的操作系统之间通过RDMA技术进行内存共享映射，从而实现快速远程内存读取和跨节点容器间的数据传输。MITOSIS可以大幅降低跨节点的实例初始化时间，从单个实例跨节点复制10,000 个新实例的时间小于1秒。然而，MITOSIS的实现涉及对操作系统内核代码的大量修改，同时还依赖于RDMA硬件的支持，这些都使得它在实际部署中会面临着挑战。</p><p>&nbsp;</p><p>除此之外，新型网络设备例如DPU硬件等也提供了基于RDMA的高速跨节点互联的机制，即通过内存映射和共享来加快网络数据传输速度，这些硬件都可以被探索用于改善Serverless平台跨节点数据传输的效率。</p><p>&nbsp;</p><p></p><h3>代码加载延迟优化</h3><p></p><p>前面章节提到的加速技术都集中在系统层和框架层，在应用层，容器运行时环境的启动和函数代码的加载也在冷启动延迟中占据了相当的比例，尤其是对于机器学习等应用，运行库的加载和框架的启动时间可达数秒 [12]。值得注意的是，即便调度时延和容器初始化时间再短，长的用户侧启动时延仍然会使得冷启动问题变得棘手。</p><p>&nbsp;</p><p>为此，Serverless平台通常会针对函数的运行时进行优化，例如华为的FunctionGraph内部针对JVM的优化技术可将简单Java应用的初始化时间降低至数十毫秒。在学术研究领域，也有针对Python函数运行时库的裁剪优化工作例如FaasLight [13]，FaasLight的核心思想是“在函数启动过程中尽可能地只加载少量运行必需的代码”。为了实现这一目标，FaaSLight构建了函数级调用图来识别和筛选与应用程序功能相关的代码，并对非强相关的代码（即可选代码）进行按需加载，避免必要代码识别不准确导致应用失败。实验结果显示FaaSLight可最高降低78%的Python代码加载延迟，平均延迟降低约28%。</p><p>&nbsp;</p><p></p><h3>非容器化运行时技术</h3><p></p><p>&nbsp;</p><p>WASM运行时。</p><p>&nbsp;</p><p>WSAM（即WebAssembly）[14] 技术也是近年来一种新兴的编程运行时，可以用在云场景中代替部分容器运行时来部署用户函数。WSAM的运行原理类似于JVM，可直接运行在宿主机OS上，其支持多种语言编码，用户函数代码在运行前需要编译成WASM运行时，继而实现跨平台运行。WASM同样提供了高性能沙盒环境，并具备高灵活性和一定的安全性。</p><p>&nbsp;</p><p>尽管WASM有诸多优势，但是也存在着诸如内存隔离性弱这样的问题，因此并不能完全替代现有的容器技术，而是在大多数情况下与Docker容器技术互相配合使用。据悉，基于WASM的函数计算平台目前在国内著名的云计算厂商内部都有布局，有的平台已经推出了初步的商用版本。</p><p>&nbsp;</p><p>在研究领域，也有针对WASM的内存隔离性开展优化的工作，例如Faasm [15]。Faasm提出了一个新的隔离抽象“faaslets”，并基于WebAssembly的software-default isolation (SFI) 机制实现了函数间的内存隔离。此外，在faaslets的设计中，来自不同租户的多个函数可共享相同的内存地址空间，这样就可以避免函数间通信时大量的数据交换所导致的性能开销。faaslets还可以与Linux cgroups机制配合使用，继而实现例如CPU, 网络资源等资源的隔离。这些优化措施可以使得Faasm获得接近2倍的性能加速同时降低内存使用10倍以上。</p><p>&nbsp;</p><p>值得注意的是，上述的这些技术手段，无论是容器技术、VM技术还是WASM技术，它们本质上都是以进程方式运行在服务器内部，因此都可以借助快照技术（例如Linux CRIU [16]）实现更快的系统状态保存和恢复，从而进一步加快初始化速度。</p><p>&nbsp;</p><p>Unikernal技术。</p><p>&nbsp;</p><p>从前面列举的一些技术的特征可以看到，函数运行环境的启动速度和安全性之间总是存在一个折中，即为了改善安全性必须添加一些中间层来增强系统的控制能力，然而中间层的引入势必会降低系统启动速度。那么是否存在能够同时满足快速启动和高安全性的技术方案呢？答案就是Unikernel [17]。Unikernel顾名思义，是指定制化的操作系统内核。它的核心思想是将“用户程序和运行必需的库文件统一编译成为一个特殊的、单地址空间的内核镜像”。由于只有一个操作系统进程（这里不再有用户态和内核态的区分），同时编译时去掉了不必要的运行库，所以Unikernel运行速度可以非常快。如果将Unikernel加载到VM运行，也可以实现高安全性和隔离性。</p><p>&nbsp;</p><p>但是Unikernel的缺点也十分明显，即灵活性差。由于Unikernel 是与某种语言紧密相关的，所以一个Unikernel只能用一种语言编写，如果用户程序有所变动，则需要重新编译Unikernel，之后加载并重启VM。同时，Unikernel的编码门槛也很高，这与Serverless计算的简化编程的设计初衷相悖，也是其难以被广泛应用的原因之一。</p><p>&nbsp;</p><p></p><h4>扩展知识介绍：</h4><p></p><p>&nbsp;</p><p>传统裸金属、VM和容器技术的区别 （在体系结构中所处的层级图、用户态和内核态概念）</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/03/039a0cf978767a8e9513d1a43b139e18.png\" /></p><p></p><p>&nbsp;</p><p>在计算机的体系结构中，各种物理设备例如CPU处理器、磁盘、网卡等首先组成计算机的物理硬件，也成为“裸金属服务器”。裸金属服务器的运行需要操作系统的支持，比如我们熟知的Linux操作系统。借助操作系统就可以编写运行各种软件例如网页服务器、数据库等来提供外部服务。</p><p>&nbsp;</p><p>在云计算场景中，通过虚拟机共享资源是早期云服务商的主要做法。借助虚拟化技术例如Microsoft hyper-V、Citrix&nbsp;Xen等，可以将裸金属服务器虚拟为多个单独的VM，VM间共享裸金属服务器（也称宿主机）的物理资源，但是在每个VM内对租户来说是独占资源的（例如CPU，内存和磁盘）。实现这一目的的手段就是借助Hypervisor（即虚拟机监视器）来管理划分各个VM使用的物理资源，实现高安全性和隔离性。Hypervisor的内部比较复杂，涉及到系统调用、物理地址划分等操作，这里不再展开。</p><p>&nbsp;</p><p>在Hypervisor划分出来的每个VM内部都可以运行一个独立的操作系统（也称Guest OS），相比于VM，宿主机的操作系统则称为Host OS。Guest OS对于租户来讲是一个类似本机的操作系统，包含运行程序所需要的各种系统调用库和可执行文件，具体的层级划分如左侧子图所示。</p><p>&nbsp;</p><p>容器则是相比VM更为轻量级的虚拟化技术，例如Linux Container（LXC）和基于LXC实现的Docker等技术（右侧子图所示）。容器的本质是一个用户态进程，因此它可以直接运行在Host OS上，我们称之为Container Engine。Container Engine可以创建出多个容器给租户使用，并通过Linux cgroups等机制实现namespace和租户资源（例如CPU和内存）的隔离。不同于VM的运作方式，容器内部并没有Guest OS，而是复用底层的宿主机的操作系统，通过创建文件系统和加载用户程序运行所需要的库来实现运作。因此，相比VM来讲，容器的运行方式更加轻便灵活，但是安全性较差（例如某个租户的程序可能导致宿主机OS被攻击，继而使得其他租户的程序也无法运行）。为了解决这个问题，早期的云厂商做法通常是将容器部署在VM中，从而实现安全性和灵活性的折中取舍。</p><p>&nbsp;</p><p>注：操作系统通常运行在内核态，权限最高，而租户程序则运行在用户态。用户态程序执行系统调用需要切换至内核态，执行完后再切换回用户态。</p><p>&nbsp;</p><p>VM、容器和Unikernel对比 （灵活性，启动速度和隔离性）</p><p>&nbsp;</p><p>前面我们提到，Serverless平台中函数运行的技术路线有VM，容器和Unikernal几种或是它们的组合。无论哪种方式，基本都在围绕技术方案的灵活性（Flexibility），启动速度（Startup Latency）和隔离性（Isolation Level）进行权衡。</p><p>&nbsp;</p><p>下图总结并列出了这几种虚拟化方案的对比：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ce4ee2d65da8ac7e7dcd854a978d3f89.png\" /></p><p></p><p>&nbsp;可以看到，VM具有高隔离性，灵活性较好，但是启动速度最慢。</p><p>&nbsp;</p><p>而传统容器以用户态进程运行，具有高度灵活性，较快的启动速度，但是隔离性较差。</p><p>安全容器是针对传统容器进行了裁剪和进一步控制，具有较高的隔离性，但是启动速度和灵活性有所降低。</p><p>&nbsp;</p><p>WASM技术类似JVM的设计理念，编译完成后即可运行，因此具有更快的启动速度和较高的灵活性，但是隔离性较差。</p><p>&nbsp;</p><p>Unikernel采用了定制化OS和单地址空间，因此可以兼顾快的启动速度和高隔离性，但是灵活性很差。</p><p>&nbsp;</p><p>不难发现，这三个要素的关系非常类似于分布式系统的CAP原理，即一致性 (Consistency)，可用性(Availability) 和分区容忍性 (Partitiontolerance)在分布式系统中最多只能同时实现两点，不可能三者兼顾。</p><p>&nbsp;</p><p>下表展示了各虚拟化技术的启动延迟等指标的具体对比：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cae984e39c26fd9736634db3c94cdde.png\" /></p><p></p><p>注：表格数据引用出处&nbsp;The serverless computing survey [31]。</p><p>&nbsp;</p><p></p><h3>小结</h3><p></p><p>在实际场景中，不同的函数计算厂商往往有自己的虚拟化技术方案，往往根据自家产品的业务线，技术积累或者客户群体需求进行综合考虑。在出于商业意图或者个人需要搭建函数计算平台进行研究时，可以根据安全性需求制定自己的技术路线。例如个人或者高校团体研究可以归类为非安全性场景，基于目前的\"VM+Kuberneters (k8s)+Docker+runtime优化\"技术栈就可以实现一个可用版本，其中VM可以提供资源强隔离性，基于多个VM节点组建k8s集群，函数计算框架例如OpenWhisk或OpenFaaS可以依托k8s进行部署。尽管同一个VM内的多租户共享会导致面临一定的安全隐患，但对于实验环境来说是足够的。</p><p>&nbsp;</p><p>在注重租户间隔离性和强调安全的场景下，AWS Lambda采用的“裸金属+Hypervisor+microVM+容器”技术路线通过microVM实现强隔离性，并且VM内一般只允许部署同一个租户的函数来提高安全性。而谷歌采用的“裸金属+Hypervisor+VM+gVisor”则是另一种技术方案，Hypervisor和VM负责提供硬件虚拟化及资源强隔离性，安全性则由gVisor提供。其中，Firecracker和gVisor均已开源。</p><p>&nbsp;</p><p>讨论：VM在每种技术方案都属于必需品的地位，但是这样势必会导致高开销的存在。那么能不能采用“裸金属+轻量级Hypervisor+Host OS+安全容器”或者直接“裸金属+Host OS+安全容器”的技术方案减少中间层的存在呢？其实这种方式并非不可以，比如剑桥大学就提出了CHERI [18]指令集扩展从而实现在硬件层面进行内存隔离，通过卸载掉虚拟化软件的部分职责，使得Hypervisor“变薄”。尽管这项工作只基于模拟器进行了实现，但是也验证了未来更轻量化的Serverless计算底座的可行性。&nbsp;</p><p></p><h2>降低冷启动发生率</h2><p></p><p></p><p>在文章的上半部分，我们主要介绍了如何加速函数实例初始化过程的一些技术方案，包括通过优化调度和镜像分发策略，采用更轻量级的虚拟化技术，或者借助RDMA硬件改善跨节点数据传输等方法，尽管这些方法已经可以将实例运行时环境的初始化的时间压缩至数十毫秒甚至是数毫秒，然而用户侧的延迟却仍然存在，例如程序状态的恢复，变量或者配置文件的重新初始化，相关库和框架的启动。具体来讲，在机器学习应用中，TensorFlow框架的启动过程往往需要花费数秒，即使实例运行时环境的启动时间再短，应用整体的冷启动时延对用户而言依然是无法接受的（注：通常大于200ms的时延可被用户察觉）。</p><p>&nbsp;</p><p>在这种情况下，可以从另一个角度入手解决冷启动问题，即降低冷启动调用的发生率。例如，通过缓存完整的函数实例，请求到达时可以快速恢复并处理请求，从而实现近乎零的初始化时延（例如Docker unpause操作时延小于0.5ms）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1e/1e899ac6cf60c3e57dfeed95b5655b4e.png\" /></p><p></p><p>&nbsp;降低冷启动发生率的相关研究可以分为如下几个方面：</p><p>&nbsp;</p><p>实例保活/实例预留：例如基于Time-to-Live的keepalive保活机制 [AWS Lambda, OpenWhisk]；或者通过并发配置接口预留一定数量的实例&nbsp;[AWS Labmda等]；这些方法原理简单，易于实现，但是在面对负载变化时缓存效率较低。基于负载特征学习的动态缓存：例如基于请求到达间隔预测的动态缓存方案 Serverless in the Wild [ASPLOS'20]；学习长短期负载变化特征的动态缓存方案 INFless [ASPLOS'22]；基于优先级的可替换缓存策略FaasCache [ATC'21]；面向异构服务器集群的低成本缓存方案 IceBreaker [ASPLOS'22]。这些动态缓存方案根据负载特征学习决定实例缓存数量或时长，从而在降低冷启动调用率的同时改善缓存资源消耗。优化请求分发提高命中率：例如兼顾节点负载和本地化执行的请求调度算法 CH-RLU [HPDC'22]。通过权衡节点负载压力和缓存实例的命中率来对请求的分发规则进行优化设计，避免节点负载过高导致性能下降，同时兼顾冷启动率。改善并发/实例共享或复用：例如允许同一函数工作流的多个函数共享Sandbox环境 SAND [ATC'18]；使用进程或线程编排多个函数到单个实例中运行 Faastlane [ATC'21]；提高实例并发处理能力减少实例创建 Fifer [Middle'20]; 允许租户复用其它函数的空闲实例减少冷启动时间 Pagurus [ATC'22]。这些实例共享或者复用技术可以同缓存方案结合使用，降低冷启动带来的性能影响。</p><p>&nbsp;</p><p>接下来，针对上述4个方面的研究工作进行详细介绍。</p><p>&nbsp;</p><p></p><h3>实例保活/实例预留</h3><p></p><p>&nbsp;</p><p>实例并发设置和预留。</p><p>&nbsp;</p><p>商用的Serverless平台例如AWS Lambda [19]、华为云FunctionGraph [20]都内置了Time-to-live实例保活机制即keepalive方法。其原理是将执行完请求的实例内存中保活一段时间再释放。如果保活期间有新的请求到达，则可以立即执行并重置剩余的保活时间。除此之外，云服务器通常会向用户提供实例并发设置的接口，用户在部署函数后可以设置预留一定数量的实例（需额外支付费用），从而降低由于突发负载导致的冷启动调用率。然而，这类静态的缓存方法易于实现，但是实例预留数量或保活时长难以根据负载变化进行实时调整，从而导致性能下降或者资源超分浪费。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f0/f0877eec57a9a08d3d36e2dae1e227a4.png\" /></p><p></p><p></p><h3>&nbsp;</h3><p></p><p></p><h3>基于负载特征学习的动态缓存</h3><p></p><p>&nbsp;</p><p>基于直方图统计的动态实例缓存。</p><p>&nbsp;</p><p>微软的研究团队通过分析Azure的生产环境trace，发现请求的到达间隔普遍分布在秒级、数分钟乃至数小时，基于Time-to-live的静态保活策略会导致大量的资源浪费。因此，作者提出了基于混合直方图的策略来统计函数请求的调用间隔，并根据这些信息动态的释放或提前拉取函数实例，从而在减少冷启动调用率的同时减少Serverless平台内部的缓存资源浪费 [21]。然而，这套设计方案只针对0-1的函数扩缩场景进行了评估，因为它只控制实例的缓存时长，而无法感知实例数量的变化，如何将这套机制运用在1-多的实例扩缩场景中还面临着一些挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bd/bdfba5c42bbd75f029bf02f2856e6ae3.png\" /></p><p></p><p>基于长短期直方图的动态实例缓存。</p><p>&nbsp;</p><p>直方图策略通过生成函数请求到达间隔的分布来决定缓存实例的何时创建和缓存时长，然而，在初期历史统计数据不足或者负载发生短期突变的场景下，得到的直方图分布往往会不具有代表性，从而导致缓存策略的实际效果下降。为了解决该问题，INFless [22]提出了一个长短期结合的混合直方图策略来改善预测精度，通过分别统计长期的直方图分布特征和短期的直方图分布特征，来共同决定缓存实例的创建时间点和保活时长，从而改善缓存效率（冷启动率/缓存资源成本）。</p><p>&nbsp;</p><p>基于优先级的可替换缓存策略。</p><p>&nbsp;</p><p>无论是Time-to-live的keepalive还是基于直方图的动态缓存方案，都是倾向于减少缓存资源的消耗，从而降低函数服务提供商的运营成本。然而，FaasCache [23]的作者认为函数计算集群的内存资源与其与闲置，不如用来缓存更多的函数实例，从而最大程度地降低冷启动调用发生率。为此，FaasCache借鉴了CPU缓存的替换算法，通过函数的最近访问时间，函数内存消耗以及冷启动时长为每个实例定义优先级。请求执行结束后的函数实例都会被长期缓存在节点中，仅当节点无可用资源时才驱逐较低优先级的实例来缓存新的函数。此外，FaasCache还设计了一个反馈控制机制来调整缓存容量的大小，通过权衡冷启动率和缓存资源成本来实现较好的缓存效率。</p><p>&nbsp;</p><p>面向异构集群的低成本缓存策略。</p><p>&nbsp;</p><p>云服务供应商通常会提供多种具有不同CPU型号的服务器给租户使用，这些VM的处理能力和价格有所差异。IceBreaker [24]针对异构函数计算集群设计了一个成本优先的缓存策略，其核心思想是“可以使用价格更为便宜的低端服务器缓存冷启动开销较低的函数，从而节省总体成本”。为了实现该目的，IceBreaker基于傅里叶变化预测函数的负载到达率，并结合实例的使用率和节点加速前亲和性来决定将函数缓存在高端服务器，低端服务器或者不进行缓存，从而以损失部分函数性能的代价降低整体缓存消耗的资源成本。</p><p>&nbsp;</p><p></p><h3>优化请求分发提高缓存命中率</h3><p></p><p>&nbsp;</p><p>此外，还有一些工作通过优化或重新设计Serverless平台的请求分发策略来改善函数实例的命中率，从而改善缓存效率。在Serverless平台内部，前端组件收到请求后，通常以hash或负载均衡的策略分发到后端节点进行处理。将同一函数的请求分发到相对固定的节点可以有效地提高节点内实例的使用率（即缓存命中率），这也称为缓存的本地性原理。但是，由于函数负载中通常存在大量的热点函数，当热点函数聚集在后端节点时，则可能导致节点资源使负载压力升高，从而引发性能下降。</p><p>&nbsp;</p><p>因此，CH-RLU [25]工作提出了一个负载感知的请求调度策略，作者基于一致性Hash的分发规则在调度过程中考虑后端节点的压力，通过将高负载的函数分发到不同的节点来避免局部热点， 从而缓解资源竞争导致的函数性能下降问题。可以看到，优化负载分发的研究工作可以同现有的缓存方案结合使用，通过协同设计可以进一步改善Serverless平台的缓存效率。</p><p></p><h3>改善并发/实例共享或复用</h3><p></p><p>&nbsp;</p><p>多函数共享Sandbox。</p><p>&nbsp;</p><p>SAND [26]工作在函数工作流场景下提出了一种多函数共享实例Sandbox的方案，它允许同一个函数工作流内的不同函数共享同样的Sandbox环境，从而减少请求执行时创建的函数实例数量，这将有助于减少请求调用的冷启动发生率。然而，这在实际场景中也有很多局限性，例如租户的多个函数可能由不同的语言运行时编写，这些函数往往无法从实例共享中获益。此外，多个函数共享Sandbox会增加实例的体积，而工作流中的函数可能被多条路径所调用，在函数扩容时则会导致不必要的资源浪费。</p><p>&nbsp;</p><p>进/线程混合的函数编排。</p><p>&nbsp;</p><p>传统的函数实例以容器为单位，每个函数运行在一个单独的容器实例内部，容器间的数据传输需要借助网络或第三方存储实现，以AWS S3为例，通过S3进行函数间数据交换会导致较高时延。为了改善函数实例间的通信效率，Faastlane [27]工作提出了一个新的函数编排组织方式，即将工作流中的不同函数以进程或线程的形式在容器实例中执行，同时采用IPC进程间通信来提高函数交互效率。对于包含敏感数据操作的函数工作流，Faastlane使用英特尔内存保护密钥 (MPK) 提供轻量级线程级隔离域。在Apache OpenWhisk 上的评测结果表明，Faastlane可以将函数工作流的处理时间降低15倍，并将函数间的交互延迟减少99.95%。</p><p>&nbsp;</p><p>提高实例并发处理能力。</p><p>&nbsp;</p><p>在Serverless平台中，函数实例通常被配置较少的资源容量，例如内存以128MB为单位递增，而CPU的配额同内存大小成比例。当请求到达后，平台通常以“一对一”的请求-实例映射方式处理请求。低的实例并发处理能力可能导致Serverless平台在面临突发负载时创建出大量的实例，不仅降低函数的服务性能，也会增加系统调度压力。为了解决该问题，Fifer [12]工作提出了一种支持多并发处理的函数实例运行方案，通过为实例分配多个独立的CPU处理线程来提高实例的吞吐。同时，通过计算实例的排队延迟，并发处理数和未来请求到达率来提前预置对应数量的实例，从而减少冷启动调用的产生。</p><p>&nbsp;</p><p>允许租户间实例抢占/复用。</p><p>&nbsp;</p><p>通常Serverless平台中的租户函数运行在单独的实例中，不同函数间的实例有独立的资源配置和运行环境。由于不同租户的函数可能具有相同的运行时语言环境，而租户的函数实例也不一定在同一时间都处于忙碌状态，因此，Pagurus [28]工作另辟蹊径，设计了一个类似“寄居蟹”的租户间实例复用机制，其核心思想是“当A租户函数的冷启动发生时，可以迅速抢占B租户的空闲实例的运行时环境，从而减少冷启动的时延”，通过实例抢占和复用，从而避免冷启动发生时从零启动的长时延问题。同时，可以看到该做法同Pod池特化技术有异曲同工之妙。</p><p></p><h3>小结</h3><p></p><p>&nbsp;</p><p>可以看到，改善实例的并发能力或是通过实例共享/复用都是为了提高已有实例的利用效率，而缓存的方案则是尽可能利用少的资源来预置更多的实例，从而降低冷启动调用的产生。上述这些方法从不同的角度缓解Serverless函数冷启动的问题，可以结合使用来改善缓存效率。</p><p>&nbsp;</p><p>在主动缓存的方案中，由于突发流量或者负载的无规律性，基于负载预测来进行缓存还存在较大难度。在实际场景中，通过主动预测+优先级缓存调整来实现缓存方案将更有潜力。受限于各家函数计算平台内部的架构，函数共享实例或者抢占复用的实施成本较高，因此开发人员通常会选择更加易于实现的缓存方案例如预热池来改善服务质量。</p><p></p><h1>总结</h1><p></p><p></p><p>Serverless的无状态设计赋予了函数计算高度弹性化的扩展能力，然而也带来了难以避免的冷启动问题。消除Serverless函数的冷启动开销还是从降低函数冷启动率和加速实例启动过程两个角度综合入手。</p><p>&nbsp;</p><p>对于冷启动开销比较大的函数，在函数计算框架的设计机制中进行优化，尽量避免冷启动发生；当冷启动发生时，采用一系列启动加速技术来缩短整个过程进行补救。在Serverless平台的内部，冷启动的管理在实践中可以做进一步精细的划分，例如针对VIP大客户、针对有规律的负载，或是针对冷启动开销小的函数，通过分类做定制化、有目的的管理可以进一步改善系统效率。</p><p>&nbsp;</p><p>在未来，基于WASM+进程fork的技术可能会有助于彻底消除冷启动问题，但是考虑到实际可用性和安全性等问题，短期内的结果可能也是WASM和Container-based函数并存的折中方案，互相取长补短，因此在当前阶段研究冷启动问题还是有较大价值的。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考文献</p><p>&nbsp;</p><p>[1] Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mohammed Danish Shaikh, Shivaram Venkataraman, Aditya Akella. Archipelago: A Scalable Low-Latency Serverless Platform. arXiv:1911.09849, 2019.</p><p>[2] Fastfreeze. <a href=\"https://github.com/twosigma/fastfreeze.\">https://github.com/twosigma/fastfreeze.</a>\" 2023.</p><p>[3] 刘方明, 李林峰, 王磊. 华为Serverless核心技术与实践[M]. 北京: 电子工业出版社, 2021.11.</p><p>[4] Docker. <a href=\"https://www.docker.com/\">https://www.docker.com/.</a>\" 2023.</p><p>[5] Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau: SOCK: Rapid Task Provisioning with Serverless-Optimized Containers. USENIX Annual Technical Conference 2018: 57-70.</p><p>[6] Kata Containers. <a href=\"https://katacontainers.io/\">https://katacontainers.io/.</a>\" 2023.</p><p>[7] Alexandru Agache, Marc Brooker, Alexandra Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, Diana-Maria Popa: Firecracker: Lightweight Virtualization for Serverless Applications. NSDI 2020: 419-434.</p><p>[8] Open-sourcing gVisor, a sandboxed contaienr runtime. <a href=\"https://github.com/google/gvisor.\">https://github.com/google/gvisor.</a>\" 2023.</p><p>[9] Dong Du, Tianyi Yu, Yubin Xia, Binyu Zang, Guanglu Yan, Chenggang Qin, Qixuan Wu, Haibo Chen: Catalyzer: Sub-millisecond Startup for Serverless Computing with Initialization-less Booting. ASPLOS 2020: 467-481.</p><p>[10] Dmitrii Ustiugov, Plamen Petrov, Marios Kogias, Edouard Bugnion, Boris Grot: Benchmarking, analysis, and optimization of serverless function snapshots. ASPLOS 2021: 559-572.</p><p>[11] Xingda Wei, Fangming Lu, Tianxia Wang, Jinyu Gu, Yuhan Yang, Rong Chen, Haibo Chen. No Provisioned Concurrency: Fast RDMA-codesigned Remote Fork for Serverless Computing. OSDI 2023.</p><p>[12] Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan Chidambaram Nachiappan, Mahmut Taylan Kandemir, Chita R. Das. Fifer: Tackling Resource Underutilization in the Serverless Era. Middleware 2020: 280-295.</p><p>[13] Xuanzhe Liu, Jinfeng Wen, Zhenpeng Chen, Ding Li, Junkai Chen, Yi Liu, Haoyu Wang, Xin Jin. FaaSLight: General Application-Level Cold-Start Latency Optimization for Function-as-a-Service in Serverless Computing. arXiv:2207.08175. 2022.&nbsp;</p><p>[14] WebAssembly. <a href=\"https://webassembly.org./\">https://webassembly.org.</a>\" 2023.</p><p>[15] Simon Shillaker, Peter R. Pietzuch: Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing. USENIX Annual Technical Conference 2020: 419-433.</p><p>[16] Linux CRIU, stands for checkpoint and resotre in userspace. <a href=\"https://github.com/checkpoint-restore/criu.\">https://github.com/checkpoint-restore/criu.</a>\" 2023.</p><p>[17] Anil Madhavapeddy, Richard Mortier, Charalampos Rotsos, David J. Scott, Balraj Singh, Thomas Gazagnaire, Steven Smith, Steven Hand, Jon Crowcroft: Unikernels: library operating systems for the cloud. ASPLOS 2013: 461-472.</p><p>[18] Robert N. M. Watson, Jonathan Woodruff, Peter G. Neumann, Simon W. Moore, Jonathan Anderson, David Chisnall, Nirav H. Dave, Brooks Davis, Khilan Gudka, Ben Laurie, Steven J. Murdoch, Robert M. Norton, Michael Roe, Stacey D. Son, Munraj Vadera: CHERI: A Hybrid Capability-System Architecture for Scalable Software Compartmentalization. IEEE Symposium on Security and Privacy 2015: 20-37.&nbsp;</p><p>[19] AWS Lambda. <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/.</a>\" 2023.</p><p>[20] 华为云FunctionGraph. <a href=\"https://www.huaweicloud.com/product/functiongraph.html.\">https://www.huaweicloud.com/product/functiongraph.html.</a>\" 2023.</p><p>[21] Mohammad Shahrad, Rodrigo Fonseca, Iñigo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, Ricardo Bianchini: Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider. USENIX Annual Technical Conference 2020: 205-218.</p><p>[22] Yanan Yang, Laiping Zhao, Yiming Li, Huanyu Zhang, Jie Li, Mingyang Zhao, Xingzhen Chen, Keqiu Li: INFless: a native serverless system for low-latency, high-throughput inference. ASPLOS 2022: 768-781.</p><p>[23] Alexander Fuerst, Prateek Sharma: FaasCache: keeping serverless computing alive with greedy-dual caching. ASPLOS 2021: 386-400.</p><p>[24] Rohan Basu Roy, Tirthak Patel, Devesh Tiwari: IceBreaker: warming serverless functions better with heterogeneity. ASPLOS 2022: 753-767.</p><p>[25]Alexander Fuerst, Prateek Sharma: Locality-aware Load-Balancing For Serverless Clusters. HPDC 2022: 227-239.</p><p>[26] Istemi Ekin Akkus, Ruichuan Chen, Ivica Rimac, Manuel Stein, Klaus Satzke, Andre Beck, Paarijaat Aditya, Volker Hilt: SAND: Towards High-Performance Serverless Computing. USENIX Annual Technical Conference 2018: 923-935.</p><p>[27] Swaroop Kotni, Ajay Nayak, Vinod Ganapathy, Arkaprava Basu: Faastlane: Accelerating Function-as-a-Service Workflows. USENIX Annual Technical Conference 2021: 805-820.</p><p>[28] Zijun Li, Quan Chen, Minyi Guo: Pagurus: Eliminating Cold Startup in Serverless Computing with Inter-Action Container Sharing. USENIX Annual Technical Conference 2022.</p><p>[29] Ao Wang, Shuai Chang, Huangshi Tian, Hongqi Wang, Haoran Yang, Huiba Li, Rui Du, Yue Cheng: FaaSNet: Scalable and Fast Provisioning of Custom Serverless Container Runtimes at Alibaba Cloud Function Compute. USENIX Annual Technical Conference 2021: 443-457.</p><p>[30] Marc Brooker, Mike Danilov, Chris Greenwood, Phil Piwonka: On-demand Container Loading in AWS Lambda. CoRR abs/2305.13162 (2023).</p><p>[31] Zijun Li, Linsong Guo, Jiagan Cheng, Quan Chen, Bingsheng He, Minyi Guo: The Serverless Computing Survey: A Technical Primer for Design Architecture. ACM Comput. Surv. 54(10s): 220:1-220:34 (2022).</p><p>&nbsp;</p>",
    "publish_time": "2023-07-17 10:40:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "企业数字化转型技术发展趋势研究报告",
    "url": "https://www.infoq.cn/article/tjvf64LLOGW69mfut9Uq",
    "summary": "<p>为了更好地引导企业数字化建设，高效推进数字化升级，帮助用户更好地获取数字化产品技术支持和应用服务，本报告从企业数字化转型技术的发展趋势、技术应用保障体系、技术转型路径需求、构建转型技术发展体系做了系统性论述，提出未来技术发展的五大趋势，并对每一项技术进行了深入详细的探讨，提出了应用解决方案和实现数字化转型的路径建议，为我国产业数字化转型提供有力支撑。</p>\n<blockquote>\n<p>本报告版权属于中国信息通信研究院和北京元年科技股份有限公司，并受法律保护。转载、摘编或利用其它方式使用本报告文字或者观点，应注明“来源：中国信息通信研究院和北京元年科技股份有限公司”。违反上述声明者，编者将追究其相关法律责任。</p>\n</blockquote>",
    "publish_time": "2023-07-17 11:11:47",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "比 JDK 最高快 170 倍，蚂蚁集团开源高性能多语言序列化框架 Fury",
    "url": "https://www.infoq.cn/article/qYslJsfjkXXjNlQPWuA2",
    "summary": "<p>Fury 是一个基于 JIT 动态编译和零拷贝的多语言序列化框架，支持Java/Python/Golang/JavaScript/C++ 等语言，提供全自动的对象多语言 / 跨语言序列化能力，和相比 JDK 最高 170 倍的性能。</p><p></p><p>代码主仓库的 GitHub 地址为：<a href=\"https://github.com/alipay/fury\">https://github.com/alipay/fury</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/95/95d1a9d1f8225559fa1ae8da44b072ee.png\" /></p><p></p><p></p><h2>背景</h2><p></p><p></p><p>序列化是系统通信的基础组件，在大数据、AI 框架和云原生等分布式系统中广泛使用。当对象需要跨进程、跨语言、跨节点传输、持久化、状态读写、复制时，都需要进行序列化，其性能和易用性影响运行效率和开发效率。</p><p></p><p>静态序列化框架 protobuf/flatbuffer/thrift 由于不支持对象引用和多态、需要提前生成代码等原因，无法作为领域对象直接面向应用进行跨语言开发。而动态序列化框架 JDK 序列化 /Kryo/Fst/Hessian/Pickle 等，尽管提供了易用性和动态性，但不支持跨语言，且性能存在显著不足，并不能满足高吞吐、低延迟和大规模数据传输场景需求。</p><p></p><p>因此，我们开发了一个新的多语言序列化框架 Fury，并正式在 Github 开源。通过一套高度优化的序列化基础原语，结合 JIT 动态编译和 Zero-Copy 等技术，同时满足了性能、功能和易用性的需求，实现了任意对象自动跨语言序列化，并提供极致的性能。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9a/9ac77edab4d330b1dffee7b034360868.jpeg\" /></p><p></p><h2>Fury 简介</h2><p></p><p></p><p>Fury 是一个基于 JIT 动态编译和零拷贝的多语言序列化框架，提供极致的性能和易用性：</p><p></p><p>支持主流编程语言 Java/ Python/ C++/ Golang/ JavaScript，其它语言可轻易扩展；统一的多语言序列化核心能力：高度优化的序列化原语；Zero-Copy 序列化 支持，支持 Out of band 序列化协议，支持堆外内存读写；基于 JIT 动态编&nbsp;技术在运行时异步多线程自动生成序列化代码优化性能，增加方法内联、代码缓存和消除死代码，减少虚方法调用 / 条件分支 /Hash 查找 / 元数据写入 / 内存读写等，提供相比别的序列化框架最高 170 倍的性能；多协议支持：兼顾动态序列化的灵活性和易用性，以及静态序列化的跨语言能力。Java 序列化：无缝替代 JDK/Kryo/Hessian，无需修改任何代码，但提供最高 170x 的性能，可以大幅提升高性能场景 RPC 调用、数据传输和对象持久化效率；100% 兼容 JDK 序列化，原生支持 JDK 自定义序列化方法 writeObject/ readObject/ writeReplace/ readResolve/ readObjectNoData跨语言对象图序列化：多语言 / 跨语言自动序列化任意对象，无需创建 IDL 文件、手动编译 schema 生成代码以及将对象转换为中间格式；多语言 / 跨语言自动序列化共享引用和循环引用，不需要关心数据重复或者递归错误；支持对象类型多态，多个子类型对象可以同时被序列化；行存序列化：提供缓存友好的二进制随机访问行存格式，支持跳过序列化和部分序列化，适合高性能计算和大规模数据传输场景；支持和 Arrow 列存自动互转 ;</p><p></p><h2>序列化核心能力</h2><p></p><p></p><p>尽管不同的场景对序列化有需求，但序列化的底层操作都是类似的。因此 Fury 定义和实现了一套序列化的基础能力，基于这套能力能够快速构建不同的多语言序列化协议，并通过编译加速等优化具备高性能。同时针对一种协议在基础能力上的性能优化，也能够让所有的序列化协议都受益。</p><p></p><h3>序列化原语</h3><p></p><p></p><p>序列化涉及的常见操作主要包括：</p><p></p><p>bitmap 位操作整数编解码整数压缩字符串创建 * 拷贝优化字符串编码：ASCII/UTF8/UTF16内存拷贝优化数组拷贝压缩优化元数据编码 &amp; 压缩 &amp; 缓存</p><p></p><p>Fury 针对这些操作在每种语言内部都做了大量的优化，结合 SIMD 指令和语言高级特性，将性能推到极致，从而方便不同协议使用。</p><p></p><h3>零拷贝序列化</h3><p></p><p></p><p>在大规模数据传输场景，一个对象图内部往往有多个 binary buffer，而序列化框架在序列化过程当中会把这些数据写入一个中间 buffer，引入多次耗时内存拷贝。Fury 借鉴了 pickle5、ray 以及 arrow 的零拷贝设计，实现了一套 Out-Of-Band 序列化协议，能够把一个对象图当中的所有 binary buffer 直接抓取出来，避免掉这些 buffer 的中间拷贝，将序列化期间的内存拷贝开销降低到 0。</p><p></p><p>下图是 Fury 关闭引用支持时 Zero-Copy 的大致序列化过程。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9f/9fc343c6bb0bcf06c8e279e8463b6c1b.png\" /></p><p></p><p>目前 Fury 内置了以下类型的 Zero-Copy 支持：</p><p></p><p>Java：所有基本类型数组、ByteBuffer、ArrowRecordBatch、VectorSchemaRootPython：array 模块的所有 array、numpy 数组、pyarrow.Table、pyarrow.RecordBatchGolang：byte slice</p><p></p><p>用户也可以基于 Fury 的接口扩展新的零拷贝类型。</p><p></p><h3>JIT 动态编译加速</h3><p></p><p></p><p>对于要序列化的自定义类型对象，其中通常包含大量类型信息，Fury利用这些类型信息在运行时直接生成高效的序列化代码，将大量运行时的操作在动态编译阶段完成，从而增加方法内联和代码缓存，减少虚方法调用 / 条件分支 /Hash 查找 / 元数据写入 / 内存读写 等，最终大幅加速了序列化性能。</p><p></p><p>对于 Java 语言，Fury 实现了一套运行时代码生成框架，定义了一套序列化逻辑的算子表达式 IR，在运行时基于对象类型的泛型信息进行类型推断，然后构建一颗描述序列化代码逻辑的表达式树，根据表达式树生成高效的 Java 代码，再在运行时通过 Janino 编译成字节码，再加载到用户的 ClassLoader 里面或者 Fury 创建的 ClassLoader 里面，最终通过 Java JIT 编译成高效的汇编代码。</p><p></p><p>由于 JVM JIT 会跳过大方法编译和内联，Fury 也实现了一套优化器，将大方法递归拆分成小方法，这样就保证了 Fury 生成的所有代码都可以被编译和内联，压榨 JVM 的性能到极致。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/10/100abd1fa3ae527be867ff6a3848a567.png\" /></p><p></p><p>同时 Fury 也支持异步多线程动态编译，将不同序列化器的代码生成任务提交到线程池执行，在编译完成之前使用解释模式执行，从而保证不会出现序列化毛刺，不需要提前预热所有类型的序列化。</p><p></p><p>Python 和 JavaScript 场景也是采用的类似代码生成方式，这样的生成方式开发门槛低，更容易排查问题。</p><p></p><p>由于序列化需要密切操作每种编程语言的对象，而编程语言并没有暴露内存模型的低阶 API，通过 Native 方法调用存在较大开销，因此我们并不能通过 LLVM 构建一个统一的序列化器 JIT 框架，而是需要在每种语言内部结合语言特性实现特定的代码生成框架以及序列化器构建逻辑。</p><p></p><h3>静态代码生成</h3><p></p><p></p><p>尽管 JIT 编译能够大幅提升序列化效率，并且在运行时能够根据数据的统计分布重新生成更优的序列化代码，但 C++/Rust 等语言不支持反射，没有虚拟机，也没有提供内存模型的低阶 API，因此我们无法针对这类语言通过 JIT 动态编译生成序列化代码。</p><p></p><p>对于此类场景，Fury 正在实现一套 AOT 静态代码生成框架，在编译时根据对象的 schema 提前生成序列化代码，然后使用生成的代码进行自动序列化。对于 Rust，未来也会通过 Rust 的 macro 在编译时生成代码，提供更好的易用性。</p><p></p><h3>缓存优化</h3><p></p><p></p><p>在序列化自定义类型时，会把字段进行重排序，保证相同接口类型的字段依次序列化，增加缓存命中的概率，同时也促进了 CPU 指令缓存，实现了更加高效的序列化。对于基本类型字段将写入顺序按照字节字段大小降序排列，这样如果开始地址是对齐的，随后的读写都会发生在内存地址对齐的位置，CPU 执行起来更加高效。</p><p></p><h2>多协议设计与实现</h2><p></p><p></p><p>基于 Fury 提供的多语言序列化核心能力，我们在这之上构建了三种序列化协议，分别适用于不同的场景：</p><p></p><p>Java 序列化：适合纯 Java 序列化场景，提供最高百倍以上的性能提升；跨语言对象图序列化：适合面向应用的多语言编程，以及高性能跨语言序列化；行存序列化：适合分布式计算引擎如 Spark/Flink/Dories/Velox/ 样本流处理框架 / 特征存储等；</p><p></p><p>后续我们也会针对一些核心场景添加新的协议，用户也可以基于 Fury 的序列化能力构建自己的协议。</p><p></p><h3>Java 序列化</h3><p></p><p></p><p>由于 Java 在大数据、云原生、微服务和企业级应用的广泛使用，对 Java 序列化的性能优化可以大幅降低系统延迟，提升吞吐率，降低服务器成本。</p><p></p><p>因此 Fury 针对 Java 序列化进行了大量极致性能优化，我们的实现具备以下能力：</p><p></p><p>极致性能：通过利用 Java 对象的类型和泛型信息，结合 JIT 编译、Unsafe 低阶操作，Fury 相比 JDK 最高有 170 倍的性能提升，相比 Kryo/Hessian 最高有 50~100 倍的性能提升。100% JDK 序列化 API 兼容性：支持了所有 JDK 自定义序列化方法 writeObject/readObject/ writeReplace/ readResolve/readObjectNoData 的语义，保证任意场景替换 JDK 序列化的正确性。而已有的 Java 序列化框架如 Kryo/Hessian 在这些场景，都存在一定的正确性问题类型前后兼容：在反序列化端和序列化端 Class Schema 不一致时，仍然可以正确反序列化，支持应用独立升级部署，独立增删字段。并且我们对元数据进行了极致的压缩和共享，类型兼容模式相比类型强一致模式做到了几乎没有任何性能损失。元数据共享：在某个上下文 (TCP 连接) 下多次序列化之间共享元数据(类名称、字段名称、Final 字段类型信息等)，这些信息会在该上下文下第一次序列化时发送到对端，对端可以根据该类型信息重建相同的反序列化器，后续序列化可以避免传输元数据，减小网络流量压力，同时也自动支持类型前后兼容。零拷贝支持：支持 Out of band 零拷贝和堆外内存读写。</p><p></p><h3>跨语言对象图序列化</h3><p></p><p></p><p>跨语言对象图序列化主要用于对动态性和易用性有更高要求的场景。尽管 Protobuf/Flatbuffer 等框架提供了多语言序列化能力，但仍然存在一些不足：</p><p></p><p>需要提前编写 IDL 并静态编译生成代码，不具备足够的动态性和灵活性；生成的类不符合面向对象设计也无法给类添加行为，并不能作为领域对象 直接用于多语言应用开发。不支持子类序列化。面向对象编程的主要特点是通过接口调用子类方法。这类模式也无法得到很好的支持。尽管 Flatbuffer 提供了 Union，Protobuf 提供了 OneOf/Any 特性，这类特性需要在序列化和反序列化时判断对象的类型，不符合面向对象编程的设计。不支持循环和共享引用，需要针对领域对象重新定义一套 IDL 并自己实现引用解析，然后在每种语言里面编写代码实现领域对象和协议对象之间的相互转换，如果对象图嵌套层数较深，则需要编写更多的代码。</p><p></p><p>结合以上几点，Fury 实现了一套跨语言的对象图序列化协议：</p><p></p><p>多语言 / 跨语言 自动序列化任意对象：在序列化和反序列化端定义两个 Class，即可自动将一种语言的对象自动序列化为另一种语言的对象，无需创建 IDL 文件、编译 schema 生成代码以及手写转换代码；多语言 / 跨语言自动序列化共享引用和循环引用；支持对象类型多态，符合面向对象编程范式，多个子类型对象可以同时被自动反序列化，无需用户手动处理；同时我们在这套协议上面也支持了 Out of band 零拷贝；</p><p></p><p>自动跨语言序列化示例：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/ed/ed811261733f7241e811bcb45c7289c7.png\" /></p><p></p><h3>行存序列化</h3><p></p><p></p><p>对于高性能计算和大规模数据传输场景，数据序列化和传输往往是整个系统的性能瓶颈。如果用户只需要读取部分数据，或者根据对象某个字段进行过滤，反序列化整个数据将带来额外开销。因此 Fury 也提供了一套二进制数据结构，在二进制数据上直读直写，避开序列化。</p><p></p><p>Apache arrow 是一个成熟的列存格式，支持二进制读写。但列存并不能满足所有场景需求，在线链路和流式计算场景的数据天然就是行存结构，同时列式计算引擎内部在涉及到数据变更和 Hash/Join/Aggregation 操作时，也会使用到行存结构。</p><p></p><p>而行存并没有一个统一标准实现，计算引擎如 Spark/Flink/Doris/Velox 等都定义了一套行存格式，这些格式不支持跨语言，且只能被自己引擎内部使用，无法用于其它框架。尽管 Flatbuffer 能够支持按需反序列化，但需要静态编译 Schema IDL 和管理 offset，无法满足复杂场景的动态性和易用性需求。</p><p></p><p>因此 Fury 在早期借鉴了 spark tungsten 和 apache arrow 格式，实现了一套可以随机访问的二进制行存结构，目前实现了 Java/Python/C++ 版本，实现了在二进制数据上面直读直写，避免掉了所有序列化开销。</p><p></p><p>下图是 Fury Row Format 的二进制格式：</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/81/81aec151ae40ab60d441c1314f7424f2.png\" /></p><p></p><p>该格式密集存储，数据对齐，缓存友好，读写更快。由于避免了反序列化，能够减少 Java GC 压力。同时降低 Python 开销，同时由于 Python 的动态性，Fury 的数据结构实现了 _getattr__/getitem/slice/ 和其它特殊方法，保证了行为跟 python dataclass/list/object 的一致性，用户没有任何感知。</p><p></p><h2>性能对比</h2><p></p><p></p><p>这里给出部分 Java 序列化性能数据，其中标题包含 compatible 的图表是支持类型前后兼容下的性能数据，标题不包含 compatible 的图表是不支持类型前后兼容下的性能数据。为了公平起见，所有测试 Fury 关闭了零拷贝特性。</p><p></p><p>更多 benchmark 数据请参考 Fury Github 官方文档：<a href=\"https://github.com/alipay/fury/tree/main/docs/benchmarks\">https://github.com/alipay/fury/tree/main/docs/benchmarks</a>\"</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/9a/9ac77edab4d330b1dffee7b034360868.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/7c/7c39dcfb78c21a8382d8a12424363d71.jpeg\" /></p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a5/a57a4ed01c9d4f3f6651681565ad50c3.jpeg\" /></p><p></p><h2>未来规划</h2><p></p><p></p><p>元数据压缩和自动共享跨语言序列化支持类型前后兼容静态代码生成框架，用于提前生成 c++/golang/rust 代码C++/Rust 支持跨语言对象图序列化Golang/Rust/JavaScript 支持行存兼容 ProtoBuffer 生态，支持根据 Proto IDL 自动生成 Fury 序列化代码新的协议实现：AI 特征存储，知识图谱序列化持续改进我们的序列化基础原语，提供更高性能实现标准化协议，提供二进制兼容性文档和易用性改进</p><p></p><h2>加入我们</h2><p></p><p></p><p>我们致力于将 Fury 打造为一个开放中立、追求极致与创新的社区项目，后续的研发与讨论等工作都会在社区以开源透明的方式进行。欢迎任何形式的参与，包括但不限于提问、代码贡献、技术讨论等。非常期待收到大家的想法和反馈，一起参与到项目的建设中来，推动项目向前发展，打造最先进的序列化框架。</p><p></p><p>代码主仓库的 GitHub 地址为：<a href=\"https://github.com/alipay/fury\">https://github.com/alipay/fury</a>\"</p><p></p><p>官方网站：<a href=\"https://furyio.org/\">https://furyio.org</a>\"</p><p></p><p>欢迎各种 Issue、PR、Discussion。</p><p></p><p>也欢迎直接加入下面的官方交流群，和我们一起交流。</p><p></p><p>微信公众号：高性能序列化框架 Fury</p><p></p><p>钉钉交流群：钉钉群号 (36170003000)</p><p></p><h4>作者简介</h4><p></p><p></p><p>杨朝坤，蚂蚁集团技术专家，Fury 框架作者。2018 年加入蚂蚁集团，先后从事流计算框架、在线学习框架、科学计算框架和 Ray 等分布式计算框架开发，对批计算、流计算、Tensor 计算、高性能计算、AI 框架、张量编译等有深入的理解。</p><p></p><p></p><h4>活动推荐</h4><p></p><p></p><p>以「启航·AIGC 软件工程变革」为主题的 QCon 全球软件开发大会·北京站将于 9 月 3-5 日在北京•富力万丽酒店举办，此次大会策划了构建未来软件的编程语言、大模型应用落地、面向 AI 的存储、AIGC 浪潮下的研发效能提升、大前端融合提效、LLMOps、异构算力、微服务架构治理、业务安全技术、FinOps 等近 30 个精彩专题。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/db/db1963da1a756b9a6b0c80b0b5123868.jpeg\" /></p><p></p><p>现在购票即可享受 8 折优惠，立减 ¥1760。咨询购票可联系票务经理 18514549229（微信同手机号）。<a href=\"https://qcon.infoq.cn/202309/beijing?utm_source=infoq&amp;utm_medium=arti&amp;utm_campaign=8&amp;utm_term=0717&amp;utm_content=mubai\">点击查看专题详情</a>\"，期待与各位开发者现场交流。</p><p></p>",
    "publish_time": "2023-07-17 11:44:05",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "探访海底捞门店数字化：减员不是目的，创造价值才是",
    "url": "https://www.infoq.cn/article/s3VTceqbD8pAulSx5rUu",
    "summary": "<p>众所周知，服务是海底捞火锅门店的一大特色；但少有人知的是，海底捞的后厨同样有着许多令人眼前一亮的“隐藏技能”。</p><p></p><p>严格的净污分区，每个关键操作位都配有高清摄像头识别员工操作规范，如果出现任何问题可以快速进行食品安全追溯；自主研发的私人订制自动配锅机，通过结合大数据和云端记录，可以满足“千人千味”的锅底专享需求；配菜区每盒菜品盒子上配备RFID芯片，只要放在智能菜品台上，就能清晰显示菜品日期、重量、供应商、以及，是否符合安全规范等信息，如果是临期或过期食品将会有显眼的颜色标识进行提示......</p><p></p><p>在全国数千家海底捞门店中，诸如此类的数字化创新场景数不胜数。而这背后，是300多款应用系统的支撑——其中，超过100应用是<a href=\"https://xie.infoq.cn/article/b478fb34061ab7dcaebce0357\">海底捞</a>\"在短短1年内构建的，另外200多个跨部门的应用系统也已经实现了统一管理和打通。对于海底捞实现进一步精细化管理，这是具有里程碑意义的关键一步。</p><p></p><h2>系统割裂，开发需求多且频繁</h2><p></p><p></p><p>海底捞的数字化，最早要从2011年前后说起。当时，海底捞门店已经开始使用平板电脑点菜，并完成了点餐、收银、排号、订餐等系统的上线。经过10多年来的发展，海底捞业务规模持续扩大，门店数量增加，现场及系统管理复杂度都大大提升。</p><p></p><p>在这个过程中，海底捞上线了菜品创新提报、内部培训认证、内部招聘和投诉、安全打分、员工身份核验、差旅管理等上百个系统。但这些系统分散在各处，数据之间互不相通，交互难度大，这一方面使得管理者难以实时掌握全面的信息，同时，对于员工而言，日常工作中也要在不同系统间来回切换，工作效率较低。</p><p></p><p>以海底捞独创的“四色卡管理模式”为例，其中红卡代表服务、蓝卡代表卫生、黄卡代表菜品出品、绿卡代表食品安全。总部和门店经理每天会针对四个维度的各个细节，比如顾客满意度、投诉情况、卫生安全、设备等进行检查和评分，从而提升门店现场的管理水平。</p><p></p><p>此前，海底捞使用内部开发的小程序作为四色卡管理工具，这意味着，该工具的用户数量超过了10万，几乎每一位员工每一天都要打开和使用该工具。但与此同时，为了处理不同工作，他们还可能同时要打开多个平台或应用进行来回切换，这不仅体验差，效率也不高。</p><p></p><p>另一个挑战来自于技术部门对业务侧需求的响应效率。</p><p></p><p>海底捞信息科技部PMO颉鹏伟告诉InfoQ记者，自主开发的四色卡管理小程序最大的弊端就是不够灵活，“我们的技术团队面对的是全国1000多家门店，如果业务一线频繁地提出需求，或者即便只有10%的门店在同一时间提出了新的需求，我们都很难做到快速高效的响应。”</p><p></p><h2>在飞书上找到了问题“解法”</h2><p></p><p></p><p>据了解，为了解决系统割裂和<a href=\"https://www.infoq.cn/article/BkVptJA3JSK5cf14RN3e\">数据烟囱</a>\"的问题，海底捞在2022年上线了飞书，并基于集成平台把大量多而散的应用系统都“搬”到了飞书上，使得内部10多万员工都可以在线同频。</p><p></p><p>“也就是说，我们把各种典型的业务场景模型化后，通过工具的手段都落到了系统上。对于员工来说，他们在手机端工作台入口就可以触达各种应用，从而完成日常的各项工作。”颉鹏伟补充说，“并且，在手机界面上还会把审批、员工身份核验等常用应用呈现出来，方便大家使用。”据他介绍，所有相关应用的排序都是经过细致调研呈现的，而这也是海底捞使用飞书进行精细化管理的一个体现。</p><p></p><p>除此之外，对于类似于“审批”这样的高频使用且对时效性要求比较高的应用，海底捞还进行了模块定制化，将“待审批”、“待处理”功能单独拉出来显示在前端页面，让员工可以快速触达。</p><p></p><p>作为一个面向内部使用的核心工具，海底捞同样把“千人千面”的理念融入在了<a href=\"https://xie.infoq.cn/article/ddc70d9de98ef0af47b0034cc\">飞书</a>\"上。举个例子，对于海底捞员工来说，飞书工作台的使用是高度私人定制的——不同部门、不同员工看到的界面是差异化的。比如，管理者需要处理审批工作，一线员工则不需要，他们关心的可能是计件工资，因此，海底捞就会把审批模块在一线员工账号上去掉，而在更显眼和便捷的位置呈现计件工资模块。</p><p></p><p>另一方面，针对业务需求快速响应的挑战，海底捞同样在飞书上找到了“解法”。</p><p></p><p>今年初，飞书发布了以“多维表格”、“飞书应用引擎”、“飞书集成平台”组成的“业务三件套”。其中，“飞书集成平台”帮助海底捞搞定了上述的系统集成问题，而另外两项功能，则使得海底捞的技术人员的效率大大提升，甚至是解放出来。</p><p></p><p>“多维表格”是一个一线业务员工也能自主进行应用搭建的功能，基于业务模版能够实现1分钟快速搭建，1小时内进行应用调试。而“飞书应用引擎”面向的则是业务相对更复杂、数据量更大的专业系统应用，基于<a href=\"https://www.infoq.cn/article/UdGTsgRmdWEUxMxV3Aii\">低代码</a>\"能力，能帮助技术人员大幅降低开发成本，快速搭建业务系统。</p><p></p><p>据颉鹏伟介绍，在上线飞书后短短1年的时间里，海底捞已经通过“多维表格”和“飞书应用引擎”构建了100多个系统应用。</p><p></p><h4>对「人」的数字化管理</h4><p></p><p></p><p>海底捞的业务重心聚焦一线门店。在经营管理过程中，店经理就是离“战场”最近，且拥有极大影响力的岗位角色。对店经理的管理，将直接决定海底捞门店经营甚至是业务经营效益。</p><p></p><p>因此，在基于飞书开发的上百个系统应用中，与店经理管理相关的应用不在少数。</p><p></p><p>以“店经理任职资格分管理”应用为例，这是海底捞借助“多维表格”把自身的管理经验和方法落在系统上的成果。基于该应用，海底捞总部实现了对店经理进行量化考核，在应用上进行考核记录不但可以线上留痕、有据可依，同时还能够实时反馈给当事人。如果考核结果低于标准线，店经理就要从岗位下来重新进行学习和提升。</p><p></p><p>与此同时，海底捞还通过“多维表格”实现了店经理工作流程管理线上化和规范化。在海底捞，有一套完整的制度和流程规定店经理的日、周、月，以及季度和年度的工作流程。过去，所有流程的执行情况散落在各个门店和不同区域。从公司角度而言，很难全面地掌握和确认流程的被执行程度和效果。</p><p></p><p>“事实上，有的流程没有被很好执行并不一定是因为大家不愿意做，而是某个流程不合理，这时候，我们是需要相关数据做支撑的。”颉鹏伟指出，出于这一原因，海底捞把店经理的相关工作流程都基于“多维表格”转到了线上，并且可以以自动提醒的方式提前把工作流程推送给店经理，为其当日的工作开展和复盘、总结提供“备忘”。</p><p></p><p>对于店经理而言，这些操作都是在统一入口、一键操作、瞬间完成。而当所有的数据汇总在一起，呈现给总部管理者的就是多维度的大盘数据，可以用来判断门店管理情况，店经理工作状态和趋势，同时评估流程的合理性。</p><p></p><h4>对「物」的数字化管理</h4><p></p><p></p><p>如果说对人的管理关乎门店经营效率，那么对物的管理，对菜品质量、食品安全的把控，则是餐饮门店经营的“生死线”。</p><p></p><p>在海底捞，对菜品质量的把控分两个方面：一是在前置环节进行流程标准、出品的把控；二是在问题暴露时，快速应对和解决。据了解，海底捞门店的库管，每天都要对食材进行验货，一旦发现产品缺陷，就要联系供应链退回，并加紧运送新货，保障门店正常运转。</p><p></p><p>但食材在送到消费者口中之前，要从供货源经过运输到达门店收货，期间还要进入冷藏库保存，有的食材还要进行餐前解冻。如果食材出现缺陷，那么原因有可能发生在任何一个环节，问题往往以散点的方式暴露出来，追溯的过程也非常漫长。</p><p></p><p>过去，库管会第一时间在工作沟通群中跟采购反馈问题，但海底捞全国有上千家门店，几十家门店会被划分到一个聊天沟通群中，同时要求库管在工作沟通群之外，还需要在海底捞自己的系统上再提交，然后由门店店长将隔天的数据统一提交给总部。总部将数据导出之后，会按照职责划分，再将各门店各产品具体缺陷信息提交给负责的采购，由采购再将该事件处理的结果录入到系统中。整个流程要耗时3-4天，而在这个过程中如果没能及时追溯到原因，相同问题还会持续出现。</p><p></p><p>为了解决这个问题，海底捞使用飞书应用引擎，搭建了一套“菜品缺陷提报”系统。通过这套系统，海底捞门店最快&nbsp;2&nbsp;小时就可以闭环处理单个菜品问题提，报流程节省了&nbsp;40%的人力，提效&nbsp;25%以上。</p><p></p><p>“菜品缺陷问题的解决和优化它并不是线性的流程，在过程中它可能经历问题退回、纠纷、重复排查等等，所以，用一个报表或简单工具是很难解决这种复杂多变问题的，它需要一个能够支持灵活流程的工具。”颉鹏伟表示。</p><p></p><h2>减员不是目的，创造价值才是</h2><p></p><p></p><p>用魔法打败魔法，用技术降低技术的门槛，对此，海底捞的技术研发团队和一线员工都在从中受益。但是，新的问题也可能随之出现——比如，当没有技术背景的人员也可以自己搭建系统应用，那么应用的稳定性、可靠性如何保障？</p><p></p><p>颉鹏伟向InfoQ解释，多维表格类技术的使用是为了让大家可以根据业务场景放心大胆地做创新，但是，在海底捞内部，所有应用的创建也都要经过严格的流程申请和检验，而不是放任自流。比如，创建的应用会先做试点，这期间如果不适用就会把它阻断在摇篮里，有任何问题都可以调整、优化，然后再进行推广；再比如，在应用正式上线后，也并不是一下子推向所有人，而是针对性的，面向有需要的角色推送。</p><p></p><p>“目前来看，我们在这一年里基于飞书应用的搭建工具的使用体验和稳定性还是比较好的。前提是我们在应用推广的收口上，有相应的规则在做把控，并不是10万多员工都可以创造应用，也不是把创建的应用都像大水漫灌似地推给每一位员工。”</p><p></p><p>与此同时，如果从硬币的另一面再来看效率提升这件事，颉鹏伟还进一步强调，“海底捞的提效并不以减员为目的，而是为了让有限的资源放到更有价值的方面。假设原本20个人可能产生&nbsp;500&nbsp;万的价值，现在20&nbsp;个人可能就可以产生1000&nbsp;万的价值。”</p><p></p><p>比如，用技术赋能技术团队，他们就得以聚焦更复杂的业务系统开发中，不断提升用户体验；而用技术赋能业务，则使得门店更多人力能被释放到前台现场的顾客服务中，不断提升客户体验。如此种种，恰恰就是海底捞做数字化的理念核心。</p>",
    "publish_time": "2023-07-17 12:32:45",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "权威报告：蚂蚁集团 TuGraph 跻身中国图数据库市场“领导者”象限",
    "url": "https://www.infoq.cn/article/ltpQXR5bhJTLuJuWC8G9",
    "summary": "<p>7 月 15 日，全球领先的IT市场研究和咨询公司 IDC 发布了最新的市场研究报告《 IDC&nbsp;MarketScape：中国图数据库市场厂商评估，2023 》。报告显示，<a href=\"https://www.infoq.cn/article/9LtMbS3xyfvteo6yuU6X\">蚂蚁集团</a>\"自研的企业级图数据管理平台 TuGraph 跻身\"领导者\"象限。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/23/236bdb9488381df7766693fb0c4048a2.png\" /></p><p>IDC 发布的“中国图数据库市场，2023 ”象限</p><p></p><p>IDC 在报告中列举了蚂蚁集团 TuGraph 的五大优势。第一，TuGraph 研发七年多来支持了蚂蚁集团 300 多种应用，在大量不同场景中长时间使用，产品成熟度、丰富度等方面具有优势。第二，具备业界鲜有的在线、流式、离线\"三线一致\"计算能力，覆盖了毫秒级延迟、分钟级和小时级等不同引擎，满足不同场景的性能需求。第三，支持每秒千万级查询的超高吞吐，毫秒级超低延迟。第四，生态工具“一站式图研发及可视化平台&nbsp;TuGraph-Platform ”降低了用户使用门槛。第五，在其他非金融场景，如社交推荐、数据血缘管理、故障归因分析等场景广泛使用，应用体系成熟。</p><p></p><p>本次报告 IDC 主要评估了中国市场上 12 家典型的企业级<a href=\"https://xie.infoq.cn/article/00df81682b46bb1cd4331169b\">图数据库</a>\"厂商，类型覆盖互联网厂商、云服务厂商、大数据厂商等。IDC 针对入选图数据库厂商的产品能力（Capabilities）和技术战略（Strategies）两个维度，考察了目前的产品技术能力、市场和生态以及未来发展战略等，评估了 16 个细分能力指标和 4 个细分战略指标，包含 72 个指标评估项，并配以不同权重。该报告为企业发展图计算和投资选型提供了有力支持。</p><p></p><p>图数据库，即以图（Graph）数据结构来进行存储和分析的数据库。与传统数据库相比，图数据库擅长关系分析，能更好地管理和组织数据，开发上层智能模型，同时也能实现海量数据的高并发、低延迟分析处理，提高数据变现的商业价值。当前图数据库应用主要集中在欺诈检测、人际关系分析和预测分析等领域。</p><p></p><p>IDC 调研发现，数字经济、产业数字化转型进入深化发展阶段，企业对于业务逻辑开发和关系挖掘需求增强，图数据库市场受大型央国企数字化转型政策的推动明显。95% 的企业认为图数据库是重要的数据管理工具，超过&nbsp;65% 的厂商认为在业务上图数据库优于其他选择。整体来看，图数据库的使用仍处于早期阶段，仍然缺乏统一标准范式，技术生态环境弱势，低操作门槛的图计算平台仍有较大缺口。</p><p></p><p>据了解，蚂蚁集团从 2015 年开始布局图技术，与<a href=\"https://www.infoq.cn/article/xNYUNddIBuiIdtRi8smB\">清华大学</a>\"合作研发了高性能图数据库 TuGraph ，扛住了蚂蚁万亿级业务的高性能要求。TuGraph 在功能完整性、吞吐率、响应时间等技术指标上处于全球领先水平，两次打破国际图数据库基准性能测试世界纪录，成为 LDBC-SNB 世界纪录保持者。TuGraph 在 2021 年帮助支付宝实现了资产损失率小于亿分之 0.98 的目标。</p><p></p><p>目前，蚂蚁集团已经开源了 TuGraph 系统中的单机版图数据 TuGraph-DB 和流式图计算引擎 TuGraph-Analytics 。TuGraph-DB 提供了完备的图数据库基础功能和成熟的产品设计，具备完整的事务支持和丰富的系统特性，可在单机上部署，使用成本低，支持 TB 级别的数据规模。而 TuGraph-Analytics 是业界首个工业级流式图计算引擎，能够在超大规模图上进行流式复杂计算。这两项开源，有望为业界带来先进的图计算技术能力，帮助建设中国图计算技术生态。</p>",
    "publish_time": "2023-07-17 14:52:15",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "GDPR巨额罚款震撼全球，聊聊隐私安全建设思路与实践",
    "url": "https://www.infoq.cn/article/Qa17vZ0ej8HFJXZPQt8W",
    "summary": "<p>本文整理自字 OPPO 安全专家沈海涛在<a href=\"https://qcon.infoq.cn/2022/guangzhou/\">QCon 2022 广州站</a>\"的演讲分享，主题为“<a href=\"https://qcon.infoq.cn/2022/guangzhou/presentation/4766\">OPPO 互联网业务隐私安全建设实践</a>\"”。</p><p>&nbsp;</p><p>&nbsp;</p><p>分享主要从四个部分展开：首先是风险概述，其次是应对风险的整体思路，然后是在一些业务中的具体实践，最后是总结与展望。</p><p>&nbsp;</p><p></p><h2>隐私安全面临的风险</h2><p></p><p>&nbsp;</p><p>从法律上讲，2018 年 GDPR 的出台是隐私安全的一个里程碑。从 2018 年开始，各企业和国家更加重视隐私相关的保护，我本人也是从 2018 年开始真正做隐私安全，我之前在做传统安全，比如黑客的攻防对抗，排查业务是不是存在安全风险等。</p><p>&nbsp;</p><p>由于 2018 年 GDPR 的出台，以及 OPPO 开始扩展一些海外业务，迫使我们更多的投入到隐私安全的工作中来，它已经成为与传统安全并列齐驱的一块工作内容，欧盟 GDPR 出台之后，全球各地都开始推出各种各样的法律法规。&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/df/df81b54d5fdf10425672a5af05ba5c2c.png\" /></p><p></p><p>接下来大家看一组数据，看题目大家已经能够大概猜到，这是一个监管的处罚情况。我们统计了截止到今年 7 月份欧盟 GDPR 执法的一些情况，1087 是处罚的案件总数，十六亿三千万是整体的处罚金额。七亿四千六百万是 GDPR 有史以来单个案件处罚最多的一个案例，涉及的是某个全球性的跨境电商。我们再看另一组数据，大家看到最后这个人民币的符号，已经猜到是我们国内的情况，国内这三个数字截止到 2022 年的 6 月份，从工信部发布的数据来看，我国对上架的 322 万款 APP 进行了检测，对其中 3000 多款应用进行了通报，甚至下架的处罚。那么最后这个 80 亿，大家应该很清楚，是最近我们国内某公司因为隐私安全的问题，受到的处罚金额。</p><p>&nbsp;</p><p>从这个数字来看，某种程度上国内的处罚力度比欧盟的 GDPR 还要大一些，国内的上限是全球营业额的 5%，GDPR 是全球营业额的 4%。</p><p>&nbsp;</p><p>分享完监管和国家法律法规的一些要求，再从消费者的角度聊一下这个事情。随着移动互联网的蓬勃发展，各种各样的 APP 在方便大家的同时也收集了大量个人信息，那么消费者怎么看待这件事情呢？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/21/218c104b87d5c37f18b628c8bde25beb.png\" /></p><p></p><p>这是 Veritas 的一份全球消费者数据隐私洞察报告，这里面有几个数字，全球 62% 的人、中国 55.7% 的人认为如果一家企业不能妥善保护用户的个人隐私数据，那他们将停止购买这家公司的产品。全球 48% 的人、国内 63.8% 的人甚至会转而购买竞争对手的产品。比较好的情况是有 59% 的全球消费者，愿意加大消费购买对个人隐私数据保护更加完善的企业类产品，甚至有 27% 的人愿意增加 25% 的消费投入。</p><p>&nbsp;</p><p>所以说无论是从监管的角度，还是从消费者的角度，隐私安全都是公司产品的一项核心竞争力，如果隐私安全做不好，可能导致销售也做不好，如果隐私安全做的好，这可能会成为整个产品的一个卖点。</p><p>&nbsp;</p><p></p><h2>应对风险的思路总览</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cd1c04eaa939427d36bf6a72b4287de1.png\" /></p><p></p><p>接下来讲一下我们整体应对风险的思路大概是什么样的。我们一直在说隐私安全，但是隐私安全到底是什么？现在也没有看到一个特别完善的定义，我尝试从我的理解定义一下到底什么是隐私安全。</p><p>&nbsp;</p><p>首先什么是隐私？有一个比较简单的定义，即不愿意公开的信息叫做隐私，那有了隐私之后，作为人就有隐私权，隐私权是什么？这是我从维基百科上查到的比较专业的定义，即个人或者群体隔离自己，或与自己有关信息的权利。</p><p>&nbsp;</p><p>隐私安全就是负责任地使用用户的数据，并且根据用户的要求，避免数据落入坏人之手，这样的一个动作。我们通过制度、技术、产品等各种方式达成保护用户隐私安全的目的，叫做隐私保护。隐私安全面临很多风险，刚才我们提到了监管，监管是底线的要求，如果做不到，那企业可能将承受巨大的处罚压力。</p><p>&nbsp;</p><p>接下来还有黑客攻击。黑客也会觊觎用户的一些数据，因为这些数据能带来很大的价值。大家有没有遇到过这样的现象，当你在网上买了一个东西后，突然有人打电话给你说要退货，还要你再寄点钱给他，然后会返还你一些好处，这就是一种诈骗。那么黑客是怎么得到这些数据的呢？有很大的概率是通过攻击拿到了一些订单数据和消费者的信息，如果说某家公司对用户的隐私完全不注意，那消费者对它的产品肯定是没有信心的，对这家公司的价值也会存在置疑。</p><p>&nbsp;</p><p>隐私安全有很多的建设阶段，这边大概说一下，后面我们会详细展开。比如最基础的是满足法律法规的要求和技术的合规。接下来会去做比如认证和相关体系的建设，自证合规向消费者传达做安全隐私的意愿和能力，最后是能够让用户感知到你有很多产品或功能来保护用户的隐私，比如说 OPPO 的密码本，你可以放心的将你各个网站的密码保存到里面，并且这个密码只能由你自己解开，OPPO 的云端是没办法解开的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d28795576f404a29530c8bd41c51116.png\" /></p><p></p><p>我们再谈一下怎么做，整体的思路不复杂。比如最早可能有很多外部标准的要求，包括法律、行业、实践的，需要把这些转化成我们内部的准则。我们的研发人员或者产品经理很难对法律条款非常熟悉，所以需要我们安全人员把它转换成产品经理和研发人员能够理解的东西，这就形成了内部的准则、要求及规范。那么这些东西怎么落地呢？光知道是不行的，如何通过流程、工具、审计来确保我们相关的标准要求落实到产品里面，是我们接下来要做的事情。</p><p>&nbsp;</p><p>因为隐私相关的法律法规更多的是规范一些比较粗粒度的要求，可能导致每个人对同一个法律条款，或者同一个要求的理解是不一样的，这个时候我们应该尽量修正我们的理解，和监管部门的理解保持一致，需要通过一些外部验证去检验我们的这个事情是不是真的做对了。比如可以主动跟一些实验室合作，国外会比较多，比如像欧盟，可能会有很多独立的安全实验室，会给你做一些相关的认证，去切合当地一些法律法规的要求，并提出一些针对性的意见。同时你也可以主动去做一些送检。另外就是事件了，无论是外部爆出来的事件，还是我们自己被监管通告的事件，都可以作为我们验证自己的标准是否已经符合监管要求的标尺，它是一个循环的东西。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/37/37574c704b9146e62716dce783e8ca5e.png\" /></p><p></p><p>讲了宏观上的一些事情，再讲一下具体的细分，我们从 2018 年到现在大概是怎么做的，首先我们要明确一个原则，就是我们到底应该怎么处理用户的数据，怎么保护用户的隐私，这个原则是要我们安全跟产品都达成一致的，大家要形成共识的。我们这个原则是在做很多内部标准落地的时候，能提供指引性的。</p><p>&nbsp;</p><p>接下来就是把我们的原则形成内部的一些技术规范，这个时候它可能就更细化了，比如说数据的分级分类是什么样的？安全的开发规范是什么样的？加解密的标准是什么样的？数据的整个生命周期是怎么做的？数据脱敏应该怎么去做？这些就会相对细化，同时这个地方离不开基础防御系统的建设，因为刚才我们也说到了，隐私面临着三部分问题，一个是监管，一个是黑客攻击，还有一个是消费者。当黑客攻击我们时，需要有基础的通用的安全防护能力，比如说防火墙、WAF、主机入侵防御检测系统等。</p><p>&nbsp;</p><p>有了标准和技术之后，我们可能已经解决了 10% 到 20% 的问题，把它真正落实到我们的产品里面去，才是最重要的一环。比如，我们会在数据的生命保护周期内的每一环都落地相应的检测标准，然后同时把我们的要求，我们的工具去落实到整个研发的生命周期里面来，在需求、设计、开发、测试，发布等阶段都有相应的安全要求。</p><p>&nbsp;</p><p>在一个企业里面，技术可能能够解决 30% 的问题，制度可能能够解决 70% 的问题，我指的制度就是组织要去支撑你做这件事，如果大家都不认可，没有人投入，那这个事情就很难落地，我们是建立了从上到下的三道防线组织，融合了安全部门和业务部门，最终我们希望达到的目的是整个公司都形成一种理念，即发自内心的尊重用户的隐私安全，而不仅仅是因为法律的约束，当然这是我们正在进行中的事情。</p><p>&nbsp;</p><p></p><h2>OPPO 隐私安全建设实践</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da87cb7b7426a576e32c818234cf50e4.png\" /></p><p></p><p>接下来步入正题，讲一下我们到底是怎么做的。我先抛出一个问题：你认为建设隐私安全最重要的事情是什么？或者决定我们关键成败的事情是什么？从技术角度上讲，答案是标准的引入，为什么呢？如果标准没有解释的很清晰，研发人员就不知道该怎么落地，如果你的标准跟国家的要求产生了非常大的偏差，我们做完的产品也会有很大的偏差。所以说标准的引入是非常重要的一环。</p><p>&nbsp;</p><p>那我们大概怎么去做呢？整体思路是首先投入人力把外部标准转化成内部语言，也就是内部的产品经理和研发人员能够理解的内部标准，标准细化之后还要有一个检测的标准，就是我们要去验证我们的产品到底是不是满足了这个标准。外部标准有这么几个来源，第一个是法律法规，法律法规规定的比较粗线条；第二个是行业标准，这个就非常重要了，很多技术机构会发布很多行标，行标里面会有非常细粒度的一些技术规范，所以说这对转化成内部的可执行的标准是非常重要的一点。</p><p>&nbsp;</p><p>接下来就会有一些合作方的要求，还有一些业界的实践、通报案例等。内部的标准我们已经讲了很多了，包括数据方面的和权限方面的，还有研发人员可以自己去评估的一些准则，最后是一些检验标准，可能会包含比如设计规范、开发规范以及最后测试的检测项目，这就是 OPPO 现在主要参考的一些外部标准的来源，比如国内会有个人信息保护法，欧盟会有 GDPR，各个不同的业务开展的国家也会有具体的相应的法律法规。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ac/aca9b93e875ef926e3e5582384ff2a9c.png\" /></p><p></p><p>行业标准是要花很大精力去解读的，它也是从法律到技术转化的中间态，这些大家也可以去关注。另外还会有很多合作方的要求，比如我们可能是基于谷歌系统、谷歌生态的，会有谷歌的一些 MBA 的要求，我们也会有 iOS 的 APP 上架，也要遵循 Apple 的一些要求，甚至跟运营商的合作也会有相应的要求。最终通报案例是大家学习中非常重要的一环，我们这边大概列出来国内处罚的套路，以及欧盟 GDPR 处罚的套路，这些就是监管重点关注的，我们需要重点去解读和转化的。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/adc32e64d4fa9e808c1bb18c2673345c.png\" /></p><p></p><p>简单给大家举一个小例子，讲一下我们从外部转到内部到底要转化成什么样子，才是我们觉得做到位的标准。比如我拿到了一个工信部 164 号文件一条要求，它的要求标题是 APP 强制频繁过度索取权限，这个就很宽泛，它会有一些简单的描述，比如重点整治软件安装运行使用的过程中，在用户没有明确拒绝之后频繁弹窗等行为的要求。但这样直接给开发人员看，他还是不理解，因为没有站在他的场景里去做更多细节的描述，这时候就需要转化成研发内部能理解的语言，也就是场景化。比如我们会在启动阶段打开注册登录运行，然后用户明确拒绝之后，每一点都要做到什么？怎么去做？要描述的非常非常详细。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c6/c6b69262cda96b904120aa4a1d507788.png\" /></p><p></p><p>还有就是我们说到的检测标准，像刚才那样转化成内部开发人员能够理解的东西之后，就真的能够落地了吗？或者说研发人员能理解吗？还是说只有产品经理能理解？可能更多的是产品经理能够理解，但研发人员还不知道他自己要做什么。那我们就需要再做一个内部标准，更加细化的转化成一个研发人员能理解的语言，我这边拿一条我们内部的标准条款举例，即 APP 向用户明示收集个人信息的频次，超出其实现产品或者服务的业务功能所必须的最低频次，这个开发能理解吗？什么叫个人信息？最低频次怎么定义？怎么去计数频次？这些可能都是我们需要去细化的东西，比如个人信息，我们就要非常明确的表达，如 IMEI、设备的 MAC 地址、用户的定位信息等，这样描述开发人员就理解了。</p><p>&nbsp;</p><p>接下来怎么去定义频繁？比如是用户非主动触发的，软件自动触发了，这就叫频繁，周期性读取的情况下，导航超过每秒一次，这个叫频繁，非常清晰地定义给开发看就行了。还有一个问题就是如何计数，比如说超过一次，这个一次怎么算？尤其是对于地理位置来说，有经度和纬度，经度获取一次算一次，还是说经纬度同时获取一次算一次？这些非常细化的场景都是需要我们投入人力去解读的，解读完之后还要转化成研发人员能理解的语言。</p><p>&nbsp;</p><p>比如获取定位信息研发，某个研发人员可能只知道某一个 API，另外一个研发人员可能用其它的 API，那我们如何保证整个的全面性？这个是需要更多的技术专家参与进来的。大家以为这样就完了吗？这里面还会有很多坑，我这边随便讲一个，就是系统的差异。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5b64fac68240edca5a14e5078ae24bc1.png\" /></p><p></p><p>我们在上图里已经给出获取 Wi-Fi 的 API 了。我们这段是获取 Wi-Fi 的代码，我们突然在某一天发现了一件事情，这个行为被某些系统认定为获取地理位置信息，我们完全不知道为什么，检测报告出来之后我们也不清楚，上面只是说超范围获取了用户的信息，具体什么信息我们不知道，我们认为是获取了 Wi-Fi，但 Wi-Fi 不是敏感信息，但最终交流之后才发现这是一个地理位置的信息。</p><p>&nbsp;</p><p>这就需要我们有非常强大的技术团队去做分析了，它是一个系统的特性，从安卓 10 开始 APP 必须要有获取位置的权限才能使用 Wi-Fi Manager 里面的一些的类，才能获取 Wi-Fi 名称， Google 自己也意识到这不是一个非常好的解决办法，所以在安卓 13 的时候做了改进，单独把权限做了拆分。所以说标准解读可能没有我们想象的那么容易，从外部标准转换到内部标准，从内部标准再转换到研发标准，这甚至不是一个团队能够解决的，需要法务同事、数据安全的同事和安全攻防专家一起紧密配合，才能把整体的标准分析的比较清楚。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e620f62fff628870d19ab2101b2b3355.png\" /></p><p></p><p>有了标准之后，我们要把这些标准落实到整个研发流程里面，那么从研发最开头的阶段说起，就是需求，如果在需求阶段没有让开发人员很好的注意到标准的变化以及标准要求，开发完成之后再返工的成本是非常大的，所以我们在需求阶段就很早的去介入了安全隐私的一些事情。</p><p>&nbsp;</p><p>这个是我们做的一个安全隐私合规的评审的系统，是人工加半自动化的，它基于 STRIDEP 做场景化的位阶建模，然后能够实现半自动化的安全隐私需求评审，当然这是最终的结果，是我们现在运用的结果，它并不是一开始就是这样一个系统。</p><p>&nbsp;</p><p>我们为什么会形成这样一个系统呢？有两方面的考虑，最早我们的隐私评审阶段都是人工介入的，人工介入有好处也有坏处，首先好处是可以把场景分析的非常清楚，坏处是会助长业务的依赖性，业务会认为这个需求评审是安全团队的事情，业务只要提出需求，具体的解决方案安全团队可以提供。另外一个是效率问题。基于这两点考虑，我们慢慢演化出这样一个系统，针对业务我们会有一个场景库，对于一些简单的场景，我们会通过专家把这个场景面临的安全的风险都列出来，如果业务方命中了这个场景，只需要按照我们的相关要求去做相应的动作就可以了。这样做的好处是业务方会有更强的参与感，会知道这个要求是什么样，下次自己去设计的时候自然就带上了。另外就是效率的提升。我们已经覆盖了大量的场景，业务方完全可以自助式的搞定这个事情。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ea/ea88d24ef8f0ab1891f5ce238eab5a9f.png\" /></p><p></p><p>在最早期的建设阶段，我们认为检测也是非常重要的，因为测试有很多工作都是依赖于人的，系统可能也是半自动化的，不能够保证百分之百，那怎样去检测业务方真实的做了相应的需求评审和开发呢，这就需要通过检测去做一些验证。刚才举了一个例子，比如频繁调用以及个人数据，那什么样的检测系统能检测出这样的问题呢？首先看你申请了什么权限，用了什么数据，通过代码扫描很简单就能看出你有没有用这个 API，所以我们首先建立了一个静态扫描系统，集合了隐私合规、安全漏洞扫描、安全编码规范以及 SDK 检测这几个功能。</p><p>&nbsp;</p><p>回到频繁调用的问题，我们知道代码里有能力这样去调用，但是代码不能输出频次，我们又针对这个问题做了一套动态扫描系统，它运行在一个沙箱里，每一次调用了什么东西，大概什么时间调用的，一段时间内的调用频次都可以统计出来。我们这个系统集合了动态行为检测、个人信息分析，甚至云真机的功能，业务方只需要自己去操作，做一遍功能测试，一些隐私的检测项就自动出结果了，同时我们也可以做一些自动化的点击触发，结合整个静态和动态的扫描系统，然后再加上一些人工检测，就能覆盖我们隐私合规的大部分检测项。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/92c90f8055c5d64e6b1b043ca97abc29.png\" /></p><p></p><p>这个是我们更全面的一些融入研发流程的建设，我们在培训、需求、架构设计、编码、测试、发布、上线，每个阶段都建立了比较多的能力，我们刚才讲的是我们建设能力的大概过程，我们先建设了隐私的评审和测试的检测能力，然后接下来才是其它的一些扩充，比如我们除了安卓系统的检测之外，还做了 iOS 的动态和静态的检测，以及快应用的检测。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/613f5df1ca6f449150ee899ccf5d7ce0.png\" /></p><p></p><p>有了这些东西之后，只能说明你具备这样的能力，但是想要让业务方完全遵守这样的规则，我们需要建立一套安全的上线流程。</p><p>&nbsp;</p><p>流程大概是这样的：在产品需求阶段，是一定要做安全隐私合规的评审的。产品研发阶段我们会有相应的一些隐私扫描，同时我们会有一些安全的 SDK 提供给业务方使用，比如数据脱敏，业务方可能不知道数据脱敏怎么做，或者说很多业务方都要做自己的数据脱敏，这可能是一个重复的建设，现在安全方提供了这样一些能力，业务方只要直接接入使用就能保证数据是没有太多敏感的问题的。</p><p>&nbsp;</p><p>在功能测试阶段，我们会有一些动态的隐私扫描和安全隐私的扫描，在上线之前，也就是功能测试完成之后，会根据业务的情况开展一些人工检测，以及制品的自动化扫描，在这个阶段一定要保证之前的需求评审通过了，整个业务上线之前要做门禁的检查，就是之前的这些安全环节有没有做？有没有达标？只有达标了之后才能发布到线上，发布到线上之后还需要做一些持续的监控分析。我们做了一些监控分析相关的系统，比如安全做了态势感知，实时扫描一些在线业务，然后也会通过一些情报去看业务有没有遭到攻击，数据隐私这块儿我们建立了一个数据治理平台，能够看到数据的使用和数据的流动情况，一旦发现不合规的问题就会及时整改。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0a/0a91f7297509b4a76509b112e1df2930.png\" /></p><p></p><p>所有流程制定完成之后，怎么样有力的执行？这就在于组织的建设。在 OPPO 把安全提到一个比较高的位置上来，所以我们建立了一个自上而下的安全组织，首先由系统高管担任我们的隐私安全委员会主任，这个委员会是系统公司级的。</p><p>&nbsp;</p><p>接下来就到了每个部门，每个部门的部长要对自己部门的安全隐私负最终责任，当然他可以指定相应的人员去开展具体的工作，我们叫安全隐私合规代表，他会去拉通整个业务部门内部的研发安全落地流程。</p><p>&nbsp;</p><p>对于安全团队，我们针对每个业务，会有相应的一个安全隐私接口人，这个接口人会拉通整个安全能力，比如我们安全团队后面有评审团队、测试团队、产品研发团队、智能风控团队和应急团队，安全隐私代表和安全隐私接口人会紧密合作，当然这个事情是需要持续运营，我们也会通过定期审计去确保一些关键策略是落地的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c3/c3064053d1df8b6475d9f9e669b30782.png\" /></p><p></p><p>我们之前讲的更多的是怎样应对监管，黑客攻击也是非常重要的一环，为了应对黑客攻击我们做了云安全的整个防御体系。首先我们从用户的请求出发，在端侧做了移动安全的一些加固和移动安全的 SDK，确保我们的端侧破解保护是有一定强度的。</p><p>&nbsp;</p><p>接下来就到了网络层，网络层我们有动态防火墙，能够限定网络区域的访问划分，我们有 OPPO 的鹰眼系统，能够监测到整个网络流量的情况，以及里面是不是有敏感信息，是不是有黑客攻击的一些蛛丝马迹。</p><p>&nbsp;</p><p>接下来到了接入层，接入层是 OPPO 整个后端业务整体的接入点，在这里我们会做一些攻击防御，比如像传统的 Web 攻击，SQL 注入、命令执行等，我们通过 OPPO 的云盾，也就是一个 WAF 去做整体的防御，然后通过 CC 防御去做 DOS 防御。</p><p>&nbsp;</p><p>在业务层，我们也提供一些能力给业务方，比如鉴权能力，确保只有正确的人才能够访问正确的数据。我们针对黑产有一些风控防御系统，我们叫做 OPPO 天御系统，它会实时检测是不是有黑产的攻击，会不会有薅羊毛的风险，同时对它去做相应的调整和处罚。</p><p>&nbsp;</p><p>在主机层我们有 OPPO 谛听这样的一些防御系统，对系统上线的容器、镜像等进行扫描，确保如果有入侵行为发生，我们能够及时发现。</p><p>&nbsp;</p><p>在数据层我们提供了安全密钥和数据脱敏这样的服务，一些产品支撑了公司的线上业务，比如像一些端到端的加密的技术，只有用户自已才能解密相关的数据，连 OPPO 都无法在云端解密这些数据。</p><p>&nbsp;</p><p>除了自建的体系之外，我们还想通过业界的一些能力补齐短板，我们建立了 OPPO 安全应急响应中心，利用白帽子的力量来增强安全隐私的能力，OSRC 除了接收传统的安全漏洞之外，还新增了一些隐私漏洞的接收范围。从现在的效果来看还是不错的，能够帮助我们补齐很多短板，优化了我们对标准的理解。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/49/49108007937337607486b148f6db0897.png\" /></p><p></p><p>当然所有的这些成长，离不开我们自己能力的持续提升，我们要有能力保证满足业务的安全需求，我们会通过几个方向持续提升能力。首先我们会跟业界，比如安全公司成立联合实验室，跟一些产学研的高校共同探索新兴的安全隐私的领域和保护技术，同时我们自己也会持续投入到一些安全攻防技术的研究里来，并且把我们的心得和想法通过公开演讲的方式同业界交流，此前我们在 Black Hat Europe 和 Black Hat Asia 上面做了公开演讲。我们在学术领域也会发一些 Paper，比如在 DMV 上发布安卓安全的文章。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0b/0b54b49168f36439b10fc967488c1b50.png\" /></p><p></p><p>我们到底做的好不好？业界的一些认证可能是我们的一些试金石，为了验证我们自己的能力，我们在全球各地引入了一些主要的认证机构去做相应的评估，比如说在欧盟我们做了 TrustArc 和 ePrivacy，在 ISO 我们做了 27701 等等，当然这只是为了检验我们的工作效果。</p><p>&nbsp;</p><p></p><h2>总结与展望</h2><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/05/054a09df4034a3ae5057261948ada5c1.png\" /></p><p></p><p>我给隐私安全建设做了能力的分级，大概分为五个阶段：</p><p>第一阶段是满足法律约束，这是底线性的东西，任何一家公司开展业务都要满足这一级；第二阶段是主动合规，此时公司会组建专业安全团队，主动满足监管的要求；第三阶段是自证合规，此阶段安全团队会跟业界一些实验室和标准认证机构合作，取得安全隐私认证，检验建设效果；第四个阶段我称作安全感知，除了做好产品安全性这件事以外，还要通过安全隐私特性（安全产品或功能）让消费者有更强的感知，让消费者认可你的产品是安全的，是能保护用户隐私的，这点非常重要，我们认为这是安全隐私工作做到比较好的一个高级阶段；最后就是保证持续处于业界第一梯队，做业界的标杆，这是非常难的，也是我们所追求的终极目标。</p><p>这就是我今天分享的所有内容，谢谢大家。</p><p></p><p></p><h5>相关阅读:</h5><p></p><p></p><p><a href=\"https://xie.infoq.cn/article/cf50fbe2c9c6ee34a3d9a0285\">关于 GDPR 体系文件介绍，介绍 GDPR 体系文件的内容和意义</a>\"</p><p><a href=\"https://www.infoq.cn/article/AZuSLCqyG2cF97ggRohX\">“史上最严”数据保护法 GDPR 是如何失败的？</a>\"</p><p><a href=\"https://www.infoq.cn/article/z0KpXWrmRIthQpgoFMAd\">面向软件开发者的“GDPR 指南”</a>\"</p><p><a href=\"https://www.infoq.cn/article/6yEwzkxOO6GmPvcLqc2W\">所有 AWS 服务 GDPR 已准备就绪</a>\"</p>",
    "publish_time": "2023-07-17 15:26:38",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "React困境与未来，何时迎来自己的“Angular.js时刻”？",
    "url": "https://www.infoq.cn/article/2U6LSkUm7V0UrsUR9oIe",
    "summary": "<p>2012年，横空出世的Angular.js一举改变了前端开发的格局，在市场上迅速取得成功。仅仅两年之后，开发团队又推出了Angular 2，基于新的范式对原始库进行全面重写。但包括我自己在内，不少开发者都不愿修改现有应用来适应新的设计思路。于是在新项目中，Angular.js不再作为优先选项，市面上其他出色的框架开始迎来自己的机会空间。</p><p></p><p>29015年，我们开始在前端开发中使用React。更简单的架构、对组件的高度关注，以及在大小代码库上始终如一的稳定生产力，让React很快成为备受好评的新选择。旺盛的人气之下，React社区也开始茁壮成长。可最近，React和Next.js团队开始推广其服务端组件——这种新的Web应用程序构建方式虽有不少优势，但并不适合大部分现有React应用。</p><p></p><p>难道说Angular.js到Angular 2的故事又要重演？React是否正在重蹈前辈的覆辙？</p><p></p><p></p><blockquote>注意：本文主要讨论React和Next.js团队引入的新功能。由于双方的密切合作，这里难以区分各项功能具体出自哪支团队之手。因此，下文将以“React”统一指代双方团队。</blockquote><p></p><p></p><p></p><h2>重新学习一切</h2><p></p><p></p><p>React的核心是一套视图库，这一点仍然保持不变：使用React Server组件，大家仍可以使用JSX构建组件，并以props的形式渲染并传递动态内容：</p><p></p><p><code lang=\"text\">function Playlist({ name, tracks }) {\n    return (\n        </code></p><div><code lang=\"text\">\n            <h1>{name}</h1>\n            \n                    {tracks.map((track, index) =&gt; (\n                        \n                    ))}\n                <table>\n                <thead>\n                    <tr>\n                        <th>Title</th>\n                        <th>Artist</th>\n                        <th>Album</th>\n                        <th>Duration</th>\n                    </tr>\n                </thead>\n                <tbody><tr>\n                            <td>{track.title}</td>\n                            <td>{track.artist}</td>\n                            <td>{track.album}</td>\n                            <td>{track.duration}</td>\n                        </tr></tbody>\n            </table>\n        </code></div><code lang=\"text\">\n    );\n}</code><p></p><p></p><p>但除此之外，其他的一切都随着服务端组件的出现而有所变化。数据获取不再依靠useEffect 或者 react-query 实现；相反，我们需要在异步组件中使用fetch：</p><p></p><p><code lang=\"text\">async function PlaylistFromId({ id }) {\n    const response = await fetch(`/api/playlists/${id}`);\n    if (!response.ok) {\n        // This will activate the closest `error.js` Error Boundary\n        throw new Error('Failed to fetch data');\n    }\n    const { name, tracks } = response.json();\n    return ;\n}</code></p><p></p><p>注意，这里的fetch函数跟浏览器fetch不同。React对其进行了增强，能够自动请求重复数据删除。为什么一定要这样调整？如果我们需要在组件树中深入访问获取的数据，由于于 useContext 已在服务端组件中被禁用 ，所以无法将fetch放置在React Context当中。现在若需要在组件树内的不同点处访问获取的数据，推荐方法是在必要时执行重新获取，再通过React执行重复数据删除。</p><p></p><p>这个fetch 函数还会默认缓存数据，无论响应缓存标头如何。实际获取过程发生在构建过程中。</p><p></p><p>如果大家希望创建一个按钮来启动POST操作，现在需要将其包含在表单内并使用服务端操作，也就是使用带有use server的函数：</p><p></p><p><code lang=\"text\">export function AddToFavoritesButton({ id }) {\n    async function addToFavorites(data) {\n        'use server';\n \n        await fetch(`/api/tracks/${id}/favorites`, { method: 'POST' });\n    }\n    return (\n        </code></p><form action=\"{addToFavorites}\"><code lang=\"text\">\n            <button type=\"submit\">Add to Favorites</button>\n        \n    );\n}</code></form><p></p><p></p><p>典型的React hooks（包括useState, useContext, useEffect）现在都会导致服务端组件出错。如果仍须使用，大家只能借助 use client escape路由，也就是强制React在客户端渲染组件。请注意，这本是Next.js中的默认操作，但在引入服务端组件之后成了可选功能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/aa/aa2ac5ab223d60a349d60bd0b414ef78.png\" /></p><p></p><p>CSS-in-JS跟服务端组件也不兼容。如果大家习惯了使用sx或者css prop直接设置组件样式，现在就必须学习CSS Modules、Tailwind或者Sass。对我来说，这种调整更像是倒退：</p><p><code lang=\"text\">// in app/dashboard/layout.tsx\nimport styles from './styles.module.css';\n \nexport default function DashboardLayout({\n    children,\n}: {\n    children: React.ReactNode,\n}) {\n    return </code></p><section><code lang=\"text\">{children}</code></section><code lang=\"text\">;\n}\n/* in app/dashboard/styles.module.css */\n.dashboard {\n    padding: 24px;\n}</code><p></p><p></p><p>那调试受了什么影响？恭喜了家人们，React DevTools无法显示React服务端组件的详细信息。我们无法在浏览器中检查组件以查看它使用的具体props或子组件。目前，调试React服务端组件的唯一方式就是借助console.log。</p><p></p><p>服务端组件的认知模型与客户端JS完全不同，只有底层JSX保持不变。所以哪怕大家精通React开发，在面对服务端组件时还是得重新学习——除非您已经拥有丰富的PHP开发经验。</p><p></p><p>说实话，React中的新功能大部分处于“Alpha”早期阶段，也许未来会在稳定版发布时得到解决。</p><p></p><p></p><h2>缺少开发生态系统</h2><p></p><p></p><p>如前所述，现在我们没法用react-query 进行数据获取。事实证明，它绝不是唯一跟React服务端组件不兼容的库。如果各位用惯了以下工具，是时候寻找替代方案了：</p><p></p><p>material-ui,chakra-ui,Emotion,styled-componentsReact-query,swr,react-hook-form,大部分SaaS提供商的SDK还有更多。</p><p></p><p>这些库通通使用标准React hooks，所以通过服务端组件调用时会出错。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a597bb9f70cdc30a55b4cb5387a4c97d.png\" /></p><p></p><p>如果大家需要这些库，就只能使用use client 指令将它们封装在强制客户端渲染的组件当中。</p><p></p><p>强调一下：React服务端组件几乎破坏了一切现有React第三方库，这些库的作者必须修改代码以重新兼容。有些人会出手，有些人可能放着不管。哪怕是前一种情况，这个过程也需要时间。</p><p></p><p>所以如果大家使用React服务端组件启动应用，那现有React生态系统将瞬间不复存在。更要命的是：客户端React还提供服务端组件尚未涵盖的多种日常工具。例如，React Context就是管理依赖项注入的绝佳方案。如果没有React Context，那服务端组件就需要单独的依赖项注入容器（Dependency Injection Container，类似Angular的办法）。如果核心团队不帮忙，这活就得靠技术社区完成。</p><p></p><p>与此同时，我们还需要手动编写大量代码。想象一下，在没有UI Kit、表单框架、智能API客户端和SaaS集成的前提下搞开发，其难度可想而知。</p><p></p><p>原先的React生态系统已经成了该项目最显著的优势，也是React得到广泛的普及的根本原因。可如今，React服务端组件可谓是自毁长城。</p><p></p><p></p><h2>没有困难，硬要创造困难</h2><p></p><p></p><p>服务端端渲染早就有其成熟方案。服务端端脚本接收请求，获取数据并生成HTML。客户端渲染也是一样，浏览器检索数据、客户端脚本随后更新DOM。</p><p></p><p>但React偏要力推服务端端加客户端混合渲染，属于没有困难硬是创造困难。这样大家既可以在服务端组件中使用客户端组件，又可以在客户端组件中使用服务端组件。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/75/750d89e316b036cdf4054a2b88ad0f91.png\" /></p><p></p><p>当客户端组件渲染服务端组件时，React服务端不会发送HTML，而是发送组件树的文本表示。之后，客户端脚本会在客户端上渲染该组件树。</p><p></p><p>如果大家习惯了使用HTML或者JSON来调试AJAX请求，肯定会对此大吃一惊。下面来看React如何用RSC Wire格式将更新从服务端组件流式传输至客户端：</p><p></p><p><code lang=\"text\">M1:{\"id\":\"./src/ClientComponent.client.js\",\"chunks\":[\"client1\"],\"name\":\"\"}\nS2:\"react.suspense\"\nJ0:[\"$\",\"@1\",null,{\"children\":[[\"$\",\"span\",null,{\"children\":\"Hello from server land\"}],[\"$\",\"$2\",null,{\"fallback\":\"Loading tweets...\",\"children\":\"@3\"}]]}]\nM4:{\"id\":\"./src/Tweet.client.js\",\"chunks\":[\"client8\"],\"name\":\"\"}\nJ3:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"@4\",null,{\"tweet\":{...}}}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"@4\",null,{\"tweet\":{...}}}]}]]}]\n</code></p><p></p><p>这种格式没有任何可读性，纯属具体实现。</p><p></p><p>但HTTP、JSON和JSX之所以如此流行，靠的就是良好的可读性。而React服务端组件显然破坏了这种优势。</p><p></p><p>React服务端组件实在晦涩难懂，对大多数开发者而言都难以阅读或调试。这样设计真能提高生产力吗？还是说只会起反作用？</p><p></p><p></p><h2>有必要这么折腾吗？</h2><p></p><p></p><p>如果单从第一性原理角度出发，那这样修改确有其合理性：使用少量AJAX的服务端渲染，能够提高Web应用程序的构建效率。Dan Abramov在Remix Conf 2023大会上对React服务端组件的开发动机做出了精彩的解释：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/182f8c2a61b0395e0a46e9c4f07c680f.png\" /></p><p></p><p>这种架构特别适合电子商务网站、博客及其他关注SEO的内容为中心类网站。</p><p></p><p>但这并不是什么新鲜概念。多年以来，Hotwire in Rails和Symfony等应用工具一直在使用这种架构。</p><p></p><p>此外，服务端组件希望解决的不少问题（包括数据获取、分部渲染等）早已在某些单页应用中有了答案。至于管理面、SaaS、B2B应用、内部应用、CRM、ERP、长周期运行应用等当中的其他问题（大捆绑包、首次加载缓慢、SEO等）其实根本不算真正的问题。</p><p></p><p>正因为如此，大部分React开发者才对单页应用架构非常满意。如果真需要做服务端渲染，我们完全可以选择生态系统比React服务端组件更成熟的其他工具。</p><p></p><p>那既然用不上，我们为什么还要认真讨论？</p><p></p><p></p><h2>构建React应用的标准方法</h2><p></p><p></p><p>我想强调的第一点，就是React正阻止人们使用单页应用架构。或者更确切地讲，他们不鼓励开发者在不配合框架的前提下使用React，而他们所推荐的框架会更多强调服务端渲染。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/970988c255b047b53fd83d1d0f7fde4d.png\" /></p><p></p><p>还有第二个问题。React.js官方文档主要推荐使用Next.js。</p><p></p><p>Next.js官方文档则主要推荐使用React服务端组件的13.4及更高版本。</p><p></p><p>换句话说，React服务端组件已经成为构建React应用的默认方式。React生态系统的新手会习惯于直接使用，但在我看来这一切还没准备好。Dan Abramov也承认这一点：</p><p></p><p></p><h5>“要让新范式真正发挥作用，还需要进行大量工作。”</h5><p></p><p></p><p>React服务端组件要求router和bundler全面更新换代，但目前这些都处于alpha阶段，还远无法适应生产开发的要求。</p><p></p><p>所以，Next.js在那急什么劲呢？</p><p></p><p>我不禁怀疑，Next.js目前的作法并不是要帮助开发者，而是想帮Vercel宣传React。SPA本身没有营销空间——一旦编译完成，SPA就是一个能在任何地方随意托管的JS文件。但服务端渲染的应用必须借助服务器才能运行，而服务器显然是可以营销的产品。也许我有点阴谋论的倾向，但除此之外我真的无法理解为什么要如此明目张胆地破坏React生态系统。‘</p><p></p><p></p><h2>现有应用不受影响</h2><p></p><p></p><p>与Angular.js到Angular 2的过渡不同，React服务端组件的推出并不算是重大变化。现有单页应用仍可适配最新版本的React，使用Pages router构建的现有Next.js应用同样可以正常运行。</p><p></p><p>因此，对于React是否将迎来自己的“Angular.js时刻”这个问题，答案显然是否定的。但如果大家现在起打算新开一个项目，那会如何选择？是拥有成熟工具和生态系统的健壮架构（单页应用），还是React团队强烈推荐的新方案（服务端组件）？这是个艰难的选择，如果人们害怕自己选错，很可能会直接转投其他框架的怀抱。</p><p></p><p>我个人认为React靠单一工具满足所有Web开发需求的愿景太过激进——或者说，至少目前的解决思路是有问题的。在我看来，最典型的证明就是Next.js文档中的下拉列表——读者可以在App router（服务端组件）和Pages router之间随意选择。如果一款工具为同一种功能提供两种截然不同的实现方法，但它真的还是同一款工具吗？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/dc/dc63f85cdea3c620fe876f8e971f04ed.png\" /></p><p></p><p>所以对于“React过度膨胀的野心是否在损害社区”，我的答案是肯定的。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>服务端组件也许的确代表着服务端框架的进步——或者至少在达到生产就绪状态后，应该有其进步意义。但对于更广泛的React社区，我担心这股潮流会带来碎片化风险，甚至威胁React多年来辛苦建立的江湖地位。</p><p></p><p>如果开发团队能听到我的声音，那我真诚希望React和Next.js两家能采取更平衡的方法。希望React团队能意识到，单页应用架构是一种非常有效的选项，仍然拥有旺盛的生命力。我也希望看到Next.js能在自己的官方文档中淡化对服务端组件的强调，或者至少要明确标注其尚属于“Alpha”功能。</p><p></p><p>当然，也许事实证明我是错的，服务端组件才是未来。毕竟开发者命里注定要不断适应新的范式，持续变化才是软件行业的永恒本质……谁知道呢？</p><p></p><p></p><h5>原文链接：</h5><p></p><p></p><p>https://marmelab.com/blog/2023/06/05/react-angularjs-moment.html</p><p></p><p></p><h5>相关阅读：</h5><p></p><p><a href=\"http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247516803&amp;idx=1&amp;sn=f584917f8afe2f7bb10f8686acb040cf&amp;chksm=f95237c0ce25bed6980bdc34c630cbda97d0325a489663440b4c904499ca81da31729d476476&amp;scene=27#wechat_redirect\">React&nbsp;开发者们的 Solid.js&nbsp;快速入门教程</a>\"</p><p><a href=\"https://www.infoq.cn/article/Tv3SyqoivXMWUoj8qSMT\">从新&nbsp;React&nbsp;文档看未来 Web 的开发趋势</a>\"</p><p><a href=\"https://www.infoq.cn/article/CZKMjHaxbf1Z7xcSzisX\">我被&nbsp;React&nbsp;劫持了，很痛苦又离不开</a>\"</p><p><a href=\"https://xie.infoq.cn/article/ec0a13741cd7c4a72eea370ae\">如何快速上手 angular.js</a>\"</p>",
    "publish_time": "2023-07-17 15:27:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中科院开发出新型锂电池固态电解质；欧拓开发 Zeta-Light 阻尼系统；罗姆开发出用于汽车内饰的新型 RGB 芯片 LED ；Jeff Dahn 电池实验室正在研发单晶无钴锂电池...",
    "url": "https://www.infoq.cn/article/h353s0jlIwFWlJpKt4jm",
    "summary": "<p>中科院研究团队开发出一种新型固态电解质 LZCO，为全固态锂电池的商业化提供了新的可能性； Vector 推出了适用于 10BASE-T1S 标准的汽车以太网接口；日本供应商 Ryobi 将采用“巨型铸造”技术生产大型<a href=\"https://xie.infoq.cn/article/83b6baf70bc2ef38f5041a475\">电动汽车</a>\"车身零部件，预计可降低制造成本 20% ；Jeff&nbsp;Dahn 团队正在研究一种不含钴的特殊单晶阴极材料—— NM64……本周汽车技术领域又有哪些新动作，一起来看。</p><p></p><p></p><h2>中科院开发出新型锂电池固态电解质</h2><p></p><p></p><p>中国科学院直属高校中国科学技术大学马骋教授领导的研究团队开发出了一种新型固态电解质——氯氧化锆锂（LZCO）。这种固态电解质可以提高全固态锂电池（ASSLB）的性能。</p><p></p><p>LZCO 是通过常见的化合物合成的，例如一水氢氧化锂、氯化锂和氯化锆。因此，它的成本低至每公斤 11.60 美元，远低于商业化门槛的每公斤 50 美元，也低于其他高性能固态电解质的价格。该材料具有高离子电导率和出色的压缩性能。它的室温离子电导率为 2.42&nbsp;mS&nbsp;cm-1 ，是目前所有固态电解质中最高的。此外，在 300&nbsp;MPa 的压力下，它的密度可以达到 94.2% ，优于其他易压缩的固态电解质。实验证明，LZCO 的综合性能可以与最先进的固态电解质相媲美。通过将 LZCO 与富镍层状正极组合成的 ASSLB 展现出出色的性能，可以在室温下稳定循环超过 2000 次，即使在 12 分钟快速充电条件下也是如此。</p><p></p><p>LZCO 的开发标志着固态电解质性能和成本效益的重大突破，为<a href=\"https://xie.infoq.cn/article/dc3ffba9b646bb8aa95f297b2\">全固态锂电池</a>\"的商业化提供了新的可能性。</p><p></p><p></p><h2>DELO 推出用于电机的耐高温双固化结构粘合剂</h2><p></p><p></p><p>DELO 推出了一种名为 DELO&nbsp;DUALBOND&nbsp;HT2990 的耐高温双固化粘合剂，专为电机制造而设计。这种粘合剂能够在短时间内进行光预固定，从而加快装配流程并提高生产效率。它在磁铁粘合和磁铁堆叠等工艺中的应用，可以形成一个固化外层，保持元器件位置稳定，而且不会渗出或渗透到其他部件上。最终固化需要在 150℃&nbsp;/&nbsp;30 分钟或 130℃&nbsp;/&nbsp;60 分钟的条件下进行，也可以选择热感应固化。 DELO&nbsp;DUALBOND&nbsp;HT2990 在钢材上的拉伸剪切强度最高可达 70&nbsp;MPa ，在室内温度下具有高达 165℃ 的玻璃化转变温度。在高温条件下，它在铝上仍然具有 7MPa 的压缩剪切强度，粘接非常牢固。</p><p></p><p></p><h2>欧拓开发 Zeta-Light 阻尼系统</h2><p></p><p></p><p>欧拓开发了 Zeta-Light ，它是一种集成阻尼功能的创新产品，可用于车辆中的地毯或内部仪表板等装饰部件，明显提高声学性能。Zeta-Light 通过先进的集成颗粒阻尼器，在轻量化的可持续技术基础上实现了隔音和吸音性能的提升，并具有减弱车身低频振动的新功能。与传统阻尼产品相比，Zeta-Light 在更低的重量下有效处理低频结构噪声，并且不受温度影响。此外，通过将 Zeta-Light 集成到可持续毛毡基部件的背面，不仅简化了物流和组装过程，还提升了可回收性。</p><p></p><p></p><h2>Vector 推出适用于 10BASE-T1S 的汽车以太网接口</h2><p></p><p></p><p>德国软件与汽车技术供应商 Vector 推出了适用于 10BASE-T1S 标准的汽车以太网接口，包括 VN5650 和 VN5240 。这些接口可以通过新的物理层模块进行升级，提供对 10BASE-T1S 和 100/1000BASE-T1 网络的仿真、分析和测试支持。用户可以使用新的物理层模块对接口进行模块化，从而支持汽车以太网环境中的 10BASE-T1S 和 100/1000BASE-T1 并行。此外，接口还可以与其他以太网标准相结合，实现媒体转换。未来还将推出 2.5G/5G/10GBASE-T1 的物理层模块，进一步扩展适用范围。</p><p></p><p></p><h2>罗姆开发出用于汽车内饰的新型 RGB 芯片 LED</h2><p></p><p></p><p>罗姆半导体开发了适用于汽车内饰的 RGB 芯片 LED&nbsp;SMLVN6RGBFU ，能够满足车辆内部的功能和状态指示以及装饰照明的需求。该产品通过精确控制 RGB 元件的发光特性和改善混色能力，减少了颜色变化。罗姆采用垂直集成生产系统，最大限度地减少了元件颜色变化，并通过独特的混色控制技术实现了准确的色彩表达。此外，罗姆还提供色度模拟系统，支持根据用户规格定制的详细色彩匹配，并通过提供调整系数和校准支持来实现精确的色彩匹配。</p><p></p><p></p><h2>Ryobi&nbsp;将采用“巨型铸造”技术生产电动汽车车身零部件</h2><p></p><p></p><p>日本主要铝制汽车零部件供应商 Ryobi 宣布，将采用“巨型铸造”技术生产大型电动汽车车身零部件，这将降低车身制造成本 20% 。该技术通过压铸大型铝制部件以取代多个钢制部件。Ryobi 加入了其他供应商的行列，致力于为电动汽车改造供应链。特斯拉、小鹏、吉利、沃尔沃、大众和丰田等车企也计划引入巨型铸造技术。该技术减少了对钢板冲压制造商的需求，对汽车供应链产生了变革。Ryobi 将在静冈县的菊川工厂建造一座大楼，并引进一台成型机。预计从 2025 年 3 月开始接收来自日本国内汽车制造商的零部件订单。巨型铸造技术还可以减少二氧化碳排放量，并提高动力消耗效率。然而，巨型铸造技术需要企业掌握专业知识，以防止形成空洞，并且在事故后需要更换整个大型部件。</p><p></p><p></p><h2>特斯拉支持的 Jeff&nbsp;Dahn 电池实验室正研发单晶无钴锂电池</h2><p></p><p></p><p>加拿大戴尔豪斯大学（Dalhousie&nbsp;University）的 Jeff&nbsp;Dahn 电池实验室正在研究一种新型锂离子电池化学物质，采用单晶、无钴阴极并掺杂钨，以提升电池性能和延长使用寿命。Dahn 与特斯拉有着密切的合作关系，特斯拉在该领域的很多技术成果归功于锂离子电池研发的先驱 Jeff&nbsp;Dahn 。</p><p></p><p>Jeff&nbsp;Dahn 和他的团队正在研究一种不含钴的特殊单晶阴极材料，称为 NM64 。与多晶电极相比，它具有更长的使用寿命和更好的稳定性。单晶 NM64 阴极由一个简单的全干合成工艺生产而成，无需用水，也不需要中间化学品，产生的废料也少。据研究显示，该电池的高压稳定性极好，高达 4.4 伏。研究还发现，在阴极上添加 0.3% 的钨涂层可以提高电池的循环稳定性。且该款新型阴极材料生产过程简单，可使用现有的商业设备，使制造更容易和更具成本效益。但是旧式的液态电解质锂离子电池仍然有市场，因此要在特斯拉电动汽车上看到该技术还得等一段时间。</p><p></p><p></p><h2>加拿大研发新型量子衍生全息技术</h2><p></p><p></p><p>加拿大渥太华大学、加拿大国家研究委员会（NRC）以及英国伦敦帝国理工学院的研究人员合作研发了一种新型的量子衍生全息技术。该技术可以记录和重构极其微弱的光束，仅由一个光子构成。与传统全息技术相比，该技术具有更好的稳定性和灵活性，可以长时间记录全息图并实现超高精度。此外，该技术还可以用于记录自发光或远处物体的全息图。该研究的成果有望在 3D 场景重建、自动驾驶、增强现实等领域得到应用，同时也有助于远距离物体的 3D 成像和描述从量子点和单原子发射的单光子的空间形状。此项研究的进展对全息技术的发展以及天文学、纳米技术和<a href=\"https://xie.infoq.cn/article/4936720755852d766170ca435\">量子计算</a>\"等领域的应用具有重要意义。</p>",
    "publish_time": "2023-07-17 16:35:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]