[
  {
    "title": "Uber 履约系统如何实现零停机时间迁移",
    "url": "https://www.infoq.cn/article/hvjHdC7W7MFaXCEs5lXZ",
    "summary": "<p>在大规模分布式系统中，将关键系统从一种架构迁移到另一种架构，不仅在技术层面存在挑战性，还需要一个精细的迁移过程。Uber 运营着全球最复杂的实时履约系统之一。本文将介绍 Uber 如何将工作负载无缝地从本地环境迁移到混合云架构，并实现零停机时间和最小的业务影响。</p><p></p><p></p><h2>系统复杂性</h2><p></p><p></p><p>Uber 的履约系统是一个具备实时性、一致性和高可用性的系统。用户持续不断地与应用程序发生互动，如发起新行程、取消已有行程和修改行程细节。</p><p></p><p>餐厅不断更新订单状态，快递员在繁忙的城市中穿梭，确保包裹能够及时送达。系统每秒钟处理超过两百万次交易，高效地管理着平台上用户和行程的状态。</p><p></p><p>我是将这个履约系统从本地架构迁移至混合云架构的技术负责人。除了开发新系统所需的代码并进行验证之外，最具挑战性的部分是设计一个可以实现零停机时间并将对客户影响减少到最小的迁移策略。</p><p></p><p></p><h2>旧系统架构</h2><p></p><p></p><p>让我来概述一下原有的履约系统。它由多个服务构成，这些服务在内存中维护用户和行程实体的实时。此外还有一些辅助服务负责处理锁定机制、搜索功能和数据存储，确保系统能够跨数据中心进行数据复制。</p><p></p><p>所有的乘客端交易都被定向到“需求”服务，而所有的司机端交易被定向到“供应”服务。这两个系统通过分布式事务技术来保持同步。</p><p></p><p>这些服务以 pod 的形式运行。一个 pod 就是一个自给自足的单元，内部的服务相互交互，共同为特定城市的履约活动提供支持。一个请求进入一个 pod，通常会在这个 pod 的内部服务之间流转，除非需要访问 pod 外部的服务数据。</p><p></p><p>在下图中，绿色和蓝色服务组分别代表两个不同的 pod，每个 pod 包含了所有服务的副本。城市 1 的流量被路由到 pod1，城市 2 的流量被路由到 pod2。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f6/f6ce1e035059f26abf4f80b33a8030cc.png\" /></p><p></p><p>此外，该系统还通过采用 saga 模式促进了 A、B 和 C 服务之间的分布式事务，确保了它们存储的实体数据保持同步。通过内存数据管理和序列化技术，每个服务内部的实体数据一致性得到了有效的保证。</p><p></p><p>这个系统在 Uber 的早期发展阶段就进行了扩展。它采取了一些架构决策，这些决策可能更贴合 Uber 不断增长的业务需求。首先，系统被设计为优先考虑可用性而非一致性。也就是说，在多个系统中保存了不同的实体，系统最终能够达到一致状态，但并没有通过真正的 ACID 兼容系统来管理跨系统的变更。由于所有数据都存储在内存中，系统在垂直扩展和特定服务的节点数量方面存在固有的限制。</p><p></p><p></p><h2>重新设计的系统</h2><p></p><p></p><p>新系统减少了服务数量。原本处理“需求”和“供应”等实体的服务被合并到一个由云数据存储提供支持的单体应用程序中。所有的事务管理的责任都被转移到了数据存储层。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a9/a91be6de8dafdb07f0c38cad92ed1625.png\" /></p><p></p><p></p><h2>新系统和已有的服务消费者</h2><p></p><p></p><p>对于这种关键系统的迁移，需要一个全面的多维策略来覆盖迁移的方方面面。在微服务环境中，每个系统都与周围的许多系统进行着复杂的交互。交互主要通过两种方式进行：API 和发布到消息总线中的事件。</p><p></p><p>新系统修改了所有的核心数据模型、API 契约和事件模式。因为有数百个 API 调用者和系统消费者，所以不可能一次性迁移完毕。我们采取了一种“向后兼容层”的策略，保持现有 API 和事件契约不变。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a5/a55076c108287465e9a4aa514dd726dd.png\" /></p><p></p><p>创建一个向后兼容层使得系统可以在不中断现有消费者使用旧接口的情况下进行重新架构。在接下来的几年中，消费者可以根据自己的节奏逐步迁移到新的 API 和事件模式，不必与整个系统的重新设计紧密耦合。同样，旧系统的事件消费者也将继续通过向后兼容层以旧模式接收事件。</p><p></p><p></p><h2>实体的生命周期</h2><p></p><p></p><p>这个过程的复杂性在于不断变化的实体状态。以系统中的三个关键实体——乘客、司机和行程为例，它们可以在不同的时间点开始和停止。旧系统将这些实体存储在不同的内存系统中，而新系统必须在数据库中反映这些变化。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/5d/5d38684735fa4c1426fee6dc09e6763b.png\" /></p><p></p><p>迁移过程的一个主要挑战是确保在过渡期间，每个实体的状态及其相互关系都能被保留，并且能够适应高频率的变化。</p><p></p><p></p><h2>迁移策略</h2><p></p><p></p><p>在迁移的每个阶段——发布之前、发布期间和发布之后——都必须采用多种策略，确保流量能够平稳地从现有系统转移到新系统。我将在文章的后续部分进行详细的讨论。</p><p></p><p></p><h2>发布之前</h2><p></p><p></p><p>影子验证</p><p></p><p>确保新旧系统间的 API 契约一致性至关重要，这有助于建立信心，保证新系统与现有系统的一致性。引入向后兼容性验证层可以确保旧系统和新系统之间 API 和事件的一致性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/80/8056b39f70dbe2c8406add3be7845c31.png\" /></p><p></p><p>每个请求都会被发送给旧系统和新系统。两个系统对请求的响应将根据键值进行比较，差异被记录到一个可观测性系统中。在每次发布之前，我们的目标是确保两个系统的响应完全一致。</p><p></p><p>在实时系统中确实存在一些细微的差别。特别是在即时系统中，并非所有调用都会成功并获得确切的响应。一种策略是将所有成功的调用视为一个具有高匹配率的群组，并确保大多数 API 调用都是成功的。在某些有效的情况下，对新系统的调用可能会失败。例如，当司机请求离线 API，而新系统中没有该用户的上线记录，可能会出现客户端错误。对于这些边缘情况，我们可以暂时忽略一致性不匹配的问题，但同时需要保持警惕。</p><p></p><p>此外，只读 API 的一致性是最容易实现的，因为它们没有副作用。对于写入 API，我们引入了一个系统，旧系统会在共享缓存中记录请求响应，新系统会重放这些响应。因为我们有跟踪标头，新系统可以透明地获取旧系统最初接收的响应，并将它们从缓存中传输到新系统，而不是进行全新的外部调用。我们因此可以在不受外部依赖影响的情况下更好地匹配新旧系统的行为，在响应和字段方面实现更高的一致性。</p><p></p><p>端到端集成测试</p><p></p><p>大规模重构是提高端到端（E2E）测试覆盖率的一个好时机。这样可以确保新系统在流量增长和新代码部署时的稳定性。在此次迁移过程中，我们将测试用例数量增加至 300 多个。</p><p></p><p>端到端测试应在开发生命周期的多个关键节点执行：在工程师编写代码时、在构建过程的预部署验证阶段、以及在生产环境中持续进行。</p><p></p><p>金丝雀黑盒测试</p><p></p><p>为了最小化潜在的负面影响，可以先只将代码暴露给一小部分请求，避免不良代码造成广泛影响。为了进一步优化策略，可以只在预生产环境中部署代码，并在这个环境中持续执行端到端测试。只有在金丝雀测试环境成功通过所有测试后才允许部署系统继续推进到生产环境。</p><p></p><p>负载测试</p><p></p><p>在将大量生产负载转到新系统之前，必须进行全面的负载测试。由于我们能够通过向后兼容层将流量重定向到新系统，所以可以将全生产流量转到新系统。此外，我们编写了自定义负载测试脚本来验证包括数据库和网络系统在内的独特集成点。</p><p></p><p>预热数据库</p><p></p><p>云服务供应商的数据库和计算资源扩展通常不是即时完成的。在面对突然增加的负载时，系统的扩展可能跟不上需求。在分布式数据存储环境中，数据的再均衡、压缩和分区拆分可能会引发热点问题。为了应对这些挑战，我们通过模拟合成数据负载实现了一定程度的缓存预热和分区拆分。这样可以确保在生产流量流向这些系统时系统能够平稳运行，无需突然进行系统扩展。</p><p></p><p>回退网络路由</p><p></p><p>由于应用程序和数据库系统分别部署在两个不同的云平台上，且一个在本地，另一个在云端，因此管理网络拓扑就变得尤为重要。两个数据中心至少建立了三条网络路由，并通过专有网络连接数据库。用超过三倍正常容量的流量对这些网络进行验证和负载测试，确保在生产环境中即使在高负载条件下网络连接也能保持稳定和可靠。</p><p></p><p></p><h2>发布期间</h2><p></p><p></p><p>流量固定</p><p></p><p>多个应用程序在相同的行程中相互协作，司机和乘客的行程代表同一个行程实体。当 API 调用从乘客端转移到新系统时，相关的行程和司机状态也必须从旧系统中迁移出来。我们已经实现了路由逻辑，确保行程能够持续进行并在同一个系统中完成。为此，我们在迁移执行前约 30 分钟开始记录所有消费者的标识符，确保相关实体的请求会被锁定在同一系统中。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c4/c4f987366a8bedad1bce14d22b7383a2.png\" /></p><p></p><p>分阶段发布</p><p></p><p>我们对选定城市的首批迁移行程进行了端到端测试。随后，我们将非活跃行程中的空闲乘客和司机迁移至新系统。迁移完成后，所有新的行程都只在新系统中启动。在这一过渡期间，两个系统并行提供服务。随着旧系统中的行程逐渐结束，乘客和司机也将逐步迁移至新系统。在一小时内，整个城市的服务完全转移到新系统。若新系统出现故障，我们可以执行相同的迁移流程，反向将服务迁移回旧系统。</p><p></p><p></p><h2>发布之后</h2><p></p><p></p><p>可观测性和回滚</p><p></p><p>在两个系统间进行切换时，确保健壮的跨系统可观测性至关重要。为此，我们开发了一个仪表盘，它能够实时显示流量逐渐从旧系统迁移到新系统。</p><p></p><p>我们的目标是在迁移过程中确保关键业务指标——行程量、可用供应、行程完成率和行程开始率——保持稳定。这些聚合指标应该在迁移过程中保持平稳，尽管在迁移的关键时刻旧系统与新系统的混合比例可能会发生变化。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/60/605670dadeec32029276a739763e8662.png\" /></p><p></p><p>由于 Uber 在数千个城市运营，并提供众多功能，因此，对这些指标进行城市级别的细致观察至关重要。在迁移数百个城市的过程中，手动监控所有指标是不现实的。我们必须开发专门的工具来自动监控这些指标，并能够突出显示那些显著偏离正常范围的指标。简单的警报系统也是无效的，因为大量没有根据每个城市的流量特点进行适当调整的无效警报会削弱对迁移过程的信心。因此，我们需要一个静态工具来深入分析每个城市的健康状况，确保所有城市在迁移过程中的平稳运行。</p><p></p><p>如果一个城市显示出不健康的迹象，我们可以选择只将该城市回滚到旧系统，而让其他城市继续运行在新系统中。</p><p></p><p>生产环境黑盒测试</p><p></p><p>与“金丝雀黑盒测试”策略类似，我们应该持续在生产系统上运行相同的测试，以便及时发现潜在问题。</p><p></p><p></p><h2>成功迁移的关键要素</h2><p></p><p></p><p>确保旧系统拥有充足的运行时间至关重要，这有助于我们专注于新系统开发而不受干扰。在着手新系统开发之前，我们花了四个月时间来提升旧系统的稳定性。跨项目规划至关重要，我们先在旧系统中开发功能，随后，在新系统开发过程中，这些功能在两个系统中并行开发，最后在在新系统中开发。为了避免长期同时维护两个系统，这种平衡策略至关重要。</p><p></p><p>在迁移关键系统时，确保这些系统具备强大的一致性保障，这是保证业务连续性的关键。40% 左右的工程资源应该放在新架构的可观测性、迁移工具和强大的回滚机制上。</p><p></p><p>关键技术要素包括：对 API 流量的全面控制，精确到单个用户级别；强大的字段级一致性；影子匹配机制；健壮的跨系统可观测性。在迁移过程中，我们需要面对充满竞态条件和特殊处理的情况，可能需要明确的业务逻辑或产品开发策略来应对。</p><p></p><p>流量向新系统的迁移在开始时保持平稳，并在迭代中逐渐增长。随着对新系统的信心增强，流量迁移往往会呈现出指数级的加速，如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8b/8bfcc1e8ded8109d31356c15a8a75782.png\" /></p><p></p><p>提前加速迁移的挑战在于，你可能会陷入同时维护两个同等重要的系统，这需要更长时间的维护。由于新系统规模的扩大，任何中断都可能对生产用户造成重大影响。因此，最佳策略是在将流量迁移到新系统之前，先对最小数量的功能集进行验证，在一段时间内保持稳定，然后在较短时间内快速增加流量迁移。</p><p></p><p>在 Uber，我们在开发和迁移过程中仅影响了不到几千名用户。本文所讨论的技术是我们从旧系统平稳过渡到新系统的关键。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/uber-migration-hybrid-cloud/\">https://www.infoq.com/articles/uber-migration-hybrid-cloud/</a>\"</p>",
    "publish_time": "2024-09-04 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "单集群 QPS 超 10w，存储成本降低 70%！招联金融基于 Apache Doris 数仓升级实践",
    "url": "https://www.infoq.cn/article/Zjm6DvZXaxJ80TmCjLYC",
    "summary": "<p></p><blockquote>作者｜严奕华，招联金融数仓团队负责人</blockquote><p></p><p></p><p>在竞争激烈的消费金融市场中，有效利用海量数据、提升业务运营效率是赢得市场的关键。早期招联采用典型的 Lambda 架构提供业务报表、数据运营、个性推荐、风险控制等数据服务，而 Lambda 过多的技术栈也引发了数据孤岛、查询效率不足、代码复用性差以及开发运维成本高昂等诸多问题。因此，招联引入 <a href=\"https://doris.apache.org/?utm_source=InfoQ&amp;utm_medium=1&amp;utm_campaign=post\">Apache Doris</a>\" 对架构进行了升级，不仅替换了冗余的技术栈，还实现了实时数仓存储和计算引擎的统一，从而大幅精简了整体架构。</p><p></p><p>如今，招联内部已有 40+ 个项目使用 Apache Doris ，拥有超百台集群节点，个别集群峰值 QPS 可达 10w+ 。通过应用 Doris ，招联金融在多场景中均有显著的收益，比如标签关联计算效率相较之前有 6 倍的提升，同等规模数据存储成本节省超 2/3，真正实现了降本提效。</p><p></p><h2>存在的问题</h2><p></p><p></p><p>早期架构由实时数仓和离线数仓两套组成，是较为典型的 Lambda 架构。由于历史原因，整个架构非常复杂，用到 Hbase、kafka、Clickhouse、 Spark、Impala、Hive、Kudu、Vertica 等多种技术栈。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/5308b5d8f31b3f7cbb96efb773aa5f7b.png\" /></p><p></p><p>该架构虽功能完备，但由于其技术栈的复杂度及能力的局限性也带来了诸多问题:</p><p></p><p>运维依赖性高：Lambda 架构包含较多的技术组件，且部分组件为闭源、内部逻辑不透明，强依赖厂家技术支持。资源利用率低：实时及离线两套架构间代码无法复用，这无疑增加了维护成本；且两套架构间资源无法合理共享和调度、数据无法复用，资源利用率非常低。数据时效性低：组件多、数据处理链路也长，多组件数据传输影响了时效性，降低了数据查询的效率。并发能力弱： Vertica、Impala 等部分查询引擎无法应对高并发场景的需求。</p><p></p><h2>升级目标</h2><p></p><p>基于以上待解决的问题，招联对未来即将升级的新架构提出了几点要求：</p><p></p><p>架构简化：精简架构，统一组件标准，解决不同架构间兼容性问题；尽量采用开源软件，底层逻辑透明化，确保平台升级迭代可控，降低运维成本及难度。混合部署与弹性伸缩：需要满足在线混合部署的使用条件，支持弹性扩容，最大化资源利用率，实现降本增效。实时分析：搭建高性能实时数仓能力，可支持上万超高 QPS、秒级别查询响应，实现数据分析实时化。</p><p></p><p>在上述目标驱使下，招联迅速定位到 Apache Doris 这一开源实时数据仓库 ，Doris 以其简洁的架构设计、丰富的数据接口、高效的查询性能以及低廉的运维成本深得内部认可，可为后续的升级和优化提供强有力的技术支撑。</p><p></p><h2>数仓生态全新升级</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/66373c54db48a5f1d1543c6509837db7.png\" /></p><p></p><p>基于 Apache Doris 的数仓生态相较于旧架构实现了极大的精简。主要变动集中在实时数仓部分，使用 Doris 替代了原先 Clickhouse、Hbase、Kafka、Vertica 等复杂的技术栈。</p><p></p><p>尽管当前架构仍然保留了离线和实时两套处理链路，但在系统设计上实现了高度的代码可复用性，Doris 实时数仓所有代码均可从离线数仓 1:1 复制，以保证两套架构的逻辑一致性和维护便捷性。不仅如此，数据也最大程度在实时及离线数仓中进行了复用，当数据进入实时数仓，经过 DWD 层、DWS 层加工处理后会同时同步到离线数仓中，既提高了数据的时效性，又确保了两套架构数据的一致性。</p><p></p><p>Apache Doris 的引入，不仅大幅简化了数仓生态整体架构，硬件成本也实现约 10% 的降低（如累加开发、运维成本，将有更大比例的节约）。同时，得益于代码和数据的高复用率，架构的运维管理也变得便捷高效。</p><p></p><h2>基于 Apache Doris 的实时数仓</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/57b27421c41a01e223d100cdc90e3112.png\" /></p><p></p><p>具体到实时数仓来说，原先由 Flink、 Kafka 、HBase 应对实时场景，Clickhouse 、Vertica 及部分 Doris 能力应对准实时场景。当前只保留 Flink 进行数据采集，其他组件均替换为 Doris， Flink 采集数据到 Doris 中，经由 ODS、DWD、DWS、DM/APP 层处理后，由 Doris 直接提供查询及分析服务。</p><p></p><p>此外，存储和计算引擎也都统一到 Doris，并通过 CCR 实现 Doris 集群读写分离和数据同步，避免单点压力过大导致系统性能下降，提高了数据查询效率以及系统的稳定性。</p><p></p><p>如何避免数据乱序：</p><p></p><p>Watermark 机制：实时数仓中，Flink 负责将 ODS 中数据消费到 Doris 中，为避免该过程出现数据乱序，可利用 Watermark 机制来容忍数据迟到，确保数据的时效性和正确性。任务串行： 为确保数据的连续性，在调度系统中实现了多批次任务串行机制，上一批次任务未完成时，下一批次就不会开始。同时引入动态窗口机制，每当发起任务时，会自动获取上一批次最新业务节点到此刻时间节点之间的数据，既能保证了批次之间的相互独立，又确保了数据处理的连续性和时效性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/14/1482a339249f9f46a1dc65a753ee1b41.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/98/988e90060ad53ce35550231c4981007e.png\" /></p><p></p><h3>01 客群筛选场景</h3><p></p><p></p><p>在市场营销、风险控制等精细化数据运营中，客群筛选是确认目标人群、制定营销策略的重要手段。</p><p></p><p>在客群筛选过程中，通常需要对集市中多张标签表进行关联计算，大约需要处理 2.4 亿条数据。之前使用 Vertica 计算引擎进行处理时，耗时 30-60 分钟；替换为 Doris 之后，仅用时 5-10 分钟即可完成，相较之前有 6 倍的性能提升。除了显著的性能提升外，Doris 作为一款开源的数据库，无需支付任何许可费用，这与商业化产品 Vertica 相比有着显著的成本优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9f/9f5adda9ff06ae214035c58af5b06647.png\" /></p><p></p><h3>02 高频点查场景</h3><p></p><p></p><p>对于某场景需求，招联需确保系统的 QPS（每秒查询次数）达到 10万次，同时，单次接口响应时间不能超过 60 ms。这意味着，除去网络传输与程序逻辑处理的耗时后，数据查询耗时需控制在 15 ms 内，对系统的性能要求十分严苛。此外，系统还承载着每日庞大的数据更新任务，最大更新量高达 20 亿条，这要求系统不仅能应对高并发，还要确保在高负载下依然能够稳定运行。</p><p></p><p>之前招联使用 Redis 来应对高并发需求，其并发能力和稳定性基本可以满足要求。但 Redis 的核心问题在于使用成本非常高昂。相比之下，Doris 不仅能够支持单节点上万 QPS 的超高并发，也具备大规模数据的快速写入能力，2000 万数据仅需 4 分钟即可写入完成。最为关键的是，Doris 在成本方面展现出非常显著的优势。</p><p></p><p>在处理同等规模的数据量时，Doris 仅需 Redis 1/3 的内存，实现存储成本的大幅降低与效率的显著提升，真正做到了降本增效。</p><p></p><h2>数据传输场景</h2><p></p><p></p><p>从前文可知，依托于 Doris 跨集群数据复制（CCR）能力，已实现 Doris 集群读写分离；另外，因招联内部业务已大范围应用 Doris， CCR 也成为数据库间数据传输的必然选择。</p><p></p><p></p><blockquote>Apache Doris 跨集群数据复制 CCR  能够在库/表级别将源集群的数据变更同步到目标集群，可用于提升在线服务的数据可用性、隔离在离线负载、建设两地三中心等。详情可参考往期技术解析博客：<a href=\"https://www.selectdb.com/blog/112\">跨集群复制功能 CCR</a>\"</blockquote><p></p><p></p><p>从测试数据来可知 CCR 传输效果：</p><p></p><p>存量数据：对于千万级数据，可在几分钟内完成同步；对于亿级别的数据，也可在预期范围内完成，比如 1 亿数据约为 220G，使用 CCR 仅耗时 1500+ 秒（25分钟）。增量数据：增量数据的同步性能则更加优异，千万级增量数据同步 1 分钟内即可完成，亿级别数据同步仅需不到 8分钟。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fb/fbcf638e50d90f2ac01ad9424319aa14.png\" /></p><p></p><h2>经验分享</h2><p></p><p>1. CCR 超时： （TRollbackTxnResult_（{Status:TStatus（{StatusCode:OK ErrorMsgs:[l}） MasterAdd ress:}） ）</p><p></p><p>网络波动存在丢包导致 RPC 超时，为确保网络稳定，可升级 CCR 版本至 2.1.4 版本可支持设置 RPC 超时时间。</p><p></p><p>2. Create table as 语法导致的 slot 一系列问题：</p><p></p><p>2.0 版本在处理 create table as 语句时，采用的是旧执行优化器，而因旧执行优化器为列字段裁剪，普遍存在 slot 相关问题。升级为 2.1 以上版本后，slot 相关问题得以解决；可以创建临时表 xxx，执行 set enable_nereids_dml = ‘true’来规避该问题。</p><p></p><h2>结束语</h2><p></p><p>截止当前，招联金融内部已有 40+ 个项目接入 Apache Doris ，总集群数近十个，集群节点超百个，某集群峰值 QPS 可达 10w+ 。未来，招联还将持续推广 Apache Doris 在内部的使用范围，并将对存算分离、数据湖能力进行探索及应用：</p><p></p><p>存算分离架构：正在探索推进中，未来将尝试基于 Apache Doris 3.0 新版本进行整体架构升级演进，以支持更灵活的弹性部署、降低运维成本。数据湖分析：未来希望借助 Doris 数据湖的能力，统一开发管理工具，满足多源异构数据的存储和分析需求；统一数据访问接口，提升异构数据访问效率；基于丰富数据管理能力，提升数据质量；并将利用 Doris 特性加速数据湖上查询效率。</p>",
    "publish_time": "2024-09-04 15:50:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "易咖智车再获2亿融资，智能工厂落地后将年产5000台L4级无人驾驶车",
    "url": "https://www.infoq.cn/article/MrrGLMo8iCKYUUj9ZSLS",
    "summary": "<p>近日，易咖智车宣布获得2亿元战略投资，本轮融资由上海国和投资、中信建投资本、无锡惠山科创联合完成。据悉，本轮融资将用于智能工厂建设和产能扩容提升。</p><p></p><p>当前中央及地方各级政府加速出台了各类扶持政策和法规，城市服务型自动驾驶车的技术和商业模式已在场景端充分验证，市场端规模化落地需求愈加旺盛。目前已有北京、上海、广州、深圳、杭州等51个城市出台了自动驾驶试点示范政策，五部委确定了20个城市（联合体）为智能网联汽车“车-路-云一体化”应用试点城市，为自动驾驶的规模化应用提供了必要的基础条件和安全的运营环境，支持自动驾驶加速落地。</p><p></p><p>凭借5年多的技术积累和产品力打磨，易咖智车在城市服务型无人车领域积累了资源和经验，累计开发了20多款产品，与60%中游算法合作伙伴深度合作，落地应用覆盖了60余座城市，累计交付超千台无人驾驶车，实际工程化落地车辆累计行驶里程超1000万公里。</p><p></p><p>据悉，易咖智车城市服务机器人南京智能制造工厂将于2025年二季度建成，正式达产后具备单班年产2万台的能力，将率先打造全球领先的L4级无人车柔性自动化生产与沉浸式体验相结合的数字化智能工厂，可为客户贴身定制个性化产线。</p><p></p><p>中信建投资本董事总经理厉辉表示：目前，低速无人驾驶赛道已经过市场较充分验证，政策环境向好，自动驾驶技术趋向成熟，行业正进入加速放量期。易咖智车已与国内物流、环卫、信采等领域多家重点客户建立合作关系，产品可适配多种应用场景，预期订单充足。</p><p></p><p>此次易咖智车与惠山科创集团、惠山高新区共同建设无人车智能工厂，项目落地后预计年产5000台L4级无人驾驶车并支撑1万台无人车的智能运维服务。</p><p></p><p>成立于2018年的易咖智车是IDV智能数字化移动平台科技公司，聚焦L4级自动驾驶线控底盘技术和服务，围绕“全线控+真智能”的“滑板底盘”核心技术，通过构建ADC产品生命池（智能设计引擎+数字化智造+云端数据挖掘），打通用户需求、产品开发、智能智造、智能运维等全环节全生命周期内的信息流、物流、知识流管理，携手AI及科技公司赋能无人驾驶在物流配送、环卫清洁、安防信采、移动充电等加快规模化商业落地。</p>",
    "publish_time": "2024-09-04 16:54:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI 生产力工具暑期迎来大爆发，极光数据：夸克新增用户规模领先",
    "url": "https://www.infoq.cn/article/DoAbIZ6i7x90V9SvPYxo",
    "summary": "<p>9 月 3 日，极光旗下月狐数据发布《AI 生产力工具暑期发展报告》。数据显示，AI 生产力工具在用户侧呈现高速增长态势，总体月活跃用户数量达 1.7 亿。其中，夸克 APP 实现暑期新增用户数量行业第一，凭借大模型、数据、场景等优势，让更多用户享受到一站式 AI 服务。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/ec/e6/ece3227ae1d1cfec9a8eb8bfa3b0c0e6.png\" /></p><p></p><p>报告显示，目前国内生成式 AI 应用快速发展，预计市场规模有望达到 4000 亿元。其中 AI+ 生产力工具赛道的产品数量迅速上升，如 AI 搜索、AI 写作等产品已经进入爆发期。</p><p>&nbsp;</p><p>今年暑期，AI 生产力工具产品在用户新增和用户粘性等数据上屡创新高。6 月，日均用户新增至 194 万。7 月，单日人均使用时长达到 14.3 分钟。极光分析师认为，AI 生产力工具以智能化及自然交互能力，很好地辅助学生进行学习以及帮助职场人群处理各类工作文档。</p><p>&nbsp;</p><p>今年暑期，夸克通过为用户提供一站式 AI 服务和系统级全场景 AI 能力，有效抓住增长机遇，实现新增用户规模领先。今年 6 月高考季，夸克高考 AI 搜索使用量超过了 1 亿次。7 月，夸克升级“超级搜索框”，以 AI 搜索为中心，通过智能问答、智能写作、智能总结，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/f9/8a/f91478ba60e2d077659c916f7f19a38a.png\" /></p><p></p><p>日前，夸克发布全新 PC 端，全面升级 AI 搜索、AI 写作、AI PPT、AI 文件总结等一系列功能。凭借“系统级全场景 AI”能力，无论是在桌面、文档还是网页中，用户都能通过快捷键、划词、截屏、右键菜单等方式，随时唤起夸克 AI 能力，为用户升级AI电脑。</p><p>&nbsp;</p><p>依托 AI 技术，凭借持续进化的产品能力，夸克正在为用户创造全新价值。在搜索速度上，夸克的首字出现速度和吐字速度领先大幅领先同类产品；在写作创作上，夸克可以撰写近 200 种体裁的文稿，在半分钟内能生成一篇高质量的文章；在 PPT 场景中，用户输入主题或选择模板，夸克就能生成一份 25 页左右的专业 PPT；在文件总结上，夸克不惧几十万字的长文和最长6小时的学习视频，通过生成脑图、抽取课件等方式让总结更简单。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/b2/43/b24dba91b8736d0bbfb14e3c4ee83943.png\" /></p><p></p><p>夸克的不断精进也让用户感受到了效率提升。网名为“明白得糊涂”的网友分享到，夸克AI搜索的亮点是会整理全网的搜索内容，特别是针对一些复杂问题，AI 会直接给出回答。用户“雨后彩虹”认为，夸克把搜索体验推上了一个新阶段，各种AI工具非常适合给学生和白领提效。</p><p>&nbsp;</p><p>夸克自诞生之初，就将 AI 技术置于其产品战略的核心位置，定位“智能搜索”获得了年轻一代用户的认可。根据夸克数据显示，作为一个用户过亿的产品，夸克中 25 岁以下的年轻用户占比超过一半。在 AI 大模型进入到比拼应用的新阶段，夸克通过新技术重新定义产品的边界，并朝向 AI 效率助手的方向前进。</p><p>&nbsp;</p><p>此前，根据七麦数据发布的《2024 年第二季度 iOS 实力 AI 产品排行榜》和新榜·AI 产品榜的最新数据显示，夸克无论是访问量还是下载量，都在 AI 产品中占据领先的优势。在苹果应用商店下载总榜和工具榜中，夸克持续保持前列位置。</p><p>&nbsp;</p><p>极光分析师认为，随着用户对效率需求的不断提升，预计行业竞争者的数量仍将上升，更多生产力工具向 AI 化发展、更多科技企业及互联网厂商加码布局。同时，厂商将面向用户需求和效率痛点持续推动产品服务的升级演化，以提升用户使用体验、建立核心竞争力。未来，AI生产力工具将基于AI能力不断丰富服务板块，从满足细分场景发展为覆盖通用需求，打开行业想象空间。</p>",
    "publish_time": "2024-09-04 17:02:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数据库顶会 VLDB 2024 论文解读｜ResLake: 字节跳动多机房资源统一管理系统解析",
    "url": "https://www.infoq.cn/article/RDEiJO7axa25dLNX0f9J",
    "summary": "<p>在字节跳动，每天有数百万的大数据作业在其全球的数十个数据中心运行。由于作业计算和存储资源的不匹配，存在将跨机房带宽用尽的风险，这会影响其他业务的运作，还会造成不同机房的资源负载不均衡。而且跨机房带宽存在成本高、延迟高、稳定性差等问题，会大幅增加作业的运行时长。为兼顾作业完成时间（Job Completion Time, JCT），并均衡不同机房之间资源的负载，字节跳动基础架构计算团队、存储团队、应用研究中心，和系统部网络团队协作，共同研发了多机房计算、存储、网络一体化资源管理系统 ResLake。ResLake 具备资源的全局视角，通过作业调度、数据调度、网络管控等手段，能够显著优化计算和存储的布局，有效降低业务运营成本。ResLake&nbsp;上线后，作业平均 JCT（最小化用户作业完成时间） 时间降低了 20%，机房间资源利用率均衡性提升了 53%，跨机房流量降低了 50%，存储成本降低了 46%。</p><p>论文链接：https://www.vldb.org/pvldb/vol17/p3934-kashaf.pdf</p><p></p><p></p><h2>背景介绍</h2><p></p><p></p><p>大数据作业与数据表之间存在错综复杂的关系（如下图所示），图(a)表明将近 50% 的作业存在跨机房读数据，34% 的从超过1个远程机房读取。图(b)表明将近 50% 的作业从多个表中读取数据，分布存在长尾效应。考虑到数据高可用和单机房容量限制，这些数据通常以多副本形式存储在多个物理机房中，机房之间通过广域网（WAN）连接。</p><p><img src=\"https://static001.geekbang.org/infoq/5b/5be53a08bdad80da8b1b231c42d82bb8.webp\" /></p><p></p><p>对于这种多机房架构，现有的解决方案主要集中在最小化跨机房带宽，以此节约跨机房带宽的成本。究其原因是在公有云环境中 WAN 成本高且带宽有限，极易成为瓶颈。这种资源管理方式存在一定的局限性，它忽略了不同资源之间的联动性，造成机房之间资源负载不均。由下图可见，不同机房之间资源利用率相差达到将近 25%。</p><p><img src=\"https://static001.geekbang.org/infoq/ab/abbe290f4c9410611fbb709368fb837a.webp\" /></p><p>现有的研究表明，计算、存储和网络几乎有相同的概率成为数据密集型作业的性能瓶颈。因此，我们将计算、存储、网络视为三种不同类型的资源，资源管理系统需要统一考虑不同机房多种资源的排布，并进行全局优化。基于以上原因，我们将多机房资源管理系统的设计目标归结为以下两点：</p><p>在作业负载不变和满足 SLO 的前提下，最小化用户作业完成时间（JCT）；在资源总量不变的前提下，最优化资源利用率。</p><p></p><h2>架构设计</h2><p></p><p></p><p>多机房架构下，资源管理系统需要具备全局视角，感知不同机房计算、存储和网络资源的异质性，并为作业和数据的全局排布做出最优决策。为此我们设计了中心化的资源管理系统 ResLake，采用分层架构，具体架构如下图：</p><p><img src=\"https://static001.geekbang.org/infoq/61/613e62516146531ce2410d7f9cecd53b.webp\" /></p><p>控制层：负责与计算层、存储层、网络层进行交互，控制层综合计算、存储、网络资源状态信息，实时对作业布局进行最优调度决策，并指导和反馈其它层进行作业迁移、数据迁移/复制、网络 Quota 调整等，提升跨机房资源的整体利用率。计算层：负责全域计算资源管理 (GRM) 。GRM 主要负责管理不同机房、不同集群的计算资源，并将最新的计算资源状态汇报给控制层，以便做出最优的作业调度决策。存储层：负责全域存储资源管理。根据控制层的决策，存储层可以对数据进行离线调度，例如变更数据副本分布、增加带 TTL 周期的缓存副本、对多副本数据进行压缩，以优化数据排布和节省存储成本。同时，存储层将存储元数据上报给控制层，用于作业数据亲和性调度决策。网络层：负责全域网络监控、网络 Quota 管控、QoS 保障等，并执行控制层下发的网络 Quota 分配和调整指令。同时，网络层将最新的网络状态、剩余带宽 Quota 等信息上报给控制层，以优化作业调度。</p><p></p><p>系统输入：当用户向 ResLake 提交作业时，需要指定作业的计算资源 Quota，如 CPU/Memory 等。网络带宽资源作为系统级资源池由 ResLake 统一分配。考虑到大数据作业通常需要读取大量离线数据，而单机房容量有限，跨机房读几乎不可避免，这类作业不仅消耗大量的跨机房带宽，并且跨机房读取延时高，导致作业 JCT 时间进一步增长。为了解决这个问题，ResLake 要求作业提交时指定读取的路径，路径可以是数据库表分区、文件路径或消息队列的偏移和长度。ResLake 通过数据特征分析，从而无需用户显式指定作业输入数据路径。</p><p></p><p>系统输出：经过 ResLake 决策的作业机房和集群。</p><p></p><p>ResLake 按照调度决策的实时性，分为在线调度和离线优化两种。实时调度侧重于进行轻量化的作业调度，而离线调度侧重于对数据分布进行优化。具体差别如下：</p><p>在线调度作业动态调度：ResLake 提供给用户的是一组虚拟队列，虚拟队列可以跟多个物理资源池关联。ResLake 根据作业读取的路径、数据的副本分布、作业资源需求、作业运行时长、物理资源负载、机房剩余带宽等指标，动态决策作业运行的最优物理资源池；数据懒加载：ResLake 允许为每个机房设置缓存。对于临时查询作业，ResLake 在作业跨机房读取数据时，缓存一份临时副本到本地缓存。从而，将后续的跨机房读取转化成本地读；数据访问特征分析：对于周期性作业，ResLake 通过对作业运行的历史数据进行分析，对作业读取路径等进行预测。对于临时 SQL 查询，ResLake 能提前从 SQL 中解析出输入路径，并根据数据量预估作业运行时长。网络 Quota 动态分配：ResLake 为每个作业分配初始网络 Quota，并对 Quota 进行动态调整，回收作业未使用的 Quota，并分配给其它 Quota 不足的作业。离线优化副本缓存：ResLake 通过分析作业历史访问特征，挖掘多机房访问的热点数据，在周期性作业启动前，将数据缓存到对应机房。为避免不必要的存储成本，我们只会同步读写比极高的数据，并且设置数据的过期时间（TTL）。副本策略优化：ResLake 分析周期性作业的历史数据访问特征，生成数据访问模式，对计算和存储机房错配的副本进行调整。温存推荐：ResLake 通过分析数据的访问行为，推荐业务将不常访问的数据放入温存，从而降低业务的存储成本。</p><p></p><h3>调度模型</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/87341e02797d1cc7d5725e8af3fc6d95.webp\" /></p><p></p><p>我们将作业调度抽象为 5 个 Meta 任务：</p><p>等待调度阶段：任务进入调度队列并等待调度的时间，通常是 ms 级，不实际消耗资源。全局调度阶段：全局调度阶段，根据作业依赖的数据副本分布、预估计算时长、物理资源池负载、物理资源池计算性能、跨机房带宽等指标，为作业找到跨机房读取数据时间最短、且物理资源等待时间最短的机房和集群，调度决策充分考虑了最优化作业 JCT 和资源池负载均衡。调度决策本身耗时也是 ms 级。数据准备阶段：ResLake 为作业寻找满足数据亲和性（计算与大部分数据在同一机房）的机房或者计算资源充足且跨机房带宽充足的机房，ResLake 保证此阶段大部分数据同步到决策机房。主要使用网络带宽，在满足计算数据亲和性时，该阶段耗时可以忽略。集群调度：ResLake 将作业分发到具体集群后，由 YARN/Godel 进行集群内资源调度。该阶段耗时包括等待资源时间和作业运行时间，主要分配计算资源。数据输出：将计算结果输出到存储，供下游计算使用。</p><p></p><p>根据 ResLake 的设计目标，我们将调度抽象为 2 个优化目标：</p><p>最小化 JCT 时间</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/53aaee09ccc74c9091ed5108c32db971.webp\" /></p><p></p><p>其中，&nbsp;为作业在机房&nbsp;&nbsp;的数据量，&nbsp;为集群&nbsp;&nbsp;内资源&nbsp;&nbsp;的处理速度，仅当作业分配到集群&nbsp;&nbsp;时&nbsp;。在调度决策时，ResLake 针对各个阶段预估近似处理时间（APT），预估方式为数据量/资源处理速度。因此，JCT 最小化目标，主要根据数据准备阶段和集群调度阶段的 APT 进行优化。</p><p></p><p>资源负载均衡</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5b/5bea5255cb89ee716dea71549f619de5.webp\" /></p><p></p><p>其中，&nbsp;为集群内资源利用。在进行调度决策时，还需要尽可能考虑全局资源的均衡性，比如当作业有多个可选择物理资源池时，选择集群负载更低的资源池，不但能兼顾集群间负载均衡，避免单个资源成为瓶颈，而且低负载集群往往处理速度更快。&nbsp;</p><p>结合以上两个优化目标，ResLake 针对作业调度问题归结为求解以下优化问题，其中，&nbsp;和&nbsp;&nbsp;为权重，取值在0~1之间，可以根据业务差异调整。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a09d9c3b5c7f5c0e654cd6149524cae.webp\" /></p><p></p><p></p><p></p><h2>系统实现</h2><p></p><p></p><h3>控制层和计算层</h3><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6f/6fe70956decc3aa572541a7d34edb891.webp\" /></p><p></p><p>控制层维护计算、存储和网络资源的全局状态视图，并作出作业调度和全局资源平衡的最优决策。如上图所示，控制层通过 SDK 接受提交的大数据作业。首先，作业将通过计算层实现的虚拟队列权限管理模块（VAM）进行权限校验。接下来，虚拟队列 Quota 管理模块（VQM）根据作业的资源量分配作业请求的资源量。ResLake 实现了统一编排框架 (UOF)，用于作业和队列管理，并于底层的计算、存储和网络层交互，进行调度决策，并将作业分发到对应计算集群。</p><p></p><p>虚拟队列编排：VQM 负责按需编排虚拟队列。对于存在大量跨机房的作业，并且虚拟队列在对应机房没有物理资源池时，虚拟队列管理模块能够为作业在目标分配临时队列，并在作业完成后回收对应资源。</p><p>作业动态调度：作业编排模块根据维护的计算、存储和网络资源的最新视图，并根据分析的作业元数据，如作业依赖数据、作业资源需求等，求解调度模型，决策最优的作业运行机房和集群，并将作业分发到对应集群。</p><p></p><h3>存储层</h3><p></p><p></p><p>提供元数据查询能力：控制层通过离线预测或者 SQL 解析得到作业依赖的数据路径后，通过存储层进一步分析数据所在机房、每个机房副本数、单个副本大小等信息，并利用这些信息进行数据准备阶段的耗时预估。</p><p>具备数据缓存加速能力：为了降低跨机房数据的带宽和延迟，ResLake 离线分析作业所需数据及其访问行为，控制层根据分析结果，在网络低谷期通知存储层发起数据复制/迁移，将数据提前缓存在本地数据中心。</p><p>执行副本重分布：存储层数据洞察服务从存储元数据节点、数据节点等存储组件收集存储指标，进而获取当前数据放置策略、存储空间占用、跨 DC 流量等指标，评估数据迁移的预期资源消耗。并通过数据管理服务实现存储副本迁移能力，实现原理是扫描所有文件块的副本分布，判断是否满足目标分布策略（如跨机房流量最少），利用元数据节点副本修复流程补充目标数据中心对应副本，并利用副本删除机制删除不符合副本放置策略的冗余块，最终实现目录级数据中心副本重分布。</p><p>对数据聚类分析：基于历史访问特征，可以对离线数据进行更为精细的编排和调整，有序地进行迁移，达到减少跨机房流量的目的。根据计算任务与数据的依赖关系，我们以计算任务与数据路径为顶点，查询任务与数据路径之间的流量为边，构建一个有向无环图（DAG）。上述问题可以概括为将此 DAG 划分为若干个子图，使得跨子图的边权重之和最小。每个子图中的数据路径放在相同机房中，因此跨子图的边就是跨机房的流量。我们使用混合整数线性规划（MILP）来解决数据聚类问题。通过数据聚类，我们能够将关联性强的表放到相同机房中，以减少跨机房数据访问。</p><p></p><p></p><h3>网络层</h3><p></p><p></p><p>执行初始配额分配：网络层的网络 Quota 管理模块（NQM）在作业提交时执行初始配额分配，并动态管理网络 Quota。对于周期性作业，我们将初始 Quota 分配为作业在最后 n（n&gt;=3）次运行中的平均带宽使用量。对于临时作业，ResLake 为作业分配一个默认 Quota 值。</p><p>动态管理网络 Quota：根据当前带宽水位为每个集群设置 Quota 回收带宽策略参数。Quota 管理策略根据剩余 Quota 和计算集群优先级进行差异化调整。当剩余 Quota 水位较高时，将优先为高优集群分配 Quota，并回收低优集群的 Quota。</p><p></p><h3>效果验证</h3><p></p><p>ResLake 上线后，作业平均 JCT 时间下降了 20%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b34dc3b013154e51a68cb58c7fa6ee1b.webp\" /></p><p></p><p>ResLake 将 CPU 利用率均衡性提升了 80%，内存利用率均衡性提升了 53%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cd/cdf10d1f86763eb0e3c1ea6b35d4b057.webp\" /></p><p></p><p>ResLake 将跨机房流量减少了 50%。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d4a4343caef566f08f72669d523e0ea3.webp\" /></p><p></p><p>ResLake 通过推荐数据进入温存，节省了 46% 的存储成本。</p><p></p><h2>总结</h2><p></p><p>针对多机房架构，通过设计多机房统一资源管理系统 ResLake，减少作业完成的同时实现不同机房之间的资源均衡。ResLake 具备计算、存储和网络的全局视角，能够全局优化资源的最优排布问题。在 ResLake 的实现上，控制层与底层计算、存储和网络层协调，以确保各种在线和离线机制的有效性。并如上文所介绍的，在字节跳动的生产实践中验证了自 ResLake 部署以来，大数据作业平均作业完成时间明显降低、资源均衡性得到显著提升，并且跨机房流量和存储成本大幅下降。</p><p></p><p>作者信息：张鑫春，字节跳动基础架构工程师。电子科技大学硕士，曾就职阿里、百度，目前在字节跳动专职于大数据资源管理系统研发工作。</p><p></p>",
    "publish_time": "2024-09-04 17:21:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "韩国“N号房”事件因 Deep Fake 再现，受害者向中国网友求救",
    "url": "https://www.infoq.cn/article/umU7XiVP0CIXWqIzeXzb",
    "summary": "<p>整理 | 华卫、核子可乐</p><p></p><p>视频 AI 生成技术的最新进展，已经开始产生一系列令人忧心的社会影响。不少精通这些技术的年轻人被发现，正在未经他人同意的情况下制作女性（大多是自己同龄人）的色情图像。</p><p></p><p>8 月 28 日，多位韩国女性在中国社交媒体平台发帖讲述“深度伪造”（Deep Fake） AI 换脸的大规模犯罪事件并向中国女性求救，引发大量关注与讨论，多个相关词条接连登上热搜。根据其描述，韩国长期发生产出和传播深度伪造色情内容的犯罪行为，而即时通讯软件 Telegram 因其加密通信和群组聊天功能受到这些犯罪者的青睐。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c5/c59b4cd9eb21c527c743820590a16d30.jpeg\" /></p><p></p><p>而早在今年 5 月，就有外媒报道，韩国首尔大学毕业生朴某和姜某，从 2021 年 7 月至 2024 年 4 月，涉嫌利用深度伪造技术换脸合成色情照片和视频并在 Telegram 上私密传播，受害女性多达 61 人，包括 12 名首尔大学学生。其中，主犯朴某（40 岁）制作了 1852 份换脸色情照片及视频，传播了约 100 条视频。</p><p></p><p>据悉，受害女性曾不断遭到相关 Telegram 群聊参与者威胁，后坚持申诉两年半得到上述调查结果。当时，韩警方相关人士称：“朴某等主犯并未通过非法合成物赚钱。他们的犯罪目的不是为了营利，而是为了满足自己的欲望。”</p><p></p><p></p><h1>一张深度伪造裸照 0.48 美元，盈利结构表明市场需求量</h1><p></p><p></p><p>所谓色情深度伪造，就是利用 AI 技术生成数字篡改的图像，且此类行为正在 Telegram 消息应用之上广泛传播已久。其中某 Telegram 频道已经拥有超 22 万名成员，能够按需提供深度伪造的裸照图像。该频道只需最简单的线上搜索就能轻松找到，通过收取费用根据成员提交的真实人物照片生成伪造图像。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/0e/0eb8a747c965db3063d8b8101b35de90.png\" /></p><p></p><p>超过 220000 名成员的 Telegram 聊天室内部的屏幕截图</p><p></p><p>8 月 21 日，《韩民族日报》通过 X 访问到该 Telegram 频道，其中设有图像机器人，能够将上传至频道中的女性照片转换为深度伪造裸照。加入该频道后，系统会弹出一条提示消息，要求用户“上传一张你喜欢的女性照片”。在按提示上传一张 AI 生成的女性照片之后，五秒之内该频道就生成了相应的深度伪造裸体图像，甚至允许用户自定义所生成图像中的各个身体部位。</p><p></p><p>该频道最多可免费生成两张深度伪造裸照，之后需要付费才能获取更多图片。每张照片费用为一颗“钻石”——售价为 0.48 美元或者 650 韩元。用户每次至少需要购买 10 颗钻石，但批量购买可以享受折扣。邀请朋友加入频道的用户也会免费获得钻石，这显然是为了扩大用户规模。</p><p></p><p>并且，该频道仅接受加密货币的付款方式，这很可能是出于保持匿名的考量。另外，该聊天室不允许用户之间相互发送消息或者图片，因此无从得知其他成员如何使用该频道生成深度伪造图像。</p><p></p><p>截至 8 月 21 日，该 Telegram 频道已拥有约 22.7 万名用户。由于 Telegram 在世界各地开放，因此聊天室中可能包含来自不同国家的成员。值得一提的是，该 Telegram 频道的访问门槛极低，在 X 和其他社交媒体上搜索特定关键字即可找到频道链接，其中一条带有链接的帖子甚至成为 X 上的“热门帖”。</p><p></p><p>尽管近期有报道称警方已经就其深度伪造罪行展开了调查，但用于推广该频道的帖子仍时有出现。专家们表示，该频道的存在就是深度伪造色情内容的现实缩影，而且很多人甚至根本不清楚这样的行为涉及犯罪。</p><p></p><p>韩国女律师协会人权事务主任 Seo Hye-jin 表示，“像这样在 Telegram 聊天室中制作性剥削及深度伪造类型的图像，并在其他群聊室中分享，正是当前诱发大规模性犯罪的根源所在。”在她看来，“如果单在图像制作阶段，相关 Telegram 聊天室中就有超过 22 万成员，那么后续传播造成的损失恐怕会非常巨大。”</p><p></p><p>而如此庞大的 Telegram 频道及其创收模式的稳定运行，很可能反映出一个可怕的现实：很多人认为制作这种极具危害性的深度伪造图像，其实只是无足轻重的小事。</p><p></p><p>“当前盈利结构的存在，表明市场的需求量很大。”韩国性暴力救济中心主任 Kim Hye-jung 总结道，“尽管对女性的性侮辱已经成为一种网络‘内容’类型，但社会对此类罪行的普遍轻视亦成为诱发性犯罪的一大关键因素。”</p><p></p><p></p><h1>深度伪造犯罪蔓延，因滥用处罚“轻”？</h1><p></p><p></p><p>近年来，韩国时常有一系列备受瞩目的数字性犯罪案件发生。韩国警方表示，今年迄今他们受理的深度伪造性犯罪案件数量已飙升至 297 起。最近一份报告显示，2023 年韩国与深度伪造图像有关的犯罪案件已高达 180 起。2022 年韩国共发生 156 起深度伪造犯罪案件，被定罪者中有 61% 为青少年。</p><p></p><p>情况之所以恶化，主要是因为韩国的监管框架漏洞百出。其法律规定，成年人若以违背他人意愿的方式，以引起性羞辱为目的处理或编辑他人的伪造视频、音频或照片内容且意图传播，可被判处最高五年监禁、或者最高 5000 万韩元的惩罚。但事实上，实际惩罚往往要比法条规定要宽松得多。</p><p></p><p>即使有人制作了深度伪造的性剥削视频，他们也很少面临监禁，仅仅拥有此类内容也不会受到惩罚。要追究制作或拥有深度伪造视频者的责任，必须证明其传播意图，这使得许多人可以轻易逃避法律制裁。法律还规定，犯罪者必须具有主观传播的意愿，意味着此类制作深度伪造色情内容的行为尚处于灰色地带。</p><p></p><p>《韩民族日报》曾对 46 起涉及伪造视频的法庭判决进行了分析，结果显示，在仅因传播伪造视频而遭到起诉的 18 人中（不包括同时被指控犯有其他罪行的嫌疑人），只有 1 人被判入狱、2 人获缓刑、2 人被罚款。在其中一案中，某年轻男子篡改了自己十几岁表弟的照片并将其通过手机通讯应用分享。初审判处其两年监禁，但上诉法院最终将刑罚改为缓刑。</p><p></p><p>相比之下，英国司法部去年 4 月宣布，无论是否传播，只要制作了深度伪造色情内容都将受到惩罚，还封锁了两个主要的深度伪造色情网站。</p><p></p><p>此外，韩国与深度伪造有关的犯罪数量及青少年犯下此类罪行的比例，都呈现出上升趋势。根据韩国国民力量党代表赵恩熙（Cho Eun-hee）撰写的相关报告，在因此类罪行而受到惩制的 120 人中，有 91 人为青少年（占比 75.8%）。</p><p></p><p>8 月 21 日，釜山市教育局表示有 4 名中学生因使用深度伪造技术对 18 名学生和 2 名教师的面部进行数字复制而受到警方调查。这些年轻的嫌疑人共制作了约 80 张受害者的色情图像，并通过移动通讯应用对外分享。2023 年，釜山共出现 12 起由学生传播同学深度伪造色情图像的案件；而到 2024 年，仅上半年就发生了 15 起同类案件。在济州岛，警方最近抓获了一名十几岁的国际学校学生，他曾使用至少 11 名同学的面部制作深度伪造色情内容。</p><p></p><p>赵恩熙表示，“这些给受害者造成不可逆转伤害的数字犯罪行为，正在青少年群体当中蔓延，很多人将甚至将其当作一种游戏。”她还强烈呼吁通过系统性法条修订来防范此类犯罪。深度伪造犯罪具有多种形式，这些图像有些被用于霸凌受害者，有时也纯为追求经济利益。</p><p></p><p>据悉，韩国青少年获取人工智能（AI）服务的门槛相当低。今年 5 月，韩国国家信息社会局发布的一项针对 2261 名青少年的调查显示，韩国约 77.5% 的青少年表示他们听说过生成式 AI，且有超过半数（52.1%）的青少年表示他们使用过该项技术。</p><p></p><p>尽管越来越多的青少年因违法使用深度伪造技术而受到惩罚，但大多数判例相对较为宽松。部分原因在于，针对未成年人的惩制力度本身就更轻。</p><p></p><p>正因为如此，犯罪专家才强调，政府应当采取措施教育学生，确保他们意识到这类犯罪行为的严重性。京畿大学犯罪心理学教授李秀贞（Lee Soo-jung）表示，关于编码等计算机技术的课程，必须同时兼顾法律和道德等应用约束内容。她强调称，这种教育必须从孩子抓起，因为年龄较小的孩子更容易将此类犯罪视为无伤大雅的恶作剧。</p><p></p><p></p><h1>结语</h1><p></p><p></p><p>专注于身份盗窃保护的美国初创公司 Security Hero 2023 年关于全球深度伪造的报告显示，韩国是深度伪造色情内容最多的国家，其中歌手和女演员占此类深度伪造内容中个人的 53%。</p><p></p><p>除了敦促社交媒体公司更积极地配合删除和屏蔽此类内容外，韩国媒体监管机构还要求法国当局在 Telegram 相关问题上定期合作，并为与 Telegram 的直接沟通提供便利。在韩国打击对性深度伪造行动的同时，Telegram 的创始人帕维尔·杜罗夫 （Pavel Durov） 在法国接受正式调查，法国当局正在调查该消息应用程序上的有组织犯罪。</p><p></p><p>此外，韩国政府表示，将推动更严格的法律，把购买或观看性剥削深度伪造定为犯罪行为。韩国警方还计划进行为期 7 个月的打击数字性犯罪，并增加监控此类事件的监管人员数量，为受害者建立 24 小时热线。Telegram 也在一份声明中表示，将积极审核其平台上的有害内容，包括非法色情内容。</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://www.straitstimes.com/asia/east-asia/deepfake-sex-crimes-widespread-among-teens-in-south-korea\">https://www.straitstimes.com/asia/east-asia/deepfake-sex-crimes-widespread-among-teens-in-south-korea</a>\"</p><p></p><p><a href=\"https://english.hani.co.kr/arti/english_edition/e_national/1154909.html\">https://english.hani.co.kr/arti/english_edition/e_national/1154909.html</a>\"</p><p></p><p><a href=\"https://www.msn.com/en-xl/news/other/editorial-53-of-global-deepfake-victims-are-korean-and-politicians-are-to-blame/ar-AA1pMBtl?ocid=BingNewsSerp\">https://www.msn.com/en-xl/news/other/editorial-53-of-global-deepfake-victims-are-korean-and-politicians-are-to-blame/ar-AA1pMBtl?ocid=BingNewsSerp</a>\"</p><p></p>",
    "publish_time": "2024-09-04 17:26:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "NASA 程序员在深空调试 Lisp的故事",
    "url": "https://www.infoq.cn/article/Vgwt1soyEzzFXX0ISqM9",
    "summary": "<p>值得庆幸，我们大多数人永远不必在1.5亿英里之外调试软件。前美国宇航局（NASA）程序员、软件工程师罗恩·加雷特（<a href=\"https://rongarret.info/\">Ron Garret</a>\"）在亚当·戈登·贝尔（Adam Gordon Bell）的<a href=\"https://corecursive.com/\">Corecursive</a>\"播客的最近一期节目中，分享了他在深空飞船任务中诊断Lisp软件故障的经验。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e9f5e7370ff3384fe82ba7d0809e190b.png\" /></p><p></p><p>&nbsp;</p><p>加雷特分享了在深空调试的<a href=\"https://corecursive.com/lisp-in-space-with-ron-garret\">非凡故事</a>\"——以及早期编程的回忆，在这个过程中，加雷特对编程世界发生的变与不变提出了令人耳目一新的视角，还探讨了为航天器编写代码的独特经验。&nbsp;</p><p>&nbsp;</p><p>他想起了在Lisp历史上一个真正辉煌的时刻所扮演的角色。</p><p>&nbsp;</p><p></p><h2>超能力用户&nbsp;</h2><p></p><p>从1988年到2000年，加雷特一直在NASA的喷气推进实验室<a href=\"https://flownet.com/ron/resume.html\">担任研究科学家</a>\"，在2001年到2004年间又担任了一次。加雷特的专长是自主移动机器人，他帮助开创了当今自主移动机器人控制架构的标准。&nbsp;</p><p>&nbsp;</p><p>加雷特的团队致力于火星探测车“旅居者”(<a href=\"https://en.wikipedia.org/wiki/Sojourner_(rover)\">Sojourner</a>\"）的机器人原型。&nbsp;</p><p>&nbsp;</p><p>在播客中，加雷特描述了在1988年非常有限的编程选项——那时候还没有Java、Python、JavaScript，甚至是C++。“就流行的语言来说，有Pascal、C、Basic和机器码，用这些语言中的任何一个完成任何事情都非常、非常困难。”大多数航天器的代码最终都是用汇编语言编写的。</p><p>&nbsp;</p><p>后来出现了Lisp——一种将问题清晰地抽象为列表和函数的语言，虽然C程序员担心悬空指针这类的事情，但Lisp有自动内存管理。“当你使用的语言提供了一些高层次的抽象概念时，完成工作要快得多，也更容易。”加雷特道，“在一个只有Lisp语言具备这种能力的世界里，掌握Lisp真的就像一种超能力。“</p><p>&nbsp;</p><p>使用Lisp，“每个问题都变成了编译器的问题”</p><p>&nbsp;</p><p>“在那个年代，它简直把其它东西都比下去了。”</p><p>&nbsp;</p><p>但那时Lisp并没有被NASA广泛使用。</p><p>&nbsp;</p><p>&nbsp;“对Lisp有相当多的偏见，因为它很奇怪，也不熟悉，还有奇怪的垃圾回收技术，你永远不知道它什么时候把你的进程停止了。” 加雷特道。</p><p>&nbsp;</p><p>加雷特的团队发现它对于内存受限的硬件很管用，Lisp可以对问题专门设计一种定制语言，然后针对机器人的硬件来编译它。正如贝尔所说，“每个问题都变成了编译器的问题。”加雷特的团队在机器人模拟器（在一台Macintosh电脑上）上煞费苦心地编写和测试了他们的代码，然后将其安装在实际的探测器上，并在阿罗约号（Arroyo）上进行了一次耗时的试驾。</p><p>&nbsp;</p><p>尽管团队开发好了，但是当旅居者（Sojourner）探测器到达火星时，它使用的还是C代码。&nbsp;</p><p>&nbsp;</p><p>后来，在1998年，NASA的一位新主管启动了 “<a href=\"https://en.wikipedia.org/wiki/New_Millennium_Program\">新千年项目</a>\"”——通过一系列深空探索任务尝试不同（更便宜）技术的试点项目。&nbsp;</p><p>&nbsp;</p><p>这意味着他们的Lisp代码获得了第二次生命，加雷特回忆道，团队为探测器开发的自主技术将被重新使用，它的新使命是飞行控制器。&nbsp;</p><p>&nbsp;</p><p>加雷特的团队开发了一种新型决策软件——用Lisp专门设计的定制语言来避免可怕的“竞争条件”（即两个并发运行的线程争夺相同的内存空间）出现的可能性。“经过大量反复的测试”——用的是和进入太空完全相同的硬件，“所以我们非常有信心它会成功。 ”</p><p>&nbsp;</p><p>“然而没有成功……”</p><p>&nbsp;</p><p></p><h2>深空故障</h2><p></p><p>加雷特解释说，在他们为期三天的飞行控制中，“它正常运行了一段时间，做了应该做的，后来突然失效了，警钟响起…...”</p><p>&nbsp;</p><p>“现在，这段证明没有死锁的代码似乎在离家1.5亿英里的地方被冻结了。“</p><p>&nbsp;</p><p>局势很紧张，“我们不知道发生了什么……能做的就是当决定下来尽其所能，我们坐等了一小时。”在会议室里，团队达成了共识，他们的命令“经过了多级管理层的审查，所有人都必须在上面签字”。</p><p>&nbsp;</p><p>在获得批准后，指令通过专用线路发送到深空网络一个70米宽的天线上，再以光速在太空中飞行……”</p><p>&nbsp;</p><p>首先，他们请求了一个回溯——常见的操作，生成当前所有活动进程的列表（正如加雷特所述，“它们正在等待什么”)。</p><p>&nbsp;</p><p>“实际上，几乎是立即看出了问题，因为有一个进程在等待本应该要发生的事……“</p><p>&nbsp;</p><p>罗恩翻出了这些非常酷的照片：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/79f52b1c5b9d6c6d12106497708c4156.jpeg\" /></p><p></p><p>&nbsp;</p><p>“问题在于，这个竞争条件原本是不可能存在的。”然而加雷特团队的一位程序员调用了一个较低级别的Lisp函数——无意中为他们精心定制的语言创建了“安全保障的终端”（加雷特责怪自己没有向程序员解释清楚) 。</p><p>&nbsp;</p><p>团队决定“手动”触发事件——让软件重新运行。&nbsp;</p><p>&nbsp;</p><p>&nbsp;“我们没有失去航天器，也完成了所有任务——从技术上说，这是一次成功。” 加雷特在播客上道，“但过程是如此的艰辛，困难重重——而且，还有政治。尽管我们确实让它工作了，但之后自治项目被取消了，它再也没有飞行过。”</p><p>&nbsp;</p><p><a href=\"https://flownet.com/gat/jpl-lisp.html\">2002年</a>\"加雷特个人网站上的一篇文章认为“Lisp在喷气推进实验室（JPL）的消亡是一个悲剧，该语言特别适合于常见的软件开发：必须在非常紧张的预算和计划下开发独一无二、高度动态的应用程序。”&nbsp;</p><p>&nbsp;</p><p>但是Lisp在C++、Java中被忽略了，究其原因是试图遵循“最佳实践”，加雷特认为，“我们混淆了最佳实践和标准实践，两者是不同的。”除此之外，“最终最好”并不是一个一成不变的标准，应取决于当前项目的具体情况。</p><p>&nbsp;</p><p>在Hacker News的一次讨论中，一位称自己是NASA工程师的评论者，曾在2009年探索月球南极任务中担任有效载荷软件工程师，并<a href=\"https://news.ycombinator.com/item?id=31253704\">表示他们使用Lisp编写了自己的定制语言</a>\"，用于仪器命令序列（以及模拟计算机）。“Lisp简单、灵活的语法和宏使<a href=\"https://blogs.nasa.gov/lcrossfdblog\">表达命令和计时模式</a>\"变得更容易 。“&nbsp;</p><p>&nbsp;</p><p>他们消除了加雷特的疑虑:“我认为Lisp仍在NASA的各个角落中使用。”</p><p>&nbsp;</p><p>原文链接：<a href=\"https://thenewstack.io/nasa-programmer-remembers-debugging-lisp-in-deep-space\">NASA Programmer Remembers Debugging Lisp in Deep Space</a>\"</p>",
    "publish_time": "2024-09-04 17:29:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "前端策略：使用框架还是纯JavaScript？",
    "url": "https://www.infoq.cn/article/FZuO08AqzpBhkSFAQdr2",
    "summary": "<p>有大量的前端框架都是用 JavaScript 编程语言实现的。然而，有时很难确定应该将哪种框架应用于新项目。或者 Web 平台的状态就是这样的，我们根本不需要使用任何框架。</p><p></p><p>框架为网页设计师提供了必要的基本功能。同样，在某些情况下，设计师也可以只使用 JavaScript 即可解决问题。</p><p></p><p>前端框架是一个代码包，用于解决常见的问题，比如数据绑定和检索、路由、代码组织以及当代应用程序所使用的其他功能。一般来说，前端框架是快速实现和 Web 开发的重要工具。</p><p></p><p>前端框架有助于构建和控制代码，它可以处理一个合理的应用程序在客户端所需的一切。</p><p></p><p></p><h2>何时使用前端框架</h2><p></p><p></p><p>要确定我们是否需要使用某个框架，请问问自己：“在没有这个框架，我们是否可以构建它？”如果任务相当简单，并不需要使用数据获取、绑定或复杂的路由，那么可能就不需要框架了。</p><p></p><p>在这个程序中，一些开发人员打开一个空白 HTML 文件，并通过绘制板对整个项目进行编码。这样的项目适用于个人网站或作品集，此时，不使用框架可能会是一个好主意，因为在这种情况下，没有特定的需求，并且结果也会相当快。</p><p></p><p>然而，在某些情况下，使用框架将会带来巨大的收益。</p><p></p><p></p><h3>前端 JavaScript 框架的优势</h3><p></p><p>下面将介绍使用 JavaScript 框架进行前端开发以提高网页设计技能的优势。</p><p></p><h4>简化的语法和结构</h4><p></p><p>JavaScript 框架将开发模式定义为使用已编写好的代码使语法更简单。这种结构为开发人员提供了一种采用框架并构建应用程序的简单方法，而无需编写构成主框架之外的很多额外代码。Angular、React 和 Vue.js 等库提供了许多规则和建议，可以使工作更加高效，并能最大限度地减少团队新成员研究代码所需的时间。</p><p></p><h4>模块化的代码组织</h4><p></p><p>框架通过将应用程序划分为小而可互换的部分，使编写模块化的代码变得更加容易。这种模块化有助于开发人员编写良好的结构化代码，并有助于团队成员之间顺利地进行协作。由于每个组件都可以单独构建、测试和维护，因此可以为维护提供更干净的代码库。</p><p></p><h4>可重用的组件</h4><p></p><p>JavaScript 前端框架还可以帮助我们开发模块——这是另一个优势。这些组件可以在一个应用程序中重复使用，甚至可以用于另外的其他项目。这种可重用性减少了项目开发所需的时间，标准化了编写的代码主体，并消除了冗余。</p><p></p><h4>丰富的生态系统和库</h4><p></p><p>JavaScript 框架已经开发展成了复杂的生态环境，其中充斥着能与框架一起使用的库、工具和插件。上述资源能为开发人员提供非常重要的帮助，因为他们可以使用现成的特性和功能来增强他们的应用程序。例如，在 React 中，没有标准的方法来管理状态、路由应用程序或处理表单。然而，有很多三方库可以解决这些问题，并能很容易地将这些解决方案合并到应用程序中。</p><p></p><h4>虚拟 DOM 与高效的渲染</h4><p></p><p>Preact、Next.js 和 Vue.js 等库使用虚拟文档对象模型（DOM）来提高渲染速度。虚拟 DOM 是实际 DOM 之上的一层，很少直接操纵应用程序的 DOM。这种方法减少了昂贵的 DOM 操作次数，使更新在处理增强和动态应用程序时更快、更高效。</p><p></p><h4>代码拆分和延迟加载</h4><p></p><p>JavaScript 框架支持更复杂的策略，比如代码拆分和延迟加载，从而提高了应用程序的效率。代码拆分有助于将应用程序代码分解为可以按需加载的片段，这反过来又有助于最大限度地减少加载时间。延迟加载还可以通过仅在需要时加载所需的组件和资源来提高工作效率，从而增强了应用程序的可用性。</p><p></p><h4>标准化开发实践</h4><p></p><p>应用 JavaScript 框架通过实现一组特定的标准可以帮助整个团队做好准备工作。这些框架附带了一些规则和标准，并推荐了对特定项目进行编码和结构化的方式或方法。到开发后期需要查看代码时，能使代码对开发人员来说更易于阅读和维护。</p><p></p><h4>对不同规模项目的适应性</h4><p></p><p>JavaScript 框架是为了实现多功能性而开发的，既可以用于大型企业，也可以用于复杂程度相当的小型企业。这些框架适用于任何应用程序——只有一个页面的简单应用程序或具有多级复杂性的大型企业的应用程序。对于开发人员来说，可以简单地开始并逐步扩展应用程序，而无需经历巨大的架构转换。</p><p></p><h4>与现代开发工具的集成</h4><p></p><p>当前的 JavaScript 框架非常灵活，可以很容易地集成到不同的开发工具和开发生命周期中。它们可以支持构建工具和包管理器，比如 npm 和 Yarn。它们还可以通过消除繁琐的活动、协调依赖关系以及编排构建过程来丰富开发过程，从而提高工作效率。</p><p></p><h2>结论</h2><p></p><p>JavaScript 是 Web 开发的未来。一切都取决于所选的框架以及云托管的专门功能和选项。JavaScript 可用于客户端脚本和服务器端或游戏服务器脚本的编写或套接字编程。然而，由于浏览器只支持一种语言，因此 JavaScript 已经赢得了它的普及，广受欢迎。</p><p></p><p>作者介绍：Jordan Smith 是俄克拉荷马州塔尔萨的一位资深网页设计师，在设计和开发网站方面拥有超过 10 年的经验。他建立了数百个专门针对每个客户的品牌和目标量身定制的网站。</p><p></p><p>原文链接：</p><p>https://thenewstack.io/frontend-strategies-frameworks-or-pure-javascript</p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p>",
    "publish_time": "2024-09-04 17:52:58",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "爱彼迎如何平滑升级 React",
    "url": "https://www.infoq.cn/article/PjF8aA10vNt5K1qP6gSj",
    "summary": "<p></p><blockquote>导读：想象一下，在不中断服务的情况下，将一个庞大的在线平台的核心技术框架升级到最新版本，听起来是不是像是一场技术界的“心脏移植手术”？爱彼迎就成功地完成了这项任务，而且没有引发任何服务降级。在这篇文章中，我们将一窥爱彼迎如何利用他们独创的 React 升级系统，逐步推出 React 的新版本，同时收集反馈和学习经验。文章不仅分享了升级过程中的哲学思考、系统设计，还总结了从这一升级中学到的宝贵教训。无论你是前端开发者、技术团队的领导者，还是对技术升级过程充满好奇的读者，这篇文章都将为你提供深刻的见解和实用的策略，帮助你在自己的项目中实现平滑的技术升级。&nbsp;如果你对如何在保持服务稳定性的同时引入新技术感兴趣，或者想知道大型技术团队如何应对升级挑战，那么这篇文章绝对值得一读。让我们一起探索爱彼迎在 React 升级之旅中的策略和实践吧！</blockquote><p></p><p></p><h2>引言</h2><p></p><p>爱彼迎的前端团队近期迎来了一项重要成就：我们成功地将所有网页界面从 React 16 升级至最新的 React 18 版本。这一壮举对于一个涵盖用户页面、房东页面及众多内部工具等多元界面的产品而言，无疑是一项浩大的工程。为了确保升级过程的安全与顺畅，我们精心打造了一套 React 升级系统 —— 这是一项可复用的基础设施，它助力我们在 monorepo 环境中逐步部署新版 React，并实时监测与评估升级效果。本文中，我们将深入探讨我们的升级策略、所构建的升级系统，以及在这次升级旅程中所积累的宝贵经验。</p><p>&nbsp;</p><p>尽管本文聚焦于 React 的升级实践，但我们所构建的系统及所汲取的经验教训，同样适用于那些需要定期进行更新的众多 Web 框架与库。</p><p></p><h2>升级的挑战</h2><p></p><p>在长期运行的项目中，升级依赖项是一项常见且至关重要的任务。它不仅能修复安全漏洞、提升系统性能，还能解锁新的 API 功能。然而，尽管部分升级过程相对直接简单，但当项目中的大量代码深度依赖于更新后的 API 或是对某些行为有细致入微的预设时，升级的难度便会骤然上升。在爱彼迎的 Web monorepo 环境中，我们坚持一个原则：除了极少数特殊情况外，每个顶级依赖项只允许保留一个版本，且整个代码库根目录下仅设有一个package.json文件。这一做法确保了 monorepo 内部代码的兼容性和一致性，有效避免了向用户推送重复包包的问题。但在引入升级系统之前，维护这一单一版本依赖的策略意味着每次升级都需进行大规模的原子性更新，这不仅要求大量的前期迁移工作，还需长时间维护一个专门的升级分支，直至最终一次性部署给用户，实现一个关键的里程碑。这样的过程不仅风险高、易出错，还需要巨大的工程投入来保障其顺利进行。</p><p>&nbsp;</p><p>理想状态下，我们期望能够实施小规模、无障碍的增量升级。但在没有一套能够在大型 monorepo 中逐步测试并推广的系统支持下，我们往往不得不经历多次升级尝试，并在遇到问题时回退版本。尤其是面对性能回归这类问题，现有的升级策略在监测和预防上显得力不从心。由于缺乏在全面部署前收集性能数据的有效手段，我们往往只能采取“一刀切”的方式，直接从 0%部署到 100%，这一过程中充满了不确定性和潜在风险。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6d/6db8368ae08cf7bd0131600663f57d13.png\" /></p><p></p><p>上图直观地展示了我们在 React 主版本和次版本升级过程中的理想与现实差距。</p><p>&nbsp;</p><p>因此，我们设计 React 升级系统的核心目标，就是将这一复杂且充满挑战的升级过程变得更加顺畅，将其从一项“英雄式”的任务转变为日常工作的常态。具体而言，我们的目标包括：</p><p>&nbsp;</p><p>逐步升级：通过分阶段升级，我们可以更早地获得反馈，及时调整策略并总结经验教训。高频迭代：保持升级的频繁性，确保当前版本与目标版本之间的差距始终保持在可控范围内。升级测试：通过精确测量升级对性能的具体影响，依托数据指导我们做出更加明智的升级决策。</p><p>&nbsp;</p><p></p><h2>React 升级系统的设计</h2><p></p><p>&nbsp;</p><p>基于上述目标，我们逆向思考，逐步勾勒出理想的系统架构蓝图。我们力求避免采用长期运行的升级分支，转而追求逐步升级的策略；同时，我们寄望于通过 A/B 测试，从生产环境中直接收集用户反馈，以此为依据来指导最终的发布决策。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a5/a503d847cda2a3d27c9e4aa404bce074.png\" /></p><p></p><p>理想升级系统的简化示意图</p><p>&nbsp;</p><p>然而，在系统初步构建的过程中，我们遭遇了几个亟待解决的关键问题：首先，需要选定一个 React 版本来负责应用的渲染工作；其次，如何在运行时动态地在两个版本间进行无缝切换，也构成了不小的挑战。下面是一个采用这种简化方法渲染基础应用程序的代码示例：</p><p>&nbsp;</p><p><code lang=\"null\">import React18 from 'react'; \nimport React16 from 'react'; // duplicated import?\n\n\nif (shouldEnableReact18()) {\n  const root = React18.createRoot(container);\n  root.render();\n} else {\n  React16.render(, container);\n}</code></p><p>&nbsp;</p><p>这里涉及两个核心问题：</p><p>&nbsp;</p><p>避免框架包体积膨胀：我们极力避免在应用程序中同时打包两个版本的 React，因为这会导致框架包的体积翻倍，增加用户的下载负担。此外，构建过程中可能需要调整 JSX 的转换规则，这有可能导致我们的组件与某个版本的 React 不兼容，进一步复杂化开发和维护过程。导入路径的混淆：另一个挑战在于如何明确指定 React 的导入路径。通常，“react”依赖项会指向特定版本的 React，但无法同时指向两个不同版本。这会造成版本冲突和不确定性。</p><p>&nbsp;</p><p>为了有效解决上述问题，我们采用了模块别名（Module Aliases）的策略来区分不同版本的 React，并结合环境变量来控制构建和运行这两个独立的 React 版本。</p><p></p><h2>模块别名（Module Aliases）</h2><p></p><p>我们利用模块别名技术解决了导入路径的混淆问题。在使用 yarn 作为包管理器的项目中，我们在package.json文件中添加了一个额外的 React 依赖项，例如：</p><p>&nbsp;</p><p><code lang=\"null\">\"react-18\": \"npm:react@18\"</code></p><p>&nbsp;</p><p>通过这种方式，我们能够从react-18包中导入 React，这在一定程度上缓解了问题。由于许多工具（如自定义解析器和构建系统）需要明确知道使用的 React 版本，我们将所有自定义工具接入到一个集中的“全局别名”配置系统中。这一配置允许我们在一个中心点为所有工具设置别名，从而使 Babel、Jest™、Webpack™及其他自定义解析逻辑在需要时能够将导入路径从react自动重定向到react-18。这一“全局别名”配置确保用户代码无需做任何修改，所有重定向操作均在幕后自动完成。</p><p></p><h2>TypeScript 差异处理</h2><p></p><p>鉴于我们的组件可能会在 React 16 或 18 环境中运行，我们希望在升级过程中使用能兼容这两个版本的类型定义。幸运的是，React 团队在主要版本之间保持了良好的向后兼容性。</p><p>&nbsp;</p><p>我们已安装 React 18 的类型定义，并为 React 18 新增的 API（如useTransition）创建了一个兼容层（shim），以确保这些 API 在 React 16 和 18 中都能正常工作。在 React 16 中，useTransition会被实现为一个空操作。对于像useId这样无法直接创建 shim 的 API，我们通过类型增强来标注这个钩子在 React 16 环境中可能不可用。</p><p>&nbsp;</p><p>针对 React 18 中涉及 TypeScript 的<a href=\"https://github.com/DefinitelyTyped/DefinitelyTyped/issues/46691\">重大变更</a>\"，我们计划在 React 18 升级全面完成后，逐步解决这些问题。通过强化类型定义，我们能够在 monorepo 中稳步修复这些新出现的 TypeScript 错误。</p><p></p><h3>环境目标</h3><p></p><p>为了有效解决 React 版本重复导入的难题，我们精心设计了两种构建产物：一种集成 React 16，另一种则搭载 React 18。我们将这两种产物分别命名为“控制”构建和“处理”构建。鉴于爱彼迎广泛采用服务器端渲染（SSR）技术，我们进一步在服务器上部署了不同的 Node.js 进程来分别运行这两个构建。借助 Kubernetes 的强大功能，我们成功构建了两个独立的 Kubernetes 环境，专门用于运行这些控制与处理构建。这一策略，我们称之为环境目标，它为实现不同版本框架的并行部署提供了坚实基础。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5d870d65aa9f1532aab8f4e108117ac6.png\" /></p><p></p><p>模块别名与环境目标策略相辅相成，共同助力我们在生产环境中灵活部署不同版本的 React 框架。</p><p></p><p>在构建流程中，我们巧妙地引入了一个名为REACT_UPGRADE的环境变量，并在 Node SSR 服务运行时加以设置。这一举措让我们能够灵活地在系统升级的不同阶段执行条件逻辑，确保升级过程的平稳过渡。</p><p>&nbsp;</p><p>值得一提的是，这一设置同样在我们的本地开发环境中大放异彩。通过将“本地”开发环境也纳入部署范畴，我们能够确保 React 版本的配置与生产环境保持高度一致。随着每个 SSR 服务逐步升级到 React 18，我们相应地将对应服务的开发环境也切换至 React 18，从而在生产与本地开发之间建立起无缝的版本同步机制。</p><p></p><h3>升级测试</h3><p></p><p>为了确保升级过程的安全无误，爱彼迎构建了一套全面的测试体系，涵盖视觉回归测试、集成测试及单元测试等多个维度。在正式将升级推向用户之前，我们逐一解决了测试套件中出现的新问题，确保系统稳定运行。</p><p>&nbsp;</p><p>在单元测试环节，面对框架内部的抽象复杂性，我们遭遇了不小的挑战。由于我们采用了 <a href=\"https://medium.com/%E7%88%B1%E5%BD%BC%E8%BF%8E-engineering/phase-ii-enzyme-d9efa717e297\">Enzyme 与 React Testing Library 的组合</a>\"，我们不得不深入修复单元测试、shim 及适配器中对 API 和框架内部的错误假设。为此，我们在 React 16 和 18 两个版本中并行运行所有单元测试，对 React 18 测试套件中的既有失败采取容忍态度，并逐步进行修复。通过维护一个“允许失败”的列表，我们有效减少了测试失败的数量，同时避免了新失败的产生。这一策略使我们能够有条不紊地在组件与测试环境中逐一攻克难题。</p><p>&nbsp;</p><p>此外，我们还利用仪表板实时追踪数百个测试失败的修复进度，将修复任务逐一合并并分配给团队成员。这一举措不仅让迁移过程对前端团队而言几乎透明无感知，还极大地增强了我们在升级上线前的信心。</p><p></p><h2>渐进发布策略</h2><p></p><p>在成功部署模块别名与环境目标技术后，我们能够在统一的代码库中并行管理 React 的两个不同版本。为了确保升级过程既安全又可验证，我们采取了一种渐进式发布新环境的策略。此策略旨在精细调控流量与产品界面的变更，以最小化单次更新带来的冲击。通过我们的实验基础设施，我们灵活地引导流量流向两个生产环境（控制组与实验组），这样便能在内部先行测试升级效果，并在发现任何问题时迅速回滚，保障服务的稳定性。</p><p>&nbsp;</p><p>然而，将发布控制细化到不同界面层面则更为复杂。在单页应用架构下，同时管理多个 React 版本意味着需要频繁地卸载并重新挂载 React 根组件，这不仅可能影响应用性能，还可能损害用户体验。</p><p>&nbsp;</p><p>为此，我们将发布升级的管理层级提升至应用级别。鉴于爱彼迎的单一代码库内嵌多个单页应用，我们利用 React 升级系统对这些应用逐一进行渐进式升级。这一方法使我们能够先在内部对单个应用实施升级，并在开发和预发布环境中进行测试，从而避免了长期维护功能分支的繁琐，有效实现了平滑过渡的升级目标。</p><p></p><h2>功能采纳与未来规划</h2><p></p><p>依托上述系统，我们已顺利完成 React 18 在爱彼迎所有 Web 界面的全面推广，且无需进行任何回滚操作。升级完成后，我们随即开始对新引入的 API 进行测试，包括<a href=\"https://react.dev/reference/react-dom/client/createRoot\">新根 API</a>\" 和<a href=\"https://react.dev/reference/react/startTransition\">并发渲染特性</a>\"。为确保升级后的稳定性，我们特意推迟了几周才正式启用这些新功能，有效规避了因需降级或恢复代码变更而带来的风险。</p><p>&nbsp;</p><p>新特性的性能优势让我们倍感振奋，目前我们正积极探索如何将这些优势进一步拓展至关键 UI 界面，以期获得更显著的效益。</p><p>&nbsp;</p><p>展望未来，为确保升级工作的持续性与高效性，我们计划利用 React 升级系统测试 <a href=\"https://react.dev/community/versioning-policy#canary-channel\">React 金丝雀通道</a>\"。通过直接指向金丝雀版本，我们能够提前洞悉即将面临的迁移任务，为迎接 <a href=\"https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024#the-next-major-version-of-react\">React 19</a>\" 做好充分准备。我们坚信，将版本更新视为一项持续进行的任务而非一次性的重大变革，将使得未来的升级之路更加顺畅无阻。</p><p></p><h2>结论</h2><p></p><p>我们构建的 React 升级系统，其核心目标在于实现逐步升级、升级测试以及高频迭代的升级流程。通过巧妙融合环境目标与别名系统，我们能够有条不紊地推进逐步升级策略，并确保每一步都经过全面的测试。目前，我们已着手在 React 19 的测试版上进行前端试验，以期提前适应并充分利用其新特性。</p><p>&nbsp;</p><p>在此，我们特别向 React 团队致以诚挚的谢意，感谢他们在维护不同版本间，尤其是主版本间向后兼容性方面所做出的不懈努力。正是这些努力，为我们的顺利升级策略奠定了坚实的基础。</p><p>&nbsp;</p><p>依托 React 升级系统，我们对成功推广 React 18 充满信心，并计划将这一高效方法应用于未来的每一次升级。我们坚信，投资于升级系统的构建与维护是极具价值的，因为技术升级本身就是一个持续不断、永无止境的过程。React 升级系统不仅帮助我们逐步测试并推出新版本，更确保了我们的应用能够持续为用户提供最佳的体验与性能。</p><p>&nbsp;</p><p>背景信息补充：</p><p>React 17 于 2020 年发布，作为一次“平稳过渡”的版本，它未引入重大新功能，但为后续的更新奠定了重要基础，其破坏性更改被严格控制在极小的范围内。当我们开始着手升级时，React 18 已正式问世，因此，我们决定直接跨越至这一版本进行升级。目前，React 19 正处于积极的测试版开发阶段，而我们正利用我们的 React 升级系统紧锣密鼓地为其到来做好充分准备。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p><a href=\"https://medium.com/airbnb-engineering/how-airbnb-smoothly-upgrades-react-b1d772a565fd\">https://medium.com/airbnb-engineering/how-airbnb-smoothly-upgrades-react-b1d772a565fd</a>\"</p><p>&nbsp;</p><p>作者简介：</p><p>&nbsp;</p><p>Andre Wiggins，爱彼迎工程师</p><p>&nbsp;</p><p>声明：本文为 InfoQ 翻译整理，未经许可禁止转载。</p>",
    "publish_time": "2024-09-04 17:59:30",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "又一款AI编码工具火出圈！OpenAI投资、碾压VS Code、8岁女孩用它45分钟就能构建一款聊天机器人",
    "url": "https://www.infoq.cn/article/w3m3rysJpfEiyRszCjTR",
    "summary": "<p>后AI时代，几乎每隔一段时间就会出现一个爆款AI应用引发业界广泛关注。近日，一款名为Cursor的AI 编码工具成为了万众瞩目的焦点。</p><p></p><p>Cursor集成了Claude 3.5 Sonnet和GPT-4o等先进模型，为用户提供了高效的编程体验。它不仅融合了开发环境的实用性，还融入了AI聊天机器人的交互性，能让用户仅使用文本提示即可编写、预测和操作代码。</p><p>&nbsp;</p><p>与GitHub Copilot等辅助工具相比，Cursor在自动化和完成度上有了显著提升，它的简单性在于可以通过聊天窗口进行操作，这意味着即使是完全不懂代码的人也可以在几分钟内运行一个功能齐全的应用程序，并不断在此基础上添加新功能，它真正做到了使编码更加民主化。</p><p>&nbsp;</p><p>它建立在与Microsoft Visual Studio Code相同的系统之上，确保了良好的兼容性和用户体验，因此迅速赢得了包括新手程序员和资深工程师在内的广泛用户群体。Perplexity、Midjourney和OpenAI的员工是付费使用该 AI 工具的 30000 名客户中的一部分。</p><p>&nbsp;</p><p>那么，这款AI编码工具到底什么来头？</p><p></p><h2>Cursor AI是谁？</h2><p></p><p>&nbsp;</p><p>Cursor AI编码工具是由Anysphere&nbsp;公司开发的。这家初创公司自 2022 年成立以来已筹集了超过 4 亿美元，并积极与包括Anthropic 和 OpenAI 在内的各多个模型合作。</p><p>&nbsp;</p><p>2022年，Michael Truell在麻省理工学院 (MIT) 求学期间结识了 Anysphere 的其他联合创始人 Sualeh Asif、Arvid Lunnemark 和 Aman Sanger，并成为了密友。四人有着共同的目标，那就是创建一个集成开发环境 (IDE)，以加快常见的编程和软件构建任务（如调试）。为此，他们成立了Anysphere 这家公司，致力于构建极其高效的人机交互系统。</p><p>&nbsp;</p><p>在公司官网上，他们写道：</p><p>&nbsp;</p><p></p><blockquote>“首先，我们正在构建未来的工程师：一个比任何单个工程师效率高出一个数量级的人机交互程序员。这种混合工程师将毫不费力地控制他们的代码库，并且不会出现低熵击键。他们将以自己的判断速度进行迭代，即使在最复杂的系统中也是如此。通过结合人工智能和人类的智慧，他们将比最好的纯人工智能系统更聪明、更精通设计。&nbsp;我们是一群研究人员和工程师。我们构建软件和模型，在有用和可能的边缘进行发明。我们的工作已经改善了数十万程序员的生活。”</blockquote><p></p><p>&nbsp;</p><p>基于这样的初心，Anyspher推出了 Cursor AI编码工具，它是微软开源代码编辑器 VS Code 的一个分支，它包含人工智能工具，旨在帮助开发人员编写代码并提出相关问题。</p><p>&nbsp;</p><p>Cursor 可以回答诸如“VS Code 中的哪项服务允许我将状态保存到磁盘？”之类的问题，并在程序员工作时提取相关文档和代码定义。</p><p>&nbsp;</p><p>Cursor 还具有由 OpenAI 模型提供支持的生成式 AI 功能，即能够根据提示生成代码。它还可以被动扫描文件并发现代码库中的潜在错误。</p><p>&nbsp;</p><p>“当人们想到‘人工智能加编码’时，他们通常会想到人工智能自动完成功能，”Anyspher联合创始人Sanger如是说。“我们认为 GitHub Copilot 和其他公司在这方面做得特别好，所以我们专注于自动完成功能之后的功能，比如查找和修复错误以及代码库问答。”</p><p>&nbsp;</p><p>虽然从表面上看，许多简单的功能（例如要求聊天机器人构建应用程序）都是用户已经可以在 Claude 或ChatGPT中实现的功能。 但Cursor的强大之处在于其与代码编辑器的集成以及快速进行更改或解决问题的能力。</p><p>&nbsp;</p><p>Anysphere&nbsp;公司 CEO Michael Truell 将Cursor描述为“程序员的 Google Docs”，这是一个内置 AI 模型的简单代码编辑器，仅使用文本提示即可编写、预测和操作代码。</p><p>&nbsp;</p><p>Truell 在接受《福布斯》采访时表示，他们的目标是让 Cursor 实现 95% 的工程师自动化，这样他们就可以专注于编码的创造性方面。这将使个人工程师能够“构建比强大团队目前所能构建的系统复杂得多的系统”。</p><p>&nbsp;</p><p></p><blockquote>“在我看来，它的真正力量在于使编码变得民主化。它还可以让没有太多编码经验的人通过输入几行文本来构建他们需要的工具。”</blockquote><p></p><p>&nbsp;</p><p>成立仅两年时间，已经完成了多轮融资，他们从<a href=\"https://www.openai.fund/\">OpenAI Startup Fund</a>\"筹集了 800 万美元的种子资金。</p><p>&nbsp;</p><p>市场上AI自动编码工具那么多，为什么Cursor 能引发热议？</p><p>&nbsp;</p><p></p><h2>Cursor 为什么会如此受欢迎？</h2><p></p><p>&nbsp;</p><p>不难发现，随着技术的不断迭代，大模型已经越来越擅长编写代码。所有主流人工智能模型现在都可以可靠地执行基本的编程任务，准确率甚至超过90%。它们开始通过规划和多轮提示策略来解决更复杂的现实世界任务。而且它们可以用80多种语言来完成。</p><p>&nbsp;</p><p>然而，编写代码不仅仅是编写新代码。开发人员的大部分时间都花在维护、调试或调整旧代码上。要做到这一点，就需要真正理解代码和系统意图。构建软件是一个从根本上创造性的过程——你可以改变困难的部分，但你不能完全自动化它们。</p><p>&nbsp;</p><p>大语言模型是程序员的强大工具，他们的编码能力会随着时间的推移而提高。但同样明显的是，对于大多数编码任务，要解决的问题不是如何让大语言模型单独运行良好，而是如何让它们与人类开发人员一起运行良好。</p><p>&nbsp;</p><p>因此，业界基本已经达成共识——程序员和 AI 模型之间的接口将很快成为开发堆栈中最重要的部分之一。所以也就不难理解为什么资本更愿意倾向Anysphere公司，因为他们在解决此问题上走得比较靠前。</p><p>&nbsp;</p><p>作为VS Code 的一个分支，Cursor 针对 AI 辅助编程进行了高度定制。它适用于所有最新的大语言模型，并支持完整的 VS Code 插件生态系统。Cursor 的特别之处在于旨在将 AI 集成到开发人员工作流程中的功能——包括下一步行动预测、自然语言编辑、与代码库聊天以及即将推出的一系列新功能。</p><p>&nbsp;</p><p>事实上，这种产品很难做好。完善文本编辑器已经花费了几十年的时间，而这个话题仍然在开发人员中引起激烈的争论。人工智能辅助编辑可以说更加复杂，需要新的工作流程（例如，如何一次编辑多个文件？如何在文件中间注入新代码？）并使用外部非确定性系统管理状态。这必须以一种高性能、感觉自然且不会干扰开发人员正常工作的方式完成。</p><p>&nbsp;</p><p>而这些事，Cursor 已经做得足够好。因此，在短短一年多的时间里，数千名用户注册了 Cursor，其中包括 OpenAI、Midjourney、Perplexity、Replicate、三星等公司，其中许多人已经开始为其付费，而且他们很少再换回其他 IDE。</p><p>&nbsp;</p><p>Figma的工程师Jordan Singer总结道：</p><p>&nbsp;</p><p></p><blockquote>“所有这些关于 Claude + Cursor 的讨论，以及能够构建任何你愿意做的事情（无论你的技能如何）都是有道理的。如果这就是未来，我想生活在其中。”</blockquote><p></p><p>&nbsp;</p><p>AI 大神 Karpathy 用完也忍不住赞叹：未来编程是不是只需要狂按 tab 就够了？</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef4237cfa27c91014233629fc2dfe3e8.png\" /></p><p></p><p>&nbsp;</p><p>更重要的是，它不只是针对专业人士，连毫无编码经验的初学者也能轻松用起来。Cloudflare 开发者关系副总裁 Ricky Robinett分享道：</p><p>&nbsp;</p><p></p><blockquote>“我八岁的女儿仅用 45 分钟就使用 Cursor AI 构建了一个聊天机器人。”</blockquote><p></p><p></p><p></p><h2>Cursor 能否取代VS Code?</h2><p></p><p>&nbsp;</p><p>那么，如此受欢迎的Cursor能否取代？或者说，Cursor 是否有很大希望与 IDE 领域的现有企业竞争？</p><p>&nbsp;</p><p><a href=\"https://survey.stackoverflow.co/2023/\">根据</a>\"StackOverflow 的 2023 年开发者调查，微软的 Visual Studio Code 仍然是最受欢迎的 IDE，约 73% 的开发者表示这是他们的首选。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/e0/e0ce520d49be5590441689d67b94a120.png\" /></p><p></p><p>图注来源：《2023 年 StackOverflow 开发者调查报告》</p><p>&nbsp;</p><p>Anysphere 团队确实认为微软是他们的主要竞争对手。他们承认这家科技巨头拥有巨大的分销优势。但他们表示，由于 Visual Studio Code 拥有广泛而多样的客户群，微软无法在不冒疏远部分用户的风险的情况下迅速做出重大改变或发布重大升级。</p><p>&nbsp;</p><p>“人工智能编码领域的天花板很高——有太多事情要做——不可能只是克隆技术然后在上面取得巨大的销售业绩，”Truell 说。“你需要不断发展技术。全球有超过 2600 万开发人员，对于那些想要真正人工智能原生体验的人来说，这是一个巨大的市场。”</p><p>&nbsp;</p><p>由4人组成的 Anysphere 创始团队雄心勃勃，他们希望在 Cursor 的开发路线图上实现一系列功能。在接下来的几个月里，他们的计划是让 Cursor 能够跨文件和整个文件夹进行更复杂的编辑，提高查找代码的能力，并从文档中学习新的库。</p><p>&nbsp;</p><p>与此同时，Truell 称，Anysphere 的受欢迎程度正在慢慢增长，平台上有数万用户，付费客户群“快速增长”。年经常性收入已超过 100 万美元——对于一家成立约两年的公司来说，这是一个好兆头。</p><p>&nbsp;</p><p>反观微软，虽然微软一直在努力“讨好”开发者（继Excel和Windows之后，该公司又在GitHub上发布了配套Copilot），但作为当今世界上最具影响力的开发软件之一，VS Code却在竞争对手的冲击之下逐渐迷失了方向。</p><p>&nbsp;</p><p>谷歌公司首席工程师Jaana Dogan评论称，“真搞不懂微软到底理不理解VS Code在市场上的实际影响力。”但不能否认的是，显然有一大批开发者不希望接受逐渐陈旧过时的VS Code。尽管GitHub上也出现了不少VS Code的分支代码仓库，但市面上的新工具仍然让VS Code的生存倍感压力。</p><p>&nbsp;</p><p>X上的一位开发者评论道：</p><p>&nbsp;</p><p></p><blockquote>“刚刚制裁了VS Code。至于理由？很可能是因为Cursor AI的发布，这股号称编码界ChatGPT的新生力量似乎成为压死VS Code骆驼的最后一根稻草。然而，我们曾经熟知的VS Code是否真会就此终结？”</blockquote><p></p><p>&nbsp;</p><p>为什么开发者会对VS Code感到失望？</p><p>&nbsp;</p><p>VS Code确实存在几个不容忽视的现实问题。前端软件工程师Mohamed Hamani表示，VS Code在Python开发中的表现相当差劲，并解释称其无法高亮显示代码中的问题。其他开发者也纷纷表示同意，VS Code效果不佳迫使他们往往选择其他 IDE，例如Jetbrains甚至用于Python的vim。</p><p>&nbsp;</p><p>哪怕是对于C#和其他一些相对传统的语言，虽然VS Code也推出过几次更新，但开发人员对于结果同样不太满意。此外，通过VS Code安装恶意扩展的难度太低。</p><p>&nbsp;</p><p>而谈到Cursor，人们普遍觉得它就是美化版或者扩展版的VS Code，能够集成多个开源大语言模型。相较于VS Code这样的纯IDE，Cursor对于大部分AI开发者来说显然更方便。</p><p>&nbsp;</p><p>X上的一位开发者解释道，“大家可以选定代码并根据其内容提出问题。也就是说，用不着在IDE和浏览器之间不断切换。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d5/d52cff4c59bf34517394bee2793d0797.png\" /></p><p></p><p>如果你还在用VS Code但又感觉非常不爽，不妨试试Cursor。</p><p>&nbsp;</p><p>有一说一，VS Code的固有功能做得不错，但在AI开发方面——特别是在与Claude 3.5 Sonnet和其他大语言模型的集成方面要远远落后于Cursor。虽然VS Code也允许集成其他大模型，例如Phi-3.5或者GPT-4，但Cursor在提供包括Llama 3.1在内的更多模型选项方面更加灵活。</p><p>&nbsp;</p><p>Cursor的编码和AI功能也应当成为引起微软关注的一记警钟，特别是努力降低VS Code与GitHub Copilot之间的整合门槛。</p><p>&nbsp;</p><p>Infinite Red创始人Jamon Holmgren表示，“我有个假设，Cursor的迅速走红应该能够唤醒微软，让他们进一步完善VS Code与GitHub Copilot之间的集成效果，而且最好能在几个月内实现。”</p><p>&nbsp;</p><p>如此说来，微软VS Code或者GitHub Copilot到底能不能赶得上这场趋势性的变革？</p><p></p><h2>Cursor来势汹汹，微软VS Code不甘示弱</h2><p></p><p>&nbsp;</p><p>就在一年之前，不少报道都把Cursor称为VS Code和ChatGPT合二为一的产物。现如今，人们开始将其视为VS Code和GitHub Copilot的替代品。自从Cursor诞生以来，放弃VS Code的声音就始终不绝于耳。但时至今日，VS Code仍然相当强大。</p><p>&nbsp;</p><p>在Reddit的讨论中，开发者们在比较Cursor和VS Code的功能时，往往更倾向于在长期规划当中选择VS Code。也就是说，多数人并不相信出自一家小型初创公司之手的Cursor真能跟整个微软帝国对抗。一位用户坦言，“即使Cursor能撑到明年，我也仍然会选择VS Code，因为它才代表着行业的标准和主流。”</p><p>&nbsp;</p><p>egghead.io创建者John Lindquist表示，他最近与VS Code的项目经理Harald Kirschner聊到了跟Cursor之间的竞争。该团队非常清楚Cursor的优势，而且正在积极筹备几项应对之策。“我想他们会带来一些惊喜。”</p><p>&nbsp;</p><p>VS Code效法Cursor的更新似乎即将推出，不知道Cursor能不能顶得住这样一波压力。出于生存的考虑，Cursor后续可能会接受OpenAI或者Anthropic等厂商的收购。</p><p>&nbsp;</p><p>至于VS Code这边，考虑到其庞大的装机规模和潜在的更新规划，开发人员实在很难断然将其卸载。毕竟微软也很明白，要想在下一阶段继续保住开发领域的优势地位，就得想办法让“使每个人都能用自然语言搞开发”的宣言成功落地。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.tomsguide.com/ai/cursor-is-chatgpt-for-coding-now-anyone-can-make-an-app-in-minutes\">https://www.tomsguide.com/ai/cursor-is-chatgpt-for-coding-now-anyone-can-make-an-app-in-minutes</a>\"</p><p><a href=\"https://techcrunch.com/2023/10/11/anysphere-raises-8m-from-openai-to-build-an-ai-powered-ide/\">https://techcrunch.com/2023/10/11/anysphere-raises-8m-from-openai-to-build-an-ai-powered-ide/</a>\"</p><p><a href=\"https://analyticsindiamag.com/developers-corner/why-developers-are-uninstalling-vs-code/\">https://analyticsindiamag.com/developers-corner/why-developers-are-uninstalling-vs-code/</a>\"</p>",
    "publish_time": "2024-09-04 18:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "当在本地就可以运行AI代码助手时，谁还需要GitHub Copilot呢？",
    "url": "https://www.infoq.cn/article/jVwqya1bolDA4NB5SjCc",
    "summary": "<p>本文最初发布于The Register。</p><p>&nbsp;</p><p>作为生成式人工智能的早期用例，代码助手实践已经获得了相当多的关注——尤其是在微软推出GitHub Copilot之后。但是，如果你不喜欢让微软动你的代码，或者不愿意每月支付10美元的费用，那么你可以构建自己的助手。</p><p>&nbsp;</p><p>虽然微软是最早将人工智能代码助手<a href=\"https://www.theregister.com/2022/12/09/github_introduces_copilot_for_business/\">商业化</a>\"并集成到IDE中的公司之一，但它远不是唯一选项。事实上，有许多为代码生成而训练的大型语言模型（LLM）。</p><p>&nbsp;</p><p>而且，你现在正在使用的电脑很有可能就能够运行这些模型。关键是以一种有实际用处的方式将它们集成到IDE中。</p><p>&nbsp;</p><p>这就轮到像Continue这样的应用程序发挥作用了。这个<a href=\"https://github.com/continuedev/continue\">开源的代码助手</a>\"被设计成可以嵌入流行的IDE，如JetBrains或Visual Studio Code，并连接到你可能已经比较熟悉的流行的LLM运行程序，如Ollama、Llama.cpp和LM Studio。</p><p>&nbsp;</p><p>像其他流行的代码助手一样，Continue支持代码补全和生成，并且能够针对不同的用例优化、注释或重构代码。此外，Continue还提供了一个具有RAG功能的集成聊天机器人，让你可以有效地与代码库对话。</p><p>&nbsp;</p><p>在本指南中，我们将搭配使用Continue与Ollama，但Continue也可以与多个专有模型（包括OpenAI和Anthropic）搭配使用——通过各模型的API，如果你愿意按令牌付费而不是每月支付固定费用的话。</p><p></p><h3>你需要做好以下准备：</h3><p></p><p>一台能够运行普通LLM的机器。一个处理器相对比较新的系统就可以，但为了获得最佳性能，我们建议使用Nvidia、AMD或Intel GPU ，且vRAM至少为6GB。如果你更喜欢用Mac电脑，那么任何Apple Silicon系统应该都可以，包括最初的M1。不过，为了能达到最佳效果，我们建议内存至少要有16GB。本指南还假设，你已经在机器上安装并运行了Ollama模型运行程序。如果没有，可以看下我们提供的<a href=\"https://www.theregister.com/2024/03/17/ai_pc_local_llm/?td=rt-3a\">这份指南</a>\"，它应该可以帮你在十分钟内运行起来。对于那些使用Intel Integrated或Arc显卡的用户，<a href=\"https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md\">这里</a>\"有一份使用IPEX-LLM部署Ollama的指南。兼容的IDE。在撰写本文时，Continue支持<a href=\"https://www.jetbrains.com/\">JetBrains</a>\"和<a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>\"。如果你想完全避开微软的遥测技术，像我们一样，开源社区构建的<a href=\"https://vscodium.com/\">VSCodium</a>\"是个不错的选择。</p><p></p><h3>安装Continue</h3><p></p><p>在本指南中，我们将在VSCodium中部署Continue。首先，启动IDE并打开扩展管理面板，搜索并安装Continue。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bcb539c921581a46ebef159e4b7a0eb7.jpeg\" /></p><p></p><p>几秒钟后，Continue的初始设置向导启动，你可以选择是在本地托管模型还是使用另一个提供商的API。</p><p>&nbsp;</p><p>在这个例子中，我们将通过Ollama在本地托管我们的模型，因此，我们将选择“Local models（本地模型）”。该配置使Continue可以使用下列开箱即用的模型。稍后，我们会讨论如何将这些模型更换为其他选项，但现在，我们先从这些模型开始：</p><p>Llama 3 8B：来自Meta的通用LLM，用于注释、优化和/或重构代码。要了解关于Llama 3的更多信息，请阅读<a href=\"https://www.theregister.com/2024/04/19/meta_debuts_llama3_llm/\">我们的发布日报道</a>\"。Nomic-embed-text：用于在本地索引代码库的嵌入式模型，使你能够在给集成聊天机器人提示时引用代码库。Starcoder2:3B：这是BigCode的一个代码生成模型，为Continue的Tab自动补全功能提供支持。</p><p>&nbsp;</p><p>如果因为某种原因，Continue跳过了启动向导，不要担心，你可以在终端运行以下命令，使用Ollama手动拉取这些模型：</p><p><code lang=\"null\">ollama pull llama3\nollama pull nomic-embed-text\nollama pull starcoder2:3b</code></p><p>&nbsp;</p><p>有关使用Ollama设置和部署模型的更多信息，请查看我们的<a href=\"https://www.theregister.com/2024/03/17/ai_pc_local_llm/?td=rt-3a\">快速入门指南</a>\"。</p><p></p><h3>遥测警告</h3><p></p><p>在继续之前，需要提醒一下，在默认情况下，Continue会收集匿名遥测数据，包括：</p><p>是否接受或拒绝建议（不包括代码或提示）；使用的模型名称和命令；生成的令牌数量；操作系统和IDE的名称；访问量。</p><p>&nbsp;</p><p>如果你不想自己的数据被收集的话，则可以修改主目录下的.continue文件，或者取消VS Code设置中的“Continue: Telemetry Enabled”复选框。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/18/18f47fa011a4813e2488500f7b893f61.jpeg\" /></p><p></p><p>要进一步了解Continue的数据收集政策，可以查看<a href=\"https://docs.continue.dev/telemetry\">这里</a>\"。</p><p></p><h3>请求就会有结果。有效吗？那是另外一回事了</h3><p></p><p>安装完成后，我们可以开始深入研究将Continue集成到工作流中的各种方法了。第一种方法可以说显而易见：从零开始生成代码片段。</p><p>&nbsp;</p><p>例如，如果你想为一个项目生成一个基本的网页，只需按下键盘上的Ctrl-I或Command-I，然后在操作栏中输入提示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/1174863748b16008918af2f1280ac67e.png\" /></p><p></p><p>在这里，我们的提示是“Generate a simple landing page in HTML with inline CSS（使用HTML生成一个包含内联CSS的简单登录页）”。提交提示后，Continue将加载相关模型（这可能需要几秒钟，取决于你的硬件），然后它会向我们提供一个代码片段，我们可以选择接受或拒绝。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/06/062871366f02ec0fb7f9ff268c51ce35.png\" /></p><p>在Continue中生成的代码将以绿色代码块的形式出现在VS Code中，你可以接受或拒绝。</p><p></p><h3>重写你的代码</h3><p></p><p>Continue还可以用于重构、注释、优化或编辑现有代码。</p><p>&nbsp;</p><p>例如，假设你有一个用于在PyTorch中运行LLM的Python脚本，你想重构它然后在Apple Silicon Mac上运行。首先，选择需要重构的文档，按下键盘上的Ctrl-I并给助手输入提示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da0ad880010c4cf38c7b30a530acc888.png\" /></p><p></p><p>几秒钟后，Continue会传出模型的建议——新生成代码用绿色高亮显示，而需要删除的代码则用红色标记。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/1f/1fbe088a4a14410439f16322de68f633.png\" /></p><p></p><p>除了重构现有代码外，该功能还可用于事后生成注释和/或文档字符串。这些功能可以在右键菜单中的“Continue”下找到。</p><p></p><h3>Tab自动补全</h3><p></p><p>虽然代码生成对于快速实现模型进行概念验证或重构现有代码很有用，但根据所使用的模型的不同，仍然可能存在一些偶然性。</p><p>&nbsp;</p><p>任何曾经要求ChatGPT生成代码块的人都知道，有时它会产生幻觉包或函数。这些幻觉相当明显，因为糟糕的代码往往会导致令人印象深刻的失败。但是，正如我们之前<a href=\"https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/\">讨论</a>\"过的那样，如果频繁提供这样的幻觉包，可能会造成安全威胁。</p><p>&nbsp;</p><p>如果不需要AI模型为你编写代码，那么Continue还支持代码补全功能。这让你可以更好地控制模型进行或不进行哪些编辑或更改。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef015207849c1480f033e5860276e429.png\" /></p><p></p><p>这个功能的工作方式有点像终端中的Tab补全。当你进行输入时，Continue会自动将你的代码输入到一个模型中——比如Starcoder2或Codestral——并提供补全字符串或函数的建议。</p><p>&nbsp;</p><p>建议显示为灰色，并且会随着你每次敲击键盘而更新。如果Continue猜测正确，那么你可以按下键盘上的Tab键来接受建议。</p><p></p><h3>与代码库对话</h3><p></p><p>除了代码生成和预测之外，Continue还提供了一个集成聊天机器人。该机器人具有RAG风格的功能。要了解更多关于RAG的信息，可以在<a href=\"https://www.theregister.com/2024/06/15/ai_rag_guide/\">这里</a>\"查看我们的实践指南，但在Continue中，它综合运用Llama 38b和nomic-embed-text嵌入式模型来实现代码库可搜索。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/63/6304875b6bacfd52a9e59edeaca07129.jpeg\" /></p><p>Continue提供了一个集成聊天机器人，它可以接入你选择的LLM。</p><p>&nbsp;</p><p>诚然，这个功能存在一些不确定性，但下面这几个例子可以说明如何使用它来提高工作流效率：</p><p>输入@docs，然后输入你的应用程序或服务的名称——&nbsp;例如Docker，最后输入你的请求。要查询关于工作目录的信息，输入@codebase&nbsp;，然后输入你的请求。将文件或文档加入模型的上下文：输入@files&nbsp;，然后选择你想要添加到下拉选项中的文件。按&nbsp;Ctrl-L将你在编辑器中选中的代码添加到聊天机器人。按Ctrl-Shift-R&nbsp;将来自VS Code终端模拟器的错误信息直接发送给聊天机器人进行诊断。</p><p></p><h3>更换模型</h3><p></p><p>在实践中，Continue的可靠性实际上取决于你选用的模型，因为这个插件本身实际上更像是一个将LLM和代码模型集成到IDE中的框架。虽然它定义了用户如何与这些模型交互，但它无法控制所生成代码的实际质量。</p><p>&nbsp;</p><p>好消息是，Continue没有与任何一种模式或技术绑定。正如我们前面提到的，它可以接入各种LLM运行程序和API。如果有新发布的模型针对你的首选编程语言进行了优化，那么除了硬件之外，没有什么可以阻止你使用它。</p><p>&nbsp;</p><p>由于我们使用Ollama作为模型服务器，所以在大多数情况下，更换模型是一项相对比较简单的任务。例如，如果你想把Llama 3换成谷歌的Gemma 29b，把Starcoder2换成Codestral，则可以运行以下命令：</p><p><code lang=\"null\">ollama pull gemma2\nollama pull codestral</code></p><p>注意：Codestral有220亿个参数和32000个令牌的上下文窗口，即使精度量化到4位，在本地运行的话，也是一个相当庞大的模型。如果遇到了程序崩溃的问题，那么你可能会想试一下小一点的东西，比如<a href=\"https://ollama.com/library/deepseek-coder\">DeepSeek Coder</a>\"的1B或7B变体。</p><p>&nbsp;</p><p>要更换用于聊天机器人和代码生成器的模型，你可以在Continue的选择菜单中选择它。或者，你可以使用Ctrl-'循环遍历下载好的模型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e0/e08589f75cc0817b76df6fc5d7e6b29b.jpeg\" /></p><p></p><p>更改Tab自动补全功能使用的模型有点麻烦，需要修改插件的配置文件。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f3/f3e3b1c3ee1ee992646fd0a9bb29bc2f.png\" /></p><p></p><p>拉取选择的模型后[1]，点击Continue侧边栏右下角的齿轮图标[2]，修改“tabAutocompleteModel”小节下的“title”和“model”条目[3]。如果你使用的是Codestral，那么这一部分配置应该是这样的：</p><p><code lang=\"null\">  \"tabAutocompleteModel\": {\n    \"title\": \"codestral\",\n    \"provider\": \"ollama\",\n    \"model\": \"codestral\"\n  },</code></p><p></p><h3>自定义编码模型调优</h3><p></p><p>默认情况下，Continue会自动收集有关你如何构建软件的数据。这些数据可用于根据你的特定风格和工作流进行自定义模型调优。</p><p>&nbsp;</p><p>需要说明的是，这些数据存储在本地主目录下的.continue/dev_data文件夹下。而且，据我们所知，默认情况下，这些数据并没有包含在Continue收集的遥测数据中。不过，如果你还是担心的话，建议你把它关掉。</p><p>&nbsp;</p><p>大型语言模型调优的具体内容超出了本文的范围，但你可以读下<a href=\"https://blog.continue.dev/its-time-to-collect-data-on-how-you-build-software/\">这篇文章</a>\"，从中了解Continue收集了哪类数据以及如何利用这些数据。</p><p>&nbsp;</p><p>我们希望可以在未来的实践中进一步探索调优过程，所以请务必在评论区分享你对本地AI工具（如Continue）的看法以及你希望我们下一步做何种尝试。</p><p>&nbsp;</p><p>&nbsp;</p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p>&nbsp;</p><p>原文链接：<a href=\"https://www.theregister.com/2024/08/18/self_hosted_github_copilot/?td=rt-3a\">https://www.theregister.com/2024/08/18/self_hosted_github_copilot/?td=rt-3a</a>\"</p>",
    "publish_time": "2024-09-04 18:08:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "飞书业务工具矩阵再升级！新一代多维表格、低代码平台等产品集体炸场",
    "url": "https://www.infoq.cn/article/Ix1ZmFtluRtI6IKSiYZy",
    "summary": "<p>9月4日，飞书发布了全新多维表格、低代码平台等系列业务工具产品，并推出了面向出海企业的跨境合规解决方案。这些产品将继续为中国企业提供实质的降本增效帮助，促进企业以更低的数字化成本解决实际业务问题。</p><p>&nbsp;</p><p>“如何能更有效的提升效率，更实实在在的降本，是企业家特别关注的话题。我们意识到，如果说以前追求效率是为了发展，那么现在追求效率则是为了生存。这些来自真实企业家的声音，也让我们更多地思考了自己的业务方向。” 谢欣表示，“多维表格和低代码平台、飞书项目都是直接为企业的一线业务服务，这几个产品将组成业界最强性能的业务工具，帮助企业更优质地降本增效。”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9c/9c742edce5c5f80db36f0726c08244fe.png\" /></p><p></p><p>图注：飞书CEO谢欣</p><p></p><p>飞书成立至今，已有海量中国优秀公司成为客户，近一年来，亦有霸王茶姬、胖东来、荣耀、公牛、雅迪这样家喻户晓的国民品牌开始全员使用飞书，他们也代表着广大中国优秀企业的共同选择。</p><p>&nbsp;</p><p>同时，飞书在2023年的ARR（年度可重复性收入）也已达到2亿美元，成为同类产品中的第一。</p><p></p><h2>新一代多维表格，让一线员工搭建系统不求人</h2><p></p><p>&nbsp;</p><p>自飞书2020年发布多维表格开始，这款产品逐渐为大众使用与熟知，并成为了一个全新品类。</p><p>&nbsp;</p><p>据飞书透露，飞书多维表格的月活数已经达到 600 万，仅过去一年，飞书用户便创建了近 4000 万个多维表格，在这些多维表格上，流转着超过 100 亿条记录。在泡泡玛特、元气森林、蔚来汽车等知名公司，飞书多维表格均以极小成本解决了重要业务需求，可贵的是，这些业务系统均由不懂技术的一线员工搭建。</p><p>&nbsp;</p><p>今日，飞书正式推出飞书多维表格数据库，这让飞书多维表格的单表容量突破了100万行，仪表盘也可统计1000万行数据，均为全球同类产品中最高。在全新的强大性能下，即使在飞书多维表格中计算10 万行、100 列公式这样复杂的数据，仍然能在5 秒内便获取业务结果。</p><p>&nbsp;</p><p>飞书多维表格还发布了全新一代仪表盘，通过飞书多维表格数据库的计算能力，由多维表格行列数据生成的仪表盘，将不再是简简单单计算、汇总、呈现数据，增加了大量计算、图表组件编组、统计分析等功能，界面也可对标全球顶尖 BI 系统。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ed/ed6922c1f6375b35f9aebdb275f649d2.png\" /></p><p></p><p>图注：多维表格</p><p></p><p>随着飞书多维表格的日益深入，数千人共同使用的多维表格也在显著增多。飞书多维表格进一步升级了高级权限，每个不同表格使用人看到的数据、单元格、仪表盘等均会因为权限不同而显示不同，并且能够按照不同的条件收获动态同步。这也是目前最为精细的一套权限管理体系。</p><p>&nbsp;</p><p>此外，飞书多维表格还发布了多个AI功能，AI智能分析仪表盘可一键获取仪表盘数据背后的问题与变化，还可自动调用公式、一键生成自动化群推送等。</p><p>&nbsp;</p><p>目前，智谱、月之暗面、零一万物等公司均入驻推出了自己的AI字段捷径，这些AI工具也为企业在不同场景提供了更为智能的业务解决方案。</p><p>&nbsp;</p><p>“飞书在4年前发布多维表格时，多维表格这一品类在中国首次出现。今天，我们通过多维表格数据库、全新仪表盘、高级权限、AI等新一代功能，重新定义了多维表格的价值与作用。真正帮助业务人员自助搭建业务系统的平台，让一线员工搭建系统不再求人。”飞书多维表格负责人施凯文表示。</p><p></p><h2>飞书低代码平台正式发布让业务开发提效更敏捷</h2><p></p><p>&nbsp;</p><p>低代码，已是一个流行多年的开发方式，应用也越来越广泛。但在国内，低代码仍任重道远。企业既想要低代码的效率，又不想要过低的能力，而目前国内的产品，明显很难满足客户的需求。</p><p>&nbsp;</p><p>历经多年的开发与共创，飞书低代码平台已在多个企业有了较好的应用。例如在字节跳动，通过飞书低代码平台，工程师们构建了大量业务系统。每周，我们活跃的系统就有上千个，每天新增记录6000万条、运行流程2000万次，每年能帮字节节约数亿元的研发成本。而在联影医疗，基于飞书低代码平台，搭建了与Salesforce高度集成的复杂的CRM系统，满足上千人团队的协作需求，仅一个系统就节省150万元运营成本。</p><p>&nbsp;</p><p>飞书低代码平台，设计了一种新的解题思路，通过全代码能力建设，提供低代码开发，用多层架构的设计，兼顾能力强大和系统易用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9a/9a25c0fbf5d94feca0d914f10aad41d8.png\" /></p><p></p><p>图注：飞书低代码平台</p><p>&nbsp;</p><p>对于IT团队，飞书低代码平台提供了全栈开发能力、全周期管理能力和全面开放能力，武装团队里的每一个开发者，让CIO的数字化蓝图落地又快又好。</p>",
    "publish_time": "2024-09-04 18:23:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]