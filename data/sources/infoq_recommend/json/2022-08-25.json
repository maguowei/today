[
  {
    "title": "谷歌推出Bigtable联邦查询，实现零ETL数据分析",
    "url": "https://www.infoq.cn/article/cfBjW6rIGjIfU4LhjezU",
    "summary": "<p>最近，<a href=\"https://www.infoq.cn/topic/google\">谷歌</a>\"宣布Bigtable联邦查询普遍可用，用户通过BigQuery可以更快地查询Bigtable中的数据。此外，查询无需移动或复制所有谷歌云区域中的数据，增加了联邦查询并发性限制，从而缩小了运营数据和分析数据之间长期存在的差距。</p><p>&nbsp;</p><p>BigQuery是谷歌云的无服务器、多云数据仓库，通过将不同来源的数据汇集在一起来简化数据分析。Cloud Bigtable是谷歌云的全托管NoSQL数据库，主要用于对时间比较敏感的事务和分析工作负载。后者适用于多种场景，如实时欺诈检测、推荐、个性化和时间序列。</p><p>&nbsp;</p><p>在以前，用户需要使用ETL工具（如Dataflow或者自己开发的Python工具）将数据从Bigtable复制到BigQuery。现在，他们可以直接使用BigQuery SQL查询数据。联邦查询BigQuery可以访问存储在Bigtable中的数据。</p><p>&nbsp;</p><p>要查询Bigtable中的数据，用户可以通过指定Cloud Bigtable URI（可以通过Cloud Bigtable控制台获得）为Cloud Bigtable数据源创建一个外部表。URI包含以下这些内容：</p><p>&nbsp;</p><p>包含Cloud Bigtable实例的项目ID——project_id；Cloud Bigtable实例ID——instance_id；要使用的应用程序配置文件ID——app_profile（可选）；要查询的表名——table_name。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0dcf4d50fbb381f0343adde4dd85fcc5.png\" /></p><p></p><p>来源：<a href=\"https://cloud.google.com/blog/products/data-analytics/bigtable-bigquery-federation-brings-hot--cold-data-closer\">https://cloud.google.com/blog/products/data-analytics/bigtable-bigquery-federation-brings-hot--cold-data-closer</a>\"</p><p>&nbsp;</p><p>在创建了外部表之后，用户就可以像查询BigQuery中的表一样查询Bigtable。此外，用户还可以利用BigQuery的特性，比如JDBC/ODBC驱动程序、用于商业智能的连接器、数据可视化工具（Data Studio、Looker和Tableau等），以及用于训练机器学习模型的AutoML表和将数据加载到模型开发环境中的Spark连接器。</p><p>&nbsp;</p><p>大数据爱好者Christian Laurer在一篇文章中解释了Bigtable联邦查询的好处。</p><p>&nbsp;</p><p></p><blockquote>你可以使用这种新的方法克服传统ETL的一些缺点，如：&nbsp;1. 更多的数据更新（为你的业务提供最新的见解，没有小时级别甚至天级别的旧数据）；2. 不需要为相同的数据存储支付两次费用（用户通常会在Bigtable中存储TB级甚至更多的数据）；3. 减少ETL管道的监控和维护。</blockquote><p></p><p>&nbsp;</p><p>最后，关于Bigtable联邦查询的更多详细信息，请参阅官方的<a href=\"https://cloud.google.com/bigquery/docs/external-data-bigtable\">文档页</a>\"。此外，所有受支持的Cloud Bigtable区域都可以使用新的联邦查询。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/08/bigtable-bigquery-zero-etl/\">Google Introduces Zero-ETL Approach to Analytics on Bigtable Data Using BigQuery</a>\"</p>",
    "publish_time": "2022-08-25 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Kafka Streams与Quarkus：实时处理事件",
    "url": "https://www.infoq.cn/article/WfA0p1XoZCJ6INdyJLyv",
    "summary": "<p>在本系列的<a href=\"https://www.infoq.cn/article/cFpvXRLmZzJBGbzAeFu5\">第一部分</a>\"中，我们学习了<a href=\"https://kafka.apache.org/\">Apache Kafka</a>\"和<a href=\"https://quarkus.io/\">Quarkus</a>\"的集成，并开发了一个简单的应用，从两个Kafka主题生产和消费事件。</p><p></p><p>在那个样例中，我们模拟了一个影视流公司，在一个Kafka主题中存储电影信息，在另外一个主题中存储了用户停止观看电影时所发生的每个事件，并捕获了电影已播放的时间。</p><p></p><p>下图展示了该应用的架构：</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1Untitled-10-1656674518432.jpeg\" /></p><p></p><p>我们可以看到，消费消息很简单，只要有消息生成，我们就可以得到它们，但是除此之外，我们也做不了其他的事情了。如果我们需要实时处理数据（比如过滤或操作事件）或者我们需要在事件之间做一些关联，单纯使用Kafka的消费API可能就不是最佳的方式了，因为这会导致代码非常复杂。</p><p></p><h2>Kafka Streams</h2><p></p><p><a href=\"https://kafka.apache.org/documentation/streams/\">Kafka Streams</a>\"项目能够帮助我们在事件产生时实时消费它们，应用各种转换，执行流连接等，并且可以选择性地将新的数据表述写回主题中。</p><p></p><p>对于有状态和无状态的流应用来说，Kafka Streams都是理想方案，它能够实现基于时间的操作（例如，围绕给定的时间段对事件进行分组），并且考虑到了Kafka生态系统中普遍存在的可扩展性、可靠性和可维护性。</p><p></p><p>Kafka Stream由三个元素组成，即输入（源处理器）、输出（sink处理器）和处理器（流处理器）。</p><p></p><p>源处理器（Source processor）：源处理器代表一个Kafka主题。源处理器会发送事件到一个或多个流处理器中。</p><p></p><p>流处理器（Stream processor）：流处理器会将转换/逻辑应用于输入流中，比如连接、分组、计数、映射等。流处理器可以连接至另一个流处理器和/或sink处理器。</p><p></p><p>Sink处理器（Sink processor）：Sink处理器代表了输出的数据，它会连接至一个Kafka主题。</p><p></p><p>**拓扑结构（topology）**是由源、处理器和sink组成的无循环图，事件会传入到一个Kafka Streams中，该实例将开始拓扑结构的执行。</p><p></p><h2>Kafka Streams和Quarkus</h2><p></p><p>Quarkus使用Quarkus KStreams扩展实现与<a href=\"https://kafka.apache.org/documentation/streams/\">Kafka Streams</a>\"集成。</p><p></p><h3>Quarkus起步</h3><p></p><p>使用Quarkus最快捷的方式是通过<a href=\"https://code.quarkus.io/\">初始化页面</a>\"添加所需的依赖。每个服务可能需要不同的依赖，你可以选择Java 11或Java 17。为了实现Quarkus与Kafka Streams的集成，我们至少需要添加Kafka Streams扩展。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1Captura%20de%20Pantalla%202022-04-10%20a%20las%2022.34.28-1656674518432.jpeg\" /></p><p></p><h2>要开发的应用</h2><p></p><p>正如在本文开始时所提到的，在本系列的<a href=\"https://www.infoq.cn/article/cFpvXRLmZzJBGbzAeFu5\">第一部分</a>\"中，我们开发了一个影视流公司，它有两个Kafka主题，其中一个用来存储电影的列表，另外一个主题会在用户停止播放电影时存储用户所在的区域（事件的键），并且会以电影id和播放时间作为事件的值。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1topic-1656674518432.jpeg\" /></p><p></p><p>所有的这些逻辑都是在名为<a href=\"https://github.com/lordofthejars/movie-plays-kafka/tree/main/movie-plays-producer\">Movie Plays Producer</a>\"的生成者服务中创建的，该服务是使用Quarkus开发的。</p><p></p><p>除此之外，我还使用Quakus开发了一个<a href=\"https://github.com/lordofthejars/movie-plays-kafka/tree/main/movie-plays-consumer\">Movie Plays Consumer</a>\"服务，它会消费这两个主题的事件并且会在控制台上展示它们（并实现了HTTP服务器端事件）。</p><p></p><p>但是，这里没有对数据进行任何处理，它只是按照原样进行了接收。如果我们想要在movies和playtimemovies主题之间进行一下连接，在获取电影播放时长的时候得到电影的详细信息而不是id的话，那又该怎么办呢？</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1join-1656674518432.jpeg\" /></p><p></p><p>如果仅仅使用Kafka消息来实现这样的逻辑会变成一项很复杂的任务，因为我们需要在一个Map中存储Movie信息，并且在每个playedmovie事件发生时，进行匹配处理。</p><p></p><h3>Movie Plays KStream</h3><p></p><p>与其为每个用例手工编写代码，不如看一下如何使用Kafka Streams，以及它是如何与Quarkus集成来解决这个问题的。</p><p></p><h4>创建项目</h4><p></p><p>导航至Quarkus的<a href=\"https://code.quarkus.io/\">初始化页面</a>\"，并选择Apache Kafka Streams扩展来实现与Kafka Streams的集成。然后，选择RestEasy和RestEasy Jackson扩展实现事件从Java对象和JSON之间的编排/解排。同时，取消选中Started Code生成选项。</p><p></p><p>请参照下面的截图：</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1Captura%20de%20Pantalla%202022-04-10%20a%20las%2017.25.38-1656674518432.jpeg\" /></p><p></p><p>你也可以跳过这个手动的步骤并导航至<a href=\"https://code.quarkus.io/?a=movie-plays-kstreams&amp;nc=true&amp;e=kafka-streams&amp;e=resteasy-jackson&amp;e=resteasy\">Kafka Stream Quarkus Generator链接</a>\"，在这里，所有的依赖都已经选择好了。然后，点击Generate your application按钮，以下载应用骨架的压缩文件。</p><p></p><p>解压文件，并在你最喜欢的IDE中打开项目。</p><p></p><h4>开发</h4><p></p><p>当开发Kafka Stream应用时，我们需要做的第一件事就是创建<a href=\"https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/Topology.html\">Topology</a>\"实例，并定义源、处理器和sink。</p><p></p><p>在Quarkus中，我们只需要创建一个_CDI_类，这个类需要包含一个返回Topology实例的方法。</p><p></p><p>创建名为TopologyProducer的类，它将会实现从这两个主题消费事件并连接它们的逻辑。最后，生成的结果将会发送至一个sink处理器，该处理器以控制台输出的形式展示结果。</p><p></p><p>还有一个元素我们没有提到，在这些场景中它非常有用，那就是Kafka Tables。</p><p></p><p>一个主题可以包含具有相同键的多个事件。例如，我们可以使用某个键插入一个电影，然后我们可以使用相同的键创建一个新的事件来对电影进行更新：</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1topic-1656674518432.jpg\" /></p><p></p><p>但是，如果我们想要让movies主题与_playtimemovies_主题进行连接的话，那我们该选择使用哪个值为1的事件呢？第一个还是第二个？在这个具体的情况中，应该选择最新的那一个，因为它包含了电影的最新版本。为了获取每个事件的最新版本，Kafka Streams有一个_表_的概念（<a href=\"https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/kstream/GlobalKTable.html\">KTable/GlobalKTable</a>\"）。</p><p></p><p>Kafka Streams会浏览指定的主题，获取每个事件的最新版本，并将其放到一个表实例中。</p><p></p><p>KafkaStream扩展并不会像Kafka Messaging集成那样自动注册<a href=\"https://kafka.apache.org/23/javadoc/org/apache/kafka/common/serialization/Serdes.html\">SerDes</a>\"，所以我们需要在拓扑中手动注册它们。</p><p></p><p><code lang=\"java\">package org.acme;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Produces;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.GlobalKTable;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Printed;\n\nimport io.quarkus.kafka.client.serialization.ObjectMapperSerde;\n\n@ApplicationScoped\npublic class TopologyProducer {\n\n   private static final String MOVIES_TOPIC = \"movies\";\n   private static final String PLAY_MOVIES_TOPIC = \"playtimemovies\";\n\n   @Produces\n   public Topology getTopCharts() {\n\n       final StreamsBuilder builder = new StreamsBuilder();\n\n// 用于Movie和PlayedMovie的SerDes\n\n       final ObjectMapperSerde movieSerder = new ObjectMapperSerde&lt;&gt;(Movie.class);\n       final ObjectMapperSerde moviePlayedSerder = new ObjectMapperSerde&lt;&gt;(MoviePlayed.class);\n\n    // 为Movies主题创建一个Global Kafka Table\n\n       final GlobalKTable moviesTable = builder.globalTable(\n               MOVIES_TOPIC,\n               Consumed.with(Serdes.Integer(), movieSerder));\n\n    // 连接至playtimemovies主题的流，每当该主题有事件生成都会被该流所消费\n\n       final KStream playEvents = builder.stream(\n               PLAY_MOVIES_TOPIC, Consumed.with(Serdes.String(), moviePlayedSerder));\n\n// PlayedMovies使用区域作为键，对象作为值。我们对内容进行map操作，让电影的id作为key（以便于进行连接）并让对象继续作为值\n// 另外，我们使用movies表的键（movieId）以及流的键（在前面的map方法中，我们也将其变成了movieId）进行连接\n\n// 最后，结果会流向控制台\n\n    playEvents\n           .map((key, value) -&gt; KeyValue.pair(value.id, value)) // Now key is the id field\n           .join(moviesTable, (movieId, moviePlayedId) -&gt; movieId, (moviePlayed, movie) -&gt; movie)\n           .print(Printed.toSysOut());\n       return builder.build();\n\n   }\n}\n</code></p><p></p><p>Movie和MoviePlayed POJO包含了实现逻辑所需的属性：</p><p></p><p>Movie对象如下所示：</p><p><code lang=\"java\">package org.acme;\n\npublic class Movie {\n\n   public int id;\n   public String name;\n   public String director;\n   public String genre;\n\n   public Movie(int id, String name, String director, String genre) {\n       this.id = id;\n       this.name = name;\n       this.director = director;\n       this.genre = genre;\n   }\n}\n</code></p><p></p><p>MoviePlayed对象如下所示：</p><p><code lang=\"java\">package org.acme;\n\npublic class MoviePlayed {\n\n   public int id;\n   public long duration;\n\n   public MoviePlayed(int id, long duration) {\n       this.id = id;\n       this.duration = duration;\n   }\n\n}\n</code></p><p></p><p>运行Kafka Stream应用之前的最后一步是配置参数，其中最重要的是quarkus.kafka-streams.topics。它是一个主题列表，在拓扑结构开始处理数据之前，它们就要存在于Kafka集群中，这是一个前提条件。</p><p></p><p>打开src/main/resources/application.properties文件并添加如下的代码行：</p><p><code lang=\"java\">kafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=earliest\nkafka-streams.metrics.recording.level=DEBUG\n\nquarkus.kafka-streams.topics=playtimemovies,movies\n</code></p><p></p><p>现在，我们可以测试一下流了。我们启动在<a href=\"https://www.infoq.cn/article/cFpvXRLmZzJBGbzAeFu5\">上一篇文章</a>\"中开发的生产者。生产者的源码可以在<a href=\"https://github.com/lordofthejars/movie-plays-kafka/tree/main/movie-plays-producer\">这里</a>\"找到。</p><p></p><p>Quarkus KStreams集成了Quarkus DevServices。所以，我们不需要启动Kafka集群，也不需要配置它的位置，因为Quarkus Dev模式会处理好所有的事情。我们只需要记住在自己的计算机上要有一个运行中的容器环境即可，比如Podman或其他兼容OCI的工具。</p><p></p><p>在终端窗口中启动生产者服务：</p><p><code lang=\"java\">cd movie-plays-producer\n./mvnw compile quarkus:dev\n\n2022-04-11 07:49:31,900 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie Cruella played for 287 minutes\n2022-04-11 07:49:31,941 INFO  [io.quarkus] (Quarkus Main Thread) movie-plays-producer 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.7.3.Final) started in 4.256s.\n2022-04-11 07:49:31,942 INFO  [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated.\n2022-04-11 07:49:31,943 INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kafka-client, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]\n2022-04-11 07:49:32,399 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie Encanto played for 162 minutes\n2022-04-11 07:49:32,899 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie The Hobbit played for 255 minutes\n2022-04-11 07:49:33,404 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie Sing 2 played for 264 minutes\n2022-04-11 07:49:33,902 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie Encanto played for 28 minutes\n2022-04-11 07:49:34,402 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie Star Trek: First Contact played for 137 minutes\n2022-04-11 07:49:34,903 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie Star Trek: First Contact played for 277 minutes\n2022-04-11 07:49:35,402 INFO  [org.acm.mov.MovieKafkaGenerator] (executor-thread-0) movie The Hobbit played for 141 minutes\n</code></p><p></p><p>在另一个终端窗口中，启动我们刚刚开发的Kafka Stream代码：</p><p><code lang=\"java\">./mvnw compile quarkus:dev\n\n2022-04-11 07:54:59,321 INFO  [org.apa.kaf.str.pro.int.StreamTask] (movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1) stream-thread [movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1] task [1_0] Restored and ready to run\n2022-04-11 07:54:59,322 INFO  [org.apa.kaf.str.pro.int.StreamThread] (movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1) stream-thread [movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1] Restoration took 74 ms for all tasks [1_0]\n2022-04-11 07:54:59,322 INFO  [org.apa.kaf.str.pro.int.StreamThread] (movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1) stream-thread [movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING\n2022-04-11 07:54:59,324 INFO  [org.apa.kaf.str.KafkaStreams] (movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea-StreamThread-1) stream-client [movie-plays-kstreams-22c86daa-cd28-4956-9d0d-57b6b282a2ea] State transition from REBALANCING to RUNNING\n[KSTREAM-LEFTJOIN-0000000005]: 4, Movie [director=Craig Gillespie, genre=Crime Comedy, id=4, name=Cruella]\n[KSTREAM-LEFTJOIN-0000000005]: 2, Movie [director=Jonathan Frakes, genre=Space, id=2, name=Star Trek: First Contact]\n[KSTREAM-LEFTJOIN-0000000005]: 4, Movie [director=Craig Gillespie, genre=Crime Comedy, id=4, name=Cruella]\n[KSTREAM-LEFTJOIN-0000000005]: 1, Movie [director=Peter Jackson, genre=Fantasy, id=1, name=The Hobbit]\n[KSTREAM-LEFTJOIN-0000000005]: 4, Movie [director=Craig Gillespie, genre=Crime Comedy, id=4, name=Cruella]\n[KSTREAM-LEFTJOIN-0000000005]: 4, Movie [director=Craig Gillespie, genre=Crime Comedy, id=4, name=Cruella]\n[KSTREAM-LEFTJOIN-0000000005]: 3, Movie [director=Jared Bush, genre=Animation, id=3, name=Encanto]\n[KSTREAM-LEFTJOIN-0000000005]: 5, Movie [director=Garth Jennings, genre=Jukebox Musical Comedy, id=5, name=Sing 2]\n</code></p><p></p><p>在输出中打印的是事件（连接所生成的结果），其中键是movieId，值是movie本身。我们现在所具有的功能是，每当一个电影停止播放时，Kafka Stream会对其进行处理并以Movie全量信息的形式对其进行展现。</p><p></p><p>到目前为止，还不算复杂，对于这样的场景，你可能会想我们根本没有必要使用Kafka Streams。但是，我们再加一些需求，这样你就能看到它的强大之处了。</p><p></p><p>现在，我们不是在用户每次停掉电影的时候都生成事件，而是只对用户观看时间超过10分钟的电影发送事件。</p><p></p><p>我们可以使用filter方法按照持续时长进行过滤。</p><p><code lang=\"java\">playEvents\n       .filter((region, event) -&gt; event.duration &gt;= 10) // filters by duration\n       .map((key, value) -&gt; KeyValue.pair(value.id, value))\n       .join(moviesTable, (movieId, moviePlayedId) -&gt; movieId, (moviePlayed, movie) -&gt; movie)\n        .print(Printed.toSysOut());\n</code></p><p></p><p>重启应用，我们可以发现观看时长小于10分钟的电影将不会被处理。</p><p></p><p>我们发现，Kafka Streams有助于代码的整洁性，接下来，我们添加最后的需求。现在，我们对每部电影的播放时长并不感兴趣，而是对每部电影有多少次超过10分钟的播放感兴趣。</p><p></p><p>到目前为止，对事件的处理都是无状态的，因为事件都是遵循这样的步骤，即接收、处理并发送至sink处理器（也就是发送至一个主题或控制台输出），但是，为了统计某部电影播放的次数，我们需要在内存记住电影被播放了多少次，并且当任意用户再次观看超过10分钟的时候，要对这个统计数字递增一次。此时，事件的处理就要以有状态的方式进行了。</p><p></p><p>我们需要做的第一件事就是创建一个Java类，以存储电影的名称及其播放的次数。</p><p><code lang=\"java\">public class MoviePlayCount {\n   public String name;\n   public int count;\n\n   public MoviePlayCount aggregate(String name) {\n       this.name = name;\n       this.count++;\n\n       return this;\n   }\n\n   @Override\n   public String toString() {\n       return \"MoviePlayCount [count=\" + count + \", name=\" + name + \"]\";\n   }\n\n}\n</code></p><p></p><p>这是一个计数器类，它依然需要两样东西：</p><p></p><p>我们需要有一个地方存储这个类的实例，确保每次触发事件的时候，它不会被重置。每当_playtimemovies_主题中有事件触发时，调用aggregate方法的逻辑。</p><p></p><p>关于第一个问题，我们需要使用<a href=\"https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/state/KeyValueBytesStoreSupplier.html\">KeyValueBytesStoreSupplier</a>\"接口。</p><p></p><p><code lang=\"java\">public static final String COUNT_MOVIE_STORE = \"countMovieStore\";\n\nKeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(COUNT_MOVIE_STORE);\n</code></p><p></p><p>对于第二个问题，Kafka Streams有一个用来聚合结果的aggregate方法。</p><p></p><p>在我们的用例中，也就是每部电影播放时长超过10分钟的次数。</p><p></p><p><code lang=\"java\">// MoviePlayCount可以被序列化和反序列化\nObjectMapperSerde moviePlayCountSerder = new ObjectMapperSerde&lt;&gt;(MoviePlayCount.class);\n\n// 这是之前的连接操作，其中键是电影id，值是电影\n.join(moviesTable, (movieId, moviePlayedId) -&gt; movieId, (moviePlayed, movie) -&gt; movie)\n// 根据键对事件进行分组，在本例中，也就是电影的id\n.groupByKey(Grouped.with(Serdes.Integer(), movieSerder))\n // 聚合方法，如果MoviePlayCount已经创建的话，获取该对象（如果尚未创建的话，会创建该实例）并调用其aggregate方法，以对观看次数进行递增\n .aggregate(MoviePlayCount::new,\n             (movieId, movie, moviePlayCounter) -&gt; moviePlayCounter.aggregate(movie.name),\n              Materialized. as(storeSupplier)\n                  .withKeySerde(Serdes.Integer())\n                  .withValueSerde(moviePlayCountSerder)\n             )\n</code></p><p></p><p>重启应用，在控制台上将会展示电影播放的次数。</p><p></p><p>提示：要重启应用，只需在终端输入“s”，应用将会自动重启。</p><p></p><p>应用重启之后，控制台将会展示每部电影的状态。</p><p></p><p><code lang=\"java\">[KTABLE-TOSTREAM-0000000011]: 4, MoviePlayCount [count=13, name=Cruella]\n[KTABLE-TOSTREAM-0000000011]: 3, MoviePlayCount [count=11, name=Encanto]\n[KTABLE-TOSTREAM-0000000011]: 5, MoviePlayCount [count=14, name=Sing 2]\n[KTABLE-TOSTREAM-0000000011]: 2, MoviePlayCount [count=15, name=Star Trek: First Contact]\n[KTABLE-TOSTREAM-0000000011]: 1, MoviePlayCount [count=16, name=The Hobbit]\n[KTABLE-TOSTREAM-0000000011]: 2, MoviePlayCount [count=16, name=Star Trek: First Contact]\n[KTABLE-TOSTREAM-0000000011]: 3, MoviePlayCount [count=12, name=Encanto]\n[KTABLE-TOSTREAM-0000000011]: 2, MoviePlayCount [count=17, name=Star Trek: First Contact]\n[KTABLE-TOSTREAM-0000000011]: 5, MoviePlayCount [count=15, name=Sing 2]\n[KTABLE-TOSTREAM-0000000011]: 4, MoviePlayCount [count=14, name=Cruella]\n[KTABLE-TOSTREAM-0000000011]: 1, MoviePlayCount [count=17, name=The Hobbit]\n[KTABLE-TOSTREAM-0000000011]: 4, MoviePlayCount [count=15, name=Cruella]\n[KTABLE-TOSTREAM-0000000011]: 4, MoviePlayCount [count=16, name=Cruella]\n</code></p><p></p><p>交互式查询</p><p></p><p>因为我们将_sink_处理器设置成了System.out 流，所以聚合结果会流向控制台。</p><p></p><p><code lang=\"text\">.toStream()\n.print(Printed.toSysOut());\n</code></p><p></p><p>但是，我们也可以将结果流发往一个Kafka主题：</p><p></p><p><code lang=\"text\">.to(\"counter_movies\",                      Produced.with(Serdes.Integer(), moviePlayCountSerder)\n);\n</code></p><p></p><p>但是，如果我们感兴趣的不是每次对新事件的反应，而只是想查询特定的电影此时播放的次数，那又该怎么办呢？</p><p></p><p>Kafka Streams的交互式查询(interactive query)允许我们直接查询底层存储，以获取给定键相关的值。</p><p></p><p>首先，我们创建一个名为MoviePlayCountData的类来存储查询结果。按照这种方式，我们可以解耦Kafka Streams使用的类与应用中其他部分所使用的类。</p><p></p><p><code lang=\"java\">public class MoviePlayCountData {\n\n   private String name;\n   private int count;\n\n   public MoviePlayCountData(String name, int count) {\n       this.name = name;\n       this.count = count;\n   }\n\n   public int getCount() {\n       return count;\n   }\n\n   public String getName() {\n       return name;\n   }\n\n}\n</code></p><p></p><p>现在，创建名为InteractiveQueries的类来实现对状态存储（KeyValueBytesStoreSupplier）的访问并根据电影的id查询它被播放的次数。</p><p></p><p><code lang=\"java\">import javax.enterprise.context.ApplicationScoped;\nimport javax.inject.Inject;\n\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.errors.InvalidStateStoreException;\nimport org.apache.kafka.streams.state.QueryableStoreTypes;\nimport org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n\nimport static org.apache.kafka.streams.StoreQueryParameters.fromNameAndType;\n\nimport java.util.Optional;\n\n@ApplicationScoped\npublic class InteractiveQueries {\n\n   @Inject\n   KafkaStreams streams;\n\n   public  getMoviePlayCountData(int id) {\n       // 获取状态存储并根据电影id获取播放次数\n       MoviePlayCount moviePlayCount = getMoviesPlayCount().get(id);\n       // 如果有结果的话\n       if (moviePlayCount != null) {\n           // 将结果包装到MoviePlayCountData中\n           return Optional.of(new MoviePlayCountData(moviePlayCount.name, moviePlayCount.count));\n       } else {\n           return Optional.empty();\n       }\n   }\n\n   // 获取状态存储\n   private ReadOnlyKeyValueStore getMoviesPlayCount() {\n       while (true) {\n           try {\n               return streams.store(fromNameAndType(TopologyProducer.COUNT_MOVIE_STORE, QueryableStoreTypes.keyValueStore()));\n           } catch (InvalidStateStoreException e) {\n               // 忽略之，此时存储尚未就绪\n           }\n       }\n   }\n\n}\n</code></p><p></p><p>现在，我们可以添加一个简单的REST端点来运行该查询。</p><p><code lang=\"java\">import java.util.Optional;\n\nimport javax.inject.Inject;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\nimport javax.ws.rs.core.Response;\nimport javax.ws.rs.core.Response.Status;\n\n@Path(\"/movie\")\npublic class MovieCountResource {\n\n   // 注入前文的类进行查询\n   @Inject\n   InteractiveQueries interactiveQueries;\n\n   @GET\n   @Path(\"/data/{id}\")\n   public Response movieCountData(@PathParam(\"id\") int id) {\n       Optional moviePlayCountData = interactiveQueries.getMoviePlayCountData(id);\n\n       // 根据结果判定返回值还是404\n       if (moviePlayCountData.isPresent()) {\n           return Response.ok(moviePlayCountData.get()).build();\n       } else {\n           return Response.status(Status.NOT_FOUND.getStatusCode(),\n                   \"No data found for movie \" + id).build();\n       }\n\n   }\n}\n</code></p><p></p><p>Kafka Stream实现的模式如下图所示：</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/articles/quarkus-with-kafka-streams/en/resources/1topology-1656674518432.jpeg\" /></p><p></p><h3>扩展</h3><p></p><p>Kafka Streams应用可以进行扩展，所以流会分布到多个实例中。在这种情况下，每个实例都包含聚合结果的一个子集，所以要想获得总的聚合结果，我们需要通过将REST API重定向到另外的实例来获取其他实例的数据。</p><p></p><p>Kafka Streams提供了一个<a href=\"https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/KafkaStreams.html#allMetadataForStore(java.lang.String)\">API</a>\"，可以知道要请求的数据是在本地Kafka Streams存储中还是在其他的主机中。</p><p></p><p>虽然这个过程并不复杂，但已经超出了本文的讨论范围。</p><p></p><h2>结论</h2><p></p><p>到目前为止，我们已经看到，将Quarkus应用连接到Apache Kafka并生产和消费主题中的消息/事件是很容易的。此外，还看到Kafka Streams让我们不仅可以消费Kafka中消息，还能够实时处理它们，进行转换、过滤等操作，例如以同步的方式消费结果数据。 这是一项强大的技术，当需要处理的数据不断变化时，它可以轻松扩展，提供实时的体验。</p><p></p><p>但是，我们还没有解决该架构的最后一个问题。通常情况下，数据并不是存储在一个地方。电影信息可能存储在关系型数据库中，而电影的播放信息则存储在一个Kafka主题中。那么，该如何保持这两个地方的信息更新，以便Kafka Streams能正确地连接数据呢？</p><p></p><p>这里有一个缺失的部分，名为Debezium的项目可以帮助我们解决这个问题。我们将用一整篇文章来介绍Debezium和Quarkus，敬请持续关注。</p><p></p><p>作者简介：</p><p>Alex Soto是红帽公司的开发者体验总监。他对Java领域、软件自动化充满热情，他相信开源软件模式。Soto是Manning的《Testing Java Microservices》和O’Reilly的《Quarkus Cookbook》两本书的共同作者，他还是多个开源项目的贡献者。自 2017 年以来，他一直是Java Champion，是国际演讲者和Salle URL 大学的教师。你可以在 Twitter 上关注他（<a href=\"https://twitter.com/alexsotob\">Alex Soto </a>\"），随时了解 Kubernetes 和 Java 领域的动态。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/quarkus-with-kafka-streams/\">Kafka Streams and Quarkus: Real-Time Processing Events</a>\"</p><p></p><p>相关阅读：</p><p>本系列第一部分：<a href=\"https://www.infoq.cn/article/cFpvXRLmZzJBGbzAeFu5\">使用 Apache Kafka 实现 Quarkus 的反应式消息</a>\"</p>",
    "publish_time": "2022-08-25 11:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]