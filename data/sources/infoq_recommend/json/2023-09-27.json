[
  {
    "title": "Google Vertex AI 推出搜索对话式界面，简化AI建模",
    "url": "https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE",
    "summary": "<p>在<a href=\"https://cloud.withgoogle.com/next?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">Google Cloud Next</a>\"大会上，谷歌<a href=\"https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-search-and-conversation-is-now-generally-available/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">介绍了其企业人工智能平台Vertex AI</a>\"的新功能，这个平台旨在实现更高级的用户工作流。</p><p></p><p>在上次<a href=\"https://blog.google/technology/developers/io-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">Google I/O大会</a>\"上作为技术预览发布之后，谷歌扩展了Vertex AI Search和Conversation功能，并宣布其普遍可用。</p><p></p><p>借助Vertex AI Search，开发人员可以从各种企业资源中检索信息，例如文档存储库、数据库、网站和其他类型的应用程序。谷歌表示，开发人员可以很轻松地构建搜索引擎，并基于这些信息与客户展开互动。</p><p></p><p></p><blockquote>Vertex AI提供了一个简单的编排层，将企业数据与生成式基础模型以及会话AI和信息检索技术结合起来。</blockquote><p></p><p></p><p>Vertex AI Conversation可用于创建声音自然、类人的聊天机器人。例如，开发人员可以基于网站、企业文档、常见问题解答、电子邮件等创建聊天机器人。</p><p></p><p></p><blockquote>Vertex AI可以帮助开发人员将确定性工作流与生成式输出相结合，将基于规则的流程与动态AI相结合，构建出迷人且可靠的应用程序。</blockquote><p></p><p></p><p>基于谷歌宣布的这些新功能构建的智能应用程序不仅仅是检索重要信息那么简单，而是可以代表用户做出动作。</p><p></p><p>新功能支持处理后续问题，无需重新开启对话，这要归功于多轮搜索。通过扩展和数据连接器与第三方应用程序进行更紧密的集成，从而可以连接到各种应用程序，如MongoDB、Datastax、Salesforce、JIRA等。基于企业数据的生成式输出增加了返回结果质量置信度。</p><p></p><p>Vertex AI Search还可以与矢量搜索相结合，利用其寻找“类似”解决方案的能力来支持语义搜索、个性化推荐、多模态搜索等。</p><p></p><p>Vertex AI Search将在未来支持访问控制，确保用户只能看到他们有权看到的信息和访问其他功能，如引用、相关性评分和摘要。</p><p></p><p>Vertex AI是一个面向非机器学习开发人员的平台。几个月前，<a href=\"https://www.infoq.com/news/2023/06/ai-ml-data-news-june12-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">Google为Vertex AI提供普遍可用的生成式AI支持</a>\"，包括基于<a href=\"https://ai.google/discover/palm2/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">PaLM 2</a>\"的文本模型，文本Embeddings API和<a href=\"https://cloud.google.com/model-garden?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">Model Garden</a>\"中的基础模型。</p><p></p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM\">https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/</a>\"</p>",
    "publish_time": "2023-09-27 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国信通院发布《中国数字化转型数实融合 IOMM 综合指数（2023)》，展现重点行业与企业数字化发展特征",
    "url": "https://www.infoq.cn/article/xH9IF2oAtLk0G7Q2AgjV",
    "summary": "<p>2023 年 9 月 13 日 -14 日，由中国信息通信研究院（简称“中国信通院”）和中国通信标准化协会联合主办的“2023 数字化转型发展大会暨首届数字原生大会”在北京召开。在 14 日上午的主论坛上，中国信通院云计算与大数据研究所副所长栗蔚对《中国数字化转型数实融合 IOMM 综合指数（2023 年）》进行解读。</p><p></p><p>栗蔚指出，随着数字经济在我国经济发展中所占比重及增速连年攀升，其作为经济“稳定器”与“加速器”的作用也愈加凸显。2022 年我国数字经济规模达 50.2 万亿元，占 GDP 比重 41.5%，并已连续 11 年显著高于同期 GDP 名义增速。而数字经济与实体经济的融合（简称“数实融合”）成为数字经济发展的核心任务，在国家战略性文件中被多次强调。</p><p></p><p>中国信通院聚焦数实融合，从宏观的经济与产业趋势洞察，到微观的业务与技术融合方法论总结实践，不断深化研究成果。继 2021 年发布“企业数字化转型发展双曲线”，2022 年推出“企业数字化转型发展双象限”之后，在 2023 年进一步提出《中国数字化转型数实融合 IOMM 综合指数》，以评价我国重点行业及其代表性企业的数字化发展水平与转型成效，指数分为“企业数字化转型 IOMM 综合评价指数”（ 简称“企业指数”）和“行业数字化转型 IOMM 综合评价指数”（ 简称“行业指数”）两部分。</p><p></p><p>栗蔚表示，通过以上指数能更加直观的呈现出不同企业及相关重点行业的整体数字化发展水平。在企业指数中，三大运营商的新兴业务增速明显，2023 上半年同比增长 19.2%，同时，以招商局集团、五矿集团、南方电网、国家电网等为代表的大型央国企，通过转型升级来提升核心竞争力，实现自身能力跃迁与运营模式创新。在行业指数中，通信行业持续领跑，积极践行数字中国战略；金融行业稳中提速，强化风险管理，推动普惠金融；制造业发展步伐加快，柔性生产、无人工厂、数字孪生产线等场景落地；医疗行业提升迅速，医疗信息系统建设加快，技术与医疗服务的结合更加紧密。</p><p></p><p>栗蔚在演讲中进一步分析了通信、金融、汽车制造、医疗等典型行业在地域发展上的特点，并对过去一年中重点行业的主要赋能型企业进行了分析，以技术服务能力和行业赋能价值作为衡量维度，分别对处于“引领者”、“专注者”、“竞争者”及“创新者”象限中的赋能企业特点进行了总结。</p><p></p><p>更多精彩，敬请阅读解读 PPT</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6b/6bf1b6d96fee0a6d077c6f9ee123b939.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/13/135420165c0d39bae45afd864d9f0bc3.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0d/0d07d1c5ea711a1e54fec9cbdb806cfd.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9b/9bf8a1fafee791b12d36c93e075cec06.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb6a8cc45c69e18350d8df70ddcdc0ca.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/02/02ed868e9d18b5884db609c95bafba05.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7c/7c606788330ba2a9a02543980d7188a3.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b0/b00666728d5279fff90f5aeb8aed320f.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/66/6669c0ed09306286c26176dff74dd006.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/51/51313acfa8e7d4f53de1902664d4d664.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e3/e31fc6a9856e6894d0743db8e2bcc263.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ffe431c10a2f682f64039ed76ac44c42.png\" /></p><p></p>",
    "publish_time": "2023-09-27 09:51:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "千亿也不够花！OpenAI 想让员工出售股票来筹资，而最大投资人微软正在“去GPT”",
    "url": "https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx",
    "summary": "<p>据报道，OpenAI正与投资者讨论可能的股票出售事宜，这家人工智能初创公司寻求以800亿—900亿美元（以当前汇率换算，约为5842.48亿-6572.79亿人民币）的估值出售股票，这一估值几乎达到今年早些时候的三倍。</p><p>&nbsp;</p><p><a href=\"https://www.wsj.com/tech/ai/openai-seeks-new-valuation-of-up-to-90-billion-in-sale-of-existing-shares-ed6229e0\">《华尔街日报》</a>\"称，此次交易预计将允许员工出售所持股份，而不是以公司发行新股的形式来筹集额外资本。知情人士称，OpenAI已经开始说服投资者，并表示今年OpenAI 营收预计将达到10亿美元，2024年将再增加数十亿美元。</p><p>&nbsp;</p><p>10亿美元的数字与8月份媒体爆出来的数字一致。据报道，OpenAI每月收入8000万美元，高于2022年全年的2800万美元。ChatGPT Plus是其2月份推出的每月20美元的ChatGPT付费版本，推动了OpenAI的收入增长。</p><p>&nbsp;</p><p>另一方面，股票出售可以让员工不必等到公司上市就能了解自己的股权价值，可以帮助公司吸引顶尖人才，并产生流动性，同时也将为OpenAI带来新的估值。800亿美元或更高的估值将使OpenAI成为全球估值最高的初创公司之一，仅次于字节跳动和马斯克的SpaceX。</p><p></p><h2>融到的钱多，但花的钱更多</h2><p></p><p>&nbsp;</p><p>报道称，OpenAI 的目标是向硅谷投资者出售价值数亿美元的现有股票。今年4月，OpenAI从红杉资本(Sequoia Capital)、Andreessen Horowitz、Thrive和K2 Global等投资方获得了3亿多美元的融资，估值达到290亿美元。这与微软今年早些时候宣布的一项大型投资无关，该投资已于今年1月完成，投资规模约为100亿美元。据不完全统计，在OpenAI接受微软的100亿美元投资之前，它在成立七年多时间内已经收到了40亿美元的投资。至此，OpenAI 累积融资金额已经有143亿美元（以当前汇率换算，约合1044.62亿人民币）。</p><p>&nbsp;</p><p>尽管早在5月份就有报道称OpenAI正在寻求筹集更多资金，但如果出售股份的措施继续下去的话，将不会为OpenAI提供额外的运营资金，只是允许其员工剥离部分股份。</p><p>&nbsp;</p><p>5月<a href=\"https://futurism.com/the-byte/openai-losing-money-chatgpt\">The Information</a>\"报道，三位了解OpenAI财务状况的人士透露，由于去年开发ChatGPT并从谷歌招聘关键员工，OpenAI的亏损大约翻了一番，达到5.4亿美元左右。咨询公司 SemiAnalysis 的首席分析师 Dylan Patel 预计，每天运行 ChatGPT 的成本为 70 万美元。</p><p>&nbsp;</p><p>根据<a href=\"https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/\">《财富》</a>\"的披露，该公司 2022 年5.445 亿美元的总支出中，计算和数据支出 4.1645 亿美元、员工支出 8931 万美元、其他未指明具体项目的运营支出 3875 万美元。这些成本在获得微软年初100亿美元投资前就已经积累起来了。</p><p>&nbsp;</p><p>OpenAI 对产品升级更新的脚步一直没有停下，很明显巨额成本投入也在继续：</p><p>&nbsp;</p><p>9月21日凌晨，OpenAI宣布其文生图工具DALL·E即将升级至DALL·E 3，并将原生集成至ChatGPT中。相比去年发布的DALL·E 2，在提示词相同的情况下，DALL·E 3对文字的理解程度及生成的图像质量显著提升。时常被诟病的“无法在图像上生成文字”的问题，也在这次升级中得到了解决。</p><p>&nbsp;</p><p>9月26日，OpenAI 推出了多模态ChatGPT，除了通过常见的文本框交互外，还将向Plus和Enterprise用户推出语音和图像，用户可以用语音对话或向 ChatGPT 显示正在谈论的内容。语音将在iOS和Android上推出（在您的设置中选择加入），图像将在所有平台上提供。</p><p>&nbsp;</p><p>此外，8 月 16 日，OpenAI 还发布公告称收购了Global Illumination的团队，这也是 OpenAI 自 2015 年成立以来的首次对外收购，但并未公开交易涉及金额。</p><p>&nbsp;</p><p>尽管在OpenAI于2月推出付费版聊天机器人后，公司收入有所增长，但随着越来越多的客户使用其人工智能技术，以及该公司对该软件未来版本进行培训，这些成本可能会继续上升。实际上，微软和其他最近的投资者承担了上述大部分成本，但如果OpenAI不能很快实现盈利，他们可能会停止投入。</p><p>&nbsp;</p><p>与此同时，8月24日有媒体报道称，OpenAI首席执行官Sam Altman下半年将奔赴阿联酋首都阿布扎比等地寻求融资，此次融资的规模巨大，不低于1000亿美元。</p><p>&nbsp;</p><p>据悉，Altman 对VC讲的故事不限于AGI通用人工智能，他表示OpenAI的目标是要实现Super intelligence（超级智能），例如可在一个月内攻克癌症。但目前OpenAI离这一目标还非常遥远，而OpenAI为实现该目标所需要的资金规模不可想象。</p><p>&nbsp;</p><p>目前，Altman还没有通过上市筹资的打算。今年6月，Altman表示，自己不想被公开市场、华尔街等起诉，所以对上市没那么感兴趣。他解释称，当开发AI时，可能会做出一些让公开市场投资者看来非常奇怪的决定。</p><p>&nbsp;</p><p></p><h2>最大的投资人，也扛不住了</h2><p></p><p>&nbsp;</p><p>微软今年早些时候向OpenAI投资100 亿美元后，持有了其 49% 的股份。作为交易的一部分，OpenAI 承诺与其合作并将其人工智能软件与微软的产品集成，以换取用于训练和运行其模型的计算资源。微软确实也竞相在自己的大多数软件产品中内置人工智能功能，包括基于GPT-4的Windows Copilot。但如今，大模型也给微软带来了压力。</p><p>&nbsp;</p><p>该公司担心，随着Windows在全球拥有超过10亿用户，运行这些人工智能功能的成本可能会迅速增长。而微软不想放弃其新人工智能产品带来的经济效益，所以正在寻找低成本的替代方案。</p><p>&nbsp;</p><p>据知情人士透露，最近几周，负责管理微软 1500 名研究人员的Peter Lee，指示其中的许多人开发对话式人工智能，其性能可能不如GPT-4，但规模更小，运营成本也低得多。</p><p>&nbsp;</p><p>据报道，微软的研究小组对于开发像 GPT-4 这样的大型人工智能模型并不抱有幻想。该团队没有与 OpenAI 相同的计算资源，也没有大量的人工来反馈LLM回答的问题以便工程师改进。</p><p>&nbsp;</p><p>微软转向更高效人工智能模型的努力目前还处于早期阶段，不过据说该公司已经开始在Bing Chat等服务中测试内部开发的模型。</p><p>&nbsp;</p><p>微软搜索部门负责人Mikhail Parakhin此前曾表示，Bing Chat 100%依赖GPT-4的创造性和精确性模式。然而，在其平衡模式中，它使用了一种名为Prometheus的新模型和图灵语言模型。后者不像GPT-4那么强大：它们可以识别并回答简单的问题，但当它们面临更棘手的问题时，便将这些问题传递给GPT-4。</p><p>&nbsp;</p><p>在编码方面，微软最近公布了其13亿参数的Phi-1模型，据说该模型是在“教科书质量”的数据上进行训练的，可以以更有效的方式生成代码，但还没有完全达到GPT-4的标准。</p><p>&nbsp;</p><p>该公司还在研究其他人工智能模型，如Orca基于Meta的开源Llama-2模型。据介绍，Orca的性能接近于OpenAI的模型，尽管它更小，资源消耗更少。</p><p>&nbsp;</p><p>报道称，微软的人工智能研究部门有大约2000张来自英伟达公司的显卡可供使用，Lee&nbsp;现在已经下令将其中的大多数显卡用于训练专注于执行特定任务的更有效的模型，而不是更通用的GPT-4。</p><p>&nbsp;</p><p>不可否认的是，OpenAI 和其他开发商，包括谷歌和 Anthropic，在开发高级LLM方面领先于微软，但微软或许能够以极低的成本参与到构建类似 OpenAI 软件质量的模型竞争中。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://siliconangle.com/2023/09/26/openai-reportedly-exploring-share-sale-80-90b-valuation/\">https://siliconangle.com/2023/09/26/openai-reportedly-exploring-share-sale-80-90b-valuation/</a>\"</p><p><a href=\"https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending\">https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending</a>\"</p><p><a href=\"https://www.infoq.cn/article/JAqkPkkgMFvV0JvcFMtj\">https://www.infoq.cn/article/JAqkPkkgMFvV0JvcFMtj</a>\"</p><p><a href=\"https://siliconangle.com/2023/09/26/microsoft-hedges-bets-seeking-cost-effective-ai-models/\">https://siliconangle.com/2023/09/26/microsoft-hedges-bets-seeking-cost-effective-ai-models/</a>\"</p>",
    "publish_time": "2023-09-27 14:10:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AI 时代下的 SUSE 新洞察：无处不在的边缘计算革命即将到来",
    "url": "https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf",
    "summary": "<p>随着生成式 AI 等前沿技术的迅速发展，一场全新的技术变革正在悄然引领数字创新的浪潮。这种变革为企业未来发展带来了无限可能，使其能够高效应对 IT 挑战、提高生产效率，创造出前所未有的商业价值。然而在这场变革的另一面，“数字信任”已经成为不可或缺的标准，各大厂商都希望通过严格的网络安全实践和强大的数据保护措施，为企业数字创新提供坚实的保障，确保其能够在充满挑战的数字时代稳健发展。SUSE 作为全球范围内创新、可靠且安全的企业级开源解决方案领导者，正在持续推动其在安全、稳定、可靠、可互操作等方面的技术进步，以帮助企业应对不断变化的市场需求。</p><p></p><p>由<a href=\"https://www.infoq.cn/article/2fHFRAFOYTE5QZaXbU43\"> SUSE</a>\" 主办的年度数字创新峰会——SUSECON 深圳 2023（下文称“本届峰会”）于 9 月 22 日圆满落幕。通过本届峰会，我们可以非常清晰地看到 SUSE 在过去一年里的探索，SUSE 在边缘计算方面联合合作伙伴，做出了多种尝试。同时，愈演愈烈的生成式 AI 为整个技术市场带来了无限机会，而在此浪潮下的 SUSE 也敏锐地把握了这一新兴技术的潜力，积极投身于相关研究和开发工作中。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8c/8c5652646de05a0a8fb32c1ad857fc86.jpeg\" /></p><p>图：SUSE 大中华区总裁陈毅威发表主题演讲</p><p></p><h2>一、边缘计算无处不在，对基础设施提出更高要求</h2><p></p><p></p><p>随着 5G、物联网、云计算等技术的快速发展，边缘计算得到了广泛应用。由于物联网设备的数量不断增加，数据产生和传输的规模也在快速增长，传统的云计算模式已经无法满足低延迟、高带宽、高可靠性的需求，于是将计算任务分配到网络边缘的设备上成为了解决这个问题的一种有效方法。</p><p></p><p>与云计算相比较，边缘计算就近布置，可以被理解为云计算的下沉。边缘计算通过将数据在靠近设备或终端进行计算和处理，大大提高了数据处理的效率和实时性，同时减轻了中央服务器和网络的负载。此外，边缘计算和云计算需要结合使用，例如在大数据分析中，边缘计算可以处理本地设备上的数据，云计算则可用于存储和分析大量数据。但这也意味着，边缘计算对基础设施有更高的要求，它需要在网络边缘的位置处理和存储数据、部署大量的小型数据中心和设备，设备规模、能源消耗、运维管理和安全性等就成为了重点考虑因素；同时，边缘设备的计算和存储资源往往比中心服务器更为有限，这也限制了其能够处理的数据量和计算的复杂度，边缘计算的能耗和硬件资源效率也面临着挑战。</p><p></p><p>为了克服这些挑战并充分发挥边缘计算的优势，SUSE 与瞬优智慧达成合作，联合发布了“鼎瞬 Peerless 实时业务协同解决方案平台”，广泛应用于工业制造、物流运输和智能城市等领域。这是一款面向企业级客户的云边端协同平台，集软、硬件于一体，在数字化全生命周期提供从云端至近边缘端到边缘端的全面支持，以实现各业务系统（如物联网终端设备等）海量数据的实时感知、实时处理、实时分析、实时决策。</p><p></p><p>值得一提的是，该解决方案整合了 SUSE 的分布式基础架构解决方案，包括 SUSE Linux Enterprise Server、SUSE Manager、Rancher 和 NeuVector，为云平台的稳定基础架构提供了强有力的支撑；SUSE Linux Enterprise Micro、Rancher、K3s、Longhorn 也为解决方案中的边缘智能网关、近边缘智能一体机提供了高可用的基础运行架构。</p><p></p><p>当前，SUSE 正在与联想、中科云谷等行业领导者一起探索边缘的更多可能。SUSE 和联想将充分发挥各自在技术、产品、市场、生态等方面的优势，以边缘硬件、OS、软件定义基础设施为核心数字底座，合力打造边缘原生的全栈式企业级解决方案。SUSE 与联想在边缘计算领域的合作主要分为三个阶段：</p><p>第一阶段：基于 SUSE 现有的边缘计算产品组合，联想提供边缘硬件设备算力支持，共同打造边缘计算节点。目前小盒子（网关）的适配工作已基本通过测试，服务器适配工作正在进行中。第二阶段：联想推出自有边缘计算平台，结合 SUSE 的操作系统，打造商业化边缘计算平台和软件平台。目前第一阶段和第二阶段同步进行，第二阶段的工作重点将聚焦在边缘服务器和边缘云平台的适配上。第三阶段：双方将在边缘计算产品系列中进行更深度的融合，将 SUSE 的操作系统、Rancher、NeuVector 等组件纳入联想的边缘操作盒一体机产品体系。同时，双方将实现统一架构的管理运维框架，包括边缘管理平台与 SUSE 和管理平台的结合，以实现全栈式全方位全要素的管理和监控运维能力。</p><p></p><p>此外，在本次峰会上，SUSE 与中科云谷达成了战略合作，联合推出了中科云谷云原生技术平台。该平台基于 Rancher 容器云、微服务、DevOps 等云原生技术构建，提供丰富、通用的数字化转型共性组件。更重要的是，中科云谷目前已经完成了对 SUSE 零信任容器安全平台 NeuVector 的全面测试，近期将正式上线生产环境；同时，双方正在探索将 SUSE Edge 边缘解决方案纳入平台以帮助客户应对复杂的边缘环境。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/87/879c61c1f6a5c3f9518e4ce7f66b6b13.jpeg\" /></p><p></p><p>客观地说，关于边缘计算的未来发展，其实就像中国科学院王义博士在本届峰会上分享的那样，“全球工业、农业、医疗等重要领域对边缘计算系统的需求逐渐清晰，并形成了大规模的市场。与传统操作系统相比，边缘计算操作系统需要在分布式协同、实时性、可用性、安全性、高能效等方面为用户提供更先进的系统能力。”边缘计算正在高速发展，已经成为了现代数字生态系统的重要组成部分，即将像水电一样无处不在。为了满足日益增长的需求，企业需要构建更加稳定、灵活和可扩展的边缘计算基础设施，为未来在数字化世界的发展提供强有力的支撑。</p><p></p><h2>二、在边缘运行云原生面临着许多挑战</h2><p></p><p></p><p>边缘计算应用规模在增加，技术挑战及企业用户需求也随之增加。云边基础设施存在差异，云原生能力直接下沉应用到边缘时，除了需要提供等同于中心的性能指标、安全隔离、容灾自治、架构感知等能力，还需要不断完善云边以及边边高速通道建设等，进而提升建设难度系数。“小而多”的边缘节点，资源复用率低，这就需要云原生技术能够根据资源池化的能力和资源性能做灵活的弹性调度，以实现更高效的资源利用。</p><p></p><p>要知道，云原生技术对部署环境有明确的要求，需要对边缘侧的海量异构资源进行灵活的适配，这就意味着用户需要对各种硬件和操作系统进行抽象，并为上层应用提供统一的接口，以实现资源池化和资源性能的灵活调度。开发者在日常工作中最常用的容器集群管理工具 Kubernetes 便是其中一个代表——边缘侧设备的 CPU、内存等计算资源配置通常较低，主要用于应用自身，难以分配更多资源供 Kubernetes 的中间层平台使用；在边缘环境网络不稳定的情况下，Kubernetes 自身难以稳定运行。而且虽然社区中目前已经有了轻量化的 Kubernetes，但多为单节点架构，并非是为了运行多节点的生产级环境而设计的，难以提供生产级的高可用服务。</p><p></p><p>于是，为了解决 Kubernetes 能够运行在边缘计算环境的挑战，让用户能通过下一代嵌入式边缘设备获得成功并具备扩展能力，SUSE 开发出了轻量化的 Kubernetes 发行版——K3s，轻量化的 Linux 操作系统 SUSE Linux Enterprise Micro, 在 Rancher2.7 中已经实现了对 K8s、K3s 以及基于 OCI 格式的 SUSE Linux Enterprise Micro 的统一管理，并与 SUSE 的超融合基础架构解决方案 Harvester 和 SUSE 的零信任容器安全平台 NeuVector 组合，推出了 SUSE Edge 云原生边缘管理解决方案，为从应用程序到 K3s 再到操作系统的整个堆栈进行了安全策略的无缝集成。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8e/8e600725ef9e5ba91eb5cd045be378fe.png\" /></p><p></p><p><a href=\"https://www.infoq.cn/article/HmppHMJseJVj7sZkR7Te\">SUSE</a>\" Edge 能够对无人监控的边缘环境进行低成本的统一管理，用户可通过统一的操作面板管理 Kubernetes 和底层操作系统，大幅降低了运维、部署工作的复杂性。SUSE Linux Enterprise Micro 是专为边缘环境中的容器化工作负载量身打造的轻量级操作系统，其安全可靠、无需维护的特性，可以实现简单而重要的边缘设备管理任务自动化运行。基于此，开发人员能够快速完成测试和编程，构建涵盖可穿戴设备、智慧城市、交通运输等众多领域的各类应用程序。</p><p></p><p>事实上，无论是通用的边缘场景，还是电信、汽车、卫星等需要额外功能的各类边缘场景，SUSE 都能基于不同用例提供完全契合客户需求的边缘解决方案。全球领先的家居建材用品零售商 Home Depot 便非常欣赏 K3s 的简易性，只需要运行单个二进制文件就可以随时拥有一个经 CNCF 认证的 Kubernetes 发行版。店内应用程序一直是容器化的 Home Depot 将所有 2300 多个零售店都转到了基于 Rancher +K3s 的新架构中，以避免手动维护容器化应用程序的可用性。</p><p></p><p>然而，云原生技术面临的技术挑战不仅在于 Kubernetes 集群的管理，安全问题同样不容忽视。作为 SUSE Edge 边缘解决方案的重要组成部分，SUSE 的零信任容器安全平台 NeuVector 可通过集中化的企业级扫描、自动扩展扫描工具以及新版 Kubernetes (1.25+) Pod 安全准入，实现更高效、更强大的多集群漏洞扫描和准入控制。据悉，NeuVector 拥有十几项技术专利，包括深层数据包检查和行为学习，可以识别容器的行为，仅允许经过批准的白名单在网络链接、进程访问和文件存取方面进行操作；在运行时还提供完整的攻击检测和预防功能，主动保护生产环境。</p><p></p><p>SUSE 安全产品战略副总裁黄飞在本次峰会上发表了题为《建立数字信任，守护开源安全》的主题演讲，正如他所说的那样：“从分布式系统架构到容器技术，SUSE 不仅帮助企业应对云原生环境中的安全挑战，还通过零信任安全策略和供应链安全工具，确保企业业务和服务的安全性。同时，SUSE 还积极参与国际通用的安全标准认证，为企业提供更加可靠的安全保障。总之，SUSE 一直致力于在安全领域为企业提供全面的解决方案。”</p><p></p><h2>三、AI 为全行业数智化转型带来了机会</h2><p></p><p></p><p>SUSE 大中华区总裁陈毅威在本届峰会上的演讲《重塑·创新的力量》中提到，“SUSE 是一家德国公司，跟 SUSE 合作，你会体验到它身上散发的强烈的德国工匠精神，即对自身产品质量的极高要求。SUSE 的文化基因就是开源，SUSE 在各领域都在不断探索。”事实就是这样，自 1992 年 SUSE 在德国纽伦堡成立，至后来 2020 年收购 Rancher、2021 年收购 NeuVector，SUSE 始终走在科技前沿，借助前沿科技助力客户更好创新。近期 ChatGPT 引爆了新一波 AI 热潮，SUSE 再次“走在了路上”。</p><p></p><p>ChatGPT 将生成式 AI 的能力带入人工智能主流应用领域，在今年初其月活跃用户就达到了 1 亿，打破了用户群增长最快的记录。AI 技术已经进入了大规模应用和普及阶段，AI 3.0 时代来临。在这种背景下，生成式 AI 及其多种应用正在重塑行业，帮助各领域实现更快、更准确的数据处理和分析，从而提高工作效率和决策精度，为企业数智化转型提供了更多的机会和可能性，助力其获得更多的商业机会和收益。</p><p></p><p>然而 AI 技术的发展往往需要进行复杂的模型训练和推理，需要大量的计算资源和存储资源，这对算力提出了更高要求，也为很多厂商提供了新的增长点。</p><p></p><p>SUSE 已经迈出了 AI 应用的第一步——在 Rancher Prime 上开发了一个 AI 助手，为用户提供自助、便捷的用户服务，同时提供准确、有效的信息。作为老牌 Linux 技术探索者，SUSE 在 Linux 方面也做了很多 AI 方面的探索。比如今年 7 月，SUSE 发布最新旗舰版企业级 Linux 平台 SUSE Linux Enterprise 15 Service Pack 5（SLE 15 SP5），可以提供对 AI/ML 工作负载至关重要的高性能计算能力，并与 Rancher 协同工作。它是第一个支持全范围机密计算的 Linux 发行版，可以保护在公有云和边缘处理的客户数据，允许客户在任意环境中运行完全加密的虚拟机，安全属性得到了大大提升。</p><p></p><p>使用超级计算机来进行顶级 AI 工作的用户、处理大型数据集和复杂 ML 过程的用户非常喜欢 SLE 15 SP5，就是因为 SUSE Linux Enterprise 拥有强大的安全功能和工具来保护 AI/ML 工作负载和数据。它提供了安全引导、访问控制、加密和审核等功能，以确保符合行业法规（如 GDPR 和 HIPAA）。</p><p>对 AI 的探索推动了 SUSE 的技术创新，不仅增强了 SUSE 的技术实力，也为其在市场上的竞争力提供了有力的保障。正如 SUSE 大中华区总裁陈毅威所说的，他十分肯定，AI 将是 SUSE 非常好的发展契机。“别忘了，硬件和应用的中间，还需要一个更为成熟的操作系统作为支撑。”</p><p></p><h2>四、写在最后</h2><p></p><p></p><p>在本届峰会上，陈毅威和 SUSE 亚太 CTO &nbsp;Vishal Ghariwala 都表达了同一个观点——随着数字化转型的加速，企业需要不断适应市场的变化，提高自身的竞争力，开源技术为企业提供了实现这一目标的重要途径。通过引入先进的开源解决方案，企业可以降低研发成本，加速产品上市时间，提高服务质量，从而在激烈的市场竞争中脱颖而出。</p><p></p><p>从 <a href=\"https://www.infoq.cn/article/WiNrWV5QrqayNtSGGjFw\">SUSE </a>\"的发展轨迹中我们可以看到，其超过三十年的开源技术积累是推动全球开源生态发展的重要力量之一。从 1992 年最初的 Linux 发行版开始，SUSE 就坚定地选择了开源的道路。旁观 CentOS 事件，SUSE 的态度也是非常明确的，开源社区应该以合作、交流、共享和创新为核心价值，任何发行版的变更都应该遵循社区的规则和流程，以确保开源社区的稳定、可靠和安全。</p><p></p><p>今年 7 月，在红帽宣布不再对外公开 Red Hat Enterprise Linux（RHEL）源代码后，SUSE 便表示将开发和维护与 RHEL 兼容的发行版，让所有人都可以不受限制地使用该发行版本，未来还计划向该项目投资超过 1000 万美元。当前，为应对这一挑战，用户可以选择使用 openSUSE、SLES、SUSE Liberty Linux 以及刚刚发布的基于 openEuler 的国产操作系统锐蜥 FlexileOS。其中，SUSE Liberty Linux 是最佳选择之一，它是一种适用于混合 Linux 环境的技术和支持解决方案，用户不仅可以获得可信的技术支持，还可以注册并接收针对于 RHEL、CentOS 和 SLES 等系统的更新。</p><p></p><p>SUSE 在 Linux 方面的持续探索之举非常令人动容，它一直都是 Linux 领域的积极探索者和创新者。SUSE 最新推出的自适应 Linux 平台 Adaptable Linux Platform（ALP）就是一个很好的例子，该平台为企业级 Linux 提供了一种在云原生环境中演进用例的新方法，可以应用于从数据中心到云端、再到边缘的任意场景，让用户专注于工作负载，从硬件和应用层抽离出来。通过使用虚拟机和容器技术，ALP 可以让工作负载独立于代码流。</p><p></p><p>ALP 重点关注安全性，最新版本通过机密计算技术提供了一种受信任的执行环境，通过隔离、加密和执行虚拟机来保护所使用的数据，还为未来的扩展式机密虚拟机 (CVM) 支持奠定了基础。其“硬件和运行时认证”功能可用于验证工作负载的完整性，与 FDE 一起初步实现了端到端的数据安全防护。同时，它与 NeuVector 集成，提供了一个安全的生态系统，让 ALP 用户通过 NeuVector 识别恶意行为，并防止底层主机操作系统或其他容器化工作负载受到影响。此外，用户在安装它时，还可选择带有 TPM 的 FDE 来支持静态数据安全性。</p><p></p><p>ALP 的新版本创新就是 SUSE 过去三十余年创新之路的缩影，这些年来 SUSE 不断推出各种创新的开源解决方案，包括存储、虚拟化、容器等一系列的技术创新。这不仅为 SUSE 赢得了市场份额、用户的信任和认可，更为整个开源领域贡献了大量有价值的资源和经验。技术的进步需要众多像 SUSE 一样的厂商共同贡献，企业间通过实施开源策略，共享资源和经验，共同应对挑战，实现自身的业务增长，促进行业的生态繁荣。</p>",
    "publish_time": "2023-09-27 14:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "重庆富民银行风险管理部副总经理李钦确认出席 FCon ，分享中小银行智能风控体系建设实践",
    "url": "https://www.infoq.cn/article/eDq0AVuIVKZwgJoCMV41",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。重庆富民银行风险管理部副总经理李钦将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5558?utm_source=infoqweb&amp;utm_medium=article\">中小银行智能风控体系建设实践</a>\"》主题分享，介绍重庆富民银行根据业务战略目标，通过在业务实施过程中不断实践，在风险管理方面推出的 F.A.R 智能风控管理综合平台。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5558?utm_source=infoqweb&amp;utm_medium=article\">李钦</a>\"，拥有 15 年商业银行、互联网银行工作经验，擅长大数据智能风险管理，数据、风险管理系统，风险策略和模型开发。熟悉互联网银行主要业务，熟悉互联网信贷主要业务模式，熟悉商业银行信息系统数据与架构，具备消费金融业务和智能化、数字化风控多年经验。</p><p></p><p>主导多款互联网信贷产品风险管理体系设计，包括线上小微贷、消费贷。熟悉主流助贷、联合贷模式。熟悉商业银行信息系统数据与架构，具备消费金融业务和智能化、数字化风控多年经验。搭建智能风控系统和工具，全程推动包含决策引擎、变量平台、模型管理平台、外部数据管理平台、监控预警平台、风险计量报表、人脸识别等一系列核心系统功能上线。</p><p></p><p>全面负责与金融科技公司的合作，接入各种数据源，搭建联合建模、联邦学习平台，与外部公司共建数据产品和风险模型。具备数字化思维，善于从数据中总结并发现问题，并开发相应的数据模型。他在本次会议的演讲内容如下：</p><p></p><p>演讲：中小银行智能风控体系建设实践</p><p></p><p>随着大数据、云计算、人工智能、区块链以及物联网等新技术的快速发展，金融科技与互联网信贷风险管理的融合日益深入，新技术的应用显著提高了作业效率、降低管理成本、增强风控能力。但中小银行在数字化转型的过程中，面临业务模式、人才结构、成本控制、战略优化等一系列问题。</p><p></p><p>重庆富民银行根据业务战略目标，通过在业务实施过程中不断实践，在风险管理方面推出了 F.A.R 智能风控管理综合平台，一站式解决风险管理的贷前审批、贷中用信、贷后催收、反欺诈等管理流程。以数据为基础、模型为核心，驱动自动化审批策略的广泛应用，日均审批 10 万余件，累计放款近 6000 亿元，积累了足够的实践经验。</p><p></p><p>该平台荣获中国人民银行牵头的《重庆市金融数据综合应用试点优秀实践项目》和银联组织的《2023 卓越数字金融大赛》数字风控银奖。</p><p></p><p>演讲提纲：</p><p></p><p>中小银行数字化转型的困难与挑战智能风控体系总体框架智能风险模型体系智能风险策略体系智能风控系统和工具体系智能风控能力评价标准智能风控未来发展方向</p><p></p><p>你将获得：</p><p></p><p>○ 如何应用大数据和人工智能技术，提高风险识别和处置能力；</p><p>○ 快速搭建智能风控基础设施，应用评价标准提升能力的评价科学性；</p><p>○ 团队建设和相关管理流程中需要关注的点。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，前 100 人可享 5 折特惠购票，咨询购票请联系：17310043226（微信同手机号）。</p>",
    "publish_time": "2023-09-27 14:44:32",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从计算到智算，如何降低AI算力使用门槛？",
    "url": "https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV",
    "summary": "<p>随着生成式 AI 技术得到广泛应用，算力产业正从“计算”迈向“智算”，这一变化给算力产业带来哪些挑战？不同应用场景对算力芯片的运算能力有何需求？如何降低 AI 算力使用门槛？近日，InfoQ《极客有约》邀请到了大禹智芯产品及解决方案负责人余曦老师，为大家分享《从计算到智算，如何降低 AI 算力使用门槛？》。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href=\"https://www.infoq.cn/video/2RLlk4XePkSXUNU4eQIw\">https://www.infoq.cn/video/2RLlk4XePkSXUNU4eQIw</a>\"</p><p>&nbsp;</p><p>姜雨生：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，微软软件工程师姜雨生。本期直播，我们邀请到了大禹智芯产品及解决方案负责人余曦老师来给我们做分享。今天的直播主题是《从计算到智算，如何降低 AI 算力的使用门槛？》。先请余曦老师，给大家做一个简单的介绍。</p><p>&nbsp;</p><p>余曦：大家好，我是余曦，来自大禹智芯。目前，我在公司负责产品和解决方案。非常高兴今天有机会与大家分享我们在 AI 智算中心的经验，包括我个人在这一领域的想法和未来的思考。</p><p>&nbsp;</p><p>我简要介绍一下大禹智芯。我们成立于 2020 年中旬，是国内最早专注于 DPU（数据处理单元）产品的公司之一。我们在技术领域涵盖了底层硬件，包括芯片和&nbsp;FPGA&nbsp;逻辑，以及硬件的不同形态，例如网卡。此外，我们拥有一支强大的软件团队，负责将&nbsp;DPU&nbsp;上的各种硬件能力整合到我们的 DPU 操作系统中。通过这个操作系统，我们可以开发应用软件，充分发挥底层硬件的性能。</p><p>&nbsp;</p><p>除了完整的 DPU 产品，包括硬件和软件，我们还开发了一些与应用相关的组件，与编排系统集成。这些编排系统包括云环境下广泛使用的 OpenStack 和 Kubernetes 等。在编排系统的基础上，我们还提供了一层服务，面向那些没有自己云管理平台但希望充分利用我们产品的用户。为了满足这一需求，我们开发了一个裸金属服务管理平台，结合了我们改进过的 OpenStack 和 DPU 卡，使用户能够快速将物理服务器转化为云化的裸金属实例。</p><p></p><h2>数据中心变迁历程</h2><p></p><p>&nbsp;</p><p>姜雨生：您在过去十年间参与了国内多个大型互联网云计算厂商数据中心网络架构的设计和建设，能分享下过去十年数据中心的变迁历程吗？主要可以分为哪几个阶段？</p><p>&nbsp;</p><p>余曦：在过去的十几年里，我一直投身于互联网行业的数据中心领域。我的经历可以大致分为三个阶段。</p><p>&nbsp;</p><p>首先是物理机阶段，这个阶段网络侧的需求相对简单。当时，主要问题是如何在物理机状态下建立一个数据中心，以充分利用物理带宽。在这个阶段，传统的二层生成数协议等都被推翻，以实现带宽的最大化利用。这个物理机时代持续了很长时间。</p><p>&nbsp;</p><p>第二阶段大约在 2012 年到 2014 年之间，即从物理机时代转向了云计算时代。这意味着从物理机向云计算或云数据中心的转变。云数据中心可以分为小型企业级云数据中心和大型大规模云计算中心。从设备厂商的角度来看，可以分为企业级数据中心和大规模数据中心（MSDC），MSDC 主要服务于大型公有云场景。我们的工作主要集中在大规模数据中心这个领域，面临着如何支持从几百台、1000 台规模扩展到数万、十万甚至几十万的台的巨大挑战。</p><p>&nbsp;</p><p>在这个规模扩展的过程中，出现了许多问题。一个重要的变化是，原先与网络相关的功能都集中在数据中心交换机层面，但随着云计算的发展和规模的扩大，所有功能开始下沉到服务器端，通过软件提供。采用软件方式提供这些功能有几个好处：</p><p>&nbsp;</p><p>第一点是关于灵活性和自主性。与硬件相比，软件更具灵活性，能够根据需要实现功能，这使得它更加灵活和自主。</p><p>&nbsp;</p><p>第二点涉及到软件角度与云计算资源的充分耦合。在云计算中，计算、网络和存储是三个关键组成部分。网络是基础，同时也连接了计算和存储。在计算和存储方面，需要快速将它们紧密结合，并且要具备灵活性。然而，这在网络方面面临许多挑战。因此，我们看到许多功能从传统的交换机层面逐渐迁移到主机上来实现。这也是为什么 DPU（数据处理单元）或智能网卡变得非常重要的原因。因为在所有功能下沉到主机后，主机会面临巨大的压力。那么，如何解决这种压力？必须引入一些新的组件来帮助主机处理网络中的复杂流程，特别是在大型云场景中的复杂流程。这就导致了像智能网卡和 DPU 这样的组件在云计算中逐渐崭露头角。</p><p>&nbsp;</p><p>此外，还有一点需要注意的是，在传统的云计算场景下，计算资源主要围绕 CPU 提供。但随着广泛应用和需求的变化，逐渐引入了异构计算组件，包括&nbsp;FPGA、GPU 以及专门用于特定算法的 AI 芯片。这些组件逐渐融入了云计算场景，与 CPU 一起提供算力，这在过去是非常不同寻常的。同时，这也为现在被称为智算中心和 AI 计算领域的发展提供了雏形。</p><p>&nbsp;</p><p>第三阶段是云计算服务逐渐与智算服务（或异构算力服务）相结合，形成了我们现在所谈论的智算中心。在智算中心内部，有一个重要组成部分，即高性能网络。当我们与客户讨论智算中心时，我们强调从网络的角度看，智算中心实际上分为两个网络：服务网和参数网（或计算网）。</p><p>&nbsp;</p><p>从服务网的角度来看，它通过这个网络将智算中心内部的计算资源，无论是 CPU 的计算能力还是 GPU 或其他异构计算资源，以云服务的形式提供出去。这个网络承载了向智算中心内的云服务提供计算资源的角色。</p><p>&nbsp;</p><p>参数网（或计算网）的主要作用是通过网络建立一个高速通道，以便计算单元之间进行快速的数据交互。这个网络在云计算数据中心的后期阶段主要存在于小规模存储场景中。例如，在国内引入 RDMA（Remote Direct Memory Access）应用时，这个应用主要基于存储场景，后端存储集群通过 RDMA 技术提高吞吐量、降低延迟，从而提高云上云盘的整体性能。当引入这些计算单元后，计算单元也需要一个高性能网络来支持数据传输。因此，高性能网络从存储环境逐渐迁移到了智算中心内的参数网。这实际上是一个演进的过程。</p><p>&nbsp;</p><p>姜雨生：数据中心是算力的最终载体，在算力需求日益增长的当下，传统数据中心面临哪些挑战？我们需要什么样的数据中心？</p><p>&nbsp;</p><p>余曦：参数网（或计算网）现在面临着巨大的挑战，因为这个网络有几个极致的要求。首先是吞吐量必须大，且从微观角度看，它要求吞吐量呈波浪形，但从宏观角度看，它应该一条直线。这意味着网络流量需要一直保持在最大带宽的状态，这是一个挑战。其次，在网络趋于跑满的情况下，仍然需要为上层应用提供良好的传输环境。传输环境涉及两个维度：丢包和延时。不同应用可能对丢包的容忍度不同，有些应用可以接受一些丢包，而有些则需要零丢包。延时也是一个重要因素，因为在数据交互中，任务的开始必须等到上一个阶段的数据交互完成。这意味着等待时间取决于最后一个任务完成的时间，而不是第一个任务完成的时间。因此，尽量减少等待时间，使数据交互在相近的时间内完成，算力单元的能力要求非常高。</p><p>&nbsp;</p><p>这就要求网络在三个方面都要表现出色，即延时、吞吐和规模。然而，传统的 TCP 网络在这三个方面通常表现为一个狭窄腰形状的等腰三角形，延时、吞吐和扩展性的平衡难以达到。现在的挑战是将 RDMA 网络调整成类似的形状，即提高吞吐、降低延时，并改善规模性能。这对所有相关方都是一个巨大的挑战。</p><p>&nbsp;</p><p>最近的市场变化表明，IB（InfiniBand）网络从原来的 HPC（高性能计算）领域突然爆发出来，这部分原因与高性能网络相关。在 HPC 场景下，IB 网络能够满足大型模型场景的需求，因此它受到了关注。然而，现在越来越多的用户，特别是在国内，开始探讨以太网是否能够提供与 IB 相似的性能和能力。</p><p></p><h2>不同场景对算力芯片的运算能力有何需求？</h2><p></p><p>&nbsp;</p><p>姜雨生：针对不同的应用场景，算力芯片的运算能力需求有何不同？能分享几个算力芯片在实际应用中的案例吗？</p><p>&nbsp;</p><p>余曦：不同的应用场景对算力芯片的运算能力有不同的需求。通常，GPU 的算力能力以 Flops（浮点运算每秒次数）来衡量，而 Flops 可以在多个维度上划分，包括 INT8、FP16 以及不同精度的整数计算（INT16、INT32、INT64 等）。不同厂家可能对这些精度有不同的定义。因此，在选择算力芯片时，不一定需要所有精度都非常强大，只需满足特定应用的需求即可。</p><p>&nbsp;</p><p>在与行业用户进行沟通和交流时，发现不同行业和应用对算法精度和性能要求不同。举例来说，有一家专门做推荐算法的公司，他们的模型训练场景与其他行业完全不同，更轻量化。虽然他们的数据量很大，模型也非常先进，但他们需要的精度更低。这是因为他们的业务要求如此。从推荐算法的角度来看，用户的行为和兴趣可以导致不同的推荐结果，因此他们更注重轻量化的模型。推荐算法是基于用户的先前行为和兴趣来进行的。从法律和法规的角度来看，用户的画像不能太精确，以避免直接关联到特定个人，因为这在法律上通常是不允许的。因此，推荐算法的设计并不一定需要极高的精度，只需要达到所需的效果即可。</p><p>&nbsp;</p><p>在不同领域，比如医疗生物制药领域，不同阶段的计算需求也不同。在医疗生物制药领域，每个阶段需要不同的算力和算法类型。这些需求可以使用不同的集群来处理，类似于流水线的概念。因此，在特定场景下，可以使用特定的算力单元来实现更高效的处理。</p><p>&nbsp;</p><p>一些公司专注于提供特定场景的 AI 算力，而不仅仅是通用的 GPU。他们的逻辑是根据特定场景和需求，为算法提供定制化的算力，以加速计算。虽然在其他方面可能相对较弱，但这种方法在特定应用场景中有着广阔的前景和市场机会。</p><p>&nbsp;</p><p>姜雨生：有数据提到，我国算力利用率仅 30%，大量算力仍处于闲置状态，如何才能提升算力的利用效率？算力产业链的各个相关方需要做哪些工作？</p><p>&nbsp;</p><p>余曦：早在很早以前，一些大型运营商就提出了算力网络或算网融合的概念。这意味着将分散的算力资源联合起来，以便共同提供这些算力资源，并在这一过程中，网络的角色变得非常关键。同时，标识和管理算力资源、调度计算任务也变得至关重要。</p><p>&nbsp;</p><p>运营商和一些边缘计算相关的公司已经在这个领域有了一套有效的算网融合模型和实施方法，可以通过虚拟网络来构建算力网络，并在其上调度不同的算力单元，以提高其利用率。当然，在实现这一目标时，还需要解决一些挑战。举个例子，与云计算领域的情况进行对比。最初，大家都在同一个云上部署应用。后来，人们开始将应用分散在不同的云上，因此出现了多云平台或多云调度的概念。同样，对于算力资源，我们也需要类似的多云平台，将不同的算力资源整合在一起，并提供一种选择不同资源的方式。这将有助于提高整体算力资源的利用效率。</p><p>&nbsp;</p><p>姜雨生：有观众提问，现在的大小模型、网络模型是怎么样的呢？</p><p>&nbsp;</p><p>余曦：对于现在的大小模型和网络模型，实际上主要涉及到网络支撑的算力节点数量。越大的模型需要更多的算力节点，这也会带来对网络的更大挑战。但总体来说，网络的组建逻辑基本相同，只是需要考虑容量更大的交换机、网络拓扑结构等因素。不管是大模型还是小模型，它们对网络的要求都是一致的，主要包括以下三个方面：吞吐量、延迟和可扩展性。这些因素在不同规模的模型中都具有重要意义。</p><p>&nbsp;</p><p>姜雨生：观众提问，研发算力芯片，芯片的质量和稳定性怎么保证？</p><p>&nbsp;</p><p>余曦：我们的公司在算力芯片领域有着不同的专注点。我们的主要工作是集中在网络处理单元上，任务是协助算力芯片进行数据搬迁操作。具体来说，我们的任务是帮助算力芯片快速而高效地将数据从一个地点（点 A）搬移到另一个地点（点 B）。这个领域是我们的专业领域。</p><p></p><h2>从“计算”到“智算”</h2><p></p><p>&nbsp;</p><p>姜雨生：随着生成式 AI 技术得到广泛应用，算力产业正进入产业“智算”时代，从“计算”到“智算”，最大的变化是什么？如何应对算力需求的变化？</p><p>&nbsp;</p><p>余曦：实际上，最大的变化在于从以 CPU 为主的算力提供服务者，演变为 CPU 周围有各种专业的辅助芯片，例如 GPU、FPGA、AI 芯片等，每个芯片都有自己的专长领域。这些辅助芯片与 CPU 一起组成了一个多样化的算力集群，以多种不同的算力方式提供服务，这就是现在智算中心的基本逻辑。</p><p>&nbsp;</p><p>从我们的角度来看，我们的主要目标是在智算中心中补充整个产品线。我们更专注于网络方面的工作，因此在智算中心的角度来看，我们的任务是将服务网上的工作卸载到 DPU 卡上，以提高处理效率并创建更多的云服务。在参数网络方面，我们正在开发基于 RDMA 的底层网络支持，虽然它不同于传统 RDMA 网络，但我们的实现能够让 RDMA 应用或网络达到更高效的状态，就像之前提到的那个满三角形的状态一样。</p><p>&nbsp;</p><p>姜雨生：当前企业在使用算力时主要存在哪些门槛？怎样才能降低算力的使用门槛？</p><p>&nbsp;</p><p>余曦：当前，在智算中心领域，企业内部搭建一个算力平台的门槛相当高。这种困难可以分为几个层面。首先是基础设施，包括服务器部署、网络部署以及基础环境的调试和优化。这些任务需要大量的人力、物力和精力，而许多企业缺乏专业的基础设施人员，他们更关注应用层面，即如何使用现有的硬件来运行模型进行计算。这是他们擅长的，但他们缺乏底层基础设施的技能。</p><p>&nbsp;</p><p>对我们来说，我们提供了一种成熟的解决方案，利用我们自己的裸金属云解决方案，使客户能够在底层基础设施层面快速构建基于裸金属的算力单元。例如，对于一个典型的 AI 服务器配置，如两台机器、每台机器 8 张 GPU 卡，客户需要快速将其投入使用。除了底层的网络配置外，还需要建立许多服务，例如客户需要能够灵活选择使用多少台机器以及这些机器上的技术环境，如操作系统、CUDA 等。通过我们提供的完整的裸金属云解决方案，客户可以在完成底层物理层的网络配置后，快速实现服务化，从而加速他们的计算工作。</p><p>&nbsp;</p><p>通过这种服务化方式，客户可以像在云计算场景下选择云实例一样，快速启动一台机器或一组机器。在选择过程中，他们可以自由选择操作系统的版本、包含的 CUDA 版本以及各种框架组件等，这些选项都以服务的形式提供。例如，如果客户需要 8 台 8 卡的机器，他们只需在 Web 界面上点击几下，几分钟内就能启动整个服务，完成后可以迅速释放资源，供其他同事或部门使用。这实际上展示了我们现在具备的快速交付的能力，构建了一个称为智算中心的底层基础设施。</p><p>&nbsp;</p><p>姜雨生：大模型时代的到来，会让 AI 芯片市场格局将发生巨变吗？您会用哪几个词来形容当前的行业状态？</p><p>&nbsp;</p><p>余曦：我认为目前国内 GPU 行业正迎来一个巨大的发展机遇。根据我们之前的沟通，国内的 GPU 厂商都非常看好未来的发展前景，并全力以赴开发下一代产品。对于上一代产品或现有产品，它们都存在一些不够满意的方面，例如跨机通信能力和算力、性能覆盖程度等。我们预计，可能在今年年底或明年上半年，国内将涌现出大量新一代的 GPU 产品，这将为国内的技术和算力平台建设增添新动力，这是一个明显的趋势。</p><p>&nbsp;</p><p>姜雨生：有观众提问，听说 AI 的网络用的是 RoCEv2？</p><p>&nbsp;</p><p>余曦：在大规模的 AI 网络部署中，目前是不使用RoCEv2的，因为存在一些问题，尤其在 AI 场景下这些问题更加显著。首先，RoCEv2 存在固有的 PFC（Priority Flow Control）问题，在整个 AI 场景下特别明显。具体来说，RoCEv2 在以下两个方面存在问题：</p><p>&nbsp;</p><p>扩展性问题：&nbsp;RoCEv2&nbsp;无法支持大规模部署，这在大规模AI网络部署中成为一个明显的障碍。PFC问题：PFC可能会导致一些问题，例如死锁（DEADLOCK），这些问题妨碍了其扩展性的提升。</p><p>&nbsp;</p><p>因此，目前来看，如果从大规模 AI 网络的实施角度考虑，通常有两种主要途径：</p><p>&nbsp;</p><p>第一种途径： 大型互联网公司内部自主研发以太网协议实现。这种自研实现可以支持非常大规模的 RDMA 网络部署，并且通常包含一些特殊定制的功能。这些公司通常将这些实现视为自家的核心竞争优势，因此对外部是不公开的，你只能看到一些相关的研究论文和借鉴的参考资料，但无法获得具体实现的细节。第二种途径： 基于英伟达的 InfiniBand（IB）网络。目前，IB 已经被证明可以在大规模 AI 算力网络中提供支持。</p><p>因此，包括国内 DPU 制造商在内的行业内企业都计划基于 DPU 实现端到端的高性能网络，该网络基于以太网，但能够提供类似 IB 网络的性能支持。这是当前的主要趋势之一。</p><p>&nbsp;</p><p>姜雨生：有观众提问，大模型训练算力和推理算力消耗量是多少？”</p><p>&nbsp;</p><p>余曦：在讨论大模型的训练和推理时，实际上涉及到两个不同的场景：训练和推理。训练阶段是模型自身在背后进行学习和优化的过程，而推理阶段是将已训练好的模型拿到前端，用于为外部提供服务。</p><p>&nbsp;</p><p>在不同的时代和情境下，训练和推理所需的算力可能存在差异。例如，目前大型模型的训练可能需要数千张卡并耗费大量算力，但一旦训练完成并生成了可用的模型，模型在前端提供推理服务时，算力的消耗可能会因前端的服务需求而变化。</p><p>&nbsp;</p><p>这个问题可以从扩展性和分发性的角度来看待。训练阶段通常需要大量计算资源，但通常是固定的。然而，在推理阶段，算力需求是动态的，并且会随着用户数量的增加而增加。因此，前端推理所需的算力可能会远远超过训练阶段的计算能力。</p><p>&nbsp;</p><p>姜雨生：有观众提问，传统网络转到英伟达的 IB 网络，需要改造什么的么&nbsp; 会影响算力么？</p><p>&nbsp;</p><p>余曦：在传统网络和 InfiniBand（IB）网络之间，一般没有改造的情况，通常是进行全新建设。现在我们主要看到的是全新建设的场景，例如，如果要建立一个智算中心以及智算中心后面的参数网络，通常是从头开始建设，而不是对现有数据中心进行改造。</p><p>&nbsp;</p><p>从两个维度来看，这一决策是有其原因的：</p><p>协议不同：传统网络通常使用以太网协议，而 InfiniBand 是一种不同的协议。除非你使用英伟达自家的 Spectrum 芯片环境，它可以进行以太网和 InfiniBand 之间的切换，否则你不能在传统以太网交换机上切换到 InfiniBand 网络。带宽需求不同：传统环境中，通常的带宽需求不是特别高。例如，在云环境中，大多数情况下使用的是 25G 以太网。然而，在参数网络中，通常以 100G 或 200G 的带宽为起点。如果要实现顶级配置，那么需要使用 400G 网络来支持整个计算过程。因此，在原有的 25G 网络下直接进行改造以支持 200G 或 400G 网络是不现实的，因此需要进行全新建设。</p><p></p><h2>高性能网络对于 AI 智能领域至关重要</h2><p></p><p>&nbsp;</p><p>姜雨生：大模型的爆发式增长给算力芯片带来哪些挑战？芯片厂商如何才能更好地满足大模型厂商的需求？大禹智芯有哪些实践经验可以分享下？</p><p>&nbsp;</p><p>余曦：这方面的经验主要涉及定制化。就定制化而言，在大模型的场景下，定制化的需求相对较少。因为只要你的支持足够完善，大模型的应用层、中间层（也可以称为中间件层或 API 层）以及底层硬件之间的解耦非常强。这种情况下，在大模型的角度来看，应用层、中间层以及底层硬件之间的明确划分是相当好的。尽管在整个 AI 大模型的运行过程中，这三者之间的协作非常紧密，耦合度很高，但从每一层到每一层的界面角度来看，划分是相当清晰的。</p><p>&nbsp;</p><p>我们主要从底层入手，专注于底层的开发。但我们的目标是为中间层或通信层以及上层应用提供服务，使它们不必感知我们底层所做的工作。尽管在我们底层可能有一套独立的实现，但对于上层应用来说，这些实现是透明的，不需要感知。从这个角度来看，对于那些只关注上层应用的用户来说，他们实际上不太关心底层的实现细节，只要能够达到所需的效果即可。在这种情况下，定制化的功能需求较少。</p><p>&nbsp;</p><p>然而，也存在一些特殊情况，即客户从上到下都关心，并且更加偏向底层的场景，可能会需要一些定制化的功能。但需要指出的是，这些定制化的功能实际上都是围绕着刚才在大模型网络中提到的那三个角，即吞吐量、延迟和扩展性展开的。不同的实现机制可能会在这三个角度上有一些不同的理念。</p><p>&nbsp;</p><p>目前，我们正在与一些头部客户讨论与实现相关的理念，希望能够汲取彼此的长处，将我们的优秀想法输出给这些头部客户，共同建立一个良性的环境，真正打造出一个高性能的算力网络。</p><p>&nbsp;</p><p>姜雨生：对于国内芯片厂商来说，这其中有哪些机遇与挑战？这个过程中，大禹智芯的技术产品策略是否发生了变化？</p><p>&nbsp;</p><p>余曦：我们的产品路线从一开始就是围绕着智算中心的两张网络来制定的。就目前而言，我们在服务网络方面已经处于一个非常成熟的状态，通过 DPU 实现了云服务的底座，这一方面已经准备就绪。现在我们正专注于投入更多的精力和资源，来加强我们所说的“算力网”，也就是背后的计算网络。在此方面，我们已经进行了许多关于算力网络的讨论，而现在我们正全力以赴地投入到算力网络的发展中。</p><p>&nbsp;</p><p>未来的愿景是，通过我们的大禹智芯网卡，能够与国家计算能力布局中的两张网络共同发挥作用。一张网络是服务网络，用于构建智算中心的云底座，另一张网络是参数网络，用于构建智算中心内部的高性能计算网络。这两张网络将协同合作，为未来的计算需求提供支持。</p><p>&nbsp;</p><p>姜雨生：下一步，大禹智芯有哪些技术探索和产品规划？有哪些技术难题是我们在未来需要解决的？</p><p>&nbsp;</p><p>余曦：我们计划在明年上半年推出我们自己的全新算力网络产品。我们对此进行了前期的研发工作。</p><p>&nbsp;</p><p>姜雨生：有专家提到“技术突破是算力发展的根本”，您怎么看？展望未来，我们应该如何更好地推动算力技术的发展和应用？</p><p>&nbsp;</p><p>余曦：在 AI 芯片领域，我们希望国内的芯片制造能力能够有明显突破。目前，AI 领域主要包括计算、存储和网络三个方面。在计算方面，国内已经有一些产品，特别是 CPU 和 GPU。然而，底层芯片性能的提升仍然是关键。这对于像我们这样专注于网络层的 DPU 制造商以及 GPU 制造商来说，都是一个积极的因素。</p><p>&nbsp;</p><p>从技术实现的角度来看，高性能网络对于 AI 智能领域至关重要。数据传输和计算之间的时间占用比例几乎是 1:1，尤其是在处理大型模型时，网络的占比可能更高。因此，提升网络能力将提高智算中心内所有算力单元的使用效率。在这个领域，有一些有趣的网络协议、想法和概念，需要一个有利的环境来实现。这个实现过程需要各方合作，将各自的能力输出，以建立一个有利于实验和创新的环境。解决问题是一个从 0 到 1 的过程，现在我们正在解决 0 到 1 的问题。一旦 0 到 1 的问题解决了，我们可以再看如何解决 1 到 10 的问题，需要哪些步骤和阶段。因此，我们希望有一个领导者，并创造一个培育创新的环境，让国内的合作伙伴能够将他们的想法付诸实践，验证效果，并确定未来的方向。这对于整个行业的发展非常重要。</p><p>&nbsp;</p><p>姜雨生：目前 DPU 行业的就业情况如何？需要哪些方面的人才？</p><p>&nbsp;</p><p>余曦：我们以 DPU 整个的技术栈来看，可以分为四个主要部分：硬件、软件、调度系统和业务平台。</p><p>&nbsp;</p><p>硬件层：硬件层涉及芯片能力，比如做自研芯片或&nbsp;FPGA就需要找相对应的团队，像FPGA&nbsp;层实现了网络和存储侧的功能，在这之上就是软件层的部署。软件层：软件层包括操作系统层和内核层的优化以及定制化。此外，还有与开发框架相关的工作，例如&nbsp;DPDK、SPDK。有这些开发框架开发经验的人才也能在参与到 DPU 这个行业之中。调度系统：这一层涉及与云服务相关的组件，包括 OVS、VPP、存储组件（如 Ceph 等）。与这些组件相关的团队也可以在 DPU 行业中发挥作用。业务平台：业务平台层包括与云平台、云管理以及一些开发相关的团队。这些团队可能在开发类似于 OpenStack、K8s&nbsp;等开源项目以及云平台上的业务平台等方面发挥作用。</p><p>&nbsp;</p><p>所以，从技术栈的角度来看，可以说我们将原本在云上的完整技术栈全部迁移到了 DPU 领域。这意味着无论是从底层硬件到上层的业务平台，都在 DPU 行业有所涉及。</p><p></p><h4>嘉宾介绍</h4><p></p><p>&nbsp;</p><p>特邀主持：</p><p>&nbsp;</p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p>&nbsp;</p><p>嘉宾：</p><p>&nbsp;</p><p>余曦，大禹智芯产品及解决方案负责人。曾任当当网首席网络架构师、思科大中华区互联网事业部总监、Fungible 中国首席架构师，在数据中心网络、云计算基础设施服务、高性能网络等领域有丰富的实践经验。近 10 年间主要参与了国内各大型互联网云计算厂商数据中心网络架构的设计和建设，见证并参与了国内数据中心从物理机时代向云计算时代发展过程中计算、网络、存储等基础设施的各个发展阶段。</p>",
    "publish_time": "2023-09-27 14:51:54",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]