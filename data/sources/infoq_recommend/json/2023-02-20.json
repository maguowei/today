[
  {
    "title": "谷歌发布云基础设施可靠性指南，帮助消费者做出正确决策",
    "url": "https://www.infoq.cn/article/Rxk0KGocM5ygWH8mS90v",
    "summary": "<p>谷歌最近为其消费提供了<a href=\"https://cloud.google.com/architecture/infra-reliability-guide\">云基础设施可靠性指南</a>\"，该指南结合了工程师的最佳实践和专业知识。</p><p></p><p>该指南的受众是那些希望为云基础设施做出正确决策以托管其工作负载的消费者。在Google Cloud的<a href=\"https://cloud.google.com/blog/products/infrastructure-modernization/design-reliable-infrastructure-for-workloads-in-google-cloud\">博客文章</a>\"中，谷歌的高级工程师Nir Tarcic和跨产品解决方案开发者Kumar Dhanagopal这样说到：</p><p></p><p></p><blockquote>Google Cloud的基础设施可靠性指南能够带领你了解<a href=\"https://cloud.google.com/architecture/infra-reliability-guide/building-blocks\">Google Cloud中构建基块的可靠性</a>\"，以及这些构建基块如何影响云资源的可用性。你会更深入地理解region、zone以及部署在单个zone、多个zone和跨region的应用的平台级可用性指标。</blockquote><p></p><p></p><p>在该指南中，消费者可以找到可供选择的<a href=\"https://cloud.google.com/architecture/infra-reliability-guide/design#deployment_architectures\">部署架构</a>\"，以便在不同的地点分配资源和部署冗余资源：</p><p></p><p><a href=\"https://cloud.google.com/architecture/infra-reliability-guide/design#single-zone\">单zone架构</a>\"对能够容忍工作负载停机或者企业在必要时能够在另外一个位置快速部署应用的场景来说是足够的。<a href=\"https://cloud.google.com/architecture/infra-reliability-guide/design#multi-zone\">多zone架构</a>\"适用于对zone中断需要保持韧性，但是能够容忍region中断造成停机的工作负载。<a href=\"https://cloud.google.com/architecture/infra-reliability-guide/design#multi-region-deployment-with-regional-load-balancing\">多region部署架构</a>\"是业务关键工作负载的理想选择，在这种场景下，高可用性至关重要，比如零售和社交媒体应用。</p><p></p><p><img src=\"https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/01/google-cloud-reliable-infra/en/resources/1Google%20Cloud%20MultiZone-1674393026723.png\" /></p><p></p><p>图片来源: <a href=\"https://cloud.google.com/architecture/infra-reliability-guide/design#deployment_architectures\">https://cloud.google.com/architecture/infra-reliability-guide/design#deployment_architectures</a>\"</p><p></p><p>消费者还可以找到关于<a href=\"https://cloud.google.com/architecture/infra-reliability-guide/traffic-load\">流量和负载管理的技术</a>\"，比如容量规划、自动扩展和<a href=\"https://cloud.google.com/architecture/infra-reliability-guide/manage-and-monitor\">变更管理指南</a>\"，以减少基础设施资源的可靠性风险。</p><p></p><p>与之类似，其他公有云供应商也有关于可靠性的指南和产品。例如，微软有一个专门的网站，提供与<a href=\"https://azure.microsoft.com/en-us/explore/reliability/#overview\">Azure可靠性</a>\"相关的产品概述、培训和文档。AWS提供了一份<a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/welcome.html\">文档</a>\"（可靠性支柱）作为其<a href=\"https://aws.amazon.com/architecture/well-architected/\">Well-Architect框架</a>\"的一部分。</p><p></p><p>谷歌的开发者关系和对外管理总监<a href=\"https://twitter.com/rseroter\">Richard Seroter</a>\"在LinkedIn的<a href=\"https://www.linkedin.com/posts/seroter_google-cloud-infrastructure-reliability-guide-activity-7010627352120496128-ImVp/?originalSubdomain=al\">帖子</a>\"中表示：</p><p></p><p></p><blockquote>公有云中有许多韧性相关的功能，你甚至不需要自己去考虑它们。有些事情就是在你不做任何事情的情况下也能更好地工作！但总的来说，系统韧性是一个架构问题。这是你需要在意的工作。这个新的Google Cloud指南可以帮助你在应用程序运行的地方建立更可靠的基础设施。</blockquote><p></p><p></p><p>最后，谷歌提供了更多的指导，包括构建可扩展和韧性应用程序的<a href=\"https://cloud.google.com/architecture/scalable-and-resilient-apps\">模式和最佳实践</a>\"。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/01/google-cloud-reliable-infra/\">Google Delivers Comprehensive Cloud Infrastructure Reliability Guide</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/oi5qfvo5NUCHeZScpaib\">揭秘 Meta 的云游戏基础设施</a>\"</p><p><a href=\"https://www.infoq.cn/article/HXgMpZwrxmhGU2DyWE9M\">SaaS 初创公司如何选择合适的云基础设施</a>\"</p><p><a href=\"https://www.infoq.cn/video/jFt3MjEx0HTdz8vJIhoZ\">应云而生 云原生基础设施服务进化</a>\"</p>",
    "publish_time": "2023-02-20 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "七年前选择用Go和Rust做数据库的创业公司，如今怎么评价这个决定？",
    "url": "https://www.infoq.cn/article/ENdwNVAliiHfPXquyLfw",
    "summary": "<p></p><blockquote>“我现在会很辩证地看待这件事情，只能说是不好不坏，但当时所谓的主流选择可能会让我们的产品变成一个平庸的系统。”</blockquote><p></p><p></p><p>即便是在此时此刻创业的公司，公司的产品决定全部采用 Go 和 Rust 也是非常艰难的决定，更何况是七年前</p><p></p><p>2015 到 2016 年，Go 不到五岁，Rust 还没发布 v1.0 版本，没有太多公司和开发者看好这两种语言，怎么会有公司选择全面采用这两种语言，还是用来写数据库和存储层代码？</p><p></p><p>如果你在七八年前听到这个故事，直觉大概是这家公司活不了太久。事实上，这家公司不仅走过了七年，还拿到了三轮融资，这家公司就是 PingCAP。</p><p></p><p>当然，编程语言只是工具，绝不是 PingCAP 可以走到今天最重要的因素，但这家公司确实因为这一选择收获了不少。本文通过与 PingCAP 创始人之一黄东旭和两位工程师的交谈，还原了这家创业公司最初选择 Go 和 Rust 的原因，以及如何解决随之而来语言本身的问题、人才问题以及对不同语言适用场景的思考。</p><p></p><h2>选择背后的原因</h2><p></p><p></p><p>“首先，我不是某一个具体的编程语言或者工具的信仰者，但在做项目时选择一个好的工具也是十分必要的。”——黄东旭</p><p></p><h3>选择的第一个要点：开发效率</h3><p></p><p></p><p>Go 语言的缔造者中有一位全世界程序员公认的大神级人物——肯尼思·汤普森（Kenneth Thompson），他是 UNIX 操作系统的主要开发者；其另一位主要设计者和早期实现者罗布·派克虽然没有直接参与最初版本的 UNIX 的开发，但同样属于贝尔实验室 UNIX 开发组的最资深成员，并且是字符编码 UTF-8 的主要实现者。</p><p></p><p>所以，Go 的出身决定了该语言具备极高的品质。而且，Go 语言从第一个版本起就开源，所以来自世界各地的程序员第一时间发现并使用了它，然后立刻就被其美妙的语言特性所吸引，比如 Go 语言使用比线程轻量得多的 goroutine 完成上下文切换可以节省高达 80% 左右的时间，这些关注者中就包括黄东旭与另一位合伙人刘奇。</p><p></p><p>早在创办 PingCAP 之前，黄东旭与刘奇就曾使用 Go 语言写过一个叫 codis 的开源软件解决当时豌豆荚业务在缓存扩展方面的问题。深入了解 Go 语言之后，二人被 Go 带来的效率提升所吸引。“同样的系统使用 C++ 开发可能需要一个月，但使用 Go 可能仅需要三天，这种级别的开发效率提升很难不让人动心。”</p><p></p><p>过去，因为效率而选择一门新兴语言的人并不是没有，比如《黑客与画家》的作者保罗·格雷厄姆 Paul Graham 曾用 Lisp 写了最早的 Web 应用 Viaweb，最终被雅虎以 5 千万余美金收购。这个故事也对 PingCAP 创始团队带来了一些影响。</p><p></p><p>PingCAP 的目标是做一个分布式数据库，也就是现在我们熟知的 TiDB。团队最开始确实考虑过 C++，毕竟大部分成员都有着 C++ 背景，大多还不错的数据库都是用 C++ 开发的，但是 C++ 非常依赖团队内部研发人员的经验、水平以及团队内部相关规范的制定，否则很容易出现问题。对于刚刚创业的 PingCAP 而言，团队显然是很难找到厉害的 C++ 研发人才。</p><p></p><p>“这里还有一个有趣的现象，业内鲜少有程序员承认自己精通 C++，即便是拥有 20 年经验的开发者，但可能学一个月就有人说自己精通 Go 了，复杂性一目了然。”</p><p></p><p>当时还存在一个安全的选择，那就是 Java。那段时间，主流的分布式系统大部分使用 Java 编写的，比如 Hadoop、Zookeeper、Cassandra、HBase 等。但面向云计算时代，团队认为需要更敏捷、更高效、同时更安全的新方式构建系统。</p><p></p><p>在当时，同样有很多软件产品选择工具时信奉其可以最大程度榨干硬件性能。“老实讲，我并不信奉开发出来的产品一定要能榨取硬件的最后一点性能，招聘具备这种能力程序员的成本可能并不比加几台机器低（分布式时代，这个问题显然可以通过加机器的方式来解决），而且用户也不会因为榨取的这一点性能而付更多钱”，黄东旭在采访中如是说道。</p><p></p><p>综合权衡下来，Go 成为了当时创业最合适的起步选择，可以快速把 TiDB 的原型开发出来。当年的 9 月份，TiDB 就在 GitHub 上开源了，后续的迭代也很快，随后一年就有客户在生产环境试用，再往后一年 TiDB 1.0 GA 版本正式发布。</p><p></p><h3>选择的第二个要点：语言本身的特性</h3><p></p><p></p><p>既然 Go 的效率得到了验证，团队为什么在后来开发 TiDB 的存储引擎 TiKV 时又选择了 Rust 呢？</p><p></p><p>TiKV 起始于 2015 年底，当时团队在 Pure Go / Go + Cgo / C++11 / Rust 几个语言之间纠结，虽然 PingCAP 的核心团队有大量的 Go 语言开发经验，另外 TiDB 的 SQL 层已经完全采用 Go 语言开发，Go 带来的开发效率的极大提升也让团队受益良多。但是在存储层的选型上，团队首先排除的就是 Pure Go 的选项，理由很简单，底层已经决定接入 RocksDB，RocksDB 本身就是个 C++ 的项目，而 Go 的 LSM-Tree 的实现大多成熟度不太够没有能和 RocksDB 相提并论的项目，如果选 Go 的话，只能选择用 Cgo 来 bridge，但是当时 Cgo 的问题同样明显，在 2015 年底，在 Go code 里调用 Cgo 的性能损失比较大，并不是在 goroutine 所在的线程直接 Call cgo 的代码，而且对于数据库来说，调用底层的存储引擎库是很频繁的，如果每次调用 RocksDB 的函数都需要这些额外开销，非常不划算，当然也可以通过一些技巧增大 Cgo 这边的调用的吞吐，比如一段时间内的调用打包成一个 cgo batch call，通过增加单个请求的延迟来增大的整体的吞吐，抹平 cgo 调用本身的开销，但是这样一来，实现就会变得非常复杂。另一方面，GC 问题仍然没有办法彻底解决, 存储层希望尽可能高效的利用内存，大量使用 syscall.Mmap 或者对象复用这些有些 hacky 的技巧，会让整体的代码可读性降低。</p><p></p><p>其实 C++11 也没什么问题，性能上肯定没问题，RocksDB 是 C++11 写的，在纠结了一小段时间后，团队认真评估了成员背景和要做的东西，最后还是没有选择 C++，原因主要是：</p><p></p><p>核心团队过去都是 C++ 的重度开发者，基本都有维护过大型 C++ 项目的经历，每个人都有点心里阴影… 悬挂指针、内存泄漏、Data race 在项目越来越大的过程中几乎很难避免，当然你可以说靠老司机带路，严格 Code Review 和编码规范可以将问题发生的概率降低，但是一旦出现问题，Debug 的成本很高，心智负担很重，而且第三方库不满足规范怎么办。</p><p></p><p>C++ 的编程范式太多，而且差异很大，又有很多奇技淫巧，统一风格同样也需要额外的学习成本，特别是团队的成员在不断的增加，不一定所有人都是 C++ 老司机，特别是大家这么多年了都已经习惯了 GC 的帮助，已经很难回到手动管理内存的时代。</p><p></p><p>缺乏包管理，集成构建等现代化的周边工具，虽然这点看上去没那么重要，但是对于一个大型项目这些自动化工具是极其重要的，直接关系到大家的开发效率和项目的迭代的速度。而且 C++ 的第三方库参差不齐，很多轮子得自己造。</p><p></p><p>Rust 在 2015 年底已经发布了 1.0，Rust 有几点特性非常吸引团队：</p><p></p><p>内存安全性高性能 (得益于 llvm 的优秀能力，运行时实际上和 C++ 几乎没区别)，与 C/C++ 的包的亲缘性强大的包管理和构建工具 Cargo更现代的语法和 C++ 几乎一致的调试调优体验，之前熟悉的工具比如 perf 之类的都可以直接复用FFI，可以无损失的链接和调用 RocksDB 的 C API</p><p></p><p>一方面，团队把安全性放在第一位，C++ 的内存管理和避免 Data race 的问题虽然靠老司机可以解决，但是仍然没有在编译器层面上强约束，把问题扼杀在摇篮之中解决的彻底，Rust 很好地解决了这个问题。另一方面，Rust 是一个非常现代化的编程语言，现代的类型系统，模式匹配，功能强大的宏，trait 等熟悉以后会极大提升开发效率。</p><p></p><p>最终，Rust 也没让团队失望，四五个人的团队从零开始花费了四个月左右的时间就开发出了 TiKV 的第一个版本。2016 年 1 月 1 日开始开发，4 月 1 日开源。同年 10 月份，TiKV 第一次被使用在生产环境，那时 TiDB 甚至都还没有发布 beta 版。TiKV 的开发非常快，发布的版本都很稳定，生产效率比 C++ 高出许多。</p><p></p><h3>选择的第三个要点：人才</h3><p></p><p></p><p>虽然当年并没有太多开发者使用 Rust，但早期探索者们的自身能力是很强的，这群人对编程本身有着极强的热爱，这对于创业公司而言是非常宝贵的人才（起步阶段的创业公司人才在精不在多），PingCAP 后续的发展也验证了这一点，曾经是 Rust 核心团队成员的 Brian Anderson 在 2018 年选择加入该公司并参与 TiKV 的研发，这种案例在 2018 年之前是极少数的。Brian 之所以愿意加入，除个人因素之外，与 PingCAP 在开源和社区方面的持续努力是分不开的。2017 年， Brian 就应邀出席过 PingCAP 举办的国内首场 Rust Meetup。2019 年， Rust Core Team 的元老 nrc 也加入了 PingCAP。</p><p></p><p>“直到今天，我依旧认为这是创业公司吸纳人才时很好的思路，否则一家创业公司通过什么去跟互联网大厂竞争，只能是更好的理念和工具。我们在没有做任何全球化品牌的时候就是靠着这张名片（指全面拥抱 Rust 生态）吸引人才加入我们的社区和公司。但很多时候，国内很多公司的问题是不敢想，认为自己凭什么可以吸引到这样的大牛加入，但凡事都要先试试。”</p><p></p><h2>选择 Rust 带来的问题</h2><p></p><p></p><p>如开篇所言，黄东旭如今认为当初的决定“不好不坏”，虽然获得了开发效率上的提升，也承受了当时语言不够成熟带来的问题。</p><p></p><p>“很多时候，人们会先通过广告了解一件事情。我们当初对 Rust 的看法也是内存安全、性能好，没有 GC 效率肯定高等，实际并不是这样的。如果你代码写的很挫，凭什么认为自己手动分配内存就比 GC 搞得好。”</p><p></p><p>起初，团队基本是把 Rust 当成 Java、C++ 在用，性能并没有明显提升，直到更专业的人才加入才把整个代码扭转到更好的道路上。</p><p></p><p>此外，Rust 的编译时间较长。“当时我们内部的 Rust 程序员经常开玩笑说一天只有 24 次编译机会，用一次少一次”，团队做了大量工作去降低 Rust 的编译时间，参与并贡献了 rust-gRPC、Raft 库，open-tracing 等项目中，并产出了大量相关的博客文章。</p><p></p><p>虽然解决这些问题占用了团队的很多时间，但团队也因此获益。“我们将 Talent Plan 的所有教程用 Rust 实现了一遍，虽然这离我们的主页主业有点远，但对后来的招聘和培训极其重要，这也是我们第一次在全球范围内好评如潮。”</p><p></p><h2>怎么判断要不要选择或者切换 Rust、GO？</h2><p></p><p></p><p>不少企业创业之初会选择基于某些开源产品来实现商业版本，这种情况下编程语言其实已经是确定的了，不需要太过纠结。如果从零开始开发某项产品，可以从以下几个方面进行考虑：</p><p></p><p>如果公司内部在 C/C++、Java 上开发规范已经做得很好了，可以先不考虑切换至 Rust。Rust 相当于自带严格的安全性限制，让程序员在大部分情况下没办法写出存在安全隐患的代码，语言本身的设计帮助规避了一些常规问题。如果是基础软件类型的企业，相关从业人员的水平还是值得信任的，一般不会犯太多低级错误。此时，Rust 的收益主要体现在数据库最核心的组件或者功能编写上，对剩下 90% 的部分而言，效率可能比安全更重要。非 Rust 不可的场景有写驱动，比如操作系统内核等，效率绝对高；SSL 加密或者产品内部的某个关键链路，比如浏览器里面的渲染引擎，这类 CPU 密集型又对安全要求较高的场景。非必要场景最好选用社区比较大的编程语言，比如 Java，相对来说也很好招人。考虑语言的向后兼容性。Go 语言在这一点上至今都做得非常好，并且也响应社区用户的意见添加了范型。社区的风格。Rust 的社区是非常开放的，这给该语言带来了很多好处，但也可能带来一些副作用，比如可能会有些分裂，这可能是 Rust 社区未来要考虑的事情，但社区内的氛围相对活跃，可以在其中寻找问题的答案或者志同道合的朋友。“Rust 近几年最新加入的复杂度主要是 Pin 异步编程，但二者的加入确实解决了一些问题，无论是哪种语言都会面临各种选择，这是不同目标平衡取舍的结果。”代码实现逻辑。Rust 语言在设计上与 Go、Java 等都不同，其规避了一些问题。使用 Rust 写出来的程序可以专注优化程序逻辑本身，比如让程序更加适应操作系统、减少线程切换等。</p><p></p><p>Rust 如今已经得到了越来越多企业和开发者的验证，但依旧有新的小众语言出现，如同当年新生的 Rust，企业应该如何判断呢？</p><p></p><h2>新型语言 Zig 也来了，怎么看？</h2><p></p><p></p><p>Zig 就是一门新的、小众的语言，目前还没有发布 1.0 版本，其将竞争对手定为了 C 语言，注意不是 C++，而就是最基础的 C，但也吸收了 Rust 等语言的一些特性。</p><p></p><p>去年中旬因为 Uber 的使用，Zig 引起了一些开发者的关注。Uber 使用 Zig 来编译其 C/C++ 代码。现在，Uber 只在 Go Monorepo（据其内部工程师介绍，Go Monorepo 比 Linux 内核还要大，有几千名工程师在开发和维护） 中使用 bazel-zig-cc，但计划尽可能地将 zig cc 推广到其他需要 C/C++ 工具链的语言。</p><p></p><p>Uber 的工程师表示，与其他工具链相比，zig-cc 提供的 C/C++ 工具链的主要优势是 glibc 版本可配制与 macOS 交叉编译（具体信息可以阅读：<a href=\"https://www.infoq.cn/article/q016NWR7OjHvOJ3Rkzc0\">https://www.infoq.cn/article/q016NWR7OjHvOJ3Rkzc0</a>\"）。</p><p></p><p>如今的 Zig 与七八年前的 Rust 情况相似又不同，当年的 Rust 背后有一群 Molliza 的工程师支持，其理念、质量和解决实际问题的能力是值得信任的，现在的 Zig 虽然有基金会支持，但实际能力还有待商榷。</p><p></p><p>受访的 PingCAP 工程师们对该语言的发展持观望态度，并建议企业在选择这类新兴小众的语言时最好是可以找到那个“非它不可”的理由，如果找不到，就证明这件事情没必要冒险采用新语言，如果有想法，最好可以亲自与创始团队面对面交流，从中得到一些判断。</p><p></p><h2>结语</h2><p></p><p></p><p>自从 ChatGPT 诞生，我们无数次对 AI 的生产力感到惊讶，其可以根据简单的输入生成代码并将这些代码轻松转换成其他编程语言。未来，开发者的工作模式可能会发生翻天覆地的变化，最重要的是我们构建这一切的思路和想法，而不是工具本身。从这个角度出来，关于编程语言优劣的争论就会变得不那么重要。</p><p></p><p>当然，如果你是某一个语言 / 工具的信仰者，着手将其带向全新高度也会比争论更有意义。</p><p></p><p>参考资料：</p><p></p><p>《与开源同行 - 揭秘 PingCAP 七年创业实践》</p><p></p><p>相关阅读：</p><p></p><p>Rust in TiKV (<a href=\"https://www.zenlife.tk/project\">https://www.zenlife.tk/project</a>\")</p><p></p><p>专题推荐：</p><p></p><p><a href=\"https://www.infoq.cn/theme/155\">《选择一门语言，重构技术栈》</a>\"</p><p></p><p>根据初步调研，企业当前存在重构技术栈的需求，原因如下：一是如今很多企业中还存在一些老旧编程语言编写的技术栈，这些技术栈难以维护，且相关人才奇缺；二是某些相对年轻的编程语言具备的特性是企业所需要的，比如Rust清晰定义了变量的生命周期，不仅摒弃 GC 这样的内存和性能杀手，还不用关心手动内存管理，让内存安全和高性能兼得。但是，使用其他编程语言更换技术栈存在很多问题，比如风险极高、人力及其他成本、更换前的评估因素、更换后的效果评估，也存在经过谨慎评估之后决定放弃某项编程语言的案例，本专题希望针对该话题进行深入采访，传递顶尖技术高手的认知。</p>",
    "publish_time": "2023-02-20 09:00:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Golang 编程“珠玑”",
    "url": "https://www.infoq.cn/article/74d3c7ddc91e9587f54af4ab0",
    "summary": "<p>作者：崔国科—— MO 研发工程师</p><p></p><h2>导读</h2><p></p><p>2017 年左右开始接触 golang，那时国内关于 golang 的资料还很少。</p><p>现在随着云原生生态的蓬勃发展，在 kubernetes、docker 等等众多明星项目的带动下，国内有越来越多的创业公司以及大厂开始拥抱 golang，各种介绍 golang 的书籍、博客和公众号文章也变得越来越多，其中不乏质量极高的资料。</p><p>相关的资料已经足够丰富，因此这篇文章不会详述 golang 的某一个方面，而是主要从工程实践的角度出发，去介绍一些东西。因为在工作过程中，我注意到一些令人沮丧的代码，其中有些甚至来自于高级程序员。</p><p>下面是本文目录概览，我们将从内存有关的话题开始：</p><p>内存相关Golang Profiling如何写性能测试</p><p></p><p></p><h2>Part 1 内存相关</h2><p></p><p></p><h3>1.1&nbsp;编译器内存逃逸分析</h3><p></p><p>先看这样一段代码：</p><p><code lang=\"go\">package main\n\n//go:noinline\nfunc makeBuffer() []byte {\n    return make([]byte, 1024)\n}\n\nfunc main() {\n    buf := makeBuffer()\n    for i := range buf {\n        buf[i] = buf[i] + 1\n    }\n}\n</code></p><p>示例代码中函数&nbsp;makeBuffer&nbsp;返回的内存位于函数栈上，在 C 语言中，这是一段错误的代码，会导致未定义的行为。</p><p>在 Go 语言中，这样的写法是允许的，Go 编译器会执行&nbsp;escape analysis：当它发现一段内存不能放置在函数栈上时，会将这段内存放置在堆内存上。例如，makeBuffer&nbsp;向上返回栈内存，编译器自动将这段内存放在堆内存上。</p><p>通过&nbsp;-m&nbsp;选项可以查看编译器分析结果：</p><p><code lang=\"go\">$ go build -gcflags=\"-m\" escape.go\n# command-line-arguments\n./escape.go:8:6: can inline main\n./escape.go:5:13: make([]byte, 1024) escapes to heap\n</code></p><p>除此之外，也存在其他一些情况会触发内存的“逃逸”：</p><p>全局变量，因为它们可能被多个 goroutine 并发访问;通过 channel 传送指针</p><p><code lang=\"go\">type Hello struct { name string }\nch := make(chan *Hello, 1)\nch &lt;- &amp;Hello{ name: \"world\"}\n</code></p><p>通过 channel 传送的结构体中持有指针</p><p><code lang=\"go\">type Hello struct { name *string }\nch := make(chan *Hello, 1)\nname := \"world\"\nch &lt;- Hello{ name: &amp;name }\n</code></p><p>局部变量过大，无法放在函数栈上本地变量的大小在编译时未知，例如&nbsp;s := make([]int, 1024)&nbsp;也许不会被放在堆内存上，但是&nbsp;s := make([]int, n)&nbsp;会被放在堆内存上，因为其大小&nbsp;n&nbsp;是个变量对&nbsp;slice&nbsp;的&nbsp;append&nbsp;操作触发了其底层数组重新分配</p><p>注意：上面列出的情况不是详尽的，并且可能随 Go 的演进发生变化。</p><p>在开发过程中，如果程序员不注意 golang 编译器的内存逃逸分析，写出的代码可能会导致“额外”的动态内存分配，而 “额外”的动态内存分配通常会和性能问题联系在一起（具体会在后面 golang gc 的章节中介绍）。</p><p>示例代码给我们的启示是：注意函数签名设计，尽量避免因函数签名设计不合理而导致的不必要内存分配。向上返回一个 slice 可能会触发内存逃逸，向下传入一个 slice 则不会，这方面&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//github.com/cockroachdb/cockroach/blob/5fbcd8a8deac0205c7df38e340c1eb9692854383/pkg/util/encoding/encoding.go%23L180\">Cockroach Encoding Function</a>\"&nbsp;给出了一个很好的例子。</p><p>接下来，我们看下接口相关的事情。</p><p></p><h3>1.2&nbsp;interface{} / any</h3><p></p><p>any 是 golang 1.18 版本引入的，跟 interface{} 等价。</p><p><code lang=\"go\">type any = interface{}\n</code></p><p>在 golang 中，接口实现 为一个“胖”指针：一个指向实际的数据，一个指向函数指针表（类似于C++ 中的虚函数表）。</p><p>我们来看下面的代码：</p><p><code lang=\"go\">package interfaces\n\nimport (\n  \"testing\"\n)\n\nvar global interface{}\n\nfunc BenchmarkInterface(b *testing.B) {\n  var local interface{}\n  for i := 0; i &lt; b.N; i++ {\n    local = calculate(i) // assign value to interface{}\n  }\n  global = local\n}\n\n// values is bigger than single machine word.\ntype values struct {\n  value  int\n  double int\n  triple int\n}\n\nfunc calculate(i int) values {\n  return values{\n    value:  i,\n    double: i * 2,\n    triple: i * 3,\n  }\n}\n</code></p><p>在性能测试&nbsp;BenchmarkInterface&nbsp;中，我们将函数&nbsp;calculate&nbsp;返回的结果赋值给&nbsp;interface{}&nbsp;类型的变量。</p><p>接下来，对&nbsp;BenchmarkInterface&nbsp;执行&nbsp;memory profile：</p><p><code lang=\"go\">$ go test -run none -bench Interface -benchmem -memprofile mem.out\n\ngoos: darwin\ngoarch: arm64\npkg: github.com/cnutshell/go-pearls/memory/interfaces\nBenchmarkInterface-8    101292834               11.80 ns/op           24 B/op          1 allocs/op\nPASS\nok      github.com/cnutshell/go-pearls/memory/interfaces        2.759s\n\n$ go tool pprof -alloc_space -flat mem.out\n(pprof) top \n(pprof) list iface.BenchmarkInterface\nTotal: 2.31GB\n    2.31GB     2.31GB (flat, cum) 99.89% of Total\n         .          .      7:var global interface{}\n         .          .      8:\n         .          .      9:func BenchmarkInterface(b *testing.B) {\n         .          .     10:   var local interface{}\n         .          .     11:   for i := 0; i &lt; b.N; i++ {\n    2.31GB     2.31GB     12:           local = calculate(i) // assign value to interface{}\n         .          .     13:   }\n         .          .     14:   global = local\n         .          .     15:}\n         .          .     16:\n         .          .     17:// values is bigger than single machine word.\n(pprof)\n</code></p><p>从内存剖析结果看到：向接口类型的变量&nbsp;local&nbsp;赋值，会触发内存“逃逸”，导致额外的动态内存分配。</p><p>go 1.18 引入范型之前，我们都是基于接口实现多态，基于接口实现多态，主要存在下面这些问题：</p><p>丢失了类型信息 ，程序行为从编译阶段转移到运行阶段；程序运行阶段不可避免地需要执行类型转换，类型断言或者反射等操作；为接口类型的变量赋值可能会导致“额外的”内存分配；基于接口的函数调用，实际的调用开销为：指针解引用（确定方法地址）+ 函数执行开销。编译器无法对其执行内联优化，也无法基于内联优化执行进一步的优化。</p><p>关于接口的使用，这里有一些提示：</p><p>代码中避免使用 interface{} 或者 any，至少避免在被频繁使用的数据结构或者函数中使用go 1.18 引入了范型，将接口类型改为范型类型，是避免额外内存分配，优化程序性能的一个手段</p><p></p><h3>1.3&nbsp;Golang gc</h3><p></p><p>前面我们了解到，golang 编译器执行 escape analysis 后，根据需要数据可能被“搬”到堆内存上。</p><p>这里简单地介绍下 golang 的 gc，从而了解写 golang 代码时为什么应该尽量避免“额外的”内存分配。</p><p>1.3.1 Introduction</p><p>gc 是 go 语言非常重要的一部分，它大大简化了程序员写并发程序的复杂度。</p><p>人们发现写工作良好的并发程序似乎也不再是那少部分程序员的独有技能。</p><p>glang gc 使用一棵树来维护堆内存对象的引用，属于追踪式的 gc，它基于“标记-清除“算法工作，主要分为两个阶段：</p><p>标记阶段- 遍历所有堆内存对象，判断这些对象是否在用；清除阶段- 遍历树，清除没有被引用的堆内存对象。</p><p>执行 gc 时，golang 首先会执行一系列操作并停止应用程序的执行，即&nbsp;stopping the world，之后恢复应用程序的执行，同时 gc 其他相关的操作还会并行地执行。所以 golang 的 gc 也被称为&nbsp;concurrent mark-and-sweep，这样做的目的是尽可能减少&nbsp;STW&nbsp;对程序运行的影响。</p><p>严格地说，STW&nbsp;会发生两次，分别在标记开始和标记结束时。</p><p>golang gc 包括一个&nbsp;scavenger，定期将不再使用的内存返还给操作系统。</p><p>也可以在程序中调用&nbsp;debug.FreeOSMemory()，手动将内存返还给操作系统。</p><p></p><p>1.3.2 gc 触发机制</p><p>相比于 java，golang 提供的 gc 控制方式比较简单：通过环境变量&nbsp;GOGC&nbsp;来控制。</p><p>runtime/debug.SetGCPercent allows changing this percentage at run time.</p><p>GOGC&nbsp;定义了触发下次 gc 时堆内存的增长率，默认值为 100，即上次 gc 后，堆内存增长一倍时，触发另一次 gc。</p><p>例如，gc 触发时当前堆内存的大小时 128MB，如果&nbsp;GOGC=100，那么当堆内存增长为 256MB时，执行下一次 gc。</p><p>另外，如果 golang 两分钟内没有执行过 gc，会强制触发一次。</p><p>我们也可以在程序中调用&nbsp;runtime.GC()&nbsp;主动触发 gc。</p><p><code lang=\"go\"># 通过设置环境变量 GODEBUG 可以显示 gc trace 信息\n\n$ GODEBUG=gctrace=1 go test -bench=. -v\n\n# 当 gc 运行时，相关信息会写到标准错误中\n</code></p><p>注意：为了减少 gc 触发次数而增加 GOGC 值并不一定能带来线性的收益，因为即便 gc 触发次数变少了，但是 gc 的执行可能会因为更大的堆内存而有所延长。在大多数情况下，GOGC 维持在默认值 100 即可。</p><p></p><p>1.3.3 gc hints&nbsp;</p><p>如果我们的代码中存在大量“额外”的堆内存分配，尤其是在代码关键路径上，对于性能的负面影响是非常大的：</p><p>首先，堆内存的分配本身就是相对耗时的操作其次，大量“额外”的堆内存分配意味着额外的 gc 过程，STW 会进一步影响程序执行效率。</p><p>极端情况下，短时间内大量的堆内存分配，可能会直接触发 OOM，gc 甚至都没有执行的机会。</p><p>所以，不要“天真”的以为 gc 会帮你搞定所有的事情：你留给 gc 处理的工作越少，你的性能才会越“体面”。</p><p>从性能优化的角度，消除那些“额外的”内存分配收益十分明显，通常也会是第一或者第二优先的选项。</p><p>然而，堆内存的使用并不能完全避免，当需要使用时，可以考虑采用某些技术，例如通过&nbsp;sync.Pool&nbsp;复用内存来减少 gc 压力。</p><p></p><p>1.3.4 有了 gc 为什么还会有内存泄漏？</p><p>即便 golang 是 gc 语言，它并不是一定没有内存泄漏，下面两种情况会导致内存泄漏的情况：</p><p>引用堆内存对象的对象长期存在；goroutine 需要消耗一定的内存来保存用户代码的上下文信息，goroutine 泄漏会导致内存泄漏。</p><p></p><p>1.3.5 代码演示</p><p>代码见于文件&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//gist.github.com/cnutshell/817b17f6eb4fa5c4383c0c7d53c744c0\">gc.go</a>\"</p><p>函数&nbsp;allocator&nbsp;通过 channel 传送&nbsp;buf&nbsp;类型的结构体，buf&nbsp;类型的结构体持有堆内存的引用；函数&nbsp;mempool&nbsp;通过 channel 接收来自&nbsp;allocator&nbsp;的 buf，循环记录在 slice 中；同时，mempool 还会定期打印应用当前内存状态，具体含义参考&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//pkg.go.dev/runtime%40go1.20%23MemStats\">runtime.MemStats</a>\"。</p><p>运行代码 gc.go：</p><p><code lang=\"go\">$ go run gc.go\nHeapSys(bytes),PoolSize(MiB),HeapAlloc(MiB),HeapInuse(MiB),HeapIdle(bytes),HeapReleased(bytes)\n 12222464,     5.00,     7.11,     7.45,  4415488,  4300800\n 16384000,    10.00,    12.11,    12.45,  3334144,  3153920\n 24772608,    18.00,    20.11,    20.45,  3334144,  3121152\n 28966912,    22.00,    24.11,    24.45,  3334144,  3121152\n 33161216,    25.00,    27.11,    27.45,  4382720,  4169728\n 37355520,    32.00,    34.11,    34.45,  1236992,   991232\n 41549824,    36.00,    38.11,    38.45,  1236992,   991232\n 54132736,    48.00,    50.11,    50.45,  1236992,   991232\n 58327040,    51.00,    53.11,    53.45,  2285568,  2039808\n</code></p><p>通过程序输出结果，我们可以了解到：如果程序中存在变量持有对堆内存的引用，那么这块堆内存不会被 gc 回收。</p><p>因此使用引用了堆内存的变量赋值时，例如将其赋值给新的变量，需要注意避免出现内存泄漏。通常建议将赋值有关的操作封装在方法中，以通过合理的 API 设计避免出现“意想不到”内存泄露。并且封装还带来的好处是提高了代码的可测性。</p><p></p><p>1.3.6 参考资料</p><p>Blog:&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//research.swtch.com/interfaces\">Go Data Structures: Interfaces</a>\"<a href=\"https://link.zhihu.com/?target=https%3A//pkg.go.dev/runtime%40go1.20%23hdr-Environment_Variables\">GOGC on golang's document</a>\"<a href=\"https://link.zhihu.com/?target=https%3A//www.bookstack.cn/read/qcrao-Go-Questions/GC-GC.md\">GC 的认识</a>\"</p><p></p><p></p><h2>Part 2 Golang Profiling</h2><p></p><p>profiler 运行用户程序，同时配置操作系统定期送出 SIGPROF 信号：</p><p>收到 SIGPRFO 信号后，暂停用户程序执行；profiler 搜集用户程序运行状态；搜集完毕恢复用户程序执行。</p><p>如此循环。</p><p>profiler 是基于采样的，对程序性能存在一定程度的影响。</p><p></p><blockquote>\"Before you profile, you must have a stable environment to get repeatable results.\"</blockquote><p></p><p></p><h3>2.1&nbsp;Supported Profiling</h3><p></p><p></p><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//pkg.go.dev/runtime/pprof%23Profile\">By default, all the profiles are listed in runtime/pprof.Profile</a>\"</blockquote><p></p><p>a. CPU Profiling</p><p>CPU profiling 使能后，golang runtime 默认每 10ms 中断应用程序，并记录 goroutine 的堆栈信息。</p><p>b. Memory Profiling</p><p>Memory profiling 和 CPU profiling 一样，也是基于采样的，它会在堆内存分配时记录堆栈信息。</p><p>默认每 1000 次堆内存分配会采样一次，这个频率可以配置。</p><p>注意：Memory profiling 仅记录堆内存分配信息，忽略栈内存的使用。</p><p>c. Block Profiling</p><p>Block profiling 类似于 CPU profiling，不过它记录 goroutine 在共享资源上等待的时间。</p><p>它对于检查应用的并发瓶颈很有帮助，Blocking 统计对象主要包括：</p><p>读/写 unbuffered channel写 full buffer channel，读 empty buffer channel加锁操作</p><p>如果基于&nbsp;net/http/pprof， 应用程序中需要调用 runtime.SetBlockProfileRate 配置采样频率。</p><p>d. Mutex Profiling</p><p>Go 1.8 引入了 mutex profile，允许你捕获一部分竞争锁的 goroutines 的堆栈。</p><p>如果基于&nbsp;net/http/pprof， 应用程序中需要调用 runtime.SetMutexProfileFraction 配置采样频率。</p><p>注意：通过&nbsp;net/http/pprof&nbsp;对线上服务执行 profiling 时，不建议修改 golang profiler 默认值，因为某些 profiler 参数的修改，例如增加 memory profile sample rate，可能会导致程序性能出现明显的降级，除非你明确的知道可能造成的影响。</p><p></p><p></p><h3>2.2 Profiling Commands</h3><p></p><p>我们可以从&nbsp;go test&nbsp;命令，或者从使用&nbsp;net/http/pprof&nbsp;的应用中获取到 profile 文件：</p><p><code lang=\"go\">## 1. From unit tests\n$ go test [-blockprofile | -cpuprofile | -memprofile | -mutexprofile] xxx.out\n\n## 2. From long-running program with `net/http/pprof` enabled\n## 2.1 heap profile\n$ curl -o mem.out http://localhost:6060/debug/pprof/heap\n\n## 2.2 cpu profile\n$ curl -o cpu.out http://localhost:6060/debug/pprof/profile?seconds=30</code></p><p>获取到 profile 文件之后，通过&nbsp;go tool pprof&nbsp;进行分析：</p><p><code lang=\"go\"># 1. View local profile\n$ go tool pprof xxx.out\n\n# 2. View profile via http endpoint\n$ go tool pprof http://localhost:6060/debug/pprof/block\n$ go tool pprof http://localhost:6060/debug/pprof/mutex</code></p><p></p><h3>2.3&nbsp;Golang Trace</h3><p></p><p>我们可以从&nbsp;go test&nbsp;命令，或者从使用&nbsp;net/http/pprof&nbsp;的应用中获取到 trace 文件：</p><p><code lang=\"go\"># 1. From unit test\n$ go test -trace trace.out\n\n# 2. From long-running program with `net/http/pprof` enabled\ncurl -o trace.out http://localhost:6060/debug/pprof/trace?seconds=5\n</code></p><p>获取到 trace 文件之后，通过&nbsp;go tool trace&nbsp;进行分析，会自动打开浏览器：</p><p><code lang=\"go\">$ go tool trace trace.out\n</code></p><p></p><h3>2.4&nbsp;Profiling Hints</h3><p></p><p>如果大量时间消耗在函数 runtime.mallocgc，意味着程序发生了大量堆内存分配，通过 Memory Profiling 可以确定分配堆内存的代码在哪里；</p><p>如果大量的时间消耗在同步原语（例如 channel，锁等等）上，程序可能存在并发问题，通常意味着程序工作流程需要重新设计；</p><p>如果大量的时间消耗在&nbsp;syscall.Read/Write，那么程序有可能执行大量小 IO；</p><p>如果 GC 组件消耗了大量的时间，程序可能分配了大量的小内存，或者分配的堆内存比较大。</p><p></p><h3>2.5&nbsp;代码演示</h3><p></p><p>代码见于文件&nbsp;<a href=\"https://link.zhihu.com/?target=https%3A//gist.github.com/cnutshell/80e1724c6bfcabe79485cf0b7167aca0\">contention_test.go</a>\"：</p><p>Block Profiling with Unit Test</p><p><code lang=\"go\">$ go test -run ^TestContention$ -blockprofile block.out\n$ go tool pprof block.out\n(pprof) top\n(pprof) web\n</code></p><p>Mutex Profiling with Unit Test</p><p><code lang=\"go\">$ go test -run ^TestContention$ -mutexprofile mutex.out\n$ go tool pprof mutex.out\n(pprof) top\n(pprof) web\n</code></p><p>Trace with Unit Test</p><p><code lang=\"go\">$ go test -run ^TestContention$ -trace trace.out\n$ go tool trace trace.out</code></p><p></p><p></p><h3>2.6&nbsp;参考资料</h3><p></p><p><a href=\"https://link.zhihu.com/?target=https%3A//pkg.go.dev/net/http/pprof%40go1.20%23hdr-Usage_examples\">net/http/pprof examples</a>\"</p><p></p><p></p><h2>Part 3 如何写性能测试</h2><p></p><p>性能问题不是猜测出来的，即便我们“强烈的认为”某处代码是性能瓶颈，也必须经过验证。</p><p></p><blockquote>\"Those who can make you believe absurdities can make you commit atrocities\" - Voltaire</blockquote><p></p><p>对于性能测试来说，很容易写出不准确的 Benchmark，从而形成错误的印象。</p><p></p><h3>3.1&nbsp;Reset or Pause timer</h3><p></p><p><code lang=\"go\">func BenchmarkFoo(b *testing.B) {\n  heavySetup()  // 在 for 循环之前执行设置工作，如果设置工作比较耗时，那么会影响测试结果的准确性\n  for i := 0; i &lt; b.N; i++ {\n    foo()\n  }\n}</code></p><p></p><p>优化方式</p><p><code lang=\"go\">func BenchmarkFoo(b *testing.B) {\n  heavySetup()\n  b.ResetTimer()  // 重置 timer，保证测试结果的准确性\n  for i := 0; i &lt; b.N; i++ {\n    foo()\n  }\n}</code></p><p>如何停止timer</p><p><code lang=\"go\">func BenchmarkFoo(b *testing.B) {\n  for i := 0; i &lt; b.N; i++ {\n    b.StopTimer() // 停止 timer\n    heavySetup()\n    b.StartTimer() // 启动 timer\n    foo()\n  }\n}</code></p><p></p><h3>3.2&nbsp;提高测试结果可信度</h3><p></p><p>对于 Benchmark，有很多因素会影响结果的准确性：</p><p>机器负载情况电源管理设置热扩展(thermal scaling)……</p><p>相同的性能测试代码，在不同的架构，操作系统下运行可能会产生截然不同的结果；</p><p>相同的 Benchmark 即便在同一台机器运行，前后也可能产生不一致的数据。</p><p>简单的方式是增加 Benchmark 运行次数或者多次运行测试来获取相对准确的结果：</p><p>通过&nbsp;-benchtime&nbsp;设置性能测试时间（默认 1秒）通过&nbsp;-count&nbsp;多次运行 Benchmark</p><p><code lang=\"go\">package benchmark\n\nimport (\n        \"sync/atomic\"\n        \"testing\"\n)\n\nfunc BenchmarkAtomicStoreInt32(b *testing.B) {\n        var v int32\n        for i := 0; i &lt; b.N; i++ {\n                atomic.StoreInt32(&amp;v, 1)\n        }\n}\n\nfunc BenchmarkAtomicStoreInt64(b *testing.B) {\n        var v int64\n        for i := 0; i &lt; b.N; i++ {\n                atomic.StoreInt64(&amp;v, 1)\n        }\n}\n</code></p><p>多次运行测试，得出置信度较高的结果：</p><p><code lang=\"go\">$ go test -bench Atomic -count 10 | tee stats.txt\n\n$ benchstat stats.txt\ngoos: darwin\ngoarch: arm64\npkg: github.com/cnutshell/go-pearls/benchmark\n                   │   stats.txt   │\n                   │    sec/op     │\nAtomicStoreInt32-8   0.3131n ± ∞ ¹\nAtomicStoreInt64-8   0.3129n ± ∞ ¹\ngeomean              0.3130n\n¹ need &gt;= 6 samples for confidence interval at level 0.95\n</code></p><p></p><blockquote>如果提示 benchstat 未找到，通过 go install 命令安装：go install&nbsp;http://golang.org/x/perf/cmd/benchstat@latest</blockquote><p></p><p></p><h3>3.3&nbsp;注意编译器优化</h3><p></p><p><code lang=\"go\">package benchmark\n\nimport \"testing\"\n\nconst (\n        m1 = 0x5555555555555555\n        m2 = 0x3333333333333333\n        m4 = 0x0f0f0f0f0f0f0f0f\n)\n\nfunc calculate(x uint64) uint64 {\n        x -= (x &gt;&gt; 1) &amp; m1\n        x = (x &amp; m2) + ((x &gt;&gt; 2) &amp; m2)\n        return (x + (x &gt;&gt; 4)) &amp; m4\n}\n\nfunc BenchmarkCalculate(b *testing.B) {\n        for i := 0; i &lt; b.N; i++ {\n                calculate(uint64(i))\n        }\n}\n\nfunc BenchmarkCalculateEmpty(b *testing.B) {\n        for i := 0; i &lt; b.N; i++ {\n                // empty body\n        }\n}\n</code></p><p>运行示例代码中的测试，两个测试的结果相同：</p><p><code lang=\"go\">$ go test -bench Calculate\ngoos: darwin\ngoarch: arm64\npkg: github.com/cnutshell/go-pearls/benchmark\nBenchmarkCalculate-8            1000000000               0.3196 ns/op\nBenchmarkCalculateEmpty-8       1000000000               0.3154 ns/op\nPASS\nok      github.com/cnutshell/go-pearls/benchmark        0.814s\n</code></p><p>那么如何避免这种情况呢，前面介绍 golang 接口的时候，给出了一个例子：</p><p><code lang=\"go\">var global interface{}\n\nfunc BenchmarkInterface(b *testing.B) {\n  var local interface{}\n  for i := 0; i &lt; b.N; i++ {\n    local = calculate(uint64(i)) // assign value to interface{}\n  }\n  global = local\n}\n</code></p><p>将被&nbsp;calculate&nbsp;的返回值赋给本地变量&nbsp;local，循环结束后将本地变量&nbsp;local&nbsp;赋值给一个全局变量&nbsp;global，这样可以避免函数&nbsp;calculate&nbsp;被编译器优化掉。</p><p></p><h3>3.4&nbsp;总结</h3><p></p><p>错误的性能测试结果会导致我们做出错误的决定，正所谓“差之毫厘，谬以千里”，写性能测试代码并不是表面上看起来的那么简单。</p>",
    "publish_time": "2023-02-20 11:36:51",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "窃取开源代码，还拉黑质疑者，这家 AI 公司试图删除代码了事",
    "url": "https://www.infoq.cn/article/PgmrICihkXIYCPc7cqyw",
    "summary": "<p>近日，一则关于 Voice. AI 从 Discord 服务器窃取开源代码，并拉黑质疑者的消息在网络上持续发酵。</p><p>Voice.ai 是一个语音转换 SDK 的开发商，他们还在多个平台上开发了类似的应用。</p><p></p><h2>违反开源协议被发现，当事人“拉黑”质疑者</h2><p></p><p></p><p>一位名叫 Ronsor 的软件开发兼安全研究员称，该公司的软件违反了其库中的两项<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247573492&amp;idx=2&amp;sn=c234eac2dcef39585e2f8b51d3010249&amp;chksm=fbeb663bcc9cef2db587c34fd3d8bab4a84a6d4b25fcdc1c9b54aca23a21f049c7a2a29f51da&amp;scene=27#wechat_redirect\">开源许可</a>\"（ <a href=\"https://www.infoq.cn/article/pXIBZMT1paUfhijmzN3b\">GPL</a>\" 和 LGPL 协议）。</p><p></p><p>据 Ronsor 的博文介绍，他在扫描该公司的 Windows 应用时发现其中包含两个第三方组件 <a href=\"https://github.com/praat/praat\">Praat</a>\" 和 libgcrypt，它们被静态链接到 VoiceAILib.dll 库中。也就是说，该公司在其专有软件中集成了开源语音分析软件 Praat 和密码库 libgcrypt 的代码，而没有发布其软件的源代码或提供适当的归属。</p><p></p><p>为了证明 Voice.ai 的应用包含与 Praat 库基本相似的代码，Ronsor 发布了该应用的反编译源代码，以方便与库中的函数进行比较。反编译 VoiceAILib.dll 后，Ronsor 发现很多函数与 Praat GitHub 存储库中的代码相匹配。</p><p></p><p>Ronsor 反编译的代码：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ee/eee2ad3b44432bdacd8c04669380df90.png\" /></p><p></p><p>原始代码：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3b/3b941dd3d88744b338a4a9d0983eae60.png\" /></p><p></p><p>这是令人担忧的，Praat 是根据 GPLv3 获得许可的 ，而 libgcrypt 是根据 LGPLv2.1 获得许可的，这些许可证根本不包含在软件中。</p><p></p><p>事实上，Voice.ai 在不遵守服务条款的情况下 违规打包了开源库， 该公司的服务条款 禁止复制、修改和重用该软件，这违反了提供这些自由的<a href=\"https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651116447&amp;idx=2&amp;sn=ee3d42df49c7e0de9798116dfcefe22c&amp;chksm=bdb92bcc8acea2da62b0dcdbd5c987348b3e23a66abcec48b0e8bb76e122bf2af1c6e921d99d&amp;scene=27#wechat_redirect\">开源许可</a>\"。Voice.ai 许可声明摘录：</p><p></p><p></p><blockquote>我们保留对 Beta 产品的所有权利、所有权。你同意 Beta 产品仅供个人使用。你不得将 Beta 产品或其任何部分或组件 出售、转让、转让、质押 或以任何方式阻碍或转让给任何第三方，或以任何方式使用它来生产、营销或支持您自己的产品。你不得向任何第三方复制、出售或营销 Beta 产品；修改、再利用、反汇编、反编译、逆向工程或以其他方式翻译 Beta 产品或其任何部分。</blockquote><p></p><p></p><p>Ronsor 还质疑称，该应用大量使用了混淆技术和它收集的数据，其中包括：主板和&nbsp;CPU&nbsp;信息、音频接口、操作系统版本、启用的网络接口、IP 地址和 MAC 地址、电脑主机名和 Voice.ai 安装路径。</p><p></p><p>“虽然其中一些信息在调试或其他方面有明显的合法用途（如音频接口、操作系统版本、安装路径），但其他信息，如计算机主机名和网络接口元数据，则与 Voice.ai 的主要功能完全不相关，”他说道。</p><p></p><p>Ronsor 认为，这些信息被发送到 Voice.ai 的服务器，在那里使用 API 生成通信加密。他还谈到，在 Discord 上的讨论中，有其他人指出，该代码包含虚拟机检测例程——可能是一种反取证技术。</p><p></p><p>Ronsor 观察到，“因为这个‘数字版权管理间谍软件’，我们无法离线运行 Voice.ai 的软件。虽然在技术上，这显然是可行的，因为它使用本地 GPU 来进行实时 AI 处理”。</p><p></p><p>在发现了诸多端倪后，Ronsor 表示他曾于 2 月 1 日试图通过 Discord 聊天工具联系 Voice.ai 公司，并在第二天通过电子邮件再次联系了他们，希望公司了解到他对违反许可的担忧。</p><p></p><p>但令人失望的是，因为他带来了麻烦，2 月 4 日，他被 Voice.ai 的 Discord 服务器封杀了，显然是这为了规避 DRM（数字版权管理）讨论。</p><p></p><p>Ronsor 表示他没有收到任何版主或开发人员的警告，而且在登录服务器期间他发送的消息少于 10 条，因此，他不相信我违反了任何合法规则。</p><p></p><p>截至 2 月 6 日（周一），他还没有收到该公司关于他的软件许可问询的答复。</p><p></p><h2>删除代码了事？</h2><p></p><p></p><p>当地时间 2 月 7 日，外媒 The Register 联系了 Ronsor，他说：“我还没有直接收到 Voice.ai 的回复。不过，他们 Discord 的版主公开表示他们已经通知了开发者，而开发者（应该）正在与他们的法律团队进行沟通。”</p><p></p><p>随着事情不断发酵，Voice.ai 坐不住了。</p><p></p><p>Voice.ai 开发人员完全坚持他们的软件根本不是恶意软件，但来自防病毒软件的广泛警告确实引发了一些问题。</p><p></p><p>2 月 8 日，Voice.ai 在接受 The Register 采访时表示，关于代码不当使用的说法是不实的，但该公司也承认，其软件包含了一些开源库，并且，在目前正在测试的更新中，他们删除了遵循 GPL 许可的代码。</p><p>Voice.ai 似乎也比较愿意友好地解决这个问题。该公司发言人于 2 月 9 日回复说，公司正在调查 Ronsor 的说法。</p><p></p><p>“我们注意到，最近有关于我们涉嫌不当使用<a href=\"https://www.infoq.cn/article/48bOExBvoPvix7RCTka1\">开源代码</a>\"的猜测。我们会非常严肃地对待这种性质的指控，我们明确声明，这些指控是虚假的，”该公司发言人在一份电子邮件声明中表示。</p><p></p><p>“我们的技术支持团队在 2 月 2 日晚上收到了来自用户 @ronsor 的源代码请求。我们的团队处理了大量的客户问询，因此，在 2 月 6 日即两个工作日后才得以处理这个请求。而此时，该用户已于 2 月 4 日发表了一篇博文，并开始在公共平台上提出指控。”</p><p></p><p>“与此同时，该用户加入了我们的公共 Discord 服务器，并参与了有关如何违反产品服务条款的对话，比如逆向工程，这导致我们的志愿者社区版主对其进行了封杀。这与源代码请求完全无关，当时没有人知道这一点，尤其是我们的 Discord 审核团队。\"</p><p></p><p>“作为一家以人工智能民主化为使命的初创公司，我们支持开源要求，完全遵守所有的开源许可。我们正设法尽快回应相关请求。我们对 @ronsor 的告知和请求表示感谢。”</p><p></p><p>“虽然我们的绝大多数代码都是由 Voice.ai 开发的闭源代码，但我们也包含了一些开源库。我们软件的核心技术并不依赖于这些库来实现。方便起见，我们将在 Github 存储库中提供相关源代码。而为了消除疑虑，我们删除了遵循 GPL3 的代码，并且几个小时内就完成了，这正是因为它只是作为一个最小的非核心功能。一旦 QA 审核通过，我们就会推送这个更新。”</p><p></p><p>“我们希望这最终会强化我们与开源社区的关系，并在此感谢 Discord 会员的支持。”</p><p></p><p>有更多细心的网友深挖后发现，Voice.ai 还违反了除上述两条许可外的其他开源许可，包括但不限于 libFLAC 的许可和 OpenH264。这些许可证需要署名，但在 Voice.ai 的代码中均没有给出。</p><p></p><p>2023 年 2 月 14 日，Voice.ai 开发者发布 0.1.26.1，似乎移除了 Praat；但是，它仍然包含 LGPL 的 libgcrypt，并且它们仍然违反其开源依赖项的许可要求。此外，他们还没有发布 0.1.25.1 的代码。</p><p></p><p>2023 年 2 月 17 日，随着 0.1.27.1 的更新，Voice.ai 开发人员终于将 libgcrypt 移到了自己的 DLL 中，并包含了其开源依赖项的许可证。不过，他们还是没有发布 0.1.25.1 的代码。</p><p></p><h2>“希望这些违规行为是出于无知，而非恶意”</h2><p></p><p></p><p>The Register 就该事件涉及的一些开发者关心的问题对 Ronsor 进行了提问。</p><p></p><p></p><blockquote>考虑到历史上和现实中开源社区对法律挑战的厌恶，你是否认为社区压力是处理所谓的开源许可违规的最佳方法？</blockquote><p></p><p></p><p>Ronsor：“如果没有证据证明是明显的恶意行为，我认为社区压力应该永远是第一选择，如果开发商遵守了许可，那么过去的违规行为就应该得到谅解。奖励良好行为很重要。如果向开发商施压无效，那么威胁采取法律行动就成了唯一的选择，还应该寻求金钱赔偿，因为诉讼需要花费时间和金钱，起初调查违规行为也需要花费时间和金钱。”</p><p></p><p>Ronsor 说，在很大程度上，他赞成自由软件基金会在这个问题上的执行原则。</p><p></p><p>Ronsor 坦言：“虽然我被 Voice.ai Discord 封杀了，但我仍然希望这些违规行为是由于无知而不是恶意。毕竟，许可很复杂。”</p><p></p><p>在开源圈里，窃取开源代码、违反开源协议的事件屡见不鲜。在社区开发者看来，这种短期投机、违反道德的行为是不可能取得成功的。</p><p></p><p>一位曾经在 Facebook 工作的开发者表示：“即使忽略与此类问题相关的法律或道德问题，如果有人窃取 GCC 并尝试将其作为自己的产品进行营销，那么他其实是在销售已经免费的产品。不太可能成为成功的商业策略，现在销售的大多数软件都是卖给企业的，即使是一丁点的许可证问题也足以扼杀特定产品的市场。”</p><p></p><p>此外，一位曾经在微软工作过的网友表示：“我记得我在 Microsoft 的第一天，我的开发经理告诉我在 Microsoft 编写代码的一条基本规则——永远不要在网络上查找任何可用的开源或第三方代码。就算单纯为了好玩儿也不能去这么做，因为你永远不知道你会下意识地复制哪一个。”</p><p></p><p>声明：本文为 InfoQ 翻译 整理，未经许可禁止转载。</p><p></p><p>参考链接：</p><p>https://www.theregister.com/2023/02/08/voiceai_open_source</p><p></p><p>https://undeleted.ronsor.com/voice.ai-gpl-violations-with-a-side-of-drm/</p><p></p><p>https://www.quora.com/Why-don%E2%80%99t-people-just-steal-open-source-code-do-a-quick-restructuring-and-sell-it-commercially-as-their-own-Is-there-a-way-to-prove-if-you-think-someone-is-using-your-OS-code-in-a-closed-source-code-format-for-profit</p>",
    "publish_time": "2023-02-20 14:32:33",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "汽车行业如何跑出“内卷圈”？直击头部车企数字化实践｜直播报名预约",
    "url": "https://www.infoq.cn/article/eUcJa1KiLxhIOJZrUjWr",
    "summary": "<p>近几年，汽车行业“卷”得厉害。增量市场放缓，新能源带动更多造车新势力入局，僧多粥少，导致存量竞争加剧。这要求车企真正“以用户为中心”，深度挖掘用户全生命周期价值，提供更符合市场需求的产品和服务。与此同时，双碳目标下日益严格的排放标准，也要求汽车行业在产品研发和生产等各个环节持续践行绿色理念。</p><p></p><p>多重挑战驱动下，汽车行业数字化转型驶向快车道。通过把大数据、云计算、人工智能、车联网等数字技术全面融入车企的全生命周期运营体系，将深度重构汽车价值链及其运营模式，帮助汽车主机厂、汽车经销商、汽车 4S 店实现降本增效、产业升级，提高竞争力。据麦肯锡预测，2030 年数字化将为汽车行业创造 10%-30% 的行业收入，为行业创收增加新动能。</p><p></p><p>汽车产业链路极长，数字化覆盖从研发、生产、供应链、营销、服务的各个环节。那么，目前各大车企在具体场景的数字化现状如何？挑战是什么？在具体的实践过程中，技术又如何与业务充分融合？</p><p></p><p>本期《超级连麦. 数智大脑》邀请了多位汽车行业专家，将着重围绕数字化研发、数字化生产和数字化供应链三大场景，直击头部车企数字化实践，共同探讨汽车行业的数字化转型路径。</p><p><img src=\"https://static001.geekbang.org/infoq/07/077036d6882cde133e2975b31b9f74cc.png\" /></p><p></p><p></p><h3>直播主题</h3><p></p><p>从研产供三场景，直击头部车企数字化实践</p><p></p><h3>时间</h3><p></p><p>2023 年 2 月 23 日 20:00-21:30</p><p></p><h3>直播嘉宾</h3><p></p><p></p><p>肖庆阳，大连亚明汽车部件股份有限公司副总经理，工信部产教融合产业实践教授，大连市工业信息化专家咨询委员会委员，大连市智能制造产业协会副会长。在企业数字化转型、智能制造、工业互联网领域有丰富的规划和实施经历，主持完成国家智能制造新模式专项 1 项；国家工业互联网试点示范项目 1 项；辽宁省创新发展工程和大连市科技重大专项各 1 项。荣获东北数字化领军人物、中国制造业杰出 CIO 等荣誉称号。</p><p></p><p>戚海飞，华晨宝马数字化生产与物流总监。拥有 20 年世界 500 强企业 IT 管理经验，曾担任华晨宝马及宝马中国数字化技术总监，创新管理负责人等。多年企业数字化转型实践者及推动者，通过创新及数字化，建立数字化能力及将新技术规模化的应用到业务场景中实现商业价值。目前致力于实践和探索企业数字化转型，工业互联网，5G，物联网，人工智能，大数据等技术在智能制造及销售市场领域的大规模应用, 实现企业在数字化领域 “在中国，为中国，为世界!”的战略。</p><p></p><p>李炜，一汽股权投资公司高级投资经理，负责投研一体化业务。拥有 10 余年工作经验，曾任一汽集团产品部智网生态主任、红旗品牌智能网联产品规划及定义业务负责人，曾先后任红旗品牌多个创新产品项目负责人。</p><p></p><p>霍太稳，极客邦科技创始人兼 CEO，InfoQ 中国创始人，极客时间创始人，TGO 鲲鹏会发起人。2007 年创立 InfoQ 中国，2014 年创立极客邦科技，2015 年发起 TGO 鲲鹏会，2017 年创立在线职业教育学习品牌极客时间，2019 年开创极客时间企业版，拓展企业服务市场。</p><p></p><p></p><h3>直播亮点</h3><p></p><p>了解汽车行业发展现状及挑战，探讨车企如何通过数字化跑出“内卷圈”从研发、生产、供应链、人才管理多个场景维度，拆解头部车企的数字化实践了解最受汽车行业关注的数字技术，及其与汽车业务场景融合的秘诀汽车数字化路径能给其他制造业转型升级带来哪些启示和参考</p><p></p><h3>直播预约</h3><p></p><p>扫码立即报名预约直播，还可以留下你关心的问题，讲师将在直播中为你解答</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9f/9f85f975e26351fed4fcb63ff77e9230.png\" /></p><p></p><p></p><h4>关于《超级连麦. 数智大脑》</h4><p></p><p>《超级连麦. 数智大脑》是 InfoQ 重磅推出的一档连麦直播栏目，由数字化领域的思想领袖、技术大咖及企业先行者等多方连线，聚焦企业数字化实践、数字人才培养、数字化技术管理等话题，剖析和拆解典型数字化场景及其痛点，意在帮助各行业企业扫清转型障碍、探寻变革路径。</p><p></p><p>每期直播将邀请 2-3 位嘉宾围绕同一话题，从不同维度展开深度讨论。我们希望能够深入行业最核心的话题，通过充分的现场碰撞和探讨，对关键问题进行深刻的剖析，既有理论也有实践，既有战略也有战术，为观众呈现更为全方位、多视角，更有代入感、更具启发性的数字化转型参考。</p>",
    "publish_time": "2023-02-20 15:09:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "马斯克猛烈抨击 ChatGPT引轰动：“这不是我想要的”",
    "url": "https://www.infoq.cn/article/sJSY1kx480W0StNrQHDH",
    "summary": "<p></p><blockquote>埃隆·马斯克 (Elon Musk) 否认了他最著名的作品之一。&nbsp;</blockquote><p></p><p>&nbsp;</p><p>特斯拉、SpaceX 和Twitter 的<a href=\"https://www.infoq.cn/article/gw50bzhlrref9gavjjdc\">首席执行官</a>\"以热爱工作每天睡觉不超过6小时而闻名，他脑子里想着几件事，其中一件事关OpenAI的发展，太平洋时间凌晨 1 点 36 分，他发表推文指责OpenAI违背初心：被微软控制，只顾赚钱。</p><p>&nbsp;</p><p>作为联合创始人之一，<a href=\"https://www.infoq.cn/article/5LYIHLWuPVU8btDrfBut\">马斯克</a>\"对OpenAI的现状非常不满：“OpenAI最初是作为一家开源（这就是为什么我把它命名为‘Open’AI）的非营利性公司而创建的，为了抗衡谷歌，但现在它已经成一家闭源的营利性公司，由微软有效控制……这完全不是我的本意。”马斯克于 2018 年离开 OpenAI 董事会，不再持有该公司股份，但显然他对OpenAI的现状依然十分关心。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d07060a625e9d5ec8b7ed59d4bfe354.jpeg\" /></p><p></p><p>&nbsp;</p><p>紧接着，他又发推文晒出一张聊天截图，让ChatGPT回复“创建一个非营利组织，然后在其下用非营利组织的资源衍生一个盈利公司”的问题，并借用ChatGPT的观点来批评OpenAI。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/af/aff4e7ac8f0dd9e0b616b9e53b67793b.jpeg\" /></p><p></p><p>&nbsp;</p><p>ChatGPT回复说，打着非营利组织的幌子开办营利性企业，是很不道德和非法的。非营利组织的资源用于慈善或公共目的，应以透明和负责任的方式运作。任何滥用非营利组织特权的企图只会导致公众对其失去信任，并可能导致法律后果。</p><p>&nbsp;</p><p>过去两个月里，ChatGPT 已经表明了人工智能取得了令人难以置信的进步，但根据马斯克的说法，这应该是我们该担心的事情。这位亿万富翁长期以来一直警告不受监管的人工智能开发可能会带来危险，他曾表示，人工智能比核弹头“危险得多”。</p><p>&nbsp;</p><p>作为 OpenAI 的联合创始人，马斯克还曾于去年在世界政府峰会上发表评论说，ChatGPT“向人们展示了人工智能已经变得非常先进”，“它只是没有大多数人可以访问的用户界面。”</p><p>&nbsp;</p><p>他补充说，汽车、飞机和医药必须遵守监管安全标准，但人工智能还没有任何规则或法规来控制其发展。“坦率地说，我认为我们需要规范人工智能安全，”马斯克说。“我认为，人工智能比汽车、飞机或药品对社会带来的风险更大。监管可能会稍微减慢人工智能的发展速度，但我认为这也可能是一件好事。”</p><p>&nbsp;</p><p></p><h2>从创立非营利组织，到与OpenAI分道扬镳</h2><p></p><p>&nbsp;</p><p>2015年，埃隆·马斯克和前 Y Combinator 总裁 Sam Altman 共同创立了 OpenAI。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b7/b7c96fd0422d4894c5b63dbb63c29b5d.png\" /></p><p></p><p>&nbsp;</p><p>人工智能是一把双刃剑，可以用来做好事，也可以拿来做坏事。作为 OpenAI 的联合创始人之一，马斯克当初成立 OpenAI 的初衷是为了确保该技术不会被滥用。马斯克等人认为 AI 不应该为个人或公司独有，它属于全人类；OpenAI 的目标是将在人工智能领域的研究结果开放地分享给全世界。马斯克坚持要求 OpenAI 开发的技术是开源的，他认为避免AI被拿来做坏事的方法就是广泛地传播这项技术，不要限制人们掌握这项技术，开放这项技术让所有人都可以用，这样就能减轻超级智能可能会带来的威胁，</p><p>&nbsp;</p><p>OpenAI成立的当年，马斯克就强调，人工智能是人类“最大的生存威胁”。马斯克并不是唯一一个警告过人工智能潜在危害的人。2014 年，史蒂芬霍金警告说人工智能可能终结人类。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/93/93084f7bb554a972d9c50e9c632c33fb.png\" /></p><p></p><p>&nbsp;</p><p>“很难想象人类水平的 AI 能给社会带来多大的好处，同样难以想象如果构建或使用不当会对社会造成多大的损害，”一份宣布 Open AI 成立的声明写道。</p><p>&nbsp;</p><p>然后到了2018年，马斯克在OpenAI 官方博客上宣布离开公司董事会。博文中提到，虽然马斯克会离开 OpenAI 董事会，但并不会与 OpenAI 彻底撇清关系，他仍然会继续为该公司提供支持。</p><p>&nbsp;</p><p>就在宣布马斯克离开 OpenAI 董事会的前一阵子，OpenAI 还发布了一篇论文，讨论了 AI 技术可能给人类带来的负面威胁，并呼吁各方对 AI 技术保持警惕，避免 AI 技术被滥用。</p><p>&nbsp;</p><p>2019 年，该公司开发了一款可以制作假新闻的人工智能工具。起初，OpenAI 表示该机器人非常擅长编写假新闻，因此他们并不准备发布它。但同年晚些时候，OpenAI 还是发布了&nbsp;NLP 模型 GPT-2（2020年，发布了GPT-3）。该模型可以生成连贯的文本段落，流畅地写出一段文章。从技术的角度来看，这确实是不小的进步，但是如果一旦被滥用，这一技术极有可能被用来生产假新闻，造成社会恐慌等负面影响。</p><p>&nbsp;</p><p>几天后，马斯克宣布跟 OpenAI 彻底分道扬镳。马斯克表示：由于自己与 OpenAI 内部一些发展观点上的不和，所以选择了退出，并且自己已经一年多没有跟 OpenAI 合作过了，之后将专注于解决特斯拉以及 SpaceX 面临的大量工程问题。</p><p>&nbsp;</p><p>不久之后，OpenAI宣布接受微软10亿美元注资，摆脱了非营利组织的地位。在新的利润结构下，OpenAI 投资者可以获得高达其原始投资的 100 倍的收益，剩下的钱将用于非营利性工作。</p><p>&nbsp;</p><p>微软通过注资，获得了GPT-3的底层代码，用来整合到微软的产品和服务当中去。在一篇博文中微软表示：“通过 GPT-3 模型，我们可以释放出巨大的商业潜能，直接帮助人们写作、描述和总结大量数据、将自然语言转换成另一种语言等等，未来的可能性只会受限于人们的想法和方案。”</p><p>&nbsp;</p><p></p><h2>马斯克积极“吃瓜”</h2><p></p><p>&nbsp;</p><p>2020年，<a href=\"https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-model/\">麻省理工学院技术评论</a>\"对微软和OpenAI的交易表达了不满，称OpenAI 本应造福人类，而现在 ，它只是在造福世界上最富有的公司之一。马斯克也发推文说，“这看起来确实是‘开放’的对立面。OpenAI 基本上被微软‘控制’了。”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/96ffed245406c448c4d000544a63910f.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>2022年11 月 30 日， OpenAI 演示了使用GPT-3.5模型的聊天机器人，该公司计划接下来发布完整的 GPT-4。</p><p>&nbsp;</p><p>与此同时，马斯克仍在发表评论。“我们离危险的强大人工智能不远了，”马斯克在推特上回应 Sam Altman的帖子时说。他称 ChatGPT “好得吓人”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d097bb6055953380200d242707d8508.png\" /></p><p></p><p>&nbsp;</p><p>对于媒体的负面报道，马斯克也积极吃瓜。</p><p>&nbsp;</p><p>2月16日，有新闻报道Bing聊天机器人发表“我不会伤害你，除非你先伤害我”的言论，马斯克转发并评论说“可能需要再润色一下……”</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/ade6100b58b3fe3d14fe4653d1ffe3e2.jpeg\" /></p><p></p><p>&nbsp;</p><p>对于stratechery.com发表的聊天机器人“好斗个性”报道，马斯克转发并表示：“有意思”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/34/345d315b69376379382c63e90dd08528.png\" /></p><p></p><p>&nbsp;</p><p>似乎他对任何Bing的负面事件都感兴趣，对于类似“Bing要报复人类”推文，马斯克都会进行点评。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5de7975732e6414d60849ae39a2b8f3b.jpeg\" /></p><p></p><p>&nbsp;</p><p>并且宣称我们需要的是一个“TruthGPT”：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/fa/fa0ffd4e101792e1873b012f97d9b991.png\" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>写在最后</h2><p></p><p>&nbsp;</p><p>对于马斯克的人工智能威胁论，虽然AI乐观派们一直认为这是反应过激，有点危言耸听，但是放到ChatGPT爆火之后，我们或许更能理解马斯克的初衷。人们也感觉到负责任地发展一项技术有多么重要，因此马斯克的这次言论得到了更多人的重视，推特用户纷纷发表评论：“疯狂的商业世界充满了假装做慈善工作的骗子。”“WokeGPT”......各方媒体也都给予了中立的报道。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/38ae369dc40a2f7b9f93b026e1715306.jpeg\" /></p><p></p><p>&nbsp;</p><p>OpenAI 是在深度学习的炒作中应运而生的，巨头公司们在 ImageNet 博得眼球之后对深度学习给予了厚望。亚马逊、谷歌、微软、IBM、Facebook……他们生怕自己落在他人之后，而要在竞争中不至于落败，就要吸引足够多的资本介入。然而资本家不是慈善机构，尽管他们身负一定的社会责任，但对于他们来说，首要关注的事情仍然是如何从投资中获得回报。</p><p>&nbsp;</p><p>OpenAI最初受到推崇，也是因为它的使命感：希望确保该技术得到安全开发，并将其收益平均分配给全世界。现在我们也知道了算法是有偏见的，同时也是脆弱的；他们可以造假，可以欺骗用户。如果被企业资本推动，往​​往会将它们的权力集中在少数人手中，并因此失去精心监督和指导，那么结果有可能是灾难性的。正像马斯克所说：“这既是积极的，也是消极的，具有巨大的前景和巨大的能力，”但同时也伴随着“巨大的危险”。</p>",
    "publish_time": "2023-02-20 15:36:13",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "ChatGPT又遇“克星”！OpenAI发布全新AI内容识别工具，成功率26%",
    "url": "https://www.infoq.cn/article/1Sb8qfMAjMivNCWtxRdV",
    "summary": "<p>过去几个月，由人工智能实验室<a href=\"https://www.infoq.cn/article/ZWixRo76hFsOw38tRHNF\">OpenAI</a>\"发布的对话式大型语言模型<a href=\"https://www.infoq.cn/article/YYqCPSdRmtkdSl2hhb9Y\">ChatGPT</a>\"在全球范围内掀起狂热之风。为了帮助用户判断哪些内容是由ChatGPT生产的，包括OpenAI、斯坦福大学等多家机构开始研究相应的<a href=\"https://www.infoq.cn/news/tQMKFz2uRpy2eUBitrgv\">AI内容识别工具</a>\"。</p><p>&nbsp;</p><p>近日，OpenAI推出一个经过训练的分类器，用以区分文本是否是由 AI 编写的。据其介绍，该分类器虽然不可能可靠地检测所有 AI 编写的文本，但能够通过提供信息来减少 AI 生成式文本是由人工编写的误判：例如，执行自动虚假内容营销，利用 AI 工具进行学术欺诈，以及将 AI 聊天机器人定位为人类。</p><p>&nbsp;</p><p>据悉，该分类器是一种语言模型，该模型对基于同一主题的人工编写文本和 AI 编写文本的数据集进行不断调整。OpenAI从人工编写的各种来源收集数据集，例如预训练数据以及提交到 InstructionGPT 的各种人工的演示，将每个文本分为提示和响应。根据提示，可以从分类器以及其他组织训练的各种不同语言模型中生成响应。OpenAI调整了 Web App 的置信度阈值，可以保持较低的误报率。换句话说，只有分类器非常有把握的时候，才会将文本标记为可能是 AI 编写的。</p><p>&nbsp;</p><p>目前，该分类器的准确性还很低。据悉，OpenAI对英语文本“Challenge Set”进行了评估，分类器正确地将 26% AI 编写的文本（真阳性）识别为“可能是 AI 编写的”，而错误地将 9% 人工编写的文本标记为 AI 编写的（假阳性）。</p><p>&nbsp;</p><p>OpenAI表示，目前该分类器仍存在局限性，所以不应该将它作为主要决策工具，而应该作为确定文本来源的其他方法的补充。具体来说：</p><p>分类器在短文本（1,000个字符以下）上非常不可靠。甚至更长的文本有时也会被错误标记。有时，人工编写的文本会被错误但自信地标记为AI 编写。建议仅对英语文本使用分类器。它在其他语言中的表现明显较差，代码不是很有效。无法可靠地识别已经十分确定的文本。例如，无法预测前1,000个素数的列表是 AI 还是人写的，因为谁来写都是一样的。编辑 AI 编写的文本可以避开分类器。虽然所有类似的分类器可以通过成功的检测来更新算法并重新训练，但目前还不清楚这种检测是否可以具有长期优势。众所周知，基于神经网络的分类器除了训练数据外的其它的校准效果很差。如果输入的文本与训练集中的非常不一样，分类器多半会得到错误的预测。</p><p>&nbsp;</p><p>OpenAI认为，分类器的可靠性，通常会随着输入文本长度的增加而提高。与OpenAI此前发布的分类器相比，新的分类器对来自最新 AI 系统编写的文本会更加有效。</p><p></p><p>参考链接：</p><p><a href=\"https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/\">https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/</a>\"</p><p></p>",
    "publish_time": "2023-02-20 15:57:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]