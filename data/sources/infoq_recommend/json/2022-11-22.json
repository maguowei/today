[
  {
    "title": "eBPF与 Service Mesh：Layer7处理不太可能在eBPF中实现",
    "url": "https://www.infoq.cn/article/7xFHFHbygmwN9cTsoEgc",
    "summary": "<p>关于eBPF的故事已经在云计算世界泛滥了一段时间。有时候，人们会把它描述成自切片面包以来最伟大的东西，有时候又嘲笑它是对现实世界无用的干扰。当然，现实情况要微妙得多，所以我们似乎有必要仔细了解一下eBPF能做什么和不能做什么——技术毕竟只是工具，我们应该为手头要处理的任务选择合适的工具。</p><p></p><p>最近经常出现的一个问题与服务网格（Service Mesh）所需的Layer7处理有关。将这个任务交给eBPF处理可能是服务网格的一个巨大胜利，因此让我们来仔细地研究一下eBPF可能扮演的角色。</p><p></p><p></p><h2>那么eBPF到底是什么</h2><p></p><p></p><p>我们先来看看这个名字——“eBPF”最初是指“扩展的伯克利包过滤器（extended Berkeley Packet Filter）”，尽管现在它不指代任何东西。伯克利包过滤器（BPF）可以追溯到近30年前——它是一种允许用户应用程序直接在操作系统内核中运行特定代码的技术（当然是经过严格审查和高度约束的代码）。BPF仅应用于网络栈，但仍然可能促成一些令人惊奇的事情。</p><p></p><p>一个经典的例子是，它让试验新类型的防火墙等东西变得非常容易。我们不需要不断地重新编译内核模块，只需要编辑eBPF代码并重新加载它们。类似的，它可以轻松地开发出一些非常强大的网络分析功能，包括你不想在内核中运行的东西。例如，如果你想使用机器学习对传入的数据包进行分类，可以使用BPF抓取感兴趣的数据包，并将它们分发给运行ML模型的用户应用程序。</p><p></p><p>以上是两个非常明显的可能应用BPF的领域【1】，除此之外还有其他一些例子——eBPF将同样的概念扩展到网络以外的领域。但所有这些都涉及一个问题——为什么它们都需要引起我们特别的注意。</p><p></p><p>简单地说，这与“隔离性”有关。</p><p></p><p></p><h2>隔离性</h2><p></p><p></p><p>计算——尤其是原生云计算——严重依赖于硬件同时为多个实体做多件事情的能力，即使有些实体对其他实体是不友好的。这就是多租户争用，我们通常使用可以调解对内存访问的硬件来管理多租户争用。例如，在Linux中，操作系统为自己创建一个内存空间（内核空间），并为每个用户程序创建一个单独的空间（总的来说就是用户空间，尽管每个程序都有自己的空间）。然后，操作系统使用硬件来防止跨空间的访问【2】。</p><p></p><p>维护系统各部分之间的这种隔离对于安全性和可靠性来说都是绝对关键的——与计算安全相关的东西基本上都依赖于它，而原生云对它的依赖甚至更严重，因为云原生要求内核能够保持容器之间的隔离。因此，内核开发人员花了数千人年的时间仔细检查与隔离性相关的每一个交互，并确保内核可以正确地处理所有东西。这是一项棘手的、微妙的、艰苦的任务。而且很不幸的是，在发现bug之前，它常常不会被注意到，而这些又占了操作系统工作的很大一部分【3】。</p><p></p><p>这项工作之所以如此棘手和微妙，部分原因在于内核和用户程序不能完全隔离：用户程序显然需要访问某些操作系统函数。传统上说，这属于系统调用。</p><p></p><p></p><h2>系统调用</h2><p></p><p></p><p>系统调用是操作系统内核向用户代码公开API的一种原始的方式。用户代码隐藏了大量细节，将请求打包并将其交给内核。内核会进行仔细检查，确保所有规则都得到了遵守，并且——如果一切正常——内核将代表用户执行系统调用，并根据需要在内核空间和用户空间之间复制数据。系统调用的关键部分是：</p><p></p><p>内核控制着一切。用户代码可以发出请求，而不是要求。检查、复制数据等操作都需要时间，导致系统调用比运行普通代码更慢，无论是用户代码还是内核代码：这是一种跨越边界的行为，会降低速度。随着时间的推移，执行速度变得越来越快，但是，为繁忙系统中的每一个网络包都执行系统调用是不可行的。</p><p></p><p>而这就是eBPF可以发挥作用的地方：不为每一个网络包（或跟踪点或其他什么）执行系统调用，而是直接将一些用户代码放到内核中！然后，内核就可以全速运行，只有在真正需要的时候才将数据分发到用户空间。（最近，人们对Linux的用户/内核交互进行了大量的反思，带来了很好的效果。<a href=\"https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjkwMTc1MzksImZpbGVHVUlEIjoiUHNJakdOeUJEZmdmZmxFUSIsImlhdCI6MTY2OTAxNzIzOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.8jHOZ5SPfkclJWvVKJOx-ulwkn0LjLcKqggqPG7OSm8\">io_uring</a>\"就是这方面的一项杰出的工作。）</p><p></p><p>当然，在内核中运行用户代码是非常危险的，因此内核要花费大量的精力来验证用户代码想要做什么事情。</p><p></p><p></p><h2>eBPF验证</h2><p></p><p></p><p>当一个用户进程启动时，内核基本上会在可能正常的范围内启动它。内核在它周围设置了护栏，并立即杀死任何试图破坏规则的用户进程，但用户代码被假定具有执行的权利。</p><p></p><p>对eBPF代码则不提供这样的礼遇。在内核中，保护屏障基本上是不存在的，它会盲目地运行用户代码，并希望它们是安全的，这为安全漏洞打开了大门（一些错误可能会带崩整台机器）。相反，eBPF代码只有在内核能够证明它是安全的情况下才会被执行。</p><p></p><p>要证明一个程序是安全的是非常困难的【4】。为了让它变得更容易些，内核极大地限制了eBPF程序可以做的事情。以下是一些例子。</p><p></p><p>不允许eBPF程序发生阻塞。不允许eBPF程序有无界的循环（事实上，直到最近它们才被允许有循环）。不允许eBPF程序超过一定的体积。验证器必须能够评估所有可能的执行路径。</p><p></p><p>验证器必须非常严格，它的决策是决定性的：它必须这样做才能维持整个云原生世界所需的隔离性保证。它在判断程序是不安全的时候还必须宁可杀错不可放过：如果它不能完全确定程序是安全的，就会将其拒绝。不幸的是，有些eBPF程序是安全的，但是验证程序不够聪明，无法通过验证——如果你遇到了这种情况，需要重写程序，直到验证器通过，或者需要给验证器打补丁并构建自己的内核【5】。</p><p></p><p>最终的结果是，<a href=\"https://xie.infoq.cn/article/34d62a334c9af2134e1ab0b02\">eBPF</a>\"成了一种高度受限的语言。也就是说，虽然对每个传入的网络数据包执行简单的检查是很容易的，但像跨多个数据包缓冲数据这样看似简单的事情却很难。在eBPF中实现HTTP/2或终止TLS是完全不可能的：它们太复杂了。</p><p></p><p>最后，所有这些都把我们引向了一个问题：将eBPF的网络功能应用到服务网格中将会是什么样子？</p><p></p><p></p><h2>eBPF和服务网格</h2><p></p><p></p><p>服务网格必须处理云原生网络的所有复杂性。例如，它们必须产生和终止mTLS、重试失败的请求、将连接从HTTP/1透明地升级到HTTP/2、基于工作负载标识执行访问策略、跨集群边界发送流量，等等。在原生云世界中有很多事情正在发生。</p><p></p><p>大多数服务网格使用边车模型来管理一切。网格将一个运行在自己容器中的代理附加到每个应用程序Pod中，代理拦截与应用程序Pod之间的网络流量，完成网格所需的任何工作。这意味着网格可以处理任意的工作负载，并且不需要更改应用程序，这对开发人员来说是一个巨大的胜利。这对平台方也是一种胜利——他们不再需要依赖应用程序开发人员来实现mTLS、重试、黄金指标【6】等，因为网格在整个集群中提供了所有这些东西，甚至更多。</p><p></p><p>另一方面，就在不久前，人们还认为部署这些代理的想法是非常疯狂的，因为他们仍然担心运行额外容器所带来的负担。但是<a href=\"https://www.infoq.cn/topic/Kubernetes\">Kubernetes</a>\"使部署变得很容易，只要你能够保持代理的轻量级和速度，它确实可以很好地工作。（“轻量级和速度”当然是带有主观性。许多网格使用通用的Envoy代理作为边车。Linkerd似乎是唯一一个使用专门构建的轻量级代理的。）</p><p></p><p>因此，一个显而易见的问题是，我们是否可以让功能从边车下沉到eBPF中。在OSI的Layer3和Layer4——IP、TCP和UDP——我们已经看到了eBPF的几个明显的优势。例如，eBPF可以让复杂的动态IP路由变得相当简单。它可以进行非常智能的包过滤，或者进行复杂的监控，而且它可以快速和低成本地完成所有这些任务。在网格需要与这些层交互的地方，eBPF似乎非常有用。</p><p></p><p>然而，在OSI Layer7情况就不同了。eBPF的执行环境如此有限，以至于HTTP和mTLS级别的协议远远超出了它的能力，至少在今天是这样。鉴于eBPF在不断地发展，也许未来的某个版本可以管理这些协议，但需要注意的是，编写eBPF非常困难，而调试可能会更加困难。许多Layer7协议都非常复杂，在相对宽松的用户空间中，它们糟糕到不能正常使用。目前还不清楚为eBPF重写它们是否可行，即使这么做是可能的。</p><p></p><p>当然，我们可以做的是将eBPF与代理配对：将核心底层功能放在eBPF中，然后将其与用户空间代码配对，以此来管理复杂性。通过这种方式，我们可以在较低的级别上获得eBPF的性能优势，同时将真正讨厌的东西留在用户空间中。这实际上是今天每个现有的“eBPF服务网格”所做的，尽管通常没有被广泛公开。</p><p></p><p>这就提出了一些关于这样的代理到底应该被放在哪里的问题。</p><p></p><p></p><h2>单主机代理与边车</h2><p></p><p></p><p>我们不为每一个应用程序Pod部署一个代理（正如我们在边车模型中所做的那样），而是在每台主机（在kubernetes中就是节点）上部署一个代理。它给管理IP路由带来了一点额外的复杂性，但似乎也提供了一些良好的规模经济，因为你所需要的代理变少了。</p><p></p><p>不过，边车比单主机代理有一些显著的好处。这是因为边车被作为应用程序的一部分，而不是独立于应用程序之外。</p><p></p><p>边车使用的资源与应用程序负载成正比，所以如果应用程序的负载不重，边车的资源使用就不会很高【7】。当应用程序负载很重时，所有Kubernetes的现有机制（资源请求和限制、OOMKiller等）都将完全按照你所习惯的方式工作。如果一个边车发生故障，它只会影响一个Pod，而现有的Kubernetes机制也能正常对Pod故障做出响应。边车操作基本上与应用程序Pod操作相同。例如，通过一个正常的Kubernetes滚动重启将边车升级到新版本。边车和它的Pod有着完全相同的安全边界：相同的安全环境、相同的IP地址，等等。例如，它只需要为它的Pod处理mTLS，这意味着它只需要单个Pod的密钥信息。如果代理中有bug，它只会泄漏这一个密钥。</p><p></p><p>对于单主机代理，所有这些问题都消失了。请记住，在<a href=\"https://www.infoq.cn/article/E8Eiuhyg4BhbEKPcN1KV\">Kubernetes</a>\"中，集群调度器决定将哪些Pod调度到给定节点上，这意味着每个节点将有效地获得一组随机的Pod。这也意味着一个给定的代理将与应用程序完全解耦。</p><p></p><p>对单代理的资源使用情况进行推断实际上是不可能的，因为它是由流向随机应用程序Pod的随机流量驱动的。反过来，这意味着代理最终会因为一些难以理解的原因而失败，而网格团队将为此承担责任。应用程序更容易受到噪音邻居（noisy neighbor）问题的影响，因为安排给指定主机上的Pod的流量都必须通过单个代理。一个高流量的Pod完全可能消耗掉节点的所有代理资源，并让其他Pod挨饿。代理可以尝试确保公平性，但如果高流量的Pod也在消耗所有节点的CPU，代理的尝试也会失败。如果一个代理失败，它会影响应用程序Pod的一个随机子集——而这个子集将不断发生变化。同样，尝试升级一个代理将影响一个类似的随机的、不断变化的应用程序Pod子集。任何故障或维护任务都会突然产生不可预测的副作用。代理现在必须跨越应用程序Pod的安全边界，这比只与单个Pod耦合的情况要复杂得多。例如，mTLS要求持有每个Pod的密钥，不能混淆了密钥与Pod。代理中的任何bug都可能导致可怕的后果。</p><p></p><p>从根本上说，边车利用了容器模型的优势：内核和Kubernetes努力在容器级别强制执行隔离和公平性，一切都能正常工作。单主机代理超出了该模型，这意味着它们必须自己解决多租户争用的所有问题。</p><p></p><p>单主机代理确实是有优势的。首先，在Pod世界中，从一个Pod到另一个Pod总是需要经过两次代理。而在单主机代理世界中，有时只需要经过一次【8】，这可以减少一点延迟。此外，你可以运行更少的代理，如果你的代理在空闲时有很高的资源使用，这样可以节省资源消耗。不过，与运维和安全方面的成本相比，这些改进带来的好处是很小的，这些问题主要可以通过使用更小、更快、更简单的代理来缓解。</p><p></p><p>我们是否可以通过改进代理来更好地处理多租户争用来缓解这些问题？也许吧。这种方法存在两个主要问题。</p><p></p><p>多租户争用是一个安全问题，安全问题最好使用更小、更简单、更容易调试的代码来处理。通过添加大量代码来处理多租户争用问题基本上与安全最佳实践是南辕北辙的。即使安全问题能够得到解决，仍然会存在业务问题。在任何时候，当我们选择进行更复杂的操作时，我们都应该问问为什么要这么做，以及谁会受益。</p><p></p><p>总之，这种代理上的变化很可能涉及大量的工作【9】，为此我们非常关注这些工作所能带来的价值。</p><p></p><p>让我们回到最初的问题：将服务网格功能下沉到eBPF会是什么样子？我们知道我们需要一个代理来维护我们所需的Layer7功能，我们还知道边车代理可以在操作系统的隔离保证范围内运行，单主机代理必须自己管理一切。这是一个不小的差异：单主机代理的潜在性能优势远远超过额外的安全问题和操作复杂性，因此无论是否使用eBPF, 边车都是最可行的选择。</p><p></p><p></p><h2>展望未来</h2><p></p><p></p><p>显然，任何服务网格的第一优先级必须是用户的操作体验。我们可以通过eBPF来获得更好的性能和更低的资源使用，这太棒了！但需要注意的是，我们不能在这个过程中牺牲用户体验。</p><p></p><p>eBPF最终能够接得住整个服务网格吗？似乎不太可能。正如上面所讨论的，在eBPF中实现所需的Layer7处理是否可行还不清楚，即使在某个时候它确实是有可能的。类似的，我们也可以通过一些其他的机制来将Layer7的功能迁移到内核中——尽管从历史上看，这方面并没有很大的推动力，也不清楚什么会真正使其引人注目。（请记住，将功能迁移到内核中意味着将移除我们在用户空间中所依赖的安全屏障。）</p><p></p><p>因此，在可预见的未来，服务网格发展的最佳路线似乎是积极地寻找在性能方面可以依赖eBPF的地方，但要接受在用户空间使用边车代理，并加倍努力让代理尽可能小、快速和简单。</p><p></p><p></p><h2>脚注</h2><p></p><p></p><p>或者至少大大简化了。至少，在程序之间没有预先安排的情况下。这超出了本文讨论的范围。剩下的大部分都是调度。事实上，这在一般情况下是不可能的。如果你想重温计算机科学课程，首先要从悬而未决的问题开始。其中一件事可能比另一件容易。特别是如果你想让你的验证器补丁被上游接受！流量、延迟、错误和饱和。假设还是一个足够轻量级的边车。但有时仍然是两次，所以这有点喜忧参半。例如，有一个有趣的<a href=\"https://twitter.com/mattklein123/status/1522925333053272065?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjkwMTc1MzksImZpbGVHVUlEIjoiUHNJakdOeUJEZmdmZmxFUSIsImlhdCI6MTY2OTAxNzIzOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.8jHOZ5SPfkclJWvVKJOx-ulwkn0LjLcKqggqPG7OSm8\">推特帖子</a>\"介绍了为Envoy这么做有多困难。</p><p></p><p>作者简介：</p><p>Flynn是Buoyant公司的技术布道者，主要关注Linkerd服务网格、Kubernetes和云原生开发。他还是Emissary-Ingress API网关的原始作者和维护者，并在软件工程领域工作了几十年的时间，始终遵循通信和安全的共同主线。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/ebpf-service-mesh/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjkwMTc1MzksImZpbGVHVUlEIjoiUHNJakdOeUJEZmdmZmxFUSIsImlhdCI6MTY2OTAxNzIzOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.8jHOZ5SPfkclJWvVKJOx-ulwkn0LjLcKqggqPG7OSm8\">eBPF and the Service Mesh</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/iZBMUMOvkY3JJdHADgBX\">中国工商银行基于 eBPF 技术的云原生可观测图谱探索与实践</a>\"</p><p><a href=\"https://www.infoq.cn/article/8YXcO6DmQPr6JwOsqKIL\">eBPF&nbsp;技术实践：加速容器网络转发，耗时降低 60%</a>\"</p><p><a href=\"https://www.infoq.cn/article/deJyvo6TQAb09ilLtd6T\">eBPF&nbsp;与 Wasm：探索服务网格数据平面的未来</a>\"</p><p></p>",
    "publish_time": "2022-11-22 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何利用社交网络分析，对舆论事件层层拆解？",
    "url": "https://www.infoq.cn/article/EpqPjURla11EjjmTNlwj",
    "summary": "<p>近年来域外社交平台涌现污名化中国的舆论事件，事件背后的组织与个人对事件的发酵与传播起着推波助澜的作用。本议题以域外抵制2022年北京冬奥会为例，以百分点数据科学基础平台为实现工具，介绍如何利用社交网络分析与机器学习分类算法，分析挖掘参与话题用户的角色作用，并识别异常机器人用户，实现恶意话题中的用户类型精准定位。</p><p></p><p>暗流涌动的污名化舆论中，“看不见的手”究竟是谁？谁在张扬推动？谁在暗中勾连？本次分享将为大家揭秘，社交网络分析如何对舆论事件层层拆解，定位关键。</p><p></p><p></p><p></p><p>四、听众受益</p><p>•&nbsp;了解社交网络分析基本原理</p><p>•&nbsp;了解如何识别社媒用户在社群中的角色作用</p><p>•&nbsp;了解如何使用百分点数据科学基础平台实现社交网络分析与可视化过程</p><p></p><p>五、适合人群</p><p>数据分析师、舆情分析师、数据分析、大数据方向的研发工程师、产品经理以及相关从业人员</p><p></p><p>六、讲师介绍</p><p></p><p>杜晓梦，百分点科技集团数据科学研究院院长，北京大学国家发展研究院Bimba特聘讲师。北京大学光华管理学院，营销建模专业博士；北京大学国家发展研究院硕士；北京大学信息管理系学士，2018年北京市科技新星。2019年获 副研究员职称 ，并带领团队获得北京市科学技术二等奖。</p><p></p><p>组建并带领百分点集团60多人的数据科学团队，2014年带领团队开发出公司核心产品“百分点自动化营销引擎”、”百分点大数据建模平台”。任北京大学新媒体营销研究中心主任助理；北大软微-百分点联合实验室联合主任；中财-百分点金融营销大数据研究中心联合主任。研究兴趣包括营销模型、消费者行为学、互联网与大数据技术、金融大数据营销等，发表英文学术论文3篇，被EI收录，获得2020年英国Emerald出版集团“杰出论文奖”（Emerald Literati Outstanding Paper）。中文学术论文5篇，参与国家自然科学基金重点项目“基于全网数据的消费者行为与偏好研究”，多次参与国际学术会议并发表演讲。</p><p>咨询及实施项目包括华为、国家广电总局、国家质检总局、中国铁路科学院、TCL、王府井百货、华润商业置地、中信集团、宁波银行、爱心保险、上海烟草、云南烟草、广西烟草、汤臣倍健等。</p>",
    "publish_time": "2022-11-22 09:29:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "公有云安全容器设计，腾讯Nodeless弹性容器技术演进和实践",
    "url": "https://www.infoq.cn/article/2JPIXj7aGW8murb40QA7",
    "summary": "<p><a href=\"https://www.infoq.cn/video/vzcywCziz2Ok7CVq4sT2\">Nodeless技术</a>\"可以让企业从节点的运维及管理中脱身，有效避免同节点pod互相影响，甚至是容器逃逸到节点上等情况，让用户集中精力在应用的开发上。文章将分享社区virtual kubelet+kata方案在公有云场景下落地的诸多局限性，以及腾讯在nodeless弹性容器技术上的创新与突破。</p><p>&nbsp;</p><p>本文整理腾讯云高级工程师吴尚儒在<a href=\"https://dive.infoq.cn/2021/beijing/track/1205\">DIVE全球基础软件创新大会 2022</a>\"的演讲分享，主题为“<a href=\"https://dive.infoq.cn/2021/beijing/presentation/4495?utm_source=shishuo&amp;sign=iq_62332d2a74d8a\">公有云安全容器设计：腾讯nodeless弹性容器技术演进和实践</a>\"”。</p><p>&nbsp;</p><p>分享主要分为四个部分展开：第一部分是经典<a href=\"https://www.qingcloud.com/\">Kubernetes集群</a>\"的问题；第二部分会介绍开源virtual kubelet+kata方案的探索；第三部分内容为腾讯云弹性容器服务EKS解决方案；第四部分则是腾讯云弹性容器服务EKS落地实践。</p><p>&nbsp;</p><p>以下是分享实录。</p><p>&nbsp;</p><p></p><h1>经典Kubernetes集群架构</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/8301037a4baeb8037701bc86a0e55725.png\" /></p><p></p><p>这是一个经典的Kubernetes集群的架构，会有一些控制面的组件如API Server，以及调度器Scheduler，控制器Controller Manager，还有存储etcd。除了控制面之外，还会有数据面Worker节点，在这些Worker节点上，会预先安装基础组件比如说kubelet、Container Runtime以及一些Daemon Set，业务将工作负载提交到API Server之后，由Scheduler发现业务Pod，并将Pod调度到合适的Worker节点，之后由kubelet负责将业务Pod启起来。这时候业务的容器会混布在Worker节点上面，不同业务之间可能会在同一个Worker节点。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1eb6bd9bdc29d4cecd7b25218254409.png\" /></p><p></p><p>这里面带来的第一个问题就是，集群里有节点，那就肯定要做一个节点的管理，特别是节点资源的规划问题。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/30/3054bc521e7a53b29f891766a2884bd8.png\" /></p><p></p><p>这是某个服务的CPU的使用量，从这张图我们可以看见这个业务有一个特别的特点，就是它会有个波峰，有个波谷，时不时CPU用量会有激增，也会降为0。但是它又不是简单的重复，比如说今天这个时间点有CPU的突增，明天的这个时间点却没有对应的用量，但是过了几天之后业务量又会有个突增。另外业务峰值也不固定，按照平常情况来说，峰值可能是2000核左右，但是它会在某一个时刻忽然翻倍，达到了5000核这样一个量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d4e05eeda38743b15bf072f5ad53400.png\" /></p><p></p><p>&nbsp;</p><p>所以说这里面就涉及到一个问题：什么时候得进行扩缩容？业务并不是简单重复的，所以要提前在合适的时间点进行扩缩容。另外要规划给业务准备多少节点，按照平时的峰值的准备，如果它忽然有个突增，短时间内达到翻倍，这时候增量部分基本上就资源可以运行了。这就是节点带来的资源管理的问题。另外，节点本身有一个维护的问题，节点会有内核，会有OS，这时候涉及到比如说安全问题需要修复，需要更新内核，需要更新OS。同时节点会有容器运行时，这时候涉及到Bug的修复，版本的管理，还有一些节点本身运行的&nbsp;Kubernetes组件，比如说kubelet组件，这时候Kubernetes&nbsp;集群也需要做升级。只要集群规模上了一定量之后，这种节点内核的升级，运行时的变更以及组件的维护就不可避免，这时候会变得非常麻烦。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/2053f270c354a26895af0506b9286bfb.png\" /></p><p></p><p>除去节点问题之后，还有另外一个就是业务混布带来的隔离性的问题，同一个Worker节点上面可能会混布不同的业务Pod，这些业务Pod都会共享节点的内核，共享节点的硬盘，以及共享节点网卡与网络。那么共享节点会带来什么问题？比如说有些Pod需要开通加载某些内核模块，一些网络的模块，但是加载这个模块就对它自己有用，对其他的业务没有用，而且网络模块可能会带来收发包的拆解，会影响其他业务的网络性能，这是一个对内核的诉求。</p><p>&nbsp;</p><p>同时还有一些内核的参数，比如说Socket Buffer这些配置都需要去改参数的，不同的业务会有不同的需求。另外一个就是存储，这些Pod它们的临时输出或者是标准输出，都会散落到这个节点上面的硬盘上面去，并且是共享同样的存储，这个时候有些业务如果它的输出的速率大于其他的业务，或者是它频繁地输出，这时候整个存储性能就会被它占用。</p><p>&nbsp;</p><p>同时还有网络，有些业务收发包比较快，因为它使用了大量的UDP的小包，这个时候包量达到了整个机器的收发包的限制之后，其他业务网络性能就会受到比较大的影响。除去这些共享的内核、存储、网络之后，还有个容器逃逸的风险，假设我在Volume里面如果有办法逃逸，我就可以跳到父目录，再往上父目录，多跳几层父目录，这时候就可以跳到这个节点上面的根目录rootfs，就可以看见节点上面所有的文件了。</p><p>&nbsp;</p><p></p><h1>开源virtual kubelet+kata方案探索</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85f8f92573a85fad1c5dc874832e174f.png\" /></p><p></p><p>针对前面提到的问题，我们先来看看开源社区有哪些解决方案，首先刚才提到节点管理的问题，开源社区确实有个方案virtual kubelet，它是将一个虚拟节点添加到一个经典的Kubernetes集群里面，这个虚拟节点上面对应着一个大的资源池，所以说落到这个虚拟节点上面Pod对应着后端的资源池就会比较大，这时候就少去了刚才提到的那种节点资源的管理，你可以理解为它后端对接的是一个伸缩弹性更强的资源池。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2346a7ed1831f6947964cf8c5a5d92d.png\" /></p><p></p><p>对于隔离，开源社区有个比较成熟的方案，kata containers安全沙箱方案，原本不同容器之间是会共享内核，共享CPU、内存、网络、存储，这时候在kata方案上，它会先在物理机上构建出安全沙箱，然后每个Pod运行在单独构建出来的安全沙箱里面，每个Pod会有个单独的内核。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff5c65ebad3403379b9264ea4f3f3444.png\" /></p><p></p><p>有了这两个开源方案之后，我们就可以做一个初步的探索，就相当于是我们通过virtual kubelet把所有的节点上面kubelet抽取出来，统一在一起，屏蔽了后端的资源池，搭建了一个虚拟节点。假设有个Pod调度到虚拟节点，我们就通过virtual kubelet去选择合适的物理机，把Pod落到有空闲资源的物理机上，业务本身就少去了一个节点资源的管理，由平台来直接对接所有的物理机池。然后对于隔离性，当Pod落到了物理机上的时候，上面的 kata 启动对应的安全沙箱，这时候也可以达到一个隔离性的目的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4f4ab5da3d34730f131148281d2a99d.png\" /></p><p></p><p>这个virtual kubelet + kata的组合方案，看起来确实可以解决我们提到的问题，但是在实际落地上，却有诸多挑战，特别是在公有云场景上面。首先公有云有一个特点就是多租户，每个租户处在不同的私有网络下面，就类似于这张图，这是一个用户的私有网络VPC，它的API Server就落在VPC里面，然后这个虚拟节点就要去承载上面Pod的创建，这个虚拟机节点位于用户的私有网络内。物理机位于底层的基础网络，containerd&nbsp;跟kata是负责创建安全沙箱的，它们要位于物理机上，所以肯定就是位于基础网络，但它们创建出来的安全沙箱，里面的业务又要位于用户的私有网络，这样才可以达到公有云多租户的场景。第一个问题，就是确保私有网络的隔离性，又能满足创建容器时所需的网络打通，这里会带来一些挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a433f8b6765d62233c93b61c5fd2f947.png\" /></p><p></p><p>除去了因&nbsp;kata&nbsp;落在底层物理机上，带来的这样一个网络打通的挑战，本身 kata&nbsp;方案在公有云上面，也会有一些挑战。首先运行时&nbsp;containerd&nbsp;在物理机基础网络里面，拉容器镜像也就需要在物理机的基础网络上面去实现。但拉镜像会有很多场景，不仅仅是云上的镜像，还可以去拉外部的镜像，或者是用户VPC里面的自建镜像仓库，这时候在物理机的基础网络里拉镜像，怎么样才可以打通外部网络或者是用户的VPC内网络？这是第一个问题。</p><p>&nbsp;</p><p>当拉完容器镜像之后，镜像也是在这个物理机上的。这时候通过这个镜像会构建一个容器的rootfs。然后再由物理机透传或者共享到安全沙箱里面，本质相当于是这个容器的rootfs还是在物理机层面，只是透传到容器里面去的，容器的IO操作还会传导过来的。另外，容器本身运行还会产生一些标准输出，这时候这些标准输出也会落在这个物理机上面，对于日志采集会有挑战，日志采集到了之后，还需要发送到用户部署在私有网络上面的Kafka，这还涉及到一个网络的打通的问题。所以在公有云场景下，整体来说会有一些些弯弯绕绕。</p><p>&nbsp;</p><p>另外就是这个容器的rootfs映射到安全沙箱里面，现在社区比较推荐的方案是virtiofs，它对物理机的内核是有要求的，也就是说它会比较挑物理机，使用面会比较窄。另外这些安全沙箱虽然也说是个虚拟机，但是它并没有办法复用虚拟机热迁技术，因为它的状态，还是跟kata、跟containerd这些跑在物理机上的组件，是有强关联的。安全沙箱里，容器运行过程中需要的一些数据，还是由运行时containerd存储在物理机层面的，并不是跟物理机完完全全独立的。所以说想要做热迁，单纯通过虚拟机的方式来迁移是不够的，还有一些关键的信息要做迁移。整体来说，它对于原本的物理机的运维带有一定的挑战。我们发现这些原因后，总结一点就是kata这种方案还不适合公有云。它可能更适合私有云，但它还不适合公有云多租户场景。</p><p>&nbsp;</p><p></p><h1>腾讯云弹性容器服务EKS解决方案</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10d057ef634cf6d37749bb20d88a8cc4.png\" /></p><p></p><p>我们自研了一套解决方案，想要在多租户场景下面做到完全隔离，必须要把运行时全部搬到虚拟机里面来，相当于是在虚拟机里面有一套完完整整的运行时的组件，在物理机上面不做共享，这时候就可以做到完完全全的隔离，一个Pod一个虚拟机。</p><p>&nbsp;</p><p>首先我们刚才提到的镜像拉取的过程，因为我们已经在虚拟机里有一个containerd，所以说它拉取镜像的时候，就是在用户的私有网络里面进行拉取的，整个流程就跟原本拉取方式没有区别。并且拉取完之后，rootfs落在这台虚拟机里面，虚拟机只要mount到容器里面来，做一个overlayfs，就可以给容器使用了，也不存在从物理机共享到虚拟机里边这种情况，另外日志采集组件也可以部署在用户的私有网络底下。</p><p>&nbsp;</p><p>所以整体来说就解决了刚才提到的问题，并且它已经对物理机没有任何的特殊要求了，就是只要这台物理机能够创建虚拟机就可以了，至于刚才提到的宿主机的组件，物理机的内核都已经没有这个需要了，而且它也没有额外的其他的依赖在物理机上面，可以直接复用原本成熟的虚拟机的热迁技术，整个方案来说就从原本的像虚拟机一样安全，直接就变成虚拟机层面的安全。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc438ae1147b99ce500debe9830bbf02.png\" /></p><p></p><p>我们有了这样一个方向之后，就看下我们的具体设计，首先比较重要的组件就是kubelet，Pod生命周期的管理，所以说我们又想把它做成一个虚拟节点，肯定是需要把kubelet的一些功能能抽取出来，就是我们把kubelet拆成了两个，一个叫EKlet，一个叫EKlet-agent。EKlet用来连接API Server，并且承载所有的Pod的删除以及创建，每个Pod的删除创建我们把它转变成一个创建或者删除轻量虚拟机的流程，也就是说我们一开始的时候没有任何的虚拟机资源，直到你需要一个Pod的时候，这时候你调度到了一个虚拟节点，虚拟节点会帮你快速地去启对应的一个虚拟机出来，在上面再去启对应的Pod。</p><p>&nbsp;</p><p>所以说这个虚拟节点对接的还是刚才提到的物理机资源池，按需创建对应的虚拟机，并且原本Kubernetes 里边通过kubelet，实现了那些logs/exec/cadvisor/port forward请求，会通过API Server传导到EKlet，再通过EKlet传递到EKlet-agent来实现。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c9ae490891232c902d478deb72f28fd.png\" /></p><p></p><p>这个是EKlet组件的设计，主机里面的EKlet-agent它就是相当于你创建完虚拟机之后，要真正启Pod的时候，由它来负责。它类似于kubelet，有一部分功能是由EKlet来实现，大部分功能会由EKlet-agent来实现。Pod的生命周期管理，Pod的健康探测，这些kubelet的功能都在EKlet-agent这边实现，并且它本身也可以直连API&nbsp;Server，负责Pod状态上报，比如Pod启动了，Pod运行终止了，或者是Pod的事件上报是通过EKlet直达API Server，同时Pod有更新，它也可以通过API Server去Watch到这个Pod更新，做对应的变更。</p><p>&nbsp;</p><p>当然，还有Pod启动的时候所需要的configmap、secret，以及所需要的集群网络service的更新就是全部由EKlet-agent来实现，并且监控数据的上报也是由EKlet来实现。另外也有一个比较重要的点，就是原本的话，节点是要维护一个状态的，但是现在我们没有一个固定的节点，只有虚拟节点，每个Pod本身都需要维护自己一个状态，我们是通过EKlet-agent上报心跳来维护这台虚拟机的状态。</p><p>&nbsp;</p><p>整体流程就相当于是你有一个Pod，然后落到了虚拟机上面去，之后虚拟节点就会按需创建出来对应的虚拟机资源，在这个虚拟机资源，通过类似kubelet的操作把这个Pod启起来，并且把这个信息上报给API Server，完成了一整套流程。那么当你在销毁的时候，这边EKlet-agent会先把这个Pod的一个资源给你销毁，并且上报了状态之后，到API Server之后，EKlet看见就会销毁对应的虚拟机。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/08f4cf5507308c605841fa6257afd207.png\" /></p><p></p><p>除去刚才整个Pod生命周期之后，还有个特点就是我们的网络方案比较特别，首先Pod会直接使用虚拟机这张网卡，所以说流量就可以直通Pod，没有宿主机转发的损耗。</p><p>&nbsp;</p><p>但是我们的Pod并没有使用host network，它本身跟EKlet-agent、containerd的网络还是隔离的，用了不同的网络命名空间，这么做的好处是因为要想要避免业务容器变更，比如说iptables对于这些管控面组件的影响。典型的场景如Istio，Istio大家知道它会有个Init Container，它启动的时候就会涉及到iptables，把所有的流量转到Sidecar上面去，之后才开始启动Sidecar以及业务容器，就是业务容器启动完之后，它的流量就会先通过Sidecar再往外发。</p><p>&nbsp;</p><p>但是在假设使用host network的场景，Init Container启动完之后，就会改一个iptables，把所有的流量转发到Sidecar，但这个Sidecar还没启动，并且这个Sidecar启动的前提是先需要去拉镜像，containerd去拉镜像的时候会碰到这个iptables规则，就会把这个流量转发到这个还没启动的Sidecar，Sidecar还没起来，就会报Connection refused，也就说你永远拉不了镜像，但是Init Container又改了iptables，整个流程就相当于是卡住了。我们做到了这样一个隔离，这时候就不用担心业务容器里面做任何的修改会影响管控面组件的流量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7e356aae6be303d6d16412ef057d98b.png\" /></p><p></p><p>另外因为整个虚拟机是一个轻量虚拟机，所以说我们对里面的运行操作系统是有个特别的优化的，首先内核是基于tlinux 5.4内核，同时会优化一些启动速度，比如说磁盘加密，随机数生成的这些模块的加载，同时会内置这些常用的日志采集组件，云上的采集组件，以及业务所需要的GPU驱动，还有各类文件存储的客户端组件，这些组件会按需开启，让业务可以无需去很完整加载这些组件的时候就可以达到开启的效果。另外因为整个虚拟机就只有业务这样一个容器，业务这样一个Pod，特权模式也是可以支持开启的，用户可以通过特权模式去变更内核参数。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/61462df15b445813428fdffe1c8427d0.png\" /></p><p></p><p>整个EKS的架构刚才提到了，它是一个Pod的虚拟机，做到一个虚拟机层面的完全隔离，做到Secure of VM，Pod间就再也没有干扰了，不同Pod间处在不同的虚拟机底下，Pod对特权模式的依赖，或者是内核参数的修改，就完完全全放开了。</p><p>&nbsp;</p><p>另外的话，我们也支持hostpath，虽然这个hostpath跟原生的hostpath会有一点点出入，原生的hostpath是你在同一个节点上面可以通过hostpath通讯，现在是你在同一个虚拟机节点上面通过hostpath是不能通讯，但是我们还是支持hostpath这样一个功能。</p><p>&nbsp;</p><p>除此之外，一个Pod一个虚拟机，所以说你也可以直接获取到真实的CPU了，就不会像Go Runtime或者是Java JDK里面，会根据宿主机的核数取到对应的CPU数。另外这种场景更适合多租户场景了，就像刚才提到的，kata创建了一个虚拟机是会有限制的，因为它本身在宿主机上面还有一些文件，它对物理机也有要求限制，需要物理机的内核版本是多少，还会在物理机上面去共享一些东西，这个时候通过一个Pod一个虚拟机方案，我们更适合那种多组户的场景下的完完全全隔离。</p><p>&nbsp;</p><p>除去安全性，这里还有个弹性的目的，就是只有在Pod创建的时候，我们才对应创建一个虚拟机资源。由于整个虚拟节点对接是云上的资源池，它只有在真的需要创建资源的时候，慢慢才会去创建对应的虚拟机，没有需要去预先准备固定的节点，另外每个Pod起来都是一个单独的虚拟机，计费方式也可以不一样，比如说你有些虚拟机是可以按竞价实例来计费，这些都可以支持的。另外性能跟云服务器一致，因为它本身就是一个虚拟机，并且它的流量是直通到业务容器里面的，是没有额外的转发跟损耗的。</p><p>&nbsp;</p><p>另外的话也是我们刚才提到了，我们就相当于是把 kubelet 的功能分在两个地方，但是并没有说去拆解它，它的兼容性跟原生k8s是一致的，除了一些节点相关的，比如虚拟机节点是没有IP，所以node port这种场景下，就没有节点IP可转发了。除去节点相关这种特性，其他特性都是复用原生kubelet这套逻辑，所以是兼容的，这是整个架构的优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb2e6300339181b1a22cea1cdaf4e138.png\" /></p><p></p><p>当然对比一下经典k8s还是有一些地方不同，比如说单Pod启动速度，因为弹性容器服务的Pod每次启动前都需要创建一台轻量虚拟机，这时候如果是冷启动，什么东西都没有，单纯从头开始创建一台虚拟机，整个启动时速度会比经典的k8s慢十几秒。</p><p>&nbsp;</p><p>我们也有些优化的措施，比如说通过保留沙箱的方式来启动，这时候会有有效地提升启动速度，虽然单Pod的启动速度会慢一些，但好处是在批量Pod启动的时候，比如说刚才提到那种从2000核到5000核这样的一种突增，这样情况下面在EKS上面你是不需要去扩容节点，因为它后面对接的就是整个云上资源池，它立马就可以批量启动这些Pod，如果你使用原生经典的&nbsp;Kubernetes&nbsp;，你就需要先去添加节点，这时候添加节点的流程是分钟级别的，并且要添加多个节点才可以去启动Pod。虽然在单Pod启动速度上面会慢，但是我们有一些优化措施来提升速度。</p><p>&nbsp;</p><p></p><h1>腾讯云弹性容器服务EKS落地实践</h1><p></p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/91/91ede2c7fd341df4a35c3639c7540478.png\" /></p><p></p><p>最后我们看一下落地实践的过程，首先最主要的实践过程就相当于是通过虚拟节点的方式，把弹性容器服务加入到经典的 Kubernetes 集群里面，比如我们就把整个可用区加到这个集群里面，就相当于是这个经典&nbsp;Kubernetes&nbsp;集群里面对接了后面整个可用区的资源池，扩容的时候就可以让业务优先使用固定节点下面的资源，直到固定节点上面没有资源池了，它再弹到虚拟节点上面来。</p><p>&nbsp;</p><p>另外我们也支持，让业务设置它只在固定节点上面保留一定的数量的副本，业务并不一定非得要等到把整个固定节点资源耗尽，它还是希望留些Buffer，但是它本身每个业务就有一些固定的量级，就相当于这些固定的数量的业务就保留在固定的节点上面。扩容的话，剩下的扩容的副本就跑到虚拟节点上面去，缩容的话，优先缩虚拟节点上的，这时候就可以充分利用固定节点上面的资源，这是我们提供的多种扩缩策略。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/4379c0ff3b26daa89ac84aaef9ec7f26.png\" /></p><p></p><p>另外我们也会对原生的一些组件进行一个修改，首先是控制器，因为就像刚才提到，虚拟节点本身上报健康情况，但是虚拟节点状态的不健康其实不应该引发Pod驱逐，因为每个Pod已经是独立出来了，我们是通过一种心跳的方式来维护每个Pod本身的状态。</p><p>&nbsp;</p><p>另外调度器也会做一个感知，原本调度器是感知到每个节点上面的资源，但现在整个调度器对接的是可用区资源池，它要做到可用区资源的感知，并且默认会做一个跨可用区打散的部署，充分去利用不同可用区的资源。另外刚才提到的每个Pod会占一个私有网络的IP，所以说调度器也要去感知IP资源。除此之外创建的话也可以让业务去指定，按照某些机型顺序去创建，比如说有一些可以优先AMD，有一些优先Intel来创建，并且在资源不足的时候我们是有实现了一个重调度的功能，相当于是在某个可用区上面发现资源不足的时候，我们会帮你换个可用区重试。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/250bb22608d1670d4ffbd06683f1911a.png\" /></p><p></p><p>除此之外还有另外一个问题是跟固定节点相比，固定节点上面可以先去拉镜像，但是你在虚拟节点上面每次启动都是一个Pod一个虚拟机，这个虚拟机里面是没有镜像的，启动速度肯定会更慢，对于这种拉镜像的过程，我们有几个解决方案。首先是默认会启用的方案叫做镜像盘复用，假设你这个Deployment有两个副本，Pod A跟Pod B，它们本身就会各启一个镜像盘，之后你在滚动更新的时候，比如Pod B销毁了，这时候它对应的镜像盘我们在后台保留，启动下一个Pod C的时候，我们帮你把镜像盘挂载进来，Pod C启动的时候，就发现这个盘上已经有那个镜像了，可以少去镜像拉取的过程，做到加速启动。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ba/bafbbb1d62038cf94f3394b69755dfeb.png\" /></p><p></p><p>当然这种情况只能是对于有预热的过程，如果说对于冷启动，我们还有另外一个方案就叫做镜像预缓存，就相当于是一开始你推送镜像的时候，我们就会先在云上先给你预加载，做一次拉取镜像操作，拉取完之后，就会对于那个盘做一个快照，之后会生成快照ID，你下次启动的时候就可以拿这些快照ID去创建对应的启动盘，这个启动盘就包含你刚才已经推上来的镜像，整个过程下来，启动的时候就相当于是瞬间完成镜像的拉取，会显著提升启动速度。</p><p>&nbsp;</p><p>而且结合hostpath之后会有一个很特别的效果，就是AI场景下面有些业务会把模型跟业务算法镜像分开，打两个镜像做版本管理，模型是一个镜像，业务是一个镜像，这时候启动的时候，如果模型跟业务算法并没有打在同一个镜像里，业务算法容器要怎么才能读到模型镜像里的数据呢？虽然两个镜像都已经拉到了启动盘上面了，但是overlayfs时互相看不见的，即便用了share volume这种方式，本身还是需要做一个拷贝，才可以将模型数据共享到业务容器。这时候就可以结合hostpath来做，因为模型镜像的模型数据已经拉到了对应的磁盘上面了，只要去找到那个目录，这时候做一个软链给到业务镜像，业务镜像就可以直接去这个目录上面读了，这也是虚拟节点上，需要使用到hostpath的一个场景。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb938026b8009b92e7c7a43816be3bc1.png\" /></p><p></p><p>解决了这个镜像的拉取之后，当然还是有一个问题，相较经典的&nbsp;Kubernetes&nbsp;集群，还需要去创建轻量虚拟机，我们还提出另外一个优化，就是保留沙箱的一个功能，在没有保留沙箱情况下面，你每次创建的时候都要新建一个轻量虚拟机，启动速度肯定是会比固定节点来的慢。通过保留沙箱就相当于是你第一次创建完之后，销毁之后我们帮你把这个沙箱保留，在下一阶段创建一个新的Pod时候，我们就去复用这个沙箱，你就少去了一个轻量虚拟机创建的一个动作，整个Pod启动就可以达到秒级启动了，这个就是我们在启动速度方面做的一些实践。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/abd1f0b909c7756fd78c631c988f6308.png\" /></p><p></p><p>因为弹性容器有个特点，就是即用即销毁，销毁完之后底层资源也就跟着销毁了，有时候需要去排障，需要去看日志，就会找不到，除去日志采集之外，还可以通过原生的terminationMessagePolicy，将退出日志保留在terminationMessagePolicy上面去，除此之外，我们也提供延迟销毁的能力，就是Pod退出之后保留底层资源一段时间，这时候就可以方便排障。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/73cf6e9bbc80f11bfde031bfb1bd1c36.png\" /></p><p></p><p>另外，就是一些Annotation的定制了，比如说固定节点上面，你想要去改DNS服务器，你想要去改内核参数，或者你想要去改运行时的一些配置，这时候你登到固定节点上面去做相应的操作就可以了，虚拟节点上面已经没有节点这个概念了，所以说我们就可以通过Annotation的方式，实现跟固定节点一样的操作过程，相当于是在固定节点上面常见的需要做一些配置，我们通过Annotation暴露给用户。在Pod启动之前，我们就会对Pod所在的宿主机进行相应的初始化，达到相应的效果。另外因为Pod本身就是虚拟机，我们更推荐用安全组的方式来实现network policy，相当于是每个Pod或者是Deployment你可以绑对应的安全组，这时候你就可以达到你想要的network policy方案。</p><p>&nbsp;</p><p></p><h1>总结</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/00/00b76251db48140e60047c092cf745ae.png\" /></p><p></p><p>整体来说，今天从经典的Kubernetes集群开始讲起，经典Kubernetes集群涉及到节点管理的问题，以及节点上面混布带来的隔离性问题，之后探索了开源社区对应的解决方案，发现开源的方案在公有云上多租户场景是有很大挑战的，特别是探索kata方案后，发现它会更加适合私有云场景，还不适合公有云场景。</p><p>&nbsp;</p><p>所以我们就提出了自研的方案，我们的这个方案复用了成熟虚拟化的技术，做到完全网络的隔离、IO隔离、内核隔离，并且我们在实现过程之中，兼容了整个Kubernetes逻辑，虽然会对它做改造，但是只是把功能拆分，并没有做功能删减，所以它会完全兼容除节点相关的&nbsp;Kubernetes&nbsp;特性，兼容特权模式。</p><p>&nbsp;</p><p>当然我们这样的方案，会带来一些跟固定节点不一样的，比如说镜像的预拉取，以及虚拟机的启动这些额外的操作，所以提供一个镜像盘的复用，以及镜像预缓存的这样的功能，去对齐固定节点，并且通过保留实例的方案来提高启动速度。另外本身固定节点上面可以做的操作，我们是通过Annotation暴露给用户，所以说可以达到同样的效果。</p><p>&nbsp;</p><p>后续除了进一步优化启动速度外，我们还会在节点相关的特性上发力，做到Kubernetes节点特性的对齐。</p>",
    "publish_time": "2022-11-22 11:48:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全新JavaScript框架Qwik：以独特的可恢复性方式提速网页应用",
    "url": "https://www.infoq.cn/article/Dh4JdDtegZpG58IdN8L0",
    "summary": "<p>AngularJS的创造者<a href=\"https://www.linkedin.com/in/misko-hevery-3883b1\">Misko Hevery</a>\"近期<a href=\"https://www.builder.io/blog/qwik-and-qwik-city-have-reached-beta\">宣布</a>\"了新网络框架<a href=\"https://qwik.builder.io/\">Qwik</a>\"测试版本的推出，声称无论应用程序有多大，Qwik都能够快速地构建。在多数情况下，Qwik会先下载1KB的JavaScript，在需要的时候才会懒加载或预处理程序和应用程序代码。</p><p></p><p>在一次名为<a href=\"https://www.youtube.com/watch?v=0dC11DMR3fU&amp;t=154s\">《如何从主线程中移除99%的JavaScript》</a>\"的演讲中，Hevery介绍了Qwik背后的原理。</p><p></p><p></p><blockquote>Qwik的目标很简单，确保复杂的网站也能在谷歌页面速度评分项上拿到100/100……归根究底，就是要让<a href=\"https://web.dev/interactive/\">互动时间</a>\"尽可能地缩短。&nbsp;如你所见，行业中的大多数框架都能在优化图片和CSS上做到尽善尽美，但JavaScript方面却又乏善可陈。因为这对于互联网上的每个人来说都是系统性的问题，我的意思是说，问题根源在于工具而不是开发者。&nbsp;用于优化JavaScript交付速度的工具是Qwik关注的问题。</blockquote><p></p><p></p><p>Misko将JavaScript在互动时间指标上负面的表现归因于<a href=\"https://en.wikipedia.org/wiki/Hydration_(web_development)\">水合</a>\"（Hydration）作用。水合在连接服务器的渲染时出现。服务器接收到客户端对页面的请求后，做出对应查询以填充界面，并将结果返回客户端。虽然对用户来说，服务器端的页面渲染速度通常要比客户端渲染的页面要快（如更快的<a href=\"https://web.dev/first-contentful-paint/\">首次内容绘制</a>\"），但页面却并不是立即就可交互的，客户端还需要下载并执行页面上的JavaScript脚本。</p><p>&nbsp;</p><p>在多数框架中，这种首次交付的HTML与应用程序的JavaScript协调的过程称作水合。在水<a href=\"https://en.wikipedia.org/wiki/Hydration_(web_development)\">合</a>\"过程中，Web应用程序框架将事件处理程序和DOM元素相连接，并初始化应用程序状态。水合之后用户操作会被事件处理程序捕捉，从而使页面可交互。</p><p>&nbsp;</p><p>Qwik保留了服务器端的渲染，通过在服务器上运行应用程序以避免水合。它将所有相关状态信息序列化，将页面内容和序列化的状态一起以HTML的形式发送给客户端。这些相关的状态信息包括时间监听器、内部数据结构，以及应用状态。借助序列化的状态，客户端可以<a href=\"https://qwik.builder.io/docs/think-qwik/#resumability--serialization\">接力完成服务器端没有执行完的任务</a>\"。</p><p>&nbsp;</p><p>处理交互性的JavaScript加载默认是延迟进行的，一般是直到用户实际使用交互时才启动，也就是说一个按钮的事件处理程序最晚可以在用户点击按钮时加载。这种即时的JavaScript获取加上<a href=\"https://www.infoq.com/news/2019/09/webexpo-2019-resource-hints-tips/\">预取策略</a>\"，利用浏览器的本地能力，在不影响页面交互性的前提下完成了文件的加载。</p><p>&nbsp;</p><p>在Qwik文档中有<a href=\"https://qwik.builder.io/docs/advanced/prefetching/\">详细</a>\"的介绍：</p><p>&nbsp;</p><p></p><blockquote>Qwik只会预取当前页面需要的代码，避免下载与静态组件相关的代码。最坏的情况是Qwik预取的代码量与现有框架的最佳情况相同，而在大多数情况下，Qwik所预取的代码只会比现有框架要少。&nbsp;除主线程之外的其他线程都可以做到代码预取，大多数浏览器甚至支持主线程之外的代码AST语法预分析。如果用户在预取完成之前开始交互，浏览器会自动优先交互模块于其他预取模块。&nbsp;Qwik可以将应用程序分解成部分，这些分块可以按照用户交互的概率高低顺序进行下载。</blockquote><p></p><p>&nbsp;</p><p>Qwik网站为开发者提供了教程、实例，以及学习和尝试Qwik的在线运行平台。以简单的计数器为例，由一个按钮和显示按钮点击次数的文本框组成，实现方法如下：</p><p></p><p><code lang=\"text\">import { component$, useStore } from '@builder.io/qwik';\n\nexport const App = component$(() =&gt; {\n  const store = useStore({ count: 0 });\n\n  return (\n    </code></p><div><code lang=\"text\">\n      <p>Count: {store.count}</p>\n      <p>\n        <button> store.count++}&gt;Click</button>\n      </p>\n    </code></div><code lang=\"text\">\n  );\n});</code><p></p><p></p><p>开发者可以通过Qwik的component$ API创建可恢复组件，有状态的组件可以通过useStore API显示其对某段状态的依赖。在处理程序的名字后附加$ 字符创建可恢复的事件处理程序（如前文例子中的onclick$ ）。通过这些手动添加的提示，Qwik可以将应用程序文件打包，以实现并优化JavaScript的懒加载。前文的计数器程序在服务器端渲染的HTML如下：</p><p></p><p><code lang=\"text\">\n\n  \n    \n      Tutorial\n    \n    \n      <!--qv q:id=0 q:key=AkbU84a8zes:-->\n      </code></p><div><code lang=\"text\">\n        <p>\n          Count:\n          <!--t=1-->0<!---->\n        </p>\n        <p>\n          <button>\n            Click\n          </button>\n        </p>\n      </code></div><code lang=\"text\">\n      <!--/qv-->\n    \n  \n  \n  \n  \n\n</code><p></p><p></p><p>注意，HTML文件是通过以下方式强化的。</p><p></p><p>q: 属性，如q:base，q:id，q:key 。包含特定框架信息的HTML注释，如<!--qv q:id=0 q:key=AkbU84a8zes:--> 。序列化状态，如 。用于在客户端恢复应用程序执行的Qwik脚本，如，window.qwikevents.push(\"click\") 。</p><p></p><p>Qwik的在线代码运行平台可以让开发者了解到程序代码是如何被切割打包的，还是用前面的计数器为例，客户端的打包方式如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e65549652f12dea638a0c48591b1dc22.png\" /></p><p></p><p>如截图所示，计数器的应用程序被分成了三个脚本。当用户点击按钮时，动态下载并执行其中两个脚本（Qwik运行时间和click事件处理程序的代码）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/abf5056ddac3e46e2a3bdcdf89911926.png\" /></p><p></p><p>参考<a href=\"https://qwik.builder.io/docs/concepts/resumable/\">Qwik文档</a>\"了解具体执行情况以及代码拆分的原理。Qwik的网站给出了大量包括<a href=\"https://qwik.builder.io/tutorial/welcome/overview/\">教程</a>\"、<a href=\"https://qwik.builder.io/examples/introduction/hello-world/\">示例</a>\"，以及演示在内的信息，还有一个可互动的<a href=\"https://qwik.builder.io/playground/\">在线代码运行</a>\"平台。Qwik社区中同样也有一个<a href=\"https://qwik-storefront.vendure.io/\">非常简单的电子商务示例</a>\"，一般对电子商务的厂商来说，页面加载速度提高收入也会增加。</p><p></p><p>Qwik团队目前由AngularJS的创造者Miško Hevery、基于Go语言Web架构Gin的创造者Manu Almeida、<a href=\"https://stenciljs.com/docs/introduction\">Web组件编译器Stencil</a>\"的创造者Adam Bradley组成。</p><p>&nbsp;</p><p>目前，Qwik已推出<a href=\"https://www.builder.io/blog/qwik-and-qwik-city-have-reached-beta\">测试版</a>\"，且采用MIT许可开源，欢迎各位在遵循Qwik<a href=\"https://github.com/BuilderIO/qwik/blob/main/CODE_OF_CONDUCT.md\">行为准则</a>\"的前提下贡献代码。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/10/qwik-fast-web-app-resumability/\">New Qwik JavaScript Framework Seeks Faster Web Apps with Unique Approach: Resumability</a>\"</p>",
    "publish_time": "2022-11-22 11:48:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "万亿数据秒级响应，Apache Doris 在360数科实时数仓中的应用",
    "url": "https://www.infoq.cn/article/Wzg9whHzANPrVhOYJT5B",
    "summary": "<p>作者：360数科中间件团队</p><p>编辑整理：SelectDB</p><p></p><p>作为以人工智能驱动的金融科技平台，360数科携手金融合作伙伴，为尚未享受到普惠金融服务的优质用户提供个性化的互联网消费金融产品，致力于成为连接用户与金融合作伙伴的科技平台。360数科旗下产品主要有 360借条、360小微贷、360分期等，截止目前，已累计帮助 141 家金融机构为 4300 万用户提供授信服务、为2630万用户提供借款服务、单季促成交易金额1106.75亿元。同时作为国内领先的信贷科技服务品牌，360数科在三季度累计注册用户数首次突破 2 亿。</p><p></p><h2>业务需求</h2><p></p><p></p><p>随着金融科技业务的不断发展，对数据的安全性、准确性、实时性提出了更严格的要求，早期 Clickhouse 集群用于分析、标签业务场景，但是存在稳定性较低、运维复杂和表关联查询较慢等问题，除此之外，我们业务中有部分报表数据分散存储在各类 DB 中，这也导致维护管理复杂度较高，亟需做出优化和重构。</p><p></p><h2>系统选型及对比</h2><p></p><p></p><p>基于以上需求及痛点，我们对实时数仓的选型目标提出了明确的需求，我们希望新的 MPP 数据库具有以下几个特点：</p><p></p><p>数据写入性能高，查询秒级兼容标准的 SQL 协议表关联查询性能优秀丰富的数据模型运维复杂度低社区活跃对商业友好，无法律风险</p><p></p><p>2022年3月开始，我们对符合以上特点的数据库 <a href=\"https://xie.infoq.cn/article/5d69f01e8729a6cdcac52ba88\">Apache Doris</a>\" 展开了为期两个月的调研测试。以下是 Apache Doris 1.1.2 在各个方面的满足情况。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/86/bd/86906b6d47cfd17yye3004af43a532bd.jpeg\" /></p><p></p><p>基于上述情况，我们决定采用 Apache Doris，除了可以满足上文提到的几个特点，我们还考虑以下几个方面：</p><p></p><p>Clickhouse 由于 Join 查询限制、函数局限性、数据模型局限性（只插入，不更新）、以及可维护性较差等原因，更适合日志存储以及保留当前存量业务，不满足我们当前的业务需求。目前Apache Doris 社区活跃、技术交流更多，SelectDB 针对社区有专职的技术支持团队，在使用过程中遇到问题均能快速得到响应解决。Apache Doris 风险更小，对商业友好，无法律风险。大数据领域 Apache 基金会项目构成了事实标准，在 360数科内部已有广泛应用，且Apache 开源协议对商业友好、无法律风险，不会有协议上的顾虑。</p><p></p><h2>平台架构</h2><p></p><p></p><p>360数科大数据平台（毓数）提供一站式大数据管理、开发、分析服务，覆盖大数据资产管理、数据开发及任务调度、自助分析及可视化、统一指标管理等多个数据生命周期流程。在整个 OLAP 中，目前 Apache Doris 主要运用离线数仓分析加速、自助 BI 报表等业务场景。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b1/b19ba7213bef5a5dd24d44600e17b4a2.png\" /></p><p></p><p>在引入 Doris 后，考虑已有数据分析业务以及数据规模，Doris 集群将先同步部分业务上优先级更高的数据。通过上述架构图可以看到，依托 Doris 强大的查询性能，我们将把 Doris 架设在 Hive 数仓的上层，为特定场景进行查询加速，这样的架构建设起来成本很低，只需要完成数据从 Hive 数仓到 Doris 集群的导入适配，因为 Doris 集群并没有产生任何新表，可以直接复用已经建设好的数据血缘关系。</p><p></p><p>数据导入方案，我们在调研了 Stream Load 和 Broker Load 之后，从导入性能、开发成本上进行了评估，在导入性能上，Broker Load 要比 Stream Load 略胜一筹，而在开发成本上两种方式并没有明显的差异。而且对于大表的同步，Broker Load 的导入方式可以做到单表一次导入一个事务，而 Stream Load 在单表数据量超 10G 时则需要拆分后进行数据导入。因此数据导入选择使用 Broker Load 来进行。</p><p></p><p>数仓即席查询方案，我们自行开发的查询引擎支持多查询引擎动态切换的机制，通过识别查询数据的元信息对本次查询做自动的查询引擎（Doris/Presto/Spark/Hive）路由和故障切换。</p><p></p><p>Doris 支持原生 MySql 协议，对标准 SQL 支持良好，使得 Doris 可以和一些 BI 工具（帆软、观远等）无缝结合，因此单独搭建了一个 Doris 报表分析集群作为 BI 工具数据源。</p><p></p><h2>应用实践</h2><p></p><p></p><h4>Doris 对 Hive数仓的查询加速方案</h4><p></p><p></p><p>在即席查询场景中，传统的查询引擎（Hive/Spark/Presto）越来越满足不了数据开发者、数据分析师对查询响应性能提出的高要求，动辄几十秒甚者分钟级的查询耗时极大的限制了相关场景的开发效率。</p><p></p><p>为提高查询性能，我们通过架设的 Doris 数仓加速层来缩短查询耗时，目前我们在不开启 Doris 缓存、不开启用物化视图等优化策略的情况下，命中 Doris 即席查询平均耗时即可从几分钟缩短至 5 秒内。</p><p></p><p>未来我们将通过分析相关查询的特征，通过开启缓存、创建相关物化视图等策略来进一步优化 Doris 的查询性能。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/7a/7a18f0639f91c9743e8f4cde61b077bb.png\" /></p><p></p><p>实现 Doris 加速的核心是支持查询引擎动态切换，查询引擎动态切换的工作机制如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/12/123d0dfc7d040d331875bd92d01af9fc.png\" /></p><p></p><p>查询引擎会及时收集 Hive 和 Doris 的元信息，包括库、表、表字段、表行数等信息，在用户提交即席查询请求时，首先会解析出用户查询的表，并按照如下顺序判断：</p><p></p><p>查询的表是否已在 Doris 同步Doris 表和 Hive 表结构是否相同Doris 表和 Hive 表表行数是否一致</p><p></p><p>如果以上要求均被满足，则会将该查询路由到 Doris，否则会依次按照 Presto、Spark、Hive 的顺序进行路由查询，当查询出现异常时，也会按照该顺序依次进行故障转移。</p><p></p><h4>慢查询慢导入分析</h4><p></p><p></p><p>对于慢查询和慢导入，Doris 提供了完善的 Profile 机制，在了解相关技术细节后，我们在线上集群开启了 Profile 收集，通过调度任务定时收集慢查询、慢导入的 Profile 信息并落库。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ad/adb1d78c66cd2871738d7eb66990f95c.png\" /></p><p></p><p>Doris 提供的 Profile 信息非常详细，例如 OLAP_SCAN_NODE 提供了原始的扫描行数，各个索引的过滤行数，每个 Instance 的 EXCHANGE_NODE 提供了接收的数据总行数和接收的数据量大小。这些信息为查询调优提供了详细的依据，我们在使用过程中针对快速定位查询性能的瓶颈进行了优化，取得了良好的效果。</p><p></p><h4>建表规范</h4><p></p><p></p><p>在我们的使用场景中，有下列类型的表：</p><p></p><p>pda表：每日全量更新，即每日分区存储全量快照数据pdi表： 每日增量更新，即每日分区存储增量数据a表：全量不分区表s表：静态非每日更新数据</p><p></p><p>由于当前 Doris 集群中所有的表都是基于 Hive 数仓中各层级的表同步而来，因此目前仅使用了 Duplcate 模型和 Unique 模型，对于 pda、pdi 和 a 表，为了降低 Doris 表的分区数，减轻 FE 元数据管理压力，我们在建 Doris 表时均启用了根据日期划分的动态分区特性，较久远的历史数据我们按年、月的维度分区归档，近期的数据按日、小时分区，未来我们计划通过程序自动识别完成历史分区的归档合并。</p><p></p><p>对于 pda 表使用场景，pda 表需要每日同步全量数据，我们采用了 Duplicate 模型，不考虑使用 Unique 模型数据去重的原因是 Doris 的导入模型本身就提供了基于任务 Label 的数据一致性保证，同步时一次调度周期的 pda 表的一个分区的导入任务能产生唯一且不变的 Label，因此我们可以保证即使错误执行了多次，该分区的数据仍然不会重复。另外，因为 Duplicate 模型相比于 Unique 模型，在导入和查询阶段均不会做预聚合去重，所以可以一定程度上加速导入和查询的性能。</p><p></p><p>对于 pdi 表使用场景，因在实际使用中 pdi 表存在少数对历史数据的部分更新场景（绝大部分是数据更新场景，基本没有数据删除场景），考虑到 Doris 数据表的分区可用性，我们采用了 Unique 模型，这样在更新历史分区的数据时不必做重建分区操作。</p><p></p><p>对于 a 表使用场景，因业务上可以接受短时间数据不可用情况，我们启用了动态分区，在做数据导入时，每次导入都会先删除历史分区，然后将全量数据导入今天的分区内，这样做的考虑是杜绝重建表操作，且实施成本相对比较低，因此我们没有采取动态更新视图绑定当日分区的方案。</p><p></p><p>在 Doris 之前的版本中，尚未实现 Hive 元数据变更同步和管理功能，为了提高效率开发了 Doris 建表工具，我们通过选择和配置数仓集群、Hive 表名、数据模型、Bucket 数量等参数，自动关联 Hive 表，解析表字段并生成对应的建表语句。经过与社区沟通得知，最近即将发布的 1.2 新版本中已经实现 Multi Catalog，支持 Hive 元数据的对接和 Schema 的自动同步，可以极大程度上减少这一部分的工作。</p><p></p><h4>监控体系</h4><p></p><p></p><p>当前 Doris 集群监控体系分为主机指标监控告警、日志告警和集群指标监控告警，总体监控体系如下。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/55/55e93f9395249a311893e56f85d0b971.png\" /></p><p></p><p>主机指标监控是基于 Open-Falcon 开发的监控告警平台，主要采集 Doris 集群节点的 CPU、IO、内存、磁盘等相关指标并进行监控告警。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/90/9033e0136140a8a873bd644c43708e6f.png\" /></p><p></p><p>集群指标监控参考了 Doris 官方文档提供的基于 <a href=\"https://prometheus.io/\">Prometheus</a>\" 和 <a href=\"https://grafana.com/\">Grafana</a>\" 和集群指标监控方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d1/d13df81da0dd2ee6db01997b0ce7f867.png\" /></p><p></p><p>日志告警仍然是基于我们的监控告警平台，主要用于监控 Doris 服务日志中容易识别但其他监控方式成本较高的监控、告警场景，是其他两种监控的补充。通过日志监控告警，我们能够准确识别数据导入任务的失败原因并能进行及时的推送通知。</p><p></p><h4>问题排查和审计日志</h4><p></p><p></p><p>为了及时排查一些极端的集群问题，上述针对 Doris 的监控体系建设仍然是不够的。为了在集群 BE 出现异常宕机时快速定位堆栈，需要在所有的 BE 节点开启 Core Dump。除此之外，审计日志在集群的日常运维中也发挥了重要作用。</p><p></p><p>对于 Doris 集群的审计日志收集一般可以通过 2 种方式：</p><p></p><p>第一种方式是通过日志收集组件、收集各个 FE 节点上的 fe.audit.log第二种方式是通过安装 Doris 提供的 Auditloader 插件（下载 Doris 源码，该插件在 doris/fe_plugins/auditloader，具体使用文档可参考官方文档：<a href=\"https://doris.apache.org/zh-CN/docs/ecosystem/audit-plugin\">审计日志插件</a>\"）。</p><p></p><p>考虑到第二种方式操作更简单，因此采用此方式进行日志采集。不过在使用 Auditloader 插件的过程中，陆续发现和修复了一些插件问题，并向社区提交了 PR，与此同时，我们定制开发了内部控制台，便于查看集群的同步任务情况，数据分布情况以及进行审计日志的检索。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b4/b484084ff01a72204f591043097f1e81.png\" /></p><p></p><p>审计日志为集群 BE 崩溃时具体 SQL 定位、客户端访问统计、查询 SQL 耗时统计、访问 SQL 特征分析等提供了详细的信息。例如，数据开发曾经反馈查询 Doris SQL 失败，检索日志出现了大量连接数超限的异常，我们通过审计日志，迅速定位到了问题原因是由于上游导入工作流 Bug 在短时间内创建较多的数据库连接。另外，对于曾经使用的低版本 Doris 出现数次 BE 异常宕机问题，我们通过 gdb 调试工具定位到崩溃时 SQL 的 query_id 后，配合审计日志也能快速的定位到导致崩溃的具体 SQL。</p><p></p><h2>优化实践</h2><p></p><p></p><h4>数据导入实践和调优</h4><p></p><p></p><p>初期数据源主要来自 Hive 数仓，因此大部分数据导入以 Broker Load 方式为主。大数据平台自助导入任务工作流适配了 Doris Broker Load 导入方式，数据开发零代码——通过简单的勾选配置即可完成自助的 Doris 数据导入工作流创建。</p><p></p><p>而在 Broker Load 的使用过程中，我们也陆续遇到了一些问题，这里拿出几个典型的问题和一些调优经验来分享。</p><p></p><p>在 Broker Load 导入时遇到的问题：</p><p></p><p>因表分桶数设置过少造成 Broker Load 导入失败，具体表现为导入任务失败且异常信息为：</p><p></p><p><code lang=\"text\">tablet writer write failed, tablet_id=xxx, txn_id=xxx, err=-238</code></p><p></p><p>我们推测造成 -238 错误的原因可能是分桶设置太少，接着我们通过 BE 节点的挂载数据来查看单个 Tablet 下的文件大小，我们发现单个 Tablet 的文件占用空间远大于官方推荐的 10GB 上限范围，这也证明了我们的推测正确，因此我们通过适当提高 Doris 表的分桶数，使得这个问题有了较大的缓解。</p><p></p><p>顺便说一下，如果出现 -235（旧版本是-215）异常，一般是由于 Compaction 过慢导致 Tablet 版本堆积超过限制，这个时候通过 Grafana 看到 BE Compaction Score 在导入前后有明显的波动，而且绝对值很高。如果遇到此问题可以参阅 ApacheDoris 公众号文章：<a href=\"https://mp.weixin.qq.com/s/cZmXEsNPeRMLHp379kc2aA\">Doris 最佳实践-Compaction调优(3) 对Compaction过程进行调优。</a>\"</p><p></p><p>因 Hive 表字段变更导致 Broker Load 导入失败：</p><p></p><p>Hive 表在使用过程中会有一些 DDL 的执行，从而导致表字段新增，我们数仓的 Hive 表均使用 ORC 格式存储，那么就会导致 Hive 表中部分历史分区的 ORC 文件中字段信息缺失（缺失新增字段），而新分区的 ORC 文件中字段是正常的，这个时候如果对历史数据重新导入，就会有下面的异常信息：</p><p></p><p><code lang=\"text\">detailMessage: ParseError : Invalid column selected xxx</code></p><p></p><p>在阅读了 Broker Load 相关代码后确认了问题原因：在一次 Broker Load 导入过程中，导入任务的字段解析器会读取一个 ORC 文件头解析字段信息，但解析器只会解析一次，如果一次导入过程中同时有新、历史分区的 ORC 文件，那么就可能导致任务失败。</p><p></p><p>修复的方法也很简单，只需针对每个 ORC 文件重新解析一次文件头的字段信息即可。在了解问题原因及分析解决思路后，我们也和社区的同学一起修复了这个问题并提交了相关 PR。</p><p></p><p>遇到空 ORC 文件时 Broker Load 导入失败：</p><p></p><p>这个问题的错误表现和问题 2 比较类似，具体原因是 Broker Load 导入过程没有对 ORC 文件做判空，遇到空 ORC 文件仍会尝试解析 ORC 文件字段信息导致报错，我们把这个问题反馈给社区后，社区的同学很快修复了该问题。</p><p></p><p>Broker Load 导入任务出现 Broker list path exception. path=hdfs:xxx</p><p></p><p>创建 Broker Load 任务，使用 Kerberos 认证访问 HDFS 的 Hive 文件导入数据，Hive 文件路径中分区和下一级目录使用通配符 *，访问所有分区所有文件，任务提交后隔 40 多秒出现如下的错误：</p><p></p><p><code lang=\"text\">type:ETL_RUN_FAIL; msg:errCode = 2, detailMessage = Broker list path exception. path=hdfs:xxx\n</code></p><p></p><p>在阅读了 Broker Load 的访问 HDFS 相关代码后确认了问题原因，Broker Load 调用 HDFS 的 LS、DU 方法时会获取文件目录信息，由于路径下的文件过多导致耗时会超过 45 秒，而 Thrift 设置的 Socket 请求超时默认小于 40 秒，所以出现了上述的 RPC 异常，问题反馈社区后，对 FE 增加了配置参数broker_timeout_ms，设置为 90 秒后解决问题。</p><p></p><p>关于 Broker Load 的导入性能调优策略</p><p></p><p>我们针对 Broker Load 导入调优的主要方向在确保 Doris 集群不承压的情况下尽可能提高导入并发度，下面根据 2 个典型的案例来说明：</p><p></p><p>部分 pdi/pda 表因为数据规模太大导致全量导入耗时过长 （导入数据源是 Hive分区表）</p><p></p><p>部分 pdi/pda 表数据规模在 T 级别，在进行全量导入时，如果只提交一个 Broker Load Job ，将因为导入任务的并发不够，导致导入耗时达到 5-6 小时。针对此问题，我们可以对导入任务进行 Job 拆分，在大数据平台也适配这种场景，支持任务的自动拆分和重试机制，具体的拆分方式如下图：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/da/da87af81d5e89ca5641197cb003e1a58.png\" /></p><p></p><p>不过要注意的是，拆分后可能会对集群有较高的写入压力，要及时监控导入任务和集群的状态，特别针对 -235 的情况可能需要进行 Compaction 调优。</p><p></p><p>部分 ads 表因为数据规模太大导致全量导入耗时过长 （导入数据源是Hive非分区表）</p><p></p><p>数据开发对部分报表的同步时效提出了很高的要求，我们在针对性的优化表同步时效时，发现一些表导入耗时较长，但通过集群监控体系发现相关表同步期间，BE、FE 节点的 CPU、内存、磁盘 IO 、网卡 IO 并没有达到瓶颈，集群的 Compaction Score 在此期间也一直稳定在低位，且整个同步过程同步任务均未出现-235、-238等相关的错误，我们推测瓶颈可能还是在导入任务的并发程度上。</p><p></p><p>因为有些表在 Hive 数仓是非分区的表，所以第 1 种通过划分分区范围拆分多个导入 Job 的方式就行不通了，理论上仍然可以通过划分不同的 HDFS 文件来拆分 Job，但是这种方式在毓数大数据平台还需要进一步去适配，所以我们还是优先考虑通过调整集群配置的方式彻底解决此问题：</p><p></p><p>首先可以通过适当调高 FE 的max_broker_concurrency去提高 Scan HDFS 文件阶段的并发度（最高调高至 BE 节点数），而对于 Table Sink 阶段，可通过调高 FE 的default_load_parallelism（设置fe.conf，可调整到 BE 节点数）和 send_batch_parallelism参数（ SQL Session 执行set global send_batch_parallelism=5或在提交 Broker Load 中的 PROPERTIES 中指定，最高调整到 5，如果超过此值，需要同步调整 be.conf 的 max_send_batch_parallelism_per_job 参数），提高该阶段并发度。通过提高 Broker Load Job 各阶段导入的并发度，相关报表的同步时效显著提升，这里我们选取 5 张典型表为例，优化前后的同步时效表现如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/24816975ebcd0f58f44b70b4c424d641.png\" /></p><p></p><h4>双机房容灾建设</h4><p></p><p></p><p>为了保障 Doris 集群的可用性，我们需要为 Doris 集群提供双机房容灾能力。Doris 目前虽然可以通过不同的 Tag 将 BE 分组部署在多个机房，但是无法解决机房出现问题时的 FE 可用性问题。经过方案调研分析，我们决定通过自行开发 Replicator 主从同步插件去实施双机房容灾建设，具体的架构如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/36/3642d3128796c117d4dd1a114adeed38.png\" /></p><p></p><p>通过在主集群安装 Replicator 插件，Replicator 插件会拦截并解析主集群执行的全量 SQL，然后经过过滤操作，筛选涉及库、表结构变更和数据增、删、改相关的 SQL，并将相关 SQL（部分 SQL 需要改写）发送到备集群进行重放。除此之外，我们在 Doris 控制台开发了 Validator 数据校验程序，定期校验主备集群间的数据结构差异和数据差异并上报，在主集群因各种问题导致不可用时，直接通过切换 DNS 解析地址到备集群 LVS 地址完成主备集群的切换。</p><p></p><h2>总结规划</h2><p></p><p></p><h4>效果总结</h4><p></p><p></p><p>从 2022 年3月份开始进行对实时数仓沟通进行调研，7月份正式上线生产，集群数据规模快速增长。目前，生产环境共有 2 个集群，数百张表，几十 TB 数据，每日有数百个同步工作流在运行，几十亿规模的数据新增/更新。在此规模下，Doris 对业务支持良好，稳定运行。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c72e5990880f14e82f7b7ddf1e81ac3a.png\" /></p><p></p><p>Doris 集群架构清晰简单，不依赖其他组件，数据模型简单，数据导入方式多样化且适配成本很低，使得我们可以快速完成前期的调研测试并在短时间内上线实施。Doris 集群作为目前公司 BI 工具的重要数据源，承载了相当一部分的报表分析业务，极大加速了报表分析的时效性。Doris 上线 3+月的时间，已经承载了小部分即席查询场景，大大缩短了相关查询的耗时。Doris 具有完善的监控机制和审计机制，极大的降低了我们的运维工作Doris 社区十分活跃，在我们使用 Doris 过程中遇到的一些疑难问题，官方也可以及时进行响应、处理。</p><p></p><h4>未来规划</h4><p></p><p></p><p>在近期的规划中，我们希望 Doris 能支撑更多的业务场景、发挥更大价值，例如基于 Doris 建立实时数仓、基于 Doris 重构用户行为画像、Doris HIVE 外表特性等。同时我们计划通过分析用户的查询 SQL 特征，结合 Doris 的查询缓存和物化视图特性，进一步提升查询效率。通过开发集群探查工具，实时探测集群数据表的数据分布情况，比如 Tablet 有没有过大，Tablet 数据分布是否均匀等，综合探查集群的运行情况并自动给出优化建议。</p><p></p><p>目前我们使用了 Doris 有大半年时间，在这半年期间一直保持和社区同学进行交流（提交 Issues 和 PRs），非常感谢 SelectDB 团队一直以来对我们的技术支持。最后祝 Apache Doris 越来越好，为基础软件建设添砖加瓦。</p>",
    "publish_time": "2022-11-22 15:06:10",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Kubernetes 环境下的数据库新命题",
    "url": "https://www.infoq.cn/article/YmtIViY6rEgkwSjfeA6i",
    "summary": "<p>伴随着互联网应用场景逐渐深入到生活的各个角落，为了确保前端用户的使用体验，对互联网产品的后端架构性能提出了更高的需求。如今，开发以及运维人员正在将工作重心和优化重点放在了后端基础设施的可用性、一致性、扩展性、弹性以及全面自动化管理等能够提升效率的技术能力层面。</p><p></p><h1>背景：Kubernetes 环境中的微服务与数据库</h1><p></p><p></p><p>应用部署的变化</p><p></p><p>一方面，在处处充斥着大数据以及高并发场景的今天，后台技术人员往往会花费更多精力在解决『大规模业务数据的存储与应用』等问题上，以确保数据库等基础设施能够实时提供最佳服务；另一方面，在云原生『大行其道』的今天，技术人员也需要根据业务发展，构思并设计能够满足未来业务诉求的云上服务架构。</p><p></p><p>而这种颠覆性的云原生技术，正是我们今天谈论的主要对象，它改变了从开发、交付、部署到维护的全部技术流程。如今，市场上的应用与业务已经普遍接受由各家云厂商所提出的『<a href=\"https://www.infoq.cn/article/riNy5Aeu3vQpD1635Sfu\">XaaS</a>\"』概念。与之伴随而来的，则是交付模式与架构形态的新变化。</p><p></p><p>架构形态的变化</p><p></p><p>在部署形态发生转变的同时，架构模式也从单一的整体式架构逐步向微服务架构演进。随着企业业务的持续扩张，前端的业务线也在持续发生着变化，这就要求后端系统能够快速响应前台每一条业务的单独诉求，而这在过去传统的架构模式中几乎是不可能实现的。</p><p></p><p>微服务通过将一项单体式服务切分为多个更小的单元，围绕业务领域组件来创建应用，使这些应用可独立地进行开发、管理和迭代。与此同时，在分散的组件中使用云架构和平台式部署、管理及服务功能，使产品交付变得更加简单。而随着 Service Mesh 的兴起，服务间的解耦以及 API 调用等操作也被更加简化，以适应业务微服务的部署、交付以及业务诉求。</p><p></p><p>而这些之所以能够兴起，正是 Kubernetes 在架构模式演进的过程中所提供的抽象平台与机制，这也是 Kubernetes 在全球流行的重要原因之一。</p><p></p><p>借用 <a href=\"https://kubernetes.io/zh-cn/docs/concepts/overview/\">Kubernetes 官方文档</a>\" 中的一段话，更能够体现 Kubernetes 的价值：</p><p></p><p></p><blockquote>“Kubernetes 为你提供了一个可弹性运行分布式系统的框架。Kubernetes 会满足你对于扩展、故障转移、部署模式等多方面的要求。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。”（摘自“为什么需要 Kubernetes，它能做什么？”部分）</blockquote><p></p><p></p><p>Kubernetes 是管理微服务生命周期的理想化平台，那么作为有状态的服务，数据库在这种大环境下应具备哪些新特性呢？</p><p></p><p>数据库层面的变化</p><p></p><p>针对上述问题，应用层采用了微服务架构来作为解决方案。但数据库的情况却有些不同。在数据库层面，面对『蜂拥而至』数据体量，常见的解决方案是对数据库层进行数据分片，将数据库改造为分布式架构。</p><p></p><p>当前，不论是 MongoDB、Cassandra、Hbase、DynamoDB 等非关系型数据库还是 Cockroach、Google Spanner、Aurora 等新型数据库，分布式架构无处不在。分布式数据库需要将单体数据库分片为更小的单元，从而实现高性能和弹性伸缩。这也就是为什么 CockroachDB 兼容 PostgreSQL 协议；Vitess 为 MySQL 提供了分片特性；AWS 开发了 Aurora-MySQL 和 Aurora-PostgreSQL。</p><p></p><p>而对于企业用户数据库选型来讲，新型的数据库解决方案，不仅要解决分布式、单元化的问题，更要考虑『如何让客户从现有的 Oracle、MySQL、PostgreSQL、SQLServer 等数据库迁移至新数据库产品中』的问题，其原因在于：传统关系型数据库仍然占据着庞大的市场份额，数据库替换的痛点是企业用户不得不面临的重要课题。</p><p></p><h1>需求：在 Kubernetes 上实现数据库的云中立</h1><p></p><p></p><p>云的兴起意味着数据库面临着新的挑战。云平台遵循“随机应变”、“一切皆服务”、“开箱即用”的原则，正逐步改变用户对于传统研发、交付、部署、维护等技术流程。</p><p></p><p>以应用程序开发人员为例，在云原生环境下，开发人员通常会在云上或 Kubernetes 上交付应用。这是否意味着数据库也应该在云端或 Kubernetes 上部署？大多数用户应该都会这样认为，这也是为什么数据库即服务（<a href=\"https://xie.infoq.cn/article/fa5af60ccf74ef156aa8830fa\">DBaaS</a>\"）的概念越来越受欢迎。</p><p></p><p>然而，如果你是一家准备上云的企业，首先要确认的一定是哪家云厂商能够为业务提供长期的技术支持。但事实上，任何人都心知肚明这是很难实现的。因此为了将风险分摊开，在企业视角中，多云就成为了市面上最广泛的选择，而将应用及数据库部署在&nbsp;Kubernetes 上完成与云厂商的解耦和依赖，正是解决这类问题的可选项之一。</p><p></p><p>这是因为 Kubernetes 本质上是容器编排的抽象层，具有高度的可配置性和可延展性，用户甚至可以在特定场景下自定义编码。例如，Kubernetes 上的挂载服务是由众多云供应商实现和提供。如果服务部署在 Kubernetes 上，应用就能够与 Kubernetes 实现交互，无需与不同类型的特定云服务或基础设施产生额外的交互。实践证明这种理念在无状态应用程序或微服务的领域非常适用。正是由于这些成功案例，人们开始思考如何将数据库部署在 Kubernetes 上，进而实现云中立。</p><p></p><p>不过这一方案的缺点在于，Kubernetes 天生适用于无状态应用，而非数据库及其它有状态应用，因此管理 Kubernetes&nbsp;上的数据库或有状态的应用比管理无状态的应用层的难度更大。为了解决这一问题，可以基于 Kubernetes 的基础机制，使用 StatefulSet 和 Persistent Volume 等方法来实现桥接和适配。MongoDB、cockachdb、PostgreSQL 等数据库的 operator 就是采用了这种方法。</p><p></p><h1>命题一：如何将单体数据库转换为更接近云原生的分布式数据库？</h1><p></p><p></p><p>上述方案已经很常见，那还有其他的方案吗？答案是肯定的。本节将会演示另一种方法，将完成从『MySQL、Oracle、PostgreSQL』等单体数据库到分布式数据库系统的升级改造，并且以一种更加接近云原生的管理方式，来实现 Kubernetes 上的分布式数据库系统的部署、管理及使用。</p><p></p><p>顾名思义，这个方案主要分为两个步骤：首先将单体数据库转换为分布式数据库，其次将该分布式成功部署在 Kubernetes 环境中并实现有效管理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/35/352fd2192cd19083b79b2d6fa8f1e65b.png\" /></p><p></p><p>其核心理念在于利用数据库最本真的两大基础能力，即计算能力和存储能力，如上图所示。任何一款数据库必然具备这样的核心组件、核心能力；传统的&nbsp;MySQL、PostgreSQL 和其它单节点数据库只是刚好将两个组件部署在一台服务器或容器上而已，而所谓的分布式数据库架构，即是采用存算分离的分布式架构，将存储节点、计算节点进行分布式化，部署在不同服务器节点上，进行内部组件的通讯，完成对上层业务的响应。</p><p></p><h3>首先，使用数据分片能力打造分布式数据库</h3><p></p><p></p><p>作为一款分布式的数据库生态系统，ShardingSphere 提供两个客户端：ShardingSphere-Proxy 和 ShardingSphere-JDBC。用户可选择使用&nbsp;ShardingSphere-Proxy 充当 “分布式”&nbsp;MySQL 或 PostgreSQL 的服务器端；或者使用 ShardingSphere-JDBC 作为 Java 应用的 Driver 端来完成同样的功能实现。无论采用哪种方式，用户都无需变更原来访问数据库的方式，这极大地降低了用户的学习曲线和迁移成本。</p><p></p><p>如果将单体数据库看做分片节点（即存储节点），将 ShardingSphere-Proxy 或 ShardingSphere-JDBC 看做全局的 Server 入口或代理端（即计算节点），他们的组合就是一种分布式数据库系统。如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9f/9f0ddea653891a506878365aee176207.png\" /></p><p></p><p>此外，ShardingSphere 内置有 DistSQL&nbsp;（分布式 SQL），用于管理分片数据库、动态控制分布式数据库系统的工作负载，如 SQL 审计、读写分离、权限等。</p><p></p><p>例如，你可以使用 CREATE TABLE t_order () SQL 在 MySQL 中创建一个新表。在 ShardingSphere-Proxy 中使用 CREATE SHARDING TABLE RULE t-order () 就可以在新升级的分布式数据库系统中创建一个全局分片表。</p><p></p><h3>其次，在 Kubernetes 层面部署分布式数据库</h3><p></p><p></p><p>目前为止，我们已经解决了数据分布式问题，但我们如何将其部署在 Kubernetes 上呢？Shardingsphere-on-cloud 提供了 ShardingSphere-Operator-Chart 和 ShardingSphere-Chart，帮助用户在 Kubernetes 上部署 ShardingSphere-Proxy 和『云上 DBA』ShardingSphere-Operator 集群。</p><p></p><p>『ShardingSphere-Chart』这个 Charts 能够帮助用户使用 helm 命令在 Kubernetes 环境中部署 ShardingSphere-Proxy 集群，包括&nbsp;Proxy 计算节点本身、治理中心、数据库连接驱动程序和 ShardingSphere-Operator。</p><p></p><p>作为『云上 DBA』，ShardingSphere-Operator 通过利用预定义的 CustomResourceDefinition，可以声明 ShardingSphere-Proxy 在 Kubernetes 上的部署结构，并持续监控运行状态，还可以基于 CPU 指标进行 Kubernetes 上的&nbsp;HPA （横向自动扩容），并能够确保 ShardingSphere-Proxy 的高可用性，以维持所需计算服务节点的副本数量，提升整个分布式数据库系统的高可用性。</p><p></p><h1>命题二：如何让分布式数据库系统变得无状态，真正实现分布式+云原生？</h1><p></p><p></p><p>如上所述，数据库系统一般由计算节点和存储节点组成。当用户使用现有的生产环境的数据库作为新分布式数据库系统的存储节点时，ShardingSphere 则可以作为全局的计算节点，提供分布式数据库计算服务，即经典的计算存储分离的架构。这种架构在 Cloud native 场景下， 特别是 Kubernates 上有了重新的解读和应用。将计算节点作为无状态的应用部署在 Kubernates 上，并利用 Kubernetes 天然的平台能力来解决传统单点数据库分布式改造的课题。而存储节点可以部署在任意位置，可以是 Kubernetes 集群内、云的 RDS、私有环境等，真正实现分布式数据库存算架构的解耦和云化问题。</p><p></p><p>用户可以使用 ShardingSphere-Chart 以及 ShardingSphere-Operator-Chart 这两款工具轻松地部署和管理 ShardingSphere 集群，并在 Kubernetes 上创建自己的分布式数据库系统，无需关注单体数据库的位置，利用 Kubernetes 管理无状态计算节点，用户所创建的数据库可在任何公有云或私有云上，通过让 Kubernetes 上的 ShardingSphere (全局计算节点) 访问任意位置的单机数据库，从而对最终用户提供透明化的、完整的分布式数据库的解决方案，大幅降低升级改造成本，并提升整体的性能和存储能力。</p><p></p><p>在这类体系中 ShardingSphere-Proxy 将作为全局计算节点处理用户请求，从分片存储节点中获取本地结果集并进行计算。这意味着无需对生产环境的现有数据库集群进行危险操作，只需要将 ShardingSphere 导入到现有的数据库基础设施层，就可以将单机数据库存储节点与 ShardingSphere 全局计算服务器连接起来，形成一套完整的分布式数据库系统解决方案。</p><p></p><p>另一方面，为了提升该集群的可用性和自动扩缩容等特性，用户可使用 ShardingSphere-Operator 按业务需求量对 ShardingSphere-Proxy（计算节点）和数据库（存储节点）进行分别的扩缩容，真正实现按需分配、一键弹性。例如，有些用户只需要增强计算能力，ShardingSphere-Operator 将在几秒钟内自动扩容 ShardingSphere-Proxy，而不会对存储能力做任何改变，无危险操作、无付费成本。也有一些用户只关注存储容量，而对计算能力无过多期望，这时他们只需要快速启动更多的空数据库实例并执行 DistSQL 命令，ShardingSphere-Proxy 将对新旧数据库重新进行数据分片，以提高容量和性能。</p><p></p><p>采用 ShardingSphere 对现有数据库集群进行平滑分片，并以更为云原生的方式将其部署于 Kubernetes 上。与其关注如何从根本上打破当前的数据库基础设施，忙于重新寻找一个可以在 Kubernetes 上作为有状态应用进行有效管理的分布式数据库，我们不如从另一个角度思考这个问题：</p><p></p><p>『如何让分布式数据库系统变得“无状态”、充分利用现有的数据库集群、真正实现分布式+云原生？』</p><p></p><p>下面将展示两个真实场景案例：</p><p></p><p>Kubernetes 上的数据库</p><p></p><p>首先使用 Helm charts 等方法将数据库（如 MySQL 和 PostgreSQL）部署到 Kubernetes 环境中，然后使用 ShardingSphere charts 成功部署了 ShardingSphere-Proxy 和 ShardingSphere-Operator 集群。完成部署后，用户可以使用原生的驱动访问方式连接 ShardingSphere-Proxy，使用 DistSQL 让 ShardingSphere-Proxy 感知到单机数据库，即分布式计算节点连接到存储节点，形成最终的分布式数据库解决方案。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/15fd39ca5b8e41d1f48a03e740720ef0.png\" /></p><p></p><p>云端或本地数据库</p><p></p><p>下图为云端、本地的数据库两种形态的部署架构。下图右半部分体现云上计算+云下存储的分布式数据库架构，即计算节点 ShardingSphere-Proxy&nbsp;及 “云上 DBA” ShardingSphere-Operator&nbsp;在 Kubernetes 上运行，但是数据库（存储节点）位于 Kubernetes 之外。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/4c/4c0c14ebef6116f4cfd7cf0a49e011c3.png\" /></p><p></p><p>方案的优势与不足</p><p></p><p>当然，这个世界上并不存在能够满足任何需求的技术银弹，每一款产品或解决方案，都会有其所擅长以及相对不足的领域。</p><p></p><h4>优势</h4><p></p><p>利用现有数据库能力</p><p>在不破坏生产环境遗留数据库架构的前提下，用户可以平滑安全地构建分布式数据库系统。</p><p></p><p>高效平稳迁移</p><p>ShardingSphere 几乎没有停机时间帮助用户完成历史数据迁移及分布式改造。</p><p></p><p>DistSQL</p><p>ShardingSphere 提供 DistSQL 支持以原生数据库的方式（即 SQL）使用分布式数据库系统的分片、数据加密、流量治理等特性。</p><p></p><p>对计算存储能力单独进行灵活扩缩容</p><p>基于计算存储分离架构，用户可以真正实现『按需』单独分别对 ShardingSphere-Proxy&nbsp;（计算能力） 和 Databases&nbsp;（存储能力）&nbsp;进行灵活扩缩容。</p><p></p><p>云原生运行和治理方式增强</p><p>由于 ShardingSphere-Proxy 本质上是一种无状态全局计算服务节点，同时充当全局数据库访问入口，因此更容易在 Kubernetes 上进行云原生的管理和部署。</p><p></p><p>多云或跨云</p><p>数据库作为有状态存储节点可以部署于 Kubernetes 或任意云端，避免单个云平台锁定。仅使用 ShardingSphere 连接节点就可构建分布式数据库系统。</p><p></p><p>数据库的其他特性</p><p>ShardingSphere 是一个围绕数据库的生态系统，除了数据分片，还提供数据加密、权限、读写分离、SQL 审计等特性等待为用户场景全面赋能。</p><p></p><p>用户可选择多个客户端或混合部署</p><p>ShardingSphere 根据用户需求提供两种客户端：ShardingSphereProxy 和 ShardingSphere-JDBC。通常，ShardingSphere-JDBC 性能更高，而 ShardingSphere-Proxy 支持所有的开发语言，提供分布式数据库集群的管理能力。ShardingSphere-JDBC 和 ShardingSphere-Proxy 混合部署可以进行优势互补。</p><p></p><p>开源支持</p><p>Apache ShardingSphere 是 Apache 基金会的一个顶级项目，开源至今已有五年以上。作为一个成熟的社区，Apache ShardingSphere 具备丰富的用户案例、文档和强大的社区支持。</p><p></p><h4>劣势</h4><p></p><p>分布式事务</p><p>事务对于分布式数据库系统也至关重要。但是由于这种技术架构不是从存储层开发的，目前它依赖 XA 协议来协调数据源的事务处理，所以说并不算一个完美的分布式事务方案。</p><p></p><p>SQL 兼容性</p><p>部分SQL 查询在存储节点（数据库）中表现良好，但在全新的分布式系统中会出现问题。我们开源社区仍在努力攻克这一难点。</p><p></p><p>全局一致性备份</p><p>虽然 ShardingSphere 定义为分布式数据库计算引擎，但多数用户倾向于将其视为分布式数据库。因此，用户需要考虑该分布式数据库系统全局备份一致性。ShardingSphere 正在针对这一特性进行研发，目前暂不支持（5.2.1），用户需要对数据库进行手动或用 RDS 备份。</p><p></p><p>成本增加</p><p>ShardingSphere 会接收所有请求，计算并发送至存储节点。每次查询都必然会增加成本，所有分布式数据库都会遇到这种情况。</p><p></p><p></p><h1>实操指南</h1><p></p><p>本节将演示如何使用 ShardingSphere 和 PostgreSQL RDS 创建分布式 PostgreSQL 数据库，以及用户如何对两个 PostgreSQL 实例进行数据分片。</p><p></p><p>以下演示过程中，ShardingSphere-Proxy 运行于 Kubernetes；PostgreSQL RDS 运行于 AWS。部署架构如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6c/6cdfbcc2e00e50491e09febf1ad457cb.png\" /></p><p></p><p>演示主要包含以下内容：</p><p></p><p>部署 ShardingSphere-Proxy 集群和 ShardingSphere-Operator。使用 DistSQL 构建分布式数据库表。测试 ShardingSphere-Proxy 集群（计算节点）的弹性伸缩能力和高可用。</p><p></p><p></p><h4>1. 准备数据库 RDS</h4><p></p><p></p><p>在 AWS 或任意云上创建两个 PostgreSQL RDS 实例作为存储节点。</p><p></p><p></p><h4>2. 部署 ShardingSphere-Operator</h4><p></p><p></p><p>下载 repo，在 Kubernetes 上创建一个名为 sharding-test 的命名空间。</p><p></p><p><code lang=\"plain\">Bash\ngit clone https://github.com/apache/shardingsphere-on-cloud\nkubectl create ns sharding-test\ncd charts/shardingsphere-operator\nhelm dependency build\ncd ../\nhelm install shardingsphere-operator shardingsphere-operator -n sharding-test\ncd shardingsphere-operator-cluster\nvim values.yaml\nhelm dependency build\ncd ..\nhelm install shardingsphere-cluster shardingsphere-operator-cluster -n sharding-test \n</code></p><p></p><p>修改并部署 shardingsphere-operator-cluster 的 values.yaml 中的 automaticScaling: true 和 proxy-frontend-database-protocol-type: PostgreSQL。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5d08dc612f7e1a822c1f17073fd10954.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b33baa2cd03bdfe93ba0013c31db11c8.png\" /></p><p></p><p>以上操作会创建一个 ShardingSphere-Proxy 集群，其中包含 1 个 Proxy 实例、2 个 Operator 实例和 1 个 Proxy 治理实例，如下所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/6a/6a7b54c332ba65adddd356de7b1c45c8.png\" /></p><p></p><p></p><h4>3. 使用 DistSQL 创建分片表</h4><p></p><p></p><p>(1) 登录 ShardingSphere-Proxy 并添加 PostgreSQL 实例。</p><p></p><p><code lang=\"plain\">Bash\nkubectl port-forward --namespace sharding-test svc/shardingsphere-cluster-shardingsphere-operator-cluster 3307:3307\npsql --host 127.0.0.1 -U root -p 3307 -d postgres\n</code></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c7/c7cf884bcbad822ecc5bf4654d7813cd.png\" /></p><p></p><p>(2) 执行 DistSQL，用 MOD（user_id, 4）创建一个分片表 t_user，显示这个逻辑表 t_user 的实际表。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/0c/0c24a503f9bb997e89a39d9df755baa2.png\" /></p><p></p><p>(3) 插入测试行，在 ShardingSphere-Proxy 上执行查询，得出最终合并结果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f4/f45e18a520e7aadfdde9cde4f9016745.png\" /></p><p></p><p>(4) 登录到两个 PostgreSQL 实例以获取它们的本地结果。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/be/be173a2dccf64ed9486693d6e21c77d6.png\" /></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d6/d6f9e2e0c74e9ac866f5b3df77d00827.png\" /></p><p></p><p>以上测试有助于理解 ShardingSphere 管理和分片数据库的功能,用户无需关注不同分片中的单个数据。</p><p></p><p></p><h4>4. 测试 ShardingSphere-Proxy 集群（计算节点）的伸缩和高可用</h4><p></p><p></p><p>如果用户认为新系统的 TPS 或 QPS 太高，可以考虑升级整个分布式数据库系统的计算能力。相较于其它分布式数据库系统，ShardingSphere-Proxy 增加计算节点的方式最为简单。ShardingSphere-Operator 能够基于 CPU 指标确保 ShardingSphere-Proxy 的可用性和弹性伸缩。此外，还可通过修改参数进行扩容或缩容，如下图所示：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/24/246419b10ced9e412b23f107d194177d.png\" /></p><p></p><p>升级完成后用户将会获取两个 ShardingSphere-Proxy 实例，表明计算能力得到了增强。</p><p></p><p>如上，若用户需要更多的存储容量，可以采取以下步骤：</p><p>在云端或本地部署启动额外的 PostgreSQL 实例；将新的存储节点添加到 ShardingSphere-Proxy 中；运行 DistSQL，使用 ShardingSphere 进行重新分片。</p><p></p><p></p><h1>小结</h1><p></p><p></p><p>本文聚焦 Kubernetes 上的一种全新分片数据库架构，利用分布式数据库存算分离的架构，借助现有单体数据库帮运维团队高效流畅地将数据库基础设施转化为现代数据库的分布式系统。通过在 Kubernetes&nbsp;重新解读和应用分布式数据库计算存储分离这种传统架构，解决了有状态数据库在 Kubernetes 上的部署、治理、使用等问题。</p><p></p><p>如今，分布式数据库、云计算、开源、大数据、数字化转型都是热门概念。这些热词传递了新概念、新想法和新方案，对解决生产问题、满足生产需求大有裨益。但毕竟世界上没有完美的方案，我们要善于接受新的思想、通过权衡优劣、依据用户独特的场景需要选出最适合自己的方案才是技术选型的王道。</p><p></p><h4>关于作者</h4><p></p><p>潘娟，SphereEx 联合创始人&amp;CTO、Apache Member &amp; Incubator Mentor、Apache ShardingSphere PMC、AWS Data Hero、腾讯云 TVP、中国木兰开源社区导师。曾负责京东数科数据库智能平台的设计与研发，现专注于分布式数据库 &amp; 中间件生态及开源领域。</p><p></p><p>参考资料：</p><p>[1] <a href=\"https://www.verifiedmarketresearch.com/product/database-as-a-service-providers-market/\">https://www.verifiedmarketresearch.com/product/database-as-a-service-providers-market/</a>\"</p><p></p><p>[2] <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/\">https://kubernetes.io/docs/concepts/extend-kubernetes/</a>\"</p><p></p><p>[3] <a href=\"https://shardingsphere.apache.org/document/current/en/overview/\">https://shardingsphere.apache.org/document/current/en/overview/</a>\"</p><p></p><p>[4] <a href=\"https://github.com/apache/shardingsphere-on-cloud\">https://github.com/apache/shardingsphere-on-cloud</a>\"</p><p></p><p>[5] <a href=\"https://shardingsphere.apache.org/document/5.2.0/en/user-manual/shardingsphere-proxy/migration/build/\">https://shardingsphere.apache.org/document/5.2.0/en/user-manual/shardingsphere-proxy/migration/build/</a>\"</p>",
    "publish_time": "2022-11-22 15:08:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "裁够了，重新招聘！马斯克“三板斧”改革后，能用硬核加班文化再造一个Twitter 2.0 吗？",
    "url": "https://www.infoq.cn/article/uNeOAuw1HXjKFBHjwRjk",
    "summary": "<p>在埃隆·马斯克接管 Twitter后的前三周内，这位 51 岁的亿万富翁<a href=\"https://www.infoq.cn/article/6NcqULAMegQoj9GApW5X\">解雇了多位内部高管</a>\"，实施了大规模裁员，失去了为公司贡献90%收入来源的主要广告商们。此外，他还表示不鼓励远程工，更是制定了“极端硬核”的生产力预期，并向员工们放出讯号警告称“有破产的可能”。</p><p>&nbsp;</p><p>马斯克对 Twitter进行的大刀阔斧的改革，导致近一半的员工主动离职。上周四晚上，数以千计的Twitter员工决定离开他们工作了几年甚至十几年的公司，其中包括对维护平台基础设施和安全性至关重要的工程师。据知情人透露，随着这部分员工的主动离开，原先7500多人的硅谷巨头现在可能只剩下不到2700人，使得Twitter的规模从科技巨头沦为创业公司。</p><p>&nbsp;</p><p>现在，这场持续约20天的雷厉风行的风波或许终于要结束了。美国科技媒体The Verge称，马斯克于11月21日表示，目前Twitter已经结束<a href=\"https://www.infoq.cn/article/jm7V0mpaHGLmP6zITIfX\">裁员</a>\"，并且正在启动招聘。</p><p></p><h2>高管大换血</h2><p></p><p>&nbsp;</p><p>马斯克在完成推特收购交易仅仅几分钟后就解雇了多名推特高管，包括首席执行官 Parag Agrawal、首席财务官 Ned Segal 和法律政策、信托和安全部门负责人 Vijaya Gadde 等。</p><p>&nbsp;</p><p>马斯克裁掉这三位高管的代价将超过2亿美元，其中 Parag Agrawal 将获得超过6000万美元。Twitter提交给美国证券交易委员会的文件显示，在Twitter与马斯克旗下X Holdings的合并协议中，这三名高管受到美国“黄金降落伞”条款的保护。</p><p>&nbsp;</p><p>然而知情人士表示，马斯克解雇高管是“有原因的”。他声称这样做有有效的法律依据，因此不需要向他们支付相关遣散费。至于为什么解雇这些高管，马斯克给出的理由是“对Twitter管理不善”，如果不是他的出价收购，Twitter的股价早就暴跌了。</p><p></p><h2>暴力裁员，推行“极端硬核”企业文化</h2><p></p><p>&nbsp;</p><p>据外媒报道，当地时间上周三凌晨，马斯克向 Twitter 员工发送一封电子邮件，在邮件中马斯克表示，“<a href=\"https://www.infoq.cn/article/LxFCa6vY353SjMTw8qpL\">Twitter </a>\"将非常坚定地向前发展，预计员工将长时间高强度地投入工作”。马斯克告诉员工，“只有出色的表现才能构成及格分数”。他还要求员工在周四下午 5 点之前做出选择——要么接受<a href=\"https://www.infoq.cn/article/J8E4r5V4UBAFmmzFvV9f\">“极其硬核”的 Twitter 2.0 计划</a>\"，要么拿着三个月薪资的遣散费走人。</p><p>&nbsp;</p><p>在周四晚上的一条推文中，马斯克说：“最优秀的人都留下来了，所以我不是特别担心。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/4a/4ae02e2e936ff1992a746192143d1efb.png\" /></p><p></p><p>&nbsp;一名离职Twitter员工在在接受The Verge采访时表示，考虑到离开的人数，Twitter 可能“很快就会崩溃”，而且“Twitter 很难恢复”。</p><p>&nbsp;</p><p>Twitter 内部的多个“关键”团队现在已经完全或几乎完全辞职，维护公司每位工程师都使用的Twitter核心系统库的团队员工也离开了。“没有这个团队，你就无法运营Twitter，”一位离职员工说道。</p><p>&nbsp;</p><p>随着员工的离开，Twitter暂时关闭了所有办公楼并暂停徽章使用。知情人表示，马斯克和他的团队“害怕员工会破坏公司”。</p><p>&nbsp;</p><p>如此大规模的员工离职，还是让马斯克稍改了此前<a href=\"https://mp.weixin.qq.com/s?__biz=MzA4NTU2MTg3MQ==&amp;mid=2655219657&amp;idx=1&amp;sn=3ffb9d00f97cadc5c2b567da8efb32af&amp;chksm=8461c8a9b31641bfa1d3842fe19dd993279c7b8d153da6cc69acf44c8564b97b795a65b28d5c&amp;scene=27#wechat_redirect\">强硬态度</a>\"。据报道，在截止日期前的最后几个小时，马斯克试图说服员工留下来。他甚至收回了自己之前定下的所有员工每周在办公室工作 40 小时的规定，表示，“只要每月召开面对面会议并获得管理层批准，就可以在家工作。”</p><p>&nbsp;</p><p>近日，一则网传的消息称，在与马斯克“并肩奋战”至深夜后，一位在Twitter工作的网友在小红书发帖表示：“入职Twitter以来最难忘的一个周末，见到了偶像Elon本人并且跟他一起奋战到了深夜。”</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/84/8439b2c6189b9812e0e691a285fd0f8a.png\" /></p><p></p><p>&nbsp;不过短短几小时后，该网友却再次更新了动态：自己已经被炒了。</p><p>&nbsp;</p><p>但目前该博主在小红书平台的该贴已经删除。</p><p></p><h2>蓝V功能上线，吓跑广告商</h2><p></p><p>&nbsp;</p><p>据Twitter公布的第二季度财报显示，截至2022年6月30日，该公司营收从上年同期的11.9亿美元下降到11.76亿美元。净亏损为2.7亿美元，而上年同期盈利6600万美元。广告收入同比增长2%至10.8亿美元，但订阅和其他收入同比下降27%至1.01亿美元。</p><p>&nbsp;</p><p>为了让Twitter活下去，在大刀阔斧地裁员之前，当地时间11月5日，Twitter在针对苹果设备的更新通知中表示，将对英国、美国、加拿大、澳大利亚和新西兰等国的用户开放Twitter Blue服务，每月收费7.99美元。此前，只有高知名度或有影响力的个人和组织才能获得这个“蓝V认证”。马斯克本人的一连串推文表明，这项“蓝V认证”将在全球范围内推广，这是马斯克对Twitter 的首次重大修改。</p><p>&nbsp;</p><p>在大裁员之后的第二天，推特蓝V收费功能提前三天上线；这是马斯克最看重的创收项目，从他入主Twitter到正式上线仅仅用了8天时间。</p><p>&nbsp;</p><p>作为全球最大的社交媒体平台之一，<a href=\"https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247569173&amp;idx=1&amp;sn=e60510bae4f1e6dba3efdbaa050d68ef&amp;chksm=fbeb76dacc9cffccf207d2c4e73a5e4f52230ba113dfe7054926b3e5a52992a001e45397ca4f&amp;scene=27#wechat_redirect\">Twitter</a>\"每年营收的90%都来自于广告收益。马斯克推出“蓝V认证”很大原因是想摆脱对于广告商的依赖。</p><p>&nbsp;</p><p>在他入主Twitter之后，辉瑞、欧莱雅、美联航、安联保险等大广告主纷纷停止了推特广告投放；此外，作为特斯拉竞争对手，通用和福特两家车企已表示将暂停在Twitter平台的广告业务。</p><p>&nbsp;</p><p>通用在声明中表示：“外媒正在与Twitter合作，进一步了解平台在新所有权下的发展方向。目前暂时暂停了广告投放，这属于正常的业务变更，同时我们在Twitter上的客户互动服务还将继续。”另外，福特公司也表示正在评估马斯克领导下平台的发展方向，不过公司目前并没有在Twitter上做广告，一般都是以官方账号与消费者互动的方式进行，这些都为Twitter未来的广告营收带来了更大的不确定性。</p><p>&nbsp;</p><p>更重要的是，由于设计的漏洞和认证的缺失，大量普通用户在交钱获得蓝V之后，更改自己的账户资料，伪装成名人、大企业甚至是政治人物发布虚假信息。蓝V反而成为了推特帮助他们扩散谣言的工具。在引发诸多混乱之后，Twitter不得不在11月11日宣布暂停蓝V收费项目。</p><p>&nbsp;</p><p>以上诸多举动引发了人们对 Twitter 是否会生存下去以及会产生什么影响的疑问。</p><p>&nbsp;</p><p>但马斯克昨日在Twitter全体大会上的发言似乎解答了这一质疑。</p><p>&nbsp;</p><p>美国科技媒体The Verge称，马斯克于11月21日表示，目前Twitter已经结束裁员，公司目前正在招聘工程和销售人员，并鼓励员工推荐合适人选。而此次招聘的员工，都有一个“前提”，接受Twitter2.0的工作模式。即为员工“长时间高强度工作”，且“只有表现出色才算及格”。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href=\"https://www.scoopwhoop.com/finance/companies-that-lost-billions-due-to-twitter-blue-tick-chaos/\">https://www.scoopwhoop.com/finance/companies-that-lost-billions-due-to-twitter-blue-tick-chaos/</a>\"</p><p>&nbsp;</p><p><a href=\"https://www.business-standard.com/article/international/elon-musk-to-relaunch-twitter-s-blue-tick-subscription-on-november-29-122111600096_1.html\">https://www.business-standard.com/article/international/elon-musk-to-relaunch-twitter-s-blue-tick-subscription-on-november-29-122111600096_1.html</a>\"</p>",
    "publish_time": "2022-11-22 16:33:12",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "小马智行拿到北京自动驾驶“前排无人”阶段测试许可 | InfoQ快讯",
    "url": "https://www.infoq.cn/article/Va4qfNap1UkaeEcH8818",
    "summary": "<p>11月21日，北京市智能网联汽车政策先行区颁发自动驾驶无人化第二阶段测试许可，小马智行成为首批获准在北京开启“前排无人，后排有人”的自动驾驶无人化测试资格的企业。这是推进 Robotaxi 行业迈向“车内无人”的过渡阶段，吹响车内无安全员的远程测试及出行服务落地的前奏。</p><p></p><p>获得许可后，小马智行10辆无人化测试车将在经开区60平方公里的核心区内开始全新的测试形式，覆盖亦庄复杂的城区道路场景，有助于安全可控地提升无人化技术验证的效率，并逐步扩大测试规模和范围。</p><p></p><p>去年10月，北京市智能网联汽车政策先行区开放自动驾驶无人化道路测试，同时发布全国首个无人化道路测试管理实施细则。实施细则明确将测试划分为“副驾有人”、“前排无人，后排有人”、“车外远程”三个阶段，有序推动产业步入整车无人化阶段。该细则从申请条件与审核、测试监督与管理、交通违法与事故处理和违规处理等角度，对申请主体及测试车辆提出要求。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e5/e52d085663925357fb241311ec4b58e7.jpeg\" /></p><p></p><p>在“主驾无人，副驾有人”测试阶段，小马智行凭借自动驾驶技术质量、冗余预案，高标准检测及评审结果拿下<a href=\"http://mp.weixin.qq.com/s?__biz=MzI5NTg2NDg3MA==&amp;mid=2247488684&amp;idx=1&amp;sn=7356e7f4e5bd2f886b209082c5b7eb46&amp;chksm=ec4c4be5db3bc2f346913cbaf5356330dcc0a32537057d9b6388eff8db5ed2d9525b5f0bc18f&amp;scene=21#wechat_redirect\">道路测试</a>\"、<a href=\"http://mp.weixin.qq.com/s?__biz=MzI5NTg2NDg3MA==&amp;mid=2247489798&amp;idx=1&amp;sn=c010acfd16bfe4b582d615d28b699c8d&amp;chksm=ec4c464fdb3bcf59476ccb1687d975104406ae507f923ebb818558c26a1c687f7869a6851083&amp;scene=21#wechat_redirect\">载人示范运营</a>\"及<a href=\"http://mp.weixin.qq.com/s?__biz=MzI5NTg2NDg3MA==&amp;mid=2247490259&amp;idx=1&amp;sn=6b57622c10c4899bee402a9e6cce20b2&amp;chksm=ec4c459adb3bcc8c59792f666891eb2f7df10868b25d7e7f659c0ef0ef30df147c55902f926c&amp;scene=21#wechat_redirect\">商业化运营</a>\"的许可，其中检测项目包括里程接单量、安全运行要求、封闭场地测试、网络安全测试等。</p><p></p><p>在亦庄核心区，打无人自动驾驶车辆已成为北京市民日常生活的一部分。在此背景下，小马智行持续推进技术无人化领域发展，将向市场推出真正具备竞争力的商业化 Robotaxi 服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/92/92d503a051bef9aa14f6d8eaf9594ffc.jpeg\" /></p><p></p><p>相较于过往的测试形式，“前排无人”、“车外远程”等测试形式有助于自动驾驶企业对极端场景应对、冗余系统方案、远程辅助平台、服务运营体验等给出更为客观的评判，降低接管中人为因素的影响，获得真实完整的表现数据。在保证测试安全的基础上，小马智行开启“前排无人”测试将推动自身在无人化领域的技术攻坚和迭代。</p><p></p><p>据悉，小马智行已在北京、广州等多个城市开启主驾无安全员的无人化测试，向公众开放相应的 Robotaxi 服务。</p>",
    "publish_time": "2022-11-22 16:50:44",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "开发者社区「开源人说」第四期大数据&AI专场精彩大放送——大师零距离",
    "url": "https://www.infoq.cn/article/631HE0BRRxIDhOLm3wl5",
    "summary": "<p>与开源中国创始人&amp;CTO红薯、白鲸开源联合创始人代立冬、阿里云莫问、叔宝、绝顶等8位重磅嘉宾亲密互动；！</p>",
    "publish_time": "2022-11-22 17:44:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "开发者社区「开源人说」第四期大数据&AI专场精彩大放送——技术真心话",
    "url": "https://www.infoq.cn/article/o27JcGg1abEPmXDKoL3j",
    "summary": "<p>聊开源初心，解读大数据AI开源趋势，与青年力量现场对话，现场金句频出，点击视频了解更多精彩内容！</p>",
    "publish_time": "2022-11-22 17:44:36",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "国内首个技术公益深度调研报告发布：技术人才成为公益发展中坚力量",
    "url": "https://www.infoq.cn/article/3Ki0eHOsrf03IulwES3A",
    "summary": "<p>11月22日，腾讯联合多家企业、院校、媒体，发布国内首个技术公益深度调研报告《公益事业新版图之技术公益人才篇》，着眼于技术人才的公益情感和公益意愿，探究技术人才参与公益的影响因素。报告显示，有近九成的技术人才曾参与公益活动，已成为公益发展中坚力量。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/30/29/30029e475e511a0d258113b661227f29.jpg\" /></p><p></p><p></p><p>腾讯用户研究与体验设计部(CDC)总经理、腾讯技术公益发起人陈妍发布调研报告内容</p><p>&nbsp;</p><p>技术人才参与公益意愿高，期待扮演多种角色发挥个人专长</p><p></p><p>随着公益实践的持续开展，技术公益的范围也在扩展。在这个过程中，“人”，尤其是“专业型人才”已成为技术公益的发展核心。</p><p>&nbsp;</p><p>报告指出，近九成的技术人才表示曾参与过公益活动，其多以贡献个人精力、捐钱捐物的方式参与。他们更加“关注社会问题和公共利益”，“希望为社会做点力所能及的事情”。同时，技术人才中有更高的比例未来愿意继续参与公益活动、克服参与时碰到的困难、并鼓励身边的人共同参与。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/a7/6d/a7c3bb39d550a54d542dd4bc6beb936d.png\" /></p><p></p><p></p><p></p><p>技术人才感兴趣的公益领域较为广泛，其中更加关注“儿童/青少年成长”、“生态保护”、“老年群体关爱”等话题。他们希望活动能够具备“传递正能量”“公开透明”“强参与感/成就感”等特征，对“能切实改善”“可持续的”“能发挥个人专长”等方面也抱有更大期待。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/93/8b/9386b549212aaffb6472fdaa4611548b.png\" /></p><p></p><p>在参与方式上，54%的技术人才更期望在公益活动中扮演“一线执行者/捐献者”的角色，43.3%的技术人才希望担任“组织/协调/监督者”的角色。对比受访总体群体，技术人才在多个角色上的选择比例更高，在“咨询顾问”一角的选择比例上尤为突出。</p><p>&nbsp;</p><p><img src=\"https://static001.infoq.cn/resource/image/32/2f/32cf11613396a7df5f8fa52dc31ff92f.png\" /></p><p></p><p></p><p>完善技术人才公益参与机制，让技术主动寻找公益</p><p></p><p>为了提高技术人才参与意愿，并实现向高参与度的转化，报告对吸引技术人才参与公益的因素进行了深度探讨，发现参与者所需付出、公益项目的内容及特征、所处环境是重要影响因素。</p><p>&nbsp;</p><p>而在从未参与过公益活动的技术人员中，有48.1%是因为不知道有哪些公益活动，或不知道如何参与。这也充分证明搭建有效的技术志愿者平台，连接技术志愿者和公益活动，将成为未来技术公益发展的重要内容。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/d3/f3/d3d1e929c7cab4235f178ba0c62164f3.png\" /></p><p></p><p></p><p>在此基础上，根据技术公益的项目机制，报告从问题提出及项目提出、参与契机/动员方式、协作及参与体验、服务落地和可持续、个体回馈/激励等阶段逐级切入，力求在公益情感、公益意愿和公益参与等方面回应技术人才的需求，完善参与机制，提高技术人员公益参与度。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/e2/b3/e2e17317c9d8b67cc1eeff322a1753b3.png\" /></p><p></p><p></p><p>&nbsp;</p><p>例如，在动员技术人员参与方面，报告提出，可以从技术人员在公益实践活动中的“认同感”、“成就感”、“贡献力量”等高影响因素入手，以文化宣导和个人情感拉动的方式，唤起技术人员公益情感，共同促进技术个体观念转变，主动参与公益活动。</p><p>&nbsp;</p><p>而在分工协作及参与体验方面，报告认为，“案例学习和分析”、“专家指导”、“跨学科交流”都可以促进技术人才充分发挥其能力，也是项目中的必要机制。此外，“参与人之间的交流、研讨”、“流程和技巧”方面的培训也能够提升技术人才在公益活动中的体验感。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/7d/3c/7d3bd85d26ccb4407ea5fe07c381233c.png\" /></p><p></p><p></p><p>&nbsp;</p><p>值得一提的是，报告还指出，过去的公益实践更多是从需求角度出发，许多技术要在公益项目遇到问题后才开始介入，具有一定的滞后性。如果变为从“技术”的角度出发，让技术主动寻找、发现、解决公益实践中的问题，或许会更加高效地进行技术公益实践。</p><p>&nbsp;</p><p>推动技术人才广泛持续参与，技术公益发展需要更多创新</p><p></p><p>对于技术公益的发展，报告表示，其现阶段仍面临理念知晓度不高、缺乏基础设施和协作模式等挑战。对此，报告中也针对性地提出了一些解决思路。</p><p>&nbsp;</p><p>对于理念知晓度不高的问题，可以通过技术公益创投等创新形式的实践，充分鼓励社会力量参与公益事业，促进技术人才认知转变。此外，也要重视正在成长中的技术人才——学生群体的技术公益理念培养，提升学生参与技术公益的意愿。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/a3/5c/a370afa9d10dca03797dc2ec4386725c.png\" /></p><p></p><p></p><p></p><p>对于基础设施和协作模式缺少的问题，可以充分利用设计人才的粘合作用，以需求为导向，发挥设计思维的不设限，在愿景驱动下帮助技术公益找到发展路径，从而带动技术整合和基础设施建设。</p><p>&nbsp;</p><p>以腾讯公益慈善基金会发起的“腾讯技术公益创投计划”为例，在多个社会关切领域遴选具备发展潜力的项目，并持续进行投入帮扶，营造技术公益的开放生态。创投计划二期也更加重视高校学生群体，增设公益创想组征集学生的技术公益方案，注入新生力量。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/12/fe/1246f1f625297c076c843d11fe6111fe.png\" /></p><p></p><p></p><p>在基础设施建设领域，腾讯面向公益行业建设的数字能力平台、以及2022年“腾讯技术公益数字工具箱”的上线，均是从平台以及生态的角度为技术公益发展提供基础支撑。</p><p>&nbsp;</p><p>近年来，技术的发展为公益事业带来了全新的视角和思路。作为技术公益领域的重要参与者，腾讯表示，未来将持续全方位助力技术公益发展，支持技术人才广泛投身公益项目，与社会各界共同用实践推动社会创新。</p>",
    "publish_time": "2022-11-22 19:31:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]