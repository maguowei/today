[
  {
    "title": "eBPF与 Service Mesh：Layer7处理不太可能在eBPF中实现",
    "url": "https://www.infoq.cn/article/7xFHFHbygmwN9cTsoEgc",
    "summary": "<p>关于eBPF的故事已经在云计算世界泛滥了一段时间。有时候，人们会把它描述成自切片面包以来最伟大的东西，有时候又嘲笑它是对现实世界无用的干扰。当然，现实情况要微妙得多，所以我们似乎有必要仔细了解一下eBPF能做什么和不能做什么——技术毕竟只是工具，我们应该为手头要处理的任务选择合适的工具。</p><p></p><p>最近经常出现的一个问题与服务网格（Service Mesh）所需的Layer7处理有关。将这个任务交给eBPF处理可能是服务网格的一个巨大胜利，因此让我们来仔细地研究一下eBPF可能扮演的角色。</p><p></p><p></p><h2>那么eBPF到底是什么</h2><p></p><p></p><p>我们先来看看这个名字——“eBPF”最初是指“扩展的伯克利包过滤器（extended Berkeley Packet Filter）”，尽管现在它不指代任何东西。伯克利包过滤器（BPF）可以追溯到近30年前——它是一种允许用户应用程序直接在操作系统内核中运行特定代码的技术（当然是经过严格审查和高度约束的代码）。BPF仅应用于网络栈，但仍然可能促成一些令人惊奇的事情。</p><p></p><p>一个经典的例子是，它让试验新类型的防火墙等东西变得非常容易。我们不需要不断地重新编译内核模块，只需要编辑eBPF代码并重新加载它们。类似的，它可以轻松地开发出一些非常强大的网络分析功能，包括你不想在内核中运行的东西。例如，如果你想使用机器学习对传入的数据包进行分类，可以使用BPF抓取感兴趣的数据包，并将它们分发给运行ML模型的用户应用程序。</p><p></p><p>以上是两个非常明显的可能应用BPF的领域【1】，除此之外还有其他一些例子——eBPF将同样的概念扩展到网络以外的领域。但所有这些都涉及一个问题——为什么它们都需要引起我们特别的注意。</p><p></p><p>简单地说，这与“隔离性”有关。</p><p></p><p></p><h2>隔离性</h2><p></p><p></p><p>计算——尤其是原生云计算——严重依赖于硬件同时为多个实体做多件事情的能力，即使有些实体对其他实体是不友好的。这就是多租户争用，我们通常使用可以调解对内存访问的硬件来管理多租户争用。例如，在Linux中，操作系统为自己创建一个内存空间（内核空间），并为每个用户程序创建一个单独的空间（总的来说就是用户空间，尽管每个程序都有自己的空间）。然后，操作系统使用硬件来防止跨空间的访问【2】。</p><p></p><p>维护系统各部分之间的这种隔离对于安全性和可靠性来说都是绝对关键的——与计算安全相关的东西基本上都依赖于它，而原生云对它的依赖甚至更严重，因为云原生要求内核能够保持容器之间的隔离。因此，内核开发人员花了数千人年的时间仔细检查与隔离性相关的每一个交互，并确保内核可以正确地处理所有东西。这是一项棘手的、微妙的、艰苦的任务。而且很不幸的是，在发现bug之前，它常常不会被注意到，而这些又占了操作系统工作的很大一部分【3】。</p><p></p><p>这项工作之所以如此棘手和微妙，部分原因在于内核和用户程序不能完全隔离：用户程序显然需要访问某些操作系统函数。传统上说，这属于系统调用。</p><p></p><p></p><h2>系统调用</h2><p></p><p></p><p>系统调用是操作系统内核向用户代码公开API的一种原始的方式。用户代码隐藏了大量细节，将请求打包并将其交给内核。内核会进行仔细检查，确保所有规则都得到了遵守，并且——如果一切正常——内核将代表用户执行系统调用，并根据需要在内核空间和用户空间之间复制数据。系统调用的关键部分是：</p><p></p><p>内核控制着一切。用户代码可以发出请求，而不是要求。检查、复制数据等操作都需要时间，导致系统调用比运行普通代码更慢，无论是用户代码还是内核代码：这是一种跨越边界的行为，会降低速度。随着时间的推移，执行速度变得越来越快，但是，为繁忙系统中的每一个网络包都执行系统调用是不可行的。</p><p></p><p>而这就是eBPF可以发挥作用的地方：不为每一个网络包（或跟踪点或其他什么）执行系统调用，而是直接将一些用户代码放到内核中！然后，内核就可以全速运行，只有在真正需要的时候才将数据分发到用户空间。（最近，人们对Linux的用户/内核交互进行了大量的反思，带来了很好的效果。<a href=\"https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjkwMTc1MzksImZpbGVHVUlEIjoiUHNJakdOeUJEZmdmZmxFUSIsImlhdCI6MTY2OTAxNzIzOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.8jHOZ5SPfkclJWvVKJOx-ulwkn0LjLcKqggqPG7OSm8\">io_uring</a>\"就是这方面的一项杰出的工作。）</p><p></p><p>当然，在内核中运行用户代码是非常危险的，因此内核要花费大量的精力来验证用户代码想要做什么事情。</p><p></p><p></p><h2>eBPF验证</h2><p></p><p></p><p>当一个用户进程启动时，内核基本上会在可能正常的范围内启动它。内核在它周围设置了护栏，并立即杀死任何试图破坏规则的用户进程，但用户代码被假定具有执行的权利。</p><p></p><p>对eBPF代码则不提供这样的礼遇。在内核中，保护屏障基本上是不存在的，它会盲目地运行用户代码，并希望它们是安全的，这为安全漏洞打开了大门（一些错误可能会带崩整台机器）。相反，eBPF代码只有在内核能够证明它是安全的情况下才会被执行。</p><p></p><p>要证明一个程序是安全的是非常困难的【4】。为了让它变得更容易些，内核极大地限制了eBPF程序可以做的事情。以下是一些例子。</p><p></p><p>不允许eBPF程序发生阻塞。不允许eBPF程序有无界的循环（事实上，直到最近它们才被允许有循环）。不允许eBPF程序超过一定的体积。验证器必须能够评估所有可能的执行路径。</p><p></p><p>验证器必须非常严格，它的决策是决定性的：它必须这样做才能维持整个云原生世界所需的隔离性保证。它在判断程序是不安全的时候还必须宁可杀错不可放过：如果它不能完全确定程序是安全的，就会将其拒绝。不幸的是，有些eBPF程序是安全的，但是验证程序不够聪明，无法通过验证——如果你遇到了这种情况，需要重写程序，直到验证器通过，或者需要给验证器打补丁并构建自己的内核【5】。</p><p></p><p>最终的结果是，<a href=\"https://xie.infoq.cn/article/34d62a334c9af2134e1ab0b02\">eBPF</a>\"成了一种高度受限的语言。也就是说，虽然对每个传入的网络数据包执行简单的检查是很容易的，但像跨多个数据包缓冲数据这样看似简单的事情却很难。在eBPF中实现HTTP/2或终止TLS是完全不可能的：它们太复杂了。</p><p></p><p>最后，所有这些都把我们引向了一个问题：将eBPF的网络功能应用到服务网格中将会是什么样子？</p><p></p><p></p><h2>eBPF和服务网格</h2><p></p><p></p><p>服务网格必须处理云原生网络的所有复杂性。例如，它们必须产生和终止mTLS、重试失败的请求、将连接从HTTP/1透明地升级到HTTP/2、基于工作负载标识执行访问策略、跨集群边界发送流量，等等。在原生云世界中有很多事情正在发生。</p><p></p><p>大多数服务网格使用边车模型来管理一切。网格将一个运行在自己容器中的代理附加到每个应用程序Pod中，代理拦截与应用程序Pod之间的网络流量，完成网格所需的任何工作。这意味着网格可以处理任意的工作负载，并且不需要更改应用程序，这对开发人员来说是一个巨大的胜利。这对平台方也是一种胜利——他们不再需要依赖应用程序开发人员来实现mTLS、重试、黄金指标【6】等，因为网格在整个集群中提供了所有这些东西，甚至更多。</p><p></p><p>另一方面，就在不久前，人们还认为部署这些代理的想法是非常疯狂的，因为他们仍然担心运行额外容器所带来的负担。但是<a href=\"https://www.infoq.cn/topic/Kubernetes\">Kubernetes</a>\"使部署变得很容易，只要你能够保持代理的轻量级和速度，它确实可以很好地工作。（“轻量级和速度”当然是带有主观性。许多网格使用通用的Envoy代理作为边车。Linkerd似乎是唯一一个使用专门构建的轻量级代理的。）</p><p></p><p>因此，一个显而易见的问题是，我们是否可以让功能从边车下沉到eBPF中。在OSI的Layer3和Layer4——IP、TCP和UDP——我们已经看到了eBPF的几个明显的优势。例如，eBPF可以让复杂的动态IP路由变得相当简单。它可以进行非常智能的包过滤，或者进行复杂的监控，而且它可以快速和低成本地完成所有这些任务。在网格需要与这些层交互的地方，eBPF似乎非常有用。</p><p></p><p>然而，在OSI Layer7情况就不同了。eBPF的执行环境如此有限，以至于HTTP和mTLS级别的协议远远超出了它的能力，至少在今天是这样。鉴于eBPF在不断地发展，也许未来的某个版本可以管理这些协议，但需要注意的是，编写eBPF非常困难，而调试可能会更加困难。许多Layer7协议都非常复杂，在相对宽松的用户空间中，它们糟糕到不能正常使用。目前还不清楚为eBPF重写它们是否可行，即使这么做是可能的。</p><p></p><p>当然，我们可以做的是将eBPF与代理配对：将核心底层功能放在eBPF中，然后将其与用户空间代码配对，以此来管理复杂性。通过这种方式，我们可以在较低的级别上获得eBPF的性能优势，同时将真正讨厌的东西留在用户空间中。这实际上是今天每个现有的“eBPF服务网格”所做的，尽管通常没有被广泛公开。</p><p></p><p>这就提出了一些关于这样的代理到底应该被放在哪里的问题。</p><p></p><p></p><h2>单主机代理与边车</h2><p></p><p></p><p>我们不为每一个应用程序Pod部署一个代理（正如我们在边车模型中所做的那样），而是在每台主机（在kubernetes中就是节点）上部署一个代理。它给管理IP路由带来了一点额外的复杂性，但似乎也提供了一些良好的规模经济，因为你所需要的代理变少了。</p><p></p><p>不过，边车比单主机代理有一些显著的好处。这是因为边车被作为应用程序的一部分，而不是独立于应用程序之外。</p><p></p><p>边车使用的资源与应用程序负载成正比，所以如果应用程序的负载不重，边车的资源使用就不会很高【7】。当应用程序负载很重时，所有Kubernetes的现有机制（资源请求和限制、OOMKiller等）都将完全按照你所习惯的方式工作。如果一个边车发生故障，它只会影响一个Pod，而现有的Kubernetes机制也能正常对Pod故障做出响应。边车操作基本上与应用程序Pod操作相同。例如，通过一个正常的Kubernetes滚动重启将边车升级到新版本。边车和它的Pod有着完全相同的安全边界：相同的安全环境、相同的IP地址，等等。例如，它只需要为它的Pod处理mTLS，这意味着它只需要单个Pod的密钥信息。如果代理中有bug，它只会泄漏这一个密钥。</p><p></p><p>对于单主机代理，所有这些问题都消失了。请记住，在<a href=\"https://www.infoq.cn/article/E8Eiuhyg4BhbEKPcN1KV\">Kubernetes</a>\"中，集群调度器决定将哪些Pod调度到给定节点上，这意味着每个节点将有效地获得一组随机的Pod。这也意味着一个给定的代理将与应用程序完全解耦。</p><p></p><p>对单代理的资源使用情况进行推断实际上是不可能的，因为它是由流向随机应用程序Pod的随机流量驱动的。反过来，这意味着代理最终会因为一些难以理解的原因而失败，而网格团队将为此承担责任。应用程序更容易受到噪音邻居（noisy neighbor）问题的影响，因为安排给指定主机上的Pod的流量都必须通过单个代理。一个高流量的Pod完全可能消耗掉节点的所有代理资源，并让其他Pod挨饿。代理可以尝试确保公平性，但如果高流量的Pod也在消耗所有节点的CPU，代理的尝试也会失败。如果一个代理失败，它会影响应用程序Pod的一个随机子集——而这个子集将不断发生变化。同样，尝试升级一个代理将影响一个类似的随机的、不断变化的应用程序Pod子集。任何故障或维护任务都会突然产生不可预测的副作用。代理现在必须跨越应用程序Pod的安全边界，这比只与单个Pod耦合的情况要复杂得多。例如，mTLS要求持有每个Pod的密钥，不能混淆了密钥与Pod。代理中的任何bug都可能导致可怕的后果。</p><p></p><p>从根本上说，边车利用了容器模型的优势：内核和Kubernetes努力在容器级别强制执行隔离和公平性，一切都能正常工作。单主机代理超出了该模型，这意味着它们必须自己解决多租户争用的所有问题。</p><p></p><p>单主机代理确实是有优势的。首先，在Pod世界中，从一个Pod到另一个Pod总是需要经过两次代理。而在单主机代理世界中，有时只需要经过一次【8】，这可以减少一点延迟。此外，你可以运行更少的代理，如果你的代理在空闲时有很高的资源使用，这样可以节省资源消耗。不过，与运维和安全方面的成本相比，这些改进带来的好处是很小的，这些问题主要可以通过使用更小、更快、更简单的代理来缓解。</p><p></p><p>我们是否可以通过改进代理来更好地处理多租户争用来缓解这些问题？也许吧。这种方法存在两个主要问题。</p><p></p><p>多租户争用是一个安全问题，安全问题最好使用更小、更简单、更容易调试的代码来处理。通过添加大量代码来处理多租户争用问题基本上与安全最佳实践是南辕北辙的。即使安全问题能够得到解决，仍然会存在业务问题。在任何时候，当我们选择进行更复杂的操作时，我们都应该问问为什么要这么做，以及谁会受益。</p><p></p><p>总之，这种代理上的变化很可能涉及大量的工作【9】，为此我们非常关注这些工作所能带来的价值。</p><p></p><p>让我们回到最初的问题：将服务网格功能下沉到eBPF会是什么样子？我们知道我们需要一个代理来维护我们所需的Layer7功能，我们还知道边车代理可以在操作系统的隔离保证范围内运行，单主机代理必须自己管理一切。这是一个不小的差异：单主机代理的潜在性能优势远远超过额外的安全问题和操作复杂性，因此无论是否使用eBPF, 边车都是最可行的选择。</p><p></p><p></p><h2>展望未来</h2><p></p><p></p><p>显然，任何服务网格的第一优先级必须是用户的操作体验。我们可以通过eBPF来获得更好的性能和更低的资源使用，这太棒了！但需要注意的是，我们不能在这个过程中牺牲用户体验。</p><p></p><p>eBPF最终能够接得住整个服务网格吗？似乎不太可能。正如上面所讨论的，在eBPF中实现所需的Layer7处理是否可行还不清楚，即使在某个时候它确实是有可能的。类似的，我们也可以通过一些其他的机制来将Layer7的功能迁移到内核中——尽管从历史上看，这方面并没有很大的推动力，也不清楚什么会真正使其引人注目。（请记住，将功能迁移到内核中意味着将移除我们在用户空间中所依赖的安全屏障。）</p><p></p><p>因此，在可预见的未来，服务网格发展的最佳路线似乎是积极地寻找在性能方面可以依赖eBPF的地方，但要接受在用户空间使用边车代理，并加倍努力让代理尽可能小、快速和简单。</p><p></p><p></p><h2>脚注</h2><p></p><p></p><p>或者至少大大简化了。至少，在程序之间没有预先安排的情况下。这超出了本文讨论的范围。剩下的大部分都是调度。事实上，这在一般情况下是不可能的。如果你想重温计算机科学课程，首先要从悬而未决的问题开始。其中一件事可能比另一件容易。特别是如果你想让你的验证器补丁被上游接受！流量、延迟、错误和饱和。假设还是一个足够轻量级的边车。但有时仍然是两次，所以这有点喜忧参半。例如，有一个有趣的<a href=\"https://twitter.com/mattklein123/status/1522925333053272065?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjkwMTc1MzksImZpbGVHVUlEIjoiUHNJakdOeUJEZmdmZmxFUSIsImlhdCI6MTY2OTAxNzIzOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.8jHOZ5SPfkclJWvVKJOx-ulwkn0LjLcKqggqPG7OSm8\">推特帖子</a>\"介绍了为Envoy这么做有多困难。</p><p></p><p>作者简介：</p><p>Flynn是Buoyant公司的技术布道者，主要关注Linkerd服务网格、Kubernetes和云原生开发。他还是Emissary-Ingress API网关的原始作者和维护者，并在软件工程领域工作了几十年的时间，始终遵循通信和安全的共同主线。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/articles/ebpf-service-mesh/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NjkwMTc1MzksImZpbGVHVUlEIjoiUHNJakdOeUJEZmdmZmxFUSIsImlhdCI6MTY2OTAxNzIzOSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.8jHOZ5SPfkclJWvVKJOx-ulwkn0LjLcKqggqPG7OSm8\">eBPF and the Service Mesh</a>\"</p><p></p><p>相关阅读：</p><p><a href=\"https://www.infoq.cn/article/iZBMUMOvkY3JJdHADgBX\">中国工商银行基于 eBPF 技术的云原生可观测图谱探索与实践</a>\"</p><p><a href=\"https://www.infoq.cn/article/8YXcO6DmQPr6JwOsqKIL\">eBPF&nbsp;技术实践：加速容器网络转发，耗时降低 60%</a>\"</p><p><a href=\"https://www.infoq.cn/article/deJyvo6TQAb09ilLtd6T\">eBPF&nbsp;与 Wasm：探索服务网格数据平面的未来</a>\"</p><p></p>",
    "publish_time": "2022-11-22 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "如何利用社交网络分析，对舆论事件层层拆解？",
    "url": "https://www.infoq.cn/article/EpqPjURla11EjjmTNlwj",
    "summary": "<p>近年来域外社交平台涌现污名化中国的舆论事件，事件背后的组织与个人对事件的发酵与传播起着推波助澜的作用。本议题以域外抵制2022年北京冬奥会为例，以百分点数据科学基础平台为实现工具，介绍如何利用社交网络分析与机器学习分类算法，分析挖掘参与话题用户的角色作用，并识别异常机器人用户，实现恶意话题中的用户类型精准定位。</p><p></p><p>暗流涌动的污名化舆论中，“看不见的手”究竟是谁？谁在张扬推动？谁在暗中勾连？本次分享将为大家揭秘，社交网络分析如何对舆论事件层层拆解，定位关键。</p><p></p><p></p><p></p><p>四、听众受益</p><p>•&nbsp;了解社交网络分析基本原理</p><p>•&nbsp;了解如何识别社媒用户在社群中的角色作用</p><p>•&nbsp;了解如何使用百分点数据科学基础平台实现社交网络分析与可视化过程</p><p></p><p>五、适合人群</p><p>数据分析师、舆情分析师、数据分析、大数据方向的研发工程师、产品经理以及相关从业人员</p><p></p><p>六、讲师介绍</p><p></p><p>杜晓梦，百分点科技集团数据科学研究院院长，北京大学国家发展研究院Bimba特聘讲师。北京大学光华管理学院，营销建模专业博士；北京大学国家发展研究院硕士；北京大学信息管理系学士，2018年北京市科技新星。2019年获 副研究员职称 ，并带领团队获得北京市科学技术二等奖。</p><p></p><p>组建并带领百分点集团60多人的数据科学团队，2014年带领团队开发出公司核心产品“百分点自动化营销引擎”、”百分点大数据建模平台”。任北京大学新媒体营销研究中心主任助理；北大软微-百分点联合实验室联合主任；中财-百分点金融营销大数据研究中心联合主任。研究兴趣包括营销模型、消费者行为学、互联网与大数据技术、金融大数据营销等，发表英文学术论文3篇，被EI收录，获得2020年英国Emerald出版集团“杰出论文奖”（Emerald Literati Outstanding Paper）。中文学术论文5篇，参与国家自然科学基金重点项目“基于全网数据的消费者行为与偏好研究”，多次参与国际学术会议并发表演讲。</p><p>咨询及实施项目包括华为、国家广电总局、国家质检总局、中国铁路科学院、TCL、王府井百货、华润商业置地、中信集团、宁波银行、爱心保险、上海烟草、云南烟草、广西烟草、汤臣倍健等。</p>",
    "publish_time": "2022-11-22 09:29:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "公有云安全容器设计，腾讯Nodeless弹性容器技术演进和实践",
    "url": "https://www.infoq.cn/article/2JPIXj7aGW8murb40QA7",
    "summary": "<p><a href=\"https://www.infoq.cn/video/vzcywCziz2Ok7CVq4sT2\">Nodeless技术</a>\"可以让企业从节点的运维及管理中脱身，有效避免同节点pod互相影响，甚至是容器逃逸到节点上等情况，让用户集中精力在应用的开发上。文章将分享社区virtual kubelet+kata方案在公有云场景下落地的诸多局限性，以及腾讯在nodeless弹性容器技术上的创新与突破。</p><p>&nbsp;</p><p>本文整理腾讯云高级工程师吴尚儒在<a href=\"https://dive.infoq.cn/2021/beijing/track/1205\">DIVE全球基础软件创新大会 2022</a>\"的演讲分享，主题为“<a href=\"https://dive.infoq.cn/2021/beijing/presentation/4495?utm_source=shishuo&amp;sign=iq_62332d2a74d8a\">公有云安全容器设计：腾讯nodeless弹性容器技术演进和实践</a>\"”。</p><p>&nbsp;</p><p>分享主要分为四个部分展开：第一部分是经典<a href=\"https://www.qingcloud.com/\">Kubernetes集群</a>\"的问题；第二部分会介绍开源virtual kubelet+kata方案的探索；第三部分内容为腾讯云弹性容器服务EKS解决方案；第四部分则是腾讯云弹性容器服务EKS落地实践。</p><p>&nbsp;</p><p>以下是分享实录。</p><p>&nbsp;</p><p></p><h1>经典Kubernetes集群架构</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/83/8301037a4baeb8037701bc86a0e55725.png\" /></p><p></p><p>这是一个经典的Kubernetes集群的架构，会有一些控制面的组件如API Server，以及调度器Scheduler，控制器Controller Manager，还有存储etcd。除了控制面之外，还会有数据面Worker节点，在这些Worker节点上，会预先安装基础组件比如说kubelet、Container Runtime以及一些Daemon Set，业务将工作负载提交到API Server之后，由Scheduler发现业务Pod，并将Pod调度到合适的Worker节点，之后由kubelet负责将业务Pod启起来。这时候业务的容器会混布在Worker节点上面，不同业务之间可能会在同一个Worker节点。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/c1/c1eb6bd9bdc29d4cecd7b25218254409.png\" /></p><p></p><p>这里面带来的第一个问题就是，集群里有节点，那就肯定要做一个节点的管理，特别是节点资源的规划问题。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/30/3054bc521e7a53b29f891766a2884bd8.png\" /></p><p></p><p>这是某个服务的CPU的使用量，从这张图我们可以看见这个业务有一个特别的特点，就是它会有个波峰，有个波谷，时不时CPU用量会有激增，也会降为0。但是它又不是简单的重复，比如说今天这个时间点有CPU的突增，明天的这个时间点却没有对应的用量，但是过了几天之后业务量又会有个突增。另外业务峰值也不固定，按照平常情况来说，峰值可能是2000核左右，但是它会在某一个时刻忽然翻倍，达到了5000核这样一个量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9d/9d4e05eeda38743b15bf072f5ad53400.png\" /></p><p></p><p>&nbsp;</p><p>所以说这里面就涉及到一个问题：什么时候得进行扩缩容？业务并不是简单重复的，所以要提前在合适的时间点进行扩缩容。另外要规划给业务准备多少节点，按照平时的峰值的准备，如果它忽然有个突增，短时间内达到翻倍，这时候增量部分基本上就资源可以运行了。这就是节点带来的资源管理的问题。另外，节点本身有一个维护的问题，节点会有内核，会有OS，这时候涉及到比如说安全问题需要修复，需要更新内核，需要更新OS。同时节点会有容器运行时，这时候涉及到Bug的修复，版本的管理，还有一些节点本身运行的&nbsp;Kubernetes组件，比如说kubelet组件，这时候Kubernetes&nbsp;集群也需要做升级。只要集群规模上了一定量之后，这种节点内核的升级，运行时的变更以及组件的维护就不可避免，这时候会变得非常麻烦。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/20/2053f270c354a26895af0506b9286bfb.png\" /></p><p></p><p>除去节点问题之后，还有另外一个就是业务混布带来的隔离性的问题，同一个Worker节点上面可能会混布不同的业务Pod，这些业务Pod都会共享节点的内核，共享节点的硬盘，以及共享节点网卡与网络。那么共享节点会带来什么问题？比如说有些Pod需要开通加载某些内核模块，一些网络的模块，但是加载这个模块就对它自己有用，对其他的业务没有用，而且网络模块可能会带来收发包的拆解，会影响其他业务的网络性能，这是一个对内核的诉求。</p><p>&nbsp;</p><p>同时还有一些内核的参数，比如说Socket Buffer这些配置都需要去改参数的，不同的业务会有不同的需求。另外一个就是存储，这些Pod它们的临时输出或者是标准输出，都会散落到这个节点上面的硬盘上面去，并且是共享同样的存储，这个时候有些业务如果它的输出的速率大于其他的业务，或者是它频繁地输出，这时候整个存储性能就会被它占用。</p><p>&nbsp;</p><p>同时还有网络，有些业务收发包比较快，因为它使用了大量的UDP的小包，这个时候包量达到了整个机器的收发包的限制之后，其他业务网络性能就会受到比较大的影响。除去这些共享的内核、存储、网络之后，还有个容器逃逸的风险，假设我在Volume里面如果有办法逃逸，我就可以跳到父目录，再往上父目录，多跳几层父目录，这时候就可以跳到这个节点上面的根目录rootfs，就可以看见节点上面所有的文件了。</p><p>&nbsp;</p><p></p><h1>开源virtual kubelet+kata方案探索</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85f8f92573a85fad1c5dc874832e174f.png\" /></p><p></p><p>针对前面提到的问题，我们先来看看开源社区有哪些解决方案，首先刚才提到节点管理的问题，开源社区确实有个方案virtual kubelet，它是将一个虚拟节点添加到一个经典的Kubernetes集群里面，这个虚拟节点上面对应着一个大的资源池，所以说落到这个虚拟节点上面Pod对应着后端的资源池就会比较大，这时候就少去了刚才提到的那种节点资源的管理，你可以理解为它后端对接的是一个伸缩弹性更强的资源池。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2346a7ed1831f6947964cf8c5a5d92d.png\" /></p><p></p><p>对于隔离，开源社区有个比较成熟的方案，kata containers安全沙箱方案，原本不同容器之间是会共享内核，共享CPU、内存、网络、存储，这时候在kata方案上，它会先在物理机上构建出安全沙箱，然后每个Pod运行在单独构建出来的安全沙箱里面，每个Pod会有个单独的内核。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ff/ff5c65ebad3403379b9264ea4f3f3444.png\" /></p><p></p><p>有了这两个开源方案之后，我们就可以做一个初步的探索，就相当于是我们通过virtual kubelet把所有的节点上面kubelet抽取出来，统一在一起，屏蔽了后端的资源池，搭建了一个虚拟节点。假设有个Pod调度到虚拟节点，我们就通过virtual kubelet去选择合适的物理机，把Pod落到有空闲资源的物理机上，业务本身就少去了一个节点资源的管理，由平台来直接对接所有的物理机池。然后对于隔离性，当Pod落到了物理机上的时候，上面的 kata 启动对应的安全沙箱，这时候也可以达到一个隔离性的目的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a4f4ab5da3d34730f131148281d2a99d.png\" /></p><p></p><p>这个virtual kubelet + kata的组合方案，看起来确实可以解决我们提到的问题，但是在实际落地上，却有诸多挑战，特别是在公有云场景上面。首先公有云有一个特点就是多租户，每个租户处在不同的私有网络下面，就类似于这张图，这是一个用户的私有网络VPC，它的API Server就落在VPC里面，然后这个虚拟节点就要去承载上面Pod的创建，这个虚拟机节点位于用户的私有网络内。物理机位于底层的基础网络，containerd&nbsp;跟kata是负责创建安全沙箱的，它们要位于物理机上，所以肯定就是位于基础网络，但它们创建出来的安全沙箱，里面的业务又要位于用户的私有网络，这样才可以达到公有云多租户的场景。第一个问题，就是确保私有网络的隔离性，又能满足创建容器时所需的网络打通，这里会带来一些挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a4/a433f8b6765d62233c93b61c5fd2f947.png\" /></p><p></p><p>除去了因&nbsp;kata&nbsp;落在底层物理机上，带来的这样一个网络打通的挑战，本身 kata&nbsp;方案在公有云上面，也会有一些挑战。首先运行时&nbsp;containerd&nbsp;在物理机基础网络里面，拉容器镜像也就需要在物理机的基础网络上面去实现。但拉镜像会有很多场景，不仅仅是云上的镜像，还可以去拉外部的镜像，或者是用户VPC里面的自建镜像仓库，这时候在物理机的基础网络里拉镜像，怎么样才可以打通外部网络或者是用户的VPC内网络？这是第一个问题。</p><p>&nbsp;</p><p>当拉完容器镜像之后，镜像也是在这个物理机上的。这时候通过这个镜像会构建一个容器的rootfs。然后再由物理机透传或者共享到安全沙箱里面，本质相当于是这个容器的rootfs还是在物理机层面，只是透传到容器里面去的，容器的IO操作还会传导过来的。另外，容器本身运行还会产生一些标准输出，这时候这些标准输出也会落在这个物理机上面，对于日志采集会有挑战，日志采集到了之后，还需要发送到用户部署在私有网络上面的Kafka，这还涉及到一个网络的打通的问题。所以在公有云场景下，整体来说会有一些些弯弯绕绕。</p><p>&nbsp;</p><p>另外就是这个容器的rootfs映射到安全沙箱里面，现在社区比较推荐的方案是virtiofs，它对物理机的内核是有要求的，也就是说它会比较挑物理机，使用面会比较窄。另外这些安全沙箱虽然也说是个虚拟机，但是它并没有办法复用虚拟机热迁技术，因为它的状态，还是跟kata、跟containerd这些跑在物理机上的组件，是有强关联的。安全沙箱里，容器运行过程中需要的一些数据，还是由运行时containerd存储在物理机层面的，并不是跟物理机完完全全独立的。所以说想要做热迁，单纯通过虚拟机的方式来迁移是不够的，还有一些关键的信息要做迁移。整体来说，它对于原本的物理机的运维带有一定的挑战。我们发现这些原因后，总结一点就是kata这种方案还不适合公有云。它可能更适合私有云，但它还不适合公有云多租户场景。</p><p>&nbsp;</p><p></p><h1>腾讯云弹性容器服务EKS解决方案</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10d057ef634cf6d37749bb20d88a8cc4.png\" /></p><p></p><p>我们自研了一套解决方案，想要在多租户场景下面做到完全隔离，必须要把运行时全部搬到虚拟机里面来，相当于是在虚拟机里面有一套完完整整的运行时的组件，在物理机上面不做共享，这时候就可以做到完完全全的隔离，一个Pod一个虚拟机。</p><p>&nbsp;</p><p>首先我们刚才提到的镜像拉取的过程，因为我们已经在虚拟机里有一个containerd，所以说它拉取镜像的时候，就是在用户的私有网络里面进行拉取的，整个流程就跟原本拉取方式没有区别。并且拉取完之后，rootfs落在这台虚拟机里面，虚拟机只要mount到容器里面来，做一个overlayfs，就可以给容器使用了，也不存在从物理机共享到虚拟机里边这种情况，另外日志采集组件也可以部署在用户的私有网络底下。</p><p>&nbsp;</p><p>所以整体来说就解决了刚才提到的问题，并且它已经对物理机没有任何的特殊要求了，就是只要这台物理机能够创建虚拟机就可以了，至于刚才提到的宿主机的组件，物理机的内核都已经没有这个需要了，而且它也没有额外的其他的依赖在物理机上面，可以直接复用原本成熟的虚拟机的热迁技术，整个方案来说就从原本的像虚拟机一样安全，直接就变成虚拟机层面的安全。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc438ae1147b99ce500debe9830bbf02.png\" /></p><p></p><p>我们有了这样一个方向之后，就看下我们的具体设计，首先比较重要的组件就是kubelet，Pod生命周期的管理，所以说我们又想把它做成一个虚拟节点，肯定是需要把kubelet的一些功能能抽取出来，就是我们把kubelet拆成了两个，一个叫EKlet，一个叫EKlet-agent。EKlet用来连接API Server，并且承载所有的Pod的删除以及创建，每个Pod的删除创建我们把它转变成一个创建或者删除轻量虚拟机的流程，也就是说我们一开始的时候没有任何的虚拟机资源，直到你需要一个Pod的时候，这时候你调度到了一个虚拟节点，虚拟节点会帮你快速地去启对应的一个虚拟机出来，在上面再去启对应的Pod。</p><p>&nbsp;</p><p>所以说这个虚拟节点对接的还是刚才提到的物理机资源池，按需创建对应的虚拟机，并且原本Kubernetes 里边通过kubelet，实现了那些logs/exec/cadvisor/port forward请求，会通过API Server传导到EKlet，再通过EKlet传递到EKlet-agent来实现。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/6c/6c9ae490891232c902d478deb72f28fd.png\" /></p><p></p><p>这个是EKlet组件的设计，主机里面的EKlet-agent它就是相当于你创建完虚拟机之后，要真正启Pod的时候，由它来负责。它类似于kubelet，有一部分功能是由EKlet来实现，大部分功能会由EKlet-agent来实现。Pod的生命周期管理，Pod的健康探测，这些kubelet的功能都在EKlet-agent这边实现，并且它本身也可以直连API&nbsp;Server，负责Pod状态上报，比如Pod启动了，Pod运行终止了，或者是Pod的事件上报是通过EKlet直达API Server，同时Pod有更新，它也可以通过API Server去Watch到这个Pod更新，做对应的变更。</p><p>&nbsp;</p><p>当然，还有Pod启动的时候所需要的configmap、secret，以及所需要的集群网络service的更新就是全部由EKlet-agent来实现，并且监控数据的上报也是由EKlet来实现。另外也有一个比较重要的点，就是原本的话，节点是要维护一个状态的，但是现在我们没有一个固定的节点，只有虚拟节点，每个Pod本身都需要维护自己一个状态，我们是通过EKlet-agent上报心跳来维护这台虚拟机的状态。</p><p>&nbsp;</p><p>整体流程就相当于是你有一个Pod，然后落到了虚拟机上面去，之后虚拟节点就会按需创建出来对应的虚拟机资源，在这个虚拟机资源，通过类似kubelet的操作把这个Pod启起来，并且把这个信息上报给API Server，完成了一整套流程。那么当你在销毁的时候，这边EKlet-agent会先把这个Pod的一个资源给你销毁，并且上报了状态之后，到API Server之后，EKlet看见就会销毁对应的虚拟机。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/08/08f4cf5507308c605841fa6257afd207.png\" /></p><p></p><p>除去刚才整个Pod生命周期之后，还有个特点就是我们的网络方案比较特别，首先Pod会直接使用虚拟机这张网卡，所以说流量就可以直通Pod，没有宿主机转发的损耗。</p><p>&nbsp;</p><p>但是我们的Pod并没有使用host network，它本身跟EKlet-agent、containerd的网络还是隔离的，用了不同的网络命名空间，这么做的好处是因为要想要避免业务容器变更，比如说iptables对于这些管控面组件的影响。典型的场景如Istio，Istio大家知道它会有个Init Container，它启动的时候就会涉及到iptables，把所有的流量转到Sidecar上面去，之后才开始启动Sidecar以及业务容器，就是业务容器启动完之后，它的流量就会先通过Sidecar再往外发。</p><p>&nbsp;</p><p>但是在假设使用host network的场景，Init Container启动完之后，就会改一个iptables，把所有的流量转发到Sidecar，但这个Sidecar还没启动，并且这个Sidecar启动的前提是先需要去拉镜像，containerd去拉镜像的时候会碰到这个iptables规则，就会把这个流量转发到这个还没启动的Sidecar，Sidecar还没起来，就会报Connection refused，也就说你永远拉不了镜像，但是Init Container又改了iptables，整个流程就相当于是卡住了。我们做到了这样一个隔离，这时候就不用担心业务容器里面做任何的修改会影响管控面组件的流量。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f7/f7e356aae6be303d6d16412ef057d98b.png\" /></p><p></p><p>另外因为整个虚拟机是一个轻量虚拟机，所以说我们对里面的运行操作系统是有个特别的优化的，首先内核是基于tlinux 5.4内核，同时会优化一些启动速度，比如说磁盘加密，随机数生成的这些模块的加载，同时会内置这些常用的日志采集组件，云上的采集组件，以及业务所需要的GPU驱动，还有各类文件存储的客户端组件，这些组件会按需开启，让业务可以无需去很完整加载这些组件的时候就可以达到开启的效果。另外因为整个虚拟机就只有业务这样一个容器，业务这样一个Pod，特权模式也是可以支持开启的，用户可以通过特权模式去变更内核参数。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/61462df15b445813428fdffe1c8427d0.png\" /></p><p></p><p>整个EKS的架构刚才提到了，它是一个Pod的虚拟机，做到一个虚拟机层面的完全隔离，做到Secure of VM，Pod间就再也没有干扰了，不同Pod间处在不同的虚拟机底下，Pod对特权模式的依赖，或者是内核参数的修改，就完完全全放开了。</p><p>&nbsp;</p><p>另外的话，我们也支持hostpath，虽然这个hostpath跟原生的hostpath会有一点点出入，原生的hostpath是你在同一个节点上面可以通过hostpath通讯，现在是你在同一个虚拟机节点上面通过hostpath是不能通讯，但是我们还是支持hostpath这样一个功能。</p><p>&nbsp;</p><p>除此之外，一个Pod一个虚拟机，所以说你也可以直接获取到真实的CPU了，就不会像Go Runtime或者是Java JDK里面，会根据宿主机的核数取到对应的CPU数。另外这种场景更适合多租户场景了，就像刚才提到的，kata创建了一个虚拟机是会有限制的，因为它本身在宿主机上面还有一些文件，它对物理机也有要求限制，需要物理机的内核版本是多少，还会在物理机上面去共享一些东西，这个时候通过一个Pod一个虚拟机方案，我们更适合那种多组户的场景下的完完全全隔离。</p><p>&nbsp;</p><p>除去安全性，这里还有个弹性的目的，就是只有在Pod创建的时候，我们才对应创建一个虚拟机资源。由于整个虚拟节点对接是云上的资源池，它只有在真的需要创建资源的时候，慢慢才会去创建对应的虚拟机，没有需要去预先准备固定的节点，另外每个Pod起来都是一个单独的虚拟机，计费方式也可以不一样，比如说你有些虚拟机是可以按竞价实例来计费，这些都可以支持的。另外性能跟云服务器一致，因为它本身就是一个虚拟机，并且它的流量是直通到业务容器里面的，是没有额外的转发跟损耗的。</p><p>&nbsp;</p><p>另外的话也是我们刚才提到了，我们就相当于是把 kubelet 的功能分在两个地方，但是并没有说去拆解它，它的兼容性跟原生k8s是一致的，除了一些节点相关的，比如虚拟机节点是没有IP，所以node port这种场景下，就没有节点IP可转发了。除去节点相关这种特性，其他特性都是复用原生kubelet这套逻辑，所以是兼容的，这是整个架构的优势。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb2e6300339181b1a22cea1cdaf4e138.png\" /></p><p></p><p>当然对比一下经典k8s还是有一些地方不同，比如说单Pod启动速度，因为弹性容器服务的Pod每次启动前都需要创建一台轻量虚拟机，这时候如果是冷启动，什么东西都没有，单纯从头开始创建一台虚拟机，整个启动时速度会比经典的k8s慢十几秒。</p><p>&nbsp;</p><p>我们也有些优化的措施，比如说通过保留沙箱的方式来启动，这时候会有有效地提升启动速度，虽然单Pod的启动速度会慢一些，但好处是在批量Pod启动的时候，比如说刚才提到那种从2000核到5000核这样的一种突增，这样情况下面在EKS上面你是不需要去扩容节点，因为它后面对接的就是整个云上资源池，它立马就可以批量启动这些Pod，如果你使用原生经典的&nbsp;Kubernetes&nbsp;，你就需要先去添加节点，这时候添加节点的流程是分钟级别的，并且要添加多个节点才可以去启动Pod。虽然在单Pod启动速度上面会慢，但是我们有一些优化措施来提升速度。</p><p>&nbsp;</p><p></p><h1>腾讯云弹性容器服务EKS落地实践</h1><p></p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/91/91ede2c7fd341df4a35c3639c7540478.png\" /></p><p></p><p>最后我们看一下落地实践的过程，首先最主要的实践过程就相当于是通过虚拟节点的方式，把弹性容器服务加入到经典的 Kubernetes 集群里面，比如我们就把整个可用区加到这个集群里面，就相当于是这个经典&nbsp;Kubernetes&nbsp;集群里面对接了后面整个可用区的资源池，扩容的时候就可以让业务优先使用固定节点下面的资源，直到固定节点上面没有资源池了，它再弹到虚拟节点上面来。</p><p>&nbsp;</p><p>另外我们也支持，让业务设置它只在固定节点上面保留一定的数量的副本，业务并不一定非得要等到把整个固定节点资源耗尽，它还是希望留些Buffer，但是它本身每个业务就有一些固定的量级，就相当于这些固定的数量的业务就保留在固定的节点上面。扩容的话，剩下的扩容的副本就跑到虚拟节点上面去，缩容的话，优先缩虚拟节点上的，这时候就可以充分利用固定节点上面的资源，这是我们提供的多种扩缩策略。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/43/4379c0ff3b26daa89ac84aaef9ec7f26.png\" /></p><p></p><p>另外我们也会对原生的一些组件进行一个修改，首先是控制器，因为就像刚才提到，虚拟节点本身上报健康情况，但是虚拟节点状态的不健康其实不应该引发Pod驱逐，因为每个Pod已经是独立出来了，我们是通过一种心跳的方式来维护每个Pod本身的状态。</p><p>&nbsp;</p><p>另外调度器也会做一个感知，原本调度器是感知到每个节点上面的资源，但现在整个调度器对接的是可用区资源池，它要做到可用区资源的感知，并且默认会做一个跨可用区打散的部署，充分去利用不同可用区的资源。另外刚才提到的每个Pod会占一个私有网络的IP，所以说调度器也要去感知IP资源。除此之外创建的话也可以让业务去指定，按照某些机型顺序去创建，比如说有一些可以优先AMD，有一些优先Intel来创建，并且在资源不足的时候我们是有实现了一个重调度的功能，相当于是在某个可用区上面发现资源不足的时候，我们会帮你换个可用区重试。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/250bb22608d1670d4ffbd06683f1911a.png\" /></p><p></p><p>除此之外还有另外一个问题是跟固定节点相比，固定节点上面可以先去拉镜像，但是你在虚拟节点上面每次启动都是一个Pod一个虚拟机，这个虚拟机里面是没有镜像的，启动速度肯定会更慢，对于这种拉镜像的过程，我们有几个解决方案。首先是默认会启用的方案叫做镜像盘复用，假设你这个Deployment有两个副本，Pod A跟Pod B，它们本身就会各启一个镜像盘，之后你在滚动更新的时候，比如Pod B销毁了，这时候它对应的镜像盘我们在后台保留，启动下一个Pod C的时候，我们帮你把镜像盘挂载进来，Pod C启动的时候，就发现这个盘上已经有那个镜像了，可以少去镜像拉取的过程，做到加速启动。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ba/bafbbb1d62038cf94f3394b69755dfeb.png\" /></p><p></p><p>当然这种情况只能是对于有预热的过程，如果说对于冷启动，我们还有另外一个方案就叫做镜像预缓存，就相当于是一开始你推送镜像的时候，我们就会先在云上先给你预加载，做一次拉取镜像操作，拉取完之后，就会对于那个盘做一个快照，之后会生成快照ID，你下次启动的时候就可以拿这些快照ID去创建对应的启动盘，这个启动盘就包含你刚才已经推上来的镜像，整个过程下来，启动的时候就相当于是瞬间完成镜像的拉取，会显著提升启动速度。</p><p>&nbsp;</p><p>而且结合hostpath之后会有一个很特别的效果，就是AI场景下面有些业务会把模型跟业务算法镜像分开，打两个镜像做版本管理，模型是一个镜像，业务是一个镜像，这时候启动的时候，如果模型跟业务算法并没有打在同一个镜像里，业务算法容器要怎么才能读到模型镜像里的数据呢？虽然两个镜像都已经拉到了启动盘上面了，但是overlayfs时互相看不见的，即便用了share volume这种方式，本身还是需要做一个拷贝，才可以将模型数据共享到业务容器。这时候就可以结合hostpath来做，因为模型镜像的模型数据已经拉到了对应的磁盘上面了，只要去找到那个目录，这时候做一个软链给到业务镜像，业务镜像就可以直接去这个目录上面读了，这也是虚拟节点上，需要使用到hostpath的一个场景。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bb/bb938026b8009b92e7c7a43816be3bc1.png\" /></p><p></p><p>解决了这个镜像的拉取之后，当然还是有一个问题，相较经典的&nbsp;Kubernetes&nbsp;集群，还需要去创建轻量虚拟机，我们还提出另外一个优化，就是保留沙箱的一个功能，在没有保留沙箱情况下面，你每次创建的时候都要新建一个轻量虚拟机，启动速度肯定是会比固定节点来的慢。通过保留沙箱就相当于是你第一次创建完之后，销毁之后我们帮你把这个沙箱保留，在下一阶段创建一个新的Pod时候，我们就去复用这个沙箱，你就少去了一个轻量虚拟机创建的一个动作，整个Pod启动就可以达到秒级启动了，这个就是我们在启动速度方面做的一些实践。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/abd1f0b909c7756fd78c631c988f6308.png\" /></p><p></p><p>因为弹性容器有个特点，就是即用即销毁，销毁完之后底层资源也就跟着销毁了，有时候需要去排障，需要去看日志，就会找不到，除去日志采集之外，还可以通过原生的terminationMessagePolicy，将退出日志保留在terminationMessagePolicy上面去，除此之外，我们也提供延迟销毁的能力，就是Pod退出之后保留底层资源一段时间，这时候就可以方便排障。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/73/73cf6e9bbc80f11bfde031bfb1bd1c36.png\" /></p><p></p><p>另外，就是一些Annotation的定制了，比如说固定节点上面，你想要去改DNS服务器，你想要去改内核参数，或者你想要去改运行时的一些配置，这时候你登到固定节点上面去做相应的操作就可以了，虚拟节点上面已经没有节点这个概念了，所以说我们就可以通过Annotation的方式，实现跟固定节点一样的操作过程，相当于是在固定节点上面常见的需要做一些配置，我们通过Annotation暴露给用户。在Pod启动之前，我们就会对Pod所在的宿主机进行相应的初始化，达到相应的效果。另外因为Pod本身就是虚拟机，我们更推荐用安全组的方式来实现network policy，相当于是每个Pod或者是Deployment你可以绑对应的安全组，这时候你就可以达到你想要的network policy方案。</p><p>&nbsp;</p><p></p><h1>总结</h1><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/00/00b76251db48140e60047c092cf745ae.png\" /></p><p></p><p>整体来说，今天从经典的Kubernetes集群开始讲起，经典Kubernetes集群涉及到节点管理的问题，以及节点上面混布带来的隔离性问题，之后探索了开源社区对应的解决方案，发现开源的方案在公有云上多租户场景是有很大挑战的，特别是探索kata方案后，发现它会更加适合私有云场景，还不适合公有云场景。</p><p>&nbsp;</p><p>所以我们就提出了自研的方案，我们的这个方案复用了成熟虚拟化的技术，做到完全网络的隔离、IO隔离、内核隔离，并且我们在实现过程之中，兼容了整个Kubernetes逻辑，虽然会对它做改造，但是只是把功能拆分，并没有做功能删减，所以它会完全兼容除节点相关的&nbsp;Kubernetes&nbsp;特性，兼容特权模式。</p><p>&nbsp;</p><p>当然我们这样的方案，会带来一些跟固定节点不一样的，比如说镜像的预拉取，以及虚拟机的启动这些额外的操作，所以提供一个镜像盘的复用，以及镜像预缓存的这样的功能，去对齐固定节点，并且通过保留实例的方案来提高启动速度。另外本身固定节点上面可以做的操作，我们是通过Annotation暴露给用户，所以说可以达到同样的效果。</p><p>&nbsp;</p><p>后续除了进一步优化启动速度外，我们还会在节点相关的特性上发力，做到Kubernetes节点特性的对齐。</p>",
    "publish_time": "2022-11-22 11:48:02",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "全新JavaScript框架Qwik：以独特的可恢复性方式提速网页应用",
    "url": "https://www.infoq.cn/article/Dh4JdDtegZpG58IdN8L0",
    "summary": "<p>AngularJS的创造者<a href=\"https://www.linkedin.com/in/misko-hevery-3883b1\">Misko Hevery</a>\"近期<a href=\"https://www.builder.io/blog/qwik-and-qwik-city-have-reached-beta\">宣布</a>\"了新网络框架<a href=\"https://qwik.builder.io/\">Qwik</a>\"测试版本的推出，声称无论应用程序有多大，Qwik都能够快速地构建。在多数情况下，Qwik会先下载1KB的JavaScript，在需要的时候才会懒加载或预处理程序和应用程序代码。</p><p></p><p>在一次名为<a href=\"https://www.youtube.com/watch?v=0dC11DMR3fU&amp;t=154s\">《如何从主线程中移除99%的JavaScript》</a>\"的演讲中，Hevery介绍了Qwik背后的原理。</p><p></p><p></p><blockquote>Qwik的目标很简单，确保复杂的网站也能在谷歌页面速度评分项上拿到100/100……归根究底，就是要让<a href=\"https://web.dev/interactive/\">互动时间</a>\"尽可能地缩短。&nbsp;如你所见，行业中的大多数框架都能在优化图片和CSS上做到尽善尽美，但JavaScript方面却又乏善可陈。因为这对于互联网上的每个人来说都是系统性的问题，我的意思是说，问题根源在于工具而不是开发者。&nbsp;用于优化JavaScript交付速度的工具是Qwik关注的问题。</blockquote><p></p><p></p><p>Misko将JavaScript在互动时间指标上负面的表现归因于<a href=\"https://en.wikipedia.org/wiki/Hydration_(web_development)\">水合</a>\"（Hydration）作用。水合在连接服务器的渲染时出现。服务器接收到客户端对页面的请求后，做出对应查询以填充界面，并将结果返回客户端。虽然对用户来说，服务器端的页面渲染速度通常要比客户端渲染的页面要快（如更快的<a href=\"https://web.dev/first-contentful-paint/\">首次内容绘制</a>\"），但页面却并不是立即就可交互的，客户端还需要下载并执行页面上的JavaScript脚本。</p><p>&nbsp;</p><p>在多数框架中，这种首次交付的HTML与应用程序的JavaScript协调的过程称作水合。在水<a href=\"https://en.wikipedia.org/wiki/Hydration_(web_development)\">合</a>\"过程中，Web应用程序框架将事件处理程序和DOM元素相连接，并初始化应用程序状态。水合之后用户操作会被事件处理程序捕捉，从而使页面可交互。</p><p>&nbsp;</p><p>Qwik保留了服务器端的渲染，通过在服务器上运行应用程序以避免水合。它将所有相关状态信息序列化，将页面内容和序列化的状态一起以HTML的形式发送给客户端。这些相关的状态信息包括时间监听器、内部数据结构，以及应用状态。借助序列化的状态，客户端可以<a href=\"https://qwik.builder.io/docs/think-qwik/#resumability--serialization\">接力完成服务器端没有执行完的任务</a>\"。</p><p>&nbsp;</p><p>处理交互性的JavaScript加载默认是延迟进行的，一般是直到用户实际使用交互时才启动，也就是说一个按钮的事件处理程序最晚可以在用户点击按钮时加载。这种即时的JavaScript获取加上<a href=\"https://www.infoq.com/news/2019/09/webexpo-2019-resource-hints-tips/\">预取策略</a>\"，利用浏览器的本地能力，在不影响页面交互性的前提下完成了文件的加载。</p><p>&nbsp;</p><p>在Qwik文档中有<a href=\"https://qwik.builder.io/docs/advanced/prefetching/\">详细</a>\"的介绍：</p><p>&nbsp;</p><p></p><blockquote>Qwik只会预取当前页面需要的代码，避免下载与静态组件相关的代码。最坏的情况是Qwik预取的代码量与现有框架的最佳情况相同，而在大多数情况下，Qwik所预取的代码只会比现有框架要少。&nbsp;除主线程之外的其他线程都可以做到代码预取，大多数浏览器甚至支持主线程之外的代码AST语法预分析。如果用户在预取完成之前开始交互，浏览器会自动优先交互模块于其他预取模块。&nbsp;Qwik可以将应用程序分解成部分，这些分块可以按照用户交互的概率高低顺序进行下载。</blockquote><p></p><p>&nbsp;</p><p>Qwik网站为开发者提供了教程、实例，以及学习和尝试Qwik的在线运行平台。以简单的计数器为例，由一个按钮和显示按钮点击次数的文本框组成，实现方法如下：</p><p></p><p><code lang=\"text\">import { component$, useStore } from '@builder.io/qwik';\n\nexport const App = component$(() =&gt; {\n  const store = useStore({ count: 0 });\n\n  return (\n    </code></p><div><code lang=\"text\">\n      <p>Count: {store.count}</p>\n      <p>\n        <button> store.count++}&gt;Click</button>\n      </p>\n    </code></div><code lang=\"text\">\n  );\n});</code><p></p><p></p><p>开发者可以通过Qwik的component$ API创建可恢复组件，有状态的组件可以通过useStore API显示其对某段状态的依赖。在处理程序的名字后附加$ 字符创建可恢复的事件处理程序（如前文例子中的onclick$ ）。通过这些手动添加的提示，Qwik可以将应用程序文件打包，以实现并优化JavaScript的懒加载。前文的计数器程序在服务器端渲染的HTML如下：</p><p></p><p><code lang=\"text\">\n\n  \n    \n      Tutorial\n    \n    \n      <!--qv q:id=0 q:key=AkbU84a8zes:-->\n      </code></p><div><code lang=\"text\">\n        <p>\n          Count:\n          <!--t=1-->0<!---->\n        </p>\n        <p>\n          <button>\n            Click\n          </button>\n        </p>\n      </code></div><code lang=\"text\">\n      <!--/qv-->\n    \n  \n  \n  \n  \n\n</code><p></p><p></p><p>注意，HTML文件是通过以下方式强化的。</p><p></p><p>q: 属性，如q:base，q:id，q:key 。包含特定框架信息的HTML注释，如<!--qv q:id=0 q:key=AkbU84a8zes:--> 。序列化状态，如 。用于在客户端恢复应用程序执行的Qwik脚本，如，window.qwikevents.push(\"click\") 。</p><p></p><p>Qwik的在线代码运行平台可以让开发者了解到程序代码是如何被切割打包的，还是用前面的计数器为例，客户端的打包方式如下：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e6/e65549652f12dea638a0c48591b1dc22.png\" /></p><p></p><p>如截图所示，计数器的应用程序被分成了三个脚本。当用户点击按钮时，动态下载并执行其中两个脚本（Qwik运行时间和click事件处理程序的代码）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/abf5056ddac3e46e2a3bdcdf89911926.png\" /></p><p></p><p>参考<a href=\"https://qwik.builder.io/docs/concepts/resumable/\">Qwik文档</a>\"了解具体执行情况以及代码拆分的原理。Qwik的网站给出了大量包括<a href=\"https://qwik.builder.io/tutorial/welcome/overview/\">教程</a>\"、<a href=\"https://qwik.builder.io/examples/introduction/hello-world/\">示例</a>\"，以及演示在内的信息，还有一个可互动的<a href=\"https://qwik.builder.io/playground/\">在线代码运行</a>\"平台。Qwik社区中同样也有一个<a href=\"https://qwik-storefront.vendure.io/\">非常简单的电子商务示例</a>\"，一般对电子商务的厂商来说，页面加载速度提高收入也会增加。</p><p></p><p>Qwik团队目前由AngularJS的创造者Miško Hevery、基于Go语言Web架构Gin的创造者Manu Almeida、<a href=\"https://stenciljs.com/docs/introduction\">Web组件编译器Stencil</a>\"的创造者Adam Bradley组成。</p><p>&nbsp;</p><p>目前，Qwik已推出<a href=\"https://www.builder.io/blog/qwik-and-qwik-city-have-reached-beta\">测试版</a>\"，且采用MIT许可开源，欢迎各位在遵循Qwik<a href=\"https://github.com/BuilderIO/qwik/blob/main/CODE_OF_CONDUCT.md\">行为准则</a>\"的前提下贡献代码。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2022/10/qwik-fast-web-app-resumability/\">New Qwik JavaScript Framework Seeks Faster Web Apps with Unique Approach: Resumability</a>\"</p>",
    "publish_time": "2022-11-22 11:48:04",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]