[
  {
    "title": "Docker Compose Watch正式可用，告别标签切换和绑定加载",
    "url": "https://www.infoq.cn/article/bZO8F3dQBdzKUAaUTpvG",
    "summary": "<p><a href=\"http://www.docker.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Docker</a>\" 宣布 <a href=\"https://www.docker.com/blog/announcing-docker-compose-watch-ga-release/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Docker Compose Watch</a>\" 普遍可用，这是一个旨在提高应用程序开发效率的工具，让开发人员能够在编码时更容易保持专注。</p><p></p><p>对于传统的容器化应用程序开发，不仅需要在Web浏览器中使用Alt+Tab来切换标签页和点击刷新按钮，还需要更多的步骤。即使使用了缓存，重新构建镜像、重新创建容器和处理启动/停止等过程都会让开发者分心。Docker Compose Watch 旨在解决这一不便，专注于解决开发中常见的痛点。</p><p></p><p>Docker Compose Watch 允许开发人员将其代码更改同步到正在执行代码的容器中，实现了类似 React 或 NextJS 的实时重新加载。它提供了对同步本地文件更改的精细化控制，可以不保留为测试目的而进行的更改。它引入了 \"<a href=\"https://www.codeepsilon.com/hot-reload-is-it-a-replacement-for-testing-and-debugging/#0?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">热重载</a>\"\"，允许在编写代码时无缝快速更新应用程序，而不会丢失应用程序的现有状态。</p><p></p><p>实现热重载的一个常见传统解决方案是通过绑定挂载在本地系统和容器之间镜像文件更改，但这需要使用一些变通方法，因为 <a href=\"https://www.docker.com/products/docker-desktop/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Docker Desktop</a>\" 中的绑定挂载与 Linux 上的 <a href=\"https://docs.docker.com/engine/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Docker Engine</a>\" 不同。确保在 Docker Desktop 的虚拟机（VM）和本地宿主之间进行无缝高效的文件共享，同时保持权限和文件通知，是一个重大挑战。</p><p></p><p>Docker Compose Watch 可以在启动时自动构建并启动所有需要的服务，无需附加到运行中的 Compose 项目。只需一个\"docker compose watch\"命令，这极大简化了开发过程，让开发人员能够专注在更重要的编码上。</p><p></p><p><a href=\"https://twitter.com/maciekpankanin/status/1704951917418270735?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Maciej Pankanin</a>\" 在 X（ Twitter） 上说，为常见服务如 nginx 添加重启命令将非常有用，开发团队已经注意到了这一反馈。<a href=\"https://twitter.com/mikesir87/status/1651624357146370052?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Michael Irwin</a>\" 在 X 上进行了积极评价，说他 \"不再需要挂载源代码\"、\"一切似乎更加迅捷\"。<a href=\"https://twitter.com/BretFisher/status/1652453188040105984?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">Bret Fisher</a>\" 在之前的 alpha 版本发布的视频中说道：</p><p></p><p></p><blockquote>\"这解决了多年来在 Mac 上一直困扰我们的性能问题\"</blockquote><p></p><p></p><p>自从在 Docker Desktop 4.18 中捆绑 Compose v2.17 的 alpha 版本发布以来，Docker Compose Watch 已经得到了大幅改进，比如通过批处理 Docker API 调用来提高速度、使用防抖机制确保重建不会太密集、添加过滤器来忽略常见代码编辑器和集成开发环境所生成的临时文件。</p><p></p><p>Docker Compose Watch is now <a href=\"https://docs.docker.com/compose/install/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">generally available</a>\" and installable standalone, bundled into Docker Desktop 4.24, and as a plugin for Docker Engine.</p><p></p><p>Docker Compose Watch 现已 <a href=\"https://docs.docker.com/compose/install/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTg2MzA4NjEsImZpbGVHVUlEIjoiNWJxbmRCMWJWT3N6V29BeSIsImlhdCI6MTY5ODYzMDU2MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.2a5eZnfSQaCCZdfuvMhK_htmEdmf28CcNExeZq86mYo\">普遍可用</a>\"，可以独立安装，可以捆绑到 Docker Desktop 4.24 中，也可以作为 Docker Engine 的插件。</p><p></p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/10/docker-compose-watch/\">https://www.infoq.com/news/2023/10/docker-compose-watch/</a>\"</p>",
    "publish_time": "2023-11-01 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "15年技术沉淀，起底阿里核心搜索引擎 Havenask 演进之路",
    "url": "https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P",
    "summary": "<p></p><h2>阿里开源搜索引擎&nbsp;Havenask&nbsp;的技术演进</h2><p></p><p></p><p>我们正处于信息爆炸式增长的时代，如何在信息海洋里迅速定位到目标信息成为人们关心的问题。搜索引擎作为互联网和应用的关键入口，向来是兵家必争之地。</p><p></p><p>然而在人们简单的搜索行为背后，对搜索引擎技术实际有诸多挑战：以电商场景为例，当遇到双11等大促活动时，百万级&nbsp;QPS&nbsp;的高并发访问，对千亿级商品&nbsp;&amp;&nbsp;订单数据、保单&nbsp;&amp;&nbsp;物流类数据时效性要求极高，那么搜索引擎该如何做到毫秒级时效？还有为了更准确理解人们的搜索意图，对搜索算法的要求越来越高，搜索引擎该如何做到算法分钟级迭代？这些都是技术上需要直面的挑战。</p><p></p><p>近年来，随着大数据技术、深度学习等&nbsp;AI&nbsp;技术的发展，搜索引擎能够更智能地帮助人们快速、准确地获取信息，我们对信息的处理能力也随之逐步提高。</p><p></p><p>阿里自研大规模分布式搜索引擎&nbsp;Havenask&nbsp;便是集大成者，基于阿里搜索十多年来的技术沉淀，Havenask&nbsp;目前广泛应用于阿里巴巴和蚂蚁集团内众多业务，如淘宝搜索和推荐、&nbsp;蚂蚁人脸支付、优酷视频搜索、阿里妈妈广告检索等。Havenask&nbsp;支持算法高效快速迭代，内置性能优异的向量检索能力；做到毫秒级查询性能，并拥有稳定性保障&nbsp;；支持单应用实例千亿级别数据，确保百万&nbsp;TPS&nbsp;高时效性。</p><p></p><p>2022&nbsp;年&nbsp;12&nbsp;月，阿里将&nbsp;Havenask&nbsp;开源，在几个月时间里&nbsp;Star&nbsp;数已超过&nbsp;1000+。为何&nbsp;Havenask&nbsp;有这样优异的表现，在短时间内获得众多开发者的喜爱？下面我们从&nbsp;Havenask&nbsp;的技术演进谈起，让大家更加深入了解&nbsp;Havenask&nbsp;以及未来更多可能性。</p><p></p><p>传送门：++https://github.com/alibaba/havenask++</p><p></p><h2>01&nbsp;Havenask&nbsp;技术演进之路</h2><p></p><p>回顾&nbsp;Havenask&nbsp;从内部自研技术走向成熟，这一路走来可分为以下阶段：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/85/85f3025dbedec2a27678b2160ae2f23d.png\" /></p><p></p><p>第一阶段：1999&nbsp;年~2008&nbsp;年，以解决各业务部门的搜索需求为主</p><p></p><p>阿里搜索技术最早可追溯&nbsp;1999&nbsp;年，起源于雅虎搜索技术，基于&nbsp;Apache&nbsp;Module&nbsp;的单机版搜索引擎，支撑淘宝、B2B&nbsp;等子公司的搜索业务需求。</p><p></p><p>第二阶段：2009&nbsp;年~2011&nbsp;年，重构搜索系统，开启自研大规模分布式高性能搜索引擎时代</p><p></p><p>自&nbsp;2008&nbsp;年起，开始构建阿里统一的搜索系统，内部代号为“iSearch”，它代表完全由阿里自研的搜索技术全新启航。iSearch&nbsp;迅速迭代&nbsp;iSearch3.0、iSearch3.2……2009&nbsp;年演进到&nbsp;iSearch4.5&nbsp;版本，也就是&nbsp;HA3（Havenask）最早的雏形。</p><p></p><p>2009&nbsp;年，Havenask&nbsp;开始逐步统一各子公司版本，去除&nbsp;Apache&nbsp;Module。2011&nbsp;年，彻底完成搜索系统的重构，HA3（Havenask）全部替代老的雅虎搜索系统，开始极致性能时代。</p><p></p><p>第三阶段：2012&nbsp;年~2018&nbsp;年，完成阿里内部搜索系统的“大统一”，进入快速迭代时代</p><p></p><p>2013年，HA3（Havenask）完成阿里集团各个业务搜索系统的“大统一”，不仅版本再次合并，还将&nbsp;B2B、淘宝等搜索团队统一整合，以产品化、规模化的方式支撑起整个集团的搜索业务。</p><p></p><p>2018&nbsp;年，随着深度学习技术的广泛应用，同时迎来信息流推荐机遇，HA3（Havenask）快速迭代，逐步形成一套以搜索引擎、在线推理引擎等为主的&nbsp;AI&nbsp;工程技术体系“AI·OS”。（OS”代表“Online&nbsp;Serving”&nbsp;）</p><p></p><p>第四阶段：2018&nbsp;年~至今，对外开源，技术普惠</p><p></p><p>2022&nbsp;年，阿里将搜索引擎&nbsp;Havenask&nbsp;开源，为更多用户提供更高性能、更低成本、更便捷易用的搜索服务。</p><p></p><p>总的来说，Havenask&nbsp;的发展是遵循先解决内部业务应用需求，再从核心业务延伸到其他业务，随着技术发展潮流不断向前演变，从单一的搜索引擎到大数据深度学习在线服务体系&nbsp;AI·OS&nbsp;的重要组成部分，打造成统一平台提供更强大的能力支撑，继而逐步开源对外，普惠开发者，这和阿里其他技术产品的发展思路是一脉相传的。</p><p></p><h2>02&nbsp;Haveansk&nbsp;架构优势</h2><p></p><p>从定位来看，Havenask&nbsp;作为阿里巴巴自主研发的大规模分布式搜索引擎，支撑起淘宝、天猫、菜鸟、优酷阿里整体的搜索业务，并扛得住双&nbsp;11&nbsp;大促活动。这背后，离不开底层架构设计，让&nbsp;Havenask&nbsp;有了坚实的技术基底。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/33/33f354cadf938d9f305d845ffdf35a82.png\" /></p><p></p><p>从架构来看，Havenask&nbsp;由四个核心模块组成：</p><p></p><p>索引系统**（Build**&nbsp;Service）。通常搜索引擎需要对原始数据构建索引，才能在提供服务时实现高性能。这部分在&nbsp;Havenask&nbsp;是支持全量、增量、实时流的复杂分布式流计算系统。</p><p></p><p>在线集群**（Havenask**&nbsp;**Runtime）****。**在线系统支持不同的数据规模分列查询，不同的查询并发做多副本。在系统里设计有类似于大脑的复杂角色，可以自动做查询处理、调度查询节点、数据节点等。如果出现机器坏了的情况，在线系统可自动识别这些情况，来保证系统的高可用。</p><p></p><p>消息中间件（Swift）。消息中间件用于实时数据传递，处理后的文档传递，是&nbsp;Havenask&nbsp;实现毫秒级时效性，支撑海量数据实时更新的基石。消息中间件&nbsp;Swift&nbsp;不仅可以用在&nbsp;Havenask&nbsp;系统中，也可以单独部署使用，与其他开源中间相比具有明显的性能和成本优势。</p><p></p><p>管控系统（Hape）。为了方便开发人员的日常运维，Havenask&nbsp;对管控运维的&nbsp;API&nbsp;进行了封装，提供方便实用的运维工具&nbsp;Hape&nbsp;，使用它开发人员可以方便的对表和集群进行管理。</p><p></p><p>据阿里巴巴智能引擎事业部云服务负责人、Havenask&nbsp;开源项目负责人郭瑞杰博士介绍，在架构设计上，Havenask&nbsp;更具备适合工业级业务场景的特性：</p><p></p><p>1、通过灵活稳定的扩展方式支持业务多样化需求，轻松应对数据规模和流量规模的快速增长；</p><p></p><p>2、通过领先的实时索引技术，提供性能出色的亚秒级实时搜索能力，通过对实时索引的不断自动整理优化，保证搜索性能持续优异；</p><p></p><p>3、传统倒排索引技术和&nbsp;AI&nbsp;时代普遍应用的向量检索技术深度结合，端到端极致性能优化，支持千亿级别文档或高维向量的极低延迟计算。</p><p></p><p>人们进行商品搜索时，由于每个人有不同的喜好，搜索引擎需实现个性化和智能化，以准确召回商品。当用户开始进行搜索时，往往是用关键词或一段自然语言的描述，搜索引擎先采用&nbsp;NLP&nbsp;技术理解和拆分成关键词，再根据关键词的语义相关性，采用向量等多路召回方式，返回有可能是用户想要找的商品信息，再对商品做粗排，粗排后收敛到集合里，再做精排，这个过程中&nbsp;Havenask&nbsp;使用了大量机器学习算法进行优化，以实现较好的用户体验。</p><p></p><p>这对搜索引擎有较高的性能要求，Havenask&nbsp;利用前置化思想，并发完成多路召回，实现非常小的延迟效果。另外在算法上，Havenask&nbsp;支持离线计算转在线计算、在线计算转离线计算做优化，还支持模型的实时更新以保证在离线的一致性。如此一来，算法工程师可以用更复杂的召回策略来做&nbsp;A/B&nbsp;测试验证效果，如果效果可以的话，可以实现分钟级上线。</p><p></p><p>在拍照搜图场景中，以淘宝拍照购物“拍立淘”为例，用户通过手机拍摄实物或通过相册照片搜索，就能搜索同款或相似商品。&nbsp;Havenask&nbsp;利用向量进行图片搜索，完成向量索引存储并将向量化后的图片与向量索引比对召回，实现高精度图片搜索。上述能力得益于&nbsp;Havenask&nbsp;和达摩院向量库&nbsp;Proxima&nbsp;深度结合，并进行端到端能力优化，支持百亿甚至千亿级别的高维度向量的低延迟计算。</p><p></p><p>总体来看，Havenask&nbsp;区别于其他产品的特点主要体现在两大场景中：一是大数据检索场景，实现亚秒级的时效性和极致的性能优化，达到较高的性价比。二是在&nbsp;AI&nbsp;场景上，Havenask&nbsp;实现异步高并发、超低召回延迟，提供在离线一致性保障机制，以及高性能高维度向量计算能力。</p><p></p><p>即使在双11特殊场景里，数据更新量突然爆增至十倍、百倍，Havenask&nbsp;仍能保证时效性在亚秒级。在查询上，单集群到近百万&nbsp;QPS&nbsp;时，Havenask&nbsp;确保查询延迟毫秒级别。另外，Havenask&nbsp;足够弹性，针对双11的流量急速变化，集群一键平滑扩缩容，变更对业务0影响，灵活应对流量峰谷。</p><p></p><h2>03&nbsp;Havenask&nbsp;开源开放，普惠开发者</h2><p></p><p>Havenask&nbsp;起源于阿里内部搜索业务需求，如今作为核心搜索引擎在阿里内部广泛应用，那么团队为什么选择将&nbsp;Havenask&nbsp;对外开源？</p><p></p><p>郭瑞杰表示，Havenask&nbsp;围绕着电商场景演化出来，在阿里核心头部业务、中台业务等均广泛使用。希望通过开源的方式让广大开发者参与进来，让&nbsp;Havenask&nbsp;迭代更快走得更远。以开源&nbsp;Elasticsearch&nbsp;为例，在十年时间中，Elasticsearch&nbsp;因为开源发展迅速，Havenask&nbsp;也期待通过开源吸引更多开发者参与进来，一起联合共创。</p><p></p><p>再者，近年来国际形势变幻莫测，人们对国产化替代诉求与日俱增。期望自主研发的&nbsp;Havenask&nbsp;能帮助一些企业实现国产化替代，让更多开发者和企业以更低的成本实现业务创新。</p><p></p><p>不仅如此，Havenask&nbsp;还提供商业版本来支持企业实现搜索场景、推荐场景、大模型应用场景创新。</p><p></p><p>“&nbsp;Havenask&nbsp;自开源后，在尚未开展过多活动的情况下，Star&nbsp;数快速突破&nbsp;1000，对我们来说还挺意外的，这也让我们坚定了后续持续建设开源&nbsp;Havenask&nbsp;的信心。”郭瑞杰说。</p><p></p><p>Havenask&nbsp;作为&nbsp;AI·OS&nbsp;体系的重要部分，沉淀了阿里&nbsp;10&nbsp;多年的搜索技术，整体系统庞大，采取逐步开源的形式对外开放，从2022年首发时的单机预览版，到如今刚刚发布的的分布式正式版，已经完成了&nbsp;Havenask&nbsp;几乎全部核心代码的开源。</p><p></p><p>在2023年9月份最新发布的&nbsp;Havenask&nbsp;1.0.0&nbsp;分布式版本中，支持读写分离与读写统一两种部署架构，可以分别满足开发者不同业务场景的需求，同时分布式版本提供基于机器资源池的集群自动化管理能力、动态表管理能力，降低开发者集群运维的成本；并且集成了自研的消息中间件，支持更完善的实时数据更新能力。</p><p></p><p>据郭瑞杰透露，在后续的版本中，&nbsp;Havenask&nbsp;会更聚焦开发者的真实使用场景，特别是大数据检索和智能检索等领域不断构建&nbsp;Havenask&nbsp;的开源生态，让&nbsp;Havenask&nbsp;更加广泛的应用在更多业务中，解决开发者面临的性能、成本、稳定性等核心问题。</p><p></p><p>与此同时，Havenask&nbsp;还开源了&nbsp;Havenask-federation（简称Fed）项目（<a href=\"https://github.com/alibaba/havenask-federation\">https://github.com/alibaba/havenask-federation</a>\"），在&nbsp;Havenask&nbsp;和&nbsp;Elasticsearch&nbsp;之间架起一条桥梁，方便&nbsp;Elasticsearch&nbsp;开源生态用户，快速迁移和扩展，实现优势互补。</p><p></p><h2>04&nbsp;Next&nbsp;Big&nbsp;Thing</h2><p></p><p>最近技术人话题离不开热门的&nbsp;ChatGPT，ChatGPT&nbsp;一经发布，大家认为被最早被颠覆的是搜索引擎。传统搜索引擎&nbsp;+&nbsp;ChatGPT&nbsp;将产生巨大化学反应，或将改写搜索引擎的产品形态。ChatGPT&nbsp;能更好地理解人们的搜索意图，为用户提供汇总答案，提供更准确的搜索结果，还能以自然语言来搜索，让搜索体验有质的提升。</p><p></p><p>郭瑞杰表示，有了&nbsp;ChatGPT&nbsp;能力加持，不仅在&nbsp;to&nbsp;C&nbsp;端搜索引擎发生巨变，在&nbsp;to&nbsp;B&nbsp;端也将催化诞生颠覆性的产品形态。其中&nbsp;to&nbsp;B&nbsp;端和&nbsp;to&nbsp;C&nbsp;端搜索引擎稍有差异，to&nbsp;B&nbsp;搜索引擎是面向企业，主要搜企业数据，而不是搜全网数据，更多的是围绕企业数据来提供更智能和更准确的答案。</p><p></p><p>针对不同行业的用户想基于大模型能力完成业务创新，Havenask&nbsp;除了在底层传统搜索引擎技术上提供帮助，也正在做如下两个方面的能力增强，并持续开源：一是向量检索。在大模型时代下，向量检索技术是大模型应用创新的基石，我们正在构建新的向量检索引擎&nbsp;VectorStore，预计性能大幅超越&nbsp;Milvus，期望能提供给开发者更高性能、更低成本的向量检索方案；二是大模型推理加速。将全面支持各种&nbsp;LLM（qwen、chatglm、baichuan、xverse、interlm、llama、falcon、mpt、starcoder&nbsp;等）的推理加速，支持量化、多机多卡分布式、上下文&nbsp;cache&nbsp;等多种特性，预计性能超越&nbsp;vllm&nbsp;15%，期望给开发者提供更低成本的大模型推理服务。</p><p></p><p>现在，我们看到阿里已先行一步：在&nbsp;2023&nbsp;阿里云峰会上，正式推出大语言模型“通义千问”，并宣布阿里所有产品未来将接入“通义千问”，进行全面改造。例如在网购场景，用户如果想开生日&nbsp;party，通义千问可以帮助生成生日活动方案和购物清单。</p><p></p><p>期待后续&nbsp;Havenask&nbsp;与“通义千问”联合创新，为人们带来更好地搜索体验，帮助企业和开发者量身定做适合业务发展的智能搜索服务，促进业务飞速增长，共享科技红利。</p><p></p><p>此外，基于&nbsp;Haveansk&nbsp;与“通义千问”打造的AI搜索产品——OpenSearch&nbsp;LLM&nbsp;智能问答版，也已在阿里云上为企业级开发者提供全托管、免运维的一站式对话式搜索服务，欢迎企业级开发者们试用。</p><p></p><p>心动不如行动，欢迎立即体验：</p><p></p><p>Havenask&nbsp;开源项目地址：<a href=\"https://github.com/alibaba/havenask\">https://github.com/alibaba/havenask</a>\"</p><p></p><p>Havenask-federation&nbsp;开源项目地址：<a href=\"https://github.com/alibaba/havenask-federation\">https://github.com/alibaba/havenask-federation</a>\"</p><p></p><p>OpenSearch&nbsp;LLM&nbsp;智能问答版：<a href=\"https://www.aliyun.com/activity/bigdata/opensearch/llmsearch?spm=5176.7946605.J_4098459070.4.15b38651FlNqqw\">https://www.aliyun.com/activity/bigdata/opensearch/llmsearch?spm=5176.7946605.J_4098459070.4.15b38651FlNqqw</a>\"</p><p></p><p>钉钉扫码加入Havenask开源官方技术交流群：</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/78/78c5cfa61c64a55cdeb0655ac7eb2849.png\" /></p><p></p><p>近期活动预告：</p><p></p><p>2023年11月1日13:10-13:25，杭州云栖大会&nbsp;B3-4&nbsp;馆，Havenask&nbsp;开源正式版发布演讲</p><p></p><p>2023年11月1日14:40-15:10，杭州云栖大会&nbsp;C&nbsp;区舞台，Havenask&nbsp;开源细节与案例分享</p><p></p><p>欢迎开发者前往会场参加，或通过线上渠道收看关注</p><p></p><p>嘉宾介绍：郭瑞杰博士，2008年加入阿里巴巴，深耕阿里搜索领域开发十余年，先后负责&nbsp;iSearch4.5、问天2、问天3等多个搜索架构及产品的设计与开发工作，现任阿里巴巴智能引擎事业部云服务负责人，阿里云计算平台事业部搜索推荐云服务负责人，Havenask&nbsp;开源项目负责人。</p>",
    "publish_time": "2023-11-01 09:04:24",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "国货李宁的数字化新故事｜《行知数字中国》完整版",
    "url": "https://www.infoq.cn/article/t1ahBzGmUMalp3HDaS9I",
    "summary": "<p>随着行业红利期的结束，在当下的数字化时代，企业要想持续茁壮成长，必须以更为精细和高效的方式经营。</p>\n<p>该视频为《行知数字中国》第二季第四期的完整版。本期我们走进李宁，我们将看到，李宁如何运用数字技术提高门店运营和库存管理的效率、改善产品研发的生命周期，每一双鞋诞生的背后都有哪些科技力量？</p>\n<p>相关文章延展阅读：<a href=\"https://www.infoq.cn/article/GcdCeRTEHlT9tUYDN55A\">国货李宁的新数字化故事：如何利用技术做运动产品的研发？</a></p>\n<p>如果对《行知数字中国》栏目或者更多企业数字化转型案例感兴趣，欢迎关注「<strong>InfoQ数字化经纬</strong>」公众号，我们将持续为您推送更多、更优质的数字化案例内容和线上线下活动。</p>",
    "publish_time": "2023-11-01 10:33:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "这家从集成商向云转型的企业，一直把数据安全视为重中之重",
    "url": "https://www.infoq.cn/article/4AZOVSkMAFIeSkwFQO61",
    "summary": "<p>随着信息技术的飞速发展，数据已经成为了企业、组织和个人最重要的资产之一。数据安全问题也变得越来越重要，云计算崛起后，云上数据安全更是成为业界关注的重点。数据安全之所以如此重要，主要体现在以下几个方面：</p><p>&nbsp;</p><p>首先，数据安全可以保护企业和个人的隐私。在日常生活中，我们经常会遇到诸如身份信息、银行账户、密码等敏感信息的泄露问题。一旦这些信息落入不法分子手中，不仅会对个人和企业造成经济损失，还可能涉及法律风险。因此，保障数据安全是维护个人和企业声誉的必要条件。</p><p>&nbsp;</p><p>其次，数据安全可以提高企业的竞争力。在当今这个数字化的时代，数据已经成为了一种有价值的资产。如果一个企业能够保护好自己的数据资产，就可以更好地利用这些数据来分析市场趋势、优化业务流程、提高生产效率等，从而在激烈的市场竞争中占据优势。如果数据安全得不到保障，企业的业务运营将面临极大的风险和挑战。</p><p>&nbsp;</p><p>此外，数据安全还涉及到合规性的问题。许多国家和地区都制定了相关的法律法规和规范标准，要求企业和组织必须遵守数据保护和隐私法规。如果一个企业因为违反了这些法规而遭受罚款、法律诉讼等后果，将对其声誉和业务产生极大的负面影响。因此，保障数据安全也是企业遵守法规的必要条件。</p><p>&nbsp;</p><p>作为一家从集成商转向云MSP的公司，华讯网络一直把数据安全放在重要位置上。华讯网络云智能事业部总经理沈佳伟在接受媒体采访时表示，“在华讯从事云业务之前，安全业务应该已经开展了小十来年，我们是先有安全再有云。我们一开始就比较注重构建云上的安全能力。”</p><p>&nbsp;</p><p>作为大中华区首家获得亚马逊云科技安全能力认证和第1级托管式安全服务提供商（MSSP）的合作伙伴，华讯网络认为，“亚马逊云科技是华讯在云的领路人。”在数据安全领域，华讯网络与亚马逊云科技的合作也非常紧密。</p><p>&nbsp;</p><p>华讯利用亚马逊云科技提供开箱即用的SIEM（Security Information and Event Management安全信息和事件管理）on OpenSearch、日志通、敏感数据保护等解决方案，结合自身对企业安全业务的的深入洞察，快速整合产品服务，为企业提供定制化、精细化的服务，帮助企业实现数据生命周期的安全防护。</p><p>&nbsp;</p><p>第一，基于亚马逊云科技SIEM&nbsp;on OpenSearch解决方案，帮助用户在安全事件响应方面定制相对聚焦的场景服务。如果用户关注应用安全，帮助用户利用特定的分析模型，以查看具体IP扫描特性；第二，基于亚马逊云科技日志通解决方案，助力用户构建一个完整的云原生安全大数据平台，统一收集应用、容器、系统、云平台等日志，实现多来源安全事件集中告警，以保障业务的安全合规；第三，基于亚马逊云科技敏感数据保护解决方案，围绕数据的安全自动化运维，将敏感数据目录与云配置管理数据库( Configuration Management Database，CMDB)，以及安全自动配置等进行结合。</p>",
    "publish_time": "2023-11-01 11:00:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士，确认担任 QCon LLM 时代的性能优化专题出品人",
    "url": "https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士将担任「<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen\">LLM 时代的性能优化</a>\"」的专题出品人。在此次专题中，你将了解到 LLM 时代的性能分析在 CPU 和 GPU 平台上，在不同计算环境下的性能表现。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen\">周经森（Kingsum Chow）</a>\"，计算机软硬件优化首席科学家、高级首席工程师。曾就职于美国英特尔公司和中国阿里巴巴集团，2023 年加入浙江大学软件学院（宁波）。二十年来与十余家世界 500 强高科技企业合作，共同推动了世界软硬件性能优化技术的发展。曾作为项目总监主持备受瞩目的云计算蓝图项目（IntelCloudBlueprint）。该项目由英特尔和甲骨文的首席执行官于 2015 年共同宣布，吸引了超过 4 万名开发者的参与，为云计算行业绘制了全新的技术蓝图，对行业发展产生了深远影响。</p><p></p><p>自 2016 年加入阿里巴巴，为中国的性能优化技术发展做出了巨大贡献。2018 年，其作为唯一一名加入 Java 全球管理组织 JavaCommunityProcess（JCP）最高执行委员会 JCP-EC 的中国企业（阿里巴巴）代表，参与制定了 Java 的全球标准。</p><p></p><p>周博士在 CPU 利用率报告不准确（数据普遍误解）方面发表的研究，引起了业界和学术界的广泛关注。周博士拥有超过 30 年的软硬件协同优化的工业实践经验，培养了大批优秀的系统性能优化人才。至今已获授权中国专利 11 项，美国专利 24 项，发表学术论文 127 篇，在过去 6 年的 QCon 中国大会上发表 2 场主题演讲，出品 2 场软件系统性能优化主题讲座。</p><p></p><p>相信周经森（Kingsum Chow）博士的到来，可以帮助提升此专题的质量，让你学习到， 通过 LLM 在不同计算环境下的性能表现，找到的最佳应用策略和优化方法，这为 LLM 的应用和发展提供了更多的可能性。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart\">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的大前端技术</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg\" /></p><p></p>",
    "publish_time": "2023-11-01 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数智赋能云课堂第 1 期·企业数字化战略规划",
    "url": "https://www.infoq.cn/article/zg4SomcKv0DBkqglWKGo",
    "summary": "<p><img alt=\"unpreview\" src=\"https://static001.infoq.cn/resource/image/f1/61/f19aba58cc0c032456f2f90099f92561.jpg\" /></p>",
    "publish_time": "2023-11-01 14:06:26",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁SOFA Stack融合大模型发布升级版 助力机构产研效能提升30%",
    "url": "https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M",
    "summary": "<p>11月1日，在云栖大会上，蚂蚁集团正式发布CodeFuse全面加持的SOFAStack5.0升级版本，向企业提供全方位研发运维智能助手相关能力。这是继蚂蚁集团在外滩大会发布代码大模型CodeFuse之后，首次公布面向行业的商业化产品进展。</p><p>&nbsp;</p><p>“大模型将为研发效能带来颠覆性机遇。”蚂蚁集团数字科技事业群产品总监马振雄在发布会上指出。</p><p>&nbsp;</p><p>记者了解到，目前CodeFuse已经与SOFA产品线全面融合，涵盖设计、研发、测试、运维等领域，形成从领域建模到智能运维的端到端Copilot产品解决方案，预计将为企业产研效能提升30%。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/5c/5cd0d1b716d80689035335361a75486b.png\" /></p><p></p><p>具体而言，客户在使用SOFAStack时，相当于为企业开发者配备专属智能副驾驶，和机器人“辅助设计”、“结对编程”、“运维助手”，通过人机交互助手提升日常代码研发、测试、运维过程中的效率和质量。对企业而言，引入智能副驾驶可以显著提升人效质量，降低总体成本。</p><p>&nbsp;</p><p>此外，SOFAStack针对Codefuse大模型提供了多任务微调和高性能推理能力，结合企业专有数据构造更懂客户业务的智能副驾驶。而随着CodeFuse在产品线中不断深度融合，SOFAStack将为企业打造新一代AI云原生PaaS平台，使其在开发运维、数据分析、应用治理、绿色计算方面取得更智能的能力，可以加速响应业务创新和价值交付。</p><p>&nbsp;</p><p>针对当下企业应用上云「更异构、更智能、更经济」的三大需求趋势，马振雄表示，SOFAStack提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。这些解决方案可以满足不同行业和企业的需求，并为企业提供更加灵活和高效的技术支持。例如，针对行业进入多云时代，边缘资源调配、云上云下应用开发等统一管理挑战，其拳头产品MESH升级架构，从原来的经典Sidecar架构开始演变为Node架构，同步进行了性能、服务治理、业务可观测能力等全方位优化。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/7c/7cd485b0811e68275758bb2d760a382d.png\" /></p><p></p><p>Forrester报告曾分析指出，以云原生为关键能力的下一代云平台， 不仅可以基于全栈云原生架构灵活适应市场变化，而且可以通过全云开发实践帮助企业在云上快速验证创新思路，还能借助云平台的各类自动化能力降本增效强化韧性。</p><p>&nbsp;</p><p>“服务网格降低了我们的上云门槛。如果做云原生改造，系统的所有代码都要重写一遍，大概需要20&nbsp;个人投入一年时间；使用网格（Mesh）只要&nbsp;5&nbsp;个人两三个月就能上云。”传统金融机构信息科技架构规划负责人在Forrester调研时表示。根据报告测算，三年内有&nbsp;10&nbsp;个单体应用不需要经过云原生改造，即可直接上云后统一治理，总体效率提升为企业带来941万元的收益。</p><p>&nbsp;</p><p>据悉，SOFAStack是国内部署云原生技术最广泛的平台之一，基于支付宝、蚂蚁集团各项业务需求进行研发迭代，并服务于超100家银行迈向云原生转型，已经构建了完整金融级的云原生PaaS解决方案。</p>",
    "publish_time": "2023-11-01 14:14:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "从互联网到云计算再到 AI 原生，百度智能云数据库的演进",
    "url": "https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l",
    "summary": "<p>作为计算机系统的三大核心基础软件之一，数据库技术的发展一直备受关注。随着云计算技术的发展，能够适合更大规模业务场景，有着高可用性、可扩展性和低成本等优势的云原生和分布式数据库逐渐成为主流。</p><p></p><p>同时，AI 技术不断向前发展，尤其是 OpenAI 掀起的这场 AI 变革，使得 AI 与云计算更紧密地结合成为了可能。云原生为数据库面向更大范围的智能化应用提供了基础条件，AI 成为云原生数据库持续演进的牵引力。</p><p></p><p>为了应对大数据和 AI 应用在行业中的挑战，<a href=\"https://www.infoq.cn/news/kqPbdlvF3Jp55PodQLAo\">百度智能云</a>\"在 2020 年率先提出“云智一体”战略，以“云计算为基础”支撑产业数字化转型，以“人工智能为引擎”深入产业生产的关键场景，为企业的数字化转型和智能化升级提供新型支持。在云智一体战略的指导下，<a href=\"https://www.infoq.cn/article/o8abj2wff5yLfGWuB0E1\">百度智能云</a>\"在数据库领域不断创新，基于百度集团各项业务的多年磨炼，对外推出了云原生数据库 GaiaDB 、云数据库 GaiaDB-X、数据传输服务 DTS 等产品和解决方案。那这些产品和相关方案，都有哪些不一样的地方，又有哪些落地实践？比如：</p><p>在互联网、云计算，以及向 AI 的演进过程中，百度智能云数据库团队是如何进行技术升级，做到支持各类业务场景，满足海量数据规模，业务要求越来越苛刻的场景的。在 AI 时代，它的最新成果和规划又有哪些？相比其他云原生数据库， GaiaDB 是如何诞生于百度集团的内部业务，其产品理念有什么独特的地方，在哪些技术上面形成了优势，可以帮助用户解决什么样的挑战？在金融行业的国产化进程，GaiaDB-X 如何承载核心业务系统，满足金融行业对基础设施的高要求，在金融客户中的成功落地场景又有哪些？完善的数据库服务不止有数据库产品本身。数据库的迁移、同步、集成同样重要。这些工作和数据库本身一样，同样是复杂又至关重要的。百度智能云的数据传输服务 DTS 如何将繁琐复杂的这类业务变得可靠简单，在业务实践中帮助头部客户成功上云？</p><p></p><p>为了帮助大家解决这些问题，<a href=\"https://www.infoq.cn/article/ACXL3WviaTtr2U0v5NlB\">百度智能云</a>\"团队特推出《百度智能云数据库》系列云智公开课。前四期课程便将围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开。从 11 月 15 日起，每周三都将有一位百度智能云的大咖与各位一起探讨百度智能云数据库的创新、变革和应用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cf/cf46378ce27ef5e83b5b6abf599ec8d3.jpeg\" /></p><p></p><p>第一讲：《从互联网到云计算再到 AI 原生，百度智能云数据库的演进》</p><p>你将获得：</p><p>全局完整地了解数据库行业的历史和发展趋势；了解百度智能云数据库在各个阶段的典型产品、应用和关键技术；了解百度智能云数据库在 AI 原生时代的创新和变革。</p><p>&nbsp;</p><p>第二讲：《高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析》</p><p>你将获得：</p><p>了解云原生数据库的不同技术路线和能力对比；了解相比传统单体数据库，云原生数据库的技术差异和挑战；了解 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p>&nbsp;</p><p>第三讲：《面向金融场景的 GaiaDB-X 分布式数据库应用实践》</p><p>你将获得：</p><p>了解金融核心系统在构建分布式数据库的技术挑战；了解 GaiaDB-X 数据库的架构及在金融场景的分布式特性；了解 GaiaDB-X 在金融机构的核心系统分布式的落地实践。</p><p>&nbsp;</p><p>第四讲：《一站式数据库上云迁移、同步与集成平台DTS的设计和实践》</p><p>你将获得：</p><p>了解数据库上云迁移、数据库同步 / 集成的业务场景，以及实践中可能遇到的技术挑战；了解百度智能云 DTS 的关键特性、核心技术和实践案例。</p>",
    "publish_time": "2023-11-01 14:23:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数智赋能云课堂第 2 期·数字化组织演进路径与方法",
    "url": "https://www.infoq.cn/article/ELqwMUsPhkVTjQ8Trrod",
    "summary": "<p><img alt=\"unpreview\" src=\"https://static001.infoq.cn/resource/image/3e/c7/3e6088715b0056aa452f4a9c9ce6b7c7.jpg\" /></p>",
    "publish_time": "2023-11-01 15:29:11",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "OpenAI 刚刚又杀死了一批初创公司",
    "url": "https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG",
    "summary": "<p></p><blockquote>围绕别人家的大模型创业，盈利快，死得也快？</blockquote><p></p><p>&nbsp;</p><p>从目前的情况看，每当OpenAI在ChatGPT上发布新功能时，都会因为对开发类似功能的初创公司造成冲击而受到指责。OpenAI日前刚刚又为ChatGPT引入了两项新功能，其一是“上传多种类型文档”、其二为“无需切换对话即可使用工具”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/25/2547e3b3e5fdb86572fd275522b04970.png\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>ChatGPT/GPT-4迎来重要更新：可上传任意PDF文档并询问其内容。无需切换聊天即可直接使用新工具。</blockquote><p></p><p>&nbsp;</p><p>在此次更新之后，ChatGPT不仅能够直接读取PDF，还可以在同一对话当中支持多种文档类型，包括PDF、图像和CSV等等。以往，用户只能在默认模式下上传图像，但现在已经可以无缝上传文档并立即开始提问，这大大扩展了ChatGPT平台的多样性和可用性。</p><p>&nbsp;</p><p>此外，用户也不再需要指定自己要用的ChatGPT模式。如今，浏览、高级数据分析（原名Code Interpreter代码解释器）和DALL-E 3现可在同一对话中直接使用。GPT将自行确定激活不同模式的适当时机，例如在用户要求创建图像时调用DALL-E。</p><p>&nbsp;</p><p>相信很多朋友都有在浏览器里浏览几百页的PDF文件，想要从中提取有用数据和摘要信息的经历，其过程相当之痛苦。正因为如此，此次更新才在ChatGPT Plus用户当中获得了高度评价。</p><p>&nbsp;</p><p>但这次更新也造成了广泛影响。有声音认为，这种新的“多模态”更新将毁掉至少数十家初创公司，其中耳熟能详的名号包括ChatPDF、AskYourPDF和PDF.ai等等。不少初创公司恰恰就是看准了之前ChatGPT无法与PDF直接交互的现状，才构建起自己的业务体系。既然现在ChatGPT可以操作PDF了，那这些后起之秀还有哪些业务空间可以挖掘？</p><p>&nbsp;</p><p></p><h2>面向AI的“打包初创公司”面临毁灭打击？</h2><p></p><p>&nbsp;</p><p>支付巨头Stripe公司产品负责人Sahar Mor在LinkedIn上写道，“OpenAI刚刚推行的一项举措可能直接消灭数十家AI公司。”他还专门提到了“打包初创公司。”这类企业在本质上就是把ChatGPT等API“打包”起来形成自己的业务，使用聊天机器人的底层技术提供某种原厂商未能直接提供的服务。</p><p>&nbsp;</p><p>但建立AI打包业务的创始人们，最初可能未必是要故意利用ChatGPT的软肋。今年3月，OpenAI宣布AI服务对外开放，欢迎开发者们将ChatGPT整合到自己的应用程序和产品当中。</p><p>也就是说，提供PDF分析功能的初创公司只是打包商里的一部分，还有很多在提供其他各种补充性功能。</p><p>&nbsp;</p><p>目前最具市场影响力的打包厂商当数Jasper AI，该公司在Coatue和Bessemer Venture Partners等大型风险投资公司的支持下，成功在今年开年之际获得15亿美元估值。</p><p>&nbsp;</p><p>他们做对了什么才得到如此夸张的估值？答案很简单，围绕OpenAI的GPT模型开发一套专门针对企业营销团队的“AI领航员”（AI Copilot）。</p><p>&nbsp;</p><p>但根据技术外媒The Information报道，随着该公司内部估值的一路走低，其业务定位似乎也陷入了困境。今年7月，Jasper AI还曾宣布裁员。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cb/cb1fafa20d1bdcf866fc9ada1a2b772a.jpeg\" /></p><p></p><p>&nbsp;</p><p></p><blockquote>Jasper 以 $1.5B 估值筹集了 1.25 亿美元，这也无济于事。也许GPT打包模式并不适合初创公司。</blockquote><p></p><p>&nbsp;</p><p>也许其他使用ChatGPT等工具为用户提供PDF交互功能的初创公司，都将面临类似的悲惨命运。</p><p>&nbsp;</p><p>今年5月，数据科学家Alex Reibman发布了ChatOCR。这是一款ChatGPT插件，能够“从PDF中读取文本，包括扫描和手写内容。”但在上周末的更新之后，他在X上开展了一项民意调查，询问用户“既然现在ChatGPT已经内置了PDF处理功能”，大家还愿不愿意继续使用插件。在210名受访者中，72.4%的人预计插件“使用量将会减少”。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/2d/2d4972ac35c66c649dec1af24f5ce82d.jpeg\" /></p><p></p><p></p><blockquote>我们是本次更新的“受害者”之一。我们运行着ChatOCR，ChatGPT商店中众多PDF处理插件之一（我们主打的是OCR）。过去3个月来，我们的月度经常性收入达到3500美元。大家认为，这次更新会对我们造成怎样的影响？</blockquote><p></p><p>&nbsp;</p><p></p><h2>是面临清算还是仍有发展前景？</h2><p></p><p>&nbsp;</p><p>在围绕PDF创业的公司中，PDF.ai是一家能赚钱、自给自足而且利润率可观的企业。PDF.ai公司创始人Damon Chen表示，“我们的目标不是成为又一家独角兽企业，几百万美元的年度经常性收入对我来说已经足够了”。</p><p>&nbsp;</p><p>OpenAI的更新对PDF.ai确实也带来了一定冲击，他承认，体量太小的初创公司终将遭到淘汰，而由风险投资供养的大块头在烧光现金之后也挺不住。</p><p>&nbsp;</p><p>但Chen仍然带有希望：“昨晚我和妻子就ChatGPT更新聊了聊。我问，如果PDF.ai最后没能成功，该怎么办？她轻描淡写地说一个项目的失败不算什么，另起炉灶好了。”</p><p>&nbsp;</p><p>如今距离ChatGPT正式亮相已过去近一年，OpenAI正逐渐为其添加更多新功能。OpenAI的终极目标是实现通用人工智能（AGI），而阅读PDF等进展只是这个庞大目标中的一小部分。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/8f/8f0d4e72b50ef5c1c97b57f25c6635a6.jpeg\" /></p><p></p><p>&nbsp;</p><p>正如Tenstorrent公司人工智能总监Shubham Saboo评论的那样，“ChatGPT 的战略：巩固、创新和统治。ChatGPT 会成为终极 AI 超级应用程序，将 Midjourney、PDF Chat、Perplexity AI 和高级数据分析全部结合在一个应用程序中。”</p><p>&nbsp;</p><p>从某种程度来说，只要不具备能与竞争对手拉开显著差距的“护城河”，初创公司就随时面临被劫掠的风险。那么在别人的 API 之上建立自己的业务还有前途吗？</p><p>&nbsp;</p><p>让人意外的是，不少人对此依然表示乐观。支付服务商Stripe公司产品负责人Sahar Mor表示，“打造用户友好的界面和更易用的功能仍有其实际意义，因此针对特定细分市场的垂直初创公司将继续保持其主导地位。真正面临风险的，主要是那些横向延伸的AI初创企业。”</p><p>&nbsp;</p><p>分析服务商Glass Acres创始人Mark Zahm也认为，“只要GPT还存在，GPT打包方案就会伴其成长并蓬勃发展……”AI爱好者Rowan Cheung则在X上分享道，“大家现在怎么不讲那些靠GPT打包方案赚大钱的故事了？”</p><p>&nbsp;</p><p>在他看来，不少初创公司在网络流量上的表现已经超越了价值数十亿美元的传统公司，而且其业务定位均匀分布在打包、微调和专有模型等各个领域。也就是说，部分GPT打包方案的月度访问量，比某些估值数十亿美元的企业还要高。</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e078f9ac6611e19b134bf9ee94d6dc2.png\" /></p><p></p><p>图片来源：<a href=\"https://www.infoq.cn/article/iLZYudwYlgARoXmjawJb\">ChatGPT 已成为 2023 年最大金矿，大家是怎么靠它挣到钱的？</a>\"</p><p>&nbsp;</p><p>开发GPT打包方案的初创公司，主要为那些需要整合AI功能的企业提供一种更经济、更高效的选项，避免从头开始构建复杂的模型体系。</p><p>&nbsp;</p><p>除了OpenAI之外，众多初创公司正在不断涌现，而生成式AI业务已经成为其冲击独角兽之路上的一股重要推力。根据风险投资公司Accel最近发布的报告，这些新独角兽企业中有60%属于生成式AI范畴。去年，欧洲和以色列的生成式AI初创公司投资总额接近10亿美元。相比之下，美国生成式AI初创公司同期获得的注资更是超过140亿美元。但正如报告中所强调，这140亿美元资金的具体分配并不均衡，单是OpenAI一家就分走了其中100亿美元。</p><p>&nbsp;</p><p>根据近期报道，部分照片AI应用和AI聊天机器人服务商赚到的绝对利润，反而还高于生成式鼻祖ChatGPT。今年9月，Chat &amp; Ask AI和ChatOn——AI聊天助手都产生了可观的收入，分别达到近338万美元和211万美元。</p><p>&nbsp;</p><p>此外，AI Chatbot——Nova和AI Chatbot: AI Chat Smith也不甘落后，同期收入分别为144万美元和172万美元。而由a16z支持的聊天机器人初创公司Character.ai也在市场上闹出不小的动静，截至今年9月下载量已达239万次。</p><p>&nbsp;</p><p>也就是说，哪怕OpenAI凭借其多模态功能领先了竞争对手十步，各位初创选手也没必要悲观放弃。相反，也许底层核心技术的升级能提供丰富灵感、帮助他们开发出更好的后续产品。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10\">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>\"</p><p><a href=\"https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10\">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>\"</p><p><a href=\"https://twitter.com/thealexker/status/1680626018522914817\">https://twitter.com/thealexker/status/1680626018522914817</a>\"</p><p><a href=\"https://twitter.com/AlexReibman/status/1718848888088793487\">https://twitter.com/AlexReibman/status/1718848888088793487</a>\"</p><p><a href=\"https://twitter.com/Saboo_Shubham_/status/1718653456926359855\">https://twitter.com/Saboo_Shubham_/status/1718653456926359855</a>\"</p>",
    "publish_time": "2023-11-01 15:40:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC 编程：代码编程模型的应用与挑战",
    "url": "https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK",
    "summary": "<p>嘉宾 | 鱼哲、刘东</p><p>编辑 | Tina</p><p>&nbsp;</p><p>生成式AI已经成为软件行业的一个重要推动力。在过去的一年里，包括网易在内的许多公司都在积极探索如何将这项技术应用到他们的产品中。如今，网易已经推出了多个生成式AI的实际应用产品，包括编码助手、大数据分析产品和低代码平台。在最新一期的“极客有约”对话节目中，鱼哲与网易杭州研究院人工智能部的算法负责人刘东一同探讨了有关大型模型产品的成本、速度和精确度等关键问题。</p><p></p><p>本文经编辑，原视频地址：https://www.infoq.cn/video/Wu1iSPABRu9NTVRrrywi</p><p>&nbsp;</p><p>亮点：</p><p>大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。微调只是为了有针对性地增强它，使其更好地满足用户指令，所以首先需要基准模型能支持该任务。我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们引入了一个\"可信AI\"的概念，包括过程可验证、用户可干预和产品可运营三个方向。从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。</p><p>&nbsp;</p><p>嘉宾简介：</p><p>鱼哲，Lepton AI 创始团队成员，产品负责人。</p><p>刘东，网易杭州研究院人工智能专家，AI算法团队及产品团队负责人，专注于前沿算法研究与商业化应用。相关技术成果曾获浙江省科技进步奖一等奖。</p><p>&nbsp;</p><p></p><h4>AIGC在软件工程领域的应用方向</h4><p></p><p>&nbsp;</p><p>鱼哲：我个人认为Codex和Copilot等工具具有广阔前景，而AIGC也在广泛推广。刘东老师，您能否简要介绍一下，网易杭州研究院在AIGC技术以及软件工程领域的技术研究方向有哪些？</p><p>&nbsp;</p><p>刘东：关注AIGC，特别是在软件研发领域，我们认为它在各个环节都有实际价值。例如，在需求分析和设计方面，大型模型已经能够提供出色的设计建议。在编码和开发阶段，Copilot已经非常成熟，可以显著提高程序员的效率。在代码调试、分析和优化阶段，大型模型也能提供有益的优化建议，包括检测潜在的错误。甚至在测试阶段，我们也尝试使用大型模型生成测试案例。在运维环节，例如线上日志的实时分析和监控，也可以受益于大型模型的能力，提高效率。</p><p>&nbsp;</p><p>从应用的角度来看，我们目前在编码和开发阶段最快地实现了AIGC的应用。特别是我们内部为研发同事提供了类似Copilot的工具，已经看到效率有所提升。此外，我们还开发了一些外部商业化产品，如在BI产品中引入了对话功能，推出了ChatBI产品，以及在低代码产品中使用大型模型来加速低代码开发效率。</p><p>&nbsp;</p><p>鱼哲：关于Copilot，我之前在GitHub上也尝试过，还尝试过Code Llama。我想代表我们的观众逐一提出一些问题。首先，所有程序员都非常熟悉的一个问题是，我们花费很多时间思考如何分解代码的功能实现，以及编写和调试代码。特别是编写单元测试，在后端开发中经常需要，虽然我们不会讨厌，但有时候确实不太愿意做这项工作。</p><p>&nbsp;</p><p>然而，AI辅助编程作为一个产品，我看到市场上已经有很多竞品，比如GitHub的Copilot、OpenAI的Codex，以及AWS的Code Whisper等。有很多成熟的产品存在。我想问一下，为什么网易杭州研究院选择在这个领域开展工作？此外，微软在收购GitHub后表示支持Copilot业务，但据报道，这一业务目前亏本，因为Copilot对于开发者来说价格相对较高。微软在财报中也提到，他们每月支持一个开发者需要100美元，但实际只收取20美元。在这种情况下，您如何看待网易在这一领域的角色和作用？</p><p>&nbsp;</p><p>刘东：我们考虑这个问题时，有几个方面的考虑。首先，从安全的角度看，每家企业都有一些核心代码不愿意与外部共享，因此希望能够拥有相对可控的服务，并在其中使用大型模型以提高程序员的效率。因此，就可控性和安全性而言，自研可能是一个较好的选择。</p><p>&nbsp;</p><p>其次，每家企业都有大量的特定代码积累，而如何有效地利用这些代码，以在其业务中发挥价值，这也是一个重要问题。像Copilot这样的云服务通常比较通用，很难让企业将其自有代码集成进去并进行优化，以适应企业自身的习惯。</p><p>&nbsp;</p><p>因此，我们通过自研的方式，利用自己的模型来更好地适应网易的需求和场景，以提供程序员更好的使用体验。这是我们的出发点。</p><p>&nbsp;</p><p>至于亏本的问题，我也认为大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。因为在这一领域，客户通常比较价格敏感，即使收费较低，用户也可能觉得价格昂贵。但从成本角度来看，确实需要较大的投入。因此，我们的考虑是综合各方因素来实施这项工作。</p><p>&nbsp;</p><p>鱼哲：除了安全性问题，您在网易情况可能会注意到一些特别适合的场景，不管是在电商还是游戏等许多场景中。您能否举一个具体的例子，展示我们在什么情境下通过自研的Copilot项目，更好地支持业务方编写其业务代码的案例呢？</p><p>&nbsp;</p><p>刘东：我们进行了大量的定制和优化，比如在游戏运营场景中，游戏经常需要举办各种活动。这些活动的方案通常由策划部门提出，要求程序员按照方案进行实现，但实现后可能代码只用一次，然后就不再使用。这种场景非常常见。</p><p>&nbsp;</p><p>但是，由于许多游戏之间存在相似性，企业的代码库中可能有很多人写过类似的代码，具有很大的参考价值。在这种情况下，如果使用通用的Copilot，它通常无法了解企业专有的代码信息，因此在这种场景下提供的提示效果可能不够理想。但如果我们进行企业定制，就可以通过一些增强的方式，将这些信息集成到提示中，然后将其提供给大型模型，使其能够参考这些代码来提供更好的提示。这样，我们就可以更好地实现降本增效。</p><p>&nbsp;</p><p>鱼哲：您刚才提到了在业务场景中进行了许多优化，特别是在模型层面。您能详细介绍一下优化工作是在模型层面进行的，还是在输送给模型的提示工程这一层进行的？或者说，在模型的不同方面都进行了优化，可以谈谈具体的优化思路是什么吗？</p><p>&nbsp;</p><p>刘东：我们的优化思路主要围绕两个关键点展开，以发挥大型模型的价值。第一个关键点是确保模型本身的强大性，这涉及将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识。</p><p>&nbsp;</p><p>第二个关键点是优化提示工程，即我们如何提供给模型更有信息量的上下文，以便模型更好地理解上下文，产生对程序员有价值的输出。我们发现，仅仅将当前代码片段的上文或下文提供给模型并让其继续生成，效果通常一般。因此，我们考虑了编程过程中的各种信息来源，包括引入的外部第三方库、工程中的其他项目文件、类似的工程项目，甚至程序员在编程过程中浏览和检索网页、查找答案以及执行粘贴和复制等操作。这些行为都是宝贵的提示信息，我们通过将这些信息融入到模型的提示中，帮助模型更好地理解当前的上下文，从而更好地输出对程序员有价值的信息。这些工作使我们的模型能够更好地与业务结合，提供更好的效果。</p><p>&nbsp;</p><p>鱼哲：您提到了模型微调，确实，在Google和其他地方，人们一直在进行对模型的微调。通过微调一个基础模型，将其完全适配到特定任务，这是一个非常有效的方法。许多人认为，只要有基本模型，然后进行一些微调，就可以将其应用到任何任务上，使其成为该任务的专家。您对这个问题是怎么看的？</p><p>&nbsp;</p><p>刘东：如果我要进行微调，内部除了我们自己训练的基础模型，还有许多开源的基准模型可供使用。我们进行了大量的评估，具体思路是，如果要进行微调，首先要分析基准模型是否足够强大，是否在具体任务上已经表现得相当不错，微调只是为了有针对性地增强它，使其更好地满足用户指令。如果基准模型根本不支持该任务，仍然强行进行微调的话，效果可能不太好，或者可能需要寻找一种成本更高的微调方式，类似于继续进行预训练，以将知识融入模型，然后再进行微调，例如LORA微调。我认为LORA微调可能只对现有的基准模型进行提升和补充有意义。当然，如果基准模型本来就不太好，那么可能不会获得太大的收益，或者预期的性能可能不会特别出色。</p><p>&nbsp;</p><p></p><h4>成本问题</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到了成本问题，确实，在特别是语言模型（LM）这个领域，模型的推理成本随着参数数量的增加呈指数级增长。我们了解到，网易内部使用Copilot不仅仅涉及生成代码，还可能涉及解释代码推理方面。用户可能以多种不同的方式使用它。您是使用一个巨大的模型或者一个具有固定参数的模型来支持所有使用方式，还是根据不同的使用方式智能地调整背后模型的大小呢？</p><p>&nbsp;</p><p>刘东：我们选择了后者的方式，即根据不同的使用方式智能地调整背后模型的大小，这是有充分考虑的。从成本和效率的角度考虑，这是一个综合的决策。特别是在编程场景中，代码提示是一个非常高频的任务，因为每输入几个字母，都会触发一次提示请求。在这种情况下，模型需要足够快，因为如果太慢，程序员可能会自己完成输入，这样就不会提供太多价值。</p><p>&nbsp;</p><p>另一方面，这个场景通常涉及到代码生成，相对来说是一个相对固定且不太复杂的任务，与通用任务相比，难度较低。因此，我们更倾向于选择规模较小的模型，以确保效率，并且不会明显降低质量。</p><p>&nbsp;</p><p>对于像代码解释、调试分析或注释生成这样的任务，难度较大，可能需要更大的模型才能实现良好的效果。但好的一点是，这些任务通常不会有太高的使用频率，因此在这种情况下，我们可以选择相对更大的模型，而不需要进行大规模的冗余部署，因为请求量本来就不会太大。这种综合考虑帮助我们更好地控制了成本。</p><p>&nbsp;</p><p>鱼哲：随着大型语言模型的发展，特别是像Copilot这样的方式，例如像LLAMA这种模型，我们是否仍然需要手动编写注释呢？大家讨厌别人不写注释，但自己也不喜欢写。</p><p>&nbsp;</p><p>刘东：写注释与不写注释在很大程度上是一个习惯问题。写注释的主要目的首先是为了给自己提供提示，使代码更容易理解和维护。其次，注释也有助于他人理解代码，尽管注释的覆盖度要求可能并不高，因为大模型可以帮助填充一些细节。然而，写注释的程度可以因程序员而异，有些程序员可能倾向于写详尽的注释，解释每个细节，而有些人可能只写简要的概述性注释。这与个体的写作风格和代码质量意识有关。</p><p>&nbsp;</p><p></p><h4>BI产品和低代码平台</h4><p></p><p>&nbsp;</p><p>鱼哲：在网易杭州研究院，我们不仅在内部广泛应用这些先进的技术，还在一些领域提供对外的技术支持和合作机会。在对外方面有哪些技术合作呢？</p><p>&nbsp;</p><p>刘东：除了为内部提供技术支持，我们还将这些大语言模型的能力整合到商业化产品中，以为客户提供更多的服务。其中，我们的代表性产品之一是BI产品。通过整合大语言模型的能力，为BI产品引入了自然语言交互功能，使用户可以通过自然语言查询所需的数据和报表，这完全是由大语言模型驱动的。</p><p>&nbsp;</p><p>另一个重点领域是低代码平台，我们的CodeWave平台，它通过低代码编程方式，降低了编程的门槛，提高了编程的效率，从而帮助企业节省成本并提高效率。在这个平台中，我们引入了大语言模型的能力，以提高效率和降低编程门槛。这两个领域是我们当前主要投入和发展的方向。</p><p>&nbsp;</p><p>鱼哲：我们还有一个低代码产品，可以介绍下这个产品的使用体验吗？</p><p>&nbsp;</p><p>刘东：Low code 不同于 0 code，简而言之，它是一种基于可拖放的方式进行软件开发的方法。它不要求专业的程序员从头编写代码，也不同于完全无需编码的 0 code 方式。在低代码中，你可能需要配置一些固定的模板，定义数据模型，设计流程结构，还可以使用预定义的组件，通过拖拽的方式连接各种逻辑，最终生成软件产品。</p><p>&nbsp;</p><p>这种方法的核心优势在于相对于传统的完全编码软件开发，用户需求较低，无需像计算机专业的本科毕业生或有丰富经验的人才那样写代码。但与 0 code方法相比，它仍然保持了软件开发的灵活性，因为它可以实现复杂的逻辑。低代码的定位介于传统软件开发和 0 code 之间，兼顾了易用性，同时也能满足一些较为复杂的软件开发需求。</p><p>&nbsp;</p><p>鱼哲：您能简单介绍下这个产品的对外发布节奏吗？我看到官网上有些资料相关。</p><p>&nbsp;</p><p>刘东：我们目前在网易数帆官方网站上提供了一些基础材料和介绍。此外，我们即将在11月2日举行2023网易数字+大会，届时将提供更详细的产品介绍以及有关技术的分享。我们期待在发布会上与大家分享更多信息。</p><p>&nbsp;</p><p></p><h4>AIGC在数据分析应用上的挑战</h4><p></p><p>&nbsp;</p><p>鱼哲：回到刚才提到的ChatBI，我在以前做业务时常常需要与BI同事沟通，例如我想了解最近三个月华北地区哪个行业的客户增长最快，哪个行业的客户有一些困难，以及他们所遇到的产品使用情况。这种情况通常需要等待一两天的时间，不管是BI同事还是我自己去做，都需要花费大量的时间来查看数据地图，查看每个表的结构以及做相关的SQL查询，因为我们需要定义特定的指标，例如复购、沉默和活跃等。这是一个非常复杂的问题，之前尝试了许多模型，但它们存在幻觉的问题，导致了一些错误的结果，这是不能接受的。在BI领域，这个问题是非常严肃的，我们不能容忍存在幻觉的问题。我想了解一下，你们是如何处理这个问题，如何解决这种复杂性挑战的。</p><p>&nbsp;</p><p>刘东：我们面对的确实是一个巨大的挑战，而且我们在这个产品上花费了很长时间，因为BI场景是非常严肃的，它的任务是提供准确的信息。如果我们只是编造数据或者输出不可信的信息，那这个任务基本上就失败了。因此，我们采取了多重方法来尽量避免这个问题。</p><p>&nbsp;</p><p>首先，我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们收集了大量的数据和各行各业的常见数据报表，通过数据增强和训练，使模型的能力更强。这个领域子模型专注于解决数据分析场景，能够通过自然语言输入生成高质量的SQL查询语句。</p><p>&nbsp;</p><p>其次，尽管模型很强大，但我们也意识到大型生成式模型不是100%可控的，因此我们在产品层面进行了多方面的工作。我们引入了一个\"可信AI\"的概念，包括过程可验证、用户可干预和产品可运营三个方向。</p><p>&nbsp;</p><p>过程可验证：我们不仅仅相信模型生成的SQL查询语句，而是使用一个查询语句解析引擎将其解析为人类可理解的语言，以确保用户了解模型的工作原理。如果发现错误，用户可以立即识别并不信任结果。用户可干预：我们允许用户对模型生成的查询进行干预。用户可以更改条件、操作等，以纠正错误或调整查询。这提供了用户对结果的额外控制。产品可运营：我们希望产品不仅仅是一个静态的工具，而是能够随着用户的使用变得更智能。我们收集用户的行为习惯，正例和反例，不断优化模型。我们也提供产品配置，以使模型理解各行各业的“黑话”和简称。通过不断的运营，使模型越来越智能，适应用户的需求。</p><p>&nbsp;</p><p>这些方法的结合，以及其他细节的优化，使我们的产品更可信、可控，提高了用户的工作效率。</p><p>&nbsp;</p><p>鱼哲：这是否意味着当用户使用产品时，他们需要在某种程度上提前注入表结构的一些信息？或者说，模型能够根据表的结构自行猜测字段的含义？</p><p>&nbsp;</p><p>刘东：大模型是通过自主猜测的。只要提供底层表结构，大模型可以自动获取这些信息，所以用户在一开始使用时通常不需要太多干预。</p><p>&nbsp;</p><p>鱼哲：您刚刚提到的这个反馈收集非常有趣，因为通过良好的RLHF方法，模型的性能可以显著提高。</p><p>&nbsp;</p><p>刘东：是的，必须逐步将其系统运营，使其随着使用而不断智能化，而不是采取一劳永逸的方式。这样做的话，问题通常不会被永久解决。但一旦将其运营起来，将负面案例的反馈馈送给它，它就会不断改进。</p><p>&nbsp;</p><p>鱼哲：刚才有个直播观众的提问：“对于这些垂直领域的模型，你们是在基础大模型的基础上进行微调，还是持续进行预训练，或者是从零开始使用领域样本训练参数较小的模型？”</p><p>&nbsp;</p><p>刘东：我们通常是基于基础的基座模型进行调整。网易内部我们已经进行了基础模型的玉言，这是一个从头开始训练的基座模型。从头开始训练的好处是，我们大致了解未来要覆盖的领域，因此在训练过程中，我们有意地将一些领域相关的数据融入其中。例如，如果要处理编程任务，就会注重将代码相关的数据纳入模型。如果要处理SQL的任务，就会加入一些SQL的数据。这个基座模型相对来说比较通用。然后，我们会在这个基础上为每个领域创建领域特定的子模型，以进行适配。</p><p>&nbsp;</p><p></p><h4>数据的重要性</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到的ChatBI的问题，如果拥有一个基础模型并为其创建功能，同时提供大量数据时，我发现在这个工作中，最大的挑战实际上不在于微调模型，而是在于找到合适的数据，并将其准备成可供模型使用的形式。我认为这是最困难的部分。</p><p>&nbsp;</p><p>在研究一篇论文时，我注意到在数据稀缺的情况下，他们提出了一个新名词叫RLAIF，即通过人工智能来生成强化学习所需的数据，以支持强化学习任务。</p><p>&nbsp;</p><p>对于像ChatBI这样的项目，我认为您需要大量的数据来对基础模型进行调整，而且需要具备高度的语义和推理能力。模型的规模不会小，而随着模型规模的增加，调整参数需要更多的精力、计算资源和数据。</p><p>&nbsp;</p><p>我想了解一下，您是如何在数据准备方面处理这些挑战的？因为实际情况是，在这类项目启动之初，数据通常不够整洁，或者很多人最初并不清楚这些数据可能会有哪些用途。您是如何处理这一问题的呢？</p><p>&nbsp;</p><p>刘东：无论是在ChatBI领域还是在以前的代码自动补全项目中，数据准备工作都是至关重要的，也是相当具有挑战性的任务。我们投入了大量精力来应对这个挑战。</p><p>&nbsp;</p><p>在ChatBI项目，我们获得数据的途径多种多样。首先，我们会在网上寻找一些开源数据。在这个领域，因为传统方法已经发展了相当长的时间，所以存在许多开源的评测数据，以及公开数据表结构的定义。我们可以利用这些表结构，以人工智能的方式自动生成问题和答案，从而使用AI来生成数据，这是一种当前相对流行的方法。此外，我们还投入了大量人力资源来进行数据的搜集和标注工作，将各个来源的数据汇总，综合使用，以满足我们的需求。</p><p>&nbsp;</p><p>鱼哲：我觉得这个趋势在从事NLP领域的同事中也非常明显。人们开始广泛使用语言模型来执行以前需要使用多个专门的小模型来完成的任务。举例来说，以前我们需要训练专门的模型来执行诸如语音到文字转换、地址解析以及标点符号分割等任务。而现在，像您刚才提到的，在生成数据方面也使用了语言模型。这引发了一个问题，即您是否认为大型语言模型会逐渐取代NLP领域中使用的多个小型专家模型呢？</p><p>&nbsp;</p><p>刘东：从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。这意味着，尽管它们非常有能力，但我们不必在每个场景中都采用大语言模型。例如，对于一些小型NLP任务，我们可以使用较小的模型，它们成本低廉，在线上表现良好，同时可以满足高并发需求，因此在这些情况下，不必迫切转向大型语言模型。</p><p>&nbsp;</p><p>当然，大语言模型的能力毋庸置疑，它们在某些复杂任务上可能表现更出色。然而，我认为大语言模型与领域专家模型之间不是相互替代的关系，而是可以共存的。在不同的情境中，可以选择使用不同的模型，以便最好地满足特定需求。这种差异化的方法可能是更好的选择。</p><p>&nbsp;</p><p></p><h4>模型选择的问题</h4><p></p><p>&nbsp;</p><p>鱼哲：因为我之前有时也会采取简便的方式，直接使用大型模型，特别是在拥有免费积分的情况下。但随着时间的推移，我发现在考虑长期投资回报时，仍需要寻找传统的常规模型。</p><p>&nbsp;</p><p>刘东：在ChatBI中，除了大型模型之外，有时需要将小型模型和大型模型结合使用。这是因为尽管大型模型拥有出色的能力，但它在成本和执行速度上可能存在一些问题。小型模型则执行速度非常快。在某些任务中，如果你不断地调用大型模型来执行和解析，可能会影响用户体验，因此需要进行综合考虑。</p><p>&nbsp;</p><p>鱼哲：您刚刚也提到了，一方面，生代码生成式大模型或ChatBI生成式大模型等，都在数据收集方面面临巨大挑战。我认为数据是其中的一个技术挑战。除了数据之外，在这个过程中还有哪些方面您认为非常具有挑战性，非常难的呢？</p><p>&nbsp;</p><p>刘东：我认为数据确实是一个巨大的挑战，不仅在收集方面，还在清洗数据方面需要耗费大量精力。清洗数据是非常关键的，因为如果不做好，会直接影响模型的效果。我们在数据清洗方面投入了大量精力，因为高质量的数据是确保模型效果的基本保障，这是第一个挑战。</p><p>&nbsp;</p><p>另一个挑战是一旦大模型的能力达到足够强，如何在实际业务场景中找到合适的应用场景，确保它能够创造价值。这方面也非常具有挑战性，因为大模型面临效率、速度和成本等问题。虽然它在很多场景下效果出色，但用户体验可能难以保证。此外，大模型作为生成模型，不可能百分之百准确。如何找到那些既能容忍错误，同时又能为用户带来实际帮助的场景，让模型成功落地，也是一个非常大的挑战。</p><p>&nbsp;</p><p>总之，技术本身的能力与业务场景的结合是非常关键的，只有找到合适的结合点，大模型的能力才能真正发挥作用，用户才能真正感受到其价值。否则，它将一直停留在演示的层面，其技术的价值和影响力都会受到限制。</p><p>&nbsp;</p><p>鱼哲：有时候出现了上下文的误解。例如，用户可能要求搜索最近三个月内是否有玩过某个游戏，但可能会被错误地理解为最近三年。在ChatBI场景中，我们可能有一个SQL生成工具，但它生成的SQL语句缺少一个关键的\"where\"子句。现在，关于ChatBI，是它能够接受用户的自然语言查询并自动触发查询任务，还是它只返回SQL代码，用户需要将SQL代码用于传统的数据仓库查询窗口中查询？</p><p>&nbsp;</p><p>刘东：我们的当前设计是完全自动的。当用户提供一个查询后，系统会立即执行，而且像之前提到的那样，各种AI操作都会在执行后进行解释，并展示各种条件。用户可以根据查询结果和这些解释来判断查询的可靠性。</p><p>&nbsp;</p><p></p><h4>大模型产品价值的体现</h4><p></p><p>&nbsp;</p><p>鱼哲：所以用户不仅可以看到生成的代码，还能了解为什么会生成这段代码。此外，生成的数据会以表格的形式展示，用户可以导出数据。产品是否还提供可视化或建模分析能力，还是用户需要自己去处理这些方面的工作？</p><p>&nbsp;</p><p>刘东：我们目前主要提供可视化展示，对于后续的建模分析，我们正在进行研究和探索。</p><p>&nbsp;</p><p>鱼哲：您之前提到技术研究院需要同时具备技术和业务的理解，而最终需要为其结果负责。客户，无论是网易集团内部还是外部，都渴望了解这些生产力工具如何提升效率。这包括自动代码生成、SQL自动生成以及低代码平台等技术，它们都旨在提高生产力。然而，生产力提升在实际中往往是一个具有挑战性的问题。难以证明这些技术是否可以直接提高生产力。关于如何证明生产力提升的问题，您是怎么解决的？</p><p>&nbsp;</p><p>刘东：我们一直在思考和探索这个问题，因为要传达技术的价值，需要从多个角度来考虑如何证明其价值。我们非常关注用户的反馈和实际使用数据，这对于衡量技术的有效性至关重要。</p><p>&nbsp;</p><p>在我们内部使用低代码工具，例如代码补全工具，类似于Copilot工具，我们详细记录用户的使用情况，特别关注用户采纳提示的比例以及AI自动生成代码的比例。这有助于我们了解技术是否真正帮助用户减少编码工作，还是只是一个演示性的工具，用户不太愿意采纳其中的建议。</p><p>&nbsp;</p><p>同样，对于ChatBI和低代码工具，我们也密切关注用户的使用情况。例如，如果没有Chat功能，很多业务人员可能无法自行使用BI工具来查询数据。但如果引入Chat功能，我们关心是否有人在使用，以及他们的使用频率。在低代码工具方面，我们使用自然语言来生成逻辑，然后观察生成情况和占比。这些数据帮助我们衡量技术是否真正提高了生产力，帮助用户在成本降低和效率提高方面取得进展。我们非常关注这些方面的数据。</p><p>&nbsp;</p><p>鱼哲：当我们致力于提高生产力效率时，如AIGC或大型模型的出现与以往的机械发明有很大不同。以前的机械或半自动机械本质上需要人的操作。例如，使用除草器可能需要有人操作设备，而自动化机械则需要人的干预。然而，像您刚才提到的ChatBI，如果今天我可以使用自然语言描述业务需求，然后它可以为我生成正确的SQL查询并检索数据。那么对于那些传统的数据分析专业人员或BI同行，他们的存在可能会面临一些挑战，因为这种技术的出现可能改变了传统的数据分析方法。</p><p>&nbsp;</p><p>刘东：我认为这些工具主要是作为一种助力的角色存在的，而人的价值仍然不可或缺。无论是在BI领域还是在编码领域，它们的核心目的是帮助人们在一些简单重复的工作中提高效率。当拥有这种提高效率的工具时，我认为人们可以解放更多的时间，用于思考业务等更有价值的事情。这包括如何改进业务、更好地理解用户需求，以及如何提供更出色的软件产品。这是一种从不同角度思考问题的方式，而不是完全取代人的角色。因为这些技术目前正在逐渐发展，它们还没有达到100%可信并且能够胜任一切的状态。</p><p>&nbsp;</p><p>鱼哲：我听闻有些公司在内部开展了类似工作遇到了许多障碍。其中一部分障碍来自于开发团队，他们担心这种工具可能会与他们的工作发生冲突，或者一旦工具成熟，公司将不再需要他们。在你们尝试推广这些工具的过程中，是否也遇到了类似的挑战？</p><p>&nbsp;</p><p>刘东：我们目前并没有遇到这类挑战，因为我们进行了一些统计，发现从软件研发的角度来看，程序员实际花在编码上的时间并不占很大比例。编码只是他们工作的一小部分，更多的工作包括需求分析、整体设计以及与其他方面的对接等。编码所占的时间并不是很多。我们的目标并不是取代程序员，而是让他们能够更多地投入需求分析和用户场景理解，以便提高编码的质量和整个软件产品的效果。</p><p>&nbsp;</p><p></p><h4>大模型产品的私有化部署</h4><p></p><p>&nbsp;</p><p>鱼哲：我觉得程序员这个称呼有点狭隘，因为在编写代码时，实际上他们不仅仅在写代码，更多的时候在进行工程工作，也就是做工程师的事情。工程师通常需要将来自外部的各种复杂难以理解的需求和分散的模块整合到一起。在这方面，生成式模型的能力是不可替代的，不论是在短期还是长期。我认为很难通过直接应用一个大型模型来完成需要花费多年时间理解和深入了解的业务。这种情况下，使用生成式模型可能不太适用，因为你需要时间来积累对业务的深刻理解，然后才能进行创新性的工程工作。</p><p>&nbsp;</p><p>你刚刚描述的这些能力听起来确实非常有趣和具有吸引力。然而，我们也明白，许多公司，包括像网易自己开发Copilot时，通常出于安全和特殊场景的考虑，不愿意使用市场上通用的产品。这种担忧在中国和美国的科技公司中都非常普遍，特别是当公司规模较大时，它们通常更倾向于采用私有化部署，无论是在公司自己的数据中心还是在云上的IDC。</p><p>&nbsp;</p><p>在传统金融领域，如银行、保险和证券等领域，这些能力可以显著提高工作效率和效能。然而，由于国家监管要求或公司性质的原因，很多银行和保险公司通常需要确保技术提供的方式支持私有化部署。我们会积极考虑这些需求，以满足不同客户的特殊要求。网易在这方面是如何考虑的？</p><p>&nbsp;</p><p>刘东：我们的模型都是基于自己的基座模型调用的，因此完全可控。我们也提供了私有化部署的能力。除了不断优化模型性能，我们还专门组建了一个工程团队，专注于推理效率的优化、部署方案的设计以及各种硬件适配工作。</p><p>&nbsp;</p><p>在私有化部署方面，我们考虑如何尽量降低用户的成本，因为大型模型的成本相当高。首先，我们根据业务场景找到了适合的模型规模，而不是盲目地追求巨大的规模，这样可以减少硬件集群的复杂性。</p><p>&nbsp;</p><p>其次，我们进行了大量的工程优化。这包括引入业界开源的先进技术，以提高性能。我们还根据模型的特点进行了自定义适配，包括自定义内核等，以提高吞吐量和效率。</p><p>&nbsp;</p><p>此外，我们还考虑了量化加速等操作，以确保资源的可控性。因此，即使使用普通的显卡，我们也可以将大型模型部署上，并为用户提供良好的体验。这些措施都有助于提供高效的私有化部署解决方案。</p><p>&nbsp;</p><p>鱼哲：在私有化部署方面，你们是可以进行多方面的性能优化的吧？比如模型量化、压缩以及重新编写一些核心代码，从而提高性能和效率。在私有化部署的时候，一种方式是用户直接使用基座模型，这是一个即插即用的解决方案。另一种是用户在使用一段时间后，需要进行自定义微调，比如强化学习微调（RLHF）或自适应微调（RLAIF）等，你们是如何解决的？</p><p>&nbsp;</p><p>刘东：我们提供两种服务模式，以满足用户的需求。首先，我们提供一种自助工具模式，类似于业界已有的微调工具和强化学习工具。这些工具包括数据集管理、标注、训练任务和部署任务等功能。用户如果拥有足够的实力和理解相关流程，可以自行使用这些工具来满足其需求。用户可以随时尝试不同的方法，进行A/B测试，以确定哪种方式效果更佳。</p><p>&nbsp;</p><p>其次，对于一些重要客户，我们也提供定制化的服务。在这种情况下，我们的算法专家会与客户合作，共同解决他们特定的问题。我们会与客户密切合作，确保他们获得最佳的解决方案。无论用户自行使用工具还是选择我们的定制化服务，我们都将竭诚为他们提供支持。</p><p>&nbsp;</p><p>鱼哲：我也认为在私有化部署后再进行模型训练是一项颇具挑战的任务。从前我在这个领域有一段时间的从业经验，我深知这种工作需要投入大量时间和精力。此外，私有化部署环境通常存在标准不一致的问题，因此需要耗费额外的时间来解决各种复杂情况。</p><p>&nbsp;</p><p>刘东：每个客户的环境都有所不同，因此我们的目标是将产品尽量标准化，同时确保工具功能完善。这种方法有助于客户自行进行调整、训练和部署，降低了使用成本和门槛。只要他们理解这个过程，几乎都可以通过简单的点击鼠标来完成，而不需要深入编程或处理复杂的问题，这对用户来说更加容易接受和理解。</p><p>&nbsp;</p><p>鱼哲：在应用 AIGC 技术能力来提高软件工程效率时，您认为在业务端的落地过程中，最关键的角色通常会是什么？</p><p>&nbsp;</p><p>刘东：我认为在这个过程的每个环节都非常关键，没有哪一个环节可以忽略。首先，理解场景和找到使大模型落地的有价值的点至关重要。然后，需要探索这些点，以确定大模型的能力是否足够可控和可解决。如果算法可以解决问题，那么从工程角度来看，是否有足够的投资回报、性价比和用户体验也是非常重要的。最后，如何将这一切标准化地交付给用户，确保他们能够持续使用，而不仅仅是一个演示，也是一个巨大的挑战。每个环节都需要表现出色，才能成功完成这项工作。</p><p>&nbsp;</p><p></p><h4>对软件工程未来的看法</h4><p></p><p>&nbsp;</p><p>鱼哲：您怎么看待编程以及软件工程的未来？</p><p>&nbsp;</p><p>刘东：我认为AI和AIGC技术并不是要取代软件工程，而是要为软件工程提供更强的支持。随着数据积累、通信和计算能力的提升，对软件工程的要求变得越来越高，而AI技术可以提高软件工程的生产力。未来，软件工程师的角色可能会发生变化，从以程序员为主导变为人机协作的模式，工程师需要花更多精力学习和应用AI技术，以提高工作效率和生成更好的软件。</p><p>&nbsp;</p><p>鱼哲：这实际上涉及到一个重要的哲学性问题，即通用性与特殊性之间的平衡。在优化时，我们必须在通用性和特殊性之间做出权衡。因为优化通常是为了特定场景和硬件而进行的，它可能会牺牲通用性。您认为，在未来多久内，我们是否会看到通过代码生成或自动化方式，针对特定硬件环境进行优化呢？比如剪枝、量化、压缩和算子的自动生成。</p><p>&nbsp;</p><p>刘东：实现这一愿景需要大量的基础积累和技术成熟度。目前的代码生成技术主要依赖于已有的能力和沉淀的代码，然后通过大型模型的学习和自动生成来解决更多问题。这种自动化生成代码的技术在广泛的硬件和环境中得到应用，可能需要更多的积累和实践。</p><p>&nbsp;</p><p>对于新的硬件和场景，手动优化仍然是必要的，因为了解硬件和业务逻辑、分析性能问题等需要人的专业知识。自动化优化工具需要基于已有的知识和经验，而不是凭空生成优化方案。因此，即使技术不断发展，仍然需要工程师的专业知识来指导和验证自动生成的代码。未来，随着技术的发展和积累，可能会有更多的通用性优化工具出现，但在新的硬件和场景中，人工干预和专业知识仍然是至关重要的。这是一个逐步演进的过程。</p><p>&nbsp;</p><p>鱼哲：这是一个非常有趣的问题，因为实际上几乎所有的模型在实际应用中都需要经过优化才能顺利落地。在许多情况下，尤其是当涉及硬件时，例如服务器端或者嵌入式设备，优化工作变得尤为重要。</p><p>&nbsp;</p><p>举一个例子，最近非常受欢迎的 Vox 模型，它能够根据自然语言指令为机器人生成指令。然而，在将这一模型应用到嵌入式设备时，通常需要进行大量的优化工作。这种优化工作可能包括模型规模的压缩，性能优化，硬件加速以及适用于嵌入式设备的特定算法的选择。对于这种情况，通常需要工程师深入了解硬件的性能特征以及特定领域的需求。</p><p>&nbsp;</p><p></p><h4>活动推荐：</h4><p></p><p>11月2日，2023网易数字+大会将于杭州举办，网易数帆将带来AIGC技术与云原生、大数据、低代码结合的进展，下午的创新技术论坛，还将全面对外分享网易杭州研究院技术创新范式，带来领域大模型技术揭秘、开源实践分享等话题，欢迎扫描下图二维码或点击链接报名围观：i.163yun.com/obq6t7202</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/11c17401a1fe96ca78f916813cd03ee1.png\" /></p><p></p><p>&nbsp;</p><p></p>",
    "publish_time": "2023-11-01 15:50:20",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "最短3天内完成接入！腾讯云向量数据库正式对外全量开放公测",
    "url": "https://www.infoq.cn/article/oYlKQgWAzavx6Y9K7dfJ",
    "summary": "<p>11月1日，腾讯云对外宣布向量数据库正式全量开放公测，同时性能层面带来较大提升。腾讯云数据库副总经理罗云表示，除了公测之外，腾讯云向量数据库单索引已经支持百亿级向量规模，支持百万级QPS毫秒级查询延迟。</p><p>&nbsp;</p><p>从今年7月份正式发布以来，腾讯云向量数据库已经服务腾讯集团内部40多个业务，日请求量达1600亿次，服务的外部客户数也已经达数百家，帮助教育、SaaS、工具、游戏等多行业客户在快速的进行AI方向的探索,并首家通过中国信通院向量数据库测试。</p><p>&nbsp;</p><p>据罗云介绍，作为国内首个从接入层、计算层、到存储层提供全生命周期AI化的向量数据库，腾讯云向量数据库实现核心能力的跨越式提升。架构层面采用云原生分布式架构，达到企业级稳定性要求；独家集成Embedding功能，对齐传统数据库的使用体验，用户无需关注向量生成过程，就可以实现超高配速快速处理数据，助企业轻松拥抱AGI生态；此外，集合DMC可视化管理工具，快速执行数据库操作，让测试、运维更简单。</p><p>&nbsp;</p><p>有数据显示，将腾讯云向量数据库用于大模型预训练数据的分类、去重和清洗相比传统方式可以实现10倍效率的提升，如果将向量数据库作为外部知识库用于模型推理，则可以将成本降低2-4个数量级。企业原先将现有数据接入一个大模型需要花1个月左右时间，使用腾讯云向量数据库后，最短3天时间即可完成，极大降低了企业的接入成本。</p><p>&nbsp;</p><p>据介绍，向量数据库是通过把数据向量化，然后进行存储和查询，可以极大地提升效率和降低成本。它能解决大模型预训练成本高、存在幻觉、没有“长期记忆”、知识更新不及时等问题，突破大模型在时间和空间上的限制，加速大模型落地行业场景。在该领域，腾讯云目前走在业内前列，针对企业的普遍诉求，腾讯云第一时间联合合作伙伴推出了一系列端到端的向量数据库解决方案，加速向量数据库在企业的大规模应用。</p>",
    "publish_time": "2023-11-01 15:59:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国工商银行软件开发中心 BizDevOps 平台建设负责人吴宏招确认出席 FCon，分享中国工商银行 BizDevOps 平台建设实践",
    "url": "https://www.infoq.cn/article/Lc8G9BT9Fhd0IGlsEVIZ",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。中国工商银行软件开发中心 BizDevOps 平台建设负责人吴宏招将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5598?utm_source=infoqweb&amp;utm_medium=article\">中国工商银行 BizDevOps 平台建设实践</a>\"》主题分享，介绍中国工商银行 BizDevOps 的建设思路和方法、端到端的价值交付看板体系与可视化管理思想，以及大模型技术与数字员工在工银 e 企研中的运用场景。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5598?utm_source=infoqweb&amp;utm_medium=article\">吴宏招</a>\"，中国工商银行软件开发中心 BizDevOps 平台建设负责人，工银 e 企研的架构师，负责研发管理领域的数字化转型工作。他在本次会议的演讲内容如下：</p><p></p><p>演讲：中国工商银行 BizDevOps 平台建设实践</p><p></p><p>如何通过建设企业级的端到端价值交付平台和电子看板体系，建立起贯穿研发全生命周期的价值流模型，借助多层次的可视化手段和数字化能力，来解决业务和科技融合过程中的复杂管理场景以及大规模团队协作的问题。</p><p></p><p>演讲提纲：</p><p></p><p>中国工商银行 BizDevOps 的建设思路和方法端到端的价值交付看板体系与可视化管理思想大模型技术与数字员工在研发过程的实践场景</p><p></p><p>你将获得：</p><p></p><p>○ 学习大型银行如何通过创新的管理模式和丰富的技术手段，建立起面向业务价值交付的 BizDevOps 工具链</p><p>○ 面对大型研发团队，如何通过数据驱动的可视化手段，建立起智慧研发管理的自组织模式，来提升研发团队的研发效能</p><p>○ 了解大模型技术与数字员工在工银 e 企研中的运用场景</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 8 折优惠 ，立省 ￥1360！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-11-01 16:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "蚂蚁集团王磊：数据安全的多维策略——技术、管理与流程的协同作战",
    "url": "https://www.infoq.cn/article/TcacE56EJzDfMlOYs3q1",
    "summary": "<p>在数字化的时代，数据成为新的驱动力，但随之而来的数据隐私与安全问题也日益凸显。蚂蚁集团的王磊博士作为隐私计算领域的专家，不仅深度探讨了“隐语”的核心理念，更分享了其背后的起源与目标。他提到，“隐语”是蚂蚁开源的隐私计算框架，旨在通过技术手段确保数据在流通中的安全与隐私。而这背后，蕴含着蚂蚁对数据安全流通的前瞻性预判与坚定承诺。本文将深入探索王磊博士的这些独特视角，以及他对数据隐私与价值流通的深刻思考。</p><p></p><h4>Part1：数据要素与隐私计算技术发展</h4><p></p><p></p><h5>InfoQ：可否简述“隐语”项目的基本概念、起源和初始目标？</h5><p></p><p>王磊：『隐语』是蚂蚁开源的一套可信隐私计算框架，目标是通过技术信任解决数据要素流通过程中的数据安全和隐私保护问题，整个项目不仅包含了目前几乎所有主流的隐私计算技术，还提供了规模化生产所需要的稳定性和可视化能力。这个项目最早是蚂蚁的一个内部项目，早在2016年底，蚂蚁的领导者们就对数据安全流通的大方向做了预判，成立一个专门的项目组（叫共享智能），来探索如何实现数据的可用不可见，从而达到数据安全流通的目的。项目探索之初，主要是围绕蚂蚁内部的业务场景进行研究和孵化，随着技术的逐渐成熟，和国家数据要素流通战略的推进，蚂蚁在去年决定把这项技术开源出来，希望通过开源来加速隐私计算技术的发展，助力国家数据要素流通的战略。</p><p></p><h5>InfoQ：第一次听到“隐语”这个词语的时候，便非常好奇这个项目的名称，它有何特殊的寓意吗？</h5><p></p><p>王磊：隐者，信可留，数据保护可安全合规；语者，声可达，数据价值可有效流动。这是整个团队经过讨论碰撞后想出来的名称，希望能见名知义，体现出这个项目最重要的两个特点：1）支撑隐私保护和数据安全，2）助力数据要素价值流通。</p><p></p><h5>InfoQ：请分享一下从隐语项目开源到现在的关键里程碑及重大成果？</h5><p></p><p>&nbsp;王磊：自2022年7月隐语开源至今，经历了近10次的更新发版，我们一直在通过与广大开发者共建的过程中，持续调研与吸收建议，通过开源的形式也让更多的专业人才能够低成本地参与到隐私计算的技术贡献中，与隐语形成合力。以下列举其中几次发版进展，也可以看到隐语始终在演进与提升：</p><p>&nbsp;2022年 7月，我们开始开源，在功能方面推出了一系列联邦学习和隐私增强机器学习算法；在预处理方面，新增水平场景下的数据标准化、离散化、分箱功能，以及垂直场景下的相关系数矩阵、WOE分箱功能，无缝对接已有的Dataframe，提供和Sklearn一致的使用体感；在安全方面，隐语进一步完善了密态设备的架构体系，新增同态加密设备，该设备支持Paillier同态加密算法，并向上层提供Numpy编程接口。同时，新增差分隐私安全原语，因此增强了对拆分学习的隐私保护。</p><p>&nbsp;2023年3月 发布隐语V0.8.0版本。为满足多方协同的数据分析需求，隐语构建了全新的“基于多方安全计算的数据分析引擎SCQL”。隐语基于MPC技术内核的底层抽象SPU设备，创新实现了一种类似SQL的多方安全分析语言 \"SCQL\"。这种语言继承了 SQL 作为常用数据分析语言的普及性、易学性和高成熟度，同时还拓展了标准 SQL 的语义，可以描述基于多个数据源的安全计算，通过“SELECT FROM”、“JOIN ON”、“GROUP BY”等语句的组合搭配，即可完成联合分析的统计结果生成。</p><p>&nbsp;2023年7月 发布隐语V1.0 版本。不仅进一步扩大了开源范围，还对整体架构进行了调优拓展，核心内容涉及产品层、资源层、互联互通等板块，总体效果涵盖性能优化、易用性跨越式提升、互联互通形态丰富。隐语1.0 版本带来全新的 MVP部署体验包，将用户体验反馈转化为成熟的产品能力，站在隐私计算初学者的角度，尽可能将准备步骤嵌入安装部署流程，提高用户与隐私计算功能直接面对面的效率；</p><p>&nbsp;23年9月 发布隐语V1.1.0版本，隐语平台(SecretPad）正式开源，复刻 MVP 部署包已发布的全部功能，另新增节点注册功能，使 SecretPad 初步具备 PoC 演示能力，用户可根据自身需求在任一层级定制自己的想要功能，打造属于自己的隐私计算产品。</p><p></p><h5>InfoQ：隐语隐私框架在哪些行业或领域得到了较为广泛的应用？在实际应用过程中遇到的主要挑战有哪些？您和您的团队是如何克服这些挑战的？</h5><p></p><p>王磊：应用最广泛的是金融领域，场景主要是信贷风控和银行卡营销。此外在广告营销，保险核赔，联合医疗等领域都有比较广泛的应用。挑战主要来自于两方面。首先是技术上，作为前沿的技术方向，隐私计算在一些能力上是不太能满足业务场景的需求的，我们尝试从多方面入手去解决这些问题，包括但不限于对需求场景进行重构、在隐私计算的各条技术路线上进行技术突破、融合多条技术路线扬长避短；其次是在心智上，因为隐私计算核心是要确保数据的安全流通，但安全在没有出问题前，在用户侧是很难被感知的，这导致很多技术和产品片面地为了追求性能和效果，在安全上混水摸鱼，我们一方面通过各种科普和宣传，加强用户的安全心智，另一方面也努力通过技术上的研究和突破，让隐语在确保安全水位不降低的前提下，性能和效果都有较大程度的提升，满足用户的场景需求。</p><p></p><h5>InfoQ：在项目开展中，有哪些实际应用场景对项目理论研究和技术升级产生了推动？</h5><p></p><p>王磊：比如在做金融风控的场景时，由于专线带宽的限制，基于MPC的金融风控建模性能无法满足要求，于是我们针对风控常用算法上进行了攻坚，通过深入的优化，最终在性能上满足了业务需求，其中LR和XGB两个算法分别被KDD21和CIKM21录用。早期我们支持各种场景算法时，每个算法都是从上到下完全自己开发，开发和维护成本极大，后来团队同学创新性地提出了基于编译器的SPU架构，使得上层可以复用现有的机器学习框架，效率和扩展性得到了极大的提升，这个工作也被USENIX ATC23录用。类似的场景推动技术升级的例子还有很多，本质上是场景的输入帮助技术指明了发展的方向，而技术的突破又能帮助场景落地实现技术价值。</p><p></p><h5>InfoQ：在进行隐私计算过程中，如何确保数据的安全性并符合相关法规？</h5><p></p><p>王磊：传统的系统安全主要聚焦在存储和传输过程中的安全性，针对的是系统外部的攻击者。而狭义的隐私计算主要是指计算过程中的安全性保障，主要针对的是系统内部的计算和数据参与者。两者的目标是不同的，因此从安全保障上，需要两种技术同时使用。而广义的隐私计算还包括受控匿名化，数据跨域管控等技术，这些技术是针对当前个保法，数据二十条等法规如何落地实现的重要探索。</p><p></p><h4>Part2：数据要素行业的未来展望</h4><p></p><p></p><h5>InfoQ：在全球化数据交流的大背景下，数据要素行业未来五到十年内将面临的主要挑战和机会是什么？</h5><p></p><p>王磊：挑战主要来自于几方面。1）法律法规的完善。与其他要素相比，数据有其独特的特点，如何在法律法规层面支撑数据要素合法合规的流动是一个巨大的挑战，当前的三权分置是一个很好的尝试。2）数据要素的应用场景能不能丰富起来。目前数据的商业化应用主要还是集中在金融和营销领域，使用的都是个人数据，而个人数据又面临着隐私保护的问题，探索和实践新的数据要素使用场景，激发数据要素多样化的商业需求，从而推动更丰富的、有商业化价值的数据产生，是未来数据要素行业做大的一个重要条件。3）技术能力上是否能支持大规模的数据要素安全流通。确保数据安全是数据要素大规模流通的一个前提条件，作为数据安全技术之一的隐私计算，目前还不具备支撑大规模数据安全可信流通的能力，技术上的突破也会是未来需要解决的挑战之一。</p><p></p><h5>InfoQ：有哪些新技术或新理论可能在未来改变数据要素行业的格局？</h5><p></p><p>王磊：当前的每条技术路线都有其优劣势，也都在进行持续的探索，哪一条技术路线未来能够有所突破并能找到适合自身应用的大规模商业化场景，目前看还是不清晰的。对于多种技术路线的融合，比如蚂蚁的TECC技术，学术业的PPMLAC技术，也是大家尝试的方向。可能未来行业的应用并不会是基于某一种技术，更多的是通过多种技术融合来解决行业问题。</p><p></p><h5>InfoQ：对于推动数据要素流通共享，金融企业应该在技术和管理上做出哪些准备和改进？</h5><p></p><p>王磊：数据要素流通共享是国家的趋势，有可能会改变很多行业的格局，提前进行针对企业自身场景的探索和布局可能会应用未来变化的重要手段。此外，作为数据要素流通的关键技术之一，企业首先需要对隐私计算技术，尤其是对安全假设和其实现的安全性，有更多的理解，这样才能根据自身的场景做更好的选型，避免在未来数据要素流通过程中因为隐匿的安全问题导致大规模数据泄露。</p><p></p><h5>InfoQ：在数据共享过程中，应该如何更好地保护数据提供方的利益，防止数据被滥用？</h5><p></p><p>王磊：确保数据安全是一个非常复杂的过程，涉及到管理、流程、技术、意识多个方面。传统的数据共享防止滥用通常是通过沙箱技术来实现的，未来在一部分安全要求不高的场景中，沙箱和运维权限的限制仍然会是一个重要的手段，但是对于一些安全性要求相对较高的数据，通过传统系统安全迭加隐私计算技术，用技术信任确保数据使用过程受控，避免数据被无端复制和滥用。</p><p></p><h5>InfoQ：在您看来，隐语项目未来的发展方向和重点将是什么？</h5><p></p><p>王磊：隐语未来会在以下三个方向持续进行建设。1）能力持续完善。在当前主流的隐私计算技术路线上持续进行探索和突破，并将能力沉淀在隐语框架上，赋能更多的行业。2）生态持续建设。成功的开源项目离不开广泛的生态，隐语社区在开放标准，社区SIG，开发者共建等方面也会持续投入，希望能形成围绕隐语的广泛的开源生态。3）场景持续探索。以终为始，隐语开源的初心就是希望通过开源和技术共建，能够赋能国家的数据要素市场化的宏伟战略，未来也会在数据要素的场景上持续进行探索，希望能为更多有价值有意义的场景进行赋能。</p><p></p><h5>InfoQ：基于您的专业经验，您对于金融行业在数据隐私保护方面有哪些特别的建议或警示？</h5><p></p><p>王磊：其实金融在数据隐私保护方面属于相对先行的行业了，于金融机构而言，他们在业务运营中过程中已经沉淀了相当多高质量、高价值数据可以被挖掘，但是仅立足于自身的数据也是不够的，往往需要通过建立更加开放的金融生态，最大化发挥跨域数据要素价值。那在这个过程中，金融机构势必需要关注如何在保护用户隐私和数据安全的前提下，让数据要素实现安全合规的流通。譬如联合风控就是隐私计算应用在金融行业的一个典型场景，并且隐语也已经有了相关落地。在某个实际应用案例中，两家银行通过蚂蚁风洞多方安全计算平台，基于隐私计算框架“隐语”进行了多方安全联合建模，针对贷前A卡、贷中B卡分别进行了建模，与单一机构提供的数据源建模效果相比，双方联合建模的模型KS大大提升，成效显著。</p><p></p><p>嘉宾介绍：</p><p>王磊博士蚂蚁集团隐私计算部隐语总经理、隐语开源框架负责人</p><p>浙江大学计算机博士，2022 年《麻省理工科技评论》中国隐私计算科技创新人物。多年来牵头并主导隐私计算框架“隐语 (SecretFlow)”的一系列研发创新、开源共建及产业落地应用。拥有海内外隐私计算相关授权专利 41 项，带领团队于 AAAI、NeurlPS、KDD、USENIX ATC、S&amp;P 等国际顶会/期刊发表论文 20 余篇，主导和参与制定国际/国家/行业/团体标准 11 项。</p><p></p><h5>活动推荐：11 月 19-20 日，首届 FCon 全球金融科技大会将落地上海，届时，王磊老师也会到场与大家进行交流。由王磊老师与InfoQ联合策划的【数据要素流通与数据合规】专题，邀请了多位大咖进行演讲分享，期待你可以从他们的交流中获得启迪。</h5><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/de39778f7e600ebc6a496dc6328c960b.png\" /></p><p></p><p>扫码或点击「阅读原文」可查看全部演讲专题，咨询购票请联系：17310043226（微信同手机号）。</p>",
    "publish_time": "2023-11-01 16:10:43",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023云栖大会公开课《AI时代数据库新格局》上线啦！",
    "url": "https://www.infoq.cn/article/s0Na8Qj6SCdXVZ8lcp1F",
    "summary": "<p>AI时代浪潮翻涌，数据库格局创新变革！《明说三人行》栏目创始人&amp;主持人、大数据人工智能资深媒体人明叔与你一同解码AI时代中国数据库新格局！</p>",
    "publish_time": "2023-11-01 18:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "2023云栖大会公开课《向量数据库——解锁非结构化数据的秘密》上线啦！",
    "url": "https://www.infoq.cn/article/9CRsA2ks2JhTXX4uCFi7",
    "summary": "<p>2023云栖大会公开课《向量数据库——解锁非结构化数据的秘密》上线啦！</p>\n<p>聚焦大模型时代向量数据库的基本原理及应用场景，Zilliz技术合伙人兼技术总监栾小凡为你揭秘如何通过向量数据库实现语意检索的创新实践！</p>",
    "publish_time": "2023-11-01 18:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]