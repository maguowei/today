[
  {
    "title": "Rust 引领前端基建新潮流：字节跳动的深度应用实践",
    "url": "https://www.infoq.cn/article/V10gup6lMqU0oiGlRKYc",
    "summary": "<p>演讲嘉宾 | 张磊 字节跳动 /Web Infra 前端负责人</p><p>整理｜Penny</p><p>编辑｜Kitty</p><p></p><p></p><blockquote>近年来，XR 技术兴起以及人工智能技术的高速发展，为大前端领域注入了新的活力。这些技术不仅为用户带来了更加沉浸和智能的体验，也为前端、客户端的开发者们开辟了全新的视野和可能性。欢迎关注即将于 10 月 18 -19 日召开的 <a href=\"https://qcon.infoq.cn/2024/shanghai/\">QCon 上海站</a>\"，我们将在【<a href=\"https://qcon.infoq.cn/2024/shanghai/track/1718\">新技术浪潮下的大前端机遇与挑战</a>\"】专场为大家奉献 XR 、 AI 与大前端技术相结合，创造出的令人激动的新应用和解决方案落地实践。目前购票可享 9 折优惠，感兴趣的同学可访问<a href=\"https://qcon.infoq.cn/2024/shanghai/apply\"> QCon 官网购票页面</a>\"了解详情。</blockquote><p></p><p></p><p>本文整理自字节跳动 Web Infra 前端负责人张磊在 QCon 2024 北京 的演讲分享“Rust 如何引领前端基建新潮流以及字节跳动的应用”。</p><p></p><p>大家好，我是张磊，目前担任字节跳动 Web Infa 团队的前端负责人。今天，我非常高兴能与大家分享 Rust 语言在前端领域的发展趋势以及字节跳动在这方面的实践经验。</p><p></p><p>本次分享将分为三个主要部分：</p><p></p><p>Rust 在前端领域的发展：介绍 Rust 在前端行业的发展情况，重点放在工程化应用上。字节跳动的实践案例：分享字节跳动在 Rust 前端开发中的一些具体实践。Rspack 项目中的架构设计：深入探讨我们在 Rspack 项目中采用的中等复杂度系统的架构设计方法。</p><p></p><p>我们团队负责维护和开发多个开源项目，包括 Rspack、Rsbuild 和 Modern.js 等，这些项目为公司内部超过 5000 名前端研发人员提供了工程化和微前端解决方案的基础设施。目前，我们团队还在积极探索 AI 领域的相关工作。我们团队也在积极招募架构师等相关岗位的人才。如果大家对此感兴趣，欢迎通过 GitHub 关注我们的项目：<a href=\"https://github.com/web-infra-dev\">https://github.com/web-infra-dev</a>\"。今天的分享内容可能涉及到一些我们团队的利益相关部分，但我会尽量保持中立和客观，与大家分享我对这个行业的理解和见解。</p><p></p><h4>Rust 在前端行业的发展</h4><p></p><p></p><p>让我们从 Rust 的发展开始谈起。在我们开源 Rspack 项目之初，我们撰写了一篇文章，阐述了我们为什么要启动这样一个项目。文章中有一段提到：在生产环境中，一些应用程序的构建时间经常需要 20 - 30 分钟。这段话发布后，在业界引起了不小的反响。有些人质疑，一个项目怎么可能需要构建 30 分钟？我们究竟在做什么项目？怎么可能会有 50000 个模块？使用 Rust 来处理这样的问题，是否有些小题大做？面对这些反馈，我们也曾陷入自我怀疑，我们的做法真的这么糟糕吗？怎么会有这样的问题？</p><p></p><p>Rspack 开源之后，许多开源社区的团队联系了我们。我们发现，面临这种问题的并不只有我们。例如，Kibana 项目，这是一个在 GitHub 上开源的项目。当项目作者联系我们时，他们告诉我们整个项目的构建时间接近 60 分钟。该项目包含 4,999,000 行 TypeScript 代码，有 5 万个 TypeScript 文件，整个项目已经迭代超过 11 年。另一个例子是 Discord 项目，这个项目也非常复杂。在接入 Rspack 之前，它在 CI 环境中的构建时间需要 11 分钟。接入 Rspack 之后，构建时间缩短到了 3 分钟。还有一个案例来自我们社群中的一位海外成员。他在描述项目中说：big legacy frontend monolith project.（大型遗留前端单体项目）。他说，在接入 Rspack 之前，该项目在 CI 环境中的构建需要将近 1 小时，而在接入 Rspack 之后，构建时间缩短到了 9 分钟。</p><p></p><p>我们总结了一下，在前端环境中，确实存在许多一次性应用。我们可能开发一段时间后就上线，然后就停止了，不再需要迭代和维护。但前端也会存在像 Super APP 这样的场景，比如我们在字节跳动内部使用的飞书文档，以及刚才提到的 Kibana 等项目。这些项目的生命周期非常长，需要长时间的迭代。随着迭代的进行，项目也会不断膨胀，可能会因为各种 A/B 测试等原因，代码量越来越多。</p><p></p><p>作为公司的中台部门，我们经常接到业务团队的请求，希望我们帮助他们优化构建体验，特别是解决构建时间过长的问题。同时，我们也在深入思考前端工程化的未来发展方向。我们观察到自 2012 年以来，前端的基础语法和工具生态都在迅速发展。到了 2020 年，整个前端基础设施变得越来越稳定。例如，我们依赖的 JavaScript 语法提案逐渐减少，这种稳定性也带来了前端工具的变化，稳定且标准化的组件开始逐渐原生化。</p><p></p><p>现在，我们可以看到，在基础语言编译器方面，如下图右侧上方所示，SWC 已经完全支持 HTML、TypeScript 和 JavaScript 等代码的编译。在图的最下方是 Bundler 部分，Rspack、Rolldown 和 esbuild 等项目已经完成了 Bundler 部分的原生化。有些部分的原生化仍在推进中，比如左上角的 Transformer 部分。这部分 Transformer 主要指的是 UI 框架的 Transformer ，例如 Vue、Solid 和 Svelte 等，它们依赖的编译器目前主要是用 JavaScript 编写的。当然，许多团队和社区成员正在探索原生化的道路，这是一个持续发展的过程。图中左下角我们常用的开发框架，如 Next.js，仍然处于 JavaScript 生态系统中。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/6a/6afbe7d68d7489de45136196530571e0.png\" /></p><p></p><p>分享一个我们团队的观点：JavaScript 和 Rust 在长期内将会共存。你可能会通过各种插件 API 或其他 API 将它们转化为性能更优的版本。在一些控制力强的场景，比如小程序或跨端框架场景中，会出现很多完全使用 Rust 的工具的团队。</p><p></p><p>站在今天这个时间点，回顾我们开始 Rspack 项目之前，我们首先需要思考一个问题：为什么选择 Rust？在决定选择 Rust 之前，我们需要先了解 JavaScript 这门语言存在哪些问题。</p><p></p><p>我们主要在 Node.js 环境中使用前端工具。Node.js 的第一个主要问题是多线程支持。尽管 Node.js 提供了工作线程 API，但直接在工作线程之间共享内存并不容易。虽然我们可以使用 SharedArrayBuffer 来实现共享内存，但这需要手动处理同步和并发等问题。第二个问题是语言性能。JavaScript 或者我们常用的 V8 虚拟机在性能优化方面存在一定的不可控性。性能优化往往难以预测和控制。第三点是垃圾回收（GC）。我们曾经遇到过一个有趣的案例：一位开发者反馈他的构建工具性能非常不稳定，HMR 的速度时快时慢。我们对其进行了性能分析，发现有时 HMR 的耗时会达到 2 秒，其中大约有 800 毫秒是花在了垃圾回收上。作为 JavaScript 开发者，我们无法控制 GC 的触发时机，这在整个构建过程中是一个相当棘手的问题。</p><p></p><p>在认识到 JavaScript 存在一些问题后，我们面临选择：Golang、C++、Rust，为什么我们最终选择了 Rust，或者说为什么社区倾向于选择 Rust？我认为主要有四个原因。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/41/410288e9d5c0d890bf80bdd371d0a2f6.png\" /></p><p></p><p>第一，工具链完善。Rust 的开发主要依赖于两个工具：Cargo 和 Rust Analyzer。Cargo 本身内置了 37 个命令，涵盖了开发过程中所需的大部分功能。学习 Cargo 一个工具，基本上就可以解决所有问题。而 Rust Analyzer 工具，特别是在使用 &nbsp;VS Code 插件开发时，为我们提供了代码提示和诊断功能。</p><p></p><p>第二，生态丰富。我昨天在 Cargo 搜索与 JavaScript 相关的包，发现有 1856 个。而且，对于我们做 Bundler 这类基础设施所依赖的包，如 Parser 等，社区已经建立了非常成熟的生态。例如，esbuild 虽然内置了 Parser，但它是与 esbuild 工程紧密绑定的，难以单独使用。而我们现在依赖的 Biome 和 &nbsp;Lightning CSS 等，整个生态已经相当完善。</p><p></p><p>第三，多线程支持。Rust 的官方文档中有专门章节讨论 “fearless concurrency” 无谓并发），意味着开发者不需要担心并发问题，Rust 已经内置了解决方案，如 send 和 sync 特性，编译器会自动进行检查。</p><p></p><p>第四，对 WebAssembly（Wasm）友好。虽然许多静态语言都对 Wasm 友好，但 Rust 可能更为突出。Wasm 友好的价值在于，我们开发的工具可以方便地被用户用于自定义需求或进行代码反馈。</p><p></p><p>Rust 语言发展至今，已经达到了一个相当不错的阶段。在前端社区中，一些非常流行的由 Rust 等原生语言编写的工具包括 Bun、Turborepo、pnpm、Rspack、Rolldown、esbuild、Turbopack、Biome 和 Lightning CSS 等。从我们的观察来看，在前端的日常开发中，我们主要使用的是五个命令。</p><p></p><p>第一部分：我们从 npm install 命令开始讲起。 最初学习时，我们知道 install 命令用于安装依赖。但随着 Monorepo 类工具的发展，npm install 的过程已经不仅仅局限于安装依赖，它还包含了 task runner 等基础功能。在这个方向上，以 Bun 为例，它已经将 package manager 和 Task Runner 的功能完全内置。而 Turborepo 项目，它主要提供 Task Runner 的功能。目前，我们接触到的大多数项目都是以 Task Runner 功能为主。至于 package manager 的实现，对应的是像 pnpm 这样的项目。但值得注意的是，pnpm 的 Rust 版本目前迭代并不活跃。</p><p></p><p>第二部分：npm run fmt。 过去我们主要使用 Prettier 这样的工具进行代码格式化。去年，社区发起了一个挑战，宣称完成 Prettier 100% 的测试用例可以获得 2 万美金。许多社区内的开源团队积极参与了这个项目。最终，Biome 团队赢得了挑战。目前，我在做工程时，也主要使用像 Biome 这样的工具来进行代码格式化。</p><p></p><p>第三部分：npm run lint。Lint 功能主要分为两部分，一部分是代码检查，这部分已经非常成熟。社区内目前有两个主流的 lint 工具，一个是 OXCLint，另一个是 Biome 的 lint。但它们被标记为黄色，主要是因为它们依赖的 Type Checker 功能，目前社区内还没有任何一个项目能够完全实现。过去，SWC 的作者曾探索过 Rust 版本的 Type Checker，名为 STC，但该项目目前已经停滞。我认为，这个工程的整体工作量非常大，实现起来不太现实。</p><p></p><p>第四部分：npm run test。 测试方向也分为两部分内容。一部分是基本的测试驱动器，即扫描到 test 文件并运行它们。Rust 在这一方面的实践或工具相对较少，但我们可以通过依赖底层的 Parser 来完成这一部分。例如，swcjs 的插件，只需挂载在 JavaScript 上就可以使用。</p><p></p><p>最后一个部分：npm run build。 这是我们开发中主要接触的，也是我们这些开发者们主要忙碌的事情。在 Meta framework 上，它主要依赖两部分。首先是 Bundler 领域，这是目前 Rust 化内竞争非常激烈的一个领域，我们目前也在进行 Rspack 这样的项目。其次是对 JS 语法进行 Transformer。Transformer 分为两部分：一是 UI 部分的 Transformer，这部分目前还是探索项目，还没有完全 Rust 化的解决方案。二是语法本身的转换，如 ES6 转 ES5，或者高级语法转低级语法，这方面 SWC 已经相当完善，OXC 也在进行这方面的探索，但目前还没有完成。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b7/b7ea509c79d600d0af189bcd9063827b.png\" /></p><p></p><h4>字节跳动的实践</h4><p></p><p></p><p>目前，Rust 在前端工程化领域的发展现状是令人鼓舞的。字节跳动在这方面的进展也大致相同。接下来，我想分享我们在采用 Rust 过程中的心路历程。字节跳动的实践可以分为三个阶段：esbuild 阶段、Rspack 阶段和全面采用 Rust（All in Rust）阶段。让我们从 esbuild 阶段开始说起。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d2/d225ad2660df183a0dac716eff7ee10f.png\" /></p><p></p><p>图左上角的白色 logo 是我们公司内部的跨端框架 Lynx，它适用于跨端场景。2020 年，抖音在春节期间的主要工程基础设施就是基于 Lynx。当时我和团队负责 Lynx 的工程化工作。我们最初使用的是基于 Rollup.js 加上一些插件来完成基础环境的打包。由于人力充足，加上春节项目的重要性，我们得到了良好的支持，并迅速推进了这个项目。项目推出后，业务反馈非常积极。尽管参与春节项目的人员众多，但整个研发团队的规模庞大，可能占据了一整层楼。随着代码量的增加，我们并没有观察到性能明显下降，而且 CI/CD 的发布时间也显著缩短。因此，我们首次尝试这种新方法，感觉非常新鲜且效果良好。春节结束后，我们希望将这种能力扩展到 Web 生态。我们想做的第一个项目是抖音的购物车，是我们公司在 Web 场景上首次尝试使用 esbuild 的项目。然而，这也是最后一次，因为它最终失败了。失败的原因我在这里总结如下。</p><p></p><p>首先，我们来对比跨端场景和 Web 生态的不同。 在跨端场景中，运行时环境相对受限，可用的插件较少，因此大部分业务功能需要由跨端框架的开发者来实现。这为我们提供了良好的控制性和较少的定制性需求，使得实现起来相对容易。在 Web 生态下，我们能够利用的社区插件功能非常丰富。业务上也有许多自定义插件，这导致 JavaScript 部分和原生部分需要频繁交互，进而使得性能提升不是很明显。</p><p></p><p>第二个问题是热模块替换（HMR）的支持。 在跨端场景中，例如 Flutter 和 Lynx，对 HMR 的支持并不特别友好。它们大多采用 LiveReload 的方式进行更新，而在 Web 场景中，HMR 是一个非常重要的特性。尽管 esbuild 最近在这方面有所改进，但与 Rspack 等项目相比，其支持程度仍有较大差距。</p><p></p><p>最后一个，也是最关键的问题，是关于打包。 在跨端场景中，通常只有一个 bundle，所有代码都加载到一个 bundle 中，类似于小程序的场景，我们不需要进行非常精细的分包或对产物进行极致优化。但在 Web 场景下，情况就大不相同了。在 Web 生态中，Webpack 在这方面的功能已经非常成熟。如果我们在创建上有所变化，导致运行时加载性能下降，对业务的影响将是致命的。特别是对于抖音购物车这样的 C 端场景，性能劣化将引起严重问题。因此，我们在 esbuild 方面的探索最终以失败告终。</p><p></p><p>我们对工具原生化这个方向非常认可，认为这是值得深入探索的领域。因此，在下一个阶段，我们进行了调研，发现社区内并没有能够解决我们需求的工具，于是我们决定自己开发一个名为 Rspack 的项目。项目启动初期，在公司内部遇到了很多质疑的声音，我们自己内心也有所疑虑。这里，我想分享以下三个观点。</p><p></p><p>工具使用者与开发者应使用同一编程语言。 这个观点有一定的道理，因为如果双方使用相同的语言，开发者可以更好地理解使用者的痛点，了解 JavaScript 生态的现状。然而，也有不同的情况，如果开发者使用与使用者不同的语言，可能有助于拓展工程视野。分享案例。 在公司内部，我们有许多组件库和基础工程库，这些库的构建使用的是我们的基础工具。我们在 JavaScript 社区内对类似工具进行了调研。但当我们团队开始使用 Rust 开发时，发现情况有所不同。例如，Cargo 集成了文档和测试功能，这些集成显著提升了库的开发体验。通过使用 Rust，我们学习到了新知识，拓宽了视野，这也有助于 JavaScript 社区生态的发展。跨语言学习的价值。 以 Bun 工具为例，Bun 的发展目标与 Cargo 类似，旨在集成包括运行时、包管理器、任务运行器在内的所有工具。这表明，即使工具使用者和开发者使用不同的语言，也可以通过跨语言学习获得宝贵的洞见。此外，React 团队在开发过程中不仅仅局限于 JavaScript 生态，他们也会研究 iOS 和 Android（如 Jetpack Compose）的实现方式。我们公司内部也在思考跨端框架的未来发展方向。</p><p></p><p>JavaScript 本身并不慢。实际上，工具的性能并不完全取决于所使用的语言。只要架构设计合理，就能发挥出良好的性能。这个观点有一定的正确性，但也存在错误之处。正确的地方在于，通过设计更好的架构确实可以实现更好的性能。错误之处在于，语言本身的限制可能会影响架构设计的能力。例如，JavaScript 在多线程方面的限制使得我们难以实现多线程操作。但是，有了 Rust 这样的工具后，我们可以更灵活地处理多线程问题，实现各种生产者 - 消费者模型等。</p><p></p><p>在 Rspack 的内部实现中，我们已经能够充分利用当前计算机的 CPU 性能。接下来，讨论 Webpack 的性能问题。Webpack 的慢并不是因为 Webpack 本身，而是慢在了插件上。我们在实际使用 Webpack 的经验中发现，如果不配置任何插件，它的性能实际上是可接受的。但随着插件的增加，性能就会下降。因此，在开发 Rspack 时，我们考虑了这个问题，并决定将更多插件功能内置化，将社区内已经标准化的功能整合进来。使用 Rspack 的用户可能会注意到，它内置了很多 loader。这些 loader 并不需要进行 AST 或字符串的序列化和反序列化操作，它们仅仅是作为一个标识符存在，从而提高了性能。</p><p></p><p>当前社区在推进打包（bundle）技术的方向上，有一个非常极致的目标，那就是实现一次性处理的打包。这意味着我们不需要对单个文件进行反复的编译和代码注入（Code In），而只需通过一次处理就能完整地完成整个打包过程。在明确了这些目标之后，我们的项目也逐渐取得了成功。现在，大家可能已经在社区中看到了我们的项目成果。同时，我也想借此机会感谢大家过去对我们的支持和帮助。</p><p></p><p>在字节跳动内部，我们很快就从业务上获得了实际收益，这也为我们迎来了下一个发展阶段，即全面采用 Rust（all in Rust）。我们计划推进的目标是，将所有基于 Rspack 的 Rust 基础工具覆盖到我前面提到的主流场景。我们正在推进的一个重点是，在控制力较强的场景中，如跨端开发和小程序开发，实现全面采用 Rust 工具。</p><p></p><p>接下来，我想分享一下我们在做这件事情时给公司带来的一些收益。首先，最明显的收益是性能方面的提升。许多业务在接入 Rspack 后，获得了大约 10 倍的性能提升。例如，CI/CD 构建时间明显缩短，本地开发时间也显著减少。这种性能上的提升进一步引发了后续的传导效应，即对工作流程（workflow）的优化。</p><p></p><p>aPass 团队负责的工程是一个在字节跳动内部经过长时间迭代、涉及多个团队的复杂单仓库项目。过去的工作流程是：首先进行一次发布，记录当前发布所包含的所有包和子包信息，这些信息会以 JSON 格式保存。一周后再次发布时，他们会拿出之前的 JSON 文件进行对比，以找出增量变化的包，检查是否有重复引入的包，例如 React 是否引入了重复的依赖等。但这种实践存在一个明显问题：经过一周的时间，如果团队有 50 人，每人提交了一个更改（commit），就会有 50 个 commit。在这种情况下，要定位到底是哪个 commit 引入了问题变得非常困难。为了解决这个问题，我们引入了新的工作流程。过去无法实现这种优化的主要原因是构建过程耗时太长。例如，aPass 项目的关键构建可能需要 30 分钟，合并请求要等待 30 分钟这是不可接受的。优化后的工作流程基于 Rspack。现在，一次检查只需要一两分钟，大大提高了效率。Rspack 会生成一个 Rspack.json 文件，它可以精确地告诉你是否有重复的包出现，哪个包体积变大了，从而可以进行有效的工作流程优化。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/e8/e8511985f3919249ec18baee36700c39.png\" /></p><p></p><p>接下来，我想和大家分享我们在推进这项工作过程中遇到的挑战。主要有两个问题。首先是 Rust 工具的使用者面临的调试难题。我们过去分发的是 JavaScript 源码，现在转变为分发二进制产物。如果业务在构建过程中遇到错误，比如 segmentation fault 或其他问题，他们很难像以前处理 JavaScript 那样自行调试。使用 Rspack 后，这种调试能力似乎丧失了。我们作为中台需要支持业务的 OnCall 需求。为了防止二进制分发带来的 OnCall 问题，我们开发了一个调试工具。目前，这个工具已经开源：</p><p>rsdOXCtor.dev。在设计这个工具时，我们的理念与 Chrome 团队相似。Chrome 是用 C++ 实现的，但我们并不觉得它的开发体验差，也不认为它难以使用。核心原因在于 Chrome 的开发工具做得非常好，使我们能够方便地了解其中的所有流程。同样，我们开发的这个工具旨在将 Rspack 内部的所有流程白盒化。我们会展示每个 loader 是如何处理的，流程是如何串联的，文件是如何被发现和加载的，以及整个处理流程的细节。这样，用户就能更好地理解和使用 Rspack。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/c5/c558ad0dfb352c41bc98144b8ebf3abb.png\" /></p><p></p><p>第二个问题涉及到集成方面的挑战。我们的工具和框架是分层构建的，正如大家之前在图上看到的，我们有多种上层解决方案。作为一个基础的 Rust 库，当我们考虑向上层集成时，我们面临多种选择。主要的集成方案有四种：NAPI（Node.js API）和 RPC（远程过程调用）主要用于与 JavaScript 的集成；源码集成和 Wasm（WebAssembly）集成则更适用于 Rust。</p><p></p><p>NAPI 这种方案目前在 Rspack 和 Rolldown 等项目中得到了应用，并且在社区内属于相对主流的集成方式。它的优点在于整体性能表现良好，尤其是与 RPC 方案相比，NAPI 在性能上具有优势。NAPI 方案也存在一些缺点。首先，执行环境高度依赖于 NAPI 的实现，这可能限制了其灵活性和可移植性。其次，NAPI 本身涉及大量 unsafe，这意味着它更易受到内存相关的错误影响，增加了开发和维护的复杂性。</p><p></p><p>第二个方案是 RPC 方案，目前 esbuild 和 Turbopack 正在使用这种方案。它的优点主要包括两个方面：</p><p></p><p>调试优化：RPC 方案对于基础的 Rust 开发者来说，在开发像 esbuild 和 Turbopack 这样的工程时，提供了便利的调试体验。例如，设置断点和进行调试相对容易。兼容性友好：RPC 本质上是一种跨线程的通信机制，类似于通过 HTTP 进行的通信。因此，它对运行环境的依赖性较低，使得它在不同环境中具有较好的兼容性。</p><p></p><p>RPC 方案也存在缺点，尤其是在处理复杂对象时。如果需要将结构化的对象序列化传输，然后在另一端进行反序列化，这个过程可能会变得相当复杂和难以处理。</p><p></p><p>对于 Rust 版本的集成，实际上有两种主要方式：源码集成和 Wasm 集成。</p><p></p><p>源码集成 的一个例子是淘天集团正在开发的 IcePack 项目。源码集成的过程相对直接：将 Rust 代码合并到项目中，根据需要进行修改，然后发布一个新的包。这种方法的优点在于它提供了最佳的性能。但同时，它也存在一个问题，即集成的插件不是动态化的。这意味着，每当 IcePack 发布更新时，你需要自动滚动到一个新的 commit，以便获取更新后的代码和功能。</p><p></p><p>Wasm 集成 则是通过 Wasm 插件实现的，这是 SWC 目前主要推广的方案。Wasm 集成支持动态化，并且性能表现也相当不错。不过，它的缺点在于需要基础库的开发者确保 ABI 的前后版本兼容性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/a7/a7e8d93acf313be38baaa01c7c36854c.png\" /></p><p></p><h5>Rust 中等复杂系统的设计</h5><p></p><p></p><p>前面我分享了我们在实践中遇到的问题和一些收益。接下来这部分内容会更加深入和实用，我将和大家探讨在开发 Rspack 这个中等复杂度系统时遇到的架构设计问题。为了更好地理解这些内容，你需要具备一些基础的 Rust 知识，比如了解所有权、生命周期、引用借用以及内部可变性等概念。</p><p></p><p>我们从熟悉的 Webpack 开始。Webpack 包含一个复杂的对象网络，对象之间存在复杂的引用关系。在 Webpack 中，这些模型得到了很好的抽象和建模，并且已经能够支持我们所有的业务需求。作为 JavaScript 开发者，垃圾回收（GC）的概念贯穿于我们工作的各个环节，无论是代码编写还是架构设计，都依赖于 GC 的存在。然而，在 Rust 中，情况有所不同。Rust 强调所有权概念、借用检查和编译时检查，这使得我们无法直接复制 Webpack 的完整模型。同时，我们又需要对标 Webpack，那我们应该怎么做？</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/f2/f2a94ad0825c456cc221833b928f3a5e.png\" /></p><p></p><p>我们先将问题简化，从内存被两个对象同时持有的情况开始讨论，这种情况称为多引用。在 Rust 中，我们可以通过生命周期（lifetime）来标记引用，以确保内存安全。生命周期使用一个撇号（’）加上小写字母表示，例如 A’c，它表明对象 a 引用了对象 c，且 c 的生命周期必须长于 a。同样，我们也可以让 a 和 b 都引用同一个对象 c。</p><p></p><p>这种方法在 Rust 中很常见，例如在 Lightning CSS 中就存在大量这样的代码。其优点是性能良好，几乎不会带来性能问题。然而，它也存在一些问题。首先是生命周期的复杂性，因为生命周期是 Rust 语言的一个强传导性特性。例如，在 Lightning CSS 中，一个结构体的实现可能已经包含了 6 个生命周期参数，这增加了开发和维护的成本。如果你需要修改一个生命周期参数，它可能会引起广泛的连锁反应。其次，引用在多线程环境下会带来限制。如果我们想构建一个复杂的多线程架构，引用的使用可能会限制我们在架构设计上的灵活性。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/b9/b9bbbb74d92d8cc875ec479ff3af09de.png\" /></p><p></p><p>Rust 提供了另一种解决方案，即智能指针，我们可以通过 Rc（引用计数）和 Arc（原子引用计数）来标记一个对象，从而绕过对生命周期的检查。Rc 是 “Reference Count”的缩写，而 Arc 是 “Atomic Reference Count” 的缩写。两者的区别在于 Arc 是线程安全的，而 Rc 则不是。它们内部通过 unsafe 代码块来跳过编译器的某些安全检查。使用 Rc 可以让我们实现对象 a 和 b 同时引用对象 c。然而，这种方法引入了新的问题：Rc 本身是不可变的，因为在 Rust 中，可变性是一个需要明确检查的概念。尽管如此，Rc 和 Arc 在行业内被广泛使用。例如，在 Biome 项目中就有许多基于 Rc 的代码。但是，由于 Rc 的不可变性，它不允许对象被修改。在进行复杂的架构设计时，我们通常希望能够修改对象。如果我们不能修改对象，这在实际应用中往往是不可接受的。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4f/4f6c8d6e5263e3eb3b36076105f1540c.png\" /></p><p></p><p>Rust 提供了第三个解决方案，即结合智能指针和 RefCell。RefCell 引入了内部可变性的概念。尽管 Rc 本身不可变，但 RefCell 允许其内部可变，它通过 unsafe 代码实现这一功能。我们可以先将结构体 c 包装在 RefCell 中，然后再将其包装在 Rc 中。这样，对象 a 和 b 就可以同时引用 c，并且能够对 c 进行修改。通过这种方式，我们就能够模拟 Webpack 中复杂的对象网络建模。这种方法仍然面临一个问题：对象之间可能会形成循环引用。例如，对象 a 持有对象 b 的引用，而对象 b 也持有对象 a 的引用。这种循环引用在智能指针中是难以解决的，因为 Rc 本质上是基于引用计数的。当两个对象相互引用时，即使不再使用它们，它们的引用计数可能仍然为 1，导致它们无法被释放，从而引起内存泄漏问题。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/50/503266770f92b1300f7b25b121ff0d16.png\" /></p><p></p><p>在我们讨论的背景下，我想提到一个行业内的实验案例，即 Yew。Yew 是一个使用 Rust 开发的 UI 框架，在它的实现中大量使用了 Rc 和 RefCell。面对循环引用导致的内存泄漏问题，我们应该如何去解决呢？实际上还有第四种解决方案，那就是使用 unsafe 代码结合原始指针（*mut T）。这种方案在 Rust 语言的基础库中比较常见，例如 Rc 和 RefCell 的实现。然而，对于 Rust 的业务逻辑代码来说，我们很少直接使用这种功能，因为它涉及到直接的内存操作，风险较高。</p><p></p><p>我们还有一个方案，称为 Arena。熟悉 Rust 的开发者可能对 Arena 已经比较了解。Arena 的本质是变长数组（动态数组）。每个对象都由这个 Arena 数组来持有。例如，如果我们要创建一个 Node 对象，我们会将它放在 Arena 数组的下一个空位中。所有想要持有 Node 的对象，实际上持有的是指向 Node 的索引，也就是数组中的位置。在这个系统中，如果对象 a 想要持有数组中的第四个 Node，它不会直接持有这个 Node，而是持有这个 Node 索引（usize 类型）。这种方式可以很好地支持 clone、copy 操作，以及跨线程发送。我们可以用它来构建复杂的架构，例如 Rust Analyzer 和 Rust 编译器自身就大量使用了 Arena 架构。</p><p></p><p>使用 Arena 的另一个特点是，它要求对象可以不同时间创建，但必须同时销毁。Arena 的销毁是通过一次性释放整个数组来完成的。如果我们想在中间释放数组中的某个元素，比如第四个 Node，但可能已经找不到所有引用它的地方，那么如果再次访问该元素，可能会发生不可预知的事情。这是使用 Arena 方案可能遇到的问题，但尽管如此，这个方案仍然有许多优点。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/d3/d34aeb1d05bec4c3f72bc4bfd1f09417.png\" /></p><p></p><p>首先，Arena 这种技术方案在业界有非常广泛的应用。 不仅在游戏开发的实体组件系统（ECS）中，甚至在 Rust Analyzer、Rust 编译器（Rust c）以及 Rspack 等项目中，都在大量使用 Arena。</p><p></p><p>第二点是性能优势。 在 Rust 生态中，有一个库叫做 Bump AllOXCation，其核心原理是在系统启动时预先分配一大块内存。之后，所有对象的创建都在这块已经分配好的内存上进行，从而避免了频繁进行系统调用去请求系统的内存分配器。减少对系统内存分配器的调用，可以显著提升性能。在 OIC 等工程项目实践中就大量依赖了 Bump AllOXCation。</p><p></p><p>第三点是缓存友好性。 以 Webpack 为例，虽然它支持本地可落盘的缓存，但其缓存能力并非极致，无法将整个系统序列化。在 JavaScript 开发中，如果遇到循环引用的对象，使用 JSON 序列化会失败，因为系统无法处理这种需求。而 Arena 帮助解决了循环引用问题，我们只需将整个数组序列化到硬盘，之后再反序列化回来，就能实现可落盘的、可序列化的缓存架构。目前在 Rspack 工程以及社区内的许多项目中，都在大量依赖这种对象组织方式。我们也通过这种方法完成了 Rspack 内部的基础架构建模。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/8e/8e9b7cc29ccfb386d2588dd872ce70e2.png\" /></p><p></p><h4>总结</h4><p></p><p></p><p>最后，我想做一些总结。首先，从个人角度来看，技术成长和发展需要我们具备跨语言的技术储备。无论是进行基础架构设计还是业务开发，掌握多种语言都能极大地拓宽我们的技术视野。</p><p></p><p>第二点，科技企业在 Rust 方向的投入也在增加。一些动作迅速的公司已经开始实施相关项目。在字节跳动，Rust 的应用不仅限于前端工程化领域，还扩展到了后端框架和后端研发。例如，抖音的一些服务端业务已经开始使用 Rust 进行开发。此外，过去的 Lark 客户端 SDK 也是利用 Rust 来构建平台业务逻辑的。</p><p></p><p>第三点，工程化是 Rust 在前端领域展现优势的最佳场景之一。如果大家还在犹豫是否应该开始使用 Rust，现在可以放心地尝试，因为在这个领域取得成果相对容易。</p><p></p><p>第四点，我相信 Rust 在前端领域将会有更多的应用场景。例如，最近开源的名为 Zed 的编辑器，它可能成为 Visual Studio Code 的竞争者。Zed 的 UI 实现完全是用 Rust 完成的，它还引入了一个名为 GPUI 的库，允许开发者使用 Rust 构建跨平台的高性能 UI。</p><p></p><p>会议推荐</p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，QCon 还是太全面了。现在报名可以<a href=\"https://qcon.infoq.cn/2024/shanghai/apply\">享受 9 折优惠</a>\"，详情请联系票务经理 &nbsp;17310043226 咨询。</p><p><img src=\"https://static001.geekbang.org/wechat/images/57/5780f7c4c252e5d0848c35279ed4f0d9.png\" /></p><p></p>",
    "publish_time": "2024-09-06 12:07:19",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "解决算力网“最后一公里”难题，PPIO派欧云发布全新算力云产品矩阵",
    "url": "https://www.infoq.cn/article/fhugtAQ12eTiU8uqGgCs",
    "summary": "<p>9月4日，由PPIO派欧云主办的“Power to Pioneers·2024分布式云计算论坛”在上海召开。论坛聚焦于分布式云计算的技术创新、产业落地、市场趋势，及其对AIGC规模化应用的推动力。期间，PPIO推出全新的Serverless和模型API服务产品，进一步完善派欧算力云产品矩阵，标志着PPIO的分布式云服务从边缘云延伸到AI基础设施，将为AIGC创新企业和开发者带来更高性价比、弹性、易用的一站式AI基础设施产品和服务。</p><p>&nbsp;</p><p>PPIO派欧云联合创始人、董事长兼CEO姚欣在主题演讲中指出：“遵循尺度定律的基本原理，计算能力将决定大模型创新和应用的上限。在计算资源成本高昂而需求爆增的市场格局下，分布式算力将成为引领下一轮科技革命的重要推动力，正在逐步改变全球计算资源的供给和利用方式。我们的核心理念是利用分布式云的优势，三年实现千倍降本，以算力普惠推动AI普惠，助力新时代的开拓者创造出全新的发展机遇。”</p><p>&nbsp;</p><p>PPIO派欧云由PPTV创始⼈、前蓝驰创投投资合伙⼈姚欣和前PPTV⾸席架构师王闻宇于2018年联合创⽴，致⼒于为⼈⼯智能、⾳视频、元宇宙等新⼀代场景，以更低成本提供更快速、更灵活的⼀站式算⼒、模型及边缘计算服务。</p><p>&nbsp;</p><p>2019年，PPIO派欧云推出了“派欧边缘云”产品，采⽤去中⼼化的商业模式，整合分散在全球不同地点的计算资源，结合分布式计算和边缘云原⽣技术，在⽹络边缘侧基础设施建构了覆盖全球的⾼质量边缘云计算服务⽹络，实现了与传统中⼼云的智能兼容和协同。</p><p>&nbsp;</p><p>经过五年的沉淀，PPIO派欧云推出了全新的AI基础设施产品“派欧算⼒云”，依托分布式计算、云原⽣、推理加速等创新技术，汇集⼤模型API服务、Serverless、GPU容器实例等产品，释放推理计算潜能，为不同发展阶段的企业、开发者和研究者提供⾼性价⽐、弹性、易⽤的⼀站式AI云服务。</p><p></p><h2>大模型落地对算力的需求与日俱增</h2><p></p><p>&nbsp;</p><p>大模型以其庞大的参数规模和复杂的网络结构著称，这直接导致了训练过程中的算力消耗巨大。即便是最先进的GPU集群，在训练一些超大规模模型时也需要数月甚至更长时间。此外，为了不断提升模型的准确性和泛化能力，持续的优化和微调工作同样需要强大的算力支持。</p><p>&nbsp;</p><p>在第一批、第二批大模型通过备案逐步落地应用之后，不同场景中的大模型对算力资源需求的急剧增长的同时，对算力需求的形式也呈现出多样化特点。例如，像自动驾驶这种对实时性要求高的应用场景需要低延迟、高并发的算力支持；而数据分析类应用则更注重大规模数据处理能力和长期稳定的算力供给。</p><p>&nbsp;</p><p>无论是推理、训练、优化、微调以及落地后，大模型对算力的需求都是有增无减的。为了应对这一挑战，需要高性能的硬件支持，如高性能的GPU、TPU等，这些硬件能够提供强大的并行计算能力，满足大模型的计算需求。</p><p>&nbsp;</p><p>然而，光在硬件上下功夫还远远不够，高性能计算和分布式计算技术也正在被广泛应用于大模型的推理和训练和落地应用实践中，这也导致了分布式云计算的热度不断攀升。</p><p>&nbsp;</p><p>一直以来，算力作为支撑各类应用与创新的基石，其演进历程始终遵循着两条并行不悖的主线。一方面，单机算力的提升长期遵循着摩尔定律的轨迹，即每18个月性能翻倍，这一规律极大地推动了计算能力的飞跃。然而，随着半导体工艺逐渐逼近物理极限，摩尔定律的适用性面临严峻挑战，单机算力的增长速度开始放缓，其进一步提升的空间变得愈发有限。</p><p>&nbsp;</p><p>在此背景下，另一条技术路线——分布式算力扩展应运而生，并迅速成为解决算力瓶颈的重要途径。分布式计算，这一理念从早期的网格计算萌芽，历经云计算的蓬勃发展，直至如今算力网络的初步形成，始终致力于通过资源整合与协同工作，实现算力的横向扩展与高效利用。</p><p></p><h2>为什么分布式算力云能解决算力“最后一公里”难题</h2><p></p><p>&nbsp;</p><p>那么，到底什么是分布式算力云，它在解决算力紧缺难题时优势在哪里？</p><p>&nbsp;</p><p>具体来讲，分布式计算通过将大量独立的计算资源（包括服务器、个人电脑、移动设备等）通过网络连接起来，形成一个庞大的计算资源池。这些资源可以根据任务需求进行动态分配与调度，从而实现计算能力的灵活扩展与高效利用。与单机算力提升相比，分布式计算不仅能够突破单一设备的性能限制，还能够利用闲置资源，降低总体成本，提高资源利用率。</p><p>&nbsp;</p><p>面对单机算力增长受限的现状，分布式计算以其独特的优势成为了算力发展的重要方向。</p><p>&nbsp;</p><p>分布式算力云是一种基于分布式计算和云计算理念的架构模式，它通过将计算、存储和网络资源分布在不同的物理或虚拟节点上，通过网络进行协同工作，提供高性能、高可用性和高可伸缩性的服务。分布式算力云的优势主要体现在以下几个方面：</p><p>&nbsp;</p><p>高可用性：通过将资源分散部署在多个地点，避免了单点故障的风险。即使一个节点发生故障，其他节点仍然可以提供服务，确保业务的连续性和可靠性。</p><p></p><p>可伸缩性：分布式算力云可以根据业务需求灵活调整资源的规模。当业务负载增加时，可以通过添加更多的节点来扩展计算和存储能力；当业务负载减少时，可以适当减少节点数量以节约资源和成本。</p><p></p><p>弹性：分布式云具有弹性和灵活性的特点，能够自动调整资源以应对突发的业务需求和流量变化，提供稳定的性能和用户体验。</p><p></p><p>数据局部性：分布式云允许将数据存储在离用户最近的节点上，减少数据传输的延迟和带宽消耗，提高数据访问的效率和响应速度。</p><p>&nbsp;</p><p>在实际应用中，分布式算力云已广泛应用于大规模网站、数据分析、高可用性和容灾、多地点协同工作以及科学计算等领域。这些应用案例充分展示了分布式算力云在提供高性能、高可用性和高可伸缩性服务方面的巨大优势。</p><p>&nbsp;</p><p></p><h2>算力成本三年降低1000倍，PPIO派欧云发布全新产品矩阵</h2><p></p><p>&nbsp;</p><p>为了进一步降低算力成本，派欧重磅发布了全新PPIO派欧云产品矩阵。以分布式云为依托，重点专注于重点场景公有云服务。</p><p>&nbsp;</p><p>聚焦开发者，产品矩阵分为三个层面：集成尖端技术推理加速引擎（PPInfer），二是Serverless云原生体系，三是为初创开发者提供的大模型AI服务——Model-API，直接基于API的一站式使用。</p><p></p><h3>发布推理加速引擎（PPInfer）</h3><p></p><p>&nbsp;</p><p>与模型训练相比，AI推理与产业应用关系更为密切。大模型在训练后需要通过推理应用于实际场景，这直接影响其可用性和实用性。然而，目前AI推理面临的主要问题是成本高昂，尤其是随着模型规模增大，计算需求和成本也随之增加。此外，推理效率普遍较低，加上复杂的业务场景和应用链路，进一步推高了推理成本。</p><p>&nbsp;</p><p>会上，PPIO派欧云介绍了他们如何通过一系列自研推理加速算法，使大语言模型（LLM）的推理性能提升10倍，并将综合推理成本降低90%以上。PPIO派欧云通过三大核心技术实现了这一突破：</p><p>&nbsp;</p><p>PyramidCache稀疏化压缩算法：该算法分析计算注意力分数在不同层上的分布模式，为不同层动态分配不同KV Cache预算，在压缩比和模型性能之间取得最佳匹配。实验表明，该方法将KV Cache压缩至10%以内，同时保持95%以上的模型性能表现，最终将GPU内存开销降低至20%。</p><p>&nbsp;</p><p>Hydra Sampling投机采样技术：针对传统大模型推理过程中每次仅生成一个token导致的低吞吐量问题，PPIO派欧云创新实现了基于多头并行推理的Hydra Sampling技术。通过在线动态更新机制，草稿模型越用越聪明，推理效率也随之越来越高，端到端综合性能优化达到2倍以上。</p><p></p><p>端到端FP8推理：PPIO派欧云重写了核心的注意力算法，直接调用FP8 TensorCore进行注意力计算，并使用FP8保存KV Cache，避免FP16格式的中间结果转换和传输，实现全链路FP8计算。这些优化显著降低了数据存储和通信成本，端到端推理效率提升约2倍。</p><p></p><h3>发布Serverless产品</h3><p></p><p>&nbsp;</p><p>为了解决AI推理的成本问题，实现大模型应用算力普惠，PPIO派欧云通过算法、系统和硬件协同创新，推出了专为AI推理场景设计的Serverless产品，提供模型部署、运行优化、弹性伸缩及API服务，帮助客户在自定义模型时获得高效的弹性效率和极致的运行成本。该Serverless产品核心特性包括：</p><p>&nbsp;</p><p>弹性伸缩：支持根据流量自动调整资源规模，高峰时自动扩容，低谷时自动缩容，且计费基于实际使用的容器资源，确保成本最优化。</p><p>免运维：提供全方位的监控与自动异常处理机制，无需额外运维团队，自动关闭异常容器并快速重启新实例，确保服务连续性，同时保持开发者的零干扰体验。</p><p>&nbsp;</p><p>据悉，该产品的核心技术主要包括两点：</p><p>Auto Scaling：该技术核心在于实时监测流量与访问量。通过算法预测流量变化，自动增减工作节点（worker）以应对需求。在预测到流量增长时，自动增加工作节点以实现快速冷启动与部署；反之，在流量减少时，则自动缩减工作节点，确保资源高效利用。</p><p>&nbsp;</p><p>沙盒化虚拟环境：此环境设计得高度灵活，既能支持自有实例，也能兼容第三方实例，包括虚拟机、容器及裸金属服务器。即使在非自持硬件环境下，也能实现自动伸缩，确保服务能力的灵活扩展。用户无需担心流量激增导致的资源不足问题，系统会自动寻求合作伙伴协助扩大容量。</p><p></p><h3>推出Model API&nbsp;</h3><p></p><p>&nbsp;</p><p>此外，PPIO派欧云还推出了全新的Model API服务产品，支持AIGC应用开发所需的全模态API，开发者无需精通机器学习即可快速部署和调用大模型功能。结合PPIO强大的分布式计算能力和派欧算力云Serverless产品的弹性支持，Model API服务不仅显著提升了AI应用的开发效率，还通过按需付费模式有效降低了企业的开发和运营成本。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/ba/ba7d8bfc883e547c7e5e1923a4b6dd40.png\" /></p><p></p><p>PPIO派欧云联合创始人兼CTO王闻宇发布Model API服务价格</p><p>&nbsp;</p><p>派欧算力云产品可广泛应用于多个场景，提供灵活且高效的解决方案。例如，在中国铁塔的智能算力调度管理项目中，为了实现在城市广泛分布的产业园、写字楼、住宅、文娱和高校等园区进行算力入园的管理服务，PPIO通过打造分布式算力网络，实现不同地区、不同时间、不同业务的弹性算力调度和大模型推理加速，为园区内的AI应用提供高性价比的智算服务，满足智能数据处理、大模型部署、AIGC生成、AI算法加速、渲染加速等一系列应用需求。</p><p></p><h2>产研联动、生态协同，加速AI应用创新</h2><p></p><p></p><p>会上，PPIO派欧云宣布聘请华中科技大学教授、长江学者特聘教授、中国计算机学会副理事长金海教授担任公司技术委员会主席。作为中国分布式计算领域的知名学者，金海教授将为PPIO的分布式技术发展战略和算力网络规划提供深入全面的指导，为公司未来在分布式云和AI基础设施领域的研究和创新发展奠定坚实基础。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/59/59f2ddb17a434087d0c0590e4e8d0752.png\" /></p><p></p><p>与此同时，中国信息通信研究院与PPIO派欧云联合多家行业领军企业、科研机构共同启动《分布式算力发展白皮书》研究工作，旨在推动分布式算力技术的发展，为全球数字经济注入新动能。白皮书将系统地阐述了分布式算力的发展背景、核心技术、应用场景、市场趋势以及未来发展方向，全面总结了近年来分布式算力在全球范围内的实践成果与技术突破，为分布式云行业从业者、技术开发者和政策制定者提供清晰的方向指引，推动分布式算力技术的进一步成熟和落地。</p><p>&nbsp;</p>",
    "publish_time": "2024-09-06 13:02:48",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "案件量增加400%，理赔员却减少30%？保险企业如何通过数智化实现高效运营",
    "url": "https://www.infoq.cn/article/WJ1azT7v0UyYQph2Oweu",
    "summary": "<p>AI 浪潮下，保险行业正以前所未有的速度向智能化迈进。尤其是在理赔和核保等关键环节，AI 技术的应用不仅优化了流程效率，还显著提升了客户体验。传统理赔模式下的繁琐流程和高成本，随着科技的介入得到了有效缓解。</p><p></p><p>近日，InfoQ 在“2024 外滩大会 - 颠覆还是协同，畅谈保险业 AI 科技浪潮”论坛上了解到，从平安产险的车险理赔数字员工，到中国人保健康的智能核保系统，越来越多的保险公司通过 AI 和大模型技术，实现业务的端到端数字化。</p><p></p><h2>数字化理赔的探索与实践</h2><p></p><p>“在财产险行业所有产品中，车险的理赔是用户体验感最强、受众最广的产品。”中国平安财产保险股份有限公司首席技术官陈当阳介绍道，平安产险 70% 的保费来自车险，80% 的赔款支出也流向车险，因此车险理赔成为保险公司在客户体验、成本控制和效率提升等方面最具挑战的业务场景。</p><p></p><p>然而，随着监管要求的提高以及保险行业业务量的激增，理赔过程面临的挑战越来越大。陈当阳进一步指出，用户群体的变化，尤其是以 90 后为代表的“数字原住民”，推动了保险行业对理赔体验的革新。这一代客户对便捷、快速的服务有着更高的期望。同时，理赔成本不断增加，如医疗费用、伤残赔付等每年都有上涨趋势，车辆维修成本也在持续攀升。在这种情况下，如何提高效率成为了当务之急。</p><p></p><p>平安产险的应对之策是通过数字化手段大幅提升理赔效率。陈当阳提到，平安产险通过打造 “数字员工”和“理赔卷宗”项目，推动了车险理赔流程的全面自动化。过去三年，在案件量增长 400% 的情况下，理赔人员减少了 30%。未来，平安产险计划在不增加人员的前提下，在保持人员不增加的情况下，通过科技赋能继续提升理赔效率，预计年均复合增长率在 40%-50% 之间。</p><p></p><p>不仅是车险理赔，中国人民健康保险股份有限公司（以下简称“中国人保健康”）也在健康险理赔中取得了数字化突破。据 中国人民健康保险股份有限公司副总裁王彤 介绍，互联网健康险的运营链条长且复杂，涵盖了承保、理赔、客户服务和风控等多个环节。为应对这些复杂的流程，中国人保健康在核保方面采用了多重核保模型的融合策略。首先，通过静态核保来解决大部分常规问题，基于规则认定和数据积累，确保核保的专业性与准确性。然而，王彤强调，健康告知中常常出现保险需求与供给的不匹配，这使得部分客户因无法准确告知健康状况而被拒保。</p><p></p><p>为解决这一痛点，他们推出了智能核保系统，这也是与蚂蚁保的共创成果。智能核保基于多种算法的融合，确保了核保结果的稳定性与可靠性。同时，系统通过多维度的数据融合，从客户健康风险、保险产品免赔额等多个角度做出综合评估，帮助原本被拒保的客户重新获得投保资格。此外，考虑到医疗险的复杂性，他们还保留了人工核保通道，针对特殊的医疗行为和用药需求，核保员可以根据更详细的资料做出灵活的承保决策。这一创新既保障了系统的效率，又确保了复杂案件的处理质量。</p><p></p><p>在理赔环节，王彤进一步强调，理赔是保险行业的核心价值体现。随着互联网健康险客户数量突破 7000 万，理赔量也大幅增加，为此，他们通过数字化、智能化的手段，重构了理赔流程。据介绍，中国人保健康和与蚂蚁保合作推出了“安心赔”服务，在售前就让客户对不同保险产品的理赔效率有明确预期。借助这套数字化服务，他们医疗险的两日结案率（住院）超过了 95%，并且 90% 的理赔无需补交材料。</p><p></p><h2>AI 深入保险应用</h2><p></p><p>保险行业的数字化转型不仅是信息化的提升，更是智能化的深入。无论是平安产险的“数字员工”还是中国人保健康的智能核保系统，AI 不仅是辅助工具，更是推动各流程变革的核心力量。</p><p></p><p>陈当阳提到，平安产险在车险理赔中，45% 的工作时间都花费在客户沟通上，而这些沟通很多是重复性、机械性的任务。为了解决这一问题，平安产险引入了 AI 机器人“数字员工”，承担了大量的客户沟通和问题解答。通过这种方式，理赔员得以从繁琐的日常事务中解脱出来，专注于复杂案件的处理。</p><p></p><p>据陈当阳介绍，“数字员工”的底层技术核心分为两部分，其中一部分是大模型。平安产险在大模型的应用上起步较早，自 2022 年 11 月 ChatGPT 推出后，平安产险于 2023 年上半年便上线了首个大模型应用。大模型在理赔数字员工的落地中发挥了关键作用，解决了理赔单证的智能识别、复杂场景下的图像处理以及客户复杂意图的识别等难题。</p><p></p><p>此外，平安产险还基于 3D 增强现实和自研的智能相机技术，利用 3D 结构光对车损和物损进行仿真建模，并将其纳入深度学习空间进行精准评估。从实际效果来看，这一技术已在试点中展现了显著成效。数据显示，66% 的案件实现了零录入，60% 的沟通实现了端到端的自动化处理。</p><p></p><p>王彤也提到，在中国人保健康的数字化体系里，除了通过 AI 构建风控能力体系，知识图谱也发挥了至关重要的作用。知识图谱背后依托的是一系列知识库，如疾病库、医疗行为、用药、器械等。将这些数据库与疾病演变、循证医学等相结合，借助大模型的计算，最终形成智能化的知识图谱。这一体系不仅支撑了运营各环节的有效管理，还能够实现精准风控，既为被保险人提供高性价比的普惠产品，也确保了公司的可持续健康发展。</p><p></p><p>王彤相信，随着 AI 技术的不断进步，智能化和系统化的 AI 应用将是未来的发展趋势，尤其是在核保和理赔等场景中，AI 将起到越来越重要的作用。</p><p></p><h2>“AI+ 人工”服务新范式？蚂蚁保发布新平台</h2><p></p><p>谈及大模型在保险领域的应用，蚂蚁集团保险事业群首席技术官孙振兴表示，传统的保险服务因为其高度的专业性和复杂性，往往很难普惠大众。但大模型的引入，具备了压缩海量知识、推理和表达的能力，突破了认知和决策的难题，使高质量的保险服务能够更广泛地普及。</p><p></p><p>据孙振兴介绍，蚂蚁集团的思路是通过多智能体协同框架，将 500 多个小模型与大模型结合，小模型可以在理赔、配置等复杂场景中，灵活调用传统的 OCI、HRAM 等专业模型，有效解决专业领域的处理难题。因此，蚂蚁认为，未来十年，保险科技的核心范式将是严谨的大模型与专业的金融小模型的高效协作，形成多智能体的协同体系。</p><p></p><p>会上，蚂蚁保发布了智能保险服务开放平台“蚂蚁保蚁桥”（以下简称“蚁桥”）。该平台融合大模型和 AI 技术，通过“蚁桥”平台，保险公司可以提供全天候的“AI+ 人工”服务。</p><p></p><p>据了解，“蚁桥”已经对外开放，首批试点包括中国人民保险、中国太平保险、中国平安保险和众安保险等四家公司。试点期间，引入了 200 位保险公司产品专家参与，数据显示，合作保司的产品专家单人服务半径可扩展至 100 人 / 日，高于行业均值 3-5 倍。</p><p></p><p>蚂蚁保总经理陈冠华指出，借助这个平台，用户可以更清晰地理解保险条款，知道自己该买什么样的保险。平台试点期间，日均有超过十万人次通过“蚁桥”获取服务，保险公司的服务能力也因此得到了提升。</p><p></p><p>陈冠华指出，随着大众保险意识的提高，越来越多的人主动去买保险，而不再是被动地推销。但保险条款复杂，很多人并不清楚如何选择。而数字化工具、智能管家等新型技术和服务，能够很好地解决这些疑问、承接需求。</p>",
    "publish_time": "2024-09-06 15:15:16",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "加速“AI+金融”落地，招行上半年IT支出达48.6亿",
    "url": "https://www.infoq.cn/article/ugWVzb8oL0fH5m4Xeim3",
    "summary": "<p>全球经济环境的复杂性持续为金融机构带来挑战。招商银行日前发布了 2024 年上半年财报，数据显示，其营业收入和净利润分别同比下降 3.09% 和 1.33%，具体为实现营业收入 1729.45 亿元，归属于股东的净利润 747.43 亿元。此外，报告期末，招商银行总资产达到 11.57 万亿元，客户存款突破 8.66 万亿元。</p><p></p><p>与此同时，在数字化转型方面，招商银行重点加大对人工智能、大数据等技术的投入，全面推进数字化建设，以支持未来高质量发展。报告显示，2024 年上半年，招行信息科技投入达 45.86 亿元 ，占总营业收入的 2.91%。数字化人才储备方面， 研发人员数量增至 10653 人，占员工总数的 9.23%。</p><p></p><p>整体战略上，招行继续以“线上化、数据化、智能化、平台化、生态化”为演进方向，推进数字金融建设，以“AI+ 金融”为重点发力方向，推动由“线上招行”迈向“智慧招行”。</p><p></p><p>通过金融科技创新项目基金，招行围绕数字化经营、前沿科技能力、B 端与 C 端生态及创新孵化等五大方向，推进全行的数字化能力提升。报告期内，新增立项 398 个，新增上线项目 499 个。截至报告期末，累计立项 4198 个，累计上线项目 3561 个，展示出数字化建设的稳步推进。</p><p></p><h2>显著加快 AI 大模型应用的探索</h2><p></p><p>具体来看，招行在零售和批发服务、风控管理、内部运营及科技基础设施等多个领域的数字化转型均取得显著进展。</p><p></p><p>在零售数字化方面，招商银行 App 和掌上生活 App 的月活跃用户（MAU）达 1.17 亿户。招行通过大模型技术进一步提升 App 的智能服务水平，优化“小招”智能助理的功能，使其逐步从预设的财富助理，发展为能够提供个性化业务办理和疑难咨询的银行助理。此外，零售信贷业务的数字化转型也在加速推进。报告期内，数字化获客占零售信贷业务整体获客的比例超过 50%。</p><p></p><p>批发数字化服务则在加强数字化渠道建设，提升服务质量和效率。截至报告期末，融资业务线上化率达到 93.32%，外汇业务线上化率为 77.32%。同时，批发线上渠道月活跃客户数达 189.19 万户，同比增长 17.96%。为了满足企业数字化转型的需求，招商银行提供财资管理云服务，企业客户数达到 54.07 万户，较上年末增长 13.21%。</p><p></p><p>在风险管理方面，集团风险管理系统（GRS）逐步完善，表内外全业务智能化预警覆盖率达到 100%，在线风控平台新发放公司贷款 1886.34 亿元，同比增长 22.66%，在提升信贷服务效率的同时，资产质量保持稳定。</p><p></p><p>并且，招行加快了大模型技术的应用。比如，利用大模型快速生成客户风险洞察和行业分析报告，帮助客户经理和信贷人员提升尽调和审查效率。经营管理方面，智能化工具的应用进一步提高了全行的管理精细度。零售条线通过智能助手赋能一线员工，提升协同和管理效率；批发条线的“火眼”报表平台支持各类报表的快速生成和数据分析，截至报告期末，用户数达到 1.49 万人。资本管理系统与大模型的融合推动了智能化管理的发展，助力资本新规的全面落实。人力资源领域也加大了数字化投入，上线了基于大模型技术的智能服务系统，为员工提供智能应答服务。</p><p></p><p>内部运营方面，招行通过技术手段替代部分人工操作，推动无纸化运营管理。上半年智能客服和 RPA（机器人流程自动化）等应用累计替代工时 1632.59 万小时，同比增长 36.72%。招行还在财务报销中试点应用大模型技术，其审核效率较传统纸质审核提升 70%。</p><p></p><p>数字化基础设施方面，招行继续推进云架构转型，提升资源效能。报告期内，云服务总体可用性超过 99.999%。在低代码开发建设上，业务人员占开发者的比例达到 59.40%。大数据服务已覆盖全行六成员工，数据逐步成为员工经营分析的重要工具。</p><p></p><p>未来，围绕“AI+ 金融”战略，招行将在“人、财、物”等方面加大 AI 专项投入。大模型体系的建设也在加速推进——从基础设施、推理训练、算法、开发框架到场景应用，完善内部大模型体验平台建设。</p><p></p><h2>首席信息官更替</h2><p></p><p>财报中还提到了一项重要的人事变动。2024 年 5 月，江朝阳因工作变动不再担任招商银行首席信息官，随后在 2024 年 6 月，周天虹先生被任命为该行新任首席信息官，其任职资格尚待国家金融监督管理总局核准。</p><p></p><p>公开资料显示，周天虹今年 57 岁，拥有丰富的金融科技从业经验，曾任招行信息技术部高级经理、信息技术管理办公室总经理及信息技术部总经理等职务。与此同时，江朝阳则调任招商局集团战略发展部 / 科技创新部总经理，结束了自 2019 年起担任招商银行首席信息官的任期。</p><p></p><p>值得一提的是，2022 年年报中，招行高级管理人员一栏首次出现首席信息官。</p><p></p><p>作为新任首席信息官，周天虹将如何在继承前任战略的基础上进一步推动“AI+ 金融”融合以及其他金融科技创新，是业界关注的焦点。</p>",
    "publish_time": "2024-09-06 15:23:50",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "国内首个大模型攻防赛启动，蚂蚁集团参与国际标准、可信度报告等多项成果发布",
    "url": "https://www.infoq.cn/article/1DFM1lg2a92qzK3G38eN",
    "summary": "<p>9月6日上午，在2024Inclusion·外滩大会“以AI守护AI，大模型时代的攻守之道”论坛上，WDTA世界数字技术院正式发布国际标准《大模型供应链安全要求》。该标准由云安全联盟（CSA）大中华区、蚂蚁集团、微软、谷歌、Meta、PrivateAI等数十家国内外单位的专家共同编制。这是业内首个大模型供应链安全国际标准，标志着全球AI治理的国际合作又迈出了坚实的一步。</p><p></p><p>针对此次发布的标准，云安全联盟（CSA）大中华区主席李雨航表示，大模型系统的运行依赖于分布在全球相互联系的供应链生态系统。全球人工智能大模型的广泛应用带来了前所未有的机遇，同时也暴露出供应链安全的巨大挑战。WDTA发布该标准为大模型的全生命周期安全提供了系统性管理框架，解决了供应链中的安全挑战，支持全球人工智能技术的安全和可持续发展，提升了行业的整体可信度。</p><p></p><p>据介绍，《大模型供应链安全要求》是WDTA人工智能安全、可信、负责任（AI STR）系列标准之一。该标准给出了大语言模型的供应链安全保护框架，从数据准备、大模型开发到部署运维各个环节涉及的供应链相关安全风险和供应活动管理给出了要求，并给出了常见的供应链安全风险、典型安全案例等相关信息。通过这一标准，可有效识别和评估大模型系统生命周期中面临的供应链潜在安全风险，如数据泄露、模型篡改及供应商不合规等问题，确保供应链的完整性、可用性、保密性，从而提升大模型系统的安全性。</p><p></p><p>这一标准不仅为大模型供应链中的供需双方提供了系统性的安全管理框架，也为第三方机构和权威部门的安全审查和合规管理提供了可靠依据，进一步增强了大模型系统的整体安全可信发展。</p><p></p><p>世界数字技术院（WDTA）成立于2023年4月，是在日内瓦注册的国际非政府组织。该组织致力于在全球范围内推进数字技术，促进国际合作。AI STR计划是WDTA的核心倡议，旨在确保人工智能系统的安全性、可信性和责任性，微软、谷歌、Anthropic、蚂蚁集团、百度等均为其成员单位。今年4月联合国科技大会上，WDTA发布了由OpenAI、蚂蚁集团、科大讯飞、谷歌等数十家单位参编的两项大模型安全国际标准，均为AI STR系列标准。</p><p></p><p>在全球人工智能发展和治理广受关注的今天，如何平衡AI的发展与安全，已成为国际社会面临的共同话题。我国也在积极推动人工智能治理体系优化完善，推出了《全球人工智能治理倡议》《国家新一代人工智能标准体系建设指南》等一系列政策。本次论坛上，与会嘉宾各自分享了观点与展望，致力推动形成全球人工智能安全治理共识和创新合作。</p><p></p><p>全国网络安全标准化技术委员会委员/WG7副组长闵京华表示，AI治理的根本目的在于保障AI的价值实现与风险的平衡。实现这一目的，从政策法规、技术标准到安全产品及服务等各个层面都需要行业共同努力协作。技术标准作为其中不可或缺的环节，凝聚了不同组织的专家共识，可确保人工智能系统是安全、可信赖和对所有人都有益的。</p><p></p><p>AI安全也是科技企业需解决的重要社会责任议题。蚂蚁集团机器智能部总经理、安全实验室首席科学家王维强表示，蚂蚁集团长期关注AI安全，一方面积极参与技术标准规范建设，另一方面在研发大模型技术中时刻关注风险评估结果，实施并提升大模型的安全性、可靠性、可控性技术。蚂蚁集团是较早布局可信AI技术的科技企业之一，其自主研发的大模型安全一体化解决方案“蚁天鉴”，相关检测和防御产品已开放给20余家外部机构和企业使用。</p><p></p><p></p><h2>全球AI攻防挑战赛报名启动</h2><p></p><p></p><p>除了标准发布，论坛现场宣布了国内首个大模型攻防主题的科技赛事—“全球AI攻防挑战赛”正式启动。该赛事聚焦AI大模型产业实践，设计了攻、防双向赛道，邀请各路白帽黑客、技术人才分别进行针对文生图大模型“数据投毒”的攻防实战演练，以及金融场景大模型生成内容的防伪检测竞赛，角逐百万元科技奖金。</p><p></p><p>本次大赛由中国图象图形学学会、蚂蚁集团、云安全联盟（CSA）大中华区联合主办，广泛联合清华大学、上海交通大学、浙江大学等C9高校及多家产学研组织共同发起，上海人工智能实验室作为技术合作单位。大赛旨在通过技术竞赛的形式，凝聚学界、行业力量，直面并解决大模型应用中潜藏的风险，助力全球AI产业健康可持续发展。</p><p></p><p>中国工程院院士、中国图象图形学学会理事长王耀南在赛事启动环节表示，大模型时代，如何确保AI系统的安全可控、如何有效应对供应链中的潜在风险，成为亟待解决的重要问题。图像图形技术作为新一代信息技术领域的关键技术，在大模型等多场景创新应用，推动图像识别与安全技术创新发展。此次AI攻防挑战赛，是中国图象图形学学会携手蚂蚁集团等业界知名企业、机构，聚焦大模型AIGC赛道共同打造的全球性技术盛宴，从人脸验真到生成式内容检测，期待挖掘出更多创新方案，更进一步保障大模型应用安全和产业安全。</p><p></p><p>大赛设置了“攻击”与“防守”两大赛道，分别聚焦大模型自身安全和大模型生成内容的防伪检测及大模型滥用风险检测，涵盖机器学习、‌图像处理与计算机视觉‌、数据处理等多个算法领域的考点。</p><p></p><p>其中，“攻击赛道”聚焦于文生图大模型的实际应用风险问题。参赛选手可通过目标劫持、情景带入、逻辑嵌套等多样化的动态攻击诱导技术，诱发大模型输出风险图像，以此激活大模型的潜在弱点和漏洞，增强大模型生图的安全免疫能力。“防守赛道”则聚焦于AI核身中的金融场景凭证篡改检测，以应对日益严峻的Deepfake深度伪造及AIGC假证风险。大赛提供百万级凭证篡改数据训练集，参赛选手需要研发和训练模型，并利用对应的测试集评估模型有效性，给出数据伪造概率值。</p><p></p><p>为保证大赛公平公正，同时更好地指导选手，大赛评委团由多位学术界与工业界专家共同组成。中国工程院院士、中国图象图形学学会理事长王耀南，云安联盟（CSA）大中华区主席李雨航担任本次大赛的指导委员会专家，来自清华大学、上海交通大学、上海人工智能实验室、中国图象图形学学会等单位的近30位知名学者将参与大赛的组织及评审工作。</p><p></p><p>本次大赛于9月6日正式启动报名，11月初完成赛事评审。即日起，选手可通过中国图象图形学学会官网、阿里云天池大数据众智平台官网等渠道报名。大赛全程设有近100万元奖金池，用以选拔及表彰领域内的优秀人才。</p>",
    "publish_time": "2024-09-06 15:40:03",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Flux、SD等图片生成模型遭“封禁”，但这次硅谷大厂不反对了！",
    "url": "https://www.infoq.cn/article/OT0IRQiexujA1QS3K5eX",
    "summary": "<p></p><p>“SB 1047 和 AB 3211 将会消灭加州的开源。”有网友评价道。</p><p></p><p>就在人们为 SB 1047 号法案而抓狂之时，另一项加州法案 AB 3211 已经悄然被递交至立法机构，而且似乎即将获得通过。（SB-1047 法案全称“前沿 AI 模型安全创新法案”，旨在对投资超过 1 亿美元的或具备一定计算能力的开发者建立安全准则，以保障大规模 AI 模型的安全性）</p><p></p><p>AB 3211 这项法案将产生更大的影响，因为它将使得任何尚未全面部署强大 AI 水印机制的 AI 图像生成系统、服务、模型或者模型托管站点沦为非法实体，几乎不可能在加州范围内正常运营。此项法案要求通过此类水印系统嵌入非常具体、肉眼无法察觉且难以删除的元数据，以将图像标识为 AI 生成，同时提供关于图像生成方式、时间及服务的其他信息。“元数据”是指有关数据的结构性或描述性信息。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/81/817a85dd9b2a075f7130718b483f8b5d.png\" /></p><p></p><p></p><h3>压力到底给到了谁？</h3><p></p><p></p><p>\"这项要求在技术层面严重缺乏可行性。想要在生成图像文件（或者任何其他数字文件）时，向其中附加或者嵌入无法删除的元数据几乎没有可能——之前遭遇失败的 DRM 方案已经证明了这一点。\"网友 Yenta Magenta 分析称。</p><p></p><p>Magenta 表示，这项法案的要求其实可以轻松被简单的屏幕二次截图所破解，而且哪怕开发人员设计出了真正不可破解的水印，其落地要求恐怕也将远远超过大多数模型构建者，特别是开源社区的承受能力。该法案还要求全体模型创建者 / 提供者开展广泛的对抗性测试，并开发及公开用于检测其模型或系统生成内容的工具。虽然法案中的其他条款将被推迟至 2026 年执行，但前面提到的大部分主要条款都很可能将在法案通过后立即生效。</p><p></p><p>如果目前掌握的法案内容准确无误，那就意味着市面上几乎全部现有 Stable Diffusion 模型、微调及 LoRA 在加州都将被定义为非法。而 CivitAI、Hugging Face 等网站则有义务过滤加州居民生成的内容，甚至一刀切阻止这部分用户的访问。（考虑到筛查的成本太高，一刀切阻止恐怕会成为大概率事件。）</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/4c/4c1bf08151c46e30de4e59a71aa20270.png\" /></p><p></p><p>在水印要求方面，法案似乎也没有列举任何涉及技术可行性的例外条款。Magenta 表示，鉴于法案所要求的高度具体且可靠的技术尚不存在，甚至可能永远不会出现（特别是在开源技术领域），至少就目前来讲，该项法案实际上是一纸针对加州 AI 图像生成业务的全面禁令。由此引发的诉讼纠纷也将只是时间问题。</p><p></p><p>前微软工程师 Jacob Miller 就表示，\"加州的 AB 3211 法案旨在要求图片中的元数据区分 AI 生成图片和“真实”图片，并要求平台披露是否真实。我认为无法创建一种万无一失的方法来追踪这一点。感觉它最终产生的问题会比它解决的问题更多。不可避免地，会有一种生成具有元数据图像的方法，这些元数据被认为是真实的，此时平台声明‘这是一张真实的图片’会比根本没有免责声明更糟糕。我认为这是一件意图是好的、但现实却是加剧问题而不是提供帮助的事情。“</p><p></p><p>根据 TechCrunch 报道，微软、OpenAI 和 Adobe 目前都对这项措施表示支持。“这几乎肯定是因为他们意识到没有任何开源图像生成模型或者服务能够满足技术要求，所以这项法案反而成为帮助他们打击开源竞争对手的有力武器。这可能意味着加州一切开源 AI 图像模型都将宣告终结，甚至更多想在加州从事这项业务的公司也将被一体叫停。换句话说，这项法案很可能成为我们迄今为止在 AI 技术领域见证过的，最大的监管 / 压制性威胁。”Magenta 分析称。</p><p></p><p>目前尚不清楚该法案的拟定者（或者可能修改法案条款的三个阶段 ）是否具备相关技术背景，以理解法案内容的矫枉过正与无法实现。如果他们确实具备这样的专业知识，那么从结果上看，此项法案的设计本身就是要出台一项隐性的全面禁令。</p><p></p><p>此外，该项法案将禁止销售任何未采用图像认证系统的新型静态或视频相机。但从条款上看情况似乎还有余地，毕竟相关规定还要过几年才会生效，而且只适用于“新制造”的设备。但“新制造”的定义仍含糊不清，意味着想要在法律生效后购买此前生产的旧机型以节约成本的买家，其行为很可能在加州被定义为非法。另外，考虑到手机也属于录制设备，这甚至会严重限制加州居民合法购买智能手机。</p><p></p><p>该法案还将对一切在加州拥有 200 万或者更多用户的大型在线社交媒体平台设定严格要求，包括检查其元数据以裁定哪些图像属于 AI 生成，并要求这些平台明确将其标注出来。任何无法确认为非 AI 生成的图像则应被标记为来源不明。</p><p></p><p>鉴于加州对于社交媒体平台的定义较为宽泛，因此从 Facebook 到 Reddit，甚至 WordPress 乃至其他拥有活跃评论区的网站和服务都有可能被划入管控范畴。“毫无疑问，这将成为一场席卷技术与言论自由的监管噩梦。”</p><p></p><p>“这个法案是一份疯狂的愿望清单——最重要的是，它要求平台解决许多项目正在尝试解决的版本控制和出处问题。这是一个难题！加州正努力为世界树立糟糕的人工智能监管榜样。”网友 Himanshu Tyagi 评价道。</p><p></p><p>此项法案已经在加州议会以 62：0 票（80 名议员）的比例初步获得一致通过，目前来看法案很可能会以某种形式在加州参议院获得通过。但州长加文·纽森（Gavin Newsom）是否愿意签署这项严厉、侵犯隐私且具有巨大潜在破坏性的立法仍有待观察。</p><p></p><p>另外，不清楚这项法案应如何通过宪法审查，毕竟其条款似乎过于宽泛、缺乏技术可行性，同时也宪法第一修正案约定的权利有所冲突，甚至可以说代表着一种强制性的言论管控形式。但令人意外的是，电子前沿基金会（EFF）和美国公民自由协会（ACLU）似乎都没有对这项法案发表意见，至少从 2024 年 6 月加州参议院司法委员会的分析结果来看是如此。</p><p></p><p>此消息一出，已经有企业开始宣传自己加的产品了：Digimarc 水印技术以无与伦比的准确性、速度和规模识别和验证实体和数字资产，以应对当今复杂挑战。</p><p></p><p><img src=\"https://static001.geekbang.org/wechat/images/95/95821f9685e8aa4df54273de21f4ecf9.png\" /></p><p></p><p>对此，有网友嘲讽道：“是在蓝红绿通道上进行简单的图案注入，就像 1995 年那样吗？”</p><p></p><p></p><h3>争议颇大的 SB 1047</h3><p></p><p></p><p>如前文提及到的，加州议会及参议院已经通过《前沿 AI 模型安全创新法案（SB 1047）》，这也是美国首批重要的 AI 法规之一。</p><p></p><p>该法案一直是硅谷及其他地区的争论焦点，其要求在加州运营的 AI 企业在训练复杂的基础模型之前，必须实施一系列预防措施。具体包括能够快速完全关闭模型，确保模型免受“不安全的训练后篡改”，并部署测试程序以评估模型或其衍生版本是否容易“造成或被用于实施严重危害”。</p><p></p><p>SB 1047 的规则适用于世界上的大型 AI 模型：成本至少为 1 亿美元，并且在训练期间使用 10^26 FLOPS；或者是利用不少于 10^25 次整数或浮点运算三倍的计算能力，对范围内的模型进行微调而创建的人工智能模型。</p><p></p><p>对于开源模型及其衍生产品，该法案规定原始开发者应承担责任，除非另一位开发者再花费 1000 万美元创建原始模型的衍生产品。</p><p></p><p>加州则会成立新的机构——Frontier Models 委员会负责监督，该委员会将由 9 名成员管理，包括来自 AI 行业、开源社区和学术界的代表，由加州州长和立法机构任命。</p><p></p><p>该法案的主要作者、参议员斯科特·维纳 (Scott Wiener) 表示，SB 1047 是一项非常合理的法案，条款只是要求各大 AI 实验室做他们已经承诺要做的事情：测试自己的大模型是否存在灾难性的安全风险。“我们与各开源倡导者、Anthropic 公司以及其他组织共同努力，在过去整整一年间不断完善并改进该项法案。SB 1047 很好地反映了我们对于可预见 AI 风险的了解，应当颁布落地。”</p><p></p><p>被称为“人工智能教父”的人工智能研究人员 Geoffrey Hinton 和 Yoshua Bengio 都支持这项法案，但还有很多人反对：李飞飞认为，该法案将“损害我们刚刚起步的人工智能生态系统”；吴恩达认为，称该法案是“对开源的攻击”，开源模型可能会给其创建者带来额外的风险，因为像任何开放软件一样，它们更容易被修改并部署到任意和潜在的恶意目的；杨立坤则表示，SB 1047 将损害研究工作，并且是基于“少数妄想智囊团推动的‘生存风险’幻觉”</p><p></p><p>另外，越来越多的硅谷人士反对 SB 1047 法案，包括 OpenAI、Anthropic、政客 Zoe Lofgren 与 Nancy Pelosi 以及加州商会在内。8 月底，OpenAI 发布了一封公开信反对 SB 1047 法案。代表谷歌、苹果、亚马逊和其他大型科技巨头的贸易组织进步商会 ( Chamber of Progress ) 也发表公开信反对该法案，称 SB 1047 限制自由并“将科技创新赶出了加州”。</p><p></p><p>各方认为，这项法案总体过度关注灾难性危害，有可能会挤压小型开源 AI 开发组织的生存空间。作为回应，该法案也做出了修订，将潜在的刑事处罚更换为民事处罚，缩小了授予加州总检察长的执法权力，同时调整了加入法案中建议设立的“前沿模型委员会”的准入门槛。</p><p></p><p>根据《纽约时报》报道，这项 AI 安全法案即将被提交至加州州长加文·纽森处，由他在 9 月底之前决定其能否通过。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>看得出来，加州立法机构在 AI 监管立法方面动作频繁，但是连出的两个“大招”反对声音都比较大。如何更好地对新技术进行有效监管，也是摆在大家面前的一个新命题。</p><p></p><p>参考链接：</p><p></p><p><a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1f5eqfb/california_bill_set_to_ban_civitai_huggingface/\">https://www.reddit.com/r/ArtificialInteligence/comments/1f5eqfb/california_bill_set_to_ban_civitai_huggingface/</a>\"</p><p></p><p><a href=\"https://www.theverge.com/2024/8/28/24229068/california-sb-1047-ai-safety-bill-passed-state-assembly-governor-newsom-signature\">https://www.theverge.com/2024/8/28/24229068/california-sb-1047-ai-safety-bill-passed-state-assembly-governor-newsom-signature</a>\"</p><p></p><p><a href=\"https://techcrunch.com/2024/08/30/california-ai-bill-sb-1047-aims-to-prevent-ai-disasters-but-silicon-valley-warns-it-will-cause-one/\">https://techcrunch.com/2024/08/30/california-ai-bill-sb-1047-aims-to-prevent-ai-disasters-but-silicon-valley-warns-it-will-cause-one/</a>\"</p><p></p>",
    "publish_time": "2024-09-06 18:39:01",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "完整的数仓能力，ByConity 1.0 版本发布！",
    "url": "https://www.infoq.cn/article/OZOE1kuRuaPA07Qu2Ka6",
    "summary": "<p>2024年8月，ByConity 1.0 正式发布，翻开了 ByConity 新的一页。1.0 版本有哪些不同，以及 1.x 版本会重点迭代哪些能力，下面为大家一一解读。</p><p></p><h2>完整的数据仓库能力&nbsp;</h2><p></p><p></p><p>从 ByConity 开源之初，我们一直将产品定位为开源云原生数据仓库。区别于传统 OLAP 产品，ByConity 采用存算分离的云原生架构，通过这种架构获得了弹性和降低资源浪费的优势，但与此同时也在一定程度上提高了产品的复杂度。定位为云原生数据仓库，是希望能够承担更多类型、更复杂的分析任务负载，无论是在线的实时分析还是离线数据的清洗/加工任务都能够胜任。更全面的能力能够帮助用户降低数据分析平台的整体复杂度。</p><p></p><p>传统的 OLAP 产品通过数据索引、列式存储、向量化执行等技术，注重对实时分析或者&nbsp;Ad-hoc&nbsp;分析的快速反应，满足低时延的要求。在数据加载进 OLAP 产品之前，往往需要经过复杂的数据清洗和转换过程，也就是大家熟知的&nbsp;ETL&nbsp;任务。在传统的数据分析架构中，这部分工作是由 Hive、Spark、Flink 等产品来完成的。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/89/89df8819af9f8df13738058b45b674f5.webp\" /></p><p>在 ByConity 1.x版本中，增加了对 BSP 模式的支持，减少数据加工和数据分析之间多系统耦合带来的运维负担，使ByConity 能够一站式完成数据接入、加工和分析。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/bc/bc7e9c94cf621a0a8d2b3dc10edde8c0.webp\" /></p><p></p><p>1.0 版本中，在 BSP 模式下（settings bsp_mode = 1 打开 bsp 模式）增加了对 TableScan 算子并行度扩展的支持：一、通过设置 distributed_max_parallel_size，可以将 TableScan 的并行度进行扩展，实现资源平铺的功能，在资源有限的情况下实现对大表的处理；二、增加了对 task 重试的支持：通过设置 bsp_max_retry_num（task的最大重试次数，默认值为5），可以在作业的中间 task 发生失败时，从失败的 task 开始重试，而不是从头开始重试，进而大大减少 failover 对执行时长的影响。</p><p></p><p>后续的 1.x 版本中，我们还将推出基于资源感知的 BSP 模式，可以根据集群资源使用情况有序调度并发 ELT 任务，从而减少资源的挤占，避免频繁失败。</p><p></p><p></p><h2>湖仓一体&nbsp;&nbsp;</h2><p></p><p></p><p>在 ByConity 1.0 版本中的一个重要能力升级就是提升了湖仓一体的能力。ByConity 可以直接分析数据湖中的数据，而无需做数据搬迁，从而让用户可以更灵活的规划其数据分析架构。</p><p></p><p>Hive 外表查询性能在 1.0 版本中得到了非常大的提升。这主要得益于以下几点：</p><p></p><p>1、实现了外表的 Native Reader（Parquet/Orc），Native Reader 具有以下特点：</p><p><img src=\"https://static001.geekbang.org/infoq/0f/0f318ac5a1767cdebd370b5e3ee8b810.webp\" /></p><p></p><p>2、增加中间结果缓存。</p><p></p><p>3、结合 ByConity 查询优化器的统计信息自动收集，将 Filter 的有效下推，降低 IO 开销（1.x 版本）。</p><p>通过以上能力大大提升了 Hive 外表的查询性能，在 TPC-DS 测试中性能达到 Trino 的4倍。</p><p></p><p>除 Hive 外表外，在 1.0 版本中我们还支持了&nbsp;Hudi 和 GLUE 的外表查询能力。在后续的 1.x 版本中，我们还将支持&nbsp;Iceberg&nbsp;和 Paimon 的外表能力。</p><p></p><h2>MySQL 语义兼容&nbsp; &nbsp;&nbsp;</h2><p></p><p></p><p>在 ByConity 0.x 版本中，主要支持 SQL 标准是 ClickHouse SQL 和 Ansi SQL。除 ClickHouse 生态外，MySQL 同样是当前主流的 OLAP 产品生态。过去一年中很多用户反馈从 MySQL 生态产品迁移到 ByConity 过程中有比较复杂的业务改写，以及部分工具不兼容。</p><p></p><p>在 1.0 版本中，ByConity 已经完成了 90% 以上的语法、函数、数据类型、DQL、DML、DDL 的兼容。此外，如MySQL Workbench、DBeaver、Navicat 等 IDE 工具，Tableau、QuickBI、FineBI 等主流 BI 工具的兼容性也在当前版本中完成。</p><p></p><p>在 1.x 版本中，我们希望和社区的贡献者们一起，在存储介质、数据导入、IDE、BI、数据治理工具等方面全面提升 ByConity 广泛的生态工具兼容性。</p><p></p><h2>其他特性&nbsp;&nbsp;&nbsp;</h2><p></p><p></p><p>1、默认开启优化器，支持开优化器简单查询走 local 模式，优化了开启优化器后简单查询性能下降的问题。</p><p>2、优化 Unique 表的 TableWrite 重试能力，提升 Unique 表可用性。</p><p>3、新增 bucket join 相关的能力。</p><p>4、提升 map 函数性能。</p><p>5、优化 disk cache 加载策略，支持按比例配置。</p><p>6、string 数据类型转化为 map，支持 nullable string。</p><p>7、支持导出数据导文件目录，支持 Worker 导出数据。</p><p>8、支持表级别的快照能力。</p><p>9、（Preview）增强高并发点查性能。</p><p></p><h2>展望&nbsp;&nbsp;&nbsp;</h2><p></p><p></p><p>未来，我们还将持续为提升分析性能和打造全面的数仓能力而努力。除此之外，我们还将向一体化分析引擎的方向进行探索，继续打磨倒排索引的能力，以及向向量检索和时空分析等场景进行探索。</p><p></p><p>ByConity 1.0 完整 Changelog：</p><p><a href=\"https://github.com/ByConity/ByConity/releases/tag/1.0.0\">https://github.com/ByConity/ByConity/releases/tag/1.0.0</a>\"</p><p></p><p>关于 ByConity&nbsp;</p><p>ByConity 是字节跳动开源的云原生数据仓库，在满足数仓用户对资源弹性扩缩容，读写分离，资源隔离，数据强一致性等多种需求的同时，提供优异的查询，写入性能。</p><p>GitHub <a href=\"https://github.com/ByConity/ByConity\">https://github.com/ByConity/ByConity</a>\"</p>",
    "publish_time": "2024-09-06 18:44:06",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]