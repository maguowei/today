[
  {
    "title": "谷歌开源 AI 微调方法： Distilling Step-by-Step",
    "url": "https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP",
    "summary": "<p>华盛顿大学和谷歌研究中心的一个团队最近开源了 <a href=\"https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html\">Distilling Step-by-Step</a>\"（逐步蒸馏），一种用于微调规模较小的语言模型的技术。与标准微调相比，逐步蒸馏需要的训练数据更少，并且生成的模型更小，但模型性能却优于参数规模是它 700 倍的小样本提示大型语言模型 （LLM）。</p><p>&nbsp;</p><p>虽然 LLM 一般可以在提示较少的情况下在多种任务上有良好的表现，但由于其内存和算力要求过高，模型的托管是比较有挑战的。规模较小的模型在微调后也可以有良好的表现，但这需要工程师手动创建针对具体任务优化的数据集。逐步蒸馏的关键思想是使用 LLM 自动生成一个小型微调数据集，其中的数据有一个输入和一个输出标签，以及选择这个输出标签的“理由”。微调过程会训练这个小模型来预测输出标签并生成对应的理由。在 NLP 基准上评估时，小型微调模型的性能优于 540B PaLM 模型，同时仅需要这个基准测试的全部微调数据的 80%。据谷歌称：</p><p></p><p></p><blockquote>我们展示了，逐步蒸馏既减少了构建针对特定任务的较小模型所需的训练数据集规模，也减少了实现甚至超越小样本提示 LLM 的性能水平所需的模型大小。总的来说，逐步蒸馏提出了一种可以高效利用资源的范例，可以解决模型大小和所需训练数据之间的权衡问题。</blockquote><p></p><p></p><p>研究表明，增加 LLM 中的参数规模可以提高其性能，目前最先进的模型（例如 PaLM）拥有数百亿个参数。然而，这些大型模型价格昂贵，且难以用于推理，因为它们需要多个并行连接的 GPU 才能把这么多参数保存在内存里。最近的研究开发出了规模稍小的模型（例如 Meta 的 Llama 2），其性能表现差不多，但参数少了一个数量级；然而，这些小一些的模型还是很庞大，需求的算力也很高。</p><p>&nbsp;</p><p>要做出在特定任务上表现良好的小模型的一种方法，是使用针对具体任务收集的数据集来微调小规模语言模型。虽然这个数据集可能相对较小（大约有数千个示例），但其数据收集起来可能还是费时费钱。另一种选择是知识蒸馏，也就是使用大型模型作为较小模型的老师。 InfoQ 最近报道了谷歌开发的一项<a href=\"https://www.infoq.com/news/2023/01/google-llm-self-improvement/\">技术</a>\"，使用 PaLM LLM 来创建训练数据集，最后生成的微调模型的性能可与规模大 10 倍的 LLM 相媲美。</p><p>&nbsp;</p><p>逐步蒸馏确实需要微调数据集，但它减少了创建高性能模型所需的数据量。源数据集通过思维链提示输入 PaLM LLM，要求模型给出其答案的理由。输出结果是修正后的微调数据集，其中包含原始输入和答案以及理由。这个较小的目标模型经过微调来执行两项任务：回答原始问题并生成理由。</p><p>&nbsp;</p><p>谷歌使用四个 NLP 基准测试评估了他们的技术，每个基准都包含一个微调数据集。他们使用逐步蒸馏来修正这些数据集，并使用了参数不到 1B 的微调 T5 模型。他们发现，这些模型在仅使用数据集的一小部分数据的情况下，性能就比基线微调模型要好；在某些情况下只要 12.5% 的数据就有这样的表现。他们还发现，他们的 770M 参数模型在 ANLI 基准测试中的性能优于大它 700 倍的 540B 参数 PaLM，同时只需要 80% 的微调数据集数据。</p><p>&nbsp;</p><p>在 X（以前的 Twitter）上关于这项工作的讨论中，人工智能企业家 Otto von Zastrow 写道：</p><p></p><p></p><blockquote>这些结果非常厉害。我会把这种办法叫做合成数据生成，而不是蒸馏，我真的很好奇，如果你根据每个示例问题的合成理由来训练原始的 LLM 会发生什么事情。</blockquote><p></p><p></p><p>逐步蒸馏的源代码和训练数据集可在 <a href=\"https://github.com/google-research/distilling-step-by-step\">GitHub</a>\" 上获取。 Google Cloud 的 Vertex AI 平台还提供该算法的非公开预览。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/10/google-distillation/\">https://www.infoq.com/news/2023/10/google-distillation/</a>\"</p>",
    "publish_time": "2023-11-08 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国卓越技术团队访谈录 & 架构师特刊：软件产品中的AIGC",
    "url": "https://www.infoq.cn/article/Ba7kqNscXOQoaZAduZsz",
    "summary": "<h2>封面故事</h2>\n<ul>\n<li>我，一个 95 后，从阿里辞职与贾扬清去硅谷创业</li>\n</ul>\n<p>“建议大家不要被大模型束缚住。实际落地时，除了大模型外，还可以充分利用许多已存在的深度学习模型或传统模型。例如在图像处理方面，卷积神经网络（CNN）实际上可能比大模型更适用。”</p>\n<h2>独家对话·大模型领航者</h2>\n<ul>\n<li>被时代选中的智谱AI：成为OpenAI，超越OpenAI</li>\n</ul>\n<p>在创业早期，智谱AI不会强迫自己去接复杂的客户需求，因为这些需求很可能让团队陷入其中无法自拔。“更复杂的问题需要暂时搁置、等到能力更成熟时再解决。”智谱AI会坦诚自己的能力在什么水平上，在该水平上可以创造什么样的价值。</p>\n<ul>\n<li>丢掉LangChain、像Docker一样编排大模型应用程序：这支十余人的年轻创业团队如何在2个月做出一个LLMOps平台？</li>\n</ul>\n<p>“在创业初期，一些朋友和投资人认为市场潜力巨大，但竞争也激烈。云服务提供商、大模型公司以及机器学习和运营的公司都进入这个领域。我们的挑战在于如何应对竞争。”</p>\n<ul>\n<li>是全部重做还是融合改造？揭秘京东云言犀升级全过程</li>\n</ul>\n<p>大模型对一些传统技术确实有影响。因为LLM之前的模型几乎全都是专项的，任务越窄，表现越好。对于更通用、泛化的任务来说，大模型带来的效果确实是颠覆性的。</p>\n<h2>AIGC 实践前沿</h2>\n<ul>\n<li>文生图大型实践：揭秘百度搜索AIGC绘画工具的背后故事</li>\n</ul>\n<p>这是一个巨大的变革，从过去用户在全网寻找图像，转变为结合了查找图像和生成图像两种方式，以满足用户更具体的需求，这也在一定程度上鼓励用户更主动地表达他们真正的需求。</p>\n<ul>\n<li>AIGC 编程：代码编程模型的应用与挑战</li>\n</ul>\n<p>大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。</p>\n<p>AIGC算法揭秘及产业落地应用分享</p>\n<p>使用大模型在某种程度上为我们提供了更多的可能性，但也引入了更多的复杂性。没有一个通用的模板或规则来告诉我们应该使用哪个模型。</p>\n<ul>\n<li>广告创意领域中AIGC的应用</li>\n</ul>\n<p>随着AI能力的提升，人们需要深入使用并掌握它。像fine-tuning、LangChain等我不会推荐，我会鼓励周围的人去深入使用AIGC，重点在于使用，好的AGI只需要被编译一次。</p>\n<h2>管理能力进阶</h2>\n<ul>\n<li>影响力打造：一位前 Twitter 8 年技术主管总结的经验教训</li>\n<li>日常沟通之道：走向果敢</li>\n<li>科技巨头是如何迷失方向的？探讨大型科技企业的问责制度</li>\n</ul>\n<h2>文章推荐</h2>\n<ul>\n<li>大模型部署昂贵的原因：用最贵的模型处理最基本任务，犹如“让兰博基尼送披萨”</li>\n<li>2023 年 AI 与开源行业：今年第一篇盘点文章出炉了</li>\n<li>ChatGPT 已成为2023年最大金矿，大家是怎么靠它挣到钱的？</li>\n</ul>",
    "publish_time": "2023-11-08 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris 2.0 如何实现导入性能提升 2-8 倍",
    "url": "https://www.infoq.cn/article/8kEox8KdpTjOh4s1gRH4",
    "summary": "<p>数据导入吞吐是 OLAP 系统性能的重要衡量标准之一，高效的数据导入能力能够加速数据实时处理和分析的效率。随着<a href=\"http://doris.apache.org/\"> Apache Doris</a>\" 用户规模的不断扩大， 越来越多用户对数据导入提出更高的要求，这也为 Apache Doris 的数据导入能力带来了更大的挑战。</p><p></p><p>为提供快速的数据写入支持，Apache Doris 存储引擎采用了类似 LSM Tree 结构。在进行数据导入时，数据会先写入 Tablet 对应的 MemTable 中，MemTable 采用 SkipList 的数据结构。当 MemTable 写满之后，会将其中的数据刷写（Flush）到磁盘。数据从 MemTable 刷写到磁盘的过程分为两个阶段，第一阶段是将 MemTable 中的行存结构在内存中转换为列存结构，并为每一列生成对应的索引结构；第二阶段是将转换后的列存结构写入磁盘，生成 Segment 文件。</p><p></p><p>具体而言，Apache Doris 在导入流程中会把 BE 模块分为上游和下游，其中上游 BE 对数据的处理分为 Scan 和 Sink 两个步骤：首先 Scan 过程对原始数据进行解析，然后 Sink 过程将数据组织并通过 RPC 分发给下游 BE。当下游 BE 接收数据后，首先在内存结构 MemTable 中进行数据攒批，对数据排序、聚合，并最终下刷成数据文件（也称 Segment 文件）到硬盘上来进行持久化存储。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d42ba7c3789fe212c67c8382350dbf46.png\" /></p><p></p><p>而我们在实际的数据导入过程中，可能会出现以下问题：</p><p></p><p>因上游 BE 跟下游 BE 之间的 RPC 采用 Ping-Pong 的模式，即下游 BE 一个请求处理完成并回复到上游 BE 后，上游 BE 才会发送下一个请求。如果下游 BE 在 MemTable 的处理过程中消耗了较长的时间，那么上游 BE 将会等待 RPC 返回的时间也会变长，这就会影响到数据传输的效率。当对多副本的表导入数据时，需要在每个副本上重复执行 MemTable 的处理过程。然而，这种方式使每个副本所在节点都会消耗一定的内存和 CPU 资源，不仅如此，冗长的处理流程也会影响执行效率。</p><p></p><p>为解决以上问题，我们在刚刚发布不久 Apache Doris 2.0 版本中（https://github.com/apache/doris/tree/2.0.1-rc04 ），对导入过程中 MemTable 的攒批、排序和落盘等流程进行优化，提高了上下游之间数据传输的效率。此外我们在新版本中还提供 “单副本导入” 的数据分发模式，当面对多副本数据导入时，无需在多个 BE 上重复进行 MemTable 工作，有效提升集群计算和内存资源的利用率，进而提升导入的总吞吐量。</p><p></p><h1>MemTable 优化</h1><p></p><p></p><h2>01  写入优化</h2><p></p><p></p><p>在 Aapche Doris 过去版本中，下游 BE 在写入 MemTable 时，为了维护 Key 的顺序，会实时对 SkipList 进行更新。对于 Unique Key 表或者 Aggregate Key 表来说，遇到已经存在的 Key 时，将会调用聚合函数并进行合并。然而这两个步骤可能会消耗较多的处理时间，从而延迟 RPC  响应时间，影响数据写入的效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef6c90e63726cbf189a4dac894d0577b.png\" /></p><p></p><p>因此我们在 2.0 版本中对这一过程进行了优化。当下游 BE 在写入 MemTable 时，不再实时维护 MemTable 中 Key 的顺序，而是将顺序的保证推迟到 MemTable 即将被下刷成 Segment 之前。此外，我们采用更高效的 pdqsort 来替代 std::sort ，实现了缓存友好的列优先排序方式，并取得了更好的排序性能。通过上述两种手段来保证 RPC 能够被及时响应。</p><p></p><h2>02  并行下刷</h2><p></p><p></p><p>在导入过程中，当下游 BE 将一个 MemTable  写入一定大小之后，会把 MemTable 下刷为 Segment 数据文件来持久化存储数据并释放内存。为了保证前文提到的 Ping-Pong RPC 性能不受影响，MemTable 的下刷操作会被提交到一个线程池中进行异步执行。</p><p></p><p>在 Apache Doris 过去版本中，对于 Unique Key 的表来说，MemTable 下刷任务是串行执行的，原因是不同 Segment 文件之间可能存在重复 Key，串行执行可以保持它们的先后顺序，而 Segment 序号是在下刷任务被调度执行时分配的。同时，在 Tablet 数量较少无法提供足够的并发时，串行下刷可能会导致系统的 IO 资源无法重复被利用。而在 Apache Doris 2.0 版本中，由于我们将 Key 的排序和聚合操作进行了后置，除了原有的 IO 负载以外，下刷任务中还增加了 CPU 负载（即后置的排序和聚合操作）。此时若仍使用串行下刷的方式，当没有足够多 Tablet 来保证并发数时，CPU 和 IO 会交替成为瓶颈，从而导致下刷任务的吞吐量大幅降低。</p><p></p><p>为解决这个问题，我们在下刷任务提交时就为其分配 Segment 序号，确保并行下刷后生成的 Segment 文件顺序是正确的。同时，我们还对后续 Rowset 构建流程进行了优化，使其可以处理不连续的 Segment 序号。通过以上改进，使得所有类型的表都可以并行下刷 MemTable，从而提高整体资源利用率和导入吞吐量。</p><p></p><h2>03  优化效果</h2><p></p><p></p><p>通过对 MemTable 的优化，面对不同的导入场景，Stream Load 的吞吐量均有不同幅度的提升（详细对比数据可见下文）。这项优化不仅适用于Stream Load ，还对 Apache Doris 支持的其他导入方式同样有效，例如 Insert Into、Broker Load、S3 Load 等，均在不同程度提升了导入的效率及性能。</p><p></p><h1>单副本导入</h1><p></p><p></p><h2>01  原理和实现</h2><p></p><p></p><p>在过去版本中，当面对多副本数据写入时，Apache Doris 的每个数据副本均需要在各自节点上进行排序和压缩，这样会造成较大的资源占用。为了节约 CPU 和内存资源，我们在 Apache Doris 在 2.0 版本中提供了单副本导入的能力，该能力会从多个副本中选择一个副本作为主副本（其他副本为从副本），且只对主副本进行计算，当主副本的数据文件都写入成功后，通知从副本所在节点直接接拉取主副本的数据文件，实现副本间的数据同步，当所有从副本节点拉取完后进行返回或超时返回（大多数副本成功即返回成功）。该能力无需一一在节点上进行处理，减少了节点的压力，而节约的算力和内存将会用于其它任务的处理，从而提升整体系统的并发吞吐能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10f6e87b64f6c1731a0c3c1841aceb35.jpeg\" /></p><p></p><h2>02  如何开启</h2><p></p><p>FE 配置：</p><p></p><p><code lang=\"text\">enable_single_replica_load = true\n</code></p><p></p><p>BE 配置：</p><p></p><p><code lang=\"text\">enable_single_replica_load = true\n</code></p><p></p><p>环境变量（insert into）</p><p></p><p><code lang=\"text\">SET  experimental_enable_single_replica_insert=true;\n</code></p><p></p><h2>03  优化效果</h2><p></p><p></p><p>对于单并发导入来说，单副本数据导入可以有效降低资源消耗。单副本导入所占的内存仅为三副本导入的 1/3（单副本导入时只需要写一份内存，三副本导入时需要写三份内存）。同时从实际测试可知，单副本导入的 CPU 消耗约为三副本导入的 1/2，可有效节约 CPU 资源。对于多并发导入来说，在相同的资源消耗下，单副本导入可以显著增加任务吞吐。同时在实际测试中，同样的并发导入任务， 三副本导入方式耗时 67 分钟，而单副本导入方式仅耗时 27 分钟，导入效率提升约 2.5 倍。具体数据请参考后文。</p><p></p><h1>性能对比</h1><p></p><p></p><p>测试环境及配置：</p><p></p><p>3 个 BE (16C 64G)，每个 BE 配置 3 块盘 （单盘读写约 150 MB/s）1 个 FE，共享其中一个 BE 的机器</p><p></p><p>原始数据使用 TPC-H SF100 生成的 Lineitem 表，存储在 FE 所在机器的一个独立的盘上（读约 150 MB/s）。</p><p></p><h2>01  Stream Load（单并发）</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e3266cd4a7d540af44ec9fd4b007de4.png\" /></p><p></p><p>以上述列举的单并发场景来说，Apache Doris 2.0 版本整体的导入性能比 1.2.6 版本提升了 2-7 倍；在多副本前提下，开启新特性单副本导入，导入性能提升了 2-8 倍。</p><p></p><h2>02  INSERT INTO （多并发）</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/de06ecae8973af9677b86655f5d91621.png\" /></p><p></p><p>以上述列举的多并发场景来说，Apache Doris 2.0 版本整体比 1.2.6 版本有小幅提升；开启新特性单副本导入后，对在多副本提导入性能提升效果明显，导入速度较 1.2.6 版提升约 50% 。</p><p></p><h1>结束语</h1><p></p><p></p><p>社区一直致力于提升 Apache Doris 导入性能这一核心能力，为用户提供更佳的高效分析体验，通过在 2.0 版本对 Memtable、单副本导入等能力进行优化，导入性能相较于之前版本已经呈现数倍提升。未来我们还将在 2.1 版本中持续迭代，结合 MemTable 的优化方法、单副本优化资源能效理念，以及基于 Streaming RPC 优化后的 IO 模型和精简的 IO 路径对导入性能进一步优化，同时减少导入对查询性能的影响，为用户提供更加卓越的数据导入体验。</p><p></p><p># 作者介绍：</p><p></p><p>陈凯杰，<a href=\"https://selectdb.com/\">SelectDB </a>\"高级研发工程师</p><p></p><p>张正宇，SelectDB 资深研发工程师</p>",
    "publish_time": "2023-11-08 09:51:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "AIGC时代，AI Agent的商业模式和创新机会 ｜InfoQ《极客有约》",
    "url": "https://www.infoq.cn/article/xLSmwEFmDqpIafLzN3io",
    "summary": "<p>后AIGC时代，AI Agent无疑是一个新沸点。AI Agent（人工智能体）是一种能够感知环境、进行决策和执行动作的智能实体。不同于传统的人工智能，AI Agent 具备通过独立思考、调用工具去逐步完成给定目标的能力。</p>\n<p>AI Agent 和大模型的区别在于，大模型与人类之间的交互是基于prompt 实现的，用户prompt 是否清晰明确会影响大模型回答的效果。而AI Agent的工作仅需给定一个目标，它就能够针对目标独立思考并做出行动。</p>\n<p>从原理上说，AI Agent的核心驱动力是大模型，在此基础上增加规划（Planning）、记忆（Memory）和工具使用（Tool Use）三个关键组件。那么，这种智能体的出现为行业发展传递出了什么样的讯号？将带来哪些机遇和变革？是不是下一个超级app的机会？&nbsp;国内哪些公司有可能跑出？</p>",
    "publish_time": "2023-11-08 10:48:52",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“算”赋千行，“智”启新程，天翼云多项成果惊艳亮相，邀您共鉴！",
    "url": "https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON",
    "summary": "<p> 近年来，我国数字化基础设施建设不断完善，人工智能产业蓬勃发展，成为当下驱动经济社会转型升级的重要力量。为抢抓人工智能发展的重大机遇，构筑我国人工智能发展先发优势，国家陆续出台了多项政策，将人工智能列为国家战略性新兴产业，鼓励人工智能行业发展与创新。</p><p></p><p>在此背景下，智算作为人工智能时代的关键生产力要素，需求呈爆发式增长。为推进算力基础设施高质量发展，充分发挥算力对数字经济的驱动作用，近日，工业和信息化部、中央网信办、教育部等六部门联合印发《算力基础设施高质量发展行动计划》明确提出，结合人工智能产业发展和业务需求，重点在西部算力枢纽及人工智能发展基础较好地区集约化开展智算中心建设，逐步合理提升智能算力占比。根据IDC报告显示，预计到2026年，中国智能算力的年复合平均增长率达到52.3%，是三倍于通用算力规模的增长速度。</p><p></p><p> 面对激增的智算需求，天翼云作为云服务国家队，从多方位升级算力基础设施，为人工智能产业发展夯实算力底座，加速科技普惠。技术方面，天翼云始终坚持科技创新，不断攻克关键核心技术，以云操作系统为核心，从底层基础软硬件技术，到上层高阶云能力，实现了全栈技术的自主可控。基础设施方面，天翼云不断完善“2+4+31+X”云网融合资源布局，构建了“集中化+区域化+属地化+边缘化”的云网基础设施，积极推进算力普惠发展；天翼云建设的新一代智算中心在算力、算效、资源利用率等方面不断追求极致，降低大模型训练、推理、部署、应用门槛。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ce40fa919dc071d54806d97739a0aef3.png\" /></p><p></p><p> 人工智能浪潮下，天翼云凭借项目沉淀、技术积累，打造智能计算平台，依托分布式架构云底座和充沛的计算、存储、网络资源，为大模型训练、智能推荐、无人驾驶、生命科学、NLP等业务场景提供智算、超算、通算多样化算力服务，激发数字经济发展新活力。同时，天翼云在通用算力资源全国布局的基础上，科学规划建设智能算力，不断夯实国云智算底座，构建AI时代强大基石。</p><p></p><p> 当今社会，科技高速发展，每一轮技术变革无不渗透在我们的日常生活中。站在大算力、大模型、大数据的“高起点”上，天翼云致力于以科技创新服务千行百业，加速算力普惠民生。为进一步推动人工智能应用落地，加速生产生活方式智慧变革，11月10日—13日，以“数字科技 焕新启航”为主题的2023数字科技生态大会即将启幕。</p><p></p><p> 届时，天翼云将亮相大会主论坛及多个分论坛，重磅发布智算领域科技创新最新成果，并带来云电脑等产品的最新升级，同时在展区，天翼云也将从科技创新、算力底座、产业引领等层面，系统性展示领先的云能力和实践成果，以国云筑基，携手业界共创智算新时代</p>",
    "publish_time": "2023-11-08 11:16:07",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "火山引擎金融解决方案负责人王建军确认出席 FCon，分享金融数字化升级：让智慧带来生产力",
    "url": "https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw",
    "summary": "<p><a href=\"https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle\">FCon 全球金融科技大会</a>\"，将于 11 月在上海召开。火山引擎金融解决方案负责人王建军将发表题为《<a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5606?utm_source=infoqweb&amp;utm_medium=article\">金融数字化升级：让智慧带来生产力</a>\"》主题分享，介绍国内外大模型发展及应用现状、火山引擎在金融行业的探索实践，以及大模型在未来行业的应用。</p><p></p><p><a href=\"https://fcon.infoq.cn/2023/shanghai/presentation/5606?utm_source=infoqweb&amp;utm_medium=article\">王建军</a>\"，10 年以上人工智能及数字化实践经验，2020 年加入字节跳动，曾服务于德勤咨询、第四范式等企业，推动过数十家金融机构的数字化转型和人工智能应用实践。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型：让智慧带来生产力</p><p></p><p>通过观察海外大模型领先应用实践，结合国内产业生产效率痛点，明确大模型应用蓝图，及围绕蓝图展开的探索与实践。</p><p></p><p>演讲提纲：</p><p></p><p>海内外大模型风起云涌；大模型能力范畴和典型应用分析；围绕金融行业的探索实践；展望未来。</p><p></p><p>你将获得：</p><p></p><p>○ 了解到国内外大模型发展及应用现状；</p><p>○ 了解火山引擎在金融行业的探索实践；</p><p>○ 共同畅想大模型未来行业应用。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle\">DevOps&nbsp;在金融企业落地实践</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle\">金融行业大模型应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle\">创新的金融科技应用</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle\">金融实时数据平台建设之路</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle\">金融安全风险管控</a>\"、<a href=\"https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle\">数据要素流通与数据合规</a>\"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！大会 8 折优惠报名倒计时仅剩 3 天，现在购票立减￥1360。咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png\" /></p><p></p>",
    "publish_time": "2023-11-08 11:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "平安银行数据研发治理一体化平台实践",
    "url": "https://www.infoq.cn/article/RZJDKpFIZ7erv9EuYiVd",
    "summary": "<p>金融大数据体系错综复杂，随着业务数据爆炸式增长以及公众对数据关注度的不断提高，体系化的数据治理变得至关重要。然而，传统治理方法存在落地难、效果差、难衡量等问题。</p><p></p><p>在<a href=\"https://archsummit.infoq.cn/202307/shenzhen/\"> ArchSummit 全球架构师峰会（深圳站）</a>\"上，平安银行数据资产管理及研发中心 / 数据及 AI 平台团队负责人廖晓格，深入探讨了金融数据体系的复杂性以及如何有效地进行智能化数据治理。他强调了构建金融级数据研发治理一体化平台的重要性，并分享了如何将数据治理体系落地到研发流程中，实现数据价值最大化的方法。本文为演讲整理，期待对你有所启发。</p><p></p><p>在11月19-20日举办的<a href=\"https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5\">FCon全球金融科技大会</a>\"上，廖晓格还将带来《金融级数据研发DataOps落地实践》的主题分享。敬请关注！</p><p></p><p>以下是ArchSummit深圳站演讲全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><p>今天分享的主题涵盖了金融行业在数据治理过程中遇到的困难与挑战、开发治理一体化平台体系的构建，以及如何进行数据虚拟化的未来展望。随着数据爆炸式增长，各行各业都面临着庞大而复杂的数据、存储成本的剧增、计算复杂度的增加、数据安全隐患等问题，这些挑战归结起来即安全、提效、降本；安全视角，如何保证数据安全分析和应用，以及敏感数据如何不泄露；提效层面，如何提升数据研发效率、如何打破不同部门、组织间的数据孤岛，提升数据融合价值应用；降本层面，如何精准量化数据成本与价值、如何量化分析数据 ROI 价值；</p><p></p><p>除此之外，对金融行业来说，大量分析师在工作期间随时会对海量数据进行分析，如何保证大数据平台 7x24 小时稳定运行至关重要；在数据测试方面，传统的方法是将生产数据脱敏后迁移到测试环境，但这一过程复杂高又难以形成闭环，并且可能有数据安全隐患；在数据时效方面，未来实时数仓的需求会日益增长；若希望数据被有效管理，成为有用的资产，形成数据生态，那数据治理就成为重中之重。</p><p></p><h4>数据治理目标</h4><p></p><p></p><p>数据治理的目标是希望能在确保数据安全及成本价值最大化的前提下，将数据治理与数据研发过程融合，实现治理线上化、治理标准化、治理智能化；将治理方法论与平台工具相结合，实现治理线上化；将数据治理涉及的多个平台、不同角色、流程、标准等融合并集成统一的数据研发治理能力工具，实现治理标准流程；更进一步，我们追求数据治理的智能化，通过内置规则和策略，实现自动识别安全隐患、敏感数据等，提升数据治理效果。而实现“三化”的同时，也不能忘记降低数据成本。</p><p></p><p>数据治理的最终目标是实现核心数据资产的自动沉淀，并为内外部提供稳定、高效的数据服务，实现最大化数据价值。</p><p><img src=\"https://static001.geekbang.org/infoq/38/383dba904283f20bc1446e136c125be4.png\" /></p><p></p><p>金融机构数据治理体系，即一套标准、一个平台、一套治理、一套资产；</p><p></p><p>基层是数据治理标准，数据治理团队需要制定相应的治理方案与治理规划。除此之外，数据研发的流程和规范的标准化也是数据治理的重中之重。</p><p>数据研发治理一体化平台融合研发全流程，在数据需求阶段，我们通常会进行影响分析和数据架构评审，以确保数据治理标准得以贯彻落实。在数据研发阶段，我们重点关注源数据治理、数据血缘和数据质量治理，以确保数据的质量和一致性。此外，指标研发阶段，我们还定义了指标规范、度量规范和维度属性规范，以提供更简单的数据使用路径，满足业务需求。当进入数据应用阶段时，我们采用统一的报表服务和 OneService 服务，为外部提供统一的数据服务，以提高数据服务效率。</p><p></p><p>在治理过程中，除了配置规则外，我们的重点是对开发流程进行贯标落地，确保数据质量的可靠性以及数据价值的评估。同时，我们还需要持续跟踪和监控规范的执行情况，关注数据应用的成本和效益，维护数据的健康度，通过评估每个数据表和指标的价值和成本，及时清除低价值或高成本的数据，从而提高数据治理的效率和准确性。</p><p></p><h4>开发治理一体化解决方案</h4><p></p><p></p><p>在数据治理方面，数据开发治理一体化平台重视规范与研发过程的融合，为确保统一性和高效性，所有数据研发规范都已纳入我们的数据治理体系中进行集成设计。在制定规范时，我们始终坚持先深入设计，后向外提供服务，以数据服务和数据指标驱动我们的数据研发流程。目前我们的数据开发流程覆盖数据同步集成、批流数据加工、指标研发等，我们致力于将数据治理规范真实落地于每一步的数据研发中，确保数据研发与数据治理的紧密结合。此外，通过全流程的数据研发，我们成功塑造了银行的核心数据资产，其中包括业务元数据、数据模型、数据表、指标和 API 等。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/61/6196f2caa5301429e9d55b0241b8a6cf.png\" /></p><p></p><p>具体看一下数据治理融入研发流程在每个阶段中的重点设计。</p><p></p><p>在设计阶段，对数据标准、数据架构、数据治理、元数据等进行规范化定义；对模型事实表、维度表进行设计 / 注册；在研发阶段，在数据集成时，我们将自动标识敏感字段，并在清洗阶段对其进行加密或逻辑脱敏处理；在数据研发时，需要构建完整的元数据资产，包括基础元数据和业务元数据。在发布阶段会对数据进行全流程自动测试，确保数据质量。而运营阶段的核心则是评估数据的成本与价值，最大化数据的 ROI。</p><p><img src=\"https://static001.geekbang.org/infoq/5f/5fe92e669e68231d881ee3fea8979141.png\" /></p><p></p><p>数据模型设计阶段，元数据是基础能力核心，遵循数仓分层、元数据命名规范、数据标准落标等，通过开发治理工具执行。</p><p></p><p>元数据从产生到应用会经过采集层、逻辑层；其中采集区的作用是能够面向自动构建出技术元数据和业务元数据，为逻辑元数据构建提供基础元数据。业务元数据源于业务逻辑和业务管理诉求，而技术元数据则覆盖库 / 表 / 字段、数据血缘等技术细节。</p><p></p><p>为方便场景端使用，我们建立了元数据逻辑层，供资产运营人员进行数据目录挂载、资产分类、资产生命周期管理等工作。其中，资产管理环节还包括资产打标、属性标注和业务流程标识，从而形成完整的业务数据地图，并向外部提供元数据服务，确保数据安全、数据权限，并实现元数据考核应用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b3/b309c6c54fe8376296ff0d4784e3e3f1.png\" /></p><p></p><p>在元数据的全生命周期过程中，我们基于行内统一数据标准规范，对元数据进行强制检查，确保元数据可用、可管、可控，包括①数据分层强制约束，包括 ODS 层、DWD 层和 ADS 层，确保数仓的严格管理，防止跨层访问；②我们为业务归宿设定标识；③命名规范自动化。我们设置了特定的词根，使得当用户命名字段时，相关的英文字段能被自动识别；④码值落标线上化。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/48/48ca464e2d7dbdaedfbb9d5b5ac511f3.png\" /></p><p></p><p>我们也非常重视血缘治理能力，整个平台 99% 的表级字段级血缘都是通过平台自动生成。在金融数仓体系里，数据加工链路复杂，上下游作业影响，重复链路等都会影响数据研发和运维效率；我们目前在 2 个过程中会进行血缘治理，其一是数据研发过程中的血缘链路治理，主要包括血缘层级依赖检查和分层依赖检查，血缘层级依赖检查包括目标作业与 DW 层作业的血缘层级深度，对于层级链路太深（15 层），禁止上线，调整脚本优化链路关系；分层依赖检查依据 ODS-DWD-ADS 分层规范，禁止进行跨层依赖；</p><p></p><p>其二是运营过程治理，包括运营时效治理和运营成本治理，运营时效根据高保作业的时效和运行时间要求，依据血缘关系自动线上化链路监测，并分析延迟影响并及时启动异常 noc；运营成本治理通过血缘链路关系计算数据热度，并针对冷数据自动实现下线管理，减少集群存储和计算成本。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9ead062ab2c337bd4c6126c182876d92.png\" /></p><p></p><p>另外血缘治理也帮助我们对所有作业的脚本进行自动解析并生成依赖关系，支持数据作业自动调度。</p><p>其中值得关注的是，①依赖关系长度：对于那些依赖关系特别长的情况，我们会检查其时效性。如发现依赖过长，我们会对其进行优化。②使用频次：通过血缘分析，我们可以观察到各表的使用情况。对于那些三个月未被访问或访问频次较低的表，我们会进行成本治理。③依赖并发：在调度过程中，我们控制并发并定义调度流程，确保数据开发者不必担心表与表之间的依赖关系。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c9/c9429b972657882ec4cb8c5b427a5d23.png\" /></p><p></p><p>数据质量治理涵盖事前校验、事中核验、事后复盘的闭环治理；</p><p></p><p>事前，我们在数据开发流程中支持自动 / 人工为表和字段设置一定监控规则，如唯一键的监测以及字段内容的监控。事中，依据母表跑批执行成果，自动执行监控检查，如发现监控异常直接进行预警，针对数据异常严重可能会造成下游数据应用场景事故的，会直接阻断整个数据处理流程，对于这些异常阻断，平台会自动告警并跟踪。事后，由质量监控平台追踪质量问题并复盘。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/39/391fb6837f42d7836cae0e4ecf0e4562.png\" /></p><p></p><p>开发测试一体化也是数据治理的重要工具。在数据测试验证场景下，很多公司可能会将数据复制到测试环境，但这样就断裂了数据链路，无法形成闭环，数据安全也无法保证。因此，我们希望所有的数据研发都能在生产环境下完成，包括研发、测试、发布等全流程数据研发过程，直接解决测试难点及问题，并且基于平台资源隔离能力以及安全保障，实现数据全线上化开发，提升数据敏捷度的同时又能保证数据质量。研发、验证、测试过程全部基于生产环境数据安全保护伞进行数据使用，确保数据安全的同时兼顾数据的高保测试。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ab/ab49588abacadf830344fc6a76eb85f7.png\" /></p><p></p><p>最后，在数据安全方面，对于全行数据安全保护也从事前、事中、事后三个视角进行安全管理。</p><p></p><p>事前，我们会协同安全 / 风险专员设定一系列安全规范；事中，我们执行多种安全技术措施，如数据加密、数据脱敏、敏感客群保护、数据外发规则以及实施智能阻断措施，确保整体数据受到充分保护；事后，我们设定规则引擎，对平台的所有访问记录进行自动审查。通过这一机制，一旦发现存在安全隐患的访问记录，平台会自动触发报警。例如，若某用户频繁查询某个特定客户，且该客户信息属于敏感字段或敏感客群，我们会立即阻断此类查询。</p><p></p><p>此外，我们的平台提供统一的 SQL 引擎进行数据查询。在此引擎中，我们执行血缘分析、权限判断等操作。最重要的是，当引擎识别到敏感数据时，会先进行脱敏处理，然后再为用户提供结果。</p><p></p><p>接下来详细介绍一下敏感数据发现的过程；在数据处理流程中，当业务数据被采集至大数据平台后，通过规则算法和人工标注，能够准确地识别到贴源层中的所有敏感字段，接着在 MID 层进行标准化自动敏感打标，并依赖血缘关系将这些敏感标记向下游传递。应用端查询数据时，根据敏感标识对访问的敏感字段及敏感脱敏类型进行加密、脱敏处理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/578469ce185b4e361e8e2fb55bea0eb0.png\" /></p><p></p><p>举个例子，采集表 A 中存在高度敏感的字段，身份证号、姓名和邮箱，我们会采取物理加密措施；而在数据查询、使用时，我们首先通过内部规则库自动识别并标记敏感字段。随后，我们通过血缘关系将这些标识传递至下游的依赖表中，比如识别到数据表 A 中存在敏感字段，根据下游依赖关系，我们识别到数据表 B 中同样存在敏感字段，并自动完成安全打标和打标继承；在此基础上，我们会进一步通过人工复核确认这些标识是否与业务标准相符。复核结束后，相关数据表便可以上线并对外提供服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/96/9679738e176436a5a4274b7f591acc1c.png\" /></p><p></p><p>随着大数据平台的发展，表字段的加密方式可能会发生变化。但是，对于整个平台来说，不可以频繁改变字段的加密方式。如果一个字段在多个表中广泛使用，并且其加密方式更改，这会带来非常大影响。</p><p></p><p>为解决这一问题，我们通过对元数据标注的方式来进行加密。例如，如果一个手机号最初采用 A 方式加密，但后续这种加密方式改为 B，我们会对每个表的分区进行标注，并在最终的查询结果中，统一所有不同加密方式的数据到最新的加密格式，再对外提供统一的查询能力，这种技术方案将对业务造成的影响降到最低，也不需要对历史数据进行跑批更新，这种能力可以为平台节省巨大的大数据“翻数”成本。</p><p></p><p>以 MapReduce 为例讲解下基于元数据的加密方案，编译阶段会通过 MapReduceCompiler 访问元数据信息，并将其存储在特定文件中。在运行阶段，每个操作单位会读取这些信息，并根据最新的加密算法加密数据。在 PostExecutionHook 阶段，任何写入操作都会被再次写回到原表，并在元数据上打上标签，标示其元数据的加密方式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/c4/c4f75ff298c7aab5950330df04d6a905.png\" /></p><p></p><p>举个例子，当表字段采用特定的加密算法如 IDX 时，我们会在作业执行期间生成相应的元数据信息。这些信息被序列化并存储。在执行作业时，系统会读取并根据最新的加密算法对所有分区进行统一加密，再进行 SQL 统计。执行结果完成后，系统会更新分区的元数据标签，确保数据的完整性和安全性。</p><p></p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef92bfcbc39200e035709fc45cb76066.png\" /></p><p></p><p>金融机构在数据分析和加工过程中，即便数据已经统一脱敏处理，但数据的运营权限仍然由各个业务方独立管理和权限分配。由于不同业务部门间的理解差异或者经营竞争意识等，比如银行内部的信用卡、消金及汽融业务，其借贷产品十分相似，导致很多数据难以自由流通。为了解决这一问题，我们在大数据平台上构建了沙箱安全屋环境。该环境能自动对数据进行抽样，用户可以无需申请权限即可访问所有业务方的数据。为了保证数据安全，我们确保沙箱中的数据只能进入，不能输出，并且将数据分析与应用之间是完全隔离的，这样就能实现数据流通共享，并且加速数据分析的效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/79/795b1d0c798ed9f526f48b24f9006ba7.png\" /></p><p></p><p>当用户提交 SQL 执行时，系统首先识别是否为沙箱环境。如果是，该请求无需经过权限控制，即用户无需申请数据权限就可以进行数据分析。当然为确保数据安全，我们会对数据执行统一脱敏采样，并改写提交的 SQL。执行后的结果仅能保存在沙箱环境中。若数据想要迁回到生产环境，系统会自动识别并进行拦截，确保数据只能进入沙箱，不能导出；在跑批流程中，我们首先从生产库读取数据，接着在沙箱中对数据进行脱敏抽样，然后将其写入沙箱环境。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/57/57114e22cdf68cc8ef14ab4bcd662894.png\" /></p><p></p><p>数据成本价值管理目前处于探索阶段，现阶段我们是从数据价值和数据成本两方面进行评估管理，逆向推动成本治理；数据价值方面，我们依据表在各个业务流程和标签、模型、报表中的使用情况，如标签调用量及业务场景使用量，来进行系统价值回流和人工标注，基于不同维度进行加权计算，最终输出每个表的价值分。数据成本方面，我们针对大数据底层的每张表都单独计算存储成本和数据计算量化成本，从而确定每张表的数据成本。</p><p><img src=\"https://static001.geekbang.org/infoq/b3/b369bbd401fee7d5ff36a4574eebc608.png\" /></p><p></p><p>最后，让我们看一下数据资产沉淀的过程；在数据研发过程中，依赖元数据治理规范工具检测通过的数据，接口推送至数据资产平台生产数据资产；这些资产再由数据开发人员进行进一步加工和认定；数据资产平台对认定后的资产进行自动分类盘点，资产编目；最终，我们为外部提供数据目录导航、数据资产搜索等，满足数据资产应用需求。</p><p></p><h4>未来展望</h4><p></p><p>尽管进行了大量的数据研发工作，但公司在数据价值最大化的问题上仍存在困难，对于整个大数据平台，数据应用和治理仍是存在很多的疑问点，例如，两张表的内容相似度达到九十几个百分点，我们是否应该合并它们？数据开发工程师和数据架构师负责定义数仓分层，包括 ODS 层、DWD 层、DWS 层等，但这些定义是否真正考虑了用户的需求？对于金融公司的数据标准，有些由咨询公司为其制定了标准，但这些标准是否适用于所有业务场景仍然是未知的。</p><p></p><p>未来，我们计划借助逻辑数仓的概念，实现数据价值最大化。改变原先从下往上构建数仓的模式，构建以“客户为中心”自动化构建数仓，我们希望通过用户的实际使用行为，自动化生成数据仓库的物化表。我们期望数据开发工程师只需定义业务逻辑，而所有存储的控制权由平台持有。我们希望通过构建逻辑数据仓库来优化大数据平台的 DAG。我们的目标是通过这种方法大幅度解决数据治理的不敏捷问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/38/384521e1e2fe3f62bd8b9bef80caf3df.png\" /></p><p>目前，我们的数据仓库主要是物化落地的实体物理表，我们打算将所有物理表转化为虚拟表。通过分析用户访问的日志，我们能够对这些虚拟表进行 SQL 解析和算子转换。基于这些分析，我们会将使用频度较高的虚拟表物化，并将其与原始的虚拟表关联起来，这样对于用户来说体验没有变化，但是实际技术存储方案是最优化的解决。这种策略实质上是依据用户的实际使用模式来构建数据仓库的流程，这意味着，数据开发工程师只需专注于业务指标的处理，平台会自动决定是否将表持久化、是否合并表以及如何持久化。尽管用户端无感，但从平台的角度，表的具体存储位置和数量都是动态的、不确定的；且这种策略确保了用户查询和批处理的时效性，实现真正的自主数据治理。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04b05c3bb51beb420c78c3f6ba23245c.png\" /></p><p></p><p>举例来说，当用户进行 API 查询、标签、指标、特征和报表等行为时，我们首先通过逻辑数据仓库为外部提供服务，同时记录这些逻辑数据仓库的访问频次和标记规则等。试想一下只要在数据仓库中修改一个字段或合并两个相似的表，所需的工作量都是巨大的。但对于逻辑数据仓库来说，其表格可以无限扩展，并且是非常轻量级的，你可以随心所欲地修改它。尽管在初期我们可能会损失一些计算资源，但在后期，我们会通过物化能力，确保时效性并更高效地为外部提供服务。</p><p></p><p>当符合物化规则后，我们会分析整体的 SQL 依赖关系，并进一步细化这些 SQL 关系，诸如将 SQL 拆分为更小的算子、输出内容、where 条件以及它所依赖的表等。我们将优化这些大型或选定表集的 DAG，并将其物化为物理表。然后，在物理层与逻辑层之间进行映射。对于用户来说，他们访问的表仍然保持不变，但存储的控制权由我们的平台持有。</p><p></p><p>嘉宾介绍：</p><p>廖晓格，平安银行数据资产管理及研发中心 / 数据及 AI 平台团队负责人，大数据及 AI 领域资深专家，十多年大数据及 AI 平台研发经验，曾在 PPTV、eBay、携程、华为负责大数据平台研发及优化工作，开源领域爱好者、熟悉 Hadoop 生态、Kubernetes 开源生态和架构设计、精通大数据相关组件技术，承担大数据基础平台、数据中台及 AI 平台建设等重要项目。</p><p></p><h4>活动推荐：</h4><p></p><p></p><p>11 月 19-20 日，首届 <a href=\"https://fcon.infoq.cn/2023/shanghai/\">FCon 全球金融科技大会</a>\"将落地上海。届时，廖晓格老师也会到场与大家交流并分享【金融级数据研发 DataOps 落地实践】，诚挚的邀请您参加本次盛会，期待您可以从他们的交流中获得启迪。</p><p><img src=\"https://static001.geekbang.org/infoq/29/299542233835dfef085279a72d6458d7.png\" /></p><p></p><p>目前为 8 折优惠购票最后 3 天，咨询购票请联系：17310043226（微信同手机号）。</p>",
    "publish_time": "2023-11-08 11:37:35",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "阿里云CTO周靖人：API和模型级别开放大模型能力，做to C 产品不是目标",
    "url": "https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu",
    "summary": "<p>10月31日，阿里云正式发布千亿级参数大模型通义千问2.0。官方数据显示，在10个权威测评中，通义千问2.0综合性能超过GPT-3.5，正在加速追赶GPT-4。此外，阿里云还发布了智能编码助手通义灵码等行业应用大模型。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/cc/cc23cc39efb9acd3e61f3863210bbba1.jpeg\" /></p><p>&nbsp;</p><p>针对阿里云的大模型策略，阿里云CTO周靖人在接受记者采访时表示，“我们的目标并不是做toC的产品，而是希望更多地把模型的能力开放出来，让更多开发者、合作伙伴使用。”</p><p>&nbsp;</p><p>据周靖人介绍，阿里云的定位是“服务好各种各样在AI时代的创业者、开发者和企业客户等。通过多层技术能力，包括AI基础设施、模型等能力，最好地支持开发者和客户，帮助他们解决在创业、落地、创新等方面的问题。”</p><p>&nbsp;</p><p>具体来讲，模型创业公司希望使用到最先进的AI基础设施；企业客户希望将开源模型与自己产品做二次结合，这类产品包括通义千问等开源模型、帮助企业做模型定制的阿里云百炼等；还有关注业务系统的开发者，通过简单的API集成到自己业务体系中。</p><p>&nbsp;</p><p>周靖人表示，这次AI技术变革的实质是一次技术体系的全面升级。对云计算来说，主要包括以下纬度：第一，系统优化，即如何利用模型能力优化复杂庞大的分布式系统，让它真正变成一个“自动驾驶的云”；第二，用模型帮助提升开发效率，即让用云这件事本身变得更加智能；第三，以模型为中心打造最好的AI基础设施，提供低成本、一站式的模型训练、微调、推理等服务。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9f/9f74a49d11d71efa482b001ac1473b48.jpeg\" /></p><p>&nbsp;</p><p>“而云厂商既要懂AI，又要懂云计算，才能在这次竞争里取得一个重要的战略性优势。”周靖人表示。具体来说，云厂商要做的事情就是让用户以更低的成本使用模型来提供服务。</p><p>&nbsp;</p><p>“今天基础设施的目标，特别是模型推理方面，不单是提升延迟等各个方面的性能，同时还要能够降低使用成本。在这方面，我们还有大量的工作需要做。”周靖人表示，阿里云不是简单开发界面的开放，而是API级别、模型级别的开放。用户可以借这些产品发挥更大的想象空间，做更多业务的创新。</p><p>&nbsp;</p><p>周靖人呼吁，大家要给这个领域一些时间。“毕竟从国内来讲，整个产业的变化是从今年开始的，甚至到了3、4月份，大家才陆陆续续发模型。在这方面，我们的确比海外要晚，海外拥有至少一年的先发优势，甚至更长的时间。”</p><p>&nbsp;</p><p>不过，周靖人表示，国内也在快速地追赶中。短短半年时间内，国内模型生态已经慢慢发展起来了。模型的生态发展起来，一定代表了算力发展得起来。</p><p>&nbsp;</p>",
    "publish_time": "2023-11-08 12:17:25",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "“2023深圳国际金融科技大赛暨微众银行2024校园招聘宣讲会”走进深大：AI、区块链、产品经理的未来在何方？",
    "url": "https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13",
    "summary": "<p>在金融科技发展的过程中，人才培养是举足轻重的关键一环。为了推进深圳市金融科技人才高地建设工作，并向高校学子提供一个展示自身知识、能力和创意的平台，深圳大学微众银行金融科技学院与微众银行联合举办了“<a href=\"https://www.infoq.cn/news/9AYU96ZSPoCZ6kyClK94\">2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛</a>\"”（下文称“大赛”）。</p><p></p><p>在本次大赛赛程中，大赛组委会特别设置了技术公开课以让同学们更加了解大赛及人工智能、区块链、金融产品经理的发展现状。继 10 月 25 日<a href=\"https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC\">第一场线上技术公开课</a>\"圆满开课，为了让同学们能够更直接、更近距离地了解大赛内容，11 月 3 日，大赛组委会联动微众银行人力资源部门共同走进<a href=\"https://www.infoq.cn/video/atB6FuOvpIQGQPWLX0Eo\">深圳大学</a>\"，在线下组织了一场 2023 深圳国际金融科技大赛的第二场技术公开课暨微众银行 2024 校园招聘宣讲会。此次活动将金融科技大赛、企业招聘和高校教育结合在一起，形成了一个良好的产学研合作模式，将高校培养成果嫁接至高质量人才输送链路之中，为学生提供实践和就业的机会的同时，将学术成果有效转化为实际产业解决方案。</p><p></p><p>本次宣讲会邀请到了深大微众金融科技学院党委书记刘山海书记、深大微众金融科技学院院长助理祁涵及微众银行的多位专家来到现场，围绕大赛赛题、赛制和微众银行 2024 校招内容展开宣讲。以下为本期公开课直播精华内容整理：</p><p></p><p></p><h2>一、深大微众金融科技学院院长助理祁涵再次介绍大赛规则及流程</h2><p></p><p></p><p>2023 深圳国际金融科技大赛—— 西丽湖金融科技大学生挑战赛致力于推动国内外高校学生探索金融科技领域的技术应用创新，促进政、学、企三方交流，全面提高学生的创新能力、实践能力和就业竞争力。</p><p>作为 2023 年深圳市金融科技节的重要一环，本届大赛在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2f51f27c732fb8f013d743bc29fb043.png\" /></p><p></p><p>本届大赛将通过初赛、复赛在各赛道分别遴选出 10 支队伍进入决赛角逐，并设置总额超过 69 万人民币的赛事奖金及参赛专属区块链数字证书，以奖励各赛道获得一等奖、二等奖、三等奖的队伍及成员。此外，本次大赛还邀请了学术和企业界的众多资深专家为参赛选手答疑解难——特邀国家统计局原副局长许宪春、微众银行首席智能官杨强、中国人民银行研究局原局长张健华等担任学术顾问，评委嘉宾来自微众银行及国内各大顶尖高校。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f52bd2e8110a8dbb969698a29a8a0175.png\" /></p><p></p><p>据祁涵介绍，本届大赛所有参赛队员必须是全日制在校大学生（包括本科生、研究生和博士生），须以团队形式参赛，每支队伍人数 2~5 人，每人只能参加一支队伍，必须要独立完成题目，最后产品的知识产权归参赛选手所有，大赛不收取任何报名费用，决赛期间的队伍食宿及往返交通费用由组委会统一安排。</p><p></p><p></p><h2>二、微众银行区块链高级架构师周禄：区块链赛道高分秘籍</h2><p></p><p></p><p>周禄在宣讲最开始就强调，区块链赛道的参赛项目需要基于 FISCO BCOS 平台及微众区块链系列开源技术设计并开发一个区块链系统，以解决 ESG 相关的某个行业或场景的痛点或问题。具体来说，选手可以将区块链技术应用于大湾区一体化、双碳、乡村振兴、公共服务等 ESG 领域。过往赛事中，有一些作品的创意就非常值得参考。比如 2019 年大赛区块链赛道第一名作品就基于 FISCO BCOS 构建了一个排污权许可区块链交易平台，配合交易纠纷仲裁、黑名单、监督审计等链上机制，健全、活化市场，实现企业、政府、公众在环保排污上的三权制衡、多元共治，以辅助排污政策制定，共建污水治理生态循环。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/86/861dcbc86a0af420dde7ecfcff970638.png\" /></p><p></p><p>2020 年区块链赛道第二名作品 WeHelp 则基于微众银行社会治理框架“善度”，使用区块链底层平台 FISCO BCOS、分布式身份解决方案 WeIdentity 等区块链技术，加速求救与救援的匹配。项目还采用可共享的分布式账本记录善行，保证数据公信力，解决求助过程中的信任问题。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b2/b2221f7277edb82f51bb676863e5ccf9.png\" /></p><p></p><p>2021 年区块链赛道第三名作品《亿点爱》旨在通过区块链构建公益众筹平台，利用隐私保护技术实现安全可信存储，保护用户个人隐私的同时，有效预防虚假筹款和善款被挪用等问题，以此促进互联网公益行业健康有序地发展。</p><p></p><p>周禄总结了上述获奖作品的特点第一就是应用方向符合 ESG 命题，其次就是充分理解了区块链的特性，并融入应用场景；以上述获奖作品为参考，周禄继续讲解了本届大赛中获得高分的锦囊。</p><p></p><p>参赛作品要赢得评委青睐，首先要选择适合区块链应用场景的正确方向。ESG 指环境、社会和治理，其本质是一种价值观，鼓励企业更多重视财务数据以外的贡献，在保护环境、有利社会和加强治理方面创造价值。而区块链是传递信任的机器，可以大幅降低信任成本；同时区块链可以保护隐私，避免个人数据泄露；区块链还是激励相容的，很容易设计激励机制；区块链的交易记录可全程追溯、可信可验证，这也是它的一大优势。而正是因为具备了这些特性，区块链技术很适合用于 ESG 实践。</p><p></p><p>此外，周禄还为同学们讲解了开发区块链应用的基本步骤。区块链应用的架构包括了用户、分布式应用 DAPP、服务接口 API、智能合约和底层平台，其中 DAPP 可以是命令行、网页、手机或 PC 应用，通过接口和平台通信；服务接口采用通用 JSON 格式 RPC 调用；智能合约采用 Solidity 语言编写；底层平台包含网络、共识、加密和存储等模块。</p><p></p><p>同学们在开发区块链应用时，可以首先使用大赛官方提供的快速建链工具搭建区块链，然后使用业务模版开发 Solidity 合约，并通过交互式控制台的 SDK 部署合约，使用 SDK 开发业务，通过 RPC 协议交互，这里的交互语言没有限制。最后部署业务系统，发起查询和上链交易。这些工作要在团队内分工合作，提高效率。</p><p></p><p>参赛选手要善用各类区块链组件，微众区块链全部开源，提供了众多组件供公众使用。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/97/97690f91de649a76fe37f04bb4e68101.png\" /></p><p></p><p>大赛官方还提供了大量代码仓库（https://github.com/FISCO-BCOS/FISCO-BCOS），包含很多参考 demo 和开源项目，选手可以直接克隆研究。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/68/68870ba93cf861429ca6fa9a514a8e81.png\" /></p><p></p><p>官方的黑客松目录收集了基于 FISCO BCOS 开发的，参与各种大赛的优秀案例，包括每个案例的项目介绍、设计文档、源代码等，供选手参考。周禄推荐各位选手充分利用上述资源，在比赛中取得佳绩。</p><p></p><p></p><h2>三、微众银行个人直通银行部室经理金虎光：银行线上场景的交互式智能柜台服务</h2><p></p><p></p><p>本届大赛产品经理赛道的赛题是《银行线上场景的交互式智能柜台服务》，金虎光在解析赛题时表示，参赛选手应当基于对话式交互的应用进行银行产品方案设计，实现对客户全场景的陪伴式交互，为客户提供智能、便捷、懂用户、有温度的线上银行服务（可选择微众银行 APP、微众银行 We2000 小程序或其他银行产品作为产品框架进行设计）。设置该赛题的背景是银行业传统的线下柜台服务帮助实现银行职员与客户之间的互动，可以处理较为复杂和个性化的问题，更容易发展信任关系，但由于网点服务存在地点和工作时间限制服务效率比较低。近年来兴起的银行线上远程服务则希望通过各种技术手段为客户带来随时随地、方便快捷的体验，同时尽可能做到像线下一样可以处理复杂、个性化的问题。从电话银行到网上银行、银行 APP 再到虚拟数字人服务，银行正在努力将线上数字银行打造成新的增长点，提升金融服务体验和质量，提升客户经营质效。</p><p></p><p>如今各家银行的线上服务都已经包括了几乎所有银行服务功能，但随着功能指数级增长，客户的线上交互也变得非常复杂。每家银行都有多个 APP，如何让客户更方便地找到所需功能是银行面临的普遍挑战。金虎光提到，评委们希望看到参赛选手的创新想法，展现出如何在功能、场景、信息繁多的背景下让用户更加便捷地体验线上银行服务。</p><p></p><p>最近火热的生成式 AI 技术可以贯穿从市场、销售到运营、研发、风控的所有银行服务，带来许多创新性的体验。金虎光建议选手可以选择某一个垂直场景或服务，或基于一揽子服务模式来利用生成式 AI 改进交互形式。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/11/1128d3ad32ef06751999b20f30492ff8.png\" /></p><p></p><p>例如作品可以为投资小白用户基于生成式 AI 技术分析、总结投研报告，辅助投资决策。选手还可以参考智能投顾、智能客服、远程面签等技术和场景拓展思路，发挥创意。例如招行 APP 左上角的小猫就会根据用户行为提供实时建议，APP 中的 AI 小招机器人则利用数字人模式提供了智能财富管理顾问服务；百信银行则探索利用虚拟空间模式服务线上用户等等。</p><p></p><p>微众银行也在做相关探索。例如用户可以在微众银行 APP 中与虚拟小 weiWE 机器人对话获得服务，或者使用微众银行小程序快速完成操作。金虎光建议，参赛同学可以选择微众银行 APP 或小程序，亦或是任何自己熟悉的银行 APP 作为底层框架来设计作品。</p><p></p><p>针对产品经理赛道的参赛规则，金虎光也做了详细解读。本赛道中，评委主要考虑以下四个维度为作品打分：</p><p>创新性：参赛作品具备创意亮点，用新思维解决现有问题，或探索新的行业模式，有望开拓新的产业运作模式，市场空间等。商业价值：参赛作品所面向的场景和用户有一定代表性，且作品能够很好地结合实际应用场景解决所描述场景痛点；参赛作品有良好的社会价值，运作合法合规，或具备一定商业价值、成长性和可持续性，值得规模化推广。完整性及可行性：进行有效的竞品、市场及用户分析，并总结产品相对竞品的优劣势、可借鉴及可创新之处，了解用户需求，针对用户痛点提出对应解决方案；有完整的产品设计方案，包括设计背景、产品流程、功能模块说明等，产品架构设计完整，产品流程可形成闭环。技术先进性：可清晰阐述使用的技术，技术有一定先进或创新性。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/b9/b9b56b49f06ebd08c59291ac2eb5f881.png\" /></p><p></p><p>同时还需要注意的是，参赛作品一定要“有需求、有场景”，同时技术是可落地的，尤其要避免创意大而空的问题，要注重实际的创意内涵。与此同时，因为银行的金融交易非常关注安全性，所以作品一定要考虑必要的安全、核身和信息保护环境，理解银行产品背后的相关设计。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/56/56908a6e25ce3a93ec23fa9448cbfc54.png\" /></p><p></p><p>金虎光还提醒同学们，参赛作品需提供完整的产品设计文档，建议包括市场及竞品分析、用户调研、产品分析和产品设计说明。初赛需提供产品设计文档（Doc 格式），复赛和决赛还需提供作品展示文档（PPT 格式）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f5/f5f39f7ae9b9a27d9dcdc5174809e6e1.png\" /></p><p></p><p></p><h2>四、微众银行人工智能资深研究员、FATE 开源社区技术委员会成员范涛：人工智能赛道讲解</h2><p></p><p></p><p>范涛首先介绍了基于微众人工智能技术开源的 FATE 项目。这是目前国内最大的联邦学习开源项目和社区之一。FATE 不仅提供了底层框架，还提供了很多应用组件。选手须基于该平台构建人工智能产品，可以充分利用 FATE 提供的各类算法和组件。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/5d/5ddedbc44d28162945c216cfdc791716.png\" /></p><p></p><p>本赛道命题为开放式，作品可基于 AI 联邦学习开源平台 FATE，设计纵向联邦学习、横向联邦学习或者联邦大模型创新性产品或算法，包括并不限于联合风控，联合营销，智能权益定价，数据交易定价等场景。项目须使用 FATE 开源技术实现，选手须将实现代码提交至 Github 供评委考核。评委基于作品的产品实现完备性、创新性和商业价值打分：</p><p>产品实现完备性占 40% 分数，考察作品在技术层面的复杂度和实现完成度。完备性包括可行性分析、方案设计文档、代码实现、测试报告等方面。创新性占 40% 分数，考察作品在设计层面的新颖度。创新性包括但不限于全新的场景痛点，用新的算法或技术手段，有效综合利用多个技术组件从而产生新的效能。商业价值考察作品在人工智能领域中的综合价值。综合价值包括但不限于作品与实际产业的贴合度、是否有效解决真实行业痛点、是否有效地发挥了联邦学习的实用价值等。</p><p></p><p>选手须组队参赛，个人可先报名，由平台协助组队；初赛作品于 11 月 27 日截止提交，包含作品介绍（PPT 格式）、技术文档（Doc 格式）、作品展示材料（包括但不限于作品部分 demo 或演示视频等）。12 月 5 日大赛公布决赛入围名单，16-17 日在深圳举办线下 36 小时封闭马拉松，选手对初赛提交作品进行开发和完善，并做现场路演答辩。</p><p></p><p>除了赛程相关的信息，范涛还向同学们解读了横向联邦学习、纵向联邦学习和联邦大模型三大技术栈：</p><p>横向联邦学习是指每个终端都有一些同质数据，但每一方都有数据隐私保护需求，仅靠自己的数据不足以构建较好的模型，所以希望综合多方数据构建模型，本质上是扩充数据样本来提升模型效果和稳健性的方法，适用于参与者数据特征重叠较多，而样本 ID 重叠较少的情况。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/82/821713d23a19373d8feb8a34f18024ff.png\" /></p><p></p><p>纵向联邦学习在金融领域应用非常广泛。它通过引用第三方数据与金融数据结合来提升风控、营销等场景的效果，本质上是通过扩充特征维度来提升模型效果，适用于参与者样本重叠较多的情况。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/15/157d0219eec8cfabb4b7fa7e65d3c274.png\" /></p><p></p><p>大模型技术与联邦迁移学习有很多结合点，后者也是大模型领域的新兴范式。通过联邦迁移学习，大模型可以和本地私有数据结合，成为适合本地数据的中小模型。该课题是本届大赛的新增部分，范涛推荐选手关注。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/9e/9ef4d47d97602fb405fcfbfaef48c6a8.png\" /></p><p></p><p>范涛最后介绍了 2022、2020、2019 年人工智能赛道名列前茅的一些作品，他希望参赛选手参考这些优秀案例，设计出令人称赞的高分作品：</p><p>2022 年：一等奖作品是面向真人体验感知的系统，通过横向联邦学习场景综合多人信息去做感知应用；二等奖作品是将联邦学习应用于工业智能，如火焰检测、工业设备缺陷检测等场景；三等奖作品是纵向联邦学习应用于电网的场景。2021 年：一等奖作品是心理健康预测监控系统，综合用户的文本、图像、社交媒体数据心理健康行为预测平台；二等奖作品是基于 FATE 构建的联邦营销一站式平台，可以综合多企业数据来做更精准的营销。三等奖作品横向联邦场景来做保险经纪人，通过 F ATE 平台保护用户隐私。2019 年：一等奖作品是横向联邦学习场景，通过车载行为数据对车险定价；二等奖作品基于联邦做了联邦图形预算法，联合多个银行的交易网络，在不泄露隐私数据的前提下预测欺诈用户；三等奖作品是基于联邦学习平台做个人数据定价，形成新的数据交易模式。</p><p></p><p></p><h2>五、写在最后</h2><p></p><p></p><p>除了以上干货内容，深大微众金融科技学院党委书记刘海山书记在本次宣讲会开场时就为同学们加油鼓气。刘书记希望参赛学生能够积极探索金融科技领域的技术应用创新，将创新成果转化为实际应用，向金融科技行业提供更有价值的技术解决方案，为深圳乃至全国的金融科技发展贡献力量。</p><p></p><p>在本次宣讲会的校招环节，微众银行零售存款部总经理邢海鹏整体介绍了微众银行的业务和技术。据其介绍，微众银行是全国首家数字银行，2019 年与深圳大学合作成立了深大微众金融科技学院。微众银行在 IT 方面的投入营收占比超过了 9%，科技人员占比超过 50%，微众银行在 AI、区块链、云计算、大数据等方面都有大量投入，并取得了一系列行业领先的成果。</p><p></p><p>微众银行人力资源部室资深经理杨帆详细介绍了微众银行的人才结构——银行员工平均年龄 33 岁，本科及以上学历超过 99%，人才来源也非常多元化。校招生身份入职的微众银行的同学，可以获得专属“私塾学习计划”，并为校招生设置了一年的培养期。</p><p></p><p>2024 年微众银行校园招聘主要分为技术研发、数据算法、产品业务、综合职能、财富管理五大类别，具体的招聘详情可以根据下列途径进行了解 ↓</p><p><img src=\"https://static001.infoq.cn/resource/image/96/fb/96ee25124fde526ded7913138c2199fb.png\" /></p><p></p>",
    "publish_time": "2023-11-08 13:58:46",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "数字化转型成功企业的3个特点：前瞻认知、早期收获和里程碑设定",
    "url": "https://www.infoq.cn/article/Vzuau7dgxQvhsWZOhf3E",
    "summary": "<p>数字经济已经成为当今全球经济的重要驱动力。它涵盖了数字技术、互联网、<a href=\"https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a\">人工智能</a>\"等领域，为企业和社会带来了巨大的发展机遇。但与此同时，随着数字经济的蓬勃发展，新兴产业与岗位不断涌现，数字人才的供给却无法满足需求，形成了巨大的人才缺口。数字人才的培养和发展已经成为企业人才发展工作的当务之急。</p><p></p><p>在此背景下，日前北京极客邦科技有限公司（以下简称“极客邦科技”）与长城战略咨询正式发布战略合作。双方将通过共同开展培训项目、知识分享和人才交流等方式，致力于加强数字人才的培养和发展，共同助力提高企业员工的数字素养和技能水平，满足数字经济时代对多样化<a href=\"https://time.geekbang.org/\">人才</a>\"的需求。</p><p></p><p>极客邦科技创始人&amp;CEO霍太稳表示，在这个充满机遇和挑战的数字时代，培养优秀的数字人才是推动企业创新与竞争力的关键。极客邦科技与长城战略咨询的合作将汇集双方的优势，共同致力于数字人才的培养与发展，为企业提供具备数字化能力和战略洞察力的人才储备，助力企业实现高效、可持续的数字化转型。</p><p></p><p>长城战略咨询总经理武文生介绍，在过去30年中，长城战略咨询一直在数字化领域进行探索，不仅在自身转型，还在协助众多客户实现数字化转型，并取得了卓越成就。通过与极客邦科技达成合作，主要希望达成三个目标：</p><p></p><p>第一，当前产业和企业对数字化人才的需求急迫，需要具备数字化专业技能，同时能够与产业需求和数字化转型密切结合的综合型人才。极客邦科技在这方面拥有丰富的实践经验，为人才培养提供了重要支持。</p><p></p><p>第二，双方共享相似的理念，强调开放共享和合作共赢，致力于建立生态系统。因此，希望通过合作为数字化人才的培养和数字化转型提供支持。</p><p></p><p>第三，期望共同帮助更多客户取得成功。极客邦科技已经成功为众多企业培养和提升数字化人才的平台，长城战略咨询协助客户通过数字技术提升管理和实现数字化转型，也取得了显著成果。双方期待强强联合，为更多客户提供支持，推动数字中国和数字化转型的发展，实现1+1大于2的效果。</p><p></p><p>在战略合作发布仪式上，双方围绕“数实融合的趋势下，企业如何抓住数字化转型新机遇”的话题展开了探讨，以下内容根据对话整理，篇幅有删减，点击链接可观看直播回放：<a href=\"https://www.infoq.cn/video/8TE8DYGhLDX5OpkTPdmS\">https://www.infoq.cn/video/8TE8DYGhLDX5OpkTPdmS</a>\"</p><p></p><h3>新经济与数字经济的内在联系</h3><p></p><p></p><h5>霍太稳：2016年“新经济”被首次写入我国《政府工作报告》，2017 年报告首次明确促进数字经济加快成长的要求。长城战略咨询作为中国新经济领域的专业咨询公司，请您阐述一下新经济的内涵，剖析一下“新经济”与“数字经济”之间的关联性？</h5><p></p><p></p><p>武文生：“新经济”实际上源自经济学家对美国硅谷的研究，其最典型的特点是“颠覆式的创新和发展”。在硅谷，产业领域大约每十年便迎来一次新的变革，由一些新兴创业公司带领着。这些公司引领着全球许多产业的革新和新兴产业的发展。</p><p></p><p>“<a href=\"https://xie.infoq.cn/article/b331efeb296df631664f8dbe8?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">新经济</a>\"”有几个显著特点：</p><p></p><p>首先，它是创业驱动的经济，许多企业从零到一、从一到一百、从一百到一万成长，因此创业在其中扮演着重要角色；</p><p></p><p>其次，“新经济”具有强烈的渗透性，不仅推动自身发展，还渗透和影响传统经济和产业。从半导体到PC，再到互联网和移动互联网，以及社交网络和最新的大模型，它们都引领了行业变革；</p><p></p><p>此外，“新经济”是试错的经济，成功并不是事先有明确的路线，而需要创业者、投资者、科学家和发明家共同探索。许多企业会失败或消失，但一旦成功，它们将具备强大的竞争力和平台性，尽管看似有混乱的时刻，但它们会通过技术和创新不断突破这种混乱。投资模式通常类似于中国的小步快跑，因此硅谷的公司融资通常分为多个阶段，从天使轮到A轮、B轮、C轮等，这样的方式有助于创新和探索。</p><p></p><p>“新经济”和“数字经济”之间存在密切关联，因为“数字经济”构成了“新经济”的主要部分。虽然数字经济的一些领域已经非常成熟，比如通信和网络，但它仍然是“新经济”的主要组成部分。“新经济”不仅局限于数字经济，还包括生命科学领域，如应对癌症和其他健康挑战的创新。此外，“新经济”还包括探索太空、绿色技术和新能源，例如清洁能源和智能汽车等领域的创新。</p><p></p><p>总之，“新经济”的核心部分是“数字经济”，但“数字经济”本身也为“新经济”的其他方面提供支持。例如，AI for Science（AI在科学研究中的应用），使许多研发领域受益，包括商业航天和智能汽车。</p><p></p><h5>霍太稳：“数字经济”实际上推动了整个“新经济”的发展，先进技术扮演了关键角色。那么从您的角度来看，目前国内的数字经济发展处于什么状态？它有什么显著特点？</h5><p></p><p></p><p>武文生：近几年来，国内的<a href=\"https://www.infoq.cn/article/JON1c1HGZiBf5joSCRlY?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">数字经济</a>\"发展取得了显著进展。</p><p></p><p>首先，我们在数字技术领域已经从跟随者发展为并跑者，甚至成了部分领域的领跑者。尽管数字技术的发展并非源自中国，但中国的企业家、科学家和工程师都迅速学习并吸收了这些知识。同时，我们也从硅谷等地汲取灵感，产生了自主创新。例如，搜狐学习了Yahoo，百度学习了Google，阿里巴巴和腾讯也在学习和借鉴的基础上发展了自己的原创性。举例来说，阿里巴巴在与eBay竞争时，通过市场竞争、市场培育、开放和免费等策略在中国市场取得了成功。腾讯QQ也在早期学习了很多，但通过微信等产品，他们加入了自己的创新元素。总之，中国的数字产业发展在全球互联网企业中占有显著地位。</p><p></p><p>更重要的是，数字经济和技术不仅加速了数字产业的发展，还赋能了传统产业，使其迈向数字化时代。中国的市场规模和应用场景也为我们的服务提供了巨大的机会。从创新药物的研发到航空、航天，都是数字技术的重要应用领域，传统产业如餐饮和服装也都经历了数字化变革。中国企业如TikTok、Tmall等在国际市场竞争中表现出色，尤其在欧美市场有强大竞争力。这些企业在全球范围内广受欢迎，为中国在国际舞台上赢得了声誉。</p><p></p><p>作为长期致力于新经济企业服务的机构，我们为中国企业的成就感到自豪。我们也欣慰地看到，中国企业正在国际化进程中蓬勃发展，同时也有像极客邦这样的企业提供培训和专业咨询服务，助力中国的数字产业和产业数字化。不过，我们也必须承认，有时企业的发展速度超越了我们的服务能力。因此，我们需要不断提高自身的水平，跟上企业的发展步伐，以确保我们能够与他们共同成长，不仅在国内，还在国际舞台上取得更大的成功。</p><p></p><h3>数实融合是数字经济发展的主要路径</h3><p></p><p></p><h5>霍太稳：国内数字经济发展相对较快，部分原因在于有许多应用场景的支持。拥有先进技术，并结合广泛的应用场景，能够推动技术不断迭代，从而加速技术的发展，因此，数字经济和实体经济的融合是不可避免的趋势。我们注意到，上个月长城战略咨询发布了全国首份“数实融合”新赛道报告，“数实融合”的本质是什么？</h5><p></p><p></p><p>武文生：我们理解的数实融合是利用数字技术来赋能实体经济，提高其质量和效率。数字技术的发展还会为实体经济带来新的增长机会，数实融合不仅提高传统产业的发展速度和水平，还为新的产业发展打开了大门，有时甚至超出了我们的想象。</p><p></p><p>典型案例之一是三一重工，该公司是全球领先的工程机械制造企业。他们在挖掘机上装备了传感器，通过这些传感器实时监测挖掘机的运行状态。这使得挖掘机在建设项目中的使用情况可以实时追踪，从而提高了工程进展的反馈速度。不仅提高了生产效率，还使其成为全球工程机械的领军者。</p><p></p><p>另一个例子是<a href=\"https://www.infoq.cn/article/wyoRNaVVYp2I4GjY9Aoc?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">致景科技</a>\"，他们通过数字技术为纺织服装行业提供各种布料和辅料，使消费者能够按需定制。这种定制化以前很难实现，但现在通过数字化技术的支持，使之成为可能，利用数字技术能够更快速地响应市场需求。</p><p></p><p>除此之外，中国的传统行业，如零售和制造业，也在数字化方面取得了良好的进展。通过数字化技术，它们能够提高生产力，包括机器人和在线数字协作等技术，以应对劳动力短缺的问题。</p><p></p><p><a href=\"https://www.infoq.cn/minibook/7JyRlrx8m8rfYcfdPrX8\">数实融合</a>\"的重要性在于，它不仅提高了生产力，还使产业能够更好地适应市场竞争，而中国拥有庞大的市场规模和丰富的应用场景，使其具备了独特的优势。中国企业具有灵活性，能够更多样化、更丰富地探索各种应用场景。此外，中国产业界和企业之间相互学习和借鉴的能力很强，这也是数实融合发展非常重要且有利的因素。</p><p></p><p>目前，数实融合已经取得了显著的生产力提升，同时我们期待它还将提高中国产业的国际竞争力。这一趋势在未来将继续发展，为中国的经济增长和创新创造了更多机会。</p><p></p><h3>在企业数字化转型中，前瞻认知、突破口选择、里程碑设定是三大关键</h3><p></p><p></p><h5>霍太稳：长城战略咨询目前已经为数百家企业提供战略咨询和信息化咨询服务，刚才您分享了许多数字化转型成功的案例，您认为他们成功的关键是什么？</h5><p></p><p></p><p>武文生：企业的基因不尽相同，有些企业天生具备数字化的基因，因其产业本身就源自数字技术。但也有许多传统企业需要进行<a href=\"https://www.infoq.cn/article/GItTCDMzzSsxMcojWydF\">数字化转型</a>\"和改革。根据我的理解和实践经验，我认为关键因素主要包括以下几点。</p><p></p><p>首先，企业的最高决策层需要拥有前瞻性的认知，愿意学习和探索新的数字化技术。这种认知需要转化为战略决策，并需要建立相应的组织。我们常说，数字化转型是一项由高层领导推动的工程，因此需要建立专门的组织，并分配专门的预算用于数字化技术和应用。</p><p></p><p>然而，仅有组织和预算并不足以保证成功。在数字化转型中，早期收获非常关键。这意味着选择合适的突破口，让整个企业，从高层到基层员工，都能在数字化过程中获益，并建立激励机制。员工需要感到数字化转型对企业带来了实际好处，如提高效率、减轻工作负担。早期收获激励大家积极参与数字化转型。</p><p></p><p>此外，设定关键里程碑也非常重要。这些里程碑有助于指导企业如何克服难关，跨越发展道路上的障碍。专业的服务机构如长城战略咨询和极客邦可以帮助企业进行规划和赋能。然而，仅依赖供应商可能不足够。我们强调甲方咨询，即帮助企业主找到适合其需求和预算的数字化转型方案。</p><p></p><p>随着云计算、移动互联网和人工智能的发展，数字化的门槛大幅降低，这是一个积极的趋势。基于云服务的架构和云计算技术的发展，让优质的数字化服务变得更加容易实现。现在，我们拥有许多出色的云服务，可应用于各个领域。</p><p></p><p>因此，数字化的可能性更广泛了。不仅大型企业，现在小企业也能够实现数字化转型。随着SaaS的发展，云架构和云服务领域涌现出大量初创公司。因此，数字化转型需要培养人才，不仅是数字化领域的，还需要数字化技术的应用，有许多专业的服务公司和团队可以提供支持。</p><p></p><h5>霍太稳：根据您的服务经验，未来几年，企业推进数字化转型需要哪些服务？长城战略咨询在其中可以提供哪些支持和服务？</h5><p></p><p></p><p>武文生：在企业数字化转型的过程中，关于人才培养和技术发展方面的问题非常关键。未来几年，我认为以下几个方面将是重点。</p><p></p><p>提升认知：企业高层管理者需要提高对数字技术和数字化平台的认知水平。这包括了解最新的技术和产品，以及如何在企业中应用它们。我们技术服务产品除了培养技术人才，还应提供短期、紧凑的培训课程，使高层管理者能够更好地理解和应用数字化技术。提供性价比高的数字化技术和平台：企业需要有更多的选择，以便选用性价比高的数字化技术和平台。这需要数字技术公司提供更多的产品和服务，以满足不同企业的需求。构建专有的技术平台和知识库：企业需要在一些专有的技术平台和知识库方面建立自己的团队和研发能力。这对于深入企业数字化的业务和流程非常重要。将数字技术与业务实践相融合：数字技术工程师和专业团队需要与企业的实践经验相结合，将企业的专有知识数字化和编码。引入年轻一代：年轻一代是数字时代的天然公民，具有对数字技术的天赋理解和世界眼光。企业应该注重引入年轻人，以利用他们的数字技术知识来提升产品和服务的体验，从而使企业更具竞争力。</p><p></p><p>综上所述，数字技术和数字化平台在帮助企业提升竞争力、品牌力以及为全球消费者提供物有所值的产品和服务方面将发挥重要作用。通过提升认知、提供更多选择、建立专有知识库、结合实践经验和引入年轻一代，中国的企业将有机会在国际市场上获得更多的认可和成功。</p><p></p><p>霍太稳：数字化转型对于企业家提出了更高的要求，同时对年轻人也提出了更多期望。数字经济的快速发展使我们需要更多的<a href=\"https://www.infoq.cn/article/u7lLw2rLqF6tLIiYZXcL\">数字人才</a>\"。在数字化转型过程中，我们极客邦双数研究院提出了“数字人才粮仓模型”理论，我们认为以下几点是关键的。</p><p></p><p>数字思维的管理者：企业需要领导层具备数字化思维，这是数字化转型成功的重要因素之一。如果领导层没有数字化思维，很难确保企业的数字化转型能够成功。充分准备：数字化转型需要耐心，投入较大，周期较长。企业家和高管需要明白这一点，不必焦虑，而应感到兴奋。数字化转型的好处包括提高决策速度、效率和效果，这对企业是有利的。依赖新技术和工具：数字化转型需要大量新技术和工具。年轻人通常具有数字经济的天然优势，因此他们是数字化转型的宝贵资源。</p><p></p><h3>弥补数字人才缺口的有效手段</h3><p></p><p></p><h5>霍太稳：企业数字化转型的核心一方面是数字技术，另一方面是数字人才，但是，从目前整个市场来看，产业界所需的高端数字人才是极其紧缺的。对此，您有什么样的洞察？您认为有效弥补人才缺口、有效培养数字人才的路径和手段有哪些？</h5><p></p><p></p><p>武文生：在企业数字化转型的过程中，我们迫切需要具备数字化技术和现代管理理念的复合型人才。此外，应用型数字化人才也非常关键，这些人才在技术应用方面发挥关键作用。</p><p></p><p>为了弥补数字人才的缺口和培养数字化人才，我们可以采取以下措施。</p><p></p><p>高校合作：高等教育机构可以与产业界和企业合作，提供应用型的数字化人才培训，满足实际需求。这有助于提高数字化人才的供给。建立应用型数字化人才培养学院，致力于满足产业界的实际需求，培养具备实际技能的数字人才。这样的学院可以联合企业，提供实际经验和实践机会。继续教育：鉴于数字技术的快速发展，建立继续教育和终身学习机制对于数字人才的培养至关重要。在线社区和在线学习平台可以提供便捷的学习资源，帮助数字人才不断更新知识和技能。将线上和线下学习相结合，提供继续教育和终身学习的机会，以满足数字人才的持续成长需求。产学合作：建立更良性的产学合作机制，使企业界的经验和学术界的知识相互渗透。这可以包括企业家在高校担任实践教授，同时学术教授积极参与产业界的创新研究。</p><p></p><p>这些措施有助于提高数字人才的供给，为企业数字化转型提供源源不断的人才支持，同时满足了产业界的实际需求。数字人才将成为中国未来的人才红利，为产业和经济的发展提供持续的动力。希望未来能够与极客帮一起为实现这一目标而努力。</p><p></p><h5>霍太稳：最后，请武总用简短的话语给那些已经走在数字化转型之路上，或者希望获得更多数字技能的年轻人提供一些期望或寄予。</h5><p></p><p></p><p>武文生：首先，对于众多企业家来说，数字化转型是一条不可或缺的道路。正如霍总所提到的，市场力量现在正在推动数字化转型，并且提供了众多专业服务和技术供选择。政府和市场竞争将进一步推动技术的发展，提供更多性价比高、更优质的技术和服务。企业家需要做出明智的决策，善用这些技术和服务，同时建立一个团队，专注于数字化业务数据和知识，这将有助于提升竞争力。长城和极客帮拥有丰富的经验，可以提供协助和意见，而极客邦社区和知识库也是免费的资源。</p><p></p><p>其次，对于年轻人来说，选择从事数字化技术研究，尤其是数字化应用技术，拥有无限前途。虽然技术不断演进，企业对技术人才的需求也在不断提高。有时候会担心技术会取代人的工作，但我们必须明白，人类拥有智慧，不断学习和进步。我们需要沿着技术的进步不断提升自身的能力，以使自己能够创造更大的价值，服务社会、消费者和企业。这需要我们持续学习、不断提高，承受压力，但也会让我们不断进步，提高自身的人力资本的价值。</p><p></p><p><img src=\"https://static001.infoq.cn/resource/image/bf/a9/bf1622c0c2be73e4d19e8643444e2fa9.jpg\" /></p><p></p>",
    "publish_time": "2023-11-08 14:20:22",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "零一万物李开复：要做ToC的超级应用，成为AI 2.0时代的微信、抖音",
    "url": "https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8",
    "summary": "<p>“我们在3月底官宣零一万物，后面团队逐渐到位，6、7月开始写下第一行代码，历时短短4个月时间，今天我们非常自豪地宣布产品亮相。”李开复在另一万物首款大模型发布会上说道。“从创立零一万物第一天开始，我的目标就是做一个世界级公司，能够进入世界的第一梯队。”</p><p>&nbsp;</p><p>自四个月前李开复宣布大模型创业，业内就给予了众多关注。千呼万唤，李开复交出了第一份答卷。11月6日，李开复带队创办的AI 2.0公司零一万物正式开源发布首款预训练大模型Yi-34B和Yi-6B。Yi-34B是一个双语（英语和中文）基础模型，经过340亿个参数训练，明显小于Falcon-180B和Meta LlaMa2-70B等其他开放模型。</p><p>&nbsp;</p><p>更多详情查看：</p><p><a href=\"https://www.infoq.cn/news/3m7F87QpDVsu8zv68k1b\">李开复4个多月后“放大招”：对标OpenAI、谷歌，发布“全球最强”开源大模型</a>\"</p><p>&nbsp;</p><p>对于模型尺寸的选择，零一万物团队认为，34B是一个黄金尺寸。虽然6B也能在某些领域，比如客服上可用，但模型毕竟越大越好，但随之而来的就是推理成本和后续训练的系列资源问题。</p><p>&nbsp;</p><p>“34B不会小到没有涌现或者涌现不够，完全达到了涌现的门槛。同时它又没有太大，还是允许高效率地单卡推理，而且不一定需要H和A级别的卡，只要内存足够，4090或3090都是可以使用的。”李开复解释道，“既满足了精度的要求，训练推理成本友好，达到涌现的门槛，是属于非常多的商业应用都可以做的。”</p><p>&nbsp;</p><p>另外，李开复提到，通用模型决定了行业模型的天花板。虽然行业大模型有相当大的价值，但是底座如果不好，也无法完成超过底座的事情，所以选底座就要选表现最好的底座。李开复自信地表示，“今天我们在中英文上就是最好的底座，没有之一，也希望更多人选择Yi-34B。”</p><p></p><h3>如何解决算力和数据问题</h3><p></p><p>&nbsp;</p><p>“模型团队非常重要，但并不是雇50个人、100人就能解决问题，而是需要很强的团队。这通常不是很大的团队，团队做得太大了反而会分散GPU资源。”李开复说道。零一万物认为，人均GPU卡能用到多少决定了模型能力的上线。</p><p>&nbsp;</p><p>零一万物内部建立了一个AI Infrastructure（人工智能基础设施技术，简称“Infra”）的团队，成员来自国内顶级公司、国内外顶级高校和跨国公司，负责大模型的研发。</p><p>&nbsp;</p><p>在预训练阶段，高价值数据是最重要的，为此零一万物在数据处理上投入了非常大的精力。</p><p>&nbsp;</p><p>首先，零一万物通过采购、合法爬虫、开源等渠道获得训练模型数据。面对庞杂、质量不齐的数据，团队会先用AI能力进行系统化筛选，之后再做人工评估，基本会从一百多T数据里留下3T左右，包括一定比例的中英文数据，该数据保留率是其他厂家的1/10左右。</p><p>&nbsp;</p><p>在训练中，Infra团队花了很长时间研究scaling law，即模型的预测能力。“我们不做各种试错，因为GPU资源非常昂贵，所以我们是要把规模化做好，当推到下一个尺寸时不要再摸索和试错了，因为尺寸越大成本越高。“李开复介绍道。</p><p>&nbsp;</p><p>Infra团队表示，整个模型训练过程其实是动力学过程，中间每一步基本上都可以通过数学方式预测出来，而不需要做大量的实验。因此，团队可以将每一千步的误差控制在千分之几范围内。不管是做数据匹配、超参搜索，还是模型结构的试验，这个方法都特别重要。</p><p>&nbsp;</p><p>Infra团队在6B上做各种实验优化算法和模型，并能丝滑地从6B推向34B。借助该能力，Yi-34B的训练成本下降了40%。</p><p>&nbsp;</p><p>“我们将这一整套的训练平台称为科学训模。很多人把训练大模型比做‘炼丹’，也有人说模型训练一下就飞了，因为它没有收敛。我们做的规模预测用数学科学可以推理，小的尺寸如果能成功，大的尺寸也大概率可以成功，我们实验后也成功了。”李开复表示。</p><p>&nbsp;</p><p>关于算力资源，零一万物在很早时候就做了资源规划，现在的算力储备可以支持其用到18个月以后。另外，团队还建立了故障预测与故障解决大模型，利用模型本身为预训练过程中可能出现的问题设计相应的解决方案，以及如何以最低成本解决这个问题。</p><p>&nbsp;</p><p>对于预训练，零一万物技术副总裁及Pretrain负责人黄文灏表示，过程中并没有特别关注指标，因为针对指标做优化也可能出现问题，所以内部会有很多衡量模型能力的方法。比如模型到底压缩了哪些信息和知识是一个值得关注指标，但只要训练数据足够高质量，training dynamics做得足够好，出来的模型效果自然会比较好。</p><p>&nbsp;</p><p>另外，由于要将模型开源，零一万物在训练模型时还注重模型在IQ和EQ方面的均衡性。团队想要模型既可以支持代码推理类任务，也可以支持情感类任务。</p><p></p><h4>开源长窗口通用模型</h4><p></p><p>&nbsp;</p><p>之前的长窗口工作都是闭源的，无论是OpenAI的32K或者Cloud的100K。零一万物发现，开发者有大量基于长窗口模型进行微调的需求，因此这次直接开源了长窗口的base模型，开发者可以根据自己的数据去微调有效的长窗口应用。</p><p>&nbsp;</p><p>一般来说，更长的窗口会带来更多的计算，计算复杂度也会指数级上升，还要解决数据完备度的问题，这些都对计算、显存、内存和通信等都是非常大的技术挑战。另外，随着窗口越来越长，计算所需时间也越来越长，一旦端到端的反馈时间太长也就没有太大的意义了。因此，大部分模型都会限定窗口大小，零一万物限定了在200K以下。</p><p>&nbsp;</p><p>技术团队进行了全栈优化，包括计算跟通信的重叠堆叠技术、序列并行的技术、通信压缩技术，包括里面关键算子的重构等。虽然后续还有进一步拓宽的余地，但考虑到实用性和成本的均衡，团队目前就开源出来现在的长度版本。</p><p>&nbsp;</p><p>李开复表示，开源对推动世界技术革命的发展有着非常重要的意义。“很多人觉得大模型需要超级多的资源，只有OpenAI、微软、谷歌、阿里、百度、腾讯这样的公司才能做，但是任何技术都是需要全球化的参与，那么开源让大家都有机会能够接触到大模型。”</p><p>&nbsp;</p><p>“这两个模型的尺寸其实就是量身定做给开源社区使用的，资源多的可以用34B，但是也不会需要特别不合理的资源，而6B可以让更多的开发者能够使用。”李开复称。</p><p>&nbsp;</p><p>对于未来会不会开源更大模型的问题，零一万物技术副总裁及AI Infra负责人戴宗宏表示，这不取决于零一万物有没有更大的模型，而是取决于开源社区里的普通开发者有没有能力，或者有没有那么多的资源用到这样的大模型。“如果在摩尔定律之下，更便宜的卡可以支撑更大的模型，我们一定会考虑把我们更大的模型开源。”</p><p></p><h3>做ToC的超级应用</h3><p></p><p>&nbsp;</p><p>“我们对于未来的一个愿景就是，大模型时代不仅仅是人类跨向AGI的重要一步，它也是一个巨大的平台机会。”李开复认为，这个机会就是创造超级应用。</p><p>&nbsp;</p><p>李开复解释称，如果说PC时代赋予给开发者用户的机会是computer on every desk，移动互联网带来的机会是随时随地的计算，smartphone on &nbsp;every hand，那么现在的AI 2.0时代带来的巨大机会就是把一个超级大脑对接和赋能给每一个应用，即AI for everyone。</p><p>&nbsp;</p><p>“PC时代，微软Office就是超级应用；移动互联网时代，微信、抖音是相当好的超级应用；AI 2.0时代，毫无疑问最大的商机也会是超级应用，所以这个方向是零一万物努力的目标。过去的两个时代值得借鉴，因为人类历史就是不断重复，每一个时代最大的机会跟上一个时代是可以推延的。”</p><p>&nbsp;</p><p>李开复的考虑是，首先一切的基础是大模型。“我觉得未来的内容应该主要是由AI来创造，人来帮忙，这个才是王道。所以我们Super APP开发第一点就是AI First、AI Native，没有大模型整个产品就不成立。”</p><p>&nbsp;</p><p>其次，商业化非常重要。AI 1.0公司面临的挑战主要就是商业化问题：要么收入没有做好，要么缺乏持续化收入。“字节、阿里、百度、谷歌、Facebook能够成为伟大的公司，就是因为他们的收入是有质量的。”李开复说道，“所以我们做的应用一定是朝着能够快速有收入，而且能够产生非常好的利润、收入是高质量的、可持续的，而不是一次性在某一个公司上打下一个单子。”</p><p>&nbsp;</p><p>李开复表示，AI 2.0时代的超级应用一定是在消费者级别的ToC超级应用。他透露，Super App的雏形将在不久后对外发布。对于这个Super App，团队会从简单的功能开始，然后根据捕捉到的用户需求和技术精髓不断迭代。此外，该应用虽然面向国内，但也会面向国外市场。</p><p>&nbsp;</p><p>“今天创业者最好的机会是在AI 2.0上面开发App，如果找对机会、聪明快速勤奋地迭代，任何一个App都有机会成为Super App，成为AI 2.0时代的微信、抖音。”李开复说道。</p><p></p><h3>未来规划</h3><p></p><p>&nbsp;</p><p>对于未来，零一万物表示，一方面会继续在34B规模上进行一系列开源动作，另一方面会进一步提高模型的智能极限。</p><p>&nbsp;</p><p>“我们已经在训练千亿参数以上模型，但是我们觉得模型参数可以再提高一到两个数量级，达到万亿或者十万亿的规模。数据上，我们现在基于几十T token的高质量数据，未来还可以提高到几百T或者几千T。模型智能还是有很大的发展。”据悉，零一万物现在已经在训练千亿模型，更大模型的所有前置实验也已完成，剩下的就是按部就班地训练。</p><p>&nbsp;</p><p>此外，零一万物已经有了一个超过十人的多模态方面的团队，未来一两个月内也会有相关产品发布。多模态已经纳入公司更长周期的规划中。</p><p>&nbsp;</p><p>&nbsp;</p>",
    "publish_time": "2023-11-08 14:31:34",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Generative AI 新世界：过去、现在和未来",
    "url": "https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE",
    "summary": "<p>人类善于分析事物。但是现在看来，机器很有可能做得更好。机器可以不知疲倦夜以继日地分析数据，不断从中找到很多人类场景用例的模式：信用卡欺诈预警、垃圾邮件检测，股票价格预测、以及个性化地推荐商品和视频等等。他们在这些任务上变得越来越聪明了。这被称为 “分析人工智能（Analytical AI）” 或”传统人工智能（Traditional AI）”。</p><p></p><p>但是人类不仅擅长分析事物，还善于创造。我们写诗、设计产品、制作游戏和编写代码。直到公元 2022 年之前，机器还没有机会在创造性工作中与人类竞争，它们只能从事分析和死记硬背的认知劳动。但是现在（是的，就是现在）机器已经开始在创造感性而美好事物的领域尝试超越人类，这个新类别被称为 “生成式人工智能（Generative AI）”。这意味着机器已经开始在创造生成全新的事物，而不是分析已经存在的旧事物。</p><p></p><p>生成式人工智能不仅会变得更快、更便宜，而且在某些情况下比人类手工创造的更好。每个需要人类创作原创作品的行业—从社交媒体到游戏、从广告到建筑、从编码到平面设计、从产品设计到法律、从营销到销售都有待全新重塑。某些功能可能会被生成式人工智能完全取代，或者激发出超越人类想象力的全新灵感。</p><p></p><p></p><blockquote><a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DInfoQ\">亚马逊云科技开发者社区</a>\"为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fuser%2Fregister%3Fshow%3Dtab1%26from%3Dindex%26sc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DInfoQ\">点这里</a>\"让它成为你的技术宝库！</blockquote><p></p><p></p><p>新世界正在到来。</p><p></p><h1>Transformer 新世界</h1><p></p><p></p><p>做为一名曾经多次穿越过市场周期的从业者，我亲历过通信行业、IT 行业、移动互联网行业等不同时代的周期，亲身体验过其间的潮起云涌，亲眼目睹过其中的天高云淡，以及最终惨烈竞争后的回归平淡。因此，面对已经开启的 AI 时代周期，与其盲目地跳进去跟随，不如先搞清楚这个新周期的一些底层逻辑，比如说：知识底座。</p><p></p><p>如果说 TCP/IP、HTML 等知识结构是上一个时代的知识底座，那么面对已经开始的 AI 时代，我们每个人是否应该先问自己一个问题：“什么是 AI 时代的知识底座？”</p><p></p><p>从到目前为止 AI 的知识发展看来，也许这个知识底座会是：Transformer。</p><p></p><h2>1 Transformer 概述</h2><p></p><p></p><p>欢迎进入 Transformer 的新世界。</p><p></p><p>在过去的五年中，人工智能世界发生了很多令人欣喜的重大变化。其中许多变化是由一篇名为 “Attention is All You Need” 的论文推动的。这篇发表于 2017 年的论文介绍了一种名为 “Transformer” 的新架构。下图为“Attention is All You Need” 的论文中描述的 Transformer 模型的架构图示。</p><p><img src=\"https://static001.geekbang.org/infoq/64/646dd7f43d6f56a389c3d3d4011d39cd.png\" /></p><p>Source:&nbsp;<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf%3Ftrk%3Dcndc-detail\">https://arxiv.org/pdf/1706.03762.pdf?trk=cndc-detail</a>\"</p><p></p><p>概括来说，Transformer 模型为机器学习领域做出了两项贡献。首先，它提高了在人工智能中使用并行计算的效率。其次，它引入了 “注意力（Attention）” 的概念，这使人工智能能够理解单词之间的关系。你所听到的技术，例如 GPT-3、BERT、Sable Diffusion 等，都是 Transformer 架构在不同领域演进的结果。</p><p></p><h2>2 注意力机制（Attention）</h2><p></p><p></p><p>什么是注意力机制？根据该论文中的描述，注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出是按值的加权总和计算的，其中分配给每个值的权重由查询的兼容性函数与相应键值计算得出。Transformer 使用多头注意力（multi-headed attention），这是对称为缩放点积注意力（scaled dot-product attention）的特定注意力函数的并行计算。如下图所示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/a2/a2e65bdfe28920f08cf204d601fdcced.png\" /></p><p>&nbsp;Source:&nbsp;<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf%3Ftrk%3Dcndc-detail\">https://arxiv.org/pdf/1706.03762.pdf?trk=cndc-detail</a>\"</p><p></p><p>上面这段对“注意力机制”的描述还是偏学术化。维基百科上的定义会更通俗易懂些：“注意力机制（英语：attention）是人工神经网络中一种模仿认知注意力的技术。这种机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重，以此将网络的关注点聚焦于数据中最重要的一小部分。数据中哪些部分比其他部分更重要取决于上下文。可以通过梯度下降法对注意力机制进行训练 ……”</p><p>可见，注意力机制的灵活性来自于它的“软权重”特性，即这种权重是可以在运行时改变的，而非像通常的权重一样必须在运行时保持固定。</p><p></p><h2>3 Transformer in Chip</h2><p></p><p></p><p>很多人工智能领域的思想领袖和专家，认为 Transformer 架构在未来五年左右并不会有太大变化。这就是为什么你会看到一些芯片制造商在其新芯片（例如 NVIDIA H100）中集成 Transformer Engine 的原因。</p><p></p><p>在 2022 年拉斯维加斯的 re:Invent 2022 中，来自 NVIDIA 的架构师分享了如何在亚马逊云科技上，使用 NVIDIA 新一代芯片做深度学习训练的专题，里面特别提到了 H100 芯片中 Transformer Engine 的设计结构和初衷。对技术架构细节感兴趣的同学，可以通过以下视频深入了解：</p><p></p><p><a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dl8AFfaCkp0E%3Ftrk%3Dcndc-detail\">https://www.youtube.com/watch?v=l8AFfaCkp0E?trk=cndc-detail</a>\"</p><p><img src=\"https://static001.geekbang.org/infoq/c4/c4c3c7c5c1c7c0e53154b6ff5698088b.png\" /></p><p></p><p></p><h2>4 Transformer 演进时间线</h2><p></p><p></p><p>一个有趣的视角是将各种 Transformer 按照出现的时间顺序排列的图示。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e9/e9cdf6f817052122267bd8a610257a40.png\" /></p><p>Source: “Transformer models: an introduction and catalog”&nbsp;<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F2302.07730%3Ftrk%3Dcndc-detail\">https://arxiv.org/abs/2302.07730?trk=cndc-detail</a>\"</p><p></p><p>我听到过的一个比较有趣的视角是：如果您之前对 Transformer 知道得不多，不要恐慌。因为您看到引领这一波生成式人工智能（Generative AI）变革的重要几篇论文的情况：</p><p></p><p>CLIP 论文在 2021 年发表；Stable Diffusion 和 DALL-E-2 在 2022 年才出现；GPT3.5、ChatGPT、Bloom 等在 2022 年底才出现……</p><p>这个新世界的演进才刚刚开始，你还有足够的时间重新开始学习 Transformer！</p><p></p><h1>Generative AI</h1><p></p><p></p><h2>1 为什么现在发生?</h2><p></p><p></p><p>Generative AI 与更广泛的人工智能具有相同的值得人类深入思考问题：“为什么现在发生？” 概括来说，这个答案是我们当下具有：</p><p>更好的模型；更多的数据；更多的计算；</p><p></p><p>Generative AI 的进化速度比我们所能想象的要快得多，为了将当前时刻置于大时代洪流的背景之下，非常值得我们大致地了解下 AI 的发展历史和曾经走过的路。</p><p></p><p>第一波浪潮：小型模型占据了至高无上的地位（2015 年之前）</p><p></p><p>小型模型在理解语言方面被认为是 “最先进的”。这些小型模型擅长分析任务，可用于从交货时间预测到欺诈分类等工作。但是，对于一般用途的生成任务，它们的表现力还不够。生成人类级写作或代码仍然是白日梦。</p><p></p><p>第二波浪潮：规模竞赛（2015 年至今）</p><p></p><p>2017 年发表的里程碑意义的论文（“Attention is All You Need”）描述了一种用于自然语言理解的新神经网络架构，这种架构名为 Transformer，它可以生成高质量的语言模型，同时更具可并行性，并且需要更少的训练时间。这些模型是 few-shot learners 的，因此可以相对容易地针对特定领域进行定制。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/e1/e13976c5aa69f65f9b401d3a9156e8b0.png\" /></p><p>Source:<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.science.org%2Fcontent%2Farticle%2Fcomputers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help%3Ftrk%3Dcndc-detail\">https://www.science.org/content/article/computers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help?trk=cndc-detail</a>\"</p><p></p><p>随着模型越来越大，它们开始提供人类层面的结果，然后是超人的结果。在 2015 - 2020 年间，用于训练这些模型的计算增加了 6 个数量级，其结果超过了人类在手写、语音和图像识别、阅读理解和语言理解方面的性能基准。GPT-3 模型在这时脱颖而出，该模型的性能比 GPT-2 有了巨大的飞跃，内容涉及从代码生成到写作等多项任务。</p><p>尽管基础研究取得了种种进展，但这些模型并不被人广泛使用。原因是它们庞大且难以运行（需要 GPU 编排等），能够使用这些模型的门槛太高（不可用或仅限封闭 BETA），而且用作云服务的成本也很高。尽管存在这些局限性，但最早的 Generative AI 应用程序开始进入竞争阶段。</p><p></p><p>第三波浪潮：更好、更快、更便宜（2022 年以后）</p><p></p><p>由于像亚马逊云科技这样的云技术公司，一直在推动云计算的普及，计算变得更加便宜。而像 diffusion model 等新技术降低了训练和运行推理所需的成本，研究界因此可以继续开发更好的算法和更大的模型。开发者访问权限从封闭 BETA 扩展到开放 BETA，或者在某些情况下扩展到开源（open-source）。对于一直缺乏 LLM 访问权限的开发人员来说，现在闸门已开放，可供探索和应用程序开发。应用程序开始蓬勃发展。</p><p></p><p>第四波浪潮：杀手级应用程序的出现（现在）</p><p></p><p>随着基础平台层的逐渐巩固，模型不断变得更好/更快/更便宜，模型访问趋向于免费和开源，应用层的创造力爆炸的时机已经成熟。</p><p></p><p>正如十年前的移动互联网爆发的前夜，由于移动通过 GPS、摄像头和移动连接等新场景、新功能释放了新类型的应用程序一样，我们预计这些大型模型将激发新一轮的 Generative AI 应用。我们预计 Generative AI 也将出现杀手级应用程序。</p><p></p><p>Source:<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail\">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>\"</p><p></p><h2>2 Generative AI: 应用层蓝图构想</h2><p></p><p></p><p>以下是 Generative AI 的应用格局图，描述了为每个类别提供支持的平台层以及将在上面构建的潜在应用程序类型。</p><p><img src=\"https://static001.geekbang.org/infoq/3d/3d38e1857cdb3133cc7dc57410d810e1.png\" /></p><p>Source:<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail\">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>\"</p><p></p><p>文本是进展最快的领域。</p><p></p><p>代码生成可能会在短期内对开发人员的生产力产生重大影响，如 <a href=\"https://www.infoq.cn/video/4oajrgIyfmkaaNFi7dJF?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">Amazon CodeWhisperer</a>\" 所示。</p><p>图像是一种较新的现象。我们看到了不同风格的图像模型的出现，以及用于编辑和修改生成的图像的不同技术。</p><p></p><p>语音合成已经存在了一段时间（例如，你好 Siri！）。就像图像一样，今天的模型也为进一步完善提供了起点。</p><p></p><p>视频和三维模型正在迅速上线。人们对这些模式开启电影、游戏、虚拟现实和实体产品设计等大型创意市场的潜力感到兴奋。</p><p></p><p>其他领域：从音频和音乐到生物学和化学，许多领域都在进行基础模型研发。</p><p></p><p>下图说明了我们如何期望基本模型取得进展以及相关应用成为可能的时间表。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/f8/f88ee1a43e3da7420420b13f7d3d63f5.png\" /></p><p>Source:<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail\">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>\"</p><p></p><h2>3 Generative AI: 文字生成图像（Text-to-Image）方向</h2><p></p><p></p><p>回顾过去的一年，有两个 AIGC 方向已经发生了让人惊艳的进步。其中一个方向就是：文字生成图像（Text-to-Image）方向。</p><p></p><p>根据来自亚马逊云科技的官方博客，用户现在可以很方便的在 SageMaker JumpStart 中使用 Stable Diffusion 模型，轻松地生成富有想象力的绘画作品。</p><p></p><p>The following images are in response to the inputs “a photo of an astronaut riding a horse on mars,” “a painting of new york city in impressionist style,” and “dog in a suit.”</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/53/53dfcc043d338fb96f5d4b625e7df2ef.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1\" /></p><p></p><p>The following images are in response to the inputs: (i) dogs playing poker, (ii) A colorful photo of a castle in the middle of a forest with trees, and (iii) A colorful photo of a castle in the middle of a forest with trees. Negative prompt: Yellow color.</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/04/04f7213f08a541711f019115c0cf0e60.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1\" /></p><p>Source:<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fblogs%2Fmachine-learning%2Fgenerate-images-from-text-with-the-stable-diffusion-model-on-amazon-sagemaker-jumpstart%2F%3Ftrk%3Dcndc-detail\">https://aws.amazon.com/cn/blogs/machine-learning/generate-images-from-text-with-the-stable-diffusion-model-on-amazon-sagemaker-jumpstart/?trk=cndc-detail</a>\"</p><p></p><p>关于文字生成图像（Text-to-Image）方向的论文解读、示例代码等我们还会有其他专题深入讨论。</p><p>以上就是关于 Transformer 和 Generative AI 的部分介绍。在下一篇文章中，我们将详细讨论关于 Generative AI 另一个重要的进步方向就是：文字生成（Text Generation）方向。分享这个领域的最新进展，以及亚马逊云科技在为支持这些大型语言模型（LLMs）的编译优化、分布式训练等方面的进展和贡献。</p><p></p><p>作者黄浩文</p><p></p><p>亚马逊云科技资深开发者布道师，专注于 AI/ML、Data Science 等。拥有 20 多年电信、移动互联网以及云计算等行业架构设计、技术及创业管理等丰富经验，曾就职于 Microsoft、Sun Microsystems、中国电信等企业，专注为游戏、电商、媒体和广告等企业客户提供 AI/ML、数据分析和企业数字化转型等解决方案咨询服务。</p><p></p><p>文章来源：<a href=\"https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F6413095e3d950b57b3f9f63d%3Fsc_medium%3Dregulartraffic%26amp%3Bsc_campaign%3Dcrossplatform%26amp%3Bsc_channel%3DInfoQ\">https://dev.amazoncloud.cn/column/article/6413095e3d950b57b3f9f63d?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=InfoQ</a>\"</p>",
    "publish_time": "2023-11-08 14:34:17",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "烧钱、裁员、叫停业务，这家曾经的自动驾驶独角兽正经历至暗时刻",
    "url": "https://www.infoq.cn/article/tcB7Ql56Q1i0UOmgSywd",
    "summary": "<p></p><h2>通用停止生产Cruise无人驾驶面包车</h2><p></p><p>11月6日，据路透社报道，通用汽车公司旗下自动驾驶汽车子公司 Cruise 宣布停止生产无人驾驶面包车 Origin。Cruise 首席执行官 Kyle Vogt 表示：\" 由于很多事情都在变化之中，我们确实与通用汽车一起做出了暂停生产 Origin 的决定。\"Vogt 补充道，公司已经生产了数百辆 Origin 汽车，短期内已经够用了。</p><p>&nbsp;</p><p>10月2日，一辆汽车在旧金山一处路口处撞倒了一名女子，她被冲击力抛向Cruise一辆无人驾驶出租车的面前。Cruise汽车碾过了她、短暂刹停了一会，之后又把她拖行了几米才最终停靠在路边。事件给受害者造成了严重伤害。</p><p>&nbsp;</p><p>10月24日，加州机动车辆管理局（California’s Department of Motor Vehicles）暂停了Cruise在加州运营无人驾驶汽车的资格，并指控该公司隐瞒了一起旧金山行人事故的关键视频。在被吊销执照几天后，Cruise主动暂停了整个车队的无人驾驶业务。</p><p>&nbsp;</p><p><img src=\"https://static001.geekbang.org/infoq/6b/6b9006b8ac2b3fd6d4826cc4719fbc75.png\" /></p><p></p><p>&nbsp;事故发生一周后，Cruise公司将其无人驾驶汽车从路面上撤下，集中停放在旧金山一处停车场内。</p><p>&nbsp;</p><p>加利福尼亚州机动车辆管理局（DMV）上周指责Cruise在最初提供给该局的事件视频中，刻意忽略了拖行该女子的片段。车管局称Cruise公司存在“歪曲”行为，并要求对方关闭在加州的无人驾驶汽车业务。</p><p>&nbsp;</p><p>两天之后，Cruise决定主动出击，直接暂停全美范围内的无人驾驶业务，约400辆汽车被撤下公共道路。此后，Cruise董事会聘请了Quinn Emanuel律师事务所来调查此事，包括指导其如何与监管机构、执法部门和媒体斡旋。</p><p>&nbsp;</p><p>Cruise董事会打算认真评估事务所给出的调查结果和行动建议。周一参加Cruise公司会议的两位消息人士表示，专门评估复杂软件系统的咨询公司Exponent也在对此次事故展开单独审查。</p><p>&nbsp;</p><p>五名前任及现任员工、连同多位业务合作伙伴都提到，Cruise员工担心公司目前面对的难题可能并没有简单的解决办法。而同行竞争对手更是担心Cruise惹出的“事端”可能导致整个产业都面临更严格的无人驾驶汽车监管要求。</p><p></p><h2>通用旗下Cruise发展历程：在无人驾驶竞赛中迷失自我</h2><p></p><p>&nbsp;</p><p>公司内部人士普遍将问题归咎于38岁的掌门人Vogt和他旗帜鲜明的“工业党”文化。这种文化将项目速度置于安全之上。在Cruise与最大无人驾驶竞争对手Waymo的竞争当中，Vogt一直希望能够全力推进、占据优势地位。在他看来，Uber与Lyft之间的对抗已经表明，谁能抢先一步、谁就能统治整个市场。</p><p>&nbsp;</p><p>纽约卡多佐法学院研究新兴汽车技术的Matthew Wansley教授认为，“Vogt是个愿意冒险的人，他倾向于迅速采取行动，身上有着典型的硅谷特色。这既解释了Cruise为何能取得成功，也解释了他们怎样一步步身陷困境。”</p><p>&nbsp;</p><p>据参加此次公司会议的两名员工透露，Vogt在周一宣布公司暂停运营时，承认自己清楚什么时候才能恢复运营、甚至有可能要考虑裁员。</p><p>&nbsp;</p><p>Vogt还承认Cruise已经在公众中失去了信任，并简单介绍了一项提高透明度、重视安全问题以恢复公众信任的计划。他任命安全副总裁Louise Zhang为公司临时首席安全官，并表示Zhang将直接向他汇报。据与会者回忆，Vogt表示“信任需要很长的时间才能慢慢建立、但崩溃往往就在瞬息之间。我们需要查清真相，并重新建立起信任。”</p><p>&nbsp;</p><p>Cruise公司拒绝了对Vogt的采访请求。通用汽车也在一份声明中指出，“仍然坚定支持Cruise的商业化目标”，对Cruise的使命和技术抱有信心，并支持其将安全放在首位。</p><p>&nbsp;</p><p>Cruise公司CEO Kyle Vogt表示，无人驾驶汽车比人类驾驶的车辆要安全得多。</p><p>&nbsp;</p><p>Vogt从十几岁起就开始研究自动驾驶汽车。13岁时，他对一辆Power Wheels骑乘玩具进行了编程，让它沿着停车场的黄线行驶。后来，他又在麻省理工学院学习期间参加了由政府资助的自动驾驶汽车比赛。</p><p>&nbsp;</p><p>2013年，他创办了Cruise Automation。这家公司对传统汽车进行改造，为其配备了传感器和计算机，能够在高速公路上实现自动行驶。三年之后，他以10亿美元的价格将Cruise卖给了通用汽车集团。</p><p>&nbsp;</p><p>交易完成之后，通用汽车总裁Dan Ammann接任Cruise公司CEO，Vogt则担任总裁兼首席技术官。</p><p>&nbsp;</p><p>前员工们表示，作为总裁，Vogt建立起Cruise工程团队，并将公司的规模从40人扩大至约2000人。他还主张尽快将产品推向更多市场，并相信公司的动作越快、就能拯救更多的生命。</p><p>&nbsp;</p><p>2021年，Vogt接任Cruise公司CEO。通用汽车CEO Mary T. Barra也开始让Vogt现身母公司的财报电话会议和展示活动。他在会上大肆宣传自动驾驶市场，并预测到2030年Cruise将覆盖100万辆汽车。</p><p>&nbsp;</p><p>Vogt不断敦促自己的公司继续扩张，从在旧金山公共道路上的行驶测试中总结经验。Cruise公司在旧金山的单次乘车平均收费为10.50美元。</p><p>&nbsp;</p><p>前员工提到，去年夏季一辆Cruise汽车与一辆在公交车道上行驶的丰田普锐斯相撞之后，公司里有人建议让车辆暂时避开设有公交车道的路线。但Vogt否定了这个想法，他说Cruise汽车就是要多接触这类道路才能应对其复杂性。该公司随后更改了软件，希望降低引发类似事故的风险。</p><p></p><p>到8月，一辆Cruise无人驾驶汽车又与一辆前往处理紧急情况的旧金山消防车相撞。之后，该公司改变了汽车对警报的检测方式。</p><p>&nbsp;</p><p>事故发生之后，市政官员和活动人士开始向加州政府高血压，要求其放慢无人驾驶车辆的扩张速度。旧金山监事会主席Aaron Peskin还呼吁Cruise提供更多碰撞相关数据，包括计划外停车、交通违规及车辆性能等记录。</p><p>&nbsp;</p><p>Peskin表示，“随着时间推移，Cruise的种种行为正不断消耗人们对它的信任。”</p><p>&nbsp;</p><p>如今，Waymo的估值从最高1750亿美元下跌到300亿美元。</p><p></p><h2>烧钱、裁员、业务停摆，自动驾驶走不出寒冬？</h2><p></p><p>&nbsp;</p><p>由于业务被冻结，人们担心Cruise会给通用汽车带来巨大的财务负担，甚至损害这家汽车巨头的声誉。因为就在加州车管局要求Cruise关闭其无人驾驶业务的几个小时前，通用掌门人Barra女士还告诉投资者，自己这家子公司拥有“光明的发展前景”。</p><p>&nbsp;</p><p>Cruise如今已有一个多星期没有收取车费、接送乘客了。在旧金山、菲尼克斯、达拉斯、休斯顿、迈阿密乃至得州奥斯汀，几百辆白橙相间的雪佛兰Bolt改造车在停车场中堆放静置。如今的尴尬局面，似乎也让Cruise在2025年达成10亿美元收入目标的豪言变成了一句玩笑。</p><p>&nbsp;</p><p>过去一年，通用汽车每个季度平均在Cruise身上烧掉5.88亿美元，较上年同期增长42%。据一位知情人士透露，Cruise运营的每辆雪佛兰Bolt的成本约为15万到20万美元。</p><p>&nbsp;</p><p>在无人驾驶业务被叫停时，Cruise旗下总计400辆无人驾驶汽车中有一半都部署在旧金山。这些车辆需要大量操作人员的支持，平均每车对应1.5位员工。两位知情人士透露，操作人员每隔2.5到3英里就会介入并协助指挥车辆。换句话说，在收到车辆疑似出现问题的移动信号之后，他们需要频繁采取措施来实施远程控制。</p><p>&nbsp;</p><p>Evercore ISI财务分析师Chris McNally表示，为了弥补不断上升的运营成本，通用汽车需要为Cruise业务注入或筹集更多资金。Barra女士也在10月底的分析师电话会议上宣布，通用汽车将在年底之前公开新的融资计划。</p><p>&nbsp;</p><p>Cruise的窘境只是整个自动驾驶行业寒冬的一个缩影。虽然以ChatGPT为代表的大语言模型在各行业逐渐落地，但这一AI大趋势似乎并没有给原本就遇冷的自动驾驶产业带来一丝暖阳。</p><p>&nbsp;</p><p>最近两年，裁员、倒闭、市值暴跌成为了自动驾驶行业的三个“热词”。</p><p>&nbsp;</p><p>英特尔旗下的自动驾驶公司Mobileye在去年将估值一减再减，从最初的500亿美元缩水到300亿美元，最后只以160亿美元的身价上市；谷歌和软银都有所投资的自动驾驶机器人Nuro在不到半年的时间内连续进行了占总员工20%和30%的裁员。</p><p>&nbsp;</p><p>就在上个月，Waymo启动了今年的第三次裁员。Waymo 发言人接受媒体采访时表示，此次裁员是内部重组过程的一部分。</p><p>&nbsp;</p><p>“作为正常业务过程的一部分，少数 Waymo 团队最近对其团队进行了调整，”该发言人拒绝提供受影响员工人数的详细信息，但表示人数很少。</p><p>&nbsp;</p><p>早在今年年初，作为Alphabet集团大规模裁员的一部分，Waymo就已经解雇了数十名员工，3月份，该公司又解雇了一批员工。</p><p>&nbsp;</p><p>据报道，Waymo 今年年初雇用了约 2500 名员工。今年早些时候的两轮裁员中，有超过200人被裁，但此次裁员后剩余的 Waymo 员工人数尚不清楚。</p><p>&nbsp;</p><p>参考链接：</p><p><a href=\"https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html\">https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html</a>\"</p><p><a href=\"https://sfstandard.com/2023/10/17/tech-layoffs-waymo-san-francisco-robotaxi/\">https://sfstandard.com/2023/10/17/tech-layoffs-waymo-san-francisco-robotaxi/</a>\"</p><p><a href=\"https://theintercept.com/2023/11/06/cruise-self-driving-cars-children/\">https://theintercept.com/2023/11/06/cruise-self-driving-cars-children/</a>\"</p>",
    "publish_time": "2023-11-08 15:09:28",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "被时代选中的智谱AI：成为OpenAI，超越OpenAI",
    "url": "https://www.infoq.cn/article/sJzsW7aMIglaaKFa9EqX",
    "summary": "<p>“追赶 OpenAI ”，是<a href=\"https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">智谱AI</a>\" CEO 张鹏对外分享时屡次提到的一句话。坦然面对不如别人需要勇气，但公开承诺要追上行业标杆，则需要实力。那么，才成立四年的智谱AI 凭什么？</p><p></p><h2>积淀与机遇，一个也不能少</h2><p></p><p>&nbsp;</p><p>众所周知，智谱AI是清华系出身的学院派创业公司。</p><p>&nbsp;</p><p>1996年，清华大学计算机系知识工程实验室申请成立，这是人工智能下的一个分支，以机器学习、数据挖掘为主要研究方向。2006年，实验室开始做工程化，并推出了AMiner系统。在这之后的10年里，实验室一直进行工程方面的研究。2016年左右，随着相关技术的成熟，实验室开始进行应用转化。直至2019年，智谱AI成立。</p><p>&nbsp;</p><p>刚成立的智谱一方面延续之前的研究，一方面积极进入市场，将实验室积累的科技成果和产品系统用于实际项目并商业化。如果没有意外，这个路线会持续一段时间。但企业战略方向往往是由技术本身和行业应用领域的热点共同决定的。</p><p>&nbsp;</p><p>2020年成为智谱AI发展的一个关键拐点。</p><p>&nbsp;</p><p>GPT-3的发布给了大家非常明确的信号，即大型模型真正具备了实际可用性。但“要不要跟进大模型”却是一个问题。</p><p>&nbsp;</p><p>创业公司战略做错一次就是致命的，虽然此刻看来当时智谱AI的选择没错，表现之一就是风投态度：此后智谱AI每年都能拿到数亿融资，目前单2023年已累计<a href=\"https://www.infoq.cn/article/AXXtqD6xU6FghjsNE408?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">融资额达到25亿人民币</a>\"。但当时情景下，这依然是一项极其冒险的事情，创始团队无法轻易决定。</p><p>&nbsp;</p><p>那如果跟进大模型呢？智谱AI也并非完全从零开始。创始团队多年积累，大模型可以看作是团队积极学习和扩充高速挖掘的延续。因此，在反复纠结和讨论后，智谱AI终于决定全面投身大模型。</p><p>&nbsp;</p><p>但在通用大模型和行业小模型的选择上，智谱AI虽然有参考 OpenAI，但还是决定坚持走通用大模型这条路。</p><p>&nbsp;</p><p>一是技术方面。张鹏认为，行业模型必须建立在通用模型的基础之上，否则独立发展的行业模型由于商业规模较小，其智能水平将受到明显的限制。此外，行业模型很容易被通用模型的能力快速超越。</p><p>&nbsp;</p><p>将行业模型建立在通用模型之上有好有坏。好处是可以节省基础模型预训练的成本和周期，享受到基础模型本身智能提升好处的同时，降低被通用模型取代的风险。坏处则是通用模型本身在行业场景中可能并不完美，因此需要专业知识积累。就像一个专业学校毕业的研究生要成为行业专家也需要时间来不断积累专业知识和经验。</p><p>&nbsp;</p><p>因此，在张鹏看来，行业模型被看作是在当前技术水平和时间点下为解决行业应用需求而催生的一种形态。虽然这种形态具有历史意义，但从更长远的角度看，它只是一个阶段性的产物。</p><p>&nbsp;</p><p>二是社会方面。模型之所以不能掌握行业专业知识，部分原因是因为行业知识的数据不完整或受到限制。这与过去十多年大数据和人工智能发展面临的问题类似，即存在数据孤岛和数据壁垒。这就导致了模型的能力必须迁就数据。</p><p>&nbsp;</p><p>这个问题的根源不是技术层面的决策，而是与当前社会发展、信息化水平、行业信息化程度、数据安全以及各种制度和机制有关的问题。</p><p>&nbsp;</p><p></p><h2>对标 OpenAI，相似但不同</h2><p></p><p>&nbsp;</p><p>同属通用模型赛道，是外界要拿智谱AI和OpenAI比，还是智谱AI自己要和OpenAI比？实际上，两者都有。国内需要有“自己的OpenAI”，而智谱AI的目标恰好也是OpenAI。</p><p>&nbsp;</p><p>“OpenAI公司一直在领跑，所以最直接的方式是先达到他们的水平。”张鹏说道。在技术选型和解决方案方面，智谱AI选择直接对标OpenAI：</p><p>&nbsp;</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ce/ce729e5998027254c52b49acd5718207.png\" /></p><p>但智谱AI并没有完全依赖OpenAI的技术经验。</p><p>&nbsp;</p><p>GPT 的问题是注意力是单向的，无法充分捕捉 NLU 任务中上下文词之间的依赖关系。虽然在GLM模型的早期研发阶段，GPT-3已经非常出色，但智谱AI选择从底层算法原理入手，将自己的理解融入进去，最终需要通过实验和应用来验证。</p><p>&nbsp;</p><p>张鹏及其团队在2017年开始关注预训练模型，那时候大模型还没有出现，市面上主要是一些几千万数量级的、相对较小的模型。</p><p>&nbsp;</p><p>团队发现，当时的模型尽管架构相似，但在算法框架方面存在许多不同，比如encoder-decoder模型、auto-encoding自编码模型、auto-regressive自回归模型等。虽然前人尝试通过多任务学习结合它们的目标来统一不同的框架，但由于自编码和自回归目标在本质上的不同，简单的统一并不能充分继承两个框架的优势。</p><p>&nbsp;</p><p>2021年，智谱AI开始自主开发训练框架，着手训练一个拥有百亿参数的模型，并在年底启动了千亿模型的训练。智谱AI的GLM模型将自回归生成和自回归填空集成，即将 NLU 任务构建为包含任务描述的填空题，这些问题通过自回归生成来回答。通过将这两种模式的优点结合起来，模型在下游任务中能够完成更多任务。因此，这个预训练模型的显著特点是单一模型能够处理多个任务，从而用更低的成本来支持更多上层任务。</p><p><img src=\"https://static001.geekbang.org/infoq/6e/6e7df84ba92b6929f6ad6cf6535896d3.png\" /></p><p></p><p>大模型主要被关注的是性能。这里的性能有两方面：一是各种评估指标上的表现，甚至是人工评估标准，二是推理效率和硬件基础成本。这两个方面的性能都非常重要，前者涉及到了模型的潜在极限水平，后者则涉及到了模型的可用性，即在产业链中使用该模型需要付出什么成本以及预期的回报是多少。</p><p>&nbsp;</p><p>对于GLM模型，智谱AI除了在解决精度、稳定性和效率上进行改进，包括算法层面的修改、算子和加速方法的选择，还有工程层面的决策，如商业集群和网络的选择以及性能优化。</p><p>&nbsp;</p><p>在早期某个阶段，模型训练的质量与数据之间存在密切的关系。为此，智谱AI也花费了一些时间和精力来获取更高质量的数据。</p><p>&nbsp;</p><p>智谱AI内部有一个专门的数据处理团队，进行数据清洗和过滤，将数据进行校准和转化等工作。智谱AI训练大模型的数据主要来自公开数据、团队多年来积累的数据、交换或采购合作伙伴数据。</p><p>&nbsp;</p><p>作为一个中英双语模型，GLM数据处理的复杂性略有增加。在模型训练中，文本需要分割成token，只有一种语言的话，token的数量是固定的，但如果涉及另一种语言，token的数量就会显著增加，整个扩展的词汇表会更大。另外，中英文混合数据的处理也是一个问题，模型需要在中英文上都表现良好，有效地跨语言工作。对此，智谱AI主要在设计训练算法以及损失函数的计算等方面做了些额外工作。</p><p>&nbsp;</p><p>对于“高质量的中文语料相对英文语料较少”的观点，张鹏并不赞同，“中文用户的数量全球最多，互联网用户也最多、活跃度也高，为什么中文数据的质量会有问题呢？”张鹏反问道。</p><p>&nbsp;</p><p>他认为，问题的根本在于数据的封闭和存在获取壁垒。可能有大量的中文用户在互联网上没有贡献高质量的内容，也可能是他们贡献了高质量的内容，但这些内容不是公开可获取的。</p><p>&nbsp;</p><p>智谱AI内部通常采用逐渐改进的方法，更倾向与自己之前的版本或标准版本进行比较，追求模型的性能，特别是某一方面上，能有明显提升。</p><p>&nbsp;</p><p>可以看到，智谱AI的产品更新频率很快。在今年3月首次推出ChatGLM基座模型后，智谱AI又在10月底将其<a href=\"https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search\">迭代到了第三代</a>\"。</p><p>&nbsp;</p><p>ChatGLM3采用了智谱AI独创的多阶段增强预训练方法、集成了自研的 AgentTuning 技术，并瞄向GPT-4V做技术升级。此外，智谱AI还推出了可手机部署的端测模型ChatGLM3-1.5B和3B，支持包括Vivo、小米、三星在内的多种手机以及车载平台，甚至支持移动平台上 CPU 芯片的推理。</p><p><img src=\"https://static001.geekbang.org/infoq/74/74af9b30795d1619456291febad1c587.png\" /></p><p></p><p>此外，对于神经网络算法的核心问题，业内在过去的六七年里一直在寻找更高效的技术架构来解决计算和智能水平问题。这是一个偏向理论和基础性研究的长期工作，智谱AI 更多通过投资或支持清华大学等基础性研究团队和机构，也会参与做前瞻性或预期性的研究工作，参与到这一命题的研发中。</p><p></p><h2>做大模型，没有好走的路</h2><p></p><p>&nbsp;</p><p>在2020年之前，智谱AI主要从事算法研究工作，研究是团队的强项，这部分工作相对容易。但到了2021年，情况有所不同。研究出身的创始成员在如何将研究成果落地上，开始遇到许多问题。</p><p>&nbsp;</p><p>具体来说，团队缺乏处理大规模数据和资源项目的经验，因此，许多事情实际上需要靠智谱AI自己摸索，一边学习一边实践。实际上也是如此，比如智谱AI训练GLM-130B时，整个研发和训练过程总共花费了8-9个月的时间，但最终稳定的训练其实只花费了不到2个月的时间，团队大部分精力都用在了适应性调整和系统调整上。</p><p>&nbsp;</p><p>不仅如此，早期的智谱AI 并没有现在的“吸金”能力，资源缺乏是其起步阶段不得不面对的问题。2021年，智谱AI 决定真正开发一个拥有130亿参数的大模型，这个项目的投资金额已经超过首年合同金额。</p><p>&nbsp;</p><p>如何解决资源困境？用张鹏的话就是到处“化缘”。团队与国家科研机构及超算中心等联系，获得支持、渡过难关。</p><p>&nbsp;</p><p>对内，智谱AI 一直注意在研发过程中合理分配和利用资源。尤其在初期，团队更加节约，租用计算资源后就以最短的时间完成工作，尽量让每一分钱花得物有所值。</p><p>&nbsp;</p><p>团队需要在不浪费资源的前提下，找到训练速度、精度和稳定性的最佳平衡点。这是一项复杂的工作：提高精度可能会使训练过程容易出问题，从而耗费额外时间和资源；反之，如果牺牲精度以保持稳定性，最终的结果可能不如预期。</p><p>&nbsp;</p><p>虽然当时缺乏可供参考的标准，但智谱AI根据一些开源项目和技术报告，设计了适合自己需求的解决方案，包括混合精度、流水线工作方式、加速方法等等。这种自定义的方法帮助智谱AI提高了资源利用率，也还需要一些时间来完善。</p><p>&nbsp;</p><p>众所周知，英伟达的GPU价格上涨，直接导致硬件成本增加。原本100万元的硬件如今需要花费1.5倍甚至1.6～1.7倍的价格来购买，大大提高了研发和应用成本。</p><p>&nbsp;</p><p>在解决硬件成本问题方面，智谱AI 选择用国产芯片替代，对模型做了各种国产GPU等硬件设备的适配。自2022年初，GLM 系列模型已支持在昇腾、神威超算、海光 DCU 架构上进行大规模预训练和推理。张鹏表示，国产芯片虽然在价格和性能方面可能距国外芯片有些距离，但在某些特定应用场景，尤其是在边缘计算等领域是可以满足需求的。</p><p>&nbsp;</p><p>通过高效动态推理和显存优化，智谱AI 表示，对比伯克利大学推出的 vLLM 以及 Hugging Face TGI 的最新版本，自己的推理速度提升了2-3倍，推理成本降低一倍，每千 tokens 仅0.5分。</p><p>&nbsp;</p><p>“一旦你经历过一次，积累了全面的经验，不管是遇到了问题还是进展顺利，你都会从中学到很多。你将不再是一张白纸，而是会根据以往的经验不断改进和完善。所以那个时候的困难主要在于缺乏经验，一旦积累了经验，后续的工作就会变得更容易。”张鹏总结道。</p><p>&nbsp;</p><p></p><h2>商业化？开源？</h2><p></p><p>&nbsp;</p><p>作为一家从研究机构出来的公司，智谱AI要比OpenAI更关注商业化。</p><p>&nbsp;</p><p>OpenAI总部位于美国硅谷，其科技创新生态系统和组织方式与国内有很大的不同。OpenAI更多是依赖资本支持积累大量资源，如微软等大公司提供资源、人才和数据，以快速实现目标。早期的OpenAI拥有足够的资源，因此并不太关心推理成本等问题。当然，OpenAI现在也开始关注加速和优化等方面的平衡问题，并且更多地依赖微软等公司来进行商业化。</p><p>&nbsp;</p><p>而智谱AI则是从成立之初便就在思考商业化的问题，“带着客户入场”也是被资本看好的因素之一。</p><p>&nbsp;</p><p>智谱AI的商业化路径主要面向企业和机构的B端用户。一方面，创始团队在B端的经验比较多。早期在学校的科技情报分析、数据挖掘等研究经历帮助智谱AI接触到了国内的科研机构、科技型企业、互联网企业，甚至一些国际顶尖科技企业，他们也成为智谱AI的首批客户来源。</p><p>&nbsp;</p><p>另一方面，向C端用户收费是比较有挑战的。智谱AI只为C端用户开发了一个免费使用的APP工具。</p><p>&nbsp;</p><p>不过在张鹏看来，无论是ToB还是ToC，两者最终都会融合，即服务企业最终也会影响到终端用户，因此两种选择本质上没有太大的区别，只是路径优先级的不同。</p><p>&nbsp;</p><p>在创业早期，智谱AI不会强迫自己去接复杂的客户需求，因为这些需求很可能让团队陷入其中无法自拔。“更复杂的问题需要暂时搁置、等到能力更成熟时再解决。”智谱AI会坦诚自己的能力在什么水平上，在该水平上可以创造什么样的价值。</p><p>&nbsp;</p><p>智谱AI也不会特别限定目标客户。张鹏表示，这一轮由大型模型引领的AI技术革新比上一代技术强大得多，具有更广泛的通用性，提供了巨大的创新空间，会影响到很多甚至之前意想不到的领域。</p><p>&nbsp;</p><p>张鹏举了一个民航的例子。民航飞行控制行业使用国际标准的数据报文来编制飞行信息，编码方式非常晦涩难懂，专业人士有时也难以理解。为了减少通信数据量和解决带宽等问题，业内通常会压缩数据，在实际使用时再将其还原。之前，企业需要庞大的团队手工编程将这些数据翻译成可读格式，非常繁琐。但将这些数据输入后让AI解释，AI能理解八九不离十。</p><p>&nbsp;</p><p>在IT行业，与商业对应的就是开源。Meta 无意打开了大模型开源的“潘多拉魔盒”，影响了很多大模型厂商对于“封闭还是开放”的选择。</p><p>&nbsp;</p><p>“我认为开源和商业化并不矛盾。事实上，已经有许多成功的开源和商业化项目，如Linux、Hadoop等，这些项目都表现出色，所以这两者并不互斥。”张鹏说道。</p><p>&nbsp;</p><p>目前，智谱AI已经开源了ChatGLM3-6B模型、多模态CogVLM-17B和智能体AgentLM等能力。开源对智谱AI来说主要有两个好处：一方面，开源社区主要依赖社区成员的共同努力和影响，项目开源后可以吸引更多的人使用，从而提高项目的质量和成熟度；另一方面，企业提供中文语境下的模型和技术，能在全球开源项目中发出中国声音，同时也能够学习和借鉴国外的先进技术和经验，这种跨文化的合作和知识共享有助于推动整个领域的发展。</p><p>&nbsp;</p><p>“在相当长的一段时间内，开源和商业化版本会并存，而且它们并不矛盾，而是相互促进、形成良性循环。”张鹏说道，“开源在保障生态多样性方面扮演着重要角色，而商业应用则关注稳定性、安全性和生态的持续性。只要能够建立良性循环，这种并存的格局将持续存在很长时间。”</p><p>&nbsp;</p><p>不过，虽然开源是免费的，但企业商业化还是需要一些成本的，资金能力不同的企业需要在成本和质量之间寻求自己的平衡。厂商则需要为不同预算范围的客户设计不同的解决方案和产品，并考虑不同的定价策略，从而使用户的成本降低。</p><p>&nbsp;</p><p></p><h2>“现在更需要商业化人才”</h2><p></p><p>&nbsp;</p><p>智谱AI和OpenAI的团队构成在某种程度上是相似的，OpenAI研究团队主要来自世界顶级大学，而智谱AI的团队主要来自清华大学。</p><p>&nbsp;</p><p>在智谱AI早期，团队构建比较简单。最初的团队起源于实验室，由一些老师、学生以及工程师组成。研究人员和科学家在实验室里带领学生一起工作，研发新技术。然后，工程师将这些技术转化为系统和应用程序，而少数商业人员与客户互动。初期，商业化工作也由工程师或研究人员来担任，他们在多个领域兼职担任不同的职责。</p><p>&nbsp;</p><p>智谱AI组织架构的发展是渐进式的：从内部研究开始，然后逐渐扩展到工程、系统平台、应用和商业化等领域，各部门之间不是独立的实体，而是相互协作、信息流畅的整体。这种紧密的团队协作方式减少了信息传递的损失，使团队能够更高效地应对快速变化的市场需求。</p><p>&nbsp;</p><p>现在，智谱AI已经有大约400名正式员工，其中约70%从事研发工作。</p><p>&nbsp;</p><p>管理方法上，智谱AI 与一般的互联网企业相似。每个人都有自己的日常任务，但当需要集中精力处理某些事情时，如客户交付、产品开发或技术研究，公司就会从各个团队中选择适合的人负责。</p><p>&nbsp;</p><p>团队的负责人在整个团队中发挥着管理和协调的关键作用，他们的职责包括确保各部门之间的高效协作。比如，在一个重要的商业化项目中，负责人的角色涵盖了项目从研究、开发到最终的市场推广的整个生命周期，这需要团队中的博士研究员、科学家、分级经理、工程师、系统专家和应用程序开发人员等人的共同协作。</p><p>&nbsp;</p><p>同样，在研究性项目中，负责人也需要协调不同层次和专业领域的团队成员，以确保项目的成功。无论是商业项目还是研究项目，都需要各方面的知识和专业技能的有机结合来解决复杂的问题和推动项目取得成功。</p><p>&nbsp;</p><p>随着公司的发展，智谱AI的团队构成也在随之变化。在早期，智谱AI要解决很多研究性问题，因此主要集中在研究团队。发展中期，团队增加了工程方面的人才，以优化模型的研发和训练，需要解决系统和应用相关的问题，并将应用推向市场。现在，智谱AI的团队更加需要商业方面的人才。</p><p>&nbsp;</p><p>“大规模模型的商业化是一个新兴领域，需要面对一些独特的挑战，尤其是在教育客户和应对客户的各种问题时。”张鹏说道。</p><p>&nbsp;</p><p>在张鹏看来，大模型时代的商业化人才需要具备强大的学习能力来快速掌握新技术和概念、需要有一定的技术敏感度、优秀的沟通能力和解决问题的能力，还要有具备市场洞察能力，以便制定有效的推广策略。</p><p>&nbsp;</p><p>对于当下智谱AI的主题是将大型模型产业化并落地应用。这一阶段要求更广泛的技能和角色，技术方面主要包括以下：</p><p>&nbsp;</p><p>数据分析师：整理、分析和处理大量数据，以确保数据的质量和有用性，以供模型的训练和应用。提示词工程师：这是一个新兴的角色，专注与大型模型进行高效沟通，以产生客户所需的数据和回应。这个角色可能不需要深入研究和训练模型，但需要懂得如何有效地使用模型。在特定领域或应用中的专家：能够为各种行业和领域提供个性化解决方案。</p><p>&nbsp;</p><p>“这个时代对IT行业来说既是幸运，也具有挑战。因为技术变化如此之快，你必须保持高效地不断了解和深入研究新技术。今天掌握的知识在短短一个月内可能就会变得过时。”张鹏说道，“持续学习是一项非常重要的任务。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>目前，大家对大模型技术的认识参差不齐，这也导致了落地上的一些问题。比如有的客户对这项技术不太了解，不清楚厂商在做什么，因此会根据他们的理解提出很多问题。而也有客户则认为他们非常了解这项技术，因此会期望过高，并设定更高的目标。实际上，大家需要在一个相对合理的范围内达成一致。这也是张鹏最近分享的原因之一。</p><p>&nbsp;</p><p>比尔盖茨曾说：“无论对谁来说，640K内存都足够了”。然而，现在随处可见大内存的手机。未来，对于任何人来说都很难预测。</p><p>&nbsp;</p><p>在张鹏看来，AIGC未来发展会很像云计算的轨迹，成为基础设施，而不是互联网生态下的应用。</p><p>&nbsp;</p><p>“在互联网应用中，有很多并行存在的应用，每个应用专注于特定场景。但基础设施领域的情况不同。基础设施的特点是随着规模的增加变得更加集中，资源的利用率越高、整体性能更高，产出投入比也更高。因此，基础设施需要规模效应，大型模型也具备这种特性。”张鹏解释道。</p><p>&nbsp;</p><p>但在当前的成本和回报条件下，基础的通用模型仍需要足够大的数据、足够低的成本、足够多的计算能力来进行训练。因此，未来可能会出现几家公司将通用模型的智能水平提升到一定程度，其他公司在此基础上做行业模型和应用的情况。</p><p>&nbsp;</p><p>谁能最终成为通用模型的“大家长”？这个问题还需要留给时间来回答。</p><p>&nbsp;</p><p>本文节选自<a href=\"https://www.infoq.cn/minibook/Ba7kqNscXOQoaZAduZsz\">《中国卓越技术团队访谈录&amp;架构师特刊》</a>\"</p><p>&nbsp;</p><p>大模型风行一年多，创业新秀们都有哪些故事？实际落地中，软件产品中的AIGC能力又如何？本期《中国卓越技术团队访谈录&amp;架构师特刊》中，LeptonAI、智谱AI、Dify.AI&nbsp;和京东云言犀团队深度分享了他们的创业思路和产品经验，来自网易、百度、广推科技等企业的技术专家，也深入探讨关于AIGC&nbsp;编程、算法及应用等话题。</p><p>&nbsp;</p><p>现在识别图中二维码或点击<a href=\"https://www.infoq.cn/minibook/Ba7kqNscXOQoaZAduZsz\">《中国卓越技术团队访谈录&amp;架构师特刊》</a>\"即可下载电子书，查看更多、更详细的精彩内容！</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/19/19d003c1f43e5a20a7cf23de18f26a99.png\" /></p><p></p><p>&nbsp;</p><p>另外，在今年 9 月份的 QCon 全球软件开发大会（北京站）中，张鹏曾作题为《ChatGLM：认知大模型与应用初探》主题演讲，完整幻灯片下载：<a href=\"https://qcon.infoq.cn/202309/beijing/presentation/5432\">https://qcon.infoq.cn/202309/beijing/presentation/5432</a>\"&nbsp;</p><p>&nbsp;</p><p>下一站 QCon 也将继续探索GenAI 和通用大模型应用探索、AI Agent 与行业融合应用的前景、面向人工智能时代的架构等方向。想要参加这场技术人的年终盛会？现在报名即可享受 7 折优惠，购票立减 ¥2040，详情可咨询票务经理 18514549229（微信同手机号）。12 月 28-29 日，上海·中优城市万豪酒店，期待见面！</p><p></p>",
    "publish_time": "2023-11-08 15:33:29",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "要务科技 CEO 石东海，确认担任 QCon 建设具备战略思维和弹性文化的组织专题出品人",
    "url": "https://www.infoq.cn/article/hsNPu3m4Kd0AsphlwCrV",
    "summary": "<p><a href=\"https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1108&amp;utm_content=shidonghai\">QCon 全球软件开发大会</a>\"，将于 12 月在上海召开。要务科技 CEO 石东海将担任「<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1607?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1108&amp;utm_content=shidonghai\">建设具备战略思维和弹性文化的组织</a>\"」的专题出品人。在此次专题中，你将了解到团队和领导者需要采取什么样的方法，来改善组织。当然，也会提到 GenAI，探讨像提示库这样的 AI 工具如何提高组织的协作弹性。</p><p></p><p><a href=\"https://qcon.infoq.cn/2023/shanghai/track/1607?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1108&amp;utm_content=shidonghai\">石东海</a>\"，北京邮电大学研究生毕业，曾任职于百度、Intel 等企业，原滴滴品质出行事业群 CTO，普惠出行事业群 CTO，出租车事业部总经理，代驾事业部总经理。现投身于工业互联网产业创业，要务科技 CEO。是技术出身转型为业务一号位的管理者，在技术架构，技术组织发展和业务管理和经营方面有丰富的经验。</p><p></p><p>相信石东海的到来，可以帮助提升此专题的质量，让你学习到组织的领导者需要具备战略思维和弹性文化，才能建立坚实的信任基础，以及有助于团队健康发展和提升效率的战术工具。以及，技术团队需要具备高适应性，才能够在需求变化、故障排除、颠覆性技术落地等情况下持续发展。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart\">智能化信创软件&nbsp;IDE</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart\">GenAI和通用大模型应用探索</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的性能优化</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart\">LLM&nbsp;时代的大前端技术</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart\">面向人工智能时代的架构</a>\"、<a href=\"https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart\">性能工程：提升效率和创新的新方法</a>\"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png\" /></p><p></p>",
    "publish_time": "2023-11-08 16:30:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]