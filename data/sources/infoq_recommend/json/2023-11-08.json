[
  {
    "title": "谷歌开源 AI 微调方法： Distilling Step-by-Step",
    "url": "https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP",
    "summary": "<p>华盛顿大学和谷歌研究中心的一个团队最近开源了 <a href=\"https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html\">Distilling Step-by-Step</a>\"（逐步蒸馏），一种用于微调规模较小的语言模型的技术。与标准微调相比，逐步蒸馏需要的训练数据更少，并且生成的模型更小，但模型性能却优于参数规模是它 700 倍的小样本提示大型语言模型 （LLM）。</p><p>&nbsp;</p><p>虽然 LLM 一般可以在提示较少的情况下在多种任务上有良好的表现，但由于其内存和算力要求过高，模型的托管是比较有挑战的。规模较小的模型在微调后也可以有良好的表现，但这需要工程师手动创建针对具体任务优化的数据集。逐步蒸馏的关键思想是使用 LLM 自动生成一个小型微调数据集，其中的数据有一个输入和一个输出标签，以及选择这个输出标签的“理由”。微调过程会训练这个小模型来预测输出标签并生成对应的理由。在 NLP 基准上评估时，小型微调模型的性能优于 540B PaLM 模型，同时仅需要这个基准测试的全部微调数据的 80%。据谷歌称：</p><p></p><p></p><blockquote>我们展示了，逐步蒸馏既减少了构建针对特定任务的较小模型所需的训练数据集规模，也减少了实现甚至超越小样本提示 LLM 的性能水平所需的模型大小。总的来说，逐步蒸馏提出了一种可以高效利用资源的范例，可以解决模型大小和所需训练数据之间的权衡问题。</blockquote><p></p><p></p><p>研究表明，增加 LLM 中的参数规模可以提高其性能，目前最先进的模型（例如 PaLM）拥有数百亿个参数。然而，这些大型模型价格昂贵，且难以用于推理，因为它们需要多个并行连接的 GPU 才能把这么多参数保存在内存里。最近的研究开发出了规模稍小的模型（例如 Meta 的 Llama 2），其性能表现差不多，但参数少了一个数量级；然而，这些小一些的模型还是很庞大，需求的算力也很高。</p><p>&nbsp;</p><p>要做出在特定任务上表现良好的小模型的一种方法，是使用针对具体任务收集的数据集来微调小规模语言模型。虽然这个数据集可能相对较小（大约有数千个示例），但其数据收集起来可能还是费时费钱。另一种选择是知识蒸馏，也就是使用大型模型作为较小模型的老师。 InfoQ 最近报道了谷歌开发的一项<a href=\"https://www.infoq.com/news/2023/01/google-llm-self-improvement/\">技术</a>\"，使用 PaLM LLM 来创建训练数据集，最后生成的微调模型的性能可与规模大 10 倍的 LLM 相媲美。</p><p>&nbsp;</p><p>逐步蒸馏确实需要微调数据集，但它减少了创建高性能模型所需的数据量。源数据集通过思维链提示输入 PaLM LLM，要求模型给出其答案的理由。输出结果是修正后的微调数据集，其中包含原始输入和答案以及理由。这个较小的目标模型经过微调来执行两项任务：回答原始问题并生成理由。</p><p>&nbsp;</p><p>谷歌使用四个 NLP 基准测试评估了他们的技术，每个基准都包含一个微调数据集。他们使用逐步蒸馏来修正这些数据集，并使用了参数不到 1B 的微调 T5 模型。他们发现，这些模型在仅使用数据集的一小部分数据的情况下，性能就比基线微调模型要好；在某些情况下只要 12.5% 的数据就有这样的表现。他们还发现，他们的 770M 参数模型在 ANLI 基准测试中的性能优于大它 700 倍的 540B 参数 PaLM，同时只需要 80% 的微调数据集数据。</p><p>&nbsp;</p><p>在 X（以前的 Twitter）上关于这项工作的讨论中，人工智能企业家 Otto von Zastrow 写道：</p><p></p><p></p><blockquote>这些结果非常厉害。我会把这种办法叫做合成数据生成，而不是蒸馏，我真的很好奇，如果你根据每个示例问题的合成理由来训练原始的 LLM 会发生什么事情。</blockquote><p></p><p></p><p>逐步蒸馏的源代码和训练数据集可在 <a href=\"https://github.com/google-research/distilling-step-by-step\">GitHub</a>\" 上获取。 Google Cloud 的 Vertex AI 平台还提供该算法的非公开预览。</p><p>&nbsp;</p><p>原文链接：</p><p><a href=\"https://www.infoq.com/news/2023/10/google-distillation/\">https://www.infoq.com/news/2023/10/google-distillation/</a>\"</p>",
    "publish_time": "2023-11-08 08:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "中国卓越技术团队访谈录 & 架构师特刊：软件产品中的AIGC",
    "url": "https://www.infoq.cn/article/Ba7kqNscXOQoaZAduZsz",
    "summary": "<h2>封面故事</h2>\n<ul>\n<li>我，一个 95 后，从阿里辞职与贾扬清去硅谷创业</li>\n</ul>\n<p>“建议大家不要被大模型束缚住。实际落地时，除了大模型外，还可以充分利用许多已存在的深度学习模型或传统模型。例如在图像处理方面，卷积神经网络（CNN）实际上可能比大模型更适用。”</p>\n<h2>独家对话·大模型领航者</h2>\n<ul>\n<li>被时代选中的智谱AI：成为OpenAI，超越OpenAI</li>\n</ul>\n<p>在创业早期，智谱AI不会强迫自己去接复杂的客户需求，因为这些需求很可能让团队陷入其中无法自拔。“更复杂的问题需要暂时搁置、等到能力更成熟时再解决。”智谱AI会坦诚自己的能力在什么水平上，在该水平上可以创造什么样的价值。</p>\n<ul>\n<li>丢掉LangChain、像Docker一样编排大模型应用程序：这支十余人的年轻创业团队如何在2个月做出一个LLMOps平台？</li>\n</ul>\n<p>“在创业初期，一些朋友和投资人认为市场潜力巨大，但竞争也激烈。云服务提供商、大模型公司以及机器学习和运营的公司都进入这个领域。我们的挑战在于如何应对竞争。”</p>\n<ul>\n<li>是全部重做还是融合改造？揭秘京东云言犀升级全过程</li>\n</ul>\n<p>大模型对一些传统技术确实有影响。因为LLM之前的模型几乎全都是专项的，任务越窄，表现越好。对于更通用、泛化的任务来说，大模型带来的效果确实是颠覆性的。</p>\n<h2>AIGC 实践前沿</h2>\n<ul>\n<li>文生图大型实践：揭秘百度搜索AIGC绘画工具的背后故事</li>\n</ul>\n<p>这是一个巨大的变革，从过去用户在全网寻找图像，转变为结合了查找图像和生成图像两种方式，以满足用户更具体的需求，这也在一定程度上鼓励用户更主动地表达他们真正的需求。</p>\n<ul>\n<li>AIGC 编程：代码编程模型的应用与挑战</li>\n</ul>\n<p>大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。</p>\n<p>AIGC算法揭秘及产业落地应用分享</p>\n<p>使用大模型在某种程度上为我们提供了更多的可能性，但也引入了更多的复杂性。没有一个通用的模板或规则来告诉我们应该使用哪个模型。</p>\n<ul>\n<li>广告创意领域中AIGC的应用</li>\n</ul>\n<p>随着AI能力的提升，人们需要深入使用并掌握它。像fine-tuning、LangChain等我不会推荐，我会鼓励周围的人去深入使用AIGC，重点在于使用，好的AGI只需要被编译一次。</p>\n<h2>管理能力进阶</h2>\n<ul>\n<li>影响力打造：一位前 Twitter 8 年技术主管总结的经验教训</li>\n<li>日常沟通之道：走向果敢</li>\n<li>科技巨头是如何迷失方向的？探讨大型科技企业的问责制度</li>\n</ul>\n<h2>文章推荐</h2>\n<ul>\n<li>大模型部署昂贵的原因：用最贵的模型处理最基本任务，犹如“让兰博基尼送披萨”</li>\n<li>2023 年 AI 与开源行业：今年第一篇盘点文章出炉了</li>\n<li>ChatGPT 已成为2023年最大金矿，大家是怎么靠它挣到钱的？</li>\n</ul>",
    "publish_time": "2023-11-08 09:00:00",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  },
  {
    "title": "Apache Doris 2.0 如何实现导入性能提升 2-8 倍",
    "url": "https://www.infoq.cn/article/8kEox8KdpTjOh4s1gRH4",
    "summary": "<p>数据导入吞吐是 OLAP 系统性能的重要衡量标准之一，高效的数据导入能力能够加速数据实时处理和分析的效率。随着<a href=\"http://doris.apache.org/\"> Apache Doris</a>\" 用户规模的不断扩大， 越来越多用户对数据导入提出更高的要求，这也为 Apache Doris 的数据导入能力带来了更大的挑战。</p><p></p><p>为提供快速的数据写入支持，Apache Doris 存储引擎采用了类似 LSM Tree 结构。在进行数据导入时，数据会先写入 Tablet 对应的 MemTable 中，MemTable 采用 SkipList 的数据结构。当 MemTable 写满之后，会将其中的数据刷写（Flush）到磁盘。数据从 MemTable 刷写到磁盘的过程分为两个阶段，第一阶段是将 MemTable 中的行存结构在内存中转换为列存结构，并为每一列生成对应的索引结构；第二阶段是将转换后的列存结构写入磁盘，生成 Segment 文件。</p><p></p><p>具体而言，Apache Doris 在导入流程中会把 BE 模块分为上游和下游，其中上游 BE 对数据的处理分为 Scan 和 Sink 两个步骤：首先 Scan 过程对原始数据进行解析，然后 Sink 过程将数据组织并通过 RPC 分发给下游 BE。当下游 BE 接收数据后，首先在内存结构 MemTable 中进行数据攒批，对数据排序、聚合，并最终下刷成数据文件（也称 Segment 文件）到硬盘上来进行持久化存储。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/d4/d42ba7c3789fe212c67c8382350dbf46.png\" /></p><p></p><p>而我们在实际的数据导入过程中，可能会出现以下问题：</p><p></p><p>因上游 BE 跟下游 BE 之间的 RPC 采用 Ping-Pong 的模式，即下游 BE 一个请求处理完成并回复到上游 BE 后，上游 BE 才会发送下一个请求。如果下游 BE 在 MemTable 的处理过程中消耗了较长的时间，那么上游 BE 将会等待 RPC 返回的时间也会变长，这就会影响到数据传输的效率。当对多副本的表导入数据时，需要在每个副本上重复执行 MemTable 的处理过程。然而，这种方式使每个副本所在节点都会消耗一定的内存和 CPU 资源，不仅如此，冗长的处理流程也会影响执行效率。</p><p></p><p>为解决以上问题，我们在刚刚发布不久 Apache Doris 2.0 版本中（https://github.com/apache/doris/tree/2.0.1-rc04 ），对导入过程中 MemTable 的攒批、排序和落盘等流程进行优化，提高了上下游之间数据传输的效率。此外我们在新版本中还提供 “单副本导入” 的数据分发模式，当面对多副本数据导入时，无需在多个 BE 上重复进行 MemTable 工作，有效提升集群计算和内存资源的利用率，进而提升导入的总吞吐量。</p><p></p><h1>MemTable 优化</h1><p></p><p></p><h2>01  写入优化</h2><p></p><p></p><p>在 Aapche Doris 过去版本中，下游 BE 在写入 MemTable 时，为了维护 Key 的顺序，会实时对 SkipList 进行更新。对于 Unique Key 表或者 Aggregate Key 表来说，遇到已经存在的 Key 时，将会调用聚合函数并进行合并。然而这两个步骤可能会消耗较多的处理时间，从而延迟 RPC  响应时间，影响数据写入的效率。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/ef/ef6c90e63726cbf189a4dac894d0577b.png\" /></p><p></p><p>因此我们在 2.0 版本中对这一过程进行了优化。当下游 BE 在写入 MemTable 时，不再实时维护 MemTable 中 Key 的顺序，而是将顺序的保证推迟到 MemTable 即将被下刷成 Segment 之前。此外，我们采用更高效的 pdqsort 来替代 std::sort ，实现了缓存友好的列优先排序方式，并取得了更好的排序性能。通过上述两种手段来保证 RPC 能够被及时响应。</p><p></p><h2>02  并行下刷</h2><p></p><p></p><p>在导入过程中，当下游 BE 将一个 MemTable  写入一定大小之后，会把 MemTable 下刷为 Segment 数据文件来持久化存储数据并释放内存。为了保证前文提到的 Ping-Pong RPC 性能不受影响，MemTable 的下刷操作会被提交到一个线程池中进行异步执行。</p><p></p><p>在 Apache Doris 过去版本中，对于 Unique Key 的表来说，MemTable 下刷任务是串行执行的，原因是不同 Segment 文件之间可能存在重复 Key，串行执行可以保持它们的先后顺序，而 Segment 序号是在下刷任务被调度执行时分配的。同时，在 Tablet 数量较少无法提供足够的并发时，串行下刷可能会导致系统的 IO 资源无法重复被利用。而在 Apache Doris 2.0 版本中，由于我们将 Key 的排序和聚合操作进行了后置，除了原有的 IO 负载以外，下刷任务中还增加了 CPU 负载（即后置的排序和聚合操作）。此时若仍使用串行下刷的方式，当没有足够多 Tablet 来保证并发数时，CPU 和 IO 会交替成为瓶颈，从而导致下刷任务的吞吐量大幅降低。</p><p></p><p>为解决这个问题，我们在下刷任务提交时就为其分配 Segment 序号，确保并行下刷后生成的 Segment 文件顺序是正确的。同时，我们还对后续 Rowset 构建流程进行了优化，使其可以处理不连续的 Segment 序号。通过以上改进，使得所有类型的表都可以并行下刷 MemTable，从而提高整体资源利用率和导入吞吐量。</p><p></p><h2>03  优化效果</h2><p></p><p></p><p>通过对 MemTable 的优化，面对不同的导入场景，Stream Load 的吞吐量均有不同幅度的提升（详细对比数据可见下文）。这项优化不仅适用于Stream Load ，还对 Apache Doris 支持的其他导入方式同样有效，例如 Insert Into、Broker Load、S3 Load 等，均在不同程度提升了导入的效率及性能。</p><p></p><h1>单副本导入</h1><p></p><p></p><h2>01  原理和实现</h2><p></p><p></p><p>在过去版本中，当面对多副本数据写入时，Apache Doris 的每个数据副本均需要在各自节点上进行排序和压缩，这样会造成较大的资源占用。为了节约 CPU 和内存资源，我们在 Apache Doris 在 2.0 版本中提供了单副本导入的能力，该能力会从多个副本中选择一个副本作为主副本（其他副本为从副本），且只对主副本进行计算，当主副本的数据文件都写入成功后，通知从副本所在节点直接接拉取主副本的数据文件，实现副本间的数据同步，当所有从副本节点拉取完后进行返回或超时返回（大多数副本成功即返回成功）。该能力无需一一在节点上进行处理，减少了节点的压力，而节约的算力和内存将会用于其它任务的处理，从而提升整体系统的并发吞吐能力。</p><p></p><p><img src=\"https://static001.geekbang.org/infoq/10/10f6e87b64f6c1731a0c3c1841aceb35.jpeg\" /></p><p></p><h2>02  如何开启</h2><p></p><p>FE 配置：</p><p></p><p><code lang=\"text\">enable_single_replica_load = true\n</code></p><p></p><p>BE 配置：</p><p></p><p><code lang=\"text\">enable_single_replica_load = true\n</code></p><p></p><p>环境变量（insert into）</p><p></p><p><code lang=\"text\">SET  experimental_enable_single_replica_insert=true;\n</code></p><p></p><h2>03  优化效果</h2><p></p><p></p><p>对于单并发导入来说，单副本数据导入可以有效降低资源消耗。单副本导入所占的内存仅为三副本导入的 1/3（单副本导入时只需要写一份内存，三副本导入时需要写三份内存）。同时从实际测试可知，单副本导入的 CPU 消耗约为三副本导入的 1/2，可有效节约 CPU 资源。对于多并发导入来说，在相同的资源消耗下，单副本导入可以显著增加任务吞吐。同时在实际测试中，同样的并发导入任务， 三副本导入方式耗时 67 分钟，而单副本导入方式仅耗时 27 分钟，导入效率提升约 2.5 倍。具体数据请参考后文。</p><p></p><h1>性能对比</h1><p></p><p></p><p>测试环境及配置：</p><p></p><p>3 个 BE (16C 64G)，每个 BE 配置 3 块盘 （单盘读写约 150 MB/s）1 个 FE，共享其中一个 BE 的机器</p><p></p><p>原始数据使用 TPC-H SF100 生成的 Lineitem 表，存储在 FE 所在机器的一个独立的盘上（读约 150 MB/s）。</p><p></p><h2>01  Stream Load（单并发）</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/3e/3e3266cd4a7d540af44ec9fd4b007de4.png\" /></p><p></p><p>以上述列举的单并发场景来说，Apache Doris 2.0 版本整体的导入性能比 1.2.6 版本提升了 2-7 倍；在多副本前提下，开启新特性单副本导入，导入性能提升了 2-8 倍。</p><p></p><h2>02  INSERT INTO （多并发）</h2><p></p><p><img src=\"https://static001.geekbang.org/infoq/de/de06ecae8973af9677b86655f5d91621.png\" /></p><p></p><p>以上述列举的多并发场景来说，Apache Doris 2.0 版本整体比 1.2.6 版本有小幅提升；开启新特性单副本导入后，对在多副本提导入性能提升效果明显，导入速度较 1.2.6 版提升约 50% 。</p><p></p><h1>结束语</h1><p></p><p></p><p>社区一直致力于提升 Apache Doris 导入性能这一核心能力，为用户提供更佳的高效分析体验，通过在 2.0 版本对 Memtable、单副本导入等能力进行优化，导入性能相较于之前版本已经呈现数倍提升。未来我们还将在 2.1 版本中持续迭代，结合 MemTable 的优化方法、单副本优化资源能效理念，以及基于 Streaming RPC 优化后的 IO 模型和精简的 IO 路径对导入性能进一步优化，同时减少导入对查询性能的影响，为用户提供更加卓越的数据导入体验。</p><p></p><p># 作者介绍：</p><p></p><p>陈凯杰，<a href=\"https://selectdb.com/\">SelectDB </a>\"高级研发工程师</p><p></p><p>张正宇，SelectDB 资深研发工程师</p>",
    "publish_time": "2023-11-08 09:51:42",
    "source": {
      "name": "infoq_recommend",
      "desc": "InfoQ推荐",
      "icon": "https://raw.githubusercontent.com/maguowei/today/master/imgs/icon/infoq.png"
    }
  }
]